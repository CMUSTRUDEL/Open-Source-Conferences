Title: LLVM Support for OpenMP 4.0 Target Regions on GPUs â€“ SC14
Publication date: 2014-12-13
Playlist: SC14 OpenMP BOF and Booth Talks
Description: 
	Presented by Samuel Antao, IBM, at SC14, November 20, 2014
Captions: 
	00:00:00,000 --> 00:00:06,299
alright so my name is Samuel in town I

00:00:03,210 --> 00:00:07,919
come from from IBM Research and today

00:00:06,299 --> 00:00:10,650
I'd like to tell you a little bit about

00:00:07,919 --> 00:00:14,130
the work that we have been doing about

00:00:10,650 --> 00:00:17,760
adding support for OpenMP of floating

00:00:14,130 --> 00:00:20,189
for GPUs in a in a clone compiler so I

00:00:17,760 --> 00:00:23,670
go directly to this the first slide here

00:00:20,189 --> 00:00:27,119
when I try to to summarize some of the

00:00:23,670 --> 00:00:29,039
properties of OpenMP so openmp is about

00:00:27,119 --> 00:00:32,550
relative language that allows the

00:00:29,039 --> 00:00:34,770
programmer to to annotate the code and

00:00:32,550 --> 00:00:37,050
controlled execution in parallel of

00:00:34,770 --> 00:00:39,989
different sections in the code so you

00:00:37,050 --> 00:00:42,360
can you can assign you can assign a

00:00:39,989 --> 00:00:45,210
given directive to given code region to

00:00:42,360 --> 00:00:47,850
specify I want these big iterations on

00:00:45,210 --> 00:00:51,239
this loop to be distributed by a lot of

00:00:47,850 --> 00:00:54,149
tracks I also can't specify tasks along

00:00:51,239 --> 00:00:55,860
with a set of dependences so I can I can

00:00:54,149 --> 00:00:59,100
execute this task in Perl and still

00:00:55,860 --> 00:01:03,750
false the execution to comply with these

00:00:59,100 --> 00:01:05,549
dependencies and also he can be used to

00:01:03,750 --> 00:01:08,490
give the compiler hints about

00:01:05,549 --> 00:01:11,939
optimizations like we have like loops

00:01:08,490 --> 00:01:14,880
that we can try to to make symbolization

00:01:11,939 --> 00:01:18,030
and things like that and another thing

00:01:14,880 --> 00:01:20,159
that is nice about openmp is that it is

00:01:18,030 --> 00:01:25,380
aspects to get it has a specification

00:01:20,159 --> 00:01:27,780
both for C C++ and Fortran so what what

00:01:25,380 --> 00:01:31,020
one thing that is new in the latest

00:01:27,780 --> 00:01:33,750
specification in four-point-oh is is the

00:01:31,020 --> 00:01:36,570
ability not only to take advantage of

00:01:33,750 --> 00:01:39,630
the oast resources but are also of

00:01:36,570 --> 00:01:41,610
targets and now or accelerators that may

00:01:39,630 --> 00:01:45,630
be attached to your system so we now

00:01:41,610 --> 00:01:48,149
have directives to to specify offloading

00:01:45,630 --> 00:01:51,960
of a given structure block into a given

00:01:48,149 --> 00:01:54,060
accelerated in my system I so I can mark

00:01:51,960 --> 00:01:57,180
this this target regions I can try to

00:01:54,060 --> 00:02:02,100
specify how I'm going to do the data

00:01:57,180 --> 00:02:05,189
movements and one thing that is true for

00:02:02,100 --> 00:02:08,039
this target regions is that anything I

00:02:05,189 --> 00:02:11,250
directive that is valid for the oast can

00:02:08,039 --> 00:02:13,900
be also present inside this dis regions

00:02:11,250 --> 00:02:16,989
and and one of this challenge that is

00:02:13,900 --> 00:02:19,060
this to someone that is trying to to

00:02:16,989 --> 00:02:22,629
implement a compiler for this is that

00:02:19,060 --> 00:02:26,129
the cogeneration for oops for a

00:02:22,629 --> 00:02:26,129
different target whoops

00:02:27,200 --> 00:02:33,110
alright so so that the challenge is that

00:02:30,260 --> 00:02:35,150
now we are we have in the same system

00:02:33,110 --> 00:02:38,540
and we hain't me to make the same

00:02:35,150 --> 00:02:41,090
compiler to support like a huge range of

00:02:38,540 --> 00:02:43,160
different targets with Andrew the

00:02:41,090 --> 00:02:44,690
challenge of the code generation for

00:02:43,160 --> 00:02:47,269
each one of these targets is potential

00:02:44,690 --> 00:02:49,310
different so that means that if we I

00:02:47,269 --> 00:02:51,530
have very good approach for on target

00:02:49,310 --> 00:02:52,849
that that approach will not be very good

00:02:51,530 --> 00:02:56,360
for other targets and this is a

00:02:52,849 --> 00:02:58,849
challenge for for the compiler all right

00:02:56,360 --> 00:03:02,180
so so in this slide here I try to

00:02:58,849 --> 00:03:05,780
illustrate a little bit alderson OpenMP

00:03:02,180 --> 00:03:08,510
implementation looks like so here we

00:03:05,780 --> 00:03:10,340
have the opening openmp naval compiler

00:03:08,510 --> 00:03:12,799
so you need something that is able to

00:03:10,340 --> 00:03:15,140
understand whatever directives I have in

00:03:12,799 --> 00:03:18,260
my program and this is this compiler is

00:03:15,140 --> 00:03:20,090
going to produce a fat binary that now

00:03:18,260 --> 00:03:23,239
not only is going to contain the else

00:03:20,090 --> 00:03:25,640
code but also the code for the devices I

00:03:23,239 --> 00:03:28,340
want to use to accelerate my my

00:03:25,640 --> 00:03:30,769
execution and these two pieces in this

00:03:28,340 --> 00:03:33,079
fight binary are going to interface with

00:03:30,769 --> 00:03:36,079
runtime libraries so we have a runtime

00:03:33,079 --> 00:03:39,739
library for the device and we also need

00:03:36,079 --> 00:03:42,620
a runtime library for the oast and this

00:03:39,739 --> 00:03:45,319
and the latter we can divide into two

00:03:42,620 --> 00:03:47,690
sub components one is to deal with

00:03:45,319 --> 00:03:50,239
operating system like request strides in

00:03:47,690 --> 00:03:52,880
up in it from the operating system and

00:03:50,239 --> 00:03:54,620
try to to you know to distribute the

00:03:52,880 --> 00:03:58,040
work load to these threats and another

00:03:54,620 --> 00:04:00,829
component is required to deal with the

00:03:58,040 --> 00:04:04,430
target to deal with offloading food to

00:04:00,829 --> 00:04:07,459
the accelerator and in implementation

00:04:04,430 --> 00:04:09,859
that we are doing this component is like

00:04:07,459 --> 00:04:12,260
to again to sub components one that is

00:04:09,859 --> 00:04:14,420
target agnostic and is this is all about

00:04:12,260 --> 00:04:16,849
maintaining you know the state to

00:04:14,420 --> 00:04:19,760
maintain like a record of the maps

00:04:16,849 --> 00:04:21,739
between what the pointer means in the US

00:04:19,760 --> 00:04:23,990
and what a point remains in a device and

00:04:21,739 --> 00:04:27,050
maintain this tape and another component

00:04:23,990 --> 00:04:29,570
that is target dependent that the

00:04:27,050 --> 00:04:32,930
runtime uses to speak with the driver so

00:04:29,570 --> 00:04:35,030
so it can offload my launched

00:04:32,930 --> 00:04:37,430
executioner device or I can move the

00:04:35,030 --> 00:04:40,510
data to the device and so for the work

00:04:37,430 --> 00:04:44,320
that we are interested in doing now we

00:04:40,510 --> 00:04:46,600
we are using some some you know a given

00:04:44,320 --> 00:04:51,190
system and if that is based on on a

00:04:46,600 --> 00:04:54,190
power 8 and NVIDIA GPUs and so you know

00:04:51,190 --> 00:04:56,920
to support these gpus we are using the

00:04:54,190 --> 00:04:59,470
CUDA runtime library to support this

00:04:56,920 --> 00:05:02,410
there's the oast part of the target of

00:04:59,470 --> 00:05:06,630
floating and and what you are interested

00:05:02,410 --> 00:05:09,370
is having clunk as a compiler for openmp

00:05:06,630 --> 00:05:11,260
so this is exactly about this component

00:05:09,370 --> 00:05:14,140
that I'm going to talk a little bit more

00:05:11,260 --> 00:05:17,650
in the Indies chocks just just a side

00:05:14,140 --> 00:05:20,350
note i'm not currently building with

00:05:17,650 --> 00:05:25,600
fortress of columbus only and CSC but

00:05:20,350 --> 00:05:29,880
c++ compiler so i'm not i'm not going to

00:05:25,600 --> 00:05:33,250
talk about frota in the english stop so

00:05:29,880 --> 00:05:35,740
all right so I don't know how many of

00:05:33,250 --> 00:05:37,960
you already know or llvm is I'm just

00:05:35,740 --> 00:05:42,420
describing you very very quickly what

00:05:37,960 --> 00:05:44,740
llvm is about so you can see llvm as

00:05:42,420 --> 00:05:47,320
open source infrastructure and you can

00:05:44,740 --> 00:05:49,870
see it is the library that one can use

00:05:47,320 --> 00:05:52,570
to build compilers so it has a very very

00:05:49,870 --> 00:05:54,850
strong strong API that allows you to

00:05:52,570 --> 00:05:57,400
interface with this library as a very

00:05:54,850 --> 00:05:59,650
mood in code base you should expect to

00:05:57,400 --> 00:06:01,330
find in this library state of our

00:05:59,650 --> 00:06:04,900
algorithms that you can find in any

00:06:01,330 --> 00:06:07,300
other you know compiler production

00:06:04,900 --> 00:06:08,560
quality compiler and one thing that is

00:06:07,300 --> 00:06:09,970
very important for the work that we have

00:06:08,560 --> 00:06:13,290
that we have been doing is that we

00:06:09,970 --> 00:06:16,330
already have support for some backhands

00:06:13,290 --> 00:06:19,660
namely the powerpc and the cuda enabled

00:06:16,330 --> 00:06:23,080
cheap used to the pptx backhanded llvm

00:06:19,660 --> 00:06:26,650
as already in place another property of

00:06:23,080 --> 00:06:28,810
olivium is it has as a very very well

00:06:26,650 --> 00:06:32,290
specified intermediate representation

00:06:28,810 --> 00:06:34,900
that makes very easy to interface with

00:06:32,290 --> 00:06:37,450
so so right now you can find a lot of

00:06:34,900 --> 00:06:40,090
different projects orbiting around llvm

00:06:37,450 --> 00:06:42,430
and one of the project really care about

00:06:40,090 --> 00:06:46,570
for this is clung so clung is nothing

00:06:42,430 --> 00:06:49,630
more than an llvm I our client for for

00:06:46,570 --> 00:06:51,910
the the sea family programming languages

00:06:49,630 --> 00:06:53,780
and what what is good about clung is a

00:06:51,910 --> 00:06:56,090
very fast development pace

00:06:53,780 --> 00:06:58,510
you can you amazed patcher you should

00:06:56,090 --> 00:07:01,160
expect to have you know the latest

00:06:58,510 --> 00:07:03,380
developments in a given specification of

00:07:01,160 --> 00:07:05,630
a language to be already implement and

00:07:03,380 --> 00:07:08,180
also nothing of tongue is that he not

00:07:05,630 --> 00:07:10,400
only acts as a front end but as also

00:07:08,180 --> 00:07:11,930
components to drive this library and

00:07:10,400 --> 00:07:13,580
call you know all this different

00:07:11,930 --> 00:07:16,550
components so I can have front-end

00:07:13,580 --> 00:07:19,010
optimizer back and everything driving by

00:07:16,550 --> 00:07:20,990
clung so one thing that is coming about

00:07:19,010 --> 00:07:24,440
these two things is that they have a

00:07:20,990 --> 00:07:27,140
very modular code base so which where

00:07:24,440 --> 00:07:29,630
which are allows the developer to more

00:07:27,140 --> 00:07:33,380
easily add new features and improve the

00:07:29,630 --> 00:07:35,720
maintainability of the code base another

00:07:33,380 --> 00:07:39,220
night thing is licensed so if you are

00:07:35,720 --> 00:07:41,930
interested in having in creating a and

00:07:39,220 --> 00:07:43,250
and create a new compiler and you want

00:07:41,930 --> 00:07:44,810
to provide a customer's this compiler

00:07:43,250 --> 00:07:48,560
you are able to do that the license

00:07:44,810 --> 00:07:51,560
allows you to do that and though in what

00:07:48,560 --> 00:07:55,370
concerns openmp the support for open mp3

00:07:51,560 --> 00:07:57,320
point own is already in place and but

00:07:55,370 --> 00:07:59,570
this has been done as an external

00:07:57,320 --> 00:08:01,040
project of Glenn compile so what

00:07:59,570 --> 00:08:03,740
happened there was a snapshot that was

00:08:01,040 --> 00:08:06,110
taken from from the main repository and

00:08:03,740 --> 00:08:08,479
the openmp development is being done on

00:08:06,110 --> 00:08:11,000
top of this I'll get into more details

00:08:08,479 --> 00:08:13,700
about you know where you can find this

00:08:11,000 --> 00:08:19,250
OpenMP implementation in the last slides

00:08:13,700 --> 00:08:21,229
of his presentation alright so so what

00:08:19,250 --> 00:08:24,410
Glenn has to do in order to support or

00:08:21,229 --> 00:08:26,570
floating so I in this picture I'm just

00:08:24,410 --> 00:08:28,580
showing you you know the toolchain a

00:08:26,570 --> 00:08:30,770
little the compiler to chain looks like

00:08:28,580 --> 00:08:32,960
and what is new when you have to support

00:08:30,770 --> 00:08:35,300
offloading is that you not only have to

00:08:32,960 --> 00:08:37,669
support those two chains like whether

00:08:35,300 --> 00:08:40,640
the the processor compiling an assembler

00:08:37,669 --> 00:08:43,159
but also have to support like some need

00:08:40,640 --> 00:08:44,990
some different 2 chainz in the same by

00:08:43,159 --> 00:08:47,540
the same compiler so all this thing

00:08:44,990 --> 00:08:49,850
needs to is already in place and clung

00:08:47,540 --> 00:08:53,380
and implemented so what happens is that

00:08:49,850 --> 00:08:56,180
for each input source i use the oast

00:08:53,380 --> 00:08:59,000
preprocessor to obtain the preprocessor

00:08:56,180 --> 00:09:01,400
source code and then i'm going to feed

00:08:59,000 --> 00:09:03,709
this paper says that file into are many

00:09:01,400 --> 00:09:06,910
food chains as many targets the

00:09:03,709 --> 00:09:09,010
user-specified so i'm going to use

00:09:06,910 --> 00:09:11,050
the two chains of these toolchains you

00:09:09,010 --> 00:09:13,120
obtained at the oast object and this and

00:09:11,050 --> 00:09:16,060
they still change obtained the target

00:09:13,120 --> 00:09:17,920
objects then I have a phase that is

00:09:16,060 --> 00:09:22,210
going to link that all the target

00:09:17,920 --> 00:09:24,550
objects inside a shared library so we're

00:09:22,210 --> 00:09:26,230
at and I mean a cell phone out say a

00:09:24,550 --> 00:09:28,570
self-contained image in the sense that

00:09:26,230 --> 00:09:30,820
there are no any symbols left to be

00:09:28,570 --> 00:09:34,690
defined and then the oast linker is

00:09:30,820 --> 00:09:37,870
going to to link the oast the oaths code

00:09:34,690 --> 00:09:40,840
as well as integrate the target image is

00:09:37,870 --> 00:09:43,900
this in a given Health section so in a

00:09:40,840 --> 00:09:47,230
given section in my object so that the

00:09:43,900 --> 00:09:50,080
runtime knows where to locate the the

00:09:47,230 --> 00:09:52,420
image code and be able also to offload

00:09:50,080 --> 00:09:55,420
this code to the device by moving this

00:09:52,420 --> 00:09:58,410
is the bit stream to the device so this

00:09:55,420 --> 00:10:02,130
is what clung is doing as a driver to

00:09:58,410 --> 00:10:04,510
when I want to support or floating

00:10:02,130 --> 00:10:07,660
alright so other component is the code

00:10:04,510 --> 00:10:10,570
generation and in order to understand

00:10:07,660 --> 00:10:12,160
cogeneration we also need to to

00:10:10,570 --> 00:10:14,980
understand the constraints of the target

00:10:12,160 --> 00:10:16,900
we care about which in this case is the

00:10:14,980 --> 00:10:19,660
GPU and they are very very unique

00:10:16,900 --> 00:10:22,300
properties of GPUs that we need to to to

00:10:19,660 --> 00:10:25,030
bear in mind one is that it has massive

00:10:22,300 --> 00:10:28,870
amount of threads that are executed in

00:10:25,030 --> 00:10:31,720
wave fronts that I call in CUDA warps we

00:10:28,870 --> 00:10:33,700
we have treads divided into a logical

00:10:31,720 --> 00:10:38,320
abstraction that groups the treads in

00:10:33,700 --> 00:10:44,400
sets that and they are assumed to

00:10:38,320 --> 00:10:44,400
collaborate more closely and yep

00:10:45,820 --> 00:10:49,510
so the question is what is the

00:10:47,290 --> 00:10:53,950
difference between a warp and a blog so

00:10:49,510 --> 00:10:56,080
so a war a war you can see it as the

00:10:53,950 --> 00:10:58,510
scab the the unit that is used to

00:10:56,080 --> 00:11:01,120
schedule the execution on a GPU so that

00:10:58,510 --> 00:11:03,880
there is the amount of tracks that are

00:11:01,120 --> 00:11:07,540
executed simultaneously at a given time

00:11:03,880 --> 00:11:09,550
in a GPU so amber block consists of

00:11:07,540 --> 00:11:13,120
several threads that are grouped in

00:11:09,550 --> 00:11:15,190
warps and and what is going to happen

00:11:13,120 --> 00:11:17,050
when you are executing a block is that

00:11:15,190 --> 00:11:19,090
one of the warps is going to be executed

00:11:17,050 --> 00:11:21,310
then the next work is going to be

00:11:19,090 --> 00:11:24,280
executed and so on and so forth so so

00:11:21,310 --> 00:11:27,460
you can say that a block contains a set

00:11:24,280 --> 00:11:31,330
of warps and the warps contain a set of

00:11:27,460 --> 00:11:35,220
threads right so the only difference is

00:11:31,330 --> 00:11:38,529
the time where they are used all right

00:11:35,220 --> 00:11:41,649
so another nothing but about the gpus is

00:11:38,529 --> 00:11:44,319
that we have the oldest huge amount of

00:11:41,649 --> 00:11:47,559
threads but the scheduling is is really

00:11:44,319 --> 00:11:49,209
I mean it's really lightweight so is it

00:11:47,559 --> 00:11:53,169
doesn't cost a lot to switch between

00:11:49,209 --> 00:11:56,919
tracks and one thing that is not very

00:11:53,169 --> 00:11:59,009
good is that it isn't it you know having

00:11:56,919 --> 00:12:02,079
one track to spawn many other threads

00:11:59,009 --> 00:12:05,319
opposes a huge overhead in terms of the

00:12:02,079 --> 00:12:07,480
execution latency and another property

00:12:05,319 --> 00:12:10,059
of GPUs is that we need to have

00:12:07,480 --> 00:12:12,160
synchronization barriers to be eat by

00:12:10,059 --> 00:12:14,679
all the treads so that we do not

00:12:12,160 --> 00:12:16,749
incurring deadlox so that's that's

00:12:14,679 --> 00:12:19,509
something that we need to to care about

00:12:16,749 --> 00:12:21,790
so having this I mean we need to be

00:12:19,509 --> 00:12:23,559
really careful in an ocular generation

00:12:21,790 --> 00:12:25,449
so you can really accumulate all these

00:12:23,559 --> 00:12:27,879
constraints this is from the GPS ah but

00:12:25,449 --> 00:12:30,699
we also have some constraints about the

00:12:27,879 --> 00:12:32,410
compiler side because we I mean this is

00:12:30,699 --> 00:12:34,239
a new feature that we are want to add

00:12:32,410 --> 00:12:36,429
into Quan and of course we want to do

00:12:34,239 --> 00:12:40,779
that without disrupting whatever is in

00:12:36,429 --> 00:12:43,929
clung so I mean we we want whenever

00:12:40,779 --> 00:12:46,480
possible to have all the openmp related

00:12:43,929 --> 00:12:48,669
features to be centralized in one blocks

00:12:46,480 --> 00:12:51,160
and we want to reuse you know all the

00:12:48,669 --> 00:12:53,139
cogeneration as that he's already

00:12:51,160 --> 00:12:55,899
present in Clongowes motion as much as

00:12:53,139 --> 00:12:58,600
possible so if we want only to process

00:12:55,899 --> 00:13:00,220
our directives and if possible to leave

00:12:58,600 --> 00:13:01,899
the clue chain of each one of the

00:13:00,220 --> 00:13:04,899
statements that are not related to the

00:13:01,899 --> 00:13:07,119
openmp and touched so that's that's our

00:13:04,899 --> 00:13:08,799
goal so so that the challenge here is to

00:13:07,119 --> 00:13:12,459
cope with the soup with these two

00:13:08,799 --> 00:13:15,369
constraints and our approach here is to

00:13:12,459 --> 00:13:16,629
try to to implement in cuda its

00:13:15,369 --> 00:13:19,179
different approaches to do this

00:13:16,629 --> 00:13:21,279
cogeneration evaluate all of them and

00:13:19,179 --> 00:13:23,470
then assign what which one is going to

00:13:21,279 --> 00:13:25,329
be best and then go ahead and implement

00:13:23,470 --> 00:13:29,169
this approach on the compiler so this is

00:13:25,329 --> 00:13:32,199
all our approach here all right so just

00:13:29,169 --> 00:13:36,459
a little bit of OpenMP here so this is a

00:13:32,199 --> 00:13:41,079
typical this is a typical code off of a

00:13:36,459 --> 00:13:43,269
target region so before eating this

00:13:41,079 --> 00:13:45,819
directive which is a target directive I

00:13:43,269 --> 00:13:47,980
only have an oath stride and this took

00:13:45,819 --> 00:13:49,000
this the directive says that everything

00:13:47,980 --> 00:13:51,700
that is inside

00:13:49,000 --> 00:13:55,570
structure block has to be executed on

00:13:51,700 --> 00:13:58,480
ative on a device right then we have

00:13:55,570 --> 00:14:01,060
another directive here that is the OMP

00:13:58,480 --> 00:14:04,390
teens and this is going to define how

00:14:01,060 --> 00:14:07,330
many teams of treads I'm going to use to

00:14:04,390 --> 00:14:09,700
execute my code and what is a team a

00:14:07,330 --> 00:14:11,920
team is something that maps really

00:14:09,700 --> 00:14:15,790
really well to a block because it means

00:14:11,920 --> 00:14:18,400
that the treads inside this team are

00:14:15,790 --> 00:14:21,340
expected to collaborate really closely

00:14:18,400 --> 00:14:24,130
however there is a you know as a subtle

00:14:21,340 --> 00:14:27,610
thing in a in a in a semantics and that

00:14:24,130 --> 00:14:30,070
is that at this point I'm I'm supposed

00:14:27,610 --> 00:14:33,850
to only have one thread executing in

00:14:30,070 --> 00:14:36,250
each one of my teams so that means that

00:14:33,850 --> 00:14:38,440
I have I'll have the CUDA block with a

00:14:36,250 --> 00:14:41,200
single thread and i'm going to call this

00:14:38,440 --> 00:14:45,070
track the master trebb only when i reach

00:14:41,200 --> 00:14:48,010
this point where i have my parallel for

00:14:45,070 --> 00:14:50,620
i'm going to i want this thread is

00:14:48,010 --> 00:14:52,930
master tread to spawn all these other

00:14:50,620 --> 00:14:55,360
tracks and so only when i reach this

00:14:52,930 --> 00:14:58,030
point i'm able to take full advantage of

00:14:55,360 --> 00:15:00,370
the massive amount of treads i have in

00:14:58,030 --> 00:15:01,960
my in my architecture and then after the

00:15:00,370 --> 00:15:04,390
parallel for again i have my sequential

00:15:01,960 --> 00:15:07,380
code and at the end I i returned to the

00:15:04,390 --> 00:15:07,380
host yep

00:15:09,100 --> 00:15:14,830
so the question is is a fact is if I can

00:15:12,100 --> 00:15:17,140
just blast these two primers in the same

00:15:14,830 --> 00:15:20,170
yeah we can do that so so we're in a

00:15:17,140 --> 00:15:22,840
spec you have also combined directives

00:15:20,170 --> 00:15:24,640
that allows you to do that I just do I

00:15:22,840 --> 00:15:26,950
was just doing this just for the sake of

00:15:24,640 --> 00:15:32,950
earthliness against but that's that sure

00:15:26,950 --> 00:15:35,950
you can do that all right ok so here I'm

00:15:32,950 --> 00:15:37,990
showing you two possible approaches to

00:15:35,950 --> 00:15:40,150
deal with this well the problems that we

00:15:37,990 --> 00:15:42,070
have in the co-chair in your left hand

00:15:40,150 --> 00:15:43,720
side I have what is called the CUDA

00:15:42,070 --> 00:15:45,580
dynamic parallelism and is something

00:15:43,720 --> 00:15:49,000
that the latest versions of Gouda

00:15:45,580 --> 00:15:51,100
already already provide so here at the

00:15:49,000 --> 00:15:55,060
very bottom I have my oast function

00:15:51,100 --> 00:15:58,300
calling my target and then and then I

00:15:55,060 --> 00:16:02,110
call a kernel for each one of my teams

00:15:58,300 --> 00:16:04,660
and then I inside is Colonel I'm going

00:16:02,110 --> 00:16:08,050
to call for each team all the threads I

00:16:04,660 --> 00:16:09,520
require so this is one of the purchasing

00:16:08,050 --> 00:16:11,530
that if i'm going to write to the code

00:16:09,520 --> 00:16:15,160
for this this is out this code is going

00:16:11,530 --> 00:16:17,140
to look like in this side so in your

00:16:15,160 --> 00:16:19,180
right hand side i have a different

00:16:17,140 --> 00:16:23,470
approach which is to have a single

00:16:19,180 --> 00:16:25,780
kernel and what I do is to spawn all the

00:16:23,470 --> 00:16:28,270
threads I'm going to require inside the

00:16:25,780 --> 00:16:31,900
target region and then I'm going to God

00:16:28,270 --> 00:16:35,500
some sequential regions by by checking

00:16:31,900 --> 00:16:37,900
the tread ID against the master tread ID

00:16:35,500 --> 00:16:39,820
in it ended this way i can i can make

00:16:37,900 --> 00:16:42,880
this region to be executed by a single

00:16:39,820 --> 00:16:45,100
trip these region execute advice by much

00:16:42,880 --> 00:16:46,360
more treads many trades you specify and

00:16:45,100 --> 00:16:48,700
you have available new system and then

00:16:46,360 --> 00:16:51,160
again my sequential by sequential part

00:16:48,700 --> 00:16:52,780
however as i'm going to show you ok so

00:16:51,160 --> 00:16:54,340
you maybe have ok so what's the

00:16:52,780 --> 00:16:56,020
motivation if i have already these in

00:16:54,340 --> 00:16:59,530
place what is the motivation for me to

00:16:56,020 --> 00:17:02,050
try the different line and then this

00:16:59,530 --> 00:17:04,360
results you motivate that so you can see

00:17:02,050 --> 00:17:06,610
in this you know left on salaries that

00:17:04,360 --> 00:17:09,280
the results for dynamic parallelism and

00:17:06,610 --> 00:17:12,010
I'm variety number of blocks the number

00:17:09,280 --> 00:17:14,800
of tracks and this is the latency of the

00:17:12,010 --> 00:17:17,440
execution and the same thing for the for

00:17:14,800 --> 00:17:19,570
the scheme that has the if master

00:17:17,440 --> 00:17:21,220
sometime right so this is the if master

00:17:19,570 --> 00:17:22,870
scheme and you can and you can see that

00:17:21,220 --> 00:17:24,730
they are ordered that is all

00:17:22,870 --> 00:17:28,030
a world of magnitude difference in

00:17:24,730 --> 00:17:30,820
execution time why because the dynamic

00:17:28,030 --> 00:17:32,760
parallelism needs to to interact with

00:17:30,820 --> 00:17:37,510
the driver in order to make this happen

00:17:32,760 --> 00:17:39,580
so it isn't with here we motivated okay

00:17:37,510 --> 00:17:41,740
we if possible we should have the

00:17:39,580 --> 00:17:44,110
threads already in place and instead of

00:17:41,740 --> 00:17:48,100
spawning new treads we should try only

00:17:44,110 --> 00:17:49,390
to manage the stretch right however

00:17:48,100 --> 00:17:51,250
there are still some problems with this

00:17:49,390 --> 00:17:54,040
we disappear with the approach of the if

00:17:51,250 --> 00:17:56,170
master and i'm not i'm not going into

00:17:54,040 --> 00:17:58,300
much detail in here but if you if you

00:17:56,170 --> 00:18:02,050
have like more complex more complex

00:17:58,300 --> 00:18:04,690
control flow if you have like the the

00:18:02,050 --> 00:18:07,090
results being generated in parallel

00:18:04,690 --> 00:18:10,030
regions to be used in sequential regions

00:18:07,090 --> 00:18:12,940
you may you may have you may need to go

00:18:10,030 --> 00:18:15,220
to each statement in my coat and adapted

00:18:12,940 --> 00:18:17,559
to an open MP implementation which is

00:18:15,220 --> 00:18:19,780
which should be highly disruptive of the

00:18:17,559 --> 00:18:21,880
compiler implementation another thing is

00:18:19,780 --> 00:18:23,530
that if i have a more complex control

00:18:21,880 --> 00:18:25,210
flow i may have different

00:18:23,530 --> 00:18:27,160
synchronization synchronizations in

00:18:25,210 --> 00:18:28,390
different paths and once some threads

00:18:27,160 --> 00:18:30,520
are going to read that synchronization

00:18:28,390 --> 00:18:32,410
some other threads are not going to do

00:18:30,520 --> 00:18:34,870
that and what and the result is that we

00:18:32,410 --> 00:18:36,640
will have a deadlock and we don't we

00:18:34,870 --> 00:18:40,300
don't want that so that the really the

00:18:36,640 --> 00:18:43,210
solution we came across was to use what

00:18:40,300 --> 00:18:45,340
what we call a control loop scheme and

00:18:43,210 --> 00:18:46,990
here so this this computes more or less

00:18:45,340 --> 00:18:51,160
the same thing and I'd shaded in gray

00:18:46,990 --> 00:18:54,309
these two areas that are managing labels

00:18:51,160 --> 00:18:57,670
for my sequential and parallel regions

00:18:54,309 --> 00:19:00,490
so here in these two places the treads

00:18:57,670 --> 00:19:02,350
each each tread is going to understand

00:19:00,490 --> 00:19:05,320
if it is a muscle off by checking insid

00:19:02,350 --> 00:19:07,660
and going to grab what its its current

00:19:05,320 --> 00:19:09,670
label that is going to determine what is

00:19:07,660 --> 00:19:11,980
the next workload that thread is

00:19:09,670 --> 00:19:14,920
supposed to execute so after this point

00:19:11,980 --> 00:19:17,740
we have a switch and based on the label

00:19:14,920 --> 00:19:19,330
the tread is going to to select ok I'm

00:19:17,740 --> 00:19:21,040
going to execute the sequential region

00:19:19,330 --> 00:19:23,620
or I'm going to do the parallel region

00:19:21,040 --> 00:19:25,390
and then this is inside a loop and all

00:19:23,620 --> 00:19:27,910
the threads are going to wait wait here

00:19:25,390 --> 00:19:31,420
until the current workload is done so i

00:19:27,910 --> 00:19:34,630
have only a single synchronization and

00:19:31,420 --> 00:19:36,669
on da and i'm able to make the coach n

00:19:34,630 --> 00:19:39,249
for each sequential and parallel

00:19:36,669 --> 00:19:41,859
region and touch because I can generate

00:19:39,249 --> 00:19:44,109
this this each one of these cases when I

00:19:41,859 --> 00:19:47,019
process my directives and I don't need

00:19:44,109 --> 00:19:49,299
to change anything inside each one of

00:19:47,019 --> 00:19:51,850
these cases so I only need to change my

00:19:49,299 --> 00:19:53,769
insertion point and emit my code in each

00:19:51,850 --> 00:19:57,070
one of these cases and the good thing is

00:19:53,769 --> 00:19:59,080
that we were able to do this which is

00:19:57,070 --> 00:20:01,840
much much more general and have more

00:19:59,080 --> 00:20:04,989
coverage of more cases and by the way we

00:20:01,840 --> 00:20:08,049
so far we were unable to find any any

00:20:04,989 --> 00:20:11,049
code that cannot cannot feed this model

00:20:08,049 --> 00:20:14,289
and we can do that only well with less

00:20:11,049 --> 00:20:16,359
than five percent overhead regarding the

00:20:14,289 --> 00:20:18,549
if master scheme I was showing you

00:20:16,359 --> 00:20:20,320
before so that's that's very good news i

00:20:18,549 --> 00:20:22,869
mean i don't have there's more details

00:20:20,320 --> 00:20:26,980
on this on a paper that we presented in

00:20:22,869 --> 00:20:28,330
a llvm 84 hpc workshop last money so if

00:20:26,980 --> 00:20:31,330
you are interested in annoying more

00:20:28,330 --> 00:20:33,519
details more into more intermediary

00:20:31,330 --> 00:20:35,859
steps in all these different approaches

00:20:33,519 --> 00:20:39,119
you should check the paper it has you

00:20:35,859 --> 00:20:42,820
know all the details for you all right

00:20:39,119 --> 00:20:46,139
okay so you may be wonder where can I

00:20:42,820 --> 00:20:50,320
get this right so in this picture here i

00:20:46,139 --> 00:20:52,029
am presenting all the openmp support has

00:20:50,320 --> 00:20:54,820
been developing into clunk so in this

00:20:52,029 --> 00:20:57,070
side we have the main repository and

00:20:54,820 --> 00:21:00,970
what happened is that in a lot in the

00:20:57,070 --> 00:21:03,340
last stable release of llvm a snapshot

00:21:00,970 --> 00:21:05,830
was taken to an external repository that

00:21:03,340 --> 00:21:07,629
is in is wet that is an inner in its

00:21:05,830 --> 00:21:10,090
website and all the openmp

00:21:07,629 --> 00:21:13,239
implementation has been added on top of

00:21:10,090 --> 00:21:15,039
this and at this moment that all the

00:21:13,239 --> 00:21:17,379
contributions that we currently confined

00:21:15,039 --> 00:21:20,499
here are being you know gradually merged

00:21:17,379 --> 00:21:23,230
into into the armor llvm main repository

00:21:20,499 --> 00:21:25,539
and we expect that somewhere during next

00:21:23,230 --> 00:21:27,220
year I mean the two things are going to

00:21:25,539 --> 00:21:30,399
be a single one and you have all this

00:21:27,220 --> 00:21:32,289
autumn all the you know the openmp

00:21:30,399 --> 00:21:34,600
implementation in a central space in

00:21:32,289 --> 00:21:36,129
this in essential place so about it or

00:21:34,600 --> 00:21:39,399
floating support we are currently

00:21:36,129 --> 00:21:42,029
developing in this in this book like the

00:21:39,399 --> 00:21:46,179
blocking here in the in the github

00:21:42,029 --> 00:21:47,679
repository all right so how to use this

00:21:46,179 --> 00:21:49,359
so I mean it's very straightforward if

00:21:47,679 --> 00:21:50,650
you want to use it usually only need to

00:21:49,359 --> 00:21:53,020
go to this website

00:21:50,650 --> 00:21:55,570
download the source code compile it as

00:21:53,020 --> 00:21:57,490
you would usually do and then you only

00:21:55,570 --> 00:21:59,920
need to call clunk say that you are

00:21:57,490 --> 00:22:03,730
interested in compiling the openmp

00:21:59,920 --> 00:22:05,560
directives and specify not only the the

00:22:03,730 --> 00:22:08,740
O's target we did in this case is the

00:22:05,560 --> 00:22:11,680
purpose is a powerpc and with a power 8

00:22:08,740 --> 00:22:14,050
cpu but also the targets we you care

00:22:11,680 --> 00:22:17,830
about only need to provide the triple of

00:22:14,050 --> 00:22:19,660
the target you want to support for for

00:22:17,830 --> 00:22:23,920
the target regions that are in your

00:22:19,660 --> 00:22:27,070
source files so only a final thought I

00:22:23,920 --> 00:22:29,530
mean some acknowledgement I mean we have

00:22:27,070 --> 00:22:32,020
been actively working on an offloading

00:22:29,530 --> 00:22:33,970
part parameter so many more components

00:22:32,020 --> 00:22:35,950
that are being contributed by other

00:22:33,970 --> 00:22:39,010
people I mean not only the code ropes

00:22:35,950 --> 00:22:41,140
themselves also the code reviews the way

00:22:39,010 --> 00:22:42,670
this specification should this the you

00:22:41,140 --> 00:22:44,470
know the offloading specification has be

00:22:42,670 --> 00:22:47,230
done and I mean some of these

00:22:44,470 --> 00:22:49,450
contributions include not only IBM Intel

00:22:47,230 --> 00:22:51,760
Texas Instrument the idea labs and many

00:22:49,450 --> 00:22:54,760
other distinguished members of the llvm

00:22:51,760 --> 00:22:56,530
community and I guess that's it and I'm

00:22:54,760 --> 00:23:01,500
happy to to address any presence even

00:22:56,530 --> 00:23:01,500
guys may have right yeah

00:23:06,880 --> 00:23:11,260
well I should probably say that I mean

00:23:09,160 --> 00:23:13,030
they are there are some some features i

00:23:11,260 --> 00:23:17,950
presented today that are not already in

00:23:13,030 --> 00:23:25,840
there all right so see so you guys can

00:23:17,950 --> 00:23:27,730
get it from here let's go ahead so we

00:23:25,840 --> 00:23:30,610
are we are actively working on this and

00:23:27,730 --> 00:23:34,180
we expect you know by the end of this

00:23:30,610 --> 00:23:35,470
here to have you know at least the

00:23:34,180 --> 00:23:38,200
support for some interesting examples

00:23:35,470 --> 00:23:40,330
already in clunk and ready and ready to

00:23:38,200 --> 00:23:43,270
use and we are very happy for you guys

00:23:40,330 --> 00:23:47,580
to try it and give us feedback and so we

00:23:43,270 --> 00:23:47,580
can know act how to do better all right

00:23:48,630 --> 00:23:56,920
to native a cuter implementation of the

00:23:52,450 --> 00:24:00,550
go thank you alright so the question is

00:23:56,920 --> 00:24:03,430
how does dumb these results that i was

00:24:00,550 --> 00:24:06,880
showing here compare I mean using openmp

00:24:03,430 --> 00:24:08,380
will compare with cuda well we currently

00:24:06,880 --> 00:24:10,570
don't have an answer for that because

00:24:08,380 --> 00:24:13,870
that I mean this will depend also on the

00:24:10,570 --> 00:24:16,930
data and all we really the data I mean I

00:24:13,870 --> 00:24:18,520
i would say that in the first

00:24:16,930 --> 00:24:20,410
saturations of the compiler you should

00:24:18,520 --> 00:24:23,050
expect we have some overhead for openmp

00:24:20,410 --> 00:24:25,030
and an end our goal is to look into the

00:24:23,050 --> 00:24:26,740
deficiencies and try to address one of

00:24:25,030 --> 00:24:29,860
them so the two things can you know

00:24:26,740 --> 00:24:32,140
start you know to to have like closer

00:24:29,860 --> 00:24:34,750
performing schools to each other right

00:24:32,140 --> 00:24:36,790
but I mean we we don't have you know all

00:24:34,750 --> 00:24:39,490
the flexibility in OpenMP that you

00:24:36,790 --> 00:24:41,350
currently have in CUDA but I mean

00:24:39,490 --> 00:24:45,450
writing an open every program is is much

00:24:41,350 --> 00:24:45,450
easier than writing equity program right

00:24:46,900 --> 00:24:52,150
so the question is what is open ACC

00:24:49,600 --> 00:24:56,710
supporting clung and and the answer is

00:24:52,150 --> 00:24:58,630
none so there I knew that there is no

00:24:56,710 --> 00:25:01,750
and that and as far as I know there is

00:24:58,630 --> 00:25:06,010
no plans to do that well actually I I

00:25:01,750 --> 00:25:08,650
should probably say this i mean ii as i

00:25:06,010 --> 00:25:11,890
think that we we all should work really

00:25:08,650 --> 00:25:15,070
hard to make you know open it MP and

00:25:11,890 --> 00:25:17,020
open a cc to merge into a single path

00:25:15,070 --> 00:25:19,390
you know because that before there was

00:25:17,020 --> 00:25:22,330
only OpenMP they were then open acc

00:25:19,390 --> 00:25:24,850
diverged from openmp and now you have

00:25:22,330 --> 00:25:27,100
some interesting features and open ncc

00:25:24,850 --> 00:25:29,140
to support targets and and what we are

00:25:27,100 --> 00:25:31,360
going to try to do is to learn you know

00:25:29,140 --> 00:25:33,700
from open acc and try to you know

00:25:31,360 --> 00:25:36,310
integrate all the good things in openmp

00:25:33,700 --> 00:25:38,470
and in the future we we should expect to

00:25:36,310 --> 00:25:40,180
for the sake of all the programmers who

00:25:38,470 --> 00:25:42,370
have you know only a single directive

00:25:40,180 --> 00:25:46,870
language to deal with everything alright

00:25:42,370 --> 00:25:51,610
so yeah the question targeting or Gouda

00:25:46,870 --> 00:25:56,560
how come that are not succeeding silicon

00:25:51,610 --> 00:25:59,670
why Hargitay or khuda i mean here's this

00:25:56,560 --> 00:25:59,670
year but why

00:26:00,020 --> 00:26:06,440
well I mean my my understanding is that

00:26:02,840 --> 00:26:08,300
I mean opencl oh ok so so let me repeat

00:26:06,440 --> 00:26:12,170
your questions so everyone can can can

00:26:08,300 --> 00:26:15,980
listen to it so the question is I mean

00:26:12,170 --> 00:26:19,910
what what why bother do this if we have

00:26:15,980 --> 00:26:21,980
opencl right so that's more so your

00:26:19,910 --> 00:26:23,390
question is why why should we try to do

00:26:21,980 --> 00:26:29,870
this and open and be through our if we

00:26:23,390 --> 00:26:32,990
already have opencl oh ok so I'm sorry

00:26:29,870 --> 00:26:35,840
I'm again ok so so the question is how

00:26:32,990 --> 00:26:39,250
do we target could instead also opens

00:26:35,840 --> 00:26:41,600
yeah well you know open the opencl

00:26:39,250 --> 00:26:44,930
programming model is very very similar

00:26:41,600 --> 00:26:47,420
to cuda and we currently care about

00:26:44,930 --> 00:26:51,830
nvidia gpus and coo days you know the

00:26:47,420 --> 00:26:53,750
domain and probably the the programming

00:26:51,830 --> 00:26:56,180
model that will allow you to get you

00:26:53,750 --> 00:26:58,910
know more advantage of the GPUs but and

00:26:56,180 --> 00:27:01,490
I suspect that if you do the same

00:26:58,910 --> 00:27:03,770
evaluation or opencl the kind of results

00:27:01,490 --> 00:27:07,300
that you would see will be very similar

00:27:03,770 --> 00:27:07,300

YouTube URL: https://www.youtube.com/watch?v=Hx_YaRIiJnw


