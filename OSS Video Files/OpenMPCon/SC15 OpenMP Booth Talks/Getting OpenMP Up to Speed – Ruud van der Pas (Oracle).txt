Title: Getting OpenMP Up to Speed â€“ Ruud van der Pas (Oracle)
Publication date: 2015-11-29
Playlist: SC15 OpenMP Booth Talks
Description: 
	SC15 OpenMP Booth Talks
Slides at http://goo.gl/hdhxUu
Captions: 
	00:00:00,000 --> 00:00:06,660
so the informal title of this talk would

00:00:03,870 --> 00:00:09,599
be OpenMP does not scale and I started

00:00:06,660 --> 00:00:11,309
being more vocal about it because I see

00:00:09,599 --> 00:00:15,389
and hear too many people saying oh

00:00:11,309 --> 00:00:17,279
OpenMP doesn't scale and actually in all

00:00:15,389 --> 00:00:19,949
the cases that turns out not to be true

00:00:17,279 --> 00:00:22,289
there's something else going on so

00:00:19,949 --> 00:00:24,109
that's what I want to talk about that's

00:00:22,289 --> 00:00:27,109
a really common and very annoying

00:00:24,109 --> 00:00:29,670
misconception so think about it a

00:00:27,109 --> 00:00:32,790
programming model cannot not scale

00:00:29,670 --> 00:00:35,850
that's just a set of recipes how to get

00:00:32,790 --> 00:00:40,800
to your goal things that cannot scale

00:00:35,850 --> 00:00:43,530
are like the implementation if I would

00:00:40,800 --> 00:00:46,530
take three days to create my threads it

00:00:43,530 --> 00:00:49,170
wouldn't scale so if the implementation

00:00:46,530 --> 00:00:50,910
can have flaws in efficiencies another

00:00:49,170 --> 00:00:52,620
thing is that I would never say people

00:00:50,910 --> 00:00:55,140
by the wrong hardware they buy the

00:00:52,620 --> 00:00:57,660
hardware that doesn't match the needs of

00:00:55,140 --> 00:01:01,020
the application like a lot of bandwidth

00:00:57,660 --> 00:01:03,480
or whatever you can imagine those are a

00:01:01,020 --> 00:01:06,960
reasons when actually it could also be

00:01:03,480 --> 00:01:09,510
you you wrote openmp and one of the

00:01:06,960 --> 00:01:11,549
beauties of opening p is easy to use but

00:01:09,510 --> 00:01:14,130
that doesn't automatically mean you're

00:01:11,549 --> 00:01:17,640
off the hook with performance and I'll

00:01:14,130 --> 00:01:21,330
talk a little bit about that so I'm

00:01:17,640 --> 00:01:22,950
going to give a top 10 of dumb things to

00:01:21,330 --> 00:01:25,140
do and of course nobody here is doing

00:01:22,950 --> 00:01:27,600
that but I just want to want to show

00:01:25,140 --> 00:01:30,780
common things that I see that cause

00:01:27,600 --> 00:01:33,420
scalability issues first of all the big

00:01:30,780 --> 00:01:35,549
mistake is you should always use OpenAPI

00:01:33,420 --> 00:01:39,840
so a dumb thing to do is don't use

00:01:35,549 --> 00:01:42,840
opening well 11 more serious thing is

00:01:39,840 --> 00:01:45,000
that you start with a code that doesn't

00:01:42,840 --> 00:01:47,340
run well on a single core what do you

00:01:45,000 --> 00:01:49,470
think will happen on ten one hundred or

00:01:47,340 --> 00:01:52,649
thousands do you think magically will go

00:01:49,470 --> 00:01:54,540
better my claim is will go worse and a

00:01:52,649 --> 00:01:57,479
simple simple reason for that is very

00:01:54,540 --> 00:02:00,329
often badly performing code means you go

00:01:57,479 --> 00:02:02,040
to the memory system too often you have

00:02:00,329 --> 00:02:03,840
a lot of cache misses you go to memory

00:02:02,040 --> 00:02:06,450
system now you do that with ten threads

00:02:03,840 --> 00:02:09,929
hundred thousand you interconnect will

00:02:06,450 --> 00:02:11,970
die so that's rule number one find some

00:02:09,929 --> 00:02:13,569
ways to improve the performance play

00:02:11,970 --> 00:02:15,340
with compile options

00:02:13,569 --> 00:02:17,439
maybe restructuring your code it's

00:02:15,340 --> 00:02:19,359
really up to you but make sure that your

00:02:17,439 --> 00:02:23,170
single cell performance is reasonably

00:02:19,359 --> 00:02:25,959
okay you will number one then a dumb

00:02:23,170 --> 00:02:28,450
thing to do is don't use a profiling

00:02:25,959 --> 00:02:29,799
tool I make this challenging I mean

00:02:28,450 --> 00:02:31,810
that's you got to have a lot of coffee

00:02:29,799 --> 00:02:33,790
to get it a dumb thing to do is don't

00:02:31,810 --> 00:02:35,739
use it to people come to me and they

00:02:33,790 --> 00:02:38,019
said oh I know where the time is spent

00:02:35,739 --> 00:02:40,540
well that may be important for your

00:02:38,019 --> 00:02:41,739
physics or chemistry but actually for

00:02:40,540 --> 00:02:43,870
the computer it could be something

00:02:41,739 --> 00:02:46,359
totally different you wouldn't be the

00:02:43,870 --> 00:02:49,030
first one where is it oh gee I never

00:02:46,359 --> 00:02:51,129
thought that was so important because it

00:02:49,030 --> 00:02:52,810
didn't play a role in your modeling so

00:02:51,129 --> 00:02:54,790
always use it to pick the one you like

00:02:52,810 --> 00:02:56,799
the tool will tell you where you're

00:02:54,790 --> 00:03:01,000
spending most of your time and that's

00:02:56,799 --> 00:03:03,790
the part to focus on another dumb thing

00:03:01,000 --> 00:03:07,209
to do is use excessive like an excessive

00:03:03,790 --> 00:03:09,609
number of parallel regions each parallel

00:03:07,209 --> 00:03:11,829
region carries overhead so once you

00:03:09,609 --> 00:03:14,889
start having them all over your code it

00:03:11,829 --> 00:03:16,750
adds up and again I'm talking about

00:03:14,889 --> 00:03:18,310
scalable performance I'm talking about

00:03:16,750 --> 00:03:20,590
what were you going to go to a hundred

00:03:18,310 --> 00:03:23,620
thread a thousand fit that's what I'm

00:03:20,590 --> 00:03:26,590
talking about so don't a better way I

00:03:23,620 --> 00:03:29,919
should be kind of constructive a better

00:03:26,590 --> 00:03:33,009
ways to have large parallel regions and

00:03:29,919 --> 00:03:34,930
put the work in there and if there's no

00:03:33,009 --> 00:03:37,239
parallel work you can for example put it

00:03:34,930 --> 00:03:42,189
in a single region if only one thread

00:03:37,239 --> 00:03:46,169
has to do it so another common common

00:03:42,189 --> 00:03:48,879
mistake is excessive use of shared data

00:03:46,169 --> 00:03:51,430
shared data is very convenient but when

00:03:48,879 --> 00:03:54,009
you modify share data cache coherence

00:03:51,430 --> 00:03:55,840
will build pitch in and that can lead to

00:03:54,009 --> 00:03:58,479
all sorts of performance side effects

00:03:55,840 --> 00:04:00,819
that I won't go into right here but be

00:03:58,479 --> 00:04:03,310
careful to share data only share data

00:04:00,819 --> 00:04:05,409
when you have to try to do computations

00:04:03,310 --> 00:04:08,049
for example on local data as long as you

00:04:05,409 --> 00:04:09,609
can and then when you're finished you

00:04:08,049 --> 00:04:12,400
may write it back into some shared data

00:04:09,609 --> 00:04:14,259
structure and that's fine and then other

00:04:12,400 --> 00:04:16,000
threads can pick it up think about it

00:04:14,259 --> 00:04:19,989
from a from a locality point of view

00:04:16,000 --> 00:04:22,659
that was five reasons but I promise you

00:04:19,989 --> 00:04:24,699
10 another dumb thing to do is to forget

00:04:22,659 --> 00:04:26,380
about the no wait cloth just a quick

00:04:24,699 --> 00:04:27,990
show of hand who's familiar with the no

00:04:26,380 --> 00:04:32,290
wait clause in oak

00:04:27,990 --> 00:04:34,660
okay okay not enough hands go up here's

00:04:32,290 --> 00:04:36,460
the thing when thread synchronized they

00:04:34,660 --> 00:04:38,889
typically do that in a construct called

00:04:36,460 --> 00:04:40,870
the barrier all threads wait in a

00:04:38,889 --> 00:04:44,229
barrier until the last one has arrived

00:04:40,870 --> 00:04:46,690
and you continue now sometimes that's

00:04:44,229 --> 00:04:49,210
overkill because the part below it is

00:04:46,690 --> 00:04:50,770
independent of what you just did you may

00:04:49,210 --> 00:04:52,960
initialize some of the things that

00:04:50,770 --> 00:04:55,150
something else is anything then OpenAPI

00:04:52,960 --> 00:04:57,580
on certain constructs has a very

00:04:55,150 --> 00:04:59,350
convenient clause called no way and it

00:04:57,580 --> 00:05:00,490
does exactly what it promises the

00:04:59,350 --> 00:05:02,979
threads won't wait they'll immediately

00:05:00,490 --> 00:05:05,260
continue with next work next piece of

00:05:02,979 --> 00:05:07,919
work you'll save a barrier and barriers

00:05:05,260 --> 00:05:10,930
are inhibitor for scalable performance

00:05:07,919 --> 00:05:13,090
small things you wouldn't put that in

00:05:10,930 --> 00:05:15,100
your first open a peacoat but once you

00:05:13,090 --> 00:05:16,449
start doing the fine-tuning and for

00:05:15,100 --> 00:05:17,889
example the tool may tell you you're

00:05:16,449 --> 00:05:22,150
spending a lot of time in the barrier

00:05:17,889 --> 00:05:24,220
well maybe I can use the no way I see a

00:05:22,150 --> 00:05:27,789
lot of locking where your reasoning is

00:05:24,220 --> 00:05:30,430
that really worth locking I mean locks

00:05:27,789 --> 00:05:33,250
are useful but again I currently looking

00:05:30,430 --> 00:05:36,310
at a code where the lock code block is

00:05:33,250 --> 00:05:38,710
like that and it's totally killed in the

00:05:36,310 --> 00:05:41,680
lock in the lock performance so when you

00:05:38,710 --> 00:05:44,800
start doing locking be careful another

00:05:41,680 --> 00:05:47,050
common one is by default openmp will

00:05:44,800 --> 00:05:49,300
schedule the threads in in a static way

00:05:47,050 --> 00:05:51,729
so you all get the same amount of

00:05:49,300 --> 00:05:53,590
iterations for example well what if

00:05:51,729 --> 00:05:56,160
there's a load imbalance what if not

00:05:53,590 --> 00:05:59,050
every iteration takes the same time

00:05:56,160 --> 00:06:01,300
think about the dynamic guided

00:05:59,050 --> 00:06:03,789
scheduling things often overlooked again

00:06:01,300 --> 00:06:08,590
a profiling tool to tell you that that's

00:06:03,789 --> 00:06:11,590
a problem all right another thing which

00:06:08,590 --> 00:06:14,500
sadly you cannot ignore these days is CC

00:06:11,590 --> 00:06:17,530
Numa I'd like to say we didn't ask for

00:06:14,500 --> 00:06:20,199
CC Numa we got it and there's a good

00:06:17,530 --> 00:06:22,599
reason because with the CC Numa system

00:06:20,199 --> 00:06:25,210
as you add a socket you add memory

00:06:22,599 --> 00:06:27,520
bandwidth because each socket has a

00:06:25,210 --> 00:06:29,710
portion of the memory so you get more

00:06:27,520 --> 00:06:32,139
scalable bandwidth but it's on you to

00:06:29,710 --> 00:06:34,419
take advantage of it if you ignore it

00:06:32,139 --> 00:06:37,719
and even simple algorithms we can show

00:06:34,419 --> 00:06:39,969
won't scale again it's not pretty but

00:06:37,719 --> 00:06:41,050
it's the way it is so don't ignore it

00:06:39,969 --> 00:06:45,729
and OpenMP has

00:06:41,050 --> 00:06:47,229
nice support to handle CC numa the worst

00:06:45,729 --> 00:06:49,240
thing to do is forget all of the above

00:06:47,229 --> 00:06:52,090
that I just explained you so go through

00:06:49,240 --> 00:06:53,979
the checklist make sure that again don't

00:06:52,090 --> 00:06:56,849
forget start with a pro pointing to I

00:06:53,979 --> 00:06:59,710
now want to show you a case study and

00:06:56,849 --> 00:07:03,190
the case study is from graph analysis

00:06:59,710 --> 00:07:04,840
and I'll have to skip all the details

00:07:03,190 --> 00:07:06,879
i'll be more than happy to talk offline

00:07:04,840 --> 00:07:10,150
about it longer than you probably like

00:07:06,879 --> 00:07:12,699
but i'll show you the results like what

00:07:10,150 --> 00:07:15,009
happens along the way so what's it grab

00:07:12,699 --> 00:07:17,680
very simple set a graph is a

00:07:15,009 --> 00:07:20,889
mathematical abstraction you have the

00:07:17,680 --> 00:07:24,430
nodes or vertices and the links between

00:07:20,889 --> 00:07:26,680
those called edges and answer is defined

00:07:24,430 --> 00:07:28,629
through the relationship and the easiest

00:07:26,680 --> 00:07:31,469
one is people you got people in your

00:07:28,629 --> 00:07:34,780
network and do you know each other does

00:07:31,469 --> 00:07:36,879
sino d if so i draw a line an imaginary

00:07:34,780 --> 00:07:39,490
line in my data structure and that

00:07:36,879 --> 00:07:43,229
builds a graph and one of the common

00:07:39,490 --> 00:07:45,460
things that people like to do is search

00:07:43,229 --> 00:07:47,169
you want to search through that graph

00:07:45,460 --> 00:07:49,240
you want to find somebody and you know

00:07:47,169 --> 00:07:51,190
the social websites they tell you like

00:07:49,240 --> 00:07:53,380
who's connected to you and who you may

00:07:51,190 --> 00:07:55,810
be interested in that's all done by

00:07:53,380 --> 00:07:57,729
searching through the graph so searching

00:07:55,810 --> 00:07:59,289
is a very common operation on graphs

00:07:57,729 --> 00:08:01,300
that there are other things you do on

00:07:59,289 --> 00:08:04,500
graphs but the search is one of the

00:08:01,300 --> 00:08:07,150
things you pretty much always do and

00:08:04,500 --> 00:08:09,940
from a benchmarking point of view that

00:08:07,150 --> 00:08:11,409
the score is not mega flops because you

00:08:09,940 --> 00:08:14,229
don't you don't do any floating point

00:08:11,409 --> 00:08:16,300
operations it's how many edges you can

00:08:14,229 --> 00:08:19,000
you traverse per second how quickly can

00:08:16,300 --> 00:08:20,740
you go through your data okay that's the

00:08:19,000 --> 00:08:24,120
number i'll be using and the vertical

00:08:20,740 --> 00:08:28,719
axis on my chart it's called teps

00:08:24,120 --> 00:08:30,279
traversed edges per second so I'm

00:08:28,719 --> 00:08:32,589
looking at this benchmark it's written

00:08:30,279 --> 00:08:35,229
in seeds be paralyzed with OpenMP or

00:08:32,589 --> 00:08:37,300
obvious and you can play with the size

00:08:35,229 --> 00:08:39,570
of the graph the graph is generated for

00:08:37,300 --> 00:08:41,860
you it's a synthetic benchmark and

00:08:39,570 --> 00:08:43,630
that's control through a value called

00:08:41,860 --> 00:08:46,630
scale and that's what you'll see on my

00:08:43,630 --> 00:08:49,660
chart the number of vertices the number

00:08:46,630 --> 00:08:51,610
of nodes is 2 to the power of scale so a

00:08:49,660 --> 00:08:54,430
scale factor of 10 will give you

00:08:51,610 --> 00:08:57,210
thousand twenty four nodes and each

00:08:54,430 --> 00:09:00,160
on average has 16 connections to others

00:08:57,210 --> 00:09:03,760
and you crank up the size of the graph

00:09:00,160 --> 00:09:06,190
that way so i took that benchmark it

00:09:03,760 --> 00:09:09,430
pulled it from from the web anybody can

00:09:06,190 --> 00:09:10,930
do that I ran it and I didn't try

00:09:09,430 --> 00:09:13,000
another hardware but I think this is

00:09:10,930 --> 00:09:15,670
fairly generic based on what I found

00:09:13,000 --> 00:09:18,029
later I rented on a two second rocket

00:09:15,670 --> 00:09:21,399
system and the performance is terrible

00:09:18,029 --> 00:09:24,040
it's game over acting after 16 threads

00:09:21,399 --> 00:09:29,610
and this was a fairly small graph 35

00:09:24,040 --> 00:09:33,279
gigabyte scale 26 all right so many and

00:09:29,610 --> 00:09:35,260
I I have that experience very fresh from

00:09:33,279 --> 00:09:37,390
last week said oh let's use a bigger

00:09:35,260 --> 00:09:40,000
machine this machine is not big enough

00:09:37,390 --> 00:09:43,810
for what we're doing so you take a

00:09:40,000 --> 00:09:47,950
bigger machine and it's actually a

00:09:43,810 --> 00:09:50,500
little bit worse wow that's a bummer ok

00:09:47,950 --> 00:09:52,720
the next thing that people do and of

00:09:50,500 --> 00:09:55,180
course I'll explain why that is but the

00:09:52,720 --> 00:09:58,390
next oh that graph is too small let's

00:09:55,180 --> 00:10:00,790
take a bigger graph see if we can have

00:09:58,390 --> 00:10:02,950
an easy way out well the curves are

00:10:00,790 --> 00:10:05,950
nearly identical it doesn't help there's

00:10:02,950 --> 00:10:08,740
no easy way up going to a bigger machine

00:10:05,950 --> 00:10:11,440
doesn't help increasing the problem size

00:10:08,740 --> 00:10:14,080
none of it that helps so what are we

00:10:11,440 --> 00:10:15,910
going to do we got to go under the hood

00:10:14,080 --> 00:10:18,130
we got to find out what's going on and

00:10:15,910 --> 00:10:21,180
again we got to start using a profiling

00:10:18,130 --> 00:10:24,670
tool so I use our profiling tool and

00:10:21,180 --> 00:10:26,800
what I did I think the value the small

00:10:24,670 --> 00:10:28,690
35 gigabyte because that was the one

00:10:26,800 --> 00:10:30,910
that didn't scale so that's convenient

00:10:28,690 --> 00:10:32,529
to focus on a small case so I picked the

00:10:30,910 --> 00:10:35,950
smallest case that doesn't perform well

00:10:32,529 --> 00:10:38,370
and what I do I look at the overhead as

00:10:35,950 --> 00:10:41,620
a function of the number of threads and

00:10:38,370 --> 00:10:44,260
all these bright colors like orange and

00:10:41,620 --> 00:10:46,570
red and and think they're all bad news

00:10:44,260 --> 00:10:49,750
they're all some sort of open it be

00:10:46,570 --> 00:10:52,060
overhead like atomic operations atomic

00:10:49,750 --> 00:10:53,860
way waiting in a critical section all

00:10:52,060 --> 00:10:57,520
these things are the cost of

00:10:53,860 --> 00:11:01,690
parallelization and to me that telltale

00:10:57,520 --> 00:11:04,150
sign is this one it's an achievement in

00:11:01,690 --> 00:11:07,769
itself to take a sequential program and

00:11:04,150 --> 00:11:10,679
manage to get seven percent overhead

00:11:07,769 --> 00:11:13,019
you're not going parallel what what does

00:11:10,679 --> 00:11:15,269
that say that says that this code has

00:11:13,019 --> 00:11:17,519
been fully paralyzed and in this case

00:11:15,269 --> 00:11:19,379
we'd OpenMP there's too much overhead

00:11:17,519 --> 00:11:21,989
you go through even on a single player

00:11:19,379 --> 00:11:24,209
so even without looking at the code you

00:11:21,989 --> 00:11:26,519
know that's bad news and as you see very

00:11:24,209 --> 00:11:31,739
quickly game over this is why this code

00:11:26,519 --> 00:11:35,009
doesn't scale another thing I looked at

00:11:31,739 --> 00:11:37,019
was the bandwidth maybe there was a

00:11:35,009 --> 00:11:40,529
bandwidth issue and indeed on this

00:11:37,019 --> 00:11:42,509
system to socket system the theoretical

00:11:40,529 --> 00:11:44,759
peak performance in on the memory

00:11:42,509 --> 00:11:48,509
bandwidth is about 130 gigabytes a

00:11:44,759 --> 00:11:51,119
second I only get 50 and when you look

00:11:48,509 --> 00:11:54,869
at this in more detail you can see it's

00:11:51,119 --> 00:11:57,839
it's pretty much all we'd bandwidth

00:11:54,869 --> 00:11:59,579
ninety percent reads and what you can

00:11:57,839 --> 00:12:02,399
see here i distinguish between two

00:11:59,579 --> 00:12:05,579
sockets blue is Sokka 0 owns the socket

00:12:02,399 --> 00:12:08,100
one well orange is hardly visible it all

00:12:05,579 --> 00:12:10,799
comes from woonsocket what does that say

00:12:08,100 --> 00:12:14,879
this is not see Seenu more friendly I am

00:12:10,799 --> 00:12:18,410
not using the bandwidth you myself so to

00:12:14,879 --> 00:12:21,179
summarize what I saw extensive

00:12:18,410 --> 00:12:24,119
communication it explodes as the

00:12:21,179 --> 00:12:26,160
increased feds as a result I can't use

00:12:24,119 --> 00:12:28,019
all the threads and they can't use all

00:12:26,160 --> 00:12:30,179
the resources in my system so i got

00:12:28,019 --> 00:12:32,399
really get bitten bandwidth is not

00:12:30,179 --> 00:12:35,220
balanced so what do you do go into the

00:12:32,399 --> 00:12:37,679
source code use your profiling tool and

00:12:35,220 --> 00:12:40,619
fixing it one by one and that's what I

00:12:37,679 --> 00:12:43,129
did one by one I found some room for

00:12:40,619 --> 00:12:45,209
using a lower level atomic functions

00:12:43,129 --> 00:12:47,339
that's questionable where do you want to

00:12:45,209 --> 00:12:50,929
do that but you know I want to grab

00:12:47,339 --> 00:12:53,399
everything I could so that's what I did

00:12:50,929 --> 00:12:55,589
so now what I call the secret sauce and

00:12:53,399 --> 00:12:57,869
bowling your stance will bad open and P

00:12:55,589 --> 00:13:00,389
and this means better open in peace I

00:12:57,869 --> 00:13:02,970
don't pretend that I fixed everything or

00:13:00,389 --> 00:13:05,569
found everything but as the results are

00:13:02,970 --> 00:13:08,699
it'll it'll definitely look better I

00:13:05,569 --> 00:13:11,100
want to show you again profiling tools I

00:13:08,699 --> 00:13:13,799
have I used our own profiling tool and

00:13:11,100 --> 00:13:17,730
what this shows is a timeline as the

00:13:13,799 --> 00:13:20,630
program is running I call a code the bad

00:13:17,730 --> 00:13:22,100
things happening in my program and

00:13:20,630 --> 00:13:24,440
there's a lot of detail here but all

00:13:22,100 --> 00:13:28,550
these Brides all these colors are kind

00:13:24,440 --> 00:13:31,730
of bad news and this is before and this

00:13:28,550 --> 00:13:34,520
is after and one thing you may notice is

00:13:31,730 --> 00:13:37,340
this part shrunk to this one and the

00:13:34,520 --> 00:13:40,600
blue is idle time this code managed to

00:13:37,340 --> 00:13:42,890
get slower as you add threads that part

00:13:40,600 --> 00:13:44,210
that's no kind of Ebola parallel

00:13:42,890 --> 00:13:47,030
computing I think we have a different

00:13:44,210 --> 00:13:49,960
goal here so one thing I quickly did jam

00:13:47,030 --> 00:13:52,240
it in a single region as a hack and

00:13:49,960 --> 00:13:54,050
actually I got better throughput of

00:13:52,240 --> 00:13:55,640
course you have to look at a more

00:13:54,050 --> 00:13:57,890
fundamental fix when you have more time

00:13:55,640 --> 00:14:01,970
but that was an easy kind of thing to do

00:13:57,890 --> 00:14:04,010
and then you see the pink colors and

00:14:01,970 --> 00:14:06,050
read them much less here they're still

00:14:04,010 --> 00:14:08,900
there I still need to do my barriers and

00:14:06,050 --> 00:14:10,940
whatever but it's much much less and and

00:14:08,900 --> 00:14:13,760
it translates too much improved

00:14:10,940 --> 00:14:16,670
performance this little worm was my

00:14:13,760 --> 00:14:20,330
original performance and I get way way

00:14:16,670 --> 00:14:23,120
better performance I wasn't done Don

00:14:20,330 --> 00:14:25,910
this code also had this so what I did I

00:14:23,120 --> 00:14:28,100
fixed the openmp now I start looking at

00:14:25,910 --> 00:14:30,230
the memory side of things so that's the

00:14:28,100 --> 00:14:34,520
better opening p2 memory optimization

00:14:30,230 --> 00:14:37,340
that that's what it's them so first of

00:14:34,520 --> 00:14:42,080
all this code did not know about first

00:14:37,340 --> 00:14:44,440
touch placement of what that means is on

00:14:42,080 --> 00:14:49,070
a system by default the threat that

00:14:44,440 --> 00:14:51,260
initializes the data owns it so if only

00:14:49,070 --> 00:14:54,440
one thread initializes your data all the

00:14:51,260 --> 00:14:56,540
data will end up in woonsocket so a

00:14:54,440 --> 00:14:58,430
simple thing to do is paralyzed the data

00:14:56,540 --> 00:15:01,280
initialization and spread out the data

00:14:58,430 --> 00:15:04,040
it's a very simple thing to do in most

00:15:01,280 --> 00:15:06,950
cases and it gets you going it may not

00:15:04,040 --> 00:15:09,950
be optimal but it certainly helps so I

00:15:06,950 --> 00:15:11,840
looked at that another thing is which is

00:15:09,950 --> 00:15:14,330
not visible in this chart this code is

00:15:11,840 --> 00:15:17,500
the data access is random it's jumping

00:15:14,330 --> 00:15:20,330
all over the memory when you see that

00:15:17,500 --> 00:15:21,620
use large pages and I hope you're

00:15:20,330 --> 00:15:23,570
familiar with that you can increase the

00:15:21,620 --> 00:15:25,610
size of a virtual memory page on the

00:15:23,570 --> 00:15:28,280
system you do that in software you don't

00:15:25,610 --> 00:15:30,350
have to reboot or something that does

00:15:28,280 --> 00:15:31,400
ways to increase the size of your page

00:15:30,350 --> 00:15:34,030
and again if you want to know the

00:15:31,400 --> 00:15:35,560
details talked with me offline so

00:15:34,030 --> 00:15:38,590
that's about the random memory access

00:15:35,560 --> 00:15:40,930
and it definitely needs large pages I

00:15:38,590 --> 00:15:43,030
also looked at the way memory was

00:15:40,930 --> 00:15:45,280
allocated and it was some really doing

00:15:43,030 --> 00:15:48,880
some dumb things like reallocating the

00:15:45,280 --> 00:15:51,670
same thing for every search and it

00:15:48,880 --> 00:15:54,460
doesn't change that's not smart I mean

00:15:51,670 --> 00:15:56,170
that's a simple logic that will will

00:15:54,460 --> 00:16:00,130
help so I did a lot of those things as

00:15:56,170 --> 00:16:02,020
well and now look at the bandwidth I got

00:16:00,130 --> 00:16:03,910
near peak performance is actually peak

00:16:02,020 --> 00:16:09,030
performance that's the kind of charge

00:16:03,910 --> 00:16:11,950
you like to see and when you look at the

00:16:09,030 --> 00:16:15,220
accumulated benefit of it I looked at

00:16:11,950 --> 00:16:18,070
scale 29 that's about 280 gigabyte of

00:16:15,220 --> 00:16:21,820
memory need at scale 30 half a terabyte

00:16:18,070 --> 00:16:24,670
a scale 31 is a little over one terabyte

00:16:21,820 --> 00:16:26,790
of memory need it and this ran here is

00:16:24,670 --> 00:16:30,310
the original performance in terms of

00:16:26,790 --> 00:16:32,470
traversed edges per second my openmp

00:16:30,310 --> 00:16:34,780
magic helped me to get substantially by

00:16:32,470 --> 00:16:38,700
the performance and then I add memory to

00:16:34,780 --> 00:16:41,830
it and I get way way better performance

00:16:38,700 --> 00:16:44,200
I'd like to translate things like speed

00:16:41,830 --> 00:16:46,720
up to what I call real money what does

00:16:44,200 --> 00:16:49,210
that mean in terms of search time what

00:16:46,720 --> 00:16:52,900
it means in this case a search time of

00:16:49,210 --> 00:16:54,580
12 hours reduce it to 10 minutes that

00:16:52,900 --> 00:16:58,450
changes your way you can do your science

00:16:54,580 --> 00:17:00,820
your work you no longer have to wait a

00:16:58,450 --> 00:17:02,500
day your go and get a coffee and you got

00:17:00,820 --> 00:17:04,960
your results that's what these speedups

00:17:02,500 --> 00:17:06,940
mean and that's I think we all often

00:17:04,960 --> 00:17:08,770
forget that speed up speed up is a

00:17:06,940 --> 00:17:13,150
number but that's what it translates to

00:17:08,770 --> 00:17:15,370
all right and I couldn't resist i took a

00:17:13,150 --> 00:17:18,070
larger grab more than 2 terabytes and

00:17:15,370 --> 00:17:19,750
indeed i keep on getting the benefits i

00:17:18,070 --> 00:17:21,820
keep on getting this is the reason I

00:17:19,750 --> 00:17:25,060
included this slide is when people come

00:17:21,820 --> 00:17:28,720
to me sir hmm it doesn't scale look at

00:17:25,060 --> 00:17:31,180
the number close to 900 threads in one

00:17:28,720 --> 00:17:33,130
single system don't tell me memory

00:17:31,180 --> 00:17:36,010
doesn't scale compared about that little

00:17:33,130 --> 00:17:39,100
red worm I had in the beginning yeah all

00:17:36,010 --> 00:17:41,590
right I wanted to see the benefit of

00:17:39,100 --> 00:17:44,680
both optimizations so what I'm showing

00:17:41,590 --> 00:17:47,050
you here the red line is what happened

00:17:44,680 --> 00:17:47,920
when I fix the open and pete as a

00:17:47,050 --> 00:17:50,800
function of the

00:17:47,920 --> 00:17:54,250
websites and you see for the small grin

00:17:50,800 --> 00:17:57,490
forgot about 25 it reduces to about 20

00:17:54,250 --> 00:18:00,910
but 20 x is a huge improvement I mean

00:17:57,490 --> 00:18:03,550
that and then adding the memory helped

00:18:00,910 --> 00:18:07,210
not as spectacular not 25 facts about

00:18:03,550 --> 00:18:09,760
about 3x but look here now large graphs

00:18:07,210 --> 00:18:15,220
actually do better that's exactly what

00:18:09,760 --> 00:18:17,530
you want then I got another idea that

00:18:15,220 --> 00:18:19,030
there's a year in between I mean it's

00:18:17,530 --> 00:18:20,470
not that I say that my desk and I

00:18:19,030 --> 00:18:22,270
cranked out all these things one after

00:18:20,470 --> 00:18:24,040
the other I mean there's some time in

00:18:22,270 --> 00:18:26,440
between I dropped it for a while I

00:18:24,040 --> 00:18:29,040
revisited this code don't wait a minute

00:18:26,440 --> 00:18:32,860
I can do some better open and Pete and

00:18:29,040 --> 00:18:35,410
that's the blue line and by the way this

00:18:32,860 --> 00:18:37,900
is on this 8-way box that did so poorly

00:18:35,410 --> 00:18:39,520
in the beginning remember it wouldn't be

00:18:37,900 --> 00:18:41,620
better than that then to fire the two

00:18:39,520 --> 00:18:43,270
socket box I didn't include the two

00:18:41,620 --> 00:18:47,290
socket numbers here but it definitely

00:18:43,270 --> 00:18:49,180
scales now and you can see by playing a

00:18:47,290 --> 00:18:51,670
little more with the opening p I get

00:18:49,180 --> 00:18:53,740
another significant boost so overall

00:18:51,670 --> 00:18:57,540
compared to the starting point I got

00:18:53,740 --> 00:19:00,790
about 50 to 70 X performance improvement

00:18:57,540 --> 00:19:06,120
so what's kind of the take away the

00:19:00,790 --> 00:19:09,520
methodology if a code doesn't scale well

00:19:06,120 --> 00:19:11,170
user profiling tool whenever people come

00:19:09,520 --> 00:19:14,080
to me says could you help with the

00:19:11,170 --> 00:19:16,150
tuning I'll only kick in when you show

00:19:14,080 --> 00:19:21,130
me the profile where's the time being

00:19:16,150 --> 00:19:23,260
spent all right use that checklist to go

00:19:21,130 --> 00:19:26,170
through bottlenecks just go through them

00:19:23,260 --> 00:19:31,360
how about my barriers how costly our day

00:19:26,170 --> 00:19:33,370
action and one by one it's not like you

00:19:31,360 --> 00:19:35,200
do that in one big blast you reduce the

00:19:33,370 --> 00:19:36,970
barriers you run again check with your

00:19:35,200 --> 00:19:38,500
profiling tool and you see an

00:19:36,970 --> 00:19:40,480
improvement wait a minute that's going

00:19:38,500 --> 00:19:43,030
the right way now sometimes it goes the

00:19:40,480 --> 00:19:45,010
wrong way don't get me wrong you have

00:19:43,030 --> 00:19:46,750
this brilliant idea and the code is

00:19:45,010 --> 00:19:48,760
slower that's that's part of it and

00:19:46,750 --> 00:19:51,250
later you realize the mistake you you

00:19:48,760 --> 00:19:53,620
may have made alright so it's an

00:19:51,250 --> 00:19:58,990
incremental approach absolutely but very

00:19:53,620 --> 00:20:00,970
rewarding to summarize again openmp

00:19:58,990 --> 00:20:02,860
cannot not scale

00:20:00,970 --> 00:20:04,929
there are other things that cause it not

00:20:02,860 --> 00:20:07,360
to scale so please don't blame open it

00:20:04,929 --> 00:20:10,600
be any more in the future name something

00:20:07,360 --> 00:20:12,820
else and in most cases that I see it is

00:20:10,600 --> 00:20:14,320
the application somewhere does it and it

00:20:12,820 --> 00:20:16,990
could be hidden i mean could be hidden

00:20:14,320 --> 00:20:21,309
and again that that could be nasty but

00:20:16,990 --> 00:20:23,260
it's not opening p yourself alright but

00:20:21,309 --> 00:20:25,690
in most cases you'll find a way to

00:20:23,260 --> 00:20:27,789
reduce it or even fix it eliminate

00:20:25,690 --> 00:20:30,970
that's the goal certainly you minimize

00:20:27,789 --> 00:20:33,179
the pain one one other thing when you

00:20:30,970 --> 00:20:36,130
want to go for ultimate scalability

00:20:33,179 --> 00:20:38,200
tackle everything even what doesn't seem

00:20:36,130 --> 00:20:40,510
to be important on a threads or 16

00:20:38,200 --> 00:20:45,130
threads will get to you when you go to a

00:20:40,510 --> 00:20:46,750
high number of fifths alright and of

00:20:45,130 --> 00:20:51,190
course the reward is enjoy your very

00:20:46,750 --> 00:20:53,559
scalable application and that was my

00:20:51,190 --> 00:20:55,870
talk remember it's not that open if it

00:20:53,559 --> 00:20:58,390
doesn't scale bad openmp doesn't scale

00:20:55,870 --> 00:21:00,669
so that's my new logo in the future

00:20:58,390 --> 00:21:04,980
thank you very much for your time and

00:21:00,669 --> 00:21:04,980

YouTube URL: https://www.youtube.com/watch?v=nTcS9082ppA


