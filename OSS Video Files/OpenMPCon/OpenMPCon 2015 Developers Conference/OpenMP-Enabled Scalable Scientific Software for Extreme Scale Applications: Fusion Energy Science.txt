Title: OpenMP-Enabled Scalable Scientific Software for Extreme Scale Applications: Fusion Energy Science
Publication date: 2015-12-13
Playlist: OpenMPCon 2015 Developers Conference
Description: 
	Prof. William Tang, Princeton University
OpenMP Con 2015 - Aachen University, Germany - September 2015
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2015-bill-tang-keynote.pdf

Abstract: A major challenge for supercomputing today is to demonstrate how advances in HPC technology translate to accelerated progress in key application domains – especially with respect to reduction in “time-to-solution” and also “energy to solution” of advanced codes that model complex physical systems. In order to effectively address the extreme concurrency present in modern supercomputing hardware, one of the most efficient algorithmic approaches has been to adopt OpenMP to facilitate efficient multi-threading approaches.

This presentation describes the deployment of Open-MP-enabled scalable scientific software for extreme scale applications – with focus on Fusion Energy Science as an illustrative application domain.

Computational advances in magnetic fusion energy research have produced particle-in-cell (PIC) simulations of turbulent kinetic dynamics for which computer run-time and problem size scale very well with the number of processors on massively parallel many-core supercomputers. For example, the GTC-Princeton (GTC-P) code, which has been developed with a “co-design” focus, has demonstrated the effective usage of the full power of current leadership class computational platforms worldwide at the petascale and beyond to produce efficient nonlinear PIC simulations that have advanced progress in understanding the complex nature of plasma turbulence and confinement in fusion systems.

Results have provided great encouragement for being able to include increasingly realistic dynamics in extreme-scale computing campaigns with the goal of enabling predictive simulations characterized by unprecedented physics realism needed to help accelerate progress in delivering clean energy. In particular, OpenMP usage experience and associated best practices in achieving these advances will be described.
Captions: 
	00:00:04,530 --> 00:00:11,440
thank you very much Michael it's a great

00:00:08,950 --> 00:00:14,650
pleasure to be here and I thank my

00:00:11,440 --> 00:00:20,560
colleague Barbara for steering me this

00:00:14,650 --> 00:00:22,510
way and it's a as michael says I think

00:00:20,560 --> 00:00:25,950
it's very very important in an

00:00:22,510 --> 00:00:28,840
interdisciplinary sense to connect the

00:00:25,950 --> 00:00:32,439
people in the computer science applied

00:00:28,840 --> 00:00:34,630
math of course the design people for the

00:00:32,439 --> 00:00:36,820
big machines and such and then very

00:00:34,630 --> 00:00:40,060
importantly the application domains

00:00:36,820 --> 00:00:43,060
because this is where all of this comes

00:00:40,060 --> 00:00:46,090
together and has an impact on on what we

00:00:43,060 --> 00:00:47,710
do in our daily lives and the various

00:00:46,090 --> 00:00:51,040
governments of the world have made this

00:00:47,710 --> 00:00:52,930
a very strong motive motivational sort

00:00:51,040 --> 00:00:57,010
of thing it's not just in the u.s. so

00:00:52,930 --> 00:00:59,950
speak to that in a minute and so as as

00:00:57,010 --> 00:01:03,550
Michael mentioned my background is in

00:00:59,950 --> 00:01:05,590
Astrophysical sciences the practical

00:01:03,550 --> 00:01:10,110
application domain that I'm focused on

00:01:05,590 --> 00:01:13,060
is delivering bountiful clean energy in

00:01:10,110 --> 00:01:15,820
associated with a fusion energi reaction

00:01:13,060 --> 00:01:19,060
and the connection between astrophysics

00:01:15,820 --> 00:01:24,220
and this is that the medium that we work

00:01:19,060 --> 00:01:26,740
in our is deals with a very hot gas of

00:01:24,220 --> 00:01:28,630
charged particles called plasmas and the

00:01:26,740 --> 00:01:30,760
connection back to astrophysics is that

00:01:28,630 --> 00:01:33,670
over ninety-nine percent of the visible

00:01:30,760 --> 00:01:36,939
universe is made up of plasmas so by way

00:01:33,670 --> 00:01:40,030
of introduction the focus of my

00:01:36,939 --> 00:01:43,060
presentation today is on hpc performance

00:01:40,030 --> 00:01:46,000
scalability and also portability because

00:01:43,060 --> 00:01:47,860
there are lots of different high

00:01:46,000 --> 00:01:50,049
performance computing architectures

00:01:47,860 --> 00:01:54,759
being developed and so if you have a

00:01:50,049 --> 00:01:58,210
very useful application you really want

00:01:54,759 --> 00:01:59,920
to be able to port it and in a and be

00:01:58,210 --> 00:02:02,470
able to deliver a good performance on a

00:01:59,920 --> 00:02:05,079
variety of different architectures so

00:02:02,470 --> 00:02:07,210
i'll talk about aspects of that in a

00:02:05,079 --> 00:02:09,970
representative application domain this

00:02:07,210 --> 00:02:11,620
is not meant to be a focus fusion energy

00:02:09,970 --> 00:02:14,180
type talk I'll give some introductory

00:02:11,620 --> 00:02:17,150
comments about it but it's more

00:02:14,180 --> 00:02:19,640
illustrative that that the more advanced

00:02:17,150 --> 00:02:22,489
sophisticated type computer science

00:02:19,640 --> 00:02:27,579
technology that that's being applied to

00:02:22,489 --> 00:02:31,340
the application like OpenMP that this is

00:02:27,579 --> 00:02:34,310
that if you identify an application

00:02:31,340 --> 00:02:36,829
domain like fusion then you can see what

00:02:34,310 --> 00:02:39,769
the what the pipeline is so to speak in

00:02:36,829 --> 00:02:42,530
terms of what this leads to so fusion

00:02:39,769 --> 00:02:45,079
energy science is is there's a reference

00:02:42,530 --> 00:02:46,760
here the slides are left behind people

00:02:45,079 --> 00:02:49,459
can look this up if you'd like to see it

00:02:46,760 --> 00:02:51,739
this is a publication that came out in

00:02:49,459 --> 00:02:54,019
size the computing and science and

00:02:51,739 --> 00:02:56,810
engineering publication just this past

00:02:54,019 --> 00:02:58,430
year the current progress that we're

00:02:56,810 --> 00:03:00,799
going to talk about here is the

00:02:58,430 --> 00:03:04,849
deployment of innovative algorithms

00:03:00,799 --> 00:03:07,159
including openmp with a modern code that

00:03:04,849 --> 00:03:09,620
delivers new scientific insights on

00:03:07,159 --> 00:03:12,079
world class systems it speaks to the

00:03:09,620 --> 00:03:13,430
portability issue and we worked hard

00:03:12,079 --> 00:03:17,780
over the course of the last couple of

00:03:13,430 --> 00:03:21,139
years to deploy this discovery science

00:03:17,780 --> 00:03:23,269
capable code on these top seven

00:03:21,139 --> 00:03:26,680
supercomputers in the world and in the

00:03:23,269 --> 00:03:29,419
near future we also have an entry to the

00:03:26,680 --> 00:03:33,139
largest projects in the u.s. such as

00:03:29,419 --> 00:03:36,739
summit the under petaflop system at oak

00:03:33,139 --> 00:03:38,650
ridge national lab we're also a part of

00:03:36,739 --> 00:03:44,449
Cory that Helen will talk about later

00:03:38,650 --> 00:03:47,269
and also on on the NSF machines and such

00:03:44,449 --> 00:03:49,629
too so I'll make some comments on future

00:03:47,269 --> 00:03:53,150
progress what the needs are and

00:03:49,629 --> 00:03:56,780
emphasize throughout that it's not

00:03:53,150 --> 00:03:59,500
possible to achieve significant progress

00:03:56,780 --> 00:04:03,250
in a timely way without a true

00:03:59,500 --> 00:04:07,699
interdisciplinary sort of a research

00:04:03,250 --> 00:04:10,009
environment to carry that out and very

00:04:07,699 --> 00:04:13,030
clearly when cannot flip products over

00:04:10,009 --> 00:04:18,199
the fence and expect things to happen so

00:04:13,030 --> 00:04:21,620
this is a slide that that I borrowed

00:04:18,199 --> 00:04:23,210
from jack dongarra from a talk that he

00:04:21,620 --> 00:04:27,290
gave at the beginning of this year

00:04:23,210 --> 00:04:30,080
before updating this to the 2015

00:04:27,290 --> 00:04:32,300
machines the main you've probably seen

00:04:30,080 --> 00:04:34,640
all this is the top 500 business and

00:04:32,300 --> 00:04:36,110
this is where our iphones and laptops

00:04:34,640 --> 00:04:39,950
and things are going the progress is

00:04:36,110 --> 00:04:41,960
really dramatic and so the and it

00:04:39,950 --> 00:04:44,180
doesn't seem to be stopping as you've

00:04:41,960 --> 00:04:46,490
heard throughout the conference so what

00:04:44,180 --> 00:04:49,250
does this mean well as as we've said

00:04:46,490 --> 00:04:52,400
earlier you want to speak of what what i

00:04:49,250 --> 00:04:55,220
want to highlight is the imperative for

00:04:52,400 --> 00:04:57,860
applications impact the actual value of

00:04:55,220 --> 00:05:01,100
extreme scale HPC to scientific domain

00:04:57,860 --> 00:05:04,880
applications and industry and the

00:05:01,100 --> 00:05:07,340
context in some respects is in the US

00:05:04,880 --> 00:05:09,490
there's a major initiative called the

00:05:07,340 --> 00:05:14,080
national strategic computing initiative

00:05:09,490 --> 00:05:18,740
the White House as has energized the

00:05:14,080 --> 00:05:21,350
across-the-board pretty much the OSTP

00:05:18,740 --> 00:05:25,040
and OMB you know the main agencies that

00:05:21,350 --> 00:05:27,440
sit above the Department of Energy NSF

00:05:25,040 --> 00:05:29,270
and so forth to mobilize major activity

00:05:27,440 --> 00:05:32,630
that will accelerate the pace to

00:05:29,270 --> 00:05:35,180
exascale and beyond but also in the in

00:05:32,630 --> 00:05:37,490
the announcement they said that it's

00:05:35,180 --> 00:05:40,250
very very important to see the direct

00:05:37,490 --> 00:05:43,010
benefits accelerated progress and things

00:05:40,250 --> 00:05:46,100
that the u.s. cares about and things are

00:05:43,010 --> 00:05:49,370
quite exciting right now and are moving

00:05:46,100 --> 00:05:51,260
forward pretty rapidly already so what

00:05:49,370 --> 00:05:53,600
are the practical considerations as you

00:05:51,260 --> 00:05:55,310
think about this the you need to get

00:05:53,600 --> 00:05:58,580
better Buy in from science and industry

00:05:55,310 --> 00:06:00,890
what is this require it means moving

00:05:58,580 --> 00:06:04,250
beyond voracious more the same just

00:06:00,890 --> 00:06:06,830
bigger and faster type computations to

00:06:04,250 --> 00:06:10,310
what what we call transformational type

00:06:06,830 --> 00:06:11,870
type advances which means the

00:06:10,310 --> 00:06:14,060
achievement of major new levels of

00:06:11,870 --> 00:06:17,870
scientific understanding in a particular

00:06:14,060 --> 00:06:20,570
application area improving this involves

00:06:17,870 --> 00:06:23,470
improving the observational experimental

00:06:20,570 --> 00:06:26,120
validation and verification which is

00:06:23,470 --> 00:06:30,860
verification is purely theoretical or

00:06:26,120 --> 00:06:34,070
computational but this this involves for

00:06:30,860 --> 00:06:36,950
example cross verifying against analytic

00:06:34,070 --> 00:06:38,390
type type estimates also cross code

00:06:36,950 --> 00:06:39,720
comparisons using different

00:06:38,390 --> 00:06:44,610
methodologies and such

00:06:39,720 --> 00:06:47,760
but the real world test is is to compare

00:06:44,610 --> 00:06:49,290
against the observational information to

00:06:47,760 --> 00:06:52,470
enhance the realistic predictive

00:06:49,290 --> 00:06:55,400
capability and this is interesting that

00:06:52,470 --> 00:06:57,570
these days is in addition to the

00:06:55,400 --> 00:07:00,030
hypothesis driven so-called first

00:06:57,570 --> 00:07:03,990
principles type approaches that we

00:07:00,030 --> 00:07:06,180
typically apply HPC to the huge area of

00:07:03,990 --> 00:07:08,310
growth is in big data driven statistical

00:07:06,180 --> 00:07:11,430
approaches machine learning and the like

00:07:08,310 --> 00:07:13,580
and and I think very quickly we're going

00:07:11,430 --> 00:07:16,110
to see a complementarity between the two

00:07:13,580 --> 00:07:18,750
well we need to deliver software

00:07:16,110 --> 00:07:21,120
engineering tools to improve time to

00:07:18,750 --> 00:07:24,210
solution energy to solution what do I

00:07:21,120 --> 00:07:26,250
mean by that the major sort of hero

00:07:24,210 --> 00:07:29,700
calculations that you carry out in any

00:07:26,250 --> 00:07:32,010
application domain to the to the

00:07:29,700 --> 00:07:34,020
application scientists the coin of the

00:07:32,010 --> 00:07:36,570
realm is what is is how long does it

00:07:34,020 --> 00:07:39,960
take but in addition these days because

00:07:36,570 --> 00:07:42,930
the computers also chew up a lot of

00:07:39,960 --> 00:07:45,360
energy in carrying out big calculations

00:07:42,930 --> 00:07:49,110
what is the energy to solution so these

00:07:45,360 --> 00:07:51,180
metrics of time to solution together

00:07:49,110 --> 00:07:53,880
with energy to solution very important

00:07:51,180 --> 00:07:57,570
and illustrate that a bit and what we're

00:07:53,880 --> 00:08:02,250
talking about my colleague David keys at

00:07:57,570 --> 00:08:05,810
a conference this this pasta just a

00:08:02,250 --> 00:08:08,790
couple of months ago at Argonne I

00:08:05,810 --> 00:08:12,030
thought a quote from his presentation

00:08:08,790 --> 00:08:13,500
was was very relevant to today billions

00:08:12,030 --> 00:08:15,330
of dollars of scientific software

00:08:13,500 --> 00:08:17,729
worldwide hangs in the balance until

00:08:15,330 --> 00:08:20,130
better algorithms arrive to span the

00:08:17,729 --> 00:08:22,500
architecture applications gap and this

00:08:20,130 --> 00:08:24,240
is very true around the world the

00:08:22,500 --> 00:08:26,729
associated challenges you're aware of

00:08:24,240 --> 00:08:30,090
hardware complexity heterogeneous

00:08:26,729 --> 00:08:32,729
multi-core involving GPUs with CPUs good

00:08:30,090 --> 00:08:37,680
example of summit 100 petaflop project

00:08:32,729 --> 00:08:40,830
in the u.s. it's that that is at Oak

00:08:37,680 --> 00:08:42,820
Ridge National Lab and then the mic and

00:08:40,830 --> 00:08:45,880
CPUs

00:08:42,820 --> 00:08:48,100
which will be in 100 petaflop or a

00:08:45,880 --> 00:08:51,190
system at Argonne National Lab the

00:08:48,100 --> 00:08:53,950
software challenges involved at the most

00:08:51,190 --> 00:08:56,080
fundamental level it's very hard to to

00:08:53,950 --> 00:08:57,370
pull people away from their legacy code

00:08:56,080 --> 00:09:00,010
approaches because so much has been

00:08:57,370 --> 00:09:01,930
invested the legacy codes are still

00:09:00,010 --> 00:09:04,660
going to be useful because you need for

00:09:01,930 --> 00:09:07,210
what you need it for the verification

00:09:04,660 --> 00:09:10,810
aspects of things you in before jumping

00:09:07,210 --> 00:09:13,840
to your new engine and such you have to

00:09:10,810 --> 00:09:16,750
verify that that it it does it does what

00:09:13,840 --> 00:09:19,980
you want it to do the software

00:09:16,750 --> 00:09:22,930
challenges though do force you to

00:09:19,980 --> 00:09:24,270
consider significant code rewrite and

00:09:22,930 --> 00:09:28,390
we've done that in our particular

00:09:24,270 --> 00:09:30,160
applications the imperative here is what

00:09:28,390 --> 00:09:32,140
is the level of accountability everybody

00:09:30,160 --> 00:09:34,000
could just say well alright all this new

00:09:32,140 --> 00:09:36,520
technology we want to be part of it we

00:09:34,000 --> 00:09:39,310
need it but how have you demonstrated

00:09:36,520 --> 00:09:42,010
that you deserve to be funded to get the

00:09:39,310 --> 00:09:44,620
resources so you need to provide

00:09:42,010 --> 00:09:47,410
specific examples of impactful

00:09:44,620 --> 00:09:49,420
scientific admission advances enabled by

00:09:47,410 --> 00:09:52,570
progress from what we don't have

00:09:49,420 --> 00:09:54,430
collective Alzheimer's so from from the

00:09:52,570 --> 00:09:57,640
terror scale to the petascale to today's

00:09:54,430 --> 00:10:01,090
multi petascale HPC capabilities if you

00:09:57,640 --> 00:10:03,220
want to apply for resources of this kind

00:10:01,090 --> 00:10:04,660
you should be able to demonstrate what

00:10:03,220 --> 00:10:06,910
have you done before what have you

00:10:04,660 --> 00:10:11,800
delivered what were the transformational

00:10:06,910 --> 00:10:16,540
changes so this is just a bit of an

00:10:11,800 --> 00:10:18,850
intro to to the application domain CNN

00:10:16,540 --> 00:10:21,580
that has done a series called moon shots

00:10:18,850 --> 00:10:24,190
for the 21st century hosted by fareed

00:10:21,580 --> 00:10:27,640
zakaria and there are five segments

00:10:24,190 --> 00:10:30,490
broadcast very recently spring a 2015 on

00:10:27,640 --> 00:10:32,950
CNN with all their production values and

00:10:30,490 --> 00:10:35,050
such and it's very accessible these are

00:10:32,950 --> 00:10:37,270
not very long presentations each of

00:10:35,050 --> 00:10:39,850
these grand challenges for the 21st

00:10:37,270 --> 00:10:41,980
century including fusion energy that

00:10:39,850 --> 00:10:44,950
I'll talk about here include mission to

00:10:41,980 --> 00:10:47,920
Mars human heart hypersonic aviation the

00:10:44,950 --> 00:10:50,890
human brain and it's a very very good

00:10:47,920 --> 00:10:53,170
series and it's very very accessible

00:10:50,890 --> 00:10:55,130
introduction to these grand challenge

00:10:53,170 --> 00:10:59,420
areas for the 21st century

00:10:55,130 --> 00:11:06,860
so so what about fusion what is this the

00:10:59,420 --> 00:11:09,380
the most widely subscribe to approach

00:11:06,860 --> 00:11:11,150
involves confining this very hot plasma

00:11:09,380 --> 00:11:14,750
that I talked about this very hot

00:11:11,150 --> 00:11:18,320
thermonuclear gas with magnetic traps

00:11:14,750 --> 00:11:20,780
because you cannot keep it confined

00:11:18,320 --> 00:11:23,360
unless you use the electromagnetic

00:11:20,780 --> 00:11:25,490
fields that work on the charged

00:11:23,360 --> 00:11:28,160
particles remember I said these are all

00:11:25,490 --> 00:11:30,590
charged particles in the system and and

00:11:28,160 --> 00:11:32,990
so this is a doughnut-shaped machine

00:11:30,590 --> 00:11:35,210
it's called a tokamak the Russians

00:11:32,990 --> 00:11:38,030
invented at first a long time ago and

00:11:35,210 --> 00:11:42,280
this is the basic fusion process where

00:11:38,030 --> 00:11:45,100
you take the isotopes of height of of

00:11:42,280 --> 00:11:48,770
water basically deuterium and tritium

00:11:45,100 --> 00:11:51,440
and you subjected to a lot of heat the

00:11:48,770 --> 00:11:53,360
fusion reaction occurs under the proper

00:11:51,440 --> 00:11:57,380
condition and if you're successful in

00:11:53,360 --> 00:12:00,140
doing that then the the constitutive

00:11:57,380 --> 00:12:02,660
products an alpha particle in a fast

00:12:00,140 --> 00:12:05,600
Neutron come out and there's less mass

00:12:02,660 --> 00:12:08,480
here than here and according to Einstein

00:12:05,600 --> 00:12:10,400
that gets converted into energy the

00:12:08,480 --> 00:12:13,370
energy multiplication factor is about

00:12:10,400 --> 00:12:16,460
450 to one this is the reaction right

00:12:13,370 --> 00:12:18,080
here so you're confining an extremely

00:12:16,460 --> 00:12:20,570
hot plasma several hundred million

00:12:18,080 --> 00:12:22,730
degrees confined by a strong magnetic

00:12:20,570 --> 00:12:24,440
field so this is like the surface of the

00:12:22,730 --> 00:12:26,600
Sun these sorts of temperature is

00:12:24,440 --> 00:12:30,590
greater than that but there's various

00:12:26,600 --> 00:12:33,650
things in physics language the the

00:12:30,590 --> 00:12:36,230
thermodynamics always will try to drive

00:12:33,650 --> 00:12:39,530
the particles and the energy out of the

00:12:36,230 --> 00:12:41,450
confinement region the every mother

00:12:39,530 --> 00:12:43,460
nature wants everything to be uniform

00:12:41,450 --> 00:12:45,530
and flat and one of the ways that

00:12:43,460 --> 00:12:48,800
achieves this is through turbulent

00:12:45,530 --> 00:12:51,470
processes very rapid transfers physics

00:12:48,800 --> 00:12:53,240
mechanisms for energy leakage from the

00:12:51,470 --> 00:12:55,550
magnetic confinement system and this is

00:12:53,240 --> 00:12:58,490
what we combat in this application

00:12:55,550 --> 00:13:01,190
domain to try to make this fusion

00:12:58,490 --> 00:13:03,310
reaction happen and if you do the as i

00:13:01,190 --> 00:13:07,820
said the energy multiplication is huge

00:13:03,310 --> 00:13:09,890
so this is a list of all of the

00:13:07,820 --> 00:13:12,410
very attractive aspects of fusion energy

00:13:09,890 --> 00:13:15,230
that CNN report talks it talks about

00:13:12,410 --> 00:13:17,930
aspects of this it's it's abundant the

00:13:15,230 --> 00:13:21,890
the you know comes from water comes from

00:13:17,930 --> 00:13:26,630
the oceans deuterium and tritium are

00:13:21,890 --> 00:13:29,630
very easy to find up find a fusion

00:13:26,630 --> 00:13:32,000
reactor cannot blow up or melt down so

00:13:29,630 --> 00:13:34,970
it cannot there's no risk of nuclear

00:13:32,000 --> 00:13:37,880
materials proliferation lots of ideal

00:13:34,970 --> 00:13:40,100
sorts of aspects and some people joke

00:13:37,880 --> 00:13:42,410
about fusion and say it's the energy

00:13:40,100 --> 00:13:44,510
source of the future and it will always

00:13:42,410 --> 00:13:47,120
be the energy source of the future

00:13:44,510 --> 00:13:49,970
because it's very very hard to to to

00:13:47,120 --> 00:13:51,860
harness and so we don't make the pitch

00:13:49,970 --> 00:13:54,440
that this is the only thing you should

00:13:51,860 --> 00:13:56,270
concentrate on it's quite complementary

00:13:54,440 --> 00:13:59,690
to other attractive energy sources as

00:13:56,270 --> 00:14:02,000
long as they're safe the for example the

00:13:59,690 --> 00:14:04,610
the natural sources of wind and solar

00:14:02,000 --> 00:14:08,150
and so forth are great but they're not

00:14:04,610 --> 00:14:10,850
going to supply in a carbon-carbon free

00:14:08,150 --> 00:14:14,930
environment freedom from fossil fuels

00:14:10,850 --> 00:14:19,100
enough energy for you to be able to move

00:14:14,930 --> 00:14:21,620
forward so this this is a nice plot that

00:14:19,100 --> 00:14:23,780
that I like to show because to counter a

00:14:21,620 --> 00:14:25,370
little bit the comment that fusion is

00:14:23,780 --> 00:14:27,290
the energy source of the future and

00:14:25,370 --> 00:14:30,230
always will be there has been a lot of

00:14:27,290 --> 00:14:31,940
progress a beginning and around 1975

00:14:30,230 --> 00:14:37,040
when I was just getting out of grad

00:14:31,940 --> 00:14:39,800
school and such the until the mid-90s or

00:14:37,040 --> 00:14:42,890
so what's shown here is the energy

00:14:39,800 --> 00:14:45,050
production out of a fusion confinement

00:14:42,890 --> 00:14:48,140
device like a tokamak over the years and

00:14:45,050 --> 00:14:51,020
here there's a very rapid rise American

00:14:48,140 --> 00:14:53,660
flag denotes the machine that was at

00:14:51,020 --> 00:14:57,830
Princeton University at the Michael

00:14:53,660 --> 00:15:00,950
mentioned where I'm at at the at the

00:14:57,830 --> 00:15:03,380
Princeton Plasma Physics lab and what

00:15:00,950 --> 00:15:05,300
was achieved here is if you if you

00:15:03,380 --> 00:15:07,960
measure how much energy comes out versus

00:15:05,300 --> 00:15:11,030
how much energy goes into the reaction

00:15:07,960 --> 00:15:14,150
here it was about two parts in 20,000

00:15:11,030 --> 00:15:17,750
here it's about point two so you went up

00:15:14,150 --> 00:15:20,120
a huge factor in 20 years and this is if

00:15:17,750 --> 00:15:21,170
you plotted against the famous Moore's

00:15:20,120 --> 00:15:22,760
law empirical

00:15:21,170 --> 00:15:25,790
this is steeper than the Moore's Law

00:15:22,760 --> 00:15:29,570
curve but unlike new laptops as I showed

00:15:25,790 --> 00:15:31,430
and Heather slide a very you know quite

00:15:29,570 --> 00:15:35,329
frequently you're not putting a new

00:15:31,430 --> 00:15:38,180
energy source out the delivery period is

00:15:35,329 --> 00:15:40,600
a long one and within three years after

00:15:38,180 --> 00:15:43,550
the u.s. made this achievement of

00:15:40,600 --> 00:15:46,040
getting out you know this point to ratio

00:15:43,550 --> 00:15:49,190
of energy out versus energy in the

00:15:46,040 --> 00:15:51,709
European so the EU came up with a more

00:15:49,190 --> 00:15:53,480
powerful machine that's still operating

00:15:51,709 --> 00:15:55,940
that set the world record of 16

00:15:53,480 --> 00:15:58,610
megawatts of fusion energy delivered and

00:15:55,940 --> 00:16:00,410
here you reached almost break even which

00:15:58,610 --> 00:16:02,870
is energy out is almost equal to energy

00:16:00,410 --> 00:16:06,010
and it's about point eight and this

00:16:02,870 --> 00:16:09,079
motivated the targeting this huge

00:16:06,010 --> 00:16:11,720
project called eater because you now

00:16:09,079 --> 00:16:14,089
have to keep the confinement sustained

00:16:11,720 --> 00:16:15,980
for a long period of time and you have

00:16:14,089 --> 00:16:18,740
to use superconducting materials and

00:16:15,980 --> 00:16:20,600
such so it's expensive and this is a 25

00:16:18,740 --> 00:16:22,820
billion dollar project that's being

00:16:20,600 --> 00:16:25,430
constructed in the South of France that

00:16:22,820 --> 00:16:27,890
CNN clip also shows the construction and

00:16:25,430 --> 00:16:31,220
this is the grand challenge to deliver

00:16:27,890 --> 00:16:34,100
this this involves the engagement of of

00:16:31,220 --> 00:16:35,660
seven nation of seven governments

00:16:34,100 --> 00:16:38,540
representing over half the world's

00:16:35,660 --> 00:16:40,430
population and it would demonstrate the

00:16:38,540 --> 00:16:44,149
scientific and technical feasibility of

00:16:40,430 --> 00:16:46,370
fusion power so it's going from the 10

00:16:44,149 --> 00:16:48,529
megawatts or so they can be confined for

00:16:46,370 --> 00:16:51,680
about a second with a gain of order

00:16:48,529 --> 00:16:53,930
unity today to going to 500 megawatts

00:16:51,680 --> 00:16:56,449
for greater than 400 seconds with a gain

00:16:53,930 --> 00:16:59,329
greater than 10 I liked it is so when

00:16:56,449 --> 00:17:00,949
people ask me if you achieve this what

00:16:59,329 --> 00:17:03,170
does this mean in terms of delivery of

00:17:00,949 --> 00:17:05,329
fusion energy this would be if you hit a

00:17:03,170 --> 00:17:08,150
gain of between 10 and 20 in this eater

00:17:05,329 --> 00:17:10,040
device that you'll be about where the

00:17:08,150 --> 00:17:12,829
Wright brothers were at Kitty Hawk the

00:17:10,040 --> 00:17:14,209
plane will fly okay but if you had

00:17:12,829 --> 00:17:15,709
interviewed the Wright brothers then

00:17:14,209 --> 00:17:17,120
they would not have been able to tell

00:17:15,709 --> 00:17:19,429
you it's competitive with trains and

00:17:17,120 --> 00:17:21,559
boats as a form of transportation so you

00:17:19,429 --> 00:17:24,559
would need a lot of technology and such

00:17:21,559 --> 00:17:26,449
to drive it forward but I would submit

00:17:24,559 --> 00:17:28,400
to you that energy futures are more

00:17:26,449 --> 00:17:30,200
important than transportation the

00:17:28,400 --> 00:17:31,720
governments around the world for energy

00:17:30,200 --> 00:17:35,090
security

00:17:31,720 --> 00:17:37,460
lots of reasons will really go

00:17:35,090 --> 00:17:40,159
aggressively into this this is a huge

00:17:37,460 --> 00:17:42,950
consortium right now as I said the main

00:17:40,159 --> 00:17:47,570
participants are the EU that's why it's

00:17:42,950 --> 00:17:51,289
based in in France but the US China

00:17:47,570 --> 00:17:55,250
Korea Russia there are many many partner

00:17:51,289 --> 00:17:57,230
nations are involved in this so the this

00:17:55,250 --> 00:18:00,200
is I'm not going to dwell on this these

00:17:57,230 --> 00:18:02,480
are the basic fundamental equations the

00:18:00,200 --> 00:18:05,029
this discoveries of distribution of

00:18:02,480 --> 00:18:07,220
particles in the system this in a

00:18:05,029 --> 00:18:09,679
particle pushing pushing sense these are

00:18:07,220 --> 00:18:12,440
eau de these are force equals mass times

00:18:09,679 --> 00:18:13,970
acceleration type things this is the way

00:18:12,440 --> 00:18:16,880
that the distribution function is

00:18:13,970 --> 00:18:20,110
represented and then if you just did

00:18:16,880 --> 00:18:23,179
this this type of simulation of the

00:18:20,110 --> 00:18:25,370
confined hot charged particles plasma

00:18:23,179 --> 00:18:27,649
behavior it's very much like a monte

00:18:25,370 --> 00:18:29,480
carlo type simulation and everyone

00:18:27,649 --> 00:18:35,270
understands what a monte carlo go does

00:18:29,480 --> 00:18:37,250
but the additional challenge for plasma

00:18:35,270 --> 00:18:39,679
physics type calculations is that you

00:18:37,250 --> 00:18:42,289
have to self-consistently solve for the

00:18:39,679 --> 00:18:45,440
field and these involve solving plus on

00:18:42,289 --> 00:18:47,360
like equations linear PDE s and oil

00:18:45,440 --> 00:18:49,789
arian coordinates or the so-called lab

00:18:47,360 --> 00:18:51,860
frame and there there are three of these

00:18:49,789 --> 00:18:54,169
equations that you have to solve couple

00:18:51,860 --> 00:18:56,480
of the solving self-consistently for the

00:18:54,169 --> 00:18:58,340
distribution function so these give the

00:18:56,480 --> 00:19:00,950
forces that push the particles through

00:18:58,340 --> 00:19:05,059
the system and this is like an

00:19:00,950 --> 00:19:08,450
exaggerated schematic of this donut

00:19:05,059 --> 00:19:11,240
shaped magnetic confinement system this

00:19:08,450 --> 00:19:14,270
shows the radial cross-section the

00:19:11,240 --> 00:19:17,360
mathematics involve a 5d so-called gyro

00:19:14,270 --> 00:19:20,090
kinetic plus on of Las applause on

00:19:17,360 --> 00:19:22,669
system the numerical approach is

00:19:20,090 --> 00:19:26,539
well-established particle cell methods

00:19:22,669 --> 00:19:29,120
are are very very well known and gyro

00:19:26,539 --> 00:19:30,380
kinetic just means that as physicists we

00:19:29,120 --> 00:19:32,179
look at the problem and just say you

00:19:30,380 --> 00:19:36,049
don't have to do it on all time scales

00:19:32,179 --> 00:19:38,480
if you do a fast averaging then you pick

00:19:36,049 --> 00:19:41,270
up the essential behavior that you need

00:19:38,480 --> 00:19:44,389
and so that's why we call a gyro kinetic

00:19:41,270 --> 00:19:45,200
you a ver Ajay over the very fast time

00:19:44,389 --> 00:19:48,080
scales

00:19:45,200 --> 00:19:51,080
to get at the key dynamical processes

00:19:48,080 --> 00:19:53,929
that you're interested in so the object

00:19:51,080 --> 00:19:57,139
so here basically it shows that you know

00:19:53,929 --> 00:20:00,740
in in the in the past decade or so you

00:19:57,139 --> 00:20:04,460
moved up from this is a typical sort of

00:20:00,740 --> 00:20:07,159
simulation 130 million grid points 30

00:20:04,460 --> 00:20:09,679
billion particles over 10,000 time steps

00:20:07,159 --> 00:20:11,840
these are not individual particles these

00:20:09,679 --> 00:20:14,269
are like super sized particles otherwise

00:20:11,840 --> 00:20:16,539
it's impossible to push like avocados

00:20:14,269 --> 00:20:18,049
number of particles but

00:20:16,539 --> 00:20:19,970
electromagnetically the sound

00:20:18,049 --> 00:20:22,789
foundations are very well established

00:20:19,970 --> 00:20:25,460
that you can push these super sized

00:20:22,789 --> 00:20:27,289
particles so the objective is the

00:20:25,460 --> 00:20:30,019
development of fish and numerical tool

00:20:27,289 --> 00:20:32,600
to realistically simulate turbulence and

00:20:30,019 --> 00:20:35,799
associated transport in these magnetic

00:20:32,600 --> 00:20:38,870
traps using high-end supercomputers and

00:20:35,799 --> 00:20:41,149
this is again another cartoon schematic

00:20:38,870 --> 00:20:44,960
you just distribute the particles on a

00:20:41,149 --> 00:20:46,760
grid and the the charged particles will

00:20:44,960 --> 00:20:50,149
sample a distribution distribution

00:20:46,760 --> 00:20:51,860
function the interactions occur on this

00:20:50,149 --> 00:20:53,840
grid with the forces that i mentioned

00:20:51,860 --> 00:20:56,269
determined by the gradient of the

00:20:53,840 --> 00:20:58,309
electrostatic potential which is

00:20:56,269 --> 00:21:02,029
calculated from the deposited charges

00:20:58,309 --> 00:21:04,010
the grid resolution is dictated by this

00:21:02,029 --> 00:21:06,200
electromagnetic quantity the Debye

00:21:04,010 --> 00:21:09,350
length that leads to finite sized

00:21:06,200 --> 00:21:11,960
particles they up to this gyro radius

00:21:09,350 --> 00:21:14,149
scale and so these are the fundamental

00:21:11,960 --> 00:21:16,250
particle and cell operations involving

00:21:14,149 --> 00:21:18,889
the gathers scatter sort of process and

00:21:16,250 --> 00:21:21,320
I won't read through all of this I don't

00:21:18,889 --> 00:21:23,299
want to run out of time here so you

00:21:21,320 --> 00:21:26,139
basically repeat this process over and

00:21:23,299 --> 00:21:29,450
over and you can associate this with

00:21:26,139 --> 00:21:33,860
identifiable well identified functions

00:21:29,450 --> 00:21:36,080
that that for optimization purposes it

00:21:33,860 --> 00:21:38,779
really focuses you on what you need to

00:21:36,080 --> 00:21:41,389
do in terms of the operations and so

00:21:38,779 --> 00:21:44,779
this just basically repeats what I just

00:21:41,389 --> 00:21:46,340
said in more detail the particle and

00:21:44,779 --> 00:21:47,899
cell approach involves two different

00:21:46,340 --> 00:21:51,169
data structures and two types of

00:21:47,899 --> 00:21:52,820
operations charged particle to grid

00:21:51,169 --> 00:21:56,280
interpolation this is the scatter

00:21:52,820 --> 00:21:58,470
process the plus on our field solve this

00:21:56,280 --> 00:22:01,650
determines the forces on the particles

00:21:58,470 --> 00:22:03,240
as you move a time evolve it and then

00:22:01,650 --> 00:22:05,220
you push it the grid the particle

00:22:03,240 --> 00:22:09,330
interpolation this is the gather process

00:22:05,220 --> 00:22:12,570
so getting down to what this impacts

00:22:09,330 --> 00:22:14,820
this is a plot of this is like how well

00:22:12,570 --> 00:22:17,160
do you keep this hot gas can find this

00:22:14,820 --> 00:22:19,620
is a measure of the confinement of

00:22:17,160 --> 00:22:21,390
confinement characteristic this

00:22:19,620 --> 00:22:24,570
determines the size of the machine

00:22:21,390 --> 00:22:31,140
because what what what we and Princeton

00:22:24,570 --> 00:22:33,930
tried to do was to address this the very

00:22:31,140 --> 00:22:35,820
fundamental problem of if you go from a

00:22:33,930 --> 00:22:38,790
small system to a large system it's a

00:22:35,820 --> 00:22:40,740
fundamental side problem size issue but

00:22:38,790 --> 00:22:43,590
there's no free lunch as you go from

00:22:40,740 --> 00:22:46,140
this is like a very small device this is

00:22:43,590 --> 00:22:48,210
like the size of the devices in the US

00:22:46,140 --> 00:22:50,370
and in most of the world this is the

00:22:48,210 --> 00:22:53,070
joint European tour so this is going out

00:22:50,370 --> 00:22:56,040
in size as you see and then you hit

00:22:53,070 --> 00:22:58,890
eater the 25 billion dollar machine

00:22:56,040 --> 00:23:00,390
being constructed now in France and the

00:22:58,890 --> 00:23:03,570
Machine size is getting bigger and

00:23:00,390 --> 00:23:07,890
bigger and there's no free lunch you

00:23:03,570 --> 00:23:11,640
know you have to provide the spatial and

00:23:07,890 --> 00:23:14,250
the phase space resolution to be able to

00:23:11,640 --> 00:23:16,620
resolve the problem well about a little

00:23:14,250 --> 00:23:20,460
more than a decade ago we delivered

00:23:16,620 --> 00:23:22,470
multi teraflop simulations using a 3d

00:23:20,460 --> 00:23:24,540
particle and sell codes and this

00:23:22,470 --> 00:23:27,300
involves the quantities that I just

00:23:24,540 --> 00:23:29,400
mentioned to you and you push this

00:23:27,300 --> 00:23:32,430
forward over seven thousand or so time

00:23:29,400 --> 00:23:34,830
steps and we're able to do a very rough

00:23:32,430 --> 00:23:37,560
estimate of what the confinement is like

00:23:34,830 --> 00:23:40,560
as you go to eater and this is the trend

00:23:37,560 --> 00:23:42,660
that that it's a predictive trend so

00:23:40,560 --> 00:23:44,550
there's a scientific discovery if you

00:23:42,660 --> 00:23:46,860
just look at things empirically that's

00:23:44,550 --> 00:23:49,590
mostly here and the trend is pretty

00:23:46,860 --> 00:23:51,300
steep but then it does roll over the the

00:23:49,590 --> 00:23:54,090
physics tells you that it will roll over

00:23:51,300 --> 00:23:57,000
but you need more physics understanding

00:23:54,090 --> 00:23:59,580
to really trust this result and so to do

00:23:57,000 --> 00:24:02,400
that the excellent scalability of the

00:23:59,580 --> 00:24:04,290
pic codes on modern platforms will

00:24:02,400 --> 00:24:06,810
enable the resolution and physics

00:24:04,290 --> 00:24:07,800
fidelity that you need for the physics

00:24:06,810 --> 00:24:10,950
understanding for

00:24:07,800 --> 00:24:12,660
large system but efficient usage of the

00:24:10,950 --> 00:24:15,330
current leadership class facilities

00:24:12,660 --> 00:24:18,240
worldwide demands as I mentioned earlier

00:24:15,330 --> 00:24:20,640
code rewrites featuring modern computer

00:24:18,240 --> 00:24:23,550
science applied math message methods

00:24:20,640 --> 00:24:26,250
including OpenMP to address the extreme

00:24:23,550 --> 00:24:30,510
concurrency data locality and memory

00:24:26,250 --> 00:24:32,540
demands so in our own work in our work

00:24:30,510 --> 00:24:36,210
at Princeton were greatly aided by the

00:24:32,540 --> 00:24:39,000
the u.s. inside projects which provided

00:24:36,210 --> 00:24:42,380
lots of cycles on the leadership class

00:24:39,000 --> 00:24:45,300
systems we also participated in the

00:24:42,380 --> 00:24:48,480
international g8 project that was six

00:24:45,300 --> 00:24:51,750
topics six problem areas were selected

00:24:48,480 --> 00:24:54,330
with one being fusion I was a USP I for

00:24:51,750 --> 00:24:57,300
this project and the interesting aspect

00:24:54,330 --> 00:24:59,760
of this in terms of portability is that

00:24:57,300 --> 00:25:02,370
we were given access to the top

00:24:59,760 --> 00:25:04,490
supercomputers in that consortium so we

00:25:02,370 --> 00:25:07,500
were able to run on the big machines in

00:25:04,490 --> 00:25:10,560
Germany here in ulica computing center

00:25:07,500 --> 00:25:16,200
on Japan's number-one machine the k

00:25:10,560 --> 00:25:19,260
computer in kobe and and then so that

00:25:16,200 --> 00:25:22,350
was a very stimulating testbed to do

00:25:19,260 --> 00:25:25,230
things so let me get to more

00:25:22,350 --> 00:25:29,610
specifically to the role of openmp in

00:25:25,230 --> 00:25:32,100
addressing these challenges the extreme

00:25:29,610 --> 00:25:34,140
concurrency adopting OpenMP is one of

00:25:32,100 --> 00:25:35,940
the most efficient algorithmic approach

00:25:34,140 --> 00:25:38,070
is to facilitate efficient

00:25:35,940 --> 00:25:40,860
multi-threading which you need to engage

00:25:38,070 --> 00:25:43,400
all these tremendous numbers of

00:25:40,860 --> 00:25:47,340
processors I mentioned portability

00:25:43,400 --> 00:25:49,680
except for GPU hardware which requires a

00:25:47,340 --> 00:25:52,410
bit more work openmp works with all

00:25:49,680 --> 00:25:54,950
multi-core processors ease of deployment

00:25:52,410 --> 00:25:57,240
is that it's now I'm very mature

00:25:54,950 --> 00:26:00,600
implementation relatively easy to use

00:25:57,240 --> 00:26:04,320
the easiest approach is to deploy OpenMP

00:26:00,600 --> 00:26:06,630
at the loop level OpenMP work best at

00:26:04,320 --> 00:26:09,300
this level in the late 90s and early

00:26:06,630 --> 00:26:11,820
2000s when it was introduced and has

00:26:09,300 --> 00:26:14,010
remained the best approach and most all

00:26:11,820 --> 00:26:16,890
application domains since then for for

00:26:14,010 --> 00:26:20,730
dealing with multi threading and

00:26:16,890 --> 00:26:21,539
challenges of this type the example in

00:26:20,730 --> 00:26:23,609
our application

00:26:21,539 --> 00:26:26,249
domain these are all global particle and

00:26:23,609 --> 00:26:28,950
sell codes the most prominent global

00:26:26,249 --> 00:26:34,289
fusion energy particle and sell codes

00:26:28,950 --> 00:26:36,269
all use this methodology in dealing with

00:26:34,289 --> 00:26:38,309
in dealing with making incremental

00:26:36,269 --> 00:26:40,859
changes you can implement OpenMP

00:26:38,309 --> 00:26:43,320
parallel ism in an incremental way one

00:26:40,859 --> 00:26:45,389
section at a time without affecting the

00:26:43,320 --> 00:26:47,309
rest of the code so it's very important

00:26:45,389 --> 00:26:49,859
feature if you're trying to bring along

00:26:47,309 --> 00:26:52,499
legacy codes to be able to handle this

00:26:49,859 --> 00:26:55,529
large legacy codes the internode

00:26:52,499 --> 00:26:58,019
parallel ism OpenMP is ideal for intra

00:26:55,529 --> 00:27:01,649
note shared memory parallel ism we found

00:26:58,019 --> 00:27:04,529
so openmp is not meant to replace MPI

00:27:01,649 --> 00:27:07,950
but very hierarchical Hardware requires

00:27:04,529 --> 00:27:10,259
our hierarchical solutions so while MPI

00:27:07,950 --> 00:27:13,200
is still the de facto solution for

00:27:10,259 --> 00:27:15,629
internode communications many currently

00:27:13,200 --> 00:27:22,409
would argue that a pea gas language

00:27:15,629 --> 00:27:24,720
works better for in some cases so there

00:27:22,409 --> 00:27:26,820
is also in addition to remember I

00:27:24,720 --> 00:27:29,009
mentioned that in addition to pushing

00:27:26,820 --> 00:27:31,019
the particles in a Monte Carlo like

00:27:29,009 --> 00:27:34,019
scheme you have to self-consistently

00:27:31,019 --> 00:27:36,720
solve for the fields and this involves

00:27:34,019 --> 00:27:40,169
these pass on cells and as you try to

00:27:36,720 --> 00:27:42,029
deploy your modern code on on modern

00:27:40,169 --> 00:27:44,249
super computing platforms it's very

00:27:42,029 --> 00:27:49,470
important that the solver technology

00:27:44,249 --> 00:27:54,029
comes along for the ride and the main

00:27:49,470 --> 00:27:56,639
solver packages the main issue that that

00:27:54,029 --> 00:27:58,940
some of us have with it is the D is that

00:27:56,639 --> 00:28:03,960
they're not quite ready for dealing with

00:27:58,940 --> 00:28:05,820
the openmp necessity needs for being

00:28:03,960 --> 00:28:08,039
able to deal with multi-threading and

00:28:05,820 --> 00:28:11,299
such in the solvers examples include

00:28:08,039 --> 00:28:14,099
major code package in the u.s. like

00:28:11,299 --> 00:28:16,080
argon National Labs petsi code which is

00:28:14,099 --> 00:28:18,330
a big solver package that most of you

00:28:16,080 --> 00:28:25,619
are probably familiar with or truly

00:28:18,330 --> 00:28:28,080
knows that at Sandia but the but in more

00:28:25,619 --> 00:28:30,869
recent discussions of we believe that

00:28:28,080 --> 00:28:33,840
Lawrence Livermore labs hyper code is

00:28:30,869 --> 00:28:34,540
very suitable especially for portability

00:28:33,840 --> 00:28:38,380
purposes

00:28:34,540 --> 00:28:41,530
such so the challenge is to incorporate

00:28:38,380 --> 00:28:43,240
multigrid place on solves with openmp to

00:28:41,530 --> 00:28:45,550
efficiently deal with extreme

00:28:43,240 --> 00:28:48,220
concurrency multi-threading issues

00:28:45,550 --> 00:28:52,960
characteristics of near future systems

00:28:48,220 --> 00:28:55,090
coming up also the enhanced physics that

00:28:52,960 --> 00:28:57,610
we bring to the table you have to move

00:28:55,090 --> 00:29:00,280
beyond just the simple electrostatic

00:28:57,610 --> 00:29:03,070
field solves to fully electromagnetic

00:29:00,280 --> 00:29:05,590
solves so while you may not be investing

00:29:03,070 --> 00:29:08,860
a tremendous amount of compute time

00:29:05,590 --> 00:29:10,840
right now on just the solver part you

00:29:08,860 --> 00:29:13,810
soon will need to as you move to the

00:29:10,840 --> 00:29:15,490
next physics stage so and what we're

00:29:13,810 --> 00:29:18,270
promising in terms of physics

00:29:15,490 --> 00:29:20,560
deliverables for summit and aurorus is

00:29:18,270 --> 00:29:23,320
you really got to get moving on the

00:29:20,560 --> 00:29:26,890
solvers and you better make them openmp

00:29:23,320 --> 00:29:29,260
compatible and so we're aggressively

00:29:26,890 --> 00:29:31,290
moving forward on this right now we're

00:29:29,260 --> 00:29:36,390
involved in the early science

00:29:31,290 --> 00:29:41,230
application program at at oakridge that

00:29:36,390 --> 00:29:44,080
leads to the the summit system and the

00:29:41,230 --> 00:29:46,450
so our particular project we're one of

00:29:44,080 --> 00:29:48,580
the two fusion projects that were

00:29:46,450 --> 00:29:52,810
selected out of about a dozen for the

00:29:48,580 --> 00:29:54,490
early science 44 summit and this

00:29:52,810 --> 00:29:57,490
involves a partnership between our

00:29:54,490 --> 00:30:00,730
project and Princeton using this gyro

00:29:57,490 --> 00:30:03,130
kinetic toroidal code Princeton together

00:30:00,730 --> 00:30:06,340
with our what we call our flagship large

00:30:03,130 --> 00:30:08,860
more legacy oriented code that is still

00:30:06,340 --> 00:30:13,870
a very very scalable particle and cell

00:30:08,860 --> 00:30:16,240
code that is a centered at UC Irvine

00:30:13,870 --> 00:30:20,260
right now actually this was developed at

00:30:16,240 --> 00:30:23,020
Princeton professor Z Hong Lin a number

00:30:20,260 --> 00:30:25,480
of years ago he was my student and he's

00:30:23,020 --> 00:30:28,990
done very well he's a full professor in

00:30:25,480 --> 00:30:32,050
physics at UC Irvine and he's also the

00:30:28,990 --> 00:30:34,900
lead in China for an eater China

00:30:32,050 --> 00:30:37,810
simulation program so we've teamed up

00:30:34,900 --> 00:30:41,110
for this early science project at

00:30:37,810 --> 00:30:44,200
oakridge and one thing is very clear

00:30:41,110 --> 00:30:45,180
will require that the requirements for

00:30:44,200 --> 00:30:47,880
for

00:30:45,180 --> 00:30:50,550
a multigrid electromagnetic field saw

00:30:47,880 --> 00:30:54,360
with openmp such as hyper is something

00:30:50,550 --> 00:30:59,370
that we will definitely aggressively

00:30:54,360 --> 00:31:00,750
pursue other things radio domain

00:30:59,370 --> 00:31:02,910
decomposition I'm not going to read

00:31:00,750 --> 00:31:05,010
through all of this but OpenMP provides

00:31:02,910 --> 00:31:08,160
the best way to address this grid-based

00:31:05,010 --> 00:31:09,660
issue and what this involves is remember

00:31:08,160 --> 00:31:11,910
I'm trying to do what we're trying to

00:31:09,660 --> 00:31:15,720
focus on is to efficiently calculate

00:31:11,910 --> 00:31:18,450
increasing problem size and and domain

00:31:15,720 --> 00:31:20,670
decomposition means a way of more

00:31:18,450 --> 00:31:23,370
efficiently representing the the

00:31:20,670 --> 00:31:25,620
algorithm such that the memory footprint

00:31:23,370 --> 00:31:30,450
is reduced as you go to larger and

00:31:25,620 --> 00:31:32,970
larger size and and the way to make it

00:31:30,450 --> 00:31:35,310
happen is we found you have to use

00:31:32,970 --> 00:31:37,980
OpenMP provides the best way to address

00:31:35,310 --> 00:31:39,630
associated grid-based issues I don't

00:31:37,980 --> 00:31:41,790
have to I wrote this on the slide so

00:31:39,630 --> 00:31:43,410
people can read this I'm not going to

00:31:41,790 --> 00:31:46,500
read this to you right now or never get

00:31:43,410 --> 00:31:49,380
through the talk the other important

00:31:46,500 --> 00:31:52,380
area where openmp is is very important

00:31:49,380 --> 00:31:54,930
is in heterogeneous acceleration the you

00:31:52,380 --> 00:31:56,430
know the GPUs and such while offloading

00:31:54,930 --> 00:31:59,870
particle-based subroutines for

00:31:56,430 --> 00:32:03,000
acceleration on GPUs or on Z on thighs

00:31:59,870 --> 00:32:05,420
for acceleration purposes usually a

00:32:03,000 --> 00:32:09,450
single MPI is launched on the host or

00:32:05,420 --> 00:32:11,310
CPU for a grid-based work openmp enables

00:32:09,450 --> 00:32:13,440
multithreading capability more readily

00:32:11,310 --> 00:32:16,170
to avoid performance degradation of the

00:32:13,440 --> 00:32:18,720
host as we shift towards multi many

00:32:16,170 --> 00:32:21,150
cores on today's architectures this

00:32:18,720 --> 00:32:23,970
you're probably quite aware of and puts

00:32:21,150 --> 00:32:27,060
a greater emphasis on the utility of

00:32:23,970 --> 00:32:30,390
OpenMP and as other talks of emphasized

00:32:27,060 --> 00:32:33,060
during the last day or so openmp is not

00:32:30,390 --> 00:32:35,970
going away it's it's really very very

00:32:33,060 --> 00:32:38,940
usable and it doesn't mean that you hit

00:32:35,970 --> 00:32:40,890
a ceiling in terms of improving but it's

00:32:38,940 --> 00:32:43,320
certainly something that will be

00:32:40,890 --> 00:32:45,720
featured and the application domains as

00:32:43,320 --> 00:32:48,720
we go forward this speaks this this

00:32:45,720 --> 00:32:51,750
graph is an illustration of our code and

00:32:48,720 --> 00:32:54,360
Princeton gtc its portability this shows

00:32:51,750 --> 00:32:57,340
how much of the top seven supercomputers

00:32:54,360 --> 00:33:00,260
around the world we've

00:32:57,340 --> 00:33:04,460
we've to what level they've been engaged

00:33:00,260 --> 00:33:08,059
and we this is the blue jean cue system

00:33:04,460 --> 00:33:10,580
at Lawrence Livermore Lab we were given

00:33:08,059 --> 00:33:12,830
access to that and we ran the thing at

00:33:10,580 --> 00:33:16,159
full scale of course it on mirror this

00:33:12,830 --> 00:33:18,700
is blue jean q k computer through the g8

00:33:16,159 --> 00:33:21,080
project we basically replicated all the

00:33:18,700 --> 00:33:23,720
performance scaling properties just

00:33:21,080 --> 00:33:25,820
going up to about forty percent of the

00:33:23,720 --> 00:33:31,130
machine and then the others are here

00:33:25,820 --> 00:33:33,470
including stampede and 1002 these are

00:33:31,130 --> 00:33:35,840
the big Xeon Phi systems and as I note

00:33:33,470 --> 00:33:38,169
here the progress that we've made on

00:33:35,840 --> 00:33:43,370
this has been hampered by the fact that

00:33:38,169 --> 00:33:45,889
both of these the the experts at these

00:33:43,370 --> 00:33:50,510
facilities have said you know if you

00:33:45,889 --> 00:33:54,529
used if you use a symmetric approach

00:33:50,510 --> 00:33:56,029
symmetric algorithmic approach the and

00:33:54,529 --> 00:33:59,210
i'll talk more about that in a little

00:33:56,029 --> 00:34:01,909
bit your you scale just fine just with a

00:33:59,210 --> 00:34:04,490
c language program up to about a

00:34:01,909 --> 00:34:06,380
thousand processors but beyond that you

00:34:04,490 --> 00:34:08,599
have to go into the so-called offload

00:34:06,380 --> 00:34:11,569
mode where the communications challenges

00:34:08,599 --> 00:34:14,780
are really tough and the efficiency

00:34:11,569 --> 00:34:18,020
becomes much more difficult and so we

00:34:14,780 --> 00:34:19,940
haven't been able so we went up to about

00:34:18,020 --> 00:34:22,190
fifty percent of the capability of the

00:34:19,940 --> 00:34:25,220
number one machine teanna to but this is

00:34:22,190 --> 00:34:28,159
all cpus ok and I'll talk a bit more

00:34:25,220 --> 00:34:31,490
about that and the dark have hatches and

00:34:28,159 --> 00:34:35,510
indicate how many use nodes and how many

00:34:31,490 --> 00:34:37,220
unused notes there are and this is just

00:34:35,510 --> 00:34:39,619
a quick reminder you know I showed you

00:34:37,220 --> 00:34:41,300
that other result as you go to larger

00:34:39,619 --> 00:34:44,089
and larger machines the confinement

00:34:41,300 --> 00:34:50,629
degradation this is tackling a different

00:34:44,089 --> 00:34:51,919
type of physics instability process that

00:34:50,629 --> 00:34:54,560
is more difficult because you're

00:34:51,919 --> 00:34:58,190
tracking the electron dynamics which

00:34:54,560 --> 00:35:01,359
move faster so the use of accelerators

00:34:58,190 --> 00:35:04,609
and things is really important but to

00:35:01,359 --> 00:35:07,220
cut to the chase these are new results

00:35:04,609 --> 00:35:10,150
that we achieved in the last three

00:35:07,220 --> 00:35:13,270
months or so and we can replicate this

00:35:10,150 --> 00:35:15,240
and replicate this plateauing as you go

00:35:13,270 --> 00:35:19,000
to larger and larger size systems and

00:35:15,240 --> 00:35:22,119
these are real heroes size calculations

00:35:19,000 --> 00:35:24,490
because you really had to use the full

00:35:22,119 --> 00:35:28,299
power the modern supercomputers to

00:35:24,490 --> 00:35:30,369
achieve these results and so getting

00:35:28,299 --> 00:35:33,039
back to the code properties there's six

00:35:30,369 --> 00:35:35,650
major subroutines in a particle global

00:35:33,039 --> 00:35:37,900
particle and sell codes and these are

00:35:35,650 --> 00:35:41,440
listed here again we talked about the

00:35:37,900 --> 00:35:43,359
Charlie gathers scatter and this is

00:35:41,440 --> 00:35:47,440
something that's very important in our

00:35:43,359 --> 00:35:49,779
in our interactions with the performance

00:35:47,440 --> 00:35:52,510
performance modeling community in this

00:35:49,779 --> 00:35:53,980
area because when they take when they

00:35:52,510 --> 00:35:56,380
look at the properties of new

00:35:53,980 --> 00:35:58,720
architectures that are coming up it's

00:35:56,380 --> 00:36:01,450
very hard to focus down focus down on a

00:35:58,720 --> 00:36:04,230
few functions and things for this class

00:36:01,450 --> 00:36:07,450
of particle and sell codes there's only

00:36:04,230 --> 00:36:10,240
you know half dozen or so here and so

00:36:07,450 --> 00:36:15,130
when I talked to torsten Hoffler who's

00:36:10,240 --> 00:36:18,730
the who's the head of the of the Intel

00:36:15,130 --> 00:36:20,500
center of excellence at in Zurich he

00:36:18,730 --> 00:36:22,089
says this is great and so we're working

00:36:20,500 --> 00:36:24,880
with him right now I'm being able to

00:36:22,089 --> 00:36:28,049
look at the future architectures but

00:36:24,880 --> 00:36:31,539
focusing on a limited number of

00:36:28,049 --> 00:36:35,109
operations and this really facilitates

00:36:31,539 --> 00:36:39,130
progress and so the idea is if you do

00:36:35,109 --> 00:36:42,010
performance modeling on on a particular

00:36:39,130 --> 00:36:44,319
architecture and you can translate that

00:36:42,010 --> 00:36:46,990
to a smaller number of operations like

00:36:44,319 --> 00:36:50,880
this then you can very quickly jump into

00:36:46,990 --> 00:36:53,799
the actual real application code and so

00:36:50,880 --> 00:36:56,049
so the people involved in performance

00:36:53,799 --> 00:36:58,359
modeling are very interested in this

00:36:56,049 --> 00:37:00,460
type of approach this I'm not going to

00:36:58,359 --> 00:37:02,049
dwell on very this shows the week

00:37:00,460 --> 00:37:05,559
scaling results on various

00:37:02,049 --> 00:37:09,099
supercomputers Titan blue waters mirror

00:37:05,559 --> 00:37:10,809
etc etc and these what are these things

00:37:09,099 --> 00:37:14,380
these are the particle and sell

00:37:10,809 --> 00:37:16,390
operations that I talked about and this

00:37:14,380 --> 00:37:20,650
is the breakdown of performance in terms

00:37:16,390 --> 00:37:22,930
of just how much time these these

00:37:20,650 --> 00:37:26,829
different operations use

00:37:22,930 --> 00:37:29,859
a cleaner version is this just focusing

00:37:26,829 --> 00:37:32,250
on a few here this this shows cpu only

00:37:29,859 --> 00:37:36,040
performance this is the wall clock time

00:37:32,250 --> 00:37:38,010
/ I on time ions or you know the larger

00:37:36,040 --> 00:37:40,750
charged particle in a fusion plasma

00:37:38,010 --> 00:37:43,180
operations amira Titan and peace dad

00:37:40,750 --> 00:37:44,770
this is the number one supercomputer in

00:37:43,180 --> 00:37:48,430
Europe right now at the Swiss

00:37:44,770 --> 00:37:51,099
supercomputing Center and and then this

00:37:48,430 --> 00:37:54,940
shows the performance on Titan MPs dent

00:37:51,099 --> 00:37:57,520
on a cpu plus GPU the operational

00:37:54,940 --> 00:38:01,059
breakdown of time for step when using 80

00:37:57,520 --> 00:38:03,880
million grid points 3 8 8 billion ions

00:38:01,059 --> 00:38:05,740
eight billion electrons and 4,000 nodes

00:38:03,880 --> 00:38:09,760
on these systems and shows the relative

00:38:05,740 --> 00:38:13,210
performance and the interesting thing is

00:38:09,760 --> 00:38:15,970
if you plot it this way what this shows

00:38:13,210 --> 00:38:19,540
is that here again is the wall clock

00:38:15,970 --> 00:38:21,400
time this again are the ABCD this is as

00:38:19,540 --> 00:38:23,619
you go to larger larger problem size

00:38:21,400 --> 00:38:26,170
these are the number of nodes engaged

00:38:23,619 --> 00:38:28,270
the number of particles per cell is a

00:38:26,170 --> 00:38:30,130
hundred particles per cell typically

00:38:28,270 --> 00:38:32,650
when I showed you the other calculations

00:38:30,130 --> 00:38:36,880
the particles per cell is chosen to be

00:38:32,650 --> 00:38:38,619
around 10 or so so we're at least an

00:38:36,880 --> 00:38:42,700
order we can actually go up to a

00:38:38,619 --> 00:38:45,250
thousand if we need to so this indicates

00:38:42,700 --> 00:38:47,710
that for this code the the GPU

00:38:45,250 --> 00:38:50,440
acceleration in gives you a speed up

00:38:47,710 --> 00:38:52,660
about a factor of two but interestingly

00:38:50,440 --> 00:38:54,970
if you look at the scaling this is

00:38:52,660 --> 00:38:57,520
performance scaling and we're using the

00:38:54,970 --> 00:38:59,470
same code for all cases although we've

00:38:57,520 --> 00:39:04,000
written the code in C language and in

00:38:59,470 --> 00:39:08,849
CUDA and so the interesting thing here

00:39:04,000 --> 00:39:11,260
is that here is CPU only on on Titan and

00:39:08,849 --> 00:39:13,569
this is not good you want things to be

00:39:11,260 --> 00:39:15,849
as flat as possible as you go to larger

00:39:13,569 --> 00:39:20,020
problem size in terms of wall clock time

00:39:15,849 --> 00:39:23,470
spent doing the calculation so this is a

00:39:20,020 --> 00:39:25,329
piece that CPU only but the outstanding

00:39:23,470 --> 00:39:28,660
aspect of this is that it deploys the

00:39:25,329 --> 00:39:32,680
Ares network ok so the communications

00:39:28,660 --> 00:39:34,540
are much superior to to what is on is on

00:39:32,680 --> 00:39:36,370
Titan and blue waters

00:39:34,540 --> 00:39:37,780
and you can see it's significant

00:39:36,370 --> 00:39:40,870
difference in terms of wall clock time

00:39:37,780 --> 00:39:44,470
for execution if you net then look at

00:39:40,870 --> 00:39:46,810
the at the hybrid performance CPU plus

00:39:44,470 --> 00:39:50,710
GPU you see the wall clock time is

00:39:46,810 --> 00:39:52,960
dropped and this is this is for Titan

00:39:50,710 --> 00:39:57,820
but it still keeper keeps on going up

00:39:52,960 --> 00:40:00,400
like this this is much flattered for 4p

00:39:57,820 --> 00:40:02,140
stint and this is all due to the Ares

00:40:00,400 --> 00:40:05,830
Network we believe the performance

00:40:02,140 --> 00:40:08,380
difference is is very reproducible this

00:40:05,830 --> 00:40:11,170
way and it emphasizes that for particle

00:40:08,380 --> 00:40:13,780
and sell codes that this the network is

00:40:11,170 --> 00:40:17,380
important and it speaks to now if you

00:40:13,780 --> 00:40:19,320
talk about co-design activities on the

00:40:17,380 --> 00:40:21,790
summit project now you have different

00:40:19,320 --> 00:40:24,760
partners right and now you have IBM

00:40:21,790 --> 00:40:26,800
together with with NVIDIA they you know

00:40:24,760 --> 00:40:30,460
they've done a major switch here and

00:40:26,800 --> 00:40:35,650
who's providing the communications Ares

00:40:30,460 --> 00:40:38,110
was developed by by Craig and deployed

00:40:35,650 --> 00:40:42,340
successfully on peace dent it's not on

00:40:38,110 --> 00:40:44,410
any other machine in the US and it does

00:40:42,340 --> 00:40:46,360
make a significant difference so is

00:40:44,410 --> 00:40:48,460
mellanox going to step up to the plate

00:40:46,360 --> 00:40:50,350
and give you the communications that you

00:40:48,460 --> 00:40:53,920
need are you going to really be able to

00:40:50,350 --> 00:40:56,770
ramp up the the communications

00:40:53,920 --> 00:40:59,190
capability commensurate with this

00:40:56,770 --> 00:41:02,770
hundred petaflop project we don't know

00:40:59,190 --> 00:41:04,930
but that's a major challenge so this

00:41:02,770 --> 00:41:07,660
shows I've been showing you weak scaling

00:41:04,930 --> 00:41:09,670
which shows which is more indicative of

00:41:07,660 --> 00:41:11,830
what happens as you increase the problem

00:41:09,670 --> 00:41:14,170
sighs this is strong scaling which

00:41:11,830 --> 00:41:18,720
basically says if you for a fixed

00:41:14,170 --> 00:41:21,840
problem size a big one you can really

00:41:18,720 --> 00:41:25,060
carry out more and more intensive

00:41:21,840 --> 00:41:29,110
studies and we've done this with the

00:41:25,060 --> 00:41:31,300
full electron dynamics and looked at 80

00:41:29,110 --> 00:41:33,520
million grid points 8 billion ions and

00:41:31,300 --> 00:41:36,220
this is the full big production run

00:41:33,520 --> 00:41:38,710
codes this is plotted on the log log

00:41:36,220 --> 00:41:42,870
plot and it shows this so what's the

00:41:38,710 --> 00:41:46,060
takeaway from this blue is the CPU large

00:41:42,870 --> 00:41:50,080
IBM IBM

00:41:46,060 --> 00:41:54,090
blue jean cue system titan is the hybrid

00:41:50,080 --> 00:41:57,040
GPU cpu system and this is peace 10 and

00:41:54,090 --> 00:42:00,340
again this is wall clock time so lower

00:41:57,040 --> 00:42:03,610
is better and here's the here's mirror

00:42:00,340 --> 00:42:07,300
and it does you know very very well you

00:42:03,610 --> 00:42:11,680
know as you go to up to 32,000 33,000

00:42:07,300 --> 00:42:15,340
processors titan doesn't do so well it I

00:42:11,680 --> 00:42:17,710
mean it's faster because of the GPUs but

00:42:15,340 --> 00:42:19,900
here as you go to many many more nodes

00:42:17,710 --> 00:42:22,510
the communications make a difference and

00:42:19,900 --> 00:42:25,360
you see it roll over like this but peace

00:42:22,510 --> 00:42:28,270
Dan is very steady it's not a as a

00:42:25,360 --> 00:42:30,220
largest system as either mirror or Titan

00:42:28,270 --> 00:42:33,640
but for the plots that we're showing

00:42:30,220 --> 00:42:37,510
here it does very very well at over

00:42:33,640 --> 00:42:39,670
4,000 nodes so this is a important

00:42:37,510 --> 00:42:42,810
lesson learned from this this is a more

00:42:39,670 --> 00:42:47,020
dense plot but it shows our full effort

00:42:42,810 --> 00:42:49,390
that was achieved a few months ago we

00:42:47,020 --> 00:42:54,400
weren't able to do very much with

00:42:49,390 --> 00:42:56,890
respect to Ken huh to CPU only but you

00:42:54,400 --> 00:42:58,750
can see that's the plot here wall clock

00:42:56,890 --> 00:43:00,370
time it's the fastest because you can

00:42:58,750 --> 00:43:02,980
really employ you know it's very

00:43:00,370 --> 00:43:05,470
powerful machine but again peace dan

00:43:02,980 --> 00:43:07,900
holds its own very well up to four

00:43:05,470 --> 00:43:10,450
thousand or so processors and going

00:43:07,900 --> 00:43:12,190
beyond this even for peace dad you

00:43:10,450 --> 00:43:14,560
really had to go to the offload mode and

00:43:12,190 --> 00:43:17,740
we're not ready for that or we haven't

00:43:14,560 --> 00:43:20,130
developed enough efficiency and some of

00:43:17,740 --> 00:43:22,930
the things that we need to discuss is in

00:43:20,130 --> 00:43:25,720
some of the comments with people after

00:43:22,930 --> 00:43:27,970
their presentations yesterday we really

00:43:25,720 --> 00:43:30,850
would like to get more insights into how

00:43:27,970 --> 00:43:33,280
to improve performance on the Xeon Phi

00:43:30,850 --> 00:43:36,040
systems and knights landing in

00:43:33,280 --> 00:43:38,410
particular nice landing for going into

00:43:36,040 --> 00:43:40,930
the future but Knights corner as it

00:43:38,410 --> 00:43:43,300
exists right now hungarian company all

00:43:40,930 --> 00:43:45,580
say look 1002 is going to remain the

00:43:43,300 --> 00:43:47,200
number one machine for the foreseeable

00:43:45,580 --> 00:43:49,450
future for the next three or four years

00:43:47,200 --> 00:43:52,240
at least until the other machines start

00:43:49,450 --> 00:43:54,400
to start to roll out and so we still

00:43:52,240 --> 00:43:57,960
need to demonstrate how well can you

00:43:54,400 --> 00:44:00,180
carry out an application

00:43:57,960 --> 00:44:02,339
on the number one machine in the world

00:44:00,180 --> 00:44:05,430
people are looking at this and so we

00:44:02,339 --> 00:44:11,460
need help in terms of more efficient

00:44:05,430 --> 00:44:14,420
deployment of length of like openmp 44.1

00:44:11,460 --> 00:44:16,710
that's ready for prime time we'd like to

00:44:14,420 --> 00:44:18,150
work in partnership with people that

00:44:16,710 --> 00:44:21,990
really know what they're doing in this

00:44:18,150 --> 00:44:24,570
area this shows the the other the all of

00:44:21,990 --> 00:44:26,700
the other different devices I'll let you

00:44:24,570 --> 00:44:30,060
look at this but that's the main message

00:44:26,700 --> 00:44:32,310
and these show again these are big

00:44:30,060 --> 00:44:36,030
heroes sorts of runs for fixed problem

00:44:32,310 --> 00:44:39,869
size and let me get to this issue

00:44:36,030 --> 00:44:43,170
because we feel pretty confident that

00:44:39,869 --> 00:44:46,320
we're doing pretty well on the more

00:44:43,170 --> 00:44:49,380
conventional well not more conventional

00:44:46,320 --> 00:44:51,780
but certainly on the on a homogeneous

00:44:49,380 --> 00:44:53,780
CPU system with the multi-threading

00:44:51,780 --> 00:44:58,650
we've done very very well with that on

00:44:53,780 --> 00:45:03,300
the GPU cpu hybrids like Titan piece

00:44:58,650 --> 00:45:06,930
dent we performed pretty well and the

00:45:03,300 --> 00:45:10,200
but the challenges is with the Zeon faiz

00:45:06,930 --> 00:45:15,869
so what have we done so far in our

00:45:10,200 --> 00:45:18,859
collaborative studies with tanta to we

00:45:15,869 --> 00:45:21,960
began by measuring the MPI bandwidth

00:45:18,859 --> 00:45:23,670
using the Intel MPI benchmark this

00:45:21,960 --> 00:45:29,550
basically gives you a feeling for what

00:45:23,670 --> 00:45:31,920
the cpu to cpu host properties are Mike

00:45:29,550 --> 00:45:34,650
2 Mike the native mode CPU to Mike

00:45:31,920 --> 00:45:36,240
symmetric operation and that gave us a

00:45:34,650 --> 00:45:40,589
feel for what was going on but very

00:45:36,240 --> 00:45:45,050
quickly so using the C version of this

00:45:40,589 --> 00:45:47,400
of our main code we're we were able to

00:45:45,050 --> 00:45:51,800
operate in the symmetric mode very

00:45:47,400 --> 00:45:56,490
easily until we try to engage the the

00:45:51,800 --> 00:45:58,500
Knights corner Xeon Phi s and then we

00:45:56,490 --> 00:46:01,020
ran into this issue with the offload

00:45:58,500 --> 00:46:05,960
mode version which needs to be developed

00:46:01,020 --> 00:46:09,750
to address many mics on one compute

00:46:05,960 --> 00:46:12,060
associated investigations include

00:46:09,750 --> 00:46:14,370
true week scaling performances we talked

00:46:12,060 --> 00:46:17,430
about with increasing problem size we'd

00:46:14,370 --> 00:46:20,250
love to do that on a huge system like

00:46:17,430 --> 00:46:23,430
Canada to starting from the smallest

00:46:20,250 --> 00:46:25,740
size up to the eater sighs we've done

00:46:23,430 --> 00:46:30,210
this successfully for the CPUs up to

00:46:25,740 --> 00:46:33,870
over 8,000 notes on 1002 but this is the

00:46:30,210 --> 00:46:36,090
big challenge the deployment of one Mike

00:46:33,870 --> 00:46:37,860
2 Mike and three Mike's respectively for

00:46:36,090 --> 00:46:41,790
these weak scaling performances we're

00:46:37,860 --> 00:46:44,190
still working at it now what about

00:46:41,790 --> 00:46:48,900
stampede that's the largest Intel Xeon

00:46:44,190 --> 00:46:53,160
Phi system in the u.s. right now the the

00:46:48,900 --> 00:46:54,540
this is again Knights corner and and we

00:46:53,160 --> 00:46:56,610
want to work on this because it's a

00:46:54,540 --> 00:47:00,210
natural lead-in for us to be able to

00:46:56,610 --> 00:47:02,760
then deploy on tanta to the number one

00:47:00,210 --> 00:47:05,400
machine and we have a good collaborative

00:47:02,760 --> 00:47:08,730
in a good collaborative relationship

00:47:05,400 --> 00:47:11,550
with this is the NSF's major Center and

00:47:08,730 --> 00:47:14,790
they're going to also upgrade to

00:47:11,550 --> 00:47:18,420
stampede to which is a knights landing

00:47:14,790 --> 00:47:22,260
system very similar to Cory at Lawrence

00:47:18,420 --> 00:47:26,430
Berkeley Lab and very similar to theta

00:47:22,260 --> 00:47:29,910
that's the machine that will lead to I

00:47:26,430 --> 00:47:32,910
think within the next year or so they're

00:47:29,910 --> 00:47:35,820
going to identify what the early science

00:47:32,910 --> 00:47:37,080
projects are for Aurora but theta is the

00:47:35,820 --> 00:47:41,220
machine that they're going to deploy

00:47:37,080 --> 00:47:43,260
first but it's not a major step up yet I

00:47:41,220 --> 00:47:47,700
mean with respect to existing machines

00:47:43,260 --> 00:47:50,520
as I said so in the US the comparable

00:47:47,700 --> 00:47:55,970
scale machines will be the Stampede two

00:47:50,520 --> 00:47:58,020
knights landing system at in Texas the

00:47:55,970 --> 00:47:59,880
quarry system at Lawrence Livermore

00:47:58,020 --> 00:48:01,950
which would be the first out the gate

00:47:59,880 --> 00:48:07,710
and we'll probably hear more from Helen

00:48:01,950 --> 00:48:10,230
about that and the and then the theta

00:48:07,710 --> 00:48:14,300
device which which is will be soon

00:48:10,230 --> 00:48:17,670
deployed at at argonne national lab so

00:48:14,300 --> 00:48:22,550
so in our interactions with with the

00:48:17,670 --> 00:48:24,200
people at the NSF Center in Texas

00:48:22,550 --> 00:48:25,820
we're trying to improve intra node

00:48:24,200 --> 00:48:28,400
communication between the host and the

00:48:25,820 --> 00:48:30,830
mics to reduce the overhead and the MPI

00:48:28,400 --> 00:48:33,440
scatter operation and this is again

00:48:30,830 --> 00:48:37,580
focusing on specific operations within

00:48:33,440 --> 00:48:39,830
our modern code improve internode

00:48:37,580 --> 00:48:42,500
communications between the mics for part

00:48:39,830 --> 00:48:45,080
the particle shift operation one of the

00:48:42,500 --> 00:48:48,020
six that I listed optimize particle

00:48:45,080 --> 00:48:50,930
loading for symmetric runs explore KNC

00:48:48,020 --> 00:48:53,510
nights corner intrinsic initiate

00:48:50,930 --> 00:48:57,170
deployment of OpenMP 4.1 when that

00:48:53,510 --> 00:48:59,600
becomes available this is a real urgent

00:48:57,170 --> 00:49:01,550
wish list item that we really want to

00:48:59,600 --> 00:49:03,710
move forward on and it's really been

00:49:01,550 --> 00:49:06,050
nice being here to hear what people are

00:49:03,710 --> 00:49:08,390
talking about the Intel experts and it's

00:49:06,050 --> 00:49:10,700
very reassuring to hear them say oh yeah

00:49:08,390 --> 00:49:12,410
yeah this is deployable now you know if

00:49:10,700 --> 00:49:16,520
you work at it but what are the

00:49:12,410 --> 00:49:18,290
platforms you can try it out on I was

00:49:16,520 --> 00:49:19,520
talking to Helen last night and she says

00:49:18,290 --> 00:49:22,730
well you know you have to have the

00:49:19,520 --> 00:49:25,820
platforms that they can actually that

00:49:22,730 --> 00:49:29,510
you can actually once you modify your

00:49:25,820 --> 00:49:31,340
code to use OpenMP 4.1 there has to be

00:49:29,510 --> 00:49:33,860
some platforms that you can access

00:49:31,340 --> 00:49:36,980
earlier than later and we'd love to do

00:49:33,860 --> 00:49:39,770
that so more actively into the next

00:49:36,980 --> 00:49:42,620
phase we need to move actively into the

00:49:39,770 --> 00:49:44,210
next phase of the true week scaling

00:49:42,620 --> 00:49:50,990
performance studies with increasing

00:49:44,210 --> 00:49:53,980
problem size before we engage tinha to

00:49:50,990 --> 00:49:56,860
this big system and you know although

00:49:53,980 --> 00:49:59,090
professor liu yu Tong Lu has been

00:49:56,860 --> 00:50:01,900
tremendously collaborative with us and i

00:49:59,090 --> 00:50:04,010
had her on my author list of items here

00:50:01,900 --> 00:50:09,380
she's the pretty one of the lead

00:50:04,010 --> 00:50:12,680
designers in china for both Ken ona and

00:50:09,380 --> 00:50:15,920
Tina to and and she's very interested

00:50:12,680 --> 00:50:18,650
these days in identifying more and more

00:50:15,920 --> 00:50:20,570
performance application codes because by

00:50:18,650 --> 00:50:25,010
the way there's an imperative from the

00:50:20,570 --> 00:50:27,170
leader of China Xi Jinping who but it

00:50:25,010 --> 00:50:30,500
got his degree in EE from Ching hua and

00:50:27,170 --> 00:50:33,250
so he's not a typical politician he

00:50:30,500 --> 00:50:36,049
knows what he what he speaks in terms of

00:50:33,250 --> 00:50:37,969
Technology technological advances

00:50:36,049 --> 00:50:40,699
and he's made it clear to the leadership

00:50:37,969 --> 00:50:43,849
HBC leadership in China that were very

00:50:40,699 --> 00:50:46,150
proud of you for producing the

00:50:43,849 --> 00:50:48,829
number-one supercomputer in the world

00:50:46,150 --> 00:50:51,439
two different architectures within four

00:50:48,829 --> 00:50:54,140
years but now we want to know what good

00:50:51,439 --> 00:50:55,969
is this for the country so the

00:50:54,140 --> 00:50:58,219
application is imperative is going to be

00:50:55,969 --> 00:51:00,949
very very strong there and they're

00:50:58,219 --> 00:51:05,449
driven to focus more attention on these

00:51:00,949 --> 00:51:07,759
things so this is this this speaks to

00:51:05,449 --> 00:51:09,589
what I mentioned energy to solution

00:51:07,759 --> 00:51:11,959
estimates if you do big production run

00:51:09,589 --> 00:51:13,880
codes which which we can do on different

00:51:11,959 --> 00:51:16,400
platforms the natural question to then

00:51:13,880 --> 00:51:18,229
ask is how much energy are you expending

00:51:16,400 --> 00:51:20,989
and going from one machine to another

00:51:18,229 --> 00:51:23,630
and so we looked at with the cooperation

00:51:20,989 --> 00:51:25,309
of the Centers this is argon this is Oak

00:51:23,630 --> 00:51:27,890
Ridge and this is the Swiss

00:51:25,309 --> 00:51:30,679
supercomputing Center where pstat the

00:51:27,890 --> 00:51:33,529
number one european machine suits we

00:51:30,679 --> 00:51:38,839
looked at performance on for 40 100

00:51:33,529 --> 00:51:42,969
nodes cpu only cpu plus GPU and here's

00:51:38,839 --> 00:51:46,160
what to keep track of how do you the the

00:51:42,969 --> 00:51:49,609
the power expended per node this is the

00:51:46,160 --> 00:51:54,019
time to solution here this is this is on

00:51:49,609 --> 00:51:58,449
Mira Titan and peace dat and the if you

00:51:54,019 --> 00:52:01,449
run it cpu only again because of the

00:51:58,449 --> 00:52:04,609
communications through the ares network

00:52:01,449 --> 00:52:07,779
p stand does very very well relative to

00:52:04,609 --> 00:52:11,809
the others the energy consumption also

00:52:07,779 --> 00:52:15,519
mira does very very well compared to the

00:52:11,809 --> 00:52:19,839
others and but now if you go to the

00:52:15,519 --> 00:52:22,910
hybrid system cpu + GPU for a real-world

00:52:19,839 --> 00:52:24,619
application here on a modern code what

00:52:22,910 --> 00:52:27,229
happens for the same number of nodes

00:52:24,619 --> 00:52:29,299
engage the power per node as you see

00:52:27,229 --> 00:52:32,509
goes up very understandably because

00:52:29,299 --> 00:52:36,890
you're engaging the GPUs now the time to

00:52:32,509 --> 00:52:40,939
solution also improves this is on Titan

00:52:36,890 --> 00:52:43,519
it comes down to the best achieved on P

00:52:40,939 --> 00:52:47,040
stamp of pieced and drops to about

00:52:43,519 --> 00:52:49,740
little bit over six in time in

00:52:47,040 --> 00:52:54,540
these units and the energy expended if

00:52:49,740 --> 00:52:57,000
you look comparatively this Titan if you

00:52:54,540 --> 00:52:58,890
wanted to like save you wanted to do

00:52:57,000 --> 00:53:02,040
comparative performance analysis between

00:52:58,890 --> 00:53:05,220
mirror and tighten the two big us

00:53:02,040 --> 00:53:10,910
machines this gives you better time to

00:53:05,220 --> 00:53:16,020
solution but you also have expended

00:53:10,910 --> 00:53:23,490
significantly more energy and so like

00:53:16,020 --> 00:53:25,500
right here and so but peace dad performs

00:53:23,490 --> 00:53:28,980
quite well as you can see this is

00:53:25,500 --> 00:53:32,310
comparable to what when mirror was able

00:53:28,980 --> 00:53:35,250
to do in a CPU only homogeneous system

00:53:32,310 --> 00:53:37,910
so naturally the swiss supercomputing

00:53:35,250 --> 00:53:41,370
center is very happy with his results

00:53:37,910 --> 00:53:43,050
this is not exhaustive very few

00:53:41,370 --> 00:53:45,630
applications are doing this right now

00:53:43,050 --> 00:53:48,660
but because of the portability that we

00:53:45,630 --> 00:53:51,570
were able to achieve and fortunately

00:53:48,660 --> 00:53:53,340
with the access that we do have to these

00:53:51,570 --> 00:53:55,530
different top supercomputers you can

00:53:53,340 --> 00:53:57,540
carry out this sort of thing and the

00:53:55,530 --> 00:53:59,670
co.design people are very interested in

00:53:57,540 --> 00:54:01,410
fact we could not have begun to do these

00:53:59,670 --> 00:54:03,630
measurements of the energy consumption

00:54:01,410 --> 00:54:05,340
without the full cooperation of the

00:54:03,630 --> 00:54:08,340
center's and their engineers and such

00:54:05,340 --> 00:54:10,110
because these power energy estimates

00:54:08,340 --> 00:54:11,910
were obtained from system

00:54:10,110 --> 00:54:14,700
instrumentation including compute nodes

00:54:11,910 --> 00:54:16,440
network blades AC to DC conversion

00:54:14,700 --> 00:54:19,410
everything under the Sun that I don't

00:54:16,440 --> 00:54:22,200
know in any kind of detail but they

00:54:19,410 --> 00:54:24,510
worked with us for concentrated period

00:54:22,200 --> 00:54:26,070
to help generate these results and I

00:54:24,510 --> 00:54:28,620
think there's going to be more and more

00:54:26,070 --> 00:54:31,230
of this as we go forward to explore the

00:54:28,620 --> 00:54:33,990
other platforms we don't know yet you

00:54:31,230 --> 00:54:35,940
notice prominently missing because we

00:54:33,990 --> 00:54:39,090
didn't achieve the performance that we

00:54:35,940 --> 00:54:41,780
wanted on the Xeon Phi nights corner

00:54:39,090 --> 00:54:44,700
systems and so once that's done

00:54:41,780 --> 00:54:47,640
professor Liu at tanah to said yeah we

00:54:44,700 --> 00:54:51,870
absolutely can also come up with the

00:54:47,640 --> 00:54:54,750
measurements of the energy solution on

00:54:51,870 --> 00:54:57,810
tanta to now why do I mention this

00:54:54,750 --> 00:54:59,859
because as I said the proper metrics to

00:54:57,810 --> 00:55:01,839
consider for applications is

00:54:59,859 --> 00:55:04,089
in addition to performance portability

00:55:01,839 --> 00:55:06,640
performance performance scaling and so

00:55:04,089 --> 00:55:08,769
forth is the metrics are time to

00:55:06,640 --> 00:55:11,440
solution how long did it take you to

00:55:08,769 --> 00:55:13,960
carry out a hero scale calculation and

00:55:11,440 --> 00:55:16,119
then how much energy did it use and then

00:55:13,960 --> 00:55:18,549
you do an economic balance between the

00:55:16,119 --> 00:55:21,309
two how important is it to you that the

00:55:18,549 --> 00:55:23,049
application runs quickly and then how

00:55:21,309 --> 00:55:26,069
much are you willing to spend in terms

00:55:23,049 --> 00:55:28,690
of energy to do it and this is a

00:55:26,069 --> 00:55:30,789
interesting illustrative example in this

00:55:28,690 --> 00:55:33,700
direction and whether you like it or not

00:55:30,789 --> 00:55:36,519
it's the sort of thing that the large

00:55:33,700 --> 00:55:39,910
facilities are going to be dealing with

00:55:36,519 --> 00:55:41,650
going forward so this is another thing

00:55:39,910 --> 00:55:43,869
that came up Rick Stevens at argonne

00:55:41,650 --> 00:55:46,180
national lab said well how much work did

00:55:43,869 --> 00:55:49,569
you really have to put into making the

00:55:46,180 --> 00:55:51,730
codes portable you know what did you how

00:55:49,569 --> 00:55:54,009
much time did you spend on these various

00:55:51,730 --> 00:55:57,400
things so we came up with this little

00:55:54,009 --> 00:56:00,970
plot this is for our most advanced

00:55:57,400 --> 00:56:05,319
simulations and such this is CPU this is

00:56:00,970 --> 00:56:07,930
if you add the GPU GPU capability and

00:56:05,319 --> 00:56:11,799
then this is for the Xeon Phi sort of

00:56:07,930 --> 00:56:15,309
thing these are two different operations

00:56:11,799 --> 00:56:19,390
that I mentioned within our global pick

00:56:15,309 --> 00:56:22,269
code CPUs of course is the baseline

00:56:19,390 --> 00:56:26,710
lines of code this is means the change

00:56:22,269 --> 00:56:31,029
in the lines of code this you didn't

00:56:26,710 --> 00:56:33,489
have to change so so for a factor of

00:56:31,029 --> 00:56:35,769
almost five speed up with a GPU you

00:56:33,489 --> 00:56:39,539
changed about a little bit over 700

00:56:35,769 --> 00:56:42,640
lines it's not a very complicated

00:56:39,539 --> 00:56:45,670
cumbersome code like a lot of the CFD

00:56:42,640 --> 00:56:48,700
codes are and but to do the Xeon Phi

00:56:45,670 --> 00:56:50,950
thing we were not very successful what

00:56:48,700 --> 00:56:53,710
we've done so far the speed up is

00:56:50,950 --> 00:56:55,809
actually a detriment right now and and

00:56:53,710 --> 00:56:57,369
so we need to improve that and the

00:56:55,809 --> 00:57:01,239
change of the lines of code is about

00:56:57,369 --> 00:57:04,359
that now for this other operation this

00:57:01,239 --> 00:57:08,049
is just one of two key operations in the

00:57:04,359 --> 00:57:09,880
pic code we do this comparative study of

00:57:08,049 --> 00:57:12,369
the lines of code that you have to

00:57:09,880 --> 00:57:12,920
modify just to give you a quantitative

00:57:12,369 --> 00:57:14,960
measure

00:57:12,920 --> 00:57:17,750
or some level of effort and such and

00:57:14,960 --> 00:57:19,490
doing all of this the port and optimize

00:57:17,750 --> 00:57:22,100
our modern code to a specific

00:57:19,490 --> 00:57:24,520
architecture and we're encouraged even

00:57:22,100 --> 00:57:28,070
though with things like this because

00:57:24,520 --> 00:57:31,250
when we first began working with the

00:57:28,070 --> 00:57:33,310
computer science experts at in Kathy

00:57:31,250 --> 00:57:37,880
Alex group at Lawrence Berkeley Lab

00:57:33,310 --> 00:57:40,400
people like Sam Williams and and and the

00:57:37,880 --> 00:57:43,340
other people and their team the first

00:57:40,400 --> 00:57:47,060
version of because we started out with a

00:57:43,340 --> 00:57:49,700
fortran 90 version of GT c and g tcp and

00:57:47,060 --> 00:57:52,840
they rewrote they worked with us to

00:57:49,700 --> 00:57:56,270
rewrite the code in c language and then

00:57:52,840 --> 00:57:58,670
and the first version of that was

00:57:56,270 --> 00:58:00,680
terrible you know we compared that and

00:57:58,670 --> 00:58:03,710
actually ran out on real systems the

00:58:00,680 --> 00:58:06,910
fortran code was winning hands down but

00:58:03,710 --> 00:58:08,690
then we stuck with it continued to in a

00:58:06,910 --> 00:58:10,820
interdisciplinary way continue to

00:58:08,690 --> 00:58:12,800
collaborate with them and then it just

00:58:10,820 --> 00:58:14,570
beat the pants off the fortran 90 code

00:58:12,800 --> 00:58:16,940
because we were able to very efficiently

00:58:14,570 --> 00:58:18,260
deploy the multi threading the domain

00:58:16,940 --> 00:58:20,450
decomposition and all these other

00:58:18,260 --> 00:58:24,140
properties that were able to execute

00:58:20,450 --> 00:58:26,480
very in a very timely way you know

00:58:24,140 --> 00:58:27,980
working with the more accessible

00:58:26,480 --> 00:58:31,100
language you have to speak the language

00:58:27,980 --> 00:58:32,480
there the computer science people that

00:58:31,100 --> 00:58:35,120
were interacting with they're not going

00:58:32,480 --> 00:58:37,610
to stop and translate everything to

00:58:35,120 --> 00:58:39,110
fortran for you so that was very

00:58:37,610 --> 00:58:40,940
efficient and so we're similarly

00:58:39,110 --> 00:58:43,190
encouraged that when you make your first

00:58:40,940 --> 00:58:44,810
attempts at these sorts of things you're

00:58:43,190 --> 00:58:46,400
going to have your nose is bloodied a

00:58:44,810 --> 00:58:49,450
little bit and you just have to stick

00:58:46,400 --> 00:58:56,240
with it and and the fact that we've been

00:58:49,450 --> 00:59:00,740
doing well overall big picture wise is

00:58:56,240 --> 00:59:02,690
quite encouraging so so we so we're

00:59:00,740 --> 00:59:06,110
looking at these operations to push and

00:59:02,690 --> 00:59:08,930
the sort operations and the code the

00:59:06,110 --> 00:59:11,120
speed up here measures that for the GPUs

00:59:08,930 --> 00:59:13,700
single node Kepler versus single Santa

00:59:11,120 --> 00:59:18,020
Sandy Bridge note the Xeon Phi single

00:59:13,700 --> 00:59:19,850
Mike versus to Sandy Bridge nodes so all

00:59:18,020 --> 00:59:25,520
right did this this i should mention

00:59:19,850 --> 00:59:26,660
that you cannot sit still in being

00:59:25,520 --> 00:59:28,940
satisfied although

00:59:26,660 --> 00:59:31,670
we're very happy with what our pic

00:59:28,940 --> 00:59:36,320
algorithms are able to do right now if

00:59:31,670 --> 00:59:39,470
you look going into the future the the

00:59:36,320 --> 00:59:41,540
the schemes that we're using are still

00:59:39,470 --> 00:59:45,350
it still can be significantly improved

00:59:41,540 --> 00:59:50,660
in terms of the locality of the of the

00:59:45,350 --> 00:59:53,750
of the algorithms deployed and and one

00:59:50,660 --> 00:59:56,480
of the ways that that if you turn to the

00:59:53,750 --> 01:00:00,160
applied math sort of area that we've

00:59:56,480 --> 01:00:04,610
also been in serious discussions with

01:00:00,160 --> 01:00:07,340
you see what like other particle and

01:00:04,610 --> 01:00:10,850
sell codes apply to laser plasma

01:00:07,340 --> 01:00:15,140
interactions that was discussed by Kevin

01:00:10,850 --> 01:00:16,580
Bowers in about 2008 he used basically

01:00:15,140 --> 01:00:19,580
although he didn't know he was using

01:00:16,580 --> 01:00:21,170
this we've had some lot of discussions

01:00:19,580 --> 01:00:23,660
with him in recent years about the

01:00:21,170 --> 01:00:26,480
mathematical foundations that went into

01:00:23,660 --> 01:00:30,680
this V pic code which was a brown

01:00:26,480 --> 01:00:34,220
breaking code that managed to run on the

01:00:30,680 --> 01:00:37,010
big machine petaflop machine at the Los

01:00:34,220 --> 01:00:39,470
Alamos the Road Runner which is infamous

01:00:37,010 --> 01:00:42,820
for being very hard to program for but

01:00:39,470 --> 01:00:45,380
Bowers did a terrific job on this and he

01:00:42,820 --> 01:00:49,600
was a finalist for the Gordon Bell and

01:00:45,380 --> 01:00:54,500
had many articles and such publish but

01:00:49,600 --> 01:00:57,320
but the basic applied math foundations

01:00:54,500 --> 01:00:59,420
or mathematics foundations is for you

01:00:57,320 --> 01:01:04,610
can write down the Hamiltonian for the

01:00:59,420 --> 01:01:07,640
system from which you can abstract the

01:01:04,610 --> 01:01:09,740
Lagrangian and then identify the action

01:01:07,640 --> 01:01:12,290
these are all you know mathematical

01:01:09,740 --> 01:01:14,630
physics descriptions but then you can

01:01:12,290 --> 01:01:17,120
carry out a variational optimization of

01:01:14,630 --> 01:01:19,610
the system of equations and then the

01:01:17,120 --> 01:01:22,910
challenge is to discretize it properly

01:01:19,610 --> 01:01:24,680
and you obtain these symplectic orbits

01:01:22,910 --> 01:01:27,290
for the particle motion what is

01:01:24,680 --> 01:01:30,220
symplectic mean symplectic just means to

01:01:27,290 --> 01:01:34,970
me that that in common layman's language

01:01:30,220 --> 01:01:36,770
that the more common sorts of time

01:01:34,970 --> 01:01:38,980
evolution time advances are like a

01:01:36,770 --> 01:01:42,070
higher order 4th order Runge

01:01:38,980 --> 01:01:45,310
methods and such those are not energy

01:01:42,070 --> 01:01:47,650
conserving so if you run them for a long

01:01:45,310 --> 01:01:50,050
long period of time you can look at the

01:01:47,650 --> 01:01:51,790
orbits and things they eventually you

01:01:50,050 --> 01:01:54,250
know lose their integrity after a while

01:01:51,790 --> 01:01:56,140
whereas symplectic orbits are like the

01:01:54,250 --> 01:01:57,460
Energizer Bunny in the commercials and

01:01:56,140 --> 01:02:02,050
it just keeps on going and going and

01:01:57,460 --> 01:02:05,350
going so so high so we've managed to

01:02:02,050 --> 01:02:07,690
look also at the so this is the full of

01:02:05,350 --> 01:02:10,240
all frequency sort of sort of approach

01:02:07,690 --> 01:02:12,820
the issues are it has limited

01:02:10,240 --> 01:02:14,470
applicability with respect to the size

01:02:12,820 --> 01:02:16,840
of the simulation region and also

01:02:14,470 --> 01:02:19,540
geometric complexity that doesn't help

01:02:16,840 --> 01:02:21,850
us with the magnetic fusion problem with

01:02:19,540 --> 01:02:24,720
the toroidal donut shaped like geometry

01:02:21,850 --> 01:02:27,160
and everything so we go back to the

01:02:24,720 --> 01:02:30,210
mathematical foundations of the

01:02:27,160 --> 01:02:33,430
geometric gyre kinetic formulation and

01:02:30,210 --> 01:02:37,660
this is my colleague at Princeton he

01:02:33,430 --> 01:02:42,850
also has a six months appointment in

01:02:37,660 --> 01:02:50,080
China he's Dean of research at at the

01:02:42,850 --> 01:02:52,060
big fusion Institute in hefei so this

01:02:50,080 --> 01:02:54,040
work provides the basic foundations for

01:02:52,060 --> 01:02:56,560
the symplectic integration of particle

01:02:54,040 --> 01:02:58,990
orbits and electromagnetic low-frequency

01:02:56,560 --> 01:03:01,300
plasma following the jar phonetic

01:02:58,990 --> 01:03:04,090
ordering which is what you still want to

01:03:01,300 --> 01:03:06,580
do the still outstanding challenges is

01:03:04,090 --> 01:03:09,010
to address the reformulation of the

01:03:06,580 --> 01:03:11,530
non-local Poisson equations because if

01:03:09,010 --> 01:03:13,900
you write down a Poisson equation that's

01:03:11,530 --> 01:03:15,790
inherently global it's not a local sort

01:03:13,900 --> 01:03:18,910
of representation so how do you

01:03:15,790 --> 01:03:22,090
represent the field equations in a

01:03:18,910 --> 01:03:24,160
non-local way and we're working on that

01:03:22,090 --> 01:03:26,619
right now but it's just I just put this

01:03:24,160 --> 01:03:29,320
up here because it says you have to

01:03:26,619 --> 01:03:31,660
always be thinking ahead you always have

01:03:29,320 --> 01:03:33,490
to worry about your seed corn activities

01:03:31,660 --> 01:03:36,190
going into the future in these

01:03:33,490 --> 01:03:37,960
application domains because to keep up

01:03:36,190 --> 01:03:39,850
with the architectural advances and

01:03:37,960 --> 01:03:42,760
things where you're going to need more

01:03:39,850 --> 01:03:44,770
you know more locality in the way that

01:03:42,760 --> 01:03:47,050
you carry out the calculations this

01:03:44,770 --> 01:03:50,500
seems like a promising approach and will

01:03:47,050 --> 01:03:52,460
be pursued so these are my concluding

01:03:50,500 --> 01:03:56,370
com

01:03:52,460 --> 01:04:00,270
so what I've tried to present to you is

01:03:56,370 --> 01:04:02,550
a is a illustration of a modern high

01:04:00,270 --> 01:04:05,190
performance computing domain application

01:04:02,550 --> 01:04:07,290
code that's capable of scientific

01:04:05,190 --> 01:04:09,600
discovery while providing good

01:04:07,290 --> 01:04:12,500
performance scaling and portability on

01:04:09,600 --> 01:04:15,210
top super computing systems worldwide

01:04:12,500 --> 01:04:16,860
together with illustrating the key

01:04:15,210 --> 01:04:19,650
metrics that I've emphasized throughout

01:04:16,860 --> 01:04:23,490
the talk time to solution and associated

01:04:19,650 --> 01:04:26,280
energy to solution OpenMP plays a key

01:04:23,490 --> 01:04:28,610
role in enabling scalable scientific

01:04:26,280 --> 01:04:31,110
software for extreme scale applications

01:04:28,610 --> 01:04:33,450
with Fusion Energi science as an

01:04:31,110 --> 01:04:35,100
illustrator domain application you don't

01:04:33,450 --> 01:04:38,450
have to be an expert in this area to

01:04:35,100 --> 01:04:40,890
appreciate this but and but basically

01:04:38,450 --> 01:04:43,320
illustrates here is an important domain

01:04:40,890 --> 01:04:47,280
application area they can engage

01:04:43,320 --> 01:04:51,120
computing an extreme scale this is the

01:04:47,280 --> 01:04:54,150
most recent article that we have current

01:04:51,120 --> 01:04:56,520
progress is achieved includes deployment

01:04:54,150 --> 01:04:58,380
of innovative algorithms within a modern

01:04:56,520 --> 01:05:00,470
application code that delivers new

01:04:58,380 --> 01:05:03,240
scientific insights so they showed

01:05:00,470 --> 01:05:05,790
people weren't able to do this at scale

01:05:03,240 --> 01:05:08,490
on a very large problem size before and

01:05:05,790 --> 01:05:10,950
but this is carried out currently on all

01:05:08,490 --> 01:05:14,400
of these top supercomputers future

01:05:10,950 --> 01:05:18,150
targets include the under petaflop

01:05:14,400 --> 01:05:21,660
summit project at oakridge that we're

01:05:18,150 --> 01:05:26,430
already engaged in we're also have

01:05:21,660 --> 01:05:29,550
access to Cory Aurora's is about to

01:05:26,430 --> 01:05:31,620
announce their their recruitment of

01:05:29,550 --> 01:05:34,020
projects I think we'll have a decent

01:05:31,620 --> 01:05:38,730
shot at getting into that stampede to

01:05:34,020 --> 01:05:41,190
the the night some landing system at the

01:05:38,730 --> 01:05:44,820
NSF's center there we will have access

01:05:41,190 --> 01:05:47,340
to that this is the big GPU system

01:05:44,820 --> 01:05:50,190
that's coming up in Japan under Satoshi

01:05:47,340 --> 01:05:52,530
Matsoukas leadership at thi tech and and

01:05:50,190 --> 01:05:55,260
we're working on a partnership with em

01:05:52,530 --> 01:05:57,540
too so there's lots of evolving things

01:05:55,260 --> 01:05:59,940
here it's very exciting lots of

01:05:57,540 --> 01:06:02,580
interesting progress future progress

01:05:59,940 --> 01:06:03,960
will require algorithmic and solver

01:06:02,580 --> 01:06:06,349
advanced is enabled by

01:06:03,960 --> 01:06:08,820
five math in an interdisciplinary

01:06:06,349 --> 01:06:11,369
co.design type environment together with

01:06:08,820 --> 01:06:13,290
computer science we need to we need

01:06:11,369 --> 01:06:17,160
better and better interactions this has

01:06:13,290 --> 01:06:20,580
been discussed in detail here the open

01:06:17,160 --> 01:06:23,339
mp4 one open ACC etcetera we're working

01:06:20,580 --> 01:06:26,930
actively on moving forward to deploy

01:06:23,339 --> 01:06:31,140
these as soon as possible and of course

01:06:26,930 --> 01:06:34,310
tying this to extreme scale HBC domain

01:06:31,140 --> 01:06:38,369
applications such as ours but but also

01:06:34,310 --> 01:06:41,330
others so I'll conclude with that thank

01:06:38,369 --> 01:06:41,330
you very much for your attention

01:07:01,320 --> 01:07:08,110
right right we try we tried the the

01:07:05,260 --> 01:07:10,360
offload mode and the overhead was very

01:07:08,110 --> 01:07:13,300
difficult for us to deal with we

01:07:10,360 --> 01:07:18,370
actually work with people at at the

01:07:13,300 --> 01:07:20,430
Texas supercomputing Center and the we

01:07:18,370 --> 01:07:24,670
didn't get as far as we needed to get

01:07:20,430 --> 01:07:26,710
Ashley ought to say you know we were

01:07:24,670 --> 01:07:28,660
strongly encouraged to submit this for a

01:07:26,710 --> 01:07:31,090
Gordon Bell prize and entry this year

01:07:28,660 --> 01:07:35,400
because is very different because most

01:07:31,090 --> 01:07:37,990
Gordon Bell prize entries focus on a on

01:07:35,400 --> 01:07:39,610
performance on one platform on one

01:07:37,990 --> 01:07:42,520
machine and this is really spanning

01:07:39,610 --> 01:07:45,670
quite a few but we came up a little

01:07:42,520 --> 01:07:48,850
short because we didn't succeed on a

01:07:45,670 --> 01:07:50,620
number one machine the Xeon Phi system

01:07:48,850 --> 01:07:53,110
so we're doubling motivated because you

01:07:50,620 --> 01:07:56,140
can always go back to the well but we

01:07:53,110 --> 01:07:58,300
need to figure out a way to do this with

01:07:56,140 --> 01:08:01,540
respect to being able to efficiently

01:07:58,300 --> 01:08:05,320
deploy the offload boat and our strategy

01:08:01,540 --> 01:08:07,920
going forward on that is to is to first

01:08:05,320 --> 01:08:11,050
work with your community here in

01:08:07,920 --> 01:08:14,440
figuring out how we can best deploy like

01:08:11,050 --> 01:08:16,690
openmp 4.1 which should help in a

01:08:14,440 --> 01:08:19,920
significant way and then we have the

01:08:16,690 --> 01:08:22,930
platforms to try it out on I mean

01:08:19,920 --> 01:08:26,740
besides you know the advantages we know

01:08:22,930 --> 01:08:29,560
the answers to to a big you know hero

01:08:26,740 --> 01:08:31,390
type calculation and we've done this

01:08:29,560 --> 01:08:33,100
using other algorithmic approach is

01:08:31,390 --> 01:08:36,430
running on other computer so we know how

01:08:33,100 --> 01:08:38,589
to hit the answer now you deployed I

01:08:36,430 --> 01:08:42,040
don't know how hard how long it takes

01:08:38,589 --> 01:08:43,390
let's say you try OpenMP 4.1 ok you get

01:08:42,040 --> 01:08:45,940
it running you have to first identify

01:08:43,390 --> 01:08:48,609
the platform we can try it on and this

01:08:45,940 --> 01:08:51,640
maybe we can get your in collaboration

01:08:48,609 --> 01:08:53,980
and and and and an access to get on a

01:08:51,640 --> 01:08:55,660
system earlier than later that would be

01:08:53,980 --> 01:08:58,060
very exciting because then we can see

01:08:55,660 --> 01:09:00,630
you know how far are you advancing on

01:08:58,060 --> 01:09:02,830
this and then we can try it out on

01:09:00,630 --> 01:09:04,839
stampede which is going to be around for

01:09:02,830 --> 01:09:09,370
a while the largest one in the US and

01:09:04,839 --> 01:09:12,130
and so and then once we

01:09:09,370 --> 01:09:17,920
gain some confidence with that then I

01:09:12,130 --> 01:09:19,900
can reappropriation hutou and say we're

01:09:17,920 --> 01:09:23,140
ready to you know not waste your time

01:09:19,900 --> 01:09:26,590
and such we really already try you know

01:09:23,140 --> 01:09:29,680
your your full system and so that's our

01:09:26,590 --> 01:09:31,570
strategy going forward but we're we're

01:09:29,680 --> 01:09:34,000
missing you know what we're missing and

01:09:31,570 --> 01:09:35,350
this is you know full disclosure here is

01:09:34,000 --> 01:09:41,920
that we haven't been very successful

01:09:35,350 --> 01:09:44,470
with the offload yes more exactly

01:09:41,920 --> 01:09:47,440
exactly because it's the communications

01:09:44,470 --> 01:09:50,140
between the host and then with with tana

01:09:47,440 --> 01:09:52,900
to there's a there's three of these xeon

01:09:50,140 --> 01:09:56,340
phi processors and that's a pain in the

01:09:52,900 --> 01:09:56,340
neck to figure out how to do that

01:10:02,630 --> 01:10:07,710
no that's that certainly that's

01:10:05,610 --> 01:10:11,160
certainly true Tim but we had a lot more

01:10:07,710 --> 01:10:14,370
time to work on that with the algorithms

01:10:11,160 --> 01:10:18,150
people and there you have to deal with

01:10:14,370 --> 01:10:20,340
the Atomics and so forth and and and the

01:10:18,150 --> 01:10:23,820
multi-threading issues are challenging

01:10:20,340 --> 01:10:27,270
there too and there that's a really good

01:10:23,820 --> 01:10:34,200
question to raise because the other we

01:10:27,270 --> 01:10:36,360
managed to do a a CUDA version of this

01:10:34,200 --> 01:10:38,040
code okay and that's the best

01:10:36,360 --> 01:10:40,710
performance you're going to get you know

01:10:38,040 --> 01:10:43,530
but but as we add more physics you know

01:10:40,710 --> 01:10:46,980
which is evolving that's why we're so

01:10:43,530 --> 01:10:50,580
interested in open ACC because the the

01:10:46,980 --> 01:10:53,910
that's a higher programming level and

01:10:50,580 --> 01:10:56,640
you can do that but then and you can

01:10:53,910 --> 01:10:58,560
test against what cuda gives you for the

01:10:56,640 --> 01:11:00,690
problems that you're doing now so we're

01:10:58,560 --> 01:11:12,410
also very interested in open ACC and

01:11:00,690 --> 01:11:14,390
working on that ok you'd be able to wait

01:11:12,410 --> 01:11:21,980
this you'd be able to run the sea on

01:11:14,390 --> 01:11:23,300
pies right now he's in the same yeah if

01:11:21,980 --> 01:11:25,850
you're going to make the same mistake

01:11:23,300 --> 01:11:27,290
you go open acct you know it's like

01:11:25,850 --> 01:11:29,120
you'll create a version of the code it

01:11:27,290 --> 01:11:30,920
will work very well in the wonderful end

01:11:29,120 --> 01:11:34,640
video platforms but it won't work well

01:11:30,920 --> 01:11:36,530
on zn5 so don't make that mistake a

01:11:34,640 --> 01:11:39,620
second time you made it once by locking

01:11:36,530 --> 01:11:40,910
yourself in kuta by not going with an

01:11:39,620 --> 01:11:42,800
open standard you're going to do that

01:11:40,910 --> 01:11:49,550
again with open acct don't you learn

01:11:42,800 --> 01:11:52,400
from your mistakes well you know I the

01:11:49,550 --> 01:11:54,980
planet is that really was go with open

01:11:52,400 --> 01:11:56,930
mp4 not zero you got you know Intel

01:11:54,980 --> 01:11:58,250
people here who are close to development

01:11:56,930 --> 01:12:00,910
team that can make sure that you're

01:11:58,250 --> 01:12:03,770
getting a technology that works and

01:12:00,910 --> 01:12:07,370
don't spend time with open ACC's you'll

01:12:03,770 --> 01:12:10,790
love to try out we'll never see platform

01:12:07,370 --> 01:12:15,470
so what's missing from open mp4 dot 0

01:12:10,790 --> 01:12:18,470
yeah what do you know

01:12:15,470 --> 01:12:21,350
I don't know specifically I just know

01:12:18,470 --> 01:12:23,000
that for dot one is is is really what

01:12:21,350 --> 01:12:25,460
people are pointing to is saying you

01:12:23,000 --> 01:12:28,610
know this is a significant improvement

01:12:25,460 --> 01:12:31,010
over for dot oh I think the experts here

01:12:28,610 --> 01:12:33,260
it can probably speak more to it but for

01:12:31,010 --> 01:12:35,090
I don't want to venture into that and

01:12:33,260 --> 01:12:41,240
just say you know what specific pieces

01:12:35,090 --> 01:12:43,850
of our parts that we locked yourself in

01:12:41,240 --> 01:12:50,480
well it's not to it again so don't go

01:12:43,850 --> 01:12:53,270
that open ACC route that's a reasonable

01:12:50,480 --> 01:12:56,510
comment Tim I you know I'm totally

01:12:53,270 --> 01:12:58,850
agnostic about this because I look at

01:12:56,510 --> 01:13:02,900
this from an applications domain

01:12:58,850 --> 01:13:05,300
perspective and say you know whatever

01:13:02,900 --> 01:13:06,980
platforms work the best and we want to

01:13:05,300 --> 01:13:09,590
be able to do this comparison and

01:13:06,980 --> 01:13:13,100
whatever programming languages work the

01:13:09,590 --> 01:13:15,230
best and and so we're just you know

01:13:13,100 --> 01:13:18,050
keeping an open mind about these things

01:13:15,230 --> 01:13:20,300
and for sure I mean you know if it's

01:13:18,050 --> 01:13:22,790
very clear that you know when you show

01:13:20,300 --> 01:13:26,030
examples like wow you know even for the

01:13:22,790 --> 01:13:28,160
the present GPU cpu systems that the

01:13:26,030 --> 01:13:30,020
Ares network makes a big difference in

01:13:28,160 --> 01:13:31,970
our code you know that that just hits

01:13:30,020 --> 01:13:34,460
you in the eye and just says come on you

01:13:31,970 --> 01:13:38,170
know you're not going to keep on going

01:13:34,460 --> 01:13:38,170
down this trail with with other things

01:13:40,770 --> 01:13:45,270
thank you very much to Professor Green

01:13:47,430 --> 01:13:52,030
if people want to follow up with other

01:13:50,440 --> 01:13:54,370
questions I'm here for the rest of the

01:13:52,030 --> 01:13:57,220
day I'd be delighted to talk to you all

01:13:54,370 --> 01:14:00,300
especially experts from Intel and such

01:13:57,220 --> 01:14:00,300

YouTube URL: https://www.youtube.com/watch?v=xhhniJ0sHNg


