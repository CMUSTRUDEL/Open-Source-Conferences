Title: Enabling Application Portability across HPC Platforms
Publication date: 2015-12-17
Playlist: OpenMPCon 2015 Developers Conference
Description: 
	Full Title: Enabling Application Portability across HPC Platforms using Open Standards: User-oriented goals for OpenMP

Presented by Tim Mattson, Intel on behalf of Alice Koniges, Lawrence Berkeley National Lab.  Co-authors: Richard Gerber and Yun He, Lawrence Berkeley National Lab. Fernanda Foertter, Oak Ridge National Lab.

Abstract: Portability plus performance are key requirements for large-scale scientific simulations on the path to exascale. Users of the high-end computing facilities such as the National Energy Research Scientific Computing Center (NERSC) and the Oak Ridge Leadership Computing Facility (OLCF) are demanding portable standards to enable their codes to run on differing high performance computing (HPC) architectures with relatively little user intervention between differing versions that have been optimized for performance.

The emerging OpenMP standards are poised to offer such portability. In this presentation, we will discuss several important goals and requirements of portable standards in the context of OpenMP.

We will also encourage audience participation as we discuss and formulate the current state-of-the-art in this area and our hopes and goals for the future. We will start by describing the current and next generation architectures at NERSC and OLCF and explain how the differences require different general programming paradigms to facilitate high-performance implementations.

Then we will overview the current work by the OpenMP standard to address application portability.We will illustrate the current state-of-the-art with examples from real application codes that are currently running at scale on our differing architectures. Finally, we will address the user needs for the next generation of architectures, and enlist audience user and vendor participation in the road to portable programming.
Captions: 
	00:00:04,710 --> 00:00:11,829
it's like just yours okay we're off and

00:00:07,390 --> 00:00:15,400
running how do I help youth this is this

00:00:11,829 --> 00:00:21,599
the magic button okay I'm unmuted off I

00:00:15,400 --> 00:00:26,080
go okay uh I am NOT Alice I am Tim and

00:00:21,599 --> 00:00:28,689
and I am here to present a presentation

00:00:26,080 --> 00:00:31,660
about enabling application portability

00:00:28,689 --> 00:00:34,060
across hpc platforms an application

00:00:31,660 --> 00:00:36,250
perspective ok so Alice couldn't make it

00:00:34,060 --> 00:00:39,010
so she sent me some slides and so I'm

00:00:36,250 --> 00:00:42,760
going to present her slides which well

00:00:39,010 --> 00:00:49,930
ok that's kind of exciting how do i

00:00:42,760 --> 00:00:52,780
enhance it why won't it work oh I have

00:00:49,930 --> 00:00:56,980
to push a different button which button

00:00:52,780 --> 00:00:58,870
did you push oh ok you know what's the

00:00:56,980 --> 00:01:00,870
changeover to Macintosh going back to

00:00:58,870 --> 00:01:04,390
Windows is like oh this is really weird

00:01:00,870 --> 00:01:06,580
ok so like I always like to start the

00:01:04,390 --> 00:01:07,900
disclaimer because you know I work in a

00:01:06,580 --> 00:01:10,230
corporation we're always afraid of

00:01:07,900 --> 00:01:13,510
getting sued so these are my views

00:01:10,230 --> 00:01:16,060
actually these are the views of some of

00:01:13,510 --> 00:01:18,700
the slides of those co-authors but we're

00:01:16,060 --> 00:01:21,310
not representing our employers so don't

00:01:18,700 --> 00:01:24,700
don't sooner score the DOA and please

00:01:21,310 --> 00:01:27,550
don't sue into I work with really smart

00:01:24,700 --> 00:01:29,979
people I'm not one of those so if I say

00:01:27,550 --> 00:01:32,050
anything stupid it's it's all me don't

00:01:29,979 --> 00:01:34,750
go back to my collaborators and blame

00:01:32,050 --> 00:01:37,270
them on my stupidity I work in Intel's

00:01:34,750 --> 00:01:38,710
labs I know nothing about Intel products

00:01:37,270 --> 00:01:40,270
there's a few slides where I talk about

00:01:38,710 --> 00:01:43,090
it until products but I really know

00:01:40,270 --> 00:01:46,630
nothing about them I get to just think

00:01:43,090 --> 00:01:48,310
silly fox point in the dark corners try

00:01:46,630 --> 00:01:51,100
and imagine what might be important down

00:01:48,310 --> 00:01:52,600
the road and think about all that stuff

00:01:51,100 --> 00:01:56,590
so it's really a lot of fun i have a

00:01:52,600 --> 00:01:58,270
great job so look this talk is something

00:01:56,590 --> 00:02:02,560
and trying an experiment so i was sent

00:01:58,270 --> 00:02:04,360
these slides from from from Alice what

00:02:02,560 --> 00:02:06,400
this really is those at conversation

00:02:04,360 --> 00:02:08,590
between two talks so we're going to move

00:02:06,400 --> 00:02:09,979
back and forth between my own slides and

00:02:08,590 --> 00:02:12,560
the slides from the nurse

00:02:09,979 --> 00:02:14,690
crowd and they'll be sort of talking to

00:02:12,560 --> 00:02:17,659
each other and you'll always be able to

00:02:14,690 --> 00:02:19,519
tell when it's me and my slides talking

00:02:17,659 --> 00:02:22,549
to you because there'll be a picture of

00:02:19,519 --> 00:02:25,129
someone in a kayak if you see a kayak

00:02:22,549 --> 00:02:27,650
picture on the slide you know it's me

00:02:25,129 --> 00:02:29,180
talking not my friends from nurse all

00:02:27,650 --> 00:02:31,760
right I want to be really careful

00:02:29,180 --> 00:02:34,489
because I'm scared to death of the deal

00:02:31,760 --> 00:02:36,170
we office in Washington coming down and

00:02:34,489 --> 00:02:39,590
nailing me so I'm just I'm really

00:02:36,170 --> 00:02:41,840
nervous and gun shy right now so I want

00:02:39,590 --> 00:02:44,150
to be really clear when it's me talking

00:02:41,840 --> 00:02:45,500
not speaking for the nurse folks and so

00:02:44,150 --> 00:02:48,260
you can tell if they'll be the kayak

00:02:45,500 --> 00:02:50,599
pictures and if you don't know I am

00:02:48,260 --> 00:02:52,819
obsessed with kayaking and if you want

00:02:50,599 --> 00:02:55,400
to be bored for our to just ask me

00:02:52,819 --> 00:02:58,220
questions about kayaking but we won't do

00:02:55,400 --> 00:03:00,620
that now ah off we go so let's talk

00:02:58,220 --> 00:03:02,540
about Corey which is a you know it's a

00:03:00,620 --> 00:03:04,310
pretty exascale supercomputer that will

00:03:02,540 --> 00:03:07,280
be going in at nurse we're really

00:03:04,310 --> 00:03:10,700
excited about it you know the idea is to

00:03:07,280 --> 00:03:12,920
try and push this continuing drive to

00:03:10,700 --> 00:03:15,530
more energy-efficient architectures more

00:03:12,920 --> 00:03:17,989
scalable architectures and this is kind

00:03:15,530 --> 00:03:20,540
of seen as a showcase for technologies

00:03:17,989 --> 00:03:22,970
on one of the swim lanes to exascale and

00:03:20,540 --> 00:03:25,609
yes indeed this is one is using you know

00:03:22,970 --> 00:03:27,620
sort of the the Intel swim lanes we're

00:03:25,609 --> 00:03:30,590
going to have lots and lots of slower

00:03:27,620 --> 00:03:32,599
cores but larger numbers of them and

00:03:30,590 --> 00:03:34,160
they're going to have great big hawk and

00:03:32,599 --> 00:03:37,700
vector units on them because everyone

00:03:34,160 --> 00:03:39,769
loves vectors and very deep memory and

00:03:37,700 --> 00:03:42,019
storage hierarchies so that's going to

00:03:39,769 --> 00:03:45,079
be the quarry machine and i just want to

00:03:42,019 --> 00:03:47,569
point out as a chemist myself i am

00:03:45,079 --> 00:03:49,459
really pleased that they are naming

00:03:47,569 --> 00:03:51,829
their machine after a chemist because we

00:03:49,459 --> 00:03:53,930
all know chemist rule right I mean

00:03:51,829 --> 00:03:55,069
aren't have you ever met a chemist you

00:03:53,930 --> 00:03:58,459
didn't just fall in love with right away

00:03:55,069 --> 00:04:00,319
they're awesome so Bravo to nurse for

00:03:58,459 --> 00:04:02,989
naming your machine after a chemist

00:04:00,319 --> 00:04:04,579
that's great so there will be over nine

00:04:02,989 --> 00:04:08,299
thousand three hundred knights landing

00:04:04,579 --> 00:04:10,609
compute nodes 1600 hawes well compute

00:04:08,299 --> 00:04:12,260
nodes in a data servers the aries

00:04:10,609 --> 00:04:14,000
interconnect which I know nothing about

00:04:12,260 --> 00:04:16,250
except it kind of comes to us through

00:04:14,000 --> 00:04:18,739
cray and Intel and is really really fast

00:04:16,250 --> 00:04:21,049
lest your file system it's really really

00:04:18,739 --> 00:04:22,250
cool but wait a minute I'm the intel guy

00:04:21,049 --> 00:04:23,150
i guess i should be talking about the

00:04:22,250 --> 00:04:25,130
hardware so

00:04:23,150 --> 00:04:27,500
let's let's divert here and let's talk a

00:04:25,130 --> 00:04:29,720
little bit about knights landing now I'm

00:04:27,500 --> 00:04:31,970
going to tell you something Xeon Phi

00:04:29,720 --> 00:04:35,090
never really did anything for me as a

00:04:31,970 --> 00:04:37,160
coprocessor sorry Intel folks you know

00:04:35,090 --> 00:04:39,440
great put it out there's a coprocessor

00:04:37,160 --> 00:04:41,810
but if I have to go and move everything

00:04:39,440 --> 00:04:43,789
from my cpu across pci to this

00:04:41,810 --> 00:04:45,949
coprocessor in order to do some

00:04:43,789 --> 00:04:48,919
computing that's just no to be a break

00:04:45,949 --> 00:04:52,520
I'm not going to do that but take that

00:04:48,919 --> 00:04:55,490
many core chip and put it in the socket

00:04:52,520 --> 00:04:57,919
now all of a sudden I am darn excited

00:04:55,490 --> 00:05:01,660
about it let's do something more let's

00:04:57,919 --> 00:05:05,479
take some deer IAM MCD ram a

00:05:01,660 --> 00:05:08,120
multi-channel DRAM right what multi-chip

00:05:05,479 --> 00:05:09,949
derail okay great multi chip dear am I

00:05:08,120 --> 00:05:12,380
don't speak acronym which is funny

00:05:09,949 --> 00:05:15,440
because everyone at Intel does but let's

00:05:12,380 --> 00:05:18,229
put some DRAM right on the package so

00:05:15,440 --> 00:05:21,830
it's really really a cool idea 36 tiles

00:05:18,229 --> 00:05:23,840
2d mesh two cores per tile with two

00:05:21,830 --> 00:05:26,150
vector units you know if you didn't love

00:05:23,840 --> 00:05:27,349
vector units enough before now you're

00:05:26,150 --> 00:05:30,139
really going to love them because we

00:05:27,349 --> 00:05:35,389
have 2 512 bit vector units per core

00:05:30,139 --> 00:05:37,010
which is a scary thought so another cool

00:05:35,389 --> 00:05:39,620
thing says you have this mesh

00:05:37,010 --> 00:05:41,960
interconnect so you don't have the huge

00:05:39,620 --> 00:05:44,990
network diameter of the original xeon

00:05:41,960 --> 00:05:47,419
phi so this is a huge impact on the

00:05:44,990 --> 00:05:51,440
behavior of the network on a chip by

00:05:47,419 --> 00:05:54,500
having this this 2d mesh its XY routing

00:05:51,440 --> 00:05:56,030
so i guess that's all say ok i'm on

00:05:54,500 --> 00:05:58,909
there's routing algorithm stuff egg you

00:05:56,030 --> 00:06:01,550
go into but i won't and and its cache

00:05:58,909 --> 00:06:02,990
coherent so you can have a shared

00:06:01,550 --> 00:06:07,030
address space scheme and it's really

00:06:02,990 --> 00:06:10,280
cool now when you have that many cores

00:06:07,030 --> 00:06:12,020
with that much cash coherence you need

00:06:10,280 --> 00:06:14,150
different ways of slicing up and

00:06:12,020 --> 00:06:16,550
organizing it so we have these different

00:06:14,150 --> 00:06:18,380
ways of clustering modes we call them

00:06:16,550 --> 00:06:21,639
for putting together the the cores and

00:06:18,380 --> 00:06:24,080
so it's going to make basically as

00:06:21,639 --> 00:06:26,479
programmers trying to figure out how to

00:06:24,080 --> 00:06:29,090
develop an optimized code there's an

00:06:26,479 --> 00:06:31,340
awful lot of dials we can play with and

00:06:29,090 --> 00:06:33,229
adjust on this chip to explore how to

00:06:31,340 --> 00:06:34,700
get the most advantage out all those

00:06:33,229 --> 00:06:35,869
cores in the memory it's going to be a

00:06:34,700 --> 00:06:38,339
lot of fun

00:06:35,869 --> 00:06:40,949
another thing that I am personally

00:06:38,339 --> 00:06:42,929
incredibly excited about and I don't say

00:06:40,949 --> 00:06:44,249
that facetiously is the fact that we're

00:06:42,929 --> 00:06:47,189
going to put the network interface

00:06:44,249 --> 00:06:49,529
component right on the package the

00:06:47,189 --> 00:06:51,779
marketing name for it is Omni path which

00:06:49,529 --> 00:06:54,479
is great because Omni path is cool I

00:06:51,779 --> 00:06:58,550
guess I wonder about these names they

00:06:54,479 --> 00:07:02,550
come up with all new path yeah okay oh

00:06:58,550 --> 00:07:04,709
but the fact is you have it on the

00:07:02,550 --> 00:07:07,919
package I'm hoping someday it's on the

00:07:04,709 --> 00:07:10,649
silicon you know you're seeing a trend

00:07:07,919 --> 00:07:15,899
in computing going back many many years

00:07:10,649 --> 00:07:18,539
which is the CPU is like a Borg sucking

00:07:15,899 --> 00:07:21,719
everything up right on your laptops

00:07:18,539 --> 00:07:23,939
we've sucked up the GPU under the CPU

00:07:21,719 --> 00:07:26,699
right as you learn more and more about

00:07:23,939 --> 00:07:29,879
how things work and what accelerators

00:07:26,699 --> 00:07:32,639
work we suck them into the CPU and so we

00:07:29,879 --> 00:07:34,889
see this Nick being sucked into the CPU

00:07:32,639 --> 00:07:36,659
which is great because in an energy

00:07:34,889 --> 00:07:39,539
constrained and latency constrained

00:07:36,659 --> 00:07:40,919
world the closer you can get everything

00:07:39,539 --> 00:07:43,919
integrated on that single piece of

00:07:40,919 --> 00:07:46,019
silicon silicon the better because it's

00:07:43,919 --> 00:07:48,449
a lot of energy to push electrons over a

00:07:46,019 --> 00:07:50,819
wire from one ship to another so I see

00:07:48,449 --> 00:07:53,519
this as a very very exciting development

00:07:50,819 --> 00:07:55,559
with the interconnect so and that by the

00:07:53,519 --> 00:07:59,789
way that's me being sucked down into a

00:07:55,559 --> 00:08:01,050
whirlpool so it was pretty cool someone

00:07:59,789 --> 00:08:03,019
just happened to be there camera right

00:08:01,050 --> 00:08:06,389
at the right moment so at any rate

00:08:03,019 --> 00:08:07,679
Knights landing you know programmers are

00:08:06,389 --> 00:08:10,289
really excited about it and really

00:08:07,679 --> 00:08:12,749
scared by it all right the optimization

00:08:10,289 --> 00:08:14,459
bozos like me are really excited because

00:08:12,749 --> 00:08:15,869
I have these different cache models I

00:08:14,459 --> 00:08:17,639
can have a flat model I can have a

00:08:15,869 --> 00:08:19,740
hybrid model for how I deal with that

00:08:17,639 --> 00:08:22,139
memory and a chip and you've added these

00:08:19,740 --> 00:08:24,539
extra interesting features to the memory

00:08:22,139 --> 00:08:26,399
hierarchy of course application

00:08:24,539 --> 00:08:28,559
programmers are also looking at that and

00:08:26,399 --> 00:08:30,089
going oh my god I couldn't handle two

00:08:28,559 --> 00:08:32,009
levels of cache and now you're putting

00:08:30,089 --> 00:08:33,810
these up two levels of cache at mct

00:08:32,009 --> 00:08:36,810
ramen what's going to happen when we put

00:08:33,810 --> 00:08:40,079
nvram on the package so you know you

00:08:36,810 --> 00:08:42,209
better learn to love complex mucking

00:08:40,079 --> 00:08:43,680
around in the guts of a memory hierarchy

00:08:42,209 --> 00:08:45,209
because there's going to be an awful lot

00:08:43,680 --> 00:08:47,790
of options out there which is going to

00:08:45,209 --> 00:08:49,800
be a lot of fun so I'm really excited

00:08:47,790 --> 00:09:00,470
about this

00:08:49,800 --> 00:09:02,670
and I'll push the right button whoa no

00:09:00,470 --> 00:09:09,000
why don't you tell see I should have

00:09:02,670 --> 00:09:18,839
used my own laptop well I have weird

00:09:09,000 --> 00:09:30,120
eyeballs and now it's all out of order

00:09:18,839 --> 00:09:35,250
and where am i okay there okay whoa so

00:09:30,120 --> 00:09:37,350
to run effectively on Cory programmers

00:09:35,250 --> 00:09:40,019
are going to have to deal with multiple

00:09:37,350 --> 00:09:42,060
domains of parallelism you're going to

00:09:40,019 --> 00:09:44,220
have to deal with mpi but that's cool

00:09:42,060 --> 00:09:47,070
every single one of us in here loves NP

00:09:44,220 --> 00:09:51,300
i right who doesn't love NP I I mean

00:09:47,070 --> 00:09:55,020
really well you're wrong MPI's awesome

00:09:51,300 --> 00:09:58,170
okay but also there's going to be a lot

00:09:55,020 --> 00:09:59,370
more node level parallelism okay so to

00:09:58,170 --> 00:10:01,860
the fact you're going to have all these

00:09:59,370 --> 00:10:05,670
cores on a node doesn't be a lot more

00:10:01,860 --> 00:10:08,370
node level parallelism you gotta exploit

00:10:05,670 --> 00:10:11,339
data parallelism and because of this

00:10:08,370 --> 00:10:13,950
more complicated memory hierarchy your

00:10:11,339 --> 00:10:16,170
code has got to take data locality into

00:10:13,950 --> 00:10:18,420
account either explicitly or maybe

00:10:16,170 --> 00:10:20,459
hopefully the compiler will take it into

00:10:18,420 --> 00:10:22,200
account but someone somewhere that the

00:10:20,459 --> 00:10:24,480
processing chain is going to have to do

00:10:22,200 --> 00:10:26,640
a lot to take locality into account now

00:10:24,480 --> 00:10:28,800
when I think my friend Alice said

00:10:26,640 --> 00:10:30,390
exploit data parallelism which she

00:10:28,800 --> 00:10:34,010
really meant is you need vectorization

00:10:30,390 --> 00:10:36,270
you need really effective vectorization

00:10:34,010 --> 00:10:38,490
so I want you to think about

00:10:36,270 --> 00:10:40,170
vectorization and I love this slide it's

00:10:38,490 --> 00:10:42,240
now out of date but it came from my

00:10:40,170 --> 00:10:44,790
friend Kirk quite sure at UC Berkeley

00:10:42,240 --> 00:10:47,910
and those little pie charts there say

00:10:44,790 --> 00:10:50,220
how much are you wasting if you don't

00:10:47,910 --> 00:10:52,709
take advantage of the vector units so

00:10:50,220 --> 00:10:54,600
with SSC you're wasting just 75 percent

00:10:52,709 --> 00:10:55,949
of the chips capability if your code

00:10:54,600 --> 00:10:59,010
doesn't take advantage of vectorization

00:10:55,949 --> 00:11:00,899
as you move to the 16 way Cindy that

00:10:59,010 --> 00:11:02,440
goes to ninety-four percent of course

00:11:00,899 --> 00:11:04,450
that's out of date because

00:11:02,440 --> 00:11:07,030
I now double the amount of vector

00:11:04,450 --> 00:11:10,090
capability because i have 2 512 bit

00:11:07,030 --> 00:11:13,810
vector units it's wasting even more so

00:11:10,090 --> 00:11:16,030
look the bottom line is it is vitally

00:11:13,810 --> 00:11:18,190
important that your codes vectorize

00:11:16,030 --> 00:11:19,720
really well okay architects love vector

00:11:18,190 --> 00:11:21,070
units I've never met a hardware

00:11:19,720 --> 00:11:23,440
architect you didn't love them I've

00:11:21,070 --> 00:11:26,770
never met an application programmers who

00:11:23,440 --> 00:11:29,680
didn't hate them all right so this this

00:11:26,770 --> 00:11:31,840
is an interesting tension that we had

00:11:29,680 --> 00:11:33,400
Intel are investing a lot of money in

00:11:31,840 --> 00:11:35,110
the industry is investing a lot of

00:11:33,400 --> 00:11:36,580
effort and figuring out how to really

00:11:35,110 --> 00:11:39,670
take advantage of these vector

00:11:36,580 --> 00:11:41,590
instructions and so you have a few

00:11:39,670 --> 00:11:43,120
options with the vector units you can

00:11:41,590 --> 00:11:45,850
let the compiler do the job which

00:11:43,120 --> 00:11:47,470
frankly is what most people do they get

00:11:45,850 --> 00:11:49,870
their program running they multi-thread

00:11:47,470 --> 00:11:52,900
it then they say compiler take care of

00:11:49,870 --> 00:11:54,940
the vectors for vectorization for me and

00:11:52,900 --> 00:11:56,920
you know that works sometimes usually

00:11:54,940 --> 00:11:58,750
doesn't but it's an option the other

00:11:56,920 --> 00:12:01,930
thing is let the compiler with language

00:11:58,750 --> 00:12:03,520
level constructs take care of it and and

00:12:01,930 --> 00:12:05,170
this is really what seems to work well

00:12:03,520 --> 00:12:06,850
you know with the ignore vector

00:12:05,170 --> 00:12:09,670
dependencies and this is why we have the

00:12:06,850 --> 00:12:10,840
sindhi construct and OpenMP and i'm not

00:12:09,670 --> 00:12:12,400
going to get into arguments whether

00:12:10,840 --> 00:12:14,230
that's the right approach or the wrong

00:12:12,400 --> 00:12:16,990
approach look I'm really just a stupid

00:12:14,230 --> 00:12:20,740
application programmer but to me whether

00:12:16,990 --> 00:12:23,590
I go pragma OMP Cindy or pragma I've

00:12:20,740 --> 00:12:25,440
edep ignore vector dependency it's the

00:12:23,590 --> 00:12:27,280
same thing to me I'm still going through

00:12:25,440 --> 00:12:28,840
figuring out what I have to do to

00:12:27,280 --> 00:12:30,790
restructure my loops so they can

00:12:28,840 --> 00:12:34,120
vectorize and then telling the system

00:12:30,790 --> 00:12:36,490
just vectorize the stupid thing so that

00:12:34,120 --> 00:12:39,670
that seems to be the approach that's

00:12:36,490 --> 00:12:42,010
really taking off now I'm really bizarre

00:12:39,670 --> 00:12:44,560
I actually like the third option which

00:12:42,010 --> 00:12:47,290
is to use the low-level vector intrinsic

00:12:44,560 --> 00:12:50,110
and so let's take a look at this using

00:12:47,290 --> 00:12:52,120
my favorite little example there I am

00:12:50,110 --> 00:12:53,560
washed up on a beach waiting for a wave

00:12:52,120 --> 00:12:55,360
to pick me up again I have a really

00:12:53,560 --> 00:12:58,930
bizarre kayak that's kind of banana

00:12:55,360 --> 00:13:01,480
shaped it's really cool hey hey we're

00:12:58,930 --> 00:13:03,430
going to do the the PI program because

00:13:01,480 --> 00:13:05,260
it fits on PowerPoint and actually does

00:13:03,430 --> 00:13:07,270
something and if you took the tutorial

00:13:05,260 --> 00:13:09,790
you can see how much time I can waste on

00:13:07,270 --> 00:13:11,620
this single stupid little program but

00:13:09,790 --> 00:13:13,640
you know here's what the code looks like

00:13:11,620 --> 00:13:15,740
and

00:13:13,640 --> 00:13:18,920
we can vectorize this by putting the

00:13:15,740 --> 00:13:21,170
pragma OMP Cindy reduction all right and

00:13:18,920 --> 00:13:23,330
when I did this it didn't do any where

00:13:21,170 --> 00:13:25,970
near what I expected and I spent a lot

00:13:23,330 --> 00:13:27,950
of time stressing over this until one of

00:13:25,970 --> 00:13:31,640
my good friends and the Robert diva at

00:13:27,950 --> 00:13:37,040
work pointed out that in see your

00:13:31,640 --> 00:13:42,050
literals are double so that's 0.5 in the

00:13:37,040 --> 00:13:44,870
I plus 0.5 times step that 4.0 over 1.0

00:13:42,050 --> 00:13:48,290
those are doubles so now i'm mixing

00:13:44,870 --> 00:13:50,120
doubles and floats inside my loop which

00:13:48,290 --> 00:13:52,190
means I'm clobbering and a lot of the

00:13:50,120 --> 00:13:54,790
opportunity for vectorization inside

00:13:52,190 --> 00:13:57,620
that loop so i had to go through and

00:13:54,790 --> 00:14:01,430
explicitly say no no no let's make those

00:13:57,620 --> 00:14:03,620
floats okay now i want to point out the

00:14:01,430 --> 00:14:06,080
reason I'm putting this up on the slide

00:14:03,620 --> 00:14:10,610
is how many of you would have known that

00:14:06,080 --> 00:14:14,090
before I told you yeah I mean three or

00:14:10,610 --> 00:14:18,170
four five okay the second Robert even

00:14:14,090 --> 00:14:19,880
told me I was like yeah of course but

00:14:18,170 --> 00:14:21,230
you know because I knew that but I

00:14:19,880 --> 00:14:24,380
didn't remember when i was playing this

00:14:21,230 --> 00:14:26,810
code you know this stuff scares me with

00:14:24,380 --> 00:14:30,830
vectorization because it's small little

00:14:26,810 --> 00:14:33,290
bitty changes that seem meaningless can

00:14:30,830 --> 00:14:35,570
have a huge impact on what the code is

00:14:33,290 --> 00:14:37,850
able to vectorize and what the feeling

00:14:35,570 --> 00:14:39,530
is working with the vector units is your

00:14:37,850 --> 00:14:42,200
having to reverse engineer or the

00:14:39,530 --> 00:14:44,870
compiler to try and figure out what it

00:14:42,200 --> 00:14:47,870
is it can actually vectorize I find that

00:14:44,870 --> 00:14:52,820
very very frustrating so I find this

00:14:47,870 --> 00:14:54,500
much better and and I know almost all

00:14:52,820 --> 00:14:56,990
the world's programmers would look at

00:14:54,500 --> 00:14:59,990
this and go you've got to be kidding but

00:14:56,990 --> 00:15:03,110
i actually prefer this look i'm going to

00:14:59,990 --> 00:15:04,910
set up what I do if I'm doing a course

00:15:03,110 --> 00:15:07,340
on vector programming a walk people

00:15:04,910 --> 00:15:09,710
through this you know step number one

00:15:07,340 --> 00:15:11,930
you unroll the loop four times because

00:15:09,710 --> 00:15:14,540
I'm doing this with ssed all right so

00:15:11,930 --> 00:15:16,790
you unroll the loop loop four times then

00:15:14,540 --> 00:15:19,190
you set up your vector registers okay

00:15:16,790 --> 00:15:21,470
based on how you want them the logic of

00:15:19,190 --> 00:15:23,570
the unrolled loop you set up the vector

00:15:21,470 --> 00:15:24,600
registers you build them and then you

00:15:23,570 --> 00:15:26,220
you

00:15:24,600 --> 00:15:28,350
as a sequence of operations on the

00:15:26,220 --> 00:15:30,329
vector registers and then what you do is

00:15:28,350 --> 00:15:32,430
you tried the limit decrease the number

00:15:30,329 --> 00:15:34,500
of temporaries by chaining the

00:15:32,430 --> 00:15:36,600
operations together so you notice I have

00:15:34,500 --> 00:15:38,250
the the application to the ramp and I

00:15:36,600 --> 00:15:40,529
have to add and the multiply all in one

00:15:38,250 --> 00:15:42,300
section ok so the codes really really

00:15:40,529 --> 00:15:45,029
hard to read but it's not that bad to

00:15:42,300 --> 00:15:47,250
work with and at least I didn't have to

00:15:45,029 --> 00:15:48,959
reverse engineer the compiler because I

00:15:47,250 --> 00:15:51,029
knew exactly what I was telling the

00:15:48,959 --> 00:15:52,709
system to do and I'm a control freak

00:15:51,029 --> 00:15:56,009
when I'm doing performance or any

00:15:52,709 --> 00:15:58,889
programming I actually like this but I

00:15:56,009 --> 00:16:02,100
understand most people don't and then i

00:15:58,889 --> 00:16:06,269
paralyzed this ok and once again if this

00:16:02,100 --> 00:16:07,649
was my stupidly advanced open NP course

00:16:06,269 --> 00:16:08,759
that no one would want to take and we go

00:16:07,649 --> 00:16:12,600
through this this would be pretty

00:16:08,759 --> 00:16:15,120
straightforward so here's what I got if

00:16:12,600 --> 00:16:16,709
I turned off vectorization so I

00:16:15,120 --> 00:16:18,810
specifically told the compiler do

00:16:16,709 --> 00:16:20,459
nothing this is the performance number i

00:16:18,810 --> 00:16:23,730
got right here so it's a like you know

00:16:20,459 --> 00:16:26,339
point zero zero five seconds then I just

00:16:23,730 --> 00:16:29,699
sort of said ok automatic vectorize and

00:16:26,339 --> 00:16:31,529
I actually am really surprised the

00:16:29,699 --> 00:16:33,389
automatic vector Iser did pretty darn

00:16:31,529 --> 00:16:36,600
good it kind of cut the performance down

00:16:33,389 --> 00:16:38,040
roughly in half then i use the OMP cindy

00:16:36,600 --> 00:16:39,870
and it didn't do as well as the

00:16:38,040 --> 00:16:43,500
vectorization I don't know why but

00:16:39,870 --> 00:16:45,990
somehow it did not do as well and then

00:16:43,500 --> 00:16:48,300
what I like about the effort i took with

00:16:45,990 --> 00:16:51,000
the SSC intrinsic is that's a realistic

00:16:48,300 --> 00:16:53,279
top bound of you know how well you can

00:16:51,000 --> 00:16:55,110
do and so you can look at that and see

00:16:53,279 --> 00:16:58,470
that the auto vectorize in the openmp

00:16:55,110 --> 00:17:01,199
simdi didn't get to the same level as my

00:16:58,470 --> 00:17:04,679
SSE instructions but they got into the

00:17:01,199 --> 00:17:07,079
ballpark they weren't awful so you know

00:17:04,679 --> 00:17:08,819
I find it I find it encouraging now I

00:17:07,079 --> 00:17:12,000
want to follow up with a slide that

00:17:08,819 --> 00:17:16,650
comes from a larger application study at

00:17:12,000 --> 00:17:18,480
Intel comparing the explicit and simply

00:17:16,650 --> 00:17:21,959
the explicit vectorization that's at the

00:17:18,480 --> 00:17:24,209
OMP simdi to the auto vectorization and

00:17:21,959 --> 00:17:25,740
you know with more complex problems with

00:17:24,209 --> 00:17:27,900
people spending more time going through

00:17:25,740 --> 00:17:30,450
it you can see that there are many many

00:17:27,900 --> 00:17:32,880
cases where the the simdi directive in

00:17:30,450 --> 00:17:34,260
OpenMP really did help giving better

00:17:32,880 --> 00:17:36,990
performance because that's that lighter

00:17:34,260 --> 00:17:37,920
color line so it's got its place and it

00:17:36,990 --> 00:17:42,320
helps sometimes

00:17:37,920 --> 00:17:45,540
but I really find with vectorization

00:17:42,320 --> 00:17:47,370
when I deal with it maybe I'm just more

00:17:45,540 --> 00:17:49,590
lame than the more intelligent people at

00:17:47,370 --> 00:17:51,690
Intel but I always go through this

00:17:49,590 --> 00:17:53,610
process of stumbling around you know you

00:17:51,690 --> 00:17:55,470
try the compiler then you put the Cindy

00:17:53,610 --> 00:17:57,060
in there and you know you'd muck around

00:17:55,470 --> 00:17:58,470
with this and you figure out why I'm not

00:17:57,060 --> 00:18:00,060
getting what I want and then I throw up

00:17:58,470 --> 00:18:03,480
my hands and go heck I'm just going to

00:18:00,060 --> 00:18:06,420
write the sse and i really am concerned

00:18:03,480 --> 00:18:08,040
as an intel stockholder that as we're

00:18:06,420 --> 00:18:10,890
betting more and more on vectorization

00:18:08,040 --> 00:18:13,710
it's not a pleasant experience for

00:18:10,890 --> 00:18:16,590
programmers and we have a lot of work to

00:18:13,710 --> 00:18:19,220
do to try and change that all right so

00:18:16,590 --> 00:18:22,620
hey what about application portability

00:18:19,220 --> 00:18:24,990
so if you look across the the do a

00:18:22,620 --> 00:18:28,050
community from nurse to oakridge to

00:18:24,990 --> 00:18:29,790
argon the pnnl you know you find that

00:18:28,050 --> 00:18:31,680
all these centers have fundamentally

00:18:29,790 --> 00:18:33,690
different architectures and that

00:18:31,680 --> 00:18:35,550
actually will never change and you know

00:18:33,690 --> 00:18:36,810
no one would no one in their right mind

00:18:35,550 --> 00:18:38,190
would actually change it you know it

00:18:36,810 --> 00:18:39,660
doesn't make sense for them all to buy

00:18:38,190 --> 00:18:42,390
in tell you know they're going to always

00:18:39,660 --> 00:18:44,970
try other vendors they're fundamentally

00:18:42,390 --> 00:18:47,760
different but still we have to ask given

00:18:44,970 --> 00:18:50,520
the high cost of building software

00:18:47,760 --> 00:18:53,160
infrastructure we really need to be able

00:18:50,520 --> 00:18:56,430
to share a software across centers and

00:18:53,160 --> 00:18:58,800
we all know I don't need to convince I

00:18:56,430 --> 00:19:00,450
think any of you that the lifespan of a

00:18:58,800 --> 00:19:02,820
piece of software is greater than the

00:19:00,450 --> 00:19:05,250
lifespan of a piece of hardware so it

00:19:02,820 --> 00:19:06,900
really is critical that software can

00:19:05,250 --> 00:19:09,150
move around to these different

00:19:06,900 --> 00:19:11,070
architectures both between centers and

00:19:09,150 --> 00:19:13,850
between generations of machines at a

00:19:11,070 --> 00:19:16,020
center and so this is a huge challenge

00:19:13,850 --> 00:19:17,850
because if I have a machine that's a

00:19:16,020 --> 00:19:19,260
whole bunch of GPUs or I have a machine

00:19:17,850 --> 00:19:20,430
that's a whole bunch of night landing

00:19:19,260 --> 00:19:23,100
those are very very different

00:19:20,430 --> 00:19:26,280
architectures and it's very painful if I

00:19:23,100 --> 00:19:28,620
can't share software between that so the

00:19:26,280 --> 00:19:31,290
the community of programmers in the do

00:19:28,620 --> 00:19:33,060
in and the supercomputing community are

00:19:31,290 --> 00:19:36,480
really struggling with trying to figure

00:19:33,060 --> 00:19:38,160
out what's their path forward and so

00:19:36,480 --> 00:19:39,690
they're having these workshops to try

00:19:38,160 --> 00:19:42,210
and pull together and learn best

00:19:39,690 --> 00:19:43,560
practices from the different centers to

00:19:42,210 --> 00:19:46,890
see if they can come up with a way

00:19:43,560 --> 00:19:48,180
forward that fixes this but you know if

00:19:46,890 --> 00:19:49,860
you think about this from the

00:19:48,180 --> 00:19:52,950
application programmers point

00:19:49,860 --> 00:19:55,020
to view you know things are looking ugly

00:19:52,950 --> 00:19:57,870
out there you know first there were

00:19:55,020 --> 00:19:59,760
vectors and we all some of us old folk

00:19:57,870 --> 00:20:01,740
there's a few of you out there that are

00:19:59,760 --> 00:20:04,080
old enough you know remember the old

00:20:01,740 --> 00:20:06,090
crate vector machines and those were

00:20:04,080 --> 00:20:08,160
relatively easy we just vectorized our

00:20:06,090 --> 00:20:09,990
loops and they worked actually pretty

00:20:08,160 --> 00:20:13,290
well in fact I remember in nineteen

00:20:09,990 --> 00:20:16,049
eighty nine ish I made a bet with

00:20:13,290 --> 00:20:18,540
someone at Cray that their compiler

00:20:16,049 --> 00:20:21,480
couldn't beat my hand optimized code and

00:20:18,540 --> 00:20:23,370
I lost that bet it was the first time a

00:20:21,480 --> 00:20:25,919
compiler really just beat me and it just

00:20:23,370 --> 00:20:27,840
shows how advanced those crema Sheen's

00:20:25,919 --> 00:20:29,730
coupled with those creo compilers work

00:20:27,840 --> 00:20:31,200
and there were very different sort of

00:20:29,730 --> 00:20:33,929
vector units and we're putting on ships

00:20:31,200 --> 00:20:36,840
today but they were really marvelous but

00:20:33,929 --> 00:20:38,520
you know the npp revolution came along

00:20:36,840 --> 00:20:40,650
the attack of the killer mike rose and

00:20:38,520 --> 00:20:43,290
so they ripped out all that vector code

00:20:40,650 --> 00:20:45,720
they wiii and put in message passing

00:20:43,290 --> 00:20:49,260
which eventually became MPI so a very

00:20:45,720 --> 00:20:51,450
pain painful adjustment on then of

00:20:49,260 --> 00:20:54,299
course multi processor nodes came along

00:20:51,450 --> 00:20:56,190
and so people added OpenMP and according

00:20:54,299 --> 00:20:58,260
to allison company it didn't really do

00:20:56,190 --> 00:21:00,809
very much and according to allison

00:20:58,260 --> 00:21:02,460
company OpenMP only works on smaller

00:21:00,809 --> 00:21:07,020
number of nodes and you know wait a

00:21:02,460 --> 00:21:09,960
minute wait a minute scaling is a factor

00:21:07,020 --> 00:21:12,270
of the algorithm and how you use the API

00:21:09,960 --> 00:21:15,690
I don't really think it's fair to say

00:21:12,270 --> 00:21:17,940
napi is fundamentally unscalable it is

00:21:15,690 --> 00:21:19,679
the people who designed it were bad you

00:21:17,940 --> 00:21:21,720
know I look at a lot of codes and I

00:21:19,679 --> 00:21:23,250
haven't looked at the nurse codes so I

00:21:21,720 --> 00:21:24,780
haven't looked at the codes that Alice

00:21:23,250 --> 00:21:26,549
and her colleagues we're thinking about

00:21:24,780 --> 00:21:28,650
when they wrote that but I've had

00:21:26,549 --> 00:21:30,720
multiple meetings with people these do e

00:21:28,650 --> 00:21:32,760
centers and what they did is they took

00:21:30,720 --> 00:21:35,100
their API code and just put a parallel

00:21:32,760 --> 00:21:37,260
for in front of a loop you mentioned

00:21:35,100 --> 00:21:39,210
this James earlier today you know if

00:21:37,260 --> 00:21:41,190
you're going to do that you're not going

00:21:39,210 --> 00:21:44,429
to get good scalability period i'm sorry

00:21:41,190 --> 00:21:46,110
you're not okay let's face it if you

00:21:44,429 --> 00:21:47,429
want good scalability you have to look

00:21:46,110 --> 00:21:49,530
at your algorithm you have to think

00:21:47,429 --> 00:21:51,570
about the data structures you have to

00:21:49,530 --> 00:21:53,280
optimize the data structures the layout

00:21:51,570 --> 00:21:55,650
you have to move the parallel overhead

00:21:53,280 --> 00:21:57,780
out and to think that you can walk up

00:21:55,650 --> 00:21:59,730
and just put parallel for in front of

00:21:57,780 --> 00:22:01,320
every loop nest and expect that you're

00:21:59,730 --> 00:22:01,770
going to get reasonable scalability it

00:22:01,320 --> 00:22:03,680
just it

00:22:01,770 --> 00:22:06,780
ain't going to happen you know I think

00:22:03,680 --> 00:22:08,640
the weakness of the GPU world is also

00:22:06,780 --> 00:22:10,650
its strength because when you come to a

00:22:08,640 --> 00:22:12,810
GPU based machine you know you're going

00:22:10,650 --> 00:22:15,570
to have to rewrite code you know you're

00:22:12,810 --> 00:22:18,060
going to have to hand optimize that that

00:22:15,570 --> 00:22:20,850
Colonel from scratch and so you accept

00:22:18,060 --> 00:22:24,480
that and you do it and in essence with

00:22:20,850 --> 00:22:27,330
OpenMP we kind of sell people this idea

00:22:24,480 --> 00:22:29,160
that yeah you just sort of put a

00:22:27,330 --> 00:22:30,990
parallel for here and put a parallel for

00:22:29,160 --> 00:22:34,020
there and you move from a you know your

00:22:30,990 --> 00:22:36,360
laptop to a xeon phi we say this all the

00:22:34,020 --> 00:22:37,650
time in our marketing literature and you

00:22:36,360 --> 00:22:41,190
know it's one of these things where it's

00:22:37,650 --> 00:22:42,810
it's partially true but you know do you

00:22:41,190 --> 00:22:44,790
really believe a code you optimized for

00:22:42,810 --> 00:22:48,840
two to four cores is going to optimize

00:22:44,790 --> 00:22:50,190
and run well on 60 Plus cores No so you

00:22:48,840 --> 00:22:52,680
know we just we really have to be

00:22:50,190 --> 00:22:54,900
careful here so I'm sorry my friends

00:22:52,680 --> 00:22:57,240
from nursed I have to take a little bit

00:22:54,900 --> 00:22:59,130
of a fence at that statement that NP

00:22:57,240 --> 00:23:00,960
ibis openmp doesn't really work and it

00:22:59,130 --> 00:23:02,280
only scales do a few processors look if

00:23:00,960 --> 00:23:06,000
you're willing to do the work it will

00:23:02,280 --> 00:23:09,000
all right okay programming models by the

00:23:06,000 --> 00:23:11,310
Dozen so if you're an application

00:23:09,000 --> 00:23:14,250
programmer and you look out there right

00:23:11,310 --> 00:23:16,350
now it's a confusing space and Alice

00:23:14,250 --> 00:23:18,660
kind of draws this wonderful analogy to

00:23:16,350 --> 00:23:20,910
Mozart in a conversation with Emperor

00:23:18,660 --> 00:23:23,310
Joseph the second you know my dear young

00:23:20,910 --> 00:23:25,020
man don't take it too hard your work is

00:23:23,310 --> 00:23:26,970
ingenious its quality work with there

00:23:25,020 --> 00:23:29,040
are too many notes just cut a few out

00:23:26,970 --> 00:23:30,870
it'll be fine you know Mozart's like

00:23:29,040 --> 00:23:33,510
well which few do you have in mind your

00:23:30,870 --> 00:23:35,280
majesty apparently this conversation

00:23:33,510 --> 00:23:37,260
occurred when the Majesty found the

00:23:35,280 --> 00:23:39,390
Emperor that he couldn't play the music

00:23:37,260 --> 00:23:41,400
that Mozart was writing so rather than

00:23:39,390 --> 00:23:42,720
figure out how to improve is playing you

00:23:41,400 --> 00:23:45,180
just figured out Mozart could get rid of

00:23:42,720 --> 00:23:47,220
some of the notes so I'd love to be a

00:23:45,180 --> 00:23:49,560
king and think that way but you know

00:23:47,220 --> 00:23:52,140
I've used these slides before but I have

00:23:49,560 --> 00:23:54,330
a point I want to make you know this is

00:23:52,140 --> 00:23:56,580
a list of programming models in the 90s

00:23:54,330 --> 00:23:58,740
all right and it's in a ridiculous list

00:23:56,580 --> 00:24:00,510
but this is what happens if you leave

00:23:58,740 --> 00:24:03,510
computer scientists and vendors to their

00:24:00,510 --> 00:24:05,580
own will okay it's fun creating a

00:24:03,510 --> 00:24:06,780
programming model and we will do it now

00:24:05,580 --> 00:24:08,880
I want you to look at this list and

00:24:06,780 --> 00:24:12,630
think about the amount of human effort

00:24:08,880 --> 00:24:14,460
and engineering skill wasted okay think

00:24:12,630 --> 00:24:15,870
about how primitive the tools are

00:24:14,460 --> 00:24:17,730
are when you come up to the system

00:24:15,870 --> 00:24:20,220
you're working on and you go oh my god

00:24:17,730 --> 00:24:22,440
why can't I have a portable debugger as

00:24:20,220 --> 00:24:23,880
I move from one system to another well

00:24:22,440 --> 00:24:26,220
how can your portable debugger team

00:24:23,880 --> 00:24:28,020
develop a technology if they have even

00:24:26,220 --> 00:24:30,630
five percent of these they decide to

00:24:28,020 --> 00:24:32,640
support you know the point a lot of megs

00:24:30,630 --> 00:24:34,230
it's not just the wasted effort in

00:24:32,640 --> 00:24:36,210
creating all of these different

00:24:34,230 --> 00:24:38,070
programming models it's the wasted

00:24:36,210 --> 00:24:40,830
effort and the tool change do not get

00:24:38,070 --> 00:24:43,020
matured and are not made available so

00:24:40,830 --> 00:24:44,880
this is incredibly damaging now it's

00:24:43,020 --> 00:24:46,680
even worse and once again I apologize

00:24:44,880 --> 00:24:48,270
several you've heard me use this slide

00:24:46,680 --> 00:24:50,730
before but you haven't seen it this is

00:24:48,270 --> 00:24:53,400
very important there's a human phenomena

00:24:50,730 --> 00:24:55,920
called choice overload okay how many of

00:24:53,400 --> 00:24:57,480
you have heard of choice overload good

00:24:55,920 --> 00:24:59,520
not too many of you so I'll only bore

00:24:57,480 --> 00:25:03,000
about a third of the room this was an

00:24:59,520 --> 00:25:04,770
amazing study it was done at the drag or

00:25:03,000 --> 00:25:06,450
grocery store which is over in Palo Alto

00:25:04,770 --> 00:25:09,020
which is the the neighborhood around

00:25:06,450 --> 00:25:11,370
Stanford it's a lot of rich upscale

00:25:09,020 --> 00:25:13,950
customers so it's a gourmet grocery

00:25:11,370 --> 00:25:15,780
store and so this research department at

00:25:13,950 --> 00:25:18,600
Stanford University did the following

00:25:15,780 --> 00:25:22,440
they put out a display of gourmet gm's

00:25:18,600 --> 00:25:24,840
one display had 24 jars the other

00:25:22,440 --> 00:25:28,230
display had six jars so you can just

00:25:24,840 --> 00:25:30,660
kind of imagine 24 jars yea big and lots

00:25:28,230 --> 00:25:32,880
of stuff six jars it's a little tiny

00:25:30,660 --> 00:25:35,070
thing all right you could go and sample

00:25:32,880 --> 00:25:37,410
the jams and then they would give you a

00:25:35,070 --> 00:25:39,570
coupon which would give a discount if

00:25:37,410 --> 00:25:41,190
you buy so think about what they could

00:25:39,570 --> 00:25:43,740
measure they could measure how many

00:25:41,190 --> 00:25:45,540
people walked by the display how many

00:25:43,740 --> 00:25:47,340
people were interested enough to sample

00:25:45,540 --> 00:25:49,680
a jam once they walked past the display

00:25:47,340 --> 00:25:53,100
and how many people subsequently

00:25:49,680 --> 00:25:56,400
purchased gym and these results are

00:25:53,100 --> 00:25:59,700
amazing the bigger display got more

00:25:56,400 --> 00:26:02,630
attention who's surprised but only three

00:25:59,700 --> 00:26:06,270
percent of the people who sampled

00:26:02,630 --> 00:26:08,550
purchased jam whereas the sixth jar

00:26:06,270 --> 00:26:11,190
display forty percent stopped and looked

00:26:08,550 --> 00:26:15,180
at it but thirty percent subsequently

00:26:11,190 --> 00:26:17,070
bought that's a dramatic number now

00:26:15,180 --> 00:26:20,940
they've repeated the study several times

00:26:17,070 --> 00:26:24,690
and what this shows is that human beings

00:26:20,940 --> 00:26:25,350
with presented to too many options they

00:26:24,690 --> 00:26:28,110
chew

00:26:25,350 --> 00:26:30,450
choose to not choose they're overwhelmed

00:26:28,110 --> 00:26:33,030
it is called choice overload you

00:26:30,450 --> 00:26:35,730
overload the human choice system and you

00:26:33,030 --> 00:26:39,140
go ah oh my god and just walk away

00:26:35,730 --> 00:26:42,150
instead of making a choice all right now

00:26:39,140 --> 00:26:44,160
yeah the retail world has learned this

00:26:42,150 --> 00:26:46,380
too I bought a bunch of furniture five

00:26:44,160 --> 00:26:48,900
years ago and I really notice the place

00:26:46,380 --> 00:26:51,450
i bought the furniture from had three

00:26:48,900 --> 00:26:53,850
bedroom sets for living room sets and

00:26:51,450 --> 00:26:55,169
I'm going to places that had five times

00:26:53,850 --> 00:26:57,000
that number and I couldn't decide I

00:26:55,169 --> 00:26:58,590
couldn't buy so it really works so think

00:26:57,000 --> 00:27:03,120
about that and now look at this slide

00:26:58,590 --> 00:27:04,860
and think what did we do when we created

00:27:03,120 --> 00:27:07,200
this profusion to programming languages

00:27:04,860 --> 00:27:08,730
if we're trying to attract application

00:27:07,200 --> 00:27:11,250
programmers to parallel computing what

00:27:08,730 --> 00:27:14,010
did we accomplish with this all right so

00:27:11,250 --> 00:27:15,900
this is a serious serious phenomena that

00:27:14,010 --> 00:27:18,000
vendors should be concerned about but

00:27:15,900 --> 00:27:19,830
also you application programmers who are

00:27:18,000 --> 00:27:21,510
trying to create a world where you can

00:27:19,830 --> 00:27:23,880
have a dependable environment when you

00:27:21,510 --> 00:27:28,130
move from machine to machine think about

00:27:23,880 --> 00:27:31,169
what this does now 2005 I was so excited

00:27:28,130 --> 00:27:33,809
because by 2005 things were looking

00:27:31,169 --> 00:27:36,059
better if you wanted a thread library

00:27:33,809 --> 00:27:37,770
you at the win32 API if you were in the

00:27:36,059 --> 00:27:40,020
windows world you had POSIX if you were

00:27:37,770 --> 00:27:42,659
elsewhere if you needed directives you

00:27:40,020 --> 00:27:45,720
had OpenMP done no other thing to

00:27:42,659 --> 00:27:47,280
discuss we had NPI great you know we

00:27:45,720 --> 00:27:49,440
knew we needed something for the managed

00:27:47,280 --> 00:27:52,049
languages you know there's x10 out there

00:27:49,440 --> 00:27:55,679
and maybe Java concur but it was a very

00:27:52,049 --> 00:27:57,000
small list and life was simple then GPUs

00:27:55,679 --> 00:27:59,039
were around in the academic literature

00:27:57,000 --> 00:28:01,080
but no one took him seriously back in

00:27:59,039 --> 00:28:04,049
2005 okay that changed shortly

00:28:01,080 --> 00:28:08,700
thereafter so I figured we had learned

00:28:04,049 --> 00:28:09,990
our lesson well um i sat down for an

00:28:08,700 --> 00:28:13,020
afternoon and looked through a computer

00:28:09,990 --> 00:28:15,240
conference proceedings from 2010 2 2012

00:28:13,020 --> 00:28:17,669
and look at the list they came up with I

00:28:15,240 --> 00:28:18,990
didn't even spend a long time this is

00:28:17,669 --> 00:28:21,419
just flipping through looking at the

00:28:18,990 --> 00:28:22,890
titles of papers you know and these are

00:28:21,419 --> 00:28:27,090
the programming models but they are

00:28:22,890 --> 00:28:30,030
talking about so somehow we have slipped

00:28:27,090 --> 00:28:31,590
back into this mentality of you have a

00:28:30,030 --> 00:28:35,580
new problem print a new programming

00:28:31,590 --> 00:28:37,799
model I want a PhD I know I'll get a PhD

00:28:35,580 --> 00:28:38,350
for creating a new programming model all

00:28:37,799 --> 00:28:41,559
right

00:28:38,350 --> 00:28:45,130
and so it's out of control and the human

00:28:41,559 --> 00:28:48,179
waste of this just burns me up and just

00:28:45,130 --> 00:28:50,679
it happens gosh it happen several times

00:28:48,179 --> 00:28:52,750
every other week or so sometimes more

00:28:50,679 --> 00:28:54,690
often someone comes up to me and says

00:28:52,750 --> 00:28:56,700
what programming model should they use

00:28:54,690 --> 00:28:58,510
i'm interested in heterogeneous

00:28:56,700 --> 00:29:00,190
platforms and I've heard about this

00:28:58,510 --> 00:29:02,320
opencl thing and I know you know a lot

00:29:00,190 --> 00:29:03,880
about that I've heard about this openmp

00:29:02,320 --> 00:29:06,190
thing I know you know a lot about that

00:29:03,880 --> 00:29:08,289
you know what programming model should i

00:29:06,190 --> 00:29:11,230
use and i have to stand there and tell

00:29:08,289 --> 00:29:13,450
them i honestly cannot tell you because

00:29:11,230 --> 00:29:15,070
i don't see the support around opencl

00:29:13,450 --> 00:29:17,200
that i thought would blow up with i

00:29:15,070 --> 00:29:18,909
thought would take off and the fact that

00:29:17,200 --> 00:29:21,460
matter is today i cannot write an open

00:29:18,909 --> 00:29:25,000
MP program that runs on a GPU cpu to an

00:29:21,460 --> 00:29:28,390
fpga I can't maybe someday soon I hope

00:29:25,000 --> 00:29:31,659
but I can't so we've really really

00:29:28,390 --> 00:29:35,049
mucked it up and it bugs me because I

00:29:31,659 --> 00:29:38,919
thought we knew better ah well so what's

00:29:35,049 --> 00:29:41,169
gone wrong so what's gone wrong that's

00:29:38,919 --> 00:29:44,020
me rolling off a bridge it's really it

00:29:41,169 --> 00:29:48,640
was really fun hurt like hit when I hit

00:29:44,020 --> 00:29:52,299
the water but I'll tell you what

00:29:48,640 --> 00:29:54,809
happened in the 90s the application

00:29:52,299 --> 00:29:57,280
community was united and were aggressive

00:29:54,809 --> 00:29:59,110
now that happened partially because

00:29:57,280 --> 00:30:01,270
government funding was channeling

00:29:59,110 --> 00:30:04,000
through a small number of programs and

00:30:01,270 --> 00:30:06,730
the people playing a leadership role in

00:30:04,000 --> 00:30:08,679
these programs were kind of left wing

00:30:06,730 --> 00:30:10,750
liberal sorts who were really into union

00:30:08,679 --> 00:30:13,390
organizing type ideas and they brought

00:30:10,750 --> 00:30:16,480
programmers together to snap the whip

00:30:13,390 --> 00:30:19,120
and it made a difference if you look at

00:30:16,480 --> 00:30:21,309
the history of the creation of MPI the

00:30:19,120 --> 00:30:23,130
applications community pushed very hard

00:30:21,309 --> 00:30:25,659
for it vendors were happy to go along

00:30:23,130 --> 00:30:27,700
right if the applications community are

00:30:25,659 --> 00:30:29,230
lining up and saying we need you to do

00:30:27,700 --> 00:30:32,200
this this this and this simplify your

00:30:29,230 --> 00:30:34,059
machines vendors are like okay yeah we

00:30:32,200 --> 00:30:35,289
got it we actually like it when the

00:30:34,059 --> 00:30:37,929
application communities come together

00:30:35,289 --> 00:30:41,200
and push around but when NP I came out

00:30:37,929 --> 00:30:43,179
the applications community used it they

00:30:41,200 --> 00:30:45,840
committed to it they worked with us they

00:30:43,179 --> 00:30:47,610
worked with the MPI forum they used it

00:30:45,840 --> 00:30:49,799
and that got it off the ground they

00:30:47,610 --> 00:30:51,659
didn't keep using NX even though there

00:30:49,799 --> 00:30:53,929
is a while there where the Intel native

00:30:51,659 --> 00:30:56,100
message passing the superior to MPI

00:30:53,929 --> 00:30:58,470
applications community was like we don't

00:30:56,100 --> 00:31:01,230
care we're not using that proprietary

00:30:58,470 --> 00:31:03,090
thing you guys have for us and it made a

00:31:01,230 --> 00:31:06,270
difference within a year mpi was

00:31:03,090 --> 00:31:08,549
everywhere okay openmp exactly the same

00:31:06,270 --> 00:31:11,520
story applications community got

00:31:08,549 --> 00:31:13,740
together my hero marries Ozel all

00:31:11,520 --> 00:31:16,320
remember her very for the rest of my

00:31:13,740 --> 00:31:18,539
days you know how firmly she would be

00:31:16,320 --> 00:31:19,529
saying that you will do this you vendors

00:31:18,539 --> 00:31:20,789
will do this and you're going to work

00:31:19,529 --> 00:31:21,899
together with us you're going to create

00:31:20,789 --> 00:31:23,460
this and we're going to get it written

00:31:21,899 --> 00:31:25,500
into the RFPs and you know what the

00:31:23,460 --> 00:31:28,130
application community lined up behind it

00:31:25,500 --> 00:31:30,659
and within a year of the spec coming out

00:31:28,130 --> 00:31:32,490
openmp was available everywhere and you

00:31:30,659 --> 00:31:36,899
an application programmer could walk up

00:31:32,490 --> 00:31:39,330
to an IBM a deck and Intel an SGI a Cray

00:31:36,899 --> 00:31:41,399
machine and have one program that you

00:31:39,330 --> 00:31:44,130
could recompile and run there this was

00:31:41,399 --> 00:31:46,590
marvelous so users when you let vendors

00:31:44,130 --> 00:31:47,880
lock you into a platform you're hurting

00:31:46,590 --> 00:31:49,320
yourself what do you think you're

00:31:47,880 --> 00:31:51,960
telling the vendor community when you

00:31:49,320 --> 00:31:54,529
use cuda and open a cc what are you

00:31:51,960 --> 00:31:57,149
telling them you're saying hey vendor

00:31:54,529 --> 00:31:59,429
we're quite happy to lock ourselves your

00:31:57,149 --> 00:32:04,020
platform yeah thanks vendor that's

00:31:59,429 --> 00:32:05,610
really cool now let me be clear i just

00:32:04,020 --> 00:32:07,799
did a slam there at my friends from

00:32:05,610 --> 00:32:09,600
nvidia are they acting wrong no it's a

00:32:07,799 --> 00:32:12,809
market economy this is capitalism

00:32:09,600 --> 00:32:14,250
capitalism 101 if I can lock my platform

00:32:12,809 --> 00:32:18,000
to my customer I can sell them more

00:32:14,250 --> 00:32:19,919
stuff and I like that all right so it's

00:32:18,000 --> 00:32:21,659
not that the vendors are evil you know

00:32:19,919 --> 00:32:23,220
what Intel would do exactly the same

00:32:21,659 --> 00:32:25,710
thing if we thought we could get away

00:32:23,220 --> 00:32:27,059
with it and maybe sometimes we try it I

00:32:25,710 --> 00:32:30,600
won't go there though because I work for

00:32:27,059 --> 00:32:32,549
them but in other words don't take this

00:32:30,600 --> 00:32:34,919
as me saying and video you're evil and

00:32:32,549 --> 00:32:37,080
you're wrong okay I don't want that

00:32:34,919 --> 00:32:38,880
message because it's not fair they're

00:32:37,080 --> 00:32:40,620
doing what they should which is how do

00:32:38,880 --> 00:32:43,740
we make as much money as possible from

00:32:40,620 --> 00:32:46,559
these customers so it really is up to

00:32:43,740 --> 00:32:48,809
you applications community and I'll come

00:32:46,559 --> 00:32:52,880
back to that later all right whoo back

00:32:48,809 --> 00:32:55,110
back to Alice's slides okay so an

00:32:52,880 --> 00:32:55,360
application programmer when you look at

00:32:55,110 --> 00:32:57,010
the

00:32:55,360 --> 00:32:59,410
world and you look at where application

00:32:57,010 --> 00:33:01,870
programmers are try and think hard about

00:32:59,410 --> 00:33:03,340
the world that they see they have these

00:33:01,870 --> 00:33:05,950
different programming models they're

00:33:03,340 --> 00:33:08,679
tied to different architectures you know

00:33:05,950 --> 00:33:10,240
openmp doesn't work with fpgas yet and

00:33:08,679 --> 00:33:12,220
if you're tied to that you're stuck with

00:33:10,240 --> 00:33:13,500
whatever opencl environment or whatever

00:33:12,220 --> 00:33:15,940
you have to work with their I mean depth

00:33:13,500 --> 00:33:17,740
application programmers live in fear

00:33:15,940 --> 00:33:19,420
that the language they're going to

00:33:17,740 --> 00:33:21,429
choose and they spend a lot of time

00:33:19,420 --> 00:33:23,799
learning this language that it's going

00:33:21,429 --> 00:33:25,210
to become unusable for the link for the

00:33:23,799 --> 00:33:28,210
platform they need to move to and

00:33:25,210 --> 00:33:31,630
they're going to be stuck so at any rate

00:33:28,210 --> 00:33:32,890
you really need open standards you

00:33:31,630 --> 00:33:35,380
really need open vendor-neutral

00:33:32,890 --> 00:33:37,679
standards and you need the community

00:33:35,380 --> 00:33:40,059
instead of running off and creating new

00:33:37,679 --> 00:33:42,460
programming models or running off and

00:33:40,059 --> 00:33:44,830
supporting new proprietary standards you

00:33:42,460 --> 00:33:46,840
need the community to commit to the hard

00:33:44,830 --> 00:33:48,970
work of supporting the open standards

00:33:46,840 --> 00:33:50,710
and it can be hard work because you

00:33:48,970 --> 00:33:53,049
never get it right out of the shoes and

00:33:50,710 --> 00:33:54,850
rather than just walk away and go what

00:33:53,049 --> 00:33:56,620
you guys don't support accelerators I'm

00:33:54,850 --> 00:33:58,780
going to go and create my own standard

00:33:56,620 --> 00:34:00,040
no you roll up the sleeves you buckle

00:33:58,780 --> 00:34:01,690
down and you figure out how to get the

00:34:00,040 --> 00:34:04,440
accelerator support you need into the

00:34:01,690 --> 00:34:06,910
standard so at any rate you know

00:34:04,440 --> 00:34:09,970
portability is difficult no one's saying

00:34:06,910 --> 00:34:12,550
it's easy but if we move towards open

00:34:09,970 --> 00:34:14,530
standards we can at least build a

00:34:12,550 --> 00:34:16,090
software infrastructure that will move

00:34:14,530 --> 00:34:20,050
forward and sustain us across

00:34:16,090 --> 00:34:22,480
generations of machines now people often

00:34:20,050 --> 00:34:24,970
talk about performance portability I get

00:34:22,480 --> 00:34:26,889
because of my background with open co my

00:34:24,970 --> 00:34:29,679
friends at Intel often teased me about

00:34:26,889 --> 00:34:31,899
it open seals not performance portable

00:34:29,679 --> 00:34:34,480
well we don't have performance

00:34:31,899 --> 00:34:36,460
portability today it doesn't exist it

00:34:34,480 --> 00:34:38,050
doesn't exist with serial programs all

00:34:36,460 --> 00:34:39,940
right it's not there will never be there

00:34:38,050 --> 00:34:42,220
it's never been there the issues not

00:34:39,940 --> 00:34:44,260
performance portability all right if

00:34:42,220 --> 00:34:45,639
your goal is a large fraction to peak

00:34:44,260 --> 00:34:47,800
performance you know what you're going

00:34:45,639 --> 00:34:50,609
to have to specialize period that's the

00:34:47,800 --> 00:34:52,810
way it goes but there's some pretty good

00:34:50,609 --> 00:34:55,780
performance portable environments out

00:34:52,810 --> 00:34:58,980
there opencl does a pretty darn good job

00:34:55,780 --> 00:35:02,650
and what time am I supposed to be done

00:34:58,980 --> 00:35:04,060
five after 05 after the half hour okay

00:35:02,650 --> 00:35:05,740
good then I'm not going to go through

00:35:04,060 --> 00:35:07,720
these slides in great detail but they'll

00:35:05,740 --> 00:35:08,750
be available to you I get well actually

00:35:07,720 --> 00:35:10,760
my

00:35:08,750 --> 00:35:12,970
chapter in the first volume of James

00:35:10,760 --> 00:35:16,130
excellent programming pearl books

00:35:12,970 --> 00:35:18,140
develops this example and we did a

00:35:16,130 --> 00:35:19,880
matrix multiply and I walk through in

00:35:18,140 --> 00:35:21,860
detail of how we did it none of this

00:35:19,880 --> 00:35:24,080
it's an exercise for the reader we show

00:35:21,860 --> 00:35:26,330
how we transform the loops we shall we

00:35:24,080 --> 00:35:28,790
turn it into a multiplication over

00:35:26,330 --> 00:35:30,470
blocks we explain how this code works

00:35:28,790 --> 00:35:33,230
now if any of you have done dense linear

00:35:30,470 --> 00:35:36,470
algebra you know how long and complex a

00:35:33,230 --> 00:35:38,150
DNS gem can get right all right this is

00:35:36,470 --> 00:35:39,800
nothing compared to how complex it can

00:35:38,150 --> 00:35:41,480
get it looks a little complicated

00:35:39,800 --> 00:35:44,300
because I have to put logic in the

00:35:41,480 --> 00:35:46,190
opencl code to figure out which piece of

00:35:44,300 --> 00:35:48,140
the matrix I'm operating on and to

00:35:46,190 --> 00:35:50,060
figure out what offsets I need to get to

00:35:48,140 --> 00:35:52,460
the next piece so that's what the ugly

00:35:50,060 --> 00:35:54,410
logic is all right but it's not that

00:35:52,460 --> 00:35:56,270
much and notice there's no assembly code

00:35:54,410 --> 00:35:58,640
in this anywhere I'm doing this all in

00:35:56,270 --> 00:36:01,310
the high-level curdle code and look at

00:35:58,640 --> 00:36:05,570
the performance I got all right oh gosh

00:36:01,310 --> 00:36:07,910
my little my little wit block there

00:36:05,570 --> 00:36:12,290
blocked the A&D number that wasn't

00:36:07,910 --> 00:36:14,330
deliberate once again this is what

00:36:12,290 --> 00:36:23,150
happens when you don't get to use your

00:36:14,330 --> 00:36:30,500
own laptop did delete yeah don't you

00:36:23,150 --> 00:36:32,360
have a delete key on here man okay i

00:36:30,500 --> 00:36:33,500
know i can't complain i'm in germany so

00:36:32,360 --> 00:36:35,710
of course it's gonna be a german

00:36:33,500 --> 00:36:35,710
keyboard

00:36:37,880 --> 00:36:47,360
oh my goodness okay and she's you

00:36:43,520 --> 00:36:49,130
changing you thinking yeah okay look I

00:36:47,360 --> 00:36:52,370
just want to put this up here because

00:36:49,130 --> 00:36:54,500
you know this is an amazing level of

00:36:52,370 --> 00:36:56,690
performance portability that a single

00:36:54,500 --> 00:36:59,080
program just recompiled as I move place

00:36:56,690 --> 00:37:03,740
to place doing dense linear algebra I'm

00:36:59,080 --> 00:37:09,050
getting 12 gigaflops off my silly little

00:37:03,740 --> 00:37:13,250
laptop I'm getting 74 gigaflops off my

00:37:09,050 --> 00:37:16,520
Xeon Phi 38 gigaflops after the graphics

00:37:13,250 --> 00:37:19,520
card on my laptop and 119 gigaflops off

00:37:16,520 --> 00:37:22,010
my nvidia tesla ok and this I'll get you

00:37:19,520 --> 00:37:23,870
to quit a second okay and I'm not trying

00:37:22,010 --> 00:37:26,060
I mean I know I could get more I've

00:37:23,870 --> 00:37:27,440
written professional math libraries I

00:37:26,060 --> 00:37:29,900
could you know I could have simply code

00:37:27,440 --> 00:37:31,850
the Colonel's I could call blahs okay

00:37:29,900 --> 00:37:33,620
there's a lot more I could do but that's

00:37:31,850 --> 00:37:35,150
amazing performance I've never seen that

00:37:33,620 --> 00:37:37,520
kind of performance portability out of

00:37:35,150 --> 00:37:38,990
openmp ever all right I've never seen

00:37:37,520 --> 00:37:41,120
anything with that level of performance

00:37:38,990 --> 00:37:42,860
portability so I'm trying to tell you

00:37:41,120 --> 00:37:44,360
there's there's stuff out there that

00:37:42,860 --> 00:37:45,680
does excellent performance portability

00:37:44,360 --> 00:37:54,350
and yes I is going to go on but you have

00:37:45,680 --> 00:37:57,050
a question okay I had to change one

00:37:54,350 --> 00:38:04,280
parameter for block size that's it just

00:37:57,050 --> 00:38:08,410
one all right okay well well you don't

00:38:04,280 --> 00:38:09,980
do it show me going to a GPU and a cpu

00:38:08,410 --> 00:38:13,010
getting this kind of performance

00:38:09,980 --> 00:38:16,220
Portability and that's the box actually

00:38:13,010 --> 00:38:17,870
the box i removed that's what it said it

00:38:16,220 --> 00:38:19,730
said you know I look forward to trying

00:38:17,870 --> 00:38:21,860
this with open mp4 tato because it can't

00:38:19,730 --> 00:38:25,010
do it today going from something as

00:38:21,860 --> 00:38:29,450
radically different as an NVIDIA GPU to

00:38:25,010 --> 00:38:32,660
a xeon Xeon Phi to a cpu of my

00:38:29,450 --> 00:38:34,550
laptop to the intel graphics card going

00:38:32,660 --> 00:38:36,470
across all of those and that's getting

00:38:34,550 --> 00:38:38,030
about twenty-five percent to fifty

00:38:36,470 --> 00:38:40,040
percent of the young KL number on the

00:38:38,030 --> 00:38:41,900
Intel parts pretty amazing from

00:38:40,040 --> 00:38:43,520
high-level code so performance

00:38:41,900 --> 00:38:45,500
Portability and this is a slide from

00:38:43,520 --> 00:38:47,240
Simon McIntosh Smith's showing that yes

00:38:45,500 --> 00:38:50,490
he did it with real applications as well

00:38:47,240 --> 00:38:52,260
because I understand come on guys d gem

00:38:50,490 --> 00:38:54,960
okay but the point I want to make is

00:38:52,260 --> 00:38:56,730
this focus on this mythical quest for

00:38:54,960 --> 00:38:59,150
performance portability completely

00:38:56,730 --> 00:39:01,500
misses the point because application

00:38:59,150 --> 00:39:04,410
developers who are looking to get a

00:39:01,500 --> 00:39:06,090
major fraction of peak performance know

00:39:04,410 --> 00:39:07,650
they're going to have to mess with the

00:39:06,090 --> 00:39:09,869
code they know they're going to have to

00:39:07,650 --> 00:39:11,700
optimize moving platform to platform I

00:39:09,869 --> 00:39:14,010
hear many of my friends and the

00:39:11,700 --> 00:39:16,020
programming environments I'd say who

00:39:14,010 --> 00:39:18,619
they have to maintain a different kernel

00:39:16,020 --> 00:39:20,850
for each platform it's like yeah so

00:39:18,619 --> 00:39:22,770
we've always had to do that you know I

00:39:20,850 --> 00:39:24,750
moved to different architecture I call a

00:39:22,770 --> 00:39:26,010
blog library tune to that architecture

00:39:24,750 --> 00:39:28,410
wipes they need two different kernel for

00:39:26,010 --> 00:39:30,780
each what's your problem okay the issue

00:39:28,410 --> 00:39:32,610
is not performance portability really I

00:39:30,780 --> 00:39:34,470
often hear that from people who just

00:39:32,610 --> 00:39:36,090
want to discount some new kid coming

00:39:34,470 --> 00:39:38,430
along like opencl know the issue is

00:39:36,090 --> 00:39:40,860
maintainability I have to be able to

00:39:38,430 --> 00:39:42,480
maintain a software base and that means

00:39:40,860 --> 00:39:44,340
i need a single software infrastructure

00:39:42,480 --> 00:39:47,119
that works everywhere that i can

00:39:44,340 --> 00:39:50,250
maintain and support over time and

00:39:47,119 --> 00:39:53,220
maintainability is the key and that's

00:39:50,250 --> 00:39:56,070
what we want a single common code base

00:39:53,220 --> 00:39:58,590
that is portable between platforms so i

00:39:56,070 --> 00:40:00,570
can maintain that software and not have

00:39:58,590 --> 00:40:02,970
to rewrite when i move place to place

00:40:00,570 --> 00:40:06,570
but just optimize a few kernels here and

00:40:02,970 --> 00:40:09,300
there all right I'm out of time but

00:40:06,570 --> 00:40:11,369
there is this deal we applications

00:40:09,300 --> 00:40:12,869
workshop where they're collecting

00:40:11,369 --> 00:40:15,119
information and where people are going

00:40:12,869 --> 00:40:17,640
almost any people are just dying to get

00:40:15,119 --> 00:40:20,010
their hands on open mp4 do with the

00:40:17,640 --> 00:40:22,680
target directives all right vendors

00:40:20,010 --> 00:40:24,930
please get busy we need it out there and

00:40:22,680 --> 00:40:26,940
we need it out there across platforms

00:40:24,930 --> 00:40:28,980
people are eager to play around with it

00:40:26,940 --> 00:40:31,200
the accelerator directives there's a

00:40:28,980 --> 00:40:32,940
huge amount of interest for people to

00:40:31,200 --> 00:40:34,350
play with that even though they may

00:40:32,940 --> 00:40:36,570
recognize they have to have different

00:40:34,350 --> 00:40:38,190
sources between different platforms fine

00:40:36,570 --> 00:40:40,040
if I can support everything from one

00:40:38,190 --> 00:40:45,590
code base my life is much much better

00:40:40,040 --> 00:40:48,840
okay and this is a case study on a

00:40:45,590 --> 00:40:50,010
particle and cell fusion code and we

00:40:48,840 --> 00:40:52,230
don't have time but i just wanted to

00:40:50,010 --> 00:40:55,020
point out that you know it's a real

00:40:52,230 --> 00:40:57,060
production level code they play it

00:40:55,020 --> 00:41:00,060
around with cuda and Fortran and they've

00:40:57,060 --> 00:41:01,570
even going through and experimenting

00:41:00,060 --> 00:41:05,250
with ifdef hang in between

00:41:01,570 --> 00:41:07,600
Ben ACC and openmp the idea being that

00:41:05,250 --> 00:41:08,980
OpenMP is not there yet and broad

00:41:07,600 --> 00:41:10,690
deployment for the accelerator

00:41:08,980 --> 00:41:12,610
directives but they can learn an awful

00:41:10,690 --> 00:41:14,470
lot about algorithmically what they need

00:41:12,610 --> 00:41:17,380
to do playing with open ACC today and

00:41:14,470 --> 00:41:19,030
then they can switch between them with

00:41:17,380 --> 00:41:21,820
just compiler directive so they can do a

00:41:19,030 --> 00:41:25,090
macro in the make file and generate the

00:41:21,820 --> 00:41:26,590
open a cc or the openmp and so I am

00:41:25,090 --> 00:41:28,750
looking forward to the day when I can

00:41:26,590 --> 00:41:31,000
just say throw away that open ACC and

00:41:28,750 --> 00:41:34,330
just dedicate yourself to OpenMP life

00:41:31,000 --> 00:41:36,130
will be better for all of us so common

00:41:34,330 --> 00:41:38,530
based software is absolutely critical

00:41:36,130 --> 00:41:40,720
you know we want to get towards

00:41:38,530 --> 00:41:43,600
performance portability but the real

00:41:40,720 --> 00:41:45,700
goal here is maintainability and for

00:41:43,600 --> 00:41:49,090
that the DOA is aggressive and investing

00:41:45,700 --> 00:41:51,420
in standards committees and you know we

00:41:49,090 --> 00:41:53,530
can produce the same standards it's very

00:41:51,420 --> 00:41:56,860
interesting I don't know how much time

00:41:53,530 --> 00:42:00,130
and if you spend in the MPI world but

00:41:56,860 --> 00:42:01,960
MPI is AD shared memory there's a

00:42:00,130 --> 00:42:04,200
something we have found incredibly

00:42:01,960 --> 00:42:06,970
fascinating called fine-grained MPI

00:42:04,200 --> 00:42:09,400
which allows me to do the sort of stuff

00:42:06,970 --> 00:42:12,130
I used to have to do openmp with for

00:42:09,400 --> 00:42:14,590
managing lightweight threads on a cpu

00:42:12,130 --> 00:42:16,870
and so one model we're doing a lot of

00:42:14,590 --> 00:42:18,850
interesting to work with is there's you

00:42:16,870 --> 00:42:20,860
know the standard model of hpc is NPI

00:42:18,850 --> 00:42:23,760
plus OpenMP we're finding a lot of

00:42:20,860 --> 00:42:26,380
success with NPI plus MPI so a lot of

00:42:23,760 --> 00:42:30,670
interesting stuff going on that MPI

00:42:26,380 --> 00:42:33,640
space so it's moving very quickly so you

00:42:30,670 --> 00:42:35,920
know in summary look you know we all

00:42:33,640 --> 00:42:39,340
know one size does not fit all we made

00:42:35,920 --> 00:42:41,050
MPI work eventually but you know you

00:42:39,340 --> 00:42:43,240
just you know you have to work on

00:42:41,050 --> 00:42:47,550
different programming models as you move

00:42:43,240 --> 00:42:50,020
around and it may be that no one

00:42:47,550 --> 00:42:51,850
heterogeneous architecture you know

00:42:50,020 --> 00:42:54,640
programming architecture will cover all

00:42:51,850 --> 00:42:56,560
all environments you know it it's not

00:42:54,640 --> 00:42:57,790
necessarily the case that you're gonna

00:42:56,560 --> 00:42:59,440
have to do everything with just one

00:42:57,790 --> 00:43:01,060
programming model but we need to get

00:42:59,440 --> 00:43:05,140
close there and whatever we use needs to

00:43:01,060 --> 00:43:06,940
be standard all right now I just want to

00:43:05,140 --> 00:43:08,770
summarize with the comments I made I

00:43:06,940 --> 00:43:10,150
think we're in bad shape and i think

00:43:08,770 --> 00:43:11,800
it's the fault of application

00:43:10,150 --> 00:43:13,930
programmers we live in a market economy

00:43:11,800 --> 00:43:14,710
and you have to recognize that your

00:43:13,930 --> 00:43:16,030
interests

00:43:14,710 --> 00:43:18,369
are not always the same as vendors

00:43:16,030 --> 00:43:19,960
interests all right not because the

00:43:18,369 --> 00:43:22,270
vendors are bad but because they want to

00:43:19,960 --> 00:43:24,190
make money so anytime you reward a

00:43:22,270 --> 00:43:26,170
vendor for bad behavior you deserve what

00:43:24,190 --> 00:43:28,240
you get all right don't come whining to

00:43:26,170 --> 00:43:31,180
me you deserve what you get open ACC and

00:43:28,240 --> 00:43:35,320
cuda programmers all right so what I am

00:43:31,180 --> 00:43:37,660
calling for is a revolution I want you

00:43:35,320 --> 00:43:40,839
application programmers to unite and

00:43:37,660 --> 00:43:42,820
take control of your destiny and stop

00:43:40,839 --> 00:43:44,410
letting vendors push you around by

00:43:42,820 --> 00:43:47,950
getting to use to use proprietary

00:43:44,410 --> 00:43:50,200
standards as we found in the 90s when

00:43:47,950 --> 00:43:52,540
the applications community united behind

00:43:50,200 --> 00:43:55,720
open industry standards we had a better

00:43:52,540 --> 00:43:58,540
world please do that today I work for a

00:43:55,720 --> 00:44:01,270
vendor I can't do it for you only you

00:43:58,540 --> 00:44:02,980
can do it and until you realize that you

00:44:01,270 --> 00:44:05,580
have real power when you join together

00:44:02,980 --> 00:44:07,869
we're going to continue with this

00:44:05,580 --> 00:44:10,089
throwing out new programming models all

00:44:07,869 --> 00:44:16,619
the time and we all need that to stop

00:44:10,089 --> 00:44:16,619
okay that's it I think well that's it

00:44:25,769 --> 00:44:31,839
um you know I don't know there's so much

00:44:29,890 --> 00:44:34,359
more work to do we've we've actually had

00:44:31,839 --> 00:44:37,930
some conversations with some of the

00:44:34,359 --> 00:44:40,390
folks in in the in the do a National

00:44:37,930 --> 00:44:42,339
Labs who've come on and said that look

00:44:40,390 --> 00:44:45,160
we have to use this barrier centric

00:44:42,339 --> 00:44:46,390
programming model we have no choice it's

00:44:45,160 --> 00:44:47,890
because of the way the codes been

00:44:46,390 --> 00:44:50,079
factored and we're not going to refactor

00:44:47,890 --> 00:44:51,940
our codes radically so vendors can you

00:44:50,079 --> 00:44:54,160
please have a runtime and compiler that

00:44:51,940 --> 00:44:56,019
removes the barriers for us can you have

00:44:54,160 --> 00:44:57,609
an analysis that you do that says oh

00:44:56,019 --> 00:44:59,259
this barrier is redundant I can

00:44:57,609 --> 00:45:01,089
understand that much as we have

00:44:59,259 --> 00:45:02,799
compilers that look at data dependency

00:45:01,089 --> 00:45:04,809
analysis figures out how to vectorize

00:45:02,799 --> 00:45:06,640
couldn't we build compilers and run

00:45:04,809 --> 00:45:09,579
times that remove those access barriers

00:45:06,640 --> 00:45:11,799
and I think that's something that has to

00:45:09,579 --> 00:45:12,640
be looked into I don't know if that's

00:45:11,799 --> 00:45:14,740
something that can be done at the

00:45:12,640 --> 00:45:15,940
standard level you know that's something

00:45:14,740 --> 00:45:17,769
though they can definitely be done at

00:45:15,940 --> 00:45:23,049
the runtime and quality of implantation

00:45:17,769 --> 00:45:24,730
I don't know I don't know I'm an

00:45:23,049 --> 00:45:30,309
application program or not a compiler

00:45:24,730 --> 00:45:32,980
person so so you know it's an

00:45:30,309 --> 00:45:36,940
interesting question is is the

00:45:32,980 --> 00:45:39,670
specification as written will it support

00:45:36,940 --> 00:45:42,670
that sort of thing I mean this is this

00:45:39,670 --> 00:45:46,089
is the criticism that people have at

00:45:42,670 --> 00:45:48,279
OpenMP and I first heard it from arch

00:45:46,089 --> 00:45:53,109
way back when he was first starting work

00:45:48,279 --> 00:45:55,809
on TV b.arch robison how OpenMP exposes

00:45:53,109 --> 00:45:57,369
the threads and it doesn't say it would

00:45:55,809 --> 00:45:59,140
be nice to have some threads it says you

00:45:57,369 --> 00:46:00,609
will have some threads and it doesn't

00:45:59,140 --> 00:46:02,019
say it would be nice if the data laid

00:46:00,609 --> 00:46:03,579
out like this to the threads it said you

00:46:02,019 --> 00:46:06,339
will have the data laid out like this

00:46:03,579 --> 00:46:08,980
which means you've tied the hands of the

00:46:06,339 --> 00:46:11,349
runtime to do an awful lot of work like

00:46:08,980 --> 00:46:12,430
work stealing like using different

00:46:11,349 --> 00:46:15,460
numbers of threads you can have

00:46:12,430 --> 00:46:17,450
composability so there's a philosophical

00:46:15,460 --> 00:46:20,510
gap there

00:46:17,450 --> 00:46:23,089
and you know it could be OpenMP we've

00:46:20,510 --> 00:46:24,770
gone too far and it may be we need to

00:46:23,089 --> 00:46:27,800
add in the spec I've been thinking about

00:46:24,770 --> 00:46:29,900
this you know we have the num threads

00:46:27,800 --> 00:46:33,320
claws on a parallel region that can take

00:46:29,900 --> 00:46:36,710
and in integer expression what if it

00:46:33,320 --> 00:46:38,359
could take a handle you know Otto you

00:46:36,710 --> 00:46:40,880
know you give it a keyword which is your

00:46:38,359 --> 00:46:43,550
programmers way of saying runtime system

00:46:40,880 --> 00:46:46,400
I would like you to get really really

00:46:43,550 --> 00:46:49,339
aggressive and it's okay if you relax

00:46:46,400 --> 00:46:51,619
the rules on the threads and exposing

00:46:49,339 --> 00:46:53,420
the threads I mean I don't know I think

00:46:51,619 --> 00:46:55,820
that's a conversation though that the

00:46:53,420 --> 00:46:57,290
standards committee should have because

00:46:55,820 --> 00:46:59,690
you know the folks coming outside

00:46:57,290 --> 00:47:01,940
looking at more descriptive approaches

00:46:59,690 --> 00:47:03,589
they're onto something and we're seeing

00:47:01,940 --> 00:47:04,910
this in the exascale community you know

00:47:03,589 --> 00:47:06,650
if you look at all the work towards a

00:47:04,910 --> 00:47:08,720
synchronous multitasking for extreme

00:47:06,650 --> 00:47:10,820
scalability it also comes down to that

00:47:08,720 --> 00:47:13,160
same thing is if you overprescribe

00:47:10,820 --> 00:47:15,859
everything you don't give the runtime

00:47:13,160 --> 00:47:18,470
room to modifying and adjust the

00:47:15,859 --> 00:47:22,700
hardware going away and coming back so I

00:47:18,470 --> 00:47:24,710
mean it's something to think about no

00:47:22,700 --> 00:47:28,069
other no other questions I wasn't

00:47:24,710 --> 00:47:32,230
controversial enough I'll try harder

00:47:28,069 --> 00:47:32,230
next time really

00:47:34,230 --> 00:47:41,540
oh no I don't want to sound like him

00:47:38,540 --> 00:47:41,540

YouTube URL: https://www.youtube.com/watch?v=_CBMRNUIzTE


