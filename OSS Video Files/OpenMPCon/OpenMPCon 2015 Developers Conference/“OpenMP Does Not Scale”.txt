Title: “OpenMP Does Not Scale”
Publication date: 2015-12-13
Playlist: OpenMPCon 2015 Developers Conference
Description: 
	Ruud van der Pas, Oracle Corporation
OpenMP Con 2015 - Aachen Germany - September 2015
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2015-ruud-scaling.pdf

Abstract: Unfortunately it is a very widespread myth that OpenMP Does Not Scale – a myth we intend to dispel in this talk.Every parallel system has its strengths and weaknesses. This is true for clustered systems, but also for shared memory parallel computers.  While nobody in their right mind would consider sending one zillion single byte messages to a single node in a cluster, people do the equivalent in OpenMP and then blame the programming model.  Also, shared memory parallel systems have some specific features that one needs to be aware of. Few do though. In this talk we use real-life case studies based on actual applications to show why an application did not scale and what was done to change this.  More often than not, a relatively simple modification, or even a system level setting, makes all the difference.
Captions: 
	00:00:18,779 --> 00:00:24,810
thank you very much I start with a

00:00:22,570 --> 00:00:27,900
little commercial break that will be

00:00:24,810 --> 00:00:31,269
very short and it will be very technical

00:00:27,900 --> 00:00:34,019
it's just to show you what we what we're

00:00:31,269 --> 00:00:36,460
doing in in spark few people know

00:00:34,019 --> 00:00:41,170
everybody's pretty much away about new

00:00:36,460 --> 00:00:43,210
Intel IBM maybe but I wanted to just

00:00:41,170 --> 00:00:45,729
show you I will show some results on

00:00:43,210 --> 00:00:49,239
this processor that's what we that's

00:00:45,729 --> 00:00:52,440
what we currently sell it has a 16 cores

00:00:49,239 --> 00:00:56,049
on one chip it runs at 3.6 gigahertz

00:00:52,440 --> 00:01:00,039
each core has eight threads so in total

00:00:56,049 --> 00:01:02,559
you get 128 threads in one chip and this

00:01:00,039 --> 00:01:05,560
systems this chip has been designed to

00:01:02,559 --> 00:01:07,450
go glueless eight way what that means is

00:01:05,560 --> 00:01:10,240
you can build an eight way system with a

00:01:07,450 --> 00:01:13,210
point-to-point interconnect and that

00:01:10,240 --> 00:01:16,000
that builds a thousand 24 thread machine

00:01:13,210 --> 00:01:19,090
and you'll see me showing some results

00:01:16,000 --> 00:01:20,979
on this kind of architecture that's the

00:01:19,090 --> 00:01:22,929
architecture of the system that I'll be

00:01:20,979 --> 00:01:26,020
using I'll be using some other spark

00:01:22,929 --> 00:01:30,219
systems but again this is the main

00:01:26,020 --> 00:01:32,679
machine I will focus on today we have

00:01:30,219 --> 00:01:37,810
them bigger the largest one we currently

00:01:32,679 --> 00:01:41,770
have is called M 632 that system has in

00:01:37,810 --> 00:01:44,289
one one box you get 384 course again

00:01:41,770 --> 00:01:47,649
running at 3.6 gigahertz in total three

00:01:44,289 --> 00:01:51,249
thousand 103 thousand threads and 32

00:01:47,649 --> 00:01:55,450
terabytes of memory I can tell you these

00:01:51,249 --> 00:01:58,479
are pretty nice OpenMP boxes but we have

00:01:55,450 --> 00:02:02,560
them smaller as well what's on the way

00:01:58,479 --> 00:02:04,030
to you in terms of being able to buy

00:02:02,560 --> 00:02:05,670
that is our new chip

00:02:04,030 --> 00:02:09,210
it's called m7

00:02:05,670 --> 00:02:11,039
and I have one in my hand here and I'm

00:02:09,210 --> 00:02:14,250
still surprised myself

00:02:11,039 --> 00:02:16,709
- this is a thirty-two cold chip this

00:02:14,250 --> 00:02:20,040
this little chip has tuned 256 threads

00:02:16,709 --> 00:02:22,530
in it and although we didn't disclose

00:02:20,040 --> 00:02:27,060
the speed yet it will be running faster

00:02:22,530 --> 00:02:29,610
than 3.6 gigahertz and most of you are

00:02:27,060 --> 00:02:32,970
younger than I am so you may not

00:02:29,610 --> 00:02:35,190
appreciate what is is but having tuned

00:02:32,970 --> 00:02:37,860
256 threads in my hand is still still

00:02:35,190 --> 00:02:40,040
something that surprises me myself again

00:02:37,860 --> 00:02:43,050
it's been designed to go into a clueless

00:02:40,040 --> 00:02:45,000
so you can connect you can build the

00:02:43,050 --> 00:02:47,790
tune to 56 core system with the

00:02:45,000 --> 00:02:49,380
point-to-point connection you can build

00:02:47,790 --> 00:02:54,800
a larger one if you want but then you'll

00:02:49,380 --> 00:02:54,800
have an extra layer of interconnect we

00:03:07,030 --> 00:03:12,709
step better ah thank you thank you the

00:03:11,060 --> 00:03:14,980
sound here is a you know for me and

00:03:12,709 --> 00:03:18,349
strange anyhow okay thank you

00:03:14,980 --> 00:03:20,180
so does a new chord s4 and I certainly

00:03:18,349 --> 00:03:22,670
not going to talk about that this is not

00:03:20,180 --> 00:03:26,330
a product presentation this is pretty

00:03:22,670 --> 00:03:28,849
much saying the same what I want to

00:03:26,330 --> 00:03:31,430
point out is a new trend in hardware

00:03:28,849 --> 00:03:33,560
it's not only about performance anymore

00:03:31,430 --> 00:03:36,080
when when new chips comes out you know

00:03:33,560 --> 00:03:37,849
have extra space on the die and instead

00:03:36,080 --> 00:03:41,269
of keeping on cranking up the

00:03:37,849 --> 00:03:43,010
performance chip vendors start looking

00:03:41,269 --> 00:03:44,480
at different functionality in our case

00:03:43,010 --> 00:03:47,390
these are called what we call the data

00:03:44,480 --> 00:03:50,360
and the Linux accelerators or dax these

00:03:47,390 --> 00:03:52,430
are our hardware accelerators to speed

00:03:50,360 --> 00:03:55,580
up certain queries in the database so

00:03:52,430 --> 00:03:57,650
it's a database acceleration and it is

00:03:55,580 --> 00:03:59,470
it isn't an hardware accelerator on the

00:03:57,650 --> 00:04:01,760
chip each chip has eight of those

00:03:59,470 --> 00:04:04,250
therefore pipelines are all together you

00:04:01,760 --> 00:04:06,890
got 32 hardware acceleration pipelines

00:04:04,250 --> 00:04:12,530
on that little chip that I was just

00:04:06,890 --> 00:04:14,750
showing so one thing I wanted to just

00:04:12,530 --> 00:04:16,579
very briefly show you you can catch me

00:04:14,750 --> 00:04:19,280
in a break I'll be around all week here

00:04:16,579 --> 00:04:22,400
to talk more about it is what we do in

00:04:19,280 --> 00:04:24,320
the hardware now we can detect basically

00:04:22,400 --> 00:04:26,479
when you do something nasty with memory

00:04:24,320 --> 00:04:28,520
actually all of you know the problem

00:04:26,479 --> 00:04:31,729
where your allocates of memory and you

00:04:28,520 --> 00:04:33,950
access outside of the bounds and the

00:04:31,729 --> 00:04:35,930
worst ones are where you access just a

00:04:33,950 --> 00:04:37,550
few elements out of the bounds because

00:04:35,930 --> 00:04:39,169
typically you get away with it your

00:04:37,550 --> 00:04:42,139
program won't crash but you'll pick up

00:04:39,169 --> 00:04:45,080
some garbage data there's also a lot of

00:04:42,139 --> 00:04:47,510
security issues around that so on this

00:04:45,080 --> 00:04:49,610
chip but what we have is each data block

00:04:47,510 --> 00:04:52,250
has what we call a collar or tag or

00:04:49,610 --> 00:04:54,139
label and while you allocate data you

00:04:52,250 --> 00:04:57,669
get you can assign a certain color and

00:04:54,139 --> 00:05:00,919
what we do in the hardware we detect

00:04:57,669 --> 00:05:02,870
access violations and you're all you

00:05:00,919 --> 00:05:04,970
know smart people or computer scientists

00:05:02,870 --> 00:05:08,479
so I don't need to say much about it

00:05:04,970 --> 00:05:12,250
here's an example I have application a

00:05:08,479 --> 00:05:15,130
has the green one so it can safely load

00:05:12,250 --> 00:05:18,500
anything that has the right color and

00:05:15,130 --> 00:05:19,610
store but something bad will happen when

00:05:18,500 --> 00:05:21,620
it tries to access

00:05:19,610 --> 00:05:25,189
a wrong caller and again the key thing

00:05:21,620 --> 00:05:27,800
is this is detected at hardware speed so

00:05:25,189 --> 00:05:30,229
you get really fast and no real time

00:05:27,800 --> 00:05:33,860
checking interesting development from

00:05:30,229 --> 00:05:36,949
the hardware going to more slides on it

00:05:33,860 --> 00:05:40,370
we just announced a a lower cost

00:05:36,949 --> 00:05:43,069
processor and the key thing here is what

00:05:40,370 --> 00:05:45,979
we did we took that m7 processor just

00:05:43,069 --> 00:05:48,349
talked about 32 cores we put eight cores

00:05:45,979 --> 00:05:50,449
onto a chip and with the extra space

00:05:48,349 --> 00:05:52,189
that we have it allowed us to pull the

00:05:50,449 --> 00:05:54,740
memory controller on the chip so you'll

00:05:52,189 --> 00:05:57,139
get lower latency and it has integrated

00:05:54,740 --> 00:05:59,870
InfiniBand on the chip this is really

00:05:57,139 --> 00:06:03,199
designed for clustering so it's a

00:05:59,870 --> 00:06:05,449
nice-nice chip and as a as this slide is

00:06:03,199 --> 00:06:08,509
showing it has the integrated memory

00:06:05,449 --> 00:06:10,430
controllers and InfiniBand on the chip

00:06:08,509 --> 00:06:12,529
so that's really really fast that's time

00:06:10,430 --> 00:06:14,719
that InfiniBand you can just you have

00:06:12,529 --> 00:06:16,580
your regular InfiniBand anyconnect but

00:06:14,719 --> 00:06:19,189
there's no overhead anymore you directly

00:06:16,580 --> 00:06:21,229
go into the chip and that's been

00:06:19,189 --> 00:06:23,750
announced and at the hot chips

00:06:21,229 --> 00:06:26,150
conference in August and again that's

00:06:23,750 --> 00:06:27,949
that's the key thing I really keep it

00:06:26,150 --> 00:06:30,800
short you can bug me for any details you

00:06:27,949 --> 00:06:33,110
like and like him I'm not really an

00:06:30,800 --> 00:06:36,110
expert on our products but these chips I

00:06:33,110 --> 00:06:38,629
know more about so ok the key of the

00:06:36,110 --> 00:06:43,129
talk will we'll start the talk right now

00:06:38,629 --> 00:06:45,860
and I gave it a challenging title OpenMP

00:06:43,129 --> 00:06:49,159
does not scale and what I'm going to

00:06:45,860 --> 00:06:51,949
show you is a bit of the dark side of

00:06:49,159 --> 00:06:53,870
OpenMP and of course after that on this

00:06:51,949 --> 00:06:58,610
beautiful day we will go back to the

00:06:53,870 --> 00:07:01,189
bright side of that of the house there's

00:06:58,610 --> 00:07:02,990
this myth and in good scientific

00:07:01,189 --> 00:07:04,550
tradition we need to first define what

00:07:02,990 --> 00:07:06,710
we're talking about that doesn't always

00:07:04,550 --> 00:07:09,740
happen but I try to be a good citizen so

00:07:06,710 --> 00:07:12,560
what's in it I mean I mean according to

00:07:09,740 --> 00:07:17,240
this this webpage is something widely

00:07:12,560 --> 00:07:20,900
believed but false right I guess you can

00:07:17,240 --> 00:07:24,080
imagine what the myth is the myth is

00:07:20,900 --> 00:07:26,539
OpenMP does not scale and as we're going

00:07:24,080 --> 00:07:29,940
to talk about for the next 40 minutes or

00:07:26,539 --> 00:07:33,300
so and what I'm going to show you

00:07:29,940 --> 00:07:35,640
is kind of an unorthodox presentation

00:07:33,300 --> 00:07:39,210
I'm going to show you an imaginary

00:07:35,640 --> 00:07:41,610
discussion with an imaginary person who

00:07:39,210 --> 00:07:45,120
comes to me and says OpenMP does not

00:07:41,610 --> 00:07:47,040
scale and I'll show you my side of the

00:07:45,120 --> 00:07:50,100
discussion only but I think it's very

00:07:47,040 --> 00:07:52,530
clear from my comments what the answers

00:07:50,100 --> 00:07:56,700
are from that imaginary person so let's

00:07:52,530 --> 00:07:59,700
get started when somebody says openmp

00:07:56,700 --> 00:08:02,700
does not scale what do you actually mean

00:07:59,700 --> 00:08:06,120
with that and a programming model cannot

00:08:02,700 --> 00:08:07,770
not scale that's just a specification

00:08:06,120 --> 00:08:09,330
that's a book that you have and that's

00:08:07,770 --> 00:08:11,760
that your cookbook that's all you can

00:08:09,330 --> 00:08:14,310
use there are things that cannot scale

00:08:11,760 --> 00:08:16,440
for example the implementation if it

00:08:14,310 --> 00:08:17,940
takes three days to create your threads

00:08:16,440 --> 00:08:21,570
yes then that's not a scalable

00:08:17,940 --> 00:08:24,510
implementation that's clear it could be

00:08:21,570 --> 00:08:26,490
the hardware for your application maybe

00:08:24,510 --> 00:08:28,680
your application has characteristics

00:08:26,490 --> 00:08:29,940
that the hardware does not support and

00:08:28,680 --> 00:08:32,460
then you don't get good parallel

00:08:29,940 --> 00:08:34,469
performance that can happen to the third

00:08:32,460 --> 00:08:38,070
one and that's often overlooked is

00:08:34,469 --> 00:08:39,599
actually you you may do something that

00:08:38,070 --> 00:08:41,190
it's not really good for performance and

00:08:39,599 --> 00:08:43,050
you probably don't even know it and

00:08:41,190 --> 00:08:45,120
again I'm going to talk about some of

00:08:43,050 --> 00:08:46,440
those things a little bit of it the dark

00:08:45,120 --> 00:08:49,140
corner or shared memory parallel

00:08:46,440 --> 00:08:51,570
programming so if you would come to me a

00:08:49,140 --> 00:08:53,220
would say well OpenMP does not scale

00:08:51,570 --> 00:08:57,240
here's some questions that I could ask

00:08:53,220 --> 00:08:59,040
so let's get the basics right so I think

00:08:57,240 --> 00:09:01,380
what you what you're trying to say is

00:08:59,040 --> 00:09:03,620
you wrote a parallel program you happen

00:09:01,380 --> 00:09:06,140
to use OpenMP and it didn't perform

00:09:03,620 --> 00:09:09,660
that's I think what you're trying to say

00:09:06,140 --> 00:09:12,410
okay did you make sure the program was

00:09:09,660 --> 00:09:15,390
fairly well optimized in sequential mode

00:09:12,410 --> 00:09:17,640
did you experiment with compiler flags

00:09:15,390 --> 00:09:21,270
and try to get the best sequential

00:09:17,640 --> 00:09:23,850
performance you didn't okay

00:09:21,270 --> 00:09:26,070
so why do you expect that program to

00:09:23,850 --> 00:09:29,520
scale because you know if it if it runs

00:09:26,070 --> 00:09:32,130
badly on on one core what do you think

00:09:29,520 --> 00:09:33,990
will happen on ten or twenty four

00:09:32,130 --> 00:09:36,030
hundred to a thousand you think of all

00:09:33,990 --> 00:09:39,330
magically go better you know my claim is

00:09:36,030 --> 00:09:42,650
a little magical ego worse so so what do

00:09:39,330 --> 00:09:45,150
you expect that you just think it should

00:09:42,650 --> 00:09:48,090
and you used all the course have you

00:09:45,150 --> 00:09:48,810
ever did you use and US law to estimate

00:09:48,090 --> 00:09:53,880
the speed-up

00:09:48,810 --> 00:09:56,490
you should expect no that's not a new EU

00:09:53,880 --> 00:09:58,020
financial bailout program that's

00:09:56,490 --> 00:10:02,550
something else and those law is

00:09:58,020 --> 00:10:04,890
something you better know about I know

00:10:02,550 --> 00:10:07,530
you can't know everything but did you

00:10:04,890 --> 00:10:10,010
use a tool to identify where spending

00:10:07,530 --> 00:10:12,480
most of your time in your program you

00:10:10,010 --> 00:10:15,990
didn't you just paralyzed all the loops

00:10:12,480 --> 00:10:19,350
in your program did you try to avoid

00:10:15,990 --> 00:10:21,390
paralyzing the inner most loops maybe in

00:10:19,350 --> 00:10:24,060
a loop nest you didn't okay

00:10:21,390 --> 00:10:26,520
did you maybe minimize the number of

00:10:24,060 --> 00:10:30,810
parallel regions you didn't it just

00:10:26,520 --> 00:10:33,030
worked fine the way it was did you look

00:10:30,810 --> 00:10:35,850
at them no wait cause to minimize the

00:10:33,030 --> 00:10:38,250
use of the barrier never heard of a

00:10:35,850 --> 00:10:41,700
variable okay well maybe you should read

00:10:38,250 --> 00:10:44,480
a little bit about that do all threads

00:10:41,700 --> 00:10:47,460
roughly perform the same amount of work

00:10:44,480 --> 00:10:50,040
you don't know but you think it's okay

00:10:47,460 --> 00:10:51,170
oh I hope you're right I hope you know

00:10:50,040 --> 00:10:54,720
what you're doing

00:10:51,170 --> 00:10:58,440
did you maximize the use of private data

00:10:54,720 --> 00:11:02,720
or did you just share most of it all you

00:10:58,440 --> 00:11:05,100
didn't sharing is easier sure yeah

00:11:02,720 --> 00:11:07,310
looks like you're using the CC Numa

00:11:05,100 --> 00:11:09,300
system did you take that into account

00:11:07,310 --> 00:11:12,240
never heard of that either

00:11:09,300 --> 00:11:15,600
that's unfortunate maybe it's for

00:11:12,240 --> 00:11:16,500
sharing affecting your performance never

00:11:15,600 --> 00:11:19,020
heard of that either

00:11:16,500 --> 00:11:22,320
maybe maybe you should learn a little

00:11:19,020 --> 00:11:25,130
bit more about these things so what did

00:11:22,320 --> 00:11:28,230
you do next to address the performance

00:11:25,130 --> 00:11:28,830
you switched to MPI oh that's

00:11:28,230 --> 00:11:30,710
interesting

00:11:28,830 --> 00:11:33,210
did that does that perform any better

00:11:30,710 --> 00:11:36,180
you don't know you're still debugging

00:11:33,210 --> 00:11:37,650
the code all right Wow okay well while

00:11:36,180 --> 00:11:40,680
you're while you're waiting for that

00:11:37,650 --> 00:11:43,260
debugging run to finish I should doesn't

00:11:40,680 --> 00:11:45,530
hang by the way and let me talk a little

00:11:43,260 --> 00:11:49,670
bit more about opening p.m. performance

00:11:45,530 --> 00:11:52,380
and that's what we're going to do that

00:11:49,670 --> 00:11:55,420
as was mentioned several times already

00:11:52,380 --> 00:11:57,610
this week the opening piece is easy too

00:11:55,420 --> 00:12:00,540
and a lot of transparency which is a

00:11:57,610 --> 00:12:02,830
good thing but it is a mixed blessing

00:12:00,540 --> 00:12:05,440
now what I like to say the fact that

00:12:02,830 --> 00:12:08,800
OpenMP is easy doesn't imply that you

00:12:05,440 --> 00:12:10,600
can be done if you write dumb code

00:12:08,800 --> 00:12:12,010
you'll get dumb performance you know

00:12:10,600 --> 00:12:13,720
compilers are limited in what they can

00:12:12,010 --> 00:12:16,360
fix certainly when it comes to the open

00:12:13,720 --> 00:12:19,750
and P level of things so again that's

00:12:16,360 --> 00:12:21,940
all going to talk about so that the easy

00:12:19,750 --> 00:12:24,790
part can be really misleading because

00:12:21,940 --> 00:12:27,730
you're jamming in parallel for or

00:12:24,790 --> 00:12:29,620
parallel do and it works in parallel but

00:12:27,730 --> 00:12:32,020
your performance may be far from optimal

00:12:29,620 --> 00:12:35,410
so performance bottlenecks may be hidden

00:12:32,020 --> 00:12:37,360
and ideally you don't you don't have

00:12:35,410 --> 00:12:39,960
that problem it just performed well but

00:12:37,360 --> 00:12:42,760
unfortunately that's not always the case

00:12:39,960 --> 00:12:46,510
and that two things I want to focus on

00:12:42,760 --> 00:12:49,540
today those are more obscure things that

00:12:46,510 --> 00:12:51,720
just happen they're silent and it can

00:12:49,540 --> 00:12:55,060
greatly impact your performance and the

00:12:51,720 --> 00:12:57,370
first one is si si Numa and the second

00:12:55,060 --> 00:12:59,620
one is for sharing just a little show of

00:12:57,370 --> 00:13:01,030
hands I promise I won't ask questions to

00:12:59,620 --> 00:13:04,390
the audience I don't like it when

00:13:01,030 --> 00:13:06,880
speakers do that but who's heard of si

00:13:04,390 --> 00:13:10,450
si Numa I'm sure most of you alright

00:13:06,880 --> 00:13:13,150
good not enough hands go up okay false

00:13:10,450 --> 00:13:14,830
sharing okay yeah I know Tim talked

00:13:13,150 --> 00:13:18,430
about it in this tutorial I saw those

00:13:14,830 --> 00:13:20,380
slides so I'll summarize it I'll be

00:13:18,430 --> 00:13:22,030
quick on it but I'll try to get to the

00:13:20,380 --> 00:13:23,950
highlight of fall sharing and so don't

00:13:22,030 --> 00:13:26,200
worry everything that is on this slide

00:13:23,950 --> 00:13:29,620
is going to be explained the funny thing

00:13:26,200 --> 00:13:33,280
is neither of these have anything to do

00:13:29,620 --> 00:13:35,290
with open people they are generic things

00:13:33,280 --> 00:13:38,710
that come with the shared memory system

00:13:35,290 --> 00:13:41,110
and what I like to say is you violate

00:13:38,710 --> 00:13:42,640
basic rules without knowing you violate

00:13:41,110 --> 00:13:45,720
them but when you talk with somebody

00:13:42,640 --> 00:13:48,250
writing let's say MPI just to pick one

00:13:45,720 --> 00:13:51,940
nobody in their right mind is going to

00:13:48,250 --> 00:13:54,010
send one zillion single byte messages to

00:13:51,940 --> 00:13:55,930
one node you just don't do that you

00:13:54,010 --> 00:13:57,190
don't even think about doing it because

00:13:55,930 --> 00:13:59,530
you know the whole system will collapse

00:13:57,190 --> 00:14:01,420
and die and your performance will be

00:13:59,530 --> 00:14:04,220
gone now people do the equivalent in

00:14:01,420 --> 00:14:06,860
openmp and they have no idea what's kind

00:14:04,220 --> 00:14:08,660
hitting them so that's what we're going

00:14:06,860 --> 00:14:10,730
to talk about but remember this has

00:14:08,660 --> 00:14:12,830
nothing to do with openmp do you happen

00:14:10,730 --> 00:14:15,080
to type in OpenMP and that's how you see

00:14:12,830 --> 00:14:17,420
it but it's an artifact of using a ship

00:14:15,080 --> 00:14:18,950
memory system so one day I hope to have

00:14:17,420 --> 00:14:21,110
some time and write a piece that's

00:14:18,950 --> 00:14:23,180
example there showing you exactly the

00:14:21,110 --> 00:14:27,980
same just to demonstrate that it's not

00:14:23,180 --> 00:14:31,450
related to opening so let's look at for

00:14:27,980 --> 00:14:35,510
sharing first all these modern systems

00:14:31,450 --> 00:14:37,400
are cache based so what you have in this

00:14:35,510 --> 00:14:39,680
is a very simplified diagram is you've

00:14:37,400 --> 00:14:41,680
got a bunch of CPUs or cores or whatever

00:14:39,680 --> 00:14:44,210
something during your computation and

00:14:41,680 --> 00:14:45,830
there's the main memory that has your

00:14:44,210 --> 00:14:49,460
data and in between other caches I'm

00:14:45,830 --> 00:14:51,950
quite sure all of you know that what

00:14:49,460 --> 00:14:55,370
happens is the unit of transfer in that

00:14:51,950 --> 00:14:58,430
system is a line a line isn't is a

00:14:55,370 --> 00:15:01,280
fairly small chunk of bytes like 64 by

00:14:58,430 --> 00:15:03,800
228 bytes that kind of range and

00:15:01,280 --> 00:15:07,670
whenever you need an element that whole

00:15:03,800 --> 00:15:10,880
line will move so that line will move

00:15:07,670 --> 00:15:14,120
from the memory into the cache so if the

00:15:10,880 --> 00:15:16,220
topmost CPU the blue one the dark blue

00:15:14,120 --> 00:15:17,720
one needs the light blue element it will

00:15:16,220 --> 00:15:20,510
get the line that has that light blue

00:15:17,720 --> 00:15:23,630
element likewise if the other CPU needs

00:15:20,510 --> 00:15:25,610
something yellow and yellow happens to

00:15:23,630 --> 00:15:27,680
be part of the same cache line it gets a

00:15:25,610 --> 00:15:30,260
copy of the same line so it's very

00:15:27,680 --> 00:15:32,480
natural to have the situation where you

00:15:30,260 --> 00:15:34,820
have multiple copies of the same line in

00:15:32,480 --> 00:15:37,090
different places of the system floating

00:15:34,820 --> 00:15:40,130
around that that happens all the time

00:15:37,090 --> 00:15:43,850
the thing is what what happens when you

00:15:40,130 --> 00:15:46,670
modify an element in that line so what

00:15:43,850 --> 00:15:50,810
if the bottom CPU changes the yellow

00:15:46,670 --> 00:15:52,820
value into something else now I still

00:15:50,810 --> 00:15:57,200
have three copies of that line but are

00:15:52,820 --> 00:15:59,480
inconsistent that's the thing that the

00:15:57,200 --> 00:16:02,120
hardware keeps track of of that's called

00:15:59,480 --> 00:16:04,700
cache coherence and what it will do it

00:16:02,120 --> 00:16:07,790
does some bits associated with each line

00:16:04,700 --> 00:16:10,370
and these bits they then they give the

00:16:07,790 --> 00:16:12,290
status of that line and what the

00:16:10,370 --> 00:16:14,780
hardware will do it will flag those

00:16:12,290 --> 00:16:15,470
other copies as invalid you can't use

00:16:14,780 --> 00:16:18,510
them

00:16:15,470 --> 00:16:20,640
now we know that the blow element has

00:16:18,510 --> 00:16:22,860
not been modified we know that but the

00:16:20,640 --> 00:16:25,140
hardware can tell because you keep track

00:16:22,860 --> 00:16:26,850
of this at a level of a cache line not

00:16:25,140 --> 00:16:29,640
at the byte level that will be too

00:16:26,850 --> 00:16:31,710
expensive to do so what will happen is

00:16:29,640 --> 00:16:35,070
the CPU that needs the blue element

00:16:31,710 --> 00:16:36,870
actually will reload the line because

00:16:35,070 --> 00:16:39,089
again it can't tell that the blue

00:16:36,870 --> 00:16:41,010
element hasn't we modified you it could

00:16:39,089 --> 00:16:44,910
safely read it if you would exactly know

00:16:41,010 --> 00:16:45,920
what's going on now that happens all the

00:16:44,910 --> 00:16:49,880
time

00:16:45,920 --> 00:16:53,010
problems start when you do that on a

00:16:49,880 --> 00:16:54,620
large scale because that cache line will

00:16:53,010 --> 00:16:57,089
start jumping through your machine

00:16:54,620 --> 00:16:58,800
because that other CPU will need the

00:16:57,089 --> 00:17:00,240
fresh cache line so it'll move from one

00:16:58,800 --> 00:17:02,160
cache in rain will go through memory

00:17:00,240 --> 00:17:03,720
first the memory will get a copy so you

00:17:02,160 --> 00:17:06,000
get a lot of extra traffic in your

00:17:03,720 --> 00:17:08,339
system that will cost you performance

00:17:06,000 --> 00:17:09,750
and again this happens all the time but

00:17:08,339 --> 00:17:13,079
if it's in the heart of your algorithm

00:17:09,750 --> 00:17:15,240
you're in trouble so when does it happen

00:17:13,079 --> 00:17:18,179
first of all you talk about modifying

00:17:15,240 --> 00:17:22,140
data as long as you read there's no

00:17:18,179 --> 00:17:24,150
false sharing those as soon as you have

00:17:22,140 --> 00:17:26,970
a store this whole thing starts if you

00:17:24,150 --> 00:17:28,800
just have a load nothing happens we're

00:17:26,970 --> 00:17:30,540
talking about multiple threads hitting

00:17:28,800 --> 00:17:32,850
the same cache line or maybe just a few

00:17:30,540 --> 00:17:34,080
cache lines all the time as I said in

00:17:32,850 --> 00:17:37,020
the heart of your algorithm the

00:17:34,080 --> 00:17:38,940
innermost loop hits on the same cache

00:17:37,020 --> 00:17:40,980
line all the time that's why you have

00:17:38,940 --> 00:17:43,080
false sharing and actually false sharing

00:17:40,980 --> 00:17:45,720
really impacts your performance really

00:17:43,080 --> 00:17:48,240
quickly in terms of the number of

00:17:45,720 --> 00:17:50,010
threads I mean you can start seeing the

00:17:48,240 --> 00:17:52,110
first effects at four threads or two

00:17:50,010 --> 00:17:54,059
already and it's gone

00:17:52,110 --> 00:17:56,520
game over it with sixteen or something

00:17:54,059 --> 00:17:59,210
like that hence the recommendation use

00:17:56,520 --> 00:18:02,400
local or private data where you can't as

00:17:59,210 --> 00:18:04,110
as Mark said you know the default shared

00:18:02,400 --> 00:18:07,559
is about the worst thing you can do for

00:18:04,110 --> 00:18:09,450
performance it may be convenient but you

00:18:07,559 --> 00:18:11,370
really don't want to share any data that

00:18:09,450 --> 00:18:13,770
you don't have to share so use private

00:18:11,370 --> 00:18:15,330
data where you can and keep your mind as

00:18:13,770 --> 00:18:18,230
long as you just read it's fine then

00:18:15,330 --> 00:18:18,230
then it doesn't matter

00:18:20,360 --> 00:18:27,250
okay okay I say I got a duplicate here

00:18:24,140 --> 00:18:32,900
very interesting sorry for that

00:18:27,250 --> 00:18:34,160
CC no ma'am another thing and I I want

00:18:32,900 --> 00:18:36,770
to spend a little more time on that

00:18:34,160 --> 00:18:39,080
because that's affecting all of you it

00:18:36,770 --> 00:18:41,330
used to be that CC Numa was the

00:18:39,080 --> 00:18:43,520
exclusive domain of people having a very

00:18:41,330 --> 00:18:45,380
large share memory machine and these

00:18:43,520 --> 00:18:48,490
days ending anything with more than one

00:18:45,380 --> 00:18:51,380
socket as a CC Neumann architecture and

00:18:48,490 --> 00:18:54,799
I'll show that with examples in a minute

00:18:51,380 --> 00:18:56,900
so here's my generic system the generic

00:18:54,799 --> 00:18:59,510
system again has a processor memory I

00:18:56,900 --> 00:19:01,549
left out the caches that's that's kind

00:18:59,510 --> 00:19:05,480
of a detail we talked about memory and

00:19:01,549 --> 00:19:07,520
CPUs and what hardware vendors have been

00:19:05,480 --> 00:19:09,620
doing is they put the memory controller

00:19:07,520 --> 00:19:12,919
on the chip so you get scalable

00:19:09,620 --> 00:19:15,350
bandwidth in this design when I add a

00:19:12,919 --> 00:19:18,350
processor I automatically add memory

00:19:15,350 --> 00:19:21,169
bandwidth before that you would have a

00:19:18,350 --> 00:19:22,940
fixed interconnect so as you added CPU

00:19:21,169 --> 00:19:24,890
as you started sharing that interconnect

00:19:22,940 --> 00:19:27,350
and the bandwidth per processor got less

00:19:24,890 --> 00:19:30,890
so there's a very good architectural

00:19:27,350 --> 00:19:34,130
reason to do this so now you add a CPU

00:19:30,890 --> 00:19:36,320
and UI bandwidth that's great and what

00:19:34,130 --> 00:19:39,290
you do you have a cache coherent

00:19:36,320 --> 00:19:43,240
interconnect that can see all the caches

00:19:39,290 --> 00:19:47,080
all of memory and you glue it together

00:19:43,240 --> 00:19:50,090
but as soon as you decouple the memory

00:19:47,080 --> 00:19:51,640
you have a cc Neumann architecture so CC

00:19:50,090 --> 00:19:55,640
new must and through cache Kohan

00:19:51,640 --> 00:19:57,850
non-uniform memory access so the

00:19:55,640 --> 00:20:02,120
question is where does your data go and

00:19:57,850 --> 00:20:04,429
here's an example this this processor

00:20:02,120 --> 00:20:07,010
happens to have some data so if my

00:20:04,429 --> 00:20:09,440
thread is running on that CPU will call

00:20:07,010 --> 00:20:12,320
or whatever then I'm fine I have the

00:20:09,440 --> 00:20:13,910
fastest memory access I can get that's

00:20:12,320 --> 00:20:18,559
the goal that's really what you want to

00:20:13,910 --> 00:20:21,040
do that's trouble if your thread is

00:20:18,559 --> 00:20:24,410
running on the far corner of that system

00:20:21,040 --> 00:20:26,360
not only do you need to travel longer to

00:20:24,410 --> 00:20:28,460
get your data it just takes longer if

00:20:26,360 --> 00:20:30,840
many threads do that they're hitting the

00:20:28,460 --> 00:20:32,550
same memory controller and you can

00:20:30,840 --> 00:20:34,530
an extra bottleneck there although the

00:20:32,550 --> 00:20:36,690
remote memory access time is usually the

00:20:34,530 --> 00:20:38,520
first order bad thing to happen it just

00:20:36,690 --> 00:20:41,310
takes longer to get to your data

00:20:38,520 --> 00:20:43,200
what's worse some threads may be quick

00:20:41,310 --> 00:20:46,110
because they access the data from the

00:20:43,200 --> 00:20:47,910
local memory and other threads access it

00:20:46,110 --> 00:20:50,820
from remote memory and they all wait in

00:20:47,910 --> 00:20:52,680
that barrier and a wait for the longest

00:20:50,820 --> 00:20:54,920
one to arrive so you got a load in

00:20:52,680 --> 00:20:57,300
moments but this is really bad

00:20:54,920 --> 00:21:00,330
how do you how do you avoid that from

00:20:57,300 --> 00:21:03,290
happening if you can okay that's what

00:21:00,330 --> 00:21:05,640
I'll talk about and as I said every

00:21:03,290 --> 00:21:07,350
system with more than one socket has a

00:21:05,640 --> 00:21:08,720
cc new architecture if you have two

00:21:07,350 --> 00:21:10,950
socket system it is a system

00:21:08,720 --> 00:21:12,270
architecture if you have a 16 socket

00:21:10,950 --> 00:21:14,010
system it is this use you know my

00:21:12,270 --> 00:21:16,860
architecture so this affects everybody

00:21:14,010 --> 00:21:19,230
and that's why it's so good that as of

00:21:16,860 --> 00:21:21,720
opening p4 at all their support for CC

00:21:19,230 --> 00:21:25,280
Numa I'm hope you're all aware of that

00:21:21,720 --> 00:21:27,810
through the various tutorials given and

00:21:25,280 --> 00:21:29,550
they're to environment variables where

00:21:27,810 --> 00:21:32,640
you can specify what you want to use

00:21:29,550 --> 00:21:34,320
cores and threads and sockets and how

00:21:32,640 --> 00:21:35,910
you want to bind your threads to that

00:21:34,320 --> 00:21:37,710
system you want to keep them close you

00:21:35,910 --> 00:21:39,210
want to spread them out again that's a

00:21:37,710 --> 00:21:41,070
topic in itself I will not talk about

00:21:39,210 --> 00:21:43,740
that I'm talking about performance here

00:21:41,070 --> 00:21:47,610
but that's how you can control this in

00:21:43,740 --> 00:21:49,260
openmp now remember ultimately the

00:21:47,610 --> 00:21:51,830
placement is under control of the

00:21:49,260 --> 00:21:55,020
operating system and that's a particular

00:21:51,830 --> 00:21:57,690
placement of your data and I'll talk

00:21:55,020 --> 00:21:59,850
about that because all these OSS they

00:21:57,690 --> 00:22:02,070
use what it's called first touch what is

00:21:59,850 --> 00:22:05,430
first touch first touch means that the

00:22:02,070 --> 00:22:08,490
thread that initializes the data will

00:22:05,430 --> 00:22:10,380
own it why would you want to do that

00:22:08,490 --> 00:22:12,720
well that's really good for sequential

00:22:10,380 --> 00:22:14,550
performance when you're running one

00:22:12,720 --> 00:22:17,700
application with one thread you want

00:22:14,550 --> 00:22:20,700
that data to be close to you so whenever

00:22:17,700 --> 00:22:22,680
you initialize it you want to own it you

00:22:20,700 --> 00:22:24,210
want every your local memory so that's

00:22:22,680 --> 00:22:26,850
why all these services have the first

00:22:24,210 --> 00:22:29,190
touch as their standard placement policy

00:22:26,850 --> 00:22:31,560
that's not necessarily what you like to

00:22:29,190 --> 00:22:33,390
have in a parallel program though you

00:22:31,560 --> 00:22:35,220
don't have all your data in one memory

00:22:33,390 --> 00:22:39,210
you want to have it scattered over the

00:22:35,220 --> 00:22:40,780
system so how do you do that again my

00:22:39,210 --> 00:22:42,730
imaginary system here

00:22:40,780 --> 00:22:45,970
and now I'm going to run this little

00:22:42,730 --> 00:22:49,180
loop I'm going to initialize a vector a

00:22:45,970 --> 00:22:50,470
very simple I got 10,000 elements and if

00:22:49,180 --> 00:22:53,470
I don't do anything

00:22:50,470 --> 00:22:56,590
one thread will execute that loop in my

00:22:53,470 --> 00:22:58,900
OpenMP program and by death by virtue of

00:22:56,590 --> 00:23:02,350
first touch that thread will have all

00:22:58,900 --> 00:23:04,030
the data in his memory and there's one

00:23:02,350 --> 00:23:06,910
thing I need a command here data

00:23:04,030 --> 00:23:10,930
placement is at the page level I didn't

00:23:06,910 --> 00:23:12,940
talk about pages at all the OS while the

00:23:10,930 --> 00:23:15,970
hardware works with cache lines the OS

00:23:12,940 --> 00:23:18,370
works with pages pages are a very large

00:23:15,970 --> 00:23:20,770
chunk of memory like 4 kilobyte a

00:23:18,370 --> 00:23:23,050
kilobyte so placement is at the page

00:23:20,770 --> 00:23:24,700
level okay that's kind of a little

00:23:23,050 --> 00:23:27,910
detail here but I just want to point it

00:23:24,700 --> 00:23:30,190
out so that that CPU has a bunch of

00:23:27,910 --> 00:23:33,730
pages in it memory in its memory that

00:23:30,190 --> 00:23:35,530
contained my vector a well that could be

00:23:33,730 --> 00:23:39,310
that could be trouble if other other

00:23:35,530 --> 00:23:41,440
threads need the data so what's my way

00:23:39,310 --> 00:23:44,710
out well my way out is used first touch

00:23:41,440 --> 00:23:46,600
to your advantage in case I for

00:23:44,710 --> 00:23:49,810
demonstration purposes I'm running this

00:23:46,600 --> 00:23:51,880
on two threads so now what happens as

00:23:49,810 --> 00:23:53,710
you all should know is that one thread

00:23:51,880 --> 00:23:55,090
will take half of the iterations and the

00:23:53,710 --> 00:23:57,040
other thread will take the next half of

00:23:55,090 --> 00:23:59,920
the iterations so they'll split that

00:23:57,040 --> 00:24:02,020
loop and both of them are supposedly

00:23:59,920 --> 00:24:04,720
running parallel and they get half of

00:24:02,020 --> 00:24:07,720
the data in them that way I distribute

00:24:04,720 --> 00:24:09,520
the data over my system the assumption

00:24:07,720 --> 00:24:12,570
is is that in the remainder that's

00:24:09,520 --> 00:24:18,070
that's where I'm going to need my data

00:24:12,570 --> 00:24:21,240
so that's cc Numa in a nutshell so now I

00:24:18,070 --> 00:24:21,240
want to look at some real cases

00:24:23,840 --> 00:24:30,740
alright y'all got some sort of name and

00:24:27,470 --> 00:24:32,450
this is about my personal space here's

00:24:30,740 --> 00:24:33,860
my favorite album in a matrix times

00:24:32,450 --> 00:24:36,680
vector it's an amazingly simple

00:24:33,860 --> 00:24:39,530
algorithm and it's amazing what you need

00:24:36,680 --> 00:24:42,380
to do to make it run well from a

00:24:39,530 --> 00:24:45,920
sequential point of view certainly you

00:24:42,380 --> 00:24:47,930
know the loop unrolling and all blocking

00:24:45,920 --> 00:24:49,460
and whatever so you got to do a lot of

00:24:47,930 --> 00:24:51,170
things even for that simple algorithm

00:24:49,460 --> 00:24:54,050
here we'll focus on the parallel side

00:24:51,170 --> 00:24:56,410
the parallel side is easy what I'm doing

00:24:54,050 --> 00:24:59,510
here this is a textbook implementation i

00:24:56,410 --> 00:25:01,460
compute results by taking the dot

00:24:59,510 --> 00:25:04,640
product of the rows of the matrix times

00:25:01,460 --> 00:25:06,890
the vector and it's highly parallel

00:25:04,640 --> 00:25:09,470
because all these dot products over the

00:25:06,890 --> 00:25:12,320
rows can be executed in parallel that's

00:25:09,470 --> 00:25:13,730
what the color code says this one can go

00:25:12,320 --> 00:25:16,850
off in parallel with this one it's

00:25:13,730 --> 00:25:18,980
embarrassingly parallel lecture and I

00:25:16,850 --> 00:25:22,010
hope that's clear that's so I can

00:25:18,980 --> 00:25:24,140
paralyze that outer loop and every

00:25:22,010 --> 00:25:26,840
self-respecting although parallelizing

00:25:24,140 --> 00:25:28,190
compiler will do that for you that's but

00:25:26,840 --> 00:25:29,900
if you don't have that or you don't use

00:25:28,190 --> 00:25:31,930
it and you want to use open and P you

00:25:29,900 --> 00:25:35,920
with jamming or open P parallel for

00:25:31,930 --> 00:25:39,980
above that red look there like this

00:25:35,920 --> 00:25:42,740
really easy and no no catches here

00:25:39,980 --> 00:25:46,280
that's how you would do the

00:25:42,740 --> 00:25:47,960
parallelization and I ran it and I run

00:25:46,280 --> 00:25:49,520
it on older hardware because that was

00:25:47,960 --> 00:25:51,080
current hardware back then when I ran it

00:25:49,520 --> 00:25:52,460
but I'd like to keep it around because I

00:25:51,080 --> 00:25:55,700
want what I'm going to show you is

00:25:52,460 --> 00:25:57,380
generic and that it has been there for a

00:25:55,700 --> 00:25:59,270
while so don't look at these product

00:25:57,380 --> 00:26:02,330
names this was just a chip that I used

00:25:59,270 --> 00:26:04,460
and I ran it and what I did I increased

00:26:02,330 --> 00:26:07,750
the size of the matrix that's the

00:26:04,460 --> 00:26:11,480
horizontal axis that's a log scale and

00:26:07,750 --> 00:26:14,060
I'm I'm plotting the speed in mega flops

00:26:11,480 --> 00:26:16,160
or whatever kind of speed no higher is

00:26:14,060 --> 00:26:19,070
better and there's a couple of things to

00:26:16,160 --> 00:26:21,140
note here first of all a little side

00:26:19,070 --> 00:26:23,180
thing I couldn't resist to put in if

00:26:21,140 --> 00:26:24,039
your matrix is too small the overhead of

00:26:23,180 --> 00:26:26,109
OpenMP

00:26:24,039 --> 00:26:29,139
you know that's actually one thing maybe

00:26:26,109 --> 00:26:30,999
mark can add to his his checklist was

00:26:29,139 --> 00:26:33,729
your problem size big enough to actually

00:26:30,999 --> 00:26:35,979
make it make it worth doing openmp so

00:26:33,729 --> 00:26:37,600
for a while it's slower pervades small

00:26:35,979 --> 00:26:39,820
matrices that's why you have the if

00:26:37,600 --> 00:26:42,220
clause to get around with the if clause

00:26:39,820 --> 00:26:43,749
you can avoid it what we see is actually

00:26:42,220 --> 00:26:46,210
you know don't look at there's all these

00:26:43,749 --> 00:26:48,549
outliers just over all that you see is

00:26:46,210 --> 00:26:50,789
you get nice scalable performance in a

00:26:48,549 --> 00:26:53,080
certain part of the memory hierarchy and

00:26:50,789 --> 00:26:55,090
actually want you we would look closer

00:26:53,080 --> 00:26:58,029
at it you can even see super linear

00:26:55,090 --> 00:27:00,220
scaling it's like more than twice as

00:26:58,029 --> 00:27:01,869
fast on two threads and that's because I

00:27:00,220 --> 00:27:05,859
get more cache bit so that's that's

00:27:01,869 --> 00:27:08,590
really great the problem is here once I

00:27:05,859 --> 00:27:10,840
start running larger matrices it's game

00:27:08,590 --> 00:27:16,989
over no matter how many threads I throw

00:27:10,840 --> 00:27:18,789
at it it's it ends at two actually the

00:27:16,989 --> 00:27:20,379
maximum speed-up that I measured is only

00:27:18,789 --> 00:27:21,999
one point six and this was an

00:27:20,379 --> 00:27:25,330
embarrassingly parallel algorithm I

00:27:21,999 --> 00:27:28,239
thought we all agreed on that ah must be

00:27:25,330 --> 00:27:31,090
that opening Pina doesn't scale so let's

00:27:28,239 --> 00:27:34,090
look at it what's happening here so

00:27:31,090 --> 00:27:35,909
we've got to go under the hood so let's

00:27:34,090 --> 00:27:38,619
look at the architecture of that system

00:27:35,909 --> 00:27:41,529
so what we have we got a bunch of course

00:27:38,619 --> 00:27:43,090
in this case each core happens to have

00:27:41,529 --> 00:27:44,919
two Hardware threads but that's a detail

00:27:43,090 --> 00:27:47,710
here what we have we have a memory

00:27:44,919 --> 00:27:51,489
hierarchy with caches some shared caches

00:27:47,710 --> 00:27:53,859
and memory and memory is on a per socket

00:27:51,489 --> 00:27:56,139
basis so this is a cc Numa architecture

00:27:53,859 --> 00:27:59,139
so wait a minute how about my data

00:27:56,139 --> 00:28:02,529
placement then did I think about it no I

00:27:59,139 --> 00:28:07,749
didn't lips

00:28:02,529 --> 00:28:10,899
it's just slower yeah so what I did know

00:28:07,749 --> 00:28:13,599
now by now you should realize that CC

00:28:10,899 --> 00:28:16,090
Numa is controlled at the level way you

00:28:13,599 --> 00:28:19,599
initialize your data when you initialize

00:28:16,090 --> 00:28:21,820
your data that's it and I should copy

00:28:19,599 --> 00:28:23,950
things around so that's it so that's the

00:28:21,820 --> 00:28:25,809
stage where you have to intervene and

00:28:23,950 --> 00:28:28,720
that's what I'm doing here this is my

00:28:25,809 --> 00:28:31,929
original data initialization absolutely

00:28:28,720 --> 00:28:33,220
trivial and all the blue was added

00:28:31,929 --> 00:28:35,810
because it was a sequential data

00:28:33,220 --> 00:28:37,730
initialization so what I do I have one

00:28:35,810 --> 00:28:39,740
our region our being a good open MP

00:28:37,730 --> 00:28:43,340
citizen not all these parallel force and

00:28:39,740 --> 00:28:45,350
inside I paralyzed these loops and the

00:28:43,340 --> 00:28:48,800
one I want to call out of course is this

00:28:45,350 --> 00:28:51,890
one the double nested loop that's for

00:28:48,800 --> 00:28:53,750
the array so what you do you do reverse

00:28:51,890 --> 00:28:56,720
engineering how are you going to use the

00:28:53,750 --> 00:28:59,210
data and then how should I initialize it

00:28:56,720 --> 00:29:02,540
so that that it happens to be in the

00:28:59,210 --> 00:29:04,790
right memory when I use it now this is

00:29:02,540 --> 00:29:07,160
an easy case I'm not saying 60 Numa is

00:29:04,790 --> 00:29:10,040
easy I'm not saying CC new - pleasant

00:29:07,160 --> 00:29:11,870
but it's something to deal with so what

00:29:10,040 --> 00:29:13,310
i'm doing here reverse engineering and

00:29:11,870 --> 00:29:15,650
you gotta believe me this is how it

00:29:13,310 --> 00:29:19,060
works out I'm a bit paranoid

00:29:15,650 --> 00:29:24,830
I even initialized redundantly I

00:29:19,060 --> 00:29:28,940
initialize the M the vector a because

00:29:24,830 --> 00:29:31,280
when you when you store you gotta own

00:29:28,940 --> 00:29:33,710
that cache line too so on the store you

00:29:31,280 --> 00:29:35,960
have the same CC Neumann effects as on

00:29:33,710 --> 00:29:38,030
the loads but you can't even measure the

00:29:35,960 --> 00:29:40,940
difference I got carried away little bit

00:29:38,030 --> 00:29:43,010
but you know yeah well you when you go

00:29:40,940 --> 00:29:45,140
for scalable performance don't leave

00:29:43,010 --> 00:29:47,480
anything on the table that's another

00:29:45,140 --> 00:29:49,340
thing so but that's why I color coded

00:29:47,480 --> 00:29:52,130
red it's a little bit bit overkill in

00:29:49,340 --> 00:29:55,400
this case the vector C is interesting

00:29:52,130 --> 00:29:59,300
because all all threads need to read all

00:29:55,400 --> 00:30:01,910
of that vector C there is no right or

00:29:59,300 --> 00:30:05,180
wrong here so what what I did I

00:30:01,910 --> 00:30:07,070
paralyzed that initialization of C so

00:30:05,180 --> 00:30:09,260
that all threads get a chunk of C but I

00:30:07,070 --> 00:30:11,630
know the threads will have to do a

00:30:09,260 --> 00:30:14,570
remote read to get that all of that

00:30:11,630 --> 00:30:17,390
vector into them into that cache now

00:30:14,570 --> 00:30:19,460
luckily that's order N and hopefully

00:30:17,390 --> 00:30:21,380
it'll fit in a nearby cache now that's

00:30:19,460 --> 00:30:24,710
the assumption but but it's something

00:30:21,380 --> 00:30:26,510
gotta realize that's not an optimal

00:30:24,710 --> 00:30:31,520
solution from a memory access point of

00:30:26,510 --> 00:30:33,470
view but it helps so here's the same

00:30:31,520 --> 00:30:35,870
thing I didn't change the algorithm I

00:30:33,470 --> 00:30:38,120
only changed my data placement

00:30:35,870 --> 00:30:41,060
and now I get about double the

00:30:38,120 --> 00:30:43,000
performance eventually this is actually

00:30:41,060 --> 00:30:45,920
very bandwidth hungry algorithm

00:30:43,000 --> 00:30:47,450
certainly as written so I'm probably

00:30:45,920 --> 00:30:48,250
running out of bandwidth here but I

00:30:47,450 --> 00:30:50,200
definitely

00:30:48,250 --> 00:30:53,230
twice the performance by the relatively

00:30:50,200 --> 00:30:54,580
simple change to show you this is

00:30:53,230 --> 00:30:57,370
generic and I'll do that very quickly

00:30:54,580 --> 00:31:00,490
I'm going to do the same on an honest to

00:30:57,370 --> 00:31:02,169
saket older spark system similar kind of

00:31:00,490 --> 00:31:04,780
architecture in a way the details are

00:31:02,169 --> 00:31:06,940
different but it's a CC Newman level

00:31:04,780 --> 00:31:09,250
it's the same idea I've got two sockets

00:31:06,940 --> 00:31:11,169
and I got a chunk of memory connected to

00:31:09,250 --> 00:31:14,200
that socket and that's connected through

00:31:11,169 --> 00:31:17,289
a cache coherent interconnect and again

00:31:14,200 --> 00:31:19,330
I see that without my special date

00:31:17,289 --> 00:31:24,640
initialization I don't get any benefit

00:31:19,330 --> 00:31:26,740
from adding adding threads and once I do

00:31:24,640 --> 00:31:28,780
my magic on my data initialization it

00:31:26,740 --> 00:31:32,890
does pay off so this is a generic

00:31:28,780 --> 00:31:35,470
optimization that I benefit from on CC

00:31:32,890 --> 00:31:38,230
newmarket dangers you're not always that

00:31:35,470 --> 00:31:41,440
lucky but I'd like to you know be

00:31:38,230 --> 00:31:44,350
optimistic this summarizes it both

00:31:41,440 --> 00:31:46,919
architectures benefit and it's

00:31:44,350 --> 00:31:51,659
definitely something worth doing

00:31:46,919 --> 00:31:53,590
alright that was that was a warming up

00:31:51,659 --> 00:31:57,850
and I want to talk about a more

00:31:53,590 --> 00:32:01,900
complicated case is a three dimensional

00:31:57,850 --> 00:32:04,990
matrix update and what I'm doing here is

00:32:01,900 --> 00:32:08,200
I'm doing this is for trend of obtaining

00:32:04,990 --> 00:32:10,750
a three dimensional array a check number

00:32:08,200 --> 00:32:12,760
one for those of you doing Fortran is

00:32:10,750 --> 00:32:15,340
this written in the right order for

00:32:12,760 --> 00:32:17,770
sequential performance remember that

00:32:15,340 --> 00:32:20,350
role yes it is the loops are written the

00:32:17,770 --> 00:32:22,539
right way for for performance you access

00:32:20,350 --> 00:32:23,860
the memory in the right way there's this

00:32:22,539 --> 00:32:27,909
nasty thing there are two data

00:32:23,860 --> 00:32:30,610
dependencies element IJ K depends on i j

00:32:27,909 --> 00:32:33,250
k minus 1 so there's dependence and on

00:32:30,610 --> 00:32:37,150
the cave variable on the third variable

00:32:33,250 --> 00:32:41,070
index and there's one on the second j

00:32:37,150 --> 00:32:44,650
depends on J minus 1 so in this picture

00:32:41,070 --> 00:32:48,100
what it says is this element it depends

00:32:44,650 --> 00:32:51,490
on these two and that means I can't just

00:32:48,100 --> 00:32:55,140
straight forward paralyze those those

00:32:51,490 --> 00:32:58,210
loops I cannot just jam in a parallel do

00:32:55,140 --> 00:32:59,980
for the J or the K loop now luckily

00:32:58,210 --> 00:33:01,059
there's one left and that's the I loop

00:32:59,980 --> 00:33:04,629
so I can successful

00:33:01,059 --> 00:33:08,139
paralyzed the inner loop there's one

00:33:04,629 --> 00:33:10,509
little little bad thing I get a barrier

00:33:08,139 --> 00:33:14,230
at the end of that loop so the barrier

00:33:10,509 --> 00:33:17,710
he is executed n square times so I don't

00:33:14,230 --> 00:33:23,309
expect this to scale very well and I

00:33:17,710 --> 00:33:26,710
hope you I hope you agree and it doesn't

00:33:23,309 --> 00:33:27,309
88 eight threads eight cores and it's

00:33:26,710 --> 00:33:29,950
game over

00:33:27,309 --> 00:33:34,299
performance drops and it's it's fairly

00:33:29,950 --> 00:33:36,309
pathetic so I kind of expected that and

00:33:34,299 --> 00:33:39,190
I hope you agree I mean there's too too

00:33:36,309 --> 00:33:41,289
many barrier calls I don't use our tool

00:33:39,190 --> 00:33:44,529
I use the performance analyzer and what

00:33:41,289 --> 00:33:47,549
one word of advice whenever you tune an

00:33:44,529 --> 00:33:50,259
application always use a profiling tool

00:33:47,549 --> 00:33:52,509
don't don't believe you know where the

00:33:50,259 --> 00:33:54,039
time is spent because to the computer it

00:33:52,509 --> 00:33:55,509
could be in a very different part than

00:33:54,039 --> 00:33:57,580
what you think is important in your

00:33:55,509 --> 00:34:00,580
application so always use of profiling

00:33:57,580 --> 00:34:03,360
tool as I'm doing here and what it shows

00:34:00,580 --> 00:34:04,799
me it and actually I do a side-by-side

00:34:03,360 --> 00:34:08,679
comparison

00:34:04,799 --> 00:34:11,470
one thread versus two and that's another

00:34:08,679 --> 00:34:13,599
advice I always start checking on two

00:34:11,470 --> 00:34:15,220
threads if I don't understand the

00:34:13,599 --> 00:34:17,470
performance problem on two threads how

00:34:15,220 --> 00:34:20,980
can I ever even try to understand it on

00:34:17,470 --> 00:34:23,260
hundred threads per thousand and very

00:34:20,980 --> 00:34:25,060
often you see the first symptoms already

00:34:23,260 --> 00:34:27,819
on a small thread count he just make a

00:34:25,060 --> 00:34:29,710
home life a lot easier and that's what

00:34:27,819 --> 00:34:32,200
I'm seeing here what I'm seeing here and

00:34:29,710 --> 00:34:34,359
let me jump to them the interesting part

00:34:32,200 --> 00:34:38,950
I see this column here is the work part

00:34:34,359 --> 00:34:41,579
and this is what you wait and what you

00:34:38,950 --> 00:34:44,550
see the work part scales reasonably well

00:34:41,579 --> 00:34:47,950
but you see quite a bit of waiting time

00:34:44,550 --> 00:34:50,109
at a waiting time increases a lot as I

00:34:47,950 --> 00:34:52,179
go from one to two threads and that's

00:34:50,109 --> 00:34:54,129
strange because I paralyzed basically

00:34:52,179 --> 00:34:56,649
paralyzed over a vector of my

00:34:54,129 --> 00:34:58,180
three-dimensional array that was what I

00:34:56,649 --> 00:35:00,160
couldn't understand for a while like why

00:34:58,180 --> 00:35:02,030
do I see all that waiting I mean I

00:35:00,160 --> 00:35:04,250
expect the very it costs to be high

00:35:02,030 --> 00:35:06,410
and you can see you can see here they

00:35:04,250 --> 00:35:08,390
barrier cost from almost nothing jumps

00:35:06,410 --> 00:35:10,550
to 2.4 seconds so that's quite an

00:35:08,390 --> 00:35:12,500
increase but it's not my real bottleneck

00:35:10,550 --> 00:35:15,560
here and that I couldn't understand for

00:35:12,500 --> 00:35:18,230
a while I see the same when I go down to

00:35:15,560 --> 00:35:20,150
the source level again a higher way time

00:35:18,230 --> 00:35:22,700
so what are we waiting for this is just

00:35:20,150 --> 00:35:24,920
a vector operation and I'm only talking

00:35:22,700 --> 00:35:28,340
about two threads I mean that can't be

00:35:24,920 --> 00:35:30,800
real load imbalance so I used the same

00:35:28,340 --> 00:35:33,290
tool and it I'm here I'm showing the

00:35:30,800 --> 00:35:34,490
time line then I gotta explain that a

00:35:33,290 --> 00:35:37,640
little bit you'll see more of these

00:35:34,490 --> 00:35:40,750
pictures the top bar with the green and

00:35:37,640 --> 00:35:44,240
the blue is the operating system state

00:35:40,750 --> 00:35:47,240
so what was the OS doing and anything

00:35:44,240 --> 00:35:48,860
but green is bad news so the blue you

00:35:47,240 --> 00:35:51,440
see here although it's a nice color

00:35:48,860 --> 00:35:53,840
it's bad news and the legend tells you

00:35:51,440 --> 00:35:55,490
that system CPU actually what it does

00:35:53,840 --> 00:35:59,300
it's initializing the data of page

00:35:55,490 --> 00:36:02,750
faults so and then what I get is the

00:35:59,300 --> 00:36:05,150
behavior for two threads here and for

00:36:02,750 --> 00:36:10,610
here for example and the top one is one

00:36:05,150 --> 00:36:14,740
thread and each color here represents

00:36:10,610 --> 00:36:19,630
something and let me show you the legend

00:36:14,740 --> 00:36:23,450
okay so green green is the compute part

00:36:19,630 --> 00:36:29,110
blue at the application level is thread

00:36:23,450 --> 00:36:31,850
idle time here and red is the barrier

00:36:29,110 --> 00:36:35,780
now what you see you see barrier calls

00:36:31,850 --> 00:36:38,810
all over the place I mean it's literally

00:36:35,780 --> 00:36:42,320
visible that a lot of variables what is

00:36:38,810 --> 00:36:46,580
strange is that the top thread doesn't

00:36:42,320 --> 00:36:49,940
have that many barrier covers so that

00:36:46,580 --> 00:36:54,020
puzzled me for a while and yet when i

00:36:49,940 --> 00:36:56,540
zoom in I see them I see the same so a

00:36:54,020 --> 00:36:59,630
lot of idle time in these threads only

00:36:56,540 --> 00:37:01,400
the master is crunching along again more

00:36:59,630 --> 00:37:03,410
of the same it just confirms that

00:37:01,400 --> 00:37:05,360
there's something bad happening and

00:37:03,410 --> 00:37:09,890
finally it dawned on me this is for

00:37:05,360 --> 00:37:12,980
sharing at work so here's what happens

00:37:09,890 --> 00:37:14,400
they're hitting the same vector on one

00:37:12,980 --> 00:37:17,880
thread there's no sharing

00:37:14,400 --> 00:37:20,329
I'm okay but what if two threads happen

00:37:17,880 --> 00:37:23,130
to hit the same part of the cache line

00:37:20,329 --> 00:37:25,950
but that's at the boundary of where I

00:37:23,130 --> 00:37:27,809
distribute the work and then I go to

00:37:25,950 --> 00:37:30,809
four threads and now there are three

00:37:27,809 --> 00:37:33,510
places where that can happen and that's

00:37:30,809 --> 00:37:35,430
for sharing because here two threads are

00:37:33,510 --> 00:37:38,400
trying to update the same cache line at

00:37:35,430 --> 00:37:42,750
the same time so this is false sharing

00:37:38,400 --> 00:37:45,630
in the real world and luckily all these

00:37:42,750 --> 00:37:47,339
processors have Hardware counters which

00:37:45,630 --> 00:37:50,099
are not for the faint of heart

00:37:47,339 --> 00:37:52,200
the names can be extremely cryptic but

00:37:50,099 --> 00:37:56,329
they are very useful so what I'm showing

00:37:52,200 --> 00:37:58,920
here is how many cache line

00:37:56,329 --> 00:38:00,990
invalidations I have now remember that

00:37:58,920 --> 00:38:03,630
diagram where I show the line is

00:38:00,990 --> 00:38:05,460
invalidated that's what I'm looking at

00:38:03,630 --> 00:38:08,430
here how often did that happen and what

00:38:05,460 --> 00:38:09,869
you see as I add the threads look at one

00:38:08,430 --> 00:38:12,690
thread is almost nothing that's the

00:38:09,869 --> 00:38:14,730
noise but now I want to go to to the

00:38:12,690 --> 00:38:17,160
fairly significant amount and you know

00:38:14,730 --> 00:38:20,039
it kind of explodes and these

00:38:17,160 --> 00:38:25,109
invalidations are a smoking gun for for

00:38:20,039 --> 00:38:28,200
sharing so this is for share okay too

00:38:25,109 --> 00:38:28,650
bad that explains my bad scaling what do

00:38:28,200 --> 00:38:31,440
I do

00:38:28,650 --> 00:38:35,910
well luckily there's this there's some

00:38:31,440 --> 00:38:37,940
hope because this is the dependency that

00:38:35,910 --> 00:38:41,609
I was showing earlier what it means that

00:38:37,940 --> 00:38:44,789
planes can be updated independently I

00:38:41,609 --> 00:38:47,220
have a dependence here these two

00:38:44,789 --> 00:38:49,859
elements but each plane can be be

00:38:47,220 --> 00:38:53,160
updated independently I hope that's more

00:38:49,859 --> 00:38:57,049
or less clear well one I can do a plane

00:38:53,160 --> 00:39:00,630
in parallel I can do blocks in parallel

00:38:57,049 --> 00:39:02,309
so what I can do is some bookkeeping to

00:39:00,630 --> 00:39:04,109
figure out for each thread what

00:39:02,309 --> 00:39:07,920
3-dimensional blockage you'll be working

00:39:04,109 --> 00:39:10,079
on and then I have a totally different

00:39:07,920 --> 00:39:11,819
approach and that should not have the

00:39:10,079 --> 00:39:14,700
for sharing that I was seeing earlier

00:39:11,819 --> 00:39:16,980
and all it all it means is that I need

00:39:14,700 --> 00:39:19,200
to change my algorithm to have a start

00:39:16,980 --> 00:39:20,230
and end value for the update and that

00:39:19,200 --> 00:39:23,740
depends on the

00:39:20,230 --> 00:39:26,110
and I'll show you the code this is

00:39:23,740 --> 00:39:30,880
straight from the code so now what I do

00:39:26,110 --> 00:39:33,550
I figure out my thread idea and then

00:39:30,880 --> 00:39:36,640
depending on the thread ID I decide what

00:39:33,550 --> 00:39:39,970
part of the matrix to work on that's a

00:39:36,640 --> 00:39:42,010
very explicit work distribution it's not

00:39:39,970 --> 00:39:44,080
hard it's usually I don't get it right

00:39:42,010 --> 00:39:46,119
the first time but after a while you get

00:39:44,080 --> 00:39:49,090
it right to define who is working on

00:39:46,119 --> 00:39:50,710
what so you define the start and end

00:39:49,090 --> 00:39:55,180
depending as a function of the thread

00:39:50,710 --> 00:39:57,310
idea you call this function again with a

00:39:55,180 --> 00:39:59,260
different start and end value you do

00:39:57,310 --> 00:40:02,440
that in a parallel region the barriers

00:39:59,260 --> 00:40:05,350
are gone here and there's no false

00:40:02,440 --> 00:40:09,580
sharing I had to do some work for that

00:40:05,350 --> 00:40:13,390
but it's worth it this is the new

00:40:09,580 --> 00:40:15,220
performance that I get and ignore those

00:40:13,390 --> 00:40:17,410
sawtooth that's that's load imbalance

00:40:15,220 --> 00:40:20,020
and you know that's kind of just look at

00:40:17,410 --> 00:40:22,510
the trend and what you see is in peak

00:40:20,020 --> 00:40:27,820
performance I get about 4x performance

00:40:22,510 --> 00:40:30,130
improvement and when you're looking at

00:40:27,820 --> 00:40:32,290
the at a profile again you see all these

00:40:30,130 --> 00:40:34,660
bad barriers are gone and there's no

00:40:32,290 --> 00:40:38,680
idle time anymore so this this looks

00:40:34,660 --> 00:40:40,150
really nice ok oh yeah it's a little bit

00:40:38,680 --> 00:40:41,830
of low balance but that's really nothing

00:40:40,150 --> 00:40:44,550
when you look at the time scale of it

00:40:41,830 --> 00:40:47,250
and it's confirmed when I look at these

00:40:44,550 --> 00:40:50,290
invalidations now look at the blue ones

00:40:47,250 --> 00:40:55,720
compared to the red one so I've pretty

00:40:50,290 --> 00:40:58,330
much got rid of the false share there's

00:40:55,720 --> 00:41:00,070
another idea and the first time you see

00:40:58,330 --> 00:41:01,660
that that's a little funny and the next

00:41:00,070 --> 00:41:03,220
time you see it's a little funny and the

00:41:01,660 --> 00:41:05,920
third time and the fourth time you start

00:41:03,220 --> 00:41:08,830
okay yeah I think I get it what you do

00:41:05,920 --> 00:41:13,000
is you push the parallel region out of

00:41:08,830 --> 00:41:15,810
that nested loop so I have a large

00:41:13,000 --> 00:41:18,220
parallel region here I do have my doom

00:41:15,810 --> 00:41:20,050
but I no longer have all the overhead of

00:41:18,220 --> 00:41:24,310
the parallel region in the heart of -

00:41:20,050 --> 00:41:27,490
that loop so how does that execute and

00:41:24,310 --> 00:41:29,619
I'll show you on two sets on two threads

00:41:27,490 --> 00:41:30,400
both threads will start with K equals

00:41:29,619 --> 00:41:32,289
two

00:41:30,400 --> 00:41:34,299
they'll both start with J equals two and

00:41:32,289 --> 00:41:37,000
then they hit that in a loop for the

00:41:34,299 --> 00:41:38,680
same value of K and J they split the

00:41:37,000 --> 00:41:40,480
loop it's it's it's a do loop it's a

00:41:38,680 --> 00:41:42,640
open if you do loop so they split the

00:41:40,480 --> 00:41:44,500
loop they do their work and then the

00:41:42,640 --> 00:41:47,829
increment J and they go on to the next

00:41:44,500 --> 00:41:50,200
block of the matrix it's a totally

00:41:47,829 --> 00:41:51,460
different approach but I'm using openmp

00:41:50,200 --> 00:41:55,089
instead of doing all the bookkeeping

00:41:51,460 --> 00:41:59,349
myself so this is a different different

00:41:55,089 --> 00:42:02,319
version of tackling this problem so by

00:41:59,349 --> 00:42:05,770
now I actually have four versions my bad

00:42:02,319 --> 00:42:07,029
one of course I tried to compiler always

00:42:05,770 --> 00:42:09,069
try if you have an automatically

00:42:07,029 --> 00:42:11,230
paralyzing compiler give it a try the

00:42:09,069 --> 00:42:12,910
mind it's definitely will vary but you

00:42:11,230 --> 00:42:16,029
know who knows what it will find for you

00:42:12,910 --> 00:42:18,010
I have my three-dimensional box and I

00:42:16,029 --> 00:42:21,339
have the last funny version that I was

00:42:18,010 --> 00:42:24,220
that I was showing and here's how how it

00:42:21,339 --> 00:42:26,740
compares the blue one was the bad one so

00:42:24,220 --> 00:42:28,750
we got rid of that one and what you see

00:42:26,740 --> 00:42:30,670
the green one is then actually the last

00:42:28,750 --> 00:42:33,819
version that I showed and it does quite

00:42:30,670 --> 00:42:36,400
well on on lower thread counts but

00:42:33,819 --> 00:42:38,140
eventually it loses out and since I'm

00:42:36,400 --> 00:42:39,940
only interested in very large dead

00:42:38,140 --> 00:42:42,430
counts I didn't even bother to look at

00:42:39,940 --> 00:42:45,789
why that was it's interesting to figure

00:42:42,430 --> 00:42:47,859
it out but okay lots of threads I get I

00:42:45,789 --> 00:42:49,750
got two versions that are very

00:42:47,859 --> 00:42:51,730
comparable in performance and actually

00:42:49,750 --> 00:42:54,010
what it turns out to be is the compiler

00:42:51,730 --> 00:42:56,500
did a really great job the compiler

00:42:54,010 --> 00:42:59,380
actually did for me what I did by hand

00:42:56,500 --> 00:43:01,299
with these blocks pretty impressive

00:42:59,380 --> 00:43:03,730
I mean don't forget there are these

00:43:01,299 --> 00:43:07,510
tools called compilers and they can do

00:43:03,730 --> 00:43:09,849
all the magic for you so so overall I

00:43:07,510 --> 00:43:11,619
got pretty you know pretty decent

00:43:09,849 --> 00:43:15,910
performance I mean it's substantially

00:43:11,619 --> 00:43:17,289
faster than whatever I had before and

00:43:15,910 --> 00:43:19,750
then played a little bit with the

00:43:17,289 --> 00:43:24,730
software prefetch which you can do on

00:43:19,750 --> 00:43:30,069
compilers and what I saw was I got good

00:43:24,730 --> 00:43:33,970
performance the problem was this when I

00:43:30,069 --> 00:43:36,750
enabled the software prefetch I got

00:43:33,970 --> 00:43:40,150
worst performance that's the red curve

00:43:36,750 --> 00:43:42,400
compared to the green one where I where

00:43:40,150 --> 00:43:43,810
disabled software people software people

00:43:42,400 --> 00:43:46,180
should help me prefetch

00:43:43,810 --> 00:43:47,980
the data before I needed so I had no

00:43:46,180 --> 00:43:53,560
idea what was going on here and that

00:43:47,980 --> 00:43:55,300
opened the new kind of worms and I'll

00:43:53,560 --> 00:43:57,040
spare you to thinking about it but what

00:43:55,300 --> 00:43:59,290
I found we have another counter that

00:43:57,040 --> 00:44:02,590
that measures how often did you have to

00:43:59,290 --> 00:44:05,110
go to remote memory that's an indicator

00:44:02,590 --> 00:44:06,910
of CC Numa you don't want to go to

00:44:05,110 --> 00:44:09,940
remote memory you want to have all your

00:44:06,910 --> 00:44:11,890
data locally so in that counter has a

00:44:09,940 --> 00:44:15,250
significant value it means that you're

00:44:11,890 --> 00:44:18,450
not CC no more friendly and that's what

00:44:15,250 --> 00:44:20,920
my new optimized algorithm suffered from

00:44:18,450 --> 00:44:22,420
that was kind of a surprise I mean it

00:44:20,920 --> 00:44:24,160
was it was done without any

00:44:22,420 --> 00:44:26,500
consideration for CC Numa

00:44:24,160 --> 00:44:30,850
so I tackled us a tackle default sharing

00:44:26,500 --> 00:44:33,520
and now I get bitten by C Sonoma and and

00:44:30,850 --> 00:44:37,060
this discounters what it shows is that

00:44:33,520 --> 00:44:39,190
the baseline the blue version although

00:44:37,060 --> 00:44:42,430
the phone badly because of the false

00:44:39,190 --> 00:44:46,000
sharing had better data locality there

00:44:42,430 --> 00:44:49,000
my optimized version and that was kind

00:44:46,000 --> 00:44:50,710
of a surprise all right so what's going

00:44:49,000 --> 00:44:53,230
on here we got to get really technical

00:44:50,710 --> 00:44:57,760
here we got to go deep deep in the

00:44:53,230 --> 00:44:59,560
dungeon I need to look at how that array

00:44:57,760 --> 00:45:01,840
X is stored in memory it's a three

00:44:59,560 --> 00:45:03,880
dimensional array and as everybody doing

00:45:01,840 --> 00:45:06,010
Fortran should know that's stored by the

00:45:03,880 --> 00:45:08,170
column so column the first column is

00:45:06,010 --> 00:45:09,730
first in memory the next column is the

00:45:08,170 --> 00:45:12,780
next one in memory that's how it's being

00:45:09,730 --> 00:45:16,540
organized so that's how this would be

00:45:12,780 --> 00:45:21,970
arranged in memory the problem is these

00:45:16,540 --> 00:45:26,370
pages let's just take an imaginary size

00:45:21,970 --> 00:45:29,620
and let's assume that the size of a page

00:45:26,370 --> 00:45:32,470
relative to the size of my array is like

00:45:29,620 --> 00:45:35,430
this it doesn't fit nicely with these

00:45:32,470 --> 00:45:38,400
columns I get some sort of overflow and

00:45:35,430 --> 00:45:44,530
each color represents a different page

00:45:38,400 --> 00:45:51,170
and here's how I'm accessing it and what

00:45:44,530 --> 00:45:54,230
I see is pages are shared between those

00:45:51,170 --> 00:45:56,420
knows and that's not a good thing again

00:45:54,230 --> 00:46:01,130
you want to have your data local you

00:45:56,420 --> 00:46:04,339
don't wanna share pages I came up with

00:46:01,130 --> 00:46:06,650
the hack the and this is actually an

00:46:04,339 --> 00:46:09,230
explanation that what I did actually

00:46:06,650 --> 00:46:13,010
Indy did not was not CC'ing you more

00:46:09,230 --> 00:46:16,160
friendly the hack that I came up with

00:46:13,010 --> 00:46:17,690
and that's just a proof of concept study

00:46:16,160 --> 00:46:19,609
I'm not saying that's what you should do

00:46:17,690 --> 00:46:21,589
in real life but I just wanted to make

00:46:19,609 --> 00:46:23,270
sure that can I can I handle this and

00:46:21,589 --> 00:46:26,329
what I did I made the array four

00:46:23,270 --> 00:46:29,000
dimensional I want to I want to

00:46:26,329 --> 00:46:31,400
emphasize that I I don't need a lot of

00:46:29,000 --> 00:46:34,250
extra storage for that but I organize my

00:46:31,400 --> 00:46:37,760
data in a different way and what I do I

00:46:34,250 --> 00:46:40,700
have the thread idea to access into a

00:46:37,760 --> 00:46:42,799
chunk of that array that's that's what

00:46:40,700 --> 00:46:44,150
I'm doing so I read a mention the array

00:46:42,799 --> 00:46:46,270
that could be a lot of work in a real

00:46:44,150 --> 00:46:50,859
application I mean could be non-trivial

00:46:46,270 --> 00:46:53,650
I realized that but that's what I did

00:46:50,859 --> 00:46:56,660
here's the new code the new code

00:46:53,650 --> 00:47:00,380
surprisingly looks very much like the

00:46:56,660 --> 00:47:04,970
old code I have my start and end value

00:47:00,380 --> 00:47:08,059
here and the forth index is now over the

00:47:04,970 --> 00:47:12,470
thread idea again a hack just to show

00:47:08,059 --> 00:47:20,089
you how far you can go but by doing this

00:47:12,470 --> 00:47:23,089
and by adapting my algorithm I get rid

00:47:20,089 --> 00:47:25,190
of all those remote misses again you got

00:47:23,089 --> 00:47:26,990
to study the slice to really digest is I

00:47:25,190 --> 00:47:30,710
know this is going very quickly but

00:47:26,990 --> 00:47:35,030
that's um that's what I did and now you

00:47:30,710 --> 00:47:37,549
see the blue the even number of misses

00:47:35,030 --> 00:47:40,609
across the board across all the threads

00:47:37,549 --> 00:47:43,579
so now I am much more seeing you more

00:47:40,609 --> 00:47:45,470
friendly and ultimately with that

00:47:43,579 --> 00:47:49,250
version I get a 28 X

00:47:45,470 --> 00:47:53,410
speed-up I mean it's worth doing to make

00:47:49,250 --> 00:47:53,410
a program almost 30 times faster

00:47:53,880 --> 00:48:01,290
okay the last case is I called a bow to

00:47:58,830 --> 00:48:05,610
bow that stands for better bad OpenMP to

00:48:01,290 --> 00:48:07,890
better OpenMP I'm not pretending I'm

00:48:05,610 --> 00:48:09,720
done with it but it certainly as as I'll

00:48:07,890 --> 00:48:12,800
show it's better opening PETA and what

00:48:09,720 --> 00:48:15,780
was typed in by the author of the code

00:48:12,800 --> 00:48:19,050
okay but it is work in progress

00:48:15,780 --> 00:48:22,230
absolutely somewhat sensitive so the

00:48:19,050 --> 00:48:24,060
following requests please no notes no

00:48:22,230 --> 00:48:27,780
pictures and if you like no attention

00:48:24,060 --> 00:48:29,970
but that's not your call so so don't try

00:48:27,780 --> 00:48:31,980
this at home yet I mean it's a very

00:48:29,970 --> 00:48:36,780
crude work I'm still working on it

00:48:31,980 --> 00:48:39,960
and in the odd hours so it's in the area

00:48:36,780 --> 00:48:41,670
of graph analysis and what's a graph

00:48:39,960 --> 00:48:43,410
this is this an embarrassingly simple

00:48:41,670 --> 00:48:46,410
introduction to graphs where the graph

00:48:43,410 --> 00:48:48,930
is a mathematical abstraction where you

00:48:46,410 --> 00:48:51,600
have objects and you have relationships

00:48:48,930 --> 00:48:53,850
between them and an object is is defined

00:48:51,600 --> 00:48:57,090
as a through vertices these are the

00:48:53,850 --> 00:48:59,400
nodes in your graph and that the edges

00:48:57,090 --> 00:49:02,070
define are defined by the relationship

00:48:59,400 --> 00:49:04,170
you put in between those two so you get

00:49:02,070 --> 00:49:06,240
two sets you get a set of vertices and a

00:49:04,170 --> 00:49:09,120
set of edges and here's a very simple

00:49:06,240 --> 00:49:13,200
example I have a a graph with just five

00:49:09,120 --> 00:49:15,210
nodes or vertices ABCD and E and the

00:49:13,200 --> 00:49:17,430
relation could be do these people know

00:49:15,210 --> 00:49:20,100
each other you know if it seems like a

00:49:17,430 --> 00:49:23,010
social network you want to you want to

00:49:20,100 --> 00:49:25,800
know who knows who so if somebody knows

00:49:23,010 --> 00:49:29,070
somebody else you draw a line and that

00:49:25,800 --> 00:49:30,990
defines an edge and in in in the

00:49:29,070 --> 00:49:35,970
practical sense in terms of the computer

00:49:30,990 --> 00:49:40,530
that's stored as a set so this a comma C

00:49:35,970 --> 00:49:42,930
means that that's this line means they

00:49:40,530 --> 00:49:45,870
have a relationship whatever that that

00:49:42,930 --> 00:49:48,630
can be so that's a graph in its simplest

00:49:45,870 --> 00:49:50,520
form and what people like to do they

00:49:48,630 --> 00:49:53,310
like to search in these graphs for

00:49:50,520 --> 00:49:56,130
information and these graphs can get

00:49:53,310 --> 00:49:57,329
very large and they can get very ugly in

00:49:56,130 --> 00:50:01,039
terms of

00:49:57,329 --> 00:50:06,180
memory usage and memory access patterns

00:50:01,039 --> 00:50:08,880
so is an example and on this code

00:50:06,180 --> 00:50:12,180
there's a a parameter called scale and

00:50:08,880 --> 00:50:14,640
that controls the size of the graph the

00:50:12,180 --> 00:50:17,009
number of nodes or vertices is 2 to the

00:50:14,640 --> 00:50:19,410
power of that number so if you give it

00:50:17,009 --> 00:50:21,029
scale of 10 it's only thousand 24 nodes

00:50:19,410 --> 00:50:23,309
that's a very small graph but as you

00:50:21,029 --> 00:50:26,999
guys it literally grows exponentially as

00:50:23,309 --> 00:50:29,869
you increase the value of scale oh five

00:50:26,999 --> 00:50:32,819
minutes I thought you had a question

00:50:29,869 --> 00:50:35,339
okay yeah it'll probably be more seven

00:50:32,819 --> 00:50:37,519
minutes but okay the number of edges is

00:50:35,339 --> 00:50:41,160
defined as sixteen times the number of

00:50:37,519 --> 00:50:43,559
vertices the key aspects of these are

00:50:41,160 --> 00:50:46,049
large memory lots of memory and when I

00:50:43,559 --> 00:50:48,239
say large I'll show you what I'm talking

00:50:46,049 --> 00:50:50,729
about terabytes of memory random access

00:50:48,239 --> 00:50:52,680
you're jumping all over memory there

00:50:50,729 --> 00:50:54,209
using a lot of lot of threads and

00:50:52,680 --> 00:50:57,029
there's really intense communication

00:50:54,209 --> 00:51:00,029
between those threads and it results in

00:50:57,029 --> 00:51:03,150
very long run times so anything you can

00:51:00,029 --> 00:51:05,219
save there as a win so here's an example

00:51:03,150 --> 00:51:09,630
of these memory requirements I mean

00:51:05,219 --> 00:51:11,579
that's exponential growth so for a small

00:51:09,630 --> 00:51:12,959
one is the scale twenty six it's only

00:51:11,579 --> 00:51:15,599
thirty five gigabytes

00:51:12,959 --> 00:51:17,609
when you talk about scale 31 you talk

00:51:15,599 --> 00:51:20,150
about a terabyte of memory that's needed

00:51:17,609 --> 00:51:24,949
so it gets out of hand fairly quickly

00:51:20,150 --> 00:51:30,319
and that code has two distinct phases

00:51:24,949 --> 00:51:33,209
and the first part in this diagram shows

00:51:30,319 --> 00:51:35,069
then the resources needed the the

00:51:33,209 --> 00:51:37,979
functions call to construct the graph

00:51:35,069 --> 00:51:40,079
and then you start searching and we if

00:51:37,979 --> 00:51:41,819
when you do the search you also verify

00:51:40,079 --> 00:51:43,680
that you found a valid solution so

00:51:41,819 --> 00:51:45,809
there's a search and verification phase

00:51:43,680 --> 00:51:47,749
and that's the that's the the metric

00:51:45,809 --> 00:51:49,890
here that the time for that part

00:51:47,749 --> 00:51:51,359
construction time is not counted

00:51:49,890 --> 00:51:57,299
although it is important it's not

00:51:51,359 --> 00:51:59,190
counted so I ran that code I ran it on a

00:51:57,299 --> 00:52:03,630
two socket machine and performance was

00:51:59,190 --> 00:52:06,540
terrible sixteen threads and stopped

00:52:03,630 --> 00:52:10,080
scaling that's really bad

00:52:06,540 --> 00:52:12,360
not good so what do most people do if

00:52:10,080 --> 00:52:16,260
they can huh must be the wrong machine

00:52:12,360 --> 00:52:18,420
let's use a bigger machine well it's

00:52:16,260 --> 00:52:20,520
actually worse on a larger machine and

00:52:18,420 --> 00:52:22,890
that's very good reasons why I don't get

00:52:20,520 --> 00:52:24,660
to that but so that's that's not the

00:52:22,890 --> 00:52:27,120
solution to your problem the code

00:52:24,660 --> 00:52:30,060
doesn't scale and a larger machine more

00:52:27,120 --> 00:52:31,220
more cores won't help you more bandwidth

00:52:30,060 --> 00:52:34,350
it won't help you

00:52:31,220 --> 00:52:37,680
well maybe the graph is too small let's

00:52:34,350 --> 00:52:40,230
run a larger graph again it doesn't

00:52:37,680 --> 00:52:42,270
matter no matter what you do you can

00:52:40,230 --> 00:52:46,430
change machine you can change graph size

00:52:42,270 --> 00:52:50,040
the performance is just genetically bad

00:52:46,430 --> 00:52:52,470
okay time for science or some would call

00:52:50,040 --> 00:52:55,260
it black art to start looking at the

00:52:52,470 --> 00:52:57,900
performance so what are the initial

00:52:55,260 --> 00:52:59,850
things well we like to look at the thing

00:52:57,900 --> 00:53:01,800
that's very easy to measure and it's

00:52:59,850 --> 00:53:03,540
called clocks per instruction how many

00:53:01,800 --> 00:53:05,160
clock cycles does each instruction need

00:53:03,540 --> 00:53:06,720
and in the best case for each

00:53:05,160 --> 00:53:08,730
architecture you can figure out that

00:53:06,720 --> 00:53:11,670
number and con this architecture has

00:53:08,730 --> 00:53:13,980
point five that means two instructions

00:53:11,670 --> 00:53:15,150
per clock cycle that's that's when

00:53:13,980 --> 00:53:17,910
that's when you reach the maximum

00:53:15,150 --> 00:53:20,100
instruction processing speed so anything

00:53:17,910 --> 00:53:23,220
higher than that is bad news it could be

00:53:20,100 --> 00:53:25,500
all sorts of things pipeline stalls TLB

00:53:23,220 --> 00:53:27,210
issues remote memory access that's the

00:53:25,500 --> 00:53:29,880
next thing to answer if that value is

00:53:27,210 --> 00:53:31,740
high so what I'm showing here now each

00:53:29,880 --> 00:53:34,020
time I see that slide I get me in mind

00:53:31,740 --> 00:53:35,910
it isn't sort of hippie curtain from the

00:53:34,020 --> 00:53:38,520
70s what we what we're looking at but

00:53:35,910 --> 00:53:41,880
what we're seeing here is that CPI value

00:53:38,520 --> 00:53:44,910
in the initial part the generation phase

00:53:41,880 --> 00:53:49,980
is very long low is good news and in the

00:53:44,910 --> 00:53:51,750
search it jumps up the the CPI is is 50

00:53:49,980 --> 00:53:53,700
that means 50 clock cycles per

00:53:51,750 --> 00:53:56,130
instruction that's very high I mean a

00:53:53,700 --> 00:53:58,230
CPI of 50 is not good news

00:53:56,130 --> 00:54:01,350
that's in the search phase in the

00:53:58,230 --> 00:54:03,230
verification phase it drops and then it

00:54:01,350 --> 00:54:07,740
jumps up again as I do the next search

00:54:03,230 --> 00:54:10,200
so high CPI value bad news let's look at

00:54:07,740 --> 00:54:12,300
the bandwidth and what I what I

00:54:10,200 --> 00:54:14,460
distinguishing is the read bandwidth and

00:54:12,300 --> 00:54:15,310
the write bandwidth and the the write

00:54:14,460 --> 00:54:17,200
bandwidth is

00:54:15,310 --> 00:54:19,110
little little thing at the bottom I need

00:54:17,200 --> 00:54:24,040
to be careful with the sound here but

00:54:19,110 --> 00:54:26,380
this year right so there's no right it's

00:54:24,040 --> 00:54:29,320
all reading data which actually makes

00:54:26,380 --> 00:54:31,660
sense but what I get here from this

00:54:29,320 --> 00:54:33,340
application is about 50 gigabytes a

00:54:31,660 --> 00:54:35,740
second at best

00:54:33,340 --> 00:54:39,430
while this system has a total memory

00:54:35,740 --> 00:54:41,620
bandwidth of 130 so I'm wasting an awful

00:54:39,430 --> 00:54:44,170
lot of memory bandwidth here and

00:54:41,620 --> 00:54:47,410
actually when you look at it when I call

00:54:44,170 --> 00:54:51,760
a code it stuck at 0 and stuck at 1 most

00:54:47,410 --> 00:54:56,080
of that is from one socket CC Numa there

00:54:51,760 --> 00:54:58,030
we are again so I don't looked at the

00:54:56,080 --> 00:55:00,070
CPU time distribution and this is a

00:54:58,030 --> 00:55:01,720
shocking slide I mean when you look at

00:55:00,070 --> 00:55:05,740
what's happening here on a single thread

00:55:01,720 --> 00:55:08,290
all these pretty bright colors like red

00:55:05,740 --> 00:55:10,330
and orange and pink they're all bad news

00:55:08,290 --> 00:55:13,690
they're all openmp overhead

00:55:10,330 --> 00:55:16,660
communication barrier atomic operations

00:55:13,690 --> 00:55:21,160
way time and what you see even on a

00:55:16,660 --> 00:55:23,290
single thread that's already 7% that's

00:55:21,160 --> 00:55:25,450
an achievement in itself to write an

00:55:23,290 --> 00:55:27,760
open up a program that spent 7 percent

00:55:25,450 --> 00:55:31,930
of its time doing things you don't need

00:55:27,760 --> 00:55:33,220
on a single thread that's a sign without

00:55:31,930 --> 00:55:35,680
knowing the code where you'll see a

00:55:33,220 --> 00:55:37,780
profile like this you know this the

00:55:35,680 --> 00:55:40,060
opening P can't be right I mean there's

00:55:37,780 --> 00:55:42,400
too much overhead there and as you see

00:55:40,060 --> 00:55:44,670
it quickly totally dominates the

00:55:42,400 --> 00:55:48,940
performance when you look at 16 threads

00:55:44,670 --> 00:55:52,180
it's it's what is it 16 80 percent or

00:55:48,940 --> 00:55:54,160
whatever overhead so this confirms the

00:55:52,180 --> 00:55:56,620
bad scaling that I saw it totally killed

00:55:54,160 --> 00:55:58,750
in opening P overhead in this the legend

00:55:56,620 --> 00:56:00,370
here tells you you know where you

00:55:58,750 --> 00:56:02,950
spending your time but this is all in

00:56:00,370 --> 00:56:04,690
bad things so hence you may have

00:56:02,950 --> 00:56:06,310
wondered why I had the paw print and I

00:56:04,690 --> 00:56:08,860
need to talk with Michael about we could

00:56:06,310 --> 00:56:11,350
trade mark that with a bad opening piece

00:56:08,860 --> 00:56:13,180
stamp is like bad opening P that's what

00:56:11,350 --> 00:56:14,710
they said this profile says this was bad

00:56:13,180 --> 00:56:17,950
opening P you shouldn't write code like

00:56:14,710 --> 00:56:20,200
that so with the sufficient amount of

00:56:17,950 --> 00:56:22,900
secret sauce and that I don't have time

00:56:20,200 --> 00:56:25,060
to go into the bad openmp was tunity to

00:56:22,900 --> 00:56:27,640
better open it just cleaning up the code

00:56:25,060 --> 00:56:28,140
and it was a long list it's like the

00:56:27,640 --> 00:56:30,420
talk

00:56:28,140 --> 00:56:32,760
today by James rinder's you go through

00:56:30,420 --> 00:56:36,330
it step by step step by step you improve

00:56:32,760 --> 00:56:38,730
the performance and now we're now only

00:56:36,330 --> 00:56:43,320
show you the result I mean I'm running

00:56:38,730 --> 00:56:46,140
in my last two minutes now with fixing

00:56:43,320 --> 00:56:49,980
the opening P both sockets now deliver

00:56:46,140 --> 00:56:51,960
full bandwidth so I get a hundred and

00:56:49,980 --> 00:56:53,940
thirty seconds gigabytes a second

00:56:51,960 --> 00:56:57,480
bandwidth and look at the difference in

00:56:53,940 --> 00:57:00,540
performance that little red worm is what

00:56:57,480 --> 00:57:03,180
I have before well some people might say

00:57:00,540 --> 00:57:05,430
open and P does not scale I mean this

00:57:03,180 --> 00:57:08,190
was the original performance than I

00:57:05,430 --> 00:57:09,900
thought I saw I cleaned up the openmp

00:57:08,190 --> 00:57:12,570
and look at what I get on the same

00:57:09,900 --> 00:57:15,840
hardware same compiler everything the

00:57:12,570 --> 00:57:18,540
same just better open in people so it's

00:57:15,840 --> 00:57:21,360
about 1314 times faster in terms of peak

00:57:18,540 --> 00:57:23,550
performance okay and now the interesting

00:57:21,360 --> 00:57:27,200
thing is which is no surprise that's

00:57:23,550 --> 00:57:29,760
faster the bigger machine is faster and

00:57:27,200 --> 00:57:32,250
the reason was that that bad opening P

00:57:29,760 --> 00:57:33,780
was blocking everything else so now when

00:57:32,250 --> 00:57:35,550
I take that eight socket machine

00:57:33,780 --> 00:57:37,740
I do get much better performance because

00:57:35,550 --> 00:57:40,980
I can exploit the architecture in the by

00:57:37,740 --> 00:57:42,330
the way same code again no funny changes

00:57:40,980 --> 00:57:44,070
for that architecture take the same

00:57:42,330 --> 00:57:45,960
binary running on a bigger machine and

00:57:44,070 --> 00:57:49,290
now I get the benefit that's why you

00:57:45,960 --> 00:57:50,190
want to do optimization I wasn't really

00:57:49,290 --> 00:57:51,480
finished yet

00:57:50,190 --> 00:57:54,840
there were some things at the memory

00:57:51,480 --> 00:57:56,790
level that could be done better so now

00:57:54,840 --> 00:57:58,710
we go from the better open and p2 memory

00:57:56,790 --> 00:58:00,660
optimizations and again I'll do this

00:57:58,710 --> 00:58:02,850
fairly quickly I'll be more than happy

00:58:00,660 --> 00:58:07,800
share details with you you know after

00:58:02,850 --> 00:58:10,350
the talk here's what I got on that 8

00:58:07,800 --> 00:58:12,630
socket machine the bandwidth is now over

00:58:10,350 --> 00:58:14,190
450 gigabytes a second that's real

00:58:12,630 --> 00:58:17,340
delivered bandwidth that's pretty high

00:58:14,190 --> 00:58:19,650
I'm really using the architecture in the

00:58:17,340 --> 00:58:24,000
right way because of my memory level

00:58:19,650 --> 00:58:25,890
optimizations and here's the blue line

00:58:24,000 --> 00:58:28,800
remember that little worm it's almost

00:58:25,890 --> 00:58:31,530
gone now then I had the purple line

00:58:28,800 --> 00:58:34,890
which was my OpenMP changes and then I

00:58:31,530 --> 00:58:38,310
pay some attention to the memory system

00:58:34,890 --> 00:58:39,469
and I got way way better performance

00:58:38,310 --> 00:58:42,019
again again

00:58:39,469 --> 00:58:43,969
maybe maybe or the next open a pecan I

00:58:42,019 --> 00:58:47,319
can talk about just what I did here I

00:58:43,969 --> 00:58:51,140
didn't have time to prepare that but the

00:58:47,319 --> 00:58:53,900
improvements are significant and overall

00:58:51,140 --> 00:58:56,599
I got 50x improvement over what I had

00:58:53,900 --> 00:58:58,539
before and actually with half almost

00:58:56,599 --> 00:59:00,440
half of the number of threads I get

00:58:58,539 --> 00:59:02,900
significantly better performance so I

00:59:00,440 --> 00:59:06,319
just got way more value out of my system

00:59:02,900 --> 00:59:09,950
and if you translate that to what I call

00:59:06,319 --> 00:59:14,329
real money it means a search time was 12

00:59:09,950 --> 00:59:16,279
hours and it's 10 minutes now that is

00:59:14,329 --> 00:59:17,930
interesting for some people I mean if

00:59:16,279 --> 00:59:19,369
you instead of having to wait half a day

00:59:17,930 --> 00:59:23,029
you get it back after you get your

00:59:19,369 --> 00:59:24,499
coffee that does change things so the

00:59:23,029 --> 00:59:26,539
last pretty much the last thing I want

00:59:24,499 --> 00:59:30,289
to show here is I was interested in the

00:59:26,539 --> 00:59:35,390
relative benefit of things so the red

00:59:30,289 --> 00:59:38,140
curve shows you the speed up as a value

00:59:35,390 --> 00:59:42,140
of as a as a function of the graph size

00:59:38,140 --> 00:59:47,299
so what I see a small graph gives me 25

00:59:42,140 --> 00:59:49,160
X as I as I make the graph bigger the

00:59:47,299 --> 00:59:52,430
benefit gets less it's not much

00:59:49,160 --> 00:59:55,519
degradation but it drops to about 20 so

00:59:52,430 --> 00:59:59,150
I see the Minish in return now on top of

00:59:55,519 --> 01:00:01,699
that I add my memory optimizations and

00:59:59,150 --> 01:00:04,759
now as the graph is bigger the benefit

01:00:01,699 --> 01:00:07,190
is higher so I really get the best of

01:00:04,759 --> 01:00:09,440
both worlds but what the message here is

01:00:07,190 --> 01:00:14,180
is the low-hanging fruit was the open

01:00:09,440 --> 01:00:17,630
and P part that's my 25 X and on top of

01:00:14,180 --> 01:00:20,509
that I got 15 X more doing my memory

01:00:17,630 --> 01:00:24,109
stuff so the message here is first clean

01:00:20,509 --> 01:00:25,999
up your OpenMP once you've cleaned up

01:00:24,109 --> 01:00:27,920
your opening P you know that's working

01:00:25,999 --> 01:00:30,469
well that you open a P level you follow

01:00:27,920 --> 01:00:32,779
all the rules then start looking at

01:00:30,469 --> 01:00:36,349
other things or a little more of the

01:00:32,779 --> 01:00:38,959
same and eventually allow me to run a

01:00:36,349 --> 01:00:41,509
graph of the size of more than two

01:00:38,959 --> 01:00:44,420
terabyte and scale to almost thousand

01:00:41,509 --> 01:00:46,120
threads and I go to too many conferences

01:00:44,420 --> 01:00:49,810
where people say oh you can't do that

01:00:46,120 --> 01:00:53,200
you can't you know 1632 that's this

01:00:49,810 --> 01:00:55,030
discharge serves them wrong okay let's

01:00:53,200 --> 01:00:57,670
finish it up so while we're waiting for

01:00:55,030 --> 01:01:00,240
your debugging run to finish do you

01:00:57,670 --> 01:01:04,750
think this was all useful it is

01:01:00,240 --> 01:01:09,130
overwhelming I know an MPA open Appy is

01:01:04,750 --> 01:01:10,990
somewhat obscure I know you're not a

01:01:09,130 --> 01:01:13,630
computer scientist you just need to get

01:01:10,990 --> 01:01:15,310
your job done that's not a good

01:01:13,630 --> 01:01:16,180
situation but I'm really sorry it's all

01:01:15,310 --> 01:01:20,470
about Darwin

01:01:16,180 --> 01:01:23,500
it's a tough world out there Oh your MPI

01:01:20,470 --> 01:01:25,390
job just finished that great but your

01:01:23,500 --> 01:01:27,280
program didn't write a file called core

01:01:25,390 --> 01:01:30,310
and wasn't there when you started the

01:01:27,280 --> 01:01:32,950
program you wonder where that file come

01:01:30,310 --> 01:01:37,410
from well you know let's talk let's have

01:01:32,950 --> 01:01:41,770
a coffee and well what did you just say

01:01:37,410 --> 01:01:43,990
somebody told you what somebody told you

01:01:41,770 --> 01:01:46,780
GPUs and OpenCL are going to solve all

01:01:43,990 --> 01:01:49,000
your problems let's make that a triple

01:01:46,780 --> 01:01:50,830
espresso and out wide let's go go out

01:01:49,000 --> 01:01:53,200
thank you very much for your time and I

01:01:50,830 --> 01:01:57,150
would love to take your questions now or

01:01:53,200 --> 01:01:57,150
later but I'm done thank you

01:02:06,040 --> 01:02:09,040
yep

01:02:33,000 --> 01:02:36,000
right

01:02:38,860 --> 01:02:45,920
that's a fave thin line to watch

01:02:42,820 --> 01:02:48,410
compilers are really good at doing local

01:02:45,920 --> 01:02:52,460
optimizations like cache blocking full

01:02:48,410 --> 01:02:54,740
cache I usually stay away from it if the

01:02:52,460 --> 01:02:57,350
low that the big picture it gets harder

01:02:54,740 --> 01:02:59,660
for them they don't know all about your

01:02:57,350 --> 01:03:01,700
data structures so changing your data

01:02:59,660 --> 01:03:03,620
structures to be more memory friendly

01:03:01,700 --> 01:03:07,610
then all of a sudden things start to fly

01:03:03,620 --> 01:03:10,130
and exploit like first touch I generally

01:03:07,610 --> 01:03:12,560
don't go down to the cache level anymore

01:03:10,130 --> 01:03:14,150
that used to be different but by now and

01:03:12,560 --> 01:03:18,290
you also got because things like

01:03:14,150 --> 01:03:21,650
prefetch where the hardware and software

01:03:18,290 --> 01:03:23,390
they try to detect patterns and and the

01:03:21,650 --> 01:03:25,820
funny thing is when you when you

01:03:23,390 --> 01:03:28,730
streamline your data access all these

01:03:25,820 --> 01:03:31,070
things will fly for you so you do that

01:03:28,730 --> 01:03:34,310
change once you say okay I want to clean

01:03:31,070 --> 01:03:35,960
up my memory access and then all of a

01:03:34,310 --> 01:03:37,820
sudden you get a lot of extra benefit

01:03:35,960 --> 01:03:39,770
out of those things so that that would

01:03:37,820 --> 01:03:43,100
be my recommendation think about the

01:03:39,770 --> 01:03:49,850
memory picture and then you know see

01:03:43,100 --> 01:03:52,150
what what happens after that you're

01:03:49,850 --> 01:03:52,150

YouTube URL: https://www.youtube.com/watch?v=5-ZepxpwmUU


