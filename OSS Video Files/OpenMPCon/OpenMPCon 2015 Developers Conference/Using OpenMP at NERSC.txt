Title: Using OpenMP at NERSC
Publication date: 2015-12-13
Playlist: OpenMPCon 2015 Developers Conference
Description: 
	Helen He, Lawrence Berkeley National Lab., Univ of California
OpenMPCon 2015 - Aachen Germany - September 2015
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2015-helen-he-nersc.pdf

This presentation will describe how OpenMP is used at NERSC. NERSC is the primary supercomputing facility for Office of Science in the US Depart of Energy (DOE). Our next production system will be an Intel Xeon Phi Knights Landing (KNL) system, with 60+ cores per node and 4 hardware threads per core. The recommended programming model is hybrid MPI/OpenMP, which also promotes portability across different system architectures.

OpenMP usage statistics, such as the percentage of codes using OpenMP, typical number of threads used, etc., on current NERSC production systems will be analyzed. We will describe what we tell our users how to use OpenMP efficiently with multiple compilers on various NERSC systems, including how to obtain best process and thread affinity for hybrid MPI/OpenMP, memory locality with NUMA domains, programming tips for adding OpenMP, strategies for improving OpenMP scaling, how to use nested OpenMP, and tools available for OpenMP. Tuning examples with real scientific user codes will also be presented on improving OpenMP performance
Captions: 
	00:00:06,580 --> 00:00:16,129
my name is Helen T so I i I'm a nurse

00:00:13,700 --> 00:00:19,599
high performance computing consultant we

00:00:16,129 --> 00:00:22,189
help scientists to run our and facility

00:00:19,599 --> 00:00:24,529
this is my outline i'm going to

00:00:22,189 --> 00:00:27,560
introduce what is nurse and what is our

00:00:24,529 --> 00:00:29,480
new system outline i'm going to talk

00:00:27,560 --> 00:00:32,450
about where is nurse in our new system

00:00:29,480 --> 00:00:35,210
corey and why NPI plus open and p is

00:00:32,450 --> 00:00:38,120
preferred going to tell you about how

00:00:35,210 --> 00:00:40,610
the basics how much openmp is used at

00:00:38,120 --> 00:00:44,390
risk and then when we tell our users

00:00:40,610 --> 00:00:46,580
about OpenMP scaling and how process and

00:00:44,390 --> 00:00:49,040
stress right affinity how important are

00:00:46,580 --> 00:00:52,460
they some scaling tips and what a tools

00:00:49,040 --> 00:00:54,760
we have for openmp then i'll present a

00:00:52,460 --> 00:00:57,440
few case studies of tuning and

00:00:54,760 --> 00:01:03,740
optimizing the performance on opening

00:00:57,440 --> 00:01:07,009
hybrid MPI openmp so nurse so what is

00:01:03,740 --> 00:01:08,990
nurse and / lbnl if i have to say just

00:01:07,009 --> 00:01:11,630
one sentence what is nurse nurse gets

00:01:08,990 --> 00:01:14,990
the primary computing facility but the

00:01:11,630 --> 00:01:18,200
problem Energy Office of Science and a

00:01:14,990 --> 00:01:22,850
week's we support about seven thousand

00:01:18,200 --> 00:01:24,409
users 850 projects in 600 codes and the

00:01:22,850 --> 00:01:28,789
right side the allocation breakdown

00:01:24,409 --> 00:01:31,399
shows six program offices from goe we

00:01:28,789 --> 00:01:34,969
emphasize a lot of science we promote

00:01:31,399 --> 00:01:38,090
science help scientists to run and we

00:01:34,969 --> 00:01:40,640
have lots of journal publications and we

00:01:38,090 --> 00:01:43,189
have have four nobel prize winners from

00:01:40,640 --> 00:01:45,469
our users and yesterday at the panel

00:01:43,189 --> 00:01:47,859
people asked whether we to collaborate

00:01:45,469 --> 00:01:51,799
with vendors and yes we do we have

00:01:47,859 --> 00:01:54,530
always actively talking to vendors even

00:01:51,799 --> 00:01:56,780
years before our system is an arrived we

00:01:54,530 --> 00:02:01,369
have lots of first kind first kind of

00:01:56,780 --> 00:02:02,500
system and nursed nurse we use nurse 678

00:02:01,369 --> 00:02:06,180
as our procure

00:02:02,500 --> 00:02:08,800
so no six this hopper is it is a first

00:02:06,180 --> 00:02:12,100
crate petascale system with the gemini

00:02:08,800 --> 00:02:14,110
in the Kinect anuska seven Edison is the

00:02:12,100 --> 00:02:16,720
first petascale systems with Intel

00:02:14,110 --> 00:02:20,230
processors areas interconnect and

00:02:16,720 --> 00:02:22,510
dragonfly with dragonfly topology norsk

00:02:20,230 --> 00:02:25,120
ate curry will be one of it first and

00:02:22,510 --> 00:02:27,940
largest que nÃ£o systems and have its

00:02:25,120 --> 00:02:30,160
unique data features the quarry phase

00:02:27,940 --> 00:02:31,690
one which has war has already arrived we

00:02:30,160 --> 00:02:33,850
aren't do lots of configuration right

00:02:31,690 --> 00:02:38,110
now corey pays to will be arriving next

00:02:33,850 --> 00:02:41,560
year and sometimes ppl confused why you

00:02:38,110 --> 00:02:43,780
sometimes nurse sometimes lP&L so lbnl

00:02:41,560 --> 00:02:46,480
is our organization Lawrence Berkeley

00:02:43,780 --> 00:02:49,090
National Lab nurse as the computing

00:02:46,480 --> 00:02:51,160
facility is the division in the lab is

00:02:49,090 --> 00:02:56,290
one of the three divisions of the

00:02:51,160 --> 00:02:59,620
computer science area the big picture is

00:02:56,290 --> 00:03:03,520
that Cory is coming its energy efficient

00:02:59,620 --> 00:03:05,790
min ecosystem has over 9,300 single core

00:03:03,520 --> 00:03:08,140
saw single socket notes it will have

00:03:05,790 --> 00:03:11,110
multiple Numa domains we always consider

00:03:08,140 --> 00:03:13,690
about Numa domains for affinity things

00:03:11,110 --> 00:03:17,500
like that it will be self hosted not an

00:03:13,690 --> 00:03:20,470
accelerator and will have total of 288

00:03:17,500 --> 00:03:23,190
logical threads with 72 cords pernoding

00:03:20,470 --> 00:03:23,190
for Hardware threads

00:03:26,170 --> 00:03:34,160
it also has a VX 512 very large vector

00:03:30,739 --> 00:03:36,769
length of 512 bits equivalent of eight

00:03:34,160 --> 00:03:39,349
double precision elements and I have

00:03:36,769 --> 00:03:43,580
unpackage high bandwidth memory & poor's

00:03:39,349 --> 00:03:46,550
buffer for facile so this is a quick

00:03:43,580 --> 00:03:49,970
comparison with Edison we have now is

00:03:46,550 --> 00:03:53,480
ivy bridge and create xc30 system versus

00:03:49,970 --> 00:03:55,519
on Corey the Knights landing the number

00:03:53,480 --> 00:04:01,040
of course per CPU or increase

00:03:55,519 --> 00:04:04,670
significantly and per core cpu is M

00:04:01,040 --> 00:04:10,670
flower the vector lens will be doubled

00:04:04,670 --> 00:04:12,470
from 2050 6512 and memory per course

00:04:10,670 --> 00:04:15,829
local memories about the same but then

00:04:12,470 --> 00:04:20,510
the fast memory we have a unload a

00:04:15,829 --> 00:04:22,700
smaller smaller per core fast memory as

00:04:20,510 --> 00:04:25,370
a high which has a five times need ER

00:04:22,700 --> 00:04:28,700
for bandwidth and how also has our first

00:04:25,370 --> 00:04:31,490
buffer so some programming

00:04:28,700 --> 00:04:36,320
considerations to running on curry it's

00:04:31,490 --> 00:04:38,930
probably if you put two singles very

00:04:36,320 --> 00:04:41,870
simple pudding your application is

00:04:38,930 --> 00:04:45,350
likely to run but with pure mmpi it

00:04:41,870 --> 00:04:49,960
probably won't fit in the memory and you

00:04:45,350 --> 00:04:49,960
would have to explore unknown power ism

00:04:50,050 --> 00:04:55,940
because you want to run pure MPI with so

00:04:52,880 --> 00:04:59,060
many tasks / note it all has lots of

00:04:55,940 --> 00:05:01,610
overhead to want to use em arnold / ism

00:04:59,060 --> 00:05:06,169
was either with both r at scaling with

00:05:01,610 --> 00:05:09,500
OpenMP or aunt vectorization to explore

00:05:06,169 --> 00:05:13,430
the thing the not explore the the larger

00:05:09,500 --> 00:05:15,020
vector lens and then for Corey also

00:05:13,430 --> 00:05:18,200
would like to explore high bandwidth

00:05:15,020 --> 00:05:21,930
memory and / special options so we

00:05:18,200 --> 00:05:24,999
recommend hybrid MPI openmp

00:05:21,930 --> 00:05:28,449
for both scaling capability and for our

00:05:24,999 --> 00:05:30,669
code portability that we have the

00:05:28,449 --> 00:05:33,569
current system the Addison hopper are

00:05:30,669 --> 00:05:37,210
crazy systems and the Babbage is our

00:05:33,569 --> 00:05:40,449
Intel Xeon Phi a nice corner testbed so

00:05:37,210 --> 00:05:44,650
we ask users can now prepare codes for

00:05:40,449 --> 00:05:45,819
query on this on these systems and once

00:05:44,650 --> 00:05:48,789
lied about Portability and

00:05:45,819 --> 00:05:52,259
maintainability many of our users are

00:05:48,789 --> 00:05:56,259
also uses at other gioi labs and

00:05:52,259 --> 00:06:00,310
regardless regardless of their different

00:05:56,259 --> 00:06:01,810
architectures users would have to to

00:06:00,310 --> 00:06:04,689
modify their codes to achieve

00:06:01,810 --> 00:06:09,159
performance they would always have want

00:06:04,689 --> 00:06:11,289
to expose more unload terrorism and open

00:06:09,159 --> 00:06:14,020
implicants reading can help and to

00:06:11,289 --> 00:06:17,080
increase vectorization he'll be pleased

00:06:14,020 --> 00:06:20,889
when p openmp simony can help that's why

00:06:17,080 --> 00:06:24,370
we want to promote OpenMP as an industry

00:06:20,889 --> 00:06:27,099
standard for Portability and it's like

00:06:24,370 --> 00:06:30,219
Tim mentioned portability is sort of

00:06:27,099 --> 00:06:33,520
hard to achieve goal but the real goal

00:06:30,219 --> 00:06:35,860
is programming maintainability you would

00:06:33,520 --> 00:06:38,589
have liked to have a single source for

00:06:35,860 --> 00:06:40,270
your application and to reduce as many

00:06:38,589 --> 00:06:42,849
as possible as much as possible for

00:06:40,270 --> 00:06:46,629
those if deaths whether if def GPU cpu

00:06:42,849 --> 00:06:53,220
ifdef different compilers ifdef open sec

00:06:46,629 --> 00:06:55,920
OpenMP cuda 4chan or things like that so

00:06:53,220 --> 00:06:58,560
this is and want to talk about

00:06:55,920 --> 00:07:00,240
application readiness for curry nurse

00:06:58,560 --> 00:07:03,720
has already begun to transition our

00:07:00,240 --> 00:07:06,090
workload to the quarry system however as

00:07:03,720 --> 00:07:07,890
I mentioned we have over 600 projects is

00:07:06,090 --> 00:07:11,430
impossible to hand holding every one of

00:07:07,890 --> 00:07:14,700
them so we have the right side is our

00:07:11,430 --> 00:07:17,640
breakdown of the application basically

00:07:14,700 --> 00:07:19,500
10 top top 10 codes would make up about

00:07:17,640 --> 00:07:21,990
forty five percent of our workload and

00:07:19,500 --> 00:07:24,450
25 with percent code when makeup

00:07:21,990 --> 00:07:26,940
sixty-six percent that workload so what

00:07:24,450 --> 00:07:30,870
we did is we chose 20 applications and

00:07:26,940 --> 00:07:33,450
as as tier 1 and tier 2 we have about 30

00:07:30,870 --> 00:07:35,430
more tier 3 with access which with tier

00:07:33,450 --> 00:07:38,010
1 and tier two teams we work with them

00:07:35,430 --> 00:07:41,940
very closely with create Intel and we

00:07:38,010 --> 00:07:44,220
have a nurse team together to look

00:07:41,940 --> 00:07:48,630
intensively after code optimization and

00:07:44,220 --> 00:07:50,250
with these three levels of code on right

00:07:48,630 --> 00:07:53,490
side you can see the green and light

00:07:50,250 --> 00:07:55,470
green which covers a lot of percentage

00:07:53,490 --> 00:07:59,490
of the application of the norsk workload

00:07:55,470 --> 00:08:01,260
some of them are as proxies we also

00:07:59,490 --> 00:08:03,450
collaborate with other govt centers and

00:08:01,260 --> 00:08:07,470
we would distribute our lessons learned

00:08:03,450 --> 00:08:10,470
to the communities so the next thing I

00:08:07,470 --> 00:08:15,210
want to talk about is the openmp usage

00:08:10,470 --> 00:08:17,820
and norsk some of slides James already

00:08:15,210 --> 00:08:20,729
shown yesterday the first slide about

00:08:17,820 --> 00:08:22,860
language is used in nursing so again I

00:08:20,729 --> 00:08:24,780
want I basically one emphasize to this

00:08:22,860 --> 00:08:27,600
OpenMP community that don't please don't

00:08:24,780 --> 00:08:29,669
forget about 4chan users because of the

00:08:27,600 --> 00:08:32,099
legacy of science applications may never

00:08:29,669 --> 00:08:35,010
use many of these application codes as I

00:08:32,099 --> 00:08:37,560
like millions of lines or have millions

00:08:35,010 --> 00:08:39,839
lines so it's hard to convert from

00:08:37,560 --> 00:08:43,660
fortune to see they are just just there

00:08:39,839 --> 00:08:46,460
and if we from this one this

00:08:43,660 --> 00:08:49,640
the data in this plot is collected from

00:08:46,460 --> 00:08:54,650
honors projects but we just choose by if

00:08:49,640 --> 00:08:56,930
we count by machine hours use then even

00:08:54,650 --> 00:09:00,140
about two-thirds of topcoats primary use

00:08:56,930 --> 00:09:04,400
fortune some some communities even just

00:09:00,140 --> 00:09:07,850
the hundred percent fortune like climate

00:09:04,400 --> 00:09:10,700
that I know of about probing models

00:09:07,850 --> 00:09:13,700
basically MPI's new dominates and forty

00:09:10,700 --> 00:09:16,670
percent of projects using OpenMP so far

00:09:13,700 --> 00:09:19,010
this is about number of projects not

00:09:16,670 --> 00:09:22,490
about hours and I'll show data about

00:09:19,010 --> 00:09:27,020
machine hours usage and the percentage

00:09:22,490 --> 00:09:31,250
of hours used at openmp with openmp and

00:09:27,020 --> 00:09:34,730
this is anas and so about if you use NPI

00:09:31,250 --> 00:09:38,150
plus X what is X so open and P is about

00:09:34,730 --> 00:09:42,020
fifty percent of that X and kuda kuda

00:09:38,150 --> 00:09:44,420
fortune open a cc is a kind of Anthony

00:09:42,020 --> 00:09:48,770
on the top ish and site besides peace

00:09:44,420 --> 00:09:50,750
rats it was because we had a GPU test

00:09:48,770 --> 00:09:55,880
bed for a while and I think users

00:09:50,750 --> 00:09:59,150
explore these options we we also have

00:09:55,880 --> 00:10:02,270
TBB and silk users a small percentage we

00:09:59,150 --> 00:10:05,600
get tickets from them sometimes

00:10:02,270 --> 00:10:09,910
okay so this is a breakdown of with a

00:10:05,600 --> 00:10:12,530
fraction of MPP hours and how much of

00:10:09,910 --> 00:10:15,950
openmp is used it's about twenty percent

00:10:12,530 --> 00:10:18,470
on Edison and it syncs our newer system

00:10:15,950 --> 00:10:22,340
and it's it's increasing over the years

00:10:18,470 --> 00:10:26,110
compared with Harper for example so you

00:10:22,340 --> 00:10:31,340
can see different number of threads and

00:10:26,110 --> 00:10:32,990
with an when the from awkward to edison

00:10:31,340 --> 00:10:35,780
you can see that number of threats used

00:10:32,990 --> 00:10:40,690
is higher because the number of course

00:10:35,780 --> 00:10:40,690
available within one humor domain and

00:10:40,870 --> 00:10:47,060
and people use open and p not because of

00:10:45,290 --> 00:10:49,220
they don't have enough memory I think

00:10:47,060 --> 00:10:51,530
it's open if you especially helps you to

00:10:49,220 --> 00:10:54,860
scale to higher core counts that's one

00:10:51,530 --> 00:10:57,260
of the main reasons and for the current

00:10:54,860 --> 00:10:59,270
system but for Corey it's it's also

00:10:57,260 --> 00:11:03,800
because of you want explore more unknown

00:10:59,270 --> 00:11:06,800
paradism so this slide shows and

00:11:03,800 --> 00:11:10,610
basically this number of threats used

00:11:06,800 --> 00:11:13,550
versus the job sighs the bigger the job

00:11:10,610 --> 00:11:15,770
sighs people use more threads this is

00:11:13,550 --> 00:11:17,930
also as the point of reason I'm because

00:11:15,770 --> 00:11:20,450
the reason I mentioned only with when

00:11:17,930 --> 00:11:23,420
you explore more with open mpi down to

00:11:20,450 --> 00:11:28,940
pure MPI you're able to run folk for

00:11:23,420 --> 00:11:31,520
capacity of the system and also I didn't

00:11:28,940 --> 00:11:35,920
show here on the slides the job size /

00:11:31,520 --> 00:11:39,310
64k even two-thirds of those jobs use up

00:11:35,920 --> 00:11:39,310
open em chi

00:11:44,150 --> 00:11:48,770
so this is an interesting story

00:11:45,980 --> 00:11:51,830
interesting slide that the different

00:11:48,770 --> 00:11:54,830
science areas on adopting openmp very

00:11:51,830 --> 00:11:57,710
differently some of that on the right

00:11:54,830 --> 00:12:02,780
side of the slides you can see these

00:11:57,710 --> 00:12:05,960
areas almost not a pure MPI yeah and so

00:12:02,780 --> 00:12:09,860
one of the best using science areas are

00:12:05,960 --> 00:12:12,970
astrophysics infusion and thread number

00:12:09,860 --> 00:12:15,830
of that they use is 12 and this one

00:12:12,970 --> 00:12:17,720
biosigns you can see their number of

00:12:15,830 --> 00:12:20,510
slides uses 24th looks like they're

00:12:17,720 --> 00:12:22,790
doing lots of like you can see

00:12:20,510 --> 00:12:26,330
embarrassingly in openmp not

00:12:22,790 --> 00:12:28,430
embarrassingly happy I they use a treat

00:12:26,330 --> 00:12:32,510
each thread and separate individually

00:12:28,430 --> 00:12:36,740
doing their work there's also an area

00:12:32,510 --> 00:12:39,470
fusion but that now nuclear physics they

00:12:36,740 --> 00:12:44,710
use number four threads as 48 turning

00:12:39,470 --> 00:12:44,710
out hyper threading on intel on Edison

00:12:46,240 --> 00:12:52,160
so next section I want to tell you what

00:12:49,640 --> 00:12:54,620
we tell our users about openmp skating

00:12:52,160 --> 00:12:56,030
or some tricks and things I won't be

00:12:54,620 --> 00:12:58,400
able to cover everything and we tell

00:12:56,030 --> 00:13:01,550
users but pick some things we think I

00:12:58,400 --> 00:13:05,120
think aren't important first one is why

00:13:01,550 --> 00:13:11,780
scaling on important so this is a slide

00:13:05,120 --> 00:13:14,810
from James riders book for for smaller

00:13:11,780 --> 00:13:17,030
number of threads on CL would win but

00:13:14,810 --> 00:13:20,060
then slowly stay on fire will catch up

00:13:17,030 --> 00:13:25,040
like James yesterday if you do eight

00:13:20,060 --> 00:13:28,730
time 16 single single position from

00:13:25,040 --> 00:13:33,320
vectorization when x 240 threats it's

00:13:28,730 --> 00:13:35,210
over 3,000 but then here we are try if

00:13:33,320 --> 00:13:37,700
we're from this skating plot we are not

00:13:35,210 --> 00:13:39,920
actually we know scaling is important on

00:13:37,700 --> 00:13:41,990
the note but we're not trying to scare

00:13:39,920 --> 00:13:44,420
our users you have to use to run all the

00:13:41,990 --> 00:13:46,220
way to 240 threats you have to scale

00:13:44,420 --> 00:13:48,580
perfectly with that or you have to use

00:13:46,220 --> 00:13:50,720
140 MPI tasks to scale perfectly

00:13:48,580 --> 00:13:52,100
basically we will tell you is a try

00:13:50,720 --> 00:13:54,769
hybrid MPI open

00:13:52,100 --> 00:13:59,269
p and find a sweet spot it's probably

00:13:54,769 --> 00:14:04,160
between say four to 15 mpi tasks x 8 to

00:13:59,269 --> 00:14:06,019
20 / something like that so what we ask

00:14:04,160 --> 00:14:10,670
users is always do your open and cheese

00:14:06,019 --> 00:14:14,029
skating study and so it all started it

00:14:10,670 --> 00:14:16,459
would depend on the the problem sighs of

00:14:14,029 --> 00:14:18,620
course and you fix the number of MCI and

00:14:16,459 --> 00:14:21,199
you find out how a bit open empty

00:14:18,620 --> 00:14:22,790
skating start to fall down and then this

00:14:21,199 --> 00:14:25,730
is the basically the optimal number

00:14:22,790 --> 00:14:27,829
threads would like to use so the next

00:14:25,730 --> 00:14:31,040
one we want to do is open and P versus

00:14:27,829 --> 00:14:33,050
NP i vs openmp skating analysis so this

00:14:31,040 --> 00:14:37,069
plot is kind of hard to read let me try

00:14:33,050 --> 00:14:39,699
to explain you see four lines here let's

00:14:37,069 --> 00:14:42,350
try to focus on the first line purple

00:14:39,699 --> 00:14:45,410
this is basically a line of saying you

00:14:42,350 --> 00:14:47,690
fix total number of MPI times openmp

00:14:45,410 --> 00:14:51,019
threads so the for this pro line is

00:14:47,690 --> 00:14:53,839
total number is 60 the first dot is they

00:14:51,019 --> 00:14:56,300
use 60 MPI tasks times one thread and

00:14:53,839 --> 00:15:00,740
the second one is 30 and tight Eyez

00:14:56,300 --> 00:15:04,040
times on two threads and then you do the

00:15:00,740 --> 00:15:07,519
same for a different way to 121 and 80

00:15:04,040 --> 00:15:09,470
and 240 so then you see where is your

00:15:07,519 --> 00:15:11,779
heart a sweet spot in this plot you

00:15:09,470 --> 00:15:15,829
would see sweet spot is about the red

00:15:11,779 --> 00:15:20,029
line at the bottom at 15 x 15 mg I tasks

00:15:15,829 --> 00:15:22,970
and then this line is a 240 way total so

00:15:20,029 --> 00:15:25,279
you know this is sorry this line is a

00:15:22,970 --> 00:15:28,730
100 turning away total so now you know

00:15:25,279 --> 00:15:32,540
your sweet spot is 15mm MPI tasks x 8

00:15:28,730 --> 00:15:34,550
open and P threads the sweet spots again

00:15:32,540 --> 00:15:37,189
still depending on your promise eyes and

00:15:34,550 --> 00:15:40,130
your algorithms but this is a good

00:15:37,189 --> 00:15:45,130
guideline to start from optimizing and

00:15:40,130 --> 00:15:45,130
always compare that with your results

00:15:46,100 --> 00:15:56,250
no this whole study is on the off day on

00:15:49,710 --> 00:15:59,160
Phi just this study case what but

00:15:56,250 --> 00:16:01,260
okay so the next few slides i wanna

00:15:59,160 --> 00:16:03,720
emphasize on process and spread

00:16:01,260 --> 00:16:06,380
affinities importance of those the first

00:16:03,720 --> 00:16:09,360
let me introduce our systems and

00:16:06,380 --> 00:16:13,500
basically a schematic plot of Numa

00:16:09,360 --> 00:16:16,770
domain and and yeah basically the core

00:16:13,500 --> 00:16:18,690
the the note and the Numa domain so for

00:16:16,770 --> 00:16:21,750
the left side is our hopper is the cray

00:16:18,690 --> 00:16:23,280
xe6 it has four Numa domains per node

00:16:21,750 --> 00:16:27,390
and sixth course current human domain

00:16:23,280 --> 00:16:30,540
and forth on the right side is our

00:16:27,390 --> 00:16:33,960
Edison and the cray xc30 system it has

00:16:30,540 --> 00:16:37,860
to Numa domains and print out 12 cores

00:16:33,960 --> 00:16:40,470
per per domain and it has also has two

00:16:37,860 --> 00:16:48,000
Hardware threads per core so this is

00:16:40,470 --> 00:16:51,330
what we get then the slide is about some

00:16:48,000 --> 00:16:54,660
of the options how you try to make sure

00:16:51,330 --> 00:16:55,860
your empty i distribute MPI rank evilly

00:16:54,660 --> 00:17:00,450
on two different Numa knows how

00:16:55,860 --> 00:17:02,670
important that so this and how to use

00:17:00,450 --> 00:17:05,550
some kind of a long time control to

00:17:02,670 --> 00:17:07,530
achieve that the AP run is sort of MPI

00:17:05,550 --> 00:17:09,990
equivalent to run our crazy system

00:17:07,530 --> 00:17:14,339
there's an option called dash S which is

00:17:09,990 --> 00:17:17,010
MPI tasks / Numa domain and the plot on

00:17:14,339 --> 00:17:19,319
the right shows a gtc performance with

00:17:17,010 --> 00:17:21,270
and without distributing your process

00:17:19,319 --> 00:17:28,230
MPI tasks under different Numa domain

00:17:21,270 --> 00:17:31,620
and the to the blue and yellow we can

00:17:28,230 --> 00:17:34,110
see that with different number of MPI

00:17:31,620 --> 00:17:36,360
tasks times openmp threats within

00:17:34,110 --> 00:17:39,360
without always and when you distribute

00:17:36,360 --> 00:17:42,990
is a win but and also in this plot he

00:17:39,360 --> 00:17:47,930
also helps also I also demonstrate the

00:17:42,990 --> 00:17:47,930
sweet spot check a choice in it

00:17:48,690 --> 00:17:53,620
now come now get down to the thread

00:17:51,460 --> 00:17:55,630
affinity after process affinity you want

00:17:53,620 --> 00:17:57,670
to make sure your threads is are you

00:17:55,630 --> 00:18:01,780
know close to where ever your MPI and

00:17:57,670 --> 00:18:05,170
the tasks are and as close as close as

00:18:01,780 --> 00:18:07,720
possible so the default for that the AP

00:18:05,170 --> 00:18:11,080
option is that CC it basically binds

00:18:07,720 --> 00:18:13,800
your thread to a specific or within a

00:18:11,080 --> 00:18:17,380
within a Numa node so it's good for

00:18:13,800 --> 00:18:20,140
other compilers basically folk we have

00:18:17,380 --> 00:18:23,980
creme pie or PGI compiler and going to

00:18:20,140 --> 00:18:26,440
compile and Intel so every city every

00:18:23,980 --> 00:18:28,690
compiler except Intel that CC CPU is

00:18:26,440 --> 00:18:30,910
great always finds you to the direct

00:18:28,690 --> 00:18:38,050
core with an intel compile it does have

00:18:30,910 --> 00:18:40,120
a basic web manager managers read so so

00:18:38,050 --> 00:18:43,000
if you do that then when that managers

00:18:40,120 --> 00:18:44,980
red is busy working on managing and then

00:18:43,000 --> 00:18:47,620
the work assigned to the thread for

00:18:44,980 --> 00:18:50,020
opening p work will be blocked so we

00:18:47,620 --> 00:18:52,780
tell users is please don't use that if

00:18:50,020 --> 00:18:55,030
you have you want to use one employee

00:18:52,780 --> 00:18:57,970
only on your note new stash CC nom

00:18:55,030 --> 00:19:00,310
basically allow you thread OpenMP swear

00:18:57,970 --> 00:19:03,790
to migrate withing the node and if you

00:19:00,310 --> 00:19:05,470
have say one MPI tasks / Numa node then

00:19:03,790 --> 00:19:06,910
you would allow your threads to migrate

00:19:05,470 --> 00:19:10,980
within your Numa node you would use that

00:19:06,910 --> 00:19:13,600
CC Numa node and there was also a Cray

00:19:10,980 --> 00:19:16,210
improvement if you have multiple MPI

00:19:13,600 --> 00:19:18,160
within a Numa node there's a smaller

00:19:16,210 --> 00:19:20,980
confinement you can allow your thread to

00:19:18,160 --> 00:19:23,430
migrate freely which you can use with CC

00:19:20,980 --> 00:19:23,430
depths

00:19:23,980 --> 00:19:32,200
so this is a flight about the KNC test

00:19:28,360 --> 00:19:36,900
bed so for this test back we have 45

00:19:32,200 --> 00:19:42,220
nodes and on each host node we have to

00:19:36,900 --> 00:19:45,100
Mike cards and so up to 20 and I'm sorry

00:19:42,220 --> 00:19:48,940
on each my card we have 60 corns times

00:19:45,100 --> 00:19:50,710
for Hardware threads since we know what

00:19:48,940 --> 00:19:54,250
the chorus chorus look like before we

00:19:50,710 --> 00:19:56,910
get the skin C test bad so because can't

00:19:54,250 --> 00:19:59,770
curry is going to be self hosted and

00:19:56,910 --> 00:20:01,840
what we tell users is that on Babbage

00:19:59,770 --> 00:20:04,059
when you try to use web which to prepare

00:20:01,840 --> 00:20:06,549
for Corey use the native mode only

00:20:04,059 --> 00:20:10,380
native mode means you run directly on

00:20:06,549 --> 00:20:12,669
the mic without worrying about internode

00:20:10,380 --> 00:20:15,669
communication without worry about how to

00:20:12,669 --> 00:20:18,700
know communication things like that so

00:20:15,669 --> 00:20:22,210
even though the like symmetric offload

00:20:18,700 --> 00:20:24,549
all these most work openmp target work

00:20:22,210 --> 00:20:28,120
everything works but for the recommend

00:20:24,549 --> 00:20:33,540
model on this machine is just native and

00:20:28,120 --> 00:20:33,540
try to focus on single node optimization

00:20:34,260 --> 00:20:40,780
so this is an illustration of the

00:20:36,910 --> 00:20:44,130
Babbage card Mike bank might card so you

00:20:40,780 --> 00:20:48,160
treated as one single Newman domain and

00:20:44,130 --> 00:20:50,980
these how many course they are so there

00:20:48,160 --> 00:20:53,710
are incubus if 'ok environment variables

00:20:50,980 --> 00:20:55,809
to control affinity there's also we all

00:20:53,710 --> 00:20:58,710
can tell users you can use on p-bog bind

00:20:55,809 --> 00:21:04,120
to do the threat control there's also

00:20:58,710 --> 00:21:06,220
the IMT I ping domain for mpi / OpenMP

00:21:04,120 --> 00:21:08,770
process and affinity control for that

00:21:06,220 --> 00:21:11,059
i'm not i'm not going to show details

00:21:08,770 --> 00:21:15,139
but there are ways to to come to

00:21:11,059 --> 00:21:17,600
managed so I talked about swear the

00:21:15,139 --> 00:21:19,909
process and flat affinity now its memory

00:21:17,600 --> 00:21:24,679
affinity so we tell users about first

00:21:19,909 --> 00:21:29,419
touch means and the memory is assignment

00:21:24,679 --> 00:21:32,149
which threat which memory is assigned to

00:21:29,419 --> 00:21:34,519
that's the way this thread is working on

00:21:32,149 --> 00:21:37,249
based on when you initialize it not when

00:21:34,519 --> 00:21:39,769
you allocate it so we tell users you can

00:21:37,249 --> 00:21:45,139
try to create another initialization

00:21:39,769 --> 00:21:47,629
section it was the exact same scheduling

00:21:45,139 --> 00:21:51,619
in number of threads you get that so the

00:21:47,629 --> 00:21:55,460
plot here shows with in without and if

00:21:51,619 --> 00:21:57,769
you on this on this hopper you have 66

00:21:55,460 --> 00:22:01,580
threads per Numa domain so once it's

00:21:57,769 --> 00:22:03,860
over six year if your memory is in the

00:22:01,580 --> 00:22:07,309
foreign you might owe me a new program

00:22:03,860 --> 00:22:09,200
performance suffers so the this is so

00:22:07,309 --> 00:22:11,149
hard very hard for real application to

00:22:09,200 --> 00:22:12,950
do all these so what we tell users is

00:22:11,149 --> 00:22:15,379
basically please use less than six

00:22:12,950 --> 00:22:18,590
threads on our hopper machine and then

00:22:15,379 --> 00:22:22,779
use boy MPI tasks that's an easy way to

00:22:18,590 --> 00:22:22,779
achieve equivalent performance

00:22:22,850 --> 00:22:28,220
and we also try to obviously promote

00:22:26,630 --> 00:22:29,660
nested open empty more and more

00:22:28,220 --> 00:22:33,140
applications are trying to do this now

00:22:29,660 --> 00:22:35,210
and this slide is going to shows how to

00:22:33,140 --> 00:22:37,790
achieve that and you want to set over

00:22:35,210 --> 00:22:40,400
empty nest is true however you do get

00:22:37,790 --> 00:22:43,310
number of threads in each level but it's

00:22:40,400 --> 00:22:46,880
more important to to get it get the best

00:22:43,310 --> 00:22:49,700
process in infinity so that best get the

00:22:46,880 --> 00:22:53,360
best process affinity binding with

00:22:49,700 --> 00:22:55,850
nested openmp so that performance were

00:22:53,360 --> 00:22:58,460
not suffer but it's not very

00:22:55,850 --> 00:23:00,800
straightforward to do so and it also

00:22:58,460 --> 00:23:03,050
there's varying factors about what

00:23:00,800 --> 00:23:04,790
compiler you use what match schedulers

00:23:03,050 --> 00:23:07,790
you use and these are flags with the

00:23:04,790 --> 00:23:14,180
runtime control and also on different

00:23:07,790 --> 00:23:15,530
systems and like different compilers i

00:23:14,180 --> 00:23:18,260
mentioned i think has a different

00:23:15,530 --> 00:23:21,200
manuals read and with on talking more

00:23:18,260 --> 00:23:24,200
autocraft our crazy system we use a peer

00:23:21,200 --> 00:23:26,300
on and we're now going to transition to

00:23:24,200 --> 00:23:27,590
slower so it's going to be s round and

00:23:26,300 --> 00:23:30,530
all the flags are going to be different

00:23:27,590 --> 00:23:32,990
in our different systems and i'm talking

00:23:30,530 --> 00:23:35,000
about crazy systems and intel K&C so

00:23:32,990 --> 00:23:38,440
it's all different factors together and

00:23:35,000 --> 00:23:41,780
it's hard to achieve but this is example

00:23:38,440 --> 00:23:44,290
settings but then i put together a web

00:23:41,780 --> 00:23:47,030
page I'll nested OpenMP with detailed

00:23:44,290 --> 00:23:50,090
examples and there's a sample code from

00:23:47,030 --> 00:23:51,860
Crais basically prints out the affinity

00:23:50,090 --> 00:23:57,050
so helps use it to check whether they

00:23:51,860 --> 00:24:00,230
get their their desired binding okay

00:23:57,050 --> 00:24:03,260
open mp4 simdi this is light from my

00:24:00,230 --> 00:24:05,420
colleague Jack he says a year ago to get

00:24:03,260 --> 00:24:07,670
a vector code you have to use intrinsic

00:24:05,420 --> 00:24:09,920
and you pray the compiler would choose

00:24:07,670 --> 00:24:12,659
to vectorize the loop or you have to use

00:24:09,920 --> 00:24:16,080
crayon compiler specific directives

00:24:12,659 --> 00:24:18,659
today you have the sim dl option to use

00:24:16,080 --> 00:24:21,179
that and of course because simply you

00:24:18,659 --> 00:24:24,179
force compiler to to vectorize for you

00:24:21,179 --> 00:24:27,029
it doesn't bypassed compile analysis so

00:24:24,179 --> 00:24:28,349
you might get some incorrect results

00:24:27,029 --> 00:24:30,059
because you tell users there's no

00:24:28,349 --> 00:24:36,629
dependency they're just just vectorize

00:24:30,059 --> 00:24:39,379
it and here is simply you can do do OMP

00:24:36,629 --> 00:24:41,759
do simdi for the for the do you would do

00:24:39,379 --> 00:24:44,429
distribute work amounts threats and then

00:24:41,759 --> 00:24:48,479
for the simdi with the distribute the

00:24:44,429 --> 00:24:50,999
work on Terra further was to fitting to

00:24:48,479 --> 00:24:54,479
the assembly vector register to do that

00:24:50,999 --> 00:24:57,330
and I personally liked open-pit eclairs

00:24:54,479 --> 00:24:59,820
md because it tells you a compiler to

00:24:57,330 --> 00:25:02,220
generate a simdi function we have a use

00:24:59,820 --> 00:25:06,479
case for that we have elemental function

00:25:02,220 --> 00:25:09,269
and the compiler it's very hard to go

00:25:06,479 --> 00:25:10,909
compiler to vectorize it so i'm going to

00:25:09,269 --> 00:25:17,129
show that later we use that phone

00:25:10,909 --> 00:25:22,019
feature and few slides and a slide about

00:25:17,129 --> 00:25:24,570
adding open and key so we recommend

00:25:22,019 --> 00:25:28,070
users to use a crave review tool which

00:25:24,570 --> 00:25:31,679
is a part of the crepe of virtuous

00:25:28,070 --> 00:25:33,749
package and it does scope analysis and

00:25:31,679 --> 00:25:36,929
based on performance data analysis

00:25:33,749 --> 00:25:41,879
performance um and that is plus the

00:25:36,929 --> 00:25:43,710
compiler the loop mark lifting analysis

00:25:41,879 --> 00:25:45,299
and optimizing vectorization results

00:25:43,710 --> 00:25:48,179
combined together it does suggest you

00:25:45,299 --> 00:25:51,320
the openmp directives to add sometimes

00:25:48,179 --> 00:25:53,039
it'll tell you I have unresolved

00:25:51,320 --> 00:25:56,159
variables I don't know whether it's

00:25:53,039 --> 00:25:59,489
shared or private but users would go

00:25:56,159 --> 00:26:01,109
back and change those it doesn't do many

00:25:59,489 --> 00:26:03,659
fancy things they don't do reduction

00:26:01,109 --> 00:26:07,379
things like that but it just a basic

00:26:03,659 --> 00:26:10,379
loop level and can start from sing like

00:26:07,379 --> 00:26:12,150
topmost how hot regions and in users can

00:26:10,379 --> 00:26:15,030
do this incrementally

00:26:12,150 --> 00:26:18,270
so that's it you have to do it on credit

00:26:15,030 --> 00:26:21,780
compiler first but then the generated

00:26:18,270 --> 00:26:23,580
that the open open empty mised code can

00:26:21,780 --> 00:26:27,540
be used to come to use other compilers

00:26:23,580 --> 00:26:30,390
to to run with it and another key and

00:26:27,540 --> 00:26:34,170
see the Intel advisor to basically helps

00:26:30,390 --> 00:26:37,260
you to kind what which sweating options

00:26:34,170 --> 00:26:42,540
are good whether you want to use openmp

00:26:37,260 --> 00:26:44,220
or TVB give you some guidance this is

00:26:42,540 --> 00:26:46,620
the list of the performance and

00:26:44,220 --> 00:26:49,290
debugging tours available on our systems

00:26:46,620 --> 00:26:51,390
basically we have vtune on both our

00:26:49,290 --> 00:26:54,150
systems and we actually promoting

00:26:51,390 --> 00:26:56,880
reading a lot right now and performance

00:26:54,150 --> 00:26:58,770
wise we have a linear map on both

00:26:56,880 --> 00:27:01,800
systems and creeper form create and

00:26:58,770 --> 00:27:06,060
tours on crazy systems and Intel tools

00:27:01,800 --> 00:27:11,400
on intel says town and debugging we use

00:27:06,060 --> 00:27:14,610
a DDT and total you mostly and next few

00:27:11,400 --> 00:27:17,190
slides are basic general program tips

00:27:14,610 --> 00:27:20,130
which hell uses how to add your OpenMP

00:27:17,190 --> 00:27:23,040
we we tell users based on your algorithm

00:27:20,130 --> 00:27:24,840
what's easier and white one day decide

00:27:23,040 --> 00:27:28,020
whether you want fine green or course

00:27:24,840 --> 00:27:31,140
grand power implementation and we tell

00:27:28,020 --> 00:27:34,920
users add OpenMP incrementally use your

00:27:31,140 --> 00:27:36,810
tools to find hotspot first and to

00:27:34,920 --> 00:27:39,660
paralyze outer loop and if you can

00:27:36,810 --> 00:27:42,780
collapse some loops minimize shared

00:27:39,660 --> 00:27:45,510
variables minimize barriers and decide

00:27:42,780 --> 00:27:49,400
and the last year is harder to do

00:27:45,510 --> 00:27:54,190
overlap MPI communication and

00:27:49,400 --> 00:27:57,830
computation and consider open mg tasking

00:27:54,190 --> 00:28:00,010
how do i check practice but i think

00:27:57,830 --> 00:28:05,030
compared with your sequential code first

00:28:00,010 --> 00:28:07,600
ompi code yeah there's a tools like a

00:28:05,030 --> 00:28:10,760
mention in the first previous slide like

00:28:07,600 --> 00:28:15,679
Intel inspector and DDT total you there

00:28:10,760 --> 00:28:19,070
also helps to check out openmp code ok

00:28:15,679 --> 00:28:21,230
so users my ask why sometimes my MPI

00:28:19,070 --> 00:28:23,600
openmp code is slower than pure MPI

00:28:21,230 --> 00:28:26,299
maybe you have a serious action cannot

00:28:23,600 --> 00:28:28,220
be what's not paralyzed we maybe you're

00:28:26,299 --> 00:28:29,960
running to the false sharing situation

00:28:28,220 --> 00:28:33,590
and there's always thread creation

00:28:29,960 --> 00:28:37,190
overhead and then Numa effects is huge

00:28:33,590 --> 00:28:41,120
always and actually easy to fix as well

00:28:37,190 --> 00:28:42,950
and then maybe you do not have in a for

00:28:41,120 --> 00:28:45,409
each thread you have loading balance

00:28:42,950 --> 00:28:47,179
amount threats and when you do MPI

00:28:45,409 --> 00:28:51,440
communication maybe your other threads

00:28:47,179 --> 00:28:53,419
are idling and last one is that

00:28:51,440 --> 00:28:59,179
sometimes you want a more optimized

00:28:53,419 --> 00:29:00,799
compilers libraries for sweating and we

00:28:59,179 --> 00:29:03,460
tell you sir if a routine does not scale

00:29:00,799 --> 00:29:06,409
well what you want to try to check

00:29:03,460 --> 00:29:10,429
examine your cereal section the critical

00:29:06,409 --> 00:29:14,270
section to innovate eliminate those and

00:29:10,429 --> 00:29:17,150
and to also to reduce them up when MP

00:29:14,270 --> 00:29:19,179
pero regions to reduce overhead and we

00:29:17,150 --> 00:29:21,650
talked about loop claps loop fusion

00:29:19,179 --> 00:29:25,520
permutation things like that and add you

00:29:21,650 --> 00:29:27,340
know weight and for loading balance you

00:29:25,520 --> 00:29:30,169
can try different scheduling options

00:29:27,340 --> 00:29:33,049
dynamic that's a good one

00:29:30,169 --> 00:29:35,179
and the other one is for open in peace

00:29:33,049 --> 00:29:38,359
scaling you want to allow you mention

00:29:35,179 --> 00:29:42,200
try to try to find your MP I and the

00:29:38,359 --> 00:29:44,539
sweet spot for the combination and the

00:29:42,200 --> 00:29:47,210
easiest one I always try to emphasize is

00:29:44,539 --> 00:29:48,470
to test different processes threat if

00:29:47,210 --> 00:29:53,330
any of the options it's actually low

00:29:48,470 --> 00:29:55,429
hanging fruit easy to get then to modify

00:29:53,330 --> 00:29:59,899
your algorithm to achieve some kind of

00:29:55,429 --> 00:30:02,600
performance so I have a improving openmp

00:29:59,899 --> 00:30:04,399
scaling web page to help users to go

00:30:02,600 --> 00:30:08,749
through some of the steps and some case

00:30:04,399 --> 00:30:11,809
studies are there as well so now I'm

00:30:08,749 --> 00:30:15,139
going to talk about case studies a lots

00:30:11,809 --> 00:30:17,059
of users provide me the error basically

00:30:15,139 --> 00:30:22,249
it's at some some of the applications

00:30:17,059 --> 00:30:25,879
that touched upon aspects i mentioned so

00:30:22,249 --> 00:30:28,309
this is npv hybrid impaired openmp so

00:30:25,879 --> 00:30:30,799
first what you notice that the thing

00:30:28,309 --> 00:30:32,960
line on the top is the memory memory

00:30:30,799 --> 00:30:36,289
footprint when you increase number of

00:30:32,960 --> 00:30:38,090
threads when MP hybrid mkay openmp helps

00:30:36,289 --> 00:30:42,889
to reduce memory footprint on the node

00:30:38,090 --> 00:30:46,359
and these two plots are quite busy the

00:30:42,889 --> 00:30:49,639
left is one and the beach mtv BTW

00:30:46,359 --> 00:30:52,309
benchmark and the right-hand side is the

00:30:49,639 --> 00:30:54,919
lu benchmark and different colors

00:30:52,309 --> 00:30:56,779
represent different compilers so one

00:30:54,919 --> 00:31:01,730
suggestion is try different compilers

00:30:56,779 --> 00:31:03,379
for your application and the different

00:31:01,730 --> 00:31:06,769
number of threads and then you would try

00:31:03,379 --> 00:31:09,919
to find you what's your sweet spot so

00:31:06,769 --> 00:31:11,720
for the for the first one you can yeah

00:31:09,919 --> 00:31:14,480
so for the first one actually open NP

00:31:11,720 --> 00:31:18,730
does not help significantly and second

00:31:14,480 --> 00:31:21,739
one and the sweet spot is at six threads

00:31:18,730 --> 00:31:25,220
so this is another application on FB

00:31:21,739 --> 00:31:26,840
camp basically it has different

00:31:25,220 --> 00:31:28,850
components and different components or

00:31:26,840 --> 00:31:34,070
behave differently even in your same

00:31:28,850 --> 00:31:36,109
application and so one thing to notice

00:31:34,070 --> 00:31:39,109
is that from one thread to three threads

00:31:36,109 --> 00:31:41,090
you would see that performance suffers

00:31:39,109 --> 00:31:43,570
only very small but then memory

00:31:41,090 --> 00:31:45,590
footprint reduced to have so this is a

00:31:43,570 --> 00:31:48,129
trade-off you would like to take

00:31:45,590 --> 00:31:48,129
actually

00:31:51,190 --> 00:31:56,260
and this is another application the

00:31:54,010 --> 00:31:59,760
story here is basically to add OpenMP

00:31:56,260 --> 00:32:02,500
incrementally there are four hotspots

00:31:59,760 --> 00:32:04,390
sections in this code the collision is

00:32:02,500 --> 00:32:06,790
the biggest it takes about ninety five

00:32:04,390 --> 00:32:10,750
percent of time so you add openmp to it

00:32:06,790 --> 00:32:13,510
and you get down about 60 times speed up

00:32:10,750 --> 00:32:16,120
for collision then at that point stream

00:32:13,510 --> 00:32:18,310
is the next house part becomes huge and

00:32:16,120 --> 00:32:20,020
you would like to add openmp do that and

00:32:18,310 --> 00:32:22,930
then you reduce another eighty-nine

00:32:20,020 --> 00:32:25,600
percent of speed up and then from step

00:32:22,930 --> 00:32:28,110
to two step three is the act and adding

00:32:25,600 --> 00:32:28,110
vectorization

00:32:28,130 --> 00:32:33,680
yes Xeon Phi and the right side

00:32:31,550 --> 00:32:36,470
basically I would like to ask people to

00:32:33,680 --> 00:32:38,900
try different OpenMP affinity choices

00:32:36,470 --> 00:32:42,290
for the Intel that you have you have

00:32:38,900 --> 00:32:44,750
balanced and scatter and again compact

00:32:42,290 --> 00:32:48,070
normally we see that balances is the

00:32:44,750 --> 00:32:53,300
best but sometimes scatters best as well

00:32:48,070 --> 00:32:56,660
they try that so this is an example of

00:32:53,300 --> 00:32:58,460
overlap communication and computation as

00:32:56,660 --> 00:33:02,390
I illustrate on the right side it's very

00:32:58,460 --> 00:33:05,210
hard to do you have to know that you

00:33:02,390 --> 00:33:06,830
have to to know what part I can be can

00:33:05,210 --> 00:33:09,710
be separated out for communication while

00:33:06,830 --> 00:33:12,140
you're doing your computation it needs

00:33:09,710 --> 00:33:15,800
also needs a higher level of MPI threat

00:33:12,140 --> 00:33:20,590
support all the compilers we have our

00:33:15,800 --> 00:33:20,590
system support that to do the overlap

00:33:27,020 --> 00:33:34,790
no what you can't put into

00:33:49,310 --> 00:33:54,780
but if you wanted to overlap you have to

00:33:51,690 --> 00:33:59,760
put it inside otherwise which our users

00:33:54,780 --> 00:34:01,380
just do that do them yes even if it's

00:33:59,760 --> 00:34:04,820
inside with how people just do the

00:34:01,380 --> 00:34:04,820
master to do the communication

00:34:12,889 --> 00:34:25,579
yeah it's not it's safe to do so yes so

00:34:16,669 --> 00:34:27,619
so these yeah so this is the the

00:34:25,579 --> 00:34:32,960
performance results from you can hyper c

00:34:27,619 --> 00:34:36,500
and d of the overlap result so this is a

00:34:32,960 --> 00:34:39,200
application of the ocean model

00:34:36,500 --> 00:34:41,000
next-generation ocean model and they

00:34:39,200 --> 00:34:43,250
have basically explore different things

00:34:41,000 --> 00:34:45,349
like they'd explode openmp tasking

00:34:43,250 --> 00:34:47,480
because it's unstructured mesh and they

00:34:45,349 --> 00:34:52,029
can it's like sort of naturally in a

00:34:47,480 --> 00:35:00,410
tasking way that they try this and and

00:34:52,029 --> 00:35:03,049
with with their block concept and then

00:35:00,410 --> 00:35:07,869
they also try with element in there and

00:35:03,049 --> 00:35:10,549
algorithm they can use the loop level of

00:35:07,869 --> 00:35:13,640
terrorism just add openmp directive on

00:35:10,549 --> 00:35:15,890
the bottom or they can do a pre compute

00:35:13,640 --> 00:35:18,490
whereas my loop boundary it's causing

00:35:15,890 --> 00:35:20,660
single program multiple data method and

00:35:18,490 --> 00:35:24,619
basically they compared all three of

00:35:20,660 --> 00:35:26,869
them and found out loop level is the

00:35:24,619 --> 00:35:30,039
performance past and it's also easiest

00:35:26,869 --> 00:35:32,960
to do so they would stick on that and

00:35:30,039 --> 00:35:36,400
the next step would like to you know

00:35:32,960 --> 00:35:36,400
explore nested OpenMP

00:35:36,620 --> 00:35:41,900
so that's the comparison of these and

00:35:39,950 --> 00:35:44,240
then they also compare different

00:35:41,900 --> 00:35:47,090
scheduling on the left side static

00:35:44,240 --> 00:35:49,430
dynamic guided and found out basically

00:35:47,090 --> 00:35:52,250
statics that's best let it add onto

00:35:49,430 --> 00:35:55,610
simdi simdi helps a little bit so decide

00:35:52,250 --> 00:35:58,100
to stick on with a static simdi of OMG

00:35:55,610 --> 00:36:00,230
on the right hand side while after you

00:35:58,100 --> 00:36:02,120
stick on to statics in the openmp they

00:36:00,230 --> 00:36:05,090
try two different combinations are mpi

00:36:02,120 --> 00:36:06,980
test no go tempo and openmp threads so

00:36:05,090 --> 00:36:11,420
they found out there sweet spot it

00:36:06,980 --> 00:36:15,200
becomes 2 times 12 so this is what they

00:36:11,420 --> 00:36:17,990
get and this plot is so the previous

00:36:15,200 --> 00:36:20,600
plot you can see it's up to 3,000 cores

00:36:17,990 --> 00:36:22,670
and now with this fixed sweet spot

00:36:20,600 --> 00:36:26,120
everything they found they start to

00:36:22,670 --> 00:36:29,570
scale up to much higher and you can see

00:36:26,120 --> 00:36:33,800
that flat MP i can only round up to 12 k

00:36:29,570 --> 00:36:35,840
and then in this case they actually

00:36:33,800 --> 00:36:38,110
tried different combinations of MPI

00:36:35,840 --> 00:36:42,800
tasks on and open MP and it found out

00:36:38,110 --> 00:36:44,870
another sweet spot 8 times 3 is the best

00:36:42,800 --> 00:36:47,300
so always try it these things aren't

00:36:44,870 --> 00:36:52,030
easy to try and always get you actually

00:36:47,300 --> 00:36:52,030
a quick turnaround results right

00:36:54,640 --> 00:37:06,580
what color yeah 12 case the best I don't

00:37:05,030 --> 00:37:12,970
exactly know

00:37:06,580 --> 00:37:17,230
could be a problem common size greatest

00:37:12,970 --> 00:37:22,570
the lessons learned is try these in it's

00:37:17,230 --> 00:37:25,900
easier than modify your code and so this

00:37:22,570 --> 00:37:29,770
is another example application back slip

00:37:25,900 --> 00:37:32,050
so traditional I in the middle bottom

00:37:29,770 --> 00:37:34,420
you can see it's the AMR so you have

00:37:32,050 --> 00:37:36,760
different levels of boxes all these

00:37:34,420 --> 00:37:41,830
boxes you you have more and more levels

00:37:36,760 --> 00:37:44,080
to try to find fight more find finder

00:37:41,830 --> 00:37:46,540
your resolution but the traditional ways

00:37:44,080 --> 00:37:48,400
out of these boxes they assign them to

00:37:46,540 --> 00:37:49,810
the different MPI tasks and an

00:37:48,400 --> 00:37:52,630
underneath MPI tasks you would use

00:37:49,810 --> 00:37:55,750
openmp the problem is it is it cause

00:37:52,630 --> 00:37:57,880
huge load imbalance this original way

00:37:55,750 --> 00:37:59,620
and in your code example you will see

00:37:57,880 --> 00:38:01,240
lots of different small sections of

00:37:59,620 --> 00:38:03,280
parallel doing and peril do another

00:38:01,240 --> 00:38:05,440
parent to another and penalty like that

00:38:03,280 --> 00:38:08,710
so this is basically the fine grain

00:38:05,440 --> 00:38:11,740
OpenMP so they did from revolution work

00:38:08,710 --> 00:38:15,130
with trying to the so called spread

00:38:11,740 --> 00:38:17,820
tiring and sweating model what they what

00:38:15,130 --> 00:38:21,040
they do is for each box they make it

00:38:17,820 --> 00:38:23,890
distribute box as little styles and they

00:38:21,040 --> 00:38:26,740
put tile as the outermost loop and

00:38:23,890 --> 00:38:30,460
within each then for each tile you would

00:38:26,740 --> 00:38:34,570
do then do you have your MP I you know

00:38:30,460 --> 00:38:37,650
and open and pee in it I'm sorry mpi is

00:38:34,570 --> 00:38:37,650
always do outside sorry

00:38:42,130 --> 00:38:44,190
Oh

00:38:50,910 --> 00:38:59,900
I think I i think my private should be

00:38:55,200 --> 00:38:59,900
as J right at least this type of here

00:39:03,440 --> 00:39:08,420
uh-huh yeah

00:39:10,670 --> 00:39:13,880
okay yeah

00:39:16,930 --> 00:39:22,750
I haven't tried my ex that the user is

00:39:20,319 --> 00:39:24,190
supplied basically the story there is

00:39:22,750 --> 00:39:26,430
they have lots of sections of

00:39:24,190 --> 00:39:29,980
fine-grained OpenMP and then thread

00:39:26,430 --> 00:39:32,500
creation destroy like that so we still

00:39:29,980 --> 00:39:34,960
owe the tiles you put tiles outside and

00:39:32,500 --> 00:39:39,880
it has much better loading but load

00:39:34,960 --> 00:39:42,430
balance and also and because taio each

00:39:39,880 --> 00:39:44,079
tile you can tune them with the small

00:39:42,430 --> 00:39:49,059
size and the tire would you know you

00:39:44,079 --> 00:39:52,540
have a good memory cache we used so so

00:39:49,059 --> 00:39:55,960
this is there a skating with all this is

00:39:52,540 --> 00:39:58,089
also on the Babbage system and it

00:39:55,960 --> 00:40:03,930
basically scales up to one and internet

00:39:58,089 --> 00:40:03,930
shreds okay

00:40:04,120 --> 00:40:11,290
so this is a strong skating box out box

00:40:07,500 --> 00:40:13,860
lip data and it was compared to MPI I'm

00:40:11,290 --> 00:40:16,990
sorry it's a it's a week scaling sorry

00:40:13,860 --> 00:40:20,980
then you with mpi you add different

00:40:16,990 --> 00:40:23,260
number of threads so m NPI plus 12

00:40:20,980 --> 00:40:26,800
openmp sweat is the best on the it's

00:40:23,260 --> 00:40:30,040
this is on Edison system to our threads

00:40:26,800 --> 00:40:33,960
is to how many you have in the new modem

00:40:30,040 --> 00:40:33,960
in the best

00:40:33,970 --> 00:40:41,109
okay down another application so this is

00:40:39,190 --> 00:40:46,060
just an interesting story with the heap

00:40:41,109 --> 00:40:48,670
erase compiler flag so we have a kernel

00:40:46,060 --> 00:40:51,160
code we have in Craig compiler optimized

00:40:48,670 --> 00:40:52,990
and Intel compile apprised at some point

00:40:51,160 --> 00:40:55,359
they are comparable and then we did some

00:40:52,990 --> 00:40:58,680
vectorization work suddenly Intel

00:40:55,359 --> 00:41:01,960
compiled version is six times slower and

00:40:58,680 --> 00:41:04,510
had some investigation and turned out it

00:41:01,960 --> 00:41:07,930
was this flag if like what it does is

00:41:04,510 --> 00:41:11,230
put the young automatic erase and things

00:41:07,930 --> 00:41:13,420
onto heap instead of stack so the story

00:41:11,230 --> 00:41:16,780
is because here the private copies on

00:41:13,420 --> 00:41:20,109
the stack is so expensive so it's slower

00:41:16,780 --> 00:41:22,150
so there are two two ways to to get

00:41:20,109 --> 00:41:25,359
performance back one is just remove this

00:41:22,150 --> 00:41:27,040
black and then in this case once you

00:41:25,359 --> 00:41:28,720
remove you put everything back on stacks

00:41:27,040 --> 00:41:31,720
we want to increase our own key stack

00:41:28,720 --> 00:41:34,000
size and another alternative is you

00:41:31,720 --> 00:41:36,780
thread private so you have sweat private

00:41:34,000 --> 00:41:36,780
copies on heap

00:41:38,049 --> 00:41:41,500
this I

00:41:41,640 --> 00:41:45,550
it's not because the allocation is so

00:41:44,560 --> 00:41:48,640
expensive

00:41:45,550 --> 00:41:57,660
because it's a lot and done so all the

00:41:48,640 --> 00:41:57,660
friends ok

00:42:01,740 --> 00:42:06,250
so basically on your on your hip you

00:42:04,450 --> 00:42:09,809
have you have to maintain throughout

00:42:06,250 --> 00:42:09,809
private copies right

00:42:34,950 --> 00:42:38,130
it's more

00:42:41,650 --> 00:42:53,870
okay yeah but it's but it's been so the

00:42:52,010 --> 00:42:55,580
story it's a fortran code so at least

00:42:53,870 --> 00:43:00,490
you wouldn't want how about you want to

00:42:55,580 --> 00:43:00,490
use yeah runtime application

00:43:12,170 --> 00:43:18,910
right you always have to maintain

00:43:15,260 --> 00:43:18,910
private copies either which way

00:43:25,020 --> 00:43:29,490
I'll step

00:43:32,680 --> 00:43:37,310
so there was an early problem found that

00:43:35,480 --> 00:43:40,550
people have allocation memory inside

00:43:37,310 --> 00:43:52,400
their peril regions that slows down that

00:43:40,550 --> 00:43:57,230
was a bug fixed rate hey so this team

00:43:52,400 --> 00:44:07,119
also explored nested openmp cray doesn't

00:43:57,230 --> 00:44:07,119
you use this flag great no

00:44:12,200 --> 00:44:18,380
yeah I'll carry always a war on this

00:44:15,800 --> 00:44:20,630
code they tried a nested openmp so far

00:44:18,380 --> 00:44:23,839
it's a little bit slower next OpenMP

00:44:20,630 --> 00:44:26,510
compared with one level and I'll ask

00:44:23,839 --> 00:44:32,589
them to try KMP heart teams and

00:44:26,510 --> 00:44:34,550
hopefully we can get this pasture so the

00:44:32,589 --> 00:44:37,280
one of the things they learn is

00:44:34,550 --> 00:44:40,670
basically because we will tell them to

00:44:37,280 --> 00:44:42,530
use OMP num threads and they only use a

00:44:40,670 --> 00:44:45,349
small section of their codes with nested

00:44:42,530 --> 00:44:47,000
openmp that all the other many many

00:44:45,349 --> 00:44:50,300
other regions they use the default

00:44:47,000 --> 00:44:52,339
number director the one level OpenMP so

00:44:50,300 --> 00:44:54,290
I tell them to do that basically you

00:44:52,339 --> 00:44:56,869
would like don't use or genome res

00:44:54,290 --> 00:44:59,869
equals 6 comma 4 put that into a num

00:44:56,869 --> 00:45:01,869
cross num threads class in your the

00:44:59,869 --> 00:45:04,430
nested peril region and then leave the

00:45:01,869 --> 00:45:05,900
OMP you can use OMP number stress

00:45:04,430 --> 00:45:08,420
environment variable for the other

00:45:05,900 --> 00:45:11,020
regions so that's more flexible in and

00:45:08,420 --> 00:45:11,020
simple

00:45:14,200 --> 00:45:20,740
that's along this the next few slides a

00:45:17,170 --> 00:45:23,109
long story of the NW cam OpenMP

00:45:20,740 --> 00:45:26,050
optimization they did a lot of ways of

00:45:23,109 --> 00:45:28,990
optimization the first slide shows in

00:45:26,050 --> 00:45:33,630
this algorithm that the baseline OpenMP

00:45:28,990 --> 00:45:36,730
performance so there are basically two

00:45:33,630 --> 00:45:39,570
two hot regions loop nest and get block

00:45:36,730 --> 00:45:43,960
the gate block doesn't scale at all and

00:45:39,570 --> 00:45:46,930
to loopnet's does so this is what you

00:45:43,960 --> 00:45:49,900
have at the baseline so there's the next

00:45:46,930 --> 00:45:56,619
slide they start to optimize the get

00:45:49,900 --> 00:45:59,200
block let me show you so get block it's

00:45:56,619 --> 00:46:02,410
still not scaling well but much much

00:45:59,200 --> 00:46:05,200
faster compared to the first one and

00:46:02,410 --> 00:46:07,420
they did lots of things like paralyzing

00:46:05,200 --> 00:46:11,920
ginger soup it paralyzed the sort

00:46:07,420 --> 00:46:16,119
algorithm loop unrolling and a memory

00:46:11,920 --> 00:46:17,859
alignment and loop reordering they have

00:46:16,119 --> 00:46:22,599
lots of things is give compiler some

00:46:17,859 --> 00:46:24,430
hints and this a speed up this is like

00:46:22,599 --> 00:46:27,490
all these algorithm things you have you

00:46:24,430 --> 00:46:30,430
can do and in the also the loop nests I

00:46:27,490 --> 00:46:34,030
think compared to the first one there's

00:46:30,430 --> 00:46:36,540
also optimization on the the performance

00:46:34,030 --> 00:46:41,349
improvement there so this is one work

00:46:36,540 --> 00:46:43,450
and then NW NW cam another and Colonel

00:46:41,349 --> 00:46:46,359
what they do they have three ways of

00:46:43,450 --> 00:46:50,680
opening they did three different OpenMP

00:46:46,359 --> 00:46:53,500
optimization the first operation is with

00:46:50,680 --> 00:46:55,780
the loop level so this algorithm is Hugh

00:46:53,500 --> 00:46:59,569
so memory and intensive that you can

00:46:55,780 --> 00:47:01,999
only run up to 60 mmpi not

00:46:59,569 --> 00:47:04,969
tasks and then so on top of that they

00:47:01,999 --> 00:47:09,259
start to use at on 101 p or two or three

00:47:04,969 --> 00:47:12,979
or four for this and for this openmp

00:47:09,259 --> 00:47:15,229
optimization the loop level and I didn't

00:47:12,979 --> 00:47:17,059
choose the second way but the third way

00:47:15,229 --> 00:47:24,109
of optimization is they explore our

00:47:17,059 --> 00:47:26,029
tasking so the if the try to find out

00:47:24,109 --> 00:47:28,489
how task could work for this for this

00:47:26,029 --> 00:47:33,019
algorithm and they add used basically

00:47:28,489 --> 00:47:35,390
the the master task to assign work and

00:47:33,019 --> 00:47:38,109
there's some local conflicts they add

00:47:35,390 --> 00:47:41,809
they did at the end to do a local

00:47:38,109 --> 00:47:44,269
reduction optimization two and three was

00:47:41,809 --> 00:47:46,759
tasking you basically you can do a pure

00:47:44,269 --> 00:47:49,519
openmp now with the first one the loop

00:47:46,759 --> 00:47:53,680
level is the 60 mpi tasks x plus

00:47:49,519 --> 00:47:57,739
different number of openmp sweats and

00:47:53,680 --> 00:47:59,989
tasking is best in this case the blue

00:47:57,739 --> 00:48:03,769
line and it scales all the way to hunt

00:47:59,989 --> 00:48:06,680
240 threads so and the story is also

00:48:03,769 --> 00:48:09,170
that everything you do and on the cns

00:48:06,680 --> 00:48:11,989
Xeon Phi and you bring back to the xeon

00:48:09,170 --> 00:48:15,709
it also speed up you can see it on the

00:48:11,989 --> 00:48:19,069
right hand side so that an overall speed

00:48:15,709 --> 00:48:22,940
up I think if they got one 1.3 times

00:48:19,069 --> 00:48:24,560
faster than pure MGI but for the sound

00:48:22,940 --> 00:48:28,260
code

00:48:24,560 --> 00:48:30,090
and so I showed you about sweets the

00:48:28,260 --> 00:48:32,640
four lines of finding out sweet spots

00:48:30,090 --> 00:48:34,670
this is another way of showing sweet

00:48:32,640 --> 00:48:43,290
spots at the heat sort of heat map and

00:48:34,670 --> 00:48:46,500
you can see different colors and where

00:48:43,290 --> 00:48:50,250
each horizontal line is now different a

00:48:46,500 --> 00:48:56,330
constant constant number of MPI times

00:48:50,250 --> 00:49:00,930
OpenMP I think I can't go to this

00:48:56,330 --> 00:49:03,800
another NW can work by another team what

00:49:00,930 --> 00:49:08,580
they have is the matrix and

00:49:03,800 --> 00:49:11,490
multiplication mpit a times B equals C

00:49:08,580 --> 00:49:15,060
and one horizontal x 1 vertical equals C

00:49:11,490 --> 00:49:19,260
so they what they invented is basically

00:49:15,060 --> 00:49:21,480
they distribute the and the matrix on at

00:49:19,260 --> 00:49:24,750
the threat level and they call them

00:49:21,480 --> 00:49:31,410
blocks so you can assign teams of box

00:49:24,750 --> 00:49:33,930
say t1 t2 t3 and then you get the sub

00:49:31,410 --> 00:49:36,420
box of the results and then you put them

00:49:33,930 --> 00:49:37,950
together but then for each thread you

00:49:36,420 --> 00:49:40,560
can assign team the threats and

00:49:37,950 --> 00:49:43,620
underneath that she would have basically

00:49:40,560 --> 00:49:48,000
it becomes nested openmp so you have a

00:49:43,620 --> 00:49:50,400
six six teams 610 MPI with six teams of

00:49:48,000 --> 00:49:54,150
four threads and these four threads are

00:49:50,400 --> 00:49:58,590
down by an m KL so you can do a nested

00:49:54,150 --> 00:50:01,740
MKL underneath it so by default mkr

00:49:58,590 --> 00:50:04,980
dynamics is is true then basically you

00:50:01,740 --> 00:50:08,220
would set within OpenMP region the

00:50:04,980 --> 00:50:10,290
number of thread is 14 m KL so in order

00:50:08,220 --> 00:50:13,140
to enable m care with multiple threads

00:50:10,290 --> 00:50:16,450
you want to set it to TD to foss at the

00:50:13,140 --> 00:50:20,870
end and then you get an

00:50:16,450 --> 00:50:23,930
very easily and usable nested nested

00:50:20,870 --> 00:50:27,470
open em gee whiz MKL down I think I

00:50:23,930 --> 00:50:29,990
should results in this case the the

00:50:27,470 --> 00:50:32,930
right hand side is basically a heat map

00:50:29,990 --> 00:50:35,780
of each little block is a different

00:50:32,930 --> 00:50:37,550
configuration of their test cases so for

00:50:35,780 --> 00:50:40,370
each test cases they compare two

00:50:37,550 --> 00:50:43,130
different algorithms and was the newer

00:50:40,370 --> 00:50:46,760
reduced algorithm you the more red

00:50:43,130 --> 00:50:52,250
yellowish color is better so that's the

00:50:46,760 --> 00:50:54,560
improvement they see i'll skip the slide

00:50:52,250 --> 00:50:57,590
basically shows OMG tag device works

00:50:54,560 --> 00:51:00,110
which is good to know and it's portable

00:50:57,590 --> 00:51:04,250
but it's not you're going to use them on

00:51:00,110 --> 00:51:06,230
Babbage alright i mentioned i want to

00:51:04,250 --> 00:51:10,340
tell you about declare simdi how this is

00:51:06,230 --> 00:51:12,650
useful so this is a prototype of the

00:51:10,340 --> 00:51:14,750
climate code colonel it's very

00:51:12,650 --> 00:51:17,060
complicated it called function and let's

00:51:14,750 --> 00:51:19,640
call it function a the function days is

00:51:17,060 --> 00:51:22,490
elemental function and then insight you

00:51:19,640 --> 00:51:25,450
call different B and C and all the way

00:51:22,490 --> 00:51:27,740
down different so the vector rights

00:51:25,450 --> 00:51:30,350
report shows I can't in line I can't

00:51:27,740 --> 00:51:32,540
vectorize things so what we did is you

00:51:30,350 --> 00:51:35,570
know you add attribute force in line and

00:51:32,540 --> 00:51:39,890
add the clear simdi for this function

00:51:35,570 --> 00:51:42,500
and now it's black slice so another

00:51:39,890 --> 00:51:44,870
thing we did is instead of back to align

00:51:42,500 --> 00:51:49,640
directives we added in openmp simdi

00:51:44,870 --> 00:51:53,510
aligned so simply align as we said it

00:51:49,640 --> 00:51:55,520
could be dangerous and however we try to

00:51:53,510 --> 00:51:58,730
push more towards port abilities you

00:51:55,520 --> 00:52:00,800
want to use an simdi aligned so for the

00:51:58,730 --> 00:52:03,080
simile aligned what we found out is that

00:52:00,800 --> 00:52:06,650
if you just put simdi without aligned

00:52:03,080 --> 00:52:10,760
class without putting into the specific

00:52:06,650 --> 00:52:12,530
arrays that aligned and you don't see

00:52:10,760 --> 00:52:15,050
that eight percent performance can so

00:52:12,530 --> 00:52:16,970
this is it for this kernel basically

00:52:15,050 --> 00:52:18,710
every single digit of performance gain

00:52:16,970 --> 00:52:20,369
is useful there's a very very little

00:52:18,710 --> 00:52:22,289
things at together get the

00:52:20,369 --> 00:52:24,890
the performer skin so eight percent to

00:52:22,289 --> 00:52:24,890
them is big

00:52:33,609 --> 00:52:37,420
that's what that's what we were trying

00:52:35,289 --> 00:52:39,430
to get ya so the compiler we would hope

00:52:37,420 --> 00:52:41,799
the compiler should know so for this

00:52:39,430 --> 00:52:43,900
this is just a colonel and they you're

00:52:41,799 --> 00:52:46,720
like fully eight loops and each of them

00:52:43,900 --> 00:52:51,029
has 10 plus it's very hard to do so they

00:52:46,720 --> 00:52:51,029
like to do it but so what

00:53:00,970 --> 00:53:04,390
is already

00:53:15,670 --> 00:53:20,800
so ask you know what just what I think

00:53:26,350 --> 00:53:32,300
that's

00:53:29,240 --> 00:53:34,460
I I would hope so yeah so basically I

00:53:32,300 --> 00:53:35,990
think as long as your English scope

00:53:34,460 --> 00:53:37,970
you're not into a separate subroutine

00:53:35,990 --> 00:53:42,190
that if the compiler does not know your

00:53:37,970 --> 00:53:42,190
scope anymore if it's no you should not

00:53:44,380 --> 00:53:50,869
yeah oh yeah either you would align it

00:53:49,070 --> 00:53:52,670
or use the like you said it was flag

00:53:50,869 --> 00:53:54,560
everything is already aligned why do you

00:53:52,670 --> 00:53:56,869
need that right but in this case we

00:53:54,560 --> 00:53:59,119
actually in the table you can see we put

00:53:56,869 --> 00:54:03,050
up we do have to align 60 people by in

00:53:59,119 --> 00:54:14,960
it but the compilers do with the aligned

00:54:03,050 --> 00:54:18,230
still helps so yes that's okay and then

00:54:14,960 --> 00:54:21,140
so we were also talking to the fortunate

00:54:18,230 --> 00:54:25,540
standard because for see you have a you

00:54:21,140 --> 00:54:28,440
can you can declare it's aligned so if

00:54:25,540 --> 00:54:31,530
it's

00:54:28,440 --> 00:54:35,369
sorry but you have you have a way to do

00:54:31,530 --> 00:54:37,170
that but for fortune I wish either you

00:54:35,369 --> 00:54:38,970
could align it in your declaration or

00:54:37,170 --> 00:54:47,150
you have some kind of equivalent of

00:54:38,970 --> 00:54:52,050
compiler flags that's yeah so here's a

00:54:47,150 --> 00:54:54,210
somebody's suing us test simdi sweet so

00:54:52,050 --> 00:54:56,670
Python script to test where all these or

00:54:54,210 --> 00:55:00,030
OMP declare some the different things

00:54:56,670 --> 00:55:03,060
and how it could get as close to a VX of

00:55:00,030 --> 00:55:05,760
always so both things we found out a

00:55:03,060 --> 00:55:08,520
line is really important so otherwise

00:55:05,760 --> 00:55:11,430
you get way off performance but still

00:55:08,520 --> 00:55:14,720
nothing can be compared to the native X

00:55:11,430 --> 00:55:14,720
AVX flag

00:55:54,450 --> 00:56:00,160
but but I think 44 compilers we would

00:55:58,240 --> 00:56:02,470
need without open a V at all you would

00:56:00,160 --> 00:56:04,930
already tell people to use X abs in your

00:56:02,470 --> 00:56:06,760
compiler blog anyway right so we will

00:56:04,930 --> 00:56:09,300
just try to use it without it and

00:56:06,760 --> 00:56:09,300
compare

00:56:33,100 --> 00:56:36,180
thank you

00:56:41,540 --> 00:56:48,460
then why would we need an alignment

00:56:44,230 --> 00:56:48,460
lined class

00:57:17,280 --> 00:57:20,580
yeah does

00:57:27,700 --> 00:57:34,190
this is what we do yeah so that's dog

00:57:32,089 --> 00:57:36,410
our goal users don't want to specify

00:57:34,190 --> 00:57:39,290
this I think for private and share you

00:57:36,410 --> 00:57:44,810
have to but for alignment we try not not

00:57:39,290 --> 00:57:48,050
to have the spirited okay there's also

00:57:44,810 --> 00:57:50,420
things I asked in the committee so this

00:57:48,050 --> 00:57:53,089
aligned there's a restriction it the

00:57:50,420 --> 00:57:55,790
type of lists appear in a line class has

00:57:53,089 --> 00:57:59,300
to be c pointer or gray painter or it's

00:57:55,790 --> 00:58:01,910
a allocatable attribute so c and c++

00:57:59,300 --> 00:58:05,270
allows arrays why would fortune have

00:58:01,910 --> 00:58:07,220
such a restriction can we relax it so we

00:58:05,270 --> 00:58:09,290
have two compilers we tried one of them

00:58:07,220 --> 00:58:14,109
conform to it it won't compile the other

00:58:09,290 --> 00:58:14,109
one compiles and does see speed up

00:58:18,410 --> 00:58:24,369
intel yeah speed up

00:58:27,589 --> 00:58:34,630
yeah I ask that it's it existing in open

00:58:32,329 --> 00:58:34,630
mp3

00:58:43,030 --> 00:58:45,660
back

00:58:47,150 --> 00:58:53,430
yeah so this is just a one last lied

00:58:50,670 --> 00:58:55,080
about what users like to have so we

00:58:53,430 --> 00:58:59,610
would like to have shredded libraries

00:58:55,080 --> 00:59:02,790
mko is part of an Intel runtime in MPI

00:58:59,610 --> 00:59:05,790
inter mpi Intel openmp interim care

00:59:02,790 --> 00:59:10,320
everything is so good together but then

00:59:05,790 --> 00:59:13,860
if you come mix different openmpi it was

00:59:10,320 --> 00:59:16,920
different open mp4 different then how do

00:59:13,860 --> 00:59:20,280
I know how many what's red I want to use

00:59:16,920 --> 00:59:22,260
for my libraries so within a peril

00:59:20,280 --> 00:59:26,040
region you call Patsy one cause single

00:59:22,260 --> 00:59:28,130
threaded but then users also on sort of

00:59:26,040 --> 00:59:33,560
way of I want and multiple

00:59:28,130 --> 00:59:33,560
multi-strategy and patsy for example

00:59:34,340 --> 00:59:43,430
that's ok

00:59:48,930 --> 00:59:59,949
both especially yes I even MP is red

00:59:56,859 --> 01:00:02,829
safe but then for mk also for like a

00:59:59,949 --> 01:00:16,299
pass see for example like em KO has both

01:00:02,829 --> 01:00:17,920
right and yeah then also the first four

01:00:16,299 --> 01:00:20,349
for multiple times right whether you

01:00:17,920 --> 01:00:24,339
want a single thread and a motor sweat

01:00:20,349 --> 01:00:26,019
different API version or you want a if I

01:00:24,339 --> 01:00:28,719
am Alive a developer should I always

01:00:26,019 --> 01:00:31,749
inquire openmp runtime to know what I

01:00:28,719 --> 01:00:33,459
whether I may impair vision or not if it

01:00:31,749 --> 01:00:35,079
fits book it's a good idea to do that

01:00:33,459 --> 01:00:38,890
but on the other hand you would burden

01:00:35,079 --> 01:00:40,930
you have to always have openmp function

01:00:38,890 --> 01:00:42,489
calls library interface into your code

01:00:40,930 --> 01:00:45,819
and you have to always compile with

01:00:42,489 --> 01:00:50,069
flags to be able to run time aware of

01:00:45,819 --> 01:00:50,069
openmp how how to do that

01:00:55,850 --> 01:01:02,690
so another thing you normally like is

01:00:59,520 --> 01:01:06,060
one consistent behaviour amount of

01:01:02,690 --> 01:01:08,610
compilers or different vendors just

01:01:06,060 --> 01:01:10,770
these are two examples like Max active

01:01:08,610 --> 01:01:12,930
levels this is for nested openmp it's

01:01:10,770 --> 01:01:15,300
different right now some some comparison

01:01:12,930 --> 01:01:18,300
is set to 1 so it you didn't get a

01:01:15,300 --> 01:01:20,070
nested openmp another example is default

01:01:18,300 --> 01:01:21,750
number of threads I we had lots of

01:01:20,070 --> 01:01:26,370
discussion on that I think primates it's

01:01:21,750 --> 01:01:28,470
dead already but so we right now that

01:01:26,370 --> 01:01:32,700
the decision is do i do ask users to

01:01:28,470 --> 01:01:35,250
always set that in explicitly but if we

01:01:32,700 --> 01:01:37,380
can have some kind of consistent

01:01:35,250 --> 01:01:40,650
behaviors like if we can define what is

01:01:37,380 --> 01:01:45,000
the max number of threads being whether

01:01:40,650 --> 01:01:47,760
it's time of course x x number of course

01:01:45,000 --> 01:01:51,030
x Hardware threads or and also it has to

01:01:47,760 --> 01:01:54,030
know what in hell has to be MC eyewear

01:01:51,030 --> 01:01:56,790
so it's very hard easily cause

01:01:54,030 --> 01:01:58,620
oversubscription but things just along

01:01:56,790 --> 01:02:03,840
this line consistent behavior is what

01:01:58,620 --> 01:02:06,450
users like so my summary is open MPs is

01:02:03,840 --> 01:02:09,390
found and powerful and we recommend

01:02:06,450 --> 01:02:13,980
users to use hybrid MPI openmp we always

01:02:09,390 --> 01:02:15,900
keep portability in mind and we'll keep

01:02:13,980 --> 01:02:21,890
promoting and working with users on

01:02:15,900 --> 01:02:21,890
using OpenMP more thank you

01:02:39,250 --> 01:02:42,810
simdi which one

01:02:44,020 --> 01:02:47,020
aligned

01:02:50,380 --> 01:02:56,010
oh I see there was a section on opening

01:02:53,110 --> 01:02:56,010
day

01:03:00,440 --> 01:03:03,250
this one

01:03:04,660 --> 01:03:07,660
sure

01:03:12,369 --> 01:03:14,430
uh

01:03:26,000 --> 01:03:31,270
yeah so I have another slide there about

01:03:34,930 --> 01:03:41,980
here so here that I compare sim the

01:03:38,360 --> 01:03:45,230
align and vector lined this differently

01:03:41,980 --> 01:03:48,350
yeah i know i would just compare for

01:03:45,230 --> 01:03:53,230
that second one is it's not force

01:03:48,350 --> 01:03:53,230
compiler to vectorize but the sim d is

01:04:01,560 --> 01:04:07,050
later that said that you recommend at

01:04:05,020 --> 01:04:07,050
least

01:04:09,920 --> 01:04:17,819
at least one in here i right oh yeah

01:04:14,550 --> 01:04:20,809
yeah that's the that's the first touch

01:04:17,819 --> 01:04:20,809
when I imagine you that

01:04:28,450 --> 01:04:36,760
foodie and sweat is actually no no mpi

01:04:31,960 --> 01:04:36,760
right yeah

01:04:44,859 --> 01:04:50,680
Crais let's create an application

01:04:47,869 --> 01:04:50,680
launcher

01:05:45,110 --> 01:05:52,480
okay ask to ask you more about it thank

01:05:50,240 --> 01:05:52,480
you

01:05:57,470 --> 01:06:01,070
create or sub

01:06:04,970 --> 01:06:15,560
oh yeah the rank reorder file yeah we

01:06:13,160 --> 01:06:19,780
use that a lot even the proof tools tell

01:06:15,560 --> 01:06:19,780

YouTube URL: https://www.youtube.com/watch?v=2COPCWR0Dfo


