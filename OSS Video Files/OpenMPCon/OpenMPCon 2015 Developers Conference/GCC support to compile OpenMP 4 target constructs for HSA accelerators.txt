Title: GCC support to compile OpenMP 4 target constructs for HSA accelerators
Publication date: 2015-12-13
Playlist: OpenMPCon 2015 Developers Conference
Description: 
	Martin Jambor, SUSE Linux.
OpenMPCon 2015 - Aachen Germany - September 2015
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2015-martin-jambor-gcc.pdf

Abstract: The talk summarizes our experience from ongoing development of a GCC branch which takes OpenMP code and compiles it so that it runs on HSA GPGPUs. We outline the architecture of the process with emphasis on the differences between compilation for the host and HSA accelerators. Furthermore, we will discuss what kinds of input we can compile in a straightforward manner and, on the contrary, what are the problematic cases and how we have tackled them. We intend to merge the branch to GCC 6, and so the talk will also serve as a preview of what will be available in the GCC released in spring 2016
Captions: 
	00:00:05,370 --> 00:00:15,910
good afternoon I came here to talk about

00:00:11,640 --> 00:00:17,830
project I've been working on for this

00:00:15,910 --> 00:00:21,460
year basically and even slightly before

00:00:17,830 --> 00:00:26,020
that and that is allowing GCC to compile

00:00:21,460 --> 00:00:29,949
open mp4 constructs for heterogeneous

00:00:26,020 --> 00:00:32,829
systems architecture my name is Martin

00:00:29,949 --> 00:00:37,479
young boy I am to train engineer to

00:00:32,829 --> 00:00:39,940
celebs in proc and well just like

00:00:37,479 --> 00:00:43,269
everybody here we've been thrown into

00:00:39,940 --> 00:00:46,120
the heterogeneous world before that I

00:00:43,269 --> 00:00:48,839
was a classical compiler optimization

00:00:46,120 --> 00:00:52,389
engineer worrying about shrink wrapping

00:00:48,839 --> 00:00:56,350
constant propagation in lining things

00:00:52,389 --> 00:01:01,179
like that but I've been asked to provide

00:00:56,350 --> 00:01:06,180
support for compiling 4h sale I will

00:01:01,179 --> 00:01:10,410
talk about that in a moment and I've

00:01:06,180 --> 00:01:13,570
been quite surprised even though I

00:01:10,410 --> 00:01:17,050
obviously knew one or two things about

00:01:13,570 --> 00:01:20,140
it just how big heterogeneous computing

00:01:17,050 --> 00:01:23,620
is about to be and probably do not have

00:01:20,140 --> 00:01:25,270
to explain this to you I reckon that

00:01:23,620 --> 00:01:27,790
most of the talks at least that the

00:01:25,270 --> 00:01:30,370
openmp con do you relate to

00:01:27,790 --> 00:01:32,800
heterogeneous computing and GPUs and and

00:01:30,370 --> 00:01:37,210
the new era of computing in one way or

00:01:32,800 --> 00:01:41,650
another when you what I would like to

00:01:37,210 --> 00:01:44,260
point about because I want to introduce

00:01:41,650 --> 00:01:48,220
HSA to you that's the first thing in my

00:01:44,260 --> 00:01:50,380
talk is that and when people talk about

00:01:48,220 --> 00:01:53,380
heterogeneous computing they usually

00:01:50,380 --> 00:01:57,190
mention two problems one problem is

00:01:53,380 --> 00:01:58,750
access to memory and the fact that in

00:01:57,190 --> 00:02:01,840
heterogeneous systems the different

00:01:58,750 --> 00:02:04,060
components have either different

00:02:01,840 --> 00:02:05,920
memories or they have very different

00:02:04,060 --> 00:02:09,050
views of the memories and the second

00:02:05,920 --> 00:02:16,300
problem that is often mentioned is that

00:02:09,050 --> 00:02:18,290
the heterogeneous devices have very

00:02:16,300 --> 00:02:20,360
distinct even the same kind of

00:02:18,290 --> 00:02:23,270
heterogeneous devices have often very

00:02:20,360 --> 00:02:25,480
distinct programming models there Isis

00:02:23,270 --> 00:02:29,660
change very often so if you now compile

00:02:25,480 --> 00:02:32,840
for special I saw that that I some may

00:02:29,660 --> 00:02:35,060
not may not be usable that could may not

00:02:32,840 --> 00:02:37,160
usable in the future because future GPUs

00:02:35,060 --> 00:02:41,810
for example may use different different

00:02:37,160 --> 00:02:46,870
instruction set and so forth and that is

00:02:41,810 --> 00:02:49,880
the reason why a few companies such as

00:02:46,870 --> 00:02:52,310
our first and foremost AMD but also arm

00:02:49,880 --> 00:02:55,880
texas instruments and many others have

00:02:52,310 --> 00:02:58,340
come together and decided to work on

00:02:55,880 --> 00:03:02,750
heterogeneous systems architecture that

00:02:58,340 --> 00:03:04,700
will solve these issues and so my first

00:03:02,750 --> 00:03:07,010
the first part of my talk is going to be

00:03:04,700 --> 00:03:09,890
a very very brief overview of some of

00:03:07,010 --> 00:03:12,290
the features of HSA if you want to know

00:03:09,890 --> 00:03:15,400
more about that if you don't already do

00:03:12,290 --> 00:03:18,380
then please don do go to HSA foundation

00:03:15,400 --> 00:03:21,320
com where you will get all the specs and

00:03:18,380 --> 00:03:26,300
a lot of additional material to have a

00:03:21,320 --> 00:03:28,489
look at but the two features that aim to

00:03:26,300 --> 00:03:31,580
solve the two big problems that

00:03:28,489 --> 00:03:35,989
heterogeneous computing is that first

00:03:31,580 --> 00:03:41,120
the CPU and GPU have not only access to

00:03:35,989 --> 00:03:44,989
the same memory the system drm but they

00:03:41,120 --> 00:03:46,970
even are able to use unified virtual

00:03:44,989 --> 00:03:50,840
address space so the part of the program

00:03:46,970 --> 00:03:52,850
on the CPU can pass a pointer to the

00:03:50,840 --> 00:03:57,320
part of the program running on a GPU and

00:03:52,850 --> 00:03:59,330
the GPU part of the application is able

00:03:57,320 --> 00:04:04,220
to grab the data and we're a condom and

00:03:59,330 --> 00:04:06,680
so this has many implications and first

00:04:04,220 --> 00:04:09,080
of those implications of course is that

00:04:06,680 --> 00:04:12,350
we do not have to do any DMA transfers

00:04:09,080 --> 00:04:16,489
we don't have to do any very expensive

00:04:12,350 --> 00:04:18,620
memory mappings when working in HSA

00:04:16,489 --> 00:04:23,889
but that's not the only implication that

00:04:18,620 --> 00:04:26,240
there is the second feature of

00:04:23,889 --> 00:04:28,550
heterogeneous systems architecture is

00:04:26,240 --> 00:04:30,979
that it is being programmed where at

00:04:28,550 --> 00:04:32,750
least the GPUs within that architecture

00:04:30,979 --> 00:04:36,020
are being programmed in something that

00:04:32,750 --> 00:04:37,610
is called HSA intermediate language the

00:04:36,020 --> 00:04:39,289
intermediate language is an intermediate

00:04:37,610 --> 00:04:41,509
language which means that it needs a

00:04:39,289 --> 00:04:43,940
finalizar to be translated to the final

00:04:41,509 --> 00:04:47,900
I so whenever you actually want to run

00:04:43,940 --> 00:04:50,300
the code which brings some overhead but

00:04:47,900 --> 00:04:53,180
it has the advantage that when the code

00:04:50,300 --> 00:04:56,060
that you compile now in 2h sale will be

00:04:53,180 --> 00:05:00,319
runnable on future GPUs in years to come

00:04:56,060 --> 00:05:03,080
and you will not have to redistribute a

00:05:00,319 --> 00:05:05,810
special version for your code to special

00:05:03,080 --> 00:05:08,590
party to for each particular GPU that

00:05:05,810 --> 00:05:11,720
you want to have run it on the

00:05:08,590 --> 00:05:16,789
intermediate language is very close to

00:05:11,720 --> 00:05:18,590
the two assembly language which is on

00:05:16,789 --> 00:05:22,430
purpose so that finalizar has an easy

00:05:18,590 --> 00:05:25,969
job it has however very very many

00:05:22,430 --> 00:05:31,849
registers and some other optimization

00:05:25,969 --> 00:05:35,240
features and one feature that is of

00:05:31,849 --> 00:05:37,310
course typical for GPU programming

00:05:35,240 --> 00:05:42,520
environment is that it is explicitly

00:05:37,310 --> 00:05:42,520
parable let me actually go back go back

00:05:43,210 --> 00:05:50,990
one slide this is an example that does

00:05:48,349 --> 00:05:55,639
vector copy as you can see it does not

00:05:50,990 --> 00:05:57,770
have any any loop in it and yeah well

00:05:55,639 --> 00:06:01,819
the reason is that is explicitly parable

00:05:57,770 --> 00:06:04,580
I suppose that many of you have direct

00:06:01,819 --> 00:06:09,050
experience with programming GPUs so I'll

00:06:04,580 --> 00:06:12,830
go very very quickly over the slide but

00:06:09,050 --> 00:06:16,370
in one way of thinking about explicit

00:06:12,830 --> 00:06:19,610
parallelism is that GPU runs very very

00:06:16,370 --> 00:06:22,260
many threats and each threat runs one

00:06:19,610 --> 00:06:29,310
iteration of the loop

00:06:22,260 --> 00:06:32,510
HSA has support for three dimensions for

00:06:29,310 --> 00:06:36,720
three of the iteration space in which

00:06:32,510 --> 00:06:39,660
each of the threats then run so on this

00:06:36,720 --> 00:06:42,720
picture which is one that I stole from

00:06:39,660 --> 00:06:47,430
the HSA foundation com website I told

00:06:42,720 --> 00:06:50,340
you before is we can see representation

00:06:47,430 --> 00:06:53,310
of a workspace grid where each of the

00:06:50,340 --> 00:06:56,460
tiny boxes is one threat that is

00:06:53,310 --> 00:07:01,500
executing its portion of the iterations

00:06:56,460 --> 00:07:03,660
3d iteration space and the iteration

00:07:01,500 --> 00:07:05,520
space itself is not only partitioned

00:07:03,660 --> 00:07:10,910
into individual threats but it is

00:07:05,520 --> 00:07:13,050
partitioned into work groups which are

00:07:10,910 --> 00:07:16,560
portions of the iteration space within

00:07:13,050 --> 00:07:20,970
which the brats can communicate they can

00:07:16,560 --> 00:07:24,630
synchronize and they can also have the H

00:07:20,970 --> 00:07:26,640
group has its shared memory which is

00:07:24,630 --> 00:07:28,710
available to do to the threats within

00:07:26,640 --> 00:07:32,370
the group but not outside of the group

00:07:28,710 --> 00:07:35,190
and then a part of the threats run in

00:07:32,370 --> 00:07:36,780
parallel they execute the same in

00:07:35,190 --> 00:07:38,580
lockstep which means that they execute

00:07:36,780 --> 00:07:43,550
the same instruction but on different

00:07:38,580 --> 00:07:46,620
different data as you can see the

00:07:43,550 --> 00:07:50,280
wording that is being used in HSA is

00:07:46,620 --> 00:07:52,860
very similar to OpenGL so we do one

00:07:50,280 --> 00:07:55,070
thread is actually called work item we

00:07:52,860 --> 00:08:03,140
don't have gangs we have work groups and

00:07:55,070 --> 00:08:07,200
and so forth so now let's leave HSA and

00:08:03,140 --> 00:08:10,020
talk about what we've been working on

00:08:07,200 --> 00:08:12,120
what you need just in case you like my

00:08:10,020 --> 00:08:16,350
talk and want it to experiment with what

00:08:12,120 --> 00:08:21,060
we've done our work we're doing work in

00:08:16,350 --> 00:08:25,710
public we do it on a branch of the new

00:08:21,060 --> 00:08:26,420
GCC compiler so if you're interested you

00:08:25,710 --> 00:08:29,540
can

00:08:26,420 --> 00:08:32,570
just use either the subversion or they

00:08:29,540 --> 00:08:34,940
get mirror and within a few months

00:08:32,570 --> 00:08:37,790
hopefully the whole GCC project will

00:08:34,940 --> 00:08:39,230
switch to get so then you will be there

00:08:37,790 --> 00:08:42,500
will be a new address but you'll be

00:08:39,230 --> 00:08:44,960
certain able to figure it out and grab

00:08:42,500 --> 00:08:48,950
an HSA branch and have a look at it in

00:08:44,960 --> 00:08:51,140
order to run your code and even compile

00:08:48,950 --> 00:08:54,710
the compiler you will need a few more

00:08:51,140 --> 00:08:57,110
things though first and foremost you

00:08:54,710 --> 00:09:02,090
will need HSA runtime currently

00:08:57,110 --> 00:09:06,650
available only from AMD which is

00:09:02,090 --> 00:09:11,660
available at this could have address now

00:09:06,650 --> 00:09:13,700
to be the problem with the AMD runtime

00:09:11,660 --> 00:09:15,620
at this moment is that this Wednesday

00:09:13,700 --> 00:09:19,400
there has been a new version released

00:09:15,620 --> 00:09:23,450
which is on one hand great news because

00:09:19,400 --> 00:09:26,090
we've been depending on one feature

00:09:23,450 --> 00:09:28,340
that's been brought about by this new

00:09:26,090 --> 00:09:32,480
release specifically dynamic parallelism

00:09:28,340 --> 00:09:34,820
at the same time since Wednesday I've

00:09:32,480 --> 00:09:38,600
seen some performance regressions I've

00:09:34,820 --> 00:09:41,510
seen some finalizar sec folds so we are

00:09:38,600 --> 00:09:44,960
currently at the moment in a state where

00:09:41,510 --> 00:09:48,380
things need to settle a little bit the

00:09:44,960 --> 00:09:51,350
new runtime needs a new it needs a patch

00:09:48,380 --> 00:09:53,060
colonel unfortunately the previous run

00:09:51,350 --> 00:09:56,330
time would work well with the reefs with

00:09:53,060 --> 00:09:58,250
the recent for that too with the recent

00:09:56,330 --> 00:10:00,350
release candidate colonel actually but

00:09:58,250 --> 00:10:04,910
you need a micron or lamb in the linux

00:10:00,350 --> 00:10:08,060
kernel because yeah I'm from Azusa Linux

00:10:04,910 --> 00:10:12,340
company so pretty much none of what I'm

00:10:08,060 --> 00:10:12,340
talking about applies to windows and

00:10:13,110 --> 00:10:21,260
and other operating systems but you need

00:10:17,279 --> 00:10:23,940
a new you need a HSA capable capable

00:10:21,260 --> 00:10:26,220
linux kernel which could which you can

00:10:23,940 --> 00:10:28,890
also grab from AMD represent or yet get

00:10:26,220 --> 00:10:32,850
at github problem with that repository

00:10:28,890 --> 00:10:36,120
is that yeah wednesday they released a

00:10:32,850 --> 00:10:38,519
new kernel with new drivers and new

00:10:36,120 --> 00:10:41,279
functionality but they only packaged it

00:10:38,519 --> 00:10:43,920
for you bun too so if you want to use

00:10:41,279 --> 00:10:45,600
something different or better than you

00:10:43,920 --> 00:10:48,060
been to for example like opensuse

00:10:45,600 --> 00:10:50,820
tumbleweed we have managed to extract

00:10:48,060 --> 00:10:53,790
all the important pieces and build

00:10:50,820 --> 00:10:57,890
ourselves an RPM that you can get from

00:10:53,790 --> 00:10:57,890
our build service at the address below

00:10:59,540 --> 00:11:05,850
then what you need to do is that you

00:11:02,459 --> 00:11:07,820
need to compile your own compiler i'll

00:11:05,850 --> 00:11:10,140
go very briefly through this as well

00:11:07,820 --> 00:11:11,959
because there's it's really not

00:11:10,140 --> 00:11:15,209
complicated and nothing to be afraid of

00:11:11,959 --> 00:11:17,339
nothing to be afraid of i still

00:11:15,209 --> 00:11:20,430
recommend at least a brief look at the

00:11:17,339 --> 00:11:23,310
documentation of building GCC that you

00:11:20,430 --> 00:11:25,170
can find in the GCC website but what

00:11:23,310 --> 00:11:29,070
really happens is it was once you have

00:11:25,170 --> 00:11:32,510
the source you either you need to get

00:11:29,070 --> 00:11:36,899
prerequisites which are BlackBerry's for

00:11:32,510 --> 00:11:40,410
dealing with big numbers complex numbers

00:11:36,899 --> 00:11:42,959
too and so forth but if you do not want

00:11:40,410 --> 00:11:45,000
to use your packaging your package

00:11:42,959 --> 00:11:48,329
manager of your linux distribution you

00:11:45,000 --> 00:11:49,980
can just down execute the script and it

00:11:48,329 --> 00:11:53,670
will download everything at the

00:11:49,980 --> 00:11:55,680
appropriate place then what i do is that

00:11:53,670 --> 00:11:59,130
i usually build it in a special

00:11:55,680 --> 00:12:01,019
directory then you configure with not

00:11:59,130 --> 00:12:05,870
just these two options but these are two

00:12:01,019 --> 00:12:09,449
options that are going to turn HSA

00:12:05,870 --> 00:12:11,730
optimization and offloading on so you so

00:12:09,449 --> 00:12:15,149
there is an enabler flow targets option

00:12:11,730 --> 00:12:18,279
that serves not only to enable HSA but

00:12:15,149 --> 00:12:22,089
also other optimizations

00:12:18,279 --> 00:12:23,829
and you need to provide a path to HSA

00:12:22,089 --> 00:12:27,670
runtime so that it can find especially

00:12:23,829 --> 00:12:30,269
the patters there and then you just run

00:12:27,670 --> 00:12:34,300
make and make install and you will get

00:12:30,269 --> 00:12:37,149
your own compiler in the directory where

00:12:34,300 --> 00:12:41,800
you configure it move for which you

00:12:37,149 --> 00:12:43,899
configure it to once you have it you get

00:12:41,800 --> 00:12:48,850
your example that you want to play with

00:12:43,899 --> 00:12:52,660
compile with fo openmp and when you want

00:12:48,850 --> 00:12:58,689
to run it you need to set the LD library

00:12:52,660 --> 00:13:02,709
path so that the all the necessary

00:12:58,689 --> 00:13:06,149
libraries which means the lib Gump the

00:13:02,709 --> 00:13:09,850
openmp runtime from the branch and the

00:13:06,149 --> 00:13:14,259
HSA runtime can be found now there is

00:13:09,850 --> 00:13:15,850
one key difference in between support

00:13:14,259 --> 00:13:20,470
for HSA and support for other

00:13:15,850 --> 00:13:22,240
accelerators in GCC and that is that the

00:13:20,470 --> 00:13:26,730
teams that were working on other

00:13:22,240 --> 00:13:31,480
accelerators which means nvidia and

00:13:26,730 --> 00:13:38,290
intel make accelerators they have chosen

00:13:31,480 --> 00:13:40,360
to force the users or make the users or

00:13:38,290 --> 00:13:43,209
distribute to the users two different

00:13:40,360 --> 00:13:46,839
compilers one for the host and one for

00:13:43,209 --> 00:13:49,120
the actual device the that you know one

00:13:46,839 --> 00:13:52,509
for the accelerator and then you have to

00:13:49,120 --> 00:13:54,639
use the you need more complex syntax of

00:13:52,509 --> 00:13:57,220
the enable afloat target to tell one

00:13:54,639 --> 00:14:03,160
compiler about the other unlike other

00:13:57,220 --> 00:14:10,720
compilers we did not do this we are able

00:14:03,160 --> 00:14:13,720
to produce code for HSA bh sale from one

00:14:10,720 --> 00:14:15,910
binary that has a few disadvantages but

00:14:13,720 --> 00:14:18,300
we do believe that the advantages over

00:14:15,910 --> 00:14:18,300
weight them

00:14:19,130 --> 00:14:24,630
so once you have this compiler what will

00:14:22,860 --> 00:14:27,360
happen the compiler will attempt to

00:14:24,630 --> 00:14:29,490
compile each target construct and each

00:14:27,360 --> 00:14:32,160
funk each function with it and declare

00:14:29,490 --> 00:14:35,550
target construct into H sale I said

00:14:32,160 --> 00:14:41,240
attempt because for various reasons that

00:14:35,550 --> 00:14:45,500
I will describe it may not succeed but

00:14:41,240 --> 00:14:49,079
we have shared virtual address space as

00:14:45,500 --> 00:14:51,269
I said before which means two things

00:14:49,079 --> 00:14:53,910
I've already mentioned one that is we

00:14:51,269 --> 00:14:55,800
can ignore the mapping clauses and we

00:14:53,910 --> 00:15:00,000
don't really have to worry about target

00:14:55,800 --> 00:15:03,000
data and target update because all you

00:15:00,000 --> 00:15:07,279
know the whole AP you both the GPU part

00:15:03,000 --> 00:15:10,260
and the CPU part has cache coherent

00:15:07,279 --> 00:15:11,670
albeit with a bit relaxed consistency

00:15:10,260 --> 00:15:13,410
but never cache coherent nevertheless

00:15:11,670 --> 00:15:15,779
view of the memory and they use the same

00:15:13,410 --> 00:15:18,360
address addresses even the same pointers

00:15:15,779 --> 00:15:20,430
but it also means one other important

00:15:18,360 --> 00:15:23,880
thing that devices which do not have

00:15:20,430 --> 00:15:26,519
shared memory do not enjoy and that is

00:15:23,880 --> 00:15:31,070
when a particular kernel for one reason

00:15:26,519 --> 00:15:35,610
or another cannot really be compiled for

00:15:31,070 --> 00:15:38,519
for GPU either because it is impossible

00:15:35,610 --> 00:15:41,839
because it makes sense or because our

00:15:38,519 --> 00:15:46,110
compiler is just not yet capable enough

00:15:41,839 --> 00:15:50,279
we can always just not emit code for

00:15:46,110 --> 00:15:52,500
that kernel and fall back on cpu

00:15:50,279 --> 00:15:55,649
implementation now this gets a little

00:15:52,500 --> 00:16:01,079
bit more difficult with failed functions

00:15:55,649 --> 00:16:04,410
nevertheless because and I will talk

00:16:01,079 --> 00:16:08,579
about it a little bit we should be able

00:16:04,410 --> 00:16:12,510
to figure out which kernels can actually

00:16:08,579 --> 00:16:18,949
be run safely and which cannot but the

00:16:12,510 --> 00:16:18,949
important thing is that because the

00:16:20,130 --> 00:16:26,620
because we do have the shared memory and

00:16:24,130 --> 00:16:31,300
the unified view of the memory it is

00:16:26,620 --> 00:16:35,500
possible that one target pragma is

00:16:31,300 --> 00:16:38,860
compiled for GPU and dust changes in the

00:16:35,500 --> 00:16:41,710
same memory and then these changes even

00:16:38,860 --> 00:16:44,320
without any interleaving target update

00:16:41,710 --> 00:16:47,290
for example are picked up by a cpu

00:16:44,320 --> 00:16:49,360
kernel which the user originally

00:16:47,290 --> 00:16:52,080
intended also to be compiled on the

00:16:49,360 --> 00:16:56,290
accelerator but but it did not succeed

00:16:52,080 --> 00:17:00,550
but let's talk about the things oh yeah

00:16:56,290 --> 00:17:02,740
one more thing and in any way just and

00:17:00,550 --> 00:17:05,290
this is a feature of all of the

00:17:02,740 --> 00:17:07,930
acceleration scheme of GCC in general

00:17:05,290 --> 00:17:11,230
the runtime is of course the thing that

00:17:07,930 --> 00:17:13,449
decides whether and to which device if

00:17:11,230 --> 00:17:16,360
you have more to offload so if somebody

00:17:13,449 --> 00:17:18,910
had both than it you know an HSA

00:17:16,360 --> 00:17:22,300
processor and intel make accelerator

00:17:18,910 --> 00:17:25,120
that you could decide using for example

00:17:22,300 --> 00:17:30,900
an environment variable to what

00:17:25,120 --> 00:17:35,580
accelerator you want to send your code

00:17:30,900 --> 00:17:40,090
but so let's have a look at what we can

00:17:35,580 --> 00:17:42,070
what we can accelerate and how so of

00:17:40,090 --> 00:17:46,300
course we are able to accelerate the

00:17:42,070 --> 00:17:49,420
very simple loops this loop is taken

00:17:46,300 --> 00:17:53,760
from stream benchmarks benchmark it is

00:17:49,420 --> 00:17:56,470
as you can see a simple vector copy and

00:17:53,760 --> 00:17:59,320
the only modification that we've done to

00:17:56,470 --> 00:18:02,200
it is that we added Oh pragma OMP target

00:17:59,320 --> 00:18:04,450
now because we have not added any

00:18:02,200 --> 00:18:06,640
mapping closest this might not actually

00:18:04,450 --> 00:18:10,660
be a portable code nevertheless it works

00:18:06,640 --> 00:18:12,970
on HSA and our GPUs can pick up the same

00:18:10,660 --> 00:18:14,680
addresses we do not attempt to do any

00:18:12,970 --> 00:18:18,810
address translation whatsoever because

00:18:14,680 --> 00:18:24,460
we don't have to and it just works

00:18:18,810 --> 00:18:28,450
however the difficulties start elsewhere

00:18:24,460 --> 00:18:33,910
and mainly from the explicitly parallel

00:18:28,450 --> 00:18:37,360
GPU programming model because you can

00:18:33,910 --> 00:18:40,270
compile this example in two ways when

00:18:37,360 --> 00:18:44,110
one way you can compile example just as

00:18:40,270 --> 00:18:49,480
the specification tells you to so you

00:18:44,110 --> 00:18:52,330
run one thread that that would run the

00:18:49,480 --> 00:18:54,400
body of our MP target that won't be

00:18:52,330 --> 00:18:57,840
target would then use dynamic

00:18:54,400 --> 00:19:01,500
parallelism to spawn a number of threats

00:18:57,840 --> 00:19:09,360
to run the body of the ump parallel and

00:19:01,500 --> 00:19:13,030
then the pragma construct would divide

00:19:09,360 --> 00:19:16,090
the iteration space in between in

00:19:13,030 --> 00:19:19,330
between that number of threats and there

00:19:16,090 --> 00:19:21,250
would be a artificial loop for each

00:19:19,330 --> 00:19:26,260
threat where each threat would loop

00:19:21,250 --> 00:19:30,250
across over its portion of the of the

00:19:26,260 --> 00:19:33,460
iteration space and that would work out

00:19:30,250 --> 00:19:36,760
of the box bhosle were just with H sale

00:19:33,460 --> 00:19:40,090
back end but the problem is that it is

00:19:36,760 --> 00:19:43,390
not efficient what we wanted to do for

00:19:40,090 --> 00:19:46,180
what we wanted to produce for GPU would

00:19:43,390 --> 00:19:48,550
be an explicit grid so that when we call

00:19:46,180 --> 00:19:51,130
the target we would provide the grid

00:19:48,550 --> 00:19:53,470
parameters the grid size we would

00:19:51,130 --> 00:19:56,380
actually say this is a 1d grid and its

00:19:53,470 --> 00:20:03,390
size is stream array size so let's say

00:19:56,380 --> 00:20:03,390
16 kilo yeah 16 kilos off

00:20:03,520 --> 00:20:13,180
the era's have 16 kilo element and we

00:20:10,100 --> 00:20:16,460
would not ideally want to have any

00:20:13,180 --> 00:20:20,300
contraflow any looping any artificial

00:20:16,460 --> 00:20:24,230
loops if we wanted to run this on a GPU

00:20:20,300 --> 00:20:27,430
so we have changed the expansion of

00:20:24,230 --> 00:20:31,610
OpenMP in GCC to do exactly this and

00:20:27,430 --> 00:20:34,280
actually copy the code very early and

00:20:31,610 --> 00:20:36,410
then process it differently even

00:20:34,280 --> 00:20:42,460
throughout the MP lowering and expansion

00:20:36,410 --> 00:20:46,370
and so yeah let me show you the results

00:20:42,460 --> 00:20:50,270
this is the first loop there are three

00:20:46,370 --> 00:20:52,610
more there is a scale there's a loop

00:20:50,270 --> 00:20:55,730
doing scaling of the whole array there

00:20:52,610 --> 00:20:59,240
is a loop that adds two huge erase

00:20:55,730 --> 00:21:05,210
together and the final one called triode

00:20:59,240 --> 00:21:10,130
does both scaling and addition and these

00:21:05,210 --> 00:21:14,150
were the results so we have this apu

00:21:10,130 --> 00:21:16,610
which we got in the spring it is an AMD

00:21:14,150 --> 00:21:21,580
engineering sample so I'm not allowed to

00:21:16,610 --> 00:21:24,640
say much more about it but this is the

00:21:21,580 --> 00:21:29,480
measurements we got when we used I'm

00:21:24,640 --> 00:21:32,480
when we used the current version of GCC

00:21:29,480 --> 00:21:34,790
on the cpus and it is including

00:21:32,480 --> 00:21:37,670
vectorization it is at the highest

00:21:34,790 --> 00:21:41,480
optimization level possible the

00:21:37,670 --> 00:21:45,170
processor the apu or the cpu in the APU

00:21:41,480 --> 00:21:48,140
is not a particularly strong one but it

00:21:45,170 --> 00:21:51,140
is the results are better for example

00:21:48,140 --> 00:21:53,660
then on my notebook which is one ero

00:21:51,140 --> 00:21:56,750
then I consider it very very nice so so

00:21:53,660 --> 00:22:00,950
it's not bad machine so these are the

00:21:56,750 --> 00:22:05,750
results for 64 kilobytes of memory is 16

00:22:00,950 --> 00:22:12,049
kilos of float on a carousel APU when we

00:22:05,750 --> 00:22:18,179
run the CPU code this is when we run

00:22:12,049 --> 00:22:21,240
the when we run the code on GPU and we

00:22:18,179 --> 00:22:24,000
actually expand it especially for GPU so

00:22:21,240 --> 00:22:26,250
we what we say for lack of a better word

00:22:24,000 --> 00:22:31,559
is that we ratified the loop we have

00:22:26,250 --> 00:22:35,190
turned it into one call into the HSA

00:22:31,559 --> 00:22:37,159
runtime giving it only giving it the

00:22:35,190 --> 00:22:42,120
colonel without any looping construct

00:22:37,159 --> 00:22:48,899
representing the parody OMP pyro OMP OMP

00:22:42,120 --> 00:22:53,269
for construct and wet and the throughput

00:22:48,899 --> 00:22:56,760
is very very nice when we just use the

00:22:53,269 --> 00:22:59,850
CPU intermediate when we just use the

00:22:56,760 --> 00:23:02,510
CPU expansion of OpenMP into GCC

00:22:59,850 --> 00:23:04,830
intermediate language and then just

00:23:02,510 --> 00:23:07,710
translated this GCC intermediate

00:23:04,830 --> 00:23:11,059
language into H sale and ran it on the

00:23:07,710 --> 00:23:18,269
GPU the performance was very very poor

00:23:11,059 --> 00:23:24,570
so the bottom line was really we

00:23:18,269 --> 00:23:27,120
probably need to handle code for GPUs

00:23:24,570 --> 00:23:29,429
differently very very early on and that

00:23:27,120 --> 00:23:32,039
qualification is very very worthwhile go

00:23:29,429 --> 00:23:36,090
now if you are familiar with the spring

00:23:32,039 --> 00:23:42,000
benchmark it SAS and its readme file

00:23:36,090 --> 00:23:46,169
that the mem in order to get mmm how to

00:23:42,000 --> 00:23:49,080
put it in order to get a representative

00:23:46,169 --> 00:23:52,260
results you should run it on much bigger

00:23:49,080 --> 00:23:56,250
eres so this is a fair comparison of the

00:23:52,260 --> 00:23:58,950
GPU and CPU when it comes to the really

00:23:56,250 --> 00:24:01,980
throughput that you can get nevertheless

00:23:58,950 --> 00:24:04,769
the traditional expansion is even worse

00:24:01,980 --> 00:24:07,320
in comparison and the because over here

00:24:04,769 --> 00:24:08,970
the bars are almost invisible that is

00:24:07,320 --> 00:24:12,260
the reason why I actually added the

00:24:08,970 --> 00:24:15,870
previous slide to the presentation

00:24:12,260 --> 00:24:19,020
but the gratification requires a little

00:24:15,870 --> 00:24:24,320
bit different way of thinking about

00:24:19,020 --> 00:24:27,510
OpenMP you no longer have the luxury of

00:24:24,320 --> 00:24:29,940
reasoning about each construct in

00:24:27,510 --> 00:24:33,570
isolation when you want to gratify this

00:24:29,940 --> 00:24:39,440
loop you have to gratify the whole thing

00:24:33,570 --> 00:24:42,390
and which means that each Clause of each

00:24:39,440 --> 00:24:45,870
construct is the clause affecting the

00:24:42,390 --> 00:24:49,830
whole grid it also means for example

00:24:45,870 --> 00:24:52,650
that the iteration space size over here

00:24:49,830 --> 00:24:55,050
from zero to stream array size is not

00:24:52,650 --> 00:24:57,210
only property of the four construct but

00:24:55,050 --> 00:24:59,190
it is also the property of the target

00:24:57,210 --> 00:25:02,610
construct because it needs to be passed

00:24:59,190 --> 00:25:06,810
to HSA runtime so that it can prepare to

00:25:02,610 --> 00:25:08,390
run so that many that many threats which

00:25:06,810 --> 00:25:12,870
by the way brings me to the previous

00:25:08,390 --> 00:25:16,020
previous presentation and the idea of

00:25:12,870 --> 00:25:19,500
whether it was wise to leave

00:25:16,020 --> 00:25:24,210
implementation defined defaults in the

00:25:19,500 --> 00:25:26,610
target in the target environment while

00:25:24,210 --> 00:25:30,570
over here if the stream array size is 16

00:25:26,610 --> 00:25:33,150
kilo that is the number of threats that

00:25:30,570 --> 00:25:35,040
the runtime will pretend to generate

00:25:33,150 --> 00:25:38,900
that there will be a very lightweight

00:25:35,040 --> 00:25:41,280
GPU threat nevertheless if you asked in

00:25:38,900 --> 00:25:43,800
inside the body of that loop give me the

00:25:41,280 --> 00:25:45,990
number of threats you would get 16,000

00:25:43,800 --> 00:25:50,150
which is unlikely on a cpu and of course

00:25:45,990 --> 00:25:56,190
if your programming techniques involve

00:25:50,150 --> 00:25:59,820
such things as creating an array of the

00:25:56,190 --> 00:26:05,090
size of the number of number of threats

00:25:59,820 --> 00:26:10,680
then you are in dire trouble so it does

00:26:05,090 --> 00:26:12,240
change so the changes although they seem

00:26:10,680 --> 00:26:15,160
to be very straightforward in the

00:26:12,240 --> 00:26:20,740
beginning there are very very many

00:26:15,160 --> 00:26:24,160
implications now one of the implication

00:26:20,740 --> 00:26:27,250
is that perfect construct nesting is

00:26:24,160 --> 00:26:33,430
required there can't in order to do

00:26:27,250 --> 00:26:35,470
Krita fication at the moment the

00:26:33,430 --> 00:26:37,810
compiler can't do it if there are any

00:26:35,470 --> 00:26:40,000
statements in between the target or in

00:26:37,810 --> 00:26:45,970
parallel or in between the Tyrell and

00:26:40,000 --> 00:26:47,800
four and the four construct because you

00:26:45,970 --> 00:26:49,630
know it is fairly different to reason

00:26:47,800 --> 00:26:53,080
about that the problem is with the

00:26:49,630 --> 00:26:55,060
current implementation that the perfect

00:26:53,080 --> 00:26:57,520
nesting is required at the intermediate

00:26:55,060 --> 00:26:59,890
language level which is a problem

00:26:57,520 --> 00:27:01,570
because especially the Fortran but not

00:26:59,890 --> 00:27:06,010
only but especially the Fortran front

00:27:01,570 --> 00:27:07,960
end or often puts statements in between

00:27:06,010 --> 00:27:12,310
them even though the even when the

00:27:07,960 --> 00:27:14,050
programmer did not put any there so this

00:27:12,310 --> 00:27:17,590
requirement will have to be relaxed

00:27:14,050 --> 00:27:21,010
somewhat and will be working to that in

00:27:17,590 --> 00:27:26,020
one way it is also similar to

00:27:21,010 --> 00:27:28,660
vectorization so that at least in the

00:27:26,020 --> 00:27:30,220
early stages what we believe is that we

00:27:28,660 --> 00:27:35,290
will be giving feedback to the

00:27:30,220 --> 00:27:37,840
programmer why we did not gratify a

00:27:35,290 --> 00:27:41,890
particular target construct containing a

00:27:37,840 --> 00:27:43,990
loop and hope that we will be able to

00:27:41,890 --> 00:27:46,570
provide enough information so that the

00:27:43,990 --> 00:27:50,890
programmer can change their code to get

00:27:46,570 --> 00:27:53,740
it pretty fine but there are things that

00:27:50,890 --> 00:27:55,540
prevent gratification for example if you

00:27:53,740 --> 00:27:57,640
specify maximum number of threats then

00:27:55,540 --> 00:28:00,220
well there is a maximum number of

00:27:57,640 --> 00:28:03,250
threats and the threads have to loop so

00:28:00,220 --> 00:28:06,790
that prevents gratification

00:28:03,250 --> 00:28:09,430
as does not automatically scheduling if

00:28:06,790 --> 00:28:13,180
you say you want a chunk of especially

00:28:09,430 --> 00:28:16,330
when you provide a chunk size if you say

00:28:13,180 --> 00:28:18,880
one a particular chunk size then for

00:28:16,330 --> 00:28:21,730
various reasons the implementation has

00:28:18,880 --> 00:28:27,460
to honor that and and and cannot change

00:28:21,730 --> 00:28:29,100
the behavior we do have a limited

00:28:27,460 --> 00:28:34,210
support for teams and distribution

00:28:29,100 --> 00:28:37,930
construct though nevertheless we for

00:28:34,210 --> 00:28:40,030
example do not allow to limit in any

00:28:37,930 --> 00:28:45,030
meaningful way the maximum number of

00:28:40,030 --> 00:28:53,370
teams so you can give a maximum number

00:28:45,030 --> 00:28:56,740
of off threats in one thread group but

00:28:53,370 --> 00:28:58,690
that's pretty much anything all that we

00:28:56,740 --> 00:29:03,280
allow at the moment this will only have

00:28:58,690 --> 00:29:06,310
to be expanded a little bit we I have

00:29:03,280 --> 00:29:09,790
almost finished support for reductions

00:29:06,310 --> 00:29:16,140
through Atomics of course on a GPU

00:29:09,790 --> 00:29:19,900
reductions should be ideally done using

00:29:16,140 --> 00:29:21,990
a recursive implementation nevertheless

00:29:19,900 --> 00:29:24,910
that is something for the next release

00:29:21,990 --> 00:29:29,220
but we do want to support them in the

00:29:24,910 --> 00:29:33,040
source code so we will have them in

00:29:29,220 --> 00:29:35,890
first version at least implemented at

00:29:33,040 --> 00:29:39,040
least using Atomics and we want to

00:29:35,890 --> 00:29:42,820
support collapse to and collapse 3 now

00:29:39,040 --> 00:29:46,420
that doesn't mean that when you that

00:29:42,820 --> 00:29:48,040
when you specify collapse 27 the code

00:29:46,420 --> 00:29:50,380
will not compile it will just not get

00:29:48,040 --> 00:29:53,560
verified because of course collapse 1 2

00:29:50,380 --> 00:29:55,720
& 3 kind of map very nicely to the one

00:29:53,560 --> 00:29:58,480
two or three dimensional grid that HSA

00:29:55,720 --> 00:30:00,700
support if you want more dimensions some

00:29:58,480 --> 00:30:04,330
looping is necessary and therefore

00:30:00,700 --> 00:30:07,060
looping will be required but there are

00:30:04,330 --> 00:30:08,840
things that actually prevent your code

00:30:07,060 --> 00:30:15,559
from compilation

00:30:08,840 --> 00:30:17,480
and first an important thing and then I

00:30:15,559 --> 00:30:19,400
think we'll actually never really work

00:30:17,480 --> 00:30:23,360
because it will never probably really

00:30:19,400 --> 00:30:25,460
make sense to compile it for GPU is that

00:30:23,360 --> 00:30:31,640
anything that looks remotely as a

00:30:25,460 --> 00:30:33,230
critical section because only if you if

00:30:31,640 --> 00:30:35,679
you remember the slide with the team's

00:30:33,230 --> 00:30:38,120
only the threats within threat group

00:30:35,679 --> 00:30:42,710
within a work group can actually

00:30:38,120 --> 00:30:45,289
communicate and synchronize and it is so

00:30:42,710 --> 00:30:50,750
difficult to try to implement critical

00:30:45,289 --> 00:30:54,429
section um that would spend work groups

00:30:50,750 --> 00:30:58,580
that is pressed probably not worth it

00:30:54,429 --> 00:31:02,299
there are things that we do not intend

00:30:58,580 --> 00:31:05,299
to support now we have not really even

00:31:02,299 --> 00:31:09,549
thought about sections and tasks and on

00:31:05,299 --> 00:31:15,100
other non loopy work sharing constructs

00:31:09,549 --> 00:31:19,070
although HSA has support for queuing

00:31:15,100 --> 00:31:21,649
kernels and actually specifying

00:31:19,070 --> 00:31:24,950
dependencies among them so in the future

00:31:21,649 --> 00:31:28,220
we may or will probably will want to

00:31:24,950 --> 00:31:31,520
have a look at whether OMP tasking

00:31:28,220 --> 00:31:33,830
constructs can be mapped into this

00:31:31,520 --> 00:31:39,140
programming model but so far we have not

00:31:33,830 --> 00:31:42,500
gone that far at all similarly we have

00:31:39,140 --> 00:31:45,590
not really thought about supporting sim

00:31:42,500 --> 00:31:49,460
deconstructs there are two reasons for

00:31:45,590 --> 00:31:52,130
that one is architectural within GCC big

00:31:49,460 --> 00:31:54,880
cause and this is the disadvantage of

00:31:52,130 --> 00:31:58,039
doing everything within one binary

00:31:54,880 --> 00:32:01,669
because the vector riser of course

00:31:58,039 --> 00:32:04,879
currently a parametric to vectorize for

00:32:01,669 --> 00:32:08,389
the cpu we need to

00:32:04,879 --> 00:32:11,730
when we we actually have to be able to

00:32:08,389 --> 00:32:13,740
have repair metrical yeah actually have

00:32:11,730 --> 00:32:16,200
to allow it to reap air mattress itself

00:32:13,740 --> 00:32:18,389
either for the CPU either for the host

00:32:16,200 --> 00:32:20,249
or for the accelerator and architectural

00:32:18,389 --> 00:32:22,200
II within GCC this is not a simple thing

00:32:20,249 --> 00:32:26,720
nevertheless you know certainly doable

00:32:22,200 --> 00:32:31,399
another task is that often we will be

00:32:26,720 --> 00:32:36,419
partitioning a straight line of code and

00:32:31,399 --> 00:32:39,330
when we do that when you vectorize a

00:32:36,419 --> 00:32:43,529
loop what you end up with with is a loop

00:32:39,330 --> 00:32:46,049
with fewer iterations over here we do

00:32:43,529 --> 00:32:49,169
not have a loop we will have to reduce

00:32:46,049 --> 00:32:51,299
the grid ties which means that it will

00:32:49,169 --> 00:32:53,190
be because of architectural reasons and

00:32:51,299 --> 00:32:54,779
interprocedural optimizations and so

00:32:53,190 --> 00:32:58,740
forth so it is also something quite

00:32:54,779 --> 00:33:01,289
difficult we do not support things like

00:32:58,740 --> 00:33:04,769
dynamic scheduling when the loop would

00:33:01,289 --> 00:33:09,210
have to call back to the runtime because

00:33:04,769 --> 00:33:12,480
if 64 threats that work in lockstep all

00:33:09,210 --> 00:33:15,809
wanted to request another chunk from the

00:33:12,480 --> 00:33:18,350
runtime on cpu that would be just you

00:33:15,809 --> 00:33:24,480
know probably is not worth implementing

00:33:18,350 --> 00:33:28,529
and let me actually hi this and this

00:33:24,480 --> 00:33:31,230
list of open and be features that we are

00:33:28,529 --> 00:33:33,419
currently not able to support and we

00:33:31,230 --> 00:33:35,279
might have problems supporting even in

00:33:33,419 --> 00:33:37,649
the long term is not complete I'm it as

00:33:35,279 --> 00:33:40,590
it is an overview of things that I've

00:33:37,649 --> 00:33:42,960
come across and I've could immediately

00:33:40,590 --> 00:33:46,559
think of when I thought what I wanted to

00:33:42,960 --> 00:33:49,440
put down list of things that I see if as

00:33:46,559 --> 00:33:52,499
very difficult to support but there are

00:33:49,440 --> 00:33:56,190
problems beyond open and PE and that is

00:33:52,499 --> 00:34:00,210
that HSA cannot do well a few other

00:33:56,190 --> 00:34:03,059
things one very obvious thing that kind

00:34:00,210 --> 00:34:08,040
of do well our indirect calls it is

00:34:03,059 --> 00:34:10,290
actually the standard provides some

00:34:08,040 --> 00:34:13,290
support for it but the runtime and the

00:34:10,290 --> 00:34:15,520
CPU has to do a very heavy lifting to

00:34:13,290 --> 00:34:19,540
allow one sing

00:34:15,520 --> 00:34:21,909
indirect call and I don't believe it is

00:34:19,540 --> 00:34:24,970
actually even implemented in the current

00:34:21,909 --> 00:34:28,440
runtime and it is just really really a

00:34:24,970 --> 00:34:34,179
difficult thing to do but that of course

00:34:28,440 --> 00:34:36,100
brings one you know one conclusion from

00:34:34,179 --> 00:34:41,110
this slide is that when you look at the

00:34:36,100 --> 00:34:43,510
first and the last item and if you I

00:34:41,110 --> 00:34:47,290
don't know about other implementations

00:34:43,510 --> 00:34:50,050
of OpenMP but at least the GCC runtime

00:34:47,290 --> 00:34:53,100
support of OpenMP programs the lib grump

00:34:50,050 --> 00:34:58,810
it is based on indirect calls and

00:34:53,100 --> 00:35:01,870
critical sections and mutexes so we

00:34:58,810 --> 00:35:07,030
really try to implement almost

00:35:01,870 --> 00:35:09,760
everything in the compiled code and we

00:35:07,030 --> 00:35:14,250
have to rely very very little on support

00:35:09,760 --> 00:35:19,150
from the runtime which is again very

00:35:14,250 --> 00:35:22,870
different approach from how CPUs openmp

00:35:19,150 --> 00:35:31,770
has been compiled and has been processed

00:35:22,870 --> 00:35:31,770
oh sure I've actually been fairly quick

00:35:32,220 --> 00:35:38,440
so what I wanted to tell you when my

00:35:34,450 --> 00:35:41,110
talk is to bring our attention to HSA

00:35:38,440 --> 00:35:43,870
and its unified virtual memory and not

00:35:41,110 --> 00:35:48,040
only that it allows you not to care very

00:35:43,870 --> 00:35:52,300
much about the mapping but also to be

00:35:48,040 --> 00:35:57,370
able to currently to compile only some

00:35:52,300 --> 00:36:00,970
target construct for for GP when all

00:35:57,370 --> 00:36:04,440
others without caring for updating

00:36:00,970 --> 00:36:08,440
membrane between and I wanted to

00:36:04,440 --> 00:36:10,060
communicate to you that HSA is going is

00:36:08,440 --> 00:36:12,340
about to be programmable with GCC

00:36:10,060 --> 00:36:15,190
through OpenMP construct and we are

00:36:12,340 --> 00:36:18,510
primarily targeting open and big

00:36:15,190 --> 00:36:20,570
instructs with the emphasis on

00:36:18,510 --> 00:36:22,220
portability we do

00:36:20,570 --> 00:36:24,620
you we don't care about the mapping

00:36:22,220 --> 00:36:28,010
constructs we be with someone else has

00:36:24,620 --> 00:36:30,080
to add them in the air but we do not you

00:36:28,010 --> 00:36:32,900
know we know that it is important that

00:36:30,080 --> 00:36:35,180
people write the same should write the

00:36:32,900 --> 00:36:38,360
same code that would run anywhere and

00:36:35,180 --> 00:36:40,670
hopefully run well anywhere you have

00:36:38,360 --> 00:36:44,120
already been already told you that fall

00:36:40,670 --> 00:36:46,820
back on cpu implementation is a crucial

00:36:44,120 --> 00:36:50,360
feature that allows us to go forward in

00:36:46,820 --> 00:36:53,240
the initial stages and encode that

00:36:50,360 --> 00:36:56,300
relies on indirect calls or critical

00:36:53,240 --> 00:37:02,720
sections it might be the thing that will

00:36:56,300 --> 00:37:04,520
allow us to component it might be the

00:37:02,720 --> 00:37:11,500
feature that will allow us to compile

00:37:04,520 --> 00:37:15,500
that code in the long term as well and I

00:37:11,500 --> 00:37:17,770
also wanted to stress that one in what

00:37:15,500 --> 00:37:24,080
we believe as an important concept of

00:37:17,770 --> 00:37:28,160
compiling OpenMP code for GPUs is the

00:37:24,080 --> 00:37:34,820
gratification and go duplication which

00:37:28,160 --> 00:37:38,540
means that this even though that the

00:37:34,820 --> 00:37:42,500
code will be duplicated very early on

00:37:38,540 --> 00:37:47,540
and for example and it works but will

00:37:42,500 --> 00:37:50,780
work best if for example the parallel

00:37:47,540 --> 00:37:57,110
for construct are allowed to do their

00:37:50,780 --> 00:38:01,070
own thing so that various clauses that

00:37:57,110 --> 00:38:04,880
for example specify the scheduling are

00:38:01,070 --> 00:38:07,700
very alien to efficient to efficient

00:38:04,880 --> 00:38:09,620
compilation for for GPU and of course

00:38:07,700 --> 00:38:12,350
the gratification as a concept is the

00:38:09,620 --> 00:38:14,230
Holy Grail what we would like to do with

00:38:12,350 --> 00:38:18,400
pretty much any

00:38:14,230 --> 00:38:26,800
loop that is supposed to be paralyzed

00:38:18,400 --> 00:38:29,560
and offloaded and yeah five minutes at

00:38:26,800 --> 00:38:31,510
least if not more faster than I thought

00:38:29,560 --> 00:38:35,940
it would be nevertheless if you got any

00:38:31,510 --> 00:38:35,940
questions you got a lot of time for them

00:38:42,090 --> 00:38:49,150
currently nope we need them no we need

00:38:46,840 --> 00:38:53,940
we need it we need it the reason why we

00:38:49,150 --> 00:38:53,940
needed our math functions

00:39:00,350 --> 00:39:08,400
that's all done by the hardware but well

00:39:03,060 --> 00:39:12,690
let me go back to the parallel loop this

00:39:08,400 --> 00:39:18,270
one if you do not gratify it what you

00:39:12,690 --> 00:39:21,870
need to do is to is to use dynamic

00:39:18,270 --> 00:39:23,850
parallelism to run another colonel so if

00:39:21,870 --> 00:39:26,850
you switch off paralyzation or if you

00:39:23,850 --> 00:39:30,600
write this code but for the fun of it

00:39:26,850 --> 00:39:33,720
you add schedule static or schedule but

00:39:30,600 --> 00:39:38,220
we have static that is compatible for

00:39:33,720 --> 00:39:41,070
HSA but will not be gratified and what

00:39:38,220 --> 00:39:42,630
will that will mean that yeah you will

00:39:41,070 --> 00:39:46,580
get only one thread running the target

00:39:42,630 --> 00:39:50,730
thing and that one fret knows how to

00:39:46,580 --> 00:39:53,040
invoke another colonel but that is again

00:39:50,730 --> 00:39:57,570
something that is code generated by the

00:39:53,040 --> 00:40:00,540
compiler and yeah we do expect that a

00:39:57,570 --> 00:40:03,150
lot of the support patterns will be

00:40:00,540 --> 00:40:05,130
generated by the compiler and not

00:40:03,150 --> 00:40:06,900
provided by by the runtime but there

00:40:05,130 --> 00:40:09,030
will be a small runtime there boobie

00:40:06,900 --> 00:40:11,900
yeah but not for much bookkeeping and

00:40:09,030 --> 00:40:11,900
these things

00:40:14,050 --> 00:40:23,480
i oh

00:40:16,400 --> 00:40:28,089
oh yeah currently doesn't work what is

00:40:23,480 --> 00:40:32,359
possible and how we plan to support

00:40:28,089 --> 00:40:34,430
something like yeah well how we support

00:40:32,359 --> 00:40:41,569
I oh so we support some limited printf

00:40:34,430 --> 00:40:46,160
is that HSA has mechanisms of GPU

00:40:41,569 --> 00:40:50,270
invoking code and CPU so it is slow of

00:40:46,160 --> 00:40:52,670
course but and it works pretty much in

00:40:50,270 --> 00:40:55,430
the same way that dynamic parallelism

00:40:52,670 --> 00:40:58,369
work so so you you create a packet and

00:40:55,430 --> 00:41:03,890
you throw it on a cue that the hardware

00:40:58,369 --> 00:41:06,680
is able to manage and then the CPU hook

00:41:03,890 --> 00:41:10,970
will be able to collect that packet look

00:41:06,680 --> 00:41:13,190
at it on a packet and do a printf so no

00:41:10,970 --> 00:41:17,359
it currently does not work because for

00:41:13,190 --> 00:41:19,970
our debugging purposes yeah we've just

00:41:17,359 --> 00:41:22,940
managed to do without it but I do

00:41:19,970 --> 00:41:27,289
understand it says you know interesting

00:41:22,940 --> 00:41:31,150
and and doable and we will have it you

00:41:27,289 --> 00:41:31,150
are of course right those calls

00:41:34,870 --> 00:41:42,680
there are two okay there are three

00:41:38,000 --> 00:41:46,370
reasons basically why the kernels that

00:41:42,680 --> 00:41:47,990
are not purified are slow yeah that's

00:41:46,370 --> 00:41:51,650
probably one thing that I then I forgot

00:41:47,990 --> 00:41:53,780
about the one is control for GPUs do not

00:41:51,650 --> 00:41:56,750
really like control flow but as long as

00:41:53,780 --> 00:41:58,190
both you know all of them go one way or

00:41:56,750 --> 00:42:00,470
the other the control flow should be

00:41:58,190 --> 00:42:02,510
okay another thing is the dynamic

00:42:00,470 --> 00:42:05,210
parallelism we have seen actually quite

00:42:02,510 --> 00:42:08,450
important slowdowns we hope that the HSA

00:42:05,210 --> 00:42:11,390
runtime is able to limit these someone

00:42:08,450 --> 00:42:16,160
and the third way the third reason why

00:42:11,390 --> 00:42:18,650
even this toy example is slow is how the

00:42:16,160 --> 00:42:21,890
iteration space is divided among threats

00:42:18,650 --> 00:42:26,720
and we believe that is the biggest bit

00:42:21,890 --> 00:42:29,570
of that is the biggest problem the way

00:42:26,720 --> 00:42:34,570
how these are of the iteration space is

00:42:29,570 --> 00:42:38,330
divided among threat is aimed at

00:42:34,570 --> 00:42:42,320
achieving the best performance with cpu

00:42:38,330 --> 00:42:44,720
Cassius so the each threat gets a

00:42:42,320 --> 00:42:47,990
contiguous chunk of the iteration space

00:42:44,720 --> 00:42:53,480
and it works you know on the elements of

00:42:47,990 --> 00:42:58,640
the when it uses those indices as

00:42:53,480 --> 00:43:01,310
indices into arrays that allows for cash

00:42:58,640 --> 00:43:03,350
locality and good cash performance but

00:43:01,310 --> 00:43:08,380
that is exactly what you do not want to

00:43:03,350 --> 00:43:08,380
do on a GPU which one important

00:43:09,570 --> 00:43:14,100
feature of gpus that allows them to

00:43:12,180 --> 00:43:18,480
achieve the performance is that they are

00:43:14,100 --> 00:43:21,270
able to coalesce memory reads and writes

00:43:18,480 --> 00:43:25,020
and when you do this it is not possible

00:43:21,270 --> 00:43:29,190
because the when they work in a lockstep

00:43:25,020 --> 00:43:32,700
and each threat has its own contiguous

00:43:29,190 --> 00:43:36,840
chunk then each access is very far from

00:43:32,700 --> 00:43:38,970
the other so so we not only want to do

00:43:36,840 --> 00:43:41,400
code duplication and different expansion

00:43:38,970 --> 00:43:47,190
for the sake of gratification but when

00:43:41,400 --> 00:43:50,510
there is a looping construct we would

00:43:47,190 --> 00:43:52,860
actually want and no scheduling clause

00:43:50,510 --> 00:43:56,100
what we want to do and what we do

00:43:52,860 --> 00:43:58,950
believe will help a lot once the dynamic

00:43:56,100 --> 00:44:02,610
parallelism overhead is diminishes

00:43:58,950 --> 00:44:06,780
somewhat is that we will just do a

00:44:02,610 --> 00:44:08,730
different distribution of the loop we

00:44:06,780 --> 00:44:11,490
also do not have you know I don't know I

00:44:08,730 --> 00:44:13,970
have as we talked about it because

00:44:11,490 --> 00:44:17,580
openmp currently does not have any

00:44:13,970 --> 00:44:21,120
support for the group local memory or at

00:44:17,580 --> 00:44:23,880
least I really couldn't find any that

00:44:21,120 --> 00:44:28,640
basically prevents tiling which i

00:44:23,880 --> 00:44:30,900
believe is another very important mm

00:44:28,640 --> 00:44:36,330
programming technique when programming

00:44:30,900 --> 00:44:38,910
GPUs but I'm the implementation guy so

00:44:36,330 --> 00:44:42,330
yeah I'm very glad that's not my fault

00:44:38,910 --> 00:44:50,460
yeah but of course once that is but once

00:44:42,330 --> 00:44:52,380
that is specified we will take that into

00:44:50,460 --> 00:44:55,080
account and of course then the

00:44:52,380 --> 00:44:58,710
distribute and teams constructs will

00:44:55,080 --> 00:45:00,900
become much more useful at the moment I

00:44:58,710 --> 00:45:02,460
just dunno what do you know that the

00:45:00,900 --> 00:45:07,150
support is really really

00:45:02,460 --> 00:45:10,260
just so that we the gratification is not

00:45:07,150 --> 00:45:10,260
prevented and simpler cases

00:45:16,780 --> 00:45:26,650
when I say GPUs I mean HSA GPUs so at

00:45:21,700 --> 00:45:28,980
the moment that I believe AMD le but we

00:45:26,650 --> 00:45:28,980
will see

00:45:40,130 --> 00:45:45,910
that's why I asked the question

00:45:42,860 --> 00:45:45,910
nvm same

00:45:48,380 --> 00:45:51,589
we would

00:45:58,390 --> 00:46:00,599
games

00:46:10,840 --> 00:46:13,840
parallel

00:46:16,839 --> 00:46:19,839
factorization

00:46:31,360 --> 00:46:33,930
lass

00:46:34,430 --> 00:46:37,430
yep

00:46:39,330 --> 00:46:42,770
develop high school

00:46:47,699 --> 00:46:52,689
okay after for me everything is

00:46:50,650 --> 00:46:56,619
parallelization but yeah because you

00:46:52,689 --> 00:46:58,509
usually do not refer to paralyzation of

00:46:56,619 --> 00:47:00,849
the outer loops as vectorization but

00:46:58,509 --> 00:47:03,929
yeah I mean of course the probe problem

00:47:00,849 --> 00:47:03,929
is very similar

00:47:13,950 --> 00:47:16,950
you

00:47:23,910 --> 00:47:26,910
really

00:47:27,369 --> 00:47:38,089
well when a lot of testing has been done

00:47:33,770 --> 00:47:43,970
by AMD people who have actually tried

00:47:38,089 --> 00:47:48,740
this on scientific benchmarks that are

00:47:43,970 --> 00:47:51,910
out there okay but yeah this is a

00:47:48,740 --> 00:47:56,720
preliminary work I mean I'm not gonna

00:47:51,910 --> 00:47:58,520
I'm not gonna pretend that yeah sure I'm

00:47:56,720 --> 00:48:01,720
not going to pretend that this is not

00:47:58,520 --> 00:48:01,720
work in progress and then

00:48:14,740 --> 00:48:17,740
exactly

00:48:24,219 --> 00:48:30,769
we're trying we have seen basic

00:48:28,239 --> 00:48:33,369
capability for the programming

00:48:30,769 --> 00:48:33,369
opportunity

00:48:44,450 --> 00:48:53,310
I'm afraid that you know being better

00:48:49,790 --> 00:48:55,620
what does me I'm afraid that the being

00:48:53,310 --> 00:48:59,580
more versatile allowing for you know

00:48:55,620 --> 00:49:04,920
critical sections which see on arguably

00:48:59,580 --> 00:49:08,040
is able to support is going to be almost

00:49:04,920 --> 00:49:10,220
impossible supporting yeah but then that

00:49:08,040 --> 00:49:10,220
would

00:49:15,030 --> 00:49:21,970
but yeah of course we the goal is to

00:49:19,060 --> 00:49:25,950
just run the same code that does not

00:49:21,970 --> 00:49:25,950
have too much synchronization in it and

00:49:45,670 --> 00:49:48,720
I'm sorry

00:49:51,950 --> 00:49:56,630
oh how it work

00:49:58,440 --> 00:50:07,260
no the finalizer is as far as I know

00:50:01,840 --> 00:50:10,780
actually a lot of am based and at least

00:50:07,260 --> 00:50:12,730
with the old runtime the previous you

00:50:10,780 --> 00:50:19,690
know before Wednesday have the previous

00:50:12,730 --> 00:50:22,780
one where I put uhto code htl has two

00:50:19,690 --> 00:50:24,970
representations one is textual and other

00:50:22,780 --> 00:50:27,700
is binary they are almost equivalent you

00:50:24,970 --> 00:50:29,980
can have comments in the binary binary

00:50:27,700 --> 00:50:33,130
representation and we put it into an elf

00:50:29,980 --> 00:50:37,960
section of its own what actually happens

00:50:33,130 --> 00:50:41,250
is that when you have yeah when the

00:50:37,960 --> 00:50:46,890
compiler produces some HS a colonel it

00:50:41,250 --> 00:50:48,910
also produces a static initialization

00:50:46,890 --> 00:50:51,190
constructor static instructor for the

00:50:48,910 --> 00:50:53,350
compilation unit where you have the

00:50:51,190 --> 00:50:56,470
colonel and that static instructor

00:50:53,350 --> 00:50:59,560
registers the brig the the H sale code

00:50:56,470 --> 00:51:06,910
with lip gob with the runtime support of

00:50:59,560 --> 00:51:09,010
for openmp when that compilation unit is

00:51:06,910 --> 00:51:13,270
loaded so either at application start or

00:51:09,010 --> 00:51:16,230
or when you open it dynamically and only

00:51:13,270 --> 00:51:20,650
then when you run for the first time a

00:51:16,230 --> 00:51:23,410
target pragma the CPU code will call in

00:51:20,650 --> 00:51:25,600
to lib gump lib gump will decide whether

00:51:23,410 --> 00:51:28,750
to paralyze or not if it decides to

00:51:25,600 --> 00:51:31,930
paralyze on HSA it will load an HSA plug

00:51:28,750 --> 00:51:35,620
in the plug-in will find Oh what does

00:51:31,930 --> 00:51:37,600
this code or where is it where is the

00:51:35,620 --> 00:51:40,180
implementation of this code for HSA it

00:51:37,600 --> 00:51:42,010
will finalize it will finalize the whole

00:51:40,180 --> 00:51:44,860
thing but then it also locate it and

00:51:42,010 --> 00:51:47,350
then it will use the HSA runtime

00:51:44,860 --> 00:51:49,890
provided by AMD to actually run the

00:51:47,350 --> 00:51:49,890
colonel

00:51:52,410 --> 00:51:57,250
input to the finalizer is brake yeah but

00:51:55,540 --> 00:52:00,600
that's really just a binary

00:51:57,250 --> 00:52:00,600
representation of HCl it

00:52:06,320 --> 00:52:19,760
yeah yeah basically yeah basically we do

00:52:15,860 --> 00:52:25,390
we clone at one point to in the

00:52:19,760 --> 00:52:28,730
compilation process of the bodies of

00:52:25,390 --> 00:52:30,920
almost all the of the target and

00:52:28,730 --> 00:52:33,890
parallel constructs at least are

00:52:30,920 --> 00:52:36,470
represented as they as a function of its

00:52:33,890 --> 00:52:39,080
own the code in the bar in the body is

00:52:36,470 --> 00:52:42,020
outlined to a special function and then

00:52:39,080 --> 00:52:43,820
that function is if you want to compile

00:52:42,020 --> 00:52:45,410
really the same stuff if we do not

00:52:43,820 --> 00:52:47,780
gratify for example that function is

00:52:45,410 --> 00:52:50,630
duplicated and then goes through the

00:52:47,780 --> 00:52:54,320
optimization pipeline with the special

00:52:50,630 --> 00:52:56,480
mark if it is for HSA GPU as this is HS

00:52:54,320 --> 00:52:59,230
a GPU thing for examples which are

00:52:56,480 --> 00:53:03,410
factorization but it doesn't switch off

00:52:59,230 --> 00:53:06,710
almost any other optimization no it

00:53:03,410 --> 00:53:10,099
doesn't and then such function is not

00:53:06,710 --> 00:53:15,930
even output into the

00:53:10,099 --> 00:53:19,049
x86 assembly it is only expanded at HCL

00:53:15,930 --> 00:53:20,999
code into the bricks action but it is

00:53:19,049 --> 00:53:23,160
linked with the CPU implementation

00:53:20,999 --> 00:53:24,959
because everything has a CPU

00:53:23,160 --> 00:53:28,430
implementation the CPU implementations

00:53:24,959 --> 00:53:28,430
always compile

00:53:34,530 --> 00:53:38,130
don't problem no problem

00:53:44,320 --> 00:53:46,320
I

00:54:01,680 --> 00:54:06,840
do you mean what why the other teams

00:54:04,180 --> 00:54:09,840
decided to have two compilers whether

00:54:06,840 --> 00:54:09,840
sure

00:54:18,220 --> 00:54:24,890
yeah I think that well we at sue said do

00:54:23,180 --> 00:54:28,660
not like this approach we think it is a

00:54:24,890 --> 00:54:32,000
mistake we think it they are misusing

00:54:28,660 --> 00:54:33,530
you know intermediate code streaming

00:54:32,000 --> 00:54:35,360
that was invented for a different

00:54:33,530 --> 00:54:37,910
purpose it was invented for link time

00:54:35,360 --> 00:54:41,390
optimization that is for example why

00:54:37,910 --> 00:54:46,040
they scream it at the same spot where

00:54:41,390 --> 00:54:48,620
lto does so we do not think that was a

00:54:46,040 --> 00:54:53,290
good idea nevertheless the first people

00:54:48,620 --> 00:54:56,540
who did it were in tell you had the team

00:54:53,290 --> 00:54:58,970
implementing the Intel's I and

00:54:56,540 --> 00:55:02,690
acceleration and for them this must have

00:54:58,970 --> 00:55:06,620
been the easiest thing because what they

00:55:02,690 --> 00:55:09,410
really wanted our to x86 64 compilers

00:55:06,620 --> 00:55:14,060
just parameterize differently different

00:55:09,410 --> 00:55:17,270
vector sizes different so yeah they saw

00:55:14,060 --> 00:55:20,540
this as the easiest option and and just

00:55:17,270 --> 00:55:23,990
that and for them I understand it makes

00:55:20,540 --> 00:55:26,090
sense for NVIDIA guys um yeah they just

00:55:23,990 --> 00:55:28,700
followed they just were till this is how

00:55:26,090 --> 00:55:31,400
this is being done and they just did it

00:55:28,700 --> 00:55:33,380
and and and it has certain advantages

00:55:31,400 --> 00:55:36,200
and the work has been done the streaming

00:55:33,380 --> 00:55:37,310
work has been done so so it might have

00:55:36,200 --> 00:55:41,450
been easier for them as well

00:55:37,310 --> 00:55:43,450
nevertheless when we had look at when we

00:55:41,450 --> 00:55:45,740
yeah when we had a look at what

00:55:43,450 --> 00:55:48,830
optimization passes would need to be

00:55:45,740 --> 00:55:51,980
repairmen tries drastically differently

00:55:48,830 --> 00:55:58,180
we didn't find that many it's vectorize

00:55:51,980 --> 00:56:01,520
ER because the HSA may not be able to it

00:55:58,180 --> 00:56:03,560
currently it only supports 128 bit

00:56:01,520 --> 00:56:06,470
vector instructions so if the CPU

00:56:03,560 --> 00:56:08,700
supported more they would have very big

00:56:06,470 --> 00:56:13,390
big problems

00:56:08,700 --> 00:56:16,509
dealing with that Gimple input and then

00:56:13,390 --> 00:56:21,210
of course the liner because we want to

00:56:16,509 --> 00:56:24,880
in line like crazy into GPU kernels

00:56:21,210 --> 00:56:27,160
unlike into CPU functions because if you

00:56:24,880 --> 00:56:29,829
in line like in life in line like crazy

00:56:27,160 --> 00:56:33,130
and try to link time optimize Firefox

00:56:29,829 --> 00:56:37,150
your memory ram you will run out very

00:56:33,130 --> 00:56:38,680
very quickly for example and those

00:56:37,150 --> 00:56:39,910
that's really it those are the two

00:56:38,680 --> 00:56:43,539
important passes that need to be

00:56:39,910 --> 00:56:46,359
repaired but rised I mean that every the

00:56:43,539 --> 00:56:48,069
derp bra there is a few other passes

00:56:46,359 --> 00:56:51,490
that use one or two parameters but

00:56:48,069 --> 00:56:53,700
that's not really step crucial thank you

00:56:51,490 --> 00:56:53,700

YouTube URL: https://www.youtube.com/watch?v=zVpqtrpJJSs


