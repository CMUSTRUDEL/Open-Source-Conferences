Title: Webinar: Mastering OpenMP Performance
Publication date: 2021-03-18
Playlist: Webinars
Description: 
	It is frustrating to parallelize your code and not get much of a performance benefit from using multiple cores. Parallel processing always comes with an additional cost over the execution of the sequential version of the program and the OpenMP API is no exception to this. If the language constructs and features are not used wisely, this overhead negatively impacts the performance gain realized using multiple cores– and this may even degrade the performance.

Join us for a webinar about how to tune your OpenMP API code to get the best performance. This presentation will be delivered by Ruud van der Pas, Distinguished Engineer in the SPARC Processor Organization at Oracle. In his presentation, Ruud will give guidelines on how to get the best performance using the OpenMP API. Several real-world examples will be shown and discussed, and he will also be available to answer your questions.

Ruud has significant experience working with the OpenMP API and participates heavily in the OpenMP Language Committee. He speaks regularly about OpenMP performance: check out his presentation from SC’20.
Captions: 
	00:00:05,920 --> 00:00:09,519
thank you everyone for joining us today

00:00:07,839 --> 00:00:12,480
for the openmp webinar

00:00:09,519 --> 00:00:15,120
mastering openmp performance this

00:00:12,480 --> 00:00:16,560
presentation will be one hour long

00:00:15,120 --> 00:00:18,800
should you have any questions during the

00:00:16,560 --> 00:00:21,600
presentation please put them in the q

00:00:18,800 --> 00:00:22,960
a feature located on your zoom toolbar

00:00:21,600 --> 00:00:25,279
you will be able to

00:00:22,960 --> 00:00:27,359
see questions from other attendees so

00:00:25,279 --> 00:00:29,039
please feel free to upvote or like the

00:00:27,359 --> 00:00:30,400
questions that interest you

00:00:29,039 --> 00:00:32,239
this will help us determine which

00:00:30,400 --> 00:00:33,760
questions to address live at the end of

00:00:32,239 --> 00:00:35,520
the session

00:00:33,760 --> 00:00:37,280
any questions that weren't answered will

00:00:35,520 --> 00:00:39,200
be addressed post event

00:00:37,280 --> 00:00:40,480
please watch for an email tomorrow with

00:00:39,200 --> 00:00:43,280
further information

00:00:40,480 --> 00:00:45,039
and a link to the recorded presentation

00:00:43,280 --> 00:00:46,000
and now i'd like to turn it over to rude

00:00:45,039 --> 00:00:49,039
vanderpost

00:00:46,000 --> 00:00:51,360
rude hi lizzy thank you

00:00:49,039 --> 00:00:53,680
well welcome to this webinar i have an

00:00:51,360 --> 00:00:55,199
awful amount of slides to show to push

00:00:53,680 --> 00:00:59,840
through in the hours so

00:00:55,199 --> 00:00:59,840
let's let's get started

00:01:00,960 --> 00:01:05,040
all right a little bit about myself my

00:01:03,120 --> 00:01:07,040
background is in mathematics and physics

00:01:05,040 --> 00:01:09,600
i worked at several companies i started

00:01:07,040 --> 00:01:11,840
my career at the university of utrecht

00:01:09,600 --> 00:01:13,200
and currently i'm at oracle i work in

00:01:11,840 --> 00:01:14,640
the oracle linux engineering

00:01:13,200 --> 00:01:16,479
organization

00:01:14,640 --> 00:01:18,479
but this is this talk is all about

00:01:16,479 --> 00:01:19,520
openmp and i've been involved with

00:01:18,479 --> 00:01:21,920
openmp

00:01:19,520 --> 00:01:24,880
since day one i was at sdi at the time

00:01:21,920 --> 00:01:27,680
and i thought that's really interesting

00:01:24,880 --> 00:01:28,960
and um and i got hooked and uh i didn't

00:01:27,680 --> 00:01:31,759
leave since then

00:01:28,960 --> 00:01:33,600
so um i'm very passionate about anything

00:01:31,759 --> 00:01:35,840
related to application performance

00:01:33,600 --> 00:01:38,720
especially of course the technical

00:01:35,840 --> 00:01:39,439
hpc type of applications and openmp in

00:01:38,720 --> 00:01:41,759
particular

00:01:39,439 --> 00:01:44,399
and today i hope to share some of my

00:01:41,759 --> 00:01:46,720
performance experiences with you

00:01:44,399 --> 00:01:48,479
before i do that just a little bit of

00:01:46,720 --> 00:01:50,159
general information i'm sure you all

00:01:48,479 --> 00:01:53,520
know this website and if

00:01:50,159 --> 00:01:55,360
if you don't please bookmark this www

00:01:53,520 --> 00:01:56,960
openmp.org that's your place to go

00:01:55,360 --> 00:01:58,960
that's your one-stop place to go for

00:01:56,960 --> 00:02:01,920
anything related to openmp

00:01:58,960 --> 00:02:03,920
for example the um the specs the 5.1

00:02:01,920 --> 00:02:07,520
specs that were released in

00:02:03,920 --> 00:02:09,119
november you can find them online if you

00:02:07,520 --> 00:02:10,720
if you really like paperwork you can

00:02:09,119 --> 00:02:13,680
also order your copy at

00:02:10,720 --> 00:02:15,760
amazon it's sold i think at cost but be

00:02:13,680 --> 00:02:17,520
aware it's a 1.7 kilo

00:02:15,760 --> 00:02:18,879
heavy document so you don't want to

00:02:17,520 --> 00:02:22,080
carry that on your travel

00:02:18,879 --> 00:02:22,879
i think all right just in case you want

00:02:22,080 --> 00:02:26,560
to have some uh

00:02:22,879 --> 00:02:28,160
some food to read um this is an older

00:02:26,560 --> 00:02:30,319
book that i was involved with i

00:02:28,160 --> 00:02:32,239
i still think it's uh quite useful if

00:02:30,319 --> 00:02:34,720
you're new to parallel computing it

00:02:32,239 --> 00:02:36,319
doesn't only cover openmp it talks about

00:02:34,720 --> 00:02:37,519
other things related to parallel

00:02:36,319 --> 00:02:40,000
computing

00:02:37,519 --> 00:02:41,840
but it doesn't go very far as in as far

00:02:40,000 --> 00:02:43,280
as openmp goes

00:02:41,840 --> 00:02:45,360
if you really want to get into the

00:02:43,280 --> 00:02:46,879
openmp basics i can highly recommend

00:02:45,360 --> 00:02:49,200
this book

00:02:46,879 --> 00:02:50,800
the openmp common core that's really a

00:02:49,200 --> 00:02:52,080
really good way to get started with

00:02:50,800 --> 00:02:54,480
openmp

00:02:52,080 --> 00:02:56,080
once you have some more experiences

00:02:54,480 --> 00:02:58,319
experience and you have some more

00:02:56,080 --> 00:03:01,200
questions about features you may want to

00:02:58,319 --> 00:03:01,519
want to look at this book the next step

00:03:01,200 --> 00:03:04,560
it

00:03:01,519 --> 00:03:07,280
it covers 4.5 so one question i often

00:03:04,560 --> 00:03:09,519
get well how about 5.5.1

00:03:07,280 --> 00:03:11,440
well my answer is very simple if you if

00:03:09,519 --> 00:03:12,239
you don't understand 4.5 don't even

00:03:11,440 --> 00:03:15,040
think about

00:03:12,239 --> 00:03:17,200
going further so this book will get you

00:03:15,040 --> 00:03:20,239
up to speed on pretty much

00:03:17,200 --> 00:03:22,400
sort of the modern openmp

00:03:20,239 --> 00:03:24,799
in this first part um i want to talk

00:03:22,400 --> 00:03:25,599
about a sort of random collection of

00:03:24,799 --> 00:03:28,080
tips and tricks

00:03:25,599 --> 00:03:30,159
things that i picked up along the way i

00:03:28,080 --> 00:03:32,159
i try to make it very practical

00:03:30,159 --> 00:03:33,200
it's pretty much all based on real life

00:03:32,159 --> 00:03:35,920
experiences

00:03:33,200 --> 00:03:37,440
but i do try to to kind of strip it down

00:03:35,920 --> 00:03:40,080
to the bare essentials and

00:03:37,440 --> 00:03:40,720
zoom in on the pain point so the

00:03:40,080 --> 00:03:42,640
examples

00:03:40,720 --> 00:03:44,400
um that i show are often quite simple

00:03:42,640 --> 00:03:45,519
but keep keep in mind that they're

00:03:44,400 --> 00:03:47,840
derived from

00:03:45,519 --> 00:03:49,519
usually a much more complex environment

00:03:47,840 --> 00:03:51,280
in the second part i'm i'm going to

00:03:49,519 --> 00:03:55,360
touch upon that somewhat more

00:03:51,280 --> 00:03:57,360
about the complexity of of the tuning

00:03:55,360 --> 00:03:59,360
first um i want to go through what i

00:03:57,360 --> 00:04:02,480
call my frustration slide

00:03:59,360 --> 00:04:05,040
there's a persistent myth that openmp

00:04:02,480 --> 00:04:06,400
does not scale at conferences people

00:04:05,040 --> 00:04:10,319
still claim that

00:04:06,400 --> 00:04:12,080
i see horrendous claims made and

00:04:10,319 --> 00:04:13,360
it's very persistent so i keep on

00:04:12,080 --> 00:04:15,599
talking about that

00:04:13,360 --> 00:04:16,880
because when you think about it openmp

00:04:15,599 --> 00:04:19,120
is a programming model

00:04:16,880 --> 00:04:20,160
uh programming model is is like your

00:04:19,120 --> 00:04:23,360
cookbook

00:04:20,160 --> 00:04:26,720
um you you have your

00:04:23,360 --> 00:04:28,880
your ingredients and with that you're

00:04:26,720 --> 00:04:31,759
going to make your recipes

00:04:28,880 --> 00:04:33,520
so if you if you blame the ingredients

00:04:31,759 --> 00:04:35,040
for failing your dish i think something

00:04:33,520 --> 00:04:37,360
is wrong but that's exactly

00:04:35,040 --> 00:04:38,639
what you do when people say open b

00:04:37,360 --> 00:04:40,479
doesn't scale

00:04:38,639 --> 00:04:42,000
of course that could be things that may

00:04:40,479 --> 00:04:45,040
impact scalability of course there can

00:04:42,000 --> 00:04:46,639
be very poor performance for example

00:04:45,040 --> 00:04:49,120
it could be that the compiler you're

00:04:46,639 --> 00:04:49,759
using or library or some other software

00:04:49,120 --> 00:04:52,639
has

00:04:49,759 --> 00:04:53,440
has a glitch and unfortunately you get

00:04:52,639 --> 00:04:55,440
hurt by that

00:04:53,440 --> 00:04:57,040
and and you'll see less than optimal

00:04:55,440 --> 00:04:58,000
performance or even worse that can

00:04:57,040 --> 00:05:00,639
happen

00:04:58,000 --> 00:05:02,160
or there could be what i call a mismatch

00:05:00,639 --> 00:05:03,360
between the system and the resource

00:05:02,160 --> 00:05:06,160
requirements

00:05:03,360 --> 00:05:08,000
a simple example is if your application

00:05:06,160 --> 00:05:10,960
requires a lot of bandwidth and you

00:05:08,000 --> 00:05:11,440
start adding threads how many threads

00:05:10,960 --> 00:05:14,160
can

00:05:11,440 --> 00:05:14,479
can the hardware actually sustain so if

00:05:14,160 --> 00:05:16,320
if

00:05:14,479 --> 00:05:18,160
that's not sufficient if the bandwidth

00:05:16,320 --> 00:05:19,759
is not sufficient you can start adding

00:05:18,160 --> 00:05:21,199
threads at some point and you won't get

00:05:19,759 --> 00:05:21,919
any benefit or maybe even the

00:05:21,199 --> 00:05:25,039
performance

00:05:21,919 --> 00:05:26,080
degrades because of contention so these

00:05:25,039 --> 00:05:28,800
things can all happen

00:05:26,080 --> 00:05:30,160
but it's not related to openmp the

00:05:28,800 --> 00:05:33,199
biggest

00:05:30,160 --> 00:05:35,840
reason why openmp applications don't

00:05:33,199 --> 00:05:39,199
scale is often that i find it's the user

00:05:35,840 --> 00:05:40,400
you did something wrong and and you

00:05:39,199 --> 00:05:42,800
didn't know it was wrong

00:05:40,400 --> 00:05:43,919
um nobody told you are you not aware

00:05:42,800 --> 00:05:44,960
there are a lot of things happening

00:05:43,919 --> 00:05:47,520
under the hood

00:05:44,960 --> 00:05:49,680
uh hidden to you uh so this is not about

00:05:47,520 --> 00:05:52,080
blaming people but it's about

00:05:49,680 --> 00:05:52,800
seeing that people write sort of bad

00:05:52,080 --> 00:05:54,800
openmp

00:05:52,800 --> 00:05:57,120
and what i hope to do in this talk this

00:05:54,800 --> 00:05:59,280
is a fairly basic introductory talk

00:05:57,120 --> 00:06:00,720
is give you some guidelines how to avoid

00:05:59,280 --> 00:06:04,000
the most common

00:06:00,720 --> 00:06:06,800
pitfalls so we're going to talk about

00:06:04,000 --> 00:06:09,039
how to get good performance the basics

00:06:06,800 --> 00:06:10,400
and my claim is if you follow these

00:06:09,039 --> 00:06:12,720
guidelines you should expect

00:06:10,400 --> 00:06:14,479
decent performance and what do i mean

00:06:12,720 --> 00:06:17,120
with that you get good performance out

00:06:14,479 --> 00:06:18,080
of the box so an openmp compiler and the

00:06:17,120 --> 00:06:19,600
runtime system

00:06:18,080 --> 00:06:22,639
should do the right thing for you it

00:06:19,600 --> 00:06:24,400
should do what you expect it to do

00:06:22,639 --> 00:06:26,560
you probably won't get blazing

00:06:24,400 --> 00:06:29,360
scalability to thousands of course

00:06:26,560 --> 00:06:30,000
but um as if you would ever go have to

00:06:29,360 --> 00:06:32,720
go to

00:06:30,000 --> 00:06:34,720
the openmp performance court the lawyers

00:06:32,720 --> 00:06:36,560
there will have no case against you you

00:06:34,720 --> 00:06:38,319
did everything according to the book

00:06:36,560 --> 00:06:40,000
so again what i'm going to talk about

00:06:38,319 --> 00:06:42,000
will definitely get you going

00:06:40,000 --> 00:06:44,000
and improve the scalability of your

00:06:42,000 --> 00:06:45,680
application

00:06:44,000 --> 00:06:47,919
now it all starts with the ease of use

00:06:45,680 --> 00:06:49,440
of openmp that's a mixed blessing i

00:06:47,919 --> 00:06:49,919
really like it i think it's a great

00:06:49,440 --> 00:06:53,520
choice

00:06:49,919 --> 00:06:57,199
but with the choice comes the downsides

00:06:53,520 --> 00:06:59,520
on the pros pro side what i like is that

00:06:57,199 --> 00:07:00,639
any idea i have is very easy and quick

00:06:59,520 --> 00:07:02,560
to implement i got an

00:07:00,639 --> 00:07:03,840
id and trying it out with openmp is

00:07:02,560 --> 00:07:06,800
usually not that hard

00:07:03,840 --> 00:07:07,280
that that's really really good it gets

00:07:06,800 --> 00:07:09,759
you

00:07:07,280 --> 00:07:10,479
to your solution much quicker but you

00:07:09,759 --> 00:07:12,319
need to know

00:07:10,479 --> 00:07:13,599
that some constructs are more expensive

00:07:12,319 --> 00:07:15,680
than others

00:07:13,599 --> 00:07:17,440
and again if nobody told you how can you

00:07:15,680 --> 00:07:20,319
how can you know so that's the big

00:07:17,440 --> 00:07:22,880
big part of this talk of course um

00:07:20,319 --> 00:07:25,599
another thing is if you write dumb code

00:07:22,880 --> 00:07:26,080
you probably get down performance

00:07:25,599 --> 00:07:27,520
actually

00:07:26,080 --> 00:07:29,199
i'm going to make that stronger you will

00:07:27,520 --> 00:07:30,000
get done performance so why do i say

00:07:29,199 --> 00:07:31,919
that and when i say

00:07:30,000 --> 00:07:34,000
dumb code that's at the openmp level if

00:07:31,919 --> 00:07:36,080
you write inefficient openmp

00:07:34,000 --> 00:07:37,440
there's often very little the compiler

00:07:36,080 --> 00:07:40,160
can do to fix it

00:07:37,440 --> 00:07:42,479
we all sort of start being spoiled by

00:07:40,160 --> 00:07:44,080
very smart compilers that fix up

00:07:42,479 --> 00:07:46,240
things that are not optimal for

00:07:44,080 --> 00:07:46,960
performance they just make it go better

00:07:46,240 --> 00:07:48,960
for us

00:07:46,960 --> 00:07:51,599
at the openmp level that's much much

00:07:48,960 --> 00:07:54,479
harder that's almost a talk in itself

00:07:51,599 --> 00:07:54,960
but be aware um you tell openmp to do

00:07:54,479 --> 00:07:57,599
something

00:07:54,960 --> 00:07:59,680
and it will do that you say run this

00:07:57,599 --> 00:08:01,360
loop in parallel it will run that loop

00:07:59,680 --> 00:08:04,000
in parallel it won't question you

00:08:01,360 --> 00:08:05,199
luckily but that means that the burden

00:08:04,000 --> 00:08:07,440
is on you to make

00:08:05,199 --> 00:08:09,039
to make sure that makes sense to do and

00:08:07,440 --> 00:08:10,720
do that in the right way

00:08:09,039 --> 00:08:12,800
so that's another thing that i quite

00:08:10,720 --> 00:08:13,440
often see people don't write efficient

00:08:12,800 --> 00:08:16,319
openmp

00:08:13,440 --> 00:08:16,720
and there you go the one thing is don't

00:08:16,319 --> 00:08:19,360
blow

00:08:16,720 --> 00:08:20,400
blame openmp um go back to the previous

00:08:19,360 --> 00:08:22,960
slide

00:08:20,400 --> 00:08:24,560
openmp just gives you gives you the

00:08:22,960 --> 00:08:26,319
ingredients and it's up to you to make

00:08:24,560 --> 00:08:28,400
your own dish out of it and whether

00:08:26,319 --> 00:08:30,080
that's taste a tasty dish or not that's

00:08:28,400 --> 00:08:33,279
on you

00:08:30,080 --> 00:08:36,800
so let's get uh specific some

00:08:33,279 --> 00:08:38,560
guidelines how to not write dumb code

00:08:36,800 --> 00:08:40,399
well it all starts with single clap

00:08:38,560 --> 00:08:41,120
performance and and that's one of those

00:08:40,399 --> 00:08:43,760
things that

00:08:41,120 --> 00:08:45,360
few people talk about but when you start

00:08:43,760 --> 00:08:47,760
parallelizing your code there's

00:08:45,360 --> 00:08:49,839
some homework you need to do and part of

00:08:47,760 --> 00:08:52,160
that is that you have to pay attention

00:08:49,839 --> 00:08:54,880
to single click performance

00:08:52,160 --> 00:08:55,920
why well if your code performs badly on

00:08:54,880 --> 00:08:57,519
a single core

00:08:55,920 --> 00:08:59,440
what do you think will happen or 10 or

00:08:57,519 --> 00:09:01,200
20 or even more

00:08:59,440 --> 00:09:02,800
you think it'll go better some magical

00:09:01,200 --> 00:09:04,959
way no it won't

00:09:02,800 --> 00:09:06,640
um in my experience if the performance

00:09:04,959 --> 00:09:09,040
is poor on a single core

00:09:06,640 --> 00:09:11,200
scalability gets to be much much harder

00:09:09,040 --> 00:09:12,320
so spend some time on improving single

00:09:11,200 --> 00:09:13,920
thread performance

00:09:12,320 --> 00:09:15,120
how much time you want to spend on that

00:09:13,920 --> 00:09:16,560
and what you're going to do that's

00:09:15,120 --> 00:09:19,279
completely up to you

00:09:16,560 --> 00:09:21,200
but remember if the if the program

00:09:19,279 --> 00:09:24,080
performs poorly on a single core

00:09:21,200 --> 00:09:25,279
parallelization may be harder to to get

00:09:24,080 --> 00:09:26,800
efficient

00:09:25,279 --> 00:09:29,440
and that's the thing that can be very

00:09:26,800 --> 00:09:30,320
sneaky that your code may actually scale

00:09:29,440 --> 00:09:33,519
well

00:09:30,320 --> 00:09:34,959
um because of course poor poor

00:09:33,519 --> 00:09:37,839
performance

00:09:34,959 --> 00:09:39,519
masks scalability issues if your code is

00:09:37,839 --> 00:09:41,440
so slow

00:09:39,519 --> 00:09:43,279
nothing else matters so some extra

00:09:41,440 --> 00:09:45,760
openmp overhead won't matter

00:09:43,279 --> 00:09:48,000
so what we see in practice is is that

00:09:45,760 --> 00:09:49,440
the slow code tends to scale well

00:09:48,000 --> 00:09:51,279
but at the end of the day it's still

00:09:49,440 --> 00:09:53,279
often slower than a more efficient

00:09:51,279 --> 00:09:56,560
algorithm for example

00:09:53,279 --> 00:09:59,040
right rule number one sounds so

00:09:56,560 --> 00:10:00,800
so simple so obvious don't parallelize

00:09:59,040 --> 00:10:02,399
what does not matter but

00:10:00,800 --> 00:10:04,240
it wouldn't be the first time that i see

00:10:02,399 --> 00:10:05,839
somebody who's sort of parallelized

00:10:04,240 --> 00:10:07,600
everything in their code

00:10:05,839 --> 00:10:08,880
and guess what they also parallelized

00:10:07,600 --> 00:10:10,560
what was not relevant

00:10:08,880 --> 00:10:12,240
and actually may be inefficient when

00:10:10,560 --> 00:10:14,160
executing in parallel

00:10:12,240 --> 00:10:15,839
so that's not a good strategy it not

00:10:14,160 --> 00:10:17,040
only saves you time it can actually hurt

00:10:15,839 --> 00:10:20,079
performance

00:10:17,040 --> 00:10:21,920
so always use a profiling tool pick

00:10:20,079 --> 00:10:24,079
whatever tool you like i don't care but

00:10:21,920 --> 00:10:25,600
use a profiling tool to to guide you

00:10:24,079 --> 00:10:26,720
where the time is spent

00:10:25,600 --> 00:10:28,399
and what you do you're going to

00:10:26,720 --> 00:10:28,959
parallelize where most of the time is

00:10:28,399 --> 00:10:31,279
spent

00:10:28,959 --> 00:10:32,079
again sounds very obvious in too many

00:10:31,279 --> 00:10:34,640
cases

00:10:32,079 --> 00:10:35,680
i don't see that happening another

00:10:34,640 --> 00:10:37,600
golden rule is

00:10:35,680 --> 00:10:39,360
don't share data unless you have to

00:10:37,600 --> 00:10:41,040
sharing data in openmp is really

00:10:39,360 --> 00:10:43,200
wonderful i really like it

00:10:41,040 --> 00:10:44,880
but it can hurt and i'll talk about that

00:10:43,200 --> 00:10:47,600
later on in this talk

00:10:44,880 --> 00:10:48,399
there are some cases where sharing data

00:10:47,600 --> 00:10:50,320
can actually

00:10:48,399 --> 00:10:51,760
cost you performance so the

00:10:50,320 --> 00:10:54,240
recommendation is

00:10:51,760 --> 00:10:56,320
use private data as much as possible and

00:10:54,240 --> 00:10:57,920
only share when you have to

00:10:56,320 --> 00:10:59,839
and when you do that you already have

00:10:57,920 --> 00:11:02,959
come a long way in getting good openmp

00:10:59,839 --> 00:11:05,279
performance another thing and i'll

00:11:02,959 --> 00:11:06,079
zoom in on that in a minute what i call

00:11:05,279 --> 00:11:08,640
think big

00:11:06,079 --> 00:11:10,079
maximize the size of the parallel region

00:11:08,640 --> 00:11:12,320
a parallel region is

00:11:10,079 --> 00:11:14,000
inherently expensive it's a lot of

00:11:12,320 --> 00:11:15,200
things happening that you don't see but

00:11:14,000 --> 00:11:17,680
that need to be done

00:11:15,200 --> 00:11:19,760
so there's a there's a cost associated

00:11:17,680 --> 00:11:21,519
with executing a parallel region

00:11:19,760 --> 00:11:23,360
the more parallel regions you have the

00:11:21,519 --> 00:11:24,079
more of that cost which is purely

00:11:23,360 --> 00:11:26,959
overhead

00:11:24,079 --> 00:11:28,240
will go on to your bill so try to put as

00:11:26,959 --> 00:11:31,040
much work as possible

00:11:28,240 --> 00:11:32,000
inside the parallel region and when you

00:11:31,040 --> 00:11:34,000
do that you may

00:11:32,000 --> 00:11:35,279
make good use of constructs like single

00:11:34,000 --> 00:11:37,360
or master

00:11:35,279 --> 00:11:39,200
where one thread will do the work only

00:11:37,360 --> 00:11:39,839
while the other threads are waiting so

00:11:39,200 --> 00:11:41,600
you can put

00:11:39,839 --> 00:11:45,360
sequential work inside the parallel

00:11:41,600 --> 00:11:47,600
region using those kind of constructs

00:11:45,360 --> 00:11:49,519
another thing is minimize the number of

00:11:47,600 --> 00:11:50,639
times a parallel region is encountered

00:11:49,519 --> 00:11:52,639
but don't bury

00:11:50,639 --> 00:11:54,800
a parallel region deep inside the

00:11:52,639 --> 00:11:56,560
algorithm in like a deeply nested loop

00:11:54,800 --> 00:11:57,600
so that each time you hit that parallel

00:11:56,560 --> 00:12:00,399
region

00:11:57,600 --> 00:12:01,200
you hit the overhead over and over again

00:12:00,399 --> 00:12:04,240
try to push

00:12:01,200 --> 00:12:05,040
the parallel region outward and have it

00:12:04,240 --> 00:12:08,399
executed

00:12:05,040 --> 00:12:10,399
as little as possible

00:12:08,399 --> 00:12:11,760
to get a little more specific on that

00:12:10,399 --> 00:12:14,880
one parallel four

00:12:11,760 --> 00:12:16,240
parallel do is fine multiple back to

00:12:14,880 --> 00:12:19,440
back is pure evil

00:12:16,240 --> 00:12:20,880
and let me show you an example

00:12:19,440 --> 00:12:22,959
this is what i see in too many

00:12:20,880 --> 00:12:24,800
applications and again it's not about

00:12:22,959 --> 00:12:26,160
blaming people if nobody ever told you

00:12:24,800 --> 00:12:28,959
how could you know

00:12:26,160 --> 00:12:31,200
but let's zoom in on this we see a bunch

00:12:28,959 --> 00:12:33,120
of for loops or do loops in fortran

00:12:31,200 --> 00:12:34,480
and each loop has been parallelized with

00:12:33,120 --> 00:12:35,839
the parallel four

00:12:34,480 --> 00:12:38,240
now there's a little thing with the

00:12:35,839 --> 00:12:40,000
parallel four the parallel 4 is a fully

00:12:38,240 --> 00:12:41,920
fledged parallel region

00:12:40,000 --> 00:12:43,600
so what we have we have n parallel

00:12:41,920 --> 00:12:44,639
regions here with all the cost

00:12:43,600 --> 00:12:48,079
associated

00:12:44,639 --> 00:12:50,639
so that's bad um but what's

00:12:48,079 --> 00:12:51,839
maybe even worse is i can't use the no

00:12:50,639 --> 00:12:53,519
weight clause

00:12:51,839 --> 00:12:55,920
as you probably know the no weight

00:12:53,519 --> 00:12:58,320
clause leaves out the implied barrier

00:12:55,920 --> 00:12:59,360
at the end of a construct it's a really

00:12:58,320 --> 00:13:01,839
really nice

00:12:59,360 --> 00:13:02,880
feature i like it a lot i use it a lot

00:13:01,839 --> 00:13:04,880
but

00:13:02,880 --> 00:13:06,079
the parallel region for good reasons has

00:13:04,880 --> 00:13:08,800
an implied

00:13:06,079 --> 00:13:09,279
barrier at the end and you can't get rid

00:13:08,800 --> 00:13:11,600
of it

00:13:09,279 --> 00:13:13,519
there's no no no wait for the power

00:13:11,600 --> 00:13:16,000
region again that makes sense

00:13:13,519 --> 00:13:18,320
but it is expensive so this is not the

00:13:16,000 --> 00:13:21,519
right way to do this kind of

00:13:18,320 --> 00:13:23,200
coding what i would recommend is create

00:13:21,519 --> 00:13:24,800
one parallel region

00:13:23,200 --> 00:13:26,880
that's the pragma omp parallel at the

00:13:24,800 --> 00:13:30,320
top line and inside

00:13:26,880 --> 00:13:30,800
use the pragma op4 and with that you can

00:13:30,320 --> 00:13:33,920
just

00:13:30,800 --> 00:13:35,760
parallelize the individual loops and and

00:13:33,920 --> 00:13:38,720
in few places there may be

00:13:35,760 --> 00:13:40,639
maybe a room to use the no weight clause

00:13:38,720 --> 00:13:42,560
in particular the last one

00:13:40,639 --> 00:13:43,760
on the end of the parallel region has an

00:13:42,560 --> 00:13:45,839
implied barrier

00:13:43,760 --> 00:13:47,680
we know that you can't get rid of it so

00:13:45,839 --> 00:13:49,120
that means that if i wouldn't put a no

00:13:47,680 --> 00:13:51,040
weight on the last loop

00:13:49,120 --> 00:13:52,880
i would have a back-to-back barrier now

00:13:51,040 --> 00:13:54,800
admittedly that second barrier is going

00:13:52,880 --> 00:13:55,519
to be fast but it will still take your

00:13:54,800 --> 00:13:57,440
time

00:13:55,519 --> 00:13:59,120
and then think about scalability as you

00:13:57,440 --> 00:14:01,440
go to higher threat counts

00:13:59,120 --> 00:14:02,959
those costs start to add up so that's

00:14:01,440 --> 00:14:05,680
not a good idea

00:14:02,959 --> 00:14:06,959
uh one little warning um i've seen

00:14:05,680 --> 00:14:08,480
people do this and then they've got

00:14:06,959 --> 00:14:10,079
surprised because what they would do

00:14:08,480 --> 00:14:12,720
they would do the pragma omb

00:14:10,079 --> 00:14:13,279
parallel for inside the parallel region

00:14:12,720 --> 00:14:15,199
that's called

00:14:13,279 --> 00:14:17,519
nested parallelism and that's not a good

00:14:15,199 --> 00:14:20,160
idea certainly not in this context

00:14:17,519 --> 00:14:21,279
so um use the pragma omp4 not the

00:14:20,160 --> 00:14:23,440
parallel four

00:14:21,279 --> 00:14:24,560
and you'll see uh you know this will be

00:14:23,440 --> 00:14:26,639
much more efficient

00:14:24,560 --> 00:14:28,240
you have less much less overhead you

00:14:26,639 --> 00:14:30,079
have only have one parallel region and

00:14:28,240 --> 00:14:33,279
again you can fine tune the code

00:14:30,079 --> 00:14:35,839
using the no weight clause

00:14:33,279 --> 00:14:37,519
so um one of the basic things is once

00:14:35,839 --> 00:14:38,399
your code is running and you get the

00:14:37,519 --> 00:14:40,000
right results

00:14:38,399 --> 00:14:42,399
start looking for where you might use

00:14:40,000 --> 00:14:43,760
the no weight clause it's very powerful

00:14:42,399 --> 00:14:45,440
but of course

00:14:43,760 --> 00:14:47,199
you know the the challenge is on you to

00:14:45,440 --> 00:14:47,839
do that in the right way if you make a

00:14:47,199 --> 00:14:51,360
mistake

00:14:47,839 --> 00:14:53,279
and the no weight no clause is not um

00:14:51,360 --> 00:14:55,199
not allowed to be used you'll probably

00:14:53,279 --> 00:14:56,560
get a data rate so be careful

00:14:55,199 --> 00:14:59,040
but again keep in mind it's a very

00:14:56,560 --> 00:15:00,480
powerful feature another thing that

00:14:59,040 --> 00:15:03,279
people often overlook

00:15:00,480 --> 00:15:05,680
is the schedule clause if if not all

00:15:03,279 --> 00:15:07,839
threads do the same amount of work

00:15:05,680 --> 00:15:10,000
we call that load balance low balancing

00:15:07,839 --> 00:15:13,279
issues and then the schedule clause can

00:15:10,000 --> 00:15:15,040
be very nice to help help out with that

00:15:13,279 --> 00:15:17,199
with all of this again and i'll say that

00:15:15,040 --> 00:15:18,560
over and over again use a profiling tool

00:15:17,199 --> 00:15:21,040
to guide you

00:15:18,560 --> 00:15:22,720
how what where should you look for an

00:15:21,040 --> 00:15:24,079
opportunity for the no wait well the

00:15:22,720 --> 00:15:25,760
profiling tool will tell you

00:15:24,079 --> 00:15:27,040
that's where you spend most of your time

00:15:25,760 --> 00:15:30,079
that's why you start looking for an

00:15:27,040 --> 00:15:32,000
opportunity to use the norway

00:15:30,079 --> 00:15:34,320
i want to show an example i wanted to do

00:15:32,000 --> 00:15:36,639
that as early on as possible

00:15:34,320 --> 00:15:37,839
and this is taken from a graph analysis

00:15:36,639 --> 00:15:40,079
benchmark code

00:15:37,839 --> 00:15:40,880
and what i'm showing here is um is the

00:15:40,079 --> 00:15:43,920
timeline

00:15:40,880 --> 00:15:44,720
that's one of the tools that i use and

00:15:43,920 --> 00:15:46,880
uh what it

00:15:44,720 --> 00:15:48,959
what it shows is is a color-coded

00:15:46,880 --> 00:15:50,000
representation of the execution of your

00:15:48,959 --> 00:15:52,959
program

00:15:50,000 --> 00:15:54,880
that very green top bar is the operating

00:15:52,959 --> 00:15:57,680
system view on the application

00:15:54,880 --> 00:15:58,000
and essentially green means good news so

00:15:57,680 --> 00:15:59,600
the

00:15:58,000 --> 00:16:01,680
the operating system level this

00:15:59,600 --> 00:16:04,560
application is running running fine

00:16:01,680 --> 00:16:05,040
time is from left to right and what we

00:16:04,560 --> 00:16:08,160
see

00:16:05,040 --> 00:16:09,120
is what we show here is the call stack

00:16:08,160 --> 00:16:12,320
of the program

00:16:09,120 --> 00:16:14,079
um at certain intervals and

00:16:12,320 --> 00:16:15,920
that's kind of like that hairy part in

00:16:14,079 --> 00:16:17,440
the beginning that's the initialization

00:16:15,920 --> 00:16:20,399
phase of the code

00:16:17,440 --> 00:16:21,600
and where the graph is being generated

00:16:20,399 --> 00:16:24,720
then we see

00:16:21,600 --> 00:16:27,279
all those wild spikes that's a recursive

00:16:24,720 --> 00:16:28,320
sorting function being called there so

00:16:27,279 --> 00:16:30,800
by looking at

00:16:28,320 --> 00:16:32,639
this this kind of chart you can get a

00:16:30,800 --> 00:16:33,759
feel for the dynamic behavior of the

00:16:32,639 --> 00:16:36,000
program

00:16:33,759 --> 00:16:37,839
and what we see here is the first part

00:16:36,000 --> 00:16:39,839
is the graph generation

00:16:37,839 --> 00:16:40,959
and then this program it's a benchmark

00:16:39,839 --> 00:16:44,000
it will do

00:16:40,959 --> 00:16:47,519
a bfs search and verify the result

00:16:44,000 --> 00:16:48,880
for 64 random keys and i use the color

00:16:47,519 --> 00:16:51,040
highlighting to

00:16:48,880 --> 00:16:52,800
gray out everything i don't care about

00:16:51,040 --> 00:16:54,480
so that's why you only see a few colors

00:16:52,800 --> 00:16:56,800
here for the call stacks

00:16:54,480 --> 00:16:58,560
and what we see is the alternating red

00:16:56,800 --> 00:17:00,079
and blue

00:16:58,560 --> 00:17:02,000
which means it is a search a

00:17:00,079 --> 00:17:02,959
verification that the result is valid

00:17:02,000 --> 00:17:05,360
another search

00:17:02,959 --> 00:17:06,079
another verification and so forth 64

00:17:05,360 --> 00:17:08,160
times

00:17:06,079 --> 00:17:09,120
that's the key part of this of this

00:17:08,160 --> 00:17:10,559
application

00:17:09,120 --> 00:17:12,880
so that's that's what we call the

00:17:10,559 --> 00:17:15,600
timeline view it gives you a very good

00:17:12,880 --> 00:17:18,319
uh overview of what happens dynamically

00:17:15,600 --> 00:17:21,439
as you execute your program

00:17:18,319 --> 00:17:22,079
now this is an openmp program so i run

00:17:21,439 --> 00:17:24,079
it on

00:17:22,079 --> 00:17:25,199
four threads and i have the same kind of

00:17:24,079 --> 00:17:27,439
view you see the

00:17:25,199 --> 00:17:28,799
initialization page you see that sorting

00:17:27,439 --> 00:17:30,320
that recursive sort

00:17:28,799 --> 00:17:32,799
but i'm going to ignore that i'm going

00:17:30,320 --> 00:17:36,080
to look at the search and verification

00:17:32,799 --> 00:17:38,240
and what we see now as we as we execute

00:17:36,080 --> 00:17:39,280
on four threads we see gaps in execution

00:17:38,240 --> 00:17:40,960
at some point

00:17:39,280 --> 00:17:42,640
there's nothing happening other than the

00:17:40,960 --> 00:17:43,440
main thread the main thread is always

00:17:42,640 --> 00:17:45,919
executing

00:17:43,440 --> 00:17:47,840
the other three threads have gaps in

00:17:45,919 --> 00:17:50,400
execution

00:17:47,840 --> 00:17:51,280
that sort of magnify when you zoom in

00:17:50,400 --> 00:17:53,520
you actually

00:17:51,280 --> 00:17:54,400
see those gaps just literally the the

00:17:53,520 --> 00:17:56,559
three

00:17:54,400 --> 00:17:58,240
extra threads they're not doing anything

00:17:56,559 --> 00:18:00,640
that's really bad news

00:17:58,240 --> 00:18:01,520
when you look a little deeper you see

00:18:00,640 --> 00:18:04,640
that's always

00:18:01,520 --> 00:18:05,520
while the red function is executing so

00:18:04,640 --> 00:18:07,679
each time the

00:18:05,520 --> 00:18:08,559
the red function is executing for some

00:18:07,679 --> 00:18:10,240
reason

00:18:08,559 --> 00:18:12,559
other than the main thread there's

00:18:10,240 --> 00:18:14,559
nothing happening that's really bad news

00:18:12,559 --> 00:18:16,480
and it's all over the place so that's

00:18:14,559 --> 00:18:18,559
the first immediate observation you make

00:18:16,480 --> 00:18:21,760
from looking at this kind of

00:18:18,559 --> 00:18:24,320
view i can sort of

00:18:21,760 --> 00:18:25,760
prove this by looking at the cpu time

00:18:24,320 --> 00:18:29,200
distribution

00:18:25,760 --> 00:18:30,320
i'm plotting here how much cpu time each

00:18:29,200 --> 00:18:32,400
step takes

00:18:30,320 --> 00:18:33,919
so when i have two threads the main

00:18:32,400 --> 00:18:36,799
threads take nearly

00:18:33,919 --> 00:18:37,280
nearly 800 seconds and the second thread

00:18:36,799 --> 00:18:39,600
only

00:18:37,280 --> 00:18:40,320
close to 600 so that's a big difference

00:18:39,600 --> 00:18:43,120
between

00:18:40,320 --> 00:18:43,600
how much cpu time each threat takes and

00:18:43,120 --> 00:18:45,760
as

00:18:43,600 --> 00:18:47,280
as you increase the number of threads it

00:18:45,760 --> 00:18:49,120
sort of gets worse

00:18:47,280 --> 00:18:51,520
so when i look at the ratio of the

00:18:49,120 --> 00:18:53,679
maximum and the minimum cpu times

00:18:51,520 --> 00:18:54,840
you see that ratio increases as the

00:18:53,679 --> 00:18:58,559
number of thread

00:18:54,840 --> 00:19:01,360
grows in other words the load balance

00:18:58,559 --> 00:19:04,880
the load imbalance gets worse as we add

00:19:01,360 --> 00:19:06,640
threats so that's not a good situation

00:19:04,880 --> 00:19:08,720
zooming in the tool told me this is

00:19:06,640 --> 00:19:11,120
where you spent a lot of your time

00:19:08,720 --> 00:19:13,039
and this is pretty ugly code i wanted to

00:19:11,120 --> 00:19:14,880
show it because that's real life

00:19:13,039 --> 00:19:16,320
uh not everything consists of the stream

00:19:14,880 --> 00:19:18,480
benchmark after all

00:19:16,320 --> 00:19:20,720
so this is even a stripped down version

00:19:18,480 --> 00:19:22,000
of the of the real code

00:19:20,720 --> 00:19:24,160
and there's a couple of things to

00:19:22,000 --> 00:19:25,840
observe here first of all we see the o

00:19:24,160 --> 00:19:29,039
and p4

00:19:25,840 --> 00:19:32,320
so that's a that's a fairly bulky

00:19:29,039 --> 00:19:33,280
openmp for loop and the outer loop is a

00:19:32,320 --> 00:19:36,240
fixed length loop

00:19:33,280 --> 00:19:37,360
it runs from k1 to old k2 minus 1. so

00:19:36,240 --> 00:19:40,559
that that's okay

00:19:37,360 --> 00:19:41,280
okay the second loop is irregular in

00:19:40,559 --> 00:19:43,919
length

00:19:41,280 --> 00:19:44,799
the length depends on some data

00:19:43,919 --> 00:19:48,240
structures

00:19:44,799 --> 00:19:50,840
that depend on k so potentially the

00:19:48,240 --> 00:19:52,559
the length of the second loop is not a

00:19:50,840 --> 00:19:55,200
constant

00:19:52,559 --> 00:19:57,200
another thing we see an if statement and

00:19:55,200 --> 00:19:59,039
in case that if statement is true

00:19:57,200 --> 00:20:00,960
there's a fair amount of work being done

00:19:59,039 --> 00:20:03,039
there's even a compare and swap

00:20:00,960 --> 00:20:04,480
uh there's some potentially some copying

00:20:03,039 --> 00:20:07,360
of data going on

00:20:04,480 --> 00:20:09,280
so if that if statement is true there's

00:20:07,360 --> 00:20:10,080
a fair amount of work that that could be

00:20:09,280 --> 00:20:11,679
performed

00:20:10,080 --> 00:20:13,280
and that's what we call the irregular

00:20:11,679 --> 00:20:16,000
control flow

00:20:13,280 --> 00:20:18,799
well in other words what that means that

00:20:16,000 --> 00:20:20,960
if a thread picks up an iteration for k

00:20:18,799 --> 00:20:23,200
the amount of work it does could

00:20:20,960 --> 00:20:25,840
strongly vary

00:20:23,200 --> 00:20:27,520
compared to another value of k so this

00:20:25,840 --> 00:20:29,440
is a classical example of a load

00:20:27,520 --> 00:20:32,799
imbalance and that's what we saw

00:20:29,440 --> 00:20:34,880
when we had those gaps now

00:20:32,799 --> 00:20:36,720
the thing is that loop was parallelized

00:20:34,880 --> 00:20:38,880
with a simple omb4

00:20:36,720 --> 00:20:40,640
and the specifications say that then you

00:20:38,880 --> 00:20:43,760
get some default scheduling

00:20:40,640 --> 00:20:45,679
and for efficiency reasons um that's

00:20:43,760 --> 00:20:47,520
quite often a static schedule

00:20:45,679 --> 00:20:48,960
static scheduling is very efficient to

00:20:47,520 --> 00:20:52,080
implement it's very efficient

00:20:48,960 --> 00:20:54,559
at runtime but it's very rigid

00:20:52,080 --> 00:20:55,919
so you get low balancing issues like in

00:20:54,559 --> 00:20:57,760
this case

00:20:55,919 --> 00:20:59,120
a better choice would be to have a more

00:20:57,760 --> 00:21:02,080
dynamic loop

00:20:59,120 --> 00:21:03,440
loop schedule clause like like dynamic

00:21:02,080 --> 00:21:06,000
or guided

00:21:03,440 --> 00:21:07,520
or if you go that way my my choice is to

00:21:06,000 --> 00:21:09,919
use the runtime clause

00:21:07,520 --> 00:21:10,960
so that i can set the actual schedule at

00:21:09,919 --> 00:21:12,880
runtime through an

00:21:10,960 --> 00:21:15,039
environment variable so that way i don't

00:21:12,880 --> 00:21:17,919
have to recompile my code all the time

00:21:15,039 --> 00:21:18,480
and that's exactly what i did here and i

00:21:17,919 --> 00:21:20,400
i

00:21:18,480 --> 00:21:22,880
ended up with setting um the dynamic

00:21:20,400 --> 00:21:23,600
schedule and a chunk size of 25 so each

00:21:22,880 --> 00:21:26,320
time you get

00:21:23,600 --> 00:21:27,600
25 iterations how do you know the chunk

00:21:26,320 --> 00:21:30,000
size should be 25

00:21:27,600 --> 00:21:32,080
well i could be very fancy about it but

00:21:30,000 --> 00:21:34,640
essentially it's crystal ball

00:21:32,080 --> 00:21:36,000
trial and error you just got to try you

00:21:34,640 --> 00:21:38,320
pick a few values you

00:21:36,000 --> 00:21:39,200
use some bisection technique to find out

00:21:38,320 --> 00:21:41,919
where

00:21:39,200 --> 00:21:43,679
what helps and of course the 25 is not

00:21:41,919 --> 00:21:47,280
carved in stone i'm quite sure if you

00:21:43,679 --> 00:21:48,880
use a 24 or 56 or something around 25

00:21:47,280 --> 00:21:51,200
you'll get similar performance it's

00:21:48,880 --> 00:21:53,360
usually not that sensitive to the value

00:21:51,200 --> 00:21:54,559
but you know if you would pick 50 it

00:21:53,360 --> 00:21:55,760
probably wouldn't be good

00:21:54,559 --> 00:21:58,240
so yes you have to do some

00:21:55,760 --> 00:22:00,000
experimentation in cases like this and

00:21:58,240 --> 00:22:01,440
that's why i like the runtime clause

00:22:00,000 --> 00:22:02,720
because i can just do that through the

00:22:01,440 --> 00:22:04,480
environment variable

00:22:02,720 --> 00:22:05,760
not having to modify my code all the

00:22:04,480 --> 00:22:07,440
time think about

00:22:05,760 --> 00:22:10,159
think about that it's a very nice

00:22:07,440 --> 00:22:12,720
feature to use

00:22:10,159 --> 00:22:13,520
it definitely pays off of what i'm

00:22:12,720 --> 00:22:16,080
showing here

00:22:13,520 --> 00:22:18,000
is the search time and that was

00:22:16,080 --> 00:22:19,679
important for this benchmark so the bfs

00:22:18,000 --> 00:22:22,640
search time in seconds

00:22:19,679 --> 00:22:23,200
that's the bar chart and the dotted

00:22:22,640 --> 00:22:25,280
lines

00:22:23,200 --> 00:22:26,640
um that's the parallel speed up

00:22:25,280 --> 00:22:28,000
normalized to the single threat

00:22:26,640 --> 00:22:31,520
performance

00:22:28,000 --> 00:22:33,840
so what we see here is that the

00:22:31,520 --> 00:22:36,720
original code in single thread is

00:22:33,840 --> 00:22:38,720
actually a little bit faster

00:22:36,720 --> 00:22:40,480
than the new code why well there's a

00:22:38,720 --> 00:22:41,840
cost for the dynamic schedule the

00:22:40,480 --> 00:22:43,679
dynamic schedule

00:22:41,840 --> 00:22:46,240
is more expensive at runtime and what we

00:22:43,679 --> 00:22:48,000
see we see a tiny bit of a slowdown

00:22:46,240 --> 00:22:49,280
so small that i really don't care but i

00:22:48,000 --> 00:22:51,520
wanted to point it out

00:22:49,280 --> 00:22:53,039
that yes the the single type performance

00:22:51,520 --> 00:22:55,679
takes a little bit of a hit

00:22:53,039 --> 00:22:57,760
but then as as you start adding threads

00:22:55,679 --> 00:23:01,200
you can definitely see how much the

00:22:57,760 --> 00:23:03,520
search time improves compared to the

00:23:01,200 --> 00:23:04,720
black dotted line for the original code

00:23:03,520 --> 00:23:07,120
that barely

00:23:04,720 --> 00:23:09,120
barely showed any improvement we now get

00:23:07,120 --> 00:23:09,840
near linear scaling for up to eight

00:23:09,120 --> 00:23:12,480
threads

00:23:09,840 --> 00:23:12,960
i was running this on eight cores only

00:23:12,480 --> 00:23:15,600
um

00:23:12,960 --> 00:23:17,679
with hyper threading enabled so beyond

00:23:15,600 --> 00:23:19,600
eight threads i see that i still get a

00:23:17,679 --> 00:23:21,440
benefit from hyper threading which is

00:23:19,600 --> 00:23:23,840
kind of nice you can't always count on

00:23:21,440 --> 00:23:26,640
that but of course i've reached my

00:23:23,840 --> 00:23:27,600
maximum gain uh when i went away in a

00:23:26,640 --> 00:23:30,559
way so i've got a little

00:23:27,600 --> 00:23:32,000
extra boost and ultimately my code is

00:23:30,559 --> 00:23:34,799
three times faster

00:23:32,000 --> 00:23:36,159
than it was before so now think about

00:23:34,799 --> 00:23:38,720
that what did it take

00:23:36,159 --> 00:23:39,520
a profiling tool to tell me what's wrong

00:23:38,720 --> 00:23:41,679
and only

00:23:39,520 --> 00:23:43,360
the only change that i made here was to

00:23:41,679 --> 00:23:45,600
change the loop scheduling

00:23:43,360 --> 00:23:47,440
strategy from static to dynamic and a

00:23:45,600 --> 00:23:49,440
junk size of 25

00:23:47,440 --> 00:23:51,840
so very small change and quite a big

00:23:49,440 --> 00:23:54,880
improvement so

00:23:51,840 --> 00:23:57,039
all right what's really important

00:23:54,880 --> 00:23:58,880
always do what i call the sanity check

00:23:57,039 --> 00:24:01,120
always verify the behavior

00:23:58,880 --> 00:24:02,799
we see that this is working out well did

00:24:01,120 --> 00:24:04,799
we indeed tackle the problem

00:24:02,799 --> 00:24:06,159
i can just highly recommend to verify

00:24:04,799 --> 00:24:07,840
that and

00:24:06,159 --> 00:24:10,320
that's what i'm doing here i'm comparing

00:24:07,840 --> 00:24:11,200
two profiles the top one is on eight

00:24:10,320 --> 00:24:12,880
threads

00:24:11,200 --> 00:24:14,240
using the original code you see all

00:24:12,880 --> 00:24:17,440
those gaps

00:24:14,240 --> 00:24:18,720
and uh on the new version that's the the

00:24:17,440 --> 00:24:20,400
lower bar

00:24:18,720 --> 00:24:22,240
you see that those gaps are pretty much

00:24:20,400 --> 00:24:25,760
gone at least in the visible view

00:24:22,240 --> 00:24:28,080
range you may see them when you zoom in

00:24:25,760 --> 00:24:30,320
so this is not telling the whole story

00:24:28,080 --> 00:24:31,520
so what i did i again looked at the cpu

00:24:30,320 --> 00:24:34,159
time distribution

00:24:31,520 --> 00:24:34,640
across the threads while we had a big

00:24:34,159 --> 00:24:36,960
gap

00:24:34,640 --> 00:24:37,679
in the in the original case with dynamic

00:24:36,960 --> 00:24:40,080
scheduling

00:24:37,679 --> 00:24:41,520
the cpu time is pretty much distributed

00:24:40,080 --> 00:24:44,240
equally

00:24:41,520 --> 00:24:44,960
so the gap is indeed gone it's not only

00:24:44,240 --> 00:24:47,600
um

00:24:44,960 --> 00:24:49,520
optical illusion it's actually really

00:24:47,600 --> 00:24:51,760
gone

00:24:49,520 --> 00:24:53,200
all right let's continue with them with

00:24:51,760 --> 00:24:55,600
the recipe

00:24:53,200 --> 00:24:57,200
uh i can't stress that enough a barrier

00:24:55,600 --> 00:24:59,520
is useful and you need them

00:24:57,200 --> 00:25:00,240
but they are expensive so use them with

00:24:59,520 --> 00:25:03,279
care

00:25:00,240 --> 00:25:04,960
so think about know why no weight

00:25:03,279 --> 00:25:06,799
or if you use an explicit barrier

00:25:04,960 --> 00:25:09,440
yourself be careful

00:25:06,799 --> 00:25:11,039
when to use them the same is true for

00:25:09,440 --> 00:25:11,919
locking in critical regions you

00:25:11,039 --> 00:25:13,919
absolutely

00:25:11,919 --> 00:25:15,840
need those in certain cases no doubt

00:25:13,919 --> 00:25:18,480
about it but be careful

00:25:15,840 --> 00:25:21,039
and specifically look at atomic

00:25:18,480 --> 00:25:23,120
operations and unfortunately i find that

00:25:21,039 --> 00:25:24,640
not many people know that openmp has

00:25:23,120 --> 00:25:25,360
atomic operations and dominant

00:25:24,640 --> 00:25:27,840
constructs

00:25:25,360 --> 00:25:28,480
i should say those atomic constructs

00:25:27,840 --> 00:25:31,919
leverage

00:25:28,480 --> 00:25:35,039
atomic instructions inside the machine

00:25:31,919 --> 00:25:37,200
they're much more efficient efficient so

00:25:35,039 --> 00:25:38,640
instead of using a critical region um

00:25:37,200 --> 00:25:40,720
sometimes you can use an atomic

00:25:38,640 --> 00:25:43,840
operation and it's way more efficient

00:25:40,720 --> 00:25:46,000
so think about those things in the end

00:25:43,840 --> 00:25:48,400
you know amdah's law is still valid as

00:25:46,000 --> 00:25:51,200
ever so everything matters

00:25:48,400 --> 00:25:52,960
and i'll get back to that later but

00:25:51,200 --> 00:25:54,559
don't leave anything on the table if you

00:25:52,960 --> 00:25:56,559
see something inefficient

00:25:54,559 --> 00:25:57,679
even if it doesn't help you today it may

00:25:56,559 --> 00:26:00,640
help you tomorrow

00:25:57,679 --> 00:26:01,679
as you go to a larger system so pick up

00:26:00,640 --> 00:26:04,799
everything you see

00:26:01,679 --> 00:26:07,200
and fix it as much as you can

00:26:04,799 --> 00:26:08,799
another golden rule is don't use nested

00:26:07,200 --> 00:26:11,840
parallelism

00:26:08,799 --> 00:26:13,440
i know some algorithms benefit from it

00:26:11,840 --> 00:26:16,240
from a functional point of view

00:26:13,440 --> 00:26:16,799
but if nested parallelism uh works for

00:26:16,240 --> 00:26:19,039
you

00:26:16,799 --> 00:26:20,799
start looking into tasking tasking is a

00:26:19,039 --> 00:26:23,520
much more flexible much more

00:26:20,799 --> 00:26:25,360
fine-grained system that nested

00:26:23,520 --> 00:26:25,919
parallelism nester parallelism was in

00:26:25,360 --> 00:26:29,360
openmp

00:26:25,919 --> 00:26:30,720
1.0 uh it wasn't bad but by now you have

00:26:29,360 --> 00:26:32,880
better alternatives

00:26:30,720 --> 00:26:34,400
especially uh remember when you start

00:26:32,880 --> 00:26:36,640
nesting parallel regions

00:26:34,400 --> 00:26:39,440
you also nest the barriers and that can

00:26:36,640 --> 00:26:41,679
get very expensive very quickly

00:26:39,440 --> 00:26:44,320
again like i just said look at tasking

00:26:41,679 --> 00:26:46,400
much more much more flexible

00:26:44,320 --> 00:26:48,080
for example the loop i just showed one

00:26:46,400 --> 00:26:49,840
of the things i want to do is see what

00:26:48,080 --> 00:26:50,640
happens with the task loop task loop is

00:26:49,840 --> 00:26:53,440
a nice

00:26:50,640 --> 00:26:54,400
nice feature to leverage tasking on a

00:26:53,440 --> 00:26:56,960
loop

00:26:54,400 --> 00:26:57,600
and it may do better than dynamic so i'm

00:26:56,960 --> 00:26:59,840
going to

00:26:57,600 --> 00:27:00,799
look into that so maybe in a future

00:26:59,840 --> 00:27:05,520
webinar i can

00:27:00,799 --> 00:27:07,200
report on that i now want to zoom in on

00:27:05,520 --> 00:27:08,960
on what i call the harder part what i

00:27:07,200 --> 00:27:09,679
just gave was some sort of checklist of

00:27:08,960 --> 00:27:12,159
things to do

00:27:09,679 --> 00:27:13,679
and not to do i'm now going to talk

00:27:12,159 --> 00:27:15,919
about memory access

00:27:13,679 --> 00:27:17,840
as you all know in openmp memory access

00:27:15,919 --> 00:27:20,480
just happens you need some variable a

00:27:17,840 --> 00:27:22,159
you'll get it there are two things to

00:27:20,480 --> 00:27:24,720
watch out for though

00:27:22,159 --> 00:27:26,799
and one of them is performance loss

00:27:24,720 --> 00:27:28,960
caused by a non-uniform memory access

00:27:26,799 --> 00:27:31,440
architecture or pneuma for short

00:27:28,960 --> 00:27:33,039
or false sharing or both well hopefully

00:27:31,440 --> 00:27:35,200
that's not the case but it could be

00:27:33,039 --> 00:27:36,320
so there are two things that may impact

00:27:35,200 --> 00:27:38,480
your performance

00:27:36,320 --> 00:27:40,240
they're at the memory level and they

00:27:38,480 --> 00:27:41,760
just happen they're sort of sneaky

00:27:40,240 --> 00:27:43,360
because you don't you don't really see

00:27:41,760 --> 00:27:43,919
it it's not like a barrier that you

00:27:43,360 --> 00:27:46,320
stick in

00:27:43,919 --> 00:27:47,919
and sort of sticks out it just happens

00:27:46,320 --> 00:27:48,880
and i'm going to spend quite some time

00:27:47,919 --> 00:27:51,760
on that

00:27:48,880 --> 00:27:53,279
on those memory things i want to stress

00:27:51,760 --> 00:27:54,480
that both of them have nothing to do

00:27:53,279 --> 00:27:56,720
with openmp

00:27:54,480 --> 00:27:58,640
um they're they're a result of using a

00:27:56,720 --> 00:28:00,640
shared memory architecture

00:27:58,640 --> 00:28:02,159
so you could write the examples that i'm

00:28:00,640 --> 00:28:02,640
going to show you could equally write

00:28:02,159 --> 00:28:05,039
them

00:28:02,640 --> 00:28:06,559
in let's say p threads or java threads

00:28:05,039 --> 00:28:07,360
and you'll see the same bad thing

00:28:06,559 --> 00:28:09,200
happening

00:28:07,360 --> 00:28:10,480
uh it's just that you happen to use

00:28:09,200 --> 00:28:12,480
openmp and that's

00:28:10,480 --> 00:28:14,240
that's then it shows but again this has

00:28:12,480 --> 00:28:15,840
nothing to do with openmp

00:28:14,240 --> 00:28:17,360
it's because we're using a shared memory

00:28:15,840 --> 00:28:20,640
architecture

00:28:17,360 --> 00:28:23,120
okay so both may have a pretty pretty

00:28:20,640 --> 00:28:24,880
big impact on performance

00:28:23,120 --> 00:28:26,720
now the second part of this talk which

00:28:24,880 --> 00:28:28,559
is imminent in a few minutes

00:28:26,720 --> 00:28:30,000
i'm going to talk about numa almost

00:28:28,559 --> 00:28:32,640
exclusively so i'll

00:28:30,000 --> 00:28:33,600
skip that for now um i want to finish

00:28:32,640 --> 00:28:35,679
this part one

00:28:33,600 --> 00:28:37,120
with what i would call what we call

00:28:35,679 --> 00:28:39,279
false sharing it's a bit of a

00:28:37,120 --> 00:28:40,399
funny name when you think about it but

00:28:39,279 --> 00:28:42,000
it's definitely

00:28:40,399 --> 00:28:45,679
something you don't want to have in your

00:28:42,000 --> 00:28:48,080
application so what is it

00:28:45,679 --> 00:28:49,279
we talk about false sharing when

00:28:48,080 --> 00:28:51,679
multiple threads

00:28:49,279 --> 00:28:53,760
modify the same cache line at the same

00:28:51,679 --> 00:28:54,880
time in other words they're hitting on a

00:28:53,760 --> 00:28:57,520
small portion of

00:28:54,880 --> 00:28:58,720
a relatively small portion of memory all

00:28:57,520 --> 00:29:01,279
of them at the same time

00:28:58,720 --> 00:29:02,559
and then modifying it and that can

00:29:01,279 --> 00:29:04,480
happen

00:29:02,559 --> 00:29:05,919
in some sort of unexpected ways you may

00:29:04,480 --> 00:29:07,760
never have thought about it but i'll

00:29:05,919 --> 00:29:08,480
show you a very simple example how

00:29:07,760 --> 00:29:11,200
easily

00:29:08,480 --> 00:29:13,200
that can happen but what happens with

00:29:11,200 --> 00:29:15,279
false sharing is that the cache line

00:29:13,200 --> 00:29:17,600
will move through the system because the

00:29:15,279 --> 00:29:20,559
thread can only make a change

00:29:17,600 --> 00:29:22,399
when it has the cache line of where the

00:29:20,559 --> 00:29:24,240
data is in its cache

00:29:22,399 --> 00:29:26,880
so if that cache line happens to be

00:29:24,240 --> 00:29:28,399
elsewhere it'll have to go and fetch it

00:29:26,880 --> 00:29:29,919
now the hardware will take care of that

00:29:28,399 --> 00:29:32,559
luckily

00:29:29,919 --> 00:29:33,360
but it still takes time and you can

00:29:32,559 --> 00:29:35,360
imagine if

00:29:33,360 --> 00:29:37,200
this happens on a large scale on a large

00:29:35,360 --> 00:29:38,799
machine this will impact performance and

00:29:37,200 --> 00:29:42,480
it does impact performance

00:29:38,799 --> 00:29:44,159
okay there's also um an extra cost that

00:29:42,480 --> 00:29:46,080
you have to update the state of the cash

00:29:44,159 --> 00:29:47,760
line that the cash coins updates

00:29:46,080 --> 00:29:50,559
but the biggest thing is that cash line

00:29:47,760 --> 00:29:52,720
starts traveling throughout your system

00:29:50,559 --> 00:29:54,320
now false sharing happens a lot and if

00:29:52,720 --> 00:29:55,760
it happens once in a while that's fine

00:29:54,320 --> 00:29:58,080
you won't even notice it

00:29:55,760 --> 00:29:59,200
the problem is if it's in the heart of

00:29:58,080 --> 00:30:01,840
your algorithm

00:29:59,200 --> 00:30:03,039
then it's not okay and you will see very

00:30:01,840 --> 00:30:05,600
poor scalability

00:30:03,039 --> 00:30:07,200
false sharing tends to hurt performance

00:30:05,600 --> 00:30:09,520
very badly

00:30:07,200 --> 00:30:10,320
so here's an absolutely trivial example

00:30:09,520 --> 00:30:13,919
that shows

00:30:10,320 --> 00:30:15,760
uh george falls sharing in into action

00:30:13,919 --> 00:30:16,960
i have a parallel region and i have a

00:30:15,760 --> 00:30:20,000
shared vector

00:30:16,960 --> 00:30:22,720
a and what i'm going to do i'm going to

00:30:20,000 --> 00:30:23,760
initialize a to zero and each thread

00:30:22,720 --> 00:30:25,840
will initialize

00:30:23,760 --> 00:30:28,000
its element to zero so i get the thread

00:30:25,840 --> 00:30:31,200
id and that element is

00:30:28,000 --> 00:30:34,480
being assigned to zero

00:30:31,200 --> 00:30:37,679
very very trivial the problem is

00:30:34,480 --> 00:30:39,360
that update of a has all the ingredients

00:30:37,679 --> 00:30:40,960
all the components for false sharing to

00:30:39,360 --> 00:30:44,480
happen we do that

00:30:40,960 --> 00:30:47,600
uh presumably at about the same time uh

00:30:44,480 --> 00:30:49,440
this is a a tiny array um you know even

00:30:47,600 --> 00:30:50,080
if you use thousand threads it won't be

00:30:49,440 --> 00:30:52,880
more than

00:30:50,080 --> 00:30:53,679
eight kilobytes so you have a tiny block

00:30:52,880 --> 00:30:55,760
of memory

00:30:53,679 --> 00:30:56,960
hit upon by all the threads at the same

00:30:55,760 --> 00:30:58,480
time all the

00:30:56,960 --> 00:31:00,399
all the conditions for false sharing are

00:30:58,480 --> 00:31:03,600
met here so that update

00:31:00,399 --> 00:31:05,919
has suffers from false sharing so here's

00:31:03,600 --> 00:31:07,760
what happens in in run time

00:31:05,919 --> 00:31:10,320
at runtime so let's say i have four

00:31:07,760 --> 00:31:12,559
threads so a has four elements

00:31:10,320 --> 00:31:13,840
and time is from top to bottom

00:31:12,559 --> 00:31:15,840
presumably let's say

00:31:13,840 --> 00:31:18,240
that zero does the first update you

00:31:15,840 --> 00:31:20,720
don't know but that means it will

00:31:18,240 --> 00:31:21,679
find the cache line wherever a of zero

00:31:20,720 --> 00:31:23,279
is

00:31:21,679 --> 00:31:25,360
find the cache line get it into its

00:31:23,279 --> 00:31:28,159
cache and do the update

00:31:25,360 --> 00:31:30,240
but then but same time thread number

00:31:28,159 --> 00:31:32,159
three will want to do the same

00:31:30,240 --> 00:31:34,720
and the hardware the hardware will

00:31:32,159 --> 00:31:36,480
arbitrate between that but at some point

00:31:34,720 --> 00:31:38,320
this other thread will need that cache

00:31:36,480 --> 00:31:40,720
like that same cache line

00:31:38,320 --> 00:31:42,399
that vector a is in so it'll have to get

00:31:40,720 --> 00:31:44,799
the fresh copy of that line

00:31:42,399 --> 00:31:47,039
move it to its cache and do the update

00:31:44,799 --> 00:31:49,679
and then history repeats

00:31:47,039 --> 00:31:51,440
so each time a thread will do an update

00:31:49,679 --> 00:31:52,640
it may mean that it will have to get a

00:31:51,440 --> 00:31:54,720
copy of that cache line

00:31:52,640 --> 00:31:56,159
from somewhere else in the system and if

00:31:54,720 --> 00:31:57,679
that's a large system

00:31:56,159 --> 00:31:59,360
that could take a while even if the

00:31:57,679 --> 00:32:00,559
small system this is actually already

00:31:59,360 --> 00:32:02,399
expensive

00:32:00,559 --> 00:32:04,159
so that's called false sharing i won't

00:32:02,399 --> 00:32:06,960
say anything more about it

00:32:04,159 --> 00:32:08,480
um it's it's the last thing to check if

00:32:06,960 --> 00:32:10,480
if nothing else explains your poor

00:32:08,480 --> 00:32:12,720
performance think about false sharing

00:32:10,480 --> 00:32:14,240
i wouldn't think about it right away but

00:32:12,720 --> 00:32:15,679
i've had several people that

00:32:14,240 --> 00:32:18,320
after i've mentioned this they would

00:32:15,679 --> 00:32:20,320
tell me oh now i know now i know why

00:32:18,320 --> 00:32:22,720
why my code is not performing well they

00:32:20,320 --> 00:32:25,360
were suffering from falsehood

00:32:22,720 --> 00:32:28,720
all right brings us to our first um

00:32:25,360 --> 00:32:30,559
tuning strategy i try to give you

00:32:28,720 --> 00:32:33,760
practical advice in this first part

00:32:30,559 --> 00:32:35,600
and i would just take a look at it and

00:32:33,760 --> 00:32:36,799
follow them and see if you can do

00:32:35,600 --> 00:32:39,519
something about it

00:32:36,799 --> 00:32:40,159
give it a try don't forget make a

00:32:39,519 --> 00:32:42,159
profile

00:32:40,159 --> 00:32:44,480
before and after always make sure that

00:32:42,159 --> 00:32:47,440
indeed your time

00:32:44,480 --> 00:32:48,880
will start making things um it's always

00:32:47,440 --> 00:32:51,200
a good practice

00:32:48,880 --> 00:32:52,480
make a profile before you begin but all

00:32:51,200 --> 00:32:56,320
that's right as well

00:32:52,480 --> 00:32:57,360
uh sometimes the details so um this is

00:32:56,320 --> 00:32:59,440
um

00:32:57,360 --> 00:33:01,600
sort of carved in stone some nasty

00:32:59,440 --> 00:33:05,279
detail can make all the difference

00:33:01,600 --> 00:33:09,840
um if you have this performance mystery

00:33:05,279 --> 00:33:12,320
so you don't know um

00:33:09,840 --> 00:33:12,960
what explains it think about pneuma fall

00:33:12,320 --> 00:33:15,120
sharing

00:33:12,960 --> 00:33:16,559
or in the worst case or both that's not

00:33:15,120 --> 00:33:20,960
very likely i hope but

00:33:16,559 --> 00:33:22,880
um you can't exclude it either all right

00:33:20,960 --> 00:33:24,640
i said i'm on a tight schedule i want to

00:33:22,880 --> 00:33:26,640
move on to the second part

00:33:24,640 --> 00:33:28,000
where i try to get closer to the real

00:33:26,640 --> 00:33:30,559
world

00:33:28,000 --> 00:33:32,480
okay i want to again show several tuning

00:33:30,559 --> 00:33:36,720
examples they're all sort of

00:33:32,480 --> 00:33:40,080
bears some relevance to real life and um

00:33:36,720 --> 00:33:41,600
the um um the first challenge is

00:33:40,080 --> 00:33:43,279
when you start tuning in code and

00:33:41,600 --> 00:33:46,480
anybody who has done that

00:33:43,279 --> 00:33:48,399
knows that is finding it finding out

00:33:46,480 --> 00:33:49,919
what goes wrong but what i show you here

00:33:48,399 --> 00:33:53,200
is the final result of an

00:33:49,919 --> 00:33:57,519
analysis and that can be a lot of work

00:33:53,200 --> 00:33:59,760
so i show you some isolated cases but in

00:33:57,519 --> 00:34:01,200
real life the situation could be much

00:33:59,760 --> 00:34:03,360
more masked than hidden

00:34:01,200 --> 00:34:04,720
okay the last part is a completely

00:34:03,360 --> 00:34:08,560
worked work through

00:34:04,720 --> 00:34:11,440
pneuma example so more examples

00:34:08,560 --> 00:34:13,119
here's one and um again all of this is

00:34:11,440 --> 00:34:14,399
picked up from real code so i didn't

00:34:13,119 --> 00:34:18,000
invent this

00:34:14,399 --> 00:34:20,079
um the single region uh i have some some

00:34:18,000 --> 00:34:20,879
code in there and in the end and

00:34:20,079 --> 00:34:23,280
followed by

00:34:20,879 --> 00:34:24,320
a barrier okay looks fairly fairly

00:34:23,280 --> 00:34:26,240
innocent code

00:34:24,320 --> 00:34:28,800
but it's wrong from a performance point

00:34:26,240 --> 00:34:29,919
of view because that second barrier is

00:34:28,800 --> 00:34:32,720
redundant

00:34:29,919 --> 00:34:34,399
the single has an implied barrier so

00:34:32,720 --> 00:34:35,200
what we have we have two back to back

00:34:34,399 --> 00:34:37,280
barriers

00:34:35,200 --> 00:34:39,040
and admittedly the second barrier is

00:34:37,280 --> 00:34:40,159
going to be faster but it's still taking

00:34:39,040 --> 00:34:42,720
the time

00:34:40,159 --> 00:34:43,599
and as you add threads the cost of that

00:34:42,720 --> 00:34:46,000
will go up

00:34:43,599 --> 00:34:48,560
so why waste it just remove it it was

00:34:46,000 --> 00:34:51,440
not necessary in this code and again a

00:34:48,560 --> 00:34:53,679
compiler will not do this for you

00:34:51,440 --> 00:34:55,599
um i'm if i'm if i'm wrong please

00:34:53,679 --> 00:34:57,920
correct me i send me a mail

00:34:55,599 --> 00:34:59,359
but so far what i've seen is compilers

00:34:57,920 --> 00:35:01,200
will just do this and this is not

00:34:59,359 --> 00:35:03,200
because the compilers are not smart

00:35:01,200 --> 00:35:05,040
it's just because the way things happen

00:35:03,200 --> 00:35:06,560
like i said it's a whole talk in itself

00:35:05,040 --> 00:35:08,320
maybe one of the webinars should be

00:35:06,560 --> 00:35:08,880
about those things about the compiler

00:35:08,320 --> 00:35:12,640
view

00:35:08,880 --> 00:35:14,079
on openmp but in practice it won't even

00:35:12,640 --> 00:35:15,920
do this very simple

00:35:14,079 --> 00:35:18,240
optimization you certainly can't count

00:35:15,920 --> 00:35:18,480
on it so don't write it and if you have

00:35:18,240 --> 00:35:21,599
it

00:35:18,480 --> 00:35:22,160
remove that secondary a much more

00:35:21,599 --> 00:35:24,160
complex

00:35:22,160 --> 00:35:26,400
complicated example where you

00:35:24,160 --> 00:35:28,160
potentially do more work and still save

00:35:26,400 --> 00:35:30,000
time now when you start doing more work

00:35:28,160 --> 00:35:31,040
you always got to be careful because the

00:35:30,000 --> 00:35:32,560
balance can

00:35:31,040 --> 00:35:34,560
create quickly going to the wrong

00:35:32,560 --> 00:35:36,480
direction not in this case

00:35:34,560 --> 00:35:39,040
and that's why i want to want to show it

00:35:36,480 --> 00:35:42,480
to you this is from a real code

00:35:39,040 --> 00:35:44,640
what it does it starts initializing one

00:35:42,480 --> 00:35:45,920
one array one vector element at the

00:35:44,640 --> 00:35:47,599
index endpoint

00:35:45,920 --> 00:35:50,079
that will go into a parallel region

00:35:47,599 --> 00:35:51,680
fairly large parallel region i only show

00:35:50,079 --> 00:35:54,800
you the relevant part for this

00:35:51,680 --> 00:35:57,040
this case study there's a for loop

00:35:54,800 --> 00:35:58,240
and a for loop runs from zero to

00:35:57,040 --> 00:36:00,960
endpoint minus one

00:35:58,240 --> 00:36:02,640
and it initializes a to minus one over

00:36:00,960 --> 00:36:04,720
that range

00:36:02,640 --> 00:36:06,560
then there's a second for loop and it

00:36:04,720 --> 00:36:08,640
starts at endpoint plus one

00:36:06,560 --> 00:36:10,400
so it's not doing the update on endpoint

00:36:08,640 --> 00:36:12,240
of course and it's initializing two

00:36:10,400 --> 00:36:13,680
minus one and then there's a whole bunch

00:36:12,240 --> 00:36:16,560
of code following it

00:36:13,680 --> 00:36:18,560
so this was sort of the openmp structure

00:36:16,560 --> 00:36:21,680
now

00:36:18,560 --> 00:36:24,960
what's wrong with this first of all

00:36:21,680 --> 00:36:26,720
both omp4 have a barrier looking at it

00:36:24,960 --> 00:36:28,880
you can you can use in no way

00:36:26,720 --> 00:36:30,720
on the on the first one i'm not sure

00:36:28,880 --> 00:36:31,839
about the second one because it depends

00:36:30,720 --> 00:36:34,800
on what happens

00:36:31,839 --> 00:36:36,079
next but um those two updates are

00:36:34,800 --> 00:36:38,000
independent so

00:36:36,079 --> 00:36:40,640
there's no weight missing we got at

00:36:38,000 --> 00:36:41,599
least two barriers in this all these opp

00:36:40,640 --> 00:36:44,160
force

00:36:41,599 --> 00:36:46,160
you have the overhead for the op4 which

00:36:44,160 --> 00:36:49,280
is not that big but again

00:36:46,160 --> 00:36:51,200
everything matters so why waste it the

00:36:49,280 --> 00:36:53,680
the big thing here is that the

00:36:51,200 --> 00:36:57,040
performance benefit depends on

00:36:53,680 --> 00:36:57,599
n the the overall loop length and the

00:36:57,040 --> 00:37:00,400
variable

00:36:57,599 --> 00:37:01,520
the value of endpoint if endpoint is

00:37:00,400 --> 00:37:04,000
small

00:37:01,520 --> 00:37:05,680
the first loop is short and that means

00:37:04,000 --> 00:37:07,920
it's probably not a good idea to to

00:37:05,680 --> 00:37:09,599
execute that in parallel certainly on on

00:37:07,920 --> 00:37:10,880
a whole bunch of threads if you're using

00:37:09,599 --> 00:37:13,200
a hundred threads

00:37:10,880 --> 00:37:14,480
an endpoint is 20 that's not a very good

00:37:13,200 --> 00:37:18,160
idea

00:37:14,480 --> 00:37:22,240
likewise if endpoint is is large

00:37:18,160 --> 00:37:24,720
then the second loop is short so

00:37:22,240 --> 00:37:27,359
and as as this code was called sort of

00:37:24,720 --> 00:37:29,200
sliding through the value of endpoint

00:37:27,359 --> 00:37:31,200
that's going to be a lot of cases where

00:37:29,200 --> 00:37:34,480
either one of these loops is inefficient

00:37:31,200 --> 00:37:36,560
so that's not a not a good idea

00:37:34,480 --> 00:37:38,240
okay but take go back to the drawing

00:37:36,560 --> 00:37:40,000
board that's often what i'd like to do

00:37:38,240 --> 00:37:41,920
and sketch things like what's really

00:37:40,000 --> 00:37:44,160
happening here what what are you really

00:37:41,920 --> 00:37:46,960
trying to accomplish well

00:37:44,160 --> 00:37:49,200
it starts with this uh at index endpoint

00:37:46,960 --> 00:37:51,599
the value is assigned to a

00:37:49,200 --> 00:37:53,599
then you have the block of a zero to

00:37:51,599 --> 00:37:54,000
endpoint minus one initialized to minus

00:37:53,599 --> 00:37:57,119
one

00:37:54,000 --> 00:37:58,800
a barrier another block update

00:37:57,119 --> 00:38:00,640
and another barrier that's what the

00:37:58,800 --> 00:38:02,640
current code is doing but when you look

00:38:00,640 --> 00:38:05,680
at the final result

00:38:02,640 --> 00:38:07,440
this is the final result and

00:38:05,680 --> 00:38:09,839
then i realized wait a minute i can do

00:38:07,440 --> 00:38:12,880
better than that so what i can do

00:38:09,839 --> 00:38:14,560
and that's the idea i just blast -1 into

00:38:12,880 --> 00:38:17,040
that whole block of memory

00:38:14,560 --> 00:38:18,880
and i do a single correction at the

00:38:17,040 --> 00:38:20,800
index endpoint

00:38:18,880 --> 00:38:22,160
well that's the idea you get and as i

00:38:20,800 --> 00:38:25,200
said in the beginning of this

00:38:22,160 --> 00:38:27,760
this talk that's why i like openmp

00:38:25,200 --> 00:38:29,520
implementing that idea is really easy

00:38:27,760 --> 00:38:31,520
the new code looks like this

00:38:29,520 --> 00:38:34,000
i now have a single for loop running

00:38:31,520 --> 00:38:36,240
from zero to n minus one

00:38:34,000 --> 00:38:37,200
blasting minus one into that into that

00:38:36,240 --> 00:38:40,240
vector

00:38:37,200 --> 00:38:42,000
and in a single i do the update so i

00:38:40,240 --> 00:38:43,359
correct that one element and that's why

00:38:42,000 --> 00:38:45,280
theoretically you're doing more work

00:38:43,359 --> 00:38:47,599
you're doing one extra assignment

00:38:45,280 --> 00:38:49,359
which is absolutely nothing in terms of

00:38:47,599 --> 00:38:53,359
extra cost but the saving is

00:38:49,359 --> 00:38:56,480
is substantial so of course i have the

00:38:53,359 --> 00:38:57,920
the no i could use the no way and um i

00:38:56,480 --> 00:38:59,440
can't use the no weight on the first

00:38:57,920 --> 00:39:00,000
loop by the way then i will get a race

00:38:59,440 --> 00:39:03,040
condition

00:39:00,000 --> 00:39:05,760
so i could still in this case could

00:39:03,040 --> 00:39:07,760
shave off one barrier i definitely have

00:39:05,760 --> 00:39:09,680
i only have one for loop so i have a

00:39:07,760 --> 00:39:10,880
reduced overhead but the big thing here

00:39:09,680 --> 00:39:12,640
of course is that

00:39:10,880 --> 00:39:15,119
now the performance only depends on the

00:39:12,640 --> 00:39:16,800
value of n which presumably is related

00:39:15,119 --> 00:39:18,880
to the problem size that i'm solving

00:39:16,800 --> 00:39:20,720
it's like this

00:39:18,880 --> 00:39:22,480
case the size of the graph this is from

00:39:20,720 --> 00:39:24,640
graph analysis as well

00:39:22,480 --> 00:39:25,920
so presumably that's large enough to

00:39:24,640 --> 00:39:28,480
make parallel computing

00:39:25,920 --> 00:39:30,160
worth doing so i eliminated the

00:39:28,480 --> 00:39:30,640
dependence of the performance on this

00:39:30,160 --> 00:39:33,839
thing called

00:39:30,640 --> 00:39:36,720
endpoint all right so

00:39:33,839 --> 00:39:38,079
these were all still fairly basic things

00:39:36,720 --> 00:39:39,599
i think if you stare

00:39:38,079 --> 00:39:42,640
long enough for the code you'll you'll

00:39:39,599 --> 00:39:44,640
find them and um

00:39:42,640 --> 00:39:45,680
the one complicating thing is as i said

00:39:44,640 --> 00:39:48,160
a few minutes ago

00:39:45,680 --> 00:39:50,000
is it maybe hidden the code will be ugly

00:39:48,160 --> 00:39:51,520
so it may take some time to really find

00:39:50,000 --> 00:39:55,040
out what's going on

00:39:51,520 --> 00:39:56,400
um my recommendation is be patient um

00:39:55,040 --> 00:39:58,160
you know keep on staring at the code

00:39:56,400 --> 00:40:00,640
keep on trying things and tackle

00:39:58,160 --> 00:40:02,560
issues one by one uh it's probably an

00:40:00,640 --> 00:40:04,480
iterative process you make a change

00:40:02,560 --> 00:40:06,160
and sometimes you see again sometimes

00:40:04,480 --> 00:40:09,040
things go slower

00:40:06,160 --> 00:40:10,880
um i could give endless talks on how

00:40:09,040 --> 00:40:12,720
i've made codes run slower but you're

00:40:10,880 --> 00:40:14,720
probably not very interested in that

00:40:12,720 --> 00:40:16,880
but such is real life of tuning you you

00:40:14,720 --> 00:40:18,079
get a brilliant idea you type it in and

00:40:16,880 --> 00:40:19,760
it's worse

00:40:18,079 --> 00:40:21,599
and later you realize it wasn't that

00:40:19,760 --> 00:40:23,359
brilliant after all so

00:40:21,599 --> 00:40:25,440
that's all part of the game but it does

00:40:23,359 --> 00:40:27,680
it does go into the tuning process

00:40:25,440 --> 00:40:30,000
it's just part of it it's not all kind

00:40:27,680 --> 00:40:31,920
of clear-cut

00:40:30,000 --> 00:40:33,440
the last part of this talk i want to

00:40:31,920 --> 00:40:36,640
talk about numa

00:40:33,440 --> 00:40:38,640
and um especially since about almost

00:40:36,640 --> 00:40:40,560
every system shipped these days has a no

00:40:38,640 --> 00:40:42,800
more architecture it used to be

00:40:40,560 --> 00:40:44,400
only your very large servers had a had

00:40:42,800 --> 00:40:47,119
enuma architecture

00:40:44,400 --> 00:40:48,160
but nowadays your two socket box or even

00:40:47,119 --> 00:40:50,000
even your single

00:40:48,160 --> 00:40:51,440
the single socket box can have pneuma

00:40:50,000 --> 00:40:54,720
inside the socket so

00:40:51,440 --> 00:40:57,280
it's everywhere so what's pneuma

00:40:54,720 --> 00:40:58,240
luma means that memory is physically

00:40:57,280 --> 00:41:00,400
distributed but

00:40:58,240 --> 00:41:01,920
logically shared so the memory chips are

00:41:00,400 --> 00:41:04,839
scattered over the system

00:41:01,920 --> 00:41:06,000
but to you it appears as one single

00:41:04,839 --> 00:41:08,319
memory

00:41:06,000 --> 00:41:10,000
sharing data is transparent you don't

00:41:08,319 --> 00:41:11,119
know where the data is you'll just get

00:41:10,000 --> 00:41:13,760
it somewhere from

00:41:11,119 --> 00:41:14,960
from wherever it is in the system it

00:41:13,760 --> 00:41:16,720
doesn't matter

00:41:14,960 --> 00:41:19,200
unless you care about performance of

00:41:16,720 --> 00:41:21,760
course if you care about performance you

00:41:19,200 --> 00:41:22,800
you got to have some some notion of

00:41:21,760 --> 00:41:26,880
where your data is

00:41:22,800 --> 00:41:29,839
and try to avoid it moves away too far

00:41:26,880 --> 00:41:30,400
so that's going to be the bulk of this

00:41:29,839 --> 00:41:33,520
this

00:41:30,400 --> 00:41:36,880
example i start with showing a fairly

00:41:33,520 --> 00:41:40,400
generic pneuma system i'm showing you a

00:41:36,880 --> 00:41:43,280
system here that has four nodes

00:41:40,400 --> 00:41:45,040
each node is is is like a parallel

00:41:43,280 --> 00:41:47,200
computer in itself

00:41:45,040 --> 00:41:48,560
it has a bunch of cores those are the

00:41:47,200 --> 00:41:51,440
green boxes

00:41:48,560 --> 00:41:53,440
it has a lot of several caches but

00:41:51,440 --> 00:41:54,640
there's always like a last level cache

00:41:53,440 --> 00:41:56,560
and typically in this kind of

00:41:54,640 --> 00:41:57,359
architecture that's shared so all the

00:41:56,560 --> 00:41:59,200
cores

00:41:57,359 --> 00:42:01,359
have access to that shared cache it's

00:41:59,200 --> 00:42:03,280
called llc last level cache

00:42:01,359 --> 00:42:05,680
and that connects to a portion of the

00:42:03,280 --> 00:42:06,560
memory what we see here is the memory is

00:42:05,680 --> 00:42:09,760
split in

00:42:06,560 --> 00:42:11,760
each node has 25 percent of the memory

00:42:09,760 --> 00:42:13,520
and there's some glue in between the

00:42:11,760 --> 00:42:16,720
cache coherent interconnect

00:42:13,520 --> 00:42:17,920
that glue makes it appear to us as a

00:42:16,720 --> 00:42:20,319
single memory

00:42:17,920 --> 00:42:22,880
so to you when you use top it will just

00:42:20,319 --> 00:42:25,040
report you the total size of the memory

00:42:22,880 --> 00:42:26,000
but in reality in the physical world

00:42:25,040 --> 00:42:28,000
it's split it

00:42:26,000 --> 00:42:29,119
and and the cache coherent interconnect

00:42:28,000 --> 00:42:32,000
makes it all work

00:42:29,119 --> 00:42:32,720
you need something you'll get it all

00:42:32,000 --> 00:42:35,040
right

00:42:32,720 --> 00:42:36,079
so to the developer this is what the

00:42:35,040 --> 00:42:38,000
system looked like

00:42:36,079 --> 00:42:40,319
you have places where you can run your

00:42:38,000 --> 00:42:42,839
threads where you can store your data

00:42:40,319 --> 00:42:44,640
and through some magic it's all

00:42:42,839 --> 00:42:46,240
connected

00:42:44,640 --> 00:42:47,760
but from performance point of view we

00:42:46,240 --> 00:42:48,640
got to start you know we've got to be

00:42:47,760 --> 00:42:50,800
careful

00:42:48,640 --> 00:42:52,480
because there's a notion of local versus

00:42:50,800 --> 00:42:54,800
remote access types

00:42:52,480 --> 00:42:56,880
so let's look at a simple example let's

00:42:54,800 --> 00:42:57,200
say my node is executing somewhere in

00:42:56,880 --> 00:43:00,160
that

00:42:57,200 --> 00:43:00,720
in that system and that's the purple box

00:43:00,160 --> 00:43:03,119
that might

00:43:00,720 --> 00:43:04,960
my threat is somewhat executed on a core

00:43:03,119 --> 00:43:06,640
somewhere in that system

00:43:04,960 --> 00:43:08,160
well where could the data be that it

00:43:06,640 --> 00:43:10,560
needs if the data is

00:43:08,160 --> 00:43:12,160
all in the memory connected directly

00:43:10,560 --> 00:43:14,640
connected to those cores

00:43:12,160 --> 00:43:16,480
i get the fastest memory access that's

00:43:14,640 --> 00:43:18,160
what we call local access and that's the

00:43:16,480 --> 00:43:18,880
best you can do you can't do any better

00:43:18,160 --> 00:43:22,560
that's the goal

00:43:18,880 --> 00:43:24,640
that's what you want but how do i know

00:43:22,560 --> 00:43:26,800
maybe my data is somewhere far away in

00:43:24,640 --> 00:43:29,280
the corner of the system

00:43:26,800 --> 00:43:30,480
again the hardware will take care of it

00:43:29,280 --> 00:43:32,800
you will get that

00:43:30,480 --> 00:43:33,680
will get that data to you but it could

00:43:32,800 --> 00:43:36,480
take a long time

00:43:33,680 --> 00:43:37,680
on the scale of things so that's pneuma

00:43:36,480 --> 00:43:39,440
in a nutshell

00:43:37,680 --> 00:43:41,359
and what you really want to do when

00:43:39,440 --> 00:43:43,520
using a numa system is avoid

00:43:41,359 --> 00:43:45,359
that second situation you want to avoid

00:43:43,520 --> 00:43:47,920
that your data is far away from where

00:43:45,359 --> 00:43:50,480
you execute

00:43:47,920 --> 00:43:53,359
so it's all about keeping the data and

00:43:50,480 --> 00:43:53,359
the threads closed

00:43:53,440 --> 00:43:59,520
and in openmp the choice was made to

00:43:56,560 --> 00:44:00,480
move a thread to the data instead of the

00:43:59,520 --> 00:44:02,079
data to the thread

00:44:00,480 --> 00:44:03,920
and the reason is very simple it's a lot

00:44:02,079 --> 00:44:05,040
easier and cheaper to move a thread and

00:44:03,920 --> 00:44:08,319
move data

00:44:05,040 --> 00:44:11,280
so the model in openmp is that

00:44:08,319 --> 00:44:12,480
you have the data somewhere and what

00:44:11,280 --> 00:44:14,880
you're going to do

00:44:12,480 --> 00:44:16,319
to try to do is make sure your threads

00:44:14,880 --> 00:44:18,880
are close to that data

00:44:16,319 --> 00:44:19,680
so this this is the concept in openmp

00:44:18,880 --> 00:44:22,720
something to keep

00:44:19,680 --> 00:44:24,880
in mind like i said the other way around

00:44:22,720 --> 00:44:26,800
is much more expensive so that's not

00:44:24,880 --> 00:44:28,880
done

00:44:26,800 --> 00:44:30,319
openmp has what we call the affinity

00:44:28,880 --> 00:44:32,240
constructs and they

00:44:30,319 --> 00:44:33,359
they allow us to control where threads

00:44:32,240 --> 00:44:35,440
run

00:44:33,359 --> 00:44:36,720
so that's what we're going to do i can't

00:44:35,440 --> 00:44:39,440
stress that enough you

00:44:36,720 --> 00:44:41,760
you you sort of ought to know where your

00:44:39,440 --> 00:44:42,560
data is and i'll get back to that in a

00:44:41,760 --> 00:44:45,040
minute

00:44:42,560 --> 00:44:46,240
but you sort of know where the data is

00:44:45,040 --> 00:44:47,280
and then you're going to make sure that

00:44:46,240 --> 00:44:50,560
the threads that

00:44:47,280 --> 00:44:53,599
need that data are close to it okay

00:44:50,560 --> 00:44:55,200
it's very powerful uh but

00:44:53,599 --> 00:44:57,119
you're in charge it's up to you to get

00:44:55,200 --> 00:44:57,760
this right and and i want to be clear on

00:44:57,119 --> 00:44:59,520
that

00:44:57,760 --> 00:45:01,760
right in this context means good

00:44:59,520 --> 00:45:03,440
performance you'll always get

00:45:01,760 --> 00:45:05,520
if you have correctly paralyzed your

00:45:03,440 --> 00:45:07,440
code you always get the right result

00:45:05,520 --> 00:45:09,119
regardless of new model this is about

00:45:07,440 --> 00:45:10,720
performance so when i say it's up to you

00:45:09,119 --> 00:45:14,079
to get this right this is about

00:45:10,720 --> 00:45:14,640
the performance of course right and you

00:45:14,079 --> 00:45:18,480
get help

00:45:14,640 --> 00:45:21,520
through two environment variables okay

00:45:18,480 --> 00:45:25,599
one is called omp places

00:45:21,520 --> 00:45:28,240
omp places is a set you define a set

00:45:25,599 --> 00:45:30,160
and that set defines where threads are

00:45:28,240 --> 00:45:31,920
allowed to execute

00:45:30,160 --> 00:45:33,520
and with that comes the nice feature

00:45:31,920 --> 00:45:36,160
that you can restrict

00:45:33,520 --> 00:45:37,680
where threads are being executed you can

00:45:36,160 --> 00:45:39,440
control you can exclude

00:45:37,680 --> 00:45:41,040
things i don't want you to run there i

00:45:39,440 --> 00:45:42,079
want you to run there so that's really

00:45:41,040 --> 00:45:43,599
very powerful

00:45:42,079 --> 00:45:45,599
and that's controlled to only three

00:45:43,599 --> 00:45:47,839
places and you'll see several examples

00:45:45,599 --> 00:45:49,680
of that

00:45:47,839 --> 00:45:51,839
you can go down and that's going to be

00:45:49,680 --> 00:45:54,319
my example to the raw

00:45:51,839 --> 00:45:56,880
low-level hardware thread ideas but

00:45:54,319 --> 00:46:00,240
there's also symbolic names like sockets

00:45:56,880 --> 00:46:00,960
cores and threads so you can say i want

00:46:00,240 --> 00:46:03,680
to have

00:46:00,960 --> 00:46:04,720
i want my threads to run on one socket i

00:46:03,680 --> 00:46:06,560
don't care which one

00:46:04,720 --> 00:46:08,000
and you shouldn't really care but i

00:46:06,560 --> 00:46:09,280
don't want you to go outside of the

00:46:08,000 --> 00:46:11,359
socket

00:46:09,280 --> 00:46:12,960
or you can say i want to use the cores

00:46:11,359 --> 00:46:14,319
in the system and you can specify how

00:46:12,960 --> 00:46:16,000
many you want to use

00:46:14,319 --> 00:46:18,160
so those are symbolic names and then the

00:46:16,000 --> 00:46:20,079
hardware will figure something out

00:46:18,160 --> 00:46:22,319
my example is going to be about using

00:46:20,079 --> 00:46:26,240
hardware thread ideas that's like the

00:46:22,319 --> 00:46:27,920
assembly level coding equivalent

00:46:26,240 --> 00:46:30,319
but it gives you full control and i want

00:46:27,920 --> 00:46:33,599
to show you how you can do that

00:46:30,319 --> 00:46:35,760
in addition to that there's a proc bind

00:46:33,599 --> 00:46:36,319
variable that controls how to map the

00:46:35,760 --> 00:46:40,000
threads

00:46:36,319 --> 00:46:42,000
onto those places and

00:46:40,000 --> 00:46:43,839
maybe sound vague but if you see the

00:46:42,000 --> 00:46:44,720
choices it starts to make sense you can

00:46:43,839 --> 00:46:47,040
say

00:46:44,720 --> 00:46:49,040
um i want to be close to the master set

00:46:47,040 --> 00:46:50,800
although i think that's going to be

00:46:49,040 --> 00:46:52,160
i think that's going to be phased out

00:46:50,800 --> 00:46:54,560
that feature um

00:46:52,160 --> 00:46:56,160
it's not very very helpful the most the

00:46:54,560 --> 00:46:58,640
commonly used ones are close

00:46:56,160 --> 00:46:59,599
and spread you can say i want the

00:46:58,640 --> 00:47:01,280
threads to be closed

00:46:59,599 --> 00:47:02,880
because they're sharing a cache and that

00:47:01,280 --> 00:47:03,520
is really good and i want them to be

00:47:02,880 --> 00:47:06,560
close

00:47:03,520 --> 00:47:08,400
so um or i want them to be spread out

00:47:06,560 --> 00:47:09,599
and those are the choices you have for

00:47:08,400 --> 00:47:11,920
the proc bind

00:47:09,599 --> 00:47:12,800
so let's look at an example little

00:47:11,920 --> 00:47:15,920
script

00:47:12,800 --> 00:47:17,760
i'm using 16 openmp threads

00:47:15,920 --> 00:47:19,040
let's say i just wanted to use two

00:47:17,760 --> 00:47:22,160
sockets

00:47:19,040 --> 00:47:23,040
i say omp places equal sockets and the

00:47:22,160 --> 00:47:24,720
number two says

00:47:23,040 --> 00:47:27,280
give me two sockets again you don't know

00:47:24,720 --> 00:47:29,440
which ones and you really shouldn't care

00:47:27,280 --> 00:47:30,559
but whenever i have threads i want to

00:47:29,440 --> 00:47:34,000
spread them

00:47:30,559 --> 00:47:35,520
as far as possible so the proc bind is

00:47:34,000 --> 00:47:37,440
set to spread so you want to push them

00:47:35,520 --> 00:47:39,599
out so if you have two threads

00:47:37,440 --> 00:47:40,640
you would expect that the runtime system

00:47:39,599 --> 00:47:42,400
will give you

00:47:40,640 --> 00:47:44,000
assignment to each each one of those

00:47:42,400 --> 00:47:46,559
sockets not

00:47:44,000 --> 00:47:48,400
schedule two threads into one socket so

00:47:46,559 --> 00:47:50,800
with spread you really push them up

00:47:48,400 --> 00:47:52,640
and then you run the code so that's

00:47:50,800 --> 00:47:53,280
that's how easy it is to control these

00:47:52,640 --> 00:47:54,800
things

00:47:53,280 --> 00:47:58,319
now of course the devil is in the fact

00:47:54,800 --> 00:47:58,319
that how do you get to those choices

00:47:58,400 --> 00:48:02,079
and that goes back to the question where

00:48:00,160 --> 00:48:05,359
is data allocated

00:48:02,079 --> 00:48:07,040
how do i know where my data is and about

00:48:05,359 --> 00:48:09,760
every operating system i know

00:48:07,040 --> 00:48:11,599
i should say multi-user operating system

00:48:09,760 --> 00:48:14,480
uses what is called first touch

00:48:11,599 --> 00:48:16,319
placement policy what it does it

00:48:14,480 --> 00:48:17,839
allocates the page this is all at the

00:48:16,319 --> 00:48:20,880
virtual memory data page

00:48:17,839 --> 00:48:22,480
level so fairly large chunk of data four

00:48:20,880 --> 00:48:25,359
kilobyte eight kilobyte

00:48:22,480 --> 00:48:26,800
maybe larger but what first touch does

00:48:25,359 --> 00:48:29,359
it allocates the

00:48:26,800 --> 00:48:31,599
in the memory closest to the thread

00:48:29,359 --> 00:48:33,760
accessing this page for the first time

00:48:31,599 --> 00:48:35,520
it really does as the name suggests the

00:48:33,760 --> 00:48:36,319
thread that touches the data for the

00:48:35,520 --> 00:48:38,960
first time

00:48:36,319 --> 00:48:40,800
becomes the owner of that data and that

00:48:38,960 --> 00:48:43,359
controls where the data will go

00:48:40,800 --> 00:48:45,839
the data will be mapped into the memory

00:48:43,359 --> 00:48:47,359
of wherever that thread is executing

00:48:45,839 --> 00:48:49,359
in whatever node that thread is

00:48:47,359 --> 00:48:51,119
executing we call that a home node

00:48:49,359 --> 00:48:54,800
that's going to be the home node

00:48:51,119 --> 00:48:56,319
for that data and that's fixed right

00:48:54,800 --> 00:48:58,240
and it makes perfect sense for

00:48:56,319 --> 00:48:59,920
sequential applications and that's why

00:48:58,240 --> 00:49:01,119
it's the default for linux and other

00:48:59,920 --> 00:49:03,200
os's

00:49:01,119 --> 00:49:05,119
because that's the right thing to do it

00:49:03,200 --> 00:49:08,240
may not be the right thing to do in a

00:49:05,119 --> 00:49:11,359
parallel application however

00:49:08,240 --> 00:49:13,599
so what if a single thread initializes

00:49:11,359 --> 00:49:14,880
most or all of the data that means that

00:49:13,599 --> 00:49:17,040
threat owns the data

00:49:14,880 --> 00:49:20,000
all that data will end up in the memory

00:49:17,040 --> 00:49:21,920
where that threat executes

00:49:20,000 --> 00:49:23,280
that's probably not a desirable

00:49:21,920 --> 00:49:25,040
situation

00:49:23,280 --> 00:49:26,800
because any other threat that needs a

00:49:25,040 --> 00:49:29,920
data will have to go and get it

00:49:26,800 --> 00:49:31,440
wherever it is it costs time in the

00:49:29,920 --> 00:49:33,040
worst case if there's a lot of threats

00:49:31,440 --> 00:49:34,319
you can even get congestion at the

00:49:33,040 --> 00:49:36,640
memory controller

00:49:34,319 --> 00:49:39,200
so all sort of bad things happen then so

00:49:36,640 --> 00:49:41,280
this is probably not what you want

00:49:39,200 --> 00:49:44,000
now luckily the solution is often not

00:49:41,280 --> 00:49:46,559
always but often surprisingly simple

00:49:44,000 --> 00:49:47,599
parallelize where the data gets

00:49:46,559 --> 00:49:49,680
initialized

00:49:47,599 --> 00:49:52,400
because that's remember first touch

00:49:49,680 --> 00:49:55,280
that's when you first touch the data

00:49:52,400 --> 00:49:58,640
and here's an absolutely trivial example

00:49:55,280 --> 00:50:01,760
i have a loop that initializes a to zero

00:49:58,640 --> 00:50:02,800
if i wouldn't do anything in terms of

00:50:01,760 --> 00:50:05,359
parallelization

00:50:02,800 --> 00:50:06,480
this this loop would be executed by

00:50:05,359 --> 00:50:08,319
sequentially

00:50:06,480 --> 00:50:09,599
and that means the thread the master

00:50:08,319 --> 00:50:12,640
thread the main thread

00:50:09,599 --> 00:50:14,240
would own all that data

00:50:12,640 --> 00:50:16,160
that's probably not what i want and you

00:50:14,240 --> 00:50:19,599
can very easily solve that by

00:50:16,160 --> 00:50:21,520
executing this in parallel um the

00:50:19,599 --> 00:50:23,440
as you know the loop will be split up

00:50:21,520 --> 00:50:26,079
each threat will work on a chunk of a

00:50:23,440 --> 00:50:26,800
so each set will have a block of a in

00:50:26,079 --> 00:50:29,200
its member

00:50:26,800 --> 00:50:32,720
okay in that way i sort of distribute a

00:50:29,200 --> 00:50:32,720
across the nodes of my system

00:50:33,119 --> 00:50:36,480
a little bit more complex example but

00:50:35,520 --> 00:50:38,319
still uh

00:50:36,480 --> 00:50:40,640
still pretty simple it's the good old

00:50:38,319 --> 00:50:42,400
matrix vector algorithm by multiplying a

00:50:40,640 --> 00:50:44,480
matrix with a vector

00:50:42,400 --> 00:50:45,839
using in c i using the standard dot

00:50:44,480 --> 00:50:49,200
product way

00:50:45,839 --> 00:50:52,000
and extremely simple to parallelize with

00:50:49,200 --> 00:50:54,319
openmp it's a no-brainer it's a one-line

00:50:52,000 --> 00:50:56,240
parallel four because i'm taking the dot

00:50:54,319 --> 00:50:57,280
product of the rows of the matrix times

00:50:56,240 --> 00:50:59,200
the vector

00:50:57,280 --> 00:51:00,960
and uh although dot products are

00:50:59,200 --> 00:51:02,960
independent so i can simply

00:51:00,960 --> 00:51:04,960
to use a parallel four over the outer

00:51:02,960 --> 00:51:07,599
loop of this this algorithm

00:51:04,960 --> 00:51:08,559
i hope that's sort of clear and of

00:51:07,599 --> 00:51:10,240
course

00:51:08,559 --> 00:51:14,880
i need to initialize my data at some

00:51:10,240 --> 00:51:16,400
point and so that's what i'm doing here

00:51:14,880 --> 00:51:18,079
so what's wrong well there's nothing

00:51:16,400 --> 00:51:19,760
wrong with this code it's correct

00:51:18,079 --> 00:51:21,839
correct parallel code but it's not

00:51:19,760 --> 00:51:23,920
numeral aware

00:51:21,839 --> 00:51:25,760
in particular the data initialization i

00:51:23,920 --> 00:51:27,359
briefly showed is sequential i didn't

00:51:25,760 --> 00:51:30,960
use openmp there

00:51:27,359 --> 00:51:31,839
so wherever i initialize the data that

00:51:30,960 --> 00:51:34,800
thread will own

00:51:31,839 --> 00:51:36,640
all of my data and it will end up in the

00:51:34,800 --> 00:51:38,160
memory of wherever that thread is

00:51:36,640 --> 00:51:40,160
executed

00:51:38,160 --> 00:51:41,920
there's a more numeral friendly way to

00:51:40,160 --> 00:51:45,119
do this and um

00:51:41,920 --> 00:51:46,480
it gets more elaborate i'll in terms of

00:51:45,119 --> 00:51:48,160
time i'll speed up a bit

00:51:46,480 --> 00:51:50,880
but what i'm doing here is i'm

00:51:48,160 --> 00:51:52,960
parallelizing the data initialization

00:51:50,880 --> 00:51:54,319
i do reverse engineering on the

00:51:52,960 --> 00:51:57,359
algorithm i'm going to

00:51:54,319 --> 00:52:00,240
see in the algorithm i know that

00:51:57,359 --> 00:52:01,520
a threat will get a number of rows of

00:52:00,240 --> 00:52:03,839
the matrix

00:52:01,520 --> 00:52:04,800
so it really should have those rows in

00:52:03,839 --> 00:52:07,119
its memory

00:52:04,800 --> 00:52:08,079
so i'm parallelizing the initialization

00:52:07,119 --> 00:52:10,800
of the

00:52:08,079 --> 00:52:11,280
of the matrix b according to that that

00:52:10,800 --> 00:52:14,800
idea

00:52:11,280 --> 00:52:17,119
so um the first o and p4

00:52:14,800 --> 00:52:18,960
will take a slice of b a number of rows

00:52:17,119 --> 00:52:22,319
of b for each thread

00:52:18,960 --> 00:52:23,760
and they will all own a part of b i hope

00:52:22,319 --> 00:52:26,240
that is sort of clear

00:52:23,760 --> 00:52:28,400
i'll skip the one in the red i i also

00:52:26,240 --> 00:52:30,000
want to initialize the result vector

00:52:28,400 --> 00:52:31,680
because that suffers from the same

00:52:30,000 --> 00:52:33,200
pneuma problem if i don't

00:52:31,680 --> 00:52:34,800
initialize this is a redundant

00:52:33,200 --> 00:52:37,359
initialization

00:52:34,800 --> 00:52:37,839
it's a very small effect it won't matter

00:52:37,359 --> 00:52:40,319
but as

00:52:37,839 --> 00:52:41,920
i say you know don't leave anything on

00:52:40,319 --> 00:52:42,559
the table but you probably can't even

00:52:41,920 --> 00:52:44,240
measure it

00:52:42,559 --> 00:52:45,760
i just want to show you that sometimes

00:52:44,240 --> 00:52:46,960
you need to do something that's not

00:52:45,760 --> 00:52:48,880
entirely obvious

00:52:46,960 --> 00:52:52,000
and in this case that's to initialize

00:52:48,880 --> 00:52:54,240
the pre-initialize the result vector

00:52:52,000 --> 00:52:55,599
which shouldn't be necessary from a from

00:52:54,240 --> 00:52:57,040
a correctness point of view

00:52:55,599 --> 00:53:00,400
i just want to make sure that a is

00:52:57,040 --> 00:53:03,440
distributed as well i also distribute c

00:53:00,400 --> 00:53:06,559
and in that way i have scattered my data

00:53:03,440 --> 00:53:08,319
over the system all right

00:53:06,559 --> 00:53:09,680
now before i show the results i want to

00:53:08,319 --> 00:53:12,240
show you

00:53:09,680 --> 00:53:13,599
two useful tools i use them all the time

00:53:12,240 --> 00:53:16,319
there are more than that

00:53:13,599 --> 00:53:19,280
but one of them is called ls cpu it

00:53:16,319 --> 00:53:21,680
shows you how things work

00:53:19,280 --> 00:53:22,559
in the system and there's numer control

00:53:21,680 --> 00:53:25,440
that shows you

00:53:22,559 --> 00:53:27,040
especially about the relative latency so

00:53:25,440 --> 00:53:27,440
this is how you where you can get them

00:53:27,040 --> 00:53:29,839
if you

00:53:27,440 --> 00:53:30,880
like to use them but i want to show the

00:53:29,839 --> 00:53:33,359
result

00:53:30,880 --> 00:53:34,880
so i'm using an older amd system and i

00:53:33,359 --> 00:53:36,720
apologize to people i

00:53:34,880 --> 00:53:38,079
i do that because this is an interesting

00:53:36,720 --> 00:53:40,000
luma box but

00:53:38,079 --> 00:53:42,480
uh since then amd has made a lot of

00:53:40,000 --> 00:53:43,920
progress so uh

00:53:42,480 --> 00:53:45,599
don't take me wrong i just wanted to

00:53:43,920 --> 00:53:48,000
have a nice lumo box to play with

00:53:45,599 --> 00:53:49,200
and this this is what is the older uh

00:53:48,000 --> 00:53:50,720
amd server

00:53:49,200 --> 00:53:52,240
so how can you tell what the new my

00:53:50,720 --> 00:53:55,280
topology is

00:53:52,240 --> 00:53:57,200
well type in lscpu

00:53:55,280 --> 00:53:59,200
and what it will tell you is something

00:53:57,200 --> 00:54:00,800
like this and you see the words you've

00:53:59,200 --> 00:54:03,839
got a whole bunch of output

00:54:00,800 --> 00:54:05,040
but you'll see the words numa node and

00:54:03,839 --> 00:54:07,359
followed by a number

00:54:05,040 --> 00:54:08,800
so we see the number zero to seven so it

00:54:07,359 --> 00:54:11,119
means there are eight pneuma nodes in

00:54:08,800 --> 00:54:13,359
this system

00:54:11,119 --> 00:54:14,880
we see a column zero two seven that

00:54:13,359 --> 00:54:16,720
means that's eight entries it means

00:54:14,880 --> 00:54:19,760
eight cores

00:54:16,720 --> 00:54:21,599
so each pneuma node has eight cores and

00:54:19,760 --> 00:54:23,119
it has two hardware threads per chord

00:54:21,599 --> 00:54:24,880
that's the two columns

00:54:23,119 --> 00:54:26,559
comma separated columns that means that

00:54:24,880 --> 00:54:27,920
two threads if there were three you

00:54:26,559 --> 00:54:30,079
would see three columns

00:54:27,920 --> 00:54:31,359
and these are the low level hardware

00:54:30,079 --> 00:54:33,839
thread ideas

00:54:31,359 --> 00:54:36,319
that i can use to control where things

00:54:33,839 --> 00:54:36,319
are running

00:54:37,119 --> 00:54:41,920
numa control does h oop i'm sorry the

00:54:40,079 --> 00:54:42,799
number control does dash eight is really

00:54:41,920 --> 00:54:46,079
useful

00:54:42,799 --> 00:54:48,240
it um it shows me the relative latency

00:54:46,079 --> 00:54:49,920
scale to 10 and the numbers are not the

00:54:48,240 --> 00:54:50,720
real latency there's some relative

00:54:49,920 --> 00:54:52,559
metric

00:54:50,720 --> 00:54:54,079
but what you see when we look at the top

00:54:52,559 --> 00:54:56,799
line we see node zero

00:54:54,079 --> 00:54:57,839
is connected of course to zero and seven

00:54:56,799 --> 00:55:01,200
other nodes

00:54:57,839 --> 00:55:01,920
one through seven its own scale latency

00:55:01,200 --> 00:55:04,480
is ten

00:55:01,920 --> 00:55:06,079
i don't know i know why it's not one but

00:55:04,480 --> 00:55:06,720
you would expect it to be scaled to one

00:55:06,079 --> 00:55:09,280
but

00:55:06,720 --> 00:55:11,359
okay it's ten that's the normalized then

00:55:09,280 --> 00:55:11,760
we see three nodes that that are further

00:55:11,359 --> 00:55:13,839
away

00:55:11,760 --> 00:55:14,799
they have a latency of 16. so they're

00:55:13,839 --> 00:55:16,480
further away

00:55:14,799 --> 00:55:18,960
again you can't take this as the real

00:55:16,480 --> 00:55:20,880
latency number some sort of it

00:55:18,960 --> 00:55:22,160
but definitely these nodes three nodes

00:55:20,880 --> 00:55:24,000
are further away

00:55:22,160 --> 00:55:26,960
and then there are four nodes four five

00:55:24,000 --> 00:55:28,720
six and seven that are even further away

00:55:26,960 --> 00:55:30,319
so i have two levels of pneuma in this

00:55:28,720 --> 00:55:32,240
box so

00:55:30,319 --> 00:55:34,799
from this output what i learned is there

00:55:32,240 --> 00:55:36,480
are eight pneuma nodes each pneuma node

00:55:34,799 --> 00:55:38,960
has eight cores

00:55:36,480 --> 00:55:40,559
each core has two hardware threads so in

00:55:38,960 --> 00:55:43,599
total i have 64

00:55:40,559 --> 00:55:45,280
cores and 128 hardware threads

00:55:43,599 --> 00:55:46,720
and there are two clusters with remote

00:55:45,280 --> 00:55:49,440
nodes one cluster

00:55:46,720 --> 00:55:50,720
is has some sort of latency of 16 those

00:55:49,440 --> 00:55:52,559
are three

00:55:50,720 --> 00:55:53,760
and there are four others that have a

00:55:52,559 --> 00:55:57,200
higher latency

00:55:53,760 --> 00:55:58,880
and i label them 32. all right so that's

00:55:57,200 --> 00:56:00,880
what we that's what we learned from this

00:55:58,880 --> 00:56:02,559
output it's very very valuable

00:56:00,880 --> 00:56:04,880
it allows you to make a diagram like

00:56:02,559 --> 00:56:06,880
this so i have my center note

00:56:04,880 --> 00:56:09,119
my center node is connected to three

00:56:06,880 --> 00:56:10,240
remote nodes with a with a longer access

00:56:09,119 --> 00:56:13,359
time the 16

00:56:10,240 --> 00:56:16,319
class of nodes and four that are

00:56:13,359 --> 00:56:17,280
32 labeled 32 so i have two levels of

00:56:16,319 --> 00:56:20,319
pneuma in this

00:56:17,280 --> 00:56:21,680
in this system right so zooming in a bit

00:56:20,319 --> 00:56:23,760
on those numbers

00:56:21,680 --> 00:56:24,799
here i show you the numbers the thread

00:56:23,760 --> 00:56:28,160
numbers for

00:56:24,799 --> 00:56:30,799
a node number zero we see we have eight

00:56:28,160 --> 00:56:32,240
cores that's the yellow box and in those

00:56:30,799 --> 00:56:33,359
vertical bars are the two hardware

00:56:32,240 --> 00:56:35,119
threads so that the

00:56:33,359 --> 00:56:37,680
the two hardware threads in the first

00:56:35,119 --> 00:56:38,880
core are zero and sixty four not zero

00:56:37,680 --> 00:56:40,640
and one

00:56:38,880 --> 00:56:42,559
uh in the second chord the first

00:56:40,640 --> 00:56:43,280
hardware thread is one the second one is

00:56:42,559 --> 00:56:45,520
00:56:43,280 --> 00:56:46,559
and so forth that's what we get from

00:56:45,520 --> 00:56:48,480
those numbers

00:56:46,559 --> 00:56:50,799
so now we know exactly what the system

00:56:48,480 --> 00:56:53,359
looks like and we can address

00:56:50,799 --> 00:56:53,920
those individual hardware threads at the

00:56:53,359 --> 00:56:56,640
open b

00:56:53,920 --> 00:56:57,599
level so that's what we do with the omb

00:56:56,640 --> 00:57:00,160
place

00:56:57,599 --> 00:57:01,359
okay i just said this a little while ago

00:57:00,160 --> 00:57:03,359
and i said i pre

00:57:01,359 --> 00:57:04,720
you know they prefer to be used but

00:57:03,359 --> 00:57:05,839
sometimes you want to go for the low

00:57:04,720 --> 00:57:08,880
level numbers

00:57:05,839 --> 00:57:10,640
and that's what i'm going to do okay so

00:57:08,880 --> 00:57:11,920
let's say i set myself the goal to

00:57:10,640 --> 00:57:14,000
distribute the open and b

00:57:11,920 --> 00:57:16,000
sets evenly across the cores and the

00:57:14,000 --> 00:57:17,280
nodes i want to spread them out over my

00:57:16,000 --> 00:57:19,520
entire system

00:57:17,280 --> 00:57:20,400
for example i want to use the first

00:57:19,520 --> 00:57:22,559
hardware thread

00:57:20,400 --> 00:57:24,960
of the first two cores of all the nodes

00:57:22,559 --> 00:57:25,520
okay i'll show you a diagram what i mean

00:57:24,960 --> 00:57:27,839
with that

00:57:25,520 --> 00:57:28,960
okay so this is what i want to do node

00:57:27,839 --> 00:57:31,520
node 0

00:57:28,960 --> 00:57:33,200
i want to use the light blue bar that's

00:57:31,520 --> 00:57:36,960
the first hardware thread in the

00:57:33,200 --> 00:57:40,160
in the core and that number we know is 0

00:57:36,960 --> 00:57:43,119
1 and and so forth that's the first

00:57:40,160 --> 00:57:44,640
first hardware thread in in each core

00:57:43,119 --> 00:57:47,839
and likewise in the second

00:57:44,640 --> 00:57:50,559
node it's 8 9 and then we've got 16 17

00:57:47,839 --> 00:57:51,200
24 25. so when you look at those numbers

00:57:50,559 --> 00:57:53,599
for a while

00:57:51,200 --> 00:57:54,799
you'll see that these are the numbers

00:57:53,599 --> 00:57:57,200
that will give me

00:57:54,799 --> 00:57:59,200
the access to those hardware threads in

00:57:57,200 --> 00:58:01,599
these cores

00:57:59,200 --> 00:58:04,960
so how do i do that with openmp well

00:58:01,599 --> 00:58:07,200
recall this was the lscpu output

00:58:04,960 --> 00:58:10,000
and i'm going to define my places using

00:58:07,200 --> 00:58:12,559
the set notation supported in openmp

00:58:10,000 --> 00:58:14,400
i start the first one i start with zero

00:58:12,559 --> 00:58:16,240
that's when the curly braces

00:58:14,400 --> 00:58:18,079
i want to have eight elements and the

00:58:16,240 --> 00:58:20,480
stride should be eight

00:58:18,079 --> 00:58:22,319
the next part of the set starts at one

00:58:20,480 --> 00:58:23,040
again eight elements with a stride of

00:58:22,319 --> 00:58:25,119
eight

00:58:23,040 --> 00:58:27,119
and all together that gives me this this

00:58:25,119 --> 00:58:29,040
sequence of numbers

00:58:27,119 --> 00:58:31,280
and when you go back into look into the

00:58:29,040 --> 00:58:33,280
diagram and the ls cpu output you see

00:58:31,280 --> 00:58:36,400
that exactly that's exactly what i

00:58:33,280 --> 00:58:38,559
what i wanted to accomplish in this case

00:58:36,400 --> 00:58:40,400
it doesn't really matter what i do but i

00:58:38,559 --> 00:58:43,200
want to keep them close

00:58:40,400 --> 00:58:44,640
and i'm using 16 threads i have 16

00:58:43,200 --> 00:58:47,520
places and i want to use

00:58:44,640 --> 00:58:47,920
16 threads note that when you go this

00:58:47,520 --> 00:58:50,400
way

00:58:47,920 --> 00:58:51,200
i use omp display and the environment

00:58:50,400 --> 00:58:52,960
variable

00:58:51,200 --> 00:58:54,799
it's really helpful because it echoes

00:58:52,960 --> 00:58:56,640
your settings it has saved me more than

00:58:54,799 --> 00:58:59,680
once

00:58:56,640 --> 00:59:01,760
and here's the result the i'm showing

00:58:59,680 --> 00:59:03,920
this for 64 threads

00:59:01,760 --> 00:59:05,200
in a cpu intensive algorithm like this

00:59:03,920 --> 00:59:07,119
using hyper threading

00:59:05,200 --> 00:59:09,520
doesn't make much sense so i'm not using

00:59:07,119 --> 00:59:12,559
my 128 threads i'm using the

00:59:09,520 --> 00:59:14,319
only the first thread for each core uh i

00:59:12,559 --> 00:59:14,720
see the performance so don't look at the

00:59:14,319 --> 00:59:16,480
number

00:59:14,720 --> 00:59:18,720
precisely this is not the fastest

00:59:16,480 --> 00:59:21,119
algorithm it's my homegrown algorithm

00:59:18,720 --> 00:59:23,119
what i want to show you is that without

00:59:21,119 --> 00:59:25,280
the newman tuning without

00:59:23,119 --> 00:59:26,960
modifying the data initialization i get

00:59:25,280 --> 00:59:30,079
that purple dotted line

00:59:26,960 --> 00:59:32,640
and i barely get any scalability

00:59:30,079 --> 00:59:34,480
when i when i start you doing the numa

00:59:32,640 --> 00:59:36,640
stuff i see some funny behavior in

00:59:34,480 --> 00:59:37,920
between i didn't really look at that but

00:59:36,640 --> 00:59:38,640
it's kind of interesting behavior in

00:59:37,920 --> 00:59:41,040
between

00:59:38,640 --> 00:59:42,160
but all together i got a performance

00:59:41,040 --> 00:59:45,760
improvement of of

00:59:42,160 --> 00:59:47,280
factor of 22. oh i'm sorry a factor of

00:59:45,760 --> 00:59:49,119
22.

00:59:47,280 --> 00:59:50,880
so just by modifying my data

00:59:49,119 --> 00:59:52,960
initialization don't

00:59:50,880 --> 00:59:55,359
i didn't modify my algorithm i only

00:59:52,960 --> 00:59:56,880
modified my data initialization so

00:59:55,359 --> 00:59:58,480
so keep that in mind that's how

00:59:56,880 --> 01:00:01,839
important this can be

00:59:58,480 --> 01:00:03,599
all right the takeaways i hope to have

01:00:01,839 --> 01:00:04,160
shown that data and threat placement

01:00:03,599 --> 01:00:06,720
matters

01:00:04,160 --> 01:00:08,240
and it could be a lot uh leverage first

01:00:06,720 --> 01:00:10,480
touch it's a great way to figure out

01:00:08,240 --> 01:00:12,960
where your data is

01:00:10,480 --> 01:00:13,599
i do think openmp has very good powerful

01:00:12,960 --> 01:00:16,000
support

01:00:13,599 --> 01:00:17,760
it's very hard to make this abstract and

01:00:16,000 --> 01:00:19,119
powerful and generic but i think the

01:00:17,760 --> 01:00:22,240
committee working on this

01:00:19,119 --> 01:00:25,760
did a great job more has been added

01:00:22,240 --> 01:00:28,799
in 5.0 and 5.1

01:00:25,760 --> 01:00:31,280
and brings me to the finals

01:00:28,799 --> 01:00:32,559
tuning strategy again always use a

01:00:31,280 --> 01:00:34,640
profiling tool

01:00:32,559 --> 01:00:36,240
don't forget serial performance i didn't

01:00:34,640 --> 01:00:38,720
talk about data structures

01:00:36,240 --> 01:00:40,799
that could be another talk maybe you may

01:00:38,720 --> 01:00:44,720
want to spend time

01:00:40,799 --> 01:00:46,240
excuse me you may want to spend time on

01:00:44,720 --> 01:00:47,839
looking at your data structures again

01:00:46,240 --> 01:00:49,440
that wasn't even touched upon in this

01:00:47,839 --> 01:00:51,280
talk

01:00:49,440 --> 01:00:53,520
uh look at those cases like barriers

01:00:51,280 --> 01:00:54,160
redundant barriers uh too many parallel

01:00:53,520 --> 01:00:55,599
regions

01:00:54,160 --> 01:00:58,240
and and those kind of things what i call

01:00:55,599 --> 01:01:00,319
the low-hanging fruit address those

01:00:58,240 --> 01:01:02,079
and again don't forget about data

01:01:00,319 --> 01:01:04,799
placement and what i call

01:01:02,079 --> 01:01:06,000
is think ahead even when you see some

01:01:04,799 --> 01:01:08,319
small inefficiency

01:01:06,000 --> 01:01:10,880
fix it because what may not matter today

01:01:08,319 --> 01:01:13,200
may matter tomorrow all right

01:01:10,880 --> 01:01:15,119
thank you um stay tuned and remember

01:01:13,200 --> 01:01:18,319
it's not that openmp doesn't scale it's

01:01:15,119 --> 01:01:20,480
a bad opening it does not scale

01:01:18,319 --> 01:01:21,760
so if we're running a little over uh i

01:01:20,480 --> 01:01:23,839
hope you found this useful

01:01:21,760 --> 01:01:25,520
and we can start looking as far as i'm

01:01:23,839 --> 01:01:27,760
concerned we can start looking at the q

01:01:25,520 --> 01:01:27,760
a

01:01:37,520 --> 01:01:41,440
great rude um well we could go ahead

01:01:40,400 --> 01:01:44,480
lucy

01:01:41,440 --> 01:01:44,480
no you go ahead kathleen

01:01:45,599 --> 01:01:49,760
okay um there were some questions that

01:01:47,920 --> 01:01:51,359
were already answered but i don't really

01:01:49,760 --> 01:01:52,480
have a way of knowing if they were

01:01:51,359 --> 01:01:54,400
answered

01:01:52,480 --> 01:01:56,960
sufficiently or to the original

01:01:54,400 --> 01:02:01,440
requester's satisfaction

01:01:56,960 --> 01:02:03,359
um okay so for now

01:02:01,440 --> 01:02:04,480
great then let's just have you take it

01:02:03,359 --> 01:02:08,480
from there

01:02:04,480 --> 01:02:13,359
okay yeah yeah let me um

01:02:08,480 --> 01:02:15,119
let me see um all right

01:02:13,359 --> 01:02:18,079
uh first question is there an easy way

01:02:15,119 --> 01:02:21,280
to deal with dynamic scheduling and numa

01:02:18,079 --> 01:02:22,640
no not really um no that's because you

01:02:21,280 --> 01:02:24,480
don't know where threads will run i

01:02:22,640 --> 01:02:27,359
guess that's that's what's behind the

01:02:24,480 --> 01:02:29,359
the question unfortunately uh no one

01:02:27,359 --> 01:02:32,160
thing that i found useful for example

01:02:29,359 --> 01:02:34,000
um in graph analysis uh when you have

01:02:32,160 --> 01:02:36,640
random memory access and it's really

01:02:34,000 --> 01:02:37,200
hard to get data localization is sort of

01:02:36,640 --> 01:02:39,200
this

01:02:37,200 --> 01:02:41,280
distribute the data equally across the

01:02:39,200 --> 01:02:43,760
system and live with

01:02:41,280 --> 01:02:45,200
some sort of higher overall latency but

01:02:43,760 --> 01:02:47,599
at least uniform

01:02:45,200 --> 01:02:48,960
so you kind of make it a bit worse but

01:02:47,599 --> 01:02:52,720
it's uniform

01:02:48,960 --> 01:02:55,920
so that's that would be my my initial

01:02:52,720 --> 01:02:58,559
recommendation um

01:02:55,920 --> 01:02:58,559
let's see

01:02:59,200 --> 01:03:02,960
okay how to parallelize luke with luke

01:03:02,000 --> 01:03:06,240
candy

01:03:02,960 --> 01:03:09,680
uh well unless it's a very special case

01:03:06,240 --> 01:03:12,000
um the lootcare independence

01:03:09,680 --> 01:03:14,079
would you rude would you please repeat

01:03:12,000 --> 01:03:16,160
the question before you answer it for

01:03:14,079 --> 01:03:17,839
oh i thought i said that but okay this

01:03:16,160 --> 01:03:22,240
is fine um

01:03:17,839 --> 01:03:22,240
um wait a minute where's

01:03:22,720 --> 01:03:28,079
oh sorry i skipped the question that let

01:03:25,200 --> 01:03:32,400
me do um the second question from matt's

01:03:28,079 --> 01:03:35,599
why not have pragmatic master um

01:03:32,400 --> 01:03:38,319
yeah um i'm not sure it's a cheaper

01:03:35,599 --> 01:03:40,240
than than single i don't like the fact

01:03:38,319 --> 01:03:42,319
that it has the

01:03:40,240 --> 01:03:44,000
doesn't have the barrier i'd like to be

01:03:42,319 --> 01:03:46,319
explicit about it

01:03:44,000 --> 01:03:48,319
so also anybody else reading the code

01:03:46,319 --> 01:03:49,520
maybe not so familiar with the intricate

01:03:48,319 --> 01:03:52,240
details of openmp

01:03:49,520 --> 01:03:54,799
will see that i had a barrier but yeah

01:03:52,240 --> 01:03:56,000
you could do master

01:03:54,799 --> 01:03:58,240
not sure whether there's a lot of

01:03:56,000 --> 01:04:01,760
difference but um sure

01:03:58,240 --> 01:04:02,960
yeah yes but as the comment is i think

01:04:01,760 --> 01:04:04,799
it's deprecated

01:04:02,960 --> 01:04:06,000
indeed i think it will be phased out i

01:04:04,799 --> 01:04:08,799
don't know when

01:04:06,000 --> 01:04:11,039
but um um i think that the plan is to

01:04:08,799 --> 01:04:12,160
move away from it because single and a

01:04:11,039 --> 01:04:15,280
no weight will will

01:04:12,160 --> 01:04:15,760
accomplish the same so but yeah at this

01:04:15,280 --> 01:04:18,400
point

01:04:15,760 --> 01:04:19,359
at this point it will be equal um sorry

01:04:18,400 --> 01:04:21,200
the loop with the

01:04:19,359 --> 01:04:22,640
loop carry dependence you you can't just

01:04:21,200 --> 01:04:24,480
straightforward parallelize those

01:04:22,640 --> 01:04:25,359
because the algorithm is inherently not

01:04:24,480 --> 01:04:27,599
parallel

01:04:25,359 --> 01:04:29,680
there are ways to sometimes transform a

01:04:27,599 --> 01:04:32,559
loop carry dependence into something

01:04:29,680 --> 01:04:35,520
parallel usually you take a hit in terms

01:04:32,559 --> 01:04:37,599
of the efficiency so it may be slower

01:04:35,520 --> 01:04:39,440
in sequential mode and then you can

01:04:37,599 --> 01:04:41,920
parallelize but once you go that way you

01:04:39,440 --> 01:04:45,039
got to be careful that the slowdown

01:04:41,920 --> 01:04:46,079
is not impacting you too much so in

01:04:45,039 --> 01:04:48,400
general you can you can

01:04:46,079 --> 01:04:50,559
you can't do that there's the um there's

01:04:48,400 --> 01:04:52,400
one thing for like a wave type of

01:04:50,559 --> 01:04:54,799
solvers that openmp has

01:04:52,400 --> 01:04:56,319
the name escapes me now for the moment

01:04:54,799 --> 01:04:59,359
but there's um

01:04:56,319 --> 01:05:01,440
there is a way to to do certain type of

01:04:59,359 --> 01:05:02,880
wave like finite difference kind of

01:05:01,440 --> 01:05:09,520
schemes

01:05:02,880 --> 01:05:14,720
all right

01:05:09,520 --> 01:05:14,720
let me see is that minimum duration

01:05:15,280 --> 01:05:20,240
no no this is a minimum duration

01:05:18,319 --> 01:05:22,160
in other words when does it when is it

01:05:20,240 --> 01:05:24,720
efficient i think that's the question

01:05:22,160 --> 01:05:25,200
that does it depend on your system what

01:05:24,720 --> 01:05:28,720
i

01:05:25,200 --> 01:05:31,280
um well what i find that

01:05:28,720 --> 01:05:32,880
if you write the efficient code vote you

01:05:31,280 --> 01:05:36,400
know if you write it efficiently

01:05:32,880 --> 01:05:37,200
it's very very sort of generic gain that

01:05:36,400 --> 01:05:39,200
you have

01:05:37,200 --> 01:05:40,880
uh you could do of course theoretical

01:05:39,200 --> 01:05:43,440
sort of observations like what's the

01:05:40,880 --> 01:05:45,599
cost of this and what's the benefit

01:05:43,440 --> 01:05:47,599
in general when you do yeah you you if

01:05:45,599 --> 01:05:48,000
you have an initialization loop that is

01:05:47,599 --> 01:05:50,640
four

01:05:48,000 --> 01:05:52,000
elements long you wouldn't run that in

01:05:50,640 --> 01:05:54,880
parallel

01:05:52,000 --> 01:05:56,960
yeah so there is some sort of gray area

01:05:54,880 --> 01:05:58,799
where things do not make much sense

01:05:56,960 --> 01:06:00,960
again that's really where that's why you

01:05:58,799 --> 01:06:03,280
want to use a profiling tool to actually

01:06:00,960 --> 01:06:05,359
guide you

01:06:03,280 --> 01:06:08,000
so there are no rules really rules of

01:06:05,359 --> 01:06:08,000
thumb i think

01:06:11,359 --> 01:06:14,240
page 12.

01:06:16,160 --> 01:06:22,559
oh like um i'm not sure what things

01:06:19,200 --> 01:06:25,599
uh okay i see that mods already uh

01:06:22,559 --> 01:06:25,599
already applied

01:06:25,760 --> 01:06:29,520
yeah that was a very controversial thing

01:06:27,839 --> 01:06:31,119
that you didn't know

01:06:29,520 --> 01:06:33,520
who would pick up the next portion of

01:06:31,119 --> 01:06:36,000
work that has been formalized now and

01:06:33,520 --> 01:06:37,680
the openmp is doing what most people

01:06:36,000 --> 01:06:39,680
expected it to do

01:06:37,680 --> 01:06:41,920
so pick up the same work so you also get

01:06:39,680 --> 01:06:46,559
some cash benefits

01:06:41,920 --> 01:06:48,079
and so the the today the runtime system

01:06:46,559 --> 01:06:50,960
tries to do the smart thing

01:06:48,079 --> 01:06:52,480
and um and especially exploit the fact

01:06:50,960 --> 01:06:53,839
that things may still be in the cache of

01:06:52,480 --> 01:06:56,240
the thread executing it

01:06:53,839 --> 01:06:59,119
so the next block of code will be sort

01:06:56,240 --> 01:07:03,440
of split up in the same way

01:06:59,119 --> 01:07:03,839
uh um i didn't really at that point i

01:07:03,440 --> 01:07:07,200
didn't

01:07:03,839 --> 01:07:07,680
um turn off hyper threading um but yeah

01:07:07,200 --> 01:07:10,079
that's

01:07:07,680 --> 01:07:12,160
something you don't have to really

01:07:10,079 --> 01:07:13,680
disable hyper threading i know sometimes

01:07:12,160 --> 01:07:14,960
it works in terms of performance

01:07:13,680 --> 01:07:17,359
sometimes it doesn't

01:07:14,960 --> 01:07:19,119
as i was showing in the last example

01:07:17,359 --> 01:07:21,680
that's why i did that

01:07:19,119 --> 01:07:23,760
by mapping controlling the mapping of

01:07:21,680 --> 01:07:25,680
openmp threads onto the hardware thread

01:07:23,760 --> 01:07:28,000
to effectively

01:07:25,680 --> 01:07:29,760
not use that that hyper threat or those

01:07:28,000 --> 01:07:31,839
hyper threats

01:07:29,760 --> 01:07:33,599
you may want to you know disabling them

01:07:31,839 --> 01:07:35,920
entirely is rather brute force you may

01:07:33,599 --> 01:07:37,760
not have that control on your system

01:07:35,920 --> 01:07:39,520
but but at the open mp level you can

01:07:37,760 --> 01:07:41,680
just say i choose to ignore them

01:07:39,520 --> 01:07:42,559
the thing is that i find that very hard

01:07:41,680 --> 01:07:45,520
to do

01:07:42,559 --> 01:07:46,960
with the pre-fabricated keywords like

01:07:45,520 --> 01:07:49,839
threads

01:07:46,960 --> 01:07:51,839
and um i know i'm more like an assembly

01:07:49,839 --> 01:07:53,200
hacker maybe at that level but i usually

01:07:51,839 --> 01:07:55,200
go for the numbers

01:07:53,200 --> 01:07:56,880
but that's hard to get when you get

01:07:55,200 --> 01:07:58,880
started when you get started you start

01:07:56,880 --> 01:08:00,799
saying places equals course

01:07:58,880 --> 01:08:02,559
and you see some benefit and and often

01:08:00,799 --> 01:08:05,200
that gets you going

01:08:02,559 --> 01:08:05,680
um these are good good ways to uh to use

01:08:05,200 --> 01:08:07,280
it but

01:08:05,680 --> 01:08:09,039
when you really want to have that fine

01:08:07,280 --> 01:08:09,680
control use those numbers and in that

01:08:09,039 --> 01:08:12,000
way

01:08:09,680 --> 01:08:13,440
use whatever hyper thread or not use

01:08:12,000 --> 01:08:17,359
whatever hyperthread

01:08:13,440 --> 01:08:20,799
there there is um let me see

01:08:17,359 --> 01:08:24,080
would it be a good idea to of course yes

01:08:20,799 --> 01:08:26,400
yeah definitely the um i

01:08:24,080 --> 01:08:28,400
want to dig deeper into that that

01:08:26,400 --> 01:08:29,759
tasking could be a very good alternative

01:08:28,400 --> 01:08:32,159
to dynamic scheduling

01:08:29,759 --> 01:08:32,799
not always though we shouldn't forget

01:08:32,159 --> 01:08:36,000
that

01:08:32,799 --> 01:08:38,000
these things from the past um do have

01:08:36,000 --> 01:08:40,000
value so it's not guaranteed but it's

01:08:38,000 --> 01:08:43,359
certainly worth considering

01:08:40,000 --> 01:08:46,910
um and yeah

01:08:43,359 --> 01:08:50,380
you're in for some experimentation okay

01:08:46,910 --> 01:08:50,380
[Music]

01:08:51,600 --> 01:08:55,600
yeah exactly uh that's i think the first

01:08:54,159 --> 01:08:56,480
question sorry i'm doing this in the

01:08:55,600 --> 01:08:59,679
wrong order

01:08:56,480 --> 01:09:03,120
um yeah so um

01:08:59,679 --> 01:09:05,400
you want to um uh

01:09:03,120 --> 01:09:06,799
you could as a fine tuning you you can

01:09:05,400 --> 01:09:09,839
pre-initialize

01:09:06,799 --> 01:09:10,960
the result data structure and in that

01:09:09,839 --> 01:09:13,040
way spread it out

01:09:10,960 --> 01:09:15,679
that's that's indeed what i was doing

01:09:13,040 --> 01:09:19,520
let me go let me go to the new questions

01:09:15,679 --> 01:09:23,120
um okay

01:09:19,520 --> 01:09:26,960
i think we got for now we got all the

01:09:23,120 --> 01:09:28,560
q a from here um

01:09:26,960 --> 01:09:30,480
i'm not sure whether we have a formal

01:09:28,560 --> 01:09:31,839
way for people to send questions other

01:09:30,480 --> 01:09:36,080
than the forum of course

01:09:31,839 --> 01:09:39,359
but uh we um we definitely encourage

01:09:36,080 --> 01:09:42,319
yeah we do we do have the openmp

01:09:39,359 --> 01:09:44,319
forum that you can access um on the

01:09:42,319 --> 01:09:46,000
openmp website and so you can always

01:09:44,319 --> 01:09:47,920
submit questions there of course that

01:09:46,000 --> 01:09:49,279
will be go directly to rue that'll go to

01:09:47,920 --> 01:09:52,080
everybody in the open mpa

01:09:49,279 --> 01:09:52,080
yeah um

01:09:53,440 --> 01:09:58,480
yeah but but many of us are on that

01:09:55,520 --> 01:10:01,120
forum and they're more active than i am

01:09:58,480 --> 01:10:02,080
unfortunately i i usually lack the time

01:10:01,120 --> 01:10:04,960
to to do that

01:10:02,080 --> 01:10:06,480
but uh that's not an excuse so so yeah

01:10:04,960 --> 01:10:08,400
we do try to keep track of what's

01:10:06,480 --> 01:10:10,000
happening on the forum and answer your

01:10:08,400 --> 01:10:13,440
questions

01:10:10,000 --> 01:10:17,840
yeah yeah maybe also post ideas for this

01:10:13,440 --> 01:10:17,840
seminar yeah

01:10:18,320 --> 01:10:23,120
okay all right lisa you want to wrap it

01:10:21,360 --> 01:10:25,440
up

01:10:23,120 --> 01:10:26,560
sure uh thank you everyone for joining

01:10:25,440 --> 01:10:29,760
uh like i said

01:10:26,560 --> 01:10:31,520
earlier uh we will have uh you'll be

01:10:29,760 --> 01:10:32,480
expecting an email tomorrow that'll give

01:10:31,520 --> 01:10:33,600
you a link

01:10:32,480 --> 01:10:36,000
to bring you to the recorded

01:10:33,600 --> 01:10:38,320
presentation and um

01:10:36,000 --> 01:10:39,520
more information about openmp so check

01:10:38,320 --> 01:10:41,600
back and

01:10:39,520 --> 01:10:44,159
um stay tuned we have more webinars

01:10:41,600 --> 01:10:45,600
coming from openmp in the future so

01:10:44,159 --> 01:10:48,560
stay tuned thank you very much for

01:10:45,600 --> 01:10:50,320
joining all right

01:10:48,560 --> 01:10:51,840
thank you thank you all for listening

01:10:50,320 --> 01:10:55,840
and staying on

01:10:51,840 --> 01:10:59,840
um that's that's nice thanks

01:10:55,840 --> 01:10:59,840

YouTube URL: https://www.youtube.com/watch?v=wybOK5pQxPQ


