Title: Webinar: A Compilers View of OpenMP
Publication date: 2021-06-10
Playlist: Webinars
Description: 
	In this webinar we will look at the interpretation of OpenMP API directives by the compiler frontend, including OpenMPâ€™s interaction with the compiler optimizations and how OpenMP runtimes are used behind the scenes to implement directives.


We will discuss how different implementation choices impact user experience and performance, either explicitly or due to their interaction with optimizations. In addition to best practices, participants will learn how to interact with the LLVM/Clang compiler to determine how OpenMP directives were implemented and optimized. We will follow with time for your questions.


The webinar is presented by Johannes Doerfert, researcher in the Math and Computer Science Division at Argonne National Laboratory. He is the code owner of the LLVM/OpenMP target offloading implementation and a member of the OpenMP language committee since 2018 with a special interest in accelerator programming.


PDF of the slides available on our website: https://www.openmp.org/events/webinar-a-compilers-view-of-the-openmp-api/
Captions: 
	00:00:01,920 --> 00:00:05,279
hello everyone

00:00:03,120 --> 00:00:07,200
thank you for joining our webinar

00:00:05,279 --> 00:00:08,639
today's session will take approximately

00:00:07,200 --> 00:00:10,160
one hour

00:00:08,639 --> 00:00:12,719
if you have any questions during the

00:00:10,160 --> 00:00:15,280
presentation please ask them using the q

00:00:12,719 --> 00:00:17,920
a feature located on your zoom toolbar

00:00:15,280 --> 00:00:19,199
at the bottom of your screen you will be

00:00:17,920 --> 00:00:21,439
able to see questions from other

00:00:19,199 --> 00:00:22,800
attendees so please upvote or like the

00:00:21,439 --> 00:00:24,160
questions that interest you

00:00:22,800 --> 00:00:25,920
this will help us determine which

00:00:24,160 --> 00:00:27,439
questions to address live at the end of

00:00:25,920 --> 00:00:29,279
the session

00:00:27,439 --> 00:00:30,880
watch for an email tomorrow with further

00:00:29,279 --> 00:00:32,239
information and a link to the recorded

00:00:30,880 --> 00:00:34,160
presentation

00:00:32,239 --> 00:00:37,280
and now i'd like to introduce johannes

00:00:34,160 --> 00:00:42,800
dawford from argonne national laboratory

00:00:37,280 --> 00:00:47,039
johannes thank you very much

00:00:42,800 --> 00:00:50,719
okay let's see if this all works

00:00:47,039 --> 00:00:52,559
okay so um my name is john is different

00:00:50,719 --> 00:00:55,360
and i work at argonne national

00:00:52,559 --> 00:00:57,360
laboratory in in chicago

00:00:55,360 --> 00:00:58,719
so a little bit about me for those that

00:00:57,360 --> 00:01:01,920
that haven't like

00:00:58,719 --> 00:01:03,199
met me before um i got my phd

00:01:01,920 --> 00:01:05,519
on computer science from zaden

00:01:03,199 --> 00:01:07,520
university in britain in germany

00:01:05,519 --> 00:01:08,560
afterwards they started as a researcher

00:01:07,520 --> 00:01:11,680
at argonne

00:01:08,560 --> 00:01:14,159
and i've been there ever since and

00:01:11,680 --> 00:01:15,119
i worked in the llvm community since

00:01:14,159 --> 00:01:18,240
00:01:15,119 --> 00:01:19,840
and in the openmp uh community since

00:01:18,240 --> 00:01:23,200
2018.

00:01:19,840 --> 00:01:24,320
and kind of recently i became the

00:01:23,200 --> 00:01:27,520
official

00:01:24,320 --> 00:01:31,600
code owner for openmp offloading in llvm

00:01:27,520 --> 00:01:34,240
so um very involved in in the openmp

00:01:31,600 --> 00:01:34,240
offloading

00:01:34,400 --> 00:01:37,840
development as part of the llvm compiler

00:01:36,479 --> 00:01:39,759
framework but also

00:01:37,840 --> 00:01:41,200
in openmp optimization which we will

00:01:39,759 --> 00:01:44,240
talk about today

00:01:41,200 --> 00:01:46,000
and um the the openmp front end

00:01:44,240 --> 00:01:49,200
the openmp runtimes everything that kind

00:01:46,000 --> 00:01:51,680
of connects there

00:01:49,200 --> 00:01:52,320
okay so let's start with a little bit of

00:01:51,680 --> 00:01:54,960
background

00:01:52,320 --> 00:01:56,719
llvm in a nutshell given that this is

00:01:54,960 --> 00:02:00,560
supposed to be a compiler talk

00:01:56,719 --> 00:02:03,840
i'll try not to actually show

00:02:00,560 --> 00:02:05,680
all of the my uh anything else but you

00:02:03,840 --> 00:02:08,479
need to know a little bit about it

00:02:05,680 --> 00:02:10,399
or which which is generally the same for

00:02:08,479 --> 00:02:12,480
all compilers so so what i'm going to

00:02:10,399 --> 00:02:15,440
tell is not

00:02:12,480 --> 00:02:17,200
always lvm specific but sometimes and it

00:02:15,440 --> 00:02:18,640
certainly comes from my perspective

00:02:17,200 --> 00:02:22,400
working on obvious

00:02:18,640 --> 00:02:24,360
but on the other hand side um i will

00:02:22,400 --> 00:02:25,680
note if something is kind of lvm

00:02:24,360 --> 00:02:28,959
specific

00:02:25,680 --> 00:02:29,440
um whenever whenever it comes up so in a

00:02:28,959 --> 00:02:31,760
nutshell

00:02:29,440 --> 00:02:33,680
vm is an open source uh compiler

00:02:31,760 --> 00:02:35,200
framework that has multiple front ends

00:02:33,680 --> 00:02:36,879
multiple backends and kind of an

00:02:35,200 --> 00:02:37,680
intermediate representation in the

00:02:36,879 --> 00:02:38,879
middle end

00:02:37,680 --> 00:02:40,800
and a lot of analyses and

00:02:38,879 --> 00:02:42,080
transformations there it's an open

00:02:40,800 --> 00:02:45,200
community and

00:02:42,080 --> 00:02:48,080
and open in a lot of other ways so

00:02:45,200 --> 00:02:50,000
as an open source compiler it is

00:02:48,080 --> 00:02:53,120
extensible and also fixable

00:02:50,000 --> 00:02:54,720
by whatever user you have so it doesn't

00:02:53,120 --> 00:02:55,440
really matter if you're a company or if

00:02:54,720 --> 00:02:59,120
you're

00:02:55,440 --> 00:03:01,920
enthusiast or or a government agency

00:02:59,120 --> 00:03:02,720
if you have a problem something is is

00:03:01,920 --> 00:03:04,959
not right

00:03:02,720 --> 00:03:06,080
or something should be better you can

00:03:04,959 --> 00:03:08,159
kind of extend it or

00:03:06,080 --> 00:03:10,159
extend it or fix it yourself which is

00:03:08,159 --> 00:03:14,000
kind of cool

00:03:10,159 --> 00:03:16,879
it is portable at least across modern

00:03:14,000 --> 00:03:18,800
commodity gpus and cpus so it has back

00:03:16,879 --> 00:03:21,920
ends for nvpdx for

00:03:18,800 --> 00:03:25,760
for nvidia devices amd gcn

00:03:21,920 --> 00:03:29,200
for amd devices and and there's more

00:03:25,760 --> 00:03:31,040
uh back-end storage cpus coming and it

00:03:29,200 --> 00:03:34,720
supports a lot of kind of

00:03:31,040 --> 00:03:37,200
um commodity cpus

00:03:34,720 --> 00:03:37,920
it has very good support for c pulse

00:03:37,200 --> 00:03:40,319
plus

00:03:37,920 --> 00:03:41,840
and pretty good support for openmp and

00:03:40,319 --> 00:03:43,920
then a lot of other of these

00:03:41,840 --> 00:03:45,440
kind of offloading languages sickle hip

00:03:43,920 --> 00:03:48,000
cuda and so on

00:03:45,440 --> 00:03:49,040
it's kind of feature complete i mean

00:03:48,000 --> 00:03:51,040
eventually

00:03:49,040 --> 00:03:53,040
and it it gives you access to the

00:03:51,040 --> 00:03:53,519
coolest features early on because a lot

00:03:53,040 --> 00:03:56,080
of the

00:03:53,519 --> 00:03:58,239
work in in those standards towards those

00:03:56,080 --> 00:03:59,680
languages is actually prototyped as part

00:03:58,239 --> 00:04:02,400
of llvm

00:03:59,680 --> 00:04:03,840
which is why you can kind of try them

00:04:02,400 --> 00:04:06,799
out early on

00:04:03,840 --> 00:04:07,200
now obviously it's performing incorrect

00:04:06,799 --> 00:04:10,239
uh

00:04:07,200 --> 00:04:13,360
with a smiley face that is that is the

00:04:10,239 --> 00:04:15,120
the goal but it's not always it's

00:04:13,360 --> 00:04:16,479
usually not the case we're just trying

00:04:15,120 --> 00:04:20,000
to

00:04:16,479 --> 00:04:21,440
okay um then how does llvm and

00:04:20,000 --> 00:04:24,639
especially the cc

00:04:21,440 --> 00:04:27,120
plus front and clang work

00:04:24,639 --> 00:04:28,080
and it's like you have to consider clang

00:04:27,120 --> 00:04:30,720
is a front end for

00:04:28,080 --> 00:04:32,080
all c like languages that includes hip

00:04:30,720 --> 00:04:34,320
cuda sickle

00:04:32,080 --> 00:04:36,639
and so on and so forth all of that goes

00:04:34,320 --> 00:04:39,120
through clang at the end of the day

00:04:36,639 --> 00:04:40,720
and what what client does it takes a c

00:04:39,120 --> 00:04:42,560
file and it transform

00:04:40,720 --> 00:04:44,960
translates it into lvm intermediate

00:04:42,560 --> 00:04:47,280
representation llvmir

00:04:44,960 --> 00:04:48,960
on that lvmir you can then do

00:04:47,280 --> 00:04:50,639
optimizations

00:04:48,960 --> 00:04:52,080
the tool if you want to do that

00:04:50,639 --> 00:04:54,720
explicitly manually

00:04:52,080 --> 00:04:56,800
is called opt but clang does pretty much

00:04:54,720 --> 00:04:59,759
what op does as well

00:04:56,800 --> 00:05:01,120
and then with llc you generate llvm

00:04:59,759 --> 00:05:03,120
machine ir

00:05:01,120 --> 00:05:05,120
and then you can you can actually

00:05:03,120 --> 00:05:06,880
optimize the machine ir as well

00:05:05,120 --> 00:05:09,680
so a certain kind of optimization only

00:05:06,880 --> 00:05:11,600
happened that happened on the machine ir

00:05:09,680 --> 00:05:13,440
and then you generate machine code from

00:05:11,600 --> 00:05:15,680
that

00:05:13,440 --> 00:05:17,520
if you want a longer introduction into

00:05:15,680 --> 00:05:19,520
lrvm especially if you're interested in

00:05:17,520 --> 00:05:21,840
kind of starting to work with fellow vm

00:05:19,520 --> 00:05:23,759
i can recommend the the youtube link in

00:05:21,840 --> 00:05:25,440
the bottom here

00:05:23,759 --> 00:05:26,800
partially because that was the tutorial

00:05:25,440 --> 00:05:28,960
i was involved in

00:05:26,800 --> 00:05:31,039
but i think i think people liked it

00:05:28,960 --> 00:05:33,759
reasonably well

00:05:31,039 --> 00:05:35,440
okay so that's that's uh enough here

00:05:33,759 --> 00:05:38,400
just to highlight

00:05:35,440 --> 00:05:41,039
the part that i work on is usually this

00:05:38,400 --> 00:05:44,160
llvmir optimization level

00:05:41,039 --> 00:05:44,160
so where we do

00:05:44,560 --> 00:05:51,280
mostly target independent optimization

00:05:46,960 --> 00:05:53,440
analysis on this llvm identification

00:05:51,280 --> 00:05:55,360
let's take a look at openmp and llvm

00:05:53,440 --> 00:05:57,039
because that's kind of an

00:05:55,360 --> 00:06:00,240
an interesting fact and it actually kind

00:05:57,039 --> 00:06:02,240
of changed over the last few years a lot

00:06:00,240 --> 00:06:04,000
so initially if you think about openmp

00:06:02,240 --> 00:06:04,319
in a compiler you think about the front

00:06:04,000 --> 00:06:06,720
end

00:06:04,319 --> 00:06:08,800
in this case clan it has an open v

00:06:06,720 --> 00:06:10,720
parser an openmp semantic analysis and

00:06:08,800 --> 00:06:12,400
an open v code generation

00:06:10,720 --> 00:06:14,560
and you think that is openmp in a

00:06:12,400 --> 00:06:16,479
compiler right like the front end that

00:06:14,560 --> 00:06:19,919
translates openmp

00:06:16,479 --> 00:06:22,160
checks everything and implements openmp

00:06:19,919 --> 00:06:22,720
maybe you also consider the runtime as

00:06:22,160 --> 00:06:25,520
part of

00:06:22,720 --> 00:06:27,199
openmp in a compiler because the runtime

00:06:25,520 --> 00:06:29,039
is usually distributed as part of the

00:06:27,199 --> 00:06:33,600
compiler so

00:06:29,039 --> 00:06:36,000
llvm has libaum and then gcc has libgum

00:06:33,600 --> 00:06:38,080
and other compilers have other openmp

00:06:36,000 --> 00:06:40,800
runtimes

00:06:38,080 --> 00:06:42,400
but there's now it is a lot more for one

00:06:40,800 --> 00:06:44,639
there is more runtimes

00:06:42,400 --> 00:06:46,080
lipbomb is the classic host runtime for

00:06:44,639 --> 00:06:48,720
openmp

00:06:46,080 --> 00:06:50,240
but we also have lip on target in all

00:06:48,720 --> 00:06:53,199
the plugins

00:06:50,240 --> 00:06:54,960
um which con like which are responsible

00:06:53,199 --> 00:06:57,199
for target of loading and then

00:06:54,960 --> 00:06:58,800
to cut and then responsible plugins are

00:06:57,199 --> 00:06:59,919
responsible to talk to the device

00:06:58,800 --> 00:07:02,319
drivers

00:06:59,919 --> 00:07:03,120
and then you have lip on target nvpdx

00:07:02,319 --> 00:07:05,680
for example

00:07:03,120 --> 00:07:07,440
which is actually executed on the device

00:07:05,680 --> 00:07:10,720
so it's a runtime that we

00:07:07,440 --> 00:07:12,880
ship with the user code onto let's say a

00:07:10,720 --> 00:07:14,960
gpu

00:07:12,880 --> 00:07:16,240
but that is not all nowadays we're

00:07:14,960 --> 00:07:18,639
working um

00:07:16,240 --> 00:07:20,560
very rapidly on the fortran front end

00:07:18,639 --> 00:07:22,479
for llvm which is called flank

00:07:20,560 --> 00:07:24,560
so there we also will have an openmp

00:07:22,479 --> 00:07:25,680
parser openmp semi and openmp code

00:07:24,560 --> 00:07:27,919
generation

00:07:25,680 --> 00:07:29,440
so so now at this point you might you

00:07:27,919 --> 00:07:31,120
might already see that there is kind of

00:07:29,440 --> 00:07:32,880
a duplication here especially in the

00:07:31,120 --> 00:07:35,840
code generation part

00:07:32,880 --> 00:07:36,240
you really don't want to duplicate that

00:07:35,840 --> 00:07:39,520
because

00:07:36,240 --> 00:07:41,120
code generating openmp brackets for cc

00:07:39,520 --> 00:07:42,560
plus and for fortran it's pretty much

00:07:41,120 --> 00:07:45,360
the same task

00:07:42,560 --> 00:07:46,800
so as part of as part of the development

00:07:45,360 --> 00:07:49,680
of flank we're also

00:07:46,800 --> 00:07:50,960
kind of building this open pr builder

00:07:49,680 --> 00:07:54,879
which is a front end

00:07:50,960 --> 00:07:57,280
independent openmp llvmir generation

00:07:54,879 --> 00:07:58,160
that favors very simple and expressive

00:07:57,280 --> 00:08:00,960
lvmir

00:07:58,160 --> 00:08:03,520
so in order to make it easier for

00:08:00,960 --> 00:08:06,560
analysis and optimization to work

00:08:03,520 --> 00:08:08,560
with that lvmir

00:08:06,560 --> 00:08:10,400
and the fun part is it is not only

00:08:08,560 --> 00:08:12,479
usable for openmp

00:08:10,400 --> 00:08:13,919
frontends like the clang opening front

00:08:12,479 --> 00:08:14,240
end or the flank open if you've run in

00:08:13,919 --> 00:08:16,080
but

00:08:14,240 --> 00:08:18,000
if you had another language that has

00:08:16,080 --> 00:08:19,840
nothing to do with openmp you could use

00:08:18,000 --> 00:08:22,879
this openmpr builder

00:08:19,840 --> 00:08:25,199
to basically take it back on all the

00:08:22,879 --> 00:08:27,280
openmp ecosystem and kind of

00:08:25,199 --> 00:08:29,599
translate your language parallelism into

00:08:27,280 --> 00:08:32,000
openmp parallelism without ever having a

00:08:29,599 --> 00:08:32,000
pragma

00:08:32,320 --> 00:08:38,080
and then the kind of potentially most

00:08:35,919 --> 00:08:40,640
uh crucial part that was added in the

00:08:38,080 --> 00:08:42,560
last few years is openmp opt

00:08:40,640 --> 00:08:44,800
that is an inter-residual optimization

00:08:42,560 --> 00:08:47,680
path as part of the llvm core

00:08:44,800 --> 00:08:49,360
optimization pipeline that contains host

00:08:47,680 --> 00:08:51,760
and device optimizations

00:08:49,360 --> 00:08:53,360
it is run by default with when you say

00:08:51,760 --> 00:08:55,279
o2 or 3

00:08:53,360 --> 00:08:58,800
and there's there will be more ways to

00:08:55,279 --> 00:09:02,080
run it later on even with o0 and l1

00:08:58,800 --> 00:09:05,200
and really what it does it does openmp

00:09:02,080 --> 00:09:07,040
aware optimizations so it knows about

00:09:05,200 --> 00:09:09,040
the semantics of openmp

00:09:07,040 --> 00:09:10,720
and it exploits that to improve the

00:09:09,040 --> 00:09:13,839
performance of a program

00:09:10,720 --> 00:09:13,839
and we'll get to that later

00:09:14,160 --> 00:09:19,920
so openmp implements or openmp

00:09:16,800 --> 00:09:21,519
implementation and optimizations

00:09:19,920 --> 00:09:23,519
let's take a look what a compiler is

00:09:21,519 --> 00:09:24,080
supposed to do on the left hand side you

00:09:23,519 --> 00:09:25,839
see

00:09:24,080 --> 00:09:27,440
an original program that is fairly

00:09:25,839 --> 00:09:29,600
simple you have a variable y

00:09:27,440 --> 00:09:31,040
it's assigned to 7 and then you use the

00:09:29,600 --> 00:09:34,160
variable inside of a loop

00:09:31,040 --> 00:09:35,839
and after the loop what you would expect

00:09:34,160 --> 00:09:38,000
the compiler to do if you turn on

00:09:35,839 --> 00:09:40,080
optimizations is to just replace y

00:09:38,000 --> 00:09:41,279
you don't need it and replace all users

00:09:40,080 --> 00:09:43,200
with seven

00:09:41,279 --> 00:09:44,720
that's going to happen with every

00:09:43,200 --> 00:09:47,120
compiler that you kind of pick up that

00:09:44,720 --> 00:09:49,200
is optimizing nowadays

00:09:47,120 --> 00:09:51,120
now if we add openmp into the mix to

00:09:49,200 --> 00:09:52,399
parallelize our loop because we heard

00:09:51,120 --> 00:09:55,200
that parallelization is kind of

00:09:52,399 --> 00:09:57,600
important we should do that

00:09:55,200 --> 00:09:59,600
you are in for surprise with a lot of

00:09:57,600 --> 00:10:02,240
compilers

00:09:59,600 --> 00:10:04,640
not only is the use of the y inside of

00:10:02,240 --> 00:10:06,320
the openmp parallel 4 not replaced with

00:10:04,640 --> 00:10:08,240
7 anymore

00:10:06,320 --> 00:10:09,519
but depending on what compiler you pick

00:10:08,240 --> 00:10:11,839
up the use

00:10:09,519 --> 00:10:13,680
after the parallel 4 might also not be

00:10:11,839 --> 00:10:16,399
replaced with 7.

00:10:13,680 --> 00:10:18,000
that's kind of that's kind of a problem

00:10:16,399 --> 00:10:20,800
so there are exceptions and i don't

00:10:18,000 --> 00:10:21,440
want to like judge all the compilers but

00:10:20,800 --> 00:10:23,839
so far

00:10:21,440 --> 00:10:26,000
i'm not aware of one that is that i can

00:10:23,839 --> 00:10:29,680
access that has that replaces the y

00:10:26,000 --> 00:10:29,680
inside the parallel loop by default

00:10:30,640 --> 00:10:35,519
now you could ask why is it important i

00:10:32,880 --> 00:10:37,519
mean replacing a y with a seven

00:10:35,519 --> 00:10:39,040
it doesn't seem to be kind of that big

00:10:37,519 --> 00:10:42,480
of a deal right

00:10:39,040 --> 00:10:44,640
so we did some experiments two years ago

00:10:42,480 --> 00:10:46,480
where we run a couple of benchmarks in

00:10:44,640 --> 00:10:48,560
two configurations

00:10:46,480 --> 00:10:50,240
the base sequential version is just the

00:10:48,560 --> 00:10:52,880
benchmark without

00:10:50,240 --> 00:10:53,839
any openmp it runs a single threaded

00:10:52,880 --> 00:10:56,240
program

00:10:53,839 --> 00:10:57,279
and we measure the time the base

00:10:56,240 --> 00:10:59,440
parallel version

00:10:57,279 --> 00:11:01,839
is the same program but open like the

00:10:59,440 --> 00:11:04,880
same benchmark but openmp enabled

00:11:01,839 --> 00:11:07,040
but then we executed every single thread

00:11:04,880 --> 00:11:09,440
so both of them are executed by a single

00:11:07,040 --> 00:11:10,880
thread but only the base sequential one

00:11:09,440 --> 00:11:12,640
was compiled

00:11:10,880 --> 00:11:14,399
as a single threaded program and the

00:11:12,640 --> 00:11:15,360
base parallel one was compiled as a

00:11:14,399 --> 00:11:19,120
parallel program

00:11:15,360 --> 00:11:20,800
just executed uh sequentially

00:11:19,120 --> 00:11:23,120
what you see here lulaish with this

00:11:20,800 --> 00:11:24,399
inputs slowed down by like three percent

00:11:23,120 --> 00:11:26,959
three and a half

00:11:24,399 --> 00:11:28,880
which is bad but you could argue i mean

00:11:26,959 --> 00:11:29,760
you get parallelization and you would

00:11:28,880 --> 00:11:31,839
never run it

00:11:29,760 --> 00:11:33,279
sequentially so so you get your benefit

00:11:31,839 --> 00:11:34,959
later on

00:11:33,279 --> 00:11:37,440
but then if you go and look at other

00:11:34,959 --> 00:11:38,800
programs for example breakfast search

00:11:37,440 --> 00:11:41,040
you already get a slowdown of eight

00:11:38,800 --> 00:11:42,079
percent which which really doesn't sound

00:11:41,040 --> 00:11:43,920
great

00:11:42,079 --> 00:11:45,680
and then obviously you kind of see where

00:11:43,920 --> 00:11:48,320
this game is going now

00:11:45,680 --> 00:11:50,320
um i'll just pick pathfinder and my

00:11:48,320 --> 00:11:52,399
slowdown is already 40

00:11:50,320 --> 00:11:53,519
so what i basically did by enabling

00:11:52,399 --> 00:11:56,720
openmp is

00:11:53,519 --> 00:11:59,440
i slowed down the

00:11:56,720 --> 00:12:01,279
um sequential version of the program by

00:11:59,440 --> 00:12:03,519
00:12:01,279 --> 00:12:04,560
in that like while you get your benefit

00:12:03,519 --> 00:12:07,440
of parallelization

00:12:04,560 --> 00:12:07,920
sure you actually slowed down a critical

00:12:07,440 --> 00:12:10,639
path

00:12:07,920 --> 00:12:11,680
by that amount so there is more cost to

00:12:10,639 --> 00:12:15,279
pay

00:12:11,680 --> 00:12:18,320
and those costs are almost entirely

00:12:15,279 --> 00:12:20,560
if not entirely because you

00:12:18,320 --> 00:12:22,160
miss out on compiler optimizations as i

00:12:20,560 --> 00:12:24,079
showed before

00:12:22,160 --> 00:12:25,360
not necessarily the we replace

00:12:24,079 --> 00:12:28,320
everything with seven

00:12:25,360 --> 00:12:29,440
but compiler optimization nevertheless

00:12:28,320 --> 00:12:32,720
and then there is

00:12:29,440 --> 00:12:35,120
the my favorite um example which is srab

00:12:32,720 --> 00:12:37,120
version two which slows down by a factor

00:12:35,120 --> 00:12:39,440
of almost two and a half

00:12:37,120 --> 00:12:40,480
so what you basically you need more than

00:12:39,440 --> 00:12:43,120
you need more than

00:12:40,480 --> 00:12:46,079
two threads to even kind of be able to

00:12:43,120 --> 00:12:48,000
potentially recover that slowdown

00:12:46,079 --> 00:12:50,079
so the first thing you did and when

00:12:48,000 --> 00:12:51,680
parallelizing is really

00:12:50,079 --> 00:12:53,040
worsen the performance of your baseline

00:12:51,680 --> 00:12:55,200
which is not really what we're looking

00:12:53,040 --> 00:12:55,200
for

00:12:55,360 --> 00:13:00,800
okay so so now we have this but

00:12:58,480 --> 00:13:01,680
what should we do about it uh or so

00:13:00,800 --> 00:13:05,279
first we go

00:13:01,680 --> 00:13:09,440
through why is this coming from in in

00:13:05,279 --> 00:13:12,880
the answer is it is due to the

00:13:09,440 --> 00:13:15,120
the way openmp is implemented as

00:13:12,880 --> 00:13:16,399
part of the compiler toolkit and let's

00:13:15,120 --> 00:13:17,920
take a look and we're looking at a

00:13:16,399 --> 00:13:19,680
parallel four

00:13:17,920 --> 00:13:21,200
because that is kind of the poster child

00:13:19,680 --> 00:13:22,800
here

00:13:21,200 --> 00:13:24,800
everything else like all the other

00:13:22,800 --> 00:13:27,600
practice and so on and so forth are kind

00:13:24,800 --> 00:13:31,040
of implemented in a very similar way

00:13:27,600 --> 00:13:33,279
um not all have the same complexity

00:13:31,040 --> 00:13:34,560
but if you understand parallel four

00:13:33,279 --> 00:13:38,480
reasonably well

00:13:34,560 --> 00:13:40,639
everything else kind of is is the same

00:13:38,480 --> 00:13:42,560
okay so we have this very simple input

00:13:40,639 --> 00:13:46,240
program in the top

00:13:42,560 --> 00:13:50,800
um a word sharing loop where we

00:13:46,240 --> 00:13:53,120
like out is in uh at two locations okay

00:13:50,800 --> 00:13:54,240
the sum of in the two locations now what

00:13:53,120 --> 00:13:55,839
do we do

00:13:54,240 --> 00:13:57,760
the first step what happens in the

00:13:55,839 --> 00:14:01,199
compiler in the front end

00:13:57,760 --> 00:14:04,399
is that it will replace this

00:14:01,199 --> 00:14:05,519
parallel work sharing loop with the call

00:14:04,399 --> 00:14:07,519
into the runtime

00:14:05,519 --> 00:14:09,360
into the openmp runtime and that call

00:14:07,519 --> 00:14:12,320
will basically tell the openmp runtime

00:14:09,360 --> 00:14:15,040
that there is a parallel for loop

00:14:12,320 --> 00:14:16,079
and it has the iteration space iteration

00:14:15,040 --> 00:14:20,399
space is from 0 to

00:14:16,079 --> 00:14:21,440
n and the values that are needed to

00:14:20,399 --> 00:14:24,480
execute this

00:14:21,440 --> 00:14:25,839
body of the parallel for loop are n in

00:14:24,480 --> 00:14:28,240
and out

00:14:25,839 --> 00:14:29,440
and then it will pass this body function

00:14:28,240 --> 00:14:33,040
which is

00:14:29,440 --> 00:14:35,600
uh and a parallel region outlined into

00:14:33,040 --> 00:14:37,120
a static function so if you look at this

00:14:35,600 --> 00:14:39,920
body function

00:14:37,120 --> 00:14:42,399
what it really is is it's just the body

00:14:39,920 --> 00:14:45,760
of the loop up there

00:14:42,399 --> 00:14:48,800
and then it has this a separate loop

00:14:45,760 --> 00:14:51,040
which is going to iterate over the chunk

00:14:48,800 --> 00:14:52,800
so each thread will basically execute

00:14:51,040 --> 00:14:55,519
this body function

00:14:52,800 --> 00:14:56,639
and it will ask the openmp runtime what

00:14:55,519 --> 00:14:59,519
is my chunk

00:14:56,639 --> 00:15:01,600
that i should execute and then the

00:14:59,519 --> 00:15:03,279
openmp runtime will

00:15:01,600 --> 00:15:05,120
provide this lower bound in this upper

00:15:03,279 --> 00:15:08,480
bound and then this threat will execute

00:15:05,120 --> 00:15:08,480
all iterations of that chunk

00:15:08,560 --> 00:15:12,480
that is more or less what happens at

00:15:10,560 --> 00:15:13,600
least in clang and generally speaking

00:15:12,480 --> 00:15:16,480
also in a lot

00:15:13,600 --> 00:15:18,000
if not all other compilers this is a

00:15:16,480 --> 00:15:21,040
little bit simplified but

00:15:18,000 --> 00:15:24,160
but the gist is there

00:15:21,040 --> 00:15:26,720
now if you look at this

00:15:24,160 --> 00:15:28,399
parallel four call that that goes into

00:15:26,720 --> 00:15:29,920
the runtime and if you look at this body

00:15:28,399 --> 00:15:32,560
function you can kind of see that

00:15:29,920 --> 00:15:34,639
there's a there's an obvious connection

00:15:32,560 --> 00:15:36,560
so the body function is passed to the

00:15:34,639 --> 00:15:37,440
runtime and all the arguments of the

00:15:36,560 --> 00:15:39,759
body function

00:15:37,440 --> 00:15:41,519
except the thread id are also passed to

00:15:39,759 --> 00:15:45,680
the runtime

00:15:41,519 --> 00:15:46,639
so we kind of tell the runtime what to

00:15:45,680 --> 00:15:48,880
call

00:15:46,639 --> 00:15:50,560
and what the arguments are because we

00:15:48,880 --> 00:15:52,000
only know that at this point

00:15:50,560 --> 00:15:53,920
like at this point where we generate

00:15:52,000 --> 00:15:55,440
this runtime call but the runtime

00:15:53,920 --> 00:15:57,120
doesn't really know that so the runtime

00:15:55,440 --> 00:15:58,959
is just going to kind of take this

00:15:57,120 --> 00:16:02,079
function pointer and and call it with

00:15:58,959 --> 00:16:05,199
the arguments that were passed

00:16:02,079 --> 00:16:06,880
now one thing to to to notice here is

00:16:05,199 --> 00:16:08,000
that we take the address of all the

00:16:06,880 --> 00:16:11,199
arguments

00:16:08,000 --> 00:16:12,959
so n in the above code was an integer

00:16:11,199 --> 00:16:16,399
and now what we actually pass is the

00:16:12,959 --> 00:16:18,560
axis of n which is an in pointer

00:16:16,399 --> 00:16:19,839
and the same holds for in and out which

00:16:18,560 --> 00:16:21,920
were floating pointers

00:16:19,839 --> 00:16:23,360
in the above example and now we pass

00:16:21,920 --> 00:16:25,360
their addresses which are floating

00:16:23,360 --> 00:16:28,639
pointer pointers

00:16:25,360 --> 00:16:30,480
and the um what that means is if you

00:16:28,639 --> 00:16:33,040
look at the code inside of body function

00:16:30,480 --> 00:16:35,920
is that we have to first dereference

00:16:33,040 --> 00:16:37,839
and in and out before we actually get to

00:16:35,920 --> 00:16:39,759
the value we're looking for

00:16:37,839 --> 00:16:41,199
so we basically introduced one level of

00:16:39,759 --> 00:16:42,959
indirection here

00:16:41,199 --> 00:16:44,800
and that that should kind of concern you

00:16:42,959 --> 00:16:46,480
because the level of indirection is

00:16:44,800 --> 00:16:50,480
always bad for performance

00:16:46,480 --> 00:16:53,680
so what happened what happened is that

00:16:50,480 --> 00:16:57,600
if you pass depending on how you share

00:16:53,680 --> 00:17:00,800
a variable the value is passed

00:16:57,600 --> 00:17:02,560
or communicated through the runtime with

00:17:00,800 --> 00:17:04,720
a different type

00:17:02,560 --> 00:17:06,720
so by default you have in a parallel

00:17:04,720 --> 00:17:09,120
forward variables are shared

00:17:06,720 --> 00:17:09,760
and if you have a variable of type t

00:17:09,120 --> 00:17:12,400
that is

00:17:09,760 --> 00:17:13,760
shared into a parallel region it will

00:17:12,400 --> 00:17:16,000
actually be passed as

00:17:13,760 --> 00:17:17,600
t pointer so the address of the variable

00:17:16,000 --> 00:17:19,760
is passed

00:17:17,600 --> 00:17:22,079
same for last private why is that

00:17:19,760 --> 00:17:24,000
because in both cases

00:17:22,079 --> 00:17:25,839
inside the parallel region we might

00:17:24,000 --> 00:17:28,000
modify the variable

00:17:25,839 --> 00:17:29,360
and for the modification to be visible

00:17:28,000 --> 00:17:32,240
after the parallel region

00:17:29,360 --> 00:17:32,720
we have to provide we have to provide a

00:17:32,240 --> 00:17:35,360
way

00:17:32,720 --> 00:17:36,480
to write in the original storage and the

00:17:35,360 --> 00:17:38,160
way that is usually done

00:17:36,480 --> 00:17:40,480
is just provide the pointer to the

00:17:38,160 --> 00:17:42,559
original storage but this

00:17:40,480 --> 00:17:44,080
indirection that we introduced here

00:17:42,559 --> 00:17:46,480
causes us to

00:17:44,080 --> 00:17:48,240
have to like call this an indirection

00:17:46,480 --> 00:17:49,919
everywhere in the parallel region so

00:17:48,240 --> 00:17:50,720
passing something is shared or last

00:17:49,919 --> 00:17:53,440
private

00:17:50,720 --> 00:17:54,880
is usually bad if you pass something at

00:17:53,440 --> 00:17:57,440
first private however

00:17:54,880 --> 00:17:59,039
we'll do we like a copy of it so if you

00:17:57,440 --> 00:18:00,480
pass a variable of type t as first

00:17:59,039 --> 00:18:02,400
private we'll pass it as

00:18:00,480 --> 00:18:04,880
type t so that is kind of what you want

00:18:02,400 --> 00:18:06,160
to do if you pass something as private

00:18:04,880 --> 00:18:06,799
we're actually not communicating

00:18:06,160 --> 00:18:08,720
anything

00:18:06,799 --> 00:18:10,720
we're just allocating a new variable

00:18:08,720 --> 00:18:13,360
inside the scope

00:18:10,720 --> 00:18:15,039
so that's also very good and now the

00:18:13,360 --> 00:18:17,840
rule

00:18:15,039 --> 00:18:19,760
you should use default first private

00:18:17,840 --> 00:18:21,440
pretty much always

00:18:19,760 --> 00:18:24,080
if that is not implemented by your

00:18:21,440 --> 00:18:25,919
compiler yet you could use default none

00:18:24,080 --> 00:18:29,200
and then first private everything and

00:18:25,919 --> 00:18:31,360
the variables explicitly

00:18:29,200 --> 00:18:32,640
i say almost because there is one more

00:18:31,360 --> 00:18:35,919
or less exception here

00:18:32,640 --> 00:18:36,640
or two one is arrays that live on your

00:18:35,919 --> 00:18:39,120
stack

00:18:36,640 --> 00:18:41,360
which are at least in in the codes that

00:18:39,120 --> 00:18:42,240
i see not that common but those ones are

00:18:41,360 --> 00:18:45,600
exceptions

00:18:42,240 --> 00:18:45,600
and the other exceptions are

00:18:46,080 --> 00:18:52,799
values that you really

00:18:49,520 --> 00:18:54,960
use to communicate between threats so

00:18:52,799 --> 00:18:56,000
but in order to determine if that is the

00:18:54,960 --> 00:18:58,080
case

00:18:56,000 --> 00:18:59,919
you can go you can go by this by this

00:18:58,080 --> 00:19:02,400
idea if you have a value and it's not a

00:18:59,919 --> 00:19:04,799
reduction and it is not

00:19:02,400 --> 00:19:06,240
uh and you don't use any synchronization

00:19:04,799 --> 00:19:10,240
primitives

00:19:06,240 --> 00:19:13,120
to guard the accesses to that value

00:19:10,240 --> 00:19:14,559
you can use first private it should

00:19:13,120 --> 00:19:17,039
almost always work

00:19:14,559 --> 00:19:18,640
and i mean you can try it out and if it

00:19:17,039 --> 00:19:21,440
doesn't work it should be fairly obvious

00:19:18,640 --> 00:19:21,440
that it doesn't work

00:19:21,600 --> 00:19:25,360
okay so that was that was kind of a a

00:19:24,320 --> 00:19:27,840
quick

00:19:25,360 --> 00:19:28,480
guide on what kind of data sharing uh to

00:19:27,840 --> 00:19:30,240
use

00:19:28,480 --> 00:19:31,840
let's go back to our example so this is

00:19:30,240 --> 00:19:33,760
the same slide i showed before

00:19:31,840 --> 00:19:35,200
you see the weak connection between the

00:19:33,760 --> 00:19:36,799
parallel uh

00:19:35,200 --> 00:19:39,600
runtime call where we pass all the

00:19:36,799 --> 00:19:42,960
values and then you see the

00:19:39,600 --> 00:19:44,559
how they end up in the parallel function

00:19:42,960 --> 00:19:47,440
as arguments

00:19:44,559 --> 00:19:48,799
now if you want to optimize this because

00:19:47,440 --> 00:19:49,840
right now it's this will not be

00:19:48,799 --> 00:19:51,440
optimized so

00:19:49,840 --> 00:19:53,440
because we go basically through the

00:19:51,440 --> 00:19:55,919
runtime through a function we don't know

00:19:53,440 --> 00:19:57,280
the compiler has no chance of optimizing

00:19:55,919 --> 00:19:58,960
this the compiler doesn't know what

00:19:57,280 --> 00:20:01,520
parallel for like the runtime called

00:19:58,960 --> 00:20:03,679
parallel for does

00:20:01,520 --> 00:20:05,280
what people proposed is let's make it

00:20:03,679 --> 00:20:07,440
explicit let's kind of have an

00:20:05,280 --> 00:20:10,240
intermediate ir that is parallel

00:20:07,440 --> 00:20:12,400
so we have in our in the compiler we

00:20:10,240 --> 00:20:14,000
represent this as a parallel for

00:20:12,400 --> 00:20:15,520
and then we just call this outline

00:20:14,000 --> 00:20:17,679
function and then it is

00:20:15,520 --> 00:20:20,720
really explicit what is happening here

00:20:17,679 --> 00:20:24,559
and you get optimization

00:20:20,720 --> 00:20:25,679
and if i would build a compiler from

00:20:24,559 --> 00:20:28,799
scratch

00:20:25,679 --> 00:20:31,200
this is what i probably would do however

00:20:28,799 --> 00:20:32,880
if you take a compiler that is existing

00:20:31,200 --> 00:20:34,960
that has a lot of optimizations that

00:20:32,880 --> 00:20:37,440
assume sequential semantics of

00:20:34,960 --> 00:20:39,280
the control flow graph and other things

00:20:37,440 --> 00:20:42,400
this is going to be

00:20:39,280 --> 00:20:44,480
like introduce very subtle and sometimes

00:20:42,400 --> 00:20:47,520
not so subtle errors

00:20:44,480 --> 00:20:51,360
so doing this and hoping it will work

00:20:47,520 --> 00:20:54,799
without a substantial amount of changes

00:20:51,360 --> 00:20:55,760
is illusionary so we kind of decided

00:20:54,799 --> 00:20:58,799
against this

00:20:55,760 --> 00:21:01,200
and instead let's go back to let's go

00:20:58,799 --> 00:21:03,280
back to the picture we had

00:21:01,200 --> 00:21:04,400
instead let's let's take a look again

00:21:03,280 --> 00:21:07,520
what we have here

00:21:04,400 --> 00:21:09,760
you see that there is we know like

00:21:07,520 --> 00:21:11,440
as as with our domain knowledge that the

00:21:09,760 --> 00:21:13,280
parallel for runtime call

00:21:11,440 --> 00:21:14,720
is going to call the body function and

00:21:13,280 --> 00:21:17,600
is going to pass n

00:21:14,720 --> 00:21:19,200
in and out we know that why don't we

00:21:17,600 --> 00:21:20,240
tell that the compiler such that the

00:21:19,200 --> 00:21:22,240
compiler

00:21:20,240 --> 00:21:23,840
can do inter procedural optimization

00:21:22,240 --> 00:21:25,440
here because it now knows

00:21:23,840 --> 00:21:27,039
that the only thing that happens here is

00:21:25,440 --> 00:21:29,760
that body function is called

00:21:27,039 --> 00:21:31,679
with those arguments so what llvm what

00:21:29,760 --> 00:21:32,799
we actually do now is we model this

00:21:31,679 --> 00:21:34,960
transitive call

00:21:32,799 --> 00:21:36,080
to this call that happens inside the

00:21:34,960 --> 00:21:38,320
runtime

00:21:36,080 --> 00:21:39,440
of the body function with those

00:21:38,320 --> 00:21:41,120
arguments

00:21:39,440 --> 00:21:43,360
now the question mark here basically

00:21:41,120 --> 00:21:45,360
says there are some arguments that

00:21:43,360 --> 00:21:47,520
we don't know what will be passed so

00:21:45,360 --> 00:21:50,880
this is kind of an

00:21:47,520 --> 00:21:52,799
um partially known call

00:21:50,880 --> 00:21:53,919
site so we don't really know what

00:21:52,799 --> 00:21:56,320
everything is passed

00:21:53,919 --> 00:21:57,840
but we know some of the things and now

00:21:56,320 --> 00:22:00,080
llvm is going to perform

00:21:57,840 --> 00:22:03,120
inter-procedural optimization across

00:22:00,080 --> 00:22:04,400
this transitive call site that we kind

00:22:03,120 --> 00:22:07,600
of

00:22:04,400 --> 00:22:08,000
pretend it exists to exist and we do

00:22:07,600 --> 00:22:10,640
this

00:22:08,000 --> 00:22:12,400
by introducing domain knowledge about

00:22:10,640 --> 00:22:14,400
this runtime call

00:22:12,400 --> 00:22:16,960
and we got to that a little bit later

00:22:14,400 --> 00:22:20,240
because it's not only that runtime

00:22:16,960 --> 00:22:23,520
that we introduced domain knowledge for

00:22:20,240 --> 00:22:26,240
okay so we did some experiments back

00:22:23,520 --> 00:22:27,520
in 2018 when we started this and

00:22:26,240 --> 00:22:29,600
prototyped this so

00:22:27,520 --> 00:22:31,120
we had a base version which is just

00:22:29,600 --> 00:22:33,520
plain o3

00:22:31,120 --> 00:22:35,600
and then we used two inter-procedural

00:22:33,520 --> 00:22:39,520
optimizations that exist in

00:22:35,600 --> 00:22:42,799
llvm which is attribute propagation and

00:22:39,520 --> 00:22:45,120
argument promotion and we we kind of

00:22:42,799 --> 00:22:46,080
taught them about these transitive call

00:22:45,120 --> 00:22:48,159
sites

00:22:46,080 --> 00:22:50,240
so we explicitly made them aware of

00:22:48,159 --> 00:22:52,159
these transitive call sets

00:22:50,240 --> 00:22:54,159
but we use the same logic for the intro

00:22:52,159 --> 00:22:56,480
procedural optimization

00:22:54,159 --> 00:22:57,679
and then we run the experiment again and

00:22:56,480 --> 00:22:59,840
now um this

00:22:57,679 --> 00:23:01,039
is the loot benchmark and you see by the

00:22:59,840 --> 00:23:03,039
like the base s

00:23:01,039 --> 00:23:04,880
and the very left that is what we had

00:23:03,039 --> 00:23:06,240
before base sequential and then

00:23:04,880 --> 00:23:08,000
the second from the left is based

00:23:06,240 --> 00:23:10,960
parallel so that those are the two

00:23:08,000 --> 00:23:12,559
that i showed before and and it kind of

00:23:10,960 --> 00:23:14,880
slowed down 25

00:23:12,559 --> 00:23:16,240
if you run it with openmp enabled and a

00:23:14,880 --> 00:23:18,480
single thread

00:23:16,240 --> 00:23:20,240
and then you see if you on the very far

00:23:18,480 --> 00:23:21,760
right if you enable

00:23:20,240 --> 00:23:24,320
both of those inter-procedural

00:23:21,760 --> 00:23:26,559
optimizations with

00:23:24,320 --> 00:23:27,440
the knowledge about these transitive

00:23:26,559 --> 00:23:29,600
calls

00:23:27,440 --> 00:23:32,080
what you get is basically the same

00:23:29,600 --> 00:23:35,280
performance as base sequential

00:23:32,080 --> 00:23:38,400
within the noise so so really

00:23:35,280 --> 00:23:40,720
adding these adding the domain knowledge

00:23:38,400 --> 00:23:42,960
and making sure that the compiler can do

00:23:40,720 --> 00:23:44,000
optimization across those outline

00:23:42,960 --> 00:23:46,559
functions

00:23:44,000 --> 00:23:47,279
is what you need to get your performance

00:23:46,559 --> 00:23:50,720
back

00:23:47,279 --> 00:23:53,039
that you kind of lost by parallelizing

00:23:50,720 --> 00:23:55,760
and then let's go to my esrat example

00:23:53,039 --> 00:23:58,960
that was really bad before 140

00:23:55,760 --> 00:23:59,600
slowdown and again if you combine both

00:23:58,960 --> 00:24:02,080
of those

00:23:59,600 --> 00:24:04,880
or actually if you um if you combine

00:24:02,080 --> 00:24:07,440
both of those

00:24:04,880 --> 00:24:09,039
analysis you get exactly the same

00:24:07,440 --> 00:24:10,000
performance as you had in the base

00:24:09,039 --> 00:24:12,559
version

00:24:10,000 --> 00:24:14,559
so teaching those inter-procedural

00:24:12,559 --> 00:24:17,120
optimizations about these apps

00:24:14,559 --> 00:24:18,240
about these transitive call sites makes

00:24:17,120 --> 00:24:19,840
the difference

00:24:18,240 --> 00:24:22,240
there is a couple of other things you

00:24:19,840 --> 00:24:23,840
you want to do that that didn't turn out

00:24:22,240 --> 00:24:26,480
in these benchmarks

00:24:23,840 --> 00:24:27,919
but the the main idea here is that even

00:24:26,480 --> 00:24:29,919
if you do early outlining

00:24:27,919 --> 00:24:31,919
even if you have the indirection through

00:24:29,919 --> 00:24:33,120
the runtime which is a good thing

00:24:31,919 --> 00:24:35,279
because it prevents you from

00:24:33,120 --> 00:24:39,440
accidentally miscompiling

00:24:35,279 --> 00:24:39,440
you can do optimization through

00:24:39,600 --> 00:24:44,480
selective modeling of the runtime

00:24:42,080 --> 00:24:47,200
behavior

00:24:44,480 --> 00:24:48,320
okay let's switch gears a little bit

00:24:47,200 --> 00:24:52,240
this was

00:24:48,320 --> 00:24:54,799
so far mostly about um

00:24:52,240 --> 00:24:56,799
like parallelism optimizations but they

00:24:54,799 --> 00:24:59,279
were not really open mp aware

00:24:56,799 --> 00:25:01,279
like we can do the same the same things

00:24:59,279 --> 00:25:04,080
for p threads or anything else so it

00:25:01,279 --> 00:25:06,240
wasn't really about openmp

00:25:04,080 --> 00:25:07,600
but we also have added or we have

00:25:06,240 --> 00:25:09,840
started to add openmp

00:25:07,600 --> 00:25:11,919
aware optimizations into the compiler

00:25:09,840 --> 00:25:13,919
this case llpm

00:25:11,919 --> 00:25:16,559
so what we really want to do is we want

00:25:13,919 --> 00:25:19,520
to have an openmp array compiler

00:25:16,559 --> 00:25:21,200
in the long run we want to have a

00:25:19,520 --> 00:25:24,000
parallelism aware compiler

00:25:21,200 --> 00:25:26,159
but openmp is a good first step and then

00:25:24,000 --> 00:25:28,880
generalize from there

00:25:26,159 --> 00:25:30,799
now remember in the very beginning i

00:25:28,880 --> 00:25:31,679
showed you the different parts of openmp

00:25:30,799 --> 00:25:33,679
and llvm

00:25:31,679 --> 00:25:35,039
and one of those parts was this openmp

00:25:33,679 --> 00:25:37,279
of path

00:25:35,039 --> 00:25:38,240
which is this inter-procedural path and

00:25:37,279 --> 00:25:39,600
by now you might have

00:25:38,240 --> 00:25:41,919
realized why this task is

00:25:39,600 --> 00:25:44,480
inter-procedural because as part of the

00:25:41,919 --> 00:25:46,960
openmp lowering we outline stuff

00:25:44,480 --> 00:25:48,799
so we have to be in the procedural to do

00:25:46,960 --> 00:25:51,840
reasoning between

00:25:48,799 --> 00:25:54,799
the location where the parallel walls

00:25:51,840 --> 00:25:55,120
and the parallel body that we outlined

00:25:54,799 --> 00:25:57,200
in

00:25:55,120 --> 00:25:58,400
the openmp up contains host and device

00:25:57,200 --> 00:26:00,720
optimizations

00:25:58,400 --> 00:26:01,440
and it runs by default now what can it

00:26:00,720 --> 00:26:04,880
do

00:26:01,440 --> 00:26:08,880
it knows about the openmp api so

00:26:04,880 --> 00:26:10,720
um get thread num in other calls

00:26:08,880 --> 00:26:12,960
and it also knows about most of the

00:26:10,720 --> 00:26:14,480
ethnos about most of the api calls and

00:26:12,960 --> 00:26:16,640
we're teaching more and more as we

00:26:14,480 --> 00:26:17,600
as we need them and it knows about the

00:26:16,640 --> 00:26:19,840
internal

00:26:17,600 --> 00:26:21,919
openmp runtime calls that were used to

00:26:19,840 --> 00:26:24,320
implement openmp

00:26:21,919 --> 00:26:25,120
and one thing it for example does it it

00:26:24,320 --> 00:26:27,760
annotates

00:26:25,120 --> 00:26:28,320
those calls with their potential effects

00:26:27,760 --> 00:26:30,799
so

00:26:28,320 --> 00:26:33,440
it knows that pretty much none of these

00:26:30,799 --> 00:26:36,320
calls can throw an exception

00:26:33,440 --> 00:26:38,400
before those calls were pure

00:26:36,320 --> 00:26:39,679
optimization barriers so when the

00:26:38,400 --> 00:26:42,000
compiler

00:26:39,679 --> 00:26:43,200
like saw one of those calls which was

00:26:42,000 --> 00:26:45,760
going to call into

00:26:43,200 --> 00:26:46,799
an unknown runtime it had to assume the

00:26:45,760 --> 00:26:50,159
worst

00:26:46,799 --> 00:26:52,240
that the program might exit and all

00:26:50,159 --> 00:26:53,760
visible state is changed or there is an

00:26:52,240 --> 00:26:56,159
exception thrown

00:26:53,760 --> 00:26:58,159
but now we actually embed this domain

00:26:56,159 --> 00:26:59,840
knowledge and now the potential effects

00:26:58,159 --> 00:27:01,840
of these runtime calls

00:26:59,840 --> 00:27:03,039
is known to the rest of the lrvm

00:27:01,840 --> 00:27:06,559
compiler

00:27:03,039 --> 00:27:08,320
and therefore more optimization

00:27:06,559 --> 00:27:09,679
we also perform very high level

00:27:08,320 --> 00:27:13,440
optimizations that are

00:27:09,679 --> 00:27:15,919
kind of not part of any other llcm path

00:27:13,440 --> 00:27:17,039
for example parallel region merging it's

00:27:15,919 --> 00:27:19,360
not like that is

00:27:17,039 --> 00:27:20,399
it is a thing that you have to have

00:27:19,360 --> 00:27:23,039
awareness

00:27:20,399 --> 00:27:24,399
of what a parallel region is what it

00:27:23,039 --> 00:27:27,120
means to merge them

00:27:24,399 --> 00:27:27,600
what you have to to verify that that is

00:27:27,120 --> 00:27:30,159
sound

00:27:27,600 --> 00:27:32,559
and how to modify the code to actually

00:27:30,159 --> 00:27:34,480
make it happen and when for example

00:27:32,559 --> 00:27:36,480
what do you do with sequential code that

00:27:34,480 --> 00:27:38,880
is in between parallel regions

00:27:36,480 --> 00:27:39,679
can you execute it redundantly or do you

00:27:38,880 --> 00:27:42,480
have to

00:27:39,679 --> 00:27:44,320
um guard it and execute it single thread

00:27:42,480 --> 00:27:46,320
it and have to kind of

00:27:44,320 --> 00:27:48,080
introduce barriers and propagate the

00:27:46,320 --> 00:27:48,960
results among all the threats and so on

00:27:48,080 --> 00:27:51,039
and so forth

00:27:48,960 --> 00:27:52,640
so all of this reasoning is kind of on a

00:27:51,039 --> 00:27:53,440
high level because you have to argue

00:27:52,640 --> 00:27:56,640
about

00:27:53,440 --> 00:28:00,960
things that are not

00:27:56,640 --> 00:28:02,080
explicit in the ir it also has a lot of

00:28:00,960 --> 00:28:04,960
gpu specific

00:28:02,080 --> 00:28:07,039
optimizations that run late in the

00:28:04,960 --> 00:28:10,320
optimization pipeline and we're going to

00:28:07,039 --> 00:28:10,320
see some of them later on

00:28:10,480 --> 00:28:16,720
now the last thing is they are

00:28:13,679 --> 00:28:17,679
like historically it was clang uh the

00:28:16,720 --> 00:28:21,039
front end that did

00:28:17,679 --> 00:28:22,960
optimizations in quotes so because we

00:28:21,039 --> 00:28:24,960
didn't have openmp opt

00:28:22,960 --> 00:28:27,679
all of the kind of tricks that we wanted

00:28:24,960 --> 00:28:30,720
to do when we compiled openmp

00:28:27,679 --> 00:28:31,279
had to happen in the front end and that

00:28:30,720 --> 00:28:34,399
had

00:28:31,279 --> 00:28:34,399
a couple of um

00:28:34,640 --> 00:28:38,320
not so nice side effects one of them is

00:28:37,600 --> 00:28:40,880
that

00:28:38,320 --> 00:28:42,559
it was very syntax driven that is if you

00:28:40,880 --> 00:28:45,279
had two programs that were pretty

00:28:42,559 --> 00:28:47,200
but were equivalent but the syntax was a

00:28:45,279 --> 00:28:48,720
little different between them

00:28:47,200 --> 00:28:50,559
one of them might be might have been a

00:28:48,720 --> 00:28:52,399
lot faster than the other

00:28:50,559 --> 00:28:54,320
and it was not really clear to anyone

00:28:52,399 --> 00:28:55,440
why that didn't really know what klang

00:28:54,320 --> 00:28:58,320
was doing

00:28:55,440 --> 00:29:00,080
and it was so far no way for like to ask

00:28:58,320 --> 00:29:02,480
kling what it was doing anyway

00:29:00,080 --> 00:29:04,240
so it was kind of a black box that

00:29:02,480 --> 00:29:05,600
sometimes performed well and sometimes

00:29:04,240 --> 00:29:07,679
didn't

00:29:05,600 --> 00:29:09,840
and it also introduced a lot of

00:29:07,679 --> 00:29:10,720
complexity which complexity leads to

00:29:09,840 --> 00:29:13,279
errors

00:29:10,720 --> 00:29:13,919
so what we are going what we are already

00:29:13,279 --> 00:29:16,880
doing for

00:29:13,919 --> 00:29:18,320
now years is we are removing a lot of

00:29:16,880 --> 00:29:20,799
the smarts from

00:29:18,320 --> 00:29:22,559
clang and putting them into a middle end

00:29:20,799 --> 00:29:23,760
and has a lot of benefits one of which

00:29:22,559 --> 00:29:25,919
is also it will

00:29:23,760 --> 00:29:26,880
all of these optimizations will also

00:29:25,919 --> 00:29:28,320
trigger for

00:29:26,880 --> 00:29:31,120
flank when you have the fortran front

00:29:28,320 --> 00:29:33,039
end and the middle end is where all the

00:29:31,120 --> 00:29:34,559
other optimizations are done so keep the

00:29:33,039 --> 00:29:36,480
front end simple

00:29:34,559 --> 00:29:37,679
and emit code that can be analyzed and

00:29:36,480 --> 00:29:40,000
optimized later on

00:29:37,679 --> 00:29:42,159
that's the that's the thing that we're

00:29:40,000 --> 00:29:45,600
doing here

00:29:42,159 --> 00:29:46,240
okay now talking about you don't really

00:29:45,600 --> 00:29:49,039
know what

00:29:46,240 --> 00:29:52,159
happened and clang either performed well

00:29:49,039 --> 00:29:54,159
or gave you code that was slow

00:29:52,159 --> 00:29:56,480
one of the things we do with the openmp

00:29:54,159 --> 00:29:57,279
opt is we really uh put a lot of effort

00:29:56,480 --> 00:29:59,760
in adding

00:29:57,279 --> 00:30:01,919
optimization remarks such that if you

00:29:59,760 --> 00:30:05,440
run this program here

00:30:01,919 --> 00:30:06,159
you want to know the lvm compiler will

00:30:05,440 --> 00:30:07,919
tell you

00:30:06,159 --> 00:30:10,000
that it actually merged those two

00:30:07,919 --> 00:30:11,279
runtime calls from get thread limit into

00:30:10,000 --> 00:30:14,480
one

00:30:11,279 --> 00:30:17,279
so if you run this and you say our path

00:30:14,480 --> 00:30:18,240
equals openmp opt which which is the

00:30:17,279 --> 00:30:21,360
remarks

00:30:18,240 --> 00:30:22,320
interface from clang it will tell you

00:30:21,360 --> 00:30:25,200
that it moved

00:30:22,320 --> 00:30:26,240
one of those calls into like a different

00:30:25,200 --> 00:30:28,799
position

00:30:26,240 --> 00:30:30,480
and then it deduplicated the calls so

00:30:28,799 --> 00:30:32,559
instead of calling the function twice

00:30:30,480 --> 00:30:33,919
it just calls it once and you resist the

00:30:32,559 --> 00:30:36,480
result because

00:30:33,919 --> 00:30:40,399
openmp is aware that the result of that

00:30:36,480 --> 00:30:40,399
function will not change in this scope

00:30:41,200 --> 00:30:47,440
then um there is also

00:30:45,279 --> 00:30:49,120
like we have also remarks for target

00:30:47,440 --> 00:30:52,159
scheduling so if you have

00:30:49,120 --> 00:30:54,080
a code that is offloaded to a gpu

00:30:52,159 --> 00:30:56,720
there is a lot of things that we want to

00:30:54,080 --> 00:30:59,519
do with regards to optimizations

00:30:56,720 --> 00:31:01,279
that are important for performance and

00:30:59,519 --> 00:31:04,640
register

00:31:01,279 --> 00:31:06,240
utilization and those things they made a

00:31:04,640 --> 00:31:09,120
lot of remarks they tell you

00:31:06,240 --> 00:31:10,640
if it worked what worked and what didn't

00:31:09,120 --> 00:31:14,399
so if you run this

00:31:10,640 --> 00:31:17,360
in lvm12 you get a a wall of text

00:31:14,399 --> 00:31:19,360
um it's kind of it wasn't great and

00:31:17,360 --> 00:31:21,279
we're working on it to make it better

00:31:19,360 --> 00:31:23,120
i'll explain what the what the wall of

00:31:21,279 --> 00:31:24,799
text kind of means or basically what

00:31:23,120 --> 00:31:28,480
optimization we're performing a little

00:31:24,799 --> 00:31:30,720
bit later so let's skip this for now

00:31:28,480 --> 00:31:32,880
now in addition to this compile time

00:31:30,720 --> 00:31:34,720
information we're also working very hard

00:31:32,880 --> 00:31:36,240
to embed better runtime information

00:31:34,720 --> 00:31:37,919
because at the end of the day

00:31:36,240 --> 00:31:39,679
if the compiler is a black box and the

00:31:37,919 --> 00:31:42,240
runtime is a black box

00:31:39,679 --> 00:31:44,000
people are unhappy because they want to

00:31:42,240 --> 00:31:44,720
know what is going on and i understand

00:31:44,000 --> 00:31:47,679
that

00:31:44,720 --> 00:31:48,799
because they want to know why things are

00:31:47,679 --> 00:31:50,640
slow or fast

00:31:48,799 --> 00:31:52,320
what they did right or wrong what the

00:31:50,640 --> 00:31:53,440
compiler decided and where they have to

00:31:52,320 --> 00:31:55,919
help

00:31:53,440 --> 00:31:58,240
so i would encourage everyone to use

00:31:55,919 --> 00:32:00,320
optimization remarks they exist not only

00:31:58,240 --> 00:32:02,480
in llvm client but they also exist in

00:32:00,320 --> 00:32:05,200
other compilers

00:32:02,480 --> 00:32:08,559
for the lbm client case we have remark

00:32:05,200 --> 00:32:12,080
explanation examples faqs that we add

00:32:08,559 --> 00:32:14,399
over time to the openmp llvm org stocks

00:32:12,080 --> 00:32:17,120
so the documentation page there

00:32:14,399 --> 00:32:18,159
is the relevant new web page that you

00:32:17,120 --> 00:32:20,960
should kind of visit

00:32:18,159 --> 00:32:21,840
and potentially revisit over time

00:32:20,960 --> 00:32:24,559
because it

00:32:21,840 --> 00:32:26,320
it's kind of new but it already contains

00:32:24,559 --> 00:32:28,960
a lot of interesting information

00:32:26,320 --> 00:32:31,200
especially about the environment flags

00:32:28,960 --> 00:32:32,960
that you can use to talk to the runtime

00:32:31,200 --> 00:32:35,039
and one of the most important ones here

00:32:32,960 --> 00:32:38,799
is lip balm target info

00:32:35,039 --> 00:32:39,679
you can use that to let the run time and

00:32:38,799 --> 00:32:43,440
i mean the

00:32:39,679 --> 00:32:46,799
the generic fast

00:32:43,440 --> 00:32:48,240
release runtime tell you what it did and

00:32:46,799 --> 00:32:50,799
why it did things

00:32:48,240 --> 00:32:52,559
and it also helps you a lot to debug

00:32:50,799 --> 00:32:54,399
potential errors because it gives you

00:32:52,559 --> 00:32:55,120
information about what pointers are

00:32:54,399 --> 00:32:57,519
mapped

00:32:55,120 --> 00:32:59,279
when is data copied how are kernels

00:32:57,519 --> 00:33:00,880
started and so on and so forth so there

00:32:59,279 --> 00:33:02,640
is quite a bit of things that you can

00:33:00,880 --> 00:33:05,200
get from the runtime nowadays

00:33:02,640 --> 00:33:06,559
even in release mode so i would

00:33:05,200 --> 00:33:08,799
encourage everyone to take a look at

00:33:06,559 --> 00:33:08,799
those

00:33:08,880 --> 00:33:12,480
okay let's talk a little bit about

00:33:10,240 --> 00:33:14,080
openmp offloading uh we will kind of a

00:33:12,480 --> 00:33:16,960
little bit to this so

00:33:14,080 --> 00:33:19,279
when you have by default you have the

00:33:16,960 --> 00:33:20,080
host openmp compilation which takes the

00:33:19,279 --> 00:33:23,360
c input

00:33:20,080 --> 00:33:24,080
preprocessor compiler backend assembler

00:33:23,360 --> 00:33:26,880
linker

00:33:24,080 --> 00:33:27,679
that's your usual compilation pipeline

00:33:26,880 --> 00:33:30,960
now with

00:33:27,679 --> 00:33:31,679
um device offloading we take the same

00:33:30,960 --> 00:33:35,039
input again

00:33:31,679 --> 00:33:37,039
and compile it again with another

00:33:35,039 --> 00:33:38,559
like in llvm we actually run a second

00:33:37,039 --> 00:33:40,159
preprocessor run

00:33:38,559 --> 00:33:41,919
we run a second compiler and then we

00:33:40,159 --> 00:33:44,159
have kind of an offload step

00:33:41,919 --> 00:33:45,840
that takes information from the host

00:33:44,159 --> 00:33:47,600
compilation

00:33:45,840 --> 00:33:49,200
as part of the device compilation and

00:33:47,600 --> 00:33:52,159
this is kind of crucial here

00:33:49,200 --> 00:33:53,200
but i'm not going to go into much detail

00:33:52,159 --> 00:33:55,679
then we go

00:33:53,200 --> 00:33:56,240
through the assembler the object file

00:33:55,679 --> 00:33:58,320
linker

00:33:56,240 --> 00:34:00,559
same as before but this time we just

00:33:58,320 --> 00:34:02,799
have a different target in this case

00:34:00,559 --> 00:34:05,760
if you see in the top the target that we

00:34:02,799 --> 00:34:09,280
compile for is mvpdx64

00:34:05,760 --> 00:34:12,720
so once we link and have

00:34:09,280 --> 00:34:15,679
an image what we do is we invoke an

00:34:12,720 --> 00:34:17,760
additional step that embeds that image

00:34:15,679 --> 00:34:19,200
into the image of the host

00:34:17,760 --> 00:34:21,760
because at the end of the day what you

00:34:19,200 --> 00:34:23,520
want is a single object file

00:34:21,760 --> 00:34:25,119
so you want all your tooling all your

00:34:23,520 --> 00:34:27,359
build setup to work

00:34:25,119 --> 00:34:29,359
even if you do offloading so we have to

00:34:27,359 --> 00:34:31,599
only generate a single file and we have

00:34:29,359 --> 00:34:34,800
to make everything work

00:34:31,599 --> 00:34:38,079
as it did work before but now there is

00:34:34,800 --> 00:34:40,560
two targets or even more potentially

00:34:38,079 --> 00:34:40,560
involved

00:34:41,200 --> 00:34:45,440
openmp offloading has quite a few tricky

00:34:43,599 --> 00:34:47,440
bits and i'm going to talk about two or

00:34:45,440 --> 00:34:49,599
three of them now

00:34:47,440 --> 00:34:50,800
one of them is is kind of sketched here

00:34:49,599 --> 00:34:53,280
so it's

00:34:50,800 --> 00:34:55,119
it's the use of math uh functions but

00:34:53,280 --> 00:34:57,040
it's not only math it could be any

00:34:55,119 --> 00:34:58,640
system header or any library at the end

00:34:57,040 --> 00:35:00,320
of today

00:34:58,640 --> 00:35:02,240
so if you have this function called

00:35:00,320 --> 00:35:04,079
science here that is inside of a begin

00:35:02,240 --> 00:35:07,359
declare target

00:35:04,079 --> 00:35:10,000
it uses the math front sign sine bit

00:35:07,359 --> 00:35:12,160
f so to determine what the sign of the

00:35:10,000 --> 00:35:14,560
float f is

00:35:12,160 --> 00:35:16,480
now the crucial part is science this

00:35:14,560 --> 00:35:17,599
function can be called from the host or

00:35:16,480 --> 00:35:19,599
and the device

00:35:17,599 --> 00:35:21,040
so it they have to be two versions of

00:35:19,599 --> 00:35:22,560
that function one that is called

00:35:21,040 --> 00:35:24,720
that is that is available on the host

00:35:22,560 --> 00:35:27,680
one that is available on the device

00:35:24,720 --> 00:35:28,160
this would all be fine if math.h would

00:35:27,680 --> 00:35:30,640
be

00:35:28,160 --> 00:35:31,760
relatively the same but if you open up

00:35:30,640 --> 00:35:33,359
your math.h

00:35:31,760 --> 00:35:35,599
and then go through all the other

00:35:33,359 --> 00:35:39,040
include files that it that it uh

00:35:35,599 --> 00:35:40,320
potentially has you potentially and

00:35:39,040 --> 00:35:43,440
depending on your version

00:35:40,320 --> 00:35:46,800
may or may not end up at code like this

00:35:43,440 --> 00:35:48,079
and sorry the important part here is

00:35:46,800 --> 00:35:50,560
that

00:35:48,079 --> 00:35:52,880
it has this if def where it checks if

00:35:50,560 --> 00:35:55,040
sse2 is available

00:35:52,880 --> 00:35:57,119
and used for map and then it uses this

00:35:55,040 --> 00:35:59,520
as inline assembly instruction that kind

00:35:57,119 --> 00:36:02,320
of only works on x86

00:35:59,520 --> 00:36:04,720
now if you compile this for a cuda gpu

00:36:02,320 --> 00:36:08,000
that inline assembly will not work

00:36:04,720 --> 00:36:09,680
i mean it will not compile so

00:36:08,000 --> 00:36:11,520
you basically have two different

00:36:09,680 --> 00:36:14,720
versions of this function

00:36:11,520 --> 00:36:16,640
for the host in the device and you have

00:36:14,720 --> 00:36:19,760
to be kind of careful on what to do

00:36:16,640 --> 00:36:21,280
in in there is potentially not and and

00:36:19,760 --> 00:36:22,720
this is only one of the problems the

00:36:21,280 --> 00:36:24,800
other problem is

00:36:22,720 --> 00:36:26,079
gpus actually don't come with a master

00:36:24,800 --> 00:36:28,240
mass.h

00:36:26,079 --> 00:36:30,079
they don't even come with a lib app like

00:36:28,240 --> 00:36:32,000
the math library

00:36:30,079 --> 00:36:34,640
and that is a real problem now because

00:36:32,000 --> 00:36:35,440
now if if something in math.h on your

00:36:34,640 --> 00:36:38,640
host

00:36:35,440 --> 00:36:40,480
is not defined but only declared

00:36:38,640 --> 00:36:42,720
and then it the implementation is inside

00:36:40,480 --> 00:36:44,640
of lipim what are you going to do

00:36:42,720 --> 00:36:46,320
you still want people to be able to use

00:36:44,640 --> 00:36:49,839
those functions

00:36:46,320 --> 00:36:51,599
so what what llvm clan does it provides

00:36:49,839 --> 00:36:53,359
the master age wrapper

00:36:51,599 --> 00:36:55,119
among other things it's not only math

00:36:53,359 --> 00:36:56,960
but but one thing that it provides is a

00:36:55,119 --> 00:37:00,079
mass.h wrapper

00:36:56,960 --> 00:37:03,520
that that defines

00:37:00,079 --> 00:37:06,320
a lot of the math functions and then

00:37:03,520 --> 00:37:07,440
it uses the declare variant feature to

00:37:06,320 --> 00:37:11,680
overload them

00:37:07,440 --> 00:37:14,640
so if you call sign bit f on the

00:37:11,680 --> 00:37:16,400
device you actually get the function

00:37:14,640 --> 00:37:19,520
that we have in this wrapper

00:37:16,400 --> 00:37:20,560
which then is implemented through means

00:37:19,520 --> 00:37:24,240
in this case for

00:37:20,560 --> 00:37:26,320
nvidia uh built-ins and then the same

00:37:24,240 --> 00:37:29,200
the same trick applies to all other gpus

00:37:26,320 --> 00:37:29,200
and so on and so forth

00:37:29,520 --> 00:37:33,520
okay so then the other tricky bit is

00:37:31,520 --> 00:37:35,119
linking as i said before you want all

00:37:33,520 --> 00:37:36,240
your build system and everything to work

00:37:35,119 --> 00:37:38,960
as it did before

00:37:36,240 --> 00:37:40,960
but that is actually really hard to do

00:37:38,960 --> 00:37:43,760
and i get sad when i think about it so

00:37:40,960 --> 00:37:45,920
we're not going to talk about it today

00:37:43,760 --> 00:37:47,920
instead we're going to talk about openmp

00:37:45,920 --> 00:37:50,400
offloading versus kernel languages and i

00:37:47,920 --> 00:37:52,960
have to kind of speed up a little bit

00:37:50,400 --> 00:37:53,520
so in the top you see how you start a

00:37:52,960 --> 00:37:55,200
kernel

00:37:53,520 --> 00:37:56,720
in in one of the popular kernel

00:37:55,200 --> 00:37:59,040
languages where the front

00:37:56,720 --> 00:38:01,200
is the the function that you start you

00:37:59,040 --> 00:38:03,839
start it with one block and four threads

00:38:01,200 --> 00:38:05,520
in the arguments on the bottom you see

00:38:03,839 --> 00:38:07,200
pretty much the same code except that

00:38:05,520 --> 00:38:10,720
you also execute this a

00:38:07,200 --> 00:38:13,280
and this b in between before and after

00:38:10,720 --> 00:38:15,440
the sum function

00:38:13,280 --> 00:38:16,400
if you look at how that that translates

00:38:15,440 --> 00:38:19,200
visually

00:38:16,400 --> 00:38:20,960
so for the upper part you have a block

00:38:19,200 --> 00:38:22,960
and you start for threads

00:38:20,960 --> 00:38:24,560
and then all of the threads execute func

00:38:22,960 --> 00:38:27,520
and that is it

00:38:24,560 --> 00:38:28,800
on the lower part conceptually you start

00:38:27,520 --> 00:38:30,800
one thread

00:38:28,800 --> 00:38:32,160
and then you execute a and then you

00:38:30,800 --> 00:38:34,160
start the rest of the threads

00:38:32,160 --> 00:38:35,920
the other three and all of them execute

00:38:34,160 --> 00:38:39,440
func and then you

00:38:35,920 --> 00:38:43,040
the one thread executes b and that's it

00:38:39,440 --> 00:38:44,839
however this is not how gpus work um

00:38:43,040 --> 00:38:46,640
the parts that are highlighted in blue

00:38:44,839 --> 00:38:49,839
here they

00:38:46,640 --> 00:38:51,520
they they can't exist so you cannot just

00:38:49,839 --> 00:38:53,280
kind of start the threads when you need

00:38:51,520 --> 00:38:54,640
them they are started at the very

00:38:53,280 --> 00:38:56,160
beginning and they exist

00:38:54,640 --> 00:38:58,720
i know you have to do something with

00:38:56,160 --> 00:39:00,480
them so what happens in the runtime is

00:38:58,720 --> 00:39:02,560
they execute something else so they

00:39:00,480 --> 00:39:06,000
execute something blue

00:39:02,560 --> 00:39:08,720
while they wait for to be

00:39:06,000 --> 00:39:10,880
told what function they're supposed to

00:39:08,720 --> 00:39:14,000
execute for real

00:39:10,880 --> 00:39:16,079
now um in llvm speak the upper one

00:39:14,000 --> 00:39:17,839
is called spmd mode when everyone

00:39:16,079 --> 00:39:20,000
executes the same stuff

00:39:17,839 --> 00:39:21,760
and the lower one is called generic mode

00:39:20,000 --> 00:39:22,880
when there's one thread that executes

00:39:21,760 --> 00:39:25,040
user code

00:39:22,880 --> 00:39:28,000
and in the other threads the workers

00:39:25,040 --> 00:39:31,599
execute only the parallel regions

00:39:28,000 --> 00:39:35,760
how is that um before we go to that

00:39:31,599 --> 00:39:38,800
they you might think why don't we just

00:39:35,760 --> 00:39:40,079
um use conditionals we say every thread

00:39:38,800 --> 00:39:42,480
starts at the beginning

00:39:40,079 --> 00:39:43,920
and we execute this a part and this b

00:39:42,480 --> 00:39:45,760
part only by the master

00:39:43,920 --> 00:39:47,359
by the main thread so we use a

00:39:45,760 --> 00:39:49,680
conditional and say

00:39:47,359 --> 00:39:50,880
oh a is only executed by the by the

00:39:49,680 --> 00:39:52,880
first thread

00:39:50,880 --> 00:39:54,480
then we wait we have a barrier so

00:39:52,880 --> 00:39:56,400
everyone is on the same page

00:39:54,480 --> 00:39:58,000
then everyone executes front with

00:39:56,400 --> 00:40:01,040
barrier again and then

00:39:58,000 --> 00:40:02,960
only the first thread executes b this is

00:40:01,040 --> 00:40:03,680
a nice idea and this optimization

00:40:02,960 --> 00:40:06,000
actually

00:40:03,680 --> 00:40:07,040
is about to happen inside of llvm if

00:40:06,000 --> 00:40:09,280
it's legal

00:40:07,040 --> 00:40:10,480
but it's obvious it's unfortunately not

00:40:09,280 --> 00:40:13,680
always legal

00:40:10,480 --> 00:40:16,400
so if the if the the green part for

00:40:13,680 --> 00:40:18,720
example contains it on parallel

00:40:16,400 --> 00:40:20,560
your behavior will change and not only

00:40:18,720 --> 00:40:22,720
will it change it will probably be

00:40:20,560 --> 00:40:23,599
not what the user wanted because now

00:40:22,720 --> 00:40:26,720
this kernel too

00:40:23,599 --> 00:40:28,720
is not executed in parallel at all

00:40:26,720 --> 00:40:31,680
because all the threads are busy kind of

00:40:28,720 --> 00:40:31,680
waiting at the barrier

00:40:32,000 --> 00:40:35,040
and if you have a barrier inside of this

00:40:34,079 --> 00:40:37,119
yellow part

00:40:35,040 --> 00:40:39,200
you even have a bigger problem because

00:40:37,119 --> 00:40:41,119
now not every thread in your team will

00:40:39,200 --> 00:40:45,040
actually reach that barrier

00:40:41,119 --> 00:40:46,800
and that is kind of bad so

00:40:45,040 --> 00:40:49,599
this optimization usually called

00:40:46,800 --> 00:40:51,359
espionization is coming soon

00:40:49,599 --> 00:40:53,200
for the cases that we can prove

00:40:51,359 --> 00:40:55,440
everything is legal and we're going to

00:40:53,200 --> 00:40:58,960
make it stronger for aluminum-13

00:40:55,440 --> 00:40:59,839
so watch out for that but how is it

00:40:58,960 --> 00:41:02,000
really done

00:40:59,839 --> 00:41:04,000
in right now how is generic mode

00:41:02,000 --> 00:41:06,240
implemented

00:41:04,000 --> 00:41:08,000
there is this one thread the the main

00:41:06,240 --> 00:41:08,480
thread that actually executes the green

00:41:08,000 --> 00:41:11,520
block

00:41:08,480 --> 00:41:14,800
then front and then the um yellow block

00:41:11,520 --> 00:41:15,680
okay the other threads are executing

00:41:14,800 --> 00:41:18,560
something else

00:41:15,680 --> 00:41:20,000
and they are waiting in this bubble and

00:41:18,560 --> 00:41:22,800
then at the point when we

00:41:20,000 --> 00:41:24,400
hit a parallel region before and after

00:41:22,800 --> 00:41:26,640
the main thread is going to

00:41:24,400 --> 00:41:27,680
tell the other threads what the parallel

00:41:26,640 --> 00:41:30,480
region is

00:41:27,680 --> 00:41:30,960
that it just encountered so it tells

00:41:30,480 --> 00:41:33,680
them

00:41:30,960 --> 00:41:35,839
it's the grey block and then the others

00:41:33,680 --> 00:41:38,160
are able to execute the grey block

00:41:35,839 --> 00:41:39,760
and hit another bubble where they wait

00:41:38,160 --> 00:41:42,079
then they tell the main thread

00:41:39,760 --> 00:41:43,760
they are done so everyone is done

00:41:42,079 --> 00:41:45,839
executing fun

00:41:43,760 --> 00:41:47,839
and then the main thread can continue

00:41:45,839 --> 00:41:49,760
while the the worker threads go back to

00:41:47,839 --> 00:41:51,839
the initial bubble and weight

00:41:49,760 --> 00:41:54,640
the right part is usually referred to as

00:41:51,839 --> 00:41:58,079
this the worker statement

00:41:54,640 --> 00:42:00,800
this is how lvmx executes generate mode

00:41:58,079 --> 00:42:02,079
now you have to see that this grey box

00:42:00,800 --> 00:42:05,119
here is actually a function

00:42:02,079 --> 00:42:07,040
pointer that represents this

00:42:05,119 --> 00:42:09,599
that represents this parallel region

00:42:07,040 --> 00:42:10,880
because we have to somehow identify a

00:42:09,599 --> 00:42:13,200
parallel region

00:42:10,880 --> 00:42:14,240
between the the main thread and the

00:42:13,200 --> 00:42:16,160
workers

00:42:14,240 --> 00:42:17,680
and the function pointer we outlined the

00:42:16,160 --> 00:42:19,839
parallel region into

00:42:17,680 --> 00:42:21,920
is it is not only an obvious choice but

00:42:19,839 --> 00:42:24,720
it's pretty much the only choice

00:42:21,920 --> 00:42:25,839
if the um it's pretty much the only

00:42:24,720 --> 00:42:29,359
choice in a generic

00:42:25,839 --> 00:42:31,200
setting now the problem with passing

00:42:29,359 --> 00:42:32,480
function pointers on the gpu is that you

00:42:31,200 --> 00:42:35,440
have indirect calls

00:42:32,480 --> 00:42:36,960
and very inspirious call edges which

00:42:35,440 --> 00:42:37,839
increase your register account quite a

00:42:36,960 --> 00:42:39,440
bit

00:42:37,839 --> 00:42:41,280
so what you really want to do is you

00:42:39,440 --> 00:42:42,240
want to try to avoid the function point

00:42:41,280 --> 00:42:43,760
i use

00:42:42,240 --> 00:42:45,920
and use some other kind of

00:42:43,760 --> 00:42:48,400
identification some other kind of

00:42:45,920 --> 00:42:50,640
ids that are not function pointers let's

00:42:48,400 --> 00:42:54,880
take a look at that optimization

00:42:50,640 --> 00:42:56,000
or yes so um this on the left is how the

00:42:54,880 --> 00:42:59,040
code really looks

00:42:56,000 --> 00:43:00,640
uh simplified on the gpu so the kernel

00:42:59,040 --> 00:43:02,640
is executed and then the

00:43:00,640 --> 00:43:04,079
threads are split into workers in the

00:43:02,640 --> 00:43:05,839
main thread

00:43:04,079 --> 00:43:07,839
the workers wait in the state machine

00:43:05,839 --> 00:43:09,119
this while one loop they wait for a new

00:43:07,839 --> 00:43:11,280
polar region to arrive

00:43:09,119 --> 00:43:12,880
to execute it and they signal that

00:43:11,280 --> 00:43:14,960
they're done

00:43:12,880 --> 00:43:16,800
the main thread is going to tell the

00:43:14,960 --> 00:43:18,240
workers here's the function pointer of

00:43:16,800 --> 00:43:20,560
the parallel region

00:43:18,240 --> 00:43:22,160
and then everyone executes it and then

00:43:20,560 --> 00:43:23,920
the main threat waits for the workers

00:43:22,160 --> 00:43:25,520
so this is this is what we discussed

00:43:23,920 --> 00:43:27,280
before already

00:43:25,520 --> 00:43:28,960
and what we can do with this is we can

00:43:27,280 --> 00:43:31,520
replace the function pointer

00:43:28,960 --> 00:43:32,400
by a unique id that represents the

00:43:31,520 --> 00:43:35,920
function

00:43:32,400 --> 00:43:39,200
so we we create a character for example

00:43:35,920 --> 00:43:41,520
uh in static uh character in the global

00:43:39,200 --> 00:43:44,240
space and we use that address

00:43:41,520 --> 00:43:45,599
to refer to the function so now we never

00:43:44,240 --> 00:43:47,760
take the address of

00:43:45,599 --> 00:43:48,880
the power function and we never pass a

00:43:47,760 --> 00:43:51,200
function binder

00:43:48,880 --> 00:43:52,240
that actually is a really big

00:43:51,200 --> 00:43:55,839
improvement over

00:43:52,240 --> 00:43:59,200
the code before this works except

00:43:55,839 --> 00:44:02,000
if the parallel region is accessible

00:43:59,200 --> 00:44:03,599
from the outside so if a kernel that we

00:44:02,000 --> 00:44:05,280
might not know

00:44:03,599 --> 00:44:06,640
could access that parallel region

00:44:05,280 --> 00:44:09,200
because now

00:44:06,640 --> 00:44:10,319
how is the kernel that we don't see

00:44:09,200 --> 00:44:12,720
going to know

00:44:10,319 --> 00:44:14,160
about this this id that we introduce or

00:44:12,720 --> 00:44:16,480
that we want to introduce

00:44:14,160 --> 00:44:18,640
it isn't so we still have to pass this

00:44:16,480 --> 00:44:21,119
function pointer

00:44:18,640 --> 00:44:23,280
if we had like an openmp assumes that

00:44:21,119 --> 00:44:24,640
says there are no external callers to

00:44:23,280 --> 00:44:26,640
this function

00:44:24,640 --> 00:44:28,000
we could still do the trick and this is

00:44:26,640 --> 00:44:29,200
kind of where we are going right now

00:44:28,000 --> 00:44:31,760
we're going to

00:44:29,200 --> 00:44:32,800
emit remarks that tell you why things

00:44:31,760 --> 00:44:34,400
didn't work out

00:44:32,800 --> 00:44:36,880
and we're going to ask the user to

00:44:34,400 --> 00:44:37,760
provide these additional information for

00:44:36,880 --> 00:44:40,400
example through

00:44:37,760 --> 00:44:42,720
lvm through openmp assumes that were

00:44:40,400 --> 00:44:44,960
introduced in 5.1

00:44:42,720 --> 00:44:47,359
and then users get better info get

00:44:44,960 --> 00:44:49,839
better performance

00:44:47,359 --> 00:44:52,000
okay so lvm13 will know about this and a

00:44:49,839 --> 00:44:54,400
couple of other things in this area

00:44:52,000 --> 00:44:56,400
let's let's skip ahead but don't forget

00:44:54,400 --> 00:44:59,920
to use remarks because that will also

00:44:56,400 --> 00:44:59,920
tell you about missed opportunities

00:45:00,240 --> 00:45:04,640
let's skip ahead what oh let's do

00:45:02,640 --> 00:45:05,680
openmp.wrong but we skip ahead over the

00:45:04,640 --> 00:45:08,960
rest

00:45:05,680 --> 00:45:12,319
anything that retroactively changes

00:45:08,960 --> 00:45:15,440
code any directive is bad idea

00:45:12,319 --> 00:45:17,760
so here you see the static index

00:45:15,440 --> 00:45:19,119
and you see that we use the alignment of

00:45:17,760 --> 00:45:22,480
x right away

00:45:19,119 --> 00:45:25,440
and then we kind of use the we use x to

00:45:22,480 --> 00:45:26,160
uh inside of a function and after all of

00:45:25,440 --> 00:45:28,319
this

00:45:26,160 --> 00:45:30,240
we change the way x is allocated and

00:45:28,319 --> 00:45:32,960
what the alignment of x is

00:45:30,240 --> 00:45:34,960
and that's kind of bad and there's not

00:45:32,960 --> 00:45:35,280
only that there's other examples this is

00:45:34,960 --> 00:45:36,960
just

00:45:35,280 --> 00:45:39,040
just one of them so anything that

00:45:36,960 --> 00:45:39,920
retroactively changes something is a bad

00:45:39,040 --> 00:45:41,839
idea

00:45:39,920 --> 00:45:43,359
it can be implemented but it's usually a

00:45:41,839 --> 00:45:46,319
hassle and the

00:45:43,359 --> 00:45:48,000
art i'm not really sure we would do that

00:45:46,319 --> 00:45:49,280
and then there's a there's a fixation on

00:45:48,000 --> 00:45:51,359
syntactic nesting

00:45:49,280 --> 00:45:53,440
so if you have an um target with atomic

00:45:51,359 --> 00:45:54,960
update inside that's fine

00:45:53,440 --> 00:45:56,560
if you have enough target teams with an

00:45:54,960 --> 00:45:59,520
atomic

00:45:56,560 --> 00:46:01,440
atomic update inside that's not fine but

00:45:59,520 --> 00:46:02,240
if you then outline this code into a

00:46:01,440 --> 00:46:05,280
function foo

00:46:02,240 --> 00:46:06,160
that's fine again so it's really like

00:46:05,280 --> 00:46:09,440
not only about

00:46:06,160 --> 00:46:11,599
atomic accesses in teams regions

00:46:09,440 --> 00:46:13,280
it's more about the idea that we have to

00:46:11,599 --> 00:46:15,599
kind of define everything with regard to

00:46:13,280 --> 00:46:18,800
syntactic nesting that i think

00:46:15,599 --> 00:46:22,319
is inherently wrong and makes things

00:46:18,800 --> 00:46:23,680
very much awkward half the time

00:46:22,319 --> 00:46:26,000
there's a lot of things that open if you

00:46:23,680 --> 00:46:26,960
got right and the device abstraction is

00:46:26,000 --> 00:46:29,280
one of them

00:46:26,960 --> 00:46:30,960
so we see that you can have one gpu that

00:46:29,280 --> 00:46:32,960
would be your device one

00:46:30,960 --> 00:46:35,200
second gpu that would be your device

00:46:32,960 --> 00:46:37,040
sorry two by zero and then device one

00:46:35,200 --> 00:46:38,960
but the device abstraction allows you to

00:46:37,040 --> 00:46:42,560
do fun things for example

00:46:38,960 --> 00:46:44,960
uh at um eight more or sorry nine more

00:46:42,560 --> 00:46:46,000
gpus in the cloud and pretend they are

00:46:44,960 --> 00:46:49,440
local

00:46:46,000 --> 00:46:51,119
so gpu-2 to gpu-10 are now shown to you

00:46:49,440 --> 00:46:53,040
as if they were local in your computer

00:46:51,119 --> 00:46:55,440
but they actually sit somewhere else

00:46:53,040 --> 00:46:57,359
and with ella sorry with fellow vm12 you

00:46:55,440 --> 00:46:57,680
actually have the ability to do exactly

00:46:57,359 --> 00:47:01,040
this

00:46:57,680 --> 00:47:04,079
so you you can make remote cpus

00:47:01,040 --> 00:47:06,480
look as if they were local

00:47:04,079 --> 00:47:08,319
and then there is um the other use case

00:47:06,480 --> 00:47:09,520
you have you have your own multi-core

00:47:08,319 --> 00:47:12,560
and you can offload to

00:47:09,520 --> 00:47:14,960
to the cpu device but what you can do in

00:47:12,560 --> 00:47:17,119
llvm13 is you're going to be able to

00:47:14,960 --> 00:47:17,760
offload to a virtual gpu that runs on

00:47:17,119 --> 00:47:19,760
your host

00:47:17,760 --> 00:47:22,079
but for all intents and purposes kind of

00:47:19,760 --> 00:47:24,720
looks and feels like a gpu

00:47:22,079 --> 00:47:26,000
the programming model the device runtime

00:47:24,720 --> 00:47:29,359
the compilation path

00:47:26,000 --> 00:47:31,920
everything kind of um is the same as for

00:47:29,359 --> 00:47:33,599
a gpu except that it runs on the host

00:47:31,920 --> 00:47:37,280
which allows you to do

00:47:33,599 --> 00:47:40,160
um debugging much easier

00:47:37,280 --> 00:47:42,079
okay why is this all possible it is

00:47:40,160 --> 00:47:43,119
possible because there is the openmp and

00:47:42,079 --> 00:47:45,280
application world

00:47:43,119 --> 00:47:46,880
and then at some point is the device

00:47:45,280 --> 00:47:49,200
abstraction world

00:47:46,880 --> 00:47:49,920
and there is a fairly nice clean cut in

00:47:49,200 --> 00:47:52,079
between

00:47:49,920 --> 00:47:52,960
that allows us to just add different

00:47:52,079 --> 00:47:56,079
devices

00:47:52,960 --> 00:47:59,200
and expose them to the openmp

00:47:56,079 --> 00:48:00,240
world even if like they have the same

00:47:59,200 --> 00:48:02,640
hardware backing

00:48:00,240 --> 00:48:04,319
or are not hardware at all or our

00:48:02,640 --> 00:48:07,920
hardware somewhere else

00:48:04,319 --> 00:48:09,920
so so this kind of abstraction layer

00:48:07,920 --> 00:48:12,960
um it's really nice and really helpful

00:48:09,920 --> 00:48:15,359
to develop a lot of cool tools

00:48:12,960 --> 00:48:16,800
okay i'm going to show the what's next

00:48:15,359 --> 00:48:20,079
slide in the end because we're running

00:48:16,800 --> 00:48:22,720
out of time and i was hoping to

00:48:20,079 --> 00:48:24,480
see if there are any questions so my

00:48:22,720 --> 00:48:26,559
final thoughts

00:48:24,480 --> 00:48:28,720
one thing that i see a lot in in

00:48:26,559 --> 00:48:30,640
academia especially is the idea that a

00:48:28,720 --> 00:48:32,079
work sharing loop is a parallel loop

00:48:30,640 --> 00:48:34,559
so on the left you have a work sharing

00:48:32,079 --> 00:48:35,359
loop on parallel four on the right you

00:48:34,559 --> 00:48:38,000
have an

00:48:35,359 --> 00:48:40,079
uh what openmp now actually offers you

00:48:38,000 --> 00:48:40,960
is a real parallel loop that is a loop

00:48:40,079 --> 00:48:42,720
that is

00:48:40,960 --> 00:48:44,000
um that doesn't have loop carrick

00:48:42,720 --> 00:48:47,200
dependencies

00:48:44,000 --> 00:48:49,280
and you use the order concur and they

00:48:47,200 --> 00:48:52,960
are not the same

00:48:49,280 --> 00:48:56,319
for one if the schedule that is chosen

00:48:52,960 --> 00:48:58,319
for the parallel work sharing loop is

00:48:56,319 --> 00:48:59,680
has a chunk size that is bigger than the

00:48:58,319 --> 00:49:02,160
loop iteration count

00:48:59,680 --> 00:49:03,680
that loop is executed sequentially so

00:49:02,160 --> 00:49:05,920
there is no parallelism at all

00:49:03,680 --> 00:49:07,680
similarly if somebody sets the number of

00:49:05,920 --> 00:49:09,280
threads before the loop before that loop

00:49:07,680 --> 00:49:12,640
is executed to one

00:49:09,280 --> 00:49:15,920
again no parallelism at all so

00:49:12,640 --> 00:49:17,680
making the pretending an unparalleled

00:49:15,920 --> 00:49:20,160
follow-up is a parallel loop

00:49:17,680 --> 00:49:21,119
can be dangerous if you if you use the

00:49:20,160 --> 00:49:24,319
parallelism

00:49:21,119 --> 00:49:27,599
to do optimizations so without

00:49:24,319 --> 00:49:29,359
openmp assumptions that is not really

00:49:27,599 --> 00:49:31,359
doable but that is what we have

00:49:29,359 --> 00:49:33,839
introduced the openmp assumptions to

00:49:31,359 --> 00:49:37,119
to allow the user to guarantee all of

00:49:33,839 --> 00:49:42,000
these nasty corner cases won't happen

00:49:37,119 --> 00:49:45,040
okay now uh 10 minutes to go and i'm

00:49:42,000 --> 00:49:46,000
happy to to answer any questions in case

00:49:45,040 --> 00:49:48,800
there are some

00:49:46,000 --> 00:49:48,800
let me take a look

00:49:49,200 --> 00:49:52,400
i think there are a few

00:49:56,839 --> 00:50:03,119
um

00:49:59,280 --> 00:50:05,440
okay um one question is

00:50:03,119 --> 00:50:06,480
i use standard libraries of c plus plus

00:50:05,440 --> 00:50:08,720
14 to the

00:50:06,480 --> 00:50:10,720
17 and to some extent c versus 20 to

00:50:08,720 --> 00:50:11,920
write parallel vacations does simple

00:50:10,720 --> 00:50:13,440
standard libraries have inherent

00:50:11,920 --> 00:50:16,880
limitations when it

00:50:13,440 --> 00:50:18,720
comes to parallelism um i mean

00:50:16,880 --> 00:50:20,880
the limitations with regards to the

00:50:18,720 --> 00:50:24,079
outlining and

00:50:20,880 --> 00:50:25,200
preventing optimizations are not openmp

00:50:24,079 --> 00:50:27,760
specific

00:50:25,200 --> 00:50:30,480
pretty much every parallel programming

00:50:27,760 --> 00:50:33,280
language is implemented in the same way

00:50:30,480 --> 00:50:34,000
if you look at the bottom most layer so

00:50:33,280 --> 00:50:37,280
all of them

00:50:34,000 --> 00:50:40,079
are suffer the same fate

00:50:37,280 --> 00:50:40,720
the abstractions that we use to bridge

00:50:40,079 --> 00:50:42,640
that gap

00:50:40,720 --> 00:50:44,160
this this transitive call types for

00:50:42,640 --> 00:50:46,160
example

00:50:44,160 --> 00:50:48,800
apply to all of them though until it

00:50:46,160 --> 00:50:51,440
applies to pthread create which is the

00:50:48,800 --> 00:50:51,920
underlying concepts on the all of them

00:50:51,440 --> 00:50:54,079
and

00:50:51,920 --> 00:50:55,920
with this transitive call site and then

00:50:54,079 --> 00:50:57,680
inter procedural optimization you get a

00:50:55,920 --> 00:51:01,119
lot of the missed opportunity

00:50:57,680 --> 00:51:04,800
missed parallelization uh back um

00:51:01,119 --> 00:51:06,720
so c plus plus parallelism

00:51:04,800 --> 00:51:08,400
is not inherently different from any

00:51:06,720 --> 00:51:10,800
other one so openmp is also not

00:51:08,400 --> 00:51:11,680
inherently different from open acc or

00:51:10,800 --> 00:51:15,280
tba

00:51:11,680 --> 00:51:17,920
and all of the tba it's all conceptually

00:51:15,280 --> 00:51:22,640
very similar

00:51:17,920 --> 00:51:22,640
um okay another question we have here

00:51:25,119 --> 00:51:28,960
the use of first private as a default

00:51:27,119 --> 00:51:30,559
data sharing method well it reduces

00:51:28,960 --> 00:51:32,720
indirect memory accesses

00:51:30,559 --> 00:51:34,800
but wouldn't it also add memory storage

00:51:32,720 --> 00:51:36,720
overhead due to storing multiple copies

00:51:34,800 --> 00:51:38,880
of the data item and also might affect

00:51:36,720 --> 00:51:41,599
consistency among threads if the

00:51:38,880 --> 00:51:41,920
variable is modified by other threads i

00:51:41,599 --> 00:51:44,640
mean

00:51:41,920 --> 00:51:45,440
absolutely if you have especially the

00:51:44,640 --> 00:51:47,599
later part

00:51:45,440 --> 00:51:49,200
if you actually modify a variable by

00:51:47,599 --> 00:51:52,400
different threads and you

00:51:49,200 --> 00:51:55,920
and that modification have to be visible

00:51:52,400 --> 00:51:56,480
then you need shared however if you do

00:51:55,920 --> 00:51:59,599
that

00:51:56,480 --> 00:52:02,400
that only works if you do

00:51:59,599 --> 00:52:02,960
explicit synchronization that means that

00:52:02,400 --> 00:52:05,920
there are

00:52:02,960 --> 00:52:06,559
some like critical some lock some some

00:52:05,920 --> 00:52:10,960
form of

00:52:06,559 --> 00:52:12,640
of uh synchronization has to be present

00:52:10,960 --> 00:52:14,000
and if you look at your program and

00:52:12,640 --> 00:52:16,559
there is none such

00:52:14,000 --> 00:52:18,000
synchronization there you actually don't

00:52:16,559 --> 00:52:20,880
modify stuff that has

00:52:18,000 --> 00:52:22,559
to be shared you might modify memory

00:52:20,880 --> 00:52:24,800
that is kind of

00:52:22,559 --> 00:52:27,280
accessible by all of them but not the

00:52:24,800 --> 00:52:30,480
variable itself for example the pointer

00:52:27,280 --> 00:52:31,520
that says where the memory is and with

00:52:30,480 --> 00:52:35,040
regards to the

00:52:31,520 --> 00:52:37,520
overhead in in storage most

00:52:35,040 --> 00:52:38,720
like with the exception of stack arrays

00:52:37,520 --> 00:52:41,280
everything else is

00:52:38,720 --> 00:52:43,200
is small like we're talking like all of

00:52:41,280 --> 00:52:43,920
the values usually fit in registers

00:52:43,200 --> 00:52:45,440
right

00:52:43,920 --> 00:52:47,200
like there might be exceptions if you

00:52:45,440 --> 00:52:48,800
have huge tracts that you don't want to

00:52:47,200 --> 00:52:51,839
pass by value

00:52:48,800 --> 00:52:55,040
but other than that um i would recommend

00:52:51,839 --> 00:52:55,040
first private all the time

00:52:55,440 --> 00:52:59,040
um talking about first private if the

00:52:57,200 --> 00:53:00,640
user first private also recommended for

00:52:59,040 --> 00:53:04,000
openmp offloading

00:53:00,640 --> 00:53:07,200
um after they like

00:53:04,000 --> 00:53:10,240
once task got introduced we actually

00:53:07,200 --> 00:53:12,960
uh like the openmp standard got it right

00:53:10,240 --> 00:53:14,800
and started to use first private for

00:53:12,960 --> 00:53:18,960
tasks as a default

00:53:14,800 --> 00:53:22,000
so openmp target it creates a task

00:53:18,960 --> 00:53:23,839
and that task will by default take all

00:53:22,000 --> 00:53:25,920
the variables as first private

00:53:23,839 --> 00:53:27,839
so the entire discussion about you

00:53:25,920 --> 00:53:29,200
should use default first private really

00:53:27,839 --> 00:53:33,920
only applies to

00:53:29,200 --> 00:53:36,640
a parallel because that is

00:53:33,920 --> 00:53:38,079
historically take like sharing variables

00:53:36,640 --> 00:53:40,960
by default is shared

00:53:38,079 --> 00:53:42,400
and that is the the problem but the task

00:53:40,960 --> 00:53:43,839
construct and it will be target

00:53:42,400 --> 00:53:45,680
construct and so on

00:53:43,839 --> 00:53:46,880
they actually share variables at first

00:53:45,680 --> 00:53:50,000
private by default

00:53:46,880 --> 00:53:50,000
so you're good on that end

00:53:50,559 --> 00:53:55,119
um very nice you know i wonder if

00:53:53,200 --> 00:53:56,960
multiple back-to-back barriers

00:53:55,119 --> 00:53:58,559
can be detected and optimized into a

00:53:56,960 --> 00:54:02,240
single barrier

00:53:58,559 --> 00:54:06,079
um yes and no so

00:54:02,240 --> 00:54:08,160
this is fairly i mean it it little bit

00:54:06,079 --> 00:54:08,880
depends so if you can prove that all of

00:54:08,160 --> 00:54:11,920
your threads

00:54:08,880 --> 00:54:15,040
actually make those barriers yes

00:54:11,920 --> 00:54:15,760
on the gpu for example that is in the in

00:54:15,040 --> 00:54:17,280
the

00:54:15,760 --> 00:54:18,800
scheme that i showed earlier with the

00:54:17,280 --> 00:54:22,000
state machine

00:54:18,800 --> 00:54:22,720
there are literally two consecutive

00:54:22,000 --> 00:54:24,960
barriers

00:54:22,720 --> 00:54:25,920
that are executed by the main threat in

00:54:24,960 --> 00:54:29,040
reality

00:54:25,920 --> 00:54:29,440
but you cannot merge them because they

00:54:29,040 --> 00:54:31,359
are

00:54:29,440 --> 00:54:32,880
like barriers that are only executed by

00:54:31,359 --> 00:54:35,440
a subset of threats

00:54:32,880 --> 00:54:36,319
on our consecutive cannot be merged if

00:54:35,440 --> 00:54:38,400
all the threats

00:54:36,319 --> 00:54:40,640
execute those barriers then you can kind

00:54:38,400 --> 00:54:42,960
of replace one of them

00:54:40,640 --> 00:54:44,400
we have a prototype of a barrier

00:54:42,960 --> 00:54:47,920
elimination

00:54:44,400 --> 00:54:50,000
and that is like given our initial

00:54:47,920 --> 00:54:53,280
experiments two years ago

00:54:50,000 --> 00:54:56,319
that becomes useful if you actually make

00:54:53,280 --> 00:54:57,599
implicit barriers explicit and that

00:54:56,319 --> 00:55:00,480
happens if you merge

00:54:57,599 --> 00:55:02,160
parallel regions as soon as you merge a

00:55:00,480 --> 00:55:05,839
parallel region with another one

00:55:02,160 --> 00:55:07,520
or then the in between at the end of the

00:55:05,839 --> 00:55:08,240
first parallel region has an implicit

00:55:07,520 --> 00:55:10,799
barrier

00:55:08,240 --> 00:55:12,160
that one has to become explicit and once

00:55:10,799 --> 00:55:14,400
you start doing these

00:55:12,160 --> 00:55:15,359
like having the barriers explicitly that

00:55:14,400 --> 00:55:18,079
is where you

00:55:15,359 --> 00:55:20,720
where you might really benefit from

00:55:18,079 --> 00:55:24,240
barrier elimination

00:55:20,720 --> 00:55:25,200
that said it really like i haven't

00:55:24,240 --> 00:55:28,000
really found it

00:55:25,200 --> 00:55:28,720
a case where without parallel region

00:55:28,000 --> 00:55:32,720
merging

00:55:28,720 --> 00:55:35,680
area elimination is is helpful that much

00:55:32,720 --> 00:55:37,119
an implicit barriers removing is is a is

00:55:35,680 --> 00:55:39,920
a tricky business to begin with

00:55:37,119 --> 00:55:41,200
i i might be wrong about that because i

00:55:39,920 --> 00:55:44,240
haven't really looked at implicit

00:55:41,200 --> 00:55:48,640
barrier on both equals

00:55:44,240 --> 00:55:51,680
um okay use cases for the vgpu in lvm13

00:55:48,640 --> 00:55:53,920
um right now if you do offloading

00:55:51,680 --> 00:55:55,599
you offload and you want to debug your

00:55:53,920 --> 00:55:58,000
offloading or you don't have a gpu

00:55:55,599 --> 00:55:59,839
available and you want to test it

00:55:58,000 --> 00:56:02,079
you can offload to the host which is

00:55:59,839 --> 00:56:05,040
your cpu

00:56:02,079 --> 00:56:05,520
however if you do that at least in claim

00:56:05,040 --> 00:56:08,240
the

00:56:05,520 --> 00:56:10,559
code generation part the device run time

00:56:08,240 --> 00:56:13,760
that that is executed

00:56:10,559 --> 00:56:16,720
pretty much everything is different than

00:56:13,760 --> 00:56:17,760
if you offload to a gpu it goes through

00:56:16,720 --> 00:56:20,960
a couple of

00:56:17,760 --> 00:56:23,520
similar um runtime calls

00:56:20,960 --> 00:56:24,880
to do some of the communication but

00:56:23,520 --> 00:56:28,720
almost like

00:56:24,880 --> 00:56:31,599
the majority of things in the entire

00:56:28,720 --> 00:56:32,559
path between compilation and execution

00:56:31,599 --> 00:56:34,880
is different

00:56:32,559 --> 00:56:36,400
if you offload to the host or if you

00:56:34,880 --> 00:56:39,440
offload to a gpu

00:56:36,400 --> 00:56:42,000
with the vgpu we bridge that gap

00:56:39,440 --> 00:56:43,599
so if you offload to a virtual gpu it's

00:56:42,000 --> 00:56:45,359
pretty much the same

00:56:43,599 --> 00:56:47,520
the same code generation it's pretty

00:56:45,359 --> 00:56:48,880
much the same device runtime

00:56:47,520 --> 00:56:50,960
everything as if you would have

00:56:48,880 --> 00:56:53,760
offloaded to a physical gpu

00:56:50,960 --> 00:56:54,640
which allows you to debug your code that

00:56:53,760 --> 00:56:57,680
is

00:56:54,640 --> 00:56:58,480
very close to what would run on the on a

00:56:57,680 --> 00:57:01,119
gpu

00:56:58,480 --> 00:57:02,880
but you can debug it on the host so you

00:57:01,119 --> 00:57:03,520
can attach a debugger you can look at

00:57:02,880 --> 00:57:06,240
the

00:57:03,520 --> 00:57:07,119
you can use gdb directly and so on and

00:57:06,240 --> 00:57:08,880
so forth

00:57:07,119 --> 00:57:10,319
and it is executed or you can use a

00:57:08,880 --> 00:57:13,119
thread sanitizer for all

00:57:10,319 --> 00:57:14,640
for all that matters but it's executed

00:57:13,119 --> 00:57:17,520
with the same

00:57:14,640 --> 00:57:20,319
um model of execution as it would have

00:57:17,520 --> 00:57:23,520
been on the gpu

00:57:20,319 --> 00:57:25,280
um do you think postponing the outline

00:57:23,520 --> 00:57:26,640
transformation will be helpful

00:57:25,280 --> 00:57:30,160
applying traditional loop level

00:57:26,640 --> 00:57:30,160
optimizations in llvm

00:57:30,559 --> 00:57:36,640
the short answer is no because llvm

00:57:32,720 --> 00:57:39,920
doesn't have strong loop optimizations

00:57:36,640 --> 00:57:40,720
the longer answer is yes there is a

00:57:39,920 --> 00:57:43,760
potential

00:57:40,720 --> 00:57:45,440
to apply like there is a potential to

00:57:43,760 --> 00:57:48,960
apply loop level optimizations

00:57:45,440 --> 00:57:52,240
if you do layer outline however

00:57:48,960 --> 00:57:53,440
you have to see that it is much more

00:57:52,240 --> 00:57:55,760
complex to do

00:57:53,440 --> 00:57:58,400
loop optimized loop level optimizations

00:57:55,760 --> 00:57:58,400
across

00:57:59,119 --> 00:58:03,920
a loop of one of which is a work sharing

00:58:01,920 --> 00:58:07,200
or parallel loop

00:58:03,920 --> 00:58:09,119
i mean that requires the

00:58:07,200 --> 00:58:10,720
loop transformation to know about this

00:58:09,119 --> 00:58:12,640
because you want to be aware what it

00:58:10,720 --> 00:58:14,240
means you have to you want to preserve

00:58:12,640 --> 00:58:15,040
the parallelism and so on and so forth

00:58:14,240 --> 00:58:17,119
so just

00:58:15,040 --> 00:58:18,400
thinking oh i could just use interchange

00:58:17,119 --> 00:58:20,160
across a parallel loop

00:58:18,400 --> 00:58:21,680
it's not going to work the way you want

00:58:20,160 --> 00:58:24,880
it

00:58:21,680 --> 00:58:28,319
and and that means that

00:58:24,880 --> 00:58:30,480
it might not be the only way to just

00:58:28,319 --> 00:58:32,240
do late outlining and therefore kind of

00:58:30,480 --> 00:58:33,520
be able to reuse because you can't

00:58:32,240 --> 00:58:36,799
actually reuse

00:58:33,520 --> 00:58:38,640
maybe there is another way that said

00:58:36,799 --> 00:58:41,200
the the abstraction that we use right

00:58:38,640 --> 00:58:44,400
now with the transitive calls

00:58:41,200 --> 00:58:46,640
is not necessarily the

00:58:44,400 --> 00:58:47,680
uh the solution either because that

00:58:46,640 --> 00:58:50,480
works well for

00:58:47,680 --> 00:58:52,480
for classic scalar optimizations but not

00:58:50,480 --> 00:58:53,760
for loop level transformations

00:58:52,480 --> 00:58:55,760
what i think for loop level

00:58:53,760 --> 00:58:57,680
transformations people should do

00:58:55,760 --> 00:59:00,400
is should use the loop level pragmas

00:58:57,680 --> 00:59:02,319
that openmp started to introduce in 5.1

00:59:00,400 --> 00:59:03,680
because they are applied early and they

00:59:02,319 --> 00:59:06,160
are aware of the

00:59:03,680 --> 00:59:07,280
openmp other fragments like work sharing

00:59:06,160 --> 00:59:10,880
and parallel

00:59:07,280 --> 00:59:14,160
and they kind of do the same thing

00:59:10,880 --> 00:59:16,240
um it's not automatic with regards to

00:59:14,160 --> 00:59:17,440
determining what to do but then you can

00:59:16,240 --> 00:59:19,440
use a

00:59:17,440 --> 00:59:22,160
an outer tuner if you want and people

00:59:19,440 --> 00:59:22,160
are working on that

00:59:22,799 --> 00:59:26,960
okay when you mention radioactive syntax

00:59:25,200 --> 00:59:28,640
it reminds me of pragma on thread

00:59:26,960 --> 00:59:30,319
private list

00:59:28,640 --> 00:59:32,319
it's the open respect changing that one

00:59:30,319 --> 00:59:33,760
as well i don't know if the opening

00:59:32,319 --> 00:59:36,559
respect is going to change

00:59:33,760 --> 00:59:37,440
the things that i think are broken um i

00:59:36,559 --> 00:59:40,640
hope so

00:59:37,440 --> 00:59:42,640
but i um i don't know i can't really

00:59:40,640 --> 00:59:46,160
tell you

00:59:42,640 --> 00:59:47,920
um other conventions to avoid

00:59:46,160 --> 00:59:49,599
using a target parallel region like

00:59:47,920 --> 00:59:51,839
overloaded operators or weird

00:59:49,599 --> 00:59:53,599
collections

00:59:51,839 --> 00:59:56,000
need the application developers to be

00:59:53,599 --> 01:00:00,720
gpu aware when writing

00:59:56,000 --> 01:00:04,319
code one thing that will just not work

01:00:00,720 --> 01:00:06,880
is if you have if you use um

01:00:04,319 --> 01:00:07,680
containers or something that are

01:00:06,880 --> 01:00:11,680
partially

01:00:07,680 --> 01:00:13,440
defined inside of the standard library

01:00:11,680 --> 01:00:15,119
so if you don't have the source code in

01:00:13,440 --> 01:00:17,760
the header you would

01:00:15,119 --> 01:00:19,200
need the standard library for the gpu

01:00:17,760 --> 01:00:21,359
which we right now don't have

01:00:19,200 --> 01:00:23,920
at least not on all devices and we

01:00:21,359 --> 01:00:25,599
really like in llvm we don't really link

01:00:23,920 --> 01:00:28,000
anything in there

01:00:25,599 --> 01:00:30,960
so that is that is a problem if you use

01:00:28,000 --> 01:00:34,559
the header only parts of these things

01:00:30,960 --> 01:00:35,680
they should work um how well they work

01:00:34,559 --> 01:00:39,280
is a little bit of a

01:00:35,680 --> 01:00:39,280
is a little bit of a question

01:00:39,359 --> 01:00:42,960
um schedule concurrent last flight what

01:00:42,079 --> 01:00:45,280
is it good for

01:00:42,960 --> 01:00:46,480
oh yeah it's order concurrent and that

01:00:45,280 --> 01:00:49,760
basically

01:00:46,480 --> 01:00:52,960
is uh saying that is the

01:00:49,760 --> 01:00:54,960
real parallel loop where you say this

01:00:52,960 --> 01:00:57,119
loop doesn't have

01:00:54,960 --> 01:00:58,799
loop carry dependencies and it actually

01:00:57,119 --> 01:01:00,720
provides has a couple of other

01:00:58,799 --> 01:01:02,559
restrictions that come with it

01:01:00,720 --> 01:01:04,319
not all of which i think are even

01:01:02,559 --> 01:01:07,280
necessary but

01:01:04,319 --> 01:01:08,880
um it really says this is a parallel

01:01:07,280 --> 01:01:11,920
loop that doesn't have loot carry

01:01:08,880 --> 01:01:15,040
dependencies you can optimize it

01:01:11,920 --> 01:01:17,440
under the assumption that for example

01:01:15,040 --> 01:01:18,480
array like accesses in different

01:01:17,440 --> 01:01:22,480
iterations

01:01:18,480 --> 01:01:24,880
do not map to the same memory

01:01:22,480 --> 01:01:25,760
they cannot so that that for example

01:01:24,880 --> 01:01:27,839
diff like

01:01:25,760 --> 01:01:29,760
based on that you can potentially prove

01:01:27,839 --> 01:01:30,319
that two arrays are not aliasing and you

01:01:29,760 --> 01:01:33,520
can

01:01:30,319 --> 01:01:35,359
you can use that um for traditional work

01:01:33,520 --> 01:01:36,319
sharing loops that is not true because

01:01:35,359 --> 01:01:39,119
with

01:01:36,319 --> 01:01:41,599
uh environment variables and these

01:01:39,119 --> 01:01:43,359
runtime calls like the setters and so on

01:01:41,599 --> 01:01:44,960
you might actually make the work sharing

01:01:43,359 --> 01:01:46,799
look sequential

01:01:44,960 --> 01:01:48,400
and then these arrays could overlap all

01:01:46,799 --> 01:01:50,000
they want and it would be legal and if

01:01:48,400 --> 01:01:52,640
you assumed they are not

01:01:50,000 --> 01:01:55,599
you might miscompile the code so i'm

01:01:52,640 --> 01:01:55,599
trying to avoid that

01:01:55,760 --> 01:01:59,200
what kind of issues could be addressed

01:01:57,280 --> 01:02:01,119
by making reasoning more semantic aware

01:01:59,200 --> 01:02:03,280
over syntax dependent

01:02:01,119 --> 01:02:04,319
um the issue is like the one i showed

01:02:03,280 --> 01:02:06,640
where you have to

01:02:04,319 --> 01:02:08,000
manually outline a little bit of code

01:02:06,640 --> 01:02:10,319
just to make it work

01:02:08,000 --> 01:02:12,559
and it wouldn't work as expected and it

01:02:10,319 --> 01:02:12,960
works it has perfectly fine semantics

01:02:12,559 --> 01:02:14,880
it's

01:02:12,960 --> 01:02:16,400
the quest it is just that the compiler

01:02:14,880 --> 01:02:18,160
says this is not allowed but if you

01:02:16,400 --> 01:02:19,760
outline it into a function everything is

01:02:18,160 --> 01:02:22,000
fine

01:02:19,760 --> 01:02:23,119
so that that kind of hoops that you have

01:02:22,000 --> 01:02:26,400
to go through

01:02:23,119 --> 01:02:28,400
uh seem seem utterly um unhelpful

01:02:26,400 --> 01:02:30,319
and there's more than just that one and

01:02:28,400 --> 01:02:31,680
a couple of those suggest kind of

01:02:30,319 --> 01:02:33,680
the standard forgot to remove

01:02:31,680 --> 01:02:34,960
restrictions over time or

01:02:33,680 --> 01:02:37,280
and i'm i'm part of the standard

01:02:34,960 --> 01:02:38,880
committees i'm i'm too plain as well

01:02:37,280 --> 01:02:40,720
uh don't get me wrong i'm not trying to

01:02:38,880 --> 01:02:43,680
like shift the burden like

01:02:40,720 --> 01:02:44,559
i screwed up defining stuff my fair

01:02:43,680 --> 01:02:47,039
share

01:02:44,559 --> 01:02:49,280
um but that is kind of what i see is

01:02:47,039 --> 01:02:51,119
problematic and if you define stuff more

01:02:49,280 --> 01:02:53,920
semantically

01:02:51,119 --> 01:02:56,960
you're able to also compose

01:02:53,920 --> 01:02:56,960
transformation in

01:02:57,200 --> 01:03:01,760
is better which openmp feature do you

01:03:00,400 --> 01:03:03,520
think will should be deprecated in the

01:03:01,760 --> 01:03:05,839
next standards

01:03:03,520 --> 01:03:06,880
that's a good question um sections for

01:03:05,839 --> 01:03:11,119
one and

01:03:06,880 --> 01:03:12,960
um this this do a cross seems to be

01:03:11,119 --> 01:03:15,359
something that i'm not sure if anyone

01:03:12,960 --> 01:03:17,839
uses that i haven't seen it yet

01:03:15,359 --> 01:03:17,839
um

01:03:21,039 --> 01:03:24,880
i want to be more careful what i see

01:03:22,559 --> 01:03:25,520
next so i'm i'm i'm going to stop at

01:03:24,880 --> 01:03:27,920
that

01:03:25,520 --> 01:03:31,200
with those two uh there's there's also a

01:03:27,920 --> 01:03:33,520
couple of

01:03:31,200 --> 01:03:36,160
clauses or options that i would that i

01:03:33,520 --> 01:03:38,160
would deprecate and remove

01:03:36,160 --> 01:03:39,920
what is the current status of utilizing

01:03:38,160 --> 01:03:41,359
user provided assumes another vm what

01:03:39,920 --> 01:03:43,039
kind of assumptions can a user provide

01:03:41,359 --> 01:03:46,720
and how are they utilized

01:03:43,039 --> 01:03:48,880
um really using we really use only one

01:03:46,720 --> 01:03:52,559
assumption and that is the assumption

01:03:48,880 --> 01:03:56,880
that no openmp or no openmp api

01:03:52,559 --> 01:03:59,920
so as part of this gpu

01:03:56,880 --> 01:04:00,240
state machine optimization we try to get

01:03:59,920 --> 01:04:03,520
rid

01:04:00,240 --> 01:04:06,240
of this fallback call

01:04:03,520 --> 01:04:07,920
that does that calls the function

01:04:06,240 --> 01:04:11,359
pointer that was passed

01:04:07,920 --> 01:04:13,440
so if if we have this id transformations

01:04:11,359 --> 01:04:15,920
hey try not to use function pointers

01:04:13,440 --> 01:04:16,799
but there's always this call that that

01:04:15,920 --> 01:04:20,240
says

01:04:16,799 --> 01:04:22,000
if i encounter a parallel region

01:04:20,240 --> 01:04:23,680
that i didn't see before that for

01:04:22,000 --> 01:04:24,880
example in a different translation unit

01:04:23,680 --> 01:04:27,440
or so

01:04:24,880 --> 01:04:28,640
in that in a different translation unit

01:04:27,440 --> 01:04:30,559
there is a parallel region that i

01:04:28,640 --> 01:04:32,079
encounter i have to kind of pass that

01:04:30,559 --> 01:04:34,160
function pointer of that parallel region

01:04:32,079 --> 01:04:37,760
to my worker so that they can actually

01:04:34,160 --> 01:04:37,760
execute that function in parallel

01:04:37,839 --> 01:04:41,839
now this is a problem because you have

01:04:39,520 --> 01:04:44,160
this indirect function called leftover

01:04:41,839 --> 01:04:45,920
but if you actually never use this

01:04:44,160 --> 01:04:46,640
feature such that you have a parallel

01:04:45,920 --> 01:04:49,359
region in

01:04:46,640 --> 01:04:50,400
a separate translation unit but you have

01:04:49,359 --> 01:04:52,799
calls that

01:04:50,400 --> 01:04:53,599
we cannot analyze you can annotate them

01:04:52,799 --> 01:04:56,640
with

01:04:53,599 --> 01:04:58,000
assumes no openmp or assumes no openmp

01:04:56,640 --> 01:05:00,960
api calls

01:04:58,000 --> 01:05:03,200
and then we will know that these calls

01:05:00,960 --> 01:05:05,119
call into a separate translation unit

01:05:03,200 --> 01:05:07,280
that we cannot analyze but you promised

01:05:05,119 --> 01:05:08,720
us that they will not have any openmp

01:05:07,280 --> 01:05:11,440
parallel regions in them

01:05:08,720 --> 01:05:13,599
so we don't need to keep this fallback

01:05:11,440 --> 01:05:17,119
function pointer call around

01:05:13,599 --> 01:05:20,960
in just removing the indirect

01:05:17,119 --> 01:05:23,039
call is potentially a big benefit

01:05:20,960 --> 01:05:24,559
so that is the only one we use right now

01:05:23,039 --> 01:05:25,200
there are others that we are going to

01:05:24,559 --> 01:05:27,520
use

01:05:25,200 --> 01:05:30,000
but they only make sense if you also

01:05:27,520 --> 01:05:31,839
have remarks that tell people what is

01:05:30,000 --> 01:05:32,880
happening why did we miss something and

01:05:31,839 --> 01:05:34,960
tell people to

01:05:32,880 --> 01:05:36,559
use them in a proper way because

01:05:34,960 --> 01:05:40,240
guessing what assumptions to use

01:05:36,559 --> 01:05:41,680
is not helpful okay that was all the

01:05:40,240 --> 01:05:45,680
questions

01:05:41,680 --> 01:05:48,799
and we're reasonably over time so

01:05:45,680 --> 01:05:51,599
i guess we should we should end this

01:05:48,799 --> 01:05:53,039
hey thank you so much johannes and thank

01:05:51,599 --> 01:05:55,280
you all for your time today

01:05:53,039 --> 01:05:57,119
we hope you enjoyed the webinar uh

01:05:55,280 --> 01:05:58,720
recording of this presentation along

01:05:57,119 --> 01:06:00,400
with the slides will be available on the

01:05:58,720 --> 01:06:02,079
openmp website

01:06:00,400 --> 01:06:03,839
we will send you a direct link in a

01:06:02,079 --> 01:06:05,440
follow-up email

01:06:03,839 --> 01:06:06,960
also as you leave the webinar please

01:06:05,440 --> 01:06:08,319
take a moment to fill out the survey

01:06:06,960 --> 01:06:10,079
that pops up

01:06:08,319 --> 01:06:11,920
your feedback is important to us to help

01:06:10,079 --> 01:06:13,280
us improve these presentations

01:06:11,920 --> 01:06:15,760
and we'd also love to hear if there's

01:06:13,280 --> 01:06:16,960
any other openmp related topics you may

01:06:15,760 --> 01:06:27,119
be interested in

01:06:16,960 --> 01:06:27,119

YouTube URL: https://www.youtube.com/watch?v=eIMpgez61r4


