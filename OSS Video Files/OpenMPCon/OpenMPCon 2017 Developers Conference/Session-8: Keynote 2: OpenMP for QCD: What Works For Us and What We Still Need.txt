Title: Session-8: Keynote 2: OpenMP for QCD: What Works For Us and What We Still Need
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Dr. Meifeng Lin, Associate Computational Scientist, Computational Science Initiative of Brookhaven National Laboratory (BNL)
Dr. Lin’s research interests include computational physics, high performance computing and numerical algorithms, with a focus on lattice quantum chromodynamics – a numerical approach to understand the fundamental interactions of particles and their effects on our visible world. In this keynote Dr Lin will will survey the common OpenMP directives used in lattice QCD software, and provide a personal wish list for OpenMP features that we may need towards the exascale computing era.
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day2-Keynote-Lin.pdf
Captions: 
	00:00:00,030 --> 00:00:10,290
all right come on you're right they do a

00:00:07,309 --> 00:00:14,330
communist reference and yeah well

00:00:10,290 --> 00:00:18,810
they're here we have a lender laughter

00:00:14,330 --> 00:00:21,689
finally you can say well later the

00:00:18,810 --> 00:00:25,170
trouble um all right so if we're going

00:00:21,689 --> 00:00:28,380
to change it is my sibling looking at

00:00:25,170 --> 00:00:31,019
potential scientists advocating okay the

00:00:28,380 --> 00:00:34,820
National Laboratory ha she's into the

00:00:31,019 --> 00:00:38,070
denied for assuming our example fine

00:00:34,820 --> 00:00:40,140
quantum physics from the merits and

00:00:38,070 --> 00:00:45,120
she's the material indication about how

00:00:40,140 --> 00:00:48,210
to apply ultimately to marry one missing

00:00:45,120 --> 00:00:48,800
and what is working today so take your

00:00:48,210 --> 00:00:50,760
wait

00:00:48,800 --> 00:00:56,730
thank you Michael for your introduction

00:00:50,760 --> 00:00:58,469
yeah so as our Chairman mentioned from

00:00:56,730 --> 00:01:02,190
Brookhaven National Lab and my

00:00:58,469 --> 00:01:05,159
background is in a lavish quantum

00:01:02,190 --> 00:01:08,549
chromodynamics in high-energy physics

00:01:05,159 --> 00:01:11,010
and but in the last few years that have

00:01:08,549 --> 00:01:13,260
been focusing on high performance

00:01:11,010 --> 00:01:15,659
computing in general outside of lattice

00:01:13,260 --> 00:01:18,450
QCD itself and first I'd like to thank

00:01:15,659 --> 00:01:20,820
the organizers for giving me the

00:01:18,450 --> 00:01:23,130
opportunity to speak here and I also

00:01:20,820 --> 00:01:26,759
want to thank you in advance for coming

00:01:23,130 --> 00:01:30,869
here so early in the morning so here's

00:01:26,759 --> 00:01:33,500
the outline of my talk for is there a

00:01:30,869 --> 00:01:33,500
pointer

00:01:35,729 --> 00:01:43,360
okay so therefore I know the audience

00:01:40,090 --> 00:01:44,770
have very diverse backgrounds so I just

00:01:43,360 --> 00:01:46,899
want to give a very quick introduction

00:01:44,770 --> 00:01:50,860
very high-level introduction to what

00:01:46,899 --> 00:01:54,420
that is QCD is and what it does and in

00:01:50,860 --> 00:01:56,649
specially in terms of the software and

00:01:54,420 --> 00:02:00,759
programming model requirement thank you

00:01:56,649 --> 00:02:03,759
very much what we are trying to achieve

00:02:00,759 --> 00:02:07,170
and just to provide some historical

00:02:03,759 --> 00:02:10,569
perspective I'll actually go over some

00:02:07,170 --> 00:02:13,420
that is the 2cd specific hardware and

00:02:10,569 --> 00:02:15,790
the evolution of the probic programming

00:02:13,420 --> 00:02:19,450
models associated with the hardware and

00:02:15,790 --> 00:02:21,849
so in particular lattice QCD community

00:02:19,450 --> 00:02:24,310
have been engaged not only in

00:02:21,849 --> 00:02:26,260
high-performance computing on in the

00:02:24,310 --> 00:02:29,319
simulation and software side but also in

00:02:26,260 --> 00:02:31,540
the hardware design so I'll mention a

00:02:29,319 --> 00:02:35,590
few of the special purple supercomputers

00:02:31,540 --> 00:02:38,980
for lattice QCD and of course lately the

00:02:35,590 --> 00:02:42,790
commodity Hardware based on CPUs and

00:02:38,980 --> 00:02:45,280
GPUs is are commonplace and that we have

00:02:42,790 --> 00:02:48,700
we are also a switching to using these

00:02:45,280 --> 00:02:52,030
are more common hardware as well and of

00:02:48,700 --> 00:02:55,959
course the main focus of this workshop

00:02:52,030 --> 00:02:59,200
is openmp and for the openmp kong the

00:02:55,959 --> 00:03:00,700
development of software using openmp so

00:02:59,200 --> 00:03:04,540
i'll also talk about a lattice QCD

00:03:00,700 --> 00:03:07,420
software and the third part of my talk

00:03:04,540 --> 00:03:11,319
will be focused on some projects that I

00:03:07,420 --> 00:03:14,590
have been personally involved in first

00:03:11,319 --> 00:03:17,100
is some optimizations that we do to a

00:03:14,590 --> 00:03:20,350
specific computer nodes for lattice QCD

00:03:17,100 --> 00:03:22,550
and I'll focus on the openmp related

00:03:20,350 --> 00:03:27,090
part and then the

00:03:22,550 --> 00:03:28,830
second project is a part of the two CDSs

00:03:27,090 --> 00:03:32,610
scale application development project

00:03:28,830 --> 00:03:35,670
which is to investigate different

00:03:32,610 --> 00:03:41,880
performance portability programming

00:03:35,670 --> 00:03:46,140
strategies for lattice QCD so that is QC

00:03:41,880 --> 00:03:48,390
the stage one quantum chromodynamics and

00:03:46,140 --> 00:03:51,450
it is a numerical framework to simulate

00:03:48,390 --> 00:03:53,880
all the support subatomic particles

00:03:51,450 --> 00:03:56,280
called quarks and gluons all the

00:03:53,880 --> 00:03:57,600
supercomputers starting with the

00:03:56,280 --> 00:04:03,770
fundamental theory of strong

00:03:57,600 --> 00:04:03,770
interactions and the basic of numerical

00:04:03,830 --> 00:04:09,060
techniques are very simple you basically

00:04:06,180 --> 00:04:12,690
discretize the theory and put them into

00:04:09,060 --> 00:04:16,950
so you can visualize now the space-time

00:04:12,690 --> 00:04:20,370
of discrete lattice and all the physics

00:04:16,950 --> 00:04:25,130
happen happening on either the lattice

00:04:20,370 --> 00:04:25,130
sites or the links connecting them and

00:04:26,210 --> 00:04:33,420
we do multicolor simulations from the

00:04:30,170 --> 00:04:35,670
theory and to generate the quantum

00:04:33,420 --> 00:04:37,380
fields of interest and then we can do

00:04:35,670 --> 00:04:41,640
some physics measurements on top of them

00:04:37,380 --> 00:04:44,070
and yeah the physics results we obtain

00:04:41,640 --> 00:04:46,050
can be compared to experiments or can

00:04:44,070 --> 00:04:49,320
help us make our theoretical predictions

00:04:46,050 --> 00:04:53,070
and the pictures on the right are just

00:04:49,320 --> 00:04:54,960
some visualization of what you may get

00:04:53,070 --> 00:04:56,850
from the multicolored simulations or

00:04:54,960 --> 00:05:01,530
what the space-time will look like the

00:04:56,850 --> 00:05:03,560
different densities of the topological

00:05:01,530 --> 00:05:03,560
charge

00:05:04,620 --> 00:05:10,259
so I didn't change this right to remove

00:05:08,310 --> 00:05:12,569
the equations but this is what the

00:05:10,259 --> 00:05:15,659
computer know of lattice QCD is like

00:05:12,569 --> 00:05:17,849
basically a matrix vector multiplication

00:05:15,659 --> 00:05:20,520
this is what we call the P slash

00:05:17,849 --> 00:05:25,050
operator so this is a very high

00:05:20,520 --> 00:05:29,940
dimension matrix and for other vectors

00:05:25,050 --> 00:05:31,620
and if you this is basically just a you

00:05:29,940 --> 00:05:34,050
know stencil operator but in four

00:05:31,620 --> 00:05:37,280
dimensions so it's a nine-point special

00:05:34,050 --> 00:05:40,110
operator and this operation makes up

00:05:37,280 --> 00:05:43,050
depending on the type of calculations it

00:05:40,110 --> 00:05:45,210
is the most computationally intensive

00:05:43,050 --> 00:05:47,789
part of lattice QCD some simulations

00:05:45,210 --> 00:05:56,940
sometimes you can take up up to like 90

00:05:47,789 --> 00:05:59,580
or 95% of the computation time and up

00:05:56,940 --> 00:06:01,259
with the motives behind lattice QCD as I

00:05:59,580 --> 00:06:04,830
mentioned we do our Monte Carlo

00:06:01,259 --> 00:06:07,259
simulations and then we also need to use

00:06:04,830 --> 00:06:09,509
molecular dynamics to evolve the gauge

00:06:07,259 --> 00:06:12,090
field complications to move through the

00:06:09,509 --> 00:06:14,880
multi color chain and we also need to

00:06:12,090 --> 00:06:18,419
solve these linear systems of equations

00:06:14,880 --> 00:06:21,779
and sometimes also need to use I can

00:06:18,419 --> 00:06:24,229
value solvers and this is just some kind

00:06:21,779 --> 00:06:28,490
of the graph that we want to evaluate

00:06:24,229 --> 00:06:28,490
from this simulation

00:06:30,229 --> 00:06:35,270
and the goal of lattice QCD simulation

00:06:33,680 --> 00:06:37,879
since the very beginning is to

00:06:35,270 --> 00:06:40,909
understand the fundamental symmetries in

00:06:37,879 --> 00:06:42,800
the universe and to help us understand

00:06:40,909 --> 00:06:47,060
the properties of the fundamental

00:06:42,800 --> 00:06:50,060
fundamental particles and for with the

00:06:47,060 --> 00:06:53,870
increasing computing power especially

00:06:50,060 --> 00:06:55,999
toward excess scale error our goal is to

00:06:53,870 --> 00:06:59,449
improve the precision by an order of

00:06:55,999 --> 00:07:01,819
magnitude and also extend the type of

00:06:59,449 --> 00:07:05,990
calculations that we can do on the

00:07:01,819 --> 00:07:09,650
supercomputers to resemble more the

00:07:05,990 --> 00:07:12,409
reality and from a software point of

00:07:09,650 --> 00:07:15,499
view we are designing a new software

00:07:12,409 --> 00:07:18,529
framework for the excess scale computing

00:07:15,499 --> 00:07:21,710
and some of the requirements of this new

00:07:18,529 --> 00:07:24,710
software include efficiency so we want

00:07:21,710 --> 00:07:28,580
to be able to make the best use of the

00:07:24,710 --> 00:07:31,159
new xsq architectures and especially for

00:07:28,580 --> 00:07:34,969
us communication has always been the

00:07:31,159 --> 00:07:38,990
bottleneck to be able to store a strong

00:07:34,969 --> 00:07:41,300
scale to many many compute nodes and we

00:07:38,990 --> 00:07:44,839
will also like to have a flexibility in

00:07:41,300 --> 00:07:47,659
the software that we have so that the

00:07:44,839 --> 00:07:50,479
not-so programming savvy users can

00:07:47,659 --> 00:07:52,719
implement different algorithms and to

00:07:50,479 --> 00:07:55,969
form different physical calculations

00:07:52,719 --> 00:07:58,659
without having to go through the

00:07:55,969 --> 00:08:04,129
low-level Hardware specific

00:07:58,659 --> 00:08:06,229
implementations and for the both for the

00:08:04,129 --> 00:08:11,029
maintenance and for the development of

00:08:06,229 --> 00:08:13,099
the software we also want to try to

00:08:11,029 --> 00:08:16,610
achieve performance portability in the

00:08:13,099 --> 00:08:19,610
design of the software so I will talk

00:08:16,610 --> 00:08:21,860
about what the current software we have

00:08:19,610 --> 00:08:26,270
right now and you'll see why this is not

00:08:21,860 --> 00:08:28,009
very important for us but I think this

00:08:26,270 --> 00:08:32,289
is a common challenge for all the

00:08:28,009 --> 00:08:32,289
applications development right now

00:08:32,520 --> 00:08:38,830
so lattice QCD requires a lot of

00:08:36,370 --> 00:08:42,190
computing power just to give you an idea

00:08:38,830 --> 00:08:44,770
a lot of the calculations we are doing

00:08:42,190 --> 00:08:49,660
right now we have done so far

00:08:44,770 --> 00:08:52,240
required computing all xml racks on the

00:08:49,660 --> 00:08:55,660
blue-jean queue for computers for months

00:08:52,240 --> 00:08:59,380
or even years to get the to get certain

00:08:55,660 --> 00:09:01,360
precision and needless to say the

00:08:59,380 --> 00:09:03,459
procedure we can achieve so far is not

00:09:01,360 --> 00:09:06,300
high enough otherwise we will stop to

00:09:03,459 --> 00:09:09,690
stop working so it is a very very

00:09:06,300 --> 00:09:13,750
computation intensive calculation and

00:09:09,690 --> 00:09:15,670
luckily for us the uniform space-time

00:09:13,750 --> 00:09:20,110
structure of the calculation makes it

00:09:15,670 --> 00:09:23,200
very good for parallel computing so here

00:09:20,110 --> 00:09:26,080
is a sketch of what we how we paralyzed

00:09:23,200 --> 00:09:28,020
our calculations so this is in 2

00:09:26,080 --> 00:09:31,570
dimensional space-time of course and

00:09:28,020 --> 00:09:34,480
what the dots are okay there are the

00:09:31,570 --> 00:09:38,170
lattice sites in our simulations and the

00:09:34,480 --> 00:09:40,540
squares are the compute nodes and we

00:09:38,170 --> 00:09:44,080
basically just you know divide evenly

00:09:40,540 --> 00:09:48,310
the ladders to map onto the different

00:09:44,080 --> 00:09:51,790
computers and because the data we need a

00:09:48,310 --> 00:09:53,740
basically nearest neighbor data so what

00:09:51,790 --> 00:09:57,480
you need to do is you need to send the

00:09:53,740 --> 00:10:01,810
data and the boundary to the nearest

00:09:57,480 --> 00:10:04,330
neighbor neighboring nodes so that when

00:10:01,810 --> 00:10:10,750
you do the simulations you can

00:10:04,330 --> 00:10:12,850
communicate with the whole system so

00:10:10,750 --> 00:10:14,200
this is the traditional way of mapping

00:10:12,850 --> 00:10:18,550
the lattice

00:10:14,200 --> 00:10:22,830
on to the compute node and later in the

00:10:18,550 --> 00:10:30,790
new software that we are developing this

00:10:22,830 --> 00:10:35,760
mapping may change a little bit so back

00:10:30,790 --> 00:10:40,150
in the 90s of because the PC

00:10:35,760 --> 00:10:42,550
philosophers were not so common then not

00:10:40,150 --> 00:10:45,610
a security series actually designed

00:10:42,550 --> 00:10:49,360
their own supercomputers and this is the

00:10:45,610 --> 00:10:51,340
one that i actually used when I started

00:10:49,360 --> 00:10:54,580
as a graduate student at Columbia which

00:10:51,340 --> 00:10:56,500
is the QCD XP machines stands for

00:10:54,580 --> 00:11:00,120
quantum chromodynamics own digital

00:10:56,500 --> 00:11:04,060
signal processors and it is based on the

00:11:00,120 --> 00:11:08,200
signal processors by I think text Texas

00:11:04,060 --> 00:11:10,570
Instruments and it's very nice because

00:11:08,200 --> 00:11:13,780
it has a 40 match for communications

00:11:10,570 --> 00:11:16,060
instability and happy where for the type

00:11:13,780 --> 00:11:21,330
of communications that that is kills

00:11:16,060 --> 00:11:25,600
these simulations require and it was so

00:11:21,330 --> 00:11:28,030
it had very good provide price

00:11:25,600 --> 00:11:29,860
performance and so we want the special

00:11:28,030 --> 00:11:34,000
apprised performance

00:11:29,860 --> 00:11:37,150
applied Golden Bell Prize in 1998 the

00:11:34,000 --> 00:11:39,100
machine was capable of running multiple

00:11:37,150 --> 00:11:41,980
instructions or different instructions

00:11:39,100 --> 00:11:44,590
on different nodes but typically it was

00:11:41,980 --> 00:11:47,830
programmed in the single program

00:11:44,590 --> 00:11:51,070
multiple data pair time and because

00:11:47,830 --> 00:11:54,220
there are no shared memory and so

00:11:51,070 --> 00:11:57,310
there's no opening piece reading used on

00:11:54,220 --> 00:11:59,290
this machine and also it had some

00:11:57,310 --> 00:12:02,590
hardware specific it's all message

00:11:59,290 --> 00:12:05,830
passing protocols so we also didn't you

00:12:02,590 --> 00:12:08,320
need to use MTI so pretty quite

00:12:05,830 --> 00:12:11,590
different from what we are used to

00:12:08,320 --> 00:12:15,280
nowadays and the next generation of the

00:12:11,590 --> 00:12:18,970
lattice QCD machine is cautious VLC CCD

00:12:15,280 --> 00:12:20,860
on a chip this machine was based on the

00:12:18,970 --> 00:12:23,680
PowerPC

00:12:20,860 --> 00:12:27,520
and it was also designed by the lattice

00:12:23,680 --> 00:12:30,910
and series and it was a collaboration

00:12:27,520 --> 00:12:34,300
between Columbia we can behave and

00:12:30,910 --> 00:12:40,690
Research Center BNL I think oh and also

00:12:34,300 --> 00:12:44,350
IPM and similar to QC GSP also has this

00:12:40,690 --> 00:12:49,930
built in high dimensional mesh for

00:12:44,350 --> 00:12:52,690
communications and is true for all

00:12:49,930 --> 00:12:55,600
hardware architecture evolution now the

00:12:52,690 --> 00:12:57,250
price performance is different it's like

00:12:55,600 --> 00:13:00,550
an order of magnitude better than choose

00:12:57,250 --> 00:13:03,970
DSP here still no shared memory so no

00:13:00,550 --> 00:13:06,610
threading the internal communication now

00:13:03,970 --> 00:13:07,600
can be done through either MPI or just

00:13:06,610 --> 00:13:10,810
our own

00:13:07,600 --> 00:13:14,800
QCD message passing GMP so human B is a

00:13:10,810 --> 00:13:17,310
high level there that was written on top

00:13:14,800 --> 00:13:22,720
of either MPI or the how we're specific

00:13:17,310 --> 00:13:26,170
communication protocols and this is a

00:13:22,720 --> 00:13:29,530
picture of TLC at the PNL and some of

00:13:26,170 --> 00:13:32,590
the people who were involved in this in

00:13:29,530 --> 00:13:39,660
the design and installation of this

00:13:32,590 --> 00:13:43,690
supercomputer and so lately in the last

00:13:39,660 --> 00:13:47,440
five to seven years we have been using

00:13:43,690 --> 00:13:51,160
the QC v CQ q CT with Cairo quad and

00:13:47,440 --> 00:13:54,460
this was a prototype machine for IBM who

00:13:51,160 --> 00:13:56,890
Gene supercomputer and now you can see

00:13:54,460 --> 00:14:02,470
here's a comparison between QC t OC and

00:13:56,890 --> 00:14:04,720
QC b CQ so on the note level GCD LC only

00:14:02,470 --> 00:14:08,050
has one compute port that's why we

00:14:04,720 --> 00:14:12,850
didn't need to use sweating and but QC d

00:14:08,050 --> 00:14:16,990
CQ each note has a 16 cores and they

00:14:12,850 --> 00:14:20,580
share memory so it became important to

00:14:16,990 --> 00:14:26,530
use threading either OpenMP or other

00:14:20,580 --> 00:14:30,250
threading programming models and this

00:14:26,530 --> 00:14:32,830
machine was has been in use since five

00:14:30,250 --> 00:14:36,360
or six years ago and we are currently

00:14:32,830 --> 00:14:41,670
still using the Blue Gene supercomputer

00:14:36,360 --> 00:14:45,160
either at DNA or the one at Argonne so

00:14:41,670 --> 00:14:48,880
the reason I wanted to show you this

00:14:45,160 --> 00:14:51,310
hardware evolution was to show this is

00:14:48,880 --> 00:14:54,940
tightly related to the software we're

00:14:51,310 --> 00:14:58,150
using because lattice QCD software has

00:14:54,940 --> 00:15:02,770
been evolving with the hardware

00:14:58,150 --> 00:15:04,600
so as we try to make best use of the

00:15:02,770 --> 00:15:06,910
available hardware we also need to make

00:15:04,600 --> 00:15:16,330
changes to the software set to be able

00:15:06,910 --> 00:15:19,560
to support the new architectures so now

00:15:16,330 --> 00:15:23,230
it is it's very common to run

00:15:19,560 --> 00:15:25,650
simulations on PC clusters

00:15:23,230 --> 00:15:32,200
interconnected with high bandwidth

00:15:25,650 --> 00:15:34,090
fabrics however I just noted that 15

00:15:32,200 --> 00:15:37,710
years ago this paper about lattice QCD

00:15:34,090 --> 00:15:40,870
on pcs has a title with a question mark

00:15:37,710 --> 00:15:42,580
and because it wasn't very well known

00:15:40,870 --> 00:15:45,610
that you could actually perform

00:15:42,580 --> 00:15:51,780
knowledge acuity simulations on just

00:15:45,610 --> 00:15:54,190
regular personal computer processors and

00:15:51,780 --> 00:15:59,470
it was very different

00:15:54,190 --> 00:16:02,740
15 years ago but now a lot of the okay

00:15:59,470 --> 00:16:07,150
I've said most of the PC clusters have

00:16:02,740 --> 00:16:12,010
processors that had contain many

00:16:07,150 --> 00:16:17,280
multiple cores and they share big cpu

00:16:12,010 --> 00:16:20,260
memories so it became important to use a

00:16:17,280 --> 00:16:23,850
shared memory programming model such as

00:16:20,260 --> 00:16:27,480
openmp to get a good performance on

00:16:23,850 --> 00:16:32,610
is type of hardware but I wanted to

00:16:27,480 --> 00:16:35,370
mention that back in map back maybe 10

00:16:32,610 --> 00:16:38,009
years ago openmp wasn't very commonly

00:16:35,370 --> 00:16:41,279
used for lattice QCD simulations what we

00:16:38,009 --> 00:16:45,899
often did was to ruin multiple NPI

00:16:41,279 --> 00:16:48,839
processes per node when you have two or

00:16:45,899 --> 00:16:51,750
four cords it it wasn't so bad if you

00:16:48,839 --> 00:16:53,759
had to run multiple MPI processes per

00:16:51,750 --> 00:16:56,069
node and also because it was easy

00:16:53,759 --> 00:17:01,670
because we already have the code that

00:16:56,069 --> 00:17:04,679
could run that had MPI implemented in it

00:17:01,670 --> 00:17:07,620
nowadays the landscape has changed a lot

00:17:04,679 --> 00:17:09,839
so especially with the new many

00:17:07,620 --> 00:17:14,059
integrated core architecture when you

00:17:09,839 --> 00:17:17,730
have 60 or 70 quarts per node it's

00:17:14,059 --> 00:17:21,329
running many MPI processes Pano's it

00:17:17,730 --> 00:17:23,370
becomes a very inefficient it's also not

00:17:21,329 --> 00:17:23,850
very it's not a very sensible thing to

00:17:23,370 --> 00:17:28,890
do

00:17:23,850 --> 00:17:31,080
so nowadays if it becomes very important

00:17:28,890 --> 00:17:34,289
for us to use the opening piece writing

00:17:31,080 --> 00:17:38,490
in the code and here's some pictures of

00:17:34,289 --> 00:17:43,049
the different dedicated not exclusively

00:17:38,490 --> 00:17:46,110
clusters in the u.s. this was 10 - thank

00:17:43,049 --> 00:17:48,510
you cluster GLS and so this this is the

00:17:46,110 --> 00:17:51,900
chase a cluster formula and the Pankiw

00:17:48,510 --> 00:17:55,110
cluster hll has been replaced by a nice

00:17:51,900 --> 00:17:57,770
vending cluster also for not exclusively

00:17:55,110 --> 00:17:57,770
simulations

00:18:01,210 --> 00:18:12,040
and now GPU computing also becomes a

00:18:08,390 --> 00:18:15,170
very very well-known and uncommon and

00:18:12,040 --> 00:18:18,020
lattice QCD series started to explore

00:18:15,170 --> 00:18:21,590
the possibility of using lists GP GPUs

00:18:18,020 --> 00:18:24,920
to do tricity simulations again back to

00:18:21,590 --> 00:18:28,720
about 10 years ago and in this paper the

00:18:24,920 --> 00:18:34,540
author's they actually use OpenCL to

00:18:28,720 --> 00:18:37,640
program on the GPUs and then all these

00:18:34,540 --> 00:18:40,280
GPU programming models have been exposed

00:18:37,640 --> 00:18:46,180
by the lattice QCD community open still

00:18:40,280 --> 00:18:49,090
open ACC and CUDA and the most popular

00:18:46,180 --> 00:18:52,970
program model used right now is

00:18:49,090 --> 00:18:57,050
concluded thanks to the CUDA library

00:18:52,970 --> 00:18:59,990
that was developed first by the group at

00:18:57,050 --> 00:19:00,650
Boston University I'll talk a little bit

00:18:59,990 --> 00:19:03,980
about it

00:19:00,650 --> 00:19:08,260
when I talk about a software for lattice

00:19:03,980 --> 00:19:12,800
QCD so as you can see with the different

00:19:08,260 --> 00:19:15,740
different hardware architectures evolved

00:19:12,800 --> 00:19:20,270
evolving over the years lattice QCD

00:19:15,740 --> 00:19:23,330
software has to be adapted to use these

00:19:20,270 --> 00:19:28,520
new hardware architectures in the last

00:19:23,330 --> 00:19:31,100
20 years and as a result there are a lot

00:19:28,520 --> 00:19:34,340
of different modules that are that were

00:19:31,100 --> 00:19:36,590
written to maximize the performance on

00:19:34,340 --> 00:19:41,420
these different hardware architectures

00:19:36,590 --> 00:19:46,460
and this is a type where I'm showing you

00:19:41,420 --> 00:19:49,960
the software landscape in the US this

00:19:46,460 --> 00:19:53,720
was the software developed under the

00:19:49,960 --> 00:19:56,120
theory side I program and there are

00:19:53,720 --> 00:20:00,620
basically four levels of

00:19:56,120 --> 00:20:03,230
different code bases the level ones it

00:20:00,620 --> 00:20:04,520
supports of just basic linear algebra a

00:20:03,230 --> 00:20:07,160
multi threading system or the

00:20:04,520 --> 00:20:10,090
message-passing support and the level

00:20:07,160 --> 00:20:14,840
two is more lattice QCD specific

00:20:10,090 --> 00:20:17,990
consider two TP plus plus 2d piece of 2d

00:20:14,840 --> 00:20:21,230
p and q PP plus plus they both support

00:20:17,990 --> 00:20:24,170
lattice data parallel layer and Qi all

00:20:21,230 --> 00:20:28,340
supports the IO observe that exclusive

00:20:24,170 --> 00:20:31,160
simulations and the level three involves

00:20:28,340 --> 00:20:33,800
codes that are related to the algorithms

00:20:31,160 --> 00:20:36,910
that we use such as the linear solvers

00:20:33,800 --> 00:20:40,670
and totally inverters and different

00:20:36,910 --> 00:20:47,120
discretization different matrix vector

00:20:40,670 --> 00:20:50,870
multiplication routines and some other C

00:20:47,120 --> 00:20:53,150
based optimizations and CUDA here is

00:20:50,870 --> 00:20:54,430
what I just mentioned you guys accused

00:20:53,150 --> 00:20:57,980
e-library

00:20:54,430 --> 00:21:00,920
written specifically for GPUs and it was

00:20:57,980 --> 00:21:04,790
it's written in pewter and it has been

00:21:00,920 --> 00:21:09,560
under active development even now so it

00:21:04,790 --> 00:21:11,210
has also been evolving to to catch up

00:21:09,560 --> 00:21:14,120
with the evolution of the GPU

00:21:11,210 --> 00:21:15,800
architectures and themselves and then

00:21:14,120 --> 00:21:19,610
the top there is the application layer

00:21:15,800 --> 00:21:23,390
of these these codes are the physicists

00:21:19,610 --> 00:21:30,280
what physicists used to do the physics

00:21:23,390 --> 00:21:33,830
calculations and coma was developed in

00:21:30,280 --> 00:21:36,530
was developed and maintained at the

00:21:33,830 --> 00:21:38,840
Jefferson Lab and CPS stands for

00:21:36,530 --> 00:21:41,960
Columbia physics system so it was

00:21:38,840 --> 00:21:43,670
developed many by the physicists at

00:21:41,960 --> 00:21:46,100
Columbia University and Brookhaven

00:21:43,670 --> 00:21:50,170
National Lab this is the one that I have

00:21:46,100 --> 00:21:55,160
been using myself and zero is a new

00:21:50,170 --> 00:21:58,400
application suite based on Lua and it

00:21:55,160 --> 00:22:01,570
calls the underlying lower low level of

00:21:58,400 --> 00:22:02,810
routines for lattice QCD milk is another

00:22:01,570 --> 00:22:06,260
us-based

00:22:02,810 --> 00:22:09,050
not a security software and to rule is

00:22:06,260 --> 00:22:12,730
also based on Lua

00:22:09,050 --> 00:22:19,520
but it has different personalities from

00:22:12,730 --> 00:22:26,270
fuel so this is what the current u.s. QC

00:22:19,520 --> 00:22:28,910
software is like and the hardware

00:22:26,270 --> 00:22:32,330
specific implementations are usually

00:22:28,910 --> 00:22:37,160
buried in this label like inverters or

00:22:32,330 --> 00:22:39,410
that D slash operators or cutis Okuda

00:22:37,160 --> 00:22:41,270
has the inverters and all these are

00:22:39,410 --> 00:22:47,810
performance critical parts of the

00:22:41,270 --> 00:22:49,910
lattice QCD simulations and as I

00:22:47,810 --> 00:22:52,550
mentioned many times because the lattice

00:22:49,910 --> 00:22:55,640
2d software has evolved over time with

00:22:52,550 --> 00:22:59,270
the changes of the hardware on the

00:22:55,640 --> 00:23:04,280
program models use a mixture of many

00:22:59,270 --> 00:23:08,570
things and over the years MPI plus X has

00:23:04,280 --> 00:23:11,900
become the most popular choice so that's

00:23:08,570 --> 00:23:17,800
why we we are planning to develop new

00:23:11,900 --> 00:23:20,210
software stack that can basically

00:23:17,800 --> 00:23:21,920
achieve performance possibilities so we

00:23:20,210 --> 00:23:24,250
don't have to maintain so many different

00:23:21,920 --> 00:23:31,090
code bases for different types of

00:23:24,250 --> 00:23:33,710
architectures and for the absolute

00:23:31,090 --> 00:23:35,660
performance critical part we may still

00:23:33,710 --> 00:23:36,620
think about calling some of the

00:23:35,660 --> 00:23:38,680
low-level

00:23:36,620 --> 00:23:42,320
optimized libraries domain-specific

00:23:38,680 --> 00:23:44,240
libraries like CUDA or other all the

00:23:42,320 --> 00:23:46,790
libraries of optimized for other

00:23:44,240 --> 00:23:48,760
architectures but we are hoping that we

00:23:46,790 --> 00:23:50,650
could have an unified

00:23:48,760 --> 00:23:57,309
therefore the other part of the

00:23:50,650 --> 00:24:01,390
artistically software okay so I haven't

00:23:57,309 --> 00:24:04,090
talked much about OpenMP yet but I hope

00:24:01,390 --> 00:24:06,760
that gives you an idea also what not as

00:24:04,090 --> 00:24:09,730
QCD community we're not excuses is

00:24:06,760 --> 00:24:13,059
community stands in terms of the usage

00:24:09,730 --> 00:24:15,100
of OpenMP what I'm going to talk about

00:24:13,059 --> 00:24:18,549
next we'll have some more technical

00:24:15,100 --> 00:24:22,960
details related to open MP the first one

00:24:18,549 --> 00:24:28,480
is a project that I did in the last two

00:24:22,960 --> 00:24:31,419
years with some with a small company and

00:24:28,480 --> 00:24:33,700
in New York Reservoir labs to optimize

00:24:31,419 --> 00:24:37,540
that these large operator in Columbia

00:24:33,700 --> 00:24:40,120
physics system so you don't have to read

00:24:37,540 --> 00:24:42,160
this out of the equations but what I

00:24:40,120 --> 00:24:44,380
want to point out is that this is the

00:24:42,160 --> 00:24:47,530
operator that we try to update optimized

00:24:44,380 --> 00:24:49,840
and it is the generalization of the D

00:24:47,530 --> 00:24:52,750
slash operations that I showed at the

00:24:49,840 --> 00:24:56,220
very beginning into five dimensions and

00:24:52,750 --> 00:25:00,510
what's nice about this operator is that

00:24:56,220 --> 00:25:06,790
one of the pieces this piece is

00:25:00,510 --> 00:25:09,549
completely independent of the one of the

00:25:06,790 --> 00:25:13,270
dimensions so so which means it's very

00:25:09,549 --> 00:25:17,730
good for vectorization so the

00:25:13,270 --> 00:25:21,130
optimization we did was first on the

00:25:17,730 --> 00:25:25,030
single node and then we try to optimize

00:25:21,130 --> 00:25:26,650
the MPI scaling as well so this is just

00:25:25,030 --> 00:25:29,140
a slide showing you the vectorization

00:25:26,650 --> 00:25:32,160
that we have to do you're all familiar

00:25:29,140 --> 00:25:36,070
with the sims ii optimization I'm sure

00:25:32,160 --> 00:25:37,960
so in this if you want to program cindy

00:25:36,070 --> 00:25:40,840
machines data layout is the key you need

00:25:37,960 --> 00:25:44,679
to make sure that the data layout for

00:25:40,840 --> 00:25:49,090
your simulations can allow you to the to

00:25:44,679 --> 00:25:51,120
vectorization to your citations for the

00:25:49,090 --> 00:25:54,330
type of operations

00:25:51,120 --> 00:25:59,389
and as I mentioned in this operator

00:25:54,330 --> 00:26:02,789
there's a one dimension or one index

00:25:59,389 --> 00:26:07,950
that is completely independent for

00:26:02,789 --> 00:26:12,090
different sites and this is just the

00:26:07,950 --> 00:26:15,269
layout that we use so this is this we

00:26:12,090 --> 00:26:18,480
use a great black checker boarding so it

00:26:15,269 --> 00:26:21,690
only has half of the length of the fifth

00:26:18,480 --> 00:26:25,590
dimension and so this is what the

00:26:21,690 --> 00:26:30,919
daytime they are looks like on the

00:26:25,590 --> 00:26:37,259
optimization we did was for password

00:26:30,919 --> 00:26:39,809
architecture so any act and we had to

00:26:37,259 --> 00:26:43,919
use simply intrinsic to implement the

00:26:39,809 --> 00:26:47,340
vectorization for this operator we toys

00:26:43,919 --> 00:26:49,919
just putting in Padma o MP CMD but the

00:26:47,340 --> 00:26:56,820
performance was very poor so this is one

00:26:49,919 --> 00:27:00,299
of the areas that I'm hoping openmp and

00:26:56,820 --> 00:27:02,490
the compiler community could help

00:27:00,299 --> 00:27:06,980
improve so that we don't have to go down

00:27:02,490 --> 00:27:06,980
to the intrinsic to get good performance

00:27:10,520 --> 00:27:13,360
and sorry

00:27:13,850 --> 00:27:23,060
yeah oh and as so NS is usually on the

00:27:19,280 --> 00:27:24,890
order of ten so the yeah vectorizing

00:27:23,060 --> 00:27:29,030
over the s direction has the

00:27:24,890 --> 00:27:32,210
disadvantage that NS has to be multiples

00:27:29,030 --> 00:27:34,100
of 16 or eight or he has to be multiples

00:27:32,210 --> 00:27:39,020
of the same d length of the machine that

00:27:34,100 --> 00:27:43,370
you are targeting yes so this particular

00:27:39,020 --> 00:27:47,690
machine the victimization in the

00:27:43,370 --> 00:27:50,690
x-direction allows us to use NS to be

00:27:47,690 --> 00:27:52,040
eight or sixteen depending on whether

00:27:50,690 --> 00:27:59,360
you want to do double or single

00:27:52,040 --> 00:28:01,880
precision calculations so this is just

00:27:59,360 --> 00:28:04,880
some other technical details not related

00:28:01,880 --> 00:28:09,020
to open impede progress or as far as I

00:28:04,880 --> 00:28:12,050
know but maybe OpenMP can also help we

00:28:09,020 --> 00:28:16,100
twice different things to improve the

00:28:12,050 --> 00:28:18,380
data locality for example tiling but it

00:28:16,100 --> 00:28:22,190
didn't help much and we also try to use

00:28:18,380 --> 00:28:25,700
space-filling curve didn't help much

00:28:22,190 --> 00:28:27,920
either but prestretching did help so we

00:28:25,700 --> 00:28:31,550
this with this is basically a stencil

00:28:27,920 --> 00:28:34,610
operator right so we if we prefetch data

00:28:31,550 --> 00:28:38,860
are needed for the next stencil we could

00:28:34,610 --> 00:28:38,860
get about 10% performance improvement

00:28:40,810 --> 00:28:50,540
that opens East Indian Wars booth

00:28:44,440 --> 00:28:52,680
Saturday that this term on GTD auction

00:28:50,540 --> 00:28:58,070
with no insurance

00:28:52,680 --> 00:29:04,430
oh this is just a fuse multi multiply

00:28:58,070 --> 00:29:05,670
ex-partner yes no the just the other

00:29:04,430 --> 00:29:12,960
arithmatic

00:29:05,670 --> 00:29:16,830
operations no no not for the FMA

00:29:12,960 --> 00:29:19,290
intrinsic we didn't use f MA intrinsics

00:29:16,830 --> 00:29:22,910
but we use the intrinsic to you know

00:29:19,290 --> 00:29:22,910
load and store data

00:29:32,549 --> 00:29:40,169
yeah so for the computation part not

00:29:36,629 --> 00:29:43,169
excuses e is very simple in the sense

00:29:40,169 --> 00:29:46,679
that we basically just have nested loops

00:29:43,169 --> 00:29:51,029
so the most commonly used OpenMP Park

00:29:46,679 --> 00:29:55,109
math is parallel hole and depending on

00:29:51,029 --> 00:29:58,469
what the operations here is you may use

00:29:55,109 --> 00:30:00,299
collapse but for this particular case we

00:29:58,469 --> 00:30:04,440
couldn't use the collapsed cause because

00:30:00,299 --> 00:30:06,749
of some dependences so we try to send

00:30:04,440 --> 00:30:10,169
three different strategies to use the

00:30:06,749 --> 00:30:13,049
OpenMP pragma versus just to paralyze

00:30:10,169 --> 00:30:20,459
the outermost loop in this case is

00:30:13,049 --> 00:30:24,989
Citigroup and we also manually expand

00:30:20,459 --> 00:30:29,190
the loops into one single giant single

00:30:24,989 --> 00:30:32,329
loop and use OpenMP pillow for our for

00:30:29,190 --> 00:30:38,249
the loop and the other approach is to

00:30:32,329 --> 00:30:42,899
insert in linearized loops but we use

00:30:38,249 --> 00:30:45,989
explicit work distribution to basically

00:30:42,899 --> 00:30:48,389
tell the threads which part of the

00:30:45,989 --> 00:30:54,239
arrange matrixes that you are supposed

00:30:48,389 --> 00:30:57,059
to work on so look if the compiler does

00:30:54,239 --> 00:31:00,169
a decent job then these two should

00:30:57,059 --> 00:31:00,169
basically be equivalent

00:31:02,980 --> 00:31:10,480
so this is just some benchmarks for

00:31:06,070 --> 00:31:14,139
different body sizes or just different

00:31:10,480 --> 00:31:16,630
workload and you can see for one spread

00:31:14,139 --> 00:31:18,039
the performance for these different

00:31:16,630 --> 00:31:21,669
implementations are basically identical

00:31:18,039 --> 00:31:23,740
as you should expect when you increase

00:31:21,669 --> 00:31:27,610
the number of threads for the thing

00:31:23,740 --> 00:31:31,059
simple pragma because we only paralyze

00:31:27,610 --> 00:31:33,820
the one of the dimensions so the length

00:31:31,059 --> 00:31:36,610
is 8 you see that you can really get

00:31:33,820 --> 00:31:40,630
scale passed a strip so the performance

00:31:36,610 --> 00:31:43,179
basically just levels off and for the

00:31:40,630 --> 00:31:45,340
other two approaches you see that the

00:31:43,179 --> 00:31:48,929
performance are basically are similar to

00:31:45,340 --> 00:31:52,210
each other and for larger large sizes

00:31:48,929 --> 00:31:53,620
again you can see the scaling it's

00:31:52,210 --> 00:31:57,100
actually scaled pretty well with the

00:31:53,620 --> 00:32:00,250
number of threads and with the larger T

00:31:57,100 --> 00:32:03,159
dimension here is a study to all these

00:32:00,250 --> 00:32:06,179
implementations basically give you a

00:32:03,159 --> 00:32:06,179
similar performance

00:32:09,590 --> 00:32:15,710
yeah this is some more checks that we

00:32:12,560 --> 00:32:18,830
did for different large sizes and one

00:32:15,710 --> 00:32:21,040
interesting thing to note is that the

00:32:18,830 --> 00:32:23,810
performance that's not really

00:32:21,040 --> 00:32:25,700
deteriorated sites even when the

00:32:23,810 --> 00:32:29,630
lighting systems in the cache anymore

00:32:25,700 --> 00:32:32,360
which is a possible indication of poor

00:32:29,630 --> 00:32:36,440
cache we use so this is a room for for

00:32:32,360 --> 00:32:41,200
the optimization but this I'm not sure

00:32:36,440 --> 00:32:44,180
this is something openmp can help or not

00:32:41,200 --> 00:32:46,490
we also found that the threat affinity

00:32:44,180 --> 00:32:49,760
was very important to get their good

00:32:46,490 --> 00:32:53,660
performance and we use a little GCC

00:32:49,760 --> 00:32:55,580
compiler and when we set this prop

00:32:53,660 --> 00:33:02,740
buying to be true we've got much better

00:32:55,580 --> 00:33:10,510
performance than without it here this is

00:33:02,740 --> 00:33:12,980
and for the strong scaling um we did

00:33:10,510 --> 00:33:17,720
overlap a computation and communication

00:33:12,980 --> 00:33:21,320
to basically overcome the communication

00:33:17,720 --> 00:33:25,900
bottleneck as I said the data we need is

00:33:21,320 --> 00:33:29,840
just that data or at the boundary of the

00:33:25,900 --> 00:33:32,480
local areas so we could do the bulk

00:33:29,840 --> 00:33:35,540
computation the stencil computations in

00:33:32,480 --> 00:33:39,190
the bulk of the lattice while waiting

00:33:35,540 --> 00:33:42,860
for the data the positive data to be

00:33:39,190 --> 00:33:46,640
transferred and this is what we did and

00:33:42,860 --> 00:33:49,010
in terms of the OpenMP specific part of

00:33:46,640 --> 00:33:51,710
this implementation we actually

00:33:49,010 --> 00:33:53,870
dedicated one where the master masters

00:33:51,710 --> 00:33:55,940
were to be in charge of the

00:33:53,870 --> 00:33:59,470
communications and the communications

00:33:55,940 --> 00:34:02,830
only and the rest of the threads are

00:33:59,470 --> 00:34:07,930
devoted to the computation

00:34:02,830 --> 00:34:09,760
and you mean you may notice that you can

00:34:07,930 --> 00:34:12,070
probably improve so do this with the

00:34:09,760 --> 00:34:13,720
openmp tasking this was something that

00:34:12,070 --> 00:34:15,160
this is something that we have been

00:34:13,720 --> 00:34:17,740
thinking about but haven't implemented

00:34:15,160 --> 00:34:21,720
but I noticed that some other large

00:34:17,740 --> 00:34:21,720
groups have tried this already

00:34:23,850 --> 00:34:29,080
yeah so this is some flow showing you

00:34:26,800 --> 00:34:31,620
are the scaling so this is the idea of

00:34:29,080 --> 00:34:34,750
scaling and this is what we achieve

00:34:31,620 --> 00:34:39,370
again when we go to larger number of

00:34:34,750 --> 00:34:42,010
nodes that communication becomes a big

00:34:39,370 --> 00:34:44,200
bottleneck you can see the computation

00:34:42,010 --> 00:34:47,770
time decreases nicely with the number of

00:34:44,200 --> 00:34:50,170
nodes but then the communication time

00:34:47,770 --> 00:34:52,420
just doesn't decrease as fast this is

00:34:50,170 --> 00:34:56,440
why we can't achieve very good strong

00:34:52,420 --> 00:34:58,600
scale for this particular case and for

00:34:56,440 --> 00:35:02,190
this particular type of architecture

00:34:58,600 --> 00:35:02,190
that we have tested on

00:35:06,620 --> 00:35:17,700
that's right but both there are two two

00:35:14,970 --> 00:35:19,440
aspects so first we want to solve a very

00:35:17,700 --> 00:35:21,990
big problem but we also want to solve

00:35:19,440 --> 00:35:24,210
the problem as fast as we can as I said

00:35:21,990 --> 00:35:26,460
earlier in a typical lattice QCD

00:35:24,210 --> 00:35:29,760
calculation takes months or years to

00:35:26,460 --> 00:35:32,610
finish so if we can show if we can have

00:35:29,760 --> 00:35:34,890
very strong scaling then you can just

00:35:32,610 --> 00:35:38,340
reduce the amount of time that it takes

00:35:34,890 --> 00:35:40,770
to do this this particular size but you

00:35:38,340 --> 00:35:43,680
are also right we also wanted to make a

00:35:40,770 --> 00:35:45,570
calculation as possible but it so one

00:35:43,680 --> 00:35:48,000
particular lattice size it already takes

00:35:45,570 --> 00:35:56,030
a long time so it helps us to have very

00:35:48,000 --> 00:35:59,030
good strong scaling so for the remaining

00:35:56,030 --> 00:36:02,250
part of the talk I'll talk about

00:35:59,030 --> 00:36:07,230
something still ongoing for the excess

00:36:02,250 --> 00:36:09,810
scale project you're probably all aware

00:36:07,230 --> 00:36:12,360
of what the importance of performance

00:36:09,810 --> 00:36:14,400
possibility and from the current

00:36:12,360 --> 00:36:17,070
software stack and show you you

00:36:14,400 --> 00:36:19,980
understand why we want to attack this

00:36:17,070 --> 00:36:23,370
problem a single version of the code is

00:36:19,980 --> 00:36:26,210
certainly easier to maintain and we are

00:36:23,370 --> 00:36:28,910
hoping that we would spend less time on

00:36:26,210 --> 00:36:34,380
integrating these different levels of

00:36:28,910 --> 00:36:36,390
api's with the application there and the

00:36:34,380 --> 00:36:38,180
question is how much performance are we

00:36:36,390 --> 00:36:41,930
willing to lose in exchange for

00:36:38,180 --> 00:36:45,090
portability because that is qct a series

00:36:41,930 --> 00:36:47,970
developers have we have traditionally

00:36:45,090 --> 00:36:51,760
been very good at squeezing the last bit

00:36:47,970 --> 00:36:56,920
of the hardware performance and

00:36:51,760 --> 00:36:59,609
if the answer may be no but with

00:36:56,920 --> 00:37:02,200
potentially more diverse architectures

00:36:59,609 --> 00:37:06,240
we able to continue our current approach

00:37:02,200 --> 00:37:08,859
with just different Hardware specific

00:37:06,240 --> 00:37:13,030
optimizations for different

00:37:08,859 --> 00:37:16,000
architectures it certainly looks subway

00:37:13,030 --> 00:37:21,240
we have so far as the result of 20 years

00:37:16,000 --> 00:37:21,240
of development so it's certainly very

00:37:22,950 --> 00:37:31,150
it's very expensive to continue this

00:37:25,900 --> 00:37:33,130
approach and the various tools that we

00:37:31,150 --> 00:37:36,790
have considered for performance

00:37:33,130 --> 00:37:39,730
portability are listed here first of all

00:37:36,790 --> 00:37:41,770
you're familiar with the compiler

00:37:39,730 --> 00:37:45,340
directives such as cell phone MP and

00:37:41,770 --> 00:37:48,490
open ACC were we were considering open

00:37:45,340 --> 00:37:51,220
ACC because well we wanted to be able to

00:37:48,490 --> 00:37:52,810
run on the GPS as well and then the

00:37:51,220 --> 00:37:56,200
other high level programming

00:37:52,810 --> 00:37:59,350
abstractions such as Roger Coco's post

00:37:56,200 --> 00:38:04,810
political sequence press amp these are

00:37:59,350 --> 00:38:06,550
more platform dependent and we have also

00:38:04,810 --> 00:38:09,280
looked at code generators or

00:38:06,550 --> 00:38:13,150
source-to-source compiler such as the

00:38:09,280 --> 00:38:16,869
just-in-time compilation and recently my

00:38:13,150 --> 00:38:19,630
colleagues at Argonne also looking at

00:38:16,869 --> 00:38:24,130
using this new language called name as a

00:38:19,630 --> 00:38:27,520
basically a code generator it generates

00:38:24,130 --> 00:38:29,800
code key code to see if it can be used

00:38:27,520 --> 00:38:34,810
to achieve performance or ability for

00:38:29,800 --> 00:38:37,300
latitudes lattice QCD yes like a

00:38:34,810 --> 00:38:39,550
conventional is but you know should we

00:38:37,300 --> 00:38:41,440
design only software with portability in

00:38:39,550 --> 00:38:43,150
my first and then optimize for

00:38:41,440 --> 00:38:45,150
performance later or the other way

00:38:43,150 --> 00:38:47,819
around

00:38:45,150 --> 00:38:50,099
I don't I don't really know can we also

00:38:47,819 --> 00:38:51,869
design our somewhere with performance

00:38:50,099 --> 00:38:54,900
possibility maps from the beginning but

00:38:51,869 --> 00:38:57,509
you've got to start somewhere so do you

00:38:54,900 --> 00:39:00,269
want to have portability first and then

00:38:57,509 --> 00:39:03,569
try to optimize for performance later or

00:39:00,269 --> 00:39:06,089
do you want to just write performance

00:39:03,569 --> 00:39:10,890
code and hope that it can be ported to

00:39:06,089 --> 00:39:13,589
other architectures easily and the

00:39:10,890 --> 00:39:17,160
co-pays that I have we have used for

00:39:13,589 --> 00:39:18,599
this particular project is great some of

00:39:17,160 --> 00:39:21,480
you may have heard of it

00:39:18,599 --> 00:39:26,279
greatest civics party large excuse me

00:39:21,480 --> 00:39:28,289
library still being developed by a group

00:39:26,279 --> 00:39:30,839
at University of Edinburgh and

00:39:28,289 --> 00:39:34,529
particularly in particular Peter Boyle

00:39:30,839 --> 00:39:38,670
and it was originally developed and

00:39:34,529 --> 00:39:43,230
optimized for various CPUs including the

00:39:38,670 --> 00:39:45,869
Intel xenon by architectures and now it

00:39:43,230 --> 00:39:47,180
is being used as a testbed for the QCD

00:39:45,869 --> 00:39:51,720
exascale

00:39:47,180 --> 00:39:54,509
performance possibility studies it uses

00:39:51,720 --> 00:39:57,210
a lot of the C++ 11 features for

00:39:54,509 --> 00:40:01,799
abstractions and for high level

00:39:57,210 --> 00:40:04,049
programming effects abilities and one

00:40:01,799 --> 00:40:08,190
nice thing about grid is that it's data

00:40:04,049 --> 00:40:10,980
layout was designed to match the CPUs in

00:40:08,190 --> 00:40:13,799
dealings earlier I mentioned that we

00:40:10,980 --> 00:40:15,509
simply decomposed the lattice but the

00:40:13,799 --> 00:40:19,140
whole adage to map onto different

00:40:15,509 --> 00:40:22,739
compute nodes with a longer simply

00:40:19,140 --> 00:40:26,249
length I also talked about a translation

00:40:22,739 --> 00:40:29,999
for the will the domain will be slash

00:40:26,249 --> 00:40:32,549
earlier as one of the projects but if I

00:40:29,999 --> 00:40:35,069
just vectorized over one dimension I run

00:40:32,549 --> 00:40:36,989
into the constraint that the length of

00:40:35,069 --> 00:40:38,370
that dimension has to be multiples of

00:40:36,989 --> 00:40:44,690
the same genus

00:40:38,370 --> 00:40:48,720
and increase the ladders each come each

00:40:44,690 --> 00:40:52,230
ladders on a computer is further

00:40:48,720 --> 00:40:55,130
decomposed into several virtual nodes

00:40:52,230 --> 00:40:58,350
and this mercial nodes are mapped into

00:40:55,130 --> 00:41:01,320
different sim dealings

00:40:58,350 --> 00:41:04,860
this approach has the advantage of being

00:41:01,320 --> 00:41:06,690
able to map onto different things in

00:41:04,860 --> 00:41:12,830
length without running into the

00:41:06,690 --> 00:41:12,830
constraint of the largest dimension

00:41:15,060 --> 00:41:24,240
and this is a cost limit showing you

00:41:19,950 --> 00:41:26,900
what the gray looks like on the this is

00:41:24,240 --> 00:41:30,840
the intermediate level so there are some

00:41:26,900 --> 00:41:34,850
architecture specific implementations

00:41:30,840 --> 00:41:37,320
for the reservation it uses intrinsic

00:41:34,850 --> 00:41:39,210
depending on the targets are the result

00:41:37,320 --> 00:41:42,150
of generic implementation without

00:41:39,210 --> 00:41:47,190
intrinsic so here you can use OMP CMD

00:41:42,150 --> 00:41:50,430
and the data types are highly abstracted

00:41:47,190 --> 00:41:53,180
so on the higher level you won't be able

00:41:50,430 --> 00:41:56,670
to see all those Hardware specific

00:41:53,180 --> 00:42:01,020
implementations and again we have a lot

00:41:56,670 --> 00:42:04,050
of follows so in grid everything is time

00:42:01,020 --> 00:42:10,440
to try and follow so anemia right who

00:42:04,050 --> 00:42:13,110
the different that is dimensions and the

00:42:10,440 --> 00:42:15,960
most commonly used

00:42:13,110 --> 00:42:18,830
pragma is again Taylor Hall and perhaps

00:42:15,960 --> 00:42:18,830
sometimes collapse

00:42:23,080 --> 00:42:29,950
and with the abstractions done in grids

00:42:27,670 --> 00:42:35,500
you can write very high-level code such

00:42:29,950 --> 00:42:38,170
as this so this expression because life

00:42:35,500 --> 00:42:41,500
is wider giant love is wide matrix

00:42:38,170 --> 00:42:44,790
matrix multiplication and here to some

00:42:41,500 --> 00:42:49,120
performance on the GPU based

00:42:44,790 --> 00:42:52,870
architectures and you can see the

00:42:49,120 --> 00:42:56,190
performance is pretty impressive across

00:42:52,870 --> 00:42:58,210
the board but it doesn't support GPUs

00:42:56,190 --> 00:43:01,390
CPU is not among the small the

00:42:58,210 --> 00:43:04,300
architectures at the moment and which we

00:43:01,390 --> 00:43:07,660
started the GPU porting effort about a

00:43:04,300 --> 00:43:10,390
year ago and first we try to use open

00:43:07,660 --> 00:43:12,850
ACC but it was completely impossible

00:43:10,390 --> 00:43:16,060
with the whole grid library because of

00:43:12,850 --> 00:43:20,100
the big copy issues is the data

00:43:16,060 --> 00:43:22,870
structure and grid are very nested and

00:43:20,100 --> 00:43:28,480
there are also other compiler related

00:43:22,870 --> 00:43:31,180
issues and so last summer we Peter

00:43:28,480 --> 00:43:34,180
actually just stripped down the grid

00:43:31,180 --> 00:43:39,010
into a great expression temper into a

00:43:34,180 --> 00:43:40,660
very more self-contained code to see if

00:43:39,010 --> 00:43:44,710
we could get this piece of the code to

00:43:40,660 --> 00:43:46,840
work on both CPUs and GPUs and the code

00:43:44,710 --> 00:43:48,640
is very simple so basically the key

00:43:46,840 --> 00:43:54,090
kernel is just this for loop that does

00:43:48,640 --> 00:43:54,090
the that evaluates different expression

00:43:54,470 --> 00:44:00,770
and the approaches we studies include

00:43:57,380 --> 00:44:04,339
open ACC we also tried open MP but it

00:44:00,770 --> 00:44:07,160
didn't work and now I'll tell you why in

00:44:04,339 --> 00:44:12,760
a bit and we also explore just-in-time

00:44:07,160 --> 00:44:16,160
compilation and CUDA of course CUDA is

00:44:12,760 --> 00:44:20,300
specific to NVIDIA GPUs as far as I know

00:44:16,160 --> 00:44:24,650
as of now and but it is a very mature

00:44:20,300 --> 00:44:29,349
programming model for NVIDIA GPUs and it

00:44:24,650 --> 00:44:31,339
also has political C++ support but the

00:44:29,349 --> 00:44:36,290
disadvantage is that you need to write

00:44:31,339 --> 00:44:39,410
some CUDA kernels and it is unavoidable

00:44:36,290 --> 00:44:46,780
who have some Co crunching which is not

00:44:39,410 --> 00:44:46,780
something that we are very fond of and

00:44:47,230 --> 00:44:54,349
for the offloading compiler support

00:44:51,680 --> 00:44:57,079
theater system probably slightly out of

00:44:54,349 --> 00:45:00,290
date are less than this was a slide from

00:44:57,079 --> 00:45:03,890
last year and you can see that in

00:45:00,290 --> 00:45:07,760
principle open MP and open ACC opening

00:45:03,890 --> 00:45:12,410
VN or open ACC should be able to support

00:45:07,760 --> 00:45:16,930
our GPU offloading but for our specific

00:45:12,410 --> 00:45:19,940
application because of the C++ features

00:45:16,930 --> 00:45:23,230
using directives has been quite

00:45:19,940 --> 00:45:26,260
difficult for us so this is just some

00:45:23,230 --> 00:45:29,630
potential implementations with different

00:45:26,260 --> 00:45:31,780
programming models for open ACC it's

00:45:29,630 --> 00:45:35,300
similar to open MP you see you have this

00:45:31,780 --> 00:45:38,720
parallel loop closes and then they touch

00:45:35,300 --> 00:45:40,750
some data sources and open MP you also

00:45:38,720 --> 00:45:45,620
have data sources and

00:45:40,750 --> 00:45:48,800
parallel computing closes and then this

00:45:45,620 --> 00:45:51,410
is the CUDA kernel that we would need to

00:45:48,800 --> 00:45:56,000
write if we want to include that as our

00:45:51,410 --> 00:45:58,640
programming model and this is some head

00:45:56,000 --> 00:46:00,110
of library that was developed in detail

00:45:58,640 --> 00:46:02,300
and if you use it just in time

00:46:00,110 --> 00:46:09,170
compilation in principle you can write

00:46:02,300 --> 00:46:12,590
it this way so open NCC did work for us

00:46:09,170 --> 00:46:15,620
but we need to use the unified virtual

00:46:12,590 --> 00:46:19,270
memory support in the PGI compiler

00:46:15,620 --> 00:46:23,000
because we couldn't do many or data

00:46:19,270 --> 00:46:28,990
copying due to the nested structures v

00:46:23,000 --> 00:46:31,970
data structures in this code and openmp

00:46:28,990 --> 00:46:34,400
we would like to be able to do this but

00:46:31,970 --> 00:46:36,800
it didn't work because if the compiler

00:46:34,400 --> 00:46:41,890
doesn't support a unified virtual memory

00:46:36,800 --> 00:46:44,560
and for these other two assistance die

00:46:41,890 --> 00:46:47,900
it's fairly simple because it's

00:46:44,560 --> 00:46:50,370
basically what they can do they are

00:46:47,900 --> 00:46:54,930
supposed to do so

00:46:50,370 --> 00:46:57,780
I want to point out that even with CUDA

00:46:54,930 --> 00:47:01,620
it makes our life much much simpler by

00:46:57,780 --> 00:47:06,710
using the managed memory so we basically

00:47:01,620 --> 00:47:11,040
have a custom memory allocator which

00:47:06,710 --> 00:47:14,240
uses intrinsic sona cpus but on GPUs

00:47:11,040 --> 00:47:20,940
it's just a Buddha may not manage to to

00:47:14,240 --> 00:47:26,090
simplify the data management so here's

00:47:20,940 --> 00:47:29,970
some performance numbers for CUDA

00:47:26,090 --> 00:47:31,770
certified and open a cc you can see this

00:47:29,970 --> 00:47:34,050
is just a pure out-of-the-box

00:47:31,770 --> 00:47:35,580
performance and you can see open SEC

00:47:34,050 --> 00:47:38,580
actually give us pretty good performance

00:47:35,580 --> 00:47:43,140
it was it's better than the other two

00:47:38,580 --> 00:47:46,380
models but because we have more control

00:47:43,140 --> 00:47:49,370
with either cuda identified we could

00:47:46,380 --> 00:47:52,950
further improve the performance to this

00:47:49,370 --> 00:47:57,930
so about 80% of the theoretical peak

00:47:52,950 --> 00:48:01,950
this is a TT X 1080 and and this is just

00:47:57,930 --> 00:48:06,360
a streaming head of a times B similar to

00:48:01,950 --> 00:48:10,290
the stream try out streaming test and

00:48:06,360 --> 00:48:13,290
you can see for this other two models we

00:48:10,290 --> 00:48:19,230
could step critical performance the

00:48:13,290 --> 00:48:23,070
catch is that we had to use a colored

00:48:19,230 --> 00:48:26,280
pointer to ensure detail Collison Singh

00:48:23,070 --> 00:48:29,250
on the GPUs and we couldn't do that with

00:48:26,280 --> 00:48:30,930
open ACC because it is the code didn't

00:48:29,250 --> 00:48:35,580
work so there were some of the

00:48:30,930 --> 00:48:38,070
challenges that we had I see that our

00:48:35,580 --> 00:48:42,030
Chairman standing there so I'll wrap up

00:48:38,070 --> 00:48:45,410
very quickly but greater data they all

00:48:42,030 --> 00:48:49,170
can actually be mapped to GPUs fairly

00:48:45,410 --> 00:48:51,950
easily to get very good performance so

00:48:49,170 --> 00:48:54,020
I'm going to skip the details but

00:48:51,950 --> 00:48:56,420
I want to show you the performance that

00:48:54,020 --> 00:48:59,240
we could achieve this result these

00:48:56,420 --> 00:49:01,280
results are the results from the kuda

00:48:59,240 --> 00:49:03,500
implementation because of the

00:49:01,280 --> 00:49:06,560
difficulties with either open MP open

00:49:03,500 --> 00:49:10,250
ACC so this is the result from Kuta for

00:49:06,560 --> 00:49:14,930
the GPU so the GPU performances are this

00:49:10,250 --> 00:49:18,589
one this is for Tesla this is Volta and

00:49:14,930 --> 00:49:22,280
this is a Pascal and this is the result

00:49:18,589 --> 00:49:24,640
for nice blending and be known let's

00:49:22,280 --> 00:49:28,579
broad well this is the Intel prod well

00:49:24,640 --> 00:49:32,770
and you can see a lot - lines are the

00:49:28,579 --> 00:49:36,349
stream trial results so the for the mini

00:49:32,770 --> 00:49:38,900
code that we have that is built on top

00:49:36,349 --> 00:49:42,380
of grid we could actually achieve pretty

00:49:38,900 --> 00:49:46,130
good performance compared to the stream

00:49:42,380 --> 00:49:48,980
trial code so we are basically getting

00:49:46,130 --> 00:49:52,760
the best performance we could possibly

00:49:48,980 --> 00:49:54,230
expect which gives us hope that it maybe

00:49:52,760 --> 00:49:57,319
it is possible to write performance

00:49:54,230 --> 00:50:00,140
portable code so I want to point out

00:49:57,319 --> 00:50:02,780
that these results are all obtained with

00:50:00,140 --> 00:50:06,230
the same code which is compile compile

00:50:02,780 --> 00:50:09,490
for different targets I want to skip

00:50:06,230 --> 00:50:09,490
this one yep

00:50:12,780 --> 00:50:16,250
never bet with you

00:50:18,070 --> 00:50:23,740
yeah so the I didn't mention this but

00:50:21,550 --> 00:50:26,590
QCD calculations are memory bandwidth

00:50:23,740 --> 00:50:32,010
bound so our compute intensity is

00:50:26,590 --> 00:50:32,010
clearly low so it's about around one

00:50:36,280 --> 00:50:42,400
so let me just summarize that askew see

00:50:40,090 --> 00:50:47,280
these regular quick structure is great

00:50:42,400 --> 00:50:47,280
vocalizations in specific applause and

00:50:47,490 --> 00:50:53,110
the diverse hardware architectures that

00:50:50,320 --> 00:50:56,140
that is key software has been optimized

00:50:53,110 --> 00:50:59,770
for resulting in significant division of

00:50:56,140 --> 00:51:02,530
code bases and OpenMP is becoming more

00:50:59,770 --> 00:51:05,950
and more important to us with the

00:51:02,530 --> 00:51:10,690
increasing of the parallelism on the

00:51:05,950 --> 00:51:13,600
node and perhaps opening peak and

00:51:10,690 --> 00:51:18,180
finally give us a way to achieve this

00:51:13,600 --> 00:51:21,880
grant unification of the code bases so

00:51:18,180 --> 00:51:24,520
we didn't we we haven't used a lot of

00:51:21,880 --> 00:51:27,160
the different openmp features but what

00:51:24,520 --> 00:51:30,580
works quite well for us is the simple

00:51:27,160 --> 00:51:33,940
parallel for loop parallel for construct

00:51:30,580 --> 00:51:36,730
and sometimes we use the API cause of to

00:51:33,940 --> 00:51:39,940
get more flexibility in the way we want

00:51:36,730 --> 00:51:43,810
to do the calculations what may or was

00:51:39,940 --> 00:51:47,700
the cindy construct can potentially

00:51:43,810 --> 00:51:51,430
simplify the programming for the vector

00:51:47,700 --> 00:51:55,780
architectures greatly and even though it

00:51:51,430 --> 00:52:00,760
didn't work for our particular case some

00:51:55,780 --> 00:52:02,800
recent work has shown that if you if you

00:52:00,760 --> 00:52:05,920
have a good data layout it's possible to

00:52:02,800 --> 00:52:10,750
achieve very good performance with this

00:52:05,920 --> 00:52:12,820
apart my MD construct and we may be able

00:52:10,750 --> 00:52:14,760
to use tasking to overlap communication

00:52:12,820 --> 00:52:17,250
and computation

00:52:14,760 --> 00:52:20,280
and what we really really need right now

00:52:17,250 --> 00:52:23,010
for the support classifications is the

00:52:20,280 --> 00:52:26,369
better support for people across GP

00:52:23,010 --> 00:52:30,750
offloading because the complex C++

00:52:26,369 --> 00:52:33,510
programming styles will require 40 copy

00:52:30,750 --> 00:52:35,970
or unified virtual memory for the

00:52:33,510 --> 00:52:38,330
accelerators and that's all thank you

00:52:35,970 --> 00:52:38,330
very much

00:52:41,869 --> 00:52:46,310
so we do have time for questions

00:52:57,510 --> 00:53:00,170
yep

00:53:01,110 --> 00:53:04,110
camp

00:53:04,290 --> 00:53:06,350
in

00:53:14,700 --> 00:53:17,700
five

00:53:20,740 --> 00:53:28,870
so this for the directive for visits

00:53:25,470 --> 00:53:32,620
implemented we in the open ACC we only

00:53:28,870 --> 00:53:38,200
try PGI because we need the unified

00:53:32,620 --> 00:53:40,120
memory support and as far as I know PGI

00:53:38,200 --> 00:53:42,670
is the only compiler that can support us

00:53:40,120 --> 00:53:45,570
because of the nested C++ data

00:53:42,670 --> 00:53:48,110
structures we really need that support

00:53:45,570 --> 00:53:50,090
yes

00:53:48,110 --> 00:53:56,570
why

00:53:50,090 --> 00:53:59,660
yes so this x and y are arrays of three

00:53:56,570 --> 00:54:01,910
by three matrices yeah so it's a very

00:53:59,660 --> 00:54:05,050
big you can just think of them as very

00:54:01,910 --> 00:54:05,050
big arrays yes

00:54:07,710 --> 00:54:10,909
[Music]

00:54:12,970 --> 00:54:15,660

YouTube URL: https://www.youtube.com/watch?v=WlGWgBimBGk


