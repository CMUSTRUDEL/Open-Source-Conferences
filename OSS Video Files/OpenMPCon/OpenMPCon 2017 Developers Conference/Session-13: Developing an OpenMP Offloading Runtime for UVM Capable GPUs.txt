Title: Session-13: Developing an OpenMP Offloading Runtime for UVM Capable GPUs
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Hashim Sharif, Lingda Li, Hal Finkel and Vikram Adve
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day2-Session3-Sharif.pdf
Captions: 
	00:00:04,100 --> 00:00:09,960
hi my name is Hashem and the work I'm

00:00:08,280 --> 00:00:12,780
presenting today is titled developing an

00:00:09,960 --> 00:00:14,429
open MP runtime for UV incapable GPUs

00:00:12,780 --> 00:00:17,190
this work is in conjunction with my

00:00:14,429 --> 00:00:19,680
advisor at UIUC vikram outlet and our

00:00:17,190 --> 00:00:25,500
collaborators how think'll at Argonne

00:00:19,680 --> 00:00:26,760
lab and Linda Lee at Brookhaven so I'll

00:00:25,500 --> 00:00:28,769
probably a bit of introduction of

00:00:26,760 --> 00:00:30,090
heterogeneous programming in why it's

00:00:28,769 --> 00:00:31,800
interesting the heterogeneous

00:00:30,090 --> 00:00:33,750
programming is a paradigm because you

00:00:31,800 --> 00:00:35,010
have these different types of processors

00:00:33,750 --> 00:00:38,460
working in conjunction with each other

00:00:35,010 --> 00:00:40,379
and it's interesting because you can

00:00:38,460 --> 00:00:42,030
specialize up different application

00:00:40,379 --> 00:00:44,010
tasks according to the particular

00:00:42,030 --> 00:00:46,770
computation needs for instance you can

00:00:44,010 --> 00:00:48,570
have GPUs and many core CPUs work in

00:00:46,770 --> 00:00:50,280
conjunction and they can be specialized

00:00:48,570 --> 00:00:52,800
for different tasks for instance for

00:00:50,280 --> 00:00:55,020
serial processing you can have many core

00:00:52,800 --> 00:00:57,120
cpus for massively parallel applications

00:00:55,020 --> 00:01:01,320
which are more throughput bound you can

00:00:57,120 --> 00:01:02,820
have GPUs and interestingly OpenMP now

00:01:01,320 --> 00:01:05,129
includes support for heterogeneous

00:01:02,820 --> 00:01:07,650
programming you can actually target your

00:01:05,129 --> 00:01:11,729
OpenMP regions for different target

00:01:07,650 --> 00:01:13,320
devices in this particular talk I'm

00:01:11,729 --> 00:01:16,830
mostly going to be talking about GPUs

00:01:13,320 --> 00:01:19,170
and in particular CUDA Unified virtual

00:01:16,830 --> 00:01:20,640
memory and what is cool diode fiber so

00:01:19,170 --> 00:01:23,340
many Okuda in fiber version memory is

00:01:20,640 --> 00:01:25,979
this abstraction that gives you a single

00:01:23,340 --> 00:01:28,799
address space across all the GPUs and

00:01:25,979 --> 00:01:30,090
CPUs in the system and this will

00:01:28,799 --> 00:01:33,329
introduce as part of the Kepler

00:01:30,090 --> 00:01:36,030
architecture in 2014 and now it's gone a

00:01:33,329 --> 00:01:38,579
step further and in the as part of the

00:01:36,030 --> 00:01:40,170
past passcode architecture it includes

00:01:38,579 --> 00:01:41,939
the automatic page migration engine

00:01:40,170 --> 00:01:46,110
which allows you to fetch pages on

00:01:41,939 --> 00:01:48,240
demand and moreover this and because of

00:01:46,110 --> 00:01:49,890
this particular technology now you can

00:01:48,240 --> 00:01:51,600
fetch pages on demand don't do not have

00:01:49,890 --> 00:01:53,250
the existing limitation that was with

00:01:51,600 --> 00:01:55,890
Kepler architecture where you had to

00:01:53,250 --> 00:01:59,100
copy all the data before the colonel had

00:01:55,890 --> 00:02:01,200
to be launched moreover in the Pascal

00:01:59,100 --> 00:02:03,719
uvm architecture you're no limit no

00:02:01,200 --> 00:02:05,969
longer limited by device memory you only

00:02:03,719 --> 00:02:08,569
limited limited by the overall system

00:02:05,969 --> 00:02:12,200
memory size we just useful

00:02:08,569 --> 00:02:13,730
you can run on very large that's down so

00:02:12,200 --> 00:02:16,010
this is an example which shows you how

00:02:13,730 --> 00:02:18,230
traditional kuda compares to kuda UVM

00:02:16,010 --> 00:02:20,209
you know it's it's it's a vector I'd

00:02:18,230 --> 00:02:23,450
example if you take two vectors x and y

00:02:20,209 --> 00:02:26,299
as a result back and returns a result in

00:02:23,450 --> 00:02:27,590
vector Z and essentially what you see

00:02:26,299 --> 00:02:30,200
right over here is that the explicit

00:02:27,590 --> 00:02:32,389
data copies are explicitly copying host

00:02:30,200 --> 00:02:34,519
buffers onto device buffers and then

00:02:32,389 --> 00:02:37,189
launching the kernel that does the

00:02:34,519 --> 00:02:40,400
vector addition on the right where you

00:02:37,189 --> 00:02:42,400
have the CUDA uvm code you see that

00:02:40,400 --> 00:02:45,230
there are no explicit copies just

00:02:42,400 --> 00:02:47,090
allocating memory in in the unified

00:02:45,230 --> 00:02:48,409
virtual memory space and you just launch

00:02:47,090 --> 00:02:53,599
the kernel and the memory can be fetched

00:02:48,409 --> 00:02:55,310
on demand essentially now how does open

00:02:53,599 --> 00:02:56,719
talk about openmp supporting

00:02:55,310 --> 00:02:58,159
heterogeneous systems and Hedges

00:02:56,719 --> 00:03:00,049
programming how does this really look in

00:02:58,159 --> 00:03:01,189
the picture so openmp was traditionally

00:03:00,049 --> 00:03:03,590
supposed to be used with

00:03:01,189 --> 00:03:05,030
as a shared memory should marry

00:03:03,590 --> 00:03:06,889
parallelism where you have these

00:03:05,030 --> 00:03:09,319
multiple threads running on different

00:03:06,889 --> 00:03:12,469
processors sharing the same physical

00:03:09,319 --> 00:03:14,090
memory and the affair data and the

00:03:12,469 --> 00:03:15,889
actors are here to clean up on this from

00:03:14,090 --> 00:03:17,540
the same memory space now with

00:03:15,889 --> 00:03:19,729
accelerators coming into the picture

00:03:17,540 --> 00:03:23,389
that are mostly attached to an external

00:03:19,729 --> 00:03:25,639
bus such as PCI Express now openmp must

00:03:23,389 --> 00:03:28,669
include these new directors that allow

00:03:25,639 --> 00:03:32,840
you to map data explicitly map data for

00:03:28,669 --> 00:03:36,919
target accelerators essentially how does

00:03:32,840 --> 00:03:39,439
this look in code well here you have an

00:03:36,919 --> 00:03:41,689
open MP open a piece shop so sort of

00:03:39,439 --> 00:03:43,970
open every program you have sex be

00:03:41,689 --> 00:03:46,340
essentially the input vectors are x and

00:03:43,970 --> 00:03:48,459
y you're calculating expression then

00:03:46,340 --> 00:03:51,079
you're storing the result back and Y and

00:03:48,459 --> 00:03:52,729
two important things here firstly with

00:03:51,079 --> 00:03:54,709
the open MP loop you have a target

00:03:52,729 --> 00:03:57,590
construct the target Clause here

00:03:54,709 --> 00:04:01,489
specifies that this loop should be

00:03:57,590 --> 00:04:03,560
compiled for for for an accelerator the

00:04:01,489 --> 00:04:05,689
accelerator to which is compiled for is

00:04:03,560 --> 00:04:08,090
specified at comma in the command line

00:04:05,689 --> 00:04:10,550
when you compiling compiling the program

00:04:08,090 --> 00:04:12,860
and then very importantly you have these

00:04:10,550 --> 00:04:15,080
data mapping clauses that specify which

00:04:12,860 --> 00:04:16,760
data needs to be copied to the device

00:04:15,080 --> 00:04:18,230
before the execution can begin and you

00:04:16,760 --> 00:04:20,090
can see that this recently there's a

00:04:18,230 --> 00:04:21,560
direction of copy as well for the X

00:04:20,090 --> 00:04:23,270
Factor because it's just infer vector

00:04:21,560 --> 00:04:26,000
doesn't need to be copied back the

00:04:23,270 --> 00:04:27,710
direction is to with Y because it's also

00:04:26,000 --> 00:04:29,120
the output the direction is to from you

00:04:27,710 --> 00:04:35,360
copy it and you also have to copy it

00:04:29,120 --> 00:04:38,720
back so how does OpenMP offloading

00:04:35,360 --> 00:04:40,880
really work in practice so there's this

00:04:38,720 --> 00:04:43,250
effort known as the liver would be

00:04:40,880 --> 00:04:45,380
target this is a clang is an open empty

00:04:43,250 --> 00:04:47,570
offloading on time is an effort by

00:04:45,380 --> 00:04:51,590
multiple vendors including Intel and IBM

00:04:47,570 --> 00:04:55,190
and essentially it enables offloading

00:04:51,590 --> 00:04:57,530
and this is how the binary looks like

00:04:55,190 --> 00:05:00,229
when you're compiling an open MP program

00:04:57,530 --> 00:05:02,720
with target support in addition to the

00:05:00,229 --> 00:05:05,660
host code right here you also have the

00:05:02,720 --> 00:05:07,700
device code and the device was compiled

00:05:05,660 --> 00:05:10,760
for for the open up constructs for which

00:05:07,700 --> 00:05:13,610
you had the target class essentially and

00:05:10,760 --> 00:05:15,200
in in the general case you had host code

00:05:13,610 --> 00:05:16,940
interacting with the host open appear on

00:05:15,200 --> 00:05:20,120
the host so gravity on time includes

00:05:16,940 --> 00:05:22,070
these routines that include implement

00:05:20,120 --> 00:05:24,050
the parallelization for the host part

00:05:22,070 --> 00:05:26,270
you know if you have a program you loop

00:05:24,050 --> 00:05:29,060
n you're running on the CPU this runtime

00:05:26,270 --> 00:05:30,890
implements that paralyzation but in

00:05:29,060 --> 00:05:33,020
addition to that now because target

00:05:30,890 --> 00:05:35,510
constructs are coming to the picture you

00:05:33,020 --> 00:05:37,280
have an offloading runtime going to be

00:05:35,510 --> 00:05:41,030
target which implements the underlying

00:05:37,280 --> 00:05:42,830
API property and the important thing to

00:05:41,030 --> 00:05:44,690
notice here is that this particular

00:05:42,830 --> 00:05:46,669
runtime is device agnostic it's not

00:05:44,690 --> 00:05:49,550
specific to any particular device in

00:05:46,669 --> 00:05:52,400
fact on the back of this you have device

00:05:49,550 --> 00:05:53,660
specific plugins that do device specific

00:05:52,400 --> 00:05:54,740
operations in this case because we've

00:05:53,660 --> 00:05:56,690
been talking about CUDA

00:05:54,740 --> 00:05:59,570
you have a CUDA device plug-in on the

00:05:56,690 --> 00:06:01,669
backend and it essentially does device

00:05:59,570 --> 00:06:03,620
specific operations for instance copying

00:06:01,669 --> 00:06:07,340
data to the GPU or launching kernel

00:06:03,620 --> 00:06:09,110
execution and that plug-in in turn calls

00:06:07,340 --> 00:06:12,080
the cooler driver API to do the actual

00:06:09,110 --> 00:06:14,660
device specific operations

00:06:12,080 --> 00:06:17,990
now coming to what what we've been doing

00:06:14,660 --> 00:06:20,780
with this with this runtime and with UVM

00:06:17,990 --> 00:06:23,360
in general we build this OpenMP

00:06:20,780 --> 00:06:27,380
framework for UVM and the goal is

00:06:23,360 --> 00:06:29,060
essentially to make OpenMP work with you

00:06:27,380 --> 00:06:31,730
know UVM architectures in a performant

00:06:29,060 --> 00:06:33,410
way another goal is other than

00:06:31,730 --> 00:06:36,100
performances to hyperbolic probability

00:06:33,410 --> 00:06:40,450
we don't want our architecture to be

00:06:36,100 --> 00:06:42,590
fundamentally confined only to UVM and

00:06:40,450 --> 00:06:45,050
to achieve these goals there some design

00:06:42,590 --> 00:06:47,420
considerations for instance what

00:06:45,050 --> 00:06:48,830
information can use from openmp to

00:06:47,420 --> 00:06:51,020
actually have such good performance for

00:06:48,830 --> 00:06:52,850
instance we just show you the open epic

00:06:51,020 --> 00:06:54,740
target classes can we use these data

00:06:52,850 --> 00:06:58,420
mapping causes to actually improve

00:06:54,740 --> 00:07:00,680
performance moreover with UVM as

00:06:58,420 --> 00:07:01,970
datasets are no longer limited by device

00:07:00,680 --> 00:07:03,410
memory you can have really large data

00:07:01,970 --> 00:07:06,050
sets so you only limited by system

00:07:03,410 --> 00:07:09,740
memory size can you make performance

00:07:06,050 --> 00:07:13,370
scale with large data sets and that this

00:07:09,740 --> 00:07:14,780
again is one of our goals so what is

00:07:13,370 --> 00:07:16,880
this framework recompose Duncan family

00:07:14,780 --> 00:07:19,130
is composed of two major components you

00:07:16,880 --> 00:07:21,440
have a compiler component now you'd have

00:07:19,130 --> 00:07:23,210
a runtime component and the compiler

00:07:21,440 --> 00:07:25,100
component essentially extracts

00:07:23,210 --> 00:07:26,810
application specific information and

00:07:25,100 --> 00:07:28,820
feeds it to the runtime and the runtime

00:07:26,810 --> 00:07:30,110
uses this information to do certain

00:07:28,820 --> 00:07:32,060
optimizations I'll show the

00:07:30,110 --> 00:07:34,760
optimizations in a minute and how these

00:07:32,060 --> 00:07:36,350
two things essentially interact for

00:07:34,760 --> 00:07:38,300
instance this application specific

00:07:36,350 --> 00:07:41,030
information could be data access

00:07:38,300 --> 00:07:42,860
probabilities how likely is a certain

00:07:41,030 --> 00:07:44,630
piece of data to be accessed because if

00:07:42,860 --> 00:07:45,080
it's likely to be accessed maybe we

00:07:44,630 --> 00:07:47,720
should

00:07:45,080 --> 00:07:50,330
copy in in advance we'll talk about this

00:07:47,720 --> 00:07:52,580
more later then comes in the open appear

00:07:50,330 --> 00:07:54,410
on time so I showed you the open MP

00:07:52,580 --> 00:07:57,440
wanta implementation live and B target

00:07:54,410 --> 00:07:59,840
which implements the API for offloading

00:07:57,440 --> 00:08:01,880
we essentially extend that in particular

00:07:59,840 --> 00:08:03,950
we extend the device cuda device plug in

00:08:01,880 --> 00:08:05,180
and we firstly make it UV incompatible

00:08:03,950 --> 00:08:07,340
it didn't have support for you know

00:08:05,180 --> 00:08:10,280
architectures when using compatible also

00:08:07,340 --> 00:08:12,250
being incorporate a couple of UVM

00:08:10,280 --> 00:08:14,420
specific optimizations that we show

00:08:12,250 --> 00:08:16,900
prefers improve performance quite

00:08:14,420 --> 00:08:16,900
significantly

00:08:16,990 --> 00:08:21,490
so starting with optimizations in the

00:08:19,780 --> 00:08:22,840
interesting part and the first

00:08:21,490 --> 00:08:27,130
optimization that we implement is

00:08:22,840 --> 00:08:28,990
prefetching and this essentially so this

00:08:27,130 --> 00:08:30,580
is how it boils down when you have on

00:08:28,990 --> 00:08:33,760
the left side you have an open D code

00:08:30,580 --> 00:08:35,289
the same code Saxby you have X&Y or the

00:08:33,760 --> 00:08:37,260
input vectors and then you could be

00:08:35,289 --> 00:08:39,340
doing expression saving it back to I

00:08:37,260 --> 00:08:42,669
essentially when this code is compiled

00:08:39,340 --> 00:08:45,100
this maps down to an API call into the

00:08:42,669 --> 00:08:47,980
runtime right here execute target region

00:08:45,100 --> 00:08:50,290
which essentially starts the invokes

00:08:47,980 --> 00:08:51,550
that in mostly computation kernel and

00:08:50,290 --> 00:08:53,320
the important thing to note here is that

00:08:51,550 --> 00:08:55,450
with you VM you don't really need to

00:08:53,320 --> 00:08:58,180
have any data puppies because data can

00:08:55,450 --> 00:09:01,390
be fetched on demand right but

00:08:58,180 --> 00:09:04,360
interestingly and importantly the the

00:09:01,390 --> 00:09:06,040
cooler driver API does include API that

00:09:04,360 --> 00:09:07,510
allows you to prefetch that data in

00:09:06,040 --> 00:09:10,300
advance if you know the data you're

00:09:07,510 --> 00:09:12,280
accessing and that basically is

00:09:10,300 --> 00:09:14,860
important or interesting because it

00:09:12,280 --> 00:09:16,240
reduces the page fault processing

00:09:14,860 --> 00:09:21,040
overhead which actually becomes quite

00:09:16,240 --> 00:09:22,960
significant so this particular

00:09:21,040 --> 00:09:25,450
prefetching optimization how does this

00:09:22,960 --> 00:09:27,580
work in practice so you have the

00:09:25,450 --> 00:09:28,960
compiler side compiler extracts certain

00:09:27,580 --> 00:09:31,150
information the goal of the compiler

00:09:28,960 --> 00:09:33,310
especially here is to act to extract

00:09:31,150 --> 00:09:34,720
data access probabilities because if a

00:09:33,310 --> 00:09:36,100
data is likely to be accessed we want to

00:09:34,720 --> 00:09:40,450
prefetch if it's not likely to be

00:09:36,100 --> 00:09:41,920
accessed letter demanding and so here

00:09:40,450 --> 00:09:44,890
comes the application source we compile

00:09:41,920 --> 00:09:47,100
it to all of Mir and then profile the

00:09:44,890 --> 00:09:49,480
application on representative inputs and

00:09:47,100 --> 00:09:51,190
then comes into the picture a compiler

00:09:49,480 --> 00:09:53,320
transformation that extracts these

00:09:51,190 --> 00:09:56,070
probabilities and associates them with

00:09:53,320 --> 00:09:58,810
the data data variables in the program

00:09:56,070 --> 00:10:00,700
then and there's another compiler

00:09:58,810 --> 00:10:02,230
transformation that packs this

00:10:00,700 --> 00:10:05,170
information the access probability is up

00:10:02,230 --> 00:10:06,820
and emits a transfer an IR the transform

00:10:05,170 --> 00:10:07,750
wire now includes this application

00:10:06,820 --> 00:10:10,450
specific information and the access

00:10:07,750 --> 00:10:12,280
probabilities and it's built into this

00:10:10,450 --> 00:10:13,990
final executable

00:10:12,280 --> 00:10:15,460
now when you have this executable the

00:10:13,990 --> 00:10:17,590
executable is going to interact with the

00:10:15,460 --> 00:10:19,660
runtime so I'm very sure with the books

00:10:17,590 --> 00:10:21,430
law for the runtime the ads executive

00:10:19,660 --> 00:10:24,310
all with the pact up with the excess

00:10:21,430 --> 00:10:26,650
probabilities it makes calls to the

00:10:24,310 --> 00:10:28,420
offloading run time to actually offload

00:10:26,650 --> 00:10:30,850
a particular kernel onto a target

00:10:28,420 --> 00:10:34,000
accelerator and now includes the access

00:10:30,850 --> 00:10:35,860
probabilities the offloading runtime for

00:10:34,000 --> 00:10:37,960
doing the device specific stuff cause

00:10:35,860 --> 00:10:42,010
the CUDA device plugin the correct

00:10:37,960 --> 00:10:43,420
device plug-in uses a cost model too and

00:10:42,010 --> 00:10:45,940
the cost model uses these access

00:10:43,420 --> 00:10:48,400
probabilities to actually determine if

00:10:45,940 --> 00:10:50,770
its profitable to copy that copy that

00:10:48,400 --> 00:10:51,820
data so on the cost model includes you

00:10:50,770 --> 00:10:52,780
know the application specific

00:10:51,820 --> 00:10:55,360
information which is the access

00:10:52,780 --> 00:10:57,100
probabilities also it has some hardware

00:10:55,360 --> 00:10:58,300
specific constants which will vary a few

00:10:57,100 --> 00:11:03,610
which will vary with different

00:10:58,300 --> 00:11:06,460
interconnects and then based on whatever

00:11:03,610 --> 00:11:09,070
decision this runtime makes it decides

00:11:06,460 --> 00:11:10,510
to prefetch to either prepare to not

00:11:09,070 --> 00:11:12,880
prefetch and then initiates prefetching

00:11:10,510 --> 00:11:16,270
between the between the host memory and

00:11:12,880 --> 00:11:18,010
the device that way is that what the

00:11:16,270 --> 00:11:21,670
prefetching optimization the other

00:11:18,010 --> 00:11:24,040
problem that comes up with UVM is memory

00:11:21,670 --> 00:11:27,160
over subscription because now you can

00:11:24,040 --> 00:11:28,900
have datasets larger than device memory

00:11:27,160 --> 00:11:30,850
that doesn't necessarily mean that you

00:11:28,900 --> 00:11:32,770
have good promise and the particular

00:11:30,850 --> 00:11:34,480
reason for this especially if you look

00:11:32,770 --> 00:11:36,100
at it in the context of the prefetching

00:11:34,480 --> 00:11:37,930
option is naive prefetching optimization

00:11:36,100 --> 00:11:40,510
is that if you've prefetch data in the

00:11:37,930 --> 00:11:42,640
computer if the data that you're

00:11:40,510 --> 00:11:43,840
prefetching is larger than device memory

00:11:42,640 --> 00:11:45,730
essentially you're going to lead to

00:11:43,840 --> 00:11:47,020
memory crashing you know you don't have

00:11:45,730 --> 00:11:50,290
enough memory and you're prefetching

00:11:47,020 --> 00:11:52,030
more than more than you have so the more

00:11:50,290 --> 00:11:54,270
intelligent approach is actually

00:11:52,030 --> 00:11:57,430
chunk up the kernel into small chunks

00:11:54,270 --> 00:11:58,990
whereas each chunk consumes data that

00:11:57,430 --> 00:12:01,510
actually does fit into memory and then

00:11:58,990 --> 00:12:03,640
you can pipeline these prefetches and

00:12:01,510 --> 00:12:05,350
compute in phases and then you can

00:12:03,640 --> 00:12:07,750
basically split up into these little

00:12:05,350 --> 00:12:11,190
chunks and now everything fits and so

00:12:07,750 --> 00:12:11,190
you have you prevent memory trash

00:12:11,669 --> 00:12:15,579
again I'm gonna show you the workflow on

00:12:13,959 --> 00:12:18,699
the compiler site and the runtime site

00:12:15,579 --> 00:12:20,769
and in the compiler again you bullet the

00:12:18,699 --> 00:12:23,529
source interval of Mir then we have a

00:12:20,769 --> 00:12:26,229
compiler transformation that extracts

00:12:23,529 --> 00:12:27,429
the openmp loop bounds why do we need to

00:12:26,229 --> 00:12:29,649
extract the open up in lieu bounds

00:12:27,429 --> 00:12:31,299
because we're going to chunk the

00:12:29,649 --> 00:12:34,239
attrition space so we need to know what

00:12:31,299 --> 00:12:36,279
you treat space really is then we have

00:12:34,239 --> 00:12:38,829
an elegant transformation to extract

00:12:36,279 --> 00:12:41,049
memory access expressions this is

00:12:38,829 --> 00:12:42,459
required because unless you don't have

00:12:41,049 --> 00:12:44,559
me marry you because you need to chunk

00:12:42,459 --> 00:12:49,629
the data pre edges corresponding to the

00:12:44,559 --> 00:12:51,429
chunked iteration space then as with the

00:12:49,629 --> 00:12:53,559
previous optimization there's a path

00:12:51,429 --> 00:12:55,089
that packs this information up and image

00:12:53,559 --> 00:12:57,159
to transfer an IR it includes this

00:12:55,089 --> 00:12:58,599
memory access expressions and the loop

00:12:57,159 --> 00:13:01,089
bounds and the runtime is then consume

00:12:58,599 --> 00:13:05,169
them essentially at the classroom ir

00:13:01,089 --> 00:13:07,149
spill into the final executable now the

00:13:05,169 --> 00:13:10,269
executable is going to interact with

00:13:07,149 --> 00:13:14,669
toronto and you see right here there's a

00:13:10,269 --> 00:13:16,899
strong 8 cg target run target call which

00:13:14,669 --> 00:13:19,119
starts the offloading of the

00:13:16,899 --> 00:13:20,350
computational kernel and it attracts

00:13:19,119 --> 00:13:22,659
with the liberal if your target runtime

00:13:20,350 --> 00:13:24,489
and now this API call includes the

00:13:22,659 --> 00:13:25,869
region information which information

00:13:24,489 --> 00:13:30,129
being the loop bounds with the memory

00:13:25,869 --> 00:13:31,839
access expressions and then the device

00:13:30,129 --> 00:13:34,869
plugin this is what the device blog

00:13:31,839 --> 00:13:36,879
essentially does is now the the the

00:13:34,869 --> 00:13:40,269
algorithm really looks like you know you

00:13:36,879 --> 00:13:42,609
have on top you have this function that

00:13:40,269 --> 00:13:44,379
computes the amount of chunks that you

00:13:42,609 --> 00:13:45,609
need you want to chunk it so this

00:13:44,379 --> 00:13:47,169
depends on what your device memory

00:13:45,609 --> 00:13:49,479
consumption really is and how much data

00:13:47,169 --> 00:13:51,939
you're your kernel is using depending on

00:13:49,479 --> 00:13:54,909
that it determines how much you need to

00:13:51,939 --> 00:13:57,970
chunk the kernel then is basically loops

00:13:54,909 --> 00:13:59,259
over the total number of chunks copies

00:13:57,970 --> 00:14:02,169
the data that the chunk is going to use

00:13:59,259 --> 00:14:05,679
and then executes that particular chunk

00:14:02,169 --> 00:14:07,359
and then synchronize tasks wakes paid

00:14:05,679 --> 00:14:08,220
for the task to complete before you can

00:14:07,359 --> 00:14:12,120
actually ex

00:14:08,220 --> 00:14:15,360
with the next job now coming to the

00:14:12,120 --> 00:14:17,250
experiments the external section the

00:14:15,360 --> 00:14:19,050
goal of the experiments is to firstly

00:14:17,250 --> 00:14:23,070
understand if these optimizations are

00:14:19,050 --> 00:14:25,890
useful at all the second or later goal

00:14:23,070 --> 00:14:27,870
is to see if the if these optimizations

00:14:25,890 --> 00:14:32,010
scale with larger datasets is the

00:14:27,870 --> 00:14:34,320
benefit scale as a baseline we are going

00:14:32,010 --> 00:14:36,000
to be comparing the HTML pages so we

00:14:34,320 --> 00:14:38,400
just demand paging new optimizations

00:14:36,000 --> 00:14:41,760
enable and comparisons are gonna be

00:14:38,400 --> 00:14:43,470
against prefetching and pipeline

00:14:41,760 --> 00:14:45,630
prefetching where you can't you we're

00:14:43,470 --> 00:14:47,400
gonna be checking the call the next

00:14:45,630 --> 00:14:49,140
walks are using is the simple Sacopee

00:14:47,400 --> 00:14:50,910
kernel that I just show you and we have

00:14:49,140 --> 00:14:52,440
the Kanes program which is taken from

00:14:50,910 --> 00:14:56,610
the Renea suit and adapted with the

00:14:52,440 --> 00:14:58,740
target process the experiment setup

00:14:56,610 --> 00:15:00,210
thanks to Oak Ridge National Lab for

00:14:58,740 --> 00:15:02,790
giving us access to their experimental

00:15:00,210 --> 00:15:06,210
cluster some in depth which has these

00:15:02,790 --> 00:15:08,520
paths for p100 parts with 16gb global

00:15:06,210 --> 00:15:10,680
memory each and they're connected with a

00:15:08,520 --> 00:15:12,720
host within mu link interconnect which

00:15:10,680 --> 00:15:15,540
is significantly H a significantly

00:15:12,720 --> 00:15:18,000
higher bandwidth than PCI Express on the

00:15:15,540 --> 00:15:20,220
software we have your user clang LLVM

00:15:18,000 --> 00:15:22,620
and we use it from the clang by Katie

00:15:20,220 --> 00:15:23,640
branch before the open if you run time

00:15:22,620 --> 00:15:25,860
you have a logo and peel them and be

00:15:23,640 --> 00:15:32,400
target again available with the clackety

00:15:25,860 --> 00:15:35,220
branch so first it is also sexy well on

00:15:32,400 --> 00:15:37,520
the x-axis you see the devices and the

00:15:35,220 --> 00:15:40,410
devices increased from left to right and

00:15:37,520 --> 00:15:43,080
this shows the data size in terms of the

00:15:40,410 --> 00:15:45,780
number of elements on log scale on the

00:15:43,080 --> 00:15:47,700
y-axis you see the time as a percentage

00:15:45,780 --> 00:15:50,070
of the baseline the baseline again being

00:15:47,700 --> 00:15:51,839
demand paging the grace the gray bar

00:15:50,070 --> 00:15:54,150
here is for demand paging the baseline

00:15:51,839 --> 00:15:55,680
and then there's a prefetching and

00:15:54,150 --> 00:15:59,520
pipelined prefetching

00:15:55,680 --> 00:16:01,710
and as you can clearly see that for all

00:15:59,520 --> 00:16:03,360
these all these cases these

00:16:01,710 --> 00:16:04,230
optimizations give give significant

00:16:03,360 --> 00:16:07,290
benefit when use the overhead

00:16:04,230 --> 00:16:09,690
significantly and for interesting for

00:16:07,290 --> 00:16:12,600
this last column the last column is the

00:16:09,690 --> 00:16:13,200
case where your data set size is larger

00:16:12,600 --> 00:16:14,760
than the

00:16:13,200 --> 00:16:16,440
device whirring so you were subscribing

00:16:14,760 --> 00:16:17,880
device number in this case you might

00:16:16,440 --> 00:16:20,010
observe that prefetching doesn't perform

00:16:17,880 --> 00:16:22,470
that well actually because your

00:16:20,010 --> 00:16:25,680
prefetching more than more than the data

00:16:22,470 --> 00:16:27,390
you have and the blue bar actually

00:16:25,680 --> 00:16:29,160
performs when your pipeline performs

00:16:27,390 --> 00:16:30,690
significantly better so there's there's

00:16:29,160 --> 00:16:32,670
a benefit to actually checking up the

00:16:30,690 --> 00:16:36,630
kernel and pipelining prefetches on the

00:16:32,670 --> 00:16:37,590
computer and okay means now if the T

00:16:36,630 --> 00:16:40,350
means I'm going to show two different

00:16:37,590 --> 00:16:43,230
results to give some some insight so

00:16:40,350 --> 00:16:46,440
there this is a k-means for penetration

00:16:43,230 --> 00:16:48,900
again on the x-axis you see different

00:16:46,440 --> 00:16:51,570
the total number of points on log scale

00:16:48,900 --> 00:16:55,140
two points are you trying to cluster and

00:16:51,570 --> 00:16:59,010
on the y-axis you have time as a

00:16:55,140 --> 00:17:01,430
percentage of the baseline and again you

00:16:59,010 --> 00:17:03,510
see that across all these columns

00:17:01,430 --> 00:17:06,360
prefetching and pipelining performs

00:17:03,510 --> 00:17:08,370
reasonably well for the last column the

00:17:06,360 --> 00:17:10,740
one that does not fit in device memory a

00:17:08,370 --> 00:17:12,030
similar observation that Prefect

00:17:10,740 --> 00:17:14,810
pipelining performs better than

00:17:12,030 --> 00:17:16,860
prefetching merely privileging

00:17:14,810 --> 00:17:20,400
particularly because everything fits and

00:17:16,860 --> 00:17:22,530
then you're not tracking memory but this

00:17:20,400 --> 00:17:24,510
is the interesting result so we have

00:17:22,530 --> 00:17:26,400
k-means with 20 attrition so increasing

00:17:24,510 --> 00:17:28,410
the number of iterations for all these

00:17:26,400 --> 00:17:30,180
columns you still see prefetching and

00:17:28,410 --> 00:17:32,670
pipeline in giving performance

00:17:30,180 --> 00:17:34,470
improvements for the last column

00:17:32,670 --> 00:17:37,410
there's your data doesn't fit in device

00:17:34,470 --> 00:17:39,960
memory interestingly demand paging is

00:17:37,410 --> 00:17:43,590
actually performing the best and the

00:17:39,960 --> 00:17:48,360
particularly reasonable reason for this

00:17:43,590 --> 00:17:50,460
is that demand page when you have this

00:17:48,360 --> 00:17:53,550
multiple number of iterations the

00:17:50,460 --> 00:17:56,340
hardware learns to use a more optimal

00:17:53,550 --> 00:17:57,900
strategy and in this particular case

00:17:56,340 --> 00:17:59,760
k-means because you have these multiple

00:17:57,900 --> 00:18:01,920
iterations that a data across all these

00:17:59,760 --> 00:18:04,350
iterations is is really used essentially

00:18:01,920 --> 00:18:07,350
so if the hardware chooses to use an

00:18:04,350 --> 00:18:10,050
optimal eviction policy then the data

00:18:07,350 --> 00:18:11,350
communication is reduced and demand

00:18:10,050 --> 00:18:12,970
paging actually performs better

00:18:11,350 --> 00:18:14,710
the solution to this is using more

00:18:12,970 --> 00:18:15,970
sophisticated pipelining strategies in

00:18:14,710 --> 00:18:21,580
the software and be working on this is

00:18:15,970 --> 00:18:23,890
current current work to summarize the

00:18:21,580 --> 00:18:27,010
whole talk so we developed an opening

00:18:23,890 --> 00:18:29,050
uvm compatible open MP framework that

00:18:27,010 --> 00:18:32,290
works with the GPU let me show that

00:18:29,050 --> 00:18:34,780
performs performs well we've implemented

00:18:32,290 --> 00:18:36,640
these two optimizations particularly to

00:18:34,780 --> 00:18:39,370
reduce the page fault processing

00:18:36,640 --> 00:18:41,650
overhead and to optimization of

00:18:39,370 --> 00:18:43,480
prefetching and pipelining and we see

00:18:41,650 --> 00:18:45,610
that it performs these optimizations

00:18:43,480 --> 00:18:48,310
perform pretty reasonable on our

00:18:45,610 --> 00:18:49,960
benchmarks at future work as I said to

00:18:48,310 --> 00:18:51,730
the previous slide you can be working on

00:18:49,960 --> 00:18:54,430
more sophisticated PI planning

00:18:51,730 --> 00:18:56,940
strategies that enable better

00:18:54,430 --> 00:18:56,940
performance

00:19:07,910 --> 00:19:13,770
so we're using the data map clauses so

00:19:10,980 --> 00:19:15,270
the programmer the assumption here is

00:19:13,770 --> 00:19:17,310
that the programmer is providing you the

00:19:15,270 --> 00:19:20,460
data map causes so if you're using if

00:19:17,310 --> 00:19:22,620
you're using open mp4 accelerators in a

00:19:20,460 --> 00:19:25,080
non UVM setting you would use you have

00:19:22,620 --> 00:19:27,870
to use the data map causes right so the

00:19:25,080 --> 00:19:29,880
argument right here is even though you

00:19:27,870 --> 00:19:31,410
could use the video VM you don't really

00:19:29,880 --> 00:19:32,850
need those causes but having those

00:19:31,410 --> 00:19:35,040
causes actually can significantly

00:19:32,850 --> 00:19:36,780
improve performance because they act as

00:19:35,040 --> 00:19:39,860
a hint to the runtime and it could

00:19:36,780 --> 00:19:39,860
preface that data in advance

00:19:46,880 --> 00:19:51,980
yes but I mean in the sense a bit UVM

00:20:03,330 --> 00:20:06,330
treatment

00:20:14,310 --> 00:20:21,210
that is very useful but there's a lot

00:20:17,370 --> 00:20:21,210
that one

00:20:35,840 --> 00:20:38,470
yeah

00:20:46,250 --> 00:20:49,250
and

00:20:49,820 --> 00:20:52,809
yeah

00:20:53,330 --> 00:20:56,200
yeah

00:21:01,419 --> 00:21:06,639
so essentially we do have so there's

00:21:04,359 --> 00:21:08,320
where the compiler compiler pass came

00:21:06,639 --> 00:21:10,479
into the picture that we do have a

00:21:08,320 --> 00:21:11,950
compiler transformation or you're

00:21:10,479 --> 00:21:13,599
basically profiling the application or

00:21:11,950 --> 00:21:16,659
representative infant seeing if the data

00:21:13,599 --> 00:21:18,459
that is mapped on it has a higher

00:21:16,659 --> 00:21:20,109
likelihood of being accessed if it

00:21:18,459 --> 00:21:22,389
doesn't have a high enough likelihood to

00:21:20,109 --> 00:21:24,249
be accessed we do not copy it what

00:21:22,389 --> 00:21:26,320
probability is high enough to be

00:21:24,249 --> 00:21:27,879
prefetched is it completely Hardware

00:21:26,320 --> 00:21:29,679
dependent you know if you move from one

00:21:27,879 --> 00:21:34,169
interconnect to another interconnect

00:21:29,679 --> 00:21:34,169
this constant will basically change

00:21:34,180 --> 00:21:37,259
[Music]

00:21:45,980 --> 00:21:49,059
that's future

00:21:54,650 --> 00:21:58,880
it sounded ebony

00:21:59,930 --> 00:22:02,770
there's nothing

00:22:06,490 --> 00:22:11,970
my environment my alliance

00:22:16,390 --> 00:22:19,140
by which

00:22:29,110 --> 00:22:34,179
I'm by which technique you mean

00:22:31,730 --> 00:22:36,559
prefetching worse to the demand patient

00:22:34,179 --> 00:22:39,080
so yeah for that purpose we profile the

00:22:36,559 --> 00:22:40,760
application so based on the data access

00:22:39,080 --> 00:22:42,590
probabilities we choose to either

00:22:40,760 --> 00:22:45,320
prefetch or demand page if the data is

00:22:42,590 --> 00:22:46,490
less likely to be access then demand

00:22:45,320 --> 00:22:48,770
paging is actually more useful because

00:22:46,490 --> 00:22:50,240
by prefetching you're copying more data

00:22:48,770 --> 00:22:53,270
than you're using and the data

00:22:50,240 --> 00:22:55,690
communication time dominates and so it's

00:22:53,270 --> 00:22:55,690
not useful

00:23:00,530 --> 00:23:08,800
they administer this Sharonda uniforms

00:23:04,550 --> 00:23:11,900
numbers shows get a hundred percent were

00:23:08,800 --> 00:23:14,570
anticipating mechanisms rituals compared

00:23:11,900 --> 00:23:19,010
to explicit copying because I'm guessing

00:23:14,570 --> 00:23:27,620
the paging is somehow over and questions

00:23:19,010 --> 00:23:29,360
we do common basis was prefetching so if

00:23:27,620 --> 00:23:30,860
we did do comparisons with the pager but

00:23:29,360 --> 00:23:32,360
the old approach right there you have to

00:23:30,860 --> 00:23:34,180
actually just copy the data right either

00:23:32,360 --> 00:23:36,110
and the promise was very comparable the

00:23:34,180 --> 00:23:38,240
prefetching so it didn't really make

00:23:36,110 --> 00:23:39,590
sense to add it but the performance is

00:23:38,240 --> 00:23:42,320
pretty much the same with prefetching so

00:23:39,590 --> 00:23:44,660
UVM is worse than that Union is worse

00:23:42,320 --> 00:23:47,360
than doing a cudamalloc not allocating

00:23:44,660 --> 00:23:50,360
in UVM and then copying it and so that

00:23:47,360 --> 00:23:52,150
is why UVM does beg the question that we

00:23:50,360 --> 00:23:55,240
need to have these optimizations to have

00:23:52,150 --> 00:23:55,240

YouTube URL: https://www.youtube.com/watch?v=LS9ZvFtQQ4Y


