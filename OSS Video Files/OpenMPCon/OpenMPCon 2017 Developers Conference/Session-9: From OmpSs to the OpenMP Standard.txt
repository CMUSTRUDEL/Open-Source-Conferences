Title: Session-9: From OmpSs to the OpenMP Standard
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Xavier Martorell, Sergi Mateo, Xavier Teruel, Josep Maria Perez, VicenÃ§ Beltran, Eduard Ayguade and Jesus Labarta
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day2-Session1-Martorell.pdf
Captions: 
	00:00:01,079 --> 00:00:05,870
okay thank you Michael for an

00:00:03,179 --> 00:00:12,889
introduction so I knew hear me world

00:00:05,870 --> 00:00:18,410
okay so I was planning to use this think

00:00:12,889 --> 00:00:21,180
is that working probably yes okay so

00:00:18,410 --> 00:00:23,190
well I will train a little bit the

00:00:21,180 --> 00:00:26,270
contributions that we have done and in

00:00:23,190 --> 00:00:30,990
fact while preparing the talk I thought

00:00:26,270 --> 00:00:33,960
well that we can go really to the past

00:00:30,990 --> 00:00:35,250
and see how we started right so a little

00:00:33,960 --> 00:00:38,430
bit of history

00:00:35,250 --> 00:00:40,860
I come from UPC I'm affiliated both with

00:00:38,430 --> 00:00:46,220
UPC Technical University of Catalonia

00:00:40,860 --> 00:00:50,760
and piercing UTC was created in 1921

00:00:46,220 --> 00:00:54,030
then until 1977 we didn't had the

00:00:50,760 --> 00:00:56,070
computer science faculty so sweet Lisa

00:00:54,030 --> 00:01:00,120
Coulthard informatica la Barcelona was

00:00:56,070 --> 00:01:02,219
created in 1977 and these days we are

00:01:00,120 --> 00:01:03,120
celebrating the 40 years anniversary

00:01:02,219 --> 00:01:08,880
okay

00:01:03,120 --> 00:01:12,090
a year later Tomas lon came and with

00:01:08,880 --> 00:01:14,459
medieval arrow they found that duck DAC

00:01:12,090 --> 00:01:20,279
the computer architecture department

00:01:14,459 --> 00:01:23,999
where I'm teaching ok in 1991 Matteo

00:01:20,279 --> 00:01:27,240
casues award created the set back this

00:01:23,999 --> 00:01:30,599
is the European Center for parallelism

00:01:27,240 --> 00:01:36,749
in Barcelona it was a center inside UPC

00:01:30,599 --> 00:01:39,990
and interval in the in 2005 we managed

00:01:36,749 --> 00:01:43,740
to join life and life sciences team and

00:01:39,990 --> 00:01:47,399
an earth science system we installed

00:01:43,740 --> 00:01:52,099
Mariano's room 1 ok did what

00:01:47,399 --> 00:01:55,799
the machine was installed in 2004 ok and

00:01:52,099 --> 00:01:57,959
BSC was funded ok DF is this consortium

00:01:55,799 --> 00:02:00,029
it's outside of the university it is

00:01:57,959 --> 00:02:03,670
participated by the university at

00:02:00,029 --> 00:02:07,420
university has 14 or 15 percent

00:02:03,670 --> 00:02:09,490
the Spanish Ministry has 51% and the

00:02:07,420 --> 00:02:11,800
general category has 33 percent

00:02:09,490 --> 00:02:14,680
something like this okay so this is a

00:02:11,800 --> 00:02:16,840
little bit like the story there is more

00:02:14,680 --> 00:02:20,680
story there this past July I was

00:02:16,840 --> 00:02:23,950
attending a JLS meeting in Illinois and

00:02:20,680 --> 00:02:26,710
Bill group was presenting like history

00:02:23,950 --> 00:02:31,390
of parallel computing in Illinois okay

00:02:26,710 --> 00:02:36,130
and then I could read their 1977 they

00:02:31,390 --> 00:02:37,930
become introduced dependency graph okay

00:02:36,130 --> 00:02:39,880
and I think this is like where

00:02:37,930 --> 00:02:41,709
everything started

00:02:39,880 --> 00:02:44,290
and I can I can explain a little bit

00:02:41,709 --> 00:02:47,350
more one of the students one of the PhD

00:02:44,290 --> 00:02:49,420
students of David Cook was a constant

00:02:47,350 --> 00:02:52,270
internal polls okay

00:02:49,420 --> 00:02:54,700
Constantin net material at some point I

00:02:52,270 --> 00:02:57,040
think they really started organization

00:02:54,700 --> 00:02:58,660
of the International Conference on in

00:02:57,040 --> 00:02:59,680
supercomputing the first station was

00:02:58,660 --> 00:03:05,530
done in Athens

00:02:59,680 --> 00:03:09,370
I think constantine proposed nano threat

00:03:05,530 --> 00:03:12,010
okay and the hierarchical task rap okay

00:03:09,370 --> 00:03:16,739
and we took that idea and we started

00:03:12,010 --> 00:03:20,320
there nanos project it happens that nano

00:03:16,739 --> 00:03:21,300
it's like no 10 to the minus 9 or

00:03:20,320 --> 00:03:26,170
something ready

00:03:21,300 --> 00:03:26,980
but in Catalan we say nanos - no

00:03:26,170 --> 00:03:30,640
children

00:03:26,980 --> 00:03:33,820
ok so it fits perfectly so from there we

00:03:30,640 --> 00:03:35,680
say we have the nanos library ok it's

00:03:33,820 --> 00:03:38,620
the library that supports nano

00:03:35,680 --> 00:03:40,720
threatened it's it's still the library

00:03:38,620 --> 00:03:46,290
that currently we use in our ops Brian

00:03:40,720 --> 00:03:49,570
model ok and in fact earlier than that

00:03:46,290 --> 00:03:51,820
1989 Edward and castles were already

00:03:49,570 --> 00:03:53,830
probably seen and if you read the first

00:03:51,820 --> 00:03:55,420
sentence of the abstract in this paper

00:03:53,830 --> 00:03:58,000
we present a new method for extracting

00:03:55,420 --> 00:04:00,610
the maximum parallelism out of do loops

00:03:58,000 --> 00:04:04,600
with tight recurrences in a sequential

00:04:00,610 --> 00:04:07,959
foreign language so personally fit ok so

00:04:04,600 --> 00:04:08,680
a little bit of motivation we like to

00:04:07,959 --> 00:04:11,290
try

00:04:08,680 --> 00:04:13,060
to help programmers to get productivity

00:04:11,290 --> 00:04:14,859
and performance okay so we are proposing

00:04:13,060 --> 00:04:19,509
this test based programming model

00:04:14,859 --> 00:04:21,970
okay Tom's with some basic support for

00:04:19,509 --> 00:04:24,570
tests and then we introduced slowly

00:04:21,970 --> 00:04:27,220
dependencies priority stuff loops okay

00:04:24,570 --> 00:04:29,800
we also did some work do Cavallaro

00:04:27,220 --> 00:04:34,419
currently at Intel did a very good job

00:04:29,800 --> 00:04:36,699
on the Findlay cogeneration we did also

00:04:34,419 --> 00:04:39,850
support for heterogeneous platforms I

00:04:36,699 --> 00:04:41,620
will show you some of the proposals on

00:04:39,850 --> 00:04:44,560
how the environment is at the end of the

00:04:41,620 --> 00:04:48,550
presentation Judit planners in our group

00:04:44,560 --> 00:04:52,590
lead the support for CUDA and she's

00:04:48,550 --> 00:04:55,599
currently in Lausanne I think I'm

00:04:52,590 --> 00:04:58,500
getting a good composition of the

00:04:55,599 --> 00:05:01,000
applications okay so that we can target

00:04:58,500 --> 00:05:02,860
coarse grained parallelism so here the

00:05:01,000 --> 00:05:04,810
idea is let's go for the coarse grained

00:05:02,860 --> 00:05:08,710
parallelism because if you if you go for

00:05:04,810 --> 00:05:10,150
very fine grain you don't really get for

00:05:08,710 --> 00:05:13,080
the performance because of the other set

00:05:10,150 --> 00:05:16,930
of management so coarse grain and also

00:05:13,080 --> 00:05:19,479
what dependent tasks allows is to

00:05:16,930 --> 00:05:22,270
explore in the future what would be the

00:05:19,479 --> 00:05:25,120
parallelism that I can find in this

00:05:22,270 --> 00:05:28,690
application in the future execution okay

00:05:25,120 --> 00:05:32,080
in the future means after three after

00:05:28,690 --> 00:05:36,669
four after ten minutes okay so we have

00:05:32,080 --> 00:05:38,860
this task that is exploring okay without

00:05:36,669 --> 00:05:41,259
having any barrier Sunita's wait it's

00:05:38,860 --> 00:05:43,690
exploring what's there in the future

00:05:41,259 --> 00:05:47,599
parallelism of the application and pops

00:05:43,690 --> 00:05:52,419
new tasks out of those okay

00:05:47,599 --> 00:05:55,939
okay so probably you already know the

00:05:52,419 --> 00:05:59,330
knife cholesky plot that we generate

00:05:55,939 --> 00:06:03,349
okay when we exploit this passing from

00:05:59,330 --> 00:06:06,709
from HTC obligations we also support so

00:06:03,349 --> 00:06:10,879
we so for basically two flavors okay

00:06:06,709 --> 00:06:13,610
this is um this is targeting single node

00:06:10,879 --> 00:06:16,939
small cluster set original machines then

00:06:13,610 --> 00:06:21,469
we have the same kind of approach that's

00:06:16,939 --> 00:06:24,800
based on different big applications for

00:06:21,469 --> 00:06:28,399
the cloud okay we call it constant items

00:06:24,800 --> 00:06:32,389
Rosa Maria Maria at BSC mycolic there is

00:06:28,399 --> 00:06:36,110
a leading this effort modern de on the

00:06:32,389 --> 00:06:37,999
site of cloud where attack is not really

00:06:36,110 --> 00:06:39,919
a piece of code is really big

00:06:37,999 --> 00:06:42,889
application generating some data in a

00:06:39,919 --> 00:06:44,389
file and then that file is a story in

00:06:42,889 --> 00:06:46,429
the cloud and then another application

00:06:44,389 --> 00:06:48,439
that depends on the data is started

00:06:46,429 --> 00:06:50,929
afterwards so here the dependences go

00:06:48,439 --> 00:06:53,089
more on objects and files and we have

00:06:50,929 --> 00:06:57,319
more dependencies on memory know on

00:06:53,089 --> 00:07:00,139
memory locations okay so let's see a

00:06:57,319 --> 00:07:05,779
little bit the influence of ohms on open

00:07:00,139 --> 00:07:09,229
MB and well I have to say that the text

00:07:05,779 --> 00:07:13,129
that I'm showing here he was not done

00:07:09,229 --> 00:07:14,029
Ananse it ohms is relatively new brand

00:07:13,129 --> 00:07:16,550
okay

00:07:14,029 --> 00:07:19,459
so initially we have nanos source I was

00:07:16,550 --> 00:07:22,490
explaining we have paraphrased to our

00:07:19,459 --> 00:07:26,509
compiler okay and we never I don't think

00:07:22,490 --> 00:07:30,439
we ever join it the two terms until 2008

00:07:26,509 --> 00:07:33,289
Navy probably with the development of

00:07:30,439 --> 00:07:37,519
tasks where we where we said well this

00:07:33,289 --> 00:07:40,069
is like a trouble okay so we managed the

00:07:37,519 --> 00:07:41,360
compiler and runtime system in a single

00:07:40,069 --> 00:07:46,030
concept

00:07:41,360 --> 00:07:49,580
the model is status s which means

00:07:46,030 --> 00:07:54,050
execution of tasks in a superscalar way

00:07:49,580 --> 00:07:59,530
and for the openmp single node a taraji

00:07:54,050 --> 00:08:04,520
news machines we call OpenMP superscalar

00:07:59,530 --> 00:08:07,639
in beef okay so this is not really ohms

00:08:04,520 --> 00:08:12,159
but I can mention that we started with

00:08:07,639 --> 00:08:15,440
the nanos project in 1996 at that time

00:08:12,159 --> 00:08:18,469
we had some contacts with a parallel

00:08:15,440 --> 00:08:20,409
computing for forum in Porton okay and I

00:08:18,469 --> 00:08:25,900
understand that the West that was the

00:08:20,409 --> 00:08:29,000
initial flavor of openmp okay we got the

00:08:25,900 --> 00:08:32,019
nanos project funded we developed the

00:08:29,000 --> 00:08:35,649
nanos library we published that in even

00:08:32,019 --> 00:08:39,169
1999 at that time we had three workshops

00:08:35,649 --> 00:08:43,010
related to open MD one in the States one

00:08:39,169 --> 00:08:47,450
in Europe one in Japan okay d1 and wampa

00:08:43,010 --> 00:08:49,370
are the European and American I don't

00:08:47,450 --> 00:08:53,510
remember the name of that of the

00:08:49,370 --> 00:08:56,180
Japanese version of e1

00:08:53,510 --> 00:08:59,209
so we developed support for multiple

00:08:56,180 --> 00:09:03,200
levels of parallelism we developed

00:08:59,209 --> 00:09:05,480
numerous support because in the project

00:09:03,200 --> 00:09:07,550
we had relations with the University of

00:09:05,480 --> 00:09:10,430
Patras from where Constantine was

00:09:07,550 --> 00:09:13,190
originally okay limitation of Nicola

00:09:10,430 --> 00:09:16,149
Poulos did a very nice job on doing this

00:09:13,190 --> 00:09:19,520
Dumas report that was done initially on

00:09:16,149 --> 00:09:22,430
Silicon Graphics machines with our with

00:09:19,520 --> 00:09:26,449
meets our 10,000 or 12,000 okay

00:09:22,430 --> 00:09:29,540
processors or old architectures those

00:09:26,449 --> 00:09:32,240
are currently in our museums okay with

00:09:29,540 --> 00:09:33,860
it also a mark on planet one of my

00:09:32,240 --> 00:09:36,980
colleagues in Barcelona did these

00:09:33,860 --> 00:09:37,760
pipeline it executions okay so in loops

00:09:36,980 --> 00:09:39,680
do

00:09:37,760 --> 00:09:43,160
kind of pipeline support like now we

00:09:39,680 --> 00:09:45,320
have the do across okay an electron did

00:09:43,160 --> 00:09:48,520
this proposal on the outer scheduling

00:09:45,320 --> 00:09:53,540
close for loops okay so this is like

00:09:48,520 --> 00:09:56,360
prey oh okay and then we we did in fact

00:09:53,540 --> 00:09:58,310
Alex do Dan did the proposal for tacking

00:09:56,360 --> 00:10:00,920
and then we did proposal for tough

00:09:58,310 --> 00:10:03,890
dependences and also the does loop and

00:10:00,920 --> 00:10:04,690
top priorities okay and something is

00:10:03,890 --> 00:10:07,820
missing here

00:10:04,690 --> 00:10:10,280
that's interesting because I realized

00:10:07,820 --> 00:10:14,120
that while we were doing the proportion

00:10:10,280 --> 00:10:18,020
of tasking we also were developing the

00:10:14,120 --> 00:10:22,790
error handling in open MP with a

00:10:18,020 --> 00:10:24,440
publication and afterwards and that

00:10:22,790 --> 00:10:28,640
should be written here I don't know what

00:10:24,440 --> 00:10:31,370
what it is not afterwards cancellation

00:10:28,640 --> 00:10:35,120
was introduced okay in in the openmp

00:10:31,370 --> 00:10:37,430
standard also okay and then well we are

00:10:35,120 --> 00:10:40,550
proposing that reductions dependences on

00:10:37,430 --> 00:10:44,810
tasks wait do we participated a lot on

00:10:40,550 --> 00:10:47,210
the om PT implementation and then

00:10:44,810 --> 00:10:50,450
current trends are multi dependences

00:10:47,210 --> 00:10:52,160
commutative dependent centers loops and

00:10:50,450 --> 00:10:55,990
things like that okay so i will go a

00:10:52,160 --> 00:10:58,220
little bit on this so let's examine the

00:10:55,990 --> 00:11:00,890
like the tasking model know the big

00:10:58,220 --> 00:11:04,280
change was to introduce the task concept

00:11:00,890 --> 00:11:06,260
okay so that was introduced in 3.0 and

00:11:04,280 --> 00:11:07,730
we allow it already nesting of tasks

00:11:06,260 --> 00:11:12,050
okay and then we have the third wait

00:11:07,730 --> 00:11:15,950
where the attack can wait for their

00:11:12,050 --> 00:11:17,980
children okay that in opening 33.1 we

00:11:15,950 --> 00:11:21,530
introduce the final in order to say well

00:11:17,980 --> 00:11:23,510
this task is can be organized so that we

00:11:21,530 --> 00:11:26,990
know that it will not generate further

00:11:23,510 --> 00:11:30,140
time okay so it's also implement this

00:11:26,990 --> 00:11:31,010
cutoff mechanism in order to avoid the

00:11:30,140 --> 00:11:35,300
creation of

00:11:31,010 --> 00:11:38,050
to find rain tax okay so then we also

00:11:35,300 --> 00:11:40,330
added the priority okay

00:11:38,050 --> 00:11:43,420
for the price tough priorities okay so

00:11:40,330 --> 00:11:46,060
these are those to exploit irregular

00:11:43,420 --> 00:11:48,220
parallelism from obligations that are

00:11:46,060 --> 00:11:51,280
based on different data structures like

00:11:48,220 --> 00:11:53,920
trees there are released so whatever and

00:11:51,280 --> 00:11:59,620
also supports recursion some of the

00:11:53,920 --> 00:12:02,800
experiments that we do with sub sorry so

00:11:59,620 --> 00:12:06,010
in the impasse dependences okay so what

00:12:02,800 --> 00:12:10,450
we do is we express the directionality

00:12:06,010 --> 00:12:12,820
of the data okay so the actual

00:12:10,450 --> 00:12:15,340
dependences between the tasks in fact

00:12:12,820 --> 00:12:17,200
are not expressed by the programmer and

00:12:15,340 --> 00:12:20,920
I think this is good so the browser only

00:12:17,200 --> 00:12:22,630
says this task touches this data uses

00:12:20,920 --> 00:12:24,480
this data okay and then the runtime

00:12:22,630 --> 00:12:26,860
system compute what are the actual

00:12:24,480 --> 00:12:28,630
dependencies between the tasks which are

00:12:26,860 --> 00:12:31,360
otherwise it will be very difficult to

00:12:28,630 --> 00:12:33,970
relate all tasks having have in mind all

00:12:31,360 --> 00:12:34,690
tasks in the mind of the parameter is

00:12:33,970 --> 00:12:38,500
complex

00:12:34,690 --> 00:12:41,140
ok so then that our activities as soon

00:12:38,500 --> 00:12:44,260
as their dependencies are satisfied and

00:12:41,140 --> 00:12:46,300
there is resources okay if the cores are

00:12:44,260 --> 00:12:48,610
busy we exactly what what's being

00:12:46,300 --> 00:12:51,520
executed there and we have a really

00:12:48,610 --> 00:12:55,900
really - so that that get accumulated

00:12:51,520 --> 00:13:00,090
there ok so these were introduced in

00:12:55,900 --> 00:13:02,950
open NT 4.0 with pin out and in out

00:13:00,090 --> 00:13:05,410
specification ok to comment about

00:13:02,950 --> 00:13:09,490
priorities some of the experiments that

00:13:05,410 --> 00:13:12,330
we do are for instance try to tune a

00:13:09,490 --> 00:13:14,770
little bit some of the properties that

00:13:12,330 --> 00:13:18,130
the concept that we introduced in open

00:13:14,770 --> 00:13:22,630
NP into so for instance in priorities we

00:13:18,130 --> 00:13:27,280
evaluated if we provide an integer for

00:13:22,630 --> 00:13:29,080
the value of the priority what would be

00:13:27,280 --> 00:13:32,740
the performance that we get in this case

00:13:29,080 --> 00:13:34,420
from mel short okay so that as soon as

00:13:32,740 --> 00:13:37,790
we reduce the number of bits of this

00:13:34,420 --> 00:13:42,140
integer if we only use 16 bits

00:13:37,790 --> 00:13:43,970
of like a short value in order to

00:13:42,140 --> 00:13:46,220
represent the priority the performance

00:13:43,970 --> 00:13:48,650
doesn't change but as soon as we only

00:13:46,220 --> 00:13:51,320
use a bit the performance really drops

00:13:48,650 --> 00:13:53,690
because we are not able to express the

00:13:51,320 --> 00:13:56,300
full set of priorities that the

00:13:53,690 --> 00:13:57,260
application would like to to to to have

00:13:56,300 --> 00:14:00,230
okay

00:13:57,260 --> 00:14:02,690
and with no priority that it's even less

00:14:00,230 --> 00:14:04,760
performant achieved okay so this is one

00:14:02,690 --> 00:14:07,370
of the things that we do and then we can

00:14:04,760 --> 00:14:10,580
recommend well it would be good at

00:14:07,370 --> 00:14:13,790
atleast a priority could support a range

00:14:10,580 --> 00:14:14,950
of 16-bit values okay that would be

00:14:13,790 --> 00:14:18,860
reasonable

00:14:14,950 --> 00:14:22,280
so we also participated in the inclusion

00:14:18,860 --> 00:14:26,150
of tough loops so this is like chunking

00:14:22,280 --> 00:14:31,760
so when you have tasks most of the times

00:14:26,150 --> 00:14:34,400
you dedicate a lot of effort to change

00:14:31,760 --> 00:14:36,050
the application show you any loop that

00:14:34,400 --> 00:14:38,540
you want to paralyze with that do with

00:14:36,050 --> 00:14:42,080
tasks you need to split the loop in two

00:14:38,540 --> 00:14:44,360
loops and use tasking in-between okay so

00:14:42,080 --> 00:14:46,580
this is in fact what TAS loop is doing

00:14:44,360 --> 00:14:50,860
automatically for you so it's a kind of

00:14:46,580 --> 00:14:54,590
productivity for the purana okay so here

00:14:50,860 --> 00:14:58,340
we can say how much how many tasks I

00:14:54,590 --> 00:15:00,770
want this look to exploit or I think

00:14:58,340 --> 00:15:04,450
it's better to just think on which grain

00:15:00,770 --> 00:15:08,150
size no should have the block and

00:15:04,450 --> 00:15:13,850
currently has loop dependences are still

00:15:08,150 --> 00:15:16,700
are in the in discussion for 5.0 okay so

00:15:13,850 --> 00:15:19,520
we use the password functionality in

00:15:16,700 --> 00:15:21,200
order to propose reductions the support

00:15:19,520 --> 00:15:25,940
for reductions is based on touch group

00:15:21,200 --> 00:15:28,670
okay so that will open the scope for the

00:15:25,940 --> 00:15:31,550
reduction and then path can just

00:15:28,670 --> 00:15:34,010
participate in the reduction okay it

00:15:31,550 --> 00:15:35,930
would be also and yesterday it was

00:15:34,010 --> 00:15:38,350
commented know having reductions in

00:15:35,930 --> 00:15:43,900
touch loop so it is something that it's

00:15:38,350 --> 00:15:49,900
altering the plants okay one of our

00:15:43,900 --> 00:15:53,560
proposals in fact is trying to relax

00:15:49,900 --> 00:15:56,170
some of the dependences specifically

00:15:53,560 --> 00:15:58,630
what we call an in/out chain okay of

00:15:56,170 --> 00:16:01,930
dependences and in our chain is like I

00:15:58,630 --> 00:16:04,420
have a set of tasks generating L of I

00:16:01,930 --> 00:16:08,020
and then I have another set of tasks

00:16:04,420 --> 00:16:10,750
that depend on this alibi and they

00:16:08,020 --> 00:16:13,330
happen in out on a particular value okay

00:16:10,750 --> 00:16:16,450
so this means that this T 2 that will be

00:16:13,330 --> 00:16:21,850
serialized in this in our chain that you

00:16:16,450 --> 00:16:25,110
see here okay so though those structures

00:16:21,850 --> 00:16:28,390
are common in in applications okay so we

00:16:25,110 --> 00:16:32,230
suggested to relax that like using

00:16:28,390 --> 00:16:34,390
commodity okay which means well in fact

00:16:32,230 --> 00:16:37,630
it's an inner chain but you can reorder

00:16:34,390 --> 00:16:40,630
the execution of the ducts okay so as

00:16:37,630 --> 00:16:45,480
soon as one task is ready to be executed

00:16:40,630 --> 00:16:45,480
it can be executed regardless of their

00:16:45,630 --> 00:16:52,630
relative creation time okay so here you

00:16:48,970 --> 00:16:56,080
see the difference either they go in the

00:16:52,630 --> 00:16:58,660
serial order or they go as soon as

00:16:56,080 --> 00:17:01,750
possible okay and the property that they

00:16:58,660 --> 00:17:04,510
should accomplish is that the operation

00:17:01,750 --> 00:17:08,470
they do on this x value is commutative

00:17:04,510 --> 00:17:12,850
okay so it can be reordered okay this is

00:17:08,470 --> 00:17:14,500
currently being proposed I think and we

00:17:12,850 --> 00:17:17,530
have another approach which is the

00:17:14,500 --> 00:17:19,930
concurrent so you can say well on result

00:17:17,530 --> 00:17:23,560
and I'm trying to use conqueror and then

00:17:19,930 --> 00:17:26,140
what the approach here is the parameter

00:17:23,560 --> 00:17:28,780
is responsible of the synchronization on

00:17:26,140 --> 00:17:30,400
that shared variable because it has in

00:17:28,780 --> 00:17:32,890
fact in concurrent will run in parallel

00:17:30,400 --> 00:17:35,890
okay as you can see here all tasks run

00:17:32,890 --> 00:17:40,060
in parallel and then the red line marks

00:17:35,890 --> 00:17:41,890
like the atomic or critical that you

00:17:40,060 --> 00:17:42,790
need in order to accumulate in this case

00:17:41,890 --> 00:17:46,380
the reserve

00:17:42,790 --> 00:17:49,960
okay I think this is probably more for

00:17:46,380 --> 00:17:54,600
kind of production support okay so it

00:17:49,960 --> 00:17:57,610
may be it may not be necessary if

00:17:54,600 --> 00:18:00,970
reduction supports the full duffle thing

00:17:57,610 --> 00:18:03,490
okay then we can also evaluate the

00:18:00,970 --> 00:18:07,390
different performance of the two

00:18:03,490 --> 00:18:10,210
approaches so this is showing by the way

00:18:07,390 --> 00:18:13,299
our tools are integrated with the

00:18:10,210 --> 00:18:16,360
parallel visualizer and analysis

00:18:13,299 --> 00:18:19,120
analysis tool that we have also BC so

00:18:16,360 --> 00:18:22,059
this is a timeline visualization on the

00:18:19,120 --> 00:18:25,059
x-axis of the activity of the number of

00:18:22,059 --> 00:18:28,299
threads that we have here okay so this

00:18:25,059 --> 00:18:30,970
is the implementation of conquering and

00:18:28,299 --> 00:18:33,970
this is the implementation of community

00:18:30,970 --> 00:18:36,460
okay so what you see here is incongruent

00:18:33,970 --> 00:18:39,190
all tasks run in parallel okay and then

00:18:36,460 --> 00:18:43,120
there is some synchronization in inside

00:18:39,190 --> 00:18:44,890
the tasks here you see that as never run

00:18:43,120 --> 00:18:46,600
in parallel but they can run on

00:18:44,890 --> 00:18:49,270
different courts of course okay because

00:18:46,600 --> 00:18:52,780
they are just scheduled randomly in the

00:18:49,270 --> 00:18:54,850
first score that gets the task okay so

00:18:52,780 --> 00:18:58,330
you see that in this case the

00:18:54,850 --> 00:19:01,000
commutative is penalizing because the

00:18:58,330 --> 00:19:04,150
serial execution is longer than the than

00:19:01,000 --> 00:19:05,950
the concurrent okay that there is

00:19:04,150 --> 00:19:08,350
another property that you can study

00:19:05,950 --> 00:19:10,660
which is what is the quality of the

00:19:08,350 --> 00:19:13,870
execution of those tasks okay and here

00:19:10,660 --> 00:19:16,510
you see that this this window here shows

00:19:13,870 --> 00:19:18,460
the duration this this time that I'm

00:19:16,510 --> 00:19:22,200
marking here is the duration of the

00:19:18,460 --> 00:19:25,570
tasks the color that you see here is

00:19:22,200 --> 00:19:28,390
light green means there is a small

00:19:25,570 --> 00:19:31,929
number of tasks with this duration dark

00:19:28,390 --> 00:19:34,780
blue which is right there right here on

00:19:31,929 --> 00:19:37,870
top of the pointer these are blue means

00:19:34,780 --> 00:19:40,390
most of the tasks have this duration so

00:19:37,870 --> 00:19:42,950
what you see here is that nearly all the

00:19:40,390 --> 00:19:46,070
tasks have the same duration okay

00:19:42,950 --> 00:19:49,700
and the duration is short compared to

00:19:46,070 --> 00:19:51,859
this execution here where there is more

00:19:49,700 --> 00:19:53,299
variability on the duration and the

00:19:51,859 --> 00:19:56,869
duration is released

00:19:53,299 --> 00:19:58,909
like doable and you can see here in fact

00:19:56,869 --> 00:20:01,249
that the resistance between the arrow

00:19:58,909 --> 00:20:04,340
the the difference between the green

00:20:01,249 --> 00:20:09,230
flat that you have in the execution is

00:20:04,340 --> 00:20:11,749
really much larger than the execution of

00:20:09,230 --> 00:20:13,850
single task here they should be the code

00:20:11,749 --> 00:20:16,850
is really very equivalent they should be

00:20:13,850 --> 00:20:18,470
the hint of the same duration but what

00:20:16,850 --> 00:20:21,109
happens here is that you have the atomic

00:20:18,470 --> 00:20:23,539
or the critical the probably that was

00:20:21,109 --> 00:20:25,970
implemented with an atomic okay and that

00:20:23,539 --> 00:20:29,570
synchronization makes the execution time

00:20:25,970 --> 00:20:32,149
a little what considerably longer for

00:20:29,570 --> 00:20:34,570
photo for all the tasks okay so this is

00:20:32,149 --> 00:20:37,070
the kind of a study that we can do

00:20:34,570 --> 00:20:39,470
another proposal that we are doing is

00:20:37,070 --> 00:20:43,369
the multi dependences so what happens

00:20:39,470 --> 00:20:46,249
here is sometimes applications may have

00:20:43,369 --> 00:20:47,210
a variable number of dependents for the

00:20:46,249 --> 00:20:52,159
tax okay

00:20:47,210 --> 00:20:54,289
and if that happens you cannot write the

00:20:52,159 --> 00:20:56,899
code with the current approach

00:20:54,289 --> 00:21:00,590
why because you would need to write now

00:20:56,899 --> 00:21:03,379
seven dependencies now only five now

00:21:00,590 --> 00:21:06,139
only one okay so you would need to do a

00:21:03,379 --> 00:21:09,049
case order on an order set of if

00:21:06,139 --> 00:21:11,629
statements if line dependence number is

00:21:09,049 --> 00:21:14,090
seven then I put the prop directive with

00:21:11,629 --> 00:21:16,850
the seven dependents if it's five I put

00:21:14,090 --> 00:21:20,119
the file okay so here we are proposing

00:21:16,850 --> 00:21:24,679
two things first to introduce the

00:21:20,119 --> 00:21:26,419
iterators okay which is well we should

00:21:24,679 --> 00:21:29,179
depend on something that is variable

00:21:26,419 --> 00:21:34,039
okay in this case is the length of the

00:21:29,179 --> 00:21:36,980
list okay so these tasks that are being

00:21:34,039 --> 00:21:39,889
executed here has an in/out on LY and

00:21:36,980 --> 00:21:42,590
then I want to execute the full loop

00:21:39,889 --> 00:21:46,100
here taking into account all the

00:21:42,590 --> 00:21:48,090
different in outs not so then all these

00:21:46,100 --> 00:21:52,169
dependences will go from Tiwa

00:21:48,090 --> 00:21:55,909
from the set of tasks of t1 to t2 okay

00:21:52,169 --> 00:21:57,659
observe that this is in some way also

00:21:55,909 --> 00:22:01,559
introducing a little bit of pressure

00:21:57,659 --> 00:22:04,770
okay because I'm doing many-to-one

00:22:01,559 --> 00:22:07,140
okay it's like overcoming the the fact

00:22:04,770 --> 00:22:11,220
that we cannot have different regions in

00:22:07,140 --> 00:22:14,010
in the memory in the dependence for for

00:22:11,220 --> 00:22:17,640
OpenMP okay so in this case it would be

00:22:14,010 --> 00:22:24,710
equivalent to right depend in on l0 l1

00:22:17,640 --> 00:22:30,600
and so on until L from today L type okay

00:22:24,710 --> 00:22:33,960
so this can be applied for instance to

00:22:30,600 --> 00:22:36,690
applications that have like essential

00:22:33,960 --> 00:22:39,539
computations okay so here depending on

00:22:36,690 --> 00:22:41,880
where is the position of the block the

00:22:39,539 --> 00:22:43,679
green block that I want to compute if

00:22:41,880 --> 00:22:47,370
it's in the corner I have only four

00:22:43,679 --> 00:22:50,039
dependences if it's in the boundary but

00:22:47,370 --> 00:22:52,409
not in a corner then i have nine six

00:22:50,039 --> 00:22:54,960
dependences it is in the center i have

00:22:52,409 --> 00:22:58,549
no independence okay so this is what

00:22:54,960 --> 00:23:04,110
this code would do okay and this is the

00:22:58,549 --> 00:23:05,850
execution that we get from a name like

00:23:04,110 --> 00:23:08,039
implementation of the application in

00:23:05,850 --> 00:23:11,010
fact this is slowly to nine because I

00:23:08,039 --> 00:23:14,760
can see warriors here so it's probably

00:23:11,010 --> 00:23:19,980
not using tasking but I mean it's a way

00:23:14,760 --> 00:23:21,659
of showing that if you just a name a

00:23:19,980 --> 00:23:28,260
notation of the application you get

00:23:21,659 --> 00:23:32,190
really very large imbalance of the work

00:23:28,260 --> 00:23:34,380
okay and this is the same approach but

00:23:32,190 --> 00:23:37,080
using multi dependences well you can see

00:23:34,380 --> 00:23:38,650
that we are really solving the issue of

00:23:37,080 --> 00:23:40,570
the

00:23:38,650 --> 00:23:43,120
of the imbalance because we are breaking

00:23:40,570 --> 00:23:46,960
a little bit more we are splitting this

00:23:43,120 --> 00:23:49,929
this execution on these these pieces and

00:23:46,960 --> 00:23:53,380
they get very well balanced in the in

00:23:49,929 --> 00:23:55,480
the different course also here the

00:23:53,380 --> 00:23:57,340
execution time that it takes one of the

00:23:55,480 --> 00:24:00,460
iterations of the application is this

00:23:57,340 --> 00:24:02,710
site this site up to here okay

00:24:00,460 --> 00:24:05,110
while the same iteration in the preview

00:24:02,710 --> 00:24:08,440
in the in the name annotation is really

00:24:05,110 --> 00:24:11,830
longer okay so you gain also performance

00:24:08,440 --> 00:24:14,429
here another set of applications that

00:24:11,830 --> 00:24:20,980
can be annotated is applications like

00:24:14,429 --> 00:24:24,760
this is a fem application and so you

00:24:20,980 --> 00:24:27,220
need to compute on this grid but you

00:24:24,760 --> 00:24:30,880
need to take care of which nodes depend

00:24:27,220 --> 00:24:33,520
on which one okay so usually in these

00:24:30,880 --> 00:24:37,350
kind of applications kind of coloring is

00:24:33,520 --> 00:24:40,240
used so if you color with the same color

00:24:37,350 --> 00:24:41,559
the nodes that do not depend on each

00:24:40,240 --> 00:24:42,340
other then you can have a set of

00:24:41,559 --> 00:24:46,059
parallel loops

00:24:42,340 --> 00:24:49,960
okay so first our loop would work on the

00:24:46,059 --> 00:24:53,200
red ones yellow blue green whatever okay

00:24:49,960 --> 00:24:56,380
and you can try to do also a kind of

00:24:53,200 --> 00:25:00,460
grouping using the multi dependences

00:24:56,380 --> 00:25:02,590
okay and then here we mix multi

00:25:00,460 --> 00:25:05,110
dependences with commutative okay so

00:25:02,590 --> 00:25:08,649
that as soon as one of the blocks is

00:25:05,110 --> 00:25:12,070
ready to start we fire the task that is

00:25:08,649 --> 00:25:14,620
exhibiting that block okay and we can

00:25:12,070 --> 00:25:19,149
annotate that using commutative and

00:25:14,620 --> 00:25:22,330
using the range of the multi dependences

00:25:19,149 --> 00:25:24,909
inside the community dependence okay and

00:25:22,330 --> 00:25:25,929
this implements a kind of graph like

00:25:24,909 --> 00:25:27,940
this okay

00:25:25,929 --> 00:25:30,190
and we compare the performance that we

00:25:27,940 --> 00:25:35,010
got in this application with multi

00:25:30,190 --> 00:25:37,930
dependences even we with the NPI on

00:25:35,010 --> 00:25:41,620
annotated so written with applications

00:25:37,930 --> 00:25:46,570
written in MPI okay so this is 512

00:25:41,620 --> 00:25:50,020
processes MPI processes and at that

00:25:46,570 --> 00:25:54,430
range we would have 64 or 128 mb a

00:25:50,020 --> 00:25:55,930
processes with eight or four threads and

00:25:54,430 --> 00:25:58,600
you can see that the performance that we

00:25:55,930 --> 00:26:01,480
have here is even lower than what we get

00:25:58,600 --> 00:26:03,850
from NPR okay so we we got some benefits

00:26:01,480 --> 00:26:06,970
of doing this

00:26:03,850 --> 00:26:11,740
commutative plus mu dependences support

00:26:06,970 --> 00:26:14,040
okay and one of the latest developments

00:26:11,740 --> 00:26:17,370
that we are doing is the weak dependence

00:26:14,040 --> 00:26:21,310
okay this is in order to support better

00:26:17,370 --> 00:26:24,280
nest attack okay so when you have single

00:26:21,310 --> 00:26:25,870
level tasks you build a graph which is

00:26:24,280 --> 00:26:28,870
really fine during synchronization

00:26:25,870 --> 00:26:31,000
between the different tasks no the

00:26:28,870 --> 00:26:34,630
dependences really can go fine-grained

00:26:31,000 --> 00:26:37,060
pass by that sometimes you are

00:26:34,630 --> 00:26:41,920
interested in having two levels of

00:26:37,060 --> 00:26:45,360
parallelism and in this case the the box

00:26:41,920 --> 00:26:49,630
that is enclosing the outer level okay

00:26:45,360 --> 00:26:52,570
and dollar low done a low to express

00:26:49,630 --> 00:26:54,970
some fine grain dependences because the

00:26:52,570 --> 00:26:57,280
dependences go from the outer level task

00:26:54,970 --> 00:26:59,380
to the alcohol test and from the inner

00:26:57,280 --> 00:27:02,260
level task to the inner level test not

00:26:59,380 --> 00:27:04,450
across so here the approach is well let

00:27:02,260 --> 00:27:08,380
a low dependences to cross these

00:27:04,450 --> 00:27:11,710
boundaries okay so here like a little

00:27:08,380 --> 00:27:14,280
bit of advantages and inconvenience no

00:27:11,710 --> 00:27:17,200
in single level path you have fine grain

00:27:14,280 --> 00:27:19,480
synchronization okay less runtime

00:27:17,200 --> 00:27:21,220
overhead anesthetist you are creating

00:27:19,480 --> 00:27:23,260
more tasks because you need to create

00:27:21,220 --> 00:27:25,960
the outer level okay so you have a

00:27:23,260 --> 00:27:27,790
little bit more overhead of creation but

00:27:25,960 --> 00:27:30,850
it's more like top-down try and model

00:27:27,790 --> 00:27:33,760
you can do a step by step okay you first

00:27:30,850 --> 00:27:37,210
go on the course rain and then go to the

00:27:33,760 --> 00:27:38,710
fine grain okay you can even search for

00:27:37,210 --> 00:27:40,800
distant coarse grained parallelism

00:27:38,710 --> 00:27:44,160
because the master thread will go

00:27:40,800 --> 00:27:46,559
and continues without any - wait okay

00:27:44,160 --> 00:27:48,210
but you have this path wait here okay

00:27:46,559 --> 00:27:51,330
and it also introduces of a pet and

00:27:48,210 --> 00:27:54,150
limit the available parallelism okay

00:27:51,330 --> 00:27:57,360
so what we propose is to introduce weak

00:27:54,150 --> 00:28:01,290
dependences okay in which even if the

00:27:57,360 --> 00:28:05,520
outer tasks ask we are created the

00:28:01,290 --> 00:28:10,200
running system manages to build that the

00:28:05,520 --> 00:28:12,450
single level tasks run okay and we are

00:28:10,200 --> 00:28:14,070
currently doing experiments on that and

00:28:12,450 --> 00:28:18,630
we are finding some applications that

00:28:14,070 --> 00:28:24,300
are also going well okay now I would

00:28:18,630 --> 00:28:29,700
like to ask how I'm regarding time 10

00:28:24,300 --> 00:28:34,790
minutes perfect so I have tried to give

00:28:29,700 --> 00:28:37,380
you a view of what we do to transfer our

00:28:34,790 --> 00:28:39,870
proposals to openmp okay but then I

00:28:37,380 --> 00:28:41,670
thought well I think it's also

00:28:39,870 --> 00:28:45,510
interesting to see what the current

00:28:41,670 --> 00:28:48,090
trends within on okay so let me explain

00:28:45,510 --> 00:28:51,660
a little bit what we do currently in

00:28:48,090 --> 00:28:57,690
ohms and I will start so showing this a

00:28:51,660 --> 00:29:01,380
schema of our own tool chain when we

00:28:57,690 --> 00:29:05,040
when we want to offload work to the GPU

00:29:01,380 --> 00:29:06,809
okay so specifically for poodle okay so

00:29:05,040 --> 00:29:09,059
we get the on this application which is

00:29:06,809 --> 00:29:11,910
which has some portions written in CUDA

00:29:09,059 --> 00:29:15,300
in this case we need to divide the CUDA

00:29:11,910 --> 00:29:19,429
code already inside kernels that are

00:29:15,300 --> 00:29:23,220
compiled with NVIDIA compiler okay so

00:29:19,429 --> 00:29:26,730
what Mercurian the compiler in this case

00:29:23,220 --> 00:29:28,950
does is its outlines some specific stuff

00:29:26,730 --> 00:29:32,700
okay these are stab Buddha files are

00:29:28,950 --> 00:29:34,650
generated by Miriam and that those files

00:29:32,700 --> 00:29:37,920
are also compiled with NVIDIA compiler

00:29:34,650 --> 00:29:39,240
then we embed everything as nvdrs on a

00:29:37,920 --> 00:29:41,130
single binary and we can

00:29:39,240 --> 00:29:43,970
right okay everything is linked at also

00:29:41,130 --> 00:29:47,850
with nanos and the horse code is just

00:29:43,970 --> 00:29:51,240
transform it by mercurial so we can have

00:29:47,850 --> 00:29:53,340
also parallelism in the course so you

00:29:51,240 --> 00:29:56,130
got you get parallelism in the code just

00:29:53,340 --> 00:29:58,080
by Holmes directives and parallelism on

00:29:56,130 --> 00:29:58,740
the GPU because you have the the food

00:29:58,080 --> 00:30:02,340
equipment

00:29:58,740 --> 00:30:04,860
okay so the idea is that at execution

00:30:02,340 --> 00:30:06,809
time we have a set of threads that are

00:30:04,860 --> 00:30:10,380
running on the course and usually have

00:30:06,809 --> 00:30:13,290
one representative for GPU okay so tasks

00:30:10,380 --> 00:30:16,670
can be annotated with target device GPU

00:30:13,290 --> 00:30:21,270
and then those tasks in this case the

00:30:16,670 --> 00:30:24,960
bio list or discolor tasks go to the vet

00:30:21,270 --> 00:30:29,179
and the tasks W tasks go to the course

00:30:24,960 --> 00:30:31,860
okay so this allows us to provide

00:30:29,179 --> 00:30:36,300
different implementations of the same

00:30:31,860 --> 00:30:39,600
algorithm either for the CPUs or for the

00:30:36,300 --> 00:30:43,230
GPU we do this in this way so here you

00:30:39,600 --> 00:30:46,050
see you have the target devices and T

00:30:43,230 --> 00:30:50,010
version of a scaled us which is doing a

00:30:46,050 --> 00:30:52,890
simple application simple operation on a

00:30:50,010 --> 00:30:55,800
pair of vectors in fact incrementation

00:30:52,890 --> 00:30:57,840
here okay and then we have the target

00:30:55,800 --> 00:31:02,160
device CUDA implements a scale test

00:30:57,840 --> 00:31:05,520
which is the kernel for the CUDA realize

00:31:02,160 --> 00:31:09,660
okay and as soon as we say that this

00:31:05,520 --> 00:31:12,000
task implements the former the S&T

00:31:09,660 --> 00:31:15,120
version the runtime system note that we

00:31:12,000 --> 00:31:17,820
it has two versions that are at that and

00:31:15,120 --> 00:31:21,900
that that can be selected to be executed

00:31:17,820 --> 00:31:24,270
given availability of resources given

00:31:21,900 --> 00:31:26,910
predicted performance on the different

00:31:24,270 --> 00:31:29,340
resources okay and in fact we have this

00:31:26,910 --> 00:31:31,590
versioning the scheduling policy in

00:31:29,340 --> 00:31:35,100
which the runtime system first evaluates

00:31:31,590 --> 00:31:38,080
the execution of scales does in SMP for

00:31:35,100 --> 00:31:41,710
instance is it executes three times

00:31:38,080 --> 00:31:45,280
the colonel in the S&T determines

00:31:41,710 --> 00:31:47,710
performance it executes the scale tasks

00:31:45,280 --> 00:31:49,660
for cuda cuda device in the GPU again

00:31:47,710 --> 00:31:52,240
three times it determines performance

00:31:49,660 --> 00:31:53,440
and then it selects afterwards which one

00:31:52,240 --> 00:31:55,150
is best

00:31:53,440 --> 00:31:57,760
okay so this would be the implementation

00:31:55,150 --> 00:32:02,260
of the CUDA kernel and then at this

00:31:57,760 --> 00:32:04,870
point the scales us is just involved in

00:32:02,260 --> 00:32:07,150
this way so either the SMT version or

00:32:04,870 --> 00:32:09,220
the CUDA version depending on the on the

00:32:07,150 --> 00:32:11,890
scheduling policy at work okay so here

00:32:09,220 --> 00:32:15,880
the tasks in fact is this orange path

00:32:11,890 --> 00:32:19,150
okay which can go in fact either to

00:32:15,880 --> 00:32:23,470
occur or to the GPU okay so we also have

00:32:19,150 --> 00:32:26,580
the version for OpenGL very similar

00:32:23,470 --> 00:32:31,450
except that instead of compiling the

00:32:26,580 --> 00:32:34,870
OpenCL kernel at compile time we compile

00:32:31,450 --> 00:32:36,700
the OpenCL kernel at execution time okay

00:32:34,870 --> 00:32:38,170
so the open field compiler is invoked

00:32:36,700 --> 00:32:41,040
automatically you don't need to do

00:32:38,170 --> 00:32:44,559
anything you only provide the term okay

00:32:41,040 --> 00:32:45,340
and we have also a version for the FPGA

00:32:44,559 --> 00:32:50,380
okay

00:32:45,340 --> 00:32:53,110
and here I guess I can move I want to

00:32:50,380 --> 00:32:59,910
show you one of the developments that we

00:32:53,110 --> 00:33:04,110
are doing so this is the result of the

00:32:59,910 --> 00:33:07,420
European project axiom okay this work is

00:33:04,110 --> 00:33:11,790
developed by the Italian company second

00:33:07,420 --> 00:33:14,800
okay and it contains this chip is the

00:33:11,790 --> 00:33:19,840
link ultra scale plan okay

00:33:14,800 --> 00:33:22,620
so it contains four arm for 64 bits and

00:33:19,840 --> 00:33:26,620
one officiate okay and we managed to

00:33:22,620 --> 00:33:29,500
offload tasks using ohms on the FPGA of

00:33:26,620 --> 00:33:31,840
this chip the ARM cores can also execute

00:33:29,500 --> 00:33:32,749
part of the application of an SMP device

00:33:31,840 --> 00:33:35,210
okay

00:33:32,749 --> 00:33:37,929
in fact I have the full system running

00:33:35,210 --> 00:33:41,090
so if at some point somebody has

00:33:37,929 --> 00:33:44,570
curiosity I can I can give you a small

00:33:41,090 --> 00:33:48,379
demo how this works okay so here what we

00:33:44,570 --> 00:33:49,970
do is instead of using Nvidia or OpenGL

00:33:48,379 --> 00:33:53,450
compiler we use the revival the

00:33:49,970 --> 00:33:56,299
challenge Bovada tool okay so we also

00:33:53,450 --> 00:34:00,320
generate a small stub in order to set

00:33:56,299 --> 00:34:02,720
the interface between the horse coat and

00:34:00,320 --> 00:34:05,359
the the bitstream that is being

00:34:02,720 --> 00:34:08,629
generated for the fpga and it is just

00:34:05,359 --> 00:34:11,389
typing make okay so I can have this kind

00:34:08,629 --> 00:34:14,000
of code which is this is matrix

00:34:11,389 --> 00:34:18,020
multiplication okay we annotate that

00:34:14,000 --> 00:34:20,899
with the shillings hls directives like

00:34:18,020 --> 00:34:23,539
in line at a partition pipeline in order

00:34:20,899 --> 00:34:25,460
for the HLS tools to optimize the

00:34:23,539 --> 00:34:29,179
generation of the resources in the bit

00:34:25,460 --> 00:34:32,149
stream we currently target 32-bit and

00:34:29,179 --> 00:34:34,609
64-bit platforms this one in the middle

00:34:32,149 --> 00:34:37,460
is the one that I was showing and we

00:34:34,609 --> 00:34:40,760
also plan to to to work for discrete

00:34:37,460 --> 00:34:43,399
FTAs okay and then this matrix multiply

00:34:40,760 --> 00:34:46,369
function is annotated with on so in the

00:34:43,399 --> 00:34:48,710
same way device of TJ I can say here I

00:34:46,369 --> 00:34:50,720
want this to the device zero and I want

00:34:48,710 --> 00:34:53,240
one instance and this means that I can

00:34:50,720 --> 00:34:56,839
have here a bit stream configured in the

00:34:53,240 --> 00:34:59,510
steering and I have one multiplication

00:34:56,839 --> 00:35:02,569
running on a tie at a time or I can just

00:34:59,510 --> 00:35:05,690
change that to I want this to be device

00:35:02,569 --> 00:35:09,799
zero with two instances then I have two

00:35:05,690 --> 00:35:11,809
bits three to I P course in the FPGA on

00:35:09,799 --> 00:35:13,609
the same beach frame and then I can run

00:35:11,809 --> 00:35:17,089
two models multiplications at the same

00:35:13,609 --> 00:35:19,579
time okay one thing that we also have

00:35:17,089 --> 00:35:22,490
developed is instrumentation from inside

00:35:19,579 --> 00:35:24,890
the apiary okay so this is the execution

00:35:22,490 --> 00:35:28,010
of tasks in the FPGA and you can see

00:35:24,890 --> 00:35:32,150
here the submission events that the FPGA

00:35:28,010 --> 00:35:34,160
is receiving so new - new - new dust the

00:35:32,150 --> 00:35:36,050
input channel how the data is

00:35:34,160 --> 00:35:38,810
transferred to the

00:35:36,050 --> 00:35:41,690
da the execution of the particular

00:35:38,810 --> 00:35:43,940
computation matrix-multiply and the

00:35:41,690 --> 00:35:46,160
output of the data back to the heart

00:35:43,940 --> 00:35:48,410
okay so you can see this fine grain

00:35:46,160 --> 00:35:50,630
instrumentation which are in by the way

00:35:48,410 --> 00:35:53,930
very useful and a little bit of

00:35:50,630 --> 00:35:57,530
evaluation this has been just done in

00:35:53,930 --> 00:35:59,080
the latest date I'm sure is here

00:35:57,530 --> 00:36:02,060
devaluation of matrix multiplication

00:35:59,080 --> 00:36:05,960
this is running on the four threads of

00:36:02,060 --> 00:36:11,600
the four cores of this board okay so arm

00:36:05,960 --> 00:36:12,980
course we are getting like 2.5 to 11

00:36:11,600 --> 00:36:16,880
 flops out of the matrix

00:36:12,980 --> 00:36:20,030
multiplication then well we have

00:36:16,880 --> 00:36:23,450
currently two bitstream generated one

00:36:20,030 --> 00:36:28,760
bit stream for the FPGA is of size 128

00:36:23,450 --> 00:36:32,240
by 128 running 100 megahertz okay so you

00:36:28,760 --> 00:36:34,640
see here that the FPGA is like between

00:36:32,240 --> 00:36:37,460
one and two quarters in this case okay

00:36:34,640 --> 00:36:40,070
and then here I'm using the implement

00:36:37,460 --> 00:36:42,290
approach where I can also use the

00:36:40,070 --> 00:36:45,080
parallelism in the course at the same

00:36:42,290 --> 00:36:50,750
time as in the FPGA and we we get a

00:36:45,080 --> 00:36:54,160
little bit more performance then we have

00:36:50,750 --> 00:36:57,770
digit for different metric sizes this is

00:36:54,160 --> 00:37:00,590
1024 and this is 2048 matrix eyes so

00:36:57,770 --> 00:37:04,610
they indicate mostly the same then when

00:37:00,590 --> 00:37:08,020
we move to a block size double size 256

00:37:04,610 --> 00:37:12,010
by 256 and also doubling the mega at the

00:37:08,020 --> 00:37:15,230
frequency in the FPGA we have a very big

00:37:12,010 --> 00:37:18,740
increase in performance we respect a

00:37:15,230 --> 00:37:19,700
single core okay and even surpassing

00:37:18,740 --> 00:37:24,700
four cores

00:37:19,700 --> 00:37:28,490
so here the FPGA alone is already giving

00:37:24,700 --> 00:37:32,390
14.90 flops I think when we increase the

00:37:28,490 --> 00:37:32,970
matrix size we get 16.5 gigaflops out of

00:37:32,390 --> 00:37:35,430
the feet

00:37:32,970 --> 00:37:39,420
and also the implements technique health

00:37:35,430 --> 00:37:42,780
so the course can still make some help

00:37:39,420 --> 00:37:44,460
and get better performance up to twenty

00:37:42,780 --> 00:37:47,940
forty Lots okay

00:37:44,460 --> 00:37:50,490
on on this on this world and I think

00:37:47,940 --> 00:37:53,280
here the lesson is well this implement

00:37:50,490 --> 00:37:59,180
in this particular case where I would

00:37:53,280 --> 00:38:02,130
say the power of the FPGA is a little

00:37:59,180 --> 00:38:05,580
better than the than the performance

00:38:02,130 --> 00:38:08,730
educating the in the arm course okay

00:38:05,580 --> 00:38:11,609
the implements greatly helped okay

00:38:08,730 --> 00:38:15,300
because the implements in GPUs when the

00:38:11,609 --> 00:38:18,270
GPU is so fast then the the course

00:38:15,300 --> 00:38:22,140
really help just a little bit okay no

00:38:18,270 --> 00:38:24,119
not really much okay okay so with this I

00:38:22,140 --> 00:38:26,369
would like to present the conclusions so

00:38:24,119 --> 00:38:29,580
I have shown a little bit our experience

00:38:26,369 --> 00:38:31,770
on our programming how we try to enable

00:38:29,580 --> 00:38:35,190
protein medium probability supporting

00:38:31,770 --> 00:38:38,430
additional architectures okay our work

00:38:35,190 --> 00:38:41,700
is publicly reliable at the brain models

00:38:38,430 --> 00:38:43,440
website and we will keep working with

00:38:41,700 --> 00:38:45,780
you in the alternative standards so

00:38:43,440 --> 00:38:48,089
supporting this kind of different

00:38:45,780 --> 00:38:52,580
techniques that we are discussing on the

00:38:48,089 --> 00:38:52,580
face to face okay so thank you

00:38:53,420 --> 00:38:59,060
[Music]

00:38:56,180 --> 00:39:02,510
any questions

00:38:59,060 --> 00:39:02,510
we have fun

00:39:05,700 --> 00:39:14,590
[Music]

00:39:12,210 --> 00:39:18,120
how

00:39:14,590 --> 00:39:18,120
you love that

00:39:21,240 --> 00:39:26,470
like

00:39:24,080 --> 00:39:27,580
[Music]

00:39:26,470 --> 00:39:29,839
something

00:39:27,580 --> 00:39:32,410
[Music]

00:39:29,839 --> 00:39:32,410
Prime

00:39:33,950 --> 00:39:37,450
you can gee

00:39:39,070 --> 00:39:44,380
yeah so we have looked at that and I

00:39:42,310 --> 00:39:47,260
would say we don't have a conclusion

00:39:44,380 --> 00:39:50,170
because we looked at that as scheduling

00:39:47,260 --> 00:39:52,600
policies so when we have an AMA note we

00:39:50,170 --> 00:39:54,670
try to escape to use a skill scheduling

00:39:52,600 --> 00:39:56,800
policy that takes into account that okay

00:39:54,670 --> 00:39:59,890
but really I don't have a conclusion

00:39:56,800 --> 00:40:03,490
there at this point okay sometimes when

00:39:59,890 --> 00:40:06,600
you do just the plain breadth-first the

00:40:03,490 --> 00:40:11,100
scheduling policy is good enough okay

00:40:06,600 --> 00:40:11,100
because that micro

00:40:16,520 --> 00:40:24,270
[Music]

00:40:19,410 --> 00:40:28,819
a transparent language is there any kind

00:40:24,270 --> 00:40:28,819
of from the user perspective

00:40:31,119 --> 00:40:39,169
yeah so I would say initially he will

00:40:36,609 --> 00:40:43,939
like see it transparently because the

00:40:39,169 --> 00:40:46,939
product will run okay I would say that

00:40:43,939 --> 00:40:49,789
he doesn't need to do anything for the

00:40:46,939 --> 00:40:52,279
course the only thing that he needs to

00:40:49,789 --> 00:40:55,640
do is when you have more GPUs you need

00:40:52,279 --> 00:40:58,069
to tell the system the Nano runtime you

00:40:55,640 --> 00:41:01,969
have for GPS okay and then the

00:40:58,069 --> 00:41:04,719
scheduling is done by Namor's then the

00:41:01,969 --> 00:41:07,789
next step would be let's try to use the

00:41:04,719 --> 00:41:09,979
socket scheduling policy and see if this

00:41:07,789 --> 00:41:12,789
can get a little bit more of performance

00:41:09,979 --> 00:41:17,269
but I don't have a solution for that

00:41:12,789 --> 00:41:19,910
okay so it's still unclear how we can

00:41:17,269 --> 00:41:23,929
map tasks to the to to the different

00:41:19,910 --> 00:41:26,059
sockets in a an efficient way

00:41:23,929 --> 00:41:28,339
one option would be and we also

00:41:26,059 --> 00:41:31,910
developed a little bit of this that

00:41:28,339 --> 00:41:34,039
following the dependences really follow

00:41:31,910 --> 00:41:36,650
the data so this task should be a

00:41:34,039 --> 00:41:39,319
schedule in this node because depends on

00:41:36,650 --> 00:41:43,939
data that is generated in that node okay

00:41:39,319 --> 00:41:45,949
but I don't have an application or an

00:41:43,939 --> 00:41:49,659
implementation that shows good results

00:41:45,949 --> 00:41:49,659
in there in that case okay

00:41:51,840 --> 00:41:54,740
all right

00:41:57,349 --> 00:41:59,979
yep

00:42:00,820 --> 00:42:03,450
yes

00:42:04,609 --> 00:42:09,319
while America is really sure what is the

00:42:07,359 --> 00:42:12,559
discussion on this level but I think

00:42:09,319 --> 00:42:15,549
it's a model and we will try to have it

00:42:12,559 --> 00:42:15,549
for five agreed

00:42:19,130 --> 00:42:22,390
we're open question

00:42:24,610 --> 00:42:28,840

YouTube URL: https://www.youtube.com/watch?v=51SHNaPv_ck


