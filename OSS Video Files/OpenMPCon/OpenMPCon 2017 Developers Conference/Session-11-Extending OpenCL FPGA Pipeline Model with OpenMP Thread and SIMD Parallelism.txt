Title: Session-11-Extending OpenCL FPGA Pipeline Model with OpenMP Thread and SIMD Parallelism
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Xinmin Tian, Hideki Saito, Satish Guggilla, Elena Demikhovsky, Matt Masten, Diego Caballero, Ernesto Su, Jin Lin and Andrew Savonichev
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day2-Session2-Su.pdf
Captions: 
	00:00:00,030 --> 00:00:02,810
yes

00:00:09,230 --> 00:00:12,989
[Music]

00:00:24,430 --> 00:00:42,310
[Music]

00:00:26,239 --> 00:00:44,390
issues a touch in yes yes yes they think

00:00:42,310 --> 00:00:49,410
[Music]

00:00:44,390 --> 00:00:52,050
thank you for you okay so this is a big

00:00:49,410 --> 00:00:56,399
project that many people participated it

00:00:52,050 --> 00:00:58,800
was led by Jimmy and he tried to come by

00:00:56,399 --> 00:01:03,170
and unfortunately he couldn't the last

00:00:58,800 --> 00:01:03,170
minute so I'm giving this presentation

00:01:03,710 --> 00:01:13,220
so let me try to motivate this a little

00:01:07,979 --> 00:01:18,570
bit by skipping to distract yes okay so

00:01:13,220 --> 00:01:23,610
a different group at Intel who owns FPGA

00:01:18,570 --> 00:01:25,649
and tools askers earth meaning the Intel

00:01:23,610 --> 00:01:27,920
compiler group to help with this

00:01:25,649 --> 00:01:33,180
challenge and what they have is this

00:01:27,920 --> 00:01:36,840
they have autonomous driving suite of

00:01:33,180 --> 00:01:41,549
applications we can in OpenCL targeted

00:01:36,840 --> 00:01:46,109
for FPGA and so the problem they face is

00:01:41,549 --> 00:01:50,460
that the development cycle is is very

00:01:46,109 --> 00:01:53,280
long meaning that it takes a long time

00:01:50,460 --> 00:01:55,530
to compile so they they they change one

00:01:53,280 --> 00:01:58,020
or two lines in the code and we take a

00:01:55,530 --> 00:02:00,659
very long time to compile for FPGA

00:01:58,020 --> 00:02:05,070
because or the nature of our program in

00:02:00,659 --> 00:02:08,460
FPGA that is very time-consuming so they

00:02:05,070 --> 00:02:11,060
want to be more productive for their

00:02:08,460 --> 00:02:11,060
developers

00:02:11,350 --> 00:02:21,190
so they have well if they target our CDU

00:02:18,660 --> 00:02:26,710
then the compilation with not Max Factor

00:02:21,190 --> 00:02:28,930
the problem is that a direct from port

00:02:26,710 --> 00:02:31,450
or maybe ninety importance just

00:02:28,930 --> 00:02:36,430
retargeting of the original application

00:02:31,450 --> 00:02:44,400
we can open CEO to run on the CPU which

00:02:36,430 --> 00:02:49,080
would be this second arrow that path is

00:02:44,400 --> 00:02:52,590
created code that runs very slowly so

00:02:49,080 --> 00:02:56,650
the the original code that runs on FPGA

00:02:52,590 --> 00:02:59,890
is maybe fifty years or hundreds faster

00:02:56,650 --> 00:03:02,830
than their emulation on the CPU so that

00:02:59,890 --> 00:03:05,190
is again not acceptable so they want

00:03:02,830 --> 00:03:12,400
something that is free to compile but

00:03:05,190 --> 00:03:15,010
also resulting in an elation that takes

00:03:12,400 --> 00:03:17,980
advantage of the CPU architectures

00:03:15,010 --> 00:03:23,680
overall the phrase or vector unit so

00:03:17,980 --> 00:03:29,580
that the the emulation runs reasonably

00:03:23,680 --> 00:03:34,690
fast so the constraints they they they

00:03:29,580 --> 00:03:40,510
the challenge basis is that it's okay to

00:03:34,690 --> 00:03:45,970
be maybe 5 X or maybe even 10 X the the

00:03:40,510 --> 00:03:48,610
in terms of the runtime over the real AP

00:03:45,970 --> 00:03:51,280
year run that 50 X or hundred is the no

00:03:48,610 --> 00:03:55,239
no ok so that's the first thing then

00:03:51,280 --> 00:03:57,670
another thing we can change the compiler

00:03:55,239 --> 00:04:00,510
we can change the tool or any the

00:03:57,670 --> 00:04:05,410
application but we're not allowed to

00:04:00,510 --> 00:04:09,280
make very big changes so we they're not

00:04:05,410 --> 00:04:12,610
okay with us writing if defin and we

00:04:09,280 --> 00:04:15,730
optimizing the entire of routines that

00:04:12,610 --> 00:04:17,919
doesn't know so of

00:04:15,730 --> 00:04:20,400
we have to do this by maybe

00:04:17,919 --> 00:04:24,550
instrumenting there

00:04:20,400 --> 00:04:28,120
they're the original open sealed program

00:04:24,550 --> 00:04:30,220
with with pragmas direct exact kind of

00:04:28,120 --> 00:04:33,550
things the data so came forth with them

00:04:30,220 --> 00:04:35,980
and also we were given just a few months

00:04:33,550 --> 00:04:40,660
to work on this so that was the

00:04:35,980 --> 00:04:47,500
challenge and with that let me go back

00:04:40,660 --> 00:04:53,350
to the peers right okay so most of this

00:04:47,500 --> 00:04:58,030
talk would be about how we change this

00:04:53,350 --> 00:04:59,380
this OpenCL applications using open

00:04:58,030 --> 00:05:01,540
Peaks so given the constraints are

00:04:59,380 --> 00:05:06,610
attributes mentioned open if it came out

00:05:01,540 --> 00:05:09,370
to be kind of the ideal method to

00:05:06,610 --> 00:05:12,010
approach this problem because it doesn't

00:05:09,370 --> 00:05:18,330
require growth changes to original

00:05:12,010 --> 00:05:21,160
application and it could give us the eye

00:05:18,330 --> 00:05:23,470
you could give us the opportunity to

00:05:21,160 --> 00:05:26,590
take advantage of the CPU architectures

00:05:23,470 --> 00:05:30,729
by using of enemies colonization

00:05:26,590 --> 00:05:33,520
vectorization and tasking and those

00:05:30,729 --> 00:05:36,669
those needs so most of the talk will be

00:05:33,520 --> 00:05:39,820
about how we change the application and

00:05:36,669 --> 00:05:43,360
then part of the talk a smaller portion

00:05:39,820 --> 00:05:47,410
will be about how we extended the

00:05:43,360 --> 00:05:50,979
original open Co compiler with do to

00:05:47,410 --> 00:05:55,630
give this the capability to to produce

00:05:50,979 --> 00:05:57,900
openmp and it's that's a smaller portion

00:05:55,630 --> 00:06:02,789
in this presentation but we have a paper

00:05:57,900 --> 00:06:08,950
percentage of the lob msec meetings in

00:06:02,789 --> 00:06:10,240
that in that aspect so we if you're

00:06:08,950 --> 00:06:15,310
interested you can

00:06:10,240 --> 00:06:18,669
look at that paper later and then the

00:06:15,310 --> 00:06:22,509
last part of this presentation is we're

00:06:18,669 --> 00:06:25,810
going to look at a real case history of

00:06:22,509 --> 00:06:30,729
this this application and see the actual

00:06:25,810 --> 00:06:38,380
result okay so first let's look at how

00:06:30,729 --> 00:06:40,509
we change this applications the to 2

00:06:38,380 --> 00:06:43,210
meters challenge to take advantage of

00:06:40,509 --> 00:06:48,789
the threading and the best appreciation

00:06:43,210 --> 00:06:50,770
in the FPGA application so the

00:06:48,789 --> 00:06:54,910
application looks like this is basically

00:06:50,770 --> 00:06:57,789
a pipeline of n stages and each in each

00:06:54,910 --> 00:07:01,479
stage of the pipeline and I'm looking at

00:06:57,789 --> 00:07:03,970
the upper diagram in interpretation of

00:07:01,479 --> 00:07:09,699
Hana you have a single thread working on

00:07:03,970 --> 00:07:14,020
some some kernel and each each stages of

00:07:09,699 --> 00:07:19,000
pipeline will use it what they call a

00:07:14,020 --> 00:07:21,610
channel to to pass the data to the next

00:07:19,000 --> 00:07:23,289
stage and it's very fine-grain so each

00:07:21,610 --> 00:07:25,270
each they are you can think of as a

00:07:23,289 --> 00:07:30,430
scalar or maybe just a thick soil and so

00:07:25,270 --> 00:07:34,690
forth so what we want to do is for

00:07:30,430 --> 00:07:37,750
because this n stages the Emmys never

00:07:34,690 --> 00:07:43,650
will be and it is definitely much

00:07:37,750 --> 00:07:47,560
smaller than the number of cores or

00:07:43,650 --> 00:07:52,000
logical threads that say a skylight node

00:07:47,560 --> 00:07:54,190
has so it's not even with paralyzed with

00:07:52,000 --> 00:07:58,000
with this pipeline that you won't get

00:07:54,190 --> 00:08:02,080
much out of our view so what we want is

00:07:58,000 --> 00:08:04,180
to each person depending which kernel

00:08:02,080 --> 00:08:07,660
will first identify the ones that are

00:08:04,180 --> 00:08:10,750
more like the bottlenecks and some

00:08:07,660 --> 00:08:12,790
of them will be vectorized like this one

00:08:10,750 --> 00:08:20,670
other right

00:08:12,790 --> 00:08:23,230
some will be some will be paralyzed and

00:08:20,670 --> 00:08:26,200
then some will be both paralyzed and

00:08:23,230 --> 00:08:30,010
veterans and some will be left untouched

00:08:26,200 --> 00:08:34,750
but all of them you see that the channel

00:08:30,010 --> 00:08:39,490
the pipeline these channels will have to

00:08:34,750 --> 00:08:44,470
be extended to the more model time is

00:08:39,490 --> 00:08:46,960
basically the vector length so it's now

00:08:44,470 --> 00:08:55,300
passing instead of scalar is cutting

00:08:46,960 --> 00:09:00,630
vectors from stage to stage so the first

00:08:55,300 --> 00:09:03,790
thing we did is to choose a subset of

00:09:00,630 --> 00:09:08,500
OpenMP that was needed for this work

00:09:03,790 --> 00:09:13,840
based on finalizing the stages and see

00:09:08,500 --> 00:09:17,140
what could best take advantage of author

00:09:13,840 --> 00:09:19,660
architecture so these are basically the

00:09:17,140 --> 00:09:25,690
directives and the runtime functions

00:09:19,660 --> 00:09:26,920
that we saw we needed to implement on on

00:09:25,690 --> 00:09:29,770
this open field

00:09:26,920 --> 00:09:32,280
compiler and then with that

00:09:29,770 --> 00:09:37,000
implementation then we can instrument

00:09:32,280 --> 00:09:41,500
the application with these directives

00:09:37,000 --> 00:09:46,320
and cloth so this enough to be the

00:09:41,500 --> 00:09:48,760
substrate that we chose to implement and

00:09:46,320 --> 00:09:51,130
so you see that we have this tradition

00:09:48,760 --> 00:09:54,210
of work sharing parallel four and so

00:09:51,130 --> 00:10:00,120
forth we also have

00:09:54,210 --> 00:10:04,170
more than newer directives like the

00:10:00,120 --> 00:10:06,990
Cindy's and the tassels clauses are

00:10:04,170 --> 00:10:09,720
pretty standard data sharing closes

00:10:06,990 --> 00:10:11,420
always nothing fancy and a few of the

00:10:09,720 --> 00:10:12,830
vectorization process

00:10:11,420 --> 00:10:19,370
[Music]

00:10:12,830 --> 00:10:22,380
this is just a simple way to say that

00:10:19,370 --> 00:10:24,000
the box the grip of the variety is

00:10:22,380 --> 00:10:27,330
basically a pipeline stage and and

00:10:24,000 --> 00:10:31,860
masters thread is your the original only

00:10:27,330 --> 00:10:36,480
thread that did the kernel and then by

00:10:31,860 --> 00:10:41,130
adding openmp we can in this diagram

00:10:36,480 --> 00:10:43,110
we're representing parallelism next part

00:10:41,130 --> 00:10:45,270
isn't even right you have three and then

00:10:43,110 --> 00:10:48,480
you have nine and then back to three and

00:10:45,270 --> 00:10:51,720
so forth so this is just representation

00:10:48,480 --> 00:10:53,970
of what happens with thread and as I

00:10:51,720 --> 00:10:57,660
said original number of stages in the

00:10:53,970 --> 00:11:00,390
pipeline water is maximum much smaller

00:10:57,660 --> 00:11:03,600
than the actual threat it's that you

00:11:00,390 --> 00:11:05,610
have in college no so the many threats

00:11:03,600 --> 00:11:08,930
lying around that we're not in you so

00:11:05,610 --> 00:11:12,680
this is what we want to use them for

00:11:08,930 --> 00:11:16,200
okay and then so this is an example of

00:11:12,680 --> 00:11:20,000
what what we do to a kernel for example

00:11:16,200 --> 00:11:23,430
the the first line here is an open field

00:11:20,000 --> 00:11:25,680
line that will help to use just one

00:11:23,430 --> 00:11:29,130
thread so that doubt me the master

00:11:25,680 --> 00:11:33,120
thread for us then this function is an

00:11:29,130 --> 00:11:36,780
example question is a dot product so all

00:11:33,120 --> 00:11:41,790
we add is is open only parallel force in

00:11:36,780 --> 00:11:46,280
the reduction to computed upper right

00:11:41,790 --> 00:11:49,170
and then so the first

00:11:46,280 --> 00:11:51,420
the top diagram here is basically

00:11:49,170 --> 00:11:56,520
represents the number of iterations of

00:11:51,420 --> 00:12:00,230
these of this loop and so the the

00:11:56,520 --> 00:12:03,510
parallel for will partition the

00:12:00,230 --> 00:12:06,750
iteration space in multiple threads and

00:12:03,510 --> 00:12:09,600
then the Cindy will factorize each

00:12:06,750 --> 00:12:14,280
thread there so this is just a

00:12:09,600 --> 00:12:18,410
high-level representation and so in this

00:12:14,280 --> 00:12:23,490
way we can take advantage of Kylie's

00:12:18,410 --> 00:12:30,840
architectural features and hopefully

00:12:23,490 --> 00:12:36,960
provide our speed up so okay so to

00:12:30,840 --> 00:12:42,300
vectorize loop this is basically use

00:12:36,960 --> 00:12:46,530
your standard on PMD and the process

00:12:42,300 --> 00:12:54,930
that we've used with use the single aim

00:12:46,530 --> 00:12:57,330
safe name the linear process of

00:12:54,930 --> 00:13:02,850
reductions and also the Clause to

00:12:57,330 --> 00:13:07,560
specify alignment so typical process

00:13:02,850 --> 00:13:10,200
that you use when you using D and here

00:13:07,560 --> 00:13:15,720
is an example of say helping with how do

00:13:10,200 --> 00:13:18,300
instrument this loop that has backward

00:13:15,720 --> 00:13:27,060
dependence so you see a is depending on

00:13:18,300 --> 00:13:32,700
time minus M so basically while what we

00:13:27,060 --> 00:13:36,300
know is if say basically this is a face

00:13:32,700 --> 00:13:39,140
if you the dependence the safe length

00:13:36,300 --> 00:13:41,400
that you choose for the vector is

00:13:39,140 --> 00:13:42,800
smaller than the dependence distance so

00:13:41,400 --> 00:13:48,679
the dependence distance here

00:13:42,800 --> 00:13:51,860
em so if you know that n is 617 or

00:13:48,679 --> 00:13:56,119
larger then 16 other safe language is

00:13:51,860 --> 00:13:58,420
okay so you can do this to vectorize so

00:13:56,119 --> 00:14:00,399
you don't need to be blocked by this

00:13:58,420 --> 00:14:05,379
backward dependence

00:14:00,399 --> 00:14:05,379
you can still vectorize this safely and

00:14:05,709 --> 00:14:14,869
the another thing we needed to do is if

00:14:11,389 --> 00:14:18,410
you look at this loop you have function

00:14:14,869 --> 00:14:26,019
calls in it you have a min and you have

00:14:18,410 --> 00:14:30,769
this Q so when we say drama for Cindy

00:14:26,019 --> 00:14:34,819
you need to also be clear these routines

00:14:30,769 --> 00:14:36,649
as with each other simply so that the

00:14:34,819 --> 00:14:41,509
compiler can automatically generate a

00:14:36,649 --> 00:14:44,569
vector version of those so here the mean

00:14:41,509 --> 00:14:50,029
and the disc you you have to add the

00:14:44,569 --> 00:14:53,119
break curve above the clear Cindy so the

00:14:50,029 --> 00:14:56,240
compiler will omit the mean vector

00:14:53,119 --> 00:15:01,069
version and a disk you vector version so

00:14:56,240 --> 00:15:03,459
the actual code for this loop then you

00:15:01,069 --> 00:15:06,170
have a vector D equals and then the

00:15:03,459 --> 00:15:10,779
vector versions of these functions are

00:15:06,170 --> 00:15:13,670
called okay so that's what will happen

00:15:10,779 --> 00:15:16,399
but will be automatically done by the

00:15:13,670 --> 00:15:19,639
compiler if you do those instrumented

00:15:16,399 --> 00:15:21,799
code and then this is what a function

00:15:19,639 --> 00:15:25,429
vectorization basically the decreasingly

00:15:21,799 --> 00:15:30,350
what what it will do so when you say

00:15:25,429 --> 00:15:33,999
decreasing B to say scalar proof scalar

00:15:30,350 --> 00:15:33,999
foo has some

00:15:35,240 --> 00:15:43,410
in here it say each scalable will

00:15:38,730 --> 00:15:46,050
produce one value right so in the the

00:15:43,410 --> 00:15:52,019
clear simply what they will do is say

00:15:46,050 --> 00:15:56,639
you're generating a vector for float so

00:15:52,019 --> 00:16:01,529
in here it will pack for calls to s 2

00:15:56,639 --> 00:16:03,959
and we return the result in a vector so

00:16:01,529 --> 00:16:08,160
this this is the this diagram basically

00:16:03,959 --> 00:16:12,149
represents path so you back through will

00:16:08,160 --> 00:16:15,019
return four floats each time so that

00:16:12,149 --> 00:16:15,019
would be perfect

00:16:17,420 --> 00:16:28,559
now if you have math library calls such

00:16:23,939 --> 00:16:32,339
as sine cosine and the floors max mean

00:16:28,559 --> 00:16:36,240
that those those are things in do

00:16:32,339 --> 00:16:39,209
provide the SML small vector math

00:16:36,240 --> 00:16:45,319
libraries that has a vector version so

00:16:39,209 --> 00:16:50,009
it's the transformation in a case is

00:16:45,319 --> 00:16:56,850
much we the compiler doesn't have to

00:16:50,009 --> 00:17:04,300
recreate the the packing is of para into

00:16:56,850 --> 00:17:07,540
a vector but just hope just call the

00:17:04,300 --> 00:17:11,640
the vector versions of it in the library

00:17:07,540 --> 00:17:16,870
which is optimized so the the

00:17:11,640 --> 00:17:20,290
instrumentation is basically in the

00:17:16,870 --> 00:17:25,329
compiler in the AVM they will basically

00:17:20,290 --> 00:17:27,780
change it to some intrinsic name for the

00:17:25,329 --> 00:17:34,559
vector version in this case instead of

00:17:27,780 --> 00:17:38,559
sine f DB come in comes this and the

00:17:34,559 --> 00:17:44,550
actual code generators will choose the

00:17:38,559 --> 00:17:44,550
right library function for that vector

00:17:45,510 --> 00:17:59,610
and then as part of tuning we also found

00:17:53,410 --> 00:18:06,670
that there is a need to specify what

00:17:59,610 --> 00:18:09,880
target processor in this case to

00:18:06,670 --> 00:18:15,040
generate code for and what happens is

00:18:09,880 --> 00:18:20,620
this basically there is there's a

00:18:15,040 --> 00:18:24,520
trade-off between what architectures

00:18:20,620 --> 00:18:26,650
what vector in this case what a vector

00:18:24,520 --> 00:18:31,840
architecture we want to support you want

00:18:26,650 --> 00:18:36,670
to support ym AMG mm or just the

00:18:31,840 --> 00:18:42,880
traditional SSE so the spread of between

00:18:36,670 --> 00:18:46,270
between the back meaning that basically

00:18:42,880 --> 00:18:49,390
performance right and flexibility what

00:18:46,270 --> 00:18:52,030
because the openmp doesn't know what the

00:18:49,390 --> 00:18:55,780
target

00:18:52,030 --> 00:18:59,170
what vector units the target processor

00:18:55,780 --> 00:19:03,310
has four that when a program actually

00:18:59,170 --> 00:19:08,220
went on in some machine so then the

00:19:03,310 --> 00:19:08,220
thread of these the code size versus

00:19:08,370 --> 00:19:15,070
these performance

00:19:11,560 --> 00:19:18,400
so Jesus approach I understand is to

00:19:15,070 --> 00:19:22,180
basically just produce a family with

00:19:18,400 --> 00:19:25,450
multi versioning I think that basically

00:19:22,180 --> 00:19:27,460
two versions for excess xmm to because

00:19:25,450 --> 00:19:30,520
there's a mass and a math person of the

00:19:27,460 --> 00:19:34,360
vector code through for waimangu for GM

00:19:30,520 --> 00:19:37,410
m and and and just so that the the

00:19:34,360 --> 00:19:40,710
compiled code will run well on any

00:19:37,410 --> 00:19:44,860
machine that you throw that I think

00:19:40,710 --> 00:19:47,320
original Intel's compiler is a small

00:19:44,860 --> 00:19:50,200
conservative he doesn't want to generate

00:19:47,320 --> 00:19:53,110
a very fat so so you only have fewer

00:19:50,200 --> 00:19:57,460
versions but eventually we saw that

00:19:53,110 --> 00:20:00,340
you'll be a good idea to be able to

00:19:57,460 --> 00:20:02,320
specify a quad processor so we can so if

00:20:00,340 --> 00:20:06,640
you know that you only want to run the

00:20:02,320 --> 00:20:09,030
targeted code at that processor then you

00:20:06,640 --> 00:20:12,000
don't have to generate seminaries and

00:20:09,030 --> 00:20:16,660
you can still have good performance so

00:20:12,000 --> 00:20:22,060
so for this particular project we also

00:20:16,660 --> 00:20:26,050
implemented this this process that is

00:20:22,060 --> 00:20:28,840
not part of opening T and these are is

00:20:26,050 --> 00:20:31,990
in diet implementation these are the

00:20:28,840 --> 00:20:36,700
different names that the clothes can

00:20:31,990 --> 00:20:39,040
accept some are these future CPUs we

00:20:36,700 --> 00:20:40,780
haven't decided a name for it because we

00:20:39,040 --> 00:20:43,090
don't get to choose those names

00:20:40,780 --> 00:20:46,690
so we just use them placeholders there

00:20:43,090 --> 00:20:55,320
so for example kennel and brought well

00:20:46,690 --> 00:20:55,320
we still don't have a good entry yes

00:21:01,800 --> 00:21:15,010
those are different that so this is for

00:21:06,280 --> 00:21:16,630
um in in when you choose the miners like

00:21:15,010 --> 00:21:24,010
X avx2

00:21:16,630 --> 00:21:31,060
yeah those are more for compatibility

00:21:24,010 --> 00:21:35,560
but in OpenMP the problem is when when

00:21:31,060 --> 00:21:38,950
you compile with with without those

00:21:35,560 --> 00:21:42,160
flags you still need to be able to

00:21:38,950 --> 00:21:48,220
support differencing so you you cannot

00:21:42,160 --> 00:21:50,740
just rely on that yeah so so if you if

00:21:48,220 --> 00:21:56,010
you had those then you could claim that

00:21:50,740 --> 00:22:02,140
you don't need but often we can I just

00:21:56,010 --> 00:22:07,020
have yeah so it is much more direct to

00:22:02,140 --> 00:22:07,020
have this kind of control within openmp

00:22:08,820 --> 00:22:18,130
okay so then I'm going to talk a little

00:22:12,850 --> 00:22:19,840
bit about so after now I've talked about

00:22:18,130 --> 00:22:23,820
endings to the application now it's

00:22:19,840 --> 00:22:29,290
changing to the compiler and OpenCL

00:22:23,820 --> 00:22:32,470
compiler was whether the code base was

00:22:29,290 --> 00:22:38,880
on a low TMI scene you will three points

00:22:32,470 --> 00:22:44,130
age or four point over at the time so

00:22:38,880 --> 00:22:49,030
what we added is we wanted to support

00:22:44,130 --> 00:22:55,600
openmp and we wanted to be able to do

00:22:49,030 --> 00:22:58,510
the the code transformation so that the

00:22:55,600 --> 00:23:02,500
lower OpenMP lowering and outlining and

00:22:58,510 --> 00:23:04,390
the things in the middle and hopefully

00:23:02,500 --> 00:23:06,310
that will allow for more optimizations

00:23:04,390 --> 00:23:10,540
and has a better integration with other

00:23:06,310 --> 00:23:13,870
passes in no penalty so the first thing

00:23:10,540 --> 00:23:18,540
is we needed our front end rather crank

00:23:13,870 --> 00:23:21,940
running to generate a set of IRS that we

00:23:18,540 --> 00:23:25,420
defined based on directives and there is

00:23:21,940 --> 00:23:27,340
a small set of additions to basically a

00:23:25,420 --> 00:23:29,590
couple of intrinsic that we added to

00:23:27,340 --> 00:23:32,680
represent the directives begin a name

00:23:29,590 --> 00:23:36,220
and then the mechanism will allow you

00:23:32,680 --> 00:23:38,950
allow us to represent directives and

00:23:36,220 --> 00:23:42,610
clauses and and so forth within this ir

00:23:38,950 --> 00:23:46,600
and is nicely integrated into the

00:23:42,610 --> 00:23:50,860
existing frame original vm then so

00:23:46,600 --> 00:23:53,590
because that is most small change so it

00:23:50,860 --> 00:23:57,750
is not disrupted it has a very minimal

00:23:53,590 --> 00:24:02,630
impact on on the error in infrastructure

00:23:57,750 --> 00:24:08,760
then based on those records then we can

00:24:02,630 --> 00:24:13,650
generate our representation of parallel

00:24:08,760 --> 00:24:16,410
regions or different constructs for

00:24:13,650 --> 00:24:18,810
OpenMP in a unified way so we can

00:24:16,410 --> 00:24:20,510
support precision vectorization or

00:24:18,810 --> 00:24:24,050
floating everything using the same

00:24:20,510 --> 00:24:30,720
framework the same region representation

00:24:24,050 --> 00:24:32,460
and then we can optimize the code by

00:24:30,720 --> 00:24:36,570
doing the transformations in the middle

00:24:32,460 --> 00:24:39,510
in producing openmp code of load the the

00:24:36,570 --> 00:24:45,560
outlining part for uploading or

00:24:39,510 --> 00:24:50,180
threading and so forth so this is a

00:24:45,560 --> 00:24:56,160
diagram summarizing the changes to LVM

00:24:50,180 --> 00:24:59,160
before so on the left is what happens to

00:24:56,160 --> 00:25:01,110
Terra tation and on the right is for

00:24:59,160 --> 00:25:07,190
vectorization and they're very similar

00:25:01,110 --> 00:25:15,030
so before this dotted line is basically

00:25:07,190 --> 00:25:19,560
what what we do before the actual code

00:25:15,030 --> 00:25:22,410
changes so it's basically just I are out

00:25:19,560 --> 00:25:27,870
of opponent with those directives and

00:25:22,410 --> 00:25:31,560
with some pre-processing okay and the

00:25:27,870 --> 00:25:35,580
bulk of the transformations for openmp

00:25:31,560 --> 00:25:37,560
is construction the graph what we call

00:25:35,580 --> 00:25:41,100
that double region graph is an

00:25:37,560 --> 00:25:45,260
abstraction to capture the regions or

00:25:41,100 --> 00:25:48,020
constructs in OpenMP and produce the

00:25:45,260 --> 00:25:53,980
prioritization renaming

00:25:48,020 --> 00:25:58,850
then the repetition through schedule

00:25:53,980 --> 00:26:02,470
dilute and then the Montessori encodes

00:25:58,850 --> 00:26:07,610
so that that is basically outlining and

00:26:02,470 --> 00:26:11,960
calling the the libraries so similarly

00:26:07,610 --> 00:26:16,670
for vectorization the changes are also

00:26:11,960 --> 00:26:18,760
before the dollar line means your the ir

00:26:16,670 --> 00:26:24,140
representation with some pre-processing

00:26:18,760 --> 00:26:27,970
then the representation that is the same

00:26:24,140 --> 00:26:31,670
double region that represents those

00:26:27,970 --> 00:26:37,850
regions then there's another layer

00:26:31,670 --> 00:26:40,610
called e-tron that is slightly lower

00:26:37,850 --> 00:26:46,250
than the double regions but represents

00:26:40,610 --> 00:26:50,660
the vectorization code much better and

00:26:46,250 --> 00:26:54,290
then we serve and we can generate vector

00:26:50,660 --> 00:26:58,160
code so these are things are not gone to

00:26:54,290 --> 00:27:01,130
the LLVM side of things and as I said

00:26:58,160 --> 00:27:08,960
earlier there there is a paper about

00:27:01,130 --> 00:27:12,180
that in the a low BM conference so now

00:27:08,960 --> 00:27:16,170
we can go to some

00:27:12,180 --> 00:27:19,800
okay Perry of some results for this

00:27:16,170 --> 00:27:24,420
particular workload called great fusion

00:27:19,800 --> 00:27:28,700
basically we achieved 35 X peda compared

00:27:24,420 --> 00:27:36,230
to the the just a pipeline version of it

00:27:28,700 --> 00:27:40,010
so things that were done for resolution

00:27:36,230 --> 00:27:43,800
we have cindy-lou vectorization and

00:27:40,010 --> 00:27:45,800
again the the channel we drive is is

00:27:43,800 --> 00:27:49,110
[Music]

00:27:45,800 --> 00:27:55,740
exponential expenditure vector but the

00:27:49,110 --> 00:27:59,160
old order is wizard and or the scalar

00:27:55,740 --> 00:28:02,280
expansion that is done by the compiler

00:27:59,160 --> 00:28:05,490
for vectorization then we have to do

00:28:02,280 --> 00:28:08,640
some strip mining distribution expansion

00:28:05,490 --> 00:28:13,740
for some of the loops where they

00:28:08,640 --> 00:28:16,650
involved subtypes and then vector length

00:28:13,740 --> 00:28:18,930
we chose based on which architecture we

00:28:16,650 --> 00:28:25,740
wanted to do so this is back this goes

00:28:18,930 --> 00:28:30,660
back to that processor close up that I

00:28:25,740 --> 00:28:33,090
mentioned earlier and we for

00:28:30,660 --> 00:28:39,390
vectorization and we also specify the

00:28:33,090 --> 00:28:41,880
single names that we needed and the user

00:28:39,390 --> 00:28:45,360
functions that are involved in vector

00:28:41,880 --> 00:28:47,730
loops then they have to be a declare

00:28:45,360 --> 00:28:51,320
Cindy so the compiler can generate a

00:28:47,730 --> 00:28:54,090
vector version of it and for the library

00:28:51,320 --> 00:28:56,430
you don't need to do that because the

00:28:54,090 --> 00:28:58,580
compiler really knows that it is a

00:28:56,430 --> 00:29:01,040
science and all those math libraries and

00:28:58,580 --> 00:29:07,210
the compiler can already do that

00:29:01,040 --> 00:29:12,320
automatically then the last one is the

00:29:07,210 --> 00:29:17,200
the again that the channel realizes is

00:29:12,320 --> 00:29:23,090
the the channel in the pipeline

00:29:17,200 --> 00:29:24,710
they are also expanded or to support

00:29:23,090 --> 00:29:29,750
Olympics research I think I saw this

00:29:24,710 --> 00:29:38,000
earlier this is the width is expanded to

00:29:29,750 --> 00:29:40,520
a vector yes example of what one of the

00:29:38,000 --> 00:29:44,420
main loops in good fusion how is the way

00:29:40,520 --> 00:29:49,220
vectorize them so from aunty Cindy

00:29:44,420 --> 00:29:51,950
choose a single length of 16 then the

00:29:49,220 --> 00:29:55,640
accumulator occupying food and the this

00:29:51,950 --> 00:29:59,690
this this ball face blue line here is a

00:29:55,640 --> 00:30:04,760
channel input and output so that is also

00:29:59,690 --> 00:30:08,810
expanded field vector then these routine

00:30:04,760 --> 00:30:11,720
names in purple those are function use

00:30:08,810 --> 00:30:14,150
basically routines in the application

00:30:11,720 --> 00:30:17,900
that we needed to decreasingly to

00:30:14,150 --> 00:30:20,240
produce the vector version of those

00:30:17,900 --> 00:30:23,980
functions so that's an example of what

00:30:20,240 --> 00:30:27,510
how really instrumental vectorization

00:30:23,980 --> 00:30:28,610
this is one with polarization

00:30:27,510 --> 00:30:31,590
[Music]

00:30:28,610 --> 00:30:35,550
actually as we both just seem to hear

00:30:31,590 --> 00:30:38,190
but the polarization of the loop is here

00:30:35,550 --> 00:30:44,460
so we have a parallel for with a bunch

00:30:38,190 --> 00:30:48,060
of reductions and so this did I say

00:30:44,460 --> 00:30:51,930
speed-up no Nastya so for this the

00:30:48,060 --> 00:30:55,250
speed-up was for you know 4.5 backers

00:30:51,930 --> 00:31:05,060
pretty good just by adding a few lines

00:30:55,250 --> 00:31:10,080
and this part is also vectorization with

00:31:05,060 --> 00:31:14,220
so we added a indeed line there again

00:31:10,080 --> 00:31:17,010
the channel in and outs are sexual so

00:31:14,220 --> 00:31:22,140
this part produce almost 6x just by

00:31:17,010 --> 00:31:27,620
adding these lines and then this is

00:31:22,140 --> 00:31:30,530
another hot function there the clear

00:31:27,620 --> 00:31:34,320
these are just a decreasing these that

00:31:30,530 --> 00:31:36,570
accompany the previous life so the

00:31:34,320 --> 00:31:40,530
functions that need that appear in the

00:31:36,570 --> 00:31:42,570
vector loops so we added a declare from

00:31:40,530 --> 00:31:45,330
the cursor in V so that the compiler

00:31:42,570 --> 00:31:49,110
automatically generates the function

00:31:45,330 --> 00:31:52,140
versions so this is a summer summary

00:31:49,110 --> 00:31:58,110
slide of the performance let's look at

00:31:52,140 --> 00:32:02,550
the bottom line so basically the total

00:31:58,110 --> 00:32:06,900
time that it takes for one computation

00:32:02,550 --> 00:32:11,940
in the pipeline is basically of course

00:32:06,900 --> 00:32:13,680
the bottleneck is the stage that takes

00:32:11,940 --> 00:32:16,290
the longest time that that would be your

00:32:13,680 --> 00:32:20,670
borrow neck so it's a matter of the

00:32:16,290 --> 00:32:24,450
times of each stage and then that you

00:32:20,670 --> 00:32:26,590
have to add the time to communicate so

00:32:24,450 --> 00:32:30,230
the the channel

00:32:26,590 --> 00:32:33,049
the the the communication from one say

00:32:30,230 --> 00:32:37,870
to the next which is called the the

00:32:33,049 --> 00:32:43,700
channel the time of that channel so in

00:32:37,870 --> 00:32:45,410
in in this application the channel

00:32:43,700 --> 00:32:50,860
overhead is quite big it's like a

00:32:45,410 --> 00:32:50,860
milliseconds and after all our

00:32:51,220 --> 00:32:59,000
optimizations we managed to make the the

00:32:56,660 --> 00:33:02,000
mass of any of those stages and lot of

00:32:59,000 --> 00:33:05,450
the stage the slowest stage to be

00:33:02,000 --> 00:33:11,600
bounded at about five milliseconds so it

00:33:05,450 --> 00:33:14,690
takes for one computation of say a frame

00:33:11,600 --> 00:33:18,320
or whatever whatever unit that is then

00:33:14,690 --> 00:33:21,380
takes about thirty thirteen cyclisation

00:33:18,320 --> 00:33:25,330
it's about thirteen elicits so that's

00:33:21,380 --> 00:33:33,610
what this 30 millisecond is so

00:33:25,330 --> 00:33:39,440
originally the 450 is the time of the

00:33:33,610 --> 00:33:44,660
the open SEL application that was

00:33:39,440 --> 00:33:49,000
targeted at the CPU that is only it is

00:33:44,660 --> 00:33:51,410
not optimized with any of this open MP

00:33:49,000 --> 00:33:54,919
directives but it's all too much

00:33:51,410 --> 00:33:58,120
only for the pipeline parallelism so

00:33:54,919 --> 00:33:58,120
that is that is

00:33:58,270 --> 00:34:08,630
and then there was also some experiments

00:34:03,500 --> 00:34:14,750
if we take away the okay if we look at

00:34:08,630 --> 00:34:17,690
three hot kernels here we choose them

00:34:14,750 --> 00:34:21,560
and then the the actual speed up for

00:34:17,690 --> 00:34:23,330
those parting with kernels where from

00:34:21,560 --> 00:34:24,710
actually did these are the ones that we

00:34:23,330 --> 00:34:27,920
saw in previous slides like they're

00:34:24,710 --> 00:34:30,700
almost a six eggs and the 4.5 - in those

00:34:27,920 --> 00:34:34,130
kernels and if you take away the

00:34:30,700 --> 00:34:37,070
channels that the cost of that the time

00:34:34,130 --> 00:34:39,050
of the channel then they become much

00:34:37,070 --> 00:34:42,260
higher so and the meaning of this is

00:34:39,050 --> 00:34:46,670
that in the actual FPGA run the channel

00:34:42,260 --> 00:34:50,690
cause is negligible as opposed to the

00:34:46,670 --> 00:34:56,980
emulation so so basically this is the

00:34:50,690 --> 00:34:56,980
the raw gain by using apparently

00:34:58,690 --> 00:35:06,430
[Music]

00:34:59,950 --> 00:35:08,500
Oh port no no for for these each each of

00:35:06,430 --> 00:35:10,300
the kernels we can choose how many

00:35:08,500 --> 00:35:15,040
stretch we want to use like for some of

00:35:10,300 --> 00:35:17,740
those we chose to use 16 thread yes yes

00:35:15,040 --> 00:35:21,790
yes it varies from current terms because

00:35:17,740 --> 00:35:26,250
basically say your room we have 50 some

00:35:21,790 --> 00:35:28,570
threads in the skylight and - the

00:35:26,250 --> 00:35:32,440
interests that are used by the pipeline

00:35:28,570 --> 00:35:34,839
so we have that many stress left we can

00:35:32,440 --> 00:35:37,570
kind of accommodate how we want to

00:35:34,839 --> 00:35:46,180
distribute so each stage can choose

00:35:37,570 --> 00:35:54,880
different number of threads so in

00:35:46,180 --> 00:35:59,099
conclusion we basically optimize from an

00:35:54,880 --> 00:36:02,079
optimal driving application applications

00:35:59,099 --> 00:36:06,220
using openmp so that we can take

00:36:02,079 --> 00:36:10,390
advantage of the architectural features

00:36:06,220 --> 00:36:13,030
of CPUs and to do that we basically

00:36:10,390 --> 00:36:15,310
change the application by adding these

00:36:13,030 --> 00:36:19,089
directives and we also change the

00:36:15,310 --> 00:36:23,859
compiler by providing capabilities to

00:36:19,089 --> 00:36:29,099
have open Co and opening T this both the

00:36:23,859 --> 00:36:32,680
product and so of course there is no

00:36:29,099 --> 00:36:35,109
one-size-fits-all solution that will

00:36:32,680 --> 00:36:37,349
satisfy all programmers or all

00:36:35,109 --> 00:36:43,290
applications so we have to do this

00:36:37,349 --> 00:36:50,370
hybrid thing and no free lunch of course

00:36:43,290 --> 00:36:54,030
you have to know try to extract the most

00:36:50,370 --> 00:36:56,910
out of your architectures by using all

00:36:54,030 --> 00:36:59,340
these different strategies and you have

00:36:56,910 --> 00:37:05,130
to need to combine different paradigms

00:36:59,340 --> 00:37:11,960
just to work with FPGA with CPUs with

00:37:05,130 --> 00:37:16,020
GPUs and so forth and we believe that

00:37:11,960 --> 00:37:20,130
the hardware and applications that are

00:37:16,020 --> 00:37:23,880
you know changing rapidly they will

00:37:20,130 --> 00:37:29,070
drive what kind of programming models

00:37:23,880 --> 00:37:31,920
language and compilers are needed and so

00:37:29,070 --> 00:37:33,900
and in this case in our experiment we

00:37:31,920 --> 00:37:36,240
kind of use an incremental approach

00:37:33,900 --> 00:37:38,790
because we first of all we didn't have

00:37:36,240 --> 00:37:40,740
the time to reinvent everything so we

00:37:38,790 --> 00:37:43,920
just we only have a few months so we

00:37:40,740 --> 00:37:46,560
basically just quickly added some things

00:37:43,920 --> 00:37:50,130
that we already know how to do into an

00:37:46,560 --> 00:37:53,010
existing compiler and so it is

00:37:50,130 --> 00:38:01,820
incremental approach actually anything

00:37:53,010 --> 00:38:01,820
in this case it works for us thank you

00:38:04,190 --> 00:38:07,900
I am someone

00:38:08,770 --> 00:38:11,829
[Music]

00:38:22,029 --> 00:38:25,119

YouTube URL: https://www.youtube.com/watch?v=ShHOqYrwDqY


