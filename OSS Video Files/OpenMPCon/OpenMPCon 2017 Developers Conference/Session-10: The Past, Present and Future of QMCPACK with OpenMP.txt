Title: Session-10: The Past, Present and Future of QMCPACK with OpenMP
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Ye Luo, Anouar Benali, Jeongnim Kim and Paul Kent
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day2-Session2-Luo.pdf
Captions: 
	00:00:03,420 --> 00:00:06,509
[Music]

00:00:07,490 --> 00:00:12,610
that's fine

00:00:09,490 --> 00:00:15,589
[Music]

00:00:12,610 --> 00:00:18,260
optimizing the climbing structure

00:00:15,589 --> 00:00:20,960
consolation package

00:00:18,260 --> 00:00:28,680
for the latest grade of high performance

00:00:20,960 --> 00:00:31,820
teacher and he is going but evolutionary

00:00:28,680 --> 00:00:31,820
[Music]

00:00:40,180 --> 00:00:44,059
thanks

00:00:41,510 --> 00:00:45,769
ok I'm you know from Oregon leadership

00:00:44,059 --> 00:00:49,070
computing facility at Argonne National

00:00:45,769 --> 00:00:52,430
Lab today I'm going to talk about the

00:00:49,070 --> 00:00:56,210
qmg pact application that past the

00:00:52,430 --> 00:00:59,449
present and the future so the battle

00:00:56,210 --> 00:01:03,859
code beacons list of those parts so if

00:00:59,449 --> 00:01:07,460
we start with UMC basics and to a bit of

00:01:03,859 --> 00:01:10,880
Chimpy pact the code then I will show

00:01:07,460 --> 00:01:15,020
that how open P is introduced from the

00:01:10,880 --> 00:01:16,820
beginning of this code later is our

00:01:15,020 --> 00:01:19,940
recent work to achieve performance

00:01:16,820 --> 00:01:23,630
portable portability on CPUs we are the

00:01:19,940 --> 00:01:26,750
open MP MV than our new experiment

00:01:23,630 --> 00:01:29,300
during the four point five uploading to

00:01:26,750 --> 00:01:32,990
get to tell for the future so this work

00:01:29,300 --> 00:01:37,399
is supported by Intel parallel computing

00:01:32,990 --> 00:01:40,849
Center at Oregon this gives us a lot of

00:01:37,399 --> 00:01:44,090
help from the Intel engineers than this

00:01:40,849 --> 00:01:47,289
research this research was using

00:01:44,090 --> 00:01:50,179
resources from argon and Oakridge and

00:01:47,289 --> 00:01:53,100
this is also supported by the exascale

00:01:50,179 --> 00:01:55,390
computing project

00:01:53,100 --> 00:01:59,380
so start with the condiment is how the

00:01:55,390 --> 00:02:02,079
basics ronimal Nutella actually is not a

00:01:59,380 --> 00:02:05,079
single method it's basically two sets of

00:02:02,079 --> 00:02:09,509
Monte Carlo method or isms solving

00:02:05,079 --> 00:02:12,280
quantum mechanics so kimchi pact is a

00:02:09,509 --> 00:02:15,190
modern high-performance open source

00:02:12,280 --> 00:02:17,739
content Monte Carlo simulation code so I

00:02:15,190 --> 00:02:20,980
mainly deal with electronic structure

00:02:17,739 --> 00:02:26,590
calculations so I can do molecules like

00:02:20,980 --> 00:02:30,430
2d systems and also solid simply pack is

00:02:26,590 --> 00:02:35,380
a C++ code and adopting MPI plus X

00:02:30,430 --> 00:02:38,799
parallelism scheme so the X is open P

00:02:35,380 --> 00:02:43,150
for CPUs and CUDA for the GTS at the

00:02:38,799 --> 00:02:45,870
moment so we are at you can get more

00:02:43,150 --> 00:02:49,209
information and film c-pack dot org and

00:02:45,870 --> 00:02:52,420
our code is publicly available at github

00:02:49,209 --> 00:02:56,819
so we get one of the application

00:02:52,420 --> 00:02:56,819
developer award of the ECP project

00:02:59,360 --> 00:03:05,270
so don't be scared this is the only life

00:03:02,780 --> 00:03:07,430
with four Mueller's so basically the

00:03:05,270 --> 00:03:09,650
what we do with quantum Monticello is

00:03:07,430 --> 00:03:11,750
solving the Schrodinger equations when

00:03:09,650 --> 00:03:12,590
you do the energy of the Schrodinger

00:03:11,750 --> 00:03:15,980
equation

00:03:12,590 --> 00:03:18,440
then it's basically you do the integral

00:03:15,980 --> 00:03:22,010
multi-dimensional integral of the

00:03:18,440 --> 00:03:24,590
Hamiltonian in the real space and you

00:03:22,010 --> 00:03:26,570
get the energy instead of the

00:03:24,590 --> 00:03:28,820
traditional method like quantum

00:03:26,570 --> 00:03:31,970
chemistry method or density functional

00:03:28,820 --> 00:03:33,680
method you do the integral directly we

00:03:31,970 --> 00:03:37,130
do it stochastically

00:03:33,680 --> 00:03:40,220
instead of doing to grow so we sample

00:03:37,130 --> 00:03:43,280
the space of using this Monte Carlo

00:03:40,220 --> 00:03:46,160
technique and sum up all the samples

00:03:43,280 --> 00:03:50,630
reflect and you get an energy with an

00:03:46,160 --> 00:03:53,630
arrow bar it's beneficial because it can

00:03:50,630 --> 00:03:57,920
be parallelized easily and you can

00:03:53,630 --> 00:04:00,470
control the the accuracy you need and so

00:03:57,920 --> 00:04:02,180
in a lot of cases actually you don't

00:04:00,470 --> 00:04:04,730
need the seven stages for your

00:04:02,180 --> 00:04:06,500
calculation because the method like like

00:04:04,730 --> 00:04:09,019
to the other method engine density

00:04:06,500 --> 00:04:13,459
functional theory so basically as a

00:04:09,019 --> 00:04:16,930
force four stages usually the method

00:04:13,459 --> 00:04:20,540
itself have arrow larger than that so

00:04:16,930 --> 00:04:23,030
this is the difference compare link

00:04:20,540 --> 00:04:25,880
stochastic methods or deterministic

00:04:23,030 --> 00:04:28,340
methods so the random work is the Monte

00:04:25,880 --> 00:04:31,370
Carlo sampling is generated generated by

00:04:28,340 --> 00:04:33,860
random walking so it's performed by

00:04:31,370 --> 00:04:37,360
Walker than evolving on Markov chain and

00:04:33,860 --> 00:04:41,030
we can employ many workers so this is a

00:04:37,360 --> 00:04:44,150
schematic of diffusion metallo slightly

00:04:41,030 --> 00:04:47,600
different from variation of L it's close

00:04:44,150 --> 00:04:51,440
in terms of coding and implementing the

00:04:47,600 --> 00:04:55,729
parallelism so you have an example of a

00:04:51,440 --> 00:04:56,660
set of workers then in time you evolve

00:04:55,729 --> 00:04:59,360
though

00:04:56,660 --> 00:05:02,510
workers you do some help you promote a

00:04:59,360 --> 00:05:04,310
new position new new setup a new

00:05:02,510 --> 00:05:08,870
configuration then you do the

00:05:04,310 --> 00:05:11,480
calculation then you get up a few

00:05:08,870 --> 00:05:14,590
quantities of the new configuration you

00:05:11,480 --> 00:05:17,390
could decide whether I accept it or not

00:05:14,590 --> 00:05:19,700
the the calculation is here doing the

00:05:17,390 --> 00:05:23,660
random walking takes a lot of time then

00:05:19,700 --> 00:05:26,180
once it's been accepted so you can keep

00:05:23,660 --> 00:05:28,550
going the next proposed move the next

00:05:26,180 --> 00:05:30,830
iteration in the future Monte Carlo

00:05:28,550 --> 00:05:33,170
there are a bit more work that workers

00:05:30,830 --> 00:05:35,300
can be they can spawn new workers and

00:05:33,170 --> 00:05:37,820
workers can be killed if they have a

00:05:35,300 --> 00:05:40,430
very low weight it's basically to get

00:05:37,820 --> 00:05:42,260
more efficient sampling and deal with a

00:05:40,430 --> 00:05:45,230
load balance but this kind of

00:05:42,260 --> 00:05:48,350
communication is small and we basically

00:05:45,230 --> 00:05:50,960
say this usually takes less than 10

00:05:48,350 --> 00:05:53,000
percent of the total computing time so

00:05:50,960 --> 00:05:56,480
our challenge in part is doing the

00:05:53,000 --> 00:05:58,190
computing quick so this is the

00:05:56,480 --> 00:06:00,910
pseudocode of the quantum Monte Carlo

00:05:58,190 --> 00:06:03,170
algorithm basically you start with a

00:06:00,910 --> 00:06:06,220
sequential loop that is the time

00:06:03,170 --> 00:06:09,070
evolution then you have the second loop

00:06:06,220 --> 00:06:11,840
over the example that's all the workers

00:06:09,070 --> 00:06:14,660
at the fourth step you have another loop

00:06:11,840 --> 00:06:17,360
introduced it specifies your loop the

00:06:14,660 --> 00:06:20,660
reason is in algorithms when we do

00:06:17,360 --> 00:06:22,760
algorithms doing single particle move is

00:06:20,660 --> 00:06:25,100
more efficient than movie for the

00:06:22,760 --> 00:06:28,580
particle at once because once you get

00:06:25,100 --> 00:06:30,500
with projection or your calculations

00:06:28,580 --> 00:06:33,020
wasted so it's better to do single

00:06:30,500 --> 00:06:36,530
particle moves this is sequential loop

00:06:33,020 --> 00:06:39,020
then inside single party move you do a

00:06:36,530 --> 00:06:41,300
bunch of derivatives over all the

00:06:39,020 --> 00:06:44,000
particles because you move one particle

00:06:41,300 --> 00:06:46,940
but the one particle interaction with

00:06:44,000 --> 00:06:49,850
all the rest particles and even in the

00:06:46,940 --> 00:06:52,580
orbital side we move one particle the

00:06:49,850 --> 00:06:54,740
single particle orbitals of for the

00:06:52,580 --> 00:06:57,560
banned product response to this particle

00:06:54,740 --> 00:06:59,309
has to be calculated so you always have

00:06:57,560 --> 00:07:01,800
an extra loop

00:06:59,309 --> 00:07:05,249
over the particles which is pretty big

00:07:01,800 --> 00:07:08,520
so it can be in the size of one case so

00:07:05,249 --> 00:07:11,249
with those two major parallelizable

00:07:08,520 --> 00:07:15,389
loops the loop to the workers can be

00:07:11,249 --> 00:07:19,439
easily distribute over MPI and I know

00:07:15,389 --> 00:07:21,779
the we usually put about tens of workers

00:07:19,439 --> 00:07:26,219
which can be distributed over to two

00:07:21,779 --> 00:07:28,969
cords or the essence of GPUs using a

00:07:26,219 --> 00:07:34,099
Pantheon CUDA that's why we get almost

00:07:28,969 --> 00:07:38,219
close to ideal speed-up in scaling and

00:07:34,099 --> 00:07:41,580
the inner loop this six a seven

00:07:38,219 --> 00:07:44,399
Terri's can be easily exposed to the

00:07:41,580 --> 00:07:49,110
situation vo GPU thread so this is the

00:07:44,399 --> 00:07:52,020
way we use the parallelism

00:07:49,110 --> 00:07:53,759
on that note so first show you the

00:07:52,020 --> 00:07:59,129
worker level parallelism how we

00:07:53,759 --> 00:08:01,379
implement it this is the management of

00:07:59,129 --> 00:08:04,889
our calculation so we have those workers

00:08:01,379 --> 00:08:07,289
to be involved these workers actually

00:08:04,889 --> 00:08:09,629
either large lightweight ones a carry

00:08:07,289 --> 00:08:11,309
only the minimum data because as I said

00:08:09,629 --> 00:08:13,110
you need to give birth and kill them

00:08:11,309 --> 00:08:15,269
then you need a really balanced so they

00:08:13,110 --> 00:08:17,490
have to be communicated so they must be

00:08:15,269 --> 00:08:21,389
small then you have those movers

00:08:17,490 --> 00:08:23,699
actually sitting on the thread so you

00:08:21,389 --> 00:08:26,189
just do an old man P parallel you have

00:08:23,699 --> 00:08:29,339
threads and have a worker mover sitting

00:08:26,189 --> 00:08:32,370
on it the whole memo data for the

00:08:29,339 --> 00:08:35,610
scratch for this track scratch during

00:08:32,370 --> 00:08:40,800
the calculation so then you just put an

00:08:35,610 --> 00:08:42,220
or mp4 just to disability' worker over

00:08:40,800 --> 00:08:44,860
those move

00:08:42,220 --> 00:08:46,960
and there's no increase in the

00:08:44,860 --> 00:08:50,200
synchronisation at all during during the

00:08:46,960 --> 00:08:52,090
calculation and until before you need to

00:08:50,200 --> 00:08:55,000
do the next iteration yes you have to

00:08:52,090 --> 00:08:57,340
just select angle synchronization so

00:08:55,000 --> 00:09:01,720
it's very efficient on the note with

00:08:57,340 --> 00:09:04,450
this scheme so implementing open T so

00:09:01,720 --> 00:09:07,420
the open T the reason is another reason

00:09:04,450 --> 00:09:10,740
we use all apparently that is to take

00:09:07,420 --> 00:09:13,570
advantage of the share memory feature so

00:09:10,740 --> 00:09:16,870
actually during the calculation those

00:09:13,570 --> 00:09:19,270
movers need to accept the coefficients

00:09:16,870 --> 00:09:22,540
beautiful calculating single particle

00:09:19,270 --> 00:09:25,690
orbitals that is a b-spline so it's a

00:09:22,540 --> 00:09:28,330
trade-off we made particularly for for

00:09:25,690 --> 00:09:30,970
quantum Monte Carlo for fast calculation

00:09:28,330 --> 00:09:36,640
but larger memory footprint you can

00:09:30,970 --> 00:09:38,650
think it in you you have explicitly

00:09:36,640 --> 00:09:41,230
localized basis set so all your

00:09:38,650 --> 00:09:43,690
calculation gets localized instead of

00:09:41,230 --> 00:09:47,400
having a huge basis set which increase

00:09:43,690 --> 00:09:52,450
in the proportional to the number of

00:09:47,400 --> 00:09:55,510
system size so but this one is big is

00:09:52,450 --> 00:09:57,340
usually at service it can be a go up to

00:09:55,510 --> 00:09:59,770
a hundred gigabytes just feel the whole

00:09:57,340 --> 00:10:03,490
known memory because they need to be

00:09:59,770 --> 00:10:06,570
accessed frequently randomly it cannot

00:10:03,490 --> 00:10:10,120
be you cannot take the data from

00:10:06,570 --> 00:10:13,120
different nodes that's true slope that's

00:10:10,120 --> 00:10:16,320
for this reason that this piece time

00:10:13,120 --> 00:10:18,700
table is replicated or every node and

00:10:16,320 --> 00:10:21,100
since it's read-only so we just

00:10:18,700 --> 00:10:24,310
initialize one once and shared by all

00:10:21,100 --> 00:10:28,020
the movers that's why we need OpenMP

00:10:24,310 --> 00:10:31,450
features to accommodate this requirement

00:10:28,020 --> 00:10:36,100
so then I'm going to talk about how we

00:10:31,450 --> 00:10:38,030
will do the particle level parallelism

00:10:36,100 --> 00:10:42,690
the fine level

00:10:38,030 --> 00:10:45,180
so the the previous parallelism is

00:10:42,690 --> 00:10:48,060
implementing from the very beginning we

00:10:45,180 --> 00:10:50,340
are happy with it since it works for the

00:10:48,060 --> 00:10:54,480
past 10 years I think it will keep

00:10:50,340 --> 00:10:58,650
working then when we start to use

00:10:54,480 --> 00:11:02,100
brooding - we noticed a few problems so

00:10:58,650 --> 00:11:06,960
this is the profile done on blue jean -

00:11:02,100 --> 00:11:08,760
of all the important kernels so Timothy

00:11:06,960 --> 00:11:11,550
actually during the calculation we

00:11:08,760 --> 00:11:14,460
involve several different type of

00:11:11,550 --> 00:11:16,500
kernels so the best pranker noise is

00:11:14,460 --> 00:11:19,410
like dealing with orbitals is more like

00:11:16,500 --> 00:11:22,110
dense linear algebra than the just row

00:11:19,410 --> 00:11:24,330
factors or distance tables it looks like

00:11:22,110 --> 00:11:26,460
a particle code so this type of

00:11:24,330 --> 00:11:29,850
calculation they have distinctive

00:11:26,460 --> 00:11:32,660
features but they all have n particle

00:11:29,850 --> 00:11:35,730
parallelism can be take advantage on

00:11:32,660 --> 00:11:37,770
those kernels we noticed actually the

00:11:35,730 --> 00:11:40,350
numerical light kernels like Josh

00:11:37,770 --> 00:11:42,540
cofactors distant tables there are taken

00:11:40,350 --> 00:11:46,230
a lot of time much higher than what we

00:11:42,540 --> 00:11:51,150
expected so we were wondering why those

00:11:46,230 --> 00:11:53,820
are happening and then so so the B

00:11:51,150 --> 00:11:56,220
spline calculation actually we happily

00:11:53,820 --> 00:11:58,560
optimize for Blue Gene - and all the

00:11:56,220 --> 00:12:01,500
rest parts we didn't do that much in the

00:11:58,560 --> 00:12:03,900
past because this was the or the

00:12:01,500 --> 00:12:06,390
dominant problem then later we with when

00:12:03,900 --> 00:12:10,230
we just nail this one then all the other

00:12:06,390 --> 00:12:13,020
goes up then we come to the Knights

00:12:10,230 --> 00:12:15,060
landing you just notice actually those

00:12:13,020 --> 00:12:17,640
blight kernels is getting even worse

00:12:15,060 --> 00:12:20,640
taking much higher percentage of the

00:12:17,640 --> 00:12:24,150
time of the total calculation so we

00:12:20,640 --> 00:12:27,750
really need to solve this issue since we

00:12:24,150 --> 00:12:32,220
we can easily use course we didn't do

00:12:27,750 --> 00:12:35,910
well on the course on the efficiency for

00:12:32,220 --> 00:12:37,550
the course so how to improve the

00:12:35,910 --> 00:12:41,820
performance

00:12:37,550 --> 00:12:43,740
there are two things we did in this in

00:12:41,820 --> 00:12:47,460
the beginning of this year you know they

00:12:43,740 --> 00:12:49,500
were lacking in this year so one part is

00:12:47,460 --> 00:12:52,500
we are all doing always doing double

00:12:49,500 --> 00:12:55,680
precision except the baseline which can

00:12:52,500 --> 00:12:58,410
do single precision so Kenya has these

00:12:55,680 --> 00:13:01,160
features so you have doubles in the

00:12:58,410 --> 00:13:05,250
precision throughput in the vector unit

00:13:01,160 --> 00:13:07,770
we try to we need to use it and we

00:13:05,250 --> 00:13:10,950
assure that it's not the accuracy is not

00:13:07,770 --> 00:13:15,450
problem and then simply efficiencies are

00:13:10,950 --> 00:13:17,610
very low in our code the main reason is

00:13:15,450 --> 00:13:21,240
we are using a lot of part of array of

00:13:17,610 --> 00:13:24,420
structures like the positions you have n

00:13:21,240 --> 00:13:27,390
particles than the three dimension these

00:13:24,420 --> 00:13:30,600
are good or object oriented programming

00:13:27,390 --> 00:13:33,270
way but it's not ideal for high

00:13:30,600 --> 00:13:35,760
performance for for the u.s. employee in

00:13:33,270 --> 00:13:37,370
general high performance computing so

00:13:35,760 --> 00:13:40,020
basically we are getting scalar

00:13:37,370 --> 00:13:43,230
performance out of those calculations

00:13:40,020 --> 00:13:47,670
and in the past when we do vectorization

00:13:43,230 --> 00:13:52,530
we only did for the describe orbitals so

00:13:47,670 --> 00:13:56,490
we have the iron fine library which we

00:13:52,530 --> 00:13:59,310
add we have the ssee and qpx is

00:13:56,490 --> 00:14:01,380
specifically for blue-jean - and that's

00:13:59,310 --> 00:14:02,760
we have a big optimization with

00:14:01,380 --> 00:14:06,540
previously in the difference table

00:14:02,760 --> 00:14:10,110
however if you look at the code I'm no

00:14:06,540 --> 00:14:12,600
idea what these things actually unless I

00:14:10,110 --> 00:14:16,590
have a menu put aside and read it

00:14:12,600 --> 00:14:19,620
carefully and this the intrinsic for 2px

00:14:16,590 --> 00:14:23,910
I can understand this only because I

00:14:19,620 --> 00:14:27,440
wrote it but it's still not ideal to

00:14:23,910 --> 00:14:31,770
read this kind of code for most people

00:14:27,440 --> 00:14:33,900
so we first solve the precision so we

00:14:31,770 --> 00:14:36,270
bring most of the calculation from

00:14:33,900 --> 00:14:38,160
double precision to single precision so

00:14:36,270 --> 00:14:40,590
you could gain a lot of memory bandwidth

00:14:38,160 --> 00:14:42,660
on bruising too and also gain

00:14:40,590 --> 00:14:48,210
performance art and nail however the

00:14:42,660 --> 00:14:49,830
gain on Kenya is not that great so it's

00:14:48,210 --> 00:14:52,650
only about 20 percent or 50 percent

00:14:49,830 --> 00:14:55,170
don't actually this 55 percent was

00:14:52,650 --> 00:14:57,230
gained because we we put more things in

00:14:55,170 --> 00:15:00,870
each VM it's not because we are doing

00:14:57,230 --> 00:15:02,850
calculation more efficient so clearly

00:15:00,870 --> 00:15:05,370
this problem is coupled with the

00:15:02,850 --> 00:15:08,430
vectorization because if you have a

00:15:05,370 --> 00:15:10,260
scalar vector you don't have that

00:15:08,430 --> 00:15:13,770
performance you have scaler doing single

00:15:10,260 --> 00:15:16,530
percent about the precision should not

00:15:13,770 --> 00:15:19,410
change that much so we really need to

00:15:16,530 --> 00:15:22,320
handle the performance of vectorization

00:15:19,410 --> 00:15:26,250
and handle both double and single

00:15:22,320 --> 00:15:28,800
precision so we focus on the

00:15:26,250 --> 00:15:31,530
vectorization from the beginning of this

00:15:28,800 --> 00:15:33,960
year actually our paper was accepted to

00:15:31,530 --> 00:15:36,840
SC and it's already on like our Kaiser

00:15:33,960 --> 00:15:40,350
so what we desire is to do the

00:15:36,840 --> 00:15:42,000
vectorization since choosing the

00:15:40,350 --> 00:15:44,970
vectorization we need to change the

00:15:42,000 --> 00:15:47,240
rotation layout in several very

00:15:44,970 --> 00:15:50,090
fundamental data structures this

00:15:47,240 --> 00:15:54,980
potentially will pollute a lot of codes

00:15:50,090 --> 00:15:56,180
we would like to have minimize changes

00:15:54,980 --> 00:15:59,160
[Music]

00:15:56,180 --> 00:16:02,160
this is for the old code part so the

00:15:59,160 --> 00:16:05,870
newly added implementation we have those

00:16:02,160 --> 00:16:07,610
requirements that's a single source both

00:16:05,870 --> 00:16:11,070
precision types

00:16:07,610 --> 00:16:14,670
no more intrinsic hope to have any

00:16:11,070 --> 00:16:18,030
vector length and alignments requirement

00:16:14,670 --> 00:16:19,050
any compute compilers and any CPU

00:16:18,030 --> 00:16:23,350
vendors

00:16:19,050 --> 00:16:26,500
actually we embed it so what we do is we

00:16:23,350 --> 00:16:29,530
are several tech tools when is the C++

00:16:26,500 --> 00:16:31,180
is a template so when you use templates

00:16:29,530 --> 00:16:33,850
actually you can deal with those types

00:16:31,180 --> 00:16:34,900
easily then we use operator overloading

00:16:33,850 --> 00:16:38,140
with it

00:16:34,900 --> 00:16:41,320
the old code we can provide the operator

00:16:38,140 --> 00:16:43,320
functional errors as the old purpose so

00:16:41,320 --> 00:16:47,890
these you don't need to change that much

00:16:43,320 --> 00:16:53,410
then in C++ class C++ class you can also

00:16:47,890 --> 00:16:56,140
handle the alignment so with openmp 4.0

00:16:53,410 --> 00:16:59,500
actually we have this simply construct

00:16:56,140 --> 00:17:01,570
with a line cloth we can use so this is

00:16:59,500 --> 00:17:03,520
actually what we do to change the data

00:17:01,570 --> 00:17:06,790
layout from array of structures to

00:17:03,520 --> 00:17:11,110
structure of arrays we are this adopter

00:17:06,790 --> 00:17:16,120
class in this container class or in this

00:17:11,110 --> 00:17:20,230
container class the you hold the data

00:17:16,120 --> 00:17:22,930
with a line vector so you have this crew

00:17:20,230 --> 00:17:26,170
we just changes the STD vector with a

00:17:22,930 --> 00:17:28,420
different alligator I really hope is C++

00:17:26,170 --> 00:17:30,430
standard can provide directly an aligned

00:17:28,420 --> 00:17:34,140
alligators he has it then we have to

00:17:30,430 --> 00:17:38,050
wrap around it then you get this aligned

00:17:34,140 --> 00:17:42,550
vector holding the data which is size of

00:17:38,050 --> 00:17:44,500
n multiplied by D is the dimension then

00:17:42,550 --> 00:17:46,690
we provide an access method this

00:17:44,500 --> 00:17:50,320
operator overloading so you can return

00:17:46,690 --> 00:17:53,350
and the old structures easily so bad

00:17:50,320 --> 00:17:56,560
backwards or compatibility then we

00:17:53,350 --> 00:18:00,760
provide access method you can't get the

00:17:56,560 --> 00:18:04,720
if you have 3d not ready three by n

00:18:00,760 --> 00:18:08,710
array then you can get a third an array

00:18:04,720 --> 00:18:11,140
or second and RHS by calling this one so

00:18:08,710 --> 00:18:13,330
in all this data in all

00:18:11,140 --> 00:18:15,040
we have those back to of this tiny

00:18:13,330 --> 00:18:17,560
vectors such as more array of three

00:18:15,040 --> 00:18:20,320
those data types for the particle

00:18:17,560 --> 00:18:23,380
coordinates gradient ashin actually now

00:18:20,320 --> 00:18:25,150
we just do it with a new class and for

00:18:23,380 --> 00:18:29,290
the O code cause you do need to change

00:18:25,150 --> 00:18:33,100
at all then alignment is just handled in

00:18:29,290 --> 00:18:36,790
sizes so we have those particles then in

00:18:33,100 --> 00:18:40,480
the computing part so it gets more clean

00:18:36,790 --> 00:18:42,700
no more the intrinsic size I said so now

00:18:40,480 --> 00:18:45,600
when you do the calculation you have a

00:18:42,700 --> 00:18:50,800
loop of 3 which loop over the 3

00:18:45,600 --> 00:18:52,960
dimensions not not with X Y Z thank you

00:18:50,800 --> 00:18:55,900
basically is try the pointers to your

00:18:52,960 --> 00:18:58,690
calculation with a single loop then you

00:18:55,900 --> 00:19:01,720
do put OMG Cindy the input a line the

00:18:58,690 --> 00:19:03,900
indicating those variables are all

00:19:01,720 --> 00:19:10,510
aligned but you do need to specify the

00:19:03,900 --> 00:19:14,110
alignment the size so okay so it's so

00:19:10,510 --> 00:19:18,220
cheap so this in this way it's very

00:19:14,110 --> 00:19:21,310
flexible to the vector length on unbasic

00:19:18,220 --> 00:19:24,880
you we have like 2 5 2 experience and

00:19:21,310 --> 00:19:27,880
now you have 512 your MD you have 1 2 8

00:19:24,880 --> 00:19:30,460
so that can be handled by this single

00:19:27,880 --> 00:19:32,290
pragma super clean and it's all

00:19:30,460 --> 00:19:37,720
vectorize you only see vector loads like

00:19:32,290 --> 00:19:39,970
restoring the in the assembly as a

00:19:37,720 --> 00:19:42,630
scientist I'm willing to change the

00:19:39,970 --> 00:19:46,060
algorithms and it helps a lot to

00:19:42,630 --> 00:19:48,610
vectorize the code so infinity times

00:19:46,060 --> 00:19:50,740
before when we do the joshua calculation

00:19:48,610 --> 00:19:53,230
we have a two-body potential actually we

00:19:50,740 --> 00:19:55,420
store the whole matrix when we proposed

00:19:53,230 --> 00:19:58,450
a move you just computer scratch state

00:19:55,420 --> 00:20:00,250
of this proposed move and if it's been

00:19:58,450 --> 00:20:01,350
accepted there you have to advertise one

00:20:00,250 --> 00:20:04,380
row and one column

00:20:01,350 --> 00:20:07,350
which is clearly not efficient for

00:20:04,380 --> 00:20:09,450
memory but actually for the calculation

00:20:07,350 --> 00:20:11,759
all you only need the decimation of this

00:20:09,450 --> 00:20:13,679
potential over J so you only need the

00:20:11,759 --> 00:20:16,110
summation of the row but that's why I

00:20:13,679 --> 00:20:19,409
change it here so it becomes just a

00:20:16,110 --> 00:20:22,230
column of summations now what you do is

00:20:19,409 --> 00:20:24,330
you do an appetizer organism when you

00:20:22,230 --> 00:20:26,789
propose a move actually you update

00:20:24,330 --> 00:20:29,580
automation by subtracting the old value

00:20:26,789 --> 00:20:31,500
and add back the new value and for this

00:20:29,580 --> 00:20:33,419
particular moved particle you just put a

00:20:31,500 --> 00:20:37,250
new value directly that making a

00:20:33,419 --> 00:20:40,139
numerical very robust so with this

00:20:37,250 --> 00:20:42,620
algorithm change you will find actually

00:20:40,139 --> 00:20:47,820
we are all the always dealing vectors so

00:20:42,620 --> 00:20:51,059
perfect so with this change actually

00:20:47,820 --> 00:20:53,519
also save a lot of a memory for in the

00:20:51,059 --> 00:20:58,259
java factor which clearly we don't want

00:20:53,519 --> 00:21:00,509
for this cheaper cost kernels and we can

00:20:58,259 --> 00:21:03,960
give more memory to the splines we

00:21:00,509 --> 00:21:07,019
really care about big systems so and

00:21:03,960 --> 00:21:10,559
also on Canyon system you you see the

00:21:07,019 --> 00:21:15,059
SVM and routine performance clearly so

00:21:10,559 --> 00:21:18,480
by the by those simply optimization and

00:21:15,059 --> 00:21:21,659
or ISM change actually on those kernels

00:21:18,480 --> 00:21:24,149
like AJ to the true body just row the

00:21:21,659 --> 00:21:26,250
distance tables in this roof line

00:21:24,149 --> 00:21:28,320
analysis I don't know if you're familiar

00:21:26,250 --> 00:21:29,909
with loops prime analysis basically if

00:21:28,320 --> 00:21:34,200
you go that direction is the ideal

00:21:29,909 --> 00:21:37,440
direction so you get flops and then the

00:21:34,200 --> 00:21:40,019
the memory access is the less frequent

00:21:37,440 --> 00:21:41,879
and you gain a lot of performance so we

00:21:40,019 --> 00:21:45,539
basically you see the breakout of

00:21:41,879 --> 00:21:48,269
different kernels in the reference code

00:21:45,539 --> 00:21:50,970
and the optimized code there's a huge

00:21:48,269 --> 00:21:56,669
reduction and we basically in a factor

00:21:50,970 --> 00:22:00,809
of 2 X across most platforms so this is

00:21:56,669 --> 00:22:05,070
a benchmark we did on several systems

00:22:00,809 --> 00:22:07,799
i/o system and system sizes like this is

00:22:05,070 --> 00:22:10,379
the largest calculation and on different

00:22:07,799 --> 00:22:13,049
processors losing to the very old one

00:22:10,379 --> 00:22:15,000
actually we are happy with you we were

00:22:13,049 --> 00:22:16,769
still running the blue jeans.you and we

00:22:15,000 --> 00:22:18,379
are happy that we can plot perform

00:22:16,769 --> 00:22:21,600
across on blue jeans.you

00:22:18,379 --> 00:22:27,539
because Excel doesn't provide new

00:22:21,600 --> 00:22:29,940
compilers try to climb high then we have

00:22:27,539 --> 00:22:32,370
the Broadway antenna also they all get

00:22:29,940 --> 00:22:36,929
significant boost in performance with

00:22:32,370 --> 00:22:41,460
our optimization so just a quick summary

00:22:36,929 --> 00:22:43,889
of the previous part so openmp helps us

00:22:41,460 --> 00:22:45,919
the share memory help us to solve the

00:22:43,889 --> 00:22:49,370
memory issue of the big fifth line table

00:22:45,919 --> 00:22:52,549
it expresses our unknown parallelism

00:22:49,370 --> 00:22:55,710
including spreading and vectorizing

00:22:52,549 --> 00:22:58,590
factorization it enables us to write

00:22:55,710 --> 00:23:01,379
very clean and understandable code and

00:22:58,590 --> 00:23:04,760
it gives us perfect rest Kaling and

00:23:01,379 --> 00:23:10,700
cindy efficiency and how about

00:23:04,760 --> 00:23:15,990
accelerators so we start to experiment

00:23:10,700 --> 00:23:19,879
open to upload to the accelerators so

00:23:15,990 --> 00:23:23,669
while we need to try this so in the past

00:23:19,879 --> 00:23:27,179
CINCPAC actually as i mentioned has open

00:23:23,669 --> 00:23:30,720
T and CUDA so internally in the code is

00:23:27,179 --> 00:23:33,679
like two big Forks and they are largely

00:23:30,720 --> 00:23:36,690
incompatible apart from the Danish

00:23:33,679 --> 00:23:39,500
initialization maybe some IO there's the

00:23:36,690 --> 00:23:41,669
same an input file may be the same but

00:23:39,500 --> 00:23:43,650
internally in the computing they are

00:23:41,669 --> 00:23:47,850
likely to join fault

00:23:43,650 --> 00:23:49,800
and so also makes us not but it becomes

00:23:47,850 --> 00:23:51,870
not possible with we can never provide

00:23:49,800 --> 00:23:54,440
an easy fullback solution if the

00:23:51,870 --> 00:23:58,070
implementation is missing on one side

00:23:54,440 --> 00:24:01,280
usually the GPU side that's why the GPU

00:23:58,070 --> 00:24:03,960
features much less than the CPU code and

00:24:01,280 --> 00:24:07,260
we always like to have the developers

00:24:03,960 --> 00:24:10,080
and hard to especially how to get

00:24:07,260 --> 00:24:13,440
someone knows both and just double the

00:24:10,080 --> 00:24:15,570
time to implement both kernels so in

00:24:13,440 --> 00:24:18,920
town long term we really need a portable

00:24:15,570 --> 00:24:23,700
solution and we don't like to depend on

00:24:18,920 --> 00:24:26,250
proprietary solutions and performance

00:24:23,700 --> 00:24:27,710
portable a portable performance is

00:24:26,250 --> 00:24:32,340
desired

00:24:27,710 --> 00:24:35,760
so performance for the ability we'd like

00:24:32,340 --> 00:24:39,240
to attach the capability of open tea in

00:24:35,760 --> 00:24:43,890
under those criteria it needs to be run

00:24:39,240 --> 00:24:47,570
on both CPU and GPU so first is the

00:24:43,890 --> 00:24:51,540
capability can we express our required

00:24:47,570 --> 00:24:54,830
concurrency with open tea and second is

00:24:51,540 --> 00:24:58,200
the performance good and portability

00:24:54,830 --> 00:25:01,260
what is the extent of required changes

00:24:58,200 --> 00:25:04,380
to make them running well on both

00:25:01,260 --> 00:25:11,010
architectures and how about the

00:25:04,380 --> 00:25:13,470
compilers libraries and tool so we don't

00:25:11,010 --> 00:25:17,400
try it a directly options impact is too

00:25:13,470 --> 00:25:20,700
too too much workload mean eternity so

00:25:17,400 --> 00:25:23,270
during our structure of optimizations we

00:25:20,700 --> 00:25:27,630
developed a set of mini apps

00:25:23,270 --> 00:25:31,260
specifically focuses pack they are

00:25:27,630 --> 00:25:37,300
standalone means you they are separated

00:25:31,260 --> 00:25:40,060
from the volume security attacks then

00:25:37,300 --> 00:25:43,300
okay yeah they are separate from CINCPAC

00:25:40,060 --> 00:25:46,150
was this specific to purpose the code

00:25:43,300 --> 00:25:47,800
actually expressed the same concurrency

00:25:46,150 --> 00:25:50,290
and customs effect so all the

00:25:47,800 --> 00:25:53,050
parallelism levels a place there and

00:25:50,290 --> 00:25:55,450
they use the state-of-art algorithm so

00:25:53,050 --> 00:25:57,580
all the legacy augur isms we have only

00:25:55,450 --> 00:26:05,040
two or the legacy origin we all have on

00:25:57,580 --> 00:26:05,040
come on GPU we just pick the best one

00:26:05,100 --> 00:26:11,590
they are designed to collaborate with

00:26:07,210 --> 00:26:14,230
non currency developers and we do it

00:26:11,590 --> 00:26:16,450
would be public on github by the end of

00:26:14,230 --> 00:26:20,680
this month so it's one of the

00:26:16,450 --> 00:26:22,360
deliverables or the ETP project so here

00:26:20,680 --> 00:26:25,440
I'm going to talk about

00:26:22,360 --> 00:26:30,610
I tried offloading things on this piece

00:26:25,440 --> 00:26:33,640
three D cubed B spline kernel this

00:26:30,610 --> 00:26:36,070
kernel is the characteristics of this

00:26:33,640 --> 00:26:38,410
Pronovias it's a base memory bound so

00:26:36,070 --> 00:26:41,800
Timothy fact as I mentioned we have many

00:26:38,410 --> 00:26:44,670
kernels some member bansemer computing

00:26:41,800 --> 00:26:48,070
compute bound this work is memory bound

00:26:44,670 --> 00:26:52,780
so I take I start from the meaning of

00:26:48,070 --> 00:26:55,360
this original version so if the two

00:26:52,780 --> 00:26:58,060
level parallelism is I wrote them

00:26:55,360 --> 00:26:59,890
separately on the left and right side so

00:26:58,060 --> 00:27:02,440
basically this side you have a parallel

00:26:59,890 --> 00:27:04,990
parallel so over all the workers just to

00:27:02,440 --> 00:27:08,590
make mimic the worker parallelism and

00:27:04,990 --> 00:27:09,660
this is the computing of set of the

00:27:08,590 --> 00:27:12,880
supply orbitals

00:27:09,660 --> 00:27:19,390
and we have oMG Cindy I mentioned

00:27:12,880 --> 00:27:23,030
earlier so let me see yeah so this bill

00:27:19,390 --> 00:27:25,710
deals with a single worker business

00:27:23,030 --> 00:27:29,700
however in this implementation might

00:27:25,710 --> 00:27:34,260
notice this inner loop i'il de which is

00:27:29,700 --> 00:27:37,830
sequential so constitute scientist okay

00:27:34,260 --> 00:27:41,940
Ranjit aside it's a problem that's why

00:27:37,830 --> 00:27:43,650
in the qmc pact pseudocode that I just

00:27:41,940 --> 00:27:45,540
repeat this here in the GPU

00:27:43,650 --> 00:27:48,540
implementation loop two and four are

00:27:45,540 --> 00:27:50,640
interchanged so this sequential loop is

00:27:48,540 --> 00:27:54,210
bring out the Walker loop is bringing

00:27:50,640 --> 00:27:56,940
and then they can be launched within a

00:27:54,210 --> 00:28:03,650
single kernel launch on the GPU just to

00:27:56,940 --> 00:28:07,160
gain efficiency so what I do to for the

00:28:03,650 --> 00:28:11,300
people uploading to the device

00:28:07,160 --> 00:28:11,300
how many minutes I have Oh

00:28:14,210 --> 00:28:22,970
okay good I think I can do it this one

00:28:20,480 --> 00:28:25,190
of the first changes to the v1 is to

00:28:22,970 --> 00:28:28,340
interchange those loops so I bring this

00:28:25,190 --> 00:28:31,639
AI electron single particle move loops

00:28:28,340 --> 00:28:33,860
out and bring the Walker loop inside so

00:28:31,639 --> 00:28:36,889
by this change you actually need to

00:28:33,860 --> 00:28:39,619
prepare those /they charges for

00:28:36,889 --> 00:28:43,869
collecting the results or or the workers

00:28:39,619 --> 00:28:48,769
then you I add back those targets

00:28:43,869 --> 00:28:51,470
pragmas I still lastly the Orion

00:28:48,769 --> 00:28:54,919
propeller for this is so the blue ones

00:28:51,470 --> 00:28:57,350
are for the CPU so when you run if you

00:28:54,919 --> 00:29:03,440
you have those pragma then when you runs

00:28:57,350 --> 00:29:05,299
if you you have those red pragma yes so

00:29:03,440 --> 00:29:08,480
if they're slightly different I'm not

00:29:05,299 --> 00:29:12,440
using a single pragma for just to make

00:29:08,480 --> 00:29:14,869
sure I get I get performance and the

00:29:12,440 --> 00:29:20,240
compiler is doing what I tell I told

00:29:14,869 --> 00:29:22,850
them exactly are on different devices so

00:29:20,240 --> 00:29:25,940
basically in this in this implementation

00:29:22,850 --> 00:29:32,450
this becomes a single kernel launch on

00:29:25,940 --> 00:29:34,789
the GPUs and I did a bit more so the

00:29:32,450 --> 00:29:37,220
data mapping is a base bit painful I

00:29:34,789 --> 00:29:39,649
don't want to do it everywhere and since

00:29:37,220 --> 00:29:42,710
the vector type infinity pack is a major

00:29:39,649 --> 00:29:46,039
type vector or SOA vector is the major

00:29:42,710 --> 00:29:48,350
type so I just do a MV vector just found

00:29:46,039 --> 00:29:54,830
a part of them is found down there on a

00:29:48,350 --> 00:29:58,730
tutorial to attempt just based on

00:29:54,830 --> 00:30:02,300
the drive to form on the base type then

00:29:58,730 --> 00:30:05,150
what I do extra is during a resize I do

00:30:02,300 --> 00:30:07,850
the data enter data mapping just messing

00:30:05,150 --> 00:30:10,460
the data pointers and then I just

00:30:07,850 --> 00:30:12,950
implement those after device Saturday

00:30:10,460 --> 00:30:13,940
from device if you really need to do

00:30:12,950 --> 00:30:16,550
that

00:30:13,940 --> 00:30:18,440
in terms the implementation most

00:30:16,550 --> 00:30:20,810
circulations on GPUs but you don't need

00:30:18,440 --> 00:30:23,330
to back there for send data but like I

00:30:20,810 --> 00:30:25,520
do a mini app I need to check the result

00:30:23,330 --> 00:30:27,710
so as to copy the data so you just you

00:30:25,520 --> 00:30:30,200
implement those function helper

00:30:27,710 --> 00:30:33,170
functions and hide all the details

00:30:30,200 --> 00:30:36,170
inside and then the destructor you just

00:30:33,170 --> 00:30:38,360
do the exit data so this in the code

00:30:36,170 --> 00:30:41,800
basically we change the data type from a

00:30:38,360 --> 00:30:46,010
vector type to our only vector type and

00:30:41,800 --> 00:30:50,570
for glue I think I'm going gather the

00:30:46,010 --> 00:30:52,820
walkers to launch a single kernel I need

00:30:50,570 --> 00:30:55,400
to collect a bunch of pointers and do

00:30:52,820 --> 00:30:58,790
the people copy to the device so I

00:30:55,400 --> 00:31:01,190
actually can reuse this data type just

00:30:58,790 --> 00:31:05,120
to collect the pointers and also save me

00:31:01,190 --> 00:31:09,190
a lot of lines of code in I'm quite

00:31:05,120 --> 00:31:13,970
happy with this kind of hiding things

00:31:09,190 --> 00:31:16,370
yes so if we we have the CUDA code and I

00:31:13,970 --> 00:31:19,850
know the Buddha code so I was thinking

00:31:16,370 --> 00:31:24,470
of what was exactly the CUDA code doing

00:31:19,850 --> 00:31:29,930
in terms of implementation details so I

00:31:24,470 --> 00:31:32,960
was thinking that so I I see that there

00:31:29,930 --> 00:31:36,530
is the kind of loop interchange in the

00:31:32,960 --> 00:31:40,160
CUDA implementation the the um the major

00:31:36,530 --> 00:31:43,220
changes the spline calculation is sorry

00:31:40,160 --> 00:31:45,290
I missed one thing earlier this panic

00:31:43,220 --> 00:31:48,149
spline psychology okay let's look at the

00:31:45,290 --> 00:31:52,169
speculation details so it's

00:31:48,149 --> 00:31:54,419
XYZ 303 dimension then you get four

00:31:52,169 --> 00:31:56,669
points in the X you do interpolation for

00:31:54,419 --> 00:31:59,669
the to the position one to calculate Y

00:31:56,669 --> 00:32:03,629
four points these points so I write this

00:31:59,669 --> 00:32:07,500
pinko XYZ loop ijk loop four for the

00:32:03,629 --> 00:32:10,529
four point the 64 points in total spline

00:32:07,500 --> 00:32:14,970
interpolation then the innermost is loop

00:32:10,529 --> 00:32:17,309
over all the bands so it's the sizes the

00:32:14,970 --> 00:32:21,629
number of electrons so it's a very large

00:32:17,309 --> 00:32:25,200
loop this one fits very well on GPUs

00:32:21,629 --> 00:32:27,690
because these days how in cash then it's

00:32:25,200 --> 00:32:30,360
just screaming to the vector units and

00:32:27,690 --> 00:32:31,110
do the computing so it's a divisions on

00:32:30,360 --> 00:32:35,090
the CPU

00:32:31,110 --> 00:32:38,340
but however on the GPU sites usually win

00:32:35,090 --> 00:32:39,960
we know that writing back refers to the

00:32:38,340 --> 00:32:41,460
global memory in the best thing so

00:32:39,960 --> 00:32:43,740
that's why I decide to do this change

00:32:41,460 --> 00:32:47,029
I'd move the loop out the England most

00:32:43,740 --> 00:32:50,610
loop out just create a local variable

00:32:47,029 --> 00:32:54,240
just to do a temporal one and do the

00:32:50,610 --> 00:32:57,929
computing reduction okay

00:32:54,240 --> 00:32:59,909
reduction just on this sorry I didn't

00:32:57,929 --> 00:33:02,460
write plus you should be processing

00:32:59,909 --> 00:33:04,559
codes so you should do that through the

00:33:02,460 --> 00:33:09,450
reduction on this temporal array and the

00:33:04,559 --> 00:33:12,779
end just write back the result so I this

00:33:09,450 --> 00:33:17,610
is more close to the 32 you so here we

00:33:12,779 --> 00:33:21,419
look at the performance so to gain from

00:33:17,610 --> 00:33:26,639
on the city from v-0 to v1 because to go

00:33:21,419 --> 00:33:29,820
is to gain to reduce the over colonel

00:33:26,639 --> 00:33:33,480
launching overhead of the curve so for

00:33:29,820 --> 00:33:36,600
the auto target region so we we need to

00:33:33,480 --> 00:33:39,450
fuse the walkers into a single call to a

00:33:36,600 --> 00:33:41,899
single kernel so but however this will

00:33:39,450 --> 00:33:44,519
require more think fork/join

00:33:41,899 --> 00:33:46,950
synchronization on the CPU side so I was

00:33:44,519 --> 00:33:49,789
checking there's how much performance we

00:33:46,950 --> 00:33:49,789
lose

00:33:49,860 --> 00:33:57,610
just weird is a shadow next to it so for

00:33:55,750 --> 00:34:00,550
this motorized calculation actually on

00:33:57,610 --> 00:34:04,720
city side we lose a bit performance then

00:34:00,550 --> 00:34:09,159
we run this v1 on GPU of the performance

00:34:04,720 --> 00:34:12,730
is not nice is a much much slower

00:34:09,159 --> 00:34:15,790
compared to the C - oh okay by the way

00:34:12,730 --> 00:34:18,790
doing on the targets and faster if you

00:34:15,790 --> 00:34:21,970
mean poly accelerators or the device

00:34:18,790 --> 00:34:25,659
with a client compiler so then I switch

00:34:21,970 --> 00:34:28,060
to the v2 and clearly we lose

00:34:25,659 --> 00:34:30,580
performance on the cpu compared to the

00:34:28,060 --> 00:34:35,050
v1 but I gain performance on the GPU

00:34:30,580 --> 00:34:37,659
still not very satisfactory so then I

00:34:35,050 --> 00:34:40,750
see let me see medium-sized maybe

00:34:37,659 --> 00:34:45,520
because my my workload is too light yeah

00:34:40,750 --> 00:34:47,550
it actually changed quite a lot so the

00:34:45,520 --> 00:34:52,570
function of the head it seems calm and

00:34:47,550 --> 00:34:57,520
seems that d1 could run as fast as v-0

00:34:52,570 --> 00:35:00,280
on the under on the CPU side then the

00:34:57,520 --> 00:35:03,550
GPU performance of us feels much slower

00:35:00,280 --> 00:35:05,860
and then the CPU version however I when

00:35:03,550 --> 00:35:09,550
I change to the v2 actually the

00:35:05,860 --> 00:35:13,450
performance now was much better than not

00:35:09,550 --> 00:35:18,670
much better than the CPU the reference

00:35:13,450 --> 00:35:21,610
performance so yeah sadly now we have

00:35:18,670 --> 00:35:22,750
one version good for c2 and one person

00:35:21,610 --> 00:35:26,560
good for GPU

00:35:22,750 --> 00:35:30,420
so I start to look at the details of

00:35:26,560 --> 00:35:32,800
this closure and especially so because

00:35:30,420 --> 00:35:35,350
when you do GPU you know that it's

00:35:32,800 --> 00:35:38,260
really important to look at the register

00:35:35,350 --> 00:35:40,270
press well-hung and also the occupancy

00:35:38,260 --> 00:35:43,210
so what I found is unfortunately the

00:35:40,270 --> 00:35:47,240
registrant is high and occupancy is low

00:35:43,210 --> 00:35:51,260
and we have the CUDA kernels

00:35:47,240 --> 00:35:53,450
and it's about 50 register views so I

00:35:51,260 --> 00:35:55,970
think there are improvements we can do

00:35:53,450 --> 00:35:59,810
and clearly on our cooler implementation

00:35:55,970 --> 00:36:03,320
we use share memory to hold the DGP you

00:35:59,810 --> 00:36:08,240
share memory to hold the the current

00:36:03,320 --> 00:36:12,170
kind of constant and the constant data

00:36:08,240 --> 00:36:14,589
and fixed size so in the CUDA

00:36:12,170 --> 00:36:17,330
implementation that way statehood GPO

00:36:14,589 --> 00:36:19,310
registers but unfortunately we could not

00:36:17,330 --> 00:36:23,060
do it at least for a moment I don't know

00:36:19,310 --> 00:36:25,730
how to do it and I could measure the

00:36:23,060 --> 00:36:28,700
higher the DDR bandwidth so high

00:36:25,730 --> 00:36:31,250
bandwidth memory on computers the DDR

00:36:28,700 --> 00:36:33,890
bandwidth when you read a metric so we

00:36:31,250 --> 00:36:38,540
are I'm only getting 80 gigabytes per

00:36:33,890 --> 00:36:40,280
second and clearly it's very low at the

00:36:38,540 --> 00:36:42,650
moment I think this problem is

00:36:40,280 --> 00:36:46,430
correlated with low occupancy because

00:36:42,650 --> 00:36:48,470
you could not parallel long in more

00:36:46,430 --> 00:36:51,320
requests to the memory to gather data so

00:36:48,470 --> 00:36:54,260
I think we can still work on this aspect

00:36:51,320 --> 00:36:56,240
to improve the performance and the

00:36:54,260 --> 00:36:58,040
single source with possible performance

00:36:56,240 --> 00:37:01,869
is not achieved at the moment on this

00:36:58,040 --> 00:37:05,240
kernel I hope the compiler can do some

00:37:01,869 --> 00:37:05,599
some looper reordering this kind of

00:37:05,240 --> 00:37:07,670
things

00:37:05,599 --> 00:37:10,580
maybe it's the challenge we will see and

00:37:07,670 --> 00:37:13,970
the compiler quality is improving over

00:37:10,580 --> 00:37:16,099
time it takes time I think there is

00:37:13,970 --> 00:37:18,650
application developer we can find more

00:37:16,099 --> 00:37:20,690
bug and help the developer or the

00:37:18,650 --> 00:37:25,220
compiler developer to improve the

00:37:20,690 --> 00:37:26,960
compilers and we I would like to have

00:37:25,220 --> 00:37:30,230
more accessible information we are

00:37:26,960 --> 00:37:34,220
compiler optimization report so I don't

00:37:30,230 --> 00:37:36,920
need to deal with a black box for the

00:37:34,220 --> 00:37:38,660
moment the performance tools I think is

00:37:36,920 --> 00:37:40,539
only a detour I don't know is there

00:37:38,660 --> 00:37:46,019
other

00:37:40,539 --> 00:37:51,160
yeah this is near the last okay

00:37:46,019 --> 00:37:56,919
perspective I still strongly believe in

00:37:51,160 --> 00:37:59,650
of the MP I in the following month and

00:37:56,919 --> 00:38:02,259
we try more kernels to do this open G

00:37:59,650 --> 00:38:05,019
offloading and I will try the other

00:38:02,259 --> 00:38:06,749
computer on kernels then we will have

00:38:05,019 --> 00:38:10,179
more understanding of the current

00:38:06,749 --> 00:38:13,630
situation of in terms of performance 420

00:38:10,179 --> 00:38:16,239
and there is a practical reason to solve

00:38:13,630 --> 00:38:17,619
the issues I mentioned earlier so I

00:38:16,239 --> 00:38:20,169
follow this 8020 rule

00:38:17,619 --> 00:38:22,269
so 80 routines takes only 20 to Central

00:38:20,169 --> 00:38:25,689
Time so in all the way we have to write

00:38:22,269 --> 00:38:28,209
C to code and GPU code and the GPU code

00:38:25,689 --> 00:38:30,969
is needed only because we do not want to

00:38:28,209 --> 00:38:32,799
move the data back and forth we don't

00:38:30,969 --> 00:38:35,650
really care that much about performance

00:38:32,799 --> 00:38:40,660
so if we can write in open p in a single

00:38:35,650 --> 00:38:42,729
source that's great I mean we don't

00:38:40,660 --> 00:38:46,150
design much on those for the 20%

00:38:42,729 --> 00:38:48,309
routines taking 80 percent of time we

00:38:46,150 --> 00:38:53,650
really like to have performance portable

00:38:48,309 --> 00:38:57,279
code but to be reality it's really too

00:38:53,650 --> 00:39:03,689
hard we could do a minimum I attached a

00:38:57,279 --> 00:39:03,689
specific code okay yes thanks everyone

00:39:03,740 --> 00:39:05,770
Oh

00:39:08,990 --> 00:39:11,770
do we

00:39:14,400 --> 00:39:19,619
I was just wondering how many

00:39:21,850 --> 00:39:25,000
[Music]

00:39:35,450 --> 00:39:40,080
that not fact

00:39:37,910 --> 00:39:43,070
one you want

00:39:40,080 --> 00:39:43,070
all in the world

00:39:43,730 --> 00:39:51,170
it's irrelevant to the number of notes

00:39:46,069 --> 00:39:55,309
that only tears - per node memory that

00:39:51,170 --> 00:39:58,700
memory on no it's replicated exactly

00:39:55,309 --> 00:40:07,240
replicated on every node it's for the

00:39:58,700 --> 00:40:10,339
same or every node yeah on a single node

00:40:07,240 --> 00:40:13,069
it's not worth to distributing them to

00:40:10,339 --> 00:40:16,520
use the access path and decide you need

00:40:13,069 --> 00:40:19,809
very fast access and chunk of data is

00:40:16,520 --> 00:40:19,809
always small so

00:40:25,170 --> 00:40:28,170
No

00:40:28,250 --> 00:40:34,849
[Music]

00:40:30,380 --> 00:40:38,150
so the sprang date had yes we we would

00:40:34,849 --> 00:40:41,869
like to have so on the currency to codes

00:40:38,150 --> 00:40:44,239
I implemented virtual UVM unified

00:40:41,869 --> 00:40:48,259
virtual addressing which is kind of slow

00:40:44,239 --> 00:40:51,170
but makes my science going so it's so

00:40:48,259 --> 00:40:53,720
good I mean it's a from you can run some

00:40:51,170 --> 00:40:57,019
you cannot run it and just infinitely

00:40:53,720 --> 00:40:59,089
that right there are internal

00:40:57,019 --> 00:41:01,730
performance yes it's not great

00:40:59,089 --> 00:41:06,319
so the unifying memory I think them the

00:41:01,730 --> 00:41:09,380
Kudus 781 not the old one yes it will

00:41:06,319 --> 00:41:12,380
help us I think for the current

00:41:09,380 --> 00:41:15,880
technology for the of the nd linked to

00:41:12,380 --> 00:41:18,950
the performance should be better however

00:41:15,880 --> 00:41:22,249
if we really need the performance I

00:41:18,950 --> 00:41:25,880
think I can do more than unified memory

00:41:22,249 --> 00:41:27,710
I can for example with this we want to

00:41:25,880 --> 00:41:30,470
v-0 change with use all the workers

00:41:27,710 --> 00:41:33,079
actually we can do most mark things we

00:41:30,470 --> 00:41:36,019
can even sing certain things to the

00:41:33,079 --> 00:41:39,170
Joshua part doesn't care about the black

00:41:36,019 --> 00:41:42,470
positions so we can actually staged the

00:41:39,170 --> 00:41:45,349
spline positions before the calculation

00:41:42,470 --> 00:41:48,049
while that if you was busy with the

00:41:45,349 --> 00:41:51,019
gestural factor calculations then you

00:41:48,049 --> 00:41:53,180
just learn when the data has arrived you

00:41:51,019 --> 00:41:56,480
pack the state high launch it to make

00:41:53,180 --> 00:41:58,819
very faster so it's really depends on

00:41:56,480 --> 00:42:03,789
what you need a laser solution or

00:41:58,819 --> 00:42:06,440
ultimate performing solutions yes but I

00:42:03,789 --> 00:42:09,380
think a lot of the easy things like the

00:42:06,440 --> 00:42:12,259
data mapping most more data movement

00:42:09,380 --> 00:42:15,859
area is definitely I don't want to care

00:42:12,259 --> 00:42:19,089
I don't want to maintain or explicitly

00:42:15,859 --> 00:42:19,089
control run yeah

00:42:23,650 --> 00:42:40,519
[Music]

00:42:42,680 --> 00:42:51,440
[Music]

00:42:45,050 --> 00:42:53,900
oh you mean MPI shared-memory no we are

00:42:51,440 --> 00:42:56,270
not doing that at this moment we have

00:42:53,900 --> 00:43:00,140
some considerations there for the

00:42:56,270 --> 00:43:04,460
biggest client able to put it on a few

00:43:00,140 --> 00:43:06,230
closeby notes for some how as I said you

00:43:04,460 --> 00:43:08,600
need to manage the data manually so you

00:43:06,230 --> 00:43:10,670
have to be careful so these the

00:43:08,600 --> 00:43:13,280
difference algorithm I have different

00:43:10,670 --> 00:43:18,100
purpose now are entangled so it's just

00:43:13,280 --> 00:43:18,100
take time to figure out one by one

00:43:18,230 --> 00:43:20,290
you

00:43:23,090 --> 00:43:26,050

YouTube URL: https://www.youtube.com/watch?v=rRBWDg7R9fw


