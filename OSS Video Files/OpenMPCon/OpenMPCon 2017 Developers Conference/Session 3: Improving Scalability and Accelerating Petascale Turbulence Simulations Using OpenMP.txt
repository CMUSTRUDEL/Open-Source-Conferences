Title: Session 3: Improving Scalability and Accelerating Petascale Turbulence Simulations Using OpenMP
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	OPENMPCON SEPTEMBER 2017: Session 3
Improving Scalability and Accelerating Petascale Turbulence Simulations Using OpenMP
Matthew Clay, Dhawal Buaria and P.K. Yeung
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day1-Session1-Clay.pdf
NOTE: VIDEO STARTS 3 MINUTES INTO TALK.
Captions: 
	00:00:00,230 --> 00:00:07,470
but so we we kind of learn through going

00:00:05,430 --> 00:00:11,070
to these types of workshops and and user

00:00:07,470 --> 00:00:14,549
workshops at NCSA and LCF and then try

00:00:11,070 --> 00:00:18,090
to use that new knowledge and our

00:00:14,549 --> 00:00:20,010
program to improve their performance but

00:00:18,090 --> 00:00:22,500
but what are our major objectives when

00:00:20,010 --> 00:00:25,859
we use OpenMP so we typically start with

00:00:22,500 --> 00:00:27,900
a pure MPI application for a fluid

00:00:25,859 --> 00:00:29,220
dynamic simulation and all in the coming

00:00:27,900 --> 00:00:32,340
slides I'll talk a little bit more about

00:00:29,220 --> 00:00:34,620
the physics but we have two different

00:00:32,340 --> 00:00:37,110
goals for the type of architecture we're

00:00:34,620 --> 00:00:40,440
running on so for a machine like blue

00:00:37,110 --> 00:00:42,210
water is a homogeneous machine we will

00:00:40,440 --> 00:00:44,100
try to use openmp to improve the

00:00:42,210 --> 00:00:46,289
scalability of the application to very

00:00:44,100 --> 00:00:48,059
large problem sizes so we'll have some

00:00:46,289 --> 00:00:50,910
simulations we want to run and when we

00:00:48,059 --> 00:00:53,370
scale them up the scalability of the

00:00:50,910 --> 00:00:55,800
program will fall but can we use OpenMP

00:00:53,370 --> 00:00:58,649
to try to mitigate some of the some of

00:00:55,800 --> 00:01:00,449
the impact the negative impact of

00:00:58,649 --> 00:01:02,520
scaling up but in heterogeneous

00:01:00,449 --> 00:01:04,049
computing environments not only do we

00:01:02,520 --> 00:01:06,119
want to achieve good scalability and

00:01:04,049 --> 00:01:08,250
efficiency at large problem sizes but

00:01:06,119 --> 00:01:10,439
there's also this problem of the Excel

00:01:08,250 --> 00:01:12,030
Excel rating the loops so we have to

00:01:10,439 --> 00:01:14,040
make sure that we're effectively using

00:01:12,030 --> 00:01:17,369
the GPUs in getting high performance

00:01:14,040 --> 00:01:21,150
that they offer so to briefly introduce

00:01:17,369 --> 00:01:23,009
the physics we are in the field of

00:01:21,150 --> 00:01:25,110
computational fluid dynamics and we

00:01:23,009 --> 00:01:28,590
study turbulence and and turbulent

00:01:25,110 --> 00:01:30,479
processes and fluid motion so turbulence

00:01:28,590 --> 00:01:33,360
is very important in our everyday lives

00:01:30,479 --> 00:01:35,159
maybe on some of your flights here you

00:01:33,360 --> 00:01:37,259
experience some turbulence and the

00:01:35,159 --> 00:01:39,689
airplane but turbulence is very

00:01:37,259 --> 00:01:42,630
ubiquitous in nature and engineering we

00:01:39,689 --> 00:01:44,670
rely on it for a lot of for a lot of

00:01:42,630 --> 00:01:46,939
applications and we need to understand

00:01:44,670 --> 00:01:49,380
that turbulence is very challenging

00:01:46,939 --> 00:01:50,759
because turbo flows are

00:01:49,380 --> 00:01:53,610
three-dimensional and they are they're

00:01:50,759 --> 00:01:55,649
multi scale and integer so we have a

00:01:53,610 --> 00:01:57,990
wide range of scales in both space and

00:01:55,649 --> 00:01:59,280
time that we have to resolve in a

00:01:57,990 --> 00:02:03,119
simulation or

00:01:59,280 --> 00:02:05,670
an experiment in a laboratory and the

00:02:03,119 --> 00:02:08,520
result is that the computations for

00:02:05,670 --> 00:02:12,030
realistic systems are are very large so

00:02:08,520 --> 00:02:15,060
the computational grids you used in some

00:02:12,030 --> 00:02:17,910
of the most cutting-edge simulations to

00:02:15,060 --> 00:02:20,160
date are of the order of 8192 cube grid

00:02:17,910 --> 00:02:22,530
point which my advisor has been working

00:02:20,160 --> 00:02:24,330
on blue waters is about half a trillion

00:02:22,530 --> 00:02:26,760
grid points and then there are some

00:02:24,330 --> 00:02:31,110
folks in the Japanese group who have

00:02:26,760 --> 00:02:33,030
gone up to 12 K cube to maybe try to

00:02:31,110 --> 00:02:35,730
draw some comparison I've worked with

00:02:33,030 --> 00:02:38,459
other labs in the past who who do more

00:02:35,730 --> 00:02:41,190
application size problems and maybe

00:02:38,459 --> 00:02:43,380
something on the order of 512 cubed good

00:02:41,190 --> 00:02:44,850
points might be more realistic for say

00:02:43,380 --> 00:02:46,739
an application problem that you would

00:02:44,850 --> 00:02:48,540
want to do in CFD but when we focus on

00:02:46,739 --> 00:02:49,890
the fundamentals of turbulence we have

00:02:48,540 --> 00:02:53,190
to have computations that are much

00:02:49,890 --> 00:02:57,930
larger than than you might see and in

00:02:53,190 --> 00:02:58,739
normal situations so but what are we

00:02:57,930 --> 00:03:01,769
going to focus on

00:02:58,739 --> 00:03:05,190
so we turbulence is kind of a state of

00:03:01,769 --> 00:03:08,459
fluid motion and there is a turbulent

00:03:05,190 --> 00:03:10,680
velocity field which will drive mixing

00:03:08,459 --> 00:03:12,540
processes in many applications so what

00:03:10,680 --> 00:03:15,600
we want to study what our main focus is

00:03:12,540 --> 00:03:17,820
on is studying the mixing of a substance

00:03:15,600 --> 00:03:19,829
in a turbulent flow so the idea is that

00:03:17,820 --> 00:03:21,209
if you take your coffee and you stir it

00:03:19,829 --> 00:03:22,769
up in the morning and you put some cream

00:03:21,209 --> 00:03:24,930
in it how does the turbulent velocity

00:03:22,769 --> 00:03:28,560
field in the cost and the coffee speed

00:03:24,930 --> 00:03:31,590
up the mixing process of the cream so

00:03:28,560 --> 00:03:33,180
the this is our main science objective

00:03:31,590 --> 00:03:35,459
for the codes that we've been working on

00:03:33,180 --> 00:03:38,940
and I don't want to get too much into

00:03:35,459 --> 00:03:40,920
the physics but it there are some

00:03:38,940 --> 00:03:42,660
important physical parameters which we

00:03:40,920 --> 00:03:45,600
must keep in mind when we run the

00:03:42,660 --> 00:03:46,920
simulation for for simulations of

00:03:45,600 --> 00:03:48,840
turbulence we have very strict

00:03:46,920 --> 00:03:51,690
resolution requirements that we must

00:03:48,840 --> 00:03:53,489
adhere to otherwise the results that we

00:03:51,690 --> 00:03:56,370
would get would not be very physical or

00:03:53,489 --> 00:03:58,500
or worth our time trying to understand

00:03:56,370 --> 00:04:00,000
so we have very strict resolution

00:03:58,500 --> 00:04:02,040
requirements based on the physics

00:04:00,000 --> 00:04:04,890
and when you consider a substance being

00:04:02,040 --> 00:04:06,720
mix in a turbulent flow the resolution

00:04:04,890 --> 00:04:09,030
requirement for the substance being

00:04:06,720 --> 00:04:10,590
mixed might actually be greater than the

00:04:09,030 --> 00:04:13,410
resolution requirements of the velocity

00:04:10,590 --> 00:04:15,150
field providing the mixing so we focus

00:04:13,410 --> 00:04:17,489
on a regime of turbulent mixing

00:04:15,150 --> 00:04:19,410
processes where the resolution that's

00:04:17,489 --> 00:04:21,510
required for the substance is much finer

00:04:19,410 --> 00:04:24,390
than what we would need for the velocity

00:04:21,510 --> 00:04:26,790
field and this is this is kind of a

00:04:24,390 --> 00:04:28,500
driving factor or force behind the

00:04:26,790 --> 00:04:32,130
numerical methods that we use in the

00:04:28,500 --> 00:04:34,140
implementation so we will have in our

00:04:32,130 --> 00:04:36,180
code and I just have two more slides

00:04:34,140 --> 00:04:37,200
that have just slide in more to kind of

00:04:36,180 --> 00:04:39,570
give the background and then we'll get

00:04:37,200 --> 00:04:40,980
into the OpenMP but we'll have a

00:04:39,570 --> 00:04:42,690
velocity field a turbulent velocity

00:04:40,980 --> 00:04:44,310
field covered by the navier-stokes

00:04:42,690 --> 00:04:46,470
equations which we solved with a Fourier

00:04:44,310 --> 00:04:47,520
pseudo spectral scheme and that's pretty

00:04:46,470 --> 00:04:49,620
much all we're going to say him to talk

00:04:47,520 --> 00:04:52,590
about the velocity field our focus is on

00:04:49,620 --> 00:04:55,470
on the scalar that's being mixed and to

00:04:52,590 --> 00:04:56,700
show you what governs the evolution of

00:04:55,470 --> 00:04:59,010
the scalar field we have an equation

00:04:56,700 --> 00:05:01,620
here so this is an advection diffusion

00:04:59,010 --> 00:05:03,030
equation for the passive scalar and you

00:05:01,620 --> 00:05:05,580
can see that we need a couple of things

00:05:03,030 --> 00:05:07,560
we need derivatives of the scalar field

00:05:05,580 --> 00:05:09,180
to form the diffusion and effective

00:05:07,560 --> 00:05:12,150
advection terms in the governing

00:05:09,180 --> 00:05:14,490
equation and we need a velocity field on

00:05:12,150 --> 00:05:17,910
the on the fine grid used for for the

00:05:14,490 --> 00:05:21,419
passive scalar so how are we going to

00:05:17,910 --> 00:05:23,669
get these terms well we we use what are

00:05:21,419 --> 00:05:25,470
called compact finite differences so

00:05:23,669 --> 00:05:27,720
this is kind of like a finite difference

00:05:25,470 --> 00:05:31,169
type approach to two numerically

00:05:27,720 --> 00:05:33,330
obtained derivatives but the the the

00:05:31,169 --> 00:05:36,030
word compact is it's very significant

00:05:33,330 --> 00:05:37,560
because there is a coupling of the grid

00:05:36,030 --> 00:05:39,510
points that are used when you evaluate

00:05:37,560 --> 00:05:41,340
the derivatives this is this contrast

00:05:39,510 --> 00:05:43,710
kind of the explicit finite differences

00:05:41,340 --> 00:05:45,060
which you might be more but which you

00:05:43,710 --> 00:05:47,280
might be familiar with for taking

00:05:45,060 --> 00:05:48,750
derivatives but compact binding

00:05:47,280 --> 00:05:51,330
differences are very nice because they

00:05:48,750 --> 00:05:54,150
have very good resolution properties and

00:05:51,330 --> 00:05:55,710
they provide very high accuracy for the

00:05:54,150 --> 00:05:59,100
derivatives that we need in in

00:05:55,710 --> 00:06:01,260
dissimulation so our basic code this is

00:05:59,100 --> 00:06:02,730
kind of like a 2d schematic of the code

00:06:01,260 --> 00:06:04,530
but what we'll do is we'll have a

00:06:02,730 --> 00:06:06,900
velocity field that's being simulated on

00:06:04,530 --> 00:06:08,610
a relatively coarse grid because the

00:06:06,900 --> 00:06:10,620
navier-stokes computation is very

00:06:08,610 --> 00:06:12,930
complicated one but then we'll have a

00:06:10,620 --> 00:06:14,730
little bit simpler computation going on

00:06:12,930 --> 00:06:16,830
for the scalar field on a much finer

00:06:14,730 --> 00:06:18,480
grid and we'll have an interpolation

00:06:16,830 --> 00:06:19,830
operation for the velocity field to go

00:06:18,480 --> 00:06:21,360
from the course to the fine grid and

00:06:19,830 --> 00:06:23,370
then we'll use the compact find any

00:06:21,360 --> 00:06:25,970
differences over here to take the

00:06:23,370 --> 00:06:28,530
derivatives of the scalar now how do we

00:06:25,970 --> 00:06:32,100
for the for the parallel implementation

00:06:28,530 --> 00:06:33,660
of this how do we do this well one thing

00:06:32,100 --> 00:06:35,700
when we when we sat down to kind of

00:06:33,660 --> 00:06:39,390
develop this code in early 2016 we

00:06:35,700 --> 00:06:40,500
thought about well what what is the what

00:06:39,390 --> 00:06:42,510
are the parameter ranges that were

00:06:40,500 --> 00:06:44,220
interested in simulating and what are

00:06:42,510 --> 00:06:46,620
the computational requirements for the

00:06:44,220 --> 00:06:48,990
velocity and scalar computations so it

00:06:46,620 --> 00:06:50,880
actually turns out that the scalar

00:06:48,990 --> 00:06:52,170
computation is so much more massive than

00:06:50,880 --> 00:06:54,480
what we mean for the velocity field that

00:06:52,170 --> 00:06:57,030
we actually use a split Communicator

00:06:54,480 --> 00:06:59,070
approach where instead of performing the

00:06:57,030 --> 00:07:00,840
entire simulation and say MPI comm world

00:06:59,070 --> 00:07:02,550
what we'll do is we'll split the

00:07:00,840 --> 00:07:04,230
communicator into two disjoint groups

00:07:02,550 --> 00:07:05,940
and we'll have a very small group of

00:07:04,230 --> 00:07:08,040
processors computing the velocity field

00:07:05,940 --> 00:07:09,990
and then they can send the the velocity

00:07:08,040 --> 00:07:12,690
field information over to a much larger

00:07:09,990 --> 00:07:16,460
scalar field computation going on so to

00:07:12,690 --> 00:07:18,480
give you kind of like a ballpark kind of

00:07:16,460 --> 00:07:20,790
understanding of the sizes of these

00:07:18,480 --> 00:07:22,110
communicators for what we do if we're

00:07:20,790 --> 00:07:24,990
running one of these simulations on

00:07:22,110 --> 00:07:27,270
Titan this scalar field computation

00:07:24,990 --> 00:07:30,720
might be using eight eight K nodes of

00:07:27,270 --> 00:07:32,640
this so maybe 18 K available on a

00:07:30,720 --> 00:07:35,190
machine but this computation here might

00:07:32,640 --> 00:07:36,630
be using something like 256 notes so

00:07:35,190 --> 00:07:38,160
there's a very small computation that's

00:07:36,630 --> 00:07:40,680
kind of coupled to a much larger

00:07:38,160 --> 00:07:42,390
computation and this this computation

00:07:40,680 --> 00:07:46,500
over here this is where we focus on for

00:07:42,390 --> 00:07:49,710
openmp improvement in the code so what

00:07:46,500 --> 00:07:51,270
are we going to do for the development

00:07:49,710 --> 00:07:53,380
of the code in the homogeneous computing

00:07:51,270 --> 00:07:55,120
environment so

00:07:53,380 --> 00:07:58,450
cut straight to the chase I think the

00:07:55,120 --> 00:08:00,250
previous talk is one one of the things

00:07:58,450 --> 00:08:02,200
he spoke about was they extract as a

00:08:00,250 --> 00:08:03,730
kernel to kind of understand and improve

00:08:02,200 --> 00:08:05,800
the performance as a larger application

00:08:03,730 --> 00:08:08,140
code so we did the same thing

00:08:05,800 --> 00:08:09,580
so in our code the compact finite

00:08:08,140 --> 00:08:12,490
differences which I spoke about earlier

00:08:09,580 --> 00:08:15,190
are the most important and expensive

00:08:12,490 --> 00:08:17,110
computation in the code and and that

00:08:15,190 --> 00:08:19,210
these are the routines for scalability

00:08:17,110 --> 00:08:20,170
becomes an issue because communication

00:08:19,210 --> 00:08:22,900
is required

00:08:20,170 --> 00:08:25,570
among the parallel processes to to fill

00:08:22,900 --> 00:08:27,940
those layers or or form the solution for

00:08:25,570 --> 00:08:31,570
the compact finite difference scheme so

00:08:27,940 --> 00:08:34,240
our focus is on the scalability of the

00:08:31,570 --> 00:08:37,450
CCD scheme in the form of a kernel which

00:08:34,240 --> 00:08:39,760
we will plug in to the main code when we

00:08:37,450 --> 00:08:41,979
actually want to run simulations so the

00:08:39,760 --> 00:08:44,560
CCD scheme like I mentioned before is a

00:08:41,979 --> 00:08:47,080
little more complicated than explicit

00:08:44,560 --> 00:08:50,050
finite differences in that that if you

00:08:47,080 --> 00:08:51,610
have a given a grid line of points all

00:08:50,050 --> 00:08:54,010
the grid points are coupled when you

00:08:51,610 --> 00:08:55,960
want to evaluate the derivatives so how

00:08:54,010 --> 00:08:58,210
do we use a domain decomposition

00:08:55,960 --> 00:09:00,760
approach if all the grid points in a

00:08:58,210 --> 00:09:02,770
given direction are needed well so in

00:09:00,760 --> 00:09:05,260
these in the pseudo spectral code what

00:09:02,770 --> 00:09:09,130
we do for the FFTs is we would have a

00:09:05,260 --> 00:09:11,140
pencil based transpose approach to to to

00:09:09,130 --> 00:09:14,170
do a similar type of operation with the

00:09:11,140 --> 00:09:16,900
FFT like we would we would align data on

00:09:14,170 --> 00:09:19,210
a process take FFT in a given direction

00:09:16,900 --> 00:09:20,740
and then transpose the entire the entire

00:09:19,210 --> 00:09:22,360
data to take the FFT in another

00:09:20,740 --> 00:09:25,090
direction you could perform a very

00:09:22,360 --> 00:09:27,010
similar strategy with the compact finite

00:09:25,090 --> 00:09:30,070
differences but it actually turns out

00:09:27,010 --> 00:09:32,440
that there is a really neat algorithm

00:09:30,070 --> 00:09:34,480
that you can use to split the linear

00:09:32,440 --> 00:09:36,430
system associated with the CCD scheme on

00:09:34,480 --> 00:09:37,420
a static three dimensional domain

00:09:36,430 --> 00:09:38,770
decomposition

00:09:37,420 --> 00:09:41,680
and then what we'll try to do is avoid

00:09:38,770 --> 00:09:43,540
some of the very expensive communication

00:09:41,680 --> 00:09:46,060
costs associated with transpose based

00:09:43,540 --> 00:09:48,550
approaches so I don't want to get into

00:09:46,060 --> 00:09:50,950
the details of the equations that are

00:09:48,550 --> 00:09:53,200
behind some of the lines of this table

00:09:50,950 --> 00:09:55,870
here but what I want to highlight is

00:09:53,200 --> 00:09:57,940
that in the CCD scheme when we want to

00:09:55,870 --> 00:10:00,460
apply it in parallel there's a natural

00:09:57,940 --> 00:10:03,190
series of communication and computation

00:10:00,460 --> 00:10:05,410
operations where the overall cost of the

00:10:03,190 --> 00:10:07,810
communication is much reduced compared

00:10:05,410 --> 00:10:10,960
to a transpose based approach so what we

00:10:07,810 --> 00:10:13,720
will try to do in our code is see how

00:10:10,960 --> 00:10:16,450
might we how might we try to overlap

00:10:13,720 --> 00:10:18,790
communication and computation for these

00:10:16,450 --> 00:10:22,210
operations to to improve scalability

00:10:18,790 --> 00:10:24,280
where obviously the the communication is

00:10:22,210 --> 00:10:27,010
what is affecting our scalability to

00:10:24,280 --> 00:10:30,340
large problem sizes so we want to see

00:10:27,010 --> 00:10:32,800
how we can use openmp to maybe to

00:10:30,340 --> 00:10:34,450
improve scalability of application by

00:10:32,800 --> 00:10:39,730
overlapping communication and

00:10:34,450 --> 00:10:42,310
computation okay so how do we go about

00:10:39,730 --> 00:10:44,530
doing this well we tried many different

00:10:42,310 --> 00:10:46,060
approaches and then we kind of finally

00:10:44,530 --> 00:10:49,420
converged on something that worked best

00:10:46,060 --> 00:10:51,790
so if we have an MPI code just pure MPI

00:10:49,420 --> 00:10:53,740
code that maybe the first thing we tries

00:10:51,790 --> 00:10:56,170
to use non-blocking communication calls

00:10:53,740 --> 00:10:57,820
so the idea here is that we have to take

00:10:56,170 --> 00:11:00,040
derivatives in three independent

00:10:57,820 --> 00:11:01,390
coordinate directions the operations for

00:11:00,040 --> 00:11:04,510
the three coordinate directions are

00:11:01,390 --> 00:11:07,060
independent of one another so if I need

00:11:04,510 --> 00:11:09,220
to perform a communication call for one

00:11:07,060 --> 00:11:11,020
coordinate direction let me post that in

00:11:09,220 --> 00:11:12,700
a non-blocking communication call for

00:11:11,020 --> 00:11:14,650
MPI and move on to computation in

00:11:12,700 --> 00:11:17,470
another coordinate direction so try to

00:11:14,650 --> 00:11:20,260
use non-blocking MPI to perform

00:11:17,470 --> 00:11:21,400
communication in the background for for

00:11:20,260 --> 00:11:26,140
another coordinate direction while I

00:11:21,400 --> 00:11:27,540
operate on it on say the given

00:11:26,140 --> 00:11:30,580
coordinate direction that interested in

00:11:27,540 --> 00:11:33,340
so that I'll show you some data later

00:11:30,580 --> 00:11:34,840
that didn't work out quite as well as we

00:11:33,340 --> 00:11:37,430
had wanted and we found a very kind of

00:11:34,840 --> 00:11:40,220
depressing paper by by Hager

00:11:37,430 --> 00:11:42,890
and 2011 where it was a crate usergroup

00:11:40,220 --> 00:11:45,350
paper and they went through and and he

00:11:42,890 --> 00:11:48,040
he says like this is what we tried to

00:11:45,350 --> 00:11:50,360
test is do the MPI implementations

00:11:48,040 --> 00:11:52,130
actually do anything with non-blocking

00:11:50,360 --> 00:11:53,750
communication behind behind the scenes

00:11:52,130 --> 00:11:55,250
like if you post a non-blocking

00:11:53,750 --> 00:11:57,470
communication called if anything

00:11:55,250 --> 00:11:59,870
happened before you get to the MPI wait

00:11:57,470 --> 00:12:03,590
and and it turned out that this was a

00:11:59,870 --> 00:12:05,540
very implementation specific and so it

00:12:03,590 --> 00:12:07,610
just kind of like we were just kind of

00:12:05,540 --> 00:12:10,160
hesitant about pursuing a pure MPI

00:12:07,610 --> 00:12:12,170
approach we didn't really know if we

00:12:10,160 --> 00:12:15,440
could trust all the implementations we

00:12:12,170 --> 00:12:17,150
would use to perform non-blocking

00:12:15,440 --> 00:12:20,450
communication as well as we would like

00:12:17,150 --> 00:12:22,100
for it to and then if we but you know we

00:12:20,450 --> 00:12:26,030
can try to do is we can try to use

00:12:22,100 --> 00:12:27,890
OpenMP in the program so for example on

00:12:26,030 --> 00:12:29,960
blue waters instead of running with 32

00:12:27,890 --> 00:12:32,030
MPI processes on each node maybe we try

00:12:29,960 --> 00:12:34,370
to run with four with eight OpenMP

00:12:32,030 --> 00:12:36,920
threads for MPI process and and that

00:12:34,370 --> 00:12:38,540
actually helps our code significantly

00:12:36,920 --> 00:12:40,370
because the communication requirements

00:12:38,540 --> 00:12:42,590
are actually tied to the number of

00:12:40,370 --> 00:12:44,480
impact processes you have if you split

00:12:42,590 --> 00:12:46,010
up the domain into smaller and smaller

00:12:44,480 --> 00:12:47,420
chunks you have more and more ghost

00:12:46,010 --> 00:12:49,400
layers as well and the communication

00:12:47,420 --> 00:12:51,350
requirements actually increases use as

00:12:49,400 --> 00:12:53,930
you strong scale with respect to MPI

00:12:51,350 --> 00:12:56,840
processes so we thought well let's just

00:12:53,930 --> 00:12:58,610
use openmp threads and and reduce the

00:12:56,840 --> 00:13:01,160
communication requirements that way and

00:12:58,610 --> 00:13:03,260
and that definitely helped a lot but we

00:13:01,160 --> 00:13:06,680
have this issue where if we have to go

00:13:03,260 --> 00:13:07,730
to say an O MP master region where the

00:13:06,680 --> 00:13:09,890
master thread is performing the

00:13:07,730 --> 00:13:11,690
communication call or or maybe just

00:13:09,890 --> 00:13:13,580
terminate the turtle region to to do a

00:13:11,690 --> 00:13:15,890
communication call maybe those threads

00:13:13,580 --> 00:13:17,060
are not being used to the the other

00:13:15,890 --> 00:13:19,490
threads are not being used to their

00:13:17,060 --> 00:13:22,480
fullest extent so we thought well let's

00:13:19,490 --> 00:13:24,980
just try another way I saw some papers

00:13:22,480 --> 00:13:28,579
by

00:13:24,980 --> 00:13:32,000
that Reverend site there maybe and maybe

00:13:28,579 --> 00:13:34,339
he's German going back even to 2003

00:13:32,000 --> 00:13:36,170
where a lot of the OpenMP experts are

00:13:34,339 --> 00:13:38,240
saying well maybe we can just try to

00:13:36,170 --> 00:13:40,339
explicitly overlap communication and

00:13:38,240 --> 00:13:42,050
computation in the code by my excluding

00:13:40,339 --> 00:13:44,779
open and D threads so if I have a

00:13:42,050 --> 00:13:47,060
handful of open in P threads let's just

00:13:44,779 --> 00:13:49,250
let one of them do communication or

00:13:47,060 --> 00:13:50,899
maybe two of them do communication while

00:13:49,250 --> 00:13:52,790
the rest perform some computation so we

00:13:50,899 --> 00:13:54,199
wanted to pursue this approach to see if

00:13:52,790 --> 00:13:58,060
it offered improvements compared to

00:13:54,199 --> 00:14:00,290
these more basic approaches but it's

00:13:58,060 --> 00:14:05,209
there are some challenges associated

00:14:00,290 --> 00:14:07,459
with with the so if I if I have a team

00:14:05,209 --> 00:14:09,230
of threads and I want one to perform

00:14:07,459 --> 00:14:11,750
communication while the others will only

00:14:09,230 --> 00:14:15,560
compute there are a number of issues

00:14:11,750 --> 00:14:17,570
that you might run into so how will I

00:14:15,560 --> 00:14:19,820
enforce the correct sequence of

00:14:17,570 --> 00:14:21,380
operations in the CCD scheme if I have a

00:14:19,820 --> 00:14:23,540
communication thread and I have some

00:14:21,380 --> 00:14:25,370
computations read some order of

00:14:23,540 --> 00:14:27,139
operations still needs to be adhere to

00:14:25,370 --> 00:14:30,230
otherwise the results would be would be

00:14:27,139 --> 00:14:32,899
garbage and then in addition to that if

00:14:30,230 --> 00:14:34,430
if I if I complete an operation say on a

00:14:32,899 --> 00:14:35,569
communication thread and the results

00:14:34,430 --> 00:14:37,910
need to be made available to the

00:14:35,569 --> 00:14:40,970
computation bread is the memory

00:14:37,910 --> 00:14:42,350
synchronized such that that the the

00:14:40,970 --> 00:14:43,430
results that they're using would be

00:14:42,350 --> 00:14:47,300
would be correct

00:14:43,430 --> 00:14:48,680
and then also we we kind of we at least

00:14:47,300 --> 00:14:50,300
when we started to think about this

00:14:48,680 --> 00:14:52,940
approach we thought we'll we'll probably

00:14:50,300 --> 00:14:54,470
not have a one-for-one ratio on the

00:14:52,940 --> 00:14:56,750
ratio of communication computation

00:14:54,470 --> 00:14:58,940
threads like presumably we would have

00:14:56,750 --> 00:15:01,970
fewer communications and computation

00:14:58,940 --> 00:15:06,019
threads so how do we want to how do we

00:15:01,970 --> 00:15:08,149
want to have the code be able to have

00:15:06,019 --> 00:15:09,260
many more computation threads compared

00:15:08,149 --> 00:15:11,750
to the communications that their

00:15:09,260 --> 00:15:13,310
communication threads and it we we

00:15:11,750 --> 00:15:15,260
thought about it for a while and we we

00:15:13,310 --> 00:15:17,180
kind of after studying the OpenMP

00:15:15,260 --> 00:15:19,040
standard we saw that that locks and

00:15:17,180 --> 00:15:21,120
nested parallelism might be might be

00:15:19,040 --> 00:15:23,430
good ways to tackle some of

00:15:21,120 --> 00:15:25,500
challenges so I thought that I would

00:15:23,430 --> 00:15:28,250
just kind of dive into some pseudocode

00:15:25,500 --> 00:15:31,410
for the routine to explain what we did

00:15:28,250 --> 00:15:33,089
so for the compact scheme we we want to

00:15:31,410 --> 00:15:35,220
take derivatives in three independent or

00:15:33,089 --> 00:15:38,190
none directions and all those operations

00:15:35,220 --> 00:15:41,850
are independent so what we will do is we

00:15:38,190 --> 00:15:44,160
will use box for each coordinate

00:15:41,850 --> 00:15:45,750
direction so we'll have say a lock for

00:15:44,160 --> 00:15:47,850
the excellent or in a direction X 2 and

00:15:45,750 --> 00:15:49,830
X 3 and if a communication or

00:15:47,850 --> 00:15:51,980
computation threat computation thread

00:15:49,830 --> 00:15:54,300
needs to operate on say the x1 direction

00:15:51,980 --> 00:15:57,270
before they perform that operation they

00:15:54,300 --> 00:15:58,710
need to obtain this lock ok and then

00:15:57,270 --> 00:16:00,240
they can release the lock when it's

00:15:58,710 --> 00:16:04,770
finished when they're finished with the

00:16:00,240 --> 00:16:06,360
task that they have to work on now but

00:16:04,770 --> 00:16:09,750
what about the what about the issue of

00:16:06,360 --> 00:16:12,450
having work sharing over a kind of like

00:16:09,750 --> 00:16:15,420
of a sub team of this of the OpenMP

00:16:12,450 --> 00:16:17,430
threads well we thought let's not try to

00:16:15,420 --> 00:16:19,320
spawn all the openmp threads that we

00:16:17,430 --> 00:16:20,970
want to use in one parallel region and

00:16:19,320 --> 00:16:23,310
then try to divide them but instead

00:16:20,970 --> 00:16:25,560
let's just spawn two threads and then

00:16:23,310 --> 00:16:27,900
for the computation task that we have to

00:16:25,560 --> 00:16:30,330
do let's spawn a nested parallel region

00:16:27,900 --> 00:16:32,420
and what that will enable us to do is

00:16:30,330 --> 00:16:35,640
for the couple for the computation

00:16:32,420 --> 00:16:37,020
thread is that then with the nested

00:16:35,640 --> 00:16:38,610
parallel region you can use all the

00:16:37,020 --> 00:16:40,650
standard work sharing constructs that

00:16:38,610 --> 00:16:42,180
you would normally do like if you didn't

00:16:40,650 --> 00:16:43,380
have this nested parallel region you

00:16:42,180 --> 00:16:45,750
just trying to have eight threads across

00:16:43,380 --> 00:16:48,540
here those computations rights can't use

00:16:45,750 --> 00:16:49,890
like OMP do and because all the threads

00:16:48,540 --> 00:16:54,779
have to encounter the constructor or

00:16:49,890 --> 00:16:57,270
none at all so we found that with locks

00:16:54,779 --> 00:16:59,370
we were able to control the sequence of

00:16:57,270 --> 00:17:00,930
operations and who is operating on what

00:16:59,370 --> 00:17:03,120
coordinate direction at a given time and

00:17:00,930 --> 00:17:05,160
then with nested parallel region we can

00:17:03,120 --> 00:17:07,199
with a nested parallel region we have

00:17:05,160 --> 00:17:08,640
this really easy way to expand the

00:17:07,199 --> 00:17:11,370
number of computations read in the code

00:17:08,640 --> 00:17:13,319
so to kind of show you how the code gets

00:17:11,370 --> 00:17:15,300
set up here we will initialize the locks

00:17:13,319 --> 00:17:17,490
we'll always start off with two threads

00:17:15,300 --> 00:17:18,260
okay communication and computation and

00:17:17,490 --> 00:17:20,569
then

00:17:18,260 --> 00:17:22,910
well the Masters thrive because we still

00:17:20,569 --> 00:17:24,980
operate an MPI thread funnelled approach

00:17:22,910 --> 00:17:27,319
so the master thread kind of goes over

00:17:24,980 --> 00:17:30,230
here and we need to set the lock in

00:17:27,319 --> 00:17:31,940
order for the operations to be completed

00:17:30,230 --> 00:17:33,950
in the right way so the we will try to

00:17:31,940 --> 00:17:36,260
perform communication calls for X 2 and

00:17:33,950 --> 00:17:38,240
X 3 first while they come while the

00:17:36,260 --> 00:17:40,370
computation threads which will try to

00:17:38,240 --> 00:17:42,410
set the X 1 locker before computations

00:17:40,370 --> 00:17:43,790
to the X 1 Direction first and there's a

00:17:42,410 --> 00:17:45,799
subtle point here that I wanted to

00:17:43,790 --> 00:17:47,450
mention what we did originally was we

00:17:45,799 --> 00:17:49,250
put this lock outside the parallel

00:17:47,450 --> 00:17:51,020
region and tried to get a barrier across

00:17:49,250 --> 00:17:53,120
the whole the whole thing to make sure

00:17:51,020 --> 00:17:54,440
the locks are initialized properly but

00:17:53,120 --> 00:17:56,660
we took a look at some of the fine print

00:17:54,440 --> 00:17:59,390
and the opening standard and if we're

00:17:56,660 --> 00:18:01,250
not we're not mistaken the lock needs to

00:17:59,390 --> 00:18:04,100
be kind of set by the same implicit task

00:18:01,250 --> 00:18:05,990
so like the I think this implicit task

00:18:04,100 --> 00:18:08,660
is different than this implicit task so

00:18:05,990 --> 00:18:11,030
we had to come up with a we use OMP test

00:18:08,660 --> 00:18:13,100
lock and a little bit complicated way to

00:18:11,030 --> 00:18:15,020
make sure that all the locks are set

00:18:13,100 --> 00:18:17,330
before the routine progresses and then

00:18:15,020 --> 00:18:18,860
once that is done we just kind of work

00:18:17,330 --> 00:18:20,960
through all of the sequences of

00:18:18,860 --> 00:18:23,929
operations for communication and

00:18:20,960 --> 00:18:25,640
computation to do to apply the CCD

00:18:23,929 --> 00:18:27,470
scheme to get derivatives in all three

00:18:25,640 --> 00:18:30,140
coordinate directions so what you would

00:18:27,470 --> 00:18:32,780
do is the the X 2 and X 3 locks were

00:18:30,140 --> 00:18:34,880
already set so the communication for X 2

00:18:32,780 --> 00:18:36,620
and then release along communication for

00:18:34,880 --> 00:18:39,710
X 3 and then release the lock meanwhile

00:18:36,620 --> 00:18:42,110
computation is going on for the X 1/4

00:18:39,710 --> 00:18:43,640
direction and then we need to be careful

00:18:42,110 --> 00:18:45,260
about the order of the lock exchange

00:18:43,640 --> 00:18:47,030
here to avoid any kind of like race

00:18:45,260 --> 00:18:48,620
conditions on the lock but when it's

00:18:47,030 --> 00:18:51,200
when when the locks are released they

00:18:48,620 --> 00:18:52,970
kind of switch hands and the threads

00:18:51,200 --> 00:18:55,370
just kind of progressed down through all

00:18:52,970 --> 00:18:57,080
the operations and then and then at this

00:18:55,370 --> 00:18:59,210
point here we have all the derivatives

00:18:57,080 --> 00:19:01,580
that we need with it for the scalar

00:18:59,210 --> 00:19:02,990
field and all coordinate direction so

00:19:01,580 --> 00:19:05,929
how did this help improve our

00:19:02,990 --> 00:19:08,090
scalability so on blue waters it's a

00:19:05,929 --> 00:19:10,610
very big machine that we run pretty big

00:19:08,090 --> 00:19:13,250
computations on blue waters so we try to

00:19:10,610 --> 00:19:14,300
run with the scalar field at 8192 cube

00:19:13,250 --> 00:19:16,100
grid points

00:19:14,300 --> 00:19:18,380
and the nodes that we like we would try

00:19:16,100 --> 00:19:20,810
to run this on like 8k nodes of blue

00:19:18,380 --> 00:19:24,020
waters that the xe6 partition of blue

00:19:20,810 --> 00:19:26,720
waters so I just kind of want to focus

00:19:24,020 --> 00:19:30,440
in here on this particular data point

00:19:26,720 --> 00:19:32,720
here to show you how the scalability was

00:19:30,440 --> 00:19:36,380
improved with this approach so the open

00:19:32,720 --> 00:19:39,110
square is the pure MPI code so this is

00:19:36,380 --> 00:19:41,000
MPI was blocking communication and then

00:19:39,110 --> 00:19:43,520
we were able to make some improvements

00:19:41,000 --> 00:19:46,100
in scalability by either using non

00:19:43,520 --> 00:19:47,450
blocking communication or just MPI

00:19:46,100 --> 00:19:48,740
blocking communication with OpenMP

00:19:47,450 --> 00:19:50,270
threads to reduce the communication

00:19:48,740 --> 00:19:52,550
requirements all that kind of all that

00:19:50,270 --> 00:19:54,620
kind of stuff and it was great but I

00:19:52,550 --> 00:19:56,480
mean we always wanted more performance

00:19:54,620 --> 00:19:58,580
and we wanted more efficiency but so we

00:19:56,480 --> 00:20:00,410
try this new approach and we can see

00:19:58,580 --> 00:20:02,750
that the star where we have dedicated

00:20:00,410 --> 00:20:05,090
communication thread to the

00:20:02,750 --> 00:20:07,760
communication tasks if you will the

00:20:05,090 --> 00:20:10,190
scalability seems to be significantly

00:20:07,760 --> 00:20:13,640
improved to the point where we had about

00:20:10,190 --> 00:20:16,340
50% scaling for this for this pure MPI

00:20:13,640 --> 00:20:19,480
of course up to 90% scaling where we

00:20:16,340 --> 00:20:22,370
dedicate communication threads so we

00:20:19,480 --> 00:20:25,400
sometimes we we try these approaches and

00:20:22,370 --> 00:20:27,050
then we want to come back and see well

00:20:25,400 --> 00:20:29,540
how was it working like what made it

00:20:27,050 --> 00:20:33,050
work or just try to look to see what's

00:20:29,540 --> 00:20:36,230
going on so we have also instrumented

00:20:33,050 --> 00:20:39,920
the code with some timers MPI W time to

00:20:36,230 --> 00:20:41,570
kind of form timeline data of the code

00:20:39,920 --> 00:20:44,810
as it's taking these derivatives and

00:20:41,570 --> 00:20:47,690
what we're showing here are our timeline

00:20:44,810 --> 00:20:49,670
for the bottom row is the communication

00:20:47,690 --> 00:20:51,650
thread so this is a communication thread

00:20:49,670 --> 00:20:53,720
and then the top row in each block is

00:20:51,650 --> 00:20:56,030
the computation threads for a weak

00:20:53,720 --> 00:20:58,100
scaling study so at a thousand 24 cubed

00:20:56,030 --> 00:21:00,800
this is a problem where scalability is

00:20:58,100 --> 00:21:03,770
not an issue for us but at 81 92 cubed

00:21:00,800 --> 00:21:06,770
on 8 K nodes scalability is more of a

00:21:03,770 --> 00:21:09,050
problem so what we do is were like what

00:21:06,770 --> 00:21:11,420
we're showing here is the the time

00:21:09,050 --> 00:21:12,830
history the timeline of the

00:21:11,420 --> 00:21:15,470
communication and computation threads

00:21:12,830 --> 00:21:18,130
where you can see that at 8 K nodes and

00:21:15,470 --> 00:21:20,290
8192 cube the communication blocks are

00:21:18,130 --> 00:21:23,500
wider right so communication is more

00:21:20,290 --> 00:21:24,850
expensive at a nodes so but how

00:21:23,500 --> 00:21:26,410
efficiently are we hiding that

00:21:24,850 --> 00:21:28,450
communication cost because that's where

00:21:26,410 --> 00:21:31,660
the improved scalability and efficiency

00:21:28,450 --> 00:21:33,010
comes from so when you if you see here

00:21:31,660 --> 00:21:34,780
while the computation threads are

00:21:33,010 --> 00:21:37,120
progressing on a computation in the x1

00:21:34,780 --> 00:21:39,790
direction the ghost layer exchanged for

00:21:37,120 --> 00:21:41,740
x2 and x3 was completely hidden so this

00:21:39,790 --> 00:21:44,380
is kind of like this is this is this is

00:21:41,740 --> 00:21:46,270
great because the the communication cost

00:21:44,380 --> 00:21:47,920
is almost being entirely hidden now

00:21:46,270 --> 00:21:49,690
unfortunately in this routine it's like

00:21:47,920 --> 00:21:50,650
a lot of the computation is front-loaded

00:21:49,690 --> 00:21:52,360
in the routine and some of the

00:21:50,650 --> 00:21:54,400
communication is loaded at the back so

00:21:52,360 --> 00:21:56,080
we might in the future try to see if we

00:21:54,400 --> 00:21:57,970
there's another rearrangement of the

00:21:56,080 --> 00:22:00,850
order of operations that might try to

00:21:57,970 --> 00:22:02,920
hide some of this communication here you

00:22:00,850 --> 00:22:04,210
can see that there was an on there's a

00:22:02,920 --> 00:22:06,430
significant amount of time the

00:22:04,210 --> 00:22:08,500
computation threads are spinning waiting

00:22:06,430 --> 00:22:10,270
to get access to this lock so they could

00:22:08,500 --> 00:22:12,610
perform a very small computation for

00:22:10,270 --> 00:22:14,470
this Green coordinate direction but

00:22:12,610 --> 00:22:17,290
overall the scalability is greatly

00:22:14,470 --> 00:22:19,750
improved with this approach now just

00:22:17,290 --> 00:22:23,530
looking to the future before I move on

00:22:19,750 --> 00:22:26,470
to the next the heterogeneous topic but

00:22:23,530 --> 00:22:28,810
we we did this approach and then we come

00:22:26,470 --> 00:22:31,060
back then we worked on the heterogeneous

00:22:28,810 --> 00:22:33,310
machine and learn about tasks on the

00:22:31,060 --> 00:22:36,010
GPUs and then we kind of learn more

00:22:33,310 --> 00:22:37,870
about explicit tasks on the CPUs so then

00:22:36,010 --> 00:22:40,330
you kind of wonder is there another way

00:22:37,870 --> 00:22:42,250
that we can do the CPU algorithm not

00:22:40,330 --> 00:22:45,130
with a low-level lock based approach but

00:22:42,250 --> 00:22:48,490
maybe we just explicit tasks on the CPUs

00:22:45,130 --> 00:22:52,210
so what we what were currently working

00:22:48,490 --> 00:22:53,980
on because now CCE 8.6 which we just as

00:22:52,210 --> 00:22:57,280
of a couple weeks ago have access to on

00:22:53,980 --> 00:23:01,090
titan a fully supports depend clause on

00:22:57,280 --> 00:23:02,920
the on explicit tasks on the CPUs before

00:23:01,090 --> 00:23:05,470
they were serialized in the program but

00:23:02,920 --> 00:23:07,630
now they're actually kind of generated

00:23:05,470 --> 00:23:09,940
and put into the task pool and later

00:23:07,630 --> 00:23:13,210
worked on so what we want to try to do

00:23:09,940 --> 00:23:16,120
is use open and P tasks to implement the

00:23:13,210 --> 00:23:18,700
same ideas so what we would try to do is

00:23:16,120 --> 00:23:19,680
have still two threads one communication

00:23:18,700 --> 00:23:21,930
and one computation

00:23:19,680 --> 00:23:23,040
thread but now have the master thread go

00:23:21,930 --> 00:23:25,080
through and generate all the

00:23:23,040 --> 00:23:28,050
communication and computation tasks and

00:23:25,080 --> 00:23:30,390
then the two threads can kind of work

00:23:28,050 --> 00:23:32,670
through these tasks having the order and

00:23:30,390 --> 00:23:35,130
forced based on the task dependencies

00:23:32,670 --> 00:23:37,410
okay so what we'll try to do for example

00:23:35,130 --> 00:23:38,790
is we're still going to use MPI blocking

00:23:37,410 --> 00:23:40,110
communications that we need to make sure

00:23:38,790 --> 00:23:42,180
that the order the communication

00:23:40,110 --> 00:23:44,130
operations is the same like I don't want

00:23:42,180 --> 00:23:45,540
to start to do a sin receive for the X 1

00:23:44,130 --> 00:23:47,340
coordinate direction and then have

00:23:45,540 --> 00:23:48,510
another process start an all-too all for

00:23:47,340 --> 00:23:50,160
the X 3 coordinate direction and I don't

00:23:48,510 --> 00:23:52,740
think empty I would like that very much

00:23:50,160 --> 00:23:54,660
so what we'll try to do is enforce the

00:23:52,740 --> 00:23:57,840
strict ordering of the communication

00:23:54,660 --> 00:24:00,360
tasks with say dependency on a dummy

00:23:57,840 --> 00:24:03,540
variable comm so the order will still be

00:24:00,360 --> 00:24:04,860
kind of dictated and in the program but

00:24:03,540 --> 00:24:06,570
there's still plenty of opportunity to

00:24:04,860 --> 00:24:08,520
overlap for the computations and the

00:24:06,570 --> 00:24:09,780
other coordinate directions this is very

00:24:08,520 --> 00:24:12,150
similar to what we did with the LOC

00:24:09,780 --> 00:24:14,130
based approach but I think that that

00:24:12,150 --> 00:24:16,320
using tasks in this way might might

00:24:14,130 --> 00:24:18,570
allow us to express the ideas in the

00:24:16,320 --> 00:24:22,260
source code in a little bit less

00:24:18,570 --> 00:24:24,060
invasive way and then I think tasks you

00:24:22,260 --> 00:24:26,670
all know the standard more than I do but

00:24:24,060 --> 00:24:27,510
I think tasks guarantee some memory

00:24:26,670 --> 00:24:31,410
synchronizations

00:24:27,510 --> 00:24:33,630
when they're created and destroyed so we

00:24:31,410 --> 00:24:35,310
are working on this the code seems to be

00:24:33,630 --> 00:24:37,170
functioning and giving correct results

00:24:35,310 --> 00:24:39,060
and we've interacted with the CCE

00:24:37,170 --> 00:24:41,580
developers to understand more about how

00:24:39,060 --> 00:24:43,200
nested parallelism is is more

00:24:41,580 --> 00:24:45,210
complicated than the setting the

00:24:43,200 --> 00:24:48,060
computation tasks will still use nested

00:24:45,210 --> 00:24:49,980
OpenMP to to to use more computation

00:24:48,060 --> 00:24:53,280
threads but I think we're going to be

00:24:49,980 --> 00:24:56,280
trying to get that scale pretty soon so

00:24:53,280 --> 00:25:00,210
now let me try to talk about what we did

00:24:56,280 --> 00:25:00,700
for heterogeneous acceleration of the

00:25:00,210 --> 00:25:05,179
apple

00:25:00,700 --> 00:25:10,340
OpenMP vortex with an emphasis on the

00:25:05,179 --> 00:25:14,929
top five so we had this application code

00:25:10,340 --> 00:25:16,759
and my colleague dable he had previous

00:25:14,929 --> 00:25:18,320
experience with GPUs and he said hey

00:25:16,759 --> 00:25:19,490
look this looks like an application we

00:25:18,320 --> 00:25:21,409
might actually be able to accelerate

00:25:19,490 --> 00:25:23,659
because in the past you tried to

00:25:21,409 --> 00:25:25,129
accelerate the story a pseudo spectral

00:25:23,659 --> 00:25:26,240
code but it's like communication

00:25:25,129 --> 00:25:29,330
dominated and that's very challenging

00:25:26,240 --> 00:25:31,370
but this code has much reduced

00:25:29,330 --> 00:25:32,929
communication requirements so we thought

00:25:31,370 --> 00:25:34,730
hey this is a prime candidate from

00:25:32,929 --> 00:25:37,789
acceleration so how do you go about

00:25:34,730 --> 00:25:40,159
doing that we're not very interested in

00:25:37,789 --> 00:25:43,369
rewriting the application in another

00:25:40,159 --> 00:25:44,629
language like CUDA we would like to try

00:25:43,369 --> 00:25:46,970
to maintain the portability of our

00:25:44,629 --> 00:25:49,580
Fortran do loops these new loops will be

00:25:46,970 --> 00:25:51,139
around long after I died I mean these

00:25:49,580 --> 00:25:54,649
Fortran - loops will be around forever

00:25:51,139 --> 00:25:57,710
so we want to maintain Fortran source

00:25:54,649 --> 00:25:59,330
code and then just leverage openmp to

00:25:57,710 --> 00:26:03,590
provide the offloading and the

00:25:59,330 --> 00:26:06,320
acceleration but it's challenging to run

00:26:03,590 --> 00:26:08,690
on Titan because I mean the memory

00:26:06,320 --> 00:26:11,149
constraints on the GPUs are very tight

00:26:08,690 --> 00:26:13,639
so when we run say if you want to run on

00:26:11,149 --> 00:26:15,590
8k nodes on Titan will the CPUs have so

00:26:13,639 --> 00:26:18,710
much memory available but the GPUs are

00:26:15,590 --> 00:26:20,600
reduced by a significant factor a memory

00:26:18,710 --> 00:26:23,179
so we need to make sure that the

00:26:20,600 --> 00:26:25,669
algorithms that we develop are are our

00:26:23,179 --> 00:26:28,610
memory conscious and and and and using

00:26:25,669 --> 00:26:30,649
memory as efficiently as possible and

00:26:28,610 --> 00:26:32,480
then as the speaker talked about earlier

00:26:30,649 --> 00:26:34,330
today like we have to minimize data

00:26:32,480 --> 00:26:37,580
movement between the host and device

00:26:34,330 --> 00:26:40,460
otherwise we'll be just swamped by the

00:26:37,580 --> 00:26:43,970
cost of moving data between the the host

00:26:40,460 --> 00:26:46,309
and device memory spaces so the goal for

00:26:43,970 --> 00:26:49,159
this effort is to accelerate the scalar

00:26:46,309 --> 00:26:50,869
field computation similar to the

00:26:49,159 --> 00:26:53,659
previous section where we just focused

00:26:50,869 --> 00:26:54,679
on the CCD scheme and scalability like

00:26:53,659 --> 00:26:56,450
we're not going to mess with the

00:26:54,679 --> 00:26:58,190
velocity field calculation all we're

00:26:56,450 --> 00:26:59,629
just going to be working on the scalar

00:26:58,190 --> 00:27:00,950
field computation because that is the

00:26:59,629 --> 00:27:02,840
that dominates the

00:27:00,950 --> 00:27:05,299
mutation costs of our of our simulations

00:27:02,840 --> 00:27:07,100
and we're going to try to minimize data

00:27:05,299 --> 00:27:09,380
movement by putting the entire scalar

00:27:07,100 --> 00:27:11,240
field computation on the GPU and this

00:27:09,380 --> 00:27:13,580
required some reworking of the code and

00:27:11,240 --> 00:27:15,799
then we want to see if we can improve

00:27:13,580 --> 00:27:19,720
scalability of application once it's

00:27:15,799 --> 00:27:21,830
accelerated by using OpenMP 4.5 and

00:27:19,720 --> 00:27:24,769
maybe this first line goes without

00:27:21,830 --> 00:27:26,450
saying but this is definitely my first

00:27:24,769 --> 00:27:29,149
experience with accelerating an

00:27:26,450 --> 00:27:32,000
application if it's challenging you have

00:27:29,149 --> 00:27:33,380
a lot to learn but you just keep

00:27:32,000 --> 00:27:35,450
learning these things and try and do

00:27:33,380 --> 00:27:38,240
things and and and something sometimes

00:27:35,450 --> 00:27:40,639
it eventually works but we want to see

00:27:38,240 --> 00:27:41,720
really if we're going to run simulations

00:27:40,639 --> 00:27:43,789
on titan if we're going to run

00:27:41,720 --> 00:27:45,380
production simulations on titan you know

00:27:43,789 --> 00:27:47,269
the the main question we want to ask is

00:27:45,380 --> 00:27:49,159
are we doing that efficiently are we

00:27:47,269 --> 00:27:51,350
using an acceptable number of nodes and

00:27:49,159 --> 00:27:54,710
getting good scalability to justify what

00:27:51,350 --> 00:27:59,000
we're doing so that we want to you know

00:27:54,710 --> 00:28:01,250
pay attention to that and then another

00:27:59,000 --> 00:28:02,870
thing is you know for the expensive

00:28:01,250 --> 00:28:04,220
computational kernels in our code like

00:28:02,870 --> 00:28:06,909
how well are they getting accelerated

00:28:04,220 --> 00:28:10,070
are they getting accelerated well enough

00:28:06,909 --> 00:28:12,679
for our particular code the CPU version

00:28:10,070 --> 00:28:14,510
of the code was using too much memory so

00:28:12,679 --> 00:28:16,820
we had to do an extensive kind of

00:28:14,510 --> 00:28:19,010
refactoring of the code to reduce memory

00:28:16,820 --> 00:28:21,110
for the GPUs and and we call them low

00:28:19,010 --> 00:28:23,059
storage algorithms it's kind of like I

00:28:21,110 --> 00:28:24,620
mean I think mathematicians actually

00:28:23,059 --> 00:28:26,000
developed like low storage algorithms

00:28:24,620 --> 00:28:27,710
for say runge-kutta time integration

00:28:26,000 --> 00:28:30,230
this is just like a low storage computer

00:28:27,710 --> 00:28:33,830
implementation of our code so we will

00:28:30,230 --> 00:28:35,690
try to reuse arrays when when we can -

00:28:33,830 --> 00:28:37,340
and so that this will allow us to reduce

00:28:35,690 --> 00:28:40,399
the number of GPUs that would never

00:28:37,340 --> 00:28:42,260
running on some other things that I'll

00:28:40,399 --> 00:28:46,039
talk about later and I'll mention again

00:28:42,260 --> 00:28:48,590
is that the code uses MPI derive data

00:28:46,039 --> 00:28:50,600
types for communication so we have

00:28:48,590 --> 00:28:52,760
stranded memory accesses for the MPI

00:28:50,600 --> 00:28:55,039
communication calls but when we move

00:28:52,760 --> 00:28:56,510
data between the host and device open

00:28:55,039 --> 00:28:56,950
and P standard says that this data has

00:28:56,510 --> 00:28:58,809
to be

00:28:56,950 --> 00:29:00,639
to give his buffer so we actually had to

00:28:58,809 --> 00:29:06,130
we actually had to change the source

00:29:00,639 --> 00:29:08,860
code to to get it to work so let me see

00:29:06,130 --> 00:29:12,130
what I can do with my remaining time so

00:29:08,860 --> 00:29:13,929
one thing that we found later I mean

00:29:12,130 --> 00:29:15,490
this is not the first thing that we did

00:29:13,929 --> 00:29:16,779
this is not the first accelerated

00:29:15,490 --> 00:29:19,120
version of the code that we developed

00:29:16,779 --> 00:29:20,950
but now that we know we know maybe we

00:29:19,120 --> 00:29:23,080
would have done it this way to begin

00:29:20,950 --> 00:29:24,309
with so what we're doing is we're

00:29:23,080 --> 00:29:25,600
starting with a kind of a ground-up

00:29:24,309 --> 00:29:27,789
approach we're looking at the most

00:29:25,600 --> 00:29:29,500
expensive kernels in the code which ones

00:29:27,789 --> 00:29:32,110
are not getting accelerated well and

00:29:29,500 --> 00:29:34,179
then trying to decide upfront if any

00:29:32,110 --> 00:29:36,820
algorithmic changes are required in

00:29:34,179 --> 00:29:38,919
order to get the these kernels to

00:29:36,820 --> 00:29:40,690
accelerate well and and and then

00:29:38,919 --> 00:29:42,850
understand what implications that has

00:29:40,690 --> 00:29:45,130
for the rest of the code design so a

00:29:42,850 --> 00:29:48,250
good case study for us was the

00:29:45,130 --> 00:29:50,440
application of CCD scheme and the

00:29:48,250 --> 00:29:53,080
coordinate direction that corresponds to

00:29:50,440 --> 00:29:55,269
the innermost memory index of the three

00:29:53,080 --> 00:29:57,279
dimensional arrays in our code so what

00:29:55,269 --> 00:29:58,630
happens when we apply the CCD scheme is

00:29:57,279 --> 00:30:01,029
we have a linear system that we solve

00:29:58,630 --> 00:30:04,450
which has a forward and reverse sweeping

00:30:01,029 --> 00:30:07,779
process to it but so there's a conflict

00:30:04,450 --> 00:30:10,120
we want to access memory on the GPUs and

00:30:07,779 --> 00:30:12,730
as contiguous as a manner as possible to

00:30:10,120 --> 00:30:15,299
get coalesce memory accesses on the GPUs

00:30:12,730 --> 00:30:18,429
but this inner loop can't be vectorized

00:30:15,299 --> 00:30:19,990
so this this kernel right here presents

00:30:18,429 --> 00:30:22,600
a big problem for running on the GPUs

00:30:19,990 --> 00:30:25,809
and we kind of thought about different

00:30:22,600 --> 00:30:28,360
ways to improve this and then maybe the

00:30:25,809 --> 00:30:30,460
simplest possible way is just flip the

00:30:28,360 --> 00:30:31,990
memory layout so for this one kernel

00:30:30,460 --> 00:30:34,000
what we want to try to do is see if we

00:30:31,990 --> 00:30:37,870
can redesign certain aspects of the code

00:30:34,000 --> 00:30:40,389
where for this kernel we would have the

00:30:37,870 --> 00:30:42,279
ability to strive over the arrays on the

00:30:40,389 --> 00:30:44,250
inner most index have coalesced memory

00:30:42,279 --> 00:30:47,710
accesses actually seams on the GPU and

00:30:44,250 --> 00:30:49,600
and also be able to vectorize or or you

00:30:47,710 --> 00:30:50,430
know thread this operation over the GPU

00:30:49,600 --> 00:30:54,060
threads

00:30:50,430 --> 00:30:55,770
so this is a this is an example of a

00:30:54,060 --> 00:30:57,570
kernel where it kind of places memory

00:30:55,770 --> 00:30:58,950
constraints on the code because if we

00:30:57,570 --> 00:31:00,870
want to get good performance out of this

00:30:58,950 --> 00:31:05,400
kernel we need this extra memory to kind

00:31:00,870 --> 00:31:09,180
of swap things around and what we also

00:31:05,400 --> 00:31:11,550
tried to do is is isolate the damage to

00:31:09,180 --> 00:31:15,600
this just this subroutine in the source

00:31:11,550 --> 00:31:17,580
code so we don't want a changed memory

00:31:15,600 --> 00:31:19,080
layout to persist throughout the entire

00:31:17,580 --> 00:31:21,780
code because we have a lot of other

00:31:19,080 --> 00:31:24,240
loops which might be negatively impacted

00:31:21,780 --> 00:31:26,400
by switching the memory layout of some

00:31:24,240 --> 00:31:29,070
of the working arrays so what we what

00:31:26,400 --> 00:31:31,050
we've done in order to improve that

00:31:29,070 --> 00:31:32,940
kernel is swap the memory layout which

00:31:31,050 --> 00:31:34,680
will give the best performance for the

00:31:32,940 --> 00:31:37,410
kernel of interest and then for the

00:31:34,680 --> 00:31:40,230
immediate adjacent kernels try to work

00:31:37,410 --> 00:31:41,910
with those two to actually swap in and

00:31:40,230 --> 00:31:44,670
out of the of the change memory layout

00:31:41,910 --> 00:31:48,150
while retaining high performance and

00:31:44,670 --> 00:31:50,220
what we found works quite well is when

00:31:48,150 --> 00:31:52,200
we want to when we want to perform some

00:31:50,220 --> 00:31:54,240
of the memory swapping for some of these

00:31:52,200 --> 00:31:56,610
important kernels we just manually

00:31:54,240 --> 00:31:57,960
introduce the blocking factors and in

00:31:56,610 --> 00:31:59,640
tune them until they gave the best

00:31:57,960 --> 00:32:01,140
performance and it turned out that the

00:31:59,640 --> 00:32:04,740
best performance offered by such an

00:32:01,140 --> 00:32:07,410
approach was very comparable to the the

00:32:04,740 --> 00:32:09,150
performance of other kernels which did

00:32:07,410 --> 00:32:12,060
not have any of these memory issues

00:32:09,150 --> 00:32:15,390
associated with them just to also kind

00:32:12,060 --> 00:32:16,830
of show like it's sometimes if we're

00:32:15,390 --> 00:32:18,900
going to try to port two different

00:32:16,830 --> 00:32:20,990
machines we need to be aware of what

00:32:18,900 --> 00:32:24,930
constructs the compilers seem to like

00:32:20,990 --> 00:32:26,670
with Cray we you might notice that we

00:32:24,930 --> 00:32:29,220
don't have like a parallel for I could

00:32:26,670 --> 00:32:31,260
do or a Sindhi here so crazy seems to

00:32:29,220 --> 00:32:33,540
like just OMP target teams distributed

00:32:31,260 --> 00:32:34,830
and then as long as that inner loop can

00:32:33,540 --> 00:32:36,510
be vectorized it seems like they

00:32:34,830 --> 00:32:37,620
partition it over the GPU threads this

00:32:36,510 --> 00:32:41,070
is something that we've noticed since

00:32:37,620 --> 00:32:42,450
day one with cray we have found that say

00:32:41,070 --> 00:32:44,610
for this particular type of loop

00:32:42,450 --> 00:32:47,309
collapse five might produce the

00:32:44,610 --> 00:32:48,840
overall the goal we're currently working

00:32:47,309 --> 00:32:52,380
on porting some of the codes with AI to

00:32:48,840 --> 00:32:54,690
IBM XLS and there we need to say have

00:32:52,380 --> 00:32:56,730
like a parallel do or maybe even a

00:32:54,690 --> 00:33:00,690
Sindhi to to partition it over the

00:32:56,730 --> 00:33:05,179
threads but anyway so to quickly show

00:33:00,690 --> 00:33:08,429
you some of the performance benefits of

00:33:05,179 --> 00:33:11,940
this approach we have conducted some

00:33:08,429 --> 00:33:13,559
small test problems for a weak scaled

00:33:11,940 --> 00:33:14,850
version of our problem so this is

00:33:13,559 --> 00:33:17,220
something on Titan which would run on

00:33:14,850 --> 00:33:18,809
two nodes but the computations are

00:33:17,220 --> 00:33:21,419
representative of what we run on

00:33:18,809 --> 00:33:23,429
eighteen notes so the weak fields a

00:33:21,419 --> 00:33:26,100
problem down and what we're looking at

00:33:23,429 --> 00:33:29,610
are the performance of these loops how

00:33:26,100 --> 00:33:32,220
does that loop behave when the memory

00:33:29,610 --> 00:33:34,290
layout is poor or when we try to swap it

00:33:32,220 --> 00:33:35,669
to improve the performance and in the

00:33:34,290 --> 00:33:38,790
interest of time I'll just draw your

00:33:35,669 --> 00:33:41,610
attention to this linear system line

00:33:38,790 --> 00:33:43,830
where we have that linear system we were

00:33:41,610 --> 00:33:45,809
solving where we were accessing the

00:33:43,830 --> 00:33:47,429
memory in a coalesced way but the inner

00:33:45,809 --> 00:33:50,190
loop couldn't be threaded or partitioned

00:33:47,429 --> 00:33:52,919
if you will so that doesn't seem to be

00:33:50,190 --> 00:33:55,470
very well-suited for the GPU and in fact

00:33:52,919 --> 00:34:00,570
we the timing compared to the other

00:33:55,470 --> 00:34:04,110
loops is is is as pretty you know

00:34:00,570 --> 00:34:05,790
concerning other three-dimensional like

00:34:04,110 --> 00:34:09,659
other three-dimensional nested loops

00:34:05,790 --> 00:34:11,700
seem to run on the order of blood say 10

00:34:09,659 --> 00:34:13,649
milliseconds but here we had a kernel

00:34:11,700 --> 00:34:15,899
which after we accelerated it was

00:34:13,649 --> 00:34:18,210
running at say 0.2 to seconds and this

00:34:15,899 --> 00:34:19,649
this will dominate the cost of the code

00:34:18,210 --> 00:34:21,840
because the speed-up is not very good

00:34:19,649 --> 00:34:23,490
but what we've done is when we swap the

00:34:21,840 --> 00:34:26,220
memory layout not only does that improve

00:34:23,490 --> 00:34:28,139
the CPU version of the code because it I

00:34:26,220 --> 00:34:30,960
think the CPU will like that as well but

00:34:28,139 --> 00:34:33,869
the cost is is reduced by maybe a factor

00:34:30,960 --> 00:34:35,609
of 6 or so for this particular kernel

00:34:33,869 --> 00:34:38,109
and we've been able to control the

00:34:35,609 --> 00:34:40,149
damage in those neighboring kernels

00:34:38,109 --> 00:34:43,470
with the loop blocking approach to

00:34:40,149 --> 00:34:48,299
achieve an overall very high speed up

00:34:43,470 --> 00:34:50,950
compared to CPU execution with ECE 8.6

00:34:48,299 --> 00:34:52,989
so what we've done is we've identified

00:34:50,950 --> 00:34:55,059
these problematic criminals and we've

00:34:52,989 --> 00:34:57,190
we've identified what changes to the

00:34:55,059 --> 00:34:58,960
algorithm are necessary in order to

00:34:57,190 --> 00:35:00,749
achieve high performance and then we

00:34:58,960 --> 00:35:02,799
have to understand what what is the

00:35:00,749 --> 00:35:04,539
implication for the rest of the code

00:35:02,799 --> 00:35:06,400
well for us I don't want to walk through

00:35:04,539 --> 00:35:08,950
all these details because time is

00:35:06,400 --> 00:35:12,069
running out but essentially what it

00:35:08,950 --> 00:35:14,470
meant for us was that we could not

00:35:12,069 --> 00:35:15,789
calculate all derivatives together so in

00:35:14,470 --> 00:35:17,349
the CPU version of the code we were

00:35:15,789 --> 00:35:18,759
calculating all derivatives together and

00:35:17,349 --> 00:35:20,440
trying to overlap communication with a

00:35:18,759 --> 00:35:22,390
computation for three independent

00:35:20,440 --> 00:35:24,160
coordinate directions but now we have

00:35:22,390 --> 00:35:26,829
enough memory on the GPU where we will

00:35:24,160 --> 00:35:28,690
calculate derivatives in x1 and and then

00:35:26,829 --> 00:35:32,019
we can we can calculate x2 and x3

00:35:28,690 --> 00:35:34,269
derivatives together but overall the the

00:35:32,019 --> 00:35:37,869
performance is is much improved with

00:35:34,269 --> 00:35:41,410
this approach now how are we using the

00:35:37,869 --> 00:35:45,549
open p 4.5 and in this end this GPU code

00:35:41,410 --> 00:35:46,869
well 2 we will still have some sections

00:35:45,549 --> 00:35:48,910
of the code where we are overlapping

00:35:46,869 --> 00:35:50,440
communication with computation for

00:35:48,910 --> 00:35:52,720
example like I said the x2 and x3

00:35:50,440 --> 00:35:54,549
derivatives can be computed together

00:35:52,720 --> 00:35:56,019
there is enough memory to do that so

00:35:54,549 --> 00:35:57,640
let's just get to a point of the code

00:35:56,019 --> 00:35:59,380
where we say we want to make a

00:35:57,640 --> 00:36:01,329
communication call in the x3 coordinate

00:35:59,380 --> 00:36:02,950
direction so like here I have an all-too

00:36:01,329 --> 00:36:05,140
all where I wanted to make this call and

00:36:02,950 --> 00:36:07,509
the x3 corner Direction the way that we

00:36:05,140 --> 00:36:10,119
overlap computation with this

00:36:07,509 --> 00:36:12,670
communication call is right before it we

00:36:10,119 --> 00:36:14,739
just launch all of the x2 kernels that

00:36:12,670 --> 00:36:17,859
we have that can run on the GPU at that

00:36:14,739 --> 00:36:19,359
time so we will just have multiple

00:36:17,859 --> 00:36:21,190
kernels I'm just showing one here with

00:36:19,359 --> 00:36:23,529
like they om k target teams distribute

00:36:21,190 --> 00:36:26,170
depend and out on an on a variable that

00:36:23,529 --> 00:36:30,249
is specific to the x2 corn interruption

00:36:26,170 --> 00:36:32,499
15:08 clause there and you'll stack up a

00:36:30,249 --> 00:36:33,819
handful of these kernels that will be

00:36:32,499 --> 00:36:36,020
running on the GPU while this

00:36:33,819 --> 00:36:38,570
communication call is

00:36:36,020 --> 00:36:40,760
in place and the idea is to just try to

00:36:38,570 --> 00:36:43,510
make the algorithm asynchronous and

00:36:40,760 --> 00:36:48,500
overlap communication with computation

00:36:43,510 --> 00:36:50,660
wherever possible so on on Titan we we

00:36:48,500 --> 00:36:53,330
get in about we get about a 5x speed up

00:36:50,660 --> 00:36:56,390
for this code for this CFD code compared

00:36:53,330 --> 00:36:58,250
to CPU only execution and what I'm

00:36:56,390 --> 00:37:02,240
showing here in this in this strong and

00:36:58,250 --> 00:37:05,540
weak scaling plot or the like the

00:37:02,240 --> 00:37:07,640
serialized timings like so the the CPU

00:37:05,540 --> 00:37:10,850
and the GPU do not act interact

00:37:07,640 --> 00:37:13,940
asynchronously for the red pluses and

00:37:10,850 --> 00:37:17,050
then for the blue axis we have allowed

00:37:13,940 --> 00:37:21,140
for some overlap with open and P 4.5

00:37:17,050 --> 00:37:23,030
tasking capabilities and you know we can

00:37:21,140 --> 00:37:25,820
always try to improve it more but I mean

00:37:23,030 --> 00:37:28,940
we are improving scalability by a pretty

00:37:25,820 --> 00:37:32,540
good margin I would say 75% weak scaling

00:37:28,940 --> 00:37:35,660
for this problem up to about 90% is very

00:37:32,540 --> 00:37:37,070
nice and while we are including strong

00:37:35,660 --> 00:37:40,430
scaling datasets I would just emphasize

00:37:37,070 --> 00:37:43,130
that the code runs so fast on Titan that

00:37:40,430 --> 00:37:45,080
we're running or production problems at

00:37:43,130 --> 00:37:47,060
this minimum nose configuration that we

00:37:45,080 --> 00:37:48,260
can this is a minimum amount this we

00:37:47,060 --> 00:37:49,369
could not run off your notes because

00:37:48,260 --> 00:37:51,530
there's not enough memory on the reviews

00:37:49,369 --> 00:37:52,790
so we run at this minimum node

00:37:51,530 --> 00:37:54,710
configuration and this is what we're

00:37:52,790 --> 00:37:56,240
happy with but we understand that for a

00:37:54,710 --> 00:37:58,580
scaling plot we also need to strong

00:37:56,240 --> 00:38:01,270
scale just a little bit so it seems like

00:37:58,580 --> 00:38:03,890
opening people 0.5 is helping us a lot

00:38:01,270 --> 00:38:06,490
just to to before I conclude I would

00:38:03,890 --> 00:38:08,780
like to just mention a couple of things

00:38:06,490 --> 00:38:10,250
oscar mentioned to me that is this would

00:38:08,780 --> 00:38:12,350
be a great opportunity to provide

00:38:10,250 --> 00:38:14,930
feedback and let us know let you let you

00:38:12,350 --> 00:38:17,390
know what what we needed or what worked

00:38:14,930 --> 00:38:20,450
and what didn't work so I mentioned this

00:38:17,390 --> 00:38:22,880
earlier but our code uses MPI

00:38:20,450 --> 00:38:26,270
communication calls with MPI derived

00:38:22,880 --> 00:38:28,760
types so the memory accesses are our

00:38:26,270 --> 00:38:31,190
strided and we let MPI take care of that

00:38:28,760 --> 00:38:32,810
for us but we're

00:38:31,190 --> 00:38:35,770
what we will need to do is we will need

00:38:32,810 --> 00:38:39,650
to update the buffers on on the host

00:38:35,770 --> 00:38:42,260
from the device and and if this inner

00:38:39,650 --> 00:38:44,390
dimension is is greater than 2 then this

00:38:42,260 --> 00:38:46,700
is a strided memory this is a non

00:38:44,390 --> 00:38:49,099
contiguous data transfer that we need to

00:38:46,700 --> 00:38:51,440
bring from from the device to the host

00:38:49,099 --> 00:38:54,170
for the communication call but the

00:38:51,440 --> 00:38:55,700
OpenMP standard I believe says that the

00:38:54,170 --> 00:38:58,460
the target data movement need to be

00:38:55,700 --> 00:39:00,109
contiguous so what we do now is I mean

00:38:58,460 --> 00:39:02,420
we're fine I mean like don't get me

00:39:00,109 --> 00:39:05,060
wrong like we love opening people 0.5

00:39:02,420 --> 00:39:07,400
and we're using it very much but what we

00:39:05,060 --> 00:39:09,349
do right now is we pack this so we tack

00:39:07,400 --> 00:39:10,910
this into a buffer and then we update

00:39:09,349 --> 00:39:12,710
and then we perform the communication

00:39:10,910 --> 00:39:15,579
call then we update again and then we

00:39:12,710 --> 00:39:15,579
unpack the buffer

00:39:22,940 --> 00:39:45,140
okay yeah so we map the entire array so

00:39:43,190 --> 00:39:50,780
it's a 3d array that we've mapped all of

00:39:45,140 --> 00:40:00,609
it to the device and now we I mean we

00:39:50,780 --> 00:40:07,000
would appreciate it because okay I think

00:40:00,609 --> 00:40:07,000
yeah if you're just in a little town

00:40:07,359 --> 00:40:13,460
okay okay yeah thank you very much um

00:40:10,640 --> 00:40:15,770
well we also found like this we like

00:40:13,460 --> 00:40:17,210
like I can't thank the cray compiler

00:40:15,770 --> 00:40:19,190
developers enough for all the time

00:40:17,210 --> 00:40:21,230
they've spent helping us understand

00:40:19,190 --> 00:40:24,020
things the one thing that I didn't

00:40:21,230 --> 00:40:25,490
understand was we needed to synchronize

00:40:24,020 --> 00:40:28,130
things it seemed like there wasn't a

00:40:25,490 --> 00:40:31,339
very intuitive construct to do this so

00:40:28,130 --> 00:40:32,660
what we do in the code is just to go

00:40:31,339 --> 00:40:34,099
back to this piece of code here I need

00:40:32,660 --> 00:40:37,250
to perform a communication call over the

00:40:34,099 --> 00:40:39,410
X 3 direction and earlier I have cued an

00:40:37,250 --> 00:40:41,569
on and asynchronous data movement to the

00:40:39,410 --> 00:40:43,339
host I need to synchronize before I

00:40:41,569 --> 00:40:45,440
perform this communication call so this

00:40:43,339 --> 00:40:48,230
is how we do it we do OMT target depend

00:40:45,440 --> 00:40:50,029
in and target it's an empty target task

00:40:48,230 --> 00:40:51,890
that doesn't do anything but it does

00:40:50,029 --> 00:40:53,990
provide the synchronization we need and

00:40:51,890 --> 00:40:56,869
from what I understand and the technical

00:40:53,990 --> 00:40:59,240
report 5 maybe I'm not the OpenMP

00:40:56,869 --> 00:41:01,730
standards person but I do believe that

00:40:59,240 --> 00:41:03,980
something like this is to be supported

00:41:01,730 --> 00:41:06,200
OMP tasks we depend where we can put

00:41:03,980 --> 00:41:07,880
this with a sync variable that we would

00:41:06,200 --> 00:41:09,680
need for given coordinate direction we

00:41:07,880 --> 00:41:11,480
would definitely appreciate that because

00:41:09,680 --> 00:41:13,789
I to me this would be more intuitive

00:41:11,480 --> 00:41:16,119
than the current way of synchronizing

00:41:13,789 --> 00:41:18,920
with an empty kind of dummy target task

00:41:16,119 --> 00:41:21,079
another thing that we that we are we are

00:41:18,920 --> 00:41:22,330
kind of still working with and trying to

00:41:21,079 --> 00:41:25,630
understand

00:41:22,330 --> 00:41:27,610
that when we have a synchronous target

00:41:25,630 --> 00:41:30,160
task like how do we get the timing

00:41:27,610 --> 00:41:31,600
information for these target tasks to

00:41:30,160 --> 00:41:33,130
kind of contrast this if I have an

00:41:31,600 --> 00:41:36,850
explicit task on the host that I put a

00:41:33,130 --> 00:41:39,520
depend cause on and or or whatever I can

00:41:36,850 --> 00:41:41,740
kind of put all the MPI W times and and

00:41:39,520 --> 00:41:43,990
I can time the beginning of the task at

00:41:41,740 --> 00:41:45,640
the end of the task and and and really

00:41:43,990 --> 00:41:47,770
understand like when did the task start

00:41:45,640 --> 00:41:49,780
when does it have complete and all that

00:41:47,770 --> 00:41:51,340
kind of information but for the target

00:41:49,780 --> 00:41:53,500
task I'm not really too sure how to do

00:41:51,340 --> 00:41:56,680
that other than using system software

00:41:53,500 --> 00:42:00,130
like env prof to get some of the kernel

00:41:56,680 --> 00:42:01,930
information so because I believe like

00:42:00,130 --> 00:42:04,030
with the target with the target task it

00:42:01,930 --> 00:42:05,500
has to and you have a team like there's

00:42:04,030 --> 00:42:08,410
not really much I can put in between the

00:42:05,500 --> 00:42:10,960
like the dudes like I I don't really

00:42:08,410 --> 00:42:12,340
know how to say get the start time in

00:42:10,960 --> 00:42:15,460
the end time for when this thing

00:42:12,340 --> 00:42:19,030
actually ran on the GPU and it would be

00:42:15,460 --> 00:42:21,340
really helpful to us when we try to say

00:42:19,030 --> 00:42:24,520
produce timeline data like this for our

00:42:21,340 --> 00:42:25,780
GPU code if the at the openmp target

00:42:24,520 --> 00:42:30,640
tasks were able to provide that

00:42:25,780 --> 00:42:33,880
information for us so anyway I just like

00:42:30,640 --> 00:42:36,940
to say that OpenMP has been very useful

00:42:33,880 --> 00:42:38,140
to us we enjoy it very much and we've

00:42:36,940 --> 00:42:40,840
been using it in our turbulence

00:42:38,140 --> 00:42:43,660
simulation codes for quite some time on

00:42:40,840 --> 00:42:46,630
blue waters we we we have found that

00:42:43,660 --> 00:42:48,280
using dedicated communication threads at

00:42:46,630 --> 00:42:50,020
least at the level blocks I want to

00:42:48,280 --> 00:42:51,880
continue to explore that that task based

00:42:50,020 --> 00:42:54,280
approach was with really improving

00:42:51,880 --> 00:42:57,190
scalability more than anything we saw

00:42:54,280 --> 00:43:00,010
with the pure MPI approach and then on

00:42:57,190 --> 00:43:03,430
Titan you know we have the good

00:43:00,010 --> 00:43:05,140
acceleration of the target task the the

00:43:03,430 --> 00:43:07,660
computational performance is very good

00:43:05,140 --> 00:43:10,060
it seems and we're also able to use the

00:43:07,660 --> 00:43:12,360
OpenMP 4.5 depend in no way to actually

00:43:10,060 --> 00:43:15,280
do the overlap to improve scalability

00:43:12,360 --> 00:43:17,710
the future work we are trying to work on

00:43:15,280 --> 00:43:20,290
a manuscript to to report these

00:43:17,710 --> 00:43:21,580
algorithms and we in the future we have

00:43:20,290 --> 00:43:23,620
to tackle the whole suit

00:43:21,580 --> 00:43:26,080
code like we can't we can't ignore that

00:43:23,620 --> 00:43:28,480
forever and then we're also working to

00:43:26,080 --> 00:43:31,330
port these kernels to summit Deb's I

00:43:28,480 --> 00:43:33,940
should say using Excel F and we've had

00:43:31,330 --> 00:43:36,220
some great interactions so far with the

00:43:33,940 --> 00:43:38,770
IBM compiler developers who have been

00:43:36,220 --> 00:43:40,680
very helpful for us to understand what

00:43:38,770 --> 00:43:43,950
what's needed for that

00:43:40,680 --> 00:43:43,950
so anyway

00:44:02,670 --> 00:44:05,960
burgers are tried

00:44:08,380 --> 00:44:14,960
so you can power

00:44:12,120 --> 00:44:14,960
just happened

00:44:15,460 --> 00:44:23,069
one that issue tomorrow the target tab

00:44:23,109 --> 00:44:26,099
the right

00:44:28,000 --> 00:44:45,300
I weigh 210 but I'm not very much

00:44:58,840 --> 00:45:03,260
should that be oily data that we and we

00:45:02,540 --> 00:45:05,360
go

00:45:03,260 --> 00:45:07,810
because once Internet nothing with eg

00:45:05,360 --> 00:45:07,810
beautiful

00:45:07,980 --> 00:45:10,980
willkommen

00:45:23,609 --> 00:45:30,739
that's all good food so I think

00:45:31,140 --> 00:45:35,250
there

00:45:32,200 --> 00:45:35,250
[Music]

00:45:37,930 --> 00:45:41,080
all right

00:45:50,000 --> 00:45:55,520
my rent with

00:45:52,580 --> 00:45:58,780
not doing it guy

00:45:55,520 --> 00:45:58,780
part of me and

00:46:00,429 --> 00:46:05,079
no so the the target tasks

00:46:06,440 --> 00:46:10,800
and of me

00:46:08,770 --> 00:46:10,800
Oh

00:46:13,180 --> 00:46:20,130
and not like maybe we have come

00:46:17,220 --> 00:46:22,650
preventing ability

00:46:20,130 --> 00:46:24,930
yes oh I think they have on Titan at

00:46:22,650 --> 00:46:27,990
least a support kind of like using the

00:46:24,930 --> 00:46:29,700
use of ice pointer and it's like I I

00:46:27,990 --> 00:46:32,100
can't remember the technical term but

00:46:29,700 --> 00:46:33,660
it's like NPI uses the device pointer to

00:46:32,100 --> 00:46:35,190
kind of figure out how to do the

00:46:33,660 --> 00:46:36,090
communication maybe a little bit more

00:46:35,190 --> 00:46:38,340
efficiently for you

00:46:36,090 --> 00:46:41,490
yeah our code is not written to do that

00:46:38,340 --> 00:46:43,320
but I mean we could try to do that in

00:46:41,490 --> 00:46:44,070
the future if the NPI implementations

00:46:43,320 --> 00:46:46,530
supported it

00:46:44,070 --> 00:46:49,500
I believe my my lab mate has done that

00:46:46,530 --> 00:46:51,590
some on Titan and summit death where his

00:46:49,500 --> 00:46:54,750
communication calls like is all tolls or

00:46:51,590 --> 00:46:57,270
they have a target like use device

00:46:54,750 --> 00:46:59,940
pointer and then the the NPI knows how

00:46:57,270 --> 00:47:02,010
to interpret that but I'm not 100% sure

00:46:59,940 --> 00:47:04,490
we this code was not develop with that

00:47:02,010 --> 00:47:04,490
in mind

00:47:13,260 --> 00:47:17,990
[Music]

00:47:15,350 --> 00:47:25,400
but it knows that the data it needs for

00:47:17,990 --> 00:47:28,530
the communication call it yeah yeah yeah

00:47:25,400 --> 00:47:28,530
[Music]

00:47:32,090 --> 00:47:36,170
[Music]

00:47:34,000 --> 00:47:38,690
yeah I know I I don't think that we

00:47:36,170 --> 00:47:41,789
would I mean I yeah yeah

00:47:38,690 --> 00:47:41,789
[Music]

00:47:43,800 --> 00:47:48,210
[Music]

00:47:46,230 --> 00:47:50,060
we provided

00:47:48,210 --> 00:47:53,720
[Music]

00:47:50,060 --> 00:47:56,950
oh great thank you yeah so I didn't know

00:47:53,720 --> 00:47:56,950
that but I'll take a look for sure

00:47:59,800 --> 00:48:01,830
Oh

00:48:07,290 --> 00:48:10,290

YouTube URL: https://www.youtube.com/watch?v=g_v6ynp00FM


