Title: Session-1-Keynote Address 1: OpenMP in the Exascale Era
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Kathryn O’Brien, Principal Research Staff Member, IBM T.J. Watson Research Center.
Over the last 8 years Kathryn has been part of the leadership team driving IBM Research’s Exascale program, where her focus has been on the evolution and development of the broader software programming and tools environment.  She is currently the IBM Research Compiler Strategist and Programming Models technical lead, as well as the PI for the CORAL NRE program. As part of this latter role she has been responsible for driving the Programming Model strategy for the IBM CORAL and future HPC and large-scale systems.
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day1-Keynote-OBrien.pdf
Captions: 
	00:00:00,130 --> 00:00:03,680
[Music]

00:00:01,979 --> 00:00:07,020
good morning

00:00:03,680 --> 00:00:12,509
I'm Catherine O'Brien from IBM research

00:00:07,020 --> 00:00:14,880
as Michael said I've been working at IBM

00:00:12,509 --> 00:00:23,359
research for a long time my background

00:00:14,880 --> 00:00:23,359
is in compilers and I hit this

00:00:24,750 --> 00:00:27,879
[Music]

00:00:35,990 --> 00:00:44,880
okay so I wanted to talk about OpenMP in

00:00:39,420 --> 00:00:47,700
the era of exascale as Michael said

00:00:44,880 --> 00:00:48,840
OpenMP has its 20th anniversary this

00:00:47,700 --> 00:00:52,440
year

00:00:48,840 --> 00:00:56,700
iBM has been involved in OpenMP for as

00:00:52,440 --> 00:00:59,210
long as it's been around and we've had

00:00:56,700 --> 00:01:03,360
some very good success stories not only

00:00:59,210 --> 00:01:06,420
in compiler implementations of OpenMP

00:01:03,360 --> 00:01:11,280
but also in actually helping to drive

00:01:06,420 --> 00:01:13,320
the openmp standard I chose to talk

00:01:11,280 --> 00:01:16,020
about open MP in the exascale era

00:01:13,320 --> 00:01:18,630
because I've been heavily involved in

00:01:16,020 --> 00:01:20,820
the choral program and the choral

00:01:18,630 --> 00:01:24,270
systems I think we would agree are the

00:01:20,820 --> 00:01:26,850
first pre exascale systems but this

00:01:24,270 --> 00:01:32,009
chart is really just a high level

00:01:26,850 --> 00:01:34,200
conceptual chart of where we are on the

00:01:32,009 --> 00:01:35,729
road map to exascale and one of the

00:01:34,200 --> 00:01:39,990
things I'd like to point out is that

00:01:35,729 --> 00:01:42,390
it's as we drive towards exascale the

00:01:39,990 --> 00:01:45,600
applications are changing dramatically

00:01:42,390 --> 00:01:49,500
so if we think about you know in the

00:01:45,600 --> 00:01:52,140
early 2010's in the Blue Gene era the

00:01:49,500 --> 00:01:55,350
real focus was on the performance of

00:01:52,140 --> 00:01:59,490
hitch PC applications Linpack in the top

00:01:55,350 --> 00:02:02,850
500 that kind of stuff looking forward

00:01:59,490 --> 00:02:05,330
to the you know the true exascale

00:02:02,850 --> 00:02:07,380
systems in the early 2020s

00:02:05,330 --> 00:02:10,979
you know we're beginning to talk about

00:02:07,380 --> 00:02:12,989
machine learning cognitive and what the

00:02:10,979 --> 00:02:15,150
architectures are they're going to you

00:02:12,989 --> 00:02:19,590
know support those kind of applications

00:02:15,150 --> 00:02:22,049
like in the blue-jean days well I guess

00:02:19,590 --> 00:02:24,660
I should say as the as the applications

00:02:22,049 --> 00:02:30,140
require more and more compute and have

00:02:24,660 --> 00:02:32,340
to handle larger and larger deficits the

00:02:30,140 --> 00:02:34,290
architectures are becoming more complex

00:02:32,340 --> 00:02:36,490
we're getting heterogeneity in the

00:02:34,290 --> 00:02:39,010
compute heterogeneity in the men

00:02:36,490 --> 00:02:41,650
in the memory and this of course puts a

00:02:39,010 --> 00:02:44,050
burden on the programmer how how to deal

00:02:41,650 --> 00:02:46,720
with all this complexity but if we think

00:02:44,050 --> 00:02:49,930
back to the blue-jean days the focus

00:02:46,720 --> 00:02:52,270
there was not on you know heterogeneity

00:02:49,930 --> 00:02:54,190
or different types of memories but you

00:02:52,270 --> 00:02:57,280
know how to deal with large number of

00:02:54,190 --> 00:02:59,590
threads and trying to get application

00:02:57,280 --> 00:03:03,250
developers even to think about threading

00:02:59,590 --> 00:03:06,820
their applications was a huge issue so

00:03:03,250 --> 00:03:09,370
moving forward the programming models

00:03:06,820 --> 00:03:11,190
for these architectures programming

00:03:09,370 --> 00:03:13,990
models have always been the bugbear of

00:03:11,190 --> 00:03:17,140
system architecture system architects

00:03:13,990 --> 00:03:22,320
and that continues to be the issue as we

00:03:17,140 --> 00:03:22,320
move forward to access skills

00:03:35,990 --> 00:03:41,170
[Music]

00:03:37,140 --> 00:03:43,210
okay so the architectural complexity it

00:03:41,170 --> 00:03:45,880
doesn't just involve the programming

00:03:43,210 --> 00:03:48,010
model it impacts all aspects update the

00:03:45,880 --> 00:03:50,350
system software stack in the execution

00:03:48,010 --> 00:03:52,390
environment the operating system in the

00:03:50,350 --> 00:03:56,400
runtime I mean the operating system has

00:03:52,390 --> 00:03:58,780
to you know as we move forward to

00:03:56,400 --> 00:04:02,380
heterogeneous systems we don't even have

00:03:58,780 --> 00:04:04,150
operating systems on the accelerators we

00:04:02,380 --> 00:04:07,390
have have to have extensions in the

00:04:04,150 --> 00:04:09,640
kernel there's very complex control

00:04:07,390 --> 00:04:12,940
systems resource managers schedulers

00:04:09,640 --> 00:04:14,440
debugging becomes the nightmare and all

00:04:12,940 --> 00:04:18,930
this as I said this is all going to

00:04:14,440 --> 00:04:18,930
increase significantly by 2022

00:04:19,200 --> 00:04:25,450
heterogeneous computer memory different

00:04:22,120 --> 00:04:28,240
memory attributes non-volatile memory

00:04:25,450 --> 00:04:31,660
how that fits in the system highly user

00:04:28,240 --> 00:04:34,360
works with non-volatile memory managing

00:04:31,660 --> 00:04:37,750
the huge reams of data and of course

00:04:34,360 --> 00:04:40,720
resiliency so imagining that we can get

00:04:37,750 --> 00:04:45,180
all this complexity supported in the

00:04:40,720 --> 00:04:48,220
system software is one thing but

00:04:45,180 --> 00:04:50,500
assuming we do that then the programming

00:04:48,220 --> 00:04:52,390
models is the interface between all that

00:04:50,500 --> 00:04:56,020
complexity and the application

00:04:52,390 --> 00:04:58,630
developers so not surprisingly focus on

00:04:56,020 --> 00:05:03,010
the programming model has been a major

00:04:58,630 --> 00:05:05,220
part of they move forward to excess

00:05:03,010 --> 00:05:05,220
scale

00:05:12,169 --> 00:05:15,810
so what are the constraints of a

00:05:14,460 --> 00:05:18,419
programming model a programming model

00:05:15,810 --> 00:05:23,040
has to be all things to all men and

00:05:18,419 --> 00:05:26,040
women and application developers it has

00:05:23,040 --> 00:05:30,139
to address a broad range of expertise it

00:05:26,040 --> 00:05:34,889
has to address the very experienced

00:05:30,139 --> 00:05:37,820
programmer it also has to address the

00:05:34,889 --> 00:05:40,770
low level of programming expertise

00:05:37,820 --> 00:05:44,430
everybody wants to be able to develop

00:05:40,770 --> 00:05:48,660
their applications quickly deploy them

00:05:44,430 --> 00:05:50,580
rapidly but performances is paramount we

00:05:48,660 --> 00:05:52,770
need to provide high-level abstractions

00:05:50,580 --> 00:05:56,430
with great arc mechanisms for critical

00:05:52,770 --> 00:05:58,410
performance paths more importantly we

00:05:56,430 --> 00:06:00,680
need to provide a migration path for

00:05:58,410 --> 00:06:03,300
legacy code and cross-platform

00:06:00,680 --> 00:06:04,620
performance portability and then we have

00:06:03,300 --> 00:06:06,900
to support a large range of

00:06:04,620 --> 00:06:10,229
implementation paths people want to use

00:06:06,900 --> 00:06:12,419
libraries they want to use languages new

00:06:10,229 --> 00:06:14,729
languages compiler extensions pride most

00:06:12,419 --> 00:06:16,650
directives and then as I alluded to

00:06:14,729 --> 00:06:18,660
earlier there's huge barriers to

00:06:16,650 --> 00:06:21,660
adoption just getting people to thread

00:06:18,660 --> 00:06:25,530
their codes you know five to ten years

00:06:21,660 --> 00:06:28,110
ago was a huge issue and actually

00:06:25,530 --> 00:06:29,970
continues you know even for some people

00:06:28,110 --> 00:06:35,940
to get them to move to threading is

00:06:29,970 --> 00:06:39,870
still an issue so the application

00:06:35,940 --> 00:06:44,910
programmers dilemma on the right-hand

00:06:39,870 --> 00:06:46,830
side we have the pipe dream of all

00:06:44,910 --> 00:06:49,560
application developers which is

00:06:46,830 --> 00:06:53,370
automatic optimization so I spent

00:06:49,560 --> 00:06:57,000
probably 20 years of my career working

00:06:53,370 --> 00:06:59,090
on optimizing compilers developing

00:06:57,000 --> 00:07:02,280
techniques for auto parallelization

00:06:59,090 --> 00:07:05,550
locality optimization in the days of

00:07:02,280 --> 00:07:08,780
cell we did automatic DMA optimization

00:07:05,550 --> 00:07:11,660
we had done some work on speculative

00:07:08,780 --> 00:07:15,510
parallelization for the perks project

00:07:11,660 --> 00:07:16,790
using helper threads and over time we

00:07:15,510 --> 00:07:18,440
came to realize

00:07:16,790 --> 00:07:22,120
no matter how good your dependents

00:07:18,440 --> 00:07:24,980
analysis is no matter how good your

00:07:22,120 --> 00:07:27,260
compiler is there's just so much

00:07:24,980 --> 00:07:29,540
information that you don't know at

00:07:27,260 --> 00:07:34,400
runtime that getting good automatic

00:07:29,540 --> 00:07:36,490
optimization is is really difficult when

00:07:34,400 --> 00:07:39,920
we started to work on this cell project

00:07:36,490 --> 00:07:43,640
I think that's when we began to realize

00:07:39,920 --> 00:07:49,090
if you really want to get the best code

00:07:43,640 --> 00:07:52,720
you need to start thinking about using

00:07:49,090 --> 00:07:56,030
hints in the compiler or a combination

00:07:52,720 --> 00:07:59,840
hinton in your code and maybe a

00:07:56,030 --> 00:08:02,540
combination of hints and and runtime

00:07:59,840 --> 00:08:02,980
techniques so while on the right-hand

00:08:02,540 --> 00:08:06,650
side

00:08:02,980 --> 00:08:11,600
optimization is obviously you know the

00:08:06,650 --> 00:08:13,190
best approach for the user it doesn't

00:08:11,600 --> 00:08:16,130
get the best control it doesn't get the

00:08:13,190 --> 00:08:17,720
best performance and it is frequently

00:08:16,130 --> 00:08:21,710
spotty

00:08:17,720 --> 00:08:23,690
very few compilers are really successful

00:08:21,710 --> 00:08:26,330
at across-the-board automatic

00:08:23,690 --> 00:08:28,520
parallelization then if we look on the

00:08:26,330 --> 00:08:30,020
left-hand side if you really want to get

00:08:28,520 --> 00:08:33,050
the highest performance you need the

00:08:30,020 --> 00:08:35,120
most intervention by the program you

00:08:33,050 --> 00:08:39,710
need to use manual techniques you need

00:08:35,120 --> 00:08:41,810
to do manual manually unroll your loops

00:08:39,710 --> 00:08:45,290
and you know you need to pour over your

00:08:41,810 --> 00:08:47,690
code and determine what the best what's

00:08:45,290 --> 00:08:49,370
the best you know iteratively look at

00:08:47,690 --> 00:08:51,560
your source code look at the code

00:08:49,370 --> 00:08:53,870
generated try out different techniques

00:08:51,560 --> 00:08:56,090
and the one thing I would say is when

00:08:53,870 --> 00:08:58,010
you get the best manual techniques quite

00:08:56,090 --> 00:09:00,770
often your compiler will screw that up

00:08:58,010 --> 00:09:02,090
so you put all this work in and you

00:09:00,770 --> 00:09:03,650
still don't end up with a bad

00:09:02,090 --> 00:09:08,510
performance

00:09:03,650 --> 00:09:11,360
so I guess over time I would say we've

00:09:08,510 --> 00:09:13,450
kind of acknowledged that automatic

00:09:11,360 --> 00:09:17,180
approaches are not going to cut it

00:09:13,450 --> 00:09:21,050
people don't really want to put the

00:09:17,180 --> 00:09:23,990
effort in to do the the pencil manual

00:09:21,050 --> 00:09:25,820
techniques that may get undone so we're

00:09:23,990 --> 00:09:28,430
kind of settling in this middle area

00:09:25,820 --> 00:09:31,660
here where you know we use a combination

00:09:28,430 --> 00:09:34,550
of higher-level pragmatist and directive

00:09:31,660 --> 00:09:37,970
which has the advantage of preserving

00:09:34,550 --> 00:09:41,560
hopefully the readability of the code so

00:09:37,970 --> 00:09:48,890
you get better performance with less

00:09:41,560 --> 00:09:51,020
outlay and hopefully good performance so

00:09:48,890 --> 00:09:58,430
that's kind of the dilemma that we see

00:09:51,020 --> 00:10:02,240
for for the programmer okay so going

00:09:58,430 --> 00:10:04,970
back to my my roadmap so looking back to

00:10:02,240 --> 00:10:06,830
the you know the blue jean era that's

00:10:04,970 --> 00:10:11,170
also the time when we started to think

00:10:06,830 --> 00:10:16,250
about the past two exascale 2010 or 2009

00:10:11,170 --> 00:10:19,130
there were many many workshops there was

00:10:16,250 --> 00:10:21,470
the iai ESP the International exascale

00:10:19,130 --> 00:10:24,820
software group that met on a regular

00:10:21,470 --> 00:10:30,650
basis at locations all over the world

00:10:24,820 --> 00:10:34,100
and the purpose of this was to define

00:10:30,650 --> 00:10:36,890
that the architecture software etcetera

00:10:34,100 --> 00:10:40,720
for for exascale and one of the big

00:10:36,890 --> 00:10:44,720
focuses back then was programming models

00:10:40,720 --> 00:10:49,450
so in those days exascale was 300

00:10:44,720 --> 00:10:53,730
petaflop in 2015 and an exaflop in 2018

00:10:49,450 --> 00:10:57,360
so back in in 2008 2009

00:10:53,730 --> 00:10:58,980
um there was a report there were reports

00:10:57,360 --> 00:11:01,529
on all the different aspects but the

00:10:58,980 --> 00:11:04,470
report on programming models concluded a

00:11:01,529 --> 00:11:06,660
number of things essentially they said

00:11:04,470 --> 00:11:11,220
that new programming models would not be

00:11:06,660 --> 00:11:14,040
feasible in the 2015 2018 timeframe if

00:11:11,220 --> 00:11:18,050
you think about it if you want to get a

00:11:14,040 --> 00:11:21,120
new language with a standards group and

00:11:18,050 --> 00:11:25,680
an accepted standard I mean that's like

00:11:21,120 --> 00:11:28,260
a minimum six or seven year effort

00:11:25,680 --> 00:11:31,829
assuming you've even you know agreed on

00:11:28,260 --> 00:11:33,240
what the language would be so it's the

00:11:31,829 --> 00:11:36,750
notion that you would be able to have a

00:11:33,240 --> 00:11:40,500
new programming model in you know 2015

00:11:36,750 --> 00:11:45,779
2018 back in 2009 was deemed to be not

00:11:40,500 --> 00:11:47,639
feasible what they recommended was to

00:11:45,779 --> 00:11:52,550
invest in a range of existing

00:11:47,639 --> 00:11:57,000
programming models monitor the sea

00:11:52,550 --> 00:11:59,069
current programming models outside the

00:11:57,000 --> 00:12:06,240
exascale community in those days that

00:11:59,069 --> 00:12:08,970
was things like CUDA OpenCL evolve the

00:12:06,240 --> 00:12:12,660
existing hybrid programming models MPI

00:12:08,970 --> 00:12:15,660
and openmp look at developing new hybrid

00:12:12,660 --> 00:12:17,370
models like MPI and PDF not sure why

00:12:15,660 --> 00:12:20,880
anybody would want to do that but it was

00:12:17,370 --> 00:12:23,790
something they were thinking of the

00:12:20,880 --> 00:12:26,250
holistic models like core a Fortran UPC

00:12:23,790 --> 00:12:28,199
H pcs were still considered regional

00:12:26,250 --> 00:12:30,180
reasonable approaches especially if you

00:12:28,199 --> 00:12:33,290
were looking for a system-wide

00:12:30,180 --> 00:12:33,290
programming model

00:12:33,360 --> 00:12:39,209
but the conclusion was that totally

00:12:35,790 --> 00:12:41,850
revolutionary approaches we're not a

00:12:39,209 --> 00:12:44,730
good idea was unlikely that revolution

00:12:41,850 --> 00:12:46,740
would happen but of course since you

00:12:44,730 --> 00:12:49,019
know this there is a research component

00:12:46,740 --> 00:12:52,050
to this we don't want to throw like the

00:12:49,019 --> 00:12:56,519
notion that it's possible you know some

00:12:52,050 --> 00:12:58,230
some new language will kind of magically

00:12:56,519 --> 00:13:01,740
appear that will make everything easy

00:12:58,230 --> 00:13:05,190
but in general the consensus in 2009

00:13:01,740 --> 00:13:08,370
2010 was monitoring the existing

00:13:05,190 --> 00:13:12,240
languages the one thing that they did

00:13:08,370 --> 00:13:14,970
make a note of was that the abstract

00:13:12,240 --> 00:13:17,579
machine model was changing and there

00:13:14,970 --> 00:13:19,290
needed to be a greater focus on intra

00:13:17,579 --> 00:13:23,430
note this is when we were beginning is

00:13:19,290 --> 00:13:26,430
around the time I guess with kind of

00:13:23,430 --> 00:13:30,959
just maybe finished with cell back then

00:13:26,430 --> 00:13:33,269
I think so cell the cell architecture I

00:13:30,959 --> 00:13:36,329
guess was the first hybrid architecture

00:13:33,269 --> 00:13:40,199
it had the power processor and they the

00:13:36,329 --> 00:13:42,690
SPU's and programmability was was not

00:13:40,199 --> 00:13:44,399
the strong point of that - we were

00:13:42,690 --> 00:13:47,339
beginning to understand that there was

00:13:44,399 --> 00:13:50,180
going to be more focus into it into a

00:13:47,339 --> 00:13:52,890
node and that the HPC programming models

00:13:50,180 --> 00:13:55,250
were not really you know they tended to

00:13:52,890 --> 00:13:58,250
follow rather than lead in the area of

00:13:55,250 --> 00:13:58,250
acceleration

00:14:01,840 --> 00:14:10,360
okay so going back to my conceptual

00:14:04,900 --> 00:14:15,760
roadmap so fast forwarding to where we

00:14:10,360 --> 00:14:19,779
are today so we're very soon gonna see

00:14:15,760 --> 00:14:23,820
delivery of the world's first fully data

00:14:19,779 --> 00:14:26,200
centric systems the Sierra and summit

00:14:23,820 --> 00:14:29,290
systems that will be delivered to

00:14:26,200 --> 00:14:33,940
Oakridge and Livermore

00:14:29,290 --> 00:14:36,460
so the roadmap to exascale to these pre

00:14:33,940 --> 00:14:40,920
exascale systems has been an exercise in

00:14:36,460 --> 00:14:43,600
Co design through collaboration so the

00:14:40,920 --> 00:14:48,070
multiple collaborations actually so they

00:14:43,600 --> 00:14:51,900
the choral systems are a combination of

00:14:48,070 --> 00:14:58,450
IBM melon oxen and video technology in

00:14:51,900 --> 00:15:00,450
2014 we were awarded a large contract to

00:14:58,450 --> 00:15:04,720
build these supercomputers

00:15:00,450 --> 00:15:07,710
for for Oak Ridge and Livermore Oak

00:15:04,720 --> 00:15:11,680
Ridge had experience with heterogeneous

00:15:07,710 --> 00:15:14,589
architectures in the Titan system but

00:15:11,680 --> 00:15:17,080
for Lawrence Livermore the move to a

00:15:14,589 --> 00:15:20,560
heterogeneous architecture was a big

00:15:17,080 --> 00:15:24,060
jump it was a big shift to to think

00:15:20,560 --> 00:15:26,740
about moving your applications to

00:15:24,060 --> 00:15:28,800
something a very different kind of

00:15:26,740 --> 00:15:32,650
architecture so one of the major

00:15:28,800 --> 00:15:35,890
concerns as we started off and this

00:15:32,650 --> 00:15:38,290
journey was that on day one we would

00:15:35,890 --> 00:15:43,720
have applications ported and ready to

00:15:38,290 --> 00:15:45,940
run on these systems also that the codes

00:15:43,720 --> 00:15:48,220
that would be ported and running on

00:15:45,940 --> 00:15:52,660
these systems in day one would still be

00:15:48,220 --> 00:15:55,330
running on in ten years time and would

00:15:52,660 --> 00:15:59,970
be available for you know the evolution

00:15:55,330 --> 00:16:03,010
of architectures so a large part of the

00:15:59,970 --> 00:16:04,430
not a large part a significant part of

00:16:03,010 --> 00:16:10,279
the

00:16:04,430 --> 00:16:12,769
program was we had a an NRI portion that

00:16:10,279 --> 00:16:15,649
was developing software for these

00:16:12,769 --> 00:16:21,760
architectures and a significant part of

00:16:15,649 --> 00:16:21,760
that enery was the establishment of

00:16:22,540 --> 00:16:27,500
centers of excellence so we have three

00:16:25,459 --> 00:16:31,970
centers of excellence so the notion is

00:16:27,500 --> 00:16:35,209
that we have IBM application team

00:16:31,970 --> 00:16:37,610
members on-site at both Oak Ridge and

00:16:35,209 --> 00:16:40,839
Livermore working very closely with the

00:16:37,610 --> 00:16:45,740
applications team so we started out with

00:16:40,839 --> 00:16:49,940
a center of excellence at Livermore and

00:16:45,740 --> 00:16:52,100
then we very quickly added an extension

00:16:49,940 --> 00:16:54,920
to set up a center of excellence at Oak

00:16:52,100 --> 00:16:57,490
Ridge and then subsequently we added

00:16:54,920 --> 00:16:59,449
another center of excellence at

00:16:57,490 --> 00:17:01,899
Livermore to deal with their

00:16:59,449 --> 00:17:06,740
institutional codes rather than their

00:17:01,899 --> 00:17:11,110
mission code and the way those the idea

00:17:06,740 --> 00:17:11,110
behind the centers of excellence is that

00:17:12,100 --> 00:17:18,410
in Oakridge they have their car codes

00:17:15,589 --> 00:17:20,600
which are people apply with their

00:17:18,410 --> 00:17:24,530
science applications and get selected

00:17:20,600 --> 00:17:27,770
and then I guess Oscar share he knows

00:17:24,530 --> 00:17:30,340
about the car applications and so the

00:17:27,770 --> 00:17:37,250
car teams work with the IBM team so to

00:17:30,340 --> 00:17:40,010
support these applications the problem

00:17:37,250 --> 00:17:43,070
with the this plan of course is that we

00:17:40,010 --> 00:17:47,240
didn't have compilers we didn't have

00:17:43,070 --> 00:17:49,580
systems so getting up and running to get

00:17:47,240 --> 00:17:52,040
these appliqués applications Center T

00:17:49,580 --> 00:17:55,220
Centers of Excellence was quite

00:17:52,040 --> 00:17:59,860
challenging but fortunately at IBM we

00:17:55,220 --> 00:18:02,480
had been working on some compilers for a

00:17:59,860 --> 00:18:05,000
research architecture and active memory

00:18:02,480 --> 00:18:07,849
tube architecture and we had started to

00:18:05,000 --> 00:18:10,149
do some work on developing a

00:18:07,849 --> 00:18:12,200
an LLVM compiler for heterogeneous

00:18:10,149 --> 00:18:13,909
architectures and we were able to shift

00:18:12,200 --> 00:18:19,210
gears very quickly when the program

00:18:13,909 --> 00:18:24,320
started and start to develop an open NP

00:18:19,210 --> 00:18:28,519
LLVM compiler for the the Kuril

00:18:24,320 --> 00:18:30,349
architecture and actually within I guess

00:18:28,519 --> 00:18:33,859
the first six months of the program

00:18:30,349 --> 00:18:35,659
starting we had a compiler that the

00:18:33,859 --> 00:18:39,169
application developers were able to

00:18:35,659 --> 00:18:42,139
start playing with it wasn't it was a

00:18:39,169 --> 00:18:45,279
very raw compiler it they were rapidly

00:18:42,139 --> 00:18:48,979
working on function but at least we had

00:18:45,279 --> 00:18:50,989
something for people to work with to

00:18:48,979 --> 00:18:52,609
start porting these applications and

00:18:50,989 --> 00:19:05,349
this turns out to have being a very key

00:18:52,609 --> 00:19:07,719
part of of the program so just

00:19:05,349 --> 00:19:10,429
confirming what I said

00:19:07,719 --> 00:19:13,210
if you look on the exascale about our

00:19:10,429 --> 00:19:18,529
website there's lots of stuff about the

00:19:13,210 --> 00:19:21,859
delivery of the the summit system to

00:19:18,529 --> 00:19:25,149
Oakridge they talk about the Dakar users

00:19:21,859 --> 00:19:28,190
they're actually installing the system

00:19:25,149 --> 00:19:29,830
right now I think we're currently a week

00:19:28,190 --> 00:19:33,499
ahead of schedule

00:19:29,830 --> 00:19:35,809
I think the livermore systems are a

00:19:33,499 --> 00:19:37,670
little bit further behind but still in

00:19:35,809 --> 00:19:41,540
progress and

00:19:37,670 --> 00:19:43,970
as they say I mean it'll be 2019 before

00:19:41,540 --> 00:19:46,640
you know all the users are on board with

00:19:43,970 --> 00:19:50,900
these but by the middle of next year we

00:19:46,640 --> 00:19:54,800
expect people to be fully exercising

00:19:50,900 --> 00:19:58,130
these systems and and yet we will have

00:19:54,800 --> 00:20:06,800
compilers openmp compilers for our users

00:19:58,130 --> 00:20:10,700
to use on these systems okay so in 2016

00:20:06,800 --> 00:20:15,800
we released the Minsky systems which was

00:20:10,700 --> 00:20:22,040
an early as an architecture based on the

00:20:15,800 --> 00:20:25,040
poet and the Pascal GPUs the the labs

00:20:22,040 --> 00:20:29,780
and other people got access early access

00:20:25,040 --> 00:20:32,870
to these systems and so that they could

00:20:29,780 --> 00:20:35,060
you know be able to start sorting their

00:20:32,870 --> 00:20:37,130
codes and have an idea of what you know

00:20:35,060 --> 00:20:40,520
what what the issues were so I'm just

00:20:37,130 --> 00:20:43,870
going to take a little look at some of

00:20:40,520 --> 00:20:46,280
the work that we did to enable

00:20:43,870 --> 00:20:48,620
high-level programming for these open

00:20:46,280 --> 00:20:51,680
Parmentier architectures and most of

00:20:48,620 --> 00:20:56,470
what I say for for open par also applies

00:20:51,680 --> 00:20:56,470
to the the choral architecture

00:20:59,110 --> 00:21:09,159
okay so from a compiler viewpoint the

00:21:05,140 --> 00:21:12,730
open part architecture looks like on the

00:21:09,159 --> 00:21:18,250
left-hand side we have the fat course

00:21:12,730 --> 00:21:20,169
with SMT it s empty threats procore 160

00:21:18,250 --> 00:21:22,899
threads per processor there's complex

00:21:20,169 --> 00:21:25,630
branch prediction and an out-of-order

00:21:22,899 --> 00:21:31,360
pipeline on the right hand side we have

00:21:25,630 --> 00:21:35,769
the highly parallel CUDA cores our sorry

00:21:31,360 --> 00:21:38,679
highly parallel GPUs with order 10 to 15

00:21:35,769 --> 00:21:40,710
threads per device thread divergence is

00:21:38,679 --> 00:21:44,049
serialized and there's limited

00:21:40,710 --> 00:21:46,350
synchronization within the CUDA blocks

00:21:44,049 --> 00:21:46,350
only

00:21:49,889 --> 00:21:54,359
in terms of the memory on the par side

00:21:52,169 --> 00:21:56,369
we have the very large caches with low

00:21:54,359 --> 00:22:00,389
memory latency and we can optimize for

00:21:56,369 --> 00:22:03,960
cache locality rise on the on the GPU

00:22:00,389 --> 00:22:06,599
side we have caches that are relatively

00:22:03,960 --> 00:22:09,349
smaller on chip small compared to the

00:22:06,599 --> 00:22:12,570
number of threads the Alto is off trip

00:22:09,349 --> 00:22:16,399
something equivalent to the l4 and in

00:22:12,570 --> 00:22:19,859
the p8 and here we want to optimize for

00:22:16,399 --> 00:22:22,700
to hide memory latency by using lots of

00:22:19,859 --> 00:22:22,700
in-flight threads

00:22:28,050 --> 00:22:35,010
so if you think about it the open power

00:22:32,340 --> 00:22:38,550
architecture is focusing on flexible

00:22:35,010 --> 00:22:41,640
acceleration so we believe that it

00:22:38,550 --> 00:22:44,610
delivers the best performance beyond the

00:22:41,640 --> 00:22:47,010
HPC field there's a balanced combination

00:22:44,610 --> 00:22:50,130
of the power processors and the GPU

00:22:47,010 --> 00:22:53,010
accelerators but going back to to my

00:22:50,130 --> 00:22:56,580
theme how do we effectively and

00:22:53,010 --> 00:23:02,520
efficiently program this open power

00:22:56,580 --> 00:23:11,970
architecture so what are the programming

00:23:02,520 --> 00:23:13,950
options for for this architecture so we

00:23:11,970 --> 00:23:17,340
considered what our options were how to

00:23:13,950 --> 00:23:22,260
program this mix of of large fat cores

00:23:17,340 --> 00:23:25,770
and small thin processors the options

00:23:22,260 --> 00:23:32,610
really if you look on the GPU side you

00:23:25,770 --> 00:23:36,120
have CUDA you can use CUDA with C C++ or

00:23:32,610 --> 00:23:38,460
Fortran in terms of high-level construct

00:23:36,120 --> 00:23:41,900
I would say there aren't really any

00:23:38,460 --> 00:23:45,150
high-level constructs with CUDA

00:23:41,900 --> 00:23:47,310
so what companies or organizations are

00:23:45,150 --> 00:23:51,570
actually actively involved well we have

00:23:47,310 --> 00:23:54,180
Nvidia PGI and IBM to the extent that

00:23:51,570 --> 00:23:57,000
IBM of course you can use to do with our

00:23:54,180 --> 00:24:01,830
Excel C compilers we did also develop

00:23:57,000 --> 00:24:05,910
accrued a Fortran compiler and CUDA is

00:24:01,830 --> 00:24:08,700
available only on the GPU you can move

00:24:05,910 --> 00:24:11,130
up a nuts you can move to a higher level

00:24:08,700 --> 00:24:14,910
programming model for your GPUs by

00:24:11,130 --> 00:24:19,290
moving to open ACC you've got parallel

00:24:14,910 --> 00:24:22,260
look like an assembly constructs its

00:24:19,290 --> 00:24:25,070
supported by Cray PGI and I think

00:24:22,260 --> 00:24:28,260
there's not I think there is a GCC

00:24:25,070 --> 00:24:30,010
implementation also and it's targeted

00:24:28,260 --> 00:24:34,900
for GPU like

00:24:30,010 --> 00:24:36,490
accelerators well then we looked at if

00:24:34,900 --> 00:24:37,840
you do that you still need to worry

00:24:36,490 --> 00:24:41,950
about how you're going to exploit the

00:24:37,840 --> 00:24:45,070
parallelism on the host so if we look at

00:24:41,950 --> 00:24:47,650
openmp it has all the high level

00:24:45,070 --> 00:24:51,010
constructs the parallel the loop like

00:24:47,650 --> 00:24:56,020
this MD the tasking it's supported by a

00:24:51,010 --> 00:25:00,540
broad range of companies IBM AMD Intel

00:24:56,020 --> 00:25:03,610
free Pascal TI etcetera and it features

00:25:00,540 --> 00:25:05,950
mostly architecture independent flexible

00:25:03,610 --> 00:25:08,110
parallelism so these were the options

00:25:05,950 --> 00:25:16,990
that we looked at for programming open

00:25:08,110 --> 00:25:19,240
power then what are the what's on the

00:25:16,990 --> 00:25:21,040
wish list for the programmers who want

00:25:19,240 --> 00:25:24,130
to program this well they want

00:25:21,040 --> 00:25:27,940
high-level parallel abstractions that

00:25:24,130 --> 00:25:30,820
are architecture independent they want

00:25:27,940 --> 00:25:34,320
the ability to cover a high range of

00:25:30,820 --> 00:25:37,660
application patterns dense worse graphs

00:25:34,320 --> 00:25:39,640
they want performance portability they

00:25:37,660 --> 00:25:42,040
want a single version of a kernel to be

00:25:39,640 --> 00:25:44,680
able to run on both the the host and the

00:25:42,040 --> 00:25:49,240
GPU they want to get the best

00:25:44,680 --> 00:25:51,220
performance possible across both they

00:25:49,240 --> 00:25:52,450
want easy incremental development they

00:25:51,220 --> 00:25:54,910
don't want to rewrite entire

00:25:52,450 --> 00:25:57,000
applications in a new language but they

00:25:54,910 --> 00:26:00,790
do want to be able to interoperate with

00:25:57,000 --> 00:26:02,920
assembly kernels and libraries and they

00:26:00,790 --> 00:26:07,800
want continuity and for continuity you

00:26:02,920 --> 00:26:07,800
really do need industry standard

00:26:07,860 --> 00:26:14,370
languages and

00:26:11,270 --> 00:26:16,679
directive and you want something that's

00:26:14,370 --> 00:26:21,360
supported everywhere and that it will

00:26:16,679 --> 00:26:25,020
survive the fashions or trends and

00:26:21,360 --> 00:26:29,970
architectures so we looked at the range

00:26:25,020 --> 00:26:31,650
of options and we decided for easy

00:26:29,970 --> 00:26:33,840
porting continuity performance

00:26:31,650 --> 00:26:36,210
portability high level abstractions that

00:26:33,840 --> 00:26:38,820
we were going to work with openmp as the

00:26:36,210 --> 00:26:44,640
programming model of choice for it for

00:26:38,820 --> 00:26:49,020
the open part architecture so they open

00:26:44,640 --> 00:26:51,840
MP 4.5 design goals write once run

00:26:49,020 --> 00:26:54,360
everywhere with the best performance so

00:26:51,840 --> 00:26:57,539
as I said if you want to program a GPU

00:26:54,360 --> 00:27:00,870
you have a range of options crudo OpenCL

00:26:57,539 --> 00:27:04,100
if you want to program a CPU Sindhi unit

00:27:00,870 --> 00:27:08,940
you can use intrinsic OpenCL you can

00:27:04,100 --> 00:27:13,679
possibly rely on auto vectorization if

00:27:08,940 --> 00:27:15,870
you want to program a CPU you've got C++

00:27:13,679 --> 00:27:18,799
1114 whatever yeah

00:27:15,870 --> 00:27:24,679
thread building blocks silk

00:27:18,799 --> 00:27:27,929
continuations whatever so with open MP

00:27:24,679 --> 00:27:31,100
4.5 and up you can use the same standard

00:27:27,929 --> 00:27:34,650
to program across this range of

00:27:31,100 --> 00:27:37,850
architectures host and accelerators so

00:27:34,650 --> 00:27:37,850
that was the choice we made

00:27:40,769 --> 00:27:49,539
and so the the open par Carl programming

00:27:45,549 --> 00:27:54,490
model roadmap that we laid out was in

00:27:49,539 --> 00:27:58,450
the short-term we would propose OpenMP

00:27:54,490 --> 00:28:02,559
4.5 and open AC C compilers available

00:27:58,450 --> 00:28:04,570
for both provide interoperability with

00:28:02,559 --> 00:28:07,690
CUDA of course but in the longer term

00:28:04,570 --> 00:28:11,440
we're saying that we would drive towards

00:28:07,690 --> 00:28:13,750
open MP 5 as the dominant approach we

00:28:11,440 --> 00:28:20,769
really would like to see a convergence

00:28:13,750 --> 00:28:23,980
in this standard and there's a lot of

00:28:20,769 --> 00:28:26,080
people are actually thinking about what

00:28:23,980 --> 00:28:28,649
would a directive optional threading

00:28:26,080 --> 00:28:30,760
model look like that could support

00:28:28,649 --> 00:28:33,130
execution of across the range of

00:28:30,760 --> 00:28:36,279
homogeneous and heterogeneous core types

00:28:33,130 --> 00:28:39,159
that I think not something we're going

00:28:36,279 --> 00:28:41,200
to see in the exascale era but it's

00:28:39,159 --> 00:28:46,389
definitely something that that people

00:28:41,200 --> 00:28:49,389
are thinking about so while the roadmap

00:28:46,389 --> 00:28:51,220
that that we laid out says you know we

00:28:49,389 --> 00:28:54,639
believe that openmp will be the dominant

00:28:51,220 --> 00:28:56,529
programming model on the IBM

00:28:54,639 --> 00:28:58,809
heterogeneous architectures we do

00:28:56,529 --> 00:29:02,019
support a range of programming models so

00:28:58,809 --> 00:29:04,809
you will be able to use CUDA with C C++

00:29:02,019 --> 00:29:11,889
and Fortran with the host compilers

00:29:04,809 --> 00:29:14,289
being GCC and Excel we do support open

00:29:11,889 --> 00:29:16,539
ACC in fact as part of our choral

00:29:14,289 --> 00:29:21,429
program we worked very closely with PDI

00:29:16,539 --> 00:29:23,940
to develop their open ACC compiler we

00:29:21,429 --> 00:29:27,540
share a power back-end in the LLVM

00:29:23,940 --> 00:29:31,020
infrastructure with PGI

00:29:27,540 --> 00:29:36,210
so those compilers are available and

00:29:31,020 --> 00:29:38,970
then for OpenMP we support two compilers

00:29:36,210 --> 00:29:43,620
we have the IBM proprietary excel

00:29:38,970 --> 00:29:48,330
compiler which supports OpenMP 4.5 for

00:29:43,620 --> 00:29:52,050
offloading I think the fat compiler will

00:29:48,330 --> 00:29:55,230
da at the end of this year and we also

00:29:52,050 --> 00:29:59,040
as part of the coral NRI we developed up

00:29:55,230 --> 00:30:02,630
we had a project that has been

00:29:59,040 --> 00:30:04,650
developing openmp in the LLVM

00:30:02,630 --> 00:30:09,720
infrastructure so that's the range of

00:30:04,650 --> 00:30:12,470
options for programming the open part

00:30:09,720 --> 00:30:12,470
architectures

00:30:14,420 --> 00:30:19,490
so I was just going to take a few

00:30:17,960 --> 00:30:21,920
minutes and kind of look a little more

00:30:19,490 --> 00:30:26,090
closely at the OpenMP programming model

00:30:21,920 --> 00:30:31,160
and why why it's so suitable for this

00:30:26,090 --> 00:30:33,740
kind of architecture so openmp of course

00:30:31,160 --> 00:30:35,750
is a fork and joint model right three

00:30:33,740 --> 00:30:38,420
sequential code is executed by the

00:30:35,750 --> 00:30:40,550
master and the parallel code is executed

00:30:38,420 --> 00:30:42,470
by the master and the workers the

00:30:40,550 --> 00:30:45,590
parallel regions terminated by a

00:30:42,470 --> 00:30:47,030
synchronization barrier and the memory

00:30:45,590 --> 00:30:49,700
touched in the parallel region is

00:30:47,030 --> 00:30:51,680
released or flushed at the barrier so

00:30:49,700 --> 00:30:57,020
that's kind of the high-level execution

00:30:51,680 --> 00:31:00,220
model it provides flexible parallelism

00:30:57,020 --> 00:31:04,430
you can you can distribute your code

00:31:00,220 --> 00:31:10,250
across threads if that's what your

00:31:04,430 --> 00:31:12,110
application or architecture prefers we

00:31:10,250 --> 00:31:15,140
have done significant performance

00:31:12,110 --> 00:31:18,710
optimization for successive small

00:31:15,140 --> 00:31:23,620
parallel loops on the other hand if your

00:31:18,710 --> 00:31:27,050
application is is more suited to Sindhi

00:31:23,620 --> 00:31:28,940
deployment you can use the parallel the

00:31:27,050 --> 00:31:30,710
parallel loops can be spread across your

00:31:28,940 --> 00:31:33,560
sympathy architecture so there's

00:31:30,710 --> 00:31:35,210
flexibility in the and the way your

00:31:33,560 --> 00:31:37,700
parallelism is deployed on your

00:31:35,210 --> 00:31:41,780
architecture in conjunction with your

00:31:37,700 --> 00:31:45,230
application requirements another

00:31:41,780 --> 00:31:48,920
important thing is thread affinity the

00:31:45,230 --> 00:31:51,980
ability to specify how your threads bind

00:31:48,920 --> 00:31:55,759
to two cores or sockets or

00:31:51,980 --> 00:31:58,309
whatever to get the best

00:31:55,759 --> 00:32:01,789
the best memory usage in in your

00:31:58,309 --> 00:32:04,820
architecture so for example if you have

00:32:01,789 --> 00:32:06,409
small compute with lots of data you

00:32:04,820 --> 00:32:10,669
might want to spread your threads across

00:32:06,409 --> 00:32:12,710
the sockets if you have compute intense

00:32:10,669 --> 00:32:17,630
and you want to optimize for locality

00:32:12,710 --> 00:32:23,059
you can keep your threads close on the

00:32:17,630 --> 00:32:28,669
threads with within your course okay so

00:32:23,059 --> 00:32:32,559
looking at taking the openmp standard

00:32:28,669 --> 00:32:36,649
which is essentially a shared memory

00:32:32,559 --> 00:32:40,309
programming model and now how do we use

00:32:36,649 --> 00:32:43,279
that for programming host plus

00:32:40,309 --> 00:32:49,460
accelerators so the T construct in

00:32:43,279 --> 00:32:52,370
OpenMP is the target so any code within

00:32:49,460 --> 00:32:57,830
a target region will be offloaded to

00:32:52,370 --> 00:33:00,230
your device and the default map to from

00:32:57,830 --> 00:33:02,990
will copy your variables over to the

00:33:00,230 --> 00:33:05,929
device for the computation then you need

00:33:02,990 --> 00:33:08,559
to copy the data back out on the

00:33:05,929 --> 00:33:08,559
completion

00:33:12,210 --> 00:33:17,519
so on the device we can see the same

00:33:15,210 --> 00:33:22,289
kind of flexible parallelism as on the

00:33:17,519 --> 00:33:26,869
host for parallel looks on the GPU the

00:33:22,289 --> 00:33:30,149
each team so we have this target teams

00:33:26,869 --> 00:33:33,119
each team corresponds to a CUDA block

00:33:30,149 --> 00:33:35,580
the OpenMP threads are CUDA threads and

00:33:33,119 --> 00:33:38,840
then the distribute schedules blocks of

00:33:35,580 --> 00:33:38,840
iterations to the teams

00:33:43,890 --> 00:33:49,650
so Cindy inside parallel is very widely

00:33:47,250 --> 00:33:53,400
used on the host to leverage the vector

00:33:49,650 --> 00:33:56,340
units for the threads the GPU doesn't

00:33:53,400 --> 00:34:00,000
have vector units but if you use the

00:33:56,340 --> 00:34:02,760
pragma OMP cindy you can map your sims

00:34:00,000 --> 00:34:05,160
ii lens onto cuda threats so we have the

00:34:02,760 --> 00:34:11,730
same kind of flexibility as you see on

00:34:05,160 --> 00:34:14,490
the host then the target constructs

00:34:11,730 --> 00:34:16,800
their implicit if you think about it

00:34:14,490 --> 00:34:19,889
their implicit tasks right and a host

00:34:16,800 --> 00:34:23,850
can actually initiate several target

00:34:19,889 --> 00:34:26,580
tasks asynchronously those target tasks

00:34:23,850 --> 00:34:28,290
on the device may have dependencies and

00:34:26,580 --> 00:34:31,409
those dependencies between the target

00:34:28,290 --> 00:34:37,919
tasks are resolved completely on the GPU

00:34:31,409 --> 00:34:40,560
without host intervention so you can

00:34:37,919 --> 00:34:46,040
exploit the like concurrency both within

00:34:40,560 --> 00:34:50,399
the host and the device simultaneously

00:34:46,040 --> 00:34:52,560
the the host threads you kind of the

00:34:50,399 --> 00:34:55,560
host threads can take advantage of

00:34:52,560 --> 00:34:58,560
multiple GPUs in a node so you kind of

00:34:55,560 --> 00:35:02,160
asynchronous execution on multiple GPUs

00:34:58,560 --> 00:35:04,320
and at the same time you can overlap the

00:35:02,160 --> 00:35:08,420
computer the device computation and

00:35:04,320 --> 00:35:12,530
communication with the with the

00:35:08,420 --> 00:35:12,530
execution on on the host

00:35:12,830 --> 00:35:19,890
and again they see any dependencies with

00:35:16,800 --> 00:35:25,280
it within the device are satisfied on

00:35:19,890 --> 00:35:27,720
the device sorry

00:35:25,280 --> 00:35:30,410
PowerPoint has encountered a problem and

00:35:27,720 --> 00:35:30,410
nature too close

00:35:58,940 --> 00:36:03,670
that was a reason not to use my laptop

00:36:40,840 --> 00:36:46,300
yes my powerpoint crashed incisal not

00:36:45,310 --> 00:36:51,130
sure what happened

00:36:46,300 --> 00:36:55,470
I have it on them if I can't get it back

00:36:51,130 --> 00:36:58,670
I have it I'm going to reboot sorry

00:36:55,470 --> 00:36:58,670
[Music]

00:37:06,510 --> 00:37:09,659
[Music]

00:37:10,340 --> 00:37:13,340
reputation

00:37:17,290 --> 00:37:19,320
ah

00:37:28,160 --> 00:37:35,410
I have my talk on a memory stick if

00:37:31,280 --> 00:37:35,410
somebody has a laptop that I'm taken

00:37:41,140 --> 00:37:45,059
oh maybe it's coming back

00:37:52,460 --> 00:37:55,619
[Music]

00:38:16,450 --> 00:38:19,890
I think is coming back to us

00:38:28,130 --> 00:38:31,340
[Music]

00:38:35,310 --> 00:38:38,449
[Music]

00:38:44,990 --> 00:38:49,540
come

00:38:46,370 --> 00:38:49,540
[Music]

00:39:07,079 --> 00:39:10,880
I got

00:39:18,539 --> 00:39:25,450
yes I don't know what I'm is just it's

00:39:21,520 --> 00:39:27,609
probably you're gonna be just like the

00:39:25,450 --> 00:39:29,740
phone sometimes in conference calls they

00:39:27,609 --> 00:39:33,460
say they haven't had any input for a

00:39:29,740 --> 00:39:36,210
long time so I guess have decided too

00:39:33,460 --> 00:39:36,210
much hot air

00:39:38,690 --> 00:39:45,970
okay so I think I covered the demand

00:39:42,650 --> 00:39:52,009
demand constructs for offloading

00:39:45,970 --> 00:39:55,880
execution to the device but really the

00:39:52,009 --> 00:39:57,980
major complexity and performance concern

00:39:55,880 --> 00:40:02,359
in programming these heterogeneous

00:39:57,980 --> 00:40:05,390
systems is the data movement right so

00:40:02,359 --> 00:40:07,329
it's very important to minimize data

00:40:05,390 --> 00:40:11,809
movement right so you really need to

00:40:07,329 --> 00:40:15,019
understand the the data scope the data

00:40:11,809 --> 00:40:18,079
types that can be mapped and the memory

00:40:15,019 --> 00:40:20,599
model so we say here it's distributed

00:40:18,079 --> 00:40:25,400
memory in the current implementation so

00:40:20,599 --> 00:40:27,940
I guess what we're saying is in our

00:40:25,400 --> 00:40:30,410
compiler implementation we assume two

00:40:27,940 --> 00:40:33,079
distinct memories the host memory and

00:40:30,410 --> 00:40:39,640
the GPU memory as you're probably aware

00:40:33,079 --> 00:40:42,980
the open par the Minsky systems have

00:40:39,640 --> 00:40:44,839
software support for unified memory

00:40:42,980 --> 00:40:46,940
which means under the covers

00:40:44,839 --> 00:40:51,680
data is moved between the host and

00:40:46,940 --> 00:40:53,359
device and the Witherspoon's volta time

00:40:51,680 --> 00:40:55,609
frame there there durably hardware

00:40:53,359 --> 00:40:57,769
support for that and the performance

00:40:55,609 --> 00:41:00,920
that what they provide in the earlier

00:40:57,769 --> 00:41:04,250
systems is more functional than

00:41:00,920 --> 00:41:07,400
performing but in the Carleton frame

00:41:04,250 --> 00:41:10,029
that should be highly performing so in

00:41:07,400 --> 00:41:13,339
theory you will not have to worry about

00:41:10,029 --> 00:41:16,549
explicitly moving your data in the

00:41:13,339 --> 00:41:20,029
current implementation we're saying that

00:41:16,549 --> 00:41:22,460
the memory model is distributed and in

00:41:20,029 --> 00:41:25,269
fact that's what the OpenMP standard

00:41:22,460 --> 00:41:25,269
support

00:41:26,369 --> 00:41:33,869
so overcoming data movement is is the

00:41:29,729 --> 00:41:36,930
big challenge so for example and in the

00:41:33,869 --> 00:41:39,359
default the data scope is limited by the

00:41:36,930 --> 00:41:44,039
target construct so in an example like

00:41:39,359 --> 00:41:48,569
this you would have to explicitly copy C

00:41:44,039 --> 00:41:51,690
between after two pieces of code inside

00:41:48,569 --> 00:41:53,369
the target region so that seems like a

00:41:51,690 --> 00:41:57,239
little bit unnecessary

00:41:53,369 --> 00:42:01,440
there are ways where you can keep C

00:41:57,239 --> 00:42:09,450
resident on the device so that eliminate

00:42:01,440 --> 00:42:11,819
the copies when you do that then the map

00:42:09,450 --> 00:42:15,089
clauses are ignored so if you want to

00:42:11,819 --> 00:42:17,700
explicitly reference your variable on

00:42:15,089 --> 00:42:22,160
the host you have to use either the

00:42:17,700 --> 00:42:22,160
update or they always qualifier

00:42:26,300 --> 00:42:31,610
I think that's Sam I thought I took

00:42:28,460 --> 00:42:35,690
better ok so then on structured data

00:42:31,610 --> 00:42:39,080
movement data that that doesn't have a

00:42:35,690 --> 00:42:42,140
lexical scope so this scope of gyration

00:42:39,080 --> 00:42:44,420
is on the device is dictated by the

00:42:42,140 --> 00:42:49,250
runtime you get that effect by with

00:42:44,420 --> 00:42:52,940
using the enter and exit clauses in an

00:42:49,250 --> 00:42:56,560
open MP then you can have some data

00:42:52,940 --> 00:42:59,510
always resident on the accelerator and

00:42:56,560 --> 00:43:02,420
again if you want to move that back but

00:42:59,510 --> 00:43:05,060
you do this by using the to declare

00:43:02,420 --> 00:43:07,100
target and preparing your data within

00:43:05,060 --> 00:43:09,080
that if you want to move it back and

00:43:07,100 --> 00:43:14,930
forth again you have to use the target

00:43:09,080 --> 00:43:18,050
update so the summary of the data scope

00:43:14,930 --> 00:43:20,240
I guess the the point of it might like

00:43:18,050 --> 00:43:24,560
to make here is that managing the data

00:43:20,240 --> 00:43:28,400
is is the most provides the most

00:43:24,560 --> 00:43:30,290
complexity and in using open MP 4.5 on

00:43:28,400 --> 00:43:34,130
the devices it's the thing that our

00:43:30,290 --> 00:43:37,520
users complain the most about and there

00:43:34,130 --> 00:43:40,850
are there are issues still that are not

00:43:37,520 --> 00:43:45,010
resolve some of these issues like the

00:43:40,850 --> 00:43:48,410
deep copy how we deal with that so

00:43:45,010 --> 00:43:51,250
something to be aware of and something

00:43:48,410 --> 00:43:53,770
that you know we need to keep working on

00:43:51,250 --> 00:43:56,870
going back to this notion of the

00:43:53,770 --> 00:43:59,690
accelerator memory model so if you

00:43:56,870 --> 00:44:05,800
assume unified memory like on the left

00:43:59,690 --> 00:44:05,800
hand side the value of C you know is

00:44:06,579 --> 00:44:12,199
may change during execution of the

00:44:09,679 --> 00:44:13,399
target region if you assume the

00:44:12,199 --> 00:44:16,339
distributed model

00:44:13,399 --> 00:44:21,699
you clearly have to copy your data in an

00:44:16,339 --> 00:44:26,059
arc so there's issues to do with

00:44:21,699 --> 00:44:31,639
assuming the unified memory and using

00:44:26,059 --> 00:44:35,689
OpenMP if you write your program without

00:44:31,639 --> 00:44:38,029
using the map clauses and rely on on

00:44:35,689 --> 00:44:41,479
unified memory to do the copies for you

00:44:38,029 --> 00:44:43,669
it's actually not a valid OpenMP program

00:44:41,479 --> 00:44:46,069
and that's something that's a bit of a

00:44:43,669 --> 00:44:49,429
concern for some of our users people

00:44:46,069 --> 00:44:51,739
want to be able to rapidly prototype

00:44:49,429 --> 00:44:54,559
their code relying on unified memories

00:44:51,739 --> 00:44:57,859
and heavily profile looking for the hot

00:44:54,559 --> 00:44:59,659
spots once they you know understand the

00:44:57,859 --> 00:45:03,439
memory patterns within the program then

00:44:59,659 --> 00:45:06,949
they want to use more specifically

00:45:03,439 --> 00:45:08,569
targeted data directives so that it's a

00:45:06,949 --> 00:45:13,239
usage model that that people are

00:45:08,569 --> 00:45:17,779
beginning to play with from a compiler

00:45:13,239 --> 00:45:22,519
perspective the compiler can selectively

00:45:17,779 --> 00:45:24,919
decide when to copy the data but the

00:45:22,519 --> 00:45:29,179
thing is to have a valid program the

00:45:24,919 --> 00:45:32,919
user must still copy the you know use

00:45:29,179 --> 00:45:32,919
the specified

00:45:33,460 --> 00:45:41,500
caused us to manage the data okay so

00:45:43,390 --> 00:45:50,039
real-world scientific applications

00:45:46,200 --> 00:45:54,250
things like multiphysics applications

00:45:50,039 --> 00:45:56,260
they have different kinds of codes that

00:45:54,250 --> 00:45:58,329
want to be executed on the same

00:45:56,260 --> 00:46:01,299
architecture so for example if you have

00:45:58,329 --> 00:46:03,400
a graph like workload that's highly

00:46:01,299 --> 00:46:05,650
irregular that benefits from large

00:46:03,400 --> 00:46:08,380
caches etc you might want to run that

00:46:05,650 --> 00:46:11,260
that coat that piece of code on your on

00:46:08,380 --> 00:46:13,480
your host processor but your data

00:46:11,260 --> 00:46:18,099
parallel pieces of your workload you

00:46:13,480 --> 00:46:20,140
might want to run on the CPU so we think

00:46:18,099 --> 00:46:22,809
that you know this architecture in

00:46:20,140 --> 00:46:28,269
combination with using OpenMP gives you

00:46:22,809 --> 00:46:30,869
the a very good flexible parallelism

00:46:28,269 --> 00:46:30,869
approach

00:46:32,260 --> 00:46:38,740
how does OpenMP differ from your typical

00:46:35,710 --> 00:46:41,470
GPU programming model while the

00:46:38,740 --> 00:46:45,700
traditional models they say execute this

00:46:41,470 --> 00:46:48,009
one exclusively parallel loop the way

00:46:45,700 --> 00:46:50,559
you do with your CUDA kernels if you

00:46:48,009 --> 00:46:53,259
think about openmp we like to say that

00:46:50,559 --> 00:46:56,490
it's just another normal openmp program

00:46:53,259 --> 00:46:59,200
on the device what do we mean that so

00:46:56,490 --> 00:47:02,680
you can leverage all the openmp

00:46:59,200 --> 00:47:05,349
constructs parallel regions parallel

00:47:02,680 --> 00:47:07,390
loops tasks there's fine grain and

00:47:05,349 --> 00:47:11,740
coarse grain synchronization and you can

00:47:07,390 --> 00:47:13,839
have sequential and parallel code OpenMP

00:47:11,740 --> 00:47:16,240
does support the traditional model - if

00:47:13,839 --> 00:47:21,849
you use the target teams distributed

00:47:16,240 --> 00:47:26,240
parallel for assembly and then here is

00:47:21,849 --> 00:47:29,339
just a list of the

00:47:26,240 --> 00:47:32,910
features in OpenMP the new features in

00:47:29,339 --> 00:47:35,549
OpenMP 4.5 and i think if you look at

00:47:32,910 --> 00:47:40,319
this you'll see that there's a huge

00:47:35,549 --> 00:47:43,500
amount of work of additional features

00:47:40,319 --> 00:47:50,250
that have been added even between 4.0

00:47:43,500 --> 00:47:52,500
and 4.5 and that's something that we

00:47:50,250 --> 00:47:56,849
need to be very careful about because

00:47:52,500 --> 00:47:58,589
the more new stuff you're adding if it's

00:47:56,849 --> 00:48:01,559
this chicken and egg thing again right

00:47:58,589 --> 00:48:03,780
so they these architectures are evolving

00:48:01,559 --> 00:48:07,079
the standard is evolving to keep Tesla's

00:48:03,780 --> 00:48:10,859
the architecture and everybody wants

00:48:07,079 --> 00:48:14,970
everything yesterday so there's a lot of

00:48:10,859 --> 00:48:18,030
changes going in there's a lot of need

00:48:14,970 --> 00:48:21,900
to keep you know backward compatibility

00:48:18,030 --> 00:48:26,099
so just a lot of stuff going on but I

00:48:21,900 --> 00:48:30,690
think the progress is is is really very

00:48:26,099 --> 00:48:34,920
good ok just a little bit of so as we

00:48:30,690 --> 00:48:36,680
work our way through you know in the

00:48:34,920 --> 00:48:39,869
centers of excellence

00:48:36,680 --> 00:48:43,230
of course everyone frantically wants to

00:48:39,869 --> 00:48:45,539
show the performance versus just

00:48:43,230 --> 00:48:48,869
executing on the hosts or performance

00:48:45,539 --> 00:48:51,480
versus using CUDA so we have a couple I

00:48:48,869 --> 00:48:54,089
mean we've actually published a fair

00:48:51,480 --> 00:48:57,660
amount of stuff in our with our LLVM

00:48:54,089 --> 00:48:59,910
compiler so one example is this Kripke

00:48:57,660 --> 00:49:03,510
which is an application that's part of

00:48:59,910 --> 00:49:06,029
our our centers of excellence so we

00:49:03,510 --> 00:49:09,210
started porting to open NPM CUDA at the

00:49:06,029 --> 00:49:13,020
same time in the open MP version we use

00:49:09,210 --> 00:49:16,440
the collapse clause and it's hard to

00:49:13,020 --> 00:49:19,000
reproduce that effect into it so

00:49:16,440 --> 00:49:22,890
we actually were surprised to see that

00:49:19,000 --> 00:49:26,350
the OpenMP performance the dark color

00:49:22,890 --> 00:49:29,290
were significantly better this is just

00:49:26,350 --> 00:49:31,180
in the first blush were significantly

00:49:29,290 --> 00:49:33,960
better than the truth code

00:49:31,180 --> 00:49:39,990
subsequently the CUDA code has been

00:49:33,960 --> 00:49:44,950
debug to catch up this is just the

00:49:39,990 --> 00:49:49,150
kernel that we use the clef another

00:49:44,950 --> 00:49:52,780
example was foolish where we have four

00:49:49,150 --> 00:49:56,290
one two three four five of the kernels

00:49:52,780 --> 00:49:59,260
we get better or comparable performance

00:49:56,290 --> 00:50:01,710
tuturro there's a couple at the bottom

00:49:59,260 --> 00:50:05,590
that we're still working on four we have

00:50:01,710 --> 00:50:09,400
worse performance included but if we can

00:50:05,590 --> 00:50:16,000
you know do as well or better that's

00:50:09,400 --> 00:50:20,500
pretty good then this is a view so this

00:50:16,000 --> 00:50:24,610
is an example of improvements in the

00:50:20,500 --> 00:50:30,250
compiler so our original code generation

00:50:24,610 --> 00:50:34,630
scheme was really very much targeted to

00:50:30,250 --> 00:50:38,830
the SPMD model of the of the GPUs and

00:50:34,630 --> 00:50:41,830
resulted in very complex control flow

00:50:38,830 --> 00:50:45,490
within the generated code and also we

00:50:41,830 --> 00:50:50,020
weren't able to do nested parallelism so

00:50:45,490 --> 00:50:52,420
we recently reworked that part of the

00:50:50,020 --> 00:50:55,120
the compiler and find that we get

00:50:52,420 --> 00:50:58,890
significant speed up so that the pink

00:50:55,120 --> 00:51:01,120
bars are showing the performance

00:50:58,890 --> 00:51:04,720
speed-up versus the original

00:51:01,120 --> 00:51:06,940
implementation in the compiler and the

00:51:04,720 --> 00:51:12,010
green and the purple are they speed up

00:51:06,940 --> 00:51:13,240
versus the existing GCC OpenMP and ACC

00:51:12,010 --> 00:51:15,670
compilers

00:51:13,240 --> 00:51:20,470
and these are collection of kernels from

00:51:15,670 --> 00:51:23,170
speck XL and others so it's just really

00:51:20,470 --> 00:51:26,770
to point out that that not only are we

00:51:23,170 --> 00:51:33,430
focusing on improved on performance

00:51:26,770 --> 00:51:35,710
comparison with CUDA and GPU performance

00:51:33,430 --> 00:51:37,870
versus host performance but we're

00:51:35,710 --> 00:51:40,710
constantly reworking the compiler rims

00:51:37,870 --> 00:51:44,290
of the LLVM compiler implementation to

00:51:40,710 --> 00:51:51,010
you know to improve the performance and

00:51:44,290 --> 00:51:54,760
get rid of overhead so just some summary

00:51:51,010 --> 00:51:58,450
thoughts and all of that as I said

00:51:54,760 --> 00:52:02,590
openmp 4.5 is a relatively new standard

00:51:58,450 --> 00:52:05,850
evolving to OpenMP 5 as I said

00:52:02,590 --> 00:52:09,040
chicken-and-egg the implementation of

00:52:05,850 --> 00:52:10,630
the compilers the firming of the

00:52:09,040 --> 00:52:12,040
standard and the porting of the

00:52:10,630 --> 00:52:16,720
applications are all happening

00:52:12,040 --> 00:52:19,960
concurrently so the lessons learned so

00:52:16,720 --> 00:52:22,920
far from porting codes and specifically

00:52:19,960 --> 00:52:25,600
from managing multiple multiple memories

00:52:22,920 --> 00:52:30,300
may lead to new features in the standard

00:52:25,600 --> 00:52:34,360
and also in the implementation the LLVM

00:52:30,300 --> 00:52:39,990
IBM comes IVM LLVM implementation is

00:52:34,360 --> 00:52:43,330
fully 4.5 compliant we also do some

00:52:39,990 --> 00:52:45,730
prototyping of standards features the

00:52:43,330 --> 00:52:50,650
experience so far importing applications

00:52:45,730 --> 00:52:53,850
is positive yes I think I've covered

00:52:50,650 --> 00:52:53,850
most of that so

00:52:55,850 --> 00:53:03,220
I have five minutes okay so looking

00:53:00,740 --> 00:53:05,450
forward to exascale so here's a real

00:53:03,220 --> 00:53:07,370
chart of where we're going to at

00:53:05,450 --> 00:53:13,430
exascale again this is from the exascale

00:53:07,370 --> 00:53:16,250
computing project website if you look at

00:53:13,430 --> 00:53:21,560
where we are today 2017 they're

00:53:16,250 --> 00:53:25,550
suggesting a huge leap in performance so

00:53:21,560 --> 00:53:28,730
between 2017 and 2021 we're going to

00:53:25,550 --> 00:53:33,430
have to have a 5x improvement in

00:53:28,730 --> 00:53:36,200
performance so the mind boggles at the

00:53:33,430 --> 00:53:38,090
at the architectural innovation that's

00:53:36,200 --> 00:53:41,270
going to get us there so the issue of

00:53:38,090 --> 00:53:45,710
programming model is not going to to go

00:53:41,270 --> 00:53:49,880
away but if I look back now what are

00:53:45,710 --> 00:53:52,640
they saying about applications and

00:53:49,880 --> 00:53:55,760
programming models so you'll notice that

00:53:52,640 --> 00:53:57,770
this notion of data science machine

00:53:55,760 --> 00:54:00,680
learning problems in addition to the

00:53:57,770 --> 00:54:05,990
traditional applications is very front

00:54:00,680 --> 00:54:09,350
and center from a programming model

00:54:05,990 --> 00:54:13,760
perspective exploiting onload memory and

00:54:09,350 --> 00:54:16,070
compute hierarchies achieving portable

00:54:13,760 --> 00:54:20,750
performance without this if stepping

00:54:16,070 --> 00:54:23,000
code is a big issue when they talk

00:54:20,750 --> 00:54:25,820
explicitly about programming model so

00:54:23,000 --> 00:54:28,940
they say openmp represents a community

00:54:25,820 --> 00:54:31,190
standard the ultimate of get objective

00:54:28,940 --> 00:54:34,370
of working effectively blah blah the

00:54:31,190 --> 00:54:38,090
point of this is OpenMP is Furley is

00:54:34,370 --> 00:54:40,330
firmly fixed as a programming model of

00:54:38,090 --> 00:54:43,800
choice for exascale

00:54:40,330 --> 00:54:46,720
so in terms of

00:54:43,800 --> 00:54:49,690
strengths and challenges what does that

00:54:46,720 --> 00:54:51,880
mean so I would argue that the advances

00:54:49,690 --> 00:54:54,130
in the last three years it's very well

00:54:51,880 --> 00:54:58,420
positioned for this dominant rolodex of

00:54:54,130 --> 00:55:00,670
skill we've shown a significant amount

00:54:58,420 --> 00:55:03,700
of progress in evolving the standard to

00:55:00,670 --> 00:55:05,800
increase to address the architectural

00:55:03,700 --> 00:55:08,260
complexity we have reference

00:55:05,800 --> 00:55:12,250
implementations and research prototyping

00:55:08,260 --> 00:55:15,910
and then there's the salt project which

00:55:12,250 --> 00:55:20,080
Barbara's is the P ion this whole

00:55:15,910 --> 00:55:22,630
project is essentially being funded to

00:55:20,080 --> 00:55:25,210
do very much the same kind of thing that

00:55:22,630 --> 00:55:27,220
we did in the LLVM compiler is part of

00:55:25,210 --> 00:55:29,470
coral worked very closely with the

00:55:27,220 --> 00:55:32,200
application developers the Standards

00:55:29,470 --> 00:55:36,570
Committee to make sure that the that

00:55:32,200 --> 00:55:42,570
openmp evolves to address the needs of

00:55:36,570 --> 00:55:46,180
the community in terms of challenges I

00:55:42,570 --> 00:55:47,710
would say the main thing is to continue

00:55:46,180 --> 00:55:50,500
supporting a broad range of

00:55:47,710 --> 00:55:53,800
heterogeneity right now we have NVIDIA

00:55:50,500 --> 00:55:56,980
GPUs we need to be sure that the

00:55:53,800 --> 00:55:59,500
implementation is general not just

00:55:56,980 --> 00:56:02,590
specific to Nvidia performance

00:55:59,500 --> 00:56:06,520
portability that's in progress there's a

00:56:02,590 --> 00:56:09,490
lot of activity to ensure performance

00:56:06,520 --> 00:56:11,470
portability increased complexity I think

00:56:09,490 --> 00:56:14,500
is probably one of the major challenges

00:56:11,470 --> 00:56:17,770
facing this standard right now if I talk

00:56:14,500 --> 00:56:22,120
to our users that one of the main

00:56:17,770 --> 00:56:24,070
problems backward compatibility getting

00:56:22,120 --> 00:56:26,170
things right the first time as I said

00:56:24,070 --> 00:56:28,330
everything's happening very quickly so

00:56:26,170 --> 00:56:31,000
that that's something we need to keep an

00:56:28,330 --> 00:56:34,350
eye on I would say broadening adoption

00:56:31,000 --> 00:56:37,030
beyond the current user community is

00:56:34,350 --> 00:56:39,640
would be important the more users the

00:56:37,030 --> 00:56:41,470
more feedback and then expanding into

00:56:39,640 --> 00:56:42,860
additional application demands like

00:56:41,470 --> 00:56:47,540
machine learning and

00:56:42,860 --> 00:56:49,310
cetera in terms of performance

00:56:47,540 --> 00:56:52,640
portability there's a lot of work going

00:56:49,310 --> 00:56:55,490
on the Centers of Excellence they have

00:56:52,640 --> 00:56:58,670
performance portability workshops we've

00:56:55,490 --> 00:57:01,160
had several hackathons that you know

00:56:58,670 --> 00:57:04,430
produce very good feedback for the

00:57:01,160 --> 00:57:05,330
compilers and for the and for the

00:57:04,430 --> 00:57:08,390
standards group

00:57:05,330 --> 00:57:10,910
I think interoperation with dsl things

00:57:08,390 --> 00:57:13,460
so that the last performance portability

00:57:10,910 --> 00:57:16,100
workshop it got a little kind of hairy

00:57:13,460 --> 00:57:18,680
because every person new to this game

00:57:16,100 --> 00:57:21,020
wants to invent their own abstraction

00:57:18,680 --> 00:57:23,690
their own DSL but a couple are coming to

00:57:21,020 --> 00:57:26,180
the forefront Raja and Coco's and I

00:57:23,690 --> 00:57:29,000
think interoperability between openmp

00:57:26,180 --> 00:57:31,730
and Raja it will be very important going

00:57:29,000 --> 00:57:33,710
forward and I think one of the things

00:57:31,730 --> 00:57:37,670
that really would be good to see in this

00:57:33,710 --> 00:57:40,880
standard is some consideration of a

00:57:37,670 --> 00:57:45,710
descriptive capability versus the

00:57:40,880 --> 00:57:48,320
prescriptive to some extent an

00:57:45,710 --> 00:57:51,470
opportunity for the compilers to be to

00:57:48,320 --> 00:57:55,520
have a freer hand within the constraints

00:57:51,470 --> 00:57:57,080
of the of the programming model this is

00:57:55,520 --> 00:58:00,350
just a chart showing work we've been

00:57:57,080 --> 00:58:04,370
doing on open MTN Raja and demonstrating

00:58:00,350 --> 00:58:07,910
that when using a comparison between

00:58:04,370 --> 00:58:10,940
using vanilla open MP and open MP within

00:58:07,910 --> 00:58:14,180
Raja and in except for the two outliers

00:58:10,940 --> 00:58:18,430
at the first two we're showing fairly

00:58:14,180 --> 00:58:18,430
good performance with Raja

00:58:18,660 --> 00:58:28,349
so concluding thoughts the programming

00:58:24,000 --> 00:58:31,259
model landscape is huge it can be vendor

00:58:28,349 --> 00:58:34,019
driven and proprietary they can be

00:58:31,259 --> 00:58:36,509
standards driven providing a unified

00:58:34,019 --> 00:58:38,519
approach and then there's as I said the

00:58:36,509 --> 00:58:42,599
research oriented that where you get the

00:58:38,519 --> 00:58:44,880
plethora of abstractions very few new

00:58:42,599 --> 00:58:47,700
models have really caught on in the past

00:58:44,880 --> 00:58:51,029
ten years many have disappeared

00:58:47,700 --> 00:58:54,079
thankfully the established hybrid models

00:58:51,029 --> 00:58:56,819
continue to have the most traction

00:58:54,079 --> 00:58:59,640
OpenMP has made significant progress in

00:58:56,819 --> 00:59:03,089
the last twenty years and I think we

00:58:59,640 --> 00:59:08,269
need to build on that and take it

00:59:03,089 --> 00:59:08,269

YouTube URL: https://www.youtube.com/watch?v=BRmYcVo1u7k


