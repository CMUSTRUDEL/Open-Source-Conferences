Title: Session-5: User-Defined Schedules for OpenMP
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Vivek Kale and William Gropp
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day1-Session2-Kale.pdf
Captions: 
	00:00:00,810 --> 00:00:07,410
my today I'd like to talk about proposal

00:00:03,959 --> 00:00:11,370
for a user-defined schedule for openmp

00:00:07,410 --> 00:00:14,849
the first author is myself and the

00:00:11,370 --> 00:00:23,689
second author is my former advisor broke

00:00:14,849 --> 00:00:29,099
up today today I just like to talk about

00:00:23,689 --> 00:00:31,439
that would need to allow for the rapid

00:00:29,099 --> 00:00:35,280
development of loop scheduling

00:00:31,439 --> 00:00:38,550
strategies and then suggest giving users

00:00:35,280 --> 00:00:42,050
of openmp applications control of the

00:00:38,550 --> 00:00:46,710
loop scheduling strategy and finally

00:00:42,050 --> 00:00:50,399
propose a user-defined schedule for

00:00:46,710 --> 00:00:57,390
addition into the OpenMP specification

00:00:50,399 --> 00:01:01,079
or standard so to start off HP EFI

00:00:57,390 --> 00:01:03,420
applications are rapidly changing and if

00:01:01,079 --> 00:01:07,380
the architectures are rapidly changing

00:01:03,420 --> 00:01:09,479
for architectures there are large number

00:01:07,380 --> 00:01:13,670
of cores per node and that's going to

00:01:09,479 --> 00:01:18,630
increase with the next generation of

00:01:13,670 --> 00:01:22,650
node architectures there's also a speed

00:01:18,630 --> 00:01:25,619
variability across cores of a node this

00:01:22,650 --> 00:01:30,689
this is often due to frequency scaling

00:01:25,619 --> 00:01:34,259
for energy energy preservation

00:01:30,689 --> 00:01:40,020
and for applications there's dynamic

00:01:34,259 --> 00:01:46,259
behavior and across course or node due

00:01:40,020 --> 00:01:50,220
to a load imbalance of a molecular

00:01:46,259 --> 00:01:53,970
dynamics of computation or embody

00:01:50,220 --> 00:01:59,149
computation or the irregular accesses of

00:01:53,970 --> 00:01:59,149
a code like an adaptive mesh refinement

00:01:59,690 --> 00:02:07,100
so with that we really need new methods

00:02:04,080 --> 00:02:12,830
to distributed applications

00:02:07,100 --> 00:02:18,959
computational work to a nodes course and

00:02:12,830 --> 00:02:22,380
so these methods have to ensure data

00:02:18,959 --> 00:02:25,769
locality and reduce synchronization

00:02:22,380 --> 00:02:29,489
overhead while also maintaining load

00:02:25,769 --> 00:02:34,010
balance they need to have an awareness

00:02:29,489 --> 00:02:39,170
of the across node parallelism such as

00:02:34,010 --> 00:02:43,110
of that handled by libraries like mph

00:02:39,170 --> 00:02:47,970
and these methods should also adapt

00:02:43,110 --> 00:02:51,780
during an applications execution due to

00:02:47,970 --> 00:02:54,200
the variability of applications at

00:02:51,780 --> 00:02:54,200
runtime

00:02:56,569 --> 00:03:01,519
the utility of novel scheduling

00:02:58,969 --> 00:03:05,409
strategies is shown through some tire

00:03:01,519 --> 00:03:09,230
work published by myself and others

00:03:05,409 --> 00:03:15,790
here's just one example of a novel loop

00:03:09,230 --> 00:03:25,030
scheduling strategy here I show a on the

00:03:15,790 --> 00:03:30,280
on your left on your right sorry the

00:03:25,030 --> 00:03:34,849
iteration in iteration at just unthread

00:03:30,280 --> 00:03:37,159
diagram where the x-axis is the

00:03:34,849 --> 00:03:41,569
iteration count and the y-axis is thread

00:03:37,159 --> 00:03:46,750
number and you see here is at the top is

00:03:41,569 --> 00:03:51,200
a static schedule representation my

00:03:46,750 --> 00:03:55,090
strategy is to mix static and dynamic

00:03:51,200 --> 00:03:59,629
scheduling by doing some fraction of the

00:03:55,090 --> 00:04:04,819
work of loop statically and then the

00:03:59,629 --> 00:04:09,290
remainder dynamically we call the amount

00:04:04,819 --> 00:04:14,299
of dynamic work the dynamic fraction and

00:04:09,290 --> 00:04:18,349
it's denoted in the diagram by FD and so

00:04:14,299 --> 00:04:20,539
the motivation of this is to limit the

00:04:18,349 --> 00:04:23,000
overhead that you have in dynamic

00:04:20,539 --> 00:04:26,169
scheduling such as coherence cache

00:04:23,000 --> 00:04:30,080
misses and synchronization overhead

00:04:26,169 --> 00:04:35,360
while also handling load imbalances such

00:04:30,080 --> 00:04:44,180
as that due to the our core variability

00:04:35,360 --> 00:04:53,289
or OS noise and also the applications

00:04:44,180 --> 00:05:00,139
load imbalance on so on the on your left

00:04:53,289 --> 00:05:04,720
you see the on your right you see the a

00:05:00,139 --> 00:05:11,960
diagram of a communication avoiding

00:05:04,720 --> 00:05:16,910
dense matrix factorization that is the

00:05:11,960 --> 00:05:21,590
same diagram is on on your right under

00:05:16,910 --> 00:05:26,180
on this side and what your what you see

00:05:21,590 --> 00:05:29,690
is an application profile of static

00:05:26,180 --> 00:05:34,610
scheduling on an AMD Opteron 16 core

00:05:29,690 --> 00:05:39,229
machine and the white is idle time what

00:05:34,610 --> 00:05:43,310
your what with a static dynamic schedule

00:05:39,229 --> 00:05:47,479
you see the idle times reduced and

00:05:43,310 --> 00:05:54,889
thereby the execution times reduced here

00:05:47,479 --> 00:05:57,659
we use the dynamic fraction of 0.1 so

00:05:54,889 --> 00:06:00,330
all right there I also just want to

00:05:57,659 --> 00:06:04,939
mention you can also have more advanced

00:06:00,330 --> 00:06:09,150
methods such as scheduling whole process

00:06:04,939 --> 00:06:16,919
and you can adjust this dynamics action

00:06:09,150 --> 00:06:21,089
based on MPI slack and lots of and I've

00:06:16,919 --> 00:06:24,319
done work with applications to show

00:06:21,089 --> 00:06:24,319
benefits here as well

00:06:27,040 --> 00:06:32,990
so that's why I want to propose a

00:06:29,840 --> 00:06:37,580
user-defined schedule and OpenMP and

00:06:32,990 --> 00:06:40,460
there's issues in doing so the practice

00:06:37,580 --> 00:06:42,320
right now of during all the scheduling

00:06:40,460 --> 00:06:44,930
strategy inside an open MP

00:06:42,320 --> 00:06:49,670
implementation really isn't adequate for

00:06:44,930 --> 00:06:53,000
this purpose also many of the open MP

00:06:49,670 --> 00:06:57,020
implementations like Lib pcc's love golf

00:06:53,000 --> 00:07:01,070
and LLVM is open MP are really hard to

00:06:57,020 --> 00:07:03,020
change and that that really hinders the

00:07:01,070 --> 00:07:09,380
development of route scheduling strategy

00:07:03,020 --> 00:07:11,630
is that a lot of rapid pace so let's

00:07:09,380 --> 00:07:14,270
just consider the scheduling code of

00:07:11,630 --> 00:07:17,390
look dump as it stands today right now

00:07:14,270 --> 00:07:19,820
it supports the static dynamic and

00:07:17,390 --> 00:07:22,780
guided schedules naturally the problem

00:07:19,820 --> 00:07:26,110
is the code structure can't really

00:07:22,780 --> 00:07:28,370
support the number and sophisticated

00:07:26,110 --> 00:07:33,160
sophistication of the static strategies

00:07:28,370 --> 00:07:36,320
that we want to explore and so the

00:07:33,160 --> 00:07:40,450
addition of a user-defined schedule into

00:07:36,320 --> 00:07:45,290
openmp libraries is possible with effort

00:07:40,450 --> 00:07:48,880
for one library and has to be different

00:07:45,290 --> 00:07:48,880
for each library

00:07:50,759 --> 00:07:58,659
look so let me just exploit some more

00:07:55,419 --> 00:08:04,889
concrete reasons for user-defined

00:07:58,659 --> 00:08:08,279
schedules first there's flexibility then

00:08:04,889 --> 00:08:10,899
because given the variety of

00:08:08,279 --> 00:08:16,089
implementations of OpenMP just having

00:08:10,899 --> 00:08:19,869
one standard way of defining a user

00:08:16,089 --> 00:08:21,639
level strategy provides a lot of

00:08:19,869 --> 00:08:24,930
flexibility to implement these

00:08:21,639 --> 00:08:27,969
strategies easily and effectively

00:08:24,930 --> 00:08:32,349
there's also emergence of threaded

00:08:27,969 --> 00:08:37,199
runtime systems such as our gobots

00:08:32,349 --> 00:08:40,779
and click threads and that that really

00:08:37,199 --> 00:08:45,579
argues in favor of stable specification

00:08:40,779 --> 00:08:47,980
of scheduling strategies especially due

00:08:45,579 --> 00:08:49,839
to the large number of new scheduling

00:08:47,980 --> 00:08:54,069
strategies that these runtime systems

00:08:49,839 --> 00:08:58,060
was developed one note is that the

00:08:54,069 --> 00:09:03,399
keywords in OpenMP of auto and runtime

00:08:58,060 --> 00:09:08,189
aren't adequate because they don't allow

00:09:03,399 --> 00:09:08,189
for this user level scheduling scheme

00:09:11,440 --> 00:09:22,440
so considering the factors and libraries

00:09:15,269 --> 00:09:27,399
I I just want to give an overview of the

00:09:22,440 --> 00:09:30,459
what we need to do we need to specify

00:09:27,399 --> 00:09:33,250
one user-defined scheduling scheme

00:09:30,459 --> 00:09:36,040
within the open MP standard that

00:09:33,250 --> 00:09:43,829
accommodates an arbitrary user-defined

00:09:36,040 --> 00:09:47,889
schedule and we need to we need to

00:09:43,829 --> 00:09:50,279
consider certain elements that are

00:09:47,889 --> 00:09:52,660
required to define a scheduler

00:09:50,279 --> 00:09:55,259
specifically there's scheduling

00:09:52,660 --> 00:09:59,560
scheduler specific data structures

00:09:55,259 --> 00:10:03,310
history records to hold history from

00:09:59,560 --> 00:10:08,220
prior invitations of loop and a

00:10:03,310 --> 00:10:08,220
specification of the behavior of threads

00:10:12,360 --> 00:10:21,070
so let me go into each of those in more

00:10:17,020 --> 00:10:24,520
detail the potential scheduling data

00:10:21,070 --> 00:10:25,270
structures include shared data

00:10:24,520 --> 00:10:29,650
structures

00:10:25,270 --> 00:10:33,000
low overhead steal cues exclusive cues

00:10:29,650 --> 00:10:37,540
met for each thread and shared queues

00:10:33,000 --> 00:10:41,170
for four from which each of the threads

00:10:37,540 --> 00:10:45,160
in team 10 DQ were each representing one

00:10:41,170 --> 00:10:50,670
chunk of loop in one chunk of loop

00:10:45,160 --> 00:10:54,940
durations the next point is about

00:10:50,670 --> 00:10:57,690
allowing threads to learn learn from

00:10:54,940 --> 00:11:04,540
previous outer iterations and that is

00:10:57,690 --> 00:11:08,410
the history record and that that needs

00:11:04,540 --> 00:11:12,070
to be supported in the user-defined

00:11:08,410 --> 00:11:16,240
schedule as well using a history

00:11:12,070 --> 00:11:19,930
tracking object examples of history

00:11:16,240 --> 00:11:23,770
information include previous values of

00:11:19,930 --> 00:11:27,630
the dynamic fraction as in this as shown

00:11:23,770 --> 00:11:30,510
in the strategy just our earlier

00:11:27,630 --> 00:11:34,240
iteration two core affinities and

00:11:30,510 --> 00:11:37,420
runtime performance profiles such as the

00:11:34,240 --> 00:11:42,930
MPI communication times from previous

00:11:37,420 --> 00:11:42,930
loop invitations also known as the slack

00:11:45,580 --> 00:11:51,530
finally there's the specification of the

00:11:48,890 --> 00:11:54,350
behavior threads and the scheduling

00:11:51,530 --> 00:11:57,190
behaviors of the main thread and other

00:11:54,350 --> 00:12:01,550
threads each need to be specified

00:11:57,190 --> 00:12:04,370
note that this behave the behaviors can

00:12:01,550 --> 00:12:09,220
be specified either via function

00:12:04,370 --> 00:12:12,890
pointers or declar to leaf such

00:12:09,220 --> 00:12:17,360
specification needs to be done while

00:12:12,890 --> 00:12:20,870
preserving generality that is that they

00:12:17,360 --> 00:12:24,920
have to the strategies have to deal with

00:12:20,870 --> 00:12:28,930
two loop iteration space in a controlled

00:12:24,920 --> 00:12:28,930
and flexible manner

00:12:37,270 --> 00:12:44,170
so here's an example of a somewhat

00:12:41,110 --> 00:12:46,899
sophisticated scheduling strategy that

00:12:44,170 --> 00:12:50,500
we demonstrated in fire work this

00:12:46,899 --> 00:12:53,649
strategy extends this static dynamic

00:12:50,500 --> 00:12:56,560
scheduling strategy that I showed

00:12:53,649 --> 00:12:59,260
earlier by influencing which threads get

00:12:56,560 --> 00:13:04,180
to execute which dynamic iterations by

00:12:59,260 --> 00:13:07,959
default these thread executes dynamic

00:13:04,180 --> 00:13:13,149
iterations by using a steel cue and to

00:13:07,959 --> 00:13:18,610
illustrate the the iteration space here

00:13:13,149 --> 00:13:21,430
is shown in this diagram and this is

00:13:18,610 --> 00:13:29,190
either and this is for four threads

00:13:21,430 --> 00:13:36,120
running one loop and say the here you

00:13:29,190 --> 00:13:42,520
see T two that's four thread two and

00:13:36,120 --> 00:13:46,690
suppose thread two is assigned from a

00:13:42,520 --> 00:13:51,100
loop let's say of 4,000 iterations the

00:13:46,690 --> 00:13:55,050
iterations 2000 to 3000 if the dynamic

00:13:51,100 --> 00:14:01,920
iteration if the dynamic fraction is 10%

00:13:55,050 --> 00:14:01,920
then that thread to execute iterations

00:14:02,220 --> 00:14:12,910
2002-2004 2900 statically and iterations

00:14:08,220 --> 00:14:17,160
2900 to 3000 dynamically

00:14:12,910 --> 00:14:20,380
since each thread DQ's iterations and

00:14:17,160 --> 00:14:29,100
spatially can take in a spatially

00:14:20,380 --> 00:14:33,550
continuous manner as seen here the

00:14:29,100 --> 00:14:39,130
approach that of this staggering is

00:14:33,550 --> 00:14:42,790
we're calling it is is the geared

00:14:39,130 --> 00:14:47,410
towards allowing for a reduced number of

00:14:42,790 --> 00:14:50,920
cache misses specifically and hansung

00:14:47,410 --> 00:14:53,470
spatial locality and improving the

00:14:50,920 --> 00:14:56,160
performance of the prefecture

00:14:53,470 --> 00:14:56,160
for example

00:15:03,040 --> 00:15:08,350
so the question is that without support

00:15:05,920 --> 00:15:12,040
for a user-defined schedule an open MT

00:15:08,350 --> 00:15:13,300
how should one implement the strategy in

00:15:12,040 --> 00:15:15,520
earlier work

00:15:13,300 --> 00:15:18,370
I developed a scheduling library that

00:15:15,520 --> 00:15:22,150
supports the staggering static staggered

00:15:18,370 --> 00:15:27,490
scheduling strategy using a macro based

00:15:22,150 --> 00:15:32,440
approach here the loop body as you can

00:15:27,490 --> 00:15:38,710
see is in enclosed by these two macros

00:15:32,440 --> 00:15:44,050
for all B gen and for all and these two

00:15:38,710 --> 00:15:47,160
macros take in deep as parameters the

00:15:44,050 --> 00:15:51,180
scheduling strategy name followed by

00:15:47,160 --> 00:15:51,180
parameters with a scheduling strategy

00:15:51,960 --> 00:16:00,520
here along with pointers to start and

00:15:57,280 --> 00:16:02,680
ended index and parameters for the

00:16:00,520 --> 00:16:10,600
scheduling strategy here the dynamic

00:16:02,680 --> 00:16:13,360
fraction the these both library both of

00:16:10,600 --> 00:16:15,220
these macros invoke library functions

00:16:13,360 --> 00:16:23,820
that correspond to the strategy's name

00:16:15,220 --> 00:16:27,370
as specified above in here with the SES

00:16:23,820 --> 00:16:33,580
so what you see here is the invitation

00:16:27,370 --> 00:16:37,300
of my of the library functions and it is

00:16:33,580 --> 00:16:41,380
actually table started invoking a loop

00:16:37,300 --> 00:16:48,060
start function initializing initializing

00:16:41,380 --> 00:16:48,060
the this loop of

00:16:48,570 --> 00:16:57,080
and then do it and then static dub

00:16:51,620 --> 00:17:04,589
putting a ado loop here for a continued

00:16:57,080 --> 00:17:08,930
of execution of that loop and then for

00:17:04,589 --> 00:17:17,240
the for all end you see a closing brace

00:17:08,930 --> 00:17:17,240
with the while loop there and barrier so

00:17:17,810 --> 00:17:27,480
essentially this these two functions of

00:17:24,650 --> 00:17:32,040
these two functions loop start in the

00:17:27,480 --> 00:17:34,380
next are necessary for the loop

00:17:32,040 --> 00:17:38,040
execution this for all began in floral

00:17:34,380 --> 00:17:41,280
end makes it easier for the application

00:17:38,040 --> 00:17:45,510
programmer to write their code to write

00:17:41,280 --> 00:17:48,540
their code so I just want to point out

00:17:45,510 --> 00:17:54,300
that these two macros though at the top

00:17:48,540 --> 00:17:57,320
of this program so you're writing your

00:17:54,300 --> 00:17:59,280
application program these two macros

00:17:57,320 --> 00:18:01,200
definitions they're essentially Kahn

00:17:59,280 --> 00:18:05,030
defines go at the top of this

00:18:01,200 --> 00:18:05,030
application code

00:18:06,279 --> 00:18:16,059
the limitations of this approach are are

00:18:13,419 --> 00:18:20,739
many even though it's useful to

00:18:16,059 --> 00:18:24,700
demonstrate each of the scheduling

00:18:20,739 --> 00:18:28,899
strategies using it the library is

00:18:24,700 --> 00:18:33,309
unable to use compiler support such as

00:18:28,899 --> 00:18:35,169
reductions and so really in order to

00:18:33,309 --> 00:18:37,210
support the user-defined schedule and

00:18:35,169 --> 00:18:41,049
OpenMP we need to propose an approach

00:18:37,210 --> 00:18:43,989
that eliminates these limitations and

00:18:41,049 --> 00:18:49,059
leads to concise code for an application

00:18:43,989 --> 00:18:51,849
program so here's a proposal for a

00:18:49,059 --> 00:18:54,279
user-defined schedule building up on our

00:18:51,849 --> 00:19:02,019
macro based approach that we've

00:18:54,279 --> 00:19:05,529
implemented already and the this is the

00:19:02,019 --> 00:19:11,529
sketch for that puzzle based on the code

00:19:05,529 --> 00:19:15,820
I just showed the here you see we start

00:19:11,529 --> 00:19:24,849
with a this dynamic fraction chunk size

00:19:15,820 --> 00:19:29,859
with time record here's well so so the

00:19:24,849 --> 00:19:33,419
key line is shown in the red with the

00:19:29,859 --> 00:19:37,899
pragma out parallel for schedule and we

00:19:33,419 --> 00:19:40,119
added a new schedule time user in the

00:19:37,899 --> 00:19:42,419
first paragraph of the schedule clause

00:19:40,119 --> 00:19:47,320
the second one the second parameter

00:19:42,419 --> 00:19:52,839
specifies the scheduling strategy which

00:19:47,320 --> 00:19:56,810
here we put as staggered in the previous

00:19:52,839 --> 00:20:01,460
iced abbreviated for us as SDS

00:19:56,810 --> 00:20:04,880
so they're the same just they're the

00:20:01,460 --> 00:20:08,350
same strategy the trunk size is

00:20:04,880 --> 00:20:11,780
specified here as another parameter

00:20:08,350 --> 00:20:13,910
dynamic faction that another yet another

00:20:11,780 --> 00:20:14,540
one and then the loop time record is yet

00:20:13,910 --> 00:20:20,780
another one

00:20:14,540 --> 00:20:23,980
each of these are optional and I'll just

00:20:20,780 --> 00:20:30,200
point out that this can be used for

00:20:23,980 --> 00:20:34,370
history such as the NPI plaque or any

00:20:30,200 --> 00:20:40,130
previous Lou tiny history that I pointed

00:20:34,370 --> 00:20:42,530
out earlier in the presentation so given

00:20:40,130 --> 00:20:44,930
that interface how should the user

00:20:42,530 --> 00:20:51,260
define scheduler be implemented by a

00:20:44,930 --> 00:20:56,480
user when a user specifies the schedule

00:20:51,260 --> 00:21:00,380
time user and a strategy named X they

00:20:56,480 --> 00:21:06,370
need to link a library that defines the

00:21:00,380 --> 00:21:14,540
functions X in it X loop start and X

00:21:06,370 --> 00:21:19,190
root next for the X here is the density

00:21:14,540 --> 00:21:25,150
strategy name like the stagger that I

00:21:19,190 --> 00:21:31,390
showed just in the previous slide so the

00:21:25,150 --> 00:21:34,940
X init function should allow a user to

00:21:31,390 --> 00:21:38,450
allocate and initialize data structures

00:21:34,940 --> 00:21:40,510
that are to be used commonly all across

00:21:38,450 --> 00:21:47,860
all parallel loops in an application

00:21:40,510 --> 00:21:54,320
that use the strategy X for functions X

00:21:47,860 --> 00:21:58,910
loop start and X loop next determine a

00:21:54,320 --> 00:22:01,550
loops indices that a thread should work

00:21:58,910 --> 00:22:07,960
on based on the parameter values for the

00:22:01,550 --> 00:22:15,620
scheduling strategy end of the loop the

00:22:07,960 --> 00:22:18,620
main the main assertion here is that a

00:22:15,620 --> 00:22:22,910
lot as long as one is allowed to define

00:22:18,620 --> 00:22:26,680
these functions one can implement a

00:22:22,910 --> 00:22:26,680
user-defined scheduler

00:22:27,920 --> 00:22:35,760
so here I just show how a user's a

00:22:33,290 --> 00:22:38,640
sketch of how that user defined

00:22:35,760 --> 00:22:44,460
scheduled or would be implemented by

00:22:38,640 --> 00:22:47,690
user and know that every thread here

00:22:44,460 --> 00:22:53,460
should call excellent next repeatedly

00:22:47,690 --> 00:22:56,490
until their there are no lock until

00:22:53,460 --> 00:23:00,440
there's no more chunks of work that

00:22:56,490 --> 00:23:00,440
remains to be scheduled to attack

00:23:05,680 --> 00:23:13,820
so here is how many different scheduling

00:23:11,330 --> 00:23:17,800
strategies can be expressed using this

00:23:13,820 --> 00:23:21,050
extension so this here is an

00:23:17,800 --> 00:23:25,130
implementation of the user defined of

00:23:21,050 --> 00:23:29,120
here's an implementation of the static

00:23:25,130 --> 00:23:33,250
dynamic scheduling scheme using this

00:23:29,120 --> 00:23:39,190
user-defined scheduling methodology and

00:23:33,250 --> 00:23:47,870
the top function is the loop start and

00:23:39,190 --> 00:23:51,890
it's it's that shows it sets up the data

00:23:47,870 --> 00:23:56,720
structures along with the locks first

00:23:51,890 --> 00:23:59,330
and then all threads use this shared

00:23:56,720 --> 00:24:01,580
loop crams data structure that was set

00:23:59,330 --> 00:24:06,100
up to calculate static durations and

00:24:01,580 --> 00:24:15,130
execute the loop body for that range

00:24:06,100 --> 00:24:18,230
then the SD loop next lock loop Rams

00:24:15,130 --> 00:24:21,380
extracts chunk to work on and unlocks

00:24:18,230 --> 00:24:24,190
loop trim this is the thread and if no

00:24:21,380 --> 00:24:28,610
choice is available we wait for barrier

00:24:24,190 --> 00:24:31,250
done equal times 10 equals 1 and we just

00:24:28,610 --> 00:24:33,930
continue executing in the loop top body

00:24:31,250 --> 00:24:38,580
for each extract to jump

00:24:33,930 --> 00:24:42,110
so that that's the simple implementation

00:24:38,580 --> 00:24:46,050
and here's an alternative implementation

00:24:42,110 --> 00:24:48,390
here we do this same static dynamic

00:24:46,050 --> 00:24:52,880
scheduling strategy using skill to use

00:24:48,390 --> 00:25:00,060
for the dynamic a trick so here we start

00:24:52,880 --> 00:25:06,510
in the we define the loop start and we

00:25:00,060 --> 00:25:10,530
set up the loop params and here we note

00:25:06,510 --> 00:25:12,390
that we onto one entry but that is a

00:25:10,530 --> 00:25:14,160
master thread on to use one entry

00:25:12,390 --> 00:25:19,050
corresponding to the dynamic range of

00:25:14,160 --> 00:25:21,900
iteration into it steals you oh and then

00:25:19,050 --> 00:25:24,840
we as before use the shared data

00:25:21,900 --> 00:25:27,030
structure to calculate the static

00:25:24,840 --> 00:25:30,810
durations and execute loop the loop body

00:25:27,030 --> 00:25:37,460
for it's right here's the loop next here

00:25:30,810 --> 00:25:39,960
we're doing this work stealing beginning

00:25:37,460 --> 00:25:47,930
using this classic silk style

00:25:39,960 --> 00:25:55,410
work-stealing algorithm and then we we

00:25:47,930 --> 00:26:00,540
text for range of the size of the of the

00:25:55,410 --> 00:26:04,650
chunk that was issued and we are we

00:26:00,540 --> 00:26:07,440
split the range into if that threshold

00:26:04,650 --> 00:26:10,230
is greater if the range is greater than

00:26:07,440 --> 00:26:13,440
threshold and on Q into the steel cube

00:26:10,230 --> 00:26:18,560
otherwise we execute the loop body for

00:26:13,440 --> 00:26:22,140
the iterations in the range and just

00:26:18,560 --> 00:26:24,020
update the count until just at the dull

00:26:22,140 --> 00:26:34,970
side whenever done

00:26:24,020 --> 00:26:37,940
so so essentially here we've shown a

00:26:34,970 --> 00:26:44,150
steel Q in the implementation for the

00:26:37,940 --> 00:26:50,270
steel Q of a steel Q's support for a

00:26:44,150 --> 00:26:52,940
user to find a schedule and then we can

00:26:50,270 --> 00:26:56,360
implement this staggered scheduling

00:26:52,940 --> 00:27:01,910
strategy that we described using this

00:26:56,360 --> 00:27:05,000
alternative approach also first we set

00:27:01,910 --> 00:27:10,480
up in this loop start the data

00:27:05,000 --> 00:27:15,350
structures this is for and then we

00:27:10,480 --> 00:27:18,470
calculate we are calculating the static

00:27:15,350 --> 00:27:24,040
iterations and executing loop body for

00:27:18,470 --> 00:27:30,679
that range and here we are executing

00:27:24,040 --> 00:27:36,559
this loop body that were X 4 d 4 d

00:27:30,679 --> 00:27:38,140
queueing chunks from the steel q and for

00:27:36,559 --> 00:27:42,559
the range

00:27:38,140 --> 00:27:46,400
l & u we execute the loop body for the

00:27:42,559 --> 00:27:50,030
iterations Loran upper and then update

00:27:46,400 --> 00:27:53,040
the count of iterations to set to done a

00:27:50,030 --> 00:27:59,460
set the done flag one does

00:27:53,040 --> 00:28:03,030
and so here as we show this follows the

00:27:59,460 --> 00:28:04,380
per threads steel queue ideas that we

00:28:03,030 --> 00:28:13,170
discussed for the staggered loop

00:28:04,380 --> 00:28:17,190
scheduling and we are we are also trying

00:28:13,170 --> 00:28:21,950
to maintain this literate spatial

00:28:17,190 --> 00:28:27,150
locality in this loop iteration of range

00:28:21,950 --> 00:28:30,750
by redistributing how the chunks of hoop

00:28:27,150 --> 00:28:36,980
iteration is dynamic the dynamically

00:28:30,750 --> 00:28:36,980
scheduled chunks are scheduled to loops

00:28:42,990 --> 00:28:52,399
so we've catched above a syntax for the

00:28:47,279 --> 00:28:52,399
API for the user-defined schedulers and

00:28:54,260 --> 00:29:06,049
these this sketch is to be hopefully

00:29:02,490 --> 00:29:13,490
discussed by the community consensus

00:29:06,049 --> 00:29:17,940
this is in no means a determined final

00:29:13,490 --> 00:29:21,240
version and I just like to note that

00:29:17,940 --> 00:29:27,649
there's actually a precedent for these

00:29:21,240 --> 00:29:32,190
users for the user-defined schedule and

00:29:27,649 --> 00:29:36,299
that is the combiner function in the

00:29:32,190 --> 00:29:39,360
user to find reductions and the zone

00:29:36,299 --> 00:29:42,090
this way I feel that the user-defined

00:29:39,360 --> 00:29:45,440
schedule shouldn't be too foreign to the

00:29:42,090 --> 00:29:45,440
OpenMP standard

00:29:47,300 --> 00:30:00,200
so to summarize we need to have that we

00:29:54,410 --> 00:30:02,090
need to have and be able to experiment

00:30:00,200 --> 00:30:04,790
with sophisticated loop scheduling

00:30:02,090 --> 00:30:07,280
strategies as soon through higher work

00:30:04,790 --> 00:30:14,380
given these emerging supercomputer

00:30:07,280 --> 00:30:18,050
architectures and applications and we

00:30:14,380 --> 00:30:20,450
should as a community discuss how to

00:30:18,050 --> 00:30:23,390
allow flexible specifications of

00:30:20,450 --> 00:30:25,670
strategies in such a users code and how

00:30:23,390 --> 00:30:30,560
to design a user level scheduling

00:30:25,670 --> 00:30:33,980
strategy so that one can use these

00:30:30,560 --> 00:30:41,720
strategies portably with a conforming

00:30:33,980 --> 00:30:44,900
OpenMP implementation and believe and I

00:30:41,720 --> 00:30:47,750
believe that supporting user defined

00:30:44,900 --> 00:30:50,450
schedules in schedulers in this way will

00:30:47,750 --> 00:30:53,990
facilitate for rapid development of

00:30:50,450 --> 00:30:57,290
scheduling strategies and finally I my

00:30:53,990 --> 00:30:59,470
hope is that experts will discuss these

00:30:57,290 --> 00:31:02,180
ideas at this conference

00:30:59,470 --> 00:31:04,160
thanks for your attention and I'd like

00:31:02,180 --> 00:31:05,990
to acknowledge the University of

00:31:04,160 --> 00:31:09,970
Southern California's lush iodized

00:31:05,990 --> 00:31:09,970
technical New Directions program

00:31:22,440 --> 00:31:31,950
the purpose of this body maybe a few are

00:31:27,450 --> 00:31:31,950
improve our robot

00:31:32,540 --> 00:31:43,050
yes you appear using interesting

00:31:39,150 --> 00:31:47,960
disability religion or sharing this

00:31:43,050 --> 00:31:54,270
strategy Ethernet and tassel tassels and

00:31:47,960 --> 00:31:58,950
these towel into comparable have looked

00:31:54,270 --> 00:32:08,570
at that for simplicity sake I showed

00:31:58,950 --> 00:32:12,840
just this how I also I think that this

00:32:08,570 --> 00:32:16,640
way I've shown it performs better in the

00:32:12,840 --> 00:32:23,730
results that I have so that's why I

00:32:16,640 --> 00:32:29,640
focused on this but this loop I have yes

00:32:23,730 --> 00:32:30,320
I have yes thank you for bringing that

00:32:29,640 --> 00:32:34,620
up

00:32:30,320 --> 00:32:38,040
so I just want to point you is this

00:32:34,620 --> 00:32:43,710
approach just a small thing the approach

00:32:38,040 --> 00:32:46,110
is to manage both load balance and data

00:32:43,710 --> 00:32:49,380
locality so you want to maximize the

00:32:46,110 --> 00:32:56,010
load balance and maximize the data will

00:32:49,380 --> 00:32:58,470
help in this approach so unlike just

00:32:56,010 --> 00:33:01,510
using a guided schedule this approach

00:32:58,470 --> 00:33:05,380
tries to maintain

00:33:01,510 --> 00:33:07,470
a low amount of data movement across

00:33:05,380 --> 00:33:11,350
scores and that's really important for

00:33:07,470 --> 00:33:13,860
next-generation supercomputer nodes with

00:33:11,350 --> 00:33:13,860
many cores

00:33:21,120 --> 00:33:27,700
haha

00:33:23,690 --> 00:33:27,700
and don't camera

00:33:29,190 --> 00:33:34,490
no that can be used to come to mind

00:33:36,300 --> 00:33:42,600
yes yeah how what will be the challenge

00:33:40,810 --> 00:33:45,070
because I've seen a real gentleman

00:33:42,600 --> 00:33:48,780
implementation yes you are opposing

00:33:45,070 --> 00:33:53,020
gentle dark light and if this

00:33:48,780 --> 00:33:54,400
exfoliating candle in terms of like

00:33:53,020 --> 00:33:57,430
using are you fixing to end up very

00:33:54,400 --> 00:34:02,560
complex a habit stick which have no

00:33:57,430 --> 00:34:07,450
overhead then that's why and I

00:34:02,560 --> 00:34:11,010
understand your disposal over in Europe

00:34:07,450 --> 00:34:14,370
your cloth that can use the first gather

00:34:11,010 --> 00:34:15,580
our practice artists who are

00:34:14,370 --> 00:34:20,490
implementable

00:34:15,580 --> 00:34:25,169
if in a run time with a reasonable for

00:34:20,490 --> 00:34:28,919
hunting so and recognizing basically

00:34:25,169 --> 00:34:32,580
utilizing the hardware efficiently that

00:34:28,919 --> 00:34:35,780
appears and that can only be done

00:34:32,580 --> 00:34:35,780
if you have a real good

00:34:36,830 --> 00:34:46,679
yes yes so this is partly why I want to

00:34:41,990 --> 00:34:50,100
discuss this with experts that because I

00:34:46,679 --> 00:34:52,379
don't know and the proposal is a

00:34:50,100 --> 00:34:56,100
proposal and it

00:34:52,379 --> 00:35:03,420
I haven't implemented these extensions

00:34:56,100 --> 00:35:08,400
yet I hope I can say that the library

00:35:03,420 --> 00:35:13,860
that this proposal is based on has low

00:35:08,400 --> 00:35:20,040
overhead that is it there the hooks that

00:35:13,860 --> 00:35:26,100
are you mentioning don't is in rope or

00:35:20,040 --> 00:35:28,610
induced lots of extra extra time during

00:35:26,100 --> 00:35:35,190
application execution so I can say that

00:35:28,610 --> 00:35:38,430
that works well so I I don't see why

00:35:35,190 --> 00:35:40,770
that the I don't see why there should be

00:35:38,430 --> 00:35:42,980
that much overhead you know plenty right

00:35:40,770 --> 00:35:42,980
time

00:35:45,560 --> 00:35:48,190
yep

00:35:48,579 --> 00:35:53,070
they're putting in

00:35:51,240 --> 00:35:56,169
of the problems we

00:35:53,070 --> 00:35:56,169
[Music]

00:35:59,930 --> 00:36:04,330
factor we want to have a dinner

00:36:04,529 --> 00:36:08,089
for work like that

00:36:08,160 --> 00:36:13,090
assuming your scheduling when I sign

00:36:11,990 --> 00:36:15,340
[Music]

00:36:13,090 --> 00:36:19,140
yeah charge

00:36:15,340 --> 00:36:23,310
but it's really likely

00:36:19,140 --> 00:36:26,850
we'll be single sensor mobile-ready she

00:36:23,310 --> 00:36:30,150
cannot work very world size right now

00:36:26,850 --> 00:36:32,970
music emitter and then around

00:36:30,150 --> 00:36:34,760
being able to do this scheduling

00:36:32,970 --> 00:36:38,310
[Music]

00:36:34,760 --> 00:36:40,910
native of talent

00:36:38,310 --> 00:36:40,910
yeah

00:36:41,780 --> 00:36:45,410
and it did not work

00:36:45,650 --> 00:36:56,849
[Music]

00:37:01,610 --> 00:37:06,880
[Music]

00:37:07,110 --> 00:37:16,840
thank you I think I've and when I've

00:37:13,380 --> 00:37:21,000
worked with this loop scheduling

00:37:16,840 --> 00:37:27,990
research I've thought of multi-core

00:37:21,000 --> 00:37:31,000
nodes as similar to GPU no GPU being

00:37:27,990 --> 00:37:34,570
possible and just very big multi-core

00:37:31,000 --> 00:37:39,460
node but it really mean it really needs

00:37:34,570 --> 00:37:47,140
to be considered of as its own thing so

00:37:39,460 --> 00:37:50,710
I definitely figure and I I have I have

00:37:47,140 --> 00:37:54,160
some results not here but just some

00:37:50,710 --> 00:38:00,520
compares and results with OpenMP 4.5

00:37:54,160 --> 00:38:06,010
device contracts compared to open mps

00:38:00,520 --> 00:38:10,420
loops scheduling constructs for for

00:38:06,010 --> 00:38:14,830
multi-core so my hope is to go in that

00:38:10,420 --> 00:38:17,130
direction but definitely I'll make a

00:38:14,830 --> 00:38:17,130
point about

00:38:17,830 --> 00:38:25,330
yes approach

00:38:40,569 --> 00:38:47,440
coming

00:38:42,930 --> 00:38:55,660
yes John exactly exactly

00:38:47,440 --> 00:39:00,010
this is the idea of the user being

00:38:55,660 --> 00:39:05,369
exposed to the scheduling strategy but

00:39:00,010 --> 00:39:07,359
making it have an automated approach or

00:39:05,369 --> 00:39:10,500
semi-automated approach where the user

00:39:07,359 --> 00:39:14,190
is exposed and there's some other tuning

00:39:10,500 --> 00:39:19,540
and all that's that's the way I see it

00:39:14,190 --> 00:39:25,319
so this approach should enable that auto

00:39:19,540 --> 00:39:29,609
tuning methodology I just I haven't used

00:39:25,319 --> 00:39:32,609
auto tuning offering works here but yes

00:39:29,609 --> 00:39:32,609
yeah

00:39:34,020 --> 00:39:37,170

YouTube URL: https://www.youtube.com/watch?v=YgVhDXkun0U


