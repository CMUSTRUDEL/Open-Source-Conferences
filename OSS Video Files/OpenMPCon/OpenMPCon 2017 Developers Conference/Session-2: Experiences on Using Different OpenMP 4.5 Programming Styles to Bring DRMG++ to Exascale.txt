Title: Session-2: Experiences on Using Different OpenMP 4.5 Programming Styles to Bring DRMG++ to Exascale
Publication date: 2017-10-15
Playlist: OpenMPCon 2017 Developers Conference
Description: 
	Arghya Chatterjee and Oscar Hernandez
Slides at http://openmpcon.org/wp-content/uploads/openmpcon2017/Day1-Session1-Chatterjee.pdf
Captions: 
	00:00:01,090 --> 00:00:06,200
all right so I'm actually going to be

00:00:04,490 --> 00:00:08,030
talking about more on the application

00:00:06,200 --> 00:00:12,110
side so Katherine kind of gave us all

00:00:08,030 --> 00:00:15,530
the hardware and the new open MP 4.5

00:00:12,110 --> 00:00:16,940
construct that let's see showed so I'm

00:00:15,530 --> 00:00:21,080
going to give you an application the

00:00:16,940 --> 00:00:24,800
side view of what problems we had or

00:00:21,080 --> 00:00:29,090
what successes we had using some of the

00:00:24,800 --> 00:00:31,340
open MP 4.5 constructs so as the Chevy

00:00:29,090 --> 00:00:33,170
mentioned it there's not a typo Vivec

00:00:31,340 --> 00:00:35,060
actually moves a lot of you might know

00:00:33,170 --> 00:00:37,550
vivec and who has it right he recently

00:00:35,060 --> 00:00:41,930
moved to Georgia Tech so it's not a typo

00:00:37,550 --> 00:00:43,640
there it's a so we moved basically ok so

00:00:41,930 --> 00:00:46,520
the application that we're talking about

00:00:43,640 --> 00:00:48,739
today is DM r g+ plus it's a physics

00:00:46,520 --> 00:00:51,800
application written based solely at

00:00:48,739 --> 00:00:53,329
Oakridge over national lab it's a I'm

00:00:51,800 --> 00:00:54,980
going to talk about the application

00:00:53,329 --> 00:00:58,370
briefly not getting into the physics

00:00:54,980 --> 00:01:01,540
details but this talk mostly what we're

00:00:58,370 --> 00:01:04,789
trying to get from the audience here is

00:01:01,540 --> 00:01:06,170
we have done stuff without using openmp

00:01:04,789 --> 00:01:09,259
but we want to know what we could have

00:01:06,170 --> 00:01:12,770
done better using open MP and i'm going

00:01:09,259 --> 00:01:13,909
to show the challenges we faced so if

00:01:12,770 --> 00:01:18,009
you think that you have a better

00:01:13,909 --> 00:01:21,319
solution we are all years basically SAP

00:01:18,009 --> 00:01:24,529
so as we all know that the data centers

00:01:21,319 --> 00:01:27,560
and the overs the oil CF machines were

00:01:24,529 --> 00:01:30,109
all the next P exascale machines the

00:01:27,560 --> 00:01:32,509
architectures becoming more complex it's

00:01:30,109 --> 00:01:35,749
hybrid in nature so you have a lot of

00:01:32,509 --> 00:01:38,439
heterogeneous cores and architectures in

00:01:35,749 --> 00:01:41,329
each of the each of these machines and

00:01:38,439 --> 00:01:44,270
what we feel like the applications at

00:01:41,329 --> 00:01:47,539
these labs need to kinda adapt to the

00:01:44,270 --> 00:01:50,830
hardware itself to get performance out

00:01:47,539 --> 00:01:53,290
of even using openmp or MPI as well

00:01:50,830 --> 00:01:55,840
so what we're doing is we are it's a

00:01:53,290 --> 00:01:58,660
standard approach that a lot of labs is

00:01:55,840 --> 00:02:00,640
design many applications out of the

00:01:58,660 --> 00:02:04,180
original application take out extract

00:02:00,640 --> 00:02:08,230
the most time-consuming kernels and then

00:02:04,180 --> 00:02:10,240
use that as so it basically serves as a

00:02:08,230 --> 00:02:12,880
foundation for exascale ready

00:02:10,240 --> 00:02:14,800
implementation and we're also Co

00:02:12,880 --> 00:02:16,330
designing this application with other

00:02:14,800 --> 00:02:18,880
programming a synchronous programming

00:02:16,330 --> 00:02:21,820
models like hoverin or C++

00:02:18,880 --> 00:02:23,650
Koko's and magma although I'm not going

00:02:21,820 --> 00:02:25,570
to be presenting any anything with

00:02:23,650 --> 00:02:27,820
Koko's and magma will show briefly about

00:02:25,570 --> 00:02:31,150
what we did with have another C++ as

00:02:27,820 --> 00:02:32,910
well so kind of the outline of the talk

00:02:31,150 --> 00:02:36,000
I'm going to talk about the application

00:02:32,910 --> 00:02:38,590
briefly with followed by what the

00:02:36,000 --> 00:02:40,480
motivation is which is kind of twofold

00:02:38,590 --> 00:02:42,850
there's a physics motivation and there's

00:02:40,480 --> 00:02:45,850
also a programming languages models

00:02:42,850 --> 00:02:48,040
motivation for us and then we talk about

00:02:45,850 --> 00:02:50,530
the mini application the kind of design

00:02:48,040 --> 00:02:53,500
implementation and I'll talk about three

00:02:50,530 --> 00:02:55,780
different program files in OpenMP that

00:02:53,500 --> 00:02:57,540
we used and he challenges we face and

00:02:55,780 --> 00:03:00,160
why we moved on to the next time and

00:02:57,540 --> 00:03:03,070
followed by some of the programming

00:03:00,160 --> 00:03:06,430
support that we currently have and some

00:03:03,070 --> 00:03:08,260
will end with some future open MD

00:03:06,430 --> 00:03:10,959
constructs that might be added which I'm

00:03:08,260 --> 00:03:12,700
not sure whether because we know that

00:03:10,959 --> 00:03:14,110
it's already open and B is pretty large

00:03:12,700 --> 00:03:17,350
that's where everyone's saying but let's

00:03:14,110 --> 00:03:20,709
see if we can convince to change a few

00:03:17,350 --> 00:03:22,540
things okay so the DMR C++ is a physics

00:03:20,709 --> 00:03:25,870
application what it does it's a density

00:03:22,540 --> 00:03:27,970
matrix renormalization group basically

00:03:25,870 --> 00:03:32,500
what we need to know is it helps you

00:03:27,970 --> 00:03:36,520
understand nanoparticles all nano quick

00:03:32,500 --> 00:03:37,870
I would say it's more like the heat how

00:03:36,520 --> 00:03:39,489
much heat is generated from these

00:03:37,870 --> 00:03:41,890
nanoparticles that's what you get from

00:03:39,489 --> 00:03:44,290
the application itself

00:03:41,890 --> 00:03:47,110
it's a sparse matrix algebra

00:03:44,290 --> 00:03:50,400
computational motif it's not a general

00:03:47,110 --> 00:03:54,040
sparse matrix so it's composed of

00:03:50,400 --> 00:03:56,500
smaller dense and sparse matrices inside

00:03:54,040 --> 00:03:58,420
each matrix cell which I'll be talking

00:03:56,500 --> 00:04:02,110
about later it will make more sense

00:03:58,420 --> 00:04:05,080
there and this is not an application

00:04:02,110 --> 00:04:07,390
that's done sitting there it's actually

00:04:05,080 --> 00:04:10,150
an ongoing application ongoing effort

00:04:07,390 --> 00:04:14,260
that physics folks are still working on

00:04:10,150 --> 00:04:16,590
at the lab so to start with what we

00:04:14,260 --> 00:04:19,450
ended up doing is we had a shared memory

00:04:16,590 --> 00:04:23,980
DM r g+ class when I said memory it was

00:04:19,450 --> 00:04:26,770
not even written in C or C++ it was

00:04:23,980 --> 00:04:30,669
actually a MATLAB and then there's some

00:04:26,770 --> 00:04:36,790
Fortran F 90 code as well what we did is

00:04:30,669 --> 00:04:40,300
we used OpenMP as a slight outer carnal

00:04:36,790 --> 00:04:43,840
outer program model to just call this

00:04:40,300 --> 00:04:46,210
carnal we use eight threads on on the

00:04:43,840 --> 00:04:47,620
titan node to call this kernel what what

00:04:46,210 --> 00:04:50,050
you see on the on your right is

00:04:47,620 --> 00:04:52,810
basically the application that runs in

00:04:50,050 --> 00:04:55,300
phases so the application what happens

00:04:52,810 --> 00:04:57,970
is it runs in five phases at least for

00:04:55,300 --> 00:04:59,860
this data set it can go up to like 10 15

00:04:57,970 --> 00:05:02,650
phases but what I'm showing you is five

00:04:59,860 --> 00:05:05,080
phases and each phase basically does a

00:05:02,650 --> 00:05:08,680
similar kind of computation so it's all

00:05:05,080 --> 00:05:10,540
the same computation what changes is the

00:05:08,680 --> 00:05:13,419
data size actually changes because the

00:05:10,540 --> 00:05:14,350
matrix basically changes as we go from

00:05:13,419 --> 00:05:17,350
ph1

00:05:14,350 --> 00:05:19,990
to ph5 and it's a convergence algorithm

00:05:17,350 --> 00:05:22,690
so it continues till it converges to the

00:05:19,990 --> 00:05:26,260
point where the physics folks need to

00:05:22,690 --> 00:05:28,210
convert and what we see here is this we

00:05:26,260 --> 00:05:30,430
probably stand in a standard deviation

00:05:28,210 --> 00:05:32,080
over index of instances what we're

00:05:30,430 --> 00:05:35,200
seeing here is as the data size

00:05:32,080 --> 00:05:37,720
increases your sparsity it's more

00:05:35,200 --> 00:05:39,610
dispersed much much it's more dispersed

00:05:37,720 --> 00:05:43,120
so what we see is there is more load

00:05:39,610 --> 00:05:45,280
imbalance once we increase the

00:05:43,120 --> 00:05:47,650
the matrix that we are trying to

00:05:45,280 --> 00:05:50,350
complete and since it's a dynamic

00:05:47,650 --> 00:05:52,240
application so over time your load

00:05:50,350 --> 00:05:54,490
imbalance kind of keeps increasing and

00:05:52,240 --> 00:05:58,870
so that was one of the reasons that we

00:05:54,490 --> 00:06:00,729
wanted to use open and B tasking to kind

00:05:58,870 --> 00:06:01,620
of attack this problem in the first

00:06:00,729 --> 00:06:04,570
place

00:06:01,620 --> 00:06:06,729
so as I mentioned the motivation is kind

00:06:04,570 --> 00:06:09,580
of twofold both physics and programming

00:06:06,729 --> 00:06:11,470
models for physics currently the DM r g+

00:06:09,580 --> 00:06:13,000
plus only works on one-dimensional

00:06:11,470 --> 00:06:17,320
problems that's what they're working on

00:06:13,000 --> 00:06:20,350
so scaling this up to three EXO scale

00:06:17,320 --> 00:06:21,699
models will help them actually solve a

00:06:20,350 --> 00:06:24,120
lot of two-dimensional and

00:06:21,699 --> 00:06:28,419
three-dimensional problems such as like

00:06:24,120 --> 00:06:30,610
semiconductors power grids and also work

00:06:28,419 --> 00:06:33,010
on first model first principle models

00:06:30,610 --> 00:06:35,169
without currently the approximate the

00:06:33,010 --> 00:06:36,340
electron-electron interaction which they

00:06:35,169 --> 00:06:38,470
will not be doing and they can actually

00:06:36,340 --> 00:06:40,000
put in those values because this takes

00:06:38,470 --> 00:06:43,449
up a lot of resource that's currently

00:06:40,000 --> 00:06:46,539
not available using the shared memory

00:06:43,449 --> 00:06:49,090
implementation that they have for the

00:06:46,539 --> 00:06:51,780
program models folks what we are more

00:06:49,090 --> 00:06:56,050
interested in is using OpenMP in a more

00:06:51,780 --> 00:06:59,530
I'll be talking Y nested parallelism we

00:06:56,050 --> 00:07:02,680
try to use open and B tasks as well so

00:06:59,530 --> 00:07:07,270
task loops and new constructs and we get

00:07:02,680 --> 00:07:08,800
support in 4.5 and so as I mentioned the

00:07:07,270 --> 00:07:11,440
code design projects we are also

00:07:08,800 --> 00:07:13,750
exploring has base program models which

00:07:11,440 --> 00:07:16,990
can also be a candidate for future open

00:07:13,750 --> 00:07:18,910
empty extensions and we want to look at

00:07:16,990 --> 00:07:21,130
what program model kind of fits in and

00:07:18,910 --> 00:07:23,780
gives us more productivity but at the

00:07:21,130 --> 00:07:25,640
same time not hampering the performance

00:07:23,780 --> 00:07:29,350
and we're also looking at a couple of

00:07:25,640 --> 00:07:31,880
compiler optimizations I'll be sub demo

00:07:29,350 --> 00:07:34,820
so let's look at the execution model

00:07:31,880 --> 00:07:37,010
briefly as I mentioned the most I was a

00:07:34,820 --> 00:07:38,570
80% time consuming part was actually

00:07:37,010 --> 00:07:41,030
calculating the sparse matrix

00:07:38,570 --> 00:07:42,500
Hamiltonian what is the Hamiltonian

00:07:41,030 --> 00:07:46,100
matrix I'm going to come in the next

00:07:42,500 --> 00:07:48,470
slide the 2-dimensional dense sparse

00:07:46,100 --> 00:07:50,720
matrix and we used three different

00:07:48,470 --> 00:07:52,940
programming styles that I will talk

00:07:50,720 --> 00:07:55,400
about nested parallelism we moved on to

00:07:52,940 --> 00:07:59,450
multi-level tasking and then multi-level

00:07:55,400 --> 00:08:02,030
tasking with nested parallelism so this

00:07:59,450 --> 00:08:05,090
is the basic kernel that we'll be

00:08:02,030 --> 00:08:07,610
looking at throughout the entire talk so

00:08:05,090 --> 00:08:10,100
it's y equals H times X where H is

00:08:07,610 --> 00:08:11,570
basically a Hamiltonian matrix this is

00:08:10,100 --> 00:08:13,400
an interesting matrix so that's why I

00:08:11,570 --> 00:08:16,610
kind of pointed out how the matrix looks

00:08:13,400 --> 00:08:18,850
like so it's we call it CIJ if the

00:08:16,610 --> 00:08:21,530
two-dimensional matrix looks pretty

00:08:18,850 --> 00:08:23,390
standard two-dimensional matrix the end

00:08:21,530 --> 00:08:25,930
patches actually comes from previous

00:08:23,390 --> 00:08:30,110
application that feeds in as an input

00:08:25,930 --> 00:08:33,979
now each cell in CIJ will actually have

00:08:30,110 --> 00:08:36,560
two matrices two vectors one dimensional

00:08:33,979 --> 00:08:39,080
vectors of size K where K is also

00:08:36,560 --> 00:08:44,720
predetermine it's usually for for our

00:08:39,080 --> 00:08:47,450
case but so what we see that is both A's

00:08:44,720 --> 00:08:51,140
and B's will have the same size but it's

00:08:47,450 --> 00:08:54,410
not true that all all cells in CIJ will

00:08:51,140 --> 00:08:55,910
have the same density of A's and B's so

00:08:54,410 --> 00:08:58,970
you'll have the same size but it's not

00:08:55,910 --> 00:09:01,160
dense or sparse so we need to figure out

00:08:58,970 --> 00:09:03,920
what the density of the matrix is before

00:09:01,160 --> 00:09:07,339
we start multiplying because that will

00:09:03,920 --> 00:09:09,589
affect the performance as well so margin

00:09:07,339 --> 00:09:11,720
goes to what we'll have this basically a

00:09:09,589 --> 00:09:14,960
CIJ matrix or like a Hamiltonian matrix

00:09:11,720 --> 00:09:17,450
here and you'll be doing a product with

00:09:14,960 --> 00:09:21,320
the X vector that gives you Y so X is

00:09:17,450 --> 00:09:23,810
also feeds in from as an input from a

00:09:21,320 --> 00:09:25,280
previous application from different

00:09:23,810 --> 00:09:28,200
applications

00:09:25,280 --> 00:09:30,330
so the sequential colonel

00:09:28,200 --> 00:09:32,130
kind of looks like this so as you would

00:09:30,330 --> 00:09:34,850
expect that there is a two-dimensional

00:09:32,130 --> 00:09:38,160
matrix multiplication si would go see

00:09:34,850 --> 00:09:40,680
zeros and see calls and then you have a

00:09:38,160 --> 00:09:42,050
this is where the A's and B's we extract

00:09:40,680 --> 00:09:46,230
A's and B's because there is a vector

00:09:42,050 --> 00:09:50,550
inside each and there's a gem called we

00:09:46,230 --> 00:09:52,790
currently use an IBM es SL to actually

00:09:50,550 --> 00:09:56,160
do the multiplications there and

00:09:52,790 --> 00:09:58,890
something to note will be is that there

00:09:56,160 --> 00:10:01,500
are codes in between as well so we

00:09:58,890 --> 00:10:04,320
dereference the value of J and we have

00:10:01,500 --> 00:10:06,330
codes in between so we have no luck with

00:10:04,320 --> 00:10:08,220
collapsing eventually when I will talk

00:10:06,330 --> 00:10:12,840
about nesting now we don't have any luck

00:10:08,220 --> 00:10:15,450
with collapsing there now so the barrel

00:10:12,840 --> 00:10:18,420
part kind of comes out pretty easy from

00:10:15,450 --> 00:10:20,010
the previous slide so the first one is a

00:10:18,420 --> 00:10:23,160
parallel four and then we have two

00:10:20,010 --> 00:10:25,650
reduction operations in J and then in K

00:10:23,160 --> 00:10:28,110
so we have to reduce operations and one

00:10:25,650 --> 00:10:29,850
standard panel for that will just copy

00:10:28,110 --> 00:10:33,600
in value so you don't have a reduction

00:10:29,850 --> 00:10:36,240
in that level so three different

00:10:33,600 --> 00:10:38,310
programming styles and problems or

00:10:36,240 --> 00:10:39,660
successes of each and then you move on

00:10:38,310 --> 00:10:43,770
to the next one so let's look at the

00:10:39,660 --> 00:10:45,810
first is the nested parallelism so what

00:10:43,770 --> 00:10:47,430
we have is OMP parallel for one see

00:10:45,810 --> 00:10:50,190
viral for with reduction and I want to

00:10:47,430 --> 00:10:53,370
barrel for with reduction as well so

00:10:50,190 --> 00:10:55,230
there's three OpenMP constructs here

00:10:53,370 --> 00:10:59,280
those are user-defined reductions

00:10:55,230 --> 00:11:01,080
because we have STL vectors and we

00:10:59,280 --> 00:11:02,460
cannot have a standard reduction so we

00:11:01,080 --> 00:11:04,980
actually wrote our own user-defined

00:11:02,460 --> 00:11:06,630
reduction to standard it's a different

00:11:04,980 --> 00:11:10,020
operation but it's only they're used and

00:11:06,630 --> 00:11:12,660
reduction there so this is what the

00:11:10,020 --> 00:11:15,270
overall the code looks like and there

00:11:12,660 --> 00:11:17,710
are more snippets of code here that is

00:11:15,270 --> 00:11:19,210
not quite relevant to the

00:11:17,710 --> 00:11:24,100
carnal that's why I'm not mentioning

00:11:19,210 --> 00:11:25,720
here but the issues of having this I'm

00:11:24,100 --> 00:11:28,840
going to show in the performance what

00:11:25,720 --> 00:11:30,940
what happened but the problem that the

00:11:28,840 --> 00:11:33,310
challenges that we face is currently we

00:11:30,940 --> 00:11:35,290
don't have any support for nested work

00:11:33,310 --> 00:11:37,690
sharing loops so you cannot you cannot

00:11:35,290 --> 00:11:39,220
have like one OMB peril region and the

00:11:37,690 --> 00:11:41,770
new one before and before and before

00:11:39,220 --> 00:11:43,450
it's not it will not let you compile as

00:11:41,770 --> 00:11:45,820
well if you end up doing it that way so

00:11:43,450 --> 00:11:48,160
you need one p peril for web developers

00:11:45,820 --> 00:11:50,320
you're creating and destroying parallel

00:11:48,160 --> 00:11:52,450
regions every time and this is

00:11:50,320 --> 00:11:55,540
implementation centric the creation and

00:11:52,450 --> 00:11:59,890
destroying but we kind of tried with

00:11:55,540 --> 00:12:01,960
Excel C GCC and clang + + and all of

00:11:59,890 --> 00:12:04,090
them gave us the same same issue

00:12:01,960 --> 00:12:08,020
basically with creation and destruction

00:12:04,090 --> 00:12:10,690
on these peril regions and currently I

00:12:08,020 --> 00:12:12,640
think we have also we are looking for

00:12:10,690 --> 00:12:15,010
some support within these work sharing

00:12:12,640 --> 00:12:17,440
loops to debug code and this is also

00:12:15,010 --> 00:12:21,100
open question that we couldn't find

00:12:17,440 --> 00:12:23,740
anything that openmp supports at this

00:12:21,100 --> 00:12:25,600
point and to have like I won't be single

00:12:23,740 --> 00:12:28,000
or a master to not let you call anything

00:12:25,600 --> 00:12:29,710
within work sharing loops so you cannot

00:12:28,000 --> 00:12:35,590
like print statements or like debug

00:12:29,710 --> 00:12:37,710
using any OpenMP constructs okay right

00:12:35,590 --> 00:12:37,710
here

00:12:38,160 --> 00:12:54,870
yeah we did Excel so okay yeah

00:12:57,510 --> 00:13:02,380
you can but you cannot have a like a

00:13:00,310 --> 00:13:08,140
critical region or a single or on be

00:13:02,380 --> 00:13:15,250
master within the work sharing loops if

00:13:08,140 --> 00:13:19,060
you have nested work sharing loops now

00:13:15,250 --> 00:13:21,460
I'm looking at the can be implementation

00:13:19,060 --> 00:13:28,330
centric but we try to it did not let us

00:13:21,460 --> 00:13:30,190
compile as well yeah okay and so this is

00:13:28,330 --> 00:13:34,240
an issue that we had with thread

00:13:30,190 --> 00:13:37,750
affinity mostly at depth tree till depth

00:13:34,240 --> 00:13:40,840
to it's ok so even if you use OMP places

00:13:37,750 --> 00:13:42,820
and prog mind so you do like a spread

00:13:40,840 --> 00:13:45,730
and then you want to keep it inside that

00:13:42,820 --> 00:13:47,860
spread region it would not hold when you

00:13:45,730 --> 00:13:49,660
go to depth 3 so we tried it with again

00:13:47,860 --> 00:13:51,610
we tried with 3 different compilers and

00:13:49,660 --> 00:13:53,560
its implementation centric it's

00:13:51,610 --> 00:13:55,780
different for different compilers but it

00:13:53,560 --> 00:13:59,400
never stays within the spread region for

00:13:55,780 --> 00:13:59,400
the outermost boundary

00:14:00,750 --> 00:14:09,010
ok ok yeah yeah so like GCC the

00:14:06,790 --> 00:14:11,910
currently does not work for you you show

00:14:09,010 --> 00:14:11,910
results as well

00:14:12,089 --> 00:14:19,769
1:17 so the nursery site use the one on

00:14:20,369 --> 00:14:26,850
1.7 right there

00:14:22,070 --> 00:14:29,459
[Music]

00:14:26,850 --> 00:14:30,839
yes so this is the the latest version

00:14:29,459 --> 00:14:33,600
that we have once I'll be showing

00:14:30,839 --> 00:14:43,109
results on Summit dev and I think we

00:14:33,600 --> 00:14:46,379
have 1.71 zc6 yeah so yeah okay okay

00:14:43,109 --> 00:14:48,329
yeah so currently with six it doesn't

00:14:46,379 --> 00:14:50,759
work at that level but yet with two

00:14:48,329 --> 00:14:55,259
levels everybody kind of holds so for

00:14:50,759 --> 00:15:00,869
every compound okay that was not what I

00:14:55,259 --> 00:15:02,639
wanted to do so we had issues with the

00:15:00,869 --> 00:15:05,729
performance because of that creation and

00:15:02,639 --> 00:15:08,459
destroying of parallel region so we were

00:15:05,729 --> 00:15:12,419
like okay let's let's try with TAS loops

00:15:08,459 --> 00:15:14,339
the new construct open area 4.5 so we

00:15:12,419 --> 00:15:18,509
ended up doing is using OMB task loops

00:15:14,339 --> 00:15:21,449
into the open and apparel force so again

00:15:18,509 --> 00:15:25,109
we have three levels orbit as loop and

00:15:21,449 --> 00:15:27,059
then we have reductions so as we all

00:15:25,109 --> 00:15:29,489
kind of know that currently tasks loops

00:15:27,059 --> 00:15:31,769
we don't have any reduction support so

00:15:29,489 --> 00:15:33,449
what we ended up have to do is you need

00:15:31,769 --> 00:15:36,269
to create a parallel region and then

00:15:33,449 --> 00:15:37,949
reduce in that parallel region which

00:15:36,269 --> 00:15:40,639
kind of defeats the purpose because

00:15:37,949 --> 00:15:44,459
that's what we had in the first place so

00:15:40,639 --> 00:15:46,049
it really nice to have some task level

00:15:44,459 --> 00:15:48,989
reduction support I know this is a

00:15:46,049 --> 00:15:51,869
common topic that has been there for a

00:15:48,989 --> 00:15:56,279
while but yeah so for us that that was a

00:15:51,869 --> 00:15:58,470
major problem in the tasking one more

00:15:56,279 --> 00:16:01,319
thing was next is task loops this we are

00:15:58,470 --> 00:16:05,759
we are kind of not sure because we did

00:16:01,319 --> 00:16:08,249
get some inputs from from GCC and so

00:16:05,759 --> 00:16:11,839
they let you compile the spec doesn't

00:16:08,249 --> 00:16:14,399
say anything if I'm not wrong write that

00:16:11,839 --> 00:16:18,720
task loops within task loops are allowed

00:16:14,399 --> 00:16:20,609
so you can actually compile but it will

00:16:18,720 --> 00:16:23,069
not give you the correct results based

00:16:20,609 --> 00:16:25,330
on different compilers actually give you

00:16:23,069 --> 00:16:28,720
different results

00:16:25,330 --> 00:16:31,780
so I yes it's the wrong answer so the

00:16:28,720 --> 00:16:33,820
rip yeah so even if the even if you

00:16:31,780 --> 00:16:35,350
write the reduction correctly and you

00:16:33,820 --> 00:16:37,690
have those parallel regions you would

00:16:35,350 --> 00:16:40,380
not get the same result so what we feel

00:16:37,690 --> 00:16:44,170
like what happened is so tasse loops

00:16:40,380 --> 00:16:47,950
certain implementations will create like

00:16:44,170 --> 00:16:49,810
a copy of like parallel catch loops so

00:16:47,950 --> 00:16:52,060
you'll have if you have a TAS loop

00:16:49,810 --> 00:16:54,190
outside and you attach them inside then

00:16:52,060 --> 00:16:57,010
you'll create that many number of test

00:16:54,190 --> 00:16:58,870
loops inside rather than breaking it up

00:16:57,010 --> 00:17:01,000
so it's not it doesn't become a task

00:16:58,870 --> 00:17:04,870
sharing loop anymore it becomes like a

00:17:01,000 --> 00:17:11,980
parallel like five iteration catchments

00:17:04,870 --> 00:17:15,520
so and this is true for a thing Excel

00:17:11,980 --> 00:17:18,210
and GCC yes both Excel and GCC at least

00:17:15,520 --> 00:17:18,210
one point six

00:17:19,650 --> 00:17:26,430
yes right yeah so you can you can create

00:17:23,339 --> 00:17:27,959
a task group and then you create tasks

00:17:26,430 --> 00:17:31,080
that's how you would get at a sloop and

00:17:27,959 --> 00:17:32,490
it works yeah so the the problem is

00:17:31,080 --> 00:17:35,850
currently with the implementation off

00:17:32,490 --> 00:17:49,550
task loop rather than the concept of

00:17:35,850 --> 00:17:52,860
tasks okay that's right because right so

00:17:49,550 --> 00:17:55,080
for the OMP parallel force or like the

00:17:52,860 --> 00:17:58,500
nested work sharing the spec actually

00:17:55,080 --> 00:18:02,420
says that you cannot compile whereas in

00:17:58,500 --> 00:18:05,640
the task loops is not it's not very

00:18:02,420 --> 00:18:07,320
exact yeah so and they also say that

00:18:05,640 --> 00:18:08,970
it's a task sharing loop not a work

00:18:07,320 --> 00:18:11,910
sharing loop so it should be pretty

00:18:08,970 --> 00:18:15,650
standard so yeah it's not the spec it's

00:18:11,910 --> 00:18:18,990
probably the implementation itself yes

00:18:15,650 --> 00:18:20,880
so as I mentioned lack of support for

00:18:18,990 --> 00:18:22,320
task level reductions which which makes

00:18:20,880 --> 00:18:24,750
it difficult for us at least and this

00:18:22,320 --> 00:18:27,900
and this is the same thing we had issues

00:18:24,750 --> 00:18:30,179
as well that you cannot add OMP

00:18:27,900 --> 00:18:33,510
constructs again inside task loops we

00:18:30,179 --> 00:18:35,520
could not add point be single I'll check

00:18:33,510 --> 00:18:38,660
on the critical though but I'm pretty

00:18:35,520 --> 00:18:38,660
sure that we couldn't add

00:18:42,400 --> 00:18:47,270
so if you have two three levels at the

00:18:45,679 --> 00:18:50,299
third level when you do master it will

00:18:47,270 --> 00:18:53,510
not let you compile so which can be

00:18:50,299 --> 00:19:03,880
totally implementation it's like one

00:18:53,510 --> 00:19:03,880
threat yeah I'm sorry yeah yeah sorry

00:19:07,720 --> 00:19:10,409
okay

00:19:10,420 --> 00:19:16,150
and this is something that we kind of

00:19:12,790 --> 00:19:18,100
needed for for our application or other

00:19:16,150 --> 00:19:22,060
physics applications that we have at the

00:19:18,100 --> 00:19:24,730
lab as well is currently tasks will let

00:19:22,060 --> 00:19:28,570
you map when you use grain size you can

00:19:24,730 --> 00:19:31,780
map threads to tasks so you can say that

00:19:28,570 --> 00:19:33,910
okay there will be one thread that can

00:19:31,780 --> 00:19:35,890
be do that and do three tasks or four

00:19:33,910 --> 00:19:40,210
tags but you cannot have something like

00:19:35,890 --> 00:19:42,070
one task doing three or four using three

00:19:40,210 --> 00:19:43,780
threads or four threads so you cannot do

00:19:42,070 --> 00:19:47,470
the mapping the other way and that's

00:19:43,780 --> 00:19:50,650
what we kind of needed is to change

00:19:47,470 --> 00:19:52,900
dynamically as well that lets say one

00:19:50,650 --> 00:19:55,300
task needs only one thread and then

00:19:52,900 --> 00:19:58,000
another task needs more more resources

00:19:55,300 --> 00:20:00,040
because it's more computer intensive so

00:19:58,000 --> 00:20:02,020
that at this point directly you cannot

00:20:00,040 --> 00:20:07,630
do it with a with a knowing pea

00:20:02,020 --> 00:20:09,100
construct so because has loops did not

00:20:07,630 --> 00:20:11,530
work for us that's why we ended up

00:20:09,100 --> 00:20:13,900
moving into tasking with nested

00:20:11,530 --> 00:20:15,400
parallelism what we ended up doing is

00:20:13,900 --> 00:20:17,620
you create an open-air apparel region

00:20:15,400 --> 00:20:19,840
everywhere and then we create won't be

00:20:17,620 --> 00:20:22,150
tasks and then you reduce in the

00:20:19,840 --> 00:20:25,090
parallel region instead of reducing in

00:20:22,150 --> 00:20:28,090
the task loops so that kind of comes

00:20:25,090 --> 00:20:30,580
back to the first programming style that

00:20:28,090 --> 00:20:33,340
problem of creation and destroying of

00:20:30,580 --> 00:20:36,070
parallel regions now this is also

00:20:33,340 --> 00:20:40,000
interesting with past losses the

00:20:36,070 --> 00:20:42,730
variables are first private so reduction

00:20:40,000 --> 00:20:44,560
kind of becomes complicated to actually

00:20:42,730 --> 00:20:46,540
do it as well because you kind of got to

00:20:44,560 --> 00:20:49,150
tweak the variable to make it thread

00:20:46,540 --> 00:20:51,190
private and then you move in

00:20:49,150 --> 00:20:54,190
inside the task loops otherwise we

00:20:51,190 --> 00:20:56,350
create like a copy equal same copy of

00:20:54,190 --> 00:20:58,330
the same variable so you want to make it

00:20:56,350 --> 00:20:59,830
thread private and then you move in hub

00:20:58,330 --> 00:21:04,120
to do a reduction if you want to do a

00:20:59,830 --> 00:21:07,690
custom reduction same issues with task

00:21:04,120 --> 00:21:09,130
affinity at depth tree so we couldn't

00:21:07,690 --> 00:21:10,390
find anything where it says that it

00:21:09,130 --> 00:21:12,010
works with depth tree I'm not sure

00:21:10,390 --> 00:21:13,070
whether people have worked with three

00:21:12,010 --> 00:21:16,460
levels of nesting

00:21:13,070 --> 00:21:18,230
in other applications but once you go to

00:21:16,460 --> 00:21:20,690
dep 3 even if you do tithe

00:21:18,230 --> 00:21:22,970
tasks you will not stay in the same

00:21:20,690 --> 00:21:26,690
region so it will move away and go to a

00:21:22,970 --> 00:21:35,480
different court and this is true for I

00:21:26,690 --> 00:21:38,420
would say GCC and then climb yes so we

00:21:35,480 --> 00:21:40,310
ended up using GCC Excel C++ and clang

00:21:38,420 --> 00:21:43,430
plus plus I'm going to show results for

00:21:40,310 --> 00:21:47,810
GCC we write it on Summit dev this is a

00:21:43,430 --> 00:21:51,830
pre like one generation off in the oil

00:21:47,810 --> 00:21:54,200
CF machines for the new summit so as to

00:21:51,830 --> 00:21:55,700
ten core IBM power8 percent processors

00:21:54,200 --> 00:21:58,330
with eight Hardware threads so we're

00:21:55,700 --> 00:22:02,060
going to show with total of 160 threads

00:21:58,330 --> 00:22:04,010
there are GPUs we did not use any target

00:22:02,060 --> 00:22:07,970
construct so we you're not showing any

00:22:04,010 --> 00:22:09,470
results on the GPUs and the D gem the

00:22:07,970 --> 00:22:13,880
actual matrix multiplication is done

00:22:09,470 --> 00:22:18,440
using IBM ES SL and we also tried a

00:22:13,880 --> 00:22:22,060
threaded version the es SL SMP and IBM

00:22:18,440 --> 00:22:25,490
does say that you have to use Excel C

00:22:22,060 --> 00:22:27,950
when you're using IPS SL otherwise they

00:22:25,490 --> 00:22:32,750
don't say whether it works or not but it

00:22:27,950 --> 00:22:35,210
gives us the same performance stuff so

00:22:32,750 --> 00:22:37,580
this is the data set it's kind of

00:22:35,210 --> 00:22:41,090
interesting because what we have is the

00:22:37,580 --> 00:22:43,790
matrix is not sparse or dense at at a

00:22:41,090 --> 00:22:45,680
large scale so all the data is kinda in

00:22:43,790 --> 00:22:48,590
the most in the principal diagonal

00:22:45,680 --> 00:22:50,060
itself and your density increases when

00:22:48,590 --> 00:22:51,620
you're moving towards the center and

00:22:50,060 --> 00:22:53,960
your sparsity increases as you're moving

00:22:51,620 --> 00:22:56,660
away from the principal diagonal so you

00:22:53,960 --> 00:23:00,050
have to be careful when even if you're

00:22:56,660 --> 00:23:02,990
tiling this matrix you still get spots

00:23:00,050 --> 00:23:05,090
where we don't actually need enough

00:23:02,990 --> 00:23:08,420
resources so that's where the task

00:23:05,090 --> 00:23:10,940
elasticity comes into play that we want

00:23:08,420 --> 00:23:13,010
task elasticity where we can change the

00:23:10,940 --> 00:23:15,980
the resources dynamically as

00:23:13,010 --> 00:23:19,070
and as I mentioned before with the

00:23:15,980 --> 00:23:20,929
different phases it even if the phases

00:23:19,070 --> 00:23:23,179
increase in the entire matrix increase

00:23:20,929 --> 00:23:26,000
the pattern stays the same so we'll have

00:23:23,179 --> 00:23:30,049
the same problem as we move forward with

00:23:26,000 --> 00:23:33,440
a bigger data set so what we are showing

00:23:30,049 --> 00:23:36,620
here is the blue lines such as speed up

00:23:33,440 --> 00:23:39,290
over the number of threads use 128

00:23:36,620 --> 00:23:44,299
threads you're paralyzing the I and the

00:23:39,290 --> 00:23:46,460
that is J and if it's clear but yeah so

00:23:44,299 --> 00:23:49,010
the each loop is paralyzed separately

00:23:46,460 --> 00:23:51,049
here so you have only paralyze I loop

00:23:49,010 --> 00:23:53,270
and then we only paralyze J loop what

00:23:51,049 --> 00:23:55,520
we're seeing here is a kind of slowed

00:23:53,270 --> 00:23:58,250
down because the J path the J loop

00:23:55,520 --> 00:24:00,860
actually does the vector reduction so

00:23:58,250 --> 00:24:03,260
whatever you see here is the overhead of

00:24:00,860 --> 00:24:07,130
acting doing the doing the reduction

00:24:03,260 --> 00:24:08,900
itself in the J region and we did not

00:24:07,130 --> 00:24:12,230
paralyze the K loop because we're using

00:24:08,900 --> 00:24:15,830
the D geminal and using the SMP mode you

00:24:12,230 --> 00:24:17,450
can act they will paralyze it right so

00:24:15,830 --> 00:24:23,030
which I'm going to talk about how the

00:24:17,450 --> 00:24:25,640
paralyzer yes LTS SL so this is the the

00:24:23,030 --> 00:24:29,480
the green lines are the projected graphs

00:24:25,640 --> 00:24:31,940
what would happen if we did not have any

00:24:29,480 --> 00:24:34,610
overheads and we just did a product of I

00:24:31,940 --> 00:24:38,120
times J whatever we got from the

00:24:34,610 --> 00:24:41,660
previous slide and the blue lines are

00:24:38,120 --> 00:24:45,470
the actual nested parallel for I for I

00:24:41,660 --> 00:24:48,710
loop and J loop that we got using OpenMP

00:24:45,470 --> 00:24:51,350
so we use outer spread and then inner

00:24:48,710 --> 00:24:53,570
closed and this is muse anime scheduling

00:24:51,350 --> 00:24:55,190
this is actually the wizard different

00:24:53,570 --> 00:24:56,720
scheduling and this is the best one that

00:24:55,190 --> 00:25:01,309
we got and that's what we are showing up

00:24:56,720 --> 00:25:04,610
here so these percentages are what is

00:25:01,309 --> 00:25:06,290
the overhead over creation and

00:25:04,610 --> 00:25:08,030
destroying of the parallel regions

00:25:06,290 --> 00:25:11,150
basically that's what we are showing in

00:25:08,030 --> 00:25:13,309
the so that it's 52 percent of the green

00:25:11,150 --> 00:25:17,080
bar and 36 percent of the green bar so

00:25:13,309 --> 00:25:20,290
can I increases which is obvious as well

00:25:17,080 --> 00:25:22,090
Kalecgos there so what we see is a huge

00:25:20,290 --> 00:25:25,480
overhead of these creation and

00:25:22,090 --> 00:25:27,610
destruction of these parallel regions

00:25:25,480 --> 00:25:31,710
again this can be implementation centric

00:25:27,610 --> 00:25:34,600
but all three compilers at least with

00:25:31,710 --> 00:25:41,290
the ones that we used has the same

00:25:34,600 --> 00:25:46,800
performances of so we also looked at how

00:25:41,290 --> 00:25:48,910
OpenMP kind of works in - with other

00:25:46,800 --> 00:25:53,980
libraries that that we are calling

00:25:48,910 --> 00:25:56,110
inside OpenMP so for D Jim for the IBM

00:25:53,980 --> 00:25:58,120
he SSL what they stayed in their in

00:25:56,110 --> 00:26:01,000
their spec is you can have a single

00:25:58,120 --> 00:26:05,470
parallel region and then you can have

00:26:01,000 --> 00:26:09,100
IBM e SSL SMP version they will not let

00:26:05,470 --> 00:26:11,530
at least with respect you cannot have

00:26:09,100 --> 00:26:14,440
multiple parallel regions and also have

00:26:11,530 --> 00:26:17,380
a spec I also have a Yi SSL SNP you can

00:26:14,440 --> 00:26:20,220
have it it will let you compile but the

00:26:17,380 --> 00:26:22,960
the behavior is not defined in the spec

00:26:20,220 --> 00:26:25,210
so what we end up doing is you create

00:26:22,960 --> 00:26:28,180
one parallel region and then you have a

00:26:25,210 --> 00:26:32,890
SSL SMP we have no other open empty

00:26:28,180 --> 00:26:35,230
constructs in between so the the current

00:26:32,890 --> 00:26:37,240
the way the current spec works at least

00:26:35,230 --> 00:26:39,010
the current implementation works is you

00:26:37,240 --> 00:26:40,750
set your number of threads using one

00:26:39,010 --> 00:26:43,000
piece at numb thread so there's nothing

00:26:40,750 --> 00:26:44,710
dynamic you have to do it at the start

00:26:43,000 --> 00:26:48,670
of your program and so you keep it

00:26:44,710 --> 00:26:51,040
whatever it is and it's not es s l's not

00:26:48,670 --> 00:26:53,110
picking up like smartly okay there is

00:26:51,040 --> 00:26:54,790
enough work will do more threads or

00:26:53,110 --> 00:26:56,980
there's not enough for we not do it if

00:26:54,790 --> 00:26:59,320
you if you specify number of credits

00:26:56,980 --> 00:27:01,750
let's say - it will run with two threads

00:26:59,320 --> 00:27:04,030
irrespective of the fact whether the

00:27:01,750 --> 00:27:06,400
work is enough or not so the user kind

00:27:04,030 --> 00:27:09,190
of needs to know what to put here and

00:27:06,400 --> 00:27:10,660
it's not dynamic as well so you you

00:27:09,190 --> 00:27:13,840
can't put different things for different

00:27:10,660 --> 00:27:15,080
locations as well and so that's what I

00:27:13,840 --> 00:27:18,289
mean by the time

00:27:15,080 --> 00:27:22,489
right assignment let's see and currently

00:27:18,289 --> 00:27:24,950
with OpenMP we didn't find anything with

00:27:22,489 --> 00:27:27,049
the consulate refer to the construct to

00:27:24,950 --> 00:27:30,710
extract some tasks level or thread level

00:27:27,049 --> 00:27:32,860
information when you're inside the IBM e

00:27:30,710 --> 00:27:35,360
ssl kernel they say that ok we are doing

00:27:32,860 --> 00:27:38,299
to using two threads but we don't get

00:27:35,360 --> 00:27:39,769
any information out from there we can

00:27:38,299 --> 00:27:43,429
see it's doing because the performance

00:27:39,769 --> 00:27:45,049
goes down right away and within the

00:27:43,429 --> 00:27:46,639
nested parent region according to the

00:27:45,049 --> 00:27:50,450
spec as well it's it's undefined

00:27:46,639 --> 00:27:53,950
behavior they don't say what their what

00:27:50,450 --> 00:27:58,249
it should got about like ten minutes

00:27:53,950 --> 00:28:00,200
yes okay so this is a kind of an

00:27:58,249 --> 00:28:04,820
orthogonal work that weíre also doing

00:28:00,200 --> 00:28:07,970
with the VAR g + + as have another c++

00:28:04,820 --> 00:28:10,159
is a is a programming model that's the

00:28:07,970 --> 00:28:12,080
diff it was actually designed at rice

00:28:10,159 --> 00:28:14,749
but now we move so it's now Georgia Tech

00:28:12,080 --> 00:28:16,009
and so the task based programming model

00:28:14,749 --> 00:28:19,789
it's kind of lightweight to work

00:28:16,009 --> 00:28:21,889
stealing base model and so we say that

00:28:19,789 --> 00:28:24,470
okay we can use eighteen live as a path

00:28:21,889 --> 00:28:27,259
to XO scale by using for the internode

00:28:24,470 --> 00:28:29,659
you would be still using a OpenMP and

00:28:27,259 --> 00:28:32,210
you'll be using like resource management

00:28:29,659 --> 00:28:34,220
using oven or c++ as a resource

00:28:32,210 --> 00:28:36,289
management tool and when you're going to

00:28:34,220 --> 00:28:38,720
the inner node you'll be using MPI UPC

00:28:36,289 --> 00:28:40,749
plus plus and open schmell and eight c++

00:28:38,720 --> 00:28:46,340
basically gives you the abstraction to

00:28:40,749 --> 00:28:48,859
to work on it so this is similar to the

00:28:46,340 --> 00:28:51,470
reductions that we support at this point

00:28:48,859 --> 00:28:56,660
in hover nor c++ we call it finish

00:28:51,470 --> 00:28:58,190
accumulators kind of an atomic operation

00:28:56,660 --> 00:29:00,950
so there's two different implementations

00:28:58,190 --> 00:29:03,290
at this point we have a eager operation

00:29:00,950 --> 00:29:04,790
which is more portable and what you end

00:29:03,290 --> 00:29:08,540
up doing is like you have three threads

00:29:04,790 --> 00:29:11,090
every thread can put one value in it or

00:29:08,540 --> 00:29:13,700
multiple values in it and it accumulates

00:29:11,090 --> 00:29:15,590
here only once you go out of the end

00:29:13,700 --> 00:29:17,750
finishes basically similar to like a end

00:29:15,590 --> 00:29:19,160
of a parallel region so when you go out

00:29:17,750 --> 00:29:20,810
of the parallel region you can do a dot

00:29:19,160 --> 00:29:22,130
get and you would get the values and

00:29:20,810 --> 00:29:27,790
that's where the accumulation happens

00:29:22,130 --> 00:29:27,790
for the ego reduction sorry oh my god

00:29:28,600 --> 00:29:35,930
and there's a lazy reduction where we

00:29:32,150 --> 00:29:37,760
have values in our different small

00:29:35,930 --> 00:29:39,860
containers and your reduction only

00:29:37,760 --> 00:29:42,290
happens when you call the gate so it

00:29:39,860 --> 00:29:44,240
it's better performance that way you're

00:29:42,290 --> 00:29:48,740
not accumulating every time you're

00:29:44,240 --> 00:29:50,960
accumulating only went 20 meter so this

00:29:48,740 --> 00:29:53,930
is a work in progress what we can do is

00:29:50,960 --> 00:29:55,190
you can create one parallel region we do

00:29:53,930 --> 00:29:56,750
not have to create multiple parallel

00:29:55,190 --> 00:29:58,460
regions we can create one parent region

00:29:56,750 --> 00:30:00,680
and you can see we have open ending

00:29:58,460 --> 00:30:03,200
tasks here and we give you the correct

00:30:00,680 --> 00:30:06,650
results using accumulator so we created

00:30:03,200 --> 00:30:09,590
as a user-defined accumulator here this

00:30:06,650 --> 00:30:11,630
is your type so you can do some divide

00:30:09,590 --> 00:30:13,430
anything basically there as long as you

00:30:11,630 --> 00:30:15,380
define it and this is where we're doing

00:30:13,430 --> 00:30:17,600
the update so that's the only update

00:30:15,380 --> 00:30:20,060
operation that we have in the entire

00:30:17,600 --> 00:30:23,390
code and you retrieve it at like when

00:30:20,060 --> 00:30:26,210
you finish the parallel region in 17 so

00:30:23,390 --> 00:30:27,500
you can do it currently using one

00:30:26,210 --> 00:30:32,540
parallel region that's what we want to

00:30:27,500 --> 00:30:34,970
kind of show here so to kind of wrap up

00:30:32,540 --> 00:30:36,500
what we're thinking is the sum of the

00:30:34,970 --> 00:30:38,830
candidates that can be for the future

00:30:36,500 --> 00:30:42,080
OpenMP support would be nice to have is

00:30:38,830 --> 00:30:43,970
support for task level reductions the

00:30:42,080 --> 00:30:46,480
tap inflation that we're talking about

00:30:43,970 --> 00:30:49,760
is like dynamic resource allocation and

00:30:46,480 --> 00:30:53,060
tap your Finity like extending of tied

00:30:49,760 --> 00:30:54,710
tasks to check like whether it can be

00:30:53,060 --> 00:30:56,350
totally implementation defined as well

00:30:54,710 --> 00:31:01,240
but at the third level

00:30:56,350 --> 00:31:03,520
it's not it doesn't hold true with red

00:31:01,240 --> 00:31:05,590
persistence again in depth tree it

00:31:03,520 --> 00:31:07,660
doesn't hold true as well

00:31:05,590 --> 00:31:11,320
currently we don't have support for

00:31:07,660 --> 00:31:13,450
dynamic OpenMP places it's static so you

00:31:11,320 --> 00:31:16,750
would create it outside when you're like

00:31:13,450 --> 00:31:18,520
calling the actual code and once you

00:31:16,750 --> 00:31:20,740
start the kernel itself you cannot

00:31:18,520 --> 00:31:23,169
change though empty places as well which

00:31:20,740 --> 00:31:25,270
we needed for a certain different kind

00:31:23,169 --> 00:31:27,100
of applications where you're the the

00:31:25,270 --> 00:31:30,549
kernel itself is growing so you want to

00:31:27,100 --> 00:31:32,710
change the DOE empty places there as

00:31:30,549 --> 00:31:35,260
well and addressing debugging challenges

00:31:32,710 --> 00:31:38,620
would be something interesting for us as

00:31:35,260 --> 00:31:40,900
well so just looking forward what we are

00:31:38,620 --> 00:31:42,490
doing right now is reusing the target

00:31:40,900 --> 00:31:45,130
directives which is kind of obvious I

00:31:42,490 --> 00:31:48,429
guess for the GPUs on Summit they're

00:31:45,130 --> 00:31:50,740
using opening apparel regions teams to

00:31:48,429 --> 00:31:53,950
create abstractions on the CPU itself so

00:31:50,740 --> 00:31:55,450
we're using teams but we're using it on

00:31:53,950 --> 00:31:58,950
the CPU so we're going to try that out

00:31:55,450 --> 00:32:04,299
and reco designing with Coco's at Sandia

00:31:58,950 --> 00:32:06,700
magma is a user you TK's DGM kernel and

00:32:04,299 --> 00:32:10,270
then Hubbard C++ with Georgia Tech and

00:32:06,700 --> 00:32:14,309
rice and yeah the original was written

00:32:10,270 --> 00:32:16,870
in 400 Java and then we moved to 200 C++

00:32:14,309 --> 00:32:20,559
yeah with that I'd like to thank our

00:32:16,870 --> 00:32:22,630
collaborators most vewy Oak Ridge

00:32:20,559 --> 00:32:26,200
National and then why away Gonzalo

00:32:22,630 --> 00:32:30,480
Oscar's here Vivek and we have research

00:32:26,200 --> 00:32:32,669
scientists from rice and Georgia Tech

00:32:30,480 --> 00:32:49,859
yeah and with that I'll take any

00:32:32,669 --> 00:32:53,220
questions thank you and say it's a very

00:32:49,859 --> 00:32:58,820
severe yes the one dimensional vector

00:32:53,220 --> 00:33:01,679
[Music]

00:32:58,820 --> 00:33:12,440
sorry I don't understand your question

00:33:01,679 --> 00:33:12,440
again let me pull up that right

00:33:13,320 --> 00:33:22,630
where is the DM from so these are so the

00:33:22,000 --> 00:33:31,540
CIJ

00:33:22,630 --> 00:33:34,560
so that's what happened here slap your

00:33:31,540 --> 00:33:34,560
CIJ matrix

00:33:37,680 --> 00:33:41,940
so that makes sense so this is the two

00:33:40,470 --> 00:33:44,250
dimensional matrix so you're calling

00:33:41,940 --> 00:33:45,420
each cell you're picking up one cell

00:33:44,250 --> 00:33:47,430
here and you're picking up certain

00:33:45,420 --> 00:33:53,420
values from here that's what you're

00:33:47,430 --> 00:33:53,420
doing a product off all right okay

00:33:53,690 --> 00:33:55,690
Oh

00:34:01,470 --> 00:34:04,610
yeah hello

00:34:05,150 --> 00:34:10,650
apart from reduction in between like

00:34:07,590 --> 00:34:14,280
here yeah so we have certain functions

00:34:10,650 --> 00:34:16,800
that need to be called and we need

00:34:14,280 --> 00:34:24,030
values for that that needs to go in side

00:34:16,800 --> 00:34:26,320
the parallel cane yeah so the DM is like

00:34:24,030 --> 00:34:29,379
the ninety percent time for for this

00:34:26,320 --> 00:34:29,379
[Music]

00:34:35,780 --> 00:34:48,050
that's where I know why that you do

00:34:40,580 --> 00:34:50,210
going and so he have collapsed free then

00:34:48,050 --> 00:34:51,859
you cannot actually if I'm right that

00:34:50,210 --> 00:35:01,700
you cannot have collapsed free with

00:34:51,859 --> 00:35:03,980
stuff in between right okay okay but we

00:35:01,700 --> 00:35:06,020
are still not using the concept of

00:35:03,980 --> 00:35:08,119
nested parallelism once you want to

00:35:06,020 --> 00:35:10,130
collect yes I think that was the point

00:35:08,119 --> 00:35:12,109
as well here is like we are showing that

00:35:10,130 --> 00:35:14,170
we're using and what's happening now

00:35:12,109 --> 00:35:16,700
rather than we already have

00:35:14,170 --> 00:35:20,030
what we we know that we can do it by

00:35:16,700 --> 00:35:21,619
changing the carnal itself yes that is

00:35:20,030 --> 00:35:23,660
always possible that is always true for

00:35:21,619 --> 00:35:26,330
all cases but without changing the

00:35:23,660 --> 00:35:27,680
carnal keeping it as it is where we can

00:35:26,330 --> 00:35:31,090
do using the open that's what we're

00:35:27,680 --> 00:35:31,090
showing rather than changing

00:35:34,980 --> 00:35:48,550
cases may not sure yeah yeah that's

00:35:43,030 --> 00:35:51,720
right even go get pulled sorry yeah when

00:35:48,550 --> 00:35:55,390
you're looking for elastic tab said

00:35:51,720 --> 00:36:01,140
exactly whether you really he deleted

00:35:55,390 --> 00:36:04,310
devote more press individual tab support

00:36:01,140 --> 00:36:04,310
[Music]

00:36:06,360 --> 00:36:13,330
every attack sure yes so what happens

00:36:11,530 --> 00:36:15,610
looking for at least in this case or

00:36:13,330 --> 00:36:19,000
like even in the cases that will grow is

00:36:15,610 --> 00:36:20,650
you will be creating tasks for places

00:36:19,000 --> 00:36:23,170
that you don't even need it as well so

00:36:20,650 --> 00:36:25,270
there is that's the granularity part and

00:36:23,170 --> 00:36:37,750
now there are cases where you need more

00:36:25,270 --> 00:36:40,750
resources for each task resource

00:36:37,750 --> 00:36:40,750
you

00:36:44,970 --> 00:36:49,730
maybe this won't work

00:36:52,010 --> 00:36:57,900
all right yeah so it said the so the

00:36:55,859 --> 00:37:00,329
tasking is more like the abstraction of

00:36:57,900 --> 00:37:02,250
the of this matrix and then we call the

00:37:00,329 --> 00:37:04,890
data parallel which is the D general and

00:37:02,250 --> 00:37:07,170
there we want more resource to be

00:37:04,890 --> 00:37:11,059
assigned to certain places less resource

00:37:07,170 --> 00:37:11,059
but that is not higher letting us

00:37:14,080 --> 00:37:17,690
right

00:37:15,320 --> 00:37:21,890
yeah yeah yeah I should have clarified

00:37:17,690 --> 00:37:26,570
lad yes never use my sauce using more

00:37:21,890 --> 00:37:29,770
than one needle how that nastiness revel

00:37:26,570 --> 00:37:29,770
and uh

00:37:32,180 --> 00:37:39,660
you know we have not we have not so yeah

00:37:36,180 --> 00:37:42,450
we would be happy to get inputs on that

00:37:39,660 --> 00:37:45,869
now currently it's only one one GPU and

00:37:42,450 --> 00:37:47,640
then yeah the breaking up of the the

00:37:45,869 --> 00:37:49,920
computation itself is going to be

00:37:47,640 --> 00:37:53,150
challenging as well because we can

00:37:49,920 --> 00:37:55,680
change the carnal totally to make it

00:37:53,150 --> 00:38:00,630
because as I showed that the data will

00:37:55,680 --> 00:38:03,049
kind of stay in the primary diagonal so

00:38:00,630 --> 00:38:05,940
what you can have is divided up that

00:38:03,049 --> 00:38:07,229
primary diagonal into small batches and

00:38:05,940 --> 00:38:09,299
send it over

00:38:07,229 --> 00:38:11,130
whereas and then you can leave the

00:38:09,299 --> 00:38:12,390
nopper triangular matrix and lower

00:38:11,130 --> 00:38:14,519
triangular matrix as a sequential

00:38:12,390 --> 00:38:16,880
version as well and it's it's not

00:38:14,519 --> 00:38:20,489
dependent so you can do it totally

00:38:16,880 --> 00:38:25,180
orthogonal II as one so it will not be a

00:38:20,489 --> 00:38:28,330
problem you know

00:38:25,180 --> 00:38:28,330
for what

00:38:29,450 --> 00:38:35,410
the teams I did not lose okay yeah

00:38:38,099 --> 00:38:45,300
they created if you are going on and and

00:38:43,460 --> 00:38:50,550
employment

00:38:45,300 --> 00:38:50,550
I'll see you at the end and

00:38:54,259 --> 00:38:59,430
yeah yeah so even if you fix so you

00:38:57,299 --> 00:39:02,759
cannot actually fix the behavior saying

00:38:59,430 --> 00:39:05,549
that don't destroy you can spin the

00:39:02,759 --> 00:39:07,650
threads you can spin the threads that

00:39:05,549 --> 00:39:09,539
are already created you can actually

00:39:07,650 --> 00:39:11,339
call it and you can spin the threads but

00:39:09,539 --> 00:39:12,960
you cannot actually say oh don't destroy

00:39:11,339 --> 00:39:15,390
the threads it's totally implementation

00:39:12,960 --> 00:39:17,430
centric at least that's what we figure

00:39:15,390 --> 00:39:19,829
out is when you have multiple of these

00:39:17,430 --> 00:39:21,719
open airy parallel regions it is

00:39:19,829 --> 00:39:25,039
creating and destroying and that's

00:39:21,719 --> 00:39:25,039
what's causing the main problem

00:39:27,380 --> 00:39:37,309
there were makin that a little um for

00:39:33,619 --> 00:39:37,309
nested church

00:39:38,740 --> 00:39:45,140
okay we did not

00:39:42,290 --> 00:39:48,690
[Music]

00:39:45,140 --> 00:39:50,099
okay we did we did not try to that yeah

00:39:48,690 --> 00:39:55,670
we should talk about that we did not try

00:39:50,099 --> 00:39:55,670

YouTube URL: https://www.youtube.com/watch?v=SsCsPLECi4s


