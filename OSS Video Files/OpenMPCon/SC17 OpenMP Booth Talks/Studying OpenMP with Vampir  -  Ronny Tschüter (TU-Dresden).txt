Title: Studying OpenMP with Vampir  -  Ronny TschÃ¼ter (TU-Dresden)
Publication date: 2017-11-23
Playlist: SC17 OpenMP Booth Talks
Description: 
	SC17 OpenMP booth talks - November 2017, Denver CO
PDF slides at http://openmp.org/resources/openmp-presentations/sc17-booth-talks
Captions: 
	00:00:00,319 --> 00:00:06,569
okay so welcome everyone and I want to

00:00:04,350 --> 00:00:09,269
give a short tour showing how you can

00:00:06,569 --> 00:00:11,250
study openmp applications with the help

00:00:09,269 --> 00:00:14,820
of the vampyre performance analysis tool

00:00:11,250 --> 00:00:17,340
and I will present you some case studies

00:00:14,820 --> 00:00:19,289
in this Tork showing typical performance

00:00:17,340 --> 00:00:23,340
problems and how you can detect it using

00:00:19,289 --> 00:00:25,730
vampyre and let me start by these sparse

00:00:23,340 --> 00:00:28,680
matrix vector multiplication example

00:00:25,730 --> 00:00:34,100
where we will see some load imbalances

00:00:28,680 --> 00:00:38,420
so yeah it's pretty simple in this case

00:00:34,100 --> 00:00:42,510
so it's a matrix multiplication with

00:00:38,420 --> 00:00:45,210
sparse populated matrixes so only the

00:00:42,510 --> 00:00:48,120
nonzero elements are stored in order to

00:00:45,210 --> 00:00:50,039
save some memory and as you can see here

00:00:48,120 --> 00:00:53,190
in the algorithm also computation is

00:00:50,039 --> 00:00:57,149
only executed for nonzero elements in a

00:00:53,190 --> 00:01:00,230
row so you can transform this simple

00:00:57,149 --> 00:01:04,320
algorithm directly into naive Oakman P

00:01:00,230 --> 00:01:08,369
program and you see there's one problem

00:01:04,320 --> 00:01:11,580
with these parallel loop so the rows of

00:01:08,369 --> 00:01:14,610
a will be evenly distributed across all

00:01:11,580 --> 00:01:18,150
threads and as you can see there might

00:01:14,610 --> 00:01:24,290
be some nonzero element and this may

00:01:18,150 --> 00:01:27,060
influence your load imbalance so if you

00:01:24,290 --> 00:01:29,369
program your application like this and

00:01:27,060 --> 00:01:31,619
you monitor your applications with

00:01:29,369 --> 00:01:34,590
vampyre and try to analyze the

00:01:31,619 --> 00:01:36,390
performance and they have a look on the

00:01:34,590 --> 00:01:38,820
resulting trace file you will see a

00:01:36,390 --> 00:01:41,130
picture like this and what you see

00:01:38,820 --> 00:01:43,710
directly on the first sketch here is

00:01:41,130 --> 00:01:45,899
that this blue color is dominating what

00:01:43,710 --> 00:01:48,060
is this blue color so as you can see in

00:01:45,899 --> 00:01:50,009
the function legend each individual

00:01:48,060 --> 00:01:52,829
function truth has its own color and

00:01:50,009 --> 00:01:55,170
blue stands for example here for open MP

00:01:52,829 --> 00:01:58,799
synchronization so especially you can

00:01:55,170 --> 00:02:00,979
imagine it as waiting time and waiting

00:01:58,799 --> 00:02:05,159
time is not a good thing for yeah

00:02:00,979 --> 00:02:08,640
performant program so what's the problem

00:02:05,159 --> 00:02:11,069
here so you have a lot of processes

00:02:08,640 --> 00:02:13,349
which do not compute very well because

00:02:11,069 --> 00:02:14,160
they have a lot of these non zero zero

00:02:13,349 --> 00:02:17,120
elements

00:02:14,160 --> 00:02:20,460
and you have Damascus wet with a lot of

00:02:17,120 --> 00:02:23,250
rows with nonzero elements we the load

00:02:20,460 --> 00:02:27,870
imbalance is really visible in these and

00:02:23,250 --> 00:02:29,760
sketch here so what can we do here to

00:02:27,870 --> 00:02:34,440
improve the performance of this program

00:02:29,760 --> 00:02:37,830
so in this case it's simply easy so we

00:02:34,440 --> 00:02:40,620
can try to use the scheduler pragma OMP

00:02:37,830 --> 00:02:43,950
and switch from dynamic from static

00:02:40,620 --> 00:02:47,450
scheduling to the dynamic scheduling so

00:02:43,950 --> 00:02:50,940
this will achieve a better load

00:02:47,450 --> 00:02:53,910
distributions across all threads and if

00:02:50,940 --> 00:02:56,190
you manipulate through source code of

00:02:53,910 --> 00:02:58,190
your program in this way compile the

00:02:56,190 --> 00:03:00,660
application again run it again and

00:02:58,190 --> 00:03:03,630
create performance data out of it and

00:03:00,660 --> 00:03:06,260
you see vampa tool you immediately see

00:03:03,630 --> 00:03:11,070
this looks much better because now

00:03:06,260 --> 00:03:14,210
almost 90% of your program is time is

00:03:11,070 --> 00:03:17,910
spent in the open P loop so it's yeah

00:03:14,210 --> 00:03:19,709
doing any sort of computation and the

00:03:17,910 --> 00:03:22,950
amount of synchronization was

00:03:19,709 --> 00:03:25,260
dramatically reduced so here we see

00:03:22,950 --> 00:03:28,230
these two pictures the unoptimized

00:03:25,260 --> 00:03:31,170
version the optimized version one but

00:03:28,230 --> 00:03:33,030
when people write and nice feature which

00:03:31,170 --> 00:03:36,690
is very helpful for these performance

00:03:33,030 --> 00:03:41,160
comparison it's called the comparison

00:03:36,690 --> 00:03:43,290
view where you can see both versions of

00:03:41,160 --> 00:03:46,980
the program side by side and you

00:03:43,290 --> 00:03:48,959
directly see the improvement in runtime

00:03:46,980 --> 00:03:52,080
of the application so the runtime is

00:03:48,959 --> 00:03:55,290
decreased by nearly a factor of two just

00:03:52,080 --> 00:03:58,560
by switching from static scheduling to

00:03:55,290 --> 00:04:01,530
dynamic scheduling and as I already told

00:03:58,560 --> 00:04:05,690
so you see these dramatically decrease

00:04:01,530 --> 00:04:08,690
in the share of synchronization

00:04:05,690 --> 00:04:08,690
primitives

00:04:09,560 --> 00:04:16,549
so this was yet a basic example showing

00:04:14,209 --> 00:04:20,209
a typical problem of load balancing

00:04:16,549 --> 00:04:23,450
within open people loops but sometimes

00:04:20,209 --> 00:04:25,480
it's not that easy so often you use a

00:04:23,450 --> 00:04:28,280
combination of multiple parallel

00:04:25,480 --> 00:04:30,620
paradigms for example you do not know

00:04:28,280 --> 00:04:33,590
use only open P you use it in

00:04:30,620 --> 00:04:37,389
combination with MPI and then you have a

00:04:33,590 --> 00:04:40,850
problem that your load imbalances within

00:04:37,389 --> 00:04:43,880
the openmp parallel loops you can see it

00:04:40,850 --> 00:04:48,290
here you have again these blue areas

00:04:43,880 --> 00:04:50,270
indicating waiting time in open p lu the

00:04:48,290 --> 00:04:53,110
problem here is that these load

00:04:50,270 --> 00:04:57,950
imbalance in the Oakland pillow

00:04:53,110 --> 00:05:02,240
propagates in your program execution and

00:04:57,950 --> 00:05:05,110
causes in addition MPI waiting times

00:05:02,240 --> 00:05:08,000
because after the parallel loop here

00:05:05,110 --> 00:05:12,820
there is some MPI communication in this

00:05:08,000 --> 00:05:16,370
case it's a weight all and so this

00:05:12,820 --> 00:05:18,919
results in one MPI process coming late

00:05:16,370 --> 00:05:21,590
in this collective operation and all the

00:05:18,919 --> 00:05:23,780
other participants have to wait until

00:05:21,590 --> 00:05:26,539
this last process arrives in the

00:05:23,780 --> 00:05:30,410
collective operation so it's not only

00:05:26,539 --> 00:05:33,950
OpenMP you have to be care of so you

00:05:30,410 --> 00:05:36,110
also should care about alteration

00:05:33,950 --> 00:05:38,810
paradigms you use and when people write

00:05:36,110 --> 00:05:41,810
you to that can show you all these

00:05:38,810 --> 00:05:44,840
effects in a single view so we can

00:05:41,810 --> 00:05:50,560
investigate all your problems with only

00:05:44,840 --> 00:05:53,750
one tool and n so this third case study

00:05:50,560 --> 00:05:59,919
Trinity it's an RNA sequence assembler

00:05:53,750 --> 00:06:05,330
and isn't a production code and in this

00:05:59,919 --> 00:06:08,690
case studied they are scaling or scaling

00:06:05,330 --> 00:06:11,300
study executed so they tested how well

00:06:08,690 --> 00:06:14,800
these application scales with multiple

00:06:11,300 --> 00:06:17,240
open P threads so they done experiments

00:06:14,800 --> 00:06:19,940
without open piece it was a sequential

00:06:17,240 --> 00:06:22,919
version with two open P threads

00:06:19,940 --> 00:06:26,430
four eight and sixteen open

00:06:22,919 --> 00:06:30,090
threats and as you can see as you

00:06:26,430 --> 00:06:33,419
increase the OpenMP number of threats

00:06:30,090 --> 00:06:36,900
your waiting time increases so also here

00:06:33,419 --> 00:06:38,340
marked in these blue colors so it's

00:06:36,900 --> 00:06:40,860
really visible here the three

00:06:38,340 --> 00:06:43,169
application does not scale well so what

00:06:40,860 --> 00:06:45,210
is the problem with this program so is

00:06:43,169 --> 00:06:48,090
it maybe as we saw in the first example

00:06:45,210 --> 00:06:51,240
and misuse or a misuse of

00:06:48,090 --> 00:06:53,819
openmp pragmas in this case it was not

00:06:51,240 --> 00:06:55,830
the open p pragma which causes some

00:06:53,819 --> 00:06:58,710
problems or the Oakman p scheduling it

00:06:55,830 --> 00:07:02,189
was the workload within the open peep

00:06:58,710 --> 00:07:06,479
merrill loop so you see these function

00:07:02,189 --> 00:07:09,120
string streams and this uses C++ objects

00:07:06,479 --> 00:07:11,370
so within these functions new string

00:07:09,120 --> 00:07:15,169
stream objects were created and this

00:07:11,370 --> 00:07:18,870
creation was and guarded by mutexes and

00:07:15,169 --> 00:07:22,919
these mutexes caused a serialization of

00:07:18,870 --> 00:07:25,379
your work execution so this problem was

00:07:22,919 --> 00:07:28,409
solved by moving the creation of the

00:07:25,379 --> 00:07:30,419
string streams out of the loop and doing

00:07:28,409 --> 00:07:33,029
only computation of in the loop and

00:07:30,419 --> 00:07:38,039
thereby they were able to achieve him

00:07:33,029 --> 00:07:41,550
better scaling so they could speed up

00:07:38,039 --> 00:07:46,770
the execution from two point three to

00:07:41,550 --> 00:07:48,899
eight point nine and you see the amount

00:07:46,770 --> 00:07:52,500
of waking time could be dramatically

00:07:48,899 --> 00:07:55,979
decreased and also these scaling studies

00:07:52,500 --> 00:07:59,060
are well supported by the rampion tool

00:07:55,979 --> 00:08:01,529
suite because of these personal

00:07:59,060 --> 00:08:04,229
comparison of multiple executions and

00:08:01,529 --> 00:08:06,629
you directly see how the application

00:08:04,229 --> 00:08:11,580
performs in the different setups of your

00:08:06,629 --> 00:08:14,460
experiments so the last and case study i

00:08:11,580 --> 00:08:18,330
want to show you here is the LSMs code

00:08:14,460 --> 00:08:22,199
and here we want to see how we can use

00:08:18,330 --> 00:08:27,229
open p to utilize efficiently hardware

00:08:22,199 --> 00:08:29,939
resources so it's not about M yeah

00:08:27,229 --> 00:08:33,839
paralyzing your workload it's more that

00:08:29,939 --> 00:08:35,940
you have in this case and a node with 20

00:08:33,839 --> 00:08:42,690
cores and you had

00:08:35,940 --> 00:08:47,940
um multiple in this case m4 GPUs and you

00:08:42,690 --> 00:08:50,670
want to try to create enough workload to

00:08:47,940 --> 00:08:55,500
utilize all the GPU resources in this

00:08:50,670 --> 00:08:57,690
use case several setups were executed

00:08:55,500 --> 00:09:00,180
and you can see usually you would expect

00:08:57,690 --> 00:09:03,390
for example that I want one mapping from

00:09:00,180 --> 00:09:06,480
fits fine so each thread for example

00:09:03,390 --> 00:09:11,910
uses one GPU but in this experiment it

00:09:06,480 --> 00:09:15,210
reveals that the setup with 20 using 20

00:09:11,910 --> 00:09:21,360
CPU cores was the fastest and want to

00:09:15,210 --> 00:09:24,750
utilize all the 16gb ocher efficiently M

00:09:21,360 --> 00:09:26,940
so that was all the case that is I want

00:09:24,750 --> 00:09:30,950
to show you I hope you enjoyed the talk

00:09:26,940 --> 00:09:30,950

YouTube URL: https://www.youtube.com/watch?v=w__CCCW9hsk


