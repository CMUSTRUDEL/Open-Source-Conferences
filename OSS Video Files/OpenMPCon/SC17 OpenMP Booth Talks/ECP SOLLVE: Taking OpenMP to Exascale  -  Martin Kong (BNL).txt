Title: ECP SOLLVE: Taking OpenMP to Exascale  -  Martin Kong (BNL)
Publication date: 2017-11-23
Playlist: SC17 OpenMP Booth Talks
Description: 
	SC17 OpenMP booth talks - November 2017 Denver CO
PDF slides at http://openmp.org/resources/openmp-presentations/sc17-booth-talks
Captions: 
	00:00:00,000 --> 00:00:05,250
that's good ok so hello everybody my

00:00:03,899 --> 00:00:07,200
name is Martin Kong

00:00:05,250 --> 00:00:09,300
I'm an assistant computational scientist

00:00:07,200 --> 00:00:12,630
in Brookhaven National Lab and today I'm

00:00:09,300 --> 00:00:15,929
going to give a small talk about the ECP

00:00:12,630 --> 00:00:21,210
solve project which consists on taking

00:00:15,929 --> 00:00:23,609
OpenMP to excess scale so the outline of

00:00:21,210 --> 00:00:26,010
my talk is the following first I'm going

00:00:23,609 --> 00:00:28,580
to give a very brief overview of the

00:00:26,010 --> 00:00:31,800
exascale computing project or ECP and

00:00:28,580 --> 00:00:34,649
the world as the salt project fit within

00:00:31,800 --> 00:00:35,940
it they are going to discuss a bit about

00:00:34,649 --> 00:00:38,250
the excess grade challenges that we are

00:00:35,940 --> 00:00:40,620
trying to address and then how we are

00:00:38,250 --> 00:00:43,860
achieving this in the context of openmp

00:00:40,620 --> 00:00:46,379
solve and also within LLVM basically the

00:00:43,860 --> 00:00:50,760
solutions and our directions and finally

00:00:46,379 --> 00:00:52,860
I will brief some concluding remarks so

00:00:50,760 --> 00:00:57,920
what is the exascale computing project

00:00:52,860 --> 00:01:00,660
so basically it's a project and effort

00:00:57,920 --> 00:01:03,180
thanks of thanks to the Department of

00:01:00,660 --> 00:01:05,010
Energy two of its kind organizations

00:01:03,180 --> 00:01:06,119
basically they office the Office of

00:01:05,010 --> 00:01:08,580
Science and the National Nuclear

00:01:06,119 --> 00:01:11,850
Security Administration the whole the

00:01:08,580 --> 00:01:15,150
basic goal is to take the reach the

00:01:11,850 --> 00:01:18,659
exascale computing performance objective

00:01:15,150 --> 00:01:21,119
which is basically achieving 50x more

00:01:18,659 --> 00:01:26,880
higher performance nowadays than the

00:01:21,119 --> 00:01:28,890
Queen beta flops systems so this exact

00:01:26,880 --> 00:01:30,810
exascale computing systems basically

00:01:28,890 --> 00:01:32,640
have a number of Chinese that we have to

00:01:30,810 --> 00:01:35,729
address of course we have the

00:01:32,640 --> 00:01:37,770
performance goals but in addition we

00:01:35,729 --> 00:01:40,950
have to operate on a particular very

00:01:37,770 --> 00:01:43,680
convenient power cup between 20 and 30

00:01:40,950 --> 00:01:46,649
megawatts then the system the whole

00:01:43,680 --> 00:01:49,229
system or ecosystem as a whole must

00:01:46,649 --> 00:01:51,689
offer a very high reliable levels of

00:01:49,229 --> 00:01:54,030
resiliency and then we are expected to

00:01:51,689 --> 00:01:56,070
be operating in a very complex material

00:01:54,030 --> 00:01:57,439
e-news software stocks interacting with

00:01:56,070 --> 00:02:04,200
different libraries applications

00:01:57,439 --> 00:02:06,390
hardware and so on so the approach the

00:02:04,200 --> 00:02:09,690
ECP is taking is building on top of some

00:02:06,390 --> 00:02:11,310
of four pillars the application

00:02:09,690 --> 00:02:13,680
development the software technology the

00:02:11,310 --> 00:02:16,560
harbor technology and the overall access

00:02:13,680 --> 00:02:18,659
systems salt falls within the category

00:02:16,560 --> 00:02:21,569
of software technology and in particular

00:02:18,659 --> 00:02:28,260
within the programming models a

00:02:21,569 --> 00:02:29,519
subdivision so salt is a project a

00:02:28,260 --> 00:02:32,250
collaboration between five different

00:02:29,519 --> 00:02:34,019
institutions the lead institution is a

00:02:32,250 --> 00:02:35,549
Brookhaven National Laboratory and we

00:02:34,019 --> 00:02:37,980
collaborate with Argonne National Lab

00:02:35,549 --> 00:02:41,280
Oakridge Lawrence Livermore or and also

00:02:37,980 --> 00:02:44,400
with Rice University so the role

00:02:41,280 --> 00:02:46,109
components of salt are basically for

00:02:44,400 --> 00:02:48,299
contributing for research and

00:02:46,109 --> 00:02:50,189
development contributions first we start

00:02:48,299 --> 00:02:52,889
with the application site the the goal

00:02:50,189 --> 00:02:54,780
is to interact a lot with application

00:02:52,889 --> 00:02:58,879
developers scientists understand the

00:02:54,780 --> 00:03:02,159
needs figure out what type of

00:02:58,879 --> 00:03:03,840
application properties and software

00:03:02,159 --> 00:03:06,719
requirements they need and expect from a

00:03:03,840 --> 00:03:10,859
PNP such that we can relay and deliver

00:03:06,719 --> 00:03:12,870
that to our collaborators taking part in

00:03:10,859 --> 00:03:14,519
the open impede language committee and

00:03:12,870 --> 00:03:16,970
the architect architectural review board

00:03:14,519 --> 00:03:19,470
such that these requirements can be

00:03:16,970 --> 00:03:23,010
translated into a specification and

00:03:19,470 --> 00:03:25,859
extension to the language and API and at

00:03:23,010 --> 00:03:27,359
some point this can be implemented both

00:03:25,859 --> 00:03:30,299
are their runtime level and when the

00:03:27,359 --> 00:03:32,639
compiler level in LLVM and regarding the

00:03:30,299 --> 00:03:35,790
the runtime were also devoting efforts

00:03:32,639 --> 00:03:38,099
to enhance the OpenMP runtime I will

00:03:35,790 --> 00:03:41,129
talk briefly more about this later on

00:03:38,099 --> 00:03:44,009
but the basic idea is to enhance the

00:03:41,129 --> 00:03:46,049
runtime by providing a very lightweight

00:03:44,009 --> 00:03:47,849
runtime based on threads and taslitz

00:03:46,049 --> 00:03:50,989
this is basically the world has been

00:03:47,849 --> 00:03:54,449
driven by Argonne National Laboratory

00:03:50,989 --> 00:03:56,159
then we also are voting efforts to

00:03:54,449 --> 00:03:59,280
different compiler optimizations with a

00:03:56,159 --> 00:04:01,199
goal of achieving performance portable

00:03:59,280 --> 00:04:03,269
transformations and some link time

00:04:01,199 --> 00:04:04,799
optimizations this is the work being

00:04:03,269 --> 00:04:07,590
performed by on the side of rice on

00:04:04,799 --> 00:04:09,389
Argonne National Lab and in the center

00:04:07,590 --> 00:04:11,849
of all these efforts we have the Priya

00:04:09,389 --> 00:04:13,560
coordination software leases and consume

00:04:11,849 --> 00:04:17,400
or compile research being performed in

00:04:13,560 --> 00:04:19,169
Brookhaven National Laboratory okay so

00:04:17,400 --> 00:04:23,070
this picture gives a basic idea of how

00:04:19,169 --> 00:04:26,800
we envision how we vision our vision of

00:04:23,070 --> 00:04:28,830
the solve project we have the

00:04:26,800 --> 00:04:31,600
so solve and the upper MP will be

00:04:28,830 --> 00:04:32,979
operating in a larger ecosystem on top

00:04:31,600 --> 00:04:34,770
of operating system now interacting a

00:04:32,979 --> 00:04:37,210
different set of application libraries

00:04:34,770 --> 00:04:39,280
applications and libraries we have the

00:04:37,210 --> 00:04:41,560
enhanced OpenMP runtime which would be

00:04:39,280 --> 00:04:43,300
implemented in overboiled on our kibbutz

00:04:41,560 --> 00:04:45,550
which is the contribution of a Argonne

00:04:43,300 --> 00:04:47,080
National Laboratory and the compiler

00:04:45,550 --> 00:04:49,570
optimization will be implemented in the

00:04:47,080 --> 00:04:50,889
Elven LLVM compiler naturally we have a

00:04:49,570 --> 00:04:53,380
lot of interaction between these two

00:04:50,889 --> 00:04:58,780
layers and then LLVM will also be

00:04:53,380 --> 00:05:01,750
providing extension and accessibility we

00:04:58,780 --> 00:05:04,510
are a clown from n for C and C++ and

00:05:01,750 --> 00:05:08,639
also for Fortran via the flung effort

00:05:04,510 --> 00:05:13,870
the PGI and Excel for a excel compare

00:05:08,639 --> 00:05:14,470
compiling of IBM ok so regarding the

00:05:13,870 --> 00:05:17,350
exascale

00:05:14,470 --> 00:05:19,750
challenges obviously the first our first

00:05:17,350 --> 00:05:21,460
goal is to harness the massive amounts

00:05:19,750 --> 00:05:24,039
of parallelism that we expect in a

00:05:21,460 --> 00:05:26,190
genius and complex systems however this

00:05:24,039 --> 00:05:29,440
also poses a problem because we have

00:05:26,190 --> 00:05:31,750
these leads towards a large exploration

00:05:29,440 --> 00:05:35,110
space in terms of optimization at the

00:05:31,750 --> 00:05:37,030
runtime and compiler level on top of

00:05:35,110 --> 00:05:41,020
that it's this adds complexity to the

00:05:37,030 --> 00:05:42,820
systems and the way we expect and hope

00:05:41,020 --> 00:05:45,610
that the users do not have to deal with

00:05:42,820 --> 00:05:48,220
this type of low-level details for

00:05:45,610 --> 00:05:50,830
instance we expect very deep memory

00:05:48,220 --> 00:05:53,620
hierarchies islands or super nodes or

00:05:50,830 --> 00:05:55,690
islands of Numan domains where the user

00:05:53,620 --> 00:05:57,160
has some notion of the type of

00:05:55,690 --> 00:05:59,800
properties that their application has

00:05:57,160 --> 00:06:01,510
but does not want to deal with the

00:05:59,800 --> 00:06:04,180
low-level details of software or

00:06:01,510 --> 00:06:07,660
hardware hence one one of the goals is

00:06:04,180 --> 00:06:09,610
to provide a high level of productivity

00:06:07,660 --> 00:06:15,550
to a user via a number of abstractions

00:06:09,610 --> 00:06:16,990
via OpenMP so regarding the the

00:06:15,550 --> 00:06:20,620
challenge that we are addressing

00:06:16,990 --> 00:06:24,389
particularly in in the sole project we

00:06:20,620 --> 00:06:24,389
have already collected a number of

00:06:25,050 --> 00:06:29,500
requirements of the users and

00:06:27,099 --> 00:06:32,020
application developers this has been

00:06:29,500 --> 00:06:34,419
have been related to the OpenMP language

00:06:32,020 --> 00:06:36,219
and architecture review board and some

00:06:34,419 --> 00:06:37,900
of these enhancements have been accepted

00:06:36,219 --> 00:06:39,510
into the latest technical report which

00:06:37,900 --> 00:06:44,450
was release it in

00:06:39,510 --> 00:06:48,320
late last week for instance we there's a

00:06:44,450 --> 00:06:50,610
in this technical report we will have

00:06:48,320 --> 00:06:54,450
custom data mappers basically for

00:06:50,610 --> 00:06:57,870
allowing or making the deep copy process

00:06:54,450 --> 00:07:00,030
less painful we will have a also a

00:06:57,870 --> 00:07:01,830
memory management API I will discuss a

00:07:00,030 --> 00:07:03,720
bit more about this but it's basically

00:07:01,830 --> 00:07:06,240
about simplifying the memory management

00:07:03,720 --> 00:07:09,570
in intelligence systems so as to let a

00:07:06,240 --> 00:07:12,210
make usage of some different properties

00:07:09,570 --> 00:07:13,650
of the application then we also expect

00:07:12,210 --> 00:07:16,590
different enhancements at the tasking

00:07:13,650 --> 00:07:19,440
level removing overheads providing

00:07:16,590 --> 00:07:21,990
better better better a scheduling models

00:07:19,440 --> 00:07:24,570
and also for performance portable code

00:07:21,990 --> 00:07:26,760
in the sense of accident targeting

00:07:24,570 --> 00:07:28,230
accelerators as well as removing

00:07:26,760 --> 00:07:33,240
different kinds of our heading nested

00:07:28,230 --> 00:07:35,210
parallelism okay so I mentioned that one

00:07:33,240 --> 00:07:38,070
of the challenges that we have is

00:07:35,210 --> 00:07:39,690
providing a memory management API which

00:07:38,070 --> 00:07:43,080
basically is one one solution for deep

00:07:39,690 --> 00:07:45,150
copy there are other types of issues

00:07:43,080 --> 00:07:46,710
that have been identified by users the

00:07:45,150 --> 00:07:48,930
idea of the memory management is that

00:07:46,710 --> 00:07:51,570
the user has an intuition or

00:07:48,930 --> 00:07:53,370
a good knowledge of the type of

00:07:51,570 --> 00:07:56,550
properties of their applications hence

00:07:53,370 --> 00:07:59,580
by providing this API the user can give

00:07:56,550 --> 00:08:02,610
hints or or prioritize different kinds

00:07:59,580 --> 00:08:06,300
of memory types for instance memories

00:08:02,610 --> 00:08:08,580
with large capacity data that is

00:08:06,300 --> 00:08:10,620
expected to be only read-only if their

00:08:08,580 --> 00:08:13,800
application mainly benefits from from

00:08:10,620 --> 00:08:15,390
high high bandwidth memory or for

00:08:13,800 --> 00:08:17,880
instance if the application should make

00:08:15,390 --> 00:08:22,520
child use different types of scratchpad

00:08:17,880 --> 00:08:24,600
or or shared memory or things like that

00:08:22,520 --> 00:08:27,840
then we're also addressing the issues

00:08:24,600 --> 00:08:30,900
regarding deep copy we have the idea is

00:08:27,840 --> 00:08:33,690
very simple in application developers

00:08:30,900 --> 00:08:35,669
and users usually implement a very

00:08:33,690 --> 00:08:39,270
complex data structures some with

00:08:35,669 --> 00:08:41,400
different kinds of of a stack allocated

00:08:39,270 --> 00:08:43,110
or heap allocated and this data has to

00:08:41,400 --> 00:08:46,190
be transferred and move between the host

00:08:43,110 --> 00:08:49,560
and the device so a DCT

00:08:46,190 --> 00:08:52,670
this kind of work is possible nowadays

00:08:49,560 --> 00:08:54,680
with the open a open P API but with very

00:08:52,670 --> 00:08:56,390
details so the idea is to raise the

00:08:54,680 --> 00:08:59,090
level abstraction such that the user can

00:08:56,390 --> 00:09:01,070
devote less time to the two pointer a

00:08:59,090 --> 00:09:03,320
pointer management and things like that

00:09:01,070 --> 00:09:09,200
under our focus on the algorithm

00:09:03,320 --> 00:09:11,800
translation in today into a target so

00:09:09,200 --> 00:09:11,800
second

00:09:15,150 --> 00:09:21,150
okay so basically think about a data

00:09:18,660 --> 00:09:23,730
structure you know object called class

00:09:21,150 --> 00:09:27,600
with different fields each field how can

00:09:23,730 --> 00:09:30,270
have other objects other methods many

00:09:27,600 --> 00:09:33,240
many cases will have pointers okay and

00:09:30,270 --> 00:09:35,370
these are these memory regions are

00:09:33,240 --> 00:09:37,950
allocated normally by default on the

00:09:35,370 --> 00:09:41,910
host but when you want to interact with

00:09:37,950 --> 00:09:43,500
a accelerator such as such as a GPU all

00:09:41,910 --> 00:09:46,830
these memory regions and the data has to

00:09:43,500 --> 00:09:49,200
be transferred to the accelerator but

00:09:46,830 --> 00:09:51,210
when you have pointers basically then

00:09:49,200 --> 00:09:53,180
you have to jump one pointer to the next

00:09:51,210 --> 00:09:56,490
to the next to the next so this is

00:09:53,180 --> 00:09:58,040
consuming a very complex data structure

00:09:56,490 --> 00:10:00,690
this can be a very painful process

00:09:58,040 --> 00:10:04,050
considering an application code base of

00:10:00,690 --> 00:10:05,730
thousands of lines so we want to raise

00:10:04,050 --> 00:10:09,060
the level of level of abstraction such

00:10:05,730 --> 00:10:11,490
that the user could provide I will

00:10:09,060 --> 00:10:12,060
discuss a merabh is a bit more about

00:10:11,490 --> 00:10:14,310
this later

00:10:12,060 --> 00:10:16,320
but the idea is that user provides small

00:10:14,310 --> 00:10:18,210
routines that give information of the

00:10:16,320 --> 00:10:19,950
data structure and these are plug-in to

00:10:18,210 --> 00:10:25,709
the runtime such as grant invokes those

00:10:19,950 --> 00:10:27,360
routines when they when the the corner

00:10:25,709 --> 00:10:34,589
of the application is uploaded to an

00:10:27,360 --> 00:10:37,529
accelerator okay then we also plans on

00:10:34,589 --> 00:10:41,220
enhancements to the OpenMP runtime this

00:10:37,529 --> 00:10:44,100
is will basically be achieved by the new

00:10:41,220 --> 00:10:46,589
openmp has been developed in Argonne

00:10:44,100 --> 00:10:51,170
National Lab in the bold project unbold

00:10:46,589 --> 00:10:58,620
I think I skipped one yes I'm sorry

00:10:51,170 --> 00:11:00,450
so Boyle stands for basically is an open

00:10:58,620 --> 00:11:03,270
P over lightweight threads it's an

00:11:00,450 --> 00:11:05,490
implementation or extension of the Intel

00:11:03,270 --> 00:11:07,830
implementation it's implemented actually

00:11:05,490 --> 00:11:11,220
in LLVM the idea is to leverage a

00:11:07,830 --> 00:11:13,710
lightweight training framework our

00:11:11,220 --> 00:11:16,250
Gobots so as to minimize overhead

00:11:13,710 --> 00:11:19,230
[Music]

00:11:16,250 --> 00:11:20,940
basically the everything would be

00:11:19,230 --> 00:11:24,120
operating on the user space rather than

00:11:20,940 --> 00:11:27,329
the operating system in this case this

00:11:24,120 --> 00:11:28,790
removes a lot of overhead and this has a

00:11:27,329 --> 00:11:31,230
number of advantages

00:11:28,790 --> 00:11:33,899
some of the advantages is of some of the

00:11:31,230 --> 00:11:36,269
goals of bolt is to provide a better

00:11:33,899 --> 00:11:38,370
support for fine grained parallelism in

00:11:36,269 --> 00:11:40,079
particular for nested pattern religions

00:11:38,370 --> 00:11:43,769
and also for application direct starting

00:11:40,079 --> 00:11:47,970
to leverage different types of tasks by

00:11:43,769 --> 00:11:49,740
a task-based modal's now how does I both

00:11:47,970 --> 00:11:51,540
achieve this extra purpose of

00:11:49,740 --> 00:11:54,029
performance basically since we hope they

00:11:51,540 --> 00:11:55,320
operate on the user space they overhead

00:11:54,029 --> 00:11:58,350
there's not much of an overhead

00:11:55,320 --> 00:12:01,920
associated when a context switching for

00:11:58,350 --> 00:12:07,290
instance this is achieved by reducing

00:12:01,920 --> 00:12:09,690
the the memory associate for storing the

00:12:07,290 --> 00:12:12,930
state of the of the thread such that

00:12:09,690 --> 00:12:15,389
this disk is configured by by the

00:12:12,930 --> 00:12:17,639
runtime via some some settings and this

00:12:15,389 --> 00:12:19,620
achieves a higher performance compared

00:12:17,639 --> 00:12:26,279
to the Intel open MP or GCC

00:12:19,620 --> 00:12:27,990
implementations of open MP then

00:12:26,279 --> 00:12:30,029
regarding the compiler research will so

00:12:27,990 --> 00:12:31,620
have four different directions these are

00:12:30,029 --> 00:12:34,500
devices potentially into three different

00:12:31,620 --> 00:12:37,500
brackets or groups first on the work

00:12:34,500 --> 00:12:39,750
that we are performing on the memory and

00:12:37,500 --> 00:12:42,420
accelerators for instance enhancing and

00:12:39,750 --> 00:12:44,339
UN level leveraging unified memory the

00:12:42,420 --> 00:12:46,529
memory management API that I discussed

00:12:44,339 --> 00:12:49,410
before previously the memory map or the

00:12:46,529 --> 00:12:51,690
deep copy approaches via mapper classes

00:12:49,410 --> 00:12:54,500
and stages and also data layout

00:12:51,690 --> 00:12:57,660
transformations for determining the best

00:12:54,500 --> 00:12:59,250
layout in in structures and arrays of

00:12:57,660 --> 00:13:01,470
structures on structures of arrays

00:12:59,250 --> 00:13:04,680
then we also have research being

00:13:01,470 --> 00:13:08,310
performed in regarding extracting and

00:13:04,680 --> 00:13:11,640
leveraging parallelism here we are we

00:13:08,310 --> 00:13:15,600
plan to use a recent extension provided

00:13:11,640 --> 00:13:19,199
by Intel that enhances the LLVM IR by

00:13:15,600 --> 00:13:22,440
providing explicit ways of describing

00:13:19,199 --> 00:13:24,209
and exposing parallelism naturally over

00:13:22,440 --> 00:13:27,329
of our goals is to enable automatic

00:13:24,209 --> 00:13:31,170
parallelization leverage newly accepted

00:13:27,329 --> 00:13:33,139
constructs in the open P language and

00:13:31,170 --> 00:13:36,569
API for instance the concurrent Klaus

00:13:33,139 --> 00:13:38,810
Klaus and also enable compiling

00:13:36,569 --> 00:13:41,069
automatic task placement and scheduling

00:13:38,810 --> 00:13:41,930
finally regarding cogeneration we

00:13:41,069 --> 00:13:43,790
planned

00:13:41,930 --> 00:13:46,339
enhance and and fine-tune the

00:13:43,790 --> 00:13:48,410
acculturation for different type of of

00:13:46,339 --> 00:13:51,230
accelerators and models such as Cindy

00:13:48,410 --> 00:13:53,960
leveraging the concurrent clause for GPU

00:13:51,230 --> 00:13:56,210
of loading and also for all kinds of

00:13:53,960 --> 00:13:59,500
processor such as about the IBM power8

00:13:56,210 --> 00:14:01,430
nine power eight or nine and Intel K and

00:13:59,500 --> 00:14:03,500
here we also include the world

00:14:01,430 --> 00:14:05,779
performing automatic offloading and also

00:14:03,500 --> 00:14:08,330
Auto generating a OpenMP promise via

00:14:05,779 --> 00:14:13,370
some analysis and transformations

00:14:08,330 --> 00:14:17,029
perform it in the LLVM compiler okay so

00:14:13,370 --> 00:14:18,680
here I will give some few details about

00:14:17,029 --> 00:14:20,690
the work being performed on unified

00:14:18,680 --> 00:14:22,430
memory these regards this is in regard

00:14:20,690 --> 00:14:26,470
on decks and the to the accelerator side

00:14:22,430 --> 00:14:32,560
so the idea is that with the new

00:14:26,470 --> 00:14:35,240
features in NVIDIA GPUs the user can

00:14:32,560 --> 00:14:38,450
they did basically to relief the user

00:14:35,240 --> 00:14:41,750
from explicit data movements between the

00:14:38,450 --> 00:14:44,540
host and the device the idea is that a

00:14:41,750 --> 00:14:47,540
user can allocate data be a specific

00:14:44,540 --> 00:14:50,270
library calls such that the this data is

00:14:47,540 --> 00:14:53,150
accessible both be from the host and

00:14:50,270 --> 00:14:54,890
from the device this case the user

00:14:53,150 --> 00:14:57,730
doesn't have to deal with a copy in copy

00:14:54,890 --> 00:15:00,950
out processing in very complex

00:14:57,730 --> 00:15:03,290
structures or objects and these this

00:15:00,950 --> 00:15:05,150
data movement is basically handled by

00:15:03,290 --> 00:15:08,089
the underlying runtime however there's

00:15:05,150 --> 00:15:10,760
still some work and remains to be done

00:15:08,089 --> 00:15:14,900
for instance not all applications will

00:15:10,760 --> 00:15:17,930
will benefit directly from the same in

00:15:14,900 --> 00:15:20,330
this in the same manner so we are

00:15:17,930 --> 00:15:23,270
developing techniques to determine how

00:15:20,330 --> 00:15:25,459
to how to access and map the data we are

00:15:23,270 --> 00:15:27,860
prefetching pipelining and pinning

00:15:25,459 --> 00:15:30,790
different parts of memory regions onto

00:15:27,860 --> 00:15:35,079
the accelerator to observe us to avoid

00:15:30,790 --> 00:15:35,079
or minimize the memory movement

00:15:36,810 --> 00:15:42,000
another piece a direction that we are

00:15:38,670 --> 00:15:43,740
following is the introduction of OpenMP

00:15:42,000 --> 00:15:46,260
mappers this is for addressing the

00:15:43,740 --> 00:15:49,230
purling of deep copy the basic idea is

00:15:46,260 --> 00:15:52,010
that the user when these are has a

00:15:49,230 --> 00:15:56,360
particular data structure or class and

00:15:52,010 --> 00:16:01,770
the user provides small routines that

00:15:56,360 --> 00:16:03,750
are used to extract information about

00:16:01,770 --> 00:16:06,120
their class this information later is

00:16:03,750 --> 00:16:08,850
this way routines are later used by the

00:16:06,120 --> 00:16:12,180
runtime so as to collect for instance

00:16:08,850 --> 00:16:13,980
the size of the total memory required by

00:16:12,180 --> 00:16:17,700
the by the class or the structure and

00:16:13,980 --> 00:16:19,830
other other mechanisms such as packing

00:16:17,700 --> 00:16:25,560
unpacking and transferring linearizing

00:16:19,830 --> 00:16:28,770
for instance in a in a in a in a list

00:16:25,560 --> 00:16:30,150
and what the idea is to raise the level

00:16:28,770 --> 00:16:34,500
of abstraction such that users don't

00:16:30,150 --> 00:16:36,690
have to deal with low-level details then

00:16:34,500 --> 00:16:39,660
one another direction that we're

00:16:36,690 --> 00:16:42,120
pursuing is the data layout

00:16:39,660 --> 00:16:44,640
transformations this is to optimize for

00:16:42,120 --> 00:16:47,070
instance codes the art base on arrays

00:16:44,640 --> 00:16:50,450
and structures and a mix or interleaving

00:16:47,070 --> 00:16:54,800
of this the idea this will be

00:16:50,450 --> 00:16:54,800
implemented in inside the LLVM compiler

00:16:55,040 --> 00:17:00,120
naturally we the idea that we have one

00:16:57,870 --> 00:17:03,720
piece of code based on arrays for

00:17:00,120 --> 00:17:06,630
instance or structures and the compiler

00:17:03,720 --> 00:17:10,500
is in charge of consuming different

00:17:06,630 --> 00:17:13,230
types of combinations of arrays and

00:17:10,500 --> 00:17:15,089
structures or different levels of course

00:17:13,230 --> 00:17:17,610
this is this depends a lot on the target

00:17:15,089 --> 00:17:20,300
that we are the target machine for

00:17:17,610 --> 00:17:22,770
instance will vary between GPUs and GPUs

00:17:20,300 --> 00:17:25,140
so this way basically we will integrate

00:17:22,770 --> 00:17:28,790
analytical models determine the the

00:17:25,140 --> 00:17:31,470
benefits on the different architectures

00:17:28,790 --> 00:17:33,690
okay so almost to conclude I want to

00:17:31,470 --> 00:17:36,120
inform that we have the solve website

00:17:33,690 --> 00:17:38,160
hosted in Brookhaven National Lab here

00:17:36,120 --> 00:17:40,650
we give brief information about the

00:17:38,160 --> 00:17:42,300
project the milestones the roadmap how

00:17:40,650 --> 00:17:45,679
we are evolving the different

00:17:42,300 --> 00:17:47,700
institutions and organization also

00:17:45,679 --> 00:17:48,440
software releases with the different

00:17:47,700 --> 00:17:52,399
runtimes with

00:17:48,440 --> 00:17:55,789
LVM enhance it a compiler the default an

00:17:52,399 --> 00:17:57,649
arquebus runtime you have any question

00:17:55,789 --> 00:18:03,080
or we like just to use the software you

00:17:57,649 --> 00:18:05,570
can access it on this on this link so to

00:18:03,080 --> 00:18:07,279
conclude access Kelly Texas clear is

00:18:05,570 --> 00:18:11,210
just around the corner we have a number

00:18:07,279 --> 00:18:15,259
of grand challenges to address salts

00:18:11,210 --> 00:18:18,350
goal is basically to enhance a OpenMP VI

00:18:15,259 --> 00:18:20,529
LLVM so as to achieve high productivity

00:18:18,350 --> 00:18:22,700
and high performance portable code and

00:18:20,529 --> 00:18:25,009
we have a number of efforts being

00:18:22,700 --> 00:18:26,299
performed at the specification level in

00:18:25,009 --> 00:18:28,570
the language level in the runtime

00:18:26,299 --> 00:18:30,169
application side and the compiler side

00:18:28,570 --> 00:18:32,509
so I'd like to thank all the

00:18:30,169 --> 00:18:34,279
collaborators and team members of this

00:18:32,509 --> 00:18:36,139
project from all the different

00:18:34,279 --> 00:18:38,210
institutions and of course the

00:18:36,139 --> 00:18:41,139
Department of Energy for funding this

00:18:38,210 --> 00:18:41,139

YouTube URL: https://www.youtube.com/watch?v=K5JPO3IVUB8


