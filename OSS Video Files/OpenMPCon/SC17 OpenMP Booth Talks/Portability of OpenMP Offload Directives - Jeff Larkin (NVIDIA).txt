Title: Portability of OpenMP Offload Directives - Jeff Larkin (NVIDIA)
Publication date: 2017-11-20
Playlist: SC17 OpenMP Booth Talks
Description: 
	SuperComputing '17 Denver - OpenMP Booth Talks - November 14, 2017
PDF Slides at http://www.openmp.org/resources/openmp-presentations/sc17-booth-talks/
Captions: 
	00:00:01,129 --> 00:00:05,730
all right good morning those of you

00:00:04,470 --> 00:00:07,950
walking by I want to come into the open

00:00:05,730 --> 00:00:11,099
MP booth we have a short talk here my

00:00:07,950 --> 00:00:14,219
name is Jeff Larkin I am invidious

00:00:11,099 --> 00:00:16,740
representative to open MP both at the

00:00:14,219 --> 00:00:19,289
ARB level at the technical level and I

00:00:16,740 --> 00:00:21,180
talked about the portability of the

00:00:19,289 --> 00:00:23,340
offloading directives that are part of

00:00:21,180 --> 00:00:25,859
open MP basically being in a video I

00:00:23,340 --> 00:00:29,279
have a vested interest in the target

00:00:25,859 --> 00:00:30,900
offloading directives and but our

00:00:29,279 --> 00:00:32,700
customers of course want these these

00:00:30,900 --> 00:00:36,510
directives to be portable architectures

00:00:32,700 --> 00:00:37,920
for many uppers they choose open MP

00:00:36,510 --> 00:00:39,840
because they want to develop a single

00:00:37,920 --> 00:00:43,590
source code that and be able to run it

00:00:39,840 --> 00:00:46,170
anywhere animals and in video we have a

00:00:43,590 --> 00:00:47,969
very distinctive architecture that's

00:00:46,170 --> 00:00:49,110
that's quite different from a lot of the

00:00:47,969 --> 00:00:50,760
more traditional shared memory

00:00:49,110 --> 00:00:53,129
architectures that openmp was designed

00:00:50,760 --> 00:00:55,440
for though what I want to do is take a

00:00:53,129 --> 00:00:58,260
look of the current status of

00:00:55,440 --> 00:01:00,000
portability of the the target upload

00:00:58,260 --> 00:01:02,809
directives and I look at that it would

00:01:00,000 --> 00:01:06,689
four measures first there's a variety of

00:01:02,809 --> 00:01:08,610
compilers available for our GPUs and how

00:01:06,689 --> 00:01:11,880
well can I take a code across those two

00:01:08,610 --> 00:01:13,409
different compilers but also if I take

00:01:11,880 --> 00:01:16,470
those those directives and I write them

00:01:13,409 --> 00:01:18,060
for the GPU can I fall back to the post

00:01:16,470 --> 00:01:20,280
and expect a good performance so that

00:01:18,060 --> 00:01:23,759
would be portability between something

00:01:20,280 --> 00:01:25,350
like a GPU and a traditional view so

00:01:23,759 --> 00:01:27,090
actually compared results with six

00:01:25,350 --> 00:01:29,549
different compilers now only four of

00:01:27,090 --> 00:01:31,619
them support NVIDIA GPUs right now so

00:01:29,549 --> 00:01:33,960
I'll include four of them in that in the

00:01:31,619 --> 00:01:38,520
initial study and then also all six of

00:01:33,960 --> 00:01:40,020
them on this post fall back so you know

00:01:38,520 --> 00:01:44,070
the goal of this study is first to look

00:01:40,020 --> 00:01:47,040
at how portable are between different

00:01:44,070 --> 00:01:48,840
compilers on the same platform and so

00:01:47,040 --> 00:01:51,030
our metric there will be of course just

00:01:48,840 --> 00:01:53,820
the raw execution time but also to

00:01:51,030 --> 00:01:57,000
quantify the compilers ability to be

00:01:53,820 --> 00:01:59,460
portable back to a multi-core CPU and so

00:01:57,000 --> 00:02:01,259
in other words if I write GPU code how

00:01:59,460 --> 00:02:03,000
well can I expect it to run on CPU as

00:02:01,259 --> 00:02:04,320
well and so for that metric what I've

00:02:03,000 --> 00:02:06,540
done is that I've taken what I would

00:02:04,320 --> 00:02:08,750
call a native open MP which is you know

00:02:06,540 --> 00:02:11,510
traditional O&P parallel form

00:02:08,750 --> 00:02:14,290
and compared it to a few different

00:02:11,510 --> 00:02:16,520
methods of writing the code for a GPU so

00:02:14,290 --> 00:02:21,350
falling forcing that to fall back to the

00:02:16,520 --> 00:02:23,930
to the host so this is more for record

00:02:21,350 --> 00:02:26,210
of the compiler versions I used so I'll

00:02:23,930 --> 00:02:28,700
go through only quickly I will mention

00:02:26,210 --> 00:02:29,800
that there are newer versions of a

00:02:28,700 --> 00:02:32,030
couple of these compilers and

00:02:29,800 --> 00:02:34,250
unfortunately wasn't able to get data

00:02:32,030 --> 00:02:38,030
with the newer compilers to to compare

00:02:34,250 --> 00:02:39,920
against so I chose a fairly simple

00:02:38,030 --> 00:02:42,170
benchmark it's a typical stencil

00:02:39,920 --> 00:02:45,050
benchmark that we've used for a lot of a

00:02:42,170 --> 00:02:46,610
lot of different codes it's a week with

00:02:45,050 --> 00:02:48,590
College Jacobi iteration so basically

00:02:46,610 --> 00:02:51,260
I'm solving for the laplacian equation

00:02:48,590 --> 00:02:53,750
if I have this red point in the middle

00:02:51,260 --> 00:02:55,100
and I want to figure out how much what

00:02:53,750 --> 00:02:56,750
its temperature would be in a future

00:02:55,100 --> 00:02:58,150
iteration I would just average my

00:02:56,750 --> 00:03:00,890
neighbors and that would kind of tell me

00:02:58,150 --> 00:03:06,590
what my temperature would become in the

00:03:00,890 --> 00:03:09,290
future so what I've done is I've skipped

00:03:06,590 --> 00:03:11,060
over oh thank you I've skipped over a

00:03:09,290 --> 00:03:12,290
lot of the initial steps of porting this

00:03:11,060 --> 00:03:14,870
code and I'm going to jump straight to

00:03:12,290 --> 00:03:17,000
the to a GPU enabled code using the

00:03:14,870 --> 00:03:18,470
teams and distribute directives now if

00:03:17,000 --> 00:03:20,209
you're not familiar with these directors

00:03:18,470 --> 00:03:21,860
first the teams directive is a way of

00:03:20,209 --> 00:03:23,660
generating lots of coarse grained

00:03:21,860 --> 00:03:26,810
parallelism which is what we would

00:03:23,660 --> 00:03:28,519
expect on a GPU so on a if you're

00:03:26,810 --> 00:03:30,610
familiar with CUDA at all these would be

00:03:28,519 --> 00:03:33,769
the the coarse grained thread blocks on

00:03:30,610 --> 00:03:36,170
OpenMP we would call them teams and so

00:03:33,769 --> 00:03:38,360
the teams directive spawns up one or

00:03:36,170 --> 00:03:40,190
more of these these thread teams and it

00:03:38,360 --> 00:03:42,650
begins execution on all of the master

00:03:40,190 --> 00:03:45,560
threads now that's not necessarily

00:03:42,650 --> 00:03:47,150
useful by itself but paired with the

00:03:45,560 --> 00:03:50,239
distribute directive then we can begin

00:03:47,150 --> 00:03:53,480
to spread our work out across this this

00:03:50,239 --> 00:03:55,610
parallelism now I still need to use the

00:03:53,480 --> 00:03:57,920
traditional parallel and loop directives

00:03:55,610 --> 00:03:59,930
in order to then activate these threads

00:03:57,920 --> 00:04:01,190
within the thread teams and I'm gonna

00:03:59,930 --> 00:04:03,709
assume that people are familiar with

00:04:01,190 --> 00:04:05,959
those directives but the thing to

00:04:03,709 --> 00:04:07,970
remember with with teams this is very

00:04:05,959 --> 00:04:10,190
coarse-grained so there's no guarantee

00:04:07,970 --> 00:04:11,810
of order among the teams there's no

00:04:10,190 --> 00:04:13,940
guarantee that any two teams they may

00:04:11,810 --> 00:04:15,290
run concurrently they may not and you

00:04:13,940 --> 00:04:17,390
can't synchronize between them so that's

00:04:15,290 --> 00:04:19,430
what enables us to make very scalable

00:04:17,390 --> 00:04:20,790
parallelism which works well on a on a

00:04:19,430 --> 00:04:23,010
GPU

00:04:20,790 --> 00:04:25,140
so this is that the simple code that I

00:04:23,010 --> 00:04:27,000
used you can see here I do a target

00:04:25,140 --> 00:04:28,230
teams distribute so that gives me the

00:04:27,000 --> 00:04:30,510
coarse grained parallelism here

00:04:28,230 --> 00:04:34,860
I do a parallel for that activates the

00:04:30,510 --> 00:04:36,720
threads within those those teams but

00:04:34,860 --> 00:04:39,030
it's important to note that actually I'm

00:04:36,720 --> 00:04:40,800
only parallelizing the outermost loop

00:04:39,030 --> 00:04:43,980
here and that'll be significant in in a

00:04:40,800 --> 00:04:44,850
moment and so here you can see I've

00:04:43,980 --> 00:04:48,000
compared a whole bunch of different

00:04:44,850 --> 00:04:49,350
compilers GCC it turns out it's a bit of

00:04:48,000 --> 00:04:52,770
an outlier so I'm actually going to drop

00:04:49,350 --> 00:04:55,050
it off of here and focus on these so but

00:04:52,770 --> 00:04:57,660
the reason GCC is an outlier is it if

00:04:55,050 --> 00:05:00,120
you do not provide a sim D directive the

00:04:57,660 --> 00:05:02,070
performance is terrible the other

00:05:00,120 --> 00:05:03,960
compilers actually do more

00:05:02,070 --> 00:05:07,980
parallelization inside without that

00:05:03,960 --> 00:05:10,290
directive so if I just just focus on

00:05:07,980 --> 00:05:12,420
that then you see we we get more in line

00:05:10,290 --> 00:05:14,760
performance the green part of the bars

00:05:12,420 --> 00:05:16,590
here is the kernel time so this is the

00:05:14,760 --> 00:05:19,650
time actually spent and measured on the

00:05:16,590 --> 00:05:21,810
GPU there's a very thin blue lines here

00:05:19,650 --> 00:05:23,490
that mark the data transfer as you can

00:05:21,810 --> 00:05:25,290
see that there's there's only nominal

00:05:23,490 --> 00:05:27,540
data transfer and this gray is

00:05:25,290 --> 00:05:30,180
everything else and it turns out that

00:05:27,540 --> 00:05:31,710
with the GCC compiler there's quite a

00:05:30,180 --> 00:05:33,390
bit of extra overhead that still needs

00:05:31,710 --> 00:05:34,560
to be optimized within the compiler and

00:05:33,390 --> 00:05:37,590
that's just because it's it's an

00:05:34,560 --> 00:05:39,330
immature compiler something to notice

00:05:37,590 --> 00:05:40,980
that the Karaka pilar did exceptionally

00:05:39,330 --> 00:05:42,900
well compared to the other compilers and

00:05:40,980 --> 00:05:44,760
it turns out that their compiler when

00:05:42,900 --> 00:05:46,680
they see targeting distribute parallel

00:05:44,760 --> 00:05:49,920
for or even just target teams distribute

00:05:46,680 --> 00:05:51,420
that is a hint to them to do their

00:05:49,920 --> 00:05:53,010
automatic parallelization so they

00:05:51,420 --> 00:05:56,360
actually do a really nice job of

00:05:53,010 --> 00:06:01,230
automatically paralyzing both loops

00:05:56,360 --> 00:06:02,820
clang and and excel and GCC don't do

00:06:01,230 --> 00:06:05,250
that automatic parallelization so you

00:06:02,820 --> 00:06:06,720
can see their performance is notably

00:06:05,250 --> 00:06:10,520
worse than Cray in this in this

00:06:06,720 --> 00:06:12,800
situation so

00:06:10,520 --> 00:06:14,360
I really want though to increase how

00:06:12,800 --> 00:06:16,520
much parallelism I'm exposing so I

00:06:14,360 --> 00:06:19,280
mentioned I was only paralyzing that

00:06:16,520 --> 00:06:21,620
outer loop I was both making my threat

00:06:19,280 --> 00:06:24,530
teams and my my threat to my work

00:06:21,620 --> 00:06:26,150
sharing was done on one loop so there's

00:06:24,530 --> 00:06:28,460
two different ways that I can get more

00:06:26,150 --> 00:06:30,319
parallelism out of this loop nest I can

00:06:28,460 --> 00:06:31,849
collapse them together and so that's

00:06:30,319 --> 00:06:34,610
using the collapse Clause down here so

00:06:31,849 --> 00:06:36,379
take the next two loops and fold them

00:06:34,610 --> 00:06:39,080
into one longer loop and that gives me a

00:06:36,379 --> 00:06:40,879
lot more iterations to paralyze across

00:06:39,080 --> 00:06:43,330
both across the threat teams and across

00:06:40,879 --> 00:06:46,580
the the threads and even the Cymbeline's

00:06:43,330 --> 00:06:48,979
or I can split my teams distribute and

00:06:46,580 --> 00:06:50,539
my my parallel four so get my coarse

00:06:48,979 --> 00:06:52,099
grained parallelism on the outside and

00:06:50,539 --> 00:06:55,009
my finer grain parallelism on the inside

00:06:52,099 --> 00:06:56,870
and I explored both of these so here's

00:06:55,009 --> 00:06:58,639
the first one where all I did was add

00:06:56,870 --> 00:07:01,340
the collapse directive so again these

00:06:58,639 --> 00:07:03,080
two loops will become one and then I'll

00:07:01,340 --> 00:07:06,530
paralyze across all of those loop

00:07:03,080 --> 00:07:07,639
iterations now this particular operation

00:07:06,530 --> 00:07:09,440
that's being done here is a stencil

00:07:07,639 --> 00:07:13,009
operation as I said there's a lot of

00:07:09,440 --> 00:07:15,110
locality here so in actuality I may kind

00:07:13,009 --> 00:07:17,810
of be blowing out some of my my locality

00:07:15,110 --> 00:07:19,159
by doing this but you'll see here in a

00:07:17,810 --> 00:07:22,819
moment what the what the performance

00:07:19,159 --> 00:07:24,590
does and now you can see again GCC is a

00:07:22,819 --> 00:07:26,539
bit of an outlier here but clang and

00:07:24,590 --> 00:07:29,360
Excel are now closer in line with what

00:07:26,539 --> 00:07:31,400
Kray did so Kray was clearly already

00:07:29,360 --> 00:07:34,639
doing some form of parallelization

00:07:31,400 --> 00:07:35,630
across both loops and by collapsing it

00:07:34,639 --> 00:07:37,940
together I've enabled the other

00:07:35,630 --> 00:07:40,039
compilers to do the same there's still a

00:07:37,940 --> 00:07:42,560
lot of overhead here as I said with GCC

00:07:40,039 --> 00:07:44,210
that needs to be optimized away it turns

00:07:42,560 --> 00:07:47,389
out that they're doing some scalar

00:07:44,210 --> 00:07:49,219
copies that the other compilers aren't

00:07:47,389 --> 00:07:51,139
doing so clear they aren't necessary and

00:07:49,219 --> 00:07:53,630
I need to work with them to try to

00:07:51,139 --> 00:07:56,120
figure that out so here is kind of the

00:07:53,630 --> 00:07:58,789
the the other ones out the times are

00:07:56,120 --> 00:08:01,039
actually wrong there that this is

00:07:58,789 --> 00:08:03,009
actually I think I'll have 3.7 seconds

00:08:01,039 --> 00:08:05,360
there so that's actually wrong and

00:08:03,009 --> 00:08:07,279
here's what happens if I split them so

00:08:05,360 --> 00:08:09,710
again coarse grain on the outside fine

00:08:07,279 --> 00:08:11,120
grain on the inside in some cases I used

00:08:09,710 --> 00:08:12,800
Cindy in some cases I didn't because the

00:08:11,120 --> 00:08:14,210
different compilers some of them want

00:08:12,800 --> 00:08:17,030
Cindy and some of them do not so I'm not

00:08:14,210 --> 00:08:20,250
showing this into here

00:08:17,030 --> 00:08:21,810
as has been a theme on the the GCC

00:08:20,250 --> 00:08:23,970
implementation which by the way this is

00:08:21,810 --> 00:08:25,470
a very immature Inlet implementational

00:08:23,970 --> 00:08:28,139
there's really only one guy that's been

00:08:25,470 --> 00:08:30,389
working on putting together the the

00:08:28,139 --> 00:08:33,019
openmp front-end and the GPU back-end so

00:08:30,389 --> 00:08:35,610
this I know will improve a lot over time

00:08:33,019 --> 00:08:37,950
you can see if we take that out to get a

00:08:35,610 --> 00:08:40,800
better idea here again both clang and

00:08:37,950 --> 00:08:44,010
Cray both do pretty well here

00:08:40,800 --> 00:08:45,950
GCC with the sim D and Excel didn't

00:08:44,010 --> 00:08:49,110
really like this parallelization scheme

00:08:45,950 --> 00:08:52,620
and so this turns out to not be the best

00:08:49,110 --> 00:08:55,380
scheme for those now I mentioned that

00:08:52,620 --> 00:08:57,029
was a measure of how well these GPU

00:08:55,380 --> 00:09:00,720
compilers compare to each other using

00:08:57,029 --> 00:09:02,610
the same code and it seems like if I

00:09:00,720 --> 00:09:04,620
take my code and if I'm able to collapse

00:09:02,610 --> 00:09:06,660
my loops the compilers actually are

00:09:04,620 --> 00:09:09,269
fairly comparable in performance so we

00:09:06,660 --> 00:09:11,910
can get pretty portable code doing that

00:09:09,269 --> 00:09:14,970
I've found that aggressively collapsing

00:09:11,910 --> 00:09:18,089
tends to produce the most uniform

00:09:14,970 --> 00:09:19,620
results on a variety of apps now what

00:09:18,089 --> 00:09:22,140
about if we want to follow these back to

00:09:19,620 --> 00:09:24,779
the host so as I said a lot of people

00:09:22,140 --> 00:09:28,770
want to write for the GPU and be able to

00:09:24,779 --> 00:09:31,140
run as well on the CPU so what if I take

00:09:28,770 --> 00:09:33,390
this and I tell the compiler I had this

00:09:31,140 --> 00:09:36,329
if clause so I'm gonna do a runtime

00:09:33,390 --> 00:09:38,700
check to see should this run on the GPU

00:09:36,329 --> 00:09:40,560
or not and by doing this I have forced

00:09:38,700 --> 00:09:41,880
the compiler to build both versions of

00:09:40,560 --> 00:09:44,579
the code both the GPU and the CPU

00:09:41,880 --> 00:09:46,589
version of the code and then at runtime

00:09:44,579 --> 00:09:49,500
it'll choose the right one and notice

00:09:46,589 --> 00:09:51,750
here I can actually just just say this

00:09:49,500 --> 00:09:53,160
if applies to the target so teams

00:09:51,750 --> 00:09:57,240
distribute parallel for are still in

00:09:53,160 --> 00:09:58,920
play now I did encounter as I was

00:09:57,240 --> 00:10:01,350
working with the PGI compiler they have

00:09:58,920 --> 00:10:02,820
not yet implemented this syntax within

00:10:01,350 --> 00:10:07,949
if I had to drop that down to this a

00:10:02,820 --> 00:10:10,350
plaintiff so to remind you what my

00:10:07,949 --> 00:10:12,839
measurement here is native OpenMP is

00:10:10,350 --> 00:10:15,240
just OMP parallel for it may or may not

00:10:12,839 --> 00:10:18,480
include Cindy I picked the best for each

00:10:15,240 --> 00:10:22,019
compiler post fallback is taking that

00:10:18,480 --> 00:10:25,140
and and using the huge GPU fly executive

00:10:22,019 --> 00:10:27,060
false so 100% means we have they're

00:10:25,140 --> 00:10:29,459
performing equally well if I say 50%

00:10:27,060 --> 00:10:32,730
that means that the hosts fallback takes

00:10:29,459 --> 00:10:35,819
twice as long as the native and here is

00:10:32,730 --> 00:10:37,829
the six compilers I tried climbed it

00:10:35,819 --> 00:10:38,279
fairly well so that unfortunately

00:10:37,829 --> 00:10:40,139
collapse

00:10:38,279 --> 00:10:42,449
doesn't build and run and actually yeah

00:10:40,139 --> 00:10:45,540
I believe if I if I recall correctly

00:10:42,449 --> 00:10:47,040
that crashes but does build decide to

00:10:45,540 --> 00:10:49,379
leave a blank here and the same with the

00:10:47,040 --> 00:10:51,149
Excel so I'm working with IBM to figure

00:10:49,379 --> 00:10:52,949
that out and it may already be fixed in

00:10:51,149 --> 00:10:55,429
a newer compiler they are able to get

00:10:52,949 --> 00:10:58,589
about 80% of the the full performance

00:10:55,429 --> 00:11:02,160
Craig does quite poorly and the reason

00:10:58,589 --> 00:11:04,709
for that is when crises a target teams

00:11:02,160 --> 00:11:07,319
distribute parallel for what they do is

00:11:04,709 --> 00:11:08,939
they actually fix it at one thread

00:11:07,319 --> 00:11:11,249
within each thread team and then they

00:11:08,939 --> 00:11:13,980
use simony parallelism on the GPU so

00:11:11,249 --> 00:11:15,540
that obviously does not translate well

00:11:13,980 --> 00:11:17,519
to the CPU because they're only using

00:11:15,540 --> 00:11:19,079
one thread on one core and they're

00:11:17,519 --> 00:11:20,610
getting the Sindhi paralyzation there so

00:11:19,079 --> 00:11:23,220
they're aware of this they just have no

00:11:20,610 --> 00:11:25,439
haven't had any motivation to really to

00:11:23,220 --> 00:11:29,490
change that given their targets they a

00:11:25,439 --> 00:11:31,309
sport GCC again does quite poorly I

00:11:29,490 --> 00:11:33,839
think they're similarly running

00:11:31,309 --> 00:11:35,009
sequentially and not vectorizing which

00:11:33,839 --> 00:11:37,949
would account for the performance

00:11:35,009 --> 00:11:40,199
difference here Excel does quite well

00:11:37,949 --> 00:11:41,519
you can see anywhere from 80 to I think

00:11:40,199 --> 00:11:44,959
this was somewhere somewhere upwards of

00:11:41,519 --> 00:11:47,790
95% so actually that's quite well

00:11:44,959 --> 00:11:49,860
compared to the other native code so

00:11:47,790 --> 00:11:51,629
Intel and PGI neither of these actually

00:11:49,860 --> 00:11:53,939
support NVIDIA GPUs but I could actually

00:11:51,629 --> 00:11:57,540
do the same experiment since I can build

00:11:53,939 --> 00:11:59,339
for a multi-core target and you can see

00:11:57,540 --> 00:12:00,569
that even just doing the teams

00:11:59,339 --> 00:12:03,540
distribute parallel for the Intel

00:12:00,569 --> 00:12:05,970
compiler does quite good and and the

00:12:03,540 --> 00:12:09,089
collapse does nearly as well I believe

00:12:05,970 --> 00:12:10,470
this was actually like 101 percent so it

00:12:09,089 --> 00:12:13,589
was so close that it was in the noise

00:12:10,470 --> 00:12:15,240
that it was actually slightly faster you

00:12:13,589 --> 00:12:16,980
can see that the split where I have the

00:12:15,240 --> 00:12:18,660
teams on the outside and the parallel

00:12:16,980 --> 00:12:20,959
form the inside it didn't do as well and

00:12:18,660 --> 00:12:22,759
I'd say that's just because they're now

00:12:20,959 --> 00:12:24,420
sequential izing on the outside

00:12:22,759 --> 00:12:25,410
paralyzing on the inside and the

00:12:24,420 --> 00:12:28,619
probably not getting particularly good

00:12:25,410 --> 00:12:30,540
cache reuse in that sense and the PGI

00:12:28,619 --> 00:12:32,999
compiler you can see does very well for

00:12:30,540 --> 00:12:35,549
teams distribute and with the collapse

00:12:32,999 --> 00:12:37,649
and not as well with the split and so

00:12:35,549 --> 00:12:41,800
similarly very close to hundred percent

00:12:37,649 --> 00:12:45,650
and this is using 17 1710

00:12:41,800 --> 00:12:47,270
so what am i takeaways so I asked two

00:12:45,650 --> 00:12:48,950
questions at the beginning of this will

00:12:47,270 --> 00:12:51,710
my target code be portable between

00:12:48,950 --> 00:12:54,560
compilers and it actually for the most

00:12:51,710 --> 00:12:57,890
part did fairly well the aggressively

00:12:54,560 --> 00:13:01,070
collapsed code did particularly well but

00:12:57,890 --> 00:13:03,380
still depending on the maturity you may

00:13:01,070 --> 00:13:05,600
get different results and I would say

00:13:03,380 --> 00:13:08,180
that the the requirement of writing OMP

00:13:05,600 --> 00:13:09,890
sim be some of the compilers demand it

00:13:08,180 --> 00:13:11,780
some the compilers don't want it and

00:13:09,890 --> 00:13:13,040
that's a little bit inconsistent at this

00:13:11,780 --> 00:13:15,380
point and hopefully that'll get cleared

00:13:13,040 --> 00:13:17,270
up can the target code be portable to

00:13:15,380 --> 00:13:20,150
the host and this is really compiler

00:13:17,270 --> 00:13:21,890
dependent so Intel Excel and I should

00:13:20,150 --> 00:13:22,430
have updated this with the latest data

00:13:21,890 --> 00:13:25,360
in PGI

00:13:22,430 --> 00:13:28,490
all three of them do quite well on this

00:13:25,360 --> 00:13:30,110
playing does okay it hits about 80% and

00:13:28,490 --> 00:13:33,140
for a lot of users that's close enough

00:13:30,110 --> 00:13:35,150
and then GCC and Craig did poorly so for

00:13:33,140 --> 00:13:36,830
a future work I'd really want to revisit

00:13:35,150 --> 00:13:38,270
this as as the compilers become more

00:13:36,830 --> 00:13:41,750
mature and I expect that a lot of these

00:13:38,270 --> 00:13:43,130
results will change so with that thank

00:13:41,750 --> 00:13:47,320
you for coming out are there any

00:13:43,130 --> 00:13:47,320
questions for me yeah

00:13:47,579 --> 00:13:54,370
so I don't recall so the question was if

00:13:51,790 --> 00:13:56,800
you leave Cindy there what happens and

00:13:54,370 --> 00:13:59,170
it's in most of the compilers they

00:13:56,800 --> 00:14:01,360
either respects MD or they ignore Cindy

00:13:59,170 --> 00:14:02,740
I did encounter one case I don't

00:14:01,360 --> 00:14:04,959
remember which parlor it was that it

00:14:02,740 --> 00:14:07,420
just balked it did not want Cindy there

00:14:04,959 --> 00:14:09,639
at all so unfortunately what I headed up

00:14:07,420 --> 00:14:14,439
having to do is is introducing if deaf

00:14:09,639 --> 00:14:19,779
and I built in you if GCC turned on

00:14:14,439 --> 00:14:21,339
Cindy otherwise turn it off and but for

00:14:19,779 --> 00:14:23,319
the most part if you have it there

00:14:21,339 --> 00:14:24,759
most of the compilers well will in the

00:14:23,319 --> 00:14:29,589
least ignore it and in many cases

00:14:24,759 --> 00:14:31,860
respect it so alright thank you for

00:14:29,589 --> 00:14:31,860

YouTube URL: https://www.youtube.com/watch?v=vut6fiX_GQw


