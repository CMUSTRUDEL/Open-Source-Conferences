Title: How To Befriend NUMA - Ruud van der Pas (Oracle)
Publication date: 2018-11-17
Playlist: SC18 OpenMP Booth Talks
Description: 
	SC18 OpenMP Booth Talk - November 13, 2018, Dallas TX
https://www.openmp.org/wp-content/uploads/SC18-BoothTalks-vanderPas.pdf
Captions: 
	00:00:00,030 --> 00:00:04,440
thank you thank you all for being here

00:00:02,639 --> 00:00:06,899
my name is Ruth Vonda Potts

00:00:04,440 --> 00:00:09,300
I work in the Oracle engineering

00:00:06,899 --> 00:00:12,480
organization as one of the performance

00:00:09,300 --> 00:00:17,330
people today I primarily want to talk

00:00:12,480 --> 00:00:17,330
about how to optimize for an uma system

00:00:18,560 --> 00:00:25,650
before we do that I need to show this

00:00:23,220 --> 00:00:26,039
safe harbor statement I'll do that

00:00:25,650 --> 00:00:28,260
quickly

00:00:26,039 --> 00:00:32,759
you'll be recorded and people can stare

00:00:28,260 --> 00:00:34,559
at this as long as they like I want to

00:00:32,759 --> 00:00:37,649
say a few words about Oracle Linux I'll

00:00:34,559 --> 00:00:39,360
keep that very short but I think too

00:00:37,649 --> 00:00:42,180
many people are not aware of it and it's

00:00:39,360 --> 00:00:46,070
really cool then I'll talk about what a

00:00:42,180 --> 00:00:49,050
general modern system looks like today

00:00:46,070 --> 00:00:52,110
then I'm going to talk more about Numa

00:00:49,050 --> 00:00:55,110
and data placement open Appy support for

00:00:52,110 --> 00:01:00,149
that and finally performance tuning

00:00:55,110 --> 00:01:01,590
example like I said well this is a very

00:01:00,149 --> 00:01:02,820
short I'll keep it short it'll be

00:01:01,590 --> 00:01:05,880
recorded and if you want more

00:01:02,820 --> 00:01:08,930
information it's all available we do

00:01:05,880 --> 00:01:11,010
have a very nice Oracle Oracle Linux

00:01:08,930 --> 00:01:12,890
implementation it's being around there

00:01:11,010 --> 00:01:15,330
for quite a long time it's 11 years

00:01:12,890 --> 00:01:16,890
since we started doing it and what I'd

00:01:15,330 --> 00:01:20,939
like to point out it's fully compatible

00:01:16,890 --> 00:01:23,009
with Red Hat and it's a fully supported

00:01:20,939 --> 00:01:26,759
product it's not some grassroots effort

00:01:23,009 --> 00:01:28,740
it's a real product and so we we have

00:01:26,759 --> 00:01:32,640
support in place we have development in

00:01:28,740 --> 00:01:36,000
place everything that you would kind of

00:01:32,640 --> 00:01:37,740
expect from a company like Oracle what

00:01:36,000 --> 00:01:39,570
I'd like to mention because few people

00:01:37,740 --> 00:01:42,240
know it as well we actively contribute

00:01:39,570 --> 00:01:43,619
back to the community we do a lot of our

00:01:42,240 --> 00:01:45,299
own development we have a kernel

00:01:43,619 --> 00:01:46,979
engineering team we have different

00:01:45,299 --> 00:01:49,140
engineering teams said I'm on the

00:01:46,979 --> 00:01:52,049
performance performance side so if you

00:01:49,140 --> 00:01:56,009
find something we'll we'll fix it and

00:01:52,049 --> 00:01:58,380
we'll put it back if you're like like to

00:01:56,009 --> 00:02:01,770
give it a try is a download side again

00:01:58,380 --> 00:02:03,869
these are the binaries can just download

00:02:01,770 --> 00:02:05,969
them and use them if you'd like to get

00:02:03,869 --> 00:02:08,640
the source it's fully open source so we

00:02:05,969 --> 00:02:10,259
don't get hub and very few people know

00:02:08,640 --> 00:02:12,840
that I think you can just get our

00:02:10,259 --> 00:02:13,150
sources and see how things work under

00:02:12,840 --> 00:02:15,700
the hood

00:02:13,150 --> 00:02:17,470
oh yeah that's a little bit about work

00:02:15,700 --> 00:02:19,900
on Linux I'll be more than happy to talk

00:02:17,470 --> 00:02:23,950
more about it after this talk I'll be in

00:02:19,900 --> 00:02:27,099
the booth for another two hours I want

00:02:23,950 --> 00:02:30,220
to talk about Numa what does a typical

00:02:27,099 --> 00:02:34,599
what I call generic Numa system looks

00:02:30,220 --> 00:02:37,060
like today and here's a and I started

00:02:34,599 --> 00:02:40,060
moving away from the woods socket as

00:02:37,060 --> 00:02:44,560
I'll explain later here we have a system

00:02:40,060 --> 00:02:47,379
with four neumann nodes and each node is

00:02:44,560 --> 00:02:51,160
is kind of identical each node has a

00:02:47,379 --> 00:02:53,109
bunch of course a bunch of caches

00:02:51,160 --> 00:02:55,900
including a last level cache typically

00:02:53,109 --> 00:02:58,870
l3 these days a level three cache that

00:02:55,900 --> 00:03:01,239
is shared among the course and this

00:02:58,870 --> 00:03:03,819
memory and the whole key thing is that

00:03:01,239 --> 00:03:08,049
the path from them from the processor to

00:03:03,819 --> 00:03:11,290
the memory is cache coherent so what we

00:03:08,049 --> 00:03:13,180
do or everybody does you connect you

00:03:11,290 --> 00:03:15,549
connect those nodes through a

00:03:13,180 --> 00:03:17,109
proprietary cache coherent interconnect

00:03:15,549 --> 00:03:18,910
and that differentiates it from a

00:03:17,109 --> 00:03:20,709
cluster and I'm sure you're all familiar

00:03:18,910 --> 00:03:24,190
with that but I just want to make sure

00:03:20,709 --> 00:03:26,380
that that I've covered this to the

00:03:24,190 --> 00:03:28,780
developer this is sort of what I think

00:03:26,380 --> 00:03:31,569
it looks like what you have you have

00:03:28,780 --> 00:03:34,629
your threads they run on that system in

00:03:31,569 --> 00:03:37,299
this case four nodes your data is

00:03:34,629 --> 00:03:39,760
somewhere and through some magic you can

00:03:37,299 --> 00:03:42,549
access any data location anywhere in the

00:03:39,760 --> 00:03:47,199
system and technically that's the cache

00:03:42,549 --> 00:03:51,639
coherency that takes care of that so in

00:03:47,199 --> 00:03:53,799
the Numa memory although it's physically

00:03:51,639 --> 00:03:55,540
distributed it's logically shared it

00:03:53,799 --> 00:03:57,090
means what does it mean it means that

00:03:55,540 --> 00:03:59,859
you can access any memory location

00:03:57,090 --> 00:04:03,579
anywhere in the system and it's

00:03:59,859 --> 00:04:07,209
accessible to all the fits the thing is

00:04:03,579 --> 00:04:10,480
you don't know what the data is and some

00:04:07,209 --> 00:04:11,949
people say it doesn't matter well unless

00:04:10,480 --> 00:04:13,540
you care about performance and that's

00:04:11,949 --> 00:04:16,349
what I'll be talking about you better

00:04:13,540 --> 00:04:16,349
take care of it

00:04:16,730 --> 00:04:22,400
so same picture here's the situation

00:04:20,270 --> 00:04:24,560
when you're executing your thread up

00:04:22,400 --> 00:04:27,440
there the in the on the left upper left

00:04:24,560 --> 00:04:31,130
node then the question is where's your

00:04:27,440 --> 00:04:33,050
data if your data if you managed to

00:04:31,130 --> 00:04:36,230
organize it so that your data is in that

00:04:33,050 --> 00:04:39,500
local memory then your omit that's the

00:04:36,230 --> 00:04:44,300
fastest memory access you can get in the

00:04:39,500 --> 00:04:46,430
up story in the worst case it's like

00:04:44,300 --> 00:04:48,740
this you're you're executing your

00:04:46,430 --> 00:04:52,030
threads here and in a far away corner of

00:04:48,740 --> 00:04:54,590
the machine your data is there and

00:04:52,030 --> 00:04:56,690
you'll get it the hardware will take

00:04:54,590 --> 00:04:59,660
care of it it's no software needed but

00:04:56,690 --> 00:05:01,130
they'll take longer and if you even if

00:04:59,660 --> 00:05:04,370
you have part of that when you run

00:05:01,130 --> 00:05:06,200
multi-threaded and some of your accesses

00:05:04,370 --> 00:05:08,270
are local and others are remote you'll

00:05:06,200 --> 00:05:11,030
still typically get slowed down by the

00:05:08,270 --> 00:05:15,950
longest memory access certainly until

00:05:11,030 --> 00:05:17,210
you hit the next barrier I need to have

00:05:15,950 --> 00:05:20,990
a little bit of terminology the word

00:05:17,210 --> 00:05:22,730
thread is somewhat overloaded when you

00:05:20,990 --> 00:05:24,680
look at their thread from a hardware

00:05:22,730 --> 00:05:27,710
point of view the hardware designers

00:05:24,680 --> 00:05:29,780
often call it a strand and I'll use that

00:05:27,710 --> 00:05:32,270
word so when I say thread it's software

00:05:29,780 --> 00:05:34,310
when I say thread strand it's Hardware

00:05:32,270 --> 00:05:37,730
just to make things a little hopefully

00:05:34,310 --> 00:05:38,560
less confusing and so that's what I'll

00:05:37,730 --> 00:05:41,930
be doing

00:05:38,560 --> 00:05:44,540
technically a strand has a unique idea

00:05:41,930 --> 00:05:46,750
in the system and it has some states

00:05:44,540 --> 00:05:49,460
like registers and maybe some cash

00:05:46,750 --> 00:05:51,260
associated with it I won't talk about it

00:05:49,460 --> 00:05:54,220
at all to us it's like the hardware

00:05:51,260 --> 00:05:54,220
versus the software

00:05:57,500 --> 00:06:02,270
the interesting thing is in Linux and

00:06:00,050 --> 00:06:04,700
I'm sure I hope you're all familiar with

00:06:02,270 --> 00:06:06,410
it there's a command called LS CPU just

00:06:04,700 --> 00:06:09,110
a show of hands who's familiar with that

00:06:06,410 --> 00:06:13,580
command all right several of you alright

00:06:09,110 --> 00:06:16,760
good that will show you what in what way

00:06:13,580 --> 00:06:20,960
the system is organized and here's an

00:06:16,760 --> 00:06:23,600
example I type in LS CPU and I get a

00:06:20,960 --> 00:06:26,360
whole whole bunch of output but to me

00:06:23,600 --> 00:06:28,670
for this this kind of tuning I want to

00:06:26,360 --> 00:06:32,140
look at the Numa nodes and what it tells

00:06:28,670 --> 00:06:38,240
me there are two Numa nodes 0 & 1 so

00:06:32,140 --> 00:06:41,000
that we know each Numa node has 26 cores

00:06:38,240 --> 00:06:45,560
how do I know that these are the

00:06:41,000 --> 00:06:47,500
comma-separated lists are our strands so

00:06:45,560 --> 00:06:50,180
what we see there are two strands in

00:06:47,500 --> 00:06:52,550
each core and in total there are twenty

00:06:50,180 --> 00:06:54,710
six cores so that's what we know so

00:06:52,550 --> 00:06:57,530
that's how to read these kind of numbers

00:06:54,710 --> 00:07:01,580
and we'll see the numbers are 0 to 25

00:06:57,530 --> 00:07:04,700
for the first strands in the first first

00:07:01,580 --> 00:07:07,610
node and then they continue to 26

00:07:04,700 --> 00:07:13,430
so strand number 26 is in my other

00:07:07,610 --> 00:07:17,510
socket that's what it tells me and so if

00:07:13,430 --> 00:07:21,140
you if you reference 0 0 comma 52 you'll

00:07:17,510 --> 00:07:23,800
have the first strand in the first core

00:07:21,140 --> 00:07:27,710
and the second strand in the first core

00:07:23,800 --> 00:07:29,090
it's a little tricky way of step long

00:07:27,710 --> 00:07:33,650
enough it starts to make some sort of

00:07:29,090 --> 00:07:35,450
sense but ok there's another command

00:07:33,650 --> 00:07:37,910
that I find really useful it's called

00:07:35,450 --> 00:07:40,130
Numa control and with the - capital H

00:07:37,910 --> 00:07:42,919
it'll again it'll echo you all the

00:07:40,130 --> 00:07:45,700
numbers and some other information but

00:07:42,919 --> 00:07:48,590
I'm interested in the latency table and

00:07:45,700 --> 00:07:51,110
these are relative latencies then for

00:07:48,590 --> 00:07:54,169
some practical reason that normalized to

00:07:51,110 --> 00:07:56,990
10 on the diagonal and what it shows you

00:07:54,169 --> 00:07:59,450
is that this the other node there's a

00:07:56,990 --> 00:08:02,210
cost in accessing that memory now

00:07:59,450 --> 00:08:04,850
practical tip don't don't read this to

00:08:02,210 --> 00:08:07,190
accurately don't eat this as this is 2.1

00:08:04,850 --> 00:08:08,900
times more expensive i don't think you

00:08:07,190 --> 00:08:11,120
can say that but it's clearly there's a

00:08:08,900 --> 00:08:14,960
higher cost going to your other no

00:08:11,120 --> 00:08:17,000
that's what it says so those two

00:08:14,960 --> 00:08:21,560
commands are kind of I always type them

00:08:17,000 --> 00:08:23,750
in way way too often so with that

00:08:21,560 --> 00:08:26,090
information we can reconstruct what our

00:08:23,750 --> 00:08:29,030
system looks like what it looks like

00:08:26,090 --> 00:08:30,680
we've seen there two notes each with

00:08:29,030 --> 00:08:33,169
their respective memory and one of the

00:08:30,680 --> 00:08:35,180
information given extra information

00:08:33,169 --> 00:08:37,159
given is the size of the memory attached

00:08:35,180 --> 00:08:39,680
to a node so you know how much memory

00:08:37,159 --> 00:08:43,099
there is is definitely memory and there

00:08:39,680 --> 00:08:45,320
are 26 cores and 52 strands in each and

00:08:43,099 --> 00:08:48,890
these are the numbers all right and

00:08:45,320 --> 00:08:53,210
again notice the jump from 25 you go to

00:08:48,890 --> 00:08:56,210
52 and the reason this is then that

00:08:53,210 --> 00:08:59,000
makes it easier to get all the all the

00:08:56,210 --> 00:09:02,060
strands across all the cores because

00:08:59,000 --> 00:09:03,860
that's 0 to 50 one and actually

00:09:02,060 --> 00:09:07,279
surprisingly often that's what you end

00:09:03,860 --> 00:09:11,420
up using more than the other way so so I

00:09:07,279 --> 00:09:12,980
think it makes sense okay so that's the

00:09:11,420 --> 00:09:18,050
hardware side now let's look at data

00:09:12,980 --> 00:09:20,660
placement the question is how do I

00:09:18,050 --> 00:09:22,640
control where my data goes okay and

00:09:20,660 --> 00:09:25,400
that's kind of the key part of this talk

00:09:22,640 --> 00:09:28,970
we're how do I know where my data is and

00:09:25,400 --> 00:09:30,950
how do I control it and what all

00:09:28,970 --> 00:09:34,250
operating systems use as far as I know

00:09:30,950 --> 00:09:36,920
Windows Mac OS they all use what it's

00:09:34,250 --> 00:09:39,380
called first touch data placement and

00:09:36,920 --> 00:09:42,589
what at simple the simple version is

00:09:39,380 --> 00:09:46,190
that the thread that touches your data

00:09:42,589 --> 00:09:48,890
for the first time owns it that dictates

00:09:46,190 --> 00:09:50,930
the location and that's unless you copy

00:09:48,890 --> 00:09:52,850
things around or explicitly move data

00:09:50,930 --> 00:09:55,190
around that's going to be the home node

00:09:52,850 --> 00:09:57,709
of that data so it's a very crucial

00:09:55,190 --> 00:09:59,480
stage and I'll get back to that but you

00:09:57,709 --> 00:10:04,250
want to make sure that that's where

00:09:59,480 --> 00:10:07,900
things are done the right way it makes

00:10:04,250 --> 00:10:07,900
sense to have this as a default

00:10:09,280 --> 00:10:19,870
I'm really sorry sorry Richard it makes

00:10:16,100 --> 00:10:23,240
sense because this behavior preserves

00:10:19,870 --> 00:10:24,800
sequential execution performance because

00:10:23,240 --> 00:10:26,660
when you run a single thread you want to

00:10:24,800 --> 00:10:28,310
have your data close to your memory and

00:10:26,660 --> 00:10:30,290
there's only one thread touching the

00:10:28,310 --> 00:10:35,570
data so this is a very logical sense

00:10:30,290 --> 00:10:39,110
makes sense but it's not so good in

00:10:35,570 --> 00:10:42,860
powerful parallel computing it works

00:10:39,110 --> 00:10:45,740
fine the what if a single thread

00:10:42,860 --> 00:10:47,540
initializes your data and most people

00:10:45,740 --> 00:10:52,040
are not aware of it this is why actually

00:10:47,540 --> 00:10:55,010
I decided to talk about it then as you

00:10:52,040 --> 00:10:56,750
should all know now all your data will

00:10:55,010 --> 00:10:58,820
be in one memory unless it doesn't fit

00:10:56,750 --> 00:11:04,340
and it will overflow to some nearby

00:10:58,820 --> 00:11:05,870
memory not only does that increase the

00:11:04,340 --> 00:11:07,820
access time for the threads that are

00:11:05,870 --> 00:11:09,560
executing elsewhere you could

00:11:07,820 --> 00:11:10,940
potentially also have a bottleneck at

00:11:09,560 --> 00:11:15,410
that memory controller because all

00:11:10,940 --> 00:11:20,960
threads are going there luckily the

00:11:15,410 --> 00:11:25,040
solution is often surprisingly simple so

00:11:20,960 --> 00:11:28,130
how we do how do we do that you paralyze

00:11:25,040 --> 00:11:30,680
the data initialization part and in that

00:11:28,130 --> 00:11:35,180
way you kind of forced the system to

00:11:30,680 --> 00:11:37,960
split your pages over your nodes and

00:11:35,180 --> 00:11:41,540
here's a very simple example I

00:11:37,960 --> 00:11:45,950
initialize my vector to 0 if I wouldn't

00:11:41,540 --> 00:11:47,390
do that in with openmp then again one

00:11:45,950 --> 00:11:50,840
thread would own all the data

00:11:47,390 --> 00:11:52,520
so very simple parallel four will do the

00:11:50,840 --> 00:11:54,740
magic as you as you probably all know

00:11:52,520 --> 00:11:57,290
that means the iterations are

00:11:54,740 --> 00:11:58,670
distributed over the threads so they all

00:11:57,290 --> 00:12:03,020
get a slice of the data

00:11:58,670 --> 00:12:06,890
I do use the static because the default

00:12:03,020 --> 00:12:08,990
is system dependent you don't want to

00:12:06,890 --> 00:12:12,140
have this like done in a dynamic way

00:12:08,990 --> 00:12:15,260
that defeats the whole purpose unless a

00:12:12,140 --> 00:12:18,470
special case ok so be careful with that

00:12:15,260 --> 00:12:21,710
I would out to be safe maybe up to the

00:12:18,470 --> 00:12:23,930
paranoid level use schedule static and

00:12:21,710 --> 00:12:28,160
and do it that way and in that way you

00:12:23,930 --> 00:12:30,680
distribute the data so the way you do

00:12:28,160 --> 00:12:34,190
that in practice is that you look at how

00:12:30,680 --> 00:12:36,440
your algorithm accesses the data and you

00:12:34,190 --> 00:12:39,650
do some reverse engineering like ah then

00:12:36,440 --> 00:12:42,350
I have to massage my initialization in

00:12:39,650 --> 00:12:44,720
such a way that it'll it'll end up

00:12:42,350 --> 00:12:46,910
mostly in the right place I don't think

00:12:44,720 --> 00:12:49,220
unless it's a very simple case I don't

00:12:46,910 --> 00:12:51,350
think you can strive for optimal

00:12:49,220 --> 00:12:54,050
placement but you can can ease the pain

00:12:51,350 --> 00:12:55,300
right so don't be discouraged if you

00:12:54,050 --> 00:12:57,980
can't get the optimal location

00:12:55,300 --> 00:13:01,570
everywhere that's that would be too good

00:12:57,980 --> 00:13:02,750
to be true it's only the case in my demo

00:13:01,570 --> 00:13:06,080
okay

00:13:02,750 --> 00:13:10,010
now luckily since for todo openmp

00:13:06,080 --> 00:13:13,370
supports Numa and that's really a big

00:13:10,010 --> 00:13:17,120
step forward and anybody using that here

00:13:13,370 --> 00:13:20,740
yeah okay good

00:13:17,120 --> 00:13:23,030
so you may have the nasty questions

00:13:20,740 --> 00:13:25,910
please start looking into it either

00:13:23,030 --> 00:13:27,440
after this talk or you know I hope it

00:13:25,910 --> 00:13:32,390
encouraged you to take a look at it and

00:13:27,440 --> 00:13:36,050
let me let me explain it okay the idea

00:13:32,390 --> 00:13:39,320
is the following the data is wherever it

00:13:36,050 --> 00:13:41,630
is what we're going to do we're going to

00:13:39,320 --> 00:13:44,270
move the thread to where you say it has

00:13:41,630 --> 00:13:45,970
to go and the reason is moving a threat

00:13:44,270 --> 00:13:50,510
is a lot cheaper than moving data

00:13:45,970 --> 00:13:52,670
alright so you're kind of responsibility

00:13:50,510 --> 00:13:54,500
is to make sure that the data is in a

00:13:52,670 --> 00:13:57,530
location where you'd like it to be like

00:13:54,500 --> 00:13:59,840
using first touch and then through the

00:13:57,530 --> 00:14:02,090
OpenMP controls you specify where the

00:13:59,840 --> 00:14:05,960
threads will run and I'll show you that

00:14:02,090 --> 00:14:07,630
will be well be examples there -

00:14:05,960 --> 00:14:13,180
environment variables to control this

00:14:07,630 --> 00:14:17,630
all right one is called OMP places and

00:14:13,180 --> 00:14:20,180
with that that's a list and in a list

00:14:17,630 --> 00:14:24,160
that defines where your threads are

00:14:20,180 --> 00:14:25,280
allowed to run so yes you can exclude

00:14:24,160 --> 00:14:27,350
places

00:14:25,280 --> 00:14:29,270
I mean locations you can say I don't

00:14:27,350 --> 00:14:32,060
want you to run there I want you to only

00:14:29,270 --> 00:14:33,490
run in this part of my machine for some

00:14:32,060 --> 00:14:36,429
reason

00:14:33,490 --> 00:14:39,610
and then you use the proc bind

00:14:36,429 --> 00:14:41,379
environment variable to specify how the

00:14:39,610 --> 00:14:44,679
threats are going to be mapped onto

00:14:41,379 --> 00:14:46,600
those places and I see some puzzled

00:14:44,679 --> 00:14:48,279
faces I'll show some examples but that's

00:14:46,600 --> 00:14:54,970
the general idea so it's a two-step

00:14:48,279 --> 00:14:57,160
approach when you do this really really

00:14:54,970 --> 00:14:59,740
use this environment variable OMP

00:14:57,160 --> 00:15:03,610
underscore display underscore end it

00:14:59,740 --> 00:15:06,100
will echo your settings so it's a sanity

00:15:03,610 --> 00:15:08,499
check and in case you've made a mistake

00:15:06,100 --> 00:15:11,019
which of course never happens to me but

00:15:08,499 --> 00:15:14,319
it may happen to you then it will tell

00:15:11,019 --> 00:15:16,300
you that oh I go to the default because

00:15:14,319 --> 00:15:17,920
you have a type you have some error in

00:15:16,300 --> 00:15:20,050
the definition please use this

00:15:17,920 --> 00:15:22,449
environment variable set it to verbose

00:15:20,050 --> 00:15:24,819
and before your program starts it will

00:15:22,449 --> 00:15:27,970
echo all the environment variables very

00:15:24,819 --> 00:15:31,959
very useful so let's look at there's

00:15:27,970 --> 00:15:34,059
some examples let's say I want to

00:15:31,959 --> 00:15:38,259
schedule all the threads on the course

00:15:34,059 --> 00:15:40,600
in my system well what I can do I can

00:15:38,259 --> 00:15:46,629
say the places are the course in my

00:15:40,600 --> 00:15:48,519
system it's as easy as that and you want

00:15:46,629 --> 00:15:50,619
to spread them out you want to leverage

00:15:48,519 --> 00:15:52,600
the bandwidth you don't want threads to

00:15:50,619 --> 00:15:56,079
be literally close to each other you

00:15:52,600 --> 00:15:58,689
want to spread them you say prop bind

00:15:56,079 --> 00:16:03,999
equals spread and then the runtime

00:15:58,689 --> 00:16:06,309
system will make that happen you have

00:16:03,999 --> 00:16:10,660
more choices in that there's sockets

00:16:06,309 --> 00:16:14,079
course threads and a user Devine defines

00:16:10,660 --> 00:16:15,999
that to give you full control there's an

00:16:14,079 --> 00:16:17,799
optional number end that will that you

00:16:15,999 --> 00:16:21,790
can use to say how many cores you'd like

00:16:17,799 --> 00:16:23,529
to use like I have eight no whatever

00:16:21,790 --> 00:16:26,230
eight cores in my system I only want to

00:16:23,529 --> 00:16:27,360
use two that's how you do that same for

00:16:26,230 --> 00:16:29,949
threads

00:16:27,360 --> 00:16:32,470
sockets cores and threads you can all

00:16:29,949 --> 00:16:37,509
limit how many how much you want to use

00:16:32,470 --> 00:16:40,930
and that defines the places like all the

00:16:37,509 --> 00:16:43,660
sockets all the cores and so forth

00:16:40,930 --> 00:16:46,300
personally I end up using the

00:16:43,660 --> 00:16:48,519
user-defined set most of the time and

00:16:46,300 --> 00:16:50,619
that's what I'll zoom in on but be aware

00:16:48,519 --> 00:16:55,689
there's this high-level symbolic

00:16:50,619 --> 00:16:57,550
notation as well okay let's say we want

00:16:55,689 --> 00:17:00,670
to schedule onto the sockets all we do

00:16:57,550 --> 00:17:02,259
is this all right but let's say we want

00:17:00,670 --> 00:17:09,309
to use for some reason I want to use

00:17:02,259 --> 00:17:13,510
strand 0 8 16 and 24 you define your

00:17:09,309 --> 00:17:16,419
list this way each each entry between

00:17:13,510 --> 00:17:17,949
the curly braces is is a place and I can

00:17:16,419 --> 00:17:20,439
have more than one number here I'm

00:17:17,949 --> 00:17:22,720
showing one number but it's set based

00:17:20,439 --> 00:17:25,360
it's a set set of sets in a way what you

00:17:22,720 --> 00:17:28,569
define so here's very simple I have I

00:17:25,360 --> 00:17:34,450
have four places and and those should be

00:17:28,569 --> 00:17:35,890
used so how about the placement policy

00:17:34,450 --> 00:17:37,870
where the threads are going you've got

00:17:35,890 --> 00:17:39,340
three choices and there's no

00:17:37,870 --> 00:17:42,520
user-defined choice for that by the way

00:17:39,340 --> 00:17:44,049
it's either master master means it's

00:17:42,520 --> 00:17:46,140
especially useful with nested

00:17:44,049 --> 00:17:49,270
parallelism where you have multiple

00:17:46,140 --> 00:17:51,610
master threads in a way you want to keep

00:17:49,270 --> 00:17:54,309
the threads in that group close to their

00:17:51,610 --> 00:17:56,830
master so that's that's that's I think

00:17:54,309 --> 00:17:59,110
the only realistic use I can see for

00:17:56,830 --> 00:18:01,960
this feature most cases people use

00:17:59,110 --> 00:18:04,779
closed or spread where you either keep

00:18:01,960 --> 00:18:06,640
the threads closed or spread and the

00:18:04,779 --> 00:18:09,460
most common uses of that is when you

00:18:06,640 --> 00:18:11,169
have two loops and what you want to do

00:18:09,460 --> 00:18:13,809
you want to have like the outer loops

00:18:11,169 --> 00:18:15,460
spread over the machine and that within

00:18:13,809 --> 00:18:17,740
that parallel region you want to keep

00:18:15,460 --> 00:18:22,840
them closed something like that you can

00:18:17,740 --> 00:18:25,299
mix and match okay so how do you do the

00:18:22,840 --> 00:18:27,610
places on our imaginary no it's not even

00:18:25,299 --> 00:18:30,539
imaginaire it's a real system with these

00:18:27,610 --> 00:18:32,980
numbers that I showed in the beginning

00:18:30,539 --> 00:18:36,070
let's say for the fun of it I want to

00:18:32,980 --> 00:18:39,100
use the first strand on each core in the

00:18:36,070 --> 00:18:40,570
first socket all right I'm not a big fan

00:18:39,100 --> 00:18:43,630
of asking questions to the audience

00:18:40,570 --> 00:18:46,210
unless it's a simple hand waving but you

00:18:43,630 --> 00:18:48,070
got to look at these numbers so let me

00:18:46,210 --> 00:18:50,529
let me an interest of time let me show

00:18:48,070 --> 00:18:54,760
you the result so what I have here is 0

00:18:50,529 --> 00:18:57,730
1 up to 25 so I have 26 places

00:18:54,760 --> 00:19:00,490
and that's exactly this list here

00:18:57,730 --> 00:19:03,820
those are the first this in the first

00:19:00,490 --> 00:19:05,860
Numa note those are the cores in that

00:19:03,820 --> 00:19:11,110
node and I want to use the first set

00:19:05,860 --> 00:19:13,030
there that's a long list it's a lot of

00:19:11,110 --> 00:19:18,670
typing so luckily there's a compact

00:19:13,030 --> 00:19:21,850
notation very powerful where I have the

00:19:18,670 --> 00:19:24,730
starting point zero strand zero I want

00:19:21,850 --> 00:19:27,820
to have 26 places and an increment of 1

00:19:24,730 --> 00:19:32,050
so that expands to this list and that's

00:19:27,820 --> 00:19:35,050
a way more convenient notation so again

00:19:32,050 --> 00:19:36,910
start idea number how many you want to

00:19:35,050 --> 00:19:39,300
have and the increment then that could

00:19:36,910 --> 00:19:39,300
be anything

00:19:39,630 --> 00:19:45,280
another example for some exotic reason I

00:19:43,630 --> 00:19:47,110
want to have the first strand in the

00:19:45,280 --> 00:19:49,810
first part of the machine and the second

00:19:47,110 --> 00:19:52,000
strand in the the first first four cores

00:19:49,810 --> 00:19:53,950
and the second strand in the first four

00:19:52,000 --> 00:19:55,480
cores in the other socket well you can

00:19:53,950 --> 00:19:57,820
imagine what that looks like that's

00:19:55,480 --> 00:19:59,470
something like this and if you've never

00:19:57,820 --> 00:20:01,360
seen this before it takes a while to get

00:19:59,470 --> 00:20:04,240
used to but eventually you actually find

00:20:01,360 --> 00:20:05,980
it easy so that's how you that's how I

00:20:04,240 --> 00:20:10,270
like to use it because it gives me full

00:20:05,980 --> 00:20:13,780
control all right let me finish with a

00:20:10,270 --> 00:20:16,090
performance tuning example my favorite

00:20:13,780 --> 00:20:17,890
algorithm it's extremely simple and it's

00:20:16,090 --> 00:20:20,130
actually a lot to say about it it's

00:20:17,890 --> 00:20:23,040
multiplying a matrix with the vector

00:20:20,130 --> 00:20:25,300
here's one way you could write it in C

00:20:23,040 --> 00:20:27,700
taking the dot products of the row of

00:20:25,300 --> 00:20:29,470
the matrix times the vector and you do

00:20:27,700 --> 00:20:31,900
that for all the rows and you have your

00:20:29,470 --> 00:20:33,520
result it's almost embarrassing the

00:20:31,900 --> 00:20:36,750
parallel because all these dot products

00:20:33,520 --> 00:20:38,680
are are independent right

00:20:36,750 --> 00:20:40,860
so that's how you would do that with

00:20:38,680 --> 00:20:43,690
openmp by the way every self-respecting

00:20:40,860 --> 00:20:45,550
auto parallelizing compiler will do this

00:20:43,690 --> 00:20:47,140
for you so if you have that they

00:20:45,550 --> 00:20:50,920
probably do the magic for you I just

00:20:47,140 --> 00:20:54,340
want to illustrate this I used an AMD

00:20:50,920 --> 00:20:56,290
epoch server with two sockets the reason

00:20:54,340 --> 00:20:59,080
is that for this kind of talk it's a

00:20:56,290 --> 00:21:00,790
great great kettle extra machine because

00:20:59,080 --> 00:21:03,400
it has although it has two sockets it

00:21:00,790 --> 00:21:06,340
has eight new my notes so it's sort of

00:21:03,400 --> 00:21:09,070
Numa within the socket and that allows

00:21:06,340 --> 00:21:12,610
me to play a little bit with

00:21:09,070 --> 00:21:16,149
and you check that with LS CPU as I

00:21:12,610 --> 00:21:18,639
showed and you also find out each Numa

00:21:16,149 --> 00:21:24,129
node has eight cores and two strands

00:21:18,639 --> 00:21:27,159
each so in total the two socket am the

00:21:24,129 --> 00:21:28,360
epoch box the high-end emissary has 64

00:21:27,159 --> 00:21:32,500
cores and hundred and twenty eight

00:21:28,360 --> 00:21:34,179
strengths so LS CPU now is a little more

00:21:32,500 --> 00:21:37,059
interesting sort of output will give you

00:21:34,179 --> 00:21:39,970
all these numbers and you'll see the

00:21:37,059 --> 00:21:42,669
latency table for all the eight nodes so

00:21:39,970 --> 00:21:45,820
what it what it says is that when our

00:21:42,669 --> 00:21:49,809
node 0 there's a cost going within my

00:21:45,820 --> 00:21:53,350
socket to node 1 2 or 3 and there's a

00:21:49,809 --> 00:21:54,879
higher cost going to off socket now the

00:21:53,350 --> 00:21:56,529
off socket part is fairly conventional

00:21:54,879 --> 00:21:58,539
the in socket is a little less

00:21:56,529 --> 00:22:04,179
conventional but that's just the way it

00:21:58,539 --> 00:22:07,509
is and this is how you find out ok my

00:22:04,179 --> 00:22:09,159
setup I didn't use the second thread by

00:22:07,509 --> 00:22:11,409
the way because for these very CPU

00:22:09,159 --> 00:22:16,299
intensive programs there's no there's no

00:22:11,409 --> 00:22:18,970
use for that second thread so I want to

00:22:16,299 --> 00:22:22,269
distribute my threads across all those

00:22:18,970 --> 00:22:24,879
course all those eight nodes so I want

00:22:22,269 --> 00:22:26,889
to have eight threads each node has one

00:22:24,879 --> 00:22:29,559
thread and then I'll have two threads in

00:22:26,889 --> 00:22:33,419
that way I maximize the bandwidth in my

00:22:29,559 --> 00:22:37,830
system well that's why I did it that way

00:22:33,419 --> 00:22:42,759
so this is an example and here's what my

00:22:37,830 --> 00:22:44,860
my settings look like and this starts to

00:22:42,759 --> 00:22:47,500
get long it's about as long as you can

00:22:44,860 --> 00:22:49,179
get for this eight node system I still a

00:22:47,500 --> 00:22:51,100
little bit struggling to see if I can

00:22:49,179 --> 00:22:54,309
make it more compact but I haven't found

00:22:51,100 --> 00:22:58,419
that yet so this is as tight as I can

00:22:54,309 --> 00:23:01,809
get it I define my places again two

00:22:58,419 --> 00:23:04,330
threads in this example and so this is

00:23:01,809 --> 00:23:06,549
the first the first Numa node I want to

00:23:04,330 --> 00:23:09,460
have two threads so it will be zero one

00:23:06,549 --> 00:23:11,289
eight nine sixteen seventeen and so

00:23:09,460 --> 00:23:14,379
forth that's how it will be expanded

00:23:11,289 --> 00:23:16,929
under the hood I want to keep my threads

00:23:14,379 --> 00:23:18,909
close to each other and just for sake of

00:23:16,929 --> 00:23:19,960
the example these are 16 threads that

00:23:18,909 --> 00:23:21,309
I'll be using

00:23:19,960 --> 00:23:26,980
so that was my setup

00:23:21,309 --> 00:23:30,340
for my experiment here's the performance

00:23:26,980 --> 00:23:33,159
as a function of the number of open and

00:23:30,340 --> 00:23:35,279
P threads and remember I've I have 128

00:23:33,159 --> 00:23:38,440
strands but I'm only using half of it

00:23:35,279 --> 00:23:41,590
the performance in gigaflop so higher is

00:23:38,440 --> 00:23:44,700
better this is very very poor

00:23:41,590 --> 00:23:47,529
performance the red the reddish line and

00:23:44,700 --> 00:23:52,659
the reason is I didn't I didn't do first

00:23:47,529 --> 00:23:55,389
touch I just had a simple sequential

00:23:52,659 --> 00:24:00,820
function this is my matrix M a vector

00:23:55,389 --> 00:24:04,029
and and it goes alright this one uses

00:24:00,820 --> 00:24:05,950
first touch and I didn't have really

00:24:04,029 --> 00:24:08,529
have the time to show you the code but

00:24:05,950 --> 00:24:10,299
I'm happy to talk about it offline it's

00:24:08,529 --> 00:24:12,399
actually fairly simple it's no magic

00:24:10,299 --> 00:24:14,619
here you just have a bunch of parallel

00:24:12,399 --> 00:24:19,570
for loops when you initialize the matrix

00:24:14,619 --> 00:24:21,100
and the vector so now I got this scaling

00:24:19,570 --> 00:24:24,369
is so poor it didn't even attach a

00:24:21,100 --> 00:24:26,019
number to it here I got 35 X it has some

00:24:24,369 --> 00:24:28,330
interesting behavior here which I

00:24:26,019 --> 00:24:31,600
haven't analyzed yeah in this plateau

00:24:28,330 --> 00:24:37,450
but ultimately this it up over the

00:24:31,600 --> 00:24:40,179
single trap it's is 35 X all together by

00:24:37,450 --> 00:24:42,639
using first touch I get 22 X performance

00:24:40,179 --> 00:24:44,139
improvement on the same hardware and I

00:24:42,639 --> 00:24:47,279
hope that number is high enough to

00:24:44,139 --> 00:24:50,110
convince you to start looking into this

00:24:47,279 --> 00:24:52,330
so some conclusions fairly obvious I

00:24:50,110 --> 00:24:55,830
think data and thread placement really

00:24:52,330 --> 00:24:59,769
matters and it could be matter a lot

00:24:55,830 --> 00:25:02,619
it's really very powerful to leverage

00:24:59,769 --> 00:25:05,590
first touch its generic I can take my

00:25:02,619 --> 00:25:07,899
code and run it on any any architecture

00:25:05,590 --> 00:25:09,580
and it'll automatically expand to do the

00:25:07,899 --> 00:25:11,350
right thing the one thing I probably

00:25:09,580 --> 00:25:13,570
want to do is adjust my places

00:25:11,350 --> 00:25:17,139
definition for that architecture but the

00:25:13,570 --> 00:25:18,850
code the binary itself will just run and

00:25:17,139 --> 00:25:23,470
that's because first touches is such a

00:25:18,850 --> 00:25:25,659
general principle I think the supporting

00:25:23,470 --> 00:25:27,159
openmp given how close this is to the

00:25:25,659 --> 00:25:32,259
hardware is very elegant and powerful

00:25:27,159 --> 00:25:34,530
and more has been added in open MP 5 dot

00:25:32,259 --> 00:25:37,560
o so maybe next year I can

00:25:34,530 --> 00:25:40,290
can talk about that in more detail this

00:25:37,560 --> 00:25:44,720
is the lawyer talk so thank you very

00:25:40,290 --> 00:25:44,720

YouTube URL: https://www.youtube.com/watch?v=0v1FGLrlt9k


