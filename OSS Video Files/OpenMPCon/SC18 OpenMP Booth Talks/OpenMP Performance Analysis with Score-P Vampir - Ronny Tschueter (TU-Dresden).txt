Title: OpenMP Performance Analysis with Score-P Vampir - Ronny Tschueter (TU-Dresden)
Publication date: 2018-11-17
Playlist: SC18 OpenMP Booth Talks
Description: 
	SC18 OpenMP Booth Talk - November 13, 2018, Dallas TX
Slides at https://www.openmp.org/wp-content/uploads/SC18-BoothTalks-Tschueter.pdf
Captions: 
	00:00:00,110 --> 00:00:05,790
solo hello everyone and my talk today I

00:00:04,410 --> 00:00:08,760
want to give you a short introduction

00:00:05,790 --> 00:00:11,490
into studying open p performance with

00:00:08,760 --> 00:00:14,309
vampire and score pete so when it comes

00:00:11,490 --> 00:00:16,830
to performance analysis we need to

00:00:14,309 --> 00:00:19,590
software components one component is a

00:00:16,830 --> 00:00:22,380
measurement system capable of recording

00:00:19,590 --> 00:00:25,109
events like function enter writes it so

00:00:22,380 --> 00:00:27,570
calls to the MPI library at application

00:00:25,109 --> 00:00:30,300
runtime recording the data and then

00:00:27,570 --> 00:00:33,660
afterwards we need a second component

00:00:30,300 --> 00:00:36,300
analyze analysis tool which provides you

00:00:33,660 --> 00:00:38,879
nice visualization of your performance

00:00:36,300 --> 00:00:41,370
data and so you get able to investigate

00:00:38,879 --> 00:00:44,160
your application behavior and optimize

00:00:41,370 --> 00:00:49,559
it and identify performance issues and

00:00:44,160 --> 00:00:51,809
bottlenecks so let me start with a brief

00:00:49,559 --> 00:00:55,800
overview of the scorpy measurement

00:00:51,809 --> 00:00:58,109
system here so scope is a jointly

00:00:55,800 --> 00:01:00,120
developed measurement infrastructure

00:00:58,109 --> 00:01:04,830
it's developed by partners in Europe

00:01:00,120 --> 00:01:07,290
like avid or WTHR --then for pensando

00:01:04,830 --> 00:01:10,520
muley Technical University trace and but

00:01:07,290 --> 00:01:13,320
also partners from the s like Oregon and

00:01:10,520 --> 00:01:15,540
what we have here is a measurement core

00:01:13,320 --> 00:01:17,820
which can connect to your applications

00:01:15,540 --> 00:01:20,490
through several adapters so we have

00:01:17,820 --> 00:01:23,130
adapters for MPI but also for compiler

00:01:20,490 --> 00:01:27,240
instrumentation and what's really

00:01:23,130 --> 00:01:30,450
interesting here is the part responsible

00:01:27,240 --> 00:01:32,610
for connecting to your application for

00:01:30,450 --> 00:01:36,000
thread level parallelism and their Wien

00:01:32,610 --> 00:01:37,170
support here MP and P threads and then

00:01:36,000 --> 00:01:39,479
you have the measurement system

00:01:37,170 --> 00:01:41,040
collecting all the data at runtime you

00:01:39,479 --> 00:01:43,829
can enhance the data with hardware

00:01:41,040 --> 00:01:47,189
counters for example poppy and then you

00:01:43,829 --> 00:01:50,399
can write out profiles or traces and use

00:01:47,189 --> 00:01:52,890
on top of it all and several kinds of

00:01:50,399 --> 00:01:55,470
analysis tools today I want to focus on

00:01:52,890 --> 00:01:58,740
traces and especially on the vampyre

00:01:55,470 --> 00:02:01,079
analysis tool but you see the Scorpion

00:01:58,740 --> 00:02:04,590
measurement system is capable of doing

00:02:01,079 --> 00:02:07,710
even more so what we are currently using

00:02:04,590 --> 00:02:10,289
for OpenMP instrumentation in scorpy is

00:02:07,710 --> 00:02:11,459
the Oh Perry to instrument

00:02:10,289 --> 00:02:13,440
this is a source to source

00:02:11,459 --> 00:02:15,930
instrumentation so you parse

00:02:13,440 --> 00:02:18,840
every application and then you annotate

00:02:15,930 --> 00:02:23,010
the open p directors and runtime library

00:02:18,840 --> 00:02:25,710
calls and if this works quite well but

00:02:23,010 --> 00:02:27,660
the major issue here is you have to

00:02:25,710 --> 00:02:30,450
write the source code pause on your own

00:02:27,660 --> 00:02:34,470
and you need a recompilation of your

00:02:30,450 --> 00:02:38,130
source code which sometimes is a little

00:02:34,470 --> 00:02:41,880
bit messy and yeah it can be done easier

00:02:38,130 --> 00:02:43,980
for example with the new introduced open

00:02:41,880 --> 00:02:47,550
MPT the tools interface which is now

00:02:43,980 --> 00:02:49,500
part of the open p5 our standard and the

00:02:47,550 --> 00:02:53,070
nice thing here to standardize interface

00:02:49,500 --> 00:02:55,640
so this is really really appreciate from

00:02:53,070 --> 00:02:58,740
the tools developer perspective and

00:02:55,640 --> 00:03:01,140
there we can access state informations

00:02:58,740 --> 00:03:02,850
but we have also access to event

00:03:01,140 --> 00:03:05,900
callbacks and it's really nice feature

00:03:02,850 --> 00:03:09,690
because now we have the chance to

00:03:05,900 --> 00:03:12,930
register for specific events like the

00:03:09,690 --> 00:03:15,810
creation of tasks or the entering of a

00:03:12,930 --> 00:03:17,070
para region and there for the event we

00:03:15,810 --> 00:03:19,200
are interested in we can register

00:03:17,070 --> 00:03:22,890
callback and then get notified by the

00:03:19,200 --> 00:03:25,080
Oakman P runtime library the one of the

00:03:22,890 --> 00:03:28,170
things here which have to be noted is

00:03:25,080 --> 00:03:30,630
that we need these report by the open p

00:03:28,170 --> 00:03:32,370
runtime implementations and there for

00:03:30,630 --> 00:03:34,890
example some of these callbacks are

00:03:32,370 --> 00:03:37,650
mandatory and other ones are optional

00:03:34,890 --> 00:03:40,459
and so we as tool developers have to

00:03:37,650 --> 00:03:43,320
take care that if we rely on optional

00:03:40,459 --> 00:03:45,540
callbacks which might not be there in

00:03:43,320 --> 00:03:50,640
every runtime we have to provide some

00:03:45,540 --> 00:03:52,650
kind of fall back or yeah in some cases

00:03:50,640 --> 00:03:56,610
we have to fall back to the old Aparri

00:03:52,650 --> 00:03:59,310
approach so this is what we can do now

00:03:56,610 --> 00:04:02,250
in scorpy with respect to maybe

00:03:59,310 --> 00:04:03,870
instrumentation and then we come to the

00:04:02,250 --> 00:04:06,570
second part now we have collected all

00:04:03,870 --> 00:04:08,910
the data from our application and then

00:04:06,570 --> 00:04:11,400
we can use a nice visualization tool to

00:04:08,910 --> 00:04:13,470
get an inside of your application and

00:04:11,400 --> 00:04:16,530
here I want to present the vampyre tool

00:04:13,470 --> 00:04:19,680
and give you some short examples on case

00:04:16,530 --> 00:04:21,930
studies and let me start with a sparse

00:04:19,680 --> 00:04:24,479
matrix vector multiplication to show you

00:04:21,930 --> 00:04:26,240
how you can identify load imbalances

00:04:24,479 --> 00:04:30,830
with scorpy and where

00:04:26,240 --> 00:04:35,090
here so the idea here is that you have a

00:04:30,830 --> 00:04:37,789
matrix multiplication this is as usual

00:04:35,090 --> 00:04:41,539
but the main point here is that we are

00:04:37,789 --> 00:04:43,250
using sparse matrices so they are less

00:04:41,539 --> 00:04:45,620
populated and primarily they are

00:04:43,250 --> 00:04:50,710
populated with serious and only a few

00:04:45,620 --> 00:04:53,930
elements have real values in it and to

00:04:50,710 --> 00:04:57,050
efficiently store it in memory we only

00:04:53,930 --> 00:05:01,250
want to store these elements which have

00:04:57,050 --> 00:05:03,970
nonzero values and the naive algorithm

00:05:01,250 --> 00:05:06,229
would look like this there you have your

00:05:03,970 --> 00:05:09,470
matrix multiplication and you're doing

00:05:06,229 --> 00:05:13,520
the computation only for these elements

00:05:09,470 --> 00:05:15,080
which have nonzero elements so if you

00:05:13,520 --> 00:05:18,380
start to implement these new if

00:05:15,080 --> 00:05:22,970
algorithm it can look like this if your

00:05:18,380 --> 00:05:25,520
pragma OMP to paralyze the loop and you

00:05:22,970 --> 00:05:27,669
have a switch statement to do with the

00:05:25,520 --> 00:05:31,520
computation only for these elements

00:05:27,669 --> 00:05:35,240
populated with non zero values and as

00:05:31,520 --> 00:05:37,099
you can see these nonzero elements

00:05:35,240 --> 00:05:40,099
especially the distribution of these non

00:05:37,099 --> 00:05:42,650
element nonzero elements may influence

00:05:40,099 --> 00:05:46,039
the load balancing of your whole

00:05:42,650 --> 00:05:47,990
application yeah and if you use scorpy

00:05:46,039 --> 00:05:51,349
to instrument such an application you

00:05:47,990 --> 00:05:54,490
run it record a trace file and then you

00:05:51,349 --> 00:05:57,919
open the recorded trace data in rem here

00:05:54,490 --> 00:06:01,280
the GUI pops up and the first look could

00:05:57,919 --> 00:06:03,440
look like this so in principle you see

00:06:01,280 --> 00:06:05,120
the in this picture you see the whole

00:06:03,440 --> 00:06:07,849
application so you have here an overview

00:06:05,120 --> 00:06:10,430
of your complete application run from

00:06:07,849 --> 00:06:14,509
the start of the application to the end

00:06:10,430 --> 00:06:16,610
you see all individual threads here on

00:06:14,509 --> 00:06:20,000
the y-axis and the time axis here on the

00:06:16,610 --> 00:06:22,729
x-axis and different colors indicate

00:06:20,000 --> 00:06:25,400
different function categories for

00:06:22,729 --> 00:06:28,280
example we have in blue and open P

00:06:25,400 --> 00:06:32,210
barrier or synchronization principles

00:06:28,280 --> 00:06:34,169
and these oranges colors represent the

00:06:32,210 --> 00:06:38,009
open P peril

00:06:34,169 --> 00:06:40,169
so this is a function which really does

00:06:38,009 --> 00:06:42,270
computation and the blue colors

00:06:40,169 --> 00:06:44,909
represent synchronization in principle

00:06:42,270 --> 00:06:47,789
you own will you want to get rid of all

00:06:44,909 --> 00:06:51,090
these synchronization keeps them as

00:06:47,789 --> 00:06:55,770
small as possible and do a computation

00:06:51,090 --> 00:07:01,050
at yeah at most in your application run

00:06:55,770 --> 00:07:02,909
and on the first few you see here that

00:07:01,050 --> 00:07:07,620
you are not efficiently usually like

00:07:02,909 --> 00:07:10,020
utilizing the resources of your machine

00:07:07,620 --> 00:07:12,689
with this kind of execution application

00:07:10,020 --> 00:07:16,819
execution and you see you have a problem

00:07:12,689 --> 00:07:21,120
with load imbalance so you only see 50%

00:07:16,819 --> 00:07:24,120
of the threads are doing useful work the

00:07:21,120 --> 00:07:25,919
other 50% of the threads are more or

00:07:24,120 --> 00:07:29,069
less idling in synchronization and

00:07:25,919 --> 00:07:31,889
waiting barriers so what can you do in

00:07:29,069 --> 00:07:34,229
this example yeah here in this example

00:07:31,889 --> 00:07:38,250
it's really easy you can at these

00:07:34,229 --> 00:07:41,149
schedule close to your open p paralyzed

00:07:38,250 --> 00:07:44,370
loop and therefore you changing the

00:07:41,149 --> 00:07:48,479
scheduling strategy you do not assign

00:07:44,370 --> 00:07:53,279
work in static chunks but you dividing

00:07:48,479 --> 00:07:55,909
your openmp loop into chunks of a size

00:07:53,279 --> 00:07:58,819
of thousand iterations and if a thread

00:07:55,909 --> 00:08:02,069
finish the work of this chunk it can

00:07:58,819 --> 00:08:08,099
request a new chunk and therefore you

00:08:02,069 --> 00:08:10,770
get dynamic load-balancing and if you

00:08:08,099 --> 00:08:14,310
run your a recompile your application

00:08:10,770 --> 00:08:16,289
run again and have a look on the

00:08:14,310 --> 00:08:18,810
resulting trace in this case you see

00:08:16,289 --> 00:08:20,849
immediately that the amount of these

00:08:18,810 --> 00:08:23,849
time spent and synchronization has

00:08:20,849 --> 00:08:27,659
decreased drastically and a nice feature

00:08:23,849 --> 00:08:30,300
in vampyre here is that you can open a

00:08:27,659 --> 00:08:33,529
comparison view on post races and now

00:08:30,300 --> 00:08:35,550
you see directly the changes of your

00:08:33,529 --> 00:08:38,039
optimization in the Coe source code so

00:08:35,550 --> 00:08:40,589
on the top here you have your original

00:08:38,039 --> 00:08:45,690
application run with the load balance in

00:08:40,589 --> 00:08:48,360
it and on the bottom in this bluish

00:08:45,690 --> 00:08:50,130
background color you see the optimized

00:08:48,360 --> 00:08:53,279
version of your application run and you

00:08:50,130 --> 00:08:55,649
directly see the decreased runtime and

00:08:53,279 --> 00:08:58,199
you also see that you get rid of all

00:08:55,649 --> 00:09:01,319
these synchronization overheads so you

00:08:58,199 --> 00:09:04,019
have reduced due to introducing dynamic

00:09:01,319 --> 00:09:06,000
load balancing you reduced the waiting

00:09:04,019 --> 00:09:10,019
time in your whole application and

00:09:06,000 --> 00:09:15,000
thereby gain a performance benefit so

00:09:10,019 --> 00:09:16,800
this was a really simple example let me

00:09:15,000 --> 00:09:20,189
come to the next example which is a

00:09:16,800 --> 00:09:22,920
little more sophisticated because now we

00:09:20,189 --> 00:09:26,850
are switching from openmp parallelism

00:09:22,920 --> 00:09:31,350
only to a combination of OpenMP and MPI

00:09:26,850 --> 00:09:34,709
parallelization and the problem here to

00:09:31,350 --> 00:09:38,040
show is that you do not have only to

00:09:34,709 --> 00:09:42,329
deal with load and imbalances in NPR and

00:09:38,040 --> 00:09:44,430
OpenMP sorry but your load imbalances in

00:09:42,329 --> 00:09:47,250
your open P regions might propagate

00:09:44,430 --> 00:09:49,980
forward and cause load imbalances in

00:09:47,250 --> 00:09:53,130
your MPI calls for example here you see

00:09:49,980 --> 00:09:56,699
you have waiting times and you have some

00:09:53,130 --> 00:09:59,610
optimum P threads which are computing

00:09:56,699 --> 00:10:02,519
for a longer time and some of these

00:09:59,610 --> 00:10:04,350
OpenMP sweats finish early and already

00:10:02,519 --> 00:10:06,959
start an MPI communication

00:10:04,350 --> 00:10:09,120
the problem here is that these MPI

00:10:06,959 --> 00:10:11,040
communication is a collective operation

00:10:09,120 --> 00:10:14,209
so if this operation can only finish

00:10:11,040 --> 00:10:17,010
after all participants

00:10:14,209 --> 00:10:19,860
entered these collective operations and

00:10:17,010 --> 00:10:22,350
you see that all other MPI processes

00:10:19,860 --> 00:10:25,680
have to wait until the last participant

00:10:22,350 --> 00:10:28,350
enters this collective operation so this

00:10:25,680 --> 00:10:32,310
is a nice example of how wait States or

00:10:28,350 --> 00:10:34,769
waiting times in OpenMP peripera regions

00:10:32,310 --> 00:10:38,100
can propagate and cause waiting

00:10:34,769 --> 00:10:41,000
situations in your MPI communications

00:10:38,100 --> 00:10:43,589
and the nice thing here is that

00:10:41,000 --> 00:10:46,319
scorpioned vampy and all the other tools

00:10:43,589 --> 00:10:48,209
are capable of showing you pass through

00:10:46,319 --> 00:10:51,209
they are not limited to MP parallelism

00:10:48,209 --> 00:10:54,059
you can also investigate the interplay

00:10:51,209 --> 00:10:58,589
of different parallelization paradigms

00:10:54,059 --> 00:11:00,869
like open MP MPI but also accelerated

00:10:58,589 --> 00:11:09,569
we celebrate the based paradigms like

00:11:00,869 --> 00:11:13,289
CUDA rock UCL and at the end of my talk

00:11:09,569 --> 00:11:17,459
I want to show you a real world example

00:11:13,289 --> 00:11:21,569
where we had an here the Trinity code in

00:11:17,459 --> 00:11:23,969
a sequencer and we tried here to compare

00:11:21,569 --> 00:11:29,279
the performance on different a number of

00:11:23,969 --> 00:11:31,979
processes so the idea here was we have

00:11:29,279 --> 00:11:34,889
these already existing code and we lit

00:11:31,979 --> 00:11:39,539
up its analysis on this code and we ran

00:11:34,889 --> 00:11:42,359
this code with one two four eight and

00:11:39,539 --> 00:11:45,329
even 16 threads and what you see here

00:11:42,359 --> 00:11:47,969
immediately is that you from the step

00:11:45,329 --> 00:11:52,379
from one thread to two you get comments

00:11:47,969 --> 00:11:56,039
benefit but in all other steps from 2 to

00:11:52,379 --> 00:11:59,339
4 8 or even 16 so it you do not see in

00:11:56,039 --> 00:12:02,639
decrease in overall runtime so you use

00:11:59,339 --> 00:12:05,519
more right I do get much benefit from it

00:12:02,639 --> 00:12:08,639
why was what was the reason in this case

00:12:05,519 --> 00:12:11,459
yeah so you see that you also have a lot

00:12:08,639 --> 00:12:14,939
of share of these bluish colors here

00:12:11,459 --> 00:12:17,759
which again represent open pipe areas so

00:12:14,939 --> 00:12:21,839
again it's openmp synchronization and

00:12:17,759 --> 00:12:24,720
barrier time waiting time and the goal

00:12:21,839 --> 00:12:28,229
here was to find the reason for these

00:12:24,720 --> 00:12:30,359
waiting times and what we find out here

00:12:28,229 --> 00:12:33,149
in this example was that we had a main

00:12:30,359 --> 00:12:35,369
loop responsible for all the simulation

00:12:33,149 --> 00:12:37,739
stuff and inside the loop

00:12:35,369 --> 00:12:39,599
there was and there were there were

00:12:37,739 --> 00:12:43,019
string streams which were created and

00:12:39,599 --> 00:12:46,289
destroyed the drawback of such kind of

00:12:43,019 --> 00:12:50,099
approach is that the creation of string

00:12:46,289 --> 00:12:52,889
streams is locked so you use an a mutex

00:12:50,099 --> 00:12:56,159
and we do this inside the loop and

00:12:52,889 --> 00:13:00,149
principle you get a lock contention so

00:12:56,159 --> 00:13:04,979
all the pro threads are competing for

00:13:00,149 --> 00:13:07,230
the lock and the goal or the the

00:13:04,979 --> 00:13:09,600
solution here in this case was to

00:13:07,230 --> 00:13:12,590
move the creation of the string stream

00:13:09,600 --> 00:13:15,960
objects out of the loop doing the

00:13:12,590 --> 00:13:20,010
creation of these C++ objects outside of

00:13:15,960 --> 00:13:22,290
the loop and just clearing the strings

00:13:20,010 --> 00:13:25,110
inside the loop so you do not create and

00:13:22,290 --> 00:13:27,390
destroy objects but you'd only cleared

00:13:25,110 --> 00:13:32,130
the content of the objects and by this

00:13:27,390 --> 00:13:36,810
we were able yet to achieve a better

00:13:32,130 --> 00:13:38,580
scaling for example here we speed we

00:13:36,810 --> 00:13:43,260
increase the speed up from two point

00:13:38,580 --> 00:13:45,810
three to eight point nine and the zero

00:13:43,260 --> 00:13:50,700
run time was decreased from 72 seconds

00:13:45,810 --> 00:13:53,880
to 45 seconds so input so here in this

00:13:50,700 --> 00:13:57,000
example we were able to achieve a 22

00:13:53,880 --> 00:14:00,780
percent performance improvement for the

00:13:57,000 --> 00:14:02,910
overall runtime so yeah this was the

00:14:00,780 --> 00:14:05,400
idea of my talk to give you a short

00:14:02,910 --> 00:14:08,730
introduction and what we are able to do

00:14:05,400 --> 00:14:12,180
with openmp from the measurement system

00:14:08,730 --> 00:14:15,780
side scorpy and also give you an idea

00:14:12,180 --> 00:14:18,720
how we can visualize openmp performance

00:14:15,780 --> 00:14:21,950
data in the vampyre analysis tool so

00:14:18,720 --> 00:14:21,950

YouTube URL: https://www.youtube.com/watch?v=u9d2_j7MlDY


