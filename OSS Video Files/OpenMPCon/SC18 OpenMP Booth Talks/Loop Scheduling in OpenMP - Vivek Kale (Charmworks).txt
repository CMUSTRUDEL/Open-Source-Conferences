Title: Loop Scheduling in OpenMP - Vivek Kale (Charmworks)
Publication date: 2018-11-17
Playlist: SC18 OpenMP Booth Talks
Description: 
	SC18 OpenMP Booth Talk - November 13, 2018, Dallas TX
Slides at https://www.openmp.org/wp-content/uploads/SC18-BoothTalks-Kale.pdf
Captions: 
	00:00:00,000 --> 00:00:06,660
okay hi my name is Vivek Kalle I'm going

00:00:04,920 --> 00:00:09,750
to be talking to you about loop

00:00:06,660 --> 00:00:13,230
scheduling for OpenMP particular I'm

00:00:09,750 --> 00:00:16,619
going to talk about the basics of loop

00:00:13,230 --> 00:00:18,480
scheduling then go into a new

00:00:16,619 --> 00:00:22,320
user-defined loop scheduling strategy

00:00:18,480 --> 00:00:25,230
that's being proposed in the open MT

00:00:22,320 --> 00:00:27,949
standard after that I'll talk a little

00:00:25,230 --> 00:00:32,669
bit about work with the collaborator

00:00:27,949 --> 00:00:37,320
Florida verbena and Christian I want ski

00:00:32,669 --> 00:00:40,079
and I'll be then discussing some work on

00:00:37,320 --> 00:00:46,309
Rajah that I have been doing at Lawrence

00:00:40,079 --> 00:00:46,309
Livermore Lab with David Beckinsale and

00:00:48,620 --> 00:00:58,260
so I'll just go through the basics here

00:00:52,260 --> 00:01:02,160
first as I mentioned so first it's

00:00:58,260 --> 00:01:06,869
important to really understand the loop

00:01:02,160 --> 00:01:11,220
schedule clause in openmp and this is

00:01:06,869 --> 00:01:14,340
how loop scheduling is specified in the

00:01:11,220 --> 00:01:19,070
technical report 6 which is the

00:01:14,340 --> 00:01:23,060
foundation of the open MP 5.0 standard

00:01:19,070 --> 00:01:26,610
so this schedule Clause here is

00:01:23,060 --> 00:01:32,100
specified after this workshare construct

00:01:26,610 --> 00:01:37,829
for and it takes in a modifier which is

00:01:32,100 --> 00:01:42,920
70 sim D or no 70 and then takes a

00:01:37,829 --> 00:01:46,590
scheduled kind and then a chunk size so

00:01:42,920 --> 00:01:49,920
what to define what this is doing

00:01:46,590 --> 00:01:53,369
it's a specification of how iterations

00:01:49,920 --> 00:01:57,600
of associated loops are divided into

00:01:53,369 --> 00:01:59,899
contiguous non-empty subsets and those

00:01:57,600 --> 00:02:03,329
are called chunks simply put you're

00:01:59,899 --> 00:02:06,119
assigning the loop iterations to the

00:02:03,329 --> 00:02:08,369
threads that are in running in parallel

00:02:06,119 --> 00:02:13,769
in this parallel for loop

00:02:08,369 --> 00:02:16,680
and these these chunks are distributed

00:02:13,769 --> 00:02:19,440
essentially to this team of threads

00:02:16,680 --> 00:02:22,049
that's defined in the OpenMP standard

00:02:19,440 --> 00:02:26,659
and that's associated with each parallel

00:02:22,049 --> 00:02:33,120
loop there's four there's five different

00:02:26,659 --> 00:02:36,810
kinds of schedulers and those are each

00:02:33,120 --> 00:02:41,849
have their pluses and minuses the static

00:02:36,810 --> 00:02:45,930
schedule has the ability to pre-assigned

00:02:41,849 --> 00:02:48,620
loop iterations to threads but before

00:02:45,930 --> 00:02:53,359
the threaded computation region starts

00:02:48,620 --> 00:02:57,180
the dynamic and guided scheduling scheme

00:02:53,359 --> 00:03:00,540
assigned the loop iterations to the

00:02:57,180 --> 00:03:03,599
threats it during the run time during

00:03:00,540 --> 00:03:09,709
and that is within the parallel flow

00:03:03,599 --> 00:03:14,090
region and then the auto schedule is for

00:03:09,709 --> 00:03:18,359
specifying that you want the compiler to

00:03:14,090 --> 00:03:21,419
figure out the best loop scheduler loop

00:03:18,359 --> 00:03:29,099
scheduling strategy for a particular

00:03:21,419 --> 00:03:35,849
loop run time is a time that is made

00:03:29,099 --> 00:03:40,769
that makes the openmp run time associate

00:03:35,849 --> 00:03:47,060
thus the parallel region with a run time

00:03:40,769 --> 00:03:54,410
determined OpenMP loop schedule so this

00:03:47,060 --> 00:03:58,199
all of these kinds are are currently the

00:03:54,410 --> 00:04:02,699
are implemented in many different OpenMP

00:03:58,199 --> 00:04:05,280
compilers and runtime libraries one

00:04:02,699 --> 00:04:10,919
issue though is that really there aren't

00:04:05,280 --> 00:04:14,940
that many loop schedules the kinds to be

00:04:10,919 --> 00:04:17,370
sufficient to and to allow for high

00:04:14,940 --> 00:04:20,190
performance for many application codes

00:04:17,370 --> 00:04:24,000
especially sophisticated

00:04:20,190 --> 00:04:26,120
applications with modern running on

00:04:24,000 --> 00:04:32,700
modern architectures with performance

00:04:26,120 --> 00:04:36,710
variability in the hardware and so what

00:04:32,700 --> 00:04:40,980
we propose is to have a user-defined

00:04:36,710 --> 00:04:46,530
scheduling strategy or user-defined

00:04:40,980 --> 00:04:50,220
schedule to allow the computer scientist

00:04:46,530 --> 00:04:53,970
that's very knowledgeable about the loop

00:04:50,220 --> 00:04:57,210
scheduling strategies or just scheduling

00:04:53,970 --> 00:05:00,510
algorithms in general to easily put in a

00:04:57,210 --> 00:05:03,600
new loop schedule within the application

00:05:00,510 --> 00:05:05,280
code and so this allows for rapid

00:05:03,600 --> 00:05:09,390
development of loop scheduling

00:05:05,280 --> 00:05:11,370
strategies so that's what we want and so

00:05:09,390 --> 00:05:15,680
just to go a little deeper into this

00:05:11,370 --> 00:05:18,540
idea the reason that you want to do this

00:05:15,680 --> 00:05:22,620
you don't want to just put in new loop

00:05:18,540 --> 00:05:28,410
scheduling strategies inside an open MP

00:05:22,620 --> 00:05:31,530
compiler is that it's really hard to go

00:05:28,410 --> 00:05:37,740
into many of the open implementations

00:05:31,530 --> 00:05:42,120
such as ll VMs our Lib Dom and it

00:05:37,740 --> 00:05:46,380
requires a lot of code changes to to

00:05:42,120 --> 00:05:50,280
really make the whole idea of loop

00:05:46,380 --> 00:05:56,580
scheduling strategy work so it's best to

00:05:50,280 --> 00:05:58,860
just take one attack at it and it and do

00:05:56,580 --> 00:06:01,710
a user-defined scheduling strategy that

00:05:58,860 --> 00:06:04,890
you can then bring up the development

00:06:01,710 --> 00:06:10,620
for to the computer scientist or the

00:06:04,890 --> 00:06:14,490
user and there's also a lot of emergence

00:06:10,620 --> 00:06:17,370
of threaded runtime systems and that's

00:06:14,490 --> 00:06:19,590
another reason that we need these loop

00:06:17,370 --> 00:06:21,710
scheduling the user-defined loop

00:06:19,590 --> 00:06:24,810
scheduling strategies and OpenMP because

00:06:21,710 --> 00:06:30,270
OpenMP needs to be able to compete with

00:06:24,810 --> 00:06:32,039
some of these novel runtime systems that

00:06:30,270 --> 00:06:32,889
have some snooze scheduling strategies

00:06:32,039 --> 00:06:37,120
in them

00:06:32,889 --> 00:06:40,960
and again note that the keywords auto

00:06:37,120 --> 00:06:42,759
and runtime they aren't adequate if they

00:06:40,960 --> 00:06:44,979
don't allow for the user level

00:06:42,759 --> 00:06:48,189
scheduling because you simply cannot

00:06:44,979 --> 00:06:53,039
have a user tell specify it say through

00:06:48,189 --> 00:06:57,969
it in through an environment variable or

00:06:53,039 --> 00:07:02,110
any other way it's so really there's

00:06:57,969 --> 00:07:05,729
nothing in the openmp specification that

00:07:02,110 --> 00:07:05,729
does what we're trying to do right now

00:07:05,819 --> 00:07:11,979
just make a quick segue on use case

00:07:10,120 --> 00:07:17,550
where user-defined scheduling is

00:07:11,979 --> 00:07:21,669
important intel has customer with a

00:07:17,550 --> 00:07:23,680
specific need for stat a scheduling

00:07:21,669 --> 00:07:27,370
strategy called static stealing and

00:07:23,680 --> 00:07:29,500
they've Intel is to create their own

00:07:27,370 --> 00:07:32,770
loop scheduled loop scheduling strategy

00:07:29,500 --> 00:07:36,370
by extending the Intel runtime library

00:07:32,770 --> 00:07:40,360
but it's very cumbersome to use that new

00:07:36,370 --> 00:07:42,159
loop schedule and it's definitely not

00:07:40,360 --> 00:07:42,879
portable across off whatever yield one

00:07:42,159 --> 00:07:48,219
patience

00:07:42,879 --> 00:07:52,719
so the user-defined loop scheduling

00:07:48,219 --> 00:07:53,229
scheme that we propose looks like the

00:07:52,719 --> 00:07:58,900
following

00:07:53,229 --> 00:08:04,300
so the here what you see is that the

00:07:58,900 --> 00:08:07,089
user defines these functions might

00:08:04,300 --> 00:08:13,839
didn't start why didn't next and my

00:08:07,089 --> 00:08:16,180
didn't Binney and so the the my didn't

00:08:13,839 --> 00:08:18,129
in it allows the user level scheduler to

00:08:16,180 --> 00:08:20,339
allocate and and initialize the data

00:08:18,129 --> 00:08:24,370
structures that are to be used across

00:08:20,339 --> 00:08:26,860
multiple parallel loops and the

00:08:24,370 --> 00:08:29,319
functions Mize didn't start and widen

00:08:26,860 --> 00:08:33,219
next they determined during the

00:08:29,319 --> 00:08:35,649
application execution how loop

00:08:33,219 --> 00:08:40,029
iterations are on queued on to the

00:08:35,649 --> 00:08:43,260
shared queue of the for the loop

00:08:40,029 --> 00:08:47,000
scheduling scheme and then

00:08:43,260 --> 00:08:50,930
how their DQ'd from that shared queue

00:08:47,000 --> 00:08:53,970
through this my debt did next

00:08:50,930 --> 00:08:58,620
during as each thread pulls iterations

00:08:53,970 --> 00:09:03,960
from the shared queue and so with these

00:08:58,620 --> 00:09:06,240
three functions you can create any loop

00:09:03,960 --> 00:09:11,870
scheduling strategy that you want as

00:09:06,240 --> 00:09:16,460
long as you define how to initialize a

00:09:11,870 --> 00:09:20,690
shared queue and you define how to DQ

00:09:16,460 --> 00:09:25,140
loop iterations from that shared you and

00:09:20,690 --> 00:09:29,840
then of course just destroy the shared

00:09:25,140 --> 00:09:35,940
queue and clean up do any garbage

00:09:29,840 --> 00:09:43,380
Cleanup that you need to do and so with

00:09:35,940 --> 00:09:47,480
that your your functions then are they

00:09:43,380 --> 00:09:52,080
use this declares clause as you can see

00:09:47,480 --> 00:09:56,370
there's a declare horse for that

00:09:52,080 --> 00:10:03,980
schedule name which is my din each that

00:09:56,370 --> 00:10:07,470
the my din is the name here and that

00:10:03,980 --> 00:10:12,320
schedule kind is essentially declared

00:10:07,470 --> 00:10:17,460
through this definition right here and

00:10:12,320 --> 00:10:20,580
then you have a start start clause here

00:10:17,460 --> 00:10:26,000
for two for associating that schedule

00:10:20,580 --> 00:10:27,870
kind with the schedule with this

00:10:26,000 --> 00:10:31,890
initialization function for that

00:10:27,870 --> 00:10:36,600
scheduled time and then likewise for

00:10:31,890 --> 00:10:40,740
their other functions now here's where

00:10:36,600 --> 00:10:44,000
you the schedule kind of in this example

00:10:40,740 --> 00:10:46,890
code you have you have this parallel for

00:10:44,000 --> 00:10:49,230
I'll just point out that in this

00:10:46,890 --> 00:10:52,110
particular user-defined schedule there's

00:10:49,230 --> 00:10:55,980
actually a history variable so that you

00:10:52,110 --> 00:10:59,130
can you can store

00:10:55,980 --> 00:11:04,680
the timings of loop iterations from

00:10:59,130 --> 00:11:06,840
previous loop indications from previous

00:11:04,680 --> 00:11:10,470
application time steps for example and

00:11:06,840 --> 00:11:13,590
so this this is the hist this is a

00:11:10,470 --> 00:11:17,310
history variable that the user-defined

00:11:13,590 --> 00:11:20,040
schedule uses and so now with this you

00:11:17,310 --> 00:11:23,450
have this new parallel for new scheduled

00:11:20,040 --> 00:11:27,120
kind might in with the identifier and

00:11:23,450 --> 00:11:30,540
and then we have the user-defined

00:11:27,120 --> 00:11:35,300
scheduling parameters up sizes one just

00:11:30,540 --> 00:11:41,700
like your static and dynamic scheduling

00:11:35,300 --> 00:11:46,770
and then you pull and separate the each

00:11:41,700 --> 00:11:50,180
other parameter by you call and separate

00:11:46,770 --> 00:11:54,570
each of the other parameters so that you

00:11:50,180 --> 00:11:56,910
can specify us as many scheduling

00:11:54,570 --> 00:12:02,610
parameters as you want in this user fund

00:11:56,910 --> 00:12:07,440
schedule so one key point is that all of

00:12:02,610 --> 00:12:10,110
this is actually not anything completely

00:12:07,440 --> 00:12:14,250
out of the ordinary from the OpenMP

00:12:10,110 --> 00:12:19,740
language committees point of view this

00:12:14,250 --> 00:12:23,100
is actually for this this methodology of

00:12:19,740 --> 00:12:27,980
declaring a new schedule and then using

00:12:23,100 --> 00:12:32,190
that schedule in in a scheduling in a

00:12:27,980 --> 00:12:37,160
parallel for region is is actually this

00:12:32,190 --> 00:12:39,750
very similar to how you you use

00:12:37,160 --> 00:12:42,360
user-defined reductions which are a new

00:12:39,750 --> 00:12:47,580
feature in OpenMP 5.0 so we've followed

00:12:42,360 --> 00:12:50,910
very closely their syntax and it should

00:12:47,580 --> 00:12:55,860
be easily or easily integrated into

00:12:50,910 --> 00:13:04,940
OpenMP because of that this is an

00:12:55,860 --> 00:13:04,940
example of a scheduling strategy that's

00:13:04,960 --> 00:13:11,510
that I developed for my thesis during my

00:13:08,810 --> 00:13:14,950
dissertation work and it's a static

00:13:11,510 --> 00:13:18,529
static dynamics loop scheduling scheme I

00:13:14,950 --> 00:13:20,960
I'd used this same syntax here this is

00:13:18,529 --> 00:13:23,600
the application loop specifying the loop

00:13:20,960 --> 00:13:31,390
scheduling scheme and I'm just showing

00:13:23,600 --> 00:13:38,540
you the details here of the actual in it

00:13:31,390 --> 00:13:43,480
next and then the finis function and so

00:13:38,540 --> 00:13:49,310
I won't go into detail on these but this

00:13:43,480 --> 00:13:52,670
this API that I defined really makes it

00:13:49,310 --> 00:13:55,010
easy to implement my static dynamic

00:13:52,670 --> 00:13:58,460
scheduling scheme in this way and you

00:13:55,010 --> 00:14:03,080
can do this for other novel scheduling

00:13:58,460 --> 00:14:07,310
strategies that are that may have been

00:14:03,080 --> 00:14:10,010
developed not in openmp but in another

00:14:07,310 --> 00:14:22,670
context they just you know pure pthreads

00:14:10,010 --> 00:14:29,360
scheduling and so so with that there we

00:14:22,670 --> 00:14:32,870
are so with that I I wanted to point out

00:14:29,360 --> 00:14:38,110
just a timeline of where we are right

00:14:32,870 --> 00:14:41,839
now i I'll just mention quickly that

00:14:38,110 --> 00:14:44,860
this the work on user-defined scheduling

00:14:41,839 --> 00:14:49,190
was actually presented in October 2017

00:14:44,860 --> 00:14:52,700
the openmp cons Stoneybrook and I've

00:14:49,190 --> 00:14:58,250
actually spoken with Michael club love

00:14:52,700 --> 00:15:02,240
few a few years before this OpenMP con

00:14:58,250 --> 00:15:04,160
conference we act we were talking about

00:15:02,240 --> 00:15:07,010
user we were talking about scheduling

00:15:04,160 --> 00:15:10,150
strategies but the idea of user-defined

00:15:07,010 --> 00:15:14,030
scheduling strategies really came about

00:15:10,150 --> 00:15:15,270
around open MB con when I presented this

00:15:14,030 --> 00:15:18,450
proposal then

00:15:15,270 --> 00:15:22,410
and then we formalize a proposal more in

00:15:18,450 --> 00:15:25,440
Austin this past year and from during

00:15:22,410 --> 00:15:30,200
that meeting in Austin we actually got

00:15:25,440 --> 00:15:33,510
some people from Intel's LLVM group to

00:15:30,200 --> 00:15:38,450
integrate the user-defined scheduling

00:15:33,510 --> 00:15:41,160
seem into their compiler and runtime so

00:15:38,450 --> 00:15:46,200
right so since then we've been

00:15:41,160 --> 00:15:49,170
continuously talking about examples for

00:15:46,200 --> 00:15:52,050
user-defined schedules and but more

00:15:49,170 --> 00:15:59,610
recently I've worked with Florina

00:15:52,050 --> 00:16:01,860
Corvina and chin are on ski and they

00:15:59,610 --> 00:16:06,810
have been very instrumental in helping

00:16:01,860 --> 00:16:10,500
with testing and thinking about news

00:16:06,810 --> 00:16:14,490
loop scheduling strategies that are much

00:16:10,500 --> 00:16:17,149
more sophisticated fancy than the ones

00:16:14,490 --> 00:16:22,920
that I've worked on during my

00:16:17,149 --> 00:16:27,050
dissertation teeth and so what we're

00:16:22,920 --> 00:16:35,070
going to go is in January 2019 we aim to

00:16:27,050 --> 00:16:39,500
have a ticket past for at a vote in

00:16:35,070 --> 00:16:46,579
Santa Clara this will be end of January

00:16:39,500 --> 00:16:51,600
we will be if assuming that it is passed

00:16:46,579 --> 00:16:55,589
we will aim to have users test and

00:16:51,600 --> 00:16:59,520
support the proposal around that time

00:16:55,589 --> 00:17:03,570
and afterwards so some of these users

00:16:59,520 --> 00:17:11,480
will actually be myself and fluorine as

00:17:03,570 --> 00:17:15,569
well I'll just now I'll switch to

00:17:11,480 --> 00:17:18,750
another use of the loop scheduling

00:17:15,569 --> 00:17:21,870
strategies that I've developed as in

00:17:18,750 --> 00:17:25,589
particular the user defines loop

00:17:21,870 --> 00:17:27,289
scheduling strategy in the raca

00:17:25,589 --> 00:17:30,739
framework at

00:17:27,289 --> 00:17:35,179
at Lawrence Livermore National Lab so

00:17:30,739 --> 00:17:39,110
what is Raja it's just a framework or a

00:17:35,179 --> 00:17:41,749
library that helps scientists at do elia

00:17:39,110 --> 00:17:44,809
labs write programs with parallelizable

00:17:41,749 --> 00:17:48,259
loops that are portable across different

00:17:44,809 --> 00:17:50,779
architectures and Lulla is one good

00:17:48,259 --> 00:17:53,690
example of mini app and use case

00:17:50,779 --> 00:17:55,309
demonstrating Raja it has several OpenMP

00:17:53,690 --> 00:17:56,899
parallel loops with irregular

00:17:55,309 --> 00:18:02,929
parallelism and there's load imbalance

00:17:56,899 --> 00:18:06,460
across across threads during the open MP

00:18:02,929 --> 00:18:10,190
parallel for execution

00:18:06,460 --> 00:18:15,950
so Raja has several policies or

00:18:10,190 --> 00:18:18,080
strategies that are used to schedule

00:18:15,950 --> 00:18:23,600
iterations of loops to cores and this

00:18:18,080 --> 00:18:28,899
doesn't mean just policies it for openmp

00:18:23,600 --> 00:18:34,059
this is policies for say open ACC's

00:18:28,899 --> 00:18:38,539
parallel parallel loops construct this

00:18:34,059 --> 00:18:42,889
this framework allows you to really try

00:18:38,539 --> 00:18:47,960
a lot of different strategies really

00:18:42,889 --> 00:18:52,970
easily within just one switch of a

00:18:47,960 --> 00:18:55,369
policy or and you can really try out a

00:18:52,970 --> 00:19:03,230
lot of techniques without trying to go

00:18:55,369 --> 00:19:05,539
deep into the the the applications it

00:19:03,230 --> 00:19:09,289
rut or the innards of the application

00:19:05,539 --> 00:19:12,350
and so I've worked with David Beckinsale

00:19:09,289 --> 00:19:18,230
from Lawrence Livermore Lab to address

00:19:12,350 --> 00:19:21,649
this to address how we can integrate the

00:19:18,230 --> 00:19:26,480
user-defined schedules into the rajah

00:19:21,649 --> 00:19:32,749
library so that we can make portable

00:19:26,480 --> 00:19:34,909
codes that that will be able to work for

00:19:32,749 --> 00:19:36,879
not just different architectures

00:19:34,909 --> 00:19:38,160
currently but next-generation

00:19:36,879 --> 00:19:44,340
architectures

00:19:38,160 --> 00:19:49,280
for excess scales architectures and I'll

00:19:44,340 --> 00:19:53,700
just give you an overview of what the

00:19:49,280 --> 00:19:56,640
what we have done in comparison to what

00:19:53,700 --> 00:20:00,060
has been done in past so this is an

00:19:56,640 --> 00:20:02,790
example of an MPI plus OpenMP code using

00:20:00,060 --> 00:20:07,220
this static dynamics loop scheduling

00:20:02,790 --> 00:20:13,620
scheme that I've mentioned earlier and

00:20:07,220 --> 00:20:17,370
this code can be transformed into this

00:20:13,620 --> 00:20:22,140
rajah library using this implementation

00:20:17,370 --> 00:20:25,110
of this policy that we called um w l WS

00:20:22,140 --> 00:20:28,110
in this stamp this is a short form for

00:20:25,110 --> 00:20:33,240
you the user-defined scheduling we did

00:20:28,110 --> 00:20:35,970
not actually call it um UDS - because

00:20:33,240 --> 00:20:39,780
this is actually a prototype of what we

00:20:35,970 --> 00:20:45,900
actually want to do for the end result

00:20:39,780 --> 00:20:48,450
of on UDS so we are we're go we've this

00:20:45,900 --> 00:20:51,030
is look in the rajah library you can try

00:20:48,450 --> 00:20:55,530
this policy out and see whether it works

00:20:51,030 --> 00:20:58,020
for you it is currently in testing right

00:20:55,530 --> 00:21:00,930
now but the key thing is you see all

00:20:58,020 --> 00:21:06,300
this code here but now when you write it

00:21:00,930 --> 00:21:09,630
in Raja all this code has been really

00:21:06,300 --> 00:21:14,550
just minimized to this amount of code

00:21:09,630 --> 00:21:17,670
and that's really the key of Raja is

00:21:14,550 --> 00:21:22,950
that it really makes it easy to write

00:21:17,670 --> 00:21:26,630
loop based codes of a very expressed

00:21:22,950 --> 00:21:31,800
with a lot of expressive limit T and

00:21:26,630 --> 00:21:36,810
it's it allows you to get portable

00:21:31,800 --> 00:21:40,400
performance very easily so so this with

00:21:36,810 --> 00:21:42,890
what we've shown here

00:21:40,400 --> 00:21:48,320
we think that there's a lot of hope for

00:21:42,890 --> 00:21:50,000
a you easy-to-use loop scheduling

00:21:48,320 --> 00:21:59,090
strategy with user-defined loop

00:21:50,000 --> 00:22:01,790
scheduling and and and here are just

00:21:59,090 --> 00:22:04,820
some results that we've got and we're

00:22:01,790 --> 00:22:06,770
currently evaluating raja on lou leche

00:22:04,820 --> 00:22:12,550
right now we've done some

00:22:06,770 --> 00:22:16,040
experimentation on basic benchmarks like

00:22:12,550 --> 00:22:19,310
Jacobi but there's actually several

00:22:16,040 --> 00:22:21,350
others that we already have implemented

00:22:19,310 --> 00:22:22,730
and we're getting results for those and

00:22:21,350 --> 00:22:27,380
they're showing some promising results

00:22:22,730 --> 00:22:29,960
so I think this idea is promising

00:22:27,380 --> 00:22:31,850
direction for Raja and for the work on

00:22:29,960 --> 00:22:36,130
user-defined schedules actually being

00:22:31,850 --> 00:22:44,890
used in a portable manner

00:22:36,130 --> 00:22:48,770
and so what the one thing is that the

00:22:44,890 --> 00:22:51,680
idea of this lightweight scheduling

00:22:48,770 --> 00:22:53,420
library is to really make it formalized

00:22:51,680 --> 00:22:56,750
with this user-defined the scheduling

00:22:53,420 --> 00:22:58,550
strategy library note that user-defined

00:22:56,750 --> 00:23:01,010
scheduling is actually not in these

00:22:58,550 --> 00:23:04,370
openmp standards so you actually can't

00:23:01,010 --> 00:23:09,530
even use user-defined schedules right

00:23:04,370 --> 00:23:12,470
now but after January of 2019 we hope

00:23:09,530 --> 00:23:15,440
that you can we can actually put

00:23:12,470 --> 00:23:18,440
user-defined schedules within the Raja

00:23:15,440 --> 00:23:21,230
framework so this is something that

00:23:18,440 --> 00:23:24,830
we're we're hoping can come together

00:23:21,230 --> 00:23:26,750
then and really make this very

00:23:24,830 --> 00:23:32,840
interesting especially in the OpenMP

00:23:26,750 --> 00:23:35,780
community and so I think the really key

00:23:32,840 --> 00:23:39,890
point here is that we're going to use

00:23:35,780 --> 00:23:43,840
Lawrence Livermore labs Apollo to

00:23:39,890 --> 00:23:48,380
Autotune the loop scheduling libraries

00:23:43,840 --> 00:23:51,230
dynamic scheduling parameters and that's

00:23:48,380 --> 00:23:52,390
going to allow for making it even easier

00:23:51,230 --> 00:23:54,190
to use these

00:23:52,390 --> 00:23:58,840
scheduling strategies in this portable

00:23:54,190 --> 00:24:01,300
manner so and I'll just say I think

00:23:58,840 --> 00:24:04,390
another in a a commonplace example of

00:24:01,300 --> 00:24:07,480
that is chunk size rather than having

00:24:04,390 --> 00:24:09,610
the user try different chunk size we're

00:24:07,480 --> 00:24:11,970
actually going to have a frame we have a

00:24:09,610 --> 00:24:14,950
framework that enables us to try

00:24:11,970 --> 00:24:19,440
different chunk sizes and it's it's

00:24:14,950 --> 00:24:25,330
based on machine learning framework and

00:24:19,440 --> 00:24:26,770
some other AI based techniques that if

00:24:25,330 --> 00:24:32,370
that has been developed at Lawrence

00:24:26,770 --> 00:24:32,370
Livermore again by David Beckinsale and

00:24:37,110 --> 00:24:49,660
so I'll just say some well not finally I

00:24:41,560 --> 00:24:53,800
will give a brief plug on the charm plus

00:24:49,660 --> 00:24:57,310
plus plus CK loop work that I've done in

00:24:53,800 --> 00:24:59,890
that's involved integrating a loop

00:24:57,310 --> 00:25:04,300
scheduling library within the charm plus

00:24:59,890 --> 00:25:08,800
plus runtime system this is this has

00:25:04,300 --> 00:25:14,370
been in a part of building with charm

00:25:08,800 --> 00:25:17,530
plus plus version 6.9 and it's uh it's a

00:25:14,370 --> 00:25:20,560
what we've done is essentially created a

00:25:17,530 --> 00:25:23,830
loop scheduling library that has gets

00:25:20,560 --> 00:25:25,870
feedback from the load that can get

00:25:23,830 --> 00:25:29,320
feedback from the load balancing of

00:25:25,870 --> 00:25:31,830
charm plus plus so the measurements of

00:25:29,320 --> 00:25:36,460
load imbalance of charm plus plus a

00:25:31,830 --> 00:25:41,110
across application time steps can

00:25:36,460 --> 00:25:45,190
actually give some idea of how the loop

00:25:41,110 --> 00:25:48,730
scheduling will perform best and this

00:25:45,190 --> 00:25:51,820
state can hold true for measurements

00:25:48,730 --> 00:25:57,310
from the load imbalance across threads

00:25:51,820 --> 00:25:58,640
or cores and that can influence how load

00:25:57,310 --> 00:26:04,630
balancing happens

00:25:58,640 --> 00:26:09,400
for a cross note look load imbalance and

00:26:04,630 --> 00:26:17,059
some of this work was actually published

00:26:09,400 --> 00:26:20,600
SC 17 as and it's in an extended I

00:26:17,059 --> 00:26:24,590
abstract on online if you'd like to look

00:26:20,600 --> 00:26:28,610
at our details and it was a best Award

00:26:24,590 --> 00:26:33,730
candidate an essay 17 we're all working

00:26:28,610 --> 00:26:38,360
on this and I'm working with Mathias and

00:26:33,730 --> 00:26:40,790
now he thunders take her to the further

00:26:38,360 --> 00:26:47,750
developed a loop scheduling library code

00:26:40,790 --> 00:26:51,799
so it's a it can be an alternative to MP

00:26:47,750 --> 00:26:54,679
but we hope to have some sort of nice

00:26:51,799 --> 00:26:57,350
integration or discussions and be

00:26:54,679 --> 00:27:00,350
community on some of the work that we've

00:26:57,350 --> 00:27:04,760
been doing here on that loop scheduling

00:27:00,350 --> 00:27:07,190
library especially talking in terms of

00:27:04,760 --> 00:27:09,290
the user-defined schedules and how we

00:27:07,190 --> 00:27:11,990
could map some of the scheduling

00:27:09,290 --> 00:27:15,770
strategies that we've developed in the

00:27:11,990 --> 00:27:23,570
CK loops scheduling library into a

00:27:15,770 --> 00:27:25,730
user-defined schedule and I think I'll

00:27:23,570 --> 00:27:29,919
just leave it up for questions the rest

00:27:25,730 --> 00:27:29,919

YouTube URL: https://www.youtube.com/watch?v=rvRVoOuzJ88


