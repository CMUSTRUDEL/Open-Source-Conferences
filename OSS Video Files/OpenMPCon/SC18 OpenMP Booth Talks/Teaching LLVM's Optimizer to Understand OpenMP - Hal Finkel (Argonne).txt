Title: Teaching LLVM's Optimizer to Understand OpenMP - Hal Finkel (Argonne)
Publication date: 2018-11-17
Playlist: SC18 OpenMP Booth Talks
Description: 
	SC18 OpenMP Booth Talk - November 13, 2018, Dallas TX
https://www.openmp.org/wp-content/uploads/SC18-BoothTalks-Finkel.pdf
Captions: 
	00:00:03,170 --> 00:00:09,570
okay good afternoon everyone

00:00:07,170 --> 00:00:11,639
my name is Hal Finkel I'm from Argonne

00:00:09,570 --> 00:00:13,799
National Laboratory leadership computing

00:00:11,639 --> 00:00:15,509
facility and I'm going to be talking to

00:00:13,799 --> 00:00:18,240
you about some work that we've been

00:00:15,509 --> 00:00:20,160
doing on openmp the goal of this work is

00:00:18,240 --> 00:00:22,529
to teach LVM to optimizer to understand

00:00:20,160 --> 00:00:23,820
OpenMP and I'm gonna talk to you a

00:00:22,529 --> 00:00:25,859
little bit about that but in some sense

00:00:23,820 --> 00:00:27,539
I'm gonna mostly tell you why we want to

00:00:25,859 --> 00:00:31,439
see chaldeans optimizer to understand

00:00:27,539 --> 00:00:32,070
OpenMP and and what are the benefits of

00:00:31,439 --> 00:00:34,770
doing so

00:00:32,070 --> 00:00:36,149
alright and I should start by saying

00:00:34,770 --> 00:00:39,719
that the work that I'm going to present

00:00:36,149 --> 00:00:41,700
is mostly the product of Johanna sofort

00:00:39,719 --> 00:00:46,440
who is a postdoctoral researcher at

00:00:41,700 --> 00:00:49,260
Argonne who works on my team okay good

00:00:46,440 --> 00:00:50,760
so to explain why we want to eat gel

00:00:49,260 --> 00:00:53,219
bands optimize their bite OpenMP and

00:00:50,760 --> 00:00:58,410
what that means first let me start by

00:00:53,219 --> 00:01:00,510
explaining how a compiler implements

00:00:58,410 --> 00:01:01,980
openmp like what does that what does

00:01:00,510 --> 00:01:04,379
that mean and how does it work

00:01:01,980 --> 00:01:07,070
so first here I have a very simple

00:01:04,379 --> 00:01:09,180
example and this example has a for loop

00:01:07,070 --> 00:01:10,770
over here oh man I can't point on this

00:01:09,180 --> 00:01:14,790
thing anyway so this is a it's a for

00:01:10,770 --> 00:01:18,479
loop and the one above the for loop

00:01:14,790 --> 00:01:20,520
there's an open MP pragma so that pragma

00:01:18,479 --> 00:01:22,560
is going to be transformed by the

00:01:20,520 --> 00:01:24,299
compiler into well something to do with

00:01:22,560 --> 00:01:26,670
parallelism obviously right that has to

00:01:24,299 --> 00:01:30,030
happen somehow so the question is how

00:01:26,670 --> 00:01:31,350
does that how does that work and here I

00:01:30,030 --> 00:01:34,110
woke up exam I'm gonna come back to that

00:01:31,350 --> 00:01:37,710
in a second so what happens here is that

00:01:34,110 --> 00:01:40,229
when the compiler generates code for

00:01:37,710 --> 00:01:43,649
this pragma it's essentially doing is it

00:01:40,229 --> 00:01:46,439
is taking the the code that's inside

00:01:43,649 --> 00:01:48,390
that pearl loop and it is somehow

00:01:46,439 --> 00:01:50,040
putting it into a separate function and

00:01:48,390 --> 00:01:52,920
that separate function is then being

00:01:50,040 --> 00:01:55,579
called by the OpenMP runtime library on

00:01:52,920 --> 00:01:59,130
multiple threads and that's how the

00:01:55,579 --> 00:02:02,939
OpenMP implementation implements this

00:01:59,130 --> 00:02:05,579
OpenMP pragma so you know what happens

00:02:02,939 --> 00:02:08,009
however is that this process of taking a

00:02:05,579 --> 00:02:11,039
code kind of out of the function in

00:02:08,009 --> 00:02:12,660
which it resided originally and moving

00:02:11,039 --> 00:02:13,560
into the separate function while

00:02:12,660 --> 00:02:16,050
necessary

00:02:13,560 --> 00:02:19,800
in order to make it run in parallel also

00:02:16,050 --> 00:02:24,530
has some undesirable side effects and so

00:02:19,800 --> 00:02:27,000
what happens is that there are

00:02:24,530 --> 00:02:29,690
optimizations so in this particular case

00:02:27,000 --> 00:02:31,980
for example there is an optimization

00:02:29,690 --> 00:02:34,640
that we would call constant propagation

00:02:31,980 --> 00:02:37,890
that it might be inhibited by this

00:02:34,640 --> 00:02:40,170
transformation so here I have a very

00:02:37,890 --> 00:02:42,860
quick example and the outputs here are

00:02:40,170 --> 00:02:45,150
our conceptual they're pseudocode but

00:02:42,860 --> 00:02:47,250
what's going on here is here we have

00:02:45,150 --> 00:02:48,799
this value of y here in this example and

00:02:47,250 --> 00:02:52,440
it's set equal to some constant number

00:02:48,799 --> 00:02:54,510
and and then later on you see we're

00:02:52,440 --> 00:02:58,709
calling these various functions with Y

00:02:54,510 --> 00:03:00,810
and nothing changes Y in this in this

00:02:58,709 --> 00:03:03,150
case but what's being chuckling or

00:03:00,810 --> 00:03:06,060
conceptually is that we will once the

00:03:03,150 --> 00:03:08,910
compiler moves the code there into this

00:03:06,060 --> 00:03:11,280
other function and calls it through the

00:03:08,910 --> 00:03:13,170
runtime library the compiler kind of

00:03:11,280 --> 00:03:16,260
internally loses track of the fact that

00:03:13,170 --> 00:03:18,150
nothing is changing Y and so stops doing

00:03:16,260 --> 00:03:19,890
this constant propagation so this is

00:03:18,150 --> 00:03:21,769
just sort of one of many examples of one

00:03:19,890 --> 00:03:24,690
of the kinds of things that can happen

00:03:21,769 --> 00:03:27,120
when you when you do this so there are

00:03:24,690 --> 00:03:30,180
other ways of dealing with this for time

00:03:27,120 --> 00:03:32,930
purposes all sort of skip this but but

00:03:30,180 --> 00:03:34,819
this is a one of these inhibitions

00:03:32,930 --> 00:03:38,400
optimization conditions that is

00:03:34,819 --> 00:03:40,709
triggered by the use of OpenMP which is

00:03:38,400 --> 00:03:43,410
why you may have seen on the title slide

00:03:40,709 --> 00:03:46,230
of the talk the subtitle here is why it

00:03:43,410 --> 00:03:49,230
is sometimes adding OpenMP slow down my

00:03:46,230 --> 00:03:51,600
code and and this is you know it's one

00:03:49,230 --> 00:03:53,459
of these kind of dirty secrets of you

00:03:51,600 --> 00:03:56,010
know HPC benchmarking right is it often

00:03:53,459 --> 00:03:58,590
times you know people will compile their

00:03:56,010 --> 00:04:01,549
code with open MPD or some other kind of

00:03:58,590 --> 00:04:04,500
parallelism enabled they will show you

00:04:01,549 --> 00:04:06,329
how that code performs on you know one

00:04:04,500 --> 00:04:08,819
thread and two threads and four threads

00:04:06,329 --> 00:04:10,560
etc and the secret is however that if

00:04:08,819 --> 00:04:12,810
they had turned off all the parallels

00:04:10,560 --> 00:04:14,609
and run it on one core it would have

00:04:12,810 --> 00:04:16,019
been faster than maybe the one core

00:04:14,609 --> 00:04:17,430
version with parallelism but also the

00:04:16,019 --> 00:04:18,389
two core version with parallelism it may

00:04:17,430 --> 00:04:20,370
be the four core version with

00:04:18,389 --> 00:04:21,539
parallelism right and you know at some

00:04:20,370 --> 00:04:23,789
point the parallel version would have

00:04:21,539 --> 00:04:25,669
been faster but but it may not be you

00:04:23,789 --> 00:04:29,330
know quite as nice as the

00:04:25,669 --> 00:04:30,620
you know results indicate okay so see

00:04:29,330 --> 00:04:31,939
here let me talk about another thing

00:04:30,620 --> 00:04:34,849
that we've been looking at this is this

00:04:31,939 --> 00:04:37,520
this problem with implicit barriers so

00:04:34,849 --> 00:04:42,110
here I have a function and this function

00:04:37,520 --> 00:04:44,509
has a has a OpenMP for loop on it here I

00:04:42,110 --> 00:04:47,060
have another function this function also

00:04:44,509 --> 00:04:49,610
has the for loop in it and again there

00:04:47,060 --> 00:04:53,090
is an open and PE prog mode that has a

00:04:49,610 --> 00:04:55,069
parallel region and that has a for loop

00:04:53,090 --> 00:04:56,840
work sharing directive in it and these

00:04:55,069 --> 00:04:58,460
things will cause the iterations from

00:04:56,840 --> 00:05:00,110
these loops in order to be you know to

00:04:58,460 --> 00:05:01,639
be split up among the various threads

00:05:00,110 --> 00:05:02,719
and they all do their various pieces and

00:05:01,639 --> 00:05:05,240
all of that seems to work pretty well

00:05:02,719 --> 00:05:06,409
all right but you know and one thing to

00:05:05,240 --> 00:05:08,180
realize however is that there's a

00:05:06,409 --> 00:05:10,219
barrier that takes place at the end of

00:05:08,180 --> 00:05:12,199
all these loops because each thread by

00:05:10,219 --> 00:05:14,120
default does it's part of the iteration

00:05:12,199 --> 00:05:15,139
space and then it stops and it waits for

00:05:14,120 --> 00:05:16,370
all the other threads to finish there

00:05:15,139 --> 00:05:19,430
are parts of the iteration space and

00:05:16,370 --> 00:05:21,349
then only then will it keep going so you

00:05:19,430 --> 00:05:23,509
know what happens is that you know in

00:05:21,349 --> 00:05:25,909
the original code you see I mean I I put

00:05:23,509 --> 00:05:27,589
all these pragmas here on top of these

00:05:25,909 --> 00:05:29,419
loops right but these loops are actually

00:05:27,589 --> 00:05:31,669
in functions and those functions are

00:05:29,419 --> 00:05:34,069
called by some other piece of code

00:05:31,669 --> 00:05:35,270
that's generically what happens and so

00:05:34,069 --> 00:05:36,409
here it's an example of something that

00:05:35,270 --> 00:05:39,349
happened this is one of the benchmarks

00:05:36,409 --> 00:05:41,149
I'll show you later and here I have some

00:05:39,349 --> 00:05:44,180
loop that has a number of iterations in

00:05:41,149 --> 00:05:46,460
it and it calls a number of functions

00:05:44,180 --> 00:05:48,649
and each of these functions can have

00:05:46,460 --> 00:05:52,580
open and P inside of it and so what

00:05:48,649 --> 00:05:54,680
happens is that you know conceptually

00:05:52,580 --> 00:05:56,149
speaking each of these has opened in P

00:05:54,680 --> 00:05:58,250
but as I said the open and P has these

00:05:56,149 --> 00:06:00,129
barriers at the end and that's something

00:05:58,250 --> 00:06:02,750
that you have to keep in mind because

00:06:00,129 --> 00:06:04,099
the back barriers like each time the

00:06:02,750 --> 00:06:04,849
threads will have to kind of stop and

00:06:04,099 --> 00:06:08,089
talk to each other

00:06:04,849 --> 00:06:09,110
that adds overhead and conceptual

00:06:08,089 --> 00:06:10,669
speaking you may want to get rid of that

00:06:09,110 --> 00:06:12,469
overhead so here's here's the problem so

00:06:10,669 --> 00:06:14,210
so first you know so here I have these

00:06:12,469 --> 00:06:15,649
functions here and the first thing you

00:06:14,210 --> 00:06:17,210
might say like well you know the

00:06:15,649 --> 00:06:18,620
compiler may inline these functions

00:06:17,210 --> 00:06:20,539
because maybe they were in the same

00:06:18,620 --> 00:06:21,949
translation unit or using LTO or what

00:06:20,539 --> 00:06:23,539
have you and that's a good thing to do

00:06:21,949 --> 00:06:26,089
so now the code conceptually speaking

00:06:23,539 --> 00:06:27,259
looks something like this so you know I

00:06:26,089 --> 00:06:28,639
took these things instead of these

00:06:27,259 --> 00:06:30,529
functions I now have these loops

00:06:28,639 --> 00:06:32,539
directly in the code but you know the

00:06:30,529 --> 00:06:34,099
pragmas and such are still there and the

00:06:32,539 --> 00:06:36,440
barriers those probably was implied are

00:06:34,099 --> 00:06:37,370
also still there alright so now I've you

00:06:36,440 --> 00:06:38,629
know I've gotten rid of the function

00:06:37,370 --> 00:06:39,470
call overhead but generically speaking

00:06:38,629 --> 00:06:41,060
thread

00:06:39,470 --> 00:06:42,260
synchronization overhead is much larger

00:06:41,060 --> 00:06:44,120
than function call overhead so you

00:06:42,260 --> 00:06:47,690
haven't necessarily won anything in that

00:06:44,120 --> 00:06:49,370
particular regard okay so the next thing

00:06:47,690 --> 00:06:51,200
a human might do so if they you know if

00:06:49,370 --> 00:06:53,210
a human does this transformation to this

00:06:51,200 --> 00:06:56,120
by hand you know the next thing they

00:06:53,210 --> 00:06:58,490
might say is oh well here for one thing

00:06:56,120 --> 00:06:59,930
I can take the parallel regions the

00:06:58,490 --> 00:07:02,390
parts where all the threads kind of have

00:06:59,930 --> 00:07:05,330
to wake up and get told what to do and I

00:07:02,390 --> 00:07:06,710
can move that outside of this of this

00:07:05,330 --> 00:07:08,300
loop so now I have all the threads

00:07:06,710 --> 00:07:11,060
running and then they just split up the

00:07:08,300 --> 00:07:13,040
various pieces of these loops and that's

00:07:11,060 --> 00:07:14,540
that's also a good thing to do and then

00:07:13,040 --> 00:07:16,760
eliminate some but not all of the

00:07:14,540 --> 00:07:19,010
overhead and then on top of that I'm

00:07:16,760 --> 00:07:20,990
want to eliminate the barriers now the

00:07:19,010 --> 00:07:22,970
barrier actually can eliminate by hand

00:07:20,990 --> 00:07:25,490
and you can do this in a very easy way

00:07:22,970 --> 00:07:27,950
because openmp provides this nil weight

00:07:25,490 --> 00:07:29,690
closet you can put on the openmp

00:07:27,950 --> 00:07:31,070
directives that's a handy thing to use

00:07:29,690 --> 00:07:33,530
because it gets rid of the barriers at

00:07:31,070 --> 00:07:35,150
the end of this first loop and also the

00:07:33,530 --> 00:07:36,440
second until there's only a barrier at

00:07:35,150 --> 00:07:39,890
the end of the third so that all makes

00:07:36,440 --> 00:07:41,270
sense and that's and the compiler and

00:07:39,890 --> 00:07:43,310
the runtime will be quite happy cuz your

00:07:41,270 --> 00:07:45,590
overhead is low now the only problem of

00:07:43,310 --> 00:07:48,200
course is that this is not code that you

00:07:45,590 --> 00:07:50,390
want to maintain because you've now

00:07:48,200 --> 00:07:52,880
inlined all the functions into this big

00:07:50,390 --> 00:07:54,740
function and its really bad practice to

00:07:52,880 --> 00:07:56,870
take your entire application and put it

00:07:54,740 --> 00:07:58,760
in one function so what you really want

00:07:56,870 --> 00:08:02,300
to do is have all these things in

00:07:58,760 --> 00:08:04,160
separate functions and nevertheless get

00:08:02,300 --> 00:08:06,229
the performance of this version so the

00:08:04,160 --> 00:08:08,479
question is how do you do that and this

00:08:06,229 --> 00:08:10,460
is the motivation here's another example

00:08:08,479 --> 00:08:12,350
come back to that here's the motivation

00:08:10,460 --> 00:08:14,750
for teaching L vamps optimizer about

00:08:12,350 --> 00:08:17,120
open and P is that this transformation

00:08:14,750 --> 00:08:19,190
is something the compiler can do all by

00:08:17,120 --> 00:08:20,900
itself and so it's really nice it's like

00:08:19,190 --> 00:08:22,640
a parlor then does this transformation

00:08:20,900 --> 00:08:27,979
all by itself and the person doesn't

00:08:22,640 --> 00:08:31,900
have to do it manually all right so

00:08:27,979 --> 00:08:34,070
here's a here's another example another

00:08:31,900 --> 00:08:36,229
code in this case is the parallel region

00:08:34,070 --> 00:08:37,700
there's a for loop there are some

00:08:36,229 --> 00:08:39,650
there's a massive region up here for the

00:08:37,700 --> 00:08:41,570
printing but in any case this is a loop

00:08:39,650 --> 00:08:44,270
that gets split over you know multiple

00:08:41,570 --> 00:08:48,380
iterations and so what we've done in

00:08:44,270 --> 00:08:50,300
side of what we've done inside of LVM

00:08:48,380 --> 00:08:51,800
when we started prototyping a number of

00:08:50,300 --> 00:08:52,400
different ways of teaching LVM to

00:08:51,800 --> 00:08:55,400
optimize

00:08:52,400 --> 00:08:58,130
about OpenMP I'm not gonna talk about

00:08:55,400 --> 00:09:00,500
the details in I'm not gonna talk about

00:08:58,130 --> 00:09:03,740
too many of the details but I'll mention

00:09:00,500 --> 00:09:05,420
that the following so so this teaching

00:09:03,740 --> 00:09:08,150
element optimizer abet OpenMP kind of

00:09:05,420 --> 00:09:12,140
involves two different things so one is

00:09:08,150 --> 00:09:14,480
teaching LLVM to optimizer about the

00:09:12,140 --> 00:09:16,730
fact that the ship with this the runtime

00:09:14,480 --> 00:09:18,230
school calls that dispatch these

00:09:16,730 --> 00:09:20,570
functions on lots of different threads

00:09:18,230 --> 00:09:22,730
these are not particularly scary in fact

00:09:20,570 --> 00:09:24,740
they behave in a very well-known manner

00:09:22,730 --> 00:09:26,930
and we can teach the optimizer what that

00:09:24,740 --> 00:09:29,120
is and what that does is it allows the

00:09:26,930 --> 00:09:32,390
optimizer to take information from the

00:09:29,120 --> 00:09:35,330
sort of original parent function and

00:09:32,390 --> 00:09:38,660
propagate it into the functions that

00:09:35,330 --> 00:09:41,660
have the the parallel code in them and

00:09:38,660 --> 00:09:43,400
this allows us to optimize better the

00:09:41,660 --> 00:09:45,560
code that's inside those power regions

00:09:43,400 --> 00:09:48,560
and so this is a graph for this piece of

00:09:45,560 --> 00:09:50,740
code here and and what we found is that

00:09:48,560 --> 00:09:53,660
the optimizer has a really hard time

00:09:50,740 --> 00:09:54,680
optimizing this piece of code because

00:09:53,660 --> 00:09:58,070
right now

00:09:54,680 --> 00:10:01,580
once it turns the so all of this code

00:09:58,070 --> 00:10:04,339
here inside this big for loop once it

00:10:01,580 --> 00:10:07,720
takes this out and puts it in a separate

00:10:04,339 --> 00:10:10,760
function it kind of loses track of the

00:10:07,720 --> 00:10:13,160
sort of properties of these arrays here

00:10:10,760 --> 00:10:14,270
that are being accessed and that causes

00:10:13,160 --> 00:10:16,160
it to generate the code that's much

00:10:14,270 --> 00:10:18,140
slower than it might otherwise be so

00:10:16,160 --> 00:10:20,450
that's kind of unfortunate but if we if

00:10:18,140 --> 00:10:24,110
we teach the optimizer how to sort of

00:10:20,450 --> 00:10:26,330
look through the transition from the

00:10:24,110 --> 00:10:29,060
serial to the parallel code then we can

00:10:26,330 --> 00:10:31,459
get rid of that kind of overhead and so

00:10:29,060 --> 00:10:34,459
here this is a you know quick

00:10:31,459 --> 00:10:38,420
performance plot of the result of doing

00:10:34,459 --> 00:10:41,630
that in LVM optimizer we're here this

00:10:38,420 --> 00:10:43,370
blue point indicates the you know

00:10:41,630 --> 00:10:45,740
original of the performance of the

00:10:43,370 --> 00:10:48,440
original benchmark when compile was full

00:10:45,740 --> 00:10:51,110
optimization and then when we enable

00:10:48,440 --> 00:10:55,029
this additional OpenMP knowledge in the

00:10:51,110 --> 00:10:57,980
optimizer we you know get a substantial

00:10:55,029 --> 00:11:01,440
improvement in the performance because

00:10:57,980 --> 00:11:03,780
now the compiler is not kind of ham

00:11:01,440 --> 00:11:05,850
you know impeding its itself by by doing

00:11:03,780 --> 00:11:07,110
this this optimization and there are a

00:11:05,850 --> 00:11:09,870
number of different possibilities here

00:11:07,110 --> 00:11:12,240
for how we can optimize it this this

00:11:09,870 --> 00:11:13,980
this first jump here from here to here

00:11:12,240 --> 00:11:16,350
actually isn't due to any particular

00:11:13,980 --> 00:11:18,810
OpenMP aware transformation so much as

00:11:16,350 --> 00:11:20,190
it is teaching the optimizer how to keep

00:11:18,810 --> 00:11:22,230
track of this information about the

00:11:20,190 --> 00:11:24,030
array properties and this enables all

00:11:22,230 --> 00:11:25,830
their existing optimizations to kick in

00:11:24,030 --> 00:11:28,260
and you get much better performance okay

00:11:25,830 --> 00:11:30,330
so that's that's this jump we can also

00:11:28,260 --> 00:11:33,360
do you know sort of other

00:11:30,330 --> 00:11:35,610
transformations to for argument

00:11:33,360 --> 00:11:37,680
privatization other things and it needs

00:11:35,610 --> 00:11:40,410
also give a slight in additional

00:11:37,680 --> 00:11:44,640
enhancement okay

00:11:40,410 --> 00:11:46,260
so here's another sort of schematic of a

00:11:44,640 --> 00:11:48,090
code that's into another one of these

00:11:46,260 --> 00:11:52,170
benchmarks of these are both from the

00:11:48,090 --> 00:11:55,200
road ania benchmark suite and so here we

00:11:52,170 --> 00:11:57,540
have two loops these these loops have

00:11:55,200 --> 00:11:58,680
openmp pragmas which are well somewhat

00:11:57,540 --> 00:11:59,700
for both because they have lots of

00:11:58,680 --> 00:12:02,340
shared and private variables

00:11:59,700 --> 00:12:04,860
nevertheless you know we can apply a

00:12:02,340 --> 00:12:07,770
very kind of similar kind of techniques

00:12:04,860 --> 00:12:09,960
to this we we've experimented with a lot

00:12:07,770 --> 00:12:14,490
of different kinds of optimizations that

00:12:09,960 --> 00:12:16,440
we can perform in LLVM these and these

00:12:14,490 --> 00:12:17,820
involve a number of different kinds of

00:12:16,440 --> 00:12:20,250
techniques and combinations of these

00:12:17,820 --> 00:12:22,590
techniques so for example here so the

00:12:20,250 --> 00:12:24,740
base here this is the original timing we

00:12:22,590 --> 00:12:27,060
also have versions where we enable this

00:12:24,740 --> 00:12:30,720
you know array property tracking the

00:12:27,060 --> 00:12:32,490
alias analysis improvements technically

00:12:30,720 --> 00:12:34,650
speaking and those those improve things

00:12:32,490 --> 00:12:35,880
a little bit we can also combine those

00:12:34,650 --> 00:12:37,590
with various kind of argument

00:12:35,880 --> 00:12:41,130
privatization and other sort of

00:12:37,590 --> 00:12:43,260
techniques in order to you know prevent

00:12:41,130 --> 00:12:46,500
the optimization of the OpenMP regions

00:12:43,260 --> 00:12:48,930
from impeding optimization of the sort

00:12:46,500 --> 00:12:51,930
of code that comes after that and that

00:12:48,930 --> 00:12:53,460
provides a very nice improvement we've

00:12:51,930 --> 00:12:57,089
also experimented with open and p

00:12:53,460 --> 00:12:59,130
specific optimizations so those include

00:12:57,089 --> 00:13:01,380
things like taking adjacent parallel

00:12:59,130 --> 00:13:02,700
regions and merging them together to

00:13:01,380 --> 00:13:05,370
eliminate the overhead of having

00:13:02,700 --> 00:13:06,310
multiple adjacent parallel regions we

00:13:05,370 --> 00:13:10,270
have

00:13:06,310 --> 00:13:13,240
then on top of that eliminated redundant

00:13:10,270 --> 00:13:14,560
barriers so there are barriers in open

00:13:13,240 --> 00:13:17,980
and P and then some of the constructs

00:13:14,560 --> 00:13:20,529
also have barriers and when you you know

00:13:17,980 --> 00:13:21,730
combine everything together well you may

00:13:20,529 --> 00:13:23,200
have redundant barrier just the way the

00:13:21,730 --> 00:13:25,240
source code is but often when you

00:13:23,200 --> 00:13:26,410
combine things like pearl regions

00:13:25,240 --> 00:13:28,300
together you end up with lots of

00:13:26,410 --> 00:13:29,320
barriers that are now redundant alright

00:13:28,300 --> 00:13:30,460
so if you have a barrier and then

00:13:29,320 --> 00:13:32,230
another barrier right after it you don't

00:13:30,460 --> 00:13:35,050
need to you just need one so you can get

00:13:32,230 --> 00:13:37,960
rid of the second one and and this

00:13:35,050 --> 00:13:40,330
actually has a measurable effect on the

00:13:37,960 --> 00:13:41,890
performance as well so that turns out to

00:13:40,330 --> 00:13:43,390
be pretty important and and we've

00:13:41,890 --> 00:13:45,580
combined all these various techniques

00:13:43,390 --> 00:13:47,140
and I mean in this case you know you can

00:13:45,580 --> 00:13:48,970
see that we get about an 8% speed-up

00:13:47,140 --> 00:13:51,700
from various combinations of these

00:13:48,970 --> 00:13:53,680
optimizations and that's that's pretty

00:13:51,700 --> 00:13:56,020
useful ok so now I just go back quickly

00:13:53,680 --> 00:13:58,720
to the to the original example that we

00:13:56,020 --> 00:14:00,700
had from from our logo this is from the

00:13:58,720 --> 00:14:02,200
Virginia CFD benchmark and again we have

00:14:00,700 --> 00:14:04,600
these various functions and inside these

00:14:02,200 --> 00:14:06,550
various functions are these various

00:14:04,600 --> 00:14:07,930
parallel regions and we have an

00:14:06,550 --> 00:14:10,089
optimization that then does this

00:14:07,930 --> 00:14:12,550
particular transformation automatically

00:14:10,089 --> 00:14:14,530
so it takes these things and it will

00:14:12,550 --> 00:14:17,050
combine the parallel constructs it takes

00:14:14,530 --> 00:14:19,780
the parallel region em and moves it out

00:14:17,050 --> 00:14:21,850
and now here you can see what the effect

00:14:19,780 --> 00:14:23,980
of that is on the performance of the

00:14:21,850 --> 00:14:26,530
program so again here the base is the

00:14:23,980 --> 00:14:29,800
original timing and various combinations

00:14:26,530 --> 00:14:31,960
of these optimizations improve the

00:14:29,800 --> 00:14:34,630
performance and so overall this is about

00:14:31,960 --> 00:14:37,980
a 12% performance increase over the

00:14:34,630 --> 00:14:41,650
original benchmark performance for the

00:14:37,980 --> 00:14:43,390
application ok so that's if its data set

00:14:41,650 --> 00:14:44,860
anyway okay so that's so that's true

00:14:43,390 --> 00:14:46,330
we've repeated these experiments with

00:14:44,860 --> 00:14:49,300
lots of other benchmarks you can see

00:14:46,330 --> 00:14:52,240
that you get performance increases on

00:14:49,300 --> 00:14:54,790
many of them so for the rest of the

00:14:52,240 --> 00:14:57,820
benchmark suite for instance this this

00:14:54,790 --> 00:14:59,670
benchmark is 10% speed-up this money

00:14:57,820 --> 00:15:03,310
it's a 3% speed-up ok so these are not

00:14:59,670 --> 00:15:05,050
particularly interesting moreover this

00:15:03,310 --> 00:15:07,510
is a pathfinder benchmark I think that's

00:15:05,050 --> 00:15:10,690
a graph algorithm that to the 17%

00:15:07,510 --> 00:15:13,120
speed-up and so this is what we get from

00:15:10,690 --> 00:15:15,040
teaching the optimizer about OpenMP and

00:15:13,120 --> 00:15:18,130
again that means a combination of two

00:15:15,040 --> 00:15:19,750
things one is

00:15:18,130 --> 00:15:21,160
sort of letting it look through open and

00:15:19,750 --> 00:15:23,410
P constructs so that existing

00:15:21,160 --> 00:15:25,510
optimizations can do the things that

00:15:23,410 --> 00:15:29,010
they're meant to do and the second thing

00:15:25,510 --> 00:15:31,420
is to implement OpenMP specific

00:15:29,010 --> 00:15:34,330
optimizations and using those specific

00:15:31,420 --> 00:15:37,060
optimizations we are able to do things

00:15:34,330 --> 00:15:40,020
like combined in a jacent apparel

00:15:37,060 --> 00:15:43,570
regions eliminate redundant barriers and

00:15:40,020 --> 00:15:44,890
and other things which will specifically

00:15:43,570 --> 00:15:47,410
increase the performance of parallel

00:15:44,890 --> 00:15:49,510
code this is what we've been what we've

00:15:47,410 --> 00:15:54,090
been working on this is in the context

00:15:49,510 --> 00:15:57,430
of Elvia and we are in the process of

00:15:54,090 --> 00:16:00,070
contributing the the components of these

00:15:57,430 --> 00:16:02,800
optimizations back up to the upstream

00:16:00,070 --> 00:16:04,570
LLVM project so that it will be able to

00:16:02,800 --> 00:16:09,180
you know download them take advantage of

00:16:04,570 --> 00:16:09,180

YouTube URL: https://www.youtube.com/watch?v=KTjkZ2mmzJE


