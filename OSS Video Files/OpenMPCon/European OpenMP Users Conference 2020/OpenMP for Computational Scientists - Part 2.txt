Title: OpenMP for Computational Scientists - Part 2
Publication date: 2020-12-21
Playlist: European OpenMP Users Conference 2020
Description: 
	This tutorial was presented at the 3rd European OpenMP Users Conference in 2020

Presented by : Tom Deakin, University of Bristol

Additional Materials (Slides and code): https://github.com/UoB-HPC/openmp-for-cs

Conference Website: https://openmpusers.org
Follow us: https://twitter.com/openmp_users 

Presentation Abstract :
In the second part, we’ll have a whistle-stop tour of the features in OpenMP for writing programs for heterogeneous nodes with GPUs. We’ll walk through the target directive for offloading both data and parallel execution to GPUs. At the end, you’ll be able to write programs using OpenMP for massively parallel GPUs.
Captions: 
	00:00:04,720 --> 00:00:08,720
okay

00:00:05,279 --> 00:00:11,679
welcome back after the uh lunch break

00:00:08,720 --> 00:00:12,960
um this is the um openmp for

00:00:11,679 --> 00:00:15,440
computational scientists

00:00:12,960 --> 00:00:16,080
uh tutorial um those of you that that

00:00:15,440 --> 00:00:18,320
joined this morning

00:00:16,080 --> 00:00:20,720
welcome back um for those of you that

00:00:18,320 --> 00:00:22,400
have joined uh for this afternoon

00:00:20,720 --> 00:00:24,560
um this morning we've spent a lot of

00:00:22,400 --> 00:00:26,880
time learning all about openmp

00:00:24,560 --> 00:00:27,760
and and have done a lot of work working

00:00:26,880 --> 00:00:30,320
our way up from

00:00:27,760 --> 00:00:32,559
uh starting with serial fortran all the

00:00:30,320 --> 00:00:35,520
way up to numa aware vectorized

00:00:32,559 --> 00:00:37,760
parallel um codes written written in

00:00:35,520 --> 00:00:40,719
openmp that are going to

00:00:37,760 --> 00:00:42,960
run well and on our on our multi-core

00:00:40,719 --> 00:00:44,879
cpus

00:00:42,960 --> 00:00:46,079
so we've already covered covered a lot

00:00:44,879 --> 00:00:48,480
of things and and

00:00:46,079 --> 00:00:50,320
this afternoon is is going to continue

00:00:48,480 --> 00:00:53,120
in that same vein we're going to look at

00:00:50,320 --> 00:00:55,360
um the support in openmp for targeting

00:00:53,120 --> 00:00:57,039
gpus

00:00:55,360 --> 00:00:58,719
this is a hands-on tutorial for those

00:00:57,039 --> 00:01:00,719
that are joining this afternoon

00:00:58,719 --> 00:01:02,399
so there's going to be a mixture of me

00:01:00,719 --> 00:01:03,680
speaking and explaining things in a sort

00:01:02,399 --> 00:01:06,880
of lecture format

00:01:03,680 --> 00:01:09,920
followed by exercises for you to

00:01:06,880 --> 00:01:13,119
try out the things that we've been um

00:01:09,920 --> 00:01:15,759
been explaining in the lectures

00:01:13,119 --> 00:01:16,479
um all of the material all the slides

00:01:15,759 --> 00:01:18,720
and

00:01:16,479 --> 00:01:19,680
the source code is available on github

00:01:18,720 --> 00:01:21,840
and you can

00:01:19,680 --> 00:01:22,880
download it from there might be useful

00:01:21,840 --> 00:01:25,840
to um

00:01:22,880 --> 00:01:27,360
to follow along or have the exercises or

00:01:25,840 --> 00:01:29,360
some of this preliminary information

00:01:27,360 --> 00:01:31,520
open as well

00:01:29,360 --> 00:01:33,439
so i think in a one of the previous q

00:01:31,520 --> 00:01:35,759
a's there's a clickable link for this

00:01:33,439 --> 00:01:37,360
maybe in the chat or the q a

00:01:35,759 --> 00:01:39,200
now uh this afternoon we're going to

00:01:37,360 --> 00:01:41,360
continue with using the q

00:01:39,200 --> 00:01:43,200
a part of zoom for taking questions as

00:01:41,360 --> 00:01:45,600
you do the exercises

00:01:43,200 --> 00:01:47,280
um so use that over the chat it's

00:01:45,600 --> 00:01:49,759
difficult to keep track of the chat

00:01:47,280 --> 00:01:51,600
um and answer questions there so ask any

00:01:49,759 --> 00:01:54,799
questions you have in the q and a

00:01:51,600 --> 00:01:55,759
part of the um of zoom there's a little

00:01:54,799 --> 00:01:58,479
button somewhere

00:01:55,759 --> 00:01:59,200
in the bottom um for you to open the

00:01:58,479 --> 00:02:01,759
window for q

00:01:59,200 --> 00:02:02,640
a so we're all going to be running on

00:02:01,759 --> 00:02:04,159
isimbard and

00:02:02,640 --> 00:02:07,600
the gpu nodes that we're going to be

00:02:04,159 --> 00:02:08,959
targeting are these nvidia p100 nodes

00:02:07,600 --> 00:02:11,360
now this morning we were running on the

00:02:08,959 --> 00:02:13,280
cpu so this afternoon we're going to be

00:02:11,360 --> 00:02:16,480
offloading our parallel codes

00:02:13,280 --> 00:02:18,640
to the gpu so that we can

00:02:16,480 --> 00:02:20,160
gain some performance there because of

00:02:18,640 --> 00:02:22,480
the extra memory bandwidth

00:02:20,160 --> 00:02:23,760
available so for those of you joining

00:02:22,480 --> 00:02:25,760
this afternoon

00:02:23,760 --> 00:02:27,120
we can get you set up with an account

00:02:25,760 --> 00:02:30,400
and i'll show you that when we get to

00:02:27,120 --> 00:02:32,239
the to the first exercise

00:02:30,400 --> 00:02:33,840
so this is the uh the plan for this

00:02:32,239 --> 00:02:35,040
afternoon i'm going to spend a bit of

00:02:33,840 --> 00:02:37,200
time now

00:02:35,040 --> 00:02:39,680
explaining about how you're able to

00:02:37,200 --> 00:02:40,959
start offloading code and regions of

00:02:39,680 --> 00:02:44,800
your program

00:02:40,959 --> 00:02:46,879
uh from the host onto a gpu

00:02:44,800 --> 00:02:47,840
device or any other kind of accelerated

00:02:46,879 --> 00:02:50,160
device

00:02:47,840 --> 00:02:50,879
and how you move data as well so how you

00:02:50,160 --> 00:02:53,360
move

00:02:50,879 --> 00:02:54,879
execution and how you move data and then

00:02:53,360 --> 00:02:56,000
we'll move quickly into the first

00:02:54,879 --> 00:02:57,760
exercise which

00:02:56,000 --> 00:02:59,360
is going to be taking the stencil code

00:02:57,760 --> 00:03:01,760
we've been working on this morning

00:02:59,360 --> 00:03:05,040
and getting it running on the gpu by

00:03:01,760 --> 00:03:07,599
transferring execution and data

00:03:05,040 --> 00:03:08,959
then after that there'll be a lecture

00:03:07,599 --> 00:03:10,879
where i'm going to explain about the

00:03:08,959 --> 00:03:12,560
parallelism on the gpu and how it's

00:03:10,879 --> 00:03:14,480
exposed in openmp

00:03:12,560 --> 00:03:15,599
we'll have a quick coffee break and then

00:03:14,480 --> 00:03:18,239
go into

00:03:15,599 --> 00:03:18,959
how you might optimize the data movement

00:03:18,239 --> 00:03:20,720
for

00:03:18,959 --> 00:03:22,239
between the host and the device within

00:03:20,720 --> 00:03:23,920
openmp

00:03:22,239 --> 00:03:25,599
and then finally we'll put all of those

00:03:23,920 --> 00:03:27,599
things together so applying the

00:03:25,599 --> 00:03:30,560
parallelism and the data movement

00:03:27,599 --> 00:03:32,239
to optimize the stencil on a gpu get it

00:03:30,560 --> 00:03:33,840
running in parallel on the gpu

00:03:32,239 --> 00:03:35,680
and optimizing some of that data

00:03:33,840 --> 00:03:39,840
transfer and will be done by

00:03:35,680 --> 00:03:42,239
um oppos 4 uk time

00:03:39,840 --> 00:03:43,440
so with that i'm going to get started

00:03:42,239 --> 00:03:46,640
with the first

00:03:43,440 --> 00:03:49,280
part of the of the gpu material

00:03:46,640 --> 00:03:50,799
we are now moving into some of the extra

00:03:49,280 --> 00:03:53,920
features that were

00:03:50,799 --> 00:03:54,720
introduced in openmp 4 and refined in

00:03:53,920 --> 00:03:57,439
4.5

00:03:54,720 --> 00:03:58,159
so we're going to be talking about the

00:03:57,439 --> 00:04:00,400
openmp

00:03:58,159 --> 00:04:02,319
target construct primarily and all of

00:04:00,400 --> 00:04:05,599
the constructs that build

00:04:02,319 --> 00:04:08,720
upon that in order to

00:04:05,599 --> 00:04:10,560
deal with this heterogeneous

00:04:08,720 --> 00:04:12,239
compute node where we have different

00:04:10,560 --> 00:04:14,799
types of computing architecture

00:04:12,239 --> 00:04:16,720
all within the same node as cpu and a

00:04:14,799 --> 00:04:18,799
gpu

00:04:16,720 --> 00:04:20,320
so why might we want to use a gpu in the

00:04:18,799 --> 00:04:23,520
first place

00:04:20,320 --> 00:04:25,759
well hardware is developing um

00:04:23,520 --> 00:04:28,160
over time to give us these very highly

00:04:25,759 --> 00:04:30,000
parallel processors and this is true of

00:04:28,160 --> 00:04:32,320
cpus and of gpus

00:04:30,000 --> 00:04:33,919
but with a gpu that the approach has

00:04:32,320 --> 00:04:36,880
typically been

00:04:33,919 --> 00:04:38,320
to construct each processor out of a

00:04:36,880 --> 00:04:42,000
large number

00:04:38,320 --> 00:04:44,160
of simple cores compared to a smaller

00:04:42,000 --> 00:04:45,120
number of more complex cores that you

00:04:44,160 --> 00:04:48,479
would imagine

00:04:45,120 --> 00:04:51,280
um like in a cpu um

00:04:48,479 --> 00:04:52,560
in doing this um they often have an

00:04:51,280 --> 00:04:54,160
increased number of floating point

00:04:52,560 --> 00:04:57,759
operations so

00:04:54,160 --> 00:04:58,320
a nvidia v100 gpu has has nearly twice

00:04:57,759 --> 00:05:00,320
as many

00:04:58,320 --> 00:05:01,440
uh floating point operations per second

00:05:00,320 --> 00:05:03,840
so the amount of

00:05:01,440 --> 00:05:05,440
double precision compute that can happen

00:05:03,840 --> 00:05:09,600
um over something like

00:05:05,440 --> 00:05:11,600
a skylake but also gpus have often

00:05:09,600 --> 00:05:13,039
taken on board the latest innovations in

00:05:11,600 --> 00:05:14,880
memory technologies

00:05:13,039 --> 00:05:16,560
in particular using things like high

00:05:14,880 --> 00:05:19,199
bandwidth memory

00:05:16,560 --> 00:05:21,120
um and cpus have only just started uh

00:05:19,199 --> 00:05:21,759
picking up that mantle we've seen that

00:05:21,120 --> 00:05:25,199
with the

00:05:21,759 --> 00:05:28,400
um the fugaku supercomputer in japan

00:05:25,199 --> 00:05:29,199
which uses the fujitsu a64fx arm-based

00:05:28,400 --> 00:05:31,120
processors

00:05:29,199 --> 00:05:33,680
they are now including high bandwidth

00:05:31,120 --> 00:05:36,560
memory on a cpu socket

00:05:33,680 --> 00:05:38,400
and as most of our codes are our our

00:05:36,560 --> 00:05:40,160
main memory bandwidth bound

00:05:38,400 --> 00:05:42,400
being able to leverage this memory

00:05:40,160 --> 00:05:44,560
technology the high bandwidth memory

00:05:42,400 --> 00:05:46,080
has only been possible with with gpus

00:05:44,560 --> 00:05:47,759
until until now and

00:05:46,080 --> 00:05:50,479
it's starting to creep into cpus at the

00:05:47,759 --> 00:05:50,479
moment as well

00:05:51,680 --> 00:05:56,880
so let's have a little kind of case

00:05:53,919 --> 00:05:59,039
study of of what gpus might look like

00:05:56,880 --> 00:06:00,880
and for this we're going to use nvidia

00:05:59,039 --> 00:06:02,880
and in this slide i'm going to be using

00:06:00,880 --> 00:06:06,560
a lot of nvidia

00:06:02,880 --> 00:06:08,639
definitions for how they construct their

00:06:06,560 --> 00:06:12,800
gpus what their architecture

00:06:08,639 --> 00:06:16,240
means to them in their own language

00:06:12,800 --> 00:06:18,479
so a gpu is made up of uh many cores

00:06:16,240 --> 00:06:20,319
and they're caused like we think of them

00:06:18,479 --> 00:06:22,639
as a cpu core

00:06:20,319 --> 00:06:23,840
nvidia call those streaming multi

00:06:22,639 --> 00:06:27,840
processors instead

00:06:23,840 --> 00:06:31,360
and a v100 gpu has 80 of those 80 sms

00:06:27,840 --> 00:06:33,280
and the p100 has saved 56 sms

00:06:31,360 --> 00:06:34,400
so compared to the cpus that's an awful

00:06:33,280 --> 00:06:37,680
lot of cores

00:06:34,400 --> 00:06:41,120
on that particular socket you know much

00:06:37,680 --> 00:06:44,319
more many more cores on the gpu

00:06:41,120 --> 00:06:47,840
each of those sms has 64

00:06:44,319 --> 00:06:48,160
single precision cuda cores now this is

00:06:47,840 --> 00:06:50,319
quite

00:06:48,160 --> 00:06:51,840
unfortunate that nvidia has chosen the

00:06:50,319 --> 00:06:54,240
word core

00:06:51,840 --> 00:06:55,520
to define what is essentially a vector

00:06:54,240 --> 00:06:58,479
lane

00:06:55,520 --> 00:07:00,720
whereas the cpu idea of a core may have

00:06:58,479 --> 00:07:02,800
vectors within that

00:07:00,720 --> 00:07:04,319
so it's important to think that when you

00:07:02,800 --> 00:07:06,720
hear the term cuda core

00:07:04,319 --> 00:07:07,759
they are organized as these two vector

00:07:06,720 --> 00:07:11,360
units of

00:07:07,759 --> 00:07:15,039
in in blocks of 32 cuda cores together

00:07:11,360 --> 00:07:18,240
and they call them warps so

00:07:15,039 --> 00:07:20,479
the takeaway is that gpus is look a lot

00:07:18,240 --> 00:07:22,880
like cpus in many ways they're really

00:07:20,479 --> 00:07:24,319
vector architectures that are made up of

00:07:22,880 --> 00:07:26,720
these smaller blocks which

00:07:24,319 --> 00:07:27,759
execute together so we have these very

00:07:26,720 --> 00:07:31,599
wide vectors

00:07:27,759 --> 00:07:33,759
32 or 64 wide and we have

00:07:31,599 --> 00:07:34,800
one of those on on a large number of

00:07:33,759 --> 00:07:36,960
cores

00:07:34,800 --> 00:07:38,160
and the cores are somewhat independent

00:07:36,960 --> 00:07:40,240
and but the

00:07:38,160 --> 00:07:42,560
the sms are somewhat independent this is

00:07:40,240 --> 00:07:43,759
but the the cores operate much more like

00:07:42,560 --> 00:07:46,319
a vector unit than

00:07:43,759 --> 00:07:48,720
than a individual core on a cpu for

00:07:46,319 --> 00:07:48,720
instance

00:07:49,199 --> 00:07:53,680
another thing to notice is that gpus are

00:07:51,840 --> 00:07:55,440
typically classified as throughput

00:07:53,680 --> 00:07:57,360
optimized

00:07:55,440 --> 00:07:59,280
so it's all about getting a large amount

00:07:57,360 --> 00:08:01,120
of stuff done in a given amount of time

00:07:59,280 --> 00:08:02,800
whereas cpus are often optimized for

00:08:01,120 --> 00:08:04,960
latency things like

00:08:02,800 --> 00:08:06,080
um caches and that sort of thing that

00:08:04,960 --> 00:08:08,479
and the deep

00:08:06,080 --> 00:08:09,759
cache hierarchy that cpus have got

00:08:08,479 --> 00:08:12,160
really help it

00:08:09,759 --> 00:08:13,280
become optimized for latency whereas

00:08:12,160 --> 00:08:15,440
whereas gpus

00:08:13,280 --> 00:08:17,360
is all about being tolerant to long

00:08:15,440 --> 00:08:20,080
latencies so they can they can

00:08:17,360 --> 00:08:21,039
they sacrifice a large latency to do

00:08:20,080 --> 00:08:24,879
things like

00:08:21,039 --> 00:08:26,400
read from memory for a throughput-based

00:08:24,879 --> 00:08:27,919
approach where you do lots and lots of

00:08:26,400 --> 00:08:29,840
things at once

00:08:27,919 --> 00:08:31,759
um lots and lots of operations in

00:08:29,840 --> 00:08:32,560
parallel and overlap them with each

00:08:31,759 --> 00:08:35,200
other so you

00:08:32,560 --> 00:08:37,360
hide all that latency it's trading off

00:08:35,200 --> 00:08:40,479
throughput for latency and and that's

00:08:37,360 --> 00:08:42,399
where the high memory bandwidth can help

00:08:40,479 --> 00:08:43,680
now to do that we need to exploit lots

00:08:42,399 --> 00:08:46,160
and lots of parallelism

00:08:43,680 --> 00:08:48,480
in our applications in order to be able

00:08:46,160 --> 00:08:51,279
to do that on a gpu

00:08:48,480 --> 00:08:53,680
so if we think about the v100 this is

00:08:51,279 --> 00:08:56,399
over 5000 individual

00:08:53,680 --> 00:08:57,680
processing elements and each of those is

00:08:56,399 --> 00:08:59,920
going to need multiple

00:08:57,680 --> 00:09:02,320
bits of work in order to overlap with

00:08:59,920 --> 00:09:03,680
each other to to make best use of that

00:09:02,320 --> 00:09:05,440
of that throughput optimized

00:09:03,680 --> 00:09:06,880
architecture

00:09:05,440 --> 00:09:09,120
so the key message is that we need to

00:09:06,880 --> 00:09:10,880
find a massive amount of parallelism

00:09:09,120 --> 00:09:12,160
in our applications in order to be able

00:09:10,880 --> 00:09:14,000
to exploit

00:09:12,160 --> 00:09:15,519
the concurrency that's available at a

00:09:14,000 --> 00:09:18,240
hardware level

00:09:15,519 --> 00:09:18,240
on the gpu

00:09:18,640 --> 00:09:22,560
so that's gpus in general and and we'll

00:09:20,800 --> 00:09:23,839
now move in on a little bit and talk

00:09:22,560 --> 00:09:26,160
about openmp

00:09:23,839 --> 00:09:29,680
and how gpus and things like that are

00:09:26,160 --> 00:09:31,680
seen as far as the openmp model

00:09:29,680 --> 00:09:33,680
like many other heterogeneous

00:09:31,680 --> 00:09:35,440
programming models things like opencl

00:09:33,680 --> 00:09:38,160
and sickle

00:09:35,440 --> 00:09:39,040
openmp has this model of a host and a

00:09:38,160 --> 00:09:42,080
device

00:09:39,040 --> 00:09:43,120
so we have a host cpu which has its cpu

00:09:42,080 --> 00:09:46,320
memory

00:09:43,120 --> 00:09:47,360
and connected to that cpu is one or more

00:09:46,320 --> 00:09:49,760
devices

00:09:47,360 --> 00:09:51,920
and that device has its own memory

00:09:49,760 --> 00:09:54,080
that's attached to it

00:09:51,920 --> 00:09:56,480
this aligns with the actual hardware we

00:09:54,080 --> 00:09:59,040
have our cpu with the dram attached

00:09:56,480 --> 00:10:00,880
we then have some pcie or nv-link or

00:09:59,040 --> 00:10:02,160
something that connects the cpu to the

00:10:00,880 --> 00:10:04,560
gpu

00:10:02,160 --> 00:10:06,399
and then the gpu has its own memory it

00:10:04,560 --> 00:10:09,760
has hbm or something like that

00:10:06,399 --> 00:10:12,079
embedded on the accelerator card itself

00:10:09,760 --> 00:10:14,560
so this model although it's uh an

00:10:12,079 --> 00:10:17,120
abstract model in the sense that it um

00:10:14,560 --> 00:10:18,880
it's just defined an openmp's view of

00:10:17,120 --> 00:10:20,880
the world it does map to how these

00:10:18,880 --> 00:10:22,320
devices are actually connected into the

00:10:20,880 --> 00:10:26,079
processors

00:10:22,320 --> 00:10:26,079
and into the into the nodes as well

00:10:27,040 --> 00:10:34,399
so the target execution model for

00:10:30,399 --> 00:10:36,399
openmp well execution begins on the host

00:10:34,399 --> 00:10:37,680
and there are zero or more devices

00:10:36,399 --> 00:10:39,680
connected to the host

00:10:37,680 --> 00:10:41,120
and the memory spaces are not shared

00:10:39,680 --> 00:10:44,399
between the host

00:10:41,120 --> 00:10:45,680
and the device now with openmp it's

00:10:44,399 --> 00:10:47,279
typically called the shared memory

00:10:45,680 --> 00:10:48,399
programming model and that's true for

00:10:47,279 --> 00:10:52,560
the threads on the

00:10:48,399 --> 00:10:55,279
host cpu but there's no um

00:10:52,560 --> 00:10:57,839
the memory isn't necessarily always

00:10:55,279 --> 00:10:58,240
readable or anything or accessible from

00:10:57,839 --> 00:11:01,360
the

00:10:58,240 --> 00:11:02,800
um from the device and the host there

00:11:01,360 --> 00:11:04,959
has to be some

00:11:02,800 --> 00:11:06,480
um there's a movement of data between

00:11:04,959 --> 00:11:08,320
the host and the device

00:11:06,480 --> 00:11:10,320
so the memory spaces are not shared it's

00:11:08,320 --> 00:11:11,360
not a free-for-all necessarily

00:11:10,320 --> 00:11:13,680
although there's some features in

00:11:11,360 --> 00:11:16,240
openmp5 that do allow for that to be the

00:11:13,680 --> 00:11:18,000
case if the hardware supports it

00:11:16,240 --> 00:11:19,519
um so the memory model you have these

00:11:18,000 --> 00:11:23,440
distinct host and device

00:11:19,519 --> 00:11:24,000
uh spaces in openmp uh some data is

00:11:23,440 --> 00:11:26,079
going to be

00:11:24,000 --> 00:11:28,160
automatically copied between the host

00:11:26,079 --> 00:11:30,320
and the device and back again and

00:11:28,160 --> 00:11:32,880
there's also a lot of controls

00:11:30,320 --> 00:11:35,440
for explicitly copying data as of when

00:11:32,880 --> 00:11:35,440
you need it

00:11:35,600 --> 00:11:40,560
so for the execution model we start on

00:11:38,240 --> 00:11:42,320
the cpu with our serial program or even

00:11:40,560 --> 00:11:45,279
our parallel

00:11:42,320 --> 00:11:46,560
parallel openmp program on the cpu but

00:11:45,279 --> 00:11:49,200
at some point

00:11:46,560 --> 00:11:50,240
uh our single host thread is going to

00:11:49,200 --> 00:11:53,760
want to

00:11:50,240 --> 00:11:55,680
transfer execution to the device so that

00:11:53,760 --> 00:11:57,279
the execution of a particular part of

00:11:55,680 --> 00:11:59,760
the code

00:11:57,279 --> 00:12:00,560
can be executed on the gpu device

00:11:59,760 --> 00:12:03,279
instead

00:12:00,560 --> 00:12:05,360
and for that we use the target construct

00:12:03,279 --> 00:12:07,360
this target directive here

00:12:05,360 --> 00:12:08,720
so in fortran we have our sentinel

00:12:07,360 --> 00:12:11,920
symbol within omp

00:12:08,720 --> 00:12:12,720
and openmp all of the the majority of

00:12:11,920 --> 00:12:14,639
the um

00:12:12,720 --> 00:12:16,880
the ways you you write openmp code is

00:12:14,639 --> 00:12:19,519
with these compiler directives

00:12:16,880 --> 00:12:20,480
so we have our target and end target and

00:12:19,519 --> 00:12:22,639
any code

00:12:20,480 --> 00:12:24,560
that any of our fortran code that exists

00:12:22,639 --> 00:12:27,040
between the target and end target

00:12:24,560 --> 00:12:29,360
is going to be offloaded to the gpu is

00:12:27,040 --> 00:12:30,959
going to be executed there instead of on

00:12:29,360 --> 00:12:32,560
the host

00:12:30,959 --> 00:12:34,480
we can have a number of clauses to the

00:12:32,560 --> 00:12:36,079
target directive and they're often used

00:12:34,480 --> 00:12:38,240
to specify

00:12:36,079 --> 00:12:41,040
some of this data movement between the

00:12:38,240 --> 00:12:41,040
host and the device

00:12:41,600 --> 00:12:44,880
it's important to know that in in terms

00:12:43,600 --> 00:12:47,279
of the execution model

00:12:44,880 --> 00:12:48,880
that the host is going to remain idle

00:12:47,279 --> 00:12:52,480
until the target region

00:12:48,880 --> 00:12:54,720
completes um so when we have the

00:12:52,480 --> 00:12:56,320
um the target region execution is going

00:12:54,720 --> 00:12:57,120
to go to the device and the host will

00:12:56,320 --> 00:13:00,000
sit there and

00:12:57,120 --> 00:13:00,800
wait um until it's until it's uh

00:13:00,000 --> 00:13:02,800
finished

00:13:00,800 --> 00:13:05,360
now the exact semantics of how all of

00:13:02,800 --> 00:13:07,839
this works is based on tasks

00:13:05,360 --> 00:13:09,360
and in the repository there's a slide

00:13:07,839 --> 00:13:11,920
deck that talks about

00:13:09,360 --> 00:13:14,639
openmp tasking if you go to the slides

00:13:11,920 --> 00:13:16,639
and pdfs and then some older pdfs

00:13:14,639 --> 00:13:18,240
in there you'll be able to have a look

00:13:16,639 --> 00:13:20,240
and see

00:13:18,240 --> 00:13:21,760
in tutorial material like this what the

00:13:20,240 --> 00:13:24,000
openmp tasking model

00:13:21,760 --> 00:13:25,360
um is like but i'm not going to go into

00:13:24,000 --> 00:13:27,920
too much detail

00:13:25,360 --> 00:13:29,040
with that for here all that you sort of

00:13:27,920 --> 00:13:31,839
need to know is that

00:13:29,040 --> 00:13:33,920
the um the offload is all synchronous so

00:13:31,839 --> 00:13:34,560
the host offloads the device waits till

00:13:33,920 --> 00:13:37,600
it's done

00:13:34,560 --> 00:13:39,920
and then execution then continues on the

00:13:37,600 --> 00:13:39,920
host

00:13:41,839 --> 00:13:46,079
so programming a gpu is often useful to

00:13:45,040 --> 00:13:48,480
think about this in

00:13:46,079 --> 00:13:50,000
kind of two parts um there's the

00:13:48,480 --> 00:13:52,000
parallelism

00:13:50,000 --> 00:13:53,839
that we need to expose on the gpu and

00:13:52,000 --> 00:13:57,360
then also the movement betw

00:13:53,839 --> 00:13:59,680
of data between the host and the device

00:13:57,360 --> 00:14:01,040
so often programming the kernels and the

00:13:59,680 --> 00:14:04,000
loops themselves

00:14:01,040 --> 00:14:04,639
so this is writing the parallel loops

00:14:04,000 --> 00:14:06,560
and things

00:14:04,639 --> 00:14:08,079
this is often the easier part of those

00:14:06,560 --> 00:14:09,760
two of those two things

00:14:08,079 --> 00:14:11,920
it's often the easy bit to get right

00:14:09,760 --> 00:14:15,519
especially if you've already got

00:14:11,920 --> 00:14:17,199
a highly optimized cpu implementation

00:14:15,519 --> 00:14:18,880
most of the development and programming

00:14:17,199 --> 00:14:20,079
time in our experience at least is

00:14:18,880 --> 00:14:22,399
making sure that we

00:14:20,079 --> 00:14:23,519
optimize for the movement of memory

00:14:22,399 --> 00:14:25,920
between the

00:14:23,519 --> 00:14:28,079
host and the device making sure that the

00:14:25,920 --> 00:14:29,519
data transfer between the two distinct

00:14:28,079 --> 00:14:31,360
memory spaces

00:14:29,519 --> 00:14:33,279
is as optimized as possible is often

00:14:31,360 --> 00:14:35,600
where we spend most of the programming

00:14:33,279 --> 00:14:35,600
time

00:14:38,320 --> 00:14:42,639
as i just said a lot of the reasons for

00:14:41,199 --> 00:14:44,240
this are if you have already got a

00:14:42,639 --> 00:14:47,120
highly optimized

00:14:44,240 --> 00:14:49,199
cpu code then and you found lots of

00:14:47,120 --> 00:14:50,560
parallelism mapping that onto the gpu

00:14:49,199 --> 00:14:52,639
in terms of the parallelism is quite

00:14:50,560 --> 00:14:53,680
straightforward and also all those

00:14:52,639 --> 00:14:54,880
optimizations

00:14:53,680 --> 00:14:56,720
and some of them i talked about this

00:14:54,880 --> 00:14:58,240
morning particularly around memory

00:14:56,720 --> 00:15:00,560
layout and for

00:14:58,240 --> 00:15:02,639
memory layout for vectorization they

00:15:00,560 --> 00:15:03,199
often apply to gpus in exactly the same

00:15:02,639 --> 00:15:04,720
way

00:15:03,199 --> 00:15:06,240
things like having coalesce memory

00:15:04,720 --> 00:15:07,920
accesses that would support good

00:15:06,240 --> 00:15:09,279
vectorization on the cpu

00:15:07,920 --> 00:15:13,279
are going to really help you have good

00:15:09,279 --> 00:15:14,639
performance on this on the gpu as well

00:15:13,279 --> 00:15:15,920
so we're going to take this this

00:15:14,639 --> 00:15:16,560
approach and we're going to break down

00:15:15,920 --> 00:15:20,160
the

00:15:16,560 --> 00:15:22,240
um the the two parts the the parallelism

00:15:20,160 --> 00:15:26,320
on the gpu and the memory movement on

00:15:22,240 --> 00:15:28,560
the gpu as well

00:15:26,320 --> 00:15:30,000
so to start with how can we actually get

00:15:28,560 --> 00:15:31,600
something running on the device well

00:15:30,000 --> 00:15:33,680
i've i've been through this briefly and

00:15:31,600 --> 00:15:36,399
we we have this target construct

00:15:33,680 --> 00:15:37,040
so we go omp target and all the code in

00:15:36,399 --> 00:15:41,199
there

00:15:37,040 --> 00:15:43,360
is going to be offloaded to the device

00:15:41,199 --> 00:15:45,040
now it's important to note that the x

00:15:43,360 --> 00:15:47,199
what the device is going to execute all

00:15:45,040 --> 00:15:49,120
that code is going to be in serial

00:15:47,199 --> 00:15:51,040
we just launched a single thread on our

00:15:49,120 --> 00:15:53,759
gpu essentially

00:15:51,040 --> 00:15:54,399
we need other constructs to be able to

00:15:53,759 --> 00:15:57,040
define

00:15:54,399 --> 00:15:58,160
and expand the parallelism that we're

00:15:57,040 --> 00:16:00,000
going to

00:15:58,160 --> 00:16:02,240
have to exploit within that target

00:16:00,000 --> 00:16:04,480
region so we're going to need to

00:16:02,240 --> 00:16:06,240
launch some some some parallel things

00:16:04,480 --> 00:16:08,880
within the target region and we'll come

00:16:06,240 --> 00:16:11,839
on to that in due course

00:16:08,880 --> 00:16:13,759
so that's important we stop executing on

00:16:11,839 --> 00:16:15,839
the host and we start executing on the

00:16:13,759 --> 00:16:19,120
device and it's serial

00:16:15,839 --> 00:16:19,120
execution on the device

00:16:20,079 --> 00:16:25,120
we can um we can allow

00:16:23,360 --> 00:16:26,800
the host to continue doing something

00:16:25,120 --> 00:16:28,800
else in the meantime we can

00:16:26,800 --> 00:16:30,240
have this idea of an asynchronous

00:16:28,800 --> 00:16:32,079
offload of a target region

00:16:30,240 --> 00:16:34,240
and for that we would add the no weight

00:16:32,079 --> 00:16:36,240
clause to the target directive

00:16:34,240 --> 00:16:37,920
so one of those clauses would be the

00:16:36,240 --> 00:16:40,560
word no wait

00:16:37,920 --> 00:16:41,600
this means that the openmp runtime

00:16:40,560 --> 00:16:44,560
packages up

00:16:41,600 --> 00:16:45,839
that target routine as a task that can

00:16:44,560 --> 00:16:49,360
then be run

00:16:45,839 --> 00:16:51,680
and as and when it feels like it and the

00:16:49,360 --> 00:16:53,680
the host can continue executing beyond

00:16:51,680 --> 00:16:54,639
the end target directive and and do

00:16:53,680 --> 00:16:56,079
something else for

00:16:54,639 --> 00:16:58,240
for instance it could do some header

00:16:56,079 --> 00:17:00,880
exchange or it could write some do some

00:16:58,240 --> 00:17:02,959
file i o something like that

00:17:00,880 --> 00:17:04,079
all the tasking mechanisms that are

00:17:02,959 --> 00:17:08,000
built into openmp

00:17:04,079 --> 00:17:09,760
are then used to wait for all of those

00:17:08,000 --> 00:17:11,679
target regions to finish typically you

00:17:09,760 --> 00:17:13,600
would use something like

00:17:11,679 --> 00:17:14,799
a task weight and that would wait for

00:17:13,600 --> 00:17:16,640
all the target regions to finish

00:17:14,799 --> 00:17:18,079
executing

00:17:16,640 --> 00:17:20,000
the other clauses that you might put on

00:17:18,079 --> 00:17:22,079
the target region are all about memory

00:17:20,000 --> 00:17:24,640
movement which i'm going to come on to

00:17:22,079 --> 00:17:24,640
in a moment

00:17:26,079 --> 00:17:29,760
so it's important to remember that we

00:17:27,679 --> 00:17:30,480
really have these two distinct memory

00:17:29,760 --> 00:17:32,559
spaces

00:17:30,480 --> 00:17:34,160
the host memory space and the target

00:17:32,559 --> 00:17:37,039
memory space

00:17:34,160 --> 00:17:39,360
now all the host memory space is shared

00:17:37,039 --> 00:17:41,520
between all the cores on the host

00:17:39,360 --> 00:17:42,640
and all the memory space on the target

00:17:41,520 --> 00:17:44,720
is shared between

00:17:42,640 --> 00:17:45,919
all the parallel executing things on the

00:17:44,720 --> 00:17:49,360
target

00:17:45,919 --> 00:17:52,559
unless we specify any sort of um data

00:17:49,360 --> 00:17:55,120
data sharing or something like that

00:17:52,559 --> 00:17:57,280
but there's no um automatic sharing of

00:17:55,120 --> 00:17:58,559
the memory between the

00:17:57,280 --> 00:18:00,880
all of the memory of the host and all of

00:17:58,559 --> 00:18:02,160
the targets so we what we have to do is

00:18:00,880 --> 00:18:04,080
we have to

00:18:02,160 --> 00:18:05,919
use a combination of the implicit

00:18:04,080 --> 00:18:07,280
movement and the explicit

00:18:05,919 --> 00:18:09,600
management of that memory movement

00:18:07,280 --> 00:18:10,880
that's available in openmp

00:18:09,600 --> 00:18:12,559
and this is pretty much the most

00:18:10,880 --> 00:18:13,440
complicated part of the offload

00:18:12,559 --> 00:18:14,880
specification

00:18:13,440 --> 00:18:16,320
if you're if you're feeling very brave

00:18:14,880 --> 00:18:17,600
and fancy reading the openmp

00:18:16,320 --> 00:18:19,360
specification

00:18:17,600 --> 00:18:21,919
there's quite a lot here in the spec

00:18:19,360 --> 00:18:23,919
that talks about how data is implicitly

00:18:21,919 --> 00:18:26,799
mapped and explicitly mapped between

00:18:23,919 --> 00:18:28,240
those two memory spaces

00:18:26,799 --> 00:18:29,919
it's also important to make sure that we

00:18:28,240 --> 00:18:32,000
get this right memory movement

00:18:29,919 --> 00:18:34,480
is is often the the performance killer

00:18:32,000 --> 00:18:37,360
in our applications

00:18:34,480 --> 00:18:37,840
if we use this nvidia v100 as an example

00:18:37,360 --> 00:18:40,480
again

00:18:37,840 --> 00:18:41,760
it has 900 gigabytes a second of memory

00:18:40,480 --> 00:18:43,679
bandwidth

00:18:41,760 --> 00:18:45,600
available to it now that's between the

00:18:43,679 --> 00:18:47,280
the parallel things on the device

00:18:45,600 --> 00:18:50,160
and the the high bandwidth memory on the

00:18:47,280 --> 00:18:51,120
device if we plug it into our cpu node

00:18:50,160 --> 00:18:52,960
with pcie

00:18:51,120 --> 00:18:55,679
we're only going to get 32 gigabytes a

00:18:52,960 --> 00:18:57,280
second peak bandwidth across the pcie

00:18:55,679 --> 00:18:58,880
so the transfers between the host and

00:18:57,280 --> 00:19:00,880
the device are relatively slow

00:18:58,880 --> 00:19:03,280
so we need to minimize them to make sure

00:19:00,880 --> 00:19:04,640
that we do as few of them as possible so

00:19:03,280 --> 00:19:06,960
that we can

00:19:04,640 --> 00:19:08,400
keep the the data resident as much as

00:19:06,960 --> 00:19:12,559
possible on the gpu

00:19:08,400 --> 00:19:13,120
and and avoid using being bottlenecked

00:19:12,559 --> 00:19:14,960
by this

00:19:13,120 --> 00:19:18,240
by this lower bandwidth that we have

00:19:14,960 --> 00:19:18,240
between the two memory spaces

00:19:19,600 --> 00:19:23,679
okay so how can we get data from the

00:19:21,440 --> 00:19:26,000
host um to the device and back again

00:19:23,679 --> 00:19:27,520
well we need to map the data between the

00:19:26,000 --> 00:19:29,760
host and the device

00:19:27,520 --> 00:19:31,280
and the way openmp does this is it it

00:19:29,760 --> 00:19:32,640
uses the variable names

00:19:31,280 --> 00:19:34,080
and really it's the you know the

00:19:32,640 --> 00:19:36,080
pointers the implementation of where

00:19:34,080 --> 00:19:37,120
that exists in memory to map these

00:19:36,080 --> 00:19:40,320
things

00:19:37,120 --> 00:19:43,280
so variable names uh really exist in

00:19:40,320 --> 00:19:45,039
in kind of um in two places at the same

00:19:43,280 --> 00:19:47,440
time essentially they exist in host

00:19:45,039 --> 00:19:49,520
space and they exist in device space

00:19:47,440 --> 00:19:50,799
and the compiler sorts out which one you

00:19:49,520 --> 00:19:53,840
mean when you use them in

00:19:50,799 --> 00:19:56,720
in your code so in this example

00:19:53,840 --> 00:19:58,720
of the code here i might have um the the

00:19:56,720 --> 00:20:00,000
first line here is x and that's an array

00:19:58,720 --> 00:20:01,919
on the host

00:20:00,000 --> 00:20:04,080
and but inside the target region when i

00:20:01,919 --> 00:20:07,280
use the variable x in my program

00:20:04,080 --> 00:20:07,840
it's going to be referring to the um the

00:20:07,280 --> 00:20:09,760
variable

00:20:07,840 --> 00:20:11,360
x the array x that's in the device

00:20:09,760 --> 00:20:12,240
memory space instead and that's because

00:20:11,360 --> 00:20:15,520
it's inside the

00:20:12,240 --> 00:20:18,320
the target region so

00:20:15,520 --> 00:20:20,480
it's left up to the openmp runtime to

00:20:18,320 --> 00:20:21,919
work out when x is in host memory or

00:20:20,480 --> 00:20:23,679
when it's in device memory

00:20:21,919 --> 00:20:25,360
so really this is like having two

00:20:23,679 --> 00:20:26,559
separate arrays the host x and the

00:20:25,360 --> 00:20:28,960
device x

00:20:26,559 --> 00:20:30,159
and they are on the device and hosting

00:20:28,960 --> 00:20:32,320
device respectively

00:20:30,159 --> 00:20:34,320
based on their sort of prefix there but

00:20:32,320 --> 00:20:34,640
we don't have to have these two copies

00:20:34,320 --> 00:20:36,480
uh

00:20:34,640 --> 00:20:38,240
explicitly in our code it's just all

00:20:36,480 --> 00:20:38,640
wrapped up into them into the model that

00:20:38,240 --> 00:20:40,799
we

00:20:38,640 --> 00:20:43,760
that we're buying into with the openmp

00:20:40,799 --> 00:20:43,760
target offload

00:20:44,799 --> 00:20:48,080
so when might data move there are very

00:20:47,520 --> 00:20:50,000
specific

00:20:48,080 --> 00:20:51,440
places that data will move and by the

00:20:50,000 --> 00:20:53,360
time you get to this afternoon i will

00:20:51,440 --> 00:20:55,600
have gone through all of these

00:20:53,360 --> 00:20:57,840
so the first is on on the beginning or

00:20:55,600 --> 00:21:00,720
the end of a target region so when we

00:20:57,840 --> 00:21:02,159
can transfer execution from the host to

00:21:00,720 --> 00:21:04,799
the device

00:21:02,159 --> 00:21:06,960
that's a point at which data may move

00:21:04,799 --> 00:21:09,600
either at the beginning or the end

00:21:06,960 --> 00:21:11,440
we can also specify places where data

00:21:09,600 --> 00:21:13,520
will move explicitly these are these

00:21:11,440 --> 00:21:14,640
target enter and exit data constructs

00:21:13,520 --> 00:21:16,559
and also of

00:21:14,640 --> 00:21:18,080
a target update construct there's a

00:21:16,559 --> 00:21:20,880
those are the specific places that

00:21:18,080 --> 00:21:20,880
memory can move

00:21:21,760 --> 00:21:25,120
so let's talk about the default behavior

00:21:24,240 --> 00:21:27,360
openmp

00:21:25,120 --> 00:21:28,159
has a lot of implicit mapping between

00:21:27,360 --> 00:21:31,679
the host

00:21:28,159 --> 00:21:32,720
and the device so um the default

00:21:31,679 --> 00:21:35,840
behavior

00:21:32,720 --> 00:21:37,919
when we start a target region

00:21:35,840 --> 00:21:39,120
is that any scalar variables are going

00:21:37,919 --> 00:21:41,840
to be mapped first

00:21:39,120 --> 00:21:42,640
private so this means if you have any

00:21:41,840 --> 00:21:45,280
scalar

00:21:42,640 --> 00:21:46,400
values that are in your code outside the

00:21:45,280 --> 00:21:47,840
target region

00:21:46,400 --> 00:21:50,559
inside the target region they're

00:21:47,840 --> 00:21:51,120
available there's going to be a local

00:21:50,559 --> 00:21:52,960
copy

00:21:51,120 --> 00:21:55,520
to every parallel thing that's running

00:21:52,960 --> 00:21:57,760
on the gpu every parallel thread

00:21:55,520 --> 00:21:59,840
and each of them has its own local copy

00:21:57,760 --> 00:22:01,919
of that that it can freely update

00:21:59,840 --> 00:22:02,960
it's going to contain the value that it

00:22:01,919 --> 00:22:05,360
was on the host

00:22:02,960 --> 00:22:07,360
because it's first private but what this

00:22:05,360 --> 00:22:09,919
does mean is that if you update it it

00:22:07,360 --> 00:22:12,400
doesn't get copied back to the host

00:22:09,919 --> 00:22:13,280
if you're familiar with any of these um

00:22:12,400 --> 00:22:15,280
other

00:22:13,280 --> 00:22:17,120
um heterogeneous programming models like

00:22:15,280 --> 00:22:20,640
opencl

00:22:17,120 --> 00:22:22,880
this would be like passing in the

00:22:20,640 --> 00:22:24,240
variables as a kernel argument like a

00:22:22,880 --> 00:22:27,840
function argument

00:22:24,240 --> 00:22:29,360
passed by value so it saves the memory

00:22:27,840 --> 00:22:31,440
copy because you can just

00:22:29,360 --> 00:22:32,480
load it as part of the offload

00:22:31,440 --> 00:22:35,520
information

00:22:32,480 --> 00:22:37,280
um to the to the target

00:22:35,520 --> 00:22:39,600
so that's scalar variables they're all

00:22:37,280 --> 00:22:41,840
mapped first private

00:22:39,600 --> 00:22:43,280
now stack arrays these are are sort of

00:22:41,840 --> 00:22:46,080
local arrays

00:22:43,280 --> 00:22:47,919
that um in fortran you don't have the

00:22:46,080 --> 00:22:50,159
allocatable property these are just

00:22:47,919 --> 00:22:51,360
arrays that you just make of a given

00:22:50,159 --> 00:22:53,840
size

00:22:51,360 --> 00:22:54,960
these are going to be mapped to from

00:22:53,840 --> 00:22:57,600
this means that

00:22:54,960 --> 00:22:58,480
when they enter a target region they're

00:22:57,600 --> 00:23:00,480
copied

00:22:58,480 --> 00:23:02,159
and all the data in there is copied from

00:23:00,480 --> 00:23:04,400
the host to the device

00:23:02,159 --> 00:23:06,880
and it's available in this in the shared

00:23:04,400 --> 00:23:09,840
memory of the device to be used

00:23:06,880 --> 00:23:10,960
throughout that target region but then

00:23:09,840 --> 00:23:12,960
when that target region

00:23:10,960 --> 00:23:14,400
ends all those stack arrays are then

00:23:12,960 --> 00:23:17,200
mapped from

00:23:14,400 --> 00:23:19,039
the device back to the host at the end

00:23:17,200 --> 00:23:21,679
of the target region

00:23:19,039 --> 00:23:23,360
so scalars are going to happen be copied

00:23:21,679 --> 00:23:26,080
automatically

00:23:23,360 --> 00:23:26,880
uh stack arrays are gonna be copied uh

00:23:26,080 --> 00:23:29,039
to and from

00:23:26,880 --> 00:23:30,720
automatically but heap arrays things

00:23:29,039 --> 00:23:32,320
with the allocatable property these are

00:23:30,720 --> 00:23:35,280
not mapped by default at all

00:23:32,320 --> 00:23:38,559
and we as programmers need to map these

00:23:35,280 --> 00:23:38,559
heap arrays ourselves

00:23:38,799 --> 00:23:43,520
and to do that we're going to be using

00:23:40,159 --> 00:23:45,440
this map clause and this specifies

00:23:43,520 --> 00:23:46,960
information about the transfer of data

00:23:45,440 --> 00:23:51,440
between the host

00:23:46,960 --> 00:23:53,520
and the device on the target region

00:23:51,440 --> 00:23:54,480
so let's say we have this array a of of

00:23:53,520 --> 00:23:56,640
length n

00:23:54,480 --> 00:23:57,919
so it's gonna be from one to n in size

00:23:56,640 --> 00:24:01,360
and we have a scalar

00:23:57,919 --> 00:24:02,480
x um in fortran it's often very

00:24:01,360 --> 00:24:04,559
convenient because

00:24:02,480 --> 00:24:05,600
the size of that array is generally

00:24:04,559 --> 00:24:08,000
embedded within the

00:24:05,600 --> 00:24:09,440
within the type the array itself knows

00:24:08,000 --> 00:24:12,640
exactly how big

00:24:09,440 --> 00:24:14,080
um how what the extent of that array is

00:24:12,640 --> 00:24:16,480
so in fortran you often don't need to

00:24:14,080 --> 00:24:17,919
specify the the size of that array that

00:24:16,480 --> 00:24:20,720
you need to copy

00:24:17,919 --> 00:24:22,320
back to the between the host and the

00:24:20,720 --> 00:24:23,600
device and the map clause we could just

00:24:22,320 --> 00:24:26,000
give it the variable name

00:24:23,600 --> 00:24:27,279
if you uh decide to write openmp

00:24:26,000 --> 00:24:29,440
programs in c

00:24:27,279 --> 00:24:31,600
whereas arrays are simply just pointers

00:24:29,440 --> 00:24:33,440
you do need to specify

00:24:31,600 --> 00:24:35,520
the amount of data to copy but with

00:24:33,440 --> 00:24:37,440
fortran you can just give it the array

00:24:35,520 --> 00:24:39,440
and it sort of remembers how big

00:24:37,440 --> 00:24:40,960
um how much how much data is in that

00:24:39,440 --> 00:24:43,840
array

00:24:40,960 --> 00:24:45,600
you can use array slicing as well and

00:24:43,840 --> 00:24:47,039
that allows you to just copy portions of

00:24:45,600 --> 00:24:48,960
the array and that will work

00:24:47,039 --> 00:24:50,880
as you would expect it uses a similar

00:24:48,960 --> 00:24:52,799
notation in openmp

00:24:50,880 --> 00:24:54,559
inside the map clause to the fortran

00:24:52,799 --> 00:24:57,039
array slicing

00:24:54,559 --> 00:24:58,720
assume size arrays get a bit thorny but

00:24:57,039 --> 00:25:00,880
it's best not to use assume size

00:24:58,720 --> 00:25:02,320
arrays inside the low level parts of

00:25:00,880 --> 00:25:05,520
your kernel anyway

00:25:02,320 --> 00:25:08,640
you can very fairly straightforward in

00:25:05,520 --> 00:25:09,360
in fortran um specify the size of those

00:25:08,640 --> 00:25:11,039
arrays

00:25:09,360 --> 00:25:13,279
so that the compiler has the very best

00:25:11,039 --> 00:25:15,279
job of knowing what the extent of those

00:25:13,279 --> 00:25:17,039
arrays actually are when you when you

00:25:15,279 --> 00:25:19,360
want to have a low level loop in your

00:25:17,039 --> 00:25:19,360
code

00:25:19,679 --> 00:25:23,600
so we have these these arrays and

00:25:22,000 --> 00:25:25,440
scalars that we want to

00:25:23,600 --> 00:25:27,440
explicitly transfer between the host and

00:25:25,440 --> 00:25:29,520
the device when when we're offloading

00:25:27,440 --> 00:25:31,840
when we we have the target region

00:25:29,520 --> 00:25:32,720
so we have our target region the omp

00:25:31,840 --> 00:25:35,200
target and

00:25:32,720 --> 00:25:37,440
end target at the end and we have our

00:25:35,200 --> 00:25:39,360
map clause and inside those parentheses

00:25:37,440 --> 00:25:42,080
we're going to list all the variables

00:25:39,360 --> 00:25:42,799
that we need to transfer the data

00:25:42,080 --> 00:25:44,240
between

00:25:42,799 --> 00:25:48,000
the host and the device it's doing a

00:25:44,240 --> 00:25:48,000
memory copy between the two spaces

00:25:48,159 --> 00:25:52,640
so the direction of the transfer is

00:25:50,400 --> 00:25:54,240
always from the host's perspective it's

00:25:52,640 --> 00:25:57,440
always going to be

00:25:54,240 --> 00:25:59,360
from the host to the device or from the

00:25:57,440 --> 00:26:00,720
device to the host it's always the host

00:25:59,360 --> 00:26:02,640
that has the

00:26:00,720 --> 00:26:05,279
the last say in the in the perspective

00:26:02,640 --> 00:26:05,279
of the direction

00:26:05,600 --> 00:26:09,840
so as well as specifying the variables

00:26:07,760 --> 00:26:12,000
in this comma separated list we can also

00:26:09,840 --> 00:26:14,240
specify this tag just before

00:26:12,000 --> 00:26:16,159
beforehand with the um with the colon

00:26:14,240 --> 00:26:17,679
and this specifies the direction of the

00:26:16,159 --> 00:26:20,559
transfer

00:26:17,679 --> 00:26:22,559
so let's have a look at map two map two

00:26:20,559 --> 00:26:24,880
let's say we've got a and x

00:26:22,559 --> 00:26:25,760
this means that on entering that target

00:26:24,880 --> 00:26:28,000
region

00:26:25,760 --> 00:26:29,440
um the array and the scalar x is going

00:26:28,000 --> 00:26:32,159
to be copied

00:26:29,440 --> 00:26:33,039
um to the device so from the host to the

00:26:32,159 --> 00:26:36,559
device

00:26:33,039 --> 00:26:36,559
and that's at the start of the region

00:26:37,120 --> 00:26:41,279
now then there's map from and we can do

00:26:40,159 --> 00:26:43,679
the same thing

00:26:41,279 --> 00:26:45,120
this means that when we exit the target

00:26:43,679 --> 00:26:48,960
region so when we have that

00:26:45,120 --> 00:26:50,720
omp end target data is going to be

00:26:48,960 --> 00:26:54,000
copied from the device

00:26:50,720 --> 00:26:54,880
back to the host so that means that it's

00:26:54,000 --> 00:26:56,640
going to

00:26:54,880 --> 00:26:58,240
move and get copied any updates we've

00:26:56,640 --> 00:27:00,799
had on the device is going to come

00:26:58,240 --> 00:27:02,720
back and be visible on the host but what

00:27:00,799 --> 00:27:04,320
this does mean is that at the start of

00:27:02,720 --> 00:27:07,279
the target region

00:27:04,320 --> 00:27:09,440
although data will the the the memory

00:27:07,279 --> 00:27:09,919
has been allocated because there's space

00:27:09,440 --> 00:27:11,760
for it

00:27:09,919 --> 00:27:13,679
because we know it exists on the device

00:27:11,760 --> 00:27:16,000
because it appears in a map clause

00:27:13,679 --> 00:27:17,120
the actual values that those map the

00:27:16,000 --> 00:27:18,960
data um

00:27:17,120 --> 00:27:20,960
takes that a and x are going to be

00:27:18,960 --> 00:27:24,399
uninitialized it's not going to contain

00:27:20,960 --> 00:27:26,399
anything um any particular values

00:27:24,399 --> 00:27:28,399
so if we initialize a on the host but

00:27:26,399 --> 00:27:30,720
don't copy it to the device

00:27:28,399 --> 00:27:32,559
then if we have it appearing in one of

00:27:30,720 --> 00:27:34,320
these map clauses

00:27:32,559 --> 00:27:36,960
there's space for it but it contains

00:27:34,320 --> 00:27:38,880
just unallocated data

00:27:36,960 --> 00:27:41,200
now sometimes we want this behavior so

00:27:38,880 --> 00:27:42,960
we want to copy data to the device

00:27:41,200 --> 00:27:45,120
update it and then bring the updated

00:27:42,960 --> 00:27:47,919
copy back and this is where the to

00:27:45,120 --> 00:27:48,480
from mapping comes in so we could map

00:27:47,919 --> 00:27:51,440
two from

00:27:48,480 --> 00:27:53,120
the two variables a and x and this is

00:27:51,440 --> 00:27:54,399
exactly the same as applying both of

00:27:53,120 --> 00:27:58,000
those maps together

00:27:54,399 --> 00:27:58,000
the map2 and the map from

00:27:58,320 --> 00:28:02,559
finally we may want to allocate some

00:28:00,880 --> 00:28:04,399
memory on the device

00:28:02,559 --> 00:28:06,240
but it's only ever going to be seen

00:28:04,399 --> 00:28:07,600
within that target region we never need

00:28:06,240 --> 00:28:11,039
to bring it back to the host

00:28:07,600 --> 00:28:11,760
within that target region for that we

00:28:11,039 --> 00:28:14,960
can use the

00:28:11,760 --> 00:28:15,279
alloc identifier on the map so we can

00:28:14,960 --> 00:28:17,520
map

00:28:15,279 --> 00:28:18,799
out a and that would allocate an array

00:28:17,520 --> 00:28:20,720
the same size as a

00:28:18,799 --> 00:28:23,600
on the device and would use be able to

00:28:20,720 --> 00:28:25,039
access it using the the variable name a

00:28:23,600 --> 00:28:26,880
so this isn't going to copy anything

00:28:25,039 --> 00:28:28,559
from the host it's not going to copy it

00:28:26,880 --> 00:28:30,640
back at the end of the target region

00:28:28,559 --> 00:28:33,279
either and the value inside that is

00:28:30,640 --> 00:28:35,200
again uninitialized

00:28:33,279 --> 00:28:36,320
if we have allocated it on the host

00:28:35,200 --> 00:28:38,320
elsewhere

00:28:36,320 --> 00:28:40,080
um and we're in some of the more

00:28:38,320 --> 00:28:42,159
unstructured ways of mapping data then

00:28:40,080 --> 00:28:42,559
we can copy the values of a back to the

00:28:42,159 --> 00:28:44,399
host

00:28:42,559 --> 00:28:45,760
with something like update for instance

00:28:44,399 --> 00:28:49,039
but we'll come to that

00:28:45,760 --> 00:28:49,039
um after the coffee break

00:28:49,440 --> 00:28:53,600
so that's it we're here we're we're

00:28:51,120 --> 00:28:56,480
already at our first exercise

00:28:53,600 --> 00:28:58,000
um so at this point where we are going

00:28:56,480 --> 00:28:59,760
to take the five-point stencil

00:28:58,000 --> 00:29:00,880
code that we've been looking at this

00:28:59,760 --> 00:29:02,000
morning and for those of you joining

00:29:00,880 --> 00:29:05,120
this afternoon

00:29:02,000 --> 00:29:06,399
um there is a file there called stencil

00:29:05,120 --> 00:29:10,000
underscore numa

00:29:06,399 --> 00:29:11,600
f90 um that's the place uh where

00:29:10,000 --> 00:29:14,080
this morning's attendees have should

00:29:11,600 --> 00:29:17,200
have roughly gotten up to

00:29:14,080 --> 00:29:20,880
uh this is a the numeraware parallel uh

00:29:17,200 --> 00:29:22,480
optimized uh five point stencil so

00:29:20,880 --> 00:29:24,559
we're going to take that code and we're

00:29:22,480 --> 00:29:26,240
going to port it to the gpu and we're

00:29:24,559 --> 00:29:26,720
going to use the information that we've

00:29:26,240 --> 00:29:29,679
had

00:29:26,720 --> 00:29:32,000
so far so the two things really to do in

00:29:29,679 --> 00:29:34,559
this exercise are going to be

00:29:32,000 --> 00:29:35,919
using the target construct to transfer

00:29:34,559 --> 00:29:39,360
execution of the main

00:29:35,919 --> 00:29:41,760
kernel to the to the gpu and then

00:29:39,360 --> 00:29:43,039
we're going to be using map clauses to

00:29:41,760 --> 00:29:45,520
transfer the data

00:29:43,039 --> 00:29:46,720
at the point of the the target transfer

00:29:45,520 --> 00:29:50,000
this this target

00:29:46,720 --> 00:29:52,399
uh region uh to to map the data

00:29:50,000 --> 00:29:54,080
to and from the device and there's two

00:29:52,399 --> 00:29:56,000
arrays a current array in a temporary

00:29:54,080 --> 00:29:57,760
array and we can use map clauses

00:29:56,000 --> 00:29:59,039
to make sure that that those arrays

00:29:57,760 --> 00:30:00,720
exist

00:29:59,039 --> 00:30:02,240
in the stencil code those arrays are

00:30:00,720 --> 00:30:04,559
allocated on the heap they're

00:30:02,240 --> 00:30:06,399
allocatable fortran array so we must use

00:30:04,559 --> 00:30:08,720
these map clauses in order to be able to

00:30:06,399 --> 00:30:10,559
transfer the data

00:30:08,720 --> 00:30:12,640
so that's the that's over to you the

00:30:10,559 --> 00:30:14,480
exercise for those of you joining and

00:30:12,640 --> 00:30:16,000
don't have an ismart account i'm just

00:30:14,480 --> 00:30:17,679
going to show up some some slides that

00:30:16,000 --> 00:30:19,840
show you how to connect

00:30:17,679 --> 00:30:22,080
and for those of you that do please get

00:30:19,840 --> 00:30:25,200
started with the exercise

00:30:22,080 --> 00:30:28,399
we're going to come back at um in about

00:30:25,200 --> 00:30:31,919
a half an hour or so at 25 to 3 so

00:30:28,399 --> 00:30:35,200
4 35 and then we'll start talking about

00:30:31,919 --> 00:30:36,720
parallelism at that point

00:30:35,200 --> 00:30:38,399
um so good luck with the exercise please

00:30:36,720 --> 00:30:39,840
use the q a um

00:30:38,399 --> 00:30:41,520
i'll be starting to look at that now to

00:30:39,840 --> 00:30:43,120
ask lots of questions

00:30:41,520 --> 00:30:44,799
um i'm just going to switch the slide

00:30:43,120 --> 00:30:46,159
deck over so that we can

00:30:44,799 --> 00:30:48,000
for those of you who don't have an isn't

00:30:46,159 --> 00:30:50,559
bad account can find out that

00:30:48,000 --> 00:30:52,159
that information okay so for those of

00:30:50,559 --> 00:30:55,159
you joining this afternoon

00:30:52,159 --> 00:30:56,320
um please go to this webpage here this

00:30:55,159 --> 00:30:59,919
tinyurl.com

00:30:56,320 --> 00:31:01,760
forward slash openmp iphone 2020

00:30:59,919 --> 00:31:03,440
when you get there there'll be a short

00:31:01,760 --> 00:31:05,440
form to fill in that will give you

00:31:03,440 --> 00:31:07,679
a training account on the izombard

00:31:05,440 --> 00:31:11,279
supercomputer as i introduced

00:31:07,679 --> 00:31:12,799
earlier um with that number you'll give

00:31:11,279 --> 00:31:13,120
you a number and then you'll be able to

00:31:12,799 --> 00:31:15,360
log

00:31:13,120 --> 00:31:18,240
in to the gateway node using this

00:31:15,360 --> 00:31:22,000
command on the second point ssh

00:31:18,240 --> 00:31:24,880
to isombar.gw4.ac.uk

00:31:22,000 --> 00:31:25,440
and the username you have is br hyphen

00:31:24,880 --> 00:31:27,120
train

00:31:25,440 --> 00:31:29,039
and then a number and that number will

00:31:27,120 --> 00:31:32,480
be whatever that form gave you

00:31:29,039 --> 00:31:34,880
in um in step one

00:31:32,480 --> 00:31:35,840
the password for all the accounts is is

00:31:34,880 --> 00:31:39,039
as on the slide

00:31:35,840 --> 00:31:41,679
openmp uh ug for user group

00:31:39,039 --> 00:31:44,159
and the number 20 with with ug as

00:31:41,679 --> 00:31:46,080
capitals

00:31:44,159 --> 00:31:47,679
so that ssh command will get you onto

00:31:46,080 --> 00:31:48,559
the gateway node and then you need to

00:31:47,679 --> 00:31:50,640
log on to the

00:31:48,559 --> 00:31:52,720
the login nodes of of the the part of

00:31:50,640 --> 00:31:55,039
isn't bad we'll be using today

00:31:52,720 --> 00:31:56,640
so your ssh into isimbard and then you

00:31:55,039 --> 00:31:58,559
will ssh into phase

00:31:56,640 --> 00:32:00,000
one and once you're there you're in the

00:31:58,559 --> 00:32:02,399
right place and you just

00:32:00,000 --> 00:32:03,679
need to change directory into this

00:32:02,399 --> 00:32:05,440
openmp for cs

00:32:03,679 --> 00:32:06,880
and in there is the slides and the

00:32:05,440 --> 00:32:09,200
exercises

00:32:06,880 --> 00:32:11,279
in the slides directory there's a pdf

00:32:09,200 --> 00:32:12,000
subdirectory that contains the pdfs that

00:32:11,279 --> 00:32:15,840
you're

00:32:12,000 --> 00:32:15,840
that i'm showing you as well

00:32:16,080 --> 00:32:19,840
once you're there to use zimbard you can

00:32:17,679 --> 00:32:21,760
just build all the exercises

00:32:19,840 --> 00:32:23,760
um the the one we'll start if you're

00:32:21,760 --> 00:32:25,679
starting today is the stencil luma

00:32:23,760 --> 00:32:27,360
so you could just type make stencil

00:32:25,679 --> 00:32:29,279
underscore numa that would build that

00:32:27,360 --> 00:32:31,039
cpu binary

00:32:29,279 --> 00:32:32,799
and then you can submit a job now you

00:32:31,039 --> 00:32:33,519
might need to change the job submission

00:32:32,799 --> 00:32:35,600
script to

00:32:33,519 --> 00:32:36,960
be the right binary name but if you're

00:32:35,600 --> 00:32:38,640
working from this morning it's probably

00:32:36,960 --> 00:32:39,200
just going to still be the same stencil

00:32:38,640 --> 00:32:42,000
so

00:32:39,200 --> 00:32:43,679
update that file submit stencil and then

00:32:42,000 --> 00:32:46,320
to start running your code

00:32:43,679 --> 00:32:48,159
you would type submit q sub and then

00:32:46,320 --> 00:32:49,360
this the job submission script submit

00:32:48,159 --> 00:32:51,760
stencil

00:32:49,360 --> 00:32:54,240
that will then run that inside the queue

00:32:51,760 --> 00:32:56,080
and produce an output in a file

00:32:54,240 --> 00:32:57,440
now you can see how your job is

00:32:56,080 --> 00:33:00,399
progressing through the queue with this

00:32:57,440 --> 00:33:02,720
command on line eight qstat minus u

00:33:00,399 --> 00:33:04,399
uh dollar user but the jobs tend to go

00:33:02,720 --> 00:33:05,919
through very fast so by the time you've

00:33:04,399 --> 00:33:07,440
typed qstat the job's gone through

00:33:05,919 --> 00:33:09,440
anyway

00:33:07,440 --> 00:33:11,039
the output is always going to be an a in

00:33:09,440 --> 00:33:11,760
a file and that file is going to be

00:33:11,039 --> 00:33:15,360
called

00:33:11,760 --> 00:33:17,919
stencil or a stencil and then a

00:33:15,360 --> 00:33:18,720
period character a full stop and then a

00:33:17,919 --> 00:33:21,840
lowercase

00:33:18,720 --> 00:33:24,240
o number uh like a character o

00:33:21,840 --> 00:33:25,200
and then a number and that number is the

00:33:24,240 --> 00:33:27,440
job number

00:33:25,200 --> 00:33:28,480
that step seven here the q sub command

00:33:27,440 --> 00:33:30,159
returned you

00:33:28,480 --> 00:33:34,159
so you can have a look in there and see

00:33:30,159 --> 00:33:34,159
the output from the the job that ran

00:33:34,799 --> 00:33:38,000
so i'll leave this up for a little while

00:33:36,880 --> 00:33:40,240
before switching back over

00:33:38,000 --> 00:33:41,360
to the exercise slide for those of you

00:33:40,240 --> 00:33:43,600
that need this

00:33:41,360 --> 00:33:44,880
url to get yourself set up with an

00:33:43,600 --> 00:33:46,799
account

00:33:44,880 --> 00:33:48,480
please use the q and zoom to ask any

00:33:46,799 --> 00:33:48,960
questions i'm going to switch over to

00:33:48,480 --> 00:33:50,960
that now

00:33:48,960 --> 00:33:52,240
and start asking any questions that have

00:33:50,960 --> 00:33:55,200
come in

00:33:52,240 --> 00:33:56,799
um and we'll start going back to talk a

00:33:55,200 --> 00:33:58,559
little bit about this exercise and move

00:33:56,799 --> 00:34:01,919
on to the next part of the

00:33:58,559 --> 00:34:07,279
um of the day at uh 2 35

00:34:01,919 --> 00:34:08,960
14 35 that's uk time

00:34:07,279 --> 00:34:11,200
if there's a great question just come in

00:34:08,960 --> 00:34:13,440
on the the chat

00:34:11,200 --> 00:34:14,879
you don't need to change the queue name

00:34:13,440 --> 00:34:16,240
or anything just keep using the

00:34:14,879 --> 00:34:18,000
submission script

00:34:16,240 --> 00:34:20,800
that's going to get you on the node with

00:34:18,000 --> 00:34:23,839
the gpu and say you'll just be able to

00:34:20,800 --> 00:34:27,359
to build your codes as before

00:34:23,839 --> 00:34:28,800
and um and just nothing needs to change

00:34:27,359 --> 00:34:30,480
in your programming environment to start

00:34:28,800 --> 00:34:31,359
running on the gpus it's all been set up

00:34:30,480 --> 00:34:33,760
for you

00:34:31,359 --> 00:34:35,599
a cray on a cray system this is

00:34:33,760 --> 00:34:37,200
generally made very straightforward we

00:34:35,599 --> 00:34:38,079
don't need any extra compiler flags to

00:34:37,200 --> 00:34:41,280
build openmp

00:34:38,079 --> 00:34:43,200
for the gpu we just need to load some

00:34:41,280 --> 00:34:45,599
specific modules which have already been

00:34:43,200 --> 00:34:47,440
loaded for all of the training accounts

00:34:45,599 --> 00:34:49,760
if you type module list you can see the

00:34:47,440 --> 00:34:52,240
sort of modules that get loaded for

00:34:49,760 --> 00:34:53,520
you to take that sort of information

00:34:52,240 --> 00:34:56,639
away to your own

00:34:53,520 --> 00:34:58,400
um cray systems with other compilers you

00:34:56,639 --> 00:35:01,280
may need to have some additional flags

00:34:58,400 --> 00:35:02,000
um and there's information about that

00:35:01,280 --> 00:35:04,480
with all the

00:35:02,000 --> 00:35:07,359
compiler documentation about what flags

00:35:04,480 --> 00:35:07,359
you might need for those

00:35:09,359 --> 00:35:15,359
still plenty of time for this exercise

00:35:12,640 --> 00:35:16,400
about another 10 minutes or so before we

00:35:15,359 --> 00:35:19,920
start the next

00:35:16,400 --> 00:35:22,160
session i might just go through the um

00:35:19,920 --> 00:35:23,359
things that i might have changed just a

00:35:22,160 --> 00:35:26,320
couple of minutes before that

00:35:23,359 --> 00:35:27,599
so in just under 10 minutes i'll i'll

00:35:26,320 --> 00:35:29,599
show you what

00:35:27,599 --> 00:35:30,880
how i might have done this exercise just

00:35:29,599 --> 00:35:34,079
using the knowledge that we've

00:35:30,880 --> 00:35:36,320
um that i've shown you so far

00:35:34,079 --> 00:35:37,680
and then we'll move on to the target

00:35:36,320 --> 00:35:41,040
parallelism section

00:35:37,680 --> 00:35:43,599
um of this afternoon so

00:35:41,040 --> 00:35:45,839
another nearly 10 minutes before we come

00:35:43,599 --> 00:35:45,839
back

00:35:47,760 --> 00:35:53,440
okay welcome back it sounds like a few

00:35:50,960 --> 00:35:57,760
of you have managed to get some

00:35:53,440 --> 00:35:59,680
some code successfully built and running

00:35:57,760 --> 00:36:02,560
i'm going to show you how i might have

00:35:59,680 --> 00:36:06,560
have done this myself just using the

00:36:02,560 --> 00:36:08,079
the beginning so let's start with

00:36:06,560 --> 00:36:10,880
with this hopefully you can see my

00:36:08,079 --> 00:36:10,880
terminal here

00:36:11,119 --> 00:36:18,640
so this is the um uh the stencil

00:36:15,359 --> 00:36:20,160
numa file and this is where

00:36:18,640 --> 00:36:22,160
uh what some of our code might have

00:36:20,160 --> 00:36:24,960
looked like after this morning

00:36:22,160 --> 00:36:25,680
um and to start with i took away all of

00:36:24,960 --> 00:36:27,760
the

00:36:25,680 --> 00:36:28,880
the parallelism around the kernel just

00:36:27,760 --> 00:36:31,599
because we were just going to use the

00:36:28,880 --> 00:36:34,880
things that we've started with so far

00:36:31,599 --> 00:36:38,880
so around this kernel i've added a

00:36:34,880 --> 00:36:41,920
the sentinel omp target to say that this

00:36:38,880 --> 00:36:43,839
this nest of loops here um let me turn

00:36:41,920 --> 00:36:47,119
on line numbers

00:36:43,839 --> 00:36:49,040
uh between lines 16 and 21 those are the

00:36:47,119 --> 00:36:50,640
loops that i want to start running on my

00:36:49,040 --> 00:36:52,640
gpu

00:36:50,640 --> 00:36:54,320
and to say that they are ending i have

00:36:52,640 --> 00:36:57,520
to have the end target

00:36:54,320 --> 00:37:00,800
um as well to show that um it's that

00:36:57,520 --> 00:37:02,240
that's where the uh the the um execution

00:37:00,800 --> 00:37:05,920
needs to return back to the host

00:37:02,240 --> 00:37:09,040
at that end target statement on line 22.

00:37:05,920 --> 00:37:12,320
so that's the execution taken care of

00:37:09,040 --> 00:37:15,680
i now need to deal with the the mapping

00:37:12,320 --> 00:37:19,359
so i now have two arrays

00:37:15,680 --> 00:37:22,000
a temp and a that are used inside

00:37:19,359 --> 00:37:23,920
this loop so i need to make sure that

00:37:22,000 --> 00:37:26,240
they are copied from the host memory

00:37:23,920 --> 00:37:28,880
space to the device memory space

00:37:26,240 --> 00:37:29,680
so to do that i'll have the map clause

00:37:28,880 --> 00:37:33,440
with a to

00:37:29,680 --> 00:37:36,320
from um instruction in there

00:37:33,440 --> 00:37:38,079
and then the arrays listed a temp and a

00:37:36,320 --> 00:37:40,400
now luckily we're in fortran

00:37:38,079 --> 00:37:42,560
and because of the the way i have

00:37:40,400 --> 00:37:45,119
specified the size of those arrays on

00:37:42,560 --> 00:37:46,160
site on lines eight and nine those

00:37:45,119 --> 00:37:48,640
arrays they know

00:37:46,160 --> 00:37:49,359
fortran knows how big they are they know

00:37:48,640 --> 00:37:52,160
that they

00:37:49,359 --> 00:37:54,240
uh go from zero to nx plus one uh in

00:37:52,160 --> 00:37:54,880
each dimension so the compiler is fully

00:37:54,240 --> 00:37:56,720
aware

00:37:54,880 --> 00:37:58,400
of the size of those arrays if they

00:37:56,720 --> 00:37:59,280
weren't i might have to use the array

00:37:58,400 --> 00:38:01,440
notation

00:37:59,280 --> 00:38:04,880
uh to specify the the length of those

00:38:01,440 --> 00:38:07,599
arrays in each dimension

00:38:04,880 --> 00:38:09,440
now at this point i knew that my

00:38:07,599 --> 00:38:11,920
execution on the device was going to be

00:38:09,440 --> 00:38:13,920
in serial so i knew that

00:38:11,920 --> 00:38:16,560
when i do this reduction of total on

00:38:13,920 --> 00:38:18,400
line 19 i'm going to be updating that

00:38:16,560 --> 00:38:20,480
variable as well so to make sure i get

00:38:18,400 --> 00:38:24,000
the answer back

00:38:20,480 --> 00:38:26,000
i also added the total scalar variable

00:38:24,000 --> 00:38:29,040
to my map clause

00:38:26,000 --> 00:38:32,400
now if i don't map the the scalar total

00:38:29,040 --> 00:38:34,800
it's going to be mapped as first private

00:38:32,400 --> 00:38:35,920
this means that it's going to be copied

00:38:34,800 --> 00:38:38,000
to the device

00:38:35,920 --> 00:38:40,320
but not copied back and there's no way

00:38:38,000 --> 00:38:42,240
for me to get that result back at all

00:38:40,320 --> 00:38:44,480
so by mapping it it's going to map it

00:38:42,240 --> 00:38:48,400
into the into the global memory of the

00:38:44,480 --> 00:38:50,320
of the gpu device so that i'm able to um

00:38:48,400 --> 00:38:52,640
have them see the updated value back on

00:38:50,320 --> 00:38:54,720
the host whereas if it's just the first

00:38:52,640 --> 00:38:56,160
uh first private variable i'm not going

00:38:54,720 --> 00:38:59,280
to be able to

00:38:56,160 --> 00:39:02,880
to see that so i've had to map

00:38:59,280 --> 00:39:04,720
the the total to from as well

00:39:02,880 --> 00:39:06,400
so that was all we were sort of uh

00:39:04,720 --> 00:39:08,560
looking at doing in this in this first

00:39:06,400 --> 00:39:09,920
exercise we've transferred control from

00:39:08,560 --> 00:39:12,640
the host to the device

00:39:09,920 --> 00:39:13,599
and i have mapped the data from the host

00:39:12,640 --> 00:39:17,440
of the device

00:39:13,599 --> 00:39:19,119
as well and back again so at this point

00:39:17,440 --> 00:39:21,040
the code should still behave correctly

00:39:19,119 --> 00:39:22,880
i'll still have a code that

00:39:21,040 --> 00:39:24,560
computes the total value correctly

00:39:22,880 --> 00:39:26,160
because it's just in serial

00:39:24,560 --> 00:39:28,000
and it's going to still do the update

00:39:26,160 --> 00:39:30,079
and updates it back but i'm just using

00:39:28,000 --> 00:39:32,560
a single thread on the gpu which is

00:39:30,079 --> 00:39:34,720
really not going to be efficient at all

00:39:32,560 --> 00:39:36,720
and that's where we come to uh now we're

00:39:34,720 --> 00:39:39,599
going to start talking about

00:39:36,720 --> 00:39:42,000
the parallelism available on the gpu and

00:39:39,599 --> 00:39:45,440
how to expose that and take advantage

00:39:42,000 --> 00:39:46,400
of it using openmp that's going to take

00:39:45,440 --> 00:39:48,480
us up until

00:39:46,400 --> 00:39:51,680
three o'clock before we'll break for

00:39:48,480 --> 00:39:51,680
coffee at that point

00:39:52,720 --> 00:39:57,839
so back to the the slides now

00:39:58,000 --> 00:40:03,040
hopefully that's switched over looks

00:40:00,800 --> 00:40:04,560
like it has a script

00:40:03,040 --> 00:40:06,240
so i'm going to start by uh

00:40:04,560 --> 00:40:06,960
foreshadowing a little bit and just

00:40:06,240 --> 00:40:09,280
saying

00:40:06,960 --> 00:40:10,800
in general if you have a parallel loop

00:40:09,280 --> 00:40:12,400
that you'd like to run

00:40:10,800 --> 00:40:14,480
on the device you're pretty much just

00:40:12,400 --> 00:40:16,800
going to be able to get away with

00:40:14,480 --> 00:40:19,119
saying target teams distribute parallel

00:40:16,800 --> 00:40:21,040
do and possibly there'll be a cindy on

00:40:19,119 --> 00:40:23,760
the end of that as well but i'll

00:40:21,040 --> 00:40:25,280
talk to that in a little bit so we have

00:40:23,760 --> 00:40:28,000
our parallel loop a do

00:40:25,280 --> 00:40:29,200
and inside there is our loop body and

00:40:28,000 --> 00:40:30,800
above and

00:40:29,200 --> 00:40:32,480
above that loop we're going to have this

00:40:30,800 --> 00:40:34,240
big long construct target teams

00:40:32,480 --> 00:40:37,200
distribute parallel do

00:40:34,240 --> 00:40:38,480
and as before we need to end the the

00:40:37,200 --> 00:40:40,640
target region with

00:40:38,480 --> 00:40:42,640
end target teams distribute parallel do

00:40:40,640 --> 00:40:44,880
as well these are using the combined

00:40:42,640 --> 00:40:47,839
constructs in openmp

00:40:44,880 --> 00:40:49,119
um and they are there um as a shorthand

00:40:47,839 --> 00:40:50,400
we could write all of these things on

00:40:49,119 --> 00:40:51,839
individual lines but

00:40:50,400 --> 00:40:54,000
we could also write them as a single

00:40:51,839 --> 00:40:55,440
line and hopefully uh have a wave that

00:40:54,000 --> 00:40:57,280
we can

00:40:55,440 --> 00:40:58,720
and they mean all of all things together

00:40:57,280 --> 00:41:00,240
then

00:40:58,720 --> 00:41:02,240
so we're going to walk through what each

00:41:00,240 --> 00:41:03,920
of these can uh constituent parts mean

00:41:02,240 --> 00:41:05,839
now we've already seen what target means

00:41:03,920 --> 00:41:08,000
so we're just going to go through the

00:41:05,839 --> 00:41:10,400
teams distribute parallel and do and

00:41:08,000 --> 00:41:12,640
talk to cindy a little bit as well

00:41:10,400 --> 00:41:14,960
it's worth saying at this point that if

00:41:12,640 --> 00:41:17,839
you just have a large parallel loop

00:41:14,960 --> 00:41:19,599
um not using a combined statement like

00:41:17,839 --> 00:41:22,319
this with all of these parts to it

00:41:19,599 --> 00:41:23,839
can have very severe performance

00:41:22,319 --> 00:41:24,960
degradation issues that you might run

00:41:23,839 --> 00:41:27,440
into

00:41:24,960 --> 00:41:29,200
if you start breaking up this um this

00:41:27,440 --> 00:41:31,359
particular construct you might find the

00:41:29,200 --> 00:41:33,200
compiler has to take different paths

00:41:31,359 --> 00:41:34,880
and parallelize the code in the way

00:41:33,200 --> 00:41:37,520
you've asked with this

00:41:34,880 --> 00:41:38,480
with the separate parts of the of the

00:41:37,520 --> 00:41:40,400
construct

00:41:38,480 --> 00:41:41,599
so if you just have a big parallel loop

00:41:40,400 --> 00:41:43,520
that you'll want to

00:41:41,599 --> 00:41:45,520
run on the target this big combined

00:41:43,520 --> 00:41:48,640
construct is is typically what you'll

00:41:45,520 --> 00:41:51,839
what you'll do is often convenient

00:41:48,640 --> 00:41:53,359
to have an omp target and then omp

00:41:51,839 --> 00:41:55,040
teams distribute parallel do on a

00:41:53,359 --> 00:41:58,000
separate line so we can

00:41:55,040 --> 00:41:58,880
have our code looking um looking clean

00:41:58,000 --> 00:42:01,119
where we can have

00:41:58,880 --> 00:42:02,480
the map clauses on the target and then

00:42:01,119 --> 00:42:03,280
the parallelism expressed on the

00:42:02,480 --> 00:42:04,560
following line

00:42:03,280 --> 00:42:06,319
and that's fine you're not going to run

00:42:04,560 --> 00:42:07,119
into performance issues with that

00:42:06,319 --> 00:42:08,560
approach

00:42:07,119 --> 00:42:10,160
it's just if you start breaking up the

00:42:08,560 --> 00:42:11,200
teams and the parallel doom and that

00:42:10,160 --> 00:42:14,160
sort of thing we can

00:42:11,200 --> 00:42:17,760
we have observed some performance issues

00:42:14,160 --> 00:42:19,359
in the past

00:42:17,760 --> 00:42:21,760
so let's talk about teams that's the

00:42:19,359 --> 00:42:22,319
first part of our of our big directive

00:42:21,760 --> 00:42:23,920
so let's

00:42:22,319 --> 00:42:25,599
let's look at target teams with some

00:42:23,920 --> 00:42:27,760
code in the middle so

00:42:25,599 --> 00:42:29,040
we we talked to this a little bit this

00:42:27,760 --> 00:42:31,680
morning

00:42:29,040 --> 00:42:32,079
when you make a lot of a lot of threads

00:42:31,680 --> 00:42:35,440
on

00:42:32,079 --> 00:42:37,280
on the um on the device on on in openmp

00:42:35,440 --> 00:42:38,560
on the cpu and on the gpu

00:42:37,280 --> 00:42:41,119
all of those threads are grouped

00:42:38,560 --> 00:42:43,200
together in a team you have a team of

00:42:41,119 --> 00:42:44,480
threads and this is true on the target

00:42:43,200 --> 00:42:47,760
device as well

00:42:44,480 --> 00:42:51,280
the openmp threads are grouped into

00:42:47,760 --> 00:42:52,640
into one or more teams this is really

00:42:51,280 --> 00:42:54,720
important because of the

00:42:52,640 --> 00:42:57,200
uh the synchronization constraints that

00:42:54,720 --> 00:43:00,720
that teams impose

00:42:57,200 --> 00:43:03,359
so in openmp you can synchronize between

00:43:00,720 --> 00:43:05,280
threads that are within the same team

00:43:03,359 --> 00:43:07,920
but you cannot synchronize

00:43:05,280 --> 00:43:09,200
between threads in different teams this

00:43:07,920 --> 00:43:11,520
is very this is true of

00:43:09,200 --> 00:43:13,119
other uh parallel program models as well

00:43:11,520 --> 00:43:15,200
things like opencl

00:43:13,119 --> 00:43:16,160
you can synchronize work items within a

00:43:15,200 --> 00:43:18,560
work group but

00:43:16,160 --> 00:43:20,400
not between work items in different work

00:43:18,560 --> 00:43:23,520
groups and it's a similar approach

00:43:20,400 --> 00:43:26,240
and a similar kind of model here so

00:43:23,520 --> 00:43:28,240
threads within a team we can use say an

00:43:26,240 --> 00:43:29,839
openmp barrier or

00:43:28,240 --> 00:43:31,359
the implicit barriers at the end of do

00:43:29,839 --> 00:43:33,440
loops those sorts of things

00:43:31,359 --> 00:43:35,280
we're allowed to synchronize between the

00:43:33,440 --> 00:43:37,440
threads in within a team but we cannot

00:43:35,280 --> 00:43:39,440
synchronize between teams

00:43:37,440 --> 00:43:41,520
in fact we don't even know what order

00:43:39,440 --> 00:43:43,599
those teams may execute in it and this

00:43:41,520 --> 00:43:45,599
is why these sort of forward progress

00:43:43,599 --> 00:43:48,240
guarantees are quite weak

00:43:45,599 --> 00:43:49,680
between teams so that we this in this

00:43:48,240 --> 00:43:51,599
means that we don't have the

00:43:49,680 --> 00:43:53,520
facility to synchronize between

00:43:51,599 --> 00:43:56,079
different teams

00:43:53,520 --> 00:43:57,680
however when we end target so we must

00:43:56,079 --> 00:43:59,599
exit the target region

00:43:57,680 --> 00:44:01,760
that is when we do synchronize between

00:43:59,599 --> 00:44:02,960
teams because the whole device performs

00:44:01,760 --> 00:44:06,560
that whole device

00:44:02,960 --> 00:44:08,560
synchronization when uh execution

00:44:06,560 --> 00:44:10,720
returns from the target device back to

00:44:08,560 --> 00:44:12,160
the host so at that point we know all

00:44:10,720 --> 00:44:14,160
our teams are finished executing but

00:44:12,160 --> 00:44:16,400
before that there's no way to get this

00:44:14,160 --> 00:44:18,880
synchronization

00:44:16,400 --> 00:44:20,560
so we have our threads in there grouped

00:44:18,880 --> 00:44:23,599
into into teams

00:44:20,560 --> 00:44:25,040
and groups of teams are called is called

00:44:23,599 --> 00:44:29,119
a league we have a league of

00:44:25,040 --> 00:44:32,319
teams and there's a team of threads

00:44:29,119 --> 00:44:35,599
so our target construct omp target this

00:44:32,319 --> 00:44:38,960
offloads serial execution to the device

00:44:35,599 --> 00:44:41,280
and with the teams the teams

00:44:38,960 --> 00:44:43,119
directive on on the target directive

00:44:41,280 --> 00:44:46,240
this creates a league of

00:44:43,119 --> 00:44:49,200
teams so when we have target teams we're

00:44:46,240 --> 00:44:51,599
going to create a league of teams

00:44:49,200 --> 00:44:52,800
and then we haven't said how many

00:44:51,599 --> 00:44:54,400
threads are in each team but actually

00:44:52,800 --> 00:44:56,400
there's just going to be one that the

00:44:54,400 --> 00:44:56,880
main threads are the master thread this

00:44:56,400 --> 00:44:59,760
is an

00:44:56,880 --> 00:45:01,760
openmp 4.5 terminology there's going to

00:44:59,760 --> 00:45:02,480
be a master thread in each of those

00:45:01,760 --> 00:45:04,560
teams

00:45:02,480 --> 00:45:07,680
and it's going to redundantly execute

00:45:04,560 --> 00:45:08,960
the code between these two statements

00:45:07,680 --> 00:45:11,119
so this is the first part of our

00:45:08,960 --> 00:45:12,800
parallelism we have target teams which

00:45:11,119 --> 00:45:16,079
transferred to the gpu

00:45:12,800 --> 00:45:18,240
we launched a league of teams and each

00:45:16,079 --> 00:45:20,800
one thread in each of those teams

00:45:18,240 --> 00:45:23,520
is going to be executing the same code

00:45:20,800 --> 00:45:23,520
in this block

00:45:24,000 --> 00:45:27,680
we've seen a similar idea if you just

00:45:25,680 --> 00:45:30,240
have a parallel statement

00:45:27,680 --> 00:45:30,720
where without any of those work sharing

00:45:30,240 --> 00:45:33,200
things like

00:45:30,720 --> 00:45:35,200
parallel do where each thread just

00:45:33,200 --> 00:45:35,520
executes exactly the same code so here

00:45:35,200 --> 00:45:37,920
it's

00:45:35,520 --> 00:45:38,720
one thread in each team executes this

00:45:37,920 --> 00:45:42,240
target region

00:45:38,720 --> 00:45:44,960
on the device so how might this look in

00:45:42,240 --> 00:45:46,960
this sort of cartoon picture

00:45:44,960 --> 00:45:48,960
so target teams is going to create a

00:45:46,960 --> 00:45:51,440
number of teams on the gpu

00:45:48,960 --> 00:45:52,000
these are these big um rectangles on the

00:45:51,440 --> 00:45:54,880
slide

00:45:52,000 --> 00:45:56,720
say we make four teams i've got a league

00:45:54,880 --> 00:45:58,480
containing four teams

00:45:56,720 --> 00:46:00,640
and each of those teams is just going to

00:45:58,480 --> 00:46:02,880
have one thread in this main or master

00:46:00,640 --> 00:46:04,720
thread

00:46:02,880 --> 00:46:06,319
so each of those threads is going to

00:46:04,720 --> 00:46:08,640
execute the code block that's been

00:46:06,319 --> 00:46:10,079
defined by the target and the end target

00:46:08,640 --> 00:46:11,440
teams

00:46:10,079 --> 00:46:13,440
and all those are going to do exactly

00:46:11,440 --> 00:46:14,079
the same code because we haven't haven't

00:46:13,440 --> 00:46:15,839
necessarily

00:46:14,079 --> 00:46:17,440
specialized it so this is how our

00:46:15,839 --> 00:46:19,440
parallelism looks so far

00:46:17,440 --> 00:46:21,440
we've we've gone beyond serial now

00:46:19,440 --> 00:46:24,880
because we have a league of teams

00:46:21,440 --> 00:46:24,880
that's executing some code

00:46:25,119 --> 00:46:30,400
so what if we have a loop we want to

00:46:28,000 --> 00:46:33,200
work share that loop across the team

00:46:30,400 --> 00:46:34,880
so that each team gets its own part of

00:46:33,200 --> 00:46:37,359
the iteration space

00:46:34,880 --> 00:46:38,800
this is a similar idea to a parallel do

00:46:37,359 --> 00:46:42,400
where we want to

00:46:38,800 --> 00:46:42,960
spread the work between the threads of a

00:46:42,400 --> 00:46:45,359
team

00:46:42,960 --> 00:46:46,800
now we want to share the work between

00:46:45,359 --> 00:46:49,599
teams

00:46:46,800 --> 00:46:50,400
so to do this we say target teams

00:46:49,599 --> 00:46:53,040
distribute

00:46:50,400 --> 00:46:54,160
and we have to put this before a do loop

00:46:53,040 --> 00:46:55,920
so that

00:46:54,160 --> 00:46:58,160
we have a loop in which to partition

00:46:55,920 --> 00:47:01,760
that iteration space

00:46:58,160 --> 00:47:03,680
it's going to use a static schedule um

00:47:01,760 --> 00:47:04,880
which is basically breaking that

00:47:03,680 --> 00:47:07,839
iteration space up

00:47:04,880 --> 00:47:08,240
into n divided by the number of teams

00:47:07,839 --> 00:47:10,480
and

00:47:08,240 --> 00:47:12,240
each each team is getting a chunk of

00:47:10,480 --> 00:47:14,240
that work

00:47:12,240 --> 00:47:15,520
so this is now being able to distribute

00:47:14,240 --> 00:47:17,839
a loop and

00:47:15,520 --> 00:47:19,599
splitting the the work to do within this

00:47:17,839 --> 00:47:23,119
target region between

00:47:19,599 --> 00:47:24,960
these parallel teams we still only have

00:47:23,119 --> 00:47:26,319
one thread in each of our teams this

00:47:24,960 --> 00:47:26,960
master thread and that's going to

00:47:26,319 --> 00:47:30,079
execute

00:47:26,960 --> 00:47:32,640
each of those chunks of the loop that is

00:47:30,079 --> 00:47:33,520
that is being executed so back to our

00:47:32,640 --> 00:47:35,440
picture

00:47:33,520 --> 00:47:36,800
let's say we had a loop that goes from 1

00:47:35,440 --> 00:47:40,079
to 100 do

00:47:36,800 --> 00:47:42,079
i equals 1 to 100

00:47:40,079 --> 00:47:44,000
and we have our target teams distribute

00:47:42,079 --> 00:47:46,720
uh construct

00:47:44,000 --> 00:47:47,760
so again say just for the slide we have

00:47:46,720 --> 00:47:50,640
four

00:47:47,760 --> 00:47:52,079
a league of four teams so our distribute

00:47:50,640 --> 00:47:54,880
is then going to break that

00:47:52,079 --> 00:47:55,920
iteration space of 100 into four and

00:47:54,880 --> 00:47:57,440
give a chunk

00:47:55,920 --> 00:47:59,280
a contiguous chunk of each of that

00:47:57,440 --> 00:48:02,160
iteration space to

00:47:59,280 --> 00:48:04,000
each of those teams to execute so you

00:48:02,160 --> 00:48:05,599
can see that i've annotated each of the

00:48:04,000 --> 00:48:09,599
teams the rectangles with

00:48:05,599 --> 00:48:12,240
with a range i equals low to high

00:48:09,599 --> 00:48:14,160
again each each team still only contains

00:48:12,240 --> 00:48:17,200
one thread but each team is now

00:48:14,160 --> 00:48:18,960
going to be computing a different um a

00:48:17,200 --> 00:48:20,079
different part of this iteration range

00:48:18,960 --> 00:48:22,160
so we're now

00:48:20,079 --> 00:48:23,440
starting to build up the parallelism and

00:48:22,160 --> 00:48:25,040
for our do loop

00:48:23,440 --> 00:48:26,960
so we're now not doing redundant work

00:48:25,040 --> 00:48:27,440
we're sharing the work out between the

00:48:26,960 --> 00:48:31,599
different

00:48:27,440 --> 00:48:31,599
um between the different teams

00:48:32,559 --> 00:48:36,640
we now come to parallel do and luckily

00:48:34,800 --> 00:48:37,119
this has exactly the same semantics as

00:48:36,640 --> 00:48:39,440
it does

00:48:37,119 --> 00:48:41,839
on the gpu as it does with the cpu so

00:48:39,440 --> 00:48:45,760
parallel do means the same on the cpu

00:48:41,839 --> 00:48:46,640
as the gpu so parallel launches threads

00:48:45,760 --> 00:48:49,040
within the team

00:48:46,640 --> 00:48:49,920
and the do distributes iterations of

00:48:49,040 --> 00:48:52,960
that loop

00:48:49,920 --> 00:48:55,680
across the threads in a team

00:48:52,960 --> 00:48:57,760
so in our constructs the target teams

00:48:55,680 --> 00:49:00,400
distribute parallel do

00:48:57,760 --> 00:49:01,839
we have already distributed the loop

00:49:00,400 --> 00:49:04,400
between the teams

00:49:01,839 --> 00:49:06,480
now the parallel is going to refer to

00:49:04,400 --> 00:49:07,680
each team each team is going to launch a

00:49:06,480 --> 00:49:11,119
number of threads

00:49:07,680 --> 00:49:13,280
and the do is going to share those

00:49:11,119 --> 00:49:14,480
iterations that have been distributed to

00:49:13,280 --> 00:49:16,800
each team

00:49:14,480 --> 00:49:19,040
across the threads uh that are now

00:49:16,800 --> 00:49:20,720
available in that team from the parallel

00:49:19,040 --> 00:49:22,640
so it's perhaps uh more straightforward

00:49:20,720 --> 00:49:25,920
to show this uh with this

00:49:22,640 --> 00:49:28,079
uh cartoon picture again so at the top

00:49:25,920 --> 00:49:30,160
we have the situation as before we have

00:49:28,079 --> 00:49:33,520
target teams distribute that's going to

00:49:30,160 --> 00:49:36,079
share a hundred iterations between four

00:49:33,520 --> 00:49:38,400
teams and you can see the range of each

00:49:36,079 --> 00:49:40,960
team at the top

00:49:38,400 --> 00:49:41,680
when i have parallel let's say that's

00:49:40,960 --> 00:49:45,440
going to create

00:49:41,680 --> 00:49:47,760
five threads five openmp threads per

00:49:45,440 --> 00:49:49,839
team so i now have 20 threads running in

00:49:47,760 --> 00:49:51,520
total five for every team

00:49:49,839 --> 00:49:52,880
lots and lots of threads this is this is

00:49:51,520 --> 00:49:55,520
how we get

00:49:52,880 --> 00:49:58,880
that massive parallelism um that we can

00:49:55,520 --> 00:50:00,880
then use on the gpu

00:49:58,880 --> 00:50:02,240
so that's the parallel we've spawned all

00:50:00,880 --> 00:50:05,599
these threads in each team

00:50:02,240 --> 00:50:07,599
and then the do is going to disturb this

00:50:05,599 --> 00:50:09,920
is going to share the work work share

00:50:07,599 --> 00:50:10,559
the iteration of the chunk within each

00:50:09,920 --> 00:50:13,359
team

00:50:10,559 --> 00:50:15,440
across the threads in that team so using

00:50:13,359 --> 00:50:17,359
the indexes within each rectangle here

00:50:15,440 --> 00:50:19,119
that's within the team we can see that

00:50:17,359 --> 00:50:20,079
each of the threads numbered from zero

00:50:19,119 --> 00:50:23,280
to four

00:50:20,079 --> 00:50:26,000
is going to get um five iterations

00:50:23,280 --> 00:50:27,040
of the loop um there's there's 25

00:50:26,000 --> 00:50:30,559
between each

00:50:27,040 --> 00:50:33,760
um between each of the four um

00:50:30,559 --> 00:50:35,040
teams so we can see the first thread in

00:50:33,760 --> 00:50:37,760
the first team is going to get

00:50:35,040 --> 00:50:38,960
say iterations one to five so this is

00:50:37,760 --> 00:50:41,839
breaking up the

00:50:38,960 --> 00:50:44,000
um the part the share of the iteration

00:50:41,839 --> 00:50:45,520
space that was distributed to each team

00:50:44,000 --> 00:50:47,200
by the distribute construct

00:50:45,520 --> 00:50:48,960
and it's work sharing that with the do

00:50:47,200 --> 00:50:50,000
construct between all the threads in the

00:50:48,960 --> 00:50:51,440
team

00:50:50,000 --> 00:50:53,440
so in this way we have lots and lots of

00:50:51,440 --> 00:50:54,079
threads now these are open mp threads

00:50:53,440 --> 00:50:55,920
lots of those

00:50:54,079 --> 00:50:58,240
launched on the device this is why

00:50:55,920 --> 00:51:00,240
having this hierarchical parallelism

00:50:58,240 --> 00:51:01,359
uh built into the language and and the

00:51:00,240 --> 00:51:04,640
devices too

00:51:01,359 --> 00:51:08,880
is very powerful we have expressed a lot

00:51:04,640 --> 00:51:10,960
of parallelism here

00:51:08,880 --> 00:51:12,559
so we can also use the cindy construct

00:51:10,960 --> 00:51:14,559
and this is valid on the distribute

00:51:12,559 --> 00:51:17,760
parallel do as well

00:51:14,559 --> 00:51:19,520
um in in general we tend to use target

00:51:17,760 --> 00:51:21,440
teams distribute parallel do cindy

00:51:19,520 --> 00:51:22,079
sometimes having the cindy in there as

00:51:21,440 --> 00:51:25,200
well is

00:51:22,079 --> 00:51:25,200
is is fine

00:51:25,599 --> 00:51:29,520
openmp just says that the simdi uh

00:51:28,559 --> 00:51:31,359
clause

00:51:29,520 --> 00:51:33,040
must mean that cindy instructions are

00:51:31,359 --> 00:51:34,800
generated that's what it says in the

00:51:33,040 --> 00:51:37,200
specification

00:51:34,800 --> 00:51:38,800
so there's some implementation uh

00:51:37,200 --> 00:51:39,760
details that differ between different

00:51:38,800 --> 00:51:41,839
compilers

00:51:39,760 --> 00:51:43,760
and sometimes compilers ignore different

00:51:41,839 --> 00:51:44,960
parts of the construct depending on the

00:51:43,760 --> 00:51:46,800
situation

00:51:44,960 --> 00:51:48,480
um i'll come to that in the next slide

00:51:46,800 --> 00:51:50,400
where we talk a little bit about how we

00:51:48,480 --> 00:51:52,559
might map openmp parallelism

00:51:50,400 --> 00:51:54,240
that the league of teams of threads of

00:51:52,559 --> 00:51:57,280
perhaps vector instructions

00:51:54,240 --> 00:51:59,119
onto gpu uh the gpu parallel models and

00:51:57,280 --> 00:52:01,359
the hierarchy that's actually available

00:51:59,119 --> 00:52:02,800
in the hardware but in general using

00:52:01,359 --> 00:52:04,960
this big construct

00:52:02,800 --> 00:52:07,119
targeting distribute parallel do seems

00:52:04,960 --> 00:52:09,359
to be the most portable solution

00:52:07,119 --> 00:52:11,040
adding cindy in can sometimes help and

00:52:09,359 --> 00:52:13,440
help with that portability

00:52:11,040 --> 00:52:13,440
as well

00:52:14,160 --> 00:52:17,680
so openmp defines three levels of

00:52:16,160 --> 00:52:20,079
parallelism we have

00:52:17,680 --> 00:52:20,800
teams we have parallel threads within a

00:52:20,079 --> 00:52:23,680
team

00:52:20,800 --> 00:52:24,720
and simdi and it's a cindy thread so

00:52:23,680 --> 00:52:27,040
this is a proper

00:52:24,720 --> 00:52:28,240
nested hierarchy where we can have a

00:52:27,040 --> 00:52:30,800
league

00:52:28,240 --> 00:52:32,079
of teams and each team has a number of

00:52:30,800 --> 00:52:35,119
threads and each of those

00:52:32,079 --> 00:52:37,599
threads may be executing vector cmd

00:52:35,119 --> 00:52:40,240
instructions

00:52:37,599 --> 00:52:41,119
so you can see how this maps to say like

00:52:40,240 --> 00:52:43,040
a

00:52:41,119 --> 00:52:44,640
a cpu architecture for example with

00:52:43,040 --> 00:52:47,359
hyper threads we can have

00:52:44,640 --> 00:52:48,960
a team of threads on every core within

00:52:47,359 --> 00:52:50,640
each core there's a number of hyper

00:52:48,960 --> 00:52:52,640
threads or

00:52:50,640 --> 00:52:54,480
or multi threads or something and they

00:52:52,640 --> 00:52:55,359
could be the two threads maybe or four

00:52:54,480 --> 00:52:57,040
threads between

00:52:55,359 --> 00:52:58,800
within each team that exists on each

00:52:57,040 --> 00:53:00,079
core and each of those threads is

00:52:58,800 --> 00:53:02,800
allowed to execute

00:53:00,079 --> 00:53:03,440
cindy instructions so on the cpu there's

00:53:02,800 --> 00:53:05,680
this

00:53:03,440 --> 00:53:07,680
you can you can see how there could be a

00:53:05,680 --> 00:53:08,960
three level hierarchy of parallelism

00:53:07,680 --> 00:53:12,160
available

00:53:08,960 --> 00:53:14,559
but on gpu hardware it's however you

00:53:12,160 --> 00:53:16,400
want to expose that there are a number

00:53:14,559 --> 00:53:18,559
of ways you can do that

00:53:16,400 --> 00:53:19,680
there really tends to only be two levels

00:53:18,559 --> 00:53:22,960
of parallelism that's

00:53:19,680 --> 00:53:26,079
that's generally exploited

00:53:22,960 --> 00:53:28,960
first of all there's these compute units

00:53:26,079 --> 00:53:31,200
and these are those um cores these sort

00:53:28,960 --> 00:53:32,480
of um sms that i showed at the start of

00:53:31,200 --> 00:53:34,079
this afternoon

00:53:32,480 --> 00:53:36,880
and then there's the processing elements

00:53:34,079 --> 00:53:38,960
within each compute unit

00:53:36,880 --> 00:53:39,920
and they could be the cuda cores for for

00:53:38,960 --> 00:53:42,319
instance as i

00:53:39,920 --> 00:53:43,920
as i um explained now they might have

00:53:42,319 --> 00:53:45,760
some hierarchy built into them where you

00:53:43,920 --> 00:53:47,119
might want to target them as as say

00:53:45,760 --> 00:53:48,319
cindy or whatever but

00:53:47,119 --> 00:53:50,480
essentially there's just these two

00:53:48,319 --> 00:53:52,400
levels there's cores and and then

00:53:50,480 --> 00:53:55,119
elements within each of those cause or

00:53:52,400 --> 00:53:55,119
compute units

00:53:55,280 --> 00:53:59,200
so with openmp the implementations of

00:53:57,680 --> 00:54:00,240
open openmp actually have a lot of

00:53:59,200 --> 00:54:03,119
flexibility in

00:54:00,240 --> 00:54:05,680
how they might map openmp these parallel

00:54:03,119 --> 00:54:08,000
concepts of teams threads and vectors

00:54:05,680 --> 00:54:10,000
to the underlying hardware it's not

00:54:08,000 --> 00:54:12,319
stipulated what each one of those has to

00:54:10,000 --> 00:54:14,400
mean in hardware and this gives the

00:54:12,319 --> 00:54:16,480
compilers flexibility to map the right

00:54:14,400 --> 00:54:17,920
ones to the right bits of hardware at

00:54:16,480 --> 00:54:20,000
the right time

00:54:17,920 --> 00:54:21,040
it's important to know that that

00:54:20,000 --> 00:54:23,040
sometimes

00:54:21,040 --> 00:54:24,720
the mapping may change depending on the

00:54:23,040 --> 00:54:26,960
context and i'll show some examples of

00:54:24,720 --> 00:54:28,800
that

00:54:26,960 --> 00:54:30,400
so let's let's talk about llvm based

00:54:28,800 --> 00:54:33,359
compilers and this

00:54:30,400 --> 00:54:34,079
and many compilers now use the llvm

00:54:33,359 --> 00:54:35,920
project

00:54:34,079 --> 00:54:37,119
as sort of a watering hole they they

00:54:35,920 --> 00:54:39,680
contribute and

00:54:37,119 --> 00:54:40,720
and gain uh enormous benefit in both

00:54:39,680 --> 00:54:42,400
directions

00:54:40,720 --> 00:54:43,760
from basing their compilers off of the

00:54:42,400 --> 00:54:46,000
technology in lvm

00:54:43,760 --> 00:54:47,359
and this means that any improvements and

00:54:46,000 --> 00:54:49,280
design decisions and

00:54:47,359 --> 00:54:51,359
mappings and things is often common

00:54:49,280 --> 00:54:53,760
between lots of compilers

00:54:51,359 --> 00:54:54,400
this is true of the craig compiler since

00:54:53,760 --> 00:54:57,520
um

00:54:54,400 --> 00:54:59,680
since version 9 cc 9 and above and today

00:54:57,520 --> 00:55:01,520
we've all been using cc10

00:54:59,680 --> 00:55:03,040
so these things apply for the compiler

00:55:01,520 --> 00:55:04,880
as well now obviously

00:55:03,040 --> 00:55:06,559
llvm is still developing its fortran

00:55:04,880 --> 00:55:09,359
capability and cray

00:55:06,559 --> 00:55:10,880
have helped there and and they use their

00:55:09,359 --> 00:55:13,760
own front end and some of their own

00:55:10,880 --> 00:55:15,680
openmp uh runtimes for for their

00:55:13,760 --> 00:55:19,680
compiler with fortran and openmp

00:55:15,680 --> 00:55:21,280
targeting gpus so in these llvm based

00:55:19,680 --> 00:55:24,559
compilers

00:55:21,280 --> 00:55:26,880
openmp teams are typically associated or

00:55:24,559 --> 00:55:28,960
or or mapped to sort overload the word

00:55:26,880 --> 00:55:30,319
map they're associated with compute

00:55:28,960 --> 00:55:33,359
units so we might

00:55:30,319 --> 00:55:35,440
map or associate each openmp team to one

00:55:33,359 --> 00:55:39,520
of those compute units or sms in

00:55:35,440 --> 00:55:41,920
nvidia language we then might map

00:55:39,520 --> 00:55:43,599
or associate each of the threads within

00:55:41,920 --> 00:55:45,599
a team to each of the processing

00:55:43,599 --> 00:55:48,079
elements within that compute unit

00:55:45,599 --> 00:55:50,160
and with llvm typically this cindy

00:55:48,079 --> 00:55:52,799
construct is ignored

00:55:50,160 --> 00:55:54,960
um the cray classic compiler which is

00:55:52,799 --> 00:55:58,799
what the fortran is based on

00:55:54,960 --> 00:56:00,079
tends to uh map the cindy instead to the

00:55:58,799 --> 00:56:03,920
processing elements that's

00:56:00,079 --> 00:56:05,359
instead of the um

00:56:03,920 --> 00:56:06,960
the threads the opening be threads to

00:56:05,359 --> 00:56:08,640
processing elements but

00:56:06,960 --> 00:56:10,000
this is changing over time and as their

00:56:08,640 --> 00:56:12,400
compiler is going to be moving

00:56:10,000 --> 00:56:14,480
more and more llvm based we'll find that

00:56:12,400 --> 00:56:15,839
we'll be reverting to the the teams and

00:56:14,480 --> 00:56:19,119
threads approach

00:56:15,839 --> 00:56:21,359
with that so let's consider this

00:56:19,119 --> 00:56:23,200
when we look at the team's distribute

00:56:21,359 --> 00:56:26,319
construct

00:56:23,200 --> 00:56:28,799
so what we find this is saying that the

00:56:26,319 --> 00:56:29,359
loop iteration should be distributed

00:56:28,799 --> 00:56:32,079
between

00:56:29,359 --> 00:56:34,400
teams we're not specifying um anything

00:56:32,079 --> 00:56:35,680
else we're not saying how many threads

00:56:34,400 --> 00:56:37,280
we're not specific we're not specified

00:56:35,680 --> 00:56:38,720
to launch any uh threads within each

00:56:37,280 --> 00:56:40,480
team so we just have a single thread

00:56:38,720 --> 00:56:42,720
within each team

00:56:40,480 --> 00:56:44,799
now we can't synchronize between teams

00:56:42,720 --> 00:56:46,160
that's defined by the semantics of

00:56:44,799 --> 00:56:48,079
openmp

00:56:46,160 --> 00:56:49,280
so when we write teams distribute what

00:56:48,079 --> 00:56:51,760
we're saying is that all

00:56:49,280 --> 00:56:53,520
iterations are independent we are not

00:56:51,760 --> 00:56:55,280
able to synchronize so

00:56:53,520 --> 00:56:57,200
that's we can't assume that we might be

00:56:55,280 --> 00:56:58,000
able to so we're telling openmp this

00:56:57,200 --> 00:57:00,079
loop

00:56:58,000 --> 00:57:01,280
has no dependencies between any of the

00:57:00,079 --> 00:57:03,839
iterations of the loop

00:57:01,280 --> 00:57:05,839
everything's independent so this means

00:57:03,839 --> 00:57:06,240
the implement implementations can and

00:57:05,839 --> 00:57:08,079
will

00:57:06,240 --> 00:57:09,599
change that mapping and they can map

00:57:08,079 --> 00:57:11,440
this to

00:57:09,599 --> 00:57:13,599
exploit all the parallelism across the

00:57:11,440 --> 00:57:14,960
whole gpu

00:57:13,599 --> 00:57:16,640
so the mapping i showed on the previous

00:57:14,960 --> 00:57:19,040
slide is going to be slightly different

00:57:16,640 --> 00:57:20,559
for teams distribute alone

00:57:19,040 --> 00:57:23,040
because everything's independent the

00:57:20,559 --> 00:57:24,839
compiler will use that information

00:57:23,040 --> 00:57:26,960
and then share the work across the whole

00:57:24,839 --> 00:57:30,079
gpu so rather than

00:57:26,960 --> 00:57:31,680
um so we'll find the openmp teams

00:57:30,079 --> 00:57:33,440
are now being mapped to processing

00:57:31,680 --> 00:57:36,160
elements instead of compute units

00:57:33,440 --> 00:57:37,920
and it doesn't really matter how those

00:57:36,160 --> 00:57:39,760
work items or threads or

00:57:37,920 --> 00:57:41,280
or cuda threads are going to be grouped

00:57:39,760 --> 00:57:43,920
into the um

00:57:41,280 --> 00:57:44,480
into the thread blocks or or work groups

00:57:43,920 --> 00:57:45,839
or

00:57:44,480 --> 00:57:47,359
how they're arranged on the compute

00:57:45,839 --> 00:57:48,880
units it doesn't matter that's

00:57:47,359 --> 00:57:50,319
implementation defined

00:57:48,880 --> 00:57:51,839
because there's no synchronization

00:57:50,319 --> 00:57:53,520
between any of the other between the

00:57:51,839 --> 00:57:55,119
threads at all

00:57:53,520 --> 00:57:57,200
between those teams at all because it's

00:57:55,119 --> 00:57:59,599
not allowed um

00:57:57,200 --> 00:58:00,480
we get it we get this uh large uh

00:57:59,599 --> 00:58:02,400
parallelism

00:58:00,480 --> 00:58:03,839
so this behaves a lot like sort of a

00:58:02,400 --> 00:58:06,559
cindy uh auto

00:58:03,839 --> 00:58:08,160
auto vectorization approach where we map

00:58:06,559 --> 00:58:09,040
we're saying all of these iterations are

00:58:08,160 --> 00:58:10,799
independent

00:58:09,040 --> 00:58:12,799
and it just gets mapped to as much

00:58:10,799 --> 00:58:15,440
parallelism as is available

00:58:12,799 --> 00:58:15,440
on the device

00:58:16,319 --> 00:58:21,839
so there's more than one possible

00:58:19,359 --> 00:58:23,839
mapping or association between the world

00:58:21,839 --> 00:58:26,880
of the openmp parallelism

00:58:23,839 --> 00:58:28,079
and the world of the hardware and how

00:58:26,880 --> 00:58:29,359
those things may be mapped and

00:58:28,079 --> 00:58:30,880
associated with each other

00:58:29,359 --> 00:58:33,280
so sometimes you need to find out what

00:58:30,880 --> 00:58:34,960
is happening the first place to look is

00:58:33,280 --> 00:58:36,640
in the compiler documentation maybe

00:58:34,960 --> 00:58:38,319
they've explained what they do and cray

00:58:36,640 --> 00:58:42,000
has done this if you type man

00:58:38,319 --> 00:58:43,920
intro underscore openmp they have a very

00:58:42,000 --> 00:58:44,480
short man page that explains how their

00:58:43,920 --> 00:58:46,799
target

00:58:44,480 --> 00:58:47,760
is is what is going to be mapped and

00:58:46,799 --> 00:58:49,839
associated

00:58:47,760 --> 00:58:51,839
for the different constructs and the

00:58:49,839 --> 00:58:55,040
rules that they apply

00:58:51,839 --> 00:58:56,400
in the c and the fortran compiler

00:58:55,040 --> 00:58:57,920
you can also have a look at the compiler

00:58:56,400 --> 00:58:58,799
output and it will tell you how it's

00:58:57,920 --> 00:59:00,559
mapped

00:58:58,799 --> 00:59:02,240
the parallelism that you've exposed in

00:59:00,559 --> 00:59:05,359
your openmp program

00:59:02,240 --> 00:59:08,079
to the underlying hardware and

00:59:05,359 --> 00:59:10,079
in cce10 there's the minus rm flag and

00:59:08,079 --> 00:59:13,440
i've put that in your make files um

00:59:10,079 --> 00:59:14,319
already this will give you a lst file a

00:59:13,440 --> 00:59:15,680
listing file

00:59:14,319 --> 00:59:18,079
which you can then go through and it

00:59:15,680 --> 00:59:21,200
annotates the source code

00:59:18,079 --> 00:59:23,359
with a big key and and kind of ascii

00:59:21,200 --> 00:59:25,280
ascii information that explains exactly

00:59:23,359 --> 00:59:26,160
what optimizations that compiler has

00:59:25,280 --> 00:59:29,119
been doing

00:59:26,160 --> 00:59:31,680
including offloading to the gpu and how

00:59:29,119 --> 00:59:34,960
it's associating

00:59:31,680 --> 00:59:37,119
all of those parts that they've exposed

00:59:34,960 --> 00:59:40,240
the openmp teams and threads and simply

00:59:37,119 --> 00:59:42,319
with the underlying hardware

00:59:40,240 --> 00:59:44,079
finally you can also profile your code

00:59:42,319 --> 00:59:46,240
and see what it's doing and how it's

00:59:44,079 --> 00:59:46,960
being mapped under the hood so because

00:59:46,240 --> 00:59:50,960
we're using

00:59:46,960 --> 00:59:52,559
um some nvidia gpus we can use nvprof

00:59:50,960 --> 00:59:54,160
and this will profile our code and give

00:59:52,559 --> 00:59:55,440
us some high level information about

00:59:54,160 --> 00:59:57,119
each of the

00:59:55,440 --> 00:59:59,280
each of the parts how much time is in

00:59:57,119 --> 01:00:00,799
memory memory movement how much time is

00:59:59,280 --> 01:00:03,200
in each of the

01:00:00,799 --> 01:00:05,280
target regions but if we have the print

01:00:03,200 --> 01:00:06,400
gpu trace flag this will show us a

01:00:05,280 --> 01:00:08,160
timeline view in

01:00:06,400 --> 01:00:10,799
in text of all the operations that

01:00:08,160 --> 01:00:12,319
happen and included in that is the

01:00:10,799 --> 01:00:13,920
number of threads per block and the

01:00:12,319 --> 01:00:15,520
number of blocks these are these are

01:00:13,920 --> 01:00:17,359
nvidia terminologies for

01:00:15,520 --> 01:00:18,799
how that parallelism has been arranged

01:00:17,359 --> 01:00:22,000
in in the underlying

01:00:18,799 --> 01:00:23,680
cuda-like implementation so if you can

01:00:22,000 --> 01:00:25,119
combine those information of how many

01:00:23,680 --> 01:00:26,480
threads are running in each block and

01:00:25,119 --> 01:00:28,319
how many blocks you have with the

01:00:26,480 --> 01:00:30,559
knowledge of what pragma you had and

01:00:28,319 --> 01:00:31,359
how big your loop was you can start to

01:00:30,559 --> 01:00:33,760
piece together

01:00:31,359 --> 01:00:35,359
what association you got between openmp

01:00:33,760 --> 01:00:37,280
parallelism and

01:00:35,359 --> 01:00:38,480
the underlying hardware now

01:00:37,280 --> 01:00:40,000
understanding this is slightly

01:00:38,480 --> 01:00:41,359
complicated but it's but it can be

01:00:40,000 --> 01:00:42,960
important to try and understand what the

01:00:41,359 --> 01:00:45,119
compiler is doing

01:00:42,960 --> 01:00:46,880
but as i said before ultimately just

01:00:45,119 --> 01:00:48,960
writing the it's finding all the

01:00:46,880 --> 01:00:50,640
parallelism and exposing it at a single

01:00:48,960 --> 01:00:51,119
level with the with a large parallel

01:00:50,640 --> 01:00:52,400
loop

01:00:51,119 --> 01:00:54,640
generally it's going to get you very

01:00:52,400 --> 01:00:56,400
good performance on on a gpu because

01:00:54,640 --> 01:00:57,920
you just have a single pragma and the

01:00:56,400 --> 01:00:59,440
compiler is free to choose the

01:00:57,920 --> 01:01:02,480
association

01:00:59,440 --> 01:01:04,240
between the elements um of the of the

01:01:02,480 --> 01:01:07,440
openmp hierarchy of parallelism

01:01:04,240 --> 01:01:07,440
and the underlying gpu

01:01:07,520 --> 01:01:10,799
so with that we've come to the the

01:01:08,799 --> 01:01:12,559
coffee break i'm gonna stick around and

01:01:10,799 --> 01:01:15,200
take some some questions after

01:01:12,559 --> 01:01:16,400
uh on the q a on zoom so if you have any

01:01:15,200 --> 01:01:18,079
questions i've seen there's been a

01:01:16,400 --> 01:01:20,319
couple uh coming in

01:01:18,079 --> 01:01:21,920
um i'm gonna sit and answer some of

01:01:20,319 --> 01:01:23,599
those questions and grab a coffee

01:01:21,920 --> 01:01:25,040
and do take some time away from the

01:01:23,599 --> 01:01:26,559
screen there's a lot of information that

01:01:25,040 --> 01:01:28,960
we're going to go through

01:01:26,559 --> 01:01:30,400
um and it's good to uh step away from

01:01:28,960 --> 01:01:33,680
the screen so we're going to

01:01:30,400 --> 01:01:34,559
start back at a quarter past three in 15

01:01:33,680 --> 01:01:36,319
minutes

01:01:34,559 --> 01:01:38,079
and with that i'm going to carry on with

01:01:36,319 --> 01:01:40,160
with the lecturing to talk about

01:01:38,079 --> 01:01:41,359
optimizing the data movement so we've

01:01:40,160 --> 01:01:43,200
pretty much finished the

01:01:41,359 --> 01:01:44,960
discussion of parallelism i'm now going

01:01:43,200 --> 01:01:45,680
to show you how to optimize the data

01:01:44,960 --> 01:01:47,440
movement

01:01:45,680 --> 01:01:49,359
and that will then equip us with all the

01:01:47,440 --> 01:01:51,760
tools that we need to

01:01:49,359 --> 01:01:53,760
finish the exercise off to build on the

01:01:51,760 --> 01:01:55,760
stencil code that we have before which

01:01:53,760 --> 01:01:57,280
is now running on the gpu but we now

01:01:55,760 --> 01:01:58,640
need to make it run in parallel and

01:01:57,280 --> 01:02:01,599
optimize the memory movement

01:01:58,640 --> 01:02:02,720
associated with that so i'll see you in

01:02:01,599 --> 01:02:07,839
about 15 minutes

01:02:02,720 --> 01:02:07,839
at quarter past 3 uk time

01:02:08,799 --> 01:02:13,520
okay welcome back from the uh coffee

01:02:11,520 --> 01:02:15,920
break i hope you've all managed to

01:02:13,520 --> 01:02:17,839
uh get a breath of fresh air and a hot

01:02:15,920 --> 01:02:20,720
drink and and all of that

01:02:17,839 --> 01:02:21,920
we're now into the into the last half of

01:02:20,720 --> 01:02:24,960
of the tutorial

01:02:21,920 --> 01:02:25,839
uh this afternoon um we're going to

01:02:24,960 --> 01:02:28,640
start by

01:02:25,839 --> 01:02:30,720
um continuing by explaining the last

01:02:28,640 --> 01:02:32,000
bits of openmp i'd like to go through

01:02:30,720 --> 01:02:33,920
today

01:02:32,000 --> 01:02:36,319
and this is how we're going to optimize

01:02:33,920 --> 01:02:38,559
the uh the data movement between the

01:02:36,319 --> 01:02:40,240
the host and the devices as i motivated

01:02:38,559 --> 01:02:41,119
earlier this is a really important part

01:02:40,240 --> 01:02:45,119
of the

01:02:41,119 --> 01:02:47,680
um of programming of things like gpus

01:02:45,119 --> 01:02:48,880
so we've learned uh in the first part of

01:02:47,680 --> 01:02:51,760
this afternoon

01:02:48,880 --> 01:02:53,280
about the transferring execution and the

01:02:51,760 --> 01:02:55,520
mapping of data and some of those

01:02:53,280 --> 01:02:56,960
implicit rules and and the explicit ways

01:02:55,520 --> 01:03:00,240
and directions that you need to map

01:02:56,960 --> 01:03:02,799
and how you map arrays and things

01:03:00,240 --> 01:03:04,720
and then um we've looked at the

01:03:02,799 --> 01:03:06,799
parallelism uh just before the coffee

01:03:04,720 --> 01:03:08,720
break so how to express this massive

01:03:06,799 --> 01:03:10,559
parallelism with with teams and threads

01:03:08,720 --> 01:03:12,880
and cindy on gpus

01:03:10,559 --> 01:03:13,920
so this final piece of the puzzle is is

01:03:12,880 --> 01:03:16,960
moving data

01:03:13,920 --> 01:03:19,920
um when you wish and not on every single

01:03:16,960 --> 01:03:19,920
target directive

01:03:20,160 --> 01:03:23,599
now the motivation for this is is this

01:03:22,880 --> 01:03:26,799
is a kind of

01:03:23,599 --> 01:03:29,760
sketch of of lots of scientific codes

01:03:26,799 --> 01:03:30,079
we're going to initialize some arrays

01:03:29,760 --> 01:03:31,920
with

01:03:30,079 --> 01:03:33,680
however we need to do that then we're

01:03:31,920 --> 01:03:35,839
probably going to have some sort of time

01:03:33,680 --> 01:03:38,880
step function maybe that loops over

01:03:35,839 --> 01:03:41,200
a bunch of update routines um

01:03:38,880 --> 01:03:42,480
lots of times maybe until convergence or

01:03:41,200 --> 01:03:44,880
until we've

01:03:42,480 --> 01:03:46,559
simulated enough time steps or things

01:03:44,880 --> 01:03:48,160
like that

01:03:46,559 --> 01:03:49,680
and within each time step we're going to

01:03:48,160 --> 01:03:51,920
update the

01:03:49,680 --> 01:03:53,359
any arrays and any data that we that we

01:03:51,920 --> 01:03:55,200
have

01:03:53,359 --> 01:03:57,359
the point is that the scientific codes

01:03:55,200 --> 01:03:59,280
tend to initialize data and run lots of

01:03:57,359 --> 01:04:02,640
kernels lots and lots of time so

01:03:59,280 --> 01:04:04,839
lots of target regions and in general we

01:04:02,640 --> 01:04:07,520
don't really worry about the

01:04:04,839 --> 01:04:09,200
initialization cost because relative to

01:04:07,520 --> 01:04:13,039
the amount of time that we spend

01:04:09,200 --> 01:04:16,400
actually executing our code is a small

01:04:13,039 --> 01:04:18,880
percentage of of runtime

01:04:16,400 --> 01:04:20,559
um so the way we currently have written

01:04:18,880 --> 01:04:22,960
our code is if we're in mpi

01:04:20,559 --> 01:04:24,319
what we'd essentially do is is we'd run

01:04:22,960 --> 01:04:25,839
part of our code with

01:04:24,319 --> 01:04:27,920
with something and then we do a big

01:04:25,839 --> 01:04:29,520
gather and scatter to reinitialize the

01:04:27,920 --> 01:04:31,680
data every iteration

01:04:29,520 --> 01:04:32,720
now obviously we know this is this is

01:04:31,680 --> 01:04:34,720
really uh

01:04:32,720 --> 01:04:36,400
not very optimal and the same is going

01:04:34,720 --> 01:04:37,760
to apply between the host and the device

01:04:36,400 --> 01:04:39,520
memory spaces

01:04:37,760 --> 01:04:41,440
as we've implemented so far we're

01:04:39,520 --> 01:04:43,200
copying data between the host and the

01:04:41,440 --> 01:04:45,520
device for every iteration

01:04:43,200 --> 01:04:48,240
inside the inside the loop inside our

01:04:45,520 --> 01:04:51,520
inside our small five-point kernel

01:04:48,240 --> 01:04:51,839
and we're just um every time we get to

01:04:51,520 --> 01:04:54,559
the

01:04:51,839 --> 01:04:56,400
target directive inside just above the

01:04:54,559 --> 01:04:58,160
nested do loops we're going to be moving

01:04:56,400 --> 01:04:59,839
data between the host and the device

01:04:58,160 --> 01:05:01,599
and back again at the end and that's

01:04:59,839 --> 01:05:02,240
pretty wasteful we just need to leave it

01:05:01,599 --> 01:05:04,160
there

01:05:02,240 --> 01:05:05,280
throughout the entire time step loop and

01:05:04,160 --> 01:05:08,640
copy it back once

01:05:05,280 --> 01:05:10,960
at the end

01:05:08,640 --> 01:05:12,720
um so as i said before there's some very

01:05:10,960 --> 01:05:15,599
specific places that uh

01:05:12,720 --> 01:05:17,039
data transfer will happen in an openmp

01:05:15,599 --> 01:05:18,079
program that uses these target

01:05:17,039 --> 01:05:19,920
directives

01:05:18,079 --> 01:05:23,119
and one of those places is at the start

01:05:19,920 --> 01:05:25,599
and end of every target region

01:05:23,119 --> 01:05:27,760
so every time you have a an omp target

01:05:25,599 --> 01:05:30,319
that's when data is allowed to

01:05:27,760 --> 01:05:31,280
allow to move be it implicitly by the

01:05:30,319 --> 01:05:35,200
rules

01:05:31,280 --> 01:05:35,200
or explicitly with the map clauses

01:05:35,280 --> 01:05:38,480
so what we actually would like to do

01:05:36,960 --> 01:05:41,359
instead is sort of

01:05:38,480 --> 01:05:41,680
set up the the device data environment

01:05:41,359 --> 01:05:44,079
set

01:05:41,680 --> 01:05:45,119
up some arrays and some data that exists

01:05:44,079 --> 01:05:46,880
on the device

01:05:45,119 --> 01:05:48,720
run through lots and lots of target

01:05:46,880 --> 01:05:51,760
regions and then copy back

01:05:48,720 --> 01:05:53,839
any data we need once we're done the

01:05:51,760 --> 01:05:56,240
idea is that we do not want to copy

01:05:53,839 --> 01:05:57,280
all the data every iteration that is we

01:05:56,240 --> 01:06:00,319
don't want to

01:05:57,280 --> 01:06:02,079
move data every time we encounter a

01:06:00,319 --> 01:06:03,920
target region because this is just very

01:06:02,079 --> 01:06:04,880
expensive we want to leave the data

01:06:03,920 --> 01:06:09,920
resident

01:06:04,880 --> 01:06:09,920
on the gpu throughout the execution

01:06:10,640 --> 01:06:14,400
so this is where these these pair of

01:06:13,280 --> 01:06:16,160
constructs

01:06:14,400 --> 01:06:17,680
come into play these are the target

01:06:16,160 --> 01:06:20,960
enter data and target

01:06:17,680 --> 01:06:23,520
exit data constructs so

01:06:20,960 --> 01:06:26,079
the way to think about this is that the

01:06:23,520 --> 01:06:26,720
the device memory the target device has

01:06:26,079 --> 01:06:29,839
a sort of

01:06:26,720 --> 01:06:32,880
device data environment that it exists

01:06:29,839 --> 01:06:34,000
and that we can manipulate that with

01:06:32,880 --> 01:06:36,079
these two

01:06:34,000 --> 01:06:37,440
uh constructs that enter data and exit

01:06:36,079 --> 01:06:40,720
data

01:06:37,440 --> 01:06:43,440
now also when we copy data around

01:06:40,720 --> 01:06:45,280
uh and on target directives that's going

01:06:43,440 --> 01:06:47,920
to copy data into this

01:06:45,280 --> 01:06:49,599
target the device data environment and

01:06:47,920 --> 01:06:51,200
then moving out again at the end of the

01:06:49,599 --> 01:06:53,119
target region

01:06:51,200 --> 01:06:54,559
but between those target regions that

01:06:53,119 --> 01:06:55,119
device data environment is going to

01:06:54,559 --> 01:06:57,599
exist

01:06:55,119 --> 01:06:58,640
if we've used these enter data and exit

01:06:57,599 --> 01:07:02,000
data constructs

01:06:58,640 --> 01:07:04,640
to set it up so that that's what happens

01:07:02,000 --> 01:07:05,599
so these are standalone constructs they

01:07:04,640 --> 01:07:08,160
don't have any

01:07:05,599 --> 01:07:10,079
um fortran body associated with them

01:07:08,160 --> 01:07:13,599
they're just a single line of

01:07:10,079 --> 01:07:14,559
openmp compiler directive so let's take

01:07:13,599 --> 01:07:16,960
this example

01:07:14,559 --> 01:07:17,920
at the bottom of the slide it's the

01:07:16,960 --> 01:07:19,920
first one

01:07:17,920 --> 01:07:21,280
uh let's start let's start in the middle

01:07:19,920 --> 01:07:23,119
and work outwards

01:07:21,280 --> 01:07:24,400
so we have our time step loop this do

01:07:23,119 --> 01:07:27,119
loop over t

01:07:24,400 --> 01:07:27,760
and it's gonna uh in inside that loop

01:07:27,119 --> 01:07:30,960
we're going to be

01:07:27,760 --> 01:07:32,960
having lots of target um target

01:07:30,960 --> 01:07:34,480
regions so for this one we just have a

01:07:32,960 --> 01:07:36,799
single target region that's going to be

01:07:34,480 --> 01:07:38,799
executed once for every iteration

01:07:36,799 --> 01:07:40,480
so we can have n target regions that are

01:07:38,799 --> 01:07:42,720
going to be um

01:07:40,480 --> 01:07:46,160
launched now we want to keep those

01:07:42,720 --> 01:07:49,440
arrays a b and c maybe a vector add

01:07:46,160 --> 01:07:51,599
inside uh the device data environment

01:07:49,440 --> 01:07:53,760
for all of the duration of that target

01:07:51,599 --> 01:07:56,160
region so to do that we're going to

01:07:53,760 --> 01:07:59,280
before we go into our loop we're going

01:07:56,160 --> 01:08:01,760
to use a target enter data construct

01:07:59,280 --> 01:08:02,799
now we to do this we just say omp target

01:08:01,760 --> 01:08:04,319
enter data

01:08:02,799 --> 01:08:07,039
and then this is where we add the map

01:08:04,319 --> 01:08:10,319
clauses that we've had from before

01:08:07,039 --> 01:08:12,400
so this is essentially breaking up um

01:08:10,319 --> 01:08:13,520
the behavior of the target directive

01:08:12,400 --> 01:08:16,159
into two

01:08:13,520 --> 01:08:17,040
so when we have a target it transfers

01:08:16,159 --> 01:08:20,319
execution

01:08:17,040 --> 01:08:23,679
and it transfers data on a target

01:08:20,319 --> 01:08:24,159
enter data we just uh do the part of

01:08:23,679 --> 01:08:27,279
that

01:08:24,159 --> 01:08:30,319
um of that offload that just sets up

01:08:27,279 --> 01:08:31,679
the uh transfers to the device um

01:08:30,319 --> 01:08:33,839
that you would normally encounter when

01:08:31,679 --> 01:08:36,159
you reach a target directive

01:08:33,839 --> 01:08:37,600
so here it's just doing the any data

01:08:36,159 --> 01:08:38,000
mapping that you do at the start of a

01:08:37,600 --> 01:08:39,600
target

01:08:38,000 --> 01:08:42,960
region and this is going to set up this

01:08:39,600 --> 01:08:45,679
device data environment for us

01:08:42,960 --> 01:08:46,400
so we have omp target enter data map and

01:08:45,679 --> 01:08:48,239
we're going to map

01:08:46,400 --> 01:08:50,640
to the device from the host the three

01:08:48,239 --> 01:08:52,080
arrays a b and c

01:08:50,640 --> 01:08:53,920
at this point this is going to trigger

01:08:52,080 --> 01:08:56,000
this memory copy at

01:08:53,920 --> 01:08:57,600
on that construct so that the arrays get

01:08:56,000 --> 01:08:59,440
copied from the host to the device and

01:08:57,600 --> 01:09:02,880
they're going to stay there

01:08:59,440 --> 01:09:05,279
until they're told otherwise

01:09:02,880 --> 01:09:06,799
we can now run our time step loop our t

01:09:05,279 --> 01:09:09,440
loop lots of times

01:09:06,799 --> 01:09:10,880
and it's going to have these target

01:09:09,440 --> 01:09:13,920
regions that execute

01:09:10,880 --> 01:09:14,480
and when they start running those target

01:09:13,920 --> 01:09:17,600
regions

01:09:14,480 --> 01:09:19,040
inherit the existing data environment

01:09:17,600 --> 01:09:21,839
however it's been set up

01:09:19,040 --> 01:09:24,000
from before so this means that inside

01:09:21,839 --> 01:09:27,040
that target region it already has

01:09:24,000 --> 01:09:28,880
the mappings of a b and c already

01:09:27,040 --> 01:09:31,359
already been mapped to it from this

01:09:28,880 --> 01:09:32,799
enter data that's happened before

01:09:31,359 --> 01:09:35,440
this means that we don't have to do the

01:09:32,799 --> 01:09:38,400
bulk transfers of a b and c

01:09:35,440 --> 01:09:41,040
now what we do do is we um we map the

01:09:38,400 --> 01:09:42,880
any pointers and any metadata that that

01:09:41,040 --> 01:09:45,040
happen with a b and c the same as we

01:09:42,880 --> 01:09:47,199
would with scalar variables as first

01:09:45,040 --> 01:09:48,799
private but they just go as kernel

01:09:47,199 --> 01:09:49,440
arguments to the target region so we

01:09:48,799 --> 01:09:51,759
don't really

01:09:49,440 --> 01:09:53,359
observe this overhead but it does mean

01:09:51,759 --> 01:09:55,199
that if we have pointers saying i'll

01:09:53,359 --> 01:09:56,880
come onto this in a moment

01:09:55,199 --> 01:09:58,640
and the the pointer values are going to

01:09:56,880 --> 01:10:02,159
be mapped correctly as we

01:09:58,640 --> 01:10:05,040
as we want so

01:10:02,159 --> 01:10:07,120
we now have our target data environment

01:10:05,040 --> 01:10:09,840
set up with the target enter data

01:10:07,120 --> 01:10:11,920
we've we've then left the data there and

01:10:09,840 --> 01:10:14,159
run lots of target regions on it

01:10:11,920 --> 01:10:15,920
and now we want to bring back data from

01:10:14,159 --> 01:10:18,239
the device and for this we could use the

01:10:15,920 --> 01:10:20,239
target exit data

01:10:18,239 --> 01:10:21,760
construct and this performs the

01:10:20,239 --> 01:10:23,360
functionality that would normally happen

01:10:21,760 --> 01:10:26,640
at the end of a target region

01:10:23,360 --> 01:10:28,320
for moving data from device to host or

01:10:26,640 --> 01:10:29,360
maybe deleting data or something like

01:10:28,320 --> 01:10:31,280
that

01:10:29,360 --> 01:10:33,199
so for this at this point we would bring

01:10:31,280 --> 01:10:35,360
data the array c

01:10:33,199 --> 01:10:36,480
because i've said target exit data map

01:10:35,360 --> 01:10:38,880
from c

01:10:36,480 --> 01:10:39,920
and this would copy c back to the host

01:10:38,880 --> 01:10:43,360
and then remove

01:10:39,920 --> 01:10:44,719
c from the device data environment

01:10:43,360 --> 01:10:47,600
so in this way you can see that we've

01:10:44,719 --> 01:10:49,360
now severely limited the amount of

01:10:47,600 --> 01:10:52,159
memory transfer that happens and we've

01:10:49,360 --> 01:10:54,159
isolated it outside of the key

01:10:52,159 --> 01:10:56,560
time step loops that it only happens

01:10:54,159 --> 01:10:58,719
once we we copy the data once

01:10:56,560 --> 01:11:01,520
use it lots of times and copy back what

01:10:58,719 --> 01:11:01,520
we need at the end

01:11:02,480 --> 01:11:05,520
so for for vector add this is how i

01:11:04,400 --> 01:11:08,800
might do that

01:11:05,520 --> 01:11:10,400
i might allocate a and b and c

01:11:08,800 --> 01:11:12,560
i have three arrays on the host that i

01:11:10,400 --> 01:11:14,800
need to allocate the memory for

01:11:12,560 --> 01:11:17,040
and then initialize a and b on the host

01:11:14,800 --> 01:11:18,320
again this is using just some fortran

01:11:17,040 --> 01:11:21,280
vector notation

01:11:18,320 --> 01:11:22,800
just as a shorthand on the slide i then

01:11:21,280 --> 01:11:25,679
need to copy

01:11:22,800 --> 01:11:27,600
and set up my device data environment so

01:11:25,679 --> 01:11:29,199
i know in vector add in my kernel

01:11:27,600 --> 01:11:31,360
about two thirds of the way down this

01:11:29,199 --> 01:11:35,040
slide i'm going to be computing

01:11:31,360 --> 01:11:36,560
a plus b and setting it to be equal to c

01:11:35,040 --> 01:11:38,480
so that means i need to make sure that

01:11:36,560 --> 01:11:41,040
the device data environment has

01:11:38,480 --> 01:11:41,920
the initialized arrays a and b but also

01:11:41,040 --> 01:11:44,080
space for c

01:11:41,920 --> 01:11:45,040
but i don't need to copy any array for c

01:11:44,080 --> 01:11:48,480
because i'm going to be

01:11:45,040 --> 01:11:49,840
writing to it only so to do this i'm

01:11:48,480 --> 01:11:53,040
going to be using my

01:11:49,840 --> 01:11:55,920
target enter data construct so i have

01:11:53,040 --> 01:11:57,199
omp with the sentinel before target

01:11:55,920 --> 01:11:59,040
enter data map

01:11:57,199 --> 01:12:00,560
and i'm going to map from the host to

01:11:59,040 --> 01:12:03,760
the device the arrays a

01:12:00,560 --> 01:12:04,800
and b then i'm also going to allocate so

01:12:03,760 --> 01:12:07,280
with a map alok

01:12:04,800 --> 01:12:09,120
i'm going to allocate space for c that's

01:12:07,280 --> 01:12:13,040
going to be the same size as the

01:12:09,120 --> 01:12:13,600
c on the host and then i'm going to have

01:12:13,040 --> 01:12:15,520
my

01:12:13,600 --> 01:12:17,120
my target region my target teams

01:12:15,520 --> 01:12:20,400
distribute parallel do

01:12:17,120 --> 01:12:22,080
and that's going to be my target my

01:12:20,400 --> 01:12:24,159
kernel essentially is going to

01:12:22,080 --> 01:12:25,280
run vector add in parallel across all

01:12:24,159 --> 01:12:28,640
the parallel parts

01:12:25,280 --> 01:12:30,400
of the gpu as as mapped by my compiler

01:12:28,640 --> 01:12:32,960
and compute the addition of a and b and

01:12:30,400 --> 01:12:34,960
store it in c

01:12:32,960 --> 01:12:36,159
so at this point some data is going to

01:12:34,960 --> 01:12:38,719
be evaluated

01:12:36,159 --> 01:12:40,719
and be copied and mapped to the device

01:12:38,719 --> 01:12:43,440
because it's a target region

01:12:40,719 --> 01:12:44,640
but a b and c the actual bulk data

01:12:43,440 --> 01:12:47,280
already exists

01:12:44,640 --> 01:12:48,239
the values a b and c essentially pointer

01:12:47,280 --> 01:12:50,080
values

01:12:48,239 --> 01:12:52,400
they're going to be mapped essentially

01:12:50,080 --> 01:12:54,640
as a zero length array or first private

01:12:52,400 --> 01:12:57,679
as kernel arguments they don't you won't

01:12:54,640 --> 01:12:59,920
notice them as a memory transfer

01:12:57,679 --> 01:13:01,120
finally there's the variable n and

01:12:59,920 --> 01:13:03,840
there's a variable in my

01:13:01,120 --> 01:13:05,679
iteration space uh as in my loop my do

01:13:03,840 --> 01:13:07,520
from one to n n is going to be mapped

01:13:05,679 --> 01:13:09,600
because it's a scalar remember that gets

01:13:07,520 --> 01:13:12,080
mapped first private again that's just

01:13:09,600 --> 01:13:12,960
a kernel argument so we don't see a map

01:13:12,080 --> 01:13:14,560
for that

01:13:12,960 --> 01:13:15,920
this means that when that target

01:13:14,560 --> 01:13:18,320
executes there's not going to be any

01:13:15,920 --> 01:13:20,159
memory transfers triggered as a result

01:13:18,320 --> 01:13:21,440
which is exactly what we want we've set

01:13:20,159 --> 01:13:23,840
the data up we

01:13:21,440 --> 01:13:25,440
run the kernel do the computation and

01:13:23,840 --> 01:13:27,360
then later at the end at the bottom of

01:13:25,440 --> 01:13:30,239
this slide is when i'm going to

01:13:27,360 --> 01:13:33,840
copy the data out of the gpu back to the

01:13:30,239 --> 01:13:33,840
host and it's the array c

01:13:34,000 --> 01:13:38,480
so we're so we've allocated c beforehand

01:13:36,480 --> 01:13:39,600
we've now filled it full of data inside

01:13:38,480 --> 01:13:41,600
the target region

01:13:39,600 --> 01:13:43,360
and we're now going to copy the data

01:13:41,600 --> 01:13:44,960
back uh to the host

01:13:43,360 --> 01:13:46,480
using his exit data and that's going to

01:13:44,960 --> 01:13:48,080
remove it from the device data

01:13:46,480 --> 01:13:49,600
environment because essentially we're at

01:13:48,080 --> 01:13:52,400
the end of the program

01:13:49,600 --> 01:13:53,360
now i could also delete a a and b from

01:13:52,400 --> 01:13:57,600
the device

01:13:53,360 --> 01:13:57,600
data environment as well at that point

01:13:59,280 --> 01:14:03,679
so with data existing on the device for

01:14:02,080 --> 01:14:06,320
a long period of time

01:14:03,679 --> 01:14:09,040
there may be times when i need to move

01:14:06,320 --> 01:14:11,840
data between the host and the device

01:14:09,040 --> 01:14:13,360
on other occasions um those occasions

01:14:11,840 --> 01:14:16,080
are when i don't want to

01:14:13,360 --> 01:14:18,320
create more sort of nested device target

01:14:16,080 --> 01:14:19,199
data regions and or i don't necessarily

01:14:18,320 --> 01:14:22,320
want to run

01:14:19,199 --> 01:14:26,000
a kernel on the device i just want to

01:14:22,320 --> 01:14:28,640
move data between an example of this

01:14:26,000 --> 01:14:31,040
could be halo exchange and i'll show you

01:14:28,640 --> 01:14:32,000
that coming up or maybe some file i o

01:14:31,040 --> 01:14:34,159
something like that

01:14:32,000 --> 01:14:36,080
the idea is the host is going to want to

01:14:34,159 --> 01:14:39,600
do something with some data

01:14:36,080 --> 01:14:41,920
between two target regions so i have a

01:14:39,600 --> 01:14:43,679
target construct that updates data i

01:14:41,920 --> 01:14:44,480
want to bring it back to the host to use

01:14:43,679 --> 01:14:47,120
it on the host

01:14:44,480 --> 01:14:48,480
the mpi for file i o something else a

01:14:47,120 --> 01:14:51,040
library call to

01:14:48,480 --> 01:14:51,920
my favorite library and then i need to i

01:14:51,040 --> 01:14:55,120
then need to

01:14:51,920 --> 01:14:57,040
continue on the on the device after that

01:14:55,120 --> 01:14:58,800
so the update construct this is another

01:14:57,040 --> 01:15:00,960
one of those standalone constructs

01:14:58,800 --> 01:15:02,480
that doesn't have any fortran code

01:15:00,960 --> 01:15:04,400
associated with it in a structured block

01:15:02,480 --> 01:15:06,560
it's a standalone line

01:15:04,400 --> 01:15:08,400
and we can use these to move data

01:15:06,560 --> 01:15:09,120
explicitly between the sort of hosting

01:15:08,400 --> 01:15:11,440
device

01:15:09,120 --> 01:15:12,880
in either direction on demand and

01:15:11,440 --> 01:15:15,840
remember this direction

01:15:12,880 --> 01:15:18,080
is always from the host so it is two

01:15:15,840 --> 01:15:21,679
from the host to the device

01:15:18,080 --> 01:15:24,800
or from the device to the host so it's

01:15:21,679 --> 01:15:25,840
a two mapping is host 2 device the from

01:15:24,800 --> 01:15:30,000
mapping

01:15:25,840 --> 01:15:33,040
is device is from device

01:15:30,000 --> 01:15:33,040
back on the host

01:15:33,280 --> 01:15:36,719
so this example here is how i may use

01:15:36,400 --> 01:15:39,920
the

01:15:36,719 --> 01:15:40,400
update construct so i start off on line

01:15:39,920 --> 01:15:42,159
one

01:15:40,400 --> 01:15:43,440
with a one of these enter data

01:15:42,159 --> 01:15:46,400
constructs it's going to set

01:15:43,440 --> 01:15:46,960
up and copy the arrays a b and c to the

01:15:46,400 --> 01:15:48,960
device

01:15:46,960 --> 01:15:50,400
so it's going to copy the memory that's

01:15:48,960 --> 01:15:52,880
been initialized on the host

01:15:50,400 --> 01:15:55,120
and copy it to the device i then have a

01:15:52,880 --> 01:15:58,000
target region on lines two to four

01:15:55,120 --> 01:15:58,719
and that's going to use a b and c and

01:15:58,000 --> 01:16:00,640
let's say

01:15:58,719 --> 01:16:02,719
inside that target region i'm going to

01:16:00,640 --> 01:16:03,440
update a i might have computed a new

01:16:02,719 --> 01:16:05,440
value of a

01:16:03,440 --> 01:16:06,800
based on the old value of a and maybe

01:16:05,440 --> 01:16:10,239
some some b's and c

01:16:06,800 --> 01:16:10,239
uh what's in b and c as well

01:16:10,320 --> 01:16:15,520
i then need to take uh the a array

01:16:13,600 --> 01:16:17,199
and move it back to the host maybe

01:16:15,520 --> 01:16:19,360
because i need to do something to it and

01:16:17,199 --> 01:16:20,000
what i need to do on on say line 10 is

01:16:19,360 --> 01:16:21,760
is is

01:16:20,000 --> 01:16:23,520
just an example of something i could do

01:16:21,760 --> 01:16:25,120
and i might need to change it to a

01:16:23,520 --> 01:16:26,159
specific value that i've that i've

01:16:25,120 --> 01:16:29,600
received a message

01:16:26,159 --> 01:16:30,400
over mpi from so to do this i then you

01:16:29,600 --> 01:16:33,840
can see on line

01:16:30,400 --> 01:16:34,400
7 the update construct this is omp

01:16:33,840 --> 01:16:38,000
target

01:16:34,400 --> 01:16:40,480
update and it says to uh from

01:16:38,000 --> 01:16:41,520
so copy the data from the device to the

01:16:40,480 --> 01:16:43,440
host from

01:16:41,520 --> 01:16:45,600
um and i want a and it's going to be the

01:16:43,440 --> 01:16:47,520
whole array from from 1 to n now again i

01:16:45,600 --> 01:16:48,000
could have left the array notation off

01:16:47,520 --> 01:16:52,640
and just

01:16:48,000 --> 01:16:52,640
use the the variable name a

01:16:52,719 --> 01:16:57,280
so at this point the this is going to

01:16:55,600 --> 01:16:59,440
essentially trigger a memory copy

01:16:57,280 --> 01:17:01,360
between the device and the host

01:16:59,440 --> 01:17:03,760
the notation here is also interesting we

01:17:01,360 --> 01:17:07,040
no longer use the map clauses we just

01:17:03,760 --> 01:17:09,120
use a from clause to um to

01:17:07,040 --> 01:17:11,199
to signify the direction because update

01:17:09,120 --> 01:17:13,440
really captures everything we need

01:17:11,199 --> 01:17:14,960
we're updating the memory and we're

01:17:13,440 --> 01:17:15,440
moving memory from the device to the

01:17:14,960 --> 01:17:18,400
host

01:17:15,440 --> 01:17:20,560
and in brackets is what we're moving on

01:17:18,400 --> 01:17:21,440
line 10 then i just update a to be

01:17:20,560 --> 01:17:22,880
something

01:17:21,440 --> 01:17:25,520
um you can imagine that this was

01:17:22,880 --> 01:17:27,840
actually just a halo exchange routine

01:17:25,520 --> 01:17:30,159
where i've done my hello exchange and i

01:17:27,840 --> 01:17:31,920
need to re-pack my buffers and i've got

01:17:30,159 --> 01:17:35,440
new information to put back in

01:17:31,920 --> 01:17:37,040
in the array then on line 13 this is

01:17:35,440 --> 01:17:39,440
where i transfer that array

01:17:37,040 --> 01:17:40,080
and copy the data there the updated data

01:17:39,440 --> 01:17:42,960
back

01:17:40,080 --> 01:17:45,440
to the device and this is where i also

01:17:42,960 --> 01:17:48,640
use the update construct

01:17:45,440 --> 01:17:49,679
so on line 13 we have omp target update

01:17:48,640 --> 01:17:52,000
01:17:49,679 --> 01:17:53,600
and this maps data from the host to the

01:17:52,000 --> 01:17:55,600
device it copies it is another memory

01:17:53,600 --> 01:17:57,760
copy from the device to

01:17:55,600 --> 01:18:00,719
from the host to the device this is

01:17:57,760 --> 01:18:02,320
going the other way host to device

01:18:00,719 --> 01:18:04,080
and again it's mapping the whole of

01:18:02,320 --> 01:18:05,760
array a

01:18:04,080 --> 01:18:07,840
and then that's that means then that the

01:18:05,760 --> 01:18:10,080
device data environment now sees the

01:18:07,840 --> 01:18:12,239
updated copy of a because it's

01:18:10,080 --> 01:18:14,560
received it from the host so in this

01:18:12,239 --> 01:18:17,040
second target region on line 15

01:18:14,560 --> 01:18:19,040
we've got the updated values of a along

01:18:17,040 --> 01:18:21,600
with whatever b and c were on the device

01:18:19,040 --> 01:18:23,280
when we when we last left them

01:18:21,600 --> 01:18:25,120
finally the end i might exit the data

01:18:23,280 --> 01:18:28,159
region and copy data

01:18:25,120 --> 01:18:28,159
back from the device

01:18:28,480 --> 01:18:33,040
so these update constructs are very

01:18:30,000 --> 01:18:36,400
useful for in combination with the enter

01:18:33,040 --> 01:18:38,000
and exit data constructs um so for us to

01:18:36,400 --> 01:18:40,320
be able to

01:18:38,000 --> 01:18:42,000
initialize all the data on our device at

01:18:40,320 --> 01:18:44,960
the start of a run

01:18:42,000 --> 01:18:46,320
run lots of kernels using the target the

01:18:44,960 --> 01:18:49,280
target regions

01:18:46,320 --> 01:18:50,640
and then ad hoc and on demand move data

01:18:49,280 --> 01:18:53,600
between the host and the device

01:18:50,640 --> 01:18:54,480
as required for interoperability with

01:18:53,600 --> 01:18:56,880
other things

01:18:54,480 --> 01:19:00,320
for example a hello exchange routine

01:18:56,880 --> 01:19:00,320
using the update construct

01:19:00,560 --> 01:19:04,480
so this is this is exactly that example

01:19:02,880 --> 01:19:05,440
with with a typical halo exchange

01:19:04,480 --> 01:19:07,600
pattern

01:19:05,440 --> 01:19:08,960
on line one i set up my data on my

01:19:07,600 --> 01:19:10,880
device my target

01:19:08,960 --> 01:19:13,040
enter data map and it's going to copy

01:19:10,880 --> 01:19:14,640
all my initialized arrays to the to the

01:19:13,040 --> 01:19:17,120
device

01:19:14,640 --> 01:19:17,679
between lines 3 and 16 is my time step

01:19:17,120 --> 01:19:20,239
loop

01:19:17,679 --> 01:19:22,480
where i'm going to run a kernel and then

01:19:20,239 --> 01:19:25,360
do a halo exchange and then

01:19:22,480 --> 01:19:27,440
run a kernel again and and keep going

01:19:25,360 --> 01:19:29,840
until i've done enough time steps

01:19:27,440 --> 01:19:31,280
and finally on on line 18 is when my

01:19:29,840 --> 01:19:33,760
simulation is done

01:19:31,280 --> 01:19:34,880
and i need to clean up the device data

01:19:33,760 --> 01:19:36,880
environment and copy back

01:19:34,880 --> 01:19:39,120
everything i need all the computed

01:19:36,880 --> 01:19:40,960
values

01:19:39,120 --> 01:19:42,480
so inside the time step loop on line

01:19:40,960 --> 01:19:44,159
four to six

01:19:42,480 --> 01:19:45,520
i have my target that's going to run my

01:19:44,159 --> 01:19:47,199
main kernel that's the main

01:19:45,520 --> 01:19:50,080
computational part of the

01:19:47,199 --> 01:19:51,120
of this code and then in order to do a

01:19:50,080 --> 01:19:54,159
halo exchange

01:19:51,120 --> 01:19:55,199
on line 9 i have my target update and

01:19:54,159 --> 01:19:57,280
it's going to be a target

01:19:55,199 --> 01:19:58,400
update from that's going to copy the

01:19:57,280 --> 01:20:00,320
data this

01:19:58,400 --> 01:20:02,560
this array that i might have packed

01:20:00,320 --> 01:20:04,080
inside the target region called halo

01:20:02,560 --> 01:20:06,159
and that's going to copy it from the

01:20:04,080 --> 01:20:09,840
device to the host so it's from

01:20:06,159 --> 01:20:12,880
device to host this halo array so now

01:20:09,840 --> 01:20:13,679
um the host has got an updated copy of

01:20:12,880 --> 01:20:16,480
this halo

01:20:13,679 --> 01:20:18,800
array called halo which i might exchange

01:20:16,480 --> 01:20:22,159
using mpi with an mpi send receive and

01:20:18,800 --> 01:20:23,679
whatever arguments i need to do that

01:20:22,159 --> 01:20:25,840
because i've then received some data i

01:20:23,679 --> 01:20:27,679
now need to copy that data

01:20:25,840 --> 01:20:29,040
back to the device so this is where i

01:20:27,679 --> 01:20:32,159
have my omp

01:20:29,040 --> 01:20:34,560
target update to and that's going to

01:20:32,159 --> 01:20:35,760
give the device and send that new

01:20:34,560 --> 01:20:38,320
updated value

01:20:35,760 --> 01:20:40,000
of the halo array back to the device

01:20:38,320 --> 01:20:41,440
ready to run the kernel that's going to

01:20:40,000 --> 01:20:44,320
use that data

01:20:41,440 --> 01:20:46,080
in the next time step iteration so you

01:20:44,320 --> 01:20:47,199
can see with just this update we and

01:20:46,080 --> 01:20:48,719
along with the enter

01:20:47,199 --> 01:20:50,560
data and exit data we have quite a

01:20:48,719 --> 01:20:53,520
powerful way of

01:20:50,560 --> 01:20:54,639
um of directing exactly when memory

01:20:53,520 --> 01:20:56,560
needs to be transferred

01:20:54,639 --> 01:20:57,679
we can set everything up run lots of

01:20:56,560 --> 01:21:00,320
target readings on it

01:20:57,679 --> 01:21:03,199
and then copy data ad hoc using the

01:21:00,320 --> 01:21:03,199
update construct

01:21:05,199 --> 01:21:08,560
so pointer swapping this is this is

01:21:06,719 --> 01:21:11,280
important in fortran 2

01:21:08,560 --> 01:21:12,960
especially when we when we consider the

01:21:11,280 --> 01:21:14,719
way that the stencil code is written

01:21:12,960 --> 01:21:15,520
they are just pointers that we can then

01:21:14,719 --> 01:21:18,400
swap between

01:21:15,520 --> 01:21:18,880
two arrays the temporary array and the

01:21:18,400 --> 01:21:20,719
um

01:21:18,880 --> 01:21:23,040
the sort of a read-only in a temporary

01:21:20,719 --> 01:21:23,040
array

01:21:23,280 --> 01:21:26,960
so it's worth noting that the mapping of

01:21:26,080 --> 01:21:29,760
the pointers

01:21:26,960 --> 01:21:31,360
occurs uh when that target region is

01:21:29,760 --> 01:21:33,040
sort of encountered as from the host

01:21:31,360 --> 01:21:35,760
perspective

01:21:33,040 --> 01:21:37,679
so this example here sort of captures uh

01:21:35,760 --> 01:21:40,239
in a nutshell what's happening with the

01:21:37,679 --> 01:21:41,920
uh the stencil code and when we start

01:21:40,239 --> 01:21:42,320
optimizing for data movement this may be

01:21:41,920 --> 01:21:44,800
how it

01:21:42,320 --> 01:21:46,960
how it could look so on line one we have

01:21:44,800 --> 01:21:49,040
uh data environment set up with uh the

01:21:46,960 --> 01:21:51,760
target enter data

01:21:49,040 --> 01:21:53,440
i have a loop and a number of target

01:21:51,760 --> 01:21:55,120
regions that will run inside that and

01:21:53,440 --> 01:21:56,880
i've just abstracted that away on line

01:21:55,120 --> 01:21:58,800
five to whatever it needs to do

01:21:56,880 --> 01:22:01,120
along with the end target that's missing

01:21:58,800 --> 01:22:02,320
on this slide too

01:22:01,120 --> 01:22:04,639
and then have a pointer swap this is

01:22:02,320 --> 01:22:07,280
just a regular pointer swap

01:22:04,639 --> 01:22:10,000
so when i re-encounter the target region

01:22:07,280 --> 01:22:13,040
for t equals two on the second iteration

01:22:10,000 --> 01:22:15,840
a and b now point to the other places

01:22:13,040 --> 01:22:17,120
now that's fine because those get um

01:22:15,840 --> 01:22:19,760
sort of evaluated

01:22:17,120 --> 01:22:20,960
every time the target construct is um is

01:22:19,760 --> 01:22:22,960
encountered

01:22:20,960 --> 01:22:24,719
we're not mapping them we're just

01:22:22,960 --> 01:22:26,480
checking what the

01:22:24,719 --> 01:22:28,400
mapping between the host and the device

01:22:26,480 --> 01:22:30,320
memory spaces are and this is done by

01:22:28,400 --> 01:22:32,400
the compiler and the run time

01:22:30,320 --> 01:22:33,360
so this means that inside that target

01:22:32,400 --> 01:22:35,520
region

01:22:33,360 --> 01:22:37,280
we will observe that a and b have been

01:22:35,520 --> 01:22:38,320
swapped even though we've swapped them

01:22:37,280 --> 01:22:40,000
on the host

01:22:38,320 --> 01:22:41,679
when we encounter them on the device

01:22:40,000 --> 01:22:42,560
we'll we'll still behave as if they've

01:22:41,679 --> 01:22:44,320
been swapped

01:22:42,560 --> 01:22:45,679
um there as well because it's just a

01:22:44,320 --> 01:22:48,159
pointer swap that id

01:22:45,679 --> 01:22:52,080
uh propagates over there because of the

01:22:48,159 --> 01:22:53,440
way that the targets are evaluated

01:22:52,080 --> 01:22:55,840
it's worth noting that there's a

01:22:53,440 --> 01:22:57,760
slightly uh a thorny slightly tricky

01:22:55,840 --> 01:23:00,000
sort of issue you could run into

01:22:57,760 --> 01:23:02,480
if you have a target region that you map

01:23:00,000 --> 01:23:04,480
from data and then the point is change

01:23:02,480 --> 01:23:07,600
inside the target region

01:23:04,480 --> 01:23:09,679
then the data will be mapped from to

01:23:07,600 --> 01:23:12,800
wherever it was pointing to

01:23:09,679 --> 01:23:14,719
when that from was encountered which was

01:23:12,800 --> 01:23:16,960
at the start of the target region

01:23:14,719 --> 01:23:18,800
this is a slightly thorny issue that

01:23:16,960 --> 01:23:20,960
you're unlikely to run into

01:23:18,800 --> 01:23:24,719
if you use these target enter data and

01:23:20,960 --> 01:23:24,719
target exit data constructs as well

01:23:28,719 --> 01:23:31,760
so asynchronous offload so far we've had

01:23:31,440 --> 01:23:34,239
the

01:23:31,760 --> 01:23:36,159
host just sitting idle whilst the device

01:23:34,239 --> 01:23:39,920
has been executing

01:23:36,159 --> 01:23:42,880
but we can have a way of the um

01:23:39,920 --> 01:23:45,280
of the device uh the the host carrying

01:23:42,880 --> 01:23:48,000
on and continuing whilst the

01:23:45,280 --> 01:23:48,960
the target region is is being busy

01:23:48,000 --> 01:23:50,560
executing

01:23:48,960 --> 01:23:52,000
so for this we have the no weight clause

01:23:50,560 --> 01:23:54,400
to the target region and this means it

01:23:52,000 --> 01:23:56,719
gets offloaded as a task

01:23:54,400 --> 01:23:58,320
at this point we must use all the openmp

01:23:56,719 --> 01:24:01,679
tasking semantics

01:23:58,320 --> 01:24:02,480
such as using task weight or a a barrier

01:24:01,679 --> 01:24:05,280
or something

01:24:02,480 --> 01:24:06,320
on the host depending on how you've

01:24:05,280 --> 01:24:09,600
designed

01:24:06,320 --> 01:24:11,520
your application for for these

01:24:09,600 --> 01:24:15,120
asynchronous tasks whether you're using

01:24:11,520 --> 01:24:15,120
dependencies or task weight

01:24:15,360 --> 01:24:19,199
so as an example for this let's say i

01:24:17,360 --> 01:24:21,520
have a big amount of work still on the

01:24:19,199 --> 01:24:23,520
gpu so i have target no weight and then

01:24:21,520 --> 01:24:24,639
my parallel loop over a very large

01:24:23,520 --> 01:24:26,960
number

01:24:24,639 --> 01:24:27,679
and then i have my two end omp end for

01:24:26,960 --> 01:24:29,520
those things

01:24:27,679 --> 01:24:30,880
now the host is just going to continue

01:24:29,520 --> 01:24:32,719
because of the no wait

01:24:30,880 --> 01:24:34,400
and it's going to call some expensive i

01:24:32,719 --> 01:24:35,920
o routine that's completely independent

01:24:34,400 --> 01:24:39,600
to whatever works being done

01:24:35,920 --> 01:24:42,239
on the target i then on the final line

01:24:39,600 --> 01:24:43,120
line 13 i call omp task weight that

01:24:42,239 --> 01:24:46,320
should be a k

01:24:43,120 --> 01:24:48,159
task weight i'll fix that typo and

01:24:46,320 --> 01:24:50,000
at that point any that any of those

01:24:48,159 --> 01:24:52,400
tasks that i've had

01:24:50,000 --> 01:24:53,199
executed before so these this target no

01:24:52,400 --> 01:24:56,080
wait

01:24:53,199 --> 01:24:57,760
at that point my host will wait until

01:24:56,080 --> 01:24:58,480
that target task is finished before i

01:24:57,760 --> 01:25:00,239
can

01:24:58,480 --> 01:25:01,920
continue on doing something else so

01:25:00,239 --> 01:25:04,400
that's an example of asynchronous

01:25:01,920 --> 01:25:04,400
offload

01:25:05,199 --> 01:25:10,159
so let's talk a little bit about uh

01:25:06,800 --> 01:25:12,719
compiler support and so craig was

01:25:10,159 --> 01:25:14,480
was as quite early in adopting um

01:25:12,719 --> 01:25:16,960
support for nvidia gpus in

01:25:14,480 --> 01:25:18,239
the end of 2015 as you've seen and what

01:25:16,960 --> 01:25:20,320
we've been using today the latest

01:25:18,239 --> 01:25:21,280
version of cce now supports all of

01:25:20,320 --> 01:25:24,000
openmp

01:25:21,280 --> 01:25:25,120
five and sum of five uh in both c and

01:25:24,000 --> 01:25:29,040
fortran and c

01:25:25,120 --> 01:25:31,679
plus as well hello vm and clang is

01:25:29,040 --> 01:25:33,679
often has um a lot of the leading edge

01:25:31,679 --> 01:25:35,600
implementations of openmp

01:25:33,679 --> 01:25:37,679
uh 4.5 offload and in particular for

01:25:35,600 --> 01:25:38,719
nvidia gpus and it's used as the base

01:25:37,679 --> 01:25:41,600
for many compilers

01:25:38,719 --> 01:25:43,040
including for cray a lot of that work

01:25:41,600 --> 01:25:46,800
originally came from

01:25:43,040 --> 01:25:48,480
ibm with their excel compiler suite

01:25:46,800 --> 01:25:51,760
and they contributed a lot of the work

01:25:48,480 --> 01:25:53,600
of the early work into into llvm

01:25:51,760 --> 01:25:55,199
the intel compilers originally began

01:25:53,600 --> 01:25:57,360
supporting openmp

01:25:55,199 --> 01:26:00,320
target for their xeon phi code

01:25:57,360 --> 01:26:02,800
processors back in 2013.

01:26:00,320 --> 01:26:04,960
with compiler a couple years later in

01:26:02,800 --> 01:26:07,920
compiler version 17

01:26:04,960 --> 01:26:09,840
they supported openmp 4.5 for xeon phi

01:26:07,920 --> 01:26:13,120
but then with their latest one api

01:26:09,840 --> 01:26:15,920
suite of compilers tools and runtimes

01:26:13,120 --> 01:26:16,880
and the one api suite now targets intel

01:26:15,920 --> 01:26:20,000
gpus with

01:26:16,880 --> 01:26:21,440
openmp so using these same compiler

01:26:20,000 --> 01:26:23,520
directories that you've seen today

01:26:21,440 --> 01:26:25,360
you're able to target nvidia gpus and

01:26:23,520 --> 01:26:28,400
intel gpus

01:26:25,360 --> 01:26:29,120
now the gcc compiler has had openmp

01:26:28,400 --> 01:26:30,960
support for

01:26:29,120 --> 01:26:33,360
lots of different architectures and with

01:26:30,960 --> 01:26:36,960
uh with the latest version 10.0

01:26:33,360 --> 01:26:39,120
it can have target for xeon phi amd gpus

01:26:36,960 --> 01:26:41,840
and nvidia gpus

01:26:39,120 --> 01:26:44,560
amd also provide their um their own

01:26:41,840 --> 01:26:47,120
openmp compiler for amd gpus

01:26:44,560 --> 01:26:49,040
so this really is a is a cross-platform

01:26:47,120 --> 01:26:49,440
portable parallel programming model that

01:26:49,040 --> 01:26:51,760
is an

01:26:49,440 --> 01:26:52,560
open standard openmp is a standardized

01:26:51,760 --> 01:26:55,040
model and

01:26:52,560 --> 01:26:57,199
and all the vendors are contributing um

01:26:55,040 --> 01:26:59,280
to that standard

01:26:57,199 --> 01:27:01,520
so that is possible to write an openmp

01:26:59,280 --> 01:27:03,040
program that will target gpus from all

01:27:01,520 --> 01:27:04,960
of these three vendors

01:27:03,040 --> 01:27:06,400
and if we look at the large machines

01:27:04,960 --> 01:27:07,840
that are going to be installed in this

01:27:06,400 --> 01:27:09,280
sort of exascale era

01:27:07,840 --> 01:27:11,760
we're going to see lots and lots of

01:27:09,280 --> 01:27:13,840
intel gpus lots of amd gpus

01:27:11,760 --> 01:27:15,040
and lots of nvidia gpus particularly in

01:27:13,840 --> 01:27:17,679
their sort of machine learning

01:27:15,040 --> 01:27:19,120
and deep learning machines as well the

01:27:17,679 --> 01:27:20,800
best place to go and learn about the

01:27:19,120 --> 01:27:21,840
compiler support is on the openmp

01:27:20,800 --> 01:27:23,360
website

01:27:21,840 --> 01:27:26,000
and the link to that is on the bottom of

01:27:23,360 --> 01:27:26,000
this slide

01:27:29,280 --> 01:27:35,520
now i talked a little bit about

01:27:32,480 --> 01:27:37,520
um nv prof before and i'm going to show

01:27:35,520 --> 01:27:41,280
you a little bit of an example for that

01:27:37,520 --> 01:27:41,520
at the moment um because with the cray

01:27:41,280 --> 01:27:45,600
and

01:27:41,520 --> 01:27:48,400
lvm kuda is is used underneath

01:27:45,600 --> 01:27:50,719
the openmp implementation for targeting

01:27:48,400 --> 01:27:52,639
nvidia gpus so us as programmers will

01:27:50,719 --> 01:27:54,320
write openmp and the compiler will turn

01:27:52,639 --> 01:27:55,440
it into whatever cuda it needs to and

01:27:54,320 --> 01:27:59,199
this means

01:27:55,440 --> 01:28:00,639
that it will it will use any of the

01:27:59,199 --> 01:28:02,639
any of the infrastructure and tools that

01:28:00,639 --> 01:28:05,679
are provided by nvidia um

01:28:02,639 --> 01:28:08,159
for for their cuda codes so it can be

01:28:05,679 --> 01:28:09,600
useful to use um the vendor tools for

01:28:08,159 --> 01:28:11,360
cuda to understand what's happening with

01:28:09,600 --> 01:28:12,880
your openmp code

01:28:11,360 --> 01:28:14,960
it's particularly useful because in

01:28:12,880 --> 01:28:16,400
openmp4.5 although there's been some

01:28:14,960 --> 01:28:20,080
environment variables added in

01:28:16,400 --> 01:28:20,880
in 5.0 um that that mean that if your

01:28:20,080 --> 01:28:23,360
target

01:28:20,880 --> 01:28:25,360
target region doesn't get executed on

01:28:23,360 --> 01:28:27,280
the gpu it can silently fall back to the

01:28:25,360 --> 01:28:31,440
cpu and you can control that behavior

01:28:27,280 --> 01:28:33,600
in 5.0 now so this is an example for the

01:28:31,440 --> 01:28:36,400
for the stencil target code of what

01:28:33,600 --> 01:28:38,719
env prof might look like so the top

01:28:36,400 --> 01:28:40,560
there it sort of gives me a summary of

01:28:38,719 --> 01:28:41,760
of how much time is spent in each of my

01:28:40,560 --> 01:28:43,280
target regions

01:28:41,760 --> 01:28:44,880
and then how much time is spent in

01:28:43,280 --> 01:28:47,440
memory transfers

01:28:44,880 --> 01:28:49,280
now the target regions are slightly uh

01:28:47,440 --> 01:28:51,440
slightly strange they look a bit weird

01:28:49,280 --> 01:28:52,960
and and they're constructed because they

01:28:51,440 --> 01:28:54,880
don't really have a name it's just

01:28:52,960 --> 01:28:56,239
line numbers and stuff in our code and

01:28:54,880 --> 01:28:57,679
that's exactly how they're expressed

01:28:56,239 --> 01:29:00,239
here we have a file name

01:28:57,679 --> 01:29:02,239
and a line number so you can see that

01:29:00,239 --> 01:29:05,440
most of the time is being spent

01:29:02,239 --> 01:29:07,280
on line 49 which is that target region

01:29:05,440 --> 01:29:11,840
in that particular file my stencil

01:29:07,280 --> 01:29:11,840
target file for instance

01:29:12,480 --> 01:29:17,120
if we have the print gpu trace

01:29:15,520 --> 01:29:18,480
then this is going to print a nice

01:29:17,120 --> 01:29:19,520
timeline of exactly what's being

01:29:18,480 --> 01:29:21,920
transferred

01:29:19,520 --> 01:29:23,520
at any given time which is which is very

01:29:21,920 --> 01:29:24,639
useful information to help tracking if

01:29:23,520 --> 01:29:28,080
we have any of these

01:29:24,639 --> 01:29:30,480
spurious memory transfers

01:29:28,080 --> 01:29:31,760
so it's worth me saying at this point

01:29:30,480 --> 01:29:34,800
that

01:29:31,760 --> 01:29:38,239
the the result of a reduction if you use

01:29:34,800 --> 01:29:41,679
a reduction clause on a target region

01:29:38,239 --> 01:29:44,480
then in openmp 4.5 that

01:29:41,679 --> 01:29:47,199
is a scalar typically a scalar if you

01:29:44,480 --> 01:29:50,480
don't specify it in a map clause

01:29:47,199 --> 01:29:50,960
then that that scalar is just mapped

01:29:50,480 --> 01:29:53,120
first

01:29:50,960 --> 01:29:55,040
private this means we won't get the

01:29:53,120 --> 01:29:57,120
result uh the reduction result back on

01:29:55,040 --> 01:29:59,760
the host so it's very important

01:29:57,120 --> 01:30:00,800
for reductions that we have a map to

01:29:59,760 --> 01:30:03,440
from

01:30:00,800 --> 01:30:04,400
um with the reduction result in if we're

01:30:03,440 --> 01:30:07,440
targeting openmp

01:30:04,400 --> 01:30:08,639
4.5 and open mp5 they've changed this

01:30:07,440 --> 01:30:10,639
and they've made that the default

01:30:08,639 --> 01:30:11,679
behavior so if you specify a reduction

01:30:10,639 --> 01:30:14,639
clause

01:30:11,679 --> 01:30:16,880
then the reduction result is going to be

01:30:14,639 --> 01:30:18,800
mapped to from

01:30:16,880 --> 01:30:21,040
so that's an extra thing to think about

01:30:18,800 --> 01:30:23,520
with the with the reduction clause

01:30:21,040 --> 01:30:24,480
so we now come up to the the final

01:30:23,520 --> 01:30:26,320
exercise

01:30:24,480 --> 01:30:27,920
for the day and this is going to apply

01:30:26,320 --> 01:30:29,520
the things that we've been talking about

01:30:27,920 --> 01:30:31,840
in the last two

01:30:29,520 --> 01:30:33,520
lecture sessions before the coffee break

01:30:31,840 --> 01:30:36,159
and after

01:30:33,520 --> 01:30:37,920
so we now have a a parallel we now have

01:30:36,159 --> 01:30:39,040
a five-point stencil code that is

01:30:37,920 --> 01:30:40,800
running on a gpu

01:30:39,040 --> 01:30:42,400
but there's no parallelism that is

01:30:40,800 --> 01:30:44,639
exposed there

01:30:42,400 --> 01:30:46,560
so use the use the target teams

01:30:44,639 --> 01:30:47,440
distribute parallel due construct to add

01:30:46,560 --> 01:30:51,280
parallelism

01:30:47,440 --> 01:30:53,760
to the kernel and also we are moving

01:30:51,280 --> 01:30:56,239
lots of data around every iteration so

01:30:53,760 --> 01:30:58,080
change the code so that it it only maps

01:30:56,239 --> 01:30:58,960
the data to and from the device using

01:30:58,080 --> 01:31:01,120
the target

01:30:58,960 --> 01:31:03,520
enter and exit data constructs before

01:31:01,120 --> 01:31:05,840
and after the iteration loop

01:31:03,520 --> 01:31:07,199
again make it do this reduction and

01:31:05,840 --> 01:31:08,800
print out the grid sum for every

01:31:07,199 --> 01:31:09,920
iteration to make sure it gets the right

01:31:08,800 --> 01:31:11,120
result

01:31:09,920 --> 01:31:13,600
so this means you need to use the

01:31:11,120 --> 01:31:14,719
reduction clause on the teams distribute

01:31:13,600 --> 01:31:16,800
parallel to

01:31:14,719 --> 01:31:19,840
and it means as i just said you need to

01:31:16,800 --> 01:31:23,040
use the map clause on the target

01:31:19,840 --> 01:31:25,199
around the exact kernel to copy

01:31:23,040 --> 01:31:27,040
the reduction result the scalar back to

01:31:25,199 --> 01:31:28,960
the host so that we can print it out

01:31:27,040 --> 01:31:31,199
or check it for convergence or something

01:31:28,960 --> 01:31:32,560
like that

01:31:31,199 --> 01:31:34,480
it's useful at this point to start to

01:31:32,560 --> 01:31:36,239
have a play and use the system and try

01:31:34,480 --> 01:31:38,080
things out and see how things go so

01:31:36,239 --> 01:31:39,920
experiment with different variations of

01:31:38,080 --> 01:31:41,840
the of the construct

01:31:39,920 --> 01:31:43,679
what performance can you get by using

01:31:41,840 --> 01:31:45,840
teams distribute what performance you

01:31:43,679 --> 01:31:48,000
get if you use target parallel do

01:31:45,840 --> 01:31:49,120
what if you have uh teams distribute

01:31:48,000 --> 01:31:50,400
parallel do sim d

01:31:49,120 --> 01:31:52,239
all those sorts of things try the

01:31:50,400 --> 01:31:52,639
different combinations and and see what

01:31:52,239 --> 01:31:55,920
you

01:31:52,639 --> 01:31:58,000
see what you get you can also profile

01:31:55,920 --> 01:31:59,199
the code with nvprof to help explore the

01:31:58,000 --> 01:32:01,679
mapping

01:31:59,199 --> 01:32:02,880
have a look in the list dot lst file

01:32:01,679 --> 01:32:04,320
that gets

01:32:02,880 --> 01:32:05,840
output by the compiler with the right

01:32:04,320 --> 01:32:08,800
compiler flag that's in the make file

01:32:05,840 --> 01:32:10,320
already to see what that mapping is like

01:32:08,800 --> 01:32:12,080
it's also good to use the collapse

01:32:10,320 --> 01:32:13,920
clause on that on our two

01:32:12,080 --> 01:32:15,760
uh a nested do loop so that you can

01:32:13,920 --> 01:32:18,080
increase the amount of parallelism

01:32:15,760 --> 01:32:19,920
so play with using the collapse clause

01:32:18,080 --> 01:32:21,520
and and not using the collapse clause

01:32:19,920 --> 01:32:24,000
and and see what sort of performance you

01:32:21,520 --> 01:32:25,120
get so that's the that's the goal of

01:32:24,000 --> 01:32:28,400
this exercise

01:32:25,120 --> 01:32:28,800
um do get back onto isn't bad and have a

01:32:28,400 --> 01:32:31,040
play

01:32:28,800 --> 01:32:32,320
and hopefully you're able to now start

01:32:31,040 --> 01:32:34,159
uh taking your

01:32:32,320 --> 01:32:35,440
um your code that you've started getting

01:32:34,159 --> 01:32:37,360
to run on a gpu

01:32:35,440 --> 01:32:39,760
and start optimizing it so it runs in

01:32:37,360 --> 01:32:41,840
parallel and with with improved memory

01:32:39,760 --> 01:32:44,880
transfers

01:32:41,840 --> 01:32:45,840
so there's lots of time um lots of time

01:32:44,880 --> 01:32:49,040
to do that

01:32:45,840 --> 01:32:50,719
um at about uh 25 past four

01:32:49,040 --> 01:32:52,880
i'll start uh showing you a little bit

01:32:50,719 --> 01:32:56,400
about um how i might have done that

01:32:52,880 --> 01:32:57,679
and and wrap up the end of the day um at

01:32:56,400 --> 01:33:00,480
hopper's form

01:32:57,679 --> 01:33:02,000
so we've got just over half an hour now

01:33:00,480 --> 01:33:04,239
to do this exercise so

01:33:02,000 --> 01:33:05,360
i'll i'll pick up all your questions in

01:33:04,239 --> 01:33:09,120
the q a

01:33:05,360 --> 01:33:14,400
um and i'll see you at 25 past six

01:33:09,120 --> 01:33:14,400
25 past four uk time 16 25

01:33:15,840 --> 01:33:19,600
we're about halfway through the exercise

01:33:18,480 --> 01:33:23,360
time now

01:33:19,600 --> 01:33:24,880
um around sort of 20 25 past i'll start

01:33:23,360 --> 01:33:28,000
um talking a little bit about

01:33:24,880 --> 01:33:29,360
um the solution um and then a short

01:33:28,000 --> 01:33:32,880
wrap-up before we

01:33:29,360 --> 01:33:35,360
uh finish it at 4 30. so so keep asking

01:33:32,880 --> 01:33:36,800
uh questions on the q a i'm happy to

01:33:35,360 --> 01:33:39,040
take all sorts of questions about

01:33:36,800 --> 01:33:41,920
about openmp and even any that you had

01:33:39,040 --> 01:33:43,920
from cpus this morning

01:33:41,920 --> 01:33:45,679
um use it as a place to start uh

01:33:43,920 --> 01:33:48,159
discussing things as well if

01:33:45,679 --> 01:33:50,159
if you're interested in that and i hope

01:33:48,159 --> 01:33:52,320
you're getting on okay uh with

01:33:50,159 --> 01:33:53,440
with uh parallelizing and optimizing the

01:33:52,320 --> 01:33:57,920
data movement

01:33:53,440 --> 01:33:57,920
for the five point stencil on the gpus

01:33:58,239 --> 01:34:02,880
okay we're drawing to the end of the

01:34:01,199 --> 01:34:05,040
tutorial today

01:34:02,880 --> 01:34:06,639
and before we finish i'd just like to do

01:34:05,040 --> 01:34:09,040
a couple of things

01:34:06,639 --> 01:34:10,800
first of all i'm going to show the

01:34:09,040 --> 01:34:13,280
sample solution that's available

01:34:10,800 --> 01:34:14,239
in the github repository and then just

01:34:13,280 --> 01:34:17,360
got a couple of

01:34:14,239 --> 01:34:19,920
things to talk about openmp 5

01:34:17,360 --> 01:34:21,120
specifically related to the um target

01:34:19,920 --> 01:34:22,560
directives

01:34:21,120 --> 01:34:25,360
and then finally just talk a little bit

01:34:22,560 --> 01:34:27,679
about um tomorrow the the openmp

01:34:25,360 --> 01:34:30,719
um talks that you'll hear at the user

01:34:27,679 --> 01:34:33,040
group tomorrow

01:34:30,719 --> 01:34:35,600
so to start with let's have a look at

01:34:33,040 --> 01:34:38,400
the um

01:34:35,600 --> 01:34:39,840
at the sample solution perhaps um so

01:34:38,400 --> 01:34:43,040
this here is the

01:34:39,840 --> 01:34:47,159
um is the code um that

01:34:43,040 --> 01:34:50,080
is in the um in the stencil underscore

01:34:47,159 --> 01:34:51,920
target.f90 code in the repository

01:34:50,080 --> 01:34:53,280
um and this contains the sort of

01:34:51,920 --> 01:34:55,280
optimized version

01:34:53,280 --> 01:34:56,719
for running on the gpu so this has the

01:34:55,280 --> 01:34:57,360
memory access patterns in it in the

01:34:56,719 --> 01:34:59,280
right way

01:34:57,360 --> 01:35:02,400
uh from if you've followed us all the

01:34:59,280 --> 01:35:05,760
way from hoppers9 this morning

01:35:02,400 --> 01:35:06,400
um it has um principally if we walk

01:35:05,760 --> 01:35:09,119
through the

01:35:06,400 --> 01:35:09,920
the whole code from from the beginning

01:35:09,119 --> 01:35:12,840
once our

01:35:09,920 --> 01:35:14,639
data was initialized on the host on line

01:35:12,840 --> 01:35:16,560
39 we're going to

01:35:14,639 --> 01:35:17,679
set up the device data environment so

01:35:16,560 --> 01:35:19,520
we're going to have a target

01:35:17,679 --> 01:35:20,880
enter data and we're going to map our

01:35:19,520 --> 01:35:23,679
two arrays a

01:35:20,880 --> 01:35:25,360
and a temp to the device data

01:35:23,679 --> 01:35:28,000
environment

01:35:25,360 --> 01:35:29,679
we then have our time step loop that we

01:35:28,000 --> 01:35:32,800
iterate through a number of times

01:35:29,679 --> 01:35:34,400
and we have our target directive this is

01:35:32,800 --> 01:35:37,040
going to offload

01:35:34,400 --> 01:35:38,560
these loops here the execution of those

01:35:37,040 --> 01:35:41,119
loops onto the device

01:35:38,560 --> 01:35:43,360
note that we have a map statement here

01:35:41,119 --> 01:35:46,960
this is going to map the scalar

01:35:43,360 --> 01:35:48,480
total to and from the device every

01:35:46,960 --> 01:35:50,560
every time this target region is

01:35:48,480 --> 01:35:52,080
encountered and executed

01:35:50,560 --> 01:35:54,239
this is important because otherwise

01:35:52,080 --> 01:35:56,000
total would be mapped first private as

01:35:54,239 --> 01:35:59,119
it's a scalar

01:35:56,000 --> 01:36:01,199
then on line 50 i have a a big directive

01:35:59,119 --> 01:36:04,239
the teams distribute parallel do

01:36:01,199 --> 01:36:06,000
that's to run the the loops in parallel

01:36:04,239 --> 01:36:07,040
and spread them out over all the

01:36:06,000 --> 01:36:09,520
processing elements

01:36:07,040 --> 01:36:10,800
of the of the target device our gpu in

01:36:09,520 --> 01:36:13,280
this case

01:36:10,800 --> 01:36:14,080
i have a reduction clause with the sum

01:36:13,280 --> 01:36:15,760
operation

01:36:14,080 --> 01:36:17,360
and on the variable total this is going

01:36:15,760 --> 01:36:21,520
to be computing

01:36:17,360 --> 01:36:23,760
um a reduction over uh over over

01:36:21,520 --> 01:36:25,760
the variable total and it's going to use

01:36:23,760 --> 01:36:27,679
the plus operation to do that

01:36:25,760 --> 01:36:29,360
and then have the collapse clause and

01:36:27,679 --> 01:36:31,360
that clause is going to collapse the

01:36:29,360 --> 01:36:32,719
next two do loops the tightly nested do

01:36:31,360 --> 01:36:35,679
loops it encounters

01:36:32,719 --> 01:36:36,719
so that we can spread nx by ny

01:36:35,679 --> 01:36:39,840
parallelism across

01:36:36,719 --> 01:36:41,600
all of the gpu so with the 4000 by 4000

01:36:39,840 --> 01:36:43,199
grid that's 16 million

01:36:41,600 --> 01:36:44,719
iterations of the loop we're going to

01:36:43,199 --> 01:36:46,000
share those amongst all the processing

01:36:44,719 --> 01:36:47,760
elements of our gpu

01:36:46,000 --> 01:36:49,520
plenty of parallelism there it's going

01:36:47,760 --> 01:36:52,239
to do a reduction for us we haven't had

01:36:49,520 --> 01:36:54,880
to implement our own reduction

01:36:52,239 --> 01:36:55,760
and and copy the result back um stored

01:36:54,880 --> 01:36:57,920
in total

01:36:55,760 --> 01:37:00,000
onto the host ready to be printed out on

01:36:57,920 --> 01:37:03,360
line 61 here

01:37:00,000 --> 01:37:06,639
note on line 57 and i undo that

01:37:03,360 --> 01:37:09,040
the ordering of the the directive so i

01:37:06,639 --> 01:37:11,119
end the team's distribute parallel due

01:37:09,040 --> 01:37:12,560
note i don't need any clauses on that

01:37:11,119 --> 01:37:17,360
it's just the constructs that i

01:37:12,560 --> 01:37:19,440
end on line 58 i have the end target

01:37:17,360 --> 01:37:20,639
which ends the offload and returns

01:37:19,440 --> 01:37:23,280
execution back

01:37:20,639 --> 01:37:24,800
to the host and then swap the pointers

01:37:23,280 --> 01:37:27,440
on the host and there was a question

01:37:24,800 --> 01:37:28,080
in the q a about this so when this loop

01:37:27,440 --> 01:37:30,080
here the

01:37:28,080 --> 01:37:31,520
the time step loop goes back into the

01:37:30,080 --> 01:37:33,679
target region

01:37:31,520 --> 01:37:34,880
then this target region will reevaluate

01:37:33,679 --> 01:37:37,440
what a point

01:37:34,880 --> 01:37:38,960
or a and a10 point to and it will give

01:37:37,440 --> 01:37:41,520
the target region

01:37:38,960 --> 01:37:42,719
uh inside there then the updated values

01:37:41,520 --> 01:37:44,800
and use

01:37:42,719 --> 01:37:47,360
use those pointing the other way around

01:37:44,800 --> 01:37:48,080
so pointer swapping works exactly as

01:37:47,360 --> 01:37:51,119
we'd expect

01:37:48,080 --> 01:37:53,040
we swap on the host and and and we then

01:37:51,119 --> 01:37:54,639
uh use those pointers as if they were a

01:37:53,040 --> 01:37:57,040
sort of function call

01:37:54,639 --> 01:37:58,400
um sent through as function parameters

01:37:57,040 --> 01:38:01,199
to the to the device

01:37:58,400 --> 01:38:02,639
inside the target region at the end we

01:38:01,199 --> 01:38:05,360
then copy back

01:38:02,639 --> 01:38:06,960
the um the updated arrays in this case

01:38:05,360 --> 01:38:10,400
i've just copied everything back

01:38:06,960 --> 01:38:13,920
um so i've got this target exit data map

01:38:10,400 --> 01:38:15,600
from a and a temp and that's it the rest

01:38:13,920 --> 01:38:17,840
of the code just remains as it did

01:38:15,600 --> 01:38:19,280
in the serial version so we've come an

01:38:17,840 --> 01:38:22,080
awful long way

01:38:19,280 --> 01:38:23,280
um today we've started with serial

01:38:22,080 --> 01:38:26,080
fortran

01:38:23,280 --> 01:38:26,719
by lunchtime we had a vectorized numa

01:38:26,080 --> 01:38:29,760
aware

01:38:26,719 --> 01:38:31,920
parallel optimized code of a five point

01:38:29,760 --> 01:38:34,080
stencil running on the cpu

01:38:31,920 --> 01:38:36,080
and then this afternoon we've taken that

01:38:34,080 --> 01:38:38,000
code and if you've joined us there you

01:38:36,080 --> 01:38:39,679
you joined us on at that point and we've

01:38:38,000 --> 01:38:41,040
we've parallelized this five-point

01:38:39,679 --> 01:38:42,960
stencil code

01:38:41,040 --> 01:38:44,480
on the gpu we've got it running fast

01:38:42,960 --> 01:38:45,840
we've got it using lots and lots of

01:38:44,480 --> 01:38:48,480
parallelism with this

01:38:45,840 --> 01:38:50,239
uh with the power constructs we've

01:38:48,480 --> 01:38:51,920
thought carefully about the movement of

01:38:50,239 --> 01:38:53,600
data between the host and the device and

01:38:51,920 --> 01:38:54,800
we've been using the device data

01:38:53,600 --> 01:38:56,560
environment

01:38:54,800 --> 01:38:58,239
and updating it and manipulating it with

01:38:56,560 --> 01:39:00,719
this with these target and

01:38:58,239 --> 01:39:02,719
exit data constructs so that's brilliant

01:39:00,719 --> 01:39:05,679
we've come an awful long way today so

01:39:02,719 --> 01:39:07,199
thanks a lot for for joining us and um

01:39:05,679 --> 01:39:08,639
for sticking with us all this time and

01:39:07,199 --> 01:39:10,560
you've done brilliantly well

01:39:08,639 --> 01:39:13,520
um and there's been a lot of information

01:39:10,560 --> 01:39:15,040
here this is actually most of the openmp

01:39:13,520 --> 01:39:18,000
files specification

01:39:15,040 --> 01:39:19,679
the biggest admission to of to openmp

01:39:18,000 --> 01:39:21,119
that i haven't really talked about today

01:39:19,679 --> 01:39:22,719
is tasking

01:39:21,119 --> 01:39:24,719
and so if you're interested in that

01:39:22,719 --> 01:39:26,159
there's a few slides that i've shared

01:39:24,719 --> 01:39:28,480
in the repository that you can have a

01:39:26,159 --> 01:39:30,239
look at about that and there are some

01:39:28,480 --> 01:39:32,239
some some good books which i'm going to

01:39:30,239 --> 01:39:35,199
talk to in a moment as well

01:39:32,239 --> 01:39:36,480
so so it's brilliant that you've all

01:39:35,199 --> 01:39:38,560
stuck with it this far and

01:39:36,480 --> 01:39:40,480
and it's great to see you all learning

01:39:38,560 --> 01:39:43,040
to use openmp to write

01:39:40,480 --> 01:39:44,400
fast parallel codes on on cpus and on

01:39:43,040 --> 01:39:47,520
gpus so

01:39:44,400 --> 01:39:47,520
so that's been brilliant

01:39:48,080 --> 01:39:51,600
so to finish up i'd just like to talk a

01:39:50,960 --> 01:39:54,800
little bit

01:39:51,600 --> 01:39:56,480
about five

01:39:54,800 --> 01:39:58,400
and a few things that are that are

01:39:56,480 --> 01:39:59,040
coming in there so i'll just get those

01:39:58,400 --> 01:40:01,280
slides

01:39:59,040 --> 01:40:01,280
up

01:40:05,600 --> 01:40:09,840
um here we go

01:40:11,119 --> 01:40:16,800
so openmp5 is is kind of an evolution of

01:40:14,719 --> 01:40:18,960
some things that are in openmp 4.5 that

01:40:16,800 --> 01:40:21,040
we've been uh doing today

01:40:18,960 --> 01:40:22,480
and those things really help making

01:40:21,040 --> 01:40:24,080
writing these performance portable

01:40:22,480 --> 01:40:26,000
programs a little bit simpler

01:40:24,080 --> 01:40:27,440
and specifically around target there's a

01:40:26,000 --> 01:40:29,600
few things i'd like to

01:40:27,440 --> 01:40:30,639
call out and and on this slide there are

01:40:29,600 --> 01:40:33,600
those

01:40:30,639 --> 01:40:35,520
main things so there's some things like

01:40:33,600 --> 01:40:36,480
the loop construct that helps us express

01:40:35,520 --> 01:40:39,360
parallelism

01:40:36,480 --> 01:40:39,840
in a more in a more character friendly

01:40:39,360 --> 01:40:42,159
way

01:40:39,840 --> 01:40:43,600
just in fewer characters there's things

01:40:42,159 --> 01:40:44,080
like mappers which are really useful for

01:40:43,600 --> 01:40:46,320
c

01:40:44,080 --> 01:40:49,040
plus and derived data types there's

01:40:46,320 --> 01:40:51,199
unified shared memory which helps us

01:40:49,040 --> 01:40:52,159
initially start porting codes to the gpu

01:40:51,199 --> 01:40:55,360
we leverage

01:40:52,159 --> 01:40:57,280
hardware support for moving data between

01:40:55,360 --> 01:40:59,199
the host and the device so we can remove

01:40:57,280 --> 01:41:02,239
all our map clauses at the expense of

01:40:59,199 --> 01:41:03,920
having to use unified shared memory

01:41:02,239 --> 01:41:05,600
there's also things like reverse offload

01:41:03,920 --> 01:41:08,320
for offloading back to the host

01:41:05,600 --> 01:41:10,239
and also this omp target offload

01:41:08,320 --> 01:41:12,800
environment variable which

01:41:10,239 --> 01:41:13,520
is useful to ensure the behavior is such

01:41:12,800 --> 01:41:16,960
that

01:41:13,520 --> 01:41:17,840
um we make sure that the the device

01:41:16,960 --> 01:41:19,760
isn't allowed

01:41:17,840 --> 01:41:21,920
isn't allowed to silently just revert

01:41:19,760 --> 01:41:23,360
back to the host for instance

01:41:21,920 --> 01:41:26,159
the other kind of key information is

01:41:23,360 --> 01:41:27,040
that in 5.0 implementations reduction

01:41:26,159 --> 01:41:29,280
variables

01:41:27,040 --> 01:41:31,199
are implicitly mapped to from so we

01:41:29,280 --> 01:41:35,360
don't have to remember to put that

01:41:31,199 --> 01:41:37,600
on the target directives ourselves

01:41:35,360 --> 01:41:39,840
so let's talk a little bit about loop

01:41:37,600 --> 01:41:41,280
omp loop is a is a construct that's been

01:41:39,840 --> 01:41:43,600
added in openmp5

01:41:41,280 --> 01:41:44,320
and it says that the iterations of the

01:41:43,600 --> 01:41:47,280
loop may

01:41:44,320 --> 01:41:48,000
execute in any order they like including

01:41:47,280 --> 01:41:51,199
um

01:41:48,000 --> 01:41:52,960
being executed concurrently so

01:41:51,199 --> 01:41:54,719
this allows our compiler to figure out

01:41:52,960 --> 01:41:55,440
how best to utilize the parallel

01:41:54,719 --> 01:41:58,639
resources

01:41:55,440 --> 01:42:00,320
this is somewhat similar to just using a

01:41:58,639 --> 01:42:01,920
target teams distribute parallel do

01:42:00,320 --> 01:42:03,840
cindy but rather than

01:42:01,920 --> 01:42:05,119
expressing it in that many characters we

01:42:03,840 --> 01:42:08,719
can just express it

01:42:05,119 --> 01:42:10,800
with four with with loop so

01:42:08,719 --> 01:42:11,840
for the target i might have a omb target

01:42:10,800 --> 01:42:14,480
and an omp loop

01:42:11,840 --> 01:42:16,080
and decorate the loop there with that

01:42:14,480 --> 01:42:19,119
and it's just going to run that loop

01:42:16,080 --> 01:42:19,840
hopefully in parallel on the target

01:42:19,119 --> 01:42:22,080
device

01:42:19,840 --> 01:42:24,880
so that's a a convenient addition to

01:42:22,080 --> 01:42:26,960
openmp5

01:42:24,880 --> 01:42:28,320
there's also unified shared memory and

01:42:26,960 --> 01:42:31,440
this

01:42:28,320 --> 01:42:34,719
this is is leveraged with this requires

01:42:31,440 --> 01:42:38,560
construct that's been added to openmp

01:42:34,719 --> 01:42:41,679
this um this allows us as developers to

01:42:38,560 --> 01:42:44,800
make um make a statement that

01:42:41,679 --> 01:42:47,440
we require a particular feature of the

01:42:44,800 --> 01:42:49,040
openmp uh specification in order to

01:42:47,440 --> 01:42:51,040
guarantee that the code that we've

01:42:49,040 --> 01:42:53,600
written is going to be correct

01:42:51,040 --> 01:42:54,159
so for here say i want to allocate an

01:42:53,600 --> 01:42:56,719
array

01:42:54,159 --> 01:42:58,239
a and i allocate it on the host if i say

01:42:56,719 --> 01:43:00,639
that my program is

01:42:58,239 --> 01:43:03,360
requires unified shared memory this

01:43:00,639 --> 01:43:05,679
means that the next bits that follow

01:43:03,360 --> 01:43:06,639
mean that i need to use that assumption

01:43:05,679 --> 01:43:09,119
that is that

01:43:06,639 --> 01:43:09,760
memory can be shared at the page level

01:43:09,119 --> 01:43:11,600
between

01:43:09,760 --> 01:43:13,119
host and the devices and there is

01:43:11,600 --> 01:43:15,520
hardware support there

01:43:13,119 --> 01:43:18,000
to in order to move pages on demand as

01:43:15,520 --> 01:43:19,760
memory is accessed

01:43:18,000 --> 01:43:21,600
so this means this array that's been

01:43:19,760 --> 01:43:24,320
allocated on the heap

01:43:21,600 --> 01:43:26,480
is allowed to just simply be used on the

01:43:24,320 --> 01:43:27,840
device as long as i'm requiring the use

01:43:26,480 --> 01:43:29,600
of unified shared memory

01:43:27,840 --> 01:43:31,600
so notice that i can have a target

01:43:29,600 --> 01:43:33,280
region that does something with a

01:43:31,600 --> 01:43:34,719
either in a sort of simple subroutine or

01:43:33,280 --> 01:43:36,639
something

01:43:34,719 --> 01:43:38,639
but i haven't had to map so there's no

01:43:36,639 --> 01:43:40,560
map clauses here and data is going to be

01:43:38,639 --> 01:43:42,880
shared between the host and the device

01:43:40,560 --> 01:43:44,480
now if you have modern enough hardware

01:43:42,880 --> 01:43:46,480
um that can support unified shared

01:43:44,480 --> 01:43:49,040
memory this is a good way for getting a

01:43:46,480 --> 01:43:49,679
large legacy application running on the

01:43:49,040 --> 01:43:51,920
gpu

01:43:49,679 --> 01:43:53,920
and then later we can then start

01:43:51,920 --> 01:43:56,000
optimizing those memory transfers by

01:43:53,920 --> 01:43:57,840
using map clauses and setting up the

01:43:56,000 --> 01:44:01,520
device data environment ourselves but

01:43:57,840 --> 01:44:02,960
it's a useful way to get started

01:44:01,520 --> 01:44:04,560
so with that we come to the end of sort

01:44:02,960 --> 01:44:05,360
of the main content of the of the

01:44:04,560 --> 01:44:06,800
tutorial

01:44:05,360 --> 01:44:08,320
there's two really brilliant books on

01:44:06,800 --> 01:44:09,440
openmp that have been released in the

01:44:08,320 --> 01:44:12,320
last few years

01:44:09,440 --> 01:44:12,880
both from mit press the first one is

01:44:12,320 --> 01:44:15,040
called

01:44:12,880 --> 01:44:16,239
the openmp common core making openmp

01:44:15,040 --> 01:44:18,719
simple again

01:44:16,239 --> 01:44:20,080
by tim and helen and alice that's a

01:44:18,719 --> 01:44:22,320
really great introduction

01:44:20,080 --> 01:44:23,119
to the the kind of the key parts of

01:44:22,320 --> 01:44:24,639
openmp

01:44:23,119 --> 01:44:26,480
many of which we've talked about this

01:44:24,639 --> 01:44:28,000
morning so if there's more information

01:44:26,480 --> 01:44:30,480
you'd like to dig more into that i can

01:44:28,000 --> 01:44:32,000
highly recommend that book

01:44:30,480 --> 01:44:33,360
then moving into the more advanced

01:44:32,000 --> 01:44:34,719
things some of which we talked about

01:44:33,360 --> 01:44:37,600
this morning things about

01:44:34,719 --> 01:44:38,800
non-uniform non-unified memory access

01:44:37,600 --> 01:44:41,520
pneuma

01:44:38,800 --> 01:44:43,440
non-uniform memory access pneuma simdee

01:44:41,520 --> 01:44:43,760
maybe some hybrid programming and also

01:44:43,440 --> 01:44:46,159
some

01:44:43,760 --> 01:44:46,880
some information about programming gpus

01:44:46,159 --> 01:44:50,800
there's this

01:44:46,880 --> 01:44:52,880
uh using openmp the next steps book

01:44:50,800 --> 01:44:54,560
also from mit press from rude eric and

01:44:52,880 --> 01:44:55,920
christian again i'd highly recommend

01:44:54,560 --> 01:44:58,000
that

01:44:55,920 --> 01:45:00,320
the openmp website is a great place to

01:44:58,000 --> 01:45:02,480
also learn more about openmp and to find

01:45:00,320 --> 01:45:04,560
links to some support forums

01:45:02,480 --> 01:45:06,159
and and things like that you can

01:45:04,560 --> 01:45:09,280
download the specification

01:45:06,159 --> 01:45:10,960
now the openmp specification um its

01:45:09,280 --> 01:45:13,199
intended audience is

01:45:10,960 --> 01:45:15,199
um is for the implementers of the

01:45:13,199 --> 01:45:16,639
standard so people that write compilers

01:45:15,199 --> 01:45:18,639
and people that write runtime so it's

01:45:16,639 --> 01:45:20,960
not really for the faint-hearted

01:45:18,639 --> 01:45:22,560
but it's not totally unreadable so if

01:45:20,960 --> 01:45:24,880
you are really interested in in the kind

01:45:22,560 --> 01:45:27,119
of very low level details of of all the

01:45:24,880 --> 01:45:28,639
rules and regulations about

01:45:27,119 --> 01:45:30,480
what each of the constructs mean the

01:45:28,639 --> 01:45:32,239
specification is is worth looking at

01:45:30,480 --> 01:45:34,320
there's also summary cards which someone

01:45:32,239 --> 01:45:36,960
very helpfully uh posted a link in the q

01:45:34,320 --> 01:45:38,639
a for us this morning there's that list

01:45:36,960 --> 01:45:41,119
of the compiler support

01:45:38,639 --> 01:45:43,280
along with um along with other tools and

01:45:41,119 --> 01:45:46,800
things that vendors provide

01:45:43,280 --> 01:45:48,719
there's also the um advanced code

01:45:46,800 --> 01:45:50,080
the example code which goes along with

01:45:48,719 --> 01:45:51,920
the specification

01:45:50,080 --> 01:45:53,679
to provide examples of how to use all

01:45:51,920 --> 01:45:54,800
the different direct

01:45:53,679 --> 01:45:56,560
all the different directives that's a

01:45:54,800 --> 01:45:58,239
very useful pdf document there are

01:45:56,560 --> 01:46:00,800
examples there in c and

01:45:58,239 --> 01:46:02,000
and fortran of all the directives in the

01:46:00,800 --> 01:46:03,360
standard

01:46:02,000 --> 01:46:05,280
finally there's an update list of all

01:46:03,360 --> 01:46:07,520
the books as well so the openmp

01:46:05,280 --> 01:46:09,760
is a website is a really great kind of

01:46:07,520 --> 01:46:12,480
place to go find out and as a launch pad

01:46:09,760 --> 01:46:14,000
to lots of openmp

01:46:12,480 --> 01:46:15,760
now before we finish i'd like to just

01:46:14,000 --> 01:46:17,040
talk a little bit about tomorrow this is

01:46:15,760 --> 01:46:18,880
the openmp

01:46:17,040 --> 01:46:21,040
users group and tomorrow we're going to

01:46:18,880 --> 01:46:24,239
be continuing and have a

01:46:21,040 --> 01:46:27,600
lots of really interesting talks from

01:46:24,239 --> 01:46:29,840
um lots of different people

01:46:27,600 --> 01:46:30,880
so i'm just going to share the agenda

01:46:29,840 --> 01:46:33,679
with you now

01:46:30,880 --> 01:46:35,440
hopefully you can see that um so we're

01:46:33,679 --> 01:46:37,119
going to start off at a half past one

01:46:35,440 --> 01:46:39,040
tomorrow afternoon that's uk

01:46:37,119 --> 01:46:40,320
time and there's lots of really great

01:46:39,040 --> 01:46:42,960
talks

01:46:40,320 --> 01:46:43,840
all afternoon in particular for those of

01:46:42,960 --> 01:46:45,920
you that have joined

01:46:43,840 --> 01:46:48,400
this afternoon and are interested in

01:46:45,920 --> 01:46:51,920
programming gpus with openmp

01:46:48,400 --> 01:46:54,239
um at 3 20 15 20 we've got a talk from

01:46:51,920 --> 01:46:56,800
nvidia who's going to be talking about

01:46:54,239 --> 01:46:58,560
best practices for using openmp on and

01:46:56,800 --> 01:47:00,000
with your gpus but as you can see the

01:46:58,560 --> 01:47:02,080
whole talk is the whole

01:47:00,000 --> 01:47:03,360
agenda looks really exciting lots of

01:47:02,080 --> 01:47:06,560
great

01:47:03,360 --> 01:47:06,880
talks about openmp from the vendors and

01:47:06,560 --> 01:47:10,159
from

01:47:06,880 --> 01:47:12,000
users and from universities and and um

01:47:10,159 --> 01:47:13,280
at big big clouds like the met office

01:47:12,000 --> 01:47:16,000
where they um

01:47:13,280 --> 01:47:17,280
are using openmp4 for big simulations

01:47:16,000 --> 01:47:18,719
for real

01:47:17,280 --> 01:47:21,440
uh the afternoon is going to be a really

01:47:18,719 --> 01:47:24,159
interesting panel discussion chaired by

01:47:21,440 --> 01:47:24,880
jim county and we'll be able to have a

01:47:24,159 --> 01:47:26,800
good

01:47:24,880 --> 01:47:28,880
good amount of time there talking about

01:47:26,800 --> 01:47:31,600
um openmp

01:47:28,880 --> 01:47:32,000
so thanks everybody for joining um tim

01:47:31,600 --> 01:47:34,960
is

01:47:32,000 --> 01:47:36,560
who's been fantastic at organizing the

01:47:34,960 --> 01:47:38,560
openmp user group

01:47:36,560 --> 01:47:40,560
this year he's going to be reaching out

01:47:38,560 --> 01:47:42,719
to you all with uh with a quick survey

01:47:40,560 --> 01:47:43,920
uh about uh for asking you for feedback

01:47:42,719 --> 01:47:45,520
about the tutorial today

01:47:43,920 --> 01:47:47,119
i'd really appreciate it if you filled

01:47:45,520 --> 01:47:49,280
that in and gave some

01:47:47,119 --> 01:47:51,199
feedback to me so that we can hopefully

01:47:49,280 --> 01:47:53,280
run this in the future and

01:47:51,199 --> 01:47:55,040
in an improved form and we do take all

01:47:53,280 --> 01:47:57,119
the feedback really seriously so

01:47:55,040 --> 01:47:58,960
thanks everybody for for joining today i

01:47:57,119 --> 01:48:00,719
hope you've enjoyed it

01:47:58,960 --> 01:48:02,080
fill in the feedback and we'll see you

01:48:00,719 --> 01:48:04,960
tomorrow afternoon

01:48:02,080 --> 01:48:09,679
um for the rest of the openmp user group

01:48:04,960 --> 01:48:09,679

YouTube URL: https://www.youtube.com/watch?v=hI8U9HD9moM


