Title: Automatic Generation of OpenMP for Weather and Climate Applications
Publication date: 2020-12-21
Playlist: European OpenMP Users Conference 2020
Description: 
	This talk was presented at the 3rd European OpenMP Users Conference in 2020

Presented by : Chris Maynard, Met Office

Conference Website: https://openmpusers.org
Follow us: https://twitter.com/openmp_users

Presentation Abstract :
The Met Office is developing a new weather and climate model to exploit exascale computing. This new model, named LFRic, is being written using a Domain Specific Language API and this allows a Domain specific compiler, PSyclone, to generate parallel code for different programming models, such as OpenMP. The DSL approach allows the science developer to be productive by writing code that looks “like the maths”. The code generation then allows the model to be portable and by exploiting different programming models. How OpenMP is being used for CPU shared memory parallelism and how accelerators and other architectures can be targeted will be described. An analysis of performance presented.
Captions: 
	00:00:04,240 --> 00:00:07,520
so we have a busy agenda today

00:00:06,319 --> 00:00:10,400
and we're going to start with with a

00:00:07,520 --> 00:00:12,080
great talk by chris maynard from the met

00:00:10,400 --> 00:00:15,440
office he's a member of the

00:00:12,080 --> 00:00:16,160
hpc optimization team and then we'll go

00:00:15,440 --> 00:00:17,520
into

00:00:16,160 --> 00:00:19,199
um other talks from you and us and

00:00:17,520 --> 00:00:20,880
others as we go and i'll introduce them

00:00:19,199 --> 00:00:22,480
as we go through

00:00:20,880 --> 00:00:24,640
so for now we'll move straight into our

00:00:22,480 --> 00:00:25,039
into our first talk with with chris if

00:00:24,640 --> 00:00:27,680
he's

00:00:25,039 --> 00:00:27,680
if he's here

00:00:28,640 --> 00:00:31,920
yep thank you for the introduction i'll

00:00:30,640 --> 00:00:34,719
uh if i

00:00:31,920 --> 00:00:35,840
click share screen i should get yeah i

00:00:34,719 --> 00:00:38,239
think great

00:00:35,840 --> 00:00:40,000
ever you're all seeing that um yeah so

00:00:38,239 --> 00:00:41,680
hello thank you very much for the

00:00:40,000 --> 00:00:43,200
uh introduction and thank you for

00:00:41,680 --> 00:00:45,920
inviting me to speak

00:00:43,200 --> 00:00:46,480
so yes i'm christopher maynard i

00:00:45,920 --> 00:00:48,719
currently

00:00:46,480 --> 00:00:51,039
i work at the met office i'm currently

00:00:48,719 --> 00:00:53,199
the acting hbc optimization

00:00:51,039 --> 00:00:54,640
manager i also have a position at the

00:00:53,199 --> 00:00:55,760
university of reading and computer

00:00:54,640 --> 00:00:58,719
science

00:00:55,760 --> 00:01:00,160
and i'm going to talk today on automatic

00:00:58,719 --> 00:01:04,479
generation of openmp

00:01:00,160 --> 00:01:06,479
for weather and climate applications

00:01:04,479 --> 00:01:08,080
by way of introduction then the met

00:01:06,479 --> 00:01:10,840
office is developing

00:01:08,080 --> 00:01:12,400
a new modeling system for exascale

00:01:10,840 --> 00:01:15,600
computing

00:01:12,400 --> 00:01:17,360
we're replacing a successful something

00:01:15,600 --> 00:01:18,880
called the unified model

00:01:17,360 --> 00:01:20,479
which does both weather modeling and

00:01:18,880 --> 00:01:23,520
climate modeling

00:01:20,479 --> 00:01:25,040
with a new model called elfrick

00:01:23,520 --> 00:01:26,640
after lewis phi richardson the first

00:01:25,040 --> 00:01:29,360
person to try a numerical

00:01:26,640 --> 00:01:32,799
weather prediction and we have a new

00:01:29,360 --> 00:01:34,000
dynamical core which is called gung-ho

00:01:32,799 --> 00:01:36,720
and the reason we're doing this is

00:01:34,000 --> 00:01:38,560
because um

00:01:36,720 --> 00:01:41,280
our current model in the unified model

00:01:38,560 --> 00:01:44,560
is on a longitude latitude grid

00:01:41,280 --> 00:01:45,680
um it's a structured mesh but we have

00:01:44,560 --> 00:01:47,759
poles

00:01:45,680 --> 00:01:49,759
and as you can see the the these lines

00:01:47,759 --> 00:01:51,840
converge as they get close to the poles

00:01:49,759 --> 00:01:54,880
and that causes us a problem if our uh

00:01:51,840 --> 00:01:57,920
discretization is based on this grid

00:01:54,880 --> 00:02:00,640
because we have to do

00:01:57,920 --> 00:02:02,079
lots of communication around the poles

00:02:00,640 --> 00:02:04,079
and that's basically

00:02:02,079 --> 00:02:06,479
is going to stop us scaling so well as

00:02:04,079 --> 00:02:08,959
we would like

00:02:06,479 --> 00:02:10,239
it's a finite difference code it's also

00:02:08,959 --> 00:02:15,040
a semi-implicit

00:02:10,239 --> 00:02:17,120
um numerical integration in in time

00:02:15,040 --> 00:02:18,480
but it's actually a very good model it's

00:02:17,120 --> 00:02:20,000
scientifically very good and it

00:02:18,480 --> 00:02:22,480
does have many good computational

00:02:20,000 --> 00:02:23,520
aspects so in moving to the new model

00:02:22,480 --> 00:02:25,440
what we want to be able to do is

00:02:23,520 --> 00:02:27,360
maintain the accuracy

00:02:25,440 --> 00:02:28,560
that we that we got by using this

00:02:27,360 --> 00:02:29,920
orthogonal uh

00:02:28,560 --> 00:02:31,920
orthogonal coordinate system and

00:02:29,920 --> 00:02:32,959
orthogonal mesh here we want to maintain

00:02:31,920 --> 00:02:35,120
the accuracy

00:02:32,959 --> 00:02:36,400
but improve the scalability and also the

00:02:35,120 --> 00:02:39,519
ability to exploit

00:02:36,400 --> 00:02:40,959
other programming models so

00:02:39,519 --> 00:02:43,040
the new model is based on something

00:02:40,959 --> 00:02:46,640
called a cube sphere mesh which is shown

00:02:43,040 --> 00:02:47,280
on the on the right over here this is a

00:02:46,640 --> 00:02:49,280
and we're using

00:02:47,280 --> 00:02:51,440
an unstructured representation of this

00:02:49,280 --> 00:02:53,040
mesh

00:02:51,440 --> 00:02:54,800
in order to maintain the accuracy that

00:02:53,040 --> 00:02:56,879
we got from this model we're using a

00:02:54,800 --> 00:02:59,519
mixed finite element method

00:02:56,879 --> 00:03:05,840
but keeping the semi-implicit time

00:02:59,519 --> 00:03:05,840
integration scheme

00:03:06,879 --> 00:03:10,319
there are several things that are quite

00:03:08,400 --> 00:03:13,519
uh that are specific to

00:03:10,319 --> 00:03:15,200
a weather and climate model

00:03:13,519 --> 00:03:17,840
we are going to solve fluid dynamics but

00:03:15,200 --> 00:03:20,239
we're going to do it in a special way

00:03:17,840 --> 00:03:22,000
because of the geometry the obvious

00:03:20,239 --> 00:03:23,760
thing being it's spherical or at least

00:03:22,000 --> 00:03:25,519
very near spherical

00:03:23,760 --> 00:03:27,599
but not only that there is orography

00:03:25,519 --> 00:03:28,239
there's mountains uh bits of rock poke

00:03:27,599 --> 00:03:31,519
up

00:03:28,239 --> 00:03:32,000
quite high into the atmosphere the other

00:03:31,519 --> 00:03:34,560
really

00:03:32,000 --> 00:03:36,560
uh striking thing about this particular

00:03:34,560 --> 00:03:39,120
domain is the atmosphere is thin

00:03:36,560 --> 00:03:40,879
and vertically stratified there's

00:03:39,120 --> 00:03:41,200
something called the command line which

00:03:40,879 --> 00:03:43,920
is

00:03:41,200 --> 00:03:45,840
maybe a hundred kilometers up uh into

00:03:43,920 --> 00:03:48,319
the atmosphere

00:03:45,840 --> 00:03:49,840
below that line almost all the

00:03:48,319 --> 00:03:51,760
atmosphere by mass

00:03:49,840 --> 00:03:53,519
is is below that line and above that

00:03:51,760 --> 00:03:56,319
line perhaps it doesn't really behave

00:03:53,519 --> 00:03:58,720
much like a gas anymore anyway

00:03:56,319 --> 00:03:59,519
but it's very thin so the diagram on the

00:03:58,720 --> 00:04:02,560
left here

00:03:59,519 --> 00:04:04,560
is joined to scale with this atmosphere

00:04:02,560 --> 00:04:06,959
being 600 kilometers deep

00:04:04,560 --> 00:04:07,760
if i drew it 100 kilometers deep to

00:04:06,959 --> 00:04:11,360
scale

00:04:07,760 --> 00:04:13,200
then it's basically just a thin line

00:04:11,360 --> 00:04:14,879
but the basic point is the radius of the

00:04:13,200 --> 00:04:18,000
earth is much much greater than the

00:04:14,879 --> 00:04:18,000
depth of the atmosphere

00:04:19,680 --> 00:04:23,680
it also rotates it's not in thermal

00:04:22,320 --> 00:04:26,160
equilibrium

00:04:23,680 --> 00:04:28,080
there's an atmosphere there's an ocean

00:04:26,160 --> 00:04:30,320
there's ice and sea ice and there's

00:04:28,080 --> 00:04:32,400
a land surface and there's moist

00:04:30,320 --> 00:04:35,280
chemical and biological processes that

00:04:32,400 --> 00:04:37,600
affect both the weather and the climate

00:04:35,280 --> 00:04:39,199
so it's very complex domain to model

00:04:37,600 --> 00:04:41,440
it's multi-component and it's

00:04:39,199 --> 00:04:44,160
multi-scale

00:04:41,440 --> 00:04:46,479
models themselves tend to be large i've

00:04:44,160 --> 00:04:48,400
ordered a million lines of code

00:04:46,479 --> 00:04:50,000
certainly the um is at least over half a

00:04:48,400 --> 00:04:52,320
million lines of code

00:04:50,000 --> 00:04:53,919
uh legacy they can take 10 or more years

00:04:52,320 --> 00:04:54,560
to develop and they can last the

00:04:53,919 --> 00:04:58,000
lifetime

00:04:54,560 --> 00:04:59,680
of the model maybe 25 years or more

00:04:58,000 --> 00:05:02,400
but they do undergo on continuous

00:04:59,680 --> 00:05:05,199
scientific development

00:05:02,400 --> 00:05:06,479
but they need to run operations and

00:05:05,199 --> 00:05:08,000
production so we have

00:05:06,479 --> 00:05:10,479
naturally very conservative about

00:05:08,000 --> 00:05:14,000
changing those to keep for correctness

00:05:10,479 --> 00:05:14,000
this is just scientifically prudent

00:05:16,160 --> 00:05:19,840
so how do we how do we what how do we

00:05:18,880 --> 00:05:21,360
create such a model

00:05:19,840 --> 00:05:23,120
what we how we program what programming

00:05:21,360 --> 00:05:26,479
model would we use

00:05:23,120 --> 00:05:27,199
so uh fortran is is um is our language

00:05:26,479 --> 00:05:29,360
of choice

00:05:27,199 --> 00:05:30,800
it's a high-level language um you've all

00:05:29,360 --> 00:05:33,440
heard of fortran right

00:05:30,800 --> 00:05:36,400
here here is some fortran but it's an

00:05:33,440 --> 00:05:38,160
abstraction of the numerical mathematics

00:05:36,400 --> 00:05:40,000
the implementation and the computer

00:05:38,160 --> 00:05:42,080
architecture is hidden

00:05:40,000 --> 00:05:43,680
and we just have we have code which is

00:05:42,080 --> 00:05:44,479
just some text which conforms to the

00:05:43,680 --> 00:05:46,880
semantics

00:05:44,479 --> 00:05:48,080
and syntax of the language definition

00:05:46,880 --> 00:05:49,360
and then we have another computer

00:05:48,080 --> 00:05:51,680
program the compiler

00:05:49,360 --> 00:05:55,520
which transforms that code into machine

00:05:51,680 --> 00:05:55,520
code for specific processes

00:05:59,600 --> 00:06:03,680
this it gives us a nice separation of

00:06:01,919 --> 00:06:06,400
concerns between on the one hand

00:06:03,680 --> 00:06:07,120
the numerical mathematics and on the

00:06:06,400 --> 00:06:08,800
other hand

00:06:07,120 --> 00:06:11,440
the implementation and computer

00:06:08,800 --> 00:06:11,440
architecture

00:06:11,759 --> 00:06:14,960
unfortunately we then break this

00:06:13,360 --> 00:06:16,800
abstraction by all the parallel

00:06:14,960 --> 00:06:18,319
performance and memory features we have

00:06:16,800 --> 00:06:20,080
to expose and then we literally hack

00:06:18,319 --> 00:06:23,919
them back together again with

00:06:20,080 --> 00:06:24,560
mpi openmp open acc opencl cuda p gas

00:06:23,919 --> 00:06:26,560
sim d

00:06:24,560 --> 00:06:29,840
compiler directors might use carcass

00:06:26,560 --> 00:06:31,520
because now sickles the latest thing

00:06:29,840 --> 00:06:32,800
not only are there a whole list of

00:06:31,520 --> 00:06:34,080
different things they're different types

00:06:32,800 --> 00:06:36,479
of things they're libraries

00:06:34,080 --> 00:06:38,000
language extensions directives compiler

00:06:36,479 --> 00:06:41,440
specific directives or

00:06:38,000 --> 00:06:43,360
or language frameworks

00:06:41,440 --> 00:06:45,680
it's all a bit of a dog's dinner here's

00:06:43,360 --> 00:06:47,199
the nice api i wanted

00:06:45,680 --> 00:06:48,680
to be able to write some code that

00:06:47,199 --> 00:06:50,080
basically looks like the numerical

00:06:48,680 --> 00:06:52,400
mathematics

00:06:50,080 --> 00:06:53,680
and here's the api i've actually got and

00:06:52,400 --> 00:06:55,039
a problem and it's worse than that

00:06:53,680 --> 00:06:55,520
because with this mixed programming

00:06:55,039 --> 00:06:57,280
model

00:06:55,520 --> 00:06:58,560
on if i don't know what happens if i set

00:06:57,280 --> 00:07:00,479
this down to three

00:06:58,560 --> 00:07:02,240
and then flick this switch over here at

00:07:00,479 --> 00:07:02,800
the same time because they how do these

00:07:02,240 --> 00:07:04,240
models

00:07:02,800 --> 00:07:06,400
programming models interact with each

00:07:04,240 --> 00:07:09,759
other

00:07:06,400 --> 00:07:09,759
so this is a bit of a problem

00:07:10,880 --> 00:07:14,880
to try and get around this problem the

00:07:12,960 --> 00:07:16,400
met office is taking a domain-specific

00:07:14,880 --> 00:07:18,960
language approach

00:07:16,400 --> 00:07:20,560
to developing a new model why well

00:07:18,960 --> 00:07:21,280
there's basically too many programming

00:07:20,560 --> 00:07:23,360
models

00:07:21,280 --> 00:07:24,880
there's a rapid evolution of diverse

00:07:23,360 --> 00:07:26,720
computer hardware

00:07:24,880 --> 00:07:28,560
and we end up with parallel opt-in

00:07:26,720 --> 00:07:30,560
optimization code polluting the science

00:07:28,560 --> 00:07:31,759
code and so we destroy the separation of

00:07:30,560 --> 00:07:33,120
concerns

00:07:31,759 --> 00:07:35,039
what we want to do is try and restore

00:07:33,120 --> 00:07:35,599
the separation of current concerns

00:07:35,039 --> 00:07:37,840
between

00:07:35,599 --> 00:07:39,199
the numerical mathematics and the

00:07:37,840 --> 00:07:41,840
computer architecture

00:07:39,199 --> 00:07:44,560
using a layered software approach which

00:07:41,840 --> 00:07:45,599
we see here on a model on the right so

00:07:44,560 --> 00:07:48,000
we have

00:07:45,599 --> 00:07:49,520
a high level disc code which is the

00:07:48,000 --> 00:07:51,919
algorithm code which is just the

00:07:49,520 --> 00:07:54,160
mathematical algorithm

00:07:51,919 --> 00:07:55,520
at the lowest level we have individual

00:07:54,160 --> 00:07:58,080
kernels which are mathematical

00:07:55,520 --> 00:08:00,479
operations on

00:07:58,080 --> 00:08:02,400
on sections of the data and in between

00:08:00,479 --> 00:08:05,680
we have the parallelization system

00:08:02,400 --> 00:08:11,199
which contains the um

00:08:05,680 --> 00:08:14,240
the parallel code and all the looping

00:08:11,199 --> 00:08:15,919
so we embed uh this in fortran in fact

00:08:14,240 --> 00:08:18,240
we're going to use fortran 2003

00:08:15,919 --> 00:08:20,720
object-oriented api to control the api

00:08:18,240 --> 00:08:23,199
between these layers

00:08:20,720 --> 00:08:25,120
and if you write the code to that api

00:08:23,199 --> 00:08:25,440
then we can use another application

00:08:25,120 --> 00:08:28,400
called

00:08:25,440 --> 00:08:28,960
cyclone which is a python application to

00:08:28,400 --> 00:08:31,759
parse

00:08:28,960 --> 00:08:32,560
transform and generate the so the side

00:08:31,759 --> 00:08:34,159
layer

00:08:32,560 --> 00:08:37,039
and it has its own intermediate

00:08:34,159 --> 00:08:37,039
representation

00:08:38,800 --> 00:08:42,000
so a little bit more about cyclone so

00:08:41,200 --> 00:08:43,919
it's

00:08:42,000 --> 00:08:45,600
it's mostly written by our colleagues at

00:08:43,919 --> 00:08:48,480
stfc hatchery center

00:08:45,600 --> 00:08:49,920
so that's rupert ford andy porter and so

00:08:48,480 --> 00:08:51,200
you see so i believe there's some other

00:08:49,920 --> 00:08:51,760
people now working on the project as

00:08:51,200 --> 00:08:54,240
well

00:08:51,760 --> 00:08:56,000
um and there's some collaborators from

00:08:54,240 --> 00:08:56,880
the bureau australian bureau of

00:08:56,000 --> 00:08:58,320
meteorology

00:08:56,880 --> 00:09:01,680
and in fact there's contributors for the

00:08:58,320 --> 00:09:03,440
met office as well including myself

00:09:01,680 --> 00:09:05,440
so we have algorithm code and kernel

00:09:03,440 --> 00:09:06,399
code we have a parser that will read

00:09:05,440 --> 00:09:07,760
this code

00:09:06,399 --> 00:09:10,160
then we can generate the actual

00:09:07,760 --> 00:09:12,160
algorithm code we can generate

00:09:10,160 --> 00:09:14,800
the parallelization system code the psi

00:09:12,160 --> 00:09:17,120
code we can also do transformations

00:09:14,800 --> 00:09:18,880
on this uh on the intermediate

00:09:17,120 --> 00:09:21,040
representation before we generate

00:09:18,880 --> 00:09:23,480
the the side layer code to do different

00:09:21,040 --> 00:09:26,480
things for example to generate

00:09:23,480 --> 00:09:26,480
openmp

00:09:28,880 --> 00:09:32,880
so what does this kind of look like well

00:09:30,399 --> 00:09:34,959
the algorithm layer is very high level

00:09:32,880 --> 00:09:36,880
we have a keyword called invoke which

00:09:34,959 --> 00:09:38,320
means do this in parallel

00:09:36,880 --> 00:09:40,399
and then the invoke will operate on

00:09:38,320 --> 00:09:41,760
kernels which take fields and here we

00:09:40,399 --> 00:09:44,160
see a number of fields

00:09:41,760 --> 00:09:45,760
and you can have multiple kernels in a

00:09:44,160 --> 00:09:46,959
single invoke that gives us a bigger

00:09:45,760 --> 00:09:50,080
scope for

00:09:46,959 --> 00:09:52,880
ordering and parallel communication etc

00:09:50,080 --> 00:09:54,480
and kernels themselves operate on a

00:09:52,880 --> 00:09:57,120
single column of data

00:09:54,480 --> 00:09:59,519
the fields are data parallel global

00:09:57,120 --> 00:09:59,519
fields

00:10:00,800 --> 00:10:04,079
the key to making this all work is in

00:10:02,720 --> 00:10:06,640
fact

00:10:04,079 --> 00:10:07,440
the it comes in the kernel which is

00:10:06,640 --> 00:10:10,480
where we

00:10:07,440 --> 00:10:12,160
encode the kernel metadata we can

00:10:10,480 --> 00:10:14,160
capture the rich domain specific

00:10:12,160 --> 00:10:16,320
information that we need to exploit for

00:10:14,160 --> 00:10:18,480
parallelism and horizontal looping

00:10:16,320 --> 00:10:19,680
so for example here we see some of the

00:10:18,480 --> 00:10:21,680
kernel method data

00:10:19,680 --> 00:10:23,519
where we're going to say what what type

00:10:21,680 --> 00:10:27,040
of arguments the kernel takes

00:10:23,519 --> 00:10:28,880
and um then some the

00:10:27,040 --> 00:10:30,480
operators of fields where they're read

00:10:28,880 --> 00:10:31,680
whether they're incremented what space

00:10:30,480 --> 00:10:33,120
they can operate on what's the

00:10:31,680 --> 00:10:36,320
relationship between them

00:10:33,120 --> 00:10:38,160
what they iterate over

00:10:36,320 --> 00:10:40,160
and it's this metadata that allows us to

00:10:38,160 --> 00:10:42,079
do the automatic code generation and the

00:10:40,160 --> 00:10:43,519
semantics of this metadata has meaning

00:10:42,079 --> 00:10:45,360
in the weather and climate

00:10:43,519 --> 00:10:46,640
model which is specific to what we're

00:10:45,360 --> 00:10:48,240
trying to do

00:10:46,640 --> 00:10:50,160
so in this case this kernel is just

00:10:48,240 --> 00:10:54,000
doing local matrix assembly

00:10:50,160 --> 00:10:56,640
so um sorry it is going to do y equals m

00:10:54,000 --> 00:10:57,839
x where m is a local has been assembled

00:10:56,640 --> 00:10:59,200
locally in the cell

00:10:57,839 --> 00:11:02,240
and the horizontal looping will be in

00:10:59,200 --> 00:11:04,079
the side layer the kernel code itself is

00:11:02,240 --> 00:11:07,040
actually very simple it just looks like

00:11:04,079 --> 00:11:07,920
uh basically fortran 90 it's loops

00:11:07,040 --> 00:11:11,279
arrays

00:11:07,920 --> 00:11:12,880
and um and and

00:11:11,279 --> 00:11:15,600
maybe some fortran intrinsics and things

00:11:12,880 --> 00:11:18,000
like that

00:11:15,600 --> 00:11:20,320
so here we see some generated scilayer

00:11:18,000 --> 00:11:20,320
code

00:11:20,640 --> 00:11:26,399
we have updated halos which we're using

00:11:24,160 --> 00:11:28,000
which is basically a halo exchange via a

00:11:26,399 --> 00:11:31,760
library called yaxed which basically

00:11:28,000 --> 00:11:33,519
calls down to mpi

00:11:31,760 --> 00:11:35,200
we might get some information from the

00:11:33,519 --> 00:11:36,320
infrastructure to do with coloring so

00:11:35,200 --> 00:11:37,600
that's loop ordering

00:11:36,320 --> 00:11:40,800
i'll tell you more about coloring in a

00:11:37,600 --> 00:11:43,760
minute here we see the openmp workshare

00:11:40,800 --> 00:11:45,680
across all the cells in the color and

00:11:43,760 --> 00:11:48,320
then there's the actual kernel call

00:11:45,680 --> 00:11:50,320
itself so i thought i'd take you just a

00:11:48,320 --> 00:11:53,200
little bit through how cyclone

00:11:50,320 --> 00:11:53,680
does this so to start with um with a

00:11:53,200 --> 00:11:56,880
single

00:11:53,680 --> 00:11:59,040
kernel invoke it gets a schedule of

00:11:56,880 --> 00:12:00,560
things it generates from the past the

00:11:59,040 --> 00:12:01,680
code it generates a schedule of things

00:12:00,560 --> 00:12:04,560
it's going to do

00:12:01,680 --> 00:12:05,760
so in this case if there's no if the if

00:12:04,560 --> 00:12:06,959
the distributed memory

00:12:05,760 --> 00:12:09,040
is switched off and there's no other

00:12:06,959 --> 00:12:09,760
transformations it's just going to loop

00:12:09,040 --> 00:12:12,399
over

00:12:09,760 --> 00:12:14,800
all the cells in the domain and then

00:12:12,399 --> 00:12:16,399
call the kernel

00:12:14,800 --> 00:12:18,160
if we apply the distributed memory

00:12:16,399 --> 00:12:20,560
transformation then

00:12:18,160 --> 00:12:21,519
based on that kernel metadata that it's

00:12:20,560 --> 00:12:23,519
just read

00:12:21,519 --> 00:12:25,360
then it can decide that it needs to do

00:12:23,519 --> 00:12:26,720
halo exchanges on the fields that are

00:12:25,360 --> 00:12:28,079
coming in to make sure they're up to

00:12:26,720 --> 00:12:30,560
date first

00:12:28,079 --> 00:12:33,200
now the loop order goes to the last cell

00:12:30,560 --> 00:12:33,200
in the halo

00:12:33,519 --> 00:12:41,519
and then it calls the kernel

00:12:39,120 --> 00:12:42,639
so we need to generate some openmp as

00:12:41,519 --> 00:12:44,560
well

00:12:42,639 --> 00:12:45,760
uh openmp is generated by a simple

00:12:44,560 --> 00:12:48,480
python script to apply

00:12:45,760 --> 00:12:49,839
the openmp transformation we can apply

00:12:48,480 --> 00:12:51,680
this to the whole model

00:12:49,839 --> 00:12:55,040
we can apply it or as fine grain to a

00:12:51,680 --> 00:12:57,760
single file or even to a single invoke

00:12:55,040 --> 00:12:59,279
and here's the openmp script so pretty

00:12:57,760 --> 00:13:00,880
much we just say it's pretty much

00:12:59,279 --> 00:13:03,200
this just does these three things we

00:13:00,880 --> 00:13:05,760
import three sets of transformations

00:13:03,200 --> 00:13:06,320
uh we need a coloring transformation the

00:13:05,760 --> 00:13:08,240
openp

00:13:06,320 --> 00:13:09,760
loop transformation and the openmp

00:13:08,240 --> 00:13:11,120
parallel transformation

00:13:09,760 --> 00:13:14,399
so we go through the list of all the

00:13:11,120 --> 00:13:16,959
invokes that that's been generated by

00:13:14,399 --> 00:13:17,920
by cyclone and then for each loop we

00:13:16,959 --> 00:13:21,680
need to check

00:13:17,920 --> 00:13:25,440
some information about the loop itself

00:13:21,680 --> 00:13:26,800
to see whether uh this particular kernel

00:13:25,440 --> 00:13:29,120
can be done as whether it is to be

00:13:26,800 --> 00:13:30,800
ordered or unordered and so this is for

00:13:29,120 --> 00:13:32,800
correctness we have to color

00:13:30,800 --> 00:13:34,160
or not color and so depending on what

00:13:32,800 --> 00:13:37,200
space the loop's over we'd

00:13:34,160 --> 00:13:38,880
have to color or not and then we can

00:13:37,200 --> 00:13:42,079
apply that transformation

00:13:38,880 --> 00:13:44,240
and then we do the openmp transformation

00:13:42,079 --> 00:13:46,399
itself loop over colors and

00:13:44,240 --> 00:13:47,600
and then put in the openmp itself and

00:13:46,399 --> 00:13:48,000
then we can print the schedule and see

00:13:47,600 --> 00:13:50,959
what we've

00:13:48,000 --> 00:13:51,839
done so the transform schedule for

00:13:50,959 --> 00:13:54,399
openmp

00:13:51,839 --> 00:13:55,519
now looks like this so again we've got

00:13:54,399 --> 00:13:58,639
the distributed

00:13:55,519 --> 00:14:00,079
uh memory hello exchanges first

00:13:58,639 --> 00:14:02,000
now the loops over the number of we have

00:14:00,079 --> 00:14:04,000
a loop over colors

00:14:02,000 --> 00:14:06,079
then we have the openmp directives and

00:14:04,000 --> 00:14:09,440
then we have for each cell in the

00:14:06,079 --> 00:14:10,959
color we're going to do the openmp loop

00:14:09,440 --> 00:14:13,360
across that

00:14:10,959 --> 00:14:17,839
and here's the generated code we saw

00:14:13,360 --> 00:14:17,839
just now

00:14:22,480 --> 00:14:26,480
so that's the basic uh the basic way

00:14:24,880 --> 00:14:27,120
things work and with that we're able to

00:14:26,480 --> 00:14:29,920
generate

00:14:27,120 --> 00:14:31,600
openmp for the whole code and so we

00:14:29,920 --> 00:14:34,880
don't actually hand write openmp

00:14:31,600 --> 00:14:36,720
at all but we can do

00:14:34,880 --> 00:14:38,079
we can now look at improving the

00:14:36,720 --> 00:14:41,279
performance of things

00:14:38,079 --> 00:14:45,040
so uh for example here we have

00:14:41,279 --> 00:14:45,600
um so with this finite element approach

00:14:45,040 --> 00:14:47,360
we have

00:14:45,600 --> 00:14:49,279
a degree of freedom that like might live

00:14:47,360 --> 00:14:52,880
on a um

00:14:49,279 --> 00:14:55,199
a cell edge now if

00:14:52,880 --> 00:14:56,480
if that space is a continuous space then

00:14:55,199 --> 00:15:00,240
that that darf can

00:14:56,480 --> 00:15:02,880
is shared between two cells if that cell

00:15:00,240 --> 00:15:04,000
those two cells are across a distributed

00:15:02,880 --> 00:15:07,519
memory partition

00:15:04,000 --> 00:15:08,880
boundary then um we might have to think

00:15:07,519 --> 00:15:10,639
a little bit carefully about how we do

00:15:08,880 --> 00:15:12,880
this so in this case this is the

00:15:10,639 --> 00:15:14,560
own cell that belongs to the partition

00:15:12,880 --> 00:15:17,199
and this is the halo cell

00:15:14,560 --> 00:15:18,639
for a partition so this degree of

00:15:17,199 --> 00:15:20,399
freedom to calculate its value we

00:15:18,639 --> 00:15:21,199
receive a contribution from both the

00:15:20,399 --> 00:15:24,959
owned

00:15:21,199 --> 00:15:26,959
and the halo cell now we could just do

00:15:24,959 --> 00:15:28,959
all the owned computation and let the

00:15:26,959 --> 00:15:30,800
neighboring

00:15:28,959 --> 00:15:32,240
partition calculate the values for this

00:15:30,800 --> 00:15:33,440
cell and then communicate them

00:15:32,240 --> 00:15:35,360
and that would give us the correct

00:15:33,440 --> 00:15:35,759
answer but we can try and do better than

00:15:35,360 --> 00:15:37,519
that

00:15:35,759 --> 00:15:39,279
we can now try and do redundant

00:15:37,519 --> 00:15:41,120
computation of the contribution in the

00:15:39,279 --> 00:15:42,560
halo to the shared dof

00:15:41,120 --> 00:15:45,279
it's redundant because it's done by

00:15:42,560 --> 00:15:47,759
multiple processors

00:15:45,279 --> 00:15:50,480
but it does mean we can do less uh less

00:15:47,759 --> 00:15:50,480
communication

00:15:50,560 --> 00:15:54,399
and not only that if we did it with mpi

00:15:52,959 --> 00:15:57,440
only

00:15:54,399 --> 00:15:58,240
for example here four mpi ranks these

00:15:57,440 --> 00:16:01,519
all have

00:15:58,240 --> 00:16:03,839
halos if i do the hybrid mode so for

00:16:01,519 --> 00:16:06,160
example one mpi task

00:16:03,839 --> 00:16:07,279
which has one big halo and then i have

00:16:06,160 --> 00:16:09,360
four openmp

00:16:07,279 --> 00:16:10,480
threads and so the data is split between

00:16:09,360 --> 00:16:13,040
those four threads

00:16:10,480 --> 00:16:14,560
the boundary boundary to area scaling

00:16:13,040 --> 00:16:16,079
means that if i do redundant

00:16:14,560 --> 00:16:18,240
communication

00:16:16,079 --> 00:16:20,240
there is less work for that redundant

00:16:18,240 --> 00:16:23,839
communication for the openmp if

00:16:20,240 --> 00:16:23,839
i use openmp threads

00:16:24,079 --> 00:16:28,560
so here's his here's a scaling plot to

00:16:27,199 --> 00:16:30,880
show that

00:16:28,560 --> 00:16:34,320
so this is for the matrix vector kernel

00:16:30,880 --> 00:16:34,320
i i showed you just now

00:16:34,839 --> 00:16:37,360
um

00:16:36,240 --> 00:16:39,279
so basically what it's saying is the

00:16:37,360 --> 00:16:42,480
time spent in the matrix uh

00:16:39,279 --> 00:16:44,160
matrix vector kernel so it's on a a c

00:16:42,480 --> 00:16:46,800
two to eight mesh so that means each

00:16:44,160 --> 00:16:48,480
panel the cube sphere is 288 cells by

00:16:46,800 --> 00:16:51,199
288 cells

00:16:48,480 --> 00:16:55,040
it's 30 levels it's running on 96 nodes

00:16:51,199 --> 00:16:57,040
of the metals cray xc 40

00:16:55,040 --> 00:16:59,120
so it's a it's a medium-sized problem

00:16:57,040 --> 00:16:59,920
shall we say um this is done at fixed

00:16:59,120 --> 00:17:03,600
resource

00:16:59,920 --> 00:17:04,559
so one openmp thread is 36 mpi ranks per

00:17:03,600 --> 00:17:07,439
node

00:17:04,559 --> 00:17:08,319
nine openmp threads is four mpi ranks

00:17:07,439 --> 00:17:12,319
per node with

00:17:08,319 --> 00:17:14,799
nine openmp threads per mpi rank

00:17:12,319 --> 00:17:16,000
as we can see here that as we increase

00:17:14,799 --> 00:17:18,160
the number of openmp

00:17:16,000 --> 00:17:20,880
threads and decrease the number of mpi

00:17:18,160 --> 00:17:22,480
ranks it takes less time to do the

00:17:20,880 --> 00:17:24,000
calculation because the redundant

00:17:22,480 --> 00:17:26,400
computation favors

00:17:24,000 --> 00:17:28,079
more threads uh the red and the yellow

00:17:26,400 --> 00:17:31,520
sticks there's just that's just

00:17:28,079 --> 00:17:33,280
a plot that has another the optimization

00:17:31,520 --> 00:17:35,039
here is just an mpicom's environment

00:17:33,280 --> 00:17:35,600
variable so it's not not relevant to

00:17:35,039 --> 00:17:38,160
what

00:17:35,600 --> 00:17:39,840
we were talking about here so that all

00:17:38,160 --> 00:17:41,600
works very nicely and that all sounds

00:17:39,840 --> 00:17:45,280
great

00:17:41,600 --> 00:17:46,880
so um but the code is undergoing active

00:17:45,280 --> 00:17:49,200
science development and that means we

00:17:46,880 --> 00:17:50,720
can have changes to the api required

00:17:49,200 --> 00:17:52,320
now they're not generate they're not

00:17:50,720 --> 00:17:54,000
supported all straight away

00:17:52,320 --> 00:17:55,840
we have to do some work in cyclone if we

00:17:54,000 --> 00:17:57,919
have to do a new thing

00:17:55,840 --> 00:17:58,960
so we can have a way of having

00:17:57,919 --> 00:18:00,799
handwritten

00:17:58,960 --> 00:18:02,000
code in the scilar which we call cycle

00:18:00,799 --> 00:18:04,000
lite

00:18:02,000 --> 00:18:05,600
so that we can still do science

00:18:04,000 --> 00:18:08,000
development and then we go away and fix

00:18:05,600 --> 00:18:11,039
the api in the code generation

00:18:08,000 --> 00:18:12,960
so the rules for this is mpi only and

00:18:11,039 --> 00:18:15,360
then we will extend the api to generate

00:18:12,960 --> 00:18:16,640
the openmp

00:18:15,360 --> 00:18:18,000
so for example here we see some

00:18:16,640 --> 00:18:18,880
advection code which is under

00:18:18,000 --> 00:18:21,039
development

00:18:18,880 --> 00:18:22,240
and it doesn't it's not yet supported by

00:18:21,039 --> 00:18:24,960
cyclone so this is

00:18:22,240 --> 00:18:27,360
uh uses a handwritten scilaire that has

00:18:24,960 --> 00:18:29,360
mpi only and as you can see of course

00:18:27,360 --> 00:18:30,640
it doesn't scare at all with an openmp

00:18:29,360 --> 00:18:33,440
number of threads because there's no

00:18:30,640 --> 00:18:36,400
openmp in it so it supports the scaling

00:18:33,440 --> 00:18:38,080
so taking this approach we have to do

00:18:36,400 --> 00:18:38,400
work but we do the work in a structured

00:18:38,080 --> 00:18:42,240
in

00:18:38,400 --> 00:18:45,280
a much more structured way

00:18:42,240 --> 00:18:47,440
but we can go further still so again we

00:18:45,280 --> 00:18:50,799
look at these this

00:18:47,440 --> 00:18:52,960
partitioned cells and we have the same

00:18:50,799 --> 00:18:57,600
degree of freedom on a shared boundary

00:18:52,960 --> 00:19:00,720
in partition one and partition two

00:18:57,600 --> 00:19:02,960
and so we have this idea that uh in cell

00:19:00,720 --> 00:19:04,160
in the sudden partition one owns the

00:19:02,960 --> 00:19:06,160
degree of freedom

00:19:04,160 --> 00:19:07,520
but for the selling p partition two it's

00:19:06,160 --> 00:19:11,200
an annex dot because

00:19:07,520 --> 00:19:13,520
it's repeated we also have to do

00:19:11,200 --> 00:19:15,440
point-wise computations for example we

00:19:13,520 --> 00:19:16,080
might be setting the field to a scale of

00:19:15,440 --> 00:19:17,919
value

00:19:16,080 --> 00:19:21,039
in which case we want to loop over the

00:19:17,919 --> 00:19:23,039
over the dots degrees of freedom

00:19:21,039 --> 00:19:25,440
looping to the own degrees of freedom

00:19:23,039 --> 00:19:29,200
means there's a halo exchange required

00:19:25,440 --> 00:19:31,280
for the partition 2 to have the correct

00:19:29,200 --> 00:19:32,720
value of that annexed off

00:19:31,280 --> 00:19:34,320
however we can now do another

00:19:32,720 --> 00:19:35,919
transformation in cyclone

00:19:34,320 --> 00:19:38,080
where we change the loop to include all

00:19:35,919 --> 00:19:40,240
the annex dofs

00:19:38,080 --> 00:19:42,080
and that results in a small increase in

00:19:40,240 --> 00:19:44,080
redundant computation

00:19:42,080 --> 00:19:45,280
but a large reduction in the number of

00:19:44,080 --> 00:19:48,240
halo exchanges

00:19:45,280 --> 00:19:49,440
and of course we can then apply openmp

00:19:48,240 --> 00:19:53,840
to

00:19:49,440 --> 00:19:53,840
that redundant computation

00:19:53,919 --> 00:19:58,160
so here we again see some more generated

00:19:56,799 --> 00:20:00,240
style layer code this time for the

00:19:58,160 --> 00:20:02,960
matrix vector

00:20:00,240 --> 00:20:05,679
and as you can see we're going to set

00:20:02,960 --> 00:20:07,679
the value of y to zero before

00:20:05,679 --> 00:20:09,280
we go into the matrix vector because

00:20:07,679 --> 00:20:10,880
it's go in this case it increments into

00:20:09,280 --> 00:20:12,559
the value of y so for this particular

00:20:10,880 --> 00:20:13,840
calculation we need to set it to zero

00:20:12,559 --> 00:20:18,159
first

00:20:13,840 --> 00:20:20,480
so this has just mpi in it and so we say

00:20:18,159 --> 00:20:21,280
okay let's do the halo exchange to do to

00:20:20,480 --> 00:20:24,880
get all those

00:20:21,280 --> 00:20:26,720
degrees of freedom correct uh and same

00:20:24,880 --> 00:20:27,600
for x and then we did the loops the last

00:20:26,720 --> 00:20:29,679
cell and the halo

00:20:27,600 --> 00:20:34,400
and then we do the actual call them the

00:20:29,679 --> 00:20:36,159
matrix vector kernel

00:20:34,400 --> 00:20:38,400
if i do the transformation i can

00:20:36,159 --> 00:20:40,480
transform the loop to now do alexdos so

00:20:38,400 --> 00:20:42,400
the first loop here

00:20:40,480 --> 00:20:43,760
we looped the last degree of freedom in

00:20:42,400 --> 00:20:45,440
the halo

00:20:43,760 --> 00:20:47,120
so we're redundantly computating into

00:20:45,440 --> 00:20:49,200
the halo

00:20:47,120 --> 00:20:50,720
and then this reduces the number of halo

00:20:49,200 --> 00:20:53,440
exchanges because there's now

00:20:50,720 --> 00:20:54,000
no there's no halo exchange on y we can

00:20:53,440 --> 00:20:56,000
also do

00:20:54,000 --> 00:20:59,760
put openmp threads around this to reduce

00:20:56,000 --> 00:20:59,760
the cost of the redundant computation

00:21:00,559 --> 00:21:03,600
and then we can see other things here so

00:21:03,120 --> 00:21:06,080
this

00:21:03,600 --> 00:21:07,760
particular matrix vector routine is

00:21:06,080 --> 00:21:08,559
colored loop order because the shared

00:21:07,760 --> 00:21:11,919
dots

00:21:08,559 --> 00:21:12,559
um so we cannot fuse this loop with that

00:21:11,919 --> 00:21:14,400
loop

00:21:12,559 --> 00:21:16,000
because um this one just loops over

00:21:14,400 --> 00:21:17,919
degrees of freedom and this one loops

00:21:16,000 --> 00:21:21,280
over the cells in a particular order to

00:21:17,919 --> 00:21:21,280
get the correct answer

00:21:22,480 --> 00:21:26,080
so having done those things we're using

00:21:24,400 --> 00:21:27,600
the alex dos and redundant computation

00:21:26,080 --> 00:21:29,120
that gives us about a 60

00:21:27,600 --> 00:21:31,120
reduction in the number of halo

00:21:29,120 --> 00:21:32,640
exchanges and we've seen that if we do

00:21:31,120 --> 00:21:35,840
redundant computation

00:21:32,640 --> 00:21:37,760
with openmp then that scales better

00:21:35,840 --> 00:21:39,200
using the openmp

00:21:37,760 --> 00:21:41,520
we can also have a look at what happens

00:21:39,200 --> 00:21:42,320
to the mpi communication as we increase

00:21:41,520 --> 00:21:45,440
the number of

00:21:42,320 --> 00:21:48,480
openmp threads with

00:21:45,440 --> 00:21:51,520
fewer mpi ranks

00:21:48,480 --> 00:21:52,960
we get fewer but larger messages

00:21:51,520 --> 00:21:54,720
and we can see how that affects local

00:21:52,960 --> 00:21:56,240
communication and also affects the

00:21:54,720 --> 00:21:58,559
global communication so

00:21:56,240 --> 00:22:00,320
here we see the small increase in cost

00:21:58,559 --> 00:22:01,760
as i increase the number of halo

00:22:00,320 --> 00:22:04,640
as i increase the number of threads and

00:22:01,760 --> 00:22:06,559
make the fewer larger messages for the

00:22:04,640 --> 00:22:07,840
calorie exchanges and then this teal

00:22:06,559 --> 00:22:09,840
color here you can see the cost of the

00:22:07,840 --> 00:22:12,159
global sum coming down slightly

00:22:09,840 --> 00:22:13,760
as i have less npr banks take part in

00:22:12,159 --> 00:22:15,280
there and if i add them together

00:22:13,760 --> 00:22:17,280
overall the communication costs are

00:22:15,280 --> 00:22:18,080
fairly neutral jumps around a little bit

00:22:17,280 --> 00:22:21,679
because of

00:22:18,080 --> 00:22:23,600
their um aries network variability

00:22:21,679 --> 00:22:24,720
um i'm probably going to run out of time

00:22:23,600 --> 00:22:26,000
fairly soon so i'm just going to go

00:22:24,720 --> 00:22:30,240
through the next couple of bits quite

00:22:26,000 --> 00:22:33,520
quickly we have a multi-grid solver

00:22:30,240 --> 00:22:35,200
and we use geometric multi-grid

00:22:33,520 --> 00:22:37,600
and that has a minimum of two by two

00:22:35,200 --> 00:22:39,360
cells per partition constraint

00:22:37,600 --> 00:22:41,039
for simple partitioning and simple

00:22:39,360 --> 00:22:42,559
communication better to help with the

00:22:41,039 --> 00:22:45,679
scaling

00:22:42,559 --> 00:22:46,559
but if i use openmp i've got bigger mpi

00:22:45,679 --> 00:22:48,320
partitions

00:22:46,559 --> 00:22:49,600
and more flexibility so it doesn't

00:22:48,320 --> 00:22:50,640
that's not helping with performance

00:22:49,600 --> 00:22:52,400
that's just helping with

00:22:50,640 --> 00:22:54,159
setting the whole problem up and making

00:22:52,400 --> 00:22:56,320
it easier to actually

00:22:54,159 --> 00:22:57,760
give us access to a load of more

00:22:56,320 --> 00:22:59,039
resolutions that we couldn't otherwise

00:22:57,760 --> 00:23:00,559
run at

00:22:59,039 --> 00:23:02,480
we're also looking at how you might use

00:23:00,559 --> 00:23:03,760
accelerators so at the moment cyclone

00:23:02,480 --> 00:23:06,960
can generate open

00:23:03,760 --> 00:23:08,880
acc um at least for the newer api we're

00:23:06,960 --> 00:23:10,480
exploring this for elfric but compilers

00:23:08,880 --> 00:23:12,000
have problems with the fortran not with

00:23:10,480 --> 00:23:13,679
the open acc

00:23:12,000 --> 00:23:16,080
tests with single kernels show this work

00:23:13,679 --> 00:23:18,640
well we're also testing single kernels

00:23:16,080 --> 00:23:21,039
with openmp 4.5

00:23:18,640 --> 00:23:22,159
using offload which is similar but not

00:23:21,039 --> 00:23:24,159
the same but

00:23:22,159 --> 00:23:25,360
the logic to set it up is certainly very

00:23:24,159 --> 00:23:26,880
similar

00:23:25,360 --> 00:23:29,120
we can also target other programming

00:23:26,880 --> 00:23:30,080
models opencl and test codes which we

00:23:29,120 --> 00:23:33,360
can then feed

00:23:30,080 --> 00:23:35,200
into the fpga toolsets we could also

00:23:33,360 --> 00:23:37,600
target coccos or sickle though we can't

00:23:35,200 --> 00:23:39,360
do that yet

00:23:37,600 --> 00:23:41,200
another thought we have this rich domain

00:23:39,360 --> 00:23:43,279
specific information captured in the

00:23:41,200 --> 00:23:44,400
scientific in cyan clone intermediate

00:23:43,279 --> 00:23:46,159
representation

00:23:44,400 --> 00:23:48,000
how can we let a compiler exploit this

00:23:46,159 --> 00:23:49,440
directly for optimization

00:23:48,000 --> 00:23:51,039
uh we have other collaborations looking

00:23:49,440 --> 00:23:51,919
at getting things going with the gpu and

00:23:51,039 --> 00:23:53,520
using other

00:23:51,919 --> 00:23:55,919
code generation tools like the clog

00:23:53,520 --> 00:23:58,720
compiler

00:23:55,919 --> 00:23:59,840
so um i'm just going to get to my

00:23:58,720 --> 00:24:02,240
conclusions then

00:23:59,840 --> 00:24:04,080
so the exascale computing we have the

00:24:02,240 --> 00:24:06,000
xsql computing software challenge

00:24:04,080 --> 00:24:07,200
how can we maintain single science

00:24:06,000 --> 00:24:08,880
source for

00:24:07,200 --> 00:24:11,120
code for different programming models

00:24:08,880 --> 00:24:13,200
needed to exploit diverse computer

00:24:11,120 --> 00:24:14,799
architectures

00:24:13,200 --> 00:24:16,480
we're using a domain specific language

00:24:14,799 --> 00:24:18,320
for the met office to restore the

00:24:16,480 --> 00:24:20,480
separation of concerns

00:24:18,320 --> 00:24:22,400
exploit the rich domain specific

00:24:20,480 --> 00:24:24,159
information that we have in the model

00:24:22,400 --> 00:24:26,159
and generate different code for

00:24:24,159 --> 00:24:28,400
different processor architectures

00:24:26,159 --> 00:24:30,559
this also allows us to have productive

00:24:28,400 --> 00:24:31,840
development of science code

00:24:30,559 --> 00:24:33,760
i've tried to show you there's an

00:24:31,840 --> 00:24:34,880
interplay between the algorithm and the

00:24:33,760 --> 00:24:36,480
programming model

00:24:34,880 --> 00:24:39,039
and for example we're exploiting

00:24:36,480 --> 00:24:40,559
redundant computation and thread scaling

00:24:39,039 --> 00:24:42,159
to improve the to through the

00:24:40,559 --> 00:24:45,279
performance um

00:24:42,159 --> 00:24:46,720
and just the record here are some papers

00:24:45,279 --> 00:24:48,080
which i'm not going to go through but

00:24:46,720 --> 00:24:50,000
that's in the talk if you want to look

00:24:48,080 --> 00:24:51,120
up what we've been doing in more detail

00:24:50,000 --> 00:24:52,799
and i'm just going to leave you with

00:24:51,120 --> 00:24:54,159
this because this is a simulation of

00:24:52,799 --> 00:24:57,039
something called acroplanet

00:24:54,159 --> 00:24:59,360
uh which is a a world which has no land

00:24:57,039 --> 00:25:01,360
but just ocean and clouds and water

00:24:59,360 --> 00:25:02,799
and that's a simulation using the new

00:25:01,360 --> 00:25:03,679
model and i think that's rather

00:25:02,799 --> 00:25:07,279
beautiful

00:25:03,679 --> 00:25:09,840
thank you thanks a lot chris that was a

00:25:07,279 --> 00:25:11,279
really interesting talk and nice to hear

00:25:09,840 --> 00:25:12,960
um about all that work

00:25:11,279 --> 00:25:14,799
and there's a question coming and i

00:25:12,960 --> 00:25:17,679
wanted to be able to uh answer it

00:25:14,799 --> 00:25:18,960
it's from jamie quinn and they ask is

00:25:17,679 --> 00:25:21,120
the dsl

00:25:18,960 --> 00:25:24,159
um in in elfric expressive enough to

00:25:21,120 --> 00:25:25,760
allow for non-cfd operations such as

00:25:24,159 --> 00:25:27,600
coupling to other codes or data

00:25:25,760 --> 00:25:30,799
assimilation

00:25:27,600 --> 00:25:35,279
um we're looking at how we extend

00:25:30,799 --> 00:25:38,320
things um so at the moment uh

00:25:35,279 --> 00:25:40,559
no so for example we

00:25:38,320 --> 00:25:42,000
the dynamics is finite element but a lot

00:25:40,559 --> 00:25:42,720
of the physical parameterizations will

00:25:42,000 --> 00:25:45,440
remain

00:25:42,720 --> 00:25:47,120
finite difference codes so we're looking

00:25:45,440 --> 00:25:49,039
at how we extend into that

00:25:47,120 --> 00:25:50,480
to keep things working on the face of it

00:25:49,039 --> 00:25:52,799
it's quite simple but then things get a

00:25:50,480 --> 00:25:54,159
little more complicated

00:25:52,799 --> 00:25:55,760
the issue of course with being domain

00:25:54,159 --> 00:25:56,080
specific is if you make your domain too

00:25:55,760 --> 00:25:57,919
big

00:25:56,080 --> 00:25:59,440
it's too difficult but if it's too small

00:25:57,919 --> 00:26:00,000
it's not useful so you have to kind of

00:25:59,440 --> 00:26:03,039
get the

00:26:00,000 --> 00:26:04,799
side of the domain kind of right so

00:26:03,039 --> 00:26:06,720
we are looking at how we extend things

00:26:04,799 --> 00:26:08,400
to do more things so

00:26:06,720 --> 00:26:11,120
we have a project looking at how we

00:26:08,400 --> 00:26:13,440
might do things with

00:26:11,120 --> 00:26:14,880
d a so for example you might think about

00:26:13,440 --> 00:26:17,039
trying to do things with a

00:26:14,880 --> 00:26:18,960
adjoint linear mod tangent adjoint

00:26:17,039 --> 00:26:20,960
linear model

00:26:18,960 --> 00:26:22,320
if you can parse all the code you could

00:26:20,960 --> 00:26:23,360
then try and do automatic

00:26:22,320 --> 00:26:24,799
differentiation

00:26:23,360 --> 00:26:26,320
in principle that's quite easy in

00:26:24,799 --> 00:26:28,559
practice it's very difficult that's kind

00:26:26,320 --> 00:26:31,760
of an exploratory project to see

00:26:28,559 --> 00:26:33,520
what we could potentially do so

00:26:31,760 --> 00:26:37,840
yeah that question goes to the heart of

00:26:33,520 --> 00:26:39,600
what it means to be a domain specific um

00:26:37,840 --> 00:26:42,080
uh

00:26:39,600 --> 00:26:43,840
using a dsl approach which is you get a

00:26:42,080 --> 00:26:44,159
lot of benefits but some things you also

00:26:43,840 --> 00:26:46,960
get

00:26:44,159 --> 00:26:47,600
to do some extra work as well so um yeah

00:26:46,960 --> 00:26:49,440
it

00:26:47,600 --> 00:26:51,039
it doesn't do everything by a long way

00:26:49,440 --> 00:26:52,640
yet but we're looking at how extended to

00:26:51,039 --> 00:26:54,159
do more and more things

00:26:52,640 --> 00:26:56,000
that's great thanks um there's a

00:26:54,159 --> 00:26:57,600
question come in on the q a but in the

00:26:56,000 --> 00:26:58,240
interest of time are you able to answer

00:26:57,600 --> 00:27:00,159
that by

00:26:58,240 --> 00:27:01,360
uh text chris while we get the uh the

00:27:00,159 --> 00:27:02,559
next speaker up

00:27:01,360 --> 00:27:04,640
yeah sure actually there's a question

00:27:02,559 --> 00:27:06,480
from mark i'll i can type that one

00:27:04,640 --> 00:27:07,760
yeah fantastic all right thank you very

00:27:06,480 --> 00:27:08,960
much

00:27:07,760 --> 00:27:10,880
if we're in person would obviously give

00:27:08,960 --> 00:27:16,880
you a very loud round of applause so

00:27:10,880 --> 00:27:16,880

YouTube URL: https://www.youtube.com/watch?v=33WvSELNg3I


