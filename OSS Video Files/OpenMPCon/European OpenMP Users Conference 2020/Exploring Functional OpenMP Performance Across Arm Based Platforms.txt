Title: Exploring Functional OpenMP Performance Across Arm Based Platforms
Publication date: 2020-12-21
Playlist: European OpenMP Users Conference 2020
Description: 
	This talk was presented at the 3rd European OpenMP Users Conference in 2020

Presented by : Phil Ridley, Arm

Conference Website: https://openmpusers.org
Follow us: https://twitter.com/openmp_users

Presentation Abstract :
Exploring Functional OpenMP Performance Across Arm Based Platforms In this work we shall discuss the usefulness of OpenMP across several readily available Arm HPC architectures. Two well-known benchmark suites will be considered: the NAS Parallel Benchmarks and the EPCC OpenMP micro-benchmark suite.

The NAS Parallel Benchmarks (https://www.nas.nasa.gov/publications/npb.html) were designed to help evaluate both MPI and OpenMP parallel performance. They consist of eight kernels, representative of those which are used commonly throughout computational fluid dynamics applications. The kernels cover memory access, Conjugate Gradient, Multi-Grid, Discrete Fourier Transforms and Linear Solvers.

The EPCC OpenMP micro-benchmark suite (https://www.epcc.ed.ac.uk/research/computing/performance-characterisation-and-benchmarking/epcc-openmp-micro-benchmark-suite) is intended to measure the overheads of synchronisation, loop scheduling and array operations in the OpenMP runtime library. As such it consists of four kernels: arraybench, schedbench, syncbench and taskbench.

By choosing these benchmarks we can demonstrate the effectiveness of OpenMP run-times for HPC applications across several Arm HPC based architectures, including the Fujitsu A64FX and Marvell® ThunderX2®, and Neoverse N1 platform: AWS Graviton2.

The Arm HPC ecosystem has matured significantly over the last few years and this includes the number of available Compilers and their associated OpenMP run-time libraries. For this work, we will consider GCC, the Arm Compiler for Linux (ACfL), the NVIDIA HPC Software Development Kit (SDK) and the HPE/Cray Compiling Environment (CCE).

For the main results, we will discuss performance for the OpenMP only variant of the NAS Parallel benchmarks across the different architectures. In particular we will investigate the effectiveness of the compilers and OpenM. Further investigation will be aided by the EPCC OpenMP micro-benchmarks to look closer at the thread-level behaviour.

On the Marvell® ThunderX2® (Figure 1), using 64 OpenMP threads (pinned to the physical cores in a node ordering manner) we compare three compilers: NVHPC,  GCC and ACfL since they generate the most interesting comparison. It’s clear that there is no overall winner: GCC wins four (CG.C, EP.C, IS.C, LU.C), ACfL wins three (BT.C.BLK,FT.C, MG.C) and NVHPC wins on SP.C.BLK. Performance difference is not solely down to the OpenMP runtimes and other compiler optimizations help to varying degrees between benchmarks. However, the compiler’s ability to generate optimized code within OpenMP regions does play an important role.

On the Fujitsu FX700 (HPE/Cray Apollo 80) (Figure 2), this time using 48 OpenMP threads, we compare three compilers: ACfL, GCC and CCE. The first noticeable difference between the previous results is that BT.C.BLK, EP.C, LU.C and SP.C.BLK exhibit much lower Mops/s than on the ThunderX2 whereas CG.C, FT.C, IS.C and MG.C are better (this is mostly due to the increased memory bandwidth which can be utilized by each OpenMP thread). This time GCC wins on five (BT.C.BLK, IS.C, LU.C, MG.C, SP.C.BLK) and CCE wins on the remaining three (CG.C, EP.C,FT.C). Again, a compiler’s ability to handle OpenMP regions is what helps.

The overall aim of this presentation is to guide a user’s choice of compiler and Arm HPC based platform, given they have a certain algorithm in mind which has already been parallelized with OpenMP.
Captions: 
	00:00:03,840 --> 00:00:06,799
i think

00:00:04,319 --> 00:00:09,040
our next speaker is here and if they're

00:00:06,799 --> 00:00:11,120
able to share their screen that will be

00:00:09,040 --> 00:00:12,080
uh brilliant that's uh it's phil ridley

00:00:11,120 --> 00:00:15,120
from from

00:00:12,080 --> 00:00:18,240
arm um welcome um

00:00:15,120 --> 00:00:20,240
so for those who don't know phil is uh

00:00:18,240 --> 00:00:21,920
works as an applications engineer for

00:00:20,240 --> 00:00:24,640
arm supporting users

00:00:21,920 --> 00:00:25,680
of um hpc platforms and tools and

00:00:24,640 --> 00:00:29,039
applications

00:00:25,680 --> 00:00:30,080
within the arm ecosystem so over to you

00:00:29,039 --> 00:00:32,320
phil

00:00:30,080 --> 00:00:34,079
hello i'm phil ridley from arm and i'm

00:00:32,320 --> 00:00:35,920
based in the uk

00:00:34,079 --> 00:00:40,239
i work in a small team which is just

00:00:35,920 --> 00:00:42,480
part of arm's overall involvement in hpc

00:00:40,239 --> 00:00:44,800
i'll start off by saying more about what

00:00:42,480 --> 00:00:46,399
our team does then i'll move on to

00:00:44,800 --> 00:00:49,120
describe some of the current

00:00:46,399 --> 00:00:51,360
arm based hpc architectures and the

00:00:49,120 --> 00:00:53,120
systems which use them

00:00:51,360 --> 00:00:55,199
following that i'll move on to the two

00:00:53,120 --> 00:00:57,520
benchmarks which i'm going to cover

00:00:55,199 --> 00:00:58,399
they are the nas parallel benchmarks

00:00:57,520 --> 00:01:00,399
which are from

00:00:58,399 --> 00:01:02,320
nasa's advanced supercomputing

00:01:00,399 --> 00:01:06,159
department

00:01:02,320 --> 00:01:08,240
and the epcc openmp micro benchmarks

00:01:06,159 --> 00:01:10,400
these are from the edinburgh parallel

00:01:08,240 --> 00:01:12,400
computing center

00:01:10,400 --> 00:01:14,799
i'll finally summarize with some

00:01:12,400 --> 00:01:17,600
suggestions and references for achieving

00:01:14,799 --> 00:01:17,600
best results

00:01:21,520 --> 00:01:25,200
so i work in a team of applications

00:01:23,680 --> 00:01:28,320
performance engineers

00:01:25,200 --> 00:01:29,040
with two of us based in the uk two in

00:01:28,320 --> 00:01:32,720
france

00:01:29,040 --> 00:01:34,799
and three in the united states

00:01:32,720 --> 00:01:36,479
each of us has specialist domain

00:01:34,799 --> 00:01:40,400
knowledge for example

00:01:36,479 --> 00:01:43,759
cfd or climate oil and gas or

00:01:40,400 --> 00:01:44,960
physical sciences and one of our aims is

00:01:43,759 --> 00:01:47,920
to help people

00:01:44,960 --> 00:01:50,240
get the best out of arm-based hpc

00:01:47,920 --> 00:01:52,479
hardware

00:01:50,240 --> 00:01:54,240
this might include performance

00:01:52,479 --> 00:01:57,439
projections for new

00:01:54,240 --> 00:01:58,320
or unreleased hardware applications

00:01:57,439 --> 00:02:02,719
performance

00:01:58,320 --> 00:02:06,840
tuning tuning system parameters

00:02:02,719 --> 00:02:09,119
or mentoring at hackathons and other

00:02:06,840 --> 00:02:10,879
events

00:02:09,119 --> 00:02:14,319
one system that you may be aware of

00:02:10,879 --> 00:02:18,080
which uses arm-based cpus is isembard

00:02:14,319 --> 00:02:21,360
ismbard is hosted at the uk met

00:02:18,080 --> 00:02:24,800
offices data center in exeter

00:02:21,360 --> 00:02:27,599
and isimbard now has two arm systems so

00:02:24,800 --> 00:02:29,599
there's an isenbard and isambard too

00:02:27,599 --> 00:02:31,840
izambard has been in service now for

00:02:29,599 --> 00:02:35,040
just over two years and it's a

00:02:31,840 --> 00:02:37,440
full hp craig xc50 system

00:02:35,040 --> 00:02:39,440
with a fast interconnect file system

00:02:37,440 --> 00:02:42,560
entune software stack

00:02:39,440 --> 00:02:44,560
so on izumbard the processors are marvel

00:02:42,560 --> 00:02:47,840
thunder x2s

00:02:44,560 --> 00:02:51,040
bear in mind that um

00:02:47,840 --> 00:02:54,400
we designed the instruction set and

00:02:51,040 --> 00:02:56,319
don't actually fabricate the silicon

00:02:54,400 --> 00:02:58,560
which is why you see the partner's name

00:02:56,319 --> 00:03:00,239
on the cpu

00:02:58,560 --> 00:03:03,200
so in addition to isimbard there are

00:03:00,239 --> 00:03:07,280
also three other

00:03:03,200 --> 00:03:09,440
uk uk-based marvel thunder x2 systems

00:03:07,280 --> 00:03:12,560
um and these are at the universities of

00:03:09,440 --> 00:03:15,040
bristol edinburgh and leicester

00:03:12,560 --> 00:03:16,000
known as the catalyst project so these

00:03:15,040 --> 00:03:20,720
are three

00:03:16,000 --> 00:03:24,080
hpe apollo 70 systems each system has

00:03:20,720 --> 00:03:27,920
um roughly uh 64 nodes

00:03:24,080 --> 00:03:32,400
of uh 32 core dual socket

00:03:27,920 --> 00:03:32,640
cpus um so they're quite large systems

00:03:32,400 --> 00:03:36,239
in

00:03:32,640 --> 00:03:39,280
in their own right and um

00:03:36,239 --> 00:03:41,360
the other part of um is ambard is

00:03:39,280 --> 00:03:42,879
ismart ii and this has only been in

00:03:41,360 --> 00:03:47,840
service now for around

00:03:42,879 --> 00:03:47,840
a few weeks and it's a

00:03:48,000 --> 00:03:52,879
hpe apollo 80 system so this has a newer

00:03:51,760 --> 00:03:55,280
processor

00:03:52,879 --> 00:03:56,640
which is also based on an arm

00:03:55,280 --> 00:03:59,840
instruction set

00:03:56,640 --> 00:04:01,280
but this time it's a fujitsu a64 fx

00:03:59,840 --> 00:04:03,120
processor

00:04:01,280 --> 00:04:04,959
so you can see the specs in the slides

00:04:03,120 --> 00:04:07,040
but essentially both is and bars are

00:04:04,959 --> 00:04:10,080
substantial hpc resources

00:04:07,040 --> 00:04:13,439
in their own right however the cpus

00:04:10,080 --> 00:04:15,439
themselves are quite different

00:04:13,439 --> 00:04:16,479
so going back to the armed cpus and

00:04:15,439 --> 00:04:19,680
ismbard

00:04:16,479 --> 00:04:22,160
the marvel thunder x2s um

00:04:19,680 --> 00:04:24,240
the nodes are configured as dual socket

00:04:22,160 --> 00:04:26,639
with 32 cores per socket

00:04:24,240 --> 00:04:27,360
however each physical core is currently

00:04:26,639 --> 00:04:30,240
configured

00:04:27,360 --> 00:04:34,320
with four symmetric multi-threads giving

00:04:30,240 --> 00:04:36,800
a total of 256 cpus per node

00:04:34,320 --> 00:04:38,320
um they don't have to be configured with

00:04:36,800 --> 00:04:41,120
four symmetric multi-threads

00:04:38,320 --> 00:04:41,759
you can have a setting of one thread per

00:04:41,120 --> 00:04:44,880
core

00:04:41,759 --> 00:04:48,479
or two threads per core either way

00:04:44,880 --> 00:04:48,800
um it doesn't matter it's how the cpus

00:04:48,479 --> 00:04:51,919
are

00:04:48,800 --> 00:04:54,800
are configured at um boot time

00:04:51,919 --> 00:04:56,320
um but as i say currently they're

00:04:54,800 --> 00:04:58,320
configured with four symmetric

00:04:56,320 --> 00:05:01,440
multithreads

00:04:58,320 --> 00:05:04,639
so the cpu is based on the rv 8.1

00:05:01,440 --> 00:05:07,280
a plus neon architecture with two

00:05:04,639 --> 00:05:09,759
floating point units per physical core

00:05:07,280 --> 00:05:11,360
obviously the more symmetric

00:05:09,759 --> 00:05:14,800
multi-threads you have

00:05:11,360 --> 00:05:17,120
um the more the floating point units

00:05:14,800 --> 00:05:20,080
need to be shared between threads

00:05:17,120 --> 00:05:20,720
um and this can affect performance if

00:05:20,080 --> 00:05:23,759
your

00:05:20,720 --> 00:05:27,199
application is floating point bound um

00:05:23,759 --> 00:05:30,080
so and the cpus also have a fixed

00:05:27,199 --> 00:05:31,520
vector register size of 128 bits

00:05:30,080 --> 00:05:34,720
essentially each socket has

00:05:31,520 --> 00:05:37,520
eight memory channels um which can

00:05:34,720 --> 00:05:40,400
facilitate good performance for many hpc

00:05:37,520 --> 00:05:42,400
applications

00:05:40,400 --> 00:05:43,759
so moving on to the armed cpu in

00:05:42,400 --> 00:05:47,440
izambard ii

00:05:43,759 --> 00:05:48,080
this is a fujitsu a64fx which is similar

00:05:47,440 --> 00:05:49,919
to the one

00:05:48,080 --> 00:05:52,160
in the fugaku system which is currently

00:05:49,919 --> 00:05:54,400
number one in the top 500.

00:05:52,160 --> 00:05:57,280
that system has a theoretical peak of

00:05:54,400 --> 00:06:00,400
over 440 petaflops

00:05:57,280 --> 00:06:03,600
the cpu um in isenbaugh ii and fugaku

00:06:00,400 --> 00:06:06,160
are similar they're not the same though

00:06:03,600 --> 00:06:07,759
they differ in clock speed and number of

00:06:06,160 --> 00:06:10,960
cores per socket

00:06:07,759 --> 00:06:13,840
um now the a64 fx

00:06:10,960 --> 00:06:16,080
on isembah2 is single socket with 48

00:06:13,840 --> 00:06:19,440
cores

00:06:16,080 --> 00:06:21,360
it's based on the rv 8.2 a plus scalable

00:06:19,440 --> 00:06:23,759
vector extension instruction set

00:06:21,360 --> 00:06:26,960
and it has a fixed vector register size

00:06:23,759 --> 00:06:29,600
of 512 bits

00:06:26,960 --> 00:06:31,120
so it's much wider vector registers than

00:06:29,600 --> 00:06:35,120
those that are on

00:06:31,120 --> 00:06:38,319
is embarked with the marvel thunder x2s

00:06:35,120 --> 00:06:41,840
the a64fx is also capable of higher

00:06:38,319 --> 00:06:44,960
memory bandwidth though the vectors

00:06:41,840 --> 00:06:49,440
are a fixed size the instruction set

00:06:44,960 --> 00:06:51,840
is using on scalable vector extension

00:06:49,440 --> 00:06:52,720
and what this means is that the compiler

00:06:51,840 --> 00:06:55,440
will produce

00:06:52,720 --> 00:06:56,400
vector lens agnostic assembly which in

00:06:55,440 --> 00:06:59,840
principle

00:06:56,400 --> 00:07:02,560
could be run on a future hardware that

00:06:59,840 --> 00:07:04,319
might have different length vectors that

00:07:02,560 --> 00:07:06,080
is without having to recompile the

00:07:04,319 --> 00:07:09,120
source code

00:07:06,080 --> 00:07:11,120
um sve code is made possible through the

00:07:09,120 --> 00:07:14,880
use of predicate registers

00:07:11,120 --> 00:07:18,639
and predication instructions um

00:07:14,880 --> 00:07:19,680
one thing uh to notice about the a64fx

00:07:18,639 --> 00:07:22,960
is that the cores

00:07:19,680 --> 00:07:25,039
are arranged um in four core memory

00:07:22,960 --> 00:07:27,919
groups or cmgs

00:07:25,039 --> 00:07:28,800
each one of the cmg has 12 cores within

00:07:27,919 --> 00:07:31,199
it

00:07:28,800 --> 00:07:33,520
and without going into too much detail a

00:07:31,199 --> 00:07:37,199
good place to start tuning your code

00:07:33,520 --> 00:07:37,599
on the a64fx is to look at performance

00:07:37,199 --> 00:07:40,319
on

00:07:37,599 --> 00:07:42,639
one of these core memory groups and then

00:07:40,319 --> 00:07:45,039
move on to the others

00:07:42,639 --> 00:07:45,680
and please also bear in mind that the l2

00:07:45,039 --> 00:07:48,879
cache

00:07:45,680 --> 00:07:51,520
on the a64fx is shared within each core

00:07:48,879 --> 00:07:51,520
memory group

00:07:51,599 --> 00:07:56,240
so in addition to isimbard and the

00:07:54,479 --> 00:07:59,680
catalyst systems

00:07:56,240 --> 00:08:02,879
you may also be aware that arm-based hpc

00:07:59,680 --> 00:08:06,639
is also available in the cloud

00:08:02,879 --> 00:08:07,759
one example is the amazon aws graviton 2

00:08:06,639 --> 00:08:12,479
processor

00:08:07,759 --> 00:08:15,280
and this can have up to 64 cpus per node

00:08:12,479 --> 00:08:18,160
and it it's really suitable for many hpc

00:08:15,280 --> 00:08:18,160
applications

00:08:18,479 --> 00:08:22,720
to summarize the main features of the

00:08:20,639 --> 00:08:25,120
cpus that i've just mentioned

00:08:22,720 --> 00:08:26,000
um they're all based on either the rv

00:08:25,120 --> 00:08:30,240
8.1

00:08:26,000 --> 00:08:34,479
or on the 8.2 instruction set with

00:08:30,240 --> 00:08:38,159
either 128-bit neon or 512-bit

00:08:34,479 --> 00:08:41,599
sve the marvel thunder x2

00:08:38,159 --> 00:08:44,240
is dual socket and works well with

00:08:41,599 --> 00:08:45,600
applications that require memory

00:08:44,240 --> 00:08:48,640
bandwidth

00:08:45,600 --> 00:08:52,399
the a64fx has

00:08:48,640 --> 00:08:54,800
large l2 cache and notably no l3 cache

00:08:52,399 --> 00:08:57,360
though to utilize the capability of the

00:08:54,800 --> 00:09:00,480
processor can sometimes require more

00:08:57,360 --> 00:09:03,519
um programming effort um

00:09:00,480 --> 00:09:06,839
the aws graviton 2 does seem

00:09:03,519 --> 00:09:08,160
like a good all-rounder and for many

00:09:06,839 --> 00:09:11,279
applications

00:09:08,160 --> 00:09:15,839
it's localized larger

00:09:11,279 --> 00:09:15,839
l1 and l2 caches do seem to have

00:09:17,279 --> 00:09:23,120
so now on to the first of the two

00:09:20,320 --> 00:09:25,680
benchmarks which we're going to discuss

00:09:23,120 --> 00:09:26,399
so the nas parallel benchmark is

00:09:25,680 --> 00:09:30,240
available

00:09:26,399 --> 00:09:34,720
in mpi and openmp variants but here

00:09:30,240 --> 00:09:37,200
i'm only going to talk about the openmp1

00:09:34,720 --> 00:09:37,920
so there are eight benchmarks in the

00:09:37,200 --> 00:09:40,959
suite

00:09:37,920 --> 00:09:44,240
um these are representative

00:09:40,959 --> 00:09:46,800
of um cfd kernels and

00:09:44,240 --> 00:09:48,480
includes algorithms such as conjugate

00:09:46,800 --> 00:09:52,080
gradient multi-grid

00:09:48,480 --> 00:09:55,680
and solvers like tridiagonal

00:09:52,080 --> 00:09:58,720
and lu type

00:09:55,680 --> 00:10:00,959
approaches so over the next few slides

00:09:58,720 --> 00:10:04,320
i'll talk about results

00:10:00,959 --> 00:10:06,880
for these on each of the three

00:10:04,320 --> 00:10:09,279
um hpc based systems that i've just

00:10:06,880 --> 00:10:09,279
mentioned

00:10:09,519 --> 00:10:13,920
so for the marvel thunder x2 cpu just

00:10:12,160 --> 00:10:16,079
like the one on izambard

00:10:13,920 --> 00:10:17,040
we'll look at three compilers which are

00:10:16,079 --> 00:10:20,480
available

00:10:17,040 --> 00:10:23,519
these are the nvidia hpc sdk compiler

00:10:20,480 --> 00:10:25,760
the arm compiler for linux and gcc

00:10:23,519 --> 00:10:27,200
the choice of these compilers is based

00:10:25,760 --> 00:10:28,079
on the fact that these are also

00:10:27,200 --> 00:10:31,120
available

00:10:28,079 --> 00:10:33,200
on the aws graviton too

00:10:31,120 --> 00:10:34,959
now the results in the graph are shown

00:10:33,200 --> 00:10:37,839
for the eight benchmarks

00:10:34,959 --> 00:10:39,440
um so we have uh three results for each

00:10:37,839 --> 00:10:42,640
benchmark

00:10:39,440 --> 00:10:45,279
with a higher result meaning better

00:10:42,640 --> 00:10:46,480
so the main point to note here is not to

00:10:45,279 --> 00:10:49,440
use the maximum

00:10:46,480 --> 00:10:50,640
number of of available threads on

00:10:49,440 --> 00:10:54,399
isembard where

00:10:50,640 --> 00:10:57,440
smt is full we can have omp num threads

00:10:54,399 --> 00:11:01,200
equal to 256

00:10:57,440 --> 00:11:04,000
giving each physical core four logical

00:11:01,200 --> 00:11:05,600
openmp threads however this is not the

00:11:04,000 --> 00:11:06,800
best way to get performance with this

00:11:05,600 --> 00:11:10,160
benchmark

00:11:06,800 --> 00:11:13,519
it's far better to use

00:11:10,160 --> 00:11:17,200
omp num thread 64 and

00:11:13,519 --> 00:11:22,480
set omp places to cause rather than

00:11:17,200 --> 00:11:24,560
threads and use omp prop buying close

00:11:22,480 --> 00:11:27,680
otherwise you you are likely to see a

00:11:24,560 --> 00:11:31,200
dramatic reduction in performance

00:11:27,680 --> 00:11:32,000
on thunder x2 systems where smt is set

00:11:31,200 --> 00:11:34,560
to one

00:11:32,000 --> 00:11:36,959
you you don't have this issue um it just

00:11:34,560 --> 00:11:36,959
works

00:11:38,160 --> 00:11:43,360
so here we have the the best results for

00:11:41,040 --> 00:11:45,920
each individual benchmark

00:11:43,360 --> 00:11:46,640
in terms of compiler choice it's really

00:11:45,920 --> 00:11:50,639
between

00:11:46,640 --> 00:11:52,959
gcc and arm each compiler does best

00:11:50,639 --> 00:11:54,720
with with four of the eight benchmarks

00:11:52,959 --> 00:11:57,839
so i'm better for some

00:11:54,720 --> 00:12:00,320
gcc for others and difference

00:11:57,839 --> 00:12:01,600
between compilers is really minor though

00:12:00,320 --> 00:12:06,000
apart from the

00:12:01,600 --> 00:12:06,000
lu decomposition example

00:12:06,320 --> 00:12:13,760
so moving on to the fujitsu a64 fx

00:12:10,880 --> 00:12:15,120
this time the compilers are hpe craze

00:12:13,760 --> 00:12:18,399
cce

00:12:15,120 --> 00:12:21,600
the arm compiler and gcc here

00:12:18,399 --> 00:12:23,820
please note that we're using omp

00:12:21,600 --> 00:12:25,040
num threads set to

00:12:23,820 --> 00:12:28,720
[Music]

00:12:25,040 --> 00:12:30,720
48 in all cases

00:12:28,720 --> 00:12:32,720
firstly notice that the overall

00:12:30,720 --> 00:12:33,839
performance for each benchmark is

00:12:32,720 --> 00:12:37,519
different from the

00:12:33,839 --> 00:12:41,440
marvel thunder x2 results some are

00:12:37,519 --> 00:12:44,959
better on the thunder x2 for example

00:12:41,440 --> 00:12:48,240
um the bt uh benchmark

00:12:44,959 --> 00:12:51,440
that is giving um 169

00:12:48,240 --> 00:12:52,240
m ups per second on thunder x2 against

00:12:51,440 --> 00:12:57,200
um

00:12:52,240 --> 00:13:00,240
130 on the a64fx

00:12:57,200 --> 00:13:00,839
and some results are actually better on

00:13:00,240 --> 00:13:03,920
the

00:13:00,839 --> 00:13:07,360
a64fx such as

00:13:03,920 --> 00:13:10,959
the mg benchmark

00:13:07,360 --> 00:13:14,320
for multi-grid showing 153 m

00:13:10,959 --> 00:13:17,360
per second against um 25

00:13:14,320 --> 00:13:19,360
on the thunder x2 secondly there is

00:13:17,360 --> 00:13:21,839
really no overall best choice of

00:13:19,360 --> 00:13:21,839
compiler

00:13:24,399 --> 00:13:30,160
so here we have the result for the best

00:13:27,120 --> 00:13:32,639
for each individual benchmark

00:13:30,160 --> 00:13:33,519
in terms of compiler choice gcc is

00:13:32,639 --> 00:13:36,639
better for some

00:13:33,519 --> 00:13:40,079
cce for others and arm seems to do well

00:13:36,639 --> 00:13:44,240
for one of the diagonal

00:13:40,079 --> 00:13:48,639
solvers so moving on to the

00:13:44,240 --> 00:13:51,440
amazon aws graviton 2 processor um

00:13:48,639 --> 00:13:52,240
we we're gonna go back to using the

00:13:51,440 --> 00:13:55,279
nvidia

00:13:52,240 --> 00:13:56,880
hpc sdk compiler the arm compiler and

00:13:55,279 --> 00:13:59,760
gcc

00:13:56,880 --> 00:14:01,040
um we're also going back to setting um

00:13:59,760 --> 00:14:05,040
just like we did on the

00:14:01,040 --> 00:14:09,440
marvel thunder x2 omp num threads um

00:14:05,040 --> 00:14:11,199
to be 64. um the first thing to notice

00:14:09,440 --> 00:14:14,320
about the results is that

00:14:11,199 --> 00:14:16,399
overall this processor is showing best

00:14:14,320 --> 00:14:17,839
performance if we look at the figures

00:14:16,399 --> 00:14:20,959
for

00:14:17,839 --> 00:14:20,959
m ops per second

00:14:22,320 --> 00:14:29,440
so here um just as we've seen for the

00:14:25,839 --> 00:14:31,040
marvel thunder x2 in terms of compiler

00:14:29,440 --> 00:14:33,540
choice it's really between

00:14:31,040 --> 00:14:34,959
gcc and arm um

00:14:33,540 --> 00:14:37,600
[Music]

00:14:34,959 --> 00:14:39,680
you know arm is better for some of the

00:14:37,600 --> 00:14:42,880
benchmarks and gcc

00:14:39,680 --> 00:14:42,880
better for the others

00:14:42,959 --> 00:14:46,720
so to summarize the results in terms of

00:14:44,880 --> 00:14:49,760
the best choice of architecture

00:14:46,720 --> 00:14:50,959
and compiler for each benchmark um for

00:14:49,760 --> 00:14:53,760
the eight benchmarks

00:14:50,959 --> 00:14:55,440
it's really between gcc or the arm

00:14:53,760 --> 00:14:58,959
compiler on

00:14:55,440 --> 00:15:02,399
um aws graviton 2 or marvel

00:14:58,959 --> 00:15:06,959
thunder x2 and between cce

00:15:02,399 --> 00:15:10,839
or gcc on the fujitsu a64 fx

00:15:06,959 --> 00:15:14,160
on the aws graviton 2 with gcc

00:15:10,839 --> 00:15:15,199
um you you can get best overall

00:15:14,160 --> 00:15:18,079
performance

00:15:15,199 --> 00:15:18,800
except for for um the odd benchmark

00:15:18,079 --> 00:15:23,120
where

00:15:18,800 --> 00:15:26,639
for example um the fourier benchmark

00:15:23,120 --> 00:15:31,600
um seems to do best on the a64fx with

00:15:26,639 --> 00:15:35,680
cce the multigrid benchmark

00:15:31,600 --> 00:15:38,639
seems to do well on the a64fx with gcc

00:15:35,680 --> 00:15:38,639
and finally the

00:15:38,720 --> 00:15:42,560
the sp diagonal solver benchmark seems

00:15:42,000 --> 00:15:46,000
to do

00:15:42,560 --> 00:15:49,120
best overall on the marvel thunder x2

00:15:46,000 --> 00:15:49,120
with the arm compiler

00:15:50,800 --> 00:15:54,079
so on to the second of the two

00:15:52,639 --> 00:15:58,560
benchmarks this

00:15:54,079 --> 00:16:00,639
is the epcc openmp micro benchmark suite

00:15:58,560 --> 00:16:02,240
and this was first developed around 20

00:16:00,639 --> 00:16:05,360
years ago

00:16:02,240 --> 00:16:07,040
it was updated nearly 10 years ago and

00:16:05,360 --> 00:16:09,519
it's in its current form

00:16:07,040 --> 00:16:11,519
it consists of a set of tests which

00:16:09,519 --> 00:16:14,120
measure the overhead of various

00:16:11,519 --> 00:16:15,440
openmp directives such as

00:16:14,120 --> 00:16:18,480
synchronization

00:16:15,440 --> 00:16:22,000
loop scheduling task scheduling

00:16:18,480 --> 00:16:24,560
and thread private data movement as such

00:16:22,000 --> 00:16:26,240
it consists of four main kernels and

00:16:24,560 --> 00:16:28,959
we're going to run these kernels

00:16:26,240 --> 00:16:29,759
on the same architectures using the same

00:16:28,959 --> 00:16:32,880
compilers

00:16:29,759 --> 00:16:36,720
that we encountered for the

00:16:32,880 --> 00:16:38,320
nas parallel benchmarks

00:16:36,720 --> 00:16:40,320
so overall there isn't that much

00:16:38,320 --> 00:16:43,600
difference in the overheads between

00:16:40,320 --> 00:16:45,600
the compilers and systems we are talking

00:16:43,600 --> 00:16:48,399
just microseconds here

00:16:45,600 --> 00:16:49,600
gcc seems to have a higher overhead than

00:16:48,399 --> 00:16:52,399
the other compilers

00:16:49,600 --> 00:16:52,720
and this is more evident for for tasks

00:16:52,399 --> 00:16:56,160
and

00:16:52,720 --> 00:16:58,079
private for scheduling it doesn't really

00:16:56,160 --> 00:17:00,720
matter which compiler

00:16:58,079 --> 00:17:01,600
and the arm compiler seems to do well at

00:17:00,720 --> 00:17:05,120
tasks on

00:17:01,600 --> 00:17:06,160
all three systems in general unless your

00:17:05,120 --> 00:17:08,720
application is

00:17:06,160 --> 00:17:09,439
highly sensitive to openmp then there's

00:17:08,720 --> 00:17:11,360
likely to be

00:17:09,439 --> 00:17:13,520
some other area of performance to be

00:17:11,360 --> 00:17:16,000
more concerned about

00:17:13,520 --> 00:17:17,199
but these benchmarks give a quick guide

00:17:16,000 --> 00:17:20,480
of what can happen

00:17:17,199 --> 00:17:23,280
within the openmp directives

00:17:20,480 --> 00:17:25,679
so that now summarizes the results and

00:17:23,280 --> 00:17:25,679
discuss

00:17:27,120 --> 00:17:32,160
so before i finish i'd just like to

00:17:28,799 --> 00:17:34,400
mention a few tips and recommendations

00:17:32,160 --> 00:17:38,160
firstly choose a compiler that gives the

00:17:34,400 --> 00:17:41,039
best results with your application

00:17:38,160 --> 00:17:42,799
and please try and use libraries that

00:17:41,039 --> 00:17:44,960
someone else has already spent their

00:17:42,799 --> 00:17:46,640
time tuning and optimizing

00:17:44,960 --> 00:17:49,600
there are plenty of good ones out there

00:17:46,640 --> 00:17:53,930
to try especially if you're using

00:17:49,600 --> 00:17:55,360
openmp multi-thread aware environments

00:17:53,930 --> 00:17:58,080
[Music]

00:17:55,360 --> 00:17:58,880
now on isembard please take care when

00:17:58,080 --> 00:18:01,280
pinning

00:17:58,880 --> 00:18:02,080
your openmp threads to the physical

00:18:01,280 --> 00:18:07,120
cores

00:18:02,080 --> 00:18:10,480
um bear in mind that smt is set to four

00:18:07,120 --> 00:18:13,360
so um there are potentially 256

00:18:10,480 --> 00:18:18,080
available cpus and this is coming from

00:18:13,360 --> 00:18:22,080
64 physical cpu cores

00:18:18,080 --> 00:18:25,280
um so more often than not

00:18:22,080 --> 00:18:25,679
only using the 64 physical cpu cores

00:18:25,280 --> 00:18:28,480
will

00:18:25,679 --> 00:18:28,960
give you the best performance so on that

00:18:28,480 --> 00:18:32,080
note

00:18:28,960 --> 00:18:35,120
i'd recommend to set omp

00:18:32,080 --> 00:18:38,640
num threads to 64. um use

00:18:35,120 --> 00:18:42,559
omp places um as cores

00:18:38,640 --> 00:18:44,240
and then set omp prop bind to close

00:18:42,559 --> 00:18:48,000
otherwise you're likely to see a

00:18:44,240 --> 00:18:48,000
dramatic reduction in performance

00:18:49,440 --> 00:18:54,080
another tip i guess is to check what

00:18:51,840 --> 00:18:57,520
resources your job scheduler

00:18:54,080 --> 00:19:00,799
has actually assigned um as a guide

00:18:57,520 --> 00:19:01,919
don't rely up upon it to pin your openmp

00:19:00,799 --> 00:19:04,640
threads

00:19:01,919 --> 00:19:05,280
sometimes job schedulers can get mixed

00:19:04,640 --> 00:19:09,440
up with

00:19:05,280 --> 00:19:11,970
openmp runtimes um so please check

00:19:09,440 --> 00:19:13,200
one way of doing this is to look at a um

00:19:11,970 --> 00:19:16,880
[Music]

00:19:13,200 --> 00:19:21,840
a small c code called xd high

00:19:16,880 --> 00:19:21,840
you can basically run that and it will

00:19:22,240 --> 00:19:26,960
turn out the thread pinning information

00:19:25,679 --> 00:19:30,559
and task

00:19:26,960 --> 00:19:31,840
assignment info as well so it's a small

00:19:30,559 --> 00:19:34,799
application

00:19:31,840 --> 00:19:34,799
but very useful

00:19:35,840 --> 00:19:40,240
so finally i just like to note that

00:19:38,400 --> 00:19:44,160
there are some very useful guides and

00:19:40,240 --> 00:19:46,320
reference materials available online

00:19:44,160 --> 00:19:48,320
and i'd be very happy to hear from

00:19:46,320 --> 00:19:50,559
anyone if there are any questions or

00:19:48,320 --> 00:19:51,520
suggestions so please do get in touch

00:19:50,559 --> 00:19:52,799
with me

00:19:51,520 --> 00:19:55,520
finally thank you very much for

00:19:52,799 --> 00:19:55,520
listening today

00:20:01,280 --> 00:20:04,880
thanks a lot phil that was really

00:20:03,039 --> 00:20:05,679
interesting nice to see the overview of

00:20:04,880 --> 00:20:09,200
the

00:20:05,679 --> 00:20:13,200
different cpus within the arm ecosystem

00:20:09,200 --> 00:20:15,760
and your results of of benchmarking

00:20:13,200 --> 00:20:17,919
some openmp benchmarks across all of

00:20:15,760 --> 00:20:19,520
them with different compilers

00:20:17,919 --> 00:20:20,960
i had a quick question for you actually

00:20:19,520 --> 00:20:24,400
if that's all right um

00:20:20,960 --> 00:20:27,360
what is um arm's plan with uh supporting

00:20:24,400 --> 00:20:28,559
openmp and the latest versions of openmp

00:20:27,360 --> 00:20:31,760
that we're going to hear

00:20:28,559 --> 00:20:32,480
um about uh later on from from michael

00:20:31,760 --> 00:20:36,320
clem

00:20:32,480 --> 00:20:40,159
um in in the arm uh hpc compilers

00:20:36,320 --> 00:20:43,679
well um yeah so you may be aware that we

00:20:40,159 --> 00:20:46,840
we are quite active in the open source

00:20:43,679 --> 00:20:51,039
communities of both gcc and

00:20:46,840 --> 00:20:53,520
llvm so um whatever we do

00:20:51,039 --> 00:20:54,320
kind of internally and whatever goes

00:20:53,520 --> 00:20:56,720
into

00:20:54,320 --> 00:20:58,080
our own commercial offering will

00:20:56,720 --> 00:21:00,559
hopefully eventually

00:20:58,080 --> 00:21:01,280
be upstreamed into the open source

00:21:00,559 --> 00:21:04,799
variants

00:21:01,280 --> 00:21:07,600
the obviously be a time delay um

00:21:04,799 --> 00:21:08,320
but yes certainly we are very active we

00:21:07,600 --> 00:21:11,200
have

00:21:08,320 --> 00:21:12,559
um three uh sorry i have three

00:21:11,200 --> 00:21:15,840
colleagues who

00:21:12,559 --> 00:21:17,120
are actively involved in the committees

00:21:15,840 --> 00:21:20,320
um

00:21:17,120 --> 00:21:20,880
so yeah it's it it will be supported in

00:21:20,320 --> 00:21:25,360
in the

00:21:20,880 --> 00:21:25,360
in the near term hopefully yeah

00:21:26,000 --> 00:21:29,760
that's great uh thanks a lot for the the

00:21:27,919 --> 00:21:32,320
talk phil so it's really nice to hear

00:21:29,760 --> 00:21:32,320

YouTube URL: https://www.youtube.com/watch?v=nsn0x7gwKvE


