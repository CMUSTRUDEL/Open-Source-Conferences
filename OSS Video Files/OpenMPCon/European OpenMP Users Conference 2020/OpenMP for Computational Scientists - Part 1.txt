Title: OpenMP for Computational Scientists - Part 1
Publication date: 2020-12-21
Playlist: European OpenMP Users Conference 2020
Description: 
	This tutorial was presented at the 3rd European OpenMP Users Conference in 2020

Presented by : Tom Deakin, University of Bristol

Additional Materials (Slides and code): https://github.com/UoB-HPC/openmp-for-cs

Conference Website: https://openmpusers.org
Follow us: https://twitter.com/openmp_users 

Presentation Abstract :
In the first part, we’ll introduce shared memory programming for multi-core CPUs using OpenMP. The most common parts of OpenMP will be explained alongside hands-on exercises for attendees to try for themselves. We’ll discuss some important performance optimisations to consider when writing shared memory programs.
Captions: 
	00:00:05,200 --> 00:00:07,359
okay

00:00:05,680 --> 00:00:09,599
good morning everybody i think we'll get

00:00:07,359 --> 00:00:10,800
started now and hopefully you can all

00:00:09,599 --> 00:00:13,840
see the screen um

00:00:10,800 --> 00:00:16,240
welcome to the uh openmp users group

00:00:13,840 --> 00:00:16,960
um and today we're going to be um going

00:00:16,240 --> 00:00:20,080
through the

00:00:16,960 --> 00:00:24,000
tutorial that um i've written about

00:00:20,080 --> 00:00:26,560
um openmp and it's really designed at um

00:00:24,000 --> 00:00:28,160
been designed to teach computational

00:00:26,560 --> 00:00:29,599
scientists people that are familiar with

00:00:28,160 --> 00:00:32,640
fortran and maybe

00:00:29,599 --> 00:00:34,480
mpi um the basics of

00:00:32,640 --> 00:00:35,760
of the openmp common core so that you're

00:00:34,480 --> 00:00:37,680
able to get

00:00:35,760 --> 00:00:38,960
up and running writing parallel programs

00:00:37,680 --> 00:00:42,079
on cpus

00:00:38,960 --> 00:00:46,719
and then also um the features

00:00:42,079 --> 00:00:48,480
in openmp to support gpus as well

00:00:46,719 --> 00:00:50,160
so once we've gone through all the

00:00:48,480 --> 00:00:51,039
information about cpus this morning then

00:00:50,160 --> 00:00:54,320
we'll move on to

00:00:51,039 --> 00:00:54,320
gpus this afternoon

00:00:54,719 --> 00:00:59,440
so today we're going to be focusing

00:00:56,559 --> 00:01:01,520
primarily on openmp4.5

00:00:59,440 --> 00:01:02,640
and that's what has the best support in

00:01:01,520 --> 00:01:04,799
all the compilers

00:01:02,640 --> 00:01:05,680
at the moment many of the compilers do

00:01:04,799 --> 00:01:09,040
have support

00:01:05,680 --> 00:01:10,240
for um some 5.0 features and i'll talk a

00:01:09,040 --> 00:01:12,960
little bit about um

00:01:10,240 --> 00:01:14,479
some some important 5.0 features um at

00:01:12,960 --> 00:01:17,200
the end of the day

00:01:14,479 --> 00:01:18,799
um 5.0 for most of the things that we'll

00:01:17,200 --> 00:01:20,240
cover today really just has some

00:01:18,799 --> 00:01:21,840
clarifications and some

00:01:20,240 --> 00:01:23,520
corner cases and and some of the rough

00:01:21,840 --> 00:01:24,880
edges shaved off

00:01:23,520 --> 00:01:27,920
and for the content that i'm going to

00:01:24,880 --> 00:01:30,240
include so although it's 4.5 this

00:01:27,920 --> 00:01:31,119
this is going to be applicable um as

00:01:30,240 --> 00:01:32,720
these are you know

00:01:31,119 --> 00:01:34,640
key concepts right at the heart of

00:01:32,720 --> 00:01:35,680
openmp

00:01:34,640 --> 00:01:37,920
we're going to be covering a lot of

00:01:35,680 --> 00:01:39,280
material today so it's a sort of buck

00:01:37,920 --> 00:01:41,119
with your seatbelts type

00:01:39,280 --> 00:01:43,200
tutorial but it is hands-on there's

00:01:41,119 --> 00:01:45,200
plenty of time scheduled for you to try

00:01:43,200 --> 00:01:47,680
out openmp yourselves

00:01:45,200 --> 00:01:48,720
both on cpus and on gpus and and we've

00:01:47,680 --> 00:01:50,159
been um

00:01:48,720 --> 00:01:51,439
very lucky to have access to a

00:01:50,159 --> 00:01:53,280
supercomputer that we're going to give

00:01:51,439 --> 00:01:55,520
you all access to for today

00:01:53,280 --> 00:01:57,200
um in order to try out um these

00:01:55,520 --> 00:01:58,719
exercises

00:01:57,200 --> 00:02:00,640
so we're going to have a mixture of of

00:01:58,719 --> 00:02:03,200
some uh some lectures me

00:02:00,640 --> 00:02:04,799
explaining all the things about openmp

00:02:03,200 --> 00:02:06,159
um followed by

00:02:04,799 --> 00:02:08,080
a number of exercises we're going to

00:02:06,159 --> 00:02:10,560
have um about

00:02:08,080 --> 00:02:12,560
five or six exercises um throughout the

00:02:10,560 --> 00:02:15,040
day

00:02:12,560 --> 00:02:16,560
so the point of the exercises is is for

00:02:15,040 --> 00:02:18,480
you to sort of have a go at

00:02:16,560 --> 00:02:21,120
some active learning to try the things

00:02:18,480 --> 00:02:23,680
that i've explained during the

00:02:21,120 --> 00:02:24,720
during the sort of lecture style parts

00:02:23,680 --> 00:02:26,800
of the

00:02:24,720 --> 00:02:28,959
um of the day and really for you to

00:02:26,800 --> 00:02:30,720
experiment with openmp and the codes and

00:02:28,959 --> 00:02:32,080
and have fun and explore what they can

00:02:30,720 --> 00:02:33,920
do

00:02:32,080 --> 00:02:36,239
the codes themselves are very simple

00:02:33,920 --> 00:02:37,440
they just have some quite basic fortran

00:02:36,239 --> 00:02:38,400
it's all going to be a five point

00:02:37,440 --> 00:02:40,160
stencil

00:02:38,400 --> 00:02:42,000
and they're not really there to sort of

00:02:40,160 --> 00:02:43,840
highlight the all the corner cases and

00:02:42,000 --> 00:02:45,440
the complexities that you might run into

00:02:43,840 --> 00:02:46,800
with a real

00:02:45,440 --> 00:02:48,720
production code but they're there to

00:02:46,800 --> 00:02:50,720
help you

00:02:48,720 --> 00:02:53,040
apply the things that you have been been

00:02:50,720 --> 00:02:55,680
learning about in in the lectures

00:02:53,040 --> 00:02:57,680
um so we only assume sort of very basic

00:02:55,680 --> 00:02:59,360
working knowledge of fortran essentially

00:02:57,680 --> 00:03:02,159
if you can read a do loop and

00:02:59,360 --> 00:03:04,239
and allocate and update some arrays

00:03:02,159 --> 00:03:06,000
that's pretty much all you need to know

00:03:04,239 --> 00:03:07,440
and if you've used some mpi before

00:03:06,000 --> 00:03:08,879
that's that's useful but we're going to

00:03:07,440 --> 00:03:12,239
talk a little bit about

00:03:08,879 --> 00:03:14,080
parallelism in general so um there are

00:03:12,239 --> 00:03:16,319
solutions provided as well

00:03:14,080 --> 00:03:17,680
and but you can only look at those as a

00:03:16,319 --> 00:03:19,440
last resort it's worth sort of

00:03:17,680 --> 00:03:20,000
struggling through and trying to figure

00:03:19,440 --> 00:03:21,680
out

00:03:20,000 --> 00:03:23,200
what what's going on with your own

00:03:21,680 --> 00:03:26,159
implementation that's one of the best

00:03:23,200 --> 00:03:26,159
ways that you can learn

00:03:28,080 --> 00:03:31,760
so all the material including the code

00:03:30,480 --> 00:03:33,599
and the slides

00:03:31,760 --> 00:03:36,080
is is available for you to download on

00:03:33,599 --> 00:03:38,400
github and the link is here

00:03:36,080 --> 00:03:40,400
and um during the day i'll post a link

00:03:38,400 --> 00:03:43,280
in the in the chat as well

00:03:40,400 --> 00:03:45,200
on on zoom so it's worth downloading a

00:03:43,280 --> 00:03:47,040
copy of that for yourselves the pdfs of

00:03:45,200 --> 00:03:48,799
the slides are there as well so you can

00:03:47,040 --> 00:03:50,080
follow along and see all the extra

00:03:48,799 --> 00:03:52,080
instructions

00:03:50,080 --> 00:03:53,360
um for accessing the supercomputer

00:03:52,080 --> 00:03:56,400
alongside the

00:03:53,360 --> 00:03:56,400
the exercise slides

00:03:57,599 --> 00:04:03,280
so today we've been um kind enough to

00:04:01,280 --> 00:04:04,319
be allowed access to isn't bars and

00:04:03,280 --> 00:04:07,680
izanbard is a

00:04:04,319 --> 00:04:09,840
um a year a uk tier 2 supercomputer

00:04:07,680 --> 00:04:11,439
so it's one of the machines that's

00:04:09,840 --> 00:04:13,599
available nationally to

00:04:11,439 --> 00:04:14,480
those of us in the uk and it's this

00:04:13,599 --> 00:04:17,759
collaboration

00:04:14,480 --> 00:04:19,759
between the gw4 so bristol bath

00:04:17,759 --> 00:04:21,120
exeter and cardiff universities along

00:04:19,759 --> 00:04:24,000
with the uk met office

00:04:21,120 --> 00:04:25,280
where the machine is physically housed

00:04:24,000 --> 00:04:28,479
along with craig hpe

00:04:25,280 --> 00:04:30,479
arm and epsrc who have very kindly

00:04:28,479 --> 00:04:33,919
funded this project

00:04:30,479 --> 00:04:35,680
so this was one of the very first um arm

00:04:33,919 --> 00:04:37,520
production supercomputers and it's now

00:04:35,680 --> 00:04:40,639
got over 21 000

00:04:37,520 --> 00:04:42,720
armed cores and alongside that it has

00:04:40,639 --> 00:04:43,919
a number of cpus and gpus all from

00:04:42,720 --> 00:04:46,639
different vendors

00:04:43,919 --> 00:04:48,400
and they are there um to enable us to

00:04:46,639 --> 00:04:49,360
make lots of architectural comparisons

00:04:48,400 --> 00:04:52,000
that's one of the sort of

00:04:49,360 --> 00:04:53,600
usps of the isenbart project and today

00:04:52,000 --> 00:04:55,120
we're going to be using some of those

00:04:53,600 --> 00:04:57,600
cpus and gpus so

00:04:55,120 --> 00:04:59,680
we're going to be using the broadwell

00:04:57,600 --> 00:05:03,120
node so they're two socket

00:04:59,680 --> 00:05:04,880
intel xeon broadwell um cpus with 18

00:05:03,120 --> 00:05:06,880
cores per socket

00:05:04,880 --> 00:05:08,479
and attached to those are some nvidia

00:05:06,880 --> 00:05:10,479
p100 gpus

00:05:08,479 --> 00:05:12,720
so this afternoon we'll be using the gpu

00:05:10,479 --> 00:05:13,360
part and and this morning we'll focus on

00:05:12,720 --> 00:05:16,080
the

00:05:13,360 --> 00:05:17,520
um the broadwell part but they are the

00:05:16,080 --> 00:05:18,880
same physical notes

00:05:17,520 --> 00:05:20,880
so obviously thanks go to steiner

00:05:18,880 --> 00:05:23,360
macintosh smith and the team at bristol

00:05:20,880 --> 00:05:24,960
who helped helped us get this tutorial

00:05:23,360 --> 00:05:26,479
up and running in particular chris edsol

00:05:24,960 --> 00:05:28,960
was fantastic in helping get

00:05:26,479 --> 00:05:31,360
the izumba set up ready for us to go

00:05:28,960 --> 00:05:31,360
today

00:05:31,919 --> 00:05:35,199
so this is just a quick agenda and this

00:05:33,919 --> 00:05:36,800
is why it might be useful for you to

00:05:35,199 --> 00:05:39,039
download the pdfs yourself

00:05:36,800 --> 00:05:40,240
so you can keep an eye on this so this

00:05:39,039 --> 00:05:40,880
morning we're going to be walking

00:05:40,240 --> 00:05:43,759
through

00:05:40,880 --> 00:05:44,320
um openmp for cpus so we're going to

00:05:43,759 --> 00:05:47,600
start with

00:05:44,320 --> 00:05:48,080
a serial fortran code and by lunchtime

00:05:47,600 --> 00:05:50,240
by

00:05:48,080 --> 00:05:52,160
in the next three hours we'll be having

00:05:50,240 --> 00:05:54,240
hopefully an optimized

00:05:52,160 --> 00:05:55,919
five-point stencil running really well

00:05:54,240 --> 00:05:58,479
on a cpu

00:05:55,919 --> 00:05:59,919
so we've got three exercises here um

00:05:58,479 --> 00:06:00,720
we're first going to parallelize the

00:05:59,919 --> 00:06:03,039
code

00:06:00,720 --> 00:06:04,800
and we're then going to start looking at

00:06:03,039 --> 00:06:06,240
reductions and the data sharing might we

00:06:04,800 --> 00:06:08,240
might have to worry about

00:06:06,240 --> 00:06:09,520
and finally we'll look at some ways to

00:06:08,240 --> 00:06:12,639
optimize um

00:06:09,520 --> 00:06:14,479
performance for for cpus then after

00:06:12,639 --> 00:06:16,800
lunch we're going to move on to

00:06:14,479 --> 00:06:18,240
gpus and we're going to now be able to

00:06:16,800 --> 00:06:20,960
transfer our

00:06:18,240 --> 00:06:23,520
cpu code onto the gpu and get it running

00:06:20,960 --> 00:06:24,880
there and and move our data around

00:06:23,520 --> 00:06:27,280
and then we're gonna look a little bit

00:06:24,880 --> 00:06:27,919
about um the parallelism that might be

00:06:27,280 --> 00:06:31,440
there

00:06:27,919 --> 00:06:33,360
how you express that on gpus and then um

00:06:31,440 --> 00:06:35,919
how you optimize the data movement

00:06:33,360 --> 00:06:37,120
between the cpu and the gpu so there's a

00:06:35,919 --> 00:06:39,039
lot to cover

00:06:37,120 --> 00:06:40,479
um but with lots of exercises there's

00:06:39,039 --> 00:06:41,039
going to be lots of time in which you

00:06:40,479 --> 00:06:43,120
can

00:06:41,039 --> 00:06:44,240
try these out all these techniques out

00:06:43,120 --> 00:06:45,919
for yourself

00:06:44,240 --> 00:06:47,440
so we'll be finishing at harpers4 all

00:06:45,919 --> 00:06:49,759
these times uh uk

00:06:47,440 --> 00:06:52,319
times if you're dialing in from

00:06:49,759 --> 00:06:52,319
elsewhere

00:06:53,039 --> 00:06:56,880
so thanks to everybody that has helped

00:06:55,440 --> 00:06:58,160
us out with with getting this tutorial

00:06:56,880 --> 00:07:00,319
ready and lots of

00:06:58,160 --> 00:07:02,400
these uh this material has all been sort

00:07:00,319 --> 00:07:02,720
of synthesized and inspired by lots of

00:07:02,400 --> 00:07:05,520
other

00:07:02,720 --> 00:07:06,000
openmp tutorials so with that i'm going

00:07:05,520 --> 00:07:08,639
to

00:07:06,000 --> 00:07:09,680
delve straight into the first set of

00:07:08,639 --> 00:07:11,199
slides

00:07:09,680 --> 00:07:13,840
so i'll just i'll just change and get

00:07:11,199 --> 00:07:14,880
that ready a work sharing this is the

00:07:13,840 --> 00:07:17,280
sort of

00:07:14,880 --> 00:07:18,639
heart of openmp and explains the the

00:07:17,280 --> 00:07:22,479
parallel model

00:07:18,639 --> 00:07:26,000
um that openmp um kind of works under

00:07:22,479 --> 00:07:28,560
so let's start with with what is openmp

00:07:26,000 --> 00:07:30,000
well openmp is this collection of

00:07:28,560 --> 00:07:31,680
compiler directives

00:07:30,000 --> 00:07:34,240
and library routines and environment

00:07:31,680 --> 00:07:35,120
variables that allow us to express

00:07:34,240 --> 00:07:38,160
parallelism

00:07:35,120 --> 00:07:40,400
on shared memory systems so these are

00:07:38,160 --> 00:07:42,560
systems where there is

00:07:40,400 --> 00:07:44,560
lots of parallel things that might be

00:07:42,560 --> 00:07:45,599
running and they can all access the

00:07:44,560 --> 00:07:47,840
memory of the

00:07:45,599 --> 00:07:49,520
of the other parallel things so we might

00:07:47,840 --> 00:07:51,039
have cause for instance and they could

00:07:49,520 --> 00:07:54,479
all see the main memory

00:07:51,039 --> 00:07:57,440
of the of the node and all those calls

00:07:54,479 --> 00:07:59,120
can access all that memory

00:07:57,440 --> 00:08:01,199
so by compiler directories we mean

00:07:59,120 --> 00:08:02,160
things that we might annotate our source

00:08:01,199 --> 00:08:04,639
code with

00:08:02,160 --> 00:08:06,240
um to enable us to express um

00:08:04,639 --> 00:08:07,280
instructions to the compiler or the

00:08:06,240 --> 00:08:09,360
runtime

00:08:07,280 --> 00:08:11,039
or change the meaning of the programs in

00:08:09,360 --> 00:08:12,400
in some way to allow us to express the

00:08:11,039 --> 00:08:14,160
parallelism that we want

00:08:12,400 --> 00:08:16,000
and the compiler is then going to take

00:08:14,160 --> 00:08:17,599
those directives and generate

00:08:16,000 --> 00:08:19,440
whatever it needs to behind the scenes

00:08:17,599 --> 00:08:21,120
in order to provide that functionality

00:08:19,440 --> 00:08:23,199
for us

00:08:21,120 --> 00:08:24,879
now much of openmp you can also express

00:08:23,199 --> 00:08:28,240
with library routines so

00:08:24,879 --> 00:08:30,400
regular api calls and we'll talk about a

00:08:28,240 --> 00:08:31,680
few of those as well

00:08:30,400 --> 00:08:33,200
there's also a set of standard

00:08:31,680 --> 00:08:34,880
environment variables that are used to

00:08:33,200 --> 00:08:36,320
set a few things like the number of

00:08:34,880 --> 00:08:39,360
parallel threads for instance

00:08:36,320 --> 00:08:41,839
and and they allow us to write

00:08:39,360 --> 00:08:43,519
portable programs with our directives

00:08:41,839 --> 00:08:45,279
and then use the environment variables

00:08:43,519 --> 00:08:47,600
to some degree to help specify

00:08:45,279 --> 00:08:50,800
and specialize the programs to exactly

00:08:47,600 --> 00:08:50,800
the hardware that we're running on

00:08:51,519 --> 00:08:56,560
so the idea here is that we have to

00:08:54,800 --> 00:08:58,160
direct the powers and we have to

00:08:56,560 --> 00:08:59,600
look at our programs find where the

00:08:58,160 --> 00:09:01,600
parallelism could be

00:08:59,600 --> 00:09:05,200
and use these compiler directives in

00:09:01,600 --> 00:09:05,200
order to write that down

00:09:05,279 --> 00:09:09,760
um so openmp really is this this

00:09:07,839 --> 00:09:13,279
specification it starts off as

00:09:09,760 --> 00:09:15,040
a pdf and it's um this is the open

00:09:13,279 --> 00:09:15,600
standard specification of all those

00:09:15,040 --> 00:09:17,360
things

00:09:15,600 --> 00:09:19,360
all those all those annotations the

00:09:17,360 --> 00:09:20,640
directives and api calls and environment

00:09:19,360 --> 00:09:22,240
variables

00:09:20,640 --> 00:09:24,240
and that you would like to make to your

00:09:22,240 --> 00:09:26,720
program in order to make it run in in

00:09:24,240 --> 00:09:26,720
parallel

00:09:26,880 --> 00:09:30,160
so these are the compiler directives and

00:09:28,800 --> 00:09:32,000
these are what they look like and

00:09:30,160 --> 00:09:33,200
and the notation i'm using here is

00:09:32,000 --> 00:09:35,680
fortran

00:09:33,200 --> 00:09:36,720
so we have the the sentence symbol and

00:09:35,680 --> 00:09:39,120
then omp

00:09:36,720 --> 00:09:40,080
this tells the compiler that it's going

00:09:39,120 --> 00:09:41,839
to be

00:09:40,080 --> 00:09:43,760
a compiler directive and it's coming

00:09:41,839 --> 00:09:45,920
from openmp

00:09:43,760 --> 00:09:47,600
then we'll have a construct so this is

00:09:45,920 --> 00:09:48,240
essentially a keyword a word that's

00:09:47,600 --> 00:09:50,160
found in

00:09:48,240 --> 00:09:51,680
in the openmp standards and it's going

00:09:50,160 --> 00:09:54,000
to tell the compiler to do

00:09:51,680 --> 00:09:56,000
something normally we'll insert some

00:09:54,000 --> 00:09:58,000
extra code on our behalf that does a

00:09:56,000 --> 00:10:00,160
particular thing

00:09:58,000 --> 00:10:02,160
we can then control maybe some some

00:10:00,160 --> 00:10:03,920
features of that construct with a number

00:10:02,160 --> 00:10:04,880
of clauses and and the clauses are

00:10:03,920 --> 00:10:07,200
optional

00:10:04,880 --> 00:10:10,800
um but we'd express one or more clauses

00:10:07,200 --> 00:10:10,800
maybe at the end of our construct

00:10:11,279 --> 00:10:15,120
these compiler directives usually apply

00:10:13,440 --> 00:10:16,880
to a structured block

00:10:15,120 --> 00:10:18,240
of of statements that's a structured

00:10:16,880 --> 00:10:19,920
block in the sense of a

00:10:18,240 --> 00:10:21,760
you know standard programming language

00:10:19,920 --> 00:10:23,279
uh terminology

00:10:21,760 --> 00:10:24,880
so because of the scoping rules in

00:10:23,279 --> 00:10:26,079
fortran where most things are just sort

00:10:24,880 --> 00:10:29,360
of single scope

00:10:26,079 --> 00:10:32,160
um in openmp we often have an

00:10:29,360 --> 00:10:34,399
end construct to match the um the

00:10:32,160 --> 00:10:38,079
construct as well so we can say

00:10:34,399 --> 00:10:39,839
um between the the construct and the end

00:10:38,079 --> 00:10:42,240
construct those are the things that the

00:10:39,839 --> 00:10:44,079
openmp is going to be applied to

00:10:42,240 --> 00:10:45,760
for instance with a with a do loop if we

00:10:44,079 --> 00:10:48,959
want to make it parallel

00:10:45,760 --> 00:10:49,279
we'd have a an omp parallel do and then

00:10:48,959 --> 00:10:53,279
a

00:10:49,279 --> 00:10:53,279
ompn parallel do for instance

00:10:53,440 --> 00:10:57,440
if you need to use the library api calls

00:10:55,839 --> 00:10:59,200
then you can just use them

00:10:57,440 --> 00:11:01,519
in the fortran using the use statement

00:10:59,200 --> 00:11:03,040
so use omp underscore lib

00:11:01,519 --> 00:11:04,720
and then you call them and all the

00:11:03,040 --> 00:11:07,040
functions start with the prefix

00:11:04,720 --> 00:11:08,079
omp underscore and then you just call

00:11:07,040 --> 00:11:12,240
that function

00:11:08,079 --> 00:11:12,240
and it would do whatever it needs to do

00:11:13,440 --> 00:11:17,920
so building with openmp once you've um

00:11:16,240 --> 00:11:19,360
littered your program with all these

00:11:17,920 --> 00:11:19,680
different compiler directories you need

00:11:19,360 --> 00:11:23,760
to

00:11:19,680 --> 00:11:27,040
instruct open the compiler in order to

00:11:23,760 --> 00:11:29,120
interpret them and do the right thing so

00:11:27,040 --> 00:11:30,560
on this slide is is the different

00:11:29,120 --> 00:11:31,360
compilers and the different flags you

00:11:30,560 --> 00:11:34,560
need to turn

00:11:31,360 --> 00:11:36,640
on openmp in the compiler now the

00:11:34,560 --> 00:11:37,920
key one for today is the cray compiler

00:11:36,640 --> 00:11:40,480
which is what we're going to be using

00:11:37,920 --> 00:11:41,040
primarily and that's because it's a cray

00:11:40,480 --> 00:11:42,959
system

00:11:41,040 --> 00:11:45,760
and that we're going to be using it also

00:11:42,959 --> 00:11:46,399
supports cpus and gpus so we have openmp

00:11:45,760 --> 00:11:50,000
across

00:11:46,399 --> 00:11:52,240
both kinds of hardware and for that the

00:11:50,000 --> 00:11:54,079
compiler is just called ftn for the

00:11:52,240 --> 00:11:57,120
fortran compiler and you'd give it your

00:11:54,079 --> 00:11:57,600
maybe f90 codes and then you'd use the

00:11:57,120 --> 00:12:01,120
flag

00:11:57,600 --> 00:12:03,680
minus h openmp um that's with the

00:12:01,120 --> 00:12:05,839
the cray uh fortran compiler in version

00:12:03,680 --> 00:12:08,000
10 that's what we're using today

00:12:05,839 --> 00:12:09,279
now those of you that are familiar with

00:12:08,000 --> 00:12:10,880
the cray compiler

00:12:09,279 --> 00:12:13,360
but maybe older versions you'll have

00:12:10,880 --> 00:12:15,040
found that in older versions you don't

00:12:13,360 --> 00:12:17,279
have to specify that you'd like to build

00:12:15,040 --> 00:12:19,920
with openmp well the behavior has now

00:12:17,279 --> 00:12:22,240
changed and the openmp is now off by

00:12:19,920 --> 00:12:24,240
default in the create compiler

00:12:22,240 --> 00:12:25,279
so just make sure you have the minus h

00:12:24,240 --> 00:12:28,720
omp flag

00:12:25,279 --> 00:12:28,720
when you're building the code as well

00:12:29,360 --> 00:12:33,360
if you need to use the api calls as i

00:12:31,120 --> 00:12:35,360
said you just use omp lib but if you're

00:12:33,360 --> 00:12:37,279
just using the directives you don't need

00:12:35,360 --> 00:12:38,160
to include the library you just need to

00:12:37,279 --> 00:12:41,360
include the flag

00:12:38,160 --> 00:12:41,360
on the command line

00:12:42,079 --> 00:12:46,720
specifically the the library using the

00:12:44,480 --> 00:12:48,639
library only gets you the the api calls

00:12:46,720 --> 00:12:49,760
you still need the the compiler flag to

00:12:48,639 --> 00:12:51,360
use the

00:12:49,760 --> 00:12:53,600
any of the compiler directors which is

00:12:51,360 --> 00:12:56,399
the the primary way that we program with

00:12:53,600 --> 00:12:56,399
with openmp

00:12:56,800 --> 00:13:00,240
so this is shed memory this is this is

00:12:58,560 --> 00:13:02,480
what it looks like we have

00:13:00,240 --> 00:13:03,440
two sockets here and this is just a sort

00:13:02,480 --> 00:13:05,519
of cartoon of

00:13:03,440 --> 00:13:06,639
of the node and we each of these two

00:13:05,519 --> 00:13:09,680
sockets has four

00:13:06,639 --> 00:13:11,680
cores numbered zero to three they're

00:13:09,680 --> 00:13:12,720
connected to each other by some sort of

00:13:11,680 --> 00:13:15,440
interconnect

00:13:12,720 --> 00:13:15,839
and they're connected to memory um which

00:13:15,440 --> 00:13:19,279
is the

00:13:15,839 --> 00:13:22,880
the main dram in our in our system

00:13:19,279 --> 00:13:24,560
so the sort of main computer memory

00:13:22,880 --> 00:13:27,200
what's important here is is we're

00:13:24,560 --> 00:13:31,279
operating under this shared memory model

00:13:27,200 --> 00:13:32,240
so any core can access any part of the

00:13:31,279 --> 00:13:34,000
memory

00:13:32,240 --> 00:13:35,600
so this means say in this sort of

00:13:34,000 --> 00:13:38,240
example um

00:13:35,600 --> 00:13:38,959
socket 0 core 2 might write something to

00:13:38,240 --> 00:13:40,800
memory

00:13:38,959 --> 00:13:42,720
and that's going to be visible according

00:13:40,800 --> 00:13:44,639
to maybe some rules but

00:13:42,720 --> 00:13:46,560
any other core on the other sockets or

00:13:44,639 --> 00:13:47,440
any cores on the same socket can still

00:13:46,560 --> 00:13:49,760
access that

00:13:47,440 --> 00:13:51,600
that memory location and eventually see

00:13:49,760 --> 00:13:54,959
that update

00:13:51,600 --> 00:13:56,880
now this is very different to mpi where

00:13:54,959 --> 00:13:58,000
processes cannot see the memory of

00:13:56,880 --> 00:14:00,160
another parallel

00:13:58,000 --> 00:14:01,519
thing without explicit communication

00:14:00,160 --> 00:14:03,920
between those two things

00:14:01,519 --> 00:14:05,920
you have to have an mpi send or receive

00:14:03,920 --> 00:14:06,320
in order to share data between the the

00:14:05,920 --> 00:14:09,440
two

00:14:06,320 --> 00:14:12,320
parallel um items but in openmp

00:14:09,440 --> 00:14:14,320
um you just have to you know we just

00:14:12,320 --> 00:14:16,800
have to notice that the

00:14:14,320 --> 00:14:20,320
all the memory is visible to all of the

00:14:16,800 --> 00:14:20,320
parallel elements on the node

00:14:21,600 --> 00:14:26,639
so that's the memory model in a nutshell

00:14:23,839 --> 00:14:28,639
and now on to the execution model

00:14:26,639 --> 00:14:30,240
so we can we can imagine serial

00:14:28,639 --> 00:14:31,120
execution might look like a straight

00:14:30,240 --> 00:14:33,199
arrow we just

00:14:31,120 --> 00:14:35,440
read each of the statements in our

00:14:33,199 --> 00:14:37,839
program from beginning to end one after

00:14:35,440 --> 00:14:40,639
each other

00:14:37,839 --> 00:14:43,199
openmp has this fork join model so the

00:14:40,639 --> 00:14:45,760
code starts off serial

00:14:43,199 --> 00:14:47,600
it then will reach a parallel statement

00:14:45,760 --> 00:14:49,760
where we'd like to fork a

00:14:47,600 --> 00:14:51,920
team of threads now there's there's a

00:14:49,760 --> 00:14:54,880
few openmp specific words but

00:14:51,920 --> 00:14:55,519
ultimately you you have a a number of

00:14:54,880 --> 00:14:58,240
parallel

00:14:55,519 --> 00:15:00,160
things which are going to be threads and

00:14:58,240 --> 00:15:01,839
they are grouped together in a concept

00:15:00,160 --> 00:15:03,839
called a teams

00:15:01,839 --> 00:15:05,199
later on with the gpus we'll see that we

00:15:03,839 --> 00:15:07,279
have leagues of teams so

00:15:05,199 --> 00:15:09,360
multiple teams with consisting of

00:15:07,279 --> 00:15:11,440
multiple threads

00:15:09,360 --> 00:15:13,920
but really we just create on typically

00:15:11,440 --> 00:15:17,600
on a cpu just one team so we have

00:15:13,920 --> 00:15:20,800
lots of threads that get forked so

00:15:17,600 --> 00:15:22,560
serial serial execution forks off into

00:15:20,800 --> 00:15:23,920
lots of different parallel streams and

00:15:22,560 --> 00:15:25,760
that's where we have our parallel

00:15:23,920 --> 00:15:28,399
execution

00:15:25,760 --> 00:15:30,480
and then later on we join all of those

00:15:28,399 --> 00:15:35,440
parallel threads back together again

00:15:30,480 --> 00:15:38,160
and we continue with serial execution

00:15:35,440 --> 00:15:40,240
so this is the the primary uh way that

00:15:38,160 --> 00:15:42,560
openmp is parallelized

00:15:40,240 --> 00:15:43,279
and this is again different to mpi where

00:15:42,560 --> 00:15:46,240
where mpi

00:15:43,279 --> 00:15:48,240
starts off in parallel and when we we

00:15:46,240 --> 00:15:50,000
launch our program with npi run we have

00:15:48,240 --> 00:15:52,079
lots of instances of our program

00:15:50,000 --> 00:15:54,399
so it's parallel from the get go whereas

00:15:52,079 --> 00:15:57,759
with openmp the process starts off

00:15:54,399 --> 00:15:58,720
just as a serial process it then creates

00:15:57,759 --> 00:16:00,720
the parallelism

00:15:58,720 --> 00:16:02,720
at a parallel statement runs in parallel

00:16:00,720 --> 00:16:05,839
and then synchronizes and joins back up

00:16:02,720 --> 00:16:07,759
and continues running in serial again

00:16:05,839 --> 00:16:10,000
so i will say that nested threads are

00:16:07,759 --> 00:16:10,720
allowed and it does exactly what you'd

00:16:10,000 --> 00:16:12,720
expect

00:16:10,720 --> 00:16:13,920
um each of the thread then creates its

00:16:12,720 --> 00:16:17,920
own teams of

00:16:13,920 --> 00:16:20,480
nested threads um so it is possible but

00:16:17,920 --> 00:16:22,000
the use cases can be um we haven't seen

00:16:20,480 --> 00:16:24,240
that many use cases for them

00:16:22,000 --> 00:16:25,360
and they're tricky to get often get

00:16:24,240 --> 00:16:27,199
right to make sure you get the right

00:16:25,360 --> 00:16:31,040
number of threads in the right place

00:16:27,199 --> 00:16:31,040
um on the on the processors

00:16:31,600 --> 00:16:35,440
so how do we go about creating openmp

00:16:33,839 --> 00:16:36,000
threads and this is the sort of hello

00:16:35,440 --> 00:16:39,519
world of

00:16:36,000 --> 00:16:42,480
how we do that so um say i want to

00:16:39,519 --> 00:16:44,880
have the the word the word hello printed

00:16:42,480 --> 00:16:48,000
by each of the parallel threads

00:16:44,880 --> 00:16:50,079
so my program starts off serially it

00:16:48,000 --> 00:16:52,480
then reaches on line three this

00:16:50,079 --> 00:16:53,120
omp parallel statement now this is the

00:16:52,480 --> 00:16:55,199
fork

00:16:53,120 --> 00:16:58,240
um that i said about on the previous

00:16:55,199 --> 00:17:00,240
slide this creates all those threads

00:16:58,240 --> 00:17:02,000
each of them each of those will then run

00:17:00,240 --> 00:17:02,720
whatever code is in that structured

00:17:02,000 --> 00:17:05,280
block so

00:17:02,720 --> 00:17:07,199
between line three and five where i have

00:17:05,280 --> 00:17:08,319
the end parallel that's the structured

00:17:07,199 --> 00:17:11,199
block

00:17:08,319 --> 00:17:11,919
and we we know that because i have a the

00:17:11,199 --> 00:17:14,240
um

00:17:11,919 --> 00:17:17,439
openmp end statement that that kind of

00:17:14,240 --> 00:17:18,799
segments my program that way

00:17:17,439 --> 00:17:20,959
so each thread will then execute

00:17:18,799 --> 00:17:22,559
whatever is in that parallel block and

00:17:20,959 --> 00:17:25,039
and it's convenient to think about this

00:17:22,559 --> 00:17:27,039
as almost redundant execution

00:17:25,039 --> 00:17:28,400
every thread executes exactly the same

00:17:27,039 --> 00:17:30,400
thing in that

00:17:28,400 --> 00:17:31,520
in that block so every thread is gonna

00:17:30,400 --> 00:17:35,039
is gonna call print

00:17:31,520 --> 00:17:36,720
with the string hello when i get to the

00:17:35,039 --> 00:17:38,640
end parallel my threads are gonna

00:17:36,720 --> 00:17:41,280
synchronize and they're gonna join back

00:17:38,640 --> 00:17:44,840
up so any code so by the time i get to

00:17:41,280 --> 00:17:47,840
line six i'm now back to my serial

00:17:44,840 --> 00:17:47,840
program

00:17:48,080 --> 00:17:51,600
so how many threads do i get well there

00:17:49,840 --> 00:17:54,160
are a number of ways to set this

00:17:51,600 --> 00:17:55,919
and and the primary way that i'd like to

00:17:54,160 --> 00:17:59,600
see everybody use that is with the

00:17:55,919 --> 00:18:01,679
omp num threads environment variable

00:17:59,600 --> 00:18:03,520
so you typically set this on your shell

00:18:01,679 --> 00:18:04,320
or or in the job submission scripts that

00:18:03,520 --> 00:18:05,840
you'll be running

00:18:04,320 --> 00:18:08,240
and that sets the number of threads that

00:18:05,840 --> 00:18:10,240
you'd like to launch

00:18:08,240 --> 00:18:12,240
you can set it with the api calls and

00:18:10,240 --> 00:18:14,400
also with clauses

00:18:12,240 --> 00:18:16,000
so in the final example you can see that

00:18:14,400 --> 00:18:18,240
i've explicitly

00:18:16,000 --> 00:18:20,000
um added a clause to the parallel region

00:18:18,240 --> 00:18:20,960
and said how many threads that i'd like

00:18:20,000 --> 00:18:24,240
to

00:18:20,960 --> 00:18:26,640
launch but obviously if i've now hard

00:18:24,240 --> 00:18:28,960
coded the number 16 into my program

00:18:26,640 --> 00:18:29,760
and i take my code and to a different

00:18:28,960 --> 00:18:31,200
machine and i

00:18:29,760 --> 00:18:33,919
try and build it and run it there and it

00:18:31,200 --> 00:18:35,360
has more than 16 cores maybe

00:18:33,919 --> 00:18:37,200
then my program is only going to be

00:18:35,360 --> 00:18:39,919
using 16 of them

00:18:37,200 --> 00:18:42,160
so it's much better to just say and

00:18:39,919 --> 00:18:43,600
express the parallelism that you have

00:18:42,160 --> 00:18:45,280
and use the environment variables to

00:18:43,600 --> 00:18:46,160
help guide the extent of that

00:18:45,280 --> 00:18:48,320
parallelism so

00:18:46,160 --> 00:18:50,799
how many threads do i want to run well i

00:18:48,320 --> 00:18:54,799
set that as an environment variable

00:18:50,799 --> 00:18:54,799
and don't hard code those things into my

00:18:54,840 --> 00:18:57,840
code

00:18:58,000 --> 00:19:02,240
so thinking about mpi parallel programs

00:19:00,960 --> 00:19:05,440
are often written in this

00:19:02,240 --> 00:19:07,600
uh spmd style so this is where all

00:19:05,440 --> 00:19:08,799
threads or parallel processes or or

00:19:07,600 --> 00:19:10,799
however you think of them

00:19:08,799 --> 00:19:11,840
the parallel things they all run the

00:19:10,799 --> 00:19:14,880
same code

00:19:11,840 --> 00:19:17,440
and they're given a unique id

00:19:14,880 --> 00:19:18,400
and they use that id to figure out which

00:19:17,440 --> 00:19:20,480
data

00:19:18,400 --> 00:19:21,919
they should operate on so we want we

00:19:20,480 --> 00:19:24,080
have an amount of data

00:19:21,919 --> 00:19:25,039
say a parallel amount of data to process

00:19:24,080 --> 00:19:27,600
and we we

00:19:25,039 --> 00:19:29,360
find the id of the parallel things so

00:19:27,600 --> 00:19:31,200
the parallel threads

00:19:29,360 --> 00:19:33,520
we then use that to work out which part

00:19:31,200 --> 00:19:36,000
of the data essentially belongs to them

00:19:33,520 --> 00:19:37,760
they do their work and and as a as a

00:19:36,000 --> 00:19:39,679
kind of bulk synchronous they all get

00:19:37,760 --> 00:19:40,880
their work done and we've processed all

00:19:39,679 --> 00:19:44,000
that data

00:19:40,880 --> 00:19:45,760
so openmp gives you api calls in which

00:19:44,000 --> 00:19:48,080
to do this

00:19:45,760 --> 00:19:49,760
and you can use them inside a parallel

00:19:48,080 --> 00:19:50,080
region obviously it doesn't make sense

00:19:49,760 --> 00:19:52,320
to

00:19:50,080 --> 00:19:54,080
ask openmp how many threads there are if

00:19:52,320 --> 00:19:54,880
you're not inside a parallel region

00:19:54,080 --> 00:19:57,840
because you're just

00:19:54,880 --> 00:19:58,559
executing in serial so to get the number

00:19:57,840 --> 00:20:01,919
of threads

00:19:58,559 --> 00:20:04,480
you call omp get numb threads that's the

00:20:01,919 --> 00:20:06,000
total amount of threads that get

00:20:04,480 --> 00:20:08,159
that have been launched typically this

00:20:06,000 --> 00:20:11,520
will equal whatever you set omp

00:20:08,159 --> 00:20:13,360
num threads the environment variable to

00:20:11,520 --> 00:20:14,720
then there's the thread id this is that

00:20:13,360 --> 00:20:17,360
unique id

00:20:14,720 --> 00:20:19,600
and that's omp get thread num and these

00:20:17,360 --> 00:20:21,760
are functions in fortran so you just

00:20:19,600 --> 00:20:25,039
capture the result in an integer

00:20:21,760 --> 00:20:26,960
variable and on the left hand side

00:20:25,039 --> 00:20:28,559
so inside a parallel region then we've

00:20:26,960 --> 00:20:30,960
we've got a number of threads and we can

00:20:28,559 --> 00:20:33,200
use these api calls to figure out

00:20:30,960 --> 00:20:34,000
what our thread id is and how many

00:20:33,200 --> 00:20:38,080
threads there are

00:20:34,000 --> 00:20:40,000
in total so to show you this in sort of

00:20:38,080 --> 00:20:42,640
practice to show you the

00:20:40,000 --> 00:20:44,880
you know a first parallel program um

00:20:42,640 --> 00:20:47,520
we'll start with vector add

00:20:44,880 --> 00:20:48,640
so this is this is the whole program in

00:20:47,520 --> 00:20:51,919
in fortran 90

00:20:48,640 --> 00:20:56,080
we have three arrays uh a b and c of

00:20:51,919 --> 00:20:58,240
of um 64-bit floating point numbers

00:20:56,080 --> 00:21:00,080
we set the length of those arrays to be

00:20:58,240 --> 00:21:02,559
just 1024

00:21:00,080 --> 00:21:04,080
elements long and we have our loop

00:21:02,559 --> 00:21:06,000
counter i

00:21:04,080 --> 00:21:08,480
so we can allocate those three arrays

00:21:06,000 --> 00:21:11,120
and just initialize them to some number

00:21:08,480 --> 00:21:13,520
and we're just having a shortcut on line

00:21:11,120 --> 00:21:15,200
nine here using the array notation in

00:21:13,520 --> 00:21:16,080
fortran so we've just initialized our

00:21:15,200 --> 00:21:19,840
data to start

00:21:16,080 --> 00:21:22,880
off with now our main parallel kernel

00:21:19,840 --> 00:21:23,760
is on lines 12 to 14. that's the do loop

00:21:22,880 --> 00:21:25,760
this is the main

00:21:23,760 --> 00:21:28,559
heart of the computation that we want to

00:21:25,760 --> 00:21:30,960
do and it's often

00:21:28,559 --> 00:21:32,559
termed the kernel is that it's like the

00:21:30,960 --> 00:21:35,280
kernel in the in the center of a

00:21:32,559 --> 00:21:35,600
of a berry or a nut or something that's

00:21:35,280 --> 00:21:38,000
the

00:21:35,600 --> 00:21:39,120
the heart and and part of our part of

00:21:38,000 --> 00:21:42,559
our code that

00:21:39,120 --> 00:21:44,320
um is the is the kind of um

00:21:42,559 --> 00:21:47,120
individual bit of work that we want to

00:21:44,320 --> 00:21:49,120
run um in parallel

00:21:47,120 --> 00:21:51,600
so for vector adds then we just have to

00:21:49,120 --> 00:21:54,080
loop over the array so from one to n

00:21:51,600 --> 00:21:56,960
we're going to add two arrays a and b

00:21:54,080 --> 00:21:58,480
element wise and store them in element c

00:21:56,960 --> 00:22:00,480
and then we're just going to allocate

00:21:58,480 --> 00:22:03,280
the memory

00:22:00,480 --> 00:22:05,280
so we already have learnt enough openmp

00:22:03,280 --> 00:22:06,799
to parallelize this program we've learnt

00:22:05,280 --> 00:22:09,039
about how to make threads with

00:22:06,799 --> 00:22:11,360
the parallel construct and we've learned

00:22:09,039 --> 00:22:13,200
how to

00:22:11,360 --> 00:22:17,440
find out and share the work just as we

00:22:13,200 --> 00:22:20,000
would in mpi with the api calls

00:22:17,440 --> 00:22:20,720
so the first step is to create the

00:22:20,000 --> 00:22:23,039
threads

00:22:20,720 --> 00:22:23,840
so around my kernel my do loop i'm going

00:22:23,039 --> 00:22:27,280
to add the

00:22:23,840 --> 00:22:29,679
parallel constructs so this will

00:22:27,280 --> 00:22:31,200
fork a number of threads before the do

00:22:29,679 --> 00:22:33,440
loop and

00:22:31,200 --> 00:22:35,760
join all those threads up at the end of

00:22:33,440 --> 00:22:39,039
the do loop

00:22:35,760 --> 00:22:40,480
so for step one we've now created all

00:22:39,039 --> 00:22:42,080
our threads but

00:22:40,480 --> 00:22:43,679
each of those threads is going to do

00:22:42,080 --> 00:22:45,440
exactly um

00:22:43,679 --> 00:22:47,280
the same thing because we haven't told

00:22:45,440 --> 00:22:48,240
them to do anything differently based on

00:22:47,280 --> 00:22:50,320
their

00:22:48,240 --> 00:22:52,000
thread id so we're going to do the

00:22:50,320 --> 00:22:53,760
entire vector addition

00:22:52,000 --> 00:22:55,280
redundantly every thread is going to do

00:22:53,760 --> 00:22:56,880
exactly the same thing

00:22:55,280 --> 00:23:00,320
this is the same as if you just call a

00:22:56,880 --> 00:23:00,320
serial program with mpi

00:23:00,480 --> 00:23:05,039
so inside my parallel region i'm going

00:23:02,880 --> 00:23:06,640
to get the thread id and the number of

00:23:05,039 --> 00:23:09,360
threads

00:23:06,640 --> 00:23:11,760
so i'm going to use those api calls and

00:23:09,360 --> 00:23:13,360
and work out what the unique id of each

00:23:11,760 --> 00:23:15,840
thread is and how many threads i have in

00:23:13,360 --> 00:23:17,760
total the view here is that i'm going to

00:23:15,840 --> 00:23:20,640
manually split up the loop so that i can

00:23:17,760 --> 00:23:22,880
share my work in in parallel

00:23:20,640 --> 00:23:24,960
but there's incorrect behavior here at

00:23:22,880 --> 00:23:25,360
run time and hopefully some of you may

00:23:24,960 --> 00:23:27,600
have

00:23:25,360 --> 00:23:29,200
may have um picked up on the clues from

00:23:27,600 --> 00:23:32,559
from a few slides ago

00:23:29,200 --> 00:23:32,559
what the problem might be here

00:23:33,679 --> 00:23:37,039
so as i said in openmp it's a shared

00:23:35,840 --> 00:23:38,960
memory model this means

00:23:37,039 --> 00:23:41,600
all variables are going to be shared

00:23:38,960 --> 00:23:45,039
between all of the threads

00:23:41,600 --> 00:23:48,480
so for this tid variable get

00:23:45,039 --> 00:23:50,960
the omp get thread number the unique id

00:23:48,480 --> 00:23:52,159
this tid is a shared variable between

00:23:50,960 --> 00:23:54,960
all the threads

00:23:52,159 --> 00:23:56,159
that means we have one copy of tid and

00:23:54,960 --> 00:23:58,720
all threads are going to be

00:23:56,159 --> 00:24:00,799
reading and writing to it so if we call

00:23:58,720 --> 00:24:02,880
this api call that's going to

00:24:00,799 --> 00:24:04,799
update that variable then they're all

00:24:02,880 --> 00:24:06,559
going to update it all at the same time

00:24:04,799 --> 00:24:07,919
and we'll end up in a really funny state

00:24:06,559 --> 00:24:10,240
where that variable doesn't mean

00:24:07,919 --> 00:24:12,799
anything anymore

00:24:10,240 --> 00:24:14,240
so luckily openmp gives us a way to give

00:24:12,799 --> 00:24:17,360
each thread its own

00:24:14,240 --> 00:24:19,840
copy of any variable that we

00:24:17,360 --> 00:24:20,559
might might need to know i need to make

00:24:19,840 --> 00:24:22,880
private

00:24:20,559 --> 00:24:24,159
so for this it's the private clause this

00:24:22,880 --> 00:24:27,360
this private clause is

00:24:24,159 --> 00:24:28,480
added to the parallel region and you

00:24:27,360 --> 00:24:30,960
list there

00:24:28,480 --> 00:24:32,400
as a comma separated list inside the

00:24:30,960 --> 00:24:34,640
parentheses

00:24:32,400 --> 00:24:37,679
all of the variables that every thread

00:24:34,640 --> 00:24:40,960
needs to have its own copy of

00:24:37,679 --> 00:24:44,080
um so to do that for tid then

00:24:40,960 --> 00:24:45,039
we would have omp parallel private and

00:24:44,080 --> 00:24:47,360
then tid

00:24:45,039 --> 00:24:48,159
in the parentheses this means when those

00:24:47,360 --> 00:24:51,760
threads get

00:24:48,159 --> 00:24:53,360
forked they um they have access to all

00:24:51,760 --> 00:24:56,400
the shared variables

00:24:53,360 --> 00:24:59,760
and they also now have their own copy

00:24:56,400 --> 00:25:02,799
their own space in which to update

00:24:59,760 --> 00:25:04,720
tid now tid does exist of course before

00:25:02,799 --> 00:25:07,039
and after the parallel region

00:25:04,720 --> 00:25:08,559
and in the next session um after the

00:25:07,039 --> 00:25:10,080
first exercise we're going to talk a

00:25:08,559 --> 00:25:10,799
little about bits about how the

00:25:10,080 --> 00:25:13,440
variables

00:25:10,799 --> 00:25:14,400
interact outside um and inside the

00:25:13,440 --> 00:25:16,960
parallel region

00:25:14,400 --> 00:25:18,240
but for now the private clause is is our

00:25:16,960 --> 00:25:20,880
first step of making

00:25:18,240 --> 00:25:23,200
um extra copies of variables that are

00:25:20,880 --> 00:25:26,159
local to every thread

00:25:23,200 --> 00:25:28,080
so this now this program is now going to

00:25:26,159 --> 00:25:29,279
operate correctly

00:25:28,080 --> 00:25:31,760
in the sense that we're going to get a

00:25:29,279 --> 00:25:33,360
unique number for every thread in the

00:25:31,760 --> 00:25:34,960
tid variable

00:25:33,360 --> 00:25:36,400
and then threads is shared we haven't

00:25:34,960 --> 00:25:38,000
said anything to do with it but they're

00:25:36,400 --> 00:25:39,440
all going to be populated

00:25:38,000 --> 00:25:41,039
with the same number every thread is

00:25:39,440 --> 00:25:42,000
going to know how many threads there are

00:25:41,039 --> 00:25:44,000
and they're all going to update

00:25:42,000 --> 00:25:45,919
it with the same number so we could make

00:25:44,000 --> 00:25:46,240
that private too we could make it shared

00:25:45,919 --> 00:25:48,640
but

00:25:46,240 --> 00:25:50,240
the end result will will be the same in

00:25:48,640 --> 00:25:52,159
terms of the functionality of the

00:25:50,240 --> 00:25:54,640
program

00:25:52,159 --> 00:25:55,279
so that's the the the second step we

00:25:54,640 --> 00:25:58,960
need to

00:25:55,279 --> 00:26:00,880
find our unique thread ids and now we

00:25:58,960 --> 00:26:01,520
need to distribute the iteration space

00:26:00,880 --> 00:26:03,520
of our loop

00:26:01,520 --> 00:26:05,679
across those threads this is this is

00:26:03,520 --> 00:26:08,320
essentially domain decomposition and we

00:26:05,679 --> 00:26:10,240
we've done this with mpi we know how to

00:26:08,320 --> 00:26:12,240
do this manually so

00:26:10,240 --> 00:26:14,159
in the loop here at the bottom you can

00:26:12,240 --> 00:26:16,640
see that i've i've changed

00:26:14,159 --> 00:26:17,840
what i iterate over it no longer goes

00:26:16,640 --> 00:26:21,279
from one to n

00:26:17,840 --> 00:26:23,440
it goes from um the portion of the array

00:26:21,279 --> 00:26:27,200
based on the number of threads and the

00:26:23,440 --> 00:26:27,200
thread id the tid variable

00:26:27,440 --> 00:26:30,559
it's important to notice especially in

00:26:29,520 --> 00:26:33,200
fortran

00:26:30,559 --> 00:26:33,919
where typically arrays are numbered from

00:26:33,200 --> 00:26:36,400
one

00:26:33,919 --> 00:26:37,440
but threads openmp threads are numbered

00:26:36,400 --> 00:26:39,279
from zero

00:26:37,440 --> 00:26:40,960
so you have to be a little bit careful

00:26:39,279 --> 00:26:42,000
if you're if you're manually working

00:26:40,960 --> 00:26:44,480
with these

00:26:42,000 --> 00:26:46,000
um thread numbers that you that you deal

00:26:44,480 --> 00:26:49,039
with the matching up between

00:26:46,000 --> 00:26:49,760
iterating from say one to n and with

00:26:49,039 --> 00:26:53,039
threads

00:26:49,760 --> 00:26:55,360
zero to n threads -1 that's their index

00:26:53,039 --> 00:26:55,360
space

00:26:56,960 --> 00:27:00,320
so that's it that's our first parallel

00:26:58,880 --> 00:27:02,240
program in openmp

00:27:00,320 --> 00:27:04,320
we've used a similar approach that we've

00:27:02,240 --> 00:27:07,360
seen with with say mpi with this

00:27:04,320 --> 00:27:10,799
spmd style but it's possible to

00:27:07,360 --> 00:27:13,039
do that in openmp and this is how

00:27:10,799 --> 00:27:14,559
it's worth stating at this point about

00:27:13,039 --> 00:27:16,320
barriers and this is

00:27:14,559 --> 00:27:18,399
mainly what i'm going to explain about

00:27:16,320 --> 00:27:20,080
synchronization today

00:27:18,399 --> 00:27:21,760
there are a lot of ways to synchronize

00:27:20,080 --> 00:27:23,600
an openmp

00:27:21,760 --> 00:27:25,039
um and there are a lot of different

00:27:23,600 --> 00:27:28,080
rules but um

00:27:25,039 --> 00:27:30,720
a primary one might be barriers

00:27:28,080 --> 00:27:32,960
so a barrier simply synchronizes all the

00:27:30,720 --> 00:27:36,399
threads in a parallel region

00:27:32,960 --> 00:27:37,520
so as an example here we might create a

00:27:36,399 --> 00:27:39,840
parallel region and

00:27:37,520 --> 00:27:42,240
have this private variable to hold the

00:27:39,840 --> 00:27:45,279
um the thread id again

00:27:42,240 --> 00:27:48,320
on line three that's what we we asked

00:27:45,279 --> 00:27:50,399
the openmp runtime to provide us with

00:27:48,320 --> 00:27:52,799
and then in in four we're going to do

00:27:50,399 --> 00:27:54,320
some amount of big work based on that

00:27:52,799 --> 00:27:56,080
and thread id maybe that's the kernel

00:27:54,320 --> 00:27:59,120
that i said about

00:27:56,080 --> 00:27:59,919
then we also need to do another piece of

00:27:59,120 --> 00:28:02,960
work

00:27:59,919 --> 00:28:05,039
on line eight that's based on uh the

00:28:02,960 --> 00:28:06,320
results of that first piece of work on

00:28:05,039 --> 00:28:08,399
line four

00:28:06,320 --> 00:28:10,080
so between line four and line eight we

00:28:08,399 --> 00:28:11,679
must make sure that all the threads have

00:28:10,080 --> 00:28:13,760
finished their work

00:28:11,679 --> 00:28:15,520
so to do that we would have this barrier

00:28:13,760 --> 00:28:17,679
this uh sentinel omp

00:28:15,520 --> 00:28:20,720
barrier and that would ensure that all

00:28:17,679 --> 00:28:22,559
threads then synchronize at that point

00:28:20,720 --> 00:28:24,399
so it's important of course to to make

00:28:22,559 --> 00:28:25,200
sure that all the threads encounter the

00:28:24,399 --> 00:28:27,520
barrier

00:28:25,200 --> 00:28:29,840
if if some threads don't encounter the

00:28:27,520 --> 00:28:31,200
barrier then that's not a correct openmp

00:28:29,840 --> 00:28:33,520
program we're going to get strange

00:28:31,200 --> 00:28:35,039
behavior at runtime

00:28:33,520 --> 00:28:37,279
so it's important to make sure all of

00:28:35,039 --> 00:28:39,679
the parallel threads are guaranteed to

00:28:37,279 --> 00:28:41,360
to reach the barrier and that barrier is

00:28:39,679 --> 00:28:44,720
going to ensure that all threads

00:28:41,360 --> 00:28:45,840
wait between um before and after that

00:28:44,720 --> 00:28:48,080
barrier statement

00:28:45,840 --> 00:28:48,960
behaves in a very similar way to an mpi

00:28:48,080 --> 00:28:50,559
barrier

00:28:48,960 --> 00:28:52,559
this also means that the memory is going

00:28:50,559 --> 00:28:54,399
to be be updated and things

00:28:52,559 --> 00:28:59,840
as well so all threads can now see the

00:28:54,399 --> 00:28:59,840
memory of the other threads

00:29:01,600 --> 00:29:07,760
so as as you've seen spmd this

00:29:05,360 --> 00:29:09,919
mpi style approach of manually splitting

00:29:07,760 --> 00:29:11,600
loops and decomposing them this requires

00:29:09,919 --> 00:29:13,039
quite a lot of bookkeeping we need to

00:29:11,600 --> 00:29:14,240
keep track of the thread id

00:29:13,039 --> 00:29:16,240
we need to keep track of the number of

00:29:14,240 --> 00:29:18,399
threads we need to manually

00:29:16,240 --> 00:29:20,000
figure out the distribution of the work

00:29:18,399 --> 00:29:22,240
between those threads we need to

00:29:20,000 --> 00:29:23,600
manually change our loop indexing that

00:29:22,240 --> 00:29:25,520
sort of thing

00:29:23,600 --> 00:29:26,880
so because this is such a common pattern

00:29:25,520 --> 00:29:30,799
of wanting to

00:29:26,880 --> 00:29:33,279
share the work of a loop between threads

00:29:30,799 --> 00:29:34,799
so split the loop iterations and share

00:29:33,279 --> 00:29:37,200
them evenly between the different

00:29:34,799 --> 00:29:39,679
threads in the openmp parallel region

00:29:37,200 --> 00:29:40,720
well openmp has these work sharing

00:29:39,679 --> 00:29:42,240
constructs that

00:29:40,720 --> 00:29:44,720
that help with this and they're used

00:29:42,240 --> 00:29:48,000
within a parallel region

00:29:44,720 --> 00:29:50,159
so this is the same vector ad code

00:29:48,000 --> 00:29:53,039
the same parallelism expressed using the

00:29:50,159 --> 00:29:55,039
openmp work sharing constructs

00:29:53,039 --> 00:29:57,039
so we create a parallel region again

00:29:55,039 --> 00:29:59,039
with our omp parallel

00:29:57,039 --> 00:30:00,799
and we just have our regular loop now

00:29:59,039 --> 00:30:02,880
this loop looks like the serial loop it

00:30:00,799 --> 00:30:04,320
just expresses the whole iteration space

00:30:02,880 --> 00:30:06,880
from one to n

00:30:04,320 --> 00:30:07,760
but before and after it i have this omp

00:30:06,880 --> 00:30:11,200
do and end

00:30:07,760 --> 00:30:11,200
do to show where it ends

00:30:11,360 --> 00:30:15,600
so this means that openmp is now being

00:30:14,880 --> 00:30:18,880
told to

00:30:15,600 --> 00:30:22,080
to share the iterations of the do loop

00:30:18,880 --> 00:30:23,760
following the uh the do construct

00:30:22,080 --> 00:30:25,760
between all threads in the parallel

00:30:23,760 --> 00:30:27,919
region and it will deal with

00:30:25,760 --> 00:30:29,279
chopping up the iteration space and

00:30:27,919 --> 00:30:31,120
sharing it between

00:30:29,279 --> 00:30:34,080
the threads it will do all that index

00:30:31,120 --> 00:30:34,080
calculation for us

00:30:34,480 --> 00:30:37,600
additionally we you'll notice here that

00:30:36,640 --> 00:30:40,640
i haven't made

00:30:37,600 --> 00:30:41,600
i um private by default and i and i

00:30:40,640 --> 00:30:44,320
probably should have done that in the

00:30:41,600 --> 00:30:47,600
previous slides too

00:30:44,320 --> 00:30:50,960
um when i use a an omp do

00:30:47,600 --> 00:30:51,279
the loop iterator so the i here in the

00:30:50,960 --> 00:30:54,000
do

00:30:51,279 --> 00:30:55,760
i is made private by default so we no

00:30:54,000 --> 00:30:56,720
longer need to have that data sharing

00:30:55,760 --> 00:30:59,360
clause

00:30:56,720 --> 00:31:01,200
so the i is then protected every thread

00:30:59,360 --> 00:31:05,120
has its own copy that it can increment

00:31:01,200 --> 00:31:07,039
up as it needs to

00:31:05,120 --> 00:31:10,320
it's also worth saying that at the end

00:31:07,039 --> 00:31:12,399
do after we've finished our do statement

00:31:10,320 --> 00:31:14,480
at do loop there is an implicit

00:31:12,399 --> 00:31:17,440
synchronization point

00:31:14,480 --> 00:31:19,679
when that occurs so we can have lots of

00:31:17,440 --> 00:31:21,919
do loops inside a parallel region

00:31:19,679 --> 00:31:23,360
and will stay running in parallel

00:31:21,919 --> 00:31:27,120
throughout them

00:31:23,360 --> 00:31:28,799
and at every end do we will

00:31:27,120 --> 00:31:30,559
each of the threads will encounter an

00:31:28,799 --> 00:31:32,480
implicit barrier

00:31:30,559 --> 00:31:33,840
so we don't have to have barriers

00:31:32,480 --> 00:31:36,480
between do statements

00:31:33,840 --> 00:31:39,279
um that's that that's all part of the

00:31:36,480 --> 00:31:41,279
implicit behavior of the work sharing

00:31:39,279 --> 00:31:42,559
so this work sharing is is typically the

00:31:41,279 --> 00:31:45,519
workhorse of

00:31:42,559 --> 00:31:46,799
of openmp you'll use parallel to create

00:31:45,519 --> 00:31:50,080
threads and do

00:31:46,799 --> 00:31:50,720
the omp do to share the iterations of do

00:31:50,080 --> 00:31:54,480
loops

00:31:50,720 --> 00:31:54,480
um across the parallel threads

00:31:55,440 --> 00:32:00,799
so again it's openmb has lots of

00:31:58,000 --> 00:32:03,360
conveniences that help us

00:32:00,799 --> 00:32:04,240
express things so on the previous slide

00:32:03,360 --> 00:32:06,240
you'll see

00:32:04,240 --> 00:32:08,480
my loop got very long i had to have an

00:32:06,240 --> 00:32:10,799
omp parallel then an omp do

00:32:08,480 --> 00:32:11,840
then my loop and then an end do an end

00:32:10,799 --> 00:32:13,360
parallel

00:32:11,840 --> 00:32:15,440
well you can just write all those things

00:32:13,360 --> 00:32:18,320
on the same line you can just say omp

00:32:15,440 --> 00:32:19,120
parallel do and that will spawn the

00:32:18,320 --> 00:32:21,120
threads

00:32:19,120 --> 00:32:22,559
and then share the work of the loop

00:32:21,120 --> 00:32:24,159
between all those threads

00:32:22,559 --> 00:32:27,200
and then when it's done at the end

00:32:24,159 --> 00:32:29,360
parallel do it will

00:32:27,200 --> 00:32:31,679
synchronize and then join those threads

00:32:29,360 --> 00:32:34,960
again and so things will continue

00:32:31,679 --> 00:32:38,799
sequentially after that what's

00:32:34,960 --> 00:32:40,399
what's good at this point is that

00:32:38,799 --> 00:32:43,120
those threads are typically not

00:32:40,399 --> 00:32:46,240
destroyed they're just left around

00:32:43,120 --> 00:32:47,760
idle by the openmp runtime

00:32:46,240 --> 00:32:49,679
so this means that we don't have to

00:32:47,760 --> 00:32:51,919
worry anymore about

00:32:49,679 --> 00:32:54,640
the cost of forking and joining threads

00:32:51,919 --> 00:32:56,159
typically we pay that overhead just once

00:32:54,640 --> 00:32:58,080
where we create the threads of the first

00:32:56,159 --> 00:32:59,760
time and then threads just hang around

00:32:58,080 --> 00:33:00,480
idle while the serial parts of our

00:32:59,760 --> 00:33:03,840
program can

00:33:00,480 --> 00:33:05,600
uh execute outside parallel regions

00:33:03,840 --> 00:33:07,039
and when another parallel region is

00:33:05,600 --> 00:33:09,679
encountered

00:33:07,039 --> 00:33:11,440
those threads can just be then used it's

00:33:09,679 --> 00:33:13,120
like a free pool of labor that's just

00:33:11,440 --> 00:33:18,000
there ready to be

00:33:13,120 --> 00:33:18,000
enlisted to help us execute in parallel

00:33:19,519 --> 00:33:23,200
so those vector ad codes are available

00:33:21,679 --> 00:33:25,200
in the repository and those are the file

00:33:23,200 --> 00:33:28,240
names there there's the serial version

00:33:25,200 --> 00:33:30,159
the spmd mpi style vector ad

00:33:28,240 --> 00:33:31,840
and then the parallel do version as well

00:33:30,159 --> 00:33:34,799
in case you want to have a little bit of

00:33:31,840 --> 00:33:34,799
a deeper look at those

00:33:35,760 --> 00:33:39,440
so that's that's really the workhorse

00:33:37,440 --> 00:33:41,440
we've seen a lot of openmp already we've

00:33:39,440 --> 00:33:44,720
seen how to create threads

00:33:41,440 --> 00:33:45,760
um execute loops in parallel and maybe

00:33:44,720 --> 00:33:48,320
deal with some of the basic

00:33:45,760 --> 00:33:49,919
synchronization between them um

00:33:48,320 --> 00:33:52,720
at the end of do loops and with barrier

00:33:49,919 --> 00:33:54,320
statements and also started to think a

00:33:52,720 --> 00:33:56,880
little bit about the data sharing

00:33:54,320 --> 00:33:57,440
making sure that data is only accessible

00:33:56,880 --> 00:33:59,919
and to

00:33:57,440 --> 00:34:02,480
the threads that in case of it needs to

00:33:59,919 --> 00:34:02,480
be private

00:34:02,559 --> 00:34:06,080
so very common pattern is is not just a

00:34:04,640 --> 00:34:08,720
single loop maybe we have

00:34:06,080 --> 00:34:10,639
a tightly nested set of loops and for

00:34:08,720 --> 00:34:11,839
example if we have a two-dimensional

00:34:10,639 --> 00:34:13,919
grid code

00:34:11,839 --> 00:34:15,440
and where every cell is independent we'd

00:34:13,919 --> 00:34:17,200
want to share

00:34:15,440 --> 00:34:20,000
all of that work all of that parallel

00:34:17,200 --> 00:34:22,560
work between all of our threads

00:34:20,000 --> 00:34:24,159
so if we just have a parallel do on a

00:34:22,560 --> 00:34:25,839
nested loop we're only going to be

00:34:24,159 --> 00:34:27,839
applying the parallelism

00:34:25,839 --> 00:34:29,440
to the first loop that that's

00:34:27,839 --> 00:34:31,760
encountered

00:34:29,440 --> 00:34:34,079
so this means say in this loop of i and

00:34:31,760 --> 00:34:35,440
j if we just have a parallel do we're

00:34:34,079 --> 00:34:38,720
gonna just parallelize

00:34:35,440 --> 00:34:40,560
over the i loop and j loop would execute

00:34:38,720 --> 00:34:42,560
sequentially well maybe the compiler

00:34:40,560 --> 00:34:46,560
would vectorize it but there's still

00:34:42,560 --> 00:34:49,520
a sequential loop um

00:34:46,560 --> 00:34:51,599
if we add the collapse clause to the do

00:34:49,520 --> 00:34:54,320
so collapse is a clause that

00:34:51,599 --> 00:34:55,359
we can add to the do construct and

00:34:54,320 --> 00:34:58,720
inside we can

00:34:55,359 --> 00:35:00,160
give it a number just a a raw um literal

00:34:58,720 --> 00:35:02,960
number number two here

00:35:00,160 --> 00:35:05,440
and that tells openmp how many loops to

00:35:02,960 --> 00:35:06,320
collapse the iteration space and combine

00:35:05,440 --> 00:35:08,720
them together

00:35:06,320 --> 00:35:10,160
into a single uh a single do loop

00:35:08,720 --> 00:35:12,160
essentially

00:35:10,160 --> 00:35:13,200
and then it then can parallelize and

00:35:12,160 --> 00:35:16,720
work share

00:35:13,200 --> 00:35:19,680
those uh the combined iteration space

00:35:16,720 --> 00:35:21,839
between all the different threads so in

00:35:19,680 --> 00:35:23,920
this example here with the collapsed

00:35:21,839 --> 00:35:25,200
clause i have my loop over i in the loop

00:35:23,920 --> 00:35:27,839
over j

00:35:25,200 --> 00:35:30,320
um i would uh want to power it would

00:35:27,839 --> 00:35:32,320
collapse i and j into a single

00:35:30,320 --> 00:35:33,520
n squared iteration and then it would

00:35:32,320 --> 00:35:36,720
run those

00:35:33,520 --> 00:35:38,320
in parallel this is very useful for

00:35:36,720 --> 00:35:40,720
finding extra parallelism and expressing

00:35:38,320 --> 00:35:42,480
it in the code

00:35:40,720 --> 00:35:44,000
so we're now pretty much at the first

00:35:42,480 --> 00:35:45,920
exercise and it's going to be

00:35:44,000 --> 00:35:46,800
parallelizing a very simple five-point

00:35:45,920 --> 00:35:49,599
stencil code

00:35:46,800 --> 00:35:50,480
and it's going to be using openmp so all

00:35:49,599 --> 00:35:53,119
the code does

00:35:50,480 --> 00:35:54,240
is it computes the average it says each

00:35:53,119 --> 00:35:56,320
cell to the average

00:35:54,240 --> 00:35:57,680
of its neighbors it's a it's a weighted

00:35:56,320 --> 00:35:59,599
average

00:35:57,680 --> 00:36:00,960
where where each of the weights is i

00:35:59,599 --> 00:36:02,720
think is just one in the code but it

00:36:00,960 --> 00:36:04,800
just it just adds up all the neighbors

00:36:02,720 --> 00:36:07,119
and north south east and west um

00:36:04,800 --> 00:36:08,800
averages them together

00:36:07,119 --> 00:36:10,560
obviously this is very similar to things

00:36:08,800 --> 00:36:12,160
like a poisson solver

00:36:10,560 --> 00:36:14,640
but we've just taken away all those sort

00:36:12,160 --> 00:36:15,680
of thorny details of that just to keep

00:36:14,640 --> 00:36:17,680
it simple to this

00:36:15,680 --> 00:36:20,560
um a simple parallel pattern of a

00:36:17,680 --> 00:36:20,560
five-point stencil

00:36:20,640 --> 00:36:25,119
so this is the exercise it's to take the

00:36:22,560 --> 00:36:27,760
code which is in in stencil.f90

00:36:25,119 --> 00:36:30,000
and parallelize it using openmp now you

00:36:27,760 --> 00:36:32,400
might try this in the spmd style

00:36:30,000 --> 00:36:34,480
or you might try it ideally with the

00:36:32,400 --> 00:36:36,160
openmp work sharing clauses

00:36:34,480 --> 00:36:39,359
and try changing the number of threads

00:36:36,160 --> 00:36:41,760
with omp num threads

00:36:39,359 --> 00:36:43,280
um so the idea is this is the main part

00:36:41,760 --> 00:36:45,599
the main loop the main kernel

00:36:43,280 --> 00:36:46,400
this a new and a and you can see that we

00:36:45,599 --> 00:36:48,800
just have the

00:36:46,400 --> 00:36:50,960
um the average of those five numbers

00:36:48,800 --> 00:36:52,079
stored in the in the new array then the

00:36:50,960 --> 00:36:53,520
pointers get swapped

00:36:52,079 --> 00:36:55,440
and we run this a number of times so

00:36:53,520 --> 00:36:58,160
that's the main loop to focus on on

00:36:55,440 --> 00:36:58,160
parallelizing

00:36:58,720 --> 00:37:02,880
um so this is the exercise so now we're

00:37:01,440 --> 00:37:04,400
going to turn over to

00:37:02,880 --> 00:37:05,680
the exercise and for that we're going to

00:37:04,400 --> 00:37:06,800
need to get you access to the

00:37:05,680 --> 00:37:08,960
supercomputer

00:37:06,800 --> 00:37:11,119
so i'll i'll just get those instructions

00:37:08,960 --> 00:37:11,119
up

00:37:19,200 --> 00:37:23,520
okay so uh isn't bad using isimba the

00:37:22,000 --> 00:37:26,320
first thing to do

00:37:23,520 --> 00:37:27,359
is go to this web page here this tiny

00:37:26,320 --> 00:37:31,359
url

00:37:27,359 --> 00:37:33,839
uh openmp2 hyphen 2020.

00:37:31,359 --> 00:37:34,560
when you go there there's a little form

00:37:33,839 --> 00:37:36,400
to fill in

00:37:34,560 --> 00:37:38,560
and uh it's just so we know who's been

00:37:36,400 --> 00:37:41,520
logging into the isabelle supercomputer

00:37:38,560 --> 00:37:42,960
and it will give you a number when you

00:37:41,520 --> 00:37:44,240
have filled in the little form and that

00:37:42,960 --> 00:37:46,400
number is your

00:37:44,240 --> 00:37:48,160
um going to be your user id it's going

00:37:46,400 --> 00:37:49,839
to form part of the user id

00:37:48,160 --> 00:37:51,680
so it's going to probably be just a two

00:37:49,839 --> 00:37:54,480
digit number

00:37:51,680 --> 00:37:57,680
then to log into isn't bard you use this

00:37:54,480 --> 00:37:57,680
ssh command so you'll be

00:37:58,839 --> 00:38:02,480
sshing2ismbard.gw4.ac.uk

00:38:00,240 --> 00:38:03,599
and the username is going to be br

00:38:02,480 --> 00:38:05,200
hyphen train

00:38:03,599 --> 00:38:07,359
and then whatever number you were given

00:38:05,200 --> 00:38:08,320
by the form in step one so if you were

00:38:07,359 --> 00:38:11,119
given number one

00:38:08,320 --> 00:38:11,440
zero one for example your username would

00:38:11,119 --> 00:38:14,800
be

00:38:11,440 --> 00:38:16,839
br hyphen train zero one and so you'd

00:38:14,800 --> 00:38:19,520
ssh into the gateway

00:38:16,839 --> 00:38:20,800
node now the passwords are the same for

00:38:19,520 --> 00:38:24,079
every single account

00:38:20,800 --> 00:38:27,119
they are openmp lowercase then

00:38:24,079 --> 00:38:28,240
uh ug for user group uppercase and then

00:38:27,119 --> 00:38:31,680
number 20.

00:38:28,240 --> 00:38:33,359
so openmp ug20 is the password for all

00:38:31,680 --> 00:38:35,359
of the accounts

00:38:33,359 --> 00:38:36,800
that gets you on to the bastion node

00:38:35,359 --> 00:38:38,480
this gateway

00:38:36,800 --> 00:38:40,880
and once you're there you can then log

00:38:38,480 --> 00:38:43,280
into isimbard phase one so you type

00:38:40,880 --> 00:38:44,079
ssh phase one and then you're going to

00:38:43,280 --> 00:38:47,200
be on

00:38:44,079 --> 00:38:49,040
on isembard proper and then you can uh

00:38:47,200 --> 00:38:50,560
change directory to the

00:38:49,040 --> 00:38:52,400
to the you know the directory that

00:38:50,560 --> 00:38:55,599
contains all of the the code that you'll

00:38:52,400 --> 00:38:55,599
need for the for today

00:38:55,680 --> 00:38:59,200
so there's lots of examples in there and

00:38:57,599 --> 00:39:01,359
showing different parts of openmp and

00:38:59,200 --> 00:39:03,760
it's stencil.f90 is that

00:39:01,359 --> 00:39:05,599
is the serial code that you'll be using

00:39:03,760 --> 00:39:08,400
so you could type make or make stencil

00:39:05,599 --> 00:39:10,400
and that will build that code for you

00:39:08,400 --> 00:39:12,880
then to run on the system you you need

00:39:10,400 --> 00:39:14,320
to use the uh the pbs queuing system but

00:39:12,880 --> 00:39:16,400
i've already given you a

00:39:14,320 --> 00:39:17,520
submission script um that does that for

00:39:16,400 --> 00:39:19,760
you

00:39:17,520 --> 00:39:21,280
so you just type q sub and then submit

00:39:19,760 --> 00:39:23,680
underscore stencil

00:39:21,280 --> 00:39:25,119
that will submit the stencil code the

00:39:23,680 --> 00:39:28,320
the binary that's been built in

00:39:25,119 --> 00:39:29,680
in step six and run it on the um on the

00:39:28,320 --> 00:39:31,520
system

00:39:29,680 --> 00:39:32,720
now you can check the status of those

00:39:31,520 --> 00:39:34,800
jobs uh with

00:39:32,720 --> 00:39:37,119
with the command on line eight but they

00:39:34,800 --> 00:39:39,440
tend to go through very quickly so you

00:39:37,119 --> 00:39:41,520
by the time you've typed uh q stat minus

00:39:39,440 --> 00:39:42,480
u dollar user it's probably already gone

00:39:41,520 --> 00:39:44,640
through

00:39:42,480 --> 00:39:45,599
then all of these standard out gets um

00:39:44,640 --> 00:39:48,560
put in a file

00:39:45,599 --> 00:39:51,200
called stencil.o that's the letter o

00:39:48,560 --> 00:39:53,440
lowercase and then the job number

00:39:51,200 --> 00:39:55,920
so the job number is given to you uh in

00:39:53,440 --> 00:39:57,599
line seven you type qsub submit stencil

00:39:55,920 --> 00:39:59,119
and it gives you a job number

00:39:57,599 --> 00:40:01,040
and then the output will be in

00:39:59,119 --> 00:40:03,599
stencil.oh and then whatever that job

00:40:01,040 --> 00:40:05,040
number was

00:40:03,599 --> 00:40:06,720
so that's it over to you we're into the

00:40:05,040 --> 00:40:08,560
first exercise and we're going to run

00:40:06,720 --> 00:40:11,440
this exercise

00:40:08,560 --> 00:40:11,920
for about 20 minutes um this is to get

00:40:11,440 --> 00:40:14,720
you

00:40:11,920 --> 00:40:16,400
logged into izambard uh parallelizing

00:40:14,720 --> 00:40:18,800
stencil

00:40:16,400 --> 00:40:21,839
with openmp so adding parallel do

00:40:18,800 --> 00:40:23,520
statements to the stencil code

00:40:21,839 --> 00:40:25,040
so i'll leave this slide up and

00:40:23,520 --> 00:40:26,720
hopefully you'll all be able to log into

00:40:25,040 --> 00:40:28,960
izambard and start

00:40:26,720 --> 00:40:30,480
writing your parallel programs with

00:40:28,960 --> 00:40:32,240
openmp

00:40:30,480 --> 00:40:34,079
if you have any questions please let us

00:40:32,240 --> 00:40:34,640
know we're going to be primarily using

00:40:34,079 --> 00:40:37,440
the q

00:40:34,640 --> 00:40:39,119
a part of zoom so rather than the chat

00:40:37,440 --> 00:40:40,480
it's quite difficult to keep track of of

00:40:39,119 --> 00:40:42,079
all the questions in in

00:40:40,480 --> 00:40:44,319
the chat in zoom we're going to be using

00:40:42,079 --> 00:40:46,000
the q a so if you hover

00:40:44,319 --> 00:40:47,680
over the shared screen somewhere

00:40:46,000 --> 00:40:48,000
there'll be a little button that says q

00:40:47,680 --> 00:40:50,640
a

00:40:48,000 --> 00:40:52,079
it's the logo is two little speech like

00:40:50,640 --> 00:40:54,240
speech bubbles

00:40:52,079 --> 00:40:55,680
and that or call out boxes those those

00:40:54,240 --> 00:40:57,280
types of things if you click on that you

00:40:55,680 --> 00:40:59,839
can ask questions

00:40:57,280 --> 00:41:01,040
and we will we will answer questions

00:40:59,839 --> 00:41:04,720
that you have

00:41:01,040 --> 00:41:06,240
through that as well um so um i'll let

00:41:04,720 --> 00:41:08,800
you get on with that i'm going to keep

00:41:06,240 --> 00:41:10,319
a close eye on the q a and please ask

00:41:08,800 --> 00:41:12,319
questions if you have any troubles with

00:41:10,319 --> 00:41:14,800
with logging in and with starting this

00:41:12,319 --> 00:41:16,079
this exercise and uh the next session

00:41:14,800 --> 00:41:18,800
will start at 10

00:41:16,079 --> 00:41:19,680
35 in in uk time but i'm going to be

00:41:18,800 --> 00:41:22,480
here

00:41:19,680 --> 00:41:22,480
all the way through

00:41:23,760 --> 00:41:28,480
i've just been uh pointed out that there

00:41:25,680 --> 00:41:29,680
was a um a small typo on on the tinyurl

00:41:28,480 --> 00:41:32,160
there should be just a

00:41:29,680 --> 00:41:32,880
dot-com so sorry about that i've added

00:41:32,160 --> 00:41:36,800
it here it's

00:41:32,880 --> 00:41:38,880
tinyurl.com openmp hyphen 2020

00:41:36,800 --> 00:41:39,839
and from there it's just a form to get

00:41:38,880 --> 00:41:43,760
you your

00:41:39,839 --> 00:41:45,119
your user id um please use the the q a

00:41:43,760 --> 00:41:46,640
for chat we've already had one question

00:41:45,119 --> 00:41:47,599
there it's great it's much easier to

00:41:46,640 --> 00:41:49,839
help

00:41:47,599 --> 00:41:52,160
answer the questions and also feel free

00:41:49,839 --> 00:41:53,040
to chip in and comment on other people's

00:41:52,160 --> 00:41:55,920
questions on the

00:41:53,040 --> 00:41:57,599
q a as well um i believe we've set zoom

00:41:55,920 --> 00:41:59,040
up to allow everybody to answer each

00:41:57,599 --> 00:42:01,599
other's questions

00:41:59,040 --> 00:42:02,319
um so you know have a little community

00:42:01,599 --> 00:42:04,160
is you know it's

00:42:02,319 --> 00:42:06,240
strange doing an online tutorial but

00:42:04,160 --> 00:42:08,079
it's really great if we're able to start

00:42:06,240 --> 00:42:09,440
interacting with with each other as well

00:42:08,079 --> 00:42:17,839
it will make a much more

00:42:09,440 --> 00:42:17,839
enjoyable day for everybody

00:42:20,800 --> 00:42:24,400
okay sorry for the troubles getting on

00:42:22,800 --> 00:42:26,000
um

00:42:24,400 --> 00:42:27,760
the good support there is is having a

00:42:26,000 --> 00:42:30,640
quick look at it into that for us and

00:42:27,760 --> 00:42:32,240
hopefully it'll be back up soon it's not

00:42:30,640 --> 00:42:33,920
some planned maintenance it's

00:42:32,240 --> 00:42:35,839
something that yeah we couldn't have

00:42:33,920 --> 00:42:37,200
predicted so hopefully it won't be too

00:42:35,839 --> 00:42:40,240
long before you're

00:42:37,200 --> 00:42:41,200
um up and running obviously if uh this

00:42:40,240 --> 00:42:43,520
first half of the

00:42:41,200 --> 00:42:44,720
of the day is all just openmp on cpus

00:42:43,520 --> 00:42:46,480
and

00:42:44,720 --> 00:42:48,640
if you have gcc or something installed

00:42:46,480 --> 00:42:51,680
you can always try

00:42:48,640 --> 00:42:53,440
the the exercise locally as well or

00:42:51,680 --> 00:42:59,839
the intel compiler of course supports

00:42:53,440 --> 00:42:59,839
supports it as well

00:43:00,640 --> 00:43:04,640
okay it looks like the system's back up

00:43:02,880 --> 00:43:05,920
now a bit of a network glitch

00:43:04,640 --> 00:43:07,760
sorry about that everybody hopefully

00:43:05,920 --> 00:43:08,240
you've all been able to at least get

00:43:07,760 --> 00:43:10,839
your

00:43:08,240 --> 00:43:13,839
um accounts that's that's a good first

00:43:10,839 --> 00:43:13,839
start

00:43:19,200 --> 00:43:23,359
okay so hopefully you're all managing to

00:43:20,800 --> 00:43:26,079
get uh logged into isimbard now

00:43:23,359 --> 00:43:28,280
um and once you're there it's the when

00:43:26,079 --> 00:43:29,920
you get through the directories it's the

00:43:28,280 --> 00:43:32,319
stencil.f90

00:43:29,920 --> 00:43:33,839
and the goal of this exercise is to

00:43:32,319 --> 00:43:35,680
parallelize the main

00:43:33,839 --> 00:43:37,280
uh the main loop the main five-point

00:43:35,680 --> 00:43:39,760
stencil

00:43:37,280 --> 00:43:40,960
with openmp use primarily using the work

00:43:39,760 --> 00:43:43,599
sharing

00:43:40,960 --> 00:43:45,599
construct so parallel and parallel do

00:43:43,599 --> 00:43:48,079
maybe the collapse clause

00:43:45,599 --> 00:43:49,680
and then make sure if there's any

00:43:48,079 --> 00:43:51,200
private data you might want to consider

00:43:49,680 --> 00:43:55,839
using the the private

00:43:51,200 --> 00:43:55,839
their private clause as well

00:43:57,520 --> 00:44:03,359
just a few more minutes now make sure

00:43:59,680 --> 00:44:05,839
you use the job submission scripts to

00:44:03,359 --> 00:44:07,040
run the binary on all the compute nodes

00:44:05,839 --> 00:44:10,079
this means we can

00:44:07,040 --> 00:44:12,000
share the compute nodes between all of

00:44:10,079 --> 00:44:13,680
the participants

00:44:12,000 --> 00:44:16,560
so it's just the the submit scripts

00:44:13,680 --> 00:44:19,119
there that will run that stencil binary

00:44:16,560 --> 00:44:19,119
for you all

00:44:21,520 --> 00:44:25,359
okay that's great it sounds like a

00:44:23,200 --> 00:44:28,240
couple of you have managed to get this

00:44:25,359 --> 00:44:28,640
um get yourselves locked into izanbard

00:44:28,240 --> 00:44:32,240
and

00:44:28,640 --> 00:44:32,720
been uh adding parallel parallel due to

00:44:32,240 --> 00:44:34,000
the

00:44:32,720 --> 00:44:36,160
main computational loops and they're

00:44:34,000 --> 00:44:38,000
seeing some nice speed ups to kind of

00:44:36,160 --> 00:44:41,040
show that you're running in

00:44:38,000 --> 00:44:43,200
in parallel so that's brilliant so

00:44:41,040 --> 00:44:44,400
now the next stage is um is another

00:44:43,200 --> 00:44:45,200
lecture that i'm going to talk to you a

00:44:44,400 --> 00:44:47,839
little bit about

00:44:45,200 --> 00:44:51,119
um data sharing and reductions just turn

00:44:47,839 --> 00:44:54,400
on the webcam too so you can

00:44:51,119 --> 00:44:56,319
see me um doesn't look like you so make

00:44:54,400 --> 00:44:59,520
it more interactive

00:44:56,319 --> 00:45:02,000
and i'm gonna do this for um until 11

00:44:59,520 --> 00:45:03,200
o'clock uk time so for the next 25

00:45:02,000 --> 00:45:05,040
minutes

00:45:03,200 --> 00:45:07,040
and then at that point we're going to

00:45:05,040 --> 00:45:08,400
have a coffee break uh just 15 minutes

00:45:07,040 --> 00:45:10,000
for you to all step away

00:45:08,400 --> 00:45:12,319
and following the coffee break we'll

00:45:10,000 --> 00:45:13,920
we'll move into the next exercise so

00:45:12,319 --> 00:45:15,920
if you're very keen you can start that

00:45:13,920 --> 00:45:17,359
exercise during the coffee break but

00:45:15,920 --> 00:45:20,480
it's good to get away from

00:45:17,359 --> 00:45:20,480
the computer if you can

00:45:20,960 --> 00:45:24,640
so we're now going to move on to some

00:45:22,880 --> 00:45:26,400
important parts of openmp

00:45:24,640 --> 00:45:28,400
and those are the data sharing clauses

00:45:26,400 --> 00:45:32,880
and also how to express

00:45:28,400 --> 00:45:34,480
reductions so to start with this is the

00:45:32,880 --> 00:45:36,240
almost a solution to the to the first

00:45:34,480 --> 00:45:40,160
exercise this is

00:45:36,240 --> 00:45:42,480
um the the main uh stencil code

00:45:40,160 --> 00:45:43,760
and above that we're going to add the

00:45:42,480 --> 00:45:46,880
omp parallel

00:45:43,760 --> 00:45:48,880
do with a collapse uh clause to say

00:45:46,880 --> 00:45:50,400
that we want to collapse those two loops

00:45:48,880 --> 00:45:53,520
together um

00:45:50,400 --> 00:45:56,240
and uh run all of the i

00:45:53,520 --> 00:45:58,400
and j iterations in parallel across all

00:45:56,240 --> 00:45:59,920
of the threads that get spawned

00:45:58,400 --> 00:46:02,079
and then just have the loop completely

00:45:59,920 --> 00:46:05,280
unmodified and then the omp

00:46:02,079 --> 00:46:05,680
end parallel do so notice i don't need

00:46:05,280 --> 00:46:08,160
to

00:46:05,680 --> 00:46:10,319
put the claws on the end on the end

00:46:08,160 --> 00:46:10,800
parallel do the clauses only apply to

00:46:10,319 --> 00:46:12,640
the

00:46:10,800 --> 00:46:15,599
uh to the starting construct they don't

00:46:12,640 --> 00:46:17,920
need to go on the end

00:46:15,599 --> 00:46:19,839
so we can imagine then we have our

00:46:17,920 --> 00:46:22,240
serial thread that's running our program

00:46:19,839 --> 00:46:25,280
and when it encounters this

00:46:22,240 --> 00:46:26,839
omp construct it's going to

00:46:25,280 --> 00:46:29,040
spawn a number of threads on the

00:46:26,839 --> 00:46:31,119
parallel

00:46:29,040 --> 00:46:32,720
it's then going to see a do collapse so

00:46:31,119 --> 00:46:34,560
it's going to have the compiler will

00:46:32,720 --> 00:46:37,040
have collapsed my loops

00:46:34,560 --> 00:46:39,119
and then it's going to be workshed the

00:46:37,040 --> 00:46:40,319
iteration space of that collapsed loop

00:46:39,119 --> 00:46:41,920
is going to be

00:46:40,319 --> 00:46:43,440
shared between all of those threads that

00:46:41,920 --> 00:46:46,480
were launched by the parallel

00:46:43,440 --> 00:46:48,240
that then runs they reach the end do the

00:46:46,480 --> 00:46:51,119
second and do of the loops

00:46:48,240 --> 00:46:52,560
or the end parallel due the threads are

00:46:51,119 --> 00:46:54,720
going to synchronize and then

00:46:52,560 --> 00:46:56,400
execution returns serially so in the

00:46:54,720 --> 00:46:56,960
code you'll see that the pointer swap is

00:46:56,400 --> 00:46:59,839
going to happen

00:46:56,960 --> 00:47:02,079
serially and my time step you know my my

00:46:59,839 --> 00:47:05,440
fake time step loop is going to be

00:47:02,079 --> 00:47:06,800
um a serial loop as well this is this is

00:47:05,440 --> 00:47:10,480
exactly the behavior that we

00:47:06,800 --> 00:47:12,319
that we'd like

00:47:10,480 --> 00:47:14,880
so it's important to remember that

00:47:12,319 --> 00:47:15,520
openmp is a shared memory programming

00:47:14,880 --> 00:47:17,680
model

00:47:15,520 --> 00:47:19,520
so this means that all the memory all

00:47:17,680 --> 00:47:21,520
the data that you allocate

00:47:19,520 --> 00:47:22,960
all the variables and everything is

00:47:21,520 --> 00:47:26,000
shared between

00:47:22,960 --> 00:47:27,359
all of the threads what this means is

00:47:26,000 --> 00:47:30,480
that there is a single

00:47:27,359 --> 00:47:30,960
copy of all the data so all that shared

00:47:30,480 --> 00:47:33,200
data

00:47:30,960 --> 00:47:35,040
there's just one copy so all the threads

00:47:33,200 --> 00:47:38,079
are going to be accessing that single

00:47:35,040 --> 00:47:40,400
unique copy of it and we saw this

00:47:38,079 --> 00:47:42,240
a little while ago with the tid variable

00:47:40,400 --> 00:47:43,839
i needed to have the private clause to

00:47:42,240 --> 00:47:46,880
make sure that it was

00:47:43,839 --> 00:47:49,839
not going to be shared

00:47:46,880 --> 00:47:51,599
so in order to avoid that you've already

00:47:49,839 --> 00:47:54,079
seen that we need the private clause

00:47:51,599 --> 00:47:54,720
this is going to give each thread its

00:47:54,079 --> 00:47:57,680
own

00:47:54,720 --> 00:47:58,800
copy of that variable so each thread

00:47:57,680 --> 00:48:00,880
really has its sort of

00:47:58,800 --> 00:48:03,119
local stack space so if you're familiar

00:48:00,880 --> 00:48:04,480
with c you'll be familiar with the stack

00:48:03,119 --> 00:48:06,400
and the heap

00:48:04,480 --> 00:48:08,000
so anything you allocate goes on the

00:48:06,400 --> 00:48:09,359
heap and all local variables

00:48:08,000 --> 00:48:11,440
are on the stack and the same the same

00:48:09,359 --> 00:48:13,760
is really true in fortran too

00:48:11,440 --> 00:48:15,520
so each thread has its own local space

00:48:13,760 --> 00:48:17,920
for any private variables

00:48:15,520 --> 00:48:20,720
so we must say with the private clause

00:48:17,920 --> 00:48:23,680
what those variables need to be

00:48:20,720 --> 00:48:24,880
that means that then each copy of that

00:48:23,680 --> 00:48:26,559
private variable

00:48:24,880 --> 00:48:28,559
essentially we're copying the private

00:48:26,559 --> 00:48:28,960
the the data that copying the private

00:48:28,559 --> 00:48:30,559
vari

00:48:28,960 --> 00:48:32,960
the the shared variable is going to be

00:48:30,559 --> 00:48:35,200
copied into the local space of each

00:48:32,960 --> 00:48:37,359
threat

00:48:35,200 --> 00:48:38,640
now in fortran it's really important

00:48:37,359 --> 00:48:41,680
that we specify

00:48:38,640 --> 00:48:43,680
which variables are private we typically

00:48:41,680 --> 00:48:45,280
especially with implicit none declare

00:48:43,680 --> 00:48:47,760
all of our variables at the top of the

00:48:45,280 --> 00:48:49,440
routine so this means that we have to

00:48:47,760 --> 00:48:50,880
think about what variables are going to

00:48:49,440 --> 00:48:52,400
be shared and which ones are going to be

00:48:50,880 --> 00:48:54,319
private

00:48:52,400 --> 00:48:56,960
in a language like c where you might

00:48:54,319 --> 00:48:59,359
have local variables just defined

00:48:56,960 --> 00:49:00,079
in place in in the local scope between

00:48:59,359 --> 00:49:02,319
the different

00:49:00,079 --> 00:49:04,319
curly braces then we don't have to

00:49:02,319 --> 00:49:05,920
necessarily worry so much because

00:49:04,319 --> 00:49:07,440
if everything's defined at the point

00:49:05,920 --> 00:49:10,319
that it's going to be used

00:49:07,440 --> 00:49:12,240
and then it's going to uh in general

00:49:10,319 --> 00:49:14,000
pick up the right scope but for fortran

00:49:12,240 --> 00:49:14,960
because we have our declarations of

00:49:14,000 --> 00:49:17,040
variables

00:49:14,960 --> 00:49:18,319
at the top of the routine lots of things

00:49:17,040 --> 00:49:19,440
are going to be shared

00:49:18,319 --> 00:49:22,800
so we need to make sure that they're

00:49:19,440 --> 00:49:22,800
listed as private

00:49:24,240 --> 00:49:27,520
so the other thing to say is about

00:49:26,160 --> 00:49:29,760
variables on the heap

00:49:27,520 --> 00:49:31,760
so these are always shared whatever you

00:49:29,760 --> 00:49:34,480
do so this means that

00:49:31,760 --> 00:49:35,760
fortran allocatable data is shared so

00:49:34,480 --> 00:49:40,319
you're not going to be making that

00:49:35,760 --> 00:49:42,160
private um so you must ensure then

00:49:40,319 --> 00:49:44,160
that you're in your parallel scheme that

00:49:42,160 --> 00:49:45,680
the different threads

00:49:44,160 --> 00:49:48,319
write to different parts of that

00:49:45,680 --> 00:49:49,839
allocatable data so we allocate an array

00:49:48,319 --> 00:49:51,599
then we don't want lots of threads all

00:49:49,839 --> 00:49:53,520
updating the same part of that array

00:49:51,599 --> 00:49:55,839
we'll get a data race and things will

00:49:53,520 --> 00:49:57,520
give us the wrong answer so we must

00:49:55,839 --> 00:50:01,680
ensure as programmers that the threads

00:49:57,520 --> 00:50:03,520
don't access the same parts of memory

00:50:01,680 --> 00:50:05,599
it's worth also saying that although the

00:50:03,520 --> 00:50:08,559
data itself is going to be shared so

00:50:05,599 --> 00:50:09,520
allocatable data the sort of pointer the

00:50:08,559 --> 00:50:11,680
handler to

00:50:09,520 --> 00:50:12,559
that data is something that we can make

00:50:11,680 --> 00:50:14,079
private so

00:50:12,559 --> 00:50:16,400
making it private is only going to

00:50:14,079 --> 00:50:19,520
affect the metadata of that variable

00:50:16,400 --> 00:50:20,640
so um if we if we have an allocatable

00:50:19,520 --> 00:50:22,319
array but we then

00:50:20,640 --> 00:50:23,920
list as private we're just having a

00:50:22,319 --> 00:50:25,839
private copy of all the

00:50:23,920 --> 00:50:27,520
information about it the sizes and

00:50:25,839 --> 00:50:28,880
extents and strides all that sort of

00:50:27,520 --> 00:50:31,599
stuff

00:50:28,880 --> 00:50:32,240
so anything that you allocate is shared

00:50:31,599 --> 00:50:34,240
and

00:50:32,240 --> 00:50:37,040
that's that's how it will be there's no

00:50:34,240 --> 00:50:37,040
way to change that

00:50:38,240 --> 00:50:42,319
so there are a number of data clauses

00:50:40,000 --> 00:50:44,480
that we um we can apply not just

00:50:42,319 --> 00:50:45,440
private and and these are used to

00:50:44,480 --> 00:50:48,640
specify

00:50:45,440 --> 00:50:50,160
um what type of of data should be

00:50:48,640 --> 00:50:51,760
available and how it should be available

00:50:50,160 --> 00:50:53,200
between the different threads

00:50:51,760 --> 00:50:55,200
so the first one is shared we can

00:50:53,200 --> 00:50:56,559
specify that a variable is shared

00:50:55,200 --> 00:50:58,400
and this means that there is a single

00:50:56,559 --> 00:51:00,240
copy of that variable so i'm going to

00:50:58,400 --> 00:51:01,760
use x as an example variable there's a

00:51:00,240 --> 00:51:03,920
single copy of x

00:51:01,760 --> 00:51:05,760
and as programmers we must then ensure

00:51:03,920 --> 00:51:06,880
that synchronization happens either with

00:51:05,760 --> 00:51:08,480
barriers or

00:51:06,880 --> 00:51:10,160
or maybe because of the way we're using

00:51:08,480 --> 00:51:12,160
a do loop um

00:51:10,160 --> 00:51:13,839
with the parallel do we're only we're

00:51:12,160 --> 00:51:14,240
not going to be updating different parts

00:51:13,839 --> 00:51:17,839
of that

00:51:14,240 --> 00:51:19,520
thousand array secondly there's there's

00:51:17,839 --> 00:51:21,839
private and we've seen this already

00:51:19,520 --> 00:51:23,680
this means that each thread gets its own

00:51:21,839 --> 00:51:26,720
local x variable

00:51:23,680 --> 00:51:27,280
what's important is that with private x

00:51:26,720 --> 00:51:30,480
is not

00:51:27,280 --> 00:51:32,480
initialized it doesn't take

00:51:30,480 --> 00:51:35,520
a particular value it could be any

00:51:32,480 --> 00:51:38,160
random junk in there

00:51:35,520 --> 00:51:39,200
secondly when you exit the parallel

00:51:38,160 --> 00:51:42,400
region

00:51:39,200 --> 00:51:45,119
the value of that x variable

00:51:42,400 --> 00:51:46,720
is undefined as well so inside the

00:51:45,119 --> 00:51:48,720
parallel region x

00:51:46,720 --> 00:51:50,480
needs to be initialized and when you

00:51:48,720 --> 00:51:52,079
leave the parallel region

00:51:50,480 --> 00:51:54,800
what all those threads will have been

00:51:52,079 --> 00:51:58,240
writing to their own local copy of x

00:51:54,800 --> 00:52:00,800
and it's undefined which of in fact what

00:51:58,240 --> 00:52:02,079
that x variable back in the serial

00:52:00,800 --> 00:52:02,720
execution at the end of the parallel

00:52:02,079 --> 00:52:04,480
region

00:52:02,720 --> 00:52:05,920
uh what value is actually stored in that

00:52:04,480 --> 00:52:08,480
x variable

00:52:05,920 --> 00:52:10,400
so private just specifies that each

00:52:08,480 --> 00:52:13,280
thread gets its own local

00:52:10,400 --> 00:52:14,000
copy that it's essentially its own

00:52:13,280 --> 00:52:15,680
little

00:52:14,000 --> 00:52:17,119
space in which you can use in place of

00:52:15,680 --> 00:52:18,640
that x variable

00:52:17,119 --> 00:52:20,720
it doesn't contain a value and when you

00:52:18,640 --> 00:52:22,800
finish executing in parallel

00:52:20,720 --> 00:52:23,760
the value that's left in that original

00:52:22,800 --> 00:52:26,720
variable

00:52:23,760 --> 00:52:26,720
is undefined

00:52:28,800 --> 00:52:33,280
the first private clause helps fix us

00:52:31,200 --> 00:52:36,480
one of those problems

00:52:33,280 --> 00:52:40,079
so this first private x says that

00:52:36,480 --> 00:52:40,559
um x is private so x gets its own copy

00:52:40,079 --> 00:52:42,400
of

00:52:40,559 --> 00:52:45,040
every thread gets its own copy of the x

00:52:42,400 --> 00:52:45,680
variable but the first part of the first

00:52:45,040 --> 00:52:48,880
private

00:52:45,680 --> 00:52:51,359
means that it's initialized um to the

00:52:48,880 --> 00:52:52,640
value of what x was before it entered

00:52:51,359 --> 00:52:54,400
the parallel region

00:52:52,640 --> 00:52:56,160
so this solves the initialization

00:52:54,400 --> 00:52:59,119
problem of of

00:52:56,160 --> 00:53:00,880
of x so sometimes this is very useful we

00:52:59,119 --> 00:53:02,559
have a variable that's been initialized

00:53:00,880 --> 00:53:03,920
but it needs to be private in each

00:53:02,559 --> 00:53:05,599
thread

00:53:03,920 --> 00:53:06,960
but we want to retain that written

00:53:05,599 --> 00:53:08,960
original value

00:53:06,960 --> 00:53:10,000
this is where first private um comes

00:53:08,960 --> 00:53:12,319
into that so we

00:53:10,000 --> 00:53:13,839
say first private x each thread gets its

00:53:12,319 --> 00:53:17,760
own variable and is

00:53:13,839 --> 00:53:20,880
initialized but then still the the value

00:53:17,760 --> 00:53:23,760
on exiting the region is undefined

00:53:20,880 --> 00:53:24,880
there's also last private this is useful

00:53:23,760 --> 00:53:28,000
for loops

00:53:24,880 --> 00:53:30,800
and again each thread gets its own

00:53:28,000 --> 00:53:31,920
x variable it's uninitialized uh going

00:53:30,800 --> 00:53:35,119
in so it contains

00:53:31,920 --> 00:53:35,920
a random number um but on exiting the

00:53:35,119 --> 00:53:38,880
region

00:53:35,920 --> 00:53:41,040
i'm finishing the parallel do the value

00:53:38,880 --> 00:53:42,160
in x is guaranteed to be whatever was

00:53:41,040 --> 00:53:45,200
the last

00:53:42,160 --> 00:53:46,880
iteration if we were sequentially

00:53:45,200 --> 00:53:49,520
iterating that

00:53:46,880 --> 00:53:50,000
that loop so if we had a do loop from 1

00:53:49,520 --> 00:53:53,359
to n

00:53:50,000 --> 00:53:53,839
and i had a last private x then whatever

00:53:53,359 --> 00:53:56,960
that

00:53:53,839 --> 00:53:59,359
nth iteration set the variable x2

00:53:56,960 --> 00:54:01,200
that would be the value of x when we

00:53:59,359 --> 00:54:04,319
exited the region

00:54:01,200 --> 00:54:06,559
so really it's just combining uh

00:54:04,319 --> 00:54:08,880
the private behavior and fixing one of

00:54:06,559 --> 00:54:10,880
the two uninitialized behaviors of it

00:54:08,880 --> 00:54:12,880
so private is everybody gets every

00:54:10,880 --> 00:54:15,280
thread gets its own copy

00:54:12,880 --> 00:54:16,400
it's not initialized and undefined on

00:54:15,280 --> 00:54:18,640
exit but first

00:54:16,400 --> 00:54:20,480
private fixes initialization and last

00:54:18,640 --> 00:54:22,800
private defines in loops

00:54:20,480 --> 00:54:25,839
what that variable is when we finish the

00:54:22,800 --> 00:54:28,000
loop off

00:54:25,839 --> 00:54:29,760
there's also thread private it's not a

00:54:28,000 --> 00:54:32,000
clause it's a directive even though it

00:54:29,760 --> 00:54:36,880
looks like a clause

00:54:32,000 --> 00:54:39,200
this is not very likely to be used

00:54:36,880 --> 00:54:39,920
if your code in fortran uses common

00:54:39,200 --> 00:54:42,480
blocks

00:54:39,920 --> 00:54:43,119
then this might be useful there's a

00:54:42,480 --> 00:54:45,760
notion of

00:54:43,119 --> 00:54:47,520
thread local storage and it's persistent

00:54:45,760 --> 00:54:50,160
across parallel regions and

00:54:47,520 --> 00:54:52,559
it all gets a bit a bit tricky so if you

00:54:50,160 --> 00:54:54,799
have common blocks in your fortran code

00:54:52,559 --> 00:54:57,200
then you might find the thread private

00:54:54,799 --> 00:54:58,799
directive to be useful

00:54:57,200 --> 00:55:01,920
but i'm not really going to go into too

00:54:58,799 --> 00:55:03,920
much detail about it in general because

00:55:01,920 --> 00:55:06,079
i don't think common blocks are that

00:55:03,920 --> 00:55:08,720
common anymore mainly modules and things

00:55:06,079 --> 00:55:08,720
that sort of thing

00:55:09,119 --> 00:55:12,640
so as a little example to show you the

00:55:11,200 --> 00:55:15,280
behavior of the different

00:55:12,640 --> 00:55:16,000
types of private i have this very simple

00:55:15,280 --> 00:55:20,079
do loop

00:55:16,000 --> 00:55:21,520
it just sets a variable this x variable

00:55:20,079 --> 00:55:22,799
and it's going to set it to the

00:55:21,520 --> 00:55:24,400
iteration number as it's going to go

00:55:22,799 --> 00:55:27,040
through and it's going to write out to

00:55:24,400 --> 00:55:28,960
the screen what what it's doing

00:55:27,040 --> 00:55:30,640
so each iteration prints the current and

00:55:28,960 --> 00:55:31,839
next value of x along with the thread

00:55:30,640 --> 00:55:32,799
number as well and we'll see what

00:55:31,839 --> 00:55:36,000
happens

00:55:32,799 --> 00:55:38,240
if we have omp parallel do private x or

00:55:36,000 --> 00:55:40,720
parallel do first private or parallel do

00:55:38,240 --> 00:55:42,079
last private

00:55:40,720 --> 00:55:44,480
you can see this in i've given you the

00:55:42,079 --> 00:55:46,160
source file for this private.f90

00:55:44,480 --> 00:55:47,760
and i ran this just with four threads

00:55:46,160 --> 00:55:48,400
and n equals 10 just to keep the

00:55:47,760 --> 00:55:52,559
iteration

00:55:48,400 --> 00:55:52,559
um the actual output very small

00:55:52,640 --> 00:55:56,640
so this is just the private example and

00:55:55,040 --> 00:55:59,760
we can see that

00:55:56,640 --> 00:56:01,760
before we run our x's set to -1 it's

00:55:59,760 --> 00:56:04,240
just whatever happened to be in there

00:56:01,760 --> 00:56:06,400
and as we go through we can see that the

00:56:04,240 --> 00:56:08,559
the work is being shared we have thread

00:56:06,400 --> 00:56:10,559
zero is is starts with the first

00:56:08,559 --> 00:56:11,520
iteration of the loop and it sets it to

00:56:10,559 --> 00:56:12,960
one

00:56:11,520 --> 00:56:15,520
and then when it goes through it it

00:56:12,960 --> 00:56:18,240
picks up it's its old value of x

00:56:15,520 --> 00:56:18,640
and and sets it to two and as you can

00:56:18,240 --> 00:56:20,960
see

00:56:18,640 --> 00:56:23,280
with these outputs all of the threads

00:56:20,960 --> 00:56:25,680
have their own independent copy of

00:56:23,280 --> 00:56:26,720
x and they all set it to zero to start

00:56:25,680 --> 00:56:29,839
with that's what it was

00:56:26,720 --> 00:56:31,440
happens to be there were no guarantees

00:56:29,839 --> 00:56:33,520
of that initial value but it happened to

00:56:31,440 --> 00:56:35,520
be zero this time and then it did its

00:56:33,520 --> 00:56:38,079
thing it said it to us

00:56:35,520 --> 00:56:38,559
whatever number of the iteration space

00:56:38,079 --> 00:56:41,280
it was

00:56:38,559 --> 00:56:41,280
it was assigned

00:56:43,040 --> 00:56:47,520
now on to first private so remember this

00:56:45,280 --> 00:56:50,240
is fixing the initialization problem

00:56:47,520 --> 00:56:51,599
so we we have our x equals minus one

00:56:50,240 --> 00:56:53,040
before the loop

00:56:51,599 --> 00:56:55,200
and then the first four iterations at

00:56:53,040 --> 00:56:55,920
the top and we can see that x is now

00:56:55,200 --> 00:56:58,400
being set

00:56:55,920 --> 00:56:58,960
inside the old value of x inside each of

00:56:58,400 --> 00:57:01,520
the threads

00:56:58,960 --> 00:57:03,200
is set to negative one this is in

00:57:01,520 --> 00:57:04,079
contrast i'm going to go back to slide

00:57:03,200 --> 00:57:07,040
eight

00:57:04,079 --> 00:57:08,880
you can see the first four outputs for

00:57:07,040 --> 00:57:11,040
the threads you see x equals zero

00:57:08,880 --> 00:57:12,400
it just happened to be zero but this

00:57:11,040 --> 00:57:14,880
we're now seeing that the

00:57:12,400 --> 00:57:16,480
the each of the local copies of x takes

00:57:14,880 --> 00:57:19,599
on the value

00:57:16,480 --> 00:57:20,960
um of whatever x was before we entered

00:57:19,599 --> 00:57:23,359
the iteration

00:57:20,960 --> 00:57:24,400
so that this is you can see that that

00:57:23,359 --> 00:57:26,319
behavior happening here

00:57:24,400 --> 00:57:28,960
again it updates it to the parts of the

00:57:26,319 --> 00:57:30,319
iteration space it's then executing

00:57:28,960 --> 00:57:32,000
and when we finish we see all those

00:57:30,319 --> 00:57:33,280
local copies that we had of x sort of

00:57:32,000 --> 00:57:36,079
disappear

00:57:33,280 --> 00:57:37,040
um and x just takes whatever whatever

00:57:36,079 --> 00:57:38,960
value it happened to be

00:57:37,040 --> 00:57:40,400
in this case it wasn't modified but but

00:57:38,960 --> 00:57:43,040
it could easily have just been

00:57:40,400 --> 00:57:45,359
is undefined exactly what that x happens

00:57:43,040 --> 00:57:45,359
to be

00:57:45,680 --> 00:57:51,359
so now on to last private um

00:57:48,720 --> 00:57:52,400
you can now see here that with

00:57:51,359 --> 00:57:55,040
lastprivate

00:57:52,400 --> 00:57:56,720
the values just take some random value

00:57:55,040 --> 00:57:58,720
as before so they don't happen to be

00:57:56,720 --> 00:58:00,720
zero anymore they just

00:57:58,720 --> 00:58:02,480
they randomly got some other numbers and

00:58:00,720 --> 00:58:04,160
they do the same setting to whatever

00:58:02,480 --> 00:58:06,160
number they need to

00:58:04,160 --> 00:58:08,000
but the after here is what's changed

00:58:06,160 --> 00:58:11,680
with lastprivate that says

00:58:08,000 --> 00:58:13,440
that the the variable x that was before

00:58:11,680 --> 00:58:16,480
the parallel region will be set

00:58:13,440 --> 00:58:19,839
to whatever the last thread did last

00:58:16,480 --> 00:58:20,960
um in that parallel region so in here is

00:58:19,839 --> 00:58:22,559
thread three

00:58:20,960 --> 00:58:24,079
thread three is going to be our last

00:58:22,559 --> 00:58:25,839
thread and it's gonna be the last time

00:58:24,079 --> 00:58:28,799
that execute so whatever

00:58:25,839 --> 00:58:29,599
thread three sets x two last that's what

00:58:28,799 --> 00:58:31,440
x will be

00:58:29,599 --> 00:58:33,440
after the parallel region and we see

00:58:31,440 --> 00:58:35,119
that here on about line

00:58:33,440 --> 00:58:37,920
uh four of the output that's when the

00:58:35,119 --> 00:58:39,280
the last time thread 3 ran and we see it

00:58:37,920 --> 00:58:41,040
sets x to 10

00:58:39,280 --> 00:58:44,880
and that's exactly the value that it

00:58:41,040 --> 00:58:46,640
takes when it exits the loop

00:58:44,880 --> 00:58:48,720
so in this way we can then use these

00:58:46,640 --> 00:58:50,480
different private clauses to

00:58:48,720 --> 00:58:52,240
make sure that we understand what

00:58:50,480 --> 00:58:53,680
variables are going to be set to

00:58:52,240 --> 00:58:56,400
entering the loop and what they're going

00:58:53,680 --> 00:58:59,359
to be after the loop has finished

00:58:56,400 --> 00:59:01,200
typically we just use private as that's

00:58:59,359 --> 00:59:03,440
we just use these variables as temporary

00:59:01,200 --> 00:59:05,359
values within the body of the loop

00:59:03,440 --> 00:59:07,119
but sometimes you need to make sure you

00:59:05,359 --> 00:59:09,280
have the right value going in

00:59:07,119 --> 00:59:10,720
and then use first private and other

00:59:09,280 --> 00:59:12,960
times you need to make sure that

00:59:10,720 --> 00:59:14,079
you pick up the last value as if it was

00:59:12,960 --> 00:59:18,559
a sequential code

00:59:14,079 --> 00:59:21,119
and that's where last private comes in

00:59:18,559 --> 00:59:22,079
we can choose the default data sharing

00:59:21,119 --> 00:59:24,880
um

00:59:22,079 --> 00:59:26,799
this is useful in fortran because pretty

00:59:24,880 --> 00:59:28,400
much variables all have this global

00:59:26,799 --> 00:59:29,599
scope within the subroutine so

00:59:28,400 --> 00:59:31,440
everything is going to be shared by

00:59:29,599 --> 00:59:33,520
default

00:59:31,440 --> 00:59:35,599
so you can force yourself to manually

00:59:33,520 --> 00:59:36,960
specify everything by using the default

00:59:35,599 --> 00:59:38,880
none clause

00:59:36,960 --> 00:59:40,000
and that makes sure you must specify for

00:59:38,880 --> 00:59:42,319
every variable

00:59:40,000 --> 00:59:45,119
whether it's shared private first

00:59:42,319 --> 00:59:47,760
private last private etc

00:59:45,119 --> 00:59:49,760
alternatively you can say default

00:59:47,760 --> 00:59:50,319
private or default first private to make

00:59:49,760 --> 00:59:52,400
everything

00:59:50,319 --> 00:59:53,920
private by default and if you have a

00:59:52,400 --> 00:59:55,200
very old code

00:59:53,920 --> 00:59:56,799
with loads and loads of temporary

00:59:55,200 --> 00:59:58,079
variables that might save a lot of

00:59:56,799 --> 01:00:00,559
typing

00:59:58,079 --> 01:00:02,160
the default behavior is is as if you'd

01:00:00,559 --> 01:00:05,680
had a default shared

01:00:02,160 --> 01:00:07,280
um clause on the parallel region and and

01:00:05,680 --> 01:00:09,440
using these default clauses you can

01:00:07,280 --> 01:00:11,599
change what that default uh happens to

01:00:09,440 --> 01:00:11,599
be

01:00:12,079 --> 01:00:15,200
so that's data sharing and and part of

01:00:14,319 --> 01:00:17,200
that is is

01:00:15,200 --> 01:00:18,799
what happens when you need to um do

01:00:17,200 --> 01:00:20,880
something like a reduction where you

01:00:18,799 --> 01:00:22,720
have to have threads cooperatively

01:00:20,880 --> 01:00:23,920
working together to compute a single

01:00:22,720 --> 01:00:26,400
value

01:00:23,920 --> 01:00:28,000
and for that i'm going to use um a kind

01:00:26,400 --> 01:00:31,040
of classic parallel example of

01:00:28,000 --> 01:00:33,280
integrating pi um

01:00:31,040 --> 01:00:34,640
so this is the the equation we just

01:00:33,280 --> 01:00:37,359
integrate this curve

01:00:34,640 --> 01:00:39,599
and the area under that curve is pi and

01:00:37,359 --> 01:00:40,799
we do it by breaking it up into a number

01:00:39,599 --> 01:00:43,760
of boxes

01:00:40,799 --> 01:00:44,640
and we basically the the trapezoidal

01:00:43,760 --> 01:00:46,480
rule

01:00:44,640 --> 01:00:49,440
and we calculate the area of each of the

01:00:46,480 --> 01:00:52,319
boxes and we add them all together

01:00:49,440 --> 01:00:54,319
and that gives us the value of pi with

01:00:52,319 --> 01:00:55,599
more and more accuracy with more and

01:00:54,319 --> 01:00:57,520
more boxes

01:00:55,599 --> 01:00:58,640
that we have so the step size

01:00:57,520 --> 01:01:02,400
essentially how

01:00:58,640 --> 01:01:04,720
wide is the box along the x-axis

01:01:02,400 --> 01:01:06,640
so this is our code we set the number of

01:01:04,720 --> 01:01:07,680
steps the number of boxes and we break

01:01:06,640 --> 01:01:11,599
up the x-axis

01:01:07,680 --> 01:01:13,440
between 0 and to give us a step size

01:01:11,599 --> 01:01:15,119
we then loop over all the boxes and

01:01:13,440 --> 01:01:18,160
compute the area

01:01:15,119 --> 01:01:20,400
which is uh on line four

01:01:18,160 --> 01:01:22,160
and we add it to sum so we're summing up

01:01:20,400 --> 01:01:23,599
the area and then we have to factor by

01:01:22,160 --> 01:01:25,920
the step size

01:01:23,599 --> 01:01:28,079
to get pi the full implementation of

01:01:25,920 --> 01:01:29,359
this is in in pi dot f90 that i've given

01:01:28,079 --> 01:01:31,680
you all

01:01:29,359 --> 01:01:33,359
and with a hundred million steps you can

01:01:31,680 --> 01:01:35,359
do this with lots and lots of boxes

01:01:33,359 --> 01:01:36,960
it takes you know 0.4 seconds on my

01:01:35,359 --> 01:01:39,680
laptop which has

01:01:36,960 --> 01:01:42,640
um i think maybe two or four parallel

01:01:39,680 --> 01:01:42,640
threads i was running with

01:01:43,920 --> 01:01:48,079
so what about parallelizing that loop

01:01:46,799 --> 01:01:49,920
that was a serial loop

01:01:48,079 --> 01:01:52,240
so i was just running this code and it

01:01:49,920 --> 01:01:53,760
took 0.4 seconds serial code and that's

01:01:52,240 --> 01:01:54,240
all gonna behave as expected we're

01:01:53,760 --> 01:01:57,359
running

01:01:54,240 --> 01:01:58,880
and summing up all the numbers so now we

01:01:57,359 --> 01:02:00,799
want to use a work sharing directive to

01:01:58,880 --> 01:02:04,160
parallelize the loop

01:02:00,799 --> 01:02:05,119
so on line four we have x x is being

01:02:04,160 --> 01:02:08,160
updated

01:02:05,119 --> 01:02:10,559
independently by every thread so we're

01:02:08,160 --> 01:02:11,839
only going to use x inside the loop

01:02:10,559 --> 01:02:13,359
but we need to make sure that the

01:02:11,839 --> 01:02:14,319
threads don't interact with each other

01:02:13,359 --> 01:02:16,319
when they're

01:02:14,319 --> 01:02:19,200
writing it so x needs to be private

01:02:16,319 --> 01:02:21,680
variable to every thread

01:02:19,200 --> 01:02:22,799
so we have a parallel do and inside the

01:02:21,680 --> 01:02:24,559
loop ii

01:02:22,799 --> 01:02:26,400
is going to be private by default

01:02:24,559 --> 01:02:29,280
because it's the loop iterator

01:02:26,400 --> 01:02:30,799
openmp tells us that we set x to be

01:02:29,280 --> 01:02:32,079
private because all threads need their

01:02:30,799 --> 01:02:34,000
own copy of x

01:02:32,079 --> 01:02:36,160
and some needs to be shared because all

01:02:34,000 --> 01:02:39,039
threads need to update some

01:02:36,160 --> 01:02:40,960
so we're going to leave that as shared

01:02:39,039 --> 01:02:41,920
but we need to be careful because we

01:02:40,960 --> 01:02:44,400
can't have

01:02:41,920 --> 01:02:45,760
any thread updating some all at the same

01:02:44,400 --> 01:02:48,240
time we're going to have a race

01:02:45,760 --> 01:02:51,280
condition so we need to update

01:02:48,240 --> 01:02:53,440
some with care we're only going to

01:02:51,280 --> 01:02:54,720
start off by saying well let's only let

01:02:53,440 --> 01:02:57,520
one thread

01:02:54,720 --> 01:02:59,280
execute that sum at any one time this

01:02:57,520 --> 01:03:02,319
means we can have a

01:02:59,280 --> 01:03:04,559
guaranteed one thread uh adding its

01:03:02,319 --> 01:03:07,359
contribution to some at any one time

01:03:04,559 --> 01:03:08,640
now openmp gives us the critical uh

01:03:07,359 --> 01:03:10,799
region for this so we

01:03:08,640 --> 01:03:12,480
around that sum i've now added on line

01:03:10,799 --> 01:03:15,200
five and seven a region

01:03:12,480 --> 01:03:15,520
an omp critical and an omp end critical

01:03:15,200 --> 01:03:18,640
to

01:03:15,520 --> 01:03:18,640
to show where it ends

01:03:18,799 --> 01:03:22,720
anything inside a critical region openmp

01:03:21,599 --> 01:03:25,200
guarantees that

01:03:22,720 --> 01:03:26,000
only one thread will execute it at a

01:03:25,200 --> 01:03:28,240
time so

01:03:26,000 --> 01:03:29,200
only one thread is going to compute the

01:03:28,240 --> 01:03:31,599
sum the 4

01:03:29,200 --> 01:03:32,720
divided by one plus x squared and then

01:03:31,599 --> 01:03:35,760
it's going to add it to

01:03:32,720 --> 01:03:37,520
sum and update some only one at a time

01:03:35,760 --> 01:03:38,880
there's no guarantees of ordering so we

01:03:37,520 --> 01:03:40,880
don't know which thread

01:03:38,880 --> 01:03:42,520
is going to update some at any given

01:03:40,880 --> 01:03:45,039
time but all we know is that it's

01:03:42,520 --> 01:03:47,520
guaranteed

01:03:45,039 --> 01:03:49,200
to only be one of them no we couldn't

01:03:47,520 --> 01:03:51,200
really use a barrier here because the

01:03:49,200 --> 01:03:53,039
barrier only makes sure that

01:03:51,200 --> 01:03:54,960
um threads are synchronized it doesn't

01:03:53,039 --> 01:03:55,359
control how many threads are updating

01:03:54,960 --> 01:03:57,119
some

01:03:55,359 --> 01:03:58,400
but also we're inside a do loop so

01:03:57,119 --> 01:03:59,839
there's no guarantee that all the

01:03:58,400 --> 01:04:01,359
threads will actually reach the barrier

01:03:59,839 --> 01:04:03,359
inside the door loop

01:04:01,359 --> 01:04:05,359
so the critical region is the first kind

01:04:03,359 --> 01:04:08,319
of attempt at running this

01:04:05,359 --> 01:04:10,240
in parallel making sure some is updated

01:04:08,319 --> 01:04:12,000
safely

01:04:10,240 --> 01:04:14,640
so yeah on my laptop with four threads

01:04:12,000 --> 01:04:16,799
the serial code was 0.4 seconds but

01:04:14,640 --> 01:04:18,160
running with critical is now over 400

01:04:16,799 --> 01:04:20,079
seconds with the same

01:04:18,160 --> 01:04:21,760
100 million iterations that's that's

01:04:20,079 --> 01:04:23,280
really slow things are bad

01:04:21,760 --> 01:04:26,559
that's probably not the right approach

01:04:23,280 --> 01:04:28,400
for calculating pi in parallel but

01:04:26,559 --> 01:04:29,680
it's important for correctness that sum

01:04:28,400 --> 01:04:32,000
is updated

01:04:29,680 --> 01:04:34,960
essentially with care with with only one

01:04:32,000 --> 01:04:37,359
thread accessing it at any given time

01:04:34,960 --> 01:04:39,359
so openmp gives us other ways to do this

01:04:37,359 --> 01:04:42,559
we can use atomics

01:04:39,359 --> 01:04:44,720
critical protects a whole block of code

01:04:42,559 --> 01:04:45,599
that all that stuff inside the critical

01:04:44,720 --> 01:04:48,000
and end critical

01:04:45,599 --> 01:04:49,760
that's what gets executed only by one

01:04:48,000 --> 01:04:51,440
foot at a time

01:04:49,760 --> 01:04:52,839
but if we just have a single operation

01:04:51,440 --> 01:04:56,079
that we want to do

01:04:52,839 --> 01:04:58,000
so for us we just want to add

01:04:56,079 --> 01:05:00,079
a number to some that's a single

01:04:58,000 --> 01:05:03,520
operation we can use atomic instead

01:05:00,079 --> 01:05:03,839
atomic operation and atomic operations

01:05:03,520 --> 01:05:06,960
are

01:05:03,839 --> 01:05:08,000
often supported by the hardware and and

01:05:06,960 --> 01:05:10,160
ensure that the hardware

01:05:08,000 --> 01:05:11,520
makes sure that only that value is

01:05:10,160 --> 01:05:13,200
updated by one

01:05:11,520 --> 01:05:16,880
thing at a time and that update is

01:05:13,200 --> 01:05:18,559
successful at when it when it happens

01:05:16,880 --> 01:05:20,880
so atomic operations are all defined

01:05:18,559 --> 01:05:22,160
with respect to the memory access of a

01:05:20,880 --> 01:05:24,960
scalar variable

01:05:22,160 --> 01:05:26,000
so if we have a variable x we could have

01:05:24,960 --> 01:05:28,160
a read atomic

01:05:26,000 --> 01:05:30,799
which that would be useful for for

01:05:28,160 --> 01:05:32,319
taking a copy of x atomically so it's

01:05:30,799 --> 01:05:33,359
not going to be changed halfway through

01:05:32,319 --> 01:05:35,280
reading it

01:05:33,359 --> 01:05:36,799
also for writing x we could have an

01:05:35,280 --> 01:05:39,680
atomic write to make sure that

01:05:36,799 --> 01:05:41,440
if we want to update x set x to a

01:05:39,680 --> 01:05:44,880
particular value

01:05:41,440 --> 01:05:46,079
so just a write only no read we can have

01:05:44,880 --> 01:05:48,000
a right atomic

01:05:46,079 --> 01:05:49,920
or there's an update atomic as well that

01:05:48,000 --> 01:05:54,559
allows us to

01:05:49,920 --> 01:05:57,119
to read x update it and write it back

01:05:54,559 --> 01:05:58,000
and there's also uh capture as well for

01:05:57,119 --> 01:06:01,920
read write and

01:05:58,000 --> 01:06:05,280
update so to have an atomic pie then

01:06:01,920 --> 01:06:06,880
we might add the atomic constructs omp

01:06:05,280 --> 01:06:08,160
atomic above the sum line

01:06:06,880 --> 01:06:11,280
so again this is going to run in

01:06:08,160 --> 01:06:13,680
parallel but update some atomically

01:06:11,280 --> 01:06:15,359
this is much better we've now improved

01:06:13,680 --> 01:06:16,079
our critical time down to only eight

01:06:15,359 --> 01:06:18,799
seconds for

01:06:16,079 --> 01:06:21,920
atomic is still faster than critical but

01:06:18,799 --> 01:06:21,920
much lower than serial

01:06:22,559 --> 01:06:26,079
what what we're finding though is that

01:06:24,400 --> 01:06:27,200
both methods are causing those threads

01:06:26,079 --> 01:06:29,119
to synchronize

01:06:27,200 --> 01:06:30,720
every time they update some either at

01:06:29,119 --> 01:06:33,520
the hardware level with atomics

01:06:30,720 --> 01:06:35,039
or with the critical regions so only one

01:06:33,520 --> 01:06:36,799
thread can run

01:06:35,039 --> 01:06:37,839
but we could do this um differently and

01:06:36,799 --> 01:06:38,640
we could pick a slightly different

01:06:37,839 --> 01:06:40,400
algorithm

01:06:38,640 --> 01:06:42,799
we could have each thread calculate its

01:06:40,400 --> 01:06:46,079
own partial sum independently

01:06:42,799 --> 01:06:47,599
and then adding it up at the end

01:06:46,079 --> 01:06:49,440
just once at the end so we'd only have

01:06:47,599 --> 01:06:51,359
to synchronize just at the very end

01:06:49,440 --> 01:06:53,119
so to do this i'm going to make an array

01:06:51,359 --> 01:06:55,599
called sum equal to the

01:06:53,119 --> 01:06:57,520
the length of the number of threads so

01:06:55,599 --> 01:06:59,760
every there's now a shared array

01:06:57,520 --> 01:07:01,680
with a space in it for every parallel

01:06:59,760 --> 01:07:04,240
thread and in each thread is going to

01:07:01,680 --> 01:07:06,960
store its partial sum in there

01:07:04,240 --> 01:07:08,160
so as it's shared memory i can just add

01:07:06,960 --> 01:07:10,880
up that sum in serial

01:07:08,160 --> 01:07:11,520
at the end it's going to be fine to read

01:07:10,880 --> 01:07:13,200
all of that

01:07:11,520 --> 01:07:15,119
update because i'll have stops and i've

01:07:13,200 --> 01:07:16,400
had an n do and a barrier so i'll be

01:07:15,119 --> 01:07:19,440
able to update the sum

01:07:16,400 --> 01:07:20,720
array and compute sum

01:07:19,440 --> 01:07:22,960
so this is the code here i've now

01:07:20,720 --> 01:07:24,640
allocated this sum array and i'm using

01:07:22,960 --> 01:07:27,200
my my thread id

01:07:24,640 --> 01:07:28,319
in order to which i've queried within

01:07:27,200 --> 01:07:30,960
the parallel region

01:07:28,319 --> 01:07:32,720
in order to do the sum now i've also

01:07:30,960 --> 01:07:34,480
added the flush here and this is a bit

01:07:32,720 --> 01:07:36,960
of a cheat but it makes sure that

01:07:34,480 --> 01:07:38,000
the value of sum is is properly written

01:07:36,960 --> 01:07:39,520
back to main memory

01:07:38,000 --> 01:07:42,480
we want to make sure that the sum array

01:07:39,520 --> 01:07:45,599
is written back to main memory

01:07:42,480 --> 01:07:47,680
and this is better we see the

01:07:45,599 --> 01:07:49,280
the improvement go up uh significantly

01:07:47,680 --> 01:07:51,920
again so we're getting much closer to

01:07:49,280 --> 01:07:53,760
our serial time

01:07:51,920 --> 01:07:56,000
but this code is is susceptible to

01:07:53,760 --> 01:07:57,760
what's known as false sharing

01:07:56,000 --> 01:07:59,119
and this false sharing occurs when

01:07:57,760 --> 01:08:00,960
different threads are going to be

01:07:59,119 --> 01:08:03,760
accessing different data

01:08:00,960 --> 01:08:05,520
and updating it specifically on the same

01:08:03,760 --> 01:08:07,760
cache line

01:08:05,520 --> 01:08:08,559
so we have our cause and there's a cache

01:08:07,760 --> 01:08:10,799
hierarchy

01:08:08,559 --> 01:08:13,039
and and memory is moved from main memory

01:08:10,799 --> 01:08:14,319
and stored and saved in the caches in

01:08:13,039 --> 01:08:15,039
these things called cache lines

01:08:14,319 --> 01:08:18,799
typically

01:08:15,039 --> 01:08:21,440
64 bytes um long

01:08:18,799 --> 01:08:22,400
so you get a number of double precision

01:08:21,440 --> 01:08:26,080
elements

01:08:22,400 --> 01:08:27,120
on there because the cache system is

01:08:26,080 --> 01:08:28,799
coherent between

01:08:27,120 --> 01:08:30,239
cores it must mean this means the

01:08:28,799 --> 01:08:32,960
hardware must make sure that

01:08:30,239 --> 01:08:34,880
if we update the cache line in any one

01:08:32,960 --> 01:08:37,440
core it has to be propagated to any

01:08:34,880 --> 01:08:39,839
other call that has that saved in cache

01:08:37,440 --> 01:08:40,880
so we have to flush it to re to memory

01:08:39,839 --> 01:08:42,799
and read it back

01:08:40,880 --> 01:08:44,880
uh every time we access something that's

01:08:42,799 --> 01:08:46,640
been updated elsewhere

01:08:44,880 --> 01:08:48,400
so our code that we wrote has got this

01:08:46,640 --> 01:08:50,400
cache flashing and that's what the

01:08:48,400 --> 01:08:52,319
flush there that i put in did it made

01:08:50,400 --> 01:08:54,640
sure that the data went back to memory

01:08:52,319 --> 01:08:56,960
so it caused that cache flashing to

01:08:54,640 --> 01:08:56,960
occur

01:08:57,359 --> 01:09:01,120
um i've i've just said all of that

01:09:01,440 --> 01:09:07,199
so we could uh instead

01:09:04,640 --> 01:09:09,199
to mitigate the false sharing use this

01:09:07,199 --> 01:09:10,000
idea of data sharing we could use first

01:09:09,199 --> 01:09:12,480
private

01:09:10,000 --> 01:09:14,640
we could make uh we could give every

01:09:12,480 --> 01:09:15,839
thread a scalar copy of some just a

01:09:14,640 --> 01:09:17,920
regular scalar

01:09:15,839 --> 01:09:19,359
use openmp to make sure it's a separate

01:09:17,920 --> 01:09:20,000
copy and nowhere near each other in

01:09:19,359 --> 01:09:22,080
cache so

01:09:20,000 --> 01:09:23,440
make sure it's first private first

01:09:22,080 --> 01:09:25,679
private is useful because we can

01:09:23,440 --> 01:09:27,440
initialize it to zero before we enter

01:09:25,679 --> 01:09:29,199
the parallel region

01:09:27,440 --> 01:09:30,960
and then we can just reduce it using a

01:09:29,199 --> 01:09:33,520
critical region with

01:09:30,960 --> 01:09:35,440
with an atomic or a sum at the end so

01:09:33,520 --> 01:09:37,759
notice here i've got a separate

01:09:35,440 --> 01:09:39,679
lines for parallel undo i spore my

01:09:37,759 --> 01:09:41,920
threads they fork

01:09:39,679 --> 01:09:43,440
i run a do loop with its own first

01:09:41,920 --> 01:09:46,239
private copy of some

01:09:43,440 --> 01:09:48,000
i end the do we synchronize and then we

01:09:46,239 --> 01:09:49,359
have a critical region where

01:09:48,000 --> 01:09:52,159
every thread it could have been an

01:09:49,359 --> 01:09:52,719
atomic adds up its contribution of the

01:09:52,159 --> 01:09:54,800
sum

01:09:52,719 --> 01:09:57,600
to the final value of pi and that's

01:09:54,800 --> 01:09:59,600
gonna produces the final answer

01:09:57,600 --> 01:10:00,960
and here we are we're now nearly four

01:09:59,600 --> 01:10:02,640
times faster three and a half times

01:10:00,960 --> 01:10:03,440
faster on four threads than i was with

01:10:02,640 --> 01:10:06,480
serial

01:10:03,440 --> 01:10:08,480
with this first private approach

01:10:06,480 --> 01:10:10,640
well that's a hassle so openmp helps us

01:10:08,480 --> 01:10:12,080
out we have reduction clauses that allow

01:10:10,640 --> 01:10:13,600
us to essentially do this sort of

01:10:12,080 --> 01:10:16,480
behavior

01:10:13,600 --> 01:10:19,120
implemented by the runtime there's all

01:10:16,480 --> 01:10:21,520
sorts of different operations we can set

01:10:19,120 --> 01:10:23,600
um to do and the format is really the

01:10:21,520 --> 01:10:25,360
same it's a reduction clause

01:10:23,600 --> 01:10:26,719
it has to be on a shared variable called

01:10:25,360 --> 01:10:28,960
var

01:10:26,719 --> 01:10:32,880
and we give it an operation so plus

01:10:28,960 --> 01:10:35,199
minus or min max anything like that

01:10:32,880 --> 01:10:36,320
this means that then the each element is

01:10:35,199 --> 01:10:38,960
then

01:10:36,320 --> 01:10:40,080
each each variable is then essentially

01:10:38,960 --> 01:10:42,719
its own

01:10:40,080 --> 01:10:43,679
partial sum or partial operation in each

01:10:42,719 --> 01:10:45,440
of the threads

01:10:43,679 --> 01:10:47,600
and then it's added up or whatever the

01:10:45,440 --> 01:10:50,239
operation is is combined at the end

01:10:47,600 --> 01:10:51,840
um so that the result is is what we'd

01:10:50,239 --> 01:10:54,000
expect

01:10:51,840 --> 01:10:55,679
we can also do a ray reduction so var

01:10:54,000 --> 01:10:58,719
could be an array

01:10:55,679 --> 01:11:00,000
and in that case the um the it's a it's

01:10:58,719 --> 01:11:03,040
a reduction on each

01:11:00,000 --> 01:11:04,560
individual element so it's a number of

01:11:03,040 --> 01:11:06,960
separate reductions it's similar to

01:11:04,560 --> 01:11:10,840
doing um mpi or reduce

01:11:06,960 --> 01:11:12,560
on a an array where it does that sort of

01:11:10,840 --> 01:11:14,560
behavior

01:11:12,560 --> 01:11:15,920
so this is the final pi reduction i

01:11:14,560 --> 01:11:17,440
didn't have to use first private and

01:11:15,920 --> 01:11:20,320
critical regions or anything

01:11:17,440 --> 01:11:21,600
i just added parallel do private x to

01:11:20,320 --> 01:11:23,760
make sure x was private

01:11:21,600 --> 01:11:26,560
and the reduction clause on my loop

01:11:23,760 --> 01:11:28,239
reduction is a sum a plus

01:11:26,560 --> 01:11:29,600
is the operation and it's over the

01:11:28,239 --> 01:11:31,280
variable sum

01:11:29,600 --> 01:11:33,600
and i know it's plus because on line

01:11:31,280 --> 01:11:35,199
five i'm adding a number to some

01:11:33,600 --> 01:11:36,640
so that's the reduction that we need to

01:11:35,199 --> 01:11:40,719
add up all the partial

01:11:36,640 --> 01:11:42,640
sums and save them

01:11:40,719 --> 01:11:44,159
so that's data sharing and reduction

01:11:42,640 --> 01:11:45,600
clauses in a nutshell

01:11:44,159 --> 01:11:47,760
and you can see that with the reduction

01:11:45,600 --> 01:11:49,520
clause it's pretty much the same as the

01:11:47,760 --> 01:11:50,640
first private time it's just slightly

01:11:49,520 --> 01:11:52,480
faster

01:11:50,640 --> 01:11:54,080
because it's going to do a better job of

01:11:52,480 --> 01:11:56,560
of all the implementation details

01:11:54,080 --> 01:11:58,320
then we could do with first private so

01:11:56,560 --> 01:11:59,600
we're nearly four x faster on four

01:11:58,320 --> 01:12:01,120
threads which is which is

01:11:59,600 --> 01:12:02,719
kind of perfect speed up which is what

01:12:01,120 --> 01:12:06,080
we'd expect for something

01:12:02,719 --> 01:12:07,440
like this that's uh easy to parallelize

01:12:06,080 --> 01:12:09,760
so with that actually we're now at the

01:12:07,440 --> 01:12:10,960
coffee break um so if you'd like to step

01:12:09,760 --> 01:12:13,360
away then do

01:12:10,960 --> 01:12:14,320
um after the coffee break which will be

01:12:13,360 --> 01:12:17,280
a quarter past

01:12:14,320 --> 01:12:18,080
11 uk time so 15 minute coffee break

01:12:17,280 --> 01:12:20,400
we're gonna

01:12:18,080 --> 01:12:22,640
come back to this exercise and this is

01:12:20,400 --> 01:12:24,000
to take your parallel five-point stencil

01:12:22,640 --> 01:12:26,480
from last time

01:12:24,000 --> 01:12:27,360
and we're going to add a reduction um in

01:12:26,480 --> 01:12:31,120
it

01:12:27,360 --> 01:12:33,360
so we're going to sum up the value in

01:12:31,120 --> 01:12:35,440
all the cells every time step

01:12:33,360 --> 01:12:36,400
and um print that out to screen just so

01:12:35,440 --> 01:12:39,040
you can keep um

01:12:36,400 --> 01:12:40,640
an idea of of that and maybe you can try

01:12:39,040 --> 01:12:42,080
different ideas maybe try some of the

01:12:40,640 --> 01:12:44,080
critical some of the atomic some of the

01:12:42,080 --> 01:12:46,320
reductions things like that

01:12:44,080 --> 01:12:47,920
so we're going to go to a break now um a

01:12:46,320 --> 01:12:49,920
quarter past i'll come back and show

01:12:47,920 --> 01:12:52,960
this exercise again

01:12:49,920 --> 01:12:55,920
and and then we'll run that exercise um

01:12:52,960 --> 01:12:57,760
until for about 20 minutes after that so

01:12:55,920 --> 01:12:58,560
the next kind of speaking session will

01:12:57,760 --> 01:13:02,239
be at 11

01:12:58,560 --> 01:13:06,159
35 but we'll start the exercise at 11

01:13:02,239 --> 01:13:08,159
15 uk time so i'll see you in in

01:13:06,159 --> 01:13:10,719
nearly 10 minutes now have a have a good

01:13:08,159 --> 01:13:10,719
coffee break

01:13:11,600 --> 01:13:15,600
okay welcome back from the uh the short

01:13:14,239 --> 01:13:18,640
coffee break

01:13:15,600 --> 01:13:20,719
um we're gonna go straight into uh an

01:13:18,640 --> 01:13:23,760
exercise which i introduced a little bit

01:13:20,719 --> 01:13:25,280
uh before the break but uh before we we

01:13:23,760 --> 01:13:27,840
do that

01:13:25,280 --> 01:13:28,560
i just thought it'd be nice to um just

01:13:27,840 --> 01:13:30,320
sort of

01:13:28,560 --> 01:13:32,080
say how far we've come we've come a long

01:13:30,320 --> 01:13:35,360
way already really

01:13:32,080 --> 01:13:38,560
we've covered a good part of the most

01:13:35,360 --> 01:13:40,400
commonly used bits of openmp

01:13:38,560 --> 01:13:43,120
so if you think about sort of an 80 20

01:13:40,400 --> 01:13:45,520
rule most programs that

01:13:43,120 --> 01:13:46,320
that you write for cpus will only use

01:13:45,520 --> 01:13:48,320
what what

01:13:46,320 --> 01:13:50,080
what we've been going through so far

01:13:48,320 --> 01:13:52,719
things like parallel do

01:13:50,080 --> 01:13:53,760
the data sharing clauses and the uh the

01:13:52,719 --> 01:13:56,880
reduction

01:13:53,760 --> 01:13:59,520
no the reduction clause so openmp is

01:13:56,880 --> 01:14:00,080
is a little bit uh deceptively simple

01:13:59,520 --> 01:14:02,800
obviously

01:14:00,080 --> 01:14:03,520
parallel programming is hard and and it

01:14:02,800 --> 01:14:05,600
keeps us all

01:14:03,520 --> 01:14:07,600
um very busy and trying to understand

01:14:05,600 --> 01:14:08,080
and find the parallelism in our programs

01:14:07,600 --> 01:14:10,080
and

01:14:08,080 --> 01:14:11,760
obviously that's something that we can

01:14:10,080 --> 01:14:13,920
we can do

01:14:11,760 --> 01:14:15,040
and we have to do as programmers but

01:14:13,920 --> 01:14:16,640
openmp is

01:14:15,040 --> 01:14:18,239
is very simple and actually then once

01:14:16,640 --> 01:14:19,920
we've found the parallelism

01:14:18,239 --> 01:14:22,239
being able to express that we can just

01:14:19,920 --> 01:14:23,760
use parallel do and make sure the data

01:14:22,239 --> 01:14:26,960
sharing is is correct

01:14:23,760 --> 01:14:29,440
sometimes with reductions as well

01:14:26,960 --> 01:14:30,239
um so we're going to go into an exercise

01:14:29,440 --> 01:14:31,840
um

01:14:30,239 --> 01:14:33,199
now and and when that's finished i'm

01:14:31,840 --> 01:14:33,600
going to be talking a little bit about

01:14:33,199 --> 01:14:36,800
um

01:14:33,600 --> 01:14:37,760
optimizing openmp codes um on numer

01:14:36,800 --> 01:14:39,360
architectures

01:14:37,760 --> 01:14:41,199
and then this afternoon after lunch

01:14:39,360 --> 01:14:44,880
we'll move on to openmp

01:14:41,199 --> 01:14:46,560
on gpus and and that will build on um

01:14:44,880 --> 01:14:49,360
uh all the things that you have you've

01:14:46,560 --> 01:14:49,360
learned this morning

01:14:49,520 --> 01:14:53,920
so for now the exercise is is this we've

01:14:52,000 --> 01:14:56,320
we've already got a parallel five-point

01:14:53,920 --> 01:14:57,840
stencil code from last time

01:14:56,320 --> 01:14:59,600
is that stencil code at the bottom of

01:14:57,840 --> 01:15:02,719
the slides and this time i'm saying well

01:14:59,600 --> 01:15:06,000
let's change the kernel a little bit

01:15:02,719 --> 01:15:09,120
let's compute the sum over

01:15:06,000 --> 01:15:12,159
all of the all of those

01:15:09,120 --> 01:15:13,760
um elements that have just updated so it

01:15:12,159 --> 01:15:16,719
should say a new

01:15:13,760 --> 01:15:18,800
so total equals total plus a new so i'll

01:15:16,719 --> 01:15:20,480
change that when we get started

01:15:18,800 --> 01:15:22,640
so let's add that line to the middle of

01:15:20,480 --> 01:15:24,719
our kernel the middle of our do loops

01:15:22,640 --> 01:15:26,880
so now we have to worry about um

01:15:24,719 --> 01:15:28,800
updating total every time

01:15:26,880 --> 01:15:30,640
so this is a reduction over the whole

01:15:28,800 --> 01:15:31,840
mesh and maybe add a print statement

01:15:30,640 --> 01:15:34,080
just to make sure that

01:15:31,840 --> 01:15:35,120
that value is up you know it's going to

01:15:34,080 --> 01:15:38,239
give us the right answers

01:15:35,120 --> 01:15:38,640
as we go so that's the goal we're going

01:15:38,239 --> 01:15:41,679
to

01:15:38,640 --> 01:15:44,000
print out the total every time step

01:15:41,679 --> 01:15:45,360
and you'll need to implement and augment

01:15:44,000 --> 01:15:47,360
the code that you've got

01:15:45,360 --> 01:15:49,040
with a parallel reduction to do this and

01:15:47,360 --> 01:15:51,280
you can try using the criticals and the

01:15:49,040 --> 01:15:54,239
atomics and and the reduction clause

01:15:51,280 --> 01:15:56,320
and to do that so that's the exercise um

01:15:54,239 --> 01:15:59,360
i'll pick this back up at 11

01:15:56,320 --> 01:16:01,840
35 and we'll we'll just walk then

01:15:59,360 --> 01:16:04,320
through at that point some of the extra

01:16:01,840 --> 01:16:05,360
things about vectorization and and

01:16:04,320 --> 01:16:07,040
pneuma

01:16:05,360 --> 01:16:08,960
and then there'll be a final exercise to

01:16:07,040 --> 01:16:09,280
try out those optimizations before lunch

01:16:08,960 --> 01:16:12,400
and

01:16:09,280 --> 01:16:14,880
that will build on this this code so

01:16:12,400 --> 01:16:16,320
uh please use the q a uh lots of

01:16:14,880 --> 01:16:18,239
questions coming through already and

01:16:16,320 --> 01:16:20,480
and i'll of course keep an eye on that

01:16:18,239 --> 01:16:23,280
as we as we go through

01:16:20,480 --> 01:16:23,679
so uh next uh the next speaking will be

01:16:23,280 --> 01:16:26,800
01:16:23,679 --> 01:16:27,520
35 and hopefully you'll be able to get

01:16:26,800 --> 01:16:29,440
on with

01:16:27,520 --> 01:16:34,159
the exercise in the meantime i'll see

01:16:29,440 --> 01:16:36,159
you then

01:16:34,159 --> 01:16:37,679
hopefully you've all got on okay with

01:16:36,159 --> 01:16:41,199
that exercise

01:16:37,679 --> 01:16:44,320
um we're now coming into the the last um

01:16:41,199 --> 01:16:44,719
kind of lecture of this morning um in

01:16:44,320 --> 01:16:47,520
this

01:16:44,719 --> 01:16:48,080
in this kind of short session um i'm

01:16:47,520 --> 01:16:50,800
going to

01:16:48,080 --> 01:16:51,199
talk a lot about how you might think

01:16:50,800 --> 01:16:54,719
about

01:16:51,199 --> 01:16:57,520
optimizing your openmp codes on cpus

01:16:54,719 --> 01:16:57,760
so we've now got parallel codes and that

01:16:57,520 --> 01:17:00,480
are

01:16:57,760 --> 01:17:03,280
running in parallel using all the cores

01:17:00,480 --> 01:17:05,280
of of a multi-core cpu

01:17:03,280 --> 01:17:07,440
and then there are a few things and much

01:17:05,280 --> 01:17:11,280
of these came in with openmp

01:17:07,440 --> 01:17:14,159
uh 4 and 4.5 and extended beyond that

01:17:11,280 --> 01:17:16,239
to allow us to write better programs

01:17:14,159 --> 01:17:17,520
faster and more efficient programs in

01:17:16,239 --> 01:17:20,880
openmp

01:17:17,520 --> 01:17:22,080
so that's the topic of this of this

01:17:20,880 --> 01:17:24,640
part and then that's going to be an

01:17:22,080 --> 01:17:26,239
exercise uh just before lunch

01:17:24,640 --> 01:17:28,640
where you'll be able to apply some of

01:17:26,239 --> 01:17:30,640
these techniques to enable you to

01:17:28,640 --> 01:17:32,480
optimize the stencil code that you have

01:17:30,640 --> 01:17:34,719
and you should be able to make some

01:17:32,480 --> 01:17:36,080
some good performance improvements with

01:17:34,719 --> 01:17:38,480
with the code even though we've seen it

01:17:36,080 --> 01:17:40,159
go a lot faster by running in parallel

01:17:38,480 --> 01:17:44,320
there's still a lot of performance to be

01:17:40,159 --> 01:17:46,159
gained by by some optimizations

01:17:44,320 --> 01:17:48,400
so to start with let's go back and have

01:17:46,159 --> 01:17:50,800
a look at the previous exercise

01:17:48,400 --> 01:17:52,159
so we already had the parallel do

01:17:50,800 --> 01:17:54,560
collapse in there to

01:17:52,159 --> 01:17:56,320
run the um the stencil operation in

01:17:54,560 --> 01:17:58,159
parallel

01:17:56,320 --> 01:18:00,320
and i asked you uh at the beginning of

01:17:58,159 --> 01:18:02,640
the exercise to add this reduction

01:18:00,320 --> 01:18:04,880
over the loop um so this is going to

01:18:02,640 --> 01:18:06,480
compute total here which i set to zero

01:18:04,880 --> 01:18:09,920
before the

01:18:06,480 --> 01:18:13,120
before the parallel do and then i um

01:18:09,920 --> 01:18:15,360
then you add it up add the temp a temp

01:18:13,120 --> 01:18:18,080
value in every cell to total

01:18:15,360 --> 01:18:20,400
um and then that's then computed every

01:18:18,080 --> 01:18:22,159
iteration you might compute that out

01:18:20,400 --> 01:18:24,239
now this is a little bit contrived but

01:18:22,159 --> 01:18:27,520
you can imagine if you were writing say

01:18:24,239 --> 01:18:29,440
a linear solver that that was um using

01:18:27,520 --> 01:18:30,400
some sort of stencil based update you

01:18:29,440 --> 01:18:33,120
would also need to

01:18:30,400 --> 01:18:33,520
to compute say an error or a residual

01:18:33,120 --> 01:18:35,840
term

01:18:33,520 --> 01:18:38,000
using some reduction which you'd then

01:18:35,840 --> 01:18:40,560
need to compare in serial

01:18:38,000 --> 01:18:42,640
maybe to determine if you are conversion

01:18:40,560 --> 01:18:44,640
or not yet that sort of thing so

01:18:42,640 --> 01:18:46,320
um the whole point of this five-point

01:18:44,640 --> 01:18:47,760
stencil is it's it's deliberately as

01:18:46,320 --> 01:18:50,640
simple as possible

01:18:47,760 --> 01:18:52,719
um but the idea is it captures a lot of

01:18:50,640 --> 01:18:54,239
the kind of the heart the essence of

01:18:52,719 --> 01:18:55,920
more complicated codes that you're going

01:18:54,239 --> 01:18:59,120
to be having to deal with as you as you

01:18:55,920 --> 01:19:00,880
do more interesting and real science

01:18:59,120 --> 01:19:02,239
so that was that that was the goal we

01:19:00,880 --> 01:19:03,440
have a five point stencil with the

01:19:02,239 --> 01:19:06,480
parallel do

01:19:03,440 --> 01:19:09,840
we add the reduction clause so reduction

01:19:06,480 --> 01:19:12,320
uh plus over the total variable

01:19:09,840 --> 01:19:13,840
and then we're telling openmp in order

01:19:12,320 --> 01:19:15,440
uh that everything it needs to know in

01:19:13,840 --> 01:19:17,840
order to reduce

01:19:15,440 --> 01:19:20,800
and compute the the sum across all

01:19:17,840 --> 01:19:20,800
updates to total

01:19:21,679 --> 01:19:26,239
so now on to vectorization um and and

01:19:24,640 --> 01:19:27,760
this is kind of a little cartoon that

01:19:26,239 --> 01:19:31,360
that shows what we mean by that

01:19:27,760 --> 01:19:33,679
so let's say we have uh two arrays

01:19:31,360 --> 01:19:35,199
a and b and we want to come to compute

01:19:33,679 --> 01:19:37,840
the sum of them and store it in

01:19:35,199 --> 01:19:37,840
array c

01:19:38,239 --> 01:19:42,960
now to do that we could use some sort of

01:19:41,120 --> 01:19:44,960
scalar operation so we'd have a loop

01:19:42,960 --> 01:19:47,600
over all the elements of a and b

01:19:44,960 --> 01:19:48,400
and i would take an element of a and an

01:19:47,600 --> 01:19:50,719
element of b

01:19:48,400 --> 01:19:52,320
add them together and store them in in c

01:19:50,719 --> 01:19:54,320
and i'd have lots of these scalar

01:19:52,320 --> 01:19:56,239
operations replicated many many times

01:19:54,320 --> 01:19:59,440
one for every

01:19:56,239 --> 01:19:59,440
entry in the array

01:19:59,679 --> 01:20:06,639
now many core processors often have

01:20:03,920 --> 01:20:08,639
vector execution as well and can can

01:20:06,639 --> 01:20:10,800
perform vector operations

01:20:08,639 --> 01:20:13,280
and this is where instead of just adding

01:20:10,800 --> 01:20:14,560
um you know say two individual numbers

01:20:13,280 --> 01:20:17,280
together to compute

01:20:14,560 --> 01:20:19,360
a third this will add blocks of numbers

01:20:17,280 --> 01:20:22,000
together to compute another block

01:20:19,360 --> 01:20:23,440
so in this example then we we might have

01:20:22,000 --> 01:20:25,600
a vector length of four

01:20:23,440 --> 01:20:27,199
which would take four doubles and it

01:20:25,600 --> 01:20:28,880
would add them together

01:20:27,199 --> 01:20:30,719
two blocks of four doubles it would add

01:20:28,880 --> 01:20:34,400
them together element-wise

01:20:30,719 --> 01:20:36,320
and to compute a final block of of four

01:20:34,400 --> 01:20:38,480
now the benefit of this is is extra

01:20:36,320 --> 01:20:40,320
parallelism with a single instruction

01:20:38,480 --> 01:20:41,440
we're able to compute that same

01:20:40,320 --> 01:20:44,080
instruction

01:20:41,440 --> 01:20:44,480
across multiple pieces of data this is

01:20:44,080 --> 01:20:45,920
where

01:20:44,480 --> 01:20:48,000
the term simdi comes from same

01:20:45,920 --> 01:20:49,280
instruction multiple data

01:20:48,000 --> 01:20:51,840
and it's just this we have a single

01:20:49,280 --> 01:20:52,639
instruction it operates on on a blocks

01:20:51,840 --> 01:20:55,760
of data these

01:20:52,639 --> 01:20:57,520
these vectors of a particular length and

01:20:55,760 --> 01:21:01,280
will enable us to then

01:20:57,520 --> 01:21:03,520
do have our processor compute things in

01:21:01,280 --> 01:21:06,880
parallel at a vector level as well as

01:21:03,520 --> 01:21:06,880
at a threaded level as well

01:21:07,360 --> 01:21:10,719
so they give you this more compute per

01:21:09,520 --> 01:21:12,960
cycle primarily

01:21:10,719 --> 01:21:14,000
it basically increases the the floating

01:21:12,960 --> 01:21:16,880
point operations per

01:21:14,000 --> 01:21:17,679
second that a processor can do because

01:21:16,880 --> 01:21:20,239
it can do

01:21:17,679 --> 01:21:21,760
more than one of these computations in

01:21:20,239 --> 01:21:24,239
parallel

01:21:21,760 --> 01:21:27,520
um it's able to do more and more of them

01:21:24,239 --> 01:21:30,560
in a given period of time

01:21:27,520 --> 01:21:32,080
because we're also doing uh more with

01:21:30,560 --> 01:21:33,920
more with a single instruction there's

01:21:32,080 --> 01:21:34,480
fewer instructions that that processor

01:21:33,920 --> 01:21:36,639
has to

01:21:34,480 --> 01:21:37,840
process so this means that at an

01:21:36,639 --> 01:21:39,600
architectural level

01:21:37,840 --> 01:21:41,199
as the processor is decoding and

01:21:39,600 --> 01:21:43,120
understanding the instruction stream

01:21:41,199 --> 01:21:44,560
there's less pressure on that and

01:21:43,120 --> 01:21:48,080
essentially there's fewer instructions

01:21:44,560 --> 01:21:48,080
that it has to process at all

01:21:49,520 --> 01:21:53,280
most codes are that we find are actually

01:21:52,239 --> 01:21:55,600
principally

01:21:53,280 --> 01:21:56,719
main memory bandwidth bounds so having

01:21:55,600 --> 01:21:59,120
extra flops

01:21:56,719 --> 01:22:01,840
doesn't really help our code go faster

01:21:59,120 --> 01:22:04,880
but we find vectorization still helps

01:22:01,840 --> 01:22:08,080
and in particular this comes because

01:22:04,880 --> 01:22:10,080
using vectors helps make good use of the

01:22:08,080 --> 01:22:12,639
memory hierarchy and this is often often

01:22:10,080 --> 01:22:14,480
the main benefit we see rather than just

01:22:12,639 --> 01:22:16,320
reading memory one piece at a time we're

01:22:14,480 --> 01:22:18,639
reading it a block at a time

01:22:16,320 --> 01:22:20,000
this is a much more structured way of

01:22:18,639 --> 01:22:22,480
accessing memory

01:22:20,000 --> 01:22:24,239
so we can move memory in bigger blocks

01:22:22,480 --> 01:22:25,760
all the way through the cache hierarchy

01:22:24,239 --> 01:22:29,360
and all the way into the actual

01:22:25,760 --> 01:22:31,040
execution units on our processor as well

01:22:29,360 --> 01:22:32,880
but thinking about vectorization you

01:22:31,040 --> 01:22:34,800
ultimately end up writing code which has

01:22:32,880 --> 01:22:36,560
good memory access patterns

01:22:34,800 --> 01:22:38,080
and they often help you maximize the

01:22:36,560 --> 01:22:40,159
memory bandwidth

01:22:38,080 --> 01:22:41,840
that your code is going to achieve so

01:22:40,159 --> 01:22:44,560
although we're not benefiting from

01:22:41,840 --> 01:22:46,480
the flops in writing in a vectorized way

01:22:44,560 --> 01:22:48,719
we're making good use of the memory

01:22:46,480 --> 01:22:50,639
um hierarchy to improve the performance

01:22:48,719 --> 01:22:51,760
of that code and this is why we often

01:22:50,639 --> 01:22:53,600
see

01:22:51,760 --> 01:22:55,360
that vectorization helps for the memory

01:22:53,600 --> 01:22:57,360
bandwidth bound codes

01:22:55,360 --> 01:23:00,400
even though the flops we don't get any

01:22:57,360 --> 01:23:00,400
benefit from the flops

01:23:02,000 --> 01:23:05,120
so compilers are very good and modern

01:23:03,840 --> 01:23:06,880
compilers in particular

01:23:05,120 --> 01:23:08,320
are are great at automatically

01:23:06,880 --> 01:23:10,880
vectorizing your loops

01:23:08,320 --> 01:23:12,880
they do a lot of analysis to figure out

01:23:10,880 --> 01:23:15,120
if it's safe to vectorize

01:23:12,880 --> 01:23:16,159
and and fortran is is particularly good

01:23:15,120 --> 01:23:18,159
at this because it

01:23:16,159 --> 01:23:19,199
it helps because the language defines

01:23:18,159 --> 01:23:22,960
that those arrays

01:23:19,199 --> 01:23:24,080
can't can't overlap like they could in c

01:23:22,960 --> 01:23:26,239
but there's a lot of analysis the

01:23:24,080 --> 01:23:28,480
compiler still needs to do it needs to

01:23:26,239 --> 01:23:29,040
look at the loop body and make sure that

01:23:28,480 --> 01:23:31,199
you're

01:23:29,040 --> 01:23:33,360
not going to be doing anything that it

01:23:31,199 --> 01:23:34,960
can't vectorize you're not going to be

01:23:33,360 --> 01:23:37,120
updating it in a way where where

01:23:34,960 --> 01:23:40,320
elements can't be essentially

01:23:37,120 --> 01:23:43,679
um updated in parallel so if they can

01:23:40,320 --> 01:23:45,760
if the compiler can help understand um

01:23:43,679 --> 01:23:46,960
can help know that that and figure out

01:23:45,760 --> 01:23:48,960
that a loop is

01:23:46,960 --> 01:23:51,760
is vectorizable it's likely to be a

01:23:48,960 --> 01:23:53,440
parallel loop then as well

01:23:51,760 --> 01:23:57,120
so that's the sort of analysis that

01:23:53,440 --> 01:23:58,639
you're sort of asking the compiler to do

01:23:57,120 --> 01:24:00,239
it's really important that you read the

01:23:58,639 --> 01:24:02,239
compiler reports to see

01:24:00,239 --> 01:24:03,520
if a compiler is vectorizing loops or

01:24:02,239 --> 01:24:06,320
not and there's some

01:24:03,520 --> 01:24:08,159
um flags on here which are useful for

01:24:06,320 --> 01:24:08,800
querying whether the compiler is going

01:24:08,159 --> 01:24:10,639
to be

01:24:08,800 --> 01:24:12,400
analyzed um is correctly analyze the

01:24:10,639 --> 01:24:13,120
loops and manage to generate vectorized

01:24:12,400 --> 01:24:16,400
code

01:24:13,120 --> 01:24:17,920
from them we often find that it's the

01:24:16,400 --> 01:24:18,960
memory access pattern which is the

01:24:17,920 --> 01:24:20,320
primary

01:24:18,960 --> 01:24:22,800
reason why you might get either

01:24:20,320 --> 01:24:24,400
inefficient auto vectorized code or

01:24:22,800 --> 01:24:26,400
code that doesn't vectorize at all so

01:24:24,400 --> 01:24:28,159
that's an important part to to look at

01:24:26,400 --> 01:24:29,120
when you're considering whether the code

01:24:28,159 --> 01:24:31,520
has

01:24:29,120 --> 01:24:32,400
has vectorized much of this will tell

01:24:31,520 --> 01:24:34,320
you if the

01:24:32,400 --> 01:24:35,520
if the compiler has thought there is a

01:24:34,320 --> 01:24:37,440
dependency between

01:24:35,520 --> 01:24:38,560
um iterations of the loop and couldn't

01:24:37,440 --> 01:24:40,239
update it

01:24:38,560 --> 01:24:42,080
and couldn't vectorize it as a result of

01:24:40,239 --> 01:24:43,760
those dependencies so understanding

01:24:42,080 --> 01:24:45,520
where those dependencies are

01:24:43,760 --> 01:24:47,120
either trying to either trying to change

01:24:45,520 --> 01:24:48,960
them in your code or using some of the

01:24:47,120 --> 01:24:51,040
tools in openmp

01:24:48,960 --> 01:24:53,679
to help the compiler vectorize is is the

01:24:51,040 --> 01:24:53,679
way forward

01:24:54,080 --> 01:24:58,320
so sometimes you need to tell the

01:24:55,920 --> 01:24:59,840
compiler that those loops are indeed

01:24:58,320 --> 01:25:01,760
vectorizable because

01:24:59,840 --> 01:25:03,760
you know better you know it's safe to

01:25:01,760 --> 01:25:07,600
vectorize those loops

01:25:03,760 --> 01:25:09,360
now openmp in openmp4 gives you simd

01:25:07,600 --> 01:25:10,880
constructs which which convey this

01:25:09,360 --> 01:25:13,120
information

01:25:10,880 --> 01:25:14,800
they tell the compiler that that loop

01:25:13,120 --> 01:25:17,360
they have to go with a loop

01:25:14,800 --> 01:25:17,840
um and it should uh the commander should

01:25:17,360 --> 01:25:20,880
then

01:25:17,840 --> 01:25:23,280
um produce vector instructions um that

01:25:20,880 --> 01:25:24,880
would execute that loop so you can

01:25:23,280 --> 01:25:26,880
combine them with with a parallel do

01:25:24,880 --> 01:25:28,080
construct so you get a parallel vector

01:25:26,880 --> 01:25:31,360
loop so this would be

01:25:28,080 --> 01:25:34,320
um omp parallel do cimd

01:25:31,360 --> 01:25:36,719
in general on a cpu we want to vectorize

01:25:34,320 --> 01:25:38,960
inner loops and parallelize outer loops

01:25:36,719 --> 01:25:41,360
although there is times when we might

01:25:38,960 --> 01:25:42,560
want to have a flat iteration space and

01:25:41,360 --> 01:25:44,880
vectorize and parallel

01:25:42,560 --> 01:25:46,560
parallelize over the same um dimension

01:25:44,880 --> 01:25:48,080
and obviously we can use the collapse

01:25:46,560 --> 01:25:50,080
clause and it will do

01:25:48,080 --> 01:25:51,679
um it would do sort of as we expect as

01:25:50,080 --> 01:25:53,360
well it would collapse the loops try and

01:25:51,679 --> 01:25:54,800
vectorize it and parallelize

01:25:53,360 --> 01:25:56,400
they'll try and parallelize it and then

01:25:54,800 --> 01:25:57,920
vectorize over the bits that is that

01:25:56,400 --> 01:26:00,800
it's broken up

01:25:57,920 --> 01:26:00,800
uh in each thread

01:26:01,760 --> 01:26:05,440
so that's fine if your loop body is

01:26:03,440 --> 01:26:06,400
something quite simple and just contains

01:26:05,440 --> 01:26:09,760
simple

01:26:06,400 --> 01:26:11,040
operations on scalar variables we've

01:26:09,760 --> 01:26:13,920
taken a scalar loop

01:26:11,040 --> 01:26:16,080
the scalar loop body and want to

01:26:13,920 --> 01:26:18,320
vectorize it into these vectors

01:26:16,080 --> 01:26:20,320
but sometimes we might have functions

01:26:18,320 --> 01:26:20,960
that operate on those scalars inside the

01:26:20,320 --> 01:26:22,800
loop

01:26:20,960 --> 01:26:24,239
so in this we're just going to have this

01:26:22,800 --> 01:26:25,920
function which i've just called magic

01:26:24,239 --> 01:26:26,480
math so it's just going to take a scalar

01:26:25,920 --> 01:26:28,800
number

01:26:26,480 --> 01:26:30,560
and up and produce a new number that we

01:26:28,800 --> 01:26:32,400
can update it with

01:26:30,560 --> 01:26:34,080
now this is often quite tricky for a

01:26:32,400 --> 01:26:35,920
compiler to vectorize

01:26:34,080 --> 01:26:38,000
if the function is very small it doesn't

01:26:35,920 --> 01:26:40,560
contain much

01:26:38,000 --> 01:26:42,239
much code and it's visible to the

01:26:40,560 --> 01:26:43,840
compiler it might just inline it

01:26:42,239 --> 01:26:46,000
and then it might be able to auto

01:26:43,840 --> 01:26:49,040
vectorize it but sometimes

01:26:46,000 --> 01:26:51,280
we might need to um use a cindy

01:26:49,040 --> 01:26:53,840
construct to tell the compiler to create

01:26:51,280 --> 01:26:55,120
a vectorized version of that function if

01:26:53,840 --> 01:26:56,719
that is indeed possible

01:26:55,120 --> 01:26:58,480
and this is how we would do that at the

01:26:56,719 --> 01:27:02,239
bottom of this slide i have this

01:26:58,480 --> 01:27:05,040
function magic maths and it just takes

01:27:02,239 --> 01:27:06,400
the value and it computes the square so

01:27:05,040 --> 01:27:08,239
this is something that's likely to get

01:27:06,400 --> 01:27:12,000
inlined but you can imagine this is a

01:27:08,239 --> 01:27:12,000
scalar function that that might not

01:27:12,080 --> 01:27:16,800
so now i have this omp declare simdi

01:27:15,280 --> 01:27:18,400
and then the name of the function

01:27:16,800 --> 01:27:21,040
underneath the function

01:27:18,400 --> 01:27:23,280
name this just tells the compiler that

01:27:21,040 --> 01:27:25,199
it should generate a cindy version of

01:27:23,280 --> 01:27:26,560
this function so that when it tries to

01:27:25,199 --> 01:27:28,800
vectorize the do loop

01:27:26,560 --> 01:27:30,639
it already knows that this function call

01:27:28,800 --> 01:27:32,320
there's a vectorized equivalent where it

01:27:30,639 --> 01:27:34,000
can send a vector

01:27:32,320 --> 01:27:35,760
into the function call and the function

01:27:34,000 --> 01:27:38,400
call will compute

01:27:35,760 --> 01:27:39,600
the square of each element-wise entry in

01:27:38,400 --> 01:27:41,280
that vector

01:27:39,600 --> 01:27:42,960
this is very useful for if you need to

01:27:41,280 --> 01:27:44,719
help the compiler

01:27:42,960 --> 01:27:46,480
stitch things together so it knows how

01:27:44,719 --> 01:27:48,480
to how to

01:27:46,480 --> 01:27:50,719
vectorize things like function calls and

01:27:48,480 --> 01:27:50,719
things

01:27:53,440 --> 01:27:57,360
with cindy we have all the data sharing

01:27:55,600 --> 01:27:57,920
and the reduction clauses so we can have

01:27:57,360 --> 01:27:59,920
a

01:27:57,920 --> 01:28:01,440
a cindy production clause and they apply

01:27:59,920 --> 01:28:04,480
to the the cindy

01:28:01,440 --> 01:28:06,000
um the simdi uh

01:28:04,480 --> 01:28:07,600
private within each vector lane

01:28:06,000 --> 01:28:10,800
essentially if we need

01:28:07,600 --> 01:28:12,320
if we need that there's also an idea

01:28:10,800 --> 01:28:14,400
about

01:28:12,320 --> 01:28:16,000
about safety we have to sometimes help

01:28:14,400 --> 01:28:19,440
the compiler deal with

01:28:16,000 --> 01:28:21,440
um with how wide a vector is safe for

01:28:19,440 --> 01:28:23,199
our particular algorithm now ideally

01:28:21,440 --> 01:28:23,920
it's all completely parallel and we can

01:28:23,199 --> 01:28:25,520
just

01:28:23,920 --> 01:28:27,040
freely vectorize to whatever vector

01:28:25,520 --> 01:28:29,360
length we need but sometimes our

01:28:27,040 --> 01:28:30,800
algorithm might not allow us

01:28:29,360 --> 01:28:33,040
and there's an example of that on this

01:28:30,800 --> 01:28:36,159
slide where we might want to add

01:28:33,040 --> 01:28:38,080
elements of the array space by four now

01:28:36,159 --> 01:28:39,040
obviously we can't do this if the vector

01:28:38,080 --> 01:28:41,600
length is bigger than

01:28:39,040 --> 01:28:42,159
four so openmp gives us a way to explain

01:28:41,600 --> 01:28:44,080
that

01:28:42,159 --> 01:28:45,679
unfortunately there is a limitation on

01:28:44,080 --> 01:28:48,159
how much you can vectorize

01:28:45,679 --> 01:28:49,679
and the safe distance the number of

01:28:48,159 --> 01:28:50,880
elements in that loop that you can

01:28:49,679 --> 01:28:53,199
vectorize

01:28:50,880 --> 01:28:54,320
um is four so we'd have the safe lend

01:28:53,199 --> 01:28:56,400
clause

01:28:54,320 --> 01:28:58,159
with the with the distance between

01:28:56,400 --> 01:29:02,000
iterations where it's safe

01:28:58,159 --> 01:29:02,639
to vectorize so this also doesn't then

01:29:02,000 --> 01:29:04,159
specify

01:29:02,639 --> 01:29:06,080
what that vector length is which is

01:29:04,159 --> 01:29:07,520
which is good it's just giving openmp

01:29:06,080 --> 01:29:09,360
extra information about

01:29:07,520 --> 01:29:12,400
our algorithm about about the

01:29:09,360 --> 01:29:15,120
correctness of our algorithm

01:29:12,400 --> 01:29:16,800
we can also give it a simd length um to

01:29:15,120 --> 01:29:17,360
actually specify what the length of that

01:29:16,800 --> 01:29:19,280
should be

01:29:17,360 --> 01:29:20,960
but as with specifying the number of

01:29:19,280 --> 01:29:22,560
threads in a clause

01:29:20,960 --> 01:29:24,320
which we don't recommend we also really

01:29:22,560 --> 01:29:25,120
don't recommend using the cindy len

01:29:24,320 --> 01:29:28,159
claus

01:29:25,120 --> 01:29:30,400
for similar reasons but sometimes

01:29:28,159 --> 01:29:32,000
being able to express this safety is

01:29:30,400 --> 01:29:33,440
important especially with say a legacy

01:29:32,000 --> 01:29:36,000
application where

01:29:33,440 --> 01:29:37,280
um where adding in vectorization of

01:29:36,000 --> 01:29:38,080
things needs to be done in a more

01:29:37,280 --> 01:29:41,760
piecemeal

01:29:38,080 --> 01:29:41,760
um piece of a way for instance

01:29:43,199 --> 01:29:47,120
as well as um as well as uh expressing

01:29:46,239 --> 01:29:48,960
things about

01:29:47,120 --> 01:29:50,400
the safety there's some more clauses and

01:29:48,960 --> 01:29:53,600
these explain

01:29:50,400 --> 01:29:56,719
about the relationship between

01:29:53,600 --> 01:29:58,639
the loop iterator variable and the

01:29:56,719 --> 01:30:00,719
vectors

01:29:58,639 --> 01:30:01,679
in particular it's useful for function

01:30:00,719 --> 01:30:04,000
causes as

01:30:01,679 --> 01:30:06,080
well for similar reasons where the

01:30:04,000 --> 01:30:09,600
function arguments you can specify

01:30:06,080 --> 01:30:11,840
properties specifically

01:30:09,600 --> 01:30:13,520
they allow you to specify the

01:30:11,840 --> 01:30:16,960
relationship of a variable

01:30:13,520 --> 01:30:17,600
um with the with its position in the

01:30:16,960 --> 01:30:19,679
vector

01:30:17,600 --> 01:30:21,360
so let's take linear the linear clause

01:30:19,679 --> 01:30:24,320
as an example so here

01:30:21,360 --> 01:30:26,560
as an example i have a loop over i and

01:30:24,320 --> 01:30:30,080
it's just going to copy

01:30:26,560 --> 01:30:32,159
um from b to a

01:30:30,080 --> 01:30:34,080
now it's just a simple copy loop and i

01:30:32,159 --> 01:30:37,600
could have used of course i

01:30:34,080 --> 01:30:38,960
in both um instances but but i didn't i

01:30:37,600 --> 01:30:41,120
for some reason i needed to have a

01:30:38,960 --> 01:30:42,880
separate index into my a array

01:30:41,120 --> 01:30:45,679
but both of them are really just having

01:30:42,880 --> 01:30:46,560
i and j gonna equal the same thing as we

01:30:45,679 --> 01:30:47,760
go through

01:30:46,560 --> 01:30:51,040
and at the end of the loop they're both

01:30:47,760 --> 01:30:53,600
going to be updated to by plus one

01:30:51,040 --> 01:30:55,840
um i would have initialized i and j

01:30:53,600 --> 01:30:57,840
correctly to start with

01:30:55,840 --> 01:30:59,440
now the compiler might not be able to

01:30:57,840 --> 01:31:02,000
figure out what the relationship

01:30:59,440 --> 01:31:03,120
is of j as it goes from each iteration

01:31:02,000 --> 01:31:05,600
of the loop

01:31:03,120 --> 01:31:07,520
so as j goes from you know goes through

01:31:05,600 --> 01:31:09,440
say for i equals one to i equals two how

01:31:07,520 --> 01:31:12,159
does j get updated and how does that

01:31:09,440 --> 01:31:13,920
affect the um the loop iterator well

01:31:12,159 --> 01:31:16,960
this linear clause is there to help

01:31:13,920 --> 01:31:19,679
help tell opening b how j relates to

01:31:16,960 --> 01:31:20,480
i essentially how j is updated every

01:31:19,679 --> 01:31:22,639
time the

01:31:20,480 --> 01:31:24,239
the loop counter changes and here we're

01:31:22,639 --> 01:31:24,719
saying it's a linear relationship it

01:31:24,239 --> 01:31:27,280
just

01:31:24,719 --> 01:31:30,159
is scaled by the same amount that the i

01:31:27,280 --> 01:31:33,360
is scaled by

01:31:30,159 --> 01:31:36,000
there's also the um the uniform

01:31:33,360 --> 01:31:37,040
clause and this says the other it's sort

01:31:36,000 --> 01:31:38,960
of the opposite that

01:31:37,040 --> 01:31:41,280
that that particular variable is

01:31:38,960 --> 01:31:42,560
constant doesn't change it's the same in

01:31:41,280 --> 01:31:44,239
every vector lane

01:31:42,560 --> 01:31:46,159
and this is this can be very useful if

01:31:44,239 --> 01:31:48,000
you're passing variables into a function

01:31:46,159 --> 01:31:50,719
and you're using the declare cindy

01:31:48,000 --> 01:31:53,840
construct you can say that that variable

01:31:50,719 --> 01:31:56,000
is fixed for all parts of that um

01:31:53,840 --> 01:31:58,080
all entries in that vector lane of that

01:31:56,000 --> 01:31:59,520
of that function call

01:31:58,080 --> 01:32:01,440
likewise if you have managed to get your

01:31:59,520 --> 01:32:03,280
memory to be aligned which is

01:32:01,440 --> 01:32:05,120
which can be slightly tricky in fortran

01:32:03,280 --> 01:32:06,880
you can then remind the compiler that

01:32:05,120 --> 01:32:07,920
the memory is has actually been aligned

01:32:06,880 --> 01:32:10,080
by how much and

01:32:07,920 --> 01:32:11,280
the align clause can be there and this

01:32:10,080 --> 01:32:13,199
is useful for

01:32:11,280 --> 01:32:15,920
for kind of propagating that alignment

01:32:13,199 --> 01:32:18,080
information in a standard way from

01:32:15,920 --> 01:32:20,000
um from where the memory was allocated

01:32:18,080 --> 01:32:21,920
to where it's actually being used and

01:32:20,000 --> 01:32:24,719
hopefully this will enable the compiler

01:32:21,920 --> 01:32:26,960
to generate better instructions it knows

01:32:24,719 --> 01:32:28,480
where that the start of the array is is

01:32:26,960 --> 01:32:29,760
starting from

01:32:28,480 --> 01:32:31,920
there's all these different clauses you

01:32:29,760 --> 01:32:35,040
can have have to help um

01:32:31,920 --> 01:32:37,760
explain extra information to the uh

01:32:35,040 --> 01:32:39,600
to the openmp compiler about the the

01:32:37,760 --> 01:32:41,840
properties of the cmd loop the vector

01:32:39,600 --> 01:32:41,840
loops

01:32:42,480 --> 01:32:46,800
so really cindy and the constructors

01:32:44,480 --> 01:32:47,199
here in openmp to give us a standard way

01:32:46,800 --> 01:32:49,920
to

01:32:47,199 --> 01:32:52,960
to help the compiler auto vectorize

01:32:49,920 --> 01:32:54,960
loops in a correct way

01:32:52,960 --> 01:32:56,719
as with with parallel you're telling the

01:32:54,960 --> 01:32:58,639
compiler that it's safe to vectorizing

01:32:56,719 --> 01:32:59,280
and it can often help ignore any of the

01:32:58,639 --> 01:33:00,880
debate

01:32:59,280 --> 01:33:03,440
data dependency analysis that that

01:33:00,880 --> 01:33:05,440
compiler might do now some compilers are

01:33:03,440 --> 01:33:07,360
a bit more conservative and may still

01:33:05,440 --> 01:33:10,239
check the data dependency but

01:33:07,360 --> 01:33:11,360
most of them are um will will believe

01:33:10,239 --> 01:33:13,520
you when you say that

01:33:11,360 --> 01:33:16,159
it is safe to vectorize as you use the

01:33:13,520 --> 01:33:16,159
cindy claus

01:33:16,639 --> 01:33:20,320
so make sure you read the compiler

01:33:18,159 --> 01:33:22,480
reports it's very useful to check

01:33:20,320 --> 01:33:24,000
that what it did before and after you've

01:33:22,480 --> 01:33:28,159
you've added the cmd clauses in

01:33:24,000 --> 01:33:30,320
as well um again declare cindy is very

01:33:28,159 --> 01:33:32,400
useful if you have function calls inside

01:33:30,320 --> 01:33:34,400
vectorized loops just to help the

01:33:32,400 --> 01:33:38,480
compiler hook everything up together so

01:33:34,400 --> 01:33:38,480
it does what you would expect it to do

01:33:39,280 --> 01:33:42,800
now derived types derived types we often

01:33:41,760 --> 01:33:45,920
find in

01:33:42,800 --> 01:33:46,320
in um in all sorts of codes in c plus

01:33:45,920 --> 01:33:48,480
and in

01:33:46,320 --> 01:33:50,560
and in fortran and in c and all of those

01:33:48,480 --> 01:33:52,719
languages are supported by openmp but

01:33:50,560 --> 01:33:53,920
this is a fortran example where i might

01:33:52,719 --> 01:33:56,639
define a cell

01:33:53,920 --> 01:33:58,719
type and in there i've got four double

01:33:56,639 --> 01:34:01,760
precision properties

01:33:58,719 --> 01:34:03,280
now an example uh in a more physical

01:34:01,760 --> 01:34:05,760
example might be where i

01:34:03,280 --> 01:34:07,679
i have a particular mesh of cells and

01:34:05,760 --> 01:34:10,480
inside each cell i might store the

01:34:07,679 --> 01:34:12,320
the velocity or the temperature or the

01:34:10,480 --> 01:34:13,679
the energy associated there's all sorts

01:34:12,320 --> 01:34:16,880
of different things i might store

01:34:13,679 --> 01:34:17,920
um based on that cell so i have these

01:34:16,880 --> 01:34:20,159
single uh

01:34:17,920 --> 01:34:21,440
reels these double precision numbers in

01:34:20,159 --> 01:34:23,840
each cell

01:34:21,440 --> 01:34:24,560
i allocate some 2d grid of those cells

01:34:23,840 --> 01:34:27,360
and then i

01:34:24,560 --> 01:34:28,400
have a loop over my grid this j and i

01:34:27,360 --> 01:34:30,560
loop

01:34:28,400 --> 01:34:33,840
and that's going to then update each of

01:34:30,560 --> 01:34:33,840
those properties in turn

01:34:34,800 --> 01:34:38,159
so what does this look like in memory

01:34:36,480 --> 01:34:39,840
well it's organized as what's known as

01:34:38,159 --> 01:34:41,679
an array of structures

01:34:39,840 --> 01:34:44,080
so i have my four properties for the

01:34:41,679 --> 01:34:45,840
first cell p one two three and four

01:34:44,080 --> 01:34:47,840
and then i move on to the next cell in

01:34:45,840 --> 01:34:48,480
the array and i have all four properties

01:34:47,840 --> 01:34:50,560
laid out

01:34:48,480 --> 01:34:52,800
after that and this continues throughout

01:34:50,560 --> 01:34:54,639
the whole uh 2d array

01:34:52,800 --> 01:34:56,960
so this two-dimensional array gets

01:34:54,639 --> 01:34:58,800
flattened into a one-dimensional array

01:34:56,960 --> 01:35:00,560
and the size of that each of the

01:34:58,800 --> 01:35:02,719
elements of the array is a cell

01:35:00,560 --> 01:35:03,840
and we just lay out all the parts of

01:35:02,719 --> 01:35:07,679
that derived type

01:35:03,840 --> 01:35:08,880
in memory so what happens when i want to

01:35:07,679 --> 01:35:11,520
vectorize

01:35:08,880 --> 01:35:12,239
the loop over cells maybe that in a loop

01:35:11,520 --> 01:35:15,280
we want to

01:35:12,239 --> 01:35:17,520
want to vectorize that but this is what

01:35:15,280 --> 01:35:19,119
happens when i want to update p1 the

01:35:17,520 --> 01:35:22,080
first property

01:35:19,119 --> 01:35:23,920
it's going to have to uh access these

01:35:22,080 --> 01:35:26,000
bits of the array with say we have a

01:35:23,920 --> 01:35:28,639
vector length of four

01:35:26,000 --> 01:35:29,360
it's going to have to to do that vector

01:35:28,639 --> 01:35:31,920
pick out the

01:35:29,360 --> 01:35:33,199
the p1 of the first four elements of our

01:35:31,920 --> 01:35:33,920
array and we know where they are in

01:35:33,199 --> 01:35:36,159
memory

01:35:33,920 --> 01:35:39,199
and we can see here so these big strides

01:35:36,159 --> 01:35:40,960
between each of those elements

01:35:39,199 --> 01:35:43,280
so this is going to gather those

01:35:40,960 --> 01:35:45,760
elements into a vector register

01:35:43,280 --> 01:35:47,360
it's then going to do the computation

01:35:45,760 --> 01:35:49,119
and then once it's

01:35:47,360 --> 01:35:51,600
been done it's going to scatter them

01:35:49,119 --> 01:35:52,960
from the vector register back in memory

01:35:51,600 --> 01:35:55,199
remember the picture i showed you right

01:35:52,960 --> 01:35:56,239
at the start of these this little set of

01:35:55,199 --> 01:35:57,920
slides and

01:35:56,239 --> 01:35:59,280
there were blocks of four contiguous

01:35:57,920 --> 01:36:01,520
bits of memory that i had to

01:35:59,280 --> 01:36:02,719
load into my vector register and that's

01:36:01,520 --> 01:36:05,199
true the vectors have to

01:36:02,719 --> 01:36:06,960
have to have uh every element populated

01:36:05,199 --> 01:36:09,520
or maybe we can do clever things with

01:36:06,960 --> 01:36:11,600
with masks it can it can do that

01:36:09,520 --> 01:36:14,080
but really if if if we look at main

01:36:11,600 --> 01:36:14,560
memory if it's if we're doing a vector

01:36:14,080 --> 01:36:16,400
opera

01:36:14,560 --> 01:36:18,159
operation on things scattered all over

01:36:16,400 --> 01:36:19,280
memory we need to gather them into the

01:36:18,159 --> 01:36:20,560
register

01:36:19,280 --> 01:36:22,400
then they're going to be computed and

01:36:20,560 --> 01:36:23,920
then scattered back out and this

01:36:22,400 --> 01:36:27,280
gathering scatter it's not a very

01:36:23,920 --> 01:36:27,280
efficient memory pattern at all

01:36:28,239 --> 01:36:33,040
also a cache line to say is only 64

01:36:30,719 --> 01:36:36,080
bytes so those first two values

01:36:33,040 --> 01:36:37,679
the p the first two p1 values are going

01:36:36,080 --> 01:36:38,320
to be on the first cache line and the

01:36:37,679 --> 01:36:40,159
second

01:36:38,320 --> 01:36:42,159
p1 values are going to be on the second

01:36:40,159 --> 01:36:45,199
cache line so i'm going to have to move

01:36:42,159 --> 01:36:47,199
128 bytes worth of memory into my cache

01:36:45,199 --> 01:36:48,800
gather it all together operate it and

01:36:47,199 --> 01:36:50,239
then scatter it back to two different

01:36:48,800 --> 01:36:52,560
cache lines so i have to read all this

01:36:50,239 --> 01:36:54,639
extra memory to fill the vector up

01:36:52,560 --> 01:36:56,400
and then going to finish going over p1

01:36:54,639 --> 01:36:58,480
and then go to p2 so

01:36:56,400 --> 01:36:59,840
stuff is not going to remain in cache

01:36:58,480 --> 01:37:00,880
that long because it's going to be

01:36:59,840 --> 01:37:03,280
overwritten

01:37:00,880 --> 01:37:05,199
this is really not using all of those

01:37:03,280 --> 01:37:06,560
cache lines very efficiently i'm just

01:37:05,199 --> 01:37:08,960
using

01:37:06,560 --> 01:37:12,639
you know an a2 two out of the eight

01:37:08,960 --> 01:37:12,639
elements that might be loaded into cache

01:37:13,040 --> 01:37:17,040
so let's switch things around let's make

01:37:15,199 --> 01:37:18,719
a new type let's make a grid type

01:37:17,040 --> 01:37:20,639
instead of a cell type

01:37:18,719 --> 01:37:22,000
and for this it's going to contain each

01:37:20,639 --> 01:37:23,600
of those properties as the

01:37:22,000 --> 01:37:26,159
two-dimensional grid so i've got

01:37:23,600 --> 01:37:28,400
an allocatable reel of these 2d arrays

01:37:26,159 --> 01:37:30,480
property one two three and four

01:37:28,400 --> 01:37:32,239
then i have my loop again and it looks

01:37:30,480 --> 01:37:34,239
very similar to the previous loop

01:37:32,239 --> 01:37:36,400
i just had to change a little bit of the

01:37:34,239 --> 01:37:38,639
of the word so i now have a grid

01:37:36,400 --> 01:37:40,000
element property i j and i'm going to

01:37:38,639 --> 01:37:42,080
update that

01:37:40,000 --> 01:37:44,000
so how about vectorizing over this well

01:37:42,080 --> 01:37:46,719
the order in memory has changed

01:37:44,000 --> 01:37:49,199
i now have all of the the p1 properties

01:37:46,719 --> 01:37:49,679
in in a row all the way throughout my 2d

01:37:49,199 --> 01:37:52,159
grid

01:37:49,679 --> 01:37:54,639
then all of the p2s and all the p3s at

01:37:52,159 --> 01:37:57,040
some other point in memory

01:37:54,639 --> 01:37:57,920
so now what happens when we vectorize

01:37:57,040 --> 01:37:59,840
well we can just

01:37:57,920 --> 01:38:02,000
go to a whole four consecutive

01:37:59,840 --> 01:38:05,600
consecutive or contiguous elements

01:38:02,000 --> 01:38:07,040
in our array this is much more efficient

01:38:05,600 --> 01:38:08,719
our memory accesses are known as

01:38:07,040 --> 01:38:10,560
coalesced

01:38:08,719 --> 01:38:11,920
that means every element of the vector

01:38:10,560 --> 01:38:14,000
lane is accessing

01:38:11,920 --> 01:38:15,280
one more than its neighbor is directly

01:38:14,000 --> 01:38:17,360
adjacent every time

01:38:15,280 --> 01:38:19,600
and these are key to having high

01:38:17,360 --> 01:38:22,000
performance code

01:38:19,600 --> 01:38:24,080
so essentially adjacent vector lanes

01:38:22,000 --> 01:38:26,960
read adjacent memory locations and this

01:38:24,080 --> 01:38:28,880
is what we need for good performance

01:38:26,960 --> 01:38:31,360
also when we think about our cache which

01:38:28,880 --> 01:38:33,280
our cash is 64 bytes

01:38:31,360 --> 01:38:34,880
so we can now fill that vector up just

01:38:33,280 --> 01:38:36,719
from a single cache line so we're making

01:38:34,880 --> 01:38:38,639
much more use of that cache line and we

01:38:36,719 --> 01:38:41,119
don't have to move twice as much memory

01:38:38,639 --> 01:38:42,480
just to fill a single vector

01:38:41,119 --> 01:38:45,600
so as you can see this is a much more

01:38:42,480 --> 01:38:47,679
efficient approach

01:38:45,600 --> 01:38:49,199
now we can expand our sort of ideas

01:38:47,679 --> 01:38:50,400
about memory access patterns and

01:38:49,199 --> 01:38:51,840
usefulness and what's good for

01:38:50,400 --> 01:38:53,119
performance

01:38:51,840 --> 01:38:54,800
let's just take this little loop and

01:38:53,119 --> 01:38:56,840
we're going to change it a few ways and

01:38:54,800 --> 01:38:59,360
see what happens to our memory access

01:38:56,840 --> 01:39:00,400
pattern so this loop just loops over an

01:38:59,360 --> 01:39:03,920
array a

01:39:00,400 --> 01:39:06,159
and we're gonna um do something with it

01:39:03,920 --> 01:39:08,719
so let's say our our memory is aligned

01:39:06,159 --> 01:39:10,400
so say a 64 byte cache line boundary

01:39:08,719 --> 01:39:12,000
and this is the ideal memory access band

01:39:10,400 --> 01:39:14,000
it's aligned and all the

01:39:12,000 --> 01:39:16,080
access is coalesced you can see this

01:39:14,000 --> 01:39:17,040
vector lane maybe i've got eight eight

01:39:16,080 --> 01:39:18,639
wide vectors

01:39:17,040 --> 01:39:20,080
and it's accessing all eight elements of

01:39:18,639 --> 01:39:21,760
that cache line

01:39:20,080 --> 01:39:23,600
and there's gonna be a line to the cache

01:39:21,760 --> 01:39:25,520
boundary as well so there's no need to

01:39:23,600 --> 01:39:27,119
load more than one cache boundary

01:39:25,520 --> 01:39:29,360
more than one cache line for every

01:39:27,119 --> 01:39:31,440
vector vector

01:39:29,360 --> 01:39:33,520
data movement that's sort of the most

01:39:31,440 --> 01:39:34,719
optimal thing we can hope for

01:39:33,520 --> 01:39:36,560
as we go through these slides they're

01:39:34,719 --> 01:39:38,960
going to get progressively worse memory

01:39:36,560 --> 01:39:41,119
access patterns

01:39:38,960 --> 01:39:42,960
let's say we have to offset my memory

01:39:41,119 --> 01:39:44,320
access pattern by by a fixed amount so

01:39:42,960 --> 01:39:46,560
we're going to offset everything by

01:39:44,320 --> 01:39:49,199
three this isn't this isn't too bad

01:39:46,560 --> 01:39:50,960
again all the access is still um

01:39:49,199 --> 01:39:53,600
coalesced

01:39:50,960 --> 01:39:55,040
where adjacent memory vector laser

01:39:53,600 --> 01:39:57,520
accessing adjacent

01:39:55,040 --> 01:39:59,520
positions in memory but we now split

01:39:57,520 --> 01:40:01,040
across cache lines because i've offset

01:39:59,520 --> 01:40:03,360
that memory location so

01:40:01,040 --> 01:40:04,800
we still have to load two cache lines uh

01:40:03,360 --> 01:40:06,320
to load this vector in

01:40:04,800 --> 01:40:08,000
but hopefully we can use the remainder

01:40:06,320 --> 01:40:09,520
of that cache line as we continue going

01:40:08,000 --> 01:40:11,199
through this loop

01:40:09,520 --> 01:40:12,960
so we still get good use of cache lines

01:40:11,199 --> 01:40:16,000
but it's still not quite as efficient as

01:40:12,960 --> 01:40:16,000
as the aligned version

01:40:17,199 --> 01:40:20,719
now strided access this is something

01:40:18,960 --> 01:40:23,760
that that's very important and quite

01:40:20,719 --> 01:40:25,119
easy to fall into this trap let's say i

01:40:23,760 --> 01:40:28,080
have a 2d array

01:40:25,119 --> 01:40:30,159
and i've got a j and an i index now

01:40:28,080 --> 01:40:33,119
remember in fortran it's the leftmost

01:40:30,159 --> 01:40:35,280
index which iterates the fastest

01:40:33,119 --> 01:40:36,480
um so that's that's going to be laid out

01:40:35,280 --> 01:40:37,840
in memory uh

01:40:36,480 --> 01:40:39,440
adjacent to each other and it's the

01:40:37,840 --> 01:40:40,960
right most one which has the biggest

01:40:39,440 --> 01:40:43,199
stride

01:40:40,960 --> 01:40:44,639
so say i have some loop a fixed j number

01:40:43,199 --> 01:40:46,320
and i want to loop over i

01:40:44,639 --> 01:40:48,880
this might be like looping over the

01:40:46,320 --> 01:40:52,400
columns of a matrix

01:40:48,880 --> 01:40:54,960
so um equivalently

01:40:52,400 --> 01:40:56,639
uh to the aji that might be eventually

01:40:54,960 --> 01:40:57,760
flattened down to the actual memory

01:40:56,639 --> 01:41:01,199
access pattern which is

01:40:57,760 --> 01:41:01,840
j plus three times i say j has an extent

01:41:01,199 --> 01:41:04,080
of three

01:41:01,840 --> 01:41:05,040
say i had um maybe might have been a

01:41:04,080 --> 01:41:08,480
vision where i had

01:41:05,040 --> 01:41:13,600
or a 3d code where i had xy

01:41:08,480 --> 01:41:13,600
xyz as my j index

01:41:13,679 --> 01:41:18,320
so this then leave me with these strided

01:41:16,320 --> 01:41:20,880
memory access patterns where there's a

01:41:18,320 --> 01:41:22,000
big gap between each vector lane it has

01:41:20,880 --> 01:41:23,600
to access

01:41:22,000 --> 01:41:25,280
this is going to really hurt the

01:41:23,600 --> 01:41:27,840
throughput because i'm not going to be

01:41:25,280 --> 01:41:30,239
using very much of each of those 64 byte

01:41:27,840 --> 01:41:32,320
cache lines i'm only using maybe

01:41:30,239 --> 01:41:34,320
one or three elements of them so i'm

01:41:32,320 --> 01:41:35,920
throwing a lot of memory away

01:41:34,320 --> 01:41:37,840
that i've moved into my cache and

01:41:35,920 --> 01:41:39,920
they're not using it

01:41:37,840 --> 01:41:41,760
um i also have to load lots of cache

01:41:39,920 --> 01:41:43,440
lines in order to fill up the vector

01:41:41,760 --> 01:41:45,040
i've only loaded four here and i'd have

01:41:43,440 --> 01:41:46,480
to keep loading if i had a vector length

01:41:45,040 --> 01:41:48,320
of eight

01:41:46,480 --> 01:41:50,719
this is very easy to fall into the trap

01:41:48,320 --> 01:41:52,480
with with multi-dimensional arrays

01:41:50,719 --> 01:41:55,199
so it's important to make sure that the

01:41:52,480 --> 01:41:58,400
order in which you iterate over the loop

01:41:55,199 --> 01:41:59,920
um is such that um ideally it's stride

01:41:58,400 --> 01:42:00,639
one that's that's what we're going for

01:41:59,920 --> 01:42:02,800
so that

01:42:00,639 --> 01:42:05,199
the innermost loop matches the innermost

01:42:02,800 --> 01:42:06,880
dimension of the multi-dimensional array

01:42:05,199 --> 01:42:08,719
so check your strides it's important to

01:42:06,880 --> 01:42:11,920
make sure that's right and that might be

01:42:08,719 --> 01:42:13,760
a clue for the exercise coming up

01:42:11,920 --> 01:42:15,520
so the worst thing you can do is is some

01:42:13,760 --> 01:42:17,520
sort of indirection is some sort of

01:42:15,520 --> 01:42:20,000
random access to memory and this gives

01:42:17,520 --> 01:42:22,159
you little reuse of cache lines

01:42:20,000 --> 01:42:23,760
this is very challenging and we often

01:42:22,159 --> 01:42:24,400
find this with unstructured codes and

01:42:23,760 --> 01:42:26,159
there's some

01:42:24,400 --> 01:42:28,960
some difficulties associated with trying

01:42:26,159 --> 01:42:30,800
to optimize those

01:42:28,960 --> 01:42:33,760
what this results in is is this our

01:42:30,800 --> 01:42:35,520
indirection we take our index we look up

01:42:33,760 --> 01:42:37,199
that index in some lookup table

01:42:35,520 --> 01:42:37,840
essentially and it gives us a new index

01:42:37,199 --> 01:42:41,199
that we it

01:42:37,840 --> 01:42:42,880
that we then index the first array with

01:42:41,199 --> 01:42:44,639
ultimately it's an unpredictable pattern

01:42:42,880 --> 01:42:46,159
so the hardware prefetches

01:42:44,639 --> 01:42:47,840
might not work as efficiently as a

01:42:46,159 --> 01:42:49,440
result of this this is a very

01:42:47,840 --> 01:42:51,199
challenging position for

01:42:49,440 --> 01:42:52,719
an application to be in and requires a

01:42:51,199 --> 01:42:57,520
lot of thought and a lot of optimization

01:42:52,719 --> 01:42:59,920
to get it to perform well

01:42:57,520 --> 01:43:00,719
so that's cindy and that's part of the

01:42:59,920 --> 01:43:03,280
picture

01:43:00,719 --> 01:43:05,199
let's move on to newman now and and this

01:43:03,280 --> 01:43:07,280
is uh moving on to

01:43:05,199 --> 01:43:09,119
uh changing a little bit of some of the

01:43:07,280 --> 01:43:11,840
abstractions i i talked about

01:43:09,119 --> 01:43:13,600
right at the start of this morning let's

01:43:11,840 --> 01:43:14,960
think about a dual socket

01:43:13,600 --> 01:43:16,560
shared memory system and this is the

01:43:14,960 --> 01:43:17,920
cartoon that i showed you before we've

01:43:16,560 --> 01:43:19,119
got two sockets each with

01:43:17,920 --> 01:43:22,000
let's say four cores and they're

01:43:19,119 --> 01:43:24,080
connected to memory now all threads

01:43:22,000 --> 01:43:25,360
and essentially running on each core

01:43:24,080 --> 01:43:27,520
have access to this

01:43:25,360 --> 01:43:29,600
any part of memory that's that doesn't

01:43:27,520 --> 01:43:31,760
change at all

01:43:29,600 --> 01:43:33,600
but in reality when we build these

01:43:31,760 --> 01:43:34,320
things we have separate sockets and each

01:43:33,600 --> 01:43:36,880
socket

01:43:34,320 --> 01:43:38,800
is physically connected to half of the

01:43:36,880 --> 01:43:40,400
available memory

01:43:38,800 --> 01:43:42,480
we still have this model which i've

01:43:40,400 --> 01:43:43,119
shown with this dash line around the

01:43:42,480 --> 01:43:45,280
memory

01:43:43,119 --> 01:43:46,560
where all cores can still access all the

01:43:45,280 --> 01:43:49,280
memory

01:43:46,560 --> 01:43:49,600
but but the trick is that say the memory

01:43:49,280 --> 01:43:52,239
is

01:43:49,600 --> 01:43:54,400
associated with this socket on the left

01:43:52,239 --> 01:43:55,520
hand side so it exists in the memory on

01:43:54,400 --> 01:43:57,280
the top left

01:43:55,520 --> 01:43:58,880
but the socket on the right hand side

01:43:57,280 --> 01:44:01,440
that particular call wants to

01:43:58,880 --> 01:44:02,800
read some memory and remember it can

01:44:01,440 --> 01:44:04,800
it's allowed to read whatever it likes

01:44:02,800 --> 01:44:07,119
from memory

01:44:04,800 --> 01:44:08,239
so that call that wants to read memory

01:44:07,119 --> 01:44:10,800
it has to go

01:44:08,239 --> 01:44:12,320
via the socket to socket interconnect

01:44:10,800 --> 01:44:14,560
and access memory

01:44:12,320 --> 01:44:16,080
uh using the other sockets memory

01:44:14,560 --> 01:44:18,159
controllers

01:44:16,080 --> 01:44:19,679
so this is going to be slightly slower

01:44:18,159 --> 01:44:22,880
than accessing memory

01:44:19,679 --> 01:44:24,159
just from its own memory controllers

01:44:22,880 --> 01:44:25,679
so the memory controllers on this

01:44:24,159 --> 01:44:27,679
picture are shown by the sort of

01:44:25,679 --> 01:44:29,280
connections between the sockets and the

01:44:27,679 --> 01:44:31,440
memory

01:44:29,280 --> 01:44:33,520
so if you're just re if each core is

01:44:31,440 --> 01:44:35,119
just reading memory from its own half of

01:44:33,520 --> 01:44:37,679
memory

01:44:35,119 --> 01:44:38,320
then we were able to use that the memory

01:44:37,679 --> 01:44:39,840
controller

01:44:38,320 --> 01:44:41,280
and get good performance and the best

01:44:39,840 --> 01:44:43,920
performance whereas if we need to read

01:44:41,280 --> 01:44:45,440
memory from the other socket

01:44:43,920 --> 01:44:48,400
the memory associated with the other

01:44:45,440 --> 01:44:50,320
socket we're able to just

01:44:48,400 --> 01:44:52,080
we have to go buy this interconnect and

01:44:50,320 --> 01:44:53,679
that's going to increase the latency a

01:44:52,080 --> 01:44:56,400
little bit and reduce our

01:44:53,679 --> 01:44:57,840
bandwidth a little bit as well this is

01:44:56,400 --> 01:45:00,000
the idea of this

01:44:57,840 --> 01:45:01,199
pneuma architecture so non-uniform

01:45:00,000 --> 01:45:03,119
memory access

01:45:01,199 --> 01:45:04,320
this means that we get different memory

01:45:03,119 --> 01:45:06,560
performance

01:45:04,320 --> 01:45:08,239
based on where that memory is located

01:45:06,560 --> 01:45:10,239
when we try to access it

01:45:08,239 --> 01:45:12,639
i essentially the the performance

01:45:10,239 --> 01:45:16,880
behavior is not uniform

01:45:12,639 --> 01:45:18,719
that's all that the numera really means

01:45:16,880 --> 01:45:19,920
now why does this happen so let's think

01:45:18,719 --> 01:45:21,199
about what happens when we want to

01:45:19,920 --> 01:45:23,440
allocate

01:45:21,199 --> 01:45:24,800
an array what what might happen well

01:45:23,440 --> 01:45:26,719
this is going to be operating system

01:45:24,800 --> 01:45:30,159
dependent and also

01:45:26,719 --> 01:45:31,920
fortran implementation dependent as well

01:45:30,159 --> 01:45:34,320
but actually allocating the memory

01:45:31,920 --> 01:45:37,119
doesn't actually allocate our memory

01:45:34,320 --> 01:45:38,000
memory is only actually allocated in the

01:45:37,119 --> 01:45:41,440
memory space

01:45:38,000 --> 01:45:45,040
when it's first used ie when we

01:45:41,440 --> 01:45:47,280
start writing to that array

01:45:45,040 --> 01:45:49,040
before we actually touch that memory

01:45:47,280 --> 01:45:50,480
before we start writing to it the memory

01:45:49,040 --> 01:45:52,400
isn't actually allocated

01:45:50,480 --> 01:45:53,840
and when we do allocate it it's

01:45:52,400 --> 01:45:57,040
allocated one

01:45:53,840 --> 01:45:58,960
page at a time now pages are an

01:45:57,040 --> 01:46:01,679
abstraction that are used by

01:45:58,960 --> 01:46:03,520
by processes and the um the operating

01:46:01,679 --> 01:46:04,080
system to help move memory around

01:46:03,520 --> 01:46:06,719
between

01:46:04,080 --> 01:46:09,199
um between different parts of the uh the

01:46:06,719 --> 01:46:12,239
disk and maybe the memory as well

01:46:09,199 --> 01:46:13,199
essentially they're just um it's like

01:46:12,239 --> 01:46:15,360
having

01:46:13,199 --> 01:46:17,040
a segmented part of the of the memory

01:46:15,360 --> 01:46:19,679
space and it moves things around

01:46:17,040 --> 01:46:21,119
and in a page at a time so when you

01:46:19,679 --> 01:46:23,440
allocate an array

01:46:21,119 --> 01:46:25,199
nothing happens and then eventually you

01:46:23,440 --> 01:46:26,639
start you writing to the array

01:46:25,199 --> 01:46:28,320
the operating system is going to give

01:46:26,639 --> 01:46:31,840
you enough pages

01:46:28,320 --> 01:46:33,440
um to store all of your array

01:46:31,840 --> 01:46:34,960
essentially a page is like a page is

01:46:33,440 --> 01:46:36,639
like a piece of paper you can store

01:46:34,960 --> 01:46:38,400
some information on it but if you need

01:46:36,639 --> 01:46:39,280
more than one piece of paper because you

01:46:38,400 --> 01:46:41,199
need to store

01:46:39,280 --> 01:46:42,560
a bigger array then you're going to have

01:46:41,199 --> 01:46:43,360
a number of pages that you're going to

01:46:42,560 --> 01:46:46,239
be given to

01:46:43,360 --> 01:46:46,239
store that data on

01:46:46,719 --> 01:46:50,880
when it's giving you these pages it uses

01:46:48,639 --> 01:46:52,480
a first touch policy this is this is

01:46:50,880 --> 01:46:53,600
this idea that i've sort of just

01:46:52,480 --> 01:46:55,840
explained

01:46:53,600 --> 01:46:57,520
the pages are allocated when the array

01:46:55,840 --> 01:47:00,880
is first

01:46:57,520 --> 01:47:03,840
written to and first accessed

01:47:00,880 --> 01:47:06,080
and the page is placed in the memory um

01:47:03,840 --> 01:47:06,560
associated with each socket by whichever

01:47:06,080 --> 01:47:09,760
thread

01:47:06,560 --> 01:47:13,280
whichever is closest to whichever thread

01:47:09,760 --> 01:47:15,440
touch that data first so what this means

01:47:13,280 --> 01:47:19,840
is that we might allocate memory

01:47:15,440 --> 01:47:21,840
but we want each thread to be able to

01:47:19,840 --> 01:47:23,760
touch the data it's going to for the

01:47:21,840 --> 01:47:25,679
first time and then it's going to then

01:47:23,760 --> 01:47:28,480
recontinue to use that data throughout

01:47:25,679 --> 01:47:28,480
the execution

01:47:28,560 --> 01:47:31,840
this helps then reduce that socket to

01:47:30,480 --> 01:47:34,080
socket

01:47:31,840 --> 01:47:35,760
memory traffic that i was talking about

01:47:34,080 --> 01:47:37,920
a couple of slides ago

01:47:35,760 --> 01:47:38,880
so if each thread can then initialize

01:47:37,920 --> 01:47:41,119
its own data

01:47:38,880 --> 01:47:42,639
and then use the same pieces of data

01:47:41,119 --> 01:47:45,119
those pages are going to be

01:47:42,639 --> 01:47:47,360
existing in each of the two halves of

01:47:45,119 --> 01:47:49,199
memory the distinct halves of memory

01:47:47,360 --> 01:47:51,600
and then the performance will be will be

01:47:49,199 --> 01:47:53,679
much improved

01:47:51,600 --> 01:47:55,679
so let's uh talk a little bit about how

01:47:53,679 --> 01:47:57,840
we do that in practice

01:47:55,679 --> 01:48:00,239
so this is the vector add code and we're

01:47:57,840 --> 01:48:02,239
going to show you how to

01:48:00,239 --> 01:48:04,000
how to take advantage of this first

01:48:02,239 --> 01:48:05,760
touch and the reason we want to do this

01:48:04,000 --> 01:48:07,920
is because it can make the main loops go

01:48:05,760 --> 01:48:11,040
faster even if we just time the

01:48:07,920 --> 01:48:12,320
main kernel parallelizing the

01:48:11,040 --> 01:48:14,000
initialization

01:48:12,320 --> 01:48:17,119
to take advantage of first touch will

01:48:14,000 --> 01:48:18,880
help our compute go faster too

01:48:17,119 --> 01:48:20,320
so on line two then we allocate our

01:48:18,880 --> 01:48:22,560
three arrays just as before

01:48:20,320 --> 01:48:23,760
but then the initialization remember

01:48:22,560 --> 01:48:25,760
right back this morning when i

01:48:23,760 --> 01:48:27,520
initialized them i just set a equals one

01:48:25,760 --> 01:48:28,320
b equals two and c equals naught using

01:48:27,520 --> 01:48:32,080
the

01:48:28,320 --> 01:48:33,679
very shorthand fortran vector notation

01:48:32,080 --> 01:48:35,199
but here i've now written them as a loop

01:48:33,679 --> 01:48:37,679
over the length of the array

01:48:35,199 --> 01:48:39,679
and i've added the openmp parallel do

01:48:37,679 --> 01:48:41,760
construct

01:48:39,679 --> 01:48:43,840
so walking through this code then i've

01:48:41,760 --> 01:48:45,440
allocated some space for memory

01:48:43,840 --> 01:48:47,600
and then when i start touching it in

01:48:45,440 --> 01:48:49,760
this parallel loop each thread is going

01:48:47,600 --> 01:48:50,320
to be touching distinct parts of those

01:48:49,760 --> 01:48:52,960
three

01:48:50,320 --> 01:48:53,360
arrays so those threads are going to be

01:48:52,960 --> 01:48:55,280
um

01:48:53,360 --> 01:48:56,800
those pages associated are going to be

01:48:55,280 --> 01:48:58,239
allocated in the different parts of

01:48:56,800 --> 01:49:01,280
memory

01:48:58,239 --> 01:49:04,719
and so hopefully then when i then access

01:49:01,280 --> 01:49:07,760
my memory in the kernel itself

01:49:04,719 --> 01:49:10,000
um i'll have the same uh parallel

01:49:07,760 --> 01:49:12,639
pattern the same parallel decomposition

01:49:10,000 --> 01:49:14,400
and then there won't really be any or a

01:49:12,639 --> 01:49:15,280
very minimal amount of socket-to-socket

01:49:14,400 --> 01:49:18,320
communication

01:49:15,280 --> 01:49:18,960
um during my computation so this is the

01:49:18,320 --> 01:49:21,599
idea we

01:49:18,960 --> 01:49:23,520
we parallelize our initialization in the

01:49:21,599 --> 01:49:26,239
same way that we parallelize

01:49:23,520 --> 01:49:27,360
our computation so that we can take

01:49:26,239 --> 01:49:30,080
advantage of this

01:49:27,360 --> 01:49:31,599
uh first touch policy and and this new

01:49:30,080 --> 01:49:34,400
up and kind of

01:49:31,599 --> 01:49:36,480
work around the the challenges of of the

01:49:34,400 --> 01:49:40,239
non-uniform memory access that we

01:49:36,480 --> 01:49:40,239
um that we that we observe

01:49:41,040 --> 01:49:43,679
so this this slide just sort of

01:49:42,159 --> 01:49:45,520
summarizes that we're parallelizing

01:49:43,679 --> 01:49:47,920
initialization how we would parallelize

01:49:45,520 --> 01:49:47,920
the loops

01:49:48,159 --> 01:49:51,360
but the operating system is allowed to

01:49:50,239 --> 01:49:53,679
move our threads around

01:49:51,360 --> 01:49:55,280
and between sockets as well so those

01:49:53,679 --> 01:49:57,040
parallel threads respond they can go

01:49:55,280 --> 01:49:58,560
anywhere and this is going to mess up

01:49:57,040 --> 01:50:01,280
all that numeral where code we just

01:49:58,560 --> 01:50:02,080
wrote so this is where openmp gives you

01:50:01,280 --> 01:50:04,239
control

01:50:02,080 --> 01:50:05,360
to be able to lock those threads down to

01:50:04,239 --> 01:50:08,400
specific cores

01:50:05,360 --> 01:50:10,080
these are called um it's the pinning is

01:50:08,400 --> 01:50:11,440
to do with pinning pinning threads to

01:50:10,080 --> 01:50:14,080
cause

01:50:11,440 --> 01:50:15,280
and openmp exposes this in in a number

01:50:14,080 --> 01:50:18,560
of ways it gives you

01:50:15,280 --> 01:50:20,880
a way to specify the place

01:50:18,560 --> 01:50:22,239
to a list of places that you want to pin

01:50:20,880 --> 01:50:24,400
threads to

01:50:22,239 --> 01:50:26,639
and then a thread pinning policy to

01:50:24,400 --> 01:50:28,400
define how those threads are pinned to

01:50:26,639 --> 01:50:30,800
the places

01:50:28,400 --> 01:50:31,920
so by default there's one place that

01:50:30,800 --> 01:50:36,560
contains all the cores

01:50:31,920 --> 01:50:38,800
in the um in the system

01:50:36,560 --> 01:50:39,599
if we use the omp proc bind environment

01:50:38,800 --> 01:50:41,360
variable

01:50:39,599 --> 01:50:43,520
that's then used to set pinning for all

01:50:41,360 --> 01:50:45,119
parallel regions in the code and again

01:50:43,520 --> 01:50:47,440
there's a clause version of this with

01:50:45,119 --> 01:50:50,639
prop bind but we recommend using the

01:50:47,440 --> 01:50:52,639
environment variable omp proc bind

01:50:50,639 --> 01:50:54,239
so if we set this to false which is

01:50:52,639 --> 01:50:55,840
typically the default threads can move

01:50:54,239 --> 01:50:57,920
around

01:50:55,840 --> 01:51:00,080
but if we set it to true we're saying

01:50:57,920 --> 01:51:01,040
that threads will not move we're pinning

01:51:00,080 --> 01:51:04,560
threads down

01:51:01,040 --> 01:51:06,639
to a particular place

01:51:04,560 --> 01:51:08,639
there's some other useful ones like um

01:51:06,639 --> 01:51:11,920
like master so if we have a

01:51:08,639 --> 01:51:13,840
um some sort of uh nested parallelism

01:51:11,920 --> 01:51:16,080
through some manner then masters can be

01:51:13,840 --> 01:51:19,040
useful there

01:51:16,080 --> 01:51:20,400
now close and spread our two uh policies

01:51:19,040 --> 01:51:22,800
remember with proc bind this is

01:51:20,400 --> 01:51:25,440
specifying the policy

01:51:22,800 --> 01:51:26,480
so with close then we're going to pin

01:51:25,440 --> 01:51:30,320
threads

01:51:26,480 --> 01:51:31,840
close to the master thread so the exact

01:51:30,320 --> 01:51:34,960
kind of algorithm that's used is

01:51:31,840 --> 01:51:37,599
specified in in the openmp specification

01:51:34,960 --> 01:51:39,520
um but if say we have the same number of

01:51:37,599 --> 01:51:41,520
threads as the number of cores

01:51:39,520 --> 01:51:43,440
then thread zero will be pinned to call

01:51:41,520 --> 01:51:47,440
zero thread one to call one

01:51:43,440 --> 01:51:50,320
thread two to core two etc that's close

01:51:47,440 --> 01:51:51,920
now with spread it tries to spread the

01:51:50,320 --> 01:51:53,119
threads out between the available

01:51:51,920 --> 01:51:54,400
resources

01:51:53,119 --> 01:51:56,000
so in the case where the number of

01:51:54,400 --> 01:51:58,320
threads is the same as the number of

01:51:56,000 --> 01:52:01,440
cores the number of places we have

01:51:58,320 --> 01:52:02,960
then we're going to pin thread 0 to core

01:52:01,440 --> 01:52:04,960
0 on the first socket

01:52:02,960 --> 01:52:06,960
then thread 1 goes to core 0 on the

01:52:04,960 --> 01:52:09,760
other socket and then thread 2

01:52:06,960 --> 01:52:11,360
goes to core 1 on the first socket again

01:52:09,760 --> 01:52:13,760
so you'll see that now

01:52:11,360 --> 01:52:15,599
say essentially even numbered threads go

01:52:13,760 --> 01:52:16,960
on one socket and odd number threads go

01:52:15,599 --> 01:52:19,280
on the other socket

01:52:16,960 --> 01:52:20,719
so if if neighboring thread ids have to

01:52:19,280 --> 01:52:23,440
communicate with each other

01:52:20,719 --> 01:52:24,960
then a spread a sparse distribution of

01:52:23,440 --> 01:52:26,560
those things across the

01:52:24,960 --> 01:52:28,719
across the cores is not going to be

01:52:26,560 --> 01:52:31,840
optimal we're going to want a close

01:52:28,719 --> 01:52:31,840
um pinning instead

01:52:32,880 --> 01:52:37,199
so the affinity that i showed you

01:52:35,280 --> 01:52:37,760
defines how the threads are assigned to

01:52:37,199 --> 01:52:40,560
places

01:52:37,760 --> 01:52:42,800
and the places define how you how the

01:52:40,560 --> 01:52:44,560
hardware resources is divided up so that

01:52:42,800 --> 01:52:45,119
you can assign the threads to them using

01:52:44,560 --> 01:52:47,520
the

01:52:45,119 --> 01:52:49,040
policy so by default as i said there's

01:52:47,520 --> 01:52:51,040
one place for the course

01:52:49,040 --> 01:52:52,800
and it's the omp places environment

01:52:51,040 --> 01:52:55,040
variable that allows you to control

01:52:52,800 --> 01:52:58,000
what that might be now there's three

01:52:55,040 --> 01:52:59,280
very common ones that you might use if

01:52:58,000 --> 01:53:01,440
you said it's a thread then

01:52:59,280 --> 01:53:03,679
each place contains a single hardware

01:53:01,440 --> 01:53:04,159
thread and if you said it's a cause each

01:53:03,679 --> 01:53:07,119
place

01:53:04,159 --> 01:53:08,960
contains a single core that might be of

01:53:07,119 --> 01:53:10,880
size one or two depending on if you have

01:53:08,960 --> 01:53:13,199
hardware threads turned on

01:53:10,880 --> 01:53:14,639
likewise you might have omp places

01:53:13,199 --> 01:53:16,480
equals to sockets which is going to

01:53:14,639 --> 01:53:18,000
contain the cores and threads on each

01:53:16,480 --> 01:53:20,320
socket

01:53:18,000 --> 01:53:22,239
now the policy is going to iterate and

01:53:20,320 --> 01:53:23,520
and be defined and all the details are

01:53:22,239 --> 01:53:26,320
in this specification but

01:53:23,520 --> 01:53:27,440
it it then it shares the threads between

01:53:26,320 --> 01:53:30,000
between places

01:53:27,440 --> 01:53:30,560
and the between the distinct places and

01:53:30,000 --> 01:53:33,199
then

01:53:30,560 --> 01:53:34,560
the the hardware resources within each

01:53:33,199 --> 01:53:36,239
place

01:53:34,560 --> 01:53:39,280
we can also use the notation which

01:53:36,239 --> 01:53:41,679
defines here four places of size

01:53:39,280 --> 01:53:44,639
four threads if that's what we wanted to

01:53:41,679 --> 01:53:46,800
do for instance and

01:53:44,639 --> 01:53:48,400
the notation is the the start and the

01:53:46,800 --> 01:53:50,639
extent not the begin and end

01:53:48,400 --> 01:53:53,119
it's a slightly tricky part of that

01:53:50,639 --> 01:53:53,119
notation

01:53:53,599 --> 01:53:57,679
so that's a lot of information about

01:53:55,280 --> 01:54:00,480
numer so we've talked about what numa is

01:53:57,679 --> 01:54:02,480
and and and it's important to initial uh

01:54:00,480 --> 01:54:04,639
to parallelize initialization

01:54:02,480 --> 01:54:06,800
um to pin the threads we're just gonna

01:54:04,639 --> 01:54:08,560
primarily typically just use omp prop

01:54:06,800 --> 01:54:11,520
bind true or possibly

01:54:08,560 --> 01:54:12,960
close or spread to essentially lock the

01:54:11,520 --> 01:54:15,199
threads down

01:54:12,960 --> 01:54:16,800
so that they don't move so they can be

01:54:15,199 --> 01:54:18,560
effective with our numa aware

01:54:16,800 --> 01:54:20,639
implementation

01:54:18,560 --> 01:54:22,400
when we start running hybrid codes with

01:54:20,639 --> 01:54:24,480
say mpi and openmp

01:54:22,400 --> 01:54:26,560
thread pinning can be a bit tricky and

01:54:24,480 --> 01:54:27,280
it's often dependent on the interaction

01:54:26,560 --> 01:54:29,760
of

01:54:27,280 --> 01:54:30,320
um the mpi library and the queuing

01:54:29,760 --> 01:54:31,840
system

01:54:30,320 --> 01:54:33,119
that your or the job scheduler that

01:54:31,840 --> 01:54:34,480
you're running on your particular

01:54:33,119 --> 01:54:36,400
supercomputer

01:54:34,480 --> 01:54:38,080
so i'm not going to get into the the

01:54:36,400 --> 01:54:40,480
gory details of that today

01:54:38,080 --> 01:54:41,760
as it's so system dependent but i will

01:54:40,480 --> 01:54:45,520
just speak a little bit about

01:54:41,760 --> 01:54:47,679
mpi and openmp and what are the benefits

01:54:45,520 --> 01:54:49,280
for maybe thinking about that

01:54:47,679 --> 01:54:51,199
well the first one is supercomputers

01:54:49,280 --> 01:54:51,679
have this sort of hierarchical structure

01:54:51,199 --> 01:54:53,679
we have

01:54:51,679 --> 01:54:54,880
shared memory nodes connected together

01:54:53,679 --> 01:54:56,800
with a network

01:54:54,880 --> 01:54:58,480
so on each node we have lots of cores

01:54:56,800 --> 01:55:01,520
and we communicate between

01:54:58,480 --> 01:55:03,760
different nodes with lots of cores so we

01:55:01,520 --> 01:55:05,040
need to have mpi or maybe pgas or

01:55:03,760 --> 01:55:06,480
something or charm plus plus or

01:55:05,040 --> 01:55:09,199
something like that to communicate

01:55:06,480 --> 01:55:12,320
between those distributed nodes

01:55:09,199 --> 01:55:13,360
but with multi-core um within each node

01:55:12,320 --> 01:55:16,000
we could run

01:55:13,360 --> 01:55:16,400
mpi everywhere we could run flat mpi and

01:55:16,000 --> 01:55:20,560
and

01:55:16,400 --> 01:55:22,320
do communication between those cores

01:55:20,560 --> 01:55:23,679
or we could take advantage of this

01:55:22,320 --> 01:55:26,560
shared memory architecture

01:55:23,679 --> 01:55:28,080
and run some sort of hybrid mpin openmp

01:55:26,560 --> 01:55:31,280
or similarly

01:55:28,080 --> 01:55:34,320
mpi with the shared memory mpi in

01:55:31,280 --> 01:55:36,400
mpi 3. what this means is we

01:55:34,320 --> 01:55:38,400
might have larger fewer messages that

01:55:36,400 --> 01:55:40,719
can take advantage of the bandwidth

01:55:38,400 --> 01:55:42,639
optimizations in networks

01:55:40,719 --> 01:55:44,480
giving you bandwidth is is just a matter

01:55:42,639 --> 01:55:45,599
of giving you more connections between

01:55:44,480 --> 01:55:48,080
nodes whereas

01:55:45,599 --> 01:55:49,199
getting the latency down is a matter of

01:55:48,080 --> 01:55:51,199
making the

01:55:49,199 --> 01:55:53,040
the link faster which is much harder to

01:55:51,199 --> 01:55:55,520
do

01:55:53,040 --> 01:55:56,960
we also have fewer mpis to manage so

01:55:55,520 --> 01:55:58,320
there's fewer to synchronize fewer for

01:55:56,960 --> 01:55:59,440
connectors to communicate with each

01:55:58,320 --> 01:56:01,840
other

01:55:59,440 --> 01:56:03,840
also we can have this idea of a reduced

01:56:01,840 --> 01:56:04,960
memory footprint for each mpi rank

01:56:03,840 --> 01:56:07,119
because

01:56:04,960 --> 01:56:08,480
a reduced memory footprint on each node

01:56:07,119 --> 01:56:11,520
sorry because we

01:56:08,480 --> 01:56:12,000
any shared data between different npr

01:56:11,520 --> 01:56:14,639
ranks

01:56:12,000 --> 01:56:16,159
we just have one copy on on that node

01:56:14,639 --> 01:56:18,800
and that can help us reduce

01:56:16,159 --> 01:56:21,199
some copies for this um communication

01:56:18,800 --> 01:56:24,239
between the nodes we don't have to

01:56:21,199 --> 01:56:24,880
share data explicitly between cores on a

01:56:24,239 --> 01:56:26,639
node

01:56:24,880 --> 01:56:28,800
we can just have a barrier and some sort

01:56:26,639 --> 01:56:30,239
of synchronization or end add do loop

01:56:28,800 --> 01:56:31,360
and then that that memory has been

01:56:30,239 --> 01:56:34,080
communicated

01:56:31,360 --> 01:56:35,679
between the cores on a node it also

01:56:34,080 --> 01:56:37,840
gives us a flexibility we can

01:56:35,679 --> 01:56:39,199
parallelize some dimensions with mpi and

01:56:37,840 --> 01:56:41,920
some with openmp

01:56:39,199 --> 01:56:42,800
it's often more convenient to express

01:56:41,920 --> 01:56:44,320
these sort of challenging

01:56:42,800 --> 01:56:47,760
multi-dimensional problems with a

01:56:44,320 --> 01:56:47,760
hierarchical parallel scheme

01:56:48,000 --> 01:56:52,639
so if you're going to use hybrid mpi and

01:56:50,320 --> 01:56:54,480
openmp you must make sure you tell mpi

01:56:52,639 --> 01:56:57,840
that you're going to be using

01:56:54,480 --> 01:57:00,400
a parallel process so by default

01:56:57,840 --> 01:57:01,520
mpi doesn't support um this if you just

01:57:00,400 --> 01:57:03,520
call npr init

01:57:01,520 --> 01:57:06,239
you'll be operating in thread single

01:57:03,520 --> 01:57:08,719
mode so you're not allowed any threads

01:57:06,239 --> 01:57:09,840
and there are a number of more and more

01:57:08,719 --> 01:57:12,000
parallel

01:57:09,840 --> 01:57:13,199
ways of of using npr and you have to use

01:57:12,000 --> 01:57:15,840
mpi thread in it

01:57:13,199 --> 01:57:18,320
to do that and ultimately it's these

01:57:15,840 --> 01:57:20,719
different levels just say

01:57:18,320 --> 01:57:22,400
which parallel bit of your program is

01:57:20,719 --> 01:57:24,719
allowed to call mpi

01:57:22,400 --> 01:57:26,400
is it the original process is it any

01:57:24,719 --> 01:57:28,000
thread but as a programmer we're

01:57:26,400 --> 01:57:30,719
responsible to synchronize

01:57:28,000 --> 01:57:31,280
or can any thread any parallel thing

01:57:30,719 --> 01:57:34,000
make

01:57:31,280 --> 01:57:35,280
any mpi call whenever it likes and the

01:57:34,000 --> 01:57:38,560
mpi library

01:57:35,280 --> 01:57:40,000
has to deal with dealing with that

01:57:38,560 --> 01:57:41,760
it's also important to make sure that

01:57:40,000 --> 01:57:43,440
our mpi still lines up

01:57:41,760 --> 01:57:46,560
to avoid any deadlock that we might have

01:57:43,440 --> 01:57:47,840
with the npr communications

01:57:46,560 --> 01:57:49,760
so that's it for this section and we're

01:57:47,840 --> 01:57:53,199
going to come to the last exercise

01:57:49,760 --> 01:57:54,480
before lunch so we have a parallel

01:57:53,199 --> 01:57:55,360
five-point stencil that has the

01:57:54,480 --> 01:57:56,560
reduction in it

01:57:55,360 --> 01:57:58,480
and this exercise we're going to

01:57:56,560 --> 01:57:59,679
optimize it and there's a few things

01:57:58,480 --> 01:58:01,199
that i'd like to think about

01:57:59,679 --> 01:58:02,800
whilst you're doing it thinking about

01:58:01,199 --> 01:58:04,560
the memory access patterns does it have

01:58:02,800 --> 01:58:04,880
an efficient memory access pattern are

01:58:04,560 --> 01:58:06,960
this

01:58:04,880 --> 01:58:09,679
is it stride one or stride something

01:58:06,960 --> 01:58:11,679
else does it vectorize on the cpu are we

01:58:09,679 --> 01:58:13,760
getting good vectorization performance

01:58:11,679 --> 01:58:15,599
what about numer can we take advantage

01:58:13,760 --> 01:58:17,360
of this non-uniform memory architecture

01:58:15,599 --> 01:58:18,880
and use openmp

01:58:17,360 --> 01:58:21,199
and the first touch policy and the

01:58:18,880 --> 01:58:23,840
pinning in order to improve the

01:58:21,199 --> 01:58:24,560
the performance of our code it might be

01:58:23,840 --> 01:58:25,920
useful to

01:58:24,560 --> 01:58:27,599
just make a little note of any

01:58:25,920 --> 01:58:29,599
performance differences you make as you

01:58:27,599 --> 01:58:31,599
make these optimizations

01:58:29,599 --> 01:58:33,280
you might also think if you if you're

01:58:31,599 --> 01:58:34,000
interested in calculating the achieve

01:58:33,280 --> 01:58:36,480
memory bandwidth

01:58:34,000 --> 01:58:37,040
of your stencil code primarily this is

01:58:36,480 --> 01:58:40,560
just

01:58:37,040 --> 01:58:43,119
the size of those two arrays in bytes

01:58:40,560 --> 01:58:43,760
times the number of iterations divided

01:58:43,119 --> 01:58:46,000
by

01:58:43,760 --> 01:58:47,679
the total runtime and that will give you

01:58:46,000 --> 01:58:50,080
a way of measuring

01:58:47,679 --> 01:58:51,199
the amount of memory so if it's n is an

01:58:50,080 --> 01:58:53,920
n by n

01:58:51,199 --> 01:58:54,320
grid that's red say 10 times you would

01:58:53,920 --> 01:58:57,760
do

01:58:54,320 --> 01:58:59,360
n times n times 10 times 8 in bytes

01:58:57,760 --> 01:59:01,679
divided by the runtime and that gives

01:58:59,360 --> 01:59:03,360
you an output of a memory brand with

01:59:01,679 --> 01:59:04,560
output in bytes per second which you can

01:59:03,360 --> 01:59:07,679
factorize to say

01:59:04,560 --> 01:59:09,040
gigabytes per second or something so

01:59:07,679 --> 01:59:10,880
that's the exercise and this really

01:59:09,040 --> 01:59:12,159
concludes the first half of this

01:59:10,880 --> 01:59:14,159
tutorial for the day so we're going to

01:59:12,159 --> 01:59:16,560
run this up until lunch

01:59:14,159 --> 01:59:18,320
lunch starts at at 12 30 and it's going

01:59:16,560 --> 01:59:20,320
to be a full hour for lunch and we'll

01:59:18,320 --> 01:59:23,760
come back at half plus one

01:59:20,320 --> 01:59:27,360
um and and move on to the gpu side of

01:59:23,760 --> 01:59:28,719
of openmp so it's over to you now

01:59:27,360 --> 01:59:30,719
let's get back on is embard and let's

01:59:28,719 --> 01:59:31,280
see if we can improve the performance of

01:59:30,719 --> 01:59:33,199
that code

01:59:31,280 --> 01:59:35,679
and feel free to speak to each other and

01:59:33,199 --> 01:59:36,880
and use the q a to speak to us and ask

01:59:35,679 --> 01:59:38,320
each other for help and

01:59:36,880 --> 01:59:40,159
and see how you're getting on with with

01:59:38,320 --> 01:59:45,280
the optimizations you're making to the

01:59:40,159 --> 01:59:48,080
stencil code

01:59:45,280 --> 01:59:49,760
okay we're coming to the end of the uh

01:59:48,080 --> 01:59:50,719
the session this morning we've come a

01:59:49,760 --> 01:59:52,719
really long way

01:59:50,719 --> 01:59:53,920
uh we started with serial fortran and

01:59:52,719 --> 01:59:57,040
we're now writing

01:59:53,920 --> 01:59:59,760
optimized um vectorized

01:59:57,040 --> 02:00:01,119
numero aware parallel fortran programs

01:59:59,760 --> 02:00:03,520
in openmp

02:00:01,119 --> 02:00:05,199
so it's been a really um dense morning

02:00:03,520 --> 02:00:05,760
where we've packed in a lot of material

02:00:05,199 --> 02:00:07,119
but

02:00:05,760 --> 02:00:08,880
hopefully you've all found it really

02:00:07,119 --> 02:00:10,560
enjoyable

02:00:08,880 --> 02:00:13,040
let me know any feedback you have it's

02:00:10,560 --> 02:00:14,400
always uh good to know what you

02:00:13,040 --> 02:00:16,320
what you think and what you liked and

02:00:14,400 --> 02:00:16,639
what you didn't like as we um as you

02:00:16,320 --> 02:00:18,480
know

02:00:16,639 --> 02:00:20,320
we take these tutorials very seriously

02:00:18,480 --> 02:00:21,920
and i like to try and improve them

02:00:20,320 --> 02:00:23,599
every time i do them so it's it's

02:00:21,920 --> 02:00:25,040
important that you let me know um that

02:00:23,599 --> 02:00:27,679
feedback however however

02:00:25,040 --> 02:00:28,880
you you'd like to so we're now going to

02:00:27,679 --> 02:00:30,719
come to a lunch break

02:00:28,880 --> 02:00:32,480
and the lunch is going to be an hour

02:00:30,719 --> 02:00:36,239
from from 12 30 to 1

02:00:32,480 --> 02:00:39,599
30 uk time so from now for for an hour

02:00:36,239 --> 02:00:41,679
we're gonna break for lunch when we come

02:00:39,599 --> 02:00:43,840
back this afternoon

02:00:41,679 --> 02:00:46,719
we're going to move on to looking at

02:00:43,840 --> 02:00:48,560
programming gpus with openmp

02:00:46,719 --> 02:00:50,320
so we're going to build on the the

02:00:48,560 --> 02:00:53,440
stencil code that we have

02:00:50,320 --> 02:00:54,639
written in in this morning and and be

02:00:53,440 --> 02:00:56,239
able to start

02:00:54,639 --> 02:00:58,800
getting it running and getting it

02:00:56,239 --> 02:01:01,760
running well on a gpu

02:00:58,800 --> 02:01:03,760
um and that's going to cover um an awful

02:01:01,760 --> 02:01:04,960
lot of the openmp specifications so

02:01:03,760 --> 02:01:06,800
if you stick with us through to the

02:01:04,960 --> 02:01:07,199
afternoon you'll have learned pretty

02:01:06,800 --> 02:01:09,280
much

02:01:07,199 --> 02:01:10,880
most things that are in the the openmp

02:01:09,280 --> 02:01:12,560
specification with a with a few

02:01:10,880 --> 02:01:15,119
exceptions of course

02:01:12,560 --> 02:01:16,080
i'm going to leave the zoom meeting open

02:01:15,119 --> 02:01:18,639
over lunch with

02:01:16,080 --> 02:01:20,159
with this agenda up we're going to keep

02:01:18,639 --> 02:01:22,080
the chat and everything open and

02:01:20,159 --> 02:01:23,520
i will try and keep an eye on that

02:01:22,080 --> 02:01:24,960
during the lunch break as well so if you

02:01:23,520 --> 02:01:25,679
have any questions or anything that

02:01:24,960 --> 02:01:28,880
you'd like to

02:01:25,679 --> 02:01:30,800
like to ask then do just keep um keep

02:01:28,880 --> 02:01:31,520
going with the exercises if you'd like

02:01:30,800 --> 02:01:33,760
to

02:01:31,520 --> 02:01:36,000
um and also ask any questions on the q a

02:01:33,760 --> 02:01:37,040
and we'll pick them up over the hour so

02:01:36,000 --> 02:01:38,960
thanks very much for

02:01:37,040 --> 02:01:41,760
for this morning thanks for listening

02:01:38,960 --> 02:01:44,239
and and trying out the exercises

02:01:41,760 --> 02:01:45,040
and we'll see you in in an hour's time

02:01:44,239 --> 02:01:49,520
at 1

02:01:45,040 --> 02:01:49,520

YouTube URL: https://www.youtube.com/watch?v=ZUxfVF6IyQw


