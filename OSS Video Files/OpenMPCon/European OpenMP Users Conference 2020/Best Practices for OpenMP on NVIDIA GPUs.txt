Title: Best Practices for OpenMP on NVIDIA GPUs
Publication date: 2020-12-21
Playlist: European OpenMP Users Conference 2020
Description: 
	This talk was presented at the 3rd European OpenMP Users Conference in 2020

Presented by : Jeff Larkin and Tim Costa, NVIDIA

Conference Website: https://europe.openmpusers.org
Follow us: https://twitter.com/openmp_users

Presentation Abstract :
The OpenMP target constructs extend OpenMP beyond the realm of CPU computing and allow codes to run on a variety of heterogenous devices. Running on diverse devices, however, requires writing code in a manner that will be scalable to a variety of devices. In this talk we will present an update on NVIDIAâ€™s OpenMP compiler and best practices on how to write OpenMP code that will perform well on NVIDIA GPUs.
Captions: 
	00:00:03,919 --> 00:00:06,000
so we'll now move on to the the next

00:00:05,520 --> 00:00:09,120
talk

00:00:06,000 --> 00:00:09,679
um that's from uh jeff larkin and tim

00:00:09,120 --> 00:00:12,320
costa

00:00:09,679 --> 00:00:12,719
both from nvidia uh jeff's a member of

00:00:12,320 --> 00:00:15,440
the

00:00:12,719 --> 00:00:17,600
the developer technology group at nvidia

00:00:15,440 --> 00:00:19,600
um and is working on performance

00:00:17,600 --> 00:00:21,520
analysis and optimization of hpc

00:00:19,600 --> 00:00:24,640
applications and represents

00:00:21,520 --> 00:00:26,960
nvidia on uh in on the openmp uh

00:00:24,640 --> 00:00:28,480
architecture review board so helping

00:00:26,960 --> 00:00:31,439
nvidia shape this

00:00:28,480 --> 00:00:33,120
openmp standard itself um i believe tim

00:00:31,439 --> 00:00:33,840
is also going to be talking as well and

00:00:33,120 --> 00:00:36,960
he's a

00:00:33,840 --> 00:00:39,200
group product manager for hp software um

00:00:36,960 --> 00:00:40,879
and in particular that means for the the

00:00:39,200 --> 00:00:42,320
compilers and libraries that nvidia

00:00:40,879 --> 00:00:45,440
provides

00:00:42,320 --> 00:00:46,559
so over to you uh tim and jeff thanks

00:00:45,440 --> 00:00:48,160
for joining us

00:00:46,559 --> 00:00:50,160
thank you are you seeing my screen right

00:00:48,160 --> 00:00:52,879
now yep looks great to me

00:00:50,160 --> 00:00:54,399
wonderful so i'm jeff um tim is going to

00:00:52,879 --> 00:00:56,719
be on the line with us uh

00:00:54,399 --> 00:00:58,640
answering your questions and i'll uh be

00:00:56,719 --> 00:01:00,079
speaking to the slides

00:00:58,640 --> 00:01:03,039
and uh thank you for the introduction

00:01:00,079 --> 00:01:05,280
and for the invitation to speak

00:01:03,039 --> 00:01:06,240
so i want to speak a bit about your best

00:01:05,280 --> 00:01:08,880
practices for

00:01:06,240 --> 00:01:10,080
writing openmp on uh on our gpus

00:01:08,880 --> 00:01:13,680
obviously we

00:01:10,080 --> 00:01:16,000
uh we are a a company that sells a lot

00:01:13,680 --> 00:01:18,880
of gpus for hpc and

00:01:16,000 --> 00:01:21,280
or uh power um of many of the the

00:01:18,880 --> 00:01:23,520
computers on the top 500 list and we

00:01:21,280 --> 00:01:26,320
openmp is one great option for for

00:01:23,520 --> 00:01:28,400
programming on these hpc systems and i

00:01:26,320 --> 00:01:29,439
want to start by giving you if my slides

00:01:28,400 --> 00:01:32,560
will advance

00:01:29,439 --> 00:01:33,840
um kind of our vision of what parallel

00:01:32,560 --> 00:01:36,320
programming looks like

00:01:33,840 --> 00:01:38,240
now and going into the future i'm hoping

00:01:36,320 --> 00:01:41,520
everything's not

00:01:38,240 --> 00:01:44,720
covering my screen here on your end

00:01:41,520 --> 00:01:47,920
so um you in our uh

00:01:44,720 --> 00:01:50,079
belief of of how parallel programming

00:01:47,920 --> 00:01:52,560
should be done uh now and going into the

00:01:50,079 --> 00:01:52,960
future uh we are very actively working

00:01:52,560 --> 00:01:54,960
with

00:01:52,960 --> 00:01:56,560
the standard languages c plus plus and

00:01:54,960 --> 00:01:59,759
fortran primarily

00:01:56,560 --> 00:02:00,399
and uh pushing them to adopt uh parallel

00:01:59,759 --> 00:02:02,320
features

00:02:00,399 --> 00:02:04,799
so on the left side of this slide you'll

00:02:02,320 --> 00:02:06,799
see examples from both c

00:02:04,799 --> 00:02:08,000
plus and fortran of ways that you can

00:02:06,799 --> 00:02:11,920
write data

00:02:08,000 --> 00:02:13,920
parallel loops in in the native language

00:02:11,920 --> 00:02:15,520
and we really believe that this is the

00:02:13,920 --> 00:02:17,680
ultimate level of

00:02:15,520 --> 00:02:18,879
of portability if we can have our

00:02:17,680 --> 00:02:22,000
developers writing

00:02:18,879 --> 00:02:23,360
in the base languages then great they've

00:02:22,000 --> 00:02:25,200
got code that they could take to any

00:02:23,360 --> 00:02:28,800
compiler to any platform

00:02:25,200 --> 00:02:31,519
and expect it to run well in in parallel

00:02:28,800 --> 00:02:33,519
but the languages are have been slow to

00:02:31,519 --> 00:02:34,080
get there and and don't yet have every

00:02:33,519 --> 00:02:37,519
feature

00:02:34,080 --> 00:02:39,599
a a developer may need and so

00:02:37,519 --> 00:02:41,360
we have some other options for you and

00:02:39,599 --> 00:02:43,760
jumping over to the far right

00:02:41,360 --> 00:02:45,599
if uh if the base languages are the

00:02:43,760 --> 00:02:48,080
ultimate in portability

00:02:45,599 --> 00:02:49,360
then the uh on the far right where we

00:02:48,080 --> 00:02:50,000
show some cuda code well that's the

00:02:49,360 --> 00:02:53,120
ultimate in

00:02:50,000 --> 00:02:54,720
performance and so you may want to

00:02:53,120 --> 00:02:56,160
write your code in this case i'm showing

00:02:54,720 --> 00:02:59,040
who to fortran this could

00:02:56,160 --> 00:03:00,239
just as well have been cuda c plus plus

00:02:59,040 --> 00:03:03,599
and demonstrate

00:03:00,239 --> 00:03:06,800
you can write your code in

00:03:03,599 --> 00:03:08,560
a very uh machine specific language that

00:03:06,800 --> 00:03:10,000
will eke out every last bit of

00:03:08,560 --> 00:03:12,800
performance for

00:03:10,000 --> 00:03:14,720
um for your application on that platform

00:03:12,800 --> 00:03:17,599
but obviously you lose a lot of

00:03:14,720 --> 00:03:18,959
uh portability in this case so we want

00:03:17,599 --> 00:03:20,560
to support developers

00:03:18,959 --> 00:03:22,239
uh who do this to get the best

00:03:20,560 --> 00:03:24,959
performance on our platform

00:03:22,239 --> 00:03:25,599
but um there's a bit of a middle ground

00:03:24,959 --> 00:03:27,840
and for us

00:03:25,599 --> 00:03:30,640
directives are that middle ground

00:03:27,840 --> 00:03:32,959
directives serve as a way to augment

00:03:30,640 --> 00:03:34,720
the base languages and fill in holes

00:03:32,959 --> 00:03:37,440
that they have in supporting

00:03:34,720 --> 00:03:38,879
our platform and in the meantime we

00:03:37,440 --> 00:03:41,440
experiment here we

00:03:38,879 --> 00:03:42,879
we uh we bring uh features here we work

00:03:41,440 --> 00:03:45,840
in openmp

00:03:42,879 --> 00:03:46,159
and open acc both to try to standardize

00:03:45,840 --> 00:03:49,280
on

00:03:46,159 --> 00:03:51,840
features uh that are needed for uh for

00:03:49,280 --> 00:03:53,040
platforms like ours and then we take

00:03:51,840 --> 00:03:54,560
those and we propose

00:03:53,040 --> 00:03:56,400
many of those features down to the base

00:03:54,560 --> 00:03:57,360
languages and hopes that eventually that

00:03:56,400 --> 00:04:00,080
that

00:03:57,360 --> 00:04:01,920
our developers could just write in c

00:04:00,080 --> 00:04:03,920
plus or fortran directly

00:04:01,920 --> 00:04:05,680
so here i've called out uh data

00:04:03,920 --> 00:04:07,760
management is one place where

00:04:05,680 --> 00:04:09,360
uh the languages currently don't have a

00:04:07,760 --> 00:04:11,120
real good mechanism for

00:04:09,360 --> 00:04:13,599
dealing with that and so here's a place

00:04:11,120 --> 00:04:17,280
where directives really do

00:04:13,599 --> 00:04:17,840
really do fit so you know we have this

00:04:17,280 --> 00:04:20,799
uh

00:04:17,840 --> 00:04:22,079
vision where the role of directives is

00:04:20,799 --> 00:04:23,759
to bridge a gap

00:04:22,079 --> 00:04:25,680
from serial programming languages to

00:04:23,759 --> 00:04:28,720
parallel programming languages and

00:04:25,680 --> 00:04:29,520
if we want directives to be able to span

00:04:28,720 --> 00:04:31,199
that gap

00:04:29,520 --> 00:04:32,720
then we need to provide a software

00:04:31,199 --> 00:04:35,520
ecosystem

00:04:32,720 --> 00:04:36,800
that that matches that vision and for

00:04:35,520 --> 00:04:39,199
that

00:04:36,800 --> 00:04:41,120
we have the nvidia hbc sdk and i was

00:04:39,199 --> 00:04:44,960
actually really pleased to see

00:04:41,120 --> 00:04:47,919
the previous speaker using the hpc sdk

00:04:44,960 --> 00:04:49,440
on on the arbon platform so this is an

00:04:47,919 --> 00:04:52,160
sdk that

00:04:49,440 --> 00:04:53,600
goes beyond just gpus it provides

00:04:52,160 --> 00:04:56,880
support for

00:04:53,600 --> 00:05:00,960
uh multi-core cpus of x86

00:04:56,880 --> 00:05:04,240
power and arm it supports um

00:05:00,960 --> 00:05:06,720
c c plus plus fortran it supports

00:05:04,240 --> 00:05:08,320
open acc and openmp and it supports a

00:05:06,720 --> 00:05:09,440
lot of libraries so it's a full

00:05:08,320 --> 00:05:11,440
ecosystem

00:05:09,440 --> 00:05:13,680
and we really do believe that you need

00:05:11,440 --> 00:05:14,960
to be able to use all of these things to

00:05:13,680 --> 00:05:16,639
get the best

00:05:14,960 --> 00:05:18,720
performance and the previous speaker

00:05:16,639 --> 00:05:21,280
mentioned being able to call into math

00:05:18,720 --> 00:05:24,000
libraries and that's actually one of my

00:05:21,280 --> 00:05:26,080
best practices here as well is having

00:05:24,000 --> 00:05:26,720
these libraries and utilizing them as

00:05:26,080 --> 00:05:29,520
part of

00:05:26,720 --> 00:05:30,160
the the big picture so uh this is our

00:05:29,520 --> 00:05:32,639
way of

00:05:30,160 --> 00:05:34,160
of delivering the software that you need

00:05:32,639 --> 00:05:36,800
to be able to

00:05:34,160 --> 00:05:38,000
uh to to bridge that that gap that i

00:05:36,800 --> 00:05:40,000
talked about between serial

00:05:38,000 --> 00:05:43,840
programming languages and the eventual

00:05:40,000 --> 00:05:43,840
uh parallel programming languages

00:05:44,000 --> 00:05:47,199
and you know we as a small part of that

00:05:46,639 --> 00:05:50,639
is the

00:05:47,199 --> 00:05:53,199
hbc compilers and many of you

00:05:50,639 --> 00:05:54,000
may have been familiar with the pgi

00:05:53,199 --> 00:05:57,520
compilers

00:05:54,000 --> 00:06:00,319
that have been around for decades and

00:05:57,520 --> 00:06:02,160
and earlier this year we rebranded those

00:06:00,319 --> 00:06:03,759
to simply the nvidia compilers and you

00:06:02,160 --> 00:06:06,880
can see here that we have

00:06:03,759 --> 00:06:08,800
nvc nvc plus plus nv fortran

00:06:06,880 --> 00:06:10,479
and they support again all of the

00:06:08,800 --> 00:06:14,560
languages including

00:06:10,479 --> 00:06:17,120
uh openmp and openacc uh on multicore

00:06:14,560 --> 00:06:18,240
and on on gpus on a variety of platforms

00:06:17,120 --> 00:06:20,240
so this is

00:06:18,240 --> 00:06:22,080
how the full ecosystem that we provide

00:06:20,240 --> 00:06:25,759
you to be able to

00:06:22,080 --> 00:06:29,360
take advantage of of uh of the features

00:06:25,759 --> 00:06:29,360
of the languages and our hardware

00:06:29,600 --> 00:06:33,360
so with that i want to talk a little bit

00:06:31,600 --> 00:06:36,240
from an application development

00:06:33,360 --> 00:06:38,479
standpoint so i work primarily in uh

00:06:36,240 --> 00:06:39,759
porting and optimizing codes i'm not on

00:06:38,479 --> 00:06:42,000
the compiler team

00:06:39,759 --> 00:06:44,319
and so over the course of uh several

00:06:42,000 --> 00:06:47,280
years of of working within openmp

00:06:44,319 --> 00:06:48,319
and working with openmp developers i've

00:06:47,280 --> 00:06:51,599
kind of come to

00:06:48,319 --> 00:06:52,080
to to these six bullets as the things

00:06:51,599 --> 00:06:53,599
that

00:06:52,080 --> 00:06:55,440
i really want people to know when

00:06:53,599 --> 00:06:57,759
they're trying to write good

00:06:55,440 --> 00:07:00,560
openmp code on gpus because you can

00:06:57,759 --> 00:07:02,080
write uh performant openmp code on gpus

00:07:00,560 --> 00:07:03,840
and you can even write code

00:07:02,080 --> 00:07:06,000
uh that can be portable to other

00:07:03,840 --> 00:07:07,840
platforms as well

00:07:06,000 --> 00:07:09,360
so i've kind of jumped straight to the

00:07:07,840 --> 00:07:11,840
conclusion here and then i'll

00:07:09,360 --> 00:07:12,560
i'll dig in on each of these bullets but

00:07:11,840 --> 00:07:14,960
you can see

00:07:12,560 --> 00:07:16,720
here you know using the features of

00:07:14,960 --> 00:07:18,080
teams and distribute to maximize your

00:07:16,720 --> 00:07:19,680
parallelism

00:07:18,080 --> 00:07:21,520
using collapsing which is another

00:07:19,680 --> 00:07:23,919
feature to expand a parallelism that's

00:07:21,520 --> 00:07:26,479
kind of a theme here for gpus you need

00:07:23,919 --> 00:07:28,400
lots of parallelism being able to

00:07:26,479 --> 00:07:29,840
effectively manage your data with

00:07:28,400 --> 00:07:31,520
the target data directives and map

00:07:29,840 --> 00:07:34,479
clauses that are provided

00:07:31,520 --> 00:07:36,479
uh using accelerated libraries uh using

00:07:34,479 --> 00:07:39,520
openmp tasking and then

00:07:36,479 --> 00:07:41,840
um as kind of a last step there is

00:07:39,520 --> 00:07:43,280
you know people use openmp because they

00:07:41,840 --> 00:07:46,560
want code that is portable

00:07:43,280 --> 00:07:47,840
between platforms and there are features

00:07:46,560 --> 00:07:50,240
such as the if clause

00:07:47,840 --> 00:07:51,840
and the metadirective although i don't

00:07:50,240 --> 00:07:53,120
know of any compilers that have that

00:07:51,840 --> 00:07:55,680
fully supported yet

00:07:53,120 --> 00:07:56,720
uh that provide a mechanism for for

00:07:55,680 --> 00:07:59,840
falling back to

00:07:56,720 --> 00:07:59,840
uh to the host platform

00:08:00,240 --> 00:08:06,080
so to start when people come to me

00:08:03,440 --> 00:08:06,879
saying they want to write openmp on a

00:08:06,080 --> 00:08:08,879
gpu

00:08:06,879 --> 00:08:10,800
frequently they have this picture in

00:08:08,879 --> 00:08:13,759
their head of traditional

00:08:10,800 --> 00:08:15,440
omp parallel four that the first thing

00:08:13,759 --> 00:08:17,199
all of us have learned in openmp or

00:08:15,440 --> 00:08:18,879
perhaps it's omp parallel do if you're

00:08:17,199 --> 00:08:20,879
coming from fortran

00:08:18,879 --> 00:08:22,560
and this has been around and has proven

00:08:20,879 --> 00:08:24,800
itself as a

00:08:22,560 --> 00:08:26,000
being able to scale to potentially

00:08:24,800 --> 00:08:28,319
thousands of threads

00:08:26,000 --> 00:08:29,840
on shared memory parallel machines and

00:08:28,319 --> 00:08:32,399
be able to be built with

00:08:29,840 --> 00:08:34,080
various compilers and perform well so

00:08:32,399 --> 00:08:35,760
people have this assumption that because

00:08:34,080 --> 00:08:37,519
of those past successes

00:08:35,760 --> 00:08:39,039
that this is how you're going to write

00:08:37,519 --> 00:08:41,760
your code

00:08:39,039 --> 00:08:43,919
going forward and in fact it's just not

00:08:41,760 --> 00:08:46,160
enough on a gpu platform

00:08:43,919 --> 00:08:47,360
so many codes already have this parallel

00:08:46,160 --> 00:08:50,959
work sharing

00:08:47,360 --> 00:08:54,080
but gpus are not shared memory parallel

00:08:50,959 --> 00:08:55,920
machines and that's what this uh this

00:08:54,080 --> 00:08:58,480
pair of directives was really designed

00:08:55,920 --> 00:08:58,880
for you know parallel four or parallel

00:08:58,480 --> 00:09:00,640
do

00:08:58,880 --> 00:09:02,080
creates a single contention group of

00:09:00,640 --> 00:09:03,839
some number of threads

00:09:02,080 --> 00:09:05,519
and they have you know a shared view of

00:09:03,839 --> 00:09:07,120
memory they have the ability to

00:09:05,519 --> 00:09:07,600
synchronize and coordinate with each

00:09:07,120 --> 00:09:10,480
other

00:09:07,600 --> 00:09:13,120
and these are features that can limit

00:09:10,480 --> 00:09:16,320
the scalability on a platform like gpus

00:09:13,120 --> 00:09:18,160
where uh we really expect a high degree

00:09:16,320 --> 00:09:18,640
of core screen parallelism to spread

00:09:18,160 --> 00:09:22,080
across

00:09:18,640 --> 00:09:24,240
the entire uh the entire gpu so

00:09:22,080 --> 00:09:26,000
having this structure while it's uh has

00:09:24,240 --> 00:09:28,160
a strong track record

00:09:26,000 --> 00:09:30,000
on a gpu platform it's really less than

00:09:28,160 --> 00:09:31,839
ideal and doesn't really

00:09:30,000 --> 00:09:33,360
exploit many of the advantages of the

00:09:31,839 --> 00:09:35,040
gpu platform

00:09:33,360 --> 00:09:36,880
and it's not to say someone couldn't

00:09:35,040 --> 00:09:39,040
write a creative parallel four

00:09:36,880 --> 00:09:40,640
that can run across an entire gpu but to

00:09:39,040 --> 00:09:41,760
really be throwing away many of the

00:09:40,640 --> 00:09:45,120
advantages

00:09:41,760 --> 00:09:46,240
that the gpus provide so instead we want

00:09:45,120 --> 00:09:49,760
to train people

00:09:46,240 --> 00:09:52,480
to use teams and distribute

00:09:49,760 --> 00:09:54,320
um unfortunately i think i think zoom is

00:09:52,480 --> 00:09:56,880
covering up parts of my slide here

00:09:54,320 --> 00:09:58,160
but teams and distribute are directives

00:09:56,880 --> 00:10:00,480
that spawn

00:09:58,160 --> 00:10:03,200
uh additional level of parallelism so if

00:10:00,480 --> 00:10:05,600
parallel four now is the middle degree

00:10:03,200 --> 00:10:07,279
of openmp parallelism at this thread

00:10:05,600 --> 00:10:09,200
level of parallelism

00:10:07,279 --> 00:10:10,880
and teams and distribute is a coarser

00:10:09,200 --> 00:10:12,320
grain of parallelism by spawning

00:10:10,880 --> 00:10:14,800
multiple thread teams

00:10:12,320 --> 00:10:15,760
and threatens thread teams well they

00:10:14,800 --> 00:10:17,600
can't

00:10:15,760 --> 00:10:19,440
uh they can't coordinate with each other

00:10:17,600 --> 00:10:21,120
in the same way they can't synchronize

00:10:19,440 --> 00:10:22,959
with each other there's actually no

00:10:21,120 --> 00:10:25,680
guarantee that any two teams

00:10:22,959 --> 00:10:26,880
are going to run at the same time this

00:10:25,680 --> 00:10:29,360
makes for

00:10:26,880 --> 00:10:31,440
highly scarab scalable parallelism

00:10:29,360 --> 00:10:35,040
parallelism that can be run on a

00:10:31,440 --> 00:10:36,480
you know a single uh small multi-core

00:10:35,040 --> 00:10:39,040
cpu or you can run

00:10:36,480 --> 00:10:40,560
lots and lots of teams across a very

00:10:39,040 --> 00:10:43,839
large gpu so this

00:10:40,560 --> 00:10:46,800
provides a huge amount of scalability

00:10:43,839 --> 00:10:49,760
for your code

00:10:46,800 --> 00:10:51,680
and that's what what gpus really want

00:10:49,760 --> 00:10:52,720
but what they all note is that because

00:10:51,680 --> 00:10:54,640
now we have

00:10:52,720 --> 00:10:56,720
multiple levels of parallelism we have

00:10:54,640 --> 00:10:58,000
the the team's level of parallelism we

00:10:56,720 --> 00:11:00,000
have the thread level of

00:10:58,000 --> 00:11:01,839
parallelism come from parallel i'm kind

00:11:00,000 --> 00:11:04,320
of skipping simdee here although cimd is

00:11:01,839 --> 00:11:06,000
yet another finer grain of parallelism

00:11:04,320 --> 00:11:07,680
now we have different ways to

00:11:06,000 --> 00:11:09,920
potentially write our code and that

00:11:07,680 --> 00:11:13,040
introduces an interesting challenge to

00:11:09,920 --> 00:11:14,800
developers so here i've put up just two

00:11:13,040 --> 00:11:18,000
different ways that i can use

00:11:14,800 --> 00:11:19,680
teams and distribute to create thread

00:11:18,000 --> 00:11:20,800
teams and distribute work to them and

00:11:19,680 --> 00:11:22,720
then parallel four

00:11:20,800 --> 00:11:24,240
to spawn the threads and work share

00:11:22,720 --> 00:11:26,560
within them

00:11:24,240 --> 00:11:28,240
one has all of the parallelism at the

00:11:26,560 --> 00:11:31,519
outermost loop this j

00:11:28,240 --> 00:11:32,399
loop and here uh j will be split into

00:11:31,519 --> 00:11:34,079
thread teams

00:11:32,399 --> 00:11:36,480
and then activate threads within those

00:11:34,079 --> 00:11:39,120
thread teams and then i will be run

00:11:36,480 --> 00:11:40,560
uh internal to those threads well maybe

00:11:39,120 --> 00:11:41,600
that's not the best way to do it maybe

00:11:40,560 --> 00:11:43,120
it's better to

00:11:41,600 --> 00:11:45,360
have the coarse grained parallelism on

00:11:43,120 --> 00:11:46,800
the j loop and the threaded parallelism

00:11:45,360 --> 00:11:48,800
on the eye loop

00:11:46,800 --> 00:11:52,079
that's another option but well one

00:11:48,800 --> 00:11:52,079
problem here is that now

00:11:52,240 --> 00:11:55,600
this may not work very well on a cpu

00:11:54,240 --> 00:11:56,880
because now i've moved my thread

00:11:55,600 --> 00:11:58,880
parallelism to a different level

00:11:56,880 --> 00:12:00,720
so uh and this is just two there's

00:11:58,880 --> 00:12:02,000
actually additional combinations we can

00:12:00,720 --> 00:12:04,480
come up with with these

00:12:02,000 --> 00:12:06,880
so how do i know ahead of time what's

00:12:04,480 --> 00:12:08,399
the best way to go

00:12:06,880 --> 00:12:09,680
and that's uh that's been a challenge

00:12:08,399 --> 00:12:10,079
and i've done some experiments in the

00:12:09,680 --> 00:12:12,720
past

00:12:10,079 --> 00:12:14,000
and spoken to a variety of performance

00:12:12,720 --> 00:12:14,320
experiments with this but i'm going to

00:12:14,000 --> 00:12:16,000
just

00:12:14,320 --> 00:12:18,240
jump straight ahead to my next best

00:12:16,000 --> 00:12:21,279
practice which is

00:12:18,240 --> 00:12:24,000
most of the time i recommend collapsing

00:12:21,279 --> 00:12:27,040
your loops if they can

00:12:24,000 --> 00:12:27,760
collapsing has the nice benefit of in

00:12:27,040 --> 00:12:30,000
this case

00:12:27,760 --> 00:12:31,040
exposing the full degree of parallelism

00:12:30,000 --> 00:12:34,000
of these loops

00:12:31,040 --> 00:12:35,519
and so now all the iterations of the j

00:12:34,000 --> 00:12:36,560
and the eye loops are available to

00:12:35,519 --> 00:12:38,639
paralyze

00:12:36,560 --> 00:12:40,800
and by writing it here as i have on the

00:12:38,639 --> 00:12:42,880
right i've given the compiler a lot of

00:12:40,800 --> 00:12:44,399
freedom to choose do i run on one thread

00:12:42,880 --> 00:12:45,600
team and paralyze

00:12:44,399 --> 00:12:47,600
pretty much like the traditional

00:12:45,600 --> 00:12:49,760
parallel four or do i run on

00:12:47,600 --> 00:12:51,519
lots of thread teams with uh with some

00:12:49,760 --> 00:12:55,279
number of threads within them

00:12:51,519 --> 00:12:57,200
that that allows me to write code that

00:12:55,279 --> 00:12:58,480
should run pretty well in a gpu and run

00:12:57,200 --> 00:13:00,639
pretty well in a cpu

00:12:58,480 --> 00:13:02,000
and maybe it's not optimal on both of

00:13:00,639 --> 00:13:03,519
those cases maybe we can do some

00:13:02,000 --> 00:13:05,519
some additional tuning to make them

00:13:03,519 --> 00:13:07,920
perform better but it should run

00:13:05,519 --> 00:13:09,920
very well on both it's now i have fairly

00:13:07,920 --> 00:13:12,639
performance portable code

00:13:09,920 --> 00:13:14,160
that i can write once and it will work

00:13:12,639 --> 00:13:17,279
in both cases

00:13:14,160 --> 00:13:20,320
so this gives us a ton of parallelism

00:13:17,279 --> 00:13:21,920
for the compiler to exploit now

00:13:20,320 --> 00:13:24,000
one thing i'll note here is that

00:13:21,920 --> 00:13:27,120
collapsing does um

00:13:24,000 --> 00:13:29,200
have two kind of unfortunate

00:13:27,120 --> 00:13:31,440
side effects here one is that well not

00:13:29,200 --> 00:13:35,200
all loops can be collapsed

00:13:31,440 --> 00:13:37,440
as of openmp 5.0 more loops

00:13:35,200 --> 00:13:38,240
can be collapsed than than previously we

00:13:37,440 --> 00:13:40,720
don't

00:13:38,240 --> 00:13:42,560
solely support uh rectangular loops now

00:13:40,720 --> 00:13:44,240
and now you can have some other

00:13:42,560 --> 00:13:46,160
shapes of loops but still not all loops

00:13:44,240 --> 00:13:48,240
are collapsible and the other side

00:13:46,160 --> 00:13:49,040
effect is i've lost a lot of locality

00:13:48,240 --> 00:13:51,680
here this is a

00:13:49,040 --> 00:13:53,040
this is a 2d stencil on the right uh

00:13:51,680 --> 00:13:55,760
there's a lot of

00:13:53,040 --> 00:13:57,680
2d locality that i could exploit both

00:13:55,760 --> 00:13:59,519
spatially and temporally

00:13:57,680 --> 00:14:00,959
and i've lost that by collapsing so you

00:13:59,519 --> 00:14:02,800
know maybe the the new

00:14:00,959 --> 00:14:04,160
tile directive could help with that but

00:14:02,800 --> 00:14:06,000
nonetheless uh

00:14:04,160 --> 00:14:07,600
as a starting point i definitely

00:14:06,000 --> 00:14:09,199
recommend you know collapsing as much as

00:14:07,600 --> 00:14:10,560
you can so target teams distribute

00:14:09,199 --> 00:14:13,040
parallel four collapse

00:14:10,560 --> 00:14:14,560
is kind of my baseline starting point

00:14:13,040 --> 00:14:17,839
for most openmp

00:14:14,560 --> 00:14:17,839
codes going on to a gpu

00:14:18,720 --> 00:14:22,720
now i'm gonna go through i've got lots

00:14:21,920 --> 00:14:25,519
and lots of

00:14:22,720 --> 00:14:26,399
uh of my loops running on the uh on the

00:14:25,519 --> 00:14:28,480
gpu

00:14:26,399 --> 00:14:30,320
excuse me so naturally it's gonna run a

00:14:28,480 --> 00:14:32,399
whole lot faster well

00:14:30,320 --> 00:14:33,600
in reality what happens for a lot of

00:14:32,399 --> 00:14:35,760
developers is they

00:14:33,600 --> 00:14:37,600
they put a few these directives on their

00:14:35,760 --> 00:14:39,040
their main loops they

00:14:37,600 --> 00:14:40,399
expect the code to run really fast and

00:14:39,040 --> 00:14:42,560
all of a sudden things have slowed down

00:14:40,399 --> 00:14:44,560
a lot

00:14:42,560 --> 00:14:46,160
and i have had some some developers then

00:14:44,560 --> 00:14:47,839
throw their hands up there and walk away

00:14:46,160 --> 00:14:50,560
and they'll say one of two things

00:14:47,839 --> 00:14:51,600
openmp doesn't work or gpus don't work

00:14:50,560 --> 00:14:53,920
and in fact

00:14:51,600 --> 00:14:55,440
neither of those statements is true uh

00:14:53,920 --> 00:14:56,560
they just need to take things in one

00:14:55,440 --> 00:14:59,839
step further

00:14:56,560 --> 00:15:03,040
and so the target directives allow us to

00:14:59,839 --> 00:15:03,839
manage the the data locality uh within

00:15:03,040 --> 00:15:06,480
our code

00:15:03,839 --> 00:15:08,560
so uh this is from a very simple toy

00:15:06,480 --> 00:15:11,519
code here where you can see i've added

00:15:08,560 --> 00:15:12,000
in this case just uh two loop nests to

00:15:11,519 --> 00:15:13,519
the device

00:15:12,000 --> 00:15:17,120
but there's a ton of data movement

00:15:13,519 --> 00:15:19,440
happening back and forth between them

00:15:17,120 --> 00:15:21,040
and the compiler is always going to be

00:15:19,440 --> 00:15:24,000
cautious it's always going to choose

00:15:21,040 --> 00:15:25,360
correctness over performance and so

00:15:24,000 --> 00:15:27,279
since the compiler doesn't

00:15:25,360 --> 00:15:29,199
know that i can share that data that i

00:15:27,279 --> 00:15:30,880
don't need that data on the cpu

00:15:29,199 --> 00:15:32,880
it's going to move it back and forth so

00:15:30,880 --> 00:15:34,560
i can take control of that by using the

00:15:32,880 --> 00:15:35,600
target data directives and the map

00:15:34,560 --> 00:15:38,800
clauses

00:15:35,600 --> 00:15:39,519
and you know going from uh the the top

00:15:38,800 --> 00:15:41,360
code

00:15:39,519 --> 00:15:42,880
uh the code at the top where i have uh

00:15:41,360 --> 00:15:45,199
lots and lots of these

00:15:42,880 --> 00:15:46,720
uh green hosted device copies and red

00:15:45,199 --> 00:15:49,199
device to host copies

00:15:46,720 --> 00:15:50,639
uh all the way down to the bottom where

00:15:49,199 --> 00:15:52,320
data movement is is virtually

00:15:50,639 --> 00:15:55,120
non-existent

00:15:52,320 --> 00:15:56,079
and at that point performance becomes uh

00:15:55,120 --> 00:15:57,839
much much better

00:15:56,079 --> 00:15:59,199
so these are these uh directives are

00:15:57,839 --> 00:16:02,639
really critical

00:15:59,199 --> 00:16:03,440
now um you know the nvidia uh compilers

00:16:02,639 --> 00:16:06,240
do support

00:16:03,440 --> 00:16:06,959
unified memory and we've had a lot of

00:16:06,240 --> 00:16:08,639
good luck with

00:16:06,959 --> 00:16:09,920
unified memory i've had some developers

00:16:08,639 --> 00:16:10,800
tell me they're never going to write

00:16:09,920 --> 00:16:13,120
another

00:16:10,800 --> 00:16:14,480
data clause again because unified memory

00:16:13,120 --> 00:16:16,639
worked really well for them

00:16:14,480 --> 00:16:18,320
but i will point out that that that

00:16:16,639 --> 00:16:20,800
causes you to rely upon

00:16:18,320 --> 00:16:22,720
you know the single compiler or the

00:16:20,800 --> 00:16:24,320
subset of compilers that support unified

00:16:22,720 --> 00:16:27,519
memory or the subset of

00:16:24,320 --> 00:16:30,399
machines that support unified memory so

00:16:27,519 --> 00:16:32,079
from a perspective of uh portability and

00:16:30,399 --> 00:16:34,160
and consistent performance

00:16:32,079 --> 00:16:36,560
you still probably want to put these

00:16:34,160 --> 00:16:38,720
data directives on there

00:16:36,560 --> 00:16:41,600
now kind of as a bonus here i'll also

00:16:38,720 --> 00:16:45,440
point out i've seen cases where

00:16:41,600 --> 00:16:48,000
uh you can take your host arrays

00:16:45,440 --> 00:16:50,079
that are on in your openmp code you can

00:16:48,000 --> 00:16:51,920
register it with the cuda runtime and

00:16:50,079 --> 00:16:53,440
i've seen some performance improvements

00:16:51,920 --> 00:16:55,519
uh happen there so

00:16:53,440 --> 00:16:57,120
uh just kind of a bonus thing to try but

00:16:55,519 --> 00:16:58,240
obviously doing so makes your code less

00:16:57,120 --> 00:16:59,920
portable so if that's

00:16:58,240 --> 00:17:01,839
if portability is your ultimate goal

00:16:59,920 --> 00:17:03,519
then uh then i wouldn't recommend trying

00:17:01,839 --> 00:17:05,839
that but it's something that to put out

00:17:03,519 --> 00:17:05,839
there

00:17:06,880 --> 00:17:10,400
now here's an important one that um

00:17:09,679 --> 00:17:12,000
really

00:17:10,400 --> 00:17:13,839
should have gone probably at the

00:17:12,000 --> 00:17:15,520
beginning but i wanted to jump into the

00:17:13,839 --> 00:17:18,720
directives and that is

00:17:15,520 --> 00:17:19,280
using accelerated libraries the previous

00:17:18,720 --> 00:17:21,280
speaker

00:17:19,280 --> 00:17:23,760
mentioned this as well that taking

00:17:21,280 --> 00:17:26,720
advantage of the libraries that

00:17:23,760 --> 00:17:28,480
we vendors provide for you is really

00:17:26,720 --> 00:17:29,679
critical to getting the best possible

00:17:28,480 --> 00:17:32,400
performance

00:17:29,679 --> 00:17:34,080
and you know i started off by saying you

00:17:32,400 --> 00:17:36,320
we're providing this hbc

00:17:34,080 --> 00:17:38,080
sdk not just a compiler but a full

00:17:36,320 --> 00:17:39,520
software stack because we want to give

00:17:38,080 --> 00:17:41,760
you the best performance and part of

00:17:39,520 --> 00:17:45,039
that is the best performance using

00:17:41,760 --> 00:17:45,520
uh math libraries so blahs ffts any of

00:17:45,039 --> 00:17:49,200
these

00:17:45,520 --> 00:17:51,600
common mathematical approaches

00:17:49,200 --> 00:17:53,679
we've provided libraries for it and i've

00:17:51,600 --> 00:17:56,799
seen people and helped people

00:17:53,679 --> 00:17:58,799
effectively use these with openmp

00:17:56,799 --> 00:18:00,880
so in the code here on the right i've

00:17:58,799 --> 00:18:04,240
i've mapped my data to the device

00:18:00,880 --> 00:18:07,280
i've uh i've initialized it and then i'm

00:18:04,240 --> 00:18:10,160
passing my device data from openmp

00:18:07,280 --> 00:18:11,120
from the runtime into a kubella saxby

00:18:10,160 --> 00:18:14,480
call

00:18:11,120 --> 00:18:17,440
um and getting the results back and so

00:18:14,480 --> 00:18:19,280
um this use device pointer clause is a

00:18:17,440 --> 00:18:21,120
way to get data out of the openmp

00:18:19,280 --> 00:18:22,400
runtime and pass it into something else

00:18:21,120 --> 00:18:24,799
whether it's

00:18:22,400 --> 00:18:25,600
kublas or qft or even native cuda

00:18:24,799 --> 00:18:27,679
kernels

00:18:25,600 --> 00:18:29,600
and so what's really nice about this is

00:18:27,679 --> 00:18:30,480
i can accelerate as much as possible

00:18:29,600 --> 00:18:34,400
with openmp

00:18:30,480 --> 00:18:36,720
and then selectively when when necessary

00:18:34,400 --> 00:18:39,600
drop down into the math libraries or

00:18:36,720 --> 00:18:42,160
cuda for the best possible performance

00:18:39,600 --> 00:18:42,880
now one key thing i'll note here is this

00:18:42,160 --> 00:18:46,000
comment where

00:18:42,880 --> 00:18:47,360
i need to synchronize uh so this call to

00:18:46,000 --> 00:18:50,799
kubla's saxby here

00:18:47,360 --> 00:18:54,400
is in addition to missing a semicolon

00:18:50,799 --> 00:18:56,720
is uh synchronous with the host and so

00:18:54,400 --> 00:18:58,240
um the host will wait for the uh for

00:18:56,720 --> 00:18:59,679
that to complete before moving on to

00:18:58,240 --> 00:19:02,400
this target update but

00:18:59,679 --> 00:19:04,480
in other codes that i have worked with

00:19:02,400 --> 00:19:06,640
they've used non-blocking calls

00:19:04,480 --> 00:19:08,000
and um it has tripped them up and they

00:19:06,640 --> 00:19:09,360
had to insert some device

00:19:08,000 --> 00:19:13,120
synchronization

00:19:09,360 --> 00:19:14,160
openmp 5.1 which is now roughly a month

00:19:13,120 --> 00:19:16,720
old and so there's

00:19:14,160 --> 00:19:18,320
not yet any uh any compilers for it adds

00:19:16,720 --> 00:19:20,559
the interop construct

00:19:18,320 --> 00:19:22,400
and that changes this this game a little

00:19:20,559 --> 00:19:24,320
bit by allowing you to put an

00:19:22,400 --> 00:19:26,080
asynchronous call if this couple of

00:19:24,320 --> 00:19:28,720
saxophone was asynchronous

00:19:26,080 --> 00:19:30,960
into a cuda stream that is shared with

00:19:28,720 --> 00:19:32,480
the openmp runtime and at that point

00:19:30,960 --> 00:19:34,160
uh you can really do some powerful

00:19:32,480 --> 00:19:38,080
things with

00:19:34,160 --> 00:19:39,600
inserting data parallel loops into cuda

00:19:38,080 --> 00:19:43,039
streams inserting

00:19:39,600 --> 00:19:44,559
um accelerated libraries circuit calls

00:19:43,039 --> 00:19:45,600
into the same streams and they got some

00:19:44,559 --> 00:19:46,640
really good performance so

00:19:45,600 --> 00:19:48,720
i'm really looking forward to that

00:19:46,640 --> 00:19:52,240
feature uh coming out because that's

00:19:48,720 --> 00:19:55,840
um has been a challenge in a variety of

00:19:52,240 --> 00:19:55,840
hackathon codes that i've worked with

00:19:56,640 --> 00:20:01,760
and next up i'll mention tasking so

00:19:59,679 --> 00:20:03,280
up until this point we've moved data

00:20:01,760 --> 00:20:05,360
parallel loops to the device

00:20:03,280 --> 00:20:07,760
we've moved our data we've shared our

00:20:05,360 --> 00:20:11,200
data with libraries well

00:20:07,760 --> 00:20:12,640
the the the current hbc uh ecosystem and

00:20:11,200 --> 00:20:14,320
the types of nodes that we're

00:20:12,640 --> 00:20:16,720
putting out there that we're selling

00:20:14,320 --> 00:20:19,280
they're complex there's usually

00:20:16,720 --> 00:20:20,080
there's one or more cpu sockets there's

00:20:19,280 --> 00:20:22,480
one or more

00:20:20,080 --> 00:20:25,120
gpus there's multiple file systems and

00:20:22,480 --> 00:20:29,280
levels of memory they're very complex

00:20:25,120 --> 00:20:30,159
and if we're solely relying on something

00:20:29,280 --> 00:20:32,720
like

00:20:30,159 --> 00:20:34,320
the target directives and nothing else

00:20:32,720 --> 00:20:36,720
some part of that picture

00:20:34,320 --> 00:20:37,919
is going to be idle at any given time if

00:20:36,720 --> 00:20:39,760
you want to get the best possible

00:20:37,919 --> 00:20:41,039
performance if you want to optimize for

00:20:39,760 --> 00:20:43,520
amdahl's law

00:20:41,039 --> 00:20:45,120
and make sure that nothing is sitting

00:20:43,520 --> 00:20:47,440
there serializing

00:20:45,120 --> 00:20:49,280
then you need to use tasking and so the

00:20:47,440 --> 00:20:53,039
openmp target directives are

00:20:49,280 --> 00:20:55,520
in fact uh tasks in the openmp sense and

00:20:53,039 --> 00:20:57,039
interact with openmp tasks and so this

00:20:55,520 --> 00:20:58,159
code on the right demonstrates where i

00:20:57,039 --> 00:21:00,320
can

00:20:58,159 --> 00:21:01,280
i can launch things using the no wait

00:21:00,320 --> 00:21:04,320
clause and

00:21:01,280 --> 00:21:06,320
and expose the dependencies and

00:21:04,320 --> 00:21:08,400
the compiler could choose actually i

00:21:06,320 --> 00:21:10,480
guess the runtime could choose to

00:21:08,400 --> 00:21:12,000
run some parts of this concurrently with

00:21:10,480 --> 00:21:15,600
others so maybe i have

00:21:12,000 --> 00:21:16,000
a data copy of a completes the data copy

00:21:15,600 --> 00:21:18,559
of b

00:21:16,000 --> 00:21:20,159
starts while the initialization of a

00:21:18,559 --> 00:21:22,320
runs and i can have things

00:21:20,159 --> 00:21:24,400
overlapping and really maximize the

00:21:22,320 --> 00:21:26,640
overall throughput of the machine

00:21:24,400 --> 00:21:28,960
now i'll say in practice i ran this

00:21:26,640 --> 00:21:30,880
exact code through a few different

00:21:28,960 --> 00:21:32,640
openmp compilers and none of them got

00:21:30,880 --> 00:21:34,320
the overlapping that i wanted

00:21:32,640 --> 00:21:35,840
but they could and hopefully in the

00:21:34,320 --> 00:21:39,919
future they will and so

00:21:35,840 --> 00:21:42,960
using tasking allows you to to try to

00:21:39,919 --> 00:21:46,000
keep the entire node as busy as possible

00:21:42,960 --> 00:21:47,440
now you know quick note make sure you

00:21:46,000 --> 00:21:49,520
get it working synchronously first

00:21:47,440 --> 00:21:51,120
that's uh that's the challenge if it's

00:21:49,520 --> 00:21:52,720
not giving you correct answers

00:21:51,120 --> 00:21:55,039
synchronously it's not going to give you

00:21:52,720 --> 00:21:57,280
correct answers asynchronously

00:21:55,039 --> 00:21:58,400
and then one bonus one actually not i'm

00:21:57,280 --> 00:22:00,960
not to the bonus one yet

00:21:58,400 --> 00:22:01,919
uh i'll point out the if clause and host

00:22:00,960 --> 00:22:04,159
fallback

00:22:01,919 --> 00:22:06,480
uh so i have seen codes that effectively

00:22:04,159 --> 00:22:07,200
use these this additional if clause here

00:22:06,480 --> 00:22:09,600
to

00:22:07,200 --> 00:22:11,600
instruct the compiler okay uh here's how

00:22:09,600 --> 00:22:13,440
i want you to paralyze my code but if

00:22:11,600 --> 00:22:16,559
i'm not using the gpu

00:22:13,440 --> 00:22:18,640
fall back to the cpu and this if clause

00:22:16,559 --> 00:22:20,559
forced the compiler to build this loop

00:22:18,640 --> 00:22:22,000
nest two different ways one for the cpu

00:22:20,559 --> 00:22:24,080
and over the gpu

00:22:22,000 --> 00:22:26,559
and uh and fall back and get good

00:22:24,080 --> 00:22:29,600
results in both cases because again

00:22:26,559 --> 00:22:31,280
teams and distribute are scalable and

00:22:29,600 --> 00:22:33,520
so you can scale it down to one thread

00:22:31,280 --> 00:22:35,039
team or up to uh to dozens or hundreds

00:22:33,520 --> 00:22:38,640
or thousands of them

00:22:35,039 --> 00:22:40,159
and now for my bonus one um i worked uh

00:22:38,640 --> 00:22:42,720
hard with the with the uh

00:22:40,159 --> 00:22:44,640
openmp committee to to adopt the loop

00:22:42,720 --> 00:22:48,240
directive a few years ago

00:22:44,640 --> 00:22:51,360
and um i believe the nvidia sdk

00:22:48,240 --> 00:22:52,960
hbc sdk will soon ship with uh loop the

00:22:51,360 --> 00:22:54,000
directive support i encourage you to try

00:22:52,960 --> 00:22:56,000
it and give us feedback

00:22:54,000 --> 00:22:58,400
on it because the goal of that directive

00:22:56,000 --> 00:23:00,320
is well to be able to write

00:22:58,400 --> 00:23:01,600
one thing and have it work well

00:23:00,320 --> 00:23:02,480
everywhere so here you can see i

00:23:01,600 --> 00:23:04,240
replaced

00:23:02,480 --> 00:23:05,840
target teams distribute parallel 4 with

00:23:04,240 --> 00:23:07,919
simply target teams loop

00:23:05,840 --> 00:23:10,000
and that asserted to the compiler that

00:23:07,919 --> 00:23:11,520
these loops can be run in

00:23:10,000 --> 00:23:13,679
these loop iterations we run in any

00:23:11,520 --> 00:23:15,679
order including concurrently

00:23:13,679 --> 00:23:17,039
and left a lot of the the scheduling

00:23:15,679 --> 00:23:19,679
decisions to the

00:23:17,039 --> 00:23:20,799
uh to the runtime and this has the

00:23:19,679 --> 00:23:23,120
potential to be something really

00:23:20,799 --> 00:23:23,760
powerful where i could start off with

00:23:23,120 --> 00:23:25,679
something that

00:23:23,760 --> 00:23:27,440
gives the compiler a lot of freedom and

00:23:25,679 --> 00:23:29,120
then i can take back some of that

00:23:27,440 --> 00:23:31,360
control as necessary

00:23:29,120 --> 00:23:32,880
using the uh teams and distribute

00:23:31,360 --> 00:23:33,520
parallel four directives and the meta

00:23:32,880 --> 00:23:36,480
directive

00:23:33,520 --> 00:23:36,799
to uh to uh to zoom in on performance

00:23:36,480 --> 00:23:39,039
but

00:23:36,799 --> 00:23:41,840
this provides a really nice starting

00:23:39,039 --> 00:23:44,559
point for developers

00:23:41,840 --> 00:23:46,320
so here's to to close i think i'm right

00:23:44,559 --> 00:23:47,760
on time the best practices i have for

00:23:46,320 --> 00:23:50,640
openmp on gpus

00:23:47,760 --> 00:23:52,320
i encourage you to to try out our sdk

00:23:50,640 --> 00:23:55,279
the i understand that the

00:23:52,320 --> 00:23:55,679
the open beta of our openmp support is

00:23:55,279 --> 00:23:58,320
uh

00:23:55,679 --> 00:23:59,039
imminent and i encourage you to go uh

00:23:58,320 --> 00:24:01,039
try it out

00:23:59,039 --> 00:24:02,840
as a christmas present to yourself and

00:24:01,039 --> 00:24:06,640
uh with that i'll take any uh

00:24:02,840 --> 00:24:08,799
questions thanks a lot jeff that was uh

00:24:06,640 --> 00:24:10,400
really interesting and great to hear

00:24:08,799 --> 00:24:12,400
uh all the good work nvidia has been

00:24:10,400 --> 00:24:15,039
doing on on supporting openmp

00:24:12,400 --> 00:24:16,400
on their gpus um there's no questions

00:24:15,039 --> 00:24:17,600
come in but i had a couple of questions

00:24:16,400 --> 00:24:19,440
for you

00:24:17,600 --> 00:24:20,880
the first one is about the loop

00:24:19,440 --> 00:24:23,200
directive that you showed

00:24:20,880 --> 00:24:24,640
a couple of slides ago sure um this is

00:24:23,200 --> 00:24:25,360
something that's that's puzzled me a

00:24:24,640 --> 00:24:27,919
little bit

00:24:25,360 --> 00:24:29,840
um when reading through if the loop

00:24:27,919 --> 00:24:32,640
directive allows me to run

00:24:29,840 --> 00:24:34,000
um the iterations the loop in any order

00:24:32,640 --> 00:24:36,480
including concurrency

00:24:34,000 --> 00:24:38,320
do i need to also specify the teams in

00:24:36,480 --> 00:24:39,840
my directive as well why can't i just

00:24:38,320 --> 00:24:41,840
say target loop

00:24:39,840 --> 00:24:43,520
so there has been a fair amount of

00:24:41,840 --> 00:24:46,159
debate on that within the

00:24:43,520 --> 00:24:47,440
committee um i'll speak from the

00:24:46,159 --> 00:24:50,480
perspective of

00:24:47,440 --> 00:24:53,039
why i proposed it as as team's loop was

00:24:50,480 --> 00:24:53,840
to separate the concerns of generating

00:24:53,039 --> 00:24:56,960
the parallelism

00:24:53,840 --> 00:24:59,360
of the thread teams and the uh

00:24:56,960 --> 00:25:00,000
exposing the concurrency of the loop and

00:24:59,360 --> 00:25:02,480
separating those

00:25:00,000 --> 00:25:04,000
two concerns uh there are certainly some

00:25:02,480 --> 00:25:06,880
who are promoting just writing

00:25:04,000 --> 00:25:09,039
omp target loop and um i'm not sure if

00:25:06,880 --> 00:25:12,400
tim knows off the top of his head we may

00:25:09,039 --> 00:25:14,240
support that in our platform as well um

00:25:12,400 --> 00:25:15,760
but if in my perspective from my

00:25:14,240 --> 00:25:17,440
perspective by putting teams there it

00:25:15,760 --> 00:25:20,960
gives me the opportunity to then

00:25:17,440 --> 00:25:21,600
say uh numb teams and adjust the numb

00:25:20,960 --> 00:25:22,799
teams and

00:25:21,600 --> 00:25:25,440
and separate those concerns of

00:25:22,799 --> 00:25:28,240
generating this this many

00:25:25,440 --> 00:25:29,360
uh teams versus um exposing the

00:25:28,240 --> 00:25:32,400
concurrency of the loop

00:25:29,360 --> 00:25:32,400
that that's my reason

00:25:33,760 --> 00:25:37,440
that's great that's that's uh great to

00:25:35,840 --> 00:25:39,840
hear um

00:25:37,440 --> 00:25:42,000
i think another question i had was where

00:25:39,840 --> 00:25:44,480
do you see the next challenges

00:25:42,000 --> 00:25:45,760
for for programming gpus and and maybe

00:25:44,480 --> 00:25:48,000
with an openmp slant

00:25:45,760 --> 00:25:49,600
uh your answer would be welcome at this

00:25:48,000 --> 00:25:52,240
event

00:25:49,600 --> 00:25:52,960
so to me i i think that the biggest

00:25:52,240 --> 00:25:54,880
challenges

00:25:52,960 --> 00:25:56,000
that that i've have seen here in the

00:25:54,880 --> 00:25:58,480
last let's say

00:25:56,000 --> 00:25:59,360
you know two years or so revolve around

00:25:58,480 --> 00:26:01,200
tasking

00:25:59,360 --> 00:26:04,880
um there's and i'll go even go back to

00:26:01,200 --> 00:26:04,880
the the tasking slide there is that

00:26:05,120 --> 00:26:10,320
when i started gpu programming it was

00:26:07,279 --> 00:26:13,360
you know one small cpu and one small gpu

00:26:10,320 --> 00:26:16,960
and that was it and now you know we sell

00:26:13,360 --> 00:26:18,640
uh you know nodes with 16 gpus and two

00:26:16,960 --> 00:26:21,679
cpus and i don't even remember how many

00:26:18,640 --> 00:26:24,880
cpu cores are on those cpus and it's

00:26:21,679 --> 00:26:27,200
uh it's really complex and so both

00:26:24,880 --> 00:26:29,440
within cuda and within openmp

00:26:27,200 --> 00:26:31,039
we're you know we've worked on a tasking

00:26:29,440 --> 00:26:33,679
subsystem so that

00:26:31,039 --> 00:26:34,400
uh you can try to keep everything busy

00:26:33,679 --> 00:26:37,440
because

00:26:34,400 --> 00:26:39,039
um at you know we have the the network

00:26:37,440 --> 00:26:39,919
the interconnect we we're interconnect

00:26:39,039 --> 00:26:42,159
company as well

00:26:39,919 --> 00:26:44,159
we have the the connection between the

00:26:42,159 --> 00:26:46,000
gpus the nv link we've got the cpus

00:26:44,159 --> 00:26:48,559
we've got the disks we've got

00:26:46,000 --> 00:26:50,000
all of the gpu parts and you know what

00:26:48,559 --> 00:26:51,520
these gps are really wide these days

00:26:50,000 --> 00:26:52,559
they take a lot of parallelism sometimes

00:26:51,520 --> 00:26:54,720
the loops aren't even

00:26:52,559 --> 00:26:55,760
big enough to fill the gpu by themselves

00:26:54,720 --> 00:26:58,159
so uh

00:26:55,760 --> 00:26:58,799
exposing the the task-based parallelism

00:26:58,159 --> 00:27:00,240
i think is

00:26:58,799 --> 00:27:02,159
is kind of the big challenge and one

00:27:00,240 --> 00:27:03,679
thing that i um

00:27:02,159 --> 00:27:05,600
i've heard a little bit discussed within

00:27:03,679 --> 00:27:06,880
the the language committee although um i

00:27:05,600 --> 00:27:08,559
haven't been as involved in language

00:27:06,880 --> 00:27:10,960
committee over the past few months as

00:27:08,559 --> 00:27:12,880
i have in the past few years but one

00:27:10,960 --> 00:27:16,559
area is

00:27:12,880 --> 00:27:16,799
binding tasks to particular gpus i think

00:27:16,559 --> 00:27:18,720
is

00:27:16,799 --> 00:27:21,200
potentially an interesting topic where i

00:27:18,720 --> 00:27:23,200
may set up an entire pipeline of

00:27:21,200 --> 00:27:25,679
read data off of disk pre-process the

00:27:23,200 --> 00:27:27,919
data send it to the gpu

00:27:25,679 --> 00:27:28,960
send it back post process and have the

00:27:27,919 --> 00:27:31,760
entire pipeline

00:27:28,960 --> 00:27:33,760
be have affinity to a particular gpu

00:27:31,760 --> 00:27:34,960
rather than simply to a particular core

00:27:33,760 --> 00:27:37,120
i think is a

00:27:34,960 --> 00:27:38,399
potentially interesting area to to look

00:27:37,120 --> 00:27:39,760
at and um

00:27:38,399 --> 00:27:42,080
so yeah i think i think many of the

00:27:39,760 --> 00:27:44,000
challenges revolve around uh tasking and

00:27:42,080 --> 00:27:45,200
i've seen a lot more interest in tasking

00:27:44,000 --> 00:27:49,039
both from

00:27:45,200 --> 00:27:51,039
uh both from openmp and from cuda

00:27:49,039 --> 00:27:52,080
great thanks jeff um there's a couple of

00:27:51,039 --> 00:27:54,320
questions coming from

00:27:52,080 --> 00:27:55,200
from those dialing in um so the first

00:27:54,320 --> 00:27:57,440
from

00:27:55,200 --> 00:27:58,960
from jamie quinn says he's asking about

00:27:57,440 --> 00:28:01,039
the loop directive again

00:27:58,960 --> 00:28:03,039
and says does it suffer from the same

00:28:01,039 --> 00:28:05,840
data locality issues as collapse

00:28:03,039 --> 00:28:07,360
and does it work well with the the tile

00:28:05,840 --> 00:28:08,240
that has been added in in the newer

00:28:07,360 --> 00:28:11,520
versions of

00:28:08,240 --> 00:28:15,039
openmp up so

00:28:11,520 --> 00:28:16,000
the way that we um defined the loop

00:28:15,039 --> 00:28:18,799
directive or at least

00:28:16,000 --> 00:28:20,799
the the goal we had initially is that um

00:28:18,799 --> 00:28:22,080
this collapse is not necessarily a

00:28:20,799 --> 00:28:25,600
mechanical collapse

00:28:22,080 --> 00:28:26,480
it's uh it is instead we actually almost

00:28:25,600 --> 00:28:29,279
use a different

00:28:26,480 --> 00:28:30,080
uh clause name because we wanted to say

00:28:29,279 --> 00:28:33,200
that the

00:28:30,080 --> 00:28:35,520
the guarantees of loop um

00:28:33,200 --> 00:28:36,640
are applied to both of these the j and

00:28:35,520 --> 00:28:38,720
the i loop so with

00:28:36,640 --> 00:28:39,840
uh a mechanical collapse may not be

00:28:38,720 --> 00:28:43,039
required that perhaps

00:28:39,840 --> 00:28:45,679
by exposing the uh the

00:28:43,039 --> 00:28:46,480
um the independence of the loop

00:28:45,679 --> 00:28:47,919
iterations

00:28:46,480 --> 00:28:49,840
you as a compiler may choose to

00:28:47,919 --> 00:28:52,960
mechanically collapse or tile

00:28:49,840 --> 00:28:54,480
or uh interchange or or whatever else so

00:28:52,960 --> 00:28:56,240
that was our goal was to give the

00:28:54,480 --> 00:28:57,760
compiler the flexibility to make those

00:28:56,240 --> 00:28:59,600
decisions so hopefully no

00:28:57,760 --> 00:29:01,360
it doesn't have that same software but

00:28:59,600 --> 00:29:02,799
it'll depend a lot on implementation i

00:29:01,360 --> 00:29:05,279
suspect that

00:29:02,799 --> 00:29:06,320
all of the initial implementations of

00:29:05,279 --> 00:29:08,480
the the loop

00:29:06,320 --> 00:29:10,159
directive will probably treat this

00:29:08,480 --> 00:29:11,520
collapse like a traditional mechanical

00:29:10,159 --> 00:29:14,799
collapse

00:29:11,520 --> 00:29:16,080
but hopefully as those uh as they mature

00:29:14,799 --> 00:29:18,080
that that it will become more of a

00:29:16,080 --> 00:29:20,159
descriptive uh collapse

00:29:18,080 --> 00:29:22,159
and i forget what the second part of

00:29:20,159 --> 00:29:24,880
that was oh how it interacts with tile

00:29:22,159 --> 00:29:25,919
um our goal was for it to be able to

00:29:24,880 --> 00:29:28,480
interact with uh

00:29:25,919 --> 00:29:30,480
with tile correctly and so that you can

00:29:28,480 --> 00:29:32,960
uh tie all these and declare the data

00:29:30,480 --> 00:29:34,799
independence of those loops

00:29:32,960 --> 00:29:36,159
that's brilliant thanks jeff there's one

00:29:34,799 --> 00:29:37,760
more question but i think we'll take

00:29:36,159 --> 00:29:39,279
that one offline just to give everyone

00:29:37,760 --> 00:29:41,279
a couple of minutes as we've gone into

00:29:39,279 --> 00:29:42,880
the break but thanks very much for your

00:29:41,279 --> 00:29:47,120
time and great presentation

00:29:42,880 --> 00:29:47,120

YouTube URL: https://www.youtube.com/watch?v=9w_2tj2uD4M


