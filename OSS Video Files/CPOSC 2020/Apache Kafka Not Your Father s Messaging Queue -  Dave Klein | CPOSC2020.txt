Title: Apache Kafka Not Your Father s Messaging Queue -  Dave Klein | CPOSC2020
Publication date: 2021-01-18
Playlist: CPOSC 2020 Recorded Sessions
Description: 
	Session Page: https://cposc.org/sessions/apache-kafka-not-fathers-message-queue/
Chat Log: https://cposc.org/chat-log-apache-kafka-not-fathers-message-queue/

The Central Pennsylvania Open Source Conference (CPOSC) is a one-day technical conference for open source users and developers. Presentations and sessions cover all aspects of open source, with talks ranging from novice to expert skill levels and featuring case studies, best practices, code-alongs and more. Join us in 2021! https://cposc.org
Captions: 
	00:00:05,200 --> 00:00:07,600
okay so

00:00:05,920 --> 00:00:09,679
for this current talk let me introduce

00:00:07,600 --> 00:00:11,759
david klein or dave klein

00:00:09,679 --> 00:00:14,000
um dave's a software developer he's an

00:00:11,759 --> 00:00:15,599
author a trainer a mentor an explorer

00:00:14,000 --> 00:00:17,760
been finding ways to solve problems with

00:00:15,599 --> 00:00:20,000
software for 25 years

00:00:17,760 --> 00:00:20,800
and having a great time doing it dave

00:00:20,000 --> 00:00:22,960
has spoken at

00:00:20,800 --> 00:00:23,840
organized tech conferences and user

00:00:22,960 --> 00:00:26,320
groups so

00:00:23,840 --> 00:00:26,960
i'd like you to uh give a big welcome to

00:00:26,320 --> 00:00:30,080
dave

00:00:26,960 --> 00:00:31,279
and uh off you go dave all right well

00:00:30,080 --> 00:00:34,160
thank you jay

00:00:31,279 --> 00:00:35,200
um i'm thrilled to almost be here i

00:00:34,160 --> 00:00:36,399
would have i wish we could have been

00:00:35,200 --> 00:00:38,719
there in person in

00:00:36,399 --> 00:00:39,760
beautiful central pennsylvania or maybe

00:00:38,719 --> 00:00:43,440
maybe next year

00:00:39,760 --> 00:00:45,120
but um welcome to this talk um

00:00:43,440 --> 00:00:46,559
it's the chocolate apache popcorn not

00:00:45,120 --> 00:00:46,800
your father's message cube now i don't

00:00:46,559 --> 00:00:48,559
know

00:00:46,800 --> 00:00:49,840
how many people are familiar with that

00:00:48,559 --> 00:00:52,480
phrase it's from an old

00:00:49,840 --> 00:00:53,199
car commercial years ago um but i just i

00:00:52,480 --> 00:00:54,640
love that

00:00:53,199 --> 00:00:56,559
the concept behind it there are things

00:00:54,640 --> 00:00:58,480
that we technologies that come along

00:00:56,559 --> 00:01:01,840
that we have an impression of

00:00:58,480 --> 00:01:03,280
and and often that impression is is

00:01:01,840 --> 00:01:05,119
not quite accurate and that's the case

00:01:03,280 --> 00:01:07,439
here um when i

00:01:05,119 --> 00:01:08,320
before i started working with kafka um

00:01:07,439 --> 00:01:09,680
well

00:01:08,320 --> 00:01:11,439
back up a little bit brief introduction

00:01:09,680 --> 00:01:12,960
myself uh as i

00:01:11,439 --> 00:01:14,799
said my name is dave klein i've been

00:01:12,960 --> 00:01:15,759
developer actually so almost 30 years 28

00:01:14,799 --> 00:01:17,920
years i've been doing

00:01:15,759 --> 00:01:19,200
in software development in various roles

00:01:17,920 --> 00:01:21,119
um

00:01:19,200 --> 00:01:22,400
and i just i've always loved technology

00:01:21,119 --> 00:01:24,640
love learning new things

00:01:22,400 --> 00:01:26,640
and i've also always loved the community

00:01:24,640 --> 00:01:28,720
around technologies

00:01:26,640 --> 00:01:30,880
years ago i was involved in another

00:01:28,720 --> 00:01:31,360
really at the time vibrant community

00:01:30,880 --> 00:01:33,759
with the

00:01:31,360 --> 00:01:35,680
groovy and grails community which i

00:01:33,759 --> 00:01:37,200
really enjoyed and

00:01:35,680 --> 00:01:40,320
10 years ago or so wrote a book on

00:01:37,200 --> 00:01:42,479
grails and that was a great experience

00:01:40,320 --> 00:01:44,079
and after that it kind of started fading

00:01:42,479 --> 00:01:45,439
away that community kind of died out and

00:01:44,079 --> 00:01:47,040
i i've missed that

00:01:45,439 --> 00:01:49,360
and and i hadn't really seen it again

00:01:47,040 --> 00:01:51,200
until until i got hooked up with

00:01:49,360 --> 00:01:53,040
the apache coffee community and it's

00:01:51,200 --> 00:01:54,720
very similar in a lot of ways as far as

00:01:53,040 --> 00:01:57,040
just the

00:01:54,720 --> 00:01:58,079
the welcoming attitude the desire to

00:01:57,040 --> 00:02:00,479
learn and explore

00:01:58,079 --> 00:02:01,520
and just the excitement of what's on the

00:02:00,479 --> 00:02:03,680
horizon

00:02:01,520 --> 00:02:04,640
so before i started working with apache

00:02:03,680 --> 00:02:06,960
kafka though

00:02:04,640 --> 00:02:08,000
um i had heard of it and i lumped it

00:02:06,960 --> 00:02:09,679
with other messaging

00:02:08,000 --> 00:02:11,440
services right you know active and key

00:02:09,679 --> 00:02:12,640
rabbit into those kinds of things

00:02:11,440 --> 00:02:14,720
and i didn't really know much about what

00:02:12,640 --> 00:02:16,480
it was until i started using it on a

00:02:14,720 --> 00:02:18,400
recent job that i got

00:02:16,480 --> 00:02:20,000
and as i learned more about it i

00:02:18,400 --> 00:02:20,840
realized that it was so much more than

00:02:20,000 --> 00:02:23,200
that

00:02:20,840 --> 00:02:24,000
um and that was gave me the idea for

00:02:23,200 --> 00:02:26,160
this talk it's

00:02:24,000 --> 00:02:28,080
just a way to introduce cockpit to those

00:02:26,160 --> 00:02:29,599
who may not be familiar with it

00:02:28,080 --> 00:02:33,120
and may have heard of it like like i

00:02:29,599 --> 00:02:33,120
said just as a messaging service

00:02:33,360 --> 00:02:36,400
so let's see if i can move my slides

00:02:35,599 --> 00:02:38,640
around

00:02:36,400 --> 00:02:38,640
nope

00:02:40,640 --> 00:02:45,200
all right so um

00:02:43,920 --> 00:02:46,720
there's another old phrase one of these

00:02:45,200 --> 00:02:48,080
things is not like the other i don't

00:02:46,720 --> 00:02:51,760
know if anything familiar with

00:02:48,080 --> 00:02:55,280
that from sesame street um it's a but

00:02:51,760 --> 00:02:56,879
just a a pattern um

00:02:55,280 --> 00:02:58,319
building kind of exercise that cesar she

00:02:56,879 --> 00:02:58,720
used to do and show different objects

00:02:58,319 --> 00:02:59,760
and

00:02:58,720 --> 00:03:01,760
you have to figure out which ones were

00:02:59,760 --> 00:03:03,599
different and so these are logos from

00:03:01,760 --> 00:03:04,000
various messaging services that people

00:03:03,599 --> 00:03:05,680
might have

00:03:04,000 --> 00:03:07,360
lumped in together and in fact they

00:03:05,680 --> 00:03:09,360
often are still lumped together

00:03:07,360 --> 00:03:10,480
some software frameworks that provide

00:03:09,360 --> 00:03:13,120
interfaces to

00:03:10,480 --> 00:03:14,560
messaging services you can plug in your

00:03:13,120 --> 00:03:16,720
various technology

00:03:14,560 --> 00:03:18,159
into their interfaces and kafka is just

00:03:16,720 --> 00:03:20,080
one of the

00:03:18,159 --> 00:03:21,920
many that they can plug into their

00:03:20,080 --> 00:03:23,840
messaging interfaces

00:03:21,920 --> 00:03:25,440
and and you can use it that way and it

00:03:23,840 --> 00:03:27,120
will somewhat work

00:03:25,440 --> 00:03:28,560
um but you're just missing out on a lot

00:03:27,120 --> 00:03:31,599
of of

00:03:28,560 --> 00:03:33,040
the power of kafka in fact in general i

00:03:31,599 --> 00:03:34,640
don't really like those kinds of

00:03:33,040 --> 00:03:36,080
tools that try to give you the least

00:03:34,640 --> 00:03:37,360
common denominator over several

00:03:36,080 --> 00:03:39,200
different technologies

00:03:37,360 --> 00:03:40,879
um each different technology has its own

00:03:39,200 --> 00:03:41,920
strengths and i think you take advantage

00:03:40,879 --> 00:03:42,959
of those strengths and you work with

00:03:41,920 --> 00:03:45,440
them directly

00:03:42,959 --> 00:03:46,799
but that's a side story maybe you know

00:03:45,440 --> 00:03:50,480
the top down the road

00:03:46,799 --> 00:03:51,760
um but like i said kafka is not just a

00:03:50,480 --> 00:03:53,040
messaging service in fact it's not a

00:03:51,760 --> 00:03:55,040
messenger service at all

00:03:53,040 --> 00:03:56,640
uh kafka was invented at linkedin and

00:03:55,040 --> 00:03:59,760
the team that invented it

00:03:56,640 --> 00:04:02,480
they created it they did it because they

00:03:59,760 --> 00:04:03,920
were trying various messaging services

00:04:02,480 --> 00:04:06,080
and messaging software to meet their

00:04:03,920 --> 00:04:08,799
needs and and it wasn't working

00:04:06,080 --> 00:04:10,319
so they created kafka which filled the

00:04:08,799 --> 00:04:13,120
gap that they had and

00:04:10,319 --> 00:04:14,319
which took off from there became very

00:04:13,120 --> 00:04:16,239
popular widely used

00:04:14,319 --> 00:04:17,919
in linkedin and then it was contributed

00:04:16,239 --> 00:04:20,400
to the apache foundation

00:04:17,919 --> 00:04:22,079
and just kept on growing from there i

00:04:20,400 --> 00:04:24,320
think at the end i got a slide showing

00:04:22,079 --> 00:04:25,680
on the apache website that shows the

00:04:24,320 --> 00:04:27,440
adoption rates

00:04:25,680 --> 00:04:28,960
or the current adoption of kafka and

00:04:27,440 --> 00:04:31,520
it's it's pretty impressive

00:04:28,960 --> 00:04:32,160
it's just taking off and it's a very

00:04:31,520 --> 00:04:34,400
powerful

00:04:32,160 --> 00:04:35,360
um technology but it's not a messaging

00:04:34,400 --> 00:04:39,120
service

00:04:35,360 --> 00:04:40,880
so what is it so yes kind of

00:04:39,120 --> 00:04:42,800
structurally it is a publish a subscribe

00:04:40,880 --> 00:04:45,840
framework and

00:04:42,800 --> 00:04:49,040
many companies use it for that use case

00:04:45,840 --> 00:04:52,639
and it works really well um

00:04:49,040 --> 00:04:53,759
but that's that's selling it short i

00:04:52,639 --> 00:04:54,639
mean there's so much more that you can

00:04:53,759 --> 00:04:57,120
do with it than just

00:04:54,639 --> 00:04:58,240
than just publishing and subscribing um

00:04:57,120 --> 00:05:00,000
you can be sending

00:04:58,240 --> 00:05:01,440
you know messages and receiving them i

00:05:00,000 --> 00:05:02,240
mean but you can do a lot more with it

00:05:01,440 --> 00:05:04,560
than just that

00:05:02,240 --> 00:05:06,720
and a lot of it comes down to the way

00:05:04,560 --> 00:05:08,560
that you view it and the architecture

00:05:06,720 --> 00:05:10,320
so i think that the real strength of

00:05:08,560 --> 00:05:13,600
apache kafka comes

00:05:10,320 --> 00:05:15,919
when you adopt it as as the heart of an

00:05:13,600 --> 00:05:18,320
event streaming platform

00:05:15,919 --> 00:05:20,800
and you can use the various components

00:05:18,320 --> 00:05:23,199
in the kafka ecosystem to build out

00:05:20,800 --> 00:05:25,039
some really impressive applications and

00:05:23,199 --> 00:05:27,120
enterprise-wide architectures

00:05:25,039 --> 00:05:28,560
now this is a simple diagram with some

00:05:27,120 --> 00:05:30,800
of these and i'll talk more about this

00:05:28,560 --> 00:05:31,840
idea these ideas a little bit later on

00:05:30,800 --> 00:05:34,720
but first i wanted to

00:05:31,840 --> 00:05:36,479
focus in on on the concept of an event

00:05:34,720 --> 00:05:38,000
because that really is a key to taking

00:05:36,479 --> 00:05:41,360
advantage of kafka

00:05:38,000 --> 00:05:42,400
is to recognize the power of the concept

00:05:41,360 --> 00:05:45,840
of an event

00:05:42,400 --> 00:05:47,600
now event contains notification and

00:05:45,840 --> 00:05:49,440
state

00:05:47,600 --> 00:05:51,280
so a notification tells you something

00:05:49,440 --> 00:05:51,840
happened and then the state gives you

00:05:51,280 --> 00:05:54,479
information

00:05:51,840 --> 00:05:55,840
about what happened now every event that

00:05:54,479 --> 00:05:57,199
comes through kafka will also have a

00:05:55,840 --> 00:05:59,919
time stamp so you'll know when

00:05:57,199 --> 00:06:01,280
things happen as well um but these two

00:05:59,919 --> 00:06:04,800
concepts here notification

00:06:01,280 --> 00:06:07,520
state combining those you can just do

00:06:04,800 --> 00:06:08,479
so much um the simple examples i have

00:06:07,520 --> 00:06:12,639
here right so

00:06:08,479 --> 00:06:14,960
an event might be something that um

00:06:12,639 --> 00:06:16,240
that happened that requires some further

00:06:14,960 --> 00:06:17,680
action like an order is placed

00:06:16,240 --> 00:06:19,039
and so another service will pick up that

00:06:17,680 --> 00:06:20,800
event and look at the information about

00:06:19,039 --> 00:06:23,120
the order and do something with it

00:06:20,800 --> 00:06:24,639
other events might just be historical

00:06:23,120 --> 00:06:27,840
like system logs

00:06:24,639 --> 00:06:29,440
or some things might be um

00:06:27,840 --> 00:06:30,800
you know have multiple purposes like the

00:06:29,440 --> 00:06:31,759
temperature reading right one

00:06:30,800 --> 00:06:33,120
application might pick up the

00:06:31,759 --> 00:06:34,400
temperature current temperature reading

00:06:33,120 --> 00:06:36,160
and just display it on the sign that you

00:06:34,400 --> 00:06:37,759
see out from the banks

00:06:36,160 --> 00:06:39,280
another service might use that pick up

00:06:37,759 --> 00:06:41,440
the temperature reading and use it to

00:06:39,280 --> 00:06:44,160
decide whether to adjust the thermostat

00:06:41,440 --> 00:06:46,720
or the heater and turn it on or off

00:06:44,160 --> 00:06:47,840
that's silly examples kind of but but

00:06:46,720 --> 00:06:50,400
the idea is that

00:06:47,840 --> 00:06:52,639
that one event could have multiple uses

00:06:50,400 --> 00:06:55,039
um

00:06:52,639 --> 00:06:57,120
and if you think about it too um an

00:06:55,039 --> 00:06:59,759
event a council can be used to carry

00:06:57,120 --> 00:07:01,280
um a command so it might be like the

00:06:59,759 --> 00:07:03,199
order place you look at as a command

00:07:01,280 --> 00:07:05,599
it's something to do now

00:07:03,199 --> 00:07:07,120
but even a command or a request is an

00:07:05,599 --> 00:07:08,639
event and so when something

00:07:07,120 --> 00:07:10,319
some other system or individual makes a

00:07:08,639 --> 00:07:12,560
request that's an event and

00:07:10,319 --> 00:07:13,599
by modeling that as an event and

00:07:12,560 --> 00:07:15,280
thinking in that

00:07:13,599 --> 00:07:17,360
that paradigm it really just opens up a

00:07:15,280 --> 00:07:20,479
lot of doors so

00:07:17,360 --> 00:07:22,160
um and actually all data pretty much all

00:07:20,479 --> 00:07:24,960
data that we have in our applications

00:07:22,160 --> 00:07:27,120
enterprise-wide they all it all

00:07:24,960 --> 00:07:29,360
originated as an event

00:07:27,120 --> 00:07:31,919
a database you can look at as a static

00:07:29,360 --> 00:07:33,680
view of the events that have happened

00:07:31,919 --> 00:07:35,039
but it's it's kind of like a dead end

00:07:33,680 --> 00:07:36,479
view right it's just it's just all you

00:07:35,039 --> 00:07:37,599
have is the current state you don't know

00:07:36,479 --> 00:07:39,280
what led to that

00:07:37,599 --> 00:07:41,120
the events and the history of those

00:07:39,280 --> 00:07:42,000
events tells you how you got to where

00:07:41,120 --> 00:07:45,360
you are

00:07:42,000 --> 00:07:46,639
and so by having the having those events

00:07:45,360 --> 00:07:48,400
in an event stream

00:07:46,639 --> 00:07:50,479
you just have so much more options you

00:07:48,400 --> 00:07:52,000
can replay events you can make a change

00:07:50,479 --> 00:07:53,120
to application

00:07:52,000 --> 00:07:54,400
there's just a lot of things you can do

00:07:53,120 --> 00:07:56,400
with it so it's just i've said that

00:07:54,400 --> 00:07:57,599
terminology matters i think and so

00:07:56,400 --> 00:07:59,599
you'll often hear and i'll probably even

00:07:57,599 --> 00:08:01,039
use the terms of record or message

00:07:59,599 --> 00:08:02,960
when speaking out kafka and those are

00:08:01,039 --> 00:08:03,759
valid terms for what how you work with

00:08:02,960 --> 00:08:05,520
kafka

00:08:03,759 --> 00:08:07,080
but i think keeping in mind that concept

00:08:05,520 --> 00:08:09,440
of an event is really important

00:08:07,080 --> 00:08:10,879
it's a logical construct it's not

00:08:09,440 --> 00:08:13,919
necessarily a physical thing

00:08:10,879 --> 00:08:15,520
but it's a very powerful tool and it

00:08:13,919 --> 00:08:16,720
helps us to get the most out of kafka i

00:08:15,520 --> 00:08:18,400
think

00:08:16,720 --> 00:08:19,840
so another thing is how kaka stores

00:08:18,400 --> 00:08:20,400
these events and receives and stores

00:08:19,840 --> 00:08:23,599
them

00:08:20,400 --> 00:08:25,759
and that's another really a very simple

00:08:23,599 --> 00:08:28,080
deceptively simple concept it's just a

00:08:25,759 --> 00:08:31,120
log right

00:08:28,080 --> 00:08:33,680
at the heart of kafka is a a

00:08:31,120 --> 00:08:34,560
log it's a pen only immutable log of

00:08:33,680 --> 00:08:37,519
events

00:08:34,560 --> 00:08:38,320
um now we're showing here's a single log

00:08:37,519 --> 00:08:40,320
but most

00:08:38,320 --> 00:08:43,200
and we in copper you'll call this log a

00:08:40,320 --> 00:08:44,159
topic and most topics in kafka will have

00:08:43,200 --> 00:08:46,480
multiple logs though

00:08:44,159 --> 00:08:48,320
called partitions and that's just first

00:08:46,480 --> 00:08:50,800
for scale

00:08:48,320 --> 00:08:52,399
and for you know increased throughput

00:08:50,800 --> 00:08:55,600
you have multiple partitions

00:08:52,399 --> 00:08:58,480
but they all each each partition is

00:08:55,600 --> 00:08:59,600
an append only immutable log so once

00:08:58,480 --> 00:09:01,519
something goes onto the log

00:08:59,600 --> 00:09:02,880
it doesn't you can't change it and you

00:09:01,519 --> 00:09:04,640
can't insert it so

00:09:02,880 --> 00:09:06,240
well excuse me you can't insert

00:09:04,640 --> 00:09:07,120
something in between two different

00:09:06,240 --> 00:09:09,519
events that are in

00:09:07,120 --> 00:09:10,640
in the log they always go on in order

00:09:09,519 --> 00:09:12,160
and they always stay in the order the

00:09:10,640 --> 00:09:14,080
position that an event lands

00:09:12,160 --> 00:09:16,000
is called its offset and that stays

00:09:14,080 --> 00:09:20,480
there permanently

00:09:16,000 --> 00:09:24,080
um now speaking of permanently

00:09:20,480 --> 00:09:25,839
a kafka topic can hold

00:09:24,080 --> 00:09:27,680
events permanently if you if you choose

00:09:25,839 --> 00:09:30,320
obviously space becomes a concern

00:09:27,680 --> 00:09:30,800
uh recently um tear storage is added and

00:09:30,320 --> 00:09:32,320
some

00:09:30,800 --> 00:09:33,839
flavors of coffee on the circuits are

00:09:32,320 --> 00:09:35,680
reached to the open source

00:09:33,839 --> 00:09:37,680
uh part yet i think it's on its way

00:09:35,680 --> 00:09:39,360
though um

00:09:37,680 --> 00:09:41,600
but with that you can more more

00:09:39,360 --> 00:09:44,800
affordably keep data into in

00:09:41,600 --> 00:09:46,560
kafka topics forever with um older data

00:09:44,800 --> 00:09:48,160
being moved off to an object store like

00:09:46,560 --> 00:09:49,680
s3 or something like that and the more

00:09:48,160 --> 00:09:52,160
active data being on your

00:09:49,680 --> 00:09:53,680
your ssds and ready to go but you still

00:09:52,160 --> 00:09:55,120
work with it all the same way

00:09:53,680 --> 00:09:56,800
it doesn't change from the user

00:09:55,120 --> 00:09:59,680
perspective other than obviously

00:09:56,800 --> 00:10:01,760
um latency as you start trying to read

00:09:59,680 --> 00:10:04,640
the older messages that are stored

00:10:01,760 --> 00:10:06,399
in different storage um but the time

00:10:04,640 --> 00:10:09,120
that that

00:10:06,399 --> 00:10:09,760
an event stays in a cockpit topic is is

00:10:09,120 --> 00:10:12,000
up to

00:10:09,760 --> 00:10:13,360
you you can configure it at the topic

00:10:12,000 --> 00:10:16,240
level you can set a

00:10:13,360 --> 00:10:16,959
time like in hours or days or weeks or

00:10:16,240 --> 00:10:19,120
years

00:10:16,959 --> 00:10:20,640
or you can set a size of it so you want

00:10:19,120 --> 00:10:20,959
this topic to grow over a certain number

00:10:20,640 --> 00:10:23,040
of

00:10:20,959 --> 00:10:24,800
megabytes or gigabytes whatever you

00:10:23,040 --> 00:10:26,240
choose and then

00:10:24,800 --> 00:10:28,000
the ones that pass the threshold will

00:10:26,240 --> 00:10:30,240
gradually be dropped off

00:10:28,000 --> 00:10:32,320
but but other than that it's basically a

00:10:30,240 --> 00:10:36,720
permanent immutable

00:10:32,320 --> 00:10:38,399
log of events and this construct is

00:10:36,720 --> 00:10:40,800
actually very simple but very powerful

00:10:38,399 --> 00:10:43,600
in that it it allows you to

00:10:40,800 --> 00:10:44,959
to work with that data in in very

00:10:43,600 --> 00:10:47,040
predictable ways

00:10:44,959 --> 00:10:48,160
so one application can read messages

00:10:47,040 --> 00:10:49,839
from the topic

00:10:48,160 --> 00:10:50,720
and another application come along read

00:10:49,839 --> 00:10:52,640
the same messages and they're getting

00:10:50,720 --> 00:10:53,920
the same messages in the same order

00:10:52,640 --> 00:10:56,880
same message you can see the same events

00:10:53,920 --> 00:10:58,399
in the same order so it

00:10:56,880 --> 00:11:01,839
allows you to build some very very

00:10:58,399 --> 00:11:01,839
powerful applications

00:11:02,720 --> 00:11:05,920
so let's look a little bit more in depth

00:11:04,720 --> 00:11:08,720
at what what the events

00:11:05,920 --> 00:11:09,920
are how they're built so in copper the

00:11:08,720 --> 00:11:12,640
two main components

00:11:09,920 --> 00:11:13,680
of an event are key and the value

00:11:12,640 --> 00:11:14,640
there's also like i mentioned a

00:11:13,680 --> 00:11:16,480
timestamp

00:11:14,640 --> 00:11:18,240
and there's also metadata that can be

00:11:16,480 --> 00:11:20,079
added um

00:11:18,240 --> 00:11:21,760
so there's there'll be things like the

00:11:20,079 --> 00:11:22,880
um

00:11:21,760 --> 00:11:24,320
i'm not sure exactly where things come

00:11:22,880 --> 00:11:25,920
in a minute if it's not top of my head

00:11:24,320 --> 00:11:27,360
but you can also add i know you can add

00:11:25,920 --> 00:11:29,120
to the metadata some metadata

00:11:27,360 --> 00:11:30,720
is like basically a hash map that you

00:11:29,120 --> 00:11:32,880
can add extra information to

00:11:30,720 --> 00:11:35,120
each event but the main things in it are

00:11:32,880 --> 00:11:38,160
the key and the value

00:11:35,120 --> 00:11:38,880
you know they can be anything um that

00:11:38,160 --> 00:11:40,800
just

00:11:38,880 --> 00:11:42,640
on on the topic once it's written it's

00:11:40,800 --> 00:11:45,760
just fights right everything is just

00:11:42,640 --> 00:11:47,839
it's just converted to bytes um

00:11:45,760 --> 00:11:49,360
when it's on on the written to the log

00:11:47,839 --> 00:11:52,320
so um

00:11:49,360 --> 00:11:53,680
but and you can use you know any kind of

00:11:52,320 --> 00:11:55,279
value for both the key and the object it

00:11:53,680 --> 00:11:57,839
can be complex values

00:11:55,279 --> 00:11:59,040
objects anything you want but more often

00:11:57,839 --> 00:11:59,519
you're going to see something like in

00:11:59,040 --> 00:12:01,519
this

00:11:59,519 --> 00:12:03,120
image up here now where the key is a

00:12:01,519 --> 00:12:05,440
simple data type a string or maybe

00:12:03,120 --> 00:12:09,440
perhaps an integer

00:12:05,440 --> 00:12:12,240
something just to use to keep track of

00:12:09,440 --> 00:12:13,920
what that value represents and then the

00:12:12,240 --> 00:12:16,800
value is usually a more complex object

00:12:13,920 --> 00:12:18,160
like a json object or avro or protobuf

00:12:16,800 --> 00:12:19,360
those are the most common formats but

00:12:18,160 --> 00:12:20,720
again it can be anything you want as

00:12:19,360 --> 00:12:23,519
long as you can serialize and

00:12:20,720 --> 00:12:25,839
sterilize it you can use any type of

00:12:23,519 --> 00:12:25,839
value

00:12:26,000 --> 00:12:29,680
so in this example i said the key is is

00:12:28,160 --> 00:12:31,120
just a simple string

00:12:29,680 --> 00:12:34,320
now the key is actually optional the

00:12:31,120 --> 00:12:36,480
value is the main thing in an event

00:12:34,320 --> 00:12:38,639
it's the information about what happened

00:12:36,480 --> 00:12:40,240
and the thing that happened can be

00:12:38,639 --> 00:12:43,279
determined based on the topic

00:12:40,240 --> 00:12:45,760
that's written to often topics are tied

00:12:43,279 --> 00:12:47,600
to the type of an event that's happening

00:12:45,760 --> 00:12:49,200
or you could also use like you have an

00:12:47,600 --> 00:12:51,680
example here an event type

00:12:49,200 --> 00:12:53,040
and that's just another architectural

00:12:51,680 --> 00:12:54,959
design i worked in some

00:12:53,040 --> 00:12:56,480
systems where you use one giant topic

00:12:54,959 --> 00:12:57,920
with a whole bunch of partitions and all

00:12:56,480 --> 00:12:59,839
the events land there

00:12:57,920 --> 00:13:01,360
and an event type determines what type

00:12:59,839 --> 00:13:02,720
of an event you know what to do with it

00:13:01,360 --> 00:13:04,839
and then those sort of parcels out to

00:13:02,720 --> 00:13:06,160
different topics based on those event

00:13:04,839 --> 00:13:07,440
types

00:13:06,160 --> 00:13:10,480
so again the different ways you can use

00:13:07,440 --> 00:13:11,839
this but the key

00:13:10,480 --> 00:13:14,160
although it's optional it's very

00:13:11,839 --> 00:13:14,959
important um because it provides the

00:13:14,160 --> 00:13:17,680
ordering

00:13:14,959 --> 00:13:18,399
as i mentioned before that the messages

00:13:17,680 --> 00:13:20,240
are written to

00:13:18,399 --> 00:13:21,519
a partition in order and that order is

00:13:20,240 --> 00:13:24,079
always guaranteed

00:13:21,519 --> 00:13:26,320
and always is fixed but if you have

00:13:24,079 --> 00:13:28,399
multiple partitions in a topic

00:13:26,320 --> 00:13:29,760
then you don't have a guarantee of

00:13:28,399 --> 00:13:31,200
ordering if you don't have a key because

00:13:29,760 --> 00:13:33,920
the key will

00:13:31,200 --> 00:13:34,720
make sure that each message each event

00:13:33,920 --> 00:13:38,800
is written

00:13:34,720 --> 00:13:41,279
to the same topic so just a quick little

00:13:38,800 --> 00:13:42,720
example of how that works as eventually

00:13:41,279 --> 00:13:44,000
landing in the topic here's the topic of

00:13:42,720 --> 00:13:48,079
three partitions

00:13:44,000 --> 00:13:50,480
so as events are landing here um they'll

00:13:48,079 --> 00:13:52,560
land in a topic based on the key

00:13:50,480 --> 00:13:53,920
or land in a partition based on the key

00:13:52,560 --> 00:13:56,720
so what happens in the default

00:13:53,920 --> 00:13:58,399
partitioner is that the the key is a

00:13:56,720 --> 00:13:58,959
hash of the key is taken and then that's

00:13:58,399 --> 00:14:01,360
modulo

00:13:58,959 --> 00:14:02,800
with the number of partitions so three

00:14:01,360 --> 00:14:04,639
partitions in this case so

00:14:02,800 --> 00:14:06,000
the result's gonna be zero one or two

00:14:04,639 --> 00:14:08,639
and that'll tell it which

00:14:06,000 --> 00:14:09,600
which partition to write that event to

00:14:08,639 --> 00:14:11,199
um

00:14:09,600 --> 00:14:13,360
and then as they come in each one is

00:14:11,199 --> 00:14:17,120
assigned to just the

00:14:13,360 --> 00:14:18,880
appropriate partition and like you see

00:14:17,120 --> 00:14:20,160
events with the same key will always be

00:14:18,880 --> 00:14:21,600
written to the same partition

00:14:20,160 --> 00:14:23,440
and that will allow you to guarantee

00:14:21,600 --> 00:14:25,360
that ordering so that

00:14:23,440 --> 00:14:26,639
all all events with a1 are always going

00:14:25,360 --> 00:14:28,839
to be in that same

00:14:26,639 --> 00:14:31,600
partition in the order that they're

00:14:28,839 --> 00:14:33,920
written

00:14:31,600 --> 00:14:35,360
so and you know this in this example i

00:14:33,920 --> 00:14:36,560
haven't nicely spread out across these

00:14:35,360 --> 00:14:38,240
but it may not always be that way it

00:14:36,560 --> 00:14:41,120
really depends on how the hash

00:14:38,240 --> 00:14:42,959
algorithm works out now you can have a

00:14:41,120 --> 00:14:45,120
situation if you've got if your keys are

00:14:42,959 --> 00:14:47,040
not very widely

00:14:45,120 --> 00:14:48,639
just distributed you might end up with a

00:14:47,040 --> 00:14:49,120
hot partition say for example if you're

00:14:48,639 --> 00:14:52,160
using

00:14:49,120 --> 00:14:54,079
a customer id you could end up with a

00:14:52,160 --> 00:14:55,680
you know one of your a major customer

00:14:54,079 --> 00:14:56,639
could overload the partition so you

00:14:55,680 --> 00:14:58,639
might want to look at different

00:14:56,639 --> 00:15:00,399
strategies in that situation

00:14:58,639 --> 00:15:01,760
if you've got a customer that represents

00:15:00,399 --> 00:15:03,279
80 of your business you might want to

00:15:01,760 --> 00:15:04,320
put them in their own topic even but

00:15:03,279 --> 00:15:04,880
there's different ways you can handle

00:15:04,320 --> 00:15:06,800
that

00:15:04,880 --> 00:15:08,880
you can also create a custom partitioner

00:15:06,800 --> 00:15:10,240
if you if you really if you need to

00:15:08,880 --> 00:15:12,079
if the default one isn't doing what you

00:15:10,240 --> 00:15:13,279
need you can just you know create your

00:15:12,079 --> 00:15:15,519
own algorithm for

00:15:13,279 --> 00:15:17,040
designing where to put things and then

00:15:15,519 --> 00:15:18,399
also another thing you can do is

00:15:17,040 --> 00:15:21,199
and we'll get this when we start talking

00:15:18,399 --> 00:15:24,880
about producers which is up next here

00:15:21,199 --> 00:15:25,519
you can send an event to a specific

00:15:24,880 --> 00:15:27,360
partition

00:15:25,519 --> 00:15:28,880
so you can if you wanted to have the

00:15:27,360 --> 00:15:29,759
code in your application to do the

00:15:28,880 --> 00:15:32,079
partition

00:15:29,759 --> 00:15:33,199
you could do that as well it's not very

00:15:32,079 --> 00:15:36,079
commonly done but

00:15:33,199 --> 00:15:37,839
it can be done so events are written to

00:15:36,079 --> 00:15:40,160
the topics

00:15:37,839 --> 00:15:40,880
with a producer you know producer is

00:15:40,160 --> 00:15:43,920
part of the

00:15:40,880 --> 00:15:46,160
apache kafka client libraries

00:15:43,920 --> 00:15:47,360
and um the one i'm most familiar with

00:15:46,160 --> 00:15:48,480
and probably the most common one is the

00:15:47,360 --> 00:15:51,120
java libraries

00:15:48,480 --> 00:15:52,240
but there are libraries out there for

00:15:51,120 --> 00:15:55,759
most of the

00:15:52,240 --> 00:15:59,360
popular languages today c-sharp

00:15:55,759 --> 00:16:02,880
golang python and several others

00:15:59,360 --> 00:16:06,160
that are widely supported and

00:16:02,880 --> 00:16:06,880
actively used so you can use the

00:16:06,160 --> 00:16:08,880
producer

00:16:06,880 --> 00:16:10,000
or the client libraries to build your

00:16:08,880 --> 00:16:11,440
application so this is

00:16:10,000 --> 00:16:13,360
the producer is an application on its

00:16:11,440 --> 00:16:15,120
own it's it's it's a component that you

00:16:13,360 --> 00:16:17,120
add into your application

00:16:15,120 --> 00:16:18,639
um speaking in terms of the job one from

00:16:17,120 --> 00:16:20,240
the most used to you'll have classes

00:16:18,639 --> 00:16:23,440
that are there that you can use

00:16:20,240 --> 00:16:25,120
to um produce events so you're in the

00:16:23,440 --> 00:16:26,320
producer class you can stash it

00:16:25,120 --> 00:16:28,720
you give it some configuration

00:16:26,320 --> 00:16:30,000
information you need to give it

00:16:28,720 --> 00:16:31,519
one or more of the brokers that you're

00:16:30,000 --> 00:16:32,880
gonna be working with speaking of which

00:16:31,519 --> 00:16:35,120
i haven't mentioned brokerage yet

00:16:32,880 --> 00:16:37,440
um so let's back up a bit and talk about

00:16:35,120 --> 00:16:40,480
the different the three major components

00:16:37,440 --> 00:16:43,199
in in the end-to-end kafka story

00:16:40,480 --> 00:16:44,079
at its core are producer broker and

00:16:43,199 --> 00:16:45,759
consumer

00:16:44,079 --> 00:16:48,639
so the broker you can kind of think of

00:16:45,759 --> 00:16:49,519
as where the topic is the brokers manage

00:16:48,639 --> 00:16:51,199
the topics that's

00:16:49,519 --> 00:16:52,639
about all they do really is manage the

00:16:51,199 --> 00:16:56,320
topics um

00:16:52,639 --> 00:16:57,920
and the partitions so um partitions are

00:16:56,320 --> 00:16:59,519
are spread across if you have multiple

00:16:57,920 --> 00:17:00,959
brokers in a cluster the partitions for

00:16:59,519 --> 00:17:02,079
a given topic will be spread across

00:17:00,959 --> 00:17:03,759
those brokers

00:17:02,079 --> 00:17:05,120
there's usually also replicas which

00:17:03,759 --> 00:17:06,799
again are optional but it's a

00:17:05,120 --> 00:17:08,079
good thing to do so you have replicas of

00:17:06,799 --> 00:17:09,199
each of your partitions and that gives

00:17:08,079 --> 00:17:11,199
you fault tolerance

00:17:09,199 --> 00:17:13,360
so if a broker goes down any partitions

00:17:11,199 --> 00:17:16,559
that were where the lead partition was

00:17:13,360 --> 00:17:16,959
on on that broker the responsibility

00:17:16,559 --> 00:17:19,120
will

00:17:16,959 --> 00:17:20,799
be handed off to a replica partition

00:17:19,120 --> 00:17:23,520
that'll become the leader

00:17:20,799 --> 00:17:24,400
so there's leader and follower part

00:17:23,520 --> 00:17:27,439
replicas for

00:17:24,400 --> 00:17:28,799
for each partition of a topic

00:17:27,439 --> 00:17:30,400
and again they're spread across multiple

00:17:28,799 --> 00:17:31,200
brokers so this gives you the fault

00:17:30,400 --> 00:17:34,320
tolerance and

00:17:31,200 --> 00:17:36,080
as well as the scalability so the broker

00:17:34,320 --> 00:17:37,440
handles the topics the producer sends

00:17:36,080 --> 00:17:38,880
events to the topics

00:17:37,440 --> 00:17:40,559
and then we'll get the consumer in a bit

00:17:38,880 --> 00:17:41,120
that reads them but the producer and the

00:17:40,559 --> 00:17:44,880
consumer

00:17:41,120 --> 00:17:47,840
are part of the client library so

00:17:44,880 --> 00:17:49,440
with the producer library you can create

00:17:47,840 --> 00:17:52,880
an instance of the producer

00:17:49,440 --> 00:17:55,280
then you can create an event

00:17:52,880 --> 00:17:57,840
using still standard objects that are

00:17:55,280 --> 00:18:00,240
provided in the library

00:17:57,840 --> 00:18:01,120
record is the most common class you use

00:18:00,240 --> 00:18:03,280
for that

00:18:01,120 --> 00:18:04,720
a producer record and then you can send

00:18:03,280 --> 00:18:06,000
it now when you send it that's where you

00:18:04,720 --> 00:18:07,360
can actually give it a partition if you

00:18:06,000 --> 00:18:09,200
want to normally you don't and you let

00:18:07,360 --> 00:18:11,039
the partitioner decide where to put it

00:18:09,200 --> 00:18:12,640
if you're not using a key the partition

00:18:11,039 --> 00:18:15,120
by default will just use round robin

00:18:12,640 --> 00:18:16,320
which gives you a very good distribution

00:18:15,120 --> 00:18:17,440
so if you're doing things like system

00:18:16,320 --> 00:18:19,280
logs or something like that where you

00:18:17,440 --> 00:18:22,880
really don't need a key or have a key

00:18:19,280 --> 00:18:26,000
you can just let the round-robin

00:18:22,880 --> 00:18:27,760
default partition take care of that so

00:18:26,000 --> 00:18:30,960
the producer writes the events

00:18:27,760 --> 00:18:30,960
one by one and

00:18:31,840 --> 00:18:35,840
i lost my focus again sorry excuse me

00:18:40,240 --> 00:18:43,840
my slide deck isn't cooperating

00:18:45,120 --> 00:18:48,320
okay so write some one by one in order

00:18:47,039 --> 00:18:49,679
as we talked about earlier

00:18:48,320 --> 00:18:53,600
and that position that you see the

00:18:49,679 --> 00:18:55,440
number below the slot is the offset

00:18:53,600 --> 00:18:56,640
now the consumer comes along consumer is

00:18:55,440 --> 00:19:00,080
also part of the

00:18:56,640 --> 00:19:01,280
client library and the consumer um

00:19:00,080 --> 00:19:03,120
has a lot of smarts built into the

00:19:01,280 --> 00:19:04,000
producer does as well i should back up

00:19:03,120 --> 00:19:04,960
again we

00:19:04,000 --> 00:19:07,760
touch on some of those things the

00:19:04,960 --> 00:19:10,720
producer along with sending the data

00:19:07,760 --> 00:19:11,600
and telling it where to go it also has

00:19:10,720 --> 00:19:14,160
the information

00:19:11,600 --> 00:19:15,280
for the the partitioning it also has the

00:19:14,160 --> 00:19:16,960
information for the

00:19:15,280 --> 00:19:18,240
um for compression so if you're using

00:19:16,960 --> 00:19:20,799
compression which is a good thing to do

00:19:18,240 --> 00:19:23,039
to increase your bandwidth

00:19:20,799 --> 00:19:24,400
or your throughput i mean so that's all

00:19:23,039 --> 00:19:28,000
handled by the producer

00:19:24,400 --> 00:19:29,360
um the there was one other thing i was

00:19:28,000 --> 00:19:32,000
going to mention the producer does

00:19:29,360 --> 00:19:32,960
now i'm drawing a blank um the broker is

00:19:32,000 --> 00:19:34,880
very

00:19:32,960 --> 00:19:36,160
basically dumb all it does is manage its

00:19:34,880 --> 00:19:36,960
topics and make sure that they're

00:19:36,160 --> 00:19:40,000
balanced

00:19:36,960 --> 00:19:40,960
appropriately and that the they're

00:19:40,000 --> 00:19:44,000
active

00:19:40,960 --> 00:19:47,360
um but the producer and the consumer are

00:19:44,000 --> 00:19:47,360
where most of the smarts are at

00:19:48,000 --> 00:19:50,480
and again those are in the client

00:19:49,200 --> 00:19:52,480
library so you don't have to write all

00:19:50,480 --> 00:19:54,559
that yourself you just use the

00:19:52,480 --> 00:19:56,080
objects that are provided for you so the

00:19:54,559 --> 00:19:56,559
consumer then comes along and starts

00:19:56,080 --> 00:19:59,120
reading

00:19:56,559 --> 00:20:00,080
the the events from the from the topics

00:19:59,120 --> 00:20:02,159
in order again

00:20:00,080 --> 00:20:03,760
now you can seek to a specific offset

00:20:02,159 --> 00:20:06,880
and start from there but you always just

00:20:03,760 --> 00:20:09,280
read forward in order um

00:20:06,880 --> 00:20:10,880
generally start either at the latest or

00:20:09,280 --> 00:20:14,159
you know at the beginning or the

00:20:10,880 --> 00:20:15,120
or the last consumed one which will be

00:20:14,159 --> 00:20:16,880
planned in a minute

00:20:15,120 --> 00:20:18,480
so the producer and consumer are

00:20:16,880 --> 00:20:19,440
completely decoupled

00:20:18,480 --> 00:20:21,120
they don't know anything about each

00:20:19,440 --> 00:20:22,799
other they can work at their own paces

00:20:21,120 --> 00:20:24,400
the producer can just keep on producing

00:20:22,799 --> 00:20:25,280
the producer slows down the consumer

00:20:24,400 --> 00:20:27,200
catches up

00:20:25,280 --> 00:20:28,880
they'll just read until there's no more

00:20:27,200 --> 00:20:30,240
events there and they'll just

00:20:28,880 --> 00:20:33,200
sit there waiting for the next ones to

00:20:30,240 --> 00:20:36,320
come along um

00:20:33,200 --> 00:20:39,840
and periodically the consumer will will

00:20:36,320 --> 00:20:42,159
record the most recent message event

00:20:39,840 --> 00:20:44,000
that is processed so that'll be it's

00:20:42,159 --> 00:20:45,679
committed offset

00:20:44,000 --> 00:20:47,760
and the committed offset is really an

00:20:45,679 --> 00:20:48,960
important tool it's used to make sure

00:20:47,760 --> 00:20:51,440
that you don't miss anything

00:20:48,960 --> 00:20:52,240
so if the consumer goes offline the

00:20:51,440 --> 00:20:54,240
producer will just

00:20:52,240 --> 00:20:56,000
continue producing and keeps on sending

00:20:54,240 --> 00:20:58,880
those events one after the other

00:20:56,000 --> 00:21:00,159
um as long as there's more stuff coming

00:20:58,880 --> 00:21:01,280
and even though nothing's being consumed

00:21:00,159 --> 00:21:03,039
it doesn't it doesn't matter to the

00:21:01,280 --> 00:21:05,039
producer it's not it's not going to

00:21:03,039 --> 00:21:06,960
get backed up or jammed or anything like

00:21:05,039 --> 00:21:08,799
that it'll just keep on doing its work

00:21:06,960 --> 00:21:10,400
and when the consumer comes back online

00:21:08,799 --> 00:21:11,840
it'll use that committed offset to see

00:21:10,400 --> 00:21:14,960
where it left off and just pick up from

00:21:11,840 --> 00:21:14,960
there and keep going forward

00:21:15,120 --> 00:21:18,720
and there's uh metrics that you can get

00:21:16,880 --> 00:21:19,919
to see if there's a lag if consumers are

00:21:18,720 --> 00:21:21,360
falling behind producers again the

00:21:19,919 --> 00:21:22,799
producer won't care but you might care

00:21:21,360 --> 00:21:23,200
and so there are metrics that you can

00:21:22,799 --> 00:21:26,559
get

00:21:23,200 --> 00:21:26,559
that will tell you those kinds of things

00:21:28,480 --> 00:21:32,240
so now as i said the producer and

00:21:30,080 --> 00:21:33,760
consumer are completely decoupled

00:21:32,240 --> 00:21:35,760
they don't know anything about about

00:21:33,760 --> 00:21:36,320
each other but they do need to know

00:21:35,760 --> 00:21:38,080
about

00:21:36,320 --> 00:21:39,440
the events and how to read them right

00:21:38,080 --> 00:21:40,640
because you have to be able to serialize

00:21:39,440 --> 00:21:42,480
and deserialize them

00:21:40,640 --> 00:21:44,159
and so for that a schema is extremely

00:21:42,480 --> 00:21:46,159
helpful again not required you could

00:21:44,159 --> 00:21:47,520
have that information just hardcoded in

00:21:46,159 --> 00:21:49,200
your applications and your producer

00:21:47,520 --> 00:21:50,400
consumer applications and if it's a

00:21:49,200 --> 00:21:51,919
small project and you're the only

00:21:50,400 --> 00:21:53,039
developer on it that might be sufficient

00:21:51,919 --> 00:21:54,480
you know what you're putting in you know

00:21:53,039 --> 00:21:55,679
what you're getting out

00:21:54,480 --> 00:21:57,760
but if you're working with multiple

00:21:55,679 --> 00:21:59,280
teams and multiple different topics with

00:21:57,760 --> 00:22:02,400
different types of events on them

00:21:59,280 --> 00:22:05,200
a schema becomes very important

00:22:02,400 --> 00:22:06,559
so the schema registry is another

00:22:05,200 --> 00:22:07,120
component it's another application that

00:22:06,559 --> 00:22:10,240
you run

00:22:07,120 --> 00:22:11,679
and it's not part of the apache kafka

00:22:10,240 --> 00:22:14,000
itself but it's another

00:22:11,679 --> 00:22:15,440
source available free tool to use

00:22:14,000 --> 00:22:18,080
provided by confluent

00:22:15,440 --> 00:22:19,039
um and it's very very helpful very

00:22:18,080 --> 00:22:21,440
valuable tool

00:22:19,039 --> 00:22:23,200
so with that a producer sends an event

00:22:21,440 --> 00:22:24,480
and passes the schema with it and the

00:22:23,200 --> 00:22:26,640
first time it does that the schema

00:22:24,480 --> 00:22:29,039
registry picks it up and stores it

00:22:26,640 --> 00:22:30,480
after that it'll just use the the key

00:22:29,039 --> 00:22:32,159
that it gives it can be used by both the

00:22:30,480 --> 00:22:33,919
producer and the consumer to say this is

00:22:32,159 --> 00:22:34,640
the schema of the event that i'm working

00:22:33,919 --> 00:22:36,480
with

00:22:34,640 --> 00:22:38,880
once when the consumer pit reads an

00:22:36,480 --> 00:22:39,360
event it grabs the id off of that after

00:22:38,880 --> 00:22:42,480
the

00:22:39,360 --> 00:22:43,919
first few bites of the event and goes to

00:22:42,480 --> 00:22:46,799
scheme registry gets the schema for it

00:22:43,919 --> 00:22:49,520
and it can decode it from that so

00:22:46,799 --> 00:22:51,440
and this game registry also enforces

00:22:49,520 --> 00:22:52,960
schema

00:22:51,440 --> 00:22:54,640
schema evolution so you can tell it

00:22:52,960 --> 00:22:56,000
whether you want that to be

00:22:54,640 --> 00:22:57,520
full compatibility or backwards

00:22:56,000 --> 00:22:58,960
compatibility force compatibility

00:22:57,520 --> 00:22:59,520
different options like that that you can

00:22:58,960 --> 00:23:01,200
set

00:22:59,520 --> 00:23:02,400
and it'll keep track of those things for

00:23:01,200 --> 00:23:03,919
you and make sure that you're not

00:23:02,400 --> 00:23:06,559
breaking your

00:23:03,919 --> 00:23:06,559
your users

00:23:07,280 --> 00:23:11,520
another uh part of the of the crafty

00:23:09,840 --> 00:23:13,200
ecosystem is copper connect and

00:23:11,520 --> 00:23:15,360
and this is one of my favorites because

00:23:13,200 --> 00:23:18,720
it allows you to do

00:23:15,360 --> 00:23:21,120
to bring kafka into an environment

00:23:18,720 --> 00:23:22,720
without affecting existing applications

00:23:21,120 --> 00:23:23,840
really

00:23:22,720 --> 00:23:26,480
in this one we're looking at a picture

00:23:23,840 --> 00:23:28,720
of a kafka connect

00:23:26,480 --> 00:23:30,240
source connector now first of all backup

00:23:28,720 --> 00:23:31,679
kafka connect is basically just

00:23:30,240 --> 00:23:34,320
an abstraction over producers and

00:23:31,679 --> 00:23:37,440
consumers right so it knows how to

00:23:34,320 --> 00:23:38,880
read and write from from kafka

00:23:37,440 --> 00:23:40,559
then you add in a plugin called the

00:23:38,880 --> 00:23:42,720
connector which which

00:23:40,559 --> 00:23:44,720
knows how to work with the external

00:23:42,720 --> 00:23:46,000
system so for a source connector you'll

00:23:44,720 --> 00:23:48,000
write add in a connector

00:23:46,000 --> 00:23:49,679
that that knows how to read from a

00:23:48,000 --> 00:23:50,799
database or read from an application

00:23:49,679 --> 00:23:54,240
like salesforce

00:23:50,799 --> 00:23:55,600
or reads from text files or reads from

00:23:54,240 --> 00:23:57,120
http

00:23:55,600 --> 00:23:58,799
so these different connectors are

00:23:57,120 --> 00:24:00,720
available there's i think almost 200 of

00:23:58,799 --> 00:24:02,000
them out there right now

00:24:00,720 --> 00:24:03,679
that you can use for all kinds of

00:24:02,000 --> 00:24:04,799
different purposes they'll read that

00:24:03,679 --> 00:24:07,520
information from those

00:24:04,799 --> 00:24:09,679
external sources and send it to

00:24:07,520 --> 00:24:11,279
cockatopics

00:24:09,679 --> 00:24:13,279
so if you have an existing application

00:24:11,279 --> 00:24:14,880
you can just hook up a connector to the

00:24:13,279 --> 00:24:15,919
database of that application or to the

00:24:14,880 --> 00:24:17,360
application itself if there's a

00:24:15,919 --> 00:24:18,400
connector for it something bigger than

00:24:17,360 --> 00:24:19,919
salesforce

00:24:18,400 --> 00:24:21,600
and you can start getting that data also

00:24:19,919 --> 00:24:22,960
in the kafka doesn't affect your current

00:24:21,600 --> 00:24:24,559
applications at all but it makes the

00:24:22,960 --> 00:24:25,919
data available to other

00:24:24,559 --> 00:24:27,679
applications that you might want to add

00:24:25,919 --> 00:24:29,840
on it just opens up a lot of

00:24:27,679 --> 00:24:31,200
opportunities

00:24:29,840 --> 00:24:32,880
and then there's sync connectors as well

00:24:31,200 --> 00:24:36,320
for writing data back out to

00:24:32,880 --> 00:24:36,320
different external systems

00:24:36,400 --> 00:24:40,400
so this is a way that you can have

00:24:38,559 --> 00:24:42,320
downstream applications can get the data

00:24:40,400 --> 00:24:44,559
in the source that they're expecting it

00:24:42,320 --> 00:24:46,000
for their own purposes without

00:24:44,559 --> 00:24:48,960
interfering with the rest of what your

00:24:46,000 --> 00:24:50,000
applications are doing so um kafka then

00:24:48,960 --> 00:24:52,799
becomes like this

00:24:50,000 --> 00:24:54,080
central uh data plane that you can just

00:24:52,799 --> 00:24:54,799
have all of your data flowing through

00:24:54,080 --> 00:24:56,480
there

00:24:54,799 --> 00:24:57,919
and you can be pulled from different

00:24:56,480 --> 00:24:59,120
sources sent to different sources and

00:24:57,919 --> 00:25:00,000
along the way you can do a lot of great

00:24:59,120 --> 00:25:02,159
things with it building your own

00:25:00,000 --> 00:25:03,840
applications off the topics directly

00:25:02,159 --> 00:25:05,200
with using consumers and producers or

00:25:03,840 --> 00:25:07,520
some other tools we're about to talk

00:25:05,200 --> 00:25:09,200
about

00:25:07,520 --> 00:25:12,559
so one of those other tools is kafka

00:25:09,200 --> 00:25:14,880
streams this is part of the patchy kafka

00:25:12,559 --> 00:25:17,120
project and or a sub project from it and

00:25:14,880 --> 00:25:19,440
it's a very powerful tool

00:25:17,120 --> 00:25:22,000
um it again builds off the consumers and

00:25:19,440 --> 00:25:22,000
producers

00:25:22,240 --> 00:25:28,159
it consumes events from the topics but

00:25:25,679 --> 00:25:29,440
then runs them through a stream

00:25:28,159 --> 00:25:32,000
with a whole bunch of different

00:25:29,440 --> 00:25:33,919
operations that you can use

00:25:32,000 --> 00:25:36,080
cocker streams provides a dsl which i'm

00:25:33,919 --> 00:25:37,120
showing some of the operations in dsl on

00:25:36,080 --> 00:25:38,480
this slide here

00:25:37,120 --> 00:25:40,799
and there's a lot more of them that you

00:25:38,480 --> 00:25:42,240
can use and you can do quite a bit with

00:25:40,799 --> 00:25:44,320
just the dsl alone

00:25:42,240 --> 00:25:45,520
if you need to go deeper there's also a

00:25:44,320 --> 00:25:46,960
processor api

00:25:45,520 --> 00:25:48,960
that allows you to do even more and

00:25:46,960 --> 00:25:49,840
create your own operations but usually

00:25:48,960 --> 00:25:51,840
operations

00:25:49,840 --> 00:25:53,840
or most these operations will return

00:25:51,840 --> 00:25:55,440
another as they call k stream

00:25:53,840 --> 00:25:57,520
object and so you can you can chain

00:25:55,440 --> 00:26:01,520
these together do all kinds of

00:25:57,520 --> 00:26:04,159
filtering mapping aggregations branching

00:26:01,520 --> 00:26:05,440
um and then you have terminal operators

00:26:04,159 --> 00:26:06,400
like two so when you're all done with

00:26:05,440 --> 00:26:08,559
your processing

00:26:06,400 --> 00:26:09,520
you write your end results out to

00:26:08,559 --> 00:26:12,480
another topic or

00:26:09,520 --> 00:26:13,760
topics so events can come in one topic

00:26:12,480 --> 00:26:15,440
and then it may have been a lot of other

00:26:13,760 --> 00:26:16,640
topics i mentioned an application that i

00:26:15,440 --> 00:26:19,200
worked on before with

00:26:16,640 --> 00:26:21,200
one giant topic with thousands of

00:26:19,200 --> 00:26:24,480
partitions

00:26:21,200 --> 00:26:25,679
and it all events landed there and then

00:26:24,480 --> 00:26:26,640
there were some rules that were in

00:26:25,679 --> 00:26:28,480
routing that was done

00:26:26,640 --> 00:26:29,760
and through a stream's application

00:26:28,480 --> 00:26:32,000
streams application

00:26:29,760 --> 00:26:34,480
and then it wrote those events back out

00:26:32,000 --> 00:26:36,640
to multiple different topics

00:26:34,480 --> 00:26:37,840
so that's a relatively common use case

00:26:36,640 --> 00:26:39,440
but there's just so much you can do with

00:26:37,840 --> 00:26:42,080
kafka streams um

00:26:39,440 --> 00:26:43,120
and as you build out your topologies as

00:26:42,080 --> 00:26:45,600
they're called

00:26:43,120 --> 00:26:46,880
they get pretty complex but they're

00:26:45,600 --> 00:26:48,480
tools out there to help you visualize

00:26:46,880 --> 00:26:49,760
them and it's just it's a very powerful

00:26:48,480 --> 00:26:52,320
tool

00:26:49,760 --> 00:26:53,440
it is java only so it's a java library

00:26:52,320 --> 00:26:54,640
that you can include in your

00:26:53,440 --> 00:26:56,720
applications i think there's somebody

00:26:54,640 --> 00:26:58,880
working on

00:26:56,720 --> 00:26:59,919
a c-sharp equivalent so there's no

00:26:58,880 --> 00:27:02,799
reason somebody couldn't

00:26:59,919 --> 00:27:04,400
write copy streams type library or

00:27:02,799 --> 00:27:04,799
similar library in other languages that

00:27:04,400 --> 00:27:07,919
just

00:27:04,799 --> 00:27:09,279
matter you know people's time and some

00:27:07,919 --> 00:27:10,400
getting it done so i said there's one in

00:27:09,279 --> 00:27:11,039
the works for c sharp but you can

00:27:10,400 --> 00:27:13,760
probably

00:27:11,039 --> 00:27:15,440
google and find that um but it's a great

00:27:13,760 --> 00:27:16,480
library if you use java already it's a

00:27:15,440 --> 00:27:21,440
great way to

00:27:16,480 --> 00:27:25,039
interact with the data in kafka

00:27:21,440 --> 00:27:26,080
um so speaking of copy strings

00:27:25,039 --> 00:27:28,080
in copper strings there's like i

00:27:26,080 --> 00:27:30,320
mentioned the k stream there's also an

00:27:28,080 --> 00:27:33,200
another

00:27:30,320 --> 00:27:34,640
constant called a k table because if you

00:27:33,200 --> 00:27:36,000
think about it oh why was my stuff not

00:27:34,640 --> 00:27:37,760
showing up here

00:27:36,000 --> 00:27:38,880
oh i left my animations i'm sorry i'm

00:27:37,760 --> 00:27:40,080
gonna get this filled in real quick so

00:27:38,880 --> 00:27:43,200
just want to use this as a

00:27:40,080 --> 00:27:45,520
static picture um so

00:27:43,200 --> 00:27:47,360
you can look at events in a topic as a

00:27:45,520 --> 00:27:48,320
stream which is a common way to work

00:27:47,360 --> 00:27:50,159
with them

00:27:48,320 --> 00:27:51,679
and um and kafka streams especially

00:27:50,159 --> 00:27:55,039
that's what you normally will start with

00:27:51,679 --> 00:27:58,320
as you know loaded from the read

00:27:55,039 --> 00:27:59,840
a stream of data from a topic but um the

00:27:58,320 --> 00:28:00,640
same data can also be represented as a

00:27:59,840 --> 00:28:03,120
table

00:28:00,640 --> 00:28:05,120
so copy streams has a k table which is

00:28:03,120 --> 00:28:07,520
is just a snapshot of the current

00:28:05,120 --> 00:28:10,880
state of all of the keys in your stream

00:28:07,520 --> 00:28:12,640
so here we have this a1 b1 c1 d1 so

00:28:10,880 --> 00:28:14,080
the stream shows what's happened with

00:28:12,640 --> 00:28:16,080
each of those those keys

00:28:14,080 --> 00:28:17,440
all along the way the table is just the

00:28:16,080 --> 00:28:21,279
current values

00:28:17,440 --> 00:28:22,480
um and so you might use a table for

00:28:21,279 --> 00:28:24,000
joining a first

00:28:22,480 --> 00:28:26,240
more static data that doesn't change as

00:28:24,000 --> 00:28:27,840
often and you can you can do joins and

00:28:26,240 --> 00:28:29,360
cockpit streams with a table and a

00:28:27,840 --> 00:28:31,520
stream so you can maybe have

00:28:29,360 --> 00:28:33,279
you have a user table that has your user

00:28:31,520 --> 00:28:34,399
information and when that some event

00:28:33,279 --> 00:28:36,159
comes through

00:28:34,399 --> 00:28:38,159
that user is doing something you can

00:28:36,159 --> 00:28:40,159
join up that with the user id

00:28:38,159 --> 00:28:41,520
and and get the user information and

00:28:40,159 --> 00:28:43,600
write something to another topic

00:28:41,520 --> 00:28:46,000
that has the information about that user

00:28:43,600 --> 00:28:48,320
and what they just did

00:28:46,000 --> 00:28:50,000
so the tables are very useful for that

00:28:48,320 --> 00:28:51,919
they all come from topics

00:28:50,000 --> 00:28:53,039
um now one thing to mention here since

00:28:51,919 --> 00:28:55,279
we're talking about this is

00:28:53,039 --> 00:28:56,399
is that a topic will normally have all

00:28:55,279 --> 00:28:58,080
of the events that's been written

00:28:56,399 --> 00:29:00,159
like we talked about earlier right it's

00:28:58,080 --> 00:29:02,480
immutable it's it's semi-permanent for

00:29:00,159 --> 00:29:04,399
as long as you want it to be

00:29:02,480 --> 00:29:06,240
but you can't have you can't configure a

00:29:04,399 --> 00:29:08,159
topic to be compacted

00:29:06,240 --> 00:29:10,320
and a compacted topic goes well with a

00:29:08,159 --> 00:29:12,159
table such as data in a topic that you

00:29:10,320 --> 00:29:13,840
are going to mainly just use in in the

00:29:12,159 --> 00:29:14,799
form of a k table in your proper

00:29:13,840 --> 00:29:17,360
exchange application

00:29:14,799 --> 00:29:19,039
say um then you might want to configure

00:29:17,360 --> 00:29:20,720
that topic to be compacted and what that

00:29:19,039 --> 00:29:21,600
will do then is it will tell prop codes

00:29:20,720 --> 00:29:24,080
to clean up

00:29:21,600 --> 00:29:25,200
all the older keys all the older events

00:29:24,080 --> 00:29:27,520
for that same key

00:29:25,200 --> 00:29:31,440
so that all you're keeping is the most

00:29:27,520 --> 00:29:31,440
current event for the given key

00:29:31,840 --> 00:29:34,640
so this might be something you would use

00:29:33,120 --> 00:29:36,320
for something like a lookup table if

00:29:34,640 --> 00:29:37,360
you're using a small set of data that

00:29:36,320 --> 00:29:39,279
you use in your

00:29:37,360 --> 00:29:40,399
in your streams applications and you're

00:29:39,279 --> 00:29:42,159
going to keep looking up to get those

00:29:40,399 --> 00:29:44,399
values for it

00:29:42,159 --> 00:29:46,320
then a compacted topic might be just

00:29:44,399 --> 00:29:47,760
what you need for that

00:29:46,320 --> 00:29:49,600
it's just it's an efficiency thing you

00:29:47,760 --> 00:29:51,919
don't need to use that you can have

00:29:49,600 --> 00:29:53,120
a table based on a very long topic

00:29:51,919 --> 00:29:54,799
that's got

00:29:53,120 --> 00:29:56,399
years old data if you want but you'll

00:29:54,799 --> 00:29:59,039
only see in the table you only see the

00:29:56,399 --> 00:30:00,559
most recent ones

00:29:59,039 --> 00:30:02,320
so that's streams and tables and so

00:30:00,559 --> 00:30:02,960
they're really it's a it's a neat idea

00:30:02,320 --> 00:30:05,520
how they can

00:30:02,960 --> 00:30:06,720
people have people talk about the stream

00:30:05,520 --> 00:30:08,240
table duality

00:30:06,720 --> 00:30:11,520
they really are both just different

00:30:08,240 --> 00:30:12,799
views of the same the same information

00:30:11,520 --> 00:30:14,559
now one thing keep in mind the table

00:30:12,799 --> 00:30:15,919
though is that it's lossy right once you

00:30:14,559 --> 00:30:17,360
once you turn something to a table you

00:30:15,919 --> 00:30:17,760
don't have the history of what led to

00:30:17,360 --> 00:30:19,919
the

00:30:17,760 --> 00:30:22,799
current state so the stream has that and

00:30:19,919 --> 00:30:24,799
keeps that history

00:30:22,799 --> 00:30:26,080
another tool that's in um that's

00:30:24,799 --> 00:30:28,399
available and again this

00:30:26,080 --> 00:30:30,320
is source available like provided by

00:30:28,399 --> 00:30:32,080
confluent

00:30:30,320 --> 00:30:33,679
you can use it freely it's another

00:30:32,080 --> 00:30:36,080
application that you install so you have

00:30:33,679 --> 00:30:38,960
a cluster of k sql db servers

00:30:36,080 --> 00:30:39,760
and k-sequel db is built on top of kafka

00:30:38,960 --> 00:30:41,360
streams

00:30:39,760 --> 00:30:42,799
but it's great if you're not in a java

00:30:41,360 --> 00:30:43,840
shop or you don't work with java don't

00:30:42,799 --> 00:30:45,919
want to learn java

00:30:43,840 --> 00:30:47,520
you can build a lot of the same types of

00:30:45,919 --> 00:30:48,720
stream applications that you could with

00:30:47,520 --> 00:30:50,159
proctor streams you can build it with

00:30:48,720 --> 00:30:53,039
case equal vb

00:30:50,159 --> 00:30:54,480
using a sql dialect and i have a really

00:30:53,039 --> 00:30:55,840
simple example here but you can get

00:30:54,480 --> 00:30:56,720
pretty complex with these and you can

00:30:55,840 --> 00:30:58,960
create streams

00:30:56,720 --> 00:31:00,559
based on topics you can create tables

00:30:58,960 --> 00:31:02,320
based on streams

00:31:00,559 --> 00:31:04,159
you can join streams and tables and come

00:31:02,320 --> 00:31:05,039
up with new results in britain and new

00:31:04,159 --> 00:31:07,200
topics

00:31:05,039 --> 00:31:08,159
um so you can do quite a bit with with

00:31:07,200 --> 00:31:11,519
this tool

00:31:08,159 --> 00:31:11,919
um and then with kafka with with k sql

00:31:11,519 --> 00:31:13,200
db

00:31:11,919 --> 00:31:14,559
you can do like your kafka streams

00:31:13,200 --> 00:31:15,760
applications where you just do all your

00:31:14,559 --> 00:31:17,440
processing and write the output to

00:31:15,760 --> 00:31:18,960
another

00:31:17,440 --> 00:31:21,039
doing that topic from another

00:31:18,960 --> 00:31:23,760
application but you also with with

00:31:21,039 --> 00:31:26,240
k-secondly there's a rest api so you can

00:31:23,760 --> 00:31:29,039
um interact with the data through rest

00:31:26,240 --> 00:31:30,799
you can send queries to it with with a

00:31:29,039 --> 00:31:32,880
rest to the rest api but you also can

00:31:30,799 --> 00:31:34,240
read you can you can read the current

00:31:32,880 --> 00:31:36,320
data in a current state

00:31:34,240 --> 00:31:37,600
so there's two types of queries in in k

00:31:36,320 --> 00:31:39,279
sql db um

00:31:37,600 --> 00:31:41,120
the one i'm showing here where it has a

00:31:39,279 --> 00:31:42,320
keyword emit changes on there if you can

00:31:41,120 --> 00:31:45,440
see that on the slide

00:31:42,320 --> 00:31:47,440
and the center bar that tells it as

00:31:45,440 --> 00:31:48,960
as the data's coming into the topic

00:31:47,440 --> 00:31:50,000
continue to process this query and give

00:31:48,960 --> 00:31:51,360
me the latest

00:31:50,000 --> 00:31:52,559
results and so the results will keep

00:31:51,360 --> 00:31:53,519
scrolling up and you'll keep getting

00:31:52,559 --> 00:31:56,240
more of them

00:31:53,519 --> 00:31:57,519
you can that's a push query and that's

00:31:56,240 --> 00:31:58,240
pushing the results back out to you

00:31:57,519 --> 00:31:59,840
constantly

00:31:58,240 --> 00:32:01,440
a poll query gives you just the current

00:31:59,840 --> 00:32:03,200
state so you can execute a full query

00:32:01,440 --> 00:32:05,919
and give it to the current state of a

00:32:03,200 --> 00:32:07,760
of a stream or table and so you can make

00:32:05,919 --> 00:32:09,519
a full query through the rest api

00:32:07,760 --> 00:32:10,880
from another application and that gives

00:32:09,519 --> 00:32:12,960
an application that isn't a job

00:32:10,880 --> 00:32:15,039
application a way to interact with

00:32:12,960 --> 00:32:17,519
with the topics in this in a stream

00:32:15,039 --> 00:32:19,679
stream application built with casey webb

00:32:17,519 --> 00:32:20,720
so it's another you know powerful tool

00:32:19,679 --> 00:32:23,440
um it is

00:32:20,720 --> 00:32:24,320
i said another application you have to

00:32:23,440 --> 00:32:27,919
install and

00:32:24,320 --> 00:32:28,320
maintain and run um in its own cluster

00:32:27,919 --> 00:32:31,519
um

00:32:28,320 --> 00:32:33,360
k single db does include um kafka

00:32:31,519 --> 00:32:36,240
connect as well so you can

00:32:33,360 --> 00:32:36,799
install and configure connectors right

00:32:36,240 --> 00:32:38,720
through with

00:32:36,799 --> 00:32:41,440
sql statements in k sql db which is

00:32:38,720 --> 00:32:43,200
another handy feature of that so you can

00:32:41,440 --> 00:32:46,080
build out a pretty pretty complex system

00:32:43,200 --> 00:32:46,080
of k sql db

00:32:46,799 --> 00:32:50,240
and now let's let's look at just

00:32:48,399 --> 00:32:53,519
bringing this all together so you got

00:32:50,240 --> 00:32:55,039
your your events being re produced and

00:32:53,519 --> 00:32:56,159
consumed for producing these topics

00:32:55,039 --> 00:32:59,279
you've got um

00:32:56,159 --> 00:33:01,039
kafka streams or k sqlb that you can do

00:32:59,279 --> 00:33:03,919
all kinds of

00:33:01,039 --> 00:33:05,760
aggregations um branching you know

00:33:03,919 --> 00:33:06,960
filtering all this processing on their

00:33:05,760 --> 00:33:09,440
streams of data

00:33:06,960 --> 00:33:11,279
as it's coming through so let's see how

00:33:09,440 --> 00:33:12,640
we can build this together into a more

00:33:11,279 --> 00:33:14,159
complex platform

00:33:12,640 --> 00:33:15,760
so we say we start off with our our

00:33:14,159 --> 00:33:17,279
application right standard application

00:33:15,760 --> 00:33:18,960
we got our client we got a server back

00:33:17,279 --> 00:33:20,480
in service it's got its own database and

00:33:18,960 --> 00:33:23,360
it's it's

00:33:20,480 --> 00:33:24,640
honestly doing its thing just happily

00:33:23,360 --> 00:33:26,880
chugging along

00:33:24,640 --> 00:33:27,679
but the data that that's going into that

00:33:26,880 --> 00:33:29,279
database is

00:33:27,679 --> 00:33:31,120
it's coming from events right events are

00:33:29,279 --> 00:33:32,640
generated with the client um

00:33:31,120 --> 00:33:34,159
running through the service and landing

00:33:32,640 --> 00:33:35,600
in the database and it's kind of like

00:33:34,159 --> 00:33:37,279
this is where events go to die you know

00:33:35,600 --> 00:33:39,039
they just sit there and they're not much

00:33:37,279 --> 00:33:39,919
more use anymore you've lost the history

00:33:39,039 --> 00:33:42,320
as things change

00:33:39,919 --> 00:33:43,760
you just have the latest information so

00:33:42,320 --> 00:33:45,679
you decide you want to start capturing

00:33:43,760 --> 00:33:47,039
those events and

00:33:45,679 --> 00:33:49,120
opening up more opportunities for them

00:33:47,039 --> 00:33:50,799
so you can bring in copy connect

00:33:49,120 --> 00:33:52,720
so every time something happens in a

00:33:50,799 --> 00:33:54,799
database that's an event it's

00:33:52,720 --> 00:33:56,480
kafka connect can capture that using a

00:33:54,799 --> 00:33:58,240
cdc or change data capture

00:33:56,480 --> 00:34:00,000
connector for whatever database you're

00:33:58,240 --> 00:34:01,440
using

00:34:00,000 --> 00:34:03,600
and write that into conquest so now you

00:34:01,440 --> 00:34:06,720
have these events in in

00:34:03,600 --> 00:34:08,240
topic topics so you can sort of open up

00:34:06,720 --> 00:34:09,599
other uses for those

00:34:08,240 --> 00:34:11,760
so then you might add a streams

00:34:09,599 --> 00:34:12,800
application so you write out a copy

00:34:11,760 --> 00:34:14,560
extremes application

00:34:12,800 --> 00:34:16,399
reading from them from those topics so

00:34:14,560 --> 00:34:18,079
as this is all happening while

00:34:16,399 --> 00:34:20,320
while people are using the client there

00:34:18,079 --> 00:34:21,440
it's writing your service it's going to

00:34:20,320 --> 00:34:23,520
the database

00:34:21,440 --> 00:34:24,720
passing through connect into the topics

00:34:23,520 --> 00:34:25,679
being streamed into your streams

00:34:24,720 --> 00:34:27,919
application

00:34:25,679 --> 00:34:29,520
and there you can combine it with some

00:34:27,919 --> 00:34:32,480
other data

00:34:29,520 --> 00:34:34,159
enrich it filter it do whatever you need

00:34:32,480 --> 00:34:35,919
to do there and write that back to

00:34:34,159 --> 00:34:38,079
other topics and then those topics you

00:34:35,919 --> 00:34:39,599
can use crop connect to send data from

00:34:38,079 --> 00:34:40,960
those new topics that were

00:34:39,599 --> 00:34:42,480
enhanced and enriched with your streams

00:34:40,960 --> 00:34:44,720
application you can send those right

00:34:42,480 --> 00:34:46,480
back into that same database and so your

00:34:44,720 --> 00:34:48,000
your existing service might start taking

00:34:46,480 --> 00:34:49,280
use of some of those things they're

00:34:48,000 --> 00:34:51,359
taking advantage of some of the things

00:34:49,280 --> 00:34:52,960
that kafka streams can do for you

00:34:51,359 --> 00:34:54,639
as far as manipulating and enhancing

00:34:52,960 --> 00:34:57,040
your data and that can you can

00:34:54,639 --> 00:34:58,400
just grab that data from new tables in

00:34:57,040 --> 00:34:59,920
the same database

00:34:58,400 --> 00:35:01,520
if you want or you could open up for

00:34:59,920 --> 00:35:03,280
other applications to use that

00:35:01,520 --> 00:35:05,280
so you can add new applications now that

00:35:03,280 --> 00:35:06,720
can be working with that same data

00:35:05,280 --> 00:35:08,560
without affecting your your existing

00:35:06,720 --> 00:35:10,400
your initial application at all you can

00:35:08,560 --> 00:35:13,200
create these new services

00:35:10,400 --> 00:35:14,800
that can use the data from those topics

00:35:13,200 --> 00:35:16,480
or it might be sending new data to

00:35:14,800 --> 00:35:17,839
topics on its own

00:35:16,480 --> 00:35:20,480
there's just all kinds of options that

00:35:17,839 --> 00:35:20,480
can do with that

00:35:20,640 --> 00:35:23,839
then you can add k sql db say for

00:35:22,480 --> 00:35:25,440
example the new application you added

00:35:23,839 --> 00:35:27,520
they weren't a java team they were

00:35:25,440 --> 00:35:28,960
a golang team but they still wanted to

00:35:27,520 --> 00:35:30,480
do some streaming

00:35:28,960 --> 00:35:32,720
stream processing with that data as well

00:35:30,480 --> 00:35:34,400
so they just kick sql db

00:35:32,720 --> 00:35:36,560
um either read directly from topics

00:35:34,400 --> 00:35:38,160
again or they can use the rest api and

00:35:36,560 --> 00:35:41,040
then call casey quickly b

00:35:38,160 --> 00:35:41,680
to get that data into their application

00:35:41,040 --> 00:35:43,520
so

00:35:41,680 --> 00:35:45,119
with all this data now flowing into into

00:35:43,520 --> 00:35:46,320
kafka and through kafka

00:35:45,119 --> 00:35:47,920
you might want to start doing other

00:35:46,320 --> 00:35:48,880
things with it like perhaps adding some

00:35:47,920 --> 00:35:51,119
analytics

00:35:48,880 --> 00:35:52,480
so you can again use connect to to

00:35:51,119 --> 00:35:54,160
stream that data out to

00:35:52,480 --> 00:35:55,520
elasticsearch or some other analytics

00:35:54,160 --> 00:35:58,400
engine as you're going

00:35:55,520 --> 00:36:00,000
or other teams you know might have

00:35:58,400 --> 00:36:01,920
downstream applications that would

00:36:00,000 --> 00:36:03,040
take advantage of this data and you can

00:36:01,920 --> 00:36:06,000
use connectors

00:36:03,040 --> 00:36:06,400
to send the data out to other databases

00:36:06,000 --> 00:36:08,640
and

00:36:06,400 --> 00:36:10,000
then on and on so this is this is a

00:36:08,640 --> 00:36:12,320
small snapshot of what you could do

00:36:10,000 --> 00:36:14,960
there's so much more that can be done

00:36:12,320 --> 00:36:17,440
using kafka in this way and copper

00:36:14,960 --> 00:36:19,760
connect really is a great enabler to

00:36:17,440 --> 00:36:21,359
allow you to just use copper throughout

00:36:19,760 --> 00:36:23,119
your enterprise

00:36:21,359 --> 00:36:24,480
without breaking things you know it's

00:36:23,119 --> 00:36:26,160
it's a it's

00:36:24,480 --> 00:36:27,839
really connect it's just a great tool

00:36:26,160 --> 00:36:29,359
for

00:36:27,839 --> 00:36:31,040
allowing us to capture the power of

00:36:29,359 --> 00:36:34,240
kafka

00:36:31,040 --> 00:36:34,240
in an incremental fashion

00:36:35,760 --> 00:36:39,040
so almost closing here since i'm almost

00:36:38,560 --> 00:36:42,079
out of time

00:36:39,040 --> 00:36:43,440
um the the there's a blog post that

00:36:42,079 --> 00:36:46,640
shows how the new york times

00:36:43,440 --> 00:36:48,560
um stores all of their content in kafka

00:36:46,640 --> 00:36:49,920
um and this is a snapshot of their of

00:36:48,560 --> 00:36:51,440
their high level

00:36:49,920 --> 00:36:52,640
view of their architecture but i would

00:36:51,440 --> 00:36:53,920
encourage you to read that blog post

00:36:52,640 --> 00:36:56,000
it's just amazing when i saw that i was

00:36:53,920 --> 00:36:58,320
just blown away with how they're

00:36:56,000 --> 00:37:00,240
able to to replay the entire history of

00:36:58,320 --> 00:37:01,040
their of all of their articles and

00:37:00,240 --> 00:37:02,240
picture you know

00:37:01,040 --> 00:37:03,839
images everything they've got all their

00:37:02,240 --> 00:37:05,200
content they can replay it and send it

00:37:03,839 --> 00:37:06,079
to the different sources or different

00:37:05,200 --> 00:37:09,200
consuming

00:37:06,079 --> 00:37:10,400
consumers as needed so if they ever you

00:37:09,200 --> 00:37:11,280
know went down they could reproduce the

00:37:10,400 --> 00:37:14,160
entire thing

00:37:11,280 --> 00:37:15,359
from the coffee topics they're storing a

00:37:14,160 --> 00:37:17,680
ton of data in kafka

00:37:15,359 --> 00:37:19,520
but it's it's there it's it's accessible

00:37:17,680 --> 00:37:20,960
and it's pretty fast

00:37:19,520 --> 00:37:23,839
i encourage you to check out that blog

00:37:20,960 --> 00:37:23,839
post

00:37:24,560 --> 00:37:30,400
so like it's kafka is just a tool

00:37:28,400 --> 00:37:31,920
it's one of many and it's part of a

00:37:30,400 --> 00:37:33,839
broader thing you can use lots of other

00:37:31,920 --> 00:37:36,480
databases

00:37:33,839 --> 00:37:37,599
analytics engines other data sources

00:37:36,480 --> 00:37:39,839
they all work together

00:37:37,599 --> 00:37:41,839
but this is a just a great way a great

00:37:39,839 --> 00:37:43,119
tool for tying all those things together

00:37:41,839 --> 00:37:44,960
so whether you're building a something

00:37:43,119 --> 00:37:46,480
like new york times have done here

00:37:44,960 --> 00:37:48,720
with a full streaming application or

00:37:46,480 --> 00:37:51,359
whether you're using it to decouple your

00:37:48,720 --> 00:37:51,760
your um yeah feel free to throw

00:37:51,359 --> 00:37:52,960
questions

00:37:51,760 --> 00:37:55,920
i know you almost use them all the time

00:37:52,960 --> 00:37:57,920
sorry i thought this would go faster um

00:37:55,920 --> 00:38:00,079
uh what i've used coffee for most of the

00:37:57,920 --> 00:38:00,720
times has been just to decouple micro

00:38:00,079 --> 00:38:03,119
services

00:38:00,720 --> 00:38:04,320
um get rid of all that http chatter and

00:38:03,119 --> 00:38:06,720
it's been great for that it really

00:38:04,320 --> 00:38:08,960
clings over microservices architecture

00:38:06,720 --> 00:38:10,720
um here's a slide i mentioned just shows

00:38:08,960 --> 00:38:12,240
the patchy kafka from the patchy copper

00:38:10,720 --> 00:38:14,400
website which shows 80 percent of all

00:38:12,240 --> 00:38:15,680
fortune 100 companies are using kafka

00:38:14,400 --> 00:38:17,760
to some degree or another and their

00:38:15,680 --> 00:38:19,359
usage of it is growing that's

00:38:17,760 --> 00:38:21,359
what will often happen once kafka lands

00:38:19,359 --> 00:38:22,000
in an enterprise it grows from there

00:38:21,359 --> 00:38:23,839
because

00:38:22,000 --> 00:38:25,440
it's so useful people start seeing it

00:38:23,839 --> 00:38:26,880
other teams start seeing the value of it

00:38:25,440 --> 00:38:28,480
and it's grows

00:38:26,880 --> 00:38:30,160
um i've seen it happen in companies i've

00:38:28,480 --> 00:38:31,359
been at where i was just at and i've

00:38:30,160 --> 00:38:32,720
seen it now that i'm at consular

00:38:31,359 --> 00:38:33,520
outreach speaking i just joined

00:38:32,720 --> 00:38:35,200
confluence

00:38:33,520 --> 00:38:36,720
this last month i used to work for a

00:38:35,200 --> 00:38:37,920
healthcare company called centene which

00:38:36,720 --> 00:38:39,280
when i submitted this talk that's where

00:38:37,920 --> 00:38:41,119
i was working

00:38:39,280 --> 00:38:42,400
but i just was so impressed with the

00:38:41,119 --> 00:38:44,000
company confluent

00:38:42,400 --> 00:38:46,000
and the way they supported the community

00:38:44,000 --> 00:38:47,200
like i said software communities are

00:38:46,000 --> 00:38:49,680
important to me

00:38:47,200 --> 00:38:50,640
and the kafka community has is it's just

00:38:49,680 --> 00:38:52,240
so vibrant and

00:38:50,640 --> 00:38:54,640
it's just it's been such a joy to be a

00:38:52,240 --> 00:38:58,000
part of that community um

00:38:54,640 --> 00:38:59,280
and so um the confidence supports it so

00:38:58,000 --> 00:39:00,720
well i'm really just really happy with

00:38:59,280 --> 00:39:01,760
that so here's some resources that they

00:39:00,720 --> 00:39:03,440
provide that you can

00:39:01,760 --> 00:39:04,960
check out i really would encourage you

00:39:03,440 --> 00:39:06,560
these books they're all free

00:39:04,960 --> 00:39:08,560
but they're very good books they're not

00:39:06,560 --> 00:39:09,680
they're not uh market aware and they're

00:39:08,560 --> 00:39:11,440
just really good books

00:39:09,680 --> 00:39:12,400
also the meetups those kafka meetups are

00:39:11,440 --> 00:39:13,760
happening all the time i really

00:39:12,400 --> 00:39:15,119
encourage you to join those

00:39:13,760 --> 00:39:17,599
so the question here someone asked about

00:39:15,119 --> 00:39:18,960
um hosted coffee services the one i

00:39:17,599 --> 00:39:21,040
recommend the most would be

00:39:18,960 --> 00:39:22,000
constant cloud uh there's some others

00:39:21,040 --> 00:39:24,079
out there msk

00:39:22,000 --> 00:39:25,440
and there's some others available too

00:39:24,079 --> 00:39:28,480
the thing i really like about confluent

00:39:25,440 --> 00:39:31,440
cloud is that um because

00:39:28,480 --> 00:39:33,040
the 80 of the committers on kafka are

00:39:31,440 --> 00:39:35,119
working for consulate

00:39:33,040 --> 00:39:37,040
they basically get a head start link so

00:39:35,119 --> 00:39:38,720
the it's the most up-to-date of all the

00:39:37,040 --> 00:39:39,680
hostess services you'll be most likely

00:39:38,720 --> 00:39:43,200
to see the latest

00:39:39,680 --> 00:39:44,720
versions when a new version of kafka

00:39:43,200 --> 00:39:45,839
comes out it'll be those features will

00:39:44,720 --> 00:39:47,040
be in constant cloud

00:39:45,839 --> 00:39:49,520
long before they'll be in most of the

00:39:47,040 --> 00:39:50,800
other hosted services um it's it's

00:39:49,520 --> 00:39:51,920
they've got some pay as you go they

00:39:50,800 --> 00:39:53,680
don't have a free tier but they have a

00:39:51,920 --> 00:39:55,680
basic here that's very inexpensive

00:39:53,680 --> 00:39:57,040
and you can get coupons which i don't

00:39:55,680 --> 00:39:58,640
have them with me unfortunately you can

00:39:57,040 --> 00:40:00,720
find coupons all over the internet

00:39:58,640 --> 00:40:03,920
that'll give you so many dollars of free

00:40:00,720 --> 00:40:05,440
usage to kick the tires um

00:40:03,920 --> 00:40:07,520
but i i would recommend constant cloud

00:40:05,440 --> 00:40:08,800
it's so it's a great service

00:40:07,520 --> 00:40:10,400
and it's just growing all the time this

00:40:08,800 --> 00:40:11,839
is amazing the team that's working on

00:40:10,400 --> 00:40:13,680
it's really adding a lot of features

00:40:11,839 --> 00:40:15,359
quickly and then just about the time so

00:40:13,680 --> 00:40:15,920
again the meetups there's one coming up

00:40:15,359 --> 00:40:18,560
next week

00:40:15,920 --> 00:40:19,359
um on kafka streams it's a tutorial

00:40:18,560 --> 00:40:21,119
workshop

00:40:19,359 --> 00:40:22,400
by bill the jack who's the author of

00:40:21,119 --> 00:40:25,040
caucus games in action

00:40:22,400 --> 00:40:26,400
and uh i really recommend that meet up

00:40:25,040 --> 00:40:28,640
um

00:40:26,400 --> 00:40:30,800
yeah kafka streams kafka really helps

00:40:28,640 --> 00:40:31,680
with etl workflows and jason's asked

00:40:30,800 --> 00:40:34,079
that question

00:40:31,680 --> 00:40:35,359
um i don't i haven't used that much i

00:40:34,079 --> 00:40:37,040
didn't come from a big data background

00:40:35,359 --> 00:40:38,640
but i know folks who have

00:40:37,040 --> 00:40:40,720
robin moffitt one of my co now

00:40:38,640 --> 00:40:42,880
co-workers came from that background

00:40:40,720 --> 00:40:43,920
and he's got a great talk on on

00:40:42,880 --> 00:40:45,760
streaming etl

00:40:43,920 --> 00:40:48,880
um which you can find and you can google

00:40:45,760 --> 00:40:51,119
that robin moffitt.apd

00:40:48,880 --> 00:40:52,480
or just you know google streaming hdtl

00:40:51,119 --> 00:40:54,480
with kafka and you'll find his talks

00:40:52,480 --> 00:40:56,240
probably

00:40:54,480 --> 00:40:57,920
and it's it's used to connect a lot of

00:40:56,240 --> 00:40:59,040
the other big data pieces that you're

00:40:57,920 --> 00:41:02,079
used to seeing

00:40:59,040 --> 00:41:04,960
um and using kafka and copy connect as

00:41:02,079 --> 00:41:07,200
the main pipe so it's a great tool for

00:41:04,960 --> 00:41:08,000
that um

00:41:07,200 --> 00:41:09,680
another thing i want to mention real

00:41:08,000 --> 00:41:11,760
quick is the community slack there's uh

00:41:09,680 --> 00:41:13,359
it's over 26 000 numbers

00:41:11,760 --> 00:41:14,960
in that slack group right now with over

00:41:13,359 --> 00:41:17,760
50 channels a very different

00:41:14,960 --> 00:41:19,280
aspect of kafka it's hosted by confluent

00:41:17,760 --> 00:41:21,280
but it's an apache kafka

00:41:19,280 --> 00:41:22,560
slack it's really it's for you even if

00:41:21,280 --> 00:41:23,440
you don't interrupt your confidence in

00:41:22,560 --> 00:41:25,280
any way

00:41:23,440 --> 00:41:26,800
if you use kafka you want to be on the

00:41:25,280 --> 00:41:28,640
slack roof and it's

00:41:26,800 --> 00:41:30,400
very active very helpful people a lot of

00:41:28,640 --> 00:41:32,480
the consulate engineers who do most of

00:41:30,400 --> 00:41:34,079
the work on kafka

00:41:32,480 --> 00:41:36,000
they are often in there answering

00:41:34,079 --> 00:41:38,160
questions and they're very helpful

00:41:36,000 --> 00:41:39,599
and then finally um you can reach me on

00:41:38,160 --> 00:41:40,640
twitter or at my email they're both on

00:41:39,599 --> 00:41:43,839
the slide there

00:41:40,640 --> 00:41:49,599
um so um i think i'm

00:41:43,839 --> 00:41:51,920
out of time is that right jay

00:41:49,599 --> 00:41:53,599
thank you for for watching and listening

00:41:51,920 --> 00:41:54,480
and again if you have any questions down

00:41:53,599 --> 00:41:56,640
the road after this

00:41:54,480 --> 00:41:58,560
feel free to reach out to me dms are

00:41:56,640 --> 00:41:59,119
open oh i can still do some q a okay

00:41:58,560 --> 00:42:00,400
great

00:41:59,119 --> 00:42:01,440
so if there's any more any more

00:42:00,400 --> 00:42:02,960
questions you go ahead and post them

00:42:01,440 --> 00:42:04,800
there now or like i said you can email

00:42:02,960 --> 00:42:07,440
me or reach me on twitter later on

00:42:04,800 --> 00:42:08,880
um the coffee community is pretty active

00:42:07,440 --> 00:42:10,640
on twitter there's a

00:42:08,880 --> 00:42:12,079
i've got a twitter list that you can

00:42:10,640 --> 00:42:14,480
find from my from my

00:42:12,079 --> 00:42:16,319
profile that's got a lot of the coffee

00:42:14,480 --> 00:42:17,359
community members um there's just some

00:42:16,319 --> 00:42:18,400
really neat people they're all over the

00:42:17,359 --> 00:42:19,440
world and that's another thing i want to

00:42:18,400 --> 00:42:19,839
mention about the meet up so just

00:42:19,440 --> 00:42:21,839
they're

00:42:19,839 --> 00:42:23,119
because they're not they're virtual now

00:42:21,839 --> 00:42:25,119
obviously i actually

00:42:23,119 --> 00:42:26,640
co-organized one in st louis we had one

00:42:25,119 --> 00:42:28,240
event before the virus hit and now

00:42:26,640 --> 00:42:29,680
they've all been virtual ever since then

00:42:28,240 --> 00:42:33,119
but it's great now you can join them all

00:42:29,680 --> 00:42:35,280
over the place i often will join um

00:42:33,119 --> 00:42:36,640
the meetups in europe or the uk around

00:42:35,280 --> 00:42:38,480
lunchtime so

00:42:36,640 --> 00:42:40,240
which is you know great timing for for

00:42:38,480 --> 00:42:41,520
us in the in the us

00:42:40,240 --> 00:42:42,560
but i've also been known to stay up in

00:42:41,520 --> 00:42:44,079
the middle of the night to catch those

00:42:42,560 --> 00:42:46,640
over in australia

00:42:44,079 --> 00:42:48,000
so um it's it's a really a great

00:42:46,640 --> 00:42:49,280
resource a great community i would

00:42:48,000 --> 00:42:50,560
encourage you to check out the meetups

00:42:49,280 --> 00:42:51,760
and if you go to the meetup hub you'll

00:42:50,560 --> 00:42:53,839
see all the ones that are coming up as

00:42:51,760 --> 00:42:55,520
well as recordings from past and depths

00:42:53,839 --> 00:42:59,839
that picture there is from a past meetup

00:42:55,520 --> 00:42:59,839
you can check out

00:43:00,560 --> 00:43:04,640
oh any any last questions there

00:43:07,040 --> 00:43:13,839
okay all right well thank you all thanks

00:43:09,119 --> 00:43:13,839

YouTube URL: https://www.youtube.com/watch?v=6R4ORJz891E


