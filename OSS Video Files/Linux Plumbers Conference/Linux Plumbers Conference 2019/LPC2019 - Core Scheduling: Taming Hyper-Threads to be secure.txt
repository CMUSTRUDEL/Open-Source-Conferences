Title: LPC2019 - Core Scheduling: Taming Hyper-Threads to be secure
Publication date: 2019-09-17
Playlist: Linux Plumbers Conference 2019
Description: 
	Core Scheduling: Taming Hyper-Threads to be secure

Speakers
 Julien Desfossez (DigitalOcean)
 Vineeth Remanan Pillai

Description
Last couple of years, we have witnessed an onslaught of vulnerabilities in the design and architecture of cpus. It is interesting and surprising to note that the vulnerabilities are mainly targeting the features designed to improve the performance of cpus - most notable being the hyperthreading(smt). While some of the vulnerabilities could be mitigated in software and cpu microcodes, couple of others didn't have any satisfiable mitigation other than making sure that smt is off and every context switch needed to flush the cache to clear the data used by the task that is being switched out. Turning smt off is not a viable alternative to many production scenarios like cloud environment where you lose a considerable amount of computing power by turning off smt. To address this, there have been community efforts to keep smt on while trying to make sure that non-trusting applications are never run concurrently in the hyperthreads of the core, they have been widely called as core scheduling.

This talk is about the development, testing and profiling efforts of core scheduling in the community. There were multiple proof of concepts - while differing in the design, ultimately trying to make sure that only mutually trusted applications run concurrently on the core. We discuss the design, implementation and performance of the POCs. We also discuss the profiling attempts to understand the correctness and performance of the patches - various powerful kernel features that we leveraged to get the most time sensitive data from the kernel to understand the effect of scheduler with the core scheduling feature. We plan to conclude with a brief discussion of the future directions of core scheduling.

The core idea about core scheduling is to have smt on and make sure that only trusted applications run concurrently on siblings of a core. If there are no group of trusting applications runnable on the core, we need to make sure that remaining siblings should idle while applications run in isolation on the core. This should also consider the performance aspects of the system. Theoretically it is impossible to reach the same level of performance where the cores are allowed to any runnable applications. But if the performance of core scheduling is worse than or same as the smt off situation, we do not gain anything from this feature other than the added complexity in the scheduler. So the idea is to achieve a considerable boost in performance compared to smt-off for the majority of production workloads.

Security boundary is another aspect of critical importance in core scheduling. What should be considered as a trust boundary? Should it be at the user/group level, process level or thread level? Should kernel be considered trusty by applications or vice-versa? With virtualization and nested virtualization in picture, this gets even more complicated. But answers to most of these questions are environment and workload dependent and hence these are implemented as policies rather than hardcoding in the code. And then arises the question - how the policies should be implemented? Kernel has a variety of mechanisms to implement these kind of policies and the proof of concepts posted upstream mainly uses cgroups. This talk also discusses other viable options for implementing the policies.
Captions: 
	00:00:00,010 --> 00:00:03,980
- Okay, so our next talk is by Julien and Vineeth here.

00:00:03,980 --> 00:00:06,730
And it's about securing hyperflooding,

00:00:06,730 --> 00:00:09,710
so Vineeth and Julien are kernel and performance engineers

00:00:09,710 --> 00:00:12,590
at Digital Ocean, their current mission

00:00:12,590 --> 00:00:14,950
is keeping up with security vulnerabilities

00:00:14,950 --> 00:00:17,800
while achieving the best possible performance

00:00:17,800 --> 00:00:19,583
out of their servers.

00:00:19,583 --> 00:00:21,840
For that matter, their talk today is about

00:00:21,840 --> 00:00:23,440
the development of core scheduling feature

00:00:23,440 --> 00:00:26,660
and they try to deliver that feature to allow

00:00:26,660 --> 00:00:28,930
hyperthreading to be secure.

00:00:28,930 --> 00:00:31,480
All right, and with this I'll pass it on to Julien.

00:00:32,563 --> 00:00:34,500
- Thanks, so hello everyone.

00:00:34,500 --> 00:00:35,882
Thank you for being here today.

00:00:35,882 --> 00:00:39,060
So today we are going to talk about core scheduling

00:00:39,060 --> 00:00:42,970
and it's a feature that has been under work

00:00:42,970 --> 00:00:44,907
for about a year on the (mumbles).

00:00:46,230 --> 00:00:51,230
And it was, the goal is to really make the user

00:00:51,280 --> 00:00:54,670
of hyperthread secure in regards to the private security

00:00:54,670 --> 00:00:55,503
even (mumbles).

00:00:56,800 --> 00:01:01,070
So today we are going to show you why we need this feature,

00:01:01,070 --> 00:01:02,630
what is actually this feature,

00:01:02,630 --> 00:01:05,920
and what were the roadblocks and problem

00:01:05,920 --> 00:01:08,923
we encountered during the past year.

00:01:10,220 --> 00:01:12,710
And how we tested and what are the next steps.

00:01:12,710 --> 00:01:16,620
So a lot of people have interacted on the main lists,

00:01:16,620 --> 00:01:19,490
not just us, it's a community effort.

00:01:19,490 --> 00:01:21,810
So and people are in the room, as well,

00:01:21,810 --> 00:01:24,230
so if you have any questions feel free to interrupt

00:01:24,230 --> 00:01:26,850
and I'm sure you will get answers.

00:01:26,850 --> 00:01:31,110
So in the past years we have seen an increase

00:01:31,110 --> 00:01:35,480
in side-channel attacks and so we have seen meltdown,

00:01:35,480 --> 00:01:37,760
we have seen spectre variant one, spectre variant two,

00:01:37,760 --> 00:01:41,770
L1TF, MDS, most of those have been fixed

00:01:41,770 --> 00:01:45,220
in the kernel, in the microcode updates.

00:01:45,220 --> 00:01:49,270
But some of those vulnerabilities still have issues

00:01:49,270 --> 00:01:52,140
with hyperthreading and SMT in general.

00:01:52,140 --> 00:01:55,374
So for L1TF and MDS in particular,

00:01:55,374 --> 00:01:58,590
there are no mitigations that are SMT safe.

00:01:58,590 --> 00:02:01,800
So the fact that we have shared resources

00:02:01,800 --> 00:02:04,160
increases the attack vector

00:02:04,160 --> 00:02:07,350
and the only safe situation currently

00:02:07,350 --> 00:02:09,930
is that we need to disable hyperthreading

00:02:09,930 --> 00:02:10,953
to be safe.

00:02:12,467 --> 00:02:15,260
Hyperthreading is (mumbles) feature,

00:02:17,770 --> 00:02:20,320
we have some of these cases where we see

00:02:21,325 --> 00:02:23,890
a not simple drop in performance when we disable

00:02:23,890 --> 00:02:26,720
hyperthreading so that's why we are really interested

00:02:26,720 --> 00:02:31,490
to see if we are able to make sure that non-trusting threads

00:02:31,490 --> 00:02:35,470
never get to share the resources exposed by SMT.

00:02:35,470 --> 00:02:38,210
And that's the core scheduling feature.

00:02:38,210 --> 00:02:42,203
So now we need to go (mumbles) in the future.

00:02:43,590 --> 00:02:44,473
- Thanks, Julien.

00:02:46,200 --> 00:02:51,200
So as he explained, disabling SMT is the safest way to go

00:02:51,580 --> 00:02:54,794
and by core scheduling we are trying to make sure

00:02:54,794 --> 00:02:59,060
we are trying to achieve a state where we can keep

00:02:59,060 --> 00:03:02,780
a SMT owned then trying to make it more secure.

00:03:02,780 --> 00:03:04,723
So these are the details.

00:03:05,580 --> 00:03:07,160
These are the main bullet points

00:03:08,070 --> 00:03:10,320
to guiding core scheduling where you need to have

00:03:10,320 --> 00:03:12,510
a core-wide knowledge when you try to schedule

00:03:12,510 --> 00:03:16,260
on to the siblings and only trusted proocesses

00:03:16,260 --> 00:03:19,090
should be able to execute at the same time on a core

00:03:19,090 --> 00:03:21,720
and if you cannot find a match,

00:03:21,720 --> 00:03:24,590
of there are no two trusted processes on the core

00:03:24,590 --> 00:03:27,310
then one of the sibling has to go idle.

00:03:27,310 --> 00:03:29,020
It has to be forced idle.

00:03:29,020 --> 00:03:31,160
And load balancing is another thing

00:03:31,160 --> 00:03:35,560
where if you cannot find a trusted process

00:03:35,560 --> 00:03:39,660
to run on one sibling then probably take it

00:03:39,660 --> 00:03:43,313
from somewhere else so that you don't waste the performance.

00:03:45,380 --> 00:03:49,310
So there are two cases here I tried to explain pictorially.

00:03:49,310 --> 00:03:53,800
So in this specific case you have a matching scenario.

00:03:53,800 --> 00:03:57,700
So those are the two run cues of which it's ordered

00:03:57,700 --> 00:04:02,680
in the order of priority, so task C is the highest priority

00:04:02,680 --> 00:04:06,150
on run cue one and task O is the highest priority

00:04:06,150 --> 00:04:07,177
in run cue two.

00:04:08,990 --> 00:04:10,630
And that's a grouping,

00:04:10,630 --> 00:04:13,200
like group C is a group of trusted process,

00:04:13,200 --> 00:04:15,963
same way as group X is a set of trusted processes.

00:04:17,634 --> 00:04:19,650
And if task C is higher priority than task O

00:04:19,650 --> 00:04:22,560
it would schedule task C and N,

00:04:22,560 --> 00:04:24,410
which is on the same group.

00:04:24,410 --> 00:04:27,120
But if task O is the highest priority

00:04:27,120 --> 00:04:30,560
of the whole core, then task O selects task A,

00:04:30,560 --> 00:04:32,893
and those two get to schedule on the core.

00:04:35,220 --> 00:04:38,470
On the second case here, these is no task match.

00:04:38,470 --> 00:04:40,230
For example, on run cue two,

00:04:40,230 --> 00:04:42,950
if task C is the highest priority on the core

00:04:42,950 --> 00:04:46,220
but it doesn't have a match on run cue two,

00:04:46,220 --> 00:04:48,913
then you basically force idle on cpu two.

00:04:50,100 --> 00:04:51,550
But on the other case,

00:04:51,550 --> 00:04:54,240
if task O is the highest priority one,

00:04:54,240 --> 00:04:57,867
we just ungroup, you have a match, which is task A,

00:04:57,867 --> 00:05:00,776
and you get to schedule task O and task A

00:05:00,776 --> 00:05:03,463
on the two siblings.

00:05:05,940 --> 00:05:07,910
Now coming to the history,

00:05:07,910 --> 00:05:09,610
the very first implementation

00:05:09,610 --> 00:05:11,980
was very virtual mission specific,

00:05:11,980 --> 00:05:14,202
it was specific for KVM,

00:05:14,202 --> 00:05:19,202
that trust boundary was all of your cpu threads

00:05:19,610 --> 00:05:21,970
could be on the same VM and could be trusted.

00:05:21,970 --> 00:05:24,630
So if any of your cpu thread runnable

00:05:24,630 --> 00:05:27,050
and if there is no VCP thread from the same VM

00:05:27,050 --> 00:05:29,823
on the other sibling, you basically force it idle.

00:05:31,190 --> 00:05:34,200
And then came the generic core scheduling iteration

00:05:34,200 --> 00:05:37,180
where instead of the trust boundary

00:05:37,180 --> 00:05:40,928
being virtual CPU threads, you have processes

00:05:40,928 --> 00:05:43,740
grouped as a trusted boundary.

00:05:43,740 --> 00:05:47,270
So you can group a list of processes to be

00:05:47,270 --> 00:05:50,463
on a single trusted group and then make it core schedulable.

00:05:52,460 --> 00:05:54,770
How to group this by the initial prototype

00:05:54,770 --> 00:05:58,483
uses cpu cgroups because it was quick and easy to prototype.

00:06:00,742 --> 00:06:04,223
Going a bit detail into the KVM based approach,

00:06:05,640 --> 00:06:08,380
as I mentioned before we have VCPU threads

00:06:08,380 --> 00:06:12,172
that's of the same VMS as the trusted group

00:06:12,172 --> 00:06:16,003
and since you need to have a corewide knowledge

00:06:16,003 --> 00:06:20,020
about the group, you basically use

00:06:20,020 --> 00:06:21,950
a sched domain shared state there

00:06:21,950 --> 00:06:24,580
to have the corewide information

00:06:24,580 --> 00:06:27,330
and whenever your cpu thread is runnable

00:06:27,330 --> 00:06:31,080
you IPA the sibling, and the sibling comes into schedule

00:06:31,080 --> 00:06:35,410
and then it tries to find a match of the same VCPU

00:06:35,410 --> 00:06:37,730
from the same VM, and if it cannot find a match

00:06:37,730 --> 00:06:40,400
it basically schedules idle.

00:06:40,400 --> 00:06:42,600
And the matching logic also took care of

00:06:42,600 --> 00:06:44,150
the other synchronization points

00:06:44,150 --> 00:06:47,163
like if a thread does a VM exit

00:06:47,163 --> 00:06:49,820
then the sibling pauses,

00:06:49,820 --> 00:06:52,260
or if the interrupt to the sibling pauses

00:06:52,260 --> 00:06:53,600
similarly on the schedule also

00:06:53,600 --> 00:06:55,373
you have the synchronization point.

00:06:58,100 --> 00:07:00,110
This is about the generic approach

00:07:00,110 --> 00:07:01,360
instead of the VCPU threads

00:07:01,360 --> 00:07:03,640
you have processes at the boundary

00:07:03,640 --> 00:07:07,420
and the idea is more or less similar

00:07:07,420 --> 00:07:11,050
but instead of VCPU threads now we have a set of processes

00:07:11,050 --> 00:07:14,180
and the idea of matching logic is (mumbles) to be different.

00:07:14,180 --> 00:07:17,510
Here what happens is a schedule on a sibling

00:07:17,510 --> 00:07:19,800
takes care of picking a process

00:07:19,800 --> 00:07:21,950
for the other sibling as well.

00:07:21,950 --> 00:07:24,490
So instead of sending an IP identically,

00:07:24,490 --> 00:07:27,180
the schedule on one sibling picks a process

00:07:27,180 --> 00:07:29,360
for the other sibling and then lets it know,

00:07:29,360 --> 00:07:31,270
okay, I have already picked one for you,

00:07:31,270 --> 00:07:33,170
you just need to execute it.

00:07:33,170 --> 00:07:36,273
And if a match cannot be found it basically forces idle.

00:07:37,729 --> 00:07:40,950
These are the details how we do that.

00:07:40,950 --> 00:07:45,720
So the policy how you group is using the cpu cgroups

00:07:45,720 --> 00:07:47,930
and there is a new file cpu.tag.

00:07:47,930 --> 00:07:50,260
If you set it to one, all the processes

00:07:50,260 --> 00:07:53,700
in that cpu group comes into one group

00:07:53,700 --> 00:07:57,633
and it's identified by a cookie, it's a 64-bit value.

00:07:59,720 --> 00:08:03,850
And every run cue actually maintains an rbtree

00:08:03,850 --> 00:08:07,090
to search the tagged processes.

00:08:07,090 --> 00:08:10,140
So when a sibling sees that the tagged process

00:08:10,140 --> 00:08:13,590
is going to be scheduled, it uses this rbtree

00:08:13,590 --> 00:08:16,750
to search on the other sibling to find a match

00:08:16,750 --> 00:08:19,283
and if there is no match, it forces idle.

00:08:22,497 --> 00:08:24,740
Up 'til there have been three iterations,

00:08:24,740 --> 00:08:27,320
v3 is the one that we are discussing on the list now.

00:08:27,320 --> 00:08:31,480
v2 was mainly about fixing build and stability issues

00:08:31,480 --> 00:08:35,480
and v3 was about fixing the core logic's bug

00:08:35,480 --> 00:08:36,413
that we had in v2.

00:08:38,580 --> 00:08:41,230
These are the issued that we find now,

00:08:41,230 --> 00:08:46,230
so now we have a core wide knowledge about scheduling,

00:08:46,860 --> 00:08:49,650
so we need to compare the priorities of processes

00:08:49,650 --> 00:08:52,330
in multiple CPUs in the same core.

00:08:52,330 --> 00:08:55,672
And for that for the (mumbles) class we use vruntime

00:08:55,672 --> 00:08:57,890
to compare the priorities

00:08:57,890 --> 00:08:59,790
but vruntime is not really the same

00:08:59,790 --> 00:09:03,970
for comparing the priority, it closes cpus.

00:09:03,970 --> 00:09:05,840
So that has been a main issue.

00:09:05,840 --> 00:09:08,180
And v1 was blindly comparing vruntime

00:09:09,530 --> 00:09:12,340
and that had starvation issues.

00:09:12,340 --> 00:09:15,320
I'll be discussing more about that in the latest slides.

00:09:15,320 --> 00:09:17,530
Second thing is about force idle.

00:09:17,530 --> 00:09:19,350
So you force idle a CPU

00:09:19,350 --> 00:09:21,690
to make the other sibling run securely

00:09:21,690 --> 00:09:26,690
but the forced idle CPU, it takes a long time to come back

00:09:27,180 --> 00:09:30,370
online to schedule a new thread.

00:09:30,370 --> 00:09:32,680
So that has been one issue.

00:09:32,680 --> 00:09:35,290
And another issue, strange issue that we were seeing

00:09:35,290 --> 00:09:40,130
is difference in performance when we do tagging.

00:09:40,130 --> 00:09:42,180
So you have process A and B

00:09:42,180 --> 00:09:44,710
so is you tag process A and untag process B

00:09:44,710 --> 00:09:45,930
you see a difference.

00:09:45,930 --> 00:09:48,460
If you untag process A and tag process B

00:09:48,460 --> 00:09:50,240
the performances are different

00:09:50,240 --> 00:09:51,610
and if you have two different tags,

00:09:51,610 --> 00:09:52,870
the performance is again different.

00:09:52,870 --> 00:09:56,123
So we had performance issues with the way we'd tag.

00:09:58,380 --> 00:10:01,840
This is the first fix that we came up with

00:10:01,840 --> 00:10:03,580
regarding the vruntime issue.

00:10:03,580 --> 00:10:06,990
So since vruntime issue cannot be compared

00:10:06,990 --> 00:10:09,700
we use the same logic that (mumbles) scheduler

00:10:09,700 --> 00:10:12,750
uses for migrating a process through a different run cue.

00:10:12,750 --> 00:10:15,688
So it's like you're decrimming the minvruntime

00:10:15,688 --> 00:10:17,780
from the source run cue

00:10:17,780 --> 00:10:20,570
and implementing with the (mumbles) run cue.

00:10:20,570 --> 00:10:25,570
And it did help, it fixed couple of starvation issues

00:10:25,850 --> 00:10:27,763
but still it was not perfect.

00:10:29,360 --> 00:10:31,550
So this is one of the corner cases

00:10:31,550 --> 00:10:33,950
where we were seeing starvation issues

00:10:33,950 --> 00:10:37,020
with the normalized vruntime.

00:10:37,020 --> 00:10:40,490
So it's a bit intense, I'll try to explain.

00:10:40,490 --> 00:10:44,170
So say you have two sibling cpu one and cpu two,

00:10:44,170 --> 00:10:46,650
and cpu two has only one task, task B.

00:10:46,650 --> 00:10:48,300
Cpu one was idle.

00:10:48,300 --> 00:10:51,137
At a later point of time task A is in cue

00:10:51,137 --> 00:10:54,730
then the matching logic picks task A

00:10:54,730 --> 00:10:56,910
but it's incompatible with task B

00:10:56,910 --> 00:10:58,853
so cpu two is forced idle.

00:10:59,720 --> 00:11:02,260
And then again one more schedule even comes

00:11:03,580 --> 00:11:05,670
you do the normalization

00:11:05,670 --> 00:11:08,243
but before normalization since task A

00:11:08,243 --> 00:11:10,420
is the only task running on cpu,

00:11:10,420 --> 00:11:15,070
the run cue's minvruntime and task A's run time is equal.

00:11:15,070 --> 00:11:18,651
It's progressing together and there is only one task.

00:11:18,651 --> 00:11:23,180
Similarly on cpu two, it's idle, it's not progressing,

00:11:23,180 --> 00:11:26,270
so the minvruntime of the run cue and the task vruntime

00:11:26,270 --> 00:11:27,580
is same.

00:11:27,580 --> 00:11:29,640
So now we start the normalization

00:11:29,640 --> 00:11:31,950
from the base vruntime we will decrement

00:11:31,950 --> 00:11:34,620
run cue's minvruntime by the same

00:11:34,620 --> 00:11:38,280
so it becomes zero then we add our cue one's minvruntime

00:11:39,380 --> 00:11:43,650
so now task A's run time and B's normalized run time

00:11:43,650 --> 00:11:47,960
is equal so when we compare and if both are equal

00:11:47,960 --> 00:11:49,590
you select the first task.

00:11:49,590 --> 00:11:51,370
So task A is selected again,

00:11:51,370 --> 00:11:53,950
and cpu two is forced idle again.

00:11:53,950 --> 00:11:58,510
This goes on until a new end cue or B cue happens.

00:11:58,510 --> 00:12:01,370
So task B never gets a chance to execute

00:12:01,370 --> 00:12:03,343
and we see this starvation issue.

00:12:06,030 --> 00:12:08,250
Now regarding the forced idle,

00:12:08,250 --> 00:12:11,040
this is also some corner cases that we were seeing

00:12:11,040 --> 00:12:14,030
so consider the same case where you have two cpus

00:12:14,030 --> 00:12:15,560
with only one task to run

00:12:16,560 --> 00:12:21,560
and task A, let it be a computing intensive task

00:12:21,820 --> 00:12:25,926
so cpu two goes forced idle and cpu one

00:12:25,926 --> 00:12:28,828
is continuously executing the only task that it has

00:12:28,828 --> 00:12:32,850
so cpu one will never get to schedule

00:12:32,850 --> 00:12:34,400
because it has only one task

00:12:34,400 --> 00:12:37,650
and cpu two never gets to schedule because scheduler

00:12:37,650 --> 00:12:41,390
thinks that it's idle even though it has been forced idle

00:12:41,390 --> 00:12:42,940
scheduler thinks that it's idle

00:12:42,940 --> 00:12:45,420
so cpu two never gets a chance to schedule.

00:12:45,420 --> 00:12:48,563
And cpu two will be forced idle for long time.

00:12:50,830 --> 00:12:53,100
These are the proposed solutions currently

00:12:53,100 --> 00:12:55,080
we are discussing in the mailing list.

00:12:55,080 --> 00:12:57,335
So for the first idle issue,

00:12:57,335 --> 00:13:00,710
the main issue is because the scheduler thinks

00:13:00,710 --> 00:13:04,060
that it's idle even though it has been forced idle.

00:13:04,060 --> 00:13:06,166
So if you can account the forced idle time

00:13:06,166 --> 00:13:08,030
using separate metrics

00:13:08,030 --> 00:13:11,020
and then specifically, explicitly trigger scheduling,

00:13:11,020 --> 00:13:12,030
that would be the way to go.

00:13:12,030 --> 00:13:14,620
That's one discussion that we have happening.

00:13:14,620 --> 00:13:17,880
And the second idea is about having a separate thread

00:13:17,880 --> 00:13:21,050
per cpu to actually do the force idle.

00:13:21,050 --> 00:13:23,730
Instead of using schedulers idle thread,

00:13:23,730 --> 00:13:25,700
if you come up with a separate idle thread

00:13:25,700 --> 00:13:27,420
just to do the force idle

00:13:27,420 --> 00:13:29,659
that way the scheduler will not be confused.

00:13:29,659 --> 00:13:33,472
And the third way, third discussion that we have

00:13:33,472 --> 00:13:35,490
is use the idle thread itself

00:13:35,490 --> 00:13:38,470
but have specific checks to distinguish between

00:13:38,470 --> 00:13:40,450
forced idle and the normal idle.

00:13:40,450 --> 00:13:43,350
That way scheduler can be less confused

00:13:43,350 --> 00:13:46,490
about what's happening when you go idle.

00:13:46,490 --> 00:13:49,083
And for the vruntime comparison across cpu,

00:13:50,380 --> 00:13:52,580
one idea is come up with a corewide run time

00:13:53,587 --> 00:13:55,716
instead of a cpu wide run time

00:13:55,716 --> 00:13:57,030
you will have a core wide run time

00:13:57,030 --> 00:13:59,710
so that the comparison becomes straightforward

00:13:59,710 --> 00:14:03,270
and another idea is go the parent,

00:14:03,270 --> 00:14:07,490
come up to a level where your comparison makes sense.

00:14:07,490 --> 00:14:11,360
You can go to the root and maybe compare two processes

00:14:11,360 --> 00:14:14,623
runtime, that way it will be much more consistent.

00:14:16,570 --> 00:14:19,180
Now I hand it over to Julien to discuss our testing

00:14:19,180 --> 00:14:21,630
and the benchmarking of course that we have done.

00:14:22,860 --> 00:14:24,340
- Thanks.

00:14:24,340 --> 00:14:27,550
So for all testing this feature we ended up having to use

00:14:27,550 --> 00:14:29,770
most of Linux tracers

00:14:29,770 --> 00:14:34,420
so the first tracing we had to do was to make sure

00:14:34,420 --> 00:14:36,305
the core scheduling was actually doing

00:14:36,305 --> 00:14:38,240
what it was supposed to do

00:14:38,240 --> 00:14:41,220
so making sure incompatible tasks were not running

00:14:41,220 --> 00:14:43,110
at the same time on the same core.

00:14:43,110 --> 00:14:46,160
So for that we used perf anf LTTng because we needed

00:14:46,160 --> 00:14:49,980
long running traces and being able to parse that in Python

00:14:49,980 --> 00:14:54,160
so those tracers output CTF traces so it was easy

00:14:54,160 --> 00:14:56,670
for us to create script.

00:14:56,670 --> 00:14:59,570
So for this specific case,

00:14:59,570 --> 00:15:02,600
really wanted to make sure known compatible tasks

00:15:02,600 --> 00:15:04,450
are running at the same time.

00:15:04,450 --> 00:15:06,100
Another interesting case for that

00:15:07,529 --> 00:15:10,600
was trying to identify when tasks are not running,

00:15:10,600 --> 00:15:12,383
when the whole core is idle.

00:15:12,383 --> 00:15:15,420
Because that was one of the problem we found

00:15:15,420 --> 00:15:19,300
at the beginning and so we needed, again,

00:15:19,300 --> 00:15:23,460
long traces for that so that's why we used those one.

00:15:23,460 --> 00:15:27,030
For debugging the actual logic of the core scheduling

00:15:27,030 --> 00:15:29,850
we used ftrace because the code is instrumented

00:15:29,850 --> 00:15:33,020
with trace_printk so it's easy for us

00:15:33,020 --> 00:15:36,210
when we have specific places in the trace

00:15:36,210 --> 00:15:40,650
to look at actually what the core scheduling logic is doing.

00:15:40,650 --> 00:15:43,000
And finally for runtime statistics

00:15:43,000 --> 00:15:48,000
we used eBPF and BPF trace to check runtime,

00:15:48,380 --> 00:15:51,470
how much time running task is off cpu.

00:15:51,470 --> 00:15:55,280
That's usually a good metric when we are not over committed

00:15:55,280 --> 00:15:59,250
just trying to see if there are some efficiency issues

00:15:59,250 --> 00:16:02,540
somewhere in the code before having to wait

00:16:02,540 --> 00:16:04,280
for the whole benchmark.

00:16:04,280 --> 00:16:09,230
So in terms of output this is one of the scripts we run

00:16:09,230 --> 00:16:13,823
that can output the data of the core scheduling statistics.

00:16:14,908 --> 00:16:17,000
So we have how much time a specific process

00:16:17,000 --> 00:16:19,330
and all its thread currently we are tagging

00:16:19,330 --> 00:16:23,081
the full process, so all the threads of the same process

00:16:23,081 --> 00:16:28,081
core scheduled during the duration of the trace.

00:16:28,750 --> 00:16:33,040
How much time the same process if core scheduled with idle

00:16:33,040 --> 00:16:37,690
so in this case it's close to 60 percent.

00:16:37,690 --> 00:16:40,350
And how much time a process has been core scheduled

00:16:40,350 --> 00:16:43,840
with another process that is not in the same tag.

00:16:43,840 --> 00:16:48,060
So that's the area we are ready to watch

00:16:48,060 --> 00:16:51,330
because we want to make sure that's not happening.

00:16:51,330 --> 00:16:54,850
We have some overlap between the two processes

00:16:54,850 --> 00:16:59,630
but it's mainly just when we send the IPI to the sibling

00:16:59,630 --> 00:17:04,180
to force idle of just to switch the process

00:17:04,180 --> 00:17:09,180
so it's not zero percent but it's inside the kernel

00:17:09,510 --> 00:17:11,733
during the IPI and scheduling code.

00:17:13,510 --> 00:17:16,060
And unknown is because at the beginning of the trace

00:17:16,060 --> 00:17:19,063
were not necessary (mumbles) of what is happening.

00:17:20,120 --> 00:17:22,080
In terms of performance validation

00:17:22,080 --> 00:17:24,950
we designed multiple micro benchmarks

00:17:24,950 --> 00:17:29,020
because for each cases we had specific worst cases

00:17:29,020 --> 00:17:33,170
so instead of having the full workload running

00:17:33,170 --> 00:17:35,770
and then trying to figure out what was happening

00:17:35,770 --> 00:17:37,790
we designed specific micro benchmarks

00:17:37,790 --> 00:17:40,550
so for example one of them right now

00:17:40,550 --> 00:17:43,090
is for the fairness issues

00:17:43,090 --> 00:17:46,810
we have two incompatible cpu intensive tasks,

00:17:46,810 --> 00:17:49,330
each of them is pinned on a different sibling

00:17:49,330 --> 00:17:50,163
of the same core

00:17:50,163 --> 00:17:52,310
so what you expect with core scheduling

00:17:52,310 --> 00:17:55,503
is to have exactly 50 percent of the time

00:17:55,503 --> 00:17:59,633
each task should be running 50 percent of the time.

00:18:01,040 --> 00:18:03,350
But right now we are seeing fairness issues

00:18:03,350 --> 00:18:07,320
and we clearly see cases where one task gets to run

00:18:07,320 --> 00:18:10,290
much more often than the other one.

00:18:10,290 --> 00:18:13,610
And we test that with (mumbles)

00:18:13,610 --> 00:18:15,330
with tagging only one process,

00:18:15,330 --> 00:18:17,040
tagging the other one and tagging the two

00:18:17,040 --> 00:18:21,570
with different tags and that's how we found the cases

00:18:21,570 --> 00:18:25,220
where the tag processes also get more chance to run.

00:18:25,220 --> 00:18:29,310
So that's one of the current micro benchmark we're using.

00:18:29,310 --> 00:18:31,395
We're also trying with overcommitted cores

00:18:31,395 --> 00:18:34,890
to see how the load balancing is working.

00:18:34,890 --> 00:18:38,443
And we have a couple of them like that.

00:18:39,630 --> 00:18:42,460
We also want to make sure we are on the right track

00:18:42,460 --> 00:18:46,395
and that we are eventually going to be able to use that

00:18:46,395 --> 00:18:49,500
on real workload and not just micro benchmarks

00:18:49,500 --> 00:18:52,010
so we have also real world scenarios

00:18:52,010 --> 00:18:54,810
where we use large virtual machines

00:18:54,810 --> 00:18:57,943
running heavy workloads like (mumbles) benchmark

00:19:00,559 --> 00:19:02,990
in 12 vcpus or more Vms.

00:19:02,990 --> 00:19:05,640
We have IO intesive Vms, cpu intensive Vms

00:19:05,640 --> 00:19:09,320
and we test them alone on the NUMA node

00:19:09,320 --> 00:19:12,380
when shared with the NUMA node and also with

00:19:12,380 --> 00:19:16,620
the noise VMs so they are just mostly idle VMs

00:19:16,620 --> 00:19:19,730
but they still need cpu time and then we can play

00:19:19,730 --> 00:19:23,690
with overcommit ratio and see how

00:19:23,690 --> 00:19:26,210
the cross configured logic behaves.

00:19:26,210 --> 00:19:30,590
So this is one of the early results we got

00:19:30,590 --> 00:19:35,330
and that's probably the best case for core scheduling

00:19:35,330 --> 00:19:36,990
is the cpu intensive Vms

00:19:36,990 --> 00:19:41,990
so in this case we have 12 vcpu VMs running in back

00:19:42,630 --> 00:19:45,130
so really cpu intensive workload

00:19:45,130 --> 00:19:48,090
on 36 logical cpu NUMA node.

00:19:48,090 --> 00:19:50,200
The VMs are floating on the node

00:19:50,200 --> 00:19:55,200
but you can count, it's 36 vcpus on 36 logical cpus.

00:19:55,380 --> 00:20:00,380
So if we disable SMT that's where we take a big hit,

00:20:01,060 --> 00:20:04,290
about 20 percent of performance impact

00:20:04,290 --> 00:20:06,870
whereas if we use cire scheduling

00:20:06,870 --> 00:20:10,050
it's almost the same as the baseline number.

00:20:10,050 --> 00:20:14,560
So that's the best case and that's why we're encouraged

00:20:14,560 --> 00:20:16,170
to continue working on core scheduling

00:20:16,170 --> 00:20:19,740
because sibling assembly will be much easier.

00:20:19,740 --> 00:20:24,740
But we see preference gain and that's our goal here.

00:20:25,010 --> 00:20:29,750
So that's the case where if you use all the cpus

00:20:29,750 --> 00:20:32,950
then core scheduling makes a lot os sense.

00:20:32,950 --> 00:20:35,800
If you only use half of the cpus,

00:20:35,800 --> 00:20:40,800
so that's the case where no SMT would not be overcommitted

00:20:41,690 --> 00:20:44,730
and we actually noticed in the main list

00:20:44,730 --> 00:20:47,500
that it's actually a preference improvement

00:20:47,500 --> 00:20:50,693
to disable hyperthreading even compared to baseline.

00:20:51,810 --> 00:20:55,410
Because we end up having less cache crashing

00:20:55,410 --> 00:20:58,470
and each thread actually has its own L1 cache

00:20:58,470 --> 00:21:03,410
and we realized that the scheduler could be made

00:21:03,410 --> 00:21:07,250
essentially aware and be more aware to place

00:21:07,250 --> 00:21:10,230
task more adequately but that's a sidetrack.

00:21:10,230 --> 00:21:13,793
It's not the focus of this, but it was interesting.

00:21:15,230 --> 00:21:18,770
In terms of IO again it's really about speed power.

00:21:18,770 --> 00:21:21,670
So in terms of IO, no major difference between

00:21:21,670 --> 00:21:23,880
no-SMT and core scheduling.

00:21:23,880 --> 00:21:27,190
If we have mixed resources or like the MySQL benchmark

00:21:27,190 --> 00:21:28,900
I was discussing before,

00:21:28,900 --> 00:21:31,590
then it becomes interesting because in this case

00:21:31,590 --> 00:21:34,490
it's a heavy workload, we have two 12 vcpus

00:21:34,490 --> 00:21:36,458
MySQL benchmark running, it's on the MySQL server,

00:21:36,458 --> 00:21:41,100
the client is hosted on a separate host.

00:21:41,100 --> 00:21:43,327
So we have network, we have the schedule,

00:21:43,327 --> 00:21:47,330
and we have cpu workload work.

00:21:47,330 --> 00:21:51,120
And on the same NUMA node we have also

00:21:51,120 --> 00:21:55,150
with 92 one vcpu idle VMs

00:21:55,150 --> 00:21:59,030
so all of that is running on 36 logical VCPUs.

00:21:59,030 --> 00:22:01,620
That's a case where no-SMT actually performs

00:22:01,620 --> 00:22:03,550
better than core scheduling.

00:22:03,550 --> 00:22:07,690
And our idea currently is that it's related to

00:22:07,690 --> 00:22:09,100
what we were discussing before

00:22:09,100 --> 00:22:12,690
that the cpu intensive tasks from the MySQL benchmark

00:22:12,690 --> 00:22:16,740
are actually preventing the idle threads

00:22:16,740 --> 00:22:17,890
to be able to perform.

00:22:17,890 --> 00:22:20,310
So that's why we are working on that,

00:22:20,310 --> 00:22:22,000
it's not the final result,

00:22:22,000 --> 00:22:23,773
it's the current state.

00:22:26,500 --> 00:22:30,259
So a good rule of thumb if you have

00:22:30,259 --> 00:22:35,200
more than 40 percent CPU idle time,

00:22:35,200 --> 00:22:37,600
then no-SMT might be a good choice

00:22:37,600 --> 00:22:39,390
but if you have less than that

00:22:39,390 --> 00:22:41,810
then core scheduling makes sense, basically.

00:22:41,810 --> 00:22:43,123
With sibling cpus.

00:22:44,320 --> 00:22:48,690
So after v3 when we fixed the fairness issues

00:22:48,690 --> 00:22:51,847
we still have lot of work to do to consider

00:22:51,847 --> 00:22:54,330
proposing for mainlining.

00:22:54,330 --> 00:22:58,840
So one of them is the process selection

00:22:58,840 --> 00:23:01,600
and matching logic between the classes.

00:23:01,600 --> 00:23:03,950
Right now it's only per class.

00:23:03,950 --> 00:23:07,693
But we would need to be more flexible about that.

00:23:08,800 --> 00:23:13,470
The big one right now is for MDS specifically

00:23:13,470 --> 00:23:16,739
because that's a vulnerability where we would need

00:23:16,739 --> 00:23:19,060
more synchronization points.

00:23:19,060 --> 00:23:20,740
We have right now with core scheduling

00:23:20,740 --> 00:23:23,760
we are protecting user space from user space

00:23:23,760 --> 00:23:26,340
but we are not protecting user space kernel

00:23:26,340 --> 00:23:29,930
from the user space so a malicious user space application

00:23:29,930 --> 00:23:34,930
could benefit from MDS to attack the host kernel.

00:23:35,180 --> 00:23:38,140
So if we want to fix that we have to add

00:23:38,140 --> 00:23:41,660
synchronization point on system cores, interrupts

00:23:41,660 --> 00:23:43,710
and also VMEXIT.

00:23:43,710 --> 00:23:45,860
And that might be very costly.

00:23:45,860 --> 00:23:49,370
So that's the current description right now

00:23:49,370 --> 00:23:52,890
for L1TF AND VM-only workload.

00:23:52,890 --> 00:23:56,880
We can get away with just having VMEXIT

00:23:56,880 --> 00:24:01,260
but for MDS on bare metal application

00:24:01,260 --> 00:24:03,720
then it needs more synchronization point

00:24:03,720 --> 00:24:07,123
and that may be, we may have to re-look at that.

00:24:09,140 --> 00:24:11,750
Last thing is also finding the right interface.

00:24:11,750 --> 00:24:16,750
We currently are using cgroup to tag the processes

00:24:16,830 --> 00:24:19,690
but we may need another interface

00:24:19,690 --> 00:24:23,180
but that's also in discussion currently.

00:24:23,180 --> 00:24:25,300
So that's it for the talk,

00:24:25,300 --> 00:24:28,560
we have plenty of time for questions.

00:24:28,560 --> 00:24:32,130
And also know that we have a micro conference this afternoon

00:24:32,130 --> 00:24:35,410
to discuss more what are the next steps.

00:24:35,410 --> 00:24:37,027
So thank you.

00:24:48,824 --> 00:24:52,241
- Does it work also for RT priority tasks

00:24:53,370 --> 00:24:55,800
or it's only normal?

00:24:55,800 --> 00:24:59,640
'Cause there are use cases where we set RT priority

00:24:59,640 --> 00:25:02,340
for virtual machines transferring.

00:25:02,340 --> 00:25:04,223
- Yeah, it does currently.

00:25:04,223 --> 00:25:09,223
The schedule actually goes through all the classes

00:25:09,234 --> 00:25:12,560
starting with the highest priority to the lowest priority.

00:25:12,560 --> 00:25:17,130
But it's not tuned for that and Peter has some patches

00:25:17,130 --> 00:25:18,840
that are ready for that so there is a lot of talk

00:25:18,840 --> 00:25:22,343
regarding that, so yeah, there are efforts going on for RTS.

00:25:24,370 --> 00:25:27,270
- Continuing on the real time topic,

00:25:27,270 --> 00:25:32,228
there are some information that you have used in the buffer

00:25:32,228 --> 00:25:37,228
(mumbles) that are global depending on the number of cpus

00:25:38,280 --> 00:25:40,900
like the admission test on sched deadline.

00:25:40,900 --> 00:25:43,610
We used a number of cpus to data mine it.

00:25:43,610 --> 00:25:47,710
And we have the RT run time share

00:25:47,710 --> 00:25:51,200
on the real time scheduler, the 541.

00:25:51,200 --> 00:25:53,793
Have you guys have a look on it,

00:25:54,800 --> 00:25:59,233
how are decisions made or are these still open problems?

00:26:00,470 --> 00:26:03,710
- As of now, there are no changes.

00:26:03,710 --> 00:26:05,260
Particularly this core scheduling patch

00:26:05,260 --> 00:26:06,950
said that we discussed currently

00:26:06,950 --> 00:26:08,520
did not have any changes specific

00:26:08,520 --> 00:26:11,220
to the things that you mentioned.

00:26:11,220 --> 00:26:13,740
So what we do in that logic is go from the

00:26:13,740 --> 00:26:15,310
highest priority sched class

00:26:15,310 --> 00:26:18,840
to the lowest priority sched class and just pick the task.

00:26:18,840 --> 00:26:21,640
So the pick task is implemented in the sched

00:26:21,640 --> 00:26:26,640
on class functions so we have not modified that part of it.

00:26:27,000 --> 00:26:28,990
So you just pick the highest priority task

00:26:28,990 --> 00:26:30,630
using the pick task function

00:26:30,630 --> 00:26:33,510
and then compares it with other core.

00:26:33,510 --> 00:26:36,920
So we did not make any changes inside the real time

00:26:36,920 --> 00:26:38,163
sched class yet.

00:26:39,820 --> 00:26:41,570
- That's one, just curious if you also

00:26:41,570 --> 00:26:45,640
try to get the (mumbles) numbers on kind of mixed thing

00:26:45,640 --> 00:26:50,390
where you try maybe to classorize the group

00:26:50,390 --> 00:26:53,950
so you want to be isolated on a partition

00:26:53,950 --> 00:26:57,042
of all the available cpus and try to make everything else

00:26:57,042 --> 00:27:01,710
on the (mumbles) here if it worked.

00:27:01,710 --> 00:27:05,560
- We did cpu pinning and we did a couple of tests

00:27:06,563 --> 00:27:09,304
where we turned SMT off, got the numbers

00:27:09,304 --> 00:27:12,230
and pinned the individual processes

00:27:12,230 --> 00:27:14,470
to see how it worked and then floated it

00:27:14,470 --> 00:27:16,730
on all the cpus to see, so yeah,

00:27:16,730 --> 00:27:20,510
we did a couple of tests and as you mentioned

00:27:24,020 --> 00:27:25,920
if it's really (mumbles) and safe

00:27:25,920 --> 00:27:28,260
then we get the benefit out of core scheduling.

00:27:28,260 --> 00:27:30,590
But is it's a mix of upload, yeah.

00:27:30,590 --> 00:27:32,140
Probably you don't want to add.

00:27:33,214 --> 00:27:35,910
- Yeah, and also we have still a lot of tests to do.

00:27:35,910 --> 00:27:38,510
For example, some pinning high yield threads

00:27:38,510 --> 00:27:41,393
on specific cpus, that's not done yet.

00:27:45,280 --> 00:27:48,430
- Have you spent any time thinking about systems

00:27:48,430 --> 00:27:51,523
with more than two HT threads per core?

00:27:54,980 --> 00:27:57,310
- The core is actually generic

00:27:57,310 --> 00:27:59,790
but we did not get the time to,

00:27:59,790 --> 00:28:02,170
we've not had the hardware to actually test it

00:28:02,170 --> 00:28:05,860
as of now there is no place where we hard cored it to two

00:28:05,860 --> 00:28:08,170
so the core is kind of very generic.

00:28:08,170 --> 00:28:12,810
We use the cpu mask to see whether there essentially

00:28:12,810 --> 00:28:13,660
of the same core.

00:28:14,830 --> 00:28:16,530
But we did not test it yet

00:28:16,530 --> 00:28:18,130
because we do not have hardware.

00:28:19,770 --> 00:28:21,670
- Thanks. - Thanks.

00:28:21,670 --> 00:28:25,030
- So the interface is cgroup.

00:28:25,030 --> 00:28:30,030
So can you tell me more about the new interface

00:28:31,374 --> 00:28:33,163
to the dyna core cookie.

00:28:35,990 --> 00:28:37,470
- There has been no proposal yet,

00:28:37,470 --> 00:28:41,150
so we were trying to get the core logic to work

00:28:41,150 --> 00:28:44,020
and now fix all the bugs.

00:28:44,020 --> 00:28:46,130
But the discussion is currently in the middle or start

00:28:46,130 --> 00:28:51,130
about maybe use PRCTL or to actually set the task groups.

00:28:52,500 --> 00:28:57,050
Because cpu cgroups might not work for every use cases

00:28:57,050 --> 00:29:00,610
because it's a privileged operation.

00:29:00,610 --> 00:29:02,760
And there are other issues about that.

00:29:02,760 --> 00:29:06,730
But it's not finalized and discussions are going on.

00:29:06,730 --> 00:29:09,980
- Another question is, there are several patches

00:29:09,980 --> 00:29:13,640
post here to fix the fairness issue on (mumbles),

00:29:13,640 --> 00:29:18,483
so is worth to pick up any (mumbles) in version four?

00:29:19,960 --> 00:29:21,393
- Can you come again?

00:29:22,231 --> 00:29:27,231
- If you want to pick any patch to fix the fairness issue

00:29:28,190 --> 00:29:29,653
to post a new version?

00:29:31,470 --> 00:29:34,420
- So as of now, I think a mix of everything works

00:29:34,420 --> 00:29:37,827
but there is no, everything has its own issues.

00:29:41,573 --> 00:29:46,000
When we take two ideas and try to clump it together

00:29:46,000 --> 00:29:50,870
it works better so it's not yet to settle the v4.

00:29:50,870 --> 00:29:52,840
It's still in the discussions

00:29:52,840 --> 00:29:56,610
but yeah, it's not yet at a place at which you want

00:29:56,610 --> 00:29:57,610
to actually proceed.

00:30:00,542 --> 00:30:03,890
- One more question, do you guys have plans to extend

00:30:03,890 --> 00:30:05,670
the mitigations kernel command line option

00:30:05,670 --> 00:30:09,490
to maybe enable core scheduling if your cpu

00:30:09,490 --> 00:30:13,040
is affected by MDS or ONTF and then maybe

00:30:13,040 --> 00:30:16,180
not enable core scheduling if you have enough processor

00:30:16,180 --> 00:30:18,540
not to be affected by those issues.

00:30:18,540 --> 00:30:22,172
- I think that's a way to go, actually.

00:30:22,172 --> 00:30:27,172
It doesn't make sense to enable this feature by default,

00:30:27,760 --> 00:30:29,990
so starting from the build process itself

00:30:29,990 --> 00:30:34,210
like if it's a config option,

00:30:34,210 --> 00:30:36,470
so starting from the build procedure itself,

00:30:36,470 --> 00:30:38,300
we tried to make sure that it doesn't interfere

00:30:38,300 --> 00:30:40,920
with the generic kernel

00:30:40,920 --> 00:30:42,710
and if it's built by default

00:30:42,710 --> 00:30:45,940
then it should be enabled by a kernel mitigation option

00:30:45,940 --> 00:30:48,970
and then once the system boots

00:30:48,970 --> 00:30:51,490
you will have to enable it by tagging.

00:30:51,490 --> 00:30:53,985
So if there is no tagged process on the system,

00:30:53,985 --> 00:30:56,850
no core part is in that world at all.

00:30:56,850 --> 00:30:59,157
So yeah, there are multiple levels for every.

00:31:00,120 --> 00:31:03,020
- Also to add to that, the administrator knows

00:31:03,020 --> 00:31:04,820
what are the groups.

00:31:04,820 --> 00:31:08,130
So we have to specify what are the trust groups.

00:31:08,130 --> 00:31:10,470
So that's why the core scheduling feature by default

00:31:10,470 --> 00:31:12,513
does nothing until tagged.

00:31:15,723 --> 00:31:17,810
- So how do you actually plan to attack the fact

00:31:17,810 --> 00:31:19,710
that even with the core scheduling bit

00:31:19,710 --> 00:31:21,240
that the kernel is still unprotected.

00:31:21,240 --> 00:31:23,280
So what do you plan there?

00:31:23,280 --> 00:31:25,627
- That's the place where we have to add

00:31:25,627 --> 00:31:27,550
the kernelization points.

00:31:27,550 --> 00:31:31,040
So right now it's only at the switch boundary,

00:31:31,040 --> 00:31:33,830
but we also need system calls and interrupts

00:31:33,830 --> 00:31:34,773
and VMEXITS.

00:31:36,133 --> 00:31:37,610
- Which will probably then make, comprise (mumbles).

00:31:39,450 --> 00:31:41,870
- Yeah, that's what we are trying to work out.

00:31:41,870 --> 00:31:43,660
And that's why a little sold that

00:31:45,320 --> 00:31:48,780
there are VM only workloads where you only run Vms

00:31:48,780 --> 00:31:51,935
and that makes more sense to use core scheduling

00:31:51,935 --> 00:31:54,740
because you have only VMEXIT to take care of, thanks.

00:31:57,729 --> 00:31:59,960
(quiet question from audience)

00:31:59,960 --> 00:32:00,983
- What was that?

00:32:02,478 --> 00:32:05,493
(laughter)

00:32:05,493 --> 00:32:08,870
- There's another proposal being discussed this week

00:32:08,870 --> 00:32:10,980
that tries to address that without having to do it

00:32:10,980 --> 00:32:11,813
synchronously.

00:32:14,770 --> 00:32:16,850
- I think we have the guys presenting it.

00:32:16,850 --> 00:32:20,440
But there's some tricks you can do with extending KPTI

00:32:20,440 --> 00:32:23,090
so that you can do some kernel execution but not all.

00:32:26,140 --> 00:32:29,837
- Hi, so I have a question that when we have two cores

00:32:29,837 --> 00:32:32,210
and one core has multiple untagged task,

00:32:33,050 --> 00:32:34,093
another core has single tagged task,

00:32:34,093 --> 00:32:36,843
then how the load balance will be handled in this case.

00:32:38,630 --> 00:32:42,090
Ideally we should not pull the task from that core, right?

00:32:42,090 --> 00:32:43,520
The busier cores.

00:32:43,520 --> 00:32:46,130
- Yeah, ideally you'll not pull it from that core.

00:32:46,130 --> 00:32:51,130
But the idea of pulling is you have a task process

00:32:51,520 --> 00:32:55,960
on one core and you are forcing the sibling to be idle.

00:32:55,960 --> 00:32:57,770
So instead of forcing the sibling to be idle

00:32:57,770 --> 00:33:01,570
if you have a task tagged process on the other core,

00:33:01,570 --> 00:33:06,570
just pull it once and then this core will be balanced

00:33:07,736 --> 00:33:08,819
from then on.

00:33:10,370 --> 00:33:11,620
- If you have not pulled the task

00:33:11,620 --> 00:33:13,890
then why not run on the other core?

00:33:13,890 --> 00:33:17,090
- You'll be only pulling task that's not running on it.

00:33:17,090 --> 00:33:17,930
So they are waiting.

00:33:17,930 --> 00:33:19,410
So if there are tasks waiting on it.

00:33:19,410 --> 00:33:20,800
So if it's already running,

00:33:20,800 --> 00:33:22,620
it doesn't make sense to pull it from there.

00:33:22,620 --> 00:33:24,730
It's only pulling from the run cue.

00:33:24,730 --> 00:33:27,260
- Okay, so this is just to balance the utilization

00:33:27,260 --> 00:33:28,100
on each core.

00:33:28,100 --> 00:33:31,897
- Exactly, and to make sure that you are making use

00:33:31,897 --> 00:33:35,423
of the SMT and enforcing security.

00:33:47,020 --> 00:33:50,103
- (mumbled question)

00:33:57,989 --> 00:34:00,100
- We have not touched those pinned tasks

00:34:00,100 --> 00:34:05,100
so it's still about by all the rules of pinning

00:34:05,400 --> 00:34:08,840
and isolation and task groups, et cetera.

00:34:08,840 --> 00:34:12,053
So only tasks that could be moved are moved.

00:34:13,790 --> 00:34:17,980
- Yeah because a lot of the exports are exploiting

00:34:17,980 --> 00:34:20,950
exactly that and they're pinning to the same cpu

00:34:20,950 --> 00:34:23,900
as the process they are trying to exploit.

00:34:23,900 --> 00:34:25,513
- So you force idle on it.

00:34:26,591 --> 00:34:30,890
Migration will not work but you still enforce

00:34:30,890 --> 00:34:33,170
one sibling to go idle.

00:34:33,170 --> 00:34:36,450
Thereby the other sibling executes in isolation.

00:34:36,450 --> 00:34:37,453
And secure.

00:34:38,660 --> 00:34:40,430
So the performance will be low

00:34:40,430 --> 00:34:42,513
but security is enforced.

00:34:44,300 --> 00:34:45,133
- I see.

00:34:48,330 --> 00:34:51,270
- With this task, how do you ensure that the system

00:34:51,270 --> 00:34:53,900
is balanced when the load balance is running?

00:34:53,900 --> 00:34:57,210
I mean, the statistic is done across all the tasks,

00:34:57,210 --> 00:35:00,470
all the thread, how do you make sure that the system

00:35:00,470 --> 00:35:01,323
stay balanced?

00:35:02,440 --> 00:35:05,410
- So there are two levels of idle balance happening,

00:35:05,410 --> 00:35:07,725
one is the system's own idle balance.

00:35:07,725 --> 00:35:11,510
And this load balance that we mentioned here

00:35:11,510 --> 00:35:15,330
kicks in only if it sees that a thread has been

00:35:15,330 --> 00:35:16,960
forced idle for long.

00:35:16,960 --> 00:35:21,761
So in a core if one other cpu has been forced idle

00:35:21,761 --> 00:35:26,240
and then it will go from the lowest sched domain

00:35:26,240 --> 00:35:27,580
to the topmost sched domain

00:35:27,580 --> 00:35:32,060
to see if there's a tagged task matching the sibling's task.

00:35:32,060 --> 00:35:33,280
Then it pulls it over.

00:35:33,280 --> 00:35:37,670
So it doesn't interfere with the system's own idle balance,

00:35:37,670 --> 00:35:40,450
it only kicks in synchronously when you see that

00:35:40,450 --> 00:35:42,050
it has been forced idle.

00:35:42,050 --> 00:35:47,050
- (mumbles) when all the cpu are used

00:35:47,460 --> 00:35:49,040
you still have to balance to make sure

00:35:49,040 --> 00:35:52,040
that you will not provide more run time to one task

00:35:52,040 --> 00:35:54,440
or one group of tasks compared to the other one.

00:35:56,690 --> 00:35:58,980
- But if the system is busy,

00:35:58,980 --> 00:36:03,210
and if there is a tagged task waiting on another cpu

00:36:03,210 --> 00:36:04,963
it makes sense to move it to here.

00:36:05,990 --> 00:36:09,030
And the system's own idle balance might pull tasks

00:36:09,030 --> 00:36:12,970
from here which are waiting back to the other task.

00:36:12,970 --> 00:36:17,970
So yeah, I see it as two different things.

00:36:20,060 --> 00:36:21,840
We haven't tested it deep enough to see

00:36:21,840 --> 00:36:23,600
if there has been border lax.

00:36:23,600 --> 00:36:25,695
But now I get your point,

00:36:25,695 --> 00:36:29,050
there can be issues where a system

00:36:29,050 --> 00:36:30,730
might be continuously load balancing

00:36:30,730 --> 00:36:33,423
due to two different, yeah, you're right.

00:36:35,150 --> 00:36:36,680
- To counter that point,

00:36:36,680 --> 00:36:39,220
have you looked at when you have a task

00:36:39,220 --> 00:36:43,330
doing load balancing to put it on a forced LL cpu

00:36:43,330 --> 00:36:46,183
of a core that matches its tag?

00:36:51,201 --> 00:36:53,310
- That's another possible issue,

00:36:53,310 --> 00:36:56,020
a new end cue might happen to a forced LL cpu

00:36:56,020 --> 00:36:58,713
because a schedule of things that it's idle.

00:36:58,713 --> 00:37:00,950
We haven't seen that in practice

00:37:00,950 --> 00:37:05,950
because most of our test cases were micro benchmarks

00:37:06,030 --> 00:37:08,750
which a calculated number of processes

00:37:08,750 --> 00:37:11,650
but when you scale up to a bigger production case

00:37:11,650 --> 00:37:12,610
we might see that.

00:37:12,610 --> 00:37:16,043
That's another thing that we need to look into, that's true.

00:37:18,222 --> 00:37:21,389
(no audible dialogue)

00:37:32,910 --> 00:37:34,616
- Well, thank you both.

00:37:34,616 --> 00:37:39,612

YouTube URL: https://www.youtube.com/watch?v=_8_xUf47-jE


