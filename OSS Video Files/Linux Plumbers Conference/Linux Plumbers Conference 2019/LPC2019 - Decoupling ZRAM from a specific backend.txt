Title: LPC2019 - Decoupling ZRAM from a specific backend
Publication date: 2019-09-20
Playlist: Linux Plumbers Conference 2019
Description: 
	Kernel Summit Track 2
Captions: 
	00:00:00,150 --> 00:00:01,330
- My name is Vitaly Wool,

00:00:01,330 --> 00:00:03,820
and I work for Konsulko Group,

00:00:03,820 --> 00:00:07,800
and I'll give a short talk about ZRAM,

00:00:08,750 --> 00:00:11,883
and decoupling it from a specific backend that it uses.

00:00:13,800 --> 00:00:15,563
A few words about myself.

00:00:16,420 --> 00:00:20,590
I've been using and promoting Linux since 1998.

00:00:22,860 --> 00:00:27,339
And I've been doing embedded Linux since 2003.

00:00:27,339 --> 00:00:29,440
Back in the days when MonteVista was big,

00:00:29,440 --> 00:00:32,530
I was working for MonteVista in Moscow, Russia,

00:00:32,530 --> 00:00:34,000
as a part of the

00:00:34,980 --> 00:00:37,043
dedicated development team.

00:00:39,840 --> 00:00:42,410
I moved to Sweden in 2009.

00:00:42,410 --> 00:00:43,640
There is a person in this room

00:00:43,640 --> 00:00:46,535
who knows part of the story.

00:00:46,535 --> 00:00:47,600
(chuckling)

00:00:47,600 --> 00:00:52,600
Yes, so we moved to Southern Sweden, Lund, in 2009.

00:00:52,940 --> 00:00:56,510
And I was working mostly as a consultant since then,

00:00:56,510 --> 00:01:00,403
mostly for, then, Sony Ericsson and after Sony Mobile,

00:01:01,620 --> 00:01:05,900
doing stuff primarily, again, in the kernel area.

00:01:05,900 --> 00:01:10,440
And, now, I'm a staff engineer at Konsulko Group,

00:01:10,440 --> 00:01:15,210
and managing director of the small Konsulko entity

00:01:15,210 --> 00:01:17,253
in Sweden called Konsulko AB,

00:01:18,460 --> 00:01:20,730
and still doing consultancy work

00:01:20,730 --> 00:01:22,963
but on behalf of Konsulko.

00:01:28,385 --> 00:01:32,480
I love travel, I have a big family.

00:01:36,890 --> 00:01:38,800
That's basically

00:01:40,470 --> 00:01:43,270
what I wanted to say about myself, a short introduction.

00:01:44,620 --> 00:01:46,360
Another introductory slide is

00:01:46,360 --> 00:01:48,540
about this presentation.

00:01:48,540 --> 00:01:49,670
This is a bit of an outline.

00:01:49,670 --> 00:01:53,470
We're gonna talk about swapping and such,

00:01:53,470 --> 00:01:55,403
and what ZRAM is and what zswap is.

00:01:56,644 --> 00:02:00,300
If you know everything about it,

00:02:00,300 --> 00:02:03,740
just let me know, we'll jump over these slides.

00:02:03,740 --> 00:02:07,770
But still to be complete with the story,

00:02:07,770 --> 00:02:10,460
I prefer to have these slides and talk

00:02:10,460 --> 00:02:11,963
a little bit about those.

00:02:13,380 --> 00:02:16,490
But if this is not necessary, just let me know.

00:02:16,490 --> 00:02:17,560
We're flexible.

00:02:17,560 --> 00:02:20,380
It's better to have more time for questions.

00:02:20,380 --> 00:02:24,813
So, swapping as such and what ZRAM is and what zswap is.

00:02:28,140 --> 00:02:30,280
Swapping backends,

00:02:30,280 --> 00:02:31,843
what we mean by that.

00:02:33,330 --> 00:02:36,720
We'll talk about ZRAM over zpool

00:02:39,400 --> 00:02:42,520
and perspective of this approach.

00:02:42,520 --> 00:02:45,160
We'll do some comparisons,

00:02:45,160 --> 00:02:46,743
and we'll jump to conclusions.

00:02:47,910 --> 00:02:50,123
Nothing really outstanding, but here we go.

00:02:54,039 --> 00:02:54,872
ZRAM and zswap.

00:02:56,600 --> 00:02:58,540
To talk about ZRAM and zswap

00:02:58,540 --> 00:03:00,930
that's green terminology

00:03:00,930 --> 00:03:01,880
and basically

00:03:02,940 --> 00:03:05,303
we're talking about swapping and compression.

00:03:06,410 --> 00:03:09,590
Swapping also is called paging sometimes,

00:03:09,590 --> 00:03:13,143
and that's because it's all about pushing the pages

00:03:13,143 --> 00:03:15,270
that are not used

00:03:15,270 --> 00:03:18,480
to a storage that has a lot of space.

00:03:18,480 --> 00:03:21,570
That is probably a little less

00:03:21,570 --> 00:03:22,887
or a lot less

00:03:28,250 --> 00:03:31,190
sorry, good performance-wise

00:03:31,190 --> 00:03:33,160
than the RAM.

00:03:33,160 --> 00:03:35,870
So, we basically push pages out of RAM

00:03:35,870 --> 00:03:37,660
to a secondary storage,

00:03:37,660 --> 00:03:40,870
which is usually a hard drive or a flash device.

00:03:40,870 --> 00:03:45,870
And by doing that we sort of trade memory for performance,

00:03:46,400 --> 00:03:50,310
because if we want to retrieve the pages

00:03:50,310 --> 00:03:53,490
that are eventually swapped,

00:03:53,490 --> 00:03:56,290
we will have to spend some time doing that

00:03:56,290 --> 00:03:57,633
to read the pages back.

00:04:00,360 --> 00:04:02,070
There is a

00:04:02,070 --> 00:04:05,070
certain idea on how we can optimize that,

00:04:05,070 --> 00:04:07,600
and that is basically cache pages

00:04:09,660 --> 00:04:12,933
before actually writing them to storage.

00:04:15,580 --> 00:04:17,830
And that is a pretty straightforward idea,

00:04:17,830 --> 00:04:19,670
but if we do that,

00:04:19,670 --> 00:04:23,240
we will lose the memory, win,

00:04:23,240 --> 00:04:25,130
if we do it in the straightforward way.

00:04:25,130 --> 00:04:27,450
So, we have to do it in a non-straightforward way.

00:04:27,450 --> 00:04:29,163
And that calls for compression.

00:04:30,000 --> 00:04:32,703
So, we're gonna keep the pages compressed,

00:04:34,390 --> 00:04:37,430
and that will probably still be better

00:04:37,430 --> 00:04:39,020
if we eventually need them,

00:04:39,020 --> 00:04:41,230
because un-compressing pages

00:04:41,230 --> 00:04:43,450
with modern CPUs will take less time

00:04:43,450 --> 00:04:46,423
than pooling them from a storage device.

00:04:49,690 --> 00:04:50,690
And the first thing

00:04:50,690 --> 00:04:53,790
the first approach

00:04:54,720 --> 00:04:56,640
that has been implemented

00:04:59,950 --> 00:05:04,550
was zswap which is actually a frontswap implementation,

00:05:04,550 --> 00:05:06,890
so there's a frontswap API,

00:05:06,890 --> 00:05:10,360
which provides a sort of transcendent memory interface

00:05:10,360 --> 00:05:11,713
for swap pages.

00:05:13,370 --> 00:05:16,580
So, it intercepts pages to be swapped

00:05:16,580 --> 00:05:19,930
and processes them in the way it wants.

00:05:19,930 --> 00:05:22,830
And that zswap process them in a very simple way.

00:05:22,830 --> 00:05:25,520
It compresses them and puts them

00:05:25,520 --> 00:05:30,330
into a certain pre-allocated area.

00:05:30,330 --> 00:05:34,263
So, in a sense zswap is compressed write-back cache.

00:05:36,600 --> 00:05:40,230
And when the pool of pages is full enough,

00:05:40,230 --> 00:05:43,373
it pushes compressed pages to secondary storage,

00:05:45,560 --> 00:05:48,310
and then pages are read back directly from the storage.

00:05:49,620 --> 00:05:53,440
It's important to mention that zswap is not self-sufficient

00:05:53,440 --> 00:05:56,100
in the sense that it cannot really function

00:05:56,100 --> 00:05:58,620
without the backing device.

00:05:58,620 --> 00:06:00,960
It's just a caching thing.

00:06:00,960 --> 00:06:04,720
So, it mostly targets desktop and server systems

00:06:04,720 --> 00:06:07,960
where you have a secondary storage for the real swap,

00:06:07,960 --> 00:06:10,023
but still want to optimize that somehow.

00:06:13,150 --> 00:06:18,080
ZRAM came in a little later, and

00:06:19,150 --> 00:06:22,060
it's similar to zswap in one sense

00:06:22,060 --> 00:06:23,923
and quite different in the other,

00:06:24,870 --> 00:06:29,163
because it implements a block device,

00:06:30,810 --> 00:06:32,800
which is standalone,

00:06:32,800 --> 00:06:35,730
with on-the-fly compression and decompression.

00:06:35,730 --> 00:06:38,960
So, it's basically a RAM disk

00:06:38,960 --> 00:06:41,703
but with on-the-fly compression and decompression.

00:06:42,740 --> 00:06:45,033
And it came in as an alternative to zswap,

00:06:46,070 --> 00:06:50,900
primarily for embedded devices, because

00:06:51,930 --> 00:06:56,930
once again, for embedded devices secondary storage is

00:06:57,010 --> 00:07:00,233
either limited or can't be used

00:07:00,233 --> 00:07:01,610
for swapping for other reasons.

00:07:01,610 --> 00:07:04,120
Just for example,

00:07:04,120 --> 00:07:07,700
we might be afraid of wearing out a flash,

00:07:07,700 --> 00:07:10,440
because it's just not replaceable

00:07:10,440 --> 00:07:12,320
on a normal embedded system.

00:07:12,320 --> 00:07:16,430
So, we don't want to swap to a flash,

00:07:16,430 --> 00:07:18,883
which is directly wired.

00:07:19,870 --> 00:07:23,580
And we still want to have some kind of a swap device.

00:07:23,580 --> 00:07:25,113
So, then, we have ZRAM,

00:07:26,290 --> 00:07:28,180
which swaps to RAM,

00:07:28,180 --> 00:07:29,653
and also has compression.

00:07:35,270 --> 00:07:37,193
Once again, it's a terminology thing.

00:07:40,460 --> 00:07:44,270
We would like to address ZRAM and zswap

00:07:44,270 --> 00:07:49,270
as front-ends, because they both process swapped-out pages

00:07:49,800 --> 00:07:52,740
and store them compressed in RAM.

00:07:52,740 --> 00:07:56,420
So, we would like to agree that we call them

00:07:57,480 --> 00:08:01,203
compressed data front-ends.

00:08:04,160 --> 00:08:08,710
And they both need a backend,

00:08:08,710 --> 00:08:12,763
which actually handles storing compressed pages.

00:08:14,090 --> 00:08:17,213
In other words, a compressed page allocator.

00:08:18,860 --> 00:08:20,790
And here we go to

00:08:21,870 --> 00:08:25,913
the backends that we currently have in the kernel.

00:08:27,950 --> 00:08:32,093
And the first backend historically is called zbud,

00:08:34,120 --> 00:08:38,770
and that one is really simple.

00:08:38,770 --> 00:08:42,370
It stores up to two objects per page.

00:08:42,370 --> 00:08:44,700
One object being bound to the beginning

00:08:44,700 --> 00:08:47,023
and the other being bound to the end.

00:08:48,560 --> 00:08:53,560
It deals with objects by rounding their size to chunks.

00:08:57,410 --> 00:09:01,690
And then it organizes unbuddied lists,

00:09:01,690 --> 00:09:05,550
and that means for all the objects

00:09:05,550 --> 00:09:07,830
that only have one buddy

00:09:09,150 --> 00:09:13,970
we add them to a corresponding list L-N,

00:09:13,970 --> 00:09:15,810
as you can see on the picture,

00:09:15,810 --> 00:09:19,000
where L-N is a list of all unbuddied objects

00:09:19,000 --> 00:09:21,293
with exactly N free chunks.

00:09:22,660 --> 00:09:27,660
So, if we want to store an object

00:09:28,400 --> 00:09:30,850
that requires N free chunks,

00:09:30,850 --> 00:09:35,850
we take the first entry off of the L-N list.

00:09:37,650 --> 00:09:39,480
Well, if it's empty,

00:09:39,480 --> 00:09:44,480
we take the first entry off of L-N plus one, and so forth.

00:09:44,780 --> 00:09:46,173
So, this is very simple,

00:09:48,860 --> 00:09:50,510
very deterministic.

00:09:50,510 --> 00:09:53,160
It has some drawbacks, but we'll talk about it later.

00:09:56,820 --> 00:09:57,963
After that,

00:09:58,800 --> 00:10:03,800
came in another allocator called zsmalloc,

00:10:05,170 --> 00:10:09,230
which was from the very beginning a part of ZRAM.

00:10:10,260 --> 00:10:15,260
And then it was transformed into a self-sufficient thing

00:10:15,370 --> 00:10:16,773
with its own API.

00:10:19,100 --> 00:10:23,003
And it works in quite a bit of a different way.

00:10:25,940 --> 00:10:30,633
It allocates zspages, so-called zspages first,

00:10:32,070 --> 00:10:35,130
which every zspage

00:10:36,440 --> 00:10:39,530
is basically

00:10:39,530 --> 00:10:44,213
consisting of a power of two physical pages.

00:10:44,213 --> 00:10:46,680
They're not contiguous,

00:10:46,680 --> 00:10:49,770
but are mapped into a contiguous space.

00:10:49,770 --> 00:10:53,853
So, they form zspage as shown on the picture.

00:10:55,790 --> 00:10:59,660
And, then, compressed objects placed contiguously

00:10:59,660 --> 00:11:01,087
within zspage.

00:11:03,270 --> 00:11:05,363
Well, as shown on the picture again.

00:11:06,420 --> 00:11:09,910
And this is, of course,

00:11:09,910 --> 00:11:12,210
a lot better approach when it comes

00:11:12,210 --> 00:11:14,103
to density of the objects,

00:11:14,970 --> 00:11:17,870
because you can see

00:11:17,870 --> 00:11:22,180
that there are no free spaces within the zspage.

00:11:22,180 --> 00:11:23,463
But on the other hand,

00:11:25,690 --> 00:11:30,530
as the time goes fragmentation issues may occur,

00:11:30,530 --> 00:11:33,500
because as you can see

00:11:33,500 --> 00:11:36,723
on the rightmost picture,

00:11:37,590 --> 00:11:40,200
if you free an object somewhere

00:11:41,510 --> 00:11:44,160
in the middle of a zspage,

00:11:44,160 --> 00:11:47,170
then there is a certain hole

00:11:47,170 --> 00:11:51,450
that can then be fulfilled

00:11:51,450 --> 00:11:53,900
by storing another object.

00:11:53,900 --> 00:11:57,410
But if it's not exactly the space of the hole,

00:11:57,410 --> 00:12:01,450
then there can be some free space left.

00:12:01,450 --> 00:12:04,633
So, we end up having to deal with fragmentation,

00:12:05,520 --> 00:12:10,510
and zsmalloc currently does implement a complex mechanism

00:12:10,510 --> 00:12:12,230
to deal with that.

00:12:12,230 --> 00:12:16,280
But still we need to understand that

00:12:18,370 --> 00:12:22,763
fragmentation issues may become tough as the time goes.

00:12:23,730 --> 00:12:24,563
Question?

00:12:24,563 --> 00:12:26,047
- [Student] Yeah, these compressed objects,

00:12:26,047 --> 00:12:28,350
are they always pages or--

00:12:28,350 --> 00:12:32,880
- Now, in ZRAM and basically

00:12:35,110 --> 00:12:35,943
in zswap, too,

00:12:36,880 --> 00:12:39,853
we are swapping out pages.

00:12:40,690 --> 00:12:45,310
So, then, we are compressing a page into something,

00:12:45,310 --> 00:12:48,810
so we can assume that the result is actually less

00:12:48,810 --> 00:12:49,680
than the paging size,

00:12:49,680 --> 00:12:52,520
because otherwise we will just store it uncompressed.

00:12:52,520 --> 00:12:53,940
- [Student] So, the compressed object

00:12:53,940 --> 00:12:55,383
is a compressed page, let's say.

00:12:55,383 --> 00:12:56,680
Yes. - Okay, great.

00:12:56,680 --> 00:12:57,640
It's not used for other stuff.

00:12:57,640 --> 00:12:59,415
It's just what we call a compressed page,

00:12:59,415 --> 00:13:02,120
that's a compressed object?

00:13:02,120 --> 00:13:02,953
- Yeah.

00:13:03,930 --> 00:13:07,360
In this context a compressed object is a compressed page,

00:13:07,360 --> 00:13:10,589
and we can assume that this is a size of page or less.

00:13:10,589 --> 00:13:12,589
- [Student] Okay, great.

00:13:15,550 --> 00:13:16,510
- So, if we

00:13:18,510 --> 00:13:20,830
if we compare in

00:13:20,830 --> 00:13:24,900
well, put together zsmalloc

00:13:25,829 --> 00:13:27,860
and zbud applicability

00:13:29,590 --> 00:13:31,703
in a sort of a chart,

00:13:33,124 --> 00:13:36,680
we can see that zsmalloc is applicable

00:13:36,680 --> 00:13:38,583
both to zswap and ZRAM.

00:13:40,110 --> 00:13:43,480
But zbud is not applicable to ZRAM, basically,

00:13:43,480 --> 00:13:46,440
because the goal of ZRAM is to do

00:13:47,350 --> 00:13:48,990
as much compression as possible.

00:13:48,990 --> 00:13:49,823
And zbud

00:13:51,630 --> 00:13:55,610
is not providing the compression ratio good enough,

00:13:55,610 --> 00:13:59,970
because as we can only place two objects per page

00:13:59,970 --> 00:14:02,090
it will never be more than two acts.

00:14:02,090 --> 00:14:04,060
And in fact they're gonna

00:14:04,060 --> 00:14:05,650
even smaller, because

00:14:07,510 --> 00:14:09,030
in some circumstances

00:14:10,550 --> 00:14:13,900
pages may not compress that well.

00:14:13,900 --> 00:14:16,400
And there may be a lot of pages

00:14:16,400 --> 00:14:19,680
slightly more than half of a page size.

00:14:19,680 --> 00:14:21,660
And in that situation

00:14:21,660 --> 00:14:25,233
you will end up basically with one object per page.

00:14:30,339 --> 00:14:31,920
And then came the idea

00:14:33,380 --> 00:14:35,437
why not modify zbud

00:14:37,540 --> 00:14:39,293
to hold three objects per page,

00:14:40,430 --> 00:14:44,010
because it is still simple

00:14:45,000 --> 00:14:47,220
in the sense that there are not many kind of cases

00:14:47,220 --> 00:14:48,053
to deal with.

00:14:49,540 --> 00:14:51,330
But it's a lot more flexible,

00:14:51,330 --> 00:14:52,910
because it's not exposed

00:14:53,870 --> 00:14:57,000
that much to the situation where you have

00:14:58,270 --> 00:15:02,433
a lot of pages that are not compressed well.

00:15:06,880 --> 00:15:08,810
So, z3fold

00:15:12,900 --> 00:15:17,370
has been implemented comparably recently.

00:15:17,370 --> 00:15:22,180
The work started after Medellin's Conference 2016.

00:15:22,180 --> 00:15:24,273
It's been accepted by mainline in 4.8.

00:15:26,390 --> 00:15:31,390
And from the very beginning it was just basically

00:15:34,870 --> 00:15:36,510
a zbud improvement

00:15:36,510 --> 00:15:40,023
that happened to be a separate backend.

00:15:42,010 --> 00:15:46,600
But, eventually, it became ZRAM ready.

00:15:46,600 --> 00:15:49,500
And then came the idea,

00:15:49,500 --> 00:15:51,693
why not use it for ZRAM if possible?

00:15:55,910 --> 00:15:57,203
Sorry, why z3fold?

00:15:58,580 --> 00:16:00,793
It is actually a good fit for zswap,

00:16:02,450 --> 00:16:07,450
because it supports reclaiming a page like zbud,

00:16:08,470 --> 00:16:10,503
as opposed to zsmalloc, which doesn't.

00:16:11,530 --> 00:16:13,480
It provides better compression than zbud,

00:16:13,480 --> 00:16:16,723
because it can store three pages instead of two.

00:16:18,490 --> 00:16:20,110
And in its current state,

00:16:20,110 --> 00:16:21,970
it scales well to multicore system

00:16:21,970 --> 00:16:24,410
because of per-CPU lists

00:16:24,410 --> 00:16:25,923
per-CPU unbuddied lists,

00:16:27,170 --> 00:16:31,223
as opposed to zbud, which doesn't use that.

00:16:33,450 --> 00:16:36,110
And at the same time it's actually good enough

00:16:37,920 --> 00:16:40,000
match for ZRAM,

00:16:40,000 --> 00:16:42,070
because of it's low latency operation

00:16:42,070 --> 00:16:45,530
and reasonable compression ratio, and

00:16:46,890 --> 00:16:49,383
good behavior on big.LITTLE systems,

00:16:52,170 --> 00:16:57,170
which comprise a lot of the embedded systems nowadays.

00:16:57,450 --> 00:17:00,563
And ZRAM is mostly for the embedded.

00:17:01,680 --> 00:17:06,430
So, z3fold is still okay for ZRAM,

00:17:06,430 --> 00:17:08,883
even though it initially targeted zswap.

00:17:10,920 --> 00:17:13,000
And if we look at that

00:17:15,080 --> 00:17:17,743
from a chart perspective,

00:17:18,800 --> 00:17:20,073
it will look like this.

00:17:22,880 --> 00:17:26,260
Z3fold retains

00:17:26,260 --> 00:17:28,540
relative simplicity,

00:17:28,540 --> 00:17:31,210
while having reasonable compression ratio,

00:17:31,210 --> 00:17:32,453
reclaim support,

00:17:34,790 --> 00:17:36,643
and therefore it's okay for zswap.

00:17:37,490 --> 00:17:41,330
With that said, it also has low latency operation,

00:17:41,330 --> 00:17:44,740
and good scalability, and therefore it's okay for ZRAM.

00:17:47,870 --> 00:17:50,280
In theory, but in practice,

00:17:50,280 --> 00:17:52,950
ZRAM would not allow any of the backends

00:17:52,950 --> 00:17:55,313
than zsmalloc, but we'll get to this.

00:17:58,780 --> 00:18:01,530
Another important thing to mention

00:18:01,530 --> 00:18:04,310
is that as we move forward

00:18:05,230 --> 00:18:08,440
there may be some new backends coming in.

00:18:08,440 --> 00:18:10,650
For instance, if we

00:18:12,453 --> 00:18:15,210
have a hardware compression module

00:18:15,210 --> 00:18:18,613
that can decompress and compress pages on-the-fly,

00:18:21,870 --> 00:18:24,343
like shown on the picture.

00:18:26,090 --> 00:18:28,560
If we have an uncompressed page

00:18:28,560 --> 00:18:31,913
that is on-the-fly transferred to a compressed page,

00:18:32,820 --> 00:18:35,870
and stored in the same place basically,

00:18:35,870 --> 00:18:39,760
then we, obviously, will have an unused remainder

00:18:41,155 --> 00:18:44,510
that we would like to use for something,

00:18:44,510 --> 00:18:47,760
and using that for a page that is being swapped out

00:18:50,580 --> 00:18:53,363
is a pretty reasonable application in my opinion.

00:18:55,660 --> 00:18:58,110
So, that calls for a backend

00:18:58,110 --> 00:19:01,650
that uses this hardware compressor

00:19:01,650 --> 00:19:03,690
to compress swapped out page,

00:19:03,690 --> 00:19:07,430
and put it in the remainder of the page

00:19:07,430 --> 00:19:11,133
that is otherwise looks already full for the system.

00:19:12,490 --> 00:19:14,250
So, this doesn't seem to be a

00:19:14,250 --> 00:19:16,713
very complicated implementation.

00:19:18,930 --> 00:19:22,040
And it seems to be a pretty good match for ZRAM,

00:19:22,040 --> 00:19:26,020
because we would have a block device

00:19:26,020 --> 00:19:29,583
using unused page ends.

00:19:31,500 --> 00:19:33,760
But once again, at this point it's not possible,

00:19:33,760 --> 00:19:36,613
because ZRAM is using zsmalloc directly.

00:19:41,180 --> 00:19:42,163
Short summary,

00:19:43,000 --> 00:19:46,983
we have two compression front-ends, zsmalloc

00:19:48,350 --> 00:19:51,960
sorry, zswap and ZRAM,

00:19:51,960 --> 00:19:56,110
and at least three backends and, maybe, more.

00:19:56,110 --> 00:19:59,800
And that calls for unification and independence,

00:19:59,800 --> 00:20:01,233
and well-defined APIs.

00:20:05,250 --> 00:20:08,990
Also, as different compression backends

00:20:08,990 --> 00:20:11,400
have different things in focus,

00:20:11,400 --> 00:20:13,710
and you may have different goals.

00:20:13,710 --> 00:20:16,550
Depending on those goals you may need

00:20:17,890 --> 00:20:22,890
some unexpected backend for your ZRAM or zswap application.

00:20:24,620 --> 00:20:28,810
And also it's beneficial for the kernel ecosystem

00:20:28,810 --> 00:20:31,810
to have simple means to switch between backends

00:20:31,810 --> 00:20:33,823
for both ZRAM and zswap.

00:20:37,960 --> 00:20:39,960
How does it match the current situation?

00:20:45,230 --> 00:20:46,063
How?

00:20:48,820 --> 00:20:53,820
Well, currently it doesn't match very well,

00:20:54,340 --> 00:20:59,273
because zsmalloc can't be used by both zswap and ZRAM.

00:21:00,160 --> 00:21:02,840
But two of the backends can be only used by zswap,

00:21:02,840 --> 00:21:07,840
because zswap uses the unified zpool API and ZRAM doesn't.

00:21:11,250 --> 00:21:16,040
And that may become a certain obstacle

00:21:16,040 --> 00:21:20,493
to using ZRAM in certain conditions.

00:21:21,500 --> 00:21:26,500
So, why don't we consider using ZRAM over zpool?

00:21:30,560 --> 00:21:35,486
But before that short slide about zpool and such.

00:21:35,486 --> 00:21:37,870
Zpool is basically an abstract API

00:21:37,870 --> 00:21:41,003
for compressed allocators or backends.

00:21:42,540 --> 00:21:47,310
It is mostly pass-through just invoking callbacks

00:21:47,310 --> 00:21:48,713
of zpool clients.

00:21:50,390 --> 00:21:53,400
Zswap has already been converted to use zpool

00:21:53,400 --> 00:21:54,527
quite a while ago.

00:21:55,907 --> 00:21:59,130
And zpool API is already implemented by

00:22:00,540 --> 00:22:03,260
all three backends that are available,

00:22:03,260 --> 00:22:05,313
zbud, zsmalloc, and z3fold.

00:22:07,330 --> 00:22:11,663
But ZRAM does not use zpool at the moment.

00:22:12,640 --> 00:22:15,963
And it is using zsmalloc API directly.

00:22:20,170 --> 00:22:22,823
Why ZRAM doesn't use zpool?

00:22:24,440 --> 00:22:27,100
Well, to be honest I don't know,

00:22:27,100 --> 00:22:30,003
but I can imagine the following reasons.

00:22:32,330 --> 00:22:35,023
First of all, there was no need,

00:22:36,160 --> 00:22:41,160
because, as we mentioned, zbud isn't really a

00:22:41,730 --> 00:22:44,130
very good match for ZRAM.

00:22:44,130 --> 00:22:47,713
So, before z3fold came in,

00:22:49,330 --> 00:22:51,500
there wasn't a real need for ZRAM

00:22:51,500 --> 00:22:53,300
to use something else than zsmalloc.

00:22:56,670 --> 00:22:58,840
Another possible reason is that

00:22:59,800 --> 00:23:02,950
we don't want to introduce a level of indirection.

00:23:02,950 --> 00:23:04,920
We want to use a certain API directly,

00:23:04,920 --> 00:23:08,103
rather than using something in between.

00:23:09,110 --> 00:23:14,110
And another possible reason out of my guess working

00:23:14,170 --> 00:23:19,170
is that zpool API does not exactly match zsmalloc API.

00:23:20,090 --> 00:23:21,313
We'll get to this later.

00:23:24,400 --> 00:23:27,310
Why I think this is not important enough

00:23:27,310 --> 00:23:32,310
to hold us from using zpool with ZRAM?

00:23:34,740 --> 00:23:38,010
Well, first of all, I believe now there is a need

00:23:38,010 --> 00:23:42,053
for ZRAM to be abstracted from a certain backend.

00:23:44,520 --> 00:23:47,440
The indirection shouldn't be a huge obstacle,

00:23:47,440 --> 00:23:50,790
because it's almost completely optimized out.

00:23:50,790 --> 00:23:52,863
It's pretty much a pass-through thing.

00:23:55,385 --> 00:23:59,680
Zpool API is not set in stone and can be extended

00:23:59,680 --> 00:24:02,910
to reflect zsmalloc's specific functions,

00:24:02,910 --> 00:24:06,323
which just then be no callbacks in all the cases.

00:24:08,420 --> 00:24:11,923
And, finally, if there are more backends to come,

00:24:14,660 --> 00:24:17,950
they will obviously implement the zpool API,

00:24:17,950 --> 00:24:21,793
and there may be a need for ZRAM to use those,

00:24:22,940 --> 00:24:24,173
and it will be ready.

00:24:29,110 --> 00:24:32,680
We will not go over all the zpool API, basically.

00:24:32,680 --> 00:24:34,803
It's a boring thing.

00:24:37,390 --> 00:24:39,200
Let us just state that

00:24:41,100 --> 00:24:44,100
a vast majority of zpool API

00:24:44,100 --> 00:24:47,620
has been live tested by using zswap, zsmalloc,

00:24:47,620 --> 00:24:49,020
and the other backends,

00:24:49,020 --> 00:24:54,020
so zpool API maps very well to zsmalloc API,

00:24:54,230 --> 00:24:56,163
except for the three functions listed.

00:24:57,320 --> 00:24:59,593
So, there are three functions to be added,

00:25:00,660 --> 00:25:02,388
zpool_compact,

00:25:02,388 --> 00:25:03,643
get_num_compacted,

00:25:03,643 --> 00:25:05,310
and huge_class_size.

00:25:07,920 --> 00:25:10,200
And the patch is out there.

00:25:10,200 --> 00:25:12,573
I mean, it's hanging around for a while.

00:25:15,420 --> 00:25:17,073
So, anyone can look at it.

00:25:19,400 --> 00:25:22,020
So, I will not overstate by saying

00:25:22,020 --> 00:25:26,263
that zpool API is almost ready to embrace ZRAM.

00:25:29,510 --> 00:25:32,910
And when it is ready

00:25:34,030 --> 00:25:36,253
like when the patch is there integrated,

00:25:37,870 --> 00:25:41,210
changes to ZRAM implementation

00:25:42,543 --> 00:25:46,020
to use zpool instead of zsmalloc

00:25:46,020 --> 00:25:48,090
are really trivial.

00:25:48,090 --> 00:25:50,903
So, this is, of course, not the full patch,

00:25:52,100 --> 00:25:54,340
but I think it gives a good idea

00:25:54,340 --> 00:25:56,860
of how simple it is to convert

00:25:56,860 --> 00:26:00,933
from using zsmalloc API into using zpool API.

00:26:04,530 --> 00:26:07,113
Yeah, it is straightforward.

00:26:08,670 --> 00:26:09,963
And, finally,

00:26:13,890 --> 00:26:16,073
it's always nice to see some charts, right?

00:26:18,330 --> 00:26:20,423
So, we'll pass over to the fun part.

00:26:22,700 --> 00:26:23,990
Compression under stress,

00:26:23,990 --> 00:26:25,630
so if you have a stress load

00:26:26,780 --> 00:26:31,083
that causes swap to be used extensively,

00:26:33,960 --> 00:26:38,700
then we can we see that zsmalloc is still a leader

00:26:38,700 --> 00:26:41,050
in the compression ratio, which is no surprise.

00:26:41,920 --> 00:26:44,043
But the fluctuations are high.

00:26:45,255 --> 00:26:48,770
Z3fold is well below

00:26:51,090 --> 00:26:53,920
but not that much below zbud.

00:26:55,610 --> 00:26:57,500
And the good thing about z3fold

00:26:57,500 --> 00:26:58,850
is that the compression ratio

00:26:58,850 --> 00:27:01,050
does not differ that much from time-to-time.

00:27:02,780 --> 00:27:06,483
So, it provides a pretty stable compression ratio over time.

00:27:09,930 --> 00:27:11,263
And at the same time,

00:27:13,550 --> 00:27:18,400
if we take random read-write performance comparison,

00:27:18,400 --> 00:27:22,683
we can see that depending on the number of threads.

00:27:24,270 --> 00:27:28,610
This was measured on a Asian P-System.

00:27:28,610 --> 00:27:33,397
Well, basically, it was a Qualcomm Platform 630.

00:27:37,040 --> 00:27:39,480
Basically, what we can see is that

00:27:42,032 --> 00:27:43,680
z3fold performance

00:27:44,660 --> 00:27:48,820
does not degrade much, or almost not at all,

00:27:48,820 --> 00:27:51,000
depending on the number of threads used

00:27:51,000 --> 00:27:52,960
to stress the system.

00:27:52,960 --> 00:27:54,953
So, scalability is good.

00:27:56,930 --> 00:28:01,930
And this might also be an important thing

00:28:02,150 --> 00:28:03,930
when you choose a backend.

00:28:03,930 --> 00:28:05,710
Well, maybe not, but

00:28:05,710 --> 00:28:07,510
still it's good to have this option.

00:28:09,670 --> 00:28:10,653
Conclusions.

00:28:11,900 --> 00:28:13,690
Well, once again, it's beneficial

00:28:13,690 --> 00:28:16,313
to have simple means to switch between backends.

00:28:17,270 --> 00:28:18,430
And that means

00:28:19,530 --> 00:28:21,537
we should use the common API,

00:28:23,710 --> 00:28:28,593
and that means that ZRAM better be decoupled from zsmalloc.

00:28:31,750 --> 00:28:34,580
We don't have to seek for this API for too long,

00:28:34,580 --> 00:28:36,220
because there is zpool API,

00:28:36,220 --> 00:28:38,293
which is already a good match for ZRAM.

00:28:39,450 --> 00:28:43,000
We need to make some extensions to that,

00:28:43,000 --> 00:28:45,797
but they're really not drastic, not at all.

00:28:48,170 --> 00:28:50,653
And, once done with that,

00:28:51,510 --> 00:28:56,300
it would be nice to have ZRAM use this zpool API,

00:28:56,300 --> 00:28:59,580
and leave kernel developers and configurators

00:29:00,550 --> 00:29:04,773
the power of choice on which backend to use.

00:29:06,650 --> 00:29:08,283
Yeah, I think that's it.

00:29:09,300 --> 00:29:10,600
Thanks for your attention.

00:29:13,260 --> 00:29:14,973
- [Moderator] Are there any questions?

00:29:17,874 --> 00:29:19,703
- Yeah, when it comes

00:29:19,703 --> 00:29:20,536
yeah.

00:29:21,650 --> 00:29:24,195
The actual compression use is that pluggable?

00:29:24,195 --> 00:29:27,103
The algorithm used to compress the pages.

00:29:28,460 --> 00:29:29,970
I mean, it's not a concern for this,

00:29:29,970 --> 00:29:32,080
but do we have pluggability

00:29:32,080 --> 00:29:33,502
in other parts of these systems?

00:29:33,502 --> 00:29:34,473
- Yes.

00:29:36,170 --> 00:29:40,870
Since the comparison is not the key part,

00:29:40,870 --> 00:29:43,470
I haven't really specified the system used

00:29:43,470 --> 00:29:45,400
for testing that well.

00:29:45,400 --> 00:29:47,430
But it's a Qualcomm 630,

00:29:47,430 --> 00:29:52,430
and it's the LZ4HC used for compression and decompression.

00:29:53,160 --> 00:29:56,380
But it doesn't have to be like that.

00:29:56,380 --> 00:29:58,010
I mean, you can use LZO.

00:29:59,801 --> 00:30:03,833
You can use ZCOMP, you can use whatever.

00:30:06,410 --> 00:30:08,560
But it's already there.

00:30:08,560 --> 00:30:13,290
There is an implementation within ZRAM

00:30:13,290 --> 00:30:18,290
that allows to take whatever compression backend I want

00:30:19,640 --> 00:30:22,620
that is there and present in the kernel.

00:30:22,620 --> 00:30:25,060
So, yeah, it's a bit funny,

00:30:25,060 --> 00:30:29,490
because ZRAM is very free and configurable

00:30:29,490 --> 00:30:32,120
when it comes to selecting a compression backend,

00:30:32,120 --> 00:30:33,990
but it's not configurable when it comes

00:30:33,990 --> 00:30:35,963
to selecting an allocation backend.

00:30:36,890 --> 00:30:39,400
- So, have you proposed switching like this,

00:30:39,400 --> 00:30:41,540
and got resistance?

00:30:41,540 --> 00:30:43,843
- Yeah, but it's been a while ago, yes.

00:30:47,330 --> 00:30:51,193
I guess we might be in a good spot to retry this.

00:30:52,076 --> 00:30:53,847
- Are you gonna propose it, again?

00:30:53,847 --> 00:30:56,860
- [Presenter] Yeah.

00:30:56,860 --> 00:30:58,540
- So, I had a question which was,

00:30:58,540 --> 00:31:00,940
are you proposing being able

00:31:00,940 --> 00:31:05,540
to do the switch at boot time, at runtime?

00:31:05,540 --> 00:31:07,700
And if it's at runtime,

00:31:07,700 --> 00:31:11,250
and there are some portions of memory being compressed

00:31:11,250 --> 00:31:13,343
using one of the other backends,

00:31:14,320 --> 00:31:17,430
is it going to be hard to actually do that switch

00:31:17,430 --> 00:31:19,560
while the system is running?

00:31:19,560 --> 00:31:20,780
Have you thought about how you

00:31:20,780 --> 00:31:22,810
actually want to make it be switchable?

00:31:24,610 --> 00:31:26,570
- The prototype that we use

00:31:27,520 --> 00:31:30,240
has a kernel parameter.

00:31:30,240 --> 00:31:34,800
So, within the ZRAM driver

00:31:34,800 --> 00:31:36,780
within the ZRAM driver implementation

00:31:37,850 --> 00:31:40,980
there is a new module parameter,

00:31:40,980 --> 00:31:44,603
which can also be specified within the kernel command line.

00:31:46,140 --> 00:31:47,180
- [Moderator] So, that's boot time?

00:31:47,180 --> 00:31:48,013
- Yes.

00:31:51,330 --> 00:31:52,163
- Have you thought about

00:31:52,163 --> 00:31:53,510
whether you wanna do anything more than that,

00:31:53,510 --> 00:31:54,850
or is boot time

00:31:54,850 --> 00:31:58,593
you think the right level of configurability?

00:32:01,220 --> 00:32:03,113
- This is sufficient for us,

00:32:04,140 --> 00:32:07,050
especially given that we can actually compile it as a module

00:32:07,050 --> 00:32:08,120
and remove the module

00:32:08,120 --> 00:32:11,403
and insert the module with a different parameter.

00:32:14,280 --> 00:32:17,100
We haven't really thought about switching at runtime,

00:32:17,100 --> 00:32:20,830
because we didn't feel the need to.

00:32:20,830 --> 00:32:22,390
I guess it should be possible,

00:32:22,390 --> 00:32:26,930
but we basically just haven't thought seriously about it.

00:32:26,930 --> 00:32:28,530
- [Moderator] Sure, that's fine.

00:32:29,500 --> 00:32:31,853
Any other questions or comments?

00:32:36,290 --> 00:32:39,140
All right, if not, let's thank the speaker.

00:32:39,140 --> 00:32:40,082
- Thank you.

00:32:40,082 --> 00:32:41,130

YouTube URL: https://www.youtube.com/watch?v=F27plJduyF8


