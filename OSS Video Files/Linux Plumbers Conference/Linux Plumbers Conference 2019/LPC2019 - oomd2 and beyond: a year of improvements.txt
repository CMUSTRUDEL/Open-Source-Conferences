Title: LPC2019 - oomd2 and beyond: a year of improvements
Publication date: 2019-09-17
Playlist: Linux Plumbers Conference 2019
Description: 
	oomd2 and beyond: a year of improvements

Speaker
 Daniel Xu (Facebook)

Description
Running out of memory on a host is a particularly nasty scenario. In the Linux kernel, if memory is being overcommitted, it results in the kernel out-of-memory (OOM) killer kicking in. Perhaps surprisingly, the kernel does not often handle this well. oomd builds on top of recent kernel development to effectively implement OOM killing in userspace. This results in a faster, more predictable, and more accurate handling of OOM scenarios.

oomd has gained a number of new features and interesting deployments in the last year. The most notable feature is a complete redesign of the control plane which enables arbitrary but "gotcha"-free configurations. In this talk, Daniel Xu will cover past, present, future, and path-not-taken development plans along with experiences gained from overseeing large deployments of oomd.
Captions: 
	00:00:00,300 --> 00:00:01,133
- Hello.

00:00:01,133 --> 00:00:03,820
Welcome to Linux Plumbers Conference 2019.

00:00:03,820 --> 00:00:05,490
We don't have a plenary so I'm just gonna

00:00:05,490 --> 00:00:07,720
go through a few housekeeping things for the week

00:00:07,720 --> 00:00:09,660
or for the three days anyway and then I'll hand off

00:00:09,660 --> 00:00:11,663
to Kareem who will get the talk started.

00:00:12,810 --> 00:00:15,230
So, first off a big thank you to our sponsors

00:00:15,230 --> 00:00:18,200
and maybe if I actually got on the right place and did that

00:00:18,200 --> 00:00:21,293
we could get our sponsors as opposed to this window.

00:00:22,320 --> 00:00:23,550
Big thank you to all of them,

00:00:23,550 --> 00:00:25,200
they're what makes this possible.

00:00:29,960 --> 00:00:31,200
Let's see here, there we are.

00:00:31,200 --> 00:00:34,070
Useful information, if you haven't hooked up to wi-fi,

00:00:34,070 --> 00:00:34,963
there it is.

00:00:36,014 --> 00:00:38,810
A sad fact of life is in my experience in the 60 years

00:00:38,810 --> 00:00:40,890
on this planet, people are not angels so we have

00:00:40,890 --> 00:00:44,970
a code of conduct and this color of lanyard is

00:00:44,970 --> 00:00:47,160
who you talk to if you have questions on that

00:00:47,160 --> 00:00:48,163
or something else.

00:00:52,230 --> 00:00:54,570
Schedule overview is at that URL.

00:00:54,570 --> 00:00:56,680
It's kind of a nice thing set up and shows you

00:00:56,680 --> 00:00:57,730
kind of where things are.

00:00:57,730 --> 00:00:59,610
There's a bunch of other options.

00:00:59,610 --> 00:01:01,730
You guys realize we start at 10 a.m.

00:01:01,730 --> 00:01:02,563
Good show.

00:01:03,800 --> 00:01:06,500
We have lunch at the Sete Colinas which I probably

00:01:06,500 --> 00:01:08,450
horribly mispronounced, restaurant ground floor,

00:01:08,450 --> 00:01:12,480
that's from 1:30 to 3 p.m, we are on Iberian Peninsula time.

00:01:12,480 --> 00:01:15,460
So if you're used to a more punctual lunch schedule,

00:01:15,460 --> 00:01:18,870
you know, just relax, we're in Portugal.

00:01:18,870 --> 00:01:21,245
Now we're not in Spain so it's not as late as it might be.

00:01:21,245 --> 00:01:24,230
(scattered laughter)

00:01:24,230 --> 00:01:26,666
Sorry, anyway if anybody's Spanish, apologies.

00:01:26,666 --> 00:01:28,350
(scattered laughter)

00:01:28,350 --> 00:01:29,700
What can I do?

00:01:29,700 --> 00:01:32,910
Tonight we have a welcome reception starting at nine,

00:01:32,910 --> 00:01:34,270
that's here, no buses.

00:01:34,270 --> 00:01:36,320
Tuesday night, on your own,

00:01:36,320 --> 00:01:38,160
unless you're in a couple of the microconferences

00:01:38,160 --> 00:01:41,110
running over, RDM and Android if I remember correctly,

00:01:41,110 --> 00:01:42,973
in which case, you're here.

00:01:43,930 --> 00:01:46,670
Wednesday after a plenary, we have buses starting at 7:30,

00:01:46,670 --> 00:01:49,580
that's a remote thing, so 7:30 downstairs,

00:01:49,580 --> 00:01:51,430
is where for you to be start that up.

00:01:52,580 --> 00:01:54,690
And with that, if I hit the right button,

00:01:54,690 --> 00:01:56,053
ah, yes, track owners.

00:01:57,020 --> 00:01:58,730
Kareem, I'll be handing off to you momentarily

00:01:58,730 --> 00:02:00,410
for this track.

00:02:00,410 --> 00:02:02,730
And if you're a presenter or a leader,

00:02:02,730 --> 00:02:05,240
please upload your slides to the LPC site,

00:02:05,240 --> 00:02:07,820
that's help for the people who look at this via video.

00:02:07,820 --> 00:02:09,800
Some of these, not everything, but some of these things

00:02:09,800 --> 00:02:11,700
are videoed so people can look at them later,

00:02:11,700 --> 00:02:12,890
and it's great if they can page through

00:02:12,890 --> 00:02:14,060
the slides separately,

00:02:14,060 --> 00:02:16,710
especially when the camera focuses on your face,

00:02:16,710 --> 00:02:18,087
and they're going, "What's on that slide

00:02:18,087 --> 00:02:19,670
"you're pointing to?"

00:02:19,670 --> 00:02:20,760
So please do that.

00:02:20,760 --> 00:02:23,565
And Etherpad written summary and so on.

00:02:23,565 --> 00:02:25,470
If you have an additional BOF thing,

00:02:25,470 --> 00:02:27,260
if a topic comes up on MC and you don't have

00:02:27,260 --> 00:02:30,160
time for it, and you talk to one of us,

00:02:30,160 --> 00:02:33,010
get a schedule, actually that's if you actually

00:02:33,010 --> 00:02:34,780
post it and send us email, let us know,

00:02:34,780 --> 00:02:36,183
contact it, whichever.

00:02:37,390 --> 00:02:39,460
So first come, first served.

00:02:39,460 --> 00:02:41,360
We have a few slots.

00:02:41,360 --> 00:02:43,520
And if you have other questions,

00:02:43,520 --> 00:02:46,600
again green lanyards, there's a list of the people.

00:02:46,600 --> 00:02:49,020
We have two of us in this room at this point

00:02:49,020 --> 00:02:50,230
that I can see.

00:02:50,230 --> 00:02:52,610
And with that, I hand it off to Kareem,

00:02:52,610 --> 00:02:54,470
and let you guys get started with

00:02:54,470 --> 00:02:56,070
Refereed Track, number one.

00:02:56,070 --> 00:02:57,160
- [Kareem] Thank you sir.

00:02:57,160 --> 00:03:00,210
Or thanks, Daniel do you wanna start getting set up?

00:03:00,210 --> 00:03:04,660
So really quick kind of presentation.

00:03:04,660 --> 00:03:08,010
So we're going to be running from 10 to 11:30,

00:03:08,010 --> 00:03:10,250
given a minute or two for people to get set up.

00:03:10,250 --> 00:03:12,580
We got two presentations this morning,

00:03:12,580 --> 00:03:15,190
one by Daniel Xu from Facebook,

00:03:15,190 --> 00:03:17,400
and the other one by Julien Desfossez

00:03:17,400 --> 00:03:19,883
and Vineeth Remanan Pillai,

00:03:20,717 --> 00:03:21,600
hopefully I'm pronouncing it properly,

00:03:21,600 --> 00:03:23,410
from Digital Ocean.

00:03:23,410 --> 00:03:27,080
The first presentation is about out of memory daemon two,

00:03:27,080 --> 00:03:29,650
and the second one is about taming hyper-threads

00:03:29,650 --> 00:03:31,403
to be secure, all right.

00:03:32,320 --> 00:03:36,430
Daniel will prefer having the questions after his talk.

00:03:36,430 --> 00:03:38,910
Julien and Vineeth don't mind you asking questions,

00:03:38,910 --> 00:03:41,280
depending on how it goes, all right.

00:03:41,280 --> 00:03:43,530
In case you've never seen the mics,

00:03:43,530 --> 00:03:45,900
these are it, right.

00:03:45,900 --> 00:03:48,200
So if you raise your hand for a question,

00:03:48,200 --> 00:03:50,303
I'll throw 'em at you, just make sure you catch it.

00:03:51,325 --> 00:03:52,810
And so on.

00:03:52,810 --> 00:03:54,540
From that, that's pretty much it from me,

00:03:54,540 --> 00:03:56,716
and I'll let Daniel get going.

00:03:56,716 --> 00:03:58,220
All right, thank you.

00:03:58,220 --> 00:03:59,053
- Thank you.

00:04:01,550 --> 00:04:04,830
All right, cool, thanks for showing up, everyone.

00:04:04,830 --> 00:04:08,340
So this talk is about oomd, yeah.

00:04:08,340 --> 00:04:09,840
So I'm Daniel, I work at Facebook,

00:04:09,840 --> 00:04:12,830
I do a lot of Linux-related things,

00:04:12,830 --> 00:04:13,780
oomd's one of them.

00:04:15,702 --> 00:04:18,010
So to give a brief overview of this presentation,

00:04:18,010 --> 00:04:20,580
I'm gonna talk about motivations and past developments.

00:04:20,580 --> 00:04:25,003
So that's like, I gave a similar talk last year,

00:04:25,003 --> 00:04:28,270
and so that'll be kind of a recap of that.

00:04:28,270 --> 00:04:29,340
Then I'll talk about the present state,

00:04:29,340 --> 00:04:31,520
so things that have happened since then,

00:04:31,520 --> 00:04:33,350
and then I'll talk about some future plans,

00:04:33,350 --> 00:04:35,530
like stuff that's not set in stone yet,

00:04:35,530 --> 00:04:38,410
stuff that I'm planning on, that we're planning on doing.

00:04:38,410 --> 00:04:40,220
This talks runs a little short,

00:04:40,220 --> 00:04:42,820
so we'll have plenty of time for Q and A at the end.

00:04:46,422 --> 00:04:49,027
So to back up a little bit, the overall goal,

00:04:50,034 --> 00:04:51,310
to talk to about why oomd exists,

00:04:51,310 --> 00:04:53,380
so first we've gotta talk about resource control

00:04:53,380 --> 00:04:54,550
at Facebook.

00:04:54,550 --> 00:04:56,610
So the goal of resource control at Facebook is

00:04:56,610 --> 00:04:59,390
resource isolation across applications

00:04:59,390 --> 00:05:00,780
in a work-conserving manner.

00:05:00,780 --> 00:05:02,310
So you don't leave stuff on the table

00:05:02,310 --> 00:05:06,170
if no one is using it, or if people need it,

00:05:06,170 --> 00:05:07,283
applications need it.

00:05:08,210 --> 00:05:09,380
This is a really active area of development,

00:05:09,380 --> 00:05:11,260
so it's not a really well-solved problem yet,

00:05:11,260 --> 00:05:12,580
as it turns out.

00:05:12,580 --> 00:05:15,240
It's pretty hard to isolate applications from each other.

00:05:15,240 --> 00:05:16,980
So if one application uses a lot of memory

00:05:16,980 --> 00:05:18,690
or if it starts slamming the disk,

00:05:18,690 --> 00:05:20,610
it's actually pretty hard to prevent

00:05:20,610 --> 00:05:21,820
that application from interfering

00:05:21,820 --> 00:05:24,187
with another application, as it turns out.

00:05:25,630 --> 00:05:28,250
The main use case that we were trying to build this for

00:05:28,250 --> 00:05:30,640
is protecting the workload right now.

00:05:30,640 --> 00:05:31,560
So if you think of it this way,

00:05:31,560 --> 00:05:33,540
there's some kind of service somewhere,

00:05:33,540 --> 00:05:35,160
you put a web server on it,

00:05:35,160 --> 00:05:37,830
you're trying to, so the whole purpose of that machine,

00:05:37,830 --> 00:05:39,510
why you're paying for it is to serve web traffic

00:05:39,510 --> 00:05:41,120
serving websites.

00:05:41,120 --> 00:05:42,800
You really wanna protect that at all costs.

00:05:42,800 --> 00:05:44,900
Everything else is generally secondary.

00:05:44,900 --> 00:05:47,140
So examples of secondary things are like,

00:05:47,140 --> 00:05:49,720
infrastructure management, like Chef or something,

00:05:49,720 --> 00:05:53,070
maybe logging daemons, maybe like health check stuff,

00:05:53,070 --> 00:05:54,750
generally stuff that can be retried at a later

00:05:54,750 --> 00:05:57,240
point in time if there's resource contention.

00:05:57,240 --> 00:05:58,880
You generally don't want these auxiliary things

00:05:58,880 --> 00:06:00,380
fighting for precious resources

00:06:00,380 --> 00:06:02,110
if the main workload really needs it

00:06:02,110 --> 00:06:03,233
at that point in time.

00:06:04,580 --> 00:06:06,980
Another use case that is potentially very useful,

00:06:06,980 --> 00:06:08,890
but we haven't done too much work in that area yet,

00:06:08,890 --> 00:06:11,430
is side-loading of batch workloads.

00:06:11,430 --> 00:06:13,790
So for example, video transcoding,

00:06:13,790 --> 00:06:15,910
you probably need to transcode this video at some point

00:06:15,910 --> 00:06:18,220
in time, something like maybe before tomorrow,

00:06:18,220 --> 00:06:20,420
but it's not urgent that we do it like right now,

00:06:20,420 --> 00:06:23,000
and so if there's spare capacity on a host,

00:06:23,000 --> 00:06:24,380
you can sort of side-load it in,

00:06:24,380 --> 00:06:25,540
and then make sure it doesn't interfere

00:06:25,540 --> 00:06:27,755
with the main workload, so you can get some extra

00:06:27,755 --> 00:06:29,203
work out of your system.

00:06:30,530 --> 00:06:32,480
So we deployed this to several internal machine pools,

00:06:32,480 --> 00:06:34,910
so in the order of 10s of 1,000s of posts.

00:06:34,910 --> 00:06:37,120
So we've been doing this for a little bit of time now,

00:06:37,120 --> 00:06:40,120
so we've had some experience of deploying in production.

00:06:40,120 --> 00:06:42,610
I'll talk about the oomd-related stuff in this talk,

00:06:42,610 --> 00:06:43,850
but yeah, it's pretty interesting,

00:06:43,850 --> 00:06:46,493
it works pretty well overall, is the general gist of it.

00:06:47,467 --> 00:06:48,740
So Tejun at Facebook, he runs this whole

00:06:48,740 --> 00:06:50,370
resource control stuff.

00:06:50,370 --> 00:06:53,130
So he'll be giving talks at places,

00:06:53,130 --> 00:06:55,160
other conferences maybe.

00:06:55,160 --> 00:06:57,960
So if you happen to be there, it'd be good to check out.

00:06:59,450 --> 00:07:01,480
So where oomd fits in all this is that

00:07:01,480 --> 00:07:03,250
oomd steps in when the kernel resource

00:07:03,250 --> 00:07:05,240
isolation breaks down.

00:07:05,240 --> 00:07:06,520
And that happens pretty frequently,

00:07:06,520 --> 00:07:09,450
the main use case being, an application uses too much

00:07:09,450 --> 00:07:12,110
memory, oomd needs to step in 'cause the kernel

00:07:12,110 --> 00:07:14,880
just can't really do a great job of preventing this,

00:07:14,880 --> 00:07:17,180
and I'll talk more about that in later slides.

00:07:19,290 --> 00:07:20,140
So what is oomd?

00:07:20,140 --> 00:07:22,430
So oomd is an out-of-memory killing daemon

00:07:22,430 --> 00:07:24,750
that exists in userspace.

00:07:24,750 --> 00:07:26,220
We believe it's faster and more accurate

00:07:26,220 --> 00:07:28,120
than kernel OOM killer, and I'll talk more about that

00:07:28,120 --> 00:07:29,723
in the next couple of slides.

00:07:30,620 --> 00:07:33,790
Under the hood it uses cgroup2, PSI, and other system stats,

00:07:33,790 --> 00:07:35,510
the more traditional ones.

00:07:35,510 --> 00:07:37,190
For those not aware, PSI stands for

00:07:37,190 --> 00:07:38,230
pressure stall information,

00:07:38,230 --> 00:07:40,223
it's tabled by Johannes at Facebook.

00:07:41,090 --> 00:07:43,240
Essentially, it tells you how much wallclock

00:07:43,240 --> 00:07:45,650
time is lost due to resource surges on a system.

00:07:45,650 --> 00:07:48,630
And this actually turns out is one of the most important

00:07:48,630 --> 00:07:51,340
stats that applications care about.

00:07:51,340 --> 00:07:52,680
You generally don't really care how much

00:07:52,680 --> 00:07:55,050
total resources there are in a system

00:07:55,050 --> 00:07:56,710
until there's not enough, right.

00:07:56,710 --> 00:07:58,580
So you really just wanna find out if there's enough,

00:07:58,580 --> 00:08:02,930
and if there's not enough, how much work am I losing

00:08:02,930 --> 00:08:04,563
due to this resource shortage.

00:08:05,970 --> 00:08:08,250
oomd's open source, it's under GPL2,

00:08:08,250 --> 00:08:09,566
and there's a link to the GitHub

00:08:09,566 --> 00:08:10,700
if you want to check it out later.

00:08:10,700 --> 00:08:12,350
I'll put these slides online too.

00:08:15,140 --> 00:08:15,973
So why oomd?

00:08:15,973 --> 00:08:17,750
So why does oomd exist?

00:08:17,750 --> 00:08:20,980
So the kernel OOM killer configuration,

00:08:20,980 --> 00:08:23,070
I think is not very intuitive,

00:08:23,070 --> 00:08:24,850
so there's like a lot of different control files

00:08:24,850 --> 00:08:25,787
and knobs you can turn,

00:08:25,787 --> 00:08:28,180
and a lot of them just have numbers in them, right.

00:08:28,180 --> 00:08:30,700
So there's like, some of the go from negative 15

00:08:30,700 --> 00:08:33,301
to positive 16, some go from, I don't know,

00:08:33,301 --> 00:08:36,877
negative some thousand to positive some thousand,

00:08:36,877 --> 00:08:38,970
and it's kinda weird, it's not very ergonomic

00:08:38,970 --> 00:08:41,210
to think about, and it gets especially tricky

00:08:41,210 --> 00:08:42,590
when you have multiple teams running things

00:08:42,590 --> 00:08:43,650
on one machine.

00:08:43,650 --> 00:08:46,400
So like, the numbers don't have any inherent meaning,

00:08:46,400 --> 00:08:47,970
they only have meaning when you combine,

00:08:47,970 --> 00:08:49,480
compare it to other numbers, right.

00:08:49,480 --> 00:08:51,210
So it's a little difficult to coordinate.

00:08:51,210 --> 00:08:53,770
It's not impossible, but it's just really hard.

00:08:53,770 --> 00:08:55,180
Generally people at Facebook have sort of

00:08:55,180 --> 00:08:56,330
given up on doing that.

00:08:58,620 --> 00:08:59,760
One of the bad things about the kernel OOM killer

00:08:59,760 --> 00:09:01,690
is it's pretty slow to act as well.

00:09:01,690 --> 00:09:04,220
So the reason behind that is that the kernel OOM killer

00:09:04,220 --> 00:09:06,210
tries to protect kernel health,

00:09:06,210 --> 00:09:08,720
so if it thinks the kernel's making forward progress,

00:09:08,720 --> 00:09:11,221
it'll just keep going, nothing else is,

00:09:11,221 --> 00:09:12,584
so the kernel just keep moving on,

00:09:12,584 --> 00:09:15,330
it won't think anything bad is happening.

00:09:15,330 --> 00:09:17,290
When in reality, userspace could be live locked

00:09:17,290 --> 00:09:18,690
at this point, because even if the kernel

00:09:18,690 --> 00:09:20,870
is refaulting pages over and over,

00:09:20,870 --> 00:09:22,070
technically that's forward progress,

00:09:22,070 --> 00:09:24,163
and technically you're getting work done,

00:09:24,163 --> 00:09:25,699
but it's at such a slow rate,

00:09:25,699 --> 00:09:26,780
that it doesn't really matter,

00:09:26,780 --> 00:09:27,950
'cause just to userspace, everything's frozen,

00:09:27,950 --> 00:09:29,450
nothing's really getting done.

00:09:32,220 --> 00:09:34,080
The kernel OOM killer also doesn't have

00:09:34,080 --> 00:09:36,790
too much context on the logical composition of a system.

00:09:36,790 --> 00:09:38,970
All it cares is that there's an application running

00:09:38,970 --> 00:09:41,940
in userspace, it doesn't really care too much

00:09:41,940 --> 00:09:42,773
what it's doing.

00:09:43,720 --> 00:09:45,130
So for example, you could think of cases

00:09:45,130 --> 00:09:46,460
where there's two applications

00:09:46,460 --> 00:09:47,650
that should always be killed together,

00:09:47,650 --> 00:09:50,350
'cause one is kind of useless without the other,

00:09:50,350 --> 00:09:52,650
and then there's other cases where there's stuff

00:09:52,650 --> 00:09:54,270
that should never be killed together,

00:09:54,270 --> 00:09:56,961
like one thing should always be killed,

00:09:56,961 --> 00:09:58,111
or never killed at all.

00:09:59,310 --> 00:10:01,100
Another issue is that there's no really great way

00:10:01,100 --> 00:10:03,090
to customize kill action,

00:10:03,090 --> 00:10:04,680
because for some applications,

00:10:04,680 --> 00:10:06,872
it's good enough just to do a SIGTERM/SIGKILL,

00:10:06,872 --> 00:10:07,910
that's the traditional way.

00:10:07,910 --> 00:10:10,550
But for others, you might want like a full song and dance.

00:10:10,550 --> 00:10:12,990
So for example, if you're running with systemd,

00:10:12,990 --> 00:10:15,350
you might have overloaded a systemctl reload

00:10:15,350 --> 00:10:17,850
to do something special like say, a hot restart.

00:10:17,850 --> 00:10:19,790
Like how the D-Bus does it, right,

00:10:19,790 --> 00:10:21,968
you can pass the file descriptors

00:10:21,968 --> 00:10:22,900
from one process to another,

00:10:22,900 --> 00:10:24,480
and that way you can reset your service,

00:10:24,480 --> 00:10:25,760
but you don't drop any of the connections,

00:10:25,760 --> 00:10:27,680
and that's really useful for a lot of things

00:10:28,945 --> 00:10:31,610
where reconnection logic isn't built in.

00:10:31,610 --> 00:10:33,170
One of the things I've worked on is

00:10:33,170 --> 00:10:34,620
network block device, or NBD.

00:10:35,620 --> 00:10:38,710
No file system can, most file systems can't handle

00:10:38,710 --> 00:10:40,310
reconnect on the block layer very well,

00:10:40,310 --> 00:10:42,557
so you really don't want that to happen,

00:10:42,557 --> 00:10:43,993
so you really need a hot restart.

00:10:45,894 --> 00:10:47,020
So another problem with the kernel OOM killer

00:10:47,020 --> 00:10:48,410
is it's somewhat non-deterministic,

00:10:48,410 --> 00:10:50,900
or at least it's pretty hard to get fully deterministic.

00:10:50,900 --> 00:10:51,820
I'm sure there's a way to do it,

00:10:51,820 --> 00:10:53,880
so if someone's done it before,

00:10:53,880 --> 00:10:55,230
I'd be curious to hear about it,

00:10:55,230 --> 00:10:58,320
but generally we've given up on it at Facebook,

00:10:58,320 --> 00:11:00,253
and just try to use oomd instead.

00:11:02,490 --> 00:11:05,110
Oh another thing to mention is most machines

00:11:05,110 --> 00:11:07,320
at Facebook run with panic_on_oom enabled,

00:11:07,320 --> 00:11:10,140
and that's why it's, the reason for that is

00:11:10,140 --> 00:11:12,360
because kernel OOM killer is somewhat non-deterministic,

00:11:12,360 --> 00:11:14,440
so if the kernel OOM killer kills something,

00:11:14,440 --> 00:11:15,710
you're not really sure what it killed,

00:11:15,710 --> 00:11:18,640
and it'd be better to just restart the box

00:11:18,640 --> 00:11:20,710
to get it back into a known good state.

00:11:20,710 --> 00:11:22,410
And that makes sense if you have enough machines

00:11:22,410 --> 00:11:24,340
and enough spare capacity to do it,

00:11:24,340 --> 00:11:26,640
and things get auto-migrated.

00:11:26,640 --> 00:11:28,020
But for a lot of things, for smaller things,

00:11:28,020 --> 00:11:29,506
it probably matters less because

00:11:29,506 --> 00:11:32,263
you can probably configure it correctly.

00:11:34,360 --> 00:11:36,070
So this is a picture of, this is a graph

00:11:36,070 --> 00:11:37,710
of oomd deployment.

00:11:37,710 --> 00:11:39,240
So this is the panic_on_oom right before

00:11:39,240 --> 00:11:40,230
and after an oomd rollout,

00:11:40,230 --> 00:11:42,731
so you can sorta see a sharp downtick

00:11:42,731 --> 00:11:45,220
when oomd gets rolled out.

00:11:45,220 --> 00:11:47,040
You'll notice that there's no units on the Y axis.

00:11:47,040 --> 00:11:48,930
That's because we can't really show exact numbers,

00:11:48,930 --> 00:11:51,270
but I am allowed to say that this is one region

00:11:51,270 --> 00:11:52,920
for one machine pool at Facebook.

00:11:53,955 --> 00:11:56,150
And yeah, you can see it's pretty effective.

00:11:56,150 --> 00:11:58,460
This sharp downtick, but you'll notice that it doesn't

00:11:58,460 --> 00:12:00,570
quite drop to zero, although it gets closer to zero

00:12:00,570 --> 00:12:02,050
than before.

00:12:02,050 --> 00:12:03,393
The reason for that is, one of thing we learned is

00:12:03,393 --> 00:12:06,050
that I think the kernel OOM killer

00:12:06,050 --> 00:12:08,620
is sort of an unavoidable fact of reality.

00:12:08,620 --> 00:12:10,750
There's just so many weird spots the kernel can get into

00:12:10,750 --> 00:12:13,240
where it'll just OOM, and there's not much you can do.

00:12:13,240 --> 00:12:14,257
I mean, it'll run out of memory,

00:12:14,257 --> 00:12:16,549
and it won't run out of memory in the way you think it does.

00:12:16,549 --> 00:12:19,310
For example, with the min_free_kbyte stuff,

00:12:19,310 --> 00:12:21,520
if you don't get enough atomic memory, it'll OOM,

00:12:21,520 --> 00:12:23,730
but there's technically more memory in the system.

00:12:23,730 --> 00:12:25,608
So it's a legitimate OOM, but it doesn't,

00:12:25,608 --> 00:12:28,463
a lot of people wouldn't think of that as an OOM, right.

00:12:31,130 --> 00:12:32,580
Yeah.

00:12:32,580 --> 00:12:34,820
All right, so that pretty much covers the,

00:12:34,820 --> 00:12:36,110
there's a recap of all the stuff that

00:12:36,110 --> 00:12:38,450
I talked about last year if anyone was present.

00:12:38,450 --> 00:12:41,390
So now moving on to the stuff that has changed

00:12:41,390 --> 00:12:42,223
since last year.

00:12:43,810 --> 00:12:45,230
So we've got oomd2 in production.

00:12:45,230 --> 00:12:47,350
Everything is running oomd2 at Facebook.

00:12:47,350 --> 00:12:49,850
And what oomd2 is, is essentially it's a rule engine.

00:12:49,850 --> 00:12:51,990
You can sorta think of it like iptables or something.

00:12:51,990 --> 00:12:54,200
So it's like if this and this and this, do that,

00:12:54,200 --> 00:12:55,790
if this or that, do whatever,

00:12:55,790 --> 00:12:57,590
and you can compose a bunch of rules

00:12:58,471 --> 00:13:00,653
orthogonally to do configuration of system.

00:13:02,070 --> 00:13:03,840
Next couple of slides, it shows some examples

00:13:03,840 --> 00:13:06,500
of oomd configuration, so it might make more sense

00:13:06,500 --> 00:13:07,790
what's going on.

00:13:07,790 --> 00:13:08,797
But for now, we'll just talk about it

00:13:08,797 --> 00:13:10,297
and hand wave it a little bit.

00:13:11,300 --> 00:13:13,570
so we tried in the past, unsuccessfully tried

00:13:13,570 --> 00:13:15,120
was a couple of different things.

00:13:15,120 --> 00:13:16,990
So the first thing was monolithic config.

00:13:16,990 --> 00:13:18,210
And the reasoning behind that was

00:13:18,210 --> 00:13:19,880
that you don't really want to configure things

00:13:19,880 --> 00:13:21,300
if you don't have to, right.

00:13:21,300 --> 00:13:22,810
It'd be really great if something just worked

00:13:22,810 --> 00:13:25,010
out of the box and it did the correct thing,

00:13:25,880 --> 00:13:27,680
but unfortunately that's not really how it works.

00:13:27,680 --> 00:13:29,860
It was not very flexible, so it didn't,

00:13:29,860 --> 00:13:30,890
we had to scrap that idea.

00:13:30,890 --> 00:13:32,300
We tried, but then, you know, there's just

00:13:32,300 --> 00:13:35,110
too many different things you need to configure.

00:13:35,110 --> 00:13:37,190
The next thing we tried was plugin-only.

00:13:37,190 --> 00:13:39,300
So essentially, you give a hook to some plugin,

00:13:39,300 --> 00:13:40,730
and it's like, okay, your host has OOMed,

00:13:40,730 --> 00:13:42,450
now please do something.

00:13:42,450 --> 00:13:43,710
Turns out that didn't work either,

00:13:43,710 --> 00:13:44,830
that was a very short-lived idea,

00:13:44,830 --> 00:13:47,200
because no one really wants to write code,

00:13:47,200 --> 00:13:48,725
and no one really wants to understand a new thing,

00:13:48,725 --> 00:13:51,290
just through OOM killing.

00:13:51,290 --> 00:13:53,210
And looking back on it, that probably wouldn't

00:13:53,210 --> 00:13:55,140
have worked anyways, even if people were motivated

00:13:55,140 --> 00:13:57,630
enough to do this, because there was just a lot of gotchas

00:13:57,630 --> 00:14:00,120
you have when the host under resource shortages.

00:14:00,120 --> 00:14:03,300
And so one of the nice things about having core plugins

00:14:03,300 --> 00:14:06,030
is that you can ship a lot of domain knowledge

00:14:06,030 --> 00:14:07,500
in these plugins without people having to

00:14:07,500 --> 00:14:09,460
think about it too hard.

00:14:09,460 --> 00:14:10,950
And so core plugins is essentially,

00:14:10,950 --> 00:14:14,660
we have a bunch of very small self-contained plugins

00:14:14,660 --> 00:14:16,120
that do one thing and one thing only,

00:14:16,120 --> 00:14:18,800
and hopefully do exactly what you think they do.

00:14:18,800 --> 00:14:20,690
So an example would be like a swap_free plugin.

00:14:20,690 --> 00:14:21,960
Swap_free plugin literally tells you

00:14:21,960 --> 00:14:23,610
how much swap is off in a system.

00:14:24,569 --> 00:14:25,950
Memory_reclaim plugin tells you if a cgroup

00:14:25,950 --> 00:14:28,720
has reclaimed memory in the past for whatever duration.

00:14:28,720 --> 00:14:29,870
Simple stuff like that.

00:14:32,359 --> 00:14:33,300
And so building on the core plugin idea,

00:14:33,300 --> 00:14:35,420
it's sort of enables something I like to sorta call

00:14:35,420 --> 00:14:37,680
gotcha free plugins, where it's like,

00:14:37,680 --> 00:14:39,360
you don't get burned by things you don't know

00:14:39,360 --> 00:14:40,870
you don't know.

00:14:40,870 --> 00:14:42,350
Of course you can still make mistakes, right,

00:14:42,350 --> 00:14:44,560
so you can still have a typo in a cgroup,

00:14:44,560 --> 00:14:46,720
but it should be really obvious it's your mistake

00:14:46,720 --> 00:14:48,673
and not the system's mistake.

00:14:50,327 --> 00:14:52,300
And so you can encode the main knowledge.

00:14:52,300 --> 00:14:54,530
So one example that is a swap_free plugin.

00:14:54,530 --> 00:14:56,210
So if you have swap enabled on a system,

00:14:56,210 --> 00:14:59,040
and there's stuff in the swap, so pages in swap,

00:14:59,040 --> 00:15:01,430
and then you decide to turn off swap,

00:15:01,430 --> 00:15:02,650
the system has to bring all those pages

00:15:02,650 --> 00:15:03,483
back into main memory,

00:15:03,483 --> 00:15:04,850
and this takes some amount of time

00:15:04,850 --> 00:15:06,650
depending on how fast your disk is,

00:15:06,650 --> 00:15:09,780
and how much stuff is in main memory at that point in time.

00:15:09,780 --> 00:15:11,180
And so during that period of time, though,

00:15:11,180 --> 00:15:13,600
when the system's bringing stuff back into main memory,

00:15:13,600 --> 00:15:16,360
/proc/swaps and /proc/meminfo presents

00:15:16,360 --> 00:15:19,211
slightly different information that can be misleading

00:15:19,211 --> 00:15:21,300
until you get burned by it.

00:15:21,300 --> 00:15:23,210
So for example, /proc/meminfo sorta tells you

00:15:23,210 --> 00:15:25,369
you're at 100% swap usage,

00:15:25,369 --> 00:15:27,669
which makes sense because you technically are.

00:15:29,760 --> 00:15:31,380
You say there's no more available space,

00:15:31,380 --> 00:15:33,100
all the stuff that's there is trying to drain out,

00:15:33,100 --> 00:15:35,490
so it's 100% usage, but /proc/swaps gives

00:15:35,490 --> 00:15:37,210
a somewhat more complete picture of the problem,

00:15:37,210 --> 00:15:39,610
which is like, yes, you're at 100% usage,

00:15:39,610 --> 00:15:41,570
but the full capacity's actually whatever,

00:15:41,570 --> 00:15:44,050
that's at the size of the partition or something, right.

00:15:44,050 --> 00:15:45,690
And so we go burned by this 'cause we were only looking

00:15:45,690 --> 00:15:47,770
at meminfo, and then so if you turn off swap,

00:15:47,770 --> 00:15:50,120
this rule immediately fires saying you're out of swap.

00:15:50,120 --> 00:15:51,835
That's not technically true

00:15:51,835 --> 00:15:52,668
'cause you turned it off already.

00:15:52,668 --> 00:15:54,280
It's just in the process of draining.

00:15:54,280 --> 00:15:57,309
And so that's just one example of a gotcha you can have.

00:15:57,309 --> 00:15:58,700
And that's one example of things you can encode

00:15:58,700 --> 00:16:01,010
into a plugin because, yeah, you know,

00:16:01,010 --> 00:16:03,390
you fix it, you can add some comments in the code,

00:16:03,390 --> 00:16:04,870
but then when people use swap_free,

00:16:04,870 --> 00:16:07,120
it just does exactly what they think it does.

00:16:08,460 --> 00:16:10,120
So this is an example of a configuration.

00:16:10,120 --> 00:16:12,040
You probably can't really read it, that's fine.

00:16:12,040 --> 00:16:13,650
I have a simplified version here,

00:16:13,650 --> 00:16:17,600
where I pseudo-coded out the uninteresting details.

00:16:17,600 --> 00:16:20,170
I'll flip back to the other slide in a bit

00:16:20,170 --> 00:16:21,273
after I explain this.

00:16:22,500 --> 00:16:23,720
So essentially what this is saying here,

00:16:23,720 --> 00:16:25,880
so this is not a full configuration,

00:16:25,880 --> 00:16:28,170
the dot dot dots there say that there's more.

00:16:28,170 --> 00:16:30,795
So usually have a couple of rulesets

00:16:30,795 --> 00:16:32,670
that you compose together and you run on a system.

00:16:32,670 --> 00:16:33,990
So what this is saying is that,

00:16:33,990 --> 00:16:36,030
so please fire my detector,

00:16:36,030 --> 00:16:38,280
so if any of the detectors fire, please run the actions.

00:16:38,280 --> 00:16:41,850
So the first detector is fire if user.slice,

00:16:41,850 --> 00:16:46,140
workload.slice and www.slice slows by over 60%.

00:16:46,140 --> 00:16:48,350
The second detector is fired if system.slice

00:16:48,350 --> 00:16:50,060
slows by over 80%.

00:16:50,060 --> 00:16:53,520
And if either of those two fire, run the action,

00:16:53,520 --> 00:16:56,210
which is kill the largest memory hog on the system.

00:16:56,210 --> 00:16:58,980
This isn't necessarily a super-realistic config,

00:16:58,980 --> 00:17:00,920
but I mean it's just an example of how things

00:17:00,920 --> 00:17:01,770
technically work.

00:17:03,780 --> 00:17:06,880
And in this manner you can compose a lot of different rules,

00:17:06,880 --> 00:17:08,960
just chain things together, and then you can arrive at

00:17:08,960 --> 00:17:10,350
some kind of configuration that hopefully

00:17:10,350 --> 00:17:13,453
deals with most of the cases that your system experiences.

00:17:14,540 --> 00:17:15,897
So if you look back here, I don't know how well

00:17:15,897 --> 00:17:17,490
you can see this, but I can just sort of

00:17:17,490 --> 00:17:18,350
briefly talk about it.

00:17:18,350 --> 00:17:21,150
You can see a pressure_above plugin right here,

00:17:21,150 --> 00:17:25,200
which looks at the pressure on the specific cgroups.

00:17:25,200 --> 00:17:27,520
You have the threshold and duration variables.

00:17:27,520 --> 00:17:28,760
Then there's a memory_reclaim plugin,

00:17:28,760 --> 00:17:31,000
it checks if a cgroup has undergone memory reclamation

00:17:31,000 --> 00:17:33,960
in the last whatever seconds, sort of a sliding window.

00:17:33,960 --> 00:17:36,110
And then there's the other plugins as well.

00:17:37,140 --> 00:17:39,833
It's sorta like an arg CRV thing, pretty typical.

00:17:40,829 --> 00:17:41,662
You have the name of the plugin,

00:17:41,662 --> 00:17:44,583
the name of the program, and then you have the arguments.

00:17:47,120 --> 00:17:48,510
Another interesting thing that's happened

00:17:48,510 --> 00:17:50,700
is drop-in configurations.

00:17:50,700 --> 00:17:52,100
So if you're familiar with systemd at all,

00:17:52,100 --> 00:17:55,230
it's essentially the same as systemd drop-ins variations,

00:17:55,230 --> 00:17:56,480
and what this allows you to do is

00:17:56,480 --> 00:17:57,960
you alter the base configuration

00:17:57,960 --> 00:18:01,058
without having to modify the base config file.

00:18:01,058 --> 00:18:02,780
And this is super useful for when containers

00:18:02,780 --> 00:18:04,000
can move on and off hosts.

00:18:04,000 --> 00:18:06,050
So if you have a shared compute infrastructure,

00:18:06,050 --> 00:18:08,050
containers can migrate onto the host,

00:18:08,050 --> 00:18:09,550
and then migrate off the host,

00:18:10,473 --> 00:18:13,130
and in some cases you want that customization.

00:18:13,130 --> 00:18:14,880
So one example is if one container

00:18:14,880 --> 00:18:17,080
is managing its own container system,

00:18:17,080 --> 00:18:18,770
so there's nested containers.

00:18:18,770 --> 00:18:20,400
So if one of the nested containers

00:18:20,400 --> 00:18:22,370
tries to OOM the box, you don't necessarily

00:18:22,370 --> 00:18:24,500
wanna kill the entire top-level container.

00:18:24,500 --> 00:18:26,990
I mean it's technically correct, but it's suboptimal.

00:18:26,990 --> 00:18:29,070
You just wanna kill the most nested one.

00:18:29,070 --> 00:18:30,820
And it's one thing to do drop-in configurations

00:18:30,820 --> 00:18:33,100
is you can just say, so these containers can carry on

00:18:33,100 --> 00:18:35,710
specialized configuration snippets.

00:18:35,710 --> 00:18:37,640
That's like oh, don't kill the top-level thing,

00:18:37,640 --> 00:18:39,010
kill the nested thing.

00:18:39,010 --> 00:18:41,760
And if that doesn't work, revert to your base behavior.

00:18:43,620 --> 00:18:45,700
So one path that we considered doing but we didn't do

00:18:45,700 --> 00:18:47,140
was in-container oomd.

00:18:47,140 --> 00:18:50,680
So inside every container you ship your own oomd instance.

00:18:50,680 --> 00:18:52,780
The benefit of that would be that you don't need to write

00:18:52,780 --> 00:18:54,180
all the strapping configuration code

00:18:54,180 --> 00:18:56,390
and do all this cleanup and setup and stuff

00:18:56,390 --> 00:18:58,710
and make sure things don't get left over.

00:18:58,710 --> 00:19:00,960
But there's a number of different problems with that.

00:19:00,960 --> 00:19:02,930
So the first problem is that it's difficult

00:19:02,930 --> 00:19:04,660
to coordinate between the nested instance

00:19:04,660 --> 00:19:07,030
and the root host instance.

00:19:07,030 --> 00:19:08,990
So if the nested instance makes a kill,

00:19:08,990 --> 00:19:10,841
how do you coordinate that with the root host instance?

00:19:10,841 --> 00:19:12,350
That's why you'd have some sort of communication.

00:19:12,350 --> 00:19:14,157
If you don't you have to very clearly

00:19:14,157 --> 00:19:15,900
delegate responsibilities.

00:19:15,900 --> 00:19:18,050
And that turned out to be pretty difficult.

00:19:18,990 --> 00:19:20,200
You also don't want communication

00:19:20,200 --> 00:19:22,282
because then there's just more dependencies,

00:19:22,282 --> 00:19:24,315
more failure powers, like if you use D-Bus,

00:19:24,315 --> 00:19:25,620
then D-Bus is a dependency, if you use a file system,

00:19:25,620 --> 00:19:27,340
well a file system doesn't always work under

00:19:27,340 --> 00:19:31,010
resource shortage 'cause if you're out of memory,

00:19:31,010 --> 00:19:33,203
like I/O doesn't really work correctly,

00:19:33,203 --> 00:19:34,963
because of the way I/O and memory are interlinked.

00:19:35,870 --> 00:19:38,020
Another problem with that is that monitoring.

00:19:38,020 --> 00:19:40,060
So this might be more of a Facebookism,

00:19:40,060 --> 00:19:43,190
but it's like in-container monitoring infrastructure

00:19:43,190 --> 00:19:45,100
is different than the out-of-container infrastructure.

00:19:45,100 --> 00:19:46,440
So we didn't want to really maintain

00:19:46,440 --> 00:19:49,250
two separate things that did the same thing.

00:19:49,250 --> 00:19:52,760
Another big issue was that it's really hard

00:19:52,760 --> 00:19:54,590
to coordinate releases for in-container

00:19:54,590 --> 00:19:55,530
and out-of-container.

00:19:55,530 --> 00:19:57,993
So if you upgrade the version on the root host instance,

00:19:57,993 --> 00:20:01,670
you have to coordinate with the in-container version too,

00:20:01,670 --> 00:20:03,330
because otherwise you have two different versions running

00:20:03,330 --> 00:20:04,690
with potentially different issues

00:20:04,690 --> 00:20:06,490
and it becomes really hard to debug.

00:20:07,661 --> 00:20:09,300
And so, even though you update the root host version,

00:20:09,300 --> 00:20:10,820
you don't necessarily want to update

00:20:10,820 --> 00:20:11,990
the in-container version 'cause that's not

00:20:11,990 --> 00:20:13,170
how things are designed.

00:20:13,170 --> 00:20:14,720
Nor is it the way it should be.

00:20:16,080 --> 00:20:17,680
So yeah, that's why we didn't do it,

00:20:17,680 --> 00:20:19,210
even though it was tempting because there was

00:20:19,210 --> 00:20:20,600
way less coding to do, right.

00:20:20,600 --> 00:20:22,060
But ultimately, I think it was more maintainable

00:20:22,060 --> 00:20:24,433
to go with the root host drop-in stuff.

00:20:26,830 --> 00:20:27,663
So we've learnt some lessons.

00:20:27,663 --> 00:20:30,050
So these are some pretty high-level lessons.

00:20:30,050 --> 00:20:32,140
Some of you probably might be familiar with this already.

00:20:32,140 --> 00:20:33,890
We also learned some more low-level things,

00:20:33,890 --> 00:20:36,260
but that's not really relevant,

00:20:36,260 --> 00:20:39,293
I don't think really anyone runs oomd on databases here.

00:20:40,890 --> 00:20:42,140
So the first thing was that most people,

00:20:42,140 --> 00:20:43,400
including myself, are really hazy

00:20:43,400 --> 00:20:45,070
on memory management details.

00:20:45,070 --> 00:20:46,160
So obviously, this being plumbers,

00:20:46,160 --> 00:20:47,719
I'm sure there's plenty of people

00:20:47,719 --> 00:20:49,810
who actually understand it, but it turns out

00:20:49,810 --> 00:20:51,030
it's very complicated under the hood

00:20:51,030 --> 00:20:52,990
and a lot of different things can cause an OOM,

00:20:52,990 --> 00:20:55,440
and it's not necessarily intuitive why that happens either.

00:20:55,440 --> 00:20:57,590
Like for example the min_free_kbytes thing.

00:20:58,840 --> 00:21:00,690
So it's pretty important that someone does it correctly,

00:21:00,690 --> 00:21:02,490
and that the work can be reused,

00:21:02,490 --> 00:21:04,350
because otherwise people spend way too much time

00:21:04,350 --> 00:21:06,100
wrapping their heads around the details,

00:21:06,100 --> 00:21:09,140
and sometimes people just wanna get some work done.

00:21:09,140 --> 00:21:11,430
Another thing is that OOMing is not a widely solved problem,

00:21:11,430 --> 00:21:12,610
at least for most places.

00:21:12,610 --> 00:21:14,860
So if you run any infrastructure on a significant scale,

00:21:14,860 --> 00:21:16,960
you're probably gonna run into OOM issues.

00:21:17,854 --> 00:21:18,860
And that's because bugs happen,

00:21:18,860 --> 00:21:19,970
you can have memory leaks and stuff,

00:21:19,970 --> 00:21:22,430
but also sometimes, some workloads just use a lot of memory,

00:21:22,430 --> 00:21:24,270
like for example, machine learning workloads,

00:21:24,270 --> 00:21:27,220
then tend to require really huge amount of memory

00:21:27,220 --> 00:21:29,902
just to do like their training stuff,

00:21:29,902 --> 00:21:31,843
and that can push machines right to the edge.

00:21:34,140 --> 00:21:35,840
Yeah, so a lot of things can trigger an OOM,

00:21:35,840 --> 00:21:37,690
and that's why understandable diagnostics are crucial.

00:21:37,690 --> 00:21:40,590
So you have the meminfo dump, that goes to the dmessage,

00:21:40,590 --> 00:21:42,000
but that's pretty cryptic for people

00:21:42,000 --> 00:21:42,870
who don't know what's going on,

00:21:42,870 --> 00:21:45,440
but it really is for people who do know what's going on.

00:21:45,440 --> 00:21:46,740
So we found what's really useful is,

00:21:46,740 --> 00:21:49,371
so each ruleset is tagged with a description.

00:21:49,371 --> 00:21:50,963
The example didn't really show this,

00:21:50,963 --> 00:21:52,620
but lot of rulesets are tagged with a sentence-long

00:21:52,620 --> 00:21:54,260
description of what it's trying to protect against.

00:21:54,260 --> 00:21:56,430
And so you just kinda show that message when the host OOMs

00:21:56,430 --> 00:21:58,508
and then it's like people have something

00:21:58,508 --> 00:21:59,341
to start looking at.

00:21:59,341 --> 00:22:01,090
Otherwise the meminfo dump is like,

00:22:01,090 --> 00:22:02,970
unless you know what you're doing already,

00:22:02,970 --> 00:22:04,920
there's like, how do you approach that?

00:22:07,410 --> 00:22:08,243
So future improvements.

00:22:08,243 --> 00:22:09,600
So there's a couple of future improvements

00:22:09,600 --> 00:22:12,270
that we could possibly do.

00:22:12,270 --> 00:22:14,780
So the first is like, I don't know if the person's

00:22:14,780 --> 00:22:16,010
in the room, but there's a,

00:22:16,010 --> 00:22:18,860
recently they had a epoll-able PSI files.

00:22:18,860 --> 00:22:22,090
So now you can wire it up so if whatever change happens

00:22:22,090 --> 00:22:25,402
in whatever duration, you can notify the epoll

00:22:25,402 --> 00:22:27,680
that something happened, and you can take

00:22:27,680 --> 00:22:28,790
appropriate action.

00:22:28,790 --> 00:22:30,090
And this was developed for Android

00:22:30,090 --> 00:22:32,140
when you need really fast response times,

00:22:32,140 --> 00:22:34,320
to like low memory situations.

00:22:34,320 --> 00:22:36,290
It seems pretty useful, we might be able to use it.

00:22:36,290 --> 00:22:38,422
So the main use case I see for oomd for this is,

00:22:38,422 --> 00:22:40,270
because we've found a bunch of complaints

00:22:40,270 --> 00:22:42,530
about oomd using a lot of CPU cycles,

00:22:42,530 --> 00:22:44,580
and when we've profiled it, it turned out

00:22:44,580 --> 00:22:47,300
it was mostly coming from memory.stat accesses,

00:22:47,300 --> 00:22:49,180
'cause the kernel's somewhat inefficient.

00:22:49,180 --> 00:22:50,880
So it used to be over, an operation

00:22:50,880 --> 00:22:52,760
'cause it had to iterate over every single cgroup

00:22:52,760 --> 00:22:55,040
on the system including cgroups that had not,

00:22:55,040 --> 00:22:57,503
are like dead but not reclaimed yet.

00:22:57,503 --> 00:22:58,350
So this gets pretty expensive

00:22:58,350 --> 00:23:00,583
on hosts that have a lot of cgroups on them.

00:23:01,430 --> 00:23:03,460
And that's recently been changed on the upstream kernel

00:23:03,460 --> 00:23:04,790
is to be an over one access,

00:23:04,790 --> 00:23:07,450
'cause now it's accounted on a per-CPU basis,

00:23:07,450 --> 00:23:10,110
which is like a per-CPU struct, that's passively counted.

00:23:10,110 --> 00:23:12,860
And so any access that it's over one, so it's much faster.

00:23:12,860 --> 00:23:14,990
So once that gets rolled out internally at Facebook,

00:23:14,990 --> 00:23:15,870
we'll see how well that works,

00:23:15,870 --> 00:23:17,770
and then if it works really well,

00:23:17,770 --> 00:23:19,410
then we might not need to do anything,

00:23:19,410 --> 00:23:21,400
but if doesn't, we might need to look at the epoll stuff

00:23:21,400 --> 00:23:22,880
to try and sort circuits and logic

00:23:22,880 --> 00:23:24,930
to try and do as little work as possible.

00:23:26,130 --> 00:23:28,660
Another thing that's coming with the kernel is iocost.

00:23:28,660 --> 00:23:30,865
I'm not sure what the progress is,

00:23:30,865 --> 00:23:32,480
but looks like it's making progress.

00:23:32,480 --> 00:23:33,830
Interesting thing about that is that,

00:23:33,830 --> 00:23:36,560
so oomd used to monitor live I/O metrics on the system,

00:23:36,560 --> 00:23:39,100
so you had to watch out for when I/O is oversubscribed,

00:23:39,100 --> 00:23:40,720
and remediate against it.

00:23:40,720 --> 00:23:42,387
Otherwise memory gets kinda screwed up too,

00:23:42,387 --> 00:23:45,633
'cause of how interconnected those two things are.

00:23:46,490 --> 00:23:48,440
So what's nice for iocost, it look it'll finally

00:23:48,440 --> 00:23:50,020
solve the problem.

00:23:50,020 --> 00:23:53,260
So iolnc worked, but it didn't work too, too well,

00:23:53,260 --> 00:23:55,260
but iocost seems to solve it much better,

00:23:55,260 --> 00:23:58,211
and what that'll let us do is avoid worrying about

00:23:58,211 --> 00:24:02,083
I/O issues, so we can just focus on the memory semantics.

00:24:03,170 --> 00:24:04,460
One interesting thing we're considering,

00:24:04,460 --> 00:24:06,480
so totally not set in stone, we still need to discuss

00:24:06,480 --> 00:24:10,410
with upstream systemd folks, is systemd oomd.

00:24:10,410 --> 00:24:11,520
So I think it'd be real interesting

00:24:11,520 --> 00:24:14,477
if we integrated oomd into systemd,

00:24:14,477 --> 00:24:15,820
'cause systemd's in this really nice spot

00:24:15,820 --> 00:24:17,610
in Linux infrastructure, where it's sitting between

00:24:17,610 --> 00:24:20,370
the kernel and the application, so they system layer.

00:24:20,370 --> 00:24:22,160
And it turns out this is a really interesting spot

00:24:22,160 --> 00:24:23,480
for oomd to live, 'cause that's kind of

00:24:23,480 --> 00:24:24,900
where it lives already.

00:24:24,900 --> 00:24:27,240
'Cause applications really shouldn't be worrying

00:24:27,240 --> 00:24:30,640
about system-level resource configuration.

00:24:30,640 --> 00:24:32,320
That's not really what they're there to do.

00:24:32,320 --> 00:24:34,230
And the kernel doesn't have too much insight

00:24:34,230 --> 00:24:39,230
into how the system should be set up in terms of resources,

00:24:39,500 --> 00:24:40,690
because, like I mentioned before,

00:24:40,690 --> 00:24:44,793
it's not really the kernel's job to manage all this policy.

00:24:45,650 --> 00:24:47,450
So one interesting thing, so I know there's been

00:24:47,450 --> 00:24:48,960
discussions on the mailing list about

00:24:48,960 --> 00:24:51,470
good out-of-the-box configurations for OOM killer,

00:24:51,470 --> 00:24:53,550
and so I think this might, so systemd oomd

00:24:53,550 --> 00:24:55,050
might be in a good position to deal with this

00:24:55,050 --> 00:24:57,330
because systemd has enough knowledge

00:24:57,330 --> 00:24:59,883
about how all the services are set up on a system,

00:24:59,883 --> 00:25:02,380
so it can introspect, and maybe it can add some more flags

00:25:02,380 --> 00:25:05,470
like say this slice, or the systemd service is interactive,

00:25:05,470 --> 00:25:07,890
so we need good responsiveness,

00:25:07,890 --> 00:25:10,170
and maybe some other stuff in that vein.

00:25:10,170 --> 00:25:12,440
And then since systemd can introspect when it starts up,

00:25:12,440 --> 00:25:14,260
and then possibly come up with a good configuration

00:25:14,260 --> 00:25:17,370
for oomd, and maybe it'll be a good enough out-of-the-box

00:25:17,370 --> 00:25:19,010
to deal with this kind of OOM issue.

00:25:19,010 --> 00:25:20,780
But I don't know, we'll have to see, obviously.

00:25:20,780 --> 00:25:25,260
So again to reiterate on, not firm plans,

00:25:25,260 --> 00:25:27,350
we still have to talk to the systemd people,

00:25:27,350 --> 00:25:29,455
and then maybe if it works out we'll play with it

00:25:29,455 --> 00:25:30,288
and see what we have to do.

00:25:31,250 --> 00:25:32,500
Yeah, I mean we'll have to be ready with oomd2

00:25:32,500 --> 00:25:35,070
if this happens, but yeah, I don't know,

00:25:35,070 --> 00:25:37,326
it's like the fourth rewrite.

00:25:37,326 --> 00:25:39,326
Just another one, right.

00:25:40,242 --> 00:25:41,130
But yeah, that's pretty much it.

00:25:41,130 --> 00:25:43,170
So, oh yeah, and one other thing,

00:25:43,170 --> 00:25:44,890
if anyone wants to talk about this systemd stuff,

00:25:44,890 --> 00:25:47,960
if they have any ideas, I would love to hear it,

00:25:47,960 --> 00:25:50,750
'cause better get this right in the beginning.

00:25:50,750 --> 00:25:53,190
But yeah, so that's the end of the content.

00:25:53,190 --> 00:25:55,040
Time for questions if anyone has any.

00:25:59,610 --> 00:26:03,219
- [Man] Okay, I am working this project since

00:26:03,219 --> 00:26:05,134
last plumbers conference.

00:26:05,134 --> 00:26:08,040
But I was wondering whether it is possible

00:26:08,040 --> 00:26:10,470
to use it with cgroup version one,

00:26:10,470 --> 00:26:13,570
because what I have seen in kernel,

00:26:13,570 --> 00:26:17,970
is that really pressure stall information

00:26:17,970 --> 00:26:20,443
is not available for cgroup V1,

00:26:21,356 --> 00:26:26,356
and I wasn't able to make it work with cgroup version one

00:26:26,780 --> 00:26:31,780
just registering the hooks inside cgroups.

00:26:33,532 --> 00:26:35,963
What do you suggest to do?

00:26:37,640 --> 00:26:39,360
- So I don't think cgroup one is gonna work.

00:26:39,360 --> 00:26:42,290
I don't think they're taking any feature patches

00:26:42,290 --> 00:26:43,830
for cgroup one anymore.

00:26:43,830 --> 00:26:46,000
And Johannes is in back there and he might know.

00:26:46,000 --> 00:26:47,970
I don't know, just copout.

00:26:47,970 --> 00:26:50,780
But yeah, so I guess my answer is proper crack answer

00:26:51,643 --> 00:26:53,420
is you have to migrate to cgroup two.

00:26:53,420 --> 00:26:54,730
And I think other projects are doing it too,

00:26:54,730 --> 00:26:57,823
like Docker is considering it and working on it.

00:26:59,640 --> 00:27:00,840
- [Man] Okay, thank you.

00:27:05,299 --> 00:27:10,010
- You talked about integrating,

00:27:10,010 --> 00:27:13,277
if I understood correctly, integrating oomd into systemd.

00:27:14,210 --> 00:27:16,910
In Android, we basically have them separately,

00:27:16,910 --> 00:27:20,190
but they talk to each other, so basically AMS,

00:27:20,190 --> 00:27:23,810
which is Android System Manager, basically,

00:27:23,810 --> 00:27:28,130
talks to LMKD and tells it what's important,

00:27:28,130 --> 00:27:30,860
what's unimportant, so might be interesting venue

00:27:30,860 --> 00:27:35,800
to look at instead of integrating everything into one

00:27:36,840 --> 00:27:39,940
piece of, one software, basically,

00:27:39,940 --> 00:27:42,300
having those separately but talk to each other

00:27:42,300 --> 00:27:45,470
to explain what's important, what's not.

00:27:45,470 --> 00:27:47,098
- Yeah, that's interesting, I'd like to talk

00:27:47,098 --> 00:27:48,473
about that later.

00:27:49,555 --> 00:27:51,680
Yeah, offline, but...

00:27:51,680 --> 00:27:53,550
Yeah, I mean it's obviously something we consider,

00:27:53,550 --> 00:27:56,790
but I don't know, if they're already talking to each other,

00:27:56,790 --> 00:27:59,853
I sort of think of that as integration already, right.

00:28:01,200 --> 00:28:03,310
Yeah, if you have to make systemd changes

00:28:03,310 --> 00:28:06,633
to probably understand that stuff, then oomd changes too.

00:28:09,370 --> 00:28:11,380
- So in GNU we are actually moving to

00:28:11,380 --> 00:28:13,760
systemd managed session now, and there's also been

00:28:13,760 --> 00:28:16,650
discussions to force the out-of-memory killer

00:28:16,650 --> 00:28:19,570
in some situations just to make sure that

00:28:20,570 --> 00:28:23,060
we are not running into low memory situations,

00:28:23,060 --> 00:28:26,050
and it seems like oomd would be a much better

00:28:26,050 --> 00:28:28,535
solution for us in principle, but we do have the issue that

00:28:28,535 --> 00:28:32,020
this GNOME session is running on the systemd

00:28:32,020 --> 00:28:34,000
user instance, which is a bit similar

00:28:34,000 --> 00:28:36,960
to the container case, and I wonder if,

00:28:36,960 --> 00:28:39,460
how well the separation works with oomd,

00:28:39,460 --> 00:28:42,910
or do we just run two instances, one on the system,

00:28:42,910 --> 00:28:47,050
one on the user session, or what do you see

00:28:47,050 --> 00:28:49,193
the path forward on that please?

00:28:50,236 --> 00:28:52,490
- Well you can always have oomd running just one instance,

00:28:52,490 --> 00:28:55,950
and you just only monitor stuff under the user session.

00:28:55,950 --> 00:28:57,660
That theoretically could work,

00:28:57,660 --> 00:28:59,430
if I'm understanding the question correctly.

00:28:59,430 --> 00:29:04,430
- So we would run oomd inside the user session only

00:29:04,850 --> 00:29:06,700
rather than on site D.

00:29:06,700 --> 00:29:08,090
- So you just run it on the root host,

00:29:08,090 --> 00:29:10,960
so you run it like, I don't know, some other slice like,

00:29:10,960 --> 00:29:12,730
we usually call it host-critical that slice,

00:29:12,730 --> 00:29:14,860
stuff that really needs to be up on the system.

00:29:14,860 --> 00:29:17,840
And then you would only monitor stuff under user.slice,

00:29:17,840 --> 00:29:19,860
'cause oomd doesn't have to monitor the entire system

00:29:19,860 --> 00:29:21,210
if you don't want it to.

00:29:21,210 --> 00:29:23,750
You can configure just to look at some portion

00:29:23,750 --> 00:29:24,600
of the hierarchy.

00:29:28,730 --> 00:29:31,940
- I think the issue is that, on the, sorry,

00:29:31,940 --> 00:29:34,280
on the host system, you don't really see

00:29:34,280 --> 00:29:36,710
what's going on inside the user.

00:29:36,710 --> 00:29:41,310
So killing of one user application might be hard

00:29:41,310 --> 00:29:42,473
from the host.

00:29:47,100 --> 00:29:48,860
- Yeah, I mean I think we should talk about this later,

00:29:48,860 --> 00:29:50,900
get a whiteboard or something.

00:29:50,900 --> 00:29:52,410
But I feel like it should be possible,

00:29:52,410 --> 00:29:54,060
maybe we just need to discuss it.

00:30:01,270 --> 00:30:02,720
- So I have two questions.

00:30:02,720 --> 00:30:07,330
One is how does oomd decide that, having killed

00:30:07,330 --> 00:30:09,730
some container, it's enough?

00:30:09,730 --> 00:30:12,930
When do we decide to, okay, to check again

00:30:12,930 --> 00:30:15,180
the preferred levels or some other metrics

00:30:15,180 --> 00:30:16,603
to decide to kill again?

00:30:18,330 --> 00:30:20,240
And the other question is...

00:30:23,970 --> 00:30:25,433
Let's stay with the first.

00:30:26,290 --> 00:30:27,260
- Okay, so to answer the question,

00:30:27,260 --> 00:30:28,430
it's not a very elegant solution,

00:30:28,430 --> 00:30:30,020
but it seems to work pretty well.

00:30:30,020 --> 00:30:32,220
If there's a post-action delay it's like,

00:30:32,220 --> 00:30:34,840
by default it's like, I don't know, 20 or 30 seconds.

00:30:34,840 --> 00:30:36,600
It just sleeps for 30 seconds,

00:30:36,600 --> 00:30:38,600
and then checks the regular stuff again.

00:30:39,480 --> 00:30:43,170
- My second question is, do you have a concept

00:30:43,170 --> 00:30:46,010
of a good and bad reclaim?

00:30:46,010 --> 00:30:49,630
As actually in some cases you may want to allow reclaim

00:30:49,630 --> 00:30:53,670
if it's not reclaiming, if it's not causing thrashing

00:30:53,670 --> 00:30:56,030
and in other cases the same amount of reclaim

00:30:56,030 --> 00:30:58,605
can be bad because it's causing thrashing.

00:30:58,605 --> 00:31:03,070
Can oomd notice that somehow?

00:31:03,070 --> 00:31:05,250
- Not right now, but that sounds like a pretty good idea,

00:31:05,250 --> 00:31:06,143
if it's possible.

00:31:15,890 --> 00:31:18,890
- I saw in your abstract you mentioned there is a

00:31:18,890 --> 00:31:23,890
predict way to detect OOM killer scenarios.

00:31:25,630 --> 00:31:29,750
Just wondering how do you predict those scenarios,

00:31:29,750 --> 00:31:34,750
and why don't just change the threshold

00:31:35,180 --> 00:31:36,923
to trigger the OOM killer?

00:31:39,460 --> 00:31:40,770
- So which thresholds are you talking about

00:31:40,770 --> 00:31:42,323
for the kernel OOM killer?

00:31:44,920 --> 00:31:49,714
- I mean, you mentioned that you can predict

00:31:49,714 --> 00:31:51,404
OOM scenarios.

00:31:51,404 --> 00:31:53,050
- Oh, okay. - So how do you do that?

00:31:53,050 --> 00:31:55,232
- So usually if the pressure values are high enough,

00:31:55,232 --> 00:31:57,230
if they're like 50%, it means the application's

00:31:57,230 --> 00:32:00,900
spending 50% of its time just waiting for resources.

00:32:00,900 --> 00:32:02,220
So it's usually a pretty good indicator

00:32:02,220 --> 00:32:04,610
that the host is pretty much out of resources.

00:32:04,610 --> 00:32:07,300
Like obviously you need to play around with the thresholds

00:32:07,300 --> 00:32:10,380
like we've done, but yeah, at a higher value,

00:32:10,380 --> 00:32:11,880
the application's pretty much doing nothing,

00:32:11,880 --> 00:32:16,880
and it's pretty good, you can pretty much tell

00:32:17,280 --> 00:32:18,730
it's trending towards OOMing.

00:32:19,740 --> 00:32:20,693
- [Audience Member] Okay.

00:32:31,760 --> 00:32:33,450
- So if I understand this correctly,

00:32:33,450 --> 00:32:36,300
the oomd is killing based on monitoring,

00:32:36,300 --> 00:32:40,650
so it's seeing that certain values are rising

00:32:40,650 --> 00:32:42,740
and then it makes a decision.

00:32:42,740 --> 00:32:45,700
Is there any desire for something similar to core dumps,

00:32:45,700 --> 00:32:47,420
where you can actually have a hook in the kernel,

00:32:47,420 --> 00:32:50,043
so the kernel can actually trigger

00:32:50,043 --> 00:32:53,070
a userspace application, in this case oomd

00:32:53,070 --> 00:32:55,629
when it jumps in, because there are certain situations

00:32:55,629 --> 00:32:58,310
with page faults and things like this,

00:32:58,310 --> 00:32:59,820
there's no way a userspace application

00:32:59,820 --> 00:33:00,820
could react in time.

00:33:01,920 --> 00:33:03,940
And so this is nice thing about core dumps,

00:33:03,940 --> 00:33:05,500
the core dumping feature in the kernel,

00:33:05,500 --> 00:33:06,920
is that you can actually configure it

00:33:06,920 --> 00:33:10,090
to trigger a userspace application to handle the core dumps.

00:33:10,090 --> 00:33:12,310
And it seems like that interface is missing

00:33:12,310 --> 00:33:13,840
for the OOM killer, that it can trigger

00:33:13,840 --> 00:33:17,430
a userspace application to handle that decision.

00:33:17,430 --> 00:33:18,550
- So I think that actually exists.

00:33:18,550 --> 00:33:22,920
There's a OOM event, eventfd I think for cgroup, I think.

00:33:22,920 --> 00:33:23,753
- In cgroups, yeah.

00:33:23,753 --> 00:33:27,307
But on a system, on the host level, there's not anything.

00:33:27,307 --> 00:33:28,140
As far as I know.

00:33:28,140 --> 00:33:29,060
- Yeah, so I think that's a good idea,

00:33:29,060 --> 00:33:31,440
but then I think you run into the issue of policy.

00:33:31,440 --> 00:33:33,800
Now you have to describe to the kernel

00:33:33,800 --> 00:33:35,560
exactly what you're looking for,

00:33:35,560 --> 00:33:37,300
and maybe if you have a bunch of different hooks,

00:33:37,300 --> 00:33:39,180
that makes a lot of sense.

00:33:39,180 --> 00:33:41,790
But in general, I feel like PSI is that hook.

00:33:41,790 --> 00:33:43,360
You just hook into PSI, if it gets high enough

00:33:43,360 --> 00:33:44,223
you do something.

00:33:45,890 --> 00:33:47,700
Yeah, in general, like if you could get it

00:33:47,700 --> 00:33:50,450
fine-grained and orthogonal enough it might make sense.

00:33:52,080 --> 00:33:52,913
- 'Cause my concern is is there something

00:33:52,913 --> 00:33:54,770
maybe you're missing, and so then if the OOM killer

00:33:54,770 --> 00:33:58,420
does jump in, you have sort of a last resort

00:33:58,420 --> 00:34:01,570
that it should call the oomd again and say, hey.

00:34:01,570 --> 00:34:03,020
It should just start killing things, right,

00:34:03,020 --> 00:34:04,810
because at that point you've totally lost control

00:34:04,810 --> 00:34:07,140
of your system if the OOM killer does jump in.

00:34:07,140 --> 00:34:07,973
- Right.

00:34:09,360 --> 00:34:13,200
Yeah, I suppose you could hook into the more obscure cases,

00:34:13,200 --> 00:34:15,408
well, and just try and trigger the OOM killer.

00:34:15,408 --> 00:34:16,333
Yeah, that might be a good idea.

00:34:16,333 --> 00:34:17,677
- [Audience Member] Just (mumbles)

00:34:17,677 --> 00:34:18,510
Thank you.

00:34:25,468 --> 00:34:28,051
- [Kareem] Any other questions?

00:34:31,298 --> 00:34:32,996
Right, well, thank you Daniel.

00:34:32,996 --> 00:34:34,282
- Yeah, thank you.

00:34:34,282 --> 00:34:36,812

YouTube URL: https://www.youtube.com/watch?v=24x1-jo9G8k


