Title: LPC2019 - LAG and hardware offload to support RDMA and IO virtualized interfaces
Publication date: 2019-09-17
Playlist: Linux Plumbers Conference 2019
Description: 
	LAG and hardware offload to support RDMA and IO virtualized interfaces

Speakers
Mr Vivek Kashyap (Intel)
Ms Anjali Singhai Jain (Intel)
Dr Piotr Uminski (Intel)
Description
Link Aggregation (LAG) is traditionally served by bonding driver. Linux bonding driver supports all LAG modes on almost any LAN drivers - in the software. However modern hardware features like SR-IOV-based virtualization and state full offloads such as RDMA are currently not well supported by this model. One of possible options to solve that is to implement LAG functionality entirely in NIC's hardware or firmware. In our presentation we present another approach, where LAG functionality for state full offloads such as RDMA and IO virtualization is implemented mostly in software, with very limited support from existing Hardware and firmware. A concept that should make the solution more generic without complicating the HW any further.

The presentation is focused on 3 areas: implementation of active-backup mode for RDMA and virtual functions, usage of RX hash value to implement flow-based active-active mode and new active-active mode for virtual functions.

Proposed implementation of the active-backup mode for RDMA is done in RDMA and LAN drivers. An application continues using direct HW support for RDMA. LAN driver (with the help of RDMA driver) observes notifications from the bonding driver and accordingly controls low-level TX scheduling and RX rules for RDMA queues. The same mechanism can be used to transparently redirect network virtual functions from active to backup. We further explore the use of RX hash to implement active-active mode.
Captions: 
	00:00:01,617 --> 00:00:05,770
- Piotr from Intel is going to give us a discussion

00:00:05,770 --> 00:00:10,320
of RDMA and IO virtualized devices and hardware offload.

00:00:10,320 --> 00:00:14,033
So if you please give a nice, warm welcome for Piotr.

00:00:15,212 --> 00:00:18,775
(audience applauds)

00:00:18,775 --> 00:00:21,853
- Hello everybody, I hope that you can hear me well.

00:00:23,230 --> 00:00:28,230
The joint activity I did together with Vivek and Anjali

00:00:29,180 --> 00:00:32,450
from Intel from Oregon.

00:00:32,450 --> 00:00:33,283
And

00:00:35,180 --> 00:00:37,475
just the corporate stuff.

00:00:37,475 --> 00:00:41,120
When doing this presentation I just try to remember

00:00:41,120 --> 00:00:44,688
what is the link aggregation and explain what issues--

00:00:44,688 --> 00:00:46,750
(man speaking away from mic)

00:00:46,750 --> 00:00:50,140
What issues we might have in that.

00:00:50,140 --> 00:00:54,740
Then, we'll go deeply on how we are going to handle

00:00:54,740 --> 00:00:58,530
the RDMA offloading for the link aggregation

00:00:58,530 --> 00:01:02,330
and access to the virtual machines.

00:01:02,330 --> 00:01:04,035
And finally, then we'll have some

00:01:04,035 --> 00:01:06,240
for the discussion, questions

00:01:06,240 --> 00:01:07,893
and you know, regular stuff.

00:01:08,930 --> 00:01:12,007
So just to remember, link aggregation

00:01:12,007 --> 00:01:16,820
provides you a separate, using multiple physical links

00:01:16,820 --> 00:01:19,420
between the host and the suite.

00:01:19,420 --> 00:01:24,420
And it's provided for the full tolerance, so active backup.

00:01:24,890 --> 00:01:28,530
If one link fail, another one can be used seamlessly

00:01:28,530 --> 00:01:31,050
to the application in the upper stack.

00:01:31,050 --> 00:01:35,350
And also the active configuration,

00:01:35,350 --> 00:01:36,950
you can utilize both links.

00:01:36,950 --> 00:01:41,300
So we increase the overall capability

00:01:41,300 --> 00:01:44,770
of the connection without changes in the upper stack.

00:01:44,770 --> 00:01:48,830
And in the Linux it's implemented mostly in the software,

00:01:48,830 --> 00:01:52,750
so you have the bonding driver.

00:01:52,750 --> 00:01:57,750
So the software entity, which sits between the TCP/IP stack

00:01:58,280 --> 00:02:03,280
and the LAN driver, which is specific for the given NIC.

00:02:03,690 --> 00:02:05,570
So it is done that way,

00:02:05,570 --> 00:02:08,097
that the bonding driver just talks to both

00:02:10,357 --> 00:02:13,350
of multiple NIC drivers.

00:02:13,350 --> 00:02:16,280
And NIC drivers

00:02:16,280 --> 00:02:20,900
might know about the bonding, but might not.

00:02:20,900 --> 00:02:25,830
And the way of communication is that the bonded driver

00:02:25,830 --> 00:02:30,543
sends some notifications using NETDEV notifications and the

00:02:33,030 --> 00:02:37,860
NIC driver might adjust it's activity to that, or might not.

00:02:37,860 --> 00:02:42,400
Most of the NIC drivers does not.

00:02:42,400 --> 00:02:44,874
So this scheme is pretty much generic

00:02:44,874 --> 00:02:48,670
and you can do the link aggregation

00:02:49,775 --> 00:02:52,130
between two ports on the same NIC.

00:02:52,130 --> 00:02:55,030
But you can also do the link aggregation

00:02:55,030 --> 00:02:57,977
between two different NICs

00:02:57,977 --> 00:03:01,567
or two ports from the different NIC.

00:03:04,700 --> 00:03:08,450
So it's fine, it works for many cases for a long time.

00:03:08,450 --> 00:03:10,859
So what is the problem?

00:03:10,859 --> 00:03:13,987
The problem is that when we are talking

00:03:13,987 --> 00:03:16,750
about the hardware acceleration.

00:03:16,750 --> 00:03:20,793
So the first thing is the hardware accelerated RDMA.

00:03:20,793 --> 00:03:24,430
So even if on the wire the RDMA packets looks like

00:03:25,651 --> 00:03:28,620
another IP packet.

00:03:28,620 --> 00:03:31,150
In fact, on the driver, they're accelerated

00:03:31,150 --> 00:03:33,400
and it's accelerated in that way

00:03:33,400 --> 00:03:37,820
that hardware queues are accessed directly,

00:03:37,820 --> 00:03:39,780
or almost directly by the application.

00:03:39,780 --> 00:03:44,780
So the packets are not going to the regular to TCP stack.

00:03:45,920 --> 00:03:50,720
So somehow they are parallel to the bonding drivers.

00:03:50,720 --> 00:03:55,147
So even if we have the setup with the bonded divers

00:03:56,620 --> 00:04:00,660
and you have the link aggregation for the LAN traffic,

00:04:00,660 --> 00:04:02,840
the same time, if you are using

00:04:02,840 --> 00:04:04,970
on the same installation RDMA,

00:04:04,970 --> 00:04:07,720
RDMA is not affected by that in regular situation.

00:04:07,720 --> 00:04:11,475
So you if don't have the, even such basic functionality

00:04:11,475 --> 00:04:14,980
like the full tolerance of the RDMA traffic,

00:04:14,980 --> 00:04:18,407
so it may affect, for example, your storage application.

00:04:18,407 --> 00:04:21,350
And okay, also you cannot boost the performance

00:04:22,399 --> 00:04:27,130
by using multiple links for the RDMA.

00:04:27,130 --> 00:04:30,230
So you are losing certain capacity

00:04:30,230 --> 00:04:33,880
which are important for the applications using the RDMA.

00:04:33,880 --> 00:04:36,610
And also this is for the virtual machine.

00:04:36,610 --> 00:04:41,120
So virtual machines directly or indirectly

00:04:41,120 --> 00:04:45,320
using SR-IOV and virtual functions.

00:04:45,320 --> 00:04:49,840
This is the efficient way of talking to the hardware

00:04:49,840 --> 00:04:53,680
without unnecessary delay.

00:04:53,680 --> 00:04:58,300
So this is fine, but it's also the virtual machine

00:04:58,300 --> 00:05:01,157
talks to the, gets its own queues

00:05:01,157 --> 00:05:05,730
and somehow work independently to the bonding driver.

00:05:05,730 --> 00:05:10,050
So there are some solution then,

00:05:10,050 --> 00:05:12,253
for example, you give,

00:05:16,197 --> 00:05:18,570
for the two ports configuration,

00:05:21,741 --> 00:05:24,350
you provide the virtual for both ports

00:05:24,350 --> 00:05:29,350
and let the virtual machine do the bonding by themself.

00:05:30,960 --> 00:05:33,081
But it's inefficient, first you have to expose

00:05:33,081 --> 00:05:36,700
the underlying network to there virtual machine.

00:05:36,700 --> 00:05:40,678
So this is somehow against the idea of virtualization.

00:05:40,678 --> 00:05:45,290
And it's also, it mean that you have multiple instances

00:05:46,184 --> 00:05:50,220
of the same bonding driver deciding what to do,

00:05:50,220 --> 00:05:51,550
which is the best way.

00:05:51,550 --> 00:05:54,343
So not very well solution.

00:05:54,343 --> 00:05:56,690
So we have also the problem with

00:05:58,280 --> 00:06:01,727
how to provide the seamless access to the

00:06:01,727 --> 00:06:05,453
link aggregation for the virtual machines.

00:06:07,270 --> 00:06:11,970
And here we have the typical configuration

00:06:13,294 --> 00:06:16,700
of the multiple, multi-port NIC here,

00:06:16,700 --> 00:06:18,750
there's two ports but might be more.

00:06:18,750 --> 00:06:21,974
And each port has its own queues.

00:06:21,974 --> 00:06:26,610
Each port is exposed to the upper layer

00:06:26,610 --> 00:06:28,820
like a separate physical function

00:06:28,820 --> 00:06:31,063
with separate virtual functions.

00:06:31,063 --> 00:06:34,696
And each port also internally consists

00:06:34,696 --> 00:06:38,840
of a single virtual ethernet bridge.

00:06:38,840 --> 00:06:42,470
So, from the upper side,

00:06:42,470 --> 00:06:45,350
upper layers it looks like two NICs,

00:06:45,350 --> 00:06:47,360
even if physically, it's one.

00:06:47,360 --> 00:06:52,360
So, how we are going to somehow handle that?

00:06:52,980 --> 00:06:56,580
We are going to implement in the very beginning

00:06:56,580 --> 00:07:01,210
active backup solution for the link aggregation

00:07:01,210 --> 00:07:06,210
and for the RDMA and for virtual functions.

00:07:06,527 --> 00:07:09,840
The idea is just to instead you

00:07:09,840 --> 00:07:12,270
now creating brand new hardware,

00:07:12,270 --> 00:07:17,270
we will just handle most of the stuff

00:07:17,460 --> 00:07:20,840
in the software with some small changes

00:07:20,840 --> 00:07:23,707
in the underlying hardware and firmware.

00:07:24,753 --> 00:07:28,007
And the concept is generic,

00:07:28,007 --> 00:07:33,007
so you can use it in multiple hardware drivers.

00:07:34,160 --> 00:07:38,280
But the details like which operation or I'm calling,

00:07:38,280 --> 00:07:43,280
they're, of course, they're hardware specific.

00:07:44,730 --> 00:07:49,730
But this approach does not need to change anything

00:07:50,370 --> 00:07:55,370
above the driver, so it's specific for the particular NIC.

00:07:56,830 --> 00:07:59,550
So let me go to the

00:08:02,386 --> 00:08:03,219
detail solution.

00:08:03,219 --> 00:08:06,320
Here is a pretty busy slide.

00:08:06,320 --> 00:08:09,045
Basically this is the situation

00:08:09,045 --> 00:08:11,548
when we have the link aggregation

00:08:11,548 --> 00:08:14,530
reconfigured and it's begun before the failover.

00:08:14,530 --> 00:08:19,530
Green is active, red is backup currently.

00:08:20,520 --> 00:08:23,274
So, first of all, for the RDMA,

00:08:23,274 --> 00:08:26,508
we allocated the RDMA queues

00:08:26,508 --> 00:08:29,380
only from the active physical function.

00:08:29,380 --> 00:08:32,590
The backup is queues,

00:08:32,590 --> 00:08:36,150
resources from the backup queues are not used.

00:08:36,150 --> 00:08:39,641
I will go back to that later,

00:08:39,641 --> 00:08:42,610
whether we can reuse it or not.

00:08:42,610 --> 00:08:45,660
And so the LAN connection is

00:08:48,870 --> 00:08:50,340
done like previously.

00:08:50,340 --> 00:08:55,340
So we have two NIC drivers for now both physical functions.

00:08:56,230 --> 00:09:00,300
One is working like active, one is working like a backup.

00:09:00,300 --> 00:09:05,300
Bonding driver is exposing a single logical interface

00:09:07,733 --> 00:09:09,860
back to the stack.

00:09:09,860 --> 00:09:13,350
The management is by NIC and statistics are calculated

00:09:13,350 --> 00:09:18,350
by the NIC hardware exposed via the NIC interface

00:09:18,404 --> 00:09:22,723
to the bonding driver and up to the stack.

00:09:23,570 --> 00:09:26,910
And, so the LAN traffic is handled exactly the same,

00:09:26,910 --> 00:09:29,743
but there's a change so far on the,

00:09:30,623 --> 00:09:32,804
without those changes we have two virtual bridges.

00:09:32,804 --> 00:09:36,280
Right now we have one singular virtual bridge.

00:09:36,280 --> 00:09:38,470
And of course, into this virtual bridge,

00:09:38,470 --> 00:09:41,320
we have to program some rules

00:09:41,320 --> 00:09:44,300
so the traffic received on port zero

00:09:44,300 --> 00:09:46,953
will go to the hardware resources allocated

00:09:46,953 --> 00:09:50,820
for the physical function zero

00:09:50,820 --> 00:09:53,660
and something were to go for the port one,

00:09:53,660 --> 00:09:56,970
is handled by the hardware queues

00:09:59,100 --> 00:10:01,451
for the port for one.

00:10:01,451 --> 00:10:05,430
So it looks almost like

00:10:06,942 --> 00:10:09,627
a regular configuration.

00:10:09,627 --> 00:10:14,423
And then, let's see what happens when we have a failover.

00:10:15,460 --> 00:10:19,143
So let's assume that port zero failed.

00:10:20,615 --> 00:10:24,370
So it will detect it, bonding driver detect it

00:10:25,990 --> 00:10:30,510
based on the regular methods like timeout like pings.

00:10:30,510 --> 00:10:34,640
And so the bonding driver tests the LAN drivers.

00:10:34,640 --> 00:10:37,010
underlying LAN drivers, okay you are no longer

00:10:38,573 --> 00:10:41,570
the backup driver, you are the active one.

00:10:41,570 --> 00:10:44,610
You are not longer the active one, you are backup

00:10:44,610 --> 00:10:47,610
and being done by the notification.

00:10:47,610 --> 00:10:51,878
So what have to be done from the transmission point of view?

00:10:51,878 --> 00:10:53,122
For the transmission part,

00:10:53,122 --> 00:10:56,890
we have to reconfigure the scheduling.

00:10:57,810 --> 00:11:02,040
So the LAN traffic is going via the new NIC port.

00:11:04,759 --> 00:11:06,794
So you don't need to change anything,

00:11:06,794 --> 00:11:09,711
but the RDMA traffic is still going

00:11:11,223 --> 00:11:15,470
via the regular queues from the port zero.

00:11:15,470 --> 00:11:18,862
So what we have to do, we don't want to change,

00:11:18,862 --> 00:11:21,190
touch the queues itself because it means

00:11:21,190 --> 00:11:23,670
that you have to touch the applications.

00:11:23,670 --> 00:11:26,760
So the session will be broken

00:11:26,760 --> 00:11:30,090
and the loss of the connectivity will take forever.

00:11:30,090 --> 00:11:31,730
So what we are going to change?

00:11:31,730 --> 00:11:33,990
We are not going to change the hardware queues,

00:11:33,990 --> 00:11:35,810
hardware queues are the same,

00:11:35,810 --> 00:11:38,330
but we are going to change the way

00:11:38,330 --> 00:11:41,640
how we schedule the traffic from those queues.

00:11:41,640 --> 00:11:44,895
So previously it was scheduled by port zero.

00:11:44,895 --> 00:11:49,460
Right now we are moving that from scheduling mechanics

00:11:49,460 --> 00:11:51,120
of scheduling three from port zero

00:11:51,120 --> 00:11:54,843
to the scheduling mechanics for port one, all traffic.

00:11:55,690 --> 00:11:58,323
So it's doable

00:11:58,323 --> 00:12:02,830
and it mean that after a short period

00:12:02,830 --> 00:12:07,050
of the traffic disruption,

00:12:07,050 --> 00:12:11,420
the same traffic will go via the new port one,

00:12:12,450 --> 00:12:14,023
which is the new active port.

00:12:15,511 --> 00:12:19,190
Okay, so that was the transmission part

00:12:19,190 --> 00:12:21,390
and there's a receiving part.

00:12:21,390 --> 00:12:24,420
It is a very basic application

00:12:24,420 --> 00:12:26,850
but as you will remember,

00:12:26,850 --> 00:12:31,840
we created a single virtual bridge for both ports.

00:12:31,840 --> 00:12:33,750
So what we have to do right now,

00:12:33,750 --> 00:12:37,180
we have to change the switch rules,

00:12:37,180 --> 00:12:42,180
so the traffic received for the port one, LAN traffic,

00:12:42,927 --> 00:12:47,927
will go to the NIC driver for the physical function one.

00:12:50,990 --> 00:12:55,534
But in the same time, the RDMA traffic is not redirected.

00:12:55,534 --> 00:12:58,200
RDMA traffic from the port one

00:12:58,200 --> 00:13:01,883
go to the old receiving queues for the RDMA.

00:13:05,644 --> 00:13:09,394
And you have also to take the control traffic

00:13:10,331 --> 00:13:13,998
like LLDP, it's receiving on the right port.

00:13:16,650 --> 00:13:19,320
So this and everything is done

00:13:19,320 --> 00:13:22,403
under the control of the bonding driver.

00:13:24,946 --> 00:13:25,913
All right so,

00:13:27,590 --> 00:13:29,180
let's move forward.

00:13:29,180 --> 00:13:33,056
One thing which I no sure whether you realize it.

00:13:33,056 --> 00:13:37,950
I clearly say, okay, I'm going to schedule the queues

00:13:37,950 --> 00:13:40,270
which belongs to one physical function

00:13:40,270 --> 00:13:43,650
over the port which belongs to another physical function.

00:13:43,650 --> 00:13:46,230
So somehow it breaks, at least,

00:13:46,230 --> 00:13:50,320
the logical separation between those two physical functions.

00:13:50,320 --> 00:13:53,690
Of course, internally, the hardware

00:13:55,137 --> 00:13:57,563
is done in such way that, at least,

00:13:59,618 --> 00:14:01,890
certain resources are shared.

00:14:02,810 --> 00:14:06,120
And but we have to allow this

00:14:09,190 --> 00:14:10,660
sharing because otherwise

00:14:12,770 --> 00:14:15,690
it's administratively disabled.

00:14:15,690 --> 00:14:17,930
So normal implementation,

00:14:17,930 --> 00:14:20,480
I have to say is done by the firmware

00:14:20,480 --> 00:14:22,970
and we have to add new functions.

00:14:22,970 --> 00:14:27,430
So if we want two or more ports

00:14:27,430 --> 00:14:29,700
of physical function together,

00:14:29,700 --> 00:14:31,960
each of them have to explicitly say,

00:14:31,960 --> 00:14:34,980
okay, I'm going to be brought up to share resources

00:14:34,980 --> 00:14:37,640
with another physical function.

00:14:37,640 --> 00:14:42,640
So if all links, all physical functions agree on that,

00:14:44,800 --> 00:14:46,450
the sharing is started.

00:14:46,450 --> 00:14:49,259
And you can move from one,

00:14:49,259 --> 00:14:53,380
you can move the queues belongs to one physical function

00:14:53,380 --> 00:14:55,636
to be scheduled to the port

00:14:55,636 --> 00:14:57,815
which formerly belongs another physical function

00:14:57,815 --> 00:14:59,341
and vice versa.

00:14:59,341 --> 00:15:04,193
Otherwise, it's not all physical functions agree on that,

00:15:04,193 --> 00:15:08,180
this means that it might be some security problems.

00:15:08,180 --> 00:15:09,400
It's not allowed.

00:15:12,725 --> 00:15:16,225
Okay, so what about the virtual functions?

00:15:17,818 --> 00:15:19,973
Virtual functions exposed,

00:15:21,537 --> 00:15:23,467
it give you, you know,

00:15:25,172 --> 00:15:30,172
the illusion of a single NIC for the virtual machine.

00:15:30,610 --> 00:15:33,450
You can either pass-through it to the machine

00:15:33,450 --> 00:15:37,070
but even if you handle it in the Hypervisor

00:15:37,070 --> 00:15:39,610
and use some software interface,

00:15:39,610 --> 00:15:43,490
to the virtual machine anyway,

00:15:43,490 --> 00:15:47,193
you are using the hardware acceleration and separation.

00:15:49,810 --> 00:15:53,100
But the virtual functions are mapped

00:15:53,100 --> 00:15:55,570
to the certain physical functions

00:15:55,570 --> 00:15:58,507
which is mapping to the certain port.

00:16:03,855 --> 00:16:08,730
In order to make it to use the bonding driver,

00:16:08,730 --> 00:16:11,148
we can actually treat those queues,

00:16:11,148 --> 00:16:16,130
the virtual queues exactly like the RDMA queues.

00:16:16,130 --> 00:16:19,251
Also, we are doing for the transmission,

00:16:19,251 --> 00:16:23,962
we are moving those queues from one scheduler tree

00:16:23,962 --> 00:16:25,595
to another scheduler tree.

00:16:25,595 --> 00:16:29,407
On the receiving side, we are redirecting traffic received

00:16:31,100 --> 00:16:36,100
from the backup port back to the original queues,

00:16:36,180 --> 00:16:38,300
in case of the failover.

00:16:38,300 --> 00:16:40,704
Pretty much the same because those queues

00:16:40,704 --> 00:16:43,960
somehow are similar to the RDMA queues

00:16:43,960 --> 00:16:44,920
on a certain level of,

00:16:44,920 --> 00:16:48,743
or a certain level of hardware management.

00:16:51,980 --> 00:16:55,430
And everything is done by the physical driver.

00:16:55,430 --> 00:16:59,400
Even if the virtual machine

00:16:59,400 --> 00:17:02,090
talks to the virtual function driver,

00:17:02,090 --> 00:17:04,850
internal implementation is such way

00:17:04,850 --> 00:17:09,850
that the virtual function driver

00:17:10,940 --> 00:17:13,510
communicates its request like

00:17:16,210 --> 00:17:18,010
queue allocation or queue behavior

00:17:18,010 --> 00:17:21,104
to the physical function driver.

00:17:21,104 --> 00:17:24,040
So physical driver, if it's not using

00:17:24,040 --> 00:17:28,120
the virtual function queues, know about them.

00:17:28,120 --> 00:17:30,250
So in case of the failover,

00:17:30,250 --> 00:17:31,880
when the bonding driver tells

00:17:33,518 --> 00:17:35,530
the NIC driver, okay, there's a failover.

00:17:35,530 --> 00:17:38,749
The physical function driver can do this,

00:17:38,749 --> 00:17:43,749
those reconfiguration without anything.

00:17:44,350 --> 00:17:49,350
And still the virtual function driver and even the VM

00:17:50,720 --> 00:17:54,146
does not need to know anything about that.

00:17:54,146 --> 00:17:58,560
Some peer packets though will be probably out serviced but

00:18:02,359 --> 00:18:07,170
there shouldn't be very, very long time for the disruption.

00:18:07,170 --> 00:18:11,070
Okay, so this is what I already described.

00:18:11,070 --> 00:18:13,443
So everything behave exactly the same

00:18:13,443 --> 00:18:17,493
or almost exactly the same like for the RDMA.

00:18:17,493 --> 00:18:18,326
And what is the difference?

00:18:18,326 --> 00:18:21,297
So the difference ism let's go back to the,

00:18:25,000 --> 00:18:27,050
let's say, basic situation.

00:18:27,050 --> 00:18:31,722
For the RDMA, RDMA is using the same IP addresses

00:18:31,722 --> 00:18:35,740
and the MAC addresses like the LAN driver.

00:18:35,740 --> 00:18:40,493
And it's quite simple for the virtual function.

00:18:40,493 --> 00:18:43,690
So for the VMs, it's most likely that each VM,

00:18:43,690 --> 00:18:46,222
it has it's own MAC address

00:18:46,222 --> 00:18:50,300
and each virtual function has its own MAC address.

00:18:50,300 --> 00:18:53,123
So for the, how it's,

00:18:55,341 --> 00:18:58,530
the bonding driver talk to the suite?

00:18:58,530 --> 00:19:02,530
Of course there are some protocol behavior,

00:19:02,530 --> 00:19:07,162
but if the bonding is implemented only on the host,

00:19:07,162 --> 00:19:08,200
it could be done.

00:19:08,200 --> 00:19:13,200
So just host automatically switch between active and backup.

00:19:13,720 --> 00:19:17,123
And it's done in such way that if we have,

00:19:18,860 --> 00:19:22,510
before the failover, the bonding driver

00:19:22,510 --> 00:19:26,640
is using the MAC address in the administratively assigned

00:19:26,640 --> 00:19:30,260
or just inherit from one of the port.

00:19:30,260 --> 00:19:32,700
But basically, port MAC address

00:19:32,700 --> 00:19:36,045
and the bonding MAC address is the same.

00:19:36,045 --> 00:19:39,573
And the suite knows what internal forwarding database,

00:19:43,508 --> 00:19:46,792
in here in our example, port zero.

00:19:46,792 --> 00:19:49,042
So if we have the failover,

00:19:51,425 --> 00:19:55,758
the host switches traffic and active to the port one

00:19:57,680 --> 00:20:02,680
and so, but we have to tell the switch

00:20:03,490 --> 00:20:05,740
that something changes.

00:20:05,740 --> 00:20:08,550
So we can, it's done currently like that.

00:20:08,550 --> 00:20:13,550
That the bonding driver sends the LOOPBACK packet

00:20:13,740 --> 00:20:17,620
with its own MAC address but over the new active port,

00:20:17,620 --> 00:20:19,410
in this case port one.

00:20:19,410 --> 00:20:21,240
So the switch will see,

00:20:21,240 --> 00:20:24,910
okay, I get the MAC address on another port,

00:20:24,910 --> 00:20:28,990
so it changes its forwarding database

00:20:28,990 --> 00:20:33,990
and start communicate over the new backup port.

00:20:37,563 --> 00:20:40,320
And in this case, we don't need for the packets sent

00:20:42,220 --> 00:20:46,212
from the stack,

00:20:46,212 --> 00:20:50,270
so we can reduce the number of lost packets.

00:20:50,270 --> 00:20:53,293
So everything is fine, but we have a problem

00:20:53,293 --> 00:20:58,130
with link aggregation for virtual machine.

00:20:58,130 --> 00:21:00,270
So let's assume that

00:21:01,837 --> 00:21:04,215
we are able to reconfigure

00:21:04,215 --> 00:21:06,390
our internal resources in such way

00:21:06,390 --> 00:21:11,390
that the queues assigned to the virtual machine

00:21:12,240 --> 00:21:14,550
will operate on the new port.

00:21:14,550 --> 00:21:16,460
There's a failover on port,

00:21:16,460 --> 00:21:18,587
there was an error on port zero.

00:21:18,587 --> 00:21:23,587
So the failover performed and our internal drivers

00:21:24,969 --> 00:21:27,120
are reconfigured that way

00:21:27,120 --> 00:21:31,350
that the traffic will go via the port one.

00:21:31,350 --> 00:21:36,350
And the bonding driver will send the LOOPBACK to the suite.

00:21:36,400 --> 00:21:37,810
Okay, look they will say,

00:21:37,810 --> 00:21:42,810
a failover and my MAC address will be visible on other port.

00:21:43,283 --> 00:21:46,070
Everything is fine, but the bonding driver

00:21:46,070 --> 00:21:48,680
know only about the MAC address

00:21:48,680 --> 00:21:51,905
of the bare metal machine or the host machine.

00:21:51,905 --> 00:21:55,840
It doesn't know anything about the virtual machines.

00:21:57,935 --> 00:22:00,440
So the suite will see that

00:22:03,111 --> 00:22:08,111
the host network stack is visible on a different port,

00:22:10,330 --> 00:22:15,330
but the suite don't know about the moving virtual machines,

00:22:16,680 --> 00:22:19,030
because suite even don't know about,

00:22:19,030 --> 00:22:22,910
that there are virtual machines running on the host.

00:22:22,910 --> 00:22:25,820
They just see some MAC addresses.

00:22:25,820 --> 00:22:29,250
And as you can still see it's only same problem.

00:22:29,250 --> 00:22:33,090
So, how we can, so we have some certain problem,

00:22:33,090 --> 00:22:35,353
which we didn't have for the RDMA.

00:22:37,524 --> 00:22:42,130
How we are going to handle that?

00:22:42,130 --> 00:22:46,970
So the idea is that the LAN driver

00:22:46,970 --> 00:22:51,560
will take the roll of the bonding driver

00:22:51,560 --> 00:22:55,610
and will send the, so this is what the LOOPBACK packets

00:22:55,610 --> 00:22:57,420
over the new active ports.

00:22:57,420 --> 00:23:02,420
So as I mentioned, the bonding driver

00:23:02,611 --> 00:23:05,090
managing the switch over.

00:23:06,810 --> 00:23:09,187
LAN physical driver receiving,

00:23:10,690 --> 00:23:13,366
know that became an active port

00:23:13,366 --> 00:23:16,722
so but I also mentioned that even the creation

00:23:16,722 --> 00:23:21,722
of the virtual functions go via the physical port.

00:23:22,700 --> 00:23:27,700
So the physical driver know about the virtual functions.

00:23:29,001 --> 00:23:33,633
When It's managing that, when it's not interactive,

00:23:33,633 --> 00:23:35,177
interact with directly.

00:23:35,177 --> 00:23:38,596
So it has the list of virtual machine,

00:23:38,596 --> 00:23:43,596
virtual functions, so the extension is that NIC driver

00:23:43,910 --> 00:23:46,220
will send LOOPBACK packets on behalf

00:23:47,905 --> 00:23:48,960
on those virtual machines.

00:23:49,904 --> 00:23:53,948
So we'll teach the switch that also MAC addresses

00:23:53,948 --> 00:23:56,585
related to those virtual machines

00:23:56,585 --> 00:24:00,303
are moved to the new active port.

00:24:04,499 --> 00:24:09,499
Okay, and it's like I promised before,

00:24:12,030 --> 00:24:16,573
the basic concept is so that we are using the resources

00:24:18,490 --> 00:24:20,650
only from one physical function.

00:24:20,650 --> 00:24:24,373
So let's, if we are bonding two physical functions,

00:24:24,373 --> 00:24:28,840
well we decided, okay, let's allocate RDMA

00:24:28,840 --> 00:24:33,840
and virtual functions for one physical function,

00:24:34,590 --> 00:24:37,380
which is usually the active one.

00:24:37,380 --> 00:24:39,810
And originally, if the failover occurs,

00:24:39,810 --> 00:24:44,810
let's move them to another port.

00:24:44,810 --> 00:24:45,946
But in this case,

00:24:45,946 --> 00:24:50,020
we somehow waste certain hardware resources.

00:24:50,020 --> 00:24:55,020
Also this approach assume that we do the bonding first

00:24:56,720 --> 00:24:59,420
and then we allocate the hardware resources.

00:24:59,420 --> 00:25:03,850
But what if it's behave differently?

00:25:03,850 --> 00:25:06,313
So actually,

00:25:07,890 --> 00:25:10,784
we can extend the original scheme

00:25:10,784 --> 00:25:15,784
in such way that we can use hardware resources

00:25:16,060 --> 00:25:18,020
from both physical functions,

00:25:18,020 --> 00:25:20,000
current active and current backup.

00:25:20,000 --> 00:25:22,990
And the difference is that the resources

00:25:22,990 --> 00:25:27,857
from the current active are used almost like

00:25:29,850 --> 00:25:32,970
without bonding, directly talk to the hardware.

00:25:32,970 --> 00:25:34,755
But from the other hand,

00:25:34,755 --> 00:25:39,230
the resources from another physical function,

00:25:39,230 --> 00:25:43,043
they are reconfigured right after the failover.

00:25:43,043 --> 00:25:45,043
So all rules are changed

00:25:46,974 --> 00:25:51,050
and the transmission scheduling is over,

00:25:51,050 --> 00:25:54,600
let's say, another physical function.

00:25:54,600 --> 00:25:58,460
So it's like in the middle.

00:25:58,460 --> 00:26:00,591
Left side is working normally,

00:26:00,591 --> 00:26:04,560
right side is working as after the failover.

00:26:04,560 --> 00:26:06,860
If the actual failover occurs,

00:26:06,860 --> 00:26:11,860
the left side, port zero is reconfigured after the failover.

00:26:12,933 --> 00:26:16,730
But the right side will go back to the original,

00:26:16,730 --> 00:26:19,240
almost to the original configuration.

00:26:19,240 --> 00:26:21,824
So it's the architecture allows you

00:26:21,824 --> 00:26:26,824
to fully utilize resources for both sides.

00:26:28,935 --> 00:26:33,935
Okay, so close to the conclusions.

00:26:34,560 --> 00:26:36,810
So in this presentation,

00:26:36,810 --> 00:26:41,810
I try to address the active backup solution for the RDMA

00:26:41,810 --> 00:26:46,810
and also active backup approach for the virtual functions

00:26:47,963 --> 00:26:50,670
in a seamless way from the VM.

00:26:50,670 --> 00:26:53,660
What is open, and we are working on it right now,

00:26:53,660 --> 00:26:55,630
is an active-active scheme

00:26:55,630 --> 00:26:59,310
so we can use both ports in the same time

00:26:59,310 --> 00:27:04,073
without, you know, affecting the RDMA applications

00:27:05,220 --> 00:27:07,473
or virtual machine setups.

00:27:08,510 --> 00:27:13,510
And this is not ready yet, maybe next year.

00:27:13,510 --> 00:27:16,350
And I will have the opportunity

00:27:16,350 --> 00:27:18,560
to talk about it maybe earlier.

00:27:18,560 --> 00:27:21,660
And I am also looking for your comments,

00:27:21,660 --> 00:27:24,530
feedbacks, information on the overall architecture,

00:27:24,530 --> 00:27:25,990
whether you like it, don't like it,

00:27:25,990 --> 00:27:28,200
you think it could be done better.

00:27:28,200 --> 00:27:31,830
And more particular about sending

00:27:31,830 --> 00:27:35,570
the LOOPBACK packets by the NIC driver.

00:27:35,570 --> 00:27:39,000
Usually it is so that the upper stack

00:27:39,000 --> 00:27:42,010
is telling the NIC driver what to send

00:27:42,010 --> 00:27:47,010
and the NIC doesn't know passing the SKBs to the hardware.

00:27:47,070 --> 00:27:49,350
Here, packets are sent

00:27:50,849 --> 00:27:53,460
by the driver itself,

00:27:53,460 --> 00:27:55,937
based on its knowledge of this,

00:27:55,937 --> 00:27:58,793
of the entire network.

00:28:03,550 --> 00:28:06,000
Okay, so thank you for your time

00:28:06,000 --> 00:28:08,302
and if you have any questions or comments,

00:28:08,302 --> 00:28:10,200
I will be happy to answer. - I have a quick question.

00:28:10,200 --> 00:28:13,590
So when you move from the active to the backup,

00:28:13,590 --> 00:28:18,590
so those RDMA buffers are user-space component, right?

00:28:20,860 --> 00:28:23,410
Does user-space have to participate in this transfer

00:28:23,410 --> 00:28:25,490
of the QPs over to the active?

00:28:25,490 --> 00:28:30,433
- No, no, no, of course, during the reconfiguration itself,

00:28:31,390 --> 00:28:35,640
it's likely that you will lose one or two packet,

00:28:35,640 --> 00:28:38,950
but it shouldn't be sufficient just to preserve

00:28:41,597 --> 00:28:42,430
the session.

00:28:42,430 --> 00:28:44,690
But basically the user-space is not involved

00:28:46,661 --> 00:28:49,661
in this transfer. - Not at all, okay.

00:28:52,950 --> 00:28:54,100
- So I have a couple of questions,

00:28:54,100 --> 00:28:56,213
but I'll try to focus on a few.

00:28:58,020 --> 00:28:59,720
We're still not talking about active-active.

00:28:59,720 --> 00:29:02,055
So active backup, why do you want everything

00:29:02,055 --> 00:29:04,340
to be allocated on one function?

00:29:04,340 --> 00:29:07,010
Why not just, I mean, if you can steer the packets

00:29:07,010 --> 00:29:08,570
from right to left, left to right

00:29:08,570 --> 00:29:09,920
and the receive and send,

00:29:09,920 --> 00:29:11,240
so you can basically allocate

00:29:11,240 --> 00:29:14,439
wherever you want the RDMA queues or the VFs

00:29:14,439 --> 00:29:18,023
and then just steer them according to the bond decision.

00:29:19,150 --> 00:29:21,680
And I'm not clear if this session should've been

00:29:21,680 --> 00:29:24,370
in the RDMA mini session or in the NETDEV,

00:29:24,370 --> 00:29:26,389
because it seems like maybe it's a driver issues.

00:29:26,389 --> 00:29:29,470
It's not touching, not the network stack

00:29:29,470 --> 00:29:31,940
and maybe not even the RDMA stack,

00:29:31,940 --> 00:29:33,754
if I'm not mistaken. - Okay, so let me

00:29:33,754 --> 00:29:36,770
answer the question step-by-step.

00:29:36,770 --> 00:29:39,720
So basically, you are absolutely right,

00:29:39,720 --> 00:29:42,850
this is simple trick and I try to explain it here.

00:29:42,850 --> 00:29:46,863
But basically simple, it's easier for me just to explain

00:29:46,863 --> 00:29:49,640
moving from one site to another site.

00:29:49,640 --> 00:29:51,920
But yes, if you really like it,

00:29:51,920 --> 00:29:54,900
you can use resources from both site

00:29:54,900 --> 00:29:56,620
and configure it such way

00:29:56,620 --> 00:29:58,683
that you can move it between them.

00:29:59,720 --> 00:30:01,808
And I'm not sure whether I follow

00:30:01,808 --> 00:30:03,930
the second part of your question.

00:30:03,930 --> 00:30:04,763
Could you...

00:30:06,958 --> 00:30:08,642
- I will rephrase his question.

00:30:08,642 --> 00:30:11,910
You talked always about RDMA

00:30:11,910 --> 00:30:14,450
but can you provide more info

00:30:14,450 --> 00:30:17,450
about which type of RFMA are your referring?

00:30:17,450 --> 00:30:20,023
Are you talking about RoCE are talking about iWARP?

00:30:20,023 --> 00:30:22,490
What is the fabric of your RDMA?

00:30:22,490 --> 00:30:26,640
- Okay, so basically it's agnostic from that.

00:30:26,640 --> 00:30:29,290
So we are talking on that level

00:30:30,619 --> 00:30:31,550
of the solution.

00:30:31,550 --> 00:30:34,240
We are talking about the queues

00:30:34,240 --> 00:30:38,500
which are moved from one scheduler to another scheduler.

00:30:38,500 --> 00:30:40,090
We are talking about the

00:30:41,289 --> 00:30:43,907
received packets from one port vessels

00:30:45,664 --> 00:30:46,497
to another port.

00:30:46,497 --> 00:30:51,497
And I'm not talking about the protocol itself

00:30:51,660 --> 00:30:54,820
because it's done, let's say before or after,

00:30:54,820 --> 00:30:56,840
depending on how the direction, so I wouldn't--

00:30:56,840 --> 00:31:01,165
- Not in all RDMA devices ethernet exists

00:31:01,165 --> 00:31:06,165
and not in all RDMA fabrics MAC is available.

00:31:07,720 --> 00:31:12,300
So you should talk about fabric,

00:31:12,300 --> 00:31:15,280
you cannot disconnect fabric at all.

00:31:15,280 --> 00:31:20,280
The second question is, related to second slide.

00:31:21,864 --> 00:31:23,031
You say that--

00:31:24,188 --> 00:31:25,673
- [Piotr] You want me to move?

00:31:27,820 --> 00:31:29,073
- Yeah. - Okay.

00:31:29,952 --> 00:31:34,952
- No I think, second, can go, yeah, problem legacy solution.

00:31:35,250 --> 00:31:37,560
- Okay. - So the hardware RDMA

00:31:37,560 --> 00:31:38,830
doesn't work with LAG.

00:31:38,830 --> 00:31:43,381
Did you take a look on MLX-5 implementation--

00:31:43,381 --> 00:31:45,530
- Okay, maybe this is-- - By chance?

00:31:45,530 --> 00:31:48,390
- So maybe this is too strong, not always,

00:31:48,390 --> 00:31:53,193
but let's say the hardware RDMA does not work directly

00:31:53,193 --> 00:31:57,449
with this solution, bonding solution.

00:31:57,449 --> 00:32:01,300
There are some solution already available

00:32:01,300 --> 00:32:03,223
on the market, like you mentioned.

00:32:04,780 --> 00:32:08,450
But it is not, okay,

00:32:08,450 --> 00:32:11,660
not done in the bonding driver in the basic.

00:32:11,660 --> 00:32:13,460
You have to do something below that.

00:32:16,460 --> 00:32:19,640
- Yes, we are doing it in ethernet driver

00:32:19,640 --> 00:32:23,310
just because RDMA it's above of it.

00:32:23,310 --> 00:32:26,550
So the one who is controlling, it's RDMA device

00:32:26,550 --> 00:32:29,190
because one thing is performed IB device

00:32:29,190 --> 00:32:31,160
and not on NeDB device.

00:32:31,160 --> 00:32:33,440
Yes, implementation itself,

00:32:33,440 --> 00:32:38,440
it's performed inside NED device, just too easy at all,

00:32:38,670 --> 00:32:43,670
so I'm not so excited to see this slide.

00:32:44,850 --> 00:32:48,280
I'd say-- - Oh, I understand your point,

00:32:48,280 --> 00:32:51,230
maybe the message is too strong.

00:32:51,230 --> 00:32:53,567
But the idea is basically that

00:32:54,683 --> 00:32:59,530
the current software solution itself is not sufficient.

00:32:59,530 --> 00:33:03,930
You have to do something more like you mentioned

00:33:03,930 --> 00:33:05,720
or like I present for that.

00:33:09,654 --> 00:33:11,821
Thank you for the comment.

00:33:18,780 --> 00:33:20,060
- So you made the comment

00:33:20,060 --> 00:33:23,310
that you're gonna leave the generic part alone,

00:33:23,310 --> 00:33:25,420
you're going to leave the driver alone,

00:33:25,420 --> 00:33:27,420
and you're going to modify the firmware.

00:33:28,350 --> 00:33:29,840
If you look at this community,

00:33:29,840 --> 00:33:33,000
we've got complete control of the generic part.

00:33:33,000 --> 00:33:35,900
We got complete control of the driver.

00:33:35,900 --> 00:33:36,990
The firmware is just that blob

00:33:36,990 --> 00:33:40,330
to who we have no control whatsoever.

00:33:40,330 --> 00:33:42,140
So you need a slide in there saying

00:33:42,140 --> 00:33:45,853
why you decided to change the hardest thing to change.

00:33:46,858 --> 00:33:47,691
- All right, so.

00:33:49,725 --> 00:33:54,725
I think that the most important thing is that one.

00:33:55,057 --> 00:33:59,113
So currently, even if you have multiple parts NIC,

00:34:00,177 --> 00:34:02,980
most of the vendors, at least what I know,

00:34:02,980 --> 00:34:05,834
they will expose each port

00:34:05,834 --> 00:34:08,430
like a separate physical function,

00:34:08,430 --> 00:34:11,701
even if it's on the same hardware, yeah.

00:34:11,701 --> 00:34:16,701
And the physical function, it means that it gives you,

00:34:16,810 --> 00:34:19,860
at least, the logical separation between resources.

00:34:19,860 --> 00:34:21,910
And if you would like the traffic

00:34:21,910 --> 00:34:23,170
from one port to another port,

00:34:23,170 --> 00:34:26,360
it means that you have to, at least,

00:34:26,360 --> 00:34:28,370
to move part of the resources.

00:34:28,370 --> 00:34:31,849
I mean, the queue belongs to the one physical

00:34:31,849 --> 00:34:32,682
to another physical function.

00:34:35,494 --> 00:34:36,997
So at least this, if you have the proper implementation

00:34:42,249 --> 00:34:44,167
and separation of physical function,

00:34:44,167 --> 00:34:46,288
you have to train it.

00:34:46,288 --> 00:34:47,788
And I'm not saying

00:34:50,320 --> 00:34:54,060
the entire implementation is generic.

00:34:54,060 --> 00:34:55,950
The is concept is what is generic.

00:34:55,950 --> 00:34:58,270
How you are exactly going to implement

00:34:58,270 --> 00:35:00,770
and what exactly you have to implement

00:35:00,770 --> 00:35:04,340
you change it in your hardware, firmware,

00:35:04,340 --> 00:35:06,603
or whatever is below the drivers,

00:35:08,690 --> 00:35:11,670
it's up the solution on that.

00:35:11,670 --> 00:35:15,020
So there is not the solution will,

00:35:15,020 --> 00:35:17,320
the exact solution will not work for everyone,

00:35:17,320 --> 00:35:19,080
but the concept I hope,

00:35:19,080 --> 00:35:20,410
that might be inspired for--

00:35:20,410 --> 00:35:23,960
- I kinda think like, when I think about the situation

00:35:23,960 --> 00:35:25,083
it's kinda like,

00:35:27,350 --> 00:35:30,510
you're in this dead end because the RDMA stuff

00:35:30,510 --> 00:35:32,840
went around all the generic code

00:35:32,840 --> 00:35:34,110
and it's bypassing everything.

00:35:34,110 --> 00:35:37,380
And the only thing that manages both sets of packet paths

00:35:37,380 --> 00:35:40,300
is that damned firmware that's in the card.

00:35:40,300 --> 00:35:43,100
And that's why this problem exists the way that it does.

00:35:46,118 --> 00:35:47,736
Whoa.

00:35:47,736 --> 00:35:50,280
- Sorry. - So I think within

00:35:50,280 --> 00:35:52,234
the constraints of the current software model we have,

00:35:52,234 --> 00:35:54,840
this may be a viable solution.

00:35:54,840 --> 00:35:56,718
But I think comes back to,

00:35:56,718 --> 00:35:59,550
really boils down to both what David and Andrew said.

00:35:59,550 --> 00:36:02,656
Is that if we had a better model,

00:36:02,656 --> 00:36:07,440
if we didn't represent the two ports as two separate PFs,

00:36:07,440 --> 00:36:09,110
which bears little relationship

00:36:09,110 --> 00:36:11,200
to what the hardware does these days,

00:36:11,200 --> 00:36:13,830
then we might be able to explore a better model.

00:36:13,830 --> 00:36:16,193
And maybe that's a conversation worth having.

00:36:17,390 --> 00:36:21,617
But perhaps, what's being proposed here is more constrained.

00:36:21,617 --> 00:36:24,523
It's to basically work within the current model.

00:36:25,920 --> 00:36:29,430
So I think from that point of view, it's kind of reasonable.

00:36:29,430 --> 00:36:30,760
You might be aware that

00:36:30,760 --> 00:36:33,130
there has been work done on alternate models,

00:36:33,130 --> 00:36:38,130
like, well, it's a misnomer, but what Switchdev turned into,

00:36:38,150 --> 00:36:41,970
and in this case, we can represent things differently.

00:36:41,970 --> 00:36:43,610
And it doesn't solve this particular problem

00:36:43,610 --> 00:36:44,650
but it is related.

00:36:44,650 --> 00:36:48,250
We have solved a related bonding problem using this

00:36:48,250 --> 00:36:51,023
without no RDMA.

00:36:51,932 --> 00:36:55,313
And also, I'll just mention that not all cards model,

00:36:56,330 --> 00:36:58,763
or necessarily model, has this PF to port model,

00:36:59,750 --> 00:37:01,350
that's true. - So what I have to say,

00:37:01,350 --> 00:37:04,580
I somehow agree that, I can only speak

00:37:04,580 --> 00:37:06,450
from the Linux point of view, yeah,

00:37:06,450 --> 00:37:09,260
I really agree on that.

00:37:09,260 --> 00:37:13,590
So this model of separation is not necessarily very nice.

00:37:13,590 --> 00:37:18,220
Especially, still you can see both drivers and you can,

00:37:18,220 --> 00:37:23,035
but also, we have also too problem of, you know,

00:37:23,035 --> 00:37:26,887
of the legacy, other operating system.

00:37:26,887 --> 00:37:31,887
So that's why I can see that the doing like you mentioned,

00:37:33,080 --> 00:37:35,010
basically we stick with that model

00:37:35,010 --> 00:37:38,510
because we would like to fight with the link aggregation

00:37:38,510 --> 00:37:40,810
not with the entire model,

00:37:40,810 --> 00:37:45,360
how we are representing ports for the operating system,

00:37:45,360 --> 00:37:48,113
for multiple operating systems.

00:37:49,560 --> 00:37:54,560
- As you mentioned, the big reason for bonding

00:37:54,980 --> 00:37:58,917
is to provide redundancy and failover.

00:37:58,917 --> 00:38:02,680
If I want a full redundancy in a configuration,

00:38:02,680 --> 00:38:04,320
I would use two different cards

00:38:04,320 --> 00:38:05,900
and would not use the two ports

00:38:05,900 --> 00:38:07,990
that are on the same card plugged into

00:38:07,990 --> 00:38:10,160
the same slot of the machine.

00:38:10,160 --> 00:38:14,930
Do you have any ideas how you could provide bonding

00:38:14,930 --> 00:38:19,200
for virtual functions SR-IOV or even RDMA,

00:38:19,200 --> 00:38:21,780
if the ports are on two different cards?

00:38:21,780 --> 00:38:25,990
- So I don't have the answer for the hardware resource

00:38:25,990 --> 00:38:27,370
because they are two cards.

00:38:27,370 --> 00:38:29,460
So even if they are from the same vendor

00:38:29,460 --> 00:38:32,053
it might be from different versions.

00:38:33,240 --> 00:38:36,320
I think that for the virtual machine,

00:38:36,320 --> 00:38:38,213
you might consider doing something

00:38:38,213 --> 00:38:43,213
in the Hypervisor level and deliver the software interface

00:38:43,690 --> 00:38:44,640
to the virtual machine.

00:38:44,640 --> 00:38:48,010
There is other topics on that, but I cannot see,

00:38:48,010 --> 00:38:49,752
don't know clear, clear path

00:38:49,752 --> 00:38:52,790
of using the hardware accelerator,

00:38:52,790 --> 00:38:55,750
you know, definitely bonding to the real hardware

00:38:55,750 --> 00:38:58,060
and the same time we have the looks

00:38:58,060 --> 00:39:01,383
just of multiple hardware cards, hardware resources.

00:39:03,490 --> 00:39:06,780
- I think you just beat me to my comment actually.

00:39:06,780 --> 00:39:09,090
Yeah, the first thing I was going to say was that,

00:39:09,090 --> 00:39:11,410
particularly in an active backup configuration,

00:39:11,410 --> 00:39:13,930
one thing most end-users like to do with bonding

00:39:13,930 --> 00:39:15,070
is provide redundancy.

00:39:15,070 --> 00:39:16,820
Not only towards network path failures,

00:39:16,820 --> 00:39:18,300
but towards hardware failures.

00:39:18,300 --> 00:39:21,083
And this seems to necessitate the use of a single card.

00:39:22,250 --> 00:39:26,350
Regarding bonding for VMs, we actually do have a mechanism

00:39:26,350 --> 00:39:28,030
in the kernel to do that,

00:39:28,030 --> 00:39:30,130
that Festoban and I did a few years ago.

00:39:30,130 --> 00:39:32,860
Ethernet stations, whereby you can allocate

00:39:32,860 --> 00:39:34,951
a certain subset of queues from a card

00:39:34,951 --> 00:39:36,950
and create and ethernet interface with those,

00:39:36,950 --> 00:39:38,410
that you can then pass into a VM

00:39:38,410 --> 00:39:40,450
and then bond there directly.

00:39:40,450 --> 00:39:41,570
So that you can aggregate bonds

00:39:41,570 --> 00:39:43,780
across multiple VMs in that way,

00:39:43,780 --> 00:39:47,113
which is perhaps not as elegant as this, but functional.

00:39:48,460 --> 00:39:50,477
- [Man] Did you have multiple MACs, unicast MACs--

00:39:51,761 --> 00:39:53,272
- Yeah. - Okay, I see.

00:39:53,272 --> 00:39:54,914
- [Man] Or you have that option.

00:39:54,914 --> 00:39:55,997
- [Man] Okay.

00:39:58,560 --> 00:39:59,910
- [Woman] Can I ask a quick question

00:39:59,910 --> 00:40:01,220
to the guy as well? - Sure, no problem.

00:40:01,220 --> 00:40:05,803
- Can you do LACP bonding over virtual functions then?

00:40:07,070 --> 00:40:09,393
- So it's not virtual func, sorry.

00:40:10,780 --> 00:40:12,430
I'll be quick, I promise.

00:40:12,430 --> 00:40:14,110
It's not virtual functions specifically,

00:40:14,110 --> 00:40:18,283
it's queue allocation towards ethernet devices.

00:40:19,200 --> 00:40:20,639
But certainly, you can create

00:40:20,639 --> 00:40:24,130
those virtual ethernet devices with queue allocations

00:40:24,130 --> 00:40:27,280
and bond them together with bonding driver

00:40:27,280 --> 00:40:29,233
that then uses LACP on top of that.

00:40:34,470 --> 00:40:36,660
- So along the same lines for LACP.

00:40:36,660 --> 00:40:37,493
So in this model,

00:40:37,493 --> 00:40:41,040
it seems like you're at the coarseness of the port,

00:40:41,040 --> 00:40:43,713
so you're just moving the MACs around and the resources.

00:40:43,713 --> 00:40:46,890
Do you see any possibility to extend this

00:40:46,890 --> 00:40:50,210
to support actual LACP of the RDMA traffic

00:40:50,210 --> 00:40:51,970
with simple redirection of the NIC

00:40:51,970 --> 00:40:53,470
to the right virtual function?

00:40:54,740 --> 00:40:56,480
For example, the ingress traffic coming

00:40:56,480 --> 00:40:58,630
from the switch in a LACP configuration

00:40:58,630 --> 00:41:01,140
is gonna be distributing it to both ports.

00:41:01,140 --> 00:41:02,983
Well, if the traffic is intended for one VF

00:41:02,983 --> 00:41:05,530
and the VF is mapped one PF,

00:41:05,530 --> 00:41:07,500
well, he's only gonna get half of the traffic.

00:41:07,500 --> 00:41:10,410
The rest goes to the other state machine, right.

00:41:10,410 --> 00:41:13,400
Well, if it's two ports on the same card,

00:41:13,400 --> 00:41:15,160
and they share the same hardware resources,

00:41:15,160 --> 00:41:17,610
they could essentially share the same state machine.

00:41:17,610 --> 00:41:20,750
In which case the ingress traffic coming to one port,

00:41:20,750 --> 00:41:22,970
port zero, that actually needs to be redirected

00:41:22,970 --> 00:41:25,110
to a VF that resides on port one,

00:41:25,110 --> 00:41:28,650
could actually be done in the hardware somehow.

00:41:28,650 --> 00:41:32,010
Do you see a possibility of doing that with this model,

00:41:32,010 --> 00:41:33,650
or extending this model to do that?

00:41:33,650 --> 00:41:37,880
- So I can see some ways, it's definitely depend

00:41:37,880 --> 00:41:40,000
on the details of the hardware,

00:41:40,000 --> 00:41:42,180
so we have to consider it.

00:41:42,180 --> 00:41:45,840
I'm not sure whether it's doable, you know,

00:41:45,840 --> 00:41:48,695
without touching the existing the hardware.

00:41:48,695 --> 00:41:51,177
One of the reason was just to limit changes

00:41:52,387 --> 00:41:56,085
on the underlying hardware firmware

00:41:56,085 --> 00:41:59,380
to the minimum, but yeah, you are right, if you extend it,

00:41:59,380 --> 00:42:01,010
it might be able to--

00:42:01,010 --> 00:42:06,010
- So MLX-5 driver does this, both for RDMA and SR-IOV VFs,

00:42:06,600 --> 00:42:08,208
so it doesn't matter where the queue was open,

00:42:08,208 --> 00:42:10,310
it can be steered.

00:42:10,310 --> 00:42:11,650
Or in the receive, as you said,

00:42:11,650 --> 00:42:15,120
both ports will redirect the packet to the queue

00:42:15,120 --> 00:42:19,253
if it's a VF or RDMA and transmission can be controlled.

00:42:20,627 --> 00:42:25,180
We don't have full entropy, we have some binding there,

00:42:25,180 --> 00:42:28,690
but basically it's supported on MLX-5, ConnectX-5.

00:42:28,690 --> 00:42:31,810
- Is it LACP support? - Yeah, yeah, LACP support.

00:42:31,810 --> 00:42:34,890
- For RDMA traffic? - RDMA and SR-IOV,

00:42:34,890 --> 00:42:36,343
yeah, but single card.

00:42:37,867 --> 00:42:39,083
- [Host] So it seems like this is

00:42:39,083 --> 00:42:42,035
a really generic problem space.

00:42:42,035 --> 00:42:46,010
- It's a driver, it's similar to what he presented,

00:42:46,010 --> 00:42:46,870
but it's a driver.

00:42:46,870 --> 00:42:49,280
So my second question was,

00:42:49,280 --> 00:42:52,530
why is this NETDEV, isn't this RDMA?

00:42:52,530 --> 00:42:55,210
Maybe the VF uses NETDEV, RDMA stack,

00:42:55,210 --> 00:42:59,670
but it's pretty much a driver solution,

00:42:59,670 --> 00:43:02,333
hooking on the bond callbacks.

00:43:03,200 --> 00:43:05,550
- So if you notice, a lot of our discussion

00:43:05,550 --> 00:43:08,350
is what do we represent this thing as

00:43:08,350 --> 00:43:10,170
and how do we hook them up together,

00:43:10,170 --> 00:43:11,750
that's everything, that's the model.

00:43:11,750 --> 00:43:15,570
That's what makes everything work and yeah.

00:43:15,570 --> 00:43:18,780
- Yeah, very interesting concept for here,

00:43:18,780 --> 00:43:22,433
is what do we do with the ECMP?

00:43:23,380 --> 00:43:24,670
There it's...

00:43:26,430 --> 00:43:28,490
- [Host] What do you iterate over and how do you do it?

00:43:28,490 --> 00:43:30,140
- I don't know, I don't know yet.

00:43:33,936 --> 00:43:36,450
- [Host] Okay, thank you very much.

00:43:36,450 --> 00:43:38,050

YouTube URL: https://www.youtube.com/watch?v=slbd2sVqgRA


