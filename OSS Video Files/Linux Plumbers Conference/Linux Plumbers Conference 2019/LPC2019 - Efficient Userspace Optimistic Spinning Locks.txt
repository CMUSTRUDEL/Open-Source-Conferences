Title: LPC2019 - Efficient Userspace Optimistic Spinning Locks
Publication date: 2019-09-20
Playlist: Linux Plumbers Conference 2019
Description: 
	Efficient Userspace Optimistic Spinning Locks

Speaker
Mr Waiman Long (Red Hat)

Description
The most commonly used simple locking functions provided by the pthread library are pthread_mutex and pthread_rwlock. They are sleeping locks and so do suffer from unpredictable wakeup latency limiting locking throughput.

Userspace spinning locks can potentially offer better locking throughput, but they also suffer other drawbacks like lock holder preemption which will waste valuable CPU time for those lock spinning CPUs. Another spinning lock problem is contention on the lock cacheline when a large number of CPUs are spinning on it.

This talk presents a hybrid spinning/sleeping lock where a lock waiter can choose to spin in userspace or in the kernel waiting for the lock holder to release the lock. While spinning in the kernel, the lock waiters will queue up so that only the one at the queue head will be spinning on the lock reducing lock cacheline contention. If the lock holder is not running, the kernel lock waiters will go to sleep too so as not to waste valuable CPU cycles. The state of kernel lock spinners will be reflected in the value of lock. Thus userspace spinners can
monitor the lock state and determine the best way forward.

This new type of hybrid spinning/sleeping locks combine the best attributes of sleeping and spinning locks. It is especially useful for applications that need to run on large NUMA systems where potentially a large number of CPUs may be pounding on a given lock.
Captions: 
	00:00:00,320 --> 00:00:02,920
- Hi, good morning lady and gentlemen.

00:00:02,920 --> 00:00:05,460
My name is Waiman Long.

00:00:05,460 --> 00:00:07,940
People also call me Longman

00:00:07,940 --> 00:00:11,410
because I'm not tall enough.

00:00:11,410 --> 00:00:14,853
So, I like to feel tall, you know.

00:00:16,440 --> 00:00:19,530
And today's topic is about

00:00:20,900 --> 00:00:24,160
a patch set that I'd done a few years ago,

00:00:24,160 --> 00:00:29,160
but at that time I don't have the time to follow up.

00:00:30,822 --> 00:00:35,822
And now, I want to continue to pursue this patch set

00:00:36,720 --> 00:00:40,360
and so I'm going to give an overview of what it is

00:00:40,360 --> 00:00:42,553
and why I am working on this.

00:00:44,820 --> 00:00:47,833
So locking in user-space.

00:00:48,770 --> 00:00:52,130
In user-space a lot of application use

00:00:52,130 --> 00:00:57,130
the pthread API provided glibc to do locking.

00:00:58,580 --> 00:01:02,240
Like the pthread mutex lock, read/write lock,

00:01:02,240 --> 00:01:07,240
or condition variable and they also have spin lock,

00:01:08,090 --> 00:01:09,813
that actually spin in user-space.

00:01:11,390 --> 00:01:13,490
The top three are the most frequently used

00:01:14,331 --> 00:01:15,893
by the application.

00:01:16,730 --> 00:01:20,690
Spin lock is also used by some application

00:01:22,030 --> 00:01:24,830
but normally isn't used as much.

00:01:24,830 --> 00:01:26,683
The problem with that is because,

00:01:27,882 --> 00:01:30,790
there are a number problem if you want to try

00:01:30,790 --> 00:01:33,530
to spin a lock in user-space.

00:01:33,530 --> 00:01:35,860
So the first three are sleeping lock.

00:01:35,860 --> 00:01:38,280
So when you cannot acquire the lock,

00:01:38,280 --> 00:01:42,620
you have to go to the kernel to sleep

00:01:42,620 --> 00:01:47,150
and wait for the kernel holder to release the lock

00:01:47,150 --> 00:01:49,813
so that you can acquire it again.

00:01:50,900 --> 00:01:54,910
And for a number of larger enterprise application,

00:01:54,910 --> 00:01:58,360
like Oracle, SAP, HANA, et cetera,

00:01:58,360 --> 00:02:01,450
they implement their own locking code

00:02:02,610 --> 00:02:05,270
because they want to squeeze out

00:02:05,270 --> 00:02:10,270
as much of the performance from the Linux kernel.

00:02:12,300 --> 00:02:16,270
The glibc code is fine, but the fact that

00:02:16,270 --> 00:02:19,400
you have to conform extra data to the POSIX standard

00:02:19,400 --> 00:02:21,173
that adds some overhead.

00:02:23,316 --> 00:02:26,100
So you have to make sure that it really conform

00:02:26,100 --> 00:02:27,803
to all the different conditions

00:02:27,803 --> 00:02:29,553
that are specified in the standard.

00:02:30,480 --> 00:02:32,520
If you implement your own locking code,

00:02:32,520 --> 00:02:35,220
you can skip some of those that are not relevant to you.

00:02:35,220 --> 00:02:38,970
So as a result, it will be generally a bit faster

00:02:38,970 --> 00:02:41,397
than just using the glibc API.

00:02:42,927 --> 00:02:44,940
But then, of course, you need to maintain

00:02:44,940 --> 00:02:46,280
your own locking code.

00:02:46,280 --> 00:02:48,520
Which is not, it's not a trivial task

00:02:48,520 --> 00:02:52,403
because locking code can be tricky sometime,

00:02:52,403 --> 00:02:56,120
especially if you want to squeeze out

00:02:56,120 --> 00:02:57,720
as much performance as possible.

00:03:00,044 --> 00:03:05,044
And all these locking API, at least for the glibc,

00:03:05,090 --> 00:03:08,706
they use the futex API provided by the kernel.

00:03:08,706 --> 00:03:11,843
Futex is fast user-space mutex.

00:03:15,990 --> 00:03:19,723
And this API used to implement the sleeping lock.

00:03:21,250 --> 00:03:23,830
Some older application,

00:03:23,830 --> 00:03:28,790
they may also use the system five semaphore API

00:03:28,790 --> 00:03:30,910
to implement the locking,

00:03:30,910 --> 00:03:35,523
but it's generally slower than using the futex API.

00:03:40,840 --> 00:03:45,840
So, what is the advantage or disadvantage

00:03:45,919 --> 00:03:50,919
of doing locking using the spinning lock

00:03:51,020 --> 00:03:52,533
or the sleeping lock?

00:03:54,970 --> 00:03:56,910
In user-space, we generally advised people

00:03:56,910 --> 00:03:59,950
to use the sleeping lock because

00:04:01,240 --> 00:04:03,310
unless the lock critical section

00:04:03,310 --> 00:04:06,036
is really small and you are kind of sure that

00:04:06,036 --> 00:04:10,890
that not many lock waiter,

00:04:10,890 --> 00:04:12,350
not many thread are going to try

00:04:12,350 --> 00:04:14,360
to acquire the lock at the same time,

00:04:14,360 --> 00:04:15,450
then you can sue a spin lock.

00:04:15,450 --> 00:04:19,903
It's generally faster than using a sleeping lock like mutex.

00:04:21,170 --> 00:04:23,391
But then, you never know.

00:04:23,391 --> 00:04:28,230
One of the reason why I want to pursue this project

00:04:28,230 --> 00:04:31,240
is because we are helping

00:04:33,485 --> 00:04:35,960
to run some certification tests

00:04:35,960 --> 00:04:39,250
for one of our ISV partner.

00:04:39,250 --> 00:04:41,997
And one of the test they have

00:04:42,954 --> 00:04:46,020
is that they have a spin lock in user-space.

00:04:46,020 --> 00:04:49,310
And we have to run hundreds of thread

00:04:49,310 --> 00:04:51,590
trying to acquire spin lock,

00:04:51,590 --> 00:04:54,130
that cause a lot problem.

00:04:54,130 --> 00:04:58,305
And the time spend on the locking code itself

00:04:58,305 --> 00:05:03,305
can go up to 80 or 90% of CPU time in a spin lock.

00:05:04,780 --> 00:05:09,780
So that also means that in some extreme cases

00:05:10,220 --> 00:05:13,830
it is possible that there is a user-space spin lock,

00:05:13,830 --> 00:05:16,561
but then, it get heavily contended

00:05:16,561 --> 00:05:20,309
and it limited application performance

00:05:20,309 --> 00:05:22,110
if that really happen.

00:05:22,110 --> 00:05:26,520
So I try to, what I'm talking about here,

00:05:26,520 --> 00:05:29,694
is to provide an alternative

00:05:29,694 --> 00:05:32,640
between a sleeping and spinning lock,

00:05:32,640 --> 00:05:37,150
that kind of, hopefully, have the best of both world.

00:05:38,890 --> 00:05:41,920
But anyway, for a spinning lock,

00:05:41,920 --> 00:05:46,330
the problem with that is that you can't really have

00:05:46,330 --> 00:05:49,540
a fair spinning lock in user-space

00:05:49,540 --> 00:05:53,070
because of the fact that a user thread

00:05:53,070 --> 00:05:56,130
can get preempted at anytime.

00:05:56,130 --> 00:05:59,160
So you are spinning a lock, you will fairly often be,

00:05:59,160 --> 00:06:01,520
you have to follow a particular order.

00:06:01,520 --> 00:06:04,980
If one of the lock waiter get preempted,

00:06:04,980 --> 00:06:08,580
then any other lock waiter behind it can't move forward

00:06:08,580 --> 00:06:10,690
and that can be a problem.

00:06:10,690 --> 00:06:15,440
So most user-space spin lock are unfair lock.

00:06:15,440 --> 00:06:17,820
And with unfair lock, the problem is that

00:06:19,100 --> 00:06:20,730
you have a lot of cacheline contention,

00:06:20,730 --> 00:06:23,130
because all the lock are spinning

00:06:23,130 --> 00:06:25,000
on the same lock cacheline.

00:06:25,000 --> 00:06:28,370
So you have cacheline passing a one all over the places,

00:06:28,370 --> 00:06:32,393
that causes performance problem for the application.

00:06:33,680 --> 00:06:38,680
And also, unfair lock also has the problem of being unfair,

00:06:38,720 --> 00:06:41,017
that you can have lock starvation,

00:06:41,017 --> 00:06:44,070
which sometime can happen

00:06:44,070 --> 00:06:46,843
depending especially on the new model system.

00:06:49,088 --> 00:06:52,430
With sleeping lock, the main problem you have

00:06:52,430 --> 00:06:54,363
is the waiter wake up latency.

00:06:56,550 --> 00:07:01,550
Oh so, oh yeah, sometime can take 10 to even 100 microsecond

00:07:05,050 --> 00:07:09,120
before the waiter can really wake up and start running.

00:07:09,120 --> 00:07:12,730
So and, sleeping lock I did,

00:07:12,730 --> 00:07:15,113
also have the problem of lock starvation.

00:07:16,800 --> 00:07:18,400
In the Linux kernel itself,

00:07:18,400 --> 00:07:22,920
we basically solved the lock cacheline contention problem

00:07:22,920 --> 00:07:27,040
by using a queuing lock, a queue spin lock.

00:07:27,040 --> 00:07:29,675
And also the read/write lock, itself,

00:07:29,675 --> 00:07:32,610
is also a queuing lock.

00:07:32,610 --> 00:07:37,110
So we don't need to, not all the waiter need to spin

00:07:37,110 --> 00:07:39,610
on the same lock cacheline at the same time.

00:07:39,610 --> 00:07:43,403
So it's kind of eliminate the lock contention problem.

00:07:46,618 --> 00:07:48,690
And although I said that

00:07:50,530 --> 00:07:54,253
we don't advise to use unfair lock in user-space,

00:07:55,260 --> 00:07:56,570
there are people,

00:07:56,570 --> 00:08:01,570
I saw a glibc patch, I think maybe years ago,

00:08:02,140 --> 00:08:07,140
that tried to implement an unfair lock in the glibc.

00:08:07,610 --> 00:08:10,233
I don't know the current status, but,

00:08:13,280 --> 00:08:15,630
again, I don't believe it is,

00:08:15,630 --> 00:08:18,740
if we really start to get contended

00:08:21,300 --> 00:08:22,640
then what I talked about,

00:08:22,640 --> 00:08:27,610
about being the lock waiter preemption problem,

00:08:27,610 --> 00:08:30,653
can cause problems with those kind of unfair lock.

00:08:34,090 --> 00:08:34,923
Okay.

00:08:36,090 --> 00:08:39,700
And the problem with the wake-up latency

00:08:39,700 --> 00:08:42,797
is that it limit the maximum locking rate

00:08:42,797 --> 00:08:44,933
you can achieve with a sleeping lock.

00:08:47,185 --> 00:08:50,538
So in order to acquire a lock,

00:08:50,538 --> 00:08:54,690
you have to wake up the lock waiter

00:08:54,690 --> 00:08:57,880
and the lock waiter have to come from the kernel

00:08:57,880 --> 00:09:01,160
and then go up to the user-space to try to acquire the lock.

00:09:01,160 --> 00:09:05,330
Or you can also have some kind of lock stealing going on.

00:09:05,330 --> 00:09:08,480
And in that case, another thread

00:09:08,480 --> 00:09:10,170
can come in and steal a lock,

00:09:10,170 --> 00:09:13,030
but then when the lock waiter wake up from the kernel,

00:09:13,030 --> 00:09:15,270
you find the lock is not available anymore

00:09:15,270 --> 00:09:18,073
and then you have to go back to the kernel to sleep.

00:09:26,960 --> 00:09:30,280
And also, for application that need to scale up

00:09:30,280 --> 00:09:33,200
to a large number of thread on the system,

00:09:33,200 --> 00:09:34,880
large number of CPU,

00:09:36,563 --> 00:09:39,090
there is a dilemma going on whether you should use

00:09:39,090 --> 00:09:41,660
a spinning lock or the sleeping lock.

00:09:41,660 --> 00:09:43,460
So on the one hand, a spinning lock,

00:09:44,889 --> 00:09:47,660
if not many thread are contending at the same time,

00:09:47,660 --> 00:09:49,610
it'll probably be faster.

00:09:49,610 --> 00:09:53,681
But if the number of thread contending on lock increases,

00:09:53,681 --> 00:09:57,180
then you have also often cacheline contention

00:09:57,180 --> 00:10:02,053
and lock waiter or lock holder preemption problem going on.

00:10:03,270 --> 00:10:04,630
But then with a sleeping lock,

00:10:04,630 --> 00:10:08,705
you kind of limit the way how many locking operation

00:10:08,705 --> 00:10:11,043
you can done per second.

00:10:13,360 --> 00:10:17,060
And so, you have to make a trade off

00:10:17,060 --> 00:10:19,330
of which one is the more likely scenario

00:10:19,330 --> 00:10:24,107
and decide whether you want a sleeping lock or a spin lock.

00:10:30,280 --> 00:10:34,530
So, this talk is about I am implementing

00:10:34,530 --> 00:10:37,130
a kind of spinning lock in user-space

00:10:37,130 --> 00:10:41,433
by the assistance from the kernel.

00:10:44,710 --> 00:10:49,710
So I sent out a patch about three years ago,

00:10:49,760 --> 00:10:52,410
at that time I called the throughput-optimized futex.

00:10:53,920 --> 00:10:58,920
Now I changed the name to optimistic spinning futex,

00:10:59,210 --> 00:11:01,910
because it's what is actually happening in the kernel.

00:11:03,050 --> 00:11:08,050
So before that, I want to talk about the futex in general.

00:11:09,000 --> 00:11:12,760
There are two main kind of futexes in the kernel.

00:11:12,760 --> 00:11:15,819
One I call the wait-wake futex.

00:11:15,819 --> 00:11:20,247
So basically, when you call the futex system call

00:11:20,247 --> 00:11:25,247
with the futex wait command, your futex is going to sleep.

00:11:27,420 --> 00:11:31,520
And then, when the lock holder,

00:11:31,520 --> 00:11:33,669
at the time when the lock produce the lock,

00:11:33,669 --> 00:11:37,090
it will see that there is a waiter in the kernel.

00:11:37,090 --> 00:11:40,460
And then you have to call the futex system call

00:11:42,940 --> 00:11:44,650
with the futex wake command

00:11:44,650 --> 00:11:48,140
to wake up the sleeping lock waiter.

00:11:48,140 --> 00:11:51,897
So that's why I call it the wait-wake futex.

00:11:51,897 --> 00:11:54,600
And there is another type of futex in kernel

00:11:56,512 --> 00:11:59,400
called the priority inversion futex.

00:11:59,400 --> 00:12:03,750
Those futex are used for mainly by the real time application

00:12:03,750 --> 00:12:06,520
to make sure that the higher priority tasks

00:12:06,520 --> 00:12:11,520
always get priority to execute first and get the lock first.

00:12:12,980 --> 00:12:14,570
But the problem with that futex is

00:12:14,570 --> 00:12:16,883
that the performance can be horrible,

00:12:17,920 --> 00:12:22,550
compared with the regular wait-wake futex.

00:12:22,550 --> 00:12:25,780
So it is used in certain case

00:12:25,780 --> 00:12:29,320
where you really need the real time

00:12:29,320 --> 00:12:33,213
to limit the maximum latency your application can have.

00:12:36,470 --> 00:12:38,000
And both type of futex require

00:12:38,000 --> 00:12:39,640
the lock waiter to sleep in there kernel

00:12:39,640 --> 00:12:43,730
until the lock holder release the lock

00:12:43,730 --> 00:12:46,713
and call the kernel to wake up the lock waiter.

00:12:48,750 --> 00:12:51,720
Optimistic spinning futex is a new kind of futex

00:12:51,720 --> 00:12:54,950
that I introduce in my patch set,

00:12:54,950 --> 00:12:59,950
which is kind of a hybrid spinning and sleeping lock.

00:13:00,182 --> 00:13:02,200
The basic idea is that,

00:13:02,200 --> 00:13:05,210
if the lock holder is actually running,

00:13:05,210 --> 00:13:07,920
that means that it make forward progress

00:13:07,920 --> 00:13:09,950
and eventually, it will release a lock

00:13:11,330 --> 00:13:13,790
in the foreseeable future.

00:13:13,790 --> 00:13:16,158
So in that case, it may be more efficient

00:13:16,158 --> 00:13:21,158
to spin on it and wait until the lock waiter release before,

00:13:23,142 --> 00:13:25,783
instead of just sleeping and let the

00:13:26,990 --> 00:13:30,460
lock holder wake you up.

00:13:30,460 --> 00:13:31,833
So in that case,

00:13:34,400 --> 00:13:37,770
all the lock waiter will spin in the kernel,

00:13:37,770 --> 00:13:40,700
but they will, inside the kernel,

00:13:40,700 --> 00:13:42,950
they use the mutex as a queue.

00:13:42,950 --> 00:13:44,680
So they line up in a queue.

00:13:44,680 --> 00:13:47,711
Only the first one in the queue,

00:13:47,711 --> 00:13:52,240
which is the mutex lock holder will spin on the futex.

00:13:52,240 --> 00:13:55,379
So at any one time, we only want one,

00:13:55,379 --> 00:14:00,379
or maybe sometimes two task to spin on the lock itself.

00:14:00,870 --> 00:14:02,520
We don't want more than that, otherwise,

00:14:02,520 --> 00:14:06,703
you have all sort of cacheline contention problem.

00:14:10,538 --> 00:14:13,750
In the rare case that lock holder are actually sleeping,

00:14:13,750 --> 00:14:18,750
maybe it got preempted or it's waiting on an IO operation,

00:14:19,800 --> 00:14:22,420
then in that case, there is no point

00:14:22,420 --> 00:14:25,240
to continue spinning in the kernel,

00:14:25,240 --> 00:14:28,040
because you don't know when it will get woken up.

00:14:28,040 --> 00:14:31,630
So in that case, all the lock waiter in the kernel

00:14:31,630 --> 00:14:34,543
will go to sleep, just like the lock holder.

00:14:36,010 --> 00:14:37,700
The reason why we need to do it in the kernel

00:14:37,700 --> 00:14:39,150
is the fact that, in the kernel,

00:14:39,150 --> 00:14:42,040
we can check the task structure to see

00:14:42,040 --> 00:14:44,170
if the task is actually running or not.

00:14:44,170 --> 00:14:47,180
But in user-space, there is currently no facility

00:14:47,180 --> 00:14:49,850
for you to know if a task is actually running

00:14:49,850 --> 00:14:51,903
or if that is sleeping.

00:14:53,050 --> 00:14:58,050
And actually, I talked to one of the lead glibc developer

00:14:58,920 --> 00:15:01,053
and he said that actually,

00:15:01,916 --> 00:15:04,450
he will like to have some kind of interface

00:15:04,450 --> 00:15:09,030
so that he can know whether a task is sleeping or running.

00:15:10,020 --> 00:15:13,890
So that he can implement some kind of spinning,

00:15:13,890 --> 00:15:15,840
optimistic spinning, in the user-space.

00:15:20,490 --> 00:15:25,350
The OS futex that I will introduce in this patch set

00:15:25,350 --> 00:15:26,820
can support the implementation

00:15:26,820 --> 00:15:28,990
of both mutex and read/write lock.

00:15:28,990 --> 00:15:33,990
but currently, it cannot support the condition variable.

00:15:35,540 --> 00:15:40,467
But it an support the mutex part of the condition variable.

00:15:41,800 --> 00:15:44,063
- Excuse me, single question.

00:15:46,380 --> 00:15:49,720
Yeah, I just wanted to ask because you told

00:15:49,720 --> 00:15:53,950
that if the thread is waiting for IO,

00:15:53,950 --> 00:15:56,470
then the spinning is not being taken

00:15:56,470 --> 00:15:59,010
in this optimistic spinning futex,

00:15:59,010 --> 00:16:01,630
because we don't know when the IO is going to be finished.

00:16:01,630 --> 00:16:03,980
- Yes. - This sounds rather like

00:16:03,980 --> 00:16:06,090
a pessimistic rather than optimistic approach

00:16:06,090 --> 00:16:11,090
because in this new, if you consider the new devices

00:16:11,870 --> 00:16:13,870
which we have like Optane, et cetera,

00:16:13,870 --> 00:16:15,020
the IO is quite fast.

00:16:15,020 --> 00:16:19,142
And in optimistic approach you would assume it is quite

00:16:19,142 --> 00:16:19,975
soon finished. - No, no, no,

00:16:19,975 --> 00:16:22,370
if you are talking about a new type

00:16:22,370 --> 00:16:24,807
of source device like peristent memory,

00:16:24,807 --> 00:16:28,320
the CPU, the task is actually running.

00:16:28,320 --> 00:16:30,387
It's calling to a function.

00:16:30,387 --> 00:16:33,230
The function will look up, will search,

00:16:33,230 --> 00:16:37,060
the persistent memory device

00:16:37,060 --> 00:16:39,370
to find the right page and copy out.

00:16:39,370 --> 00:16:43,343
So that is done within the task context.

00:16:44,540 --> 00:16:46,263
So the task is not sleeping.

00:16:47,844 --> 00:16:50,490
What I mean by waiting for IO is like source device.

00:16:50,490 --> 00:16:55,380
You have to actually call and do a device transfer

00:16:55,380 --> 00:16:59,470
and you had to issue some command to the source device

00:16:59,470 --> 00:17:01,830
and then the task have to actually sleep,

00:17:01,830 --> 00:17:05,270
waiting for interrupt to happen before it can get the data.

00:17:05,270 --> 00:17:07,810
Then in that case, the task go to sleep.

00:17:07,810 --> 00:17:10,373
But if the task is trying

00:17:10,373 --> 00:17:13,733
to copy data from persistent device, it's not sleeping.

00:17:15,630 --> 00:17:19,840
- Okay, so every time the task holding the mutex

00:17:19,840 --> 00:17:22,760
will go to sleep, anyone who is waiting for this

00:17:22,760 --> 00:17:23,930
will also go to sleep--

00:17:23,930 --> 00:17:25,380
- Yes, yes. - Avoid the spinning.

00:17:25,380 --> 00:17:28,264
And there would be no way to control this, so?

00:17:28,264 --> 00:17:31,350
- That is how the optimistic spinning

00:17:31,350 --> 00:17:32,467
work in the kernel right now.

00:17:32,467 --> 00:17:37,460
So if you are in kernel module,

00:17:37,460 --> 00:17:41,140
you try to lock a mutex,

00:17:41,140 --> 00:17:43,000
it would do exactly the same thing.

00:17:43,000 --> 00:17:45,200
If the task is running, it will spin on it.

00:17:45,200 --> 00:17:48,970
If the task go to sleep, it will go to sleep also.

00:17:48,970 --> 00:17:51,808
So we just follow what the current convention

00:17:51,808 --> 00:17:53,260
are in the kernel.

00:17:53,260 --> 00:17:54,460
- [Man] Okay, thank you.

00:17:57,200 --> 00:17:59,480
- And a lot of block devices that are really fast

00:17:59,480 --> 00:18:01,840
have a pulling mode where the task doesn't sleep.

00:18:01,840 --> 00:18:04,290
So that would probably address your concerns too.

00:18:06,170 --> 00:18:10,793
- So when you ask questions, request from the video,

00:18:11,660 --> 00:18:14,543
to stand up and speak, please.

00:18:22,500 --> 00:18:23,333
- Okay.

00:18:25,270 --> 00:18:28,523
Why we want to have this kind of new futex?

00:18:29,697 --> 00:18:31,290
Because in terms of performance,

00:18:31,290 --> 00:18:32,560
there are two main problems

00:18:32,560 --> 00:18:36,833
with the use of the wait-wake futex in user-space lock.

00:18:38,833 --> 00:18:41,080
The first problem is that,

00:18:41,080 --> 00:18:43,060
what I actually talked about before,

00:18:43,060 --> 00:18:45,530
that the lock waiter had to go to sleep

00:18:45,530 --> 00:18:49,760
and wait for the lock holder

00:18:49,760 --> 00:18:51,900
to call the kernel to wake it up.

00:18:51,900 --> 00:18:56,523
So it take time and there's some latency involved.

00:18:57,610 --> 00:19:02,610
And most locking code will allow some kind of lock stealing

00:19:03,030 --> 00:19:05,830
so that a lock in user-space,

00:19:05,830 --> 00:19:07,110
when you try to acquire the lock

00:19:07,110 --> 00:19:11,940
and see that the lock is free,

00:19:11,940 --> 00:19:13,360
you will try to acquire the lock.

00:19:13,360 --> 00:19:16,900
But if there are waiter waiting on the lock,

00:19:16,900 --> 00:19:19,013
you would sort of spin in the futex,

00:19:20,187 --> 00:19:23,262
well, in that case, it had to go into the kernel also.

00:19:23,262 --> 00:19:26,980
And with the optimistic spinning futex,

00:19:26,980 --> 00:19:29,790
one major difference, is that it won't set the bit

00:19:29,790 --> 00:19:30,850
saying that it is sleeping,

00:19:30,850 --> 00:19:34,540
until there is actually a task that go into sleep.

00:19:34,540 --> 00:19:38,890
In most case, the lock waiter are spinning in the kernel,

00:19:38,890 --> 00:19:40,090
so it won't set the bit,

00:19:40,090 --> 00:19:43,720
so that will allow lock spinning to happen

00:19:43,720 --> 00:19:45,810
without going into the kernel.

00:19:45,810 --> 00:19:47,920
I will show you some performance data later,

00:19:47,920 --> 00:19:50,420
show you the difference between using

00:19:50,420 --> 00:19:55,420
the typical wait-wake futex and the OS futex.

00:19:55,490 --> 00:19:57,830
And with a heavily contended mutex,

00:19:57,830 --> 00:19:59,640
you will find out that

00:20:01,167 --> 00:20:05,560
the locking performance are actually constrained

00:20:08,373 --> 00:20:11,020
by spin lock hash bucket,

00:20:11,020 --> 00:20:13,360
the futex hash bucket spin lock in the kernel.

00:20:13,360 --> 00:20:16,120
And in some extreme case I saw

00:20:17,670 --> 00:20:22,670
maybe 50% of time spent to trying acquire,

00:20:22,780 --> 00:20:26,550
or 50% more of the time spent

00:20:26,550 --> 00:20:31,203
trying to acquire the hash bucket spin lock.

00:20:34,040 --> 00:20:36,528
With the OS futex,

00:20:36,528 --> 00:20:39,100
the lock waiter in the kernel are not sleeping,

00:20:39,100 --> 00:20:40,760
so you won't sacrifice saying

00:20:40,760 --> 00:20:43,110
that you have go to kernel to acquire the lock.

00:20:44,470 --> 00:20:45,303
And

00:20:46,553 --> 00:20:50,920
you also don't need to go into kernel to wake up

00:20:50,920 --> 00:20:54,500
the lock waiter that are sleeping.

00:20:54,500 --> 00:20:57,950
And that will deal with lot of overhead,

00:20:57,950 --> 00:20:59,900
of using the OS futex

00:21:01,860 --> 00:21:03,980
and that also allow the task to continue.

00:21:03,980 --> 00:21:06,990
Once you release a lock you can move forward

00:21:06,990 --> 00:21:09,110
without worrying about going in your kernel

00:21:09,110 --> 00:21:10,773
and wake other people's up.

00:21:16,310 --> 00:21:20,160
And the design of the OS futex is that

00:21:23,195 --> 00:21:26,140
in order to improve the locking throughput,

00:21:26,140 --> 00:21:30,380
there are few thing we dot for the OS futex.

00:21:30,380 --> 00:21:33,460
One is, as the name imply, optimistic spinning.

00:21:33,460 --> 00:21:35,900
Which means that if the task is running,

00:21:35,900 --> 00:21:39,263
it will spin in the kernel without going to sleep.

00:21:40,180 --> 00:21:43,460
And another thing is encourage lock stealing

00:21:43,460 --> 00:21:48,460
so you can steal in the user-space.

00:21:48,680 --> 00:21:51,463
Because you don't say, in the futex,

00:21:51,463 --> 00:21:54,660
it doesn't say that there is a waiter in the kernel

00:21:54,660 --> 00:21:57,523
that you need to go to the kernel to wake it up.

00:21:58,630 --> 00:22:01,080
Then, you can do lock stealing in the user-space.

00:22:02,380 --> 00:22:05,313
And we also allow,

00:22:06,221 --> 00:22:08,720
there is an option for the OS futex

00:22:09,620 --> 00:22:12,390
to allow you to do either the kernel locking

00:22:12,390 --> 00:22:15,030
or the OS locking.

00:22:15,030 --> 00:22:16,620
Kernel locking mean, you actually

00:22:16,620 --> 00:22:20,800
acquire the lock in the kernel and then you go back up.

00:22:20,800 --> 00:22:24,470
But the problem with that is you add some additional latency

00:22:25,990 --> 00:22:27,940
to transition the kernel switch,

00:22:27,940 --> 00:22:29,400
oh no, not the kernel switch,

00:22:29,400 --> 00:22:31,790
to transition from a kernel to user-space

00:22:31,790 --> 00:22:34,200
to your overall lock hold full time.

00:22:34,200 --> 00:22:37,555
So there's another option that you,

00:22:37,555 --> 00:22:40,366
once you find out that the lock is free,

00:22:40,366 --> 00:22:42,780
you didn't acquire the lock in the kernel.

00:22:42,780 --> 00:22:44,484
Instead, you go up to user-space

00:22:44,484 --> 00:22:46,673
and try to acquire the lock in user-space.

00:22:50,270 --> 00:22:52,060
The simple code that I implementing

00:22:52,060 --> 00:22:55,900
is to do user-space locking a few time,

00:22:55,900 --> 00:22:58,010
you still cannot acquire the lock,

00:22:58,010 --> 00:23:00,163
then do the locking in the kernel.

00:23:06,640 --> 00:23:11,640
And in order to avoid the problem of lock starvation,

00:23:11,930 --> 00:23:15,860
there is a builtin lock hand-off mechanism

00:23:15,860 --> 00:23:17,450
to force lock hand-off

00:23:17,450 --> 00:23:22,450
to the the first task in the waiting queue.

00:23:23,440 --> 00:23:26,710
So in this way this allow

00:23:28,300 --> 00:23:30,530
lock from the user-space to see the lock.

00:23:30,530 --> 00:23:33,060
And you make sure that it will get it,

00:23:33,060 --> 00:23:38,060
it will be the first waiter in the locking queue.

00:23:38,630 --> 00:23:42,520
And but then, of course, in this case,

00:23:42,520 --> 00:23:43,810
you do the locking in the kernel

00:23:43,810 --> 00:23:46,990
and so there's some additional latency involved.

00:23:46,990 --> 00:23:51,990
So this mechanism only invoked if it wait for a certain,

00:23:52,130 --> 00:23:55,458
you can't apply the lock after a certain period of time.

00:23:55,458 --> 00:23:57,870
Currently, I think I set the threshold

00:23:57,870 --> 00:24:00,130
to be at one, five millisecond or so.

00:24:00,130 --> 00:24:03,030
You can acquire the lock given that time,

00:24:03,030 --> 00:24:07,080
then you will force the lock handoff to happen

00:24:07,080 --> 00:24:08,323
when the lock is free.

00:24:16,220 --> 00:24:19,083
The OS futex is similar to the PI futex

00:24:19,083 --> 00:24:21,540
in semantics, in how it work.

00:24:21,540 --> 00:24:24,323
So in order to acquire lock,

00:24:25,460 --> 00:24:30,460
you have to atomically put your thread ID to the futex word.

00:24:30,750 --> 00:24:33,760
And this is how the lock waiting

00:24:33,760 --> 00:24:36,703
in kernel find out which person you are.

00:24:38,600 --> 00:24:40,723
So once you are done with the lock,

00:24:41,841 --> 00:24:44,400
the thread puller is that they often,

00:24:44,400 --> 00:24:47,530
they can choose to whether do the locking in user-space

00:24:47,530 --> 00:24:48,943
or do locking in the kernel.

00:24:49,870 --> 00:24:52,015
And a lot of time, all you need to do

00:24:52,015 --> 00:24:55,440
is just clear the futex.

00:24:55,440 --> 00:24:59,757
And as long as the lock waiter,

00:24:59,757 --> 00:25:03,500
there is a bit in the futex called futex waiter,

00:25:03,500 --> 00:25:06,040
if it's not set, then that is all you need to do,

00:25:06,040 --> 00:25:06,873
just clear it.

00:25:06,873 --> 00:25:07,706
If it is set, then you have to go

00:25:07,706 --> 00:25:09,413
in the kernel to do the lock.

00:25:10,810 --> 00:25:14,213
And the code is pretty easy for the mutex.

00:25:17,357 --> 00:25:20,720
OS futex also support read/write lock,

00:25:20,720 --> 00:25:23,290
what I call a shared lock.

00:25:23,290 --> 00:25:26,210
So if you want to acquire a writer lock,

00:25:26,210 --> 00:25:28,160
you acquire an exclusive lock.

00:25:28,160 --> 00:25:31,810
And you do a reader lock, you acquire a shared lock.

00:25:31,810 --> 00:25:35,080
So this slide show you what you can do

00:25:35,940 --> 00:25:37,800
to acquire a shared lock.

00:25:37,800 --> 00:25:41,320
A shared lock is indicated by a special bit

00:25:41,320 --> 00:25:42,800
in the futex word indicating

00:25:42,800 --> 00:25:46,383
that it is an shared lock instead of an exclusive lock.

00:25:47,440 --> 00:25:52,440
And there are some option that change the code a bit,

00:25:53,439 --> 00:25:57,513
to allow you to prefer the reader or prefer the writer more.

00:25:58,870 --> 00:26:00,770
And a lot of time, you just have

00:26:00,770 --> 00:26:02,460
to decrement the reader count.

00:26:02,460 --> 00:26:04,650
But if the count reach zero,

00:26:04,650 --> 00:26:08,393
you have to atomically clear the futex shared flag as well.

00:26:11,500 --> 00:26:15,080
If some how there are waiter in the kernel,

00:26:15,080 --> 00:26:18,470
then it get a bit more complicated in how you can do that.

00:26:18,470 --> 00:26:20,203
I won't go into detail here,

00:26:22,300 --> 00:26:25,943
if anyone have interest, we can talk after the presentation.

00:26:28,210 --> 00:26:31,150
Okay, this has some of performance data

00:26:31,150 --> 00:26:36,150
that I gather using user-space micro-benchmark that I wrote,

00:26:37,220 --> 00:26:42,220
that I integrated into the perf bench tools.

00:26:42,330 --> 00:26:47,330
So it's basically just a 2-socket 48-core 96 threads system

00:26:50,500 --> 00:26:52,820
with 96 mutex locking thread.

00:26:52,820 --> 00:26:55,020
And I measure how many locking operation

00:26:55,020 --> 00:26:57,083
you can done per second.

00:26:58,100 --> 00:27:01,640
I use the WW futex, wait-wake futex,

00:27:01,640 --> 00:27:04,890
it's of my implementation of the mutex

00:27:04,890 --> 00:27:07,640
which is relatively straightforward.

00:27:07,640 --> 00:27:12,570
And then, I used the OS futex and I also used the glibc,

00:27:12,570 --> 00:27:15,390
the pthread API in the glibc to do the locking

00:27:15,390 --> 00:27:17,720
and compare the result.

00:27:17,720 --> 00:27:20,820
What you can see from the side is that

00:27:22,142 --> 00:27:27,142
the OS futex is about 20% faster than the WW futex,

00:27:27,340 --> 00:27:32,150
but it depend on how long is your critical section.

00:27:32,150 --> 00:27:37,010
The longer it is, the performance data increases.

00:27:37,010 --> 00:27:40,500
I will show that in the next slide.

00:27:40,500 --> 00:27:43,020
And one thing you can observe is that

00:27:44,460 --> 00:27:47,273
the number of system calls needed to make into a kernel.

00:27:48,160 --> 00:27:50,250
The futex lock system call

00:27:50,250 --> 00:27:54,150
for both WW futex and OS futex, is about the same.

00:27:54,150 --> 00:27:56,520
The wait-wake futex had a little bit more syscall

00:27:56,520 --> 00:27:58,146
to do the locking.

00:27:58,146 --> 00:28:00,320
But in terms of the unlock syscall

00:28:00,320 --> 00:28:02,956
you can see that in the case of wait-wake futex,

00:28:02,956 --> 00:28:05,103
I had to go in the kernel

00:28:05,103 --> 00:28:08,055
about 13 million times to do the lock.

00:28:08,055 --> 00:28:09,596
But with the OS futex,

00:28:09,596 --> 00:28:13,513
it's only 19 time, so that is a big difference.

00:28:14,362 --> 00:28:16,826
And most of the performance benefits,

00:28:16,826 --> 00:28:17,951
I believe is due to the fact

00:28:17,951 --> 00:28:22,868
that I don't need to the kernel to wake up the lock waiter.

00:28:29,323 --> 00:28:31,164
The last row in the table,

00:28:31,164 --> 00:28:36,164
it just means that a number of kernel lock that I need to,

00:28:36,281 --> 00:28:37,444
number of lock that I actually acquire

00:28:37,444 --> 00:28:39,555
in the kernel instead of user-space.

00:28:39,555 --> 00:28:40,780
As I said before,

00:28:40,780 --> 00:28:45,780
my current simple implementation of OS futex

00:28:45,980 --> 00:28:50,830
is that I do four user-space lock attempt

00:28:50,830 --> 00:28:54,970
before I switch to do it in the kernel.

00:28:54,970 --> 00:28:58,160
In the kernel, it can acquire the lock

00:28:58,160 --> 00:28:59,750
after a certain period of time,

00:28:59,750 --> 00:29:01,549
it will force a lock hand-off to make sure

00:29:01,549 --> 00:29:03,913
that lock starvation won't happen.

00:29:07,020 --> 00:29:08,483
Okay, this is another slide

00:29:08,483 --> 00:29:11,366
that show the relative performance

00:29:11,366 --> 00:29:14,350
when I increase the critical section length,

00:29:14,350 --> 00:29:17,440
which in this case is just a number of pause instructions

00:29:17,440 --> 00:29:20,550
that are usually in the lock critical section.

00:29:20,550 --> 00:29:23,432
So you can see that as I increase the length

00:29:23,432 --> 00:29:28,230
of the critical section, the locking rate decreases.

00:29:28,230 --> 00:29:29,063
But

00:29:32,107 --> 00:29:33,800
for the wait-wake futex,

00:29:33,800 --> 00:29:36,503
it decreases faster than the OS futex.

00:29:37,350 --> 00:29:40,010
So the performance benefit actually increases.

00:29:40,010 --> 00:29:43,093
If you have a bigger lock critical section.

00:29:46,540 --> 00:29:47,373
Yeah.

00:29:47,373 --> 00:29:49,139
- [Man] Did you try to run more than 100 thread?

00:29:49,139 --> 00:29:50,641
- [Woman] Excuse me.

00:29:50,641 --> 00:29:55,641
- [Man] Did you try to run more than 100 thread, 500 thread?

00:29:55,650 --> 00:29:56,793
- Oh in that case,

00:29:59,737 --> 00:30:03,760
because this system only have 96 multiple CPU,

00:30:06,520 --> 00:30:10,763
so at any one time, you can only have 96 thread running.

00:30:12,219 --> 00:30:14,030
If you have more thread than that,

00:30:14,030 --> 00:30:16,810
then some of them have to be either get preempted

00:30:16,810 --> 00:30:19,283
or sleeping in he kernel waiting for a lock.

00:30:20,700 --> 00:30:24,203
- What we observed, so until you have a number of threads,

00:30:24,203 --> 00:30:26,990
concurrent threads, equal to number of cores

00:30:26,990 --> 00:30:30,196
within the machine, it can scale.

00:30:30,196 --> 00:30:33,010
Every time you have bigger and on my scale

00:30:33,010 --> 00:30:36,587
we have 1000 users, it will just go down, that's all.

00:30:37,692 --> 00:30:41,777
- Okay, I will, let me see, did I,

00:30:42,710 --> 00:30:43,543
I don't remember.

00:30:43,543 --> 00:30:46,800
I had run some test with number of threads

00:30:46,800 --> 00:30:49,763
bigger than the number of CPU.

00:30:51,060 --> 00:30:53,950
The OS futex still perform better than

00:30:56,020 --> 00:31:00,310
the wait-wake futex, but I do not have the data

00:31:00,310 --> 00:31:02,053
with me right now, so.

00:31:03,020 --> 00:31:04,390
- Especially on read/write locks,

00:31:04,390 --> 00:31:06,850
when you have reader counters,

00:31:06,850 --> 00:31:08,580
you only have read-only workloads

00:31:08,580 --> 00:31:10,930
or just having this counter

00:31:10,930 --> 00:31:15,040
just killed by CPU cachelines authorization.

00:31:15,040 --> 00:31:16,847
And so, everything just going down--

00:31:17,980 --> 00:31:20,200
- Yes, read/write lock is a problem

00:31:20,200 --> 00:31:21,372
because in terms of the performance,

00:31:21,372 --> 00:31:23,930
read/write lock are worst than mutex.

00:31:23,930 --> 00:31:27,790
Again, so that is, the performance I got

00:31:27,790 --> 00:31:32,790
with my own implementation of the wait-wake futex

00:31:32,929 --> 00:31:36,830
and then with the OS futex.

00:31:36,830 --> 00:31:39,720
And in this particular case, I have an equal number

00:31:39,720 --> 00:31:43,410
of reader and writer trying to acquire

00:31:43,410 --> 00:31:46,569
the read lock and write lock at the same time,

00:31:46,569 --> 00:31:48,850
again, with 96 thread.

00:31:48,850 --> 00:31:52,610
And you can see that glibc actually performed better

00:31:52,610 --> 00:31:56,300
than my simple implementations of the read/write lock.

00:31:56,300 --> 00:32:01,300
There was, I think about two years ago,

00:32:02,100 --> 00:32:06,004
glibc undergo a major rewrite of the read/write lock code.

00:32:06,004 --> 00:32:09,000
Before that, what I remember is

00:32:09,000 --> 00:32:12,680
my wait-wake futex code performed better than glibc.

00:32:12,680 --> 00:32:17,180
And now, with the new glibc read/write lock code,

00:32:17,180 --> 00:32:19,410
it perform better than my simple

00:32:21,881 --> 00:32:24,340
wait-wake lock implementation.

00:32:24,340 --> 00:32:28,730
But still, the OS futex is significantly faster

00:32:28,730 --> 00:32:30,820
than the other two.

00:32:30,820 --> 00:32:34,790
- [Man] Yeah, so just try 500 threads.

00:32:34,790 --> 00:32:37,179
Try 500 threads and see how you do.

00:32:37,179 --> 00:32:42,179
- Okay, I will run some tests

00:32:42,602 --> 00:32:47,602
when we have more thread than

00:32:47,704 --> 00:32:50,293
the actual CPU available.

00:32:52,800 --> 00:32:54,530
At the time-- - Or just by the fact

00:32:54,530 --> 00:32:56,230
that we have counter inside

00:32:56,230 --> 00:32:58,710
and it's one counter for all threads.

00:32:58,710 --> 00:33:00,650
- Yeah, there's one reader count.

00:33:00,650 --> 00:33:03,840
- [Man] You will synchronize all CPU cores in the cache?

00:33:03,840 --> 00:33:05,093
- The way that,

00:33:07,370 --> 00:33:09,060
no, no, actually,

00:33:09,060 --> 00:33:11,630
because is, the way it work,

00:33:11,630 --> 00:33:13,300
you would try to acquire a lock in user-space,

00:33:13,300 --> 00:33:15,490
if you can't, you go into a kernel to do it.

00:33:15,490 --> 00:33:17,700
- Yeah, but, wait, you are not even locking.

00:33:17,700 --> 00:33:20,260
So every time you have readers come in the read/write lock,

00:33:20,260 --> 00:33:23,490
you have to increase number of readers right?

00:33:23,490 --> 00:33:25,730
It's come and leaving, come and leaving.

00:33:25,730 --> 00:33:28,353
- Depending on how you implement your read/write lock.

00:33:28,353 --> 00:33:31,410
There are many ay you can implement read/write lock

00:33:31,410 --> 00:33:35,380
instead of mutex, mutex is really easy.

00:33:35,380 --> 00:33:37,260
Read/write is far more complicated.

00:33:37,260 --> 00:33:40,410
You can have a very sophisticated algorithm

00:33:40,410 --> 00:33:43,053
to implement read/write lock in user-space.

00:33:46,263 --> 00:33:48,130
And the glibc read/write lock implementation

00:33:48,130 --> 00:33:50,820
in one example is pretty complex

00:33:50,820 --> 00:33:54,195
And that is how they improved performance

00:33:54,195 --> 00:33:55,763
with the new implementation.

00:33:57,030 --> 00:34:02,030
With the old futex, the user-space read/write lock code

00:34:02,670 --> 00:34:07,630
is pretty more simpler than

00:34:07,630 --> 00:34:09,840
what will be if you used the various futex.

00:34:09,840 --> 00:34:12,630
- So just try with 500 concurrent threads

00:34:12,630 --> 00:34:14,883
and show us the result.

00:34:14,883 --> 00:34:16,110
- Yeah.

00:34:16,110 --> 00:34:21,110
And yeah, the way that, maybe I show you in the next slide.

00:34:22,630 --> 00:34:23,860
- [Man] Sorry, Waiman, I have one question

00:34:23,860 --> 00:34:25,180
on that previous one.

00:34:25,180 --> 00:34:26,220
Oh no, this might be it.

00:34:26,220 --> 00:34:27,526
I was just gonna ask you if you had breakdowns

00:34:27,526 --> 00:34:29,920
of the readers and writers to see if they're all fair,

00:34:29,920 --> 00:34:31,590
but that might be this slide.

00:34:31,590 --> 00:34:32,870
- The previous one, yeah.

00:34:32,870 --> 00:34:33,703
- No but I think, yeah, I think it might be

00:34:33,703 --> 00:34:34,810
the next slide has actually answered it.

00:34:34,810 --> 00:34:37,810
So wondered if, are all three of these implementations fair?

00:34:40,247 --> 00:34:41,190
- Not really. - Because with glibc,

00:34:41,190 --> 00:34:43,136
for example, it could be all the readers.

00:34:43,136 --> 00:34:43,969
- Not really. - And then,

00:34:43,969 --> 00:34:47,527
it's not really. - Not really, I can show you.

00:34:47,527 --> 00:34:50,510
In order to see the fairness

00:34:50,510 --> 00:34:54,690
of the different lock implementation, what I did is,

00:34:54,690 --> 00:34:58,270
in the previous slide, the locking thread

00:34:58,270 --> 00:35:00,401
kinda alternate between doing read lock

00:35:00,401 --> 00:35:02,651
and doing a write lock, so.

00:35:04,183 --> 00:35:08,182
But in this case, I dedicate half of the thread

00:35:08,182 --> 00:35:09,389
to doing the read lock,

00:35:09,389 --> 00:35:13,160
half of he thread to doing write lock exclusively.

00:35:13,160 --> 00:35:14,683
And this is what happened.

00:35:16,394 --> 00:35:18,623
With the you prefer the reader,

00:35:19,920 --> 00:35:22,870
the wait-wake futex implementation,

00:35:22,870 --> 00:35:24,480
there is lock starvation.

00:35:24,480 --> 00:35:26,838
You just can't acquire the write lock at all.

00:35:26,838 --> 00:35:28,803
And similarly, for glibc.

00:35:30,920 --> 00:35:33,563
So lock starvation really can happen.

00:35:34,870 --> 00:35:38,840
With the OF futex, because I implemented

00:35:38,840 --> 00:35:40,770
a lock hand-off mechanism in the kernel,

00:35:40,770 --> 00:35:42,220
so it would make sure that

00:35:44,290 --> 00:35:45,683
no matter how,

00:35:46,630 --> 00:35:49,770
that every lock waiter will have a chance

00:35:49,770 --> 00:35:51,250
to acquire the lock.

00:35:51,250 --> 00:35:52,083
And

00:35:53,620 --> 00:35:57,810
so you see that even though the current implementation,

00:35:57,810 --> 00:36:01,037
definitely prefer reader more than the writer,

00:36:01,904 --> 00:36:03,893
the writer can still acquire the lock.

00:36:07,290 --> 00:36:10,713
And even so, there is no lock starvation.

00:36:11,580 --> 00:36:16,580
But the second table show you the other case

00:36:17,000 --> 00:36:18,780
when you prefer the writer.

00:36:18,780 --> 00:36:22,754
In this case, the wait-wake futex implementation

00:36:22,754 --> 00:36:24,663
become much more fair.

00:36:25,560 --> 00:36:28,550
So writer lock, in this case, writer is preferred.

00:36:28,550 --> 00:36:33,143
For the OS futex, it's still prefer the reader

00:36:33,143 --> 00:36:35,200
because of the implementation.

00:36:35,200 --> 00:36:36,820
I can certainly change the implementation

00:36:36,820 --> 00:36:40,030
to prefer the writer a bit more.

00:36:40,030 --> 00:36:43,300
But anyway, in the preferred writer mode,

00:36:43,300 --> 00:36:45,110
it's performance is actually better

00:36:45,110 --> 00:36:47,700
than the in the prefer reader mode.

00:36:47,700 --> 00:36:51,843
So that's why currently the preferred writer is the default.

00:36:53,012 --> 00:36:55,663
And for glibc, it is better than the prefer reader mode,

00:36:59,840 --> 00:37:02,570
but still, there is some kind

00:37:02,570 --> 00:37:04,970
of lock starvation going on when you are reader.

00:37:15,381 --> 00:37:16,214
Okay.

00:37:18,239 --> 00:37:21,400
There are still more work I think I need to do

00:37:21,400 --> 00:37:25,303
to complete the OS futex patch set.

00:37:28,250 --> 00:37:32,780
This slide list the work that I need to do

00:37:32,780 --> 00:37:36,033
before it is ready to for sending out the patch.

00:37:37,020 --> 00:37:38,450
The first thing, the most important one

00:37:38,450 --> 00:37:42,020
is that I need to find a user for the OS futex.

00:37:42,020 --> 00:37:47,020
It can be, I had attempt to try to add it into the glibc

00:37:47,810 --> 00:37:51,697
but it is not so easy sometimes

00:37:51,697 --> 00:37:54,863
to convince the glibc maintainer to take it.

00:37:56,580 --> 00:38:01,580
And then another possibility is to find an application

00:38:01,762 --> 00:38:06,762
that can use the OS futex like the open source PostgreSQL

00:38:07,410 --> 00:38:12,410
or some other database, open-source database product.

00:38:13,580 --> 00:38:15,530
And another thing that I need to do is,

00:38:16,380 --> 00:38:20,770
all the futex syscall support a timeout argument.

00:38:20,770 --> 00:38:23,953
So you can go in your kernel, try to lock it,

00:38:25,908 --> 00:38:28,440
if you exceed a certain timeout limit,

00:38:28,440 --> 00:38:31,677
it will go back out and return an error to you

00:38:31,677 --> 00:38:34,750
saying that it just wait too long

00:38:34,750 --> 00:38:36,490
before you can acquire the lock.

00:38:36,490 --> 00:38:39,530
And in order to support that,

00:38:39,530 --> 00:38:42,300
I need to add a mutex lock variant

00:38:42,300 --> 00:38:44,650
that has a timeout argument,

00:38:44,650 --> 00:38:46,763
which currently, we don't have that.

00:38:47,730 --> 00:38:52,409
The only lock that have a timeout is the RT-mutex.

00:38:52,409 --> 00:38:54,963
But mutex doesn't have the timeout.

00:38:57,830 --> 00:39:01,800
And I also want to investigate different way

00:39:01,800 --> 00:39:05,900
of trying to more spinning in the user-space,

00:39:05,900 --> 00:39:09,143
instead of just focus mainly in the kernel.

00:39:09,143 --> 00:39:12,133
Because you can acquire the lock in user-space,

00:39:15,160 --> 00:39:18,783
it's faster than going to the kernel and go back up.

00:39:18,783 --> 00:39:23,330
And currently, there are two different alternative

00:39:23,330 --> 00:39:24,480
that I am looking into.

00:39:25,570 --> 00:39:27,383
I can use a bit in the futex word

00:39:27,383 --> 00:39:31,520
to indicate that the task is running.

00:39:31,520 --> 00:39:33,700
That is what the glibc guy

00:39:33,700 --> 00:39:36,870
want to have a mechanism to do that.

00:39:36,870 --> 00:39:41,870
And so one way to do it is to use the OS futex.

00:39:44,970 --> 00:39:49,261
And another alternative that I'm investigating

00:39:49,261 --> 00:39:54,261
is I'm trying to allow one locker to spin in the user-space

00:39:55,060 --> 00:39:57,150
and one and only one only.

00:39:57,150 --> 00:40:00,240
You have more locker trying to spin in user-space,

00:40:00,240 --> 00:40:03,860
it will create a lot more cacheline contention problem.

00:40:03,860 --> 00:40:05,320
But if you have just one

00:40:06,230 --> 00:40:09,230
in the user-space and one in the kernel,

00:40:09,230 --> 00:40:11,510
then the situation isn't that bad.

00:40:11,510 --> 00:40:12,343
So

00:40:15,670 --> 00:40:18,770
hopefully, I will have something ready

00:40:18,770 --> 00:40:23,770
in a few months to send out to the kernel community.

00:40:25,970 --> 00:40:29,350
And that is pretty much it,

00:40:29,350 --> 00:40:31,050
that's the end of my presentation.

00:40:33,350 --> 00:40:37,350
And any other question that you have.

00:40:40,580 --> 00:40:42,120
- I have one question. - Yeah.

00:40:42,120 --> 00:40:45,836
- You have, you set a timeout value.

00:40:45,836 --> 00:40:48,673
What's the timeout value you mentioned, 1.5?

00:40:51,260 --> 00:40:53,410
- No the timeout-- - Oh, sorry.

00:40:53,410 --> 00:40:54,243
- I'm not following

00:40:54,243 --> 00:40:55,076
my own instructions, here. - The futex itself,

00:40:55,076 --> 00:40:57,280
allows you to specify a timeout value.

00:40:57,280 --> 00:41:00,050
The timeout can be any value.

00:41:00,050 --> 00:41:02,857
So you can timeout for one millisecond

00:41:02,857 --> 00:41:07,700
or timeout for one hour. - That's the caller's choice?

00:41:07,700 --> 00:41:12,007
- But because this is what the futex API provide,

00:41:12,007 --> 00:41:15,070
I need to be consistent with that.

00:41:15,070 --> 00:41:17,000
And in order to really do that

00:41:17,000 --> 00:41:22,000
especially, if you go into microsecond level,

00:41:22,740 --> 00:41:27,717
then, I really need to have a variant mutex lock

00:41:27,717 --> 00:41:29,833
that have a timeout argument.

00:41:36,429 --> 00:41:39,596
- [Woman] Any other questions for him?

00:41:41,260 --> 00:41:45,601
- Okay, thank you very much for your time.

00:41:45,601 --> 00:41:50,272
Hopefully, I will have something to send out

00:41:50,272 --> 00:41:53,240
in the near future.

00:41:53,240 --> 00:41:57,003

YouTube URL: https://www.youtube.com/watch?v=xDMlXJ6uHbI


