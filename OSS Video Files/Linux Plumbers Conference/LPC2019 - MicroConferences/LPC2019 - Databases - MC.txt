Title: LPC2019 - Databases - MC
Publication date: 2019-11-18
Playlist: LPC2019 - MicroConferences
Description: 
	Database Microconference Summary

io_uring was the initial topic both gauging its maturity and examination if there where existing performance problems. A known problem of buffered writes triggering stalls was raised, and is being already worked on by separating writes into multiple queues/locks. Existing tests showed comparable performance on read both of O_DIRECT and buffered. MySQL showing twice as bad performance currently however this is hoped that this is the known issue.

Write barriers are needed for writing the reversing a partial transaction such that it is written before the tablespace changes such that, in the case of power failure, the partial transaction can be reversed to preserve the Atomicity principle. The crux of the problem is that a write needs to be durable on disk (like fsynced) before another write. SCSI standards contain an option that has never been implemented however for the large part, no hardware level support exists. While the existing userspace implementation uses fsync, its a considerable overhead and it ensure that all file pending writes are synced, when only on aspect is needed. The way forward seems to be use/extend the chained write approach in io_uring.

O_ATOMIC, the promise of write all or nothing (and the existing block remaining intact), was presented as a requirement. We examined cases of existing hardware support by Shannon, FusionIO and Google storage all have different access mechanism and isn't discoverable and gets rather dependent on the filesystem implementation. The current userspace workaround is to double write the same data to ensure a write tear doesn't occur. There may be a path forward by using a NVDIMM aspect as a staging area attached to the side of a filesystem. XFS has a copy on write mechanism that is work in progress and its currently recommend to wait for this on a database workload (assumed to be: [ioctl_ficlonerange|http://man7.org/linux/man-pages/man2/ioctl_ficlonerange.2.html]).

XFS / Ext4 behaviours that exhibited greater throughput with more writes. Some theories for this where proposed but without more data it was hard to say. There was a number of previous bugs where an increase in hardware performance resulted in bigger queues and decreased performance. A full bisection between kernel versions to identify a commit was suggested. There was some correctness aspects fixed between the 4.4 version and the current version, but these may need to be reexamined. Quite possible two effects where in play. Off cpu analysis, in particular using an eBPF based mechanism of sampling proposed by Josef discussion, would result in better identification of what threads are waiting and where. A later discussion covered differences between unix and tcp loopback implementations and the performance of sockets where there was 0 backlog (gained 15% performance) also needs a similar level of probing/measurement to be actionable.

SQLite and the IO Errors discussions covered a large gap in POSIX specification of what can happen in errors. An example is an experimentally found chain of 6 syscalls seemed to be required to reliably rename a file. A document describing what is needed to perform some basic tasks and that result in uniform behaviour across filesystems would alleviate much frustration, guesswork and disappointment. Attendees where invited to ask a set of specific questions on linux-fsdevel@vger.kernel.org where they could be answered and pushed into documentation enhancements.

The reimplementation of MySQL redo log reduced the number of mutexes however left gaps in synchronization mechanism between worker threads. The use of spin vs mutex locks to synchronise between stages was lacking in some APIs. Waiman Long in the talk Efficient Userspace Optimistic Spinning Locks resented some options in a presentation later this day (however lacked saturation test cases).

Syscall overhead and CPU cache issues weren't covered in time however some of this was answered in Tuesdays Tracing BoF and other LPC session covered these.

The LWN article https://lwn.net/Articles/799807/ covers SQLite and Postgresql IO Errors topics in more detail.

All of the topics presented cover the needs of database implementers present in the discussion. Many thanks to our userspace friends:

    Sergei Golubchik (MariaDB)
    Dimitri Kravtchuk (MySQL)
    Pawel Olchawa (MySQL)
    Richard Hipp (SQLite)
    Andres Freund (Postgresql)
    Tomas Vondra (Postgresql)
    Josef Ahmad (MongoDB)

MC lead
Daniel  Black
Captions: 
	00:00:00,950 --> 00:00:03,005
- Well, welcome everyone.

00:00:03,005 --> 00:00:04,840
Daniel Blackford, if you don't know me.

00:00:04,840 --> 00:00:07,046
Today we're going to be covering

00:00:07,046 --> 00:00:08,890
the Database Micro Conference.

00:00:08,890 --> 00:00:12,590
I've got a number of great discussion leaders here

00:00:12,590 --> 00:00:15,090
to present on the topics below

00:00:15,090 --> 00:00:17,580
that you have undoubtedly seen on the calendar,

00:00:17,580 --> 00:00:18,950
so I'll get right into it.

00:00:20,000 --> 00:00:22,220
I'm gonna try to run today's session

00:00:22,220 --> 00:00:26,084
under like a De Bono's Blue Hat Six Hat Theory

00:00:26,084 --> 00:00:29,870
to actually lead the discussion as much

00:00:29,870 --> 00:00:32,606
as possible in most directions before coming

00:00:32,606 --> 00:00:36,047
to a conclusion because the first idea

00:00:36,047 --> 00:00:39,360
that we come up with isn't always the best one

00:00:39,360 --> 00:00:41,070
and it's not always the worst.

00:00:41,070 --> 00:00:44,100
So we're going to try to do that.

00:00:44,100 --> 00:00:47,600
So on the Blue Hat side of the intro,

00:00:47,600 --> 00:00:51,280
I'm going to reaffirm that this is a discussion

00:00:51,280 --> 00:00:54,830
as you've seen done particularly well in some

00:00:54,830 --> 00:00:59,830
of the talks so far, but here so particularly well.

00:01:01,810 --> 00:01:06,540
Some of the other discussions you may have know have lead

00:01:06,540 --> 00:01:11,420
to particularly solution-orientated things.

00:01:11,420 --> 00:01:13,730
I think by talking to our speakers here

00:01:13,730 --> 00:01:16,940
that they're more to here to actually present

00:01:16,940 --> 00:01:21,000
the problems rather than a particular solution, in a way.

00:01:28,905 --> 00:01:31,550
Under the White Hat today, we're going

00:01:31,550 --> 00:01:33,750
to be covering the six thinking hats

00:01:33,750 --> 00:01:37,150
so we've got a Blue Hat to cover the management side,

00:01:37,150 --> 00:01:39,450
the White Hat, as I mentioned before,

00:01:39,450 --> 00:01:42,023
leads into factual information

00:01:42,023 --> 00:01:44,880
that we know about the problem.

00:01:45,800 --> 00:01:49,740
A Green Hat sort of leads to the development

00:01:49,740 --> 00:01:53,330
of opportunities and potential solutions.

00:01:54,630 --> 00:01:58,020
Red hat just to get a gauge of our feelings on

00:01:58,020 --> 00:02:00,870
the solutions or the problem statement

00:02:00,870 --> 00:02:02,940
or the general process as it is.

00:02:04,570 --> 00:02:07,390
A Yellow Hat to sort of encompass

00:02:07,390 --> 00:02:10,840
the optimism around particular solutions.

00:02:11,710 --> 00:02:15,200
A Black Hat to look at the caution aspects of it.

00:02:15,200 --> 00:02:18,860
And yeah, we'll probably end up with a Blue Hat there then

00:02:18,860 --> 00:02:22,680
to sort of queue in the discussion on that.

00:02:25,780 --> 00:02:28,230
I will be sort of timing things

00:02:28,230 --> 00:02:30,400
and sort of keeping things going.

00:02:31,680 --> 00:02:35,860
But is there any other questions for now before we start?

00:02:39,500 --> 00:02:40,760
Nope, excellent.

00:02:42,764 --> 00:02:43,597
Dimitri, it's all yours.

00:03:09,654 --> 00:03:11,909
- Nobody wants to wait?

00:03:11,909 --> 00:03:12,992
- [Man] Sure.

00:03:14,382 --> 00:03:16,132
- Thank you. - Thanks.

00:03:20,731 --> 00:03:22,746
I would (speaking off mic) another one.

00:03:22,746 --> 00:03:24,760
- You want this one?

00:03:24,760 --> 00:03:29,760
- Yeah, more (speaking off mic) on speaker off.

00:03:33,595 --> 00:03:38,595
- Okay.

00:03:38,780 --> 00:03:41,850
So it seems like I will speak a lot here

00:03:41,850 --> 00:03:45,460
so I at least tell you what we are doing

00:03:45,460 --> 00:03:47,790
so we will be able to start soon.

00:03:48,940 --> 00:03:53,940
So I am in MySQL Performance team.

00:03:55,990 --> 00:03:59,860
So while I was working in Benchmark Center at Sun before

00:03:59,860 --> 00:04:04,860
and then Sun acquired, oh, MySQL Oracle required Sun.

00:04:05,940 --> 00:04:08,310
So MySQL now is at Oracle.

00:04:09,320 --> 00:04:11,800
And while I was doing performance stuff for fun

00:04:11,800 --> 00:04:15,460
there at MySQL but finally they proposed me

00:04:15,460 --> 00:04:18,090
to do it full stuff so it's really exciting

00:04:19,040 --> 00:04:21,320
and the most exciting thing, we have

00:04:21,320 --> 00:04:24,030
so many problems and we wanted to expose

00:04:24,030 --> 00:04:29,020
it here and expecting them with common effort,

00:04:29,020 --> 00:04:31,340
we can bring something more here.

00:04:31,340 --> 00:04:36,340
So the first topic which we wanted to discuss,

00:04:37,140 --> 00:04:38,770
so this is totally opposite

00:04:38,770 --> 00:04:42,080
to all other major conference when people present

00:04:42,080 --> 00:04:45,338
in solution, they see we are presenting problems.

00:04:45,338 --> 00:04:46,171
(audience laughing)

00:04:46,171 --> 00:04:49,750
So I really hope you got your good breakfast

00:04:49,750 --> 00:04:54,400
and wake it up so the first point is about IO-uring,

00:04:55,980 --> 00:05:00,420
in fact, in MySQL, so energy be using asynchronous IO

00:05:00,420 --> 00:05:01,870
from a long time.

00:05:01,870 --> 00:05:06,870
It was given even use an internal homemade synchronous IO,

00:05:08,250 --> 00:05:13,250
so since version 5.5, we move it to the native

00:05:13,530 --> 00:05:15,430
IO synchronization and now there is a lot

00:05:15,430 --> 00:05:20,180
of excitement about IO-uring library so people

00:05:20,180 --> 00:05:23,510
were reporting some very nice results,

00:05:23,510 --> 00:05:26,840
nice observations but it's still per IO test

00:05:26,840 --> 00:05:29,103
with historically we know them,

00:05:29,103 --> 00:05:33,017
there was so many storage when they are showing them,

00:05:33,017 --> 00:05:36,750
they have excellent results on pure IO test,

00:05:36,750 --> 00:05:38,700
when we're on MySQL on this,

00:05:38,700 --> 00:05:42,030
just twice lower than what we already have,

00:05:42,030 --> 00:05:46,310
so we are very sense sensible here

00:05:46,310 --> 00:05:48,370
and developers are critical on this.

00:05:48,370 --> 00:05:50,330
So from a long time, for example,

00:05:50,330 --> 00:05:54,180
there was a pressure on us why we don't use POSIX mutexes,

00:05:54,180 --> 00:05:56,380
why we don't use talent libraries,

00:05:56,380 --> 00:05:59,930
we made a fault we made general layer,

00:05:59,930 --> 00:06:03,910
then you can use any kind of lock primitive today

00:06:03,910 --> 00:06:08,060
in Moscow, and it's always given worse reason

00:06:08,060 --> 00:06:11,250
than what we have in homemade implementation.

00:06:11,250 --> 00:06:15,210
So for this case to motivate developers,

00:06:15,210 --> 00:06:18,210
I need to have some good examples

00:06:18,210 --> 00:06:21,660
from feedback about so is it stable enough

00:06:21,660 --> 00:06:24,410
or is this library, so any feedback you have,

00:06:24,410 --> 00:06:29,390
is it really better than default IO in all cases?

00:06:29,390 --> 00:06:32,490
What kind of problem you have, you see already,

00:06:32,490 --> 00:06:35,820
so what shall we do?

00:06:36,940 --> 00:06:38,490
So...

00:06:38,490 --> 00:06:40,698
- [Man] Yeah, I think now's a good time

00:06:40,698 --> 00:06:43,279
for anyone who's started to use IO-uring

00:06:43,279 --> 00:06:46,580
to explain what they've seen so...

00:06:53,672 --> 00:06:54,505
- Hi, can you hear me?

00:06:54,505 --> 00:06:55,720
Yeah, okay.

00:06:55,720 --> 00:06:59,020
I've started to work on a patch set for PostgreSQL

00:06:59,020 --> 00:07:03,780
that use IO-uring and the results are pretty decent,

00:07:03,780 --> 00:07:07,400
not great yet, the CPU usage for IO-uring

00:07:07,400 --> 00:07:10,677
is still much much higher than if you do it

00:07:10,677 --> 00:07:14,134
for current all right or for direct.

00:07:14,134 --> 00:07:17,211
I think I've started to mail with Expo

00:07:17,211 --> 00:07:22,211
to fix some of those issues and it seems like

00:07:22,848 --> 00:07:27,848
that's mostly fixable and based on my very prototype code,

00:07:29,230 --> 00:07:32,900
you can expect pretty decent wins from using it,

00:07:32,900 --> 00:07:36,880
especially because you can, with a recent Cisco,

00:07:36,880 --> 00:07:38,380
it's getting much more expensive due

00:07:38,380 --> 00:07:41,810
to the all security fixes being able

00:07:41,810 --> 00:07:43,160
to have only one Cisco in like a lot

00:07:43,160 --> 00:07:46,180
of writes in my benchmarks is quite valuable.

00:07:47,200 --> 00:07:48,850
- But also you mention it then...

00:07:51,206 --> 00:07:53,872
- [Man] Yeah, that's fine.

00:07:53,872 --> 00:07:56,789
(speaking off mic)

00:07:58,420 --> 00:08:00,070
- If I recall, you mention it's--

00:08:00,070 --> 00:08:02,900
- [Man] Can I ask for quick clarification?

00:08:02,900 --> 00:08:06,010
Is is the more CPU you use for over direct?

00:08:06,010 --> 00:08:07,960
- Yeah, for not using IO-direct.

00:08:07,960 --> 00:08:10,170
The CPU usage is higher because all

00:08:10,170 --> 00:08:12,710
the writes go through the work queue

00:08:12,710 --> 00:08:16,490
and the work queue then spawns like, don't know,

00:08:16,490 --> 00:08:19,093
six, like two-time CPUs workers,

00:08:19,093 --> 00:08:22,514
and they all contend on the inode-lock

00:08:22,514 --> 00:08:25,990
so all you get is a lot more contention inode-lock.

00:08:25,990 --> 00:08:27,890
And that makes things lower

00:08:27,890 --> 00:08:31,657
and I think Jans was suggesting we just reduced

00:08:31,657 --> 00:08:32,700
the number but have a separate work queue

00:08:33,808 --> 00:08:35,480
for writers and then that problem gets solved

00:08:35,480 --> 00:08:36,870
by having less concurrency.

00:08:36,870 --> 00:08:38,740
Which is obviously not the best approach,

00:08:38,740 --> 00:08:41,180
but I think it will, for now, be better,

00:08:41,180 --> 00:08:43,410
obviously be better if writes could work

00:08:43,410 --> 00:08:46,460
with you, whatever they, no wait, flag thing,

00:08:46,460 --> 00:08:49,160
and then could actually be much faster but

00:08:49,160 --> 00:08:51,210
that seems like a bit further off.

00:08:51,210 --> 00:08:52,640
- [Man] Right, and so the reason why I asked

00:08:52,640 --> 00:08:55,900
for the clarification was in general IO

00:08:55,900 --> 00:08:58,790
in the Kernel is split up between direct IO and buffered,

00:08:59,874 --> 00:09:01,460
and one works very well and one is kind

00:09:01,460 --> 00:09:05,157
of a best-effort, we hope it works for you so--

00:09:05,157 --> 00:09:07,320
- [Questioner] Depending on who you ask,

00:09:07,320 --> 00:09:10,231
the answer which is which would be different.

00:09:10,231 --> 00:09:12,800
- [Man] But so hopefully you don't see

00:09:12,800 --> 00:09:15,310
an increased CPU usage from direct IO because

00:09:15,310 --> 00:09:17,970
that should faster 100% of the time

00:09:17,970 --> 00:09:22,600
than the AIO stuff uring is replacing.

00:09:22,600 --> 00:09:24,790
- I see very little performance difference between

00:09:24,790 --> 00:09:28,070
the two but then also a post-crisis direct arrow support,

00:09:28,070 --> 00:09:30,340
that's also in my passion, it's so crappy that,

00:09:30,340 --> 00:09:34,880
I'm not sure that's something to take anything from.

00:09:34,880 --> 00:09:38,830
- But you also mentioned that yesterday's,

00:09:38,830 --> 00:09:42,050
and if you use this for IO writes,

00:09:42,050 --> 00:09:42,980
you have a problem, right?

00:09:42,980 --> 00:09:44,490
So it's only on the writes if you have problem,

00:09:44,490 --> 00:09:45,710
not on reads?

00:09:45,710 --> 00:09:47,880
- Yeah the performance problem I see was purely

00:09:47,880 --> 00:09:50,858
on writes, because on reads, IO-uring,

00:09:50,858 --> 00:09:53,660
for buffered reads supports or just

00:09:53,660 --> 00:09:56,820
the general reach this call and with supports

00:09:56,820 --> 00:09:58,960
a flag that allows it to return,

00:10:00,460 --> 00:10:03,780
to quickly test whether there is any data in the cache,

00:10:03,780 --> 00:10:06,670
and if so, it doesn't have to spawn the worker thread,

00:10:06,670 --> 00:10:10,600
and that's kind of makes the problem much less pronounced.

00:10:10,600 --> 00:10:12,970
- It's only about Gopher in your case.

00:10:12,970 --> 00:10:13,960
- Yes. - So with IO-direct,

00:10:13,960 --> 00:10:15,080
the story is different.

00:10:15,080 --> 00:10:15,913
- [Man] Yes.

00:10:16,793 --> 00:10:18,790
- Yeah, because we always advance them to use,

00:10:18,790 --> 00:10:21,520
and in fact, during the past two days,

00:10:21,520 --> 00:10:23,670
when I saw all the problems people reporting

00:10:23,670 --> 00:10:26,560
about Kernel memory management,

00:10:28,400 --> 00:10:31,720
we are doing better currently to use our own buffer pool

00:10:31,720 --> 00:10:34,500
so probably for polarizes fell it makes sense

00:10:34,500 --> 00:10:36,260
to investigators as well.

00:10:36,260 --> 00:10:38,570
- I think we're going to have to use O-direct

00:10:38,570 --> 00:10:40,480
for very well-tuned databases

00:10:40,480 --> 00:10:42,870
and use buffered IO for the untuned databases,

00:10:42,870 --> 00:10:45,270
which is unfortunately a very large fraction

00:10:45,270 --> 00:10:47,070
of postcodes databases, so I think we are going

00:10:47,070 --> 00:10:49,730
to need to support both, and probably default

00:10:49,730 --> 00:10:51,460
to not using direct IO.

00:10:52,370 --> 00:10:54,240
- [Man] So one of the things that Jans

00:10:54,240 --> 00:10:57,237
and I talked about after you mentioned the mutex contention,

00:10:57,237 --> 00:11:00,080
in the buffered IO right path, again,

00:11:00,080 --> 00:11:02,260
this is only the buffered IO right path,

00:11:02,260 --> 00:11:05,300
the file systems all call something called

00:11:05,300 --> 00:11:08,700
balanced dirty pages which is basically us asking

00:11:08,700 --> 00:11:11,120
the memory management system if we've made too

00:11:11,120 --> 00:11:13,360
many dirty pages and should we wait,

00:11:13,360 --> 00:11:15,670
and when the memory management system says yes,

00:11:15,670 --> 00:11:19,540
you need to wait now, we do this with the inode-lock help,

00:11:20,660 --> 00:11:22,451
which seems unwise.

00:11:22,451 --> 00:11:24,663
(laughing)

00:11:24,663 --> 00:11:27,070
It's really not a good way to decrease contention on

00:11:27,070 --> 00:11:28,370
the inode-lock because we're waiting

00:11:28,370 --> 00:11:30,510
for an arbitrary period of time

00:11:30,510 --> 00:11:32,300
for no reason that will help.

00:11:32,300 --> 00:11:35,130
So that's some low-hanging fruit to make that thing.

00:11:36,004 --> 00:11:38,330
(speaking off mic)

00:11:38,330 --> 00:11:41,119
It's actually per file system but we've all done

00:11:41,119 --> 00:11:42,183
it the same way.

00:11:42,183 --> 00:11:43,310
- Okay, interesting.

00:11:44,575 --> 00:11:48,200
- So the same question about this,

00:11:48,200 --> 00:11:50,100
do you have any kind of tune-in

00:11:50,100 --> 00:11:53,270
for sorting about how many IO you will accept in parallel?

00:11:55,260 --> 00:11:56,980
- Could you repeat the question?

00:11:56,980 --> 00:12:00,190
- So how many are your writes, and are you reads,

00:12:00,190 --> 00:12:01,590
you will accept in parallel?

00:12:03,240 --> 00:12:07,150
- At the moment that's, actually that is in such

00:12:07,150 --> 00:12:09,580
an early state that I have just hard coded it

00:12:09,580 --> 00:12:14,580
and I haven't done benchmarks to come to conclusions.

00:12:15,420 --> 00:12:17,600
- For that, at least in the case of dark tiles,

00:12:17,600 --> 00:12:18,920
I think fixing the write and

00:12:18,920 --> 00:12:21,160
the wait thing will help you both.

00:12:22,043 --> 00:12:23,870
Because as soon as you start getting me again,

00:12:23,870 --> 00:12:28,870
you know you'll have to wait and I install them, I think.

00:12:28,980 --> 00:12:31,410
- Well, but we expect all of of IO to block,

00:12:31,410 --> 00:12:35,230
so that's fine, but we often have drives good lock,

00:12:35,230 --> 00:12:37,130
pretty deep queues and it's fine for us

00:12:37,130 --> 00:12:40,650
to use those queues so, and we probably need

00:12:40,650 --> 00:12:45,320
to be a bit more complicated depending on the workload.

00:12:45,320 --> 00:12:47,940
- Yeah, because from next problems later,

00:12:47,940 --> 00:12:50,620
you will see them, we have some strange situations

00:12:50,620 --> 00:12:53,800
and we don't, we can hit some starvation of

00:12:53,800 --> 00:12:56,730
the storage which we cannot explain on file system layer,

00:12:56,730 --> 00:12:59,630
or whatever, and seems like we need

00:12:59,630 --> 00:13:02,845
to get visibility on all layers to understand what happens.

00:13:02,845 --> 00:13:07,580
So in fact, well, you will see later when we do more writes,

00:13:07,580 --> 00:13:09,770
we are going faster for no reason.

00:13:09,770 --> 00:13:12,270
So in fact, okay? (speaking off mic)

00:13:12,270 --> 00:13:13,810
Yeah.

00:13:13,810 --> 00:13:16,100
So let's finish with IO-uring.

00:13:16,100 --> 00:13:17,240
So any other feedback?

00:13:17,240 --> 00:13:18,590
So how you feel it.

00:13:18,590 --> 00:13:20,490
So everybody tested this or (mumbles).

00:13:24,344 --> 00:13:25,897
- [Man] So Matt, have you done benchmarking,

00:13:25,897 --> 00:13:27,413
do I remember right?

00:13:27,413 --> 00:13:28,919
- [Matt] No, just bug testing.

00:13:28,919 --> 00:13:31,242
- [Man] Just bug testing, yeah.

00:13:31,242 --> 00:13:34,575
Yeah, as far as I know, it's always better.

00:13:34,575 --> 00:13:36,735
It's always ready, always better.

00:13:36,735 --> 00:13:37,756
- 100%. - Yeah.

00:13:37,756 --> 00:13:39,244
- Yes. - Except for buffered writes.

00:13:39,244 --> 00:13:42,185
- Device vendor, so it's a good way

00:13:42,185 --> 00:13:44,480
to really test the device.

00:13:44,480 --> 00:13:49,480
For a slow-ish device at least you get very close to

00:13:49,735 --> 00:13:52,650
or same as what SPDK gives you,

00:13:52,650 --> 00:13:56,730
because there's still, in terms of the overhead,

00:13:56,730 --> 00:13:58,990
SPDK's still way lower so if you have

00:13:58,990 --> 00:14:01,110
a really, really, really fast device,

00:14:01,110 --> 00:14:03,320
SPDK will still beat IO-uring.

00:14:04,750 --> 00:14:07,940
But compared to IO, definitely yes.

00:14:07,940 --> 00:14:09,170
It's been everywhere.

00:14:10,130 --> 00:14:14,260
- So our Oracle Linux team made some testing

00:14:14,260 --> 00:14:17,690
but on pure IO again, so they observed assumption

00:14:17,690 --> 00:14:21,030
up to 15% better performance than classic IO,

00:14:22,450 --> 00:14:26,350
well, as it represent, in fact, big changes

00:14:26,350 --> 00:14:28,470
because it's not fully compatible, right,

00:14:28,470 --> 00:14:32,681
with classic IO interface or the codes will be adapted.

00:14:32,681 --> 00:14:37,681
And so, well, 15% I expected to see something more bigger,

00:14:37,836 --> 00:14:42,836
you know, to really motivated developers so I...

00:14:43,070 --> 00:14:46,790
- The thing is it's saving on your, basically,

00:14:46,790 --> 00:14:49,400
your CPU and the system call costs

00:14:49,400 --> 00:14:54,400
so what's in your overall IO lens URL completion time

00:14:55,200 --> 00:14:57,930
was the percentage of that part because there's

00:14:57,930 --> 00:15:00,850
the device part that you can't get away with,

00:15:00,850 --> 00:15:02,140
it's still there.

00:15:02,140 --> 00:15:06,900
And it's going to depend, the percentage gained you're going

00:15:06,900 --> 00:15:09,600
to see was going to depend on how fast your device is.

00:15:10,730 --> 00:15:14,450
- Especially on the first day here there

00:15:14,450 --> 00:15:18,460
was guys speaking about scheduling optimization and so on,

00:15:18,460 --> 00:15:20,380
and as soon as your IO board,

00:15:20,380 --> 00:15:23,371
they supposed that, okay, it's not impacting,

00:15:23,371 --> 00:15:25,440
in fact, I don't know if people realize what today's storage

00:15:25,440 --> 00:15:28,199
is so fast and everything is CPU bound

00:15:28,199 --> 00:15:33,120
so pure IO-- - That is not so true,

00:15:33,970 --> 00:15:37,930
because you say a device is so fast so take

00:15:37,930 --> 00:15:42,930
a good enterprise SSD, you're still 50-60 nano second prior.

00:15:43,430 --> 00:15:45,460
- Yeah and-- - And your whole stack

00:15:45,460 --> 00:15:47,900
is eating, what, two, three half seconds?

00:15:47,900 --> 00:15:49,190
Microseconds? - No, no.

00:15:49,190 --> 00:15:54,190
So with MySQL, to saturate the storage, obtain storage,

00:15:54,970 --> 00:15:56,330
48 course we're using--

00:15:56,330 --> 00:15:58,530
- Yeah but obtain is 10 microsecond.

00:15:58,530 --> 00:16:00,693
That's a different class of storage.

00:16:00,693 --> 00:16:01,920
- [Presenter] Agree, agree.

00:16:01,920 --> 00:16:04,575
- So I'm talking about normal flash enterprises.

00:16:04,575 --> 00:16:08,520
The IO stack overhead in the entire IO pass

00:16:08,520 --> 00:16:09,720
is still very small.

00:16:09,720 --> 00:16:11,610
So IO-uring, yes, you have some gain,

00:16:11,610 --> 00:16:14,640
but in terms of percentages, it's minimal.

00:16:14,640 --> 00:16:17,790
If you go to obtain your 15 microsecond reads,

00:16:17,790 --> 00:16:20,560
yeah, okay, that's that's way smaller,

00:16:20,560 --> 00:16:23,700
and then the what, four, five microsecond of the IO stack

00:16:23,700 --> 00:16:26,720
is a big chunk of the overall IO time

00:16:26,720 --> 00:16:28,893
so you get. - Yeah but its end,

00:16:28,893 --> 00:16:31,630
even on the remote storage going by the networks

00:16:31,630 --> 00:16:34,210
on network and CPU, your IO CPU,

00:16:34,210 --> 00:16:37,010
and then if you use Optane, it will be fast enough,

00:16:37,010 --> 00:16:38,160
and again, you need to just

00:16:38,160 --> 00:16:40,277
use a network. - Yes, yes.

00:16:40,277 --> 00:16:42,080
- [Man] Another comment over here.

00:16:42,080 --> 00:16:43,740
- Actually good question for the crowd.

00:16:43,740 --> 00:16:45,740
How many people here have used IO-uring?

00:16:47,870 --> 00:16:48,770
Okay.

00:16:48,770 --> 00:16:52,200
How many people here would be deploying on SSDs, okay.

00:16:54,440 --> 00:16:56,110
How many people here would be deploying with

00:16:56,110 --> 00:16:58,590
an NVRAM controller with the right cache in the way?

00:17:00,510 --> 00:17:01,343
Okay.

00:17:01,343 --> 00:17:03,500
How many people here would be deploying on VMs?

00:17:05,440 --> 00:17:08,850
And AWS as your cloud systems?

00:17:09,940 --> 00:17:13,680
Okay, so we have a fairly wide range of users that was the..

00:17:13,680 --> 00:17:15,100
- Yeah. - Today if you don't

00:17:15,100 --> 00:17:19,081
use flash tourism you really don't like your data, right?

00:17:19,081 --> 00:17:21,924
(group laughing)

00:17:21,924 --> 00:17:22,777
- [Man] Before you have a lot of people.

00:17:24,380 --> 00:17:26,550
- Even a lot, so in fact,

00:17:26,550 --> 00:17:30,670
today when you see flutter become cheaper and cheaper so...

00:17:32,190 --> 00:17:34,340
- Right, part of the problem you have

00:17:34,340 --> 00:17:35,173
with that is lifetimes.

00:17:35,173 --> 00:17:37,669
If you have to guarantee five, six,

00:17:37,669 --> 00:17:38,900
seven-year appliance lifetimes,

00:17:38,900 --> 00:17:40,970
SSDs and a constant right load become

00:17:40,970 --> 00:17:42,220
an interesting trade-off.

00:17:45,495 --> 00:17:50,080
- [Man] (speaking off mic) Got two and a half gigs.

00:17:50,080 --> 00:17:52,850
- Let's see so, well, probably next year we'll

00:17:52,850 --> 00:17:55,750
be able to show the solution above this.

00:17:57,387 --> 00:18:00,541
Indeed, it's very exciting but, you know?

00:18:00,541 --> 00:18:01,588
- [Man] Yeah.

00:18:01,588 --> 00:18:04,320
- So I guess to summarize the session,

00:18:04,320 --> 00:18:07,010
there was some low-hanging fruit in the way

00:18:07,010 --> 00:18:11,350
the buffered IO writes occur that can be addressed.

00:18:12,490 --> 00:18:14,920
Was there any other major, sort of,

00:18:14,920 --> 00:18:16,820
gaps that people identified?

00:18:17,670 --> 00:18:19,610
- [Man] No wait for writes.

00:18:19,610 --> 00:18:23,929
- Writes, okay, cool.

00:18:23,929 --> 00:18:26,290
So yeah, I think you're up next.

00:18:33,450 --> 00:18:35,133
I don't know just...

00:18:41,911 --> 00:18:43,496
- [Man] Thank you.

00:18:43,496 --> 00:18:46,246
(group clapping)

00:18:51,340 --> 00:18:52,640
- [Producer] We're into the interlude.

00:18:52,640 --> 00:18:54,600
Can people introduce themselves as

00:18:54,600 --> 00:18:56,790
they pick up the mic? - Oh, I'm sorry.

00:18:58,027 --> 00:19:01,350
- [Man] Great.

00:19:30,044 --> 00:19:33,112
(man speaking off mic)

00:19:33,112 --> 00:19:34,900
- [Woman] Welcome to the table (muttering).

00:19:34,900 --> 00:19:36,495
- Oh okay, that one, yep.

00:19:36,495 --> 00:19:39,412
(woman whispering)

00:19:40,836 --> 00:19:41,669
Yep, okay.

00:19:42,800 --> 00:19:47,800
Okay, thank you.

00:20:06,383 --> 00:20:10,150
And anyone with a laptop open, feel free to join

00:20:10,150 --> 00:20:12,610
the ether pad on the URL on the front

00:20:12,610 --> 00:20:16,650
and I helped take those with me. (laughs)

00:20:19,314 --> 00:20:22,550
- I basically just wanted to discuss an idea

00:20:22,550 --> 00:20:25,830
or something that I maybe don't find,

00:20:25,830 --> 00:20:27,860
oh yeah, I'm (mutters) Luchek,

00:20:27,860 --> 00:20:30,170
I've broken many difficult operation

00:20:30,170 --> 00:20:35,170
and this VP, my DB server engineering, and I was working

00:20:38,540 --> 00:20:41,950
on MySQL since 1998.

00:20:45,050 --> 00:20:45,883
So yeah.

00:20:47,610 --> 00:20:49,540
Well that's basically, just to discuss an idea

00:20:49,540 --> 00:20:52,660
and something that maybe I don't want

00:20:52,660 --> 00:20:54,840
to (mutters) because I don't know why

00:20:54,840 --> 00:20:58,798
I was able to find that leave information about it.

00:20:58,798 --> 00:21:03,798
And the thing is that for write-ahead logging

00:21:04,430 --> 00:21:06,460
and many databases do that so it's not

00:21:06,460 --> 00:21:08,470
like something like particularly unique.

00:21:08,470 --> 00:21:11,012
You need to write and to log you need to make sure

00:21:11,012 --> 00:21:12,390
that it's the disk and then you write to the data

00:21:12,390 --> 00:21:16,260
and this can be done if you just everything after writing

00:21:16,260 --> 00:21:20,450
the log, but it's really too expensive.

00:21:20,450 --> 00:21:24,550
And what would make much cheaper liability journey would

00:21:24,550 --> 00:21:29,550
be described as, because if we do just do two writes,

00:21:31,050 --> 00:21:33,480
we need, I'm auditing to show,

00:21:33,480 --> 00:21:35,940
you need to be sure that the first write happens

00:21:35,940 --> 00:21:38,210
and is actually physically written before

00:21:38,210 --> 00:21:40,180
the second one, you don't care when

00:21:40,180 --> 00:21:43,750
this particular happen and again, I think it guarantees

00:21:43,750 --> 00:21:46,220
that it will happen well, when you think that's,

00:21:46,220 --> 00:21:47,420
again, let's jump ahead.

00:21:48,410 --> 00:21:53,410
Yes so I found this thing was in Kernel earlier.

00:21:53,810 --> 00:21:57,220
That was removed and it was done in drivers

00:21:58,257 --> 00:22:01,490
for file systems, and it was removed because it was found

00:22:01,490 --> 00:22:04,540
to be like to heaven providing too stronger

00:22:04,540 --> 00:22:07,250
and these not for what they requested.

00:22:07,250 --> 00:22:08,780
And they said and said that the systems just need

00:22:08,780 --> 00:22:13,470
to ensure that, just need to care rather themselves,

00:22:13,470 --> 00:22:16,190
and this is apparently what databases do

00:22:16,190 --> 00:22:19,409
but ultimately, all databases do that

00:22:19,409 --> 00:22:24,409
and does look to me like this belongs to the Kernel.

00:22:26,150 --> 00:22:30,580
And this is what, yeah, the chart just shows basically what

00:22:30,580 --> 00:22:35,580
a write barrier is, as this been, like memory barrier.

00:22:38,410 --> 00:22:43,410
So this is write coming in this chronological order

00:22:43,970 --> 00:22:46,171
with one second and one is on.

00:22:46,171 --> 00:22:51,140
And the Kernel, disk controller, whatever,

00:22:51,140 --> 00:22:53,970
they can do all the writes and that's,

00:22:53,970 --> 00:22:58,270
everyone, I don't have interest but I can blow

00:22:58,270 --> 00:23:00,260
it up later if anybody's interested.

00:23:00,260 --> 00:23:04,606
But they perform better if you really do lots of writes.

00:23:04,606 --> 00:23:07,250
They have lots of, instead of like,

00:23:07,250 --> 00:23:09,170
I know every thinking after every block,

00:23:09,170 --> 00:23:10,800
if they have lots of logs that in the order then

00:23:10,800 --> 00:23:15,610
to do, well, provide better performance.

00:23:16,540 --> 00:23:18,720
They can optimize writes better if they have lots

00:23:18,720 --> 00:23:21,640
of writes in the queue so that they get them optimally.

00:23:21,640 --> 00:23:26,180
So that's basically those various tell anything below

00:23:27,320 --> 00:23:29,740
the database that can authorize, not to authorize,

00:23:29,740 --> 00:23:31,470
it was better so they can treat it

00:23:31,470 --> 00:23:33,690
and authorize between two barriers

00:23:33,690 --> 00:23:37,445
and that's, all it is, this provides us

00:23:37,445 --> 00:23:42,280
for the try guarantees that many databases need.

00:23:44,630 --> 00:23:49,630
And if that would have been everything means basically here,

00:23:49,700 --> 00:23:52,020
everything would mean basically wait until all

00:23:52,020 --> 00:23:55,460
of this is written and this also wait

00:23:55,460 --> 00:23:58,370
and write barrier just issues write barrier

00:23:58,370 --> 00:24:01,072
and everything continues and eventually later

00:24:01,072 --> 00:24:04,280
this all be written in this order.

00:24:04,280 --> 00:24:07,030
But we don't need to wait until we actually have to go,

00:24:07,030 --> 00:24:09,550
we need to wait at coming point, yes.

00:24:09,550 --> 00:24:11,860
- Have you measured how much of

00:24:11,860 --> 00:24:14,150
the benefit you can get by using IO-uring?

00:24:14,150 --> 00:24:16,540
Because you can actually have drain operations inside

00:24:16,540 --> 00:24:18,879
the queue and have things also

00:24:18,879 --> 00:24:20,630
as a Kernel-- - No, no, no.

00:24:20,630 --> 00:24:23,707
We are looking at the uring, we didn't try it.

00:24:23,707 --> 00:24:24,910
- [Man] Okay.

00:24:24,910 --> 00:24:28,030
- Just to clarify, so on the picture here,

00:24:28,030 --> 00:24:31,370
the vertical thing area, the barriers are interested in

00:24:31,370 --> 00:24:34,920
so it could have synced today today, but you want that

00:24:34,920 --> 00:24:35,753
in the Kernel. - Yes, yes,

00:24:35,753 --> 00:24:36,900
but as I said, it couldn't be synced,

00:24:36,900 --> 00:24:39,040
it is synced now but fsync is more expensive.

00:24:39,040 --> 00:24:39,873
- Okay. - In terms of you have

00:24:39,873 --> 00:24:44,480
to wait and this is just depends on the ordering

00:24:44,480 --> 00:24:45,930
but it does not-- - Okay.

00:24:45,930 --> 00:24:48,860
So one more question about that.

00:24:48,860 --> 00:24:53,240
Do you have a strong requirement for the writes between

00:24:53,240 --> 00:24:57,250
the the barrier or fsync to be in order

00:24:57,250 --> 00:25:02,250
or is it just a chunk of writes order with the buyers?

00:25:02,460 --> 00:25:05,070
So can I have, for example, the writes in

00:25:05,070 --> 00:25:08,270
the middle here with two buyers being reordered?

00:25:08,270 --> 00:25:09,420
Do you care about that?

00:25:09,420 --> 00:25:11,380
- [Speaker] I don't understand.

00:25:11,380 --> 00:25:13,730
- So the writes have to be, say they are sequential,

00:25:13,730 --> 00:25:15,180
they have to be sequential or not?

00:25:15,180 --> 00:25:18,600
- [Speaker] No, the barrier simply driven,

00:25:19,626 --> 00:25:20,628
those are the ordering.

00:25:20,628 --> 00:25:21,814
Those are the order. - Oh, oh really,

00:25:21,814 --> 00:25:22,850
are crossed barriers. - In video order but--

00:25:22,850 --> 00:25:24,700
- Within the barriers is fine?

00:25:24,700 --> 00:25:26,640
Reordering with the barrier

00:25:26,640 --> 00:25:28,380
is fine? - Yes, exactly.

00:25:28,380 --> 00:25:30,040
That's the whole point. - Okay.

00:25:30,040 --> 00:25:31,480
All right.

00:25:31,480 --> 00:25:33,460
- Oh, sorry. - That would make

00:25:33,460 --> 00:25:35,880
sense on the database point of view.

00:25:35,880 --> 00:25:39,220
- Just to clarify this is writes across multiple files,

00:25:39,220 --> 00:25:40,053
is that correct?

00:25:40,900 --> 00:25:44,510
- Yes so this thing is actually,

00:25:44,510 --> 00:25:47,190
it could have been an API that says you just want

00:25:47,190 --> 00:25:49,140
these two particular writes to be in this order

00:25:49,140 --> 00:25:51,140
if you know what everything else,

00:25:51,140 --> 00:25:53,860
but I think it will be probably a lot more complex

00:25:53,860 --> 00:25:55,610
so even think about that. - Right, because you need

00:25:55,610 --> 00:25:57,393
to write-- - So this is

00:25:57,393 --> 00:25:58,702
as simple write barriers.

00:25:58,702 --> 00:26:00,630
- So you need to write to the log file.

00:26:00,630 --> 00:26:02,066
- Yes. - You need those

00:26:02,066 --> 00:26:03,800
to complete before you start writing

00:26:03,800 --> 00:26:05,382
to the database. - This is the write

00:26:05,382 --> 00:26:08,089
to the log file, this is the write to the data file.

00:26:08,089 --> 00:26:11,306
This is, again, log file, this is the data file related

00:26:11,306 --> 00:26:15,830
to this log file so-- - Right, how many log files

00:26:15,830 --> 00:26:20,030
and how many data files are there at one time?

00:26:20,030 --> 00:26:24,260
- So by default, in the builder to log files but--

00:26:25,670 --> 00:26:27,140
- How many database files?

00:26:27,140 --> 00:26:29,070
- Yeah, there could be a lot of data files.

00:26:29,070 --> 00:26:31,400
That's not the point because there are also,

00:26:31,400 --> 00:26:34,016
this could be also other applications running

00:26:34,016 --> 00:26:37,010
to other writes and from this, I don't know,

00:26:37,010 --> 00:26:39,170
this point of view, it doesn't know

00:26:39,170 --> 00:26:40,720
which one belongs to which.

00:26:40,720 --> 00:26:42,610
And I just see a stream of writes

00:26:42,610 --> 00:26:46,280
and the simple case, we just want

00:26:46,280 --> 00:26:47,570
to prevent reordering across

00:26:47,570 --> 00:26:49,117
the barrier. - So does

00:26:49,117 --> 00:26:52,120
the barrier operation apply to all writes coming from

00:26:52,120 --> 00:26:53,310
the same process?

00:26:53,310 --> 00:26:57,140
- Not necessarily so, as I said,

00:26:57,140 --> 00:27:00,190
it could be, I don't know how it could look--

00:27:00,190 --> 00:27:01,480
- How do you tell the Kernel

00:27:01,480 --> 00:27:03,690
which file descriptors - Yes, yes.

00:27:03,690 --> 00:27:06,440
- the barrier applies to. - So in the most simple way,

00:27:07,871 --> 00:27:10,630
it would just be some point in the old stream

00:27:10,630 --> 00:27:13,090
of requests to write something to disk

00:27:13,090 --> 00:27:16,900
and to prevent the ordering those writes

00:27:16,900 --> 00:27:19,554
to those blocks and those writes.

00:27:19,554 --> 00:27:23,680
If it'll be a very fine, great interface by,

00:27:23,680 --> 00:27:25,550
say, this write to this file should

00:27:25,550 --> 00:27:27,560
be before that write to that file.

00:27:27,560 --> 00:27:29,030
It'll be also fine.

00:27:29,030 --> 00:27:31,420
But it'll probably be a bit more complicated

00:27:31,420 --> 00:27:34,900
to implement on the lower level.

00:27:34,900 --> 00:27:37,100
It would be anything between this one

00:27:37,100 --> 00:27:39,680
or that one or anything between those two.

00:27:39,680 --> 00:27:42,560
Of course, all the database actually need

00:27:42,560 --> 00:27:44,160
to say that this write should be

00:27:45,014 --> 00:27:47,340
to this file should be before that write to that file.

00:27:49,096 --> 00:27:50,520
- So at least from an interface point of view,

00:27:50,520 --> 00:27:54,160
as was mentioned, IO-uring is where we're hoping

00:27:54,160 --> 00:27:56,480
this kind of thing will be available

00:27:56,480 --> 00:27:57,490
or will be used. - Yeah.

00:27:57,490 --> 00:27:59,670
- 'Cause it's already in the interface, right?

00:27:59,670 --> 00:28:00,503
- Okay.

00:28:00,503 --> 00:28:02,250
- And this is exactly why it's there.

00:28:02,250 --> 00:28:04,320
One of the problems, one of the reasons why

00:28:04,320 --> 00:28:05,550
it was taken out of the Kernel in

00:28:05,550 --> 00:28:08,010
the past is because in general

00:28:08,010 --> 00:28:11,670
it assumes way too much knowledge about what needs

00:28:11,670 --> 00:28:15,170
to happen on an individual file system level

00:28:15,170 --> 00:28:17,950
in order for something to be fully consistent on disk.

00:28:17,950 --> 00:28:19,600
- And file system, actually yes,

00:28:19,600 --> 00:28:24,600
actually do have write barriers to between write into

00:28:24,960 --> 00:28:27,750
the journal and write into the files.

00:28:27,750 --> 00:28:29,660
There's no way to export this to

00:28:29,660 --> 00:28:33,370
the usual and let applications to control

00:28:33,370 --> 00:28:36,080
the writes between different types of different files.

00:28:36,080 --> 00:28:38,170
- [Man] Yeah, and so the problem that we have

00:28:38,170 --> 00:28:41,050
is some of the times, like if you're in a file system,

00:28:41,050 --> 00:28:43,560
these barriers are going to be fsyncs.

00:28:43,560 --> 00:28:45,051
- Yep. - But hopefully

00:28:45,051 --> 00:28:45,884
we queue up with IO-uring.

00:28:45,884 --> 00:28:46,717
- Yeah.

00:28:46,717 --> 00:28:48,880
- [Man] And if you're down in a block device level,

00:28:48,880 --> 00:28:51,030
these barriers are going to be simple cache flushes.

00:28:51,030 --> 00:28:52,510
- Yep. - Again, hopefully queued up

00:28:52,510 --> 00:28:55,953
with IO-uring, and the really great thing about

00:28:55,953 --> 00:28:58,224
the IO-uring interface is that it actually allows you

00:28:58,224 --> 00:29:01,740
to chain these things so that you can actually send them

00:29:01,740 --> 00:29:04,690
in via a single system call, without having

00:29:04,690 --> 00:29:05,950
to wait for the results.

00:29:05,950 --> 00:29:09,920
- Yes, that was basically the main point because

00:29:09,920 --> 00:29:13,903
if you would flush after every single log write

00:29:16,050 --> 00:29:21,050
that would basically mean a full stop after every write,

00:29:21,540 --> 00:29:23,960
but if there's too many writes to different files

00:29:23,960 --> 00:29:26,630
and then you do one, even if it's flush

00:29:26,630 --> 00:29:29,950
and cache flush and somebody down below waits,

00:29:29,950 --> 00:29:33,620
it still can group them efficiently the way

00:29:34,634 --> 00:29:36,960
that hardware I can do talk to my performance.

00:29:37,980 --> 00:29:42,058
- [Man] But so I hope this is here today in IO-uring.

00:29:42,058 --> 00:29:43,505
- Okay, great. - It's certainly

00:29:43,505 --> 00:29:44,620
the intention.

00:29:44,620 --> 00:29:46,600
- Yeah, I guess my question was related

00:29:46,600 --> 00:29:50,730
to if you're not waiting, you're just trying

00:29:50,730 --> 00:29:55,730
to enforce ordering, how would you know that the device

00:29:56,260 --> 00:29:58,710
is not gonna reorder those in their device queue?

00:29:59,864 --> 00:30:01,270
- [Guy] You don't.

00:30:01,270 --> 00:30:02,620
- [Man] That's why it's a barrier.

00:30:02,620 --> 00:30:04,835
- Well that's the point of this thing.

00:30:04,835 --> 00:30:09,450
It means that it should not be reordered across

00:30:09,450 --> 00:30:11,040
the barriers, it's not a feature that exists,

00:30:11,040 --> 00:30:14,563
the feature that I'm discussing that we make,

00:30:14,563 --> 00:30:17,850
but it would make a lot of sense to have

00:30:17,850 --> 00:30:21,610
and progress after (mutters) the IO-uring already has.

00:30:21,610 --> 00:30:23,610
- This is not just at the RS level,

00:30:23,610 --> 00:30:26,420
it has to send a request to the device

00:30:26,420 --> 00:30:29,365
to also enforce that type of barrier.

00:30:29,365 --> 00:30:32,080
- Yeah, yes, for a device it might be a cache flush.

00:30:32,080 --> 00:30:33,760
- [Man] That's the always hope.

00:30:33,760 --> 00:30:35,153
- Yes.

00:30:35,153 --> 00:30:37,351
- [Man] And the reason is if you cover completely more

00:30:37,351 --> 00:30:40,222
to be like, all was right, microwaving for reviews,

00:30:40,222 --> 00:30:42,912
in border security, we have a guarantee

00:30:42,912 --> 00:30:45,873
that there are the (mutters) you would acknowledge

00:30:45,873 --> 00:30:47,859
the device but you don't know what the device

00:30:47,859 --> 00:30:49,219
is really to review scale.

00:30:49,219 --> 00:30:51,488
- Yes, well, that's the point that a database,

00:30:51,488 --> 00:30:55,060
it shouldn't really know, okay, about those details,

00:30:55,060 --> 00:30:57,730
and the driver, if the driver knows it's multi-queue device

00:30:57,730 --> 00:31:00,910
and then the driver will do fsync on all the queues

00:31:00,910 --> 00:31:03,810
or whatever, it's just like an application would do.

00:31:03,810 --> 00:31:05,549
- [Man] Yeah, no, I agree with you,

00:31:05,549 --> 00:31:07,579
I'm just saying, how many advise editors

00:31:07,579 --> 00:31:08,580
can you have? - Yeah.

00:31:08,580 --> 00:31:09,520
(group laughing)

00:31:09,520 --> 00:31:10,620
- [Man] I can't (mutters) that

00:31:10,620 --> 00:31:12,244
I have to do that with. - Yep.

00:31:12,244 --> 00:31:13,694
- [Man] Flush will always be.

00:31:15,540 --> 00:31:18,968
- So I haven't used IO-uring so the extent

00:31:18,968 --> 00:31:23,104
of my knowledge is the (mumbles) article.

00:31:23,104 --> 00:31:27,887
But what I read is that the completion queue implies

00:31:29,806 --> 00:31:34,806
that operations can be completed in different order, right?

00:31:35,680 --> 00:31:37,616
Is there a way to enforce a barrier then,

00:31:37,616 --> 00:31:39,090
based on that, you know? - No.

00:31:39,090 --> 00:31:41,311
- It seems like something-- - No way.

00:31:41,311 --> 00:31:43,231
- Yeah-- - Sorry, not happening.

00:31:43,231 --> 00:31:44,531
- No, no. - Daniel, could

00:31:45,502 --> 00:31:47,287
you put the mic over there. - So there is, the IO-uring...

00:31:47,287 --> 00:31:50,440
- Sorry, and then by barrier here, I mean,

00:31:50,440 --> 00:31:53,410
in the context, not in the context of the storage,

00:31:53,410 --> 00:31:57,271
but in the context of Sega's proposal, right?

00:31:57,271 --> 00:31:59,674
(man speaking off mic)

00:31:59,674 --> 00:32:01,120
Huh? - So the IO-uring

00:32:01,120 --> 00:32:05,900
specification has a way to say this operation

00:32:05,900 --> 00:32:09,150
can only start after this other operation is complete.

00:32:10,000 --> 00:32:13,970
And so that chaining logic is arbitrary.

00:32:14,805 --> 00:32:15,638
- Right. - It could be a write

00:32:15,638 --> 00:32:18,443
that happens after a read or a read that happens after

00:32:18,443 --> 00:32:19,800
a write or an fsync that happens after a write

00:32:19,800 --> 00:32:23,210
or arbitrary combinations thereof.

00:32:23,210 --> 00:32:24,260
- Right. - And so, that's why

00:32:24,260 --> 00:32:27,180
I was saying it's meant to be able to provide this kind

00:32:27,180 --> 00:32:30,660
of operation where, you know, something that you send down

00:32:30,660 --> 00:32:33,340
is the barrier, whether it be a cache flush,

00:32:33,340 --> 00:32:34,730
if you're on a device, or an fsync,

00:32:34,730 --> 00:32:37,500
if you're on a file system or whatever you might need,

00:32:37,500 --> 00:32:41,010
and then it allows reordering, as you say,

00:32:41,010 --> 00:32:44,110
'cause there's no implicit ordering unless

00:32:44,110 --> 00:32:47,020
you specifically apply the chaining flag.

00:32:47,020 --> 00:32:49,380
There's no explicit ordering of what happens.

00:32:50,760 --> 00:32:52,060
Did that answer?

00:32:52,060 --> 00:32:53,616
- It does, yeah.

00:32:53,616 --> 00:32:54,617
- [Man] Okay.

00:32:54,617 --> 00:32:56,630
- So I'm curious because we use write barriers

00:32:56,630 --> 00:32:58,780
and say okay, I want this write and then this write

00:32:58,780 --> 00:33:01,287
and we're gonna use a barrier so all

00:33:01,287 --> 00:33:02,761
the other writes get carried with it in order

00:33:02,761 --> 00:33:03,594
to force the ordering of those two.

00:33:04,696 --> 00:33:06,720
Why is it we can't use FUA or Native Command Queuing

00:33:06,720 --> 00:33:08,410
at the disk level in order to enforce

00:33:08,410 --> 00:33:11,060
that ordering instead of having to have a full flush?

00:33:12,410 --> 00:33:15,014
- Because you're going to kill the device.

00:33:15,014 --> 00:33:16,800
(man speaks off mic)

00:33:16,800 --> 00:33:18,390
You're going to kill the device performance

00:33:18,390 --> 00:33:20,160
if you enforce that.

00:33:20,160 --> 00:33:22,840
And basically just turn the right cache off on the device

00:33:22,840 --> 00:33:24,560
and that's what you're going to get.

00:33:24,560 --> 00:33:25,670
- Except the other writes can then

00:33:25,670 --> 00:33:27,380
be ordered anywhere anywhere you want

00:33:28,285 --> 00:33:30,223
and you don't need to have the barriers

00:33:30,223 --> 00:33:31,802
to force unwanted rights to go with it.

00:33:31,802 --> 00:33:34,887
- Well, so on NVMESSD, I'd say probably

00:33:34,887 --> 00:33:39,887
it's not going to change much, on a HDD, definitely secure.

00:33:40,730 --> 00:33:44,130
Your brand-new HDD with right cache disabled

00:33:44,130 --> 00:33:45,930
and that's what you're going to get.

00:33:47,780 --> 00:33:51,216
(man speaking off mic)

00:33:51,216 --> 00:33:54,514
- Yeah, okay.

00:33:54,514 --> 00:33:58,400
- With right cache disable, no it's going to be horrible.

00:34:00,320 --> 00:34:03,670
HDDs with write cache disabled are really horrible.

00:34:03,670 --> 00:34:05,670
- [Man] They're horrible but sometimes you have to,

00:34:07,400 --> 00:34:09,504
you either have to use FUA or you have

00:34:09,504 --> 00:34:10,656
to turn off the cache in order

00:34:10,656 --> 00:34:11,852
to turn it for you. - Yes, no, for that

00:34:11,852 --> 00:34:12,991
I can see, yes, I know.

00:34:12,991 --> 00:34:13,920
I'm just talking about performance here.

00:34:15,258 --> 00:34:16,645
- [Man] Can I have the microphone for a sec?

00:34:16,645 --> 00:34:19,550
So in respect to the FUA,

00:34:19,550 --> 00:34:22,760
the way file systems implement barrier operations

00:34:23,683 --> 00:34:27,040
for journal commits is first we do the cache flush

00:34:27,040 --> 00:34:29,760
to get everything that the transaction might depend on

00:34:29,760 --> 00:34:32,680
on disk, and then usually the file systems have some block

00:34:32,680 --> 00:34:34,920
that's really important, super block,

00:34:34,920 --> 00:34:37,120
route block and the B-tree, whatever it might be,

00:34:37,120 --> 00:34:39,920
and then we'll FUA down that one last block.

00:34:39,920 --> 00:34:42,780
So we do use the FUA bits,

00:34:42,780 --> 00:34:45,120
but we don't use it on every IO,

00:34:45,120 --> 00:34:47,140
we use it for the specific thing that needs

00:34:47,140 --> 00:34:49,500
to happen right this very second,

00:34:49,500 --> 00:34:53,150
and also doesn't have any other timing dependencies.

00:34:53,150 --> 00:34:56,662
Like that super block, it doesn't matter if it's going down

00:34:56,662 --> 00:34:59,420
in parallel with a whole bunch of other writes

00:34:59,420 --> 00:35:02,050
that aren't dependent on the transaction.

00:35:02,050 --> 00:35:05,983
It just needs to be the thing that finishes the transaction.

00:35:07,670 --> 00:35:12,670
- Just to clarify here, FUA, the first minute access,

00:35:13,320 --> 00:35:15,890
is going to guarantee that it's going to medium

00:35:15,890 --> 00:35:18,230
so you don't need another flush.

00:35:18,230 --> 00:35:20,480
It's not going into anything on the ordering.

00:35:21,460 --> 00:35:24,240
The HD can still do that write FUA

00:35:24,240 --> 00:35:26,620
and any other it wants with other read-in writes.

00:35:26,620 --> 00:35:28,717
- [Man] That's why I said it had to be in combination

00:35:28,717 --> 00:35:31,080
of NCQ, you couldn't do it just, and let the disk reorder,

00:35:31,080 --> 00:35:33,320
you had to give it an order to queue.

00:35:33,320 --> 00:35:35,640
- That doesn't exist in the specs.

00:35:35,640 --> 00:35:38,409
The order tags in schedule, for example,

00:35:38,409 --> 00:35:41,320
are there, nobody ever implemented that.

00:35:43,070 --> 00:35:44,390
And the block I was talking in,

00:35:44,390 --> 00:35:46,490
it doesn't care about the tag number

00:35:46,490 --> 00:35:48,267
and it's not there either.

00:35:50,573 --> 00:35:53,298
An NCQ does that now, it doesn't have...

00:35:53,298 --> 00:35:54,478
- [Man] Yeah.

00:35:54,478 --> 00:35:57,820
- But now if we bring this to practice,

00:35:57,820 --> 00:36:02,120
how it works, so in fact, in most configuration director,

00:36:02,120 --> 00:36:05,670
I use it for NODB, so for any page write,

00:36:05,670 --> 00:36:08,690
as soon as you do write, you know then it's so--

00:36:08,690 --> 00:36:10,172
- Direct IO... - No, direct IO

00:36:10,172 --> 00:36:12,166
is second driver. - No, I mean it's

00:36:12,166 --> 00:36:14,580
going to storage, right?

00:36:14,580 --> 00:36:15,963
- No. - No, it's going

00:36:15,963 --> 00:36:19,580
to the on disk cache, well, the storage has a cache

00:36:19,580 --> 00:36:20,980
and it goes to the cache on the disk.

00:36:20,980 --> 00:36:23,220
- Going on storage layer, let's call it lazy,

00:36:23,220 --> 00:36:24,540
it's not on-- - Yes, yes,

00:36:24,540 --> 00:36:28,011
but if it's a parallel PH, I want it written down.

00:36:28,011 --> 00:36:30,250
- Yeah, I agree.

00:36:30,250 --> 00:36:32,350
- So that's what I'm pointing out.

00:36:32,350 --> 00:36:33,680
- Okay, but it's going already on

00:36:33,680 --> 00:36:35,260
the storage layer, right? - Yes, yes.

00:36:35,260 --> 00:36:40,260
- So in fact, once you go fsync so, well,

00:36:40,410 --> 00:36:43,180
even with directive so we have to call fsync

00:36:43,180 --> 00:36:44,990
and the only, what's important,

00:36:44,990 --> 00:36:48,050
is when you want to remove the page from the cache,

00:36:48,050 --> 00:36:50,330
then you want to be sure then it was already written.

00:36:50,330 --> 00:36:51,580
- Yeah. - So well, in fact,

00:36:51,580 --> 00:36:54,930
once you made the fsync for the file,

00:36:54,930 --> 00:36:56,770
you know then up to this layer

00:36:56,770 --> 00:36:59,400
is already-- - Yes, fsync you do,

00:36:59,400 --> 00:37:01,710
you want do fsync when you want to make sure

00:37:01,710 --> 00:37:03,760
that at this point, everything is written down

00:37:03,760 --> 00:37:06,160
and that's what you want then it happens.

00:37:06,160 --> 00:37:09,270
If you do some writes in the middle,

00:37:09,270 --> 00:37:11,050
you don't really want to wait at this point.

00:37:11,050 --> 00:37:11,890
- Wait, wait, wait. - But if you doing sync,

00:37:11,890 --> 00:37:13,730
there's no other choice.

00:37:13,730 --> 00:37:14,680
- Yeah, but it's different story for Commit,

00:37:14,680 --> 00:37:17,605
because for Commit you Commit your (muttering).

00:37:17,605 --> 00:37:18,450
- Yes, I'm not talking about Commit here.

00:37:19,612 --> 00:37:22,390
- Yeah but the main point is and at some point you need

00:37:23,846 --> 00:37:26,180
to accept and confirm Commit to the user because,

00:37:26,180 --> 00:37:28,009
I mean, all this time, - Yes.

00:37:28,009 --> 00:37:28,842
- use all the wait, right? - Yes.

00:37:28,842 --> 00:37:32,430
But during the transaction, you still might need

00:37:32,430 --> 00:37:35,290
to write to the write ahead log and the data.

00:37:35,290 --> 00:37:37,480
And at this point in time, you don't really want to wait.

00:37:38,769 --> 00:37:39,602
This one when the other

00:37:39,602 --> 00:37:40,543
would be-- - Yeah but,

00:37:40,543 --> 00:37:42,620
that's why, so (mumbles) has this call it (mumbles) Commit,

00:37:44,213 --> 00:37:45,875
so in fact, Checkpoint.

00:37:45,875 --> 00:37:46,708
- Yeah. - It's first on Checkpoint.

00:37:46,708 --> 00:37:47,541
- [Speaker] On Checkpoint, you also want to make.

00:37:47,541 --> 00:37:49,390
- Yeah, but because during this time,

00:37:50,286 --> 00:37:52,150
in fact, nobody waiting for page writes,

00:37:52,150 --> 00:37:54,570
only for redo logs, so redo logs enough to--

00:37:54,570 --> 00:37:55,930
- Yeah but this one, exactly,

00:37:55,930 --> 00:37:57,730
but not waiting for redo log writes.

00:37:58,920 --> 00:38:03,320
- How then you confirm the data was committed when--

00:38:03,320 --> 00:38:04,620
- When you do Commit you do fsync

00:38:04,620 --> 00:38:07,670
and everything up to this point is written down.

00:38:07,670 --> 00:38:12,300
- Fine, in fact, when you have short transactions

00:38:12,300 --> 00:38:13,840
and we have few users--

00:38:13,840 --> 00:38:16,250
- If you have short transactions that don't write anything

00:38:16,250 --> 00:38:18,860
to disk then you don't need to listen about,

00:38:18,860 --> 00:38:20,360
don't need to care about that.

00:38:22,130 --> 00:38:25,600
- I think to these mostly it's short transactions, right?

00:38:25,600 --> 00:38:28,050
- Yes, there are other use cases as well

00:38:28,050 --> 00:38:33,050
and I'm sure MySQL is used in these use cases too.

00:38:34,040 --> 00:38:36,410
- Very quickly, did I understand correctly that direct IO

00:38:36,410 --> 00:38:37,600
is direct to the right buffer - MySQL is not, it is

00:38:37,600 --> 00:38:38,980
- of the device? - a basic general database

00:38:38,980 --> 00:38:39,930
if you're using it.

00:38:41,077 --> 00:38:42,406
- Of the device, whatever you guys

00:38:42,406 --> 00:38:43,537
are developing. - Yeah.

00:38:43,537 --> 00:38:44,720
But the device is then free

00:38:44,720 --> 00:38:45,974
to reorder things any way it wants.

00:38:45,974 --> 00:38:47,113
- Reorder, write cache,

00:38:47,113 --> 00:38:48,840
you can, yeah, you can. - Yeah, okay.

00:38:48,840 --> 00:38:50,500
I just wanted to make sure I understood that, thank you.

00:38:50,500 --> 00:38:52,898
- [Man] Yeah, one of the really confusing things

00:38:52,898 --> 00:38:54,320
about file systems and direct IO is

00:38:54,320 --> 00:38:57,630
that direct IO promises that it has started the IO,

00:38:57,630 --> 00:38:59,200
and it also, if you wait for it,

00:38:59,200 --> 00:39:01,370
it promises the IO has completed.

00:39:01,370 --> 00:39:04,070
But that actually is completely decoupled from whether

00:39:04,070 --> 00:39:06,527
or not that write is persistent on disk.

00:39:06,527 --> 00:39:09,030
(group laughing)

00:39:09,030 --> 00:39:10,366
- [Guy] Yeah, a very key point.

00:39:10,366 --> 00:39:11,199
- Yeah. (group laughing)

00:39:11,199 --> 00:39:12,910
- This is a Linux reality, right?

00:39:12,910 --> 00:39:15,060
- [Man] Yes, yeah, and very few people talk about

00:39:15,060 --> 00:39:16,560
this when they talk about direct IO because

00:39:16,560 --> 00:39:20,452
it's really a way to control the page cache in the Kernel,

00:39:20,452 --> 00:39:25,452
and very much not have anything to do with whether

00:39:25,710 --> 00:39:28,410
or not it's actually persistent on disk.

00:39:28,410 --> 00:39:30,550
So you need to fsync or osync

00:39:30,550 --> 00:39:34,470
or do a barrier or what you have to do completely depends

00:39:34,470 --> 00:39:35,910
on which file system you're writing to,

00:39:35,910 --> 00:39:37,950
but you have to do something else.

00:39:37,950 --> 00:39:40,100
- You have no idea the amount of emails

00:39:40,100 --> 00:39:43,357
I get about buggy devices are losing data.

00:39:43,357 --> 00:39:44,920
(group laughing)

00:39:44,920 --> 00:39:48,360
And you dig and that guy is doing fsync with it.

00:39:48,360 --> 00:39:51,580
- If you are device vendor

00:39:51,580 --> 00:39:53,330
and you've got many emails about that,

00:39:53,330 --> 00:39:56,340
why don't you, I don't know, better to debug the cache

00:39:56,340 --> 00:39:57,920
to write down if anything happens.

00:39:57,920 --> 00:40:00,004
- [Man] Well that's what we tell customers.

00:40:00,004 --> 00:40:02,123
Disable the write cache, do fsync,

00:40:02,123 --> 00:40:03,794
do something because the writes--

00:40:03,794 --> 00:40:06,562
- No, but if you put in any of--

00:40:06,562 --> 00:40:09,162
- [Man] Because that's what NVRAM would do.

00:40:09,162 --> 00:40:11,261
Battery by controller's (mumbles).

00:40:11,261 --> 00:40:12,917
So you can put it in front of the disk instead

00:40:12,917 --> 00:40:14,196
of being this response better.

00:40:14,196 --> 00:40:15,540
- [Guy] More expensive or less performance?

00:40:15,540 --> 00:40:16,790
- No, if you put a better in the cache,

00:40:16,790 --> 00:40:18,569
it's not make less performance, it's just...

00:40:18,569 --> 00:40:20,302
- Sorry. - If you put a battery on it,

00:40:20,302 --> 00:40:24,060
the power won't disappear immediately.

00:40:24,060 --> 00:40:26,160
It's not making this less performance.

00:40:26,160 --> 00:40:27,400
- [Man] No, but it's more expensive,

00:40:27,400 --> 00:40:28,290
more work. - Yeah, it's more expensive,

00:40:28,290 --> 00:40:30,003
I understand. - More keeps,

00:40:30,003 --> 00:40:31,003
more everything.

00:40:31,003 --> 00:40:33,904
- [Man] I understand it's more expensive so I just was...

00:40:33,904 --> 00:40:36,562
- And traditionally they tried it

00:40:36,562 --> 00:40:39,080
so people would put battery back cache

00:40:39,080 --> 00:40:40,939
so they had enough time to try

00:40:40,939 --> 00:40:41,772
and keep the medium spinning

00:40:41,772 --> 00:40:44,640
and the straight CDs versus Ts, it's less of a problem.

00:40:44,640 --> 00:40:46,820
And it was more expensive and nobody wanted

00:40:46,820 --> 00:40:49,310
it because they could buy an NVRAM RAID controller

00:40:49,310 --> 00:40:51,290
- Okay. - and put it in front

00:40:51,290 --> 00:40:52,220
of it for cheaper than what

00:40:52,220 --> 00:40:54,190
the four disks behind it cost. - Okay.

00:40:54,190 --> 00:40:56,160
Okay. - So it was exactly

00:40:56,160 --> 00:40:56,993
a cost trade off.

00:40:56,993 --> 00:40:58,446
- Yes, exactly.

00:40:58,446 --> 00:41:00,025
So I've heard about this earlier,

00:41:00,025 --> 00:41:01,310
I haven't heard about this for years

00:41:01,310 --> 00:41:03,320
and I was almost just wondered about trade-offs,

00:41:03,320 --> 00:41:05,080
if many people are complaining,

00:41:05,080 --> 00:41:06,400
but it turns out it's still more expensive.

00:41:06,400 --> 00:41:08,150
They complain but they still don't want it

00:41:08,150 --> 00:41:09,060
- Yeah. - Okay.

00:41:10,020 --> 00:41:10,853
All right.

00:41:12,390 --> 00:41:16,593
- So almost out of time so is there any final conclusion

00:41:16,593 --> 00:41:19,715
that we could (speaking off mic).

00:41:19,715 --> 00:41:21,023
- Well, for me, the conclusion would be

00:41:21,023 --> 00:41:24,510
to really try the-- - IO (speaking off mic).

00:41:24,510 --> 00:41:27,104
- IO-uring, yes, IO-uring. (man speaking off mic)

00:41:27,104 --> 00:41:28,479
- Yeah, concluding the topic

00:41:28,479 --> 00:41:29,312
of discussion. - And see if it can do all

00:41:30,198 --> 00:41:31,669
that then the problem solved. - Okay.

00:41:31,669 --> 00:41:34,419
(group clapping)

00:41:42,098 --> 00:41:45,348
(men speaking off mic)

00:41:46,847 --> 00:41:50,495
- [Guy] Also going to get through here see the completion.

00:41:50,495 --> 00:41:51,781
- [Man] Sorry, what's this idea of?

00:41:51,781 --> 00:41:52,614
- So to... - No idea.

00:41:52,614 --> 00:41:55,527
No idea, sorry. (men whispering)

00:41:55,527 --> 00:41:59,940
Trust me, I only work here. (laughs)

00:41:59,940 --> 00:42:02,260
Okay, yeah, our talker is, yeah.

00:42:03,213 --> 00:42:07,252
- [Man] Groups of IOs.

00:42:07,252 --> 00:42:09,455
- There's only one. - Sure.

00:42:09,455 --> 00:42:12,872
(men whispering off mic)

00:42:20,856 --> 00:42:25,856
- [Man] Okay.

00:42:36,020 --> 00:42:41,020
- So here again, it will be just one slide.

00:42:41,612 --> 00:42:46,612
Invitation for discussions, in fact, InnoDB design it.

00:42:49,420 --> 00:42:54,040
So first thing, the DB's using by default 16K page writes.

00:42:54,040 --> 00:42:56,950
Okay, and Linux has 4K writes.

00:42:56,950 --> 00:43:01,950
So many times I hear them 4K writes are atomic on Linux.

00:43:03,424 --> 00:43:05,890
(group laughing)

00:43:05,890 --> 00:43:10,080
So last time it was huge discussion with Kernel developers.

00:43:10,080 --> 00:43:11,190
No, you have to do it.

00:43:11,190 --> 00:43:13,940
So okay, I do it, but if you're on your election

00:43:13,940 --> 00:43:15,849
with your money so.

00:43:15,849 --> 00:43:18,730
So InnoDB has these solution double writes.

00:43:18,730 --> 00:43:21,630
In fact, we write in each page twice,

00:43:21,630 --> 00:43:26,630
so we have dedicated file when we write first the page,

00:43:26,950 --> 00:43:30,350
if it was okay then write to the normal place,

00:43:30,350 --> 00:43:32,070
and if something crash it in the middle

00:43:32,070 --> 00:43:35,140
so we know then we can recover from the first write.

00:43:35,140 --> 00:43:36,940
Or just discover the write.

00:43:36,940 --> 00:43:39,940
It never happens because it was not committed

00:43:39,940 --> 00:43:41,090
and then rewrite again.

00:43:41,990 --> 00:43:46,230
So the problem is, and we do twice more,

00:43:46,230 --> 00:43:50,650
IO write operations, so all people here in flash storage,

00:43:50,650 --> 00:43:54,120
they are unhappy because it will die twice faster, right?

00:43:55,430 --> 00:43:58,740
So in the past Fusion-IO deliberate solution

00:43:58,740 --> 00:44:02,340
but unfortunately it was totally proprietary.

00:44:02,340 --> 00:44:05,980
So it, well, of course drive was made by Fusion

00:44:05,980 --> 00:44:10,880
and it's fine, so they implemented their own driver,

00:44:10,880 --> 00:44:13,245
and own file system, which can guarantee

00:44:13,245 --> 00:44:17,810
then every write, whatever says, it will be atomic.

00:44:18,790 --> 00:44:21,430
So by implementing all this layer,

00:44:21,430 --> 00:44:23,370
unfortunately it was closed source

00:44:23,370 --> 00:44:26,600
and when Fusion-IO was acquired by some disks

00:44:26,600 --> 00:44:28,980
and some disks was acquired by,

00:44:28,980 --> 00:44:31,470
I don't know how many position it was,

00:44:31,470 --> 00:44:33,080
but it just stop it.

00:44:34,180 --> 00:44:39,180
At least there was a patch said about atomic flag for files.

00:44:40,360 --> 00:44:44,240
This stayed for six years now and still not in upstream.

00:44:45,501 --> 00:44:50,501
If one day it will come, so at least we can bypass all

00:44:51,020 --> 00:44:54,730
this storage, not twice, write twice every time,

00:44:54,730 --> 00:44:57,310
and just guarantee then if we use this option,

00:44:57,310 --> 00:44:59,310
then it's going up.

00:44:59,310 --> 00:45:02,300
So any expectation I need feedback on this so...

00:45:04,916 --> 00:45:09,450
- It sounds like you're making it a Kernel issue

00:45:09,450 --> 00:45:14,450
but how could you support anything that, well,

00:45:15,986 --> 00:45:19,880
we should probably also define a bit more what you mean

00:45:19,880 --> 00:45:24,270
by atomic but what I want to say is without device support

00:45:24,270 --> 00:45:27,160
and specify stuff at the device level,

00:45:27,160 --> 00:45:29,740
there is no way you can make that work.

00:45:29,740 --> 00:45:32,150
- You see, right now I am in direct discussion

00:45:32,150 --> 00:45:33,850
with Intel guys on Optane.

00:45:33,850 --> 00:45:36,888
They say we can do it by device but Linux can not do.

00:45:36,888 --> 00:45:39,450
And then you have channel in China,

00:45:39,450 --> 00:45:42,820
they have, we do it, it's on device and it's guaranteed.

00:45:42,820 --> 00:45:45,260
And I ask him guys how you can do it

00:45:45,260 --> 00:45:47,220
if Linux is not supported.

00:45:47,220 --> 00:45:49,620
So they claim they have atomic writes

00:45:50,500 --> 00:45:53,140
but who can believe it?

00:45:55,030 --> 00:45:58,000
- And, I think one of the comments you made is,

00:45:58,000 --> 00:46:00,380
what does he mean by atomic and I think it's,

00:46:00,380 --> 00:46:02,550
I wanna make sure this is all completed

00:46:02,550 --> 00:46:04,200
- Yeah. - or it hasn't completed

00:46:04,200 --> 00:46:05,570
by the time it returns. - Exactly.

00:46:05,570 --> 00:46:07,890
All our (mumbles), in fact-- - And that's part

00:46:07,890 --> 00:46:08,723
of the problem.

00:46:08,723 --> 00:46:11,970
- If I want to write 164k in one single

00:46:11,970 --> 00:46:13,500
IO operation, just... - Yep.

00:46:13,500 --> 00:46:15,840
And again, that can't happen without device support

00:46:15,840 --> 00:46:19,950
because when it gets scheduled it's four different sectors

00:46:19,950 --> 00:46:22,400
on the disk and different sectors.

00:46:22,400 --> 00:46:24,110
This one gets written, this one gets written,

00:46:24,110 --> 00:46:25,670
off we go, okay?

00:46:25,670 --> 00:46:28,707
That means you can accept all of them written

00:46:28,707 --> 00:46:30,806
but you'll never be able to get the,

00:46:30,806 --> 00:46:32,770
or none of them, written part of it.

00:46:33,630 --> 00:46:36,243
- You see, it's flash storage, right?

00:46:36,243 --> 00:46:38,370
And on flash storage, we always write in new blocks.

00:46:38,370 --> 00:46:39,370
We are not re-write.

00:46:40,505 --> 00:46:42,550
So you always can say then it was finished totally

00:46:42,550 --> 00:46:45,680
or you just come back, right?

00:46:46,530 --> 00:46:49,330
- And the same with other smarter storage systems.

00:46:49,330 --> 00:46:51,600
Like if you have some network attached storage,

00:46:51,600 --> 00:46:53,940
the other side probably can't provide that on

00:46:53,940 --> 00:46:58,540
its own level because it has enough battery cache date RAM

00:46:58,540 --> 00:47:00,600
and your end that can't or

00:47:00,600 --> 00:47:04,630
that can provide atomic writes over a larger sector.

00:47:04,630 --> 00:47:06,950
I said like, a lot of hardware provides that

00:47:06,950 --> 00:47:09,740
and a lot of the cloud-type block storage

00:47:09,740 --> 00:47:11,060
also provides that.

00:47:11,060 --> 00:47:12,380
So it's not impossible.

00:47:13,710 --> 00:47:15,530
- I just want to clarify that everybody excepts

00:47:15,530 --> 00:47:18,960
it requires device support, but what do mean

00:47:18,960 --> 00:47:21,966
is saying that even when devices do support it,

00:47:21,966 --> 00:47:24,560
(mumbles) doesn't allow application to use that.

00:47:24,560 --> 00:47:26,710
- [Man] Okay, so just to clarify.

00:47:26,710 --> 00:47:30,910
atomic, you mean, the data is all in or nothing is in?

00:47:30,910 --> 00:47:31,743
- [Guy] Yeah.

00:47:31,743 --> 00:47:32,576
- [Man] It's not about completion.

00:47:32,576 --> 00:47:33,510
- Yeah, exactly.

00:47:33,510 --> 00:47:34,343
- Okay.

00:47:35,369 --> 00:47:37,300
- [Man] It's not just that the data is all

00:47:37,300 --> 00:47:40,300
in or nothing is in, it's the data is all in or

00:47:40,300 --> 00:47:44,050
the old data is completely intact and unchanged

00:47:44,050 --> 00:47:44,900
on disk. - Exactly.

00:47:44,900 --> 00:47:49,060
- [Man] And so on the file system, so Btrfs at least has

00:47:49,060 --> 00:47:51,870
the ability to do this because it's copy-on-write.

00:47:51,870 --> 00:47:54,580
The XFS developers have, I think,

00:47:54,580 --> 00:47:57,577
really serious efforts to add interfaces.

00:47:57,577 --> 00:48:01,030
To do this, I believe, via the XFS copy-on-write machinery,

00:48:01,030 --> 00:48:02,790
I haven't looked at it closely,

00:48:02,790 --> 00:48:04,760
there are performance implications to that, right?

00:48:04,760 --> 00:48:07,205
Because, you know you're doing copy-on-write

00:48:07,205 --> 00:48:08,038
at the file system level and it's not

00:48:09,241 --> 00:48:10,934
the same as writing in place.

00:48:10,934 --> 00:48:15,934
The Fusion-IO implementation, I don't expect to see,

00:48:16,901 --> 00:48:21,310
largely because there just were a number

00:48:21,310 --> 00:48:24,360
of corner cases that it didn't deal with,

00:48:24,360 --> 00:48:28,201
that I think, were certainly acceptable in

00:48:28,201 --> 00:48:31,150
a really specific data center environment where you're able

00:48:31,150 --> 00:48:34,110
to curate exactly what's going on and make

00:48:34,110 --> 00:48:36,430
a lot of rules for how the files are used,

00:48:36,430 --> 00:48:40,340
but I don't think it's a great general purpose solution.

00:48:40,340 --> 00:48:42,320
And I can say that because I wrote it.

00:48:42,320 --> 00:48:46,320
- Yeah, you said, BTRFS so I tested so

00:48:46,320 --> 00:48:49,530
it looks promising but performance is like this.

00:48:49,530 --> 00:48:52,328
- [Man] Yeah, Btrfs is not meant for this workload, right?

00:48:52,328 --> 00:48:55,195
- ZFS is too aggressive so it's good

00:48:55,195 --> 00:48:59,720
but this copy-on-the-write story, and you do much more,

00:48:59,720 --> 00:49:02,890
you do twice, three times more IO because by design

00:49:02,890 --> 00:49:06,270
it will recheck every time than it was really written,

00:49:06,270 --> 00:49:08,320
and so you can get the same performance

00:49:08,320 --> 00:49:10,540
but you need three times more IO traffic.

00:49:10,540 --> 00:49:11,847
- [Man] Right.

00:49:11,847 --> 00:49:13,649
- And again, exams, so people complain

00:49:13,649 --> 00:49:14,620
and then you still need to rebuild.

00:49:14,620 --> 00:49:17,690
In one year, you still rebuild your old file system when

00:49:17,690 --> 00:49:20,810
it's 100 terabytes so not easy, right?

00:49:20,810 --> 00:49:23,030
- And yeah, so just to agree with you,

00:49:23,030 --> 00:49:25,350
Btrfs will not work well for this,

00:49:25,350 --> 00:49:27,151
at least for the MySQL workload.

00:49:27,151 --> 00:49:30,980
XFS, certainly that is, it's bread and butter, right?

00:49:30,980 --> 00:49:33,580
I don't know if there are copy-on-write supports works well

00:49:33,580 --> 00:49:36,040
in this configuration or not.

00:49:36,040 --> 00:49:37,545
That would be a question for them.

00:49:37,545 --> 00:49:39,890
- Question, SQLite doesn't have the choice

00:49:39,890 --> 00:49:42,910
of what file system it's using, we just use whatever

00:49:42,910 --> 00:49:44,310
the application hands to us.

00:49:45,165 --> 00:49:47,110
How can we find out whether or not this is supported?

00:49:48,010 --> 00:49:49,690
- [Man] Well so, right now that answer

00:49:49,690 --> 00:49:52,494
is super easy because it's not.

00:49:52,494 --> 00:49:55,244
(group laughing)

00:49:58,060 --> 00:50:00,850
So nobody exports O-atomic right now.

00:50:00,850 --> 00:50:03,600
I believe there are some patches in the works

00:50:03,600 --> 00:50:07,147
that I certainly expect to happen relatively soon.

00:50:07,147 --> 00:50:10,260
- [Guy] F2FS, we can do it F2FS.

00:50:10,260 --> 00:50:12,140
- [Man] Okay, great.

00:50:12,140 --> 00:50:14,230
- And we just got some funky IOCTLs

00:50:14,230 --> 00:50:16,580
that have to call to figure out whether or not we can do it.

00:50:17,729 --> 00:50:19,900
But it'd be nice if we had some kind of standard interface

00:50:19,900 --> 00:50:23,200
that would say oh, we support this capability or we don't.

00:50:23,200 --> 00:50:24,690
- [Man] Yeah, it would, I agree.

00:50:28,340 --> 00:50:31,500
- So in fact, right now, you have a copy on

00:50:31,500 --> 00:50:33,360
the write solution or something

00:50:33,360 --> 00:50:36,343
if you incremental like (mumbles) on what to be doing.

00:50:36,343 --> 00:50:39,090
So you are more safe on these but...

00:50:40,570 --> 00:50:41,830
But then you have more writes.

00:50:41,830 --> 00:50:45,650
- [Man] So a solution that is important, purely by software,

00:50:45,650 --> 00:50:48,570
the file system level would be fine to you guys,

00:50:48,570 --> 00:50:50,870
you don't require anything on the device side?

00:50:51,900 --> 00:50:52,733
- [Guy] Say that again.

00:50:52,733 --> 00:50:54,320
- Just to be sure.

00:50:54,320 --> 00:50:55,210
- [Man] Say again.

00:50:56,233 --> 00:50:58,920
- Do you want that both only at the file system level,

00:50:58,920 --> 00:51:00,590
is it fine with such association,

00:51:00,590 --> 00:51:03,140
or are you also want something at the device level?

00:51:05,894 --> 00:51:10,610
- [Man] I think MySQL is unique because they have a,

00:51:10,610 --> 00:51:14,570
it's actually possible with MySQL for O-atomic to be useful.

00:51:14,570 --> 00:51:19,570
Like they they need 16K blocks, that's bigger than 4K

00:51:19,620 --> 00:51:22,690
but it's small enough that it's reasonable for us

00:51:22,690 --> 00:51:24,750
to ask the hardware to do this.

00:51:24,750 --> 00:51:27,100
And for whatever reason, us asking

00:51:27,100 --> 00:51:29,420
the hardware people has never worked out.

00:51:29,420 --> 00:51:31,230
Where the hardware people say oh, yeah,

00:51:31,230 --> 00:51:33,280
we can do 16K, what else do you need?

00:51:33,280 --> 00:51:34,680
And then the file system people say well,

00:51:34,680 --> 00:51:36,818
actually, how about Infinite?

00:51:36,818 --> 00:51:39,507
(group laughing)

00:51:39,507 --> 00:51:41,040
- That's right, basically.

00:51:41,040 --> 00:51:42,670
- Just to be clear, the same exists for,

00:51:42,670 --> 00:51:45,450
true for Postgres, we don't use a double byte buffer

00:51:45,450 --> 00:51:47,875
but otherwise we have the same problem

00:51:47,875 --> 00:51:49,242
and our blocks are 8K by default.

00:51:49,242 --> 00:51:51,390
So it's very similar story.

00:51:51,390 --> 00:51:52,760
- [Man] Totally reasonable. (speaking off mic)

00:51:52,760 --> 00:51:55,800
- So you see, the story is pretty amazing because

00:51:56,863 --> 00:52:00,070
so right now, you need historical double right buffer

00:52:00,070 --> 00:52:03,520
on InnoDB, it was just in the system space,

00:52:03,520 --> 00:52:04,830
now it can be anywhere.

00:52:04,830 --> 00:52:08,530
So in fact, what was amazing and 15 years ago,

00:52:08,530 --> 00:52:11,460
Seagate delivered storage which

00:52:11,460 --> 00:52:15,300
was having persistent memory on board.

00:52:15,300 --> 00:52:17,210
And in fact, today we could be able

00:52:17,210 --> 00:52:20,977
to use this solution just to place double writing

00:52:22,208 --> 00:52:25,510
to this persistent memory which will be extremely fast

00:52:25,510 --> 00:52:27,880
and safe because it's on the same device,

00:52:27,880 --> 00:52:30,330
and continue to write to the flash device.

00:52:30,330 --> 00:52:33,920
Unfortunately, nobody wanted this and there is no way,

00:52:33,920 --> 00:52:37,180
so we got the device but there was even no driver for this.

00:52:37,180 --> 00:52:40,070
So this kind of solution can work.

00:52:40,070 --> 00:52:42,800
So we tested also Intel per system memory

00:52:42,800 --> 00:52:45,210
so which was reported by Google the same way,

00:52:45,210 --> 00:52:48,690
so we wanted to place double write on this device.

00:52:48,690 --> 00:52:53,690
Unfortunately, Linux today, when you run file system

00:52:53,860 --> 00:52:58,740
on persistent memory, it given you worst performance

00:52:58,740 --> 00:53:00,970
and if you use Optane like SSD.

00:53:01,920 --> 00:53:04,050
So it's again, about the software.

00:53:04,050 --> 00:53:07,190
So if we place double write on the system memory,

00:53:07,190 --> 00:53:09,370
we have worst performance, if you place

00:53:09,370 --> 00:53:12,060
it on just another Optane device.

00:53:12,060 --> 00:53:14,420
So well, again, about software.

00:53:14,420 --> 00:53:16,390
So once it will be improve it

00:53:16,390 --> 00:53:18,340
and probably story will be better word.

00:53:19,794 --> 00:53:23,133
- But you will not get rid of double writes with (mumbles).

00:53:24,698 --> 00:53:28,060
It's always written somewhere.

00:53:28,060 --> 00:53:29,729
Have you seen the device on-- - I agree, but you see,

00:53:29,729 --> 00:53:34,150
my point here is in any case on flash storage,

00:53:34,150 --> 00:53:36,340
we still write new blocks, okay?

00:53:37,300 --> 00:53:40,000
You not rewrite the same pages, right?

00:53:40,000 --> 00:53:42,600
So you can say, you can always roll back, in fact.

00:53:43,434 --> 00:53:44,630
(man speaking off mic)

00:53:44,630 --> 00:53:46,810
Well, but you can roll back, what I mean.

00:53:46,810 --> 00:53:48,530
So if write was not finished,

00:53:48,530 --> 00:53:49,920
you can say it was not finished

00:53:49,920 --> 00:53:52,750
and just your previous image, just write image, right?

00:53:55,672 --> 00:53:59,172
- Yes, just that in this we standardize it

00:54:00,229 --> 00:54:03,390
'cause what was on that block you're now aware?

00:54:03,390 --> 00:54:05,240
And now want to lose that one because

00:54:06,425 --> 00:54:07,752
there was another one right there.

00:54:07,752 --> 00:54:09,120
- This is the reason why Oracle database don't see all

00:54:09,120 --> 00:54:12,330
these problems because they create their own layer,

00:54:13,486 --> 00:54:14,680
they don't use file systems, they use ASM,

00:54:14,680 --> 00:54:17,329
doing everything, zero device, and that's all.

00:54:17,329 --> 00:54:20,930
And we are too kind with users, right?

00:54:20,930 --> 00:54:22,510
People want to see files. - I mean,

00:54:22,510 --> 00:54:25,230
the Oracle file, Oracle just pretends

00:54:25,230 --> 00:54:27,672
if 64K will be adopted.

00:54:27,672 --> 00:54:28,520
(group laughing)

00:54:28,520 --> 00:54:31,210
So yeah, 64K is atomic, I've declared it.

00:54:31,210 --> 00:54:33,420
- You know, if we close that source software,

00:54:33,420 --> 00:54:34,620
it's so easy, right?

00:54:34,620 --> 00:54:36,310
You just believe slides and that

00:54:37,183 --> 00:54:38,200
so you can not check, right?

00:54:38,200 --> 00:54:40,071
So... (group laughing)

00:54:40,071 --> 00:54:43,726
- So you made a comment about persistent memory

00:54:43,726 --> 00:54:46,292
and of the (mumbles) one way or the another.

00:54:46,292 --> 00:54:50,252
And that brings the question, what more do you...

00:54:50,252 --> 00:54:51,950
(man speaking off mic)

00:54:51,950 --> 00:54:54,675
Yeah, sorry.

00:54:54,675 --> 00:54:57,485
Which brings up a question, instead of trying to push

00:54:57,485 --> 00:55:00,980
the support down to the device,

00:55:00,980 --> 00:55:03,010
why don't we try and incorporate persistent memory

00:55:03,010 --> 00:55:05,300
as a staging area, because what we really want is

00:55:05,300 --> 00:55:08,040
a fast write that we can walk away from

00:55:08,040 --> 00:55:10,010
and then drain it back down to the device.

00:55:10,010 --> 00:55:12,610
So that it either gets to the device this time

00:55:12,610 --> 00:55:14,490
or it gets the device next time,

00:55:14,490 --> 00:55:15,580
but we either finish the write

00:55:15,580 --> 00:55:17,390
or we don't end something fast,

00:55:17,390 --> 00:55:20,030
and then we can manage draining it to the media.

00:55:20,030 --> 00:55:23,790
So take generic interface, persistent memory,

00:55:23,790 --> 00:55:25,950
make that available to all of the file systems

00:55:25,950 --> 00:55:28,570
so the device spenders aren't stuck with that.

00:55:28,570 --> 00:55:32,710
- Yeah, but that's why if you can have device which has,

00:55:32,710 --> 00:55:34,790
you say this is double writes only, it's very small,

00:55:34,790 --> 00:55:37,170
it's just one megabyte, let's say, in fact.

00:55:37,170 --> 00:55:41,060
So, you know, if somebody need to buy additional device

00:55:41,060 --> 00:55:45,090
for one megabyte of data or, you know, we look silly.

00:55:45,090 --> 00:55:48,110
So if it's integrated on the cheap or on the drive

00:55:48,110 --> 00:55:51,330
and we just address it and it's just on the same storage,

00:55:51,330 --> 00:55:52,660
it works. - They're starting to do

00:55:52,660 --> 00:55:54,650
NVDEMS that you could use as memory,

00:55:54,650 --> 00:55:56,850
and I'm not sure if they're gonna be successful or not

00:55:58,034 --> 00:55:59,690
but that was one of the things we looked at, was just make--

00:55:59,690 --> 00:56:02,100
- But that's an interesting discussion because

00:56:02,100 --> 00:56:04,161
it's not just here to the start,

00:56:04,161 --> 00:56:07,213
it's (speaking off mic).

00:56:07,213 --> 00:56:08,103
And then have a whole portal there.

00:56:09,940 --> 00:56:12,700
- [Man] But I mean, it's also the case that Linux needs

00:56:12,700 --> 00:56:14,550
to support it at some point because it's not just on

00:56:14,550 --> 00:56:16,490
the storage level, because if you have like

00:56:17,717 --> 00:56:20,530
a network, like a cloud-type product, storage product,

00:56:20,530 --> 00:56:22,560
all they support that today.

00:56:22,560 --> 00:56:26,230
You can do atomic writes over Google's block device storage.

00:56:26,230 --> 00:56:28,630
They support that and they can just emulate it on

00:56:28,630 --> 00:56:31,510
the other side because they support it

00:56:31,510 --> 00:56:32,610
but Linux can't support it,

00:56:32,610 --> 00:56:35,650
if you use anything larger than 4K.

00:56:35,650 --> 00:56:38,900
So that side also needs to approve and it's

00:56:38,900 --> 00:56:41,600
a really large improvement to not do it all the writes

00:56:41,600 --> 00:56:44,051
in that case because we don't send it over

00:56:44,051 --> 00:56:47,600
the wire which literally produces the volume by two.

00:56:48,565 --> 00:56:49,398
- I agree.

00:56:51,080 --> 00:56:55,580
I was just pointing out that if we want that support,

00:56:55,580 --> 00:56:57,960
having it, I'm all for it, but if we push

00:56:57,960 --> 00:57:02,590
it down as down to the device,

00:57:02,590 --> 00:57:04,700
the discussion is not just Linux anymore,

00:57:04,700 --> 00:57:07,030
it's then to TCTMGE, it's much bigger,

00:57:07,030 --> 00:57:08,180
it's multi-year effort.

00:57:09,110 --> 00:57:13,210
And so do we really need atomicity of

00:57:13,210 --> 00:57:15,360
the writes supporting down to the device,

00:57:15,360 --> 00:57:18,790
or can we do things like adding NGO-RAM

00:57:18,790 --> 00:57:22,550
or copy-on-write or whatever other software method,

00:57:22,550 --> 00:57:25,601
just stop at Linux level to the atomic writes.

00:57:25,601 --> 00:57:29,867
- Yeah, because we see there was also another proposal just

00:57:29,867 --> 00:57:34,220
to use (mumbles) file system which will guarantee you,

00:57:34,220 --> 00:57:36,580
right, and again, it was not confirmed, so.

00:57:39,442 --> 00:57:40,275
- [Man] Yeah, the thing was that I overestimated it so.

00:57:41,486 --> 00:57:42,436
- Yeah, exactly so.

00:57:44,260 --> 00:57:47,147
So I am also curious how this (mumbles) channel

00:57:49,430 --> 00:57:51,300
is working with (mumbles) right, so,

00:57:52,778 --> 00:57:54,280
and they're claiming to have atomic writes.

00:57:54,280 --> 00:57:55,930
And I discuss it with these guys.

00:57:57,299 --> 00:57:58,967
How do you do it if you can not guarantee

00:57:58,967 --> 00:58:00,890
them all there so atomic, right?

00:58:00,890 --> 00:58:03,946
Or probably in China they have different style while paying.

00:58:03,946 --> 00:58:06,678
(man speaking off mic)

00:58:06,678 --> 00:58:07,511
- [Man] Comment here.

00:58:07,511 --> 00:58:09,240
- Yeah, I just had one question which is if

00:58:09,240 --> 00:58:12,190
the device vendor is going to guarantee

00:58:12,190 --> 00:58:16,500
that your writes are atomic in some sense,

00:58:16,500 --> 00:58:19,160
is it the case then they're just saying user direct,

00:58:20,150 --> 00:58:23,280
is that all that they are saying that

00:58:23,280 --> 00:58:26,050
the device can do it but Linux

00:58:26,050 --> 00:58:28,222
really isn't exposing it. - No, no, no.

00:58:28,222 --> 00:58:31,945
- [Man] So there's nothing there, right?

00:58:31,945 --> 00:58:34,324
If you go to this standard, if you go to (speaking off mic).

00:58:34,324 --> 00:58:35,582
- [Questioner] Yeah.

00:58:35,582 --> 00:58:37,010
- [Man] Will relatively.

00:58:37,010 --> 00:58:39,440
- I mean from the point of like a cloud vendor

00:58:39,440 --> 00:58:42,090
or whatever offering network attached storage.

00:58:42,090 --> 00:58:44,920
Is the cloud vendor saying, you know,

00:58:44,920 --> 00:58:49,630
we've got mat, oh sorry, okay, yeah.

00:58:49,630 --> 00:58:52,550
- I know that Google says on their special verified Kernel

00:58:52,550 --> 00:58:55,905
and they and over their block device, whatever,

00:58:55,905 --> 00:58:58,810
for their block device, they support it

00:58:58,810 --> 00:59:01,680
and you better hope that they're right and...

00:59:02,611 --> 00:59:04,032
(group laughing)

00:59:04,032 --> 00:59:06,620
Like the guarantee, I heard them talk about it,

00:59:06,620 --> 00:59:09,840
it was very, I guess you hope.

00:59:12,161 --> 00:59:13,300
- It's like a record.

00:59:13,300 --> 00:59:14,280
Wishful thinking.

00:59:17,140 --> 00:59:19,510
- [Man] Yeah so O-direct comes into the equation

00:59:19,510 --> 00:59:21,760
just because it is the easiest way

00:59:21,760 --> 00:59:25,750
for an application to give a single chunk of IO

00:59:25,750 --> 00:59:28,960
to the Kernel and have a path where we expect

00:59:28,960 --> 00:59:31,480
it to go down in a 16K unit.

00:59:32,400 --> 00:59:34,070
It actually doesn't always happen,

00:59:34,070 --> 00:59:35,730
there are nasty corner cases in there

00:59:35,730 --> 00:59:38,220
that are really hard to get rid of,

00:59:38,220 --> 00:59:40,690
but O-direct is the easiest way

00:59:40,690 --> 00:59:45,610
for an application to talk to a block device.

00:59:45,610 --> 00:59:50,420
- So you see, on ZFS Appliance, guys implementing this,

00:59:50,420 --> 00:59:53,458
and unfortunately all, well,

00:59:53,458 --> 00:59:57,750
people didn't buy it enough so it's going down, right?

00:59:57,750 --> 00:59:58,990
But it was possible to do

00:59:58,990 --> 01:00:02,020
and especially they even implemented an FS interface

01:00:02,020 --> 01:00:03,780
which could be direct NFS,

01:00:03,780 --> 01:00:05,860
and you can send whatever block you wanted to,

01:00:05,860 --> 01:00:07,670
it will be atomic, just because

01:00:07,670 --> 01:00:09,520
it was ZFS in the background.

01:00:09,520 --> 01:00:12,760
- [Man] They also had an SSD in front of the array.

01:00:12,760 --> 01:00:14,970
- Yeah, so in fact, we have everything

01:00:14,970 --> 01:00:16,430
and we nothing, in fact, right?

01:00:16,430 --> 01:00:20,320
- Sorry, I was part of that group so. (laughs)

01:00:20,320 --> 01:00:23,980
- So if I understand well, no expectation for atomic.

01:00:25,740 --> 01:00:27,700
- I think we've concluded, it's hard to do at

01:00:27,700 --> 01:00:31,080
the device level and it probably isn't cost-efficient

01:00:31,080 --> 01:00:32,530
to do it at the device level,

01:00:33,600 --> 01:00:35,070
but looking for a facility where you can manage something

01:00:35,070 --> 01:00:37,816
that's persistent and some thing's

01:00:37,816 --> 01:00:40,940
that not atomic on the way.

01:00:40,940 --> 01:00:43,380
Like Google is apparently done under the covers

01:00:43,380 --> 01:00:44,900
is probably worth thinking about

01:00:44,900 --> 01:00:46,900
and creating an interface for.

01:00:46,900 --> 01:00:48,400
- [Man] Well, and I would talk

01:00:49,262 --> 01:00:51,960
with XFS developers about their, I believe,

01:00:51,960 --> 01:00:55,000
I'm not 100% sure, but they're a copy-on-write-based,

01:00:55,000 --> 01:00:56,719
O-atomic plans.

01:00:56,719 --> 01:00:58,760
- [Guy] Christophe is coming tomorrow.

01:00:58,760 --> 01:01:00,606
- [Man] Christophe is coming tomorrow.

01:01:00,606 --> 01:01:01,640
It'll be a real thing.

01:01:01,640 --> 01:01:02,727
- [Presenter] ZFS you mean or...

01:01:02,727 --> 01:01:03,729
- [Man] XFS.

01:01:03,729 --> 01:01:04,731
- XFS? - Yes.

01:01:04,731 --> 01:01:05,564
- XFS has a copy-on-write mechanism.

01:01:05,564 --> 01:01:08,683
It's, like everything else on XFS,

01:01:08,683 --> 01:01:10,480
it really depends on a lot of the layout,

01:01:10,480 --> 01:01:11,430
whether it's gonna be performed or not,

01:01:11,430 --> 01:01:12,280
but it does work.

01:01:14,130 --> 01:01:16,510
- Well, we will speak about XFS just in 10 minutes so...

01:01:18,133 --> 01:01:21,290
- Yeah, gone a little over time with this one

01:01:21,290 --> 01:01:23,350
so straight into your next one.

01:01:23,350 --> 01:01:25,500
- Okay, so from my side, I will try

01:01:25,500 --> 01:01:27,740
to motivate storage vendors.

01:01:27,740 --> 01:01:29,860
We are working, so Intel is very motivated

01:01:29,860 --> 01:01:32,680
to put Optane with atomic writes.

01:01:34,345 --> 01:01:36,490
So if, let's say, if vendors will have it,

01:01:36,490 --> 01:01:38,090
what will be about Linux, right?

01:01:39,010 --> 01:01:42,140
So the next topic will be about XT4.

01:01:42,140 --> 01:01:44,070
So this is pretty amazing stories,

01:01:44,070 --> 01:01:49,070
in fact, historically, we always get better performance

01:01:49,220 --> 01:01:53,660
with MySQL on EXT4.

01:01:53,660 --> 01:01:57,120
XFS was reported good for any agenic workload,

01:01:57,120 --> 01:01:59,220
but when you start MySQL,

01:01:59,220 --> 01:02:02,130
it was always slower companion to XT4.

01:02:02,130 --> 01:02:06,800
So I saw them the same story for PostgreSQL guys as well,

01:02:06,800 --> 01:02:11,800
but the story started changing just recently.

01:02:12,180 --> 01:02:17,180
So in fact I was running my tests for close on my machine

01:02:17,240 --> 01:02:21,490
and I was surprised that so we have most

01:02:21,490 --> 01:02:25,430
of same configuration in the lab for all our guys,

01:02:25,430 --> 01:02:27,890
you know, for every benchmark testing.

01:02:27,890 --> 01:02:29,510
And I was surprised at this, it's exactly

01:02:29,510 --> 01:02:31,270
the same machine which was showing exactly

01:02:31,270 --> 01:02:33,240
the same result few months ago,

01:02:33,240 --> 01:02:35,760
started to show it twice worse result,

01:02:35,760 --> 01:02:38,250
and the reason was it just, guys,

01:02:38,250 --> 01:02:43,180
upgraded the Kernel, or this was a pretty amazing discovery

01:02:43,180 --> 01:02:45,580
which they bring back to Oracle Linux guys

01:02:45,580 --> 01:02:49,500
and we have so here you can can see this was

01:02:49,500 --> 01:02:53,340
the old Kernel, the blue line so it's Kernel three, 4.1.

01:02:54,325 --> 01:02:55,880
- [Man] All units.

01:02:55,880 --> 01:02:58,810
- [Presenter] And so, well this was Oracle Linux but--

01:02:58,810 --> 01:03:00,560
- [Man] What are the units?

01:03:00,560 --> 01:03:02,330
- [Presenter] Sorry, this is number

01:03:02,330 --> 01:03:04,040
of transactions per second.

01:03:04,040 --> 01:03:07,190
This is coming from one concurrent user,

01:03:07,190 --> 01:03:08,880
two, four and so on.

01:03:08,880 --> 01:03:11,730
So tier 1000, okay, so it's just growing.

01:03:11,730 --> 01:03:13,910
In fact, all graphs, and I will show you,

01:03:13,910 --> 01:03:15,010
they're all like this.

01:03:15,939 --> 01:03:17,280
So it's just depends per second

01:03:17,280 --> 01:03:21,440
and there's growing load from one user to 1000, 1024.

01:03:22,340 --> 01:03:25,830
- [Man] How do you know it's EXT4 that's progressing?

01:03:25,830 --> 01:03:28,257
Rather than something else it incurred?

01:03:28,257 --> 01:03:31,290
- [Presenter] Because we only use an EXT4,

01:03:31,290 --> 01:03:34,100
nothing else, so nothing reported around,

01:03:34,100 --> 01:03:37,320
it just cannot change it and with EXT4.

01:03:38,720 --> 01:03:41,570
And this is a story, when the latest one, Kernel Five,

01:03:42,410 --> 01:03:43,310
here and the same.

01:03:44,240 --> 01:03:46,680
The workload is IO-bound, so in fact,

01:03:46,680 --> 01:03:49,070
you don't see it if you have everything in memory,

01:03:49,070 --> 01:03:50,760
it will just write little bit,

01:03:50,760 --> 01:03:53,977
this is one we have a mix of freedom writes in parallel so--

01:03:56,630 --> 01:03:58,990
- [Man] What's the backing device?

01:03:58,990 --> 01:04:00,830
- In backup, it's Optane.

01:04:01,860 --> 01:04:05,030
So it's, you have a model.

01:04:05,030 --> 01:04:06,510
And also, this workload,

01:04:06,510 --> 01:04:09,950
it's not totally IO-bound too aggressively

01:04:09,950 --> 01:04:11,810
so you still have margins this storage,

01:04:11,810 --> 01:04:13,060
storage is not saturated.

01:04:15,892 --> 01:04:17,223
- [Man] All right.

01:04:17,223 --> 01:04:20,960
- So in fact, well, my impression is we hit some kind

01:04:20,960 --> 01:04:23,720
of saturation on the file system layer,

01:04:23,720 --> 01:04:27,740
I don't know how or how it happens.

01:04:27,740 --> 01:04:30,820
The problem isn't there is no instrumentation,

01:04:30,820 --> 01:04:34,370
no feedback, well, at least, yep?

01:04:35,632 --> 01:04:37,600
- [Man] Did you try and block (mumbling)?

01:04:37,600 --> 01:04:39,595
- Yes. - Do you think it's--

01:04:39,595 --> 01:04:40,580
- Yeah, exactly, so it's-- - Did you see

01:04:41,429 --> 01:04:42,485
a difference in stretcher?

01:04:42,485 --> 01:04:44,130
- So it's what Oracle Linux guys ask of me,

01:04:44,130 --> 01:04:46,390
so they give me this, give me this.

01:04:46,390 --> 01:04:48,970
So I suppose and then expect that I will never collect

01:04:48,970 --> 01:04:50,850
this and it will be fine.

01:04:50,850 --> 01:04:52,150
So I collected everything.

01:04:53,900 --> 01:04:55,810
No feedback, nothing to say.

01:04:55,810 --> 01:04:59,070
So yes, fine, we have data but nothing to say would happens.

01:05:00,061 --> 01:05:01,550
- Did you see a difference in the ordering

01:05:01,550 --> 01:05:06,130
of the spec provider or a difference in the patterns anyway?

01:05:06,130 --> 01:05:09,440
- I did not see any difference so--

01:05:09,440 --> 01:05:11,730
- [Man] So which Kernels were involved here?

01:05:11,730 --> 01:05:13,690
I'm sorry, I can't see the labels.

01:05:13,690 --> 01:05:15,320
- So in fact, Kernel Three,

01:05:15,320 --> 01:05:16,630
Kernel-- - 5.05.

01:05:16,630 --> 01:05:19,110
- Four-dot-one was still okay,

01:05:19,110 --> 01:05:21,540
and then after 4.14 something,

01:05:21,540 --> 01:05:22,790
it started to go down.

01:05:22,790 --> 01:05:24,882
- All right. - Multi-queue IO (mumbles).

01:05:24,882 --> 01:05:27,500
(speaking off mic)

01:05:27,500 --> 01:05:30,820
- So it's hard to guess from the graph.

01:05:32,300 --> 01:05:35,730
I would expect, I mean, so XFS in MySQL

01:05:35,730 --> 01:05:37,440
is what Facebook uses internally,

01:05:37,440 --> 01:05:41,230
we have certainly no performance problems on XFS with MySQL.

01:05:42,680 --> 01:05:45,400
I would expect them both to be roughly IO-bound

01:05:45,400 --> 01:05:46,930
to the speed of a device,

01:05:46,930 --> 01:05:48,840
plus or minus locking in the Kernel.

01:05:50,260 --> 01:05:52,630
And so, I would think, with configuration,

01:05:52,630 --> 01:05:53,930
they should both be equal.

01:05:54,942 --> 01:05:56,577
So it's certainly surprising to hear

01:05:56,577 --> 01:05:58,790
that one is better than the other.

01:05:58,790 --> 01:06:01,460
For this, my guess, just looking at the graph,

01:06:01,460 --> 01:06:03,880
is either the multi-queue stuff,

01:06:03,880 --> 01:06:07,540
or somewhere in the four between four-six

01:06:07,540 --> 01:06:10,890
and higher Kernels, extended four realized

01:06:10,890 --> 01:06:15,890
that their implementation of data ordered,

01:06:16,550 --> 01:06:18,710
so this is what the file systems do

01:06:18,710 --> 01:06:20,900
in order to make sure that we don't expose stale blocks

01:06:20,900 --> 01:06:23,950
on disk, I forget the exact Kernel version

01:06:23,950 --> 01:06:26,280
where they found the bug, but they realized that

01:06:26,280 --> 01:06:29,360
it was not actually ordering 100% of the time,

01:06:29,360 --> 01:06:33,030
so they added additional correctness which,

01:06:33,030 --> 01:06:37,138
not surprisingly, is often additional slowness so--

01:06:37,138 --> 01:06:40,100
- Why, why slower so it's... - Sure.

01:06:40,100 --> 01:06:42,010
Yeah, it should-- - Because it's correcter.

01:06:42,010 --> 01:06:44,877
- Yeah, it's... (group laughing)

01:06:44,877 --> 01:06:47,890
So you have options, actually, to disable mass.

01:06:47,890 --> 01:06:48,800
So you could write. - Yes.

01:06:48,800 --> 01:06:50,100
It's easily testable.

01:06:50,100 --> 01:06:51,690
- Oh. - Just test data write

01:06:51,690 --> 01:06:52,523
back versus--

01:06:53,890 --> 01:06:55,490
- Barrier equals zero, one, two.

01:06:57,472 --> 01:06:59,222
- Yeah. - But it could enable

01:06:59,222 --> 01:07:02,160
and then I will write back to (speaking off mic).

01:07:02,160 --> 01:07:04,990
- I think I always run and test with body of zero.

01:07:06,288 --> 01:07:07,640
- Whoa. - And with data equals

01:07:07,640 --> 01:07:11,620
write-back or data equals ordering?

01:07:11,620 --> 01:07:14,430
- I think with which it to what is by default now.

01:07:14,430 --> 01:07:16,272
- [Man] Yeah, so that is something

01:07:16,272 --> 01:07:18,110
that changed somewhere in the four series,

01:07:18,110 --> 01:07:20,208
I'm sorry, I forget where.

01:07:20,208 --> 01:07:25,208
This should be, it shouldn't be too hard to figure it out.

01:07:26,110 --> 01:07:27,740
Let's say that.

01:07:27,740 --> 01:07:29,814
- [Guy] But it's probably not going to come back.

01:07:29,814 --> 01:07:32,440
(group laughing - And I mean if it's...

01:07:32,440 --> 01:07:35,440
- You see, just a comment, the most painful

01:07:35,440 --> 01:07:37,380
for me here, just to see them,

01:07:37,380 --> 01:07:40,740
there is no instrumentation just to see what is wrong,

01:07:40,740 --> 01:07:42,981
there is no feedback from so.

01:07:42,981 --> 01:07:44,090
(man speaking off mic)

01:07:44,090 --> 01:07:45,310
- [Man] Yes.

01:07:45,310 --> 01:07:48,359
- Yeah but, well, what I mean then from five system layer,

01:07:48,359 --> 01:07:51,140
everybody use file system route

01:07:51,140 --> 01:07:54,150
so you should have some kind of instrumentation inside

01:07:54,150 --> 01:07:57,446
or I don't know from (mumbles), from whatever else,

01:07:57,446 --> 01:07:58,580
then you can see then we have some bad events

01:07:58,580 --> 01:07:59,670
or something here from-- - But you do,

01:07:59,670 --> 01:08:01,600
but that's all there. - Yeah, yeah we do.

01:08:01,600 --> 01:08:03,589
- Yeah, I think there are trace binds

01:08:03,589 --> 01:08:06,320
for VFS, CXT4 and the block layer, right?

01:08:08,542 --> 01:08:09,933
- [Man] But finding them is not trivial.

01:08:09,933 --> 01:08:10,935
- Yeah.

01:08:10,935 --> 01:08:11,768
- You should test everything your Kernel release

01:08:15,178 --> 01:08:17,995
in between because you can fast produce

01:08:17,995 --> 01:08:22,022
the multi-queue blockers, or a couple things

01:08:22,022 --> 01:08:24,130
there was a lot of performance issue then,

01:08:24,130 --> 01:08:27,140
shall we standardize in one as good as before,

01:08:27,140 --> 01:08:29,630
so 4.14 may still be-- - Fine, but what

01:08:29,630 --> 01:08:30,640
about Kernel Five?

01:08:31,780 --> 01:08:33,900
- Because that was fixed along the way.

01:08:33,900 --> 01:08:36,020
Now it's (muttering). - Yeah, put in four.

01:08:36,020 --> 01:08:38,240
- Yeah, but Kernel Five is the same problem.

01:08:38,240 --> 01:08:41,940
- [Man] Oh, so there are a lot of ways that

01:08:41,940 --> 01:08:42,800
I would look at this.

01:08:42,800 --> 01:08:44,070
I mean, are we using more CPU,

01:08:44,070 --> 01:08:45,770
are we doing more physical IO,

01:08:45,770 --> 01:08:47,190
are we spending time waiting? - No.

01:08:48,032 --> 01:08:49,950
- [Man] Like we have a lot of metrics that I would use

01:08:50,836 --> 01:08:53,399
to figure out what's going on, it shouldn't be hard.

01:08:53,399 --> 01:08:56,170
- Yeah, but it's, well, I did not put,

01:08:56,170 --> 01:08:57,660
if you want, I have all the metrics

01:08:57,660 --> 01:08:59,510
if you are curious about.

01:08:59,510 --> 01:09:02,490
So yes, of course this is we have worst performance

01:09:02,490 --> 01:09:05,140
so we'll do less IO and so on.

01:09:05,140 --> 01:09:10,140
So one of feeling myself I have them in the latest Kernel,

01:09:11,360 --> 01:09:14,650
what was fixed, it was scalability problem

01:09:14,650 --> 01:09:18,730
about direct-IO reads.

01:09:18,730 --> 01:09:21,160
Before it was not scaling so we can easily saturate

01:09:21,160 --> 01:09:22,330
when we use direct-IO.

01:09:23,170 --> 01:09:24,730
On EXT4, you were limited.

01:09:26,154 --> 01:09:29,150
You're just hitting contention in the Kernel

01:09:29,150 --> 01:09:31,340
and you can not read anymore.

01:09:31,340 --> 01:09:35,260
And my feelings and before this limitation

01:09:35,260 --> 01:09:37,437
was doing some kind of struggling,

01:09:37,437 --> 01:09:41,990
because the problem is you read and you write on

01:09:41,990 --> 01:09:44,010
the same time, you have a mix of this

01:09:44,010 --> 01:09:47,970
and users can read too much, so in fact,

01:09:47,970 --> 01:09:51,131
your writes will be stored,

01:09:51,131 --> 01:09:53,510
waiting them before it's finished.

01:09:53,510 --> 01:09:55,550
So something happens on storage layer which

01:09:55,550 --> 01:09:57,380
is not reported from nowhere,

01:09:57,380 --> 01:09:58,920
probably reported somewhere internally,

01:09:59,757 --> 01:10:00,590
but will, until now, I did not find any guys

01:10:02,786 --> 01:10:05,666
with file system skills which can look inside

01:10:05,666 --> 01:10:07,583
and point on something.

01:10:11,447 --> 01:10:12,668
Yep.

01:10:12,668 --> 01:10:14,891
- It's a crappy answer but on five section,

01:10:14,891 --> 01:10:18,658
at least you get (speaking off mic).

01:10:18,658 --> 01:10:20,447
I hate it when they let somebody else do it.

01:10:20,447 --> 01:10:23,400
But I certainly will tell you at least

01:10:23,400 --> 01:10:26,056
it did happen here and you can see the source of it.

01:10:26,056 --> 01:10:28,091
And from that, you're probably gonna find out,

01:10:28,091 --> 01:10:30,895
is it something they broke or is something they fixed?

01:10:30,895 --> 01:10:32,542
And if it's something they fixed,

01:10:32,542 --> 01:10:33,410
it's not gonna get any better.

01:10:33,410 --> 01:10:35,230
Sorry, if it's something they fixed,

01:10:35,230 --> 01:10:36,760
it's not gonna get any better.

01:10:36,760 --> 01:10:39,280
We had the same problem with UDP packets.

01:10:39,280 --> 01:10:42,240
We switched from an ancient Kernel to 4.9

01:10:43,080 --> 01:10:45,980
and suddenly our packets per second dropped.

01:10:45,980 --> 01:10:47,790
And we went through and did the bisection

01:10:47,790 --> 01:10:50,130
and there was a big chunk of work

01:10:50,130 --> 01:10:52,540
to reduce UDP packet latency,

01:10:52,540 --> 01:10:55,010
and it turns out, in reducing latency,

01:10:55,010 --> 01:10:56,650
they also reduced the ability

01:10:56,650 --> 01:10:58,490
to generate packets per second.

01:10:58,490 --> 01:11:00,890
So we realized we had to find a different way

01:11:00,890 --> 01:11:02,290
to get our performance back.

01:11:04,590 --> 01:11:06,570
- You see, I can not advise MySQL users,

01:11:06,570 --> 01:11:08,900
not a regular Kernel if you use EXT4, right?

01:11:08,900 --> 01:11:09,733
So it will be--

01:11:10,737 --> 01:11:13,195
- [Man] Nobody will find just changing

01:11:13,195 --> 01:11:14,070
the deposit (mumbles) then change it back.

01:11:17,416 --> 01:11:20,399
- At least you can find out what the obligation need to,

01:11:20,399 --> 01:11:23,576
sorry, at least you can figure out what the change was,

01:11:23,576 --> 01:11:27,370
even if you can't see it in the block traces,

01:11:27,370 --> 01:11:29,190
you can at least find out what the change was

01:11:29,190 --> 01:11:30,840
and what the motivation was,

01:11:30,840 --> 01:11:33,370
and then you can go buy the developer

01:11:33,370 --> 01:11:34,720
a drink and argue about it.

01:11:36,428 --> 01:11:38,760
- [Man] Have to find out if (speaking off mic).

01:11:38,760 --> 01:11:41,610
- I think, first I will need to blog about the problem.

01:11:41,610 --> 01:11:45,520
So I expected to know then here we'll find the solution,

01:11:46,500 --> 01:11:48,800
so probably it should be just mention it

01:11:48,800 --> 01:11:50,670
and people will look on this.

01:11:50,670 --> 01:11:53,030
- Dimitri, or you can go to tracing way.

01:11:53,030 --> 01:11:55,910
Since the problem lies between the problem,

01:11:55,910 --> 01:11:59,553
well, the symptoms lie between 4.1

01:11:59,553 --> 01:12:04,553
and 4.14 and you should have trace points

01:12:04,640 --> 01:12:06,290
for those versions.

01:12:07,376 --> 01:12:10,160
And it should be very easy to list trace points

01:12:10,160 --> 01:12:13,840
that are available so you can get like a feeling,

01:12:13,840 --> 01:12:15,980
whereas the latency, for example, increased.

01:12:15,980 --> 01:12:19,560
It is a block layer or is it something up

01:12:19,560 --> 01:12:21,410
to stack like the XTE4.

01:12:21,410 --> 01:12:24,290
- Trace point you mean from the zipper, trace point or--

01:12:24,290 --> 01:12:26,290
- Yeah, you can instrument it with PERF.

01:12:27,421 --> 01:12:30,850
You can use your BPF, should be available starting

01:12:30,850 --> 01:12:34,370
from 4.1, not sure how many features are out there.

01:12:34,370 --> 01:12:36,904
But you get a feeling how much, for example,

01:12:36,904 --> 01:12:41,904
latency is a Kernel upgrade introducing, for example.

01:12:42,420 --> 01:12:43,530
Assuming that's the issue.

01:12:43,530 --> 01:12:45,610
- I expect and then people who are working

01:12:45,610 --> 01:12:48,480
on file system, it's kind of a generic problem.

01:12:48,480 --> 01:12:51,490
You should deliver something like IOstat which

01:12:52,559 --> 01:12:53,660
is showing you then something going wrong

01:12:53,660 --> 01:12:56,498
or you have, because saturation layer,

01:12:56,498 --> 01:12:58,590
well you can not just discover.

01:12:58,590 --> 01:13:00,480
So many people may have it in production,

01:13:00,480 --> 01:13:02,684
on cloud, whatever you want, so you use

01:13:02,684 --> 01:13:05,210
the storage is the same, right?

01:13:05,210 --> 01:13:06,690
So it's just doing what it can

01:13:06,690 --> 01:13:11,170
so something happens on a file system layer and why,

01:13:11,170 --> 01:13:12,942
so any device can be here.

01:13:12,942 --> 01:13:15,800
So in fact, in my case, it was Optane,

01:13:15,800 --> 01:13:17,640
Oracle Linux guy discovered the same

01:13:17,640 --> 01:13:20,750
on there and with me which is not that fast and so on

01:13:20,750 --> 01:13:23,080
but the tendency is the same.

01:13:23,080 --> 01:13:26,290
And so here is twice, double regression

01:13:26,290 --> 01:13:28,900
so well, you may have 30 person, 20.

01:13:28,900 --> 01:13:31,590
- Do you expect to see the same regression

01:13:31,590 --> 01:13:34,400
with other storage vendors, storage solutions?

01:13:34,400 --> 01:13:35,600
- Yeah, exactly. - You see it?

01:13:35,600 --> 01:13:36,800
- Yeah. - Right.

01:13:36,800 --> 01:13:40,020
- So because, well, you will on the,

01:13:40,020 --> 01:13:42,278
but next story will be about XFS.

01:13:42,278 --> 01:13:45,630
And so the first reaction was oh,

01:13:45,630 --> 01:13:47,480
it's Optane problem so always Intel.

01:13:48,410 --> 01:13:52,060
So then we try to reproduce it on other storage

01:13:52,060 --> 01:13:54,120
and we discovered the same storage.

01:13:54,120 --> 01:13:55,760
- Okay.

01:13:55,760 --> 01:13:59,490
- And even on, so I'm also testing Oracle Cloud

01:13:59,490 --> 01:14:01,020
which is used in block storage which

01:14:01,020 --> 01:14:02,520
is remote storage or network.

01:14:03,775 --> 01:14:06,024
The story is the same.

01:14:06,024 --> 01:14:10,910
So it's pure Kernel layer about file system.

01:14:14,730 --> 01:14:16,720
- You don't see it with XFS?

01:14:16,720 --> 01:14:18,420
- So with XFS, it's even more fun,

01:14:19,466 --> 01:14:20,677
guys. - Yeah, but you don't see

01:14:20,677 --> 01:14:21,510
this very problem.

01:14:21,510 --> 01:14:22,960
- You will see.

01:14:26,400 --> 01:14:29,410
So I think it's time. - Yeah.

01:14:29,410 --> 01:14:30,540
- Any other comments?

01:14:31,990 --> 01:14:35,830
Well, probably you can test yourself as well but, again,

01:14:35,830 --> 01:14:39,552
so people who are doing (mumbling) don't see the problem.

01:14:39,552 --> 01:14:44,552
And this is our, probably, main, I don't know,

01:14:45,465 --> 01:14:50,465
target and dilemmas and so in the past we worked

01:14:51,850 --> 01:14:54,460
with many storage vendors so storage

01:14:54,460 --> 01:14:56,200
is very good from pure IO test,

01:14:56,200 --> 01:14:58,220
very good for Oracle database.

01:14:58,220 --> 01:15:01,780
When we run MySQL, it's just twice was and another one.

01:15:01,780 --> 01:15:03,820
Well why happens, but it's MySQL.

01:15:04,852 --> 01:15:06,420
(group laughing)

01:15:06,420 --> 01:15:08,506
- [Man] So the only solution to get this fixed

01:15:08,506 --> 01:15:09,964
is to wrap it up and go to benchmark

01:15:09,964 --> 01:15:11,050
that any developer could use to recreate it.

01:15:11,050 --> 01:15:12,030
- Yeah, exactly.

01:15:12,030 --> 01:15:13,650
- [Man] And then send it to the mailing list

01:15:14,540 --> 01:15:16,457
that I'm sure the extended four people will be able

01:15:16,457 --> 01:15:19,440
to explain for IO. - Yeah.

01:15:19,440 --> 01:15:21,750
Currently I'm preparing just a bundle of scripts,

01:15:21,750 --> 01:15:24,700
that anybody can just deploy or run and you get your story.

01:15:26,234 --> 01:15:28,348
- [Man] But my recommendation would be

01:15:28,348 --> 01:15:30,741
to turn it into a bio dot. - Yeah.

01:15:30,741 --> 01:15:33,501
- [Man] If you've used the bio benchmark and utility,

01:15:33,501 --> 01:15:35,420
that is something that every single adult person has

01:15:38,474 --> 01:15:40,360
on their development machine and it is a common,

01:15:40,360 --> 01:15:43,080
well understood way to take off.

01:15:43,080 --> 01:15:44,240
- This is the problem, again,

01:15:44,240 --> 01:15:47,080
because it's too generic and it can not simulate everything

01:15:47,080 --> 01:15:49,098
what happens inside in the code.

01:15:49,098 --> 01:15:50,570
- [Man] Just run the block post with--

01:15:50,570 --> 01:15:51,770
- And replay the block post.

01:15:51,770 --> 01:15:53,831
- Yeah. - If you can see it

01:15:53,831 --> 01:15:56,069
just easily with MySQL, you can make it happen with Myo,

01:15:56,069 --> 01:16:01,069
I guarantee it.

01:16:01,130 --> 01:16:03,869
- Interesting.

01:16:03,869 --> 01:16:04,702
(men speaking off mic)

01:16:04,702 --> 01:16:05,800
Okay, there was another question here.

01:16:06,878 --> 01:16:08,870
- [Man] I was just gonna say exactly what they said

01:16:10,010 --> 01:16:12,500
because if you can trace it and then just replay

01:16:12,500 --> 01:16:15,502
it and make it all happen, then that can be like

01:16:15,502 --> 01:16:17,682
at least you can just record and trace essentially.

01:16:17,682 --> 01:16:18,983
- So you mean if I collect block phase,

01:16:18,983 --> 01:16:20,865
I can replay it with--

01:16:20,865 --> 01:16:22,199
- [Man] Bio can replay a block phase.

01:16:22,199 --> 01:16:24,261
- Yeah. - Yes, block trace itself

01:16:24,261 --> 01:16:25,480
has like a... - Okay.

01:16:25,480 --> 01:16:27,232
(men speaking off mic)

01:16:27,232 --> 01:16:29,100
Yeah but how to do it will replay it,

01:16:29,100 --> 01:16:30,850
we'll just-- - But if it hurts you,

01:16:30,850 --> 01:16:32,587
it hurts you.

01:16:32,587 --> 01:16:33,954
But you can have it.

01:16:33,954 --> 01:16:35,587
- Spare this. - The problem with doing

01:16:35,587 --> 01:16:38,959
this video on block trace reproduction

01:16:38,959 --> 01:16:43,959
is that this bug is probably either generating extra IO

01:16:44,034 --> 01:16:45,759
or generating extra waits.

01:16:45,759 --> 01:16:47,915
And so the block trace will simulate that extra IO

01:16:47,915 --> 01:16:49,827
and simulate the extra waits,

01:16:49,827 --> 01:16:51,642
but that's not something that you can fix

01:16:51,642 --> 01:16:54,671
because the quality of the waiting is not,

01:16:54,671 --> 01:16:56,219
am I being-- - So you recorded

01:16:56,219 --> 01:16:57,558
the effects of the bug.

01:16:57,558 --> 01:16:59,603
- Yeah. - And if that won't help you,

01:16:59,603 --> 01:17:02,050
diagnose it, on the other hand, look at it.

01:17:02,050 --> 01:17:06,652
- The problem is wait happens somewhere else

01:17:06,652 --> 01:17:09,669
for because some kind of condition happens.

01:17:09,669 --> 01:17:13,843
Will it be seen in block trace on the same--

01:17:13,843 --> 01:17:17,806
- [Man] But you will see more and more IO on the patterns.

01:17:17,806 --> 01:17:19,854
- [Guy] You'll see it in the pattern of the IO

01:17:19,854 --> 01:17:23,119
but it won't recreate far.

01:17:23,119 --> 01:17:25,901
(speaking off mic) Is my assumption.

01:17:25,901 --> 01:17:30,901
- Okay, interesting point, so I will note this, okay.

01:17:32,087 --> 01:17:34,894
So I will prepare the test case

01:17:34,894 --> 01:17:38,390
as well and see you on the block trace.

01:17:40,680 --> 01:17:42,490
- Well, while we're at that,

01:17:42,490 --> 01:17:47,480
it was asked about XFS so just if there was nothing else,

01:17:47,480 --> 01:17:48,682
can we move on to that one?

01:17:48,682 --> 01:17:49,515
- Yeah.

01:17:50,690 --> 01:17:54,900
So I now, EXT4 was fun enough, right?

01:17:57,020 --> 01:18:00,690
So now the story, we aid into the story, XFS.

01:18:01,950 --> 01:18:04,520
So just to show you what happens here.

01:18:04,520 --> 01:18:06,270
So you understand, we have double writes

01:18:06,270 --> 01:18:08,970
so we writing data twice, okay?

01:18:10,010 --> 01:18:14,660
The red line is XFS without double write, okay?

01:18:14,660 --> 01:18:18,930
The blue line is EXT4 without double write as well.

01:18:18,930 --> 01:18:21,370
So you see, twice difference.

01:18:21,370 --> 01:18:26,020
It's what I said and EXT4 was always better in the past.

01:18:26,020 --> 01:18:28,777
- [Man] Previous topic was. (speaking off mic)

01:18:28,777 --> 01:18:33,777
- So this one is caption version four-dot-one, okay?

01:18:34,810 --> 01:18:37,920
So that's why the story changes with the latest Kernel.

01:18:37,920 --> 01:18:42,920
So this is on the old Kernel, so EXTFS is doing worse, okay?

01:18:43,120 --> 01:18:46,310
But when we enabled double write,

01:18:46,310 --> 01:18:51,190
so green line is EXT4, and yellow line is XFS.

01:18:51,190 --> 01:18:55,080
So in fact, on XFS when we do twice more writes,

01:18:55,080 --> 01:18:56,080
we are going faster.

01:18:58,080 --> 01:19:01,556
You follow the logic?

01:19:01,556 --> 01:19:03,623
(group laughing)

01:19:03,623 --> 01:19:04,810
- [Man] Then where's the problem?

01:19:04,810 --> 01:19:08,000
- It's twice more right then we have better performance.

01:19:08,000 --> 01:19:10,079
- [Man] Maybe you should try three times.

01:19:10,079 --> 01:19:15,079
(man speaking off mic) (group laughing)

01:19:15,232 --> 01:19:18,370
- I was thinking about it so--

01:19:18,370 --> 01:19:21,100
- Dimitri, sorry, which IO scheduler did you drive

01:19:21,100 --> 01:19:23,010
in place for the disk at the time?

01:19:23,010 --> 01:19:26,700
- IO schedule, I think it's a default one.

01:19:26,700 --> 01:19:29,680
Oracle, you can switch it to new before I recall,

01:19:29,680 --> 01:19:32,670
well, I stop it to look on this because in

01:19:32,670 --> 01:19:35,660
the past we always play it, we compare a difference,

01:19:35,660 --> 01:19:37,520
can do it some from--

01:19:37,520 --> 01:19:40,030
- Yeah, 'cause sometimes, and I'm trying

01:19:40,030 --> 01:19:42,990
to remember, this is from ancient history,

01:19:42,990 --> 01:19:45,170
a couple of them will delay writes

01:19:45,170 --> 01:19:46,300
to the point where it actually gets in

01:19:46,300 --> 01:19:47,860
the way of a smaller stream.

01:19:47,860 --> 01:19:49,270
Once you start pushing a harder stream,

01:19:49,270 --> 01:19:52,050
you're actually getting it scheduled on the device earlier.

01:19:52,050 --> 01:19:53,860
That's one of the reasons we usually just turn it off

01:19:53,860 --> 01:19:55,700
and let the device do a deal with it.

01:19:57,120 --> 01:20:00,560
- So with all the, both 4.12 is going

01:20:03,580 --> 01:20:06,130
to be non for an opt-in device, no scheduler,

01:20:06,130 --> 01:20:09,310
by default, unless your UDEV is changing something.

01:20:09,310 --> 01:20:11,060
- [Presenter] Yeah, yeah, nice one.

01:20:12,620 --> 01:20:16,360
- So I think, as I was saying earlier,

01:20:16,360 --> 01:20:18,160
I would expect XFS in extended four

01:20:18,160 --> 01:20:22,250
to at least be equivalent in pretty much every configuration

01:20:22,250 --> 01:20:26,680
so like we're turning double writes off,

01:20:26,680 --> 01:20:29,199
extended four is doing that much better than XFS.

01:20:29,199 --> 01:20:31,140
There's something wrong.

01:20:31,140 --> 01:20:33,660
There's a configuration error on XFS

01:20:33,660 --> 01:20:35,270
that's making it slow.

01:20:35,270 --> 01:20:38,997
It's probably the alignment of the journal IO, XFS,

01:20:41,070 --> 01:20:43,870
when you make the file system it looks at

01:20:43,870 --> 01:20:45,869
the device that you're making it on

01:20:45,869 --> 01:20:48,140
and it makes some assumptions about what fast will be.

01:20:48,140 --> 01:20:50,530
Those assumptions are sometimes wrong.

01:20:50,530 --> 01:20:54,170
So there are actually two different problems

01:20:54,170 --> 01:20:55,220
in that graph, right?

01:20:56,108 --> 01:20:57,651
First, they should at least be equivalent

01:20:57,651 --> 01:21:00,015
or XFS may sometimes be higher because

01:21:00,015 --> 01:21:03,690
it has significantly less lock contention

01:21:03,690 --> 01:21:06,440
on an inode during direct IO.

01:21:06,440 --> 01:21:08,937
Because XFS allows two people to be in

01:21:08,937 --> 01:21:12,300
the direct IO path at once, extended four and Btrfs do not.

01:21:12,300 --> 01:21:16,221
So, I don't know ,there are a lot of questions there,

01:21:16,221 --> 01:21:19,930
(laughs) let me put it that way.

01:21:19,930 --> 01:21:21,420
- Any other advice?

01:21:23,030 --> 01:21:26,440
Because I switch it to the next fun slide.

01:21:28,040 --> 01:21:31,890
So this was a huge headache because it's out

01:21:31,890 --> 01:21:36,890
of logic and so on and let's call it by chance

01:21:39,100 --> 01:21:43,655
so the guy who is working on record, on InnoDB,

01:21:43,655 --> 01:21:48,590
he just made something broken which bring me attention,

01:21:48,590 --> 01:21:51,150
all the write path was totally broken

01:21:51,150 --> 01:21:54,740
so we started to write in chaotic manner,

01:21:54,740 --> 01:21:56,140
but it improved performance.

01:21:57,230 --> 01:22:00,740
So I started to dig in this way and trying to understand,

01:22:00,740 --> 01:22:05,243
and in fact, I discovered this very strange story about,

01:22:07,730 --> 01:22:10,390
so on InnoDB we have IO write threads

01:22:10,390 --> 01:22:13,920
so which are responsible for IO writing, okay?

01:22:13,920 --> 01:22:17,250
And I just retried to limit them

01:22:17,250 --> 01:22:19,960
so because historically we'll use 16 just

01:22:19,960 --> 01:22:22,470
to allow more writes in parallel,

01:22:22,470 --> 01:22:24,690
but now the storage is faster.

01:22:24,690 --> 01:22:27,820
And what happens, so in fact this is what we see

01:22:27,820 --> 01:22:31,980
so we have a drop when we have too much IO write thread,

01:22:31,980 --> 01:22:34,280
so we aggressively write them.

01:22:34,280 --> 01:22:39,280
And this is the case, so with field without double write,

01:22:40,600 --> 01:22:44,190
this is the case when we have only four IO write threads.

01:22:44,190 --> 01:22:47,870
And in fact, we reach higher performance than the EXT4.

01:22:48,740 --> 01:22:50,290
So without using double write.

01:22:53,300 --> 01:22:55,970
And just because we are less aggression writes,

01:22:57,730 --> 01:23:00,230
so if we are very aggressive, we have slowed down.

01:23:01,910 --> 01:23:03,760
- [Man] How many threads are we talking about here?

01:23:04,632 --> 01:23:06,393
- So here is four during 16.

01:23:06,393 --> 01:23:11,393
So 16 it was by default in all my tests.

01:23:11,580 --> 01:23:13,560
Historically, because storage was slow

01:23:13,560 --> 01:23:15,220
so you need to write more in parallel

01:23:15,220 --> 01:23:18,750
to hide the latency, now-- - For the CPU course?

01:23:18,750 --> 01:23:19,583
- 48.

01:23:23,099 --> 01:23:26,349
(man speaking off mic)

01:23:30,140 --> 01:23:32,520
What is the cube?

01:23:32,520 --> 01:23:37,351
- [Man] There you go.

01:23:37,351 --> 01:23:40,440
- Some years ago, we actually started trying

01:23:40,440 --> 01:23:43,190
to use NVRAM for the journal, and we actually found out

01:23:43,190 --> 01:23:45,215
that there were a number of issues where

01:23:45,215 --> 01:23:47,555
the journal kind of presumed that IO is on

01:23:47,555 --> 01:23:48,930
the order of milliseconds, and when you suddenly

01:23:48,930 --> 01:23:51,939
had IO on the order of small microseconds,

01:23:51,939 --> 01:23:54,780
lists started getting very large,

01:23:54,780 --> 01:23:57,505
things started going N-squared instead of (mumbles),

01:23:57,505 --> 01:24:00,920
contention looks like the obvious issue here.

01:24:02,670 --> 01:24:04,790
The fact that it got better with a smaller number

01:24:04,790 --> 01:24:07,580
of threads, so contention is almost certainly an issue,

01:24:07,580 --> 01:24:09,800
and that's one of the things with faster storage.

01:24:09,800 --> 01:24:11,490
There's a lot of things that were tested in

01:24:11,490 --> 01:24:16,490
the presence of millisecond behavior that don't do well when

01:24:17,410 --> 01:24:19,010
it becomes microsecond behavior.

01:24:21,660 --> 01:24:23,840
- So again, lack of instrumentation.

01:24:23,840 --> 01:24:26,940
So at least for XFS, I found there is a lot of tones

01:24:26,940 --> 01:24:29,780
of statistic reported so I need to bring them

01:24:29,780 --> 01:24:33,540
and look on this to see if it makes some difference or not,

01:24:34,400 --> 01:24:37,530
but again, I was surprised that it is nice and simple,

01:24:37,530 --> 01:24:41,750
just to see there is starvation on file system layer or not.

01:24:41,750 --> 01:24:42,990
- Have you checked where there is actually,

01:24:42,990 --> 01:24:45,879
the files are still layered, the sum's much more likely

01:24:45,879 --> 01:24:48,779
to be on my sequel layer that it just is contention there.

01:24:49,670 --> 01:24:50,930
- I expected them.

01:24:50,930 --> 01:24:54,820
So my first point was about, then probably with engine,

01:24:54,820 --> 01:24:58,730
too much IO, so right now, we use a synchronous IO

01:24:58,730 --> 01:25:02,410
with all direct, so it just sends in many IO.

01:25:02,410 --> 01:25:06,120
So I wanted to make the changes to limit how much

01:25:06,120 --> 01:25:08,400
we will sound, did not help.

01:25:08,400 --> 01:25:10,790
So if we even will help few IOs instead

01:25:10,790 --> 01:25:12,790
of too (mumbles) or something like that.

01:25:14,736 --> 01:25:16,037
- No, what I mean is like that some data structures

01:25:16,037 --> 01:25:17,203
in my sequel are contended because you

01:25:17,203 --> 01:25:18,220
have too many threads and you're just not scaling that good.

01:25:18,220 --> 01:25:21,830
- Yeah, but there is no contention on MySQL layer.

01:25:21,830 --> 01:25:22,910
This is the point.

01:25:22,910 --> 01:25:27,320
So what's interesting also then I was,

01:25:28,660 --> 01:25:32,860
I should limit concurrency on user threads

01:25:32,860 --> 01:25:37,860
because our IO reads are not asynchronous,

01:25:38,190 --> 01:25:41,552
so as they are normal is direct, normal writes.

01:25:41,552 --> 01:25:46,060
And more users that have more IO reads will be done

01:25:46,060 --> 01:25:49,282
in parallel, and more it will saturate the layer, IO layer.

01:25:49,282 --> 01:25:54,000
So if I reduce number of IO reads, they will do,

01:25:54,000 --> 01:25:55,300
I have better performance.

01:25:56,913 --> 01:26:01,640
So, in fact, reads are blocking writes, okay?

01:26:01,640 --> 01:26:05,020
So we need to find the balance between IO reads

01:26:05,020 --> 01:26:07,950
and IO writes, to let them leave working together.

01:26:12,160 --> 01:26:16,003
- [Man] So in general, I would suggest looking

01:26:16,003 --> 01:26:18,942
at a very eye level from the Kernel point of view,

01:26:18,942 --> 01:26:19,775
you know, have we increased CPU usage due to contention?

01:26:22,452 --> 01:26:24,020
And then it's very easy to figure out

01:26:24,020 --> 01:26:27,425
where that convention is. - CPU's not fully used at all.

01:26:27,425 --> 01:26:29,540
- [Man] Well it doesn't matter if it's fully used,

01:26:29,540 --> 01:26:32,160
it matters if the power's changed, right?

01:26:32,160 --> 01:26:35,430
And then assuming the CPU usage hasn't changed,

01:26:35,430 --> 01:26:40,430
there is something you can Google for an off CPU profiling,

01:26:41,721 --> 01:26:46,721
you can search for off CPU profiling

01:26:47,016 --> 01:26:50,390
which is something that Brendan Gregg talks a lot about.

01:26:50,390 --> 01:26:52,390
There are a number of different ways to do it.

01:26:53,408 --> 01:26:56,800
But they can tell us where we are spending our time waiting

01:26:56,800 --> 01:27:00,680
and it's actually much more informative than just looking

01:27:00,680 --> 01:27:03,680
for things in these data or other ways.

01:27:03,680 --> 01:27:07,220
But if we aren't spending more time waiting for CPU,

01:27:07,220 --> 01:27:09,850
we're spending more time waiting for the drive

01:27:09,850 --> 01:27:12,190
and that might be due to unfairness inside the drive

01:27:12,190 --> 01:27:15,688
or it might be due to unfairness inside the file system,

01:27:15,688 --> 01:27:17,161
it's hard to say.

01:27:17,161 --> 01:27:18,700
- [Guy] So let's make sure you're kind

01:27:18,700 --> 01:27:21,100
of gonna talk about obviously CPU profiling.

01:27:21,100 --> 01:27:22,546
It's not good.

01:27:22,546 --> 01:27:23,892
- Awesome. - So...

01:27:23,892 --> 01:27:26,073
- [Man] Chris, do you you know

01:27:26,073 --> 01:27:27,960
if write back frothing turned up around,

01:27:27,960 --> 01:27:31,610
this is, what, 4.14, this Kernel?

01:27:31,610 --> 01:27:33,300
- [Guy] That's a very good point.

01:27:33,300 --> 01:27:35,750
Hopefully the write back throttling isn't

01:27:35,750 --> 01:27:38,549
a factor in the direct IO here but it might be.

01:27:38,549 --> 01:27:42,090
It's certainly something to take a look at.

01:27:42,090 --> 01:27:43,720
It could also be something that caused

01:27:43,720 --> 01:27:47,120
the extended four problems you were talking about before.

01:27:47,120 --> 01:27:50,280
- There's also the block layer request limiting thing

01:27:50,280 --> 01:27:53,141
that went, I think, in and around that time

01:27:53,141 --> 01:27:54,900
and I saw pretty bad performance regressions

01:27:54,900 --> 01:27:57,620
in Postgres with that so, think you can--

01:27:57,620 --> 01:27:59,270
- The write back's talking?

01:27:59,270 --> 01:28:00,510
- No, on the block layer, you can,

01:28:00,510 --> 01:28:03,160
there's a request throttling mechanism now

01:28:03,160 --> 01:28:06,796
which you reduce limit, the latency spikes

01:28:06,796 --> 01:28:09,600
and I've seen that, and you can turn it off.

01:28:09,600 --> 01:28:11,680
It's like one settings and I've seen

01:28:11,680 --> 01:28:15,110
that reverse performance quite badly on fast storage.

01:28:15,110 --> 01:28:18,655
- [Man] Plus CPU profiling will show all of those things.

01:28:18,655 --> 01:28:20,597
It would be very useful.

01:28:20,597 --> 01:28:23,100
- [Guy] It can show many things.

01:28:23,100 --> 01:28:24,420
- Okay.

01:28:24,420 --> 01:28:27,680
So this is a fast orange and I was able

01:28:27,680 --> 01:28:32,680
to find work around by doing less writes in parallel, okay?

01:28:32,770 --> 01:28:36,190
And to finish the story, this is a slow storage.

01:28:38,880 --> 01:28:42,950
Slow storage but TPCC workload, IO-bound as well.

01:28:42,950 --> 01:28:44,930
Small machine, it's only 12 course.

01:28:44,930 --> 01:28:48,050
All this is there, well, and again.

01:28:48,050 --> 01:28:52,210
So the yellow line, well, orange line,

01:28:52,210 --> 01:28:56,650
is double write enabled and all others we've disabled

01:28:56,650 --> 01:29:00,390
and one IO writes thread to and so on.

01:29:00,390 --> 01:29:01,240
Nice and healthy.

01:29:07,000 --> 01:29:09,640
So how we can do better by doing more writes?

01:29:11,953 --> 01:29:13,801
- Scheduling.

01:29:13,801 --> 01:29:16,068
There's a lot of substitutes where you can push them

01:29:16,068 --> 01:29:21,068
and they're not missing opportunities then.

01:29:21,198 --> 01:29:22,373
Sorry, thanks man.

01:29:22,373 --> 01:29:24,531
- [Producer] You're welcome.

01:29:24,531 --> 01:29:27,230
- There's a lot of subsystems where, if you push them,

01:29:27,230 --> 01:29:29,610
you're not missing opportunities to schedule.

01:29:29,610 --> 01:29:31,390
In a particular IO and networking is

01:29:31,390 --> 01:29:33,340
the kind of places you see that.

01:29:33,340 --> 01:29:35,718
So one of the things you really do you want

01:29:35,718 --> 01:29:38,054
to look for is off CPU here because if it's not scheduling,

01:29:38,054 --> 01:29:41,630
it's waiting to do something and where it's waiting

01:29:41,630 --> 01:29:43,880
to do that something is probably informative.

01:29:45,510 --> 01:29:46,950
- Okay.

01:29:46,950 --> 01:29:51,950
So it's all about scheduling and messing about

01:29:52,050 --> 01:29:54,520
the file system internals, what you want--

01:29:54,520 --> 01:29:57,447
- The internals could be where the, sorry,

01:29:57,447 --> 01:30:00,650
the internals could be where it's happening.

01:30:00,650 --> 01:30:02,140
I mean, there's a lot of places where

01:30:02,140 --> 01:30:04,600
I just finished this but when I picked

01:30:04,600 --> 01:30:06,199
it up because there's something waiting.

01:30:06,199 --> 01:30:08,720
Oh, there is nothing waiting, I'm gonna go block,

01:30:08,720 --> 01:30:10,440
and it's gonna take time to wait up.

01:30:10,440 --> 01:30:12,550
So there are often overheads when you're not

01:30:12,550 --> 01:30:16,950
at full-tilt boogie that aren't there when, sorry,

01:30:16,950 --> 01:30:19,400
overheads that disappear when you go full-tilt boogie,

01:30:19,400 --> 01:30:21,550
that appear only when you're not there yet.

01:30:26,245 --> 01:30:31,110
- It's actually like the new API for networking, right?

01:30:31,110 --> 01:30:33,300
You know, it switches to something that's faster

01:30:33,300 --> 01:30:35,250
if you get a higher rate of events.

01:30:39,932 --> 01:30:42,471
- [Man] But it's weird.

01:30:42,471 --> 01:30:43,350
- But it's weird.

01:30:43,350 --> 01:30:48,350
- Yeah so I'm pushing also then we have better visibility

01:30:48,600 --> 01:30:50,460
also another layer from the code,

01:30:51,992 --> 01:30:56,260
to see, because right, MySQL code was just lagging about,

01:30:56,260 --> 01:30:58,630
storage was faster than we can use so

01:30:58,630 --> 01:31:02,610
it was so we all, before it was just to send more.

01:31:02,610 --> 01:31:05,670
Let's try some more then at least we'll be able

01:31:05,670 --> 01:31:07,130
to do more, probably.

01:31:07,130 --> 01:31:09,897
And now so we realize and okay, we can do that much

01:31:09,897 --> 01:31:13,600
and this system will not fall.

01:31:15,377 --> 01:31:18,225
Yep.

01:31:18,225 --> 01:31:22,038
- Sorry.

01:31:22,038 --> 01:31:26,030
- So I know, is it the Skyler folk,

01:31:26,030 --> 01:31:31,030
they make their users do benchmarking of their disks

01:31:31,070 --> 01:31:33,540
in order to try and measure things like queues

01:31:33,540 --> 01:31:35,580
and so on, precisely to try

01:31:35,580 --> 01:31:38,080
and avoid getting to this case

01:31:38,080 --> 01:31:41,680
where sending too much actually makes things worse rather

01:31:41,680 --> 01:31:45,250
than better so if things are getting that fast

01:31:45,250 --> 01:31:49,210
and getting that contended, I guess maybe you could end up

01:31:49,210 --> 01:31:51,437
in that scenario too but...

01:31:51,437 --> 01:31:55,530
- Yeah, but here you say the slower device so.

01:31:55,530 --> 01:31:57,000
- Yeah. - Even on slower device

01:31:57,000 --> 01:31:59,380
we discovered something.

01:31:59,380 --> 01:32:01,591
Okay, thank you very much so, thank you.

01:32:01,591 --> 01:32:02,424
(clapping)

01:32:02,424 --> 01:32:03,520
- [Man] Thank you.

01:32:05,690 --> 01:32:07,300
- Okay, welcome back everyone.

01:32:08,580 --> 01:32:12,900
It's our honor here to welcome Dr. Richard Hipp,

01:32:12,900 --> 01:32:15,380
the author of SQLite who's just going

01:32:15,380 --> 01:32:18,630
to give us a talk about why it matters

01:32:18,630 --> 01:32:19,860
to Kernel developers.

01:32:21,030 --> 01:32:22,360
- Thank you, thank you for letting me be here.

01:32:22,360 --> 01:32:25,030
I actually have about two hours worth of material

01:32:25,030 --> 01:32:27,680
but I only have five minutes to present it in.

01:32:27,680 --> 01:32:32,680
So what I've done is, oops, wrong button.

01:32:35,840 --> 01:32:38,590
I don't know how to operate a, there we go.

01:32:38,590 --> 01:32:42,160
So I had this briefing paper here that goes

01:32:42,160 --> 01:32:44,190
into more detail about all the questions I have

01:32:44,190 --> 01:32:46,770
and that sort of thing, I'll talk about that in a minute.

01:32:46,770 --> 01:32:48,670
I just want to give you a brief introduction

01:32:48,670 --> 01:32:50,980
of what SQLite is and what it's all about.

01:32:50,980 --> 01:32:52,780
The first thing you need to know about SQLite

01:32:52,780 --> 01:32:55,040
is that it's in everything and everywhere.

01:32:55,040 --> 01:32:57,680
I don't know if it's in that Sony camera but it might be.

01:32:57,680 --> 01:32:59,580
It's certainly in your phone.

01:32:59,580 --> 01:33:02,090
It's in your car, probably.

01:33:02,090 --> 01:33:05,100
It's in your TV set, it's in most,

01:33:05,100 --> 01:33:07,600
a lot of the applications you run on your desktop.

01:33:09,815 --> 01:33:12,330
You know, a typical Android phone

01:33:12,330 --> 01:33:15,821
has about 200 SQLite databases on it

01:33:15,821 --> 01:33:20,350
and they collectively do an in excess

01:33:21,420 --> 01:33:23,930
of five gigabytes of IO per day, per phone,

01:33:25,120 --> 01:33:26,690
times two and a half billion phones,

01:33:26,690 --> 01:33:28,990
it's a lot of IO that's coming out of SQLite.

01:33:28,990 --> 01:33:32,350
If you're a file system person,

01:33:32,350 --> 01:33:35,290
SQLite's when you, it's about half

01:33:35,290 --> 01:33:40,290
of all your client on a phone, it does a lot of stuff,

01:33:42,040 --> 01:33:43,250
and there's many many other uses.

01:33:43,250 --> 01:33:44,600
So it's all over the place.

01:33:45,500 --> 01:33:48,500
It actually, it's fast, a lot of people think well,

01:33:48,500 --> 01:33:51,070
it's a relational database, it has to be slow.

01:33:51,070 --> 01:33:53,550
Turns out that we did some studies

01:33:53,550 --> 01:33:58,300
and you can pull out a 10K blob with

01:33:58,300 --> 01:34:01,830
the database query at about the same speed

01:34:01,830 --> 01:34:04,319
as you can read it from a separate file on disk.

01:34:04,319 --> 01:34:07,290
Now, this is on Linux.

01:34:07,290 --> 01:34:10,760
On other platforms the database is a lot faster.

01:34:10,760 --> 01:34:13,680
It turns out that reading things from files on disk

01:34:13,680 --> 01:34:15,661
is really fast on Linux, so it's about the same speed.

01:34:15,661 --> 01:34:18,660
And of course as the size of the blob gets bigger,

01:34:18,660 --> 01:34:20,190
the file system has an advantage,

01:34:20,190 --> 01:34:23,080
but if the size of the blob is smaller than about 10K,

01:34:23,080 --> 01:34:24,990
the database is actually faster.

01:34:24,990 --> 01:34:29,730
So it's not a sluggard, it actually does a pretty good job.

01:34:29,730 --> 01:34:32,440
SQLite is different from all the other database engines.

01:34:32,440 --> 01:34:34,040
We've got MySQL here with MyRDB,

01:34:35,162 --> 01:34:36,290
Postgres, all of these others are kind

01:34:36,290 --> 01:34:38,430
of designed to run in a data center

01:34:38,430 --> 01:34:42,090
and whereas SQLite runs out on the edge of the network,

01:34:42,960 --> 01:34:47,050
in IOT devices and that sort of thing.

01:34:47,050 --> 01:34:50,120
It is not a separate process, it's not a separate thread.

01:34:51,040 --> 01:34:53,880
SQLite is a subroutine.

01:34:53,880 --> 01:34:56,130
There's nothing running in the background

01:34:56,130 --> 01:34:58,550
to take care of background processes

01:34:58,550 --> 01:35:00,750
and so it's a very different sort of thing

01:35:00,750 --> 01:35:03,650
and you can have multiple processes accessing

01:35:03,650 --> 01:35:05,390
the same database at the same time.

01:35:05,390 --> 01:35:07,430
We can't use direct IO, or we could,

01:35:07,430 --> 01:35:09,850
but it wouldn't make sense to use direct IO for that

01:35:09,850 --> 01:35:14,040
because there's multiple applications

01:35:14,040 --> 01:35:15,460
that are reading the same file

01:35:15,460 --> 01:35:17,680
and they're not coordinating their buffers.

01:35:17,680 --> 01:35:19,650
So we're kind of dependent upon

01:35:19,650 --> 01:35:22,030
the operating system's buffer cache

01:35:22,030 --> 01:35:24,120
to take care of that for us.

01:35:24,120 --> 01:35:26,470
An SQLite database is a single file on disk,

01:35:27,730 --> 01:35:31,790
possibly with some journal files to control

01:35:33,960 --> 01:35:37,780
of the atomic Commit capability,

01:35:37,780 --> 01:35:41,640
but there's no configuration file.

01:35:41,640 --> 01:35:43,930
There's nothing that we can do to say oh,

01:35:45,243 --> 01:35:47,640
we're running on a particular file system

01:35:47,640 --> 01:35:50,140
that has these capabilities, we have to discover all

01:35:50,140 --> 01:35:53,440
of that at runtime and we would really like

01:35:53,440 --> 01:35:55,640
to have capabilities to discover more,

01:35:55,640 --> 01:35:58,030
because there's a lot of things that we'd like to know.

01:35:59,918 --> 01:36:01,360
Of course, in any database, it's important

01:36:01,360 --> 01:36:05,100
to be acid, atomic, consistent, isolated and durable,

01:36:05,100 --> 01:36:08,720
and there's three ways we implement that in SQLite.

01:36:08,720 --> 01:36:11,970
The default method is slow it's a rollback journal,

01:36:11,970 --> 01:36:15,340
but it's default because it works anywhere.

01:36:15,340 --> 01:36:17,769
Now, if we know that we're not running on

01:36:17,769 --> 01:36:21,620
a network file system, we can do a write ahead log

01:36:21,620 --> 01:36:25,000
and that's faster, but it also requires

01:36:25,000 --> 01:36:28,390
some shared memory, and if we run a network file system,

01:36:29,240 --> 01:36:31,660
we can't have shared memory between all the users

01:36:31,660 --> 01:36:33,810
because we could have different users on different machines

01:36:33,810 --> 01:36:36,310
that were accessing the database at the same time.

01:36:38,393 --> 01:36:39,360
- [Woman] Do you mind if I (mumbles).

01:36:39,360 --> 01:36:40,690
- No, I don't mind if you change it

01:36:40,690 --> 01:36:44,433
to the different display at all.

01:36:44,433 --> 01:36:49,240
But we don't have a good way to determine

01:36:49,240 --> 01:36:51,920
if we're on a network file system or not.

01:36:51,920 --> 01:36:54,470
So we kind of have to default to the slower method.

01:36:56,570 --> 01:36:59,500
So if we had a way to do that, that would be really cool.

01:37:01,560 --> 01:37:03,080
I'll stop while we talk.

01:37:03,080 --> 01:37:04,560
Well, I'll keep going.

01:37:04,560 --> 01:37:06,500
- [Man] Is this implemented as a library?

01:37:07,746 --> 01:37:09,512
- Yeah, it's just a library, yeah.

01:37:09,512 --> 01:37:11,062
So if we just had a wait, yeah.

01:37:11,062 --> 01:37:13,510
- [Man] You can't tell from looking at (mumbles)

01:37:13,510 --> 01:37:14,802
with your (speaking off mic).

01:37:14,802 --> 01:37:16,939
- [Speaker] Well, I mean, if we could and we could maintain

01:37:16,939 --> 01:37:19,490
a database of what's a network file system and what isn't.

01:37:20,784 --> 01:37:22,194
- [Man] You're thinking more than just NFX, okay.

01:37:22,194 --> 01:37:24,160
- Yeah, right.

01:37:25,260 --> 01:37:26,900
I mean, that means that we'd have to,

01:37:26,900 --> 01:37:29,000
and see, a lot of times the SQLite is statically linked

01:37:29,000 --> 01:37:31,710
with the application, and so if we were trying

01:37:31,710 --> 01:37:35,220
to keep a list of this and somebody statically linked it,

01:37:35,220 --> 01:37:37,820
there's no way for us to upgrade that for them.

01:37:37,820 --> 01:37:39,970
So it'd be nice if we had a way

01:37:39,970 --> 01:37:44,289
to just ask if this was a network file system or not.

01:37:44,289 --> 01:37:47,670
We also have the ability to make use of

01:37:47,670 --> 01:37:51,060
the atomic write capabilities through an F2FS,

01:37:51,060 --> 01:37:53,390
and I was going to say a lot about that

01:37:53,390 --> 01:37:55,660
and I was going to, and to my surprise,

01:37:55,660 --> 01:37:59,470
during the break, the author of F2FS,

01:37:59,470 --> 01:38:03,560
Jaegeuk Kim, presented himself. (laughs)

01:38:03,560 --> 01:38:06,133
I did not know he was here at the conference.

01:38:06,133 --> 01:38:07,880
But this is a really cool thing,

01:38:07,880 --> 01:38:09,553
it only works on Linux.

01:38:09,553 --> 01:38:11,320
It's really fast.

01:38:13,585 --> 01:38:15,200
The anecdotal reports I've received are that

01:38:15,200 --> 01:38:17,470
if you take an old Android phone that used

01:38:17,470 --> 01:38:20,898
to be running EXT4 and you reflash it to use F2FS,

01:38:20,898 --> 01:38:25,040
it suddenly starts seeming like a perky new phone.

01:38:26,120 --> 01:38:28,160
It makes a significant difference.

01:38:28,160 --> 01:38:31,820
And so, you know, if there was some way to,

01:38:31,820 --> 01:38:34,400
and the way we interface with that right now

01:38:34,400 --> 01:38:36,450
is there's a bunch of goofy IOCTLs

01:38:36,450 --> 01:38:38,736
that we have to call to check to see

01:38:38,736 --> 01:38:40,570
if the device is capable of it and then

01:38:40,570 --> 01:38:42,300
to control the transactions.

01:38:43,170 --> 01:38:45,210
But it really would be nice if there was

01:38:45,210 --> 01:38:48,550
a generic way to do this with any file system

01:38:48,550 --> 01:38:51,760
that supported it.

01:38:51,760 --> 01:38:55,100
Other things to discuss, yeah we really like reliable ways

01:38:55,100 --> 01:38:56,960
to discover properties of the file system.

01:38:56,960 --> 01:39:01,680
An example is when you create a new file on disk

01:39:01,680 --> 01:39:03,570
and you write to it and fsync it,

01:39:03,570 --> 01:39:05,990
do you also need to open the directory

01:39:05,990 --> 01:39:08,150
that contains that file and fsync that directory

01:39:08,150 --> 01:39:10,150
to make sure that the file doesn't end up in lost

01:39:10,150 --> 01:39:11,600
and found after a power loss?

01:39:12,780 --> 01:39:15,110
And is that a file system-dependent thing

01:39:15,110 --> 01:39:17,545
or is it always true that we have to do that

01:39:17,545 --> 01:39:19,160
or what's the answer to that?

01:39:19,160 --> 01:39:20,860
Right now, we assume that you have to do that

01:39:20,860 --> 01:39:22,770
and we're actually opening the directory

01:39:22,770 --> 01:39:24,370
and we're fsyncing the directory after

01:39:24,370 --> 01:39:27,680
at any time we create a file, but if we could avoid doing

01:39:27,680 --> 01:39:30,060
that it would be really cool and things would go faster.

01:39:30,060 --> 01:39:33,030
- I just wanted to ask, did you have any information,

01:39:33,030 --> 01:39:34,510
any hints, that made you think that

01:39:34,510 --> 01:39:36,160
it might be file system-dependent,

01:39:36,160 --> 01:39:38,060
that you don't have to know that?

01:39:38,060 --> 01:39:40,850
- I don't know because maybe with journaling modes

01:39:40,850 --> 01:39:44,380
or mount options or something that it would just work.

01:39:44,380 --> 01:39:45,860
I don't know.

01:39:45,860 --> 01:39:47,140
I'm user space.

01:39:47,140 --> 01:39:48,610
Am I out of time already?

01:39:48,610 --> 01:39:51,156
Have I already used my five minutes?

01:39:51,156 --> 01:39:52,157
- Go ahead.

01:39:52,157 --> 01:39:53,283
- Okay, thank you.

01:39:53,283 --> 01:39:54,980
Yes Shawn.

01:39:54,980 --> 01:39:56,920
- Or we can take it offline.

01:39:56,920 --> 01:39:57,753
- [Presenter] Yes.

01:39:58,638 --> 01:39:59,930
- Yeah, probably that.

01:39:59,930 --> 01:40:00,974
If we can take it offline. - Yeah, let's just take

01:40:00,974 --> 01:40:02,083
it offline, I've got a whole list of things

01:40:02,083 --> 01:40:03,187
and then of course the F barrier

01:40:03,187 --> 01:40:05,190
that we talked about before and an idea of

01:40:05,190 --> 01:40:07,550
a lot of times we have big holes in

01:40:07,550 --> 01:40:10,640
the file where we, it's places we don't care about.

01:40:10,640 --> 01:40:12,270
And we could tell the operating system,

01:40:12,270 --> 01:40:17,180
hey, this big chunk of 64k, we don't care about,

01:40:17,180 --> 01:40:18,760
you can make it a hole in the file system

01:40:18,760 --> 01:40:21,010
where no allocated space and if it returns zeros,

01:40:21,010 --> 01:40:21,990
we don't care.

01:40:21,990 --> 01:40:23,930
Would that be useful to the file system?

01:40:23,930 --> 01:40:25,810
If so, we need a way to tell

01:40:25,810 --> 01:40:27,270
the file system about that hole.

01:40:27,270 --> 01:40:29,810
And so, in summary, here's the resources,

01:40:29,810 --> 01:40:33,020
there's a briefing paper there with about two hours worth

01:40:33,020 --> 01:40:36,360
of discussion topics, and that's how you get

01:40:36,360 --> 01:40:39,800
in touch with me, and we should really stay in touch

01:40:39,800 --> 01:40:42,880
so that it, SQLite has always worked really,

01:40:42,880 --> 01:40:44,330
really well on Linux.

01:40:45,930 --> 01:40:47,830
Let's work together to keep it that way.

01:40:51,256 --> 01:40:52,089
(clapping) Comments.

01:40:55,340 --> 01:40:57,210
Question or should we go on immediately to the next talk?

01:40:57,210 --> 01:41:00,247
- Go for questions and comments,

01:41:00,247 --> 01:41:03,679
solutions that exist.

01:41:03,679 --> 01:41:05,640
- FS capabilities.

01:41:05,640 --> 01:41:08,370
- Yeah, just the ability to get detailed FS capabilities

01:41:08,370 --> 01:41:10,840
and I get specific questions in the briefing paper.

01:41:10,840 --> 01:41:13,370
- So the thing is that file systems,

01:41:13,370 --> 01:41:16,210
so there is POSIX which basically mandates

01:41:16,210 --> 01:41:18,390
that you have to call everything on the directory, yeah?

01:41:18,390 --> 01:41:19,420
- [Presenter] Yeah.

01:41:19,420 --> 01:41:23,910
- And generally, file system developers do not want

01:41:23,910 --> 01:41:27,880
to give you more guarantees because

01:41:27,880 --> 01:41:29,898
it ties their hands a bit. - Sure.

01:41:29,898 --> 01:41:34,376
- And so because there are other people who are pushing

01:41:34,376 --> 01:41:39,200
the other way around, like give me maximum performance

01:41:39,200 --> 01:41:43,200
and I don't care about all these details of consistency,

01:41:43,200 --> 01:41:45,580
I can devise it myself.

01:41:45,580 --> 01:41:48,510
So currently we are kind of middle ground

01:41:48,510 --> 01:41:51,850
so we promise what the posted mandates and so.

01:41:51,850 --> 01:41:54,595
So you are required to have seen the directory,

01:41:54,595 --> 01:41:58,050
on the other hand, the file system somehow magically

01:41:58,050 --> 01:42:01,480
made sure, for example, in EXT4, okay,

01:42:01,480 --> 01:42:04,340
we do fsync for you in fact under the hood

01:42:04,340 --> 01:42:06,500
because there is lots of broken applications

01:42:06,500 --> 01:42:08,038
that wouldn't work otherwise. - Sure.

01:42:08,038 --> 01:42:12,270
- And but the second, I think, you will issue if you are

01:42:12,270 --> 01:42:14,730
a good guy is going to be pretty cheap

01:42:14,730 --> 01:42:16,800
because we already record when we are

01:42:16,800 --> 01:42:19,220
for our last fsync then VCO,

01:42:19,220 --> 01:42:21,470
there is nothing to do, - Okay.

01:42:21,470 --> 01:42:22,770
- just bail out.

01:42:22,770 --> 01:42:24,890
So the second fsync is going to be pretty cheap too.

01:42:24,890 --> 01:42:25,723
- [Man] Other questions,

01:42:25,723 --> 01:42:27,200
we're out of time. - If there's current activity.

01:42:27,200 --> 01:42:28,810
- [Man] Yeah, yeah, if there is no concurrent activity.

01:42:28,810 --> 01:42:31,100
- [Presenter] Yeah, yeah, so this is the sort

01:42:31,100 --> 01:42:33,310
of thing I need to hear, I need authoritatively

01:42:33,310 --> 01:42:36,033
from people like you who know.

01:42:36,033 --> 01:42:37,132
- Yeah.

01:42:37,132 --> 01:42:38,132
- [Presenter] Thank you.

01:42:38,132 --> 01:42:40,455
- [Man] Is there a way to detect

01:42:40,455 --> 01:42:43,140
if we need a thing or not if you're doing that (mumbles)?

01:42:43,140 --> 01:42:44,613
- Sorry?

01:42:44,613 --> 01:42:45,827
- [Man] Is there a way to detect that

01:42:45,827 --> 01:42:46,886
from the application?

01:42:46,886 --> 01:42:50,096
- No, no way to detect and we don't want to give you away.

01:42:50,096 --> 01:42:51,060
(group laughing)

01:42:51,060 --> 01:42:54,377
Because that would kind of support the brokenness, yeah?

01:42:54,377 --> 01:42:56,913
(laughing) - Because it said you have to.

01:42:56,913 --> 01:42:58,231
- Yeah.

01:42:58,231 --> 01:43:00,820
- I guess not so long ago there was

01:43:00,820 --> 01:43:04,010
a patch on Fsinfo exposing that.

01:43:04,010 --> 01:43:06,500
I don't think it's been merged but that

01:43:06,500 --> 01:43:10,210
at least exposed if it was a network file system,

01:43:11,100 --> 01:43:15,350
it was in that spec, is how much do you want to extend it?

01:43:19,640 --> 01:43:20,473
Just a thing.

01:43:22,510 --> 01:43:24,640
- [Man] Throw it.

01:43:25,770 --> 01:43:28,810
- Just quickly, I think Falicake and there's

01:43:28,810 --> 01:43:31,770
a Falicake core which can actually dig holes in files

01:43:31,770 --> 01:43:34,610
and send it or give the file system a hint

01:43:34,610 --> 01:43:37,750
that actually that you don't care about that data anymore.

01:43:37,750 --> 01:43:39,580
I don't know whether you necessarily want to do that

01:43:39,580 --> 01:43:41,020
because it may have ramifications

01:43:41,020 --> 01:43:43,850
if you ever need to do IO to that.

01:43:43,850 --> 01:43:45,780
- [Presenter] Yeah, yeah, I would have ultimately want

01:43:45,780 --> 01:43:46,740
to write to it again

01:43:46,740 --> 01:43:47,573
in the future. - Yeah.

01:43:47,573 --> 01:43:49,487
- [Presenter] And at that point, yeah, I'd want you

01:43:49,487 --> 01:43:50,949
to fill it back in, it's just it,

01:43:50,949 --> 01:43:51,782
right now, I don't need it.

01:43:51,782 --> 01:43:53,113
- Okay.

01:43:53,113 --> 01:43:54,000
- [Presenter] And I didn't know if it might be useful

01:43:54,000 --> 01:43:55,390
to the file system or not.

01:43:55,390 --> 01:43:56,223
- Okay.

01:43:57,358 --> 01:43:58,849
- [Man] It is 'cause you start training the box

01:43:58,849 --> 01:44:00,291
for flash, sorry. - Yeah.

01:44:00,291 --> 01:44:02,227
Yeah, but-- - It is useful because then

01:44:02,227 --> 01:44:05,080
the file system can start doing trims

01:44:05,080 --> 01:44:06,290
on those blocks instead of having

01:44:06,290 --> 01:44:07,780
to keep them maintained in the box.

01:44:07,780 --> 01:44:10,570
- [Presenter] Sure, but if it's not not

01:44:10,570 --> 01:44:12,720
a flash file system, if it--

01:44:12,720 --> 01:44:13,870
- Their case, actually,

01:44:13,870 --> 01:44:15,640
I guess SSDS. - Yeah right, I guess--

01:44:15,640 --> 01:44:17,160
- SSDS trim happily. - Yeah, SSDS.

01:44:17,160 --> 01:44:17,993
Right, SSDs.

01:44:18,994 --> 01:44:20,910
But if it's like EXT4 with extents and stuff,

01:44:20,910 --> 01:44:23,887
you don't want to go punching a whole bunch of holes in it

01:44:23,887 --> 01:44:26,270
because so I guess it to be the file systems of anything,

01:44:26,270 --> 01:44:27,930
how do I know?

01:44:27,930 --> 01:44:29,870
- Yeah, I guess it doesn't pay off really.

01:44:29,870 --> 01:44:31,670
[Presenter] You don't think it's worth doing?

01:44:31,670 --> 01:44:33,390
- No, I don't think it's worth doing.

01:44:33,390 --> 01:44:36,111
So what file system could do conceivably is

01:44:36,111 --> 01:44:39,440
it could mark blocks as unwritten,

01:44:39,440 --> 01:44:43,437
so basically saying these blocks have unknown contents.

01:44:43,437 --> 01:44:47,440
So if the file system like punching holes

01:44:47,440 --> 01:44:48,410
is one option for this,

01:44:48,410 --> 01:44:50,614
- Yeah. - but then that's sort

01:44:50,614 --> 01:44:52,231
of expensive because you have to free blocks

01:44:52,231 --> 01:44:53,306
and when you reuse, you have - Yeah.

01:44:53,306 --> 01:44:54,307
- to allocate blocks.

01:44:54,307 --> 01:44:55,430
It fragments the file, it fragmented the extent three,

01:44:56,620 --> 01:44:59,740
really performance down. (laughs)

01:44:59,740 --> 01:45:01,070
- Yep, yep. - Performance is going

01:45:01,070 --> 01:45:02,430
to be down.

01:45:02,430 --> 01:45:05,050
But cheaper way would be to just mark the expenses

01:45:05,050 --> 01:45:07,521
and written, so basically we don't know the contents.

01:45:07,521 --> 01:45:09,435
- Yeah, that'd be fine. - And when you write to it,

01:45:09,435 --> 01:45:11,420
we just mark them as written again.

01:45:11,420 --> 01:45:13,670
- Sure. - That's that's much cheaper--

01:45:13,670 --> 01:45:15,920
- [Presenter] And is it useful, that's the question.

01:45:15,920 --> 01:45:17,900
- But the question is like what would you get for

01:45:17,900 --> 01:45:19,770
it because the space is still allocated

01:45:19,770 --> 01:45:22,198
so from file system point of view, there is no advantage.

01:45:22,198 --> 01:45:26,500
You could trim, but honestly, given the state

01:45:26,500 --> 01:45:28,941
of trim if current devices isn't worth it either.

01:45:28,941 --> 01:45:30,620
- Oh my. (laughing)

01:45:30,620 --> 01:45:32,490
- Like unless you are trimming gigabytes,

01:45:32,490 --> 01:45:34,560
it's not worth it. - Okay, right.

01:45:34,560 --> 01:45:36,914
- That's true. - That's wonderful

01:45:36,914 --> 01:45:39,395
information, this is exactly what I'm looking for,

01:45:39,395 --> 01:45:40,410
I have about a dozen other questions like this on

01:45:40,410 --> 01:45:41,580
the briefing paper.

01:45:41,580 --> 01:45:42,560
- Yeah. (laughs)

01:45:42,560 --> 01:45:44,790
- [Presenter] Please hunt me up and tell me the things.

01:45:45,999 --> 01:45:49,082
We're gonna move on to the next talk.

01:45:50,262 --> 01:45:52,059
Do you want the mic or do you wanna have a...

01:45:52,059 --> 01:45:53,220
- [Man] You need a lot of volume before trim

01:45:53,220 --> 01:45:54,921
gets to be useful.

01:45:54,921 --> 01:45:56,004
- [Guy] Yeah.

01:45:57,159 --> 01:45:57,992
- Yeah, but with handheld.

01:45:59,390 --> 01:46:00,637
- Yeah.

01:46:00,637 --> 01:46:02,851
- There's our reason. - It's fine.

01:46:02,851 --> 01:46:03,855
- [Man] We're good?

01:46:03,855 --> 01:46:06,433
- It does not, okay, this thing, she might.

01:46:12,134 --> 01:46:13,920
- [Man] Is trimming a more useful system?

01:46:13,920 --> 01:46:16,022
- That's here to me, that's why it sounds.

01:46:16,022 --> 01:46:17,238
- Great. (men speaking off mic)

01:46:17,238 --> 01:46:20,266
- [Man] She's lost a few pages from...

01:46:20,266 --> 01:46:21,298
- [Guy] Is it cracking from it because then

01:46:21,298 --> 01:46:22,516
it doesn't show any. (mumbles) - Good device,

01:46:22,516 --> 01:46:23,590
we'll just throw it away

01:46:23,590 --> 01:46:27,490
when we can. - Yeah.

01:46:27,490 --> 01:46:29,294
(people speaking off mic) - Yeah.

01:46:29,294 --> 01:46:30,627
- And actually can't. - If this changes in

01:46:30,627 --> 01:46:34,450
the future, y'all need to let me know.

01:46:34,450 --> 01:46:37,480
- It doesn't recognize. - You see what I'm saying?

01:46:37,480 --> 01:46:38,630
Technology is evolving.

01:46:40,472 --> 01:46:41,390
- [Man] You want my work now?

01:46:42,680 --> 01:46:45,530
- [Guy] Yeah, sure, there's now, look at (mumbles).

01:46:45,530 --> 01:46:47,830
- [Man] Yeah but this is initially,

01:46:50,080 --> 01:46:51,917
I don't see it changing in the near future.

01:46:51,917 --> 01:46:53,670
Either it can but...

01:46:59,370 --> 01:47:01,070
Extremely on with all cases - Yeah.

01:47:01,070 --> 01:47:02,889
- [Man] they will be always painful.

01:47:02,889 --> 01:47:05,306
(man laughs)

01:47:08,550 --> 01:47:10,030
- Unfortunately, there's a problem

01:47:10,030 --> 01:47:11,870
with my computer not recognizing it but

01:47:11,870 --> 01:47:14,420
so I can just start talking without that.

01:47:16,271 --> 01:47:18,390
I don't know what to do right now about it.

01:47:18,390 --> 01:47:21,850
What I always want to complain about is that at the moment,

01:47:21,850 --> 01:47:24,800
it's very, the durability situation around Postgres

01:47:24,800 --> 01:47:28,990
is very around Linux, it's very hard.

01:47:30,410 --> 01:47:34,310
Every different file system has different behavior

01:47:34,310 --> 01:47:36,830
and the general error handling behavior

01:47:36,830 --> 01:47:38,230
is just not documented.

01:47:38,230 --> 01:47:40,680
Let me just send him the slide so it's easier.

01:47:56,052 --> 01:47:57,744
- Just upload in on the screen.

01:47:57,744 --> 01:48:00,820
- Oh, that's better.

01:48:00,820 --> 01:48:01,653
Sorry for that.

01:48:22,137 --> 01:48:24,220
- It's not working, okay.

01:48:26,870 --> 01:48:31,870
- Okay.

01:48:38,975 --> 01:48:41,510
My one big problem is that basically

01:48:41,510 --> 01:48:45,560
all Ciscos don't document what happens in our case.

01:48:45,560 --> 01:48:48,251
Some of them have started to add some minimal documentation

01:48:48,251 --> 01:48:53,251
but usually it's either incorrect or out-of-date,

01:48:53,290 --> 01:48:56,180
like fsync doesn't even have any documentation what kind

01:48:56,180 --> 01:49:01,180
of errors can happen, and I think that makes

01:49:01,570 --> 01:49:03,670
it really hard for user space applications

01:49:04,554 --> 01:49:06,490
to actually write correct code.

01:49:06,490 --> 01:49:08,569
And I think that's also one reason

01:49:08,569 --> 01:49:10,400
why you see all these applications doing different things

01:49:10,400 --> 01:49:12,670
for durability because there's no documentation

01:49:12,670 --> 01:49:14,880
what you're supposed to do and the POSIX guarantees

01:49:14,880 --> 01:49:16,750
are completely vague. - Yeah.

01:49:16,750 --> 01:49:18,440
- Like there's no way that - Thank you.

01:49:18,440 --> 01:49:20,590
- everyone is interpreting the same way.

01:49:20,590 --> 01:49:22,620
- So that exactly where this came from, yeah?

01:49:22,620 --> 01:49:26,650
The thing is that the standards basically define

01:49:26,650 --> 01:49:29,730
what should happen when things are normal,

01:49:29,730 --> 01:49:33,140
and then that actually starts don't quite define

01:49:33,140 --> 01:49:34,940
the fsync semantics yet.

01:49:34,940 --> 01:49:38,390
Because it's like durability and stuff like that

01:49:38,390 --> 01:49:41,840
is very out of what POSIX tries to define.

01:49:41,840 --> 01:49:43,950
- Right. - So even less

01:49:43,950 --> 01:49:45,560
the error handling like when

01:49:45,560 --> 01:49:48,210
the hardware does something our of generic.

01:49:48,210 --> 01:49:51,450
So that's where there is no standardization

01:49:51,450 --> 01:49:54,580
and it's where everything has grown kind of odd.

01:49:54,580 --> 01:49:56,390
- But then I think Linux should start

01:49:56,390 --> 01:49:59,720
to have some documentation - Oh, I share your pain.

01:49:59,720 --> 01:50:01,309
- that says (audio cuts out).

01:50:01,309 --> 01:50:02,491
(group laughing)

01:50:02,491 --> 01:50:04,594
At the very least, you need to do these and these steps

01:50:04,594 --> 01:50:06,260
and they might be unnecessary for some file systems

01:50:06,260 --> 01:50:07,910
but if so you're on your own

01:50:07,910 --> 01:50:09,930
and you better read the Kernel code.

01:50:09,930 --> 01:50:13,225
Even if that seems to be the absolute minimum

01:50:13,225 --> 01:50:17,440
for to get anywhere, and the second related problem

01:50:17,440 --> 01:50:19,910
is that even if a lot of the durability operations

01:50:19,910 --> 01:50:22,726
that are very important to get performance impose on Linux,

01:50:22,726 --> 01:50:24,930
have warnings like this

01:50:24,930 --> 01:50:27,850
which basically says never ever use this,

01:50:27,850 --> 01:50:29,590
but when you complain about performance,

01:50:29,590 --> 01:50:31,310
they say use this.

01:50:31,310 --> 01:50:34,260
Like sync file range's documentation literally says,

01:50:34,260 --> 01:50:35,990
"This system quote's extremely dangerous

01:50:35,990 --> 01:50:37,640
"and should not be used in portable programs."

01:50:37,640 --> 01:50:39,620
I mean, the portal program part, okay,

01:50:39,620 --> 01:50:41,968
I get that because it's Linux-specific

01:50:41,968 --> 01:50:44,730
but it's the answer to solving a lot

01:50:44,730 --> 01:50:47,851
of performance problems but it's extremely dangerous.

01:50:47,851 --> 01:50:51,220
You gotta make a call which one you mean, at some point.

01:50:51,220 --> 01:50:54,340
- Yeah so probably it should be rephrased like

01:50:54,340 --> 01:50:56,935
because so the burning is there because

01:50:56,935 --> 01:51:01,880
it is really not useful for achieving any kind

01:51:01,880 --> 01:51:05,730
of durability or data integrity, yeah.

01:51:05,730 --> 01:51:10,180
It's really useful for early flashing of data just

01:51:10,180 --> 01:51:12,730
as a kind of, it can be considered to hint

01:51:12,730 --> 01:51:15,010
to the Kernel that maybe to do something.

01:51:15,010 --> 01:51:17,490
Like in practice, it happens, in practice,

01:51:17,490 --> 01:51:20,310
the Kernel will do something but the durability

01:51:20,310 --> 01:51:22,938
is not really guaranteed after this,

01:51:22,938 --> 01:51:23,940
this is called complete.

01:51:23,940 --> 01:51:25,320
And that's why there is this big fat warning us,

01:51:25,320 --> 01:51:27,470
we don't want to deal with users who use this

01:51:27,470 --> 01:51:29,420
and say oh, where is my data, you know?

01:51:29,420 --> 01:51:31,102
- Right, right. (man laughs)

01:51:31,102 --> 01:51:32,920
But at the same time, you provided it for a reason

01:51:32,920 --> 01:51:35,010
and how are you supposed to actually use

01:51:35,010 --> 01:51:37,989
it safely without reading the Kernel code?

01:51:37,989 --> 01:51:39,805
And that just seems like--

01:51:39,805 --> 01:51:41,076
- [Man] I agree with rephrasing it.

01:51:41,076 --> 01:51:42,000
(group laughs)

01:51:42,000 --> 01:51:44,570
- And one other very big problem is that

01:51:44,570 --> 01:51:46,670
the error behavior between different parts

01:51:47,648 --> 01:51:48,481
of Linux is completely different,

01:51:48,481 --> 01:51:50,900
depending on which file system you use,

01:51:50,900 --> 01:51:52,800
you get either the old version after

01:51:53,671 --> 01:51:55,343
an error of the page after an error

01:51:55,343 --> 01:51:57,410
or the new version of a page after an error.

01:51:57,410 --> 01:52:00,370
And if it's S, you might not see the error at all

01:52:01,891 --> 01:52:03,010
because it will be happen at close.

01:52:04,077 --> 01:52:05,450
There needs to be some documentation

01:52:05,450 --> 01:52:07,820
of what users can expect and what you users need

01:52:07,820 --> 01:52:10,990
to be checking because otherwise they'll never,

01:52:10,990 --> 01:52:13,990
you can't really complain about people writing crappy code

01:52:13,990 --> 01:52:16,830
if that's all the guidance people have.

01:52:16,830 --> 01:52:18,160
And I think it's really problematic

01:52:18,160 --> 01:52:19,750
for user space developers,

01:52:19,750 --> 01:52:21,730
and it also makes your life harder

01:52:22,947 --> 01:52:24,400
because then we do something

01:52:24,400 --> 01:52:26,200
and then you afterwards complain,

01:52:26,200 --> 01:52:27,050
how did you get the idea that that was

01:52:27,050 --> 01:52:28,000
a safe thing to do?

01:52:28,000 --> 01:52:31,857
But it's the only way we can only guess

01:52:31,857 --> 01:52:35,410
and interpret the results of other operating systems

01:52:35,410 --> 01:52:38,680
or from Linux 2.6 or whatever.

01:52:38,680 --> 01:52:40,340
- But how deep do you want that to go?

01:52:40,340 --> 01:52:44,160
I mean, the file system implements are hampered by what

01:52:44,160 --> 01:52:46,720
the block drivers give them, the block drivers

01:52:46,720 --> 01:52:49,310
and hampered by what the device actually implements

01:52:49,310 --> 01:52:51,440
and the device semantics vary so much.

01:52:51,440 --> 01:52:55,220
So how deep can you try and assemble complete behavior?

01:52:55,220 --> 01:52:57,720
- I mean, I don't think it has to be complete behavior.

01:52:57,720 --> 01:53:01,050
And I think you can't really expect the block driver

01:53:01,050 --> 01:53:05,470
to fix, or the block layer, to fix that device lying

01:53:05,470 --> 01:53:06,730
to you about its drive cache.

01:53:06,730 --> 01:53:09,790
I mean, there's no way Linux can fix that really,

01:53:09,790 --> 01:53:12,710
except for having like an endless blacklist or something.

01:53:12,710 --> 01:53:16,710
So that part, I don't really blame Linux for that.

01:53:16,710 --> 01:53:21,310
But saying that hey, if you want that durability guarantee,

01:53:21,310 --> 01:53:24,010
you need to do this operation and then all

01:53:24,010 --> 01:53:26,440
the file systems behave in a similar way,

01:53:27,440 --> 01:53:32,440
and that the block layer will return errors in

01:53:32,590 --> 01:53:34,660
a consistent way instead of like right now,

01:53:34,660 --> 01:53:37,104
you get a completely different error,

01:53:37,104 --> 01:53:38,400
if there's a thin provisioning target below it,

01:53:38,400 --> 01:53:40,449
all the guarantees are off

01:53:40,449 --> 01:53:43,108
and you get spaces from random operation

01:53:43,108 --> 01:53:45,659
that aren't returning in a space by documentation.

01:53:45,659 --> 01:53:47,239
It's really just hard for us. - So let's take

01:53:47,239 --> 01:53:49,860
the read after write issue.

01:53:49,860 --> 01:53:52,510
Presuming you understood the right cache

01:53:52,510 --> 01:53:55,321
is turned on or turned off and it's actually gonna go

01:53:55,321 --> 01:53:56,975
to the media, the power goes off.

01:53:56,975 --> 01:53:58,861
Once upon a time, people believed that there was enough

01:53:58,861 --> 01:54:00,140
of juice for it to finish writing the sector,

01:54:00,140 --> 01:54:02,400
there isn't any more.

01:54:02,400 --> 01:54:04,760
So halfway through a sector, the power goes off,

01:54:04,760 --> 01:54:06,940
that sector has been written off now.

01:54:06,940 --> 01:54:08,830
The next time you read it, there's gonna be an error.

01:54:08,830 --> 01:54:12,280
That's true for some of rotating media,

01:54:12,280 --> 01:54:14,980
it's not true for SSDs, it's not true for Flash,

01:54:14,980 --> 01:54:16,990
it's not true for networking file systems,

01:54:16,990 --> 01:54:18,870
so the varying device semantics,

01:54:18,870 --> 01:54:22,796
you're gonna make a particular guarantee very hard to make.

01:54:22,796 --> 01:54:25,000
- I mean, that part is is problematic

01:54:25,000 --> 01:54:27,560
but I think that's really not the kind

01:54:27,560 --> 01:54:28,920
of guarantee I'm looking for.

01:54:28,920 --> 01:54:29,753
- [Man] Okay.

01:54:29,753 --> 01:54:31,430
- What I'm thinking more is that hey,

01:54:31,430 --> 01:54:35,088
if you get an error about F from fsync,

01:54:35,088 --> 01:54:38,130
what happens if you retry the fsync?

01:54:38,130 --> 01:54:40,560
Will it actually retry the data right

01:54:40,560 --> 01:54:42,610
or will it just throw away the data?

01:54:42,610 --> 01:54:44,960
It's the latter kind of somewhat in the moment.

01:54:46,393 --> 01:54:47,226
But that's not documented anywhere except

01:54:47,226 --> 01:54:50,600
if you read some ELKML discussions,

01:54:50,600 --> 01:54:52,910
and that kind of thing needs to be documented in my opinion.

01:54:52,910 --> 01:54:55,260
- So you're looking for documented failure semantics

01:54:55,260 --> 01:54:57,320
which is the place that POSIX is really weak?

01:54:57,320 --> 01:54:58,753
- [Man] Yeah.

01:54:58,753 --> 01:54:59,586
- Okay. - Yeah.

01:54:59,586 --> 01:55:01,150
And, I think the other side of that coin

01:55:01,150 --> 01:55:04,540
is documented ways to do things in a safe manner

01:55:04,540 --> 01:55:08,250
because like that's also just a guessing game right now.

01:55:08,250 --> 01:55:10,650
We're talking to Kernel developers over a beer

01:55:10,650 --> 01:55:11,900
which is nice, (man chuckles)

01:55:11,900 --> 01:55:14,190
but hard to do remotely.

01:55:16,950 --> 01:55:21,140
I mean, that's this part here where there's no guidance how

01:55:23,270 --> 01:55:25,760
to write safe code that is durable.

01:55:25,760 --> 01:55:28,930
And whatever you do, somebody will complain.

01:55:28,930 --> 01:55:31,417
But if you complain about performance,

01:55:31,417 --> 01:55:33,350
somebody will complain that we do too much.

01:55:33,350 --> 01:55:34,920
If we complain about data loss,

01:55:34,920 --> 01:55:37,399
somebody will complain that we don't do enough

01:55:37,399 --> 01:55:41,050
and the opinions will contradict each other wildly

01:55:41,050 --> 01:55:46,050
and that also then leads to you not being able

01:55:46,410 --> 01:55:47,610
to write code that is performant

01:55:47,610 --> 01:55:50,464
because we all have, all applications

01:55:50,464 --> 01:55:53,390
have different performance characteristics depending

01:55:53,390 --> 01:55:55,860
on what kind of solution we implemented.

01:55:55,860 --> 01:56:00,860
And one extreme example is by the definition of some,

01:56:02,280 --> 01:56:06,590
or but the documented opinions of by some developers,

01:56:06,590 --> 01:56:08,150
that's what you kind of need to do to do

01:56:08,150 --> 01:56:11,900
a safe atomic rename and it's like kind of insane.

01:56:13,301 --> 01:56:15,140
But is it the right way?

01:56:15,140 --> 01:56:17,060
I mean, you can't really interpret it out

01:56:17,060 --> 01:56:19,240
of POSIX what exactly you need to do,

01:56:19,240 --> 01:56:20,550
at least not by my reading.

01:56:20,550 --> 01:56:22,920
I try to reason about the standard there.

01:56:24,860 --> 01:56:26,750
I don't know whether that is the right thing to do

01:56:26,750 --> 01:56:30,220
but it's what Postgres now does forever, for everything.

01:56:30,220 --> 01:56:33,019
But it's like hard to precisely.

01:56:33,019 --> 01:56:36,210
And if it's required, then please document that somewhere.

01:56:37,050 --> 01:56:37,960
- [Man] Why are you (speaking off mic)?

01:56:38,820 --> 01:56:40,800
- Because otherwise, depending on the file system,

01:56:40,800 --> 01:56:43,850
you actually lose the file or they'll get

01:56:43,850 --> 01:56:47,150
the old file name back or something.

01:56:47,150 --> 01:56:50,187
- Well, I think the fsync new file maybe is not needed but--

01:56:50,187 --> 01:56:53,900
- Wait, why do you need to fsync all of them?

01:56:53,900 --> 01:56:58,900
- Because otherwise you file directly might not be under

01:56:59,470 --> 01:57:00,470
the old name safely.

01:57:02,640 --> 01:57:04,690
- I mean. this behavior is actually based

01:57:04,690 --> 01:57:08,230
on actual testing of crashes of the database, right?

01:57:08,230 --> 01:57:13,160
So we've been actually doing power loss tests on the code

01:57:13,160 --> 01:57:17,048
and this is the thing that actually finally fixed that.

01:57:17,048 --> 01:57:17,881
(group laughing)

01:57:17,881 --> 01:57:19,780
So is it really needed?

01:57:19,780 --> 01:57:21,760
I mean, I don't know.

01:57:21,760 --> 01:57:23,590
It works on an EXT4.

01:57:23,590 --> 01:57:28,590
It works on XFS, does it work on other file systems?

01:57:29,880 --> 01:57:31,730
I don't know.

01:57:31,730 --> 01:57:35,620
I mean, it finally made the data all go away, right?

01:57:35,620 --> 01:57:39,230
So we, that's the main reason why we do that.

01:57:40,986 --> 01:57:42,800
- Okay then.

01:57:42,800 --> 01:57:44,833
Frankly, I'm not very convinced that

01:57:44,833 --> 01:57:46,995
a lot of critics stop people doing the (mumbles)

01:57:46,995 --> 01:57:51,678
by just trying to use their code to make (mumbles).

01:57:51,678 --> 01:57:53,490
Some of it's built between.

01:57:53,490 --> 01:57:54,920
It has not. - I mean, there has--

01:57:54,920 --> 01:57:58,110
- It's also what, it makes some sense that you need

01:57:58,110 --> 01:57:59,970
to have for an atomic that you need

01:57:59,970 --> 01:58:02,670
the old content to be durable before you can rely

01:58:02,670 --> 01:58:05,670
on either seeing either the before state or the after state.

01:58:07,761 --> 01:58:10,589
So that's why you need to--

01:58:10,589 --> 01:58:12,763
- Let's say, from a database point of view,

01:58:12,763 --> 01:58:14,716
it can update some value from one to and,

01:58:14,716 --> 01:58:16,053
you can update from two to three

01:58:16,053 --> 01:58:18,380
and other in between to make sure the two is durable.

01:58:18,380 --> 01:58:21,522
So we don't need the old way to be durable before

01:58:21,522 --> 01:58:24,061
to be able to turn in.

01:58:24,061 --> 01:58:25,230
- Yeah. (man speaking off mic)

01:58:25,230 --> 01:58:29,780
I guess what will be good, so I completely agree

01:58:29,780 --> 01:58:33,796
that we are lacking any documentation in this sense

01:58:33,796 --> 01:58:38,069
and I guess file system people would be willing

01:58:38,069 --> 01:58:41,000
to kind of improve the situation.

01:58:41,000 --> 01:58:44,190
But like maybe could you come up with

01:58:44,190 --> 01:58:45,500
a concrete concerns like you have

01:58:45,500 --> 01:58:49,390
what do I need to consistently create file

01:58:49,390 --> 01:58:51,930
and file in a directory?

01:58:51,930 --> 01:58:55,190
What need I do to do atomic renaming?

01:58:55,190 --> 01:58:58,620
Now, what happens when there is a IO error during fsync,

01:58:58,620 --> 01:59:01,010
am I getting the IO consistently?

01:59:01,010 --> 01:59:02,970
What will happen with the data?

01:59:02,970 --> 01:59:05,460
What guarantees are you willing to give me

01:59:05,460 --> 01:59:08,828
and bring this up on FS development list

01:59:08,828 --> 01:59:11,460
and we could, I guess...

01:59:14,842 --> 01:59:17,556
- [Man] But you went ahead and answered by continuing

01:59:17,556 --> 01:59:19,490
to talk, never mind. (group laughing)

01:59:19,490 --> 01:59:22,995
- So we could take it from there and probably create

01:59:22,995 --> 01:59:25,690
a file in (mumbles) documentation directory,

01:59:25,690 --> 01:59:27,910
at least in a show that's probably the most,

01:59:27,910 --> 01:59:30,510
where we will document at least

01:59:30,510 --> 01:59:32,500
the currently we are willing to give.

01:59:32,500 --> 01:59:37,363
And for example, yes, how the hell do I reliably create

01:59:38,560 --> 01:59:40,710
a file, is a good question and I guess,

01:59:40,710 --> 01:59:42,960
and if I was a system developer.

01:59:42,960 --> 01:59:45,010
What you have is probably what you need to do, that's good.

01:59:45,010 --> 01:59:47,176
With the rename it's kind of sad.

01:59:47,176 --> 01:59:49,565
(group laughing)

01:59:49,565 --> 01:59:51,950
- Yeah. - And I guess there would

01:59:51,950 --> 01:59:54,650
be other file system people as well interested

01:59:54,650 --> 01:59:57,461
in digging quite so much is needed because I,

01:59:57,461 --> 02:00:02,130
at least, one of the fsyncs for EXT4 and XFS,

02:00:02,130 --> 02:00:05,920
I don't should be needed, but maybe,

02:00:05,920 --> 02:00:08,720
even I as a guy who buys this stuff,

02:00:08,720 --> 02:00:10,720
wrote this stuff, missed something so...

02:00:12,048 --> 02:00:14,408
- I mean, the fsync all files stuff is only needed in

02:00:14,408 --> 02:00:17,920
the case you haven't previously fsynced under that file name

02:00:17,920 --> 02:00:19,150
and the only reason we need to is that

02:00:19,150 --> 02:00:21,170
is because we do some of that during crash recovery

02:00:21,170 --> 02:00:23,730
where you can guarantee that we've previously fsynced it.

02:00:23,730 --> 02:00:26,410
So if you-- - Yeah, but it's (audio cuts).

02:00:26,410 --> 02:00:31,410
Yeah so, yeah, if it bring these concrete questions

02:00:32,260 --> 02:00:36,960
and we can create a document which will kind

02:00:36,960 --> 02:00:39,550
of at least answer some of these.

02:00:39,550 --> 02:00:40,730
- [Man] Can you tell me again exactly

02:00:40,730 --> 02:00:43,360
which mailing list we send these concrete questions to?

02:00:43,360 --> 02:00:44,230
- Yes sure.

02:00:44,230 --> 02:00:45,780
Linux, FS Double.

02:00:48,100 --> 02:00:49,010
(man speaking off mic)

02:00:49,010 --> 02:00:54,010
Yeah, Double, at VR (mumbles) yeah, that's the stand out.

02:00:58,600 --> 02:01:00,898
- Do you have a concrete list of operations

02:01:00,898 --> 02:01:02,990
that you already have identified?

02:01:02,990 --> 02:01:06,610
- I mean, I can come up with one fairly easily.

02:01:06,610 --> 02:01:08,160
- Yeah, if you can come together with

02:01:08,160 --> 02:01:10,530
the sensiblest of questions, that's great.

02:01:10,530 --> 02:01:13,120
Because we as file system developers,

02:01:13,120 --> 02:01:15,498
for asked lot of things is kind of obvious

02:01:15,498 --> 02:01:20,498
and we don't have, yeah.

02:01:22,308 --> 02:01:23,953
- [Presenter] Sorry for that.

02:01:23,953 --> 02:01:25,140
(group laughing)

02:01:25,140 --> 02:01:28,230
- And we kind of miss the questions because it's--

02:01:28,230 --> 02:01:29,570
- Yeah, it's obvious to you.

02:01:29,570 --> 02:01:32,100
- Yeah, it's obvious to us and we even don't think about

02:01:32,100 --> 02:01:36,160
these questions, like we think very, very low level

02:01:36,160 --> 02:01:39,650
at kind of like what this Cisco should currently,

02:01:39,650 --> 02:01:43,140
but you know, already they, what the user needs

02:01:44,096 --> 02:01:48,370
to do to actually useful operation for the application

02:01:48,370 --> 02:01:51,650
that's obviously set off several system calls usually,

02:01:51,650 --> 02:01:55,120
and then was the overall semantics is kind of unobvious.

02:01:56,212 --> 02:01:57,610
So and we are not used to,

02:01:57,610 --> 02:02:00,613
usually we don't think that high. (laughs)

02:02:00,613 --> 02:02:02,790
- Okay, I'll try to propose a list of questions

02:02:02,790 --> 02:02:06,070
and a CCU or somebody.

02:02:06,980 --> 02:02:09,240
- So two questions for you. (man speaking off mic)

02:02:09,240 --> 02:02:11,359
- Just anywhere now. - So one,

02:02:11,359 --> 02:02:14,331
are you looking to get kind of like

02:02:14,331 --> 02:02:18,780
the lowest common denominator of calls that you would need?

02:02:18,780 --> 02:02:20,880
Because obviously, different file systems

02:02:20,880 --> 02:02:22,190
are gonna behave differently.

02:02:23,525 --> 02:02:25,036
Some of them may not require that same incantation.

02:02:25,036 --> 02:02:26,650
Are you looking for kind of like worst case scenario,

02:02:26,650 --> 02:02:28,320
that's what you want.

02:02:28,320 --> 02:02:30,710
So even if you're running on a different file system,

02:02:30,710 --> 02:02:32,401
yeah you might be doing extra operations

02:02:32,401 --> 02:02:35,160
that are unnecessary for that file system

02:02:35,160 --> 02:02:36,820
but you're gonna do 'em just 'cause...

02:02:36,820 --> 02:02:37,900
- Yes basically. - Okay.

02:02:37,900 --> 02:02:40,280
- I mean, if there is a too large performance penalty,

02:02:40,280 --> 02:02:41,680
it's possible that you would

02:02:42,597 --> 02:02:43,640
to do something file system-dependent,

02:02:44,494 --> 02:02:46,600
but because all of this changes over the years so much

02:02:46,600 --> 02:02:51,400
and we constantly run on old Kernel versions,

02:02:51,400 --> 02:02:52,950
Kernel versions with a lot of back parts,

02:02:52,950 --> 02:02:54,330
it's just, in my opinion,

02:02:54,330 --> 02:02:57,720
unfeasible to maintain that except

02:02:57,720 --> 02:03:00,430
for maybe the most extreme performance regressions.

02:03:00,430 --> 02:03:02,680
So I really want to have the common denominator

02:03:02,680 --> 02:03:05,830
and then also ensure by that new file systems

02:03:05,830 --> 02:03:08,260
have somewhere where they can align their semantics to.

02:03:08,260 --> 02:03:10,160
Because one problem is that if somebody writes

02:03:10,160 --> 02:03:12,610
a new file system, they just come up with their semantics

02:03:12,610 --> 02:03:14,650
from scratch and that usually means

02:03:14,650 --> 02:03:16,434
that they're completely broken (group laughing)

02:03:16,434 --> 02:03:18,682
and then somebody reports the obvious bug,

02:03:18,682 --> 02:03:20,641
and then that got fixed and then all

02:03:20,641 --> 02:03:21,930
the unobvious bugs comes out over

02:03:21,930 --> 02:03:23,160
the next 10 years. - Sure.

02:03:23,160 --> 02:03:25,790
And the second question I had is,

02:03:25,790 --> 02:03:29,850
assuming a world where you got what you wanted from fsync,

02:03:29,850 --> 02:03:31,450
you got an error condition,

02:03:31,450 --> 02:03:34,300
can you gracefully recover from that?

02:03:34,300 --> 02:03:37,040
- Yeah, we can do crash recovery and just restart

02:03:37,040 --> 02:03:39,630
the database, replay from the journal and go from there.

02:03:39,630 --> 02:03:43,060
- Because obviously, the failure for fsync could vary.

02:03:43,060 --> 02:03:46,325
I mean, it could be like hey, the disk I was going

02:03:46,325 --> 02:03:47,890
to is gone and it's never coming back.

02:03:49,054 --> 02:03:50,612
- I mean, in that case, obviously,

02:03:50,612 --> 02:03:53,800
it can't really recover because there's no data to recover.

02:03:53,800 --> 02:03:58,800
But it at least allows us to, in the transient phase

02:03:58,860 --> 02:04:01,401
which actually, these days, is common due

02:04:01,401 --> 02:04:03,770
to like network issues and so on,

02:04:03,770 --> 02:04:05,630
you can safely recover, and in other cases,

02:04:05,630 --> 02:04:07,490
the database crashes and doesn't come up,

02:04:07,490 --> 02:04:09,520
and then you can do automated failover.

02:04:09,520 --> 02:04:12,460
So that allows that too but not being able

02:04:12,460 --> 02:04:13,293
to detect them, - Okay.

02:04:13,293 --> 02:04:14,980
- the failures that actually occur,

02:04:16,115 --> 02:04:18,420
makes it worse because the data will stay up,

02:04:18,420 --> 02:04:21,616
lose data, and nobody will be the wiser.

02:04:21,616 --> 02:04:23,760
- Yeah. - And that part actually leads

02:04:23,760 --> 02:04:26,078
into the first part that, at the moment,

02:04:26,078 --> 02:04:27,750
we don't have a way to detect what kind

02:04:27,750 --> 02:04:29,160
of failures we're running into

02:04:29,160 --> 02:04:32,220
because you can get an IO error

02:04:32,220 --> 02:04:34,400
and you don't know whether it's transient,

02:04:34,400 --> 02:04:36,050
you don't know whether there will be

02:04:36,050 --> 02:04:40,530
an IO error somewhere else on the file system,

02:04:40,530 --> 02:04:42,360
even if we just had something like

02:04:42,360 --> 02:04:45,084
if you do this start FS or something

02:04:45,084 --> 02:04:46,830
and get an error count that we can monitor

02:04:46,830 --> 02:04:49,290
to say hey, has this error count increased,

02:04:49,290 --> 02:04:53,310
and if it has increased, we can shut down or something.

02:04:53,310 --> 02:04:55,290
Even that would be a huge progress,

02:04:55,290 --> 02:04:57,910
because at the moment, basically you fail over solutions,

02:04:57,910 --> 02:05:01,040
what they have to do is to build lock scraping facilities

02:05:01,040 --> 02:05:05,390
that monitor the Kernel output and try

02:05:05,390 --> 02:05:07,850
to parse whether those are fatal issues,

02:05:07,850 --> 02:05:09,840
which is really hard because sometimes you'll

02:05:11,011 --> 02:05:13,460
get eight reset messages that are completely harmless,

02:05:13,460 --> 02:05:16,418
and sometimes they mean everything's bad.

02:05:16,418 --> 02:05:19,500
Which that can't be a sensible approach

02:05:19,500 --> 02:05:23,610
that every site implements their own Kernel lock parser

02:05:23,610 --> 02:05:26,180
and your risk ticks to detect whether there's something bad.

02:05:26,180 --> 02:05:29,550
And I think there Linux needs to provide something

02:05:29,550 --> 02:05:31,700
that allows applications to determine that.

02:05:33,636 --> 02:05:36,265
(man speaking off mic)

02:05:36,265 --> 02:05:37,098
- Okay.

02:05:38,190 --> 02:05:41,120
There are way to get it from disks.

02:05:41,120 --> 02:05:42,900
There's no way to find out what happened on

02:05:42,900 --> 02:05:44,600
the other end of a network connection.

02:05:46,370 --> 02:05:49,590
Flushes, at least the way that doing SATR or SAS emulation,

02:05:49,590 --> 02:05:50,790
you can get information.

02:05:51,640 --> 02:05:54,279
Getting it from the file system's gonna be hard

02:05:54,279 --> 02:05:56,191
because file systems don't typically deal

02:05:56,191 --> 02:05:58,221
with things like smart data.

02:05:58,221 --> 02:05:59,483
They don't deal with the ability

02:05:59,483 --> 02:06:01,132
to identify what devices they're being laid out

02:06:01,132 --> 02:06:03,840
on without you doing some ex cathedra inspection.

02:06:03,840 --> 02:06:05,590
So there is information available

02:06:05,590 --> 02:06:08,310
from devices figuring out how to get to the device

02:06:08,310 --> 02:06:10,440
and get the information, and more importantly,

02:06:10,440 --> 02:06:11,690
whether it's recoverable.

02:06:11,690 --> 02:06:13,420
You know, SATR drives go offline,

02:06:13,420 --> 02:06:15,580
until you cycle power, they're not gonna come back.

02:06:15,580 --> 02:06:17,410
Is that recoverable or isn't it?

02:06:18,830 --> 02:06:22,050
- I mean, if that kind of error,

02:06:22,050 --> 02:06:23,540
I don't really care whether it's recoverable

02:06:23,540 --> 02:06:25,570
because I will involve and admin anyway.

02:06:26,571 --> 02:06:27,404
So if I market it as non-recoverable,

02:06:27,404 --> 02:06:28,990
like if I treat it as an unrecoverable error,

02:06:28,990 --> 02:06:30,630
it's completely fine.

02:06:30,630 --> 02:06:33,610
The only case where I want to know whether

02:06:33,610 --> 02:06:36,030
I can just, like the important thing is whether,

02:06:36,030 --> 02:06:38,709
no, just retrying the right or the fsync,

02:06:38,709 --> 02:06:42,030
whether that has a chance of success,

02:06:42,030 --> 02:06:44,900
that is somewhat important because then--

02:06:44,900 --> 02:06:47,810
- So you'd like the E hopeful error return?

02:06:47,810 --> 02:06:48,643
- [Presenter] Say again.

02:06:48,643 --> 02:06:51,010
- You'd like the E hopeful error return it's like--

02:06:51,010 --> 02:06:51,843
- [Presenter] Kinda.

02:06:51,843 --> 02:06:52,740
- Instead of something went wrong

02:06:52,740 --> 02:06:54,960
and we can't tell you anymore, it's--

02:06:54,960 --> 02:06:56,402
(man speaks off mic)

02:06:56,402 --> 02:06:57,761
- But it's, - Yeah.

02:06:57,761 --> 02:06:59,419
- it's more complicated, like it's not just that.

02:06:59,419 --> 02:07:00,977
I mean, even if it's just EIO,

02:07:00,977 --> 02:07:02,848
knowing that there hasn't been an EIO

02:07:02,848 --> 02:07:04,070
to an application on the same drive,

02:07:05,332 --> 02:07:06,620
that isn't my process, is really important

02:07:06,620 --> 02:07:08,450
because there's hot backup programs

02:07:08,450 --> 02:07:11,780
and if the hot backup program encountered the error,

02:07:11,780 --> 02:07:15,010
I won't see that as the database anymore.

02:07:15,010 --> 02:07:16,930
I need to be able to, as a database,

02:07:16,930 --> 02:07:18,670
to see, hey, something has been concurrent

02:07:18,670 --> 02:07:20,620
about data corruption issues.

02:07:21,460 --> 02:07:23,010
- So just like a SIGSEGV gives you some more information

02:07:23,010 --> 02:07:26,038
about the type of fault and maybe some information

02:07:26,038 --> 02:07:27,210
about where the fault was, you're looking

02:07:27,210 --> 02:07:29,660
for more information on IO errors in particular.

02:07:30,830 --> 02:07:31,990
Yeah. - Yeah, I don't need

02:07:31,990 --> 02:07:34,040
to know whether it was on block 17,

02:07:34,040 --> 02:07:37,230
or whether it was a I can't remap or anymore or whatever.

02:07:40,535 --> 02:07:41,368
- [Man] Sorry.

02:07:41,368 --> 02:07:44,120
(group laughing)

02:07:44,120 --> 02:07:47,372
- So what kind of error can get from fsync,

02:07:47,372 --> 02:07:50,240
what kind of error could happen that you get from fsync

02:07:50,240 --> 02:07:53,840
that you decide okay, now I can try to repeat the write?

02:07:53,840 --> 02:07:56,620
- I mean, like some operating systems just allow you

02:07:56,620 --> 02:07:57,980
to retry the fsync

02:07:57,980 --> 02:07:59,820
and the data will work, - Yes.

02:07:59,820 --> 02:08:00,870
- that's one thing. - Yes, you're ready

02:08:00,870 --> 02:08:02,830
to try after any error,

02:08:02,830 --> 02:08:05,112
but in what case you can not you try even

02:08:05,112 --> 02:08:06,566
if the disk was gone, but in

02:08:06,566 --> 02:08:07,399
what case it will make sense? - You can't,

02:08:07,399 --> 02:08:08,300
it will throw data away.

02:08:08,300 --> 02:08:10,294
- Yeah, yes I know.

02:08:10,294 --> 02:08:11,610
You can still try in the loop, I mean,

02:08:11,610 --> 02:08:13,950
you can try but after watch error,

02:08:13,950 --> 02:08:16,270
it makes sense to retry that hope that it'll succeed.

02:08:16,270 --> 02:08:18,630
- I mean, I'm more interested in that there has been

02:08:18,630 --> 02:08:22,400
and error that wasn't caught with Postgres,

02:08:22,400 --> 02:08:24,300
because I won't be able to see errors

02:08:24,300 --> 02:08:26,150
that are caught by other applications.

02:08:26,150 --> 02:08:31,011
- Yes, so my question is, (men speaking off mic)

02:08:31,011 --> 02:08:33,230
with example of an error after which it makes sense

02:08:33,230 --> 02:08:34,740
to try hoping for a success.

02:08:34,740 --> 02:08:36,710
Or no matter where there are

02:08:36,710 --> 02:08:39,068
the best thing you can do is to crash

02:08:39,068 --> 02:08:40,230
and then just recover. - I mean, it's network.

02:08:40,230 --> 02:08:42,442
If it's a transient network issue

02:08:42,442 --> 02:08:44,520
then it can't and the Kernel handled that in a way

02:08:44,520 --> 02:08:46,770
that is retryable, that would be relevant, because

02:08:46,770 --> 02:08:49,182
that happens regularly. - Yeah but then it should

02:08:49,182 --> 02:08:50,750
be handled not by you but by the Kernel there.

02:08:50,750 --> 02:08:52,810
The FS, it should have been. - So I think from now on,

02:08:52,810 --> 02:08:54,930
we're running a bit short on time

02:08:54,930 --> 02:08:59,713
so can we write those up as questions for the FS developers

02:08:59,713 --> 02:09:02,991
and continue on later on, be like one.

02:09:02,991 --> 02:09:05,112
(group laughing)

02:09:05,112 --> 02:09:05,945
- I do my maths.

02:09:05,945 --> 02:09:06,820
Right, I'll get started.

02:09:06,820 --> 02:09:08,584
- Yes please, James.

02:09:08,584 --> 02:09:09,810
- Hi, okay.

02:09:09,810 --> 02:09:13,370
So this is a tool we've implemented at MongoDB

02:09:13,370 --> 02:09:17,196
and we're just trying to look for feedback here on how

02:09:17,196 --> 02:09:21,510
and if it could be useful for you as well.

02:09:21,510 --> 02:09:24,730
So I'm gonna talk about the goals and the profiler.

02:09:24,730 --> 02:09:27,960
I'm just gonna show a screenshot of the profiler first,

02:09:27,960 --> 02:09:30,720
then a quick look at the design.

02:09:30,720 --> 02:09:33,600
I'm gonna cover an example and then like

02:09:33,600 --> 02:09:35,100
to get some feedback from you.

02:09:36,170 --> 02:09:39,350
Right, goal, the goal intersects actually our pain point

02:09:39,350 --> 02:09:42,000
which is the fact that we would like to see

02:09:42,964 --> 02:09:47,964
a 100% thread profile of our server, our MongoDB server.

02:09:49,460 --> 02:09:52,680
We also want to see in a time variant way,

02:09:52,680 --> 02:09:55,724
as opposed to, for example, the standard

02:09:55,724 --> 02:10:00,724
on CPU analysis tools like Perf or off CPU analysis

02:10:01,260 --> 02:10:05,270
which is now a part of BPF tools

02:10:05,270 --> 02:10:08,840
and it's on Brendan Greggs's own homepage as well.

02:10:09,910 --> 02:10:14,890
That would give us information, for example,

02:10:14,890 --> 02:10:17,240
a 10-second, 20-second static view.

02:10:18,130 --> 02:10:23,130
We're trying to catch transient sporadic events to happen,

02:10:23,600 --> 02:10:25,040
for example, over a long time,

02:10:25,040 --> 02:10:27,770
so we want to get something dynamic,

02:10:27,770 --> 02:10:30,300
for example, sampled every one second.

02:10:32,050 --> 02:10:35,350
One use case here is a customer, for example,

02:10:35,350 --> 02:10:39,939
that comes back to us and says I had a performance issue,

02:10:39,939 --> 02:10:44,939
that happens once a day or once every three days

02:10:45,000 --> 02:10:46,760
and lasts 10 seconds.

02:10:46,760 --> 02:10:48,710
So instead of rushing it every time

02:10:48,710 --> 02:10:52,710
the problem happens or overly instrumenting the server,

02:10:52,710 --> 02:10:55,120
we want to have something that's low overhead

02:10:55,120 --> 02:10:57,420
with low data generation.

02:10:57,420 --> 02:10:59,540
That can capture all the information we need

02:10:59,540 --> 02:11:03,660
to start our analysis and then analyze it after the fact.

02:11:05,100 --> 02:11:08,630
This is what the profile looks like and really,

02:11:08,630 --> 02:11:11,590
the thread profile section is what we're talking about.

02:11:11,590 --> 02:11:15,360
Not sure if it reads well out from out there.

02:11:15,360 --> 02:11:19,280
We also have a top section, it's called service status here

02:11:19,280 --> 02:11:22,670
that covers some metrics.

02:11:22,670 --> 02:11:24,130
We actually have hundreds

02:11:24,130 --> 02:11:27,900
of service status diagnostic data metrics,

02:11:27,900 --> 02:11:30,140
and they are built-in metrics that we instrument

02:11:30,140 --> 02:11:33,190
the server to get some diagnostic information

02:11:33,190 --> 02:11:37,290
to help us resolve the performance issues that we see.

02:11:37,290 --> 02:11:39,940
The reason why we like the thread profiler

02:11:39,940 --> 02:11:44,940
is that it can give us even more insight

02:11:46,060 --> 02:11:49,880
on what's happening on the server

02:11:49,880 --> 02:11:52,660
and this goes beyond what we already instrument

02:11:52,660 --> 02:11:56,510
and gives us also view on what happens at the Kernel level.

02:11:56,510 --> 02:11:57,860
Now, a quick view at this.

02:11:57,860 --> 02:12:01,710
This is just a really, a synthetic example.

02:12:04,440 --> 02:12:08,620
From time A, we see a drop in the throughput, right?

02:12:08,620 --> 02:12:10,270
And this is a command throughput.

02:12:11,243 --> 02:12:14,880
Anyway, it means that this probably is a performance issue.

02:12:14,880 --> 02:12:18,140
The thread profile view, and I'd really caught this

02:12:19,041 --> 02:12:20,400
because it tends to be quite verbose,

02:12:21,494 --> 02:12:25,940
in this case, it shows that some WiredTiger eviction threads

02:12:25,940 --> 02:12:30,050
are going from sleeping, so four threads sleeping,

02:12:30,050 --> 02:12:32,660
and by the way, the blue line means that

02:12:32,660 --> 02:12:34,610
the threads are off CPU.

02:12:34,610 --> 02:12:37,530
And then, from time A,

02:12:37,530 --> 02:12:40,320
the performance degradation correlates

02:12:40,320 --> 02:12:45,320
with more activity in eviction threads.

02:12:45,720 --> 02:12:47,200
Now you see blue and red.

02:12:47,200 --> 02:12:51,970
The blue is the off CPU, the off CPU time,

02:12:51,970 --> 02:12:54,800
the red is the sampled on CPU time.

02:12:54,800 --> 02:12:57,470
This is a call three.

02:12:57,470 --> 02:13:01,430
So the ancestor is the caller and then it goes down

02:13:01,430 --> 02:13:03,840
to the leave and then you can see that

02:13:03,840 --> 02:13:08,840
for call threads ultimately it gets back to the scheduler.

02:13:10,600 --> 02:13:15,430
Now, just a quick view at the design.

02:13:15,430 --> 02:13:17,930
It's conceptually very simple.

02:13:17,930 --> 02:13:22,250
It uses two K probes, schedule and finish task switch.

02:13:22,250 --> 02:13:25,590
They're marked blue because that's off CPU.

02:13:25,590 --> 02:13:27,730
It basically tracks the time that when

02:13:27,730 --> 02:13:31,070
a thread goes off CPU, which is scheduled,

02:13:31,070 --> 02:13:33,580
it's been scheduled back on the waiting queue,

02:13:33,580 --> 02:13:36,020
and finish task which is the time when

02:13:36,020 --> 02:13:38,620
it's due back on CPU.

02:13:38,620 --> 02:13:40,750
We have this Perf sample event

02:13:40,750 --> 02:13:45,750
which samples on CPU threads stack traces.

02:13:47,690 --> 02:13:51,450
Every once in a while, this is really,

02:13:51,450 --> 02:13:54,970
this side is similar to what Perf record already does.

02:13:54,970 --> 02:13:58,240
And we have an eBPF program that does

02:13:58,240 --> 02:14:00,299
the aggregation at sorts.

02:14:00,299 --> 02:14:03,620
So it does it quite efficiently and then pumps

02:14:03,620 --> 02:14:06,260
a Perf ring buffer for the use

02:14:06,260 --> 02:14:10,230
of space side of the profiler to gather this information,

02:14:10,230 --> 02:14:14,460
sample it every one second, and admit the stack traces.

02:14:14,460 --> 02:14:16,010
So we get the profile and data.

02:14:16,979 --> 02:14:21,770
Now, this is pretty useful because

02:14:21,770 --> 02:14:25,720
it will help us understand and characterize

02:14:25,720 --> 02:14:30,720
a problem where there is CPU saturation, for example.

02:14:31,270 --> 02:14:33,900
But also, and perhaps more importantly,

02:14:33,900 --> 02:14:37,770
to understand when and characterize

02:14:37,770 --> 02:14:41,080
a problem that is caused by an off CPU,

02:14:42,125 --> 02:14:43,325
so time spent elsewhere.

02:14:44,290 --> 02:14:47,470
For example, on mutex contention.

02:14:47,470 --> 02:14:52,470
So poor concurrency, for example.

02:14:52,700 --> 02:14:57,700
Or something that's bound by something like

02:14:57,840 --> 02:15:01,090
the IO interconnect or the storage.

02:15:02,495 --> 02:15:04,045
And here's a practical example.

02:15:04,960 --> 02:15:09,960
So I reproduce a stall that we see on EXT4

02:15:10,340 --> 02:15:14,430
with very slow storage, and by very slow storage here,

02:15:14,430 --> 02:15:19,430
I actually mean a storage whose throughput capacity,

02:15:20,530 --> 02:15:25,530
sorry, workload, where the amount of dirty data exceeds

02:15:28,180 --> 02:15:30,200
the storage capacity to flush it.

02:15:31,710 --> 02:15:35,230
I made this example very explicit, I mean,

02:15:35,230 --> 02:15:38,500
and dramatic as well, so I put a workload

02:15:38,500 --> 02:15:40,370
that really exceeds the capacity.

02:15:41,580 --> 02:15:46,580
So what we see here with our instrumentation,

02:15:46,650 --> 02:15:50,050
the diagnostic data that already exists

02:15:50,050 --> 02:15:54,170
is really a gap, right?

02:15:54,170 --> 02:15:57,020
From A to C, so for about three minutes,

02:15:57,020 --> 02:15:58,510
we get no information.

02:15:58,510 --> 02:16:01,990
So in this case, yeah, we probably have

02:16:01,990 --> 02:16:04,010
some other cloud monitoring tools

02:16:04,010 --> 02:16:07,490
that will give us some more diagnostic information

02:16:07,490 --> 02:16:11,460
but from this specialized standpoint, we have a gap.

02:16:12,401 --> 02:16:17,401
We also know, this is YSESP workload

02:16:17,410 --> 02:16:21,116
that externally by checking the throughput,

02:16:21,116 --> 02:16:23,910
the actual operational style happens

02:16:23,910 --> 02:16:26,589
from B to C so this side here.

02:16:26,589 --> 02:16:31,589
And here it's just likely that our thread

02:16:32,160 --> 02:16:37,160
that collects all the metrics has somehow stalled, right?

02:16:37,320 --> 02:16:40,340
So enter the thread profiler here.

02:16:41,250 --> 02:16:45,440
Now, I've been here just cutting and pasting some,

02:16:45,440 --> 02:16:48,310
so chopping up the thread profiler output just

02:16:48,310 --> 02:16:50,910
to make the interpretation easier,

02:16:50,910 --> 02:16:52,810
because otherwise, it's quite verbose.

02:16:54,250 --> 02:16:57,610
Anyway, what we can see here, and I'm not sure

02:16:57,610 --> 02:17:00,990
if you can read it over there,

02:17:00,990 --> 02:17:05,330
but there is our checkpoint thread that is doing,

02:17:05,330 --> 02:17:07,930
quite unsurprisingly, an F data sync.

02:17:07,930 --> 02:17:12,640
So the F data sync starts slightly before A,

02:17:12,640 --> 02:17:15,460
so slightly before the issue,

02:17:15,460 --> 02:17:19,070
and it ends just by the end of it so point C.

02:17:20,308 --> 02:17:23,210
Let's see what's happening on other threads.

02:17:24,530 --> 02:17:27,420
Now, at the time where our metrics stall,

02:17:28,310 --> 02:17:33,310
we see that a call trace that is doing a,

02:17:34,700 --> 02:17:37,592
it's calling to the open system call,

02:17:37,592 --> 02:17:40,810
it's somehow waiting on the journal, right?

02:17:40,810 --> 02:17:43,780
And the EXT4 journal JBD2.

02:17:43,780 --> 02:17:46,740
And I marked that over there.

02:17:48,128 --> 02:17:51,090
There is something more over there that just below in

02:17:51,090 --> 02:17:55,840
the stack frame that says that we're actually blocking

02:17:55,840 --> 02:17:59,320
on add transaction credits, we'll get to there in a second.

02:18:02,380 --> 02:18:05,340
Right now, something that happens before

02:18:05,340 --> 02:18:07,350
the actual operational stall.

02:18:08,780 --> 02:18:11,040
We see that one WiredTiger thread,

02:18:12,271 --> 02:18:17,271
that's eviction thread, is doing a write system call, right?

02:18:18,380 --> 02:18:20,060
That means they'll probably,

02:18:20,060 --> 02:18:22,940
it's evicting a page that was found to be dirty

02:18:22,940 --> 02:18:26,500
so it's writing that back.

02:18:26,500 --> 02:18:30,740
Again, the eviction seems, the thread

02:18:30,740 --> 02:18:33,440
as doing eviction seems to go off CPU

02:18:33,440 --> 02:18:35,670
for the duration of the fsync.

02:18:35,670 --> 02:18:40,670
Again, the EXT4 journal seems to be related to this.

02:18:42,610 --> 02:18:44,900
Finally the operational stall.

02:18:45,950 --> 02:18:49,480
We look at the thread that's performing the operation,

02:18:49,480 --> 02:18:52,379
that's an update operation and if we dig down,

02:18:52,379 --> 02:18:57,000
it's actually trying to page in some data

02:18:58,310 --> 02:18:59,540
at the WiredTiger level.

02:18:59,540 --> 02:19:02,420
And for some reason, it's unable to.

02:19:02,420 --> 02:19:07,420
And in fact, the source, the actual symbolized traces

02:19:10,310 --> 02:19:13,290
that we're spinning, we're actually spinning with

02:19:13,290 --> 02:19:15,643
the back half, so we're spending most of

02:19:15,643 --> 02:19:17,700
the time off CPU on LIBC select.

02:19:17,700 --> 02:19:20,300
And the most likely reason for this stall

02:19:20,300 --> 02:19:25,236
is that this .4 depends on three where

02:19:25,236 --> 02:19:29,150
that very page was being evicted,

02:19:29,150 --> 02:19:32,540
and now it's pending on the EXT4 journal.

02:19:32,540 --> 02:19:36,480
So what this means is really

02:19:36,480 --> 02:19:39,240
that we've reconstructed a timeline of the issue.

02:19:40,220 --> 02:19:42,500
We probably haven't caused it,

02:19:42,500 --> 02:19:45,320
but now we characterize how the system degrades

02:19:45,320 --> 02:19:50,320
as the problem keeps mounting up.

02:19:50,810 --> 02:19:55,810
I'm by no means an EXT4 expert, however,

02:19:57,760 --> 02:19:59,570
by looking at the documentation,

02:19:59,570 --> 02:20:04,570
I see that the JBD2 journal start call can actually,

02:20:05,270 --> 02:20:09,435
and will indeed, sleep if we're over committing

02:20:09,435 --> 02:20:10,268
the journal buffer.

02:20:10,268 --> 02:20:13,260
So in this case, with all of this in mind,

02:20:13,260 --> 02:20:17,580
my theory is that the F data sync is causing

02:20:17,580 --> 02:20:21,900
a lot of write activity, that's, again,

02:20:21,900 --> 02:20:24,110
saturating the storage.

02:20:24,110 --> 02:20:27,100
And this is somehow impairing the ability

02:20:27,100 --> 02:20:30,620
of the journal to commit and flush the data off its buffer.

02:20:30,620 --> 02:20:33,780
So this over-commissioning, it's causing

02:20:33,780 --> 02:20:38,380
the WiredTiger eviction to stall.

02:20:39,790 --> 02:20:42,370
This is a pretty simple example, it's not surprising.

02:20:42,370 --> 02:20:46,050
I mean, we're operating with,

02:20:47,330 --> 02:20:49,810
we're pretty much using more resources than

02:20:49,810 --> 02:20:52,190
the specs of the machine, right?

02:20:53,880 --> 02:20:57,050
So yeah, I'd like to wrap up on this.

02:20:57,050 --> 02:21:00,130
We find this to be a very promising tool.

02:21:01,280 --> 02:21:05,160
Our plan is to, well, validate this scale

02:21:05,160 --> 02:21:06,810
because we would like to get the,

02:21:08,320 --> 02:21:11,150
to make sure that the performance impact here

02:21:11,150 --> 02:21:13,840
is kept within reasonable bounds

02:21:13,840 --> 02:21:17,670
and we're aiming at somewhere around up

02:21:17,670 --> 02:21:22,670
to 5% performance impact on CPU-bound workloads.

02:21:24,720 --> 02:21:25,580
That's it, yeah.

02:21:25,580 --> 02:21:30,297
Off CPU analysis can help explain a lot of issues.

02:21:31,530 --> 02:21:34,882
I am positive that can probably give some input on

02:21:34,882 --> 02:21:37,550
the issue we're talking about earlier

02:21:37,550 --> 02:21:41,937
on where XFS seems to be faster,

02:21:43,160 --> 02:21:48,160
it seems to be slower when you do a number of writes,

02:21:52,650 --> 02:21:55,690
and it happens to be faster when, to these writes,

02:21:55,690 --> 02:21:58,690
we also add a sort of synthetic bug is right

02:21:58,690 --> 02:22:00,540
and that doesn't really add up.

02:22:01,706 --> 02:22:06,397
So my hunch is that probably, with approach like this,

02:22:06,397 --> 02:22:11,397
you can figure out how the EXT4 or

02:22:12,620 --> 02:22:15,680
the block device or whatever is downstream

02:22:15,680 --> 02:22:18,150
in terms of the Kernel side,

02:22:19,145 --> 02:22:21,190
it's spending its time both on and off CPU.

02:22:21,190 --> 02:22:25,216
And this is something we can not see very easily doing

02:22:25,216 --> 02:22:29,220
a Perf record or with traditional methods

02:22:29,220 --> 02:22:34,040
like GDB stack traces, sorry, GDB sampling.

02:22:35,130 --> 02:22:38,170
You would get an overhead

02:22:38,170 --> 02:22:40,370
that will probably change the overall shape.

02:22:42,420 --> 02:22:43,730
(man speaking off mic)

02:22:43,730 --> 02:22:46,380
No, at least not at the moment.

02:22:46,380 --> 02:22:49,850
Fundamentally, it's very simple to implement really.

02:22:49,850 --> 02:22:54,610
We just shoehorned the off CPU analysis tool

02:22:54,610 --> 02:22:58,534
that Brendan Gregg has and yeah, we added a sampler.

02:22:58,534 --> 02:23:02,680
Instead of aggregating all this data and outputting

02:23:02,680 --> 02:23:06,720
that at the very end, do that same task once a second.

02:23:07,960 --> 02:23:10,260
- [Man] So do you plan to open source it or...

02:23:13,070 --> 02:23:13,960
- I don't know.

02:23:13,960 --> 02:23:16,230
At this stage, we're really trying

02:23:16,230 --> 02:23:18,960
to validate it and understand

02:23:18,960 --> 02:23:21,700
if this works well with all the workloads

02:23:21,700 --> 02:23:23,944
for a sustained amount of time.

02:23:23,944 --> 02:23:28,580
Probably, yeah we haven't thought about this yet.

02:23:32,710 --> 02:23:35,059
- [Man] So I guess rather than just the open sourcing,

02:23:35,059 --> 02:23:40,059
is there any other information that was there,

02:23:40,066 --> 02:23:45,066
that would be useful to diagnose the problem

02:23:45,304 --> 02:23:47,116
or is there more information that should be collected

02:23:47,116 --> 02:23:52,116
than those traces, the analysis and fixing?

02:23:55,533 --> 02:23:59,526
- So I guess it's a good start, yeah?

02:23:59,526 --> 02:24:03,515
Like of course (audio cuts out) are ideas

02:24:03,515 --> 02:24:06,810
for useful information like you could,

02:24:06,810 --> 02:24:10,470
for example, what is very difficult to do back

02:24:10,470 --> 02:24:13,360
in cases like this is you have some process blocked

02:24:13,360 --> 02:24:16,436
on select because it is actually offloaded some work

02:24:16,436 --> 02:24:20,300
to another processing, it's now waiting for the result,

02:24:20,300 --> 02:24:23,980
and this is very difficult to trace through.

02:24:23,980 --> 02:24:28,980
So such a debug, it would be very useful

02:24:29,660 --> 02:24:31,840
if in a debug tool, you could see actually how

02:24:31,840 --> 02:24:34,920
to file descriptors of different processes are linked,

02:24:34,920 --> 02:24:36,790
so that if select waits somewhere,

02:24:36,790 --> 02:24:40,490
you could actually efficiently find out which are

02:24:40,490 --> 02:24:43,860
the other processes it is waiting on.

02:24:43,860 --> 02:24:47,070
- [Presenter] So like a dependency chain basically, right?

02:24:47,070 --> 02:24:47,903
- Yeah.

02:24:47,903 --> 02:24:50,310
- So understand straightaway who's the victim

02:24:50,310 --> 02:24:51,851
and what's the cause. - Yeah.

02:24:51,851 --> 02:24:52,850
- Yeah. - Yeah.

02:24:52,850 --> 02:24:54,570
- Yeah, that's a good point actually

02:24:54,570 --> 02:24:57,380
'cause probably one of the possible drawbacks,

02:24:57,380 --> 02:25:00,140
which we see in general with off CPU analysis,

02:25:00,140 --> 02:25:01,910
and this tool is no exception,

02:25:01,910 --> 02:25:04,260
is that the output is quite verbose.

02:25:04,260 --> 02:25:05,640
- Yeah. - So most of the time,

02:25:05,640 --> 02:25:08,340
the first approach you want to do is pattern matching.

02:25:08,340 --> 02:25:11,390
You know, when when this issue occurs.

02:25:11,390 --> 02:25:14,650
During this time frame, what are

02:25:14,650 --> 02:25:18,460
the thread profiles that stick out, right?

02:25:18,460 --> 02:25:20,390
But even in that case, there's a lot of,

02:25:20,390 --> 02:25:23,330
you know, it's initially some guesswork.

02:25:23,330 --> 02:25:25,710
So you have to lay out some hypothesis.

02:25:25,710 --> 02:25:28,450
Then what we ideally and mentally do

02:25:28,450 --> 02:25:33,120
is we use Kernel instrumentation like BPF trace

02:25:33,120 --> 02:25:34,770
as an example, right?

02:25:34,770 --> 02:25:39,050
But ideally, if we had some information

02:25:39,050 --> 02:25:41,470
as to what caused what, right?

02:25:41,470 --> 02:25:46,050
Maybe some say some errors that point to events,

02:25:46,050 --> 02:25:49,120
that would be ideal, we're not there yet.

02:25:49,120 --> 02:25:49,953
- [Man] No.

02:25:52,384 --> 02:25:53,384
- All right.

02:25:55,473 --> 02:25:58,556
- Okay, cool, is there anything else?

02:26:00,078 --> 02:26:02,492
Not much, anyway, Pawel.

02:26:02,492 --> 02:26:04,526
- Yep. - Up to you.

02:26:04,526 --> 02:26:07,276
(group clapping)

02:26:09,020 --> 02:26:11,370
- Okay, Pawel Oichawa from the Oracle.

02:26:11,370 --> 02:26:14,910
And I made some changes to the redo log in MySQL

02:26:14,910 --> 02:26:18,376
which I wanted to shortly present and then tell about

02:26:18,376 --> 02:26:23,376
the things that I'm looking for in that answers some hope.

02:26:24,801 --> 02:26:28,390
So basically, the new redo logo consists

02:26:29,780 --> 02:26:32,080
of two changes, major changes,

02:26:32,080 --> 02:26:33,240
that were applied to

02:26:33,240 --> 02:26:36,280
the previous redo log we had in the MySQL.

02:26:36,280 --> 02:26:39,620
The first thing is that previously,

02:26:39,620 --> 02:26:42,656
we had these locks from mutex which protected

02:26:42,656 --> 02:26:47,656
and serialized the rights to the redo log buffer,

02:26:47,950 --> 02:26:52,000
so it was all protected by the mutex and caused

02:26:52,000 --> 02:26:55,725
the contention on the locks mutex which was easily visible

02:26:55,725 --> 02:26:59,190
and via our 20 drink counters which we use

02:26:59,190 --> 02:27:00,890
to detect where is the contention,

02:27:01,990 --> 02:27:05,580
especially for LTP high workload.

02:27:07,291 --> 02:27:09,370
The second part of the changes is related

02:27:09,370 --> 02:27:11,320
to the background threads which we introduced,

02:27:11,320 --> 02:27:13,730
which take care of writing the redo log

02:27:13,730 --> 02:27:15,620
from the log buffer to this,

02:27:15,620 --> 02:27:18,690
to the file system cache and from there doing their things

02:27:18,690 --> 02:27:21,250
to get finally some durability.

02:27:21,250 --> 02:27:26,250
And of course, the last problem

02:27:30,227 --> 02:27:35,010
that we have to solve, it was to effectively wait on

02:27:36,816 --> 02:27:39,513
the reader being written or flushed during

02:27:39,513 --> 02:27:41,100
the transaction commit.

02:27:41,100 --> 02:27:42,940
So just to shortly described what was there.

02:27:42,940 --> 02:27:45,440
So previously there was a mutex.

02:27:45,440 --> 02:27:48,180
All the threads were fighting for the mutex

02:27:48,180 --> 02:27:50,800
and one that acquired could write to the log buffer

02:27:50,800 --> 02:27:53,260
and then it exchanged the mutex for

02:27:53,260 --> 02:27:56,983
the other mutex to do other part but still,

02:27:56,983 --> 02:28:01,983
before he finished, the next one could not acquire

02:28:03,250 --> 02:28:05,170
this mutex so it was just waiting here

02:28:05,170 --> 02:28:06,700
and then the next one was waiting here

02:28:06,700 --> 02:28:09,510
so the contention was basically around

02:28:09,510 --> 02:28:11,600
this lock system mutex here.

02:28:11,600 --> 02:28:13,960
And currently, what we did,

02:28:13,960 --> 02:28:16,060
is that we eliminated the lock system mutex

02:28:16,060 --> 02:28:18,740
and we allowed all the threads to concurrently write

02:28:18,740 --> 02:28:20,330
to the lock buffer.

02:28:20,330 --> 02:28:25,330
And then, they can do this stage in a different time

02:28:25,500 --> 02:28:27,588
so they can interleave with each other

02:28:27,588 --> 02:28:32,240
and then they start adding the dirty pages

02:28:32,240 --> 02:28:35,720
to the flush lists which must be preceded with

02:28:35,720 --> 02:28:37,370
the write to the redo log,

02:28:37,370 --> 02:28:41,770
because when you, I mean...

02:28:45,480 --> 02:28:47,320
Okay, everything I would describe,

02:28:47,320 --> 02:28:49,860
this is the mini transaction commit which

02:28:49,860 --> 02:28:52,112
is related to the operation when you want

02:28:52,112 --> 02:28:54,880
to atomically modify some dirty pages

02:28:54,880 --> 02:28:57,220
for which you need to first right a head lock

02:28:57,220 --> 02:28:59,860
and then ensure that that the pages are ready to flush.

02:28:59,860 --> 02:29:02,360
So before they are flushed, you have

02:29:02,360 --> 02:29:04,490
the redo log in the place.

02:29:06,100 --> 02:29:09,090
So how did we relax this order,

02:29:09,090 --> 02:29:12,460
because for some reason it was important.

02:29:12,460 --> 02:29:16,150
Yes, it was important and what we did was

02:29:16,150 --> 02:29:18,161
that we allowed, of course,

02:29:18,161 --> 02:29:20,910
to do these concurrent operations in parallel

02:29:20,910 --> 02:29:24,468
so they can be finished in different order

02:29:24,468 --> 02:29:26,370
in which they started.

02:29:27,510 --> 02:29:30,670
But then we'd allow just limited size of

02:29:30,670 --> 02:29:32,370
the window for depending operations

02:29:32,370 --> 02:29:35,710
which are not finished, and each operation which,

02:29:35,710 --> 02:29:38,450
after it's finished, reports that it's finished,

02:29:38,450 --> 02:29:40,190
so we can track up to which point

02:29:40,190 --> 02:29:43,200
of the operations finally are finished.

02:29:46,530 --> 02:29:49,170
So this is basically the idea that,

02:29:49,170 --> 02:29:53,040
instead of doing things in a serialized way,

02:29:53,040 --> 02:29:54,800
we can do things in parallel,

02:29:55,992 --> 02:29:59,370
not taking care about the order in which they are finished

02:29:59,370 --> 02:30:03,180
but just every doing such stuff in parallel needs

02:30:03,180 --> 02:30:04,970
to report when he finished.

02:30:06,260 --> 02:30:11,076
So this allows us to order the mini transaction commits,

02:30:11,076 --> 02:30:13,730
how they happen, a little bit differently

02:30:13,730 --> 02:30:15,610
if you look on how time progresses

02:30:15,610 --> 02:30:17,510
and how LSN value progresses.

02:30:17,510 --> 02:30:19,420
LSN is the sequence number which

02:30:19,420 --> 02:30:21,730
is increased every time you write to the redo log.

02:30:22,670 --> 02:30:24,960
So finally, they can be a little bit,

02:30:26,070 --> 02:30:28,140
the order is a little bit relaxed.

02:30:28,140 --> 02:30:30,800
So yeah, they can happen in any order,

02:30:30,800 --> 02:30:35,080
you can think, but if you look at the constraints,

02:30:35,080 --> 02:30:37,160
you will notice that the order is not

02:30:37,160 --> 02:30:42,070
as much relaxed as previously might think

02:30:42,070 --> 02:30:45,670
because we allowed threads just to do things in any order.

02:30:46,890 --> 02:30:49,540
The point is that the limited window of

02:30:49,540 --> 02:30:52,170
the pending operations, which are still not finished,

02:30:52,170 --> 02:30:54,690
this limitation allows us to have a guarantee

02:30:54,690 --> 02:30:57,960
that the destruction of the order is not that much.

02:31:01,560 --> 02:31:06,560
But to track which concurrent tasks have been finished,

02:31:07,832 --> 02:31:10,800
we need to have a special data structure which

02:31:10,800 --> 02:31:13,970
is implemented as the lock-free cyclic RI,

02:31:13,970 --> 02:31:16,900
and in that RI, we have all the tasks

02:31:16,900 --> 02:31:19,420
that are finished being reported.

02:31:19,420 --> 02:31:22,264
And the background thread needs to basically track up

02:31:22,264 --> 02:31:26,470
to which point all the concurrent tasks have been finished.

02:31:29,590 --> 02:31:32,302
So the first problem we have here

02:31:32,302 --> 02:31:34,900
is does this background thread, which for example

02:31:34,900 --> 02:31:36,290
is writing also the redo log.

02:31:36,290 --> 02:31:39,231
So it knows up to which point the redo log buffer

02:31:39,231 --> 02:31:41,718
is filled in even though the concurrent threads can fill

02:31:41,718 --> 02:31:46,230
the log buffer in different fragments concurrently,

02:31:46,230 --> 02:31:48,740
but they do report when they finished their parts.

02:31:50,144 --> 02:31:52,973
So because of this reported knowledge,

02:31:52,973 --> 02:31:57,973
the background thread can easily track how much

02:31:58,140 --> 02:32:03,070
of the redo is already prepared and do the next write.

02:32:03,070 --> 02:32:07,050
But after we moved the job of writing the redo log

02:32:07,050 --> 02:32:10,650
to the dedicated thread, there is a problem

02:32:10,650 --> 02:32:13,598
with waiting on the next job to be done.

02:32:13,598 --> 02:32:18,598
So for instance, the redo log thread needs to wait for

02:32:19,403 --> 02:32:21,750
the user threads to provide him next data

02:32:21,750 --> 02:32:23,610
to be written to the disk.

02:32:24,630 --> 02:32:28,560
So if we just follow the very basic idea

02:32:28,560 --> 02:32:30,670
that we can just wait on the event,

02:32:30,670 --> 02:32:32,616
I mean, on the condition of variable

02:32:32,616 --> 02:32:34,950
and just wake up the write of thread when

02:32:34,950 --> 02:32:37,350
the next data arrives, this introduces

02:32:37,350 --> 02:32:42,350
a very huge latency issue because waking up takes some time

02:32:43,785 --> 02:32:48,785
and it goes through if the log writer was waiting in

02:32:53,270 --> 02:32:56,110
the wait queue and you need to wait,

02:32:56,110 --> 02:32:58,590
any you wanted to wake up, it will not be

02:32:58,590 --> 02:33:01,590
that instant and, in comparison,

02:33:01,590 --> 02:33:04,320
if you just do the spin loop and let him

02:33:04,320 --> 02:33:07,670
just wait actively doing the busy wait,

02:33:07,670 --> 02:33:11,839
then it can just start as soon as

02:33:11,839 --> 02:33:15,450
the data arrived to the log buffer.

02:33:15,450 --> 02:33:18,540
So basically we have to introduce some kind

02:33:18,540 --> 02:33:20,740
of strategy for waiting.

02:33:21,710 --> 02:33:24,470
And the strategy we introduced was that

02:33:24,470 --> 02:33:26,980
the threads start with the spin loop,

02:33:26,980 --> 02:33:31,980
because if the data is expected to be soon,

02:33:32,990 --> 02:33:36,660
that this will be the most efficient way.

02:33:36,660 --> 02:33:38,260
But after some limit is reached,

02:33:39,481 --> 02:33:41,690
we switch to the waiting on the condition variable

02:33:41,690 --> 02:33:43,260
with a minimum timeout value.

02:33:44,180 --> 02:33:47,196
So, in this case, we could be woken up

02:33:47,196 --> 02:33:50,459
by any thread that wanted us, for example,

02:33:50,459 --> 02:33:51,950
to write the redo log to disk.

02:33:53,903 --> 02:33:58,083
And each time, the timeout is small so even

02:34:00,268 --> 02:34:05,080
if there is no reason to write the log buffer to disk,

02:34:05,080 --> 02:34:06,560
we can just try to see if there

02:34:06,560 --> 02:34:08,780
is anything we can proceed with

02:34:08,780 --> 02:34:12,770
so we wake up on the small timeout.

02:34:12,770 --> 02:34:15,810
But then if nothing happens and the server is little,

02:34:15,810 --> 02:34:17,830
it would waste a lot of CPU time

02:34:17,830 --> 02:34:19,720
so basically every few wait,

02:34:19,720 --> 02:34:22,769
we increase the time out, like multiply it by two

02:34:22,769 --> 02:34:27,769
to find the proper mechanism which balances between how

02:34:29,822 --> 02:34:34,810
the latency is slow decreased and how the CPU

02:34:34,810 --> 02:34:36,780
is being used too much.

02:34:38,150 --> 02:34:41,150
So the questions have here

02:34:41,150 --> 02:34:44,100
is how to do this adaptive waiting a better way.

02:34:45,580 --> 02:34:48,690
Because this is just a approach we come up

02:34:48,690 --> 02:34:51,760
with after we noticed that the spinning

02:34:51,760 --> 02:34:54,480
is helping us a lot, but on the other hand,

02:34:54,480 --> 02:34:58,500
we didn't want to waste CPU time on spinning in case

02:34:58,500 --> 02:35:01,930
of workloads or workloads that don't need it as much.

02:35:04,060 --> 02:35:07,320
So the question is if we could just let know scheduler

02:35:07,320 --> 02:35:09,900
about some critical barriers of execution, for example.

02:35:09,900 --> 02:35:13,168
So if the transaction commit is going on,

02:35:13,168 --> 02:35:16,040
we could hint that this is not the fragment

02:35:16,040 --> 02:35:18,110
in which we would like to be preempted

02:35:18,110 --> 02:35:22,850
because in such case, there would be a hole in

02:35:22,850 --> 02:35:24,850
the sequence of a lesson values which would

02:35:24,850 --> 02:35:26,560
be not filled in the log buffer

02:35:26,560 --> 02:35:29,380
and everybody else would stay unhappy because of that.

02:35:30,490 --> 02:35:32,640
So if we could force a proper scheduling

02:35:33,529 --> 02:35:36,500
for threads following the pattern I described

02:35:36,500 --> 02:35:38,390
that we create tasks, do some work,

02:35:38,390 --> 02:35:41,880
and report the task is done for these threads.

02:35:41,880 --> 02:35:43,790
Because we know what is the order

02:35:43,790 --> 02:35:48,790
that should be ideal in a situation.

02:35:50,450 --> 02:35:53,690
If we wanted to schedule the threads ourselves,

02:35:53,690 --> 02:35:56,000
we would know how to do it, but the scheduler

02:35:56,000 --> 02:36:01,000
has no idea about what order he should select.

02:36:01,410 --> 02:36:04,738
So basically if we could just hint the scheduler,

02:36:04,738 --> 02:36:07,804
that there is some particular order in

02:36:07,804 --> 02:36:10,160
which we should do it, it would help us.

02:36:11,530 --> 02:36:14,170
So the other point is that if the log writer

02:36:14,170 --> 02:36:17,730
is actively waiting for the data to be delivered

02:36:17,730 --> 02:36:19,790
and there are multiple user threads,

02:36:19,790 --> 02:36:21,720
it would make sense to wake up

02:36:21,720 --> 02:36:25,330
a proper user thread which probably could be,

02:36:25,330 --> 02:36:27,580
for example, preempted too quickly.

02:36:27,580 --> 02:36:30,570
So there is a mechanism when you use

02:36:30,570 --> 02:36:32,620
the mutex which is a priority version

02:36:32,620 --> 02:36:35,240
which is useful in such case when the thread

02:36:35,240 --> 02:36:37,850
is waiting on the mutex which is acquired

02:36:42,340 --> 02:36:43,870
by the other thread.

02:36:43,870 --> 02:36:46,840
So the other thread could have the priority bumped.

02:36:46,840 --> 02:36:50,563
But in our case, there is no mutex here any longer

02:36:50,563 --> 02:36:54,649
so if there was any way to do something similar

02:36:54,649 --> 02:36:59,010
without him relying on mutex, that would be helpful.

02:37:02,440 --> 02:37:04,610
So what we did, basically,

02:37:04,610 --> 02:37:06,570
we have the spin loops waiting for,

02:37:06,570 --> 02:37:10,300
when we need to wait for written or flush recall,

02:37:10,300 --> 02:37:12,852
we basically have two options.

02:37:12,852 --> 02:37:15,220
We can either use the spin loop in front of

02:37:15,220 --> 02:37:18,210
the waiting on the event, or we can just avoid using

02:37:18,210 --> 02:37:19,110
the spin loop at all.

02:37:19,110 --> 02:37:22,680
So in our test it's turned out that for very low workload,

02:37:22,680 --> 02:37:25,480
when there is not so many concurrent threads,

02:37:25,480 --> 02:37:27,520
it makes sense to do the spinning, unfortunately,

02:37:27,520 --> 02:37:30,400
because if we simply relied on waiting

02:37:30,400 --> 02:37:34,330
on the condition variable, it took too much time.

02:37:34,330 --> 02:37:37,928
So the TPS was decreased.

02:37:37,928 --> 02:37:42,928
The threads were wasting their time on waiting on

02:37:43,600 --> 02:37:45,950
the event and then being quite woken up.

02:37:48,610 --> 02:37:50,670
But for higher workloads where there is

02:37:50,670 --> 02:37:53,180
a lot of threads, it makes no sense

02:37:53,180 --> 02:37:56,860
to spin because it's just a wasting coffee of CPU cycles.

02:37:56,860 --> 02:37:59,770
So what we do with monitor, how much CPU is being used

02:37:59,770 --> 02:38:01,630
on the server, and I don't like this part

02:38:01,630 --> 02:38:04,436
but we didn't come up with anything which could be

02:38:04,436 --> 02:38:07,676
a better solution here, so if the CPU,

02:38:07,676 --> 02:38:11,935
if there is a lot of CPU on the server, unused,

02:38:11,935 --> 02:38:16,935
we can afford to do the spinning but as soon as the CPU

02:38:17,810 --> 02:38:20,170
is too busy on the server, we avoid to do the spinning.

02:38:20,170 --> 02:38:22,250
This is what we come up with.

02:38:25,490 --> 02:38:28,630
We would like to not to spin at all

02:38:28,630 --> 02:38:31,240
or leave this decision to...

02:38:31,240 --> 02:38:34,600
- So what we have in the Kernel for situation is that

02:38:34,600 --> 02:38:37,550
so we do all the optimistic spinning

02:38:37,550 --> 02:38:40,220
on sleeping works like mutex and stuff like that.

02:38:41,101 --> 02:38:43,600
And what we do there is that the first run

02:38:43,600 --> 02:38:46,860
that is trying to acquire the mutex records himself,

02:38:46,860 --> 02:38:48,770
as the one who is going to spin,

02:38:48,770 --> 02:38:50,760
everyone else just goes to sleep.

02:38:50,760 --> 02:38:54,540
So if there are follow-up processes that arrive at

02:38:54,540 --> 02:38:57,180
the same mutex, they see there is already someone spinning

02:38:57,180 --> 02:39:00,551
so they just go to sleep, I think.

02:39:00,551 --> 02:39:05,551
And so because like they think that there is like

02:39:05,620 --> 02:39:08,260
the high contention because they are already second one

02:39:08,260 --> 02:39:11,800
or even more ones who are waiting on the mutex, yeah.

02:39:11,800 --> 02:39:16,495
So that's what we do, it works reasonably well so...

02:39:16,495 --> 02:39:18,280
- [Man] Is Kernel internally?

02:39:18,280 --> 02:39:21,130
- Yeah, yeah, so this is Kernel internal implementation

02:39:21,130 --> 02:39:21,963
of a lock, yeah.

02:39:22,931 --> 02:39:24,601
- It's not exposed to use-- - No, no, no.

02:39:24,601 --> 02:39:28,164
- Can we have some kind of (speaking off mic)

02:39:28,164 --> 02:39:33,164
or what is representation to get it (speaking off mic).

02:39:35,444 --> 02:39:38,557
- Well yeah, but I guess you can,

02:39:38,557 --> 02:39:42,120
B thread conditional variables and stuff like

02:39:42,120 --> 02:39:44,930
that is this kind of thing but, yeah,

02:39:44,930 --> 02:39:47,160
you could probably work on improving that, yeah.

02:39:47,160 --> 02:39:50,770
So Wyman has actually, just this afternoon,

02:39:50,770 --> 02:39:52,370
Wyman has a talk about efficient

02:39:52,370 --> 02:39:55,225
user space optimistic - Yeah.

02:39:55,225 --> 02:39:57,321
- spinning locks, yeah.

02:39:57,321 --> 02:40:02,110
So I guess talk to him and I am not guy doing

02:40:02,110 --> 02:40:03,877
this kind of stuff yet.

02:40:03,877 --> 02:40:05,100
Yep.

02:40:06,500 --> 02:40:09,310
- But on PDL, they have adopted mutexes

02:40:09,310 --> 02:40:10,960
and you use them in MySQL.

02:40:12,966 --> 02:40:15,880
- Yeah, the point is so, from a long time,

02:40:15,880 --> 02:40:19,440
we can go to pressure to implement different ways

02:40:19,440 --> 02:40:23,170
because POSIX threads were greens, mutexes,

02:40:23,170 --> 02:40:25,367
or whatever it was called, are more adaptive and so on.

02:40:25,367 --> 02:40:28,640
So we spending time to implement framework

02:40:28,640 --> 02:40:31,790
which allowing you to use any kind of implementation.

02:40:31,790 --> 02:40:34,690
And still homemade implementation's the most efficient one.

02:40:34,690 --> 02:40:37,690
So we're still using atomics inside

02:40:37,690 --> 02:40:42,570
and we have pose instruction on spinning

02:40:42,570 --> 02:40:45,190
and it's still most efficient so nice and better.

02:40:49,040 --> 02:40:51,803
- Well, just work on improving what's

02:40:51,803 --> 02:40:53,680
in the libraries, yeah?

02:40:53,680 --> 02:40:55,810
That's the best I just do.

02:40:55,810 --> 02:40:59,020
Like if you know there are some performance improvements

02:40:59,020 --> 02:41:02,880
to the locking then just look how P thread mutex

02:41:02,880 --> 02:41:06,960
is implemented or talk to people who are implementing that

02:41:06,960 --> 02:41:10,200
and try to improve what's there because

02:41:10,200 --> 02:41:11,970
that's how open-source works.

02:41:11,970 --> 02:41:14,515
- You say it was the same story about

02:41:14,515 --> 02:41:18,430
so when this implementation was tested

02:41:18,430 --> 02:41:20,990
on generic testing, - Yeah.

02:41:20,990 --> 02:41:22,500
- how mutexes are working,

02:41:22,500 --> 02:41:25,610
it's always showed batteries or then what we have

02:41:25,610 --> 02:41:27,560
in homemade mutexes.

02:41:27,560 --> 02:41:29,340
So (mumbles) work it a lot of this.

02:41:29,340 --> 02:41:32,420
And once you run real workload with MySQL,

02:41:32,420 --> 02:41:33,950
this is opposite story.

02:41:33,950 --> 02:41:34,783
- Oh, I see.

02:41:40,844 --> 02:41:44,094
(man speaking off mic)

02:41:46,980 --> 02:41:51,900
- Okay so what we needed was basically

02:41:51,900 --> 02:41:54,850
a single condition variable for each fragment

02:41:54,850 --> 02:41:59,040
of the redo log up to which it makes sense to wait for.

02:41:59,040 --> 02:42:03,860
So for example let's suppose we had those user threads.

02:42:03,860 --> 02:42:06,050
The first user thread wanted to have redo flushed up

02:42:06,050 --> 02:42:10,025
to this point, the second wanted to have flashed redo up

02:42:10,025 --> 02:42:11,780
to this point, and those guys wanted

02:42:12,899 --> 02:42:14,750
to have flush redo up to some point in this block.

02:42:14,750 --> 02:42:17,300
So these are consecutive blocks of redo log which

02:42:17,300 --> 02:42:18,700
is a write ahead log for us.

02:42:21,497 --> 02:42:23,330
And each of these user threads

02:42:23,330 --> 02:42:25,450
is doing basically a transaction commit.

02:42:25,450 --> 02:42:28,649
So each of them is waiting for some a lesson,

02:42:28,649 --> 02:42:31,440
up to which the data must be flushed to disk

02:42:31,440 --> 02:42:33,320
before he can report the user thread,

02:42:33,320 --> 02:42:36,130
the transaction is safe and committed.

02:42:37,060 --> 02:42:41,710
So because if we suppose that spinning was not enough

02:42:41,710 --> 02:42:45,670
so they probably, or we didn't want

02:42:45,670 --> 02:42:47,080
to afford spinning at all.

02:42:47,080 --> 02:42:50,340
So they would like to wait on the condition variable,

02:42:50,340 --> 02:42:51,650
B3 condition variable.

02:42:53,200 --> 02:42:56,000
But they cannot simply wait on a single one

02:42:56,000 --> 02:43:01,000
because in such case if redo was flushed forward,

02:43:04,010 --> 02:43:06,510
we would need to wake up all of them

02:43:06,510 --> 02:43:09,430
so they can notice that redo is flushed farther

02:43:09,430 --> 02:43:11,590
but it's still not enough for me, for example,

02:43:11,590 --> 02:43:14,230
so I need to retry waiting on the event.

02:43:14,230 --> 02:43:17,766
So this is how it worked in previous MySQL version,

02:43:17,766 --> 02:43:19,490
previous InnoDB version.

02:43:19,490 --> 02:43:23,420
So the change is that we charted this,

02:43:24,290 --> 02:43:28,565
we created and bets and somebody wants to wait

02:43:28,565 --> 02:43:33,565
for LSN flushed up to certain point,

02:43:34,600 --> 02:43:37,870
he selects the proper bet and each bet has

02:43:37,870 --> 02:43:40,000
its own B thread condition variable.

02:43:40,000 --> 02:43:43,160
So there is not a single one on which everybody

02:43:43,160 --> 02:43:45,840
is doing await, and then everybody's woken up

02:43:45,840 --> 02:43:47,450
and only if you are satisfied

02:43:47,450 --> 02:43:50,540
and everybody else needs to go sleep again.

02:43:50,540 --> 02:43:54,070
But instead of that, if redo is,

02:43:54,070 --> 02:43:56,640
let's say, flushed from here to here,

02:43:57,480 --> 02:44:00,520
we wake up guys waiting on this and this event.

02:44:01,890 --> 02:44:04,910
So these guys still are not being woken up

02:44:05,750 --> 02:44:07,050
so they can sleep farther.

02:44:08,050 --> 02:44:10,040
This way we avoid false wake ups,

02:44:11,300 --> 02:44:16,300
unless then this progress somewhere in

02:44:16,480 --> 02:44:18,955
the middle of the log block so it's not

02:44:18,955 --> 02:44:22,161
the most to the end, so in case,

02:44:22,161 --> 02:44:24,530
for example, data was flushed up

02:44:24,530 --> 02:44:25,970
to the middle of this block,

02:44:27,922 --> 02:44:29,390
we would wake up those two guys

02:44:29,390 --> 02:44:31,160
and maybe only one of them would be happy

02:44:31,160 --> 02:44:34,690
but the other one would still wait because he wanted,

02:44:34,690 --> 02:44:36,160
for example, the redo to be flushed up

02:44:36,160 --> 02:44:38,920
to this point which is the farther point.

02:44:38,920 --> 02:44:40,520
But fortunately, what we do,

02:44:40,520 --> 02:44:43,960
is that whenever we have a complete block to be flushed,

02:44:43,960 --> 02:44:46,480
we only flush complete blocks,

02:44:46,480 --> 02:44:48,990
we avoid flushing incomplete block.

02:44:48,990 --> 02:44:53,512
This is possible because of the redo threat we have

02:44:53,512 --> 02:44:56,260
in background which all the time tries

02:44:56,260 --> 02:44:58,920
to write to file system cache all

02:44:58,920 --> 02:45:01,160
the complete blocks we have.

02:45:01,160 --> 02:45:04,280
So at least a single block of redo

02:45:04,280 --> 02:45:08,987
is prepared and complete, we write only complete blocks.

02:45:10,600 --> 02:45:13,670
So if the last one is incomplete, we just skip it.

02:45:13,670 --> 02:45:16,270
Then the write is finished, meanwhile,

02:45:16,270 --> 02:45:19,540
the incomplete block could become filled in.

02:45:19,540 --> 02:45:21,960
So again, in the next round we would just write

02:45:21,960 --> 02:45:23,110
the complete block.

02:45:23,110 --> 02:45:25,810
So this way, we try to avoid writing incomplete blocks

02:45:25,810 --> 02:45:28,556
of the redo log, and thanks to that,

02:45:28,556 --> 02:45:31,900
this mechanism makes sense because

02:45:31,900 --> 02:45:35,870
if only complete blocks are being written all the time,

02:45:35,870 --> 02:45:39,000
the LSN is shifted between the boundaries

02:45:39,000 --> 02:45:40,630
of the consecutive log blocks,

02:45:40,630 --> 02:45:44,100
and we wake up only those that were really interested

02:45:44,100 --> 02:45:46,690
in being woken up so nobody is unhappy because

02:45:46,690 --> 02:45:47,980
of being woken up.

02:45:59,370 --> 02:46:00,680
Okay?

02:46:00,680 --> 02:46:04,580
So the question I have, which I noticed when playing

02:46:04,580 --> 02:46:08,220
with this stuff around and doing tests,

02:46:08,220 --> 02:46:13,220
the first of all, could this be B thread signal broadcast

02:46:13,296 --> 02:46:17,290
be faster, is it synchronous, asynchronous?

02:46:17,290 --> 02:46:19,790
Because, from the perspective of

02:46:19,790 --> 02:46:22,368
the thread which is doing the notification

02:46:22,368 --> 02:46:25,068
to wake up other threads, it is not cheap

02:46:25,068 --> 02:46:28,747
and we actually had to introduce a special riddle

02:46:28,747 --> 02:46:31,736
of threat which is called notifier thread

02:46:31,736 --> 02:46:36,736
which only keeps, the only thing it does keep going through

02:46:40,180 --> 02:46:44,110
this RI which is a slight (mumbles), yes?

02:46:46,750 --> 02:46:50,232
Because yeah, we have only fixed number of bets.

02:46:50,232 --> 02:46:53,620
And it keeps notifying threads when

02:46:53,620 --> 02:46:55,450
the LSN is progressed.

02:46:55,450 --> 02:46:57,470
In the first implementation, it was

02:46:57,470 --> 02:47:00,200
the reader-writer thread after it finished write

02:47:01,950 --> 02:47:04,130
or the log flusher thread, the same atoms.

02:47:04,130 --> 02:47:05,680
After it finished writer flush,

02:47:07,480 --> 02:47:09,810
it then proceeded with notification.

02:47:09,810 --> 02:47:13,750
But the time required to do the notification stuff

02:47:13,750 --> 02:47:16,820
was so significant that because of that,

02:47:16,820 --> 02:47:20,270
the next write or the next flush was delayed

02:47:20,270 --> 02:47:23,033
and that overall throughput was much lower.

02:47:23,033 --> 02:47:28,033
So introducing special threats which only do

02:47:28,240 --> 02:47:30,120
the notification in loop,

02:47:30,970 --> 02:47:35,000
increase this transaction per second but

02:47:37,090 --> 02:47:38,390
it sounds a little bit stupid

02:47:38,390 --> 02:47:42,526
because all we need to is just schedule

02:47:42,526 --> 02:47:47,526
a wake up for some other thread, yes?

02:47:47,720 --> 02:47:50,240
So my first question is why is

02:47:50,240 --> 02:47:53,150
this exploration so expensive?

02:47:53,150 --> 02:47:55,760
Is it waiting until this thread in the wait queue

02:47:55,760 --> 02:47:57,970
is being actually woken up?

02:47:57,970 --> 02:48:00,640
Or how is it possible that waking up

02:48:00,640 --> 02:48:05,640
a single thread is so expensive from the perspective

02:48:06,269 --> 02:48:09,310
of the thread which is doing the notification?

02:48:10,790 --> 02:48:14,860
'Cause we just have some optimization for that.

02:48:14,860 --> 02:48:19,550
The next point is is there any way,

02:48:19,550 --> 02:48:22,760
is there any chance to optimize the wake up execution path

02:48:22,760 --> 02:48:25,160
and the wait queues in the Kernel?

02:48:25,160 --> 02:48:28,876
So this could work much faster because

02:48:28,876 --> 02:48:32,840
the current performance is very much relied

02:48:32,840 --> 02:48:37,840
on how effectively we can wake up quickly threads.

02:48:41,060 --> 02:48:43,995
- [Man] What kinds are you seeing?

02:48:43,995 --> 02:48:45,384
What's the name of

02:48:45,384 --> 02:48:50,384
what you're doing? - Yep.

02:48:51,053 --> 02:48:55,830
I think, something like 50 microseconds is the...

02:48:56,980 --> 02:48:59,470
- [Man] Like time to wake up a single thread?

02:48:59,470 --> 02:49:01,470
- No, no, no, time to wait, the moment

02:49:01,470 --> 02:49:03,136
this was time to wait.

02:49:03,136 --> 02:49:07,490
But 10 microseconds, something like that could be.

02:49:09,580 --> 02:49:11,630
Do I come with single condition variable?

02:49:13,756 --> 02:49:17,006
(man speaking off mic)

02:49:21,030 --> 02:49:26,030
The next question is how long does

02:49:28,270 --> 02:49:32,180
the post instruction take because we found it can vary,

02:49:33,940 --> 02:49:38,133
and because the waiting process which we have

02:49:38,133 --> 02:49:40,170
is basically goes like this.

02:49:40,170 --> 02:49:44,035
First, we do this spin loop of post instructions

02:49:44,035 --> 02:49:48,820
and now the question is how many iterations should we do?

02:49:50,000 --> 02:49:54,920
And I think any fixed value doesn't look proper

02:49:54,920 --> 02:49:57,980
because it should depend on the architecture.

02:49:57,980 --> 02:49:59,950
But for now we have the fixed value.

02:50:01,885 --> 02:50:06,200
So if we could somehow get the information,

02:50:06,200 --> 02:50:10,680
how long could we expect the pause would take on

02:50:10,680 --> 02:50:13,860
a specific architecture, that would be helpful.

02:50:13,860 --> 02:50:15,790
Should we just measure it at the start up

02:50:15,790 --> 02:50:19,814
by ourselves, doing some benchmarks, I don't know.

02:50:19,814 --> 02:50:21,188
- No, that doesn't make any sense.

02:50:21,188 --> 02:50:23,688
That's not what pause is for, right?

02:50:23,688 --> 02:50:27,788
Pause is gonna depend on what your other thread is doing.

02:50:27,788 --> 02:50:29,392
- [Presenter] Yeah.

02:50:29,392 --> 02:50:30,747
- So you're never gonna get an answer

02:50:30,747 --> 02:50:32,170
to that question, right?

02:50:33,710 --> 02:50:35,420
- [Presenter] So how could I know how long should

02:50:35,420 --> 02:50:36,253
I do the spin loop? - You can't though,

02:50:36,253 --> 02:50:39,006
you can't use pause in that way.

02:50:39,006 --> 02:50:41,022
You have to look for something else.

02:50:41,022 --> 02:50:46,022
You can't count pauses, it doesn't make any sense at all.

02:50:46,854 --> 02:50:51,854
- It's kind of like a (speaking off mic) changes

02:50:53,087 --> 02:50:54,237
where you multiply it by 10.

02:50:54,237 --> 02:50:57,594
So initially we wanted to have (mumbles) inside.

02:50:57,594 --> 02:50:59,348
Then finally could get it to big.

02:50:59,348 --> 02:51:02,265
(speaking off mic)

02:51:04,413 --> 02:51:09,413
The internal file. (mumbles)

02:51:17,546 --> 02:51:20,460
- Okay, the other question we have is

02:51:20,460 --> 02:51:22,120
the single-threaded workload.

02:51:23,640 --> 02:51:24,990
In single-threaded workloads,

02:51:24,990 --> 02:51:28,020
because of shifting work between threads,

02:51:28,020 --> 02:51:30,960
we don't gain anything, actually, we have a loss, yes?

02:51:30,960 --> 02:51:32,070
Of the latency we had.

02:51:33,380 --> 02:51:36,862
So the question is could we have kind

02:51:36,862 --> 02:51:41,862
of effect that whenever we wanted to wait

02:51:44,390 --> 02:51:47,970
for some other thread until it does its job which

02:51:47,970 --> 02:51:51,600
is a worker thread which is working in the background,

02:51:51,600 --> 02:51:54,890
could we have a way to hint the scheduler

02:51:54,890 --> 02:51:59,890
that this is the time we would like other job

02:52:01,220 --> 02:52:02,600
to be done in the other thread?

02:52:02,600 --> 02:52:05,620
But normally, yes, normally it would happen

02:52:06,683 --> 02:52:08,638
in other thread but for our case,

02:52:08,638 --> 02:52:10,460
because it is like the single-threaded almost scenario,

02:52:10,460 --> 02:52:13,430
could we just switch that particular thread

02:52:13,430 --> 02:52:16,520
and do it right away, so kind of, I don't know,

02:52:16,520 --> 02:52:19,330
proxy execution was presented during this conference.

02:52:20,190 --> 02:52:23,350
So it sounded like a possible solution

02:52:23,350 --> 02:52:28,350
if we could just have this chain of things

02:52:28,400 --> 02:52:30,280
to be done which normally are executed

02:52:30,280 --> 02:52:32,530
in concurrent threads to be executed

02:52:32,530 --> 02:52:35,017
as fast as possible in case when

02:52:35,017 --> 02:52:37,510
the workload is almost single-threaded

02:52:37,510 --> 02:52:41,040
and this whole architecture is not suited well

02:52:41,040 --> 02:52:43,260
for these kind of things because otherwise we would need

02:52:43,260 --> 02:52:44,690
to end up with two implementations,

02:52:44,690 --> 02:52:47,530
one implementation which is prepared for

02:52:47,530 --> 02:52:50,520
a case where there is a highly concurrent environment,

02:52:50,520 --> 02:52:52,120
in which case we have these worker threads

02:52:52,120 --> 02:52:55,080
and a lot of users doing things concurrently,

02:52:55,080 --> 02:52:58,700
and the other implementation which is optimized

02:52:58,700 --> 02:53:03,700
for a single-threaded or almost single-threaded workloads.

02:53:04,260 --> 02:53:06,590
- So essentially, what you would like to do

02:53:06,590 --> 02:53:10,410
is like say I am now running and I now want

02:53:10,410 --> 02:53:13,460
to give up my time slice and please run

02:53:13,460 --> 02:53:16,960
this thread instead of me to do the work.

02:53:16,960 --> 02:53:18,148
- [Presenter] Yeah.

02:53:18,148 --> 02:53:19,560
- So we have kind of producer-consumer kind

02:53:19,560 --> 02:53:21,709
of relationship between threads

02:53:21,709 --> 02:53:25,590
the consumer now wants to give up his time slice

02:53:25,590 --> 02:53:28,380
to the producer. - Yes, yes, yes.

02:53:28,380 --> 02:53:32,500
So for instance, the transaction was being committed,

02:53:34,300 --> 02:53:36,080
in normal case, what I do is that

02:53:36,080 --> 02:53:38,627
I am waiting until some background thread log writer writes

02:53:38,627 --> 02:53:42,795
and flushes redo log up to disk up to a certain point.

02:53:42,795 --> 02:53:46,630
But because I know that it's almost

02:53:46,630 --> 02:53:49,380
a single-threaded workload, I would like to do it myself

02:53:49,380 --> 02:53:50,780
because it would be faster.

02:53:50,780 --> 02:53:54,200
I would quickly just started to write

02:53:54,200 --> 02:53:56,010
and have it finished and that way

02:53:56,010 --> 02:53:58,750
it would be faster than letting know other thread,

02:53:58,750 --> 02:54:02,790
hey guy, do it for me and then he's woken up, for example,

02:54:02,790 --> 02:54:05,960
and he notices oh, okay, this is better.

02:54:05,960 --> 02:54:07,720
- If you want to do it in your thread

02:54:07,720 --> 02:54:09,260
then you can certainly do but that's

02:54:09,260 --> 02:54:10,930
a user space kind of thing.

02:54:10,930 --> 02:54:12,660
- [Presenter] Yeah, it's doable in user space

02:54:12,660 --> 02:54:15,060
but it adds up with an implementation--

02:54:15,060 --> 02:54:16,090
- Yes, I understand.

02:54:16,090 --> 02:54:19,530
You have to have the basically more complex implementation

02:54:19,530 --> 02:54:21,017
for yourself. - Yeah.

02:54:21,017 --> 02:54:22,700
- But then the Kernel still has to do

02:54:22,700 --> 02:54:25,214
the threads which at least here.

02:54:25,214 --> 02:54:26,710
So there is one thing, like a scheduling decision when

02:54:26,710 --> 02:54:30,640
I give the time slice to the producer

02:54:30,640 --> 02:54:33,150
and that could be influenced, I can imagine,

02:54:33,150 --> 02:54:33,983
I am not a CPU--

02:54:35,339 --> 02:54:36,172
- [Man] For somethings.

02:54:36,172 --> 02:54:39,220
- Yeah, yeah, like Kernel tries to actually infer

02:54:39,220 --> 02:54:43,380
this produce-consumer relationship in the CPU schedule

02:54:43,380 --> 02:54:45,990
and tries to follow them but obviously it's heuristic

02:54:45,990 --> 02:54:48,010
and it doesn't have work all the time.

02:54:48,010 --> 02:54:50,730
And it doesn't work in quite some queue spaces.

02:54:50,730 --> 02:54:54,650
But so one thing is when do we give the time slice

02:54:54,650 --> 02:54:56,860
and that could be inferred but then you still have

02:54:56,860 --> 02:54:59,420
to pay the cost of switching to another thread.

02:54:59,420 --> 02:55:03,270
You have to reset up the CPU, register parameter,

02:55:04,266 --> 02:55:06,616
like the protections between the threads

02:55:06,616 --> 02:55:08,683
because you can not just allow two threads to--

02:55:08,683 --> 02:55:09,940
- Yes but then you end up with a compromise between

02:55:09,940 --> 02:55:11,930
the complexity and the source code

02:55:11,930 --> 02:55:13,590
and the effect you get, you're in

02:55:13,590 --> 02:55:15,320
the middle of that, I agree. - Yes, you will be

02:55:15,320 --> 02:55:16,250
in the middle.

02:55:16,250 --> 02:55:18,030
- But it could be a better...

02:55:18,880 --> 02:55:21,980
- Yeah it could be-- - If the opposite way

02:55:21,980 --> 02:55:24,520
would be just to say no, we don't want increased complexity

02:55:24,520 --> 02:55:25,870
in source codes, I'm sorry.

02:55:26,805 --> 02:55:29,770
- Yeah, yeah, yeah, so what I wanted to say

02:55:29,770 --> 02:55:33,160
is I can see way to hinder the scheduler

02:55:33,160 --> 02:55:36,100
that you know this, I want now this process to run,

02:55:36,100 --> 02:55:37,550
I can imagine this to happen.

02:55:39,205 --> 02:55:41,465
But like doing something more complex in

02:55:41,465 --> 02:55:43,170
the Kernel, it would be kind of difficult.

02:55:47,287 --> 02:55:49,220
- If you just have a lock, that process takes

02:55:49,220 --> 02:55:52,660
and you acquire it, that actually works to wake it up

02:55:52,660 --> 02:55:53,950
because if you wait for - Yeah.

02:55:53,950 --> 02:55:57,380
- a lock, let's hope by another person will switch directly

02:55:57,380 --> 02:55:59,890
to that process in the same time slice.

02:55:59,890 --> 02:56:03,990
And we do that a bunch in Postgres and it works quite well.

02:56:03,990 --> 02:56:06,704
Our cost of switching is a bit higher because

02:56:06,704 --> 02:56:08,650
of the processes, but other than that,

02:56:08,650 --> 02:56:11,360
most of these transitions work by acquiring

02:56:11,360 --> 02:56:13,820
a lock that the other presence already holds

02:56:13,820 --> 02:56:15,534
and it automatically switches there.

02:56:15,534 --> 02:56:17,090
- So this is the protein version,

02:56:17,090 --> 02:56:18,200
I think, that is working here.

02:56:18,200 --> 02:56:20,100
- Yeah, generally just like there's a,

02:56:21,231 --> 02:56:22,860
it's not quite full prior to version,

02:56:22,860 --> 02:56:25,950
but generally if you hold the Kernel level lock

02:56:25,950 --> 02:56:28,258
or try to acquire a Kernel level lock

02:56:28,258 --> 02:56:29,120
that sells it by another process,

02:56:29,996 --> 02:56:32,146
there's a transition to that other process.

02:56:33,050 --> 02:56:35,020
For most lock types, I don't know whether it's all of them.

02:56:35,020 --> 02:56:37,330
It works for like sever force,

02:56:37,330 --> 02:56:39,480
that's what we happen to use but,

02:56:39,480 --> 02:56:42,090
and the way we implement we hope,

02:56:42,090 --> 02:56:43,260
it happens that they always block,

02:56:43,260 --> 02:56:45,210
so that's why we always switch but...

02:56:47,970 --> 02:56:49,070
- Okay so yeah, but...

02:56:56,130 --> 02:56:59,130
Okay, I would just thank, think if I could just double.

02:57:02,910 --> 02:57:06,120
Okay, so this is the end of this part.

02:57:07,539 --> 02:57:11,143
- Yeah, we're going quite a bit over time, sorry.

02:57:11,143 --> 02:57:14,711
(speaking off mic) Hopefully the left will bring

02:57:14,711 --> 02:57:19,711
that to light today. (speaking off mic)

02:57:39,273 --> 02:57:44,273
- Because we have only five minutes to, yeah.

02:57:49,597 --> 02:57:54,597
I'm coming around.

02:57:55,276 --> 02:58:00,276
Yep.

02:58:03,432 --> 02:58:07,803
Okay.

02:58:07,803 --> 02:58:12,544
So this was about (mumbles).

02:58:12,544 --> 02:58:15,023
Just to show you the points that Pawel mentioned.

02:58:15,023 --> 02:58:20,023
So this is a historical graph now as from development.

02:58:20,850 --> 02:58:23,660
This is with spinning code.

02:58:23,660 --> 02:58:26,610
So you see how bigger is different

02:58:26,610 --> 02:58:29,298
in performance comparing to the condition wait

02:58:29,298 --> 02:58:31,580
on any other implementation.

02:58:31,580 --> 02:58:34,347
But of course when we have no more CPU cycles,

02:58:34,347 --> 02:58:36,140
we are going down.

02:58:36,140 --> 02:58:39,330
So that's why there was a trade off here in implementation.

02:58:40,165 --> 02:58:43,290
So a few other points.

02:58:44,610 --> 02:58:47,605
So this about thread communication.

02:58:47,605 --> 02:58:52,605
Next point was about CPU cache story.

02:58:52,630 --> 02:58:55,180
So in fact what we are looking for,

02:58:56,120 --> 02:58:58,510
we discuss it about during these three days,

02:58:58,510 --> 02:59:01,480
especially with Intel guys about so

02:59:01,480 --> 02:59:04,500
it seems like the only way to understand what happens

02:59:04,500 --> 02:59:05,500
is to use V-tune.

02:59:06,850 --> 02:59:11,740
So our problem is we want some people doing create question

02:59:11,740 --> 02:59:14,500
are able to discover faster what happened.

02:59:14,500 --> 02:59:17,590
So this just gives you example of what

02:59:18,712 --> 02:59:21,320
it's a benchmark test, what's dbSTRESS, okay?

02:59:21,320 --> 02:59:24,022
So pure read-only workload.

02:59:24,022 --> 02:59:26,306
One select doing so it's, well,

02:59:26,306 --> 02:59:29,850
scaling at least, we have better performance

02:59:29,850 --> 02:59:32,566
on two CPU circuit comparing to one, okay?

02:59:32,566 --> 02:59:36,120
And you can see what happens,

02:59:36,120 --> 02:59:40,170
what is reported by Perf start from CPU cache.

02:59:40,170 --> 02:59:44,090
So there is some LLC mease, fine.

02:59:44,090 --> 02:59:46,540
So when we discuss it with guys,

02:59:46,540 --> 02:59:50,449
so oh you already have problems here but here it's Caden,

02:59:50,449 --> 02:59:53,780
here it's not Caden at all.

02:59:53,780 --> 02:59:56,050
So we have worst performance and it's read-only,

02:59:56,050 --> 02:59:58,660
we're only doing reads,

02:59:58,660 --> 03:00:01,530
and on two CPU circuits we have worst performance.

03:00:01,530 --> 03:00:03,720
So we know at least why.

03:00:03,720 --> 03:00:06,560
So okay, for this story, at least, we know why.

03:00:06,560 --> 03:00:08,960
We want to see it reported by CPU

03:00:08,960 --> 03:00:13,960
or by any other starts and we can have from CPU cache.

03:00:14,490 --> 03:00:17,320
And what happens exactly there

03:00:17,320 --> 03:00:21,500
is several spreads will write and change

03:00:21,500 --> 03:00:22,750
the same variable inside.

03:00:24,040 --> 03:00:26,480
So you don't have any cache mean

03:00:27,452 --> 03:00:29,240
so simple layout statistics so,

03:00:29,240 --> 03:00:31,890
or seems like V-Tune to report this.

03:00:31,890 --> 03:00:33,690
The problem is that the kind of test

03:00:35,380 --> 03:00:36,990
that run in 24 hours and we have thousand commits

03:00:36,990 --> 03:00:38,050
of changes per day.

03:00:39,562 --> 03:00:41,550
And our create guys can not sit down all

03:00:41,550 --> 03:00:44,480
the day with V-Tune and look on this.

03:00:44,480 --> 03:00:46,290
So we need to have some kind

03:00:46,290 --> 03:00:48,830
of script-able solution which can report

03:00:48,830 --> 03:00:50,260
then we heed this problem.

03:00:50,260 --> 03:00:51,910
And I'm pretty sure we have daunts

03:00:51,910 --> 03:00:53,994
of such kind of problems because from

03:00:53,994 --> 03:00:58,420
the past in MySQL code, there was manual exist.

03:00:58,420 --> 03:01:02,680
So we probably still have many places which can be improved

03:01:02,680 --> 03:01:06,170
so we have some file sharing or something like this

03:01:06,170 --> 03:01:09,580
so synchronization between CPU course which

03:01:09,580 --> 03:01:11,710
is blocking and it's not reported.

03:01:11,710 --> 03:01:14,080
You have no waits, you still use CPU,

03:01:14,080 --> 03:01:16,580
everything looks fine, but you are going slowly.

03:01:18,180 --> 03:01:21,540
So if you have some advices so we're working directly

03:01:21,540 --> 03:01:26,150
with Intel guys, Brendan, if you have some staff about this,

03:01:26,150 --> 03:01:29,620
we'll be happy to script it on and see how this,

03:01:29,620 --> 03:01:32,590
I'm pretty sure Netflix has the same story.

03:01:32,590 --> 03:01:36,630
In fact, any or Facebook guys, they have it as well

03:01:36,630 --> 03:01:39,480
because everything which is multi-thread, he do the same.

03:01:40,960 --> 03:01:44,800
So any advice welcome so you, well,

03:01:44,800 --> 03:01:47,180
we are open to any discussion because this

03:01:47,180 --> 03:01:50,380
is very painful, especially when simple read-only

03:01:50,380 --> 03:01:54,780
is not again scanning.

03:01:54,780 --> 03:01:56,061
No advice?

03:01:56,061 --> 03:01:59,810
Well, we've got long discussion with Intel guys

03:01:59,810 --> 03:02:03,030
so they're also working on their side on this

03:02:03,030 --> 03:02:04,830
so they want to show something.

03:02:04,830 --> 03:02:06,250
So this is CPU side.

03:02:07,410 --> 03:02:11,980
Now about the network, in fact, as I told you this morning,

03:02:11,980 --> 03:02:16,390
so MySQL is just daunts of problem on any side.

03:02:16,390 --> 03:02:21,210
So in fact, on MySQL you can use unique circuit

03:02:21,210 --> 03:02:25,200
which is lock on sonic and you use loop back,

03:02:25,200 --> 03:02:27,750
so this is the difference between, well,

03:02:27,750 --> 03:02:30,290
it's not red ones and this is just,

03:02:30,290 --> 03:02:33,110
this is about loop back IP port,

03:02:33,110 --> 03:02:34,660
and this is about UNIX circuit.

03:02:35,921 --> 03:02:37,771
So difference can go up to 19 person.

03:02:38,770 --> 03:02:41,470
And seems like it's directly going from IP stack.

03:02:42,800 --> 03:02:47,800
So initially, well, we saw them, okay,

03:02:48,320 --> 03:02:51,450
it's on Intel but we have the same story on ARM,

03:02:51,450 --> 03:02:55,360
and so it's directly related to the Linux Kernel only.

03:02:55,360 --> 03:02:57,260
And if we can improve your assumption,

03:02:58,260 --> 03:03:01,700
so it probably can improve, you image 20 person better

03:03:01,700 --> 03:03:03,980
on the network, it's something.

03:03:03,980 --> 03:03:06,580
So better traffic probably more efficient and so on.

03:03:08,690 --> 03:03:12,200
So this is one story about the network.

03:03:12,200 --> 03:03:13,120
Any ideas here?

03:03:14,220 --> 03:03:15,970
So what can be different between loop back

03:03:15,970 --> 03:03:19,120
and, in fact, it's the same circuit interface, right?

03:03:19,120 --> 03:03:21,500
Just implementation define, it's different.

03:03:22,458 --> 03:03:25,970
- But I assume that IDs they got tested will have

03:03:27,864 --> 03:03:30,070
to much more (speaking off mic) IDs.

03:03:32,246 --> 03:03:36,001
But you'll have to talk to the 14 guys

03:03:36,001 --> 03:03:41,001
who can answer exactly why this is (mumbles).

03:03:41,551 --> 03:03:43,812
I can imagine that the thing IP socket

03:03:43,812 --> 03:03:48,517
which basically has full of the, whatever IP Mondays

03:03:48,517 --> 03:03:52,682
is more difficult than to think up two things (mumbles),

03:03:52,682 --> 03:03:57,456
like machine all of the things.

03:03:57,456 --> 03:04:00,216
Very different in Hungary.

03:04:00,216 --> 03:04:02,121
- Yeah, on Solaris unfortunately,

03:04:02,121 --> 03:04:04,908
it was even more worse so at least Linux

03:04:04,908 --> 03:04:09,731
is doing better work on this level but still.

03:04:09,731 --> 03:04:13,425
And the final topic is about backlog.

03:04:13,425 --> 03:04:18,425
So backlog is use it, you know when you listen

03:04:18,990 --> 03:04:22,090
on IP circuits you cannot accept all connections

03:04:22,090 --> 03:04:25,095
which are coming in batch so you will have backlog

03:04:25,095 --> 03:04:29,880
just to keep them in queue and not reject, okay?

03:04:29,880 --> 03:04:33,975
So mainly it will protect you from situation going down.

03:04:33,975 --> 03:04:38,090
And all network will move to another one.

03:04:38,090 --> 03:04:40,429
So we have 10 solvent users coming back

03:04:40,429 --> 03:04:44,280
and we have 10 solvent connections going in altogether.

03:04:44,280 --> 03:04:45,970
It's very easy to kill the server,

03:04:45,970 --> 03:04:49,210
you know it will be just like a duress attack.

03:04:49,210 --> 03:04:51,480
So to use a backlog is the only way

03:04:51,480 --> 03:04:54,350
to keep them in queue so they will not be rejected

03:04:54,350 --> 03:04:56,880
and users, what they do is reject the track,

03:04:56,880 --> 03:04:59,850
why it will retry, retry, retry and we kill everything.

03:05:00,690 --> 03:05:05,050
So you can, first you need to configurate on OS level

03:05:05,050 --> 03:05:08,580
and on MySQL level, we also have the option

03:05:08,580 --> 03:05:11,610
to say how big your backlog will be.

03:05:11,610 --> 03:05:14,730
So it's when we starting to listen on circuits

03:05:14,730 --> 03:05:16,570
so we use this option.

03:05:16,570 --> 03:05:21,570
The most common workload which we have from AD,

03:05:21,990 --> 03:05:25,410
where publication's on, amazing even (mumbles) still can

03:05:25,410 --> 03:05:28,860
do this because they can not use persistent connection.

03:05:28,860 --> 03:05:33,280
So you have a lot of context about every user.

03:05:34,354 --> 03:05:36,893
So every time it will need to connect to database,

03:05:36,893 --> 03:05:37,870
do some query, disconnect.

03:05:37,870 --> 03:05:41,200
So we have massive attack on connect-disconnect,

03:05:41,200 --> 03:05:43,566
so it's massively connect and disconnect and so on.

03:05:43,566 --> 03:05:46,320
And what's amazing, so if we don't use backlog,

03:05:47,170 --> 03:05:49,150
we have up to 15 persons

03:05:49,150 --> 03:05:53,030
so probably even more better performance.

03:05:53,030 --> 03:05:57,090
And there is no impact so in fact what it's doing,

03:05:57,930 --> 03:06:02,930
how is this, the full impact because users sounds normal

03:06:03,110 --> 03:06:06,470
so there is no storm opposite and flying

03:06:06,470 --> 03:06:09,913
and still about the network.

03:06:12,070 --> 03:06:14,510
So this was observed also by create guys

03:06:14,510 --> 03:06:17,730
so they wanted to find the most optimal value

03:06:17,730 --> 03:06:19,260
and they did not find any difference

03:06:19,260 --> 03:06:22,110
if you use one or 1000 or 10,000.

03:06:22,110 --> 03:06:24,650
But if you have zero, oh, you have better performance.

03:06:25,620 --> 03:06:27,550
So we can not say our users use zero

03:06:27,550 --> 03:06:31,366
if they need to go faster because they still have risks

03:06:31,366 --> 03:06:34,580
and if you have a storm of connection, you will die.

03:06:36,200 --> 03:06:39,270
But probably there is something to improve because

03:06:39,270 --> 03:06:42,680
if there is no storm, there should be no difference here.

03:06:45,900 --> 03:06:47,180
What is doing Netflix?

03:07:04,020 --> 03:07:07,697
Well, I feel you are, you've got your... (laughs)

03:07:09,486 --> 03:07:11,541
- Does it-- - Going to hour flow.

03:07:11,541 --> 03:07:13,904
- Does a per profile show where the--

03:07:13,904 --> 03:07:15,224
- No, in fact, nothing happens.

03:07:15,224 --> 03:07:19,061
There is no way anything, there is no contentions.

03:07:19,061 --> 03:07:21,790
You know, all is fine.

03:07:21,790 --> 03:07:24,250
In fact, the main problem before and always

03:07:24,250 --> 03:07:26,800
was in our code, in our own code.

03:07:29,108 --> 03:07:30,990
- On top loop 30 steps. - Yeah.

03:07:30,990 --> 03:07:35,040
In fact, since the previous version,

03:07:35,040 --> 03:07:39,330
so from 20,000 we went to 100,000 connections per second

03:07:39,330 --> 03:07:41,330
than we can do, so it's in our code.

03:07:42,853 --> 03:07:43,686
- Okay.

03:07:43,686 --> 03:07:47,320
- But why backlog is just option on only piece work,

03:07:48,314 --> 03:07:49,379
it's nicer now.

03:07:49,379 --> 03:07:51,490
- Could we have seen the backlog?

03:07:51,490 --> 03:07:53,552
I mean, we observe it so I've got tools

03:07:53,552 --> 03:07:55,237
to see the same backlog, see how far away

03:07:55,237 --> 03:07:56,760
I am from blowing it out.

03:07:56,760 --> 03:07:59,419
I haven't seen a semi out to zero it means better.

03:07:59,419 --> 03:08:01,744
That's interesting.

03:08:01,744 --> 03:08:02,745
- Me as well.

03:08:02,745 --> 03:08:05,998
So I just asked, okay, you know we have no ideas,

03:08:05,998 --> 03:08:08,644
right, any kind of what-if, and one amounts to zero.

03:08:08,644 --> 03:08:10,879
And they were surprised of is it normal,

03:08:10,879 --> 03:08:14,070
I say I don't, probably not at all.

03:08:19,752 --> 03:08:22,630
- [Man] I mean, that can happen in the duration on

03:08:22,630 --> 03:08:26,850
the same backlog, exceeds if you fail early

03:08:26,850 --> 03:08:29,412
and let it retry.

03:08:29,412 --> 03:08:31,512
So now that I think of it, I have seen

03:08:31,512 --> 03:08:33,350
a situation where reducing

03:08:33,350 --> 03:08:35,657
the backlog into its performance.

03:08:35,657 --> 03:08:39,550
- Yeah, normally yes because if you will retrace our...

03:08:39,550 --> 03:08:43,685
So we observed in the past,

03:08:43,685 --> 03:08:48,230
because our many restrictions so in Oracle,

03:08:48,230 --> 03:08:51,890
Oracle take in all security points very critically.

03:08:51,890 --> 03:08:54,692
And so with SSL, there was some kind of global lock

03:08:54,692 --> 03:08:59,610
which was limiting globally per system, all connections.

03:08:59,610 --> 03:09:01,630
So you can not just, when you do connect,

03:09:01,630 --> 03:09:04,930
it will generate some random ID which you need

03:09:04,930 --> 03:09:06,500
to use and so on.

03:09:06,500 --> 03:09:08,760
But in this case, nice and like this, so.

03:09:11,070 --> 03:09:15,970
- So this is DCPN unique sockets or both?

03:09:18,807 --> 03:09:21,117
No, oh, sorry, this is protected or you...

03:09:21,117 --> 03:09:24,850
- So in fact, to run quicker,

03:09:24,850 --> 03:09:26,930
it will be unique circuit because,

03:09:29,110 --> 03:09:31,620
so everything can be stored so, in fact,

03:09:31,620 --> 03:09:33,870
there is some limitation hidden on empty socket.

03:09:33,870 --> 03:09:35,510
If you're going too fast, well,

03:09:37,066 --> 03:09:38,920
and that's creators has run in locally.

03:09:38,920 --> 03:09:42,460
So it can be a unique circuit but then they test

03:09:42,460 --> 03:09:45,543
it on AP as well and the story is the same.

03:09:52,140 --> 03:09:54,090
Okay, so well, we will continue to dig.

03:09:55,130 --> 03:09:57,240
Brendan probably have some ideas,

03:09:57,240 --> 03:10:00,450
we discussed about file systems as well, dig in.

03:10:00,450 --> 03:10:04,730
So you have some tools about file system observations?

03:10:07,037 --> 03:10:08,282
- I have a lot coming out of,

03:10:08,282 --> 03:10:10,733
I have a new book on BP effort, the roping source,

03:10:10,733 --> 03:10:12,645
trying to get (mumbles).

03:10:12,645 --> 03:10:13,930
So other tools, for example,

03:10:13,930 --> 03:10:15,785
apart from file systems on mention the wait on

03:10:15,785 --> 03:10:18,648
the backlog, you get a good idea with late this year

03:10:18,648 --> 03:10:21,817
on the backlog queue but that would make sense as well.

03:10:21,817 --> 03:10:23,793
I wanna see your latency histogram,

03:10:23,793 --> 03:10:26,699
how many milliseconds are my connections waiting

03:10:26,699 --> 03:10:28,083
to see the backlog,

03:10:28,083 --> 03:10:31,186
'cause that would really help explain situations like this.

03:10:31,186 --> 03:10:33,904
But we can do that by using BPF and take ropes

03:10:33,904 --> 03:10:36,419
and coming up with custom questions.

03:10:36,419 --> 03:10:39,374
- So you think that all the latency

03:10:39,374 --> 03:10:43,535
is what CPU affects on later on those indicators?

03:10:43,535 --> 03:10:46,815
- No, it just is, for a similar backlog,

03:10:46,815 --> 03:10:50,912
it's just losing too many connections in the Kernel

03:10:50,912 --> 03:10:52,886
and the application is taking too long

03:10:52,886 --> 03:10:57,463
to go and call it sept.

03:10:57,463 --> 03:11:00,810
And so this can add up to be (mumbles) milliseconds.

03:11:00,810 --> 03:11:03,787
So that might explain what is in it.

03:11:03,787 --> 03:11:05,285
- Yeah.

03:11:05,285 --> 03:11:06,963
So while we are here about file system,

03:11:06,963 --> 03:11:11,735
do we have any problem on file systems?

03:11:11,735 --> 03:11:14,409
- No, not at the moment.

03:11:14,409 --> 03:11:15,910
Well-- - So because

03:11:15,910 --> 03:11:19,531
I recall you used MySQL on sometimes so.

03:11:19,531 --> 03:11:21,230
- Yeah, we do have some MySQL.

03:11:21,230 --> 03:11:24,148
- So this is just, we discussed before

03:11:24,148 --> 03:11:29,148
about XFS EXT4 and we have different performance totally,

03:11:29,848 --> 03:11:32,110
and especially better performance

03:11:32,110 --> 03:11:33,510
on XFS if we do more rights.

03:11:35,730 --> 03:11:37,767
- All the things are complicated.

03:11:37,767 --> 03:11:39,218
You need to get into the file system and--

03:11:39,218 --> 03:11:41,382
- Okay, if you have any tools or comments

03:11:41,382 --> 03:11:43,130
or we'll be happy to use it but...

03:11:46,494 --> 03:11:51,494
(sighs) Well, just to finish all this problems article

03:11:53,880 --> 03:11:56,530
so you can find on my blog site.

03:11:56,530 --> 03:12:00,710
And I also have some tools which help internalize.

03:12:00,710 --> 03:12:03,650
Now, mostly about MySQL before it was about everything.

03:12:04,699 --> 03:12:08,961
Well, that's all.

03:12:08,961 --> 03:12:10,594
(man speaking off mic)

03:12:10,594 --> 03:12:12,883
Thank you very much, so well, it's very...

03:12:12,883 --> 03:12:13,920
(man speaking off mic) (group clapping)

03:12:13,920 --> 03:12:15,970
- Okay, yeah, thank you everyone today.

03:12:15,970 --> 03:12:19,340
Yeah, thanks to all our speakers and thank you participants

03:12:19,340 --> 03:12:21,250
for lending your knowledge on that

03:12:21,250 --> 03:12:23,810
and I'm sure there's gonna be many more emails

03:12:23,810 --> 03:12:26,040
to follow up in the right way.

03:12:26,040 --> 03:12:28,790
So much appreciate you all for coming

03:12:28,790 --> 03:12:30,440

YouTube URL: https://www.youtube.com/watch?v=-oP2BOsMpdo


