Title: LPC2019 - Scheduler MC
Publication date: 2019-11-18
Playlist: LPC2019 - MicroConferences
Description: 
	Scheduler MC

The Linux Plumbers 2019 Scheduler Microconference is about all scheduler topics, which are not Realtime

Potential topics:
- Load Balancer Rework - prototype
- Idle Balance optimizations
- Flattening the group scheduling hierrachy
- Core scheduling
- Proxy Execution for CFS
- Improving scheduling latency with SCHED_IDLE task
- Scheduler tunables - Mobile vs Server
- nohz
- LISA for scheduler verification

We plan to continue the discussions that started at OSPM in May'19 and get a wider audience outside the core scheduler developers at LPC.

MC leads:
Juri Lelli juri.lelli@redhat.com, Vincent Guittot vincent.guittot@linaro.org, Daniel Bristot de Oliveira bristot@redhat.com, Subhra Mazumdar subhra.mazumdar@oracle.com, Dhaval Giani dhaval.giani@gmail.com
Captions: 
	00:00:00,300 --> 00:00:01,170
- Hi everybody.

00:00:01,170 --> 00:00:02,880
Thanks for joining us.

00:00:06,410 --> 00:00:07,620
Yeah, come on, Diane.

00:00:10,330 --> 00:00:13,560
To just wait to make you quickly join us.

00:00:16,293 --> 00:00:19,820
(men speaking off mic)

00:00:19,820 --> 00:00:22,080
So it's at the end.

00:00:26,120 --> 00:00:27,580
So yeah, thanks for joining us

00:00:27,580 --> 00:00:29,240
for this scheduler microconf.

00:00:30,640 --> 00:00:32,970
We'll have two slot of one hour and a half,

00:00:32,970 --> 00:00:35,170
starting with the core scheduling

00:00:35,170 --> 00:00:39,460
and then it's the proxy execution.

00:00:39,460 --> 00:00:40,293
Let me check.

00:00:43,280 --> 00:00:45,610
Then the schedule,

00:00:45,610 --> 00:00:48,910
then we'll have a break and a talk on

00:00:48,910 --> 00:00:52,190
the load balance re-work, flattening the hierarchy,

00:00:52,190 --> 00:00:54,650
the scheduler demand and the cache,

00:00:54,650 --> 00:00:58,380
we'll have a description on TUBA scared.

00:00:58,380 --> 00:01:00,800
We'll have some discussion on TurboSched

00:01:01,785 --> 00:01:04,510
and we'll finish with the task latency-nice.

00:01:05,410 --> 00:01:06,940
So don't hesitate to ask question.

00:01:06,940 --> 00:01:08,901
The goal is really the interact, voice problem.

00:01:08,901 --> 00:01:09,830
Yes?

00:01:11,600 --> 00:01:14,140
It's not a presentation, it's a discussion.

00:01:14,140 --> 00:01:16,110
So I think we are just on time,

00:01:16,110 --> 00:01:18,790
maybe waiting a few more minute further

00:01:18,790 --> 00:01:22,430
to we will display the note.

00:01:22,430 --> 00:01:23,330
Yeah, two minutes.

00:01:25,399 --> 00:01:27,040
So just wait for two more minutes.

00:01:28,280 --> 00:01:30,930
We have a white board, if you want to draw something.

00:01:31,950 --> 00:01:34,390
Yeah, don't hesitate to take notes in Datapad.

00:01:37,800 --> 00:01:41,600
But help us to remind two weeks or three weeks after

00:01:41,600 --> 00:01:42,433
what we said.

00:01:48,800 --> 00:01:51,830
And then I think that if there is not enough time

00:01:51,830 --> 00:01:54,580
we can continue some of these solutions in the hallway.

00:02:00,880 --> 00:02:04,581
(man speaking off mic)

00:02:04,581 --> 00:02:05,581
- Up to you.

00:02:06,441 --> 00:02:10,149
(man speaking off mic)

00:02:10,149 --> 00:02:11,310
- Do you have the notes?

00:02:11,310 --> 00:02:12,610
- I already said it, yeah.

00:02:13,490 --> 00:02:16,370
So here is the link for the Datapad

00:02:16,370 --> 00:02:18,580
to take the note so don't hesitate to

00:02:18,580 --> 00:02:22,510
connect and add your understanding of the microconf.

00:02:36,285 --> 00:02:40,535
(men talking and laughing off mic)

00:03:25,209 --> 00:03:27,876
- It should show for 45 minutes.

00:03:30,814 --> 00:03:32,570
- Yep, it'll depend on no questions.

00:03:32,570 --> 00:03:33,403
- Yeah, exactly.

00:03:33,403 --> 00:03:35,333
If there's some things which seems clear,

00:03:35,333 --> 00:03:36,817
we can move to a specific area.

00:03:36,817 --> 00:03:39,930
And if you take too much time, we will warn you about

00:03:39,930 --> 00:03:42,808
because time is running and we will have to--

00:03:42,808 --> 00:03:46,058
(men mumbling off mic)

00:04:10,130 --> 00:04:11,600
- I can start?

00:04:11,600 --> 00:04:12,690
Can I start, I can?

00:04:13,630 --> 00:04:14,463
Hello everyone.

00:04:14,463 --> 00:04:15,590
Thank you for being here today.

00:04:15,590 --> 00:04:18,970
So today we are going to talk about cost scheduling.

00:04:20,090 --> 00:04:22,340
As you may have heard, there have been a couple

00:04:22,340 --> 00:04:25,290
of security vulnerabilities in the past year.

00:04:25,290 --> 00:04:30,290
So yes, and two of them mainly affect hyperthreading.

00:04:31,790 --> 00:04:32,926
So the idea was sent about a year ago

00:04:32,926 --> 00:04:37,926
about having core scheduling.

00:04:39,360 --> 00:04:42,180
So to be able to group tasks of

00:04:42,180 --> 00:04:44,880
the same trust domain on the same call

00:04:44,880 --> 00:04:48,570
and that way avoid leaking data from one process

00:04:48,570 --> 00:04:52,030
to the other without having to disable hyperthreading

00:04:52,030 --> 00:04:55,270
because of the performance implications.

00:04:55,270 --> 00:04:56,870
So that's the main idea,

00:04:56,870 --> 00:05:00,299
so grouping trusted tasks and find quickly

00:05:00,299 --> 00:05:03,540
trusted tasks to run on the same call.

00:05:04,560 --> 00:05:06,850
If we don't find two tasks that can

00:05:06,850 --> 00:05:08,320
run at the same time on the same call,

00:05:08,320 --> 00:05:11,420
we force either on the other sibling.

00:05:11,420 --> 00:05:16,420
That way we can do a security implication.

00:05:17,100 --> 00:05:19,490
And we also want to load balance between CPUs

00:05:19,490 --> 00:05:23,410
to pull tasks that are compatible and then run

00:05:23,410 --> 00:05:25,190
on the same CPU on the same core,

00:05:25,190 --> 00:05:27,280
instead of being spread and having to force

00:05:27,280 --> 00:05:29,210
idle a lot of time.

00:05:29,210 --> 00:05:32,660
So today's presentation, it's just we want

00:05:32,660 --> 00:05:35,320
to look at what are the next steps

00:05:35,320 --> 00:05:38,890
because we are now the third version.

00:05:38,890 --> 00:05:41,670
We have a few issues to fix but now

00:05:41,670 --> 00:05:45,080
we really want to look into how we can mainline this.

00:05:45,080 --> 00:05:47,413
So that's more of a discussion because

00:05:47,413 --> 00:05:52,413
one of the concern is that MGS impact courier

00:05:53,290 --> 00:05:57,620
has another implication but it would require

00:05:57,620 --> 00:05:59,740
synchronization points on system calls,

00:05:59,740 --> 00:06:03,490
interrupts, and also as well as VN exit.

00:06:03,490 --> 00:06:08,120
For I1TF, VN exit, INF, and for VM only,

00:06:08,120 --> 00:06:10,320
VM exit also INF.

00:06:10,320 --> 00:06:14,430
But we know that the performance implication

00:06:14,430 --> 00:06:17,090
of adding additional synchronization points

00:06:17,090 --> 00:06:19,160
on system call and interrupts.

00:06:19,160 --> 00:06:23,060
So I think that's the main topic we want to address today

00:06:23,060 --> 00:06:27,050
is that and we want also to hear from you

00:06:27,050 --> 00:06:29,121
if you have other ideas of roadblocks

00:06:29,121 --> 00:06:34,121
or things you want to see to mainline this feature.

00:06:34,130 --> 00:06:35,570
So that's my thought.

00:06:35,570 --> 00:06:38,470
After that, we will have a performance discussion

00:06:38,470 --> 00:06:41,720
and also another idea on the same concept

00:06:41,720 --> 00:06:43,960
based out of all our core scheduling

00:06:43,960 --> 00:06:45,140
instead of call scheduling.

00:06:45,140 --> 00:06:47,420
So now let's discuss.

00:06:52,570 --> 00:06:54,990
Okay, should we first do the presentation?

00:06:57,450 --> 00:07:00,700
(man speaking off mic)

00:07:02,852 --> 00:07:04,130
So we can discuss after.

00:07:06,860 --> 00:07:09,160
Hi everybody, I'm Schubert and I work

00:07:09,160 --> 00:07:10,950
in Oracle Linux.

00:07:10,950 --> 00:07:13,223
When all this MDS stuff came out

00:07:13,223 --> 00:07:17,580
while virtual machines was one use case

00:07:17,580 --> 00:07:18,730
definitely a win for us,

00:07:18,730 --> 00:07:22,928
the other use case that you had was for database.

00:07:22,928 --> 00:07:27,510
So Oracle database has its own kind of

00:07:27,510 --> 00:07:30,300
virtualization technology collocated directly

00:07:30,300 --> 00:07:33,530
to my DNN to where root database

00:07:33,530 --> 00:07:38,210
can house multiple lightweight pluggable databases,

00:07:38,210 --> 00:07:39,043
as we call them.

00:07:41,470 --> 00:07:44,250
In this case, you can imagine hundreds

00:07:44,250 --> 00:07:47,100
or even thousands of such pluggable database

00:07:47,100 --> 00:07:49,910
or PDBs will run in the one system.

00:07:50,920 --> 00:07:53,648
And if somehow one PDB is able to run some

00:07:53,648 --> 00:07:58,020
untrusted code, it can leak data from other

00:07:58,020 --> 00:08:00,420
into PDBs in the same host.

00:08:01,820 --> 00:08:06,820
So from the performance testing point of view,

00:08:09,950 --> 00:08:13,000
we started testing with the database use case

00:08:13,000 --> 00:08:16,010
when Peter first posted his batches.

00:08:16,990 --> 00:08:21,660
And I have been testing consistently for V2 and V3.

00:08:21,660 --> 00:08:24,990
It has more or less remained the same from V1

00:08:25,970 --> 00:08:28,910
but in some corner cases that I think the

00:08:28,910 --> 00:08:30,320
digitalization folks were seeing.

00:08:30,320 --> 00:08:34,210
But for my test case I didn't see much difference

00:08:34,210 --> 00:08:35,720
so far from V1 to V3.

00:08:36,630 --> 00:08:41,630
In this light, what I'm showing is two DB instances.

00:08:42,320 --> 00:08:44,970
Think of them as two PDB instances,

00:08:44,970 --> 00:08:47,010
which are running on the same host,

00:08:47,010 --> 00:08:52,010
which is a 44 core, two-socket, X36 system.

00:08:52,030 --> 00:08:55,180
It is running a OLTP workload.

00:08:55,180 --> 00:08:56,470
I use TPCC for this.

00:08:59,326 --> 00:09:01,670
The idea was the compare how much performance

00:09:01,670 --> 00:09:05,620
we're losing with respect to the baseline,

00:09:05,620 --> 00:09:08,490
which is I'm not using any core scheduling,

00:09:09,620 --> 00:09:11,470
in the percentage gain column.

00:09:12,340 --> 00:09:14,910
Firstly I valued the number of users,

00:09:16,370 --> 00:09:17,890
just to ramp up the load a little bit.

00:09:17,890 --> 00:09:20,510
At 32 users I think that one of

00:09:20,510 --> 00:09:22,990
the system utilization was around 50%.

00:09:24,220 --> 00:09:26,350
If you can see the percentage gain,

00:09:26,350 --> 00:09:27,750
which is all negative.

00:09:27,750 --> 00:09:32,470
So we are seeing substantial performance loss,

00:09:34,870 --> 00:09:38,090
mostly of the first idleness that the

00:09:38,090 --> 00:09:42,320
core scheduler is forcing certain hyperthreads through

00:09:42,320 --> 00:09:44,830
because it can not find the matching tasks.

00:09:44,830 --> 00:09:49,090
And it goes all the way up to 30% utilization for 32 users.

00:09:50,110 --> 00:09:53,390
I also have the percentage idle baseline

00:09:53,390 --> 00:09:54,380
and the percentage idle,

00:09:54,380 --> 00:09:56,730
which is the core scheduling column.

00:09:56,730 --> 00:10:00,750
And as you can see, the idleness has gone up

00:10:00,750 --> 00:10:02,790
quite a bit compared to the baseline.

00:10:02,790 --> 00:10:05,750
So while there is extra work being done

00:10:05,750 --> 00:10:08,250
for core scheduling, like enqueuing and dequeuing

00:10:08,250 --> 00:10:10,080
from our separate run queue.

00:10:10,080 --> 00:10:13,190
But I think that the idleness is actually

00:10:13,190 --> 00:10:16,670
one of the major sources of regression

00:10:16,670 --> 00:10:19,930
because the differences of idleness are pretty huge.

00:10:19,930 --> 00:10:23,359
So unless we can cover that gap,

00:10:23,359 --> 00:10:27,530
probably we won't get much closer to the performance.

00:10:30,490 --> 00:10:34,750
I think that as the patch stands today,

00:10:34,750 --> 00:10:37,670
one change that we need to add further

00:10:37,670 --> 00:10:42,610
is also change the wakeup part because currently

00:10:42,610 --> 00:10:47,610
there is no matching of cookie is attempted

00:10:48,260 --> 00:10:49,830
when that thread is woken up.

00:10:50,780 --> 00:10:54,990
So it just uses the same code path as before.

00:10:54,990 --> 00:10:59,990
There is no putting in anomaly threads to cores.

00:11:00,580 --> 00:11:02,410
And also another thing that doesn't help

00:11:02,410 --> 00:11:06,110
is kindly the scheduler first tries

00:11:06,110 --> 00:11:09,230
to spread across cores and then it tries

00:11:09,230 --> 00:11:11,494
to find out the CPUs or idle siblings.

00:11:11,494 --> 00:11:14,150
So it's spreading out those codes is basically

00:11:14,150 --> 00:11:17,660
and doing the exact opposite of minimizing

00:11:17,660 --> 00:11:21,720
the probability of trying to pack matching cookies

00:11:21,720 --> 00:11:23,125
into the cores.

00:11:23,125 --> 00:11:26,950
So I'm hoping that once we have this change

00:11:26,950 --> 00:11:28,560
probably some of the gap will be closed

00:11:28,560 --> 00:11:32,300
and we'll get the regression further down.

00:11:33,430 --> 00:11:37,330
And the other column that I have, the last column

00:11:37,330 --> 00:11:40,250
is actually the experiment with no SMT.

00:11:40,250 --> 00:11:44,190
So basically it is enabled a hyperthread in each core.

00:11:45,200 --> 00:11:48,550
And as you can see, for the lower users

00:11:48,550 --> 00:11:53,090
actually you gain performance compared to the baseline.

00:11:53,090 --> 00:11:55,730
And this is not something unusual.

00:11:55,730 --> 00:11:59,420
There is actually a very long email

00:11:59,420 --> 00:12:03,630
from Mel Gorman, who sent a lot of performance data

00:12:03,630 --> 00:12:06,830
of defense benchmarks and he showed that

00:12:07,900 --> 00:12:09,070
it's all over the place.

00:12:09,070 --> 00:12:11,290
The sibling HT is not like

00:12:11,290 --> 00:12:13,180
a sure shot of losing performance.

00:12:13,180 --> 00:12:16,090
You will gain performance, in many cases you'll lose.

00:12:16,090 --> 00:12:18,370
And the reasons behind that can be varied.

00:12:20,340 --> 00:12:22,150
So it's not an easy answer that

00:12:22,150 --> 00:12:24,800
usable HT is always a bad idea.

00:12:25,650 --> 00:12:28,670
And finally, for 32 users actually you see

00:12:28,670 --> 00:12:33,570
we suddenly see the performance cliff with HT disabled.

00:12:33,570 --> 00:12:35,740
That's because the number of threads have gone up so much

00:12:35,740 --> 00:12:39,220
that now you are short of CPUs.

00:12:39,220 --> 00:12:43,700
But still the loss is not so much as core scheduling.

00:12:45,270 --> 00:12:48,701
So yeah, right now the jury is still out

00:12:48,701 --> 00:12:50,100
what is the right approach.

00:12:51,120 --> 00:12:55,590
At least for this use case, maybe this ability

00:12:55,590 --> 00:12:58,280
hyperthreading could be the right solution.

00:12:58,280 --> 00:13:01,050
So this is where things stand as of now.

00:13:02,060 --> 00:13:05,790
So we are hopefully adding more improvement

00:13:05,790 --> 00:13:09,010
of performance and of coming weeks and months.

00:13:12,260 --> 00:13:13,820
That was the only slide I had.

00:13:15,750 --> 00:13:18,720
I think Aubrey will talk next, yeah.

00:13:26,330 --> 00:13:27,520
- Hi, my name is Aubrey.

00:13:27,520 --> 00:13:30,530
I come from Intel Solving Source Technology Center.

00:13:31,930 --> 00:13:36,040
Today's topic is actually core scheduling applications.

00:13:38,530 --> 00:13:42,840
This topic actually from a customer in China.

00:13:42,840 --> 00:13:46,410
So they wanted to improve their

00:13:46,410 --> 00:13:49,130
server machine's utilization so they

00:13:49,130 --> 00:13:53,400
collocated some latency critical workload

00:13:53,400 --> 00:13:57,610
with some kind of best effort workload.

00:13:58,480 --> 00:14:02,180
And those best effort workload typically

00:14:02,180 --> 00:14:06,160
is done in training workload.

00:14:07,120 --> 00:14:10,410
And they found out the latency of

00:14:10,410 --> 00:14:13,090
latency critical workload is affected

00:14:13,090 --> 00:14:15,270
by the best effort workload

00:14:16,220 --> 00:14:18,780
and we helped them to figure out

00:14:18,780 --> 00:14:23,780
that AVX512 is a major cause.

00:14:24,140 --> 00:14:28,311
So the AVX512 instructions can accelerate

00:14:28,311 --> 00:14:32,200
the compute the intensity workload.

00:14:35,100 --> 00:14:38,230
The best case we saw by impact

00:14:38,230 --> 00:14:40,490
and the FPU Julia benchmark,

00:14:40,490 --> 00:14:44,160
we see the improvement is up to 30%.

00:14:45,320 --> 00:14:50,320
And I have run an image recognition training benchmark

00:14:53,040 --> 00:14:58,040
over Tensorflow and I actually saw a 30% improvement.

00:15:00,640 --> 00:15:05,640
So comparing to AVX2 to AVX512 instructions

00:15:07,220 --> 00:15:09,840
used wider vector instructions

00:15:09,840 --> 00:15:11,637
and the wider registers

00:15:11,637 --> 00:15:15,670
so it often can improve the throughput.

00:15:16,570 --> 00:15:20,930
But it also can temporarily draw large amounts

00:15:20,930 --> 00:15:23,290
of additional calling down CPU.

00:15:25,510 --> 00:15:28,870
That means it will consume more power.

00:15:28,870 --> 00:15:32,169
But the power budget is limited so when

00:15:32,169 --> 00:15:34,220
AVX512 is running on the core,

00:15:34,220 --> 00:15:36,510
the core max frequency will be reduced.

00:15:37,790 --> 00:15:41,950
If we look at the picture on the right side,

00:15:41,950 --> 00:15:45,710
if there is a core running AVX512 and its frequency

00:15:45,710 --> 00:15:50,710
is 3.5 gigahertz but if they run non-AVX tasks,

00:15:50,903 --> 00:15:55,210
the frequency can be 3.8 gigahertz.

00:15:56,650 --> 00:15:59,620
So things get worse if there are more

00:15:59,620 --> 00:16:02,670
core cores running on AVX512.

00:16:03,830 --> 00:16:06,790
For a good case we can see that if

00:16:06,790 --> 00:16:11,080
nonAVX and the AVX suffered on two different cores,

00:16:11,080 --> 00:16:12,770
so everyone's heavy.

00:16:13,620 --> 00:16:17,440
But for the bad case, if AVX and the nonAVX

00:16:17,440 --> 00:16:20,210
is mixed together on the same core at the same time,

00:16:21,860 --> 00:16:24,420
the AVX task is not effective

00:16:24,420 --> 00:16:27,260
but the nonAVX task is effective.

00:16:27,260 --> 00:16:30,070
It's frequency is reduced, so its performance,

00:16:30,070 --> 00:16:33,560
its throughput and latency were all impacted.

00:16:37,280 --> 00:16:42,280
We actually can use core scheduling to separate

00:16:43,030 --> 00:16:45,820
nonAVX and AVX easily.

00:16:45,820 --> 00:16:48,850
We just need to add a magical number

00:16:49,790 --> 00:16:54,790
to the core cookie or AVX tasks and that's it.

00:16:56,110 --> 00:16:58,710
And we verified it works.

00:17:01,300 --> 00:17:05,200
The best case we saw 10% throughput improvement

00:17:05,200 --> 00:17:08,839
and 30% latency reduced.

00:17:08,839 --> 00:17:10,030
That is good.

00:17:11,530 --> 00:17:15,110
But we're not sure if you can

00:17:16,890 --> 00:17:19,751
cache out projects, sorry--

00:17:19,751 --> 00:17:23,001
(man speaking off mic)

00:17:24,390 --> 00:17:28,170
- Sorry, so the latency improvements

00:17:28,170 --> 00:17:30,350
and the throughput you measure, is that for

00:17:30,350 --> 00:17:32,630
the nonAVX part or the AVX part.

00:17:33,470 --> 00:17:35,350
- [Aubrey] It was the nonAVX part.

00:17:35,350 --> 00:17:36,670
- The nonAVX part? - Yeah.

00:17:36,670 --> 00:17:38,920
- Have you looked into what happens

00:17:38,920 --> 00:17:41,720
to the AVX part when you schedule them on the same core?

00:17:43,690 --> 00:17:47,100
- [Aubrey] We didn't look at AVX part because

00:17:47,100 --> 00:17:48,310
the frequency is fixed.

00:17:50,110 --> 00:17:52,580
- Yeah, but if they're not sharing resources,

00:17:52,580 --> 00:17:54,570
they might be stepping on each other's foot

00:17:54,570 --> 00:17:56,390
and being slower.

00:17:56,390 --> 00:17:59,480
- [Aubrey] You mean the resource contention, right?

00:17:59,480 --> 00:18:00,780
- Yeah. - Okay, I got it.

00:18:03,281 --> 00:18:05,790
(man speaking off mic)

00:18:05,790 --> 00:18:08,522
It's no problem for me.

00:18:08,522 --> 00:18:12,500
Because those deep learning training tasks

00:18:12,500 --> 00:18:15,550
are long-term and they're low priority

00:18:15,550 --> 00:18:17,848
so we actually don't care about that.

00:18:17,848 --> 00:18:21,320
(man speaking off mic)

00:18:21,320 --> 00:18:24,290
To make use of CPU idles instead of

00:18:24,290 --> 00:18:26,860
forced idle in core scheduling.

00:18:29,680 --> 00:18:32,630
- The problem is that you're all looking at

00:18:32,630 --> 00:18:35,000
your particular favorite use case.

00:18:36,090 --> 00:18:39,510
The thing what I'm missing in all of those thing

00:18:39,510 --> 00:18:42,060
of the submissions, is what is happening

00:18:42,060 --> 00:18:43,890
to the general purpose use case?

00:18:43,890 --> 00:18:47,240
What is happening on a lightly loaded systems

00:18:47,240 --> 00:18:51,670
with a lot of similar tasks thingies?

00:18:51,670 --> 00:18:56,090
What is happening to the AVX workload

00:18:56,090 --> 00:18:58,790
which actually cares about AVX performance

00:18:58,790 --> 00:19:01,340
because Matthew is completely right,

00:19:01,340 --> 00:19:03,950
if I put them on the same core,

00:19:03,950 --> 00:19:08,400
they will suffer because they share the resource.

00:19:08,400 --> 00:19:11,140
If I put them on two different cores then,

00:19:11,140 --> 00:19:14,200
of course the frequency of both cores goes down

00:19:14,200 --> 00:19:16,390
but my AVX throughput goes up.

00:19:19,634 --> 00:19:22,413
The problem I see with all these things

00:19:23,270 --> 00:19:27,660
we have discussed in the last half year,

00:19:27,660 --> 00:19:30,610
whether it's related to the speculation stuff

00:19:30,610 --> 00:19:32,720
or to this kind of things,

00:19:32,720 --> 00:19:34,900
it's all very, very use case focused

00:19:34,900 --> 00:19:37,580
and I'm missing the big picture.

00:19:37,580 --> 00:19:41,230
So we don't want to build in mechanisms

00:19:41,230 --> 00:19:44,480
which just fit one single use case.

00:19:44,480 --> 00:19:46,910
That's totally bonkers.

00:19:46,910 --> 00:19:49,180
Because everybody else will suffer from it

00:19:49,180 --> 00:19:51,500
and then we can fix the fallout there

00:19:51,500 --> 00:19:54,360
and we're going to create more duct tape

00:19:54,360 --> 00:19:56,520
and more heuristics to figure out what to do

00:19:56,520 --> 00:19:59,180
for a particular workload is correct.

00:19:59,180 --> 00:20:01,590
So I'm not seeing the big picture here.

00:20:01,590 --> 00:20:05,150
It's all too narrow-banded focused on a single thing.

00:20:10,800 --> 00:20:13,510
- I kind of agree and disagree with that.

00:20:13,510 --> 00:20:18,510
I guess in this case you optimized for the nonAVX scenario

00:20:20,260 --> 00:20:24,080
but hypothetically you could optimize

00:20:24,080 --> 00:20:28,611
to avoid AVX call affinity so your AVX tasks

00:20:28,611 --> 00:20:30,860
will perform much better.

00:20:30,860 --> 00:20:35,860
I guess we can possibly solve the same problem

00:20:36,340 --> 00:20:40,190
or multiple problems using the same solution.

00:20:40,190 --> 00:20:44,200
You can decide what kind of scheduling you want for your--

00:20:44,200 --> 00:20:47,830
- [Thomas] What is designing that user space?

00:20:47,830 --> 00:20:50,293
What is designing that?

00:20:50,293 --> 00:20:52,345
- User space. - Yep, user space.

00:20:52,345 --> 00:20:54,090
- [Thomas] And what's the fact

00:20:54,090 --> 00:20:57,090
if there is no user space decision?

00:20:57,090 --> 00:20:59,790
- [Man in Front] There should be user space decision.

00:20:59,790 --> 00:21:02,070
- I mean, if you blew the system up,

00:21:02,070 --> 00:21:07,020
and there is nothing configuring that stuff,

00:21:07,020 --> 00:21:09,270
then it should have the same default,

00:21:09,270 --> 00:21:12,390
which is not worse than what we have now.

00:21:12,390 --> 00:21:17,390
If it then starts to do silly, bonkersy faults

00:21:17,610 --> 00:21:21,400
then I'm going a lot of people are going to be unhappy,

00:21:21,400 --> 00:21:22,540
including me.

00:21:23,530 --> 00:21:25,020
(everyone laughing)

00:21:25,020 --> 00:21:27,374
- By default it would be off.

00:21:27,374 --> 00:21:29,810
We have the starting from the build.

00:21:29,810 --> 00:21:32,710
It's a configure option and once you have

00:21:32,710 --> 00:21:36,750
cornerline parameters to actually enable it or disable it.

00:21:36,750 --> 00:21:40,160
And even if it's enabled, once the system comes up,

00:21:40,160 --> 00:21:42,550
if your process are not grouped,

00:21:42,550 --> 00:21:46,060
many they are not tagged or they are not grouped,

00:21:46,060 --> 00:21:48,140
then this feature will not be active

00:21:48,140 --> 00:21:49,510
and the core part will not be taken.

00:21:49,510 --> 00:21:50,810
- [Thomas] What means not grouped?

00:21:50,810 --> 00:21:53,540
A lot of systems today come up with groups.

00:21:54,470 --> 00:21:56,320
If you mean group as a cgroup?

00:21:56,320 --> 00:21:57,790
- Grouping has to be done...

00:21:57,790 --> 00:21:59,580
No, C groups is there but you have

00:21:59,580 --> 00:22:01,910
to specifically enable a file on the C group,

00:22:01,910 --> 00:22:04,107
which is by default zero. - Okay.

00:22:04,107 --> 00:22:07,500
So the default is just keep the current state.

00:22:07,500 --> 00:22:08,650
- Exactly, exactly.

00:22:08,650 --> 00:22:10,310
- [Thomas] Okay, I'm fine with that.

00:22:12,360 --> 00:22:15,807
- Thank you, so I think our core scheduling

00:22:15,807 --> 00:22:19,900
is a good option for us but I'm not sure

00:22:19,900 --> 00:22:22,830
you can catch our project schedule

00:22:24,310 --> 00:22:27,010
and steal them for other options.

00:22:35,110 --> 00:22:37,850
- Hello, my name is Han Scherner and you might know me

00:22:37,850 --> 00:22:40,411
that I publish the core scheduling patch.

00:22:40,411 --> 00:22:42,580
And in this context, I just want

00:22:42,580 --> 00:22:45,380
to raise the awareness for all the other use cases

00:22:45,380 --> 00:22:46,330
that are out there.

00:22:47,250 --> 00:22:50,650
We might not be able to do with just core scheduling

00:22:50,650 --> 00:22:53,480
but where we need to do something more.

00:22:53,480 --> 00:22:56,350
On the left hand side, that's the classical core scheduling

00:22:56,350 --> 00:22:58,170
that we are currently talking about.

00:22:58,170 --> 00:23:03,170
So synchronizing the execution of two tasks across time

00:23:04,080 --> 00:23:09,080
but not going beyond core level.

00:23:09,260 --> 00:23:10,940
That works well enough for

00:23:10,940 --> 00:23:13,120
the A1TF and the MDS use cases.

00:23:13,120 --> 00:23:15,977
It also works for the, where's Aubrey,

00:23:15,977 --> 00:23:20,640
for the AVX use case.

00:23:20,640 --> 00:23:22,780
But, for example, it stops working for

00:23:22,780 --> 00:23:25,220
other resource contention use cases.

00:23:25,220 --> 00:23:27,190
If you are not that lucky to have a CPU

00:23:27,190 --> 00:23:31,530
or a processor that can control the frequency per core

00:23:31,530 --> 00:23:34,650
but only per package, then you would still

00:23:34,650 --> 00:23:36,080
need something like this but you would

00:23:36,080 --> 00:23:39,880
basically want to scratch out core and write socket there

00:23:39,880 --> 00:23:44,500
so that you can actually pair all the AVX tasks

00:23:47,080 --> 00:23:49,890
or unimportant tasks across all CPUS.

00:23:49,890 --> 00:23:53,550
So that you can, for example, get tired or

00:23:53,550 --> 00:23:58,040
boost frequencies for those tasks you actually care about.

00:23:59,450 --> 00:24:02,870
On the other hand, on the right hand side,

00:24:04,030 --> 00:24:07,470
is a different example which basically represents

00:24:07,470 --> 00:24:09,613
all kind of parallel applications,

00:24:09,613 --> 00:24:13,350
where you have some optimization.

00:24:13,350 --> 00:24:15,130
For example, at SMP level.

00:24:16,020 --> 00:24:18,250
So in that case, you don't really need

00:24:18,250 --> 00:24:23,250
the isolation property, at least not across the outer level.

00:24:23,250 --> 00:24:24,710
But you might still want to have

00:24:24,710 --> 00:24:26,710
the isolation property on the SNT level.

00:24:29,130 --> 00:24:34,090
And that's basically it.

00:24:34,090 --> 00:24:35,990
So there are use cases around it

00:24:35,990 --> 00:24:38,210
and if we go the long way of implementing

00:24:38,210 --> 00:24:42,348
a new scheduling mechanism for our something,

00:24:42,348 --> 00:24:46,320
for core scheduling, I think we should go

00:24:46,320 --> 00:24:49,550
the long route and kind of making it little more versatile.

00:24:51,170 --> 00:24:53,420
And that's a point that I want to get across.

00:25:00,260 --> 00:25:01,093
- We got into a point about using CPUs

00:25:01,093 --> 00:25:05,545
yet to group the tiles

00:25:05,545 --> 00:25:08,087
into a certain self-CPU?

00:25:08,087 --> 00:25:13,087
So CPU sets would work well as long as your system

00:25:13,680 --> 00:25:15,180
is not over committed.

00:25:15,180 --> 00:25:18,420
As soon as you get into a more over committed scenario,

00:25:18,420 --> 00:25:19,950
CPU sets stop working because

00:25:19,950 --> 00:25:24,490
they don't give you the simultaneousness guarantee

00:25:24,490 --> 00:25:28,746
if you have more load on the same core CPU system.

00:25:28,746 --> 00:25:32,070
- Yeah, but even with CPUs that you can have

00:25:32,070 --> 00:25:34,882
two CPU set but they can share some of the CPU,

00:25:34,882 --> 00:25:36,970
so you can configure it this way.

00:25:36,970 --> 00:25:38,470
It don't need to be exclusive.

00:25:39,790 --> 00:25:41,930
- But this doesn't give you coordination

00:25:41,930 --> 00:25:43,520
across different CPUs.

00:25:43,520 --> 00:25:44,970
I might be missing the point.

00:25:47,920 --> 00:25:49,930
- You want them to want to get that, yes,

00:25:49,930 --> 00:25:53,127
you need some kind of core scheduling implementation, yep.

00:26:02,760 --> 00:26:04,330
- One question is I haven't seen

00:26:04,330 --> 00:26:06,060
in this morning's presentation is

00:26:06,060 --> 00:26:09,990
how you ensure the fairness of other group

00:26:09,990 --> 00:26:11,290
with the cross scheduling?

00:26:13,300 --> 00:26:14,951
(man speaking off mic)

00:26:14,951 --> 00:26:17,473
- It depends on who you ask

00:26:22,298 --> 00:26:24,456
- Can I comment here?

00:26:24,456 --> 00:26:27,530
- How you are ensure because you are forcing

00:26:28,919 --> 00:26:32,972
a task from the same group to be scheduled

00:26:32,972 --> 00:26:36,890
So how do you ensure that a group of tasks

00:26:36,890 --> 00:26:39,710
will not get more runtime than the other one?

00:26:39,710 --> 00:26:44,550
- So how the algorithm works is on a per

00:26:44,550 --> 00:26:46,390
run queue basis, you'll have it sorted

00:26:46,390 --> 00:26:48,036
like know what process to run next,

00:26:48,036 --> 00:26:49,720
so you'll pick that.

00:26:51,189 --> 00:26:53,300
Basically you'll try to take the process

00:26:53,300 --> 00:26:56,060
with the highest priority or which has been stalled most,

00:26:56,060 --> 00:26:58,280
if it's a re-runtime based.

00:26:58,280 --> 00:27:00,840
So you'll have a code wide view now.

00:27:00,840 --> 00:27:02,697
Take the process with the highest priority

00:27:02,697 --> 00:27:04,910
and then tries to match it.

00:27:04,910 --> 00:27:07,600
But what the system focuses re-runtime changes

00:27:07,600 --> 00:27:10,800
and the process that needs more runtime,

00:27:10,800 --> 00:27:12,540
it'll be fair automatically.

00:27:12,540 --> 00:27:15,013
So you'll get a group running so other

00:27:15,013 --> 00:27:18,570
groups will be waiting and it's re-runtime is smaller now.

00:27:18,570 --> 00:27:20,870
So next schedule will automatically pick that.

00:27:22,580 --> 00:27:25,990
So it's basically CSF itself, when you are taking about

00:27:25,990 --> 00:27:28,130
fair script class, CSF itself.

00:27:28,130 --> 00:27:31,370
But when you try to run a group,

00:27:31,370 --> 00:27:35,860
you try to make sure that one of the tasks in that group

00:27:35,860 --> 00:27:37,310
is the highest priority task.

00:27:44,280 --> 00:27:46,620
- [Man With Mic] Say you have two CPUs and you peak

00:27:46,620 --> 00:27:48,440
the highest priority one on--

00:27:49,491 --> 00:27:50,491
- On the whole code.

00:27:51,980 --> 00:27:53,990
- Okay, so this is per call.

00:27:53,990 --> 00:27:55,770
- Per call, yeah, that's why we need to have

00:27:55,770 --> 00:27:59,850
a per call knowledge abled which task has the most priority.

00:28:01,150 --> 00:28:04,390
So you consider both run queues and see

00:28:04,390 --> 00:28:05,500
which one has the highest priority

00:28:05,500 --> 00:28:08,450
in both the run queues and then pick your sibling for that.

00:28:09,340 --> 00:28:13,070
- Okay, but what if, I'm not sure if it can happen,

00:28:13,070 --> 00:28:15,360
but what if the process you're looking for

00:28:15,360 --> 00:28:18,980
so the corresponding has the same cookies

00:28:18,980 --> 00:28:20,570
on running another core?

00:28:21,750 --> 00:28:23,260
Can that happen?

00:28:23,260 --> 00:28:25,840
You have let's say four CPUs, two core

00:28:25,840 --> 00:28:29,540
and you pick core zero that has priority.

00:28:29,540 --> 00:28:31,540
But then the corresponding one you would

00:28:31,540 --> 00:28:34,530
want to put on the other CPU or the first core

00:28:34,530 --> 00:28:36,250
is actually running--

00:28:36,250 --> 00:28:37,620
- Another core? - Yeah.

00:28:37,620 --> 00:28:39,960
- As of now, it can happen and in that case,

00:28:39,960 --> 00:28:42,210
what happens is this particular core

00:28:42,210 --> 00:28:44,720
will run, one thread a cookie and then the other

00:28:44,720 --> 00:28:46,920
one would be forced idle because

00:28:46,920 --> 00:28:48,860
it doesn't know about the other code yet.

00:28:48,860 --> 00:28:51,540
But there is a load balancing logic also,

00:28:51,540 --> 00:28:53,790
which kicks in and tries to pull it.

00:28:53,790 --> 00:28:55,820
- [Man In Mic] So it tries to get in to be seen.

00:28:55,820 --> 00:28:56,653
- Yeah.

00:28:58,460 --> 00:28:59,920
(man speaking off mic)

00:28:59,920 --> 00:29:04,150
- Hi, just a quick point, the way people are using cores

00:29:04,150 --> 00:29:05,110
is not very clear.

00:29:05,110 --> 00:29:06,290
When you're talking about cores its it like

00:29:06,290 --> 00:29:09,590
literally one core or is it a bunch of,

00:29:09,590 --> 00:29:11,840
some SMP cores as one core?

00:29:13,050 --> 00:29:14,620
To me a core is literally just on CPU

00:29:14,620 --> 00:29:17,280
so I'm kind of confused when you give the answer to him.

00:29:17,280 --> 00:29:22,280
- Core consists of SMTs like hyperthreads.

00:29:23,193 --> 00:29:25,660
- Okay. - So the ideology to

00:29:25,660 --> 00:29:29,020
bring it literally onto the same page,

00:29:29,020 --> 00:29:31,210
a socket can have multiple cores

00:29:31,210 --> 00:29:33,230
and that core can have multiple hyperthreads.

00:29:33,230 --> 00:29:34,517
That's done--

00:29:34,517 --> 00:29:35,350
- In this case, were just talking about

00:29:35,350 --> 00:29:36,183
between the hyperthreads.

00:29:36,183 --> 00:29:37,080
- Exactly. - Okay.

00:29:37,080 --> 00:29:39,100
- So the idea that we are trying to solve

00:29:39,100 --> 00:29:42,169
is the security problem because of hyperthreads.

00:29:42,169 --> 00:29:45,336
(men speaking of mic)

00:29:46,310 --> 00:29:48,130
- So what I'm actually really curious on,

00:29:48,130 --> 00:29:50,510
I'm not sure if this conversation right now

00:29:50,510 --> 00:29:51,850
is actually getting us anywhere.

00:29:51,850 --> 00:29:53,810
What I'm really curious on,

00:29:53,810 --> 00:29:56,280
we've been having core and core scheduling

00:29:56,280 --> 00:29:58,080
discussions for what, two years now?

00:29:59,340 --> 00:30:03,142
And as far as I know, nothing is upstream, right?

00:30:03,142 --> 00:30:05,840
We have millions of systems out there

00:30:05,840 --> 00:30:07,470
running some kind of home grown

00:30:07,470 --> 00:30:11,200
core scheduling systems but not we

00:30:11,200 --> 00:30:13,370
as in Linux users, in general.

00:30:13,370 --> 00:30:16,200
But all of that is downstream code,

00:30:16,200 --> 00:30:17,550
which is a terrible situation to be in,

00:30:17,550 --> 00:30:18,880
it's not where you want to be.

00:30:18,880 --> 00:30:20,470
So what is keeping us from actually

00:30:20,470 --> 00:30:22,150
just having a solution?

00:30:22,150 --> 00:30:24,580
- Exactly, so the whole point of this MC

00:30:24,580 --> 00:30:28,330
is actually trying to come up with ideas

00:30:28,330 --> 00:30:31,350
that would help to actually mainline it.

00:30:31,350 --> 00:30:35,760
And what is keeping us from pushing it to mainline

00:30:35,760 --> 00:30:39,060
is the difference in ideology.

00:30:40,200 --> 00:30:45,200
So MDS and picture, MDS we can have

00:30:45,997 --> 00:30:50,110
two threads on the same code attacking each other

00:30:50,110 --> 00:30:53,190
or a thread can in the USLM can attack the kernel.

00:30:53,190 --> 00:30:56,830
And this specific core scheduling feature

00:30:56,830 --> 00:30:58,710
can not mitigate that.

00:30:58,710 --> 00:31:00,340
So core scheduling tries to make sure

00:31:00,340 --> 00:31:02,220
that trusted tasks can execute

00:31:02,220 --> 00:31:03,900
at the same time on the core.

00:31:03,900 --> 00:31:06,474
But what if you have two trusted tasks running

00:31:06,474 --> 00:31:09,010
and one task goes into the kernel?

00:31:09,010 --> 00:31:12,370
Either way this is core or a VM exit or an interrupt.

00:31:12,370 --> 00:31:15,700
Now you have one task and one kernel running

00:31:15,700 --> 00:31:19,170
and this task can not be trusted with respect to the kernel.

00:31:19,170 --> 00:31:21,350
So that has been the whole topic

00:31:21,350 --> 00:31:26,350
which is preventing going to mainline.

00:31:26,570 --> 00:31:29,960
So we are trying to see what can be the solutions now.

00:31:31,143 --> 00:31:34,393
(man speaking off mic)

00:31:37,609 --> 00:31:38,442
- [Man With Mic] Can you turn around

00:31:38,442 --> 00:31:39,880
and just wave so I know?

00:31:39,880 --> 00:31:44,030
- There is a separate isolation, the address isolation

00:31:44,030 --> 00:31:46,070
floating around for that but

00:31:46,070 --> 00:31:50,990
from what I read from that one thread, it's not rosy.

00:31:50,990 --> 00:31:53,960
And I think Thomas had some comments on that.

00:31:55,330 --> 00:31:58,010
Yeah, but we need both to fully secure.

00:32:00,986 --> 00:32:04,236
(man speaking off mic)

00:32:06,420 --> 00:32:11,420
- On the core scheduling for this core scheduling thing,

00:32:13,390 --> 00:32:17,680
it seems like the core scheduling is a lot more complicated

00:32:17,680 --> 00:32:20,930
and I have not seen any benchmark results yet.

00:32:20,930 --> 00:32:23,260
Maybe I just missed them but I haven't seen

00:32:23,260 --> 00:32:25,070
any to justify the extra complexity.

00:32:29,946 --> 00:32:33,020
- I don't think I posted the updated ones,

00:32:33,020 --> 00:32:34,470
I have some summary old ones.

00:32:36,260 --> 00:32:37,900
You're probably not interested in those.

00:32:37,900 --> 00:32:40,190
It's more complicated,

00:32:41,902 --> 00:32:43,200
I don't know if it's more complicated.

00:32:43,200 --> 00:32:46,000
It's maybe more complicated on a core level

00:32:46,000 --> 00:32:49,850
but not on a conceptual level, I think (laughing).

00:32:51,610 --> 00:32:55,840
Because or for example the decision which task to pick,

00:32:55,840 --> 00:32:58,900
I think you do that online and have

00:32:58,900 --> 00:33:01,350
to compare actually all the queues.

00:33:01,350 --> 00:33:04,160
In my approach, that data is already aggregated

00:33:04,160 --> 00:33:07,380
in a run queue, in that case for a core.

00:33:07,380 --> 00:33:09,270
So there is a core global run queue,

00:33:09,270 --> 00:33:11,800
which basically excludes pairs of tasks

00:33:11,800 --> 00:33:14,460
or has pairs of tasks, simply speaking.

00:33:14,460 --> 00:33:16,300
And there you don't have the issue

00:33:16,300 --> 00:33:19,300
of deciding which task do I need to pull next

00:33:19,300 --> 00:33:20,780
and just take the one that's first

00:33:20,780 --> 00:33:23,080
in the run queue and that's it.

00:33:23,080 --> 00:33:26,800
The other logic is more how do I aggregate

00:33:27,760 --> 00:33:29,680
the priorities I want?

00:33:29,680 --> 00:33:31,950
- And from a security perspective,

00:33:31,950 --> 00:33:35,090
I guess if the mitigation is running off hyperthreading

00:33:35,090 --> 00:33:38,220
then the comparison has to be against that.

00:33:38,220 --> 00:33:41,517
- The security problems are the same.

00:33:41,517 --> 00:33:43,500
- So at the lower end where you're not

00:33:43,500 --> 00:33:44,920
the worst subscribed, yes turning off

00:33:44,920 --> 00:33:46,450
hyperthreading is better

00:33:46,450 --> 00:33:49,050
but as you start getting over subscribed,

00:33:49,050 --> 00:33:52,050
at that point in time the benefits are

00:33:52,050 --> 00:33:54,220
starting to skew towards core scheduling.

00:33:54,220 --> 00:33:56,500
- What nobody asked though, - Sorry.

00:33:56,500 --> 00:33:58,131
- [Man] I was wondering between the two,

00:33:58,131 --> 00:34:01,700
is the extra complexity of a more complicated thing

00:34:01,700 --> 00:34:03,250
justified versus the simpler thing?

00:34:03,250 --> 00:34:04,370
- [Man] - Okay, I'm sorry--

00:34:04,370 --> 00:34:06,930
- Trying to figure out what code

00:34:06,930 --> 00:34:09,790
do we want to maintain for the next decade or so.

00:34:10,900 --> 00:34:12,610
- For core scheduling specifically,

00:34:12,610 --> 00:34:15,810
we have done the performance benchmarks and

00:34:15,810 --> 00:34:17,840
all of them are posted on the thread.

00:34:17,840 --> 00:34:20,930
But basically if you use the CPU,

00:34:20,930 --> 00:34:22,470
core scheduling makes sense.

00:34:22,470 --> 00:34:25,760
If you are IO band, we really don't care.

00:34:25,760 --> 00:34:29,450
But it's about 40% idle.

00:34:29,450 --> 00:34:32,220
If you go lower than 40% idle then

00:34:32,220 --> 00:34:36,450
you can see the shift between no SMT being useful

00:34:36,450 --> 00:34:38,750
and having less impact than core scheduling.

00:34:39,701 --> 00:34:42,051
But it's really just pure CPU power, that's it.

00:34:45,420 --> 00:34:46,470
- Maybe one addition.

00:34:48,310 --> 00:34:51,970
So for what you said more complex solution,

00:34:51,970 --> 00:34:54,950
the overhead is basically similar to the

00:34:54,950 --> 00:34:56,820
currently still nested C groups.

00:34:56,820 --> 00:35:01,583
So one hierarchy level is included into one nested C group.

00:35:04,930 --> 00:35:07,420
But as long as you don't make use of the

00:35:07,420 --> 00:35:09,530
core scheduling feature there,

00:35:09,530 --> 00:35:12,180
you should be able to avoid this overhead completely.

00:35:13,222 --> 00:35:15,340
- [Man] I'm actually working on ripping out the hierarchy

00:35:15,340 --> 00:35:16,490
but that's a bad loser.

00:35:28,300 --> 00:35:33,300
- In favor of core scheduling, what we think is

00:35:33,568 --> 00:35:37,260
a great, it doesn't fix all the problems.

00:35:38,220 --> 00:35:40,620
Still there is availability when a user task

00:35:40,620 --> 00:35:42,550
is running with the kernel task.

00:35:42,550 --> 00:35:46,607
But there are use cases, like for workloads

00:35:48,800 --> 00:35:50,150
that needs this basically.

00:35:50,150 --> 00:35:52,315
There are use cases where it can make use of it

00:35:52,315 --> 00:35:54,810
and be very secure.

00:35:54,810 --> 00:35:59,810
So with the mitigation options exactly in place,

00:36:00,090 --> 00:36:04,060
I think it would be better to have core scheduling

00:36:04,060 --> 00:36:06,420
mainlined so that people who want to use

00:36:06,420 --> 00:36:09,830
can use it but by default it's turned off.

00:36:10,980 --> 00:36:14,320
For example, VM like workloads, for LMTF

00:36:14,320 --> 00:36:17,790
you need the privilege boundaries basically on VM exits.

00:36:19,748 --> 00:36:21,330
And on an ideal theoretical case,

00:36:21,330 --> 00:36:23,520
there are not much VM exits, unless you are not

00:36:23,520 --> 00:36:24,890
configured correctly.

00:36:24,890 --> 00:36:29,890
And for MDS, for virtualization workloads,

00:36:30,170 --> 00:36:32,190
I think VM exits are only privileged boundary

00:36:32,190 --> 00:36:35,840
because you have VMs running and you don't have

00:36:35,840 --> 00:36:37,340
user LAN processes which tries

00:36:37,340 --> 00:36:39,970
to leak information from the kernel.

00:36:39,970 --> 00:36:43,240
So like this there are specific use cases

00:36:43,240 --> 00:36:45,278
which can make use of it.

00:36:45,278 --> 00:36:48,528
(man speaking off mic)

00:36:55,720 --> 00:36:58,090
- You have the same problem with L1TF.

00:36:58,090 --> 00:37:03,090
Once one sibling goes out of the guest into the host,

00:37:04,289 --> 00:37:09,289
then it will fill up the L1,

00:37:10,290 --> 00:37:13,190
which is shared with host data.

00:37:14,050 --> 00:37:19,050
Which then the other sibling would

00:37:19,470 --> 00:37:22,010
in a malicious way can expose.

00:37:22,010 --> 00:37:27,010
It's not, it's hard but it's actually an insane fast

00:37:30,520 --> 00:37:33,590
so it's very efficient so you can do it.

00:37:33,590 --> 00:37:36,790
- Yes but we need to take care of the VM exit condition.

00:37:36,790 --> 00:37:38,620
VM exit is like a privileged boundary

00:37:38,620 --> 00:37:40,890
which we need to take care of.

00:37:40,890 --> 00:37:42,930
But if we can take care of the VM exit

00:37:42,930 --> 00:37:43,986
privilege boundary,

00:37:43,986 --> 00:37:47,010
like if we can make sure that on a VM exit

00:37:47,010 --> 00:37:49,910
you don't run an untrusted task on the sibling,

00:37:49,910 --> 00:37:51,430
then we are safe.

00:37:51,430 --> 00:37:56,430
- Yeah, but if you did a group thingie,

00:37:57,380 --> 00:37:59,150
this grouping thingie, and you have two

00:37:59,150 --> 00:38:04,150
untrusted guest VCPUs running on that core,

00:38:04,920 --> 00:38:08,310
once one goes out, you have to kick the other out as well.

00:38:08,310 --> 00:38:10,310
- Yes, so the VM exit is the boundary

00:38:10,310 --> 00:38:12,520
that we are planning to kick out the other one.

00:38:12,520 --> 00:38:14,963
- And you basically have the same problem

00:38:14,963 --> 00:38:19,963
on the host side for Cisco's interrupts and everything else.

00:38:24,390 --> 00:38:26,410
- That's correct, but what I'm trying to say is

00:38:26,410 --> 00:38:28,680
on a virtualization workload,

00:38:28,680 --> 00:38:30,180
if you just take it off VM exit,

00:38:30,180 --> 00:38:32,720
you don't need to take it off Cisco's running threads,

00:38:33,720 --> 00:38:35,450
you just need to take it off the VM exit

00:38:35,450 --> 00:38:39,080
because if you put a security boundary there--

00:38:39,080 --> 00:38:43,610
- No, it's the same because from the kernel's point of view,

00:38:43,610 --> 00:38:48,100
you either go into user space via sys exit

00:38:48,100 --> 00:38:50,730
or you go into user space via VM enter,

00:38:50,730 --> 00:38:52,520
it's the same thing.

00:38:52,520 --> 00:38:56,060
From a kernel point of view, you're just going out

00:38:56,060 --> 00:38:59,760
into a different domain.

00:38:59,760 --> 00:39:02,580
But both are untrusted and I don't care

00:39:02,580 --> 00:39:05,970
which one is more trusted or less trusted,

00:39:05,970 --> 00:39:07,510
both are untrusted.

00:39:07,510 --> 00:39:10,340
And the boundaries, either you go out to user space

00:39:10,340 --> 00:39:12,290
or you'll go into a VM.

00:39:12,290 --> 00:39:16,860
But the returning from them by any means

00:39:16,860 --> 00:39:21,860
what is either a VM exit or a sys call

00:39:22,250 --> 00:39:26,030
or an interrupt, which hits user space,

00:39:26,030 --> 00:39:28,780
then you'll have the same situation

00:39:28,780 --> 00:39:31,790
that you have one in the kernel boundary

00:39:31,790 --> 00:39:33,680
and one in the untrusted boundary.

00:39:36,060 --> 00:39:37,830
So it's not any different.

00:39:39,200 --> 00:39:41,610
- [Man] You are right but VM exit is the first thing

00:39:41,610 --> 00:39:43,570
that happens when a CPU comes out.

00:39:43,570 --> 00:39:46,940
Like a VCPU comes out to a kernel or a user,

00:39:46,940 --> 00:39:48,306
VM exit is the first thing that happens.

00:39:48,306 --> 00:39:50,130
- No, no,

00:39:50,130 --> 00:39:53,280
you have idle, when you're talking about guests

00:39:53,280 --> 00:39:54,970
you have VM exit.

00:39:54,970 --> 00:39:57,910
When you're talking about malicious code

00:39:57,910 --> 00:40:00,520
that are landing on the host, you have Cisco

00:40:00,520 --> 00:40:02,170
or interrupt or an exception.

00:40:02,170 --> 00:40:03,490
[Man] You are right, you are right.

00:40:03,490 --> 00:40:05,620
So I was telling about a specific

00:40:05,620 --> 00:40:07,308
use case of virtualization.

00:40:07,308 --> 00:40:08,910
So my point--

00:40:08,910 --> 00:40:10,850
- But you're back to that point

00:40:10,850 --> 00:40:12,940
where I was criticizing before.

00:40:12,940 --> 00:40:16,520
Don't stop at your specific use case,

00:40:16,520 --> 00:40:18,470
look at the full picture.

00:40:18,470 --> 00:40:21,580
And it's exactly the same problem,

00:40:21,580 --> 00:40:23,380
it doesn't matter how you paint it.

00:40:24,230 --> 00:40:27,600
Whether it's called VM exit or it's called sys enter,

00:40:27,600 --> 00:40:28,620
it doesn't matter.

00:40:28,620 --> 00:40:33,070
It's the same thing from a security point of view.

00:40:35,410 --> 00:40:39,580
Forget about the mechanism, talk about the concept.

00:40:39,580 --> 00:40:43,780
The concept is one thread moves into

00:40:43,780 --> 00:40:47,515
a different security domain and then

00:40:47,515 --> 00:40:50,850
you have to act on the other thread

00:40:50,850 --> 00:40:52,100
in some way or the other.

00:40:52,970 --> 00:40:54,890
That's the whole point and you have to do it

00:40:54,890 --> 00:40:58,740
for all the mechanisms we have, of course,

00:40:58,740 --> 00:41:01,300
and that's where things gets really interesting.

00:41:02,220 --> 00:41:05,110
- [Man] Yeah, so if we consider all these cases

00:41:05,110 --> 00:41:08,320
like sys calls, VM exits, all these cases,

00:41:08,320 --> 00:41:13,207
then the fix will be like considering the no SMT case,

00:41:14,870 --> 00:41:17,070
but the performance would be much, much worser.

00:41:17,070 --> 00:41:18,690
I agree, I agree with that.

00:41:18,690 --> 00:41:20,172
- Yeah sure, but you have to do it,

00:41:20,172 --> 00:41:21,940
otherwise it's a half solution.

00:41:23,195 --> 00:41:26,037
- Jeff, Jeff, if I may, just one thing.

00:41:26,037 --> 00:41:30,075
There seems to a lot of discussion between

00:41:30,075 --> 00:41:32,180
is it black or white.

00:41:32,180 --> 00:41:34,580
So what I'm wondering is whether you can use

00:41:34,580 --> 00:41:36,721
internal kernel statistics information

00:41:36,721 --> 00:41:41,721
to differentiate between a user space, CPU-intensive task

00:41:43,800 --> 00:41:46,030
and a task that is going back and forth

00:41:46,030 --> 00:41:48,220
between security domains very often.

00:41:48,220 --> 00:41:50,130
So that task going back and forth

00:41:50,130 --> 00:41:52,870
would not benefit from doing core scheduling

00:41:52,870 --> 00:41:54,490
if it wants to mitigate the MDS.

00:41:55,357 --> 00:41:57,880
- [Thomas] But it's the same problem for guests

00:41:57,880 --> 00:42:01,180
who do a lot of VM exits.

00:42:01,180 --> 00:42:03,180
We have the numbers already.

00:42:03,180 --> 00:42:05,050
It becomes insane slow.

00:42:05,900 --> 00:42:08,470
- [Man] But those should be used to figure out

00:42:08,470 --> 00:42:10,930
where it makes sense to use core scheduling

00:42:10,930 --> 00:42:11,900
for specific tasks.

00:42:13,280 --> 00:42:15,567
- Yeah, but still you have to have to,

00:42:15,567 --> 00:42:20,567
that doesn't make the underlying requirement go away,

00:42:24,450 --> 00:42:29,450
which says if one transitions into the kernel

00:42:29,480 --> 00:42:31,560
then you have to act upon the other.

00:42:32,960 --> 00:42:34,950
And whether that's frequent or whether

00:42:34,950 --> 00:42:37,770
you disable it if it's frequent or not

00:42:37,770 --> 00:42:39,510
that's a totally different playground

00:42:39,510 --> 00:42:41,830
that plays into the performance thing.

00:42:41,830 --> 00:42:43,440
The other is a correctness thing

00:42:43,440 --> 00:42:45,950
and do not conflate them.

00:42:45,950 --> 00:42:50,950
You need the protection mechanism in any case.

00:42:51,640 --> 00:42:55,690
And if you decide it's too heavy for that workload

00:42:55,690 --> 00:43:00,070
and I just isolate that on a single core forever

00:43:00,070 --> 00:43:03,790
and basically disable SMT on that core

00:43:03,790 --> 00:43:06,090
by software means without disabling it,

00:43:06,090 --> 00:43:10,770
actually, then this is just a policy thing

00:43:10,770 --> 00:43:11,603
- Exactly.

00:43:11,603 --> 00:43:14,310
You're right and that's why the cost scheduling feature

00:43:14,310 --> 00:43:15,870
is really flexible.

00:43:15,870 --> 00:43:19,730
We saw that if you what we see the VM on the new workloads

00:43:19,730 --> 00:43:22,840
where we only run untrusted code and the safe.

00:43:22,840 --> 00:43:25,350
is considered safe.

00:43:25,350 --> 00:43:27,700
That's the whole point, it's just we can configurate

00:43:27,700 --> 00:43:28,533
however we want.

00:43:28,533 --> 00:43:29,470
It's not the one--

00:43:29,470 --> 00:43:31,290
- [Thomas] Do you have all

00:43:31,290 --> 00:43:33,550
the transition cases mitigated by now?

00:43:33,550 --> 00:43:36,400
- We did them, we did them, of course we did them.

00:43:36,400 --> 00:43:38,250
- [Thomas] No, the question was

00:43:38,250 --> 00:43:39,960
they are not there yet, right?

00:43:39,960 --> 00:43:41,520
- No, they're not there. - Yep.

00:43:41,520 --> 00:43:44,020
- Right now we have only user space to user space,

00:43:44,020 --> 00:43:44,853
that's the only one.

00:43:44,853 --> 00:43:46,930
But we want to add them and then we will have

00:43:46,930 --> 00:43:50,040
the flexibility to look at how we would want

00:43:50,040 --> 00:43:51,540
to configure because you're absolutely right,

00:43:51,540 --> 00:43:54,570
we need a way to express and all media administrators

00:43:54,570 --> 00:43:56,730
knows their workload.

00:43:56,730 --> 00:43:58,710
That's absolutely in-- - Right.

00:43:58,710 --> 00:44:01,020
- So I think the thing to remember is

00:44:02,040 --> 00:44:04,192
this isn't only to scheduler a problem?

00:44:04,192 --> 00:44:07,310
Handling the Cisco mocks and synchronizing

00:44:07,310 --> 00:44:08,790
on kernel entry/exist is really isn't

00:44:08,790 --> 00:44:10,720
something we can do in the scheduler.

00:44:10,720 --> 00:44:12,870
So full mitigation is going to consist of

00:44:12,870 --> 00:44:16,572
something that does that and co-scheduling.

00:44:16,572 --> 00:44:19,132
You could imagine that in the case

00:44:19,132 --> 00:44:21,840
like in our problem case where something

00:44:21,840 --> 00:44:24,370
is causing lots of exits and is causing

00:44:24,370 --> 00:44:27,522
lots of interference, that we give it its own tag

00:44:27,522 --> 00:44:32,400
so that we schedule that on it's own core

00:44:32,400 --> 00:44:35,090
without actually needing to turn off S & T.

00:44:35,090 --> 00:44:37,100
But that would be an optimization we could make.

00:44:37,100 --> 00:44:39,350
But I do think you need to do something like,

00:44:40,630 --> 00:44:43,090
the address space isolation trick so we don't need

00:44:43,090 --> 00:44:46,070
to do it on every sys call and we don't need to do 1NMF.

00:44:46,070 --> 00:44:48,520
But that's a separate piece from the scheduler.

00:44:48,520 --> 00:44:50,740
So I think we need to be able to talk about pieces

00:44:50,740 --> 00:44:53,138
separately and talk about how we can assemble them

00:44:53,138 --> 00:44:56,090
into a mitigation as the set.

00:44:56,090 --> 00:44:58,500
But it certainly won't be one sized fits all

00:44:58,500 --> 00:45:01,360
and the scheduler stuff won't handle the Cisco sync.

00:45:02,280 --> 00:45:04,372
- No, the scheduler won't handle the Cisco

00:45:04,372 --> 00:45:08,069
but you have to think hard about it.

00:45:08,069 --> 00:45:13,069
What are you going to do with the Cisco sync?

00:45:13,716 --> 00:45:18,716
Because will innovatively end up in this scheduler again.

00:45:20,030 --> 00:45:21,720
- I 100% agree with you.

00:45:21,720 --> 00:45:25,070
I would say that what you're saying

00:45:25,070 --> 00:45:26,540
is missing from the slide deck

00:45:26,540 --> 00:45:28,890
is the picture of here are the three pieces we need

00:45:28,890 --> 00:45:30,540
and here's how they fit together.

00:45:31,390 --> 00:45:33,336
I think that's a totally fair statement.

00:45:33,336 --> 00:45:36,820
It also doesn't mean we can't talk about

00:45:38,300 --> 00:45:39,960
pretend we have those other two pieces

00:45:39,960 --> 00:45:41,180
that we don't need.

00:45:41,180 --> 00:45:43,140
What do we need for this other piece?

00:45:43,140 --> 00:45:46,250
- [Thomas] No, no, I don't say that we have

00:45:46,250 --> 00:45:49,370
to delay course scheduling per se

00:45:51,300 --> 00:45:53,060
because we don't have the other pieces but--

00:45:53,060 --> 00:45:54,820
- What we have is the picture.

00:45:54,820 --> 00:45:57,340
- [Thomas] But we have to have the picture right now

00:45:57,340 --> 00:45:59,880
in order to get it straight because

00:45:59,880 --> 00:46:02,110
otherwise when we add the other pieces,

00:46:02,110 --> 00:46:05,490
we start over and rip out half of the code

00:46:05,490 --> 00:46:07,620
and replace it again and that's what I want to avoid.

00:46:07,620 --> 00:46:08,950
- I completely agree with you.

00:46:08,950 --> 00:46:11,250
I think the major discussion though is actually

00:46:11,250 --> 00:46:14,240
not on the scheduler side but if we can come

00:46:14,240 --> 00:46:18,490
to some consensus on the address-based isolation side.

00:46:18,490 --> 00:46:20,320
- [Thomas] Which has its own set of problems.

00:46:20,320 --> 00:46:21,740
- You know, I agree.

00:46:21,740 --> 00:46:25,360
I'm just saying I think we can define

00:46:25,360 --> 00:46:27,800
what the scheduler side of the picture looks like.

00:46:27,800 --> 00:46:29,370
We haven't defined the other side

00:46:29,370 --> 00:46:30,610
that is a take at doing it.

00:46:30,610 --> 00:46:32,350
We need to either agree on doing it that way

00:46:32,350 --> 00:46:34,520
or reject it and do something else.

00:46:34,520 --> 00:46:38,330
But the scheduler side is more defined than the other side.

00:46:38,330 --> 00:46:39,890
- [Thomas] Yeah, I'll agree with that.

00:46:39,890 --> 00:46:43,867
- So are you saying that for laws which says

00:46:43,867 --> 00:46:44,700
core scheduling plus ASI

00:46:44,700 --> 00:46:49,370
will still be HT visible,

00:46:49,370 --> 00:46:51,210
so just give that on for these cases.

00:46:51,210 --> 00:46:54,145
- The whole point of the address-based isolation

00:46:54,145 --> 00:46:57,560
is we don't need to stand.

00:46:57,560 --> 00:46:59,530
We don't need to stand typical VM exits,

00:46:59,530 --> 00:47:01,750
we don't need to stand Ciscos.

00:47:01,750 --> 00:47:04,009
Which means that the performance numbers we have

00:47:04,009 --> 00:47:07,740
are the actual performance numbers for mitigation.

00:47:07,740 --> 00:47:09,930
Now that's our priority assuming

00:47:09,930 --> 00:47:11,450
we fill in the other side of the picture

00:47:11,450 --> 00:47:12,900
with address-based isolation.

00:47:14,850 --> 00:47:16,490
So I agree with Thomas that we need

00:47:16,490 --> 00:47:17,860
to put up the whole picture and we need

00:47:17,860 --> 00:47:19,010
to agree in that.

00:47:19,010 --> 00:47:20,490
We don't need to implement it

00:47:20,490 --> 00:47:21,940
but we need to figure out the discussion

00:47:21,940 --> 00:47:22,860
on the other side of the picture.

00:47:22,860 --> 00:47:25,570
Once we have that, we could actually talk about

00:47:27,360 --> 00:47:29,460
this so oh we know we're gonna

00:47:29,460 --> 00:47:31,020
lean on that for the other parts.

00:47:31,020 --> 00:47:33,180
But that's what's complicating some of the discission.

00:47:34,930 --> 00:47:37,861
- [Thomas] So we need the picture in order to see

00:47:37,861 --> 00:47:41,530
what might be missing on the scheduler side,

00:47:41,530 --> 00:47:44,760
which we need to make the other parts work.

00:47:44,760 --> 00:47:49,709
And before we shove something in which we have to

00:47:49,709 --> 00:47:52,902
redo it half a year later again.

00:47:52,902 --> 00:47:54,660
- I completely agree on that point.

00:47:55,870 --> 00:47:56,703
- [Man With Mic] Thank you.

00:47:56,703 --> 00:47:59,380
- Sorry, we are running out of time for this topic.

00:48:00,830 --> 00:48:02,730
So we have to move to the next one but

00:48:04,412 --> 00:48:06,640
we can easily continue the discussion

00:48:06,640 --> 00:48:07,690
on the other one just after.

00:48:07,690 --> 00:48:08,840
- [Thomas] Fair enough.

00:48:16,550 --> 00:48:18,073
- Okay, so moving on.

00:48:18,073 --> 00:48:22,930
Uri Lelli working for that in the real-time team.

00:48:26,490 --> 00:48:31,410
So lately I've been working on this proxy execution area.

00:48:34,730 --> 00:48:36,890
Basically working on patches from Peter

00:48:36,890 --> 00:48:40,490
and try to see if we can get that mainline.

00:48:40,490 --> 00:48:43,110
So why we need that quick introduction,

00:48:43,110 --> 00:48:46,580
just five minutes and we have 50 minutes to discuss this.

00:48:46,580 --> 00:48:51,530
So hopefully everybody here is really familiar

00:48:51,530 --> 00:48:53,100
with the priority inheritance mechanism

00:48:53,100 --> 00:48:54,600
and how is it solved.

00:48:54,600 --> 00:48:57,120
Just here you have a simple example.

00:48:57,120 --> 00:49:00,360
So usually the standard example is three tasks

00:49:00,360 --> 00:49:01,680
on the same CPU.

00:49:01,680 --> 00:49:03,910
A low priority one, high priority one,

00:49:03,910 --> 00:49:05,450
and a middle priority one.

00:49:05,450 --> 00:49:07,720
The low and high share some resources

00:49:07,720 --> 00:49:11,170
which are protected by mutex and rtmutex in this case

00:49:11,170 --> 00:49:14,090
so it's probability enabled.

00:49:14,090 --> 00:49:17,535
And another trick is when these task have blocks.

00:49:17,535 --> 00:49:20,750
So these tasks will actually lock the mutex

00:49:20,750 --> 00:49:24,230
and then it is preempted by the highest priority one.

00:49:24,230 --> 00:49:26,080
But then when that highest priority one

00:49:26,080 --> 00:49:28,020
tries to block threads to acquire,

00:49:28,020 --> 00:49:29,540
the same mutex blocks on it

00:49:29,540 --> 00:49:32,550
and now it will priority of B.

00:49:34,190 --> 00:49:36,990
So this guy would enable to be highest priority one

00:49:36,990 --> 00:49:38,890
and will basically continue executing,

00:49:38,890 --> 00:49:43,890
even if medium priority task it wakes up.

00:49:44,010 --> 00:49:45,690
Without that basically this guy,

00:49:45,690 --> 00:49:48,400
which has nothing to do with the other two,

00:49:48,400 --> 00:49:51,050
can actually preempt the lower priority one

00:49:51,050 --> 00:49:55,070
and can definitely take the processor

00:49:56,300 --> 00:49:58,080
from the higher priority one,

00:49:58,080 --> 00:50:00,220
creating all sort of problems.

00:50:00,220 --> 00:50:05,090
So currently in Linux this problem is solved for,

00:50:05,090 --> 00:50:07,120
for example, for FIFO using basically

00:50:07,120 --> 00:50:08,710
everything in the priority.

00:50:08,710 --> 00:50:12,590
So you just copy the priority of this guy

00:50:12,590 --> 00:50:16,090
into this guy task route and then use that priority

00:50:16,090 --> 00:50:20,950
when deciding between these two tasks, which one to pick.

00:50:20,950 --> 00:50:25,340
First cadet line, we have something which looks similar

00:50:25,340 --> 00:50:27,800
but is kind of broken.

00:50:27,800 --> 00:50:31,230
That's because basically the priority for that line

00:50:31,230 --> 00:50:36,230
is covered in glass is represented by the band deadline,

00:50:38,010 --> 00:50:41,360
which well it's dynamic so it always changes.

00:50:41,360 --> 00:50:44,740
And it basically lets the notion of

00:50:44,740 --> 00:50:46,180
inheriting the bandwidth.

00:50:46,180 --> 00:50:49,290
So band line has this notion of the runtime over deadlines,

00:50:49,290 --> 00:50:53,320
so it's actually a percentage of CPU time

00:50:53,320 --> 00:50:55,610
that is granted to tasks.

00:50:55,610 --> 00:50:58,330
We currently don't inherit that thing.

00:50:59,760 --> 00:51:03,500
And this also prohibits us to make scheduled deadline

00:51:03,500 --> 00:51:05,600
available for route usage,

00:51:05,600 --> 00:51:09,480
just because that can actually open the door

00:51:09,480 --> 00:51:11,670
to other kind of problems.

00:51:11,670 --> 00:51:15,420
So the idea will be to start actually inheriting

00:51:15,420 --> 00:51:18,350
more than single values but actually properties

00:51:18,350 --> 00:51:19,360
of the tasks.

00:51:20,280 --> 00:51:23,550
And, for example, for deadline if you apply

00:51:23,550 --> 00:51:27,510
the same thing for deadline, this guy here,

00:51:27,510 --> 00:51:29,530
so the lowest priority one,

00:51:29,530 --> 00:51:33,390
would actually inherit the full properties

00:51:33,390 --> 00:51:34,750
of the highest one.

00:51:34,750 --> 00:51:36,240
In this case, the bandwidth.

00:51:36,240 --> 00:51:38,176
So it will actually consume the bandwidth

00:51:38,176 --> 00:51:41,270
of this guy when this guy blocks.

00:51:44,060 --> 00:51:46,850
Let's see, yeah, so this is basically

00:51:46,850 --> 00:51:47,900
what I want to discuss.

00:51:47,900 --> 00:51:52,900
I guess two main ideas, if you want to have

00:51:53,020 --> 00:51:55,770
a better understanding of how easy this is implemented,

00:51:56,940 --> 00:52:01,200
first thing is that we divide the notion

00:52:01,200 --> 00:52:04,850
of a scheduling context, which are the scheduling property

00:52:06,261 --> 00:52:08,320
of a task, which can be the priority,

00:52:08,320 --> 00:52:10,530
can be the runtime over period.

00:52:12,470 --> 00:52:17,030
And the donors are gonna actually donate both priorities.

00:52:17,030 --> 00:52:18,900
And I guess the other big difference

00:52:20,860 --> 00:52:24,830
with respect to how the rtmutex is worked today

00:52:24,830 --> 00:52:29,830
is that for example in case this guy blocks

00:52:30,460 --> 00:52:33,768
on the other guy, it is not actually

00:52:33,768 --> 00:52:35,640
removed for the enqueue.

00:52:35,640 --> 00:52:39,750
So we keep it on the enqueue, we just say okay,

00:52:39,750 --> 00:52:42,100
this guys is blocked on this other guy

00:52:42,100 --> 00:52:44,520
and now we let basically the scheduler run.

00:52:44,520 --> 00:52:47,000
And if this guy is actually picked up by the scheduler

00:52:47,000 --> 00:52:48,985
because it's still the highest priority one,

00:52:48,985 --> 00:52:53,985
then we basically construct the donor chain.

00:52:57,720 --> 00:53:02,403
So we basically go there and see which

00:53:02,403 --> 00:53:07,310
guys are blocked on this mutex and then we can actually know

00:53:07,310 --> 00:53:08,872
which is the highest priority one

00:53:08,872 --> 00:53:12,150
from which we want to inherit.

00:53:12,150 --> 00:53:16,000
So I think since we basically have only 10 minutes,

00:53:17,690 --> 00:53:20,940
we've been discussing these things last year.

00:53:20,940 --> 00:53:24,670
So I managed to basically pass the first version last year

00:53:24,670 --> 00:53:29,670
and unfortunately I kind of posted a newer version.

00:53:30,340 --> 00:53:34,560
I re-based the set and am basically work on it currently.

00:53:34,560 --> 00:53:37,280
There's still a few issues so as soon as I get it

00:53:38,786 --> 00:53:40,360
a little more stable, I'll post a new version.

00:53:40,360 --> 00:53:44,752
But we discussed this thing in May.

00:53:44,752 --> 00:53:45,585
In May let's say if you have

00:53:45,585 --> 00:53:46,418
a small Scheduler and Power Management Conference

00:53:46,418 --> 00:53:51,418
and there were a few guys there or over here

00:53:54,450 --> 00:53:57,970
and we're discussing if this makes sense

00:53:57,970 --> 00:54:00,030
or what are the problems around this.

00:54:00,030 --> 00:54:04,190
And my take from that discussion was that

00:54:04,190 --> 00:54:05,960
the biggest problem that we have is

00:54:05,960 --> 00:54:08,100
how do we handle the fact that

00:54:10,920 --> 00:54:15,380
the lock owner and the other tasks

00:54:15,380 --> 00:54:18,050
that blocks on the same lock can actually

00:54:18,050 --> 00:54:19,860
be running on different CPUs.

00:54:19,860 --> 00:54:23,460
So SMP is actually kind of tricky to handle.

00:54:23,460 --> 00:54:28,460
The current solution is to basically migrate

00:54:29,240 --> 00:54:33,380
all the tasks that are actually blocked on the mutex

00:54:33,380 --> 00:54:37,240
on the owner CPU and then let the scheduler

00:54:37,240 --> 00:54:38,420
run on the owner CPU.

00:54:38,420 --> 00:54:40,480
Select the highest priority one,

00:54:40,480 --> 00:54:44,870
which might at the point it will be actually

00:54:44,870 --> 00:54:49,020
be the highest priority task blocked on the mutex,

00:54:49,020 --> 00:54:51,025
and then use the mechanism.

00:54:51,025 --> 00:54:56,025
So this works if you don't have affinities for tasks.

00:54:57,667 --> 00:55:01,240
Even if we ignore affinities, since we are

00:55:01,240 --> 00:55:04,160
basically interested in the scheduling properties

00:55:04,160 --> 00:55:07,482
more than what the task is actually doing,

00:55:07,482 --> 00:55:09,800
that might be a problem for permission control

00:55:09,800 --> 00:55:12,530
for deadline because you can basically

00:55:12,530 --> 00:55:16,200
be breaking your calculation.

00:55:16,200 --> 00:55:20,037
So when you meet the task, then if the task migrates

00:55:20,037 --> 00:55:22,550
and the bandwidth contribution,

00:55:22,550 --> 00:55:25,900
the task wasn't accounted for in the destination CPU

00:55:25,900 --> 00:55:29,420
then you might break guarantees for the others.

00:55:31,519 --> 00:55:36,519
So the answer basically from the discussion

00:55:36,700 --> 00:55:39,750
that we had there was we can not really do this

00:55:39,750 --> 00:55:44,750
because it will break permission control.

00:55:46,729 --> 00:55:49,979
(man speaking off mic)

00:55:51,077 --> 00:55:53,177
I continued to think about it and actually

00:55:55,600 --> 00:55:59,360
I realized that currently permission control,

00:55:59,360 --> 00:56:03,590
we perform that at root domain level.

00:56:03,590 --> 00:56:08,130
But we don't actually enforce that every CPU

00:56:08,130 --> 00:56:10,910
has its own bandwidth location.

00:56:10,910 --> 00:56:15,310
So if I have four CPUs and they have 50%

00:56:15,310 --> 00:56:18,280
of available bandwidth on each,

00:56:18,280 --> 00:56:21,270
I'm not putting actually 50% and I'm checking

00:56:21,270 --> 00:56:24,330
that 50% when I migrate tasks.

00:56:24,330 --> 00:56:26,530
I just migrate tasks considering deadlines.

00:56:26,530 --> 00:56:31,530
So I can already be putting 100% of band on one CPU

00:56:32,190 --> 00:56:34,920
just because there are two tasks running at 50.

00:56:34,920 --> 00:56:39,920
So in this case, if the mutex thing actually happened

00:56:42,740 --> 00:56:45,700
inside the root domain, I think that's actually safe.

00:56:46,830 --> 00:56:47,663
That was my--

00:56:52,320 --> 00:56:57,210
- Yeah, so I would say permission control for

00:56:57,210 --> 00:57:00,560
slightly layered first get this thing to actually work.

00:57:03,130 --> 00:57:06,110
But the problem is the shared resource.

00:57:06,110 --> 00:57:08,380
A lot of the admission control theory

00:57:08,380 --> 00:57:11,610
does not have that determined.

00:57:11,610 --> 00:57:14,010
- Yeah, I agree that first we need to make this work

00:57:14,010 --> 00:57:17,060
but my problem is that if we go and say okay,

00:57:17,060 --> 00:57:20,610
migrating the tasks that are waiting

00:57:20,610 --> 00:57:24,370
so the potential donors to the CPU is gonna be working,

00:57:26,130 --> 00:57:29,140
the decision that's always going to be implemented.

00:57:29,140 --> 00:57:31,560
If then we found out that that's not actually working,

00:57:31,560 --> 00:57:33,440
we have to basically change it right there.

00:57:33,440 --> 00:57:36,141
- But it's the only possible auction,

00:57:36,141 --> 00:57:38,210
there isn't anything much else.

00:57:38,210 --> 00:57:41,110
It same like in the schedulability theory.

00:57:41,110 --> 00:57:45,620
A lot of the shared resources are ignored.

00:57:49,240 --> 00:57:51,340
Once you pulled actual looks in,

00:57:51,340 --> 00:57:54,650
a lot of the workloads are non-schedulable

00:57:54,650 --> 00:57:59,603
by all the algorithms proposed.

00:58:03,040 --> 00:58:04,730
- My question here is

00:58:04,730 --> 00:58:08,060
is this a practical problem or

00:58:08,060 --> 00:58:09,630
is this a theoretical problem?

00:58:09,630 --> 00:58:11,930
- It's actually both because basically

00:58:11,930 --> 00:58:13,420
the simplest thing, maybe the only thing

00:58:13,420 --> 00:58:15,418
we can actually implement and do

00:58:15,418 --> 00:58:18,750
is to migrate the task roadblock

00:58:18,750 --> 00:58:23,750
on the local owner CPU and let them run there.

00:58:23,810 --> 00:58:24,980
That's what's already--

00:58:24,980 --> 00:58:27,970
- Yeah, but this is a transitional state, right?

00:58:28,820 --> 00:58:30,140
It's not a permanent state,

00:58:30,140 --> 00:58:32,220
it's a transitional state up to the point

00:58:33,615 --> 00:58:34,660
where the lock drops.

00:58:34,660 --> 00:58:37,741
- Yeah, that is true but then you're basically using

00:58:37,741 --> 00:58:42,402
the bandwidth of tasks that were on other CPUs

00:58:42,402 --> 00:58:45,060
and you're migrating that thing on the CPU where

00:58:45,060 --> 00:58:46,720
the local one is running. - I know.

00:58:46,720 --> 00:58:48,970
- If there are other tasks that are running

00:58:48,970 --> 00:58:50,870
and consuming bandwidth, you are basically

00:58:50,870 --> 00:58:53,760
over subscribing temporarily the team.

00:58:53,760 --> 00:58:56,460
- [Thomas] Yeah, but the question is,

00:58:56,460 --> 00:58:58,620
is their temporary over subscription

00:58:58,620 --> 00:59:01,560
really, really hurting anyone

00:59:01,560 --> 00:59:03,870
because its going away immediately

00:59:03,870 --> 00:59:05,910
after the lock is dropped.

00:59:05,910 --> 00:59:08,680
And for that time there you have those

00:59:09,580 --> 00:59:12,460
scheduling properties moved over

00:59:12,460 --> 00:59:15,630
onto the other enqueue where the lock owner sits

00:59:15,630 --> 00:59:18,900
and it's a fine tune and you can not move it away.

00:59:20,600 --> 00:59:23,250
Now you can mark those not contributing

00:59:23,250 --> 00:59:26,520
to the overall accounting temporarily

00:59:26,520 --> 00:59:31,520
so your mission control won't go out the window

00:59:32,340 --> 00:59:37,340
for that period because that's your main concern here.

00:59:37,860 --> 00:59:40,550
- Also the next time it might move

00:59:40,550 --> 00:59:42,570
some bandwidth to the other CPUs.

00:59:42,570 --> 00:59:43,910
So statistically it will--

00:59:43,910 --> 00:59:46,260
- So that's basically the same point

00:59:46,260 --> 00:59:49,350
that I actually had after the discussion

00:59:49,350 --> 00:59:51,390
we had in Pisa because as I said,

00:59:51,390 --> 00:59:53,753
if the task, if both the lock owner

00:59:53,753 --> 00:59:56,200
and the tasker blocks on the mutex,

00:59:56,200 --> 00:59:59,080
we're already automated in a real domain.

00:59:59,080 --> 01:00:01,320
That means that their respective bandwidth

01:00:01,320 --> 01:00:02,970
was already admitted to the system.

01:00:02,970 --> 01:00:05,100
So if they're scheduling different CPUs

01:00:05,100 --> 01:00:07,070
all on the same, it doesn't matter

01:00:07,070 --> 01:00:09,530
because it doesn't actually check currently

01:00:09,530 --> 01:00:11,170
where they're actually scheduled.

01:00:11,170 --> 01:00:13,410
The problem raises if they're actually

01:00:13,410 --> 01:00:15,470
coming from different root domains.

01:00:15,470 --> 01:00:19,800
So if the feed is let's say don't interrupt

01:00:19,800 --> 01:00:21,870
but then again if they don't have a lock

01:00:21,870 --> 01:00:24,210
and then they actually sharing resources,

01:00:24,210 --> 01:00:26,860
that's maybe a design issue.

01:00:26,860 --> 01:00:29,230
So that's basically, I wanted to discuss this

01:00:29,230 --> 01:00:31,600
because if we say that's not a problem

01:00:31,600 --> 01:00:34,492
and we can go ahead with what's implemented right now,

01:00:34,492 --> 01:00:36,870
it's simpler because it's already there,

01:00:36,870 --> 01:00:38,830
we just make it work.

01:00:38,830 --> 01:00:39,880
- Well we'll have to.

01:00:41,513 --> 01:00:43,680
The kernel is always a global resource

01:00:43,680 --> 01:00:47,420
so the whole multiple root domain

01:00:47,420 --> 01:00:51,540
is always an idealized situation anyway.

01:00:52,470 --> 01:00:56,685
We'll just have to live with a wee bit of violation there.

01:00:56,685 --> 01:01:00,250
I mean, don't trigger proxy will get you inversions.

01:01:01,420 --> 01:01:02,943
That's worse.

01:01:02,943 --> 01:01:05,288
- Lower problem, not a problem.

01:01:05,288 --> 01:01:06,840
- Just really quick.

01:01:06,840 --> 01:01:11,120
So it seemed to migrate over during the scheduling time.

01:01:11,120 --> 01:01:13,320
What's the added overhead in that migration?

01:01:14,480 --> 01:01:16,430
If it matters if the lock's not held that long,

01:01:16,430 --> 01:01:18,840
wouldn't just moving the task back and forth--

01:01:18,840 --> 01:01:20,840
- [Peter] It's not about the overhead,

01:01:20,840 --> 01:01:24,587
it's about the injected bandwidth that

01:01:26,590 --> 01:01:27,710
hasn't been accounted for.

01:01:27,710 --> 01:01:31,000
- But overhead will add injection bandwidth to it

01:01:31,000 --> 01:01:33,620
so the question is can that overhead cause,

01:01:33,620 --> 01:01:35,390
have you done tests to see--

01:01:35,390 --> 01:01:36,630
- Not yet.

01:01:36,630 --> 01:01:38,790
I guess that's probably the next step.

01:01:38,790 --> 01:01:41,010
I mean once this thing is actually stable

01:01:41,010 --> 01:01:42,966
then we'd like to test this up.

01:01:42,966 --> 01:01:45,302
On the other hand, if we don't migrate

01:01:45,302 --> 01:01:48,010
I guess it creates probably more trouble

01:01:48,010 --> 01:01:50,960
because what we do with the tasks that is--

01:01:50,960 --> 01:01:52,850
- [Man] Yeah, so we have to migrate because

01:01:52,850 --> 01:01:54,610
if your dependency changed from CPU,

01:01:54,610 --> 01:01:56,670
you could end up in a situation where

01:01:56,670 --> 01:01:59,403
you want to run the same tasks multiple times

01:01:59,403 --> 01:02:00,740
and this is bad.

01:02:02,343 --> 01:02:05,290
(man speaking off mic)

01:02:05,290 --> 01:02:09,585
- Is there any way to tag the locks?

01:02:09,585 --> 01:02:11,890
And I also see it done with specific domains.

01:02:11,890 --> 01:02:14,840
So you would do proxy execution only within one domain

01:02:14,840 --> 01:02:17,860
and that would be restrained to this.

01:02:19,317 --> 01:02:20,707
- [Man] - No, you want to always do it.

01:02:20,707 --> 01:02:23,560
- Anyway, we're running out of time so

01:02:23,560 --> 01:02:26,880
since I was expecting not to finish this discussion,

01:02:26,880 --> 01:02:29,500
it gets 10 minutes because it's quite a tricky one.

01:02:29,500 --> 01:02:32,064
I guess we can continue (crosstalking).

01:02:32,064 --> 01:02:33,897
Yeah, okay, thank you.

01:02:38,060 --> 01:02:41,227
(audience applauding)

01:02:46,086 --> 01:02:47,086
- Thank you.

01:02:50,030 --> 01:02:51,670
So I'm taking a little bit different aspect

01:02:51,670 --> 01:02:54,190
of making scheduling deadlines safe.

01:02:54,190 --> 01:02:55,023
Rather than--

01:02:55,023 --> 01:02:56,170
- [Man] You have to hold the mic close to your mouth.

01:02:56,170 --> 01:02:57,238
- Like that, is that better?

01:02:57,238 --> 01:02:59,340
How about that?

01:03:00,930 --> 01:03:01,763
Okay.

01:03:02,810 --> 01:03:05,610
So making deadlines safe but instead of making it safe

01:03:05,610 --> 01:03:08,100
for admission policies or inversions or whatever,

01:03:08,100 --> 01:03:09,050
for kernel threads.

01:03:13,212 --> 01:03:14,045
I've got one of these around here somewhere.

01:03:14,045 --> 01:03:15,193
How 'bout that one?

01:03:15,193 --> 01:03:16,390
Is that right, yeah.

01:03:16,390 --> 01:03:18,780
So how many people saw the thread where Dmitry

01:03:18,780 --> 01:03:21,010
was doing some fuzz testing on kernel's test

01:03:21,010 --> 01:03:24,915
and just totally got RCU with a couple of people?

01:03:24,915 --> 01:03:26,423
(man talking off mic) - I'm sorry?

01:03:26,423 --> 01:03:28,756
(man talking off mic)

01:03:28,756 --> 01:03:31,140
oh, okay, yeah, so we got RC CPU stall warnings.

01:03:31,140 --> 01:03:32,590
If you want to look, the slides will be up

01:03:32,590 --> 01:03:34,626
at some point and there's the URL for it.

01:03:34,626 --> 01:03:37,810
And Ted Ts'a looked at it and says

01:03:37,810 --> 01:03:41,410
you know when you set your scheduling cycle,

01:03:41,410 --> 01:03:43,160
if I use the wrong word, I'm sorry,

01:03:44,240 --> 01:03:47,310
to period, excuse me, to 26 days,

01:03:48,490 --> 01:03:50,509
you have to expect some breakage.

01:03:50,509 --> 01:03:51,990
- [Man] 146 years.

01:03:51,990 --> 01:03:52,823
- 146 years.

01:03:52,823 --> 01:03:54,600
Okay, 26 days of CPU time, I'm sorry.

01:03:54,600 --> 01:03:56,920
I'm not reading my own slide even.

01:03:56,920 --> 01:04:00,130
Anyway, so an automatic thing, if somebody gets

01:04:00,130 --> 01:04:02,470
an RCCB stall warning and it's not something

01:04:02,470 --> 01:04:04,620
totally broken is hey, put some con rescheds

01:04:04,620 --> 01:04:05,950
in somewhere and that'll help and

01:04:05,950 --> 01:04:08,800
then that totally didn't do anything in this case at all.

01:04:10,730 --> 01:04:13,330
But the question is what should we be doing instead?

01:04:16,707 --> 01:04:19,260
In this case, 146 years, you could end up

01:04:19,260 --> 01:04:22,880
waiting 146 years but if you have a CPU-bound task,

01:04:22,880 --> 01:04:26,310
you're gonna wait 26 days and that's a long time

01:04:26,310 --> 01:04:27,440
and not just for RCU.

01:04:29,020 --> 01:04:30,210
Yeah, go ahead, Peter.

01:04:31,852 --> 01:04:33,240
- I actually proposed a patch that went out

01:04:33,240 --> 01:04:35,120
with third edition because simple things

01:04:35,120 --> 01:04:37,510
can be difficult too.

01:04:37,510 --> 01:04:39,590
That sets limits on period.

01:04:39,590 --> 01:04:42,590
It basically mandates a minimum and a maximum period--

01:04:42,590 --> 01:04:44,440
- [Presenter] What were the numbers, if you can tell me?

01:04:46,470 --> 01:04:50,070
- I think it's like one mil versus four seconds.

01:04:50,070 --> 01:04:51,570
- One mil versus four seconds.

01:04:53,150 --> 01:04:55,920
One mil, I'm not too worried about it being too short.

01:04:55,920 --> 01:04:58,370
That's not my problem though it's probably yours.

01:05:00,890 --> 01:05:03,550
Four seconds, I'm a little bit nervous about that.

01:05:03,550 --> 01:05:05,664
I'd feel better with one but--

01:05:05,664 --> 01:05:09,617
- [Man] I helped to pick the number I felt like.

01:05:09,617 --> 01:05:12,660
(everyone laughing)

01:05:12,660 --> 01:05:13,860
- I'm sure that no matter what number you pick,

01:05:13,860 --> 01:05:15,514
somebody's gonna be torked off.

01:05:15,514 --> 01:05:18,764
(man speaking off mic)

01:05:22,349 --> 01:05:23,182
- [Man] Mike.

01:05:23,182 --> 01:05:26,660
- Yeah, essentially Yuri, not Mike (laughing).

01:05:27,720 --> 01:05:29,900
- No, they were saying that post data is

01:05:29,900 --> 01:05:32,210
basically run like the other thing.

01:05:32,210 --> 01:05:34,300
Basically every fur second there will be

01:05:34,300 --> 01:05:36,190
5% available to--

01:05:37,821 --> 01:05:40,650
- [Presenter] Okay, yeah.

01:05:40,650 --> 01:05:44,900
- So there's a basis of reality in the number four.

01:05:45,810 --> 01:05:47,110
- There's a basis of reality in the number four,

01:05:47,110 --> 01:05:48,420
Clark said. - Yeah.

01:05:48,420 --> 01:05:50,590
- It wasn't just picked out of the sky.

01:05:50,590 --> 01:05:52,920
- It was actually two to the power of 32

01:05:52,920 --> 01:05:54,720
because that's a really nice number.

01:05:58,843 --> 01:06:01,070
- (laughing) Okay, and what was the units on that?

01:06:01,070 --> 01:06:01,903
- Nanos.

01:06:01,903 --> 01:06:03,750
- Oh, okay, that might be reasonable.

01:06:05,320 --> 01:06:08,320
Okay, so what does this imply?

01:06:08,320 --> 01:06:11,240
So you've got this patch, you have a limit of

01:06:11,240 --> 01:06:13,430
one millisecond low end, four seconds on the high end

01:06:13,430 --> 01:06:15,740
for the period. - Yeah.

01:06:18,150 --> 01:06:20,749
- So no matter what you did you'd have that period there.

01:06:20,749 --> 01:06:25,749
And that means that your quantity of time

01:06:27,410 --> 01:06:29,930
would have to be four seconds or less, presumably,

01:06:29,930 --> 01:06:31,030
or it will reject you.

01:06:32,678 --> 01:06:35,340
- So that specific configuration will be rejected.

01:06:35,340 --> 01:06:36,250
- Okay, yes.

01:06:36,250 --> 01:06:39,983
- 146 years is a wee bit over four seconds.

01:06:39,983 --> 01:06:40,816
(audience laughing)

01:06:40,816 --> 01:06:41,649
- Last I checked.

01:06:41,649 --> 01:06:44,330
Of course it depends on what object you're using

01:06:44,330 --> 01:06:45,510
to compute years.

01:06:45,510 --> 01:06:48,690
Yeah, there aren't too many that have

01:06:48,690 --> 01:06:50,770
millisecond revolution rates but there are

01:06:50,770 --> 01:06:53,070
probably some astral outliers there somewhere.

01:06:54,480 --> 01:06:57,100
Anyway, so in other words,

01:06:57,100 --> 01:06:58,090
if you just set it at four seconds,

01:06:58,090 --> 01:07:00,860
the biggest time you can specify is four seconds.

01:07:02,730 --> 01:07:04,760
- [Man] Yeah, the biggest period would be four seconds.

01:07:04,760 --> 01:07:06,290
- Not just period but the number,

01:07:06,290 --> 01:07:08,010
the biggest amount of time.

01:07:08,010 --> 01:07:12,350
- Yeah, we have like a 95% rule in there somewhere.

01:07:12,350 --> 01:07:15,360
So you can not get more than 95% of four seconds.

01:07:15,360 --> 01:07:17,400
- [Presenter] Okay, so no more than 3.8 seconds.

01:07:17,400 --> 01:07:18,445
- Something like that, yeah.

01:07:18,445 --> 01:07:20,860
- [Presenter] Okay.

01:07:22,550 --> 01:07:26,910
So the additional 22 seconds per four seconds,

01:07:26,910 --> 01:07:28,560
that's available to anybody or to

01:07:29,400 --> 01:07:30,460
lower scheduling classes?

01:07:30,460 --> 01:07:32,480
Is sched_FIFO different than sched_other in this thing

01:07:32,480 --> 01:07:34,077
or how does that work?

01:07:34,077 --> 01:07:35,890
(man speaking off mic)

01:07:35,890 --> 01:07:39,463
Give him the mic.

01:07:39,463 --> 01:07:40,794
Yeah, the other idea. - Yeah, the other thing.

01:07:40,794 --> 01:07:44,710
I meant about the other thing.

01:07:44,710 --> 01:07:46,650
Yeah, that's available for other guys

01:07:46,650 --> 01:07:50,370
but I think that's basically for this particular issue.

01:07:50,370 --> 01:07:53,888
But what Peter thought to propose

01:07:53,888 --> 01:07:58,040
is to actually use deadline to schedule

01:07:58,040 --> 01:08:02,200
lower priority classes for let's say 5% of the time.

01:08:02,200 --> 01:08:05,810
- Okay, let me interrupt you and ask a stupid question.

01:08:05,810 --> 01:08:07,790
So I was uncomfortable four seconds.

01:08:07,790 --> 01:08:11,570
Does this mean I can say the RCU grace period k thread

01:08:11,570 --> 01:08:13,770
and its kernel threads meet a deadline

01:08:13,770 --> 01:08:15,370
of one second instead of four seconds?

01:08:15,370 --> 01:08:18,239
Would that work?

01:08:18,239 --> 01:08:19,323
- [Man] I mean I could change the number.

01:08:19,323 --> 01:08:21,454
- Yeah, okay.

01:08:21,454 --> 01:08:25,480
That wasn't what I was asking, Peter.

01:08:25,480 --> 01:08:27,490
What I'm asking you is do different,

01:08:28,460 --> 01:08:29,340
Paul's looking a little nervous

01:08:29,340 --> 01:08:31,190
so maybe the answer is no (laughing).

01:08:32,210 --> 01:08:35,027
Give him the mic, give Paul the mic.

01:08:35,027 --> 01:08:36,700
I could ask the question, I suppose,

01:08:36,700 --> 01:08:37,533
but that would be too easy.

01:08:37,533 --> 01:08:38,366
- Let me ask you a question.

01:08:38,366 --> 01:08:41,853
I don't think the RCU threads is in the

01:08:41,853 --> 01:08:43,730
deadline schedule class, is it?

01:08:43,730 --> 01:08:45,200
- No. - It's not now.

01:08:45,200 --> 01:08:46,460
But it sounds like they're having

01:08:46,460 --> 01:08:48,810
some kind of thing that guarantees

01:08:48,810 --> 01:08:50,870
the aggregate rest of the system

01:08:50,870 --> 01:08:52,770
some amount of time every four seconds.

01:08:52,770 --> 01:08:55,250
- [Man] Yeah but that could be given to another thread.

01:08:55,250 --> 01:08:57,938
- It could. - And then you get within the

01:08:57,938 --> 01:08:59,518
four seconds after that.

01:08:59,518 --> 01:09:01,290
So that period ends up actually

01:09:01,290 --> 01:09:04,760
being totally arbitrary, even if you make the period small.

01:09:04,760 --> 01:09:05,593
- [Presenter] Right, right, I got that.

01:09:05,593 --> 01:09:08,870
One reaction I could take would be to,

01:09:10,210 --> 01:09:12,030
under some circumstances or always,

01:09:12,030 --> 01:09:13,950
take the RCU grace period k threads

01:09:13,950 --> 01:09:17,210
and sched_deadline them.

01:09:18,350 --> 01:09:19,528
Go ahead.

01:09:19,528 --> 01:09:22,510
- But let's say that all these are similar

01:09:22,510 --> 01:09:25,380
to the current RT throttling theme.

01:09:25,380 --> 01:09:30,380
So there is a ST mechanism which by default

01:09:30,770 --> 01:09:32,760
you have 5% of the time they would--

01:09:32,760 --> 01:09:37,240
- Nah, you could still say the same for the CPU.

01:09:37,240 --> 01:09:38,073
- You can do what to the CPU?

01:09:38,073 --> 01:09:42,026
- What he said is that as we have a global scheduler,

01:09:42,026 --> 01:09:47,026
we tried to ensure that we use online 5% of CPU.

01:09:47,340 --> 01:09:51,600
But as we have two CPUs, we can run 100% in one

01:09:51,600 --> 01:09:52,830
and the rest in the other.

01:09:52,830 --> 01:09:54,250
It can be even they need either or--

01:09:54,250 --> 01:09:55,940
- And SU, Chris, can depend--

01:09:55,940 --> 01:09:57,770
- [Presenter] So it can be kind of tough for k software,

01:09:57,770 --> 01:10:00,580
if you were running RC software, for example,

01:10:00,580 --> 01:10:04,820
or if real-time the RCU per CPU kernel threads.

01:10:04,820 --> 01:10:07,503
- For real-time we killed that.

01:10:07,503 --> 01:10:09,991
I thought we did for mainline as well but I forgot.

01:10:09,991 --> 01:10:11,610
- [Presenter] Killed what exactly?

01:10:11,610 --> 01:10:13,150
- Just showing off the runtime.

01:10:13,150 --> 01:10:18,150
So make it 95% for CPU, no show.

01:10:18,360 --> 01:10:20,160
- [Presenter] Okay, that would be easier

01:10:20,160 --> 01:10:21,687
from a kernel thread viewpoint.

01:10:21,687 --> 01:10:22,820
(man speaking off mic)

01:10:22,820 --> 01:10:25,080
Oh, give him the mic or throw another mic up here

01:10:25,080 --> 01:10:26,440
so you're not passing them back and forth,

01:10:26,440 --> 01:10:27,273
unless someone else wants one.

01:10:27,273 --> 01:10:31,000
- So basically the idea, what you implemented and

01:10:31,000 --> 01:10:33,730
correct me if I'm wrong, basically what you have

01:10:33,730 --> 01:10:36,650
is then you would have a one deadline scheduler

01:10:36,650 --> 01:10:39,927
per CPU and that ping will have basically 5%

01:10:39,927 --> 01:10:43,907
of CPU time over say four seconds, right?

01:10:43,907 --> 01:10:44,930
- The server?

01:10:44,930 --> 01:10:46,602
- For the server team.

01:10:46,602 --> 01:10:51,602
- That's another thing I also prototyped

01:10:51,670 --> 01:10:56,670
a deadline server and this will run CFS as a deadline task.

01:10:57,710 --> 01:11:02,113
So every 5% by default of the maximum period

01:11:06,610 --> 01:11:10,390
will force run CFS tasks irrespective of

01:11:10,390 --> 01:11:14,350
any available real-time or deadline task.

01:11:14,350 --> 01:11:16,530
I mean it will be a deadline task but--

01:11:16,530 --> 01:11:18,393
- I do have a question for this point.

01:11:18,393 --> 01:11:22,520
In your implementation, are you moving all the tasks

01:11:22,520 --> 01:11:25,130
to the server or not?

01:11:25,130 --> 01:11:26,480
- [Presenter] Excuse me, you guys have to share a mic.

01:11:26,480 --> 01:11:29,930
Can somebody else hold it in case somebody else wants one?

01:11:29,930 --> 01:11:32,625
There we are, sorry.

01:11:32,625 --> 01:11:35,510
I offered to throw this one but they said that wasn't safe.

01:11:35,510 --> 01:11:38,930
- Peter, Peter that serer can not be configured?

01:11:40,770 --> 01:11:44,490
- Currently, no, this is why they have RFC on.

01:11:44,490 --> 01:11:48,080
It was a really crude hack but it should be configurable.

01:11:48,080 --> 01:11:50,350
- Yeah, so we could actually say,

01:11:50,350 --> 01:11:54,240
when the system starts we give that 10% every second

01:11:54,240 --> 01:11:55,390
or something like that.

01:11:56,570 --> 01:12:00,600
And then everything else which comes in that deadline

01:12:01,950 --> 01:12:04,800
can interleaf with that. - Correct.

01:12:06,390 --> 01:12:08,080
- You just have to make sure that

01:12:08,080 --> 01:12:10,100
it finally meets the deadline, right?

01:12:11,390 --> 01:12:14,010
- But the question is task selection or just

01:12:14,010 --> 01:12:15,610
takes from the regular CS queue?

01:12:19,188 --> 01:12:20,021
- [Man] - The problem is you can still indefinitely,

01:12:20,021 --> 01:12:23,540
even with a balanced server, you can still

01:12:23,540 --> 01:12:25,697
indefinitely postpone the RCU as well.

01:12:25,697 --> 01:12:28,530
- You need like a third CPU unless you--

01:12:28,530 --> 01:12:31,370
- You have a third CPU, so you effectively--

01:12:31,370 --> 01:12:33,980
- Well then you need the SU thread into deadline.

01:12:33,980 --> 01:12:38,000
- That's something later for the room.

01:12:38,000 --> 01:12:39,040
- No, no, but I'm saying, you can have

01:12:39,040 --> 01:12:40,520
other CFS tasks in front of it.

01:12:40,520 --> 01:12:41,370
= well that's the

01:12:43,410 --> 01:12:45,030
- Well that's a problem for Paul.

01:12:49,854 --> 01:12:52,190
- But there is something to think,

01:12:52,190 --> 01:12:56,660
if you move all of your tasks to a deadline server

01:12:56,660 --> 01:12:58,900
that runs into a deadline run queue--

01:12:58,900 --> 01:13:00,820
- [Presenter] You don't get the 5%.

01:13:00,820 --> 01:13:05,060
- It's not even the 5%, it's that you translate your,

01:13:05,060 --> 01:13:06,710
today it's implicit here.

01:13:06,710 --> 01:13:08,850
We don't have these in as scheduled

01:13:08,850 --> 01:13:13,170
but our schedulers are scheduled in a fix priority

01:13:14,070 --> 01:13:17,280
as if it was we first run sched_deadline,

01:13:17,280 --> 01:13:19,030
then sched_FIFO, then sched_order.

01:13:20,710 --> 01:13:24,040
If you move sched_order task inside a container,

01:13:24,040 --> 01:13:28,300
you would put the priority in the FIFO scheduler.

01:13:29,980 --> 01:13:31,820
- Sure but we can add another CPU.

01:13:31,820 --> 01:13:34,770
- Yeah, but if you add another, we will need

01:13:34,770 --> 01:13:39,770
the property of knowing that the FIFO thread,

01:13:43,287 --> 01:13:45,540
the highest priority thread, if it's alone with other tasks,

01:13:45,540 --> 01:13:48,440
if it's alone it will not necessarily be

01:13:48,440 --> 01:13:50,090
the highest priority thread.

01:13:50,090 --> 01:13:53,620
Because at that time, the time of the raise

01:13:53,620 --> 01:13:58,620
the scheduler server that is running these other tasks

01:13:59,480 --> 01:14:03,170
might have a higher priority because of a shorter deadline.

01:14:03,170 --> 01:14:05,710
So we will not have the property of having

01:14:05,710 --> 01:14:09,870
the sched_FIFO over the sched_other always

01:14:09,870 --> 01:14:13,030
and that's the vast majority of your users right now.

01:14:15,001 --> 01:14:17,300
- That goes with what Thomas asked,

01:14:17,300 --> 01:14:19,140
it's just inconfigurable.

01:14:19,140 --> 01:14:22,210
Because we know it's just a few hardcoded lines of C

01:14:22,210 --> 01:14:25,080
but yeah we should acknowledge them.

01:14:25,080 --> 01:14:29,260
- And you know bring a question from Luka Bainey.

01:14:29,260 --> 01:14:31,313
He was looking at these configuration and

01:14:37,360 --> 01:14:42,360
his original idea included a C group interface for it.

01:14:43,664 --> 01:14:45,010
(man speaking off mic)

01:14:45,010 --> 01:14:48,240
Yeah, but to be that thing disabled by the forward

01:14:48,240 --> 01:14:50,210
you need to have a way to configure.

01:14:50,210 --> 01:14:52,940
How would you configure this in runtime?

01:14:52,940 --> 01:14:53,773
That will be the--

01:14:53,773 --> 01:14:56,960
- What I would do, this is kind of the thing...

01:14:56,960 --> 01:14:58,908
Oh, yes, go ahead, Patrick.

01:14:58,908 --> 01:15:00,380
- This is just out of curiosity.

01:15:00,380 --> 01:15:05,160
If you take let's say 5% of your sched_deadline deadline

01:15:06,660 --> 01:15:08,150
and give it to the kernel,

01:15:08,150 --> 01:15:10,400
what happens to kernel threads, which are going to

01:15:10,400 --> 01:15:14,470
specific CPUs when, as I understand,

01:15:14,470 --> 01:15:16,880
the concept of deadline is systemwide.

01:15:16,880 --> 01:15:19,930
So you might be I guess having zero time

01:15:19,930 --> 01:15:23,430
on specific CPUs and loads of time on other CPUs.

01:15:25,190 --> 01:15:30,190
- So the deadline server is strictly for CPU.

01:15:30,190 --> 01:15:31,023
- [Patrick] Okay.

01:15:32,380 --> 01:15:33,448
- Okay, so--

01:15:33,448 --> 01:15:35,863
(man speaking off mic)

01:15:35,863 --> 01:15:38,760
- Jim, I might have something from select scheduling?

01:15:41,770 --> 01:15:44,910
- Yeah. - Well this is close to that.

01:15:44,910 --> 01:15:47,773
- But we don't have such a Slack,

01:15:47,773 --> 01:15:51,330
we are not scheduling these by the deadline

01:15:51,330 --> 01:15:53,197
but by the slack time.

01:15:53,197 --> 01:15:58,110
This is another scheduler like it is.

01:15:58,110 --> 01:16:00,460
They are all deadline schedulers

01:16:00,460 --> 01:16:02,060
but they are not EDF schedulers.

01:16:04,132 --> 01:16:04,965
These are, they're then schedulers

01:16:04,965 --> 01:16:05,798
but they are not EDM schedulers.

01:16:05,798 --> 01:16:08,728
- This is closest to select panel things.

01:16:08,728 --> 01:16:11,120
- So what it really comes down to is

01:16:12,036 --> 01:16:14,510
my overall question is can I get away with

01:16:14,510 --> 01:16:16,210
just leaving myself the sched_other

01:16:16,210 --> 01:16:18,610
and allowing people to do FIFO priority

01:16:18,610 --> 01:16:20,680
or should I take a more dynamic approach

01:16:21,570 --> 01:16:25,445
where I accept the fact that I'm not getting enough CPU

01:16:25,445 --> 01:16:29,331
and move to FIFO or move to deadline or something?

01:16:29,331 --> 01:16:32,829
- My take on this is that to serve this particular program,

01:16:32,829 --> 01:16:36,830
the first solution is imposing limits

01:16:36,830 --> 01:16:38,200
on the actual parameters

01:16:38,200 --> 01:16:41,490
it's basically working just because it doesn't allow

01:16:41,490 --> 01:16:43,260
something like this to enter the program.

01:16:43,260 --> 01:16:46,430
It doesn't solve all the problems of course

01:16:46,430 --> 01:16:50,340
and then having repeat a second solution

01:16:50,340 --> 01:16:55,340
we might actually have handling other sort of problems.

01:16:56,400 --> 01:16:58,770
And then if you really want your code bytes,

01:16:58,770 --> 01:17:02,810
for example, to be as scheduled, if you have also

01:17:02,810 --> 01:17:04,500
other deadlines going in that same task

01:17:04,500 --> 01:17:07,980
then maybe one thing that can be thought

01:17:07,980 --> 01:17:10,637
is to use deadline pool for those threads.

01:17:10,637 --> 01:17:14,350
- [Presenter] In the real-time case, the priority

01:17:14,350 --> 01:17:17,740
of the callback indication is the operator's responsibility

01:17:17,740 --> 01:17:18,573
not mine.

01:17:19,970 --> 01:17:23,410
- Unless you do elevation and RCU.

01:17:23,410 --> 01:17:25,560
All of these changes push out the problem point

01:17:25,560 --> 01:17:27,160
further and further and further and further

01:17:27,160 --> 01:17:28,440
bu they don't eliminate it.

01:17:28,440 --> 01:17:30,410
- [Presenter] Right, in other words you have to use...

01:17:30,410 --> 01:17:32,090
And isn't that default now in RT?

01:17:32,090 --> 01:17:34,960
Doesn't RT default to RC priority listening being on?

01:17:36,460 --> 01:17:37,960
Or am I confused?

01:17:37,960 --> 01:17:38,860
- I think it does.

01:17:39,890 --> 01:17:41,570
- Yeah, there is a config option.

01:17:41,570 --> 01:17:42,740
- [Presenter] Yeah, there is a config option but

01:17:42,740 --> 01:17:44,650
I thought that RT recently switched it

01:17:44,650 --> 01:17:46,220
so by default in RT it's on.

01:17:46,220 --> 01:17:48,253
- It might be time to just flip that.

01:17:48,253 --> 01:17:49,365
- There's a question.

01:17:49,365 --> 01:17:51,880
- I just want to bring up a point that

01:17:51,880 --> 01:17:56,700
we had a similar problem with the CPU frag k plugs

01:17:56,700 --> 01:17:58,470
I think a few years ago.

01:17:58,470 --> 01:18:00,870
So I feel like this is a similar problem,

01:18:00,870 --> 01:18:01,870
if I'm not mistaken.

01:18:02,790 --> 01:18:04,936
I'm just wondering if there's some

01:18:04,936 --> 01:18:07,780
more general mechanism that is needed to solve this

01:18:07,780 --> 01:18:10,970
so we're not talking about some other important

01:18:10,970 --> 01:18:12,070
thread in a few years?

01:18:16,680 --> 01:18:18,470
- For those threads actually,

01:18:18,470 --> 01:18:21,047
the hack is actually worse than this.

01:18:21,047 --> 01:18:26,047
Their highest variety, we basically don't account for them.

01:18:26,640 --> 01:18:28,740
When we have to run it's run.

01:18:28,740 --> 01:18:31,350
It's like kind of the stop effort,

01:18:31,350 --> 01:18:35,358
it's something that's highest priority than everything.

01:18:35,358 --> 01:18:38,690
(man speaking off mic)

01:18:38,690 --> 01:18:42,572
- Peter, we did have some conversation at a high level

01:18:42,572 --> 01:18:47,250
about ideas of mixing EDF and FAIR for the

01:18:47,250 --> 01:18:50,520
unscheduled EDF time, which does actually have

01:18:50,520 --> 01:18:53,250
nice properties for problems like this

01:18:53,250 --> 01:18:58,030
but it is not a quick fix.

01:19:00,340 --> 01:19:04,160
- Maybe for the future we need to think about

01:19:05,070 --> 01:19:09,050
another way to scheduling still using deadline

01:19:09,050 --> 01:19:12,040
but taking care of first CPU things and

01:19:12,040 --> 01:19:13,320
then trying to move like--

01:19:13,320 --> 01:19:14,776
- We're working on putting together some stuff.

01:19:14,776 --> 01:19:17,560
- Yeah, I was working for same partition scheduler

01:19:17,560 --> 01:19:22,560
that's a possibility but the point now is that

01:19:24,188 --> 01:19:27,680
the current implementation for same partition

01:19:27,680 --> 01:19:30,967
doesn't support the local

01:19:30,967 --> 01:19:35,967
(man speaking in foreign language) CBS.

01:19:39,240 --> 01:19:41,690
Because of the same problems of blocking

01:19:41,690 --> 01:19:43,740
in the middle of the execution.

01:19:43,740 --> 01:19:48,130
But I think that a better solution would involve

01:19:48,130 --> 01:19:52,070
things like same partition or PCP control of the bandwidth.

01:19:54,090 --> 01:19:55,740
But see this is a lot of work.

01:19:55,740 --> 01:19:58,348
- I'd like to channel Linus for a moment here.

01:19:58,348 --> 01:20:01,230
His next question I believe at this point would be

01:20:01,230 --> 01:20:05,560
okay, if we do all this, can config RCU k thread

01:20:05,560 --> 01:20:06,393
file go away?

01:20:09,098 --> 01:20:11,560
- [Man] It's a lot of problem.

01:20:11,560 --> 01:20:16,140
- But it might.

01:20:16,140 --> 01:20:17,460
I think it's a yeah.

01:20:17,460 --> 01:20:21,400
Like fundamental solution is if you have

01:20:21,400 --> 01:20:22,422
some part or some sort of the slides

01:20:22,422 --> 01:20:25,610
and there areAs some slides the RT is NORDS.

01:20:26,980 --> 01:20:28,260
So as a very long-term solution

01:20:28,260 --> 01:20:31,930
is actually everybody move to RT.

01:20:32,900 --> 01:20:37,040
We have scalability problems but that may be solvable.

01:20:37,040 --> 01:20:42,040
- RT's got a lot better on the overheard throughput case

01:20:42,360 --> 01:20:43,543
than it used to be.

01:20:43,543 --> 01:20:47,110
- RT including this, so maybe everybody

01:20:47,110 --> 01:20:47,943
we should maybe fuse CFS with

01:20:47,943 --> 01:20:52,630
Skype RI conceptually so we can

01:20:52,630 --> 01:20:56,570
run everything with the QS parameters.

01:21:00,890 --> 01:21:05,890
- The QS for, I'll take pity on my poor admins

01:21:06,720 --> 01:21:11,270
that have to configure this, the QS for the RCU

01:21:11,270 --> 01:21:14,860
callback offload threads, is a very interesting

01:21:14,860 --> 01:21:16,820
and strange thing to compute

01:21:16,820 --> 01:21:20,090
because it depends on the workload.

01:21:20,090 --> 01:21:22,515
- [Man] Nah, nah, nah, nah, it's simpler than that.

01:21:22,515 --> 01:21:24,710
- Or the CPU if it needs it.

01:21:26,230 --> 01:21:30,470
- [Man] Sure but this is a game that's saying

01:21:30,470 --> 01:21:32,615
how far do you want to push it out to the tab.

01:21:32,615 --> 01:21:35,960
So getting some QS parameter pushes it

01:21:35,960 --> 01:21:39,567
a very long way out to the tab versus null parameter.

01:21:41,789 --> 01:21:45,263
- [Thomas] You need that mission control for RCU.

01:21:45,263 --> 01:21:47,360
- Yeah, what we're gonna do, there are only

01:21:47,360 --> 01:21:49,330
a few thousand call SU instances.

01:21:49,330 --> 01:21:50,790
What we'll do is we're gonna add,

01:21:50,790 --> 01:21:53,668
make it be brilliant and it'll just tell you no,

01:21:53,668 --> 01:21:56,425
I'm sorry, you can't call our CPU right now (laughing).

01:21:56,425 --> 01:21:59,675
(man speaking off mic)

01:22:00,860 --> 01:22:01,693
- [Presenter] That makes it easier,

01:22:01,693 --> 01:22:02,526
you won't have to change the APR,

01:22:02,526 --> 01:22:03,710
you just drop it silently.

01:22:06,594 --> 01:22:07,427
- [Thomas] There you go.

01:22:07,427 --> 01:22:08,830
You'd get a memory leak but who cares.

01:22:08,830 --> 01:22:11,200
- [Man] The RCU loop's in progress.

01:22:11,200 --> 01:22:12,310
- Yeah, that's right.

01:22:13,500 --> 01:22:18,150
But anyway, that's I guess the two extreme positions.

01:22:18,150 --> 01:22:22,020
So going back when I made up the slide,

01:22:22,880 --> 01:22:24,770
not having paid attention to the discussion,

01:22:24,770 --> 01:22:28,541
which is my fault, it looks like we're doing

01:22:28,541 --> 01:22:31,580
the first line of defense is making

01:22:31,580 --> 01:22:33,910
sched_setattr your requests where silly

01:22:33,910 --> 01:22:37,130
is now defined as four seconds max

01:22:37,130 --> 01:22:38,510
and a no second minimum.

01:22:38,510 --> 01:22:39,510
We'll know it's about four but

01:22:39,510 --> 01:22:41,699
maybe I'm being overly nervous.

01:22:41,699 --> 01:22:42,890
(man speaking off mic)

01:22:42,890 --> 01:22:46,210
Well it is an occupational hazard of being RC maintainer

01:22:46,210 --> 01:22:47,400
to be overly concerned.

01:22:48,680 --> 01:22:51,630
Possibly the scheduler handler might have the same problem.

01:22:53,670 --> 01:22:55,913
To some extent, I have the ability to make it

01:22:55,913 --> 01:22:56,746
contain itself.

01:22:56,746 --> 01:22:59,910
I give that to the guy booting the kernel

01:22:59,910 --> 01:23:02,240
with the priority control.

01:23:02,240 --> 01:23:05,932
That obviously only helps up to FIFO,

01:23:05,932 --> 01:23:06,880
it doesn't help with deadline.

01:23:06,880 --> 01:23:09,820
I could make thing saying here's the deadline,

01:23:09,820 --> 01:23:11,870
tell me what I need to do and I'll do it.

01:23:13,863 --> 01:23:15,280
I'm a little nervous about adding that many knobs.

01:23:15,280 --> 01:23:18,300
- [Man] Do you think you'll to be able to codify

01:23:18,300 --> 01:23:20,940
your runtime and then code the deposit.

01:23:20,940 --> 01:23:22,570
- But then some poor guy is gonna want to have

01:23:22,570 --> 01:23:24,050
a different runtime and period for each

01:23:24,050 --> 01:23:27,080
of many tens of RCU gigas.

01:23:27,080 --> 01:23:28,200
- [Man] But you could derive it

01:23:28,200 --> 01:23:31,010
from the other values, right?

01:23:31,010 --> 01:23:32,080
- Maybe but I--

01:23:32,080 --> 01:23:34,300
- If you set assess columns to be four seconds

01:23:34,300 --> 01:23:38,570
and whatever it is, 5%, you could set your deadline

01:23:38,570 --> 01:23:40,702
to be 20 milliseconds every four seconds.

01:23:40,702 --> 01:23:44,480
- I could but again it depends on how far,

01:23:44,480 --> 01:23:45,640
as you said, it depends on how far you

01:23:45,640 --> 01:23:47,360
want to push it out to the edges.

01:23:47,360 --> 01:23:49,270
The thing is that if people aren't interesting in

01:23:49,270 --> 01:23:51,360
pushing the edges, they just take the default, which is--

01:23:51,360 --> 01:23:52,670
- I believe what I'm saying, as a default,

01:23:52,670 --> 01:23:55,560
you could do that and it would go hours to the tuning.

01:23:57,410 --> 01:24:01,040
- Okay, so let me make sure I understand what you're saying.

01:24:01,040 --> 01:24:02,420
What you're saying is instead of being

01:24:02,420 --> 01:24:04,030
sched_other by default, it could be sched_deadline

01:24:04,030 --> 01:24:07,500
by default and make it so that my deadlines

01:24:07,500 --> 01:24:10,310
are a function of the various system things

01:24:10,310 --> 01:24:14,350
that are set, either by default or by the user.

01:24:14,350 --> 01:24:16,260
But at that point, I've gotten myself into a position

01:24:16,260 --> 01:24:17,970
where I'm kicking my contact switch rate

01:24:17,970 --> 01:24:18,803
through the ceiling,

01:24:18,803 --> 01:24:20,050
which is a reason why people don't like

01:24:20,050 --> 01:24:22,560
using sched_FIFO through the RCU k threads.

01:24:23,770 --> 01:24:26,903
Okay, so I can have that be the,

01:24:26,903 --> 01:24:28,850
so that's a valuable thing, I can have it be

01:24:28,850 --> 01:24:32,690
the first line of defense as an intermediate thing

01:24:32,690 --> 01:24:34,930
between k FIFO and whatever the heck

01:24:34,930 --> 01:24:36,630
you would do for deadline, I'm not sure.

01:24:36,630 --> 01:24:38,550
But have it be another level of automation

01:24:38,550 --> 01:24:39,650
and saying I'm a little bit worried

01:24:39,650 --> 01:24:41,800
as opposed to go to this priority.

01:24:41,800 --> 01:24:43,430
Does that make sense?

01:24:43,430 --> 01:24:46,580
- [Man] Yeah, this is why the previous conversation about

01:24:47,710 --> 01:24:50,540
can you have a fair and deadline at the same time

01:24:50,540 --> 01:24:51,920
is interesting.

01:24:51,920 --> 01:24:53,390
- Yeah, okay.

01:24:53,390 --> 01:24:56,340
- Steve, do you remember a hack you did

01:24:56,340 --> 01:25:01,093
some time ago that when you receive a timer

01:25:01,093 --> 01:25:06,093
for dispatching the RSU stall, you raised the priority

01:25:06,870 --> 01:25:08,240
of the RSU thread for that CPU?

01:25:08,240 --> 01:25:09,290
Do you remember that?

01:25:11,048 --> 01:25:12,510
- [Steve] Sort of.

01:25:12,510 --> 01:25:14,190
- That might be a solution for your problem.

01:25:14,190 --> 01:25:15,880
- [Presenter] Can you say it, I didn't follow that entirely.

01:25:15,880 --> 01:25:16,890
Can you say it again?

01:25:16,890 --> 01:25:18,980
- If you're about to have--

01:25:18,980 --> 01:25:20,841
- [Thomas] No, that's not a solution.

01:25:20,841 --> 01:25:22,300
(everyone laughing)

01:25:22,300 --> 01:25:24,260
- [Presenter] Well, let him tell me the non-solution, c'mon.

01:25:24,260 --> 01:25:25,830
- It was a Steven's hack, it wasn't mine.

01:25:25,830 --> 01:25:27,450
It was Steve's idea.

01:25:27,450 --> 01:25:29,950
- [Presenter] Well stop arguing about whose idea it was.

01:25:29,950 --> 01:25:32,900
- [Thomas] But he started off at the wrong point because

01:25:32,900 --> 01:25:36,440
if you think that anything Stevens comes up with

01:25:36,440 --> 01:25:39,390
in a hurry is a solution then you're doing it wrong

01:25:39,390 --> 01:25:40,540
in the first place.

01:25:40,540 --> 01:25:41,990
- My intention was this joke.

01:25:45,195 --> 01:25:46,757
- [Thomas] And you take me again.

01:25:46,757 --> 01:25:47,590
- [Presenter] Just for my own edification,

01:25:47,590 --> 01:25:49,510
what was Steve's non-solution?

01:25:51,460 --> 01:25:53,440
- If I recall correctly, it was an internal elect

01:25:53,440 --> 01:25:55,550
that they have on kind of a thread head.

01:25:55,550 --> 01:25:59,230
One thing before a possible RSU stall,

01:26:00,928 --> 01:26:05,632
the interrupt raised the priority of the RCU

01:26:05,632 --> 01:26:08,700
thread that was about to explode and then it went.

01:26:09,680 --> 01:26:11,350
- [Daniel] Well, if it makes you feel any better, Thomas,

01:26:11,350 --> 01:26:12,760
that's what I would have done if somebody

01:26:12,760 --> 01:26:13,720
whacked me over the head and told me

01:26:13,720 --> 01:26:15,843
that I had to fix it like now.

01:26:15,843 --> 01:26:16,810
(everyone laughing)

01:26:16,810 --> 01:26:19,810
Or Steve, for that matter, if it makes you feel better.

01:26:19,810 --> 01:26:21,060
Giving you something, right.

01:26:22,272 --> 01:26:23,384
- [Steve] Thank you, Daniel.

01:26:23,384 --> 01:26:25,050
(everyone laughing) - No problem.

01:26:25,050 --> 01:26:26,360
It's a service we provide.

01:26:26,360 --> 01:26:27,900
Okay, if you've gotten down through these,

01:26:27,900 --> 01:26:29,960
if we have this, I don't know that Cisco

01:26:29,960 --> 01:26:32,352
needs to do anything because we would have

01:26:32,352 --> 01:26:35,930
the set scheduler yell at it if it did something stupid,

01:26:35,930 --> 01:26:38,880
whatever stupid means in this context.

01:26:38,880 --> 01:26:42,810
Then we've been talking about ways of doing

01:26:42,810 --> 01:26:45,250
the bandwidth throttling the groups in odd ways,

01:26:46,316 --> 01:26:48,210
kind of, not quite.

01:26:51,700 --> 01:26:53,780
I haven't heard anything new through the conversation

01:26:53,780 --> 01:26:56,340
but maybe I just haven't recognized it.

01:26:56,340 --> 01:26:58,240
So what have you guys discussed

01:26:58,240 --> 01:27:01,120
that doesn't fit into one of those boxes?

01:27:05,830 --> 01:27:07,840
- Just to add to that, actually we're using

01:27:07,840 --> 01:27:09,790
the fourth solution.

01:27:09,790 --> 01:27:11,810
So the new deadline server thing.

01:27:11,810 --> 01:27:14,420
I was actually thinking that if we add that

01:27:14,420 --> 01:27:18,510
and so CFS does actually run using basically deadline,

01:27:18,510 --> 01:27:21,110
then the RT throttling doesn't--

01:27:22,284 --> 01:27:23,200
- That can go.

01:27:23,200 --> 01:27:24,470
- They can go, right?

01:27:24,470 --> 01:27:27,126
But that's actually nice because basically it's implied.

01:27:27,126 --> 01:27:29,576
So yeah, unless that's a--

01:27:29,576 --> 01:27:31,200
(man speaking off mic)

01:27:31,200 --> 01:27:34,200
That's not a point to probably spend time on.

01:27:35,779 --> 01:27:37,436
- [Man] I think that that solution for that

01:27:37,436 --> 01:27:40,022
would require something like a nearest next

01:27:40,022 --> 01:27:43,413
the first scheduler.

01:27:43,413 --> 01:27:48,413
Or it isn't, so it's-- - What's the hack?

01:27:53,760 --> 01:27:57,570
- So at that point I proposed a combining EDF

01:27:57,570 --> 01:27:59,470
and least laxity first to do multi--

01:28:03,910 --> 01:28:05,160
- [Man] Multicriticality.

01:28:07,540 --> 01:28:08,710
- And the priority, that wasn't me,

01:28:08,710 --> 01:28:11,013
that was paid by somebody.

01:28:11,013 --> 01:28:12,207
- The decimal trick.

01:28:12,207 --> 01:28:14,610
- [Man] Yeah, that's a good way to look at it.

01:28:14,610 --> 01:28:18,090
- Yeah.

01:28:18,090 --> 01:28:21,720
- Okay, so we've got a couple of minutes left.

01:28:21,720 --> 01:28:24,421
We could do a few more questions or we could

01:28:24,421 --> 01:28:26,480
beat people to wherever it is

01:28:26,480 --> 01:28:28,250
that they go to break, because I can't remember.

01:28:28,250 --> 01:28:29,282
Coffee at least.

01:28:29,282 --> 01:28:30,730
(men speaking off mic)

01:28:30,730 --> 01:28:35,453
All right, going once, going twice, break!

01:28:36,683 --> 01:28:40,630
(audience applauding)

01:28:40,630 --> 01:28:44,420
- It's time to start the next one hour and a half.

01:28:44,420 --> 01:28:47,450
So the next topic is the rework the load balance

01:28:48,680 --> 01:28:51,800
in which I am working on currently.

01:28:53,750 --> 01:28:55,650
So if we go through a few slide.

01:28:56,550 --> 01:28:58,600
One is a status and the other one,

01:28:58,600 --> 01:29:01,000
I have three open item that I'd like to discuss.

01:29:01,940 --> 01:29:04,090
So just for those who are not really aware,

01:29:05,090 --> 01:29:06,400
there is a patch that came out

01:29:06,400 --> 01:29:08,739
about reworking the load balance

01:29:08,739 --> 01:29:12,870
and the mention of that now we have

01:29:12,870 --> 01:29:17,870
more a level of a type of classification of the group of CPU

01:29:18,450 --> 01:29:21,350
instead of just overloading, balance, or misfit.

01:29:23,004 --> 01:29:25,030
Mainly I did one, we have at capacity,

01:29:26,000 --> 01:29:30,220
fully busy, overloaded, and all the special case

01:29:30,220 --> 01:29:33,370
of imbalance, as unpacking, or misfit.

01:29:34,720 --> 01:29:35,553
That's one difference.

01:29:35,553 --> 01:29:39,070
The other main difference is the type of migration.

01:29:39,070 --> 01:29:44,070
So until now, we're only able to move some load

01:29:45,280 --> 01:29:48,060
and now you can select, depending of the two case

01:29:48,060 --> 01:29:50,542
and the situation, you can either continue to move some load

01:29:50,542 --> 01:29:53,790
to try to balance the load mainly when you overload it.

01:29:53,790 --> 01:29:55,999
But you can also decide to move a task

01:29:55,999 --> 01:29:59,290
or to move some utilization.

01:29:59,290 --> 01:30:02,990
So that help us to have a better

01:30:05,100 --> 01:30:09,710
way to balance when the system is not fully overloaded.

01:30:10,630 --> 01:30:13,650
Typically for the case where the system is overloaded,

01:30:13,650 --> 01:30:15,510
that would not change a lot of things.

01:30:15,510 --> 01:30:18,880
But it's mainly when we are midload or light loaded case.

01:30:22,650 --> 01:30:26,850
So yeah, several placement have been fixed

01:30:26,850 --> 01:30:30,500
like the one task per CPU, which is now working currently,

01:30:30,500 --> 01:30:31,590
as far as I can tell.

01:30:32,550 --> 01:30:34,740
Also we have a better spread of the tasks

01:30:34,740 --> 01:30:38,270
in the cgroup on numa and I don't know if Phil is there.

01:30:40,250 --> 01:30:42,230
So I worked with Phil about making sure

01:30:42,230 --> 01:30:45,280
that now we have a well-balanced system on numa

01:30:45,280 --> 01:30:48,440
when you have some cgroup with load and normal task.

01:30:49,730 --> 01:30:53,120
And also certified that if you are preempted by a t task,

01:30:53,120 --> 01:30:56,580
the CFS now will be moved to an idle CPU and finished,

01:30:56,580 --> 01:30:57,730
which was not the case.

01:31:01,090 --> 01:31:04,558
One of the change that I have done a source is

01:31:04,558 --> 01:31:07,290
I'm using the load override instead of the

01:31:07,290 --> 01:31:10,120
runnable load average, which is one of the things

01:31:10,120 --> 01:31:13,460
that I would like to discuss a little bit more with you.

01:31:13,460 --> 01:31:16,810
So my understand of using the runnable load

01:31:16,810 --> 01:31:18,990
in the load balance, it was mainly to fix the case

01:31:18,990 --> 01:31:21,840
when you have a huge amount of tasks sleeping in blocked,

01:31:22,740 --> 01:31:25,600
the group of CPU can be seen as overloaded

01:31:25,600 --> 01:31:30,000
or as if there is almost nothing runnable on the run queue.

01:31:30,000 --> 01:31:31,200
That was there for that.

01:31:32,550 --> 01:31:35,010
With the new load balance rework,

01:31:37,720 --> 01:31:40,400
because we are not only looking at the load

01:31:40,400 --> 01:31:41,610
when we are looking at the load,

01:31:41,610 --> 01:31:43,690
it's only when we are overloaded,

01:31:43,690 --> 01:31:46,120
which means that all the CPU are used and fully used

01:31:46,120 --> 01:31:47,590
and more than used.

01:31:47,590 --> 01:31:49,650
So and this case it makes sense to come back

01:31:49,650 --> 01:31:51,400
and look and sort the blocked load.

01:31:54,830 --> 01:31:58,370
For me that the only case where we are using the runnable

01:31:58,370 --> 01:32:01,454
but maybe there are other case that I'm missing.

01:32:01,454 --> 01:32:06,454
- So part of the problem there is one of the things

01:32:12,190 --> 01:32:15,150
we discussed before where we strictly do it by wait,

01:32:15,150 --> 01:32:16,650
where we probably want to actually go

01:32:16,650 --> 01:32:19,734
by like hour running, then by weight, then by util.

01:32:19,734 --> 01:32:20,567
(man speaking off mic)

01:32:20,567 --> 01:32:22,190
Right, but I'm just saying, with that

01:32:22,190 --> 01:32:23,540
when you just go by weight,

01:32:24,990 --> 01:32:26,410
that's when you need a runnable load here.

01:32:26,410 --> 01:32:28,475
You could use block load if you had

01:32:28,475 --> 01:32:30,430
the NR running past first.

01:32:31,280 --> 01:32:32,113
- [Vincent] Okay.

01:32:32,113 --> 01:32:34,640
- 'Cause blocked load tells you what might weight backup.

01:32:34,640 --> 01:32:36,870
But when you're just going by weight,

01:32:38,390 --> 01:32:42,780
so this can be, I'm not saying it should be

01:32:42,780 --> 01:32:44,660
but I'm saying it could potentially be undone

01:32:44,660 --> 01:32:45,944
by your patch.

01:32:45,944 --> 01:32:48,790
- Okay.

01:32:50,020 --> 01:32:53,730
On the load balance part, I have replaced that variable

01:32:53,730 --> 01:32:58,200
by the load and sense some definite improvement

01:32:58,200 --> 01:32:59,033
in the test.

01:32:59,033 --> 01:33:04,025
I have tried to split between the improvement link

01:33:05,390 --> 01:33:08,510
to this change and other runnable workload.

01:33:08,510 --> 01:33:13,300
So there's some improvement it seems to both change.

01:33:13,300 --> 01:33:16,350
But moving back to the load makes an improvement

01:33:16,350 --> 01:33:18,750
in the fully loaded and the overloaded use case.

01:33:20,019 --> 01:33:23,440
So that's why and I'd like to go for then that.

01:33:25,260 --> 01:33:27,760
So I have also patched for the wakeup path as well

01:33:28,680 --> 01:33:31,370
where I have removed the runnable as well.

01:33:31,370 --> 01:33:36,370
And keep in mind that now we are looking at idle CPU first,

01:33:37,240 --> 01:33:39,000
which is quite similar to what we are doing

01:33:39,000 --> 01:33:41,210
with the rework on the load balance.

01:33:41,210 --> 01:33:43,950
So we have everything which is in looking for either

01:33:43,950 --> 01:33:46,080
a spec of positive first and then

01:33:46,080 --> 01:33:48,150
if there is not, we assume that everything

01:33:48,150 --> 01:33:50,700
is validated and we take into account this blocked.

01:33:51,960 --> 01:33:54,740
The only things that have not been done

01:33:54,740 --> 01:33:57,560
and I want to look at is the numa statistic part,

01:33:57,560 --> 01:33:59,940
which is still looking at the runnable.

01:33:59,940 --> 01:34:02,190
But that's probably something that I should have

01:34:02,190 --> 01:34:05,580
a look at as well because I think that

01:34:05,580 --> 01:34:06,930
it's just a matter of, yeah...

01:34:06,930 --> 01:34:10,180
And maybe I just wonder for the numa statistic

01:34:10,180 --> 01:34:12,850
if we should go further and do something more

01:34:12,850 --> 01:34:15,120
like what we are doing in the runnable.

01:34:15,120 --> 01:34:17,730
If all the node overloaded or not

01:34:17,730 --> 01:34:19,490
or we should just stay on the load?

01:34:19,490 --> 01:34:20,670
I don't know, to be honest.

01:34:20,670 --> 01:34:23,530
I'm not an expert on numa and I'm not sure

01:34:23,530 --> 01:34:25,480
about what really make sense

01:34:25,480 --> 01:34:28,130
from a policy point of view for the numa.

01:34:28,130 --> 01:34:30,300
If you have any advice then don't hesitate to wave.

01:34:33,099 --> 01:34:35,803
- [Man] Yeah, we'll have to look at it again.

01:34:39,390 --> 01:34:41,830
- From the test from back when I was still working

01:34:41,830 --> 01:34:46,612
on numa stuff, I found that leaving a CPU idle

01:34:46,612 --> 01:34:50,260
is almost always worse than getting locality wrong.

01:34:50,260 --> 01:34:51,093
- [Vincent] Okay.

01:34:51,093 --> 01:34:52,330
- So I think the direction you're going

01:34:52,330 --> 01:34:54,360
with the load balancer, in general,

01:34:54,360 --> 01:34:56,500
is probably the right direction for numa as well.

01:34:56,500 --> 01:34:58,640
- [Vincent] Okay, good, good to know.

01:35:01,910 --> 01:35:03,310
- [Man] Some of the performance patches we carry

01:35:03,310 --> 01:35:05,416
are exactly in that direction as well.

01:35:05,416 --> 01:35:10,290
That generally speaking the L1 and L2 is so small

01:35:10,290 --> 01:35:12,690
relative to the L3, even on Skylight,

01:35:12,690 --> 01:35:16,899
that the time to rewarm it is always lower than

01:35:16,899 --> 01:35:19,030
the time to wait, in practice.

01:35:20,780 --> 01:35:22,820
- And I saw another benefit.

01:35:22,820 --> 01:35:25,880
If we are able to remove all the runnable load,

01:35:25,880 --> 01:35:28,220
so we have a discussion about the flattened hierarchy

01:35:28,220 --> 01:35:29,980
after, which means that even for that,

01:35:29,980 --> 01:35:31,870
that would remove one of the reason

01:35:31,870 --> 01:35:33,750
to go through all the hierarchy.

01:35:33,750 --> 01:35:35,910
When we are enqueuing and dequeuing

01:35:35,910 --> 01:35:38,060
we have to maintain this runnable load

01:35:38,060 --> 01:35:39,380
and if we are not choosing no,

01:35:39,380 --> 01:35:40,680
we can skip it completely,

01:35:41,880 --> 01:35:43,690
which then gives some more benefit.

01:35:43,690 --> 01:35:45,320
Because the load is just about

01:35:45,320 --> 01:35:47,380
when we are migrating the task.

01:35:47,380 --> 01:35:52,060
So that's another reason for which

01:35:52,060 --> 01:35:54,990
I'd like us to go more in that direction,

01:35:54,990 --> 01:35:59,380
unless there are some good reason to keep it.

01:35:59,380 --> 01:36:01,980
But I haven't seen any problem for now.

01:36:03,145 --> 01:36:04,820
(man speaking off mic)

01:36:04,820 --> 01:36:09,820
- [Man] Peter, take the mic.

01:36:11,000 --> 01:36:11,833
- It works.

01:36:11,833 --> 01:36:13,780
So yeah, if we can completely get rid of it,

01:36:13,780 --> 01:36:15,450
that would be very good.

01:36:15,450 --> 01:36:20,250
- [Vincent] Thank you.

01:36:21,460 --> 01:36:22,880
- I was curious about the comment like

01:36:22,880 --> 01:36:26,050
the cases that were fixed bytes in production,

01:36:26,050 --> 01:36:29,487
wouldn't those progress or why not?

01:36:31,330 --> 01:36:33,020
- Those cases are fixed by the fact

01:36:33,020 --> 01:36:34,550
that we do a path by running.

01:36:38,425 --> 01:36:41,780
- The load balancer used to only look at the load numbers

01:36:42,690 --> 01:36:47,690
and then when you're idle, it would see a large load

01:36:48,070 --> 01:36:50,820
due to the blocked load, even though there were really

01:36:50,820 --> 01:36:53,090
no runnable tasks and that would confuse the thing.

01:36:53,090 --> 01:36:55,340
But now we're first looking at number running,

01:36:55,340 --> 01:36:56,520
are there runnable tasks?

01:36:56,520 --> 01:36:59,190
If there are none then we'll look at the utilization.

01:36:59,190 --> 01:37:00,910
And only in the overloaded case,

01:37:00,910 --> 01:37:03,200
when everybody has an overload to do,

01:37:03,200 --> 01:37:05,370
we'll look at the actual load number.

01:37:05,370 --> 01:37:07,870
And in that scenario load average

01:37:07,870 --> 01:37:11,260
is the right number to look at.

01:37:11,260 --> 01:37:13,190
- The reason it's nice to look at load average though

01:37:13,190 --> 01:37:14,328
is blocked average,

01:37:14,328 --> 01:37:16,160
in the case where you're overloaded

01:37:16,160 --> 01:37:17,360
but have a local idle,

01:37:17,360 --> 01:37:19,110
you're just trying to patch a local hole

01:37:19,110 --> 01:37:21,803
rather than fundamentally move the balance of the system

01:37:21,803 --> 01:37:24,330
and the blocked load gives you a nice proxy

01:37:24,330 --> 01:37:26,964
for how much load you expect to back up onto the CPU,

01:37:26,964 --> 01:37:28,480
which we have to throw away when we

01:37:28,480 --> 01:37:30,030
can only look at runnable load.

01:37:36,680 --> 01:37:38,630
- Okay, so that was for the first item.

01:37:39,720 --> 01:37:42,520
So we have 30 minute, we still have 20 minute.

01:37:42,520 --> 01:37:45,270
The other one is the depiction of the overloaded state.

01:37:46,130 --> 01:37:49,880
So right now we are at capacity, fully busy,

01:37:49,880 --> 01:37:52,400
and overloaded, and some other special case.

01:37:56,925 --> 01:37:59,700
The point is that in some case we failed

01:37:59,700 --> 01:38:04,090
to clearly detect if the CPU is overloaded or not.

01:38:04,090 --> 01:38:07,180
So this is a simple trace, so I have just run

01:38:07,180 --> 01:38:11,420
at bench, which mainly just overload the system.

01:38:11,420 --> 01:38:14,877
What you can see there, just to show this kind of problem.

01:38:14,877 --> 01:38:17,290
On the top it's all the tasks that are scheduled.

01:38:17,290 --> 01:38:20,029
So we can see on the CPU we'll always have something running

01:38:20,029 --> 01:38:22,020
and sometimes a lot of things.

01:38:22,020 --> 01:38:23,150
And then at the metrics.

01:38:23,150 --> 01:38:25,820
So it's the util average and the util est.

01:38:25,820 --> 01:38:28,370
So mainly data utilization of the CPU.

01:38:28,370 --> 01:38:30,080
And we can see that even if the system

01:38:30,080 --> 01:38:33,350
is almost always busy with some thing,

01:38:33,350 --> 01:38:36,930
in some case that's 100 millisecond times slice.

01:38:38,567 --> 01:38:41,100
We can see that the util average and util est

01:38:41,100 --> 01:38:43,020
can be below the threshold saying that

01:38:43,020 --> 01:38:45,890
we are not fully busy and overloaded.

01:38:45,890 --> 01:38:49,170
And that's mainly because when we are migrating the tasks,

01:38:49,170 --> 01:38:50,630
we are migrating utilization,

01:38:50,630 --> 01:38:52,130
which is useful for us to know

01:38:53,140 --> 01:38:54,840
when we are not overloaded.

01:38:55,840 --> 01:38:58,300
But in the case where the system is overloaded,

01:38:59,290 --> 01:39:03,410
the utilization number reflect what the task want

01:39:03,410 --> 01:39:06,280
or just what the task can have access to?

01:39:18,950 --> 01:39:21,550
- There is a distinct lack of (microphone feedback).

01:39:27,360 --> 01:39:28,960
There's a distinct lack of idle time

01:39:28,960 --> 01:39:32,820
and I seem to remember that for CPU frequency scheduling

01:39:32,820 --> 01:39:36,960
we also have a measure where we accrue idle time.

01:39:36,960 --> 01:39:41,420
So maybe you can abuse that to

01:39:41,420 --> 01:39:43,910
detect the lack of idle time and otherwise

01:39:43,910 --> 01:39:45,760
maybe we should look at it different.

01:39:48,460 --> 01:39:50,230
- Looking at idle time can be an idea.

01:39:50,230 --> 01:39:51,660
The other point was about--

01:39:55,030 --> 01:39:58,200
- I was gonna, after we chatted

01:39:58,200 --> 01:40:00,450
and maybe I play with something here as well,

01:40:02,290 --> 01:40:05,430
there is one more trick we can do on the pelt signal

01:40:05,430 --> 01:40:10,430
where we can expand the period but on updates

01:40:12,050 --> 01:40:14,710
where we haven't had an update in awhile,

01:40:14,710 --> 01:40:18,180
account that update with a longer time delta.

01:40:19,240 --> 01:40:21,260
And you effectively can get a signal

01:40:21,260 --> 01:40:24,990
that is as responsive as the current signal on change

01:40:24,990 --> 01:40:28,070
but much more stable around sleep and wake

01:40:28,070 --> 01:40:29,790
because it effectively has a...

01:40:29,790 --> 01:40:34,790
You can, I played with some ways to change the math

01:40:34,840 --> 01:40:37,386
so that you can make the period dynamic

01:40:37,386 --> 01:40:40,450
with the current numbers and it actually

01:40:40,450 --> 01:40:41,700
looked pretty reasonable.

01:40:42,713 --> 01:40:45,040
- The problem I see that if we play with that

01:40:45,040 --> 01:40:47,788
is that utilization is used for the frequency scaling

01:40:47,788 --> 01:40:51,050
and in some case we want to scale that--

01:40:51,050 --> 01:40:52,670
- No, no, but what I'm saying is you

01:40:52,670 --> 01:40:54,130
can make that signal more,

01:40:55,060 --> 01:40:58,850
there's a way to expand it to cover--

01:40:59,910 --> 01:41:01,899
- Okay. - So that we don't get,

01:41:01,899 --> 01:41:03,510
the problem we have right now is that

01:41:03,510 --> 01:41:05,930
when we have an idle, we drop too much

01:41:05,930 --> 01:41:07,597
or we gain too much.

01:41:07,597 --> 01:41:08,970
- What in this case?

01:41:08,970 --> 01:41:10,660
In this case in front of the CPU

01:41:11,610 --> 01:41:13,000
the test is always running just

01:41:13,000 --> 01:41:15,660
because of migration in this case.

01:41:15,660 --> 01:41:17,600
- [Man] There are other cases where we see

01:41:17,600 --> 01:41:19,700
the signal moving too much.

01:41:19,700 --> 01:41:22,220
- So numa in addition instead of the util.est?

01:41:23,250 --> 01:41:24,730
- [Man] No I'm just saying is there's a way

01:41:24,730 --> 01:41:25,954
to make the signal more stable.

01:41:25,954 --> 01:41:27,010
- Oh yeah, okay.

01:41:27,910 --> 01:41:32,653
- Hey Vincent, you hear me okay now?

01:41:34,030 --> 01:41:37,200
I had a question on, if it's always running,

01:41:37,200 --> 01:41:39,730
why isn't util.avg and util.est pretty high?

01:41:41,680 --> 01:41:43,740
If something's migrating out and something's migrating in

01:41:43,740 --> 01:41:46,170
and it's running, shouldn't it all be high anyway?

01:41:47,090 --> 01:41:49,680
- [Vincent] In this case it's just that

01:41:50,730 --> 01:41:53,760
we are migrating something out because

01:41:53,760 --> 01:41:56,440
the utilization can be more than the capacity of the CPU.

01:41:56,440 --> 01:41:57,940
- So like all the threads are new threads,

01:41:57,940 --> 01:41:59,760
that's why the utilization is low?

01:41:59,760 --> 01:42:00,890
- Yeah. - Okay.

01:42:00,890 --> 01:42:02,550
Because they're not long running threads.

01:42:02,550 --> 01:42:03,550
They're all a bunch of new threads

01:42:03,550 --> 01:42:04,540
and they're long running because

01:42:04,540 --> 01:42:05,780
they're getting migrated out?

01:42:05,780 --> 01:42:06,850
- [Vincent] Yeah, that's just because

01:42:06,850 --> 01:42:08,720
of all the migration that can happen.

01:42:08,720 --> 01:42:10,680
That's why we are in such situation.

01:42:11,804 --> 01:42:16,804
- So for example, if you have five always running tasks

01:42:17,090 --> 01:42:21,225
then they will all get a 20% utilization

01:42:21,225 --> 01:42:23,980
and your CPU will be 100% utilized.

01:42:23,980 --> 01:42:27,450
Then if you migrate two tasks away,

01:42:27,450 --> 01:42:31,780
the utilization will instantly drop to 60%

01:42:31,780 --> 01:42:34,570
even though your CPU will never be idle

01:42:34,570 --> 01:42:37,800
and it will then just need to readjust back to 100%.

01:42:37,800 --> 01:42:40,580
So if you have a lot of migration back and forth,

01:42:40,580 --> 01:42:42,980
you'll get these jumps in the utilization.

01:42:46,550 --> 01:42:48,590
- And that's why I'm coming back to

01:42:50,100 --> 01:42:52,920
we still have the util average and util est there

01:42:52,920 --> 01:42:54,570
and that's the value of the load average

01:42:54,570 --> 01:42:56,070
and the runnable load average.

01:42:57,600 --> 01:42:59,300
And you can see that is really high.

01:42:59,300 --> 01:43:02,830
So in my case, all my tasks have the nice value of zero.

01:43:05,020 --> 01:43:08,340
And I will just explain, one main difference

01:43:08,340 --> 01:43:10,510
between the runnable and the util average

01:43:10,510 --> 01:43:12,970
is that in the runnable we are taking into account

01:43:12,970 --> 01:43:14,660
the waiting time for the task.

01:43:16,960 --> 01:43:18,510
Which mean that if you look at that,

01:43:18,510 --> 01:43:20,563
it mean that we have a lot of waiting time

01:43:20,563 --> 01:43:23,270
in the CPU right now compared to the utilization.

01:43:23,270 --> 01:43:24,730
Even if the utilization is low

01:43:24,730 --> 01:43:26,670
but your runnable value is high

01:43:26,670 --> 01:43:30,720
it mean that some tasks are waiting for quite a long time.

01:43:32,310 --> 01:43:37,310
So my goal would be to see if we can use that to mitigate.

01:43:38,230 --> 01:43:41,450
It mean that typically if there is no waiting time,

01:43:41,450 --> 01:43:44,880
the runnable and the util average should be quite similar

01:43:44,880 --> 01:43:46,430
and should be exactly the same,

01:43:48,020 --> 01:43:50,020
if we're not taking into account the weight.

01:43:50,020 --> 01:43:51,430
When we start to be above it mean

01:43:51,430 --> 01:43:53,520
that we have some waiting time.

01:43:53,520 --> 01:43:54,970
That doesn't mean that we are overloaded

01:43:54,970 --> 01:43:56,770
but until some point.

01:43:56,770 --> 01:43:59,320
So maybe the tricky point, we need to know

01:43:59,320 --> 01:44:01,627
exactly what is the threshold to say okay,

01:44:01,627 --> 01:44:03,550
even if the utilization is low,

01:44:03,550 --> 01:44:05,680
we are overloaded because the waiting time is high.

01:44:05,680 --> 01:44:08,200
That's something that I would like to have a look

01:44:08,200 --> 01:44:10,790
to have a better view of when the system is overloaded

01:44:10,790 --> 01:44:13,970
and to support this kind of migration.

01:44:13,970 --> 01:44:18,970
And if we are able to remove this runnable load average,

01:44:19,690 --> 01:44:22,010
which mean that we will have one metric

01:44:22,010 --> 01:44:25,290
and I think this runnable time,

01:44:27,357 --> 01:44:29,780
not load because the load is no more involved,

01:44:29,780 --> 01:44:31,690
can be used instead.

01:44:31,690 --> 01:44:33,230
Keep in mind that in this case,

01:44:33,230 --> 01:44:36,030
the goal is not to adapt queue in the queuing,

01:44:36,030 --> 01:44:38,160
it's like the loadable average value.

01:44:38,160 --> 01:44:40,970
So that's something that just have to be debated

01:44:40,970 --> 01:44:44,129
when we are migrating tasks and not all the time.

01:44:44,129 --> 01:44:46,644
- So I think you kind of already hinted at this

01:44:46,644 --> 01:44:48,520
when you were explaining it and maybe I

01:44:48,520 --> 01:44:50,400
didn't get it properly.

01:44:50,400 --> 01:44:53,810
In the case when average and util.est

01:44:53,810 --> 01:44:56,538
are going down because two big threads migrated away,

01:44:56,538 --> 01:44:58,840
you only want to do what you're trying to do

01:44:58,840 --> 01:45:02,147
if the company still have a significant amount of load.

01:45:02,147 --> 01:45:04,440
Maybe after that whatever gets migrated

01:45:04,440 --> 01:45:06,670
is gonna be two tiny tasks and you do want

01:45:06,670 --> 01:45:09,198
to let the util average come down.

01:45:09,198 --> 01:45:10,430
So how do you plan to distinguish between

01:45:10,430 --> 01:45:12,420
one happening versus another?

01:45:12,420 --> 01:45:15,860
- I will not switch, I just want to look at both.

01:45:15,860 --> 01:45:19,500
The point is that if you have a low utilization

01:45:19,500 --> 01:45:21,870
but high runnable value, it mean that

01:45:21,870 --> 01:45:25,506
the tasks were waiting to run on the CPU.

01:45:25,506 --> 01:45:30,420
So you must let them fill this new available

01:45:30,420 --> 01:45:33,360
so after migrating a task you have some spare capacity

01:45:33,360 --> 01:45:36,150
that will be filled by these waiting tasks.

01:45:36,150 --> 01:45:36,983
- [Man] So all the waiting tasks

01:45:36,983 --> 01:45:38,808
are just like tiny tasks though.

01:45:38,808 --> 01:45:40,814
- In this case, yeah.

01:45:40,814 --> 01:45:42,740
The other tricky point is there.

01:45:44,020 --> 01:45:45,760
But normally the runnable should be low

01:45:45,760 --> 01:45:48,757
if it's a tiny task because it will not weight that much.

01:45:50,336 --> 01:45:53,085
- [Man] Okay.

01:45:53,085 --> 01:45:55,910
- So to continue on, that's some

01:45:55,910 --> 01:45:57,410
statistic that I have catched.

01:45:58,770 --> 01:46:01,470
So this is the mainline, this is where the patch sits

01:46:01,470 --> 01:46:04,497
so we have tried each time we have run

01:46:04,497 --> 01:46:09,450
we are getting the statistic for doing a load balance.

01:46:10,640 --> 01:46:15,640
There are the different type of authorization of the group.

01:46:18,110 --> 01:46:20,210
So for the mainline we have only the three

01:46:21,093 --> 01:46:23,140
and with the patchset we have more.

01:46:24,370 --> 01:46:28,430
Here is the number of time we have completed this statistic

01:46:28,430 --> 01:46:31,380
for the mainline and patchset so we can see the difference.

01:46:32,560 --> 01:46:35,160
We can see that with migrating some.

01:46:36,990 --> 01:46:40,530
Aescopus, he stayed there, as fully busy,

01:46:40,530 --> 01:46:41,650
which is a good thing.

01:46:41,650 --> 01:46:43,520
We are also reduced the number.

01:46:43,520 --> 01:46:46,590
I can't truly explain why we have reduced so much

01:46:46,590 --> 01:46:49,630
because it's on the same time that we have

01:46:49,630 --> 01:46:52,430
reduced the number of time we are computing the statistic.

01:46:52,430 --> 01:46:56,262
Maybe it's because we're offering the CPU a more

01:46:56,262 --> 01:47:00,060
better use so they are busy and because

01:47:00,060 --> 01:47:02,230
we aren't creating the load balance

01:47:02,230 --> 01:47:04,873
when we are busy, we are doing less of it.

01:47:09,830 --> 01:47:13,380
That's another place where I think it was Vanota

01:47:13,380 --> 01:47:17,350
raised that right now when you have more than one task

01:47:17,350 --> 01:47:22,350
with CPU either you detect that you are at capacity

01:47:24,430 --> 01:47:25,810
or you are fully loaded.

01:47:25,810 --> 01:47:28,020
And in order to say that I'm fully busy,

01:47:28,020 --> 01:47:31,250
you must be exactly at the value of the threshold,

01:47:32,300 --> 01:47:34,170
which doesn't really make sense.

01:47:34,170 --> 01:47:36,120
We should have some tasks there.

01:47:36,120 --> 01:47:41,120
It's mainly because right now the range is not a range,

01:47:41,170 --> 01:47:44,286
it's just one value, and we should extend this range.

01:47:44,286 --> 01:47:46,350
I would say we should not remove this,

01:47:46,350 --> 01:47:47,630
we should extend the range.

01:47:47,630 --> 01:47:48,930
- [Man] Are these peaking?

01:47:48,930 --> 01:47:49,763
- Not the point.

01:47:49,763 --> 01:47:51,200
So let's say just that for now

01:47:51,200 --> 01:47:54,884
with the crowd metric we can only detect for one value

01:47:54,884 --> 01:47:56,040
but we should find a way to extend

01:47:56,040 --> 01:48:00,160
to have more tasks, to have more time, the fully busy state.

01:48:01,250 --> 01:48:03,070
So to compare them often.

01:48:03,070 --> 01:48:06,750
And that will probably remove this kind of situation more

01:48:06,750 --> 01:48:09,380
because it's made up of that sometime.

01:48:09,380 --> 01:48:12,110
We have some CPU with no running task.

01:48:12,110 --> 01:48:14,266
That can happen because we have a lot

01:48:14,266 --> 01:48:19,266
of waking up tasks but I think we could even do better.

01:48:20,590 --> 01:48:22,450
So that was just the highlight that

01:48:22,450 --> 01:48:27,080
we can probably do better and moving a CPU

01:48:27,080 --> 01:48:31,170
and overloaded in fully busy state.

01:48:33,140 --> 01:48:35,770
So as I understand it, you seem to be using

01:48:35,770 --> 01:48:39,470
a lot of utilization of the CPUS as a criteria.

01:48:39,470 --> 01:48:41,660
And maybe this fits in some of the things you said

01:48:41,660 --> 01:48:44,800
but have you considered rather than using utilization,

01:48:44,800 --> 01:48:47,570
using the waiting for CPU time?

01:48:47,570 --> 01:48:49,780
The average, so it's basically using

01:48:49,780 --> 01:48:54,780
the wakeup latency as a way to figure out

01:48:55,730 --> 01:49:00,440
from which CPU should I pull tasks to put them elsewhere?

01:49:00,440 --> 01:49:04,130
So ideally the CPUs that take more time

01:49:04,130 --> 01:49:07,070
between the wakeup and the moment the task runs,

01:49:07,070 --> 01:49:09,620
those are the ones from which you want to pull task

01:49:10,530 --> 01:49:13,140
so that it might be a different metric you get to use

01:49:13,140 --> 01:49:14,290
instead of utilization.

01:49:16,870 --> 01:49:17,703
- There is two things.

01:49:17,703 --> 01:49:19,840
One, in order to set up the CPU

01:49:19,840 --> 01:49:21,420
where we want to pull a task,

01:49:21,420 --> 01:49:23,850
the load average includes this waiting time

01:49:23,850 --> 01:49:26,190
because the load average we have the runnable time.

01:49:26,190 --> 01:49:28,870
The runnable is not only the running

01:49:28,870 --> 01:49:31,670
but waiting out for the time you are waiting for CPU.

01:49:31,670 --> 01:49:32,650
- [Man] Okay, perfect.

01:49:36,880 --> 01:49:40,130
(man speaking off mic)

01:49:41,137 --> 01:49:45,137
- [Man] Thomas, do you mind speaking in the mic?

01:49:48,591 --> 01:49:51,530
- It'd still be worthwhile to split that out

01:49:51,530 --> 01:49:53,940
and compare and contrast what happens when

01:49:53,940 --> 01:49:56,360
you only look at the wait time,

01:49:56,360 --> 01:50:01,360
at the accumulated wait time at this moment because--

01:50:01,690 --> 01:50:04,920
- Yeah but looking at each--

01:50:04,920 --> 01:50:06,890
- After your wait time goes you know you

01:50:06,890 --> 01:50:10,650
can accumulate it in the scheduler.

01:50:10,650 --> 01:50:15,650
So if your wait time goes up, then you have obviously

01:50:15,680 --> 01:50:18,410
more work on that thing than you can fit in.

01:50:18,410 --> 01:50:20,230
It's not only a capacity problem.

01:50:21,300 --> 01:50:26,300
So you have one thing, which is taking a full time slice,

01:50:26,500 --> 01:50:28,840
and then you have five other things which might be

01:50:28,840 --> 01:50:33,808
just taking a fraction but they still have to wait.

01:50:33,808 --> 01:50:36,211
And if you have another CPU but they have

01:50:36,211 --> 01:50:40,180
five runnable tasks but the average wait time

01:50:40,180 --> 01:50:42,999
is very, very slow, you don't want to pull

01:50:42,999 --> 01:50:46,970
because they can nicely fit there.

01:50:47,890 --> 01:50:50,280
So it might be worthwhile to look at that

01:50:50,280 --> 01:50:53,730
as a separate experiment and see what comes out of there.

01:50:54,720 --> 01:50:55,553
- I can have a look.

01:50:55,553 --> 01:50:57,620
But for me clearly the runnable and

01:50:59,380 --> 01:51:00,720
that's part of the metric already.

01:51:00,720 --> 01:51:03,250
I mean if you have only some really short

01:51:03,250 --> 01:51:06,370
running tasks or waiting tasks, the load average

01:51:06,370 --> 01:51:10,010
will be lower than if you have a long waiting time.

01:51:11,200 --> 01:51:13,930
Except that it's not a microsecond value,

01:51:13,930 --> 01:51:18,550
just a range of value, just another point.

01:51:18,550 --> 01:51:21,000
- [Thomas] Right, but it might be interesting to

01:51:21,000 --> 01:51:22,027
do them separate.

01:51:22,027 --> 01:51:23,700
- Yeah, maybe yeah.

01:51:23,700 --> 01:51:24,650
We can have a look.

01:51:25,850 --> 01:51:27,300
Want to say something, Peter?

01:51:31,010 --> 01:51:34,280
- Having a look can't hurt but like always

01:51:34,280 --> 01:51:38,200
we want to avoid dying by the number of statistics we track.

01:51:38,200 --> 01:51:41,970
- Yeah, yeah, but it was just pulled.

01:51:41,970 --> 01:51:43,490
- For having a look that's fine.

01:51:43,490 --> 01:51:47,103
- For having the information.

01:51:50,770 --> 01:51:53,840
- In the past in Solaris was, if I recall well,

01:51:53,840 --> 01:51:58,840
it was an option for user space.

01:51:59,040 --> 01:52:02,660
So if you expect them to have high load,

01:52:02,660 --> 01:52:05,690
you may say that until my wait time for CPU

01:52:05,690 --> 01:52:08,850
did not out pass this value, don't migrate at all.

01:52:08,850 --> 01:52:12,790
So there would be no CPU idle or if it will be,

01:52:12,790 --> 01:52:14,840
it will be busy in few millisecond.

01:52:15,710 --> 01:52:19,821
So at least you avoid this migration problem is all.

01:52:19,821 --> 01:52:24,821
- Okay.

01:52:29,967 --> 01:52:30,979
We still have a few minutes.

01:52:30,979 --> 01:52:34,670
For the last item its the fairness.

01:52:34,670 --> 01:52:36,480
Essentially you raise that SPM

01:52:36,480 --> 01:52:38,650
and it's not sold in the current patchset.

01:52:39,510 --> 01:52:42,900
It's just that when you have the typical example,

01:52:42,900 --> 01:52:45,170
N+1 task on N CPUs system,

01:52:46,500 --> 01:52:48,840
which means that you can't balance this system.

01:52:50,260 --> 01:52:52,880
And right now what drive the migration of the tasks

01:52:52,880 --> 01:52:55,630
between all the CPU is the number of failed,

01:52:55,630 --> 01:52:57,930
so every time we have the load balance,

01:52:57,930 --> 01:53:01,870
we see that one CPU has more tasks than the other.

01:53:01,870 --> 01:53:04,000
So the balancer tried to pull these tasks

01:53:05,040 --> 01:53:06,540
or to pull some load.

01:53:06,540 --> 01:53:11,380
And then when it decide if it can't balance

01:53:11,380 --> 01:53:13,772
after a number of time we'll force the migration.

01:53:13,772 --> 01:53:17,580
Even if we haven't been able now

01:53:17,580 --> 01:53:19,900
try to just show and see if it backed up

01:53:19,900 --> 01:53:21,700
for the next load balance iteration.

01:53:23,050 --> 01:53:26,377
And the other metric that triggered the load balance

01:53:26,377 --> 01:53:29,645
if the N balance that is completed

01:53:29,645 --> 01:53:34,645
is just below the load of the task after load of the task.

01:53:36,510 --> 01:53:38,460
So for the second load it's clearly a matter

01:53:38,460 --> 01:53:41,570
of running the load average value and so on.

01:53:41,570 --> 01:53:45,070
So just to say that right now the migration is random.

01:53:45,070 --> 01:53:46,830
And even that, that's a point there,

01:53:46,830 --> 01:53:49,090
I even should add that when this one migrated

01:53:49,090 --> 01:53:50,950
in fact almost all the CPU tried

01:53:50,950 --> 01:53:54,200
to pull the tasks at the same time

01:53:54,200 --> 01:53:57,640
and it's mainly a matter of catch a CPU being faster

01:53:57,640 --> 01:54:00,220
than the other one to take the lock and pull the task.

01:54:00,220 --> 01:54:04,920
So it's clearly random and for me it's a problem

01:54:04,920 --> 01:54:06,470
because if we are random there,

01:54:06,470 --> 01:54:08,820
we'll be even more random in complete use case.

01:54:10,118 --> 01:54:11,450
So I haven't found any solution right now.

01:54:11,450 --> 01:54:13,550
Just wanted merely to raise the problem.

01:54:13,550 --> 01:54:16,880
How can we ensure a better finish instead of just

01:54:16,880 --> 01:54:20,110
expecting a random migration to solve the problem.

01:54:22,130 --> 01:54:23,380
So if you have any ideas.

01:54:26,246 --> 01:54:31,246
- So a global re-run time approximation

01:54:32,040 --> 01:54:33,670
would allow you to fix this.

01:54:34,560 --> 01:54:37,050
- [Presenter] Yeah, it would have to be a global, yeah.

01:54:44,090 --> 01:54:46,460
- Just out of curiosity, if you find that

01:54:46,460 --> 01:54:49,110
the n balance is only one task,

01:54:49,110 --> 01:54:51,650
why don't you just not migrate in this case?

01:54:51,650 --> 01:54:53,800
Perhaps it does not matter that much.

01:54:53,800 --> 01:54:55,310
- Because of the fairness.

01:54:55,310 --> 01:54:58,010
All the tasks have the same nice priority

01:54:58,010 --> 01:55:00,870
so they should all have almost the same weight.

01:55:00,870 --> 01:55:03,100
And in this case, you can see that these two tasks

01:55:03,100 --> 01:55:07,160
have less running time than the other one.

01:55:07,160 --> 01:55:10,570
There is no reason why they should run off time less

01:55:10,570 --> 01:55:12,140
or only derive time.

01:55:12,140 --> 01:55:14,360
- [Man] But if you ping pong the cache

01:55:14,360 --> 01:55:16,170
over various cache lines,

01:55:16,170 --> 01:55:18,565
you're actually destroying the system performance

01:55:18,565 --> 01:55:20,320
rather than improving things.

01:55:20,320 --> 01:55:23,000
- Yeah, but if you have one of these two tasks

01:55:23,000 --> 01:55:25,710
we would like to force the migration.

01:55:25,710 --> 01:55:29,570
I agree that for those tasks they are fine.

01:55:29,570 --> 01:55:33,760
But if your group is there, you will have

01:55:33,760 --> 01:55:35,630
less throughput than the other one.

01:55:35,630 --> 01:55:39,720
- [Man] So the people that care about completion times,

01:55:39,720 --> 01:55:42,240
for their batch jobs, for example, they care.

01:55:42,240 --> 01:55:45,100
They want all their tasks to complete

01:55:45,100 --> 01:55:46,350
at roughly the same time.

01:55:47,210 --> 01:55:51,530
And if you slowly migrate to one task around equally,

01:55:51,530 --> 01:55:54,550
which doesn't happen here, then all the tasks

01:55:54,550 --> 01:55:57,550
get roughly equal compute and then

01:55:57,550 --> 01:56:00,550
they'll roughly finish at the same time.

01:56:02,570 --> 01:56:05,280
Whereas if you keep them on the same CPU,

01:56:05,280 --> 01:56:07,910
then they'll run 50/50 and your worst day

01:56:08,840 --> 01:56:13,370
completion time is like twice what it would otherwise be.

01:56:13,370 --> 01:56:16,340
So we don't want to ping pong the task around

01:56:16,340 --> 01:56:18,960
at high frequency but we want to rotate it around

01:56:18,960 --> 01:56:20,840
at a low frequency, just to--

01:56:22,000 --> 01:56:24,160
- [Man] But you need a global runtime signal to do that.

01:56:24,160 --> 01:56:25,840
- [Peter] You need a global signal, yeah.

01:56:30,702 --> 01:56:33,952
(man speaking off mic)

01:56:40,860 --> 01:56:43,390
- So the other point about the fairness is,

01:56:43,390 --> 01:56:45,500
maybe it kind of goes to Peter's comment

01:56:45,500 --> 01:56:47,920
about having a global via runtime,

01:56:47,920 --> 01:56:49,850
is that if you're having little and big CPUs

01:56:49,850 --> 01:56:53,660
and you have four or eight big threads running all the time,

01:56:53,660 --> 01:56:55,360
the ones that get stuck on little,

01:56:55,360 --> 01:56:58,030
they get really low performance.

01:56:58,030 --> 01:56:59,960
And the ones on big and it could disrupt them all

01:56:59,960 --> 01:57:01,380
would be better.

01:57:01,380 --> 01:57:02,213
- [Man] Yeah, that's also a scenario.

01:57:02,213 --> 01:57:04,560
- That's another common case you need to solve.

01:57:06,445 --> 01:57:10,521
The on that did good on the little core

01:57:10,521 --> 01:57:11,354
they are unlikely to compare to the other one.

01:57:17,070 --> 01:57:20,940
- In that case you'd have to conclude

01:57:22,410 --> 01:57:26,120
the computer capacity into the--

01:57:26,120 --> 01:57:28,040
- [Vincent] Yeah, that can be a criteria.

01:57:30,090 --> 01:57:32,230
That's another step too.

01:57:36,650 --> 01:57:37,960
- We're out of time.

01:57:37,960 --> 01:57:39,890
Maybe one last question.

01:57:39,890 --> 01:57:41,539
We have only one minute, no more.

01:57:41,539 --> 01:57:43,460
Or we can continue after.

01:57:45,760 --> 01:57:46,790
Okay, thank you.

01:57:48,072 --> 01:57:51,239
(audience applauding)

01:58:09,775 --> 01:58:12,010
- Okay, I have a presentation that explains how

01:58:12,010 --> 01:58:15,270
all the flattening of the run controls

01:58:16,485 --> 01:58:17,490
for the CPU controller works.

01:58:17,490 --> 01:58:18,990
That presentation is tomorrow.

01:58:22,966 --> 01:58:25,540
This is mostly me asking questions on how to solve

01:58:25,540 --> 01:58:27,700
some odds and ends in the code that

01:58:27,700 --> 01:58:29,390
I really have no figured out yet.

01:58:30,957 --> 01:58:34,100
And with two very quick slides,

01:58:34,100 --> 01:58:37,640
giving the fastest intro I could come up with

01:58:37,640 --> 01:58:41,420
on how the CPU controller is structured today

01:58:42,510 --> 01:58:45,610
and what am I kind of sort of changing it into.

01:58:47,537 --> 01:58:51,414
So what we have today is every task has a

01:58:51,414 --> 01:58:56,414
sched_entity and every cgroup has a sched_entity

01:58:58,300 --> 01:58:59,460
on every CPU.

01:59:00,640 --> 01:59:04,810
And when we have something like post audio,

01:59:04,810 --> 01:59:08,380
which system deep puts way down,

01:59:08,380 --> 01:59:10,230
several levels down in the hierarchy,

01:59:11,410 --> 01:59:13,740
when it starts to run, the sched_entity

01:59:13,740 --> 01:59:18,210
of the process gets enqueued, cgroup run queue,

01:59:18,210 --> 01:59:21,530
then the sched_entity of the cgroup

01:59:21,530 --> 01:59:26,530
gets enqueued higher up and you keep going up the hierarchy.

01:59:26,590 --> 01:59:28,210
And at every level in the hierarchy,

01:59:28,210 --> 01:59:31,850
we look at all the scheduler attributes

01:59:31,850 --> 01:59:32,780
of the thing.

01:59:32,780 --> 01:59:34,850
So we look at the runtime,

01:59:34,850 --> 01:59:37,590
we track the amount of CPU time used.

01:59:37,590 --> 01:59:40,210
We look at the way it's in the priority

01:59:40,210 --> 01:59:45,210
and then after a few fractions of a microsecond

01:59:46,790 --> 01:59:48,509
when post audio goes back to sleep,

01:59:48,509 --> 01:59:51,109
we do it all over again to tear down that hierarchy.

01:59:52,302 --> 01:59:55,900
For normal desktop workloads, it's not too big of a deal.

01:59:55,900 --> 01:59:57,080
For many a server workloads,

01:59:57,080 --> 01:59:59,230
it is also not too big a deal.

01:59:59,230 --> 02:00:02,020
But there are also some workloads out there

02:00:02,020 --> 02:00:06,370
that have maybe 10 or 20 thousand context switches

02:00:06,370 --> 02:00:09,510
per context per CPU and the overhead of

02:00:09,510 --> 02:00:12,720
doing this stuff all the time is prohibitively expensive.

02:00:15,630 --> 02:00:17,210
What's the plan?

02:00:17,210 --> 02:00:19,670
The plan is to do as little as possible

02:00:19,670 --> 02:00:21,620
and make the scheduler a little lazier.

02:00:22,890 --> 02:00:24,750
So what I'm doing is parking

02:00:24,750 --> 02:00:26,250
all hierarchy off to the side.

02:00:27,540 --> 02:00:30,500
The hierarchy, the cgroups themselves

02:00:30,500 --> 02:00:31,850
are never on the run queue.

02:00:32,800 --> 02:00:36,130
The main processors themselves are on the run queue

02:00:36,130 --> 02:00:39,700
and one of the things that the code already does

02:00:39,700 --> 02:00:44,700
is compute a hierarchical load for each task.

02:00:44,950 --> 02:00:47,810
And just like that, I can also compute

02:00:47,810 --> 02:00:50,140
a hierarchical weight for the different tasks.

02:00:51,170 --> 02:00:55,250
And I can use that for the task priority,

02:00:55,250 --> 02:00:57,400
the same way nice levels are handled today.

02:01:00,420 --> 02:01:03,240
If your runtime can be scaled the same way

02:01:03,240 --> 02:01:05,230
it handles for nice levels today.

02:01:05,230 --> 02:01:09,630
Today every time we run a task for some amount of time

02:01:09,630 --> 02:01:13,611
where this is the amount of time in nanoseconds that it ran,

02:01:13,611 --> 02:01:16,260
we increase the virtual runtime basically

02:01:16,260 --> 02:01:20,830
by nice load zero divided by the weight of the task.

02:01:21,710 --> 02:01:23,910
And instead of using the task weight

02:01:23,910 --> 02:01:26,130
I now have the hierarchical task weight.

02:01:29,070 --> 02:01:31,620
So if you have a task that is inside

02:01:31,620 --> 02:01:35,260
a low priority cgroup, it would be treated similarly

02:01:35,260 --> 02:01:38,550
to say a task with a high nice level.

02:01:41,320 --> 02:01:42,510
And it seems to mostly work,

02:01:42,510 --> 02:01:45,140
there are a few gotchas.

02:01:45,140 --> 02:01:48,040
One thing is the preemption code that we have today.

02:01:49,180 --> 02:01:52,610
It does some things that work well

02:01:53,630 --> 02:01:56,940
in normal operation but when you have

02:01:56,940 --> 02:02:01,900
different tasks of wildly different priorities,

02:02:01,900 --> 02:02:04,290
things kind of fall apart and I'd like

02:02:04,290 --> 02:02:08,370
to ask people some questions of what I can do about that.

02:02:08,370 --> 02:02:13,370
Specifically the runtime difference is caused

02:02:13,820 --> 02:02:17,710
by all the tasks that ran on the CPU since

02:02:17,710 --> 02:02:21,360
the woken up task went to sleep,

02:02:21,360 --> 02:02:22,810
not just by the current task.

02:02:24,410 --> 02:02:28,490
And on the other hand, the granularity of

02:02:29,710 --> 02:02:32,510
whether or not to wake things up depends only on

02:02:32,510 --> 02:02:34,990
the priority of the woken up task.

02:02:36,450 --> 02:02:39,130
And that can result in things like

02:02:39,130 --> 02:02:41,970
deciding to preempt a task that just started running

02:02:42,980 --> 02:02:46,110
because some other task used the CPU before.

02:02:47,320 --> 02:02:52,260
And it can also result in a high priority task

02:02:53,280 --> 02:02:55,850
preempting a really much higher priority task

02:02:55,850 --> 02:02:57,640
because the priority of the second task

02:02:57,640 --> 02:02:59,820
is not used at all in these calculations.

02:03:02,920 --> 02:03:03,753
(man speaking off mic)

02:03:03,753 --> 02:03:08,753
Oh, no, the priority of the task that is being woken up

02:03:11,100 --> 02:03:15,730
is the only thing that determines the wake up granularity.

02:03:15,730 --> 02:03:19,030
The priority of the task that is currently running

02:03:19,030 --> 02:03:23,890
does not matter and the priority of the other tasks

02:03:23,890 --> 02:03:27,253
that helped accumulate your runtime while

02:03:27,253 --> 02:03:29,940
this task was asleep also does not matter.

02:03:31,275 --> 02:03:34,525
(man speaking off mic)

02:03:40,982 --> 02:03:45,482
- [Man] Hey Seth, could you speak into the microphone?

02:03:59,237 --> 02:04:02,941
- Okay, so what I was saying is that usually

02:04:02,941 --> 02:04:07,330
the priority of the tasks which are currently runnable

02:04:07,330 --> 02:04:11,060
they are taken into account in the sched slide

02:04:11,060 --> 02:04:13,660
that is computed for the point.

02:04:15,080 --> 02:04:18,480
- Except the sched slide that is computed for preemption,

02:04:19,520 --> 02:04:21,670
it's done by wake up round.

02:04:21,670 --> 02:04:24,190
It's just a go delta fair on this slide.

02:04:25,361 --> 02:04:28,190
And that does not take the currently running task

02:04:28,190 --> 02:04:33,020
into account, only the woken up task.

02:04:34,650 --> 02:04:36,970
This is in a wakeup preempt entity.

02:04:38,490 --> 02:04:40,490
- Yeah, but in fact you have the skip prior

02:04:40,490 --> 02:04:42,990
and you have these trundle of first slips,

02:04:42,990 --> 02:04:46,940
something like that, and you take off of the skip prior.

02:04:46,940 --> 02:04:49,843
That's where the place where how much

02:04:49,843 --> 02:04:51,190
you will preempt the currently runnable.

02:04:51,190 --> 02:04:55,370
- Yes, but that amount is scaled here

02:04:55,370 --> 02:04:57,920
and if we have a high priority cgroup

02:04:58,821 --> 02:05:03,821
then that number may be reduced by a lot.

02:05:05,510 --> 02:05:10,230
The number that place entity comes up with

02:05:10,230 --> 02:05:12,460
with the gentle first sleepers,

02:05:12,460 --> 02:05:16,010
might be reduced by another factor of 10 in this function

02:05:16,010 --> 02:05:18,310
if we have a task from a high priority cgroup.

02:05:20,570 --> 02:05:23,690
And at that point you can have a task

02:05:23,690 --> 02:05:26,640
from that high priority cgroup interrupting another task

02:05:26,640 --> 02:05:28,450
from that same cgroup because they

02:05:28,450 --> 02:05:29,950
are both really high priority.

02:05:34,333 --> 02:05:35,950
- [Man] Peter, can you talk in that mic?

02:05:39,160 --> 02:05:42,353
- It's been many, many years but much of this code

02:05:46,640 --> 02:05:50,180
predates cgroups and I agree that for cgroups

02:05:50,180 --> 02:05:51,430
they're just weird stuff.

02:05:52,690 --> 02:05:55,350
What I can remember is that this was

02:05:58,330 --> 02:06:03,330
an attempt to make nice to also to have your task

02:06:04,580 --> 02:06:08,210
more eagerly preempt a non-nice task.

02:06:08,210 --> 02:06:10,910
And this was fairly important at the time

02:06:10,910 --> 02:06:15,560
because X ran as minus five or something,

02:06:15,560 --> 02:06:16,410
I can't remember.

02:06:19,206 --> 02:06:20,990
But yeah, this is a long time ago.

02:06:20,990 --> 02:06:23,780
- [Derek] And what I'm wondering is not really

02:06:23,780 --> 02:06:25,810
what the code should do right now

02:06:25,810 --> 02:06:27,650
but what the behavior is.

02:06:27,650 --> 02:06:30,870
What is the behavior that people want from this code?

02:06:30,870 --> 02:06:33,480
Do people want higher priority tasks

02:06:33,480 --> 02:06:35,130
to preempt lower priority ones?

02:06:36,270 --> 02:06:38,320
And when looking at equal priority tasks,

02:06:39,670 --> 02:06:40,920
do you want something that's slept

02:06:40,920 --> 02:06:42,970
for a longer amount of time to wake up something

02:06:42,970 --> 02:06:43,970
that's been running?

02:06:45,100 --> 02:06:47,960
- The preemption program I'm actually more concerned about

02:06:47,960 --> 02:06:52,950
is not this but the case where you have a task,

02:06:55,580 --> 02:06:59,520
you can have a high weight group and a task in that group

02:06:59,520 --> 02:07:01,940
not get much of the h weight,

02:07:01,940 --> 02:07:05,150
which will actually massively depress its

02:07:05,150 --> 02:07:06,440
preemption priority.

02:07:06,440 --> 02:07:08,920
That's actually I think the more concerning case.

02:07:08,920 --> 02:07:11,170
- [Derek] That's definitely an issue as well.

02:07:11,170 --> 02:07:13,770
I've got questions on that in the next slide

02:07:13,770 --> 02:07:15,860
or the one after that. - Okay.

02:07:15,860 --> 02:07:19,050
Because I feel like this one doesn't

02:07:19,050 --> 02:07:22,190
have legs in the first place and you can maybe argue

02:07:22,190 --> 02:07:24,590
about how to make the place right but the other case

02:07:24,590 --> 02:07:26,305
I don't see how you can make right.

02:07:26,305 --> 02:07:27,980
- [Derek] Well I'm just wondering mostly what behavior

02:07:27,980 --> 02:07:30,480
would people like to see from the preemption code?

02:07:32,540 --> 02:07:35,800
Not necessarily on how but what would you like it?

02:07:35,800 --> 02:07:37,400
How would you like it to behave?

02:07:39,890 --> 02:07:42,700
- Yeah so traditionally, Derek, there was the expectation

02:07:42,700 --> 02:07:46,990
that a higher priority, so lower nice,

02:07:46,990 --> 02:07:51,350
would also run faster or have lower latency.

02:07:56,920 --> 02:07:58,087
Yeah, exactly.

02:07:59,666 --> 02:08:02,857
I don't know-- - We gave up on that.

02:08:02,857 --> 02:08:05,348
- Sort of. - Sort of because

02:08:05,348 --> 02:08:07,140
that caused other problems.

02:08:07,140 --> 02:08:09,210
- Yeah, so this is a remnant of that.

02:08:11,347 --> 02:08:14,973
- As soon as you have a task migration, it's all bonkers.

02:08:14,973 --> 02:08:17,660
- And just real quick, I was wondering,

02:08:17,660 --> 02:08:19,850
you keep asking what's the behavior people want

02:08:19,850 --> 02:08:21,490
and I don't think people case about

02:08:21,490 --> 02:08:22,440
what's the preemption thing.

02:08:22,440 --> 02:08:25,990
I think people care more about use cases that are seen,

02:08:25,990 --> 02:08:28,160
okay make the modifications and see that it's held.

02:08:28,160 --> 02:08:29,260
Because a lot of times we say oh,

02:08:29,260 --> 02:08:30,880
this sounds like the best thing to do

02:08:30,880 --> 02:08:32,700
but once you get it into real world practice,

02:08:32,700 --> 02:08:35,240
everyone says why is my mouse jittery now?

02:08:35,240 --> 02:08:38,250
And then a lot of times a lot of the cases

02:08:38,250 --> 02:08:39,970
way back when when this code was written,

02:08:39,970 --> 02:08:44,270
I remember the responses you got from a lot of users

02:08:44,270 --> 02:08:46,630
was just my desktop seems smoother.

02:08:48,210 --> 02:08:51,130
How do you measure that?

02:08:51,130 --> 02:08:55,100
- So we've touched on this in a few other discussions.

02:08:55,100 --> 02:08:57,910
Without global re-run time, this run is actually just

02:08:57,910 --> 02:08:59,560
a total on a number generator

02:08:59,560 --> 02:09:00,930
because it actually just depends

02:09:00,930 --> 02:09:03,080
on your pan group entity, which is per CPU.

02:09:03,980 --> 02:09:06,500
And that is gonna have totally random placement,

02:09:06,500 --> 02:09:07,910
based on whether you're on the CPU

02:09:07,910 --> 02:09:10,490
you were running previously or a new CPU.

02:09:10,490 --> 02:09:14,280
- But this code, the only suspect behavior I saw

02:09:14,280 --> 02:09:19,280
was when a, basically what happened is that

02:09:19,830 --> 02:09:23,070
a task in a high priority group was sleeping

02:09:23,070 --> 02:09:24,620
for a long amount of time,

02:09:25,800 --> 02:09:29,130
at which point the runtime advantages got

02:09:29,130 --> 02:09:30,970
offered amenity runtime.

02:09:30,970 --> 02:09:33,720
Scott, by place entity?

02:09:36,116 --> 02:09:38,200
- [Scott] Place entity with gentle fell asleep but yeah.

02:09:38,200 --> 02:09:40,430
- And at that point, because Scott does

02:09:40,430 --> 02:09:41,980
a fair skills sit down further,

02:09:44,089 --> 02:09:47,240
he ended up with something so short

02:09:47,240 --> 02:09:51,900
that a task of that priority would almost always

02:09:51,900 --> 02:09:53,970
preempt another task of same priority.

02:09:54,910 --> 02:09:56,450
- [Scott] But you could argue maybe that's fair

02:09:56,450 --> 02:09:58,587
if it's been sleeping for a long time.

02:09:58,587 --> 02:10:00,200
(man speaking off mic)

02:10:00,200 --> 02:10:01,780
- It's with my code.

02:10:01,780 --> 02:10:04,112
With the hierarchical code it does not happen.

02:10:04,112 --> 02:10:05,970
- [Scott] But if it's been sleeping for a long time,

02:10:05,970 --> 02:10:07,772
you'd probably want it to have higher preemption

02:10:07,772 --> 02:10:09,170
priority, right.

02:10:09,170 --> 02:10:10,750
That was fair because that's been in my salon.

02:10:10,750 --> 02:10:12,700
- Well, for this particular case,

02:10:12,700 --> 02:10:17,460
the CCTO, then granularity, was set to a larger number

02:10:17,460 --> 02:10:20,230
than the maximum, than place entity.

02:10:22,710 --> 02:10:25,630
Which should effectively disable

02:10:27,200 --> 02:10:31,050
the preemption for same priority tasks.

02:10:32,090 --> 02:10:35,216
But it didn't, but that's not the way the scope works.

02:10:35,216 --> 02:10:36,200
And I think that's correct.

02:10:37,560 --> 02:10:41,280
I don't think , the alternative to,

02:10:42,680 --> 02:10:44,420
it's two hacks that I tried for this.

02:10:44,420 --> 02:10:46,970
One of them is scaling the vdiff

02:10:46,970 --> 02:10:49,120
with the priority of the current task.

02:10:49,120 --> 02:10:51,027
The other one that I tried is scaling

02:10:51,027 --> 02:10:55,640
the fair runtime advantage that place entity allows.

02:10:55,640 --> 02:11:00,020
They both sem to work roughly the same in the destination.

02:11:01,660 --> 02:11:02,787
While high priority tasks still

02:11:02,787 --> 02:11:06,044
preempts lower priority ones.

02:11:06,044 --> 02:11:10,630
And with the wakeup granularity larger

02:11:10,630 --> 02:11:14,120
than the maximum amount returned by place entity,

02:11:14,120 --> 02:11:17,190
you end up never preempting the same priority task.

02:11:19,369 --> 02:11:22,060
- I would posit, this is probably like

02:11:22,060 --> 02:11:24,250
number three or four on the problem list.

02:11:24,250 --> 02:11:25,750
- [Derek] Fair enough, let's move on.

02:11:27,363 --> 02:11:28,196
- Only the ones that are harder.

02:11:28,196 --> 02:11:29,470
- Yeah, this one is harder.

02:11:31,961 --> 02:11:34,130
Now calc group shares was knowledge ramp up, logic

02:11:34,130 --> 02:11:37,780
where you always end up with a non-zero weight.

02:11:39,450 --> 02:11:41,020
We do not have anything like that

02:11:41,020 --> 02:11:42,580
and update CSF our QH load

02:11:43,620 --> 02:11:46,040
for task H load and task H weight.

02:11:46,040 --> 02:11:48,490
I'm wondering if you want something like that there.

02:11:48,490 --> 02:11:51,270
Or if we should have the ramp up logic

02:11:51,270 --> 02:11:52,470
in a different location.

02:11:54,540 --> 02:11:55,890
Especially for task h load.

02:12:09,123 --> 02:12:10,250
- [Man] The way you're gonna use it,

02:12:10,250 --> 02:12:12,038
we feel like we need it.

02:12:12,038 --> 02:12:15,180
- [Man] We can't hear back here.

02:12:15,180 --> 02:12:17,580
- Doing what you're doing with it we do need it.

02:12:21,690 --> 02:12:24,858
- [Derek] I can look into other than that.

02:12:24,858 --> 02:12:27,510
- Neither of these fix the problem I mentioned just now,

02:12:27,510 --> 02:12:28,440
right though? - No.

02:12:28,440 --> 02:12:31,450
- If you have a low weight task and a high weight group,

02:12:31,450 --> 02:12:35,940
it's h load will be low still and its preemption priority

02:12:35,940 --> 02:12:38,297
will be embedded from where it should be.

02:12:38,297 --> 02:12:40,220
- [Derek] Yep, I will have to look into that as well.

02:12:40,220 --> 02:12:41,920
- And I will see how you fix that.

02:12:43,070 --> 02:12:44,320
It mathematically breaks.

02:12:47,510 --> 02:12:51,180
- Okay, I think we can fix that the same way

02:12:51,180 --> 02:12:54,470
h load calculates that.

02:12:54,470 --> 02:12:56,340
We can come up with a way for that.

02:12:56,340 --> 02:12:57,930
I think you have another problem.

02:12:57,930 --> 02:12:59,070
I think there are two big problems,

02:12:59,070 --> 02:13:04,070
that and low priority groups, you're going to give them

02:13:04,840 --> 02:13:05,740
way too much time.

02:13:08,640 --> 02:13:10,250
- [Derek] Yep, you're right, there's a bug there

02:13:10,250 --> 02:13:12,810
and I think I know how to fix that one.

02:13:12,810 --> 02:13:16,150
- if you only ave a minute left, we can talk after.

02:13:16,150 --> 02:13:19,590
- Another thing, right now on MQ,

02:13:19,590 --> 02:13:21,780
I walked the hierarchy all the time

02:13:21,780 --> 02:13:24,021
and that ends up being about half to overhead

02:13:24,021 --> 02:13:28,140
of the new code.

02:13:28,140 --> 02:13:30,900
And I'm wondering if I can get away with no walking

02:13:30,900 --> 02:13:34,010
if the last time we walked, there was a task on there?

02:13:34,010 --> 02:13:35,760
so we know the weight of the group.

02:13:48,490 --> 02:13:51,010
The last one is just CFS bandwidth stuff,

02:13:51,010 --> 02:13:51,860
which is not as--

02:13:53,315 --> 02:13:55,440
- Then we'll make it. - That's fair.

02:13:58,199 --> 02:14:00,827
- [Man] I have a few problems that's actually easier to fix.

02:14:05,800 --> 02:14:07,200
- There is the talk tomorrow where you can

02:14:07,200 --> 02:14:08,973
continue discussing this.

02:14:08,973 --> 02:14:11,140
(man speaking off mic)

02:14:11,140 --> 02:14:13,810
Yep, tomorrow at five, actually explaining what

02:14:13,810 --> 02:14:17,390
all of the runtime stuff means and offer

02:14:17,390 --> 02:14:19,330
few of the stuff I'm doing.

02:14:19,330 --> 02:14:21,190
Except for if it's CFS bandwidth stuff,

02:14:21,190 --> 02:14:22,800
which seemed like it would be

02:14:22,800 --> 02:14:25,370
a little too much for the other talk.

02:14:25,370 --> 02:14:27,500
- [Man] That is tomorrow, right?

02:14:27,500 --> 02:14:28,800
- Yep, it's tomorrow at 5.

02:14:29,924 --> 02:14:33,041
- Thanks, Chuck.

02:14:33,041 --> 02:14:36,291
(men speaking off mic)

02:14:51,150 --> 02:14:52,830
- We can move on to next session,

02:14:52,830 --> 02:14:54,940
unless there are comments on this

02:14:54,940 --> 02:14:57,640
for people who know how the CFS bandwidth stuff works.

02:14:59,210 --> 02:15:00,309
- [Man] I agree with that.

02:15:00,309 --> 02:15:05,309
- Sounds good.

02:15:05,504 --> 02:15:08,671
(audience applauding)

02:15:35,960 --> 02:15:37,660
- All right, I guess we can start.

02:15:38,700 --> 02:15:40,820
You might now notice the time has changed

02:15:40,820 --> 02:15:41,790
from what is announced.

02:15:41,790 --> 02:15:43,880
That is because I hadn't written my slides

02:15:43,880 --> 02:15:44,980
when I submitted that.

02:15:45,990 --> 02:15:46,860
And what I've written though,

02:15:46,860 --> 02:15:49,030
I probably would change the title again.

02:15:49,030 --> 02:15:52,750
The idea is I'm gonna describe some weird topological

02:15:52,750 --> 02:15:56,110
we have on our servers and then talk about

02:15:56,110 --> 02:15:59,340
some way of perhaps generalizing,

02:15:59,340 --> 02:16:00,910
a way to handle the scripts,

02:16:00,910 --> 02:16:04,616
and change the way we build schedule maintenance.

02:16:04,616 --> 02:16:06,320
if you're not finding your schedule,

02:16:06,320 --> 02:16:08,380
it might be in balloon talk, I don't know.

02:16:09,440 --> 02:16:11,770
So the short one to come out is

02:16:11,770 --> 02:16:14,153
is ThunderX from Calvium,

02:16:14,153 --> 02:16:16,861
two sockets of 48 CPUs and the topology

02:16:16,861 --> 02:16:21,861
looks something like this, which looks okay.

02:16:21,870 --> 02:16:23,300
You have your two sockets and you have

02:16:23,300 --> 02:16:24,900
the number available at the top.

02:16:26,140 --> 02:16:28,080
The thing is I have trust issues

02:16:28,080 --> 02:16:29,960
and so I didn't really believe in that.

02:16:29,960 --> 02:16:31,631
Even during the box it says you have

02:16:31,631 --> 02:16:33,910
two boxes of functioning CPUs.

02:16:33,910 --> 02:16:35,740
I found some random paper that said

02:16:35,740 --> 02:16:36,890
that it's not the case.

02:16:37,970 --> 02:16:40,320
It was really only hinted by the paper

02:16:40,320 --> 02:16:44,000
because this primary has a box with

02:16:44,000 --> 02:16:47,110
eight small boxes in it and that's

02:16:47,110 --> 02:16:48,280
all the information I had.

02:16:48,280 --> 02:16:52,510
So I said from that and I used LM Bench

02:16:52,510 --> 02:16:55,163
to try and see the dependencies

02:16:55,163 --> 02:16:59,640
and the interactions from CPUs to CPU.

02:16:59,640 --> 02:17:02,620
So the idea is I do a recommendation on every CPU.

02:17:02,620 --> 02:17:05,790
I pick on CPU when some member of the population

02:17:05,790 --> 02:17:08,650
using LM Bench and then I tried to place

02:17:08,650 --> 02:17:10,400
a similar task on another CPU and I try

02:17:10,400 --> 02:17:11,820
all of the communications.

02:17:11,820 --> 02:17:16,260
The idea is if some CPU is showing some resource

02:17:16,260 --> 02:17:18,510
like maybe some interconnect or something,

02:17:18,510 --> 02:17:20,240
the variance that I get would be lower

02:17:20,240 --> 02:17:21,730
than if they didn't.

02:17:21,730 --> 02:17:23,370
That's the gist of it.

02:17:23,370 --> 02:17:26,570
I'm using that and some fancy hyacoglycerine

02:17:26,570 --> 02:17:27,940
I can generate what is called

02:17:27,940 --> 02:17:32,120
a angiogram, which is a tree that represents dependencies.

02:17:32,120 --> 02:17:37,120
What I can see is I have eight trees of six CPUs each

02:17:37,270 --> 02:17:39,243
and it's exactly how it was described in the paper

02:17:39,243 --> 02:17:42,320
, they didn't say how the tree

02:17:42,320 --> 02:17:43,440
even exists.

02:17:43,440 --> 02:17:46,050
But I could confirm that they lead to boxes

02:17:46,050 --> 02:17:48,006
was something that I could reproduce in reality.

02:17:48,006 --> 02:17:52,040
And you remember, it is a beautiful late.

02:17:52,040 --> 02:17:55,880
it looks like if there was a 76 in terms of

02:17:57,564 --> 02:17:59,036
the numbering scheme but yeah,

02:17:59,036 --> 02:18:00,420
it is a plain SMP.

02:18:00,420 --> 02:18:03,560
So if I actually represent that in the skim debate,

02:18:04,560 --> 02:18:05,890
I would have done it like this.

02:18:05,890 --> 02:18:10,007
So you have your IT demands of six CPUs each.

02:18:10,007 --> 02:18:11,790
You know that I'm available because

02:18:11,790 --> 02:18:14,520
it's not the same as MP so it's not additionally rated.

02:18:14,520 --> 02:18:16,610
And then you would have room and the others right

02:18:16,610 --> 02:18:18,680
but as you can see, it doesn't fit in the slide.

02:18:20,270 --> 02:18:22,080
So you get some improvements for that.

02:18:22,080 --> 02:18:25,070
Of course so for HotBench, you get better runtime

02:18:25,070 --> 02:18:26,930
because if you have small masks,

02:18:26,930 --> 02:18:28,420
it's gonna take less time to rent.

02:18:28,420 --> 02:18:31,140
Don't be too impressed by that minus 25% because

02:18:31,140 --> 02:18:31,973
there are single governments on the single registry

02:18:31,973 --> 02:18:32,806
is absymal, it's really bad.

02:18:32,806 --> 02:18:33,639
Only, pro housing, it has 96 of them.

02:18:33,639 --> 02:18:34,472
So if on one CPU you was going lsss things,

02:18:34,472 --> 02:18:35,305
you could have \aa...

02:18:35,305 --> 02:18:36,249
It's really bad.

02:18:36,249 --> 02:18:39,262
The only pro it has is that it has 96 of them.

02:18:39,262 --> 02:18:44,262
So if on one CPU, he was counting less things,

02:18:44,280 --> 02:18:47,127
you can have a piece of runtime improvement.

02:18:47,127 --> 02:18:50,160
Memory bank, I would expect to see some better results

02:18:50,160 --> 02:18:53,830
when you are not overloaded or over committed in tasks

02:18:53,830 --> 02:18:55,770
because then you have a better use of your other

02:18:55,770 --> 02:18:56,870
available bandwidths.

02:18:56,870 --> 02:18:59,700
Instead of having tasks that are containing with each other.

02:18:59,700 --> 02:19:03,810
I can improve it, I just wish I was on a bench.

02:19:03,810 --> 02:19:06,610
- So now it's all of the craziest ideas I got from that.

02:19:09,520 --> 02:19:10,750
How do you spot the scheduler?

02:19:10,750 --> 02:19:14,440
It might not just be for special hardware.

02:19:16,270 --> 02:19:19,360
I'm trying to have a go back to the latency talk

02:19:19,360 --> 02:19:20,690
that we have later on.

02:19:22,190 --> 02:19:23,040
There's both sides to that.

02:19:23,040 --> 02:19:24,980
So for latency-nice, what came out of the discussion

02:19:24,980 --> 02:19:27,240
is people were at both ends of the spectrum.

02:19:27,240 --> 02:19:29,240
There are people who want to scan less things

02:19:29,240 --> 02:19:31,320
and there are people who want to scan all the things.

02:19:31,320 --> 02:19:34,350
So it's not necessarily a solution.

02:19:34,350 --> 02:19:35,660
But a question that arises from that

02:19:35,660 --> 02:19:38,390
is does it make sense to run so any CPUs

02:19:38,390 --> 02:19:39,300
in the same domain?

02:19:39,300 --> 02:19:42,730
So in that example, the MC domain had 48 CPUs.

02:19:42,730 --> 02:19:43,710
Is that realistical?

02:19:43,710 --> 02:19:46,210
Does it still make sense nowadays?

02:19:46,210 --> 02:19:49,280
My point being, the distance from one CPU to another

02:19:49,280 --> 02:19:50,890
is not gonna be the same.

02:19:51,941 --> 02:19:53,400
I don't mean physical difference

02:19:53,400 --> 02:19:56,850
so that your nanometer scale and it measures

02:19:56,850 --> 02:19:57,899
the distance from one CPU to another.

02:19:57,899 --> 02:20:00,110
It's gonna be different, of course.

02:20:00,110 --> 02:20:04,000
What I mean is if you have a read me

02:20:04,000 --> 02:20:05,357
sitting in your cache line

02:20:05,357 --> 02:20:09,030
- And the sub region, I was CPU's,

02:20:10,380 --> 02:20:13,720
well depending on how the collections are made,

02:20:13,720 --> 02:20:17,040
it might take some more time or less time.

02:20:17,040 --> 02:20:19,890
The question though is nowadays

02:20:19,890 --> 02:20:22,443
or coming our future evidence of

02:20:22,443 --> 02:20:25,980
network-in-chief in new in a single socket.

02:20:27,627 --> 02:20:29,490
From my description was some other people,

02:20:29,490 --> 02:20:32,270
they expect or they suspect that what Cavium

02:20:32,270 --> 02:20:35,260
did in the circuit is they might have a cacheout meter

02:20:35,260 --> 02:20:37,650
shared between some of the CPUs which

02:20:37,650 --> 02:20:39,620
would explain where we get that.

02:20:39,620 --> 02:20:42,520
But so we're starting to have something where

02:20:42,520 --> 02:20:44,710
does it still make sense just to have our

02:20:44,710 --> 02:20:46,840
three hardcoded domains.

02:20:46,840 --> 02:20:48,310
So of course it's by default.

02:20:48,310 --> 02:20:51,820
There are some architectures that we do differently.

02:20:53,490 --> 02:20:57,620
A390 has five domains, or something like that.

02:20:57,620 --> 02:20:59,670
So you could do that pair architecture

02:21:00,830 --> 02:21:04,810
or you could try to maybe generalize this differently.

02:21:04,810 --> 02:21:07,550
If you look at the ACPI spike,

02:21:07,550 --> 02:21:09,900
you can describe the topology as crazy as you like

02:21:09,900 --> 02:21:13,010
with as many different logos as you would want.

02:21:13,010 --> 02:21:16,240
On R64 for the topology we just crash everything

02:21:16,240 --> 02:21:19,880
to make it fit inside SMT, MC, and DIE.

02:21:19,880 --> 02:21:21,710
And so maybe we could change that

02:21:21,710 --> 02:21:23,730
and have something that is a bit more clever.

02:21:25,350 --> 02:21:26,930
I picked SAP as an example but

02:21:26,930 --> 02:21:28,280
it could be something else.

02:21:33,846 --> 02:21:36,375
(man speaking off mic)

02:21:36,375 --> 02:21:38,520
- [Man] So like you said, it's pro-architecture.

02:21:38,520 --> 02:21:40,510
If it makes sense and the measurement shows

02:21:40,510 --> 02:21:43,220
that it does then go for it.

02:21:43,220 --> 02:21:45,910
You don't have to squash it all together.

02:21:45,910 --> 02:21:50,370
If there's an actual topology there, then describe it.

02:21:50,370 --> 02:21:51,570
That's what they're for.

02:21:55,450 --> 02:21:57,339
- So in this case there is a doubt?

02:21:57,339 --> 02:22:02,339
- If I go back to, it's very nice old hardware.

02:22:04,510 --> 02:22:06,110
The LLC is just in L2.

02:22:06,110 --> 02:22:07,610
You have L1 and then show L2 between

02:22:07,610 --> 02:22:08,510
all the LLC rules.

02:22:09,835 --> 02:22:11,630
- And that's also, that's why we did some changes

02:22:11,630 --> 02:22:13,510
in the topology to be able so that

02:22:13,510 --> 02:22:16,820
we can supersede the different kind of level.

02:22:16,820 --> 02:22:20,222
So right now it's used by architecture but--

02:22:20,222 --> 02:22:22,650
- Yeah, that's what the 390 is doing.

02:22:22,650 --> 02:22:25,520
- If R64 thinks that it should

02:22:25,520 --> 02:22:26,970
dynamically discover more to do.

02:22:26,970 --> 02:22:29,340
- The question is is that really just R64?

02:22:29,340 --> 02:22:31,090
Can we expect to see that on other,

02:22:31,950 --> 02:22:33,140
that would be the question?

02:22:33,140 --> 02:22:36,122
Would it make sense to have some sort of generalization?

02:22:36,122 --> 02:22:39,564
- Maybe something like a Twitter.

02:22:39,564 --> 02:22:41,742
The point is that some of this will reflect

02:22:41,742 --> 02:22:46,742
hardware (speaking off mic).

02:22:52,050 --> 02:22:54,280
- AMD has something like this.

02:22:54,280 --> 02:22:56,790
- Is that the sub numeric clustering?

02:22:56,790 --> 02:23:00,440
- No, it's how they built their chips.

02:23:00,440 --> 02:23:03,240
They did small clusters as well, stuck together.

02:23:07,540 --> 02:23:09,460
Other manufacturers might or might not

02:23:09,460 --> 02:23:13,580
have something like that, I don't know.

02:23:13,580 --> 02:23:15,190
- Another thing is that if we have

02:23:15,190 --> 02:23:17,710
some sort of information, and in here

02:23:17,710 --> 02:23:19,920
this would be the distance between caches

02:23:19,920 --> 02:23:22,740
with distance being an arbitrary distance you would define,

02:23:22,740 --> 02:23:24,730
we could improve some of the properties we have?

02:23:24,730 --> 02:23:27,640
So like I like imbalanced percent between

02:23:27,640 --> 02:23:28,730
the scheduled domains because right now

02:23:28,730 --> 02:23:29,880
it's hardcoded values.

02:23:30,860 --> 02:23:34,200
SMT has one value, MC has one value

02:23:34,200 --> 02:23:36,320
but it's per architecture.

02:23:36,320 --> 02:23:38,730
But then you could try to make something

02:23:39,651 --> 02:23:41,260
like the architecture defines how you could set it.

02:23:42,430 --> 02:23:46,280
- But then from a user space then it

02:23:46,280 --> 02:23:47,978
depends on what you want to do.

02:23:47,978 --> 02:23:50,660
Some different value ops have been set

02:23:50,660 --> 02:23:53,440
but as far as I can tell, it's not hardcoded.

02:23:53,440 --> 02:23:55,440
Just that we have to start with a value.

02:23:59,853 --> 02:24:03,700
- Yeah but getting actual useful topology information

02:24:03,700 --> 02:24:06,930
from architectures is non-trivial.

02:24:08,165 --> 02:24:11,990
And this is how we ended up with the default.

02:24:14,870 --> 02:24:18,830
But I mean just build whatever you need for ARM

02:24:18,830 --> 02:24:22,190
and if there are more architectures that start to extend,

02:24:22,190 --> 02:24:25,910
we'll look up unifying some of it, if it makes sense.

02:24:28,830 --> 02:24:30,417
- [Presenter] Good, fair enough.

02:24:30,417 --> 02:24:35,417
Question in the back?

02:24:40,161 --> 02:24:44,520
- ARM did define an ACPI table specifically for this,

02:24:45,677 --> 02:24:48,220
the processor, yeah. - TPTC.

02:24:50,822 --> 02:24:54,260
- But by definition, that table is not strictly for ARM.

02:24:54,260 --> 02:24:56,440
It is to describe any CPU topology

02:24:56,440 --> 02:24:59,060
and the cache structures associated with it.

02:24:59,060 --> 02:25:01,340
So I think a lot of the information you need

02:25:01,340 --> 02:25:05,850
is already present, it's just acting on it.

02:25:05,850 --> 02:25:07,470
- When I looked on the ASP aspect,

02:25:07,470 --> 02:25:10,070
you have lots of information on the,

02:25:12,780 --> 02:25:14,650
okay, yeah, so it depends on what kind of information.

02:25:14,650 --> 02:25:15,840
I think most of it for the cache

02:25:15,840 --> 02:25:18,480
is like numbers of sets, numbers of ways.

02:25:18,480 --> 02:25:20,430
I think it's the way you would build

02:25:20,430 --> 02:25:24,040
your hierarchy of your nodes in the PPPT table.

02:25:24,040 --> 02:25:26,520
You could describe something that is more than three nodes.

02:25:26,520 --> 02:25:29,116
- Right and the table's supposed to be able to describe

02:25:29,116 --> 02:25:32,660
what your clusters look like, how wide they are,

02:25:32,660 --> 02:25:34,920
how deep they are, how the caches are

02:25:34,920 --> 02:25:37,330
actually associated with those structures

02:25:37,330 --> 02:25:41,440
and fortunately the way the current topology

02:25:42,510 --> 02:25:46,780
is described in the register, I know we're getting it wrong

02:25:46,780 --> 02:25:48,740
on Thunder, in particular.

02:25:48,740 --> 02:25:51,170
- So on ARM you can forget about the topology.

02:25:51,170 --> 02:25:54,382
- Yeah, right.

02:25:54,382 --> 02:25:58,210
But something to investigate is if there's something

02:25:58,210 --> 02:26:00,600
specifically missing from that table,

02:26:00,600 --> 02:26:04,260
then let's just add it because not only ARM

02:26:04,260 --> 02:26:06,310
is using it but I know other architectures

02:26:06,310 --> 02:26:09,860
are considering it that might not have in the past,

02:26:11,100 --> 02:26:12,980
they used some CPU ID thing.

02:26:14,020 --> 02:26:16,440
And then there's some new up and coming architectures

02:26:16,440 --> 02:26:18,280
that are considering using it as well.

02:26:18,280 --> 02:26:20,939
So we may as well go with the standard, right.

02:26:20,939 --> 02:26:25,939
- [Presenter] Yep.

02:26:27,490 --> 02:26:30,210
- [Man] All this trust in ACPI just makes me sad.

02:26:31,722 --> 02:26:32,940
(everyone laughing) - I picked ACPI because

02:26:32,940 --> 02:26:36,710
it's the only thing we can rely on, I mean we own.

02:26:36,710 --> 02:26:38,180
- [Man] This is such a sad, sad world.

02:26:38,180 --> 02:26:41,050
- I know, I know. (everyone laughing)

02:26:41,050 --> 02:26:43,840
But it's better than having nothing of interest in.

02:26:43,840 --> 02:26:45,390
- [Thomas] Well my condolences.

02:26:47,300 --> 02:26:48,133
- Thank you.

02:26:50,170 --> 02:26:54,250
- Every time we have only to rely on ACPI we are doomed.

02:26:57,304 --> 02:26:58,570
Welcome to the wonderful world.

02:27:00,267 --> 02:27:01,350
- All right, think we can move to the next talk?

02:27:01,350 --> 02:27:02,808
But I don't know how much time we have left,

02:27:02,808 --> 02:27:05,142
but I think I'm close to the end.

02:27:05,142 --> 02:27:06,100
Thanks.

02:27:07,723 --> 02:27:10,890
(audience applauding)

02:27:19,923 --> 02:27:23,518
- [Man] You can take the mic he has,

02:27:23,518 --> 02:27:28,518
the one he left on the chair.

02:27:45,676 --> 02:27:50,676
- Okay this is basically a talk

02:27:53,000 --> 02:27:56,660
that's just an extension of a OSPM talk that I gave

02:27:56,660 --> 02:27:58,760
regarding the TurboSched.

02:28:01,330 --> 02:28:04,230
For the newcomers I will give a short introduction

02:28:04,230 --> 02:28:05,230
that I wanted to do.

02:28:06,200 --> 02:28:07,720
I wanted to do basically task picking

02:28:07,720 --> 02:28:09,670
so that I could have a fewer number of

02:28:09,670 --> 02:28:10,900
core scheduler times.

02:28:12,280 --> 02:28:14,300
If I get less number of core activity

02:28:14,300 --> 02:28:17,120
I can sustain Power budget and then I can sustain

02:28:17,120 --> 02:28:18,950
a more frequency or the type of frequency

02:28:18,950 --> 02:28:20,730
for a longer duration of time.

02:28:20,730 --> 02:28:23,200
So what I basically did is that

02:28:23,200 --> 02:28:25,780
we can not do the task picking for all the tasks

02:28:25,780 --> 02:28:29,000
so I will do task picking for only a Jitter task,

02:28:29,000 --> 02:28:32,820
which a Jitter is like some operating system noise

02:28:32,820 --> 02:28:33,940
or which are unimportant tasks

02:28:33,940 --> 02:28:36,000
which should not be given a high frequency.

02:28:36,000 --> 02:28:40,040
And that can impact on a busier core,

02:28:40,040 --> 02:28:43,070
which is already running on a fewer number of tasks.

02:28:43,070 --> 02:28:45,840
So the first thing is that with a classified task,

02:28:45,840 --> 02:28:47,240
how we classify we can do...

02:28:54,580 --> 02:28:58,070
So how we'll do the classification is

02:28:59,226 --> 02:29:01,040
that I go up this weekend and do any of the classification

02:29:01,040 --> 02:29:03,020
over the weekend automatic Jitter classification,

02:29:03,020 --> 02:29:05,810
which is a difficult thing.

02:29:05,810 --> 02:29:07,780
First thing is label the task classification.

02:29:07,780 --> 02:29:09,580
Once we classify the task as Jitter,

02:29:09,580 --> 02:29:12,530
we will try to find an easier core.

02:29:12,530 --> 02:29:16,240
So really search for a non idle core basically

02:29:16,240 --> 02:29:17,780
across a domain.

02:29:17,780 --> 02:29:20,250
The domain is still tell us which domain we should pick up,

02:29:20,250 --> 02:29:21,850
an LLC domain or a DIE domain,

02:29:21,850 --> 02:29:23,920
which can be different in different architectures.

02:29:23,920 --> 02:29:26,496
So when we try to find a core

02:29:26,496 --> 02:29:30,820
which is some capacity to document this Jitter task

02:29:30,820 --> 02:29:32,420
and which should not be idle.

02:29:32,420 --> 02:29:33,950
So what should be the core capacities

02:29:33,950 --> 02:29:36,250
is a new challenge that I wanted to face.

02:29:37,584 --> 02:29:41,010
And when we tried to pick a task on a normal core,

02:29:41,010 --> 02:29:43,420
so this is what the algorithm will look like.

02:29:43,420 --> 02:29:46,040
When we have a task, let's say 12% of utilization,

02:29:46,040 --> 02:29:49,140
we'll try to find a core which has spare capacity left

02:29:49,140 --> 02:29:50,900
and then after picking a core

02:29:50,900 --> 02:29:54,070
we will try to find an idlest CPU inside a core.

02:29:55,520 --> 02:29:58,300
So this is our overall algorithm

02:29:58,300 --> 02:30:01,550
and I have found that I'm about to get

02:30:01,550 --> 02:30:04,020
a total percentage performance benefit

02:30:04,020 --> 02:30:06,370
on IBM Power line system with attuned workload.

02:30:08,480 --> 02:30:11,330
Currently the LLCs reinforced it.

02:30:15,550 --> 02:30:18,350
There are three challenge that I wanted to discuss here.

02:30:19,190 --> 02:30:22,080
One thing is the core capacity computation.

02:30:22,080 --> 02:30:24,690
How we will define what is the capacity of a core?

02:30:24,690 --> 02:30:26,370
So currently what I'm trying to do

02:30:26,370 --> 02:30:29,540
is I'm trying to find the server capacity

02:30:29,540 --> 02:30:32,380
of each CPU and try to integrate it

02:30:32,380 --> 02:30:34,540
based on some formula.

02:30:34,540 --> 02:30:36,650
So what is the objective for this capacity

02:30:36,650 --> 02:30:38,970
integration is that three objectives is that

02:30:38,970 --> 02:30:42,800
I wanted to find a non idle core from busy core,

02:30:43,850 --> 02:30:45,700
which should not be going to be idle.

02:30:47,411 --> 02:30:50,600
I can select a core which is having 2% of utilization

02:30:50,600 --> 02:30:52,680
but should I pick that core.

02:30:52,680 --> 02:30:54,704
We have some kind of threshold like 12.5%

02:30:54,704 --> 02:30:57,950
but the more utilization if a core has been operating,

02:30:57,950 --> 02:31:01,654
I would try to apply a percentage to task on that core.

02:31:01,654 --> 02:31:03,780
I want to find out whether it has

02:31:03,780 --> 02:31:06,660
enough capacity left to pack a Jitter,

02:31:06,660 --> 02:31:09,160
otherwise it doesn't make sense to worry too much.

02:31:15,380 --> 02:31:17,200
This is what the formula I used.

02:31:17,200 --> 02:31:19,190
I tried to find the capacity of the first CPU

02:31:19,190 --> 02:31:21,090
sibling so it is basically all the

02:31:21,090 --> 02:31:23,780
stars of later that's what I considered.

02:31:23,780 --> 02:31:26,020
Now I want to scale it with the respect to

02:31:26,020 --> 02:31:27,850
the number of online threads in a core.

02:31:27,850 --> 02:31:31,030
So the K factor is a linear factor

02:31:31,030 --> 02:31:35,410
which is having 1.54 at 74, at SMT-8 it has a K core to two.

02:31:35,410 --> 02:31:37,540
So I want that it to be two times the capacity

02:31:37,540 --> 02:31:40,260
of a single thread for SMT-8 to core.

02:31:42,532 --> 02:31:44,300
- [Man] But that's a lie because Power 9

02:31:44,300 --> 02:31:45,950
doesn't have SMT-8.

02:31:45,950 --> 02:31:47,920
- Power 9 doesn't have but the scheduler has

02:31:47,920 --> 02:31:51,349
a view of SMT-8.

02:31:51,349 --> 02:31:54,416
So that is basically fused off two cores.

02:31:54,416 --> 02:31:57,300
- Yes, but how about Power 8 because

02:31:57,300 --> 02:31:59,520
with Power 8 it is an actual SMT-8 and

02:31:59,520 --> 02:32:00,980
its K will not be two.

02:32:02,960 --> 02:32:06,030
- My experiment has shown that it is equal to two.

02:32:06,910 --> 02:32:08,440
It's more or less equal to two.

02:32:08,440 --> 02:32:12,740
I can not say two but maybe like 1.8 or around there.

02:32:12,740 --> 02:32:14,784
- It's a lot less than two.

02:32:14,784 --> 02:32:19,784
- (laughing) Yes, that's the question like

02:32:22,250 --> 02:32:24,970
what should be the core capacity or

02:32:24,970 --> 02:32:26,900
how should we define the core capacity here.

02:32:26,900 --> 02:32:31,900
- So Prahsat, was is a capacity awhile ago

02:32:33,280 --> 02:32:35,220
and I was very glad to see it go

02:32:36,410 --> 02:32:38,270
and here you come again.

02:32:38,270 --> 02:32:39,690
It just makes me sad.

02:32:48,240 --> 02:32:50,510
- I removed that also because the information

02:32:50,510 --> 02:32:54,445
was there for one pass but at runtime

02:32:54,445 --> 02:32:56,931
if you are using the frequency scaling,

02:32:56,931 --> 02:33:00,320
you were no more able to catch this

02:33:00,320 --> 02:33:02,410
because the SD value was not there,

02:33:04,078 --> 02:33:06,090
it was nil by default, so that's why.

02:33:06,090 --> 02:33:08,520
So then if you revert what we have removed

02:33:10,800 --> 02:33:14,500
and then if you revert what we have removed

02:33:14,500 --> 02:33:17,710
you will still miss a part in order to be able

02:33:17,710 --> 02:33:21,900
to get the capacity of all the core.

02:33:21,900 --> 02:33:23,570
- The delivered capacity of a CPU.

02:33:23,570 --> 02:33:24,570
- Yeah.

02:33:24,570 --> 02:33:26,330
But it's still the core.

02:33:29,010 --> 02:33:31,140
- Core capacity, right?

02:33:32,064 --> 02:33:33,330
- Yeah, maybe, yeah.

02:33:33,330 --> 02:33:35,428
- Yes but I wanted to just find

02:33:35,428 --> 02:33:37,840
whether a core is hanging off capacity

02:33:37,840 --> 02:33:39,730
with respect to another core.

02:33:39,730 --> 02:33:42,012
Inside the core I just wondered if we can

02:33:42,012 --> 02:33:44,690
the least busiest CPU or maybe the other CPU.

02:33:47,815 --> 02:33:50,250
- But how did the ASM thing,

02:33:50,250 --> 02:33:52,660
does Power nine still have asymmetric floating?

02:33:53,830 --> 02:33:55,420
- [Presenter] Power seven had, yeah.

02:33:55,420 --> 02:33:58,260
- Oh, Power eight and nine don't have it anymore?

02:33:58,260 --> 02:33:59,290
- [Presenter] No.

02:33:59,290 --> 02:34:00,980
- No asymmetric again.

02:34:00,980 --> 02:34:02,451
- [Presenter] They are symmetric.

02:34:02,451 --> 02:34:03,880
- Okay.

02:34:03,880 --> 02:34:08,084
- [Presenter] Power seven has asymmetricity across cores.

02:34:08,084 --> 02:34:13,084
- Awesome.

02:34:16,630 --> 02:34:21,630
- We can go with SMT capacity because we do

02:34:21,800 --> 02:34:24,890
in load balancing we do SMT capacity calculation

02:34:24,890 --> 02:34:26,050
for calculating whether it's the capacity

02:34:26,050 --> 02:34:28,390
of group and since we just need to

02:34:28,390 --> 02:34:29,960
calculate across the cores.

02:34:49,830 --> 02:34:53,700
The other thing is, as I mentioned,

02:34:53,700 --> 02:34:56,990
we need a classification, whether a task is Jitter.

02:34:58,950 --> 02:35:01,729
I tried to hold projects for many classification.

02:35:01,729 --> 02:35:04,340
A Siebel-based implementation

02:35:04,340 --> 02:35:06,690
and a Cisco-based implementation.

02:35:06,690 --> 02:35:09,920
For the Cisco we can say that a particular task

02:35:09,920 --> 02:35:10,830
is Jitter or not.

02:35:12,400 --> 02:35:15,400
Siebel seems to be a good thing for disk device.

02:35:17,210 --> 02:35:20,150
Cisco is a good way for coding community as I see.

02:35:21,686 --> 02:35:23,900
I think this is a general challenge or latency-niceness

02:35:23,900 --> 02:35:24,733
and you climb first that by adding a Cisco first to market

02:35:24,733 --> 02:35:29,733
while trying to do for cgroup

02:35:30,050 --> 02:35:32,410
what was the procedure is the question

02:35:32,410 --> 02:35:33,660
to the community as well.

02:35:39,850 --> 02:35:43,440
- Right so we held that discussion with

02:35:47,570 --> 02:35:50,500
the other stuff.

02:35:53,960 --> 02:35:56,790
Latency is nice of course but also the stuff you added.

02:35:56,790 --> 02:35:57,867
What's it called again?

02:35:57,867 --> 02:36:00,627
- You climb. - Yeah, you climb.

02:36:00,627 --> 02:36:04,110
And has expressed that previously as well.

02:36:06,710 --> 02:36:10,260
He really wants and I think that makes sense

02:36:10,260 --> 02:36:13,190
to have their task properties first

02:36:14,170 --> 02:36:18,260
and then the Siebel interface for aggregate task things.

02:36:21,950 --> 02:36:23,550
- [Presenter] Yeah, fair enough.

02:36:25,002 --> 02:36:27,759
- So what's not getting tested?

02:36:27,759 --> 02:36:30,380
- Yeah, you don't remember the system, do you?

02:36:33,080 --> 02:36:35,270
- When the master operation override,

02:36:35,270 --> 02:36:36,251
the task that sits on where this is called interface

02:36:36,251 --> 02:36:37,084
and then moves to an different cgroup,

02:36:37,084 --> 02:36:42,084
which has a different value.

02:36:48,266 --> 02:36:49,940
- That one's interesting.

02:36:51,240 --> 02:36:53,990
For nice, for example, it is a combination

02:36:53,990 --> 02:36:55,710
of two so that's a multiplication.

02:36:56,760 --> 02:37:01,760
It's a fast weight times the cgroup weight.

02:37:02,080 --> 02:37:05,870
With you climb, it is an aggregate clamp.

02:37:08,190 --> 02:37:12,250
The cgroup gives you restrictions and

02:37:12,250 --> 02:37:14,540
the task is allowed to move inside of that.

02:37:17,350 --> 02:37:19,330
For this I have no idea.

02:37:19,330 --> 02:37:21,250
I still hate the hole Jittery word.

02:37:25,333 --> 02:37:26,600
- We still haven't to hear about what it does.

02:37:27,910 --> 02:37:29,960
- Yeah, or why he wants this.

02:37:29,960 --> 02:37:32,470
You just want to idle the entire system?

02:37:33,650 --> 02:37:38,650
I remember asking this in Pisa and I forgot again.

02:37:39,300 --> 02:37:42,616
- I basically want a new pass consolidation

02:37:42,616 --> 02:37:44,261
and mainly for each--

02:37:44,261 --> 02:37:47,416
- What is Jitter.

02:37:47,416 --> 02:37:51,030
I know what Jitter is in terms of

02:37:52,100 --> 02:37:56,404
if you look at a periodic timeline or something like that

02:37:56,404 --> 02:38:00,330
but I can't for the hell figure out

02:38:00,330 --> 02:38:03,460
what task, what a Jittering task means.

02:38:06,076 --> 02:38:09,790
- For me the task is a Jitter run,

02:38:09,790 --> 02:38:12,260
it is all sort in the kind of workload.

02:38:12,260 --> 02:38:15,108
And it doesn't matter for me to give it

02:38:15,108 --> 02:38:20,108
a CPU fastest or add more frequency to it.

02:38:20,522 --> 02:38:22,360
I should not wake up an idle core

02:38:22,360 --> 02:38:24,767
for a small kind of workload.

02:38:24,767 --> 02:38:26,950
That kind of workload I would say has a Jitter,

02:38:26,950 --> 02:38:29,401
some kind of operating system noise.

02:38:29,401 --> 02:38:30,988
- Normal people pull up that

02:38:30,988 --> 02:38:33,495
direct background total, right Jim?

02:38:33,495 --> 02:38:36,710
(several people talking at one time)

02:38:36,710 --> 02:38:38,880
- I think he's talking about really counting

02:38:38,880 --> 02:38:40,710
running threads, not just background

02:38:40,710 --> 02:38:42,140
because you can have a background that's running

02:38:42,140 --> 02:38:44,030
for a long time and I don't think you'd want a background.

02:38:44,030 --> 02:38:46,740
- But then call it short-running background threads

02:38:46,740 --> 02:38:49,327
because it's confusing the hell out of me.

02:38:49,327 --> 02:38:51,680
- It's actually one in the short-running thread

02:38:51,680 --> 02:38:54,680
because it doesn't want the keep us in the CPU.

02:38:54,680 --> 02:38:57,000
- And a short-running thread which doesn't matter

02:38:57,000 --> 02:38:58,000
which is background.

02:38:59,200 --> 02:39:01,050
- And small is what he wants.

02:39:01,050 --> 02:39:02,780
- Yeah, short-running background task.

02:39:02,780 --> 02:39:06,120
So that describes it well because that's what it is.

02:39:06,120 --> 02:39:08,860
You don't care when it runs or where it runs,

02:39:08,860 --> 02:39:11,280
you just have to run it occasionally.

02:39:11,280 --> 02:39:15,170
And one of the properties, you know it's run short,

02:39:16,950 --> 02:39:20,460
so by some definition of short.

02:39:20,460 --> 02:39:23,550
And if you do that and give it an interface

02:39:23,550 --> 02:39:27,200
which lets it claim I'm a short-running,

02:39:28,300 --> 02:39:32,480
I don't care task, then actually we should look at it,

02:39:32,480 --> 02:39:35,940
if it's really short-running, and clap it on the finger

02:39:35,940 --> 02:39:40,433
if it's not because then you create other problems.

02:39:40,433 --> 02:39:41,750
- Give it some guilt.

02:39:41,750 --> 02:39:43,380
- [Presenter] Yeah, being short doesn't mean

02:39:43,380 --> 02:39:44,900
it's unimportant.

02:39:44,900 --> 02:39:47,769
By short-run it's sometimes a very necessity.

02:39:47,769 --> 02:39:51,420
- What is a background?

02:39:51,420 --> 02:39:54,420
- If your short-running task is important

02:39:54,420 --> 02:39:57,540
then don't call it a short-running background task

02:39:57,540 --> 02:40:00,520
because the background task is by definition not important.

02:40:00,520 --> 02:40:01,878
- Right, that's another problem.

02:40:01,878 --> 02:40:06,878
Another problem is where you classified,

02:40:07,100 --> 02:40:09,870
it looks like you have manual classification.

02:40:11,230 --> 02:40:13,800
So what if you classify it as a short background

02:40:13,800 --> 02:40:16,170
and it's ran it actually runs for a long time?

02:40:16,170 --> 02:40:18,770
- Yeah, we should duck it over the head and kill it.

02:40:20,118 --> 02:40:21,975
(everyone laughing)

02:40:21,975 --> 02:40:24,867
- Okay, there goes your microphone.

02:40:24,867 --> 02:40:26,953
- Can you move to a program if the task

02:40:26,953 --> 02:40:29,453
is not the short-running load?

02:40:30,670 --> 02:40:32,780
- If task is not short-running then what?

02:40:32,780 --> 02:40:35,896
- Why are you only specifying short

02:40:35,896 --> 02:40:40,896
background running tasks, not only background tasks?

02:40:40,989 --> 02:40:44,270
- We can put a short-running task on a core

02:40:46,060 --> 02:40:48,870
with the fact that it won't hit the performance

02:40:48,870 --> 02:40:50,820
of other task running in the same core.

02:40:52,500 --> 02:40:54,170
- Yes, from what I remember at OSPM

02:40:54,170 --> 02:40:57,520
you had a secondly objective that was

02:40:57,520 --> 02:41:00,290
to stay in the program mode for as long as you could.

02:41:00,290 --> 02:41:01,858
- Correct. - And that's why you wanted

02:41:01,858 --> 02:41:04,890
to consolidate onto a single core

02:41:04,890 --> 02:41:09,200
so that the other core on the CPU can idle

02:41:09,200 --> 02:41:12,379
and that's how you get a bigger boost.

02:41:12,379 --> 02:41:14,010
(men speaking off mic)

02:41:14,010 --> 02:41:19,010
- [Man] Can you have a microphone?

02:41:21,780 --> 02:41:24,080
- I just wanted to say something.

02:41:25,360 --> 02:41:27,510
Why do we need to attack them?

02:41:27,510 --> 02:41:29,960
Why can't you just look at the pell value.

02:41:29,960 --> 02:41:32,860
If you know what your turbo schedule is,

02:41:32,860 --> 02:41:35,220
I think you can work out for how long the task

02:41:35,220 --> 02:41:37,680
should maximum running for it to make sense.

02:41:37,680 --> 02:41:40,560
Or how sure that you'd be running to make sense

02:41:40,560 --> 02:41:41,990
to pack it.

02:41:41,990 --> 02:41:44,490
And that should be somewhat proportional to

02:41:44,490 --> 02:41:45,780
the pell value of the task.

02:41:45,780 --> 02:41:47,470
You could just put in the filter saying

02:41:47,470 --> 02:41:50,450
that if the pell utilization is less than X

02:41:50,450 --> 02:41:55,450
then you just pack it, if it's bigger then you don't.

02:41:57,420 --> 02:41:58,940
- Except those people that have

02:41:58,940 --> 02:42:03,253
very short-running services but they do

02:42:03,253 --> 02:42:05,220
care about latency.

02:42:05,220 --> 02:42:07,020
So they don't want queuing.

02:42:07,020 --> 02:42:10,160
- Yeah, try running live tasks.

02:42:10,160 --> 02:42:15,160
- So you don't just look at the utilization then.

02:42:15,320 --> 02:42:18,730
- So I heard that in rerun.

02:42:18,730 --> 02:42:21,170
Don't let this affect your initial classification.

02:42:21,170 --> 02:42:22,003
Initially--

02:42:23,750 --> 02:42:25,860
- There is another presentation in Power

02:42:25,860 --> 02:42:27,300
talking about the latency-nice.

02:42:27,300 --> 02:42:29,630
So if we use utilization to identify

02:42:29,630 --> 02:42:31,600
a short-running task and we use

02:42:31,600 --> 02:42:34,636
the latency-nice or whatever we call it,

02:42:34,636 --> 02:42:36,950
then and that case using those two matrices

02:42:36,950 --> 02:42:38,860
you can identify those tasks and move them

02:42:38,860 --> 02:42:41,060
without having to get anything else,

02:42:41,060 --> 02:42:42,460
apart from the latency-nice.

02:42:44,510 --> 02:42:46,670
- But if the task is so short-running,

02:42:46,670 --> 02:42:48,490
waking up an additional core that might be

02:42:48,490 --> 02:42:51,000
in deep sleep to actually enable your turbo state

02:42:51,000 --> 02:42:53,355
might take longer than just running the task

02:42:53,355 --> 02:42:54,924
in the CPU that's already busy.

02:42:54,924 --> 02:42:58,174
(man speaking off mic)

02:42:59,580 --> 02:43:02,290
- [Presenter] I don't want to wake up an idle core,

02:43:02,290 --> 02:43:03,570
another CPU, but--

02:43:03,570 --> 02:43:05,650
- Exactly, so what is the harm

02:43:05,650 --> 02:43:07,530
in always packing them?

02:43:07,530 --> 02:43:09,910
Peter was saying that for some type

02:43:09,910 --> 02:43:12,750
you don't want to pack them for latency reasons

02:43:12,750 --> 02:43:15,490
but is it always better to wake up another core

02:43:15,490 --> 02:43:16,590
if it's in deep sleep?

02:43:17,611 --> 02:43:19,250
- It might not be deep sleep.

02:43:19,250 --> 02:43:20,510
- Well, if it's not in deep sleep

02:43:20,510 --> 02:43:24,281
then you're probably not in Turbo anyway.

02:43:24,281 --> 02:43:26,640
- The other thing to keep in mind

02:43:26,640 --> 02:43:28,740
is many of these workloads also want

02:43:28,740 --> 02:43:31,200
consistent latencies, they don't want very high

02:43:31,200 --> 02:43:32,033
tail latencies.

02:43:34,405 --> 02:43:37,390
Yeah, that might not work.

02:43:40,735 --> 02:43:43,290
- Has somebody tried,

02:43:43,290 --> 02:43:45,940
what I did is a pipe on the idle cores,

02:43:45,940 --> 02:43:47,890
not on the busy cores.

02:43:47,890 --> 02:43:52,890
So still deserve an option but more picky on the cores.

02:43:52,990 --> 02:43:56,060
- So what is the goal in this?

02:43:56,060 --> 02:43:58,153
It won't take just the number of idle CPUs.

02:43:58,153 --> 02:44:00,994
(man speaking off mic)

02:44:00,994 --> 02:44:04,737
- You can just say interleaf.

02:44:04,737 --> 02:44:06,690
Yeah, you can interleaf some packets

02:44:06,690 --> 02:44:08,560
because you're still trying to find the idle core.

02:44:08,560 --> 02:44:11,329
But you can try smaller run testing,

02:44:11,329 --> 02:44:14,330
the cross bytes they don't stack on each other.

02:44:14,330 --> 02:44:18,970
- Can you repeat exactly what you said?

02:44:18,970 --> 02:44:21,210
- If you try to pack and deny idle core,

02:44:21,210 --> 02:44:23,570
it's gonna reduce the number of idle cores

02:44:23,570 --> 02:44:25,120
and his goal seems to be to increase

02:44:25,120 --> 02:44:26,370
the number of idle cores.

02:44:27,970 --> 02:44:31,970
- It's like still a kind of a latency

02:44:31,970 --> 02:44:34,890
but your package is a bit tighter.

02:44:34,890 --> 02:44:37,210
So it's not like piking into a piece of core

02:44:37,210 --> 02:44:39,327
and having to wait in the line queue.

02:44:39,327 --> 02:44:41,260
- So when you say idling you don't really mean idling.

02:44:41,260 --> 02:44:42,900
You mean is low utilized CPU.

02:44:43,960 --> 02:44:45,790
Oh, if you do the hour's utilization

02:44:45,790 --> 02:44:48,040
it can be a higher and you'd have to pump them

02:44:48,040 --> 02:44:49,940
into a higher utilized CPU.

02:44:49,940 --> 02:44:51,550
On the cuff selection.

02:44:54,936 --> 02:44:56,662
- So they're kind of dynamic.

02:44:56,662 --> 02:44:58,278
- I want to respond to that.

02:44:58,278 --> 02:44:59,250
I don't think that's what he wants.

02:45:00,180 --> 02:45:01,013
- No, it's not.

02:45:01,013 --> 02:45:03,570
That's just an alternative.

02:45:03,570 --> 02:45:06,070
- This is quite similar to the ES,

02:45:06,070 --> 02:45:08,220
where we tried to pack some tasks

02:45:08,220 --> 02:45:09,280
from the NM CPU. - Yes.

02:45:09,280 --> 02:45:11,763
- Have you tried the CRES in Promado?

02:45:11,763 --> 02:45:14,290
- Yeah, I have tried it out. - Okay.

02:45:14,290 --> 02:45:17,920
- But it tries to do it for a single core

02:45:17,920 --> 02:45:19,750
or a single frequency domain, that

02:45:19,750 --> 02:45:22,340
Power data domain has always used.

02:45:22,340 --> 02:45:24,390
But I want there to be the least number

02:45:24,390 --> 02:45:26,890
of cores to be active as possible

02:45:26,890 --> 02:45:27,920
and not exactly one.

02:45:28,900 --> 02:45:30,800
So I just wanted because the number of

02:45:32,650 --> 02:45:34,910
busier cores, basically.

02:45:34,910 --> 02:45:37,158
I want to talk it out more than the number of cores.

02:45:37,158 --> 02:45:39,128
- [Man] Maybe they can be extended.

02:45:39,128 --> 02:45:42,860
- Yeah, in OSPM if you know some kind of framework,

02:45:42,860 --> 02:45:43,730
like ES.

02:45:44,580 --> 02:45:47,330
First otherwise what seems to be difficult as of now

02:45:49,170 --> 02:45:52,460
because its complexities are very high for servers

02:45:52,460 --> 02:45:54,090
when the number of CPUs increase.

02:45:54,090 --> 02:45:57,070
I think the ES is not meant for more than eight CPUs.

02:45:58,367 --> 02:46:01,120
- Lets continue this discussion offline

02:46:01,120 --> 02:46:02,500
because you're out of time.

02:46:02,500 --> 02:46:04,443
- [Man] Yeah, so one more thing,

02:46:09,743 --> 02:46:11,790
you need the capacity to find out

02:46:11,790 --> 02:46:15,370
when your goal is full, right? - Yes.

02:46:15,370 --> 02:46:17,970
- Could we not use the exact same heuristic

02:46:17,970 --> 02:46:19,440
we get out of load balancing because

02:46:19,440 --> 02:46:23,670
we just 'til a presentation ago,

02:46:23,670 --> 02:46:25,820
had that same question.

02:46:25,820 --> 02:46:26,850
When are we full?

02:46:31,022 --> 02:46:34,690
(Thomas speaking off mic).

02:46:34,690 --> 02:46:35,740
Less for some people.

02:46:37,690 --> 02:46:40,660
- When the core is full I will say that,

02:46:41,870 --> 02:46:46,870
let's say for 78 or two of the CPUs are full

02:46:46,910 --> 02:46:49,550
at a 74, 1.5 times the CPUs are full.

02:46:50,640 --> 02:46:53,430
Then I want to bail out or I don't

02:46:53,430 --> 02:46:56,600
want to pick that core, something like that because--

02:46:56,600 --> 02:46:59,200
- Yeah, but that should be more or less

02:46:59,200 --> 02:47:00,880
the same point where we switch

02:47:00,880 --> 02:47:03,600
load balancing from utilization to overload.

02:47:04,540 --> 02:47:05,490
- Yeah, okay.

02:47:11,663 --> 02:47:14,045
- We have to switch (speaking off mic).

02:47:14,045 --> 02:47:16,426
- Okay, we'll talk offline.

02:47:16,426 --> 02:47:17,259
- Yeah.

02:47:19,940 --> 02:47:20,773
- Thank you.

02:47:20,773 --> 02:47:23,814
(audience applauding)

02:47:39,280 --> 02:47:40,113
- Hello.

02:47:42,302 --> 02:47:43,300
Is it working? - Okay.

02:47:45,837 --> 02:47:48,300
I'll talk about task latency-nice.

02:47:49,680 --> 02:47:53,160
This is still in very in essence stage work

02:47:53,160 --> 02:47:54,990
in terms of ideas and implementation

02:47:54,990 --> 02:47:58,200
so most likely we'll have more questions than answers.

02:48:00,832 --> 02:48:03,471
I talked about the wake up scalability problem

02:48:03,471 --> 02:48:08,250
last LPC and showed we had a heuristic.

02:48:09,570 --> 02:48:13,560
So the problem, there was algoristic could solve it.

02:48:13,560 --> 02:48:17,120
So the problem I talked about was workloads

02:48:17,120 --> 02:48:19,870
where we had very short running threads

02:48:19,870 --> 02:48:21,370
maybe like a few microseconds

02:48:21,370 --> 02:48:25,980
or we just hitting the availability in microseconds.

02:48:25,980 --> 02:48:27,550
For example OLTP workloads.

02:48:29,000 --> 02:48:32,910
The wakeup part and select idle CPU

02:48:32,910 --> 02:48:34,900
it can end up searching the entire

02:48:34,900 --> 02:48:38,380
last level cache, which is getting bigger.

02:48:38,380 --> 02:48:41,570
So we needed a way to control such costs

02:48:41,570 --> 02:48:44,320
because for short-running treads,

02:48:44,320 --> 02:48:48,220
it didn't make sense to spend too much time searching.

02:48:50,190 --> 02:48:52,650
Select idle CPU was part of the problem.

02:48:54,500 --> 02:48:56,990
The other issue was there is also a select idle

02:48:56,990 --> 02:49:00,190
core that is also a bottleneck in such scenarios.

02:49:01,660 --> 02:49:02,980
So how to fix that?

02:49:07,670 --> 02:49:12,120
In OSPM this year, we had more brainstorming

02:49:12,120 --> 02:49:15,978
and we kind of, there is a consensus that

02:49:15,978 --> 02:49:20,310
we can solve it by task latency-nice.

02:49:21,650 --> 02:49:26,580
There's a new property that we can introduce.

02:49:26,580 --> 02:49:29,880
So it is like nice but for latency.

02:49:29,880 --> 02:49:32,250
So this will give us a mechanism

02:49:34,340 --> 02:49:38,480
to control the property but then in the

02:49:38,480 --> 02:49:41,400
end of the route, we can map this mechanism

02:49:41,400 --> 02:49:45,570
to multiple policies and once that policy

02:49:45,570 --> 02:49:50,290
can be to control such latency in

02:49:50,290 --> 02:49:51,320
the select idle CPU.

02:49:52,610 --> 02:49:55,760
It can also map to other latency-related

02:49:55,760 --> 02:49:58,710
scheduling behaviors, for example, preemption.

02:50:01,630 --> 02:50:05,840
We also had some other behaviors that can be mapped

02:50:05,840 --> 02:50:07,990
and once I posted the initial pass

02:50:07,990 --> 02:50:11,090
there were a lot of discussions and solutions

02:50:11,090 --> 02:50:12,240
that I'll get to later.

02:50:14,030 --> 02:50:16,950
So implementation wise also it generated

02:50:17,800 --> 02:50:21,700
a lot of feedback so I have the same problems

02:50:21,700 --> 02:50:24,090
as the past mentioned like

02:50:24,090 --> 02:50:26,580
what is the interface to choose.

02:50:26,580 --> 02:50:30,970
We can have a systemwide interface like

02:50:30,970 --> 02:50:35,080
Sys, Proxy, Skylar, Novel, have all the schedule active

02:50:35,080 --> 02:50:37,090
in others right now.

02:50:38,000 --> 02:50:41,090
We can have a per task schedule say that you built.

02:50:42,560 --> 02:50:45,292
And definitely we can have a cgroup interface

02:50:45,292 --> 02:50:50,292
by having a new file CPU latency-nice

02:50:50,650 --> 02:50:55,050
that should obey all the tasks in the cgroup.

02:50:56,460 --> 02:50:59,130
There are also a lot of discussion about

02:50:59,130 --> 02:51:02,810
what should be the valued range and the default value.

02:51:03,910 --> 02:51:06,330
Peter seems to think that it should still

02:51:06,330 --> 02:51:09,780
stick to the existing nice range that we have

02:51:10,850 --> 02:51:11,830
and the default--

02:51:12,670 --> 02:51:15,353
- So I think there has been discussion.

02:51:15,353 --> 02:51:16,960
- In the middle, in the middle.

02:51:16,960 --> 02:51:18,550
- We could change the name and no longer

02:51:18,550 --> 02:51:20,500
have to worry about the minus 20 to 19

02:51:20,500 --> 02:51:23,100
because I suspect that range might not be enough.

02:51:23,100 --> 02:51:25,138
- So my argument was that if you call it nice,

02:51:25,138 --> 02:51:27,320
it had better behave like nice,

02:51:27,320 --> 02:51:29,590
otherwise you're just confusing people.

02:51:29,590 --> 02:51:30,423
- I will.

02:51:31,490 --> 02:51:33,580
- So you're on CS101 right now.

02:51:33,580 --> 02:51:35,530
How do you name things?

02:51:35,530 --> 02:51:36,487
- Latency-nappy.

02:51:36,487 --> 02:51:39,860
(everybody laughing)

02:51:39,860 --> 02:51:41,460
- That's a better name actually.

02:51:42,910 --> 02:51:46,360
Okay, but one thing I encountered that

02:51:46,360 --> 02:51:48,460
we probably don't need too big of a range.

02:51:49,790 --> 02:51:53,700
I think that trick that zero to one, zero through four,

02:51:54,600 --> 02:51:56,400
that those probably are not working.

02:51:57,773 --> 02:51:59,160
And the other thing is what is the default?

02:51:59,160 --> 02:52:00,650
Should it be right in the middle

02:52:00,650 --> 02:52:03,703
or it should be in the minimum or the max?

02:52:03,703 --> 02:52:05,188
- [Man] Right in the middle.

02:52:05,188 --> 02:52:07,771
(crosstalking)

02:52:12,390 --> 02:52:14,360
- I'm somewhat confused about what

02:52:14,360 --> 02:52:17,140
you're trying to accomplish because

02:52:17,140 --> 02:52:21,450
the idle core search algorithm stops

02:52:21,450 --> 02:52:22,900
when it's found in idle core,

02:52:24,051 --> 02:52:25,380
which means that if you--

02:52:25,380 --> 02:52:26,640
- Yeah but it can throw.

02:52:26,640 --> 02:52:28,440
- Yeah, it can take a longer time,

02:52:28,440 --> 02:52:32,630
but if you end up missing an idle core

02:52:32,630 --> 02:52:34,680
because you stopped searching,

02:52:34,680 --> 02:52:37,060
your latency will be much higher.

02:52:37,060 --> 02:52:40,500
- No actually in this case it might help

02:52:40,500 --> 02:52:43,270
because you have wasted here a lot of time.

02:52:43,270 --> 02:52:45,180
So it hurts more than it help overall.

02:52:46,210 --> 02:52:51,210
- If this is for workloads that have

02:52:51,510 --> 02:52:55,160
very short time slices, is there something

02:52:55,160 --> 02:52:58,980
that you could figure out automatically,

02:52:58,980 --> 02:53:03,673
they this task here reschedules 10 times per millisecond,

02:53:09,550 --> 02:53:11,000
let's just sit here and wait.

02:53:12,180 --> 02:53:15,760
- Or something like the average runtime of the tasks

02:53:19,060 --> 02:53:20,880
probably can also be used.

02:53:20,880 --> 02:53:23,390
So yeah online learning can be used

02:53:23,390 --> 02:53:26,500
but this is still up for discussion.

02:53:26,500 --> 02:53:29,490
Should it be stated by user errors through the kernel

02:53:29,490 --> 02:53:31,720
created automatically and do the right thing?

02:53:31,720 --> 02:53:33,920
- [Man] I think everybody likes low latency.

02:53:35,050 --> 02:53:40,050
- Yeah, but I've heard some predictable latency

02:53:42,290 --> 02:53:43,440
more than lower latency.

02:53:43,440 --> 02:53:44,440
- Yeah, that's fair.

02:53:46,710 --> 02:53:51,710
- And the idle core search, I have no idea

02:53:51,840 --> 02:53:53,350
whether that can be made faster

02:53:53,350 --> 02:53:56,220
because faster in this case is very tricky.

02:53:56,220 --> 02:54:00,090
It also includes touching as few cache lines as you can.

02:54:00,090 --> 02:54:04,014
So probably many items can be met

02:54:04,014 --> 02:54:06,660
through this but I will trust

02:54:06,660 --> 02:54:09,903
that more stuff will fail once you try out

02:54:09,903 --> 02:54:12,130
actually all of the work latency products

02:54:12,130 --> 02:54:14,070
that doesn't work as you expected.

02:54:15,840 --> 02:54:18,060
There should be some way to disable

02:54:18,060 --> 02:54:19,830
the select idle core.

02:54:22,072 --> 02:54:24,772
At least let us have the escape feature to disable it.

02:54:25,910 --> 02:54:29,620
And the other question just before I get to,

02:54:29,620 --> 02:54:32,360
the other question in my mind is via

02:54:32,360 --> 02:54:34,680
this latency-nice property,

02:54:34,680 --> 02:54:38,440
should we be controlling or changing any of

02:54:39,730 --> 02:54:42,310
first order behavior versus second order behaviors.

02:54:42,310 --> 02:54:44,900
And by that I mean so in one of the discussions

02:54:44,900 --> 02:54:47,142
in the email Patrick suggested that

02:54:47,142 --> 02:54:49,960
the select idle core can also be disabled

02:54:49,960 --> 02:54:54,500
if the latency-nice value is below a particular threshold.

02:54:54,500 --> 02:54:57,000
But in that case we are completely changing

02:54:57,000 --> 02:54:59,810
that why negative, behavior of the scheduler

02:54:59,810 --> 02:55:03,300
rather than moderating our secondary behavior,

02:55:03,300 --> 02:55:05,730
which is more or less.

02:55:05,730 --> 02:55:07,280
Or as in this case, like we are saying,

02:55:07,280 --> 02:55:09,210
we're not search for idle cores at all,

02:55:09,210 --> 02:55:11,630
which is a qualitative change and this could

02:55:14,542 --> 02:55:19,360
So in schedulable, tunable, controlled file starter,

02:55:19,360 --> 02:55:20,610
we are doing changes.

02:55:21,480 --> 02:55:22,380
So I do something similar.

02:55:22,380 --> 02:55:25,460
I think Paul in OSPM, we he had a lot

02:55:25,460 --> 02:55:26,750
of discussion about this.

02:55:28,926 --> 02:55:32,180
What does that tunable meaning?

02:55:32,180 --> 02:55:33,720
There's a lot of back and forth.

02:55:34,700 --> 02:55:37,720
So I have similar questions also now.

02:55:40,178 --> 02:55:43,920
- It seems like you are dealing with a tradeoff.

02:55:43,920 --> 02:55:48,050
When searching the cores, searching for an idle core

02:55:48,980 --> 02:55:52,390
takes a large fraction of your runtime

02:55:52,390 --> 02:55:55,200
of the task that you are looking around for

02:55:56,570 --> 02:56:00,420
and the time until the current tasks

02:56:00,420 --> 02:56:04,540
of one of the CPUs that you want is going away,

02:56:04,540 --> 02:56:06,220
is very short as well.

02:56:06,220 --> 02:56:08,110
- Yeah, it's like-- - That seems like

02:56:08,110 --> 02:56:10,550
they're also old things that you could measure

02:56:10,550 --> 02:56:12,890
and automatically figure out a threshold

02:56:12,890 --> 02:56:14,990
that works for the different programs.

02:56:14,990 --> 02:56:19,990
- Yeah, I have sense that kind of online learning

02:56:21,690 --> 02:56:22,523
has had,.

02:56:24,980 --> 02:56:26,340
I used to work in Solaris before

02:56:26,340 --> 02:56:29,770
and we had some teacher like that called transient trader,

02:56:29,770 --> 02:56:31,780
which used to try to find out

02:56:31,780 --> 02:56:33,090
if their trade is transient or not,

02:56:33,090 --> 02:56:34,290
like vegetating tasks.

02:56:36,040 --> 02:56:38,670
We had a major problem where the learning

02:56:38,670 --> 02:56:40,538
was not exactly correct and

02:56:40,538 --> 02:56:43,486
it was falsely classifying certain threads.

02:56:43,486 --> 02:56:45,410
And it became an ongoing problem

02:56:45,410 --> 02:56:47,090
and we could never really fix it.

02:56:47,090 --> 02:56:49,361
So that's a risk.

02:56:49,361 --> 02:56:52,540
But yeah, definitely in 30 we can explore that.

02:56:53,700 --> 02:56:56,100
- There are also other cases where

02:56:56,100 --> 02:56:58,728
the user space is informed and knows

02:56:58,728 --> 02:57:02,549
when they all have a tasking the system change

02:57:02,549 --> 02:57:05,620
and you can feed this information

02:57:05,620 --> 02:57:06,453
to the share drive.

02:57:06,453 --> 02:57:08,200
For example, changing this value

02:57:08,200 --> 02:57:11,600
and changing the way you actually manage a task

02:57:11,600 --> 02:57:13,120
at wakeup time.

02:57:13,120 --> 02:57:14,670
- [Man] Like the Android background.

02:57:14,670 --> 02:57:15,503
- Yeah, exactly.

02:57:15,503 --> 02:57:18,510
So one of the ideas was we can try to use

02:57:18,510 --> 02:57:21,130
this mechanism for example to wake up a task

02:57:21,130 --> 02:57:22,080
in a core part or another,

02:57:22,080 --> 02:57:26,670
depending on what all is the specific goal of that task

02:57:26,670 --> 02:57:28,220
in a certain moment in Android.

02:57:31,319 --> 02:57:33,870
- Kind of related, I do think we need

02:57:33,870 --> 02:57:36,940
a better mechanism for a background task,

02:57:36,940 --> 02:57:37,773
a batch task.

02:57:39,020 --> 02:57:41,560
But I only really understand how to do that

02:57:41,560 --> 02:57:44,470
as one background priority.

02:57:44,470 --> 02:57:46,410
I don't understand what the other values

02:57:46,410 --> 02:57:50,880
between one and 18 would mean for a value like this.

02:57:53,040 --> 02:57:54,512
- Initially I thought it was like

02:57:54,512 --> 02:57:57,570
at least for the searching cost,

02:57:57,570 --> 02:58:01,530
we can map it to how much to search that relation.

02:58:01,530 --> 02:58:02,920
- [Man] What does that mean?

02:58:02,920 --> 02:58:05,180
- How many CPUs let's say manage to

02:58:05,180 --> 02:58:08,928
only search 1% of the LLC domain,

02:58:08,928 --> 02:58:11,920
map it to some percentage of

02:58:11,920 --> 02:58:14,030
the total LLC domain searching costs.

02:58:17,860 --> 02:58:22,860
- Yeah, I don't think, I don't think that makes sense.

02:58:25,050 --> 02:58:27,250
I don't think the numbers are in your favor.

02:58:29,043 --> 02:58:32,810
- One possible usage is I'm not taking about the range,

02:58:32,810 --> 02:58:35,100
which has to be defined, but when we do

02:58:35,100 --> 02:58:37,790
vruntime normalization for a task at wakeup time,

02:58:37,790 --> 02:58:39,880
depending on the relative value,

02:58:39,880 --> 02:58:42,840
we can decide if we want to give more

02:58:42,840 --> 02:58:44,860
priority to a task or another in terms

02:58:44,860 --> 02:58:47,440
to avoid the waking up task to preempt

02:58:47,440 --> 02:58:48,820
a currently running task.

02:58:48,820 --> 02:58:49,680
- I agree with that.

02:58:49,680 --> 02:58:51,130
- So some kind of bench ads.

02:58:51,130 --> 02:58:52,910
- I do think we need for example

02:58:52,910 --> 02:58:55,350
a better background task mechanism.

02:58:55,350 --> 02:58:59,360
But I think the better way to have the discussion

02:58:59,360 --> 02:59:00,910
is to go in the opposite direction

02:59:00,910 --> 02:59:04,820
and say what other classes you want to have.

02:59:04,820 --> 02:59:07,680
Background task is an example of a class.

02:59:08,550 --> 02:59:09,910
Are there others?

02:59:09,910 --> 02:59:12,310
Rather than trying to say we're gonna have this

02:59:12,310 --> 02:59:15,410
generic 40 value range--

02:59:15,410 --> 02:59:18,950
- Yes, for example, both from between background

02:59:18,950 --> 02:59:19,810
or if it's a system task or

02:59:19,810 --> 02:59:21,600
is it an application task.

02:59:21,600 --> 02:59:23,800
So there is a little bit of variability.

02:59:23,800 --> 02:59:25,640
So the idea is just to come up with a mechanism

02:59:25,640 --> 02:59:26,780
which is generic to--

02:59:28,370 --> 02:59:30,000
- I'm saying before you come up with a mechanism,

02:59:30,000 --> 02:59:32,660
count the classes and then figure out

02:59:32,660 --> 02:59:34,130
how your mechanism maps to those classes.

02:59:34,130 --> 02:59:36,710
- So the work profile where you view

02:59:36,710 --> 02:59:37,543
the final profile.

02:59:37,543 --> 02:59:39,220
- Just a human decomposition of what's

02:59:39,220 --> 02:59:40,980
the problem you're trying to solve

02:59:40,980 --> 02:59:43,730
so that you can actually map that to an API rather than--

02:59:43,730 --> 02:59:45,510
- But you are assuming that there are

02:59:45,510 --> 02:59:46,610
four discrete classes.

02:59:48,307 --> 02:59:51,807
- Guys, time's up so we need to get going.

02:59:52,834 --> 02:59:56,013
And AB is officially kind of over now.

02:59:56,013 --> 02:59:59,410
Just a few last minute things they wanted to say.

03:00:00,860 --> 03:00:03,390
- Before I hand the mic to Steve,

03:00:03,390 --> 03:00:05,090
thank you very much for attending.

03:00:06,280 --> 03:00:07,113
Thank you all. (audience applauding)

03:00:07,113 --> 03:00:08,390
Thank you to all our speakers.

03:00:09,570 --> 03:00:12,260
The discussions are not over yet.

03:00:12,260 --> 03:00:15,190
Keep going on talking about it in the hallway.

03:00:15,190 --> 03:00:17,780
BOFs are possible and of course the mailing list.

03:00:17,780 --> 03:00:19,592
- [Man] There's also the Hack-a-thon room

03:00:19,592 --> 03:00:22,330
which is currently empty a lot of the time

03:00:22,330 --> 03:00:24,450
and useful for such groups.

03:00:24,450 --> 03:00:26,730
- So for those who did not hear Rick,

03:00:26,730 --> 03:00:30,000
there's a Hack-a-thon room which is empty most of the time

03:00:30,000 --> 03:00:32,580
and maybe that's a place to go have more discussions

03:00:32,580 --> 03:00:34,837
and hack some code out.

03:00:34,837 --> 03:00:37,410
- Okay, so you don't have to film this or anything

03:00:37,410 --> 03:00:40,240
but just to let you guys know, reception is downstairs.

03:00:40,240 --> 03:00:41,960
It starts at seven to nine o'clock.

03:00:41,960 --> 03:00:44,640
It's hors d'oeuvres, drinks, there probably

03:00:44,640 --> 03:00:46,380
may not be enough food for you for dinner but

03:00:46,380 --> 03:00:48,330
there's a lot of restaurants open late.

03:00:49,420 --> 03:00:51,880
And also everyone should have received a link

03:00:51,880 --> 03:00:54,230
about voting for the TAB elections.

03:00:54,230 --> 03:00:56,580
Everyone's got that, please vote.

03:00:56,580 --> 03:00:58,566
By the way, I'm running, Steve Rostead, so...

03:00:58,566 --> 03:01:01,081
(everyone laughing)

03:01:01,081 --> 03:01:03,705
Hey, I got the mic, I might as well use it.

03:01:03,705 --> 03:01:05,420
But if you want to talk to me more about the TAB,

03:01:05,420 --> 03:01:06,253
feel free.

03:01:06,253 --> 03:01:08,341
Other than that, I'll be in the reception area.

03:01:08,341 --> 03:01:11,317
So thank you very much and enjoy yourselves.

03:01:11,317 --> 03:01:13,808

YouTube URL: https://www.youtube.com/watch?v=1ZmUPouac-w


