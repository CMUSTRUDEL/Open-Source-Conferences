Title: LPC2019 - Improving Route Scalability with Nexthop Objects
Publication date: 2019-11-18
Playlist: LPC2019 - Networking Summit Track
Description: 
	Improving Route Scalability with Nexthop Objects

Speaker
 David Ahern

Description
Route entries in a FIB tend to be very redundant with respect to nexthop configuration with many routes using the same gateway, device and potentially encapsulations such as MPLS. The legacy API for inserting routes into the kernel requires the nexthop data to be included with each route specification leading to duplicate processing verifying the nexthop data, an effect that is magnified as the number of paths in the route increases (e.g., ECMP).

A new API was recently committed to the kernel for managing nexthops as separate objects from routes. The nexthop API allows nexthops to be created first and then routes can be added referencing the nexthop object. This API allows routes to be managed with less overhead (e.g., dramatically reducing the time to insert routes) and enables new capabilities such as atomically updating a nexthop configuration without touching the route entries using it.

This talk will discuss the nexthop feature touching on the kernel side implementation, reviewing the userspace API and what to expect for notifications, performance improvements and potential follow on features. While the nexthop API is motivated by Linux as a NOS, it is useful for other networking deployments as well such as routing on the host and XDP.
Captions: 
	00:00:00,620 --> 00:00:02,070
- All right, hello, everyone.

00:00:03,070 --> 00:00:04,230
My name's David Ahern,

00:00:04,230 --> 00:00:06,770
and I'm gonna talk about the Linux routing API,

00:00:06,770 --> 00:00:09,110
and specifically some work that was done

00:00:09,110 --> 00:00:10,870
over the past couple of years,

00:00:10,870 --> 00:00:14,483
to improve scalability with managing routes.

00:00:15,960 --> 00:00:19,560
This idea of separating nexthops out of routes

00:00:19,560 --> 00:00:20,790
was something I first proposed

00:00:20,790 --> 00:00:23,660
like two years ago at Netconf in Seoul,

00:00:23,660 --> 00:00:26,110
and it was based on a proof of concept with IPv4.

00:00:26,960 --> 00:00:30,200
It showed really great results

00:00:30,200 --> 00:00:33,100
in how things can be done better.

00:00:33,100 --> 00:00:35,080
It has been a very long road

00:00:35,080 --> 00:00:38,870
to get this work completed and in, into the kernel,

00:00:38,870 --> 00:00:42,400
most notably due to IPv6 and how radically different

00:00:42,400 --> 00:00:44,430
its implementation was from IPv4,

00:00:45,740 --> 00:00:48,733
but I am very happy with the way things ended up.

00:00:50,090 --> 00:00:54,880
All right, I will start with a high-level summary

00:00:54,880 --> 00:00:59,770
of the routing changes and the results.

00:00:59,770 --> 00:01:02,020
Most people don't care about the details

00:01:02,020 --> 00:01:04,120
of how routes are managed in the kernel.

00:01:04,120 --> 00:01:07,590
I get that, so if you remember nothing else

00:01:07,590 --> 00:01:09,860
about all the changes that went in,

00:01:09,860 --> 00:01:12,460
I hope that this summary will stick.

00:01:12,460 --> 00:01:14,100
From there, I'll back up and look

00:01:14,100 --> 00:01:17,090
at the driving use case that motivated

00:01:17,090 --> 00:01:20,370
splitting the nexthops out as separate objects,

00:01:20,370 --> 00:01:22,930
review the legacy routing API,

00:01:22,930 --> 00:01:26,530
dive into the new nexthop API and what it provides,

00:01:26,530 --> 00:01:28,560
and then show the benefits to it.

00:01:28,560 --> 00:01:30,730
It's not just about lowering the overhead

00:01:30,730 --> 00:01:31,590
of managing routes,

00:01:31,590 --> 00:01:34,270
but it also enables other kinds of features

00:01:34,270 --> 00:01:37,470
and fixes some pain points that exist for IPv6.

00:01:39,640 --> 00:01:40,473
All right.

00:01:41,570 --> 00:01:44,770
With that, the current API

00:01:44,770 --> 00:01:46,830
for managing routes in the kernel,

00:01:46,830 --> 00:01:49,780
it requires all the nexthop information,

00:01:49,780 --> 00:01:52,940
that is, what is the next device, the next gateway,

00:01:52,940 --> 00:01:56,570
the next path, the next node in a network path,

00:01:56,570 --> 00:01:59,330
to be combined with the prefix,

00:01:59,330 --> 00:02:01,300
all that pushed into a single message

00:02:01,300 --> 00:02:02,700
and sent down to the kernel.

00:02:04,040 --> 00:02:07,140
So, networking paths tend to be redundant,

00:02:07,140 --> 00:02:10,600
and what this means is the kernel is repeatedly handling

00:02:10,600 --> 00:02:12,930
and validating information over and over again,

00:02:12,930 --> 00:02:14,920
the same information over and over again,

00:02:14,920 --> 00:02:17,423
because to the kernel each route is separate.

00:02:18,410 --> 00:02:20,440
This overhead really appears

00:02:20,440 --> 00:02:24,290
as the number of paths in the route increases.

00:02:24,290 --> 00:02:27,123
And so, you can see for IPv6, or IPv4,

00:02:27,990 --> 00:02:31,720
with four paths in the route, the number of prefixes

00:02:31,720 --> 00:02:35,860
that you can put into the kernel per second drops by 60%,

00:02:35,860 --> 00:02:38,400
and for IPv6, it drops by 45,

00:02:38,400 --> 00:02:40,980
it drops down to 45% of that rate.

00:02:40,980 --> 00:02:44,360
And so, the more paths, it's a dramatic decrease

00:02:44,360 --> 00:02:48,803
in the ability to insert routes into the kernel.

00:02:51,160 --> 00:02:53,520
This overhead is addressed by splitting out

00:02:53,520 --> 00:02:56,820
that typically redundant information into a separate object,

00:02:56,820 --> 00:02:59,220
something that has its own life cycle

00:02:59,220 --> 00:03:01,830
for creating, updating, destroying,

00:03:01,830 --> 00:03:05,023
and then having the routes reference that by an ID.

00:03:06,110 --> 00:03:07,434
It's a very simple concept

00:03:07,434 --> 00:03:10,643
but with huge implications on performance.

00:03:14,260 --> 00:03:18,100
While splitting out the nexthops as separate objects,

00:03:18,100 --> 00:03:21,010
you still see a little bit of this tapering.

00:03:21,010 --> 00:03:23,130
If you come back, you know, I'll show this overlay

00:03:23,130 --> 00:03:28,130
at the end where the before this nexthop intermediate step,

00:03:28,230 --> 00:03:30,900
because this is a new API,

00:03:30,900 --> 00:03:33,320
most processes don't know about this new API,

00:03:33,320 --> 00:03:34,520
there's gonna be this time window

00:03:34,520 --> 00:03:36,180
of backwards compatibility,

00:03:36,180 --> 00:03:38,870
so when notifications are sent to user space,

00:03:38,870 --> 00:03:41,200
the nexthop information is expanded,

00:03:41,200 --> 00:03:44,200
so there's still a little bit of tapering for both v4 and v6

00:03:45,500 --> 00:03:48,610
but the rate at which you can insert things into the kernel

00:03:48,610 --> 00:03:50,810
has gone up significantly.

00:03:50,810 --> 00:03:54,533
So, from looking at just a single path,

00:03:54,533 --> 00:03:59,533
IPv4 shows a 30% gain, IPv6 shows a 40% gain,

00:03:59,920 --> 00:04:02,120
and then when you look down at 32 paths,

00:04:02,120 --> 00:04:06,850
it's just, it's off the charts in increase and performance.

00:04:06,850 --> 00:04:09,930
But, more importantly, it allows us potential

00:04:09,930 --> 00:04:14,180
for constant rate times, which is what the end goal is.

00:04:14,180 --> 00:04:16,980
It shouldn't have this increasing overhead

00:04:16,980 --> 00:04:18,830
as you increase the number of paths.

00:04:18,830 --> 00:04:22,010
And so, this is possible once your entire operating system

00:04:22,010 --> 00:04:24,800
says, I understand the new API,

00:04:24,800 --> 00:04:27,540
or if it doesn't care, then no change is needed.

00:04:27,540 --> 00:04:29,810
And then we'll have a sysctl or something

00:04:29,810 --> 00:04:32,900
that says we don't need the backwards compatibility,

00:04:32,900 --> 00:04:35,253
just go with the new performance.

00:04:36,840 --> 00:04:38,633
All right, with that,

00:04:40,260 --> 00:04:42,730
let's back up and take a look at the use case

00:04:42,730 --> 00:04:47,163
that really pushed the need for this new API.

00:04:48,099 --> 00:04:50,830
As I mentioned in previous talks,

00:04:50,830 --> 00:04:53,000
network operating systems are moving

00:04:53,000 --> 00:04:55,650
to putting more and more information into the kernel.

00:04:57,090 --> 00:05:01,000
A true Linux-based networking operating system,

00:05:01,000 --> 00:05:02,670
the kernel is the source of truth,

00:05:02,670 --> 00:05:04,710
and that's the case for Cumulus Linux.

00:05:04,710 --> 00:05:06,810
Everything is put into the kernel.

00:05:06,810 --> 00:05:08,850
Any user space applications

00:05:08,850 --> 00:05:10,820
that are interested in this information

00:05:10,820 --> 00:05:13,740
have Netlink messaging or Netlink notifications

00:05:13,740 --> 00:05:16,860
to receive updates as things are done.

00:05:16,860 --> 00:05:20,959
The difference gets into how do you program the hardware?

00:05:20,959 --> 00:05:24,180
In the case of Cumulus Linux, the hardware programming

00:05:24,180 --> 00:05:26,770
is done by a user space process.

00:05:26,770 --> 00:05:29,550
That process is linked to an SDK.

00:05:29,550 --> 00:05:32,640
As the routing daemon, for example, FRR,

00:05:32,640 --> 00:05:34,840
pushes routes into the kernel,

00:05:34,840 --> 00:05:38,190
the user space application gets notification of that,

00:05:38,190 --> 00:05:41,960
it will map the Linux model onto the hardware model,

00:05:41,960 --> 00:05:45,153
invoke some SDK functions in that program's hardware.

00:05:47,020 --> 00:05:51,275
A very similar architecture to this is switchdev.

00:05:51,275 --> 00:05:53,690
In this case, you don't have a user space driver,

00:05:53,690 --> 00:05:55,510
you have an internal driver.

00:05:55,510 --> 00:05:58,430
You're not relying on user space notifications,

00:05:58,430 --> 00:06:01,180
you're relying on internal notifications.

00:06:01,180 --> 00:06:03,840
But still, it's the same architecture, right?

00:06:03,840 --> 00:06:07,220
You have routing daemons such as FRR

00:06:07,220 --> 00:06:08,730
pushing information into the kernel

00:06:08,730 --> 00:06:10,370
which is the source of truth,

00:06:10,370 --> 00:06:12,610
and then there's some way to get that information

00:06:12,610 --> 00:06:13,760
down into the hardware.

00:06:14,690 --> 00:06:16,260
Take away the hardware,

00:06:16,260 --> 00:06:18,840
take away the offload ASIC, for example,

00:06:18,840 --> 00:06:20,390
shrink the number of interfaces,

00:06:20,390 --> 00:06:22,140
and it's really the same kind of architecture

00:06:22,140 --> 00:06:24,570
that you see for routing on the host, right,

00:06:24,570 --> 00:06:26,770
when you got virtual machines and containers,

00:06:26,770 --> 00:06:31,710
and what's of interest is the ability to adapt quickly

00:06:31,710 --> 00:06:34,233
to any kind of changes in the networking.

00:06:35,110 --> 00:06:36,400
All right.

00:06:36,400 --> 00:06:40,470
A similar architecture

00:06:40,470 --> 00:06:43,130
gets into these, what I would call a hybrid approach,

00:06:43,130 --> 00:06:47,700
where you don't rely on notifications, be it user space

00:06:47,700 --> 00:06:50,500
or kernel-based notifications to program hardware.

00:06:50,500 --> 00:06:52,500
Instead, you could easily have this done

00:06:52,500 --> 00:06:54,730
through some other mechanism in user space

00:06:54,730 --> 00:06:58,810
where the routing daemon feeds all this information,

00:06:58,810 --> 00:07:01,060
or multiple routing daemons could feed this information

00:07:01,060 --> 00:07:05,550
into some kind of a FIB process which flattens things out

00:07:05,550 --> 00:07:09,970
and sends one set to the Linux kernel based on its model

00:07:09,970 --> 00:07:14,970
and sends another set of commands to the ASIC offload driver

00:07:15,130 --> 00:07:17,370
which then programs the hardware.

00:07:17,370 --> 00:07:20,400
But really, the key point here is,

00:07:20,400 --> 00:07:22,290
you have a large route capacity

00:07:22,290 --> 00:07:24,930
in either the ASIC or the kernel, for example,

00:07:24,930 --> 00:07:28,350
you have a very few from number of front panel ports,

00:07:28,350 --> 00:07:30,780
the kernel is the source of truth for all of this,

00:07:30,780 --> 00:07:31,890
or not necessarily the source of truth

00:07:31,890 --> 00:07:36,430
but it contains all the forwarding data that's being used,

00:07:36,430 --> 00:07:39,320
and we're looking at this from not just scalability

00:07:39,320 --> 00:07:42,030
for today but scalability for tomorrow.

00:07:42,030 --> 00:07:45,000
With routing on the hosts, as the number of CPUs increases,

00:07:45,000 --> 00:07:47,310
there's more containers, more VMs,

00:07:47,310 --> 00:07:50,040
things like Project Calico bringing L3 down

00:07:50,040 --> 00:07:53,096
to the host more and more, and then looking at things like

00:07:53,096 --> 00:07:56,930
switchdev and Spectrum ASICs, or Spectrum-2, for example,

00:07:56,930 --> 00:07:58,193
with a million routes.

00:07:59,840 --> 00:08:04,840
The key there is that you've got huge number of prefixes,

00:08:05,130 --> 00:08:08,930
a few number of paths in your network,

00:08:08,930 --> 00:08:11,070
and so there's a large redundancy

00:08:11,070 --> 00:08:14,560
in the information that the routing,

00:08:14,560 --> 00:08:17,160
that the routes are pointing to.

00:08:17,160 --> 00:08:19,990
This redundant information is the source of the overhead.

00:08:19,990 --> 00:08:22,910
So, in some cases, you could have hundreds of thousands

00:08:22,910 --> 00:08:25,860
of routes all pointing to the same path,

00:08:25,860 --> 00:08:28,720
be it a single path or a multipath route,

00:08:28,720 --> 00:08:31,200
and so every time you push that information to the kernel,

00:08:31,200 --> 00:08:33,073
it's gotta keep getting verified.

00:08:35,270 --> 00:08:36,980
Let's take a look at routing suites.

00:08:36,980 --> 00:08:41,320
BGP was the one that kinda pushed the change that I was,

00:08:41,320 --> 00:08:44,030
that I made for the nexthop information.

00:08:44,030 --> 00:08:46,540
The way BGP exchange information,

00:08:46,540 --> 00:08:49,573
when BGP on one switch peers with another one,

00:08:50,480 --> 00:08:53,810
the nexthop information is a separate attribute

00:08:53,810 --> 00:08:57,033
in this message, along with a list of prefixes.

00:08:57,870 --> 00:09:00,483
So, routing daemons have information separate.

00:09:01,480 --> 00:09:03,680
It goes to push that information to the kernel.

00:09:03,680 --> 00:09:07,670
The kernel says, well, my API says I need the prefix

00:09:07,670 --> 00:09:10,240
along with the device, along with the gateway,

00:09:10,240 --> 00:09:13,460
and I need that for each path, combine all that information,

00:09:13,460 --> 00:09:15,943
and push it down to the kernel.

00:09:18,380 --> 00:09:20,570
The kernel then has to take a look at that message

00:09:20,570 --> 00:09:24,010
and, for each one of those nexthops, has to validate it.

00:09:24,010 --> 00:09:26,780
If there's a gateway, does that gateway,

00:09:26,780 --> 00:09:28,270
if I do a FIB look-up on that gateway,

00:09:28,270 --> 00:09:30,020
does it map to the egress device

00:09:30,020 --> 00:09:32,340
that was in that message, for example?

00:09:32,340 --> 00:09:33,820
It'll have to allocate some memory

00:09:33,820 --> 00:09:36,550
for the nexthop information.

00:09:36,550 --> 00:09:40,270
And because of cached routes and exceptions,

00:09:40,270 --> 00:09:42,800
it also has some PCPU per-CPU information

00:09:42,800 --> 00:09:44,850
that it will get allocated.

00:09:44,850 --> 00:09:46,470
And then it does some more massaging,

00:09:46,470 --> 00:09:49,610
like IPv4 will try to consolidate this information,

00:09:49,610 --> 00:09:51,433
but after it's done the validation.

00:09:52,504 --> 00:09:55,190
And then that information is then passed to,

00:09:55,190 --> 00:09:58,030
once it is put into the FIB,

00:09:58,030 --> 00:10:01,410
that information is passed to notifiers, for example,

00:10:01,410 --> 00:10:04,780
which have to send messages to user space going up

00:10:04,780 --> 00:10:07,060
or messages down to the switchdev driver

00:10:07,060 --> 00:10:09,183
going down to the internal driver.

00:10:11,760 --> 00:10:13,960
Routing daemons, information separate,

00:10:13,960 --> 00:10:17,293
kernel requires that information to be pushed together.

00:10:18,620 --> 00:10:21,090
Before we get to the what the ASIC driver has to do,

00:10:21,090 --> 00:10:23,610
let's look at how the hardware programming has to happen,

00:10:23,610 --> 00:10:25,730
because the Linux model and the hardware model

00:10:25,730 --> 00:10:26,880
are a little different.

00:10:27,720 --> 00:10:29,490
From the hardware perspective,

00:10:29,490 --> 00:10:31,440
you have these front-panel ports,

00:10:31,440 --> 00:10:34,630
they get represented as netdevs in the kernel,

00:10:34,630 --> 00:10:36,350
but what the hardware wants to see,

00:10:36,350 --> 00:10:37,447
if you're gonna do L3-forwarding,

00:10:37,447 --> 00:10:39,670
is you've gotta create a router interface

00:10:39,670 --> 00:10:40,743
on top of that port.

00:10:41,612 --> 00:10:43,750
That router interface is what corresponds

00:10:43,750 --> 00:10:46,703
to what Linux calls the egress device in the route.

00:10:47,950 --> 00:10:50,300
You also have to have that host route.

00:10:50,300 --> 00:10:54,120
So, the gateway, what really matters is the MAC information,

00:10:54,120 --> 00:10:56,667
so that gateway has to be resolved into a host route

00:10:56,667 --> 00:10:59,400
and the host route gets programmed in the hardware.

00:10:59,400 --> 00:11:01,750
Once I have the router interface and the host route,

00:11:01,750 --> 00:11:03,840
I can create this thing called a nexthop,

00:11:03,840 --> 00:11:06,020
which is, with the limited resources of hardware,

00:11:06,020 --> 00:11:08,610
says let's create this nexthop once

00:11:08,610 --> 00:11:11,030
and have multiple things point to it.

00:11:11,030 --> 00:11:13,530
If I'm gonna create a multipath route,

00:11:13,530 --> 00:11:14,970
I create a series of those

00:11:14,970 --> 00:11:16,673
and then I create a nexthop group.

00:11:17,510 --> 00:11:19,590
At that point, I can then program

00:11:19,590 --> 00:11:21,670
the LPM entry into hardware,

00:11:21,670 --> 00:11:24,570
and then after that, multiple entries can correspond

00:11:24,570 --> 00:11:26,973
to that same nexthop or nexthop group.

00:11:30,130 --> 00:11:32,720
The kernel has this information integrated,

00:11:32,720 --> 00:11:34,750
it sends the notification up to user space

00:11:34,750 --> 00:11:36,480
or down to the kernel.

00:11:36,480 --> 00:11:38,880
Both of those drivers have to turn around

00:11:38,880 --> 00:11:41,650
and break that information apart

00:11:41,650 --> 00:11:44,230
and say, hey, I've already created this nexthop

00:11:44,230 --> 00:11:46,320
in the back end, in the hardware, right,

00:11:46,320 --> 00:11:48,340
'cause it wants to only do it once.

00:11:48,340 --> 00:11:51,420
It doesn't have the resources to do separate nexthops

00:11:51,420 --> 00:11:53,863
for every route that's getting programmed.

00:11:54,750 --> 00:11:57,320
That means the driver has to kinda keep track

00:11:57,320 --> 00:11:59,450
of this device in gateway

00:11:59,450 --> 00:12:03,880
or this path corresponds to this hardware entity.

00:12:03,880 --> 00:12:06,960
So, now, it's gotta take that integrated kernel information,

00:12:06,960 --> 00:12:09,217
break it apart, do a search for something unique,

00:12:09,217 --> 00:12:11,567
and if it doesn't exist, then go and create it.

00:12:13,290 --> 00:12:14,980
When we look at this from an end-to-end,

00:12:14,980 --> 00:12:18,240
from BGP down to programming the hardware,

00:12:18,240 --> 00:12:20,070
you have a lot of wasted cycles

00:12:20,070 --> 00:12:23,320
validating repetitive information.

00:12:23,320 --> 00:12:25,730
And so, that was the point of saying,

00:12:25,730 --> 00:12:28,750
you know, let's break this apart.

00:12:28,750 --> 00:12:32,070
Why can't we have the redundant information

00:12:32,070 --> 00:12:34,693
as a separate object with its own life cycle?

00:12:35,647 --> 00:12:39,690
It will have its own add, create, modify set of commands,

00:12:39,690 --> 00:12:41,030
its own set of attributes

00:12:41,030 --> 00:12:45,310
for specifying a device and a gateway.

00:12:45,310 --> 00:12:47,510
And then once that exists,

00:12:47,510 --> 00:12:49,400
prefixes can be pushed down to the kernel

00:12:49,400 --> 00:12:52,860
and just point to that nexthop by an ID.

00:12:52,860 --> 00:12:56,180
So, now, the validation only becomes, is that ID valid.

00:12:56,180 --> 00:12:59,470
It's not, is this device valid, is the device up,

00:12:59,470 --> 00:13:02,920
is the carrier up, is the gateway resolved to that device.

00:13:02,920 --> 00:13:04,820
All of that information is done once

00:13:04,820 --> 00:13:06,723
and now we just point to it by an ID.

00:13:08,210 --> 00:13:13,203
Very simple idea but huge ramifications on performance.

00:13:23,690 --> 00:13:24,850
All right, the API.

00:13:24,850 --> 00:13:27,730
Like I said, it's its own object.

00:13:27,730 --> 00:13:29,930
Based on the way rtnetlink is done,

00:13:29,930 --> 00:13:32,830
we've got new RTM commands

00:13:32,830 --> 00:13:36,560
for adding a nexthop, deleting the nexthop,

00:13:36,560 --> 00:13:39,860
updating the nexthop, retrieving the,

00:13:39,860 --> 00:13:42,500
getting a dump of what exists in the kernel.

00:13:42,500 --> 00:13:45,683
And then there's new NHA attributes that go with that.

00:13:46,890 --> 00:13:49,943
As usual, things are put into the UAPI directory.

00:13:50,845 --> 00:13:55,370
I purposely made the NHA, the nexthop attributes,

00:13:55,370 --> 00:13:58,880
direct parallels to the RTA versions.

00:13:58,880 --> 00:14:03,640
In fact, most of the code changes needed for this feature

00:14:03,640 --> 00:14:06,000
were on refactoring the core code

00:14:06,000 --> 00:14:09,700
so that the nexthop code and the in-kernel,

00:14:09,700 --> 00:14:12,350
the core integrated route APIs,

00:14:12,350 --> 00:14:14,930
it uses the exact same code on the back end

00:14:14,930 --> 00:14:18,650
for validation and initialization

00:14:18,650 --> 00:14:20,623
of the in-kernel data structures.

00:14:23,030 --> 00:14:24,770
There's two kinds of nexthops.

00:14:24,770 --> 00:14:28,800
There's a basic nexthop, which is a single path,

00:14:28,800 --> 00:14:31,010
here's a device, here's a gateway,

00:14:31,010 --> 00:14:33,963
or that basic nexthop can be a blackhole.

00:14:35,140 --> 00:14:37,380
You create one or more of those

00:14:37,380 --> 00:14:39,983
and then you can create a group based off of that.

00:14:41,590 --> 00:14:43,470
And, of course, all of those have their own ID

00:14:43,470 --> 00:14:47,940
so that the ID is the key reference for nexthops.

00:14:47,940 --> 00:14:51,700
That ID is definitely unique within a network namespace,

00:14:51,700 --> 00:14:54,400
and that ID can be used by the ASIC drivers then

00:14:54,400 --> 00:14:56,580
to track have I seen this before

00:14:56,580 --> 00:14:59,792
and have I created this in the back end already.

00:14:59,792 --> 00:15:02,730
The kernel allows you to specify an ID.

00:15:02,730 --> 00:15:05,600
For example, FRR creates its own IDs for these

00:15:05,600 --> 00:15:07,620
and tracks it internally.

00:15:07,620 --> 00:15:08,770
But you don't have to.

00:15:08,770 --> 00:15:10,750
If you don't wanna do the ID management,

00:15:10,750 --> 00:15:12,390
you can allow the kernel to do it.

00:15:12,390 --> 00:15:14,260
It has a counter where it keeps track

00:15:14,260 --> 00:15:16,350
of what was the last assigned ID,

00:15:16,350 --> 00:15:18,990
and it will find a unique one that is not in use

00:15:18,990 --> 00:15:20,490
and assign it to that nexthop.

00:15:21,356 --> 00:15:22,189
Okay.

00:15:23,370 --> 00:15:24,830
My original goal

00:15:24,830 --> 00:15:28,250
was to not have any address-family-specific things

00:15:28,250 --> 00:15:29,780
related with this.

00:15:29,780 --> 00:15:32,770
The cached routes and the exceptions

00:15:32,770 --> 00:15:36,193
kinda forced me to make these address-family-specific.

00:15:37,320 --> 00:15:39,260
When you create a basic nexthop,

00:15:39,260 --> 00:15:43,030
you do have to say is this an AF_INET or an AF_INET6,

00:15:43,030 --> 00:15:46,130
so an IPv4 or an IPv6 nexthop.

00:15:49,340 --> 00:15:51,810
Yeah, so then the gateway is validated

00:15:51,810 --> 00:15:53,433
based on that address family.

00:15:57,771 --> 00:15:59,353
To create a multipath nexthop,

00:16:00,350 --> 00:16:03,680
you would then create one or more basic nexthops

00:16:03,680 --> 00:16:07,090
and then create a nexthop again with its own ID

00:16:07,090 --> 00:16:11,693
but with a list of references to the individual basic ones.

00:16:13,010 --> 00:16:16,200
I purposely wanted to overload the struct nexthop

00:16:16,200 --> 00:16:19,300
to mean either a single path or a multipath,

00:16:19,300 --> 00:16:20,670
because the core code,

00:16:20,670 --> 00:16:23,260
when I show some changes later that were done to it,

00:16:23,260 --> 00:16:26,200
it doesn't really need to care, it simply needs to say,

00:16:26,200 --> 00:16:27,820
do I have an external object,

00:16:27,820 --> 00:16:30,520
and we'll let the nexthop code itself say,

00:16:30,520 --> 00:16:33,230
whether I have a single one that I return

00:16:33,230 --> 00:16:36,400
or do I need to walk the individual ones.

00:16:36,400 --> 00:16:38,830
So, it's a way of making the core changes once

00:16:38,830 --> 00:16:40,390
and then allow features to be done

00:16:40,390 --> 00:16:41,893
inside of the nexthop code.

00:16:44,450 --> 00:16:46,010
Constraints.

00:16:46,010 --> 00:16:48,610
We always want everything to be unconstrained

00:16:48,610 --> 00:16:52,673
and as flexible as possible but reality kinda kicked in.

00:16:53,934 --> 00:16:57,010
For multipath groups, you cannot do nested,

00:16:57,010 --> 00:17:00,220
you cannot have a multipath nexthop group

00:17:00,220 --> 00:17:02,713
inside of a multipath nexthop group.

00:17:03,742 --> 00:17:04,990
I will come back to that.

00:17:04,990 --> 00:17:08,540
I do want to allow groups within a group

00:17:08,540 --> 00:17:12,690
but that's a different style, a different topic.

00:17:12,690 --> 00:17:15,120
If you're gonna have a group with a nexthop,

00:17:15,120 --> 00:17:18,060
I'm sorry, if you're gonna have a group with a blackhole,

00:17:18,060 --> 00:17:21,360
then that group can only have one nexthop.

00:17:21,360 --> 00:17:25,670
The idea there is you cannot have a multipath route,

00:17:25,670 --> 00:17:26,980
one of those is a blackhole,

00:17:26,980 --> 00:17:29,520
the rest of them are normal device gateways,

00:17:29,520 --> 00:17:30,904
and so some traffic disappears

00:17:30,904 --> 00:17:32,470
(chuckling) based on the selection.

00:17:32,470 --> 00:17:34,620
Really, the intention there is,

00:17:34,620 --> 00:17:37,890
if you're gonna blackhole traffic, you do it all.

00:17:37,890 --> 00:17:41,880
And then if you want to do an atomic update on prefixes,

00:17:41,880 --> 00:17:44,460
you can redo that group and have it be something

00:17:44,460 --> 00:17:46,223
like a more normal device gateway.

00:17:48,170 --> 00:17:50,580
The same nexthop ID cannot be used

00:17:50,580 --> 00:17:53,040
more than once within a group.

00:17:53,040 --> 00:17:54,713
The kernel has to track,

00:17:55,940 --> 00:17:57,780
kind of both backwards and forwards,

00:17:57,780 --> 00:18:01,720
with the, this nexthop group references this nexthop

00:18:01,720 --> 00:18:03,600
which references this device,

00:18:03,600 --> 00:18:05,570
so that when a device event happens,

00:18:05,570 --> 00:18:07,710
it can go the other direction and say,

00:18:07,710 --> 00:18:10,270
this device disappeared so this nexthop disappears

00:18:10,270 --> 00:18:13,070
so this nexthop group needs to get updated.

00:18:13,070 --> 00:18:15,853
That tracking kind of limited it to,

00:18:17,180 --> 00:18:19,280
an single basic nexthop

00:18:19,280 --> 00:18:22,663
can only be in a group one time.

00:18:25,230 --> 00:18:27,770
The other one is, the other basic constraint

00:18:27,770 --> 00:18:30,780
is you cannot change the type.

00:18:30,780 --> 00:18:33,470
If you create a nexthop ID1

00:18:33,470 --> 00:18:36,100
as a basic device gateway nexthop,

00:18:36,100 --> 00:18:39,993
you cannot do an update and try to make ID1 a group.

00:18:41,310 --> 00:18:43,950
That gets into some implementation details

00:18:43,950 --> 00:18:47,960
about the RCU data and some references to it.

00:18:47,960 --> 00:18:49,280
I didn't want that overhead

00:18:49,280 --> 00:18:52,273
of having to be able to toggle between the types.

00:18:54,640 --> 00:18:56,120
From a route perspective,

00:18:56,120 --> 00:18:58,633
a new attribute was added, RTA_NH_ID.

00:19:00,330 --> 00:19:03,620
This is how you would tell a particular route entry

00:19:03,620 --> 00:19:04,637
is using this nexthop.

00:19:04,637 --> 00:19:07,530
You just have to specify the ID for it.

00:19:07,530 --> 00:19:09,340
If you specify that ID,

00:19:09,340 --> 00:19:11,460
then you cannot give any other information

00:19:11,460 --> 00:19:13,330
like the device, the gateway,

00:19:13,330 --> 00:19:15,170
or any kind of encap information,

00:19:15,170 --> 00:19:16,610
because that would then be a duplicate

00:19:16,610 --> 00:19:19,610
and we don't want the kernel to have to validate two things.

00:19:21,200 --> 00:19:23,760
By having this nexthop ID by reference,

00:19:23,760 --> 00:19:26,150
we get to skip all those validation checks,

00:19:26,150 --> 00:19:28,620
which is where we get most of our performance from,

00:19:28,620 --> 00:19:31,470
but we'd still have to do some basic minimum checks.

00:19:31,470 --> 00:19:33,730
We have to verify that your ID that you gave

00:19:33,730 --> 00:19:36,350
is a valid ID for a nexthop.

00:19:36,350 --> 00:19:40,310
We have to make sure that the nexthop type

00:19:40,310 --> 00:19:41,890
is valid for the route.

00:19:41,890 --> 00:19:46,890
Specifically, we allow v6 nexthops with v4 routes

00:19:47,550 --> 00:19:51,460
but you cannot have v4 nexthops with v6 routes.

00:19:51,460 --> 00:19:54,300
For v4 gateways, use the mapped addressing

00:19:54,300 --> 00:19:56,143
as a way of implementing that.

00:19:57,474 --> 00:19:59,400
And then the scope checks for IPv4,

00:19:59,400 --> 00:20:03,130
you gotta make sure that the prefix, if it's a host scope,

00:20:03,130 --> 00:20:06,470
it then can't have an invalid scope for nexthops,

00:20:06,470 --> 00:20:07,630
things like that.

00:20:07,630 --> 00:20:11,073
But those are very fast and super minimal from a checking.

00:20:12,910 --> 00:20:14,360
Coexistence of models.

00:20:14,360 --> 00:20:15,763
This is an opt-in API.

00:20:17,380 --> 00:20:19,960
If you like the old way of doing routes,

00:20:19,960 --> 00:20:21,640
you don't have to change.

00:20:21,640 --> 00:20:23,760
But if you're running something at scale

00:20:23,760 --> 00:20:25,760
and you really want these efficiencies,

00:20:25,760 --> 00:20:28,760
it's well worth the flip.

00:20:28,760 --> 00:20:31,673
I have tried to make this as painless as possible.

00:20:34,220 --> 00:20:36,410
And then, as a part of this

00:20:36,410 --> 00:20:39,243
backwards-compatibility transition period,

00:20:40,380 --> 00:20:43,800
when notifications are sent to user space,

00:20:43,800 --> 00:20:47,370
the nexthop definition,

00:20:47,370 --> 00:20:49,340
if you've given a nexthop ID,

00:20:49,340 --> 00:20:51,497
it will expand and add the device

00:20:51,497 --> 00:20:53,800
and the gateway and the encap information

00:20:53,800 --> 00:20:55,940
into the user space notification

00:20:55,940 --> 00:20:59,800
so that existing apps like SNMPD, NTPD,

00:20:59,800 --> 00:21:01,770
you'd be surprised how many applications

00:21:01,770 --> 00:21:03,957
listen for route changes,

00:21:03,957 --> 00:21:06,707
and you would wonder why they listen for route changes,

00:21:08,040 --> 00:21:10,950
anyway, those applications will still get that information

00:21:10,950 --> 00:21:13,793
and then in time, when they get all converted,

00:21:14,640 --> 00:21:16,140
will be able to turn that off.

00:21:20,450 --> 00:21:21,640
All right.

00:21:21,640 --> 00:21:23,430
Now, we're gonna take a little bit of a detour

00:21:23,430 --> 00:21:25,343
into the kernel side of this,

00:21:26,410 --> 00:21:28,890
kinda give an overview of where that code is

00:21:28,890 --> 00:21:30,490
and some changes that were made.

00:21:31,880 --> 00:21:35,200
All of the code, I attempted to keep it consolidated

00:21:35,200 --> 00:21:39,960
into nexthop.c or nexthop.h as much as possible.

00:21:39,960 --> 00:21:42,430
I wanted the intrusions into the core code

00:21:42,430 --> 00:21:44,363
to be as minimal as possible.

00:21:45,700 --> 00:21:50,180
Basically, if route entry has a nexthop by reference,

00:21:50,180 --> 00:21:52,700
then go to this helper in the .h file

00:21:52,700 --> 00:21:55,650
or call this function out of the .c file.

00:21:55,650 --> 00:21:56,767
The intention there is that,

00:21:56,767 --> 00:21:59,310
I'm gonna talk about a feature later,

00:21:59,310 --> 00:22:01,280
if someone has the time to implement this,

00:22:01,280 --> 00:22:05,000
they don't have to wade into IPv4 code or IPv6 code.

00:22:05,000 --> 00:22:07,350
They can consolidate, or keep their focus

00:22:07,350 --> 00:22:09,180
on just this nexthop code

00:22:09,180 --> 00:22:11,683
as a way of implementing new changes.

00:22:13,320 --> 00:22:16,900
The nexthops are stored in a per-network namespace rbtree

00:22:16,900 --> 00:22:19,393
with that ID as the lookup key.

00:22:20,550 --> 00:22:25,140
Again, the majority of the patches to get this feature in

00:22:25,140 --> 00:22:28,630
was refactoring, refactoring to get things like fib_nh

00:22:28,630 --> 00:22:31,610
as the primary input to functions

00:22:31,610 --> 00:22:34,703
or the fib{6}_nh existence and into functions.

00:22:36,530 --> 00:22:37,963
Inside of that,

00:22:39,250 --> 00:22:41,690
everything relies on some common information,

00:22:41,690 --> 00:22:44,380
be it a device, be it a gateway address,

00:22:44,380 --> 00:22:46,240
be it encap information,

00:22:46,240 --> 00:22:51,240
be it references to cached routes or exceptions.

00:22:51,760 --> 00:22:53,640
And so, anything that was gonna be common

00:22:53,640 --> 00:22:56,640
across both of the layers,

00:22:56,640 --> 00:22:58,040
I moved into a fib_nh_common

00:22:58,930 --> 00:23:01,970
and I tried to get the code to reference that

00:23:01,970 --> 00:23:03,410
to the greatest degree possible.

00:23:03,410 --> 00:23:07,250
If you didn't need to know something specific about IPv6,

00:23:07,250 --> 00:23:11,063
you can use a fib_nh_common as the input argument.

00:23:12,450 --> 00:23:15,430
And then the exporting the initialization

00:23:15,430 --> 00:23:17,430
or release functions for these

00:23:17,430 --> 00:23:19,990
so that the nexthop code calls in to, again,

00:23:19,990 --> 00:23:24,080
get that same validation across the two implementations

00:23:24,080 --> 00:23:25,253
or the two APIs.

00:23:27,720 --> 00:23:30,070
Struct nexthop is the basic

00:23:33,090 --> 00:23:35,670
structure for this nexthop information.

00:23:35,670 --> 00:23:37,270
It has things like a list head

00:23:37,270 --> 00:23:41,420
for tracking which FIB entries are referencing this nexthop,

00:23:41,420 --> 00:23:42,730
which ones have a reference to it,

00:23:42,730 --> 00:23:47,310
so if the nexthop is deleted, you can very quickly go back

00:23:47,310 --> 00:23:49,890
to all the FIB entries that need to get evicted,

00:23:49,890 --> 00:23:53,090
or if a device goes down, which device reference,

00:23:53,090 --> 00:23:57,270
there's a device hash to know which nexthops are using it,

00:23:57,270 --> 00:23:59,680
so again to go backwards and be able to evict things

00:23:59,680 --> 00:24:01,573
in a very quick and sane order.

00:24:03,540 --> 00:24:06,317
There's a tracking for groups.

00:24:06,317 --> 00:24:08,440
Nexthops that are inside of a group,

00:24:08,440 --> 00:24:10,930
when the nexthop disappears,

00:24:10,930 --> 00:24:13,470
then the group needs to be updated as well.

00:24:13,470 --> 00:24:16,120
There's list heads to keep track of that information.

00:24:18,490 --> 00:24:19,573
Data structures.

00:24:20,580 --> 00:24:23,670
The primary change was just to come in

00:24:23,670 --> 00:24:26,920
and add nexthop*.h or *nh

00:24:26,920 --> 00:24:30,619
to both the fib_info and the fib6_info.

00:24:30,619 --> 00:24:35,360
The fib_info for IPv4 is the core data structure

00:24:35,360 --> 00:24:37,920
for the nexthop information,

00:24:37,920 --> 00:24:40,183
other metadata, like the priority,

00:24:42,030 --> 00:24:46,480
TOS, hash, anyway.

00:24:46,480 --> 00:24:49,240
fib_info is the primary (chuckling) data structure for 4.

00:24:49,240 --> 00:24:52,810
From the refactoring that I did in 2018,

00:24:52,810 --> 00:24:56,340
I made fib6_info as much of a parallel

00:24:56,340 --> 00:24:59,390
to the fib_info as I could for IPv6.

00:24:59,390 --> 00:25:02,700
It's not exactly the same but it's as close as I could get

00:25:02,700 --> 00:25:07,603
just based on what I needed to eventually get to this.

00:25:09,400 --> 00:25:13,720
fib_info and fib6_info both have a nexthop struct,

00:25:13,720 --> 00:25:15,573
so fib_nh or fib{6}_nh,

00:25:16,560 --> 00:25:20,040
and that's the one that contains the actual data.

00:25:20,040 --> 00:25:24,910
And so, that data structure then becomes part of the nexthop

00:25:24,910 --> 00:25:26,910
'cause when I do a fib lookup,

00:25:26,910 --> 00:25:29,120
it ultimately will have a prefix,

00:25:29,120 --> 00:25:31,130
and I want the nexthop information.

00:25:31,130 --> 00:25:34,990
And so, I changed v4 to, on the FIB result,

00:25:34,990 --> 00:25:37,450
it returns the common and it doesn't really care

00:25:37,450 --> 00:25:39,000
is it a v4, is it a v6.

00:25:39,000 --> 00:25:41,230
I don't really care because what I want is,

00:25:41,230 --> 00:25:43,960
I have an egress device and I have an address,

00:25:43,960 --> 00:25:45,940
and then later functions that care

00:25:45,940 --> 00:25:49,800
can do the container up to go from the common to the 4

00:25:49,800 --> 00:25:51,113
or the common to the 6.

00:25:53,000 --> 00:25:55,153
And then from a nexthop group perspective,

00:25:56,907 --> 00:26:00,440
the nexthop group has a series of references

00:26:00,440 --> 00:26:01,710
to individual nexthops,

00:26:01,710 --> 00:26:05,880
and then of course those reference the fib{6}_nh

00:26:05,880 --> 00:26:07,323
or the fib_nh directly.

00:26:13,520 --> 00:26:15,890
IPv6 was hands-down

00:26:15,890 --> 00:26:18,823
(chuckling) the largest pain in the butt to get this done.

00:26:21,030 --> 00:26:24,963
The way multipath was done for IPv6,

00:26:26,620 --> 00:26:30,430
each individual leg was a separate route entry,

00:26:30,430 --> 00:26:32,730
and then those route entries were linked together

00:26:32,730 --> 00:26:34,063
through a siblings list.

00:26:35,000 --> 00:26:39,300
The way then the IPv6 code was done

00:26:39,300 --> 00:26:41,070
was that if it was fib6_info,

00:26:41,070 --> 00:26:43,840
rt6_info in the original implementation,

00:26:43,840 --> 00:26:46,540
was the primary data structure that was passed around.

00:26:48,300 --> 00:26:50,550
What I need for nexthops

00:26:50,550 --> 00:26:53,000
is something similar to what IPv4 has,

00:26:53,000 --> 00:26:56,790
which is one prefix, so one fib_info or one fib6_info,

00:26:57,715 --> 00:26:59,083
references a whole bunch of fib{6}_nh's or fib_nh's.

00:27:02,070 --> 00:27:04,020
And so, that's the nexthop information.

00:27:05,050 --> 00:27:08,310
And so, the majority of this time was spent

00:27:08,310 --> 00:27:12,270
trying to take that existing IPv6 code,

00:27:12,270 --> 00:27:14,100
move the things that were,

00:27:14,100 --> 00:27:16,550
well, move entries into the fib{6}_nh

00:27:16,550 --> 00:27:19,330
when they were relevant to the nexthop,

00:27:19,330 --> 00:27:21,910
refactor the code so that a fib{6}_nh

00:27:21,910 --> 00:27:26,040
was the primary thing that it took to do the operations,

00:27:26,040 --> 00:27:27,990
and then iterate over

00:27:27,990 --> 00:27:30,300
so when you're walking the FIB entries,

00:27:30,300 --> 00:27:32,660
you have to, when you hit a fib6_info

00:27:32,660 --> 00:27:36,770
that has a nexthop object with a multipath route,

00:27:36,770 --> 00:27:38,940
you then have to walk each nexthop

00:27:38,940 --> 00:27:40,773
inside of that fib6_info.

00:27:42,570 --> 00:27:44,560
I forget the name of the iterator,

00:27:44,560 --> 00:27:47,203
but I added a nexthop function that basically says,

00:27:48,560 --> 00:27:52,160
I have a route entry that points to an external nexthop,

00:27:52,160 --> 00:27:53,780
that nexthop is a group,

00:27:53,780 --> 00:27:58,570
that group has X number of fib{6}_nh's inside of it,

00:27:58,570 --> 00:28:01,670
so give me a call back and I'll just keep invoking

00:28:01,670 --> 00:28:04,010
that original function this many times

00:28:04,010 --> 00:28:06,590
to see if I can find whatever it is you're looking for

00:28:06,590 --> 00:28:09,090
or to make whatever change you're looking to make.

00:28:11,230 --> 00:28:14,780
The end result is that IPv6 looks

00:28:14,780 --> 00:28:18,180
a lot more like IPv4 in the way it walks the FIB

00:28:18,180 --> 00:28:20,970
or the way it handles information.

00:28:20,970 --> 00:28:24,110
It's closer but still some annoying differences

00:28:24,110 --> 00:28:25,853
between the two protocols.

00:28:27,540 --> 00:28:28,540
Okay.

00:28:28,540 --> 00:28:31,010
Let's leave the kernel implementation aside

00:28:31,010 --> 00:28:33,543
and look at something more user-oriented.

00:28:34,520 --> 00:28:36,850
Here's a couple of examples of how you would create

00:28:36,850 --> 00:28:40,202
a basic nexthop using this new,

00:28:40,202 --> 00:28:43,035
iproute2 has a new subcommand nexthop,

00:28:43,035 --> 00:28:45,643
and so with ip nexthop, you can create

00:28:46,639 --> 00:28:49,590
a basic nexthop using a device in a gateway

00:28:49,590 --> 00:28:51,690
as done through one of these two commands.

00:28:52,580 --> 00:28:54,920
You could add a blackhole, in which case, again,

00:28:54,920 --> 00:28:57,963
if you add a blackhole, you can add a specify nothing else.

00:29:00,440 --> 00:29:02,040
Or you can then create a group

00:29:02,040 --> 00:29:05,213
which is something that references the other two.

00:29:06,390 --> 00:29:09,010
These are some very simple examples

00:29:09,010 --> 00:29:12,307
of how to use this from an iproute2 perspective.

00:29:12,307 --> 00:29:15,723
And then you add the route pointing to it by that ID.

00:29:18,260 --> 00:29:22,810
Now, let's look at what it means to take a legacy command

00:29:22,810 --> 00:29:25,290
and convert that to the new API.

00:29:25,290 --> 00:29:27,420
'Cause a lot of the tests under self-test,

00:29:27,420 --> 00:29:29,220
I took this process here

00:29:29,220 --> 00:29:30,970
and created two passes over the code,

00:29:30,970 --> 00:29:34,270
where the first one uses the legacy API for routing,

00:29:34,270 --> 00:29:36,040
and then I create a second pass

00:29:36,040 --> 00:29:37,920
which uses the new infrastructure,

00:29:37,920 --> 00:29:41,860
and it really was a matter of taking that existing command,

00:29:41,860 --> 00:29:45,600
pulling out that nexthop, putting ip in front,

00:29:45,600 --> 00:29:50,600
giving it an ID, doing that for each leg in the path

00:29:50,670 --> 00:29:53,543
or in the, yeah, the route, the path of the route,

00:29:54,420 --> 00:29:56,793
if it's multipath then create the group ID,

00:29:57,700 --> 00:30:00,130
and then reinsert that route using the nexthop ID

00:30:00,130 --> 00:30:02,163
as opposed to the expanded information.

00:30:03,520 --> 00:30:07,190
I purposely tried to keep the syntax and the attributes

00:30:07,190 --> 00:30:09,150
as closely aligned as possible

00:30:09,150 --> 00:30:11,870
so where (chuckling) you can almost do a very quick replace

00:30:11,870 --> 00:30:15,323
on any existing code to flip from old to new.

00:30:16,780 --> 00:30:17,613
All right?

00:30:18,660 --> 00:30:19,820
The benefits.

00:30:19,820 --> 00:30:24,630
I've already talked about somewhat reducing the processing

00:30:24,630 --> 00:30:27,510
of the already validated gateway information

00:30:27,510 --> 00:30:29,170
and device information.

00:30:29,170 --> 00:30:33,780
When you start getting into things like MPLS

00:30:33,780 --> 00:30:36,630
and entering into a label switched path,

00:30:36,630 --> 00:30:38,550
you'll have additional overhead that comes

00:30:38,550 --> 00:30:40,840
with validating that information.

00:30:40,840 --> 00:30:42,610
Again, you can do all that stuff once

00:30:42,610 --> 00:30:45,113
and then create those references to it.

00:30:48,369 --> 00:30:50,550
The second thing it provides

00:30:50,550 --> 00:30:53,360
is that ability go get better alignment

00:30:53,360 --> 00:30:56,370
across these protocols, IPv4 and IPv6,

00:30:56,370 --> 00:30:58,780
so they're not quite so different.

00:30:58,780 --> 00:31:02,200
As you start scaling things like the memory consumption,

00:31:02,200 --> 00:31:05,040
because IPv6 had completely separate

00:31:07,380 --> 00:31:10,160
fib6_info's for each one of the data paths,

00:31:10,160 --> 00:31:14,150
or each one of the route entries in the multipath route,

00:31:14,150 --> 00:31:16,370
you start getting huge memory consumption

00:31:16,370 --> 00:31:19,870
by replicating that structure for each leg.

00:31:19,870 --> 00:31:24,390
With this nexthop information, that's reduced quite a bit.

00:31:24,390 --> 00:31:26,540
And then you get that better alignment

00:31:26,540 --> 00:31:28,040
with hardware offload.

00:31:28,040 --> 00:31:30,580
In the case of the network operating system

00:31:30,580 --> 00:31:35,040
was that driving use case for this change,

00:31:35,040 --> 00:31:36,620
and so now you get better alignment

00:31:36,620 --> 00:31:38,300
with the end-to-end goal,

00:31:38,300 --> 00:31:41,510
going from BGP through the kernel down to hardware,

00:31:41,510 --> 00:31:43,310
better alignment with those objects.

00:31:45,310 --> 00:31:46,790
This is the overlay of those graphs

00:31:46,790 --> 00:31:48,640
I presented in the executive summary.

00:31:49,970 --> 00:31:54,970
Left graph is IPv4, right side is IPv6.

00:31:55,050 --> 00:31:57,210
Blue is the legacy API.

00:31:57,210 --> 00:32:00,830
You can see that's a huge step staircase

00:32:00,830 --> 00:32:03,030
down in performance.

00:32:03,030 --> 00:32:06,550
By having these separate objects referencing those IDs,

00:32:06,550 --> 00:32:10,240
we get a great, a huge increase in performance,

00:32:10,240 --> 00:32:13,000
and then when we get that legacy code out of the way,

00:32:13,000 --> 00:32:15,363
we get that benefit of constant rate.

00:32:19,690 --> 00:32:22,433
This is a flame graph showing legacy APIs.

00:32:22,433 --> 00:32:24,540
It's just kinda, you know, revalidates

00:32:24,540 --> 00:32:28,453
that it was the initialization of,

00:32:29,760 --> 00:32:34,650
that redundant gateway in the nexthop device information

00:32:34,650 --> 00:32:36,430
and allocating that data structure,

00:32:36,430 --> 00:32:39,550
allocating that memory for the cached routes and stuff.

00:32:39,550 --> 00:32:41,710
That's where the predominant overhead is

00:32:41,710 --> 00:32:43,010
when you're adding routes.

00:32:44,460 --> 00:32:48,490
The fib_create_info is what's processing that information.

00:32:48,490 --> 00:32:53,090
The next one over inside that circle is the fib_insert.

00:32:53,090 --> 00:32:54,970
That's where you really wanna be spending your time

00:32:54,970 --> 00:32:56,830
'cause you're adding something to the FIB,

00:32:56,830 --> 00:33:00,050
so let's focus that, our overhead there.

00:33:00,050 --> 00:33:02,720
And then your rtmsg_fib is what's sending that notification

00:33:02,720 --> 00:33:03,913
up to user space.

00:33:05,140 --> 00:33:08,453
With nexthops, that fib_create_info is gone.

00:33:09,610 --> 00:33:12,950
Instead, your overhead is the table insert

00:33:12,950 --> 00:33:15,823
and that user space notification.

00:33:17,700 --> 00:33:19,730
The same goes for IPv6.

00:33:19,730 --> 00:33:23,900
It was the fib6_info create

00:33:23,900 --> 00:33:27,596
that's the huge overhead of adding routes.

00:33:27,596 --> 00:33:30,410
With nexthop APIs, that's gone.

00:33:30,410 --> 00:33:33,610
Now, we're just focused on the important things which are,

00:33:33,610 --> 00:33:35,570
I'm adding something to a database,

00:33:35,570 --> 00:33:37,960
and user space wants to be notified

00:33:37,960 --> 00:33:39,610
that there was something changed.

00:33:42,930 --> 00:33:45,640
In addition to just inserting routes in the kernel,

00:33:45,640 --> 00:33:48,573
what else does this new API allow us to do?

00:33:50,010 --> 00:33:52,330
A common event in network environments

00:33:52,330 --> 00:33:55,650
is a link event causes a change

00:33:55,650 --> 00:33:59,113
in how certain prefixes are reached.

00:33:59,970 --> 00:34:03,690
With the legacy API, if you have a multipath route

00:34:03,690 --> 00:34:05,440
and one leg goes down,

00:34:05,440 --> 00:34:06,873
FRR or BIRD or whatever has to come in

00:34:06,873 --> 00:34:10,580
and has to do a route replace on all of those routes

00:34:10,580 --> 00:34:13,340
to remove that leg that just disappeared.

00:34:13,340 --> 00:34:15,590
So, if you've got 10,000 routes,

00:34:15,590 --> 00:34:17,770
you have to replace 10,000 routes

00:34:17,770 --> 00:34:20,793
or you have to delete and re-add 10,000 routes.

00:34:21,750 --> 00:34:24,180
When the prefixes are pointing to an object,

00:34:24,180 --> 00:34:27,360
standalone object like this, you can do that update once.

00:34:27,360 --> 00:34:29,300
So, instead of doing N updates,

00:34:29,300 --> 00:34:32,080
you just come in and update that nexthop spec that says,

00:34:32,080 --> 00:34:35,100
well, here's the new information, if it's a group,

00:34:35,100 --> 00:34:38,720
I just give a group without that one leg that disappeared.

00:34:38,720 --> 00:34:41,593
So, instead of N messages, it's one message.

00:34:44,380 --> 00:34:48,510
Another one that fell out of this was RFC 5549,

00:34:48,510 --> 00:34:51,970
which is that ability to use IPv6 addresses

00:34:51,970 --> 00:34:53,773
with IPv4 routes.

00:34:55,410 --> 00:34:58,680
The biggest need for that is something like BGP unnumbered,

00:34:58,680 --> 00:35:00,950
where you don't wanna do individual addresses

00:35:00,950 --> 00:35:02,730
to have two BGPs peer,

00:35:02,730 --> 00:35:04,930
but rather whatever that link local address is

00:35:04,930 --> 00:35:08,973
for the connection, have the BGP use that.

00:35:09,980 --> 00:35:13,230
This piece, because of all the refactoring

00:35:13,230 --> 00:35:17,990
to get this common fib_nh data structure

00:35:17,990 --> 00:35:19,293
for both v4 and v6,

00:35:21,040 --> 00:35:25,490
the gateway inside of that can be either a v4 or a v6,

00:35:25,490 --> 00:35:28,090
and so this kind of capability just falls out of it.

00:35:29,070 --> 00:35:31,640
It was something that I had in mind from the beginning,

00:35:31,640 --> 00:35:35,160
and as I went along, it just kinda worked

00:35:35,160 --> 00:35:39,400
where you don't even need the nexthop objects to get this.

00:35:39,400 --> 00:35:43,090
You can use RTA_VIA with 5.2

00:35:43,090 --> 00:35:45,850
and maintain your existing route API

00:35:45,850 --> 00:35:48,563
and get this v6 with v4 capability.

00:35:52,330 --> 00:35:54,680
All right, future extensions.

00:35:54,680 --> 00:35:57,700
This is a big one, from what I can see,

00:35:57,700 --> 00:35:59,600
from another NOS-type environment,

00:35:59,600 --> 00:36:04,100
which is you've programmed your hardware

00:36:04,100 --> 00:36:06,280
with a certain configuration, right,

00:36:06,280 --> 00:36:10,690
so here's your paths for a route, and an event happens.

00:36:10,690 --> 00:36:13,620
It takes time for FRR to process

00:36:13,620 --> 00:36:16,410
or any routing daemon to process that state change

00:36:16,410 --> 00:36:19,160
and then come back and reprogram hardware.

00:36:19,160 --> 00:36:21,690
What would be good is to have this fast rerouting,

00:36:21,690 --> 00:36:24,060
kind of an active backup nexthop capability,

00:36:24,060 --> 00:36:27,127
which is, you've already programmed the hardware

00:36:27,127 --> 00:36:29,310
and the kernel ahead of time that says,

00:36:29,310 --> 00:36:32,290
if this nexthop goes down for any reason,

00:36:32,290 --> 00:36:34,470
immediately start using this other one,

00:36:34,470 --> 00:36:37,040
and then that'll give me time to come back

00:36:37,040 --> 00:36:40,470
and update the specification

00:36:40,470 --> 00:36:43,033
to handle that event that just happened.

00:36:44,820 --> 00:36:49,510
I've tried to structure the code in a way that the group,

00:36:49,510 --> 00:36:52,000
the idea of a group can be easily expanded

00:36:52,000 --> 00:36:54,930
to have an active backup kind of a setup.

00:36:54,930 --> 00:36:58,380
So, if someone has the time to work on this,

00:36:58,380 --> 00:37:00,070
it really should be something you just go in

00:37:00,070 --> 00:37:04,950
and muddle around the code of nexthop.h and nexthop.c,

00:37:04,950 --> 00:37:07,640
see how the group semantics work,

00:37:07,640 --> 00:37:08,920
see how some of those hooks

00:37:08,920 --> 00:37:11,630
that the core networking code invokes,

00:37:11,630 --> 00:37:13,290
and it should be fairly trivial

00:37:13,290 --> 00:37:15,053
to implement something like this.

00:37:18,240 --> 00:37:19,900
Status.

00:37:19,900 --> 00:37:21,700
Everything is end of the kernel today

00:37:21,700 --> 00:37:23,820
and what will be 5.3, I guess,

00:37:23,820 --> 00:37:26,640
released potentially this Sunday.

00:37:26,640 --> 00:37:30,660
5.2 contained the initial refactoring.

00:37:30,660 --> 00:37:33,720
Like I said, IPv6 was such a pain

00:37:33,720 --> 00:37:35,840
and a lot of refactoring that was needed

00:37:35,840 --> 00:37:38,170
to get some commonality.

00:37:38,170 --> 00:37:41,700
So, a lot of that refactoring was done in 5.2.

00:37:41,700 --> 00:37:45,270
5.3 has the remainder of that refactoring,

00:37:45,270 --> 00:37:47,603
and then it has the nexthop API itself.

00:37:48,770 --> 00:37:52,830
The code also has a large self-test

00:37:52,830 --> 00:37:56,200
to try to cover all different corner cases,

00:37:56,200 --> 00:37:58,950
the constraints that I've mentioned earlier,

00:37:58,950 --> 00:38:01,383
the capabilities like v6 with v4.

00:38:02,810 --> 00:38:05,370
So, hopefully, that self-test

00:38:06,500 --> 00:38:09,440
has a really good representation

00:38:09,440 --> 00:38:12,093
of what should be allowed and what shouldn't be allowed.

00:38:13,815 --> 00:38:14,898
And then FRR.

00:38:15,940 --> 00:38:19,620
The FRR guys started earlier this year

00:38:19,620 --> 00:38:23,890
and they, I meant to update this with the pull requests,

00:38:23,890 --> 00:38:26,080
they had submitted a pull request on GitHub

00:38:26,080 --> 00:38:29,783
to include this API into mainline FRR.

00:38:31,220 --> 00:38:33,550
The first pull request got bogged down.

00:38:33,550 --> 00:38:35,110
I don't know if it was compile failures

00:38:35,110 --> 00:38:38,603
or one of the continuous integration tests was failing.

00:38:39,460 --> 00:38:42,200
But that was a week or 10 days ago.

00:38:42,200 --> 00:38:44,110
It might be in there now.

00:38:44,110 --> 00:38:45,310
This initial support,

00:38:45,310 --> 00:38:49,773
it focused on correctness over performance.

00:38:51,210 --> 00:38:53,890
What I mean by focusing on correctness over performance,

00:38:53,890 --> 00:38:55,183
there's room to improve.

00:38:56,700 --> 00:38:59,220
Performance-wise, we're seeing a benefit

00:38:59,220 --> 00:39:02,680
but not the huge benefit that I can see with IP batch,

00:39:02,680 --> 00:39:05,400
and that was because the person who worked on that code

00:39:05,400 --> 00:39:10,140
was focused on the way FRR organized information.

00:39:10,140 --> 00:39:14,990
We got a huge 30% or more reduction in memory

00:39:14,990 --> 00:39:18,700
and so maybe that pendulum swung too much on the memory side

00:39:18,700 --> 00:39:21,910
and there's some additional hash lookups that it's doing

00:39:21,910 --> 00:39:24,340
that they could probably maybe give up

00:39:24,340 --> 00:39:27,860
on some of that memory reduction to save some CPU cycles

00:39:27,860 --> 00:39:30,780
and get a better performance overhead from that.

00:39:30,780 --> 00:39:33,790
The bottom line there is FRR is a work in progress,

00:39:33,790 --> 00:39:35,560
initial support is there,

00:39:35,560 --> 00:39:38,060
and, as with anything, you need that initial support

00:39:38,060 --> 00:39:40,510
before you can improve it and make things better.

00:39:43,460 --> 00:39:45,000
What's next?

00:39:45,000 --> 00:39:47,730
From my perspective, I have this patch

00:39:47,730 --> 00:39:50,610
to handle the backwards compatibility.

00:39:50,610 --> 00:39:53,000
That's what I used to create the data.

00:39:53,000 --> 00:39:55,080
I meant to send it out but it's really,

00:39:55,080 --> 00:39:58,700
I haven't had time to handle all the testing cases

00:39:58,700 --> 00:40:01,600
to make sure it works exactly as I want it to work,

00:40:01,600 --> 00:40:04,743
like the things that should be cut out are cut out.

00:40:06,110 --> 00:40:09,670
I guess I should get to that so that 5.4 will be the,

00:40:09,670 --> 00:40:13,300
I don't know when 5.4 comes out, anyway, it'll be out soon,

00:40:13,300 --> 00:40:15,470
although I suspect it will take some amount of time

00:40:15,470 --> 00:40:19,320
before all the user space

00:40:19,320 --> 00:40:21,922
gets updated to handle this.

00:40:21,922 --> 00:40:23,253
MPLS code.

00:40:24,490 --> 00:40:26,800
It would be good to get the same kind of capability

00:40:26,800 --> 00:40:30,630
into MPLS routing so that they can also benefit

00:40:30,630 --> 00:40:35,220
from this kind of performance improvements,

00:40:35,220 --> 00:40:37,480
and then someone with the time and interest

00:40:38,440 --> 00:40:39,800
to be able to do fast rerouting.

00:40:39,800 --> 00:40:41,290
That's another really cool feature

00:40:41,290 --> 00:40:42,740
that would be good to get in.

00:40:44,520 --> 00:40:45,353
That is all.

00:40:47,060 --> 00:40:50,150
- Before we get to questions, it seems like the

00:40:51,180 --> 00:40:54,223
turn the backwards compatibility thing off is,

00:40:55,060 --> 00:40:56,913
feels to me like a dumb filter.

00:40:57,830 --> 00:41:00,130
It's a capability of the listener.

00:41:00,130 --> 00:41:01,590
- It's in notifications.

00:41:01,590 --> 00:41:04,360
- Yes, but notifications go to listeners, right?

00:41:04,360 --> 00:41:07,890
- The notifications are a system-wide thing

00:41:07,890 --> 00:41:10,420
and you can't have a per-process flag.

00:41:10,420 --> 00:41:13,260
So, dumps could have a per-process flag

00:41:13,260 --> 00:41:14,150
but notifications cannot.

00:41:14,150 --> 00:41:16,500
(audience member speaking faintly)

00:41:16,500 --> 00:41:17,773
- Oh, okay. - Yeah.

00:41:20,200 --> 00:41:22,000
- Trying to figure out if there's some way we can,

00:41:22,000 --> 00:41:26,040
everyone who would potentially receive these notifications

00:41:26,040 --> 00:41:29,250
can say I really just want those NH IDs.

00:41:29,250 --> 00:41:31,450
That way, we wouldn't need the sysctl thing.

00:41:32,680 --> 00:41:35,220
It's easy to make the kernel internally consistent

00:41:35,220 --> 00:41:38,160
in that regard, and then you have the broadcast to the--

00:41:38,160 --> 00:41:41,020
- There is a little more to it than just that.

00:41:41,020 --> 00:41:43,180
- Okay. - Than just expanding.

00:41:43,180 --> 00:41:44,650
I guess, yeah, now that I look at this,

00:41:44,650 --> 00:41:47,663
I forgot my notification slide, which is,

00:41:49,010 --> 00:41:51,800
when things happen, the idea was to create

00:41:51,800 --> 00:41:54,670
as few notifications in the user space as possible.

00:41:54,670 --> 00:41:57,880
So, if you have a link event

00:41:57,880 --> 00:41:59,750
and the kernel does things internally

00:41:59,750 --> 00:42:01,100
based on that link event,

00:42:01,100 --> 00:42:03,170
then there should not be an additional notification

00:42:03,170 --> 00:42:04,362
from any other code.

00:42:04,362 --> 00:42:07,150
For example, carrier down on a link,

00:42:07,150 --> 00:42:10,750
the nexthop code comes along and says, gone, gone, gone,

00:42:10,750 --> 00:42:14,340
all routes using this nexthop, evicted, evicted, evicted,

00:42:14,340 --> 00:42:16,640
and there's no notifications for any of those.

00:42:17,810 --> 00:42:20,310
Route updates is another one.

00:42:20,310 --> 00:42:23,250
If someone comes along and updates that nexthop ID,

00:42:23,250 --> 00:42:25,050
the specification for it,

00:42:25,050 --> 00:42:27,560
a notification is generated in user space,

00:42:27,560 --> 00:42:30,180
the backwards compatibility says I have to then walk

00:42:30,180 --> 00:42:31,013
all the FIB entries-- - All the routes

00:42:31,013 --> 00:42:32,460
that refer to it. - Exactly,

00:42:32,460 --> 00:42:33,673
and send notifications.

00:42:35,251 --> 00:42:37,440
That's a backwards compatibility thing.

00:42:37,440 --> 00:42:40,380
Once something is updated, that notification alone

00:42:40,380 --> 00:42:42,060
tells user space everything they need to know.

00:42:42,060 --> 00:42:43,630
- Exactly, it's a single op. - So, it's a little more

00:42:43,630 --> 00:42:47,430
than just, it really is a true backwards compatibility flag.

00:42:47,430 --> 00:42:48,830
- I see.

00:42:48,830 --> 00:42:50,076
Did that answer your...

00:42:50,076 --> 00:42:53,430
(audience member speaks faintly)

00:42:53,430 --> 00:42:54,263
- Thank you.

00:42:54,263 --> 00:42:55,960
Thank you for the detailed explanation.

00:42:55,960 --> 00:42:58,480
I object to the sysctl approach

00:42:58,480 --> 00:43:00,617
since I have five or six daemons listening to that

00:43:00,617 --> 00:43:03,660
and I can't update them all at the same time.

00:43:03,660 --> 00:43:07,130
However, do you have something that will do the,

00:43:07,130 --> 00:43:10,490
allow you to do the notification filtering

00:43:10,490 --> 00:43:12,060
to get it the way you want?

00:43:12,060 --> 00:43:13,260
- [David] You mean per dump?

00:43:13,260 --> 00:43:14,620
- Per dump, yes. - So, per application?

00:43:14,620 --> 00:43:15,453
- Yes. - Yeah.

00:43:15,453 --> 00:43:18,620
- We could have a flag that says, we're doing a dump,

00:43:18,620 --> 00:43:21,310
this is a specific application that wants a dump,

00:43:21,310 --> 00:43:24,130
I already understand this, don't need to help me out.

00:43:24,130 --> 00:43:24,963
- Please. - In which case,

00:43:24,963 --> 00:43:27,300
you shove a lot more route messages,

00:43:27,300 --> 00:43:29,188
route entries into a single message.

00:43:29,188 --> 00:43:30,963
- [Host] Yup, I think that makes a lot of sense.

00:43:30,963 --> 00:43:32,839
- [Audience Member] Is there a hack you can do?

00:43:32,839 --> 00:43:34,699
(speaks faintly)

00:43:34,699 --> 00:43:35,532
- Hold on, throw it. - Okay.

00:43:35,532 --> 00:43:36,650
- [Man] Throw it that way.

00:43:40,505 --> 00:43:41,970
- [Man In White Shirt] I did this hack way back

00:43:41,970 --> 00:43:43,223
in the Quagga days to,

00:43:43,223 --> 00:43:44,460
- 2008. (man chuckles)

00:43:44,460 --> 00:43:47,640
- to solve the problem of Quagga hearing itself talking,

00:43:47,640 --> 00:43:49,010
and the kernel didn't want it,

00:43:49,010 --> 00:43:51,700
was always notifying itself about its own changes.

00:43:51,700 --> 00:43:54,970
You can actually write a small, classic BPF program

00:43:54,970 --> 00:43:58,590
that says, I don't, with a socket filter, that says,

00:43:58,590 --> 00:44:01,015
I don't want X.

00:44:01,015 --> 00:44:03,060
- The problem is that you've already filled that information

00:44:03,060 --> 00:44:04,960
into the message. - Right.

00:44:04,960 --> 00:44:07,210
- And what you want-- - But you save the copy

00:44:07,210 --> 00:44:08,780
to user space and wake up for that message.

00:44:08,780 --> 00:44:12,460
- What you want in this case is when FRR, for example,

00:44:12,460 --> 00:44:14,820
already understands the idea of nexthops,

00:44:14,820 --> 00:44:16,480
and it does a route dump,

00:44:16,480 --> 00:44:18,890
it doesn't need the message

00:44:18,890 --> 00:44:19,940
to contain the device gateway encap,

00:44:19,940 --> 00:44:23,653
so in which case the, fib_dump_info,

00:44:25,660 --> 00:44:27,950
whatever that function name is, it doesn't even need to walk

00:44:27,950 --> 00:44:28,783
that code at all. - Oh, okay.

00:44:28,783 --> 00:44:32,020
- So then you put it in the message to then skip them.

00:44:32,020 --> 00:44:35,230
And so, really, what happens is when your route dump

00:44:35,230 --> 00:44:38,790
only contains a prefix and nexthop ID,

00:44:38,790 --> 00:44:41,850
you can shove a lot more information into a single message

00:44:41,850 --> 00:44:44,953
and so your dump requests will happen a lot faster.

00:44:46,760 --> 00:44:48,301
- I have a different question so if you wanna--

00:44:48,301 --> 00:44:49,690
- [Man In Tie-Dyed Shirt] Go head (speaks faintly).

00:44:49,690 --> 00:44:52,170
- I was wondering, how do you handle

00:44:54,200 --> 00:44:57,280
backwards compatibility on the user space side?

00:44:57,280 --> 00:44:59,270
- What do you mean? - How does FRR figure out

00:44:59,270 --> 00:45:02,070
if the kernel understands this?

00:45:02,070 --> 00:45:03,960
Like, if you run the same FRR code

00:45:03,960 --> 00:45:05,020
on an old kernel. (speaks faintly)

00:45:05,020 --> 00:45:06,010
- Yes.

00:45:06,010 --> 00:45:11,010
You can do an RTM nexthop, a new nexthop dump,

00:45:12,210 --> 00:45:14,920
and if the kernel comes back and says, huh,

00:45:14,920 --> 00:45:16,577
then it doesn't understand this API.

00:45:16,577 --> 00:45:18,520
- Okay, and how much overhead,

00:45:18,520 --> 00:45:20,300
do you know, in the FRR code?

00:45:20,300 --> 00:45:21,550
How much, how difficult is it

00:45:21,550 --> 00:45:23,557
to support both, I guess? - We talked about the fact

00:45:23,557 --> 00:45:25,220
that they need a runtime switch

00:45:25,220 --> 00:45:28,270
because the same FRR has to run on both kernels,

00:45:28,270 --> 00:45:31,550
and the question's gonna be how to they implement that.

00:45:31,550 --> 00:45:33,193
I don't know, I'd have to ask.

00:45:34,310 --> 00:45:37,740
- Okay, I accumulated a pile of questions 'cause...

00:45:37,740 --> 00:45:39,280
- [David] (chuckling) All right.

00:45:39,280 --> 00:45:42,460
- I have major issues with atomic route updates in general

00:45:42,460 --> 00:45:44,710
and this solves some of those for me,

00:45:44,710 --> 00:45:46,913
not enough of them, but that's okay.

00:45:48,640 --> 00:45:52,833
Going backwards, how do I know the next nexthop ID?

00:45:53,910 --> 00:45:56,120
- When you start up, just like you do a route dump,

00:45:56,120 --> 00:45:57,833
you would do a nexthop dump.

00:45:58,720 --> 00:45:59,760
That would give you anything

00:45:59,760 --> 00:46:01,790
that's already been configured in the kernel.

00:46:01,790 --> 00:46:05,330
You would then get that, the entire history of, you know,

00:46:05,330 --> 00:46:07,270
like all the objects by ID,

00:46:07,270 --> 00:46:08,180
- So, it's not-- - and you would maintain

00:46:08,180 --> 00:46:09,920
a cache, if that's what you--

00:46:09,920 --> 00:46:11,870
- Well, I have multiple sources of routes.

00:46:11,870 --> 00:46:15,070
I have RA, I have in my case the Babel daemon,

00:46:15,070 --> 00:46:17,050
- Sure. - I have other stuff.

00:46:17,050 --> 00:46:19,380
So, the IDs are sequential

00:46:19,380 --> 00:46:21,200
and then totally in their own space

00:46:21,200 --> 00:46:23,833
or are they per scope, I mean, per protocol?

00:46:24,820 --> 00:46:26,580
- Now, what you're saying is something different.

00:46:26,580 --> 00:46:29,420
This gets into user space daemons.

00:46:29,420 --> 00:46:31,580
When I showed that hybrid NOS architecture

00:46:31,580 --> 00:46:34,060
where you could have multiple coming into a FIB,

00:46:34,060 --> 00:46:36,640
that's kind of like what FRR has with Zebra,

00:46:36,640 --> 00:46:40,990
is the FIB process, and you could have OSPF or BGP or RIP

00:46:40,990 --> 00:46:42,600
or these other daemons running,

00:46:42,600 --> 00:46:44,780
and they send all that information to Zebra,

00:46:44,780 --> 00:46:47,230
and Zebra has to say, well, I'm only gonna send

00:46:47,230 --> 00:46:49,130
certain things down to the kernel,

00:46:49,130 --> 00:46:53,120
let me create a unique nexthop, I'll assign my ID to it,

00:46:53,120 --> 00:46:55,020
'cause the kernel's just a dummy database, right,

00:46:55,020 --> 00:46:57,290
it doesn't do any kind of logic like that,

00:46:57,290 --> 00:46:59,250
and FRR consolidates that,

00:46:59,250 --> 00:47:01,680
says here's my set of unique nexthops,

00:47:01,680 --> 00:47:03,470
shoves that information to the kernel,

00:47:03,470 --> 00:47:05,280
and then puts those prefixes into the kernel

00:47:05,280 --> 00:47:07,110
referencing those IDs.

00:47:07,110 --> 00:47:08,850
- I get that. - That's a user space problem,

00:47:08,850 --> 00:47:09,683
not a kernel problem. - Yes.

00:47:09,683 --> 00:47:13,480
Well, it's a monolithic number space and I usually have

00:47:13,480 --> 00:47:14,484
five or six daemons - It is per namespace

00:47:14,484 --> 00:47:16,200
- conflicting with each other. - and that's because

00:47:16,200 --> 00:47:19,580
the routes themselves, when you go to reference a route,

00:47:19,580 --> 00:47:20,720
you would insert that route

00:47:20,720 --> 00:47:23,030
and you'd have an ID pointing to an object,

00:47:23,030 --> 00:47:25,750
and so, what gets created is, like I said,

00:47:25,750 --> 00:47:28,830
that struct nexthop* points to the actual object,

00:47:28,830 --> 00:47:31,400
so you can't have two with the same ID.

00:47:31,400 --> 00:47:32,786
- [Toki] The problem is if the user space daemons

00:47:32,786 --> 00:47:33,955
- So, there is-- - are not coordinating.

00:47:33,955 --> 00:47:35,770
- [David] Hold on, one second.

00:47:35,770 --> 00:47:39,113
There is a protocol attribute so that you can say,

00:47:40,260 --> 00:47:43,493
here's the list of nexthops created by Zebra, for example.

00:47:44,550 --> 00:47:47,940
You can do protocol listings but not by ID.

00:47:47,940 --> 00:47:49,930
The ID itself is still universal

00:47:49,930 --> 00:47:51,140
for that numbered namespace.

00:47:51,140 --> 00:47:53,010
- The longstanding bug I've been dealing with

00:47:53,010 --> 00:47:54,486
is you can't do a route replace

00:47:54,486 --> 00:47:57,851
that's specific to the protocol in IPv6.

00:47:57,851 --> 00:47:58,821
- [David] Hold on. (chuckles)

00:47:58,821 --> 00:48:00,213
- [Man In Tie-Dyed Shirt] Go ahead, Toki.

00:48:00,213 --> 00:48:02,420
- I think also the problem is,

00:48:02,420 --> 00:48:03,860
if you have two user space daemons

00:48:03,860 --> 00:48:05,860
that are not coordinating, - Yes.

00:48:05,860 --> 00:48:10,190
- do they just grab a range of IDs

00:48:10,190 --> 00:48:11,590
and say I'm gonna use these?

00:48:12,558 --> 00:48:14,520
What if they step on each other's toes?

00:48:14,520 --> 00:48:17,340
- Sure, that is possible, and they'll have to figure out

00:48:17,340 --> 00:48:20,190
if that nexthop ID is already in use, right?

00:48:20,190 --> 00:48:22,890
So, what I would expect is that these daemons, at least,

00:48:22,890 --> 00:48:25,660
would do a dump and they would see what already exists.

00:48:25,660 --> 00:48:27,700
Because if that specification already exists,

00:48:27,700 --> 00:48:30,140
again, typically, there's only so many egress devices,

00:48:30,140 --> 00:48:32,330
there's only so many nexthops,

00:48:32,330 --> 00:48:34,070
individual nexthops. - How big is the ID field?

00:48:34,070 --> 00:48:34,903
- I'm sorry?

00:48:34,903 --> 00:48:36,620
- [Toki] How many bits is the ID field?

00:48:36,620 --> 00:48:37,853
- 32, u32.

00:48:40,750 --> 00:48:42,700
You were asking about v6 replace.

00:48:42,700 --> 00:48:45,350
Yes, that is a known pain point for multipath routes.

00:48:48,000 --> 00:48:51,230
- Could we have a nexthop via a MAC address

00:48:51,230 --> 00:48:54,213
instead of via some sort of IP address?

00:48:55,900 --> 00:48:57,920
- That would be an easy extension, I think,

00:48:57,920 --> 00:48:59,330
the way this is set up,

00:48:59,330 --> 00:49:01,330
because ultimately that's what you care about

00:49:01,330 --> 00:49:03,100
from a hardware perspective,

00:49:03,100 --> 00:49:05,720
but what you're doing is bypassing

00:49:05,720 --> 00:49:07,290
neighbor discovery as well.

00:49:07,290 --> 00:49:08,850
The network address,

00:49:08,850 --> 00:49:11,540
it being a network entry in a network address,

00:49:11,540 --> 00:49:14,363
and that gets resolved to then a link address,

00:49:15,360 --> 00:49:20,360
to bypass that and go straight to this prefix uses this MAC,

00:49:20,390 --> 00:49:24,683
I mean, I could, software can do anything, right?

00:49:27,240 --> 00:49:30,940
- [Audience Member] David, for the backward compat stuff,

00:49:30,940 --> 00:49:33,800
would it be cheaper to introduce another group?

00:49:33,800 --> 00:49:34,780
- Another... - Route group,

00:49:34,780 --> 00:49:35,923
multi-class group?

00:49:37,510 --> 00:49:40,483
Newer group so only listeners were listening on.

00:49:42,120 --> 00:49:45,090
- This gets into the route notifications,

00:49:45,090 --> 00:49:47,310
not the nexthop notifications.

00:49:47,310 --> 00:49:48,570
- Yes. - And so, now,

00:49:48,570 --> 00:49:52,210
what you're saying is send a message twice.

00:49:52,210 --> 00:49:56,330
I think what you would see is there's a huge performance hit

00:49:56,330 --> 00:49:58,024
to doing something twice. - To do it twice?

00:49:58,024 --> 00:50:01,380
Even if there are no listeners on the,

00:50:01,380 --> 00:50:03,230
- But see, the notification - do we create--

00:50:03,230 --> 00:50:04,650
- code that generates the message

00:50:04,650 --> 00:50:07,000
has no idea that there's no listeners.

00:50:07,000 --> 00:50:09,970
That's actually something I would like to see,

00:50:09,970 --> 00:50:11,730
some kind of way of tracking that says,

00:50:11,730 --> 00:50:13,072
nobody cares, don't even generate

00:50:13,072 --> 00:50:14,580
the message. - Don't even generate.

00:50:14,580 --> 00:50:15,413
- Yeah.

00:50:18,250 --> 00:50:20,380
- You've shown the performance improvements

00:50:20,380 --> 00:50:23,960
on the route exertion.

00:50:23,960 --> 00:50:26,240
Does it have any effect on route lookup?

00:50:26,240 --> 00:50:28,467
- On? - Route lookup.

00:50:28,467 --> 00:50:29,584
- Does-- - On route lookup.

00:50:29,584 --> 00:50:30,650
- Yeah.

00:50:30,650 --> 00:50:32,593
- No measurable performance on my end.

00:50:34,790 --> 00:50:37,830
There theoretically should be some small amount

00:50:37,830 --> 00:50:40,220
but the testing that I've done,

00:50:40,220 --> 00:50:43,120
and it was a performance benchmark

00:50:43,120 --> 00:50:45,530
that I think Dave started years back

00:50:45,530 --> 00:50:48,130
that Vincent Bernat made some changes

00:50:48,130 --> 00:50:49,760
when he did his blog post,

00:50:49,760 --> 00:50:52,820
and then I took that and made some simplifications.

00:50:52,820 --> 00:50:54,113
When I was running those,

00:50:55,970 --> 00:50:59,493
the performance variation run to run was,

00:51:00,870 --> 00:51:02,770
the changes to this nexthop was within

00:51:02,770 --> 00:51:05,340
that performance variation that I see run to run,

00:51:05,340 --> 00:51:08,500
so it's, there probably is some small amount

00:51:08,500 --> 00:51:10,530
but it seems to be in the noise overall.

00:51:10,530 --> 00:51:11,690
- [Host] You're talking about route bench, right,

00:51:11,690 --> 00:51:13,060
RT bench or whatever I called it?

00:51:13,060 --> 00:51:14,030
- Yeah. - Route bench, yeah.

00:51:14,030 --> 00:51:14,863
- Yes.

00:51:14,863 --> 00:51:16,020
- [Host] Okay, we're way past,

00:51:16,020 --> 00:51:17,820
and to respect the next presenter,

00:51:17,820 --> 00:51:19,010
we think that's the last question.

00:51:19,010 --> 00:51:20,813
Thank you very much, David.

00:51:20,813 --> 00:51:22,613

YouTube URL: https://www.youtube.com/watch?v=HIqvUiwDHGk


