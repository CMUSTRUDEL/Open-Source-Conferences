Title: LPC2019 - Traffic footprint characterization of workloads using BPF
Publication date: 2019-11-18
Playlist: LPC2019 - Networking Summit Track
Description: 
	Traffic footprint characterization of workloads using BPF

Speaker
 Aditi Ghag (VMware)

Description
Application workloads are becoming increasingly diverse in terms of their network resource requirements and performance characteristics. As opposed to long running monoliths deployed in virtual machines, containerized workloads can be as short lived as few seconds. Today, container orchestrators that schedule these workloads primarily consider their CPU and memory resource requirements since they can easily be quantified. However, network resources characterization isn’t as straight forward. Ineffective scheduling of containerized workloads, which could be throughput intensive or latency sensitive, can lead to adverse network performance. Hence, I propose characterizing and learning network footprints of applications running in a cluster, which can be used while scheduling them in containers/VMs such that their network performance can be improved.

There is a well-known network issue, which is achieving low latency for mice flows (those that send relatively small amounts of data) by separating them from the elephant flows (those that send a lot of data). I’ve written an eBPF program in C that runs at various hook points in the Linux connection tracking (aka conntrack) kernel functions in order to detect network elephant flow, and attribute them to the container or VM, where the flows ingress or egress from. The agent that loads this eBPF program from user space runs in every host in a cluster. It then feeds this learnt information to a container (or VM) scheduling system such that they can use this information proactively, while scheduling containerized workloads with light network footprint (e.g., microservices, functions) and heavy network footprint (e.g., data analytics, data computational applications) on the same cluster, in order to improve their latency and throughput, respectively.

eBPF facilitates running the programs with minimal CPU overhead, in a pluggable, tunable and safe manner, and without having to change any kernel code. It’s also worthwhile to discuss how the workload’s learnt network footprint can be used for dynamically allocating or tuning Linux network resources like bandwidth, vcpu/vhost-net allocation, receive-side scaling (RSS) queue mappings, etc.
I'll submit a paper with the (working) source code snippets and details if the talk is accepted.
Captions: 
	00:00:00,890 --> 00:00:03,030
- Hi everyone, I'm Aditi Ghag.

00:00:03,030 --> 00:00:03,910
I'm a software engineer

00:00:03,910 --> 00:00:06,860
in the Network Virtualization Team at VMware.

00:00:06,860 --> 00:00:08,820
And I'm going to talk to you about a framework

00:00:08,820 --> 00:00:12,750
that uses eBPF to identify network characteristics

00:00:12,750 --> 00:00:14,653
of application workloads.

00:00:15,500 --> 00:00:17,080
The title is a bit of a mouthful,

00:00:17,080 --> 00:00:19,070
but I'll explain it with the help of a use case

00:00:19,070 --> 00:00:20,873
over the course of my talk.

00:00:23,400 --> 00:00:26,066
Network resource requirements

00:00:26,066 --> 00:00:27,760
quantifying network resource requirements

00:00:27,760 --> 00:00:30,490
of application workloads is a challenging task,

00:00:30,490 --> 00:00:32,630
because network is a highly dynamic

00:00:32,630 --> 00:00:35,810
and distributed resource.

00:00:35,810 --> 00:00:38,180
The spectrum of applications that are getting deployed

00:00:38,180 --> 00:00:42,440
in data centers and cloud environments today is widening.

00:00:42,440 --> 00:00:44,040
And, so, getting visibility

00:00:44,040 --> 00:00:47,510
into network characteristics of these application workloads

00:00:47,510 --> 00:00:50,550
will enable intelligent placement of these workloads

00:00:50,550 --> 00:00:52,550
in containers and VM.

00:00:52,550 --> 00:00:55,470
So, this work takes a stab at making network

00:00:55,470 --> 00:01:00,090
a first-class citizen in areas like container placement.

00:01:00,090 --> 00:01:02,827
So, I'll start off by talking about the motivation

00:01:02,827 --> 00:01:04,970
and the scheduling use case.

00:01:04,970 --> 00:01:07,490
We'll then take a look at how eBPF is used

00:01:07,490 --> 00:01:11,910
to track resource consumption at a granular level.

00:01:11,910 --> 00:01:13,420
And, then, in the end,

00:01:13,420 --> 00:01:16,210
we'll take a look at how this enhanced network information

00:01:16,210 --> 00:01:19,450
is used to augment container schedulers.

00:01:19,450 --> 00:01:21,030
And, finally, we'll take a look

00:01:21,030 --> 00:01:24,523
at some of the other potential use cases of the framework.

00:01:25,810 --> 00:01:27,920
So, data centers and cloud environments today

00:01:27,920 --> 00:01:30,900
host applications that are quite diverse

00:01:30,900 --> 00:01:33,700
in terms of their network resource requirements

00:01:33,700 --> 00:01:35,300
and performance characteristics.

00:01:36,670 --> 00:01:38,590
So, there are latency sensitive applications

00:01:38,590 --> 00:01:42,920
like web search queries, user-facing eCommerce website,

00:01:42,920 --> 00:01:46,090
or there is like in memory key value stores.

00:01:46,090 --> 00:01:48,850
And their latency requirements are quite stringent,

00:01:48,850 --> 00:01:50,590
and these latency requirements are

00:01:50,590 --> 00:01:52,540
usually defined by easy load terms

00:01:52,540 --> 00:01:56,310
like 99th percentile response latency.

00:01:56,310 --> 00:01:59,430
On the other hand, you have data analytics,

00:01:59,430 --> 00:02:03,160
Hadoop-like workloads that are quite throughput intensive.

00:02:03,160 --> 00:02:04,620
And the throughput requirements are

00:02:04,620 --> 00:02:08,420
usually satisfied in today's virtualized environments

00:02:08,420 --> 00:02:13,420
with any capacities greater than 10 gigs per second.

00:02:13,530 --> 00:02:14,720
On the other hand,

00:02:14,720 --> 00:02:16,290
on the other side of the spectrum

00:02:16,290 --> 00:02:18,060
you have functions like workloads

00:02:18,060 --> 00:02:21,880
that can be as short-lived as few seconds or few minutes.

00:02:21,880 --> 00:02:24,470
And, then, finally, you have microservice applications,

00:02:24,470 --> 00:02:28,660
which are highly distributed and communication intensive.

00:02:28,660 --> 00:02:32,650
So, the individual network latency for inter-services calls

00:02:32,650 --> 00:02:33,920
cumulatively contribute

00:02:33,920 --> 00:02:36,993
to end-to-end or application response time.

00:02:39,100 --> 00:02:41,520
There has been a recent trend in the industry

00:02:41,520 --> 00:02:44,160
where these applications are getting containerized,

00:02:44,160 --> 00:02:45,350
and they get deployed

00:02:45,350 --> 00:02:49,770
by a container orchestration platform, like Kubernetes.

00:02:49,770 --> 00:02:51,710
And an effective resource scheduler needs

00:02:51,710 --> 00:02:55,120
to account for network characteristics of these workloads,

00:02:55,120 --> 00:02:58,023
so that their performance requirements are satisfied.

00:03:00,200 --> 00:03:03,450
So, I'll take a quite detour and give an oversimplified

00:03:03,450 --> 00:03:06,223
and a brief introduction to Kubernetes scheduling.

00:03:07,400 --> 00:03:08,290
Kubernetes is one of the

00:03:08,290 --> 00:03:11,663
most widely used container orchestrated platforms.

00:03:12,570 --> 00:03:15,610
To set the context, Kubernetes cluster is formed

00:03:15,610 --> 00:03:18,130
from a master node and a pool of worker nodes.

00:03:18,130 --> 00:03:22,070
And in most deployments these nodes are virtual machines.

00:03:22,070 --> 00:03:25,300
The master node runs Kubernetes' controlling components,

00:03:25,300 --> 00:03:27,140
including a scheduler.

00:03:27,140 --> 00:03:29,230
And, excuse me,

00:03:29,230 --> 00:03:32,570
this scheduler is tasked with scheduling a

00:03:32,570 --> 00:03:33,950
newly deployed container

00:03:33,950 --> 00:03:37,050
on one of the available worker nodes.

00:03:37,050 --> 00:03:39,600
And the scheduling decisions are made based

00:03:39,600 --> 00:03:42,330
on available capacity on worker nodes

00:03:42,330 --> 00:03:44,240
for resources like CPU,

00:03:44,240 --> 00:03:47,540
memory, disk, and some policy constraints.

00:03:47,540 --> 00:03:49,250
So, when a new part gets deployed

00:03:49,250 --> 00:03:51,870
a user can specify how many CPU cores

00:03:51,870 --> 00:03:53,960
or memory the pod needs.

00:03:53,960 --> 00:03:56,620
And based on these requirements the scheduler will try

00:03:56,620 --> 00:03:57,900
to match these requirements

00:03:57,900 --> 00:04:01,063
with the available capacity on worker nodes.

00:04:01,970 --> 00:04:04,770
So, as we saw in the first slide,

00:04:04,770 --> 00:04:07,550
most of these applications rely on network

00:04:07,550 --> 00:04:10,660
for getting their performance requirements satisfied.

00:04:10,660 --> 00:04:13,550
The question is how do we add network awareness

00:04:13,550 --> 00:04:15,630
to the scheduler.

00:04:15,630 --> 00:04:20,290
Now, resources like CPU and memory can be easily quantified,

00:04:20,290 --> 00:04:22,803
but that's not the case with network.

00:04:24,690 --> 00:04:26,750
So, with this I'll talk about

00:04:26,750 --> 00:04:29,320
how we can characterize workloads based

00:04:29,320 --> 00:04:30,883
on their network footprint.

00:04:32,110 --> 00:04:35,160
I focus on the well-known issue in networking,

00:04:35,160 --> 00:04:38,720
which is how to achieve low latency for mice flows

00:04:38,720 --> 00:04:40,720
in presence of elephant flows.

00:04:40,720 --> 00:04:42,690
So, elephant flows are these long-lived,

00:04:42,690 --> 00:04:46,210
throughput-intensive flows versus mice flows,

00:04:46,210 --> 00:04:49,760
which are short-lived and latency sensitive.

00:04:49,760 --> 00:04:53,730
By nature, elephant flows tend to fill up network buffers,

00:04:53,730 --> 00:04:58,140
which in turn causes packet drops and queuing delays.

00:04:58,140 --> 00:05:01,803
And this ends up increasing the tail latency of mice flows.

00:05:02,980 --> 00:05:05,030
So, we use this characterization

00:05:05,030 --> 00:05:09,700
to define workloads running in containers or VMs

00:05:09,700 --> 00:05:11,810
that source or sink elephant flows

00:05:11,810 --> 00:05:14,280
as the ones with heavy network footprint.

00:05:14,280 --> 00:05:15,800
And the other workloads

00:05:15,800 --> 00:05:17,973
tend to have light network footprint.

00:05:25,740 --> 00:05:28,700
To get a sense of how elephant flows affect mice flows,

00:05:28,700 --> 00:05:30,610
I kind of did some experiments.

00:05:30,610 --> 00:05:33,800
So, I have two KVM hypervisors here on top.

00:05:33,800 --> 00:05:35,750
I have installed a Kubernetes cluster

00:05:35,750 --> 00:05:37,430
with five worker nodes,

00:05:37,430 --> 00:05:40,540
and then I deployed sockperf

00:05:40,540 --> 00:05:43,760
and iperf applications in parts.

00:05:43,760 --> 00:05:46,640
The sockperf benchmark utilities used

00:05:46,640 --> 00:05:50,460
to generate mice flows, and iperf generate elephant flows.

00:05:50,460 --> 00:05:53,880
And then this sockperf client makes a connection request

00:05:53,880 --> 00:05:54,880
to sockperf server,

00:05:54,880 --> 00:05:57,590
and I measure the 99th percentile latency

00:05:57,590 --> 00:05:59,580
for the sockperf client.

00:05:59,580 --> 00:06:03,170
And in the baseline scenario, in the absence

00:06:03,170 --> 00:06:04,076
sorry.

00:06:04,076 --> 00:06:06,318
- [Moderator] It's in front of the mic.

00:06:06,318 --> 00:06:07,257
Check.

00:06:07,257 --> 00:06:08,090
Thanks.

00:06:08,944 --> 00:06:10,710
- So, in the baseline scenario,

00:06:10,710 --> 00:06:13,620
the 99th percentile for sockperf client

00:06:13,620 --> 00:06:15,850
is around 0.28 milliseconds.

00:06:15,850 --> 00:06:17,940
And as we play around with the placement

00:06:17,940 --> 00:06:19,910
of elephant flow parts,

00:06:19,910 --> 00:06:22,540
we can see that the latency goes on increasing.

00:06:22,540 --> 00:06:24,220
And in the worst case scenario

00:06:24,220 --> 00:06:28,020
when the sockperf client, iperf clients are deployed

00:06:28,020 --> 00:06:30,190
on worker nodes, then hypervisor one.

00:06:30,190 --> 00:06:32,600
And they're communicating with their respective servers

00:06:32,600 --> 00:06:35,400
on worker nodes deployed on hypervisor two.

00:06:35,400 --> 00:06:37,920
The latency is 50 times

00:06:37,920 --> 00:06:41,260
than that of the baseline scenario.

00:06:41,260 --> 00:06:43,373
So, this just validates the theory.

00:06:45,750 --> 00:06:47,980
So, with this background,

00:06:47,980 --> 00:06:50,090
I'll move on to my second part of the presentation

00:06:50,090 --> 00:06:55,090
is to how we can detect and map elephant flows in end hosts.

00:06:55,920 --> 00:06:59,010
So, there have been past solutions

00:06:59,010 --> 00:07:04,010
that propose solutions that can detect elephant flows.

00:07:05,380 --> 00:07:08,410
These solutions are based on some kind of sampling

00:07:08,410 --> 00:07:10,240
or in-network mechanisms,

00:07:10,240 --> 00:07:12,723
where they use protocols like NetFlow or sFlow,

00:07:13,600 --> 00:07:17,500
or you can just have this detection logic implemented

00:07:17,500 --> 00:07:19,600
in network switches directly.

00:07:19,600 --> 00:07:21,240
But these solutions mainly focus

00:07:21,240 --> 00:07:23,310
on detecting elephant flows,

00:07:23,310 --> 00:07:27,540
and not so much on locating their sources or sinks.

00:07:27,540 --> 00:07:30,830
So, I propose that we detect

00:07:30,830 --> 00:07:34,290
or we implement this logic in end hosts or hypervisors

00:07:34,290 --> 00:07:36,410
to not only detect elephant flows

00:07:36,410 --> 00:07:38,250
but we can easily map them

00:07:38,250 --> 00:07:41,810
to containers or VMs that generate these.

00:07:41,810 --> 00:07:43,780
So, excuse me,

00:07:43,780 --> 00:07:45,380
with the traditional approaches,

00:07:45,380 --> 00:07:46,693
there are some drawbacks.

00:07:47,810 --> 00:07:50,810
With the latter part of our objective,

00:07:50,810 --> 00:07:53,690
which is being able to attribute an elephant flow

00:07:53,690 --> 00:07:55,530
to its source.

00:07:55,530 --> 00:07:59,423
If you were to detect endpoints, containers or VMS,

00:07:59,423 --> 00:08:00,810
that generate flows,

00:08:00,810 --> 00:08:03,320
we would have to keep track of IP addresses

00:08:03,320 --> 00:08:05,190
associated with workloads.

00:08:05,190 --> 00:08:08,210
And that might not scale well with environments

00:08:08,210 --> 00:08:11,820
in containers which are already dynamic

00:08:11,820 --> 00:08:13,373
and highly scalable.

00:08:15,521 --> 00:08:17,650
And, moreover, in overly environments

00:08:17,650 --> 00:08:20,440
the application traffic is encapsulated

00:08:20,440 --> 00:08:22,620
in Geneve or VXLAN headers,

00:08:22,620 --> 00:08:24,500
so the network switches would have

00:08:24,500 --> 00:08:26,500
to then parse

00:08:26,500 --> 00:08:29,430
extra contexts like virtual network identifiers

00:08:29,430 --> 00:08:32,180
to keep track of flows

00:08:32,180 --> 00:08:35,030
and their corresponding containers or VMs.

00:08:35,030 --> 00:08:39,210
So, that's why we implement this logic

00:08:39,210 --> 00:08:42,280
in hypervisors where they are more closer

00:08:42,280 --> 00:08:45,790
to application and have more context.

00:08:45,790 --> 00:08:47,900
So, the takeaway from this slide is

00:08:47,900 --> 00:08:49,770
that we have two objectives.

00:08:49,770 --> 00:08:53,670
First, we wanna learn workload network footprint,

00:08:53,670 --> 00:08:56,310
and the workloads that generate elephant flows

00:08:56,310 --> 00:08:58,340
tend to have heavy network footprint.

00:08:58,340 --> 00:08:59,680
And the second objective is

00:08:59,680 --> 00:09:01,720
we wanna map these elephant flows

00:09:01,720 --> 00:09:03,930
at the infrastructure node level,

00:09:03,930 --> 00:09:06,430
so that a scheduler can

00:09:06,430 --> 00:09:09,390
easily isolate a heavy network footprint

00:09:09,390 --> 00:09:11,193
away from light network footprint.

00:09:13,360 --> 00:09:15,490
So, with this background,

00:09:15,490 --> 00:09:18,120
we'll take a look at how eBPF can be used

00:09:18,120 --> 00:09:21,260
to achieve these goals.

00:09:21,260 --> 00:09:23,940
Linux kernel networking stack has a feature

00:09:23,940 --> 00:09:27,970
called connection tracking which is Conntrack for short.

00:09:27,970 --> 00:09:30,770
When it's enabled Conntrack tracks lifecycle

00:09:30,770 --> 00:09:32,603
of every flow in a system.

00:09:34,060 --> 00:09:34,893
And

00:09:36,420 --> 00:09:39,640
eBPF allows us to run user-supplied programs

00:09:39,640 --> 00:09:40,473
inside of kernel.

00:09:40,473 --> 00:09:44,883
And this audience hardly needs any introduction to eBPF.

00:09:46,000 --> 00:09:48,500
But, essentially, we use eBPF programs

00:09:48,500 --> 00:09:52,873
to trace events happening in Conntrack kernel module.

00:09:54,380 --> 00:09:55,970
So, here are some of the data structures

00:09:55,970 --> 00:09:58,020
that the eBPF program uses.

00:09:58,020 --> 00:10:01,150
The eBPF programs creates a BPF hash map

00:10:01,150 --> 00:10:04,800
to keep track of additional flow attributes.

00:10:04,800 --> 00:10:09,800
And then these are the BPF hash map entry flow_key and flow

00:10:10,410 --> 00:10:11,760
the value.

00:10:11,760 --> 00:10:13,580
So, on the left-hand side it's a flow_key,

00:10:13,580 --> 00:10:14,980
which just defines a part

00:10:14,980 --> 00:10:16,880
that we use to identify a flow.

00:10:16,880 --> 00:10:19,860
And then the flow_stats has additional flow attributes.

00:10:19,860 --> 00:10:22,380
So, the first member is an interface.

00:10:22,380 --> 00:10:26,290
This is an interface that a flow originates from.

00:10:26,290 --> 00:10:28,920
The next field is a time stamp value

00:10:28,920 --> 00:10:32,490
that we use to keep track of flow duration.

00:10:32,490 --> 00:10:34,440
The third one is a zone identifier,

00:10:34,440 --> 00:10:36,530
and I'll explain why this is needed

00:10:36,530 --> 00:10:38,310
in just a couple of slides.

00:10:38,310 --> 00:10:41,220
And the final field is a flag

00:10:41,220 --> 00:10:43,773
that indicates whether a flow is an elephant flow.

00:10:45,430 --> 00:10:48,250
So, let's start through the different hook points

00:10:48,250 --> 00:10:50,947
that we add in Conntrack.

00:10:52,720 --> 00:10:55,460
So, the first hook point is executed

00:10:55,460 --> 00:10:58,430
when a new flow or related events happen.

00:10:58,430 --> 00:11:01,910
So, we are tracing calls to Conntrack function

00:11:01,910 --> 00:11:03,440
that delivers cached events.

00:11:03,440 --> 00:11:04,870
And, now, you must be wondering

00:11:04,870 --> 00:11:07,260
why I didn't use a Conntrack function

00:11:07,260 --> 00:11:11,100
that simply adds a flow to a hash map.

00:11:11,100 --> 00:11:14,730
It's because first I want to keep track

00:11:14,730 --> 00:11:17,250
of flows that are committed.

00:11:17,250 --> 00:11:20,190
And the second is it's sort of an optimization

00:11:20,190 --> 00:11:24,060
where we only look for flows that are assured.

00:11:24,060 --> 00:11:27,040
And Conntrack defines an assured flow

00:11:27,040 --> 00:11:30,010
as a flow that is seeing traffic in both the directions.

00:11:30,010 --> 00:11:34,100
So, this helps in filtering out the stray traffic.

00:11:34,100 --> 00:11:36,720
So, in this routine we create a flow entry

00:11:36,720 --> 00:11:41,023
in the BPF hash map that we maintain in the program.

00:11:42,050 --> 00:11:44,120
The second hook point is executed

00:11:44,120 --> 00:11:46,190
when flow statistics are updated.

00:11:46,190 --> 00:11:49,220
So, we are tracing calls to Conntrack function

00:11:49,220 --> 00:11:50,890
that does accounting.

00:11:50,890 --> 00:11:53,580
And in this function

00:11:53,580 --> 00:11:56,730
we'll first parse kernel data structures.

00:11:56,730 --> 00:11:59,210
So, from the skb data structure

00:11:59,210 --> 00:12:01,680
we'll parse the interface name

00:12:01,680 --> 00:12:03,030
from the Conntrack function,

00:12:03,030 --> 00:12:05,270
so we get the zone identifier.

00:12:05,270 --> 00:12:08,050
And then based on the user-defined threshold

00:12:08,050 --> 00:12:10,880
for flow duration and bytes consumed,

00:12:10,880 --> 00:12:14,810
this routine determines if flow is an elephant flow.

00:12:14,810 --> 00:12:18,450
Once it detects an elephant flow,

00:12:18,450 --> 00:12:20,470
it'll mark the elephant flow flag

00:12:20,470 --> 00:12:23,060
in the flow_stats structure at the bottom.

00:12:23,060 --> 00:12:25,930
And then it'll generate an event to user space,

00:12:25,930 --> 00:12:26,810
and for that we use

00:12:26,810 --> 00:12:29,693
this add_mapped_elephant_flows BPF table.

00:12:31,390 --> 00:12:33,980
Again, the flow_stats structure

00:12:35,429 --> 00:12:37,920
has information like interface name

00:12:37,920 --> 00:12:40,580
that helps us in locating

00:12:40,580 --> 00:12:44,950
which VM interface this flow came from.

00:12:44,950 --> 00:12:46,090
Excuse me.

00:12:46,090 --> 00:12:48,420
And, finally, the third hook point executes

00:12:48,420 --> 00:12:49,820
when a flow is a deleted.

00:12:49,820 --> 00:12:52,773
So, in this routine,

00:12:52,773 --> 00:12:55,860
if the flow is marked as an elephant flow,

00:12:55,860 --> 00:12:59,510
then this routine will generate a delete elephant flow event

00:12:59,510 --> 00:13:00,610
to user space.

00:13:00,610 --> 00:13:05,560
And to further clear out this entry from BPF map.

00:13:05,560 --> 00:13:09,190
Again, we use a different BPF table called

00:13:09,190 --> 00:13:11,530
let's delete mapped elephant flows table

00:13:11,530 --> 00:13:14,680
to relay the information to user space.

00:13:14,680 --> 00:13:17,580
And the flow_stats will indicate which VM interface

00:13:17,580 --> 00:13:20,033
that this flow originated from.

00:13:22,150 --> 00:13:25,750
So, with this program we have achieved our first goal,

00:13:25,750 --> 00:13:29,310
which is to not only detect elephant flows,

00:13:29,310 --> 00:13:31,440
but map them to VM

00:13:31,440 --> 00:13:35,580
from where these elephant flows come from.

00:13:35,580 --> 00:13:38,940
But if you remember I had another goal,

00:13:38,940 --> 00:13:42,170
which is to learn workload network footprint.

00:13:42,170 --> 00:13:45,390
And for that we use Conntrack Zones.

00:13:45,390 --> 00:13:47,560
So, if there are multiple containers

00:13:47,560 --> 00:13:49,080
running inside a VM,

00:13:49,080 --> 00:13:51,760
they share the VM's common interface.

00:13:51,760 --> 00:13:55,260
And in order for us to be able to attribute an elephant flow

00:13:55,260 --> 00:13:57,740
to one of these containers,

00:13:57,740 --> 00:13:59,650
we need to know

00:13:59,650 --> 00:14:02,140
excuse me, we need to know which elephant flow

00:14:02,140 --> 00:14:05,040
sorry, which container a particular elephant flow

00:14:05,040 --> 00:14:06,330
originated from.

00:14:06,330 --> 00:14:08,790
So, we use Conntrack Zone identifiers

00:14:08,790 --> 00:14:11,460
to identify a particular container.

00:14:11,460 --> 00:14:16,193
Conntrack zones are used for namespacing and fairness.

00:14:17,380 --> 00:14:21,640
So, our eBPF hooks store these Conntrack Zone identifiers

00:14:21,640 --> 00:14:23,120
in the BPF map,

00:14:23,120 --> 00:14:25,890
and then once they detect an elephant flow,

00:14:25,890 --> 00:14:29,420
they use this I.D. to locate the container,

00:14:29,420 --> 00:14:32,020
and then the workload running inside the container

00:14:32,020 --> 00:14:33,630
is then marked as one

00:14:33,630 --> 00:14:35,353
with heavy network footprint.

00:14:36,540 --> 00:14:40,920
So, with this foundation of eBPF tracing with Conntrack,

00:14:40,920 --> 00:14:45,160
let's see how we can make container scheduler,

00:14:45,160 --> 00:14:47,963
which is Kubernetes in this case, network aware.

00:14:49,460 --> 00:14:50,830
Excuse me.

00:14:50,830 --> 00:14:52,810
We explicitly tag workloads

00:14:52,810 --> 00:14:54,410
with network footprint information,

00:14:54,410 --> 00:14:56,850
so that when these workloads are redeployed

00:14:56,850 --> 00:14:58,940
in new containers,

00:14:58,940 --> 00:15:02,020
their network footprint information can be proactively used

00:15:02,020 --> 00:15:04,000
by scheduler to schedule them away

00:15:04,000 --> 00:15:06,180
from heavy network footprints.

00:15:06,180 --> 00:15:08,353
And let's see this in action.

00:15:09,730 --> 00:15:12,600
So, the Kubernetes cluster is marked in blue color,

00:15:12,600 --> 00:15:15,130
and then we have a data collector

00:15:15,130 --> 00:15:17,950
that runs on every hypervisor.

00:15:17,950 --> 00:15:20,102
It's a distributed agent.

00:15:20,102 --> 00:15:21,850
The data collector is a Python front-end

00:15:21,850 --> 00:15:24,850
that loads the eBPF program that we just saw.

00:15:24,850 --> 00:15:28,330
And then once the eBPF program

00:15:28,330 --> 00:15:32,010
creates add or delete elephant flows events,

00:15:32,010 --> 00:15:34,690
the data collector will store this information

00:15:34,690 --> 00:15:36,100
in its own cache.

00:15:36,100 --> 00:15:37,860
So, it'll aggregate this information

00:15:37,860 --> 00:15:41,070
and send an update to Kubernetes scheduler.

00:15:41,070 --> 00:15:43,588
The information that gets relayed to the scheduler

00:15:43,588 --> 00:15:45,990
has the network's

00:15:45,990 --> 00:15:47,390
has information like

00:15:48,440 --> 00:15:51,140
there are these worker nodes running on a hypervisor,

00:15:51,140 --> 00:15:54,648
and how many number of elephant flows there are

00:15:54,648 --> 00:15:56,440
per worker node.

00:15:56,440 --> 00:15:58,870
So, when a new part gets deployed

00:15:58,870 --> 00:16:00,970
Kubernetes scheduler will now filter

00:16:00,970 --> 00:16:04,180
and rank nodes based on CPU, memory,

00:16:04,180 --> 00:16:07,150
other resources, and the traffic footprint of part.

00:16:07,150 --> 00:16:10,120
So, if this part has a light network footprint,

00:16:10,120 --> 00:16:12,040
then based on the network information

00:16:12,040 --> 00:16:14,120
that we received from the data collector

00:16:14,120 --> 00:16:15,500
it'll try to schedule it

00:16:16,780 --> 00:16:18,050
for worker nodes

00:16:18,050 --> 00:16:22,700
away from worker nodes that have heavy network footprint,

00:16:22,700 --> 00:16:25,093
or more number of elephant flows running.

00:16:27,480 --> 00:16:30,150
So, this is just one use case that I described,

00:16:30,150 --> 00:16:33,450
but there could potentially be other use cases

00:16:33,450 --> 00:16:34,900
for the framework.

00:16:34,900 --> 00:16:37,340
So, extending the team of resource management

00:16:37,340 --> 00:16:38,640
we can use this framework

00:16:38,640 --> 00:16:41,010
to assign more number

00:16:41,010 --> 00:16:44,800
of receive-side RSS queues to workloads

00:16:44,800 --> 00:16:48,250
or workloads that generate elephant flows.

00:16:48,250 --> 00:16:51,900
We can assign more bandwidth to such workloads.

00:16:51,900 --> 00:16:54,100
And, then, we can use this framework

00:16:54,100 --> 00:16:56,170
to enable hardware offloading

00:16:56,170 --> 00:16:58,920
for elephant flows, specifically,

00:16:58,920 --> 00:17:00,510
because for mice flows

00:17:00,510 --> 00:17:03,480
depending on what kind of processing you're doing

00:17:03,480 --> 00:17:05,250
the overhead that you incur

00:17:05,250 --> 00:17:09,254
in calling the hardware or diver APIs

00:17:09,254 --> 00:17:12,400
might just add too much overhead.

00:17:12,400 --> 00:17:15,620
And, finally, there is a load-balancing scheme

00:17:15,620 --> 00:17:17,320
known as flowlet generation.

00:17:17,320 --> 00:17:20,970
This was a scheme that was proposed by MIT researchers

00:17:20,970 --> 00:17:21,983
a few year back.

00:17:22,920 --> 00:17:26,050
Essentially, the proposal is to split a TCP flow

00:17:26,050 --> 00:17:28,480
into smaller units known as flowlets.

00:17:28,480 --> 00:17:31,900
And these flowlets can be simultaneously sent

00:17:31,900 --> 00:17:35,230
onto multiple ECMP parts in the network.

00:17:35,230 --> 00:17:36,480
And the idea is to do this

00:17:36,480 --> 00:17:39,500
with as little reordering as possible.

00:17:39,500 --> 00:17:41,140
So, we can use this framework

00:17:41,140 --> 00:17:44,710
to only enable flowlet generation for elephant flows,

00:17:44,710 --> 00:17:48,343
because for mice flows we might expose them to reordering.

00:17:50,260 --> 00:17:53,943
So, I listed some next steps for my work.

00:17:55,220 --> 00:17:57,570
So, instead of the BPF hash map

00:17:57,570 --> 00:18:00,160
we could potentially use Conntrack metadata

00:18:00,160 --> 00:18:02,933
to store flow attributes.

00:18:04,210 --> 00:18:07,100
I have validated that my framework works

00:18:07,944 --> 00:18:12,110
with minimal performance penalty,

00:18:12,110 --> 00:18:15,860
with up to a hundred parts, but I need to validate

00:18:15,860 --> 00:18:18,580
that the performance overhead is

00:18:18,580 --> 00:18:22,090
as minimal as possible in high-scale environments.

00:18:22,090 --> 00:18:24,460
And, finally,

00:18:24,460 --> 00:18:25,390
excuse me.

00:18:25,390 --> 00:18:27,620
So, the framework does characterization

00:18:27,620 --> 00:18:30,330
based on latency and throughput attributes,

00:18:30,330 --> 00:18:31,740
but we can extend the framework

00:18:31,740 --> 00:18:34,520
to learn other network characteristics,

00:18:34,520 --> 00:18:37,060
such as if workloads are saving

00:18:37,060 --> 00:18:39,040
incoming connections at high package rate,

00:18:39,040 --> 00:18:43,473
or figuring out dependencies between workloads.

00:18:44,760 --> 00:18:46,650
So, that's all I have for you.

00:18:46,650 --> 00:18:47,563
Thank you so much.

00:18:53,070 --> 00:18:54,410
- [Programmer] Can you go back to the slides

00:18:54,410 --> 00:18:56,273
where you have the latency numbers?

00:18:58,300 --> 00:18:59,157
- Sorry, the what?

00:18:59,157 --> 00:19:00,360
- [Programmer] The latency numbers

00:19:00,360 --> 00:19:02,460
in the very beginning of the presentation.

00:19:04,023 --> 00:19:05,213
- [Moderator] Where you had the iperf

00:19:05,213 --> 00:19:07,040
and the other examples.

00:19:07,040 --> 00:19:07,873
- Perfect, so

00:19:09,460 --> 00:19:11,000
what is sockperf?

00:19:11,000 --> 00:19:15,860
So, is it similar to netperf TCP-RR or CRR?

00:19:15,860 --> 00:19:17,480
So, is that connection wealth,

00:19:17,480 --> 00:19:20,290
and what's the size of the RPC message there?

00:19:20,290 --> 00:19:25,040
- So sockperf is a network benchmark utility from Mellanox.

00:19:25,040 --> 00:19:28,090
It's a framework that's specifically used

00:19:28,090 --> 00:19:32,960
to measure latency for small flows.

00:19:32,960 --> 00:19:36,790
So, in this case, I'm using sockperf in a ping-pong mode,

00:19:36,790 --> 00:19:39,240
where I send small messages.

00:19:39,240 --> 00:19:41,950
The message size is around 50 kilobytes,

00:19:41,950 --> 00:19:46,950
and then you can specify how many messages are being sent.

00:19:47,360 --> 00:19:50,900
So, I'm specifically creating small flows here.

00:19:53,820 --> 00:19:56,530
- [Programmer] And this is the connection involved or not?

00:19:56,530 --> 00:20:00,020
So, is it establishing new TCP session every time,

00:20:00,020 --> 00:20:03,290
or it's just request-reply,

00:20:03,290 --> 00:20:06,230
because it's why I'm asking.

00:20:06,230 --> 00:20:07,920
And the second question to this is

00:20:07,920 --> 00:20:09,100
to understand the numbers

00:20:09,100 --> 00:20:12,390
and where this 50x slowdown comes from

00:20:12,390 --> 00:20:16,030
is what is underlying CNI provider

00:20:16,030 --> 00:20:17,880
in these Kubernetes clusters.

00:20:17,880 --> 00:20:18,713
If it's

00:20:20,120 --> 00:20:22,970
whatever versus Cilium.

00:20:22,970 --> 00:20:24,480
We just saw Joe talking

00:20:24,480 --> 00:20:26,900
about how they reduced the latency.

00:20:26,900 --> 00:20:28,140
This does so well.

00:20:28,140 --> 00:20:31,190
Maybe, using Cilium and this will bring down

00:20:31,190 --> 00:20:33,763
this 50x to, I don't know, two x.

00:20:36,460 --> 00:20:38,440
So, what is CNI?

00:20:38,440 --> 00:20:40,483
What is the networking plug-in for Kubernetes you used?

00:20:40,483 --> 00:20:44,970
- So, the CNI in this particular PUC is NS6.

00:20:44,970 --> 00:20:48,800
And it's not so much about what CNI you are using.

00:20:48,800 --> 00:20:50,230
It's also about the fact

00:20:50,230 --> 00:20:52,770
that the KVM hypervisors that I'm using have

00:20:52,770 --> 00:20:55,700
only one gig of bandwidth,

00:20:55,700 --> 00:20:58,173
so that might be the bottleneck as well.

00:20:59,600 --> 00:21:02,550
- So, congestion control is kicking in and stuff like that.

00:21:06,270 --> 00:21:07,550
- [Programmer] I guess tracking that down,

00:21:07,550 --> 00:21:09,350
another question would be,

00:21:09,350 --> 00:21:11,530
what is the queue test that you're running and stuff,

00:21:11,530 --> 00:21:13,790
because this could also be a queuing problem

00:21:13,790 --> 00:21:15,400
on the VM side,

00:21:15,400 --> 00:21:17,730
because your mice flows are being prioritized

00:21:17,730 --> 00:21:20,100
behind the elephant flows.

00:21:20,100 --> 00:21:21,534
Perhaps, another way to solve this is

00:21:21,534 --> 00:21:23,490
to solve the queuing problem at the front.

00:21:23,490 --> 00:21:25,993
- So, yes, as I mentioned here,

00:21:27,810 --> 00:21:31,230
the reason why we see tail latency being increased

00:21:31,230 --> 00:21:32,910
is because elephant flows tend

00:21:32,910 --> 00:21:34,463
to fill up the quick queues.

00:21:35,360 --> 00:21:39,160
And that's the reason behind the increased latency.

00:21:39,160 --> 00:21:40,540
- [Programmer] But shouldn't you

00:21:40,540 --> 00:21:41,373
I mean

00:21:42,426 --> 00:21:44,670
okay, I guess another way would be

00:21:44,670 --> 00:21:48,170
to prioritize the mice flows over the elephant flows, right?

00:21:48,170 --> 00:21:49,003
- So, first of all,

00:21:49,003 --> 00:21:50,450
you need to know

00:21:50,450 --> 00:21:52,530
which workloads are generating elephant flows

00:21:52,530 --> 00:21:54,930
and which workloads are generating mice flows.

00:21:54,930 --> 00:21:56,560
- Please, just run FQ scheduler

00:21:56,560 --> 00:21:57,890
on all your bottleneck links

00:21:57,890 --> 00:21:59,900
and this problem will go away,

00:21:59,900 --> 00:22:01,380
and complain at your CNI

00:22:01,380 --> 00:22:04,150
if they're not installing this already.

00:22:04,150 --> 00:22:05,343
We have solved this.

00:22:06,183 --> 00:22:07,556
The clearing part. - What is it called, again?

00:22:07,556 --> 00:22:09,094
FQ scheduler. - FQ scheduler.

00:22:09,094 --> 00:22:11,094
- Check FQ's scheduler.

00:22:11,094 --> 00:22:11,927
- [Presenter] Okay.

00:22:11,927 --> 00:22:13,240
- It should be the default, if it isn't,

00:22:13,240 --> 00:22:14,496
go complain to your vendor

00:22:14,496 --> 00:22:15,830
and tell them, hey, get the default.

00:22:15,830 --> 00:22:17,920
- So, an interesting point,

00:22:17,920 --> 00:22:19,940
because the qdisc and schedulers have been brought up

00:22:19,940 --> 00:22:21,200
is that

00:22:21,200 --> 00:22:22,650
since we have packet schedulers

00:22:22,650 --> 00:22:25,500
they implicitly do flow classification.

00:22:25,500 --> 00:22:27,190
And flow classification is part

00:22:27,190 --> 00:22:30,030
of the work that your module is doing

00:22:30,030 --> 00:22:34,220
to identify mice versus elephant flows.

00:22:34,220 --> 00:22:36,080
Perhaps, not precisely in the way, manner,

00:22:36,080 --> 00:22:38,080
and specification by which you do it.

00:22:38,080 --> 00:22:39,640
But the logic might be there.

00:22:39,640 --> 00:22:41,520
So, another way to implement this might be

00:22:41,520 --> 00:22:45,210
to do a BPF hook in the packet schedulers themselves.

00:22:45,210 --> 00:22:46,250
However, considering that,

00:22:46,250 --> 00:22:47,650
the only thing I can't figure out

00:22:47,650 --> 00:22:49,450
is how to replace your Conntrack zone

00:22:49,450 --> 00:22:51,270
to identify the pod, again,

00:22:51,270 --> 00:22:53,930
for the purposes of where to store the information.

00:22:53,930 --> 00:22:56,390
So, it's just something to consider.

00:22:56,390 --> 00:22:59,077
- Can we identify in which network namespace

00:22:59,077 --> 00:23:00,943
the qdisc instance is?

00:23:02,150 --> 00:23:03,730
- The network device, but I thought

00:23:03,730 --> 00:23:05,810
all them go through the same network device

00:23:05,810 --> 00:23:06,643
or I create this. - Correct.

00:23:06,643 --> 00:23:08,090
- Yes, that's the restriction.

00:23:08,090 --> 00:23:10,630
So, she needs another vector for the key.

00:23:10,630 --> 00:23:12,480
- Can we get a global identifier

00:23:12,480 --> 00:23:14,960
that identifies the flow when the packet first appears

00:23:14,960 --> 00:23:16,510
and goes all the way through the kernel?

00:23:16,510 --> 00:23:19,310
We can put, hardwire. - Tag it with something?

00:23:19,310 --> 00:23:21,830
Yeah, hardwire it. - Like an skb marker

00:23:21,830 --> 00:23:22,830
or something like this?

00:23:22,830 --> 00:23:24,950
- [Programmer] Yeah. (moderator laughing)

00:23:24,950 --> 00:23:26,520
And for hardware stuff,

00:23:26,520 --> 00:23:28,710
and hardware flow and not just touch it,

00:23:28,710 --> 00:23:30,650
and timestamp it also.

00:23:30,650 --> 00:23:32,330
- It's generally very useful

00:23:32,330 --> 00:23:34,640
to have some kind of key like that for the path

00:23:34,640 --> 00:23:35,613
of a packet.

00:23:36,510 --> 00:23:39,270
- Well, it's not so much about using specific elements

00:23:39,270 --> 00:23:40,700
like Conntrack or anything.

00:23:40,700 --> 00:23:44,250
We just need is a flow processing module

00:23:44,250 --> 00:23:47,263
that tracks flows to-- - Exactly.

00:23:48,130 --> 00:23:49,163
The only reason I bring it up is

00:23:49,163 --> 00:23:51,620
because I think certain packet schedulers

00:23:51,620 --> 00:23:54,720
may in fact already do the mice-elephant classification,

00:23:54,720 --> 00:23:56,480
and you would just have that information available

00:23:56,480 --> 00:23:57,910
to do it. - Got it.

00:24:00,950 --> 00:24:04,583
- Another thing is with big elephant flows like Spark.

00:24:06,040 --> 00:24:08,260
You pretty much run your processing

00:24:08,260 --> 00:24:09,700
where the big data is,

00:24:09,700 --> 00:24:11,003
even if it's distributed.

00:24:12,490 --> 00:24:15,770
How would your system work in that case

00:24:15,770 --> 00:24:17,940
when you have both the consumer and the client

00:24:17,940 --> 00:24:19,450
and the server of the data

00:24:19,450 --> 00:24:21,393
on the same physical node?

00:24:22,330 --> 00:24:23,460
Do we even need this?

00:24:23,460 --> 00:24:25,120
I mean, that'll obviate

00:24:25,120 --> 00:24:26,170
you don't need this.

00:24:26,170 --> 00:24:27,890
And then if it happened,

00:24:27,890 --> 00:24:28,990
how would you detect it?

00:24:28,990 --> 00:24:31,343
They're all on the same VM.

00:24:33,313 --> 00:24:35,662
It's not really leaving the box at that point.

00:24:35,662 --> 00:24:36,790
- [Moderator] Well, the management node

00:24:36,790 --> 00:24:38,700
would have this information, correct?

00:24:38,700 --> 00:24:40,180
- Yes, the management node knows

00:24:40,180 --> 00:24:42,780
where the data is and where to land this little client

00:24:42,780 --> 00:24:44,500
and it'll put them both in the same place

00:24:44,500 --> 00:24:47,630
using IP and stuff like that.

00:24:47,630 --> 00:24:50,770
- But I think Spark it's usually distributed,

00:24:50,770 --> 00:24:52,280
so there is a stabilized component

00:24:52,280 --> 00:24:54,770
and you have a distributed agent.

00:24:54,770 --> 00:24:56,730
- Absolutely, the data is distributed

00:24:56,730 --> 00:24:58,080
across multiple disks,

00:24:58,080 --> 00:25:00,520
but when you want to answer a big query,

00:25:00,520 --> 00:25:03,980
you'll run it in all those little subsections

00:25:03,980 --> 00:25:05,070
where the data is.

00:25:05,070 --> 00:25:08,330
The client will be close to where the data itself is.

00:25:08,330 --> 00:25:10,070
And then you do all the map

00:25:10,070 --> 00:25:11,760
and then the reduce phase is only

00:25:11,760 --> 00:25:16,589
where you do the consolidate it and return the result.

00:25:16,589 --> 00:25:18,010
Typically, you're not gonna have

00:25:18,010 --> 00:25:19,570
these elephants running everywhere

00:25:19,570 --> 00:25:21,780
and destroying the forest type of stuff.

00:25:21,780 --> 00:25:24,350
- Right, it all depends on your environment

00:25:24,350 --> 00:25:26,450
and what kind of workloads you're running.

00:25:31,100 --> 00:25:32,293
- Any other questions?

00:25:33,630 --> 00:25:35,328
Thank you very much, Aditi.

00:25:35,328 --> 00:25:37,737

YouTube URL: https://www.youtube.com/watch?v=4k7AHgk2kW4


