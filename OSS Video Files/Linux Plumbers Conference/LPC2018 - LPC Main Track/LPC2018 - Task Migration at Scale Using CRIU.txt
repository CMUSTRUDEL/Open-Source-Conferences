Title: LPC2018 - Task Migration at Scale Using CRIU
Publication date: 2018-12-04
Playlist: LPC2018 - LPC Main Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/69/
speaker:  Victor Marmol (Google), Andy Tucker (Google)


The Google computing infrastructure uses containers to manage millions of simultaneously running jobs in data centers worldwide. Although the applications are container aware and are designed to be resilient to failures, evictions due to resource contention and scheduled maintenance events can reduce overall efficiency due to the time required to rebuild complex application state. This talk discusses the ongoing use of the open source Checkpoint/Restore in Userspace (CRIU) software to migrate container workloads between machines without loss of application state, allowing improvements in efficiency and utilization. Weâ€™ll present our experiences with using CRIU at Google, including ongoing challenges supporting production workloads, current state of the project, changes required to integrate with our existing container infrastructure, new requirements from running CRIU at scale, and lessons learned from managing and supporting migratable containers. We hope to start a discussion around the future direction of CRIU as well as task migration in Linux as a whole.
Captions: 
	00:00:05,560 --> 00:00:09,430
I know I don't know about you but I

00:00:07,509 --> 00:00:11,410
really enjoyed the little things they

00:00:09,430 --> 00:00:12,910
had last time during the break so my

00:00:11,410 --> 00:00:16,090
priority is to get as many of those as I

00:00:12,910 --> 00:00:19,000
can this time around hi my name is

00:00:16,090 --> 00:00:20,770
Victor I'm an engineer at Google we've

00:00:19,000 --> 00:00:23,019
actually come here a small set of us

00:00:20,770 --> 00:00:25,480
there's a few more folks here from

00:00:23,019 --> 00:00:26,800
Google kind of really talking about our

00:00:25,480 --> 00:00:29,140
experience with task migration in

00:00:26,800 --> 00:00:30,279
particular our use of creo it's been

00:00:29,140 --> 00:00:32,500
about a year and a half that we've had

00:00:30,279 --> 00:00:34,120
this project on going to try to provide

00:00:32,500 --> 00:00:36,010
native tasks migration and our internal

00:00:34,120 --> 00:00:37,510
computer infrastructure at Google so we

00:00:36,010 --> 00:00:38,830
wanted to kind of come talk about our

00:00:37,510 --> 00:00:40,270
experience with creo talk about our

00:00:38,830 --> 00:00:43,120
experience migrating a very large

00:00:40,270 --> 00:00:44,830
infrastructure using tasks migration

00:00:43,120 --> 00:00:46,480
what that has looked like will what

00:00:44,830 --> 00:00:48,490
we've learned please feel free to ask

00:00:46,480 --> 00:00:50,620
questions at any time we actually talked

00:00:48,490 --> 00:00:52,690
a little about this last Tuesday of the

00:00:50,620 --> 00:00:54,760
containers micro conference so we'll be

00:00:52,690 --> 00:00:56,260
going about similar things hopefully in

00:00:54,760 --> 00:00:58,000
more detail and hopefully more time for

00:00:56,260 --> 00:00:59,890
questions so feel free to ask any

00:00:58,000 --> 00:01:01,450
questions that you might have at any

00:00:59,890 --> 00:01:05,110
time if I don't know the answer someone

00:01:01,450 --> 00:01:06,130
here I'm sure will so one of the things

00:01:05,110 --> 00:01:07,659
that I always like to talk about is sort

00:01:06,130 --> 00:01:09,460
of where we're coming from so I'll talk

00:01:07,659 --> 00:01:11,409
Allura about our work inside Google but

00:01:09,460 --> 00:01:12,820
outside of Google folks in our team have

00:01:11,409 --> 00:01:14,890
been involved in a lot of the open

00:01:12,820 --> 00:01:16,590
source projects our own containers as

00:01:14,890 --> 00:01:19,030
well as cluster management software

00:01:16,590 --> 00:01:20,679
managing of compute jobs and

00:01:19,030 --> 00:01:23,109
infrastructure so we've had folks who've

00:01:20,679 --> 00:01:24,549
been maintainer of Dockers out of our

00:01:23,109 --> 00:01:26,469
team came both of the projects in the

00:01:24,549 --> 00:01:27,369
middle sea advisor as well as that we

00:01:26,469 --> 00:01:29,109
contained up for you

00:01:27,369 --> 00:01:31,719
let me contain that for you is not a

00:01:29,109 --> 00:01:32,979
project many people know about but in

00:01:31,719 --> 00:01:35,289
essence it's actually some of the core

00:01:32,979 --> 00:01:37,119
technology that we use for actual

00:01:35,289 --> 00:01:39,609
isolation it came out right around the

00:01:37,119 --> 00:01:41,799
time as docker and so kind of got

00:01:39,609 --> 00:01:43,719
overshadowed by rightfully so to be

00:01:41,799 --> 00:01:45,549
honest I think darker manages a lot of

00:01:43,719 --> 00:01:48,159
things better than this does this only

00:01:45,549 --> 00:01:49,299
manages resource isolation and a bunch

00:01:48,159 --> 00:01:51,039
of folks in our team were involved in

00:01:49,299 --> 00:01:52,869
kubernetes towards the beginning not so

00:01:51,039 --> 00:01:53,889
much every day nowadays I'll talk a

00:01:52,869 --> 00:01:55,780
little bit more about what we do

00:01:53,889 --> 00:01:57,459
nowadays but kind of we've been try to

00:01:55,780 --> 00:01:59,560
stay close to the community at the same

00:01:57,459 --> 00:02:02,200
time bring things back and forth from

00:01:59,560 --> 00:02:03,819
what we do internally at Google so

00:02:02,200 --> 00:02:05,560
inside Google we actually do is we're

00:02:03,819 --> 00:02:07,539
all part of the team that's called Borg

00:02:05,560 --> 00:02:08,979
so Borg is our internal system for

00:02:07,539 --> 00:02:10,780
cluster management it's the thing that

00:02:08,979 --> 00:02:12,250
runs everything at Google and by

00:02:10,780 --> 00:02:14,530
everything I mean everything like all

00:02:12,250 --> 00:02:15,830
servers at Google run board all jobs at

00:02:14,530 --> 00:02:18,140
Google run on top

00:02:15,830 --> 00:02:19,700
ward so when you set up a virtual

00:02:18,140 --> 00:02:24,470
machine and Google compute engine

00:02:19,700 --> 00:02:26,480
it runs on board when you have a request

00:02:24,470 --> 00:02:27,770
coming into YouTube or web search its

00:02:26,480 --> 00:02:30,590
most likely being served by a machine

00:02:27,770 --> 00:02:31,730
that one way or another is in Borg so

00:02:30,590 --> 00:02:33,200
it's pretty exciting the fact that we

00:02:31,730 --> 00:02:35,450
can actually do that with such a large

00:02:33,200 --> 00:02:37,130
compute and so many clusters all around

00:02:35,450 --> 00:02:39,230
the world it gives a lot of

00:02:37,130 --> 00:02:41,540
opportunities but at the same time

00:02:39,230 --> 00:02:44,300
brings a lot of challenges and

00:02:41,540 --> 00:02:45,680
particularly such a large and vocal user

00:02:44,300 --> 00:02:47,270
base so we'll talk a little bit about

00:02:45,680 --> 00:02:50,570
that in particular in the context of

00:02:47,270 --> 00:02:53,060
migration so I want to talk a little bit

00:02:50,570 --> 00:02:54,650
more about Borg primarily to motivate

00:02:53,060 --> 00:02:56,959
why we're working on migration what it

00:02:54,650 --> 00:02:58,130
means and to specify a little bit of the

00:02:56,959 --> 00:03:00,170
language that we use throughout the

00:02:58,130 --> 00:03:02,300
presentation again feel free if you have

00:03:00,170 --> 00:03:03,950
any questions at any time so at a very

00:03:02,300 --> 00:03:05,390
high-level Borg looks a little bit like

00:03:03,950 --> 00:03:07,310
what's on the diagram you have on the

00:03:05,390 --> 00:03:09,590
cluster level a system called the board

00:03:07,310 --> 00:03:11,840
master this is our scheduler and manages

00:03:09,590 --> 00:03:14,120
compute jobs manages the lifecycle of

00:03:11,840 --> 00:03:16,580
machines as well as I compute job on

00:03:14,120 --> 00:03:19,130
each machine we run a daemon called the

00:03:16,580 --> 00:03:21,620
Borg let this is the Borg node agent and

00:03:19,130 --> 00:03:24,709
manages the jobs as well as the hardware

00:03:21,620 --> 00:03:26,090
on the actual machines that we run on if

00:03:24,709 --> 00:03:29,989
you're a user you interact with Borg

00:03:26,090 --> 00:03:32,000
through a web UI some CL eyes as well as

00:03:29,989 --> 00:03:33,950
our configuration language board config

00:03:32,000 --> 00:03:35,570
in essence it describes this is what I

00:03:33,950 --> 00:03:37,250
want my job to look like this is what I

00:03:35,570 --> 00:03:37,940
wanted to do please go ahead and make

00:03:37,250 --> 00:03:40,360
this happen

00:03:37,940 --> 00:03:42,650
although honestly if you're a user of

00:03:40,360 --> 00:03:44,390
Borg I Google a lot of times you don't

00:03:42,650 --> 00:03:46,220
really interact with Borg directly work

00:03:44,390 --> 00:03:48,530
has a lot of very good primitives but

00:03:46,220 --> 00:03:50,090
it's not easy to use all by itself and

00:03:48,530 --> 00:03:51,830
so generally what has happened is that

00:03:50,090 --> 00:03:54,230
folks have built frameworks on top of

00:03:51,830 --> 00:03:55,760
Borg so stuff like MapReduce or flume

00:03:54,230 --> 00:03:59,209
which is the new version of MapReduce or

00:03:55,760 --> 00:04:01,250
frameworks built on top of Borg to run a

00:03:59,209 --> 00:04:02,750
particular type of workload so those are

00:04:01,250 --> 00:04:05,930
both sort of batch processing workloads

00:04:02,750 --> 00:04:08,000
we have a video processing workload or

00:04:05,930 --> 00:04:10,250
framework used by many teams at Google

00:04:08,000 --> 00:04:12,320
so those are usual you'd interact with

00:04:10,250 --> 00:04:15,260
and then that system under the covers

00:04:12,320 --> 00:04:17,510
uses borg borg is heavily invested in

00:04:15,260 --> 00:04:19,850
containers absolutely everything on Borg

00:04:17,510 --> 00:04:21,140
runs in a container including that

00:04:19,850 --> 00:04:23,240
virtual machine that you might purchase

00:04:21,140 --> 00:04:25,010
from Google compute engine they do all

00:04:23,240 --> 00:04:26,230
one way or another end up burning as

00:04:25,010 --> 00:04:29,080
containers in Borg it's

00:04:26,230 --> 00:04:31,330
of our core pieces so when you do talk

00:04:29,080 --> 00:04:32,680
about sort of primitives of Borg the

00:04:31,330 --> 00:04:34,300
main thing we talk about is a task a

00:04:32,680 --> 00:04:36,190
task so sort of our unit of compute it's

00:04:34,300 --> 00:04:38,680
the thing that runs on an individual

00:04:36,190 --> 00:04:42,070
machine and actually does useful work

00:04:38,680 --> 00:04:45,130
for users there's a couple of key parts

00:04:42,070 --> 00:04:46,450
of key properties of what go into a task

00:04:45,130 --> 00:04:48,550
that are relevant for this discussion

00:04:46,450 --> 00:04:50,110
one of those is priority you can think

00:04:48,550 --> 00:04:51,850
of priority as how quickly will we

00:04:50,110 --> 00:04:52,930
schedule you if you're high priority we

00:04:51,850 --> 00:04:54,250
schedule you quickly if you're low

00:04:52,930 --> 00:04:56,020
priority we might never schedule you

00:04:54,250 --> 00:04:58,180
depending on the load and a particular

00:04:56,020 --> 00:05:00,820
cluster we also have this concept we

00:04:58,180 --> 00:05:02,440
call a class or application class in

00:05:00,820 --> 00:05:03,370
essence it's a hint to us of how

00:05:02,440 --> 00:05:05,530
latency-sensitive

00:05:03,370 --> 00:05:06,610
are you and this is very very important

00:05:05,530 --> 00:05:08,260
to us because there's a lot of

00:05:06,610 --> 00:05:11,080
optimizations we can do depending on

00:05:08,260 --> 00:05:13,330
this so if you're a front-end web server

00:05:11,080 --> 00:05:14,740
serving users queries for web search

00:05:13,330 --> 00:05:15,880
you're probably very latency sensitive

00:05:14,740 --> 00:05:17,980
when you really care about those

00:05:15,880 --> 00:05:19,990
milliseconds whereas if you're my Map

00:05:17,980 --> 00:05:21,970
Reduce you probably don't care if I

00:05:19,990 --> 00:05:23,830
finish this minute next minute or maybe

00:05:21,970 --> 00:05:26,380
even next hour and so you're much more

00:05:23,830 --> 00:05:28,120
batch latency tolerant workload and so

00:05:26,380 --> 00:05:29,440
depending on what that is we can do some

00:05:28,120 --> 00:05:31,420
optimizations based on that and

00:05:29,440 --> 00:05:34,030
migration is a great example of where

00:05:31,420 --> 00:05:35,950
that comes in handy if you look at the

00:05:34,030 --> 00:05:37,690
diagram it kind of shows you for a

00:05:35,950 --> 00:05:39,790
thousand foot view of what a task looks

00:05:37,690 --> 00:05:41,460
like they're usually one-to-one map to a

00:05:39,790 --> 00:05:43,810
container the container isolate Reese

00:05:41,460 --> 00:05:45,520
isolates the resources you'll run

00:05:43,810 --> 00:05:47,530
multiple processes within your container

00:05:45,520 --> 00:05:49,090
as part of your task and you also have

00:05:47,530 --> 00:05:51,280
these packages associated with it

00:05:49,090 --> 00:05:53,710
usually just bring static data your

00:05:51,280 --> 00:05:55,840
binary your filesystem stuff like that

00:05:53,710 --> 00:05:58,180
one of the interesting features of Borg

00:05:55,840 --> 00:06:00,370
is that we do actually share each task

00:05:58,180 --> 00:06:02,520
airs the IP with the machine so we do

00:06:00,370 --> 00:06:04,990
actually have to do portal which is a

00:06:02,520 --> 00:06:07,810
massive headache and we'll talk a little

00:06:04,990 --> 00:06:09,940
bit about how am I suppose is this is

00:06:07,810 --> 00:06:11,680
again sort of native Linux so these are

00:06:09,940 --> 00:06:13,210
native processes if you want something

00:06:11,680 --> 00:06:13,750
like a VM you can run the VM and sign

00:06:13,210 --> 00:06:16,150
your tasks

00:06:13,750 --> 00:06:21,580
the Borg doesn't necessarily facilitate

00:06:16,150 --> 00:06:23,020
that in many which ways so now is when

00:06:21,580 --> 00:06:24,640
we sort of really start talking about

00:06:23,020 --> 00:06:26,170
why we care about migration and we care

00:06:24,640 --> 00:06:28,300
about migrations is because of this

00:06:26,170 --> 00:06:31,000
because of evictions so as I mentioned

00:06:28,300 --> 00:06:32,470
we have these priorities and what

00:06:31,000 --> 00:06:33,160
happens is if you have a higher priority

00:06:32,470 --> 00:06:34,630
task

00:06:33,160 --> 00:06:36,910
it means the resources lower priority

00:06:34,630 --> 00:06:39,110
tasks tasks get evicted this means that

00:06:36,910 --> 00:06:41,000
Borg forcefully kills you

00:06:39,110 --> 00:06:42,380
times you get a notification one to five

00:06:41,000 --> 00:06:44,690
minutes please serve evacuate the

00:06:42,380 --> 00:06:46,370
machine we need it for someone else and

00:06:44,690 --> 00:06:49,100
for the most most of the part we're able

00:06:46,370 --> 00:06:51,230
to provide that for users however it

00:06:49,100 --> 00:06:53,660
does take you actually having to manage

00:06:51,230 --> 00:06:55,460
them we have an SLR on how often we will

00:06:53,660 --> 00:06:57,260
give you an eviction but it's actually

00:06:55,460 --> 00:06:59,180
pretty permissive so we can evict you

00:06:57,260 --> 00:07:01,160
more often than you would really like to

00:06:59,180 --> 00:07:03,170
be evicted and this is something kind of

00:07:01,160 --> 00:07:04,970
ingrained at Google of like what you're

00:07:03,170 --> 00:07:07,070
going to fail your TAS is gonna fail the

00:07:04,970 --> 00:07:09,380
hardware goes bad if you run enough jobs

00:07:07,070 --> 00:07:10,730
one of them's gonna impound to fail so

00:07:09,380 --> 00:07:12,350
you have to deal with failure anyway

00:07:10,730 --> 00:07:14,570
what's an eviction what's making that

00:07:12,350 --> 00:07:16,250
10x as bad as it was if anything it

00:07:14,570 --> 00:07:17,810
makes you practice that so it's kind of

00:07:16,250 --> 00:07:20,080
really ingrained to the culture to some

00:07:17,810 --> 00:07:22,190
degree so the top three reasons for

00:07:20,080 --> 00:07:24,500
evictions preemption as I mentioned

00:07:22,190 --> 00:07:26,450
higher priority tasks evicting a lower

00:07:24,500 --> 00:07:27,890
priority task the second one is software

00:07:26,450 --> 00:07:29,420
software upgrades we try to pretty

00:07:27,890 --> 00:07:31,370
aggressively upgrade our kernels or

00:07:29,420 --> 00:07:33,830
firmware and other pieces of software

00:07:31,370 --> 00:07:35,420
that require a reboot and so we will

00:07:33,830 --> 00:07:37,160
evacuate all the tasks to make sure we

00:07:35,420 --> 00:07:39,620
can bring the Machine down and bring it

00:07:37,160 --> 00:07:41,780
back up with a new software and finally

00:07:39,620 --> 00:07:42,890
rebalancing for availability yesterday

00:07:41,780 --> 00:07:44,840
there was a talk from some other

00:07:42,890 --> 00:07:46,520
co-workers at Google around a memory

00:07:44,840 --> 00:07:47,810
bandwidth antagonism and at the cluster

00:07:46,520 --> 00:07:49,370
level they have some neat control

00:07:47,810 --> 00:07:51,230
mechanisms where they realize the

00:07:49,370 --> 00:07:52,640
machine is too packed with memory

00:07:51,230 --> 00:07:55,070
bandwidth and so they move the task to

00:07:52,640 --> 00:07:56,900
another machine they see the task sees

00:07:55,070 --> 00:07:59,330
it has an eviction and so this is kind

00:07:56,900 --> 00:08:02,390
of another use case that we have and so

00:07:59,330 --> 00:08:04,460
that really leads to the effects of

00:08:02,390 --> 00:08:05,690
evictions there's and the evictions

00:08:04,460 --> 00:08:07,130
pretty much suck like no one really

00:08:05,690 --> 00:08:09,350
likes them they're very difficult to

00:08:07,130 --> 00:08:11,030
handle if I'm a task I have to write

00:08:09,350 --> 00:08:12,830
custom code for my application to handle

00:08:11,030 --> 00:08:14,930
what happens when an eviction happens

00:08:12,830 --> 00:08:16,460
serialize my state somewhere borg

00:08:14,930 --> 00:08:18,650
doesn't help you in any way you'll have

00:08:16,460 --> 00:08:20,570
to figure that out on your own and even

00:08:18,650 --> 00:08:22,310
if I'm running a framework we can't

00:08:20,570 --> 00:08:24,260
really do work that generalizes for

00:08:22,310 --> 00:08:26,480
other frameworks even if I'm MapReduce I

00:08:24,260 --> 00:08:28,430
can only really checkpoint you in the

00:08:26,480 --> 00:08:30,080
units of work that you have and

00:08:28,430 --> 00:08:31,460
sometimes those are pretty granular we

00:08:30,080 --> 00:08:33,380
actually get feedback from some of our

00:08:31,460 --> 00:08:35,390
users that use these frameworks that

00:08:33,380 --> 00:08:36,920
even when the framework works a hundred

00:08:35,390 --> 00:08:38,870
percent of the time they still lose

00:08:36,920 --> 00:08:41,870
about five percent of their cycles from

00:08:38,870 --> 00:08:43,220
the framer not being able to check when

00:08:41,870 --> 00:08:45,560
other granularity that they would like

00:08:43,220 --> 00:08:47,780
so even if the framework use doesn't

00:08:45,560 --> 00:08:49,070
well it's still a problem and this is

00:08:47,780 --> 00:08:50,450
particularly a problem for a lot of

00:08:49,070 --> 00:08:52,370
these patch workloads I've run a low

00:08:50,450 --> 00:08:54,200
priority so they see a lot more eviction

00:08:52,370 --> 00:08:56,089
if your high priority tasks it's not as

00:08:54,200 --> 00:08:57,980
much of a problem and so really kind of

00:08:56,089 --> 00:09:00,200
we found a very good sweet home for

00:08:57,980 --> 00:09:03,560
migration for this very large batch

00:09:00,200 --> 00:09:05,180
workloads that we have and so the

00:09:03,560 --> 00:09:07,790
solution that we have sort of outlined

00:09:05,180 --> 00:09:09,800
for that type of workload is is that of

00:09:07,790 --> 00:09:11,510
a transparent migration we know exactly

00:09:09,800 --> 00:09:13,040
when we're going to evict you we know

00:09:11,510 --> 00:09:15,350
exactly how much time we have to evict

00:09:13,040 --> 00:09:17,240
you ask part of Bourg if we could handle

00:09:15,350 --> 00:09:18,589
that eviction for you it's wonderful you

00:09:17,240 --> 00:09:20,089
as an application don't need to know

00:09:18,589 --> 00:09:22,250
anything about it

00:09:20,089 --> 00:09:24,950
and so that's what we aim to do with

00:09:22,250 --> 00:09:26,690
this project be able to change your

00:09:24,950 --> 00:09:29,600
eviction to a transparent migration as

00:09:26,690 --> 00:09:31,190
part of work and so we'll talk a little

00:09:29,600 --> 00:09:33,200
bit about what that entails what we've

00:09:31,190 --> 00:09:34,760
been able to do through that I started

00:09:33,200 --> 00:09:36,860
to think about that there are two main

00:09:34,760 --> 00:09:38,930
options of how to do this that we came

00:09:36,860 --> 00:09:40,400
up with one is checkpoint restore as

00:09:38,930 --> 00:09:42,589
you're familiar with you stop the task

00:09:40,400 --> 00:09:44,360
there's some blackout period as you

00:09:42,589 --> 00:09:46,370
migrate the tape this state over to the

00:09:44,360 --> 00:09:48,620
the new machine and then you restart the

00:09:46,370 --> 00:09:50,600
task from that new machine this is

00:09:48,620 --> 00:09:53,150
simpler but you do have a pretty large

00:09:50,600 --> 00:09:54,740
block out the other option is sort of

00:09:53,150 --> 00:09:56,000
live migration where you have a

00:09:54,740 --> 00:09:57,830
combination of a brown owner and a

00:09:56,000 --> 00:10:00,260
blackout the blackout should be smaller

00:09:57,830 --> 00:10:01,640
as you slow down or partially stop the

00:10:00,260 --> 00:10:03,290
process migrate some of those stayed

00:10:01,640 --> 00:10:05,029
over and then we can solve that once you

00:10:03,290 --> 00:10:05,959
get to the destination live migration

00:10:05,029 --> 00:10:07,330
would be great

00:10:05,959 --> 00:10:10,640
but it is a little bit more complicated

00:10:07,330 --> 00:10:12,200
being lazy or trying to do the easier

00:10:10,640 --> 00:10:14,839
thing first we opted to go with Czech

00:10:12,200 --> 00:10:16,670
primer store and start with that and so

00:10:14,839 --> 00:10:18,800
what are the challenges as part of that

00:10:16,670 --> 00:10:20,570
so one of the one of the key ones that

00:10:18,800 --> 00:10:21,920
we really had to deal with was dealing

00:10:20,570 --> 00:10:24,050
with a network particularly how do you

00:10:21,920 --> 00:10:25,400
migrate network connections this is

00:10:24,050 --> 00:10:27,500
pretty difficult it's you've made more

00:10:25,400 --> 00:10:29,510
difficult by the second point of we

00:10:27,500 --> 00:10:31,550
allocate ports what if you got allocated

00:10:29,510 --> 00:10:32,390
port 20 20 and it's not available and

00:10:31,550 --> 00:10:34,370
that other machine that you're gonna

00:10:32,390 --> 00:10:36,680
migrate to now the scheduler has to know

00:10:34,370 --> 00:10:38,029
what ports are allocated and what ports

00:10:36,680 --> 00:10:39,830
week and what machines we can move you

00:10:38,029 --> 00:10:41,990
to depending on what ports are available

00:10:39,830 --> 00:10:43,820
it makes a schedulers life harder and it

00:10:41,990 --> 00:10:47,029
makes it more difficult to migrate a lot

00:10:43,820 --> 00:10:48,410
of tasks at the same time storage is

00:10:47,029 --> 00:10:50,450
always interesting and difficult if you

00:10:48,410 --> 00:10:52,100
have 10 gigs on a disk it's gonna take a

00:10:50,450 --> 00:10:53,300
long time to get that out and that's

00:10:52,100 --> 00:10:55,310
seriously going to affect your ability

00:10:53,300 --> 00:10:58,910
to migrate it so that's something that

00:10:55,310 --> 00:11:00,560
we'd have to deal with this other one is

00:10:58,910 --> 00:11:03,589
a little bit easier I think for a lot of

00:11:00,560 --> 00:11:05,720
workload outside of Google but a lot of

00:11:03,589 --> 00:11:06,329
tests inside Google derive a lot of

00:11:05,720 --> 00:11:08,369
information

00:11:06,329 --> 00:11:09,869
from the actual system itself so they

00:11:08,369 --> 00:11:12,209
read into the system they understand

00:11:09,869 --> 00:11:13,709
what path are in what kernel we're

00:11:12,209 --> 00:11:15,029
running what the CPU looks like and

00:11:13,709 --> 00:11:17,069
drive a lot of information from that

00:11:15,029 --> 00:11:19,319
it's very hard for us to migrate a job

00:11:17,069 --> 00:11:20,549
while they care so much about the system

00:11:19,319 --> 00:11:22,319
that they run in so this is something

00:11:20,549 --> 00:11:24,509
that we have to make sure we tackle this

00:11:22,319 --> 00:11:26,220
part of the challenges and the final one

00:11:24,509 --> 00:11:27,899
in well we identified as the hardest

00:11:26,220 --> 00:11:30,600
part is like migrating the process of a

00:11:27,899 --> 00:11:32,129
Linux process is hard a VM is in theory

00:11:30,600 --> 00:11:34,619
easier because you have a lot more

00:11:32,129 --> 00:11:36,449
control of how that's running not so

00:11:34,619 --> 00:11:37,739
much so with a process in Linux you have

00:11:36,449 --> 00:11:39,589
to understand exactly where the cut

00:11:37,739 --> 00:11:43,199
points are and go from there

00:11:39,589 --> 00:11:45,480
and so through the project we were able

00:11:43,199 --> 00:11:47,639
to for the most part address most of

00:11:45,480 --> 00:11:49,399
these a lot of that was deciding it

00:11:47,639 --> 00:11:51,569
wasn't a problem and a lot of that was

00:11:49,399 --> 00:11:53,759
making use of other software that other

00:11:51,569 --> 00:11:55,049
folks have written so for networking one

00:11:53,759 --> 00:11:57,389
of the things that we did it is decided

00:11:55,049 --> 00:11:58,920
to worry about network connections we

00:11:57,389 --> 00:12:01,199
decided we'll just drop them and have

00:11:58,920 --> 00:12:05,040
users handle out themselves we'll talk

00:12:01,199 --> 00:12:06,540
about how that worked out for poor

00:12:05,040 --> 00:12:07,980
allocation we decided to do what most

00:12:06,540 --> 00:12:10,110
people will do which is have an errand a

00:12:07,980 --> 00:12:11,879
space of a unique IP address this is

00:12:10,110 --> 00:12:13,319
actually what kubernetes moved to in a

00:12:11,879 --> 00:12:15,299
large part from their experience with

00:12:13,319 --> 00:12:18,419
borg that poor allocation really isn't

00:12:15,299 --> 00:12:19,679
super awesome and so for these my

00:12:18,419 --> 00:12:21,449
gradable tasks we have a networking

00:12:19,679 --> 00:12:22,769
space with a unique ipv6 address they

00:12:21,449 --> 00:12:25,379
have the full port space we no longer

00:12:22,769 --> 00:12:26,999
have to worry about that for storage we

00:12:25,379 --> 00:12:29,939
simplified our life by saying no storage

00:12:26,999 --> 00:12:32,009
no local storage at all sorry we'll talk

00:12:29,939 --> 00:12:33,869
about how that worked out as well and

00:12:32,009 --> 00:12:35,249
and what I mentioned about virtualizing

00:12:33,869 --> 00:12:36,989
the local resources most people just say

00:12:35,249 --> 00:12:38,579
oh great linux things faces that sounds

00:12:36,989 --> 00:12:40,559
awesome we're actually a pretty late

00:12:38,579 --> 00:12:42,989
adopter of main spaces for one reason or

00:12:40,559 --> 00:12:44,610
another so this is actually in ourselves

00:12:42,989 --> 00:12:46,079
a change that we had to make inside

00:12:44,610 --> 00:12:48,119
Google is really adopt fully all

00:12:46,079 --> 00:12:49,619
namespaces and the final one and I think

00:12:48,119 --> 00:12:52,019
kind of for us to hear of the hour is

00:12:49,619 --> 00:12:53,279
cream is really not having to worry

00:12:52,019 --> 00:12:55,619
about migrating the state of a Linux

00:12:53,279 --> 00:12:57,149
process was a godsend for us and say

00:12:55,619 --> 00:12:58,589
there's a tremendous amount of time and

00:12:57,149 --> 00:12:59,999
we'll talk a lot into what went into

00:12:58,589 --> 00:13:02,910
that and whether it was hard or

00:12:59,999 --> 00:13:04,499
difficult so I'll run through the

00:13:02,910 --> 00:13:06,779
migration workflow kind of at a high

00:13:04,499 --> 00:13:08,249
level to then describe each of the

00:13:06,779 --> 00:13:10,559
individual pieces that we mentioned and

00:13:08,249 --> 00:13:12,779
how we tackle the problems that we spoke

00:13:10,559 --> 00:13:14,519
of so at a high level we start with the

00:13:12,779 --> 00:13:16,079
tasks in a machine I will start with

00:13:14,519 --> 00:13:17,879
particularly the workflow we have to do

00:13:16,079 --> 00:13:19,019
more on checkpoint restore as I

00:13:17,879 --> 00:13:19,570
mentioned is very important in the

00:13:19,019 --> 00:13:21,310
environment

00:13:19,570 --> 00:13:23,260
as isolator as possible so that's where

00:13:21,310 --> 00:13:25,960
we have that unique hi babies 600 all

00:13:23,260 --> 00:13:27,460
the namespaces no local storage and we

00:13:25,960 --> 00:13:28,840
actually kind of get a lot of help from

00:13:27,460 --> 00:13:30,850
Google libraries Google is a pretty

00:13:28,840 --> 00:13:32,650
interesting place where for a large part

00:13:30,850 --> 00:13:34,630
we're very vertically integrated all the

00:13:32,650 --> 00:13:36,850
applications for the most part that run

00:13:34,630 --> 00:13:37,960
at Google are built in-house and so that

00:13:36,850 --> 00:13:39,310
means that we use a lot of these common

00:13:37,960 --> 00:13:42,070
libraries everybody uses the same

00:13:39,310 --> 00:13:44,530
library works all very well for a lot of

00:13:42,070 --> 00:13:46,000
reasons that we'll go into but that is a

00:13:44,530 --> 00:13:47,380
key part component into how we're able

00:13:46,000 --> 00:13:49,990
to isolate the environments that folks

00:13:47,380 --> 00:13:51,910
running so once we decide to do the

00:13:49,990 --> 00:13:54,370
migration we go ahead we pause a task

00:13:51,910 --> 00:13:56,110
and cRIO comes in does all the hard work

00:13:54,370 --> 00:13:58,330
cyril ice is a state of the task we

00:13:56,110 --> 00:14:00,070
transfer that state over to distributed

00:13:58,330 --> 00:14:01,900
storage so Colossus is sort of the new

00:14:00,070 --> 00:14:04,990
version of GFS it's a distributed file

00:14:01,900 --> 00:14:08,110
system so we go ahead we copy all the

00:14:04,990 --> 00:14:09,670
state over at that point we turn down

00:14:08,110 --> 00:14:12,430
the task the machine no longer knows of

00:14:09,670 --> 00:14:14,080
this task the scheduler itself then

00:14:12,430 --> 00:14:15,880
decides where do I put this task this

00:14:14,080 --> 00:14:17,650
could take some time for a low priority

00:14:15,880 --> 00:14:19,750
task so you can imagine how to find a

00:14:17,650 --> 00:14:22,120
place to fit a low priority task in a

00:14:19,750 --> 00:14:24,370
potentially alreadypacked cluster and

00:14:22,120 --> 00:14:26,590
once we do find a space we ask the

00:14:24,370 --> 00:14:28,480
machine to start recreating the task we

00:14:26,590 --> 00:14:30,550
grab the check point from the distribute

00:14:28,480 --> 00:14:32,920
storage we once again ask Rio to do our

00:14:30,550 --> 00:14:34,900
hard work and deserialize the state and

00:14:32,920 --> 00:14:37,450
then we continue running the task as we

00:14:34,900 --> 00:14:38,980
had before so again the isolation of the

00:14:37,450 --> 00:14:42,190
scope of this environment in particular

00:14:38,980 --> 00:14:44,140
is very important as the task itself for

00:14:42,190 --> 00:14:46,060
the most part doesn't really see much

00:14:44,140 --> 00:14:47,440
changing the IP does change that's

00:14:46,060 --> 00:14:49,960
something we'll have to deal with the

00:14:47,440 --> 00:14:51,640
storage came with me the the machine for

00:14:49,960 --> 00:14:53,920
the most part looks the same with few

00:14:51,640 --> 00:14:55,870
exceptions and the Google libraries do a

00:14:53,920 --> 00:14:57,970
little bit of that work for us so that

00:14:55,870 --> 00:14:59,620
and they're being pretty easy and so

00:14:57,970 --> 00:15:00,790
looking into each of these individual

00:14:59,620 --> 00:15:02,830
pieces why were some of these

00:15:00,790 --> 00:15:04,360
simplifications possible and some of the

00:15:02,830 --> 00:15:06,520
things that were hard how did we tackle

00:15:04,360 --> 00:15:07,810
them some particularly networking so

00:15:06,520 --> 00:15:09,430
networking at Google is pretty

00:15:07,810 --> 00:15:11,830
interesting once again because of this

00:15:09,430 --> 00:15:13,240
vertical integration so pretty much all

00:15:11,830 --> 00:15:15,610
Natur traffic at Google who goes through

00:15:13,240 --> 00:15:18,010
RPC we have a common library called

00:15:15,610 --> 00:15:19,780
stubby externally most folks refer to it

00:15:18,010 --> 00:15:22,510
as G RPC and it's a library that

00:15:19,780 --> 00:15:23,650
everybody uses and so if the library is

00:15:22,510 --> 00:15:26,020
able to handle it

00:15:23,650 --> 00:15:27,580
everybody is sort of able to handle it

00:15:26,020 --> 00:15:29,320
and so we actually have some hooks into

00:15:27,580 --> 00:15:30,790
the library that allow us to understand

00:15:29,320 --> 00:15:32,230
how the connections are doing

00:15:30,790 --> 00:15:33,940
and be able to deal with it that way as

00:15:32,230 --> 00:15:36,310
I mentioned we have an ironing space

00:15:33,940 --> 00:15:38,380
with a unique ipv6 address not have to

00:15:36,310 --> 00:15:40,480
worry about ports we also have something

00:15:38,380 --> 00:15:42,940
called BNs the Borg name service in

00:15:40,480 --> 00:15:44,350
essence its DNS before Borg you give

00:15:42,940 --> 00:15:46,390
that to the RPC library they know what

00:15:44,350 --> 00:15:47,500
to do they'll they'll reconnect if they

00:15:46,390 --> 00:15:49,240
see the connection drop

00:15:47,500 --> 00:15:51,160
they'll redo the lookup they'll do

00:15:49,240 --> 00:15:53,290
lookup when they find out that the that

00:15:51,160 --> 00:15:55,450
what we're connecting to has changed in

00:15:53,290 --> 00:15:56,770
case the task has moved so it's really

00:15:55,450 --> 00:15:58,330
neat because it handles a lot of these

00:15:56,770 --> 00:16:00,010
things with connections for you and it's

00:15:58,330 --> 00:16:02,020
another area where having everybody use

00:16:00,010 --> 00:16:05,050
the same library really sort of worked

00:16:02,020 --> 00:16:07,030
out to a large deal for us and so as I

00:16:05,050 --> 00:16:08,950
mentioned stubby and gr PC for the most

00:16:07,030 --> 00:16:10,480
part automatically reconnect we'll talk

00:16:08,950 --> 00:16:12,370
about what happens when they don't which

00:16:10,480 --> 00:16:14,440
is super awesome this is for the most

00:16:12,370 --> 00:16:16,360
part again transparent to users and so

00:16:14,440 --> 00:16:18,130
for a very bulk large majority of our

00:16:16,360 --> 00:16:19,950
workload this sort of works out of the

00:16:18,130 --> 00:16:22,600
box which is really neat

00:16:19,950 --> 00:16:25,150
storage is another place where we're

00:16:22,600 --> 00:16:27,040
kind of able to simplify a little bit by

00:16:25,150 --> 00:16:28,750
dealing with most applications of Google

00:16:27,040 --> 00:16:30,610
are stateless and by stateless what I

00:16:28,750 --> 00:16:32,560
mean is that they typically require very

00:16:30,610 --> 00:16:34,750
little local storage local machine

00:16:32,560 --> 00:16:36,730
storage if they're going to use storage

00:16:34,750 --> 00:16:38,140
they use a distributed storage system

00:16:36,730 --> 00:16:39,700
whether that be something like Colossus

00:16:38,140 --> 00:16:41,980
or something like spanner which is

00:16:39,700 --> 00:16:44,350
another large massively replicated

00:16:41,980 --> 00:16:46,570
storage distributed file system that

00:16:44,350 --> 00:16:48,400
also is database and so for the most

00:16:46,570 --> 00:16:50,290
part most tasks using one of those two

00:16:48,400 --> 00:16:52,420
which get all their traffic through RPC

00:16:50,290 --> 00:16:53,140
for the most part don't have to worry

00:16:52,420 --> 00:16:54,640
about it

00:16:53,140 --> 00:16:56,920
we do provide a little bit of local

00:16:54,640 --> 00:16:58,630
storage in the form of tempo fests and

00:16:56,920 --> 00:17:00,550
by a little I mean a little it's like a

00:16:58,630 --> 00:17:03,880
couple of Meg's and this for the most

00:17:00,550 --> 00:17:05,650
part ends up being what most TAS need we

00:17:03,880 --> 00:17:07,840
do have sort of a long tail of users

00:17:05,650 --> 00:17:10,360
they need a bigger disk or they actually

00:17:07,840 --> 00:17:13,240
need disk or SSD we do have a PD a

00:17:10,360 --> 00:17:15,100
persistent disk offering very similar to

00:17:13,240 --> 00:17:17,770
what we offer for our Google compute

00:17:15,100 --> 00:17:18,940
engine virtual machines so folks can use

00:17:17,770 --> 00:17:21,490
that that's a little bit easier to

00:17:18,940 --> 00:17:23,380
migrate in the network environment we do

00:17:21,490 --> 00:17:25,960
have some cases where we have local SSD

00:17:23,380 --> 00:17:27,760
or local HDD and for the most part we

00:17:25,960 --> 00:17:29,170
just decided not to migrate those it's

00:17:27,760 --> 00:17:31,000
actually worked out pretty well we have

00:17:29,170 --> 00:17:33,340
yet a user that uses Sosa's kind of

00:17:31,000 --> 00:17:35,410
wanted to request migration they end up

00:17:33,340 --> 00:17:37,330
already having to do a few crazy things

00:17:35,410 --> 00:17:39,250
that we haven't had to deal with too too

00:17:37,330 --> 00:17:40,720
much again as I mentioned the fact that

00:17:39,250 --> 00:17:42,340
all this goes over the network and goes

00:17:40,720 --> 00:17:44,020
through the RPC library is great because

00:17:42,340 --> 00:17:44,340
to us they just look like other RPC

00:17:44,020 --> 00:17:46,169
connect

00:17:44,340 --> 00:17:47,820
connections and so that really

00:17:46,169 --> 00:17:48,960
simplifies our work so storage is

00:17:47,820 --> 00:17:52,429
actually an area that we don't have to

00:17:48,960 --> 00:17:52,429
worry about as much as you would expect

00:17:52,970 --> 00:17:56,549
task environment so this was pretty

00:17:54,929 --> 00:17:57,809
boring for most folks that are used to

00:17:56,549 --> 00:17:59,850
something like docker where it just

00:17:57,809 --> 00:18:02,130
looks like full cgroups full namespaces

00:17:59,850 --> 00:18:03,630
an isolated filesystem for us it was

00:18:02,130 --> 00:18:05,190
actually a bunch of work to get here as

00:18:03,630 --> 00:18:07,049
I mentioned namespaces was not something

00:18:05,190 --> 00:18:08,850
that we really had as much of and this

00:18:07,049 --> 00:18:10,590
isolated filesystem thing was preaching

00:18:08,850 --> 00:18:11,909
U for us for the most part tasida to

00:18:10,590 --> 00:18:13,679
need anything so we didn't give them

00:18:11,909 --> 00:18:15,210
anything and now we needed to give them

00:18:13,679 --> 00:18:17,370
something and so we went ahead and

00:18:15,210 --> 00:18:20,370
provided a mechanism to do that through

00:18:17,370 --> 00:18:22,320
a package in essence we do run the

00:18:20,370 --> 00:18:23,880
processes with as little capabilities as

00:18:22,320 --> 00:18:26,159
possible so definitely no route and

00:18:23,880 --> 00:18:28,110
definitely as limited capabilities as we

00:18:26,159 --> 00:18:30,690
can possibly get away with but again

00:18:28,110 --> 00:18:32,399
like for most of the outside world this

00:18:30,690 --> 00:18:34,350
looks very typical to what you expect

00:18:32,399 --> 00:18:35,850
nowadays out of a full container system

00:18:34,350 --> 00:18:41,549
when you talk about containers they look

00:18:35,850 --> 00:18:43,020
a little bit like that and that's where

00:18:41,549 --> 00:18:44,610
creo comes in we're really kind of

00:18:43,020 --> 00:18:47,159
significantly simplified a lot of the

00:18:44,610 --> 00:18:48,630
stuff that we had to do crew is able to

00:18:47,159 --> 00:18:50,789
serialize the state of the tasks that we

00:18:48,630 --> 00:18:52,740
have so we do use crew in a motor

00:18:50,789 --> 00:18:54,960
sterilizes everything from the point of

00:18:52,740 --> 00:18:56,309
view of the task so we have the board

00:18:54,960 --> 00:18:57,929
that is able to create this container

00:18:56,309 --> 00:18:59,669
create the cgroups create the filesystem

00:18:57,929 --> 00:19:01,529
and the isolation it does that pretty

00:18:59,669 --> 00:19:03,000
well it works well out of the box we

00:19:01,529 --> 00:19:04,470
don't have to touch it too much so we

00:19:03,000 --> 00:19:06,360
let the boarder let manage that it'll

00:19:04,470 --> 00:19:08,309
create and destroy that environment in

00:19:06,360 --> 00:19:11,250
that system and we let crew just worry

00:19:08,309 --> 00:19:13,620
about the actual process that the task

00:19:11,250 --> 00:19:15,539
is running so what we have is whenever

00:19:13,620 --> 00:19:17,520
we decide to migrate the Borg late--

00:19:15,539 --> 00:19:19,440
which runs as route for can execs the

00:19:17,520 --> 00:19:21,720
migrator which again runs this route and

00:19:19,440 --> 00:19:25,350
it runs inside the task container so the

00:19:21,720 --> 00:19:26,520
resources being attribute the resources

00:19:25,350 --> 00:19:27,899
being used by the migrator are

00:19:26,520 --> 00:19:29,640
attributed to the process which is very

00:19:27,899 --> 00:19:31,200
important that way we don't have to

00:19:29,640 --> 00:19:32,669
allocate extra resources on the machine

00:19:31,200 --> 00:19:34,590
to make that work

00:19:32,669 --> 00:19:36,510
the migrator is in essence a Google

00:19:34,590 --> 00:19:38,700
binary that has a few Google buy

00:19:36,510 --> 00:19:41,039
libraries built in to do stuff like talk

00:19:38,700 --> 00:19:42,240
to the remote storage as well as to be

00:19:41,039 --> 00:19:43,679
able to do a couple of privileged

00:19:42,240 --> 00:19:46,169
operations that we don't want to have

00:19:43,679 --> 00:19:47,250
creo do and the main reason we don't

00:19:46,169 --> 00:19:49,409
want to do that is because we do run

00:19:47,250 --> 00:19:51,059
crew you ask the user as a task user and

00:19:49,409 --> 00:19:53,880
this works surprisingly well with very

00:19:51,059 --> 00:19:55,710
few exceptions where the kree process is

00:19:53,880 --> 00:19:57,890
able to serialize the entire state of

00:19:55,710 --> 00:19:59,300
the process without having to worry

00:19:57,890 --> 00:20:00,860
having any kind of extra special

00:19:59,300 --> 00:20:02,270
capabilities it sends all that

00:20:00,860 --> 00:20:03,800
information over to the mind greater the

00:20:02,270 --> 00:20:06,080
my greater pretends to be a page server

00:20:03,800 --> 00:20:07,910
for Cree you meaning that the pages just

00:20:06,080 --> 00:20:09,710
gets streamed to it it continues the

00:20:07,910 --> 00:20:11,120
streaming and encrypts them and it

00:20:09,710 --> 00:20:12,740
compresses them and it sensible or to

00:20:11,120 --> 00:20:15,440
the distributed storage so it ends up

00:20:12,740 --> 00:20:17,120
being there's a very very small fixed

00:20:15,440 --> 00:20:18,980
overhead that we need on the container

00:20:17,120 --> 00:20:21,230
to actually migrate all the information

00:20:18,980 --> 00:20:22,460
out of it which is pretty neat this was

00:20:21,230 --> 00:20:24,050
not the case initially and we were

00:20:22,460 --> 00:20:27,830
running into many out of resource

00:20:24,050 --> 00:20:30,710
conditions when that was the case any

00:20:27,830 --> 00:20:32,840
questions so far I tend to go to a fast

00:20:30,710 --> 00:20:36,470
pace and I get that feedback all the

00:20:32,840 --> 00:20:38,060
time alright I'll continue so how does

00:20:36,470 --> 00:20:39,140
this worked out in practice today it's

00:20:38,060 --> 00:20:41,450
actually been really awesome

00:20:39,140 --> 00:20:42,950
so most migrations what the user sees

00:20:41,450 --> 00:20:44,480
today is about one to two minutes of

00:20:42,950 --> 00:20:46,070
blackout time once a two minutes from

00:20:44,480 --> 00:20:47,870
the time of the task stops running we

00:20:46,070 --> 00:20:49,820
might get all the state and bring it

00:20:47,870 --> 00:20:51,710
back up on the other side this is

00:20:49,820 --> 00:20:53,390
particularly dominated by sending the

00:20:51,710 --> 00:20:55,520
information to and from the distributed

00:20:53,390 --> 00:20:57,530
storage as well as scheduling as I

00:20:55,520 --> 00:20:59,960
mentioned at low priorities it takes

00:20:57,530 --> 00:21:02,390
like 30 seconds for us to find a machine

00:20:59,960 --> 00:21:04,310
that will fit this job the right way

00:21:02,390 --> 00:21:05,840
so these endo beings the large sources

00:21:04,310 --> 00:21:07,370
of delay the actual serializing

00:21:05,840 --> 00:21:09,020
checkpoint restore it's actually pretty

00:21:07,370 --> 00:21:11,390
fast it's a few seconds 10 seconds

00:21:09,020 --> 00:21:14,270
except in some agrees tail end cases

00:21:11,390 --> 00:21:15,320
which we'll talk about and 90% of the

00:21:14,270 --> 00:21:17,240
time we're able to checkpoint

00:21:15,320 --> 00:21:18,650
restore successfully which is pretty

00:21:17,240 --> 00:21:19,880
good but we want it to be much much

00:21:18,650 --> 00:21:23,000
better and we're working towards that

00:21:19,880 --> 00:21:24,740
the common causes of failures today for

00:21:23,000 --> 00:21:26,900
the most part we used to have a lot of

00:21:24,740 --> 00:21:28,190
these out of resource conditions and we

00:21:26,900 --> 00:21:29,660
still see some of them particularly

00:21:28,190 --> 00:21:31,400
around things like threads and memory

00:21:29,660 --> 00:21:33,080
where we have these highly highly

00:21:31,400 --> 00:21:34,100
threaded tasks around thousands of

00:21:33,080 --> 00:21:35,930
threads

00:21:34,100 --> 00:21:38,720
sometimes it takes a very very long time

00:21:35,930 --> 00:21:40,700
to checkpoint those and so that we've

00:21:38,720 --> 00:21:42,140
seen a lot of timeouts around that we

00:21:40,700 --> 00:21:43,520
also have some places where we don't

00:21:42,140 --> 00:21:45,470
actually replicate the host environment

00:21:43,520 --> 00:21:47,300
as well as we would like these are all

00:21:45,470 --> 00:21:48,710
bugs were working through those we see a

00:21:47,300 --> 00:21:51,470
lot less of those now than we did before

00:21:48,710 --> 00:21:53,210
and finally kind of various failures in

00:21:51,470 --> 00:21:54,980
serialization for the most part they end

00:21:53,210 --> 00:21:56,600
up being things like custom Google

00:21:54,980 --> 00:21:59,180
kernel features we have a couple of

00:21:56,600 --> 00:22:00,710
custom kernel sis calls at a Korea

00:21:59,180 --> 00:22:02,750
doesn't understand because they're not

00:22:00,710 --> 00:22:04,700
open source and so either turning those

00:22:02,750 --> 00:22:06,560
off for applications that use this or

00:22:04,700 --> 00:22:07,830
adding support for those in crew you

00:22:06,560 --> 00:22:09,690
have been

00:22:07,830 --> 00:22:12,930
the mode of operation that we've had so

00:22:09,690 --> 00:22:14,550
far so what have we heard from our users

00:22:12,930 --> 00:22:15,840
so it's been pretty awesome the users

00:22:14,550 --> 00:22:17,460
have been extremely excited about this

00:22:15,840 --> 00:22:20,040
in particular if you're a batch job this

00:22:17,460 --> 00:22:22,080
is a huge lifesaver just last week we

00:22:20,040 --> 00:22:23,850
had a very happy user that came to us

00:22:22,080 --> 00:22:25,830
telling us they used to run this very

00:22:23,850 --> 00:22:27,300
large massive parallel processing

00:22:25,830 --> 00:22:29,670
pipeline it took them six hours every

00:22:27,300 --> 00:22:31,110
day they turned on migration literally

00:22:29,670 --> 00:22:32,670
all they did was out of configuration

00:22:31,110 --> 00:22:36,300
they turn on migration and now it takes

00:22:32,670 --> 00:22:38,310
four and a half hours and so 20 in their

00:22:36,300 --> 00:22:40,470
run time just from this feature kind of

00:22:38,310 --> 00:22:42,660
gives you a little piece of how a lot of

00:22:40,470 --> 00:22:45,780
these very large pipelines even when one

00:22:42,660 --> 00:22:47,970
small piece down for some period of time

00:22:45,780 --> 00:22:49,680
it's a pretty expensive in the long run

00:22:47,970 --> 00:22:51,300
so we're pretty excited about that we're

00:22:49,680 --> 00:22:53,700
having a lot of users that just come to

00:22:51,300 --> 00:22:55,320
us trying to adopt migration it's very

00:22:53,700 --> 00:22:57,150
easy for them to adopt it and so that's

00:22:55,320 --> 00:22:58,680
a large reason why they're very excited

00:22:57,150 --> 00:22:59,250
about it and why they're so willing to

00:22:58,680 --> 00:23:01,980
adopt it

00:22:59,250 --> 00:23:04,710
however as things go with users they

00:23:01,980 --> 00:23:06,270
always ask for more better features so

00:23:04,710 --> 00:23:07,710
one of those examples are notifications

00:23:06,270 --> 00:23:09,660
they want notifications for everything

00:23:07,710 --> 00:23:11,010
they want to know what I'm about to be

00:23:09,660 --> 00:23:12,750
migrated there's a couple of things that

00:23:11,010 --> 00:23:15,960
maybe we don't handle as well maybe they

00:23:12,750 --> 00:23:17,760
need to tell the master service that is

00:23:15,960 --> 00:23:19,590
giving them work oh please wait a second

00:23:17,760 --> 00:23:20,820
I'm going to migrate or maybe they want

00:23:19,590 --> 00:23:21,930
to tell us actually I don't have

00:23:20,820 --> 00:23:23,790
anything interesting to migrate don't

00:23:21,930 --> 00:23:26,430
mind great me so we've gone this

00:23:23,790 --> 00:23:28,980
requests and we'll about being able to

00:23:26,430 --> 00:23:30,150
address that for users the other finding

00:23:28,980 --> 00:23:33,240
that we thought that was kind of pretty

00:23:30,150 --> 00:23:35,520
surprising and on the kind of the chart

00:23:33,240 --> 00:23:38,100
there on your right is sort of the

00:23:35,520 --> 00:23:39,300
blackout time and applications that deal

00:23:38,100 --> 00:23:41,520
with this has sort of the largest

00:23:39,300 --> 00:23:43,620
blackout time so we find that a lot of

00:23:41,520 --> 00:23:45,480
these batch latency tolerant workloads

00:23:43,620 --> 00:23:46,740
you can give them a blackout time of 10

00:23:45,480 --> 00:23:48,870
minutes some of them and they're happy

00:23:46,740 --> 00:23:51,180
right around one minute is sort of an

00:23:48,870 --> 00:23:53,190
automatic reconnection time and so below

00:23:51,180 --> 00:23:54,450
that they don't even notice any blackout

00:23:53,190 --> 00:23:57,420
time below a minute is completely

00:23:54,450 --> 00:23:58,770
transparent to them for the most part so

00:23:57,420 --> 00:24:00,180
for batch workloads this is a great

00:23:58,770 --> 00:24:03,450
offering and we've had a lot of success

00:24:00,180 --> 00:24:05,520
they're lazy sensitive workloads your

00:24:03,450 --> 00:24:07,350
front-end web servers those not so much

00:24:05,520 --> 00:24:09,090
those we talk to them about 30-second

00:24:07,350 --> 00:24:11,790
timeout even ten ten-second blackouts

00:24:09,090 --> 00:24:14,340
and they they don't like that they kind

00:24:11,790 --> 00:24:19,350
of go away when we start talking about

00:24:14,340 --> 00:24:20,970
that for them really the this level of

00:24:19,350 --> 00:24:21,900
blackout is unacceptable

00:24:20,970 --> 00:24:23,640
particular when they're talking about

00:24:21,900 --> 00:24:25,320
certain queries that is sort of a level

00:24:23,640 --> 00:24:28,140
of milliseconds they're not able to

00:24:25,320 --> 00:24:29,789
handle that for most of our discussion

00:24:28,140 --> 00:24:32,309
sort of hundreds of milliseconds seems

00:24:29,789 --> 00:24:33,720
to be okay but there's a long way to go

00:24:32,309 --> 00:24:35,309
from one to two minutes to hundreds of

00:24:33,720 --> 00:24:37,020
milliseconds there's a lot of work

00:24:35,309 --> 00:24:38,429
people have to do so thankfully there's

00:24:37,020 --> 00:24:40,950
a lot of bachelor clothes and a lot of

00:24:38,429 --> 00:24:45,049
our capacity goes to that so we'll be

00:24:40,950 --> 00:24:45,049
trying to address that use case first

00:24:46,520 --> 00:24:51,690
adoption challenges some of our favorite

00:24:49,380 --> 00:24:53,340
portions of what have we found so I

00:24:51,690 --> 00:24:55,320
mentioned that you know users that

00:24:53,340 --> 00:24:57,090
Google are used to dealing with failures

00:24:55,320 --> 00:24:58,260
you're told your machine can fail your

00:24:57,090 --> 00:24:59,850
tasks can be evicted at any time

00:24:58,260 --> 00:25:01,200
shouldn't be a problem all the code

00:24:59,850 --> 00:25:03,450
should be able to handle that in theory

00:25:01,200 --> 00:25:06,059
not at all in practice it turns out

00:25:03,450 --> 00:25:08,250
people handle failures a lot less well

00:25:06,059 --> 00:25:09,840
than you'd expect them to we've had some

00:25:08,250 --> 00:25:11,820
terrible terrible conditions there was a

00:25:09,840 --> 00:25:14,760
job that we had another one of these

00:25:11,820 --> 00:25:16,320
large parallel pipelines that were we

00:25:14,760 --> 00:25:17,580
were successfully check mine restoring

00:25:16,320 --> 00:25:19,380
the workload it was awesome all our

00:25:17,580 --> 00:25:21,600
metrics show that this thing was blazing

00:25:19,380 --> 00:25:23,850
like a 100% success rate it was awesome

00:25:21,600 --> 00:25:25,679
the user was super excited we looked at

00:25:23,850 --> 00:25:27,960
their metrics they were not seeing any

00:25:25,679 --> 00:25:29,760
any improvement in their run times we

00:25:27,960 --> 00:25:31,860
had we dove in looked a little bit

00:25:29,760 --> 00:25:34,710
deeper what we found the application was

00:25:31,860 --> 00:25:36,419
doing was that once the ones they were

00:25:34,710 --> 00:25:37,860
restored the network connection did

00:25:36,419 --> 00:25:39,570
actually drop most users just

00:25:37,860 --> 00:25:41,390
reestablish it but they chose a

00:25:39,570 --> 00:25:43,770
particular setting that tells them don't

00:25:41,390 --> 00:25:45,330
reestablish it tell me about it so they

00:25:43,770 --> 00:25:46,620
got told about this and they decided to

00:25:45,330 --> 00:25:47,190
drop whatever work they were doing and

00:25:46,620 --> 00:25:49,260
start from scratch

00:25:47,190 --> 00:25:51,659
so in essence they were getting evicted

00:25:49,260 --> 00:25:53,549
the same way although we were doing all

00:25:51,659 --> 00:25:55,049
the work of you know handily destroying

00:25:53,549 --> 00:25:57,720
their process and returning it back to

00:25:55,049 --> 00:25:59,100
state so that was a bit unfortunate but

00:25:57,720 --> 00:26:00,600
actually found a surprising number of

00:25:59,100 --> 00:26:02,669
users that do this it's not the block

00:26:00,600 --> 00:26:04,049
majority but it's enough that it's been

00:26:02,669 --> 00:26:05,520
a concern as far as adoption is

00:26:04,049 --> 00:26:06,630
concerned and so we've tried to work

00:26:05,520 --> 00:26:08,669
with some of the core libraries to

00:26:06,630 --> 00:26:10,470
figure out how can we detect these kinds

00:26:08,669 --> 00:26:12,750
of cases and deal with and deal with

00:26:10,470 --> 00:26:14,190
them the other part I mention is for the

00:26:12,750 --> 00:26:15,710
isolation of the task environment is

00:26:14,190 --> 00:26:18,150
particularly important and interesting

00:26:15,710 --> 00:26:20,309
we've seen a lot of problems with the

00:26:18,150 --> 00:26:21,090
change of IP I mentioned we have this

00:26:20,309 --> 00:26:22,590
awesome solution

00:26:21,090 --> 00:26:24,419
there's board name service all the

00:26:22,590 --> 00:26:26,640
libraries use it it's great it's not a

00:26:24,419 --> 00:26:28,679
problem if you use that we still kind of

00:26:26,640 --> 00:26:30,690
have some users that don't for whatever

00:26:28,679 --> 00:26:33,000
reason and so they'll hard-code sort of

00:26:30,690 --> 00:26:34,040
host : port they don't expect that to

00:26:33,000 --> 00:26:36,230
change in the runtime

00:26:34,040 --> 00:26:38,330
the task and when it does it kind of

00:26:36,230 --> 00:26:40,610
becomes an interesting problem for them

00:26:38,330 --> 00:26:42,050
for this every single time we've just

00:26:40,610 --> 00:26:44,360
fixed it by telling to use the DNS

00:26:42,050 --> 00:26:46,730
address instead of using host : port and

00:26:44,360 --> 00:26:48,230
it works pretty much out of the box as a

00:26:46,730 --> 00:26:50,210
drop-in replacement because all the

00:26:48,230 --> 00:26:51,620
libraries know what to do with it but

00:26:50,210 --> 00:26:53,390
there's a fair a surprisingly large

00:26:51,620 --> 00:26:54,530
number of users that don't use that and

00:26:53,390 --> 00:26:56,800
so that has been another area that's

00:26:54,530 --> 00:26:59,780
been difficult to drive towards adoption

00:26:56,800 --> 00:27:01,220
and the final one is sort of users being

00:26:59,780 --> 00:27:02,960
overly sensitive of what the environment

00:27:01,220 --> 00:27:04,760
looks like so whether old my kernel

00:27:02,960 --> 00:27:06,350
version changed the features that I want

00:27:04,760 --> 00:27:08,420
to use are different or the underlying

00:27:06,350 --> 00:27:10,310
CPU is different now I want to handle

00:27:08,420 --> 00:27:12,140
things differently or like the rack I

00:27:10,310 --> 00:27:14,150
was in changed and I was optimizing for

00:27:12,140 --> 00:27:15,830
that one way or another some of these

00:27:14,150 --> 00:27:17,510
workloads that have been very sensitive

00:27:15,830 --> 00:27:19,130
to that kind of work has been a little

00:27:17,510 --> 00:27:21,380
bit difficult sometimes we provide too

00:27:19,130 --> 00:27:23,180
many hooks to go into your environment

00:27:21,380 --> 00:27:25,310
understand what it looks like natively

00:27:23,180 --> 00:27:27,260
so these are not kind of areas that we

00:27:25,310 --> 00:27:28,940
have solved just yet but they're areas

00:27:27,260 --> 00:27:30,530
of active work that we're trying to

00:27:28,940 --> 00:27:31,940
understand how to improve on each of

00:27:30,530 --> 00:27:33,680
these pieces it's very interesting

00:27:31,940 --> 00:27:36,320
because for the most part no one does

00:27:33,680 --> 00:27:38,030
any one of these individual one but the

00:27:36,320 --> 00:27:39,560
combination of all these together means

00:27:38,030 --> 00:27:41,450
almost everybody's doing at least one of

00:27:39,560 --> 00:27:44,930
these so that's been a little bit of a

00:27:41,450 --> 00:27:46,580
challenge to get through I'll put the

00:27:44,930 --> 00:27:47,780
slides up online by the way for the

00:27:46,580 --> 00:27:50,540
folks taking pictures I should have said

00:27:47,780 --> 00:27:53,990
that earlier but continue thinking so

00:27:50,540 --> 00:27:55,780
creo so it's awesome because we have

00:27:53,990 --> 00:27:58,310
been an extremely happy user of creo

00:27:55,780 --> 00:28:00,230
because we've been extremely surprised

00:27:58,310 --> 00:28:02,210
how easy it was to go from grabbing creo

00:28:00,230 --> 00:28:04,700
building it and running it and it just

00:28:02,210 --> 00:28:07,430
worked out of the box so many things

00:28:04,700 --> 00:28:08,510
kind of exactly fell into place we

00:28:07,430 --> 00:28:10,720
really have had a very very good

00:28:08,510 --> 00:28:13,310
experience with it both actually getting

00:28:10,720 --> 00:28:15,800
crew working how it addresses all our

00:28:13,310 --> 00:28:17,840
problems how well it's been able to

00:28:15,800 --> 00:28:19,970
migrate as well as the community both

00:28:17,840 --> 00:28:21,560
folks ask that we can ask for help as

00:28:19,970 --> 00:28:23,630
well as advancement in areas like live

00:28:21,560 --> 00:28:25,130
migration it's really been wonderful to

00:28:23,630 --> 00:28:27,860
work with the community as well as take

00:28:25,130 --> 00:28:29,420
part and and use the technology itself

00:28:27,860 --> 00:28:30,800
it's worked so well that it's an area

00:28:29,420 --> 00:28:31,970
that we would like to continue investing

00:28:30,800 --> 00:28:34,130
and part of the reason why we're here

00:28:31,970 --> 00:28:35,390
and so many of us are here is exactly

00:28:34,130 --> 00:28:36,740
for that it's an area that we want to

00:28:35,390 --> 00:28:37,910
make sure that we continue to invest in

00:28:36,740 --> 00:28:40,160
and take part in that community

00:28:37,910 --> 00:28:42,440
especially as it becomes a more core

00:28:40,160 --> 00:28:45,500
piece of this particular project around

00:28:42,440 --> 00:28:46,550
migration tasks so we have had sort of a

00:28:45,500 --> 00:28:47,040
couple of changes that have gone through

00:28:46,550 --> 00:28:48,500
you

00:28:47,040 --> 00:28:50,430
honestly maybe there's about a dozen

00:28:48,500 --> 00:28:52,080
patches that we've had most of those

00:28:50,430 --> 00:28:54,060
sent upstream most of them around a

00:28:52,080 --> 00:28:55,260
couple of pieces around improving the

00:28:54,060 --> 00:28:57,540
performance for a couple of things

00:28:55,260 --> 00:28:59,430
around migration or there's been some

00:28:57,540 --> 00:29:01,110
limitations around how many things we

00:28:59,430 --> 00:29:01,950
can migrate and this and that and we for

00:29:01,110 --> 00:29:04,140
the most where we send those patches

00:29:01,950 --> 00:29:06,180
upstream there's a few patches we

00:29:04,140 --> 00:29:07,560
haven't there's been Google isms you can

00:29:06,180 --> 00:29:08,550
imagine some of those custom google

00:29:07,560 --> 00:29:10,350
assist calls that no one really cares

00:29:08,550 --> 00:29:11,640
about outside of Google we haven't sent

00:29:10,350 --> 00:29:15,870
those upstream but there hasn't been a

00:29:11,640 --> 00:29:17,130
particularly big issue so again in one

00:29:15,870 --> 00:29:21,270
word it works very well it's been

00:29:17,130 --> 00:29:23,040
amazing things I could improve one of

00:29:21,270 --> 00:29:25,710
those things actually we talked about

00:29:23,040 --> 00:29:27,150
last last to stable security it's not

00:29:25,710 --> 00:29:29,070
really something that needs to improve

00:29:27,150 --> 00:29:30,900
but if you read the documentation there

00:29:29,070 --> 00:29:32,520
were commando run crew west route or run

00:29:30,900 --> 00:29:34,290
in it with a lot of capabilities there

00:29:32,520 --> 00:29:36,540
should be the default state it does do a

00:29:34,290 --> 00:29:38,040
lot of funky stuff so it makes sense for

00:29:36,540 --> 00:29:39,690
us we were able to simplify a little bit

00:29:38,040 --> 00:29:41,130
because Borg tlit does handle the setup

00:29:39,690 --> 00:29:42,720
and teardown of the environment which is

00:29:41,130 --> 00:29:44,730
where most of those privileges come and

00:29:42,720 --> 00:29:46,410
we've also added a couple patches that

00:29:44,730 --> 00:29:47,940
crew doesn't fail when certain

00:29:46,410 --> 00:29:50,700
privileged operations that maybe we

00:29:47,940 --> 00:29:52,170
don't need going to play for us we did

00:29:50,700 --> 00:29:53,790
ask our security team to look at this

00:29:52,170 --> 00:29:55,530
they told us they were much more

00:29:53,790 --> 00:29:57,360
comfortable is if we ran this and user

00:29:55,530 --> 00:29:59,310
name space without user without their

00:29:57,360 --> 00:30:01,020
root user mapped in and over the last

00:29:59,310 --> 00:30:02,250
few capabilities as possible and so

00:30:01,020 --> 00:30:04,320
we've been working very hard to try to

00:30:02,250 --> 00:30:05,670
reduce the capabilities that crew needs

00:30:04,320 --> 00:30:07,830
in order for you to be able to

00:30:05,670 --> 00:30:10,200
checkpoint restore as a user name space

00:30:07,830 --> 00:30:11,700
and so we've been working on that it was

00:30:10,200 --> 00:30:12,630
pretty awesome last Tuesday we had a lot

00:30:11,700 --> 00:30:14,490
of discussions it seems like we're not

00:30:12,630 --> 00:30:16,200
the only folks that have this use keys

00:30:14,490 --> 00:30:17,850
in mind so there's a lot of hope that

00:30:16,200 --> 00:30:21,780
this will become more of a support and

00:30:17,850 --> 00:30:23,100
in common set up for you other things

00:30:21,780 --> 00:30:24,390
that we kind of found about we talked a

00:30:23,100 --> 00:30:26,010
little bit about performance this goes

00:30:24,390 --> 00:30:27,870
past just for you there's some parts of

00:30:26,010 --> 00:30:29,700
the kernel we're like the key threads is

00:30:27,870 --> 00:30:31,080
a linked list and so if you want to wait

00:30:29,700 --> 00:30:32,910
put in all the threads you end up being

00:30:31,080 --> 00:30:35,100
an N squared operation which is not

00:30:32,910 --> 00:30:36,750
super awesome and it bugs down when we

00:30:35,100 --> 00:30:39,120
have these thousand thousand thousands

00:30:36,750 --> 00:30:40,920
of threads workloads but again that's

00:30:39,120 --> 00:30:42,240
something that I think our kernel team

00:30:40,920 --> 00:30:44,610
so far at least has expressed interest

00:30:42,240 --> 00:30:46,620
in being able to improve some parts of

00:30:44,610 --> 00:30:48,990
this we mentioned security so I won't go

00:30:46,620 --> 00:30:51,420
too much into it the talk right before

00:30:48,990 --> 00:30:54,180
here was talking about switching where

00:30:51,420 --> 00:30:56,250
the source code for some projects was

00:30:54,180 --> 00:30:57,870
hosted so I'm not going to go too much

00:30:56,250 --> 00:30:59,760
into this it's a religious debate about

00:30:57,870 --> 00:31:00,450
how to contribute patches we find it to

00:30:59,760 --> 00:31:02,820
be a little bit

00:31:00,450 --> 00:31:04,530
or to send text patches via email but

00:31:02,820 --> 00:31:09,030
that's just a personal opinion that I

00:31:04,530 --> 00:31:10,290
will not expand on too much the other

00:31:09,030 --> 00:31:11,670
areas that I think we're working with

00:31:10,290 --> 00:31:12,840
and we're very excited the community has

00:31:11,670 --> 00:31:14,490
kind of already started some work

00:31:12,840 --> 00:31:16,320
towards this live migration we really

00:31:14,490 --> 00:31:17,970
see it as long-term the goal that we

00:31:16,320 --> 00:31:19,470
would like to go towards we want to

00:31:17,970 --> 00:31:21,030
invest into this and we're happy to see

00:31:19,470 --> 00:31:23,550
a lot of the community has already

00:31:21,030 --> 00:31:25,740
started work towards this the other one

00:31:23,550 --> 00:31:27,630
that Andy mentioned on Tuesday was

00:31:25,740 --> 00:31:29,970
handling time time is hard especially

00:31:27,630 --> 00:31:31,560
like Google a lot of libraries look into

00:31:29,970 --> 00:31:33,990
the hardware counters that deal with

00:31:31,560 --> 00:31:36,120
time and it's very hard to remove this

00:31:33,990 --> 00:31:37,680
to either have the libraries handle it

00:31:36,120 --> 00:31:39,630
or make it somewhat transparent to them

00:31:37,680 --> 00:31:41,040
and so that's where a time namespace

00:31:39,630 --> 00:31:43,680
which was proposed by Andrzej and some

00:31:41,040 --> 00:31:45,390
of the folks from from Korea really

00:31:43,680 --> 00:31:46,380
would help out a lot it doesn't fix all

00:31:45,390 --> 00:31:48,810
the problems but it fix the book

00:31:46,380 --> 00:31:50,760
majority of them for us and so I think

00:31:48,810 --> 00:31:52,080
this is one of the spaces that we really

00:31:50,760 --> 00:31:53,790
kind of raise our hands and say yes it's

00:31:52,080 --> 00:31:56,940
just timing space solves a real problem

00:31:53,790 --> 00:32:00,900
that we have that we would like for the

00:31:56,940 --> 00:32:02,130
community so a future work as we move

00:32:00,900 --> 00:32:03,930
forward with this project we're very

00:32:02,130 --> 00:32:05,400
excited we're how far we've been able to

00:32:03,930 --> 00:32:06,870
come in a year and a half but we're even

00:32:05,400 --> 00:32:09,720
more excited about sort of where it's

00:32:06,870 --> 00:32:12,870
going to go long term our goal really is

00:32:09,720 --> 00:32:14,370
to reduce that's black out time as well

00:32:12,870 --> 00:32:15,240
as to improve adoption so a lot of the

00:32:14,370 --> 00:32:18,750
work that we'll be doing is building

00:32:15,240 --> 00:32:21,240
into more like insuring them or Google

00:32:18,750 --> 00:32:22,290
library is able to handle migration as

00:32:21,240 --> 00:32:24,840
well as figure out some of those tough

00:32:22,290 --> 00:32:26,160
problems when people have when people

00:32:24,840 --> 00:32:27,500
have code that looks into their

00:32:26,160 --> 00:32:30,270
environment or what that looks like

00:32:27,500 --> 00:32:32,190
our first step towards reducing blackout

00:32:30,270 --> 00:32:35,820
times will be moving to to

00:32:32,190 --> 00:32:37,320
machine-to-machine migration so instead

00:32:35,820 --> 00:32:39,240
of going through the remote storage

00:32:37,320 --> 00:32:40,650
we're going to just go directly from one

00:32:39,240 --> 00:32:42,870
machine to another from our test this

00:32:40,650 --> 00:32:45,630
reduces it to about 30 seconds of

00:32:42,870 --> 00:32:48,000
blackout time which is great we save the

00:32:45,630 --> 00:32:49,350
round-trip for the storage as well as

00:32:48,000 --> 00:32:50,370
the scheduling time so that ends up

00:32:49,350 --> 00:32:51,930
being pretty cool

00:32:50,370 --> 00:32:54,120
and finally our real goal is live

00:32:51,930 --> 00:32:55,530
migration from discussions with the creo

00:32:54,120 --> 00:32:58,110
community this is going to be hard it's

00:32:55,530 --> 00:33:00,240
gonna take some time but really is where

00:32:58,110 --> 00:33:01,980
we'd like to go for a long term once we

00:33:00,240 --> 00:33:04,050
get to those you know single-digit

00:33:01,980 --> 00:33:05,670
seconds maybe even to hundreds of

00:33:04,050 --> 00:33:06,960
milliseconds we can start talking to

00:33:05,670 --> 00:33:08,080
many more users and many of those

00:33:06,960 --> 00:33:11,470
latency sensitive work

00:33:08,080 --> 00:33:13,300
we'll be interested in this so I think

00:33:11,470 --> 00:33:15,250
with that I'm more than happy to kind of

00:33:13,300 --> 00:33:16,630
take any questions any folks have the

00:33:15,250 --> 00:33:18,520
only thing I want to say is you know I'm

00:33:16,630 --> 00:33:20,440
the person here but really this is work

00:33:18,520 --> 00:33:21,700
more than a dozen people over a year and

00:33:20,440 --> 00:33:23,890
a half it's pretty awesome we've been

00:33:21,700 --> 00:33:25,000
able to get here I'm also super excited

00:33:23,890 --> 00:33:26,770
how much help we've actually been able

00:33:25,000 --> 00:33:28,320
to get from the Korea community as well

00:33:26,770 --> 00:33:30,610
as other pieces of open-source software

00:33:28,320 --> 00:33:33,100
so it's really been awesome we're really

00:33:30,610 --> 00:33:35,140
kind of been able to take full advantage

00:33:33,100 --> 00:33:37,120
of that and we plan to continue to be

00:33:35,140 --> 00:33:38,320
involved with that with that kind of any

00:33:37,120 --> 00:33:44,230
questions I'm more than happy to answer

00:33:38,320 --> 00:33:45,280
I got a few can you go back to the

00:33:44,230 --> 00:33:48,370
previous slide

00:33:45,280 --> 00:33:51,070
so yeah this migration time you have

00:33:48,370 --> 00:33:55,870
already reduced it to about 30 seconds

00:33:51,070 --> 00:34:00,130
can you give us like a rundown of like

00:33:55,870 --> 00:34:02,650
what takes most of this 30 seconds yeah

00:34:00,130 --> 00:34:05,170
this looks I mean from my perspective

00:34:02,650 --> 00:34:08,169
this looks like a bit too much actually

00:34:05,170 --> 00:34:09,610
yeah so 30 seconds I think would be at

00:34:08,169 --> 00:34:10,899
the tail so like when you have a lot of

00:34:09,610 --> 00:34:12,669
these applications that have thousands

00:34:10,899 --> 00:34:14,620
of threads thousands of various pieces

00:34:12,669 --> 00:34:16,570
of state or even memory that's really

00:34:14,620 --> 00:34:19,330
when you see 30 seconds oh yeah like

00:34:16,570 --> 00:34:22,000
checkpoint is it restore or is it

00:34:19,330 --> 00:34:23,230
migrate in the image files or for the

00:34:22,000 --> 00:34:25,120
most part a little bit of both actually

00:34:23,230 --> 00:34:26,290
power behind you has actually the person

00:34:25,120 --> 00:34:29,950
that do the experiments I don't know if

00:34:26,290 --> 00:34:32,649
you want to expand on it a bit so

00:34:29,950 --> 00:34:35,740
there's there's a few components the big

00:34:32,649 --> 00:34:38,530
one is the one square part on the weight

00:34:35,740 --> 00:34:40,800
feed well wait for so if you have

00:34:38,530 --> 00:34:45,610
thousand threads in a process than just

00:34:40,800 --> 00:34:51,100
the way taking a lot of time this was

00:34:45,610 --> 00:34:53,350
mentioned yeah yeah

00:34:51,100 --> 00:34:55,960
migrating the image takes some time and

00:34:53,350 --> 00:35:01,240
and there's also a part of scheduling so

00:34:55,960 --> 00:35:04,390
detriments in it we might need the low

00:35:01,240 --> 00:35:06,070
priority task might wait longer to be

00:35:04,390 --> 00:35:09,070
scheduled on you machine

00:35:06,070 --> 00:35:12,460
so with a muffin to machine migration we

00:35:09,070 --> 00:35:13,960
need to find a place where we can

00:35:12,460 --> 00:35:16,260
migrate the tasks before we can actually

00:35:13,960 --> 00:35:18,390
start migrating

00:35:16,260 --> 00:35:21,090
and this kind of pathological the larger

00:35:18,390 --> 00:35:25,170
task the longer we might wait until we

00:35:21,090 --> 00:35:31,530
find the space so this sort of outside

00:35:25,170 --> 00:35:36,119
of Korea and my other question was I'm

00:35:31,530 --> 00:35:38,720
sorry can you go back is this one yeah

00:35:36,119 --> 00:35:42,510
this one there about the weight feed

00:35:38,720 --> 00:35:46,050
this one yeah so are you planning to

00:35:42,510 --> 00:35:50,790
involve some kernel guys into looking

00:35:46,050 --> 00:35:54,570
that because the original yeah a little

00:35:50,790 --> 00:35:56,730
bit of history here the Korea

00:35:54,570 --> 00:35:59,880
predecessor was in Colonel checkpoint

00:35:56,730 --> 00:36:03,359
restore and was fast and good but not

00:35:59,880 --> 00:36:05,730
mirja ball to upstream so what walked

00:36:03,359 --> 00:36:06,359
around that by implemented in all in

00:36:05,730 --> 00:36:09,330
userspace

00:36:06,359 --> 00:36:11,490
and the whole idea was to like gain some

00:36:09,330 --> 00:36:13,200
traction make sure it works and then

00:36:11,490 --> 00:36:15,200
move some components into the kernel

00:36:13,200 --> 00:36:18,270
where the need is demonstrated we're

00:36:15,200 --> 00:36:22,260
clearly stuck doing this from user space

00:36:18,270 --> 00:36:27,210
so I think we are about to be at this

00:36:22,260 --> 00:36:29,520
point so the next step should be pointed

00:36:27,210 --> 00:36:31,890
out so are you actually planning to do

00:36:29,520 --> 00:36:33,570
that or yeah so I think we're starting

00:36:31,890 --> 00:36:35,790
to have those discussions now at least

00:36:33,570 --> 00:36:37,290
we've hardly started as a first step

00:36:35,790 --> 00:36:38,910
kind of talking to the folks inside or

00:36:37,290 --> 00:36:40,410
inside Google that work on the kernel

00:36:38,910 --> 00:36:42,630
trying to said ok what would it take

00:36:40,410 --> 00:36:43,830
before we really start also reaching out

00:36:42,630 --> 00:36:45,330
part of the reason we're here is exactly

00:36:43,830 --> 00:36:47,130
that to talk about stuff like this so

00:36:45,330 --> 00:36:48,960
yes most definitely I think this is a

00:36:47,130 --> 00:36:50,690
place where we could really use the

00:36:48,960 --> 00:36:53,810
kernel being a little bit more helpful

00:36:50,690 --> 00:36:53,810
thank you

00:36:55,100 --> 00:37:06,810
any other questions no just a little bit

00:37:04,110 --> 00:37:09,450
about how you export to remote remote

00:37:06,810 --> 00:37:12,690
storage to application because I guess

00:37:09,450 --> 00:37:14,790
if you have some sort of fuse or in the

00:37:12,690 --> 00:37:16,650
face of raster mount inside application

00:37:14,790 --> 00:37:21,390
it's not possible to emigrated migrated

00:37:16,650 --> 00:37:24,930
via creo technique yeah so you must find

00:37:21,390 --> 00:37:27,000
some way to expert expert remote storage

00:37:24,930 --> 00:37:30,060
in some other way rather than simple

00:37:27,000 --> 00:37:31,770
projects like mount yes yeah that's

00:37:30,060 --> 00:37:33,090
where kind of the the libraries come in

00:37:31,770 --> 00:37:35,340
like when you talk about distributed

00:37:33,090 --> 00:37:37,560
storage like Colossus like spanner they

00:37:35,340 --> 00:37:39,420
get access over RPC so we actually have

00:37:37,560 --> 00:37:40,530
file libraries that translate all this

00:37:39,420 --> 00:37:41,580
for you so it's not really something

00:37:40,530 --> 00:37:43,320
that I would grab a third-party

00:37:41,580 --> 00:37:44,910
application run it here and they could

00:37:43,320 --> 00:37:47,430
use Colossus for the most part that's

00:37:44,910 --> 00:37:49,110
not possible so it really is another

00:37:47,430 --> 00:37:50,460
place where instead of having to deal

00:37:49,110 --> 00:37:54,420
with plastic semantics we just deal with

00:37:50,460 --> 00:37:57,810
our pcs there is a case of kind of our

00:37:54,420 --> 00:38:00,450
persistent disk that I don't remember

00:37:57,810 --> 00:38:02,910
what they do under the covers for that

00:38:00,450 --> 00:38:05,880
that's more plastics II but also we have

00:38:02,910 --> 00:38:07,590
some other components that handle that

00:38:05,880 --> 00:38:12,900
that I'm not sure offhand I don't know

00:38:07,590 --> 00:38:14,190
if anybody for the most part most of our

00:38:12,900 --> 00:38:16,460
distributed system we deal with to our

00:38:14,190 --> 00:38:16,460
PC

00:38:20,900 --> 00:38:25,670
could you provide some details police

00:38:22,819 --> 00:38:28,269
how do you connect another clean space

00:38:25,670 --> 00:38:30,799
to external or I'm sorry I can't oh

00:38:28,269 --> 00:38:32,420
sorry sorry

00:38:30,799 --> 00:38:34,519
how do you connect network nice places

00:38:32,420 --> 00:38:38,150
to the external work is it some IP

00:38:34,519 --> 00:38:42,079
Berlin or did you hurry I'm sorry I

00:38:38,150 --> 00:38:47,599
stole could you provide some details

00:38:42,079 --> 00:38:49,190
please how it works now how do configure

00:38:47,599 --> 00:38:51,680
Netherton space to connect it to

00:38:49,190 --> 00:38:54,440
external world is it like IP VLAN or

00:38:51,680 --> 00:38:57,200
something like this or yeah so we have

00:38:54,440 --> 00:39:01,009
an IP VLAN set up on the machine so it

00:38:57,200 --> 00:39:03,410
shares it shares I think / 64 with the

00:39:01,009 --> 00:39:04,819
rest of the machine so then the packets

00:39:03,410 --> 00:39:07,539
get routed there and then IP VLAN gets

00:39:04,819 --> 00:39:07,539
right within the machine

00:39:12,790 --> 00:39:17,260
I'm not sure what a connection is

00:39:15,040 --> 00:39:18,850
between Borg and kubernetes but are you

00:39:17,260 --> 00:39:20,160
planning to bring any of this into

00:39:18,850 --> 00:39:22,780
kubernetes

00:39:20,160 --> 00:39:24,220
that's a very good question we don't

00:39:22,780 --> 00:39:25,660
have any immediate plans we do work

00:39:24,220 --> 00:39:27,580
pretty closely with the kubernetes team

00:39:25,660 --> 00:39:29,710
and a lot of the things that I've seen

00:39:27,580 --> 00:39:32,230
notice things like moving to like IP IP

00:39:29,710 --> 00:39:34,510
/ tasks is something that they've

00:39:32,230 --> 00:39:35,440
champion and worked through closely so I

00:39:34,510 --> 00:39:36,910
think that if anything they might have

00:39:35,440 --> 00:39:40,240
an easier time doing some of these

00:39:36,910 --> 00:39:41,560
things then we have had no we don't kind

00:39:40,240 --> 00:39:42,850
of any immediate plans but it definitely

00:39:41,560 --> 00:39:44,770
feels like an area that we should make

00:39:42,850 --> 00:39:46,690
sure we start poking that team about

00:39:44,770 --> 00:39:52,860
this would be more generally useful for

00:39:46,690 --> 00:39:58,330
more than just us be much more difficult

00:39:52,860 --> 00:40:00,100
yes yes that is true as well so they

00:39:58,330 --> 00:40:01,750
could earn itis it will be much more

00:40:00,100 --> 00:40:03,820
difficult because then you need to do

00:40:01,750 --> 00:40:05,470
checkpoint arbitrary docker container

00:40:03,820 --> 00:40:07,480
rather than something that's

00:40:05,470 --> 00:40:09,790
controllable and uses no defensive

00:40:07,480 --> 00:40:11,380
Google libraries that will be a lot of

00:40:09,790 --> 00:40:13,450
the tough parts even everything with

00:40:11,380 --> 00:40:15,310
storage kubernetes is very varied in the

00:40:13,450 --> 00:40:17,050
way it handles storage so be somewhat

00:40:15,310 --> 00:40:20,860
limiting so some of those problems will

00:40:17,050 --> 00:40:23,260
be harder but wasn't there already some

00:40:20,860 --> 00:40:24,580
work to have dr. support for you I know

00:40:23,260 --> 00:40:26,770
there was a patch long ago I don't know

00:40:24,580 --> 00:40:29,320
that was finally merged or not I don't

00:40:26,770 --> 00:40:30,850
know if you guys know sometimes the

00:40:29,320 --> 00:40:31,480
checkpoint it's checkpoint sometimes or

00:40:30,850 --> 00:40:45,000
store okay

00:40:31,480 --> 00:40:50,560
it's unfortunate state people just try

00:40:45,000 --> 00:40:53,490
yeah no one uses works a 70 tree works

00:40:50,560 --> 00:40:53,490
with 73 suspect

00:41:04,039 --> 00:41:08,279
this sounds this sounds like a very

00:41:06,509 --> 00:41:14,279
contentious question so I will let my

00:41:08,279 --> 00:41:17,059
co-presenter Andy answer yes

00:41:14,279 --> 00:41:17,059
we'll talk offline

00:41:24,900 --> 00:41:28,619
so yeah I don't have any particularly

00:41:26,550 --> 00:41:30,900
great answer here I mean pull requests

00:41:28,619 --> 00:41:36,990
are somewhat easier to do with github

00:41:30,900 --> 00:41:41,150
since I mean it's just how do you how do

00:41:36,990 --> 00:41:43,080
you to massage this into some some

00:41:41,150 --> 00:41:46,290
representation of your change that

00:41:43,080 --> 00:41:48,510
people get done looking at so it's just

00:41:46,290 --> 00:42:01,859
from coming outside of mostly outside of

00:41:48,510 --> 00:42:03,150
the the external community but yes I

00:42:01,859 --> 00:42:06,359
think that would be a little bit simpler

00:42:03,150 --> 00:42:09,900
for us not better but simple for some

00:42:06,359 --> 00:42:11,369
people thank you cool yeah if anybody

00:42:09,900 --> 00:42:13,290
else has any questions I'm around and

00:42:11,369 --> 00:42:14,700
now they're snacks so feel free to catch

00:42:13,290 --> 00:42:16,400
up with me or Andy or any of the other

00:42:14,700 --> 00:42:19,989
folks on the team thank you

00:42:16,400 --> 00:42:19,989

YouTube URL: https://www.youtube.com/watch?v=fqBci4HpJR0


