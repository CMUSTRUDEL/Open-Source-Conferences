Title: LPC2018 - SCHED_DEADLINE desiderata and slightly crazy ideas
Publication date: 2018-12-04
Playlist: LPC2018 - LPC Main Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/62/
speaker:   Juri Lelli (Red Hat), Daniel Bristot de Oliveira (Red Hat)

The SCHED_DEADLINE scheduling policy is all but done. Even though it existed in mainline for several years, many features are yet to be implemented; some are already available as immature code, some others only exist as wishes.

In this talk Juri Lelli and Daniel Bristot De Oliveira will give the audience in-depth details of what’s missing, what’s under development and what might be desirable to have. The intent is to provide as much information as possible to people attending, so that a fruitful discussion might be held later on during hallway and micro conference sessions.

Examples of what is going to be presented are:

Non-root usage
CGroup support
Re-working RT Throttling to use DL servers
Better Priority Inheritance (AKA proxy execution)
Schedulability improvements
Better support for tracing
Captions: 
	00:00:05,800 --> 00:00:12,260
right so thanks everybody for coming or

00:00:09,440 --> 00:00:14,719
stain we're gonna actually do this

00:00:12,260 --> 00:00:17,119
presentation together with Daniel lately

00:00:14,719 --> 00:00:29,059
working for that RT group as as Daniel

00:00:17,119 --> 00:00:30,529
and busy looking after deadlines okay

00:00:29,059 --> 00:00:33,230
that's good because basically I have to

00:00:30,529 --> 00:00:33,980
assume your basic kind of really

00:00:33,230 --> 00:00:35,960
comfortable

00:00:33,980 --> 00:00:38,140
we put scattered Linney's you don't have

00:00:35,960 --> 00:00:42,920
time for introduction if you need more

00:00:38,140 --> 00:00:46,820
like general info please ask me in the

00:00:42,920 --> 00:00:52,400
next few days be more than helpful right

00:00:46,820 --> 00:00:53,960
so actually I'm here because I I know I

00:00:52,400 --> 00:00:55,880
already knew that there are a lot of

00:00:53,960 --> 00:00:58,760
things that need still to be addressed

00:00:55,880 --> 00:01:02,270
and implemented to extend this basically

00:00:58,760 --> 00:01:05,239
the bear set of things that the kernel

00:01:02,270 --> 00:01:08,509
mainline already has but then especially

00:01:05,239 --> 00:01:11,390
in last bell I always wonder myself but

00:01:08,509 --> 00:01:13,040
especially in the last few months I may

00:01:11,390 --> 00:01:17,659
actually be one okay but is this really

00:01:13,040 --> 00:01:20,870
useful and that I got the answer this

00:01:17,659 --> 00:01:22,670
year so there is a I guess I mean it is

00:01:20,870 --> 00:01:25,430
used for someone guess there are

00:01:22,670 --> 00:01:27,320
products that are heavily using this so

00:01:25,430 --> 00:01:29,659
then I thought okay so let's spend more

00:01:27,320 --> 00:01:34,030
time to actually extend and fixing thing

00:01:29,659 --> 00:01:37,850
and probably make these even more useful

00:01:34,030 --> 00:01:40,790
that's why we're here today alright so

00:01:37,850 --> 00:01:43,159
this is basically what I'd like to what

00:01:40,790 --> 00:01:45,440
we like to to cover today I'm really

00:01:43,159 --> 00:01:50,080
handling the first half of these points

00:01:45,440 --> 00:01:50,080
and then they know we do we do the rest

00:01:50,590 --> 00:01:58,310
right so this seems to me like kind of

00:01:55,610 --> 00:02:02,270
the highest priority thing that needs we

00:01:58,310 --> 00:02:05,570
address to possibly extend the user data

00:02:02,270 --> 00:02:09,039
into more users probably already know

00:02:05,570 --> 00:02:13,010
that only route is actually able to use

00:02:09,039 --> 00:02:16,599
the this scheduling the scheduling

00:02:13,010 --> 00:02:19,540
policy as of today and

00:02:16,599 --> 00:02:24,189
yeah so basically we have been asked on

00:02:19,540 --> 00:02:26,530
the real time mailing list why is it is

00:02:24,189 --> 00:02:30,640
that and what we can do to actually

00:02:26,530 --> 00:02:35,769
relax this this constraint and make the

00:02:30,640 --> 00:02:41,370
thing used by also normal users there

00:02:35,769 --> 00:02:44,500
are basically two two main points that

00:02:41,370 --> 00:02:48,790
actually needs to be addressed before we

00:02:44,500 --> 00:02:51,700
can actually be safe in written busy

00:02:48,790 --> 00:02:56,889
relax in the assumption the first thing

00:02:51,700 --> 00:02:59,980
is that currently we have a priority

00:02:56,889 --> 00:03:03,489
narrative mechanism which is kind of

00:02:59,980 --> 00:03:07,060
it's a safe route usage but then the

00:03:03,489 --> 00:03:10,030
same thing if that's actually used by

00:03:07,060 --> 00:03:13,599
normal user might be not so safe anymore

00:03:10,030 --> 00:03:16,239
I'll explain why right after this and

00:03:13,599 --> 00:03:20,919
then I guess another thing that needs to

00:03:16,239 --> 00:03:24,220
be extended and improved is how we

00:03:20,919 --> 00:03:28,780
manage the bandwidth associated with

00:03:24,220 --> 00:03:31,180
these non-root users you route basically

00:03:28,780 --> 00:03:37,030
delegate in some way the available

00:03:31,180 --> 00:03:38,650
bandwidth to to users all right so yeah

00:03:37,030 --> 00:03:41,400
actually there is also a third point

00:03:38,650 --> 00:03:45,760
over there there is we currently lack

00:03:41,400 --> 00:03:50,829
Lipsy interface support for actually in

00:03:45,760 --> 00:03:57,370
the cisco let's get the cetera Cisco I

00:03:50,829 --> 00:04:00,549
actually sent a query email in the Lib C

00:03:57,370 --> 00:04:03,970
of meaningless so basically extending

00:04:00,549 --> 00:04:06,220
the pit read wrappers it's it seems

00:04:03,970 --> 00:04:08,769
feasible so those guys were saying okay

00:04:06,220 --> 00:04:10,750
yeah I guess there is someone needs to

00:04:08,769 --> 00:04:12,400
do it but it's actually feasible so I

00:04:10,750 --> 00:04:16,030
guess it's one of the points we won't

00:04:12,400 --> 00:04:18,220
address but to me looks like it's not

00:04:16,030 --> 00:04:23,340
really useful until we actually cover

00:04:18,220 --> 00:04:23,340
the other two points because yeah

00:04:25,280 --> 00:04:33,140
okay so what it means getting better

00:04:29,480 --> 00:04:33,890
priority inheritance alright so what's

00:04:33,140 --> 00:04:35,750
the problem

00:04:33,890 --> 00:04:38,900
the problem is that currently we

00:04:35,750 --> 00:04:42,110
implement deadly inheritance which

00:04:38,900 --> 00:04:44,840
actually has some slightly problems by

00:04:42,110 --> 00:04:49,400
itself let's say that what we have is

00:04:44,840 --> 00:04:51,860
that when when some when a deadlines a

00:04:49,400 --> 00:04:56,990
squishy blocks on musics we can inherit

00:04:51,860 --> 00:05:00,740
its its deadline so it's basically the

00:04:56,990 --> 00:05:04,610
extent of the fixed priority inheritance

00:05:00,740 --> 00:05:09,110
thing and the big problem with that is

00:05:04,610 --> 00:05:12,410
that we also need to better relax the

00:05:09,110 --> 00:05:15,560
runtime enforcement of this boosted task

00:05:12,410 --> 00:05:18,440
because basically we want the team to

00:05:15,560 --> 00:05:20,600
battle it is that I want I know that the

00:05:18,440 --> 00:05:23,750
thing is inside the critical section so

00:05:20,600 --> 00:05:26,300
I would like to that thing to finish as

00:05:23,750 --> 00:05:29,180
soon as possible so I don't want to stop

00:05:26,300 --> 00:05:31,400
stop it while executing inside a

00:05:29,180 --> 00:05:33,440
critical section and actually I don't

00:05:31,400 --> 00:05:36,110
think there is much more that we can do

00:05:33,440 --> 00:05:39,410
at this point we I mean with with this

00:05:36,110 --> 00:05:42,320
basic mechanism what we actually would

00:05:39,410 --> 00:05:45,320
need so let's say that the the proper

00:05:42,320 --> 00:05:47,300
thing to to be done is to actually you

00:05:45,320 --> 00:05:50,000
know it not only the deadline but also

00:05:47,300 --> 00:05:52,880
the runtime so basically the idea sooner

00:05:50,000 --> 00:05:56,380
be able to inherit but both runtime

00:05:52,880 --> 00:05:59,060
period so basically inheriting the whole

00:05:56,380 --> 00:06:01,940
band with each of the donors and use

00:05:59,060 --> 00:06:06,410
that potentially to execute in a

00:06:01,940 --> 00:06:09,560
critical section this will allow us to

00:06:06,410 --> 00:06:12,169
keep the runtime reinforcement on so

00:06:09,560 --> 00:06:16,520
basically that means that will be safe

00:06:12,169 --> 00:06:18,800
to actually give non-root users the

00:06:16,520 --> 00:06:23,370
possibility to use the the policy also

00:06:18,800 --> 00:06:25,750
to deal with shared shared data and

00:06:23,370 --> 00:06:31,340
[Music]

00:06:25,750 --> 00:06:34,180
basically that means that we will we

00:06:31,340 --> 00:06:37,640
would like to let the imitates owner

00:06:34,180 --> 00:06:38,990
executing we'd be scheduling context

00:06:37,640 --> 00:06:42,470
information

00:06:38,990 --> 00:06:47,810
who is blocked on him and his breathe us

00:06:42,470 --> 00:06:51,260
to the proxy execution concept yeah I

00:06:47,810 --> 00:06:54,320
just wanted to show you like a simple

00:06:51,260 --> 00:06:56,690
example why we want this and by what we

00:06:54,320 --> 00:06:59,330
have is bad basically what can happens

00:06:56,690 --> 00:07:01,640
if you don't have any mechanism you

00:06:59,330 --> 00:07:04,400
probably I mean already know this but

00:07:01,640 --> 00:07:09,500
what can happen is that you have low

00:07:04,400 --> 00:07:10,970
priority low priority guy that is entry

00:07:09,500 --> 00:07:13,160
activity resection because it's

00:07:10,970 --> 00:07:17,330
basically locking in this case mutex a

00:07:13,160 --> 00:07:19,820
then there is a higher highest priority

00:07:17,330 --> 00:07:23,030
guys that locks the same so basically

00:07:19,820 --> 00:07:25,340
blocks on mutex a so they're low

00:07:23,030 --> 00:07:27,800
priority guys can in the critical

00:07:25,340 --> 00:07:30,350
section but while doing so the reason

00:07:27,800 --> 00:07:33,410
for example in these in the example a

00:07:30,350 --> 00:07:35,300
medium priority guy that is actually

00:07:33,410 --> 00:07:37,220
scheduled on the CPU because it's a

00:07:35,300 --> 00:07:39,730
higher priority than the lowest one

00:07:37,220 --> 00:07:41,960
because this deadline is actually

00:07:39,730 --> 00:07:44,390
earlier than the lower priority

00:07:41,960 --> 00:07:46,340
so it basically preempts the Garrity was

00:07:44,390 --> 00:07:48,620
actually securing the problem is not

00:07:46,340 --> 00:07:51,800
that is preempted be empty in the low

00:07:48,620 --> 00:07:53,630
priority one but it's actually delaying

00:07:51,800 --> 00:07:55,610
the execution of the highest peril he

00:07:53,630 --> 00:07:57,560
tried the guy and actually in this case

00:07:55,610 --> 00:08:03,110
is causing a deadline miss in the

00:07:57,560 --> 00:08:06,230
highest priority one we what we have

00:08:03,110 --> 00:08:09,200
today we can actually fix this problem

00:08:06,230 --> 00:08:12,160
because basically the the low priority

00:08:09,200 --> 00:08:14,810
one will actually inherit the highest

00:08:12,160 --> 00:08:17,600
priority deadline the problem that we

00:08:14,810 --> 00:08:20,060
have today is that since the lowest

00:08:17,600 --> 00:08:22,820
priority one is outside of run time

00:08:20,060 --> 00:08:26,120
enforcement you can actually continue

00:08:22,820 --> 00:08:29,290
executing for how much time he needs her

00:08:26,120 --> 00:08:32,590
once and then can actually be

00:08:29,290 --> 00:08:36,230
jeopardizing the execution or whatever

00:08:32,590 --> 00:08:36,590
task actually need through to run on the

00:08:36,230 --> 00:08:42,980
system

00:08:36,590 --> 00:08:44,660
so that's bad as well with priority with

00:08:42,980 --> 00:08:47,690
the proper priority

00:08:44,660 --> 00:08:50,150
inherit mechanism which is proxy

00:08:47,690 --> 00:08:52,210
execution what we can actually achieve

00:08:50,150 --> 00:08:55,540
is that the Wendy

00:08:52,210 --> 00:08:59,800
highest-priority guy blocks on mutex a

00:08:55,540 --> 00:09:01,720
it will basically give the lowest

00:08:59,800 --> 00:09:06,720
priority one the possibility to actually

00:09:01,720 --> 00:09:12,100
execute using the highest priority

00:09:06,720 --> 00:09:15,760
process basically been with it so in

00:09:12,100 --> 00:09:19,480
this case well this case probably the

00:09:15,760 --> 00:09:21,540
system was bad design so the highest

00:09:19,480 --> 00:09:23,410
priority one will probably miss the

00:09:21,540 --> 00:09:25,870
deadlines well just because the lowest

00:09:23,410 --> 00:09:28,750
one was exceeded for too much but at

00:09:25,870 --> 00:09:31,810
least the medium one will actually be

00:09:28,750 --> 00:09:35,500
able to complete inside he is his

00:09:31,810 --> 00:09:37,300
deadline parameter okay so that's

00:09:35,500 --> 00:09:41,830
basically what we want to achieve and

00:09:37,300 --> 00:09:46,570
the way we think these can be achieved

00:09:41,830 --> 00:09:49,899
is about to implement this process

00:09:46,570 --> 00:09:51,670
execution actually I have to send Peter

00:09:49,899 --> 00:09:55,209
because weather has started this this

00:09:51,670 --> 00:09:58,240
work based on his ideas and patches

00:09:55,209 --> 00:10:00,459
I guess the is pretty complex thing so

00:09:58,240 --> 00:10:03,400
I'll try to just give you an overview of

00:10:00,459 --> 00:10:06,760
how he works and I guess one of the main

00:10:03,400 --> 00:10:09,510
points want to understand for

00:10:06,760 --> 00:10:12,610
understanding how the team work is that

00:10:09,510 --> 00:10:15,490
inside it a struct we can actually

00:10:12,610 --> 00:10:18,550
potentially divide between information

00:10:15,490 --> 00:10:22,830
that pertains to scheduling so all the

00:10:18,550 --> 00:10:26,800
information for example contained in the

00:10:22,830 --> 00:10:29,830
task struct DL which points to the ask a

00:10:26,800 --> 00:10:31,630
deadline entity information where we

00:10:29,830 --> 00:10:34,150
actually track for example run time and

00:10:31,630 --> 00:10:36,730
deadlines and then there is another part

00:10:34,150 --> 00:10:39,339
which relates to the actual execution so

00:10:36,730 --> 00:10:42,010
what information the scheduler and the

00:10:39,339 --> 00:10:42,610
system needs to actually run a task

00:10:42,010 --> 00:10:44,650
properly

00:10:42,610 --> 00:10:47,380
for example Afiya tea I mean respect

00:10:44,650 --> 00:10:50,589
India Theatre the task is one of those

00:10:47,380 --> 00:10:52,959
information that there is actually

00:10:50,589 --> 00:10:55,779
needed and basically what we want to

00:10:52,959 --> 00:10:57,520
inherit is the part of the of the

00:10:55,779 --> 00:11:01,920
scaling so we want to be able to use the

00:10:57,520 --> 00:11:05,530
scattering part to actually execute the

00:11:01,920 --> 00:11:08,890
boost task on on a CPU

00:11:05,530 --> 00:11:13,060
so if we go back to an example what does

00:11:08,890 --> 00:11:16,750
it mean is that as soon as the highest

00:11:13,060 --> 00:11:20,770
per te tasks blocks on the on the mutex

00:11:16,750 --> 00:11:23,500
a in this case it will let the lowest

00:11:20,770 --> 00:11:28,290
priority one actually X continue running

00:11:23,500 --> 00:11:31,120
but using he's it's scared deadline

00:11:28,290 --> 00:11:34,750
information so basically low lowest

00:11:31,120 --> 00:11:36,580
priority one will able to continue run

00:11:34,750 --> 00:11:39,910
in at basically highest priority but

00:11:36,580 --> 00:11:43,810
still be run inside the highest priority

00:11:39,910 --> 00:11:46,480
server I mean the the bandwidth how do

00:11:43,810 --> 00:11:49,030
we do that basically we use the the

00:11:46,480 --> 00:11:51,880
mutex has the point of synchronization

00:11:49,030 --> 00:11:55,270
so we track what are the tasks blocked

00:11:51,880 --> 00:11:59,830
on the mutex and we actually have a

00:11:55,270 --> 00:12:01,510
medics owner so as soon as the mutex one

00:11:59,830 --> 00:12:05,110
for example is preempted or something

00:12:01,510 --> 00:12:08,020
happens on a cpu we go there and we try

00:12:05,110 --> 00:12:09,670
to select which one of the tasks is

00:12:08,020 --> 00:12:13,150
blocked currently blocked on the mutex

00:12:09,670 --> 00:12:17,050
can be selected as the proxy for the

00:12:13,150 --> 00:12:19,780
mutex owner i guess one thing to realize

00:12:17,050 --> 00:12:23,350
here and it's kind of big difference

00:12:19,780 --> 00:12:26,710
with respect to how the current priority

00:12:23,350 --> 00:12:28,960
variance mechanism work is that the the

00:12:26,710 --> 00:12:31,540
task blocked on the mutex are not really

00:12:28,960 --> 00:12:33,610
removed i mean are not removed from the

00:12:31,540 --> 00:12:35,830
from your queue so that they can be

00:12:33,610 --> 00:12:37,480
selected by the scheduler but when

00:12:35,830 --> 00:12:38,050
basically the schedule is select one of

00:12:37,480 --> 00:12:41,220
those

00:12:38,050 --> 00:12:47,020
we actually kick the mechanism and then

00:12:41,220 --> 00:12:49,420
basically use the the the run I mean the

00:12:47,020 --> 00:12:52,480
scheduling information of the task that

00:12:49,420 --> 00:12:55,300
would have been selected for run but we

00:12:52,480 --> 00:12:58,180
actually put the the owner on the CPU to

00:12:55,300 --> 00:13:02,730
run so that's basically the trick let's

00:12:58,180 --> 00:13:02,730
say alright

00:13:05,220 --> 00:13:12,720
right so we basically addressing this

00:13:09,370 --> 00:13:16,389
from the point of view of trying to make

00:13:12,720 --> 00:13:18,790
protein Athens for deadline better but

00:13:16,389 --> 00:13:21,490
personally I think that the mechanism

00:13:18,790 --> 00:13:24,069
itself might be actually more general

00:13:21,490 --> 00:13:27,100
than just dealing with let's say Artie

00:13:24,069 --> 00:13:31,629
priorities so when you have a mechanism

00:13:27,100 --> 00:13:33,970
that makes you able to use the schedule

00:13:31,629 --> 00:13:37,420
information of other tasks to actually

00:13:33,970 --> 00:13:40,000
handle the execution of some other tasks

00:13:37,420 --> 00:13:41,980
you can principal thing to extend the

00:13:40,000 --> 00:13:45,310
team to other to other type of

00:13:41,980 --> 00:13:47,500
information for example yeah nice levels

00:13:45,310 --> 00:13:50,170
for CFS of course Artie priority but

00:13:47,500 --> 00:13:55,240
also there is a new proposition from

00:13:50,170 --> 00:13:57,430
from Patrick which is which is a new

00:13:55,240 --> 00:14:02,709
interface which a user can actually

00:13:57,430 --> 00:14:05,050
specify utilization level for a task or

00:14:02,709 --> 00:14:08,439
set of tasks and those will influence

00:14:05,050 --> 00:14:10,480
for example frequency selection or load

00:14:08,439 --> 00:14:13,240
balance in decision in the future in

00:14:10,480 --> 00:14:16,870
principle we can actually think of using

00:14:13,240 --> 00:14:21,339
the proxy execution idea to let those

00:14:16,870 --> 00:14:24,189
tasks automatically inherit those those

00:14:21,339 --> 00:14:30,819
property so yeah it might be more

00:14:24,189 --> 00:14:35,259
general than originally intended okay so

00:14:30,819 --> 00:14:37,779
that was it for for the proxy so for the

00:14:35,259 --> 00:14:42,189
first point I wanted to cover there will

00:14:37,779 --> 00:14:43,600
be a lot in the afternoon on the RT

00:14:42,189 --> 00:14:45,519
micro conference so if you want to

00:14:43,600 --> 00:14:48,610
discuss more there will be time there or

00:14:45,519 --> 00:14:51,009
of course stop me in the hallway next

00:14:48,610 --> 00:14:53,500
things to cover is about C group support

00:14:51,009 --> 00:14:57,399
so currently deadline doesn't support C

00:14:53,500 --> 00:14:59,920
group instead it's only using the Cisco

00:14:57,399 --> 00:15:03,610
so it's one one-to-one association

00:14:59,920 --> 00:15:08,050
between data reservation so bandwidth

00:15:03,610 --> 00:15:11,889
allocation and single threads group

00:15:08,050 --> 00:15:16,290
support might be helpful and they might

00:15:11,889 --> 00:15:16,290
actually mean two different things

00:15:16,340 --> 00:15:21,510
you might be wanting to use cgroups

00:15:19,080 --> 00:15:23,840
to just basically do bad with the

00:15:21,510 --> 00:15:26,130
management so allocate the available

00:15:23,840 --> 00:15:29,040
bandwidth or the system to different

00:15:26,130 --> 00:15:31,200
users or actually you want to use the

00:15:29,040 --> 00:15:33,210
team to actually perform hierarchical

00:15:31,200 --> 00:15:38,400
scheduling I'll explain in a bit what

00:15:33,210 --> 00:15:41,400
that means so the simplest thing you you

00:15:38,400 --> 00:15:45,630
you can do and I actually send patches

00:15:41,400 --> 00:15:48,420
awhile ago on a mini list is to Basel

00:15:45,630 --> 00:15:51,780
Etsy so demonstrators receive a fraction

00:15:48,420 --> 00:15:53,840
of the total bandwidth to users using

00:15:51,780 --> 00:15:58,970
the same interface that actually r/t

00:15:53,840 --> 00:16:01,560
scat FIFO or are already already have

00:15:58,970 --> 00:16:04,400
this basically the schedule itself is

00:16:01,560 --> 00:16:06,930
not hierarchical in meaning that

00:16:04,400 --> 00:16:10,170
schedule and entities will still be

00:16:06,930 --> 00:16:14,640
treated at the root level so as single

00:16:10,170 --> 00:16:17,810
tasks but what you can use is that you

00:16:14,640 --> 00:16:23,130
can use the CPU RT runtime and period

00:16:17,810 --> 00:16:25,560
microsecond files to actually I mean

00:16:23,130 --> 00:16:27,810
split with bandit you are available

00:16:25,560 --> 00:16:30,000
among your deadline tasks so basically

00:16:27,810 --> 00:16:34,140
using a sim interface to actually deal

00:16:30,000 --> 00:16:37,320
with deadline bandit the implementation

00:16:34,140 --> 00:16:40,620
that I did basically this requires RT

00:16:37,320 --> 00:16:44,850
groups scared to be configuring not sure

00:16:40,620 --> 00:16:54,000
how widely that is used that is one of

00:16:44,850 --> 00:16:58,110
the questions yeah that we want to I've

00:16:54,000 --> 00:17:05,370
been hearing that people are actually

00:16:58,110 --> 00:17:12,150
sometimes maybe using it right yeah yeah

00:17:05,370 --> 00:17:15,839
I got the same patient so it's are using

00:17:12,150 --> 00:17:22,230
RT groups CAD to avoid causing

00:17:15,839 --> 00:17:27,510
starvation before RT priorities yeah so

00:17:22,230 --> 00:17:29,520
I mean I'm wondering why I mean if we

00:17:27,510 --> 00:17:31,530
want to remove it because we need

00:17:29,520 --> 00:17:33,810
we'll simplify things for implementing

00:17:31,530 --> 00:17:36,690
something let's say new but they're

00:17:33,810 --> 00:17:40,230
gonna guess if we had to support it we

00:17:36,690 --> 00:17:42,420
had to there is no point I guess maybe

00:17:40,230 --> 00:17:45,870
the next point that I'm gonna cover

00:17:42,420 --> 00:17:49,370
might actually help in let's say fixing

00:17:45,870 --> 00:17:51,600
what r/t scat group is actually doing so

00:17:49,370 --> 00:17:53,550
well that's basically that there is the

00:17:51,600 --> 00:17:56,370
first thing that we might want to have

00:17:53,550 --> 00:18:00,170
it might be useful because then it's

00:17:56,370 --> 00:18:04,200
it's already a way to give system

00:18:00,170 --> 00:18:06,920
administrators a way to manage bandwidth

00:18:04,200 --> 00:18:10,320
but it might not be the only thing

00:18:06,920 --> 00:18:13,610
needed so the next thing that we want to

00:18:10,320 --> 00:18:18,870
probably have is a proper hierarchical

00:18:13,610 --> 00:18:21,480
scheduling of this there is there has

00:18:18,870 --> 00:18:23,880
been actually a posting by alessio in

00:18:21,480 --> 00:18:26,460
the past on a mini list but then I guess

00:18:23,880 --> 00:18:29,640
development stopped a bit yes time time

00:18:26,460 --> 00:18:33,270
reason what it does is basically

00:18:29,640 --> 00:18:38,190
creating a two level at least in the

00:18:33,270 --> 00:18:41,880
implementation scheduler where a root

00:18:38,190 --> 00:18:44,550
level you have both single entities and

00:18:41,880 --> 00:18:46,740
group entities in this case the second

00:18:44,550 --> 00:18:49,680
entity which is a deadline scheduling

00:18:46,740 --> 00:18:52,260
entity is actually associated to with a

00:18:49,680 --> 00:18:56,250
group of tasks and those tasks are

00:18:52,260 --> 00:18:58,740
actually five four entities what that

00:18:56,250 --> 00:19:01,490
mean is that basically the scheduler

00:18:58,740 --> 00:19:04,140
first select the at root level

00:19:01,490 --> 00:19:06,690
considering basically priorities so data

00:19:04,140 --> 00:19:08,670
in priorities and then you have to run a

00:19:06,690 --> 00:19:11,270
second level scheduler that selects

00:19:08,670 --> 00:19:15,600
which one of the FIFO tasks needs to be

00:19:11,270 --> 00:19:18,120
to be run the interface you use is the

00:19:15,600 --> 00:19:20,790
same as of today you just need to

00:19:18,120 --> 00:19:23,760
basically write the run time period

00:19:20,790 --> 00:19:26,160
parameters and then you echo your tasks

00:19:23,760 --> 00:19:33,030
I mean you're five foot task inside the

00:19:26,160 --> 00:19:36,540
tasks file in the in this group why this

00:19:33,030 --> 00:19:38,340
might be helpful this is a simple

00:19:36,540 --> 00:19:42,720
example let's say you have a pipeline of

00:19:38,340 --> 00:19:44,460
tasks if you don't have the C group

00:19:42,720 --> 00:19:48,630
and that's basically what you have today

00:19:44,460 --> 00:19:51,419
is the force plot then let's say if you

00:19:48,630 --> 00:19:54,059
have four data line entities the dhol

00:19:51,419 --> 00:19:56,429
are basically part of this pipeline what

00:19:54,059 --> 00:19:59,280
you have to do is for each single entity

00:19:56,429 --> 00:20:01,440
you have to go there and assign them run

00:19:59,280 --> 00:20:04,049
time and deadlines so there might be

00:20:01,440 --> 00:20:05,429
kind of cumbersome or even not possible

00:20:04,049 --> 00:20:08,100
because basically maybe you don't know

00:20:05,429 --> 00:20:09,480
actually the the single and the single

00:20:08,100 --> 00:20:11,510
entities deadlines and runtime

00:20:09,480 --> 00:20:15,720
parameters so there might be actually

00:20:11,510 --> 00:20:18,360
not very easy to to use instead if we

00:20:15,720 --> 00:20:21,900
have the group support so the group

00:20:18,360 --> 00:20:25,169
support you just need to specify a whole

00:20:21,900 --> 00:20:27,630
run time over deadlines so just two

00:20:25,169 --> 00:20:29,429
parameters that those are your group

00:20:27,630 --> 00:20:33,480
parameters and then you put all your

00:20:29,429 --> 00:20:35,909
five for tasks that are basically

00:20:33,480 --> 00:20:38,130
composing your day your pipeline inside

00:20:35,909 --> 00:20:41,549
is a reservation it'll be basically

00:20:38,130 --> 00:20:43,950
handle so basically you reduce the

00:20:41,549 --> 00:20:51,960
problem of specifying in this case for

00:20:43,950 --> 00:20:54,330
couple of parameters to just one I guess

00:20:51,960 --> 00:20:57,720
one of the problems that at least the

00:20:54,330 --> 00:21:00,960
implementation that we had has is that

00:20:57,720 --> 00:21:03,960
while having for example in this case on

00:21:00,960 --> 00:21:06,150
SMP system of course you might speed up

00:21:03,960 --> 00:21:08,190
things because one of the stages of your

00:21:06,150 --> 00:21:11,880
pipeline may be running concurrently on

00:21:08,190 --> 00:21:14,010
different on different cores at the same

00:21:11,880 --> 00:21:16,289
time the problem is that how the thing

00:21:14,010 --> 00:21:19,020
is implemented today is that you

00:21:16,289 --> 00:21:21,570
actually reserved the fraction that the

00:21:19,020 --> 00:21:25,470
group has asked the fractional bandwidth

00:21:21,570 --> 00:21:29,520
on all D on all the course so you might

00:21:25,470 --> 00:21:31,440
be wasting bandwidth on the course are

00:21:29,520 --> 00:21:33,990
not actually used there are different

00:21:31,440 --> 00:21:35,900
ways we can probably deal with that for

00:21:33,990 --> 00:21:38,789
example one thing we can do is that

00:21:35,900 --> 00:21:42,919
let's say that the CPU n is not used in

00:21:38,789 --> 00:21:47,460
this example and we can actually try to

00:21:42,919 --> 00:21:48,510
use the leftover bandwidth in cases we

00:21:47,460 --> 00:21:51,929
already have it is a mechanism

00:21:48,510 --> 00:21:55,080
implemented in the email line called

00:21:51,929 --> 00:21:56,190
grub so you busy reclaim the

00:21:55,080 --> 00:21:57,929
you've been with me so they might

00:21:56,190 --> 00:21:59,789
probably help in this case but it's

00:21:57,929 --> 00:22:05,220
something that we have to figure out and

00:21:59,789 --> 00:22:07,019
look at right so yeah that concludes my

00:22:05,220 --> 00:22:11,429
part I guess we can do questions

00:22:07,019 --> 00:22:19,260
afterwards I have to enable still

00:22:11,429 --> 00:22:23,519
roasted time in short I will enable the

00:22:19,260 --> 00:22:25,980
Steven roasted mode so heavily will okay

00:22:23,519 --> 00:22:28,470
one of the ideas also is to rework the

00:22:25,980 --> 00:22:30,750
realtime proudly to use the old servers

00:22:28,470 --> 00:22:33,539
which is something very similar to what

00:22:30,750 --> 00:22:34,409
you already explained but we work on

00:22:33,539 --> 00:22:37,440
another level

00:22:34,409 --> 00:22:40,019
so rather than working in the cigrip

00:22:37,440 --> 00:22:43,919
level we would move that idea of

00:22:40,019 --> 00:22:47,580
hierarchical to the okay no wait I'm

00:22:43,919 --> 00:22:53,460
going to fastest return to daniel mode

00:22:47,580 --> 00:22:55,169
so what is the real time traveling the

00:22:53,460 --> 00:22:57,690
real time throttling is a safeguard for

00:22:55,169 --> 00:23:02,250
misbehaving real-time tasks so they

00:22:57,690 --> 00:23:04,799
would not monopolize the CPU and in such

00:23:02,250 --> 00:23:07,799
way to restart normal tasks so we have

00:23:04,799 --> 00:23:11,460
the period of one second by default and

00:23:07,799 --> 00:23:16,019
we say that we can use 95% of the time

00:23:11,460 --> 00:23:19,350
running real-time tasks but if we run

00:23:16,019 --> 00:23:22,350
for this long we will throw the our

00:23:19,350 --> 00:23:25,409
queues and then we let the normal tasks

00:23:22,350 --> 00:23:28,470
to run why do we need it we need it

00:23:25,409 --> 00:23:31,019
because there are some local per CPU

00:23:28,470 --> 00:23:33,149
tasks that need to run periodically to

00:23:31,019 --> 00:23:37,139
do housekeeping like our see you Fred

00:23:33,149 --> 00:23:39,570
for example so we need to to left this

00:23:37,139 --> 00:23:41,730
room for normal tasks and also to not

00:23:39,570 --> 00:23:44,100
block us out of the system if

00:23:41,730 --> 00:23:46,620
misbehaving real-time tasks runs forever

00:23:44,100 --> 00:23:51,809
and we cannot even stop it

00:23:46,620 --> 00:23:54,480
so okay that's the case for single core

00:23:51,809 --> 00:23:58,260
where we have the runtime of repaired

00:23:54,480 --> 00:24:01,889
for real real-time tasks and the space

00:23:58,260 --> 00:24:06,899
available for normal tasks the rest okay

00:24:01,889 --> 00:24:08,820
when we have the boot course we have

00:24:06,899 --> 00:24:11,759
also this limitation but

00:24:08,820 --> 00:24:14,610
we can take one time for from one CPU

00:24:11,759 --> 00:24:16,950
and move to another so we allowed the

00:24:14,610 --> 00:24:20,700
real-time tasks to run a little bit more

00:24:16,950 --> 00:24:22,919
so for example here we have a CPU with

00:24:20,700 --> 00:24:25,590
just a fewer deadline task that is not

00:24:22,919 --> 00:24:27,809
using all the available run time we can

00:24:25,590 --> 00:24:30,690
get this time move to the other CPU that

00:24:27,809 --> 00:24:32,549
is full of real-time tasks so this is

00:24:30,690 --> 00:24:33,649
the RT run time sharing enabled by

00:24:32,549 --> 00:24:36,539
default

00:24:33,649 --> 00:24:38,429
so yeah explaining things IDs we see

00:24:36,539 --> 00:24:44,399
that things work so what is the deal

00:24:38,429 --> 00:24:48,330
here so one of the problems that comes

00:24:44,399 --> 00:24:51,570
also from people that runs one busy loop

00:24:48,330 --> 00:24:55,019
in real time is that when we have the RT

00:24:51,570 --> 00:24:57,720
praten enable we may have the other case

00:24:55,019 --> 00:25:03,269
in which the real-time tasks are Prato

00:24:57,720 --> 00:25:06,600
to run nothing to go idle and so some of

00:25:03,269 --> 00:25:09,029
these users I'm not saying if they are

00:25:06,600 --> 00:25:11,700
doing the right thing by running busily

00:25:09,029 --> 00:25:13,710
for real-time but they exist so they

00:25:11,700 --> 00:25:16,259
complain that by using the real-time

00:25:13,710 --> 00:25:18,539
frothing and not using the arterial time

00:25:16,259 --> 00:25:21,149
sure we might have this undesired

00:25:18,539 --> 00:25:24,139
behavior in which we are profiting they

00:25:21,149 --> 00:25:29,750
are accuse the arc use of real-time

00:25:24,139 --> 00:25:32,610
threads to run nothing and when we have

00:25:29,750 --> 00:25:34,769
multiple CPUs we might have also the

00:25:32,610 --> 00:25:39,000
case in which we take a runtime from

00:25:34,769 --> 00:25:41,279
another CPU and run but we don't what we

00:25:39,000 --> 00:25:44,639
end up missing the fact that we can have

00:25:41,279 --> 00:25:48,870
no real-time tasks here and we end up

00:25:44,639 --> 00:25:51,049
starving this task so we take run time

00:25:48,870 --> 00:25:54,029
from a CPU that is that has real-time

00:25:51,049 --> 00:25:55,830
available move to another CPU and it

00:25:54,029 --> 00:25:58,590
will allows the runtime tasks run

00:25:55,830 --> 00:26:02,879
forever starving safest tasks and cows

00:25:58,590 --> 00:26:08,909
and ours who stalls and lock them locks

00:26:02,879 --> 00:26:14,389
plants so that's why we are thinking on

00:26:08,909 --> 00:26:17,039
real work in the real time profiting so

00:26:14,389 --> 00:26:20,759
rather than implementing throttling as a

00:26:17,039 --> 00:26:24,749
way to to provoke the

00:26:20,759 --> 00:26:26,749
real-time tasks or run queues we will

00:26:24,749 --> 00:26:30,739
work in the same idea of the deadline

00:26:26,749 --> 00:26:33,869
providing run time and the bandwidth for

00:26:30,739 --> 00:26:41,999
runtime tap for real-time tasks and non

00:26:33,869 --> 00:26:46,429
real-time tasks so in the case that we

00:26:41,999 --> 00:26:50,279
will not by using like for example a

00:26:46,429 --> 00:26:52,529
route deadline scheduler per CPU in

00:26:50,279 --> 00:26:54,960
which we have a constant bandwidth

00:26:52,529 --> 00:26:57,929
server or any other server to provide

00:26:54,960 --> 00:27:00,629
run time for these tasks and on order to

00:26:57,929 --> 00:27:03,690
provide run time for normal tasks we

00:27:00,629 --> 00:27:05,429
would avoid postpone in this task

00:27:03,690 --> 00:27:09,659
because it would have some runtime

00:27:05,429 --> 00:27:12,480
guarantee and it will because of the

00:27:09,659 --> 00:27:17,429
deadlines property it will be scheduled

00:27:12,480 --> 00:27:19,739
and not unpro like rather than it will

00:27:17,429 --> 00:27:21,299
not run because it was brought out by

00:27:19,739 --> 00:27:23,639
these because this was proto but

00:27:21,299 --> 00:27:28,129
actually because this has a priority and

00:27:23,639 --> 00:27:31,590
it will have the possibility of running

00:27:28,129 --> 00:27:34,850
for example we can have a 900

00:27:31,590 --> 00:27:38,730
milliseconds reservation for the

00:27:34,850 --> 00:27:42,419
real-time tasks over one millisecond ok

00:27:38,730 --> 00:27:45,450
it's over one second and we can have

00:27:42,419 --> 00:27:48,749
like 50 milliseconds every second for

00:27:45,450 --> 00:27:50,909
the CBS for DCPS of the normal tasks so

00:27:48,749 --> 00:27:53,489
it will get its priority based on

00:27:50,909 --> 00:27:56,159
deadlines so in the future this task

00:27:53,489 --> 00:28:02,489
will become the highest priority and you

00:27:56,159 --> 00:28:05,369
will be able to run ok this this seems

00:28:02,489 --> 00:28:07,309
to resolve the problem of starving this

00:28:05,369 --> 00:28:16,830
task because it will have a reservation

00:28:07,309 --> 00:28:20,100
but ok it is this is the baseline but in

00:28:16,830 --> 00:28:22,980
the case that we don't run a normal test

00:28:20,100 --> 00:28:27,450
for the all the time we can reclaim the

00:28:22,980 --> 00:28:32,700
time for running real-time tasks so we

00:28:27,450 --> 00:28:34,510
will be able to resolve this problem by

00:28:32,700 --> 00:28:39,040
prioritizing this test

00:28:34,510 --> 00:28:44,940
using deadlines and this problem by

00:28:39,040 --> 00:28:49,180
using reclaiming using the VL server but

00:28:44,940 --> 00:28:52,420
things are not as simple as it seems

00:28:49,180 --> 00:28:55,000
because for example this is scheduled

00:28:52,420 --> 00:28:57,120
here as both the normal tasks and

00:28:55,000 --> 00:29:00,640
real-time tasks has the same apparent

00:28:57,120 --> 00:29:03,610
this timeline is also possible using the

00:29:00,640 --> 00:29:05,970
EDF scheduler because both have the same

00:29:03,610 --> 00:29:08,430
priority I can schedule any of these

00:29:05,970 --> 00:29:10,960
reservations and this is also feasible

00:29:08,430 --> 00:29:14,760
scheduling for the deadline scheduler

00:29:10,960 --> 00:29:17,470
but this is not what we want we want the

00:29:14,760 --> 00:29:19,240
why we still have run time for the

00:29:17,470 --> 00:29:22,450
deadline tasks we want to prioritize

00:29:19,240 --> 00:29:25,570
this and so we need another scheduler

00:29:22,450 --> 00:29:31,630
that it's not necessarily BDF one as it

00:29:25,570 --> 00:29:34,870
is so the scattered line it doesn't

00:29:31,630 --> 00:29:37,720
directly applies for this case and we

00:29:34,870 --> 00:29:40,480
need to find another way to schedule

00:29:37,720 --> 00:29:43,120
things on this way one possibility is

00:29:40,480 --> 00:29:45,130
using the early deadline zero laxity

00:29:43,120 --> 00:29:47,980
which is another schedule to easily

00:29:45,130 --> 00:29:51,910
resolve this problem or we can use

00:29:47,980 --> 00:29:53,680
somehow to levels out and moreover ok we

00:29:51,910 --> 00:29:56,500
cannot apply the sky deadline

00:29:53,680 --> 00:29:59,700
we need another scheduler also based in

00:29:56,500 --> 00:30:01,900
deadline but using another dynamic and

00:29:59,700 --> 00:30:04,630
the first thing that comes to our mind

00:30:01,900 --> 00:30:07,060
by proclaiming here is to use what we

00:30:04,630 --> 00:30:11,260
already have on the scattered line which

00:30:07,060 --> 00:30:14,290
is grub but that's not actually it

00:30:11,260 --> 00:30:15,970
doesn't map directly here because ok

00:30:14,290 --> 00:30:20,170
this will take a long time to explain

00:30:15,970 --> 00:30:24,490
and I will skip but believe me this the

00:30:20,170 --> 00:30:27,280
CBI's here would in this case when we

00:30:24,490 --> 00:30:29,830
have let's suppose that we have a

00:30:27,280 --> 00:30:31,270
deadline tasks running and they stop

00:30:29,830 --> 00:30:34,540
running because they are blocking on a

00:30:31,270 --> 00:30:37,450
on a log then the normal tasks would

00:30:34,540 --> 00:30:39,910
start running and they would try to

00:30:37,450 --> 00:30:42,490
steal time from these before use grub

00:30:39,910 --> 00:30:46,030
and so it might end up running more than

00:30:42,490 --> 00:30:48,130
its run time available and so the

00:30:46,030 --> 00:30:50,710
deadline task would reclaim less time

00:30:48,130 --> 00:30:53,860
that it would be possible but this

00:30:50,710 --> 00:30:58,030
requires a longer explanation and we

00:30:53,860 --> 00:31:00,190
escaped because of time but so this cat

00:30:58,030 --> 00:31:04,180
deadline does doesn't directly applies

00:31:00,190 --> 00:31:06,100
and grab doesn't directly applies so the

00:31:04,180 --> 00:31:09,570
baseline of the idea we think it's

00:31:06,100 --> 00:31:13,090
correct to use the servers for the

00:31:09,570 --> 00:31:15,640
schedulers and prioritize the scheduler

00:31:13,090 --> 00:31:17,650
using the deadline but we need to figure

00:31:15,640 --> 00:31:20,410
out another way to schedule to provide

00:31:17,650 --> 00:31:22,390
these guarantees because it will be the

00:31:20,410 --> 00:31:25,450
current code that we have have these

00:31:22,390 --> 00:31:29,430
drawbacks that we don't want to have so

00:31:25,450 --> 00:31:29,430
this requires a lot of more reasoning

00:31:29,790 --> 00:31:43,060
okay I think we are out of budget

00:31:34,120 --> 00:31:45,250
already ok ok ok so another topic is

00:31:43,060 --> 00:31:53,620
improving the scheduled ability of the

00:31:45,250 --> 00:31:56,260
system ok there are some in the current

00:31:53,620 --> 00:31:58,840
scheduler we have here a global

00:31:56,260 --> 00:32:02,050
scheduler in which one scheduler takes

00:31:58,840 --> 00:32:05,680
care of our CPUs or we can separate a

00:32:02,050 --> 00:32:11,470
CPU and let it run alone as partition

00:32:05,680 --> 00:32:14,080
scheduler but both of these cases there

00:32:11,470 --> 00:32:16,750
are some task sets that we could

00:32:14,080 --> 00:32:21,100
correctly fit because they don't use out

00:32:16,750 --> 00:32:23,530
CPUs but with either global or partition

00:32:21,100 --> 00:32:25,930
this schedule is not feasible

00:32:23,530 --> 00:32:28,270
we cannot schedule out tests and such

00:32:25,930 --> 00:32:31,360
way to deliver the response before the

00:32:28,270 --> 00:32:36,040
deadline for example with this task set

00:32:31,360 --> 00:32:38,590
I use some partition I can put this task

00:32:36,040 --> 00:32:42,360
here this task here and try to fit this

00:32:38,590 --> 00:32:47,440
one it does not fit here either here so

00:32:42,360 --> 00:32:50,410
if I put these tasks on the CPU these DS

00:32:47,440 --> 00:32:54,610
will miss the deadlines as well so this

00:32:50,410 --> 00:32:57,550
task set doesn't fit in the in the

00:32:54,610 --> 00:33:00,520
partition behavior and it also doesn't

00:32:57,550 --> 00:33:01,600
fix doesn't fit in the global behavior

00:33:00,520 --> 00:33:05,289
because out

00:33:01,600 --> 00:33:07,150
has the same deadline I and okay

00:33:05,289 --> 00:33:09,400
running this before and this before it's

00:33:07,150 --> 00:33:13,539
a correct behavior I would not have time

00:33:09,400 --> 00:33:17,500
to run this so this task set is not

00:33:13,539 --> 00:33:21,520
scheduled able neither on the global nor

00:33:17,500 --> 00:33:24,250
on the partition so what people in

00:33:21,520 --> 00:33:25,900
academic side are working to fix this

00:33:24,250 --> 00:33:29,230
problem they are thinking on using a

00:33:25,900 --> 00:33:34,659
same partition scheduler and the idea is

00:33:29,230 --> 00:33:36,789
that we can put some tasks in it on CPUs

00:33:34,659 --> 00:33:39,580
and when we don't have more time

00:33:36,789 --> 00:33:44,679
available to pin that we split one task

00:33:39,580 --> 00:33:47,020
on two or more CPUs for a offline system

00:33:44,679 --> 00:33:49,330
which is not our reality because we have

00:33:47,020 --> 00:33:53,110
tasks arriving while the system executes

00:33:49,330 --> 00:33:58,210
is beyond Brandenburg was able to use

00:33:53,110 --> 00:34:00,309
almost 99% of the CPU time so it's he

00:33:58,210 --> 00:34:02,140
can prove you all mostly proof you out

00:34:00,309 --> 00:34:04,690
CPUs with real-time tasks while

00:34:02,140 --> 00:34:07,570
attaining all the deadlines and that's

00:34:04,690 --> 00:34:10,419
awesome but it relies on something that

00:34:07,570 --> 00:34:14,200
we don't have that is knowing all the

00:34:10,419 --> 00:34:16,389
real-time tasks on beforehand our system

00:34:14,200 --> 00:34:18,609
is an online system that can receive new

00:34:16,389 --> 00:34:23,220
real-time tasks as the system evolves so

00:34:18,609 --> 00:34:27,609
this doesn't apply on our kernel but a

00:34:23,220 --> 00:34:29,859
like a relaxed way to do the same thing

00:34:27,609 --> 00:34:34,619
was already developed in the academic

00:34:29,859 --> 00:34:38,470
side by the people in Italy in Pisa and

00:34:34,619 --> 00:34:40,929
they did one version of these they based

00:34:38,470 --> 00:34:43,780
on these but considering that the system

00:34:40,929 --> 00:34:47,440
can have more tasks arriving during the

00:34:43,780 --> 00:34:51,250
execution and so how good is this

00:34:47,440 --> 00:34:55,230
approach that they developed when the

00:34:51,250 --> 00:34:59,050
global EDF is on its worst performance

00:34:55,230 --> 00:35:00,550
there is scheduler like it can runs 50%

00:34:59,050 --> 00:35:03,850
of CPU time while attending the

00:35:00,550 --> 00:35:06,040
deadlines but on this same partition

00:35:03,850 --> 00:35:09,369
scalar that they developed the system

00:35:06,040 --> 00:35:12,660
can run up to 94% of CPU time of all

00:35:09,369 --> 00:35:15,099
CPUs and still

00:35:12,660 --> 00:35:16,930
giving the response time before the

00:35:15,099 --> 00:35:19,510
deadline so we can schedule way more

00:35:16,930 --> 00:35:21,760
tasks using this approach and even when

00:35:19,510 --> 00:35:24,970
the global we D F for example that's

00:35:21,760 --> 00:35:28,180
very good it's still slower like it's

00:35:24,970 --> 00:35:31,329
still 30% lower than the same partition

00:35:28,180 --> 00:35:33,940
approach so we can schedule way more

00:35:31,329 --> 00:35:36,069
tasks using the same partition then

00:35:33,940 --> 00:35:39,190
using global and these other lines are

00:35:36,069 --> 00:35:41,470
just partition which is also possible to

00:35:39,190 --> 00:35:46,349
the deadlines but still they are all

00:35:41,470 --> 00:35:48,579
lower than B then the same partition one

00:35:46,349 --> 00:35:50,710
so for example how does the same

00:35:48,579 --> 00:35:53,890
partition works let's suppose that we

00:35:50,710 --> 00:35:57,849
have that task set that's not possible

00:35:53,890 --> 00:36:00,549
neither on the partition or on global we

00:35:57,849 --> 00:36:03,819
can ping all these two tasks on the CPUs

00:36:00,549 --> 00:36:08,680
and they will always run here and then

00:36:03,819 --> 00:36:11,859
we get this desk here and split it into

00:36:08,680 --> 00:36:14,290
two reservations one reservation will

00:36:11,859 --> 00:36:16,599
have three units of time rather than

00:36:14,290 --> 00:36:18,880
four and it will have a constraint

00:36:16,599 --> 00:36:22,900
deadline of 8 so it will have a higher

00:36:18,880 --> 00:36:26,200
priority than the other test and also

00:36:22,900 --> 00:36:29,559
this task here it has okay here we have

00:36:26,200 --> 00:36:33,130
three of the four but you still need run

00:36:29,559 --> 00:36:36,190
the other unit of time so we put the

00:36:33,130 --> 00:36:39,430
reservation on another CPU with runtime

00:36:36,190 --> 00:36:42,010
of one a constraint deadline of one so

00:36:39,430 --> 00:36:45,839
also it migrates to DCP you it will

00:36:42,010 --> 00:36:49,270
start running and a period of nine and

00:36:45,839 --> 00:36:51,430
in this case for example voila we have

00:36:49,270 --> 00:36:55,089
the scheduled ability of that task set

00:36:51,430 --> 00:36:57,190
that's not possible currently so we are

00:36:55,089 --> 00:36:59,770
able to schedule more tasks using the

00:36:57,190 --> 00:37:05,200
same partition than the global and the

00:36:59,770 --> 00:37:07,329
partition one so the good point of this

00:37:05,200 --> 00:37:09,940
is that the vast majority of the

00:37:07,329 --> 00:37:12,730
real-time problems are reduced to single

00:37:09,940 --> 00:37:16,530
core and the single core is scheduling

00:37:12,730 --> 00:37:19,540
is way easier to do then move to pokers

00:37:16,530 --> 00:37:22,630
we have least overhead less overhead

00:37:19,540 --> 00:37:25,059
because when we are running global we

00:37:22,630 --> 00:37:25,660
are always trying to decide where to put

00:37:25,059 --> 00:37:28,270
that

00:37:25,660 --> 00:37:32,070
and this decision takes time like we are

00:37:28,270 --> 00:37:34,990
pushing and pulling tasks every time and

00:37:32,070 --> 00:37:38,470
we don't need to decide where to put the

00:37:34,990 --> 00:37:40,330
tasks why the tasks are executing five

00:37:38,470 --> 00:37:44,200
minutes okay we can do it

00:37:40,330 --> 00:37:46,450
so we don't need to decide where to put

00:37:44,200 --> 00:37:48,910
the tasks because we already decided on

00:37:46,450 --> 00:37:50,770
beforehand when the tasks arrived so we

00:37:48,910 --> 00:37:52,810
decide to put this part on the CPU and

00:37:50,770 --> 00:37:54,790
part on that CPU so we don't need to

00:37:52,810 --> 00:37:58,840
take this decision in run time and this

00:37:54,790 --> 00:38:01,480
this means low overhead there is no need

00:37:58,840 --> 00:38:03,520
to pull tasks just push tasks and this

00:38:01,480 --> 00:38:05,350
reduces a lot the time doing this

00:38:03,520 --> 00:38:08,020
operation and this operation takes the

00:38:05,350 --> 00:38:10,300
bronchiole log and potentially can cause

00:38:08,020 --> 00:38:13,690
latency so we have the side effect of

00:38:10,300 --> 00:38:17,710
not not taking the logs of the schedule

00:38:13,690 --> 00:38:21,630
too long reducing the overhead of the

00:38:17,710 --> 00:38:24,640
poo and taking the scheduling spin locks

00:38:21,630 --> 00:38:26,800
for shorter time and these helps to

00:38:24,640 --> 00:38:30,130
reduce latency which is a good thing may

00:38:26,800 --> 00:38:34,300
mainly for things for systems with a lot

00:38:30,130 --> 00:38:36,369
of CPUs migrations are bounded but I

00:38:34,300 --> 00:38:40,359
will required to have more time to

00:38:36,369 --> 00:38:43,000
explain these but at least we hi we are

00:38:40,359 --> 00:38:45,850
with a very high load of tasks the

00:38:43,000 --> 00:38:48,369
system will mostly not migrate tasks all

00:38:45,850 --> 00:38:51,820
the tests will be paying to CPUs we will

00:38:48,369 --> 00:38:55,500
only start to migrate in tasks as we go

00:38:51,820 --> 00:38:59,320
operand digitalization so we have less

00:38:55,500 --> 00:39:02,830
migrations because tests are mostly

00:38:59,320 --> 00:39:05,470
Panetta CPU and we were able to use

00:39:02,830 --> 00:39:08,470
affinities affinities come for free we

00:39:05,470 --> 00:39:10,119
can use affinities for tasks and this is

00:39:08,470 --> 00:39:17,200
something that we don't have a global

00:39:10,119 --> 00:39:19,750
scheduler the point is that currently we

00:39:17,200 --> 00:39:25,540
have an admission control for the

00:39:19,750 --> 00:39:28,840
deadline tasks but it is sufficient but

00:39:25,540 --> 00:39:31,170
not necessary in the sense that we may

00:39:28,840 --> 00:39:35,490
accept a task set that's not actually

00:39:31,170 --> 00:39:37,410
scalable but this

00:39:35,490 --> 00:39:40,529
the amount of time that we require to do

00:39:37,410 --> 00:39:43,200
these emission tests is it's very slow a

00:39:40,529 --> 00:39:45,630
very low but in this case we will need

00:39:43,200 --> 00:39:47,789
to use the admission control that is the

00:39:45,630 --> 00:39:49,920
way that we split test on CPUs we need

00:39:47,789 --> 00:39:52,470
to put this in the kernel and it needs

00:39:49,920 --> 00:39:55,950
to run in the kernel and but it's

00:39:52,470 --> 00:39:59,190
necessary and sufficient one so it will

00:39:55,950 --> 00:40:01,140
turn the admission control complex but

00:39:59,190 --> 00:40:03,660
this complexity takes time just in very

00:40:01,140 --> 00:40:06,359
few points like when receiving a new

00:40:03,660 --> 00:40:10,170
task or rains resetting the affinity of

00:40:06,359 --> 00:40:11,670
tasks or in hot bogging CPUs but still

00:40:10,170 --> 00:40:18,150
we need to bring these to the kernel and

00:40:11,670 --> 00:40:19,710
it's a little bit complex and ok we in

00:40:18,150 --> 00:40:21,359
the current scalar in the same partition

00:40:19,710 --> 00:40:23,670
scalar we might have problems with

00:40:21,359 --> 00:40:29,670
constrain deadlines and that's something

00:40:23,670 --> 00:40:32,720
that we need to think more ok we are

00:40:29,670 --> 00:40:37,950
running out of time so I always keep the

00:40:32,720 --> 00:40:42,529
better tracing support and that's that's

00:40:37,950 --> 00:40:42,529
all questions

00:40:47,480 --> 00:40:52,080
and I'm gland I'm glad I skip the trace

00:40:50,610 --> 00:40:57,140
of support because bitter history is

00:40:52,080 --> 00:40:57,140
here and I want to be alive this after

00:41:06,790 --> 00:41:10,940

YouTube URL: https://www.youtube.com/watch?v=cx6WfmZjkCI


