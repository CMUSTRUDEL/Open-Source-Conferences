Title: LPC2018 - oomd: a userspace OOM killer
Publication date: 2018-12-04
Playlist: LPC2018 - LPC Main Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/58/
speaker:  Daniel Xu (Facebook)


Running out of memory on a host is a particularly nasty scenario. In the Linux kernel, if memory is being overcommitted, it results in the kernel out-of-memory (OOM) killer kicking in. In this talk, Daniel Xu will cover why the Linux kernel OOM killer is surprisingly ineffective and how oomd, a newly opensourced userspace OOM killer, does a more effective and reliable job. Not only does the switch from kernel space to userspace result a more flexible solution, but it also directly translates to better resource utilization. His talk will also do a deep dive into the Linux kernel changes and improvements necessary for oomd to operate.
Captions: 
	00:00:05,600 --> 00:00:09,470
all right uh-huh everyone my name is

00:00:07,670 --> 00:00:11,780
Daniel I work at Facebook as a

00:00:09,470 --> 00:00:13,580
production engineer I work on a team

00:00:11,780 --> 00:00:15,049
called kernel applications and the

00:00:13,580 --> 00:00:17,450
trailer of our team is pretty much to

00:00:15,049 --> 00:00:19,100
make the Linux kernel more usable and

00:00:17,450 --> 00:00:21,970
that typically means sitting on user

00:00:19,100 --> 00:00:24,679
space side but using pretty new features

00:00:21,970 --> 00:00:29,089
so this talk is about MD which stands

00:00:24,679 --> 00:00:31,640
for out of memory demon and Cyndi's user

00:00:29,089 --> 00:00:33,860
space its run time dependency free so

00:00:31,640 --> 00:00:37,280
you don't need stuff like a system do

00:00:33,860 --> 00:00:39,260
anything to be running I claim that it's

00:00:37,280 --> 00:00:40,640
deterministic faster and more flexible

00:00:39,260 --> 00:00:42,969
than the kernel loom killer and I'll

00:00:40,640 --> 00:00:46,370
talk more about that in the later slides

00:00:42,969 --> 00:00:48,350
boondis also open sourced I put it under

00:00:46,370 --> 00:00:50,930
GPL - and there's the github link there

00:00:48,350 --> 00:00:52,640
there's also sent augmentation that a

00:00:50,930 --> 00:00:56,930
guy named Thomas sir you should check

00:00:52,640 --> 00:00:58,789
that out if you're interested so the

00:00:56,930 --> 00:01:00,949
agenda for this talk is I'm gonna go

00:00:58,789 --> 00:01:04,129
over the motivation the mechanisms and

00:01:00,949 --> 00:01:05,000
then the results behind doom D and then

00:01:04,129 --> 00:01:06,410
at least some time at the end for

00:01:05,000 --> 00:01:08,060
questions or discussion if anyone's

00:01:06,410 --> 00:01:09,860
interested

00:01:08,060 --> 00:01:11,479
this talk is gonna be a little bit short

00:01:09,860 --> 00:01:15,320
so it's short and sweet so you're gonna

00:01:11,479 --> 00:01:19,159
get some of your day back so motivation

00:01:15,320 --> 00:01:20,720
so why create D so I think it's what we

00:01:19,159 --> 00:01:22,729
back up and talk about out of memory

00:01:20,720 --> 00:01:23,960
conditions I want to sue most of you in

00:01:22,729 --> 00:01:26,240
this room know what memory overcommit is

00:01:23,960 --> 00:01:28,579
so I'm not gonna really go into that the

00:01:26,240 --> 00:01:31,610
short version is malloc really usually

00:01:28,579 --> 00:01:33,380
doesn't return null pointer but

00:01:31,610 --> 00:01:35,630
sometimes when you try to access the

00:01:33,380 --> 00:01:37,670
memory your machine's gonna because you

00:01:35,630 --> 00:01:38,689
have no more physical memory left and so

00:01:37,670 --> 00:01:41,329
what happens after that

00:01:38,689 --> 00:01:43,040
well configuring what happens during a

00:01:41,329 --> 00:01:45,020
new is actually somewhat complicated and

00:01:43,040 --> 00:01:47,140
not very intuitive there's a bunch of

00:01:45,020 --> 00:01:49,250
these knobs I listed there under proc

00:01:47,140 --> 00:01:50,990
there's like lose quorum score just

00:01:49,250 --> 00:01:53,540
whatever and some of these numbers go

00:01:50,990 --> 00:01:54,860
from like negative 16 to positive 15 or

00:01:53,540 --> 00:01:57,920
something and then some of them start at

00:01:54,860 --> 00:02:00,020
zero and go to positive 1,000 if you sit

00:01:57,920 --> 00:02:01,789
down and actually read em killed out see

00:02:00,020 --> 00:02:03,049
the numbers actually make sort of make

00:02:01,789 --> 00:02:04,700
sense and I would understand why they're

00:02:03,049 --> 00:02:06,680
there but I think from a user

00:02:04,700 --> 00:02:09,050
perspective like user facing perspective

00:02:06,680 --> 00:02:11,710
it's not exactly it's not the most easy

00:02:09,050 --> 00:02:13,580
thing to use at least in my opinion

00:02:11,710 --> 00:02:15,290
another problem with the kernel loom

00:02:13,580 --> 00:02:17,330
killer is that the kernel in killer is

00:02:15,290 --> 00:02:18,650
actually pretty slow to act so by the

00:02:17,330 --> 00:02:19,130
time it kicks in it's already usually

00:02:18,650 --> 00:02:20,600
too

00:02:19,130 --> 00:02:22,220
for the workload the workload could have

00:02:20,600 --> 00:02:24,050
been livelock - just locked up or

00:02:22,220 --> 00:02:25,940
whatever just stalled for any period of

00:02:24,050 --> 00:02:27,710
time and the reason behind that is

00:02:25,940 --> 00:02:29,060
because the Colonel loom killer tries to

00:02:27,710 --> 00:02:31,850
protect the colonel health it doesn't

00:02:29,060 --> 00:02:33,380
really concern itself for userspace too

00:02:31,850 --> 00:02:36,590
much so for example if the colonel

00:02:33,380 --> 00:02:38,240
thinks the system is making progress be

00:02:36,590 --> 00:02:39,770
it just training pages in and out of the

00:02:38,240 --> 00:02:41,150
page cache then it's gonna keep just

00:02:39,770 --> 00:02:43,100
it's gonna think like alright sure

00:02:41,150 --> 00:02:44,540
that's fine we're gonna keep going

00:02:43,100 --> 00:02:46,040
but userspace could have been live

00:02:44,540 --> 00:02:47,510
locked for like up to minutes already

00:02:46,040 --> 00:02:49,250
and we see that a lot of the times like

00:02:47,510 --> 00:02:50,660
workload usually stops responding to

00:02:49,250 --> 00:02:52,550
heartbeats for a good period of time and

00:02:50,660 --> 00:02:53,660
then the colonel um Kelly kicks in and

00:02:52,550 --> 00:02:58,640
then kind of screws with the rest of the

00:02:53,660 --> 00:03:01,070
system the colonel mq doesn't really

00:02:58,640 --> 00:03:03,230
have any good context on the logical

00:03:01,070 --> 00:03:04,640
composition of a system either so maybe

00:03:03,230 --> 00:03:06,650
there's two processes that should never

00:03:04,640 --> 00:03:07,910
be killed together maybe there's two of

00:03:06,650 --> 00:03:09,050
them or only one of them should ever be

00:03:07,910 --> 00:03:10,700
killed and the other should be alive or

00:03:09,050 --> 00:03:13,580
something like that or you can also

00:03:10,700 --> 00:03:14,930
imagine more complicated scenarios and

00:03:13,580 --> 00:03:16,760
there's no great way to customize ID

00:03:14,930 --> 00:03:18,440
there so there's an event FD you can use

00:03:16,760 --> 00:03:22,670
but it's still slow for the reasons I

00:03:18,440 --> 00:03:24,530
mentioned in the previous slide as far

00:03:22,670 --> 00:03:26,510
as customisation goes for some processes

00:03:24,530 --> 00:03:28,220
of sick ill or sick time is just fine

00:03:26,510 --> 00:03:30,770
but I felt it was you might want a song

00:03:28,220 --> 00:03:33,920
and dance and so the example I always

00:03:30,770 --> 00:03:36,110
use is a container management daemon so

00:03:33,920 --> 00:03:37,520
for example dr. D or something so if the

00:03:36,110 --> 00:03:38,780
system runs outta memory you don't want

00:03:37,520 --> 00:03:40,100
to kill the management daemon you

00:03:38,780 --> 00:03:42,140
actually want to kill the workloads that

00:03:40,100 --> 00:03:44,030
are running underneath it because if you

00:03:42,140 --> 00:03:46,130
kill the management team and then these

00:03:44,030 --> 00:03:47,690
workloads are gonna start doing crazy

00:03:46,130 --> 00:03:49,130
things potentially and just not be

00:03:47,690 --> 00:03:49,820
managed and that's kind of a bad

00:03:49,130 --> 00:03:52,040
situation

00:03:49,820 --> 00:03:55,370
and it's somewhat hard to express using

00:03:52,040 --> 00:03:56,660
the proc knobs that I showed before it's

00:03:55,370 --> 00:03:57,500
also kind of non-deterministic or at

00:03:56,660 --> 00:03:59,090
least it's really hard to get

00:03:57,500 --> 00:04:01,280
deterministic so you can envision

00:03:59,090 --> 00:04:02,900
multiple workloads or more than a work

00:04:01,280 --> 00:04:04,730
little more than one process and it's a

00:04:02,900 --> 00:04:06,500
if you start out new processes it's kind

00:04:04,730 --> 00:04:10,300
of hard to turn the knobs in time to get

00:04:06,500 --> 00:04:10,300
a just right to do exactly what you want

00:04:11,860 --> 00:04:16,720
so Facebook actually suffers from a good

00:04:14,260 --> 00:04:19,120
deal of out of memory issues so I listed

00:04:16,720 --> 00:04:20,680
a couple of the biggest offenders here

00:04:19,120 --> 00:04:22,960
and I'll go through them kind of briefly

00:04:20,680 --> 00:04:23,710
so the first is the continuous build and

00:04:22,960 --> 00:04:25,900
test platform

00:04:23,710 --> 00:04:27,790
it's called sand castle so pretty much

00:04:25,900 --> 00:04:30,910
any time a Facebook Developer checks in

00:04:27,790 --> 00:04:33,220
some code to the to be reviewed a build

00:04:30,910 --> 00:04:34,690
is fired off so it builds the program or

00:04:33,220 --> 00:04:37,090
whatever and then it runs the continuous

00:04:34,690 --> 00:04:38,380
tests or something like that building

00:04:37,090 --> 00:04:41,530
and linking tends to take a lot of

00:04:38,380 --> 00:04:43,480
memory so yeah it takes a lot of memory

00:04:41,530 --> 00:04:44,680
to do that and we also tend to build it

00:04:43,480 --> 00:04:45,730
in memory because if you built a disk

00:04:44,680 --> 00:04:46,710
you're gonna wear out the disk really

00:04:45,730 --> 00:04:48,670
really fast

00:04:46,710 --> 00:04:50,290
these build and test jobs are also

00:04:48,670 --> 00:04:51,850
usually stacked on a single box so you

00:04:50,290 --> 00:04:53,620
typically have between three and five on

00:04:51,850 --> 00:04:55,840
a single box or whatever number it is

00:04:53,620 --> 00:04:57,670
these days so it's pretty easy to

00:04:55,840 --> 00:04:59,460
actually do these boxes so MD comes in

00:04:57,670 --> 00:05:01,510
and can actually help a lot with this

00:04:59,460 --> 00:05:02,980
the other as I mentioned before is our

00:05:01,510 --> 00:05:05,020
container and service platform which is

00:05:02,980 --> 00:05:06,850
called Tupperware developers Facebook

00:05:05,020 --> 00:05:09,130
can run services and jobs and whatever

00:05:06,850 --> 00:05:10,750
they want in this platform and these

00:05:09,130 --> 00:05:13,300
jobs are typically co-located but the

00:05:10,750 --> 00:05:15,280
other jobs and most the time this is

00:05:13,300 --> 00:05:16,540
fine but for some cases you can run out

00:05:15,280 --> 00:05:18,130
of memories in the box for example when

00:05:16,540 --> 00:05:19,900
you check in a memory leak or memory

00:05:18,130 --> 00:05:21,340
bomb or just workloads that are

00:05:19,900 --> 00:05:23,560
unfortunately stacked on another one

00:05:21,340 --> 00:05:26,200
that take up too much memory on so

00:05:23,560 --> 00:05:27,850
that's pretty bad we also have an

00:05:26,200 --> 00:05:29,770
interesting place we run out of memory a

00:05:27,850 --> 00:05:31,090
lot which is Thapa racks which is so

00:05:29,770 --> 00:05:33,190
Facebook we have commodity top rack

00:05:31,090 --> 00:05:35,440
switches and it's called F boss and it

00:05:33,190 --> 00:05:36,790
runs follow us on top of that and this

00:05:35,440 --> 00:05:38,080
is actually pretty resource constrained

00:05:36,790 --> 00:05:39,940
environment because this is because it's

00:05:38,080 --> 00:05:42,610
like so c'mon do whatever it's only as

00:05:39,940 --> 00:05:44,080
four gigs of ram so it's not a lot so if

00:05:42,610 --> 00:05:45,550
you're serving a lot of traffic and then

00:05:44,080 --> 00:05:46,660
somehow you fetching a pageant gauge at

00:05:45,550 --> 00:05:47,770
the same time it can actually zoom in

00:05:46,660 --> 00:05:50,020
the box which is really unfortunate

00:05:47,770 --> 00:05:51,730
because take care of racks which is like

00:05:50,020 --> 00:05:53,410
you're gonna lose the entire rack and

00:05:51,730 --> 00:05:56,380
potentially kill a lot of things that

00:05:53,410 --> 00:05:58,030
depend on the network in general the

00:05:56,380 --> 00:06:00,630
theme is any multi-tenant platform where

00:05:58,030 --> 00:06:02,770
you can have custom code running is a

00:06:00,630 --> 00:06:06,160
potentially subject to out of memory

00:06:02,770 --> 00:06:07,570
issues so a lot of these tea runners

00:06:06,160 --> 00:06:10,240
they actually tend to turn on panic on

00:06:07,570 --> 00:06:12,220
whom because because it's like so hard

00:06:10,240 --> 00:06:14,140
to get the configuration right rather

00:06:12,220 --> 00:06:15,640
than let a system limp long and it's a

00:06:14,140 --> 00:06:17,110
semi degraded state where you don't know

00:06:15,640 --> 00:06:19,000
what's alive and what's not it's better

00:06:17,110 --> 00:06:20,140
to actually have the Box reboot because

00:06:19,000 --> 00:06:23,240
when it comes up you actually know

00:06:20,140 --> 00:06:25,190
everything you want to run is running

00:06:23,240 --> 00:06:27,500
it's not optimal it's an engineering

00:06:25,190 --> 00:06:29,420
trade-off right because now you have all

00:06:27,500 --> 00:06:31,040
these boxes that take like 15 30 minutes

00:06:29,420 --> 00:06:33,230
to reboot in the data center there's not

00:06:31,040 --> 00:06:36,250
doing anything besides rebooting and so

00:06:33,230 --> 00:06:36,250
you're losing a lot of resources there

00:06:37,389 --> 00:06:43,040
mundi also put is a part of a Fe tax to

00:06:40,669 --> 00:06:44,510
and to back a little bit the FB tax is

00:06:43,040 --> 00:06:46,250
the tax that every server at Facebook

00:06:44,510 --> 00:06:47,960
pays just to exist you got to run these

00:06:46,250 --> 00:06:50,240
like logging and health check and

00:06:47,960 --> 00:06:52,040
whatever demons and so the tax you pay

00:06:50,240 --> 00:06:55,250
is like whatever resources these widely

00:06:52,040 --> 00:06:56,450
distributed binaries need and the idea

00:06:55,250 --> 00:06:57,950
behind FP tax to is you want to isolate

00:06:56,450 --> 00:06:59,890
the workload from this kind of

00:06:57,950 --> 00:07:02,120
interference so if someone checks in

00:06:59,890 --> 00:07:03,530
some code that leaks memory in these

00:07:02,120 --> 00:07:05,510
auxilary applications you don't want

00:07:03,530 --> 00:07:07,840
your workload to suffer as a result and

00:07:05,510 --> 00:07:10,490
so this is taejun's thing at Facebook

00:07:07,840 --> 00:07:12,110
currently and joseph has also helped out

00:07:10,490 --> 00:07:14,630
neither of them are here to this week

00:07:12,110 --> 00:07:15,740
but uh the talk is recorded online in

00:07:14,630 --> 00:07:17,090
other conferences and you check it out

00:07:15,740 --> 00:07:19,190
if you're interested there's a bunch of

00:07:17,090 --> 00:07:24,380
links to all the relevant stuff to some

00:07:19,190 --> 00:07:26,030
of the relevant stuff to be taxed too so

00:07:24,380 --> 00:07:28,910
mechanism so how does Wendy actually

00:07:26,030 --> 00:07:30,800
work so the main thing amis is is psi

00:07:28,910 --> 00:07:32,479
which is a stands for pressure stall

00:07:30,800 --> 00:07:34,460
information which is a relatively new

00:07:32,479 --> 00:07:35,990
kernel feature done by Facebook's

00:07:34,460 --> 00:07:39,140
yohannes there was a talk earlier today

00:07:35,990 --> 00:07:40,550
about using kia android i was pretty

00:07:39,140 --> 00:07:42,260
cool i was actually talking to you

00:07:40,550 --> 00:07:44,510
Hannes earlier saying I needed an event

00:07:42,260 --> 00:07:46,310
or a notification interface for a psi

00:07:44,510 --> 00:07:48,830
which is it's great to see that work is

00:07:46,310 --> 00:07:52,820
being done because it's better than

00:07:48,830 --> 00:07:54,320
palling so the core of md is also the

00:07:52,820 --> 00:07:57,470
plug-in system so all detection and

00:07:54,320 --> 00:07:59,330
action behaviors are customizable this

00:07:57,470 --> 00:08:02,060
is typically done via writing some C++

00:07:59,330 --> 00:08:03,169
code or configuring stuff using a JSON

00:08:02,060 --> 00:08:05,750
config file that I'll show a little

00:08:03,169 --> 00:08:06,680
later the end goal is say no one has to

00:08:05,750 --> 00:08:08,870
write any code all you have to do is

00:08:06,680 --> 00:08:11,350
write some configuration stuff very

00:08:08,870 --> 00:08:13,490
lightweight configuration stuff I

00:08:11,350 --> 00:08:14,870
provide a default in detecting and

00:08:13,490 --> 00:08:16,550
killer and it's actually pretty sensible

00:08:14,870 --> 00:08:18,380
and it works pretty well across a

00:08:16,550 --> 00:08:20,000
variety of workloads we tend to be able

00:08:18,380 --> 00:08:25,280
to run MD in the stock configuration

00:08:20,000 --> 00:08:27,650
across some platforms MD doesn't

00:08:25,280 --> 00:08:29,210
actually it doesn't only monitor memory

00:08:27,650 --> 00:08:31,039
pressure but we also get the i/o

00:08:29,210 --> 00:08:32,590
pressure for free because of Psi Psi

00:08:31,039 --> 00:08:35,080
supports monitoring

00:08:32,590 --> 00:08:36,640
pressure we also monitor slop depletion

00:08:35,080 --> 00:08:38,050
because swap when a system runs out of

00:08:36,640 --> 00:08:41,110
swap weird things can happen for example

00:08:38,050 --> 00:08:43,720
memory down low semantics in the context

00:08:41,110 --> 00:08:45,160
of c group c group - you can't really

00:08:43,720 --> 00:08:46,990
guarantee that from the kernel so when

00:08:45,160 --> 00:08:48,580
you run out of swap path law pretty

00:08:46,990 --> 00:08:52,120
pathological things can't happen so we

00:08:48,580 --> 00:08:53,530
monitor that in general MD exists to

00:08:52,120 --> 00:08:55,450
remediate things when kernel resource

00:08:53,530 --> 00:09:00,670
isolation isn't enough which actually

00:08:55,450 --> 00:09:02,710
happens to be a pretty big problem so

00:09:00,670 --> 00:09:04,600
this is the original MD config it's a

00:09:02,710 --> 00:09:06,970
JSON blob and this is pretty much with a

00:09:04,600 --> 00:09:07,990
lot of tears run with in production so

00:09:06,970 --> 00:09:09,730
what it says here is your monitoring

00:09:07,990 --> 00:09:12,700
system that slice and the dot slice

00:09:09,730 --> 00:09:14,530
thing is just the system deism it could

00:09:12,700 --> 00:09:15,910
be anything really and then you have a

00:09:14,530 --> 00:09:18,460
kill list and so what this is saying is

00:09:15,910 --> 00:09:19,960
if machine runs out of memory if and if

00:09:18,460 --> 00:09:23,080
chef is using more than a gig of memory

00:09:19,960 --> 00:09:24,970
please kill a shot first and then it

00:09:23,080 --> 00:09:26,020
also says sshd is being blacklisted

00:09:24,970 --> 00:09:28,930
because you don't really want to lose

00:09:26,020 --> 00:09:30,160
SSH access to box and then it says using

00:09:28,930 --> 00:09:34,960
the default zoom killer and default

00:09:30,160 --> 00:09:36,730
detector pretty straightforward stuff so

00:09:34,960 --> 00:09:37,720
MD as exists like as the config I just

00:09:36,730 --> 00:09:39,850
showed it works pretty well

00:09:37,720 --> 00:09:41,920
it runs on a lot of hosts of Facebook

00:09:39,850 --> 00:09:43,390
and it does a lot of good things but as

00:09:41,920 --> 00:09:45,790
we've onboarding more users and found

00:09:43,390 --> 00:09:47,680
more use cases and just added more

00:09:45,790 --> 00:09:49,420
features in general it's become apparent

00:09:47,680 --> 00:09:51,130
you need to you need to kind of a

00:09:49,420 --> 00:09:53,560
redesign because you know software

00:09:51,130 --> 00:09:54,850
development goes after you slap on

00:09:53,560 --> 00:09:56,800
enough features you realize you gotta

00:09:54,850 --> 00:09:58,060
like redesign things a little bit

00:09:56,800 --> 00:10:01,270
otherwise it's gonna collapse under its

00:09:58,060 --> 00:10:02,530
own weight I'm still iterating pretty

00:10:01,270 --> 00:10:04,510
quickly and playing around the details

00:10:02,530 --> 00:10:06,040
we've essentially established that this

00:10:04,510 --> 00:10:07,960
is a useful problem to be solved here

00:10:06,040 --> 00:10:10,540
right now it's mostly just an interface

00:10:07,960 --> 00:10:14,290
problem in general around something yep

00:10:10,540 --> 00:10:15,880
pretty useful internally we've dubbed it

00:10:14,290 --> 00:10:17,890
MD 2 which is a much more flexible

00:10:15,880 --> 00:10:20,230
version of MD as we come across these

00:10:17,890 --> 00:10:22,780
new requirements I put up a giant patch

00:10:20,230 --> 00:10:24,130
on github so like people know that it's

00:10:22,780 --> 00:10:25,380
happening and I'm just not gonna blow

00:10:24,130 --> 00:10:27,310
over the code after they've studied it

00:10:25,380 --> 00:10:29,290
I'm not sure if anyone's actually

00:10:27,310 --> 00:10:33,430
studying the code but it's there if

00:10:29,290 --> 00:10:36,220
anyone's curious so this is the MD 2

00:10:33,430 --> 00:10:38,350
config it looks pretty similar it's

00:10:36,220 --> 00:10:40,510
somewhat different it's uh this is still

00:10:38,350 --> 00:10:41,980
JSON but a interesting side note from

00:10:40,510 --> 00:10:44,020
the plantation I created an intermediate

00:10:41,980 --> 00:10:45,430
representation so it would take really

00:10:44,020 --> 00:10:46,810
like a couple hours to write

00:10:45,430 --> 00:10:48,520
frontend two parts to config and then

00:10:46,810 --> 00:10:50,050
generate the intermediate representation

00:10:48,520 --> 00:10:52,660
which can be compiled back into the data

00:10:50,050 --> 00:10:54,820
structures in theory could have like a

00:10:52,660 --> 00:10:56,920
very compact IP tables like syntax to do

00:10:54,820 --> 00:10:59,100
this kind of stuff which it's kind of

00:10:56,920 --> 00:11:01,210
what I designed this configure and

00:10:59,100 --> 00:11:03,550
what's circled in yellow is pseudocode

00:11:01,210 --> 00:11:05,649
so it's not actually the stuff you type

00:11:03,550 --> 00:11:07,240
in I'm gonna skip forward you can see

00:11:05,649 --> 00:11:08,410
like this is actually expands to I'm not

00:11:07,240 --> 00:11:09,550
gonna leave it out there's really no

00:11:08,410 --> 00:11:11,140
point in squinting at it because you're

00:11:09,550 --> 00:11:13,750
gonna realize it says the same thing as

00:11:11,140 --> 00:11:15,610
this so what this is saying is if your

00:11:13,750 --> 00:11:18,040
workload slows down by more than 5% due

00:11:15,610 --> 00:11:20,070
to resource shortages or if the auxilary

00:11:18,040 --> 00:11:22,540
service is slowed down by more than 40%

00:11:20,070 --> 00:11:26,529
please kill a memory hog in the

00:11:22,540 --> 00:11:28,600
auxiliary services C group and so this

00:11:26,529 --> 00:11:29,740
is a very basic example you tend to want

00:11:28,600 --> 00:11:32,490
to chain these together in orthogonal

00:11:29,740 --> 00:11:36,550
fashions to create like a very robust

00:11:32,490 --> 00:11:37,720
detection framework yeah I worked sort

00:11:36,550 --> 00:11:42,520
of like iptables chained a bunch of

00:11:37,720 --> 00:11:44,440
rules and actions together so

00:11:42,520 --> 00:11:45,670
development status so yeah I mentioned

00:11:44,440 --> 00:11:47,529
earlier still iterating pretty quickly

00:11:45,670 --> 00:11:49,450
if anyone wants to see any use cases

00:11:47,529 --> 00:11:50,830
supported yet please just send me an

00:11:49,450 --> 00:11:53,050
email or something I'm happy to

00:11:50,830 --> 00:11:55,089
accommodate that configurator face

00:11:53,050 --> 00:11:57,250
really isn't considered too stable yet I

00:11:55,089 --> 00:11:58,600
still working on it but it mostly have

00:11:57,250 --> 00:12:01,750
the idea down I have like some sort of

00:11:58,600 --> 00:12:02,740
like grab be be enough thing I wrote if

00:12:01,750 --> 00:12:04,839
you have any interest in using it please

00:12:02,740 --> 00:12:06,459
send me an issue or send me an email or

00:12:04,839 --> 00:12:09,490
anything like that I'm happy to chat

00:12:06,459 --> 00:12:12,220
about whatever so results so how well

00:12:09,490 --> 00:12:14,320
does India actually work so this is a

00:12:12,220 --> 00:12:16,660
graph of memory usage over time on a

00:12:14,320 --> 00:12:19,120
build host if you notice there's no

00:12:16,660 --> 00:12:20,560
y-axis and that's intentional the lawyer

00:12:19,120 --> 00:12:22,390
said I couldn't have any numbers on the

00:12:20,560 --> 00:12:24,880
y-axis the reasons that'll be more

00:12:22,390 --> 00:12:27,070
apparent the next slide but I just had

00:12:24,880 --> 00:12:28,600
to take it out anyways but you notice

00:12:27,070 --> 00:12:30,459
there like around 11:00 a.m. there's a

00:12:28,600 --> 00:12:33,070
sharp dip in memory usage after slowly

00:12:30,459 --> 00:12:35,410
growing over time and that is because

00:12:33,070 --> 00:12:37,690
you know he made a kill and kind of save

00:12:35,410 --> 00:12:39,160
the box for moving you imagine an 11:00

00:12:37,690 --> 00:12:43,450
a.m. and some sort of build job got

00:12:39,160 --> 00:12:45,790
kicked off so this graph is the panic on

00:12:43,450 --> 00:12:46,990
rate before and before an afternoon do

00:12:45,790 --> 00:12:49,390
you roll out so this is just for one

00:12:46,990 --> 00:12:51,370
region for one tear so you can see there

00:12:49,390 --> 00:12:53,260
there's a very sharp dip in the panic

00:12:51,370 --> 00:12:54,250
enumerate and that's when the roll that

00:12:53,260 --> 00:12:57,010
happens so you can see it's actually

00:12:54,250 --> 00:12:59,390
pretty immediately effective what kind

00:12:57,010 --> 00:13:00,920
of panic renews that can prevent

00:12:59,390 --> 00:13:03,170
if you also notice there's it doesn't

00:13:00,920 --> 00:13:04,670
drop to zero because um we still need a

00:13:03,170 --> 00:13:07,040
tune in d a little bit more it doesn't

00:13:04,670 --> 00:13:09,260
work 100 it's not wonderful foolproof

00:13:07,040 --> 00:13:13,760
but it's a very nice benefit if you run

00:13:09,260 --> 00:13:18,010
it on your servers yeah that was it

00:13:13,760 --> 00:13:18,010
there's any questions now's a good time

00:13:20,050 --> 00:13:26,740
and do you do anything extra to make

00:13:24,080 --> 00:13:30,290
sure that only is responsive when the

00:13:26,740 --> 00:13:31,760
system is running out of memory yeah so

00:13:30,290 --> 00:13:33,020
we run it under a slice called host

00:13:31,760 --> 00:13:43,190
critical that slice which is kind of

00:13:33,020 --> 00:13:45,020
disturbing so the question is are you

00:13:43,190 --> 00:13:47,330
doing anything specific to make sure

00:13:45,020 --> 00:13:50,000
that um these responsive when machine is

00:13:47,330 --> 00:13:52,250
running out of memory yeah so we run em

00:13:50,000 --> 00:13:54,860
D under AC group to slice called host

00:13:52,250 --> 00:13:57,230
critical and guarantees 64 megabytes of

00:13:54,860 --> 00:13:58,820
memory dumb in which kind of essentially

00:13:57,230 --> 00:14:00,740
it's kind of just unlocks it into memory

00:13:58,820 --> 00:14:03,730
although in the future we could look at

00:14:00,740 --> 00:14:03,730
actually I'm locking it

00:14:10,030 --> 00:14:23,130
okay how do you guarantee the 64 max how

00:14:17,530 --> 00:14:23,130
do you guarantee 64 megabytes for Aldi

00:14:25,890 --> 00:14:33,100
okay how do you guarantee 64 megabytes

00:14:29,350 --> 00:14:34,540
for Omni how do I guarantee that uh I

00:14:33,100 --> 00:14:35,980
don't know there's not really any hard

00:14:34,540 --> 00:14:37,900
guarantees I've just observed it using

00:14:35,980 --> 00:14:39,700
less than 30 in general and I'll bump it

00:14:37,900 --> 00:14:41,800
up if I need more but it's I wrote it in

00:14:39,700 --> 00:14:44,050
C++ and I just somewhat careful with

00:14:41,800 --> 00:14:49,750
memory so it doesn't use too much to

00:14:44,050 --> 00:14:53,980
process text okay and another question

00:14:49,750 --> 00:14:57,760
what's the improvement in Omni -

00:14:53,980 --> 00:14:58,750
compared to home do you want so it was a

00:14:57,760 --> 00:14:59,740
question what's the difference between

00:14:58,750 --> 00:15:01,750
it yeah

00:14:59,740 --> 00:15:03,400
the main difference is it supports more

00:15:01,750 --> 00:15:05,380
use cases so for example stuff like

00:15:03,400 --> 00:15:08,200
cross Seagram on doing right now whom do

00:15:05,380 --> 00:15:09,610
you one can only monitor 1c group for

00:15:08,200 --> 00:15:11,590
this you can monitor an arbitrary number

00:15:09,610 --> 00:15:13,630
plus the design of whom do you want is

00:15:11,590 --> 00:15:14,830
somewhat monolithic is just like if you

00:15:13,630 --> 00:15:17,020
want to change something you have to

00:15:14,830 --> 00:15:18,490
override a bunch of stuff which isn't

00:15:17,020 --> 00:15:20,260
which isn't too bad right now but indeed

00:15:18,490 --> 00:15:21,730
to everything is much more modular so

00:15:20,260 --> 00:15:23,380
like you can make code changes without

00:15:21,730 --> 00:15:25,450
like accidentally breaking other stuff

00:15:23,380 --> 00:15:26,860
so it's really an internal redesigned

00:15:25,450 --> 00:15:30,640
functionality why is it should be the

00:15:26,860 --> 00:15:40,630
same or better thank you

00:15:30,640 --> 00:15:44,350
I noticed a place where you're asking it

00:15:40,630 --> 00:15:47,380
not to kill OpenSSH if you thought about

00:15:44,350 --> 00:15:50,020
doing like inspecting the current sort

00:15:47,380 --> 00:15:52,780
of kernels core adjust so if it's like a

00:15:50,020 --> 00:15:56,110
thousand and minus a thousand and you

00:15:52,780 --> 00:15:58,810
just don't kill it

00:15:56,110 --> 00:15:59,950
so yeah I mean you can write people have

00:15:58,810 --> 00:16:01,899
asked for that in the past it's like a

00:15:59,950 --> 00:16:03,580
Newton's please inspect the scores on

00:16:01,899 --> 00:16:05,770
the answer is usually like if you want

00:16:03,580 --> 00:16:07,779
it you can write a plug-in for it I mean

00:16:05,770 --> 00:16:09,940
we could but it's like I don't think

00:16:07,779 --> 00:16:18,850
it's super necessary given how well psi

00:16:09,940 --> 00:16:22,440
works so one of the rules was that if

00:16:18,850 --> 00:16:25,510
the work load slice gets slower than X

00:16:22,440 --> 00:16:27,790
how do you manage the CPU use it of

00:16:25,510 --> 00:16:30,010
their own D if there are a lot of slices

00:16:27,790 --> 00:16:34,600
and you need to monitor a lot of rules

00:16:30,010 --> 00:16:38,350
is there or are you simply do you only

00:16:34,600 --> 00:16:40,600
have a few enough slices on the machine

00:16:38,350 --> 00:16:43,170
that you don't have to worry about it or

00:16:40,600 --> 00:16:46,320
are you doing something else

00:16:43,170 --> 00:16:48,130
so is that asking if there's like a

00:16:46,320 --> 00:16:49,330
performance penalty if you just like

00:16:48,130 --> 00:16:51,580
running D on a system with a lot of

00:16:49,330 --> 00:16:56,260
slices like is it just gonna like take a

00:16:51,580 --> 00:16:58,450
lot of resources on the system to

00:16:56,260 --> 00:17:02,829
monitor this all the time to make sure

00:16:58,450 --> 00:17:04,240
that okay yeah so it monitors all the

00:17:02,829 --> 00:17:07,120
time it Paul's every five seconds

00:17:04,240 --> 00:17:08,770
currently so that's for some of the use

00:17:07,120 --> 00:17:10,449
cases it's not five seconds it's too

00:17:08,770 --> 00:17:12,850
slow and even something like 100

00:17:10,449 --> 00:17:15,160
milliseconds is too slow because um swab

00:17:12,850 --> 00:17:16,959
can fill up on these fancy new nvme SSDs

00:17:15,160 --> 00:17:19,329
super fast and then like the system just

00:17:16,959 --> 00:17:21,189
screws itself and like I don't know how

00:17:19,329 --> 00:17:23,110
many single-digit milliseconds or

00:17:21,189 --> 00:17:24,459
whatever and so that's why the event

00:17:23,110 --> 00:17:26,770
interface is so interesting to me

00:17:24,459 --> 00:17:28,390
because then instead of like so I think

00:17:26,770 --> 00:17:29,500
the proposal is like it's edge triggered

00:17:28,390 --> 00:17:31,990
and then once it's edge triggered based

00:17:29,500 --> 00:17:34,480
on a certain increase in like pressure

00:17:31,990 --> 00:17:36,550
then I can Paul really fast and not

00:17:34,480 --> 00:17:38,380
worry about wasting a lot of CPU but

00:17:36,550 --> 00:17:40,600
currently I think I last time I looked

00:17:38,380 --> 00:17:42,070
at it uses like over like a couple days

00:17:40,600 --> 00:17:44,350
it uses like a couple milliseconds of

00:17:42,070 --> 00:17:46,600
CPU time so it's not that much at all it

00:17:44,350 --> 00:17:47,710
just kind of goes through the Sisyphus

00:17:46,600 --> 00:17:49,480
and just kind of looks at some files

00:17:47,710 --> 00:17:51,630
it's all in memory so should be pretty

00:17:49,480 --> 00:17:51,630
fast

00:17:54,670 --> 00:18:02,180
so currently does it is it monitoring

00:17:58,670 --> 00:18:05,480
the system level or the also the C group

00:18:02,180 --> 00:18:07,700
level as well so it should be able to

00:18:05,480 --> 00:18:09,580
allied both although sometimes the

00:18:07,700 --> 00:18:12,230
secret level looms are a little more

00:18:09,580 --> 00:18:13,760
depends on how you configure it but the

00:18:12,230 --> 00:18:15,470
idea zum deikun alai both of them

00:18:13,760 --> 00:18:17,210
because the C group level the CG ohms

00:18:15,470 --> 00:18:19,040
can suffer from the same issues as the

00:18:17,210 --> 00:18:20,450
host love looms because things tend to

00:18:19,040 --> 00:18:25,340
slow down a lot and then kind of stall

00:18:20,450 --> 00:18:27,680
for a bit and then the CGM happens so if

00:18:25,340 --> 00:18:31,340
the system is under so how do you make

00:18:27,680 --> 00:18:33,950
sure the where the C group or the UM D

00:18:31,340 --> 00:18:37,430
gets this memory if it is gonna trigger

00:18:33,950 --> 00:18:40,910
and figure the actual killing of this

00:18:37,430 --> 00:18:43,060
stuff for whatever like is granted all

00:18:40,910 --> 00:18:47,420
the memory allocation that path will

00:18:43,060 --> 00:18:50,210
success succeed yes I said memory dam in

00:18:47,420 --> 00:18:54,020
that's to 64 Meg's and in theory that

00:18:50,210 --> 00:18:57,230
kind of like means like don't reclaim

00:18:54,020 --> 00:19:00,920
below this thing but even if let's

00:18:57,230 --> 00:19:03,370
suppose it's even below that it's on

00:19:00,920 --> 00:19:07,370
some part it might have to do some

00:19:03,370 --> 00:19:14,120
allocation maybe however you're checking

00:19:07,370 --> 00:19:15,230
the like or something yeah so this is a

00:19:14,120 --> 00:19:18,110
problem I was actually thinking about a

00:19:15,230 --> 00:19:19,700
while ago I was told it wouldn't be a

00:19:18,110 --> 00:19:22,370
problem and in practice I haven't seen

00:19:19,700 --> 00:19:23,510
that been a problem but yeah in the

00:19:22,370 --> 00:19:25,190
future you could always do things like

00:19:23,510 --> 00:19:26,660
you could allocate to be very beginning

00:19:25,190 --> 00:19:28,250
and just seems like very like a custom

00:19:26,660 --> 00:19:30,590
allocate or something like that allocate

00:19:28,250 --> 00:19:33,560
from a pool but yeah I haven't seen that

00:19:30,590 --> 00:19:36,820
been a problem you can block the memory

00:19:33,560 --> 00:19:39,110
yeah that's one thing we could do Tim

00:19:36,820 --> 00:19:41,090
have you started looking at the tail end

00:19:39,110 --> 00:19:42,710
of the cases that it doesn't cover you

00:19:41,090 --> 00:19:44,300
showed that it improved dramatically but

00:19:42,710 --> 00:19:46,370
then there's still a tail in case you do

00:19:44,300 --> 00:19:48,890
you have any idea of what those are yes

00:19:46,370 --> 00:19:50,210
we've been looking at that uh not like

00:19:48,890 --> 00:19:52,550
not too seriously because we've had a

00:19:50,210 --> 00:19:53,510
pretty big win already but we looked at

00:19:52,550 --> 00:19:55,010
some of the cases and one of the cases

00:19:53,510 --> 00:19:57,320
like I mentioned before is the really

00:19:55,010 --> 00:19:59,210
fast SSDs like just filling up swap

00:19:57,320 --> 00:20:01,640
really fast and these some of the other

00:19:59,210 --> 00:20:04,190
things like it doesn't handle very fast

00:20:01,640 --> 00:20:05,820
bursts to well sometimes for example if

00:20:04,190 --> 00:20:07,799
you try to allocate like 100

00:20:05,820 --> 00:20:09,210
like some sometimes it doesn't catch it

00:20:07,799 --> 00:20:10,499
immediately because again it Paul's

00:20:09,210 --> 00:20:12,239
every five seconds and it's not just

00:20:10,499 --> 00:20:15,749
sometimes you can get a little window

00:20:12,239 --> 00:20:19,320
you miss do you do you have anything

00:20:15,749 --> 00:20:20,729
that how do when you kill a process does

00:20:19,320 --> 00:20:22,109
it have to wait for that process to get

00:20:20,729 --> 00:20:23,729
scheduled again to die or do you do

00:20:22,109 --> 00:20:26,729
anything like the oom Reaper does where

00:20:23,729 --> 00:20:30,359
it somehow preemptively steals its

00:20:26,729 --> 00:20:31,649
memory away from it because I remember

00:20:30,359 --> 00:20:34,499
when I was looking at this at some point

00:20:31,649 --> 00:20:36,539
in time is the current the old oom

00:20:34,499 --> 00:20:38,220
killer before the Reaper would mark a

00:20:36,539 --> 00:20:39,690
process as ready to die and then it

00:20:38,220 --> 00:20:41,729
needed to wait for it to get scheduled

00:20:39,690 --> 00:20:42,960
again before the sync kill could get

00:20:41,729 --> 00:20:44,999
processed and the thing could actually

00:20:42,960 --> 00:20:46,830
die and I don't know I was wondering if

00:20:44,999 --> 00:20:49,200
if you've done something similar to what

00:20:46,830 --> 00:20:50,879
the room Reaper did I've been nothing

00:20:49,200 --> 00:20:53,009
fancy it sends its it killed by default

00:20:50,879 --> 00:20:54,690
okay so do you have you noticed that it

00:20:53,009 --> 00:20:56,820
takes a while to get to the process ever

00:20:54,690 --> 00:20:59,460
or is it is it get received pretty

00:20:56,820 --> 00:21:01,349
quickly so we seen the things before

00:20:59,460 --> 00:21:03,179
with like there's a mmmm Sam talked

00:21:01,349 --> 00:21:05,129
earlier today that I really liked cuz we

00:21:03,179 --> 00:21:07,289
did run into an MSM issue yeah because

00:21:05,129 --> 00:21:09,359
like I think I'm like sick kilpatt

00:21:07,289 --> 00:21:11,009
there's something like I don't know we

00:21:09,359 --> 00:21:13,590
tried to do read ahead while holding

00:21:11,009 --> 00:21:15,119
them up some lock and then the iOS a

00:21:13,590 --> 00:21:16,379
blocked because the IO submission path

00:21:15,119 --> 00:21:17,369
needed more memory and so the whole

00:21:16,379 --> 00:21:20,299
thing was just deadlock

00:21:17,369 --> 00:21:24,229
so yeah fancy work may be needed but uh

00:21:20,299 --> 00:21:24,229
we have had some issues

00:21:27,770 --> 00:21:31,660
any more lessons

00:21:37,169 --> 00:21:43,679
does it kill itself like oom D itself is

00:21:39,869 --> 00:21:44,840
immune to being killed or if it's taking

00:21:43,679 --> 00:21:46,889
too much memory

00:21:44,840 --> 00:21:48,269
yeah it's immune to being killed by

00:21:46,889 --> 00:21:50,309
itself because it doesn't mind er they

00:21:48,269 --> 00:21:51,899
own its own slice it runs in but I

00:21:50,309 --> 00:21:53,759
suppose the system killer could kill it

00:21:51,899 --> 00:22:02,279
theoretically but hopefully doesn't

00:21:53,759 --> 00:22:04,649
happen so did you disable carnal home

00:22:02,279 --> 00:22:06,389
killer and now we don't disable it so

00:22:04,649 --> 00:22:09,389
what if like in those five seconds

00:22:06,389 --> 00:22:11,100
kernel gives the process so I could

00:22:09,389 --> 00:22:13,440
repeat the last point as I understand

00:22:11,100 --> 00:22:16,499
you you pull for the system resources

00:22:13,440 --> 00:22:18,659
right every five seconds so what if in

00:22:16,499 --> 00:22:21,899
those five seconds kernel kills a

00:22:18,659 --> 00:22:24,059
process before you could catch it yeah

00:22:21,899 --> 00:22:25,499
that happens sometimes and the fix for

00:22:24,059 --> 00:22:26,940
that is so I have a lot of twin turbos

00:22:25,499 --> 00:22:28,289
you can do infinity like how fast the

00:22:26,940 --> 00:22:30,330
kicks in and like what thresholds it

00:22:28,289 --> 00:22:31,710
kicks in because I ve bills like a time

00:22:30,330 --> 00:22:33,649
series in memory of the memory pressure

00:22:31,710 --> 00:22:37,230
and does some sort of like very crappy

00:22:33,649 --> 00:22:39,509
inferences on that but yeah the answer

00:22:37,230 --> 00:22:40,830
that is like just the idea is indeed

00:22:39,509 --> 00:22:41,999
always kicks in before the kernel I'm

00:22:40,830 --> 00:22:43,529
killer cuz it kinda lone killer super

00:22:41,999 --> 00:22:45,239
slow but in those cases where it doesn't

00:22:43,529 --> 00:22:50,299
catch it it's probably worth looking at

00:22:45,239 --> 00:22:50,299
tuning em do you catch those cases okay

00:22:52,249 --> 00:22:55,249
anymore

00:22:56,380 --> 00:23:03,190
okay thank you very much

00:23:00,110 --> 00:23:03,190

YouTube URL: https://www.youtube.com/watch?v=lz1V3ZeZMVo


