Title: LPC2019 - Finding more DRAM
Publication date: 2019-11-18
Playlist: LPC2019 - LPC Main Track
Description: 
	Finding more DRAM

Speakers
 Shakeel Butt (Google)
 Suren Baghdasaryan (Google)
 Yu Zhao (Google)

Description
The demand of DRAM across different platforms is increasing but the cost is not decreasing. Thus DRAM is a major factor of the total cost across all kinds of devices like mobile, desktop or servers. In this talk we will be presenting the work we are doing at Google, applicable to Android, Chrome OS and data center servers, on extracting more memory out of running applications without impacting performance.

The key is to proactively reclaim idle memory from the running applications. For the Android and Chrome OS, the user space controller can provide hints of the idle memory at the applications level while the servers running multiple workloads, an idle memory tracking mechanism is needed. With such hints the kernel can proactively reclaim memory given that estimated refault cost is not high. Using in-memory compression or second tier memory, the refault cost can be reduced drastically.

We have developed and deployed the proactive reclaim and idle memory tracking across Google data centers [1]. Defining idle memory as memory not accessed in the last 2 mins, we found 32% idle memory across data centers and we were able to reclaim 30% of this idle memory, while not impacting the performance. This results in 3x cheaper memory for our data centers. 98% of the applications spend only around 0.1% of their CPU on memory compression and decompression. Also the idle memory tracking on average takes less than 11% of a single logical CPU.

The cost of proactive reclaim and idle memory tracking is reasonable for the data centers cost of ownership of memory, however, it imposes challenges for power constrained devices based on Android and Chrome OS. These devices run diverse applications e.g. Chrome OS can run Android and Linux in a VM. To that end, we are working on making idle memory tracking and proactive reclaim feasible for such devices. Henceforth, we are interested and would like to initiate discussion on making proactive reclaim useful for other use-cases as well.

[1] Software-Defined Far Memory in Warehouse-Scale Computers, ACM ASPLOS 2019.
Captions: 
	00:00:01,180 --> 00:00:03,130
- So my name is Shakeel.

00:00:03,130 --> 00:00:06,300
I work on Linux kernel memory management

00:00:07,160 --> 00:00:11,303
for data centers for server side at Google.

00:00:12,180 --> 00:00:17,180
- And I'm Suren, I work for Android team at Google,

00:00:17,270 --> 00:00:18,813
also in memory management.

00:00:19,890 --> 00:00:22,540
- So today we are going to talk about more

00:00:22,540 --> 00:00:27,200
on the core techniques and principles

00:00:27,200 --> 00:00:29,840
to improve the memory utilization

00:00:29,840 --> 00:00:32,280
without impacting the performance

00:00:32,280 --> 00:00:34,860
across different computing platforms

00:00:34,860 --> 00:00:37,053
from mobile devices to data centers.

00:00:38,790 --> 00:00:42,840
So first thing, why do we need more DRAM?

00:00:42,840 --> 00:00:46,180
So let's talk about the mobile devices.

00:00:46,180 --> 00:00:49,880
So the demand for the RAM

00:00:51,030 --> 00:00:53,600
from the application is keep increasing.

00:00:53,600 --> 00:00:56,460
But there's a limit how much RAM we can put

00:00:57,581 --> 00:00:59,010
on a mobile device due

00:00:59,010 --> 00:01:03,830
to manufacturer's cost, size.

00:01:03,830 --> 00:01:06,270
And once the device is there,

00:01:06,270 --> 00:01:08,290
you have built the customer's device,

00:01:08,290 --> 00:01:12,593
you cannot really retrofit more RAM into the device.

00:01:16,923 --> 00:01:18,460
On the other end of this computer platform

00:01:18,460 --> 00:01:19,950
for the data centers,

00:01:19,950 --> 00:01:22,640
it's mainly the cost.

00:01:22,640 --> 00:01:26,210
So DRAM cost is one of the major factor

00:01:26,210 --> 00:01:29,640
of the whole data center cost.

00:01:29,640 --> 00:01:32,900
And usually the RAM

00:01:32,900 --> 00:01:37,103
is over-provisioned and under-utilized.

00:01:39,370 --> 00:01:44,370
To reduce this cost, you have to increase the utilization

00:01:45,670 --> 00:01:47,790
without impacting the performance.

00:01:47,790 --> 00:01:52,790
And for the data center, usually the RAM is more expensive.

00:01:53,150 --> 00:01:54,260
It's error correcting.

00:01:54,260 --> 00:01:59,260
A couple of bits per bytes are used for reliability.

00:02:04,440 --> 00:02:06,753
So the current status,

00:02:09,800 --> 00:02:13,490
currently for the Android, there is shortage of RAM.

00:02:15,303 --> 00:02:18,070
What current Android does is,

00:02:18,070 --> 00:02:21,313
it start killing background applications.

00:02:22,220 --> 00:02:26,683
The impact of that particular operation is,

00:02:27,530 --> 00:02:32,530
once you use a switchback to the background application,

00:02:32,980 --> 00:02:35,760
it will be kind of like restart from scratch.

00:02:35,760 --> 00:02:37,090
So it's a cold start,

00:02:37,090 --> 00:02:41,400
which is slow and power-hungry.

00:02:41,400 --> 00:02:45,560
So it will impact the battery,

00:02:45,560 --> 00:02:48,023
and also the user experience as well.

00:02:52,030 --> 00:02:54,070
On the other end, for the data centers,

00:02:54,070 --> 00:02:55,773
usually overcommit the memory.

00:02:56,630 --> 00:02:58,450
And when you overcommit the memory,

00:02:58,450 --> 00:03:00,520
there's more chances

00:03:00,520 --> 00:03:02,960
you will hit the global memory pressure.

00:03:02,960 --> 00:03:04,490
And when you hit global memory pressure,

00:03:04,490 --> 00:03:06,310
you will go into direct reclaim.

00:03:06,310 --> 00:03:07,840
And once you are in the direct reclaim,

00:03:07,840 --> 00:03:11,930
there is no performance isolation between the jobs.

00:03:11,930 --> 00:03:15,110
And there are so many different ways things can go wrong

00:03:15,110 --> 00:03:17,020
in the direct reclaim,

00:03:17,020 --> 00:03:19,031
particularly in the Linux kernel.

00:03:19,031 --> 00:03:22,010
There are so many different heuristics

00:03:22,010 --> 00:03:24,763
and many different things can go wrong there.

00:03:28,580 --> 00:03:30,603
Even when you are overcommitting,

00:03:33,693 --> 00:03:34,526
particularly for the server

00:03:34,526 --> 00:03:36,070
where you're running multiple jobs,

00:03:38,369 --> 00:03:40,380
bad jobs, load-intensive jobs,

00:03:40,380 --> 00:03:42,863
you never want to go into direct reclaim.

00:03:44,910 --> 00:03:46,450
And the second thing is,

00:03:46,450 --> 00:03:49,850
once when you have reclaim memory,

00:03:49,850 --> 00:03:53,810
and you refault those memory, it can be expensive

00:03:54,950 --> 00:03:59,950
because you can be faulting from slow storage devices.

00:04:03,660 --> 00:04:07,520
Overcommit can impact the system

00:04:07,520 --> 00:04:10,100
by inducing global memory pressure

00:04:10,100 --> 00:04:14,623
and attributing more refaults, which are costly.

00:04:17,560 --> 00:04:21,323
So the solution we are looking at is basically,

00:04:23,130 --> 00:04:25,710
instead of waiting for the whole system

00:04:25,710 --> 00:04:27,920
to go into the memory pressure,

00:04:27,920 --> 00:04:30,890
why not proactively reclaim memory?

00:04:30,890 --> 00:04:35,630
So here there are reclaimed unneeded memory,

00:04:35,630 --> 00:04:39,480
which is unneeded, hopefully in the near future,

00:04:39,480 --> 00:04:41,993
and which is also very cheap to refault.

00:04:43,230 --> 00:04:46,050
So there are two points, unneeded memory

00:04:46,050 --> 00:04:47,183
and cheap to refault.

00:04:48,170 --> 00:04:53,163
So the way Android approach this problem is,

00:04:54,050 --> 00:04:58,900
Android has a way at the user spacer, good way,

00:04:58,900 --> 00:05:02,800
which is like a front-facing application

00:05:02,800 --> 00:05:04,680
and the background application,

00:05:04,680 --> 00:05:06,150
very much clear-cut there.

00:05:06,150 --> 00:05:07,300
So Android can say,

00:05:07,300 --> 00:05:08,960
okay, these are the background application,

00:05:08,960 --> 00:05:11,500
why not just reclaim memory from there?

00:05:11,500 --> 00:05:15,310
And cheap refault is,

00:05:15,310 --> 00:05:18,960
why not instead of reclaiming the file pages

00:05:18,960 --> 00:05:21,390
or the pages which you have to refault

00:05:21,390 --> 00:05:23,170
from the slow storage,

00:05:23,170 --> 00:05:25,740
why not use in-memory compression,

00:05:25,740 --> 00:05:27,300
which is very fast?

00:05:27,300 --> 00:05:29,120
It's a couple of micro-seconds.

00:05:29,120 --> 00:05:32,190
And Android is currently using zram.

00:05:32,190 --> 00:05:35,363
On the other end, for the server side,

00:05:37,260 --> 00:05:40,040
in the server, you don't really know

00:05:40,040 --> 00:05:43,710
when you are running a lot of mixed workload,

00:05:43,710 --> 00:05:45,813
load-intensive jobs, bad jobs,

00:05:49,130 --> 00:05:52,690
Android cannot differentiate at the job level.

00:05:52,690 --> 00:05:57,400
So we need more fine-grained mechanism

00:05:57,400 --> 00:06:00,633
to track which memory we should be reclaiming.

00:06:02,130 --> 00:06:06,040
And I will go in more detail

00:06:07,676 --> 00:06:09,383
what we did at Google.

00:06:12,820 --> 00:06:14,960
We also use in-memory compression,

00:06:14,960 --> 00:06:17,320
but we use zswap in the server.

00:06:17,320 --> 00:06:21,010
And this is deployed across the Google center.

00:06:26,060 --> 00:06:29,930
- So on Android side, as Shakeel said,

00:06:29,930 --> 00:06:32,300
we are trying to avoid cold starts,

00:06:32,300 --> 00:06:35,090
which is a restart from scratch.

00:06:35,090 --> 00:06:37,410
And to do that, we need to make sure

00:06:37,410 --> 00:06:41,020
that we can keep more background applications alive

00:06:41,020 --> 00:06:44,250
while not impacting the interactive ones.

00:06:44,250 --> 00:06:47,610
So the idea is, how can we shrink the background ones,

00:06:47,610 --> 00:06:51,177
which our user doesn't interrupt right now.

00:06:54,610 --> 00:06:56,390
What's wrong with this picture?

00:06:56,390 --> 00:06:57,790
And that's what we saw,

00:06:57,790 --> 00:07:02,790
it's the size of this big orange section,

00:07:03,530 --> 00:07:05,740
which represents cached applications

00:07:05,740 --> 00:07:07,460
or background applications.

00:07:07,460 --> 00:07:10,420
So what we see is the biggest hogger

00:07:10,420 --> 00:07:12,520
of the memory are the background applications,

00:07:12,520 --> 00:07:14,104
which is understandable.

00:07:14,104 --> 00:07:15,590
There are many more background applications

00:07:15,590 --> 00:07:17,143
than foreground applications.

00:07:19,650 --> 00:07:24,650
So basically our goal is to reclaim this memory,

00:07:24,890 --> 00:07:26,403
which is mostly idle,

00:07:28,343 --> 00:07:32,400
and what kind of heuristics we can come up with to do that.

00:07:32,400 --> 00:07:37,400
And at the rightmost end of this picture is

00:07:37,440 --> 00:07:39,500
when memory claim happened

00:07:39,500 --> 00:07:41,550
and finally reclaim that memory.

00:07:41,550 --> 00:07:43,410
So how can we do that proactively

00:07:43,410 --> 00:07:48,160
so that we don't get the system into a bad situation

00:07:48,160 --> 00:07:50,183
before this memory gets reclaimed?

00:07:52,820 --> 00:07:56,983
So the idea is because in Android,

00:07:59,209 --> 00:08:04,209
we can distinguish foreground and background applications,

00:08:04,930 --> 00:08:07,870
and our data analysis shows that

00:08:07,870 --> 00:08:09,890
if application went into the background

00:08:09,890 --> 00:08:13,668
and stays there for at least 10, 15 seconds,

00:08:13,668 --> 00:08:15,350
it's a good indication

00:08:15,350 --> 00:08:18,620
that it's not gonna be re-used anytime soon.

00:08:18,620 --> 00:08:22,440
What we can do is basically reclaim the memory from it

00:08:22,440 --> 00:08:24,120
as soon as that state happens.

00:08:24,120 --> 00:08:27,260
So it went into background sites there

00:08:27,260 --> 00:08:28,863
for 15 seconds, let's say.

00:08:31,770 --> 00:08:32,960
We can hint a kernel

00:08:32,960 --> 00:08:36,430
that this application is not going to be used

00:08:36,430 --> 00:08:39,180
anytime soon, go ahead and reclaim

00:08:43,731 --> 00:08:48,148
some of the memory which is easily paged in later on.

00:08:49,640 --> 00:08:51,900
So the only thing we are missing is mechanism to do that.

00:08:51,900 --> 00:08:54,750
So we are working on implementing a new syscall

00:08:54,750 --> 00:08:57,120
called process_madvise.

00:08:57,120 --> 00:09:02,120
And it also includes two additional madvise options,

00:09:02,120 --> 00:09:04,750
which is madvise cold and madvise pageout.

00:09:05,870 --> 00:09:08,067
madvise cold deactivates the pages,

00:09:08,067 --> 00:09:12,100
but basically it moves the pages from active LRU

00:09:12,100 --> 00:09:13,997
into head of the inactive LRU,

00:09:14,910 --> 00:09:18,350
which speeds up the reclaim of those pages.

00:09:18,350 --> 00:09:22,873
And madvise pageout immediately reclaims those pages.

00:09:24,537 --> 00:09:27,393
And I'll explain why we have two different options.

00:09:28,720 --> 00:09:32,170
Those two madvise options are already

00:09:32,170 --> 00:09:34,100
in the MM tree, they are immersed there.

00:09:34,100 --> 00:09:37,530
So hopefully we'll see them pretty soon upstream.

00:09:37,530 --> 00:09:40,910
And process_madvise is still under development.

00:09:40,910 --> 00:09:44,670
And as a result, when we ran our tests,

00:09:44,670 --> 00:09:49,130
we get about 15% less kills from dogfood population,

00:09:49,130 --> 00:09:53,810
basically on some limited set of users,

00:09:53,810 --> 00:09:56,170
and up to 30% less kills

00:09:56,170 --> 00:09:58,810
when we are running our stress tests,

00:09:58,810 --> 00:10:00,883
which are synthetic stress tests.

00:10:02,530 --> 00:10:06,420
So why did we come up with those two different options?

00:10:06,420 --> 00:10:09,775
Because when we started experimenting with this,

00:10:09,775 --> 00:10:13,040
first we decided, let's reclaim all the pages

00:10:15,950 --> 00:10:17,880
in a non-destructive way.

00:10:17,880 --> 00:10:21,100
Because madvise has already options

00:10:21,100 --> 00:10:24,510
like do not need and free,

00:10:24,510 --> 00:10:28,520
which reclaims them, but it basically hints the kernel

00:10:28,520 --> 00:10:32,990
that this page, we don't need them, period.

00:10:32,990 --> 00:10:34,550
So they can be dropped.

00:10:34,550 --> 00:10:37,320
In our case, we want to tell the kernel,

00:10:37,320 --> 00:10:40,151
these pages are not needed right now.

00:10:40,151 --> 00:10:43,160
They don't destroy the data in them.

00:10:43,160 --> 00:10:44,790
But you can reclaim the memory

00:10:44,790 --> 00:10:46,303
in a non-destructive way.

00:10:48,300 --> 00:10:49,690
So the first experiment was,

00:10:49,690 --> 00:10:52,120
let's reclaim non-destructively,

00:10:52,120 --> 00:10:54,480
all the pages of the background applications,

00:10:54,480 --> 00:10:58,130
and what we saw was regressions.

00:10:58,130 --> 00:11:02,320
So we saw regressions on some specific tests.

00:11:02,320 --> 00:11:04,880
And after analyzing the results,

00:11:04,880 --> 00:11:08,660
we realized that reclaiming file-backed pages,

00:11:08,660 --> 00:11:11,640
which is basically clear pages,

00:11:11,640 --> 00:11:14,330
dropping them in and paging them out,

00:11:14,330 --> 00:11:17,593
paging them back in is too costly.

00:11:19,190 --> 00:11:23,520
So what can we do to improve that is,

00:11:23,520 --> 00:11:25,890
we decided instead of reclaiming them,

00:11:25,890 --> 00:11:27,660
let's deactivate them.

00:11:27,660 --> 00:11:30,620
That basically speeds up the reclaim in the future

00:11:30,620 --> 00:11:33,510
when memory pressure rises

00:11:33,510 --> 00:11:36,030
without the need to page them in

00:11:36,030 --> 00:11:39,033
if they are re-used sometime soon.

00:11:39,960 --> 00:11:42,760
And that was the second experiment

00:11:42,760 --> 00:11:45,440
where we saw mostly improvements,

00:11:45,440 --> 00:11:48,400
but in one case there was a regression.

00:11:48,400 --> 00:11:49,993
And after analyzing that,

00:11:51,968 --> 00:11:53,810
we found out that the reason

00:11:53,810 --> 00:11:56,480
for the regression is the timing

00:11:56,480 --> 00:12:00,450
of those deactivations on reclaim.

00:12:00,450 --> 00:12:04,240
So when the system is already under pressure,

00:12:04,240 --> 00:12:08,340
you don't want to do the application compaction like that

00:12:08,340 --> 00:12:12,270
because you are creating races between kswapd

00:12:12,270 --> 00:12:14,199
or direct reclaim

00:12:14,199 --> 00:12:17,050
with your processes

00:12:17,050 --> 00:12:19,600
which try to compact the application.

00:12:19,600 --> 00:12:24,290
So by timing the application compaction at the right moment

00:12:24,290 --> 00:12:26,090
when there is no memory pressure,

00:12:26,090 --> 00:12:28,453
we got rid of this regression also.

00:12:30,790 --> 00:12:34,100
So in the end, we got those numbers,

00:12:34,100 --> 00:12:37,870
15% less kills in the field

00:12:37,870 --> 00:12:42,083
and 30% less kills running the stress tests.

00:12:42,990 --> 00:12:46,240
Some of the examples like, a popular game can be compressed

00:12:46,240 --> 00:12:49,670
from 1.7 gigabytes in the background

00:12:49,670 --> 00:12:53,850
into 700 megabytes when it's compacted into zram,

00:12:53,850 --> 00:12:57,890
which is pretty good decrease in size.

00:12:57,890 --> 00:13:00,760
We don't see a noticeable penalty on warm starts.

00:13:00,760 --> 00:13:03,420
Warm starts is basically when we need to decompress this

00:13:03,420 --> 00:13:04,793
into memory back.

00:13:05,640 --> 00:13:07,490
And that's because we are using zram,

00:13:08,555 --> 00:13:11,110
which is in RAM compression, basically.

00:13:11,110 --> 00:13:13,450
So it's much faster than paging

00:13:14,803 --> 00:13:18,380
in the application from scratch.

00:13:18,380 --> 00:13:21,640
And we do see a number of cold starts dropping

00:13:21,640 --> 00:13:23,223
because we kill less.

00:13:25,600 --> 00:13:28,643
So back to the implementation,

00:13:29,730 --> 00:13:31,450
we have those two hints.

00:13:31,450 --> 00:13:36,023
And I explained why we have them separate, not just one.

00:13:43,510 --> 00:13:46,740
So process_madvise is still under development

00:13:46,740 --> 00:13:49,790
because of the one issue we hit.

00:13:49,790 --> 00:13:54,210
And that will be one of the discussion points later on

00:13:54,210 --> 00:13:55,690
in this presentation.

00:13:55,690 --> 00:13:57,690
But other than that,

00:13:57,690 --> 00:14:00,683
new madvise options are already in the MM tree,

00:14:01,534 --> 00:14:02,593
so expect them soon.

00:14:04,492 --> 00:14:06,300
And what process_madvise does

00:14:06,300 --> 00:14:07,970
is basically the same thing as madvise,

00:14:07,970 --> 00:14:12,970
but it can operate on the VMAs of a different process.

00:14:13,530 --> 00:14:17,860
So that's done so that system management software

00:14:17,860 --> 00:14:22,860
can hint kernel about some other processes of memory,

00:14:25,310 --> 00:14:26,623
that it can be shrunk.

00:14:32,210 --> 00:14:34,880
- So now coming back to the proactive reclaim

00:14:34,880 --> 00:14:36,300
for the data center.

00:14:36,300 --> 00:14:40,970
So here as I mentioned before, it's not like Android,

00:14:40,970 --> 00:14:44,300
you know about the background of the program application.

00:14:44,300 --> 00:14:48,640
There are a lot of latency sensitive jobs

00:14:48,640 --> 00:14:53,220
and bad jobs of different priorities running on the server.

00:14:53,220 --> 00:14:57,020
So we have a more fine gain mechanism to find the memory

00:14:57,020 --> 00:14:58,980
which we should be reclaiming.

00:14:58,980 --> 00:15:03,310
So even before going in those details,

00:15:03,310 --> 00:15:05,470
let me just give you the high level

00:15:05,470 --> 00:15:08,093
how at Google we do the memory overcommit.

00:15:09,040 --> 00:15:13,940
So the high level, it's basically replace the part

00:15:13,940 --> 00:15:18,580
of DRAM with the cheap slow memory or far memory.

00:15:18,580 --> 00:15:22,710
So how provisioned memory is like someone requests

00:15:22,710 --> 00:15:24,473
for x amount of memory,

00:15:25,810 --> 00:15:27,260
which basically translates

00:15:27,260 --> 00:15:31,073
to some part of its DRAM,

00:15:31,928 --> 00:15:34,270
and the part is a cheap slow memory.

00:15:34,270 --> 00:15:38,460
And this model is generic enough

00:15:38,460 --> 00:15:42,420
that the cheap slow memory is completely transparent

00:15:42,420 --> 00:15:43,750
to the user.

00:15:43,750 --> 00:15:47,068
And zswap is one example.

00:15:47,068 --> 00:15:49,263
It can be any other PMEM or remote swap.

00:15:51,739 --> 00:15:56,739
And the size of this cheap slow memory corresponds

00:15:56,810 --> 00:16:01,810
to our estimation of how much idle memory these jobs have.

00:16:05,470 --> 00:16:10,470
And this is the graph we also have

00:16:10,840 --> 00:16:13,030
in our paper in S-Plus.

00:16:13,030 --> 00:16:17,640
So basically this tells us how much opportunity do we have

00:16:18,730 --> 00:16:20,340
across the Google data centers.

00:16:20,340 --> 00:16:25,340
This is data for multi-tenant data center.

00:16:26,230 --> 00:16:27,483
What it tells you,

00:16:29,120 --> 00:16:31,613
if we define an idle memory,

00:16:32,680 --> 00:16:33,820
the access is basically

00:16:33,820 --> 00:16:35,360
how you are defining the idle memory.

00:16:35,360 --> 00:16:38,463
If the idle memory is two minutes,

00:16:39,587 --> 00:16:41,330
a page is not accessed for the last two minutes.

00:16:41,330 --> 00:16:43,310
If that's the idle memory,

00:16:43,310 --> 00:16:45,430
then the opportunity,

00:16:45,430 --> 00:16:49,690
there is 32% memory is idle

00:16:49,690 --> 00:16:54,320
across these Google data centers.

00:16:54,320 --> 00:16:57,700
However, if we set this thing,

00:16:57,700 --> 00:16:59,090
there is also

00:17:01,513 --> 00:17:04,570
a 13-14% refault rate

00:17:04,570 --> 00:17:07,383
if we decide to go with this definition.

00:17:10,422 --> 00:17:13,900
14% of this idle memory

00:17:13,900 --> 00:17:16,300
is becoming known idle

00:17:16,300 --> 00:17:18,653
or hot memory permanent.

00:17:19,540 --> 00:17:23,440
So we have to find a spot

00:17:23,440 --> 00:17:24,590
where we are comfortable

00:17:24,590 --> 00:17:28,273
with the opportunity versus the cost.

00:17:32,330 --> 00:17:37,330
So before going to the why of what we did,

00:17:40,010 --> 00:17:43,420
the question is why the current Linux kernel provides.

00:17:43,420 --> 00:17:46,290
Why not use whatever there is already there?

00:17:46,290 --> 00:17:51,290
So Linux kernel has a kswapd, a background reclaimer,

00:17:52,410 --> 00:17:54,850
which we did not use.

00:17:54,850 --> 00:17:58,750
The reason is, the whole memory reclaim

00:17:58,750 --> 00:18:03,750
in the Linux kernel is full of tons of heuristics,

00:18:04,080 --> 00:18:06,700
and complicated heuristics and stuff.

00:18:06,700 --> 00:18:11,600
And the kswapd is based on more off of watermarks,

00:18:11,600 --> 00:18:14,700
like if the free memory goes below some watermark,

00:18:14,700 --> 00:18:16,223
then start reclaiming.

00:18:17,414 --> 00:18:20,670
And reclaim until you hit the higher watermark.

00:18:20,670 --> 00:18:24,692
While here we don't really care about the watermarks.

00:18:24,692 --> 00:18:28,590
Even if the free memory is more than 50%,

00:18:28,590 --> 00:18:32,857
but you have a lot of idle memory there

00:18:32,857 --> 00:18:36,623
for a couple of minutes, just reclaim those.

00:18:39,620 --> 00:18:43,800
And the Linux kernel does provide a mechanism

00:18:44,740 --> 00:18:47,690
how to find idle memory or the hot memory.

00:18:47,690 --> 00:18:51,220
So Linux kernel does have the page idle tracking,

00:18:51,220 --> 00:18:55,950
which has a very high CPU overhead and the memory overhead.

00:18:55,950 --> 00:19:00,340
So this is the pseudocode what it looks like is basically,

00:19:00,340 --> 00:19:05,270
you go through the whole memory, read the flags,

00:19:05,270 --> 00:19:07,680
see if it is something you are interested in,

00:19:07,680 --> 00:19:10,960
then get the cgroup of that page.

00:19:10,960 --> 00:19:14,250
And then get the access bits of that page,

00:19:14,250 --> 00:19:19,250
if this was accessed in the last time you checked this page.

00:19:20,350 --> 00:19:21,183
If it is accessed,

00:19:21,183 --> 00:19:24,730
it means in that cycle it was accessed, so it is a hold.

00:19:24,730 --> 00:19:26,283
Otherwise, it's a idle page.

00:19:28,100 --> 00:19:31,730
So we have implemented kstaled,

00:19:31,730 --> 00:19:34,620
which is kind of like similar approach.

00:19:34,620 --> 00:19:39,220
But it's a in-kernel idle page tracking.

00:19:39,220 --> 00:19:41,010
It doesn't have memory overhead

00:19:41,010 --> 00:19:44,540
because of all this information are stored in page,

00:19:44,540 --> 00:19:46,400
or page flags, more specifically.

00:19:46,400 --> 00:19:48,400
But it has the similar CPU overhead.

00:19:48,400 --> 00:19:50,570
You still have to go through all the PFNs

00:19:51,925 --> 00:19:54,353
and find there,

00:19:55,440 --> 00:19:57,653
are these pages were accessed or not.

00:19:59,720 --> 00:20:02,290
And once you have this information,

00:20:02,290 --> 00:20:06,240
you can track the age of each page.

00:20:06,240 --> 00:20:09,200
And based on the threshold, so we have a separate thread,

00:20:09,200 --> 00:20:11,220
which also does scanning,

00:20:11,220 --> 00:20:15,863
and it sees if the page is colder, or idle,

00:20:17,709 --> 00:20:20,209
or more idle than some threshold, just reclaim it.

00:20:21,490 --> 00:20:24,790
So kstaled maintains the age

00:20:24,790 --> 00:20:28,260
while kreclaimd reclaims them based on some threshold.

00:20:28,260 --> 00:20:31,730
And then we have per-memcg job knobs

00:20:31,730 --> 00:20:34,340
where you can specify

00:20:34,340 --> 00:20:37,780
what that threshold should be for this job.

00:20:37,780 --> 00:20:39,700
And also it provides you some histograms,

00:20:39,700 --> 00:20:43,500
the refault histograms, et cetera.

00:20:44,440 --> 00:20:48,793
Now it still have the CPU overhead.

00:20:52,634 --> 00:20:54,801
So this overhead is linear

00:20:55,920 --> 00:20:58,850
to the size of the RAM.

00:20:58,850 --> 00:21:01,890
As you increase the size, it will also increase

00:21:01,890 --> 00:21:04,230
because you are scanning the whole PFN.

00:21:04,230 --> 00:21:08,480
And if you increase the frequency of your scan,

00:21:08,480 --> 00:21:10,623
again, you're spending more CPU time.

00:21:11,640 --> 00:21:14,360
And for the inspection, what we found out

00:21:14,360 --> 00:21:19,360
that the main cost of this scanning is basically the rmap.

00:21:19,710 --> 00:21:22,743
So in Linux, when you have a page,

00:21:26,366 --> 00:21:28,767
you can reverse walk and map walk

00:21:32,220 --> 00:21:34,270
to see where this page is mapped,

00:21:34,270 --> 00:21:36,703
in which address space is this page mapped.

00:21:38,144 --> 00:21:40,930
So you go to all the page tables this page is mapped

00:21:40,930 --> 00:21:44,140
and those page tables will have the access bit,

00:21:44,140 --> 00:21:47,290
this page through this mapping was accessed.

00:21:47,290 --> 00:21:51,733
So you transferred all those accesses to the page referenced

00:21:53,020 --> 00:21:56,720
bit or page idle bit of this page.

00:21:56,720 --> 00:22:00,370
So this reverse map walk

00:22:00,370 --> 00:22:04,353
was 50% time spent on this thing.

00:22:07,390 --> 00:22:10,463
To read the implementing of optimization,

00:22:10,463 --> 00:22:13,350
instead of going through the page

00:22:13,350 --> 00:22:16,780
and going back to all the mapping

00:22:16,780 --> 00:22:18,640
where this page was mapped,

00:22:18,640 --> 00:22:22,063
why not, whenever someone creates the mapping,

00:22:23,120 --> 00:22:25,820
have separate map structure

00:22:25,820 --> 00:22:27,740
to link all the PMD.

00:22:27,740 --> 00:22:29,200
So PMD is the

00:22:33,123 --> 00:22:34,683
second-level page table entry.

00:22:35,730 --> 00:22:38,550
So there we basically linked all the PMDs

00:22:38,550 --> 00:22:40,173
that were created on the system.

00:22:42,076 --> 00:22:44,560
So now instead of from the page,

00:22:44,560 --> 00:22:48,410
going back to the PMDs, we go from the PMDs to the pages.

00:22:48,410 --> 00:22:52,150
So after each cycle, traverse the PMDs

00:22:52,150 --> 00:22:54,720
and transfer the access bits

00:22:54,720 --> 00:22:56,673
from here to the corresponding pages.

00:23:00,760 --> 00:23:04,650
So you don't have to traverse all the PFNs,

00:23:04,650 --> 00:23:08,690
all the pages, and reverse track and start here.

00:23:08,690 --> 00:23:10,910
If you have not too many pages mapped,

00:23:10,910 --> 00:23:13,273
then you will be traversing very less pages.

00:23:14,380 --> 00:23:17,460
So this is one optimization.

00:23:17,460 --> 00:23:21,410
But still you have to maintain the age.

00:23:21,410 --> 00:23:23,440
So those pages which are not accessed,

00:23:23,440 --> 00:23:25,640
you have to increment their age.

00:23:25,640 --> 00:23:27,850
So there is still this cost.

00:23:27,850 --> 00:23:31,170
We are like this follow our work to also remove this thing.

00:23:31,170 --> 00:23:33,703
But at the moment, this is what we are doing.

00:23:39,260 --> 00:23:41,800
To find the access bits,

00:23:41,800 --> 00:23:43,740
you don't have to traverse the PFNs,

00:23:43,740 --> 00:23:46,530
but for ageing, you still need to.

00:23:46,530 --> 00:23:48,880
And by using this optimization,

00:23:48,880 --> 00:23:53,880
we were able to reduce the cost by 3.5x, the CPU cost.

00:23:54,090 --> 00:23:57,550
And similarly, the other thread, the reclaimer,

00:23:57,550 --> 00:24:01,493
it doesn't really need to scan the PFN as well.

00:24:02,660 --> 00:24:04,100
So this threshold checking,

00:24:04,100 --> 00:24:08,400
the first thread can see this was above some threshold,

00:24:08,400 --> 00:24:12,040
it can queue this page for the other thread to reclaim.

00:24:12,040 --> 00:24:15,870
So we removed the scanning from the kreclaimd.

00:24:15,870 --> 00:24:19,123
And it's also CPU reduced by 1.5x.

00:24:20,730 --> 00:24:24,323
So I presented this work at the LSFMM,

00:24:26,270 --> 00:24:30,380
the Memory Management Maintainers conference.

00:24:30,380 --> 00:24:35,170
So their main concern was why we are bypassing the LRUs.

00:24:35,170 --> 00:24:37,763
So what we are doing is,

00:24:40,350 --> 00:24:43,350
Linux maintains the inactive LRU, active LRU.

00:24:43,350 --> 00:24:46,210
So we are bypassing all of those LRUs

00:24:46,210 --> 00:24:48,000
and reclaiming directly those pages,

00:24:48,000 --> 00:24:51,670
maintaining their ages and reclaiming those.

00:24:51,670 --> 00:24:53,370
So first, they are like,

00:24:53,370 --> 00:24:57,720
why are you bypassing the existing infrastructure?

00:24:57,720 --> 00:25:00,623
And the second was the CPU cost is still high.

00:25:03,900 --> 00:25:07,150
There is further work we need to do,

00:25:07,150 --> 00:25:09,983
which we are doing to further reduce the cost.

00:25:11,150 --> 00:25:15,650
So here the potential way forward what I see

00:25:15,650 --> 00:25:17,300
and I think we are currently doing,

00:25:17,300 --> 00:25:21,280
it's basically what the Chrome OS team is doing is

00:25:21,280 --> 00:25:25,500
to decouple the ageing from the memory pressure.

00:25:25,500 --> 00:25:28,650
So Linux kernel also does the ageing.

00:25:28,650 --> 00:25:30,773
It's inactive LRU, active LRU,

00:25:34,150 --> 00:25:35,800
it presents the ages of the page.

00:25:36,960 --> 00:25:40,880
So all the pages which are accessed more are

00:25:40,880 --> 00:25:42,390
in the active LRU,

00:25:42,390 --> 00:25:45,400
which are accessed just once or in the inactive.

00:25:45,400 --> 00:25:50,400
And it sorts the pages based on the memory pressure.

00:25:50,530 --> 00:25:54,030
So what I think we have to do is to decouple that ageing

00:25:54,030 --> 00:25:55,390
from the memory pressure.

00:25:55,390 --> 00:25:58,313
Don't wait for the memory pressure to sort the LRUs.

00:26:00,260 --> 00:26:04,600
And once you have decoupled the ageing,

00:26:04,600 --> 00:26:09,600
proactively age, and use the age to sort the LRUs.

00:26:11,020 --> 00:26:13,300
So even if there is a global pressure

00:26:14,705 --> 00:26:18,463
and memory claim comes, check reclaim,

00:26:20,143 --> 00:26:22,483
LRU will be more up-to-date.

00:26:23,890 --> 00:26:26,960
And the second is more of,

00:26:26,960 --> 00:26:31,590
instead of fixed-kernel triggering the ageing

00:26:31,590 --> 00:26:34,250
or the reclaiming after a cycle,

00:26:34,250 --> 00:26:37,270
we can provide a tunable to the users

00:26:37,270 --> 00:26:42,270
who want to, how much CPU they can spend, for example.

00:26:43,760 --> 00:26:45,023
For the data centers,

00:26:46,540 --> 00:26:50,500
doing every two minutes, it's not that much CPU cost.

00:26:50,500 --> 00:26:54,580
But maybe for something in between the mobile devices

00:26:54,580 --> 00:26:57,030
and the data centers, the laptops, the Chrome OS,

00:26:59,231 --> 00:27:02,890
it can be configured by the users

00:27:02,890 --> 00:27:06,740
or the admins to, instead of every two minutes,

00:27:06,740 --> 00:27:10,427
maybe reduce the frequency

00:27:13,130 --> 00:27:17,180
or to start ageing

00:27:17,180 --> 00:27:19,260
when the free memory is

00:27:20,606 --> 00:27:22,760
below half or something.

00:27:22,760 --> 00:27:26,160
So give the user more control

00:27:26,160 --> 00:27:28,653
how much CPU they want to spend here.

00:27:30,150 --> 00:27:32,343
So this is still work going on.

00:27:37,310 --> 00:27:40,213
To conclude, and the discussion point, first thing,

00:27:43,160 --> 00:27:48,160
one major conclusion and the point I want to make is,

00:27:48,180 --> 00:27:53,180
if you are going to increase the memory utilization,

00:27:53,180 --> 00:27:56,500
you want to reduce the memory cost of your systems

00:27:57,340 --> 00:28:02,340
and increase the memory utilization,

00:28:02,440 --> 00:28:05,240
try avoid direct reclaim

00:28:07,110 --> 00:28:08,270
at all cost.

00:28:08,270 --> 00:28:11,013
Because when you are in the direct reclaim,

00:28:12,080 --> 00:28:15,330
there are a lot of different heuristics there

00:28:15,330 --> 00:28:17,630
which can go wrong in a lot of different ways.

00:28:19,118 --> 00:28:20,650
Recently we have an issue

00:28:20,650 --> 00:28:23,050
where in the direct reclaim,

00:28:23,050 --> 00:28:28,050
when the system went to the memory pressure,

00:28:29,320 --> 00:28:31,983
and then there was a burst of natural traffic.

00:28:33,778 --> 00:28:38,778
And what happened was one CPU which has the swap lock,

00:28:39,570 --> 00:28:42,240
was starved by the interrupt.

00:28:42,240 --> 00:28:46,330
And the other CPU cannot swap, cannot reclaim,

00:28:46,330 --> 00:28:49,090
because this CPU has the lock,

00:28:49,090 --> 00:28:52,090
while it was being starved by the interrupts.

00:28:52,090 --> 00:28:55,280
So there are, like I said,

00:28:55,280 --> 00:28:56,645
in the memory reclaim,

00:28:56,645 --> 00:28:57,600
there are a lot of heuristics,

00:28:57,600 --> 00:29:02,580
and there are a lot of different subsystems involved there.

00:29:02,580 --> 00:29:07,000
Of course there is LRU user pages of disk pages,

00:29:07,000 --> 00:29:10,760
and then there are kernel memory shrinkers.

00:29:10,760 --> 00:29:14,050
And there are file systems like XFS.

00:29:14,050 --> 00:29:18,482
In the shrinker, it synchronously dump the pages

00:29:18,482 --> 00:29:23,290
like block, and dump the pages to the disk,

00:29:23,290 --> 00:29:26,990
which basically it is blocking all the reclaimers

00:29:26,990 --> 00:29:31,360
which want to look at memory for a long time.

00:29:31,360 --> 00:29:33,230
So when you go in the direct reclaim,

00:29:33,230 --> 00:29:36,770
there is no guarantee of isolation.

00:29:36,770 --> 00:29:40,640
And if you are running multiple workloads,

00:29:40,640 --> 00:29:42,290
going the direct reclaim

00:29:42,290 --> 00:29:45,517
will destroy the performance isolation.

00:29:49,190 --> 00:29:51,370
- And on Android's side,

00:29:51,370 --> 00:29:55,830
more specifically on process_madvise implementation side,

00:29:55,830 --> 00:30:00,220
we have a couple of still open questions.

00:30:00,220 --> 00:30:05,173
One of them is, do we need to vectorize the process_madvise?

00:30:06,560 --> 00:30:09,483
So when you issue such madvise,

00:30:11,860 --> 00:30:15,590
do we need ability to specify multiple VMAs at once?

00:30:15,590 --> 00:30:20,510
Or are we okay with calling this syscall multiple times

00:30:20,510 --> 00:30:22,250
for each VMA,

00:30:22,250 --> 00:30:25,680
which, depending on number of VMAs you are operating on,

00:30:25,680 --> 00:30:27,670
might be costly just because

00:30:27,670 --> 00:30:30,510
syscalls are costly by themselves.

00:30:30,510 --> 00:30:32,930
And the second and more important question is

00:30:32,930 --> 00:30:35,533
that there is a slight race over there.

00:30:39,000 --> 00:30:40,283
Let's say we want to reclaim one of the VMAs

00:30:40,283 --> 00:30:43,000
over a different process.

00:30:43,000 --> 00:30:46,400
We need to read its mmaps, the proc maps,

00:30:46,400 --> 00:30:49,794
and then call the processes madvise

00:30:49,794 --> 00:30:53,200
with the VMAs that we found out that we want to reclaim.

00:30:53,200 --> 00:30:54,270
So there is a race

00:30:54,270 --> 00:30:57,490
because between reading the maps

00:30:57,490 --> 00:31:00,500
and calling process_madvise,

00:31:00,500 --> 00:31:02,940
the VMAs might change.

00:31:02,940 --> 00:31:04,720
So you might end up hinting

00:31:04,720 --> 00:31:06,890
that some VMA is not needed,

00:31:06,890 --> 00:31:10,803
but by that time, that VMA might be a different VMA.

00:31:14,620 --> 00:31:16,530
Two options that we see is,

00:31:16,530 --> 00:31:20,770
one, because those hints are non-destructive,

00:31:20,770 --> 00:31:23,870
if we make a mistake, the worse that can happen,

00:31:23,870 --> 00:31:27,070
we will have to page in some of the memory back.

00:31:27,070 --> 00:31:28,700
So it's non-destructive.

00:31:28,700 --> 00:31:33,180
So second option is to come up with additional syscall,

00:31:33,180 --> 00:31:35,820
which would basically take a snapshot ID

00:31:37,300 --> 00:31:41,580
of the other processes memory.

00:31:41,580 --> 00:31:44,840
So let's say we associate the counter

00:31:44,840 --> 00:31:47,480
with the MM of that process.

00:31:47,480 --> 00:31:49,623
And every time we call this function,

00:31:51,630 --> 00:31:54,410
it will return as that counter.

00:31:54,410 --> 00:31:59,410
But if MM changes, let's say VMA mapping changes there,

00:31:59,530 --> 00:32:01,530
then that counter increments.

00:32:01,530 --> 00:32:05,390
And we provide that snapshot ID with the process_madvise.

00:32:05,390 --> 00:32:08,853
And between those two events,

00:32:09,990 --> 00:32:11,100
the counter changed,

00:32:11,100 --> 00:32:13,363
then the process_madvise returns,

00:32:14,750 --> 00:32:16,890
the information you provide is stale,

00:32:16,890 --> 00:32:20,480
your snapshot ID or cookie is stale.

00:32:20,480 --> 00:32:22,853
So you need to redo same thing.

00:32:25,400 --> 00:32:26,950
We will be detecting changes

00:32:26,950 --> 00:32:28,260
between those two events

00:32:28,260 --> 00:32:30,660
when we read the VMA map

00:32:30,660 --> 00:32:34,560
and when we sent the process_madvise call

00:32:35,440 --> 00:32:39,530
to hint the kernel about some of the VMAs.

00:32:39,530 --> 00:32:41,790
Those are the two open questions.

00:32:41,790 --> 00:32:46,410
And we would love some input on this.

00:32:46,410 --> 00:32:48,263
If somebody is interested,

00:32:49,430 --> 00:32:52,343
we are open to discuss those two questions.

00:32:53,420 --> 00:32:58,230
Overall, we are leaning towards using just single VMA

00:32:58,230 --> 00:33:02,880
for the process_madvise parameters question

00:33:02,880 --> 00:33:07,880
because with the vectorized parameters,

00:33:10,093 --> 00:33:12,530
there's a question of how do we handle errors.

00:33:12,530 --> 00:33:17,050
So let's say we are hinting about five different VMAs,

00:33:17,050 --> 00:33:20,463
and on the second one, we encountered some error.

00:33:21,590 --> 00:33:22,950
What do we do with the previous one?

00:33:22,950 --> 00:33:24,573
Do we roll back what we did?

00:33:26,200 --> 00:33:28,690
What do we do with the rest of the VMAs?

00:33:28,690 --> 00:33:32,570
So it's kind of a difficult question to answer

00:33:32,570 --> 00:33:34,890
because in some cases we want one way,

00:33:34,890 --> 00:33:36,980
in the other case, we want another way.

00:33:36,980 --> 00:33:41,400
So just a single VMA would be

00:33:42,580 --> 00:33:43,750
much easier to handle

00:33:43,750 --> 00:33:45,970
and simple to implement in the kernel.

00:33:45,970 --> 00:33:48,330
And from kernel maintainers,

00:33:48,330 --> 00:33:50,220
we also got this question,

00:33:50,220 --> 00:33:52,753
why process_madvise should be fast.

00:33:55,200 --> 00:34:00,200
Syscalls usually are not expected to be fast.

00:34:00,300 --> 00:34:03,950
So why do we need a vectorized version?

00:34:03,950 --> 00:34:06,920
We understand that you might need to call it multiple times,

00:34:06,920 --> 00:34:09,140
but it's a syscall.

00:34:09,140 --> 00:34:11,093
That's what you have to expect.

00:34:12,040 --> 00:34:13,700
And on the second question,

00:34:13,700 --> 00:34:15,370
we are still debating

00:34:15,370 --> 00:34:16,430
whether we want to go

00:34:16,430 --> 00:34:20,300
with a complicated snapshot ID approach

00:34:20,300 --> 00:34:23,687
or just say we allow errors.

00:34:25,314 --> 00:34:28,270
If we hinted and VMAs changed under us,

00:34:28,270 --> 00:34:29,830
it's non-destructive hint.

00:34:29,830 --> 00:34:34,130
And we will have to limit process_madvise

00:34:34,130 --> 00:34:36,420
to work with only non-destructive hints in that case.

00:34:36,420 --> 00:34:37,590
So for example,

00:34:37,590 --> 00:34:40,840
madvise free would not work with this function

00:34:40,840 --> 00:34:42,353
because it is destructive.

00:34:45,183 --> 00:34:47,417
- Ask a question, good.

00:34:47,417 --> 00:34:49,700
- So the argument that syscalls are slow

00:34:49,700 --> 00:34:52,093
and should be slow is a bullshit argument.

00:34:54,410 --> 00:34:56,560
The Linux kernel in particular has gone to great lengths,

00:34:56,560 --> 00:34:57,800
for example, poll, and select,

00:34:57,800 --> 00:34:59,670
and all sorts of networking things, contortions,

00:34:59,670 --> 00:35:01,630
to get exactly the right syscall

00:35:01,630 --> 00:35:02,940
to make it perform well.

00:35:02,940 --> 00:35:04,639
And you should do the same here.

00:35:04,639 --> 00:35:05,472
Because if you're gonna be calling it

00:35:05,472 --> 00:35:06,630
for five full gigabyte,

00:35:06,630 --> 00:35:10,730
repeatedly running through several million VMAs.

00:35:10,730 --> 00:35:12,720
You're not gonna want to do a syscall for each one of them.

00:35:12,720 --> 00:35:14,520
So I don't buy that argument at all.

00:35:16,759 --> 00:35:18,720
The usage for Chrome OS and for Android,

00:35:18,720 --> 00:35:22,700
where we have 16 gigabytes or less of memory typically,

00:35:22,700 --> 00:35:24,790
is gonna be very different than from the data center

00:35:24,790 --> 00:35:25,780
where you have five full gigabytes.

00:35:25,780 --> 00:35:27,040
So the need is gonna be different.

00:35:27,040 --> 00:35:28,440
So maybe you could have both

00:35:29,734 --> 00:35:34,734
or figure out a way to make it be called

00:35:34,780 --> 00:35:36,300
in a way that is efficient,

00:35:36,300 --> 00:35:37,890
because on the Android and Chrome OS side,

00:35:37,890 --> 00:35:40,810
we care about the power use not as much as the syscall time.

00:35:40,810 --> 00:35:42,590
So if the CPU is in a low-power state,

00:35:42,590 --> 00:35:43,830
and it's running slow,

00:35:43,830 --> 00:35:45,040
and it's not very expensive to do,

00:35:45,040 --> 00:35:46,640
then we don't really care how long it takes.

00:35:46,640 --> 00:35:48,370
We just care that it gets done.

00:35:48,370 --> 00:35:49,203
But in the data center,

00:35:49,203 --> 00:35:50,560
you might care a lot more about long it takes

00:35:50,560 --> 00:35:52,110
'cause you want to get done

00:35:52,110 --> 00:35:53,910
and then you can iterate later again.

00:35:53,910 --> 00:35:55,993
So it's very different requirements.

00:35:58,720 --> 00:36:00,260
- For us, it wouldn't be a problem.

00:36:00,260 --> 00:36:02,943
But we know that we designing not just for Android.

00:36:05,270 --> 00:36:07,350
If we want this to go upstream,

00:36:07,350 --> 00:36:10,353
we have to think about many more use cases.

00:36:11,196 --> 00:36:13,060
So that's why we are opening this for discussion.

00:36:13,060 --> 00:36:14,660
- I think for Android and Chrome OS,

00:36:14,660 --> 00:36:16,830
using what the data center wants is fine.

00:36:16,830 --> 00:36:19,130
I don't see a conflict there at all.

00:36:19,130 --> 00:36:19,963
I'm just pointing out

00:36:19,963 --> 00:36:22,533
that the base level is gonna be different.

00:36:24,150 --> 00:36:25,510
- Okay, thank you.

00:36:33,573 --> 00:36:37,130
- Here we are using zram and zswap in two cases.

00:36:37,130 --> 00:36:38,270
What are the main differences

00:36:38,270 --> 00:36:40,800
why we don't use the same thing both places?

00:36:40,800 --> 00:36:44,263
Is there a reason to prefer one for one particular device?

00:36:47,050 --> 00:36:51,580
- We started using zswap a couple of years back,

00:36:51,580 --> 00:36:52,873
five years back.

00:36:55,420 --> 00:36:59,540
One difference I know is,

00:36:59,540 --> 00:37:02,000
zswap is the front swap device

00:37:02,000 --> 00:37:05,000
where a page can be rejected.

00:37:05,000 --> 00:37:10,000
And in terms of implementation, it's easier.

00:37:10,420 --> 00:37:13,420
We don't want to swap out the incompressible pages

00:37:13,420 --> 00:37:16,740
because it will even in case of fragmentation,

00:37:20,435 --> 00:37:22,290
and reduce the memory utilization even.

00:37:22,290 --> 00:37:27,290
And I think for the Android reason,

00:37:27,880 --> 00:37:30,043
the zram is mainly the maintainer.

00:37:30,953 --> 00:37:33,840
- It's just historical, basically.

00:37:33,840 --> 00:37:38,160
- Not particular any technical reason.

00:37:38,160 --> 00:37:40,100
- But do they do anything differently

00:37:40,100 --> 00:37:43,213
in terms of if you switch one to another, all the same?

00:37:44,215 --> 00:37:48,023
- There is one difference, but I think it can be fixed.

00:37:49,650 --> 00:37:50,880
- I have another question,

00:37:50,880 --> 00:37:53,750
when there's the memory pressure to Android going to zram,

00:37:53,750 --> 00:37:57,160
or they don't do anything in a memory pressure situation?

00:37:57,160 --> 00:37:58,883
It's only for proactive stuff?

00:37:59,760 --> 00:38:01,656
- Once there's a memory pressure situation,

00:38:01,656 --> 00:38:02,489
zram is basically all swaps.

00:38:02,489 --> 00:38:07,053
So there's anonymous pages will be swapped out into zram.

00:38:11,690 --> 00:38:13,150
What else?

00:38:13,150 --> 00:38:15,330
Oh, proactive reclaim also uses basically

00:38:15,330 --> 00:38:19,420
when we hint, that some memory is not needed,

00:38:19,420 --> 00:38:21,340
basically madvise pageout,

00:38:21,340 --> 00:38:25,090
mostly used for swapping anonymous pages.

00:38:25,090 --> 00:38:28,143
So that's when zram comes to life.

00:38:29,940 --> 00:38:34,940
And that's the main usage for madvise pageout right now.

00:38:36,170 --> 00:38:38,380
We don't use that for file back pages

00:38:38,380 --> 00:38:40,070
for the reasons I outlined,

00:38:40,070 --> 00:38:41,730
that it's just too costly.

00:38:41,730 --> 00:38:44,920
If we make a mistake and we page out,

00:38:44,920 --> 00:38:48,073
file back pages, and bringing them back is much more costly

00:38:48,073 --> 00:38:52,113
than just decompressing them from zram.

00:38:56,017 --> 00:38:58,630
- [Audience Member] So when you're looking at proc pid maps,

00:38:58,630 --> 00:39:02,350
and then figuring out what process_madvise to invoke,

00:39:02,350 --> 00:39:04,140
what is your criteria there?

00:39:04,140 --> 00:39:07,660
Is it just like you're looking for anonymous VMAs?

00:39:07,660 --> 00:39:12,660
- In our case, we know that this process is in background.

00:39:13,070 --> 00:39:14,370
It has been in background.

00:39:15,293 --> 00:39:16,240
And it's not likely to be used.

00:39:16,240 --> 00:39:19,520
So we scan for anonymous VMAs

00:39:19,520 --> 00:39:22,700
and we give the pageout hint

00:39:22,700 --> 00:39:24,970
and for file back VMAs,

00:39:24,970 --> 00:39:27,800
we just give the madvise calls

00:39:31,933 --> 00:39:35,490
so that they are moved into the inactive LRU

00:39:35,490 --> 00:39:37,823
and reclaimed earlier if needed.

00:39:38,680 --> 00:39:40,562
So we don't force,

00:39:40,562 --> 00:39:42,548
just to be clear,

00:39:42,548 --> 00:39:45,750
m_advise call does not force those pages out.

00:39:45,750 --> 00:39:48,460
It just moves them into inactive LRU

00:39:48,460 --> 00:39:50,310
so that they can be reclaimed easier.

00:39:52,770 --> 00:39:55,530
- [Audience Member] Do you not have to worry about

00:39:55,530 --> 00:39:57,290
that only do it if that's the only mapping

00:39:57,290 --> 00:39:58,903
to them for called?

00:39:58,903 --> 00:40:01,743
- Yes, it does that only if the page is private.

00:40:08,240 --> 00:40:09,353
- [Moderator] Any more questions?

00:40:12,141 --> 00:40:14,213
Then thank you, Suren and Shakeel.

00:40:14,213 --> 00:40:15,057
- Thank you.

00:40:15,057 --> 00:40:16,845

YouTube URL: https://www.youtube.com/watch?v=GvBZihda9R0


