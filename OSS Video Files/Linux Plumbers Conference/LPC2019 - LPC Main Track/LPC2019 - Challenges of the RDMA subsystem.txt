Title: LPC2019 - Challenges of the RDMA subsystem
Publication date: 2019-11-18
Playlist: LPC2019 - LPC Main Track
Description: 
	Challenges of the RDMA subsystem

Speaker
Mr Jason Gunthorpe (Mellanox Technologies)
Description
The RDMA subsystem in Linux (drivers/infiniband) is now becoming widely used and deployed outside its traditional use case of HPC. This wider deployment is creating demand for new interactions with the rest of the kernel and many of these topics are challenging. 

This talk will include a brief overview of RDMA technology followed by an examination & discussion of the main areas where the subsystem has presented challenges in Linux: 

Very complex user API. An overview of the current design, and some reflection on historical poor choices 

The DMA from user space programming model and the challenge matching that to the DMA API in Linux 

Development of user space drivers along with kernel drivers 

Delegation of security decisions to HW 

Interaction with file systems, DAX, and the page cache for long term DMA 

Inter-operation with GPU, DMABUF, VFIO and other direct DMA subsystems 

Growing breadth of networking functionality and overlap with netdev, virtio, and nvme 

Fragmentation of wire protocols and resulting HW designs 

Placing high performance as paramount and how this results in HW restrictions limiting the architecture and APIs of the subsystem 

The advent of new general computation acceleration hardware is seeing new drivers proposed for Linux that have many similar properties to RDMA. These emerging drivers are likely to face these same challenges and can benefit from lessons learned. 

RDMA has been a successful mini-conference at the last three LPC events, and this talk is intended to complement the proposed RDMA micro-conference this year. This longer more general topic is intended to engage people unfamiliar with the RDMA subsystem and the detailed topics that would be included in the RDMA track. 

The main goal would be to help others in the kernel community have more background for RDMA and its role when making decisions. In part this proposal is motivated by the number of times I heard the word 'RDMA' mentioned at LSF/MM.  Often as some opaque consumer of some feature. 

Jason Gunthorpe is a Sr. Principal Engineer at Mellanox and has been the co-maintainer for the RDMA subsystem for the last year and a half. He has 20 years' experience working with the Linux kernel and in RDMA and InfiniBand technologies.
Captions: 
	00:00:00,130 --> 00:00:02,430
- All right, I seem to have forgotten to put

00:00:02,430 --> 00:00:04,470
my name on there, but I'm Jason Gunthorpe,

00:00:04,470 --> 00:00:06,160
I co-maintain the RDMA subsystem

00:00:06,160 --> 00:00:07,720
with Doug here in the front row.

00:00:07,720 --> 00:00:09,800
And congratulations to all of you

00:00:09,800 --> 00:00:11,500
for completing the first day of Plumbers,

00:00:11,500 --> 00:00:12,690
jet lag not withstanding,

00:00:12,690 --> 00:00:14,970
and welcome to our session.

00:00:14,970 --> 00:00:17,050
So, this is a long presentation that

00:00:17,050 --> 00:00:18,300
I'm going to skip some slides on

00:00:18,300 --> 00:00:20,340
because it got to be too long.

00:00:20,340 --> 00:00:24,110
The whole thing is on the materials part of the website,

00:00:24,110 --> 00:00:25,140
so you can download the rest

00:00:25,140 --> 00:00:27,750
if you wanna take a look at some of the things so.

00:00:27,750 --> 00:00:30,230
I'm not here to explain what RDMA is in any detail

00:00:30,230 --> 00:00:33,010
I've only given two slides to that topic.

00:00:33,010 --> 00:00:34,700
Just so that everyone's kind of familiar,

00:00:34,700 --> 00:00:37,260
it's part of drivers/InfiniBand in the kernel

00:00:37,260 --> 00:00:40,230
which we now call RDMA for historical reasons,

00:00:40,230 --> 00:00:42,540
which we'll talk about a little later.

00:00:42,540 --> 00:00:45,930
It's not as big as the GPU subsystem for instance,

00:00:45,930 --> 00:00:48,500
that I was just in another session about,

00:00:48,500 --> 00:00:51,450
but it is pretty sizable.

00:00:51,450 --> 00:00:52,920
And it is one of the larger subsystems

00:00:52,920 --> 00:00:55,240
that goes into the Linux kernel these days.

00:00:55,240 --> 00:00:57,920
And it's been around as a concept

00:00:57,920 --> 00:00:59,620
at least since late 1990s,

00:00:59,620 --> 00:01:02,660
and has gone through a number of rounds of standardization

00:01:02,660 --> 00:01:03,920
by different standards bodies

00:01:03,920 --> 00:01:06,020
and different technologies

00:01:06,020 --> 00:01:07,380
and different mixes and things.

00:01:07,380 --> 00:01:11,780
And today, kind of the longest standing

00:01:11,780 --> 00:01:14,270
and most current would be InfiniBand.

00:01:14,270 --> 00:01:15,760
I work in RoCE,

00:01:15,760 --> 00:01:17,130
although there are lots of other things

00:01:17,130 --> 00:01:19,890
doing parts of RDMA or elements there

00:01:19,890 --> 00:01:21,973
in which again is a different slide.

00:01:23,210 --> 00:01:25,400
So, drivers/InfiniBand has become this home

00:01:25,400 --> 00:01:28,180
for all these things that are under the RDMA umbrella

00:01:28,180 --> 00:01:29,430
that actually have nothing to do with InfiniBand.

00:01:29,430 --> 00:01:32,420
InfiniBand was just the first thing that implemented

00:01:32,420 --> 00:01:34,480
something that looked like RDMA,

00:01:34,480 --> 00:01:36,040
and it's even getting broader than that.

00:01:36,040 --> 00:01:38,300
We're seeing people put anything to do

00:01:38,300 --> 00:01:40,950
with stateless networking offload

00:01:40,950 --> 00:01:42,390
into the RDMA directory,

00:01:42,390 --> 00:01:44,990
which is starting to get a little confusing.

00:01:44,990 --> 00:01:46,810
So, if none of you,

00:01:46,810 --> 00:01:49,420
if someone here has never heard of RDMA before,

00:01:49,420 --> 00:01:50,900
this is kind of where it fits in the industry,

00:01:50,900 --> 00:01:52,063
it's networking.

00:01:53,440 --> 00:01:54,880
It's kind of like TCP in that

00:01:54,880 --> 00:01:57,950
it sits up high in the networking stack

00:01:57,950 --> 00:02:00,670
and it relies intensively on hardware offload.

00:02:00,670 --> 00:02:02,170
And generally speaking,

00:02:02,170 --> 00:02:05,150
if you're interested in zero copy data transfer,

00:02:05,150 --> 00:02:07,810
or high network bandwidth, or low latency,

00:02:07,810 --> 00:02:09,210
this is the kind of technology

00:02:09,210 --> 00:02:11,700
that you're gonna be interested in using,

00:02:11,700 --> 00:02:15,203
especially if you need something like TCP, so.

00:02:16,690 --> 00:02:21,690
The general industry purpose for it is that

00:02:22,570 --> 00:02:25,410
bandwidth and latency that you can get

00:02:25,410 --> 00:02:26,820
if you bypass the kernel

00:02:27,710 --> 00:02:29,623
for your high performance networking.

00:02:30,879 --> 00:02:32,140
Ever since I've been involved with it,

00:02:32,140 --> 00:02:34,130
since the very early 2000s,

00:02:34,130 --> 00:02:37,690
it's been somewhere between 5X and 10X bandwidth win

00:02:37,690 --> 00:02:40,740
over a single stream TCP on the same system.

00:02:40,740 --> 00:02:42,680
I think today it's closer to 5X,

00:02:42,680 --> 00:02:44,300
maybe when I started it was 10X.

00:02:44,300 --> 00:02:47,000
These are numbers that are sufficiently large that

00:02:47,000 --> 00:02:50,840
if your application is needing this kind of performance,

00:02:50,840 --> 00:02:52,210
it's worthwhile to go outside of

00:02:52,210 --> 00:02:53,960
the traditional ethernet to get it.

00:02:55,250 --> 00:02:57,660
And latency just, it's the same deal,

00:02:57,660 --> 00:02:59,710
once you bypass the kernel you eliminate

00:02:59,710 --> 00:03:00,920
all that syscall overhead

00:03:00,920 --> 00:03:03,410
and things just get multiples faster.

00:03:03,410 --> 00:03:05,490
And this is actually getting worse as we go along,

00:03:05,490 --> 00:03:07,961
in time it seems the system calls.

00:03:07,961 --> 00:03:09,460
Some years, it gets faster,

00:03:09,460 --> 00:03:10,440
some years, they get slower,

00:03:10,440 --> 00:03:12,810
we're in kind of a slower year right now.

00:03:12,810 --> 00:03:14,930
And, of course size.

00:03:14,930 --> 00:03:17,420
You can have typical RDMA solutions

00:03:17,420 --> 00:03:19,270
are targeting data center scale things

00:03:19,270 --> 00:03:23,780
with tens to many, many tens of thousands of nodes

00:03:23,780 --> 00:03:25,023
in a single network.

00:03:26,610 --> 00:03:31,610
So, typical industry areas you'll see using this is HPC,

00:03:32,350 --> 00:03:34,960
which is your weather simulations chemistry,

00:03:34,960 --> 00:03:36,730
that kind of stuff.

00:03:36,730 --> 00:03:37,730
To give you a sense of scale.

00:03:37,730 --> 00:03:40,360
The top machine right now for the HPC

00:03:40,360 --> 00:03:43,550
is a machine called Summit run by the Department of Energy.

00:03:43,550 --> 00:03:48,550
It has 2.5 million power PC cores and 10 megawatts of power.

00:03:48,990 --> 00:03:51,090
I tried to contextualize this a little bit

00:03:51,090 --> 00:03:54,610
and the Internet told me that Facebook is estimated

00:03:54,610 --> 00:03:58,410
to use 60 megawatts for their entire global data centers,

00:03:58,410 --> 00:04:00,830
at least maybe of the few years ago.

00:04:00,830 --> 00:04:04,600
So this one HPC computer is a sixth of a Facebook.

00:04:04,600 --> 00:04:06,710
The one that just got ordered.

00:04:06,710 --> 00:04:07,920
It's called CORAL 2.

00:04:07,920 --> 00:04:10,940
It's estimated to be 20-30 megawatts for the single machine

00:04:10,940 --> 00:04:12,180
and they bought three of them.

00:04:12,180 --> 00:04:15,180
So the DOE will be running a Facebook worth of computers

00:04:15,180 --> 00:04:16,040
in a couple years.

00:04:16,040 --> 00:04:18,490
At least, a few years old of Facebook of course.

00:04:18,490 --> 00:04:21,193
This is enormous scale.

00:04:23,160 --> 00:04:24,270
And then we get into,

00:04:24,270 --> 00:04:25,840
sort of our traditional enterprise stuff.

00:04:25,840 --> 00:04:28,000
Where it's databases, VM clustering,

00:04:28,000 --> 00:04:29,210
all kinds of things.

00:04:29,210 --> 00:04:30,803
Linux is very strong here,

00:04:31,680 --> 00:04:33,780
but also Windows is extremely strong.

00:04:33,780 --> 00:04:35,630
Particularly when it comes to things like

00:04:35,630 --> 00:04:38,413
SMB runs over RDMA in Windows,

00:04:38,413 --> 00:04:40,200
and everything so it's a bright thing.

00:04:40,200 --> 00:04:42,690
And finally our hyperscales.

00:04:42,690 --> 00:04:44,440
They're a little more secretive about what they're doing,

00:04:44,440 --> 00:04:46,610
but I guess you can imagine.

00:04:46,610 --> 00:04:49,560
And I like RDMA in a way because it really does

00:04:49,560 --> 00:04:50,910
impact people's lives.

00:04:50,910 --> 00:04:52,743
My house is under that blue line.

00:04:53,720 --> 00:04:58,720
So, the calculation of the Hurricane Dorian track

00:04:58,930 --> 00:05:02,160
allowed a lot of people who were in harm's way to

00:05:02,160 --> 00:05:05,280
make preparations, have enough food and gas

00:05:06,120 --> 00:05:09,260
to survive without services.

00:05:09,260 --> 00:05:11,310
And this was enabled by RDMA networks

00:05:11,310 --> 00:05:12,490
on high-performance clusters

00:05:12,490 --> 00:05:15,640
run by NOAA and the Canadian Hurricane Forecasting Center

00:05:15,640 --> 00:05:17,000
and other things.

00:05:17,000 --> 00:05:20,330
This is something that's important to the world,

00:05:20,330 --> 00:05:21,670
and there are many, many examples.

00:05:21,670 --> 00:05:24,060
I just choose that one because it's personal for me.

00:05:24,060 --> 00:05:25,460
My family's fine, thank you.

00:05:28,210 --> 00:05:30,730
So, I wanted to make this presentation

00:05:30,730 --> 00:05:32,110
mainly about the rest of the slides.

00:05:32,110 --> 00:05:36,470
Which is what I see the relationship of the RDMA system,

00:05:36,470 --> 00:05:38,350
subsystem, with the rest of they kernel, kind of is.

00:05:38,350 --> 00:05:41,660
And where we follow kernel convention really well,

00:05:41,660 --> 00:05:43,770
and sometimes we're, kind of,

00:05:43,770 --> 00:05:45,350
maybe forging a new course.

00:05:45,350 --> 00:05:46,850
Maybe it's a little different.

00:05:48,860 --> 00:05:50,270
But, this is what I'm going to spend

00:05:50,270 --> 00:05:52,540
the rest of the slides quickly going over.

00:05:52,540 --> 00:05:55,320
So inside the kernel we have an in-kernel API

00:05:55,320 --> 00:05:57,760
to access the networking technology.

00:05:57,760 --> 00:05:59,910
And it's relatively straightforward.

00:05:59,910 --> 00:06:01,700
It's based on command cues and completion cues,

00:06:01,700 --> 00:06:03,260
which will be familiar to anyone

00:06:03,260 --> 00:06:04,990
that's programmed modern DMA hardware.

00:06:04,990 --> 00:06:06,663
It uses the DMA API.

00:06:07,610 --> 00:06:08,650
It's got kind of an odd thing

00:06:08,650 --> 00:06:11,260
where you have to DMA map to the local IOMMU,

00:06:11,260 --> 00:06:13,890
and then also DMA map to the NIC,

00:06:13,890 --> 00:06:15,180
the network controller.

00:06:15,180 --> 00:06:17,110
Which is not something you usually see,

00:06:17,110 --> 00:06:20,440
but why it's there will become clear as we go on.

00:06:20,440 --> 00:06:23,530
And inside the kernel I know there's lots and lots of pieces

00:06:23,530 --> 00:06:24,420
of the kernel using it.

00:06:24,420 --> 00:06:26,420
There's a ton of storage drivers,

00:06:26,420 --> 00:06:28,870
a couple of different networking drivers.

00:06:28,870 --> 00:06:30,570
It spreads everywhere.

00:06:30,570 --> 00:06:32,540
And I mean even in the last talk you saw,

00:06:32,540 --> 00:06:35,141
there were two slides that mentioned RDMA

00:06:35,141 --> 00:06:37,080
and they talk about Kernel CI.

00:06:37,080 --> 00:06:39,020
And this has been my experience recently,

00:06:39,020 --> 00:06:41,070
is that for some reason a lot of people

00:06:41,070 --> 00:06:42,860
are talking about it more in their own presentations,

00:06:42,860 --> 00:06:43,693
for their own topics,

00:06:43,693 --> 00:06:48,693
because it seems to be touching different areas, like.

00:06:49,150 --> 00:06:51,370
Testing RDMA is important,

00:06:51,370 --> 00:06:53,340
and complicated and needs real hardware.

00:06:53,340 --> 00:06:55,610
You can't do it in VM so a iSCSI solution

00:06:56,570 --> 00:06:58,420
is very very relevant.

00:06:58,420 --> 00:07:00,830
And of course, it's got a very big and exciting user API

00:07:00,830 --> 00:07:03,680
that's probably not unlike

00:07:03,680 --> 00:07:05,990
some of the other big subsystems like GPU.

00:07:05,990 --> 00:07:07,810
And because it's zero copy

00:07:07,810 --> 00:07:11,810
and because it removes the kernel from the networking path,

00:07:11,810 --> 00:07:13,240
it does all these weird things.

00:07:13,240 --> 00:07:15,900
DMA interrupts user-space security

00:07:15,900 --> 00:07:17,800
is provided by the hardware.

00:07:17,800 --> 00:07:19,470
And we even have the driver and user-space.

00:07:19,470 --> 00:07:21,340
So this is starting to look more like GPU

00:07:21,340 --> 00:07:25,943
than it is a traditional NetDev kind of driver system.

00:07:28,090 --> 00:07:30,130
And along those lines,

00:07:30,130 --> 00:07:31,810
we're seeing this crop up all across the kernel.

00:07:31,810 --> 00:07:35,000
These same kinds of concepts because it is necessary

00:07:35,000 --> 00:07:35,833
to do this if you want

00:07:35,833 --> 00:07:37,340
the highest performance of your hardware.

00:07:37,340 --> 00:07:39,510
So, GPU and RDMA were probably

00:07:39,510 --> 00:07:41,590
at the very leading-edge of this.

00:07:41,590 --> 00:07:44,530
We're seeing these special AI drivers

00:07:44,530 --> 00:07:47,163
I've seen get merged lately are also very similar.

00:07:48,820 --> 00:07:51,340
Someone was posting about making a Accelerator Framework

00:07:51,340 --> 00:07:53,310
to kind of bundle these together.

00:07:53,310 --> 00:07:55,680
VFIO is doing a lot of the same stuff,

00:07:55,680 --> 00:07:57,503
just on a very narrow focus.

00:07:59,310 --> 00:08:02,100
And I think I saw just a few hours ago,

00:08:02,100 --> 00:08:05,230
the IOMMU PCI track was talking all about

00:08:05,230 --> 00:08:07,540
this pass ID stuff for doing

00:08:09,490 --> 00:08:12,100
DMA to user-space essentially is a good use

00:08:12,100 --> 00:08:13,810
for pass ID things.

00:08:13,810 --> 00:08:15,540
Overall, we're getting better

00:08:15,540 --> 00:08:18,840
but we still have a big lack of real kernel support

00:08:18,840 --> 00:08:20,230
for this kind of subsystem.

00:08:20,230 --> 00:08:22,010
There's a lot of things that the subsystems

00:08:22,010 --> 00:08:23,030
that need these features too,

00:08:23,030 --> 00:08:24,437
kinda just sort of go hackity hack,

00:08:24,437 --> 00:08:25,730
and we've run into trouble.

00:08:25,730 --> 00:08:28,410
And we've talked about some of the trouble before

00:08:28,410 --> 00:08:29,337
and we're still talking about it,

00:08:29,337 --> 00:08:31,410
and it's not entirely solved yet.

00:08:31,410 --> 00:08:34,240
But, so I think the first challenge

00:08:34,240 --> 00:08:36,050
of running a big subsystem like RDMA

00:08:36,050 --> 00:08:38,270
is the question of what belongs in your subsystem.

00:08:38,270 --> 00:08:40,260
I mean, some subsystems maybe are very clear,

00:08:40,260 --> 00:08:42,470
if you have for instance and I2C subsystem

00:08:42,470 --> 00:08:44,180
if it's not doing I2C,

00:08:44,180 --> 00:08:45,480
it probably doesn't belong.

00:08:45,480 --> 00:08:49,480
In this case, we have kind of a really general topic

00:08:49,480 --> 00:08:54,181
of offloaded hardware accelerated kernel bypass networking.

00:08:54,181 --> 00:08:57,940
And we violate all of those topics in some of our drivers.

00:08:57,940 --> 00:08:58,773
We have, example,

00:08:58,773 --> 00:09:01,240
some drivers that do not do hardware acceleration

00:09:01,240 --> 00:09:03,810
that trapped to the kernel and do work in the kernel.

00:09:03,810 --> 00:09:04,790
We have, you know

00:09:04,790 --> 00:09:07,700
drivers that don't implement all of our APIs

00:09:07,700 --> 00:09:10,440
because their hardware doesn't support it.

00:09:10,440 --> 00:09:13,055
So, we've had the debate most recently

00:09:13,055 --> 00:09:15,440
when merging the Amazon EFA driver

00:09:15,440 --> 00:09:18,827
about what constitutes an acceptable RDMA driver

00:09:18,827 --> 00:09:22,303
and we've kind of come to some detente maybe.

00:09:23,480 --> 00:09:25,330
So far, everything has to do with networking

00:09:25,330 --> 00:09:26,880
that's kind of a important part.

00:09:26,880 --> 00:09:27,713
If you're not doing networking,

00:09:27,713 --> 00:09:29,253
you're definitely not RDMA I think.

00:09:31,760 --> 00:09:35,470
But, we've allowed people not to implement the kernel API.

00:09:35,470 --> 00:09:38,220
We've allowed people not to do the RDMA part

00:09:38,220 --> 00:09:41,200
which is the direct data placement to user-space.

00:09:41,200 --> 00:09:43,230
You can do indirect data placement to user-space

00:09:43,230 --> 00:09:45,890
and still be considered RDMA.

00:09:45,890 --> 00:09:47,960
We have two drivers in that example,

00:09:47,960 --> 00:09:49,170
and we have a couple of drivers

00:09:49,170 --> 00:09:51,260
that use the kernel to do their offloading,

00:09:51,260 --> 00:09:52,623
even though they're not,

00:09:54,860 --> 00:09:55,810
they're still RDMA.

00:09:55,810 --> 00:09:57,150
And we have two software drivers

00:09:57,150 --> 00:09:58,380
that don't have any hardware offloaded,

00:09:58,380 --> 00:10:01,090
are completely fluent in software with kernel system calls

00:10:01,090 --> 00:10:02,540
and we're still calling them RDMA because

00:10:02,540 --> 00:10:04,050
they implement the networking protocols

00:10:04,050 --> 00:10:05,220
like iWARP and InfiniBand.

00:10:05,220 --> 00:10:07,400
So, it gets a little confusing,

00:10:07,400 --> 00:10:09,760
but I think we're doing pretty good.

00:10:09,760 --> 00:10:11,680
But then things get a little strange.

00:10:11,680 --> 00:10:13,010
See, we start talking about hardware

00:10:13,010 --> 00:10:14,410
that's clearly RDMA hardware,

00:10:14,410 --> 00:10:16,160
it implements all the relevant standards,

00:10:16,160 --> 00:10:17,620
but then it does more.

00:10:17,620 --> 00:10:18,680
It's got extensions.

00:10:18,680 --> 00:10:22,040
It has all kinds of things which are not in the standard,

00:10:22,040 --> 00:10:24,550
were never even thought about when the standard was written,

00:10:24,550 --> 00:10:28,000
because the standard was written in 1996 and

00:10:28,000 --> 00:10:30,350
the word virtualization hadn't been invented yet.

00:10:30,350 --> 00:10:35,313
We didn't even have SMP really in a commodity sense, so,

00:10:36,340 --> 00:10:39,380
let alone all the stateless offloads that NetDev does.

00:10:39,380 --> 00:10:41,940
So, people have added extensions.

00:10:41,940 --> 00:10:43,693
Tons and tons of extensions.

00:10:45,420 --> 00:10:47,020
Primarily driven by performance.

00:10:47,020 --> 00:10:48,940
People want to get the greatest performance

00:10:48,940 --> 00:10:50,850
out of their network that they can get.

00:10:50,850 --> 00:10:53,130
And as we've seen that requires new things,

00:10:53,130 --> 00:10:54,510
always requires new things.

00:10:54,510 --> 00:10:57,970
NetDev is always inventing new offloads and hardware,

00:10:57,970 --> 00:10:59,693
and we have them all in RDMA too.

00:11:00,760 --> 00:11:02,810
So we have this concept of standard verbs

00:11:02,810 --> 00:11:04,413
and device specific verbs.

00:11:05,730 --> 00:11:07,090
They kind of live side-by-side.

00:11:07,090 --> 00:11:08,820
And the standard verbs are the things you can read about

00:11:08,820 --> 00:11:10,330
in this pack and the device specific ones

00:11:10,330 --> 00:11:12,670
you can talk to your device provider,

00:11:12,670 --> 00:11:13,670
learn about how they work.

00:11:13,670 --> 00:11:16,810
And kind of run this weird parallel thing going,

00:11:16,810 --> 00:11:18,680
so that we can allow people to innovate

00:11:18,680 --> 00:11:20,790
without necessarily having to attempt

00:11:20,790 --> 00:11:22,540
to standardize everything too early

00:11:22,540 --> 00:11:24,040
before it's really been baked.

00:11:26,170 --> 00:11:27,230
As far as I know,

00:11:27,230 --> 00:11:29,320
RDMA is kind of unique in this regard.

00:11:29,320 --> 00:11:32,460
All the subsystems seem to have some struggles.

00:11:32,460 --> 00:11:35,570
Daniel was talking about for GPU about his

00:11:36,760 --> 00:11:38,200
way his display pipelines work

00:11:38,200 --> 00:11:39,650
and he standardized some parts

00:11:39,650 --> 00:11:42,210
and some parts are not yet standardized I assume.

00:11:42,210 --> 00:11:44,290
So we're kind of in the same boat.

00:11:44,290 --> 00:11:46,810
It's been rare to see something get

00:11:49,030 --> 00:11:50,800
promoted to something that's standard.

00:11:50,800 --> 00:11:52,970
Most things have been staying in the device-specific,

00:11:52,970 --> 00:11:56,500
and that seems to be the direction that we're going into.

00:11:56,500 --> 00:11:58,440
So to support this device-specific stuff,

00:11:58,440 --> 00:11:59,720
it all leaks into user-space.

00:11:59,720 --> 00:12:01,560
None of the device-specific stuff is really

00:12:01,560 --> 00:12:03,540
accessible in the kernel.

00:12:03,540 --> 00:12:05,150
This is all user-space it turns out.

00:12:05,150 --> 00:12:07,410
So to expose your device-specific stuff,

00:12:07,410 --> 00:12:10,150
people have invented all kinds of exciting ways to do it.

00:12:10,150 --> 00:12:12,270
And the user API is really, really big

00:12:12,270 --> 00:12:14,250
and really complicated.

00:12:14,250 --> 00:12:15,630
There's actually three different kinds,

00:12:15,630 --> 00:12:18,400
which I think the next slides are gonna talk about.

00:12:18,400 --> 00:12:20,660
And it's gigantics.

00:12:20,660 --> 00:12:23,090
And a lot of it follow the standard,

00:12:23,090 --> 00:12:25,150
the big parts of it follow the standard.

00:12:25,150 --> 00:12:26,190
So you can read the standard

00:12:26,190 --> 00:12:28,480
and learn about the user API and try and understand it,

00:12:28,480 --> 00:12:32,090
and then about another maybe third or fourth

00:12:32,090 --> 00:12:33,860
is just driver stuff.

00:12:33,860 --> 00:12:35,967
Stuff to allow the user-space part of the driver

00:12:35,967 --> 00:12:38,070
and the kernel part of the driver to talk to each other

00:12:38,070 --> 00:12:40,823
and do what that piece of hardware needs them to do.

00:12:43,870 --> 00:12:48,450
So, everything has been kind of aged here.

00:12:48,450 --> 00:12:51,193
It's been a long road to get to where we are today.

00:12:52,908 --> 00:12:54,543
I think it was first merged about 15 years ago,

00:12:54,543 --> 00:12:58,220
2005-2004 ish right now.

00:12:58,220 --> 00:12:59,670
And a lot of things didn't exist.

00:12:59,670 --> 00:13:02,020
We didn't have user include/uapi/.

00:13:02,020 --> 00:13:04,530
So people just put their user API

00:13:04,530 --> 00:13:05,980
in the normal kernel headers.

00:13:06,841 --> 00:13:08,740
It was a big effort to get them out,

00:13:08,740 --> 00:13:10,853
and we haven't even got them all out yet.

00:13:11,920 --> 00:13:13,210
Because the kernel headers couldn't

00:13:13,210 --> 00:13:14,280
be brought into user-space.

00:13:14,280 --> 00:13:15,703
The user-space had another copy of all this stuff,

00:13:15,703 --> 00:13:18,053
that was just peculiar and different.

00:13:19,080 --> 00:13:20,430
The user-space got abandoned,

00:13:20,430 --> 00:13:22,253
Because, I dunno, because.

00:13:23,620 --> 00:13:25,910
And when people wanted to do the extensions

00:13:25,910 --> 00:13:26,743
I mentioned earlier,

00:13:26,743 --> 00:13:29,860
they couldn't fit them into the original user ABI,

00:13:29,860 --> 00:13:31,010
so they added another one.

00:13:31,010 --> 00:13:31,990
And that didn't quite work,

00:13:31,990 --> 00:13:32,950
so they added another one.

00:13:32,950 --> 00:13:34,580
And, and, and,

00:13:34,580 --> 00:13:39,173
so, 15 years later we have this compatibility mess as well.

00:13:40,150 --> 00:13:41,050
And then of course,

00:13:41,050 --> 00:13:43,670
people writing a driver aren't expecting

00:13:43,670 --> 00:13:45,880
to encounter user ABI in their driver,

00:13:45,880 --> 00:13:47,660
that's not normal in a kernel space,

00:13:47,660 --> 00:13:49,120
at least in Linux users.

00:13:49,120 --> 00:13:51,080
User ABI is usually in the core subsystem.

00:13:51,080 --> 00:13:52,870
So we often see people working on drivers

00:13:52,870 --> 00:13:55,650
that don't quite grasp that they're doing user ABI stuff

00:13:55,650 --> 00:13:58,890
and they kinda accidentally stumble into mistakes

00:13:58,890 --> 00:14:00,180
that they shouldn't be making

00:14:00,180 --> 00:14:02,750
and we try and keep the lid on that.

00:14:02,750 --> 00:14:07,140
But it's very difficult and we don't have enough reviewers.

00:14:07,140 --> 00:14:11,160
So here's a quick example of how some of our stuff evolved

00:14:11,160 --> 00:14:14,923
and how it's seems to just be very strange.

00:14:14,923 --> 00:14:17,310
In 2005 when this all got started,

00:14:17,310 --> 00:14:19,850
the original patches for RDMA were done with I/O control

00:14:19,850 --> 00:14:21,180
like you would expect.

00:14:21,180 --> 00:14:24,960
To call up to the cert kernel and get a response back.

00:14:24,960 --> 00:14:26,930
And at that time NetLink was just kind of

00:14:26,930 --> 00:14:30,010
getting started I think and people were sort of

00:14:30,010 --> 00:14:32,440
of the opinion that I/O control was not to be used,

00:14:32,440 --> 00:14:35,590
so you should use a write interface on the file descriptor.

00:14:35,590 --> 00:14:36,520
This is much better.

00:14:36,520 --> 00:14:38,410
What they really meant is you should use a write interface

00:14:38,410 --> 00:14:40,420
and then a read interface to get your answer back.

00:14:40,420 --> 00:14:41,930
That's not what RDMA did,

00:14:41,930 --> 00:14:43,330
it just did write.

00:14:43,330 --> 00:14:47,110
And then the kernel would copy to a user back,

00:14:47,110 --> 00:14:49,950
to a pointer that the write had in it.

00:14:49,950 --> 00:14:52,590
And this, somebody pointed out eventually,

00:14:52,590 --> 00:14:54,293
was a really bad idea.

00:14:55,960 --> 00:14:57,380
Really, really bad idea.

00:14:57,380 --> 00:14:59,683
To the point that Linux required,

00:15:01,080 --> 00:15:04,200
well he required that we disabled our interface.

00:15:04,200 --> 00:15:05,360
Our user interface.

00:15:05,360 --> 00:15:08,260
Because of how incredibly insecure it was, so we

00:15:09,650 --> 00:15:12,630
compromised and we made it so that if you call fork,

00:15:12,630 --> 00:15:14,940
or pass the file descriptor to another process

00:15:14,940 --> 00:15:16,010
that it's disabled.

00:15:16,010 --> 00:15:17,250
So you could use it in one process,

00:15:17,250 --> 00:15:18,360
but you couldn't use it in another.

00:15:18,360 --> 00:15:20,380
Which contained the security problem.

00:15:20,380 --> 00:15:21,750
But this was an issue,

00:15:21,750 --> 00:15:23,780
so we had to develop a new interface.

00:15:23,780 --> 00:15:25,600
So then we had, you know, version one, version two

00:15:25,600 --> 00:15:28,520
and the version three that used IOCTL existing concurrently

00:15:28,520 --> 00:15:31,170
and all covering a different portion of the user API.

00:15:31,170 --> 00:15:33,640
It was a complete, complete disaster.

00:15:33,640 --> 00:15:36,790
And now, 2018, it finally all got fixed

00:15:36,790 --> 00:15:40,500
and IOCTL is the one user API to rule them all.

00:15:40,500 --> 00:15:42,700
I could give an entire presentation on this slide,

00:15:42,700 --> 00:15:45,120
I'm just gonna put this slide up.

00:15:45,120 --> 00:15:46,820
If you're kind of curious about this

00:15:46,820 --> 00:15:48,590
and you maybe have a use for this kind of stuff,

00:15:48,590 --> 00:15:49,580
come send me a note,

00:15:49,580 --> 00:15:50,413
come talk to me.

00:15:51,360 --> 00:15:53,510
Our IOCTL interface was designed

00:15:53,510 --> 00:15:55,710
around the similar ideas to NetLink.

00:15:55,710 --> 00:15:56,950
So instead of using a struct,

00:15:56,950 --> 00:16:01,950
you have an array of integers that describe a value,

00:16:02,540 --> 00:16:03,630
a variable sized value.

00:16:03,630 --> 00:16:06,270
So you can build up a function call,

00:16:06,270 --> 00:16:09,440
using parameters that are described in a very loose way,

00:16:09,440 --> 00:16:12,510
so that you don't have problems if you wanna add more,

00:16:12,510 --> 00:16:14,610
or make them bigger, or change them.

00:16:14,610 --> 00:16:16,580
Semantics, you have a lot of tools

00:16:16,580 --> 00:16:19,040
to manage compatibility going forward.

00:16:19,040 --> 00:16:21,260
And it's supported with some crazy,

00:16:21,260 --> 00:16:23,600
I didn't make this but,

00:16:23,600 --> 00:16:27,830
this crazy macro language in the pre-processor

00:16:27,830 --> 00:16:30,830
that's kind of reminiscent of what was done for TracePoints.

00:16:31,710 --> 00:16:33,767
We describe it, I dunno,

00:16:33,767 --> 00:16:34,830
it kind of reminds me of IDL.

00:16:34,830 --> 00:16:37,793
Although I think that would be a naughty word in this crowd.

00:16:39,870 --> 00:16:41,394
So, yeah, like I said,

00:16:41,394 --> 00:16:44,230
I could give an entire presentation on how that stuff works.

00:16:44,230 --> 00:16:46,300
I think it's turned out well if you have this kind of need,

00:16:46,300 --> 00:16:47,957
where you have a lot of stuff in user-space

00:16:47,957 --> 00:16:50,400
and the kernel that needs to be structured

00:16:50,400 --> 00:16:52,750
but is inherently dynamic in its,

00:16:52,750 --> 00:16:55,560
everybody is always changing it and drivers are changing it.

00:16:55,560 --> 00:16:57,160
It makes it much, much more difficult

00:16:57,160 --> 00:16:59,690
for drivers to commit a mistake that's serious

00:16:59,690 --> 00:17:01,810
and would go undetected because everything's

00:17:01,810 --> 00:17:03,193
summarized really cleanly.

00:17:06,235 --> 00:17:08,070
So like the rest of the kernel, you know,

00:17:08,070 --> 00:17:10,180
we struggle with the concept of should things be a mid-layer

00:17:10,180 --> 00:17:12,000
where you're sitting between your function calls,

00:17:12,000 --> 00:17:13,957
or should they be a toolbox where you call out to the driver

00:17:13,957 --> 00:17:15,670
and the driver constructs what it wants

00:17:15,670 --> 00:17:18,113
by calling back into the core code.

00:17:19,558 --> 00:17:21,440
We've got examples of both right now.

00:17:21,440 --> 00:17:22,720
And a lot of the newer stuff

00:17:22,720 --> 00:17:24,530
is moving toward this toolbox idea,

00:17:24,530 --> 00:17:26,740
supported by the previous slide.

00:17:26,740 --> 00:17:28,240
The neat thing about the previous slide,

00:17:28,240 --> 00:17:31,280
is that it lets the drivers declare their own functions

00:17:32,561 --> 00:17:34,280
in the IOCTL interface,

00:17:34,280 --> 00:17:37,940
which is really important if your drivers are

00:17:37,940 --> 00:17:40,650
providing extensions and other unique capability

00:17:40,650 --> 00:17:42,690
which seems to be where we've come to

00:17:42,690 --> 00:17:44,403
in the design of our subsystem.

00:17:47,150 --> 00:17:50,880
But this usually brings the first reaction

00:17:50,880 --> 00:17:52,590
when you explain this to someone in a lot of details.

00:17:52,590 --> 00:17:53,547
They're like, "Well what about security?

00:17:53,547 --> 00:17:56,087
"You've now delegated a lot of decisions to the driver,

00:17:56,087 --> 00:17:58,127
"the driver's talking directly to user-space.

00:17:58,127 --> 00:18:00,050
"How do you know that that driver is secure?"

00:18:00,050 --> 00:18:01,700
And it's actually worse in our case,

00:18:01,700 --> 00:18:02,820
because not only do we have to worry

00:18:02,820 --> 00:18:04,140
if the driver is secure,

00:18:04,140 --> 00:18:05,410
we have to worry if the hardware

00:18:05,410 --> 00:18:06,670
behind the driver is secure.

00:18:06,670 --> 00:18:09,500
Because remember, RDMA is all about

00:18:09,500 --> 00:18:11,440
removing the kernel from the I/O path.

00:18:11,440 --> 00:18:15,000
So, my user-space writes a command buffer in user-space,

00:18:15,000 --> 00:18:17,380
and my hardware DMAs, that command buffer,

00:18:17,380 --> 00:18:19,670
the kernel never touches the command buffer,

00:18:19,670 --> 00:18:21,810
it never gets in the way of that DMA.

00:18:21,810 --> 00:18:23,070
So there's no possibility

00:18:23,070 --> 00:18:25,850
for the kernel to provide security.

00:18:25,850 --> 00:18:28,410
Thus, we have a model where the hardware

00:18:28,410 --> 00:18:31,823
has to provide security to user-space.

00:18:32,680 --> 00:18:34,720
And this is baked into the specifications.

00:18:34,720 --> 00:18:37,360
The InfiniBand specification was designed with this in mind,

00:18:37,360 --> 00:18:39,540
with, like, a full understanding of operating systems

00:18:39,540 --> 00:18:41,960
and I think they did a pretty good job.

00:18:41,960 --> 00:18:44,950
So there's hardware objects for user-space processes

00:18:44,950 --> 00:18:48,450
that contain their security and things makes sense.

00:18:48,450 --> 00:18:50,820
It does require that every device essentially implement

00:18:50,820 --> 00:18:54,010
an IOMMU on the device, which is okay.

00:18:54,010 --> 00:18:55,540
It's still okay, but hopefully

00:18:56,610 --> 00:18:57,820
you know maybe we'll see that migrate

00:18:57,820 --> 00:18:59,070
to use something like Path-ID,

00:18:59,070 --> 00:19:01,250
so the IOMMU is only in the processor.

00:19:01,250 --> 00:19:02,940
But today it's in the device.

00:19:02,940 --> 00:19:05,623
The devices generally do a pretty good job.

00:19:07,073 --> 00:19:08,840
Things get a little more murky when you talk about ethernet.

00:19:08,840 --> 00:19:10,650
Because ethernet has

00:19:10,650 --> 00:19:12,800
a different security model and the ethernet

00:19:14,330 --> 00:19:16,440
piece of the specification was kinda bolted on

00:19:16,440 --> 00:19:17,303
toward the end.

00:19:20,730 --> 00:19:23,290
I call it sort of an unofficial standard expectation

00:19:23,290 --> 00:19:26,000
of an RDMA driver is that it respects all of the

00:19:26,000 --> 00:19:28,450
Cap Net things that ethernet defines.

00:19:28,450 --> 00:19:31,340
So, if you're in user-space and you don't have CAP_NET_RAW,

00:19:31,340 --> 00:19:33,110
you certainly can't form your own packets.

00:19:33,110 --> 00:19:34,600
You can form your own pay logs,

00:19:34,600 --> 00:19:36,360
but you can't create your own headers,

00:19:36,360 --> 00:19:38,860
can't use the broadcast asterisk, that kind of thing.

00:19:38,860 --> 00:19:41,100
This is proving to be a lot more difficult to enforce

00:19:41,100 --> 00:19:44,730
reliably as there's no real neutral standard

00:19:44,730 --> 00:19:46,260
describing all of these requirements

00:19:46,260 --> 00:19:48,210
and the people implementing the hardware

00:19:48,210 --> 00:19:49,710
don't seem to know about them.

00:19:51,690 --> 00:19:53,960
I'm not completely sure what to do about this.

00:19:53,960 --> 00:19:58,240
As there doesn't seem to be a lot of industry

00:19:58,240 --> 00:20:01,660
energy to define these things at the standardization body,

00:20:01,660 --> 00:20:03,853
since nobody's complaining.

00:20:05,000 --> 00:20:06,173
But I think it does.

00:20:07,090 --> 00:20:11,080
In this slide I've spent a lot of time thinking about, so,

00:20:11,080 --> 00:20:14,340
in the classic sense, like a NetDev driver

00:20:14,340 --> 00:20:16,910
used to be a good example with things like AFX DP,

00:20:16,910 --> 00:20:17,900
it's moving away from this.

00:20:17,900 --> 00:20:20,320
But in the classic kernel driver,

00:20:20,320 --> 00:20:22,050
if you read an operating systems textbook

00:20:22,050 --> 00:20:24,260
you'll see that the driver really just exists

00:20:24,260 --> 00:20:25,140
inside the kernel.

00:20:25,140 --> 00:20:28,110
It talks to the hardware and maybe the kernel

00:20:28,110 --> 00:20:31,350
provides an abstract POSIX API towards user space.

00:20:31,350 --> 00:20:33,670
And in no case does user space ever

00:20:33,670 --> 00:20:34,790
touch the hardware directly.

00:20:34,790 --> 00:20:38,060
It's kept contained, the kernel provides virtualization

00:20:38,060 --> 00:20:40,546
and resource sharing and security.

00:20:40,546 --> 00:20:44,350
RDMA from day zero extended this model to say that

00:20:44,350 --> 00:20:46,380
part of our driver existing user space

00:20:46,380 --> 00:20:49,960
and we do DMA directly between user space and hardware

00:20:49,960 --> 00:20:52,660
without kernel involvement, that was the entire point.

00:20:53,520 --> 00:20:57,250
And our latest generation of stuff, in RDMA

00:20:57,250 --> 00:21:00,170
takes us even further and it puts part of what you

00:21:00,170 --> 00:21:02,030
typically consider to be the driver,

00:21:02,030 --> 00:21:03,510
into the hardware itself.

00:21:03,510 --> 00:21:05,510
So we have a thin driver in the kernel

00:21:05,510 --> 00:21:08,600
that's kind of just allowing user space in the hardware

00:21:08,600 --> 00:21:11,900
to communicate in a way where it's able to enforce

00:21:11,900 --> 00:21:13,740
a security model.

00:21:13,740 --> 00:21:15,740
But we've delegated all of the security

00:21:15,740 --> 00:21:17,970
a lot of the security stuff into the hardware

00:21:17,970 --> 00:21:20,210
and we've provided a way for the hardware

00:21:20,210 --> 00:21:22,550
to identify a user process.

00:21:22,550 --> 00:21:24,530
And it's kind of interesting.

00:21:24,530 --> 00:21:26,253
We already are doing this,

00:21:27,150 --> 00:21:29,670
in the industry, for virtualization.

00:21:29,670 --> 00:21:31,590
Cause now you often have one piece of hardware

00:21:31,590 --> 00:21:33,130
that's serving multiple VMs

00:21:33,130 --> 00:21:35,640
and the hardware's asked to provide security

00:21:35,640 --> 00:21:38,250
against the different VMs and partition them.

00:21:38,250 --> 00:21:39,910
This is kind of the logical extension,

00:21:39,910 --> 00:21:41,210
instead of having one or two VMs

00:21:41,210 --> 00:21:42,650
we can have thousands of processes

00:21:42,650 --> 00:21:45,360
and the hardware provides a boundary between them

00:21:45,360 --> 00:21:48,683
that's about as strong as a boundary between VMs.

00:21:53,096 --> 00:21:55,700
So far if you really really care about security

00:21:55,700 --> 00:21:57,710
you tend not to like this.

00:21:57,710 --> 00:22:01,510
But also we've seen that any time

00:22:01,510 --> 00:22:03,350
you have anything, you can have bugs

00:22:03,350 --> 00:22:05,140
that cause security impacts.

00:22:05,140 --> 00:22:08,070
Last year saw the announcement that all of our CPUs

00:22:08,070 --> 00:22:09,403
are apparently insecure.

00:22:11,080 --> 00:22:13,810
And if you want the performance,

00:22:13,810 --> 00:22:16,340
you end up having to rely on hardware to do security.

00:22:16,340 --> 00:22:18,230
We accept this in the CPU area,

00:22:18,230 --> 00:22:21,500
or CPUs have IOMMUs, they have ASIDs.

00:22:21,500 --> 00:22:23,220
It's all the same stuff.

00:22:23,220 --> 00:22:25,080
Now it's on the NIC instead of the CPU.

00:22:25,080 --> 00:22:27,330
If your trust your NIC maybe you're okay.

00:22:27,330 --> 00:22:29,420
But it is very strange, it's very very strange

00:22:29,420 --> 00:22:31,940
that something on the other end of the PCI device

00:22:31,940 --> 00:22:34,190
is being asked to provide more security

00:22:34,190 --> 00:22:37,603
than just labeling its TLPs with the correct BDF.

00:22:38,860 --> 00:22:40,580
However, we're very lucky.

00:22:40,580 --> 00:22:42,710
RDMA has a very robust set of standards

00:22:42,710 --> 00:22:44,210
that guide the security things

00:22:44,210 --> 00:22:45,940
and as long as the hardware implementation

00:22:45,940 --> 00:22:48,420
follows the standards and dots all their i's

00:22:48,420 --> 00:22:49,640
and their conformance statements,

00:22:49,640 --> 00:22:52,030
then there's a good chance that

00:22:52,030 --> 00:22:54,053
it's at least architecturally correct.

00:22:55,260 --> 00:22:58,580
Even if it wasn't who would find the bugs,

00:22:58,580 --> 00:23:00,220
is kind of where I get to.

00:23:00,220 --> 00:23:01,510
Cause the drivers are very big,

00:23:01,510 --> 00:23:03,443
the hardware is very very complicated.

00:23:04,950 --> 00:23:09,225
But fundamentally, this is what separates RDMA from NetDev.

00:23:09,225 --> 00:23:11,620
Is RDMA has accepted hardware offloaded networking,

00:23:11,620 --> 00:23:12,720
which means we've accepted

00:23:12,720 --> 00:23:16,070
there's another processor touching our packets than our own

00:23:16,070 --> 00:23:18,863
and NetDev did not make that choice.

00:23:21,070 --> 00:23:22,184
So this got mixed into containers this year,

00:23:22,184 --> 00:23:23,870
that was a big effort.

00:23:23,870 --> 00:23:26,780
Containers were exciting because the specifications

00:23:26,780 --> 00:23:28,890
from a million years ago also didn't have

00:23:28,890 --> 00:23:29,770
the word container in them.

00:23:29,770 --> 00:23:30,750
They didn't have virtualization,

00:23:30,750 --> 00:23:32,360
they certainly didn't have container.

00:23:32,360 --> 00:23:35,543
So they had a little bit more of a difficult time.

00:23:36,948 --> 00:23:38,670
What ended up being the most successful answer

00:23:38,670 --> 00:23:41,270
is that to treat each container as a VM

00:23:41,270 --> 00:23:44,290
and the hardware, we have hardware that can already

00:23:45,630 --> 00:23:48,150
provide partitions between VMs that are very very similar

00:23:48,150 --> 00:23:50,260
to the partition you want for a container.

00:23:50,260 --> 00:23:53,630
So containers are about as strong as a VM,

00:23:53,630 --> 00:23:55,430
in RDMAs world, at least for the

00:23:55,430 --> 00:23:56,980
implementations providing them.

00:23:57,870 --> 00:24:00,630
So this is when I see Dan over there and Ira over there.

00:24:00,630 --> 00:24:02,320
This is close to their hearts,

00:24:02,320 --> 00:24:05,220
that RDMA does, DMA from user-space, hooray.

00:24:05,220 --> 00:24:07,840
And well, that doesn't work so good it turns out,

00:24:07,840 --> 00:24:09,390
because you can make the kernel crash

00:24:09,390 --> 00:24:11,563
if you breathe hard apparently.

00:24:12,838 --> 00:24:15,510
You don't wanna do this to your file back pages

00:24:15,510 --> 00:24:17,270
because you'll get into trouble.

00:24:17,270 --> 00:24:19,940
It doesn't work with DAX for a different set of reasons.

00:24:19,940 --> 00:24:22,290
There's gonna be an entire session on Wednesday

00:24:22,290 --> 00:24:24,520
in the RDMA track on Wednesday morning,

00:24:24,520 --> 00:24:25,970
talking about some of this stuff.

00:24:25,970 --> 00:24:28,080
There was a really successful session last year,

00:24:28,080 --> 00:24:30,070
I hope we can keep making progress.

00:24:30,070 --> 00:24:32,690
Hear Ira has got a really good proposal for leasing

00:24:32,690 --> 00:24:34,650
that might solve this finally.

00:24:34,650 --> 00:24:37,090
And we also wanna talk about ZONE_DEVICE

00:24:37,090 --> 00:24:38,710
because my friend's doing GPUs.

00:24:38,710 --> 00:24:40,110
They wanna work with ZONE_DEVICE pages,

00:24:40,110 --> 00:24:41,810
and we can't RDMA to those either.

00:24:43,473 --> 00:24:46,050
And a perennial argument that I have with Christophe Helwig

00:24:46,050 --> 00:24:49,830
is that RDMA and GPU and pretty much everyone doing

00:24:49,830 --> 00:24:52,740
DMA to user-space makes the assumption that the architecture

00:24:52,740 --> 00:24:54,290
is gonna be DMA cache coherent,

00:24:54,290 --> 00:24:56,060
so that we don't need something like the

00:24:56,060 --> 00:24:58,410
DMA API in the kernel.

00:24:58,410 --> 00:25:00,870
I can just, if I DMA to my page, then I can just turn around

00:25:00,870 --> 00:25:02,150
and read from that page right away

00:25:02,150 --> 00:25:03,323
and it's gonna be okay.

00:25:05,660 --> 00:25:08,270
Nobody that I've ever been aware of has tried to make

00:25:08,270 --> 00:25:10,960
something like the DMA API in user-space that can

00:25:10,960 --> 00:25:14,950
flush the data cache of the processor at the right moments.

00:25:14,950 --> 00:25:18,220
And because we didn't even start out with that concept

00:25:18,220 --> 00:25:20,170
there's tons and tons of library stack

00:25:20,170 --> 00:25:22,200
built on top of this thing that

00:25:22,200 --> 00:25:23,820
doesn't have a place to put

00:25:24,890 --> 00:25:27,600
the cache flushing, it's impossible.

00:25:27,600 --> 00:25:31,690
So, what myself and the GPU maintainers have asked for

00:25:31,690 --> 00:25:34,520
in the past is, can we just know that the architecture

00:25:34,520 --> 00:25:36,683
needs cache flushing and turn off our APIs?

00:25:38,250 --> 00:25:40,170
That has not been popular.

00:25:40,170 --> 00:25:42,750
So, if you do it wrong, you get to keep all the pieces

00:25:42,750 --> 00:25:44,150
till they don't do it wrong.

00:25:46,570 --> 00:25:49,800
The other topic, this also will be discussed at length,

00:25:49,800 --> 00:25:52,400
this Wednesday morning in the RDMA track,

00:25:52,400 --> 00:25:54,150
is DMA-ing between devices.

00:25:54,150 --> 00:25:56,950
This is something that becomes very interesting for RDMA,

00:25:56,950 --> 00:25:59,200
as it is essentially the fastest pipe

00:25:59,200 --> 00:26:00,773
into and our of your system.

00:26:02,240 --> 00:26:04,490
Pretty much, if you're going out of your system right now

00:26:04,490 --> 00:26:07,090
you're probably going on an RDMA pipe

00:26:07,090 --> 00:26:10,490
and if you wanna slosh data around your system

00:26:10,490 --> 00:26:12,400
and not have it touch system memory,

00:26:12,400 --> 00:26:16,083
you wanna do something called peer to peer transfers on PCI.

00:26:17,180 --> 00:26:19,950
We now have support for that in the kernel

00:26:19,950 --> 00:26:22,880
between certain cases with MVME devices

00:26:22,880 --> 00:26:26,140
and we'd very much like this to be extended to user space.

00:26:26,140 --> 00:26:28,140
So there's gonna be an entire track on that.

00:26:28,140 --> 00:26:31,430
This is the diagram I made to talk about it on Wednesday.

00:26:31,430 --> 00:26:34,500
Come Wednesday if you wanna know how this diagram works.

00:26:34,500 --> 00:26:36,543
I like it, it's got a nice symmetry.

00:26:39,590 --> 00:26:41,330
And of course DMA from user-space again,

00:26:41,330 --> 00:26:43,490
is one of the big challenges to know.

00:26:44,998 --> 00:26:48,940
GPUs and RDMA have this ability to do On Demand Paging.

00:26:48,940 --> 00:26:52,610
We call it in the RDMA world, the GPU people tend to call it

00:26:52,610 --> 00:26:55,710
shared virtual memory or shared virtual address space or

00:26:55,710 --> 00:26:56,543
whole bunch of other things.

00:26:56,543 --> 00:26:58,710
And I think the IOMMU people call it

00:26:58,710 --> 00:27:01,700
something else yet again, but this is the idea.

00:27:01,700 --> 00:27:05,290
That I can have a page in system memory

00:27:05,290 --> 00:27:06,270
and I can page fault it.

00:27:06,270 --> 00:27:08,550
I can put it out on swap, even though I'm DMA-ing to it.

00:27:08,550 --> 00:27:11,330
And when my DMA would like to access it again,

00:27:11,330 --> 00:27:15,180
my CPU takes it, essentially a page fault from DMA,

00:27:15,180 --> 00:27:17,680
indicating that I'd like to have that memory back.

00:27:18,520 --> 00:27:20,730
Very exciting, it makes the system very dynamic.

00:27:20,730 --> 00:27:24,560
We don't have to lock pages in memory

00:27:24,560 --> 00:27:26,250
to do network transfers for them.

00:27:26,250 --> 00:27:29,290
Everybody loves this, but it's also very difficult.

00:27:29,290 --> 00:27:31,490
So we're working with the HMM stuff

00:27:31,490 --> 00:27:34,220
to try and get HMM and RDMA and GPU using

00:27:34,220 --> 00:27:37,560
a similar set of kernel APIs to accomplish this.

00:27:37,560 --> 00:27:39,066
I think we'll talk about this a little bit more

00:27:39,066 --> 00:27:41,340
on Wednesday as well,

00:27:41,340 --> 00:27:42,967
but there's already systems shipping like

00:27:42,967 --> 00:27:46,170
POWER9 CAPI is already doing this

00:27:46,170 --> 00:27:49,420
stuff with hardware mediated IOMMU,

00:27:49,420 --> 00:27:52,450
and it seems to me like for a long time

00:27:52,450 --> 00:27:54,010
that's gonna be the industry direction.

00:27:54,010 --> 00:27:58,543
But it's been a long time so, maybe it'll take a while.

00:27:59,470 --> 00:28:01,990
So my last little part of this presentation is

00:28:01,990 --> 00:28:03,750
the challenges we have with NetDev.

00:28:03,750 --> 00:28:06,160
As you can see, RDMA is networking

00:28:06,160 --> 00:28:08,480
and NetDev is networking, so why do we have

00:28:08,480 --> 00:28:11,270
two things in the kernel doing networking?

00:28:11,270 --> 00:28:13,780
Especially when, this is my observation,

00:28:13,780 --> 00:28:15,890
when I was making this slide here,

00:28:15,890 --> 00:28:17,870
everything seems to end up on IP.

00:28:17,870 --> 00:28:20,620
I mean today we started out with MVME, it's a great example.

00:28:20,620 --> 00:28:23,270
We had MVME over PCIE and it made a lot of sense.

00:28:23,270 --> 00:28:24,200
Everybody was happy.

00:28:24,200 --> 00:28:26,800
Then we put it over RDMA because you always wanna access

00:28:26,800 --> 00:28:28,990
your storage over RDMA, I don't know why.

00:28:28,990 --> 00:28:31,390
It's very popular and people were mostly happy.

00:28:31,390 --> 00:28:34,430
So then we were running MVME over RDMA over IP.

00:28:34,430 --> 00:28:37,010
Okay, now we have MVME over TCP,

00:28:37,010 --> 00:28:40,360
so MVME is even more directly over IP.

00:28:40,360 --> 00:28:42,640
And some of my friends tell me that they're making

00:28:42,640 --> 00:28:45,300
computational MVME, so they're gonna have Linux

00:28:45,300 --> 00:28:47,250
running on the MVME drive,

00:28:47,250 --> 00:28:50,550
and I bet we're this far away from running IP over MVME

00:28:50,550 --> 00:28:54,010
over RDMA over IP, and that'll be fantastic.

00:28:54,010 --> 00:28:57,170
Everything becomes RDMA and IP in the end.

00:28:57,170 --> 00:28:59,510
So, why are they different

00:28:59,510 --> 00:29:00,870
when they're obviously so similar?

00:29:00,870 --> 00:29:04,410
And it's because NetDev has taken the very strong,

00:29:04,410 --> 00:29:06,130
and I think correct stand,

00:29:06,130 --> 00:29:08,660
that NetDev is going to have full visibility.

00:29:08,660 --> 00:29:11,910
So there are not so many off loads in NetDev

00:29:11,910 --> 00:29:12,810
that are stateful.

00:29:14,060 --> 00:29:16,800
They're sort of eeking into the stateful space a little bit,

00:29:16,800 --> 00:29:19,110
more than they certainly were 10 years ago.

00:29:19,110 --> 00:29:22,610
But fundamentally, there is no TCP offload engine

00:29:22,610 --> 00:29:24,480
as it's been called in NetDev.

00:29:24,480 --> 00:29:28,100
Like, you cannot terminate TCP on your networking card

00:29:28,100 --> 00:29:30,500
and feed it into a socket like you can in Windows.

00:29:30,500 --> 00:29:31,880
NetDev has said no to that,

00:29:31,880 --> 00:29:34,240
and they said no to that for security

00:29:34,240 --> 00:29:36,400
and also because it doesn't interact

00:29:36,400 --> 00:29:37,560
with the net stack at all.

00:29:37,560 --> 00:29:38,960
You lose things like net filter,

00:29:38,960 --> 00:29:41,530
you lose things like BPF inspection of your packets

00:29:41,530 --> 00:29:43,233
because they never reach the CPU.

00:29:44,900 --> 00:29:47,440
For this reason RDMA can never be part of NetDev.

00:29:47,440 --> 00:29:49,800
So it ended up as a separate subsystem

00:29:49,800 --> 00:29:52,710
and it's really, I think, distinguished from NetDev

00:29:52,710 --> 00:29:56,180
is that RDMA should have some kind of stateful offload.

00:29:56,180 --> 00:29:58,240
Where hardware has the option to implement

00:29:58,240 --> 00:30:00,930
some kind of operation, entirely on hardware

00:30:00,930 --> 00:30:02,950
and packets can never see the CPU.

00:30:02,950 --> 00:30:04,930
But it gives us this weird tension

00:30:04,930 --> 00:30:09,000
where now that RDMA is running over ethernet,

00:30:09,000 --> 00:30:11,200
we're sharing the same physical wire.

00:30:11,200 --> 00:30:14,050
Often we're now also sharing the same Mac address

00:30:14,050 --> 00:30:16,750
and we're sharing the same IP address as NetDev,

00:30:16,750 --> 00:30:18,750
cause it's the same hardware in the end.

00:30:20,520 --> 00:30:22,770
It's kinda weird, if you go through the RDMA paths,

00:30:22,770 --> 00:30:24,670
you skip all the stuff I mentioned.

00:30:24,670 --> 00:30:27,100
Bonding, NetDev, BPF, all of it gets skipped,

00:30:27,100 --> 00:30:29,100
because RDMA goes right to the hardware.

00:30:29,950 --> 00:30:31,653
But it's using the same IP.

00:30:33,800 --> 00:30:37,030
This is something that would do better to be more formalized

00:30:37,030 --> 00:30:41,123
but for now this is just sort of how it works.

00:30:43,510 --> 00:30:44,560
iWarp got really messy.

00:30:44,560 --> 00:30:47,260
Not only did NetDev not want there to be any offload,

00:30:47,260 --> 00:30:50,540
they didn't want RDMA to have offload either, so

00:30:51,590 --> 00:30:54,580
they wouldn't allow iWarp to reserve port numbers.

00:30:54,580 --> 00:30:57,980
Cause iWarp is IETF integrated with TCP

00:30:57,980 --> 00:30:59,670
when they designed the specifications.

00:30:59,670 --> 00:31:02,610
So you need to reserve a port number for the hardware to use

00:31:02,610 --> 00:31:05,610
and the NetDev compromise was that you could reserve

00:31:05,610 --> 00:31:08,460
the port number in user space, but not in the kernel.

00:31:08,460 --> 00:31:12,060
So iWarp has a damon in user space

00:31:12,060 --> 00:31:13,670
where the RDMA stack goes to user space

00:31:13,670 --> 00:31:16,667
and says "Hey, I need a port, to run my TCP on."

00:31:16,667 --> 00:31:19,810
And that user space damon seems to be responsible

00:31:19,810 --> 00:31:23,230
and everybody seems to have been happy with that.

00:31:23,230 --> 00:31:26,860
And of course we have NetDev running on top of RDMA

00:31:26,860 --> 00:31:29,590
and in both cases it's been kind of an odd fit

00:31:29,590 --> 00:31:31,650
cause IP over IB defines a new

00:31:32,540 --> 00:31:33,920
neighbor discovery protocol.

00:31:33,920 --> 00:31:36,140
It has a different LL ladder.

00:31:36,140 --> 00:31:37,737
So some of this was fit into NetDev

00:31:37,737 --> 00:31:39,420
and some of it, which was kind of left as

00:31:39,420 --> 00:31:41,870
that weird, it's that weird user of NetDev

00:31:41,870 --> 00:31:44,570
that does things differently for address resolution.

00:31:44,570 --> 00:31:47,340
You can find in the commits, you know,

00:31:47,340 --> 00:31:49,327
Dave Miller making commits to RDMA,

00:31:49,327 --> 00:31:51,210
and they're like "Ah this is stupid."

00:31:51,210 --> 00:31:53,910
So, we try and keep it all together but

00:31:57,170 --> 00:31:58,470
really the two technologies are

00:31:58,470 --> 00:31:59,500
going in different directions.

00:31:59,500 --> 00:32:01,830
But then, we come to something like DPDK

00:32:01,830 --> 00:32:03,520
and they come back and they converge again.

00:32:03,520 --> 00:32:07,040
So, DPDK, for those of you who might not know,

00:32:07,040 --> 00:32:09,180
is called the Data Plane Development Kit,

00:32:09,180 --> 00:32:12,720
and it is a user space library that you could imagine,

00:32:12,720 --> 00:32:15,270
it uses VFIO to directly talk to the NIC

00:32:15,270 --> 00:32:16,790
and obtain a lot of the things

00:32:16,790 --> 00:32:19,720
that I've been talking about here for UDP packets.

00:32:19,720 --> 00:32:24,200
It achieves zero copy, low latency, user space,

00:32:24,200 --> 00:32:25,593
you know, kernel bypass,

00:32:26,980 --> 00:32:30,240
except, this is again, overlapping with RDMA

00:32:30,240 --> 00:32:32,260
and at least for Mellanox,

00:32:32,260 --> 00:32:36,020
the DPDK driver, the PMD,

00:32:36,020 --> 00:32:39,670
is implemented on top of RDMA instead of VFIOs, so.

00:32:39,670 --> 00:32:42,180
We're using RDMA to provide ethernet services

00:32:42,180 --> 00:32:45,420
to DPDK in user space,

00:32:45,420 --> 00:32:47,590
with all the offloads and all the capabilities

00:32:47,590 --> 00:32:49,403
that you have in the kernel, so.

00:32:50,270 --> 00:32:51,650
Kind of went full circle.

00:32:51,650 --> 00:32:54,193
Even though we're taking a different path to get to the NIC.

00:32:56,220 --> 00:32:57,440
It's still providing ethernet,

00:32:57,440 --> 00:33:00,350
because at the end of the day, it's an ethernet NIC.

00:33:00,350 --> 00:33:03,500
And this almost, I think, broke the whole RDMA model

00:33:03,500 --> 00:33:05,970
because when you say you wanna do DPDK,

00:33:05,970 --> 00:33:08,480
it's immediately about ultra high performance.

00:33:08,480 --> 00:33:10,030
So, everything matters.

00:33:10,030 --> 00:33:12,560
Like the size of your cache line alignments.

00:33:12,560 --> 00:33:15,610
You wanna access every strange feature that the NIC can do.

00:33:15,610 --> 00:33:18,370
Every single stateless offload the NIC can do,

00:33:18,370 --> 00:33:21,393
and it doesn't fit with the traditional RDMA API.

00:33:23,050 --> 00:33:25,680
And then we're seeing even more where we're having,

00:33:25,680 --> 00:33:30,060
I'm hearing people proposing VFIO-MDEV as connected to RDMA

00:33:30,060 --> 00:33:32,806
which is gonna become quite interesting.

00:33:32,806 --> 00:33:35,150
I'm waiting to see those patches.

00:33:35,150 --> 00:33:39,593
But, I see, I left 10 minutes for questions.

00:33:40,530 --> 00:33:42,420
20 year anniversary for Mellanox,

00:33:42,420 --> 00:33:44,810
and this'll be the last time you hear their name

00:33:44,810 --> 00:33:46,183
at Plumbers I think, so.

00:33:47,970 --> 00:33:49,120
Look, they had a big party,

00:33:49,120 --> 00:33:50,870
they sold the company, it's okay.

00:33:50,870 --> 00:33:51,703
To that guy.

00:33:53,740 --> 00:33:56,713
All right, questions, comments, tomatoes.

00:34:00,230 --> 00:34:02,373
Leon, in the red please.

00:34:04,130 --> 00:34:05,893
Oooh, all right.

00:34:08,263 --> 00:34:10,030
- So, given all the complexity of RDMA,

00:34:10,030 --> 00:34:13,740
does it make sense to, I don't know,

00:34:13,740 --> 00:34:15,810
move to (mumbles) net site completely?

00:34:15,810 --> 00:34:16,975
- [Jason] Move to the sorry?

00:34:16,975 --> 00:34:19,620
- To (mumbles) net, like make user net fast.

00:34:19,620 --> 00:34:24,500
Yeah so, for example, one of problems we have with Mellanox,

00:34:24,500 --> 00:34:27,670
if we have one flow from one IP and one port

00:34:27,670 --> 00:34:32,670
holds the (mumbles) basically it won't be in one core.

00:34:32,800 --> 00:34:33,633
- [Jason] Okay.

00:34:33,633 --> 00:34:36,290
- So, if we would distribute, if you would have ability to

00:34:36,290 --> 00:34:40,740
distribute this flow among let's say eight or 16 cores,

00:34:40,740 --> 00:34:44,360
I will be able to saturate a link, like 100 gigabit

00:34:44,360 --> 00:34:47,080
- [Jason] So, I mean... - So we don't need RDMA

00:34:47,080 --> 00:34:49,985
to speed things up, we just need, you know,

00:34:49,985 --> 00:34:52,250
(mumbles) procession on all cores we have.

00:34:52,250 --> 00:34:54,380
- So there's a distinction here.

00:34:54,380 --> 00:34:57,210
So most of the users who want RDMA care about

00:34:57,210 --> 00:34:59,768
the TCP part of this, all right.

00:34:59,768 --> 00:35:00,601
- [Man In Red Shirt] Yes.

00:35:00,601 --> 00:35:03,360
- And furthermore, they don't want to pay the cost of TCP

00:35:03,360 --> 00:35:07,500
because every single clock cycle you spend doing TCP stuff

00:35:07,500 --> 00:35:10,180
on your supercomputer, is a clock cycle that could've

00:35:10,180 --> 00:35:12,030
been spent thinking about hurricanes.

00:35:13,640 --> 00:35:17,040
And it turns out that you can make dedicated hardware to do

00:35:17,040 --> 00:35:20,890
reliable messaging that uses a lot less power

00:35:20,890 --> 00:35:23,640
then if you do it with something like a Xeon.

00:35:23,640 --> 00:35:25,180
That's kind of the fundamental use case.

00:35:25,180 --> 00:35:29,365
- Yeah but then you have to pay 300, 3000 dollars,

00:35:29,365 --> 00:35:33,913
(mumbles) like 600, you know, it's very cost sensitive.

00:35:36,398 --> 00:35:38,250
- For the community that wants this stuff, yeah.

00:35:38,250 --> 00:35:41,640
You know, they pay the cost because it gives their ROI.

00:35:41,640 --> 00:35:43,890
For maybe what you're talking about,

00:35:43,890 --> 00:35:47,228
you're right, you could do it with packet processing.

00:35:47,228 --> 00:35:50,100
And I'm not sure what your application is but

00:35:50,100 --> 00:35:54,820
like DPDK does do per core pinning of raw ethernet flows

00:35:54,820 --> 00:35:58,920
or even higher level flows through the Mellanox PMD.

00:35:58,920 --> 00:36:01,300
So this is already possible.

00:36:01,300 --> 00:36:02,870
I don't know a lot about user nets,

00:36:02,870 --> 00:36:04,420
I can't really comment on that.

00:36:07,191 --> 00:36:10,077
Whoa, we lost their mics.

00:36:10,077 --> 00:36:11,763
- [Man With Glasses] Is there two coming at me?

00:36:13,410 --> 00:36:15,200
I can only look at one thing at a time.

00:36:15,200 --> 00:36:19,440
Um, but the other issue, or the other thing to remember is

00:36:19,440 --> 00:36:22,590
that RDMA hardware actually is quite reliable,

00:36:22,590 --> 00:36:24,890
and doesn't need to retry often.

00:36:24,890 --> 00:36:27,920
So, a lot of these protocols are built on top of that.

00:36:27,920 --> 00:36:30,960
And so because the underlying hardware is different,

00:36:30,960 --> 00:36:32,330
they get a lot of their performance.

00:36:32,330 --> 00:36:33,860
And yes, the hardware's more expensive

00:36:33,860 --> 00:36:36,290
but they get that performance without ever having to do

00:36:36,290 --> 00:36:37,900
retries or things like that.

00:36:37,900 --> 00:36:40,957
So, it's all about performance for these people.

00:36:40,957 --> 00:36:43,930
- [Man In Red Shirt] (mumbles)

00:36:43,930 --> 00:36:47,540
- Yeah, I understand, you know, so.

00:36:47,540 --> 00:36:50,490
And that's part of the reason that things like Rocky exist.

00:36:53,483 --> 00:36:55,060
- [Jason] And I think I should qualify performance.

00:36:55,060 --> 00:36:56,960
- So it comes back to the point that Jason said is

00:36:56,960 --> 00:36:59,510
people running this stuff, they don't care

00:36:59,510 --> 00:37:01,770
about doing that in software at all.

00:37:01,770 --> 00:37:03,020
They don't need it .

00:37:03,020 --> 00:37:05,420
- [Jason] All right, and just to keep everyone's in mind,

00:37:05,420 --> 00:37:07,610
the first slide, when I said the top computer right now

00:37:07,610 --> 00:37:08,673
is 10 mega watts.

00:37:09,770 --> 00:37:10,890
When we talk about performance,

00:37:10,890 --> 00:37:13,000
the number of people have really started to care about

00:37:13,000 --> 00:37:15,770
is not how fast your machine is, although that's important,

00:37:15,770 --> 00:37:17,320
it's how much power it draws.

00:37:17,320 --> 00:37:20,220
A 10 mega watt computer in a room,

00:37:20,220 --> 00:37:22,540
that's probably about this big,

00:37:22,540 --> 00:37:25,571
is a significant engineering challenge.

00:37:25,571 --> 00:37:27,323
- [Man In Red Shirt] Is it based on angle?

00:37:27,323 --> 00:37:28,990
- Yes. No wait, no that one's, sorry,

00:37:28,990 --> 00:37:30,170
that one was Power PC.

00:37:30,170 --> 00:37:32,240
That one was Power PC, the CORAL 2 one,

00:37:32,240 --> 00:37:34,310
that's gonna be 20 or 30 will be Intel.

00:37:34,310 --> 00:37:36,560
Apparently, that's what the news says anyway.

00:37:37,800 --> 00:37:39,880
- I have two questions, first one,

00:37:39,880 --> 00:37:42,100
is your home in Alabama?

00:37:42,100 --> 00:37:42,970
- [Jason] Sorry?

00:37:42,970 --> 00:37:45,320
- Is your home in Alabama?

00:37:45,320 --> 00:37:47,540
The hurricane Dorian was... - [Jason] Oh no.

00:37:47,540 --> 00:37:49,810
- ...heading that direction. - [Jason] No, fortunately,

00:37:49,810 --> 00:37:52,330
fortunately that was a misreporting.

00:37:52,330 --> 00:37:54,380
I live in Nova Scotia, so it's in Canada.

00:37:55,610 --> 00:37:58,750
You know, it went up, not West.

00:37:58,750 --> 00:38:00,130
- Use a sharpie.

00:38:00,130 --> 00:38:03,327
The second one is, you mentioned about like 10,000,

00:38:05,478 --> 00:38:07,970
is it 10,000 nodes, right?

00:38:07,970 --> 00:38:09,525
- [Jason] So when I was talking about

00:38:09,525 --> 00:38:11,743
the sense of the network. - The largest supercomputer.

00:38:13,280 --> 00:38:16,030
- Those numbers are a little bit harder to come by, but

00:38:17,360 --> 00:38:18,750
you can talk about the number of cores,

00:38:18,750 --> 00:38:19,730
the number of sockets,

00:38:19,730 --> 00:38:22,160
and then the number of network endpoints essentially,

00:38:22,160 --> 00:38:25,013
the number of nodes is usually how it's expressed, so.

00:38:27,820 --> 00:38:31,870
10,000 is probably a mid-sized computer at this point.

00:38:31,870 --> 00:38:32,703
- [Man In Blue Sweater] Okay.

00:38:32,703 --> 00:38:33,536
- But there's all kinds of ratios.

00:38:33,536 --> 00:38:34,550
- Maybe even more than that.

00:38:34,550 --> 00:38:35,700
- [Man With Glasses] But Jason, you're talking about

00:38:35,700 --> 00:38:36,750
network end points.

00:38:36,750 --> 00:38:38,577
- Yeah, well, network cards essentially.

00:38:38,577 --> 00:38:39,930
- [Man With Glasses] Right, so.

00:38:39,930 --> 00:38:43,550
Right, this is, I work for Intel now,

00:38:43,550 --> 00:38:44,930
but I used to do RDMA.

00:38:46,273 --> 00:38:48,360
(laughter)

00:38:48,360 --> 00:38:50,663
- That came out wrong, I think.

00:38:51,800 --> 00:38:53,907
I think that came out a little wrong,

00:38:53,907 --> 00:38:55,960
but I appreciate the message.

00:38:55,960 --> 00:38:59,380
- Well, I recently changed jobs, so.

00:38:59,380 --> 00:39:01,623
But, right.

00:39:02,540 --> 00:39:06,800
In HPC, people refer to nodes as the physical,

00:39:06,800 --> 00:39:08,400
usually network endpoint.

00:39:08,400 --> 00:39:09,940
Sometimes a node might have network,

00:39:09,940 --> 00:39:14,530
but that's not CP- so nodes could have 88 CPUs

00:39:14,530 --> 00:39:16,910
or hundreds of CPUs.

00:39:16,910 --> 00:39:20,850
So, 10,000 network end points is like the size of the fabric

00:39:20,850 --> 00:39:22,759
or the size of the network.

00:39:22,759 --> 00:39:25,960
- So how do you solve congestion problems?

00:39:25,960 --> 00:39:29,130
- Oh my goodness, that's like an endlessly long question, so

00:39:31,336 --> 00:39:33,120
- [Man With Glasses] I've done presentations on that

00:39:33,120 --> 00:39:35,860
and yes, that's a whole nother different story

00:39:35,860 --> 00:39:37,370
that has nothing to do with the kernel.

00:39:37,370 --> 00:39:39,560
- Yeah yeah, but - (mumbles)

00:39:39,560 --> 00:39:41,112
- Yeah, there's lots of-

00:39:41,112 --> 00:39:41,945
- [Man With Glasses] To some extent, yes.

00:39:41,945 --> 00:39:43,890
- Different solutions, but this is why it's not ethernet.

00:39:43,890 --> 00:39:46,570
Because ethernet doesn't solve that problem, at all.

00:39:46,570 --> 00:39:48,990
Really, without a lot of extensions so

00:39:48,990 --> 00:39:50,880
people said I can't do that with ethernet,

00:39:50,880 --> 00:39:52,100
I'm gonna build my own stuff.

00:39:52,100 --> 00:39:54,060
There's all kinds of examples of their own stuff.

00:39:54,060 --> 00:39:55,670
Some have been more successful than others.

00:39:55,670 --> 00:39:58,820
Some have ended, some have succeeded.

00:39:58,820 --> 00:40:01,430
But it's a very very big complicated question

00:40:01,430 --> 00:40:05,920
and the ethernet just wasn't built for this kind of stuff.

00:40:05,920 --> 00:40:10,150
Like, my favorite HPC network is the 11 dimensional

00:40:10,150 --> 00:40:11,520
hyper cube that NASA runs.

00:40:11,520 --> 00:40:12,353
This means,

00:40:14,090 --> 00:40:17,290
that their wiring was optimized for

00:40:17,290 --> 00:40:20,880
cable length, because that's important

00:40:20,880 --> 00:40:22,317
as part of their cost factor.

00:40:22,317 --> 00:40:23,947
- [Man With Glasses] Yeah, but that's not even right.

00:40:23,947 --> 00:40:24,780
- What? - [Man With Glasses] Yeah.

00:40:24,780 --> 00:40:26,267
- It's sort of right.

00:40:26,267 --> 00:40:27,510
- [Man With Glasses] No no no, that's right.

00:40:27,510 --> 00:40:30,140
But that's not even the most interesting network,

00:40:30,140 --> 00:40:31,450
but that's okay.

00:40:31,450 --> 00:40:33,827
We can debate that over drinks.

00:40:33,827 --> 00:40:35,048
- You can have your own favorite.

00:40:35,048 --> 00:40:36,850
- You can have beer and (mumbles).

00:40:36,850 --> 00:40:38,203
Any final questions?

00:40:39,600 --> 00:40:40,710
- All right, thank you everyone.

00:40:40,710 --> 00:40:43,030

YouTube URL: https://www.youtube.com/watch?v=jZ319soBxW8


