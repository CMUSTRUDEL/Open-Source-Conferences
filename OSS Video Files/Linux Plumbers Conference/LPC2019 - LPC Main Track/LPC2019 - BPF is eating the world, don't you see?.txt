Title: LPC2019 - BPF is eating the world, don't you see?
Publication date: 2019-11-18
Playlist: LPC2019 - LPC Main Track
Description: 
	BPF is eating the world, don't you see?

Speaker
 Arnaldo Carvalho de Melo (Red Hat Inc.)

Description
The BPF VM in the kernel is being used in ever more scenarios where running a restricted, validated program in kernel space provides a super powerful mix of flexibility and performance which is transforming how a kernel work.

That creates challenges for developers, sysadmins and support engineers, having tools for observing what BPF programs are doing in the system is critical.

A lot has been done recently in improving tooling such as perf and bpftool to help with that, trying to make BPF fully supported for profiling, annotating, tracing, debugging.

But not all perf tools can be used with JITed BPF programs right now, areas that need work, such as putting probes and collecting variable contents as well as further utilizing BTF for annotation are areas that require interactions with developers to gather insights for further improvements so as to have the full perf toolchest available for use with BPF programs.

These recent advances and this quest for feedback about what to do next should be the topic of this talk.
Captions: 
	00:00:00,090 --> 00:00:01,710
- My name is Arnaldo Melo.

00:00:01,710 --> 00:00:04,863
I work for Red Hat for a long time already.

00:00:07,274 --> 00:00:10,170
And I maintain the user space parts

00:00:10,170 --> 00:00:13,723
of the perf tool set in the Linux kernel.

00:00:17,180 --> 00:00:20,360
Today, I'm here to talk about

00:00:21,860 --> 00:00:24,543
how to see that BPF is being used.

00:00:26,460 --> 00:00:29,680
I'll give some examples of where it is being used,

00:00:29,680 --> 00:00:32,660
of why it's being used,

00:00:32,660 --> 00:00:34,650
but I will not get that much into depth

00:00:34,650 --> 00:00:36,350
because there are lots of other talks

00:00:36,350 --> 00:00:39,457
and there is always talks about BPF,

00:00:39,457 --> 00:00:43,300
in recent conferences, and you should expect

00:00:43,300 --> 00:00:46,120
to see even more in the future.

00:00:46,120 --> 00:00:50,563
But let's see how we can see BPF.

00:00:53,320 --> 00:00:56,663
There are lots reason why people are using BPF,

00:00:58,468 --> 00:01:01,170
but there was this worry

00:01:01,170 --> 00:01:06,080
that support organizations like the one I work for

00:01:07,131 --> 00:01:11,520
would have difficulty trouble

00:01:11,520 --> 00:01:14,893
understanding what's happening on the machines of customers,

00:01:16,770 --> 00:01:18,993
because up to some time,

00:01:20,370 --> 00:01:21,900
it was difficult to see

00:01:23,430 --> 00:01:26,850
that there was something else running on their machine

00:01:26,850 --> 00:01:31,260
than the kernel applications and modules, et-cetera

00:01:31,260 --> 00:01:33,610
if there is a new component, there's something

00:01:33,610 --> 00:01:38,610
that works in a different way, and that we need to know

00:01:38,633 --> 00:01:43,550
when it is involved, what was running it at some point,

00:01:43,550 --> 00:01:47,783
so you can try to fix things when they get broken.

00:01:49,360 --> 00:01:50,513
So what's BPF?

00:01:51,481 --> 00:01:53,830
Just the normal...

00:01:53,830 --> 00:01:54,663
set

00:01:55,610 --> 00:01:58,090
Everybody by now should know what's BPF,

00:01:58,090 --> 00:02:00,633
but there you have it.

00:02:06,400 --> 00:02:08,440
So, where is it being used?

00:02:08,440 --> 00:02:10,950
It's being used in high speed networking, firewalls,

00:02:10,950 --> 00:02:15,763
tracing, to do in-kernel metrics, flexible filtering.

00:02:16,953 --> 00:02:19,810
It's a really simple concept,

00:02:19,810 --> 00:02:24,480
you have a program that the kernel validates

00:02:24,480 --> 00:02:27,780
and that it makes sure that it is not going to crash

00:02:27,780 --> 00:02:31,720
your machine and that you can connect to several places

00:02:31,720 --> 00:02:33,750
in the kernel and add functionality

00:02:34,944 --> 00:02:39,820
when in the past, you had to do normal kernel programming

00:02:39,820 --> 00:02:44,290
and use kernel modules, which are way more dangerous

00:02:44,290 --> 00:02:47,433
than what you can do with BPF.

00:02:49,290 --> 00:02:51,910
XDP goes on top of that

00:02:53,204 --> 00:02:55,380
and uses BPF to provide

00:02:57,494 --> 00:03:00,193
a more flexible way of handling packets.

00:03:04,831 --> 00:03:09,640
Since it works early in the packet lifetime,

00:03:09,640 --> 00:03:13,980
you can even go the extra mile and get this bytecode

00:03:13,980 --> 00:03:17,600
and offload it to hardware, so if it's even out

00:03:17,600 --> 00:03:20,353
of your main machine.

00:03:21,260 --> 00:03:26,260
There are points that are tangential to what I'm talking now

00:03:26,800 --> 00:03:30,952
which is how do you debug, let's say XDP

00:03:30,952 --> 00:03:35,952
you could want to get into the flow so there are talks.

00:03:37,200 --> 00:03:40,208
I think unfortunately, right now, talking about how to

00:03:40,208 --> 00:03:43,110
chain XDP programs...

00:03:49,260 --> 00:03:51,210
and for tracing as well.

00:03:51,210 --> 00:03:55,430
Then we are using BPF as the observer, we are not observing

00:03:55,430 --> 00:03:59,360
BPF, this talk is about observing BPF,

00:03:59,360 --> 00:04:01,633
not using BPF as the observer,

00:04:03,120 --> 00:04:07,160
but then with BPF, as you can connect

00:04:07,160 --> 00:04:10,750
in several places, you can as well, collect information

00:04:10,750 --> 00:04:15,490
and you can act on this information and you can aggregate

00:04:15,490 --> 00:04:18,610
this information in maps and then, periodically,

00:04:18,610 --> 00:04:22,670
you can dump it to user space, or you can do everything

00:04:24,800 --> 00:04:27,539
all this aggregation in the kernel and then don't

00:04:27,539 --> 00:04:31,470
traverse through user space with lots of information

00:04:31,470 --> 00:04:32,800
just to be post-processed.

00:04:32,800 --> 00:04:36,730
So you get things at the origin, but this is not

00:04:36,730 --> 00:04:39,910
what this talk is about, for knowing more about how

00:04:39,910 --> 00:04:42,910
to do that you go and read this book when it's available.

00:04:42,910 --> 00:04:44,951
- [Audience Member] Yes, thank you, thank you very much.

00:04:44,951 --> 00:04:47,410
(audience laughs)

00:04:47,410 --> 00:04:49,623
- Yeah, there are lots of things in there.

00:04:50,681 --> 00:04:55,681
For using BPF as the observer, now let's observe BPF.

00:04:58,000 --> 00:05:02,790
So I will start with an example of one thing that everybody

00:05:02,790 --> 00:05:07,170
has in their machines, or even some people not liking it.

00:05:07,170 --> 00:05:12,170
But most people have it, so systemd uses BPF for some

00:05:12,429 --> 00:05:15,630
functionality, right now it's something simple

00:05:15,630 --> 00:05:20,130
and this is stressing some boundaries that are being

00:05:20,130 --> 00:05:24,127
discussed in the kernel community about how to use BPF

00:05:24,127 --> 00:05:29,010
with unprivileged users not just with root.

00:05:29,010 --> 00:05:33,560
So this is finally making people think about the security

00:05:33,560 --> 00:05:36,180
implications of using perf and BPF

00:05:37,207 --> 00:05:42,207
as non-root user.

00:05:42,210 --> 00:05:44,481
But back to systemd.

00:05:44,481 --> 00:05:49,481
You can use BPF and systemd to do per unit IP access lists

00:05:50,870 --> 00:05:55,310
and accounting, so BPF has lots of programs

00:05:56,650 --> 00:05:59,560
those types of programs dictate where you can attach

00:05:59,560 --> 00:06:01,660
this program and what this program can do.

00:06:02,550 --> 00:06:06,353
And there are also lots of maps which get

00:06:06,353 --> 00:06:10,910
preexisting kernel data structures and make them available

00:06:10,910 --> 00:06:15,910
in a convenient way inside BPF programs and in user space.

00:06:17,270 --> 00:06:21,370
So you can use that to coordinate work among lots

00:06:21,370 --> 00:06:23,550
of BPF programs connected inside the kernel

00:06:23,550 --> 00:06:28,220
and the applications on user space.

00:06:28,220 --> 00:06:29,330
Okay so...

00:06:30,960 --> 00:06:34,950
Looking at how it does, you will start with this.

00:06:34,950 --> 00:06:36,513
This is just one use of BPF.

00:06:37,380 --> 00:06:40,393
You just systemd-run -p,

00:06:41,472 --> 00:06:43,266
you ask for IPAccounting

00:06:43,266 --> 00:06:44,240
and then you run something.

00:06:44,240 --> 00:06:49,100
It's running, and so inside this, we have a BPF program

00:06:49,100 --> 00:06:51,093
was put in place, or several programs.

00:06:53,060 --> 00:06:57,100
If you then go ahead and do some...

00:06:59,220 --> 00:07:02,209
ask for information, the status for this service

00:07:02,209 --> 00:07:05,700
you're gonna see that this line was not there before.

00:07:05,700 --> 00:07:09,250
It's related to this, so you have IP, zero bytes in

00:07:09,250 --> 00:07:11,040
zero bytes out.

00:07:11,040 --> 00:07:12,910
If you then go in...

00:07:12,910 --> 00:07:17,910
and ping send to, ping packets of 56 bytes

00:07:18,135 --> 00:07:22,500
and do the status again, you're gonna see that it accounted

00:07:22,500 --> 00:07:27,280
for that because it hooked somewhere related to containers

00:07:27,280 --> 00:07:29,010
which was that...

00:07:31,112 --> 00:07:34,710
BPF_PROG_TYPE_CGROUP_SKB

00:07:34,710 --> 00:07:38,400
and so it was able to...

00:07:39,510 --> 00:07:42,230
have access to the network traffic

00:07:42,230 --> 00:07:46,433
that's going through to and from this systemd unit.

00:07:51,020 --> 00:07:53,120
Then for blocking addresses,

00:07:53,120 --> 00:07:55,683
which is a little bit more complex.

00:07:57,162 --> 00:08:00,100
You started with this IPAddressDeny, 1.1.1.1

00:08:02,050 --> 00:08:05,240
and then when it ping to some other address

00:08:05,240 --> 00:08:06,620
it goes through, that's okay.

00:08:06,620 --> 00:08:09,283
But for that one, it's not permitted.

00:08:10,868 --> 00:08:13,763
So this data, there's something doing this.

00:08:15,536 --> 00:08:18,920
But let's see how we can see, how is that.

00:08:18,920 --> 00:08:23,510
So the program I use to see BPF, the most basic one.

00:08:23,510 --> 00:08:26,740
The most canonical one is bpftool.

00:08:26,740 --> 00:08:31,380
It has a lot of sub commands, the first one we see is prog.

00:08:31,380 --> 00:08:34,860
So bpftool prog | tail -6 because I know that

00:08:34,860 --> 00:08:37,705
the three lines of information for each of the programs

00:08:37,705 --> 00:08:40,490
and the last two ones are the ones

00:08:40,490 --> 00:08:43,340
that were put in place by systemd.

00:08:46,767 --> 00:08:51,130
As always, I can ask for maps, what are the maps

00:08:51,130 --> 00:08:54,120
that are in place while that thing is running.

00:08:54,120 --> 00:08:57,670
So there is this map, the type of the map is lpm_trie

00:08:58,940 --> 00:09:01,750
specific type of map, that's more appropriate

00:09:01,750 --> 00:09:04,815
to what systemd is wanting to do in that case.

00:09:04,815 --> 00:09:08,308
It gets information about how much memory it uses,

00:09:08,308 --> 00:09:11,630
the maximum number of entries, all of the information

00:09:11,630 --> 00:09:14,490
about the programs, the size of the bytecode.

00:09:14,490 --> 00:09:18,277
The size of the JITed code for the bytecode.

00:09:19,500 --> 00:09:21,600
You can do a map dump.

00:09:21,600 --> 00:09:26,600
So you see, I get that ID, I know that's the one.

00:09:26,720 --> 00:09:28,710
And then what is in that map?

00:09:28,710 --> 00:09:32,030
You have some information and the keys

00:09:32,030 --> 00:09:36,280
has 32 bytes and those are the...

00:09:38,332 --> 00:09:42,290
the bits and then you have the contents, one, one, one, one.

00:09:42,290 --> 00:09:45,350
That's the address I wanted to block.

00:09:45,350 --> 00:09:47,190
Then there is a value...

00:09:47,190 --> 00:09:52,190
it's how systemd says it wants to block or do something.

00:09:55,050 --> 00:09:58,933
The key, if you look at the key that is used by systemd

00:09:58,933 --> 00:10:03,933
that's used for a bpf_lpm_trie, you have the prefix length

00:10:04,617 --> 00:10:08,780
and then the data which is arbitrary.

00:10:08,780 --> 00:10:13,670
So that's how systemd puts in place programs

00:10:13,670 --> 00:10:18,670
in a map and how you can see what's in the system

00:10:19,380 --> 00:10:20,213
at that point.

00:10:22,710 --> 00:10:25,832
You can, as well, ask for dumping the bytecode

00:10:25,832 --> 00:10:28,619
for that program, here I used the tag.

00:10:28,619 --> 00:10:31,215
You have the ID which is a free running

00:10:31,215 --> 00:10:34,760
that goes on incrementing and there's the tag

00:10:34,760 --> 00:10:38,310
that's related to the content or that's generated

00:10:38,310 --> 00:10:42,263
associated with this program, so you have the bytecode.

00:10:43,970 --> 00:10:46,860
And you can have the JITed code, as well.

00:10:46,860 --> 00:10:51,003
You ask for JIT here, and in this case it's exactly 64.

00:10:53,090 --> 00:10:53,923
Okay.

00:10:55,693 --> 00:10:58,860
So that already allows us

00:10:58,860 --> 00:11:03,860
to see what's there once at a point in time.

00:11:05,460 --> 00:11:06,710
- Yeah just a question.

00:11:06,710 --> 00:11:10,097
How do you get the tag of a BPF program?

00:11:10,097 --> 00:11:12,557
Like the one that you showed before?

00:11:17,187 --> 00:11:19,790
- It's here and here.

00:11:19,790 --> 00:11:21,733
- [Audience Member] All right, cool, thank you.

00:11:24,350 --> 00:11:29,350
- So that already allows us to see what's happening

00:11:29,560 --> 00:11:31,420
at some point in time.

00:11:31,420 --> 00:11:35,568
But it would be nice if we could see more

00:11:35,568 --> 00:11:39,000
to get the kind of observability that we have

00:11:39,000 --> 00:11:44,000
with other non BPF, with the kernel or with library

00:11:44,071 --> 00:11:47,326
preexisting before the pre-BPF days.

00:11:47,326 --> 00:11:52,326
So there's BTF, BTF it started as a type form.

00:11:52,570 --> 00:11:55,810
Just as a way for you to have in a compact form

00:11:55,810 --> 00:11:59,690
the types that are used by some program, so the types

00:11:59,690 --> 00:12:04,430
for the BPF program and it would be really nice

00:12:04,430 --> 00:12:06,473
if we had the types for the kernel.

00:12:08,355 --> 00:12:13,355
So it started as a type format, but then now we have

00:12:14,430 --> 00:12:17,350
source/line information, function names and signatures

00:12:17,350 --> 00:12:20,930
is the latest addition, it will allow us to do lots

00:12:20,930 --> 00:12:22,730
of interesting things that have been discussed

00:12:22,730 --> 00:12:23,823
at this conference.

00:12:25,840 --> 00:12:28,360
And there is this pahole tool,

00:12:28,360 --> 00:12:33,053
which uses the kernel DWARF information to generate BTF

00:12:34,090 --> 00:12:38,379
and then there is a great reduction in doing so

00:12:38,379 --> 00:12:40,670
because the BTF form is more compact,

00:12:40,670 --> 00:12:44,040
but the interesting thing is this deduplication

00:12:44,040 --> 00:12:47,820
that libbpf does and pahole uses

00:12:47,820 --> 00:12:52,820
to remove all the types that are in all the objects file

00:12:53,210 --> 00:12:55,110
for the kernel and then, instead of having

00:12:55,110 --> 00:12:57,480
a debugging profile with 200 megabytes,

00:12:57,480 --> 00:12:59,520
you have just 2 megabytes.

00:12:59,520 --> 00:13:02,703
All the data structures inside the kernel are there.

00:13:03,680 --> 00:13:05,430
And then you can use this for all sorts

00:13:05,430 --> 00:13:09,880
of interesting things, you don't need any more headers.

00:13:09,880 --> 00:13:12,747
You can check if a BPF program that was complied

00:13:12,747 --> 00:13:16,600
for an older kernel is accessing data structures

00:13:16,600 --> 00:13:19,200
that changed and then do reallocation.

00:13:19,200 --> 00:13:20,750
This is something that will be discussed

00:13:20,750 --> 00:13:22,713
by other people during the conference.

00:13:24,270 --> 00:13:27,210
And it gets always available via CSFS.

00:13:27,210 --> 00:13:31,438
The patch is already accepted, so you will be able to.

00:13:31,438 --> 00:13:34,240
You always will have access to type information

00:13:34,240 --> 00:13:37,370
for the Linux kernel if you have the right tools,

00:13:37,370 --> 00:13:41,540
which is pahole by now, but in the future, maybe GCC.

00:13:41,540 --> 00:13:45,740
Clone generates this, so there won't be discussion

00:13:45,740 --> 00:13:48,620
about this type performance during the...

00:13:48,620 --> 00:13:51,450
But the end result is that it will be like CFI.

00:13:51,450 --> 00:13:52,853
It will be always available.

00:13:57,307 --> 00:14:01,790
Sys_bpf and BTF, the loader, which libbpf has a loader

00:14:01,790 --> 00:14:06,690
it will look at the object that you are

00:14:06,690 --> 00:14:10,587
asking to, the object file, the bytecode, bpf bytecode

00:14:10,587 --> 00:14:13,430
elf file which contains this bytecode

00:14:13,430 --> 00:14:18,350
and look, oh there is a btf, elf session.

00:14:18,350 --> 00:14:21,810
It will parse this thing, do some validation

00:14:21,810 --> 00:14:24,500
and then use...

00:14:25,415 --> 00:14:27,760
BPF_BTF_LOAD commands

00:14:27,760 --> 00:14:30,560
and then we associate this with the BPF program.

00:14:30,560 --> 00:14:32,570
The kernel will do formal validation.

00:14:32,570 --> 00:14:34,627
The in-kernel verify doesn't trust libbpf

00:14:34,627 --> 00:14:38,930
and it shouldn't, and then the tooling will be able

00:14:40,000 --> 00:14:41,680
to use these...

00:14:41,680 --> 00:14:46,179
to pretty print BPF maps or for doing other things

00:14:46,179 --> 00:14:48,433
that we're gonna see in the next slides.

00:14:49,960 --> 00:14:53,350
So now we go to visibility of BPF in perf.

00:14:53,350 --> 00:14:57,955
Now we saw basic things, at one point in time.

00:14:57,955 --> 00:15:02,955
Now let's see how we can see those BPF coming and going

00:15:03,660 --> 00:15:08,240
and we make sure that we take notes that they were there

00:15:08,240 --> 00:15:12,380
at some point in time where, what was the source code

00:15:12,380 --> 00:15:14,730
what was the bytecode, what was the JITed code.

00:15:16,440 --> 00:15:19,330
How many times did the samples happen?

00:15:19,330 --> 00:15:21,383
They have hints of interest happen there.

00:15:23,559 --> 00:15:26,033
I was with this running,

00:15:28,480 --> 00:15:31,130
I say that I shouldn't do that because it will crash

00:15:31,130 --> 00:15:32,480
in the middle of the presentation.

00:15:32,480 --> 00:15:34,210
The only computer that crashed was that one.

00:15:34,210 --> 00:15:35,570
I don't know why.

00:15:35,570 --> 00:15:38,190
But this is perf trace using BPF

00:15:38,190 --> 00:15:40,023
to do system call trace.

00:15:41,573 --> 00:15:44,783
Syscall trace, like strace.

00:15:46,190 --> 00:15:47,640
For all the syscalls in the system,

00:15:47,640 --> 00:15:51,052
except for some that are for the tool itself.

00:15:51,052 --> 00:15:54,190
And it uses BPF to get...

00:15:58,332 --> 00:16:01,010
pathnames and structs socketed here.

00:16:01,010 --> 00:16:04,260
Things that you would require ptrace to do,

00:16:04,260 --> 00:16:08,680
but you do with BPF instead, you hook into the syscall_enter

00:16:08,680 --> 00:16:12,010
and then at that point, you get all the payload

00:16:12,010 --> 00:16:14,070
for the tracepoint and then it's,

00:16:14,070 --> 00:16:17,550
oh this thing has a pointer, so let me get this

00:16:19,344 --> 00:16:23,450
with BPF functions to get from user space to kernel space,

00:16:23,450 --> 00:16:28,390
copy that and then put an augmented trace point,

00:16:28,390 --> 00:16:30,920
with the pointed payloads so that can be processed.

00:16:30,920 --> 00:16:34,710
And here, it's something that you can do with the things

00:16:34,710 --> 00:16:35,993
that I described so far.

00:16:38,040 --> 00:16:42,660
Here I'm doing a perf top, live mode

00:16:42,660 --> 00:16:47,660
looking at this sys_enter BPF program

00:16:47,660 --> 00:16:51,629
with this tag that was passed to the raw syscall,

00:16:51,629 --> 00:16:56,390
sys_enter tracepoint, and I was looking at this...

00:16:58,610 --> 00:17:01,300
It starts with cycles and I said most of the cycles

00:17:01,300 --> 00:17:04,123
are on the first instruction, that's curious.

00:17:05,000 --> 00:17:08,423
But this is a syscall, let's go back to the presentation.

00:17:09,560 --> 00:17:12,390
So visibility of BPF in perf.

00:17:12,390 --> 00:17:16,280
There are new perf record types that you ask the kernel

00:17:16,280 --> 00:17:20,232
when you start a session that every time a new kernel symbol

00:17:20,232 --> 00:17:22,570
is put in place, please let me know.

00:17:22,570 --> 00:17:25,040
BPF is one of them, this is not just for BPF

00:17:25,040 --> 00:17:29,472
but it's good for BPF, you have PERF_RECORD_BPF_EVENT

00:17:29,472 --> 00:17:34,472
that it says, oh a new program is being loaded.

00:17:34,600 --> 00:17:37,773
Or a program that was loaded is being unloaded.

00:17:38,650 --> 00:17:41,530
So in this, I will get things like...

00:17:43,520 --> 00:17:46,030
where it is, for the case symbol.

00:17:46,030 --> 00:17:48,993
Where it is, what its name, the range, some flags.

00:17:50,690 --> 00:17:55,550
And for the BPF event, we're gonna get if it's load, unload

00:17:55,550 --> 00:17:57,280
and the tag, so...

00:17:58,780 --> 00:18:03,734
Perf record starts a thread and it listens to this BPF event

00:18:03,734 --> 00:18:08,150
and does the dance when the new BPF program is loaded,

00:18:08,150 --> 00:18:09,667
it asks the kernel for,

00:18:09,667 --> 00:18:13,047
"Give me source code, give me its bytecode.

00:18:13,047 --> 00:18:17,340
"Give me its JITed code, give me type information."

00:18:17,340 --> 00:18:19,380
that it has, and then it stashes everything

00:18:19,380 --> 00:18:23,860
on the perf data file for later use, or for use as it goes,

00:18:23,860 --> 00:18:25,863
like it was showing on the perf top.

00:18:27,520 --> 00:18:31,550
So there are types, function prototypes, as I said.

00:18:31,550 --> 00:18:32,610
Let me try to...

00:18:33,820 --> 00:18:38,390
We can do annotation and that was done by Song Liu

00:18:38,390 --> 00:18:43,390
at Facebook and it uses binutils libopcodes.

00:18:43,876 --> 00:18:47,610
We should do special handling in the future,

00:18:47,610 --> 00:18:49,210
like oh, there is a BPF spinlock

00:18:50,520 --> 00:18:53,100
there are information that that is a BPF spinlock

00:18:53,100 --> 00:18:55,530
so we could do something with this information

00:18:55,530 --> 00:18:57,410
on the annotation view.

00:18:57,410 --> 00:19:00,360
That's not done yet, but it's a possibility for the future.

00:19:03,070 --> 00:19:06,920
Then we have, let me see.

00:19:06,920 --> 00:19:09,473
Okay, so there is this thing, so this is a

00:19:09,473 --> 00:19:11,050
perf top running...

00:19:14,236 --> 00:19:16,650
And you see the BPF program is, this is the default view

00:19:16,650 --> 00:19:21,030
of perf top that shows the DSO and then the function.

00:19:21,030 --> 00:19:24,092
And in here, I just said filter out

00:19:24,092 --> 00:19:29,092
show me just the functions that have BPF in its name.

00:19:29,620 --> 00:19:33,690
And among them, the sys_enter and sys_exit.

00:19:33,690 --> 00:19:37,340
So it was taking 0.74% of the cycles in the system

00:19:37,340 --> 00:19:42,200
when doing system calls, system wide things like tracing.

00:19:43,990 --> 00:19:48,343
And then perf annotate that's what you saw

00:19:48,343 --> 00:19:52,658
some time with this, now here just with cycles precise

00:19:52,658 --> 00:19:56,170
which gets just the first, the second one

00:19:56,170 --> 00:19:59,520
is not a precise event, so you get it spread out

00:19:59,520 --> 00:20:01,340
like in the first and the second.

00:20:01,340 --> 00:20:05,010
There's some skid at the time you collect the sample.

00:20:05,010 --> 00:20:08,310
The data was curious, so I try rebooting the kernel

00:20:08,310 --> 00:20:12,020
with those two things on the command line,

00:20:12,020 --> 00:20:13,487
the kernel command line.

00:20:16,170 --> 00:20:18,233
It changed things, yeah.

00:20:21,020 --> 00:20:25,190
So this is something I copied from my internal mailing list

00:20:25,190 --> 00:20:29,167
at Red Hat, perf report being used to investigate XDP.

00:20:30,770 --> 00:20:33,520
Should make sense to the guys who are working with XDP.

00:20:35,170 --> 00:20:38,720
Now to some new stuff that we are discussing.

00:20:38,720 --> 00:20:42,267
On my way here, Alexei will say "Oh people who are working

00:20:42,267 --> 00:20:45,587
"with bpftool don't want to mess with ncurses, perf top

00:20:45,587 --> 00:20:47,817
"has this ncurses interface.

00:20:47,817 --> 00:20:51,347
"Perhaps we could reuse it to do a bpftop

00:20:51,347 --> 00:20:55,837
"using that kernel.bpf_stats_enabled, which does

00:20:55,837 --> 00:20:58,317
"a clock at the beginning of the BPF program

00:20:58,317 --> 00:21:02,560
"and after when it is enabled."

00:21:02,560 --> 00:21:04,690
And then we started talking and thinking about

00:21:04,690 --> 00:21:07,352
doing this as a synthetic event, so that you can have

00:21:07,352 --> 00:21:10,900
multi-column like you're gonna get what he wants,

00:21:10,900 --> 00:21:13,590
but since it's using the machinery for perf top

00:21:13,590 --> 00:21:15,900
you would have this information plus cycles

00:21:15,900 --> 00:21:19,470
and structs and ITLB misses or whatever.

00:21:19,470 --> 00:21:21,660
In multiple columns, whatever.

00:21:21,660 --> 00:21:24,663
And later, we want to do a software PMU.

00:21:27,700 --> 00:21:30,200
You would have, let's say, a plugin or a thread

00:21:30,200 --> 00:21:35,030
or whatever, that you would access metrics that are provided

00:21:35,030 --> 00:21:38,133
by other system tools, SAR or whatever.

00:21:39,200 --> 00:21:43,101
But as a perf event, then behind the scenes

00:21:43,101 --> 00:21:46,810
all the machinery would be used to,

00:21:46,810 --> 00:21:48,910
oh this is a kernel thing I'm gonna get from the kernel.

00:21:48,910 --> 00:21:50,980
Oh no, this is something that I have to ask

00:21:50,980 --> 00:21:53,280
this other component periodically.

00:21:53,280 --> 00:21:58,280
And then I would combine this in the same workflow for perf.

00:22:00,563 --> 00:22:04,193
What Alexei is doing is just, I make some notes

00:22:04,193 --> 00:22:06,100
and that's interesting for Mitch I think.

00:22:06,100 --> 00:22:07,670
I was not aware of that.

00:22:07,670 --> 00:22:11,430
You do this, and when you enable

00:22:12,551 --> 00:22:15,640
and you look at the FDs for the perf program

00:22:15,640 --> 00:22:20,640
that was setting up BPF events, you have this new,

00:22:21,865 --> 00:22:25,735
you look at what are the Fds that are BPF programs.

00:22:25,735 --> 00:22:30,735
So in this case, there were eight I think, or more

00:22:30,980 --> 00:22:34,760
and then you go and look at the fdinfo for that thing.

00:22:34,760 --> 00:22:36,833
That's an interface I was not even aware.

00:22:37,750 --> 00:22:40,564
And then you get this straight to two extra lines, here

00:22:40,564 --> 00:22:43,283
and you get the tag and if it was JITed, the program type

00:22:43,283 --> 00:22:46,220
lots of other information, that's where we are going

00:22:46,220 --> 00:22:49,020
to collect this information to have the bpf top working.

00:22:50,050 --> 00:22:55,050
And something else that I also discussed with him is,

00:22:56,430 --> 00:23:01,170
oh that thing is low cost, but it's just the number

00:23:01,170 --> 00:23:03,290
of nanoseconds and the number of time that it's running.

00:23:03,290 --> 00:23:07,364
I want to do perf stat, I do perf stat right now

00:23:07,364 --> 00:23:11,610
with a PID, just for that PID, just for that TID.

00:23:11,610 --> 00:23:14,600
Just for that CPU, or cgroups, so on and so forth.

00:23:14,600 --> 00:23:17,830
But I want to say, just for that BPF program.

00:23:17,830 --> 00:23:20,030
I want to know how many cache misses or ITLB

00:23:20,880 --> 00:23:24,060
or whatever, any of those hardware counters,

00:23:24,060 --> 00:23:26,586
but just for that BPF program.

00:23:26,586 --> 00:23:30,283
So that's something that we'll try to prototype.

00:23:31,280 --> 00:23:34,830
The idea is that all the workflow for perf annotate, record,

00:23:34,830 --> 00:23:38,493
report, should be available with BPF, when it makes sense.

00:23:39,510 --> 00:23:43,870
And perf probe as well, you can dump the source code

00:23:43,870 --> 00:23:48,660
for a BPF program, so perhaps you could use a perf probe

00:23:48,660 --> 00:23:53,660
to do that as well and retrieve the original bytecode

00:23:54,310 --> 00:23:57,060
do changes in it to collect things or to see

00:23:57,060 --> 00:24:01,192
if it gets in some place or whatever and then re-inject it.

00:24:01,192 --> 00:24:03,213
But this is just a wide idea.

00:24:05,140 --> 00:24:10,140
To know if the control flow passed through some place

00:24:10,180 --> 00:24:13,110
we could do something better, which is Intel PT

00:24:14,150 --> 00:24:18,230
processor trace and set up some hardware address filters

00:24:18,230 --> 00:24:21,360
and then ask for all the branches that are taking

00:24:21,360 --> 00:24:24,926
just in that range that is the BPF program.

00:24:24,926 --> 00:24:26,760
It is not working right now for BPF programs

00:24:26,760 --> 00:24:31,250
so I'm in contact with Alex Shishkin at Intel

00:24:31,250 --> 00:24:33,250
and Adrian Hunter, trying to figure out

00:24:33,250 --> 00:24:34,993
how to get this to work.

00:24:37,279 --> 00:24:40,420
So that's what I had to say

00:24:40,420 --> 00:24:42,170
and I hope that you have questions.

00:24:45,910 --> 00:24:48,360
There are some links for interesting information.

00:24:49,270 --> 00:24:51,640
- So one thing we have coming

00:24:51,640 --> 00:24:55,660
in the next merge window is a secure boot lockdown patch

00:24:55,660 --> 00:24:58,360
for which we have to disable BPF entirely

00:24:58,360 --> 00:25:02,856
for the whole machine in the kernel because we can't prove

00:25:02,856 --> 00:25:05,140
at the moment, that you can't use it

00:25:05,140 --> 00:25:09,910
to alter the kernel image and you can't use it to access

00:25:12,762 --> 00:25:16,116
private keys that are held in the kernel session keys.

00:25:16,116 --> 00:25:18,520
Do you know if there's any way we can get around that?

00:25:18,520 --> 00:25:21,270
- Yeah I'm not aware of this specific problem.

00:25:21,270 --> 00:25:24,260
As I say that, I work more on the user space side of it

00:25:24,260 --> 00:25:26,790
and that's a lot of stuff, but perhaps there is somebody

00:25:26,790 --> 00:25:31,433
here in the room, I was hoping that could get into that.

00:25:35,200 --> 00:25:37,630
- So we could run BPF under like the

00:25:37,630 --> 00:25:40,190
KPTI map or something with restricted access

00:25:40,190 --> 00:25:42,530
to restrict address, but it would have

00:25:42,530 --> 00:25:44,760
a fairly high entry/exit cost.

00:25:44,760 --> 00:25:46,070
It's something we thought about because we have

00:25:46,070 --> 00:25:48,763
the exact same problem as you, especially eBPF.

00:25:49,660 --> 00:25:51,930
BPF's a little nicer, eBPF we have problems

00:25:51,930 --> 00:25:53,230
with conditional branches.

00:25:58,610 --> 00:25:59,840
- [Audience Member] I have a couple questions.

00:25:59,840 --> 00:26:00,673
- Yeah.

00:26:00,673 --> 00:26:02,270
- The perf top output was great,

00:26:02,270 --> 00:26:05,680
it's amazing to see the cycles in the BPF program.

00:26:05,680 --> 00:26:08,441
Since BPF programs are so fast, you'd expect

00:26:08,441 --> 00:26:12,673
you would need PEBS on a system for this to work at all.

00:26:14,130 --> 00:26:15,950
- Yeah, can you--

00:26:15,950 --> 00:26:18,180
- Because my workloads are all in the cloud

00:26:18,180 --> 00:26:20,280
and I don't have access to PEBS.

00:26:20,280 --> 00:26:23,050
So every time I do perf top or CPU sampling

00:26:23,050 --> 00:26:25,310
I get massive interrupt skid.

00:26:25,310 --> 00:26:26,210
- [Arnaldo] Okay.

00:26:26,210 --> 00:26:29,280
- So I'm just clarifying, I'm pretty sure I'm gonna need

00:26:29,280 --> 00:26:32,203
PEBS to see this accurately, at all.

00:26:34,894 --> 00:26:38,250
- Yeah, yeah, yep, yep.

00:26:38,250 --> 00:26:42,880
I hope the hardware vendors are listening to you.

00:26:42,880 --> 00:26:45,230
- I mean, I'm asking, Amazon has been good.

00:26:45,230 --> 00:26:47,110
They've turned on PMCs, which is great.

00:26:47,110 --> 00:26:48,900
Now I need them to turn on PEBS, there's probably

00:26:48,900 --> 00:26:51,420
some AWS people here, and so.

00:26:51,420 --> 00:26:53,650
The other question is you're showing the top level

00:26:53,650 --> 00:26:57,990
but how much of a stack trace is visible?

00:26:57,990 --> 00:27:00,473
So if a BPF program calls a BPF call.

00:27:00,473 --> 00:27:04,413
Do we see, a little bit of the stack?

00:27:05,673 --> 00:27:08,090
- So if a BPF program calls another BPF?

00:27:08,090 --> 00:27:11,604
- If it calls a BPF call, one of its built-ins.

00:27:11,604 --> 00:27:14,940
Or whatever, is there some level of stack trace

00:27:14,940 --> 00:27:15,840
that we would see?

00:27:16,890 --> 00:27:20,580
- I haven't get into BPF calls yet but the idea is

00:27:20,580 --> 00:27:23,220
that it works just like for the kernel

00:27:23,220 --> 00:27:26,320
and then if you would do something like...

00:27:28,320 --> 00:27:29,500
Here and...

00:27:31,720 --> 00:27:35,183
For jobs it's working, but for calls it would work as well.

00:27:35,183 --> 00:27:36,516
So let me see...

00:27:38,873 --> 00:27:41,010
- Because I guess I should know this question myself

00:27:41,010 --> 00:27:43,200
but I don't, I don't know if BPF itself

00:27:43,200 --> 00:27:47,354
trashes the frame pointer or if it somehow establishes

00:27:47,354 --> 00:27:51,633
a stack winder, like how do I even work a stack from BPF?

00:27:52,598 --> 00:27:56,310
I should know that answer, I don't, just curious.

00:27:56,310 --> 00:27:58,910
- Yeah no I don't have an answer right now for that.

00:28:02,050 --> 00:28:05,140
The idea is that the same kind of functionality

00:28:05,140 --> 00:28:09,170
that you have, where it makes sense and where it's possible

00:28:09,170 --> 00:28:14,170
should be available here, so as we go implementing it

00:28:14,530 --> 00:28:17,973
and as we show that this is available and already useful

00:28:17,973 --> 00:28:21,440
to a large degree, people will start getting those questions

00:28:21,440 --> 00:28:24,960
and then we'll try to address them, but...

00:28:24,960 --> 00:28:26,480
- [Audience Member] Yeah I feel right now, if you look

00:28:26,480 --> 00:28:27,920
at a stack trace, it might just be killed.

00:28:27,920 --> 00:28:29,350
It might be killed by BPF.

00:28:29,350 --> 00:28:30,607
- Right.

00:28:30,607 --> 00:28:31,440
- [Audience Member] It just depends on whether it's honoring

00:28:31,440 --> 00:28:32,650
the frame pointer, or whatever.

00:28:32,650 --> 00:28:33,483
- Yeah.

00:28:42,965 --> 00:28:44,715
(laughs) It was to troll the thing.

00:28:52,470 --> 00:28:53,470
Any other questions?

00:29:00,052 --> 00:29:02,700
- So just now you had the slide

00:29:02,700 --> 00:29:06,503
for the BPF tracing outside the Linux.

00:29:08,220 --> 00:29:12,830
So yeah, this one, so can you elaborate

00:29:12,830 --> 00:29:15,477
what's the issue for right now, because that assem--

00:29:15,477 --> 00:29:19,273
- I think it's just a bug. I think it's just a bug on the,

00:29:20,127 --> 00:29:21,560
I don't know, I don't think that this is a limitation

00:29:21,560 --> 00:29:23,853
for Intel PT.

00:29:25,311 --> 00:29:29,380
When I ask Adrian Hunter,

00:29:29,380 --> 00:29:32,847
I interact most, he said "Oh it's not working right now."

00:29:34,287 --> 00:29:36,780
and he asked Alex Shishkin, I think that he's in vacation

00:29:36,780 --> 00:29:41,660
because he didn't answer so far, because the way that

00:29:43,326 --> 00:29:46,266
those hardware filters work is kind of like

00:29:46,266 --> 00:29:49,820
reusing the same thing for tracepoint filters.

00:29:49,820 --> 00:29:52,490
So in the command line, it could say some tracepoint

00:29:52,490 --> 00:29:55,030
and then dash, dash filter, the filter expression

00:29:55,030 --> 00:29:58,610
and this gets into an I/O cto and gets to tracepoints

00:29:58,610 --> 00:30:00,670
or to the hardware tracer.

00:30:00,670 --> 00:30:05,670
And it works for the kernel, not for outside of this range.

00:30:05,910 --> 00:30:09,670
But this seems to be just a bug, so I don't think

00:30:09,670 --> 00:30:13,440
that there is, what I want to do is to do perf something.

00:30:13,440 --> 00:30:16,290
Dash e, Intel PT and set the filter

00:30:18,886 --> 00:30:21,300
and the filter expression would be a BPF tag.

00:30:21,300 --> 00:30:24,500
And then it will just be BPF calling something

00:30:24,500 --> 00:30:27,410
and then it gets this expression automatically

00:30:27,410 --> 00:30:29,343
translated into the range of the form

00:30:29,343 --> 00:30:32,160
that the hardware filter expects

00:30:32,160 --> 00:30:34,600
and then you're gonna greatly reduce

00:30:34,600 --> 00:30:38,807
because Intel PT is voluminous, it's lots of information.

00:30:38,807 --> 00:30:41,853
But just for that small thing, that could be interesting.

00:30:43,050 --> 00:30:43,903
- [Audience Member] Thanks.

00:30:48,660 --> 00:30:49,760
- Any other questions?

00:30:51,600 --> 00:30:54,571
So thank you for coming. (applause)

00:30:54,571 --> 00:30:56,390
And I hope that you use this to improve Linux

00:30:56,390 --> 00:30:58,113

YouTube URL: https://www.youtube.com/watch?v=RLIunZreRFI


