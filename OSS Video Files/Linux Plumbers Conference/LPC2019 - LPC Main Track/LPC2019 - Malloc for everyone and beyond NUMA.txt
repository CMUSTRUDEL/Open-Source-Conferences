Title: LPC2019 - Malloc for everyone and beyond NUMA
Publication date: 2019-11-18
Playlist: LPC2019 - LPC Main Track
Description: 
	Malloc for everyone and beyond NUMA

Speaker
 Jerome Glisse (Red Hat)

Description
With heterogeneous computing, program's data (range of virtual addresses) have to move to different physical memory during the lifetime of an application to keep it local to compute unit (CPU, GPU, FPGA, ...). NUMA have been the model used so far but it has assumptions that do not work with all the memory type we now have. This presentation will explore the various types of memory and how we can expose and use them through unified API.
Captions: 
	00:00:00,200 --> 00:00:03,680
- So, today I want to talk a bit about Malloc for Everyone,

00:00:03,680 --> 00:00:05,933
and I will explain what I mean by that,

00:00:06,800 --> 00:00:08,070
and it's not exactly only malloc.

00:00:08,070 --> 00:00:12,720
But first I want to go over

00:00:12,720 --> 00:00:16,530
you know just stating some fact about 2D computings

00:00:16,530 --> 00:00:18,933
and what we're seeing overall in the market.

00:00:19,900 --> 00:00:21,010
So you know, for a long time,

00:00:21,010 --> 00:00:23,980
we've been looking at jobs on our servers

00:00:23,980 --> 00:00:26,190
that are just being kind of boring accounting job.

00:00:26,190 --> 00:00:28,380
We had web server, we have the database,

00:00:28,380 --> 00:00:32,670
we had like things that are really kind of boring,

00:00:32,670 --> 00:00:36,140
if you will, but, in the last few years,

00:00:36,140 --> 00:00:37,703
we have seen a really,

00:00:39,490 --> 00:00:43,970
kind of a spread of various new workloads

00:00:43,970 --> 00:00:45,020
that are highly parallel.

00:00:45,020 --> 00:00:49,190
For instance, EI, all the thing you can see

00:00:49,190 --> 00:00:52,020
with model training, model execution.

00:00:52,020 --> 00:00:55,219
And they old, you know, fail be on musum,

00:00:55,219 --> 00:00:58,560
multiples of kind of, competition we can see for instance

00:00:58,560 --> 00:01:01,380
image processing and recognition and all that

00:01:01,380 --> 00:01:03,540
and all these kind of new workload,

00:01:03,540 --> 00:01:07,140
they really really have a massively parallel can

00:01:07,140 --> 00:01:09,390
of properties to them.

00:01:09,390 --> 00:01:14,210
And what we see is that CPU is not really that good

00:01:14,210 --> 00:01:16,040
at actually executing this kind of workload

00:01:16,040 --> 00:01:21,040
and we see GP, FPG or DSP being like 200 times faster

00:01:21,140 --> 00:01:23,520
actually at crunching number on this kind of workload.

00:01:23,520 --> 00:01:27,840
It's various on image processing and bunch more,

00:01:27,840 --> 00:01:30,780
many manual stuff like fast-forwarding and all of that.

00:01:30,780 --> 00:01:34,640
So, we've seen this change of workload

00:01:34,640 --> 00:01:36,650
we cannot expect everybody to run.

00:01:36,650 --> 00:01:39,080
And more and more people are doing this kind of workload

00:01:39,080 --> 00:01:42,313
better across the board for many reasons.

00:01:44,588 --> 00:01:47,400
Another thing I want to point out is that we see

00:01:47,400 --> 00:01:50,040
a really increase inside the programmers royalty

00:01:50,040 --> 00:01:51,130
and you know people are studying

00:01:51,130 --> 00:01:53,930
to build their program as blocks so you know

00:01:53,930 --> 00:01:55,720
we're gonna use these blocks from these people

00:01:55,720 --> 00:01:57,963
that does that thing and they're gonna use,

00:01:57,963 --> 00:01:59,330
this is a block from some other folks

00:01:59,330 --> 00:02:00,740
that does these other things.

00:02:00,740 --> 00:02:02,600
And then we're gonna mix and match

00:02:02,600 --> 00:02:03,560
a bunch of different blocks

00:02:03,560 --> 00:02:04,930
that coming from different people

00:02:04,930 --> 00:02:06,260
and then coming from different projects

00:02:06,260 --> 00:02:07,140
and so on and so forth.

00:02:07,140 --> 00:02:10,580
I think we have all seen how people just get up to

00:02:10,580 --> 00:02:15,160
mix and match various library and various building blocks.

00:02:15,160 --> 00:02:18,960
And so you see much more, you know,

00:02:18,960 --> 00:02:22,150
you see programs that are not fully done

00:02:22,150 --> 00:02:24,960
by only a team programmers but programs that is just

00:02:24,960 --> 00:02:27,550
actually an integration of multiples or blocks,

00:02:27,550 --> 00:02:30,520
building blocks that have been done by multiples or people.

00:02:30,520 --> 00:02:33,457
And other thing we also see is like the,

00:02:33,457 --> 00:02:36,440
the program pipeline, that used to be your program

00:02:36,440 --> 00:02:37,810
is just taking this kind of input,

00:02:37,810 --> 00:02:40,230
is giving you this kind of output,

00:02:40,230 --> 00:02:42,537
so we never seen much more people doing,

00:02:42,537 --> 00:02:45,000
having programs that are much more versatile

00:02:45,000 --> 00:02:47,600
and people, the user, the end user can actually say

00:02:47,600 --> 00:02:49,260
hey, I wanted that competition with that thing

00:02:49,260 --> 00:02:52,340
and that other thing so you know, people kind of

00:02:52,340 --> 00:02:55,240
build a competition graph on the go

00:02:55,240 --> 00:02:58,123
and it's under the control of the end users.

00:02:58,123 --> 00:03:00,860
Another thing also is that we see the input

00:03:00,860 --> 00:03:03,600
that I said is not only coming from the disk, local disk,

00:03:03,600 --> 00:03:06,240
but it can come from networks and it can come from

00:03:06,240 --> 00:03:09,080
sensor, it can come from various other places

00:03:09,080 --> 00:03:11,490
so you know, it used to be that the data

00:03:11,490 --> 00:03:13,650
that we're getting has input inside another program

00:03:13,650 --> 00:03:17,410
was coming for a very boring just final disk

00:03:17,410 --> 00:03:20,670
but it's really much more diverse now.

00:03:20,670 --> 00:03:23,320
And the key point is that, basically what I want to get at

00:03:23,320 --> 00:03:25,080
really is that the programmers,

00:03:25,080 --> 00:03:26,852
the people that are doing the program,

00:03:26,852 --> 00:03:30,540
they no longer control all the code that we are using.

00:03:30,540 --> 00:03:32,920
You know we just take that library and get out

00:03:32,920 --> 00:03:34,380
with the visual library over,

00:03:34,380 --> 00:03:36,340
and we take this building block there

00:03:36,340 --> 00:03:39,160
and we mix and match different things from different places

00:03:39,160 --> 00:03:40,100
and from different people,

00:03:40,100 --> 00:03:41,780
and even sometimes from different languages.

00:03:41,780 --> 00:03:43,470
So you know you have like,

00:03:43,470 --> 00:03:45,340
hey this part of my program is actually python

00:03:45,340 --> 00:03:46,487
and this part is actually C++

00:03:46,487 --> 00:03:48,320
and this part is C and what not.

00:03:48,320 --> 00:03:51,360
So you know you see this mix and match of various things

00:03:51,360 --> 00:03:54,070
across the board, and it does mean that, you know,

00:03:54,070 --> 00:03:56,240
people don't have over control

00:03:56,240 --> 00:03:58,530
over what every block is doing, you know.

00:03:58,530 --> 00:04:02,470
We don't, we cannot fix or change every single blocks.

00:04:02,470 --> 00:04:03,820
What we want is to use blocks

00:04:03,820 --> 00:04:06,120
and be able to mix and match different blocks.

00:04:07,150 --> 00:04:10,913
So, the point to that is that,

00:04:13,090 --> 00:04:13,923
the,

00:04:15,270 --> 00:04:18,320
everything processor does or GPU or FPG does,

00:04:18,320 --> 00:04:20,060
it's really working on data release.

00:04:20,060 --> 00:04:22,920
And when you look at it, I was already measuring that,

00:04:22,920 --> 00:04:24,720
the data is coming from various sources,

00:04:24,720 --> 00:04:27,190
nobody has used network, storage, sensors.

00:04:27,190 --> 00:04:31,050
And one of the issue when you have that is

00:04:31,050 --> 00:04:34,400
if you're using multiple device, a CPU and GPU and what not,

00:04:34,400 --> 00:04:36,560
if every time you have to pass that around

00:04:36,560 --> 00:04:39,580
you have to go through the CPU, it's kind of a

00:04:39,580 --> 00:04:42,410
bottleneck really because CPU is kind of the middle man.

00:04:42,410 --> 00:04:44,490
I'm getting this data from network,

00:04:44,490 --> 00:04:45,880
this CPU is doing some processing,

00:04:45,880 --> 00:04:48,780
for instance the packet processing is the complex thing

00:04:48,780 --> 00:04:50,793
to do from the packets and then,

00:04:50,793 --> 00:04:53,340
copying the data inside some memory buffer

00:04:53,340 --> 00:04:55,200
and then the memory for it get copy again

00:04:55,200 --> 00:04:57,340
inside the GPU for instance.

00:04:57,340 --> 00:04:59,530
And so you have like all these steps

00:04:59,530 --> 00:05:02,410
going one after the other and the CPU is just a bottleneck

00:05:02,410 --> 00:05:05,350
because the CPU is glorified eh, I'm getting data here

00:05:05,350 --> 00:05:07,800
and I'm just giving it over to this other guy.

00:05:07,800 --> 00:05:11,369
And so, basically it's kind of a waste

00:05:11,369 --> 00:05:13,780
and not something we want to,

00:05:13,780 --> 00:05:15,980
it's something we want to address basically.

00:05:18,160 --> 00:05:22,210
Another thing is also that you have an interleave of things.

00:05:22,210 --> 00:05:23,980
So you know you can start doing some competition

00:05:23,980 --> 00:05:26,470
on your CPU, so let's say you do something that is

00:05:26,470 --> 00:05:28,460
really prone to be done on CPU,

00:05:28,460 --> 00:05:30,840
and then you do some other algorithm

00:05:30,840 --> 00:05:33,550
that is highly parallel and then you can do it on GPU,

00:05:33,550 --> 00:05:35,200
and you take the output of that and then you do

00:05:35,200 --> 00:05:38,330
something else on the CPU and then you do something else

00:05:38,330 --> 00:05:41,080
and it goes on an FPGA and then so on and so forth.

00:05:41,080 --> 00:05:44,460
So you're moving your computation pipeline is going

00:05:44,460 --> 00:05:47,250
from one device the the next and so on and so forth.

00:05:47,250 --> 00:05:51,330
And so, again, if every time you have to go from,

00:05:51,330 --> 00:05:53,910
let's say you want to go to a GPU to an FPGA.

00:05:53,910 --> 00:05:56,400
If you have to go through a CPU in between,

00:05:56,400 --> 00:05:57,860
it's kind of bottleneck so you know,

00:05:57,860 --> 00:06:00,000
if you have the CPU at the center of everything,

00:06:00,000 --> 00:06:02,970
you have like the CPU sitting as a middleman

00:06:02,970 --> 00:06:05,430
and as a bottleneck to everybody.

00:06:05,430 --> 00:06:09,340
So that's really, I like CPU obviously too,

00:06:09,340 --> 00:06:12,250
but sometimes CPU, if it get in the way,

00:06:12,250 --> 00:06:14,330
I don't want them to waste their time basically

00:06:14,330 --> 00:06:16,980
just by being a glorified middleman

00:06:16,980 --> 00:06:20,123
that passes things around and just sit in between people.

00:06:21,379 --> 00:06:25,550
So then I wanted to clearly define what is an address space

00:06:25,550 --> 00:06:27,910
before I go much more inside

00:06:27,910 --> 00:06:30,150
what is the purpose of this talks.

00:06:30,150 --> 00:06:32,040
So an address space is really just a mapping

00:06:32,040 --> 00:06:34,060
between a visual address and,

00:06:34,060 --> 00:06:35,560
which is a pointer inside your program.

00:06:35,560 --> 00:06:38,618
When you do malloc, you get a pointer out of malloc.

00:06:38,618 --> 00:06:40,430
When you do mmap of something, you get a pointer out of it.

00:06:40,430 --> 00:06:42,630
A pointer is just a virtual address

00:06:42,630 --> 00:06:46,160
and the virtual address of mapping with physical memories

00:06:46,160 --> 00:06:48,440
so you know like your program never really knows

00:06:48,440 --> 00:06:51,070
what kind of physical memory is accessing to.

00:06:51,070 --> 00:06:54,090
Your program just gets virtual addresses and just use that.

00:06:54,090 --> 00:06:56,580
Your program is fully unaware about the mapping between

00:06:56,580 --> 00:06:58,410
the virtual address and the physical memories

00:06:58,410 --> 00:06:59,720
that is being used.

00:06:59,720 --> 00:07:02,370
And in fact, on most operating systems,

00:07:02,370 --> 00:07:05,080
that mapping can evolve during the lifetime of the program

00:07:05,080 --> 00:07:08,140
so you know, you start with the same virtual address,

00:07:08,140 --> 00:07:10,220
at one point it's using this physical memory

00:07:10,220 --> 00:07:12,387
and sometimes later on, the operating system

00:07:12,387 --> 00:07:15,460
has been doing some of the stuff, and for some reason,

00:07:15,460 --> 00:07:17,030
you're using different physical memory

00:07:17,030 --> 00:07:18,580
for the same virtual addresses.

00:07:19,564 --> 00:07:20,397
And it's just the operating system

00:07:20,397 --> 00:07:21,230
that is doing stuff in the background

00:07:21,230 --> 00:07:23,870
and you don't have no control from that on that front

00:07:23,870 --> 00:07:25,700
inside your program, besides some Cisco

00:07:25,700 --> 00:07:29,934
but lets ignore them for the purpose of this talk.

00:07:29,934 --> 00:07:32,137
So, each program inside your computer

00:07:32,137 --> 00:07:34,710
has it's own address basically so when you

00:07:34,710 --> 00:07:37,360
start exhibiting a program, you have your own address space.

00:07:37,360 --> 00:07:39,620
It means like all the malloc you do, it's,

00:07:39,620 --> 00:07:42,357
you don't share any addresses with any other processes

00:07:42,357 --> 00:07:44,490
on your computers.

00:07:44,490 --> 00:07:49,290
And you start to sharing, so it's really, our process

00:07:49,290 --> 00:07:51,500
is really the outcome of a four course exec

00:07:51,500 --> 00:07:53,960
and so like, you can have some kind of sharing

00:07:53,960 --> 00:07:55,200
between the child and the parents

00:07:55,200 --> 00:07:59,060
so you can integrate some of the pointers from your parents,

00:07:59,060 --> 00:08:01,300
but depends on all the flag you pass around.

00:08:01,300 --> 00:08:03,300
But the idea is really when you do fork and exec

00:08:03,300 --> 00:08:05,320
you get a different address space inside the child

00:08:05,320 --> 00:08:07,060
and from the parent.

00:08:07,060 --> 00:08:08,310
You also have CPU trends

00:08:08,310 --> 00:08:10,000
that everybody is kind of worried of

00:08:10,000 --> 00:08:12,030
and it's really where you see most other people doing

00:08:12,030 --> 00:08:14,830
what is, fork is kind of the dying, dying thing

00:08:14,830 --> 00:08:16,530
from the old Linux days.

00:08:16,530 --> 00:08:18,553
People are much more using threads because threads

00:08:18,553 --> 00:08:20,820
mean you have the same address space across all the threads

00:08:20,820 --> 00:08:22,480
so it does mean that all the threads

00:08:22,480 --> 00:08:24,900
on your program can exchange that between each other

00:08:24,900 --> 00:08:27,580
because we have all the same address spaces.

00:08:27,580 --> 00:08:29,150
So you know, pointers on one thread

00:08:29,150 --> 00:08:31,190
doesn't mean the same thing inside the other thread.

00:08:31,190 --> 00:08:32,340
And so on and so forth.

00:08:33,346 --> 00:08:36,090
And the bottom line here is really that the address space

00:08:36,090 --> 00:08:38,110
technically on your process

00:08:38,110 --> 00:08:40,080
is really managed by malloc and free.

00:08:40,080 --> 00:08:42,992
Malloc allocated virtual addresses and free just,

00:08:42,992 --> 00:08:45,820
free that virtual addresses.

00:08:45,820 --> 00:08:46,850
So malloc and free is really

00:08:46,850 --> 00:08:49,982
what managed the address spaces.

00:08:49,982 --> 00:08:52,250
I'm talking from C point of view but you know,

00:08:52,250 --> 00:08:54,180
all the other languages and in behind it's just a

00:08:54,180 --> 00:08:57,110
Cisco inside which can help, which is mmap and munmap

00:08:57,110 --> 00:08:59,930
but you know from a C language purposes, malloc and free

00:08:59,930 --> 00:09:02,633
and if you look at simple spaces gonna be new

00:09:02,633 --> 00:09:03,540
and so on and so forth but in behind

00:09:03,540 --> 00:09:06,282
it's the same Cisco that goes inside the same,

00:09:06,282 --> 00:09:08,023
inside the next kernels so it doesn't matter.

00:09:09,720 --> 00:09:12,200
So I know that we know what an address space is.

00:09:12,200 --> 00:09:13,790
Let's talk about device address spaces.

00:09:13,790 --> 00:09:15,640
So when you have the device,

00:09:15,640 --> 00:09:17,500
it can have it's own address spaces

00:09:17,500 --> 00:09:20,956
that is different from your process CPU address spaces.

00:09:20,956 --> 00:09:23,350
And these kind of address spaces obviously managed

00:09:23,350 --> 00:09:25,360
by different function and you know,

00:09:25,360 --> 00:09:27,750
depending on which device you use, it's just gonna be

00:09:27,750 --> 00:09:29,352
some device, CPI, malloc, something,

00:09:29,352 --> 00:09:32,302
or device API malloc or device in, deivce PF3

00:09:32,302 --> 00:09:34,220
and so on and so forth.

00:09:34,220 --> 00:09:36,290
You know it's the same thing but different function

00:09:36,290 --> 00:09:38,457
and it's kind of painful so,

00:09:38,457 --> 00:09:43,140
the idea is you have an address space on your devices

00:09:43,140 --> 00:09:46,200
that doesn't match what you have on your CPU

00:09:46,200 --> 00:09:48,720
so a pointer on your device doesn't mean the same thing

00:09:48,720 --> 00:09:50,140
as on your CPU.

00:09:50,140 --> 00:09:53,701
And you know, you have this designation between the two

00:09:53,701 --> 00:09:56,690
and so when you have to communicate that from your CPU

00:09:56,690 --> 00:10:00,210
to a GPU for instance, you have to copy from one other space

00:10:00,210 --> 00:10:02,050
to the other address space.

00:10:02,050 --> 00:10:04,010
And, you know it's kind of a,

00:10:04,010 --> 00:10:06,203
it's not that painful when you're doing just

00:10:06,203 --> 00:10:07,920
some kind of flat data structure

00:10:07,920 --> 00:10:10,870
so you an image, just like an array basically.

00:10:10,870 --> 00:10:13,717
Or when you're doing a, VM matrix vitriols

00:10:14,810 --> 00:10:15,643
and that kind of stuff, it's kind of easy

00:10:15,643 --> 00:10:18,730
because it's a flat thing, there is no pointer inside it.

00:10:18,730 --> 00:10:21,070
But it's really really hard and error prone

00:10:21,070 --> 00:10:22,770
when you're starting to do that kind of thing,

00:10:22,770 --> 00:10:25,370
duplication from one address space to the other,

00:10:25,370 --> 00:10:27,810
when you have a lot of pointers inside the data structure

00:10:27,810 --> 00:10:30,470
that you are trying to duplicate on the new address spaces.

00:10:30,470 --> 00:10:33,860
So if it's a lease or if it's a tree or if it's a graph

00:10:33,860 --> 00:10:36,577
or you know, anytime you have pointers inside

00:10:36,577 --> 00:10:38,590
the data structure you want to duplicate,

00:10:38,590 --> 00:10:40,490
it means you have to duplicate every pointers

00:10:40,490 --> 00:10:44,600
and, I want to give you to walk through an example

00:10:44,600 --> 00:10:46,700
with you, just a very simple example

00:10:47,540 --> 00:10:50,350
of duplicating a list inside of the device address space.

00:10:50,350 --> 00:10:53,761
And let's say you get a listing entry and so,

00:10:53,761 --> 00:10:57,060
you first initialize the device, the device pointer

00:10:57,060 --> 00:10:59,980
for duplicating the list inside the device, device list.

00:10:59,980 --> 00:11:01,900
So you're gonna go, least for H entry

00:11:01,900 --> 00:11:05,640
you're gonna go over all the entry inside your CPU list.

00:11:05,640 --> 00:11:08,283
And for every entry, you're gonna devise API malloc,

00:11:09,195 --> 00:11:10,028
a new entry inside the device,

00:11:11,749 --> 00:11:14,150
and then you do not, you're gonna do device name copy

00:11:14,150 --> 00:11:17,460
to copy the data of your, of your, from your CPU

00:11:17,460 --> 00:11:19,601
to your device memory address spaces.

00:11:19,601 --> 00:11:22,140
Then you just initialize the next pointers

00:11:22,140 --> 00:11:23,914
of your single leg-list.

00:11:23,914 --> 00:11:27,350
If you're not the first, the first nod in the list

00:11:27,350 --> 00:11:29,440
then you know that there was a nod before you

00:11:29,440 --> 00:11:31,110
and then you need to set the next pointer

00:11:31,110 --> 00:11:32,210
of the previous node,

00:11:32,210 --> 00:11:34,130
but you need to use the device CPI point,

00:11:34,130 --> 00:11:36,200
the device pointers so you know, you're not talking

00:11:36,200 --> 00:11:37,420
about the CPU pointer,

00:11:37,420 --> 00:11:40,840
you have to get the device pointer from that.

00:11:40,840 --> 00:11:43,490
And so, you know, it's a single list,

00:11:43,490 --> 00:11:45,140
it's already lot of courts,

00:11:45,140 --> 00:11:46,720
and if you have to do that for graph,

00:11:46,720 --> 00:11:48,940
or any other complex data set,

00:11:48,940 --> 00:11:51,524
it's really, really error prone and all that

00:11:51,524 --> 00:11:52,970
and when you have to debug something, let's say then,

00:11:52,970 --> 00:11:54,210
and you have to do the reverse

00:11:54,210 --> 00:11:56,310
when your GPU is done for instance,

00:11:56,310 --> 00:11:58,576
so you're not in straight computation with the GPU

00:11:58,576 --> 00:11:59,720
and the GPU might actually change that list, you know,

00:11:59,720 --> 00:12:01,800
might remove notes, might add stuff,

00:12:01,800 --> 00:12:04,000
and so on and so forth, and so the list you get

00:12:04,000 --> 00:12:06,020
at the output of the GPU may be something different

00:12:06,020 --> 00:12:07,950
so you might have to do the reverse operation

00:12:07,950 --> 00:12:11,360
if you want to get the data back inside the list on the CPU.

00:12:11,360 --> 00:12:13,990
So it's, you know, you see, it's really,

00:12:13,990 --> 00:12:15,230
what it's really painful really,

00:12:15,230 --> 00:12:17,550
and we see the sample list example you can think of

00:12:17,550 --> 00:12:19,480
which is one pointer so,

00:12:19,480 --> 00:12:22,092
look at your data search or look at people data search

00:12:22,092 --> 00:12:23,560
who are when we're doing complex workloads

00:12:23,560 --> 00:12:26,530
and you will see that they have much more than one pointer

00:12:26,530 --> 00:12:28,910
for every note inside their graph and,

00:12:28,910 --> 00:12:30,875
it's really becoming really painful.

00:12:30,875 --> 00:12:33,080
And so then the question becomes,

00:12:33,080 --> 00:12:34,927
wouldn't it be easier if all the compute device

00:12:34,927 --> 00:12:37,360
you have on your system can use the same address spaces

00:12:37,360 --> 00:12:41,200
which means that via, for one pointers,

00:12:41,200 --> 00:12:44,020
you get the same physical memory across all your devices

00:12:44,020 --> 00:12:45,780
and we all agree what that pointer

00:12:45,780 --> 00:12:48,169
points to at every point in time.

00:12:48,169 --> 00:12:50,050
I mean, I think,

00:12:50,050 --> 00:12:51,750
I think it's kind of an abuse,

00:12:51,750 --> 00:12:54,210
yes it's definitely easier, you don't have to do any

00:12:54,210 --> 00:12:56,250
of this copy, you don't have to do device manual log,

00:12:56,250 --> 00:12:57,640
you don't have to do device copy,

00:12:57,640 --> 00:12:59,900
you don't have to do hey, what is the device pointer

00:12:59,900 --> 00:13:01,860
for that address and so on and so forth.

00:13:01,860 --> 00:13:04,460
So you know it's much easier if,

00:13:04,460 --> 00:13:06,170
if you don't have to worry about A,

00:13:06,170 --> 00:13:07,530
what is this device address space,

00:13:07,530 --> 00:13:09,540
what needs to be CPU other space

00:13:09,540 --> 00:13:12,120
and try to mix and match all of them.

00:13:12,120 --> 00:13:14,087
It's even worse when you have multiple devices.

00:13:14,087 --> 00:13:15,623
So I think you know, it's,

00:13:16,650 --> 00:13:18,127
it's kind of an obvious answer.

00:13:18,127 --> 00:13:20,180
And you might think well okay,

00:13:20,180 --> 00:13:22,520
so the solution is there,

00:13:22,520 --> 00:13:24,880
any sample you know we only have that extreme

00:13:24,880 --> 00:13:27,170
on some nouveau for instance, naming the AGP

00:13:27,170 --> 00:13:28,440
with the initial driver.

00:13:28,440 --> 00:13:32,320
We have that on some MDG CPU two.

00:13:32,320 --> 00:13:34,610
So it's something that does work, it does extreme,

00:13:34,610 --> 00:13:38,710
so you like, okay so maybe the, we all said, you know,

00:13:38,710 --> 00:13:41,810
the problem is fixed but there is one other thing

00:13:41,810 --> 00:13:44,410
we need to take into account is that

00:13:44,410 --> 00:13:48,220
nobody we have multiple type of physical memory.

00:13:48,220 --> 00:13:50,630
You have your DDR DIMM that you used to have

00:13:50,630 --> 00:13:52,390
so you know it's your main system memory.

00:13:52,390 --> 00:13:53,800
It's fast enough,

00:13:53,800 --> 00:13:56,203
around 60 gigabyte per second for instance.

00:13:57,880 --> 00:14:00,930
But you also see a new kind of type of memories

00:14:00,930 --> 00:14:03,840
popping up on CPU, you have have HBM memory

00:14:03,840 --> 00:14:05,816
which is really a fast memory.

00:14:05,816 --> 00:14:08,330
Depending on which CPU you're looking at,

00:14:08,330 --> 00:14:11,626
gonna be between 200 and one terabyte of bandwidth.

00:14:11,626 --> 00:14:14,677
And you also have device memory, device,

00:14:14,677 --> 00:14:17,060
many devices have their own local memories

00:14:17,060 --> 00:14:19,760
that is also much more, much more faster than

00:14:19,760 --> 00:14:21,410
what you get inside the memory.

00:14:21,410 --> 00:14:24,210
Again, the range is, you know, can go up to

00:14:24,210 --> 00:14:27,003
one terabyte of bandwidth, it's really, really fast.

00:14:28,500 --> 00:14:32,370
An ERV issue that you have all your device on the system

00:14:32,370 --> 00:14:35,500
or tech on tech visit troller and with a CPU to be expressed

00:14:35,500 --> 00:14:38,290
for instance, most common case in most systems.

00:14:38,290 --> 00:14:40,149
And that can really be a bottleneck,

00:14:40,149 --> 00:14:42,010
so you know that the PC express,

00:14:42,010 --> 00:14:44,630
if you have like 16 lane of PC express

00:14:44,630 --> 00:14:47,720
on the PC express free is gonna be 32 gigabyte of bandwidth.

00:14:47,720 --> 00:14:51,245
So it's of the bandwidth of memory which is kind of slow.

00:14:51,245 --> 00:14:53,515
So usually you want to,

00:14:53,515 --> 00:14:57,350
you know when you look at that, tell yourself,

00:14:57,350 --> 00:14:59,003
I want to use the fastest memory,

00:14:59,003 --> 00:15:03,430
a physical memory to back the range of virtual addresses.

00:15:03,430 --> 00:15:05,570
Remember the virtual addresses is pointing

00:15:05,570 --> 00:15:07,510
to some physical memory in periods of mapping

00:15:07,510 --> 00:15:10,450
between the virtual address and the physical memory.

00:15:10,450 --> 00:15:11,980
And so at any point in time,

00:15:11,980 --> 00:15:14,200
you want to use the fastest memory possible

00:15:14,200 --> 00:15:15,750
for whoever is using the work.

00:15:15,750 --> 00:15:16,930
So you know if you are on CPU,

00:15:16,930 --> 00:15:18,240
you want to use the fastest memory

00:15:18,240 --> 00:15:19,700
from the CPU point of view.

00:15:19,700 --> 00:15:21,760
If you are on GPU, you want to use the fastest memory

00:15:21,760 --> 00:15:23,683
from the GPU point of view.

00:15:23,683 --> 00:15:25,409
And if you are on the FPGA,

00:15:25,409 --> 00:15:26,242
you want to use the fastest memory

00:15:26,242 --> 00:15:28,513
from the FPGA point of view, and so on and so forth.

00:15:30,340 --> 00:15:33,504
And I was talking about at beginning that you know,

00:15:33,504 --> 00:15:36,463
the data set can move from one device to the other

00:15:36,463 --> 00:15:37,630
so when you stop working on the CPU,

00:15:37,630 --> 00:15:38,907
then you do something else on the GPU

00:15:38,907 --> 00:15:40,720
and then you do something else on the FPGA

00:15:40,720 --> 00:15:43,165
and then you, maybe you do something else

00:15:43,165 --> 00:15:45,465
on some network adapter, NRG may for instance.

00:15:46,370 --> 00:15:49,200
And so your data set is not, you know,

00:15:49,200 --> 00:15:51,080
always work on by just one computer.

00:15:51,080 --> 00:15:55,085
It can be work on by CPU, GPA, FPGA, and so on and so forth.

00:15:55,085 --> 00:15:57,960
And so what is fastest at one point in time

00:15:57,960 --> 00:16:00,730
might not be the fastest thing at the next point in time.

00:16:00,730 --> 00:16:02,863
When you move from CPU from CPU point of view,

00:16:02,863 --> 00:16:05,680
this memory might be faster than from GPU point of view.

00:16:05,680 --> 00:16:07,310
This other memory is faster.

00:16:07,310 --> 00:16:11,640
And it just exactly NUMA all over again.

00:16:11,640 --> 00:16:13,170
For people that aren't familiar with NUMA,

00:16:13,170 --> 00:16:14,920
NUMA is when you have multiple CPU,

00:16:15,776 --> 00:16:18,960
usually it used to be CPU so I got post with what NUMA,

00:16:18,960 --> 00:16:20,960
what it is, it's gonna be CPU on the send out.

00:16:20,960 --> 00:16:23,540
So you had multiple CPU and they are connected to each other

00:16:23,540 --> 00:16:26,910
with a bus, and in time when CPU need to access the memory,

00:16:26,910 --> 00:16:30,320
that is actually so, each memory is tied to one CPU.

00:16:30,320 --> 00:16:32,530
And when you add one CPU, you need to access the memories

00:16:32,530 --> 00:16:34,550
that is tied to another CPU.

00:16:34,550 --> 00:16:37,700
That CPU have to actually go talk to the second CPU

00:16:37,700 --> 00:16:40,010
and this talk usually takes time,

00:16:40,010 --> 00:16:43,080
and add on C is a bottleneck because of bandwidth

00:16:43,080 --> 00:16:46,500
between two CPU is not as fast as accessing local memory.

00:16:46,500 --> 00:16:47,333
So,

00:16:50,260 --> 00:16:52,400
what it means really to be able to use the fastest memory

00:16:52,400 --> 00:16:54,530
at any point in time means, it means that you have to

00:16:54,530 --> 00:16:57,070
be able to migrate the physical memory

00:16:57,070 --> 00:17:00,270
backing of virtual addresses from one type of memory

00:17:00,270 --> 00:17:02,180
to another type of memory.

00:17:02,180 --> 00:17:05,530
And you know, it's something we have been doing for NUMA

00:17:05,530 --> 00:17:09,492
and so for that reason, we have a bunch of Cisco in API

00:17:09,492 --> 00:17:12,380
that does all the need to do that on Linux.

00:17:12,380 --> 00:17:14,770
And I'm just listing the three we have.

00:17:14,770 --> 00:17:16,950
So we have migrate pages, we have move pages

00:17:16,950 --> 00:17:19,800
and we have mbind, memory bind really,

00:17:19,800 --> 00:17:22,190
it's binding memory and I just want to go over each of them

00:17:22,190 --> 00:17:24,160
and look at what they look like.

00:17:24,160 --> 00:17:27,220
So migrate pages is really, it's kind of a big hammer.

00:17:27,220 --> 00:17:30,254
It's really simple, you get the process ID,

00:17:30,254 --> 00:17:34,130
you, you give it at least of nodes so you know,

00:17:34,130 --> 00:17:36,731
max nodes is just the number of nodes you're gonna give him.

00:17:36,731 --> 00:17:39,410
And you say okay I want to go from that node

00:17:39,410 --> 00:17:42,130
from the noma node to this new noma nodes.

00:17:42,130 --> 00:17:44,390
And it's gonna move all the pages at once

00:17:44,390 --> 00:17:46,730
from one set of nodes to another set of nodes.

00:17:46,730 --> 00:17:49,130
So this kind of Cisco have been really done

00:17:49,130 --> 00:17:51,990
because when you have noma system sometime you want to

00:17:51,990 --> 00:17:53,940
move a process from a set of CPU's

00:17:53,940 --> 00:17:56,550
and from one set of course to another set of course

00:17:56,550 --> 00:17:58,710
because you know trying to node balance your workload

00:17:58,710 --> 00:18:00,340
across multiple cores so you know,

00:18:00,340 --> 00:18:01,500
you don't want one core to sit,

00:18:01,500 --> 00:18:03,060
I don't know when you have one core that have

00:18:03,060 --> 00:18:05,830
ton of processes working on the same course.

00:18:05,830 --> 00:18:07,530
So this is kind of the big hammer.

00:18:08,501 --> 00:18:10,050
And for the kind of work line I was talking about,

00:18:10,050 --> 00:18:11,810
you know when you have something running on CPU,

00:18:11,810 --> 00:18:12,710
something running on GPU

00:18:12,710 --> 00:18:15,114
and all that happening concurrently,

00:18:15,114 --> 00:18:15,947
multiple threads and so on and so forth,

00:18:15,947 --> 00:18:18,210
you basically don't want to re-wait the whole process

00:18:18,210 --> 00:18:20,597
that I set from one thing to the other.

00:18:20,597 --> 00:18:25,155
You just want to migrate range of your virtual addresses

00:18:25,155 --> 00:18:27,495
the range that is work on GPU, want it to move to GPU

00:18:27,495 --> 00:18:29,580
and the range that work on FPGA,

00:18:29,580 --> 00:18:31,967
you want it move to FPGA fastest memory.

00:18:31,967 --> 00:18:35,395
So it's not the one we're looking to, we want to have.

00:18:35,395 --> 00:18:37,330
The next one is move pages.

00:18:37,330 --> 00:18:39,240
So again you gave a processed ID

00:18:39,240 --> 00:18:42,170
and you gave a list of virtual addresses,

00:18:42,170 --> 00:18:44,190
pages that actually list the virtual addresses.

00:18:44,190 --> 00:18:46,040
And you say I want to immigrate

00:18:46,040 --> 00:18:47,460
every single virtual addresses

00:18:47,460 --> 00:18:50,170
and it's just one page at at time.

00:18:50,170 --> 00:18:53,600
I want to move them to this new set of node.

00:18:53,600 --> 00:18:56,985
And so, it does move individual pages

00:18:56,985 --> 00:18:59,560
and so it's really, you know it's highly flexible.

00:18:59,560 --> 00:19:01,730
You can A, you can say I want to move that page

00:19:01,730 --> 00:19:03,720
and that other pages and that pages

00:19:03,720 --> 00:19:05,340
and I want to move them there.

00:19:06,600 --> 00:19:08,710
The issue is that the kind of,

00:19:08,710 --> 00:19:11,040
the kind of data set we're looking into,

00:19:11,040 --> 00:19:14,470
node is really big, we're talking about images and all that

00:19:14,470 --> 00:19:16,970
that can take multiple gigabyte of memory,

00:19:16,970 --> 00:19:18,270
and we're talking about, you know,

00:19:18,270 --> 00:19:21,480
when you do inferences and EI, your data set input

00:19:21,480 --> 00:19:24,840
is usually in multiple gigabyte, you know 32, 64 gigabyte.

00:19:24,840 --> 00:19:29,176
So it's a lot of pages really and if you at all match pages

00:19:29,176 --> 00:19:30,390
that you have to move, just one gigabyte,

00:19:30,390 --> 00:19:31,890
it's a staggering amount.

00:19:31,890 --> 00:19:34,170
And so if every time you have to,

00:19:34,170 --> 00:19:37,487
you have to build this array of pages, pointers

00:19:37,487 --> 00:19:38,630
for every time you want to move a gigabyte of memory,

00:19:38,630 --> 00:19:42,038
it's a long and time-consuming thing to do on a CPU.

00:19:42,038 --> 00:19:44,730
And yeah, cherry picking is really,

00:19:44,730 --> 00:19:45,750
it's really not efficient.

00:19:45,750 --> 00:19:49,830
It's not, so it doesn't look exactly what we want to do.

00:19:49,830 --> 00:19:51,705
The last one is memory bind.

00:19:51,705 --> 00:19:54,261
So this one take a virtual addresses,

00:19:54,261 --> 00:19:57,950
the size, so it's, you know just basically say

00:19:57,950 --> 00:20:00,600
I want to run that range of virtual addresses

00:20:00,600 --> 00:20:03,540
and then I want to move that range to these set of nodes

00:20:03,540 --> 00:20:06,480
so the node mask is actually a bit map,

00:20:06,480 --> 00:20:08,830
and every time you set a bit inside that bit map,

00:20:08,830 --> 00:20:11,470
it means that that node that corresponds to that bit

00:20:11,470 --> 00:20:12,840
is one of the memories

00:20:12,840 --> 00:20:15,723
where you might want to move that memory.

00:20:17,750 --> 00:20:18,583
So,

00:20:19,770 --> 00:20:22,300
you know it's much more what looks like

00:20:22,300 --> 00:20:24,700
what we want to do, so we want to be able to arrange

00:20:24,700 --> 00:20:27,530
a range of virtual race to offset of one of nodes.

00:20:27,530 --> 00:20:31,000
Really it looks quite closely to what we want to achieve

00:20:31,000 --> 00:20:34,713
to be able to migrate memory to device memory.

00:20:36,428 --> 00:20:39,053
There is few, few go check with that one.

00:20:41,380 --> 00:20:45,667
One other thing is that when you move to some device memory,

00:20:45,667 --> 00:20:47,610
this device memory is a constraint of rules.

00:20:47,610 --> 00:20:51,130
You know you might have like 16 gigabyte of device memory

00:20:51,130 --> 00:20:53,050
but your main memory is one terabyte of memory.

00:20:53,050 --> 00:20:54,400
And your data set might not fit

00:20:54,400 --> 00:20:55,850
inside the device memory at all

00:20:55,850 --> 00:20:58,750
so you might have other processes using the device memory.

00:20:59,790 --> 00:21:02,080
When you're in program, it's all about sharing resources

00:21:02,080 --> 00:21:05,340
across the board, and so what you really want to

00:21:05,340 --> 00:21:08,500
be able to do, you want to be able to have an order list

00:21:08,500 --> 00:21:11,875
of preferences where move your memory you say.

00:21:11,875 --> 00:21:14,900
For this one for virtual address, I would like for us to use

00:21:14,900 --> 00:21:16,430
the fastest memory for the GPU

00:21:16,430 --> 00:21:18,740
and you know that the second-fastest memory

00:21:18,740 --> 00:21:20,560
gonna be that one and so you say well,

00:21:20,560 --> 00:21:23,710
try this one first, if you run out of the first one,

00:21:23,710 --> 00:21:25,490
try this other second physical memory

00:21:25,490 --> 00:21:28,220
and if you run out of second one, try the third one.

00:21:28,220 --> 00:21:30,550
You know you have this kind of fall back list

00:21:30,550 --> 00:21:33,360
where you say I want to go first try to use the fastest one

00:21:33,360 --> 00:21:35,910
and then the second fastest and so on and so forth.

00:21:37,030 --> 00:21:40,000
So you know, the nodemask thing is kind of a wasteful thing

00:21:40,000 --> 00:21:42,060
because it doesn't give you an order, just a bit map

00:21:42,060 --> 00:21:43,660
that just say this is a list of things

00:21:43,660 --> 00:21:46,540
where you can migrate memory but it's not an order list.

00:21:46,540 --> 00:21:49,373
You don't know which one is the preferential one.

00:21:50,760 --> 00:21:51,643
So,

00:21:52,970 --> 00:21:55,193
another issue we have is that,

00:21:57,412 --> 00:21:58,670
node is really a new Mac concept

00:21:58,670 --> 00:22:02,260
and node is really tied to CPU and CPU core and CPU socket.

00:22:02,260 --> 00:22:03,600
It's for its recall reason

00:22:03,600 --> 00:22:05,643
because that's where NUMA did start.

00:22:07,140 --> 00:22:10,000
A node completely failed to capture the complexity

00:22:10,000 --> 00:22:12,910
you find inside a node, so when you have a CPU,

00:22:12,910 --> 00:22:15,451
your CPU might be connected to multiple devices.

00:22:15,451 --> 00:22:18,080
You know, if you're just look at your desktop

00:22:18,080 --> 00:22:21,240
or your laptop, just one more node basically and you have,

00:22:21,240 --> 00:22:25,380
you know you have your GPU, you have your SIM card,

00:22:25,380 --> 00:22:28,166
you have your network, you have your WiFi control

00:22:28,166 --> 00:22:29,370
and so on and so forth.

00:22:29,370 --> 00:22:33,210
And inside each node you also have different kind of memory

00:22:33,210 --> 00:22:36,320
so you have your CPU main memory, your DDR DIMM.

00:22:36,320 --> 00:22:38,360
You also have your CPU HBM memory

00:22:38,360 --> 00:22:40,887
so it's on a stack on top of CPU.

00:22:40,887 --> 00:22:43,293
You could have personal memory,

00:22:43,293 --> 00:22:45,960
what we're seeing popping up now.

00:22:45,960 --> 00:22:47,243
And personal memory is kind of the memory

00:22:47,243 --> 00:22:49,670
that is slightly slower than your main memory

00:22:49,670 --> 00:22:50,927
but it's much bigger usually

00:22:50,927 --> 00:22:53,840
so you know you can easily get one drawback done

00:22:53,840 --> 00:22:55,895
of personal memory.

00:22:55,895 --> 00:22:59,880
So instead of giving a node, basically we want to

00:22:59,880 --> 00:23:02,130
be able to specify a physical memory directly.

00:23:02,130 --> 00:23:03,770
We don't want to see this node,

00:23:03,770 --> 00:23:06,510
we want to say inside that node this physical memory.

00:23:06,510 --> 00:23:10,902
So we want to be able to point out the exact physical memory

00:23:10,902 --> 00:23:12,940
and not just an older node where there is

00:23:12,940 --> 00:23:15,090
many many different things inside the node.

00:23:16,260 --> 00:23:20,560
So what we really want, at least from my point of view,

00:23:20,560 --> 00:23:22,186
is this kind of cease code.

00:23:22,186 --> 00:23:24,610
So the name really came be anything,

00:23:24,610 --> 00:23:25,990
I don't mind the naming.

00:23:25,990 --> 00:23:28,410
So it's exactly like mbind pretty much,

00:23:28,410 --> 00:23:31,062
so you give all virtual addresses, you give a size

00:23:31,062 --> 00:23:35,587
and then you give the node really so you,

00:23:35,587 --> 00:23:37,526
the node is kind of flags here so you can,

00:23:37,526 --> 00:23:41,490
really doesn't matter but it's the same thing as mbind

00:23:41,490 --> 00:23:43,055
as a node argument too.

00:23:43,055 --> 00:23:45,910
And then you list, you give a list, an order list

00:23:45,910 --> 00:23:47,905
of physical memory ID's

00:23:47,905 --> 00:23:51,692
where you actually want to migrate the thing.

00:23:51,692 --> 00:23:53,600
And that list is just so you say

00:23:53,600 --> 00:23:55,951
the first one entering the list to the first one

00:23:55,951 --> 00:23:56,959
you want migrate the range,

00:23:56,959 --> 00:23:58,590
so this is really the one you want to use

00:23:58,590 --> 00:24:00,370
and the second one is like, well,

00:24:00,370 --> 00:24:02,047
if you run out of first one,

00:24:02,047 --> 00:24:04,030
try to allocate on the second one

00:24:04,030 --> 00:24:05,290
and if you run out of second one

00:24:05,290 --> 00:24:07,763
try to look in the third one and so on and so forth.

00:24:09,320 --> 00:24:12,550
So you know it's,

00:24:12,550 --> 00:24:15,260
it can also be a replacement for mbind really because

00:24:15,260 --> 00:24:17,120
it really can do the same thing as mbind.

00:24:17,120 --> 00:24:19,150
And usually when you have mbind and when you look at

00:24:19,150 --> 00:24:21,530
the mbind Cisco and if you do a system praise

00:24:21,530 --> 00:24:24,330
on most of the many workloads you will see that

00:24:24,330 --> 00:24:26,100
mbind usually have a node mass

00:24:26,100 --> 00:24:28,900
because only very few bit sets, so the size of the

00:24:28,900 --> 00:24:31,220
physical memory ID will be really small

00:24:31,220 --> 00:24:33,460
because most of the time you know people, we don't,

00:24:33,460 --> 00:24:34,810
we don't go and do mbind

00:24:34,810 --> 00:24:37,047
and try to use all the node basically we just say,

00:24:37,047 --> 00:24:39,590
hey go do mbind I'm gonna use this and this node

00:24:39,590 --> 00:24:42,040
you know only like a, I'm full of node basically.

00:24:43,620 --> 00:24:46,115
So this is really what I want to get at,

00:24:46,115 --> 00:24:48,559
this is kind of Cisco would like to get inside the links

00:24:48,559 --> 00:24:52,239
cannot all be able to do a physical memory migration.

00:24:52,239 --> 00:24:55,190
And one thing that is missing here is

00:24:55,190 --> 00:24:58,240
we need to be available to identify the physical memory

00:24:58,240 --> 00:25:00,573
which includes all device memory.

00:25:00,573 --> 00:25:03,364
And what we have today in sudden use canal,

00:25:03,364 --> 00:25:07,030
so we have like this directory where you can look at ID

00:25:07,030 --> 00:25:09,880
for NUMA system basically.

00:25:09,880 --> 00:25:12,160
So it's Cis device system memory.

00:25:12,160 --> 00:25:15,270
It's kind of breaks suddenly, so you will get a file,

00:25:15,270 --> 00:25:19,500
and ID for every 128 megabyte of memory on x86

00:25:19,500 --> 00:25:22,151
so it means if you have like a,

00:25:22,151 --> 00:25:27,151
64 gigabyte you will have a 512 ID for every 64 gigabyte

00:25:27,520 --> 00:25:29,890
but that 64 gigabyte is the same kind of memory

00:25:29,890 --> 00:25:31,700
so you know it's kind of a waste to have so many ID

00:25:31,700 --> 00:25:32,533
for the same memory.

00:25:32,533 --> 00:25:35,990
It's a, it was done for some weird m m reason.

00:25:35,990 --> 00:25:37,610
At a time it looks like a good idea

00:25:37,610 --> 00:25:40,400
but I think everybody kind of agrees it was really

00:25:40,400 --> 00:25:41,900
one of the worst thing we did.

00:25:44,330 --> 00:25:45,163
So,

00:25:46,470 --> 00:25:49,490
and also there is another thing to know about is that

00:25:49,490 --> 00:25:51,870
we have a lot of existing programs running today

00:25:51,870 --> 00:25:54,047
and this program look inside that directory

00:25:54,047 --> 00:25:56,670
and we look at the ID inside that directory

00:25:56,670 --> 00:26:00,780
and we expect that this ID match memory with

00:26:00,780 --> 00:26:03,720
some kind of expectation which is like the memories

00:26:03,720 --> 00:26:06,028
cache coherency the memory can do automatic

00:26:06,028 --> 00:26:08,510
and so on and so forth, you know it's your main memory

00:26:08,510 --> 00:26:11,220
so people have been used to have this main memory

00:26:11,220 --> 00:26:13,030
can do all these kind of things.

00:26:13,030 --> 00:26:15,564
The thing with device memory is the device memory might not

00:26:15,564 --> 00:26:19,770
actually have the same memory model as your main memory.

00:26:19,770 --> 00:26:22,814
For instance, device memory may not be cache coherent.

00:26:22,814 --> 00:26:23,940
So that means that when the CPU

00:26:23,940 --> 00:26:25,640
don't access the device memory,

00:26:25,640 --> 00:26:26,870
it cannot to cache coherency,

00:26:26,870 --> 00:26:28,500
you don't have any cache coherency.

00:26:28,500 --> 00:26:30,490
Obviously from a CPU thread point of view,

00:26:30,490 --> 00:26:31,800
it's really kind of a,

00:26:31,800 --> 00:26:33,610
you know it kind of break your programming model

00:26:33,610 --> 00:26:36,660
from you CoC++ program model.

00:26:36,660 --> 00:26:38,330
You also have lot of issue with atomics

00:26:38,330 --> 00:26:41,680
and so on and so forth so obviously if you start

00:26:41,680 --> 00:26:43,637
to put device memory inside that directory,

00:26:43,637 --> 00:26:45,840
and you start to put, and you have all program

00:26:45,840 --> 00:26:48,520
that are not aware of that, we might start to use

00:26:48,520 --> 00:26:50,400
the device memory and so many things will break

00:26:50,400 --> 00:26:52,020
because device memory you cannot do

00:26:52,020 --> 00:26:54,100
atomics and cache coherency and their program will break

00:26:54,100 --> 00:26:55,890
in a weird way, and we don't understand why

00:26:55,890 --> 00:26:57,210
and we're gonna complain.

00:26:57,210 --> 00:26:59,830
So obviously we cannot suddenly not reuse the system,

00:26:59,830 --> 00:27:03,563
system memory as it is, we need something new.

00:27:05,330 --> 00:27:06,163
So we need,

00:27:07,710 --> 00:27:10,690
we need some kind of, so the way the thing works

00:27:10,690 --> 00:27:12,550
inside and is not for that

00:27:12,550 --> 00:27:14,240
what we call the device driver model.

00:27:14,240 --> 00:27:17,740
So we need a new device driver model for the physical memory

00:27:17,740 --> 00:27:19,260
and we want something really simple,

00:27:19,260 --> 00:27:21,810
we want one ID by per type of memory

00:27:21,810 --> 00:27:25,650
so you know if you have system for gigabyte of DIMM

00:27:25,650 --> 00:27:27,850
inside your system, we want one ID for that.

00:27:27,850 --> 00:27:31,707
If you have a 30, 32 gigabyte of HBM on your CPU,

00:27:31,707 --> 00:27:33,160
you want an ID for that.

00:27:33,160 --> 00:27:34,860
If you have another NUMA node,

00:27:34,860 --> 00:27:37,070
with another one terabyte of memory,

00:27:37,070 --> 00:27:39,200
you want an ID for that and so on and so forth.

00:27:39,200 --> 00:27:42,139
So you know we want an ID per type of memory.

00:27:42,139 --> 00:27:44,920
And each node can have multiple ID because each node

00:27:44,920 --> 00:27:46,420
can have multiple type of memory you know

00:27:46,420 --> 00:27:49,487
one node was HBM, DDR, personal memory

00:27:49,487 --> 00:27:52,150
and we also, it's the same for device.

00:27:52,150 --> 00:27:54,990
So when you have a device that is inside the node

00:27:54,990 --> 00:27:57,950
attached to the CPU, you know that device itself

00:27:57,950 --> 00:28:01,160
can have HBM, can have DDR, can have personal memory

00:28:01,160 --> 00:28:02,799
and so on and so forth.

00:28:02,799 --> 00:28:05,620
So you know we want some kind of new directory

00:28:05,620 --> 00:28:08,625
so again, the naming I don't care about the naming really.

00:28:08,625 --> 00:28:12,000
And inside, so inside that directory mainly what you have

00:28:12,000 --> 00:28:15,340
is one file for every device, every memory,

00:28:15,340 --> 00:28:16,513
every physical memory.

00:28:17,380 --> 00:28:19,890
The name of the file is just VIV actually.

00:28:19,890 --> 00:28:23,710
And inside that thing you get a link to the node.

00:28:23,710 --> 00:28:26,660
So the node so, one other thing is that

00:28:26,660 --> 00:28:29,840
every CPU is on a node and every device is on a node

00:28:29,840 --> 00:28:32,350
because every device is connected to a CPU so

00:28:32,350 --> 00:28:35,230
you always have these kind of links between the two.

00:28:35,230 --> 00:28:37,500
And you also, if the memory you're looking at

00:28:37,500 --> 00:28:39,443
is also a memory from a device memory

00:28:39,443 --> 00:28:41,410
then you also want to have a link to the device

00:28:41,410 --> 00:28:43,410
so you know A, this memory belongs to that device.

00:28:43,410 --> 00:28:46,020
You want to be able to know what kind of connection.

00:28:46,020 --> 00:28:48,310
And you can also want, definitely want to have mention

00:28:48,310 --> 00:28:50,800
formation, like what is the bandwidth, what is the latency

00:28:50,800 --> 00:28:52,240
so you can compare so you know,

00:28:52,240 --> 00:28:54,810
when you have two memory on the device

00:28:54,810 --> 00:28:57,060
and you want to be able to know A, which is the fastest,

00:28:57,060 --> 00:28:59,610
what is, this one is the lowest latency,

00:28:59,610 --> 00:29:01,880
this one is the fastest and so on and so forth.

00:29:01,880 --> 00:29:04,100
So you want to have this kind of formation available

00:29:04,100 --> 00:29:05,530
to the application.

00:29:05,530 --> 00:29:06,793
So this is what,

00:29:07,740 --> 00:29:09,950
these are the two thing I really want to get to

00:29:09,950 --> 00:29:12,760
and I've been working then, posting bunch of patches

00:29:12,760 --> 00:29:15,183
around that topics so you know,

00:29:16,200 --> 00:29:20,473
two things is a new Cisco need a bunch of new ID's

00:29:20,473 --> 00:29:24,380
and so the ID's, the ID's side of thing is that thing

00:29:24,380 --> 00:29:27,870
so you know a new directory with physical memory ID

00:29:27,870 --> 00:29:31,705
and view all other Cisco to be able to migrate memory.

00:29:31,705 --> 00:29:34,650
And that's really all I have on that front

00:29:34,650 --> 00:29:38,513
and what I want to ask is like, anybody against anything or?

00:29:40,259 --> 00:29:42,410
(microphone dropping)

00:29:42,410 --> 00:29:45,633
- Just on the last bit of device memory.

00:29:47,350 --> 00:29:51,080
That seems fine when you've got something static.

00:29:51,080 --> 00:29:53,970
What happens if the device disappears, do you like,

00:29:53,970 --> 00:29:56,610
have I, if I have an application and I've read

00:29:56,610 --> 00:30:00,350
my system device memory ID list and I've decided I wanna use

00:30:00,350 --> 00:30:03,380
these ID's and then the device disappears, how do I find,

00:30:03,380 --> 00:30:05,900
like is there gonna be, do you have a notification method

00:30:05,900 --> 00:30:08,530
or something that tells you when you have memory ID's appear

00:30:08,530 --> 00:30:10,540
or some of the ones you're using disappear because

00:30:10,540 --> 00:30:13,820
I could see if a user-space prof is reading this once,

00:30:13,820 --> 00:30:15,950
traveling along in its way and then finding out in like

00:30:15,950 --> 00:30:17,980
four days that the graphics card went away

00:30:17,980 --> 00:30:19,590
and then trying to still use it and getting you know,

00:30:19,590 --> 00:30:22,130
inval or RPO, just...

00:30:22,130 --> 00:30:23,740
- Yeah so right now we have the same issue

00:30:23,740 --> 00:30:26,800
with the existing directory of exiting FS

00:30:26,800 --> 00:30:28,990
so you can have the file that match

00:30:30,791 --> 00:30:33,510
DDR DIMM disappearing on you.

00:30:33,510 --> 00:30:36,578
And suddenly the same ID can be reused for a new CPU

00:30:36,578 --> 00:30:38,860
so it can kind of pop up at different places

00:30:38,860 --> 00:30:42,050
for different memory, so there is already this issue

00:30:42,050 --> 00:30:44,610
but we can use bunch of system Cisco where you know,

00:30:44,610 --> 00:30:48,280
you get a file event whenever the file goes away.

00:30:48,280 --> 00:30:50,280
So you know there is this, there is a way to fix that

00:30:50,280 --> 00:30:52,510
from a file system point of view.

00:30:52,510 --> 00:30:54,910
It's not fully hook up last time I checked.

00:30:54,910 --> 00:30:57,840
So right now I don't think, I don't thinK you get a signal

00:30:57,840 --> 00:31:01,500
when inside system memory when one of the device goes,

00:31:01,500 --> 00:31:03,100
one of the directory goes away.

00:31:03,100 --> 00:31:04,360
I think there is no signal right now.

00:31:04,360 --> 00:31:06,580
- Yeah I don't, I'm more worried about

00:31:06,580 --> 00:31:08,960
establishing a new order where we just like,

00:31:08,960 --> 00:31:11,320
people write applications that assume it's static

00:31:11,320 --> 00:31:13,540
because they will if it is and if they're not given

00:31:13,540 --> 00:31:16,640
an upfront way to check, they'll write the first,

00:31:16,640 --> 00:31:19,510
five years application and all be dealing with an S Static.

00:31:19,510 --> 00:31:23,040
- Yeah so my solution to that is to never recycle ID's.

00:31:23,040 --> 00:31:26,480
So when you give an ID, that ID will never be used again.

00:31:26,480 --> 00:31:28,790
So you know it was used once for that physical memory

00:31:28,790 --> 00:31:31,040
but it will never be used again so that you know,

00:31:31,040 --> 00:31:34,138
if you got a Cisco from an application that used that ID

00:31:34,138 --> 00:31:36,540
you say well, you have an old view of the world

00:31:36,540 --> 00:31:38,020
but I'm just gonna ignore it.

00:31:38,020 --> 00:31:39,590
- How do you, like making sure that there's

00:31:39,590 --> 00:31:42,120
some defined error code that return saying

00:31:42,120 --> 00:31:45,625
your ID of the world is bad, please look at it again.

00:31:45,625 --> 00:31:46,483
- Yeah.

00:31:48,140 --> 00:31:50,510
- [Man In Crowd] Defining the binding error codes I suppose

00:31:50,510 --> 00:31:52,780
to be a bit more than just e inval.

00:31:52,780 --> 00:31:57,018
- Yeah like something, yeah so one other Cisco actually

00:31:57,018 --> 00:31:58,183
this one,

00:31:59,083 --> 00:32:00,930
this one morph pages,

00:32:00,930 --> 00:32:02,230
the stages are very basically

00:32:02,230 --> 00:32:05,500
give you a bunch of stages about what was the end result

00:32:05,500 --> 00:32:08,248
of the remuneration so we can have the same kind of thing

00:32:08,248 --> 00:32:09,960
basically say hey, this is the list of things

00:32:09,960 --> 00:32:13,140
that can happen and here is what happened in the end.

00:32:22,560 --> 00:32:24,197
- First of all I would like to thank you

00:32:24,197 --> 00:32:25,913
for your research.

00:32:27,340 --> 00:32:29,790
Now, listening to that, I'm returning to

00:32:29,790 --> 00:32:31,810
the early days of the micro controllers

00:32:31,810 --> 00:32:35,660
where we had separate memory spaces for flash,

00:32:35,660 --> 00:32:40,660
for onchipram, for external ram, whatever it was.

00:32:41,270 --> 00:32:46,270
So from this point of view the illustration here

00:32:46,590 --> 00:32:48,510
seems very similar.

00:32:48,510 --> 00:32:53,510
That is because when the memory space was designed,

00:32:54,070 --> 00:32:58,730
there were even no idea about PCIE for example

00:32:58,730 --> 00:33:03,580
where your end point device may become buzzmaster.

00:33:03,580 --> 00:33:06,373
And request to transfer memory.

00:33:07,630 --> 00:33:12,630
Now they seems that we need significant changes, you say.

00:33:14,120 --> 00:33:17,930
The problem that I see, first this for example,

00:33:17,930 --> 00:33:21,980
the PCI devices which has limited memory window

00:33:21,980 --> 00:33:25,140
compared to the base address register

00:33:25,140 --> 00:33:29,400
so they cannot see the whole memory of the system.

00:33:29,400 --> 00:33:33,030
They would need to continue assistance

00:33:33,030 --> 00:33:35,747
from the microprocessor,

00:33:35,747 --> 00:33:38,490
programming the base address register to be able

00:33:38,490 --> 00:33:41,290
to see the memory which they want to see.

00:33:41,290 --> 00:33:44,850
- So you're talking about the device accessing the m memory.

00:33:44,850 --> 00:33:45,683
- [Man In Crowd] Yeah.

00:33:45,683 --> 00:33:47,670
- So that's actually, there is no,

00:33:47,670 --> 00:33:49,010
so it's only the other way around.

00:33:49,010 --> 00:33:51,020
It's when the CPU access the device memory,

00:33:51,020 --> 00:33:52,587
then you have the PC express bar.

00:33:52,587 --> 00:33:55,040
The other way around is always, it's called a,

00:33:56,768 --> 00:34:00,137
TLP, TLD, so it's a package, so the PC express

00:34:00,137 --> 00:34:02,760
is a packet particle, so it's kind of a network actually

00:34:02,760 --> 00:34:05,408
so when the device want to access m memory,

00:34:05,408 --> 00:34:07,237
it sends the packet to the CPU.

00:34:07,237 --> 00:34:09,380
Then that package has a virtual address,

00:34:09,380 --> 00:34:11,620
a physical, device base address actually.

00:34:11,620 --> 00:34:14,550
And that address can be translated by an IO menu

00:34:14,550 --> 00:34:17,005
if you have an IO menu-- - We don't have,

00:34:17,005 --> 00:34:19,170
- If you have an IO menu, if you don't have an IO menu

00:34:19,170 --> 00:34:21,270
and if you want, so basically if you want to be

00:34:21,270 --> 00:34:25,380
PCI press compliance, the device must be able to access

00:34:25,380 --> 00:34:27,070
any physical addresses.

00:34:27,070 --> 00:34:31,007
- Yeah it have to be able to access any particular address

00:34:31,007 --> 00:34:34,670
but it should have programmed the base address register

00:34:34,670 --> 00:34:36,883
for that, right? - No.

00:34:37,910 --> 00:34:40,090
That's for the wiring, the base address register is when,

00:34:40,090 --> 00:34:42,723
is when the CPU is sitting at PC express bar.

00:34:43,930 --> 00:34:46,512
So it's when you're going from CPU trying to access

00:34:46,512 --> 00:34:47,684
device memory.

00:34:47,684 --> 00:34:50,580
But if you going from the device, which can be anything,

00:34:50,580 --> 00:34:52,830
so the device want to access main memory,

00:34:52,830 --> 00:34:55,510
then it goes from the device to the CPU.

00:34:55,510 --> 00:34:57,550
And in that other direction,

00:34:57,550 --> 00:34:59,630
if you are PC express compliant,

00:34:59,630 --> 00:35:01,450
PC express three compliant I think,

00:35:01,450 --> 00:35:04,324
it's monitory with PC express three I think.

00:35:04,324 --> 00:35:08,370
Then you need to be able to access any physical memory.

00:35:08,370 --> 00:35:09,940
- Okay.

00:35:09,940 --> 00:35:14,940
So basically, we really need such effort.

00:35:15,340 --> 00:35:20,340
I think that but people will also appreciate it greatly

00:35:20,640 --> 00:35:24,406
because there are a lot of back scaling issues,

00:35:24,406 --> 00:35:29,406
bandwidth calculation done in the back world

00:35:29,550 --> 00:35:33,980
which highlights to the memory buses.

00:35:33,980 --> 00:35:35,820
- So the issue with bandwidth,

00:35:35,820 --> 00:35:37,960
when you report the bandwidth number,

00:35:37,960 --> 00:35:39,800
you know it's the best case bandwidth really

00:35:39,800 --> 00:35:41,970
because you know other thing might be happening on the bus,

00:35:41,970 --> 00:35:44,380
multiple device might be trying to access the same memory

00:35:44,380 --> 00:35:45,600
and so on and so forth.

00:35:45,600 --> 00:35:48,970
So when you report the bandwidth of any links between

00:35:48,970 --> 00:35:52,810
any kind of two device, CPU device or two devices together,

00:35:52,810 --> 00:35:54,150
it's the best case scenario.

00:35:54,150 --> 00:35:55,870
So what you're telling the people,

00:35:55,870 --> 00:35:59,040
in the best case if you're the only one using that processes

00:35:59,040 --> 00:36:01,923
that the best thing you might be able to achieve.

00:36:03,100 --> 00:36:05,710
So I don't know if that answer what--

00:36:05,710 --> 00:36:09,010
- Yeah yeah, just this work will be appreciated by them.

00:36:09,010 --> 00:36:12,490
- Yeah, I believe you know I'm not doing that because

00:36:12,490 --> 00:36:14,790
everything is cool, I believe there is a lot of people

00:36:14,790 --> 00:36:16,690
that would like to see something like that

00:36:16,690 --> 00:36:18,810
and what I really want to get at is like

00:36:18,810 --> 00:36:21,350
do people like V Cisco and, you know,

00:36:23,547 --> 00:36:26,407
do you like that kind of Cisco and do you,

00:36:26,407 --> 00:36:28,826
do you like also to be able to have a new way to,

00:36:28,826 --> 00:36:31,830
I don't know, defy every physical memory on your system.

00:36:31,830 --> 00:36:34,320
Because right now there is no way to identify

00:36:34,320 --> 00:36:35,610
every physical memory, like no common way.

00:36:35,610 --> 00:36:38,630
So if you want to know the, if you wanted to use

00:36:38,630 --> 00:36:41,658
some GPU memory you will have to used to this GPU driver

00:36:41,658 --> 00:36:43,655
and if you want to use FPGA then you will have to use

00:36:43,655 --> 00:36:45,532
to talk to this FPGA driver.

00:36:45,532 --> 00:36:48,218
And every driver have it's own API,

00:36:48,218 --> 00:36:50,900
it's on O-U-C-K-L and so on and so forth so,

00:36:50,900 --> 00:36:54,803
you have to learn to speak 10, 20, 50 languages

00:36:54,803 --> 00:36:57,660
before being able to use all of these devices together

00:36:57,660 --> 00:37:01,300
and what I wanted to go, to have is some kind of Cisco,

00:37:01,300 --> 00:37:03,680
let's say this is a common language for everybody

00:37:03,680 --> 00:37:05,320
and if you want to talk with everybody,

00:37:05,320 --> 00:37:07,770
if you have this common languages and people can,

00:37:07,770 --> 00:37:09,860
you know, we don't have to, program don't have to

00:37:09,860 --> 00:37:12,330
learn about all the languages basically.

00:37:12,330 --> 00:37:15,372
- Jerome, should you take a vote?

00:37:15,372 --> 00:37:17,622
(laughing)

00:37:21,020 --> 00:37:22,970
- I know you're gonna hate me for this.

00:37:24,310 --> 00:37:28,770
So this is fine if you're just moving linear memory around

00:37:28,770 --> 00:37:30,230
but what happens if you have a device

00:37:30,230 --> 00:37:33,200
who has a tiling axis pattern that makes the device memory

00:37:34,772 --> 00:37:35,605
twice as fast if you use the tiling axis pattern.

00:37:35,605 --> 00:37:38,610
How do you intend to expose something like that or...

00:37:38,610 --> 00:37:40,750
- So you mean tiling from the physical point of view

00:37:40,750 --> 00:37:42,550
or tiling - Yes exact, yeah.

00:37:42,550 --> 00:37:45,360
So there's a video ram and if you access it linearly

00:37:45,360 --> 00:37:47,640
it goes up one X and if you access the tile

00:37:47,640 --> 00:37:50,560
it goes at four x, is that separate physical memory ID's

00:37:50,560 --> 00:37:52,860
or is that, you know.

00:37:52,860 --> 00:37:56,570
- So actually if you look at mbind, so if you look at mbind,

00:37:56,570 --> 00:37:58,943
the mode actually, have a natural leaf flag.

00:37:59,925 --> 00:38:01,800
So you know you can think about hey,

00:38:01,800 --> 00:38:03,076
maybe we need a flag for eventually

00:38:03,076 --> 00:38:04,610
but the tiling we have on GPU like you know

00:38:04,610 --> 00:38:07,370
the variety of different tiling we have and

00:38:07,370 --> 00:38:09,950
some tiling gonna be really good for that other structure

00:38:09,950 --> 00:38:11,890
and other tiling gonna be good for that thing

00:38:11,890 --> 00:38:13,550
and will depend on the size of the picture

00:38:13,550 --> 00:38:14,743
or like you know,

00:38:15,790 --> 00:38:19,060
so yeah I agree with you that kind of the tallying thing

00:38:19,060 --> 00:38:23,543
is, I do not see how I can fit tiling inside that.

00:38:23,543 --> 00:38:26,650
And I think it's really kind of specific to the device

00:38:26,650 --> 00:38:29,010
but it does not mean that the device by itself

00:38:29,010 --> 00:38:31,240
so you know when you do a PNP mbind,

00:38:31,240 --> 00:38:33,290
you will have a callback inside the device

00:38:33,290 --> 00:38:34,260
or inside the cam bell.

00:38:34,260 --> 00:38:36,450
And then you can assume that the device driver

00:38:36,450 --> 00:38:41,450
have some knowledge, so you it's kind of a side band

00:38:41,540 --> 00:38:43,810
to do that, I don't know if we want to have really

00:38:43,810 --> 00:38:45,140
that explicitly here.

00:38:45,140 --> 00:38:47,340
I'm not against having something explicit here

00:38:47,340 --> 00:38:48,990
but if I have to explain tiling,

00:38:48,990 --> 00:38:51,503
GPU tilings to random people,

00:38:52,594 --> 00:38:54,794
we will just hate me for that and it's like.

00:38:57,422 --> 00:38:58,650
- [Man In Crowd] What if we have multiple associated buffers

00:38:59,756 --> 00:39:01,290
like we do with some GPU's today who--

00:39:01,290 --> 00:39:03,830
- You mean so like multiple different physical addresses

00:39:03,830 --> 00:39:05,383
that will get used?

00:39:05,383 --> 00:39:08,760
- Yeah like today for rendering you have,

00:39:08,760 --> 00:39:11,210
for intel rendering for instance for,

00:39:11,210 --> 00:39:13,980
you have a scan out buffer with a particular scan out mode

00:39:13,980 --> 00:39:17,138
it will actually require separate memory spaces,

00:39:17,138 --> 00:39:19,850
separate buffers to be assigned to the

00:39:19,850 --> 00:39:22,410
same object effectively.

00:39:22,410 --> 00:39:25,480
- Yeah so it's not, so here I'm kind of ignoring

00:39:25,480 --> 00:39:27,290
any thing about graphics because graphics

00:39:27,290 --> 00:39:30,110
has many socan rock cases, like the one you just,

00:39:30,110 --> 00:39:32,290
so you know I don't want to really use that fully

00:39:32,290 --> 00:39:33,940
for graphics.

00:39:33,940 --> 00:39:35,890
I know that some folks-- - I thought it went malloc

00:39:35,890 --> 00:39:37,393
for everybody.

00:39:37,393 --> 00:39:40,170
(laughing)

00:39:40,170 --> 00:39:42,677
Malloc for everybody but me. - Yeah.

00:39:42,677 --> 00:39:43,980
(laughing)

00:39:43,980 --> 00:39:46,787
So for graphics I believe like volcan things

00:39:46,787 --> 00:39:50,460
will you know, in many cases in volcan for texture

00:39:50,460 --> 00:39:52,500
and things that doesn't have this kind of thing,

00:39:52,500 --> 00:39:53,800
you can use that.

00:39:53,800 --> 00:39:56,150
But there will be corner cases for GPU

00:39:56,150 --> 00:39:57,653
like scan out buffer,

00:40:00,100 --> 00:40:03,330
the sport structure on some GPU will not be able to do that

00:40:03,330 --> 00:40:05,150
and you know there will be corner cases

00:40:05,150 --> 00:40:07,170
where you don't want to use malloc's sadly.

00:40:07,170 --> 00:40:10,400
I wish but, I think that you know it's more like

00:40:10,400 --> 00:40:12,940
the outwear people need to fix to be able to be,

00:40:12,940 --> 00:40:15,780
to use malloc also for scan out buffer.

00:40:15,780 --> 00:40:17,730
But you know if you go to talk to them.

00:40:17,730 --> 00:40:20,067
They, I know it's like they--

00:40:21,650 --> 00:40:24,440
- And do you wanna try and put some more sophisticated

00:40:24,440 --> 00:40:28,090
properties in here like a prop list of what the properties,

00:40:28,090 --> 00:40:30,580
the memory are instead of just the flags?

00:40:30,580 --> 00:40:33,140
- So you get the property of the memory from

00:40:33,140 --> 00:40:34,780
the CISS FS directory.

00:40:34,780 --> 00:40:36,693
Inside the CISS FS directory so,

00:40:37,700 --> 00:40:40,020
- Yeah but the property of how the memory is being used

00:40:40,020 --> 00:40:42,270
for this particular-- - Oh, okay so yeah,

00:40:42,270 --> 00:40:43,810
it's gonna be used for read only,

00:40:43,810 --> 00:40:46,230
it's gonna be used read mostly, it's gonna be used--

00:40:46,230 --> 00:40:47,780
- For strive, - Yeah.

00:40:47,780 --> 00:40:49,911
- Piling patterns-- - Yes that's goes back

00:40:49,911 --> 00:40:52,150
to the tiling thing you know.

00:40:52,150 --> 00:40:53,670
- You were talking about just using flags

00:40:53,670 --> 00:40:56,220
and we've ended up having to go to a list of properties

00:40:56,220 --> 00:40:57,158
for the memory.

00:40:57,158 --> 00:40:59,570
- Yeah. - So more complicated API.

00:40:59,570 --> 00:41:01,010
- The issue there is like you know

00:41:01,010 --> 00:41:02,660
trying to sell Cisco to people

00:41:02,660 --> 00:41:04,207
and you're trying to say this is during Cisco

00:41:04,207 --> 00:41:06,900
for any kind of devices and if we are,

00:41:06,900 --> 00:41:08,860
the GPU things we have is--

00:41:08,860 --> 00:41:10,320
- If you can solve the GPU problem,

00:41:10,320 --> 00:41:11,920
you can solve anybody's problem though I'm sure.

00:41:11,920 --> 00:41:13,502
- Yeah that's true, I agree.

00:41:13,502 --> 00:41:16,110
(laughing)

00:41:16,110 --> 00:41:18,050
So yeah I can actually add properties

00:41:18,050 --> 00:41:20,528
like exactly what like we have in during.

00:41:20,528 --> 00:41:23,000
At least properties and then people can pass down

00:41:23,000 --> 00:41:24,560
the list of properties and then we have to learn

00:41:24,560 --> 00:41:25,730
about the properties,

00:41:25,730 --> 00:41:27,780
and it's something I'm open to, you know.

00:41:29,019 --> 00:41:31,510
It's just selling a Cisco is really hard

00:41:31,510 --> 00:41:32,730
and if you have to explain every,

00:41:32,730 --> 00:41:34,510
you know you have to explain every argument

00:41:34,510 --> 00:41:36,405
and what is behind every argument

00:41:36,405 --> 00:41:38,490
and what is the reason behind every argument so,

00:41:38,490 --> 00:41:40,490
and at the same time once you have a Cisco,

00:41:40,490 --> 00:41:42,782
you know Cisco is frozen in time so if you hold

00:41:42,782 --> 00:41:45,180
everything you need then, so maybe I should actually

00:41:45,180 --> 00:41:47,350
go to the hard way and have everything we need,

00:41:47,350 --> 00:41:49,970
I just for GPU and say because GPU has been around

00:41:49,970 --> 00:41:52,436
for long enough and we have so many things.

00:41:52,436 --> 00:41:56,160
We can assume that we have the most complex example

00:41:56,160 --> 00:41:57,930
and if we can address the most complex things

00:41:57,930 --> 00:41:59,241
then we should be able--

00:41:59,241 --> 00:42:00,174
- [Man In Crowd] I'm sure that it will solve

00:42:00,174 --> 00:42:01,246
anything you could potentially...

00:42:01,246 --> 00:42:02,476
- Anything you throw at us, now,

00:42:02,476 --> 00:42:04,426
we already do anything you throw at us.

00:42:05,450 --> 00:42:06,960
- [Man In Crowd] We'll come up with more crazy I'm sure.

00:42:06,960 --> 00:42:08,290
- Yeah I'm sure.

00:42:08,290 --> 00:42:09,460
You know if it's at least a 40's

00:42:09,460 --> 00:42:10,560
then you have at least a 40's

00:42:10,560 --> 00:42:12,167
so you cannot new probe list to the Cisco.

00:42:12,167 --> 00:42:15,082
- [Man In Crowd] Right, yeah properties with values,

00:42:15,082 --> 00:42:15,915
it's, yeah.

00:42:17,250 --> 00:42:20,360
And it's so easy to use with the security models

00:42:20,360 --> 00:42:21,952
and function tracing and,

00:42:21,952 --> 00:42:23,920
- Yeah also. - People like that.

00:42:23,920 --> 00:42:25,670
- Yeah they like because they can see what is happening

00:42:25,670 --> 00:42:26,972
because right now when you--

00:42:26,972 --> 00:42:29,240
- They like the big arrays of external values so you can,

00:42:29,240 --> 00:42:33,510
yeah, so, security model just explodes in your face and.

00:42:33,510 --> 00:42:34,830
- Yeah because right now, yeah,

00:42:34,830 --> 00:42:37,220
with I-O-C-T-L, all the VI still to GPU.

00:42:37,220 --> 00:42:39,320
All the secret kept tools people we with IV

00:42:39,320 --> 00:42:41,160
have absolutely no clue what we're doing.

00:42:41,160 --> 00:42:43,343
And we sometimes we do nasty stuff.

00:42:45,590 --> 00:42:47,030
- [Moderator] Okay, we're about at the end.

00:42:47,030 --> 00:42:49,690
Let's thank the speaker and good questions.

00:42:49,690 --> 00:42:50,523

YouTube URL: https://www.youtube.com/watch?v=_Pvd3s4_orA


