Title: LPC2018 - Experiences Evaluating DC-TCP
Publication date: 2018-12-04
Playlist: LPC2018 - Networking Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/95/
speaker:  Lawrence Brakmo (Facebook), Boris Burkov (Facebook), Greg Leclercq (Facebook), Murat Mugan (Facebook)


In this talk we describe our experiences in evaluating DC-TCP. Preliminary testing with Netesto uncovered issues with our NIC that affected fairness between flows, as well as bugs in the DC-TCP code path in Linux that resulted in RPC tail latencies of up to 200ms. Once we fixed those issues, we proceeded to test in a 6 rack mini cluster running some of our production applications. This testing demonstrated very large decreases in packet discards (12 to 1000x) at a cost of larger CPU utilization. In addition to describing the issues and fixes, we provide detailed experimental results and explore the causes of the larger CPU utilization as well as discuss partial solutions to this issue.

Note: We plan to test on a much larger cluster and have those results available before the conference.
Captions: 
	00:00:07,169 --> 00:00:09,900
for this

00:00:08,010 --> 00:00:11,430
in collaboration with world--well Cobb

00:00:09,900 --> 00:00:13,110
correctly cleric Anwar

00:00:11,430 --> 00:00:16,230
and from Facebook and this is about

00:00:13,110 --> 00:00:19,980
experience evaluating DC TCP for the

00:00:16,230 --> 00:00:22,529
permanent in our data centers so

00:00:19,980 --> 00:00:24,630
standard TCP congestion control which

00:00:22,529 --> 00:00:26,700
only reacts to packet losses has many

00:00:24,630 --> 00:00:29,370
problems one of them is that it can

00:00:26,700 --> 00:00:32,099
result in standing queues queues that do

00:00:29,370 --> 00:00:33,390
not dissipate and this increases trail

00:00:32,099 --> 00:00:35,790
Leighton you can also increase the

00:00:33,390 --> 00:00:38,489
latencies due to the time it takes to

00:00:35,790 --> 00:00:41,610
recover from glasses and it also

00:00:38,489 --> 00:00:43,980
penalize the smaller RPC flows okay so

00:00:41,610 --> 00:00:45,570
the reason for the standing queues is

00:00:43,980 --> 00:00:49,079
that because you'd only react to packet

00:00:45,570 --> 00:00:50,280
losses the only mechanism it has to know

00:00:49,079 --> 00:00:54,210
whether is using all of the available

00:00:50,280 --> 00:00:55,739
bandwidth is to go over it grow the

00:00:54,210 --> 00:00:56,910
queues until packets are drop and then

00:00:55,739 --> 00:00:59,489
it will slow down right and that's why

00:00:56,910 --> 00:01:04,860
we have the standard zigzag behavior of

00:00:59,489 --> 00:01:06,990
TCP and if analysis model flows because

00:01:04,860 --> 00:01:09,660
of these standing queues if the RTT

00:01:06,990 --> 00:01:13,470
grows from tens of microseconds to one

00:01:09,660 --> 00:01:15,659
millisecond ten kilobyte RPC can only go

00:01:13,470 --> 00:01:16,049
up to eight megabits per second that's

00:01:15,659 --> 00:01:17,820
it

00:01:16,049 --> 00:01:21,450
right it doesn't matter how fragile in

00:01:17,820 --> 00:01:23,010
case whereas an one megabyte RPC could

00:01:21,450 --> 00:01:26,190
it could achieve up to eight gigabits

00:01:23,010 --> 00:01:28,860
per second right secondly same 10

00:01:26,190 --> 00:01:31,170
kilobytes every one millisecond that

00:01:28,860 --> 00:01:35,670
limits you how fast you can go so

00:01:31,170 --> 00:01:37,880
standard congest log base tcp typically

00:01:35,670 --> 00:01:40,170
results in penalizing smaller IPC flows

00:01:37,880 --> 00:01:44,310
condition avoidance on the other hand

00:01:40,170 --> 00:01:46,619
which reacts to increasing queues has

00:01:44,310 --> 00:01:48,210
been proposed as a solution to this

00:01:46,619 --> 00:01:50,820
progress of lost based congestion

00:01:48,210 --> 00:01:52,890
control and of these DCT CP

00:01:50,820 --> 00:01:56,670
is the most commonly used in data

00:01:52,890 --> 00:01:59,820
centers so this is TCP uses CCA

00:01:56,670 --> 00:02:02,100
early congestion notification where

00:01:59,820 --> 00:02:04,229
twitches mark packets when they arrive

00:02:02,100 --> 00:02:06,869
and the queue size is bigger than a

00:02:04,229 --> 00:02:10,350
given threshold right so the idea is to

00:02:06,869 --> 00:02:13,470
mark packets when we're seeing the early

00:02:10,350 --> 00:02:16,260
stages of congestion queue growth and

00:02:13,470 --> 00:02:18,420
the original response of TCP to vision

00:02:16,260 --> 00:02:20,370
markings was just too reactive who was a

00:02:18,420 --> 00:02:23,100
packet drop

00:02:20,370 --> 00:02:24,670
TCP Reno would decrease this congestion

00:02:23,100 --> 00:02:28,100
window by 50%

00:02:24,670 --> 00:02:30,350
and in many network topologies this

00:02:28,100 --> 00:02:32,270
response is way too aggressive and you

00:02:30,350 --> 00:02:35,180
can reduce in underutilizing the links

00:02:32,270 --> 00:02:37,400
right and one of the problems is that it

00:02:35,180 --> 00:02:39,290
doesn't differentiate between transient

00:02:37,400 --> 00:02:41,870
congestion congestion that made last

00:02:39,290 --> 00:02:44,450
legend an RTT where the cube grows but

00:02:41,870 --> 00:02:46,819
then it dissipates completely within a

00:02:44,450 --> 00:02:49,040
given RTT right so it doesn't disappear

00:02:46,819 --> 00:02:51,290
doesn't differentiate between that and

00:02:49,040 --> 00:02:58,580
standing congestion congestion that is

00:02:51,290 --> 00:03:03,080
Latin for many many artists this is TCP

00:02:58,580 --> 00:03:04,580
also uses ECM to the tech ingestion but

00:03:03,080 --> 00:03:06,980
instead of able to react in the same way

00:03:04,580 --> 00:03:10,250
it reacts proportionally to the level of

00:03:06,980 --> 00:03:12,890
congestion so it measures how many bytes

00:03:10,250 --> 00:03:15,380
or packets so were marked with

00:03:12,890 --> 00:03:19,310
congestion per RTT and then it reacts

00:03:15,380 --> 00:03:22,280
proportionally so though for example if

00:03:19,310 --> 00:03:24,140
you have the follow the packets were

00:03:22,280 --> 00:03:28,880
marked it will reduce the congestion

00:03:24,140 --> 00:03:31,730
window by 50% but doing it just like

00:03:28,880 --> 00:03:33,049
that would also have the problem that it

00:03:31,730 --> 00:03:37,600
doesn't deal well with trashing

00:03:33,049 --> 00:03:37,600
congestion so for audacity radishes

00:03:38,520 --> 00:03:48,070
well okay so to deal with transient

00:03:44,920 --> 00:03:52,150
congestion the response is based on a

00:03:48,070 --> 00:03:54,520
moving average so the markings you know

00:03:52,150 --> 00:03:56,830
had to be for example we follow the

00:03:54,520 --> 00:03:59,710
packets are marked pipe in one RTT it

00:03:56,830 --> 00:04:01,150
will not react as aggressively as he

00:03:59,710 --> 00:04:07,000
follow the police are marked for many

00:04:01,150 --> 00:04:09,700
activities okay so that's an

00:04:07,000 --> 00:04:10,960
introduction to EC and EC TCP and this

00:04:09,700 --> 00:04:13,510
is about the tag so we're going to

00:04:10,960 --> 00:04:17,590
describe our experiences evaluating DC

00:04:13,510 --> 00:04:19,420
TCP so we went to titled test the

00:04:17,590 --> 00:04:22,150
initial tests were running within a rat

00:04:19,420 --> 00:04:24,940
using the test oh the network test

00:04:22,150 --> 00:04:27,130
toolkit to create traffic and capture

00:04:24,940 --> 00:04:29,350
you know information about congestion

00:04:27,130 --> 00:04:35,020
and and the metrics and this uncovered

00:04:29,350 --> 00:04:38,640
various issues so and the following

00:04:35,020 --> 00:04:43,260
tests were running using six racks

00:04:38,640 --> 00:04:48,160
testing some data center services and

00:04:43,260 --> 00:04:51,040
I'll describe more the defect so what

00:04:48,160 --> 00:04:53,550
issue do we see in the rack testing we

00:04:51,040 --> 00:04:57,370
saw infernally between senders

00:04:53,550 --> 00:05:01,150
regardless of the TCP congestion control

00:04:57,370 --> 00:05:06,580
being used we also saw Infernus between

00:05:01,150 --> 00:05:09,640
flows even with only one sender we're

00:05:06,580 --> 00:05:12,070
using ECN and finally we also saw hater

00:05:09,640 --> 00:05:17,290
latencies high tailed latencies when

00:05:12,070 --> 00:05:19,030
using DC TCP an issue between the secret

00:05:17,290 --> 00:05:22,600
test that we saw is the higher CPU

00:05:19,030 --> 00:05:26,950
utilization with disappea and I will

00:05:22,600 --> 00:05:31,300
talk about all of these so infernally

00:05:26,950 --> 00:05:35,980
between between servers so we have an

00:05:31,300 --> 00:05:39,480
experiment where we had a three server

00:05:35,980 --> 00:05:42,940
sending to one and what we saw is that

00:05:39,480 --> 00:05:45,250
two of those servers is got 25% of the

00:05:42,940 --> 00:05:47,080
bandwidth and the third server got 50%

00:05:45,250 --> 00:05:50,560
of the bandwidth independently of which

00:05:47,080 --> 00:05:51,370
congestion control we used so ignore the

00:05:50,560 --> 00:05:53,350
Box in the middle

00:05:51,370 --> 00:05:56,110
Stud explanation so we have three server

00:05:53,350 --> 00:05:57,729
sending to one and what happened is that

00:05:56,110 --> 00:06:01,150
the switch architecture that we were

00:05:57,729 --> 00:06:03,100
using it has two buffers that are

00:06:01,150 --> 00:06:06,970
associated with each output port and

00:06:03,100 --> 00:06:09,220
then the input port I've divided between

00:06:06,970 --> 00:06:10,840
buffer a and buffer be I mean that

00:06:09,220 --> 00:06:12,840
experiment by coincidence

00:06:10,840 --> 00:06:15,370
two of the service were using buffer a

00:06:12,840 --> 00:06:18,820
one of the server's was using buffer B

00:06:15,370 --> 00:06:22,600
and then the switch run Robins between

00:06:18,820 --> 00:06:25,120
both buffers so you know what happens is

00:06:22,600 --> 00:06:27,130
that flowed using buffer a will get half

00:06:25,120 --> 00:06:28,360
of the bandwidth and load using buffer B

00:06:27,130 --> 00:06:30,520
will get the other half

00:06:28,360 --> 00:06:35,590
assuming flows are going to both buffers

00:06:30,520 --> 00:06:39,400
right so what I had to do is ensure that

00:06:35,590 --> 00:06:42,669
my test I already servers use port for

00:06:39,400 --> 00:06:46,289
one buffer otherwise when I'm analyzing

00:06:42,669 --> 00:06:49,090
behavior I cannot differentiate between

00:06:46,289 --> 00:06:50,979
effect doing - let's say DC TCP and

00:06:49,090 --> 00:06:54,479
effect doing to the switch architecture

00:06:50,979 --> 00:06:58,330
and I'm mentioning these things to just

00:06:54,479 --> 00:07:00,520
indicate that when doing testing we have

00:06:58,330 --> 00:07:04,240
to be very careful to track any

00:07:00,520 --> 00:07:05,919
behaviors so that we know for the fault

00:07:04,240 --> 00:07:09,780
lies is the fault of the congestion

00:07:05,919 --> 00:07:12,520
control is a default of something else

00:07:09,780 --> 00:07:17,950
another problem with source and furnace

00:07:12,520 --> 00:07:22,300
between flows when using ecn okay so how

00:07:17,950 --> 00:07:24,280
we have it will have two flows and one

00:07:22,300 --> 00:07:25,930
of the flows will get 23 gigabits per

00:07:24,280 --> 00:07:28,539
second and the other flow will get half

00:07:25,930 --> 00:07:31,030
a gigabit per second right so obviously

00:07:28,539 --> 00:07:33,520
this was with a NIC that was twenty five

00:07:31,030 --> 00:07:35,199
gigabits per second and what I did here

00:07:33,520 --> 00:07:38,169
then I wrote a tool to analyze the

00:07:35,199 --> 00:07:40,539
pickups and four-ish know I could look

00:07:38,169 --> 00:07:44,320
at per RTT metrics and could also expand

00:07:40,539 --> 00:07:46,180
it to look at the per packet details and

00:07:44,320 --> 00:07:47,530
I discovered that one of the flow for

00:07:46,180 --> 00:07:49,930
one of the fold the activities were by

00:07:47,530 --> 00:07:52,930
model there were either 50 microseconds

00:07:49,930 --> 00:07:54,310
or 1.3 milliseconds more or less right

00:07:52,930 --> 00:07:56,680
there were two in between one and the

00:07:54,310 --> 00:07:58,870
other one and this was hurting obviously

00:07:56,680 --> 00:08:00,610
the the bandwidth so this is more or

00:07:58,870 --> 00:08:03,700
less what it looked like you know so

00:08:00,610 --> 00:08:04,639
that's our tt-that 2.1 means for flow

00:08:03,700 --> 00:08:07,699
too

00:08:04,639 --> 00:08:09,889
Artie T 1 the LTTE was 1.2 million

00:08:07,699 --> 00:08:11,840
seconds 140 milliseconds and then for

00:08:09,889 --> 00:08:13,849
the third Artie T was 40 micro seconds

00:08:11,840 --> 00:08:15,590
and then he jump again 2 1 / 2 minutes

00:08:13,849 --> 00:08:20,599
ago so obviously when the Artie T is

00:08:15,590 --> 00:08:22,069
large the banquette collapses and after

00:08:20,599 --> 00:08:25,699
doing more analysis it turned out that

00:08:22,069 --> 00:08:27,830
the NIC firmware will be using was using

00:08:25,699 --> 00:08:30,650
very large coalescing values and a 1

00:08:27,830 --> 00:08:33,110
millisecond timer and this is this were

00:08:30,650 --> 00:08:36,169
not transparent like if you look at the

00:08:33,110 --> 00:08:39,769
quality values of the knee you know in

00:08:36,169 --> 00:08:41,209
ETS - that were reasonable right but he

00:08:39,769 --> 00:08:44,029
was doing he had a new feature that

00:08:41,209 --> 00:08:47,890
we're doing something else that it was

00:08:44,029 --> 00:08:47,890
totally current performance right now

00:08:48,880 --> 00:08:53,750
the more interesting issue we saw was

00:08:51,620 --> 00:08:58,459
heightened latency with the central TCP

00:08:53,750 --> 00:09:01,370
and so we were running a one megabyte

00:08:58,459 --> 00:09:03,440
and 10 megabyte our pcs and compared to

00:09:01,370 --> 00:09:08,300
cubic we have heightened latency so for

00:09:03,440 --> 00:09:10,910
example if we look at the first set for

00:09:08,300 --> 00:09:12,470
cubic latencies the one megabyte our pcs

00:09:10,910 --> 00:09:16,910
were two point six and five point five

00:09:12,470 --> 00:09:18,980
milliseconds for the 9999 presenta for

00:09:16,910 --> 00:09:22,310
the 10 kilo vital pieces there were 1.1

00:09:18,980 --> 00:09:25,279
and 1.2 mil cycles the DC TCP latencies

00:09:22,310 --> 00:09:27,949
were 43 milliseconds and 208

00:09:25,279 --> 00:09:31,070
milliseconds for the 1 megabyte and 52

00:09:27,949 --> 00:09:32,930
milliseconds and 212 milliseconds for

00:09:31,070 --> 00:09:36,110
the technical part pieces obviously

00:09:32,930 --> 00:09:38,690
these numbers indicate you know like the

00:09:36,110 --> 00:09:41,649
43 and the 53 are very close to the 50

00:09:38,690 --> 00:09:44,839
40 millisecond delay lag timer on the

00:09:41,649 --> 00:09:48,769
208 212 are close to the RTO 200

00:09:44,839 --> 00:09:51,680
millisecond timers right once we fix the

00:09:48,769 --> 00:09:58,070
problem the latencies collapse you know

00:09:51,680 --> 00:10:01,370
5.8 6.9 146 and 203 microseconds so note

00:09:58,070 --> 00:10:04,760
that the 19 opponent latencies for DC

00:10:01,370 --> 00:10:06,890
TCP are larger than for cubic and the

00:10:04,760 --> 00:10:08,899
reason for that is that the thing

00:10:06,890 --> 00:10:10,790
kilobyte our pcs are getting much higher

00:10:08,899 --> 00:10:13,310
throughput you know like 10 times higher

00:10:10,790 --> 00:10:14,690
throughput so obviously that leads to

00:10:13,310 --> 00:10:16,640
poor further 1 megabyte RPC

00:10:14,690 --> 00:10:17,940
so the height and latencies are not a

00:10:16,640 --> 00:10:20,010
sign that

00:10:17,940 --> 00:10:22,500
something is wrong it's a sign that

00:10:20,010 --> 00:10:25,290
something is really really good we are

00:10:22,500 --> 00:10:28,740
being more fair between the small and

00:10:25,290 --> 00:10:31,650
the larger pieces so what caused these

00:10:28,740 --> 00:10:34,880
high tail latencies and then after

00:10:31,650 --> 00:10:37,350
spending a lot of time analyzing pickups

00:10:34,880 --> 00:10:39,690
we find out that there were two issues

00:10:37,350 --> 00:10:45,180
affecting the tail agencies one was

00:10:39,690 --> 00:10:46,860
causing our TOS when the receiver was

00:10:45,180 --> 00:10:48,930
sending a duplicate act to the sender

00:10:46,860 --> 00:10:51,570
and Nawrocki in the last packet received

00:10:48,930 --> 00:10:56,430
so then the sender in their app talked

00:10:51,570 --> 00:11:00,990
about the last packet in an either RPC

00:10:56,430 --> 00:11:04,080
or sometimes doing a a big batch it

00:11:00,990 --> 00:11:06,450
would have to do an RTO to send the next

00:11:04,080 --> 00:11:10,470
packet and the other one was done under

00:11:06,450 --> 00:11:12,960
some conditions we were delaying Deacs

00:11:10,470 --> 00:11:15,930
so we were waiting 40 milliseconds to

00:11:12,960 --> 00:11:16,950
send an acknowledgment when we should

00:11:15,930 --> 00:11:19,980
not have done that

00:11:16,950 --> 00:11:22,520
right and it was related to the ECM in

00:11:19,980 --> 00:11:25,440
both case it was related to ECM behavior

00:11:22,520 --> 00:11:27,870
in the bearing case was we were getting

00:11:25,440 --> 00:11:29,490
the congestion information to the

00:11:27,870 --> 00:11:31,410
receiver and the receiver rather than

00:11:29,490 --> 00:11:32,940
sending an immediate act to tell the

00:11:31,410 --> 00:11:34,920
sender hey you know there's congestion

00:11:32,940 --> 00:11:39,600
it was just delaying it for 40

00:11:34,920 --> 00:11:41,430
milliseconds and these problems were

00:11:39,600 --> 00:11:43,830
being triggered by Colonel patches from

00:11:41,430 --> 00:11:48,480
2015 they were in the corner for three

00:11:43,830 --> 00:11:50,130
years and we didn't realize it and I

00:11:48,480 --> 00:11:52,020
shouldn't mention that that that's you

00:11:50,130 --> 00:11:54,270
gave at are comparing congestion

00:11:52,020 --> 00:11:56,850
controls and I talked a little bit about

00:11:54,270 --> 00:11:58,560
DC TCP but then I for some other

00:11:56,850 --> 00:12:00,180
experience I didn't talk about DC TCP

00:11:58,560 --> 00:12:03,290
because I mentioned I was seeing some

00:12:00,180 --> 00:12:07,650
weird behaviors the right eye will do to

00:12:03,290 --> 00:12:10,110
boxing the kernels so yes the word due

00:12:07,650 --> 00:12:12,960
to bugs in the kernel the figures are

00:12:10,110 --> 00:12:17,540
now upstream by usual Neal and myself

00:12:12,960 --> 00:12:17,540
and they solve this problem

00:12:19,550 --> 00:12:26,990
okay so the next set of tests will run

00:12:23,480 --> 00:12:30,290
with six right tests so we have three

00:12:26,990 --> 00:12:32,149
racks on one side for storage racks on

00:12:30,290 --> 00:12:35,000
the left side we have worker rocks that

00:12:32,149 --> 00:12:38,300
will read data from the storage and we

00:12:35,000 --> 00:12:43,930
have the switches in between connecting

00:12:38,300 --> 00:12:46,910
everything and you know the links

00:12:43,930 --> 00:12:49,250
between the top of rack or the storage

00:12:46,910 --> 00:12:50,899
racks and the top of rocks on the other

00:12:49,250 --> 00:12:53,209
side are 100 gigabit per second link

00:12:50,899 --> 00:12:54,709
right so we have a lot of we have a lot

00:12:53,209 --> 00:13:00,230
of bandwidth between two sets of rocks

00:12:54,709 --> 00:13:02,480
and what we did is we did three sets of

00:13:00,230 --> 00:13:06,110
experiment using the three racks - right

00:13:02,480 --> 00:13:08,300
and one rack to test different levels of

00:13:06,110 --> 00:13:13,130
congestion obviously if we only have one

00:13:08,300 --> 00:13:16,100
rack we have a lot of bandwidth on the

00:13:13,130 --> 00:13:18,620
right side going you know totally one

00:13:16,100 --> 00:13:20,630
rack so we can easily over saturate the

00:13:18,620 --> 00:13:21,860
legs and we just wanted to see what

00:13:20,630 --> 00:13:27,470
would be the behavior under those

00:13:21,860 --> 00:13:29,870
conditions so the first result are for

00:13:27,470 --> 00:13:31,370
that three worker racks for the service

00:13:29,870 --> 00:13:33,829
with less congestion so the first row

00:13:31,370 --> 00:13:38,480
tells us than the maximum link

00:13:33,829 --> 00:13:42,500
utilization going to the worker rag was

00:13:38,480 --> 00:13:45,920
70% so we were only using 70% of the

00:13:42,500 --> 00:13:49,640
bandwidth maximum and even they were

00:13:45,920 --> 00:13:51,529
using this card you know 85 million bits

00:13:49,640 --> 00:13:54,140
being this guy which is not very much is

00:13:51,529 --> 00:14:01,220
a point two percent of the data being

00:13:54,140 --> 00:14:04,480
transmitted and for DC TCP were seeing

00:14:01,220 --> 00:14:10,399
much fewer discard but or the order of

00:14:04,480 --> 00:14:12,680
that three hundred times less discard

00:14:10,399 --> 00:14:15,529
which are not these cards on the racks

00:14:12,680 --> 00:14:17,600
for the worker so the workers also talk

00:14:15,529 --> 00:14:23,390
to each other so there's some traffic

00:14:17,600 --> 00:14:24,230
between the worker racks and for the CPU

00:14:23,390 --> 00:14:26,660
overhead

00:14:24,230 --> 00:14:30,190
we only saw like a small one percent

00:14:26,660 --> 00:14:33,490
overhead on the workers

00:14:30,190 --> 00:14:36,279
and here in the last two rows I'm saying

00:14:33,490 --> 00:14:38,279
the percentage see marked packets right

00:14:36,279 --> 00:14:43,420
and let me go back to the previous one

00:14:38,279 --> 00:14:47,230
so this detection is done on the

00:14:43,420 --> 00:14:50,199
receiver right so for example when that

00:14:47,230 --> 00:14:52,569
is flowing from the storage racks to the

00:14:50,199 --> 00:14:54,579
worker racks the worker rights are the

00:14:52,569 --> 00:15:00,399
one that can detect the EZ and marking

00:14:54,579 --> 00:15:02,290
of congestion right and similarly if the

00:15:00,399 --> 00:15:07,180
turret racks are seeing it so in this

00:15:02,290 --> 00:15:10,350
following sign here the one that is we

00:15:07,180 --> 00:15:13,779
seem more CPU is the worker CPU and

00:15:10,350 --> 00:15:20,259
that's you know that we need the one

00:15:13,779 --> 00:15:22,209
that is seen 12 percent mark so in the

00:15:20,259 --> 00:15:23,920
next one now we have two worker racks

00:15:22,209 --> 00:15:27,209
and the maximum linkage to the

00:15:23,920 --> 00:15:30,639
stationary knife I mean this is over

00:15:27,209 --> 00:15:33,250
fully saturated the links like we are

00:15:30,639 --> 00:15:37,089
way over subscribe and as I resolved

00:15:33,250 --> 00:15:38,620
cubic is wrapping 160 billion bits

00:15:37,089 --> 00:15:41,649
during the duration of the experiment

00:15:38,620 --> 00:15:46,779
which securing 2.6 percent of the data

00:15:41,649 --> 00:15:50,529
being sent in contra de TCP is

00:15:46,779 --> 00:15:53,769
discarding a thousand times less and is

00:15:50,529 --> 00:15:57,000
not discovering anything in the worker

00:15:53,769 --> 00:15:57,000
type of rack switches

00:15:58,769 --> 00:16:07,089
however the CPU overhead on the storage

00:16:03,040 --> 00:16:10,540
rack is 14 percent and this is system

00:16:07,089 --> 00:16:13,509
CPU if the whole season can do 100%

00:16:10,540 --> 00:16:16,240
fully utilized you know this is not per

00:16:13,509 --> 00:16:19,569
node or vapor core and if it is it the

00:16:16,240 --> 00:16:23,230
whole system so just a huge over and if

00:16:19,569 --> 00:16:26,290
we look at the ECM mark we're seeing 64

00:16:23,230 --> 00:16:31,059
percent markings right so almost every

00:16:26,290 --> 00:16:33,689
other packet is being marked on the

00:16:31,059 --> 00:16:33,689
receiving end

00:16:35,910 --> 00:16:45,780
so these are the counters from the from

00:16:40,680 --> 00:16:48,240
TCP so typically the receiver or the

00:16:45,780 --> 00:16:51,360
data is the one that says sees the ACNC

00:16:48,240 --> 00:16:52,800
marking right so this means that we so

00:16:51,360 --> 00:16:55,530
the worker is reading data from the

00:16:52,800 --> 00:16:59,250
storage and the packet the data packets

00:16:55,530 --> 00:17:02,060
when they arrive 63% of those so

00:16:59,250 --> 00:17:02,060
congestion Oh

00:17:08,310 --> 00:17:13,230
distortion mark they also seen some you

00:17:11,850 --> 00:17:15,959
know there's some traffic between the

00:17:13,230 --> 00:17:18,540
storage nodes also and and the workers

00:17:15,959 --> 00:17:20,760
also send traffic to to the storage node

00:17:18,540 --> 00:17:22,709
and there's traffic between a storage

00:17:20,760 --> 00:17:24,900
network they are moving data around you

00:17:22,709 --> 00:17:29,520
know to make it more available and

00:17:24,900 --> 00:17:31,950
things like that and the final one is we

00:17:29,520 --> 00:17:33,600
only have one rack right always it

00:17:31,950 --> 00:17:35,550
cannot be more or subscriber means like

00:17:33,600 --> 00:17:37,110
but there's a lot more pressure we're

00:17:35,550 --> 00:17:38,910
wrapping a lot more packets for cubic

00:17:37,110 --> 00:17:40,650
and now we're dropping a lot more

00:17:38,910 --> 00:17:42,600
packets also for DT TCP I mean this is

00:17:40,650 --> 00:17:45,240
disappeared not a magic bullet if you're

00:17:42,600 --> 00:17:47,400
trying to push way too much data you

00:17:45,240 --> 00:17:50,700
know you cannot prevent it still better

00:17:47,400 --> 00:17:52,230
than cubic is by a factor of 12 but the

00:17:50,700 --> 00:17:54,090
previous one was a factor of a thousand

00:17:52,230 --> 00:17:57,330
right but we should never be in a

00:17:54,090 --> 00:17:58,380
situation where we are doing this right

00:17:57,330 --> 00:18:01,140
I mean it's just

00:17:58,380 --> 00:18:03,690
wherever subscribe our workload should

00:18:01,140 --> 00:18:07,980
be more intelligent than that typically

00:18:03,690 --> 00:18:10,770
as I said the world that I use here was

00:18:07,980 --> 00:18:13,280
artificial because I wanted to push it

00:18:10,770 --> 00:18:16,140
and our application would not push it

00:18:13,280 --> 00:18:17,490
begun to this limit right the

00:18:16,140 --> 00:18:20,580
interesting thing to see is that the

00:18:17,490 --> 00:18:25,650
star CPU the overhead decreased from 14%

00:18:20,580 --> 00:18:28,560
to 10% and we also see the percent mark

00:18:25,650 --> 00:18:31,080
increase even further to 73% right so

00:18:28,560 --> 00:18:35,040
even though we have more markings the

00:18:31,080 --> 00:18:37,950
overhead is less and that gives us an

00:18:35,040 --> 00:18:40,620
idea about what's going on which I will

00:18:37,950 --> 00:18:44,010
talk about the the next slide so in this

00:18:40,620 --> 00:18:46,920
slide we'll do a quick summary so we

00:18:44,010 --> 00:18:49,600
seem much for pure discards with data

00:18:46,920 --> 00:18:54,430
center TCP as expected

00:18:49,600 --> 00:18:57,400
ten 200,000 times fewer discards but we

00:18:54,430 --> 00:19:01,570
also seen a higher CPU utilization when

00:18:57,400 --> 00:19:03,700
the link is oversaturated so when we

00:19:01,570 --> 00:19:06,370
have so many percent utilization there's

00:19:03,700 --> 00:19:07,270
no overhead when we're using 99%

00:19:06,370 --> 00:19:13,690
utilization

00:19:07,270 --> 00:19:14,980
we're seeing 40% more CPU and we I do

00:19:13,690 --> 00:19:17,080
not think it's going to be an Asian

00:19:14,980 --> 00:19:17,530
production traffic but we still need to

00:19:17,080 --> 00:19:21,430
dust

00:19:17,530 --> 00:19:27,880
you know our following tests so what's

00:19:21,430 --> 00:19:30,910
going on so the problem is that what we

00:19:27,880 --> 00:19:32,740
cannot call this as efficiently when we

00:19:30,910 --> 00:19:37,090
are marking right we cannot call this

00:19:32,740 --> 00:19:39,360
packet from you know a marked packet can

00:19:37,090 --> 00:19:42,550
I be a college with an unmarked packet

00:19:39,360 --> 00:19:45,610
therefore the receiver is needs to

00:19:42,550 --> 00:19:49,230
handle a lot more packets a lot smaller

00:19:45,610 --> 00:19:54,400
packets odro and lro are less efficient

00:19:49,230 --> 00:19:56,710
and the problem is also than when we

00:19:54,400 --> 00:19:58,990
send the axe back were saying a lot more

00:19:56,710 --> 00:20:00,700
axe to the sender so it's not only the

00:19:58,990 --> 00:20:03,670
receiver that it's undoing a lot more

00:20:00,700 --> 00:20:06,520
work the sender needs to manage a lot

00:20:03,670 --> 00:20:10,920
more packets because again we can add

00:20:06,520 --> 00:20:13,090
you know the 4dc tcp the acknowledgment

00:20:10,920 --> 00:20:14,560
indicate that was congestion and it only

00:20:13,090 --> 00:20:18,100
applies to the amount of data that is

00:20:14,560 --> 00:20:20,050
being acknowledged so the worst case

00:20:18,100 --> 00:20:22,030
scenario is when every other packet is

00:20:20,050 --> 00:20:26,790
has an ECM marking with in that case we

00:20:22,030 --> 00:20:30,340
can do zero coalescing right so and as

00:20:26,790 --> 00:20:32,500
as the number of packets being marked

00:20:30,340 --> 00:20:35,170
increases then we can call this more you

00:20:32,500 --> 00:20:39,490
know more marked packets can be call it

00:20:35,170 --> 00:20:40,750
and the overhead is less again not sure

00:20:39,490 --> 00:20:42,670
how much of an issue this is going to be

00:20:40,750 --> 00:20:47,490
on production workloads probably not as

00:20:42,670 --> 00:20:49,980
bad but we don't fully know right now

00:20:47,490 --> 00:20:52,649
and

00:20:49,980 --> 00:20:54,870
I had the hope who have running the

00:20:52,649 --> 00:20:57,480
bigger experiment with production

00:20:54,870 --> 00:21:01,730
traffic using full clusters but it's

00:20:57,480 --> 00:21:04,620
taken forever to to be able to upgrade

00:21:01,730 --> 00:21:07,889
everything on production to the newer

00:21:04,620 --> 00:21:09,330
kernels to do this experiment so we're

00:21:07,889 --> 00:21:13,529
in the process of running the classic

00:21:09,330 --> 00:21:15,630
word experiment but we haven't finished

00:21:13,529 --> 00:21:19,919
them so I wasn't able to provide that

00:21:15,630 --> 00:21:21,570
data I'm also planning to look at

00:21:19,919 --> 00:21:23,730
techniques to the relationshipy overhead

00:21:21,570 --> 00:21:25,559
when using DC tcp if it becomes an issue

00:21:23,730 --> 00:21:27,570
for production traffic as I say it's not

00:21:25,559 --> 00:21:29,639
clear to me whether it's only an issue

00:21:27,570 --> 00:21:35,899
for artificial workloads but not an

00:21:29,639 --> 00:21:40,110
issue for production traffic and that's

00:21:35,899 --> 00:21:42,510
yep so from our early experiment this is

00:21:40,110 --> 00:21:45,510
TCP it's really great and I've run many

00:21:42,510 --> 00:21:47,730
DCT experiments in the other context you

00:21:45,510 --> 00:21:49,440
know with lots of flaws with RPC sizes

00:21:47,730 --> 00:21:53,519
and all that and it's turn out to be

00:21:49,440 --> 00:21:55,529
very very effective both on fairness

00:21:53,519 --> 00:21:58,740
between different opposite sizes and

00:21:55,529 --> 00:22:02,340
also reducing retransmissions and I'm

00:21:58,740 --> 00:22:04,769
also using it for the BPF network

00:22:02,340 --> 00:22:05,480
resource control that I'm talking about

00:22:04,769 --> 00:22:07,919
tomorrow

00:22:05,480 --> 00:22:11,389
DC TCP is very effective on that

00:22:07,919 --> 00:22:14,809
environment that's it

00:22:11,389 --> 00:22:14,809
any questions

00:22:17,860 --> 00:22:23,630
hi a word are and it asks when you were

00:22:21,320 --> 00:22:25,730
running cubic and ECT CPA at the same

00:22:23,630 --> 00:22:29,270
time in parallel it feels like that

00:22:25,730 --> 00:22:32,770
could be completely kill CCP but yeah so

00:22:29,270 --> 00:22:35,870
usually it's very difficult to mix

00:22:32,770 --> 00:22:37,100
congestion avoidance with tries to

00:22:35,870 --> 00:22:39,320
prevent congestion and congestion

00:22:37,100 --> 00:22:42,080
control with periodically curious

00:22:39,320 --> 00:22:44,210
congestion it's problematic to do it

00:22:42,080 --> 00:22:46,760
right so depending on which congested

00:22:44,210 --> 00:22:49,040
wouldn't you do one or the other would

00:22:46,760 --> 00:22:52,070
be hurt so typically for example we were

00:22:49,040 --> 00:22:54,920
using the late base RTT in many

00:22:52,070 --> 00:22:57,800
instances the the delayed base would be

00:22:54,920 --> 00:23:01,030
heard unless you have a mechanism to try

00:22:57,800 --> 00:23:04,910
to detect this like VBR sometime does

00:23:01,030 --> 00:23:07,280
but when you're using easy end it

00:23:04,910 --> 00:23:10,120
depends on how you're implementing your

00:23:07,280 --> 00:23:13,720
cues your marking on the switch

00:23:10,120 --> 00:23:17,300
typically you're using red and typically

00:23:13,720 --> 00:23:21,110
if the packet is ACN enabled you will

00:23:17,300 --> 00:23:23,390
mark based on your thresholds but if the

00:23:21,110 --> 00:23:25,730
packet is not even enable you will drop

00:23:23,390 --> 00:23:28,430
it once you reach the lower threshold

00:23:25,730 --> 00:23:30,460
so in that case cubic is penalized not

00:23:28,430 --> 00:23:33,680
ECT CP and a solution that we are

00:23:30,460 --> 00:23:37,370
planning to deploy is to have segregated

00:23:33,680 --> 00:23:40,430
traffic so that we have non ecn traffic

00:23:37,370 --> 00:23:43,340
uses 1q and easy introduces another cue

00:23:40,430 --> 00:23:45,530
so from my understanding like basically

00:23:43,340 --> 00:23:47,480
it will require to rework the whole

00:23:45,530 --> 00:23:50,150
network to be able to support like

00:23:47,480 --> 00:23:51,590
separate queue for the issue so that

00:23:50,150 --> 00:23:53,510
wishes will need to have like an extra

00:23:51,590 --> 00:23:55,880
q1 for ACN and you can leave the

00:23:53,510 --> 00:23:57,800
existing queues the same to handle you

00:23:55,880 --> 00:23:59,960
know legacy so basically it's like not

00:23:57,800 --> 00:24:02,120
as simple as like enable DC tcp on the

00:23:59,960 --> 00:24:04,580
unhorsed but you'll also need to rework

00:24:02,120 --> 00:24:05,840
the whole network right yeah because you

00:24:04,580 --> 00:24:08,240
always have to do it because ecn

00:24:05,840 --> 00:24:09,980
requires marking on the switches right

00:24:08,240 --> 00:24:11,690
so you have to do it there but you're

00:24:09,980 --> 00:24:16,270
writing that you will need to have

00:24:11,690 --> 00:24:20,000
possibly more queues to segregate and

00:24:16,270 --> 00:24:22,600
even if you move all of your data center

00:24:20,000 --> 00:24:24,920
traffic to DC TCP you still have

00:24:22,600 --> 00:24:27,050
external traffic coming in that will be

00:24:24,920 --> 00:24:28,310
probably cubic or PBR and you will need

00:24:27,050 --> 00:24:30,230
to secure get it so

00:24:28,310 --> 00:24:32,530
yes you end up having more kids for that

00:24:30,230 --> 00:24:35,000
thank you

00:24:32,530 --> 00:24:37,250
one thing that occurs to me thinking

00:24:35,000 --> 00:24:39,050
about all this is perhaps we should

00:24:37,250 --> 00:24:41,330
consider having a relaxed gr Oh

00:24:39,050 --> 00:24:43,460
coalescing mode that allow us to coil us

00:24:41,330 --> 00:24:46,370
even if there's you see a marking in the

00:24:43,460 --> 00:24:48,140
middle yes because what you're really

00:24:46,370 --> 00:24:50,000
interested in as the receiver is that

00:24:48,140 --> 00:24:52,370
there was easy on that it did happen

00:24:50,000 --> 00:24:54,170
somewhere not necessarily now for DC to

00:24:52,370 --> 00:24:55,640
see before DC tcp really matters where

00:24:54,170 --> 00:24:57,860
it happens no no no neck where it

00:24:55,640 --> 00:25:01,250
happens yes how many bytes experience it

00:24:57,860 --> 00:25:03,610
right so so so maybe somehow GRL could

00:25:01,250 --> 00:25:11,810
maintain that piece of state as it

00:25:03,610 --> 00:25:14,600
coalesces turn out that most of the time

00:25:11,810 --> 00:25:17,420
we don't have this pattern of one of two

00:25:14,600 --> 00:25:20,030
packet being easy and mark it's Ethan is

00:25:17,420 --> 00:25:22,100
really persistent condition on a switch

00:25:20,030 --> 00:25:24,320
somewhere on the bottleneck so it's very

00:25:22,100 --> 00:25:29,090
rare you have a mix and interleave of

00:25:24,320 --> 00:25:33,320
packet with C and you have like trains

00:25:29,090 --> 00:25:36,020
of packets so you could add some other

00:25:33,320 --> 00:25:40,030
earlier something which propagate to the

00:25:36,020 --> 00:25:44,720
TCP stack oh i received 20 or 30 packets

00:25:40,030 --> 00:25:49,030
of which 10 were marked with e so you

00:25:44,720 --> 00:25:52,460
could do that i I did that but I know

00:25:49,030 --> 00:25:54,770
doing crazy portions because you also

00:25:52,460 --> 00:25:56,570
have a reduction of suing on the sender

00:25:54,770 --> 00:25:59,270
and the sender sends water here so

00:25:56,570 --> 00:26:05,930
packets small audience yes and that's

00:25:59,270 --> 00:26:07,820
also why the the extra and especially at

00:26:05,930 --> 00:26:09,950
Google we use micro micro second

00:26:07,820 --> 00:26:12,350
timestamps so if you have if you are

00:26:09,950 --> 00:26:14,810
sending multiple to sauce shanks there

00:26:12,350 --> 00:26:16,820
are most of the time having different

00:26:14,810 --> 00:26:19,040
microsecond time stamps so the gyro

00:26:16,820 --> 00:26:24,110
cannot aggregate them on the receive

00:26:19,040 --> 00:26:25,460
side yeah now going back to there the

00:26:24,110 --> 00:26:28,000
one that's what the largest increases

00:26:25,460 --> 00:26:31,880
the worker which is the receiver right

00:26:28,000 --> 00:26:33,650
so it probably related maybe to the teat

00:26:31,880 --> 00:26:35,510
this Model Ts also you can also do it

00:26:33,650 --> 00:26:41,500
less coalescence because the smaller

00:26:35,510 --> 00:26:41,500
tier so so any other questions

00:26:42,990 --> 00:26:46,900
so you mentioned earlier on you had

00:26:45,100 --> 00:26:48,640
written a tool that did the analysis

00:26:46,900 --> 00:26:51,310
keycaps could you talk a little bit

00:26:48,640 --> 00:26:53,740
about what that was it was just like a

00:26:51,310 --> 00:26:55,900
Python program that would have you know

00:26:53,740 --> 00:26:58,960
like looks at all the packets and he'd

00:26:55,900 --> 00:27:02,230
put them together per flow and then he

00:26:58,960 --> 00:27:03,730
keeps track of like when a packet was

00:27:02,230 --> 00:27:05,470
saying when it was received

00:27:03,730 --> 00:27:07,540
I mean similar to cotton in some ways

00:27:05,470 --> 00:27:10,690
like were shocked he's doing but I was

00:27:07,540 --> 00:27:13,510
able to like analyze things like at the

00:27:10,690 --> 00:27:16,150
RTT level so for the flourish flow I

00:27:13,510 --> 00:27:18,220
could look at one flow and say okay give

00:27:16,150 --> 00:27:20,290
me the numbers per RTD right so I would

00:27:18,220 --> 00:27:23,800
say how many packets were in fly what

00:27:20,290 --> 00:27:26,260
was the RTT for that RTT and that helped

00:27:23,800 --> 00:27:27,850
me understand what was going on that

00:27:26,260 --> 00:27:30,340
would be a very useful thing generally

00:27:27,850 --> 00:27:32,950
okay when I have time I need to clean it

00:27:30,340 --> 00:27:36,160
because just like a hack then very

00:27:32,950 --> 00:27:39,040
quickly but I'll try to find time to do

00:27:36,160 --> 00:27:42,160
that I have two questions

00:27:39,040 --> 00:27:45,520
yeah that the congestion mentioned in

00:27:42,160 --> 00:27:49,690
your multiple rack yes diagram was it in

00:27:45,520 --> 00:27:51,880
the spine switch or the top of the Rex

00:27:49,690 --> 00:27:53,410
which is where was the condition you can

00:27:51,880 --> 00:27:55,990
see here that most of the discards are

00:27:53,410 --> 00:28:00,310
in the SW so it's all those were on the

00:27:55,990 --> 00:28:02,920
spine switches okay have you experienced

00:28:00,310 --> 00:28:05,200
congestion in the top of thorax which is

00:28:02,920 --> 00:28:07,600
and any effects of DC TCP on the

00:28:05,200 --> 00:28:09,810
round-trip time to circle back to the

00:28:07,600 --> 00:28:09,810
host

00:28:10,410 --> 00:28:16,180
so for these you know there is a little

00:28:12,850 --> 00:28:18,370
bit of discard on the rack type of rack

00:28:16,180 --> 00:28:20,230
for the workers and I also done other

00:28:18,370 --> 00:28:23,410
experiment with DT TCP for traffic

00:28:20,230 --> 00:28:25,720
within Iraq right and in general DC TCP

00:28:23,410 --> 00:28:27,970
behaves really really well I mean it

00:28:25,720 --> 00:28:31,000
gives the windows small it gives the RTT

00:28:27,970 --> 00:28:32,230
much much smaller than cubic and and

00:28:31,000 --> 00:28:35,380
that's why we get a fairness between

00:28:32,230 --> 00:28:37,960
small and large my question was you have

00:28:35,380 --> 00:28:39,610
you have to top of the racks which is on

00:28:37,960 --> 00:28:41,200
your storage side as well as on the

00:28:39,610 --> 00:28:43,270
worker side and you have a spine in

00:28:41,200 --> 00:28:46,330
between and if yes when the congestion

00:28:43,270 --> 00:28:48,970
is on on to the worker side the

00:28:46,330 --> 00:28:51,990
round-trip time to get back the EC enak

00:28:48,970 --> 00:28:53,940
through the spine and to the other host

00:28:51,990 --> 00:28:55,529
this is within a data

00:28:53,940 --> 00:28:57,600
enter some indie identities I mean I

00:28:55,529 --> 00:29:00,899
don't remember explicitly for this but

00:28:57,600 --> 00:29:02,730
was less than a millisecond you know the

00:29:00,899 --> 00:29:05,190
hardware RTT would have been the micro

00:29:02,730 --> 00:29:06,750
you know probably a hundred microseconds

00:29:05,190 --> 00:29:08,730
or less for the hardwood you know for

00:29:06,750 --> 00:29:13,730
the time to send it with no congestion

00:29:08,730 --> 00:29:13,730
so it wasn't it was small okay thanks

00:29:15,230 --> 00:29:36,179
anyone else one question so it looks

00:29:33,750 --> 00:29:39,350
like the testing for regression for this

00:29:36,179 --> 00:29:42,120
particular problem copy you have a very

00:29:39,350 --> 00:29:45,360
you have to set up quite a few things is

00:29:42,120 --> 00:29:48,059
there any way to make the test simpler

00:29:45,360 --> 00:29:52,559
so to continue to regression test as we

00:29:48,059 --> 00:29:55,649
make changes to the kernel so Nate oh I

00:29:52,559 --> 00:29:57,659
do like rat level test what I can use

00:29:55,649 --> 00:29:59,370
Nettie aim to introduce latencies and

00:29:57,659 --> 00:30:00,899
all that and those are very convenient

00:29:59,370 --> 00:30:02,700
because like within a test or they're

00:30:00,899 --> 00:30:05,789
very optimized and it produces really

00:30:02,700 --> 00:30:06,960
nice graphs and tables with a massage

00:30:05,789 --> 00:30:09,480
information you want you can even

00:30:06,960 --> 00:30:11,460
collect pickups if you wanted to from

00:30:09,480 --> 00:30:14,460
remote host and send them automatically

00:30:11,460 --> 00:30:16,559
right so everything but we wanted to

00:30:14,460 --> 00:30:19,860
test in production environment with real

00:30:16,559 --> 00:30:23,009
workloads and it's been much much longer

00:30:19,860 --> 00:30:24,379
that time in months much longer than

00:30:23,009 --> 00:30:27,450
expected

00:30:24,379 --> 00:30:30,299
that's why I was wondering that can this

00:30:27,450 --> 00:30:34,289
test can you can we is there a

00:30:30,299 --> 00:30:35,759
possibility to make the test simpler in

00:30:34,289 --> 00:30:37,379
the sense that that's what I did right

00:30:35,759 --> 00:30:38,970
and that's what I described at the

00:30:37,379 --> 00:30:41,909
original at the beginning but I saw

00:30:38,970 --> 00:30:44,580
these three issues and if I had not done

00:30:41,909 --> 00:30:47,309
that initially if I had gone directly to

00:30:44,580 --> 00:30:48,210
production testing we have seen like oh

00:30:47,309 --> 00:30:50,129
this is horrible

00:30:48,210 --> 00:30:51,750
this is TCP is broken it doesn't work

00:30:50,129 --> 00:30:53,909
right there's probably no way that we

00:30:51,750 --> 00:30:57,269
have cut those issues in that

00:30:53,909 --> 00:30:58,740
environment so I I did the easier tale

00:30:57,269 --> 00:31:00,929
like smaller scale test but at some

00:30:58,740 --> 00:31:03,149
stage you don't have to deploy or test

00:31:00,929 --> 00:31:06,320
it in production right and that's what

00:31:03,149 --> 00:31:08,289
we're trying to do but for

00:31:06,320 --> 00:31:18,750
and testing usually takes a lot longer

00:31:08,289 --> 00:31:24,319
sadly ok thank you very much excuse

00:31:18,750 --> 00:31:24,319

YouTube URL: https://www.youtube.com/watch?v=340dcnyNCkU


