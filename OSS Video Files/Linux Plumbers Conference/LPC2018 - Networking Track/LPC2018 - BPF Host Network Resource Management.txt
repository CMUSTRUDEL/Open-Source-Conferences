Title: LPC2018 - BPF Host Network Resource Management
Publication date: 2018-12-04
Playlist: LPC2018 - Networking Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/113/
speaker:  Lawrence Brakmo (Facebook),  Alexei Starovoitov (Facebook)


Linux currently provides mechanisms for managing and allocating many of the system resources such as CPU, Memory, etc. Network resource management is more complicated since networking deals not only with a local resource, such as CPU management does, but can also deal with a global resource. The goal is not only to provide a mechanism for allocating the local network resource (NIC bandwidth), but also to support management of network resources external to the host, such as link and switch bandwidths.

For networking, the primary mechanism for allocating and managing bandwidth has been the traffic control (tc) subsystem. While tc allows for shaping of outgoing traffic and policing of incoming traffic, it suffers from some drawbacks. The first drawback is a history of performance issues when using the Hierarchical Queuing Discipline (htb) which is usually required for anything other than simple shaping needs. A second drawback is the lack of flexibility usually provided by general programming constructs.

We are in the process of designing and implementing a BPF based framework for efficiently supporting shaping of both egress and ingress traffic based on both local and global network allocations.
Captions: 
	00:00:05,570 --> 00:00:09,770
if you are with us yesterday you got to

00:00:07,910 --> 00:00:12,170
see an awesome presentation by our next

00:00:09,770 --> 00:00:15,259
speaker Lawrence Grecco on data center

00:00:12,170 --> 00:00:16,939
in TCP analysis but today he'll be

00:00:15,259 --> 00:00:19,730
talking about BPF a host network

00:00:16,939 --> 00:00:21,259
resource management I finally figured

00:00:19,730 --> 00:00:23,890
out what's behind him submitting

00:00:21,259 --> 00:00:26,599
multiple talks at the conference

00:00:23,890 --> 00:00:28,939
regardless of whether you like BPF or

00:00:26,599 --> 00:00:31,550
not you still get to see him make a

00:00:28,939 --> 00:00:36,469
presentation so without further ado

00:00:31,550 --> 00:00:38,510
Florence Blackwell thank you

00:00:36,469 --> 00:00:42,200
so she mention that after submitted the

00:00:38,510 --> 00:00:44,930
DC TCP Talk Alexa was very mad because

00:00:42,200 --> 00:00:47,660
the ratio of BPF and nan BPF tags had

00:00:44,930 --> 00:00:49,670
decreased right so then we decided to

00:00:47,660 --> 00:00:53,090
make a presentation and I will work on

00:00:49,670 --> 00:00:55,670
resource management so this is a

00:00:53,090 --> 00:00:57,260
collaboration with Alexa I will do the

00:00:55,670 --> 00:00:58,880
presentation and answer the easy

00:00:57,260 --> 00:01:00,640
questions she'll answer the hard

00:00:58,880 --> 00:01:03,590
questions

00:01:00,640 --> 00:01:05,479
so Linux already supports our locating

00:01:03,590 --> 00:01:09,229
and managing many different resources

00:01:05,479 --> 00:01:10,760
such as CPU and memory Network

00:01:09,229 --> 00:01:13,430
communication is we were harder because

00:01:10,760 --> 00:01:15,560
it's not only a local resource it is

00:01:13,430 --> 00:01:19,340
also a global resource we do not only

00:01:15,560 --> 00:01:21,560
want to manage our egos bandwidth but we

00:01:19,340 --> 00:01:27,280
may also want to manage bandwidth at

00:01:21,560 --> 00:01:29,900
different links in the network so we

00:01:27,280 --> 00:01:34,190
require mechanisms to manage bandwidth

00:01:29,900 --> 00:01:37,100
locally and also externally this already

00:01:34,190 --> 00:01:39,619
on the kinesin 20 control that allows us

00:01:37,100 --> 00:01:42,979
to shape up going traffic and to poly

00:01:39,619 --> 00:01:45,170
police incoming traffic and it has been

00:01:42,979 --> 00:01:46,580
used for also external bandwidth you're

00:01:45,170 --> 00:01:50,960
managing external bandwidth like

00:01:46,580 --> 00:01:53,540
Google's been waiting for sir but TC has

00:01:50,960 --> 00:01:56,470
a history of performance issues you

00:01:53,540 --> 00:01:59,750
don't know when we're using using HTTP

00:01:56,470 --> 00:02:01,759
and there's also an issue when we're

00:01:59,750 --> 00:02:05,540
trying to enforce a bandwidth to

00:02:01,759 --> 00:02:10,060
Shepherd traffic or going traffic where

00:02:05,540 --> 00:02:12,470
we can end up with standing queues and

00:02:10,060 --> 00:02:14,930
even though we can use Cordell to try to

00:02:12,470 --> 00:02:17,689
recreate the size of the queues there's

00:02:14,930 --> 00:02:18,950
also some issues with Cadell that I can

00:02:17,689 --> 00:02:22,430
talk about at the end of the

00:02:18,950 --> 00:02:24,950
and more importantly it like the

00:02:22,430 --> 00:02:29,480
flexibility provided by a more general

00:02:24,950 --> 00:02:31,459
mechanism like BPF and didn't put a heat

00:02:29,480 --> 00:02:33,380
more importantly it doesn't have a cute

00:02:31,459 --> 00:02:37,180
logo like bps right and that's really

00:02:33,380 --> 00:02:40,010
really important for me as you can see

00:02:37,180 --> 00:02:42,590
so the goal is to create a BPF days

00:02:40,010 --> 00:02:44,690
framework for she's efficiently

00:02:42,590 --> 00:02:48,319
supporting the shaping of both equals

00:02:44,690 --> 00:02:53,390
and ingress traffic babe based both on

00:02:48,319 --> 00:02:55,160
local and global network allocations the

00:02:53,390 --> 00:02:56,980
recent assumption for simplicity for us

00:02:55,160 --> 00:03:00,130
is that most of the traffic is either

00:02:56,980 --> 00:03:03,799
TCP or has similar congestion control

00:03:00,130 --> 00:03:05,989
although you know we we also shape the

00:03:03,799 --> 00:03:08,239
other traffic especially for egress is

00:03:05,989 --> 00:03:10,310
release if you have rapid for ingress

00:03:08,239 --> 00:03:13,340
it's something that we're planning to

00:03:10,310 --> 00:03:16,190
implement and we also want to eliminate

00:03:13,340 --> 00:03:18,769
and reduce standing queues so that we do

00:03:16,190 --> 00:03:20,450
not increase latency and we wonder

00:03:18,769 --> 00:03:26,989
flexibility that comes for free when we

00:03:20,450 --> 00:03:29,239
use BPF so sign up with you

00:03:26,989 --> 00:03:33,680
we're using the existing egress and

00:03:29,239 --> 00:03:37,489
ingress C group as KB hooks for egress

00:03:33,680 --> 00:03:40,609
wheither use ECM marking or calls to our

00:03:37,489 --> 00:03:43,880
VP of help participe enter congestion

00:03:40,609 --> 00:03:48,549
window reduction for tcp or just drop

00:03:43,880 --> 00:03:50,600
the packet for ingress we use ECN or

00:03:48,549 --> 00:03:53,840
something similar because we need to

00:03:50,600 --> 00:03:57,230
notify the sender to slow down I'll drop

00:03:53,840 --> 00:03:58,880
in packets locally it doesn't give the

00:03:57,230 --> 00:04:00,980
bandwidth with last you know the packets

00:03:58,880 --> 00:04:02,720
were already came in and we need a

00:04:00,980 --> 00:04:04,790
mechanism to make sure those packets do

00:04:02,720 --> 00:04:07,239
not come in at a rate that is not

00:04:04,790 --> 00:04:07,239
acceptable

00:04:07,430 --> 00:04:11,840
we also have the idea of using the

00:04:09,319 --> 00:04:13,489
scopes to manage bandwidth so in the

00:04:11,840 --> 00:04:15,920
days that we could have a scope for a C

00:04:13,489 --> 00:04:18,169
group so that we can enforce bandwidth

00:04:15,920 --> 00:04:19,579
for a particular C group but also have

00:04:18,169 --> 00:04:21,019
scopes that may be applied to a

00:04:19,579 --> 00:04:25,010
particular

00:04:21,019 --> 00:04:27,789
backbone link or some you know rack on

00:04:25,010 --> 00:04:31,550
the other side and they did that if

00:04:27,789 --> 00:04:32,749
socket can be long - couple scopes only

00:04:31,550 --> 00:04:37,399
no more than four

00:04:32,749 --> 00:04:39,979
and then we can enforce the language

00:04:37,399 --> 00:04:45,949
allocations based on whatever is

00:04:39,979 --> 00:04:48,259
limiting us at any particular time so

00:04:45,949 --> 00:04:50,809
this is an example scope but is a bad

00:04:48,259 --> 00:04:53,239
example sorry rather than having two

00:04:50,809 --> 00:04:55,189
holes it should be one host because

00:04:53,239 --> 00:04:58,779
we're gonna have scopes that are common

00:04:55,189 --> 00:05:01,219
to two holes right now so imagine that

00:04:58,779 --> 00:05:03,559
there's only one host we have two

00:05:01,219 --> 00:05:07,099
different C groups on that host and then

00:05:03,559 --> 00:05:09,589
like for one flow we could have three

00:05:07,099 --> 00:05:11,779
scopes the one for AC group and then one

00:05:09,589 --> 00:05:14,509
for the backbone and then another one

00:05:11,779 --> 00:05:17,360
for you know ingress link to a

00:05:14,509 --> 00:05:19,279
particular server and similarly for the

00:05:17,360 --> 00:05:22,099
second one who have a different cigrip

00:05:19,279 --> 00:05:28,279
scope but that other two scopes could be

00:05:22,099 --> 00:05:30,259
the same so how do we do the bandwidth

00:05:28,279 --> 00:05:34,159
management right now we're using a

00:05:30,259 --> 00:05:36,409
virtual cue to track bandwidth use first

00:05:34,159 --> 00:05:38,800
code and it's a very simple structure

00:05:36,409 --> 00:05:40,759
all we need to have is to have a credit

00:05:38,800 --> 00:05:42,709
for the particular bandwidth so it's

00:05:40,759 --> 00:05:45,319
like a kind of like a leaky bucket and

00:05:42,709 --> 00:05:47,929
we need to have the last time when we

00:05:45,319 --> 00:05:50,239
updated the credit so for egress

00:05:47,929 --> 00:05:52,879
whenever we're going to send a packet we

00:05:50,239 --> 00:05:55,129
need to update the credit based on how

00:05:52,879 --> 00:05:58,459
much time passed since we send a packet

00:05:55,129 --> 00:06:00,619
in the past and we also need to

00:05:58,459 --> 00:06:02,300
obviously bound it because we don't send

00:06:00,619 --> 00:06:04,789
a packet for a long time we don't want

00:06:02,300 --> 00:06:06,529
to have like very large amount of credit

00:06:04,789 --> 00:06:11,149
right so the credit needs to be bounded

00:06:06,529 --> 00:06:13,279
and then when we you know where we're

00:06:11,149 --> 00:06:15,439
going to send a packet we need to

00:06:13,279 --> 00:06:19,249
decrease the credit based on the length

00:06:15,439 --> 00:06:22,339
of the packet in bytes but on the wire

00:06:19,249 --> 00:06:25,009
right so typically we deal with TSO

00:06:22,339 --> 00:06:28,369
packets so we need to do some accounting

00:06:25,009 --> 00:06:31,669
to add the overhead for the all the

00:06:28,369 --> 00:06:34,219
packet headers that will be used bits on

00:06:31,669 --> 00:06:36,949
the wire you know the extra TCP headers

00:06:34,219 --> 00:06:38,989
one for each small packet that was in

00:06:36,949 --> 00:06:44,809
the wire for the IP for the ethernet

00:06:38,989 --> 00:06:46,040
etcetera to get better accounting so the

00:06:44,809 --> 00:06:48,790
code called just an algorithm

00:06:46,040 --> 00:06:51,740
we're using to enforce the bandwidth

00:06:48,790 --> 00:06:53,990
it's you know we use the credit and if

00:06:51,740 --> 00:06:56,360
the credit is positive then everything

00:06:53,990 --> 00:06:57,530
is great you know we can keep sending so

00:06:56,360 --> 00:07:00,730
when the credit is negative it means

00:06:57,530 --> 00:07:03,590
that we had used more than the allowed

00:07:00,730 --> 00:07:06,140
bandwidth but we need to be able to do

00:07:03,590 --> 00:07:07,910
that to deal with bursts right I mean

00:07:06,140 --> 00:07:10,610
that's why we have queuing in general

00:07:07,910 --> 00:07:12,800
and right now we're using as I said and

00:07:10,610 --> 00:07:14,240
this is done in the BPF program is just

00:07:12,800 --> 00:07:17,360
happens to do for I'm doing right now

00:07:14,240 --> 00:07:18,680
but the whole point of this work is we

00:07:17,360 --> 00:07:21,290
had the flexibility that people can

00:07:18,680 --> 00:07:23,030
experiment with different condition

00:07:21,290 --> 00:07:25,460
algorithms you know to turn to enforce

00:07:23,030 --> 00:07:28,670
it so in my case I have two thresholds a

00:07:25,460 --> 00:07:32,090
marked threshold and a drop threshold so

00:07:28,670 --> 00:07:35,480
if the credit falls in the yellow area

00:07:32,090 --> 00:07:37,250
still greater than the marked threshold

00:07:35,480 --> 00:07:39,110
I will send it and I will not worry

00:07:37,250 --> 00:07:42,320
about it and in between the mark and the

00:07:39,110 --> 00:07:46,570
threshold I will mark it and what I mean

00:07:42,320 --> 00:07:50,480
might mark depends on the type of packet

00:07:46,570 --> 00:07:52,700
that is going out and then if we go

00:07:50,480 --> 00:07:55,610
beyond the drug threshold we will drop

00:07:52,700 --> 00:07:59,690
the packet in practice I also have a

00:07:55,610 --> 00:08:01,730
small area for small packets because I

00:07:59,690 --> 00:08:03,980
don't want to drop like sim packets in

00:08:01,730 --> 00:08:06,980
ik packets or small control packets

00:08:03,980 --> 00:08:09,050
unnecessarily so I allocate some amount

00:08:06,980 --> 00:08:13,310
of memory that can only be used by small

00:08:09,050 --> 00:08:15,740
packets and it turns out that it's it's

00:08:13,310 --> 00:08:17,300
very useful and in some of my experiment

00:08:15,740 --> 00:08:19,420
was required to this one I was running

00:08:17,300 --> 00:08:23,060
multiple flows some of the flow to

00:08:19,420 --> 00:08:25,190
initially were not able to start because

00:08:23,060 --> 00:08:31,690
their control packets were being dropped

00:08:25,190 --> 00:08:34,900
you know for networks for example and so

00:08:31,690 --> 00:08:37,580
each of their some of the threshold

00:08:34,900 --> 00:08:40,550
ignore the bottom one that's wrong so I

00:08:37,580 --> 00:08:46,130
have like six hundred and twenty packet

00:08:40,550 --> 00:08:49,900
sizes for the 12 shows right now for I

00:08:46,130 --> 00:08:53,570
think I skipped that okay so for marking

00:08:49,900 --> 00:08:55,550
if the packet has to port CCN I would

00:08:53,570 --> 00:08:59,779
use an easy end mark for it right so for

00:08:55,550 --> 00:09:04,100
DC TCP I can market for TCP that is 90

00:08:59,779 --> 00:09:06,230
tn what I will do is I will call a

00:09:04,100 --> 00:09:08,930
helper function that will call the

00:09:06,230 --> 00:09:11,120
congestion window reduction for TCP and

00:09:08,930 --> 00:09:13,009
I call it with a linear probability for

00:09:11,120 --> 00:09:14,930
that closer I aim to that drug threshold

00:09:13,009 --> 00:09:17,389
the more likely I am to call it right

00:09:14,930 --> 00:09:20,899
and the idea here is like similar to red

00:09:17,389 --> 00:09:22,670
I don't want to call too many in a row

00:09:20,899 --> 00:09:27,649
because TCP is going to behave badly

00:09:22,670 --> 00:09:31,459
under those cases and the probability

00:09:27,649 --> 00:09:34,370
I'm doing is just linear so when the

00:09:31,459 --> 00:09:36,139
credit goes to the marking threshold I

00:09:34,370 --> 00:09:38,269
will start marking it you know and the

00:09:36,139 --> 00:09:40,279
closer I am to that threshold the higher

00:09:38,269 --> 00:09:43,850
the probability that I would call

00:09:40,279 --> 00:09:47,449
congestion window reduction for it and I

00:09:43,850 --> 00:09:49,220
think in the future I plan to also start

00:09:47,449 --> 00:09:51,230
experimenting with different marking

00:09:49,220 --> 00:09:53,600
functions you know that have more

00:09:51,230 --> 00:09:56,990
interesting shapes and also change the

00:09:53,600 --> 00:09:58,910
behavior based probably on the RTT you

00:09:56,990 --> 00:10:01,399
know like TCP has this issue with

00:09:58,910 --> 00:10:04,490
furnace between flows with different

00:10:01,399 --> 00:10:07,160
activities and one way to handle that

00:10:04,490 --> 00:10:08,870
would be for the marking function to

00:10:07,160 --> 00:10:11,269
take the RTT into account so that we get

00:10:08,870 --> 00:10:17,809
more fairness when we are shaping the

00:10:11,269 --> 00:10:21,050
egress when we so initially we saw some

00:10:17,809 --> 00:10:23,000
issues the first one is that when we

00:10:21,050 --> 00:10:26,209
drop a packet in a queuing discipline

00:10:23,000 --> 00:10:28,100
you know in the traffic control TCP is

00:10:26,209 --> 00:10:29,809
notified that the packet was dropped and

00:10:28,100 --> 00:10:32,629
then it will automatically call the

00:10:29,809 --> 00:10:35,660
congestion window reduction when a

00:10:32,629 --> 00:10:39,410
packet is call with the V PFC group

00:10:35,660 --> 00:10:41,920
egress it is TCP is not notified so TCP

00:10:39,410 --> 00:10:44,300
doesn't know that it was wrapped and

00:10:41,920 --> 00:10:46,220
initially the I thought I was bad but

00:10:44,300 --> 00:10:49,610
was really good but that allows us to

00:10:46,220 --> 00:10:51,279
implement our own algorithm right for

00:10:49,610 --> 00:10:53,449
example in this case the probabilistic

00:10:51,279 --> 00:10:56,480
marking algorithm to reduce the

00:10:53,449 --> 00:10:59,629
congestion window so it turned out to be

00:10:56,480 --> 00:11:02,779
for the better the other thing we know I

00:10:59,629 --> 00:11:05,990
notice is that we were having high tail

00:11:02,779 --> 00:11:08,329
latencies in cases when we were dropping

00:11:05,990 --> 00:11:11,120
a packet but where there were no packets

00:11:08,329 --> 00:11:13,710
in transit so there were no acts coming

00:11:11,120 --> 00:11:20,730
in that would trigger or trust me

00:11:13,710 --> 00:11:22,470
or that packet so for example if I was

00:11:20,730 --> 00:11:25,080
running an experiment within Iraq and I

00:11:22,470 --> 00:11:26,730
had a lot of flaws it's a nine flows the

00:11:25,080 --> 00:11:29,520
obvious congestion window size to fully

00:11:26,730 --> 00:11:32,520
utilize let's say one gigabit limit for

00:11:29,520 --> 00:11:35,279
the C group would be one packet or

00:11:32,520 --> 00:11:37,080
congestion window one or less so that

00:11:35,279 --> 00:11:39,990
that means word I would need to drop

00:11:37,080 --> 00:11:41,640
packets to enforce that limit and there

00:11:39,990 --> 00:11:43,560
would be another packet to trigger you

00:11:41,640 --> 00:11:45,240
know send him we send in that packet

00:11:43,560 --> 00:11:48,390
right

00:11:45,240 --> 00:11:51,420
and normally TCP uses the probe timer to

00:11:48,390 --> 00:11:53,660
resend it which is 200 milliseconds so

00:11:51,420 --> 00:11:58,140
we were seeing really bad performance so

00:11:53,660 --> 00:11:59,850
the solution was to detect when we drop

00:11:58,140 --> 00:12:02,250
a packet and there are no packets in

00:11:59,850 --> 00:12:04,110
flight and we'd used the probe timer to

00:12:02,250 --> 00:12:06,570
like 20 milliseconds and I have some

00:12:04,110 --> 00:12:11,850
numbers of the improvement because of

00:12:06,570 --> 00:12:14,399
that and the other issue that I saw was

00:12:11,850 --> 00:12:18,450
that to update a credit and the last

00:12:14,399 --> 00:12:20,540
time that's a critical section and when

00:12:18,450 --> 00:12:22,920
I started having more and more flows

00:12:20,540 --> 00:12:24,660
things started to behaving really weird

00:12:22,920 --> 00:12:26,760
like I would put a limit of one gigabit

00:12:24,660 --> 00:12:28,640
and it would if I had it in that flows

00:12:26,760 --> 00:12:33,300
it would go to one and a half big a bit

00:12:28,640 --> 00:12:35,970
and the reason was that I would update

00:12:33,300 --> 00:12:38,070
the credit and then the last time would

00:12:35,970 --> 00:12:40,070
be updated by somebody else before I did

00:12:38,070 --> 00:12:43,770
it and things would look really weird so

00:12:40,070 --> 00:12:49,500
they are right now we don't have peanuts

00:12:43,770 --> 00:12:51,600
and vpf and so my hack was to spin like

00:12:49,500 --> 00:12:53,580
the whole BPF program which obviously is

00:12:51,600 --> 00:12:56,040
another ideal but I wanted to be able to

00:12:53,580 --> 00:13:00,529
keep experimenting the solutions that

00:12:56,040 --> 00:13:03,029
were working on are two one is Alexa and

00:13:00,529 --> 00:13:05,130
other people in the team are working on

00:13:03,029 --> 00:13:07,230
implementing spin lock for BPF and

00:13:05,130 --> 00:13:14,850
another solution would be to use a data

00:13:07,230 --> 00:13:16,079
structure that does not require locks so

00:13:14,850 --> 00:13:18,120
now I'm going to talk about some of the

00:13:16,079 --> 00:13:20,310
experiments I mean I'm talking really

00:13:18,120 --> 00:13:20,520
fast so I'm going to be able to let you

00:13:20,310 --> 00:13:26,040
go

00:13:20,520 --> 00:13:27,300
probably sooner than expected so the

00:13:26,040 --> 00:13:27,540
experience right now we're only time

00:13:27,300 --> 00:13:30,089
with

00:13:27,540 --> 00:13:32,279
one scope okay so obviously I need to

00:13:30,089 --> 00:13:34,110
deploy with multiple scopes multiple

00:13:32,279 --> 00:13:37,440
entities a lot more work to be done

00:13:34,110 --> 00:13:40,920
still and for this experiment one host

00:13:37,440 --> 00:13:44,250
says to another host in the same rack

00:13:40,920 --> 00:13:46,050
and we're imposing a limit typically one

00:13:44,250 --> 00:13:51,180
gigabit per second or five gigabit per

00:13:46,050 --> 00:13:56,610
second and I'm sending one to five or

00:13:51,180 --> 00:13:58,440
nine flows one of those for the two five

00:13:56,610 --> 00:14:02,279
and nine is ten kilobyte RPC the other

00:13:58,440 --> 00:14:05,250
one is like one megabyte RPC and then I

00:14:02,279 --> 00:14:08,220
limit the rate either by this BPF nrm

00:14:05,250 --> 00:14:12,870
network resource management or to twist

00:14:08,220 --> 00:14:15,089
TC using HTTP and in some cases i also

00:14:12,870 --> 00:14:21,149
experimented with adding latency through

00:14:15,089 --> 00:14:25,079
net en like ten milliseconds okay so

00:14:21,149 --> 00:14:27,329
these results initially are to choke

00:14:25,079 --> 00:14:29,250
case the difference between the default

00:14:27,329 --> 00:14:31,170
pro timer and reducing it in the case

00:14:29,250 --> 00:14:34,920
where we'll top in the packet under

00:14:31,170 --> 00:14:38,490
another packets in transit so let's say

00:14:34,920 --> 00:14:42,000
so we have one two and enclose the first

00:14:38,490 --> 00:14:44,220
two columns are for cubic and they

00:14:42,000 --> 00:14:47,459
showed the aggregate bandwidth for all

00:14:44,220 --> 00:14:48,990
of the flaws the first column is with

00:14:47,459 --> 00:14:50,880
the large block timer the second one

00:14:48,990 --> 00:14:52,649
when I redid the prop timer to 10

00:14:50,880 --> 00:14:55,410
milliseconds when there are no other

00:14:52,649 --> 00:14:58,170
packets in flight so obviously a big

00:14:55,410 --> 00:15:01,500
change is the with one flow we can only

00:14:58,170 --> 00:15:04,170
get half the bandwidth that we were

00:15:01,500 --> 00:15:06,209
enforcing but when we reviewed the timer

00:15:04,170 --> 00:15:10,230
we could go to a hundred megabits per

00:15:06,209 --> 00:15:12,180
second as opposed to five hundred and

00:15:10,230 --> 00:15:13,829
four to five and and flows you know

00:15:12,180 --> 00:15:16,800
what's a little better not notice here

00:15:13,829 --> 00:15:19,740
that in this case I'm going beyond the

00:15:16,800 --> 00:15:21,839
one gigabit per limiting force and the

00:15:19,740 --> 00:15:25,439
reason is that these numbers were before

00:15:21,839 --> 00:15:31,439
I did a spin lock hack to prevent this

00:15:25,439 --> 00:15:34,550
problem right so if we look at the 99%

00:15:31,439 --> 00:15:37,079
latencies for the RPCs

00:15:34,550 --> 00:15:40,139
and I think is a believer for the one

00:15:37,079 --> 00:15:41,340
megabyte our pcs with the normal protein

00:15:40,139 --> 00:15:43,560
or the legacy

00:15:41,340 --> 00:15:46,530
horrible you know like we have our tails

00:15:43,560 --> 00:15:49,860
or we have we're depending on the probe

00:15:46,530 --> 00:15:53,640
timer or the or RTOS there will be about

00:15:49,860 --> 00:15:56,610
200 milliseconds once we have the hacker

00:15:53,640 --> 00:15:58,710
fixed to reduce the probe timer then the

00:15:56,610 --> 00:16:00,890
tail latencies for cubic decrease to

00:15:58,710 --> 00:16:04,980
more reasonable levels

00:16:00,890 --> 00:16:06,840
similarly for DC TCP this anticip is

00:16:04,980 --> 00:16:09,060
really good at fully utilizing the the

00:16:06,840 --> 00:16:12,690
bandwidth even without the probe timer

00:16:09,060 --> 00:16:15,180
so it's not affected as much by that by

00:16:12,690 --> 00:16:19,010
the latencies you know are better

00:16:15,180 --> 00:16:26,480
especially when we have a lot of flaws

00:16:19,010 --> 00:16:32,910
with the smaller probe time so now these

00:16:26,480 --> 00:16:35,430
are numbers comparing different tcp

00:16:32,910 --> 00:16:40,020
flavors so in the first case i'm using

00:16:35,430 --> 00:16:41,820
cubic with TC and HTV to pose the 1

00:16:40,020 --> 00:16:45,120
gigabit per second limit right so I want

00:16:41,820 --> 00:16:46,830
all of the flows from this C group not

00:16:45,120 --> 00:16:49,410
to exceed one little bit per limit I

00:16:46,830 --> 00:16:51,990
mean using TC second one I am using VC

00:16:49,410 --> 00:16:55,260
TCP ignored the EC n equal to 0 does

00:16:51,990 --> 00:16:57,600
about the Z TCP uses CCM and then I'm

00:16:55,260 --> 00:17:00,480
using cubic with and we already see em

00:16:57,600 --> 00:17:02,730
and using the three bottom ones used in

00:17:00,480 --> 00:17:04,320
RM right and if we look at the graphs I

00:17:02,730 --> 00:17:06,810
highlighted some interesting things one

00:17:04,320 --> 00:17:11,070
is something that I already covered that

00:17:06,810 --> 00:17:12,120
when one flow cubic with no ECL is

00:17:11,070 --> 00:17:14,850
underutilizing

00:17:12,120 --> 00:17:17,040
the bandwidth we have a 1 gigabit limit

00:17:14,850 --> 00:17:20,030
but it's only using about 800 megabits

00:17:17,040 --> 00:17:24,870
per second the other one which are the

00:17:20,030 --> 00:17:27,720
the circles so this graph is showing two

00:17:24,870 --> 00:17:30,540
things one is the aggregate rate on the

00:17:27,720 --> 00:17:34,470
left axis and with the bars and then the

00:17:30,540 --> 00:17:37,980
diamonds are showing the rtt seen by

00:17:34,470 --> 00:17:40,410
these flows the latency and we notice

00:17:37,980 --> 00:17:43,650
that by using TC the latencies are

00:17:40,410 --> 00:17:48,860
really large are like 6 milliseconds as

00:17:43,650 --> 00:17:51,770
opposed to be microseconds for the

00:17:48,860 --> 00:17:54,700
interim solution

00:17:51,770 --> 00:17:58,700
right so the problem with DC is that

00:17:54,700 --> 00:18:01,520
like if I'm using HTTP by itself with

00:17:58,700 --> 00:18:04,040
fair queuing we need it you will not

00:18:01,520 --> 00:18:08,059
start shaping until it starts dropping

00:18:04,040 --> 00:18:09,800
so it will feel the a thousand packet

00:18:08,059 --> 00:18:15,200
queue and then you will start you know

00:18:09,800 --> 00:18:17,540
like dropping packets for cubic so we

00:18:15,200 --> 00:18:20,300
end up with a standing queue we could

00:18:17,540 --> 00:18:22,880
use Cordell 224 smaller queue the prime

00:18:20,300 --> 00:18:25,490
would Cordell is that it dropped packets

00:18:22,880 --> 00:18:27,740
on egress not ingress so it cannot

00:18:25,490 --> 00:18:30,530
notify TCP that you drop the packet and

00:18:27,740 --> 00:18:32,360
for TCP this is like a a packet drop

00:18:30,530 --> 00:18:34,220
someplace in the network so there are

00:18:32,360 --> 00:18:37,490
some issues that kind of chord because

00:18:34,220 --> 00:18:39,350
of that also but we don't have this

00:18:37,490 --> 00:18:41,900
issue with an RM you know we don't have

00:18:39,350 --> 00:18:44,059
funding queues with an array we are not

00:18:41,900 --> 00:18:47,090
doing any queuing the TC did disappear

00:18:44,059 --> 00:18:49,280
during some period but the way it's been

00:18:47,090 --> 00:18:53,000
implemented with algorithm it doesn't

00:18:49,280 --> 00:18:56,179
create any any standing queues so this

00:18:53,000 --> 00:18:59,030
one is going to show the rates for the

00:18:56,179 --> 00:19:03,650
one kilobyte the one megabyte are pcs

00:18:59,030 --> 00:19:05,720
and the ten kilobyte are pieces so we're

00:19:03,650 --> 00:19:08,750
showing the rate of the one megabyte

00:19:05,720 --> 00:19:11,450
with the bars and we're showing the race

00:19:08,750 --> 00:19:17,090
for the 10 kilo by our pcs with the

00:19:11,450 --> 00:19:20,390
Diamonds the thing you can see if we see

00:19:17,090 --> 00:19:22,760
is that the rates are much smaller when

00:19:20,390 --> 00:19:24,850
we use TC and the reason obviously is

00:19:22,760 --> 00:19:27,440
that because we have a standing queue

00:19:24,850 --> 00:19:30,590
every RTT is going to be 6 milliseconds

00:19:27,440 --> 00:19:33,110
right so if you can only send 10

00:19:30,590 --> 00:19:36,440
kilobytes per 6 milliseconds does the

00:19:33,110 --> 00:19:38,870
rtt so that limits your rate because we

00:19:36,440 --> 00:19:40,820
don't have a standing queue the NRL

00:19:38,870 --> 00:19:43,730
solution whether it's cubic the CTC

00:19:40,820 --> 00:19:46,910
Pearson like that can be more fair to

00:19:43,730 --> 00:19:51,530
the smaller IPC flows you know they they

00:19:46,910 --> 00:19:56,000
achieve almost very close to parity as

00:19:51,530 --> 00:19:59,559
compared to the 1 megabyte RT C's and if

00:19:56,000 --> 00:19:59,559
there any question feel free to stop me

00:20:00,100 --> 00:20:04,669
what about the latencies

00:20:02,419 --> 00:20:07,220
so once again this is the one you get at

00:20:04,669 --> 00:20:10,399
one gigabit per second limit we're

00:20:07,220 --> 00:20:12,710
looking at the 99 push my teen I'm point

00:20:10,399 --> 00:20:15,919
nine percent latencies on the left axis

00:20:12,710 --> 00:20:19,220
with the bars and the 50% latencies on

00:20:15,919 --> 00:20:22,580
the right axis with the Diamonds okay

00:20:19,220 --> 00:20:23,779
and the idea here is to compare how do

00:20:22,580 --> 00:20:29,480
they look like you know are they the

00:20:23,779 --> 00:20:34,100
same or not and we noted that for the

00:20:29,480 --> 00:20:37,700
one megabyte our pcs the latencies are

00:20:34,100 --> 00:20:42,529
similar for using TC ID using the city

00:20:37,700 --> 00:20:45,049
CP whereas using cubic either with ACN

00:20:42,529 --> 00:20:48,529
or non ecn the latency is you know

00:20:45,049 --> 00:20:51,679
there's a disparity in the latencies the

00:20:48,529 --> 00:20:53,749
50% latencies under 99% latency you know

00:20:51,679 --> 00:20:58,879
did are much larger we're having some

00:20:53,749 --> 00:21:01,730
issues which may be related to the

00:20:58,879 --> 00:21:04,389
algorithm I chose to do the marking of

00:21:01,730 --> 00:21:07,669
cubic traffic right you had a linear

00:21:04,389 --> 00:21:09,740
function to do the marking I need to

00:21:07,669 --> 00:21:13,940
play with different algorithms but DC

00:21:09,740 --> 00:21:17,389
TCP does really really well if we now

00:21:13,940 --> 00:21:23,049
look at that tank kilobyte RPC latency

00:21:17,389 --> 00:21:23,049
now the problem is TC obviously right

00:21:24,070 --> 00:21:36,970
let's see so the latencies are very high

00:21:29,210 --> 00:21:42,950
as compared to this official P let's see

00:21:36,970 --> 00:21:44,779
and the tell a kiss is also for for

00:21:42,950 --> 00:21:50,059
cubic an arraignment still a little

00:21:44,779 --> 00:21:51,860
higher so with you know as I say may be

00:21:50,059 --> 00:21:54,619
due to the greedy mobile using pop but

00:21:51,860 --> 00:21:56,690
for the city city they look great

00:21:54,619 --> 00:21:58,580
I mean like the ninety-nine point nine

00:21:56,690 --> 00:22:04,029
and the filter percentile are very close

00:21:58,580 --> 00:22:04,029
together right so that's very nice

00:22:08,570 --> 00:22:15,630
okay so conclusion for this experiment

00:22:12,590 --> 00:22:18,270
we have similar aggregate rate for all

00:22:15,630 --> 00:22:20,490
the different experiments we have higher

00:22:18,270 --> 00:22:22,800
TDS when we are using the queuing

00:22:20,490 --> 00:22:25,400
discipline to secure discipline to to

00:22:22,800 --> 00:22:29,160
enforce the one gigabit per second limit

00:22:25,400 --> 00:22:32,430
using TC size unfair meaning thank Allah

00:22:29,160 --> 00:22:34,980
by our pcs are punish the gear up to 20

00:22:32,430 --> 00:22:37,230
times less bandwidth but the one

00:22:34,980 --> 00:22:42,270
megabyte our pcs because of the standing

00:22:37,230 --> 00:22:45,330
queue cubic and cubic ECM using an array

00:22:42,270 --> 00:22:47,720
map higher tell latencies and this

00:22:45,330 --> 00:22:51,510
ethnicity has really nice tail latencies

00:22:47,720 --> 00:22:55,770
you know ten twenty times smaller that's

00:22:51,510 --> 00:22:58,470
compared to the other ones so what I did

00:22:55,770 --> 00:23:01,950
now is I was curious to see what would

00:22:58,470 --> 00:23:04,500
happen if I increase the latency to ten

00:23:01,950 --> 00:23:06,930
milliseconds so I used neti m2 on the

00:23:04,500 --> 00:23:11,790
receiving side to increase the latency

00:23:06,930 --> 00:23:14,900
to ten milliseconds and you know let's

00:23:11,790 --> 00:23:22,710
see what happened so what happened was

00:23:14,900 --> 00:23:26,010
the rates for TC yeah HTV with fur

00:23:22,710 --> 00:23:31,040
cueing the the river one flow was not as

00:23:26,010 --> 00:23:35,340
good a series for DC TCP or cubic right

00:23:31,040 --> 00:23:37,440
the TC achieve a lower rate the

00:23:35,340 --> 00:23:39,450
aggregated rate for nine flows are just

00:23:37,440 --> 00:23:42,420
about the same for all of them except

00:23:39,450 --> 00:23:49,200
for HTV which were queuing is a little

00:23:42,420 --> 00:23:56,240
bit less the one flow one megabyte 99%

00:23:49,200 --> 00:23:59,580
latencies are really good for DC TCP 13

00:23:56,240 --> 00:24:02,760
milliseconds for cubic

00:23:59,580 --> 00:24:08,820
also with nrm and delivered higher for

00:24:02,760 --> 00:24:12,570
the htb solution however the ninth flows

00:24:08,820 --> 00:24:15,060
are better for the queuing discipline

00:24:12,570 --> 00:24:18,060
solution as opposed to the interim

00:24:15,060 --> 00:24:20,429
solution they're not too bad for the DC

00:24:18,060 --> 00:24:23,970
TCP 143 versus 100

00:24:20,429 --> 00:24:25,769
or 85 and the other ones maybe also you

00:24:23,970 --> 00:24:27,419
know like all of these I need to pay a

00:24:25,769 --> 00:24:29,519
run with the algorithm to do the marking

00:24:27,419 --> 00:24:31,859
which thank God is BPF so we can do

00:24:29,519 --> 00:24:38,100
whatever we want to it is not fixed in

00:24:31,859 --> 00:24:39,989
the kernel or or an API okay so what are

00:24:38,100 --> 00:24:45,629
the summary and I think I already

00:24:39,989 --> 00:24:47,309
mentioned it I'm going to skip this and

00:24:45,629 --> 00:24:51,509
then I also run with five gigabits per

00:24:47,309 --> 00:24:56,729
second just to see how things look like

00:24:51,509 --> 00:24:58,739
and you know everybody can has good

00:24:56,729 --> 00:25:00,389
aggregate rate you know I'm at the way

00:24:58,739 --> 00:25:03,179
this rate that I'm sure here are the

00:25:00,389 --> 00:25:05,340
good put not the actual wire rate right

00:25:03,179 --> 00:25:07,320
so they had to be less than the five

00:25:05,340 --> 00:25:13,769
gigabits per second because this is the

00:25:07,320 --> 00:25:15,419
other good put going through so they all

00:25:13,769 --> 00:25:21,929
received good aggregate rate with one

00:25:15,419 --> 00:25:26,609
and nine flows the latencies are better

00:25:21,929 --> 00:25:33,749
for HDTV they're not too bad for DC TCP

00:25:26,609 --> 00:25:36,629
and the Tecla by latency is are better

00:25:33,749 --> 00:25:38,129
for DC TCP and the NR in solution again

00:25:36,629 --> 00:25:40,859
because we're gonna have any of the

00:25:38,129 --> 00:25:42,330
standing queues but you can also see

00:25:40,859 --> 00:25:44,309
that the rate that they achieve are much

00:25:42,330 --> 00:25:45,809
higher for the interim solution because

00:25:44,309 --> 00:25:49,859
there is no standing queue the thing

00:25:45,809 --> 00:25:51,929
kilobyte flows are you know about 200

00:25:49,859 --> 00:25:53,999
megabits per second where there are only

00:25:51,929 --> 00:25:56,099
35 megabits per second with the htb

00:25:53,999 --> 00:25:58,710
solution right because of the standing

00:25:56,099 --> 00:26:00,899
queue and I probably need to experiment

00:25:58,710 --> 00:26:03,149
with the Coryell and see further the

00:26:00,899 --> 00:26:05,309
trade-off between dropping packets in

00:26:03,149 --> 00:26:11,210
the ingress and having a smaller

00:26:05,309 --> 00:26:14,899
standing queue because of that okay I

00:26:11,210 --> 00:26:14,899
think I said all this

00:26:16,390 --> 00:26:25,390
okay so for the Egret how do you find

00:26:21,190 --> 00:26:30,100
out the latest like remember I'd send

00:26:25,390 --> 00:26:31,630
some okay so I don't have the numbers

00:26:30,100 --> 00:26:34,299
here but I'll tell you about that number

00:26:31,630 --> 00:26:36,549
so I also played around with the ingress

00:26:34,299 --> 00:26:38,980
solution right so I use the same

00:26:36,549 --> 00:26:42,270
algorithm you know ever talk you with

00:26:38,980 --> 00:26:49,450
the same markings and what I do is like

00:26:42,270 --> 00:26:53,230
I did it for both cubic by itself and DC

00:26:49,450 --> 00:26:56,290
TCP but obviously with cubic you need to

00:26:53,230 --> 00:26:57,730
drop and it's a drab so the lit inside

00:26:56,290 --> 00:27:01,840
bad so I'm just going to talk about the

00:26:57,730 --> 00:27:04,090
numbers for DC TCP shaping work great

00:27:01,840 --> 00:27:08,320
when I checked it at one gigabit per

00:27:04,090 --> 00:27:11,080
second I got one gigabit per second

00:27:08,320 --> 00:27:13,240
right and the latency it looked really

00:27:11,080 --> 00:27:14,919
good I mean there was no penalty the

00:27:13,240 --> 00:27:17,200
difference between the 50 percentile and

00:27:14,919 --> 00:27:19,299
the 99 percent of latency for the one

00:27:17,200 --> 00:27:23,200
megabyte and that the kilobytes were

00:27:19,299 --> 00:27:31,809
very close so for shaping ingress it

00:27:23,200 --> 00:27:34,030
work extremely well I had an issue where

00:27:31,809 --> 00:27:36,400
I was actually dropping if you know

00:27:34,030 --> 00:27:39,100
because I get used the same algorithm so

00:27:36,400 --> 00:27:42,309
if you got into the drop zone I was

00:27:39,100 --> 00:27:45,549
wrapping packet so in one one of the

00:27:42,309 --> 00:27:47,080
experiment with the DC TCP I had to drop

00:27:45,549 --> 00:27:50,860
and then the latencies were really bad

00:27:47,080 --> 00:27:52,150
but as long as I was not wrapping it

00:27:50,860 --> 00:27:53,380
worked really well so it seemed like it

00:27:52,150 --> 00:27:56,530
would work really well as a solution for

00:27:53,380 --> 00:27:58,809
ingress and I think one of the things we

00:27:56,530 --> 00:28:01,600
want to experiment also is that can we

00:27:58,809 --> 00:28:04,030
use this idea to also protect from

00:28:01,600 --> 00:28:07,419
Minka's in the host right where we could

00:28:04,030 --> 00:28:08,919
have a scope we have a scope or Percy

00:28:07,419 --> 00:28:10,900
group who could also have this call for

00:28:08,919 --> 00:28:15,850
the whole machine Android to enforce

00:28:10,900 --> 00:28:17,770
let's say 9 gigabit per second limit so

00:28:15,850 --> 00:28:21,549
that we can absorb burst and can use the

00:28:17,770 --> 00:28:24,309
queuing in the switch but you know

00:28:21,549 --> 00:28:26,679
hopefully the may protect us against

00:28:24,309 --> 00:28:28,950
incas especially like in the multi host

00:28:26,679 --> 00:28:32,000
environment where the

00:28:28,950 --> 00:28:36,000
bang with the coming bandwidths to the

00:28:32,000 --> 00:28:38,250
Nick and the host penguins are quite

00:28:36,000 --> 00:28:39,809
different right from 50 GB per second to

00:28:38,250 --> 00:28:45,539
12 and a half gigabit per second in some

00:28:39,809 --> 00:28:48,389
cases okay my future work play around

00:28:45,539 --> 00:28:50,789
with different marking algorithms you

00:28:48,389 --> 00:28:52,529
know we need to use the ITT for marking

00:28:50,789 --> 00:28:54,029
for marking algorithms to get more

00:28:52,529 --> 00:28:56,519
fairness I need to do those experiments

00:28:54,029 --> 00:28:59,760
I also want to do explain with testing

00:28:56,519 --> 00:29:03,539
multiple scopes concurrent flog with

00:28:59,760 --> 00:29:07,409
different entities play with flog with

00:29:03,539 --> 00:29:19,830
different variants and do more work with

00:29:07,409 --> 00:29:20,610
the ingress nrm and that's it there's

00:29:19,830 --> 00:29:27,960
some pretty cool stuff

00:29:20,610 --> 00:29:28,950
any questions for yeah it's very

00:29:27,960 --> 00:29:30,840
interesting you can do this

00:29:28,950 --> 00:29:33,929
experimentation I was wondering about

00:29:30,840 --> 00:29:36,840
the the different RT t so the you load

00:29:33,929 --> 00:29:40,100
the RTO to be conveniently took not much

00:29:36,840 --> 00:29:45,299
higher than the Nedim delay right yeah

00:29:40,100 --> 00:29:47,399
you had a 20 millisecond retransmit time

00:29:45,299 --> 00:29:49,409
oh no no no it was a probe timer or the

00:29:47,399 --> 00:29:51,779
probe Tyler yes the RTOS theta 200

00:29:49,409 --> 00:29:55,230
milliseconds okay and I only do the

00:29:51,779 --> 00:29:58,080
proton I decrease it only when the inner

00:29:55,230 --> 00:30:00,000
rain drops a packet and there

00:29:58,080 --> 00:30:01,440
are no packets in flight because then

00:30:00,000 --> 00:30:07,200
you know I may need to depend on the pro

00:30:01,440 --> 00:30:10,380
timer to resend the packet okay so how

00:30:07,200 --> 00:30:13,799
closely is that related to the chosen

00:30:10,380 --> 00:30:15,809
sort of emulated RTT on your network and

00:30:13,799 --> 00:30:17,250
how does that affect when you thin the

00:30:15,809 --> 00:30:19,380
network it was within Iraq there was

00:30:17,250 --> 00:30:21,659
about 20 microseconds but you thought

00:30:19,380 --> 00:30:23,700
you had an atom of 10 oh no that was one

00:30:21,659 --> 00:30:26,070
experiment just like all of the others

00:30:23,700 --> 00:30:28,740
were within Iraq so the RTT was 20

00:30:26,070 --> 00:30:32,130
microseconds so if you look at the RTT

00:30:28,740 --> 00:30:36,960
between using in RM and using TC within

00:30:32,130 --> 00:30:40,139
RM the the others are TT whether I was

00:30:36,960 --> 00:30:42,660
just in one flow or nine flows was about

00:30:40,139 --> 00:30:45,980
a hundred microseconds more or less

00:30:42,660 --> 00:30:48,720
with when I used TC with fur cueing

00:30:45,980 --> 00:30:52,200
below it where HTTP I want you to be

00:30:48,720 --> 00:30:54,060
with with for cueing the RTP was around

00:30:52,200 --> 00:30:56,790
six milliseconds because of the cueing

00:30:54,060 --> 00:30:58,440
so okay since you bring that up yeah

00:30:56,790 --> 00:31:03,690
how do you get a standing cue if you

00:30:58,440 --> 00:31:06,470
have tsq enabled an F Q so if I was

00:31:03,690 --> 00:31:10,200
using the HTV queuing discipline and

00:31:06,470 --> 00:31:11,910
enforcing you know the one gigabit per

00:31:10,200 --> 00:31:14,250
second break and then I have free cream

00:31:11,910 --> 00:31:17,910
below it so by the photo he will do is

00:31:14,250 --> 00:31:20,430
he will queue until so it will not send

00:31:17,910 --> 00:31:21,900
packets faster than the limit you impose

00:31:20,430 --> 00:31:24,120
so the packet that it doesn't send you

00:31:21,900 --> 00:31:26,130
will kill and while your queue of a

00:31:24,120 --> 00:31:27,510
thousand packets I understand it with

00:31:26,130 --> 00:31:30,330
somebody's multiple flows because the

00:31:27,510 --> 00:31:31,860
single flow as TCP small queues yes will

00:31:30,330 --> 00:31:45,690
not queue that much traffic to begin

00:31:31,860 --> 00:31:48,200
with right I tell the living for STB was

00:31:45,690 --> 00:31:48,200
four thousand

00:31:59,690 --> 00:32:06,210
okay I get you the default colonel and

00:32:03,749 --> 00:32:08,820
those are the numbers I got so it were

00:32:06,210 --> 00:32:11,820
you using parallel flows because because

00:32:08,820 --> 00:32:16,940
it's Jesus so I I went I use either one

00:32:11,820 --> 00:32:16,940
flow to five and nine concurrent flows

00:32:17,480 --> 00:32:25,830
and I tried like stb with her cueing and

00:32:22,710 --> 00:32:28,080
has to be with like I don't think I

00:32:25,830 --> 00:32:29,490
 Corral and I think I tell you

00:32:28,080 --> 00:32:33,450
assess a big with with without

00:32:29,490 --> 00:32:35,070
specifying the the the bottom clean

00:32:33,450 --> 00:32:38,249
discipline those are the numbers I got

00:32:35,070 --> 00:32:41,610
so I assumed it was from the Kuna fresh

00:32:38,249 --> 00:32:43,710
TV I need the main point is that maybe

00:32:41,610 --> 00:32:45,480
should investigate what happens in the

00:32:43,710 --> 00:32:47,100
single flow case because at the socket

00:32:45,480 --> 00:32:55,110
level you should be limited by tsq

00:32:47,100 --> 00:32:58,350
theoretically okay unless you change a

00:32:55,110 --> 00:33:00,809
TCP spawncube default with ten flows you

00:32:58,350 --> 00:33:04,769
cannot have more than 40 packets in HTTP

00:33:00,809 --> 00:33:08,899
you cannot so so there is something here

00:33:04,769 --> 00:33:12,269
so remember that let's say 40 packets so

00:33:08,899 --> 00:33:15,600
the rate is one gigabit per second yeah

00:33:12,269 --> 00:33:19,159
it doesn't so for one packet is quite 10

00:33:15,600 --> 00:33:21,960
microseconds or no longer smoke you

00:33:19,159 --> 00:33:24,659
control exactly no number of packets in

00:33:21,960 --> 00:33:26,519
the first sometimes understand 40

00:33:24,659 --> 00:33:28,970
packets what is the latency introduced

00:33:26,519 --> 00:33:37,049
by 40 packets at one gigabit per second

00:33:28,970 --> 00:33:40,169
I don't know it's 10 microseconds per

00:33:37,049 --> 00:33:43,379
packet at one gigabit per second so far

00:33:40,169 --> 00:33:43,950
it would be about 400 yards so I'll look

00:33:43,379 --> 00:33:47,039
into it

00:33:43,950 --> 00:33:49,200
as I said I didn't change anything I was

00:33:47,039 --> 00:33:51,450
running on our host that improves

00:33:49,200 --> 00:33:56,210
something so so maybe that was changing

00:33:51,450 --> 00:33:56,210
some of the parameters I'll look into it

00:33:58,070 --> 00:34:02,429
hey Larry yeah I was wondering whether

00:34:00,389 --> 00:34:03,929
you did anything with multiple different

00:34:02,429 --> 00:34:06,119
types of congestion control running at

00:34:03,929 --> 00:34:08,339
the same time at all playground no no I

00:34:06,119 --> 00:34:10,280
was limited in time a lot of these

00:34:08,339 --> 00:34:12,679
started to come together

00:34:10,280 --> 00:34:14,810
close to this time so we're just trying

00:34:12,679 --> 00:34:17,000
to run experiment like I mentioned I

00:34:14,810 --> 00:34:26,179
want to run explain with different RTP

00:34:17,000 --> 00:34:29,419
different condition controls sorry about

00:34:26,179 --> 00:34:31,129
the slides Lawrence oh my fault I waited

00:34:29,419 --> 00:34:32,060
too long please please submit a little

00:34:31,129 --> 00:34:35,179
bit earlier and you won't have this

00:34:32,060 --> 00:34:37,520
problem maybe I did it on pressure or my

00:34:35,179 --> 00:34:38,929
fault or that's fine

00:34:37,520 --> 00:34:45,500
thank you very much long as I was

00:34:38,929 --> 00:34:47,419
awesome who is here for every single

00:34:45,500 --> 00:34:52,760
networking track session both yesterday

00:34:47,419 --> 00:34:56,480
and today raise your hand you guys need

00:34:52,760 --> 00:34:59,720
to seek professional help of some sort

00:34:56,480 --> 00:35:01,310
no that's awesome that's great it's nice

00:34:59,720 --> 00:35:03,920
to see how a lot of familiar faces I

00:35:01,310 --> 00:35:05,330
hope everyone enjoyed themselves it was

00:35:03,920 --> 00:35:13,660
pretty good networking track don't you

00:35:05,330 --> 00:35:18,349
think that's a good response I like that

00:35:13,660 --> 00:35:21,650
I get the sense that it was really

00:35:18,349 --> 00:35:22,790
packed densely and that some people

00:35:21,650 --> 00:35:25,760
would like a little bit of space in

00:35:22,790 --> 00:35:29,030
between it's your fault for submitting

00:35:25,760 --> 00:35:31,339
so much good material so if you want

00:35:29,030 --> 00:35:37,400
more space in between talks submit crap

00:35:31,339 --> 00:35:40,400
next time and less all day so that was

00:35:37,400 --> 00:35:44,570
great I really appreciate everyone

00:35:40,400 --> 00:35:46,010
submitting talks and I hope that

00:35:44,570 --> 00:35:47,930
everyone who even if they didn't get

00:35:46,010 --> 00:35:49,280
accept at this time we can continue to

00:35:47,930 --> 00:35:51,050
submit because it's not because your

00:35:49,280 --> 00:35:53,180
talk was no good it's because there was

00:35:51,050 --> 00:35:54,500
a lot of competition and just please

00:35:53,180 --> 00:35:58,369
realize that when you analyze the

00:35:54,500 --> 00:35:59,960
situation it looks like we had a wide

00:35:58,369 --> 00:36:01,640
breadth of work and there's a lot of

00:35:59,960 --> 00:36:04,130
cool things to hack on when we had our

00:36:01,640 --> 00:36:07,040
head on home after having some beers

00:36:04,130 --> 00:36:08,210
tonight I really look forward to people

00:36:07,040 --> 00:36:11,540
making progress in all the work that

00:36:08,210 --> 00:36:14,210
they're doing and you know it's it's a

00:36:11,540 --> 00:36:15,440
common refrain but really I really want

00:36:14,210 --> 00:36:18,109
beep you have to be more approachable

00:36:15,440 --> 00:36:22,430
for people more usable easier to deploy

00:36:18,109 --> 00:36:25,970
easier to work on I used the model of

00:36:22,430 --> 00:36:28,549
if people can if fashion designers can

00:36:25,970 --> 00:36:30,079
write Arduino programs then your average

00:36:28,549 --> 00:36:32,029
system admins should be able to write an

00:36:30,079 --> 00:36:33,950
x DP program and install it on your

00:36:32,029 --> 00:36:37,460
infrastructure and that's where we need

00:36:33,950 --> 00:36:39,200
to go what do you like it or not yeah

00:36:37,460 --> 00:36:43,279
so I really want to see more work in

00:36:39,200 --> 00:36:45,319
that direction a reminder I revealed the

00:36:43,279 --> 00:36:47,630
secret that some of my filters when

00:36:45,319 --> 00:36:49,609
reviewing patches get disabled may or

00:36:47,630 --> 00:36:52,430
may not get disabled if you submit a

00:36:49,609 --> 00:36:54,589
test case or a piece of documentation so

00:36:52,430 --> 00:36:58,210
please take advantage of that this may

00:36:54,589 --> 00:37:00,619
be a limited time offer can I make any

00:36:58,210 --> 00:37:02,150
promises about the future of filters

00:37:00,619 --> 00:37:05,930
that might get installed in my ex DP

00:37:02,150 --> 00:37:07,730
program in my brain so that's that's

00:37:05,930 --> 00:37:10,250
about that we need more documentation

00:37:07,730 --> 00:37:13,160
particularly for your driver interfaces

00:37:10,250 --> 00:37:14,390
and how to write nice drivers that's

00:37:13,160 --> 00:37:16,579
that

00:37:14,390 --> 00:37:18,529
moving forward we'll do a little bit of

00:37:16,579 --> 00:37:20,240
a post analysis of how things went me

00:37:18,529 --> 00:37:22,010
and Daniel will talk about it and talk

00:37:20,240 --> 00:37:25,450
to the LPC committee how everything went

00:37:22,010 --> 00:37:27,740
things pretty positive so not in

00:37:25,450 --> 00:37:28,670
anticipating anything major but we'll

00:37:27,740 --> 00:37:30,289
figure out what we're gonna do moving

00:37:28,670 --> 00:37:33,260
forward well it will do help you see

00:37:30,289 --> 00:37:35,240
again we'll get someone else to host us

00:37:33,260 --> 00:37:38,289
do things on our own whatever we want to

00:37:35,240 --> 00:37:40,490
do so that's the plan moving forward

00:37:38,289 --> 00:37:43,309
once again I want to thank the LPC

00:37:40,490 --> 00:37:45,470
committee I want to thank the Technical

00:37:43,309 --> 00:37:47,180
Committee and I want to thank all of you

00:37:45,470 --> 00:37:50,599
for attending and making this the event

00:37:47,180 --> 00:37:52,830
that it is thank you very much and have

00:37:50,599 --> 00:37:59,210
a pleasant evening

00:37:52,830 --> 00:37:59,210

YouTube URL: https://www.youtube.com/watch?v=mQev63NwOmo


