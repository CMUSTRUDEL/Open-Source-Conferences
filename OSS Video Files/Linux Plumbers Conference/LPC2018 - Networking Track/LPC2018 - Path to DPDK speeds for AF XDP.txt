Title: LPC2018 - Path to DPDK speeds for AF XDP
Publication date: 2018-12-04
Playlist: LPC2018 - Networking Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/99/
speaker:  Björn Töpel (Intel),  Magnus Karlsson (Intel)


AF_XDP is a new socket type for raw frames to be introduced in 4.18
(in linux-next at the time of writing). The current code base offers
throughput numbers north of 20 Mpps per application core for 64-byte
packets on our system, however there are a lot of optimizations that
could be performed in order to increase this even further. The focus
of this paper is the performance optimizations we need to make in
AF_XDP to get it to perform as fast as DPDK.

We present optimization that fall into two broad categories: ones that
are seamless to the application and ones that requires additions to
the uapi. In the first category we examine the following:

Loosen the requirement for having an XDP program. If the user does
not need an XDP program and there is only one AF_XDP socket bound to
a particular queue, we do not need an XDP program. This should cut
out quite a number of cycles from the RX path.

Wire up busy poll from user space. If the application writer is
using epoll() and friends, this has the potential benefit of
removing the coherency communication between the RX (NAPI) core and
the application core as everything is now done on a single
core. Should improve performance for a number of use cases. Maybe it
is worth revisiting the old idea of threaded NAPI in this context
too.

Optimize for high instruction cache usage through batching as has
been explored in for example Cisco's VPP stack and Edward Cree in
his net-next RFC "Handle multiple received packets at each stage".

In the uapi extensions category we examine the following
optimizations:

Support a new mode for NICs with in-order TX completions. In this
mode, the completion queue would not be used. Instead the
application would simply look at the pointer in the TX queue to see
if a packet has been completed. In this mode, we do not need any
backpreassure between the completion queue and the TX queue and we
do not need to populate or publish anything in the completion queue
as it is not used. Should improve the performance of TX for in-order
NICs significantly.

Introduce the "type-writer" model where each chunk can contain
multiple packets. This is the model that e.g., Chelsio has in its
NICs. But experiments show that this mode also can provide better
performance for regular NICs as there are fewer transactions on the
queues. Requires a new flag to be introduced in the options field of
the descriptor.

With these optimization, we believe we can reach our goal of close to
40 Mpps of throughput for 64-byte packets in zero-copy mode. Full
analysis with performance numbers will be presented in the final
paper.
Captions: 
	00:00:06,920 --> 00:00:11,690
all right so I'll be talking about about

00:00:10,160 --> 00:00:15,230
some performance work that we've been

00:00:11,690 --> 00:00:27,050
doing with AF XP and it's me and Magnus

00:00:15,230 --> 00:00:30,439
sitting there so I just took this slide

00:00:27,050 --> 00:00:34,700
from the from the excellent XD paper by

00:00:30,439 --> 00:00:36,020
to kill the previous presentation has

00:00:34,700 --> 00:00:38,090
been great about the introductions I'll

00:00:36,020 --> 00:00:40,969
skip I'll just assume that people know

00:00:38,090 --> 00:00:43,820
what XP is and what it works or how it

00:00:40,969 --> 00:00:47,989
works can I get a pointer you see that

00:00:43,820 --> 00:00:52,190
yeah so this is where we're at so in a

00:00:47,989 --> 00:00:55,309
affects to peel and so I'll just do a

00:00:52,190 --> 00:00:58,309
quick recap what the f XP is and why

00:00:55,309 --> 00:01:00,559
it's useful so first of all why it's

00:00:58,309 --> 00:01:03,859
it's a mechanism to toss packets quickly

00:01:00,559 --> 00:01:05,150
out to use around and also selling back

00:01:03,859 --> 00:01:06,500
is quick with when useless so first

00:01:05,150 --> 00:01:09,440
question would be why why do we need

00:01:06,500 --> 00:01:12,140
this like why not use just the Linux

00:01:09,440 --> 00:01:16,009
kernel stack and the reason why we

00:01:12,140 --> 00:01:17,930
started off is that typically like for

00:01:16,009 --> 00:01:20,540
radio access networks they usually need

00:01:17,930 --> 00:01:23,270
they have their own weird proprietary

00:01:20,540 --> 00:01:25,670
protocol that they there's no reason to

00:01:23,270 --> 00:01:27,110
do that work in the stack so they just

00:01:25,670 --> 00:01:33,110
want to have the packet and do that in

00:01:27,110 --> 00:01:35,720
you Savannah pretty much so that's sort

00:01:33,110 --> 00:01:39,130
of the rationale for it so if it XP was

00:01:35,720 --> 00:01:45,500
introduced in 4 to 18 so it's available

00:01:39,130 --> 00:01:49,189
right now and so how does work so let's

00:01:45,500 --> 00:01:52,329
start from a x2p perspective so from

00:01:49,189 --> 00:01:56,420
next P perspective AF XP is just another

00:01:52,329 --> 00:01:59,570
packet sync so for example there's no

00:01:56,420 --> 00:02:01,880
difference from AF XP to say CPU map so

00:01:59,570 --> 00:02:04,670
for CPU map you use the expiry direcor

00:02:01,880 --> 00:02:06,799
redirect action to take a frame that you

00:02:04,670 --> 00:02:11,480
received in the XP program and you hand

00:02:06,799 --> 00:02:13,580
it via the CPU map and you'll get you

00:02:11,480 --> 00:02:14,860
get the frame executors in another CPU

00:02:13,580 --> 00:02:16,989
on

00:02:14,860 --> 00:02:20,260
[Music]

00:02:16,989 --> 00:02:22,870
in there in the living stack and same

00:02:20,260 --> 00:02:24,489
thing with for example dev map you

00:02:22,870 --> 00:02:26,739
receive a packet and then you hand it

00:02:24,489 --> 00:02:28,720
via the dev map and then he'll go out

00:02:26,739 --> 00:02:33,700
via that device that you specified in

00:02:28,720 --> 00:02:37,360
the dev map and for AF XP you simply say

00:02:33,700 --> 00:02:39,459
you use the X Akkad map which is a no at

00:02:37,360 --> 00:02:41,440
the moment when we choose the name it

00:02:39,459 --> 00:02:44,110
felt like your thing it's really hard to

00:02:41,440 --> 00:02:47,799
renounce so I know X map where X I could

00:02:44,110 --> 00:02:50,230
map but you use that map to redirect to

00:02:47,799 --> 00:02:52,840
the socket so that is that you place the

00:02:50,230 --> 00:02:55,690
AF XP socket into this map and then you

00:02:52,840 --> 00:02:56,500
redirect it out to this map on the

00:02:55,690 --> 00:02:58,750
egress side

00:02:56,500 --> 00:03:02,709
nothing special there's no exp problem

00:02:58,750 --> 00:03:06,190
there so you just yeah it just gets sent

00:03:02,709 --> 00:03:10,359
out one different from regular eye net

00:03:06,190 --> 00:03:12,459
sockets is that we use a couple rings to

00:03:10,359 --> 00:03:17,640
pass information back and forth from the

00:03:12,459 --> 00:03:22,480
kernel instead of using system calls so

00:03:17,640 --> 00:03:24,609
right and some sort of key parts of this

00:03:22,480 --> 00:03:28,419
there's something called the human which

00:03:24,609 --> 00:03:31,180
is something that use application

00:03:28,419 --> 00:03:35,680
allocates so it's a memory range which

00:03:31,180 --> 00:03:39,250
is divided into chunks and the chunks

00:03:35,680 --> 00:03:42,280
are so like fixed size memory pieces so

00:03:39,250 --> 00:03:44,079
human consists of multiple chunks and we

00:03:42,280 --> 00:03:45,609
use different kind of rings to pass

00:03:44,079 --> 00:03:48,699
information back and forth when kernel

00:03:45,609 --> 00:03:50,769
so that is that these rings they do not

00:03:48,699 --> 00:03:53,379
pass data they actually just pass

00:03:50,769 --> 00:03:58,060
descriptors to these chunks which is

00:03:53,379 --> 00:04:01,870
part of the human right so let's have a

00:03:58,060 --> 00:04:06,010
quick look at the Rings again so this is

00:04:01,870 --> 00:04:08,440
the view from a use application the idea

00:04:06,010 --> 00:04:10,840
again the application allocates a range

00:04:08,440 --> 00:04:14,410
of memory packet buffers the packet

00:04:10,840 --> 00:04:18,150
buffers of chunks one packet is one

00:04:14,410 --> 00:04:22,419
chunk of this packet buffer memory range

00:04:18,150 --> 00:04:25,840
so let's just take a quick review of the

00:04:22,419 --> 00:04:30,460
receive flow so an application want to

00:04:25,840 --> 00:04:33,310
receive packets seem to say chunk ABC

00:04:30,460 --> 00:04:36,520
so the application starts off by placing

00:04:33,310 --> 00:04:41,290
these ABC descriptors into the field

00:04:36,520 --> 00:04:43,930
ring the kernel so by doing so you sort

00:04:41,290 --> 00:04:45,760
of past the ownership of these rings -

00:04:43,930 --> 00:04:48,690
from the youths application to the

00:04:45,760 --> 00:04:48,690
kernel okay

00:04:48,750 --> 00:04:57,220
the kernel will then fill these chunks

00:04:53,410 --> 00:04:58,690
pointed out by the descriptors in in

00:04:57,220 --> 00:05:02,470
various ways for example one way would

00:04:58,690 --> 00:05:07,540
be copying data into this ring another

00:05:02,470 --> 00:05:09,210
way would be placing placing this memory

00:05:07,540 --> 00:05:15,250
area into hardware so that the hardware

00:05:09,210 --> 00:05:17,790
feel the chunk so that's the first way

00:05:15,250 --> 00:05:20,920
is a cup mode other ways serrocold mode

00:05:17,790 --> 00:05:23,800
when the chunk is filled the kernel

00:05:20,920 --> 00:05:26,890
notifies the application again via the

00:05:23,800 --> 00:05:29,590
aurochs ring so again it takes for

00:05:26,890 --> 00:05:32,680
example descriptor ABC places that into

00:05:29,590 --> 00:05:37,080
the ORAC spring and the application can

00:05:32,680 --> 00:05:45,580
then read off the ring that's that there

00:05:37,080 --> 00:05:49,840
as for the egress side it's it's similar

00:05:45,580 --> 00:05:51,790
no different is that the user

00:05:49,840 --> 00:05:53,950
application fills in the data upfront so

00:05:51,790 --> 00:05:56,290
it takes a chunk fill in the data from

00:05:53,950 --> 00:06:01,450
the packet buffer place the descriptor

00:05:56,290 --> 00:06:03,070
on the orgs ring and then you actually

00:06:01,450 --> 00:06:05,050
need to do a system call her to notify

00:06:03,070 --> 00:06:07,930
the kernel that hey Colonel there's data

00:06:05,050 --> 00:06:10,240
in this transmission ring so please pick

00:06:07,930 --> 00:06:12,670
the day off so the colonel picks off the

00:06:10,240 --> 00:06:15,910
data or descriptors from the aurochs

00:06:12,670 --> 00:06:18,100
ring sends up the packet and then

00:06:15,910 --> 00:06:21,510
whenever things done the application

00:06:18,100 --> 00:06:21,510
gets notified via the completion ring

00:06:23,770 --> 00:06:30,620
all right so there is that what what

00:06:28,220 --> 00:06:32,900
kind of optimizations can we do to get

00:06:30,620 --> 00:06:37,100
to the same level or can get to the same

00:06:32,900 --> 00:06:44,540
level as deep decay for example so we

00:06:37,100 --> 00:06:46,310
started off with a baseline for when the

00:06:44,540 --> 00:06:49,910
circle patches were introduced so right

00:06:46,310 --> 00:06:53,270
now it's to to network devices that

00:06:49,910 --> 00:06:55,490
supports several cockpits the ixd GB

00:06:53,270 --> 00:06:59,630
which is a 10 gig NIC from Intel and

00:06:55,490 --> 00:07:01,490
then there's the i-40 which is a 48 NIC

00:06:59,630 --> 00:07:04,550
so with the we did all the measurements

00:07:01,490 --> 00:07:06,260
on Linux 420 which is in horsey - now

00:07:04,550 --> 00:07:09,440
how thank you

00:07:06,260 --> 00:07:12,889
and these kind of benchmarks they're

00:07:09,440 --> 00:07:15,169
only limited by packet rates so we're

00:07:12,889 --> 00:07:17,440
only looking at small packet 64 bytes

00:07:15,169 --> 00:07:20,030
and we're only measuring the rates and

00:07:17,440 --> 00:07:23,690
the baseline you'll see the grass later

00:07:20,030 --> 00:07:29,780
so the baseline is between 15 million

00:07:23,690 --> 00:07:31,310
packets per second and 22 so what can we

00:07:29,780 --> 00:07:34,220
do to improve here so that's this is

00:07:31,310 --> 00:07:37,280
sort of the strategies like do less I do

00:07:34,220 --> 00:07:39,800
not execute fewer instructions other

00:07:37,280 --> 00:07:41,479
thing is like talk less like try to

00:07:39,800 --> 00:07:46,760
minimize the coherence of traffic for

00:07:41,479 --> 00:07:51,860
example and that is like - talking

00:07:46,760 --> 00:07:53,780
between two cores for example again do

00:07:51,860 --> 00:07:56,240
more at the same time that it's like be

00:07:53,780 --> 00:07:58,310
more gentle to the caches like be better

00:07:56,240 --> 00:08:02,560
at the data cache em in the instruction

00:07:58,310 --> 00:08:06,350
cache a final thing that sort of is a

00:08:02,560 --> 00:08:07,910
was Ken tricky to financial is that all

00:08:06,350 --> 00:08:11,660
these measurements were done with

00:08:07,910 --> 00:08:13,880
meltdown mitigation and red line so what

00:08:11,660 --> 00:08:15,470
what meltdown does from a performance

00:08:13,880 --> 00:08:18,800
perspective is that the system calls

00:08:15,470 --> 00:08:21,770
will be more expensive for melt for the

00:08:18,800 --> 00:08:25,400
spectra mitigation if you're using red

00:08:21,770 --> 00:08:27,860
lines every indirect call will be really

00:08:25,400 --> 00:08:30,880
expensive or you won't speculate do

00:08:27,860 --> 00:08:33,190
respective execution for

00:08:30,880 --> 00:08:36,040
in direct costs so that means it'll be

00:08:33,190 --> 00:08:38,950
really expensive to do that and the

00:08:36,040 --> 00:08:41,950
third thing is that we're actually the

00:08:38,950 --> 00:08:45,580
fifth thing can we sort of extend the

00:08:41,950 --> 00:08:53,820
user API and get performance benefits

00:08:45,580 --> 00:08:56,870
from that so this is the set up a

00:08:53,820 --> 00:08:58,320
somewhat old intel server

00:08:56,870 --> 00:09:01,390
[Music]

00:08:58,320 --> 00:09:03,220
unless note that we always use two cores

00:09:01,390 --> 00:09:07,150
for the banished bronx that mean that

00:09:03,220 --> 00:09:10,180
there's one to go like a soft RQ thread

00:09:07,150 --> 00:09:13,860
and running in the kernel mode and then

00:09:10,180 --> 00:09:19,060
there's one application running that

00:09:13,860 --> 00:09:20,680
induces base I'll get into the busy pole

00:09:19,060 --> 00:09:23,950
but in the base post analysis on one

00:09:20,680 --> 00:09:25,720
core but I'll get back to that to be

00:09:23,950 --> 00:09:29,260
sure that we're not kept by hardware we

00:09:25,720 --> 00:09:32,050
used to net records and to soccer so we

00:09:29,260 --> 00:09:35,950
need to make sure that so we're not kept

00:09:32,050 --> 00:09:40,380
by by the network or Rosie or are we

00:09:35,950 --> 00:09:43,360
kept by the CPU and again we're using

00:09:40,380 --> 00:09:50,320
proprietary load generator to generate

00:09:43,360 --> 00:09:53,470
traffic all right so let's start off

00:09:50,320 --> 00:09:57,460
with so for the receive side we pretty

00:09:53,470 --> 00:09:59,970
much did five different things the first

00:09:57,460 --> 00:10:02,380
thing that we started off with which is

00:09:59,970 --> 00:10:05,470
actually user API exchanges and the

00:10:02,380 --> 00:10:07,210
reason with that is a lot of men much of

00:10:05,470 --> 00:10:11,790
the feedback that we got from the API is

00:10:07,210 --> 00:10:15,430
that it's kind of clunky to use and many

00:10:11,790 --> 00:10:17,890
many users of AF XP just want to create

00:10:15,430 --> 00:10:21,280
the socket attach it to queue and then

00:10:17,890 --> 00:10:24,370
and then be ready but so all these

00:10:21,280 --> 00:10:26,200
things with like using maps attached to

00:10:24,370 --> 00:10:29,890
XP Pro grams that's sort of a hurdle for

00:10:26,200 --> 00:10:32,440
people start using it so what we've done

00:10:29,890 --> 00:10:35,680
first tier is that and this also reduces

00:10:32,440 --> 00:10:38,950
the amount of code compared to the X

00:10:35,680 --> 00:10:41,800
akkad based one so there's a new circuit

00:10:38,950 --> 00:10:46,269
option called XP attached and a new BBF

00:10:41,800 --> 00:10:49,539
called called BPF exact redirect

00:10:46,269 --> 00:10:53,379
so that is that you take one XP sockets

00:10:49,539 --> 00:10:56,919
you attach it directly to one Q and then

00:10:53,379 --> 00:11:00,099
if you use this BBF helper the BPF

00:10:56,919 --> 00:11:02,409
helper will return redirect if there's a

00:11:00,099 --> 00:11:04,029
socket bound to that Q if there's not a

00:11:02,409 --> 00:11:08,229
socket bound it'll just pass it to the

00:11:04,029 --> 00:11:09,999
regular stack so semantically it's a bit

00:11:08,229 --> 00:11:12,549
different from the redirect map but it's

00:11:09,999 --> 00:11:14,949
sort of you can think of it it's it's

00:11:12,549 --> 00:11:19,509
this note that most AF XP users want to

00:11:14,949 --> 00:11:21,819
use so the good thing about this that

00:11:19,509 --> 00:11:26,979
this Co sort of reduces the amount of

00:11:21,819 --> 00:11:31,959
EPF code so we can yeah decrease the

00:11:26,979 --> 00:11:33,759
amount of instruction is executed there

00:11:31,959 --> 00:11:37,779
are there and this sort of ties in to

00:11:33,759 --> 00:11:40,119
the second bullet is that we take this

00:11:37,779 --> 00:11:41,859
minimal program that most users want and

00:11:40,119 --> 00:11:44,649
we make that in software they call it

00:11:41,859 --> 00:11:48,549
built in XP program so that error is

00:11:44,649 --> 00:11:52,989
that if there's so think of it as a two

00:11:48,549 --> 00:11:55,179
level hierarchy so external regular XP

00:11:52,989 --> 00:11:58,239
programs will have priority over the

00:11:55,179 --> 00:12:01,809
built in one so for example if a a fxb

00:11:58,239 --> 00:12:05,049
use that one - you know uses attaches

00:12:01,809 --> 00:12:06,099
and touches the socket to a queue it'll

00:12:05,049 --> 00:12:09,339
start

00:12:06,099 --> 00:12:10,959
although the built in XP program given

00:12:09,339 --> 00:12:15,399
that there's not a external program

00:12:10,959 --> 00:12:17,019
running and similarly if there's if the

00:12:15,399 --> 00:12:19,749
built-in program is running and you load

00:12:17,019 --> 00:12:22,269
an external one the external one will

00:12:19,749 --> 00:12:24,549
have priority over built in one so the

00:12:22,269 --> 00:12:26,979
nice thing about this is then we have x3

00:12:24,549 --> 00:12:28,809
program that we know while behaviors

00:12:26,979 --> 00:12:31,059
that means that we can actually can

00:12:28,809 --> 00:12:33,159
remove one indirect call in the BBF Pro

00:12:31,059 --> 00:12:36,899
program which is you'll see the graphs

00:12:33,159 --> 00:12:40,719
later which gives a good performance hit

00:12:36,899 --> 00:12:43,809
so that's sort of the first big thing

00:12:40,719 --> 00:12:48,249
that would add the third item is that

00:12:43,809 --> 00:12:50,349
remove yet another indirect call so at

00:12:48,249 --> 00:12:52,439
least this this one was new to me so if

00:12:50,349 --> 00:12:56,409
you're if you have a switch statement

00:12:52,439 --> 00:12:57,730
and which has more than or five or more

00:12:56,409 --> 00:13:00,130
items

00:12:57,730 --> 00:13:05,190
it'll be generated as a jump table

00:13:00,130 --> 00:13:08,320
obviously but the jump table is or

00:13:05,190 --> 00:13:10,180
there's indirection to the jump table so

00:13:08,320 --> 00:13:14,760
they'll actually give an indirect call

00:13:10,180 --> 00:13:17,560
so we just you can either like pass a

00:13:14,760 --> 00:13:20,950
option to GCC to say like hey do not

00:13:17,560 --> 00:13:24,850
generate in jump tables but we actually

00:13:20,950 --> 00:13:27,610
did the dirty ugly way just created like

00:13:24,850 --> 00:13:32,110
a real nasty if-else statement but I'd

00:13:27,610 --> 00:13:38,200
give a good performance report for tank

00:13:32,110 --> 00:13:40,029
driver optimizations like more at least

00:13:38,200 --> 00:13:42,220
the into drivers or they're very focused

00:13:40,029 --> 00:13:45,130
for obvious reasons for its base KB path

00:13:42,220 --> 00:13:48,160
so there were a lot of things that were

00:13:45,130 --> 00:13:51,760
being done that only made sense for the

00:13:48,160 --> 00:13:57,190
sqb so we sort of separated the path has

00:13:51,760 --> 00:14:01,440
purpose so the AF XP or XP path it's

00:13:57,190 --> 00:14:04,510
much more lean what else

00:14:01,440 --> 00:14:07,420
the final optimization is also about

00:14:04,510 --> 00:14:11,130
reducing the amount instruction so right

00:14:07,420 --> 00:14:15,010
now it typical call like for each frame

00:14:11,130 --> 00:14:16,870
when you when you receive when the when

00:14:15,010 --> 00:14:18,600
the device receive a frame people these

00:14:16,870 --> 00:14:23,199
calls are being done so you start off by

00:14:18,600 --> 00:14:26,050
executed BPF program would run xtp the

00:14:23,199 --> 00:14:29,019
program returns XP redirects then you

00:14:26,050 --> 00:14:30,640
call in to the redirect call and then

00:14:29,019 --> 00:14:32,170
you do the flash of the map so this is

00:14:30,640 --> 00:14:37,480
like the typical things are being call

00:14:32,170 --> 00:14:39,790
every time problem is or problem this is

00:14:37,480 --> 00:14:46,300
implemented with a per CPU struct called

00:14:39,790 --> 00:14:48,970
BPA redirect info and so you need to do

00:14:46,300 --> 00:14:54,040
the per CPU lookup in every call so if

00:14:48,970 --> 00:14:55,630
we instead pass and explicit context

00:14:54,040 --> 00:15:00,250
into all this function we can actually

00:14:55,630 --> 00:15:03,519
shave off some instructions and another

00:15:00,250 --> 00:15:06,730
sort of side notice is that we have the

00:15:03,519 --> 00:15:09,970
driver has some knowledge that these

00:15:06,730 --> 00:15:11,520
functions doesn't have or don't have

00:15:09,970 --> 00:15:14,970
with these combination

00:15:11,520 --> 00:15:16,560
so you can do some optimization but what

00:15:14,970 --> 00:15:19,620
do you pay for this much uglier code

00:15:16,560 --> 00:15:21,990
unfortunately so here are the results

00:15:19,620 --> 00:15:24,180
for the receive side and this Sonora

00:15:21,990 --> 00:15:25,590
Oryx drop is just it's a toy snore you

00:15:24,180 --> 00:15:27,720
you take that you take the frame when

00:15:25,590 --> 00:15:32,310
you toss it away so we start off with a

00:15:27,720 --> 00:15:36,030
baseline we create the XP attach mode

00:15:32,310 --> 00:15:43,050
where we instead of using the X rocket

00:15:36,030 --> 00:15:45,240
map we yeah we use the detached mode so

00:15:43,050 --> 00:15:46,980
we get two million packets there we

00:15:45,240 --> 00:15:49,590
remove the indirect call because we're

00:15:46,980 --> 00:15:52,740
using a built-in program we'll go up to

00:15:49,590 --> 00:15:56,820
23 there the silver from silver to

00:15:52,740 --> 00:16:00,210
yellow that was by removing the switch

00:15:56,820 --> 00:16:02,010
statement and then again some drivers

00:16:00,210 --> 00:16:03,870
optimization which is you know that's a

00:16:02,010 --> 00:16:05,870
good thing for just you know rewriting a

00:16:03,870 --> 00:16:11,190
switch statement

00:16:05,870 --> 00:16:14,790
and finally removing or have an explicit

00:16:11,190 --> 00:16:17,580
state in all the DPF calls from kernel

00:16:14,790 --> 00:16:24,810
from a driver's perspective I need to

00:16:17,580 --> 00:16:27,240
speed up so again this is to course and

00:16:24,810 --> 00:16:28,680
it's sort of it like it toy application

00:16:27,240 --> 00:16:31,910
we just take to take the package and

00:16:28,680 --> 00:16:34,560
tossed away in terms of the eager aside

00:16:31,910 --> 00:16:38,400
we also started off here bit with a

00:16:34,560 --> 00:16:40,860
complaint from or some not a complaint

00:16:38,400 --> 00:16:43,740
but I feedback from users is that many

00:16:40,860 --> 00:16:47,190
users just want to create an XP socket

00:16:43,740 --> 00:16:49,140
where they just tied to one TX frame do

00:16:47,190 --> 00:16:51,750
you want to create multiple sockets tie

00:16:49,140 --> 00:16:53,310
them to different hardware cues but you

00:16:51,750 --> 00:16:56,660
want to use the same you ma'am

00:16:53,310 --> 00:17:03,380
so for example think key we're like

00:16:56,660 --> 00:17:06,540
different QoS classes for example and

00:17:03,380 --> 00:17:07,680
actually when we did this we realized

00:17:06,540 --> 00:17:10,860
that we were actually kept by the

00:17:07,680 --> 00:17:14,370
hardware so it was when we're doing this

00:17:10,860 --> 00:17:18,870
we actually moved to two Nick's again

00:17:14,370 --> 00:17:21,199
much larger batching so as you see the

00:17:18,870 --> 00:17:23,650
eager site is really sensitive to

00:17:21,199 --> 00:17:27,500
batching sizes

00:17:23,650 --> 00:17:29,930
what else right now we share the X T P Q

00:17:27,500 --> 00:17:34,370
with a FX three packs that means that

00:17:29,930 --> 00:17:37,310
they clean up when we actually before we

00:17:34,370 --> 00:17:41,320
complete back the sound frames to

00:17:37,310 --> 00:17:43,670
reducer that the logic that walks

00:17:41,320 --> 00:17:46,280
through the hardware ring has to be more

00:17:43,670 --> 00:17:51,310
complicated bit because we're sharing

00:17:46,280 --> 00:17:54,380
the ring between X X T P and a fxp I

00:17:51,310 --> 00:17:55,970
find a fun thing that we did is adding a

00:17:54,380 --> 00:17:58,250
new socket option for in order

00:17:55,970 --> 00:18:00,620
completion so that is that let's remove

00:17:58,250 --> 00:18:04,580
the completion ring and instead just

00:18:00,620 --> 00:18:09,230
bump the tail the tail pointer when the

00:18:04,580 --> 00:18:12,080
packet has no sound so again sort of a

00:18:09,230 --> 00:18:15,860
toy example for the receive side when we

00:18:12,080 --> 00:18:17,590
started off with 25 or toy example

00:18:15,860 --> 00:18:20,510
because we don't touch the data we just

00:18:17,590 --> 00:18:22,520
create packet a prompt and then just try

00:18:20,510 --> 00:18:29,450
to stand it up so try to fill the pipe

00:18:22,520 --> 00:18:32,870
read much start off with 25 add support

00:18:29,450 --> 00:18:34,670
for multiple T excuse not really a big

00:18:32,870 --> 00:18:36,800
benefit but when we increase the batch

00:18:34,670 --> 00:18:41,000
size in the drive room get this really

00:18:36,800 --> 00:18:43,700
big bump from light blue to silver what

00:18:41,000 --> 00:18:45,830
else not do not share the hardware ring

00:18:43,700 --> 00:18:49,910
with it with X to P will go from 54 to

00:18:45,830 --> 00:18:52,310
58 and then all the way up to 68 when we

00:18:49,910 --> 00:18:52,970
remove the completion ring so this is a

00:18:52,310 --> 00:18:58,130
really good

00:18:52,970 --> 00:19:02,000
I mean performance token for the gray

00:18:58,130 --> 00:19:05,200
side another thing that would want to

00:19:02,000 --> 00:19:07,550
text experiment with was like how can we

00:19:05,200 --> 00:19:09,500
make the coherence the traffic between

00:19:07,550 --> 00:19:13,460
course because as I said earlier we had

00:19:09,500 --> 00:19:16,790
to we had to course running for the

00:19:13,460 --> 00:19:18,500
previous benchmarks so today is that if

00:19:16,790 --> 00:19:20,630
we're using the base appalling behavior

00:19:18,500 --> 00:19:24,640
which is which can be used for regular

00:19:20,630 --> 00:19:28,910
iron sockets and the idea there is that

00:19:24,640 --> 00:19:31,010
you call Paul or the system call Paul

00:19:28,910 --> 00:19:34,610
then you enter the kernel and the kernel

00:19:31,010 --> 00:19:35,400
calls the not people from the Paul

00:19:34,610 --> 00:19:38,309
context

00:19:35,400 --> 00:19:42,510
so today is that you entered an optical

00:19:38,309 --> 00:19:44,820
you feel the you pick packets from the

00:19:42,510 --> 00:19:46,320
hardwood ring you create the descriptors

00:19:44,820 --> 00:19:49,740
to the aurochs ring and then you

00:19:46,320 --> 00:19:52,350
complete and then you turn from the paul

00:19:49,740 --> 00:19:56,760
code so this is doing everything from

00:19:52,350 --> 00:20:00,900
one core so the first two benchmarks and

00:19:56,760 --> 00:20:04,650
then we added one which is l2 forward

00:20:00,900 --> 00:20:06,600
which is a there we actually touch the

00:20:04,650 --> 00:20:09,720
data we just the same we touch touch one

00:20:06,600 --> 00:20:14,179
cache line of the packet just what MAC

00:20:09,720 --> 00:20:18,150
address as you can see like the total

00:20:14,179 --> 00:20:21,870
the total is worse but Orianna it's 39

00:20:18,150 --> 00:20:23,429
for to course but it's 34 1 4 so the the

00:20:21,870 --> 00:20:27,270
per core performance is much better

00:20:23,429 --> 00:20:29,929
actually and the same thing for the

00:20:27,270 --> 00:20:29,929
transmission side

00:20:36,070 --> 00:20:42,700
right so deep decay so deep decay is

00:20:40,359 --> 00:20:46,919
it's sort of it's a packet processing

00:20:42,700 --> 00:20:49,929
library and it's it's really performant

00:20:46,919 --> 00:20:55,749
but they bypass pretty much everything

00:20:49,929 --> 00:20:59,409
bypass the the drivers yeah everything

00:20:55,749 --> 00:21:01,119
is done in user space so that is or the

00:20:59,409 --> 00:21:03,129
downside is which many of our customers

00:21:01,119 --> 00:21:06,070
said is like they have to re-implement

00:21:03,129 --> 00:21:08,019
everything that that Linux does in user

00:21:06,070 --> 00:21:10,059
space so you get great networking

00:21:08,019 --> 00:21:12,090
performance but you have to redo

00:21:10,059 --> 00:21:12,090
everything

00:21:13,559 --> 00:21:17,349
what sort of novel about TB care were

00:21:16,179 --> 00:21:19,269
one the whole thing is that they're

00:21:17,349 --> 00:21:22,269
using vectorize drivers which tickles

00:21:19,269 --> 00:21:26,649
the colonel doesn't use there's great

00:21:22,269 --> 00:21:28,889
paper for from Steven that very

00:21:26,649 --> 00:21:30,909
highlights the details have deep decayed

00:21:28,889 --> 00:21:35,739
differs from Linux

00:21:30,909 --> 00:21:40,840
I recommend that so this is sort of the

00:21:35,739 --> 00:21:42,759
hard facts we start off we have the orcs

00:21:40,840 --> 00:21:45,519
drop ntx push and then they'll tip

00:21:42,759 --> 00:21:47,499
forward so the first two benchmarks we

00:21:45,519 --> 00:21:51,099
don't touch the data with touch data and

00:21:47,499 --> 00:21:52,779
last one so if we're going to compare

00:21:51,099 --> 00:21:55,179
apples to apples we should compare that

00:21:52,779 --> 00:21:56,529
deep decay scaler drivers and scaler

00:21:55,179 --> 00:22:00,190
meaning we're not using any kind of

00:21:56,529 --> 00:22:04,119
vectorization so on on the ingress side

00:22:00,190 --> 00:22:08,590
we have 52 for deep decay for one core

00:22:04,119 --> 00:22:14,080
whereas we have 34 one core I'm 39 for

00:22:08,590 --> 00:22:16,090
the run to completion scenario on the

00:22:14,080 --> 00:22:20,169
transmission side is much better so the

00:22:16,090 --> 00:22:21,549
gap is smaller so 51 versus 62 but I

00:22:20,169 --> 00:22:23,289
think the most interesting one is that

00:22:21,549 --> 00:22:26,559
when you actually touch the data which

00:22:23,289 --> 00:22:31,450
is a real application you sort of you

00:22:26,559 --> 00:22:35,889
get to the same the same numbers still I

00:22:31,450 --> 00:22:37,479
mean we get the same numbers but for the

00:22:35,889 --> 00:22:39,220
deep decay you still have some cycles

00:22:37,479 --> 00:22:41,889
left for other things but still are

00:22:39,220 --> 00:22:44,259
things kind of interesting one thing

00:22:41,889 --> 00:22:46,289
that we could sort of think about is if

00:22:44,259 --> 00:22:49,330
it would make sense to implement

00:22:46,289 --> 00:22:51,700
vectorize drivers in linux I don't know

00:22:49,330 --> 00:22:54,580
if you look at the deep decay drivers

00:22:51,700 --> 00:22:56,140
that's a vectorized they they barely

00:22:54,580 --> 00:22:59,620
maintainable you can't read the code and

00:22:56,140 --> 00:23:02,440
it's really messy but the performance is

00:22:59,620 --> 00:23:04,330
good so I don't know and again for a

00:23:02,440 --> 00:23:10,500
real application I'm not sure that it

00:23:04,330 --> 00:23:17,640
would make sense actually right so

00:23:10,500 --> 00:23:20,530
what's next try to upstream everything

00:23:17,640 --> 00:23:22,210
so the first two brief three things

00:23:20,530 --> 00:23:25,600
given that the attach and the built-in

00:23:22,210 --> 00:23:30,370
BPF program is a good idea I'm really

00:23:25,600 --> 00:23:31,840
open for suggestions there it really

00:23:30,370 --> 00:23:36,660
makes sense for an AF x to be

00:23:31,840 --> 00:23:40,690
perspective so again can be discussed

00:23:36,660 --> 00:23:44,860
the second thing is that we really need

00:23:40,690 --> 00:23:47,350
to put a FXP support into the PPF

00:23:44,860 --> 00:23:51,430
because right now FX piece it's too hard

00:23:47,350 --> 00:23:52,960
to consume it so like people they start

00:23:51,430 --> 00:23:56,070
looking at it but it's a bit too

00:23:52,960 --> 00:23:58,450
complicated so I think adding like

00:23:56,070 --> 00:24:01,480
simple helpers into maybe PF would

00:23:58,450 --> 00:24:07,500
really help for people to get going with

00:24:01,480 --> 00:24:09,400
it and then also the supporting multiple

00:24:07,500 --> 00:24:12,250
transmission sockets for multiple

00:24:09,400 --> 00:24:14,700
hardware queues and again self testing

00:24:12,250 --> 00:24:14,700
samples

00:24:20,010 --> 00:24:34,650
I'm almost out of time but so I think

00:24:31,169 --> 00:24:38,340
I'll summarize some of these so as you

00:24:34,650 --> 00:24:40,410
see so for example for egress we're

00:24:38,340 --> 00:24:43,890
pretty good I mean performance-wise I

00:24:40,410 --> 00:24:46,320
think we're close enough for that ten

00:24:43,890 --> 00:24:49,590
percent from decay which is I think

00:24:46,320 --> 00:24:53,160
that's acceptable as for this receive

00:24:49,590 --> 00:24:55,110
side we still need to do more so one

00:24:53,160 --> 00:24:57,630
thing that we started looking to see if

00:24:55,110 --> 00:25:01,470
we can have like it explicit you to

00:24:57,630 --> 00:25:04,470
pages support meaning that in the field

00:25:01,470 --> 00:25:08,280
ring instead of passing every descriptor

00:25:04,470 --> 00:25:10,500
you pass for example like this page is

00:25:08,280 --> 00:25:12,809
now you know you transfer ownership for

00:25:10,500 --> 00:25:15,150
a whole page to to the colonel and then

00:25:12,809 --> 00:25:18,049
maybe we can get down the Carreras of

00:25:15,150 --> 00:25:18,049
traffic between the course

00:25:22,200 --> 00:25:27,190
skip the second turn also there's a lot

00:25:25,450 --> 00:25:31,450
of code that are really similar to what

00:25:27,190 --> 00:25:33,039
the InfiniBand guys are doing so I think

00:25:31,450 --> 00:25:35,219
it will make sense to see if we can

00:25:33,039 --> 00:25:38,859
share for example would the u-men

00:25:35,219 --> 00:25:44,889
principle stuff like that with with

00:25:38,859 --> 00:25:52,119
their already inviting people okay

00:25:44,889 --> 00:25:55,539
so summary roughly two and a half x

00:25:52,119 --> 00:25:58,690
performance for both Oryx and TX busy

00:25:55,539 --> 00:26:00,159
poling seems to be good way and even

00:25:58,690 --> 00:26:03,849
though I mean it's a good idea even

00:26:00,159 --> 00:26:05,559
though with the Melton patches that will

00:26:03,849 --> 00:26:07,629
pay a lot for each system call so

00:26:05,559 --> 00:26:11,619
instead of 50 cycles would pay roughly

00:26:07,629 --> 00:26:15,820
250 cycles deep decay is still faster

00:26:11,619 --> 00:26:22,599
but FX pay is sort of entire for a real

00:26:15,820 --> 00:26:24,309
benchmark and I say drivers but I'm I'm

00:26:22,599 --> 00:26:26,589
at the end also Intel drives we need to

00:26:24,309 --> 00:26:28,989
think about how should we sort of think

00:26:26,589 --> 00:26:33,729
about going forward when xtp is this new

00:26:28,989 --> 00:26:36,429
fast path within the in the kernel so

00:26:33,729 --> 00:26:40,450
not only like we need to deal with more

00:26:36,429 --> 00:26:41,739
things than just the skb and again

00:26:40,450 --> 00:26:43,450
there's a lot of people that have been

00:26:41,739 --> 00:26:46,749
really helpful and you know tried it out

00:26:43,450 --> 00:26:51,099
and give a suggestion so thanks a lot

00:26:46,749 --> 00:26:53,879
guys it's been little fun and finally

00:26:51,099 --> 00:26:53,879
revenue logo

00:26:59,310 --> 00:27:05,440
yeah so a lot of people ask me about the

00:27:02,020 --> 00:27:07,330
vectorization issue and to your point

00:27:05,440 --> 00:27:09,910
like once you start touching the data

00:27:07,330 --> 00:27:11,200
that's the vectorization really doesn't

00:27:09,910 --> 00:27:13,540
mean by you very much

00:27:11,200 --> 00:27:15,850
I mean when you look at the code it's

00:27:13,540 --> 00:27:18,640
Auto Knights for me it's too messy but

00:27:15,850 --> 00:27:19,450
you know you're I mean yes it's a matter

00:27:18,640 --> 00:27:21,850
of taste

00:27:19,450 --> 00:27:25,840
I'm sure it's theoretically very sexy to

00:27:21,850 --> 00:27:28,030
think that you can parse eight receive

00:27:25,840 --> 00:27:29,590
descriptors in parallel and do them in

00:27:28,030 --> 00:27:31,180
two instructions and all that stuff but

00:27:29,590 --> 00:27:33,820
in the end if it doesn't bias any

00:27:31,180 --> 00:27:35,140
performance for real use cases you know

00:27:33,820 --> 00:27:37,810
it's hard to just the PI pursuing

00:27:35,140 --> 00:27:39,100
something like that however maybe at

00:27:37,810 --> 00:27:42,070
some point we can find some way to

00:27:39,100 --> 00:27:44,350
legitimately let the actual xdp programs

00:27:42,070 --> 00:27:47,260
do vectorization to a certain extent and

00:27:44,350 --> 00:27:49,360
to see in what scenarios that makes

00:27:47,260 --> 00:27:53,350
sense and how possible that is with the

00:27:49,360 --> 00:27:55,330
way that the FPU in the kernel mechanism

00:27:53,350 --> 00:27:56,740
works is can we let that happen in all

00:27:55,330 --> 00:27:58,540
the contexts that actually people

00:27:56,740 --> 00:28:00,070
potentially execute in and all those

00:27:58,540 --> 00:28:03,070
difficult and hard questions that would

00:28:00,070 --> 00:28:05,050
need to be answered so that's what I

00:28:03,070 --> 00:28:08,140
think about the whole vectorization

00:28:05,050 --> 00:28:10,210
thing we'll just have to see you have to

00:28:08,140 --> 00:28:15,700
noticed a lot of xdp and BPF development

00:28:10,210 --> 00:28:18,070
is driven by use not oh it seems as if

00:28:15,700 --> 00:28:19,420
this would help or it seems as if this

00:28:18,070 --> 00:28:21,310
is to be a direction which go we're at

00:28:19,420 --> 00:28:23,710
the point where people won't need to use

00:28:21,310 --> 00:28:27,550
xdp people need to use BPM so we know

00:28:23,710 --> 00:28:30,600
what people need in the future

00:28:27,550 --> 00:28:30,600
any questions

00:28:32,570 --> 00:28:42,470
so for the zero copy I just

00:28:39,010 --> 00:28:46,700
you take userspace memory read you give

00:28:42,470 --> 00:28:51,020
that to a card yep that basically means

00:28:46,700 --> 00:28:56,660
I can never free this memory yeah at all

00:28:51,020 --> 00:28:59,300
so yeah I mean yeah it's locked but but

00:28:56,660 --> 00:29:02,570
the user can can mess up yeah like paws

00:28:59,300 --> 00:29:04,400
if you know just the library API you

00:29:02,570 --> 00:29:07,820
want to do like all right create this

00:29:04,400 --> 00:29:14,000
context start using now I want to you

00:29:07,820 --> 00:29:15,950
know stop free context I really there

00:29:14,000 --> 00:29:19,310
could be ways you know to swap in some

00:29:15,950 --> 00:29:30,290
other pages instead without that it's it

00:29:19,310 --> 00:29:33,170
library it's kind of problematic imagine

00:29:30,290 --> 00:29:36,650
like I'm thinking like you want we'd

00:29:33,170 --> 00:29:39,470
want to maybe use this in qmu yeah and

00:29:36,650 --> 00:29:42,350
hey we would you could stack it up into

00:29:39,470 --> 00:29:47,390
a guest right into a virtual machine why

00:29:42,350 --> 00:29:49,850
not yeah except we have this device at

00:29:47,390 --> 00:29:54,580
some point the guest might come and say

00:29:49,850 --> 00:29:58,820
I reset this virtual device or remove it

00:29:54,580 --> 00:30:04,880
we'd want to maybe give this VM other

00:29:58,820 --> 00:30:07,910
memory instead okay somehow watchin that

00:30:04,880 --> 00:30:10,690
I call and I get back my memory or some

00:30:07,910 --> 00:30:10,690
other memory instead

00:30:13,130 --> 00:30:28,640
if the guess has memory reassigned right

00:30:19,820 --> 00:30:31,760
yeah yeah I didn't say anything story

00:30:28,640 --> 00:30:35,270
like RDMA if you look at it you can

00:30:31,760 --> 00:30:37,700
actually come and say well destroy it

00:30:35,270 --> 00:30:39,799
it's just not very good in that any

00:30:37,700 --> 00:30:44,299
accesses will fail you don't want you

00:30:39,799 --> 00:30:46,190
need to fail this is like you need to

00:30:44,299 --> 00:30:48,190
pin the memory test memory if you are

00:30:46,190 --> 00:30:52,820
using this model just like a sorry movie

00:30:48,190 --> 00:30:55,330
we have to pin so what then just deals

00:30:52,820 --> 00:30:55,330
SIV

00:30:55,600 --> 00:31:02,090
but like I said so you want like a more

00:30:58,880 --> 00:31:08,980
flexible model for you mum yeah and

00:31:02,090 --> 00:31:12,590
that's again the pelicans are okay

00:31:08,980 --> 00:31:14,150
is there any hardware offload ever going

00:31:12,590 --> 00:31:15,919
to come into this is something we could

00:31:14,150 --> 00:31:17,630
use it's like the hashing and the

00:31:15,919 --> 00:31:20,720
timestamp from the neck part I hope

00:31:17,630 --> 00:31:23,270
hopefully at least our like the one that

00:31:20,720 --> 00:31:25,280
our customer that really wants this they

00:31:23,270 --> 00:31:28,280
said like we can't use this unless we

00:31:25,280 --> 00:31:30,770
have some way of like expressing

00:31:28,280 --> 00:31:32,690
metadata so that's sort of that the next

00:31:30,770 --> 00:31:34,789
big thing so without that that said this

00:31:32,690 --> 00:31:38,240
is not really useful I guess that also

00:31:34,789 --> 00:31:39,919
comes down to we have not every adapter

00:31:38,240 --> 00:31:43,429
supports that so we're going to have to

00:31:39,919 --> 00:31:45,110
get to the point that XD peak right very

00:31:43,429 --> 00:31:46,400
well metadata and that type of thing

00:31:45,110 --> 00:31:50,140
which doesn't seem to be well supported

00:31:46,400 --> 00:31:52,309
in there any thought of let's say if has

00:31:50,140 --> 00:31:54,110
P a packet or something like that

00:31:52,309 --> 00:31:55,909
there's well it's not really reference

00:31:54,110 --> 00:31:57,559
County but I could have more than one

00:31:55,909 --> 00:32:00,350
listener in the reserve are going to be

00:31:57,559 --> 00:32:05,679
some kind of support for that like I

00:32:00,350 --> 00:32:10,299
noticed that I can't return like exit EP

00:32:05,679 --> 00:32:13,950
it's always xdp drop it's never

00:32:10,299 --> 00:32:19,419
I mean like a clone functional yeah yeah

00:32:13,950 --> 00:32:25,749
again and I think William said neither

00:32:19,419 --> 00:32:27,429
from being water William that's not the

00:32:25,749 --> 00:32:29,409
first time we've heard about a need for

00:32:27,429 --> 00:32:35,169
a clone so you guys want clone yeah

00:32:29,409 --> 00:32:38,469
right and so people wanting oh yes sir

00:32:35,169 --> 00:32:39,759
yes I pass it to the clone as well we'd

00:32:38,469 --> 00:32:41,979
also like to be able to close lash

00:32:39,759 --> 00:32:52,149
coffee package choose your space that's

00:32:41,979 --> 00:32:53,160
it anyone else okay well thank you very

00:32:52,149 --> 00:32:57,769
much thank you

00:32:53,160 --> 00:32:57,769

YouTube URL: https://www.youtube.com/watch?v=JmGfJok32Kw


