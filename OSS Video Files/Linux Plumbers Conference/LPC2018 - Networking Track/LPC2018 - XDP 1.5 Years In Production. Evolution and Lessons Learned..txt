Title: LPC2018 - XDP 1.5 Years In Production. Evolution and Lessons Learned.
Publication date: 2018-12-04
Playlist: LPC2018 - Networking Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/109/
speaker:  Nikita V. Shirokov (Facebook)


Today every packet which is reaching Facebook’s network is being processed by XDP enabled application. We have been using it for more then 1.5 years and this talk is about evolution of XDP and BPF which has been driven by our production needs. I’m going to talk about history of changes in core BPF components as well as will show why and how it was done. What performance improvements did we get (with synthetics and real world data) and how it was implemented. Also I’m going to talk about issues and shortcoming of BPF/XDP which we have found during our operations, as well as some gotchas and corner cases. In the end we are going to discuss on what is still missing and which part could be improved.

Topics and areas of existing BPF/XDP infrastructure which are going to be covered in this talk:

why helpers such as bpf_adjust_head/bpf_adjust_tail has been added
unittesting and microbenchmarking with bpf_prog_test_run: how to add test coverage of you BPF program and track the regression (we are going to cover how spectre affected BPF kernel infrastructure and what tweaks has been made to get some performance back)
how map-in-map helps us to scale and make sure that we don't waste memory
NUMA aware allocation for BPF maps
inline lookups for BPF arrays/map-in-map
Lessons which we have learned during operation of XDP:

BPF instruction counts vs complexity
How to attach more then one XDP program to the interface
when LLVM and verifier are not the same: some tricks to force LLVM to generate proper BPF
we will briefly discuss HW limitation: NIC's bandwidth vs packet per second performance
Missing parts: what and why could be added:

the need for hardware checksumming offload
bounded loops: what they would allow us to do
Captions: 
	00:00:05,630 --> 00:00:12,389
okay yeah let's start yes so Nikita

00:00:09,690 --> 00:00:14,519
Shirokov will give us a perspective on

00:00:12,389 --> 00:00:16,320
using XDP in production at Facebook and

00:00:14,519 --> 00:00:18,000
let us know how that went and you know

00:00:16,320 --> 00:00:21,390
what kind of experiences they had and

00:00:18,000 --> 00:00:22,640
what we can learn from so off you go

00:00:21,390 --> 00:00:25,230
thank you

00:00:22,640 --> 00:00:27,060
hi everyone my name is Nikita or can

00:00:25,230 --> 00:00:29,880
Facebook traffic team and this talk is

00:00:27,060 --> 00:00:31,770
about XTP our IP rational experience and

00:00:29,880 --> 00:00:33,510
what we have learned by running it in

00:00:31,770 --> 00:00:36,270
our production network for one and a

00:00:33,510 --> 00:00:38,489
half year so even if this talk is going

00:00:36,270 --> 00:00:40,710
to sounds like like a narrative story

00:00:38,489 --> 00:00:42,719
the actual goals here is to show how the

00:00:40,710 --> 00:00:44,879
BPF infrastructure could be used to

00:00:42,719 --> 00:00:47,670
build network application and at the

00:00:44,879 --> 00:00:51,410
same time to share our experience like

00:00:47,670 --> 00:00:54,420
reigning kings DP on a large scale so

00:00:51,410 --> 00:00:56,339
talking about operational experience

00:00:54,420 --> 00:00:59,100
we have plenty of that since the May of

00:00:56,339 --> 00:01:00,899
2017 every packet toward facebook.com

00:00:59,100 --> 00:01:03,749
has been processed by xdp enabled

00:01:00,899 --> 00:01:05,700
application which one it's our layer

00:01:03,749 --> 00:01:08,340
followed bouncer it's open sourced it's

00:01:05,700 --> 00:01:09,930
on the it hub the following plane this

00:01:08,340 --> 00:01:12,119
is not the layer followed bossing talk

00:01:09,930 --> 00:01:13,409
if you wonder like why we're doing that

00:01:12,119 --> 00:01:15,570
and how we're doing that there is a

00:01:13,409 --> 00:01:17,369
separate one but for this talk this is

00:01:15,570 --> 00:01:19,950
basically what you need to know about

00:01:17,369 --> 00:01:21,780
where followed bouncer black box seats

00:01:19,950 --> 00:01:24,090
receive the traffic from the user

00:01:21,780 --> 00:01:27,540
it's a stateful service it have a like

00:01:24,090 --> 00:01:30,030
state table first of all as the key

00:01:27,540 --> 00:01:31,950
destination is value if you have a hit

00:01:30,030 --> 00:01:33,930
on that table you know where to send the

00:01:31,950 --> 00:01:35,729
packet if there is a means and the

00:01:33,930 --> 00:01:37,469
resolution all logic which update this

00:01:35,729 --> 00:01:40,920
table and in turn who sent the packet to

00:01:37,469 --> 00:01:42,570
the backend in our implementation we're

00:01:40,920 --> 00:01:45,420
using encapsulation

00:01:42,570 --> 00:01:48,420
so my keys when it's being sent to the

00:01:45,420 --> 00:01:50,340
destination is APN IP encapsulated so

00:01:48,420 --> 00:01:52,140
why why we have decided to build

00:01:50,340 --> 00:01:55,380
something on top of the SDP a well

00:01:52,140 --> 00:01:58,020
everything started in October of 2016

00:01:55,380 --> 00:01:59,579
like if you didn't follow the news about

00:01:58,020 --> 00:02:01,649
the networking this is basically when

00:01:59,579 --> 00:02:04,079
one of the biggest botnet has been

00:02:01,649 --> 00:02:06,899
revealed the Mirai botnet like there was

00:02:04,079 --> 00:02:10,530
one of the biggest attack around that

00:02:06,899 --> 00:02:14,250
I'm told one of the news website the

00:02:10,530 --> 00:02:16,500
size of attack was 600 megabits per

00:02:14,250 --> 00:02:18,030
second but if you're running like state

00:02:16,500 --> 00:02:18,750
was seriously like automatically

00:02:18,030 --> 00:02:21,300
multiplies

00:02:18,750 --> 00:02:23,730
number by 1.5 and this is basically how

00:02:21,300 --> 00:02:25,710
you how many packets per second he will

00:02:23,730 --> 00:02:30,180
get on your state for service if that

00:02:25,710 --> 00:02:32,490
was the TCP syn flood so around the time

00:02:30,180 --> 00:02:35,510
for the load bouncing we were running

00:02:32,490 --> 00:02:37,980
high PVS and like with our day-to-day

00:02:35,510 --> 00:02:40,560
traffic patterns we were like more or

00:02:37,980 --> 00:02:42,930
less happy with IPV ask but in the end

00:02:40,560 --> 00:02:45,030
we have decided hey let's profile IP vs

00:02:42,930 --> 00:02:48,150
under license in flood so we built out a

00:02:45,030 --> 00:02:50,610
strap like one server like multiple

00:02:48,150 --> 00:02:54,000
cores multiple gifts of memory multiple

00:02:50,610 --> 00:02:56,430
gigs of network connection we're running

00:02:54,000 --> 00:02:58,950
a PBS their second server is we were

00:02:56,430 --> 00:03:00,630
using further as a traffic generator we

00:02:58,950 --> 00:03:02,310
were using like slightly modified

00:03:00,630 --> 00:03:04,920
version of the package junk from the

00:03:02,310 --> 00:03:08,040
Linux kernel the only difference that it

00:03:04,920 --> 00:03:12,840
like we teach it to generate TCP syn

00:03:08,040 --> 00:03:15,630
packets so syn flood basically means

00:03:12,840 --> 00:03:18,920
each packet is half UNIX or Linux or

00:03:15,630 --> 00:03:22,020
sport clone 0 in package an

00:03:18,920 --> 00:03:25,110
implementation an idea is that every new

00:03:22,020 --> 00:03:27,780
packet force IP vs to create a new

00:03:25,110 --> 00:03:30,360
session so we start to profile and this

00:03:27,780 --> 00:03:32,430
is what I have seen course which are

00:03:30,360 --> 00:03:34,310
responsible for the req handling was

00:03:32,430 --> 00:03:38,310
completely packed

00:03:34,310 --> 00:03:40,739
why and turns out like to bring down

00:03:38,310 --> 00:03:43,110
this like huge expensive server you just

00:03:40,739 --> 00:03:46,380
need like small service one gigabit

00:03:43,110 --> 00:03:50,280
connection so PVS under flap is like not

00:03:46,380 --> 00:03:52,650
fun to operate anyway you know why well

00:03:50,280 --> 00:03:54,390
IPS is stateful service and turns out I

00:03:52,650 --> 00:03:56,970
create the new connection is super

00:03:54,390 --> 00:03:58,650
expensive this is the part where people

00:03:56,970 --> 00:04:00,959
I usually decide to like rewrite

00:03:58,650 --> 00:04:04,110
something we scan on bypass I just like

00:04:00,959 --> 00:04:06,030
net map or DB DK but we have Alexi

00:04:04,110 --> 00:04:07,709
working with us and we have decided to

00:04:06,030 --> 00:04:11,070
go another way and basically build

00:04:07,709 --> 00:04:13,170
something on top of TCP so when you

00:04:11,070 --> 00:04:15,269
start to like write something with a new

00:04:13,170 --> 00:04:16,590
technology first thing first is like he

00:04:15,269 --> 00:04:19,109
checked the documentation

00:04:16,590 --> 00:04:21,720
fortunately like definition about text

00:04:19,109 --> 00:04:24,030
appear on that time basically there was

00:04:21,720 --> 00:04:26,849
like no documentation at all luckily

00:04:24,030 --> 00:04:30,360
alexei was working with us but

00:04:26,849 --> 00:04:31,919
unfortunately it doesn't scale not every

00:04:30,360 --> 00:04:32,430
company could have like alexei working

00:04:31,919 --> 00:04:36,270
with them

00:04:32,430 --> 00:04:37,860
Daniel so if you think about like single

00:04:36,270 --> 00:04:39,240
slide from the whole presentation which

00:04:37,860 --> 00:04:41,759
you need to remember this one

00:04:39,240 --> 00:04:43,740
documentation measure luckily it today

00:04:41,759 --> 00:04:45,990
we hope this awesome documentation from

00:04:43,740 --> 00:04:48,210
the Syrian folks and if you write

00:04:45,990 --> 00:04:51,870
something with BPF this is like first

00:04:48,210 --> 00:04:53,789
places to start so anyway we started to

00:04:51,870 --> 00:04:55,860
look into BPF in substructure to figure

00:04:53,789 --> 00:04:58,080
out like what is missing what we need to

00:04:55,860 --> 00:05:00,830
add there to build our load bouncer

00:04:58,080 --> 00:05:03,300
so first thing first encapsulation right

00:05:00,830 --> 00:05:06,090
we receive the packet we do the

00:05:03,300 --> 00:05:08,190
encapsulation the way XDP was working

00:05:06,090 --> 00:05:10,919
around that time is that it allocates

00:05:08,190 --> 00:05:12,990
per page per packet but it writes back

00:05:10,919 --> 00:05:14,460
it in the beginning of the page and as

00:05:12,990 --> 00:05:17,250
you can see there is like not a lot of

00:05:14,460 --> 00:05:19,760
headroom folding encapsulation so

00:05:17,250 --> 00:05:23,460
marking came up was idea hey let's

00:05:19,760 --> 00:05:25,650
change something let's start or I get

00:05:23,460 --> 00:05:27,810
unspecified offset and actually and then

00:05:25,650 --> 00:05:30,690
also let's add the help which would

00:05:27,810 --> 00:05:33,720
allow us to move the pointer to the

00:05:30,690 --> 00:05:36,720
start of the packet so we stay this is

00:05:33,720 --> 00:05:38,520
how it starts to looks like you start to

00:05:36,720 --> 00:05:41,250
write on the HTTP packet Headroom offset

00:05:38,520 --> 00:05:43,590
by default 256 for most of the drivers

00:05:41,250 --> 00:05:47,520
as far as the remember of Windows like

00:05:43,590 --> 00:05:50,099
192 but anyway with VP of helper I just

00:05:47,520 --> 00:05:51,630
had helper you can move the pointer you

00:05:50,099 --> 00:05:52,620
have the Headroom for the encapsulation

00:05:51,630 --> 00:05:55,020
you're good to go

00:05:52,620 --> 00:05:58,199
the next problem was that how to

00:05:55,020 --> 00:06:02,370
implement this stateful table you can do

00:05:58,199 --> 00:06:03,539
like hash table but with hash table

00:06:02,370 --> 00:06:05,190
you're kind of forced to have some

00:06:03,539 --> 00:06:07,530
separate thread which you are going to

00:06:05,190 --> 00:06:11,060
iterate over the hash table to figure

00:06:07,530 --> 00:06:13,680
out like if it state is expired or not

00:06:11,060 --> 00:06:15,690
the other solution which Joe mentioned

00:06:13,680 --> 00:06:18,659
yesterday is to use how are you and

00:06:15,690 --> 00:06:21,840
basically overuse that type of map which

00:06:18,659 --> 00:06:24,720
would automatically evict the entry if

00:06:21,840 --> 00:06:27,570
it's not being used unfortunately there

00:06:24,720 --> 00:06:28,409
was no were you around that time so mark

00:06:27,570 --> 00:06:31,639
sorry

00:06:28,409 --> 00:06:34,409
this is how can you use DVF request head

00:06:31,639 --> 00:06:36,449
anyway unfortunately there was like no

00:06:34,409 --> 00:06:41,099
other year around that time so Martin

00:06:36,449 --> 00:06:43,800
was forced to implement one and as usual

00:06:41,099 --> 00:06:45,870
there is like multiple time same map

00:06:43,800 --> 00:06:49,650
could have multiple flavors there

00:06:45,870 --> 00:06:52,050
one single key single value works great

00:06:49,650 --> 00:06:54,330
as long as you have like single writer

00:06:52,050 --> 00:06:59,430
doesn't scale if you have multiple of

00:06:54,330 --> 00:07:02,400
them second flavor is per CPU single key

00:06:59,430 --> 00:07:05,940
multiple value each CPU could write to

00:07:02,400 --> 00:07:08,490
the dedicated area but unfortunately for

00:07:05,940 --> 00:07:10,350
the load bouncer you want for every CPU

00:07:08,490 --> 00:07:13,320
to have the same value so there is lot

00:07:10,350 --> 00:07:16,680
not a lot of benefits for us so Martin

00:07:13,320 --> 00:07:19,500
add the special flag for the LRU BPF no

00:07:16,680 --> 00:07:21,840
common are you an idea is that you have

00:07:19,500 --> 00:07:24,330
this huge hole are you every sick CPU

00:07:21,840 --> 00:07:27,420
could look up through the whole map but

00:07:24,330 --> 00:07:31,290
it writes only to the CPU specific area

00:07:27,420 --> 00:07:33,480
so that's give you the ability to single

00:07:31,290 --> 00:07:36,060
key single value and at the same time to

00:07:33,480 --> 00:07:40,950
be friendly if you have like multiple

00:07:36,060 --> 00:07:42,540
writers so um looks like we had almost

00:07:40,950 --> 00:07:45,210
everything who starts to look what else

00:07:42,540 --> 00:07:47,370
is missing and this is actually like tip

00:07:45,210 --> 00:07:50,640
and lesson it's like the biggest benefit

00:07:47,370 --> 00:07:54,000
of that PPF is that it's highly

00:07:50,640 --> 00:07:56,280
programmable like and most performance

00:07:54,000 --> 00:07:57,600
benefits are going to be because you

00:07:56,280 --> 00:08:00,210
know your environment and where your

00:07:57,600 --> 00:08:02,420
program is going to run for example

00:08:00,210 --> 00:08:04,620
David was talking about how you can use

00:08:02,420 --> 00:08:06,810
forwarding table from the Linux kernel

00:08:04,620 --> 00:08:09,990
but in how scenario have like share

00:08:06,810 --> 00:08:12,240
single interface connected to the very

00:08:09,990 --> 00:08:14,070
top of tracks which basically you don't

00:08:12,240 --> 00:08:15,630
need to do IP lookup at all like there

00:08:14,070 --> 00:08:17,730
is going to be single entry like send

00:08:15,630 --> 00:08:19,200
everything to the default router so what

00:08:17,730 --> 00:08:21,000
we were doing is just like rewrite the

00:08:19,200 --> 00:08:26,930
destination MAC to be the MAC address of

00:08:21,000 --> 00:08:32,850
the top of the rack switch so yeah

00:08:26,930 --> 00:08:34,920
anyway kernel site was like more or less

00:08:32,850 --> 00:08:37,700
ready and it took us some time to

00:08:34,920 --> 00:08:41,370
implement the user space helpers and

00:08:37,700 --> 00:08:43,130
like our application and around the

00:08:41,370 --> 00:08:45,870
generous 2017

00:08:43,130 --> 00:08:49,290
we had the initial version of our load

00:08:45,870 --> 00:08:51,960
bouncer so again the same lab

00:08:49,290 --> 00:08:54,750
environment server traffic generator and

00:08:51,960 --> 00:08:56,670
presenter starts to send traffic syn

00:08:54,750 --> 00:08:59,070
flood motion more traffic more traffic

00:08:56,670 --> 00:08:59,520
more traffic we send ten times more

00:08:59,070 --> 00:09:01,860
camper

00:08:59,520 --> 00:09:11,730
the IP vs and as you can see we still

00:09:01,860 --> 00:09:13,950
have a lot of headroom so yeah why just

00:09:11,730 --> 00:09:17,850
ten times why not to send more well

00:09:13,950 --> 00:09:21,090
basically this is another lesson or keep

00:09:17,850 --> 00:09:24,480
like profile everything right before

00:09:21,090 --> 00:09:26,070
whose IP vs CPU was the bottleneck now

00:09:24,480 --> 00:09:28,430
something else became a bottleneck in

00:09:26,070 --> 00:09:31,140
our environment that was a specific

00:09:28,430 --> 00:09:33,840
model of the network interface card and

00:09:31,140 --> 00:09:36,690
basically like if someone tell you that

00:09:33,840 --> 00:09:39,150
hey this is like say 40 G nib it doesn't

00:09:36,690 --> 00:09:42,960
mean that you can reach that bamboo

00:09:39,150 --> 00:09:45,210
squeeze like 64 bytes of packets why um

00:09:42,960 --> 00:09:47,280
basically because like Nick is doing a

00:09:45,210 --> 00:09:50,370
lot of stuff as Wallach arrow says it

00:09:47,280 --> 00:09:52,530
needs to parse the packet um consumes

00:09:50,370 --> 00:09:55,380
like some cycles on the Nick as well

00:09:52,530 --> 00:09:57,450
like before versus with six like usually

00:09:55,380 --> 00:09:59,040
to parse with v4 packet is like more

00:09:57,450 --> 00:10:01,710
expensive from the next point of view

00:09:59,040 --> 00:10:03,570
compared to v6 surprisingly like TCP is

00:10:01,710 --> 00:10:06,810
kind of slower than VDP as well in terms

00:10:03,570 --> 00:10:08,880
of like packet per second the biggest

00:10:06,810 --> 00:10:11,220
problem here from my point of view is

00:10:08,880 --> 00:10:14,250
that nowadays like Nick's are kind of

00:10:11,220 --> 00:10:15,900
black box they have this bottleneck but

00:10:14,250 --> 00:10:17,880
you don't know where it is located it

00:10:15,900 --> 00:10:20,550
would be nice if something like it eh -

00:10:17,880 --> 00:10:22,920
would be able to like tell you hey this

00:10:20,550 --> 00:10:25,500
Nick is running on like 50 percent of

00:10:22,920 --> 00:10:27,360
capacity there is no such thing today

00:10:25,500 --> 00:10:30,630
like the only way you figure out like

00:10:27,360 --> 00:10:32,910
that you reach this limit is like then

00:10:30,630 --> 00:10:34,650
when you suddenly stops to drop starts

00:10:32,910 --> 00:10:41,490
to drop like million rows of packets per

00:10:34,650 --> 00:10:45,000
second but anyway um this is what we get

00:10:41,490 --> 00:10:48,300
as you can see some GPUs have usage

00:10:45,000 --> 00:10:50,880
around 15% some of them in around 45 but

00:10:48,300 --> 00:10:52,710
why this is how the topology of that

00:10:50,880 --> 00:10:56,160
Sara's world looks like basically is to

00:10:52,710 --> 00:10:58,440
socket servers to normal domains and the

00:10:56,160 --> 00:11:00,600
way we can figure the recess on our

00:10:58,440 --> 00:11:02,280
necks is that we have single make

00:11:00,600 --> 00:11:04,980
mapping between received queue and the

00:11:02,280 --> 00:11:07,650
CPU but it configured in a way that half

00:11:04,980 --> 00:11:11,010
of the CPUs are on the Numa not zero

00:11:07,650 --> 00:11:12,990
half of them anymore not one but default

00:11:11,010 --> 00:11:15,390
bps behavior

00:11:12,990 --> 00:11:17,940
is that BBF will allocate all the maps

00:11:15,390 --> 00:11:21,660
on the same normal domain as the user

00:11:17,940 --> 00:11:23,760
space so as you can see half of the CPUs

00:11:21,660 --> 00:11:26,430
kind of forced to traverse the QPR link

00:11:23,760 --> 00:11:28,440
for the map lookups and because of that

00:11:26,430 --> 00:11:32,730
they have like most most all cycles and

00:11:28,440 --> 00:11:34,770
higher CPU usage you can see in ECG on

00:11:32,730 --> 00:11:38,370
like yeah one of the node like it's more

00:11:34,770 --> 00:11:40,680
memory than another and when we starts

00:11:38,370 --> 00:11:42,540
to profile well it's a stateful service

00:11:40,680 --> 00:11:44,760
what does functions related to the

00:11:42,540 --> 00:11:47,180
hilary housekeeping basically to update

00:11:44,760 --> 00:11:51,450
the or you when you create the new entry

00:11:47,180 --> 00:11:53,610
for one same profile but without BPF no

00:11:51,450 --> 00:11:56,460
commonality flag as you can see like now

00:11:53,610 --> 00:12:01,290
you have this low contention and going

00:11:56,460 --> 00:12:03,600
back state related routines so we

00:12:01,290 --> 00:12:07,590
decided hey let's run the same stuff but

00:12:03,600 --> 00:12:09,450
in stable small this is how the same

00:12:07,590 --> 00:12:14,390
program looks like when it's stateless

00:12:09,450 --> 00:12:17,280
and doesn't try to the LRU at all why

00:12:14,390 --> 00:12:19,500
well because aside from that allow you

00:12:17,280 --> 00:12:23,130
which is kinda big the other BPS map

00:12:19,500 --> 00:12:25,170
maps as small and cache friendly so all

00:12:23,130 --> 00:12:27,840
of them could be allocated in like layer

00:12:25,170 --> 00:12:29,340
later three of the cpu and you don't

00:12:27,840 --> 00:12:31,670
need to go to the main memory to get

00:12:29,340 --> 00:12:31,670
from them

00:12:32,120 --> 00:12:37,470
pure when it stateless mode

00:12:35,070 --> 00:12:40,350
fun fact around that time BPF doesn't

00:12:37,470 --> 00:12:43,290
work well with perv so this clean up

00:12:40,350 --> 00:12:47,130
model is actually at the BPF program but

00:12:43,290 --> 00:12:51,560
somehow in purple put pure showed some

00:12:47,130 --> 00:12:54,540
random single from the kernel anyway

00:12:51,560 --> 00:12:56,670
because when we were like running on

00:12:54,540 --> 00:13:00,360
multiple cores we were saturated by the

00:12:56,670 --> 00:13:03,810
hardware to get the baseline we run the

00:13:00,360 --> 00:13:06,480
same test but like seen flat on the old

00:13:03,810 --> 00:13:07,620
bouncer but like only with the single

00:13:06,480 --> 00:13:09,750
core to get the baseline

00:13:07,620 --> 00:13:11,520
doesn't matter what the number was we'll

00:13:09,750 --> 00:13:18,540
use this as just a relative number

00:13:11,520 --> 00:13:21,540
relative data point so some tips which

00:13:18,540 --> 00:13:22,680
we find when we do the initial

00:13:21,540 --> 00:13:25,410
deployment well

00:13:22,680 --> 00:13:26,560
jutice your friend around that time by

00:13:25,410 --> 00:13:29,139
default it was this

00:13:26,560 --> 00:13:31,269
and multiple times but after this era

00:13:29,139 --> 00:13:34,240
restart I saw that like performance of

00:13:31,269 --> 00:13:36,009
the program goes down by four times like

00:13:34,240 --> 00:13:37,600
took me like multiple means to figure

00:13:36,009 --> 00:13:40,329
out what's going on then I have realized

00:13:37,600 --> 00:13:43,449
the edge it was disabled Irene abled the

00:13:40,329 --> 00:13:47,379
jeat performance was still bad so like

00:13:43,449 --> 00:13:51,490
you need really a star tip EPF program

00:13:47,379 --> 00:13:54,879
if you enable the J I think like nowaday

00:13:51,490 --> 00:13:57,100
it's enabled by default and you cannot

00:13:54,879 --> 00:14:00,629
disabled it after speculative

00:13:57,100 --> 00:14:03,660
mitigations anyway another stuff is that

00:14:00,629 --> 00:14:07,569
our monitoring system was using the

00:14:03,660 --> 00:14:09,639
counters from the kernel but because 60p

00:14:07,569 --> 00:14:12,069
was working before Kono's tcp/ip stack

00:14:09,639 --> 00:14:14,139
we had some situations when like load

00:14:12,069 --> 00:14:16,829
balancer was doing like million packets

00:14:14,139 --> 00:14:19,149
per second million gigabits per second

00:14:16,829 --> 00:14:21,279
but from the monitoring point of view it

00:14:19,149 --> 00:14:23,829
was looks like that it doesn't do

00:14:21,279 --> 00:14:25,029
anything at all so yeah don't forget to

00:14:23,829 --> 00:14:27,699
implement the counters in your

00:14:25,029 --> 00:14:29,410
application and don't forget to change a

00:14:27,699 --> 00:14:34,559
mini-dress system to look into that

00:14:29,410 --> 00:14:37,059
counters someone asked yesterday like

00:14:34,559 --> 00:14:40,089
how to do X DB program chaining and

00:14:37,059 --> 00:14:42,100
basically current limitation and is that

00:14:40,089 --> 00:14:44,170
you can attach only single vpf program

00:14:42,100 --> 00:14:47,139
to the interface HTTP program to the

00:14:44,170 --> 00:14:51,160
interface so what we came up with is a

00:14:47,139 --> 00:14:54,519
with like super small and diming VPS

00:14:51,160 --> 00:14:56,800
program which being attached to the high

00:14:54,519 --> 00:14:59,980
80s you know interface and the way it

00:14:56,800 --> 00:15:01,660
works it have BPF program array and the

00:14:59,980 --> 00:15:05,290
only work which this small program is

00:15:01,660 --> 00:15:07,269
doing is its check if there is something

00:15:05,290 --> 00:15:09,939
pretty straight in the position zero of

00:15:07,269 --> 00:15:12,639
that array if it's there once the

00:15:09,939 --> 00:15:14,889
control flow to that BPF program that

00:15:12,639 --> 00:15:17,019
will give program like run some logic

00:15:14,889 --> 00:15:18,370
doing the same stuff in the end check if

00:15:17,019 --> 00:15:21,189
like if someone is registered in

00:15:18,370 --> 00:15:24,309
position one position two three four if

00:15:21,189 --> 00:15:26,290
nothing been registered there it's just

00:15:24,309 --> 00:15:27,929
the default action and in our case it

00:15:26,290 --> 00:15:31,449
was 60p pass

00:15:27,929 --> 00:15:34,600
example as you can see on the left side

00:15:31,449 --> 00:15:38,290
this is how the route PPF program looks

00:15:34,600 --> 00:15:40,150
like super simple on the right side this

00:15:38,290 --> 00:15:43,570
is how every program

00:15:40,150 --> 00:15:45,490
and in that example the program on the

00:15:43,570 --> 00:15:48,010
right was rigged is written position

00:15:45,490 --> 00:15:50,080
zero so it internally knows that it

00:15:48,010 --> 00:15:54,520
needs to iterate in the program array

00:15:50,080 --> 00:15:57,339
from position one and on earth next

00:15:54,520 --> 00:15:58,870
stuff is that well you run something

00:15:57,339 --> 00:16:02,830
with the GDP you need to debug and

00:15:58,870 --> 00:16:05,950
troubleshoot so TCP dump works in chaos

00:16:02,830 --> 00:16:09,430
tcp/ip stack XDP dam XTP works before

00:16:05,950 --> 00:16:11,440
her own tcp/ip stack so we can came up

00:16:09,430 --> 00:16:17,260
with like super unique and original name

00:16:11,440 --> 00:16:19,960
for our like troubleshooting tool we

00:16:17,260 --> 00:16:21,820
call it xdp dump and the way it works

00:16:19,960 --> 00:16:24,100
it's like create PPF program with

00:16:21,820 --> 00:16:26,440
species with the help of BCC on the fly

00:16:24,100 --> 00:16:29,560
it creates some filters if there is a

00:16:26,440 --> 00:16:31,930
match it will send first and bytes of

00:16:29,560 --> 00:16:34,720
the packet through the pair of pipe to

00:16:31,930 --> 00:16:37,600
the user space and user space was read

00:16:34,720 --> 00:16:39,610
from the per pipe as example you run the

00:16:37,600 --> 00:16:41,710
program and it works with the help of

00:16:39,610 --> 00:16:43,690
like this route routed program route

00:16:41,710 --> 00:16:45,610
array so it'll install itself in

00:16:43,690 --> 00:16:48,089
position zero so it runs before any

00:16:45,610 --> 00:16:50,800
other HTTP application on the same host

00:16:48,089 --> 00:16:54,520
so as you can see you can specify filter

00:16:50,800 --> 00:16:57,880
it will print like five top all on the

00:16:54,520 --> 00:17:00,040
CLI but most important stuff here is

00:16:57,880 --> 00:17:02,860
that the same program could write the

00:17:00,040 --> 00:17:05,260
output to the file in pickup format so

00:17:02,860 --> 00:17:10,929
you can use the regular tool for the

00:17:05,260 --> 00:17:13,650
offline processing anyway by the May of

00:17:10,929 --> 00:17:17,260
00:17:13,650 --> 00:17:21,309
we have deployed this load balancer

00:17:17,260 --> 00:17:23,589
everywhere on our H Network and this is

00:17:21,309 --> 00:17:26,260
where we are today and basically this

00:17:23,589 --> 00:17:27,880
time we spent to figure out like what

00:17:26,260 --> 00:17:33,070
needs to be improved and what could be

00:17:27,880 --> 00:17:35,200
improved so evolution right yesterday by

00:17:33,070 --> 00:17:37,360
people show you this insane numbers how

00:17:35,200 --> 00:17:40,150
well each DP works when you like don't

00:17:37,360 --> 00:17:42,640
touch packet at all which is sounds

00:17:40,150 --> 00:17:45,250
great on the paper but it's super

00:17:42,640 --> 00:17:48,340
unrealistic in the real world so in our

00:17:45,250 --> 00:17:51,640
load bonds we have around 60 railed cups

00:17:48,340 --> 00:17:52,990
around three her stable lookups so I

00:17:51,640 --> 00:17:53,710
like she came up with the idea how we

00:17:52,990 --> 00:17:56,140
can do

00:17:53,710 --> 00:18:01,029
in very far basically how to remove one

00:17:56,140 --> 00:18:03,909
of the interaction we did the practicing

00:18:01,029 --> 00:18:08,470
get plus 3% compared to the previous

00:18:03,909 --> 00:18:11,080
version sounds great then we start to

00:18:08,470 --> 00:18:13,390
look into hey how we configure our

00:18:11,080 --> 00:18:15,340
network interface card so we have this

00:18:13,390 --> 00:18:18,220
one-to-one mapping between the receive

00:18:15,340 --> 00:18:20,679
queue and the CPU so basically we know

00:18:18,220 --> 00:18:23,740
every CPU number which is responsible

00:18:20,679 --> 00:18:30,190
for the Roq handling but the problem

00:18:23,740 --> 00:18:30,789
with BPF per map per CPU Maps is that by

00:18:30,190 --> 00:18:34,000
default

00:18:30,789 --> 00:18:36,820
BPF will allocate some area for a recipe

00:18:34,000 --> 00:18:39,940
in the system but if you have number of

00:18:36,820 --> 00:18:45,880
received use less than number of CPUs

00:18:39,940 --> 00:18:47,770
you have this allocated area for CPUs

00:18:45,880 --> 00:18:49,990
which are not responsible for the packet

00:18:47,770 --> 00:18:52,929
forwarding so we started to think like

00:18:49,990 --> 00:18:56,320
what if we could somehow allocate memory

00:18:52,929 --> 00:18:57,490
only for the forwarding course Martin

00:18:56,320 --> 00:19:00,460
came up with the idea of

00:18:57,490 --> 00:19:02,289
BPF map and map so basic basically BPF

00:19:00,460 --> 00:19:04,600
map and map is a special type of the map

00:19:02,289 --> 00:19:07,779
which where you can use like index or

00:19:04,600 --> 00:19:09,250
whatever you want is the key and the

00:19:07,779 --> 00:19:14,409
value is going to be pointer to the

00:19:09,250 --> 00:19:17,110
another BPF map so we know which cpus

00:19:14,409 --> 00:19:18,789
are responsible for the forwarding so

00:19:17,110 --> 00:19:22,539
the way we using that is like we have

00:19:18,789 --> 00:19:25,840
the EPFL probe which allow you to get

00:19:22,539 --> 00:19:30,820
the number cpuid you use the cpu ID as

00:19:25,840 --> 00:19:33,429
the key and you the program will return

00:19:30,820 --> 00:19:39,399
you the pointer to the cpu specifically

00:19:33,429 --> 00:19:41,380
you map before after we get three gigs

00:19:39,399 --> 00:19:44,320
of memory but back from each of the

00:19:41,380 --> 00:19:48,070
server pretty great starts to do the

00:19:44,320 --> 00:19:49,570
same test as before simpler as you can

00:19:48,070 --> 00:19:54,279
see like there is some improvement in

00:19:49,570 --> 00:19:57,850
numbers starts to look into profile

00:19:54,279 --> 00:20:01,029
local Numa knows we'd be like perf

00:19:57,850 --> 00:20:04,239
starts to work with BPF

00:20:01,029 --> 00:20:07,209
anyway as you can see BPF program first

00:20:04,239 --> 00:20:09,609
on the first place rocks being lower IQ

00:20:07,209 --> 00:20:14,289
safe this is basically what Hillary

00:20:09,609 --> 00:20:15,909
update is on the third on remote no mo

00:20:14,289 --> 00:20:18,849
not like more or less the same but

00:20:15,909 --> 00:20:20,739
raspbian lock I you can see it and how

00:20:18,849 --> 00:20:25,209
it would more often because if like have

00:20:20,739 --> 00:20:27,729
master cycles anyway starts to compare

00:20:25,209 --> 00:20:29,229
performance plus than personal person

00:20:27,729 --> 00:20:32,309
compared to the previous person past

00:20:29,229 --> 00:20:36,159
fourteen compared to the original one

00:20:32,309 --> 00:20:40,869
and then we did a lot of changes in XD

00:20:36,159 --> 00:20:42,969
PA endings DP program and it was like

00:20:40,869 --> 00:20:45,099
really painful to test them so basically

00:20:42,969 --> 00:20:47,109
we need to setup again like your several

00:20:45,099 --> 00:20:49,269
physical server physical client can't

00:20:47,109 --> 00:20:51,639
send something to the server some zero

00:20:49,269 --> 00:20:53,589
replies you compare like reply to what

00:20:51,639 --> 00:20:54,389
you want to see if it's the same you

00:20:53,589 --> 00:20:56,999
pass the test

00:20:54,389 --> 00:20:59,649
super manual super hard to automate

00:20:56,999 --> 00:21:02,139
worst case if you like messed up with

00:20:59,649 --> 00:21:03,969
your BPF program like that syrup would

00:21:02,139 --> 00:21:07,239
be completely locked down and you need

00:21:03,969 --> 00:21:09,489
to power reset it so it starts to think

00:21:07,239 --> 00:21:11,979
like hey it would be nice to have some

00:21:09,489 --> 00:21:14,619
kind of unit testing Chris BPF and Malik

00:21:11,979 --> 00:21:16,659
she came up with the idea of special

00:21:14,619 --> 00:21:18,399
parameter for the PPR syscall VP have

00:21:16,659 --> 00:21:21,369
proc test run and the way it works is

00:21:18,399 --> 00:21:23,499
that you already have a PDF program

00:21:21,369 --> 00:21:25,569
loaded somewhere in this is called

00:21:23,499 --> 00:21:29,169
specified the descriptor of the of that

00:21:25,569 --> 00:21:32,219
vpf program were to get the data input

00:21:29,169 --> 00:21:35,709
data from were to write the output data

00:21:32,219 --> 00:21:37,869
you run the program program write the

00:21:35,709 --> 00:21:41,229
output to the location of the memory

00:21:37,869 --> 00:21:43,209
where the data out pointing to show you

00:21:41,229 --> 00:21:45,639
what the return value of the DPF program

00:21:43,209 --> 00:21:48,759
was like XDP past XD PTX needs to be

00:21:45,639 --> 00:21:51,189
something somewhere in your user space

00:21:48,759 --> 00:21:54,279
you have the memory channel with like

00:21:51,189 --> 00:21:56,049
packet with the memory how the packet

00:21:54,279 --> 00:21:58,329
supposed to looks like after the PPF

00:21:56,049 --> 00:22:03,369
program test run if they are the same

00:21:58,329 --> 00:22:06,669
just as past if they are not but like in

00:22:03,369 --> 00:22:10,089
our scenario as the input we used base64

00:22:06,669 --> 00:22:13,640
encoded packets with test description as

00:22:10,089 --> 00:22:16,670
an output HTTP return value

00:22:13,640 --> 00:22:18,860
again base64-encoded packet we did this

00:22:16,670 --> 00:22:21,770
for every possible code pass in our

00:22:18,860 --> 00:22:23,660
BPF program and who have the skinny

00:22:21,770 --> 00:22:26,929
testing framework if everything is green

00:22:23,660 --> 00:22:28,970
good to go if something is great you

00:22:26,929 --> 00:22:34,090
need to rework this part and you can

00:22:28,970 --> 00:22:38,360
like run this in automated automated way

00:22:34,090 --> 00:22:41,030
but going back to our exhibit program

00:22:38,360 --> 00:22:43,820
and exhibit environment this is how it

00:22:41,030 --> 00:22:46,070
was working but in India world this is

00:22:43,820 --> 00:22:48,530
what you really want to have so the

00:22:46,070 --> 00:22:51,770
remote Numa node will have already use

00:22:48,530 --> 00:22:53,270
allocated on the same note as the CPUs

00:22:51,770 --> 00:22:55,790
which are responsible for the reco

00:22:53,270 --> 00:22:57,980
handling so this is where Martin came up

00:22:55,790 --> 00:23:00,250
with idea that hey let's specify the

00:22:57,980 --> 00:23:02,630
Numa hint during the BPF map creation

00:23:00,250 --> 00:23:05,179
basically when you create the BPF map

00:23:02,630 --> 00:23:07,309
you want there is a float where you can

00:23:05,179 --> 00:23:12,350
specify hey I want to create this BPF

00:23:07,309 --> 00:23:15,080
map on that specific nominal before as

00:23:12,350 --> 00:23:20,150
you can see in nominal zero have all the

00:23:15,080 --> 00:23:22,850
memory after the beep you know my hint

00:23:20,150 --> 00:23:26,360
they became more or less balanced start

00:23:22,850 --> 00:23:29,900
around perfect testing as you can see

00:23:26,360 --> 00:23:32,140
like not using remote human node helps a

00:23:29,900 --> 00:23:32,140
lot

00:23:33,549 --> 00:23:40,250
starts to run pair of tests again we get

00:23:37,640 --> 00:23:42,410
plus four percent but this was always

00:23:40,250 --> 00:23:44,240
running on the local Numa not so this

00:23:42,410 --> 00:23:51,770
like great improvement in performance

00:23:44,240 --> 00:23:55,309
like not really visible here anyway so

00:23:51,770 --> 00:23:58,250
yeah XD p+ fragmentation so problem is

00:23:55,309 --> 00:24:01,160
that in XDP context he cannot fragment

00:23:58,250 --> 00:24:02,929
the packet but we could end up in

00:24:01,160 --> 00:24:05,179
situation one receive when we receive

00:24:02,929 --> 00:24:07,040
the packet from the user and after

00:24:05,179 --> 00:24:10,040
encapsulation it was bigger than named

00:24:07,040 --> 00:24:12,620
here so what usually people are doing in

00:24:10,040 --> 00:24:14,540
such scenarios is that for internal

00:24:12,620 --> 00:24:17,360
network they just increase the MTU

00:24:14,540 --> 00:24:19,010
everywhere and if we talking about the

00:24:17,360 --> 00:24:21,020
Facebook's network you can divide it

00:24:19,010 --> 00:24:23,690
into two segments one of them is like

00:24:21,020 --> 00:24:25,520
point of presence this is like clusters

00:24:23,690 --> 00:24:27,230
which contains multiple acts like it

00:24:25,520 --> 00:24:29,690
that all over the world

00:24:27,230 --> 00:24:31,760
and they are not that big so it was

00:24:29,690 --> 00:24:35,000
kinda easy to schedule maintenance there

00:24:31,760 --> 00:24:36,559
and increase the MTU size everywhere but

00:24:35,000 --> 00:24:38,510
at the same time we have our data

00:24:36,559 --> 00:24:41,240
centers with millions of servers and

00:24:38,510 --> 00:24:42,950
this is not as easy in terms of like

00:24:41,240 --> 00:24:47,000
changing MTU compared to the point of

00:24:42,950 --> 00:24:49,760
presence so we need like some temporary

00:24:47,000 --> 00:24:52,970
solution for that so I came up with the

00:24:49,760 --> 00:24:54,860
idea with VP FX DPR just tell helper and

00:24:52,970 --> 00:24:57,500
the way it works is like more or less

00:24:54,860 --> 00:24:59,870
the same as HTTP I just had but it just

00:24:57,500 --> 00:25:03,080
instead of pointer to the beginning of

00:24:59,870 --> 00:25:04,760
the packet it allows you to move the

00:25:03,080 --> 00:25:07,399
pointer to the end of the packet and

00:25:04,760 --> 00:25:08,750
that help you to create the ICMP message

00:25:07,399 --> 00:25:10,340
so basically idea is pretty

00:25:08,750 --> 00:25:14,299
straightforward you receive the packet

00:25:10,340 --> 00:25:16,250
from the user or from the client and you

00:25:14,299 --> 00:25:17,600
realized hey after encapsulation it

00:25:16,250 --> 00:25:20,240
would be bigger than MTU

00:25:17,600 --> 00:25:23,029
so what I really want is I want somehow

00:25:20,240 --> 00:25:26,269
to signal the Thunderer that hey you

00:25:23,029 --> 00:25:28,309
need to send like smaller packets so

00:25:26,269 --> 00:25:30,919
this is where I simply pack it too big

00:25:28,309 --> 00:25:34,700
and I came into the picture and the way

00:25:30,919 --> 00:25:37,639
it's created it using the sum first and

00:25:34,700 --> 00:25:40,159
bytes of the packet which triggered that

00:25:37,639 --> 00:25:42,919
message as the payload so basically with

00:25:40,159 --> 00:25:45,200
ETL just tell you can shrink the

00:25:42,919 --> 00:25:46,880
original packet then we see the GDP I

00:25:45,200 --> 00:25:50,990
just had you just create a shrimpy

00:25:46,880 --> 00:25:54,950
header IP header macro trends and this a

00:25:50,990 --> 00:25:57,159
simple message to the user the way being

00:25:54,950 --> 00:26:00,889
used is like more or less the same as

00:25:57,159 --> 00:26:04,130
xtp I just had you can just specify how

00:26:00,889 --> 00:26:07,149
much you want to shrink the packet so

00:26:04,130 --> 00:26:07,149
everything was great

00:26:08,340 --> 00:26:17,550
until until them starts to build out

00:26:14,080 --> 00:26:19,660
colonel with rep alliance where every

00:26:17,550 --> 00:26:21,760
every indirect call became super

00:26:19,660 --> 00:26:24,370
expensive so we start to run like the

00:26:21,760 --> 00:26:29,070
same tested before a single-core send as

00:26:24,370 --> 00:26:32,260
much as possible TCP syn flood boom

00:26:29,070 --> 00:26:33,670
comparator original version so yeah

00:26:32,260 --> 00:26:35,200
luckily for us

00:26:33,670 --> 00:26:37,510
Daniel came up with idea of like

00:26:35,200 --> 00:26:40,210
inlining not only lookups which already

00:26:37,510 --> 00:26:43,660
was in mind for us but with updates as

00:26:40,210 --> 00:26:47,020
well so after Daniel push we still

00:26:43,660 --> 00:26:51,910
better than original version so thanks

00:26:47,020 --> 00:26:54,280
Daniel this is nice to have features

00:26:51,910 --> 00:26:57,030
from my point of view and I'm mostly

00:26:54,280 --> 00:27:01,330
working on the load bouncing stuff so

00:26:57,030 --> 00:27:03,370
I'd really like to see somehow to be

00:27:01,330 --> 00:27:05,740
able to use TCP check sum Inc are

00:27:03,370 --> 00:27:08,740
floating from the HDP context so today

00:27:05,740 --> 00:27:10,330
we're using IP IP encapsulation but if

00:27:08,740 --> 00:27:14,280
you want to use something like I don't

00:27:10,330 --> 00:27:17,260
know VIX lon or UDP based encapsulation

00:27:14,280 --> 00:27:19,870
basically he cannot do that from the xdp

00:27:17,260 --> 00:27:21,760
because unity requires that checksumming

00:27:19,870 --> 00:27:24,820
across the whole packet not just the

00:27:21,760 --> 00:27:27,490
header the other interesting stuff is

00:27:24,820 --> 00:27:29,740
that like nowadays like almost all new

00:27:27,490 --> 00:27:32,640
protocols I built with security in mind

00:27:29,740 --> 00:27:35,920
so almost every field is encrypted and

00:27:32,640 --> 00:27:37,630
from the load bouncer point of view it

00:27:35,920 --> 00:27:40,960
would be nice to be able at least to

00:27:37,630 --> 00:27:43,270
like decrypt seth's 8 16 bytes for some

00:27:40,960 --> 00:27:45,400
specific header which could be used for

00:27:43,270 --> 00:27:48,910
the routing for example quick connection

00:27:45,400 --> 00:27:51,700
idea today you cannot do that and also

00:27:48,910 --> 00:27:54,220
it's like super hard to do the TV

00:27:51,700 --> 00:27:56,470
parsing from the HDP one you don't have

00:27:54,220 --> 00:27:58,450
support for the loops so for example

00:27:56,470 --> 00:28:00,810
it's hard to parse TCP options from the

00:27:58,450 --> 00:28:00,810
HTTP

00:28:01,150 --> 00:28:06,850
yeah I'm wish I had more time there is

00:28:04,270 --> 00:28:09,850
like numerous improvements in like very

00:28:06,850 --> 00:28:12,340
far in Shillong parts there is like the

00:28:09,850 --> 00:28:14,950
way to use BPF proc test run for the

00:28:12,340 --> 00:28:16,930
mimic microbe each marking initiated

00:28:14,950 --> 00:28:20,860
like BPA function calls so it's like

00:28:16,930 --> 00:28:24,190
much easier now a day to write C program

00:28:20,860 --> 00:28:26,830
which would be translated to the BPF

00:28:24,190 --> 00:28:30,040
because in the past when the clonk

00:28:26,830 --> 00:28:32,260
wasn't able to have like enough

00:28:30,040 --> 00:28:34,510
registers it starts to the register

00:28:32,260 --> 00:28:37,540
spinning but at the same time if I

00:28:34,510 --> 00:28:40,929
didn't like that anyway there is a way

00:28:37,540 --> 00:28:42,850
and this is why I want bonded loops so

00:28:40,929 --> 00:28:44,980
there is like some experimental patches

00:28:42,850 --> 00:28:47,860
we have internally to create the shim

00:28:44,980 --> 00:28:50,140
cookie from the XDP context and it works

00:28:47,860 --> 00:28:55,090
like super fast compared to the original

00:28:50,140 --> 00:28:58,270
kernel but again without bounded loops

00:28:55,090 --> 00:29:01,510
it's hard to pass the tcp options so

00:28:58,270 --> 00:29:04,420
yeah so one thing is interesting to me

00:29:01,510 --> 00:29:06,429
is like you say you need you would like

00:29:04,420 --> 00:29:09,160
to have transmit checksum offloading

00:29:06,429 --> 00:29:10,809
right yes so in the context that you're

00:29:09,160 --> 00:29:13,030
operating in you you have a packet

00:29:10,809 --> 00:29:14,770
already which presumably has its own pre

00:29:13,030 --> 00:29:16,179
computed checksum that is correct or not

00:29:14,770 --> 00:29:17,200
correct but you don't care you're yeah

00:29:16,179 --> 00:29:20,410
I'm balancing right

00:29:17,200 --> 00:29:24,760
if you encapsulate the inner packets

00:29:20,410 --> 00:29:24,970
checksum is zero so do you understand

00:29:24,760 --> 00:29:28,179
that

00:29:24,970 --> 00:29:29,470
no no it's really that if you if you

00:29:28,179 --> 00:29:30,640
check some over a packet that has a

00:29:29,470 --> 00:29:32,770
checksum computed already

00:29:30,640 --> 00:29:34,390
oh yeah result is zero mm-hmm so you

00:29:32,770 --> 00:29:35,920
only need to compute the checksum over

00:29:34,390 --> 00:29:37,720
at the headers which is relatively cheap

00:29:35,920 --> 00:29:39,660
and can be done in software we can make

00:29:37,720 --> 00:29:43,750
a BPF helper for that to kind of like

00:29:39,660 --> 00:29:46,300
make that encapsulation system situation

00:29:43,750 --> 00:29:51,490
possible I think we use a similar trick

00:29:46,300 --> 00:29:54,059
for doing a tunnel encapsulated GSO

00:29:51,490 --> 00:29:54,059
mm-hm

00:29:54,760 --> 00:29:58,120
well ready have the helper which can do

00:29:56,920 --> 00:30:00,970
the BPF checksum

00:29:58,120 --> 00:30:03,880
in xdp context but it's kind of limited

00:30:00,970 --> 00:30:07,960
it can do only 512 bytes at once and it

00:30:03,880 --> 00:30:10,120
must be like the power of like do I

00:30:07,960 --> 00:30:11,500
think like I might be like divided by 4

00:30:10,120 --> 00:30:13,570
it might work cuz you only need to do

00:30:11,500 --> 00:30:14,930
the headers from yeah I haven't thought

00:30:13,570 --> 00:30:22,460
about that so that's a possibility

00:30:14,930 --> 00:30:25,820
without any questions for Nikita what

00:30:22,460 --> 00:30:32,170
happens if the client doesn't take the

00:30:25,820 --> 00:30:34,730
ICMP message but like I mean like

00:30:32,170 --> 00:30:39,110
usually they do but if they are broken

00:30:34,730 --> 00:30:43,730
they're broken and luckily for us the

00:30:39,110 --> 00:30:45,890
way the simple message was for the

00:30:43,730 --> 00:30:48,470
external clients we don't do that we

00:30:45,890 --> 00:30:50,480
only do that from the when inside our

00:30:48,470 --> 00:30:53,330
data centers when the client says is

00:30:50,480 --> 00:30:55,280
also controlled by us so we can control

00:30:53,330 --> 00:31:00,020
the client and usually we also have like

00:30:55,280 --> 00:31:03,530
TCP welp MTU enabled basically this is

00:31:00,020 --> 00:31:05,180
like empty you on the TCP layer so it

00:31:03,530 --> 00:31:06,760
doesn't form a denial of service because

00:31:05,180 --> 00:31:09,410
you have more control over the client

00:31:06,760 --> 00:31:11,360
for the external users would not send

00:31:09,410 --> 00:31:17,230
this IP packet to forget the big

00:31:11,360 --> 00:31:17,230
messages anyone else

00:31:22,270 --> 00:31:27,080
hello hey yesterday

00:31:25,280 --> 00:31:29,180
they showed some performance figures

00:31:27,080 --> 00:31:31,490
when you receive the packets and

00:31:29,180 --> 00:31:33,770
basically dropped them so what type of

00:31:31,490 --> 00:31:36,940
performance figures did you get with all

00:31:33,770 --> 00:31:40,010
those hash lookups and array look up

00:31:36,940 --> 00:31:42,710
there was a number from the Intel

00:31:40,010 --> 00:31:46,250
yesterday there was the numbers in the

00:31:42,710 --> 00:31:48,830
HDPE paper by turkey so the truth is

00:31:46,250 --> 00:31:52,790
that it depends on the nick hardware it

00:31:48,830 --> 00:31:55,370
depends on the cpu and it's yeah you can

00:31:52,790 --> 00:31:59,350
check out Turkish and yes pressed paper

00:31:55,370 --> 00:32:01,790
on the numbers I can tell them so sorry

00:31:59,350 --> 00:32:04,310
yeah I was just going to answer that we

00:32:01,790 --> 00:32:07,190
actually took the open-source version of

00:32:04,310 --> 00:32:11,210
your atoms and ran as part of the paper

00:32:07,190 --> 00:32:14,480
so we have those numbers go check out

00:32:11,210 --> 00:32:16,430
the paper it's pretty good yeah I'm

00:32:14,480 --> 00:32:20,710
surprised like IPS numbers in that paper

00:32:16,430 --> 00:32:24,980
most likely you didn't do the flat stuff

00:32:20,710 --> 00:32:26,960
they're way too good all right Thank You

00:32:24,980 --> 00:32:32,539
Nikita thank you

00:32:26,960 --> 00:32:32,539

YouTube URL: https://www.youtube.com/watch?v=E1QKn_AjuJk


