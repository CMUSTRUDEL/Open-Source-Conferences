Title: LPC2018 - XDP Acceleration Using NIC Metadata, Continued
Publication date: 2018-12-04
Playlist: LPC2018 - Networking Track
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/101/
speaker:  P. J. Waskiewicz (Intel), Neerav Parikh (Intel)


This talk is a continuation of the initial XDP HW-based hints work presented at NetDev 2.1 in Seoul, South Korea.

It will start with focus on showcasing new prototypes to allow an XDP program to request required HW-generated metadata hints from a NIC. The talk will show how the hints are generated by the NIC and what are the performance characteristics for various XDP applications. We also want to demonstrate how such a metadata can be helpful for applications that use AF_XDP sockets.

The talk with then discuss planned upstreaming thoughts, and look to generate more discussion around implementation details, programming flows, etc., with the larger audience from the community.
Captions: 
	00:00:05,600 --> 00:00:13,400
so this work is a continuation of what

00:00:10,080 --> 00:00:15,809
we started last year at net dev in Seoul

00:00:13,400 --> 00:00:17,850
just wanted to give some credit here to

00:00:15,809 --> 00:00:20,640
one of my co-workers at Intel Neera of

00:00:17,850 --> 00:00:22,800
Parikh who couldn't make the trip he was

00:00:20,640 --> 00:00:24,920
involved in this on the architecture

00:00:22,800 --> 00:00:28,800
side and then we had the opportunity to

00:00:24,920 --> 00:00:30,810
collaborate with Saeed from Mellanox who

00:00:28,800 --> 00:00:33,030
will be joining me here in a few minutes

00:00:30,810 --> 00:00:35,280
to share some of the data that he's also

00:00:33,030 --> 00:00:40,289
been working on so hopefully we'll find

00:00:35,280 --> 00:00:42,239
this useful so just a level set everyone

00:00:40,289 --> 00:00:45,210
in case people didn't get to see the

00:00:42,239 --> 00:00:47,699
presentation last year or see the the

00:00:45,210 --> 00:00:49,770
recordings we'll do a quick recap of

00:00:47,699 --> 00:00:53,489
what we started last year what what was

00:00:49,770 --> 00:00:58,039
the idea what was the goal on looking at

00:00:53,489 --> 00:01:00,120
accelerating XDP using hardware offloads

00:00:58,039 --> 00:01:02,730
we'll look at some of those performance

00:01:00,120 --> 00:01:04,470
results that kind of encouraged us to

00:01:02,730 --> 00:01:08,040
continue working on this because it

00:01:04,470 --> 00:01:09,630
seemed very very promising and then we

00:01:08,040 --> 00:01:11,210
decided that for this year we should

00:01:09,630 --> 00:01:15,450
probably look at some real-world

00:01:11,210 --> 00:01:16,619
benchmarks and applications and not to

00:01:15,450 --> 00:01:18,090
steal too much thunder of the load

00:01:16,619 --> 00:01:21,420
balancer that'll be described tomorrow

00:01:18,090 --> 00:01:25,170
we did look at a layer for load balancer

00:01:21,420 --> 00:01:26,150
and how it react to manipulation using

00:01:25,170 --> 00:01:28,490
hardware offloads

00:01:26,150 --> 00:01:30,470
and we found some very interesting data

00:01:28,490 --> 00:01:33,030
interesting in a good way in a bad way

00:01:30,470 --> 00:01:35,909
and then Syed will come up and talk

00:01:33,030 --> 00:01:38,729
about some similar real-world tests that

00:01:35,909 --> 00:01:41,549
he was using on n cap D cap with

00:01:38,729 --> 00:01:44,909
tunneling and then we'll get into what

00:01:41,549 --> 00:01:46,710
Dave kind of alluded to we started the

00:01:44,909 --> 00:01:50,219
description of how do we actually

00:01:46,710 --> 00:01:53,130
program from an e BPF program or an xdp

00:01:50,219 --> 00:01:54,180
program down to the NIC to say I want

00:01:53,130 --> 00:01:55,710
these Hardware offloads

00:01:54,180 --> 00:01:58,409
please express them for me in some

00:01:55,710 --> 00:02:00,689
format and that has been kind of a an

00:01:58,409 --> 00:02:03,390
area of discussion on the mailing lists

00:02:00,689 --> 00:02:04,619
especially in the IO visor community we

00:02:03,390 --> 00:02:06,689
haven't really ground out on a

00:02:04,619 --> 00:02:08,520
resolution as to what it looks like but

00:02:06,689 --> 00:02:11,730
I think we have an idea of where we want

00:02:08,520 --> 00:02:13,700
to take it and then where do we go from

00:02:11,730 --> 00:02:16,290
there

00:02:13,700 --> 00:02:19,080
so I'm not gonna read through the big

00:02:16,290 --> 00:02:20,940
eye chart but

00:02:19,080 --> 00:02:22,380
this was about four different slides

00:02:20,940 --> 00:02:25,260
from last year crammed into one so

00:02:22,380 --> 00:02:27,180
excuse the the the tiny font but

00:02:25,260 --> 00:02:29,610
basically what we were looking at is

00:02:27,180 --> 00:02:31,680
every XDP program at some point has to

00:02:29,610 --> 00:02:34,290
parse the header right has to know what

00:02:31,680 --> 00:02:35,850
kind of packet it's dealing with and we

00:02:34,290 --> 00:02:38,340
kind of looked at this problem from a

00:02:35,850 --> 00:02:40,770
different perspective that the NIC the

00:02:38,340 --> 00:02:42,090
hardware already did the parsing already

00:02:40,770 --> 00:02:44,280
had to identify what that thing was

00:02:42,090 --> 00:02:45,840
where the headers are is there any

00:02:44,280 --> 00:02:48,270
relevant data that it needs to pull out

00:02:45,840 --> 00:02:52,230
to compute things like the RSS checks or

00:02:48,270 --> 00:02:54,390
the RSS hash validate check sums etc so

00:02:52,230 --> 00:02:57,270
can we leverage that metadata coming out

00:02:54,390 --> 00:02:59,790
of the hardware to hand that off to XTP

00:02:57,270 --> 00:03:02,310
so that XDP just didn't have to do that

00:02:59,790 --> 00:03:07,050
right so we can just save CPU cycles

00:03:02,310 --> 00:03:08,459
that was the that was the premise some

00:03:07,050 --> 00:03:10,860
of the other things that we we approach

00:03:08,459 --> 00:03:12,709
this with were keeping this as Hardware

00:03:10,860 --> 00:03:15,150
agnostic as possible right we have

00:03:12,709 --> 00:03:16,650
variety of vendors with a variety of

00:03:15,150 --> 00:03:18,140
features that kind of do the same thing

00:03:16,650 --> 00:03:21,000
but in various different ways

00:03:18,140 --> 00:03:23,820
we wouldn't want to impose any specific

00:03:21,000 --> 00:03:25,320
vendor secret sauce into the XDP

00:03:23,820 --> 00:03:32,570
framework that would kind of pollute it

00:03:25,320 --> 00:03:35,519
for others to be able to utilize it and

00:03:32,570 --> 00:03:37,320
this was on on also two sides how could

00:03:35,519 --> 00:03:39,690
we take existing NICs that are out there

00:03:37,320 --> 00:03:40,980
that are kind of dumb maybe they can do

00:03:39,690 --> 00:03:43,620
a couple different types of offloads

00:03:40,980 --> 00:03:45,959
could we squeeze a little bit of blood

00:03:43,620 --> 00:03:48,420
out of the turnip from that and looking

00:03:45,959 --> 00:03:50,670
forward to more intelligent more

00:03:48,420 --> 00:03:52,230
flexible NICs that are showing up in the

00:03:50,670 --> 00:03:55,290
market now and things planned in the

00:03:52,230 --> 00:03:57,870
future how can we either take what

00:03:55,290 --> 00:03:59,760
they're providing already into the xtp

00:03:57,870 --> 00:04:01,709
framework or how could we influence the

00:03:59,760 --> 00:04:06,989
design of that hardware to better

00:04:01,709 --> 00:04:08,489
benefit xdp right and one of the last

00:04:06,989 --> 00:04:10,709
things that we closed with last year

00:04:08,489 --> 00:04:14,940
that we kind of threw it out there and

00:04:10,709 --> 00:04:18,930
we punted and we might be punting again

00:04:14,940 --> 00:04:20,729
is the not just the metadata itself but

00:04:18,930 --> 00:04:23,490
how can we take advantage of some of the

00:04:20,729 --> 00:04:26,550
hardware's capabilities of matching an

00:04:23,490 --> 00:04:28,860
action like t cams to be able to take

00:04:26,550 --> 00:04:30,360
some kind of an identification thing and

00:04:28,860 --> 00:04:32,240
then take an action and actually drop

00:04:30,360 --> 00:04:33,560
the packet in the hardware instead of

00:04:32,240 --> 00:04:35,539
having to refer it back to the driver

00:04:33,560 --> 00:04:37,699
you know can can we offload some of

00:04:35,539 --> 00:04:42,500
these eb @ PF xtp semantics to the

00:04:37,699 --> 00:04:44,150
hardware so not getting this is a graph

00:04:42,500 --> 00:04:47,360
that we showed last year just to again

00:04:44,150 --> 00:04:49,639
level set everyone the very very

00:04:47,360 --> 00:04:53,900
important thing to focus on here is the

00:04:49,639 --> 00:04:55,610
bottom 3x DP 1 x DP 3 x DP hints with

00:04:53,900 --> 00:04:59,660
jibt because you know we just don't do

00:04:55,610 --> 00:05:01,310
know we know jet jet and the three

00:04:59,660 --> 00:05:03,949
programs that we have here are the the

00:05:01,310 --> 00:05:07,190
sample kernel code for x DP 1 and x 2 p

00:05:03,949 --> 00:05:09,229
3 x DP 1 is we parse the packet identify

00:05:07,190 --> 00:05:13,220
the protocol count it in a map and then

00:05:09,229 --> 00:05:16,280
drop it right there's no other parsing x

00:05:13,220 --> 00:05:18,259
DB 3 was to show how fast can we just

00:05:16,280 --> 00:05:19,610
drop packets we don't parse anything we

00:05:18,259 --> 00:05:22,940
don't take any cache misses we don't

00:05:19,610 --> 00:05:24,320
have to prefetch and we don't do any

00:05:22,940 --> 00:05:26,360
type of counting or accounting in the

00:05:24,320 --> 00:05:28,820
maps so that's kind of like the raw

00:05:26,360 --> 00:05:31,130
theoretical performance and then the

00:05:28,820 --> 00:05:33,229
third one was we would take the packet

00:05:31,130 --> 00:05:35,690
type identified by the by the driver by

00:05:33,229 --> 00:05:37,610
the actual hardware and this goes into

00:05:35,690 --> 00:05:39,229
the whole discussion of how do we common

00:05:37,610 --> 00:05:40,580
eyes packet types identified by hardware

00:05:39,229 --> 00:05:43,159
that that's a can of worms that we may

00:05:40,580 --> 00:05:45,110
not want to get into you right now but

00:05:43,159 --> 00:05:49,639
we go ahead and take that that's passed

00:05:45,110 --> 00:05:51,710
from the driver into the XDP program see

00:05:49,639 --> 00:05:53,930
which packet type it is count it and

00:05:51,710 --> 00:05:56,449
then drop it so we never parse the

00:05:53,930 --> 00:05:58,250
actual packet in the xtp program we just

00:05:56,449 --> 00:05:59,870
get the data that's referred to it and

00:05:58,250 --> 00:06:04,130
so the rightmost

00:05:59,870 --> 00:06:05,840
bar we hit a hardware limitation of that

00:06:04,130 --> 00:06:08,360
that version of the NIC with that

00:06:05,840 --> 00:06:10,310
firmware but it had very low CPU

00:06:08,360 --> 00:06:13,250
utilization so we saw that this had

00:06:10,310 --> 00:06:15,830
really really big potential that if the

00:06:13,250 --> 00:06:16,969
hardware was giving you the data that we

00:06:15,830 --> 00:06:19,130
can get some really really good

00:06:16,969 --> 00:06:24,949
performance so that spawned the rest of

00:06:19,130 --> 00:06:26,900
this work so there's some new stuff so

00:06:24,949 --> 00:06:29,000
this next slide and the slide after this

00:06:26,900 --> 00:06:31,460
deal with the layer four load balancer

00:06:29,000 --> 00:06:33,229
this is loosely based on the cat ran

00:06:31,460 --> 00:06:36,409
load balancer that was mentioned many

00:06:33,229 --> 00:06:40,070
times today from Facebook

00:06:36,409 --> 00:06:43,639
we took kind of a version of that and

00:06:40,070 --> 00:06:45,620
made some modifications so what we did

00:06:43,639 --> 00:06:48,470
was we had two types of

00:06:45,620 --> 00:06:51,620
intz changes that we made so the first

00:06:48,470 --> 00:06:55,280
one was just p-type packet type and the

00:06:51,620 --> 00:06:56,720
second one hence type 2 was everything

00:06:55,280 --> 00:06:59,060
from hence type once a packet type

00:06:56,720 --> 00:07:02,270
plusher 4-tuple source destination IP

00:06:59,060 --> 00:07:07,040
source destination port and then your

00:07:02,270 --> 00:07:08,600
RSS hash so in this case and we have

00:07:07,040 --> 00:07:10,280
various things here and I know it's a

00:07:08,600 --> 00:07:12,949
bit of an eye chart but the important

00:07:10,280 --> 00:07:15,110
ones here are the bottom three the XDP

00:07:12,949 --> 00:07:20,300
load balancer no hints with four cues

00:07:15,110 --> 00:07:23,180
that's that blueish bar here right there

00:07:20,300 --> 00:07:25,850
and then hints type one we took a little

00:07:23,180 --> 00:07:27,800
bit of a hit in performance we didn't

00:07:25,850 --> 00:07:29,419
dig terribly deeply into this but we

00:07:27,800 --> 00:07:31,340
looked at a couple things and it looked

00:07:29,419 --> 00:07:34,940
more of a we were actually taking a

00:07:31,340 --> 00:07:36,320
cache hit or a cache miss hit because we

00:07:34,940 --> 00:07:38,510
weren't parsing the rest of the header

00:07:36,320 --> 00:07:40,370
we were getting the packet type and

00:07:38,510 --> 00:07:42,470
that's one of the first things the XDP

00:07:40,370 --> 00:07:44,570
program does and that does effectively a

00:07:42,470 --> 00:07:46,490
prefetch to get the rest of the deeper

00:07:44,570 --> 00:07:48,800
part of the header and in this case we

00:07:46,490 --> 00:07:50,510
were already using the p-type and we

00:07:48,800 --> 00:07:51,740
were jumping into the you know deeper

00:07:50,510 --> 00:07:53,660
into the header we were taking a cache

00:07:51,740 --> 00:07:57,099
miss and it actually hurt us in this

00:07:53,660 --> 00:07:57,099
case and it was like ooh that sucks

00:07:57,430 --> 00:08:02,240
but then including the rest of that part

00:07:59,870 --> 00:08:04,520
of the header we can see that we

00:08:02,240 --> 00:08:07,250
actually basically doubled performance

00:08:04,520 --> 00:08:09,950
between no hints at all and using the

00:08:07,250 --> 00:08:11,539
hints to go ahead and identify this and

00:08:09,950 --> 00:08:16,280
then decide what to do in terms of the

00:08:11,539 --> 00:08:18,740
load balancing action two things to

00:08:16,280 --> 00:08:22,729
mention here one the hardware that this

00:08:18,740 --> 00:08:24,919
was done on we were able to generate the

00:08:22,729 --> 00:08:27,289
packet type from the hardware right that

00:08:24,919 --> 00:08:29,000
it can parse that already the RSS hash

00:08:27,289 --> 00:08:32,029
is also generated by the hardware but

00:08:29,000 --> 00:08:33,860
the rest of this we kind of faked right

00:08:32,029 --> 00:08:35,719
our Hardware didn't have the capability

00:08:33,860 --> 00:08:38,630
to pull that out into some kind of

00:08:35,719 --> 00:08:40,039
hardware structure like a descriptor so

00:08:38,630 --> 00:08:41,750
we actually had the driver parse this

00:08:40,039 --> 00:08:43,459
out of the packet header everything's

00:08:41,750 --> 00:08:45,050
already cache hot but we mimicked it as

00:08:43,459 --> 00:08:47,029
far as the xtv program was concerned

00:08:45,050 --> 00:08:49,370
that metadata was generated by the

00:08:47,029 --> 00:08:54,620
hardware so that's point number one

00:08:49,370 --> 00:08:58,300
so there's extra cycles going on point

00:08:54,620 --> 00:09:00,790
number one point one is because

00:08:58,300 --> 00:09:03,700
the way that we pass the metadata across

00:09:00,790 --> 00:09:05,440
we pass it as part of the XDP buff right

00:09:03,700 --> 00:09:07,930
so we have we put it in front of the the

00:09:05,440 --> 00:09:10,180
packet payload and we don't have the

00:09:07,930 --> 00:09:11,800
ability to DMA that directly in front of

00:09:10,180 --> 00:09:14,380
the packet payload yet so we have the

00:09:11,800 --> 00:09:16,750
overhead of a mem copy or a few mem

00:09:14,380 --> 00:09:19,240
copies in this case so that's data point

00:09:16,750 --> 00:09:21,460
number two and the third one is that

00:09:19,240 --> 00:09:23,950
title at the top was the load balancer

00:09:21,460 --> 00:09:25,930
with no state tracking and that's very

00:09:23,950 --> 00:09:28,150
important and this is where I said we

00:09:25,930 --> 00:09:31,930
had some interesting data that was good

00:09:28,150 --> 00:09:38,740
and not so great so looking at this with

00:09:31,930 --> 00:09:42,640
state tracking yes correct these are T X

00:09:38,740 --> 00:09:44,770
actions yes that's a good point yeah the

00:09:42,640 --> 00:09:46,690
question was these are straight TX

00:09:44,770 --> 00:09:52,150
actions not redirects and that is

00:09:46,690 --> 00:09:54,610
correct their TX so in this case this is

00:09:52,150 --> 00:09:58,120
the same exact setup now granted we

00:09:54,610 --> 00:10:02,020
boiled it down to using four Q's again

00:09:58,120 --> 00:10:05,050
on these last three and the no hints

00:10:02,020 --> 00:10:07,060
which is the big red one and the hints

00:10:05,050 --> 00:10:10,900
type two which is everything have the

00:10:07,060 --> 00:10:13,330
same exact performance and we were like

00:10:10,900 --> 00:10:15,520
well crap that's not exactly what we

00:10:13,330 --> 00:10:18,940
expected so there was a lot of

00:10:15,520 --> 00:10:22,900
head-scratching and we did find that the

00:10:18,940 --> 00:10:24,790
extra overhead of the map lookups and

00:10:22,900 --> 00:10:27,340
updates and everything to maintain the

00:10:24,790 --> 00:10:29,830
state was actually eating into the

00:10:27,340 --> 00:10:33,040
performance that we were gaining by

00:10:29,830 --> 00:10:36,250
having this metadata being passed along

00:10:33,040 --> 00:10:38,200
the CPU usage was different which was

00:10:36,250 --> 00:10:39,550
kind of promising so we're you know we

00:10:38,200 --> 00:10:42,130
had some Headroom but we had to figure

00:10:39,550 --> 00:10:45,120
out where it was getting stuck in I have

00:10:42,130 --> 00:10:49,900
any idea to order magnitude of CPU usage

00:10:45,120 --> 00:10:51,610
that's the next slide but but this one

00:10:49,900 --> 00:10:53,380
this one was kind of baffling right that

00:10:51,610 --> 00:10:56,200
we stared at this for a while and

00:10:53,380 --> 00:10:57,790
actually got pretty discouraged however

00:10:56,200 --> 00:11:01,090
the next slide will show that it's not

00:10:57,790 --> 00:11:03,130
as bad as we thought this link at the

00:11:01,090 --> 00:11:04,870
bottom is also referenced in the paper

00:11:03,130 --> 00:11:08,920
for this this is where we actually have

00:11:04,870 --> 00:11:12,189
our driver patches showing how we mimic

00:11:08,920 --> 00:11:15,639
the the hints this is on the i-40 driver

00:11:12,189 --> 00:11:18,429
by the way so we started digging into

00:11:15,639 --> 00:11:21,040
some perf stats trying to figure out

00:11:18,429 --> 00:11:24,879
where cycle counts are and what's going

00:11:21,040 --> 00:11:27,639
on and so on the right side basically

00:11:24,879 --> 00:11:30,489
what we saw and why I mentioned the mem

00:11:27,639 --> 00:11:32,259
copies and the other parsing that was

00:11:30,489 --> 00:11:36,879
happening we were seeing that there was

00:11:32,259 --> 00:11:38,879
some cycle count basically the CPU was

00:11:36,879 --> 00:11:42,970
stalled while we were waiting for some

00:11:38,879 --> 00:11:45,549
cache hit cache miss resolutions and

00:11:42,970 --> 00:11:47,970
then also the mem copies and the parsing

00:11:45,549 --> 00:11:51,009
itself were causing a lot of cache trash

00:11:47,970 --> 00:11:52,359
so we believe that based and this is all

00:11:51,009 --> 00:11:53,619
paper math we didn't actually verify

00:11:52,359 --> 00:11:57,279
this with hardware because we don't have

00:11:53,619 --> 00:11:58,540
hardware that does this yet that we were

00:11:57,279 --> 00:12:00,869
pretty confident that we can get at

00:11:58,540 --> 00:12:03,279
least a seven percent gain in the

00:12:00,869 --> 00:12:05,230
stateful load balancing so it's not

00:12:03,279 --> 00:12:07,149
earth shattering but it is something

00:12:05,230 --> 00:12:09,609
that is worth pursuing right that that's

00:12:07,149 --> 00:12:12,279
seven seven percent CPU cycles on a host

00:12:09,609 --> 00:12:18,579
that you can put to better use on your

00:12:12,279 --> 00:12:20,290
on your xeon hosts so this this data is

00:12:18,579 --> 00:12:22,239
as much as we could get for the

00:12:20,290 --> 00:12:24,549
conference here we do have next steps

00:12:22,239 --> 00:12:26,350
that we want to go ahead use some of our

00:12:24,549 --> 00:12:29,169
newer hardware that is able to go ahead

00:12:26,350 --> 00:12:31,179
and do some of this stuff in silicon to

00:12:29,169 --> 00:12:33,939
verify this we have some other pocs in

00:12:31,179 --> 00:12:36,970
flight that can really hone in on this

00:12:33,939 --> 00:12:39,149
even more we just couldn't get it by the

00:12:36,970 --> 00:12:41,559
time for the conference due to time I

00:12:39,149 --> 00:12:42,790
knew just see in the confluence of the

00:12:41,559 --> 00:12:47,259
fact that you have this software

00:12:42,790 --> 00:12:49,509
simulator of your hints in combination

00:12:47,259 --> 00:12:52,569
with all the memory traffic added by the

00:12:49,509 --> 00:12:53,319
state tracking stuff and then you go

00:12:52,569 --> 00:12:55,509
over to cliff

00:12:53,319 --> 00:12:57,579
yeah it was pretty pretty eye-opening

00:12:55,509 --> 00:12:58,959
that we we just kind of hit this barrier

00:12:57,579 --> 00:13:02,860
and then it was just like everything to

00:12:58,959 --> 00:13:05,230
stop at a certain point so some of the

00:13:02,860 --> 00:13:07,179
next steps and and the previous talk was

00:13:05,230 --> 00:13:09,789
very interesting on the stateless nation

00:13:07,179 --> 00:13:11,259
nature we do believe that some of the

00:13:09,789 --> 00:13:13,480
applications here may be stateful

00:13:11,259 --> 00:13:15,160
connections our stateful based

00:13:13,480 --> 00:13:17,679
xdp programs aren't the right way to

00:13:15,160 --> 00:13:20,259
attack with these types of techniques

00:13:17,679 --> 00:13:20,840
but stateless ones obviously if we you

00:13:20,259 --> 00:13:23,510
know just go

00:13:20,840 --> 00:13:26,210
to this you know we're seeing double the

00:13:23,510 --> 00:13:28,280
performance and stateless approaches so

00:13:26,210 --> 00:13:30,920
that's the next step we want to go after

00:13:28,280 --> 00:13:32,960
some more state lists based XDP programs

00:13:30,920 --> 00:13:37,190
to see where we can really have some

00:13:32,960 --> 00:13:39,140
value out here but Sayid I'm gonna come

00:13:37,190 --> 00:13:41,990
up he's going to go ahead and continue

00:13:39,140 --> 00:13:55,670
on some other data that he's been able

00:13:41,990 --> 00:13:58,370
to gather ok so thanks PJ for the nice

00:13:55,670 --> 00:14:04,580
interbank background for my no marks

00:13:58,370 --> 00:14:09,110
lights this is the first so I'm going to

00:14:04,580 --> 00:14:12,680
discuss now our experiments with our own

00:14:09,110 --> 00:14:15,620
patches for the hardware hints and the

00:14:12,680 --> 00:14:19,970
challenges for the API we've been

00:14:15,620 --> 00:14:26,380
working on to make this generic for

00:14:19,970 --> 00:14:29,750
every vendor so before we go into the

00:14:26,380 --> 00:14:35,060
details for the API I'm gonna show some

00:14:29,750 --> 00:14:39,980
performance we saw with exhibit X IP

00:14:35,060 --> 00:14:46,130
tunnel kernel sample which which also

00:14:39,980 --> 00:14:47,750
kind doing what any load balancer

00:14:46,130 --> 00:14:50,900
running production today without the

00:14:47,750 --> 00:14:53,180
load bouncing right so it's trying to

00:14:50,900 --> 00:14:56,900
receive packet classify those packets

00:14:53,180 --> 00:14:59,800
find the destination IP and the

00:14:56,900 --> 00:15:02,900
destination TCP or UDP port and

00:14:59,800 --> 00:15:06,290
according to that we look up into a PPF

00:15:02,900 --> 00:15:10,910
table or maybe F map and find the IP

00:15:06,290 --> 00:15:13,130
tunnel corresponding to the IP and IP

00:15:10,910 --> 00:15:16,160
tunnel corresponding to that packet and

00:15:13,130 --> 00:15:20,330
send it back to wire so what we try to

00:15:16,160 --> 00:15:23,140
do there is to save some cpu by avoiding

00:15:20,330 --> 00:15:26,830
the lookup and parsing with a packet

00:15:23,140 --> 00:15:26,830
using some kind of

00:15:27,100 --> 00:15:33,820
TC roll - mark - mark these packets with

00:15:31,570 --> 00:15:37,930
a hardware what we call a flow mark in

00:15:33,820 --> 00:15:41,230
the hardware and by by saving CPU with

00:15:37,930 --> 00:15:43,509
with just looking up this flow mark this

00:15:41,230 --> 00:15:49,209
Uniqlo mark which is we have a unique

00:15:43,509 --> 00:15:52,750
flow mark pair source this three to

00:15:49,209 --> 00:15:58,029
topple that we have that TCP port and

00:15:52,750 --> 00:16:01,149
destination IP and we see 50 almost 50

00:15:58,029 --> 00:16:04,600
to 70% for some cases performance

00:16:01,149 --> 00:16:05,740
improvement and then we hit the PCI

00:16:04,600 --> 00:16:09,990
bottom like that

00:16:05,740 --> 00:16:13,000
Jasper was talking about at almost 50 55

00:16:09,990 --> 00:16:16,509
packet per second so expect more

00:16:13,000 --> 00:16:20,459
we have patches to overcome this PCI

00:16:16,509 --> 00:16:23,279
bottleneck and we expect even the

00:16:20,459 --> 00:16:29,470
program without the hint would would hit

00:16:23,279 --> 00:16:37,690
100 cpu mark much much before the

00:16:29,470 --> 00:16:39,730
version with a hardware hint so

00:16:37,690 --> 00:16:44,470
regarding the API we would like to have

00:16:39,730 --> 00:16:51,370
a generic API which would provide a nice

00:16:44,470 --> 00:16:54,339
way to enable the whole thing DP driver

00:16:51,370 --> 00:16:57,670
hints for extra P programs for legacy

00:16:54,339 --> 00:17:00,220
necks like the RG bei for TE and the

00:16:57,670 --> 00:17:01,600
monix devices they currently they fit in

00:17:00,220 --> 00:17:04,630
the middle between legacy and

00:17:01,600 --> 00:17:08,290
programmable necks so programmable mix

00:17:04,630 --> 00:17:10,150
will provide a more flexible way to ask

00:17:08,290 --> 00:17:10,839
for specific hardware hints and not

00:17:10,150 --> 00:17:13,329
everything

00:17:10,839 --> 00:17:15,640
so today legacy annex makes provide

00:17:13,329 --> 00:17:18,220
everything that scripture and we would

00:17:15,640 --> 00:17:21,040
like also to have future needs to

00:17:18,220 --> 00:17:24,449
provide specific hardware hands for

00:17:21,040 --> 00:17:26,409
specific programs or specific use cases

00:17:24,449 --> 00:17:29,110
also we would like to have an

00:17:26,409 --> 00:17:33,480
association for arcs metadata so for a

00:17:29,110 --> 00:17:35,710
simple use case they have to provide all

00:17:33,480 --> 00:17:37,760
all the hardware hints that are

00:17:35,710 --> 00:17:41,470
available in a specific hardware or

00:17:37,760 --> 00:17:44,300
a subset of them for all our excuse

00:17:41,470 --> 00:17:48,050
would like also progressive programs run

00:17:44,300 --> 00:17:52,160
without requirement a data at all and we

00:17:48,050 --> 00:17:55,640
would like also to have there are xq

00:17:52,160 --> 00:17:58,340
beta data this is useful for FX DP used

00:17:55,640 --> 00:18:01,340
cases which we will have a specific

00:17:58,340 --> 00:18:03,710
socket run in a specific range on a

00:18:01,340 --> 00:18:08,980
specific set of workload and it would

00:18:03,710 --> 00:18:08,980
like to see a specific set of metadata

00:18:09,040 --> 00:18:15,560
so would like to have the same program

00:18:12,560 --> 00:18:17,930
programming module that we already have

00:18:15,560 --> 00:18:21,260
today with a specific with a small

00:18:17,930 --> 00:18:23,810
change that you would ask the driver to

00:18:21,260 --> 00:18:26,690
provide your hands for the program

00:18:23,810 --> 00:18:28,310
you're attaching and with the API that

00:18:26,690 --> 00:18:31,190
I'm going to discuss in the next next

00:18:28,310 --> 00:18:33,470
slide you'll get the hints that you

00:18:31,190 --> 00:18:36,950
requested in a generic way which would

00:18:33,470 --> 00:18:42,500
war in the same programming model for

00:18:36,950 --> 00:18:44,390
each and every nick provider and we

00:18:42,500 --> 00:18:46,340
would like also to support XP X DB

00:18:44,390 --> 00:18:51,440
metadata of configuration and at what

00:18:46,340 --> 00:18:54,890
time or compile time by having a dynamic

00:18:51,440 --> 00:18:57,410
way and/or API to ask for the metadata

00:18:54,890 --> 00:19:03,770
that you're worrying for your specific

00:18:57,410 --> 00:19:07,220
xdp program and use case so one approach

00:19:03,770 --> 00:19:13,820
is the first option that I have

00:19:07,220 --> 00:19:16,280
implemented in I think last summer so

00:19:13,820 --> 00:19:19,250
what I did is a simple solution where

00:19:16,280 --> 00:19:23,960
you have a pre known will define

00:19:19,250 --> 00:19:26,450
metadata that are already known by the

00:19:23,960 --> 00:19:29,240
kernel and already used even by escapees

00:19:26,450 --> 00:19:32,630
today escapees few fields that are

00:19:29,240 --> 00:19:36,440
populated with these metadata and these

00:19:32,630 --> 00:19:41,360
fields are stored in a nice fields

00:19:36,440 --> 00:19:44,060
offset array each each each element in

00:19:41,360 --> 00:19:46,100
the array defines where this metadata is

00:19:44,060 --> 00:19:48,830
stored in the xdp buffer metadata

00:19:46,100 --> 00:19:50,120
pointer if it's there it's going to give

00:19:48,830 --> 00:19:50,630
you the offset if it's not there it's

00:19:50,120 --> 00:19:54,260
going to say

00:19:50,630 --> 00:19:56,809
- one for example I'm not going to do go

00:19:54,260 --> 00:19:58,940
into details but the way to do for

00:19:56,809 --> 00:20:01,160
example to see in the in the last line

00:19:58,940 --> 00:20:05,150
up to number one you can see how we

00:20:01,160 --> 00:20:06,950
access the inflow mark just by having a

00:20:05,150 --> 00:20:10,940
difference to an offset array for the

00:20:06,950 --> 00:20:13,490
well-known XD pm8 of low mark and when

00:20:10,940 --> 00:20:16,010
it's well known you have the offset and

00:20:13,490 --> 00:20:18,799
you know the size and the format that

00:20:16,010 --> 00:20:23,570
flow marks you just typecast it and

00:20:18,799 --> 00:20:25,730
you'll have it right so the problem is

00:20:23,570 --> 00:20:29,799
with it with this approach it it's a

00:20:25,730 --> 00:20:32,900
fixed up approach for for a well-known

00:20:29,799 --> 00:20:34,280
metadata but if you if you want to

00:20:32,900 --> 00:20:35,990
introduce a new one you will have to

00:20:34,280 --> 00:20:37,940
introduce it to the driver kernel and

00:20:35,990 --> 00:20:42,049
use a space program so it's really not

00:20:37,940 --> 00:20:45,140
scalable and 3d not fixed flexible but

00:20:42,049 --> 00:20:49,010
really nice easy way to access and know

00:20:45,140 --> 00:20:51,590
the format of the metered data and here

00:20:49,010 --> 00:20:57,530
come option number two where we would

00:20:51,590 --> 00:21:00,370
use the BTF approach which is PPF type

00:20:57,530 --> 00:21:01,929
format was introduced by facebook at

00:21:00,370 --> 00:21:05,630
4.15

00:21:01,929 --> 00:21:09,200
and the idea there i think it's simple

00:21:05,630 --> 00:21:14,710
because bit if just a nice way to

00:21:09,200 --> 00:21:17,950
describe see like structures and you can

00:21:14,710 --> 00:21:20,750
describe it in a specific description

00:21:17,950 --> 00:21:23,179
translated to c language or c header

00:21:20,750 --> 00:21:26,120
files and just compile with it so it can

00:21:23,179 --> 00:21:29,900
inscribe the metadata is coming in from

00:21:26,120 --> 00:21:32,750
the hardware in a nice header file so

00:21:29,900 --> 00:21:34,850
the idea is that you before drive for

00:21:32,750 --> 00:21:37,280
loading drags the P program or even

00:21:34,850 --> 00:21:40,490
before compiling it you would query the

00:21:37,280 --> 00:21:42,590
driver the hardware or the neck gonna

00:21:40,490 --> 00:21:46,640
give you back that btf format you're

00:21:42,590 --> 00:21:50,750
gonna translate it into c header file

00:21:46,640 --> 00:21:56,470
and then just compile your program and

00:21:50,750 --> 00:21:58,130
just hit like just in your Pico just

00:21:56,470 --> 00:22:03,539
it's

00:21:58,130 --> 00:22:06,570
and access hatch or checksum or whatever

00:22:03,539 --> 00:22:08,279
a fluid or metadata are looking for if

00:22:06,570 --> 00:22:12,779
it's provided by the hardware of course

00:22:08,279 --> 00:22:16,230
and we would like optionally to have it

00:22:12,779 --> 00:22:20,640
driver version and even specific

00:22:16,230 --> 00:22:23,400
configuration in your link and we would

00:22:20,640 --> 00:22:26,130
like also to verify that the driver on

00:22:23,400 --> 00:22:27,900
the PPF load of the XDP program so you

00:22:26,130 --> 00:22:31,770
would we would know that you're using

00:22:27,900 --> 00:22:35,130
the right bit if version I'm not using

00:22:31,770 --> 00:22:37,950
an old one a different hardware so we'd

00:22:35,130 --> 00:22:39,840
feel that if you're using a PDF program

00:22:37,950 --> 00:22:44,809
which exit the program which was

00:22:39,840 --> 00:22:44,809
compiled with without dated PDF format

00:22:44,840 --> 00:22:53,700
this will enable us to have different

00:22:49,789 --> 00:22:57,799
offload or metadata formats for

00:22:53,700 --> 00:23:00,120
different necks and this will allow

00:22:57,799 --> 00:23:02,580
flexibility and scalability without

00:23:00,120 --> 00:23:05,159
every time the need to change the kernel

00:23:02,580 --> 00:23:08,130
in order to introduce new types and have

00:23:05,159 --> 00:23:11,250
new types for metadata and hardware

00:23:08,130 --> 00:23:18,179
hints for newer versions of hardware

00:23:11,250 --> 00:23:23,640
firmer or even proprietary Hardware

00:23:18,179 --> 00:23:26,159
hands so I'm going to discuss some some

00:23:23,640 --> 00:23:30,059
pros and cons between the two approaches

00:23:26,159 --> 00:23:31,500
and I feel that we all agree and what

00:23:30,059 --> 00:23:34,830
have we had in the discussion the melon

00:23:31,500 --> 00:23:38,340
is that we all agree to go to the btf

00:23:34,830 --> 00:23:40,950
approach but I would like to emphasize

00:23:38,340 --> 00:23:45,000
some pros and cons between the two

00:23:40,950 --> 00:23:46,860
approaches so losing the btf is good for

00:23:45,000 --> 00:23:49,799
allowing as I said allowing vendor

00:23:46,860 --> 00:23:51,470
specific or defined hints very easily

00:23:49,799 --> 00:23:54,960
without the need to change the kernel

00:23:51,470 --> 00:23:57,000
just install the new firmware where they

00:23:54,960 --> 00:23:59,190
the driver and it will give you the new

00:23:57,000 --> 00:24:04,080
format and the new and the new offload

00:23:59,190 --> 00:24:07,200
you're looking for metadata access of

00:24:04,080 --> 00:24:09,299
xdp program is figured out at compile

00:24:07,200 --> 00:24:13,039
time you don't have to use offset array

00:24:09,299 --> 00:24:13,039
to calculate the offset of

00:24:14,330 --> 00:24:19,500
meet the data that you're looking for

00:24:16,410 --> 00:24:22,530
just you can pile the driver with him BJ

00:24:19,500 --> 00:24:28,860
if the scripture you have direct access

00:24:22,530 --> 00:24:31,830
to what is really bothering me and is

00:24:28,860 --> 00:24:33,960
that with the vtf approach is that you

00:24:31,830 --> 00:24:37,640
will have to recompile you drop your

00:24:33,960 --> 00:24:40,100
things DP program with every version of

00:24:37,640 --> 00:24:43,500
whatever you make that you're using so

00:24:40,100 --> 00:24:47,880
and even if you operate a few more you

00:24:43,500 --> 00:24:50,610
will get a new PDF version you'll have

00:24:47,880 --> 00:24:52,740
to recompile the way the xdp program I

00:24:50,610 --> 00:24:57,780
think this is really bad for production

00:24:52,740 --> 00:25:00,690
but it's really flexible so I wonder if

00:24:57,780 --> 00:25:03,090
we can get to the point where if the

00:25:00,690 --> 00:25:07,800
standardized fields in some common

00:25:03,090 --> 00:25:10,500
structure yes ok nevermind I'm not gonna

00:25:07,800 --> 00:25:12,240
steal your thunder yeah so we also need

00:25:10,500 --> 00:25:14,430
to figure out how we do we standardize

00:25:12,240 --> 00:25:17,300
some feels like has chicks under all the

00:25:14,430 --> 00:25:20,130
same or the same for all hardware necks

00:25:17,300 --> 00:25:23,910
so it's really up to naming convention

00:25:20,130 --> 00:25:26,310
right so if you would like to see only

00:25:23,910 --> 00:25:29,340
the hash so would act she would request

00:25:26,310 --> 00:25:32,400
from the driver out like your bf

00:25:29,340 --> 00:25:35,340
descriptor with only the hash you mask

00:25:32,400 --> 00:25:38,190
everything mark him as reserved and just

00:25:35,340 --> 00:25:40,170
compile your program with pointing to

00:25:38,190 --> 00:25:46,200
the hairs that you got with the BF

00:25:40,170 --> 00:25:48,090
descriptor of that specific neck and if

00:25:46,200 --> 00:25:48,870
you recompile the same program with

00:25:48,090 --> 00:25:51,240
different necks

00:25:48,870 --> 00:25:57,900
you will you will you will get the same

00:25:51,240 --> 00:26:00,750
behavior you will get the hash field but

00:25:57,900 --> 00:26:02,540
it's really up to the NIC vendors to

00:26:00,750 --> 00:26:05,010
decide or define what are the

00:26:02,540 --> 00:26:05,580
standardized fields and how they look

00:26:05,010 --> 00:26:07,680
like

00:26:05,580 --> 00:26:09,560
the problem is that with offsets it's

00:26:07,680 --> 00:26:16,310
gonna be a different offset for every

00:26:09,560 --> 00:26:21,220
Nik and also sometimes it's up to the

00:26:16,310 --> 00:26:21,220
endianness of it's gonna be a big Indian

00:26:23,980 --> 00:26:34,550
to translate in the XDP program which is

00:26:29,060 --> 00:26:37,250
also kind of sucks here are links to the

00:26:34,550 --> 00:26:40,520
work in progress for Intel and for my

00:26:37,250 --> 00:26:49,760
onyx as well please feel free to review

00:26:40,520 --> 00:26:51,380
it and look at it I think we your no no

00:26:49,760 --> 00:26:53,660
not yet today we haven't released those

00:26:51,380 --> 00:26:55,640
yeah we we still also still considering

00:26:53,660 --> 00:27:02,930
the PDF approach and we really need an

00:26:55,640 --> 00:27:05,330
agreement for this so yeah next steps is

00:27:02,930 --> 00:27:08,300
to get an agreement whether we are going

00:27:05,330 --> 00:27:13,670
to the btf approach or not so we start

00:27:08,300 --> 00:27:15,280
because it's a lot of programming so

00:27:13,670 --> 00:27:18,440
yeah we need to give that out of the way

00:27:15,280 --> 00:27:22,630
next thing training I think is a very

00:27:18,440 --> 00:27:26,690
simple approach to achieve because

00:27:22,630 --> 00:27:28,810
training to pass the the metadata from

00:27:26,690 --> 00:27:31,270
VP program to the next one we just

00:27:28,810 --> 00:27:35,150
populate the metadata

00:27:31,270 --> 00:27:39,140
with whatever metadata you want because

00:27:35,150 --> 00:27:41,690
it's an EP program specific thing now

00:27:39,140 --> 00:27:44,120
there's a problem with a placement of

00:27:41,690 --> 00:27:49,900
the XDP metadata today it sets before

00:27:44,120 --> 00:27:54,130
the xdp before the X DB packet and it's

00:27:49,900 --> 00:27:57,350
it's it's okay if you're not going to do

00:27:54,130 --> 00:27:59,380
x DP I just hit if you're not going to

00:27:57,350 --> 00:28:03,250
use the headroom at all but if you are

00:27:59,380 --> 00:28:06,700
for example the encapsulating or

00:28:03,250 --> 00:28:06,700
changing the headers

00:28:07,310 --> 00:28:12,710
increasing the packet says whatever

00:28:09,410 --> 00:28:14,270
you're doing adding a villain header you

00:28:12,710 --> 00:28:18,280
would hit a problem when they hit a

00:28:14,270 --> 00:28:20,540
problem with the overriding the XDP

00:28:18,280 --> 00:28:23,180
metadata and you will have to make copy

00:28:20,540 --> 00:28:25,430
oh every time you're going to do just

00:28:23,180 --> 00:28:28,010
hit and you would lose any performance

00:28:25,430 --> 00:28:31,370
hit you we you got from the hardware

00:28:28,010 --> 00:28:33,110
hand so so there are some kind so that

00:28:31,370 --> 00:28:34,760
there are some solution for this in my

00:28:33,110 --> 00:28:36,950
patches I did invalidate a meta data

00:28:34,760 --> 00:28:38,720
once I read them so once I got I made a

00:28:36,950 --> 00:28:42,050
date and xdp program I just invalidate

00:28:38,720 --> 00:28:46,010
the metadata I just said anymore in

00:28:42,050 --> 00:28:48,440
there so once I called just hit there

00:28:46,010 --> 00:28:52,430
was no metadata to move around so this

00:28:48,440 --> 00:28:52,910
fixed the issue locally for me one way

00:28:52,430 --> 00:28:55,280
to do it

00:28:52,910 --> 00:28:58,670
we thought about placing the metadata

00:28:55,280 --> 00:29:01,010
hard the metadata into the hard start to

00:28:58,670 --> 00:29:06,230
the start of the page that also would

00:29:01,010 --> 00:29:10,850
help and I just discussed it with Jasper

00:29:06,230 --> 00:29:14,570
we have I got a nice idea if it's

00:29:10,850 --> 00:29:17,330
possible I think just percent is to just

00:29:14,570 --> 00:29:20,150
expose the hardware descriptor into the

00:29:17,330 --> 00:29:22,250
xdp program as a read-only pointer that

00:29:20,150 --> 00:29:24,680
would help a lot with the btf if we can

00:29:22,250 --> 00:29:29,480
describe the metadata the harbour

00:29:24,680 --> 00:29:36,170
descriptor with a PDF format and give it

00:29:29,480 --> 00:29:39,890
to the yes

00:29:36,170 --> 00:29:43,010
so looking beyond this right so we in

00:29:39,890 --> 00:29:45,370
this morning's first talk we also looked

00:29:43,010 --> 00:29:47,330
at how can some of this hardware

00:29:45,370 --> 00:29:50,690
acceleration maybe you apply to things

00:29:47,330 --> 00:29:53,300
like helping the TX checks I'm off load

00:29:50,690 --> 00:29:56,000
on either X DB TX to redirect right can

00:29:53,300 --> 00:29:59,750
we play some metadata to help on the way

00:29:56,000 --> 00:30:02,000
back out of the Nick for end cap D cap

00:29:59,750 --> 00:30:05,090
QoS if we're doing traffic shaping in

00:30:02,000 --> 00:30:07,670
any way what can we do with this

00:30:05,090 --> 00:30:09,280
framework to help expand that so these

00:30:07,670 --> 00:30:13,100
are things that we're gonna look at next

00:30:09,280 --> 00:30:16,940
once we get past the btf formatting and

00:30:13,100 --> 00:30:18,680
how we how we handle that and then the

00:30:16,940 --> 00:30:19,410
the big one I think that we're gonna get

00:30:18,680 --> 00:30:21,300
some really

00:30:19,410 --> 00:30:23,340
bang for a buck on some of the new

00:30:21,300 --> 00:30:25,980
Knicks coming out in the next few years

00:30:23,340 --> 00:30:28,470
or some of them that are existing today

00:30:25,980 --> 00:30:30,360
as well is how do we go with these flow

00:30:28,470 --> 00:30:33,500
look about lookups and actions how do we

00:30:30,360 --> 00:30:35,190
utilize some of this advanced internal

00:30:33,500 --> 00:30:38,160
infrastructure in the Knicks that can

00:30:35,190 --> 00:30:40,920
either do like internal switching you

00:30:38,160 --> 00:30:44,460
know through based on an action in a tee

00:30:40,920 --> 00:30:46,410
cam can we really push some of that down

00:30:44,460 --> 00:30:48,870
so that we still have the XT PE program

00:30:46,410 --> 00:30:49,950
core running in the kernel that we have

00:30:48,870 --> 00:30:51,840
introspection into the actual

00:30:49,950 --> 00:30:53,660
application running but then get the

00:30:51,840 --> 00:30:55,560
benefit of hardware offload

00:30:53,660 --> 00:30:57,090
transparently to the program and that's

00:30:55,560 --> 00:30:58,410
that's really something I think is gonna

00:30:57,090 --> 00:30:59,100
be pretty powerful but there's a lot of

00:30:58,410 --> 00:31:01,260
work to do here

00:30:59,100 --> 00:31:03,000
xt p is a slow path for the hardware

00:31:01,260 --> 00:31:07,920
yeah XD p is the slow path for the

00:31:03,000 --> 00:31:15,030
hardware exactly and yeah and for a

00:31:07,920 --> 00:31:16,320
question or two if we managed to

00:31:15,030 --> 00:31:17,820
standardize not only the naming

00:31:16,320 --> 00:31:19,230
convention in the BTS but also the

00:31:17,820 --> 00:31:25,560
offsets when we still need to recompile

00:31:19,230 --> 00:31:27,660
xt i'm sorry please peek into the box I

00:31:25,560 --> 00:31:31,020
didn't talk to the box

00:31:27,660 --> 00:31:33,000
so the thing that maybe isn't sinking in

00:31:31,020 --> 00:31:35,370
is that the whole idea is that LLVM will

00:31:33,000 --> 00:31:38,610
support symbolic loads of offsets in

00:31:35,370 --> 00:31:40,920
structures so you'll get a load to like

00:31:38,610 --> 00:31:42,900
task bid for a trace point right and

00:31:40,920 --> 00:31:45,240
then the verifier will translate that

00:31:42,900 --> 00:31:46,440
using btf into the actual hexadecimal

00:31:45,240 --> 00:31:48,570
load and rewrite the load instruction

00:31:46,440 --> 00:31:52,020
for you so this all be transparent so

00:31:48,570 --> 00:31:53,760
our original proposal last year was to

00:31:52,020 --> 00:31:56,190
standardize the format so that it was

00:31:53,760 --> 00:31:57,930
rigid and like Syed said that's just not

00:31:56,190 --> 00:31:59,970
scalable you have some new offload come

00:31:57,930 --> 00:32:01,890
you have some new capability now you

00:31:59,970 --> 00:32:03,360
have to change the kernel right and we

00:32:01,890 --> 00:32:06,390
want to avoid that at all costs

00:32:03,360 --> 00:32:06,780
Oh John's got it up okay it's an easy

00:32:06,390 --> 00:32:10,080
one

00:32:06,780 --> 00:32:11,700
I think this always comes up that we

00:32:10,080 --> 00:32:13,470
want a pointer to the descriptor and

00:32:11,700 --> 00:32:15,150
then we want to figure out how to map it

00:32:13,470 --> 00:32:16,920
have you ever just considered having

00:32:15,150 --> 00:32:19,050
like a JIT for the driver on the back

00:32:16,920 --> 00:32:20,640
end that translates the the lookup into

00:32:19,050 --> 00:32:22,680
a descriptor because it but you're

00:32:20,640 --> 00:32:24,510
behind the verifier after the JIT you

00:32:22,680 --> 00:32:26,820
could have like a I for to eejit that

00:32:24,510 --> 00:32:28,050
map's the hash into the descriptor Reed

00:32:26,820 --> 00:32:29,280
and then you wouldn't you wouldn't have

00:32:28,050 --> 00:32:32,360
this question come up all the time about

00:32:29,280 --> 00:32:37,429
how do we optimize for my descriptor

00:32:32,360 --> 00:32:40,580
or like a BT of BPF template something

00:32:37,429 --> 00:32:41,690
like that so that gets cheated before

00:32:40,580 --> 00:32:44,630
and I think that sounds like a cool idea

00:32:41,690 --> 00:32:48,110
that's a really good idea alright last

00:32:44,630 --> 00:32:50,899
question John went easy on us

00:32:48,110 --> 00:32:52,669
yeah so the we had that idea of doing

00:32:50,899 --> 00:32:54,049
the rewrite the the problem becomes like

00:32:52,669 --> 00:32:55,460
you actually have to generate the code

00:32:54,049 --> 00:32:57,500
when you want to do the rewrite because

00:32:55,460 --> 00:32:59,480
the kids in the descriptors are usually

00:32:57,500 --> 00:33:03,010
masked like whether the field is present

00:32:59,480 --> 00:33:03,010
or not is dependent on something else

00:33:03,130 --> 00:33:09,260
the verifier has access to the actual

00:33:06,190 --> 00:33:10,549
full BPF program it can emit masking and

00:33:09,260 --> 00:33:12,200
using a temp register and stuff like

00:33:10,549 --> 00:33:15,100
that but yes that is a good point the

00:33:12,200 --> 00:33:18,130
fields are inhomogeneous 32-bit values

00:33:15,100 --> 00:33:22,869
okay thank you very much

00:33:18,130 --> 00:33:22,869

YouTube URL: https://www.youtube.com/watch?v=BzHiEGzFJC0


