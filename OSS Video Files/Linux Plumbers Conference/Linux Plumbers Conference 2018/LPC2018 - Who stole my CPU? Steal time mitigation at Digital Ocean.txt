Title: LPC2018 - Who stole my CPU? Steal time mitigation at Digital Ocean
Publication date: 2018-11-28
Playlist: Linux Plumbers Conference 2018
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/262/
speaker:  Leonid Podolny (DigitalOcean), Vineeth Remanan Pillai (DigitalOcean)


Steal time due to hypervisor overcommitment is a widespread and well-understood phenomena in virtualized environments. 
However, sometimes steal appears even when a hypervisor is not overcommitted. This talk will lay out our quest for better utilization of hypervisor hardware by reducing steal. We will talk about kernel heuristics causing it, how we handle disabling these heuristics by implementing a userspace daemon and the issues that arise from this.
Captions: 
	00:00:06,109 --> 00:00:12,209
my name is Leo if we will be talking

00:00:10,049 --> 00:00:14,760
about our talk is called who stole my CP

00:00:12,209 --> 00:00:24,689
will be talking about still time in

00:00:14,760 --> 00:00:27,779
digital we work at digitalocean digital

00:00:24,689 --> 00:00:29,460
ocean is a public cloud provider we are

00:00:27,779 --> 00:00:30,810
providing developers and businesses with

00:00:29,460 --> 00:00:33,000
a reliable easy to do

00:00:30,810 --> 00:00:34,860
cloud computing platform with virtual

00:00:33,000 --> 00:00:38,820
servers could call them droplets and

00:00:34,860 --> 00:00:40,680
object storage spaces and more the team

00:00:38,820 --> 00:00:43,620
that we are on if the systems

00:00:40,680 --> 00:00:47,100
engineering we are responsible for hydro

00:00:43,620 --> 00:00:49,440
hypervisors in our fleet essentially for

00:00:47,100 --> 00:00:53,670
everything above the level of harder and

00:00:49,440 --> 00:00:57,930
below the first party services that are

00:00:53,670 --> 00:01:00,620
responsible for things like spawning

00:00:57,930 --> 00:01:00,620
virtual machines

00:01:04,000 --> 00:01:09,790
we will be talking today about the

00:01:07,770 --> 00:01:13,320
historical evolution of our thinking

00:01:09,790 --> 00:01:15,970
about steal time on our hypervisors

00:01:13,320 --> 00:01:18,009
first of all we will talk about what

00:01:15,970 --> 00:01:19,979
what is still just for her for the sake

00:01:18,009 --> 00:01:23,399
of completeness then we will talk about

00:01:19,979 --> 00:01:26,619
what coated steel and and how we

00:01:23,399 --> 00:01:30,159
categorized steel that we are seeing to

00:01:26,619 --> 00:01:31,659
establish a framework of for the further

00:01:30,159 --> 00:01:34,750
discussion then we will talk about

00:01:31,659 --> 00:01:39,670
mitigation of steel first we will talk

00:01:34,750 --> 00:01:43,450
about a kernel heuristics that we found

00:01:39,670 --> 00:01:45,250
that that cost you and and then we will

00:01:43,450 --> 00:01:47,890
talk about implementation the

00:01:45,250 --> 00:01:49,330
implementation will be talking about a

00:01:47,890 --> 00:01:51,130
user space demon front and our

00:01:49,330 --> 00:01:53,890
hypervisor which which is called octopus

00:01:51,130 --> 00:02:00,069
and eight challenges that we ran into

00:01:53,890 --> 00:02:04,810
while while implementing first of all

00:02:00,069 --> 00:02:11,200
what is still a when you open it to like

00:02:04,810 --> 00:02:13,150
top perhaps because of the a performance

00:02:11,200 --> 00:02:16,989
problem on your virtual machine you will

00:02:13,150 --> 00:02:18,970
typically see the CPU time categorized

00:02:16,989 --> 00:02:24,640
into several categories such as

00:02:18,970 --> 00:02:30,370
I await system user one such category is

00:02:24,640 --> 00:02:34,090
the steal time a environment means that

00:02:30,370 --> 00:02:36,940
that's the time that your virtual CPU

00:02:34,090 --> 00:02:39,730
waited to receive it a physical CPU to

00:02:36,940 --> 00:02:42,010
run on especially the essentially that

00:02:39,730 --> 00:02:48,040
the time that it spent sitting in

00:02:42,010 --> 00:02:48,700
ranking from the perspective of the the

00:02:48,040 --> 00:02:51,250
hypervisor

00:02:48,700 --> 00:02:54,340
evcp you is just another OS tasks so it

00:02:51,250 --> 00:02:56,140
can it can lose CPU when it happens from

00:02:54,340 --> 00:02:58,350
the perspective of the guest the time

00:02:56,140 --> 00:03:03,180
just stops

00:02:58,350 --> 00:03:04,350
a still exists only a only within the

00:03:03,180 --> 00:03:05,790
virtual motion me if you look on the

00:03:04,350 --> 00:03:08,580
physical motion that still though a zero

00:03:05,790 --> 00:03:12,000
another another side note that I forgot

00:03:08,580 --> 00:03:13,350
to mention we had a kind of an internal

00:03:12,000 --> 00:03:16,230
discussion when we were preparing for

00:03:13,350 --> 00:03:17,550
this talk if still can be used as a noun

00:03:16,230 --> 00:03:19,770
we say h2o

00:03:17,550 --> 00:03:23,990
it's a I'm not sure how whites president

00:03:19,770 --> 00:03:23,990
but we do say h2o

00:03:24,320 --> 00:03:37,260
like couldn't really conclusion since

00:03:34,980 --> 00:03:40,350
the the process of preemption of guests

00:03:37,260 --> 00:03:44,370
the guests have no control off then it

00:03:40,350 --> 00:03:45,570
means that they they can't even even do

00:03:44,370 --> 00:03:48,470
the accounting they they rely on

00:03:45,570 --> 00:03:54,720
hypervisor to do the accounting for them

00:03:48,470 --> 00:03:56,340
okay why still exists the obvious reason

00:03:54,720 --> 00:04:02,100
is when they the hypervisor is

00:03:56,340 --> 00:04:03,800
overcommitted a when they combine

00:04:02,100 --> 00:04:06,450
virtual machines on the hypervisor push

00:04:03,800 --> 00:04:10,410
through more CPU work than there is

00:04:06,450 --> 00:04:14,100
physical physical CPUs to give you

00:04:10,410 --> 00:04:16,020
system the less obvious fact is that

00:04:14,100 --> 00:04:18,000
sometimes there are very there are idle

00:04:16,020 --> 00:04:19,620
resources from the hypervisor and you

00:04:18,000 --> 00:04:21,600
still and still the dursa motion

00:04:19,620 --> 00:04:25,160
experience still which superficially

00:04:21,600 --> 00:04:25,160
should never happen but

00:04:30,630 --> 00:04:33,170
it

00:04:34,150 --> 00:04:37,480
when we look at steel of the of the

00:04:36,040 --> 00:04:39,190
diversions we look at it from two

00:04:37,480 --> 00:04:41,350
different perspectives one of them is

00:04:39,190 --> 00:04:43,840
you take an individual virtual machine

00:04:41,350 --> 00:04:48,490
and see if it experiences still if it if

00:04:43,840 --> 00:04:50,139
it experiences to a by by looking at an

00:04:48,490 --> 00:04:51,790
individual there's motion you can know

00:04:50,139 --> 00:04:53,650
if it experiences performance problems

00:04:51,790 --> 00:04:57,360
they still doesn't automatically

00:04:53,650 --> 00:05:01,210
translate them to performance problems

00:04:57,360 --> 00:05:04,000
can from the other side you can look at

00:05:01,210 --> 00:05:04,630
the hypervisor and see if if still is

00:05:04,000 --> 00:05:06,400
the hypervisor

00:05:04,630 --> 00:05:08,860
white-white phenomena you would look at

00:05:06,400 --> 00:05:10,600
things like statistical distribution of

00:05:08,860 --> 00:05:13,630
still across virtual motion time I would

00:05:10,600 --> 00:05:15,040
like to emphasize that hypervisor self

00:05:13,630 --> 00:05:18,370
never experience it still we're still

00:05:15,040 --> 00:05:22,870
looking at still on within virtual

00:05:18,370 --> 00:05:27,430
machines but you look at things like a

00:05:22,870 --> 00:05:30,660
are there the end experience to what's

00:05:27,430 --> 00:05:34,240
the distribution the averages outliers

00:05:30,660 --> 00:05:36,669
all the other statistical stuff another

00:05:34,240 --> 00:05:40,930
important thing is to look if they the

00:05:36,669 --> 00:05:42,729
hypervisor has idle resources this

00:05:40,930 --> 00:05:45,370
perspective is useful because it will

00:05:42,729 --> 00:05:48,300
tell you it will tell you if the verge

00:05:45,370 --> 00:05:48,300
solutions can be helped

00:05:52,230 --> 00:05:55,780
the useful metric we're looking for it's

00:05:54,790 --> 00:05:57,940
still from the perspective of an

00:05:55,780 --> 00:06:02,170
individual virtual machine is a sum of

00:05:57,940 --> 00:06:04,180
utilization and and and still if it's

00:06:02,170 --> 00:06:06,490
close to 100 it means that the Machine

00:06:04,180 --> 00:06:09,210
does experience performance performance

00:06:06,490 --> 00:06:13,600
problems and say it would have used

00:06:09,210 --> 00:06:15,790
stolen time had had had they still

00:06:13,600 --> 00:06:22,270
wasn't there if the machine is

00:06:15,790 --> 00:06:25,980
relatively idle probably not and again

00:06:22,270 --> 00:06:28,870
but when you look at the hypervisor from

00:06:25,980 --> 00:06:33,250
still from the hiker as a white

00:06:28,870 --> 00:06:34,540
phenomena the question here is whether

00:06:33,250 --> 00:06:36,190
there are idle resources

00:06:34,540 --> 00:06:40,030
if there are no idle resources there is

00:06:36,190 --> 00:06:42,040
nothing that can be done the only thing

00:06:40,030 --> 00:06:43,840
that can be done is migrate some of the

00:06:42,040 --> 00:06:46,900
the virtual machine to another I'd

00:06:43,840 --> 00:06:50,710
rather to to make it become to make it

00:06:46,900 --> 00:06:54,430
less overcommitted if there are a there

00:06:50,710 --> 00:06:56,410
is two but with idle resources then it's

00:06:54,430 --> 00:07:00,240
some kind of miss configuration of your

00:06:56,410 --> 00:07:00,240
your hypervisor and it can be helped

00:07:03,410 --> 00:07:10,510
that's an example here we have ten

00:07:07,400 --> 00:07:13,310
virtual machines a all running top and

00:07:10,510 --> 00:07:15,590
we see first of all we see that the

00:07:13,310 --> 00:07:20,810
hypervisor itself is not over committed

00:07:15,590 --> 00:07:23,480
it's only 81 percent percent youth

00:07:20,810 --> 00:07:25,460
however there are several machines that

00:07:23,480 --> 00:07:28,670
experience high level of Steel of Steel

00:07:25,460 --> 00:07:34,760
20 and 30% the other notable thing here

00:07:28,670 --> 00:07:37,400
is that larger beams are experienced

00:07:34,760 --> 00:07:39,230
higher higher level levels of Steel than

00:07:37,400 --> 00:07:46,330
smaller ones it's essentially smaller

00:07:39,230 --> 00:07:46,330
ones a star of the larger ones out and

00:07:50,290 --> 00:07:57,260
thanks leo so this synthetic example

00:07:54,590 --> 00:08:01,400
that we have created here to demonstrate

00:07:57,260 --> 00:08:05,210
you this scenario was visible on a

00:08:01,400 --> 00:08:06,560
portion of a freight a while back and we

00:08:05,210 --> 00:08:08,570
started investigating what's what's

00:08:06,560 --> 00:08:10,460
happening what's like why the VMS are

00:08:08,570 --> 00:08:12,920
facing steel when you have enough

00:08:10,460 --> 00:08:15,080
resources in the system and the first

00:08:12,920 --> 00:08:17,590
thing first thing that we noted was

00:08:15,080 --> 00:08:20,900
there was migration threats that spiking

00:08:17,590 --> 00:08:22,910
meaning some kind of migration is

00:08:20,900 --> 00:08:26,060
happening like no a new process

00:08:22,910 --> 00:08:28,250
migration is happening so this is the

00:08:26,060 --> 00:08:31,610
thing that we have we have understood

00:08:28,250 --> 00:08:32,990
from the fleet by default that vm spans

00:08:31,610 --> 00:08:35,390
all the all the new my nodes in the

00:08:32,990 --> 00:08:37,160
system and linux has this feature called

00:08:35,390 --> 00:08:38,930
automatic Numa balancing where it tries

00:08:37,160 --> 00:08:39,349
to keep the task and memory closer

00:08:38,930 --> 00:08:43,070
together

00:08:39,349 --> 00:08:44,780
and so it does it by two different

00:08:43,070 --> 00:08:46,610
methods one is the memory follow CPU

00:08:44,780 --> 00:08:49,730
model where the memories migrated to the

00:08:46,610 --> 00:08:52,790
node where the task is running and CPU

00:08:49,730 --> 00:08:55,010
for a memory model where the task is

00:08:52,790 --> 00:08:57,860
migrated to the node which houses most

00:08:55,010 --> 00:09:00,140
of the memory of the task either make

00:08:57,860 --> 00:09:02,780
either way it's it's like taking system

00:09:00,140 --> 00:09:05,290
time CPU time which could have been used

00:09:02,780 --> 00:09:08,050
by the VMS and it turns out to be steal

00:09:05,290 --> 00:09:11,370
so they have to fix it

00:09:08,050 --> 00:09:14,440
tim the VMS to specific me my notes and

00:09:11,370 --> 00:09:16,150
disable my balancing so you pin it to a

00:09:14,440 --> 00:09:17,410
specific new my note all the regions in

00:09:16,150 --> 00:09:19,090
the hypervisor you pin it to a specific

00:09:17,410 --> 00:09:21,040
human note and you disable the pneuma

00:09:19,090 --> 00:09:22,510
balancing there is no more migration

00:09:21,040 --> 00:09:26,230
happening and theoretically the steel

00:09:22,510 --> 00:09:30,400
should come down and yes the steel did

00:09:26,230 --> 00:09:30,850
come down but as you can see the larger

00:09:30,400 --> 00:09:34,630
vm's

00:09:30,850 --> 00:09:38,260
the steel came down from 20 to 4% still

00:09:34,630 --> 00:09:41,020
there is steel why is it happening that

00:09:38,260 --> 00:09:42,640
led us to the understanding and

00:09:41,020 --> 00:09:46,690
investigation of this thing called

00:09:42,640 --> 00:09:49,780
process grouping by process grouping I

00:09:46,690 --> 00:09:53,590
mean Linux has a feature where it tries

00:09:49,780 --> 00:09:56,800
to group related processes together to

00:09:53,590 --> 00:09:58,870
allocate resources to them so there are

00:09:56,800 --> 00:10:00,790
multiple ways by which the kernel does

00:09:58,870 --> 00:10:05,560
that one is the C groups there is thing

00:10:00,790 --> 00:10:09,100
called CPUC groups so in lippert by

00:10:05,560 --> 00:10:12,400
default it creates one CPUC group per VM

00:10:09,100 --> 00:10:15,940
and the shares are allocated equal to

00:10:12,400 --> 00:10:18,400
all the secrets so and without changing

00:10:15,940 --> 00:10:20,740
any any default parameters so I will

00:10:18,400 --> 00:10:23,740
explain it with a small small example so

00:10:20,740 --> 00:10:25,930
consider a eight CPU hypervisor eight

00:10:23,740 --> 00:10:28,120
CPU means 800 percentage of CPU time and

00:10:25,930 --> 00:10:30,580
you have three bm's here one is the

00:10:28,120 --> 00:10:33,460
large VM with four V CPUs and there are

00:10:30,580 --> 00:10:35,830
two smaller VMs with two V CPUs so

00:10:33,460 --> 00:10:38,770
labert creates three separate C groups

00:10:35,830 --> 00:10:41,140
one for each VM and shares are allocated

00:10:38,770 --> 00:10:43,240
equally so one by third of the share is

00:10:41,140 --> 00:10:44,890
given to each of these VMs every VM get

00:10:43,240 --> 00:10:48,130
two hundred and sixty-six percent of the

00:10:44,890 --> 00:10:49,840
share for the smaller VMs out of the two

00:10:48,130 --> 00:10:52,990
hundred and sixty-six percentage each

00:10:49,840 --> 00:10:54,730
view CPU gets half the share so it's

00:10:52,990 --> 00:10:57,370
like surplus its hundred percent and

00:10:54,730 --> 00:11:00,550
there is theoretically no steal but on

00:10:57,370 --> 00:11:02,830
the larger VM the 266 percentage is

00:11:00,550 --> 00:11:07,780
divided among four V CPU threads which

00:11:02,830 --> 00:11:12,070
is like 66 percent age and and the 34

00:11:07,780 --> 00:11:14,690
percent days turns out to be steal so

00:11:12,070 --> 00:11:17,450
grouping is a problem so how do we solve

00:11:14,690 --> 00:11:20,120
this the the immediate thing that we

00:11:17,450 --> 00:11:23,270
tried to do was allocated shares equally

00:11:20,120 --> 00:11:24,800
sorry accordingly so the larger VMs get

00:11:23,270 --> 00:11:27,230
larger shares and smaller VM gets

00:11:24,800 --> 00:11:31,130
smaller shares that's the theoretically

00:11:27,230 --> 00:11:33,140
supposed to work but it didn't work as

00:11:31,130 --> 00:11:36,740
we expected in our in our fleet because

00:11:33,140 --> 00:11:38,540
see groups were not scaling Lippard by

00:11:36,740 --> 00:11:41,360
default creates a deep hierarchy of a

00:11:38,540 --> 00:11:43,370
see group and as a number of processes

00:11:41,360 --> 00:11:45,560
in the system increases it was not

00:11:43,370 --> 00:11:48,250
scaling so we were not seeing the result

00:11:45,560 --> 00:11:50,900
that we were expecting

00:11:48,250 --> 00:11:52,760
second thing is like let's disable the C

00:11:50,900 --> 00:11:54,890
group as a whole so when you disable the

00:11:52,760 --> 00:11:56,780
C group as a whole every see every task

00:11:54,890 --> 00:12:00,260
is considered equal by the scheduler and

00:11:56,780 --> 00:12:01,820
it will be given equal shares it's also

00:12:00,260 --> 00:12:04,400
supposed to work but there were a couple

00:12:01,820 --> 00:12:06,260
of side effects one thing is we lost the

00:12:04,400 --> 00:12:08,480
capability to control the VM CPU

00:12:06,260 --> 00:12:10,370
utilization so our control plane had

00:12:08,480 --> 00:12:14,150
this feature of controlling individual

00:12:10,370 --> 00:12:16,250
virtual machines CPU utilization and it

00:12:14,150 --> 00:12:18,140
sent the request to lipfird and liberty

00:12:16,250 --> 00:12:21,980
used the CPUC group to actually control

00:12:18,140 --> 00:12:24,260
the cpu resource the utilization and we

00:12:21,980 --> 00:12:26,300
completely lost the capability the less

00:12:24,260 --> 00:12:27,890
apparent thing was once you disable the

00:12:26,300 --> 00:12:30,380
C group another feature called Auto

00:12:27,890 --> 00:12:32,330
Group kicks in Auto group is another

00:12:30,380 --> 00:12:34,750
implicit linux feature where it tries to

00:12:32,330 --> 00:12:38,090
group processes based on the process

00:12:34,750 --> 00:12:41,180
properties like user who started the

00:12:38,090 --> 00:12:42,650
process session ID etc so once you

00:12:41,180 --> 00:12:44,540
disable the C group the Auto Group

00:12:42,650 --> 00:12:46,850
feature kicks in it tries to group all

00:12:44,540 --> 00:12:48,500
the tasks in the VM into one one C group

00:12:46,850 --> 00:12:50,750
and we are back to square one

00:12:48,500 --> 00:12:54,560
same same issue larger VMs get penalized

00:12:50,750 --> 00:12:56,690
so let's disable autograph and that also

00:12:54,560 --> 00:12:59,360
did not help because once you disable

00:12:56,690 --> 00:13:01,910
auto group all the existing processes

00:12:59,360 --> 00:13:05,420
will still be in the auto group only the

00:13:01,910 --> 00:13:07,880
new process gets yes like starts without

00:13:05,420 --> 00:13:09,530
the zero without the auto group so that

00:13:07,880 --> 00:13:12,830
also did not help for the running viens

00:13:09,530 --> 00:13:14,240
in the system then this is a working

00:13:12,830 --> 00:13:17,630
solution that we came up with

00:13:14,240 --> 00:13:20,630
it's called per host C group labert by

00:13:17,630 --> 00:13:22,550
default creates per VM C groups but now

00:13:20,630 --> 00:13:24,819
we have a single seat loop in the system

00:13:22,550 --> 00:13:25,720
which houses all the tasks of

00:13:24,819 --> 00:13:28,629
all the virtual machines in the

00:13:25,720 --> 00:13:31,809
hypervisor and we have modified Lippert

00:13:28,629 --> 00:13:33,669
to actually create a percocet group and

00:13:31,809 --> 00:13:35,609
move all the all the virtual machines

00:13:33,669 --> 00:13:37,779
into that C group based on it to another

00:13:35,609 --> 00:13:40,749
but we lost the capability of

00:13:37,779 --> 00:13:43,720
controlling this virtual machine CPU

00:13:40,749 --> 00:13:45,279
utilization so we the our modified

00:13:43,720 --> 00:13:47,739
Lippert what it does this minute record

00:13:45,279 --> 00:13:50,739
when it receives a request from the

00:13:47,739 --> 00:13:52,959
control plane it transparently moves the

00:13:50,739 --> 00:13:54,850
VM that has to be controlled out of the

00:13:52,959 --> 00:13:56,859
Big C group and puts it in a smaller C

00:13:54,850 --> 00:13:59,139
group and a place all the parameters

00:13:56,859 --> 00:14:01,779
over there and that way you just control

00:13:59,139 --> 00:14:03,639
the VM and when you want to revert it

00:14:01,779 --> 00:14:05,649
back the control plane sensor request

00:14:03,639 --> 00:14:08,319
and labert transparently moves it back

00:14:05,649 --> 00:14:12,399
to the bigger C group and it becomes the

00:14:08,319 --> 00:14:16,179
part of the big single C group so this

00:14:12,399 --> 00:14:18,069
works and the steel comes down to give

00:14:16,179 --> 00:14:19,779
you a pictorial explained explanation

00:14:18,069 --> 00:14:22,529
yeah I'll take the same example as

00:14:19,779 --> 00:14:25,600
before so you have a HCP hypervisor with

00:14:22,529 --> 00:14:28,569
800 percentage of CPU time but now

00:14:25,600 --> 00:14:30,729
there's a single C group so the total

00:14:28,569 --> 00:14:33,639
shares will be divided by all the all

00:14:30,729 --> 00:14:35,409
the CCP's equally or so 1 by 8th of the

00:14:33,639 --> 00:14:38,529
share will be received by all the older

00:14:35,409 --> 00:14:40,629
B CPUs and everyone gets hundred percent

00:14:38,529 --> 00:14:45,279
CPU and theoretically there is no steal

00:14:40,629 --> 00:14:48,100
and this is the screen shot that we took

00:14:45,279 --> 00:14:50,529
from the same synthetic workload that we

00:14:48,100 --> 00:14:52,689
had with all the fixes that I have

00:14:50,529 --> 00:14:54,220
mentioned I applied in the system the

00:14:52,689 --> 00:14:58,029
steel came down considerably

00:14:54,220 --> 00:15:00,309
so if you see the larger VMs have very

00:14:58,029 --> 00:15:04,029
very less steel now and the steel steel

00:15:00,309 --> 00:15:06,069
is more or less equal now but still the

00:15:04,029 --> 00:15:08,589
steel is not down to zero even though

00:15:06,069 --> 00:15:10,869
the system had idle resources so I'll be

00:15:08,589 --> 00:15:13,720
handing it over to do to explain the

00:15:10,869 --> 00:15:15,519
last phenomena and also the practical

00:15:13,720 --> 00:15:19,559
implementation of all the strategies

00:15:15,519 --> 00:15:19,559
that I mentioned before Oh Julia

00:15:21,220 --> 00:15:25,730
so it's even if when you've mentioned

00:15:23,870 --> 00:15:31,720
they still doesn't get all the way down

00:15:25,730 --> 00:15:34,970
to zero but a other remaining suit is

00:15:31,720 --> 00:15:39,680
attributable to two idle games that run

00:15:34,970 --> 00:15:41,779
on really busy hypervisors so under our

00:15:39,680 --> 00:15:43,580
definitions previously it doesn't

00:15:41,779 --> 00:15:46,640
translate into performance problems

00:15:43,580 --> 00:15:50,440
because they they it's a it's a CFS

00:15:46,640 --> 00:15:50,440
feature to deprioritize

00:15:50,560 --> 00:16:01,760
idle idle tasks on country if there is

00:15:55,610 --> 00:16:03,650
other other work to be done so our

00:16:01,760 --> 00:16:05,300
current understanding is that it doesn't

00:16:03,650 --> 00:16:11,360
translate into the forum problem so we

00:16:05,300 --> 00:16:14,680
we stopped our investigation there a now

00:16:11,360 --> 00:16:18,670
let's talk about about implementation

00:16:14,680 --> 00:16:22,190
even you've mentioned several slides ago

00:16:18,670 --> 00:16:23,630
we fool in pneumo partition our systems

00:16:22,190 --> 00:16:27,110
essentially would take all the virtual

00:16:23,630 --> 00:16:29,600
machines on the hypervisor we divided

00:16:27,110 --> 00:16:32,780
them into two roughly equal piles and

00:16:29,600 --> 00:16:37,000
each pile goes on on HTTP only on a CPU

00:16:32,780 --> 00:16:37,000
socket with its corresponding nominal a

00:16:37,630 --> 00:16:45,620
the logical place to implement this this

00:16:42,860 --> 00:16:51,470
logic would be in the OS task scheduler

00:16:45,620 --> 00:16:52,820
however it's certain hard and we can and

00:16:51,470 --> 00:16:55,270
we can get away with a much easier

00:16:52,820 --> 00:16:55,270
solution

00:16:59,140 --> 00:17:06,920
we have a user space team on the front

00:17:02,420 --> 00:17:09,980
on the on the hypervisor and to manage

00:17:06,920 --> 00:17:11,710
virtual resources with regard to

00:17:09,980 --> 00:17:14,510
physical assistance essentially it

00:17:11,710 --> 00:17:17,930
manages the Devi CPU and keema threads

00:17:14,510 --> 00:17:24,530
and such and pin them to to individual

00:17:17,930 --> 00:17:29,240
CPUs and new models the logic of momo

00:17:24,530 --> 00:17:36,640
partitioning is implemented there two

00:17:29,240 --> 00:17:39,080
piles that I just just explained a they

00:17:36,640 --> 00:17:41,120
the reason that we can get away with

00:17:39,080 --> 00:17:42,920
with implementing it in user space is

00:17:41,120 --> 00:17:46,930
because we don't really need a drift

00:17:42,920 --> 00:17:53,750
level resolution they realistically

00:17:46,930 --> 00:17:56,990
things change the resolution of a ours

00:17:53,750 --> 00:17:59,000
if you were talking about flow of VM

00:17:56,990 --> 00:18:01,250
creates and destroys or minutes occurs

00:17:59,000 --> 00:18:04,670
we are looking at it workload and we are

00:18:01,250 --> 00:18:06,830
looking at at the workload so drift

00:18:04,670 --> 00:18:08,090
level resolution if it's a necessary and

00:18:06,830 --> 00:18:11,020
we can get away with with the

00:18:08,090 --> 00:18:11,020
implementing user space

00:18:11,320 --> 00:18:15,620
several things that the doctor puts does

00:18:13,490 --> 00:18:17,510
first of all nono partitioning I just

00:18:15,620 --> 00:18:20,780
talked about it the other thing is that

00:18:17,510 --> 00:18:22,520
since we do know the normal partitioning

00:18:20,780 --> 00:18:27,410
we lose the the load balancing

00:18:22,520 --> 00:18:30,170
capabilities of of Linux scheduler one

00:18:27,410 --> 00:18:33,050
of the CPU socket becomes busy when the

00:18:30,170 --> 00:18:36,890
other the second one might remain

00:18:33,050 --> 00:18:42,920
relatively relatively idle in this case

00:18:36,890 --> 00:18:43,779
we migrate virtual machines between CPU

00:18:42,920 --> 00:18:48,070
sockets

00:18:43,779 --> 00:18:51,309
this is a huge pain point I will talk

00:18:48,070 --> 00:18:55,450
about in the in the following slide the

00:18:51,309 --> 00:18:58,859
other responsibilities of octopus is we

00:18:55,450 --> 00:19:05,639
have plans that are so-called

00:18:58,859 --> 00:19:11,879
CPU optimized offering essentially you

00:19:05,639 --> 00:19:15,609
solution they receive 100% of of sip of

00:19:11,879 --> 00:19:18,729
of CPU 100 cents of the time and it's

00:19:15,609 --> 00:19:21,249
it's implemented by by octopus by by

00:19:18,729 --> 00:19:28,899
pinning the TV CPUs to see peels

00:19:21,249 --> 00:19:34,989
one-to-one now about the Numa migrations

00:19:28,899 --> 00:19:36,969
a the primary pain point with these new

00:19:34,989 --> 00:19:42,239
migrations if the process one of the

00:19:36,969 --> 00:19:42,239
nominal might become older committed a

00:19:43,619 --> 00:19:47,859
the way that we we handle it is that

00:19:46,059 --> 00:19:54,129
with during the Numa migration which

00:19:47,859 --> 00:19:56,169
apparently is a enable swap essentially

00:19:54,129 --> 00:19:57,999
before the migration we enable thought

00:19:56,169 --> 00:20:00,039
we would do to the management and then

00:19:57,999 --> 00:20:02,649
we swap off in order to drain the RAM

00:20:00,039 --> 00:20:10,019
back into a sorry drain the swamp back

00:20:02,649 --> 00:20:10,019
into physical RAM a this is

00:20:11,240 --> 00:20:19,560
the the it might sound like like a hack

00:20:16,950 --> 00:20:21,450
but we are using here the fact that the

00:20:19,560 --> 00:20:23,090
process of swapping off is not Mumbai

00:20:21,450 --> 00:20:25,440
where essentially if you're moving

00:20:23,090 --> 00:20:29,790
moving a virtual machine from node 0 to

00:20:25,440 --> 00:20:32,820
node 1 a and there is not enough room

00:20:29,790 --> 00:20:36,810
and then when you swap off some of the

00:20:32,820 --> 00:20:40,380
of the the RAM that spilled over into in

00:20:36,810 --> 00:20:44,310
in to swap might end up back in in in in

00:20:40,380 --> 00:20:49,530
the source node and in node 0 it might

00:20:44,310 --> 00:20:51,210
sound as a as as a bad thing but the

00:20:49,530 --> 00:20:54,690
progress was made the essentially next

00:20:51,210 --> 00:20:58,460
iteration of focus will move more more

00:20:54,690 --> 00:21:06,330
ramen and the the process converges that

00:20:58,460 --> 00:21:09,240
that was one big pain point a the other

00:21:06,330 --> 00:21:12,090
thing is a very rare situation of home

00:21:09,240 --> 00:21:15,540
killed during Numa migrations it's

00:21:12,090 --> 00:21:17,640
essentially when when you move a virtual

00:21:15,540 --> 00:21:19,860
machine from again from node 0 turn to

00:21:17,640 --> 00:21:24,050
node 1 and node 1 there is another DM

00:21:19,860 --> 00:21:24,050
that allocates Ram really aggressively

00:21:25,880 --> 00:21:33,630
you might might not swap as as fast as

00:21:30,360 --> 00:21:35,370
you might want any and and the the the

00:21:33,630 --> 00:21:41,430
virtual machine might might get and get

00:21:35,370 --> 00:21:44,090
em killed a slight variation of this

00:21:41,430 --> 00:21:47,400
problem is during the swap of faith a

00:21:44,090 --> 00:21:51,260
when swap is already disabled then it

00:21:47,400 --> 00:21:51,260
might happen just because there is no

00:21:52,820 --> 00:21:59,120
this problem is made worse by the fact

00:21:55,710 --> 00:22:02,250
that the swap of is is not not efficient

00:21:59,120 --> 00:22:06,450
in in most extreme cases with thoughts

00:22:02,250 --> 00:22:08,250
were both taking hours to place and it's

00:22:06,450 --> 00:22:10,860
obviously completely unacceptable for

00:22:08,250 --> 00:22:12,320
our for for for the flow I just

00:22:10,860 --> 00:22:16,460
described because you can start a new

00:22:12,320 --> 00:22:19,370
migration before the previous ended and

00:22:16,460 --> 00:22:22,680
also again as I mentioned swap of might

00:22:19,370 --> 00:22:27,180
expose us to an increased risk of

00:22:22,680 --> 00:22:32,190
looming so so taking this was out of the

00:22:27,180 --> 00:22:33,780
question we revived a page that was that

00:22:32,190 --> 00:22:37,080
was posted well came out several years

00:22:33,780 --> 00:22:48,420
ago and now so hopefully is much more

00:22:37,080 --> 00:22:50,460
efficient another thing that we are

00:22:48,420 --> 00:22:52,860
working one it's very much work in

00:22:50,460 --> 00:22:56,960
progress is to allow mobile migrations

00:22:52,860 --> 00:23:04,020
soil gracefully and as opposed to to me

00:22:56,960 --> 00:23:08,550
and this is the work in progress when we

00:23:04,020 --> 00:23:11,390
get operations will surely posted for

00:23:08,550 --> 00:23:11,390
review and inclusion

00:23:43,210 --> 00:23:51,160
we don't yeah we don't we don't have

00:23:49,460 --> 00:23:53,840
bigger vm's

00:23:51,160 --> 00:23:57,880
that's bad that need to spend to two

00:23:53,840 --> 00:23:57,880
different new my notes you know we don't

00:24:31,429 --> 00:24:35,419
we haven't we haven't looked into that

00:24:33,559 --> 00:24:37,789
kind of things yet because most of the

00:24:35,419 --> 00:24:40,340
steel that we saw saw in the fleet is on

00:24:37,789 --> 00:24:42,559
very idle droplets so so once we have

00:24:40,340 --> 00:24:44,419
all these optimizations all the busy to

00:24:42,559 --> 00:24:46,879
be CV virtual machines are really happy

00:24:44,419 --> 00:24:48,769
so so but but there is still more work

00:24:46,879 --> 00:24:52,239
to do this is this is a kind of ongoing

00:24:48,769 --> 00:24:52,239
practice yeah thanks for the point

00:25:03,260 --> 00:25:10,370
so we started with top vent then went to

00:25:07,550 --> 00:25:13,430
Perth so perf was really helpful in

00:25:10,370 --> 00:25:15,410
seeing the contact switches and the via

00:25:13,430 --> 00:25:18,170
means and the VM outs and what's

00:25:15,410 --> 00:25:20,120
happening once a vm once a vm thread

00:25:18,170 --> 00:25:22,480
becomes runnable what other things were

00:25:20,120 --> 00:25:25,310
happening in between because

00:25:22,480 --> 00:25:29,090
before it actually becomes running so

00:25:25,310 --> 00:25:32,120
perforce really helpful for that and for

00:25:29,090 --> 00:25:35,060
the numa numa stuff we didn't we didn't

00:25:32,120 --> 00:25:36,350
we didn't use any like much much of the

00:25:35,060 --> 00:25:37,640
things we are really apparent so we do

00:25:36,350 --> 00:25:38,810
not use any debugging techniques to

00:25:37,640 --> 00:25:41,230
actually actually find out what's

00:25:38,810 --> 00:25:41,230
happening

00:25:53,860 --> 00:26:03,850
sorry Kenny can you repeat it so yeah

00:26:01,520 --> 00:26:06,440
before balancing what used to happen is

00:26:03,850 --> 00:26:08,480
the work loaded the hypervisor is really

00:26:06,440 --> 00:26:11,000
dynamic so there are there are

00:26:08,480 --> 00:26:12,860
middleware that actually comes up and

00:26:11,000 --> 00:26:15,049
that's a lot of things to spawn the VM

00:26:12,860 --> 00:26:17,840
and the VM becomes spikes at some time

00:26:15,049 --> 00:26:20,029
so the memory becomes really unbalanced

00:26:17,840 --> 00:26:21,649
and you next constantly tries to move it

00:26:20,029 --> 00:26:23,149
because sometimes the memory moves to

00:26:21,649 --> 00:26:24,950
another node and then it tries to move

00:26:23,149 --> 00:26:26,630
the process to that node and sometimes

00:26:24,950 --> 00:26:28,940
the other thing happens because of the

00:26:26,630 --> 00:26:30,260
dynamic nature of the hypervisor so it's

00:26:28,940 --> 00:26:32,120
not only the VMS that is running its

00:26:30,260 --> 00:26:33,440
route of secondary software also running

00:26:32,120 --> 00:26:36,320
on the system that makes it really

00:26:33,440 --> 00:26:37,970
dynamic so we thought we thought

00:26:36,320 --> 00:26:39,440
actually pinning to an uma node

00:26:37,970 --> 00:26:41,029
we constantly saw this migration

00:26:39,440 --> 00:26:53,360
happening and that was accounting for

00:26:41,029 --> 00:26:54,860
the steal yes yeah so we we account for

00:26:53,360 --> 00:26:57,649
the utilization of the virtual machines

00:26:54,860 --> 00:26:59,960
and then decide on which bucket it

00:26:57,649 --> 00:27:04,179
should fall and then balance it using

00:26:59,960 --> 00:27:04,179
using the users uses diamond

00:27:20,380 --> 00:27:27,020
hi so I was just trying to like figure

00:27:24,380 --> 00:27:30,350
out whether any anything can be done to

00:27:27,020 --> 00:27:32,510
make you know things work out of the box

00:27:30,350 --> 00:27:34,309
a little bit better so if I understand

00:27:32,510 --> 00:27:36,830
correctly your issue with Noma was

00:27:34,309 --> 00:27:39,049
really it was just moving processes

00:27:36,830 --> 00:27:42,320
around too much it was moving memory

00:27:39,049 --> 00:27:44,240
around too much so like maybe a knob

00:27:42,320 --> 00:27:47,299
that kind of makes it a little bit

00:27:44,240 --> 00:27:53,600
aggressive would have made a difference

00:27:47,299 --> 00:27:58,040
for you you know so what on Omar think

00:27:53,600 --> 00:28:15,290
that that just just plays with stats and

00:27:58,040 --> 00:28:29,559
rest of their process behavior it

00:28:15,290 --> 00:28:29,559
doesn't necessarily a lot of rushing

00:28:31,179 --> 00:28:38,299
yeah so like if you maybe just just make

00:28:36,110 --> 00:28:39,950
it a bit less aggressive that was so

00:28:38,299 --> 00:28:41,540
maybe help one another thing is the

00:28:39,950 --> 00:28:43,070
dynamic city of the virtual machine

00:28:41,540 --> 00:28:46,280
sometimes it spikes sometimes it doesn't

00:28:43,070 --> 00:28:48,500
spike so once once we have it settled

00:28:46,280 --> 00:28:50,540
into two different Numa nodes sometimes

00:28:48,500 --> 00:28:53,090
one new monoid gets over committed

00:28:50,540 --> 00:28:55,520
because because of the CP utilization of

00:28:53,090 --> 00:28:57,679
all the VMS on that node at the same

00:28:55,520 --> 00:28:59,419
time the other node might be idle so

00:28:57,679 --> 00:29:01,720
even if even if you make it kind of

00:28:59,419 --> 00:29:04,130
static and then make it less aggressive

00:29:01,720 --> 00:29:06,230
VM should be facing performance issues

00:29:04,130 --> 00:29:08,330
so we need to have a constant way of

00:29:06,230 --> 00:29:10,250
monitoring whether the node becomes over

00:29:08,330 --> 00:29:15,400
committed and then balanced based on

00:29:10,250 --> 00:29:15,400
based on what is happening okay

00:29:15,429 --> 00:29:21,230
maybe and second question is so you

00:29:19,190 --> 00:29:24,260
mentioned scalability issues with VC

00:29:21,230 --> 00:29:27,230
groups is that solved with C groups we -

00:29:24,260 --> 00:29:31,009
that's what it was supposed to address

00:29:27,230 --> 00:29:32,929
so is that also visit way to we do is

00:29:31,009 --> 00:29:34,909
like that's that's a good question we

00:29:32,929 --> 00:29:36,769
too we are currently testing it out to

00:29:34,909 --> 00:29:39,379
see what how much of a difference is

00:29:36,769 --> 00:29:41,450
there but we don't have any any real

00:29:39,379 --> 00:29:47,350
concrete results to actually show it

00:29:41,450 --> 00:29:47,350
here but yeah that that's it yeah

00:29:48,370 --> 00:29:58,600
any other questions okay

00:29:55,070 --> 00:29:58,600

YouTube URL: https://www.youtube.com/watch?v=FruPyQ0DRWc


