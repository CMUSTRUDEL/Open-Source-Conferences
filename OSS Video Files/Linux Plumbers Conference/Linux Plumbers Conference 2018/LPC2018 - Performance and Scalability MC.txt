Title: LPC2018 - Performance and Scalability MC
Publication date: 2018-11-28
Playlist: Linux Plumbers Conference 2018
Description: 
	https://linuxplumbersconf.org/event/2/sessions/24/#20181114

Proposed topics for this microconference include
 optimizations for mmap_sem range locking
 clearly defining what mmap_sem protects
 scalability of page allocation, zone-lock, and lru_lock
 swap scalability
 variable hotpatching (self-modifying code) multithreading kernel work
 improved workqueue interaction with CPU hotplug events
 proper (and optimized) cgroup accounting for workqueue threads
 automatically scaling the threshold values for per-CPU counters

We are also accepting additional topics. In particular, we are curious to hear about real-world bottlenecks that people are running into, as well as scalability work-in-progress that needs face-to-face discussion.
Captions: 
	00:00:05,980 --> 00:00:09,590
one I would like to thank a couple of

00:00:08,270 --> 00:00:15,110
people involved in making this event

00:00:09,590 --> 00:00:18,460
possible Paul McKenney Dave Hansen David

00:00:15,110 --> 00:00:21,560
Laura Barroso and Dhaval Gianni all

00:00:18,460 --> 00:00:23,839
William helped us it was the first time

00:00:21,560 --> 00:00:25,460
we've done this actually Iko Iko

00:00:23,839 --> 00:00:30,609
organized this event with jung hwan from

00:00:25,460 --> 00:00:30,609
intel and passionate a passion from

00:00:31,450 --> 00:00:36,920
Microsoft a little bit of a stutter

00:00:34,360 --> 00:00:39,199
anyway also I want to remind everybody

00:00:36,920 --> 00:00:41,500
that this is an interactive event and to

00:00:39,199 --> 00:00:43,670
not be shy with your questions and ideas

00:00:41,500 --> 00:00:44,359
with that I would like to introduce our

00:00:43,670 --> 00:00:46,129
first speaker

00:00:44,359 --> 00:00:47,750
Tim Chen from Intel he'll be talking

00:00:46,129 --> 00:01:02,390
about scheduler task accounting for C

00:00:47,750 --> 00:01:05,420
groups so a few years ago like I think

00:01:02,390 --> 00:01:09,740
was three years ago we have a benchmark

00:01:05,420 --> 00:01:11,090
teams called that does database

00:01:09,740 --> 00:01:15,770
benchmark sex

00:01:11,090 --> 00:01:19,700
PVCC and then II they came to me and and

00:01:15,770 --> 00:01:22,070
complains that on the latest kernels

00:01:19,700 --> 00:01:23,800
that when they start testings on for

00:01:22,070 --> 00:01:29,720
socket haswell's and all of a sudden

00:01:23,800 --> 00:01:31,820
they see it a person's depredations in

00:01:29,720 --> 00:01:37,130
performance that compares to the numbers

00:01:31,820 --> 00:01:39,440
that they have seen before and so so we

00:01:37,130 --> 00:01:42,080
start looking like what happens that way

00:01:39,440 --> 00:01:46,250
why don't the new kernels maybe that

00:01:42,080 --> 00:01:49,160
happens so it turns out that this

00:01:46,250 --> 00:01:53,450
becomes the default options configs get

00:01:49,160 --> 00:01:56,630
other groups and so they even though

00:01:53,450 --> 00:02:00,140
they are not purposely using C group but

00:01:56,630 --> 00:02:05,320
with scared Auto Group so automatically

00:02:00,140 --> 00:02:09,310
say that the word close now gets into

00:02:05,320 --> 00:02:14,320
cgroups yeah so we found that if we turn

00:02:09,310 --> 00:02:16,840
out like this this to config hey or just

00:02:14,320 --> 00:02:22,150
disable the scat other groups they they

00:02:16,840 --> 00:02:24,310
get back the throughput loss and so

00:02:22,150 --> 00:02:26,950
forth all cans they lose that eight

00:02:24,310 --> 00:02:29,430
percent and a socket is even worse they

00:02:26,950 --> 00:02:33,940
lose they nineteen percent of the

00:02:29,430 --> 00:02:36,880
proponents and we did we did a recent

00:02:33,940 --> 00:02:40,360
check on on this particular workloads as

00:02:36,880 --> 00:02:42,910
like a four sockets twenty eight cores

00:02:40,360 --> 00:02:47,320
two sockets at canon like systems and

00:02:42,910 --> 00:02:49,620
things get a lower that compares to the

00:02:47,320 --> 00:02:56,739
four socket house well noise becomes a

00:02:49,620 --> 00:02:59,680
12% deputation so where does this what

00:02:56,739 --> 00:03:03,820
causes this performance degradation when

00:02:59,680 --> 00:03:06,250
c groups get used so we look at the

00:03:03,820 --> 00:03:09,580
profiles that when one thing jumped out

00:03:06,250 --> 00:03:14,730
is this functions call update vlog

00:03:09,580 --> 00:03:19,299
averages and if you disable see groups

00:03:14,730 --> 00:03:22,530
this particular functions that come

00:03:19,299 --> 00:03:26,860
disappears from from the profile and is

00:03:22,530 --> 00:03:29,980
it's really high five percents top of

00:03:26,860 --> 00:03:36,190
everything say yeah

00:03:29,980 --> 00:03:40,230
and so what so yeah so here's a snapshot

00:03:36,190 --> 00:03:42,790
of this dysfunctions that up

00:03:40,230 --> 00:03:45,310
and you'll notice that there's a for

00:03:42,790 --> 00:03:48,189
loop that goes through there all the

00:03:45,310 --> 00:03:50,230
active C groups but actually we don't

00:03:48,189 --> 00:03:53,379
have a whole lot of C groups that

00:03:50,230 --> 00:03:55,750
running time for for for these benchmark

00:03:53,379 --> 00:03:59,530
teams are just using probably just one

00:03:55,750 --> 00:04:01,989
one C groups what active C groups they

00:03:59,530 --> 00:04:06,069
pour it for them so this for loop

00:04:01,989 --> 00:04:10,920
probably just go through once and then

00:04:06,069 --> 00:04:13,720
but this functions so it C so that's a

00:04:10,920 --> 00:04:16,840
there's a each check that C say whether

00:04:13,720 --> 00:04:19,479
it's on on this particular CPU there's a

00:04:16,840 --> 00:04:21,699
there's a load change and if there's a

00:04:19,479 --> 00:04:23,760
low change that you you go and call this

00:04:21,699 --> 00:04:30,669
function update task group load average

00:04:23,760 --> 00:04:33,099
and the the the task groups is every CC

00:04:30,669 --> 00:04:36,610
groups a task group so so you were

00:04:33,099 --> 00:04:39,810
actually update so what happens is this

00:04:36,610 --> 00:04:45,180
task group load average is the global

00:04:39,810 --> 00:04:48,660
counters and then you have like for this

00:04:45,180 --> 00:04:51,669
benchmark teams that they have like

00:04:48,660 --> 00:04:56,169
about twelve twelve hundred rats all

00:04:51,669 --> 00:05:00,490
runnings die on on the CPU so other CPUs

00:04:56,169 --> 00:05:04,720
are very active I constantly doing this

00:05:00,490 --> 00:05:07,750
up update and constantly hitting this

00:05:04,720 --> 00:05:10,449
function so so this task group load

00:05:07,750 --> 00:05:12,580
average counter will get updated very

00:05:10,449 --> 00:05:15,220
frequently and because it's it's a

00:05:12,580 --> 00:05:19,599
global counter so the the update is done

00:05:15,220 --> 00:05:22,870
using atomic atomic add operations

00:05:19,599 --> 00:05:27,039
though and it's a for socket system so

00:05:22,870 --> 00:05:32,050
you have like the cache line Mouse

00:05:27,039 --> 00:05:35,530
bouncing all over is then so it is

00:05:32,050 --> 00:05:36,680
pretty pretty expensive the output for

00:05:35,530 --> 00:05:41,510
this mesh mark

00:05:36,680 --> 00:05:44,070
so here's here's a problem yeah and

00:05:41,510 --> 00:05:47,660
because they pour for this benchmark

00:05:44,070 --> 00:05:54,180
teams that they were close that they're

00:05:47,660 --> 00:05:56,010
the task tracks it has a lot of blocking

00:05:54,180 --> 00:05:58,290
said because they need to interact with

00:05:56,010 --> 00:05:59,520
other stuff so they block and wake up

00:05:58,290 --> 00:06:03,780
wake up

00:05:59,520 --> 00:06:06,030
so there are a lot of calls - I don't

00:06:03,780 --> 00:06:09,630
know balance that that triggers this

00:06:06,030 --> 00:06:13,800
update blocked averages and this happens

00:06:09,630 --> 00:06:16,350
they very often for them and I think the

00:06:13,800 --> 00:06:18,480
problem will get worse because remember

00:06:16,350 --> 00:06:21,000
there are four loops that we we see just

00:06:18,480 --> 00:06:23,460
know it's just for it you actually loop

00:06:21,000 --> 00:06:27,720
through that other order see groups that

00:06:23,460 --> 00:06:30,990
on on this particular CPU or the active

00:06:27,720 --> 00:06:33,660
single forum so if you have like more

00:06:30,990 --> 00:06:38,220
see group hierarchies then you might be

00:06:33,660 --> 00:06:39,840
banging on more counters and you might

00:06:38,220 --> 00:06:42,450
be updating more coverage so I think the

00:06:39,840 --> 00:06:45,150
the problem will we'll get words that if

00:06:42,450 --> 00:06:48,210
you have several layers of C group

00:06:45,150 --> 00:06:51,900
hierarchies but as it as as it is that

00:06:48,210 --> 00:06:56,210
even with that one C group level say

00:06:51,900 --> 00:07:01,680
that the problem is pretty bad already

00:06:56,210 --> 00:07:05,280
so I don't have a very good solution but

00:07:01,680 --> 00:07:08,750
this is something that is it's a very

00:07:05,280 --> 00:07:11,670
naive things that that I thought about

00:07:08,750 --> 00:07:13,790
but I think Peter will have a very hard

00:07:11,670 --> 00:07:13,790
time

00:07:14,220 --> 00:07:30,790
right we yeah we we try and we we tried

00:07:24,010 --> 00:07:37,450
that I give you a patch two years ago

00:07:30,790 --> 00:07:39,340
that splits it into proud stuff I never

00:07:37,450 --> 00:07:46,030
heard back from that and then I forgot

00:07:39,340 --> 00:07:52,060
about it yeah I gave it to the T BCC

00:07:46,030 --> 00:07:57,670
teams head I yeah I so this is this is

00:07:52,060 --> 00:08:01,150
something that they I think is they have

00:07:57,670 --> 00:08:08,860
to they have to try and try out I think

00:08:01,150 --> 00:08:10,600
I we we did it once and so you don't

00:08:08,860 --> 00:08:12,400
have to aggregate it back into a global

00:08:10,600 --> 00:08:13,690
one you can right keep it split out and

00:08:12,400 --> 00:08:15,940
then whenever you need to value just

00:08:13,690 --> 00:08:22,210
some old four or five or however many

00:08:15,940 --> 00:08:24,070
things you have I mean we'll just have

00:08:22,210 --> 00:08:26,950
them in separate cache lines of course I

00:08:24,070 --> 00:08:29,770
think the patch did that I remember like

00:08:26,950 --> 00:08:34,960
I said two years ago yeah so so we'll

00:08:29,770 --> 00:08:42,640
have to get back and and do that do do

00:08:34,960 --> 00:08:46,390
the work yeah there yeah so this is this

00:08:42,640 --> 00:08:47,260
is something that maybe I think will

00:08:46,390 --> 00:08:50,650
work so

00:08:47,260 --> 00:08:57,640
so actually Peters like maybe it's a

00:08:50,650 --> 00:08:59,920
question so you think that with a note

00:08:57,640 --> 00:09:04,180
like something like a no counter is say

00:08:59,920 --> 00:09:07,610
this is acceptable for is there

00:09:04,180 --> 00:09:15,900
something acceptable for upstream

00:09:07,610 --> 00:09:19,110
sure if it works isn't this going to

00:09:15,900 --> 00:09:21,090
depend on the number of tasks and how

00:09:19,110 --> 00:09:23,940
fast they're switching to hotness of the

00:09:21,090 --> 00:09:32,370
atomic increment so this is just sort of

00:09:23,940 --> 00:09:36,210
delaying the problem right if you do the

00:09:32,370 --> 00:09:39,830
atomic operations inside of a level

00:09:36,210 --> 00:09:42,900
three cache domain it's relatively cheap

00:09:39,830 --> 00:09:45,570
when you cross no domains they get

00:09:42,900 --> 00:09:48,270
really expensive okay so it's a cache

00:09:45,570 --> 00:09:50,310
migration not the content not okay

00:09:48,270 --> 00:09:52,830
contention we'll just because you're

00:09:50,310 --> 00:09:55,680
staying inside a cache yeah I mean you

00:09:52,830 --> 00:09:59,280
can still see that the costs increase

00:09:55,680 --> 00:10:02,460
even inside of a node but once you do

00:09:59,280 --> 00:10:11,160
the you know domain boundary cross it it

00:10:02,460 --> 00:10:13,320
really is also I guess with the new CPUs

00:10:11,160 --> 00:10:17,400
coming out of patch needs to be updated

00:10:13,320 --> 00:10:20,570
to be per LLC and not per node yeah you

00:10:17,400 --> 00:10:20,570
want to Prakash domain

00:10:25,740 --> 00:10:34,680
I guess the other questions I have is

00:10:29,400 --> 00:10:37,380
will it be useful to there might be

00:10:34,680 --> 00:10:39,050
other is kind of similar is global

00:10:37,380 --> 00:10:43,140
counters

00:10:39,050 --> 00:10:46,670
use a lot inside the kernel so there

00:10:43,140 --> 00:10:50,340
will it be useful to have like a note

00:10:46,670 --> 00:10:55,590
per note kind of the accounted similar

00:10:50,340 --> 00:11:02,940
issues are per CPU ty infrastructure so

00:10:55,590 --> 00:11:08,900
that might be a low last memory cost

00:11:02,940 --> 00:11:08,900
leader per CPU County

00:11:13,180 --> 00:11:22,580
then it wouldn't work yeah I was just

00:11:20,690 --> 00:11:24,890
asking you know how important is it that

00:11:22,580 --> 00:11:29,300
we keep track of the load average Peter

00:11:24,890 --> 00:11:33,410
says it's important I think you need

00:11:29,300 --> 00:11:40,460
that on the low balancing so that each

00:11:33,410 --> 00:11:42,410
times you actually tries to see each

00:11:40,460 --> 00:11:45,290
time you try to low balance you actually

00:11:42,410 --> 00:11:49,180
need to update all this data yeah before

00:11:45,290 --> 00:11:49,180
you can actually balance

00:12:04,100 --> 00:12:12,730
I was also curious I have others people

00:12:07,610 --> 00:12:12,730
hit this issue's

00:12:19,740 --> 00:12:24,129
we've seen performance issues with the

00:12:22,949 --> 00:12:28,180
cgroups

00:12:24,129 --> 00:12:30,009
enforcement in this criteria as well the

00:12:28,180 --> 00:12:32,740
systems that I started looking at our

00:12:30,009 --> 00:12:34,899
single node so it's obviously not this

00:12:32,740 --> 00:12:36,369
same thing that you're seeing we see

00:12:34,899 --> 00:12:41,579
about a 3/4 percent performance

00:12:36,369 --> 00:12:44,619
regression by using the CPU controller

00:12:41,579 --> 00:12:49,089
so you actually have this you see the

00:12:44,619 --> 00:12:51,790
degradation oh yeah it's just not

00:12:49,089 --> 00:12:53,649
probably not coming from the same

00:12:51,790 --> 00:12:56,259
function that you see most of your

00:12:53,649 --> 00:12:59,429
degradation come but simply from nquing

00:12:56,259 --> 00:13:01,600
in DQ in hierarchically things like that

00:12:59,429 --> 00:13:05,249
yeah that's just a little pointer

00:13:01,600 --> 00:13:05,249
chasing that you're otherwise don't have

00:13:06,059 --> 00:13:21,160
so it might not help even it's like even

00:13:10,050 --> 00:13:22,839
to the per node counting repair for LLC

00:13:21,160 --> 00:13:24,850
would probably solve part of the problem

00:13:22,839 --> 00:13:27,389
you're seeing but probably not

00:13:24,850 --> 00:13:27,389
everything

00:13:40,040 --> 00:13:42,639
okay

00:13:49,830 --> 00:13:58,860
in that diagram you have basically had a

00:13:52,230 --> 00:14:00,690
way of no counter right that's that they

00:13:58,860 --> 00:14:05,700
have to be in separate cache line is

00:14:00,690 --> 00:14:08,160
that right it will be like four it will

00:14:05,700 --> 00:14:10,440
be separate counters for you no no I

00:14:08,160 --> 00:14:14,430
mean they will have to be separate cache

00:14:10,440 --> 00:14:17,130
right yeah that will quickly expand the

00:14:14,430 --> 00:14:20,090
size of the yeah that's that's a

00:14:17,130 --> 00:14:20,090
downside yeah

00:14:23,150 --> 00:14:28,200
so if you pursue this solution or

00:14:25,830 --> 00:14:30,060
revisit Peters patch might consider

00:14:28,200 --> 00:14:32,850
trying to abstract this into some kind

00:14:30,060 --> 00:14:34,620
of a distributed counter distributed

00:14:32,850 --> 00:14:36,330
aggregator so it could be used in other

00:14:34,620 --> 00:14:40,830
cases as well because this is a common

00:14:36,330 --> 00:14:44,790
problem yeah yeah that's that's the the

00:14:40,830 --> 00:14:48,870
thought I had maybe we should have like

00:14:44,790 --> 00:14:51,390
no calendar well I'm saying turn that

00:14:48,870 --> 00:14:53,580
into a reusable objects or reusable

00:14:51,390 --> 00:14:56,970
interface that other clients could use

00:14:53,580 --> 00:14:59,100
it like Rick said you needed for l3 not

00:14:56,970 --> 00:15:01,890
promote and there are mostly the same

00:14:59,100 --> 00:15:05,000
for our chips but other people have

00:15:01,890 --> 00:15:05,000
different ideas now

00:15:32,020 --> 00:15:37,110
yeah there's no more questions thank you

00:15:35,110 --> 00:15:43,929
very much

00:15:37,110 --> 00:15:43,929
[Applause]

00:16:02,450 --> 00:16:11,230
but sometimes I sit on the back so

00:16:06,079 --> 00:16:11,230
people don't see me do it's ready

00:17:42,240 --> 00:17:55,050
I'm partial to Taschen in the last year

00:17:47,770 --> 00:18:02,400
I've done a number of boot improvements

00:17:55,050 --> 00:18:08,460
fixes improved page initialization CPU

00:18:02,400 --> 00:18:13,600
startup also did some early time stamps

00:18:08,460 --> 00:18:17,520
projects and basically improved kernel

00:18:13,600 --> 00:18:22,560
boot time in various places improved

00:18:17,520 --> 00:18:26,200
initialization of system hash tables but

00:18:22,560 --> 00:18:28,420
today I want to talk about something

00:18:26,200 --> 00:18:34,840
else it's more like an academic question

00:18:28,420 --> 00:18:40,150
and so basically the question is how to

00:18:34,840 --> 00:18:42,480
update a grading system seamlessly so if

00:18:40,150 --> 00:18:46,660
something runs on the operating system

00:18:42,480 --> 00:18:48,580
it's almost not noticeable to that VM

00:18:46,660 --> 00:18:50,470
instance or container or some process

00:18:48,580 --> 00:18:53,610
that the operating system underneath it

00:18:50,470 --> 00:18:53,610
has been updated

00:18:55,980 --> 00:19:07,320
so in a cloud so in in a cloud it's a

00:19:03,690 --> 00:19:18,019
common situation where we have one

00:19:07,320 --> 00:19:18,019
server and it's shared into many VMs and

00:19:18,889 --> 00:19:31,230
it's it's the problem how to update that

00:19:24,260 --> 00:19:35,159
server without affecting the VMS so this

00:19:31,230 --> 00:19:38,820
is just one case as I said it's the

00:19:35,159 --> 00:19:44,100
problem is also related not only to the

00:19:38,820 --> 00:19:47,730
VMS and we need to update hosts greg

00:19:44,100 --> 00:19:54,779
rabbul a'lamin of course because if you

00:19:47,730 --> 00:19:58,049
want them to be stable and secure and on

00:19:54,779 --> 00:20:01,500
the other hand in the cloud we basically

00:19:58,049 --> 00:20:05,039
cannot afford to downtime as our

00:20:01,500 --> 00:20:07,330
customers expect 99.9 percent of their

00:20:05,039 --> 00:20:09,550
uptime

00:20:07,330 --> 00:20:14,230
so the question how how can we do that

00:20:09,550 --> 00:20:16,960
and there are some ways to address this

00:20:14,230 --> 00:20:20,110
problem right so the existing methods

00:20:16,960 --> 00:20:22,720
include hot patching so the card

00:20:20,110 --> 00:20:29,890
patching works to update the kernel

00:20:22,720 --> 00:20:31,960
we can fix some security issues with the

00:20:29,890 --> 00:20:33,580
hot patching but it doesn't work for

00:20:31,960 --> 00:20:35,620
everything I mean we can update the text

00:20:33,580 --> 00:20:40,540
we cannot really update the data that

00:20:35,620 --> 00:20:44,050
well and also even the text gets

00:20:40,540 --> 00:20:49,780
permanently slower compared to before

00:20:44,050 --> 00:20:51,100
the hot patching so and with the hot

00:20:49,780 --> 00:20:53,500
patching it's hard to introduce a new

00:20:51,100 --> 00:21:01,900
feature it's like it's possible to fix

00:20:53,500 --> 00:21:03,700
some like some small security problem

00:21:01,900 --> 00:21:08,110
but not even like a big one like for

00:21:03,700 --> 00:21:12,790
example my meltdown was the way VM works

00:21:08,110 --> 00:21:15,780
had to be redone so for that we need a

00:21:12,790 --> 00:21:17,710
fool the boot full update so the second

00:21:15,780 --> 00:21:19,300
possible solution is of course live

00:21:17,710 --> 00:21:24,370
migration but that requires extra

00:21:19,300 --> 00:21:27,040
hardware right we have one server and

00:21:24,370 --> 00:21:29,520
then we can like migrate what granted

00:21:27,040 --> 00:21:35,710
that server to another machine and

00:21:29,520 --> 00:21:39,610
update that server and for that if if we

00:21:35,710 --> 00:21:42,760
are talking about DBMS the VMS have to

00:21:39,610 --> 00:21:45,190
be configured to run on the smallest

00:21:42,760 --> 00:21:48,320
common denominator between the two

00:21:45,190 --> 00:21:50,420
servers in terms of the

00:21:48,320 --> 00:21:54,110
the hardware and the hypervisor

00:21:50,420 --> 00:21:55,970
interface so the VMS can be slower

00:21:54,110 --> 00:21:59,990
compared to if they were configured to

00:21:55,970 --> 00:22:03,230
run on a native hardware native

00:21:59,990 --> 00:22:05,320
hypervisor so it's not an ideal solution

00:22:03,230 --> 00:22:08,990
either and of course there are some

00:22:05,320 --> 00:22:11,450
mitigations for this problem so we can k

00:22:08,990 --> 00:22:15,290
exact skip the firmware just load the

00:22:11,450 --> 00:22:18,380
new kernel jump to it booted and we can

00:22:15,290 --> 00:22:20,990
keep some of the application state in

00:22:18,380 --> 00:22:22,940
memory and reattach to it later so they

00:22:20,990 --> 00:22:26,300
could keep VM state and memory we could

00:22:22,940 --> 00:22:31,250
keep some other state in memory that is

00:22:26,300 --> 00:22:33,080
not as trivial though because when we

00:22:31,250 --> 00:22:35,510
are attaching the state we have to

00:22:33,080 --> 00:22:38,510
consider like the state was that was

00:22:35,510 --> 00:22:41,410
kept in the OS before the keyboarding so

00:22:38,510 --> 00:22:46,760
but some of the application state can be

00:22:41,410 --> 00:22:56,540
kept in memory so the two solutions that

00:22:46,760 --> 00:22:58,910
I thought of from for this problem is a

00:22:56,540 --> 00:23:03,980
DS so the first solution is using the

00:22:58,910 --> 00:23:08,630
cooperative modulating and it's

00:23:03,980 --> 00:23:11,300
basically when we have one kernel

00:23:08,630 --> 00:23:14,240
running and handling all the job while

00:23:11,300 --> 00:23:16,640
the other one is simultaneously booting

00:23:14,240 --> 00:23:18,680
on the same machine and we give slice of

00:23:16,640 --> 00:23:22,250
time to that other one to keep booting

00:23:18,680 --> 00:23:25,280
and even to bring up its user space so

00:23:22,250 --> 00:23:29,210
we do K exactly from one kernel to

00:23:25,280 --> 00:23:34,360
another cooperatively so we suspend as

00:23:29,210 --> 00:23:34,360
an example so the the green one is

00:23:34,910 --> 00:23:41,090
current colonel that is running and that

00:23:38,520 --> 00:23:44,190
is that candles the VM so it would

00:23:41,090 --> 00:23:46,530
suspend the VMs jump to the new kernel K

00:23:44,190 --> 00:23:49,800
exactly the new kernel would boot for a

00:23:46,530 --> 00:23:53,010
short time then jump back we zoom VM run

00:23:49,800 --> 00:23:54,690
VM suspend and and so on the problem

00:23:53,010 --> 00:23:55,740
with this approach there are actually

00:23:54,690 --> 00:23:57,360
very many problems with this approach

00:23:55,740 --> 00:23:59,430
it's not very scalable and it's slow

00:23:57,360 --> 00:24:01,290
because the K exactly between two

00:23:59,430 --> 00:24:04,020
kernels would be as slow we would have

00:24:01,290 --> 00:24:06,750
to capture CPUs we would have to us the

00:24:04,020 --> 00:24:09,480
devices consume the devices suspending

00:24:06,750 --> 00:24:11,610
assuming VMs every time it's it's not

00:24:09,480 --> 00:24:14,340
really seamless I mean it might work in

00:24:11,610 --> 00:24:18,300
a smaller hard-working some specialized

00:24:14,340 --> 00:24:23,040
hardware device QA Seng is cheap but

00:24:18,300 --> 00:24:26,100
overall it's not really not really

00:24:23,040 --> 00:24:28,230
scalable in a good solution and even

00:24:26,100 --> 00:24:32,910
academically I don't think it's

00:24:28,230 --> 00:24:35,430
something that that would work for this

00:24:32,910 --> 00:24:38,000
case cooperative multitasking might

00:24:35,430 --> 00:24:42,030
still work for some other things but

00:24:38,000 --> 00:24:46,350
probably not solve this problem so the

00:24:42,030 --> 00:24:46,920
second solution is vm to get to bare

00:24:46,350 --> 00:24:55,200
metal

00:24:46,920 --> 00:25:00,060
it's basically we have one host to s

00:24:55,200 --> 00:25:06,060
that runs on a bare metal and it has

00:25:00,060 --> 00:25:10,560
several VMs running like on this via VM

00:25:06,060 --> 00:25:14,820
manager s and we need to update this

00:25:10,560 --> 00:25:19,440
host s so we start a new host OS inside

00:25:14,820 --> 00:25:23,160
the VM but the new VM gets a memory

00:25:19,440 --> 00:25:25,080
layout the same as the like bare metal

00:25:23,160 --> 00:25:28,650
so the memory list is exactly the same

00:25:25,080 --> 00:25:30,510
in addition to that the CPUs are exactly

00:25:28,650 --> 00:25:35,220
the same so all the instructions that

00:25:30,510 --> 00:25:37,050
are normally emulated are either not

00:25:35,220 --> 00:25:40,080
emulated or if they have to be emulated

00:25:37,050 --> 00:25:43,140
like CPU agents all simulated

00:25:40,080 --> 00:25:44,790
it traps to the underlying host OS and

00:25:43,140 --> 00:25:50,340
the host s he turns the true value to

00:25:44,790 --> 00:25:54,480
the new host to s that is in running in

00:25:50,340 --> 00:25:57,840
currently in a VM so we boot the new

00:25:54,480 --> 00:26:01,820
hostess we live migrate the VM instances

00:25:57,840 --> 00:26:08,150
into it and after we've done all of that

00:26:01,820 --> 00:26:13,590
the old process would k exact into the

00:26:08,150 --> 00:26:15,450
DVM the updated into the new updated

00:26:13,590 --> 00:26:20,940
host OS that is still running in the VM

00:26:15,450 --> 00:26:24,870
and in we would have like a new entry

00:26:20,940 --> 00:26:30,059
point where we would start converting

00:26:24,870 --> 00:26:34,950
the extended page tables to become

00:26:30,059 --> 00:26:38,160
actually real real physical pages so we

00:26:34,950 --> 00:26:41,760
could either copy them to the right

00:26:38,160 --> 00:26:44,640
places which is slower but more reliable

00:26:41,760 --> 00:26:47,070
oh we could fix the page tables within

00:26:44,640 --> 00:26:49,380
the OS which i think is more fragile and

00:26:47,070 --> 00:26:52,260
I don't like this idea and I think that

00:26:49,380 --> 00:26:56,130
if we do not like migrate the VM prior

00:26:52,260 --> 00:26:57,990
to this jump from VM to bare metal there

00:26:56,130 --> 00:27:01,770
is not that much coping needs to be done

00:26:57,990 --> 00:27:06,240
and we can actually attach to the

00:27:01,770 --> 00:27:07,590
running VMs after this switch so and

00:27:06,240 --> 00:27:10,800
again there are some problems with this

00:27:07,590 --> 00:27:14,280
approach to the main ones is actually

00:27:10,800 --> 00:27:17,880
hard to handle the devices so we would

00:27:14,280 --> 00:27:20,760
need the old host s would need to guess

00:27:17,880 --> 00:27:24,540
and three zoom into the new one but at

00:27:20,760 --> 00:27:29,940
least it needs to be done once so this

00:27:24,540 --> 00:27:34,400
is the second solution for a seamless

00:27:29,940 --> 00:27:34,400
update of operating system and

00:27:34,540 --> 00:27:41,950
I wanted to ask if there are any other

00:27:37,540 --> 00:27:45,010
solution so if you have comments on like

00:27:41,950 --> 00:27:50,730
these two approaches and maybe have some

00:27:45,010 --> 00:27:50,730
more thoughts ideas and so

00:27:54,460 --> 00:27:59,399
any thoughts

00:28:09,980 --> 00:28:13,029
[Music]

00:28:13,799 --> 00:28:21,989
if we reduce the time of booting to BM

00:28:17,369 --> 00:28:24,779
to minimum to see the still this is a

00:28:21,989 --> 00:28:26,629
big issue what is the time that you see

00:28:24,779 --> 00:28:29,070
in the in the booting of VMs that it's

00:28:26,629 --> 00:28:31,230
generating so much issues I have seen

00:28:29,070 --> 00:28:35,999
some times of less than 200 milliseconds

00:28:31,230 --> 00:28:38,789
sometimes less than 200 milliseconds to

00:28:35,999 --> 00:28:41,129
do what to the boot the BM at somehow no

00:28:38,789 --> 00:28:44,429
that's a booting vm is fast I'm talking

00:28:41,129 --> 00:28:48,450
about updating the hypervisor so like

00:28:44,429 --> 00:28:51,600
the actual OS that manages their virtual

00:28:48,450 --> 00:28:54,779
machines so it's not actually updating

00:28:51,600 --> 00:28:59,789
the virtual machine yes it's possible to

00:28:54,779 --> 00:29:02,970
boot vm in under a second and it's even

00:28:59,789 --> 00:29:09,230
a bare-metal you can boot it together to

00:29:02,970 --> 00:29:12,119
the fast kernels optimized well and but

00:29:09,230 --> 00:29:14,429
but still if you update the full stack

00:29:12,119 --> 00:29:18,090
like from the current system user

00:29:14,429 --> 00:29:24,330
services it takes time especially on

00:29:18,090 --> 00:29:30,059
large machines which hosts many VMs on

00:29:24,330 --> 00:29:32,429
top of them how are you proposing to

00:29:30,059 --> 00:29:34,710
create a virtual machine on the host

00:29:32,429 --> 00:29:37,080
that has the whole memory of the host

00:29:34,710 --> 00:29:39,929
Wow the other virtual machines are still

00:29:37,080 --> 00:29:45,480
using this memory yes that's a that's a

00:29:39,929 --> 00:29:49,619
good question so when we boot that host

00:29:45,480 --> 00:29:53,730
OS inside the VM it sees all the memory

00:29:49,619 --> 00:29:55,950
it's not using it so once we start life

00:29:53,730 --> 00:29:58,859
migrating the VMS inside that one those

00:29:55,950 --> 00:30:02,929
VM that memory that is occupied but

00:29:58,859 --> 00:30:08,249
those VMs is actually released from the

00:30:02,929 --> 00:30:14,669
actual machine and add it inside this is

00:30:08,249 --> 00:30:17,039
allocated inside the new host OS VM this

00:30:14,669 --> 00:30:19,379
needs a lot of work from user space to

00:30:17,039 --> 00:30:20,909
work because currently you cannot create

00:30:19,379 --> 00:30:23,820
a virtual machine without actually

00:30:20,909 --> 00:30:28,679
having their resources you cannot create

00:30:23,820 --> 00:30:31,470
the instance to start with I'm pretty

00:30:28,679 --> 00:30:33,359
sure when you start with a VM it doesn't

00:30:31,470 --> 00:30:35,399
allocate all the memory by the way no it

00:30:33,359 --> 00:30:41,580
doesn't or gate it but the user space

00:30:35,399 --> 00:30:44,309
requires it for example for with k vm if

00:30:41,580 --> 00:30:48,899
you can have the free resources and well

00:30:44,309 --> 00:30:53,669
i mean if you're not touching the

00:30:48,899 --> 00:30:55,950
resources it's not going to know that

00:30:53,669 --> 00:30:59,009
those resources are missing and you can

00:30:55,950 --> 00:31:02,570
actually like start the VM with even

00:30:59,009 --> 00:31:02,570
more memory than the Machine has

00:31:07,330 --> 00:31:12,139
just curious have any of these methods

00:31:10,190 --> 00:31:14,090
being tried the prototype door it's just

00:31:12,139 --> 00:31:15,889
yes that's that's what actually I

00:31:14,090 --> 00:31:18,529
planned to do to start prototyping and

00:31:15,889 --> 00:31:22,850
see if this is actually doable

00:31:18,529 --> 00:31:24,919
that's something I'm eager to try to to

00:31:22,850 --> 00:31:27,169
try to go from vme to bare metal I'm

00:31:24,919 --> 00:31:31,460
very interested like just doing a very

00:31:27,169 --> 00:31:32,809
simple qmv emulation so the hostess is

00:31:31,460 --> 00:31:36,409
actually going to be in came you then

00:31:32,809 --> 00:31:46,340
start something inside the game you and

00:31:36,409 --> 00:31:49,519
then go okay hi

00:31:46,340 --> 00:31:51,559
when can you move to the second solution

00:31:49,519 --> 00:31:53,179
you suggested yes you said migrate

00:31:51,559 --> 00:31:56,330
they're running beams inside the new

00:31:53,179 --> 00:31:58,279
house so you mean while the new virtual

00:31:56,330 --> 00:32:02,200
machine is running with the new code you

00:31:58,279 --> 00:32:05,210
want to migrate the other VMs into it

00:32:02,200 --> 00:32:07,639
yes so that's one of the solution is to

00:32:05,210 --> 00:32:11,600
migrate the other VMs inside the new

00:32:07,639 --> 00:32:12,590
host OS while it's still running as a VM

00:32:11,600 --> 00:32:15,139
so it will be like a nested

00:32:12,590 --> 00:32:17,960
virtualization yeah is it possible

00:32:15,139 --> 00:32:23,990
because I know that virtualization is

00:32:17,960 --> 00:32:27,080
fully supported by Intel right now and I

00:32:23,990 --> 00:32:30,789
know we've tried once to you to do a

00:32:27,080 --> 00:32:34,010
nested virtualization for example we

00:32:30,789 --> 00:32:35,960
using a Mellanox card and we couldn't do

00:32:34,010 --> 00:32:40,190
it there no care no patches to support

00:32:35,960 --> 00:32:42,019
it is the warrant when we tried it so it

00:32:40,190 --> 00:32:44,090
should it should be supported now this

00:32:42,019 --> 00:32:46,610
faster I know but

00:32:44,090 --> 00:32:52,429
I haven't tried it myself but the spice

00:32:46,610 --> 00:32:55,669
I research it it should work and Intel

00:32:52,429 --> 00:32:58,480
documents the literature virtualization

00:32:55,669 --> 00:33:01,039
quite extensively and in its

00:32:58,480 --> 00:33:03,169
documentation it will be interesting to

00:33:01,039 --> 00:33:07,010
hear that it happened

00:33:03,169 --> 00:33:09,409
I gave him forum there were several

00:33:07,010 --> 00:33:10,909
several talks about doing to just that

00:33:09,409 --> 00:33:14,740
so I think it's always possible at this

00:33:10,909 --> 00:33:14,740
point I don't know the exact details

00:33:21,160 --> 00:33:27,260
what do you do about things I guess or

00:33:23,330 --> 00:33:31,900
I'll be when you're trying to initialize

00:33:27,260 --> 00:33:33,980
the the BFS and push those up yes so

00:33:31,900 --> 00:33:43,700
this is an excellent excellent session

00:33:33,980 --> 00:33:47,780
and so I was first thinking to handle

00:33:43,700 --> 00:33:50,410
the okay so for the prototyping they one

00:33:47,780 --> 00:33:53,720
that I'm talking about in the K mu I'm

00:33:50,410 --> 00:33:57,410
going to try without the devices at all

00:33:53,720 --> 00:34:02,240
just memory and spear you know like

00:33:57,410 --> 00:34:06,260
external devices but as I say it's

00:34:02,240 --> 00:34:09,740
somebody handling devices need to be

00:34:06,260 --> 00:34:12,260
designed well but the new host OS I

00:34:09,740 --> 00:34:14,960
think we'll start handling the physical

00:34:12,260 --> 00:34:21,250
functions while and still running and in

00:34:14,960 --> 00:34:21,250
the VM so we would the old hostess would

00:34:21,399 --> 00:34:27,649
stop the devices and give the handling

00:34:24,619 --> 00:34:31,010
children new OS they're still running in

00:34:27,649 --> 00:34:34,580
the VM and then when it's began like

00:34:31,010 --> 00:34:38,169
when it will become the bare metal it

00:34:34,580 --> 00:34:38,169
will just continue using them as

00:34:39,270 --> 00:34:48,260
as physical devices and then you see

00:34:45,630 --> 00:34:48,260
your password

00:34:59,810 --> 00:35:12,289
okay any more questions thank you

00:35:05,720 --> 00:35:12,289
[Applause]

00:36:32,119 --> 00:36:38,010
okay to come all wired up here so hi

00:36:36,390 --> 00:36:39,780
folks thanks for coming I'm Steve's

00:36:38,010 --> 00:36:41,850
Astaire and I'm gonna talk about some

00:36:39,780 --> 00:36:44,430
work I've been doing on improving CPU

00:36:41,850 --> 00:36:48,180
utilization with an enhancement to the

00:36:44,430 --> 00:36:49,320
task scheduler so I only have a few

00:36:48,180 --> 00:36:51,180
slides there bill means to tell you

00:36:49,320 --> 00:36:52,500
what's coming so you know you know what

00:36:51,180 --> 00:36:54,150
to wait for and when to ask questions

00:36:52,500 --> 00:36:56,310
hello - welcome anytime

00:36:54,150 --> 00:36:57,930
so first time that I very briefly talk

00:36:56,310 --> 00:37:00,240
about the existing load balancing

00:36:57,930 --> 00:37:02,220
mechanisms in the scheduler then I'm

00:37:00,240 --> 00:37:04,700
going to describe the new method that

00:37:02,220 --> 00:37:07,530
I've introduced into one of those places

00:37:04,700 --> 00:37:09,960
present some performance data talk a

00:37:07,530 --> 00:37:14,340
little bit about some pneuma issues and

00:37:09,960 --> 00:37:18,510
then talk about future directions so

00:37:14,340 --> 00:37:20,430
moving right into it so first I should

00:37:18,510 --> 00:37:22,830
mention I've been working on the CFS

00:37:20,430 --> 00:37:25,470
task area although the ideas could be

00:37:22,830 --> 00:37:28,110
extended to others so in C if that's a

00:37:25,470 --> 00:37:30,119
little balancing at a high level there

00:37:28,110 --> 00:37:32,610
are a few places where we do this number

00:37:30,119 --> 00:37:34,619
one - task wakes up and the schedule has

00:37:32,610 --> 00:37:38,130
to find a place to run it and ideally we

00:37:34,619 --> 00:37:41,100
like to find an idol core or CPU and so

00:37:38,130 --> 00:37:44,220
we have to do a little push and right

00:37:41,100 --> 00:37:47,010
now the code searches in the last level

00:37:44,220 --> 00:37:49,800
cache set of CPUs you know hoping for an

00:37:47,010 --> 00:37:52,410
idle core CPU and the search is limited

00:37:49,800 --> 00:37:55,740
depending on how busy the system is so

00:37:52,410 --> 00:37:58,380
the scheduler tracks the recent cost of

00:37:55,740 --> 00:38:00,869
these searches and also tracks the

00:37:58,380 --> 00:38:03,840
average idle time recently both of those

00:38:00,869 --> 00:38:05,910
decay over time and at some point during

00:38:03,840 --> 00:38:08,700
the search for this idle CPU if the cost

00:38:05,910 --> 00:38:12,119
exceeds the average idle it stops

00:38:08,700 --> 00:38:14,550
looking and and just defaults to placing

00:38:12,119 --> 00:38:16,619
the task where it woke up I mean I've

00:38:14,550 --> 00:38:18,270
noted the echo paths that do some of

00:38:16,619 --> 00:38:21,450
these things since I'm gonna be tapping

00:38:18,270 --> 00:38:23,310
into them later so the next place we do

00:38:21,450 --> 00:38:25,350
some balancing is when a CPU goes idle

00:38:23,310 --> 00:38:28,920
so it's run the last runnable task on

00:38:25,350 --> 00:38:31,140
its run queue and so we have to pull a

00:38:28,920 --> 00:38:34,650
runnable task from some other CPU some

00:38:31,140 --> 00:38:36,570
you know deserving waiting task so this

00:38:34,650 --> 00:38:38,250
also does a search and it searches the

00:38:36,570 --> 00:38:40,380
scheduling domains starting from the

00:38:38,250 --> 00:38:44,430
smallest most local domain up to the

00:38:40,380 --> 00:38:48,420
broadest so up the hierarchy and this is

00:38:44,430 --> 00:38:50,220
also a bounded search again we the

00:38:48,420 --> 00:38:52,650
scheduler members the costs of searching

00:38:50,220 --> 00:38:54,990
each domain recently and again compares

00:38:52,650 --> 00:38:58,170
that average idle so as it looks first

00:38:54,990 --> 00:39:00,930
in the core domain and then the sock

00:38:58,170 --> 00:39:02,850
domain and then multi socket domains and

00:39:00,930 --> 00:39:04,560
each of those steps says well have I

00:39:02,850 --> 00:39:08,490
spent too much time searching already if

00:39:04,560 --> 00:39:09,840
so stop searching and this can be

00:39:08,490 --> 00:39:12,420
expensive particularly in large systems

00:39:09,840 --> 00:39:15,150
at those higher level domains you might

00:39:12,420 --> 00:39:17,730
be looking at hundreds of CPUs it's a

00:39:15,150 --> 00:39:19,440
linear search and so this cost can get

00:39:17,730 --> 00:39:21,869
up into the you know tens or even

00:39:19,440 --> 00:39:24,440
hundreds of microseconds across all

00:39:21,869 --> 00:39:27,150
these domains that are searched

00:39:24,440 --> 00:39:30,030
let's see okay and then lastly I'm

00:39:27,150 --> 00:39:32,940
periodically there's attempt to globally

00:39:30,030 --> 00:39:34,560
smooth out the load looking at all the

00:39:32,940 --> 00:39:37,470
run queues and trying to push tests

00:39:34,560 --> 00:39:40,050
around to balance things and that's a

00:39:37,470 --> 00:39:41,940
proactive attempt at keeping the CPUs

00:39:40,050 --> 00:39:44,700
fed so you know making sure they all

00:39:41,940 --> 00:39:45,780
have multiple ready to run tasks and and

00:39:44,700 --> 00:39:48,290
they don't go I don't know we don't have

00:39:45,780 --> 00:39:48,290
to do these searches

00:39:48,530 --> 00:39:55,820
okay so what I've done is I've extended

00:39:51,840 --> 00:40:01,109
this second area where the CPU goes idle

00:39:55,820 --> 00:40:03,750
so in my new code when the CPU goes idle

00:40:01,109 --> 00:40:09,030
I searched for CPUs in the last level

00:40:03,750 --> 00:40:10,920
cache to me often l3 know nowadays the

00:40:09,030 --> 00:40:12,720
search is not limited and the goal here

00:40:10,920 --> 00:40:16,290
is to find the first CPU you run across

00:40:12,720 --> 00:40:18,300
that has more than one runnable job and

00:40:16,290 --> 00:40:20,550
I else where this talk I call it an

00:40:18,300 --> 00:40:21,010
overloaded CPU that term is also used in

00:40:20,550 --> 00:40:24,250
the current

00:40:21,010 --> 00:40:26,140
for a real-time and you want to steal

00:40:24,250 --> 00:40:28,180
one task you don't really care what task

00:40:26,140 --> 00:40:30,880
you get the goal is to find some work to

00:40:28,180 --> 00:40:32,320
do as quickly as possible and in my

00:40:30,880 --> 00:40:34,270
measurements this can be done pretty

00:40:32,320 --> 00:40:36,730
quickly it's typically one or two

00:40:34,270 --> 00:40:39,280
microseconds to do this and as a result

00:40:36,730 --> 00:40:41,500
it's cheap enough that you can afford to

00:40:39,280 --> 00:40:44,490
do this search every time the CPU goes

00:40:41,500 --> 00:40:46,240
idle so you don't give up and

00:40:44,490 --> 00:40:48,490
essentially if there is something

00:40:46,240 --> 00:40:52,330
runnable out there and you LLC we're

00:40:48,490 --> 00:40:54,690
gonna find your we're going to run it ok

00:40:52,330 --> 00:41:00,130
so that's the the overload stealing so

00:40:54,690 --> 00:41:01,690
yeah a few more bits here so to to speed

00:41:00,130 --> 00:41:04,270
up the search and make it more scalable

00:41:01,690 --> 00:41:07,750
I had an auxilary data structure here I

00:41:04,270 --> 00:41:10,990
keep a second bitmap per last level

00:41:07,750 --> 00:41:14,650
cache of the overloaded CPUs so whenever

00:41:10,990 --> 00:41:16,990
a task goes on or off CPU if you cross

00:41:14,650 --> 00:41:20,440
that threshold of one runnable job on

00:41:16,990 --> 00:41:22,780
the CPU i either set or clear the bit in

00:41:20,440 --> 00:41:25,540
this mask and so we have this nice

00:41:22,780 --> 00:41:28,030
compact representation for the LLC of

00:41:25,540 --> 00:41:30,130
all the potential candidate CPUs that

00:41:28,030 --> 00:41:32,020
have an extra task on my bale of steel

00:41:30,130 --> 00:41:33,369
and then of course when we look at it

00:41:32,020 --> 00:41:36,940
there are other criteria that can be

00:41:33,369 --> 00:41:38,770
applied it's possible that it's possible

00:41:36,940 --> 00:41:41,290
that the task is not allowed to run on

00:41:38,770 --> 00:41:43,270
this idle CPU so sometimes that bitmap

00:41:41,290 --> 00:41:46,480
is a bit bad it's necessary but not

00:41:43,270 --> 00:41:49,240
sufficient another detail here though is

00:41:46,480 --> 00:41:50,859
that that the setting and clearing these

00:41:49,240 --> 00:41:52,540
 is atomic there's no lock

00:41:50,859 --> 00:41:53,530
protecting that but that can be a

00:41:52,540 --> 00:41:55,690
bottleneck

00:41:53,530 --> 00:41:59,109
you know bitmaps are very dense at one

00:41:55,690 --> 00:42:01,330
bit per CPU on a you know an

00:41:59,109 --> 00:42:04,510
architecture with a 64 byte cache line

00:42:01,330 --> 00:42:06,070
you could have 512 CPU is all mapping to

00:42:04,510 --> 00:42:07,630
the same cache line and of course you

00:42:06,070 --> 00:42:10,119
can't do Atomics the higher rates than

00:42:07,630 --> 00:42:12,100
that that would not scale either and so

00:42:10,119 --> 00:42:15,700
I added a new primitive called the

00:42:12,100 --> 00:42:17,470
sparse mask model after the CPU mask

00:42:15,700 --> 00:42:20,280
primitive it's already in the chrome I

00:42:17,470 --> 00:42:24,000
mean in the sparse mask only use a few

00:42:20,280 --> 00:42:26,700
active bits a few are

00:42:24,000 --> 00:42:28,860
irrelevant bits per cache line that's

00:42:26,700 --> 00:42:31,350
actually a compile-time well no it's a

00:42:28,860 --> 00:42:34,710
it's a runtime creation time constant I

00:42:31,350 --> 00:42:36,660
use 8 for my work so 8 bits per 64 bytes

00:42:34,710 --> 00:42:39,030
are used in spit map the others are

00:42:36,660 --> 00:42:41,670
ignored so I've written all the the set

00:42:39,030 --> 00:42:44,550
traversal and manipulation primitives to

00:42:41,670 --> 00:42:47,010
to skip those unused bikes and the

00:42:44,550 --> 00:42:49,260
skills very nicely I've measured the

00:42:47,010 --> 00:42:53,580
overhead of the Atomics that's set and

00:42:49,260 --> 00:42:56,010
clear these bits at very high very high

00:42:53,580 --> 00:42:59,420
context which rates and it's negligible

00:42:56,010 --> 00:43:01,980
you don't there's no contention measured

00:42:59,420 --> 00:43:04,770
so let's see yeah so I mentioned the API

00:43:01,980 --> 00:43:06,960
is patterned after CPU mask which is

00:43:04,770 --> 00:43:09,540
used pretty extensively in the scheduler

00:43:06,960 --> 00:43:11,400
code and therefore it's familiar api

00:43:09,540 --> 00:43:13,380
it's pretty easy to take any place that

00:43:11,400 --> 00:43:15,060
currently used CPU mask and drop in the

00:43:13,380 --> 00:43:19,110
sparse mask so it's a familiar

00:43:15,060 --> 00:43:22,500
abstraction to the programmer and just a

00:43:19,110 --> 00:43:24,890
few other aspects of this so the way

00:43:22,500 --> 00:43:28,710
this is integrated in the current code

00:43:24,890 --> 00:43:30,480
currently when the CPU goes idle and

00:43:28,710 --> 00:43:33,840
which is planning the next task to run

00:43:30,480 --> 00:43:36,350
it calls idle balance in my patch I

00:43:33,840 --> 00:43:38,369
still do that a first call idle balance

00:43:36,350 --> 00:43:41,250
idle balance as I mentioned before

00:43:38,369 --> 00:43:43,320
searches all the scheduling domains so

00:43:41,250 --> 00:43:47,369
it gives it gives the code a chance to

00:43:43,320 --> 00:43:48,960
find tasks to run outside of the last

00:43:47,369 --> 00:43:50,550
level cache you know other Numa nodes

00:43:48,960 --> 00:43:52,619
and may be pulled it over and run it and

00:43:50,550 --> 00:43:54,600
and that's important because my stealing

00:43:52,619 --> 00:43:56,850
code only looks within the last level

00:43:54,600 --> 00:43:59,280
cache but if idle balance returns no

00:43:56,850 --> 00:44:01,080
candidate perhaps because it you know

00:43:59,280 --> 00:44:03,030
exited early thought the search was

00:44:01,080 --> 00:44:04,320
gonna be more expensive then I call this

00:44:03,030 --> 00:44:08,010
new stealing code to look for a

00:44:04,320 --> 00:44:10,140
candidate within the last level cache so

00:44:08,010 --> 00:44:12,119
I chose the last level cache is the

00:44:10,140 --> 00:44:14,700
scope for a few reasons

00:44:12,119 --> 00:44:17,520
one is I wanted to avoid Numa issues

00:44:14,700 --> 00:44:19,530
because they get tricky although we see

00:44:17,520 --> 00:44:22,380
I'll see you'll see later on that I

00:44:19,530 --> 00:44:24,420
wasn't entirely successful but second is

00:44:22,380 --> 00:44:27,960
very efficient so you've got the set of

00:44:24,420 --> 00:44:31,710
CPUs maintaining run queues and

00:44:27,960 --> 00:44:33,030
associated metadata locks etc and by

00:44:31,710 --> 00:44:35,940
looking within that last level cache

00:44:33,030 --> 00:44:37,180
much that metadata stays within the same

00:44:35,940 --> 00:44:39,190
LLC and it's pretty

00:44:37,180 --> 00:44:43,960
cheap and fast to access and so the

00:44:39,190 --> 00:44:46,059
search time is lower and actually one

00:44:43,960 --> 00:44:48,520
more climbing here I I said that the the

00:44:46,059 --> 00:44:50,650
search here looks over all the CPUs and

00:44:48,520 --> 00:44:54,069
last level cache it does but it doesn't

00:44:50,650 --> 00:44:56,920
in two steps so first I look within the

00:44:54,069 --> 00:44:59,470
CPUs within the the core the same cores

00:44:56,920 --> 00:45:01,599
the LCP you try to maximize the the

00:44:59,470 --> 00:45:03,640
cache locality there and if there are no

00:45:01,599 --> 00:45:07,990
candidates within that very close range

00:45:03,640 --> 00:45:09,180
then I go up to the broader LLC so and

00:45:07,990 --> 00:45:12,910
I've got a reference here to this

00:45:09,180 --> 00:45:14,799
work-in-progress patch out there any

00:45:12,910 --> 00:45:22,230
questions before I move on to some

00:45:14,799 --> 00:45:22,230
performance data okay

00:45:24,130 --> 00:45:30,130
alright so the results I'm getting I've

00:45:27,880 --> 00:45:32,050
tested a number of workloads and gotten

00:45:30,130 --> 00:45:34,720
some you know nice performance

00:45:32,050 --> 00:45:36,340
improvements but turns out hacked bench

00:45:34,720 --> 00:45:40,000
is very illustrative for this so I'm

00:45:36,340 --> 00:45:41,950
using it so on this test that compared

00:45:40,000 --> 00:45:44,710
the baseline kernel to a carnival with

00:45:41,950 --> 00:45:49,480
the stealing enhancement using hack

00:45:44,710 --> 00:45:53,170
bench and running it on a1 sockets Xeon

00:45:49,480 --> 00:45:57,460
system 10 cores two strands each so 20

00:45:53,170 --> 00:46:00,550
CPUs total so hack bench here is run

00:45:57,460 --> 00:46:02,770
with a series of different numbers of

00:46:00,550 --> 00:46:06,060
groups now each group in hack bench

00:46:02,770 --> 00:46:09,160
represents 40 tasks so I need to row

00:46:06,060 --> 00:46:12,820
here first row represents a run of 40

00:46:09,160 --> 00:46:14,620
tasks and X is 80 120 and so on so we've

00:46:12,820 --> 00:46:17,920
got the baseline results on the top the

00:46:14,620 --> 00:46:19,660
new kernel on the bottom and so what I'm

00:46:17,920 --> 00:46:22,600
showing here is some of the secondary

00:46:19,660 --> 00:46:24,850
statistics they can get out of proc

00:46:22,600 --> 00:46:27,910
sched stat when you run this which

00:46:24,850 --> 00:46:30,370
incidentally is totally awesome now that

00:46:27,910 --> 00:46:32,380
you can compile in sched stat to the

00:46:30,370 --> 00:46:34,210
kernel and then enable it dynamically

00:46:32,380 --> 00:46:36,430
it's just a huge boon to doing any

00:46:34,210 --> 00:46:39,850
development of the scheduler so kudos to

00:46:36,430 --> 00:46:42,220
everyone who gave that to us so the data

00:46:39,850 --> 00:46:45,390
I'm showing here the elapsed time in

00:46:42,220 --> 00:46:48,480
seconds in the first column and then the

00:46:45,390 --> 00:46:51,010
CPU utilization of the sum of user and

00:46:48,480 --> 00:46:53,800
system time here

00:46:51,010 --> 00:46:56,890
the next column the average time slicing

00:46:53,800 --> 00:46:58,480
milliseconds not too interesting for

00:46:56,890 --> 00:47:00,010
this experiment other than to know just

00:46:58,480 --> 00:47:03,280
really small for hack bench this is a

00:47:00,010 --> 00:47:06,580
tremendously chatty workload the total

00:47:03,280 --> 00:47:09,640
number of calls to the scheduler across

00:47:06,580 --> 00:47:12,610
the whole run but at per CPU so per CPU

00:47:09,640 --> 00:47:16,030
total the number times the CPU went idle

00:47:12,610 --> 00:47:18,940
the number times they cast locum CPU and

00:47:16,030 --> 00:47:21,220
then this next last column the call if

00:47:18,940 --> 00:47:23,830
the percent fine time this is the cost

00:47:21,220 --> 00:47:27,250
of the scheduler functions that do the

00:47:23,830 --> 00:47:31,060
push and pull matchup of tasks and CPUs

00:47:27,250 --> 00:47:32,410
as a percent of total CPU time so you

00:47:31,060 --> 00:47:33,319
see in the baseline Colonel we're

00:47:32,410 --> 00:47:35,089
ranging from

00:47:33,319 --> 00:47:37,849
point three percent to about you know

00:47:35,089 --> 00:47:39,109
point five point six percent and that's

00:47:37,849 --> 00:47:41,359
very relevant of course because I want

00:47:39,109 --> 00:47:43,400
to show that the stealing code is still

00:47:41,359 --> 00:47:46,609
efficient then the last column shows the

00:47:43,400 --> 00:47:47,779
number of steals per CPU in the baseline

00:47:46,609 --> 00:47:50,539
kernel there are none it doesn't have

00:47:47,779 --> 00:47:52,430
stealing so part of my patch adds a few

00:47:50,539 --> 00:47:55,190
extra stats to measure things like the

00:47:52,430 --> 00:47:57,440
fine time the number of steals they're

00:47:55,190 --> 00:47:59,059
really intended for develop used to only

00:47:57,440 --> 00:48:01,369
probably won't go back upstream unless

00:47:59,059 --> 00:48:03,459
there's some demand form but anyway the

00:48:01,369 --> 00:48:07,009
the second table I'm showing the

00:48:03,459 --> 00:48:09,739
stealing code so same columns and if you

00:48:07,009 --> 00:48:12,259
look at the CPU utilization shown here

00:48:09,739 --> 00:48:16,729
and also plotted in the upper right on

00:48:12,259 --> 00:48:19,339
our 20cm running the smallest experiment

00:48:16,729 --> 00:48:20,690
forty tasks we're on the baseline

00:48:19,339 --> 00:48:23,539
colonel about seventy five percent

00:48:20,690 --> 00:48:25,699
utilized but it turns out that's pretty

00:48:23,539 --> 00:48:27,949
uneven some CPUs have more than one

00:48:25,699 --> 00:48:30,469
runnable task and others are idle quite

00:48:27,949 --> 00:48:32,690
often and stealing smooth that out and

00:48:30,469 --> 00:48:36,319
so you can see as we add more and more

00:48:32,690 --> 00:48:37,219
tasks whereas the baseline kernel in red

00:48:36,319 --> 00:48:39,380
on the bottom

00:48:37,219 --> 00:48:44,019
slowly converges towards on two percent

00:48:39,380 --> 00:48:46,880
the stealing colonel rapidly reaches 180

00:48:44,019 --> 00:48:49,369
corresponding improvement in the elapsed

00:48:46,880 --> 00:48:53,119
time the percentage of speed-up shown in

00:48:49,369 --> 00:48:55,400
the in the lower right over there and on

00:48:53,119 --> 00:48:56,959
stats show that indeed stealing is doing

00:48:55,400 --> 00:48:58,819
its job it's helping us so the steel

00:48:56,959 --> 00:49:02,359
column we're actually doing a number of

00:48:58,819 --> 00:49:04,269
steals per CPU across the length of this

00:49:02,359 --> 00:49:06,619
run but what's kind of interesting is

00:49:04,269 --> 00:49:08,749
you know the steals are still a

00:49:06,619 --> 00:49:12,019
relatively small percentage of the

00:49:08,749 --> 00:49:13,699
overall schedule events you know 1% or

00:49:12,019 --> 00:49:16,359
so but it ends up making a pretty big

00:49:13,699 --> 00:49:20,449
difference in the overall utilization

00:49:16,359 --> 00:49:22,130
and what also means is the the finds

00:49:20,449 --> 00:49:23,449
percent find time is pretty low because

00:49:22,130 --> 00:49:25,910
they you know we didn't have to do many

00:49:23,449 --> 00:49:28,549
steals to make things better so the

00:49:25,910 --> 00:49:31,249
percentage of CPU that we're using to do

00:49:28,549 --> 00:49:33,799
this went up by about 0.3 percent maybe

00:49:31,249 --> 00:49:36,049
0.4 percent but we're getting a much

00:49:33,799 --> 00:49:38,450
larger payback in the overall speed-up

00:49:36,049 --> 00:49:41,420
of the workload

00:49:38,450 --> 00:49:44,900
so this is the basic data any questions

00:49:41,420 --> 00:49:53,089
before we move on we can come back some

00:49:44,900 --> 00:49:55,099
curse to you later how much did you

00:49:53,089 --> 00:49:57,349
record like how much more migration

00:49:55,099 --> 00:49:59,839
happens how much what should happen how

00:49:57,349 --> 00:50:03,410
much how much more the tasks migrated oh

00:49:59,839 --> 00:50:05,750
yeah actually migrate sure well this is

00:50:03,410 --> 00:50:09,770
a OneNote system so I allow arbitrary

00:50:05,750 --> 00:50:12,230
migration within the last level - so you

00:50:09,770 --> 00:50:14,210
know I don't I don't apply I don't try

00:50:12,230 --> 00:50:17,780
to force things to stay within the l1 or

00:50:14,210 --> 00:50:19,579
the l2 I mean just like the number in

00:50:17,780 --> 00:50:21,140
general cross cross no young for Numa

00:50:19,579 --> 00:50:26,089
and there's no no just between CPUs I

00:50:21,140 --> 00:50:31,490
mean oh 20p use um boise yeah I have

00:50:26,089 --> 00:50:32,900
that data I'm not showing it here yeah I

00:50:31,490 --> 00:50:35,059
have to dig that up to see what the old

00:50:32,900 --> 00:50:36,589
versus new but but essentially you know

00:50:35,059 --> 00:50:39,369
that the stealing numbers would be the

00:50:36,589 --> 00:50:41,750
Delta for the new number of moves and

00:50:39,369 --> 00:50:43,579
again it's pretty small percent of the

00:50:41,750 --> 00:50:46,160
overall moves things move around a lot

00:50:43,579 --> 00:50:48,200
yeah again particularly hack punch we've

00:50:46,160 --> 00:50:50,480
got all these senders and receivers

00:50:48,200 --> 00:50:52,099
talking to each other a lot you know

00:50:50,480 --> 00:50:57,609
that the wake up code tends to bring one

00:50:52,099 --> 00:51:01,869
to the other frequently anything else

00:50:57,609 --> 00:51:01,869
and in the far back me and Mike

00:51:04,370 --> 00:51:09,570
so you introduce the bitmask so I'm

00:51:07,770 --> 00:51:12,390
wondering since it is a sparse bitmask

00:51:09,570 --> 00:51:15,690
and essentially while stealing you have

00:51:12,390 --> 00:51:18,090
to find the next on overloaded CQ index

00:51:15,690 --> 00:51:19,830
right starting from a current index so

00:51:18,090 --> 00:51:22,260
don't you have to like scan through a

00:51:19,830 --> 00:51:24,540
lot of bytes before you get to the next

00:51:22,260 --> 00:51:26,280
one so how efficient is that and why

00:51:24,540 --> 00:51:29,010
just using a regular width mask and

00:51:26,280 --> 00:51:30,330
trying right right well so in the

00:51:29,010 --> 00:51:33,000
implementation of the sparked this

00:51:30,330 --> 00:51:35,190
sparse that masked the eight significant

00:51:33,000 --> 00:51:37,620
bits are all in the first word so I can

00:51:35,190 --> 00:51:42,150
skip over the remaining seven words in

00:51:37,620 --> 00:51:43,920
the cache line so that well well again

00:51:42,150 --> 00:51:45,420
you know I look in the first word of a

00:51:43,920 --> 00:51:48,090
cache line you know looking for the

00:51:45,420 --> 00:51:49,890
first eight bits if there's nothing

00:51:48,090 --> 00:51:52,020
there I can skip all the way to the next

00:51:49,890 --> 00:51:54,510
we're in the next cache line so I'm not

00:51:52,020 --> 00:51:58,050
you know I'm only looking at one eighth

00:51:54,510 --> 00:52:00,480
of the words now yeah it is true in that

00:51:58,050 --> 00:52:02,670
first word is true in that first word

00:52:00,480 --> 00:52:05,340
I'm only looking at you know eight bits

00:52:02,670 --> 00:52:07,980
out of the 64 bits and you could imagine

00:52:05,340 --> 00:52:09,870
a regular bit mask they can squeeze them

00:52:07,980 --> 00:52:12,120
down and in Traverse more efficiently

00:52:09,870 --> 00:52:14,100
but again the scaling issue is the

00:52:12,120 --> 00:52:17,160
dominant factor here you really have to

00:52:14,100 --> 00:52:23,000
avoid the cost of maintaining these bits

00:52:17,160 --> 00:52:26,490
in the transitions on and off CPU yes

00:52:23,000 --> 00:52:30,030
well you have any more performance

00:52:26,490 --> 00:52:32,550
numbers that can show that some cases so

00:52:30,030 --> 00:52:34,570
much migrations like Stephen said before

00:52:32,550 --> 00:52:38,660
could

00:52:34,570 --> 00:52:40,550
downgrade the performance of the at the

00:52:38,660 --> 00:52:43,190
end I mean not the total CPU utilization

00:52:40,550 --> 00:52:45,830
but the total time that benchmark take

00:52:43,190 --> 00:52:47,690
to execute at the end because of many

00:52:45,830 --> 00:52:51,470
transitions or migration between the

00:52:47,690 --> 00:52:52,790
city the process between the CPUs we

00:52:51,470 --> 00:52:56,570
have more results of performance that

00:52:52,790 --> 00:52:57,950
were not only this one I didn't quite

00:52:56,570 --> 00:53:00,080
get the thrust to your questions you

00:52:57,950 --> 00:53:02,810
thing do I have more results than this

00:53:00,080 --> 00:53:04,340
to show a improvement yes I have a large

00:53:02,810 --> 00:53:07,220
number of results there in the patch

00:53:04,340 --> 00:53:10,340
series if you want to go look at it by I

00:53:07,220 --> 00:53:11,570
don't run some Java workloads other

00:53:10,340 --> 00:53:15,020
networking workloads

00:53:11,570 --> 00:53:18,410
you know read/write micro benchmarks you

00:53:15,020 --> 00:53:21,110
know full scale database Oracle TPCC and

00:53:18,410 --> 00:53:23,180
you know we're seeing real improvements

00:53:21,110 --> 00:53:28,160
across that whole range of benchmarks on

00:53:23,180 --> 00:53:32,420
on single and multi node systems okay I

00:53:28,160 --> 00:53:35,660
have a question here so currently what

00:53:32,420 --> 00:53:38,990
is what is the status of or what are the

00:53:35,660 --> 00:53:41,810
organic criteria for migration and how

00:53:38,990 --> 00:53:43,700
does this stealing impact that specific

00:53:41,810 --> 00:53:46,040
algorithm if there is anything so for

00:53:43,700 --> 00:53:48,020
example why did the CPS decide to go to

00:53:46,040 --> 00:53:51,620
idle when there was apparently

00:53:48,020 --> 00:53:54,170
sufficient workload to be performed what

00:53:51,620 --> 00:53:56,780
can we do to maybe address that in

00:53:54,170 --> 00:53:58,660
addition to this short row but well so

00:53:56,780 --> 00:54:01,700
we still have some idle transitions

00:53:58,660 --> 00:54:02,570
because at that moment the search didn't

00:54:01,700 --> 00:54:05,960
find any

00:54:02,570 --> 00:54:07,430
Orvil or CPUs now you know scheduling is

00:54:05,960 --> 00:54:10,340
by its nature a tremendously racy

00:54:07,430 --> 00:54:11,480
problem right at that instant there

00:54:10,340 --> 00:54:13,430
might have been nothing available to

00:54:11,480 --> 00:54:14,780
steal but then the moment you stop

00:54:13,430 --> 00:54:16,880
looking maybe something becomes runnable

00:54:14,780 --> 00:54:19,810
when and you know therefore that's why

00:54:16,880 --> 00:54:24,040
we don't hit perfect 100% utilization

00:54:19,810 --> 00:54:27,290
follow if nobody else is it primarily

00:54:24,040 --> 00:54:31,130
addressing bursty workloads or or

00:54:27,290 --> 00:54:34,990
sustained high workloads what would what

00:54:31,130 --> 00:54:34,990
do you think would be most benefited

00:54:35,260 --> 00:54:40,510
it certainly helps a lot with the bursty

00:54:37,480 --> 00:54:42,930
workloads because the existing load

00:54:40,510 --> 00:54:45,610
mounts mechanisms because of the cost

00:54:42,930 --> 00:54:47,140
balance a bit more coarsely and so

00:54:45,610 --> 00:54:49,960
they're they're not going to move things

00:54:47,140 --> 00:54:52,420
around at a high frequency

00:54:49,960 --> 00:54:55,590
but again in general the stealing will

00:54:52,420 --> 00:54:58,960
benefit kind of normal context rate

00:54:55,590 --> 00:55:00,790
workloads as well because it's you know

00:54:58,960 --> 00:55:02,320
you just never have to wait to run

00:55:00,790 --> 00:55:11,170
another thing you can very quickly find

00:55:02,320 --> 00:55:12,550
the next a runnable task okay so I'm

00:55:11,170 --> 00:55:15,940
matching for your schemes

00:55:12,550 --> 00:55:19,240
I know balance a overheads goes like

00:55:15,940 --> 00:55:21,100
goes down quite a bit it does that's

00:55:19,240 --> 00:55:25,780
right so the measured average cost the

00:55:21,100 --> 00:55:27,340
biobalance sure it shrinks and as a

00:55:25,780 --> 00:55:29,950
result which can actually be a good

00:55:27,340 --> 00:55:31,960
thing because it gives idle balance

00:55:29,950 --> 00:55:34,990
future chances to run more often and

00:55:31,960 --> 00:55:39,460
maybe pull things across Numa notes do

00:55:34,990 --> 00:55:42,310
you have like quantifications of like

00:55:39,460 --> 00:55:48,790
how much time that you spending I know

00:55:42,310 --> 00:55:51,340
balance before and after is they not

00:55:48,790 --> 00:55:55,660
broken out but idle balance is included

00:55:51,340 --> 00:55:58,810
in the percent fine column cost there so

00:55:55,660 --> 00:56:00,840
not a dramatic difference right at least

00:55:58,810 --> 00:56:04,150
at the gross level of you know the

00:56:00,840 --> 00:56:06,660
billions of cycles of CPU time I have

00:56:04,150 --> 00:56:06,660
per second

00:56:06,700 --> 00:56:12,470
just one last question sorry yes you

00:56:09,049 --> 00:56:15,770
have data for any real workloads apart

00:56:12,470 --> 00:56:17,869
from the synthetic benchmarks yes as I

00:56:15,770 --> 00:56:19,970
mentioned before probably the most

00:56:17,869 --> 00:56:24,170
complex thing we've run is the Oracle

00:56:19,970 --> 00:56:27,020
database on OLTP workloads on a large

00:56:24,170 --> 00:56:28,430
multi node system and across a variety

00:56:27,020 --> 00:56:30,020
of low levels there

00:56:28,430 --> 00:56:33,650
I'm trying remember the day that but

00:56:30,020 --> 00:56:34,880
we're seeing good you know middle maybe

00:56:33,650 --> 00:56:37,880
up to 80% or so

00:56:34,880 --> 00:56:39,410
improvements in throughput and we'll get

00:56:37,880 --> 00:56:45,410
better on that that that setup was

00:56:39,410 --> 00:56:47,990
somewhat restricted so you you're

00:56:45,410 --> 00:56:50,240
bypassing the current idle a balance

00:56:47,990 --> 00:56:52,700
when that when the CPU become idle where

00:56:50,240 --> 00:56:56,119
we currently have a way to be to pull

00:56:52,700 --> 00:56:59,210
some tasks you are creating a new one do

00:56:56,119 --> 00:57:00,410
you know if this better result are

00:56:59,210 --> 00:57:02,900
coming from the fact that you are

00:57:00,410 --> 00:57:04,670
skipping the test with average idle oh

00:57:02,900 --> 00:57:07,849
it's only because you have a simple

00:57:04,670 --> 00:57:09,829
policy to pull one waiting task I mean

00:57:07,849 --> 00:57:12,410
have you try for example to to remove

00:57:09,829 --> 00:57:14,599
this constraint of as soon as we are

00:57:12,410 --> 00:57:17,059
above the average idle we we stop the

00:57:14,599 --> 00:57:19,730
current idle balance yes I have

00:57:17,059 --> 00:57:22,329
experimented with that and you know the

00:57:19,730 --> 00:57:25,789
the current policies and thresholds are

00:57:22,329 --> 00:57:27,829
very well tuned for for a large search

00:57:25,789 --> 00:57:29,569
you know you start messing them and tell

00:57:27,829 --> 00:57:32,960
them do more work and change the

00:57:29,569 --> 00:57:35,029
thresholds and and the search cost goes

00:57:32,960 --> 00:57:37,400
way up and the throughput goes way down

00:57:35,029 --> 00:57:39,260
so so yeah with I played a lot with the

00:57:37,400 --> 00:57:40,849
parameters the existing framework and

00:57:39,260 --> 00:57:41,990
didn't make any progress and improving

00:57:40,849 --> 00:57:45,130
performance till we went to this

00:57:41,990 --> 00:57:45,130
alternate method

00:57:45,869 --> 00:57:51,660
you mentioned that currently new

00:57:49,540 --> 00:57:56,470
runnable tasks can be scheduled on a

00:57:51,660 --> 00:57:58,630
separate difference if you write what on

00:57:56,470 --> 00:58:01,359
separate differences are yer honorable

00:57:58,630 --> 00:58:03,310
tasks can be scheduled on a different

00:58:01,359 --> 00:58:05,980
non not current CPU

00:58:03,310 --> 00:58:07,599
oh well it's right so so the yeah I mean

00:58:05,980 --> 00:58:10,089
the runnable task at some point in the

00:58:07,599 --> 00:58:11,800
past was cute and some cpu it's got a

00:58:10,089 --> 00:58:14,740
you know queue length greater than one

00:58:11,800 --> 00:58:19,240
and the CPU goes idle and he gets too

00:58:14,740 --> 00:58:22,210
skilled over no I mean when but new task

00:58:19,240 --> 00:58:25,180
becomes runnable yeah current code can

00:58:22,210 --> 00:58:27,970
decided to run it on a different view

00:58:25,180 --> 00:58:30,040
right away yes yeah so that's the push

00:58:27,970 --> 00:58:32,079
side I don't modify the push side they

00:58:30,040 --> 00:58:34,300
all have this efficient stealing do we

00:58:32,079 --> 00:58:35,710
still need to schedule tasks on the

00:58:34,300 --> 00:58:37,480
difference EP you're just always

00:58:35,710 --> 00:58:38,079
schedule on the current and that the

00:58:37,480 --> 00:58:40,329
ceilin

00:58:38,079 --> 00:58:42,099
well you could do that I mean both push

00:58:40,329 --> 00:58:44,140
and pull are balancing mechanisms you

00:58:42,099 --> 00:58:46,630
could be lazy on the push side and let

00:58:44,140 --> 00:58:48,609
stealing pull the thing over yes

00:58:46,630 --> 00:58:50,589
however that you know that there's some

00:58:48,609 --> 00:58:52,450
costs around that you do have to walk a

00:58:50,589 --> 00:58:55,359
couple run queues to move things around

00:58:52,450 --> 00:58:57,369
it's better to make a more intelligent

00:58:55,359 --> 00:58:58,720
and efficient decision in both

00:58:57,369 --> 00:59:02,680
directions you get better performance

00:58:58,720 --> 00:59:04,000
overall and I mean as you saying way it

00:59:02,680 --> 00:59:06,520
would work but it wouldn't it would be a

00:59:04,000 --> 00:59:10,180
little less efficient only doing it from

00:59:06,520 --> 00:59:12,220
the pull side so you mentioned that this

00:59:10,180 --> 00:59:15,790
stealin doesn't still across Neuman owns

00:59:12,220 --> 00:59:18,069
right it could that that's a future

00:59:15,790 --> 00:59:20,230
extension and I'll talk about that in a

00:59:18,069 --> 00:59:22,510
minute actually so how does the balance

00:59:20,230 --> 00:59:24,490
in the cross Numa nodes happen idle

00:59:22,510 --> 00:59:26,560
balance is still called and it still

00:59:24,490 --> 00:59:30,400
traverses all the scheduling domains the

00:59:26,560 --> 00:59:33,520
top most of which is a Numa domain cross

00:59:30,400 --> 00:59:35,740
cross note okay so it happens if your

00:59:33,520 --> 00:59:38,440
new steal encode doesn't find anything

00:59:35,740 --> 00:59:40,690
the reverse I call the old the old

00:59:38,440 --> 00:59:43,720
balance code first and if that doesn't

00:59:40,690 --> 00:59:46,630
find a candidate I call my new one I see

00:59:43,720 --> 00:59:49,690
have you tried to do didn't like first

00:59:46,630 --> 00:59:52,450
rate yes I have I I have tried in the

00:59:49,690 --> 00:59:54,490
other way I didn't I didn't see much

00:59:52,450 --> 00:59:55,710
difference in the cases I did a deep

00:59:54,490 --> 00:59:57,150
dive on

00:59:55,710 --> 00:59:59,369
you know possibly fill it two more

00:59:57,150 --> 01:00:02,520
workloads we'd see one but so I didn't

00:59:59,369 --> 01:00:05,490
see any compelling reason to do it the

01:00:02,520 --> 01:00:07,109
new way first versus the old now in

01:00:05,490 --> 01:00:11,460
future directions I'll talk about ways

01:00:07,109 --> 01:00:13,349
to combine what auto balance does and

01:00:11,460 --> 01:00:16,710
what stealing does and maybe only have

01:00:13,349 --> 01:00:18,720
one mechanism less questions yeah you

01:00:16,710 --> 01:00:22,320
said that you pack the eight beats in

01:00:18,720 --> 01:00:23,190
the first bite of the that that's right

01:00:22,320 --> 01:00:26,760
that's right

01:00:23,190 --> 01:00:29,940
have you tried to just give each CPU one

01:00:26,760 --> 01:00:33,839
bite so that you have one I did um I did

01:00:29,940 --> 01:00:36,450
try that I tried one byte per CPU and

01:00:33,839 --> 01:00:38,910
updating without Atomics and it was a

01:00:36,450 --> 01:00:41,700
lot less efficient doing normal loads

01:00:38,910 --> 01:00:43,560
and stores and I think that was only on

01:00:41,700 --> 01:00:45,450
Xeon and try with other architectures

01:00:43,560 --> 01:00:47,130
but um but that was one of my really

01:00:45,450 --> 01:00:48,720
experiment so I set that aside and I

01:00:47,130 --> 01:00:52,200
want to look at that more but my

01:00:48,720 --> 01:00:53,760
suspicion is you know that the processor

01:00:52,200 --> 01:00:55,980
when you do an atomic you're actually

01:00:53,760 --> 01:00:57,510
telling the CPU something that it

01:00:55,980 --> 01:00:59,640
doesn't know when you're just doing

01:00:57,510 --> 01:01:01,500
normal loads and stores that there's a

01:00:59,640 --> 01:01:03,359
chance for it to optimize the update at

01:01:01,500 --> 01:01:05,310
the appropriate level and a cache

01:01:03,359 --> 01:01:07,290
hierarchy and I think that's what's

01:01:05,310 --> 01:01:10,109
going on there that's why these actual

01:01:07,290 --> 01:01:14,339
atomic updates on each bit is more

01:01:10,109 --> 01:01:15,869
efficient than a per byte update you

01:01:14,339 --> 01:01:17,700
know that the per byte is great for

01:01:15,869 --> 01:01:20,030
programming flexibility you know the

01:01:17,700 --> 01:01:23,460
programmer doesn't have to think about

01:01:20,030 --> 01:01:25,109
you know Stumm data sharing and how its

01:01:23,460 --> 01:01:27,150
shared the cache hierarchy does the

01:01:25,109 --> 01:01:29,160
right thing moving things around moving

01:01:27,150 --> 01:01:32,250
things around every time you set a byte

01:01:29,160 --> 01:01:34,080
but the atomic can you know make the

01:01:32,250 --> 01:01:36,089
update at a higher level the higher our

01:01:34,080 --> 01:01:38,280
cache hierarchy that's then seem shared

01:01:36,089 --> 01:01:40,490
by everyone and it just turns out to be

01:01:38,280 --> 01:01:40,490
better

01:01:41,330 --> 01:01:46,080
so sorry I came in late so I didn't I

01:01:44,700 --> 01:01:47,850
didn't get to make this comment earlier

01:01:46,080 --> 01:01:50,250
in describing this farce bitmask Sark

01:01:47,850 --> 01:01:51,690
over here yes but what you described

01:01:50,250 --> 01:01:53,340
sounds exactly like a data structure

01:01:51,690 --> 01:01:55,500
that we added for the multi cue block

01:01:53,340 --> 01:02:00,120
layer so I don't know if you came across

01:01:55,500 --> 01:02:03,840
lip spit map so what is called lib what

01:02:00,120 --> 01:02:06,060
not what sbit Omega P so sparse P okay

01:02:03,840 --> 01:02:07,380
basically yeah so it'd be really cool if

01:02:06,060 --> 01:02:09,360
we could just reuse that code instead of

01:02:07,380 --> 01:02:11,430
having two implementations at the same

01:02:09,360 --> 01:02:13,970
idea sure the one avoid duplication of

01:02:11,430 --> 01:02:16,290
code yes sure cool thanks there was

01:02:13,970 --> 01:02:19,230
probably not the same there is an

01:02:16,290 --> 01:02:20,910
existing sparse set abstraction in the

01:02:19,230 --> 01:02:22,530
kernel but it's very different than what

01:02:20,910 --> 01:02:25,590
I have it's really designed around being

01:02:22,530 --> 01:02:27,660
able to sleep and wake on when bits are

01:02:25,590 --> 01:02:29,520
set in the set and there was no overlap

01:02:27,660 --> 01:02:30,960
between that my work I wasn't aware of

01:02:29,520 --> 01:02:31,650
the abstraction you're talking about all

01:02:30,960 --> 01:02:34,340
yes

01:02:31,650 --> 01:02:36,210
so the at least the the sparse payment

01:02:34,340 --> 01:02:38,220
implementation that we have it has the

01:02:36,210 --> 01:02:39,840
two modes of operation there's just the

01:02:38,220 --> 01:02:41,880
sparse bitmap and there's a sparse map

01:02:39,840 --> 01:02:43,440
with a wait queue on top of it but you

01:02:41,880 --> 01:02:46,200
can use this part sparse bitmap by

01:02:43,440 --> 01:02:49,670
itself oh you do have awake you then

01:02:46,200 --> 01:02:49,670
maybe I was looking at that maybe

01:02:54,040 --> 01:02:59,620
okay well we'll figure it offline but

01:02:56,650 --> 01:03:00,940
but bit point chicken yeah okay all

01:02:59,620 --> 01:03:02,710
right we're actually we're coming close

01:03:00,940 --> 01:03:04,090
on time and to stay in schedule I should

01:03:02,710 --> 01:03:09,250
probably get through the last few slides

01:03:04,090 --> 01:03:11,080
here so the new Mishu so I wanted to

01:03:09,250 --> 01:03:13,900
avoid Numa issues by sticking to the

01:03:11,080 --> 01:03:16,300
local level cache and you know Quan

01:03:13,900 --> 01:03:18,610
hopes but it didn't work that way so it

01:03:16,300 --> 01:03:20,410
turns out that the stealing code caused

01:03:18,610 --> 01:03:22,450
some regression and hack bench on the

01:03:20,410 --> 01:03:24,850
large Numa systems like eight node

01:03:22,450 --> 01:03:27,520
systems and you know very very puzzling

01:03:24,850 --> 01:03:29,320
why should it do that and as you as I

01:03:27,520 --> 01:03:31,300
looked into it I can see that the CPU

01:03:29,320 --> 01:03:33,370
time have pretty much increased across

01:03:31,300 --> 01:03:35,530
the board you know all functions kernel

01:03:33,370 --> 01:03:38,290
usual and you know uniformly inflated

01:03:35,530 --> 01:03:39,790
it's just taking longer and looking even

01:03:38,290 --> 01:03:41,950
further I can see that that was

01:03:39,790 --> 01:03:44,970
correlated with more cross node

01:03:41,950 --> 01:03:47,800
migrations of these hacked bench tasks

01:03:44,970 --> 01:03:49,870
now why should that be well it's this

01:03:47,800 --> 01:03:53,710
little bit of code in the wakeup path

01:03:49,870 --> 01:03:56,890
where we we wake and so we have a waking

01:03:53,710 --> 01:03:59,260
task wake someone else and we're making

01:03:56,890 --> 01:04:01,750
decision the way key who's come runnable

01:03:59,260 --> 01:04:03,550
is he gonna run on the place that the

01:04:01,750 --> 01:04:05,530
Waker just woke him up or is he gonna go

01:04:03,550 --> 01:04:07,960
in as previous CPU so we make that

01:04:05,530 --> 01:04:09,820
decision and the code looks and says you

01:04:07,960 --> 01:04:12,550
know essentially looks as well as the

01:04:09,820 --> 01:04:15,220
previous CPU idle is this current CPU

01:04:12,550 --> 01:04:16,840
almost idle and by almost idle there's

01:04:15,220 --> 01:04:20,560
this thing called a synchronous wakeup

01:04:16,840 --> 01:04:24,160
where the assumption is if say offender

01:04:20,560 --> 01:04:25,330
is waking receiver that the Senators

01:04:24,160 --> 01:04:28,510
going to be done very soon and gonna

01:04:25,330 --> 01:04:30,310
gouache CPU and therefore on the

01:04:28,510 --> 01:04:32,680
sender's CPU if he's the only one

01:04:30,310 --> 01:04:34,630
running he's gonna go off that's gonna

01:04:32,680 --> 01:04:36,250
go idle very soon so it makes a lot of

01:04:34,630 --> 01:04:38,290
sense to put the receiver there and get

01:04:36,250 --> 01:04:40,360
him to take his place and pick it up in

01:04:38,290 --> 01:04:41,920
the l1 cache and all is wonderful so

01:04:40,360 --> 01:04:44,680
there's that logic it it works well

01:04:41,920 --> 01:04:47,140
however what happens with stealing is

01:04:44,680 --> 01:04:49,450
that this condition you know the number

01:04:47,140 --> 01:04:52,030
running on the CPU is one is to more

01:04:49,450 --> 01:04:54,940
often stealing for any given number of

01:04:52,030 --> 01:04:56,950
tasks space versus the steal kernel the

01:04:54,940 --> 01:04:58,810
load is just smoother this n are running

01:04:56,950 --> 01:05:01,690
the one condition is to more often and

01:04:58,810 --> 01:05:04,720
so we make this decision to move to the

01:05:01,690 --> 01:05:07,040
you know wakers CPU more often and and

01:05:04,720 --> 01:05:08,630
this is agnostic about Numa it'll

01:05:07,040 --> 01:05:10,280
move things right across the pneuma node

01:05:08,630 --> 01:05:13,310
and that's what's happening so we're

01:05:10,280 --> 01:05:17,960
seeing more migrations in the stealing

01:05:13,310 --> 01:05:22,100
kernel and and it's lower so - yes

01:05:17,960 --> 01:05:24,020
general yeah okay so move right on so

01:05:22,100 --> 01:05:27,350
anyway so I've had to disable this for

01:05:24,020 --> 01:05:28,910
more than two Numa nodes I suspect this

01:05:27,350 --> 01:05:31,880
is really a hack bench effect and

01:05:28,910 --> 01:05:33,740
realistic workloads will run just fine

01:05:31,880 --> 01:05:36,920
and actually love people's help in

01:05:33,740 --> 01:05:39,050
evaluating this theory and so if you

01:05:36,920 --> 01:05:41,150
download my patches and try it there's a

01:05:39,050 --> 01:05:43,130
command-line tunable you can set a

01:05:41,150 --> 01:05:46,070
schedule node limit and that's the

01:05:43,130 --> 01:05:47,810
number of nodes above which stealing is

01:05:46,070 --> 01:05:50,030
disabled so set that to eight or higher

01:05:47,810 --> 01:05:51,250
and give things a try and tell me what

01:05:50,030 --> 01:05:53,960
you try No

01:05:51,250 --> 01:05:56,540
okay and these are details of that I'm

01:05:53,960 --> 01:05:58,640
not going to go into so future work so

01:05:56,540 --> 01:06:00,860
the current patches I told you what it

01:05:58,640 --> 01:06:02,510
does but possible extensions these same

01:06:00,860 --> 01:06:06,140
principles the sparse set could be

01:06:02,510 --> 01:06:07,850
applied in the RT task load balancing it

01:06:06,140 --> 01:06:10,640
already looks for overloaded CPUs it

01:06:07,850 --> 01:06:13,090
could be made more efficient we could we

01:06:10,640 --> 01:06:15,950
could combine stealing an idle balance

01:06:13,090 --> 01:06:18,110
there trying to do the same thing and

01:06:15,950 --> 01:06:21,080
whereas now the stealing tracks

01:06:18,110 --> 01:06:23,540
overloaded CPUs on a per LLC basis as

01:06:21,080 --> 01:06:24,920
you traverse the scheduling domains you

01:06:23,540 --> 01:06:27,770
could look for overlap between your

01:06:24,920 --> 01:06:31,490
domain and the appropriate / LLC set and

01:06:27,770 --> 01:06:33,380
decide which overloaded CPU set to

01:06:31,490 --> 01:06:35,390
traverse and pick a candidate from so

01:06:33,380 --> 01:06:38,780
some day perhaps we could replace the

01:06:35,390 --> 01:06:40,520
existing idle balance with that arm has

01:06:38,780 --> 01:06:43,850
this well arm and other architectures

01:06:40,520 --> 01:06:46,010
have this feature where some CPUs are

01:06:43,850 --> 01:06:48,200
faster than others and as a result you

01:06:46,010 --> 01:06:50,150
know these so-called misfit tasks where

01:06:48,200 --> 01:06:52,760
task is the only guy running on a CPU

01:06:50,150 --> 01:06:54,980
but it's running in a slow CPU and a

01:06:52,760 --> 01:06:57,830
fast CPU Michael idle well you like that

01:06:54,980 --> 01:06:59,300
fast CPU bill pick this guy up except

01:06:57,830 --> 01:07:00,920
the the target here has an hour run

01:06:59,300 --> 01:07:03,350
equal one it doesn't look low overloaded

01:07:00,920 --> 01:07:06,200
so you could possibly extend this logic

01:07:03,350 --> 01:07:08,720
to have a second set of you know an hour

01:07:06,200 --> 01:07:10,670
running equal one things on slow CPUs

01:07:08,720 --> 01:07:15,020
and they're stealing candidates when a

01:07:10,670 --> 01:07:16,820
fast CPU goes idle and I'm gonna skip a

01:07:15,020 --> 01:07:18,320
few these for time and lastly the sparse

01:07:16,820 --> 01:07:19,849
mask could be applied in other places

01:07:18,320 --> 01:07:22,430
you can imagine using

01:07:19,849 --> 01:07:24,529
on the push side as well to make the

01:07:22,430 --> 01:07:27,499
search for idle cores and CPUs more

01:07:24,529 --> 01:07:29,630
efficient so um all these things could

01:07:27,499 --> 01:07:32,839
each map to a task sometime in future

01:07:29,630 --> 01:07:34,819
and I hope to pursue those so that's it

01:07:32,839 --> 01:07:35,930
Daniel tells me we're out of time so if

01:07:34,819 --> 01:07:39,490
you want to talk more I should probably

01:07:35,930 --> 01:07:46,110
do it after in the hall so thank you

01:07:39,490 --> 01:07:46,110
[Applause]

01:08:41,389 --> 01:08:44,389
hello

01:08:58,230 --> 01:09:05,259
stuff like that okay uh hello everyone

01:09:02,319 --> 01:09:08,109
I'm sure grandmas under so I will talk

01:09:05,259 --> 01:09:11,730
more about the scheduler scalability on

01:09:08,109 --> 01:09:17,170
the push side that touched a little bit

01:09:11,730 --> 01:09:18,869
and so I focus mostly on the context

01:09:17,170 --> 01:09:21,489
which intensive workloads where

01:09:18,869 --> 01:09:23,559
essentially the schedule that is pushed

01:09:21,489 --> 01:09:26,319
to the limit and by context which

01:09:23,559 --> 01:09:29,049
intensive I mean where workload sweat

01:09:26,319 --> 01:09:30,369
threads are waking up doing a little bit

01:09:29,049 --> 01:09:32,380
of computation and then going to sleep

01:09:30,369 --> 01:09:37,839
and that's happening like all over the

01:09:32,380 --> 01:09:40,179
place and like lots of threads so on the

01:09:37,839 --> 01:09:41,559
push side actually there are like two

01:09:40,179 --> 01:09:44,109
paths the current to the real and

01:09:41,559 --> 01:09:47,289
scheduler follows there is one slow path

01:09:44,109 --> 01:09:51,339
I think people call it where it is taken

01:09:47,289 --> 01:09:53,139
if it's a new exact task like in for

01:09:51,339 --> 01:09:54,610
nearly four tasks or there is no

01:09:53,139 --> 01:09:58,449
affinity between the Waker and the wiki

01:09:54,610 --> 01:10:02,170
and the other path is we call the fast

01:09:58,449 --> 01:10:05,590
path where it says much faster is just

01:10:02,170 --> 01:10:09,070
trying to find an idols with you as soon

01:10:05,590 --> 01:10:11,079
as possible to schedule newly woken up

01:10:09,070 --> 01:10:14,949
tasks so that's the path mostly I'm

01:10:11,079 --> 01:10:17,050
focusing on today so it starts for the

01:10:14,949 --> 01:10:19,360
fair class it actually the story starts

01:10:17,050 --> 01:10:21,190
with the call to select idle sibling

01:10:19,360 --> 01:10:24,969
which is essentially trying to find the

01:10:21,190 --> 01:10:27,880
best possible CPU in the last level

01:10:24,969 --> 01:10:30,130
cache domain so its first tries to find

01:10:27,880 --> 01:10:32,380
an fully idle core because obviously

01:10:30,130 --> 01:10:35,289
like who doesn't like a fully idle core

01:10:32,380 --> 01:10:37,270
it's a basic you any see between that

01:10:35,289 --> 01:10:39,789
calls the best to run and if it fails

01:10:37,270 --> 01:10:43,539
then it tries to find any idle CPU in

01:10:39,789 --> 01:10:46,119
the air LC domain and then it again

01:10:43,539 --> 01:10:47,889
tries back and tries to find another

01:10:46,119 --> 01:10:51,159
idle sweep you like last try in the

01:10:47,889 --> 01:10:52,929
local core I think because the logic

01:10:51,159 --> 01:10:54,550
there is like you have spent some time

01:10:52,929 --> 01:10:57,550
searching the LLC dome and maybe in the

01:10:54,550 --> 01:10:59,110
meantime something got idle in the local

01:10:57,550 --> 01:11:01,329
course you just give one more try and

01:10:59,110 --> 01:11:03,239
believe me or not this actually the

01:11:01,329 --> 01:11:04,579
selector innocently has some effect

01:11:03,239 --> 01:11:11,010
if you remove it you will see some

01:11:04,579 --> 01:11:12,869
difference so currently the soil focus

01:11:11,010 --> 01:11:15,449
on select Idol code and select I'll CPU

01:11:12,869 --> 01:11:18,749
the select Idol core it can actually I

01:11:15,449 --> 01:11:21,840
trait basically all the signals in LC

01:11:18,749 --> 01:11:24,239
domain so it basically is scan score by

01:11:21,840 --> 01:11:26,159
core and effectively finds a core where

01:11:24,239 --> 01:11:30,449
all the signals are either it basically

01:11:26,159 --> 01:11:33,989
returns from that and after that comes

01:11:30,449 --> 01:11:36,269
the Select Idol CPU which also can

01:11:33,989 --> 01:11:39,989
actually end up in the worst case

01:11:36,269 --> 01:11:42,749
scanning the enter LLC domain so it

01:11:39,989 --> 01:11:44,550
actually tries to be intelligent but I

01:11:42,749 --> 01:11:47,760
am actually not fully convinced by the

01:11:44,550 --> 01:11:51,030
current logic now so it either its inner

01:11:47,760 --> 01:11:53,849
CPUs where n is determined by the the

01:11:51,030 --> 01:11:55,949
average idle time of the the current CPU

01:11:53,849 --> 01:11:58,590
and also the average cost of scanning

01:11:55,949 --> 01:12:00,780
the LLC domain and the logic right now

01:11:58,590 --> 01:12:02,849
is you scan up to the point where it is

01:12:00,780 --> 01:12:06,539
equal to the time the average idle time

01:12:02,849 --> 01:12:09,389
of the CPU and on top of that so this I

01:12:06,539 --> 01:12:11,969
am I I don't see like what is the logic

01:12:09,389 --> 01:12:14,400
in that why it should really depend on

01:12:11,969 --> 01:12:17,360
the on these two things because when a

01:12:14,400 --> 01:12:19,710
task is woken up you essentially want to

01:12:17,360 --> 01:12:21,659
put it on some idle secure as soon as

01:12:19,710 --> 01:12:23,730
possible and that shouldn't depend on

01:12:21,659 --> 01:12:26,130
what is the average idle time of the CPU

01:12:23,730 --> 01:12:28,380
that is including the task so I don't

01:12:26,130 --> 01:12:30,179
I'm not fully convinced there and I

01:12:28,380 --> 01:12:33,230
remember there were some email exchanges

01:12:30,179 --> 01:12:35,940
with Peter some time back and he kind of

01:12:33,230 --> 01:12:39,119
agreed and then he sent some of his own

01:12:35,940 --> 01:12:42,300
patches so so there is some contention

01:12:39,119 --> 01:12:44,880
point there and it also has an arbitrary

01:12:42,300 --> 01:12:46,409
first factor right now it has some 512

01:12:44,880 --> 01:12:50,340
number and if you read the comments like

01:12:46,409 --> 01:12:52,079
it helps hack bench so it's like fine

01:12:50,340 --> 01:12:55,349
tuned for one particular benchmark it

01:12:52,079 --> 01:12:57,030
seems and in the worst case I think even

01:12:55,349 --> 01:12:59,699
with all these things going on it can

01:12:57,030 --> 01:13:06,510
end up scanning the entire socket which

01:12:59,699 --> 01:13:07,860
doesn't scale so so my my feeling is

01:13:06,510 --> 01:13:10,139
that it will be probably very hard

01:13:07,860 --> 01:13:11,920
firstly I am not convinced that this

01:13:10,139 --> 01:13:14,200
formula should be dynamic in

01:13:11,920 --> 01:13:16,330
in the first place and even if you try

01:13:14,200 --> 01:13:18,750
to do something intelligent and try to

01:13:16,330 --> 01:13:21,280
come up with some dynamic formula

01:13:18,750 --> 01:13:24,370
probably it will for some percentage of

01:13:21,280 --> 01:13:29,440
time it will not work really and if you

01:13:24,370 --> 01:13:31,690
step back ideally I feel that this cost

01:13:29,440 --> 01:13:33,220
time of searching should be actually

01:13:31,690 --> 01:13:35,140
constant time it should not really

01:13:33,220 --> 01:13:38,800
depend on kind of how loaded the system

01:13:35,140 --> 01:13:41,410
is and stuff like that so it's it should

01:13:38,800 --> 01:13:44,400
have some upper bound on the under

01:13:41,410 --> 01:13:48,280
search like constant upper bounds and

01:13:44,400 --> 01:13:50,350
what should the pounds be so essentially

01:13:48,280 --> 01:13:53,170
I started experimenting with a lot of

01:13:50,350 --> 01:13:55,480
hard coded numbers like a lower bound

01:13:53,170 --> 01:13:57,580
with two CPUs upper bound with four CPUs

01:13:55,480 --> 01:13:59,980
and I tried different architectures of

01:13:57,580 --> 01:14:02,530
assembly 2 which is essentially Intel

01:13:59,980 --> 01:14:06,100
machines and then we had some spark

01:14:02,530 --> 01:14:11,260
machines which is a 78 hyper threading

01:14:06,100 --> 01:14:14,200
and experimenting some I kind of

01:14:11,260 --> 01:14:16,270
realized that what works well is a lower

01:14:14,200 --> 01:14:19,960
bound of one code an upper bound of two

01:14:16,270 --> 01:14:21,970
cores so you may ask why I don't have a

01:14:19,960 --> 01:14:25,150
concrete answer to that but I have some

01:14:21,970 --> 01:14:27,130
feeling that this it's easy so a lot of

01:14:25,150 --> 01:14:29,920
the scheduler algorithm is essentially

01:14:27,130 --> 01:14:31,660
like working on domains like you have

01:14:29,920 --> 01:14:34,240
the core domain and the LC domain and

01:14:31,660 --> 01:14:36,100
stuff like that so some load balancing

01:14:34,240 --> 01:14:37,930
it could be that the domain next to you

01:14:36,100 --> 01:14:39,760
so the core is the lowest domain and

01:14:37,930 --> 01:14:41,920
could be that the condiment next to you

01:14:39,760 --> 01:14:44,440
kind of has a different load profile so

01:14:41,920 --> 01:14:47,080
it's usually a good idea to at least

01:14:44,440 --> 01:14:49,870
cross your current pool boundary when

01:14:47,080 --> 01:14:51,520
you are searching for an idle CPU but

01:14:49,870 --> 01:14:54,010
then you also don't want to search too

01:14:51,520 --> 01:14:55,900
much so to is kind of the and the

01:14:54,010 --> 01:14:57,370
minimum number that you allows you to go

01:14:55,900 --> 01:15:00,130
beyond the code level but still have

01:14:57,370 --> 01:15:03,580
like search a different code so that

01:15:00,130 --> 01:15:06,310
kind of when I try to I kind of worked

01:15:03,580 --> 01:15:12,520
on at least the spark and the Intel

01:15:06,310 --> 01:15:15,340
machines that I had so and the other

01:15:12,520 --> 01:15:16,239
thing is that the CPU ID animals and so

01:15:15,340 --> 01:15:18,849
on

01:15:16,239 --> 01:15:21,219
actually the CPU IDs are interleaved so

01:15:18,849 --> 01:15:23,139
CPU zero is one one accord and one is on

01:15:21,219 --> 01:15:25,659
the next core and it kind of hops

01:15:23,139 --> 01:15:28,090
whereas spark and certain other

01:15:25,659 --> 01:15:31,809
architectures I think have like and they

01:15:28,090 --> 01:15:34,329
exhaust one core first and then move on

01:15:31,809 --> 01:15:35,889
to the next course so but if you have an

01:15:34,329 --> 01:15:38,260
upper bound of one lower bound of one

01:15:35,889 --> 01:15:40,090
code and upper bound of two course any

01:15:38,260 --> 01:15:41,860
kind of possible CPU adding immersions

01:15:40,090 --> 01:15:48,309
you will always actually see more than

01:15:41,860 --> 01:15:50,610
one and search more than one core so but

01:15:48,309 --> 01:15:53,800
if you now the problem comes that if you

01:15:50,610 --> 01:15:57,579
bound the searches the problem is that

01:15:53,800 --> 01:15:59,440
you can be localized in an effect like

01:15:57,579 --> 01:16:01,090
if it is a loaded CPU are not searching

01:15:59,440 --> 01:16:04,530
enough so there could be some idle CPUs

01:16:01,090 --> 01:16:08,590
far away that you are not able to find

01:16:04,530 --> 01:16:10,469
so actually some of the this addition

01:16:08,590 --> 01:16:13,239
actually came from Steve when you are

01:16:10,469 --> 01:16:18,340
trying to do experiment with this stuff

01:16:13,239 --> 01:16:22,360
and what we found was if we we can just

01:16:18,340 --> 01:16:25,389
keep a per CPU variable to track the

01:16:22,360 --> 01:16:27,880
search limit of how much we have

01:16:25,389 --> 01:16:29,980
searched to in a very loaded case if you

01:16:27,880 --> 01:16:32,349
didn't find any idle CPU your persecute

01:16:29,980 --> 01:16:34,869
variable still like mark the end of your

01:16:32,349 --> 01:16:36,670
search and next time when you start

01:16:34,869 --> 01:16:38,650
searching again when you become a

01:16:36,670 --> 01:16:40,420
runnable you start searching from that

01:16:38,650 --> 01:16:42,130
so even though you were searching a

01:16:40,420 --> 01:16:44,650
small window you were kind of able to

01:16:42,130 --> 01:16:47,980
spread around your search window in the

01:16:44,650 --> 01:16:50,499
LLC domain so you don't get localized so

01:16:47,980 --> 01:16:53,860
these two techniques coupled together

01:16:50,499 --> 01:17:02,710
actually kind of worked well for most of

01:16:53,860 --> 01:17:05,409
the benchmarks I tried so now coming to

01:17:02,710 --> 01:17:07,179
the Select idle core which is which is

01:17:05,409 --> 01:17:10,469
the first one that the scheduler tries

01:17:07,179 --> 01:17:13,480
so it has currently a dynamic switch

01:17:10,469 --> 01:17:15,400
that it tries to disable the scanning

01:17:13,480 --> 01:17:16,929
essentially it flags like okay there are

01:17:15,400 --> 01:17:20,590
no idle course available so don't even

01:17:16,929 --> 01:17:22,690
bother but then it is still a bottleneck

01:17:20,590 --> 01:17:24,950
because you have one or a very low

01:17:22,690 --> 01:17:29,240
number of course it's still like goes on

01:17:24,950 --> 01:17:31,220
all the way searching so so can we have

01:17:29,240 --> 01:17:33,890
some efficient data structures to do it

01:17:31,220 --> 01:17:37,040
fast I haven't done exhaustive research

01:17:33,890 --> 01:17:40,550
on these but I'm somewhat skeptical

01:17:37,040 --> 01:17:43,090
because what I found was the scheduler

01:17:40,550 --> 01:17:45,760
fast but is is very very sensitive so

01:17:43,090 --> 01:17:47,990
any kind of data structures if you think

01:17:45,760 --> 01:17:50,000
if you were touching too many cache

01:17:47,990 --> 01:17:51,590
lines or you are doing any kind of

01:17:50,000 --> 01:17:55,190
atomic operation so anything can

01:17:51,590 --> 01:17:58,250
completely ruin your margins so it will

01:17:55,190 --> 01:18:00,620
hurt probably more than help you so what

01:17:58,250 --> 01:18:03,080
I found was actually just disabling the

01:18:00,620 --> 01:18:06,230
idle core search on this context which

01:18:03,080 --> 01:18:09,290
intensive workloads particularly on the

01:18:06,230 --> 01:18:12,200
x86 the Intel machines that we had

01:18:09,290 --> 01:18:14,570
actually improves so that clearly proves

01:18:12,200 --> 01:18:16,540
that it hurts more than currently the

01:18:14,570 --> 01:18:19,820
state hurt more hurts more than it helps

01:18:16,540 --> 01:18:23,420
so the idea was basically can we have

01:18:19,820 --> 01:18:25,550
then a new scheduler feature so

01:18:23,420 --> 01:18:28,400
scheduler already had like four or five

01:18:25,550 --> 01:18:31,220
existing features so I just wanted to

01:18:28,400 --> 01:18:33,980
add one more to that so that we can

01:18:31,220 --> 01:18:38,780
visible the idle core search in the

01:18:33,980 --> 01:18:42,140
runtime and it improves most workloads

01:18:38,780 --> 01:18:44,150
regresses some for example the hack

01:18:42,140 --> 01:18:45,530
pinch on SPARC I found that actually

01:18:44,150 --> 01:18:49,850
discipling the idle core search

01:18:45,530 --> 01:18:51,980
completely can regress a little bit like

01:18:49,850 --> 01:18:59,720
compared to just with these elect a

01:18:51,980 --> 01:19:03,260
discipline selected recipients so here

01:18:59,720 --> 01:19:06,680
are some numbers I have these are the

01:19:03,260 --> 01:19:10,310
Intel x86 machines we have total 44

01:19:06,680 --> 01:19:12,350
cores and 88 CPUs so I have had bench

01:19:10,310 --> 01:19:13,610
and then some networking workload

01:19:12,350 --> 01:19:16,700
although it is running in the same like

01:19:13,610 --> 01:19:18,800
single node you perf this message

01:19:16,700 --> 01:19:22,040
messaging back and forth with 8 kilobyte

01:19:18,800 --> 01:19:24,920
and then of course the workload we care

01:19:22,040 --> 01:19:28,160
about the most is the Oracle database

01:19:24,920 --> 01:19:32,300
well DPR TPCC which is of ATP workload

01:19:28,160 --> 01:19:35,000
so so you might not be impressed by the

01:19:32,300 --> 01:19:38,300
improvements a lot

01:19:35,000 --> 01:19:40,220
especially like hack bench it's a there

01:19:38,300 --> 01:19:42,410
are some big improvements and they're

01:19:40,220 --> 01:19:45,610
like one big improvement and then mostly

01:19:42,410 --> 01:19:48,560
like and the lower single-digit same

01:19:45,610 --> 01:19:53,660
slightly better for being you perf and

01:19:48,560 --> 01:19:54,980
then in the Oracle database you'll see

01:19:53,660 --> 01:19:56,330
like some improvements showing up

01:19:54,980 --> 01:20:01,580
towards the higher end where the number

01:19:56,330 --> 01:20:04,460
of users are more so I think like the

01:20:01,580 --> 01:20:06,680
effect of the Select ideally CPU is most

01:20:04,460 --> 01:20:09,020
likely showing up towards the higher

01:20:06,680 --> 01:20:10,580
because the dynamic switching is

01:20:09,020 --> 01:20:12,080
probably filtering out most of the

01:20:10,580 --> 01:20:14,110
select idle core things so that is

01:20:12,080 --> 01:20:17,120
probably not making a different good

01:20:14,110 --> 01:20:22,910
baseline and under the patches so this

01:20:17,120 --> 01:20:27,980
is just with select idle CPU changes so

01:20:22,910 --> 01:20:30,260
next I tried with the new scheduler

01:20:27,980 --> 01:20:33,050
feature so basically it is a valid using

01:20:30,260 --> 01:20:36,590
no see score and you can see that the

01:20:33,050 --> 01:20:38,810
improvements have increased a lot again

01:20:36,590 --> 01:20:40,400
probably hack bench not so much and by

01:20:38,810 --> 01:20:42,350
the way I have tried on some other

01:20:40,400 --> 01:20:43,730
machines like slightly smaller machines

01:20:42,350 --> 01:20:46,880
and it hat which actually shows like

01:20:43,730 --> 01:20:50,600
much higher improvement like crossing

01:20:46,880 --> 01:20:52,550
10% easily on many number of groups and

01:20:50,600 --> 01:20:54,670
you can see like the you perf

01:20:52,550 --> 01:20:58,700
improvement is also quite a bit and

01:20:54,670 --> 01:21:01,060
again this time the Oracle database TPCC

01:20:58,700 --> 01:21:04,640
workloads work red actually is showing

01:21:01,060 --> 01:21:08,300
something substantial like more across

01:21:04,640 --> 01:21:09,890
the chart and this is not like yeah at

01:21:08,300 --> 01:21:16,970
least like row goes up to beyond four

01:21:09,890 --> 01:21:20,540
percent so so I'll stop here if there

01:21:16,970 --> 01:21:23,000
are any questions about this because I

01:21:20,540 --> 01:21:25,580
think that yeah the current state of the

01:21:23,000 --> 01:21:28,760
push side is not really optimized right

01:21:25,580 --> 01:21:30,890
now so we need to look at it you can we

01:21:28,760 --> 01:21:32,300
can spend a lot of time trying a lot of

01:21:30,890 --> 01:21:36,230
heuristics which I actually have done

01:21:32,300 --> 01:21:40,130
but I found that doing easier things

01:21:36,230 --> 01:21:42,950
like this is actually like it gives more

01:21:40,130 --> 01:21:44,420
stability and you actually probably get

01:21:42,950 --> 01:21:46,810
the improvement

01:21:44,420 --> 01:21:49,130
trying to be super smart and like

01:21:46,810 --> 01:21:52,880
complicated data structures and and

01:21:49,130 --> 01:21:56,780
ending up hunting so this is I I just

01:21:52,880 --> 01:21:58,880
wanted to probably also point Peter that

01:21:56,780 --> 01:22:01,550
way we need to look at this side a

01:21:58,880 --> 01:22:04,070
little bit more I have sent the patches

01:22:01,550 --> 01:22:05,780
of swim you can find there are some

01:22:04,070 --> 01:22:07,820
initial discussions with Peter and some

01:22:05,780 --> 01:22:09,680
other folks back and forth and then yeah

01:22:07,820 --> 01:22:14,300
it kind of I haven't heard back after I

01:22:09,680 --> 01:22:18,940
sent like v2 so I can go back and start

01:22:14,300 --> 01:22:22,820
the discussion again after it done now

01:22:18,940 --> 01:22:25,430
switching gears a little bit so so we

01:22:22,820 --> 01:22:27,470
are spending so much time optimizing the

01:22:25,430 --> 01:22:30,260
scheduler the sleep wake up and all

01:22:27,470 --> 01:22:32,540
those paths and it occurs to me that on

01:22:30,260 --> 01:22:34,970
some cases we should just plain avoid it

01:22:32,540 --> 01:22:37,970
so what are the cases so if if you

01:22:34,970 --> 01:22:39,440
profile your benchmark so this is my

01:22:37,970 --> 01:22:43,310
opinion that if you profile your page

01:22:39,440 --> 01:22:44,600
marked and you find like most likely I

01:22:43,310 --> 01:22:47,540
have seen like there are a few common

01:22:44,600 --> 01:22:49,790
hot stacks which much like calling

01:22:47,540 --> 01:22:52,460
schedule and in particular like if your

01:22:49,790 --> 01:22:54,620
benchmark has one stack that is kind of

01:22:52,460 --> 01:22:57,800
dominating and calling schedule all the

01:22:54,620 --> 01:23:00,410
time and that is also a kind of a

01:22:57,800 --> 01:23:02,960
well-defined point like in this case I

01:23:00,410 --> 01:23:04,430
am pointing to pipe read and write it

01:23:02,960 --> 01:23:08,020
can be either like reading from file

01:23:04,430 --> 01:23:10,640
system or reading from net Network I

01:23:08,020 --> 01:23:13,220
think we should just avoid have some

01:23:10,640 --> 01:23:14,900
mechanism to PC wait and avoid the enter

01:23:13,220 --> 01:23:16,250
over it all together and this is not

01:23:14,900 --> 01:23:18,470
just the scheduling overhead there is

01:23:16,250 --> 01:23:20,270
context switch over here as is going

01:23:18,470 --> 01:23:22,010
into the sleep queue and like you're

01:23:20,270 --> 01:23:25,720
moving from that I'm just about to enter

01:23:22,010 --> 01:23:28,670
stuff and currently I found that so

01:23:25,720 --> 01:23:30,380
whereas profiling the OLTP stuff the

01:23:28,670 --> 01:23:35,060
pipe thing actually showed up at the top

01:23:30,380 --> 01:23:39,550
and so I thought that we should probably

01:23:35,060 --> 01:23:39,550
look into there is a question there

01:23:42,220 --> 01:23:47,890
one thing that concerns me littlest all

01:23:45,940 --> 01:23:49,620
the tests that you did are with the

01:23:47,890 --> 01:23:53,590
system that is essentially fully

01:23:49,620 --> 01:23:55,540
utilized and a lot of workloads out

01:23:53,590 --> 01:23:59,620
there for the system is maybe 50 60

01:23:55,540 --> 01:24:02,470
percent busy and so it was good response

01:23:59,620 --> 01:24:04,620
times for requests that come in and

01:24:02,470 --> 01:24:07,870
finding an idle core for that might be

01:24:04,620 --> 01:24:13,240
more you so 40 or I don't have the idle

01:24:07,870 --> 01:24:15,970
times here for the Oracle database TPCC

01:24:13,240 --> 01:24:18,280
runs as far as I remember for 20 users

01:24:15,970 --> 01:24:19,810
it was like a lot of idleness it was

01:24:18,280 --> 01:24:22,050
probably like more than 50 percent idle

01:24:19,810 --> 01:24:25,300
and then that's why I kind of varied it

01:24:22,050 --> 01:24:27,790
up to all the way up to 220 where I

01:24:25,300 --> 01:24:29,290
think the idleness is 80 percent or

01:24:27,790 --> 01:24:30,610
something like that like beyond a point

01:24:29,290 --> 01:24:33,130
I cannot drive it anymore

01:24:30,610 --> 01:24:36,730
kind of the throughput plateaued at 220

01:24:33,130 --> 01:24:38,800
users so I kind of stopped there so at

01:24:36,730 --> 01:24:44,970
least for the TPCC I think the idleness

01:24:38,800 --> 01:24:47,920
range powers a lot so coober as I recall

01:24:44,970 --> 01:24:51,610
you did some experiments keeping the

01:24:47,920 --> 01:24:55,650
existing comparison of cost to idle time

01:24:51,610 --> 01:24:57,910
and applying the remembered search there

01:24:55,650 --> 01:24:59,530
right so in other words you left you let

01:24:57,910 --> 01:25:01,630
the logic search as much as it currently

01:24:59,530 --> 01:25:02,980
does but it would stop and then you

01:25:01,630 --> 01:25:07,900
picked up where you left off next time

01:25:02,980 --> 01:25:10,810
around did that give a lot of things

01:25:07,900 --> 01:25:12,790
like that but this one was the best as

01:25:10,810 --> 01:25:15,760
far as I remember I was like trying all

01:25:12,790 --> 01:25:18,640
different combinations like removing

01:25:15,760 --> 01:25:20,140
idle search and as Steve said plus that

01:25:18,640 --> 01:25:21,790
and then like they're like seven or

01:25:20,140 --> 01:25:24,130
eight different combinations I tried and

01:25:21,790 --> 01:25:26,170
what jumped out was this one kind of

01:25:24,130 --> 01:25:28,270
this simple thing does the best performs

01:25:26,170 --> 01:25:30,600
the best so that's why I kind of stuck

01:25:28,270 --> 01:25:30,600
with it

01:25:34,660 --> 01:25:43,760
okay so some coming back to the pipe

01:25:40,310 --> 01:25:45,230
stuff so yeah I think for the pipe read

01:25:43,760 --> 01:25:48,920
and the right we should have some

01:25:45,230 --> 01:25:51,590
busybody mechanism at this actually has

01:25:48,920 --> 01:25:54,310
precedence so networking has already

01:25:51,590 --> 01:25:56,720
similar mechanism in place so I kind of

01:25:54,310 --> 01:25:58,010
copied that part of the code a little

01:25:56,720 --> 01:26:01,220
bit like just so how would they

01:25:58,010 --> 01:26:05,690
implemented it and it's a very small

01:26:01,220 --> 01:26:07,730
change and again like we might go down

01:26:05,690 --> 01:26:10,310
the rabbit hole thinking like can we

01:26:07,730 --> 01:26:12,560
have different dynamic formula of the

01:26:10,310 --> 01:26:14,210
optimal spin time like different

01:26:12,560 --> 01:26:16,570
workloads on different architectures can

01:26:14,210 --> 01:26:19,850
he be smart enough and this was like

01:26:16,570 --> 01:26:22,160
well government he had some comments

01:26:19,850 --> 01:26:24,110
like that but my feeling on this is this

01:26:22,160 --> 01:26:26,380
is again like we can spend a lot of time

01:26:24,110 --> 01:26:28,940
and effort trying to figure this out but

01:26:26,380 --> 01:26:30,290
it's it's going to be very tough because

01:26:28,940 --> 01:26:31,880
all different there are so many

01:26:30,290 --> 01:26:34,820
architectures with different spin time

01:26:31,880 --> 01:26:36,440
so the easiest and more pragmatic way

01:26:34,820 --> 01:26:38,810
would be essentially to just have a

01:26:36,440 --> 01:26:41,000
tunable and all architectures or

01:26:38,810 --> 01:26:43,490
whatever workload they're running they

01:26:41,000 --> 01:26:45,380
can just set it to the sweet spot

01:26:43,490 --> 01:26:48,370
depending on their operating conditions

01:26:45,380 --> 01:26:52,100
and get the benefit

01:26:48,370 --> 01:26:56,300
so I'm also looking like for more

01:26:52,100 --> 01:26:58,730
workloads to try for the pipes thing so

01:26:56,300 --> 01:27:00,500
obvious things that jumps is the hack

01:26:58,730 --> 01:27:03,710
bench pipe and the UNIX bench pipe where

01:27:00,500 --> 01:27:05,660
the kind of message passing where pipe

01:27:03,710 --> 01:27:10,450
so that's obvious that that should give

01:27:05,660 --> 01:27:13,130
improvement and I also plan to try OLTP

01:27:10,450 --> 01:27:15,290
because I still haven't installed on one

01:27:13,130 --> 01:27:17,420
of the systems that I tend to try it on

01:27:15,290 --> 01:27:18,470
so I'll probably in the next version of

01:27:17,420 --> 01:27:22,070
the patch

01:27:18,470 --> 01:27:26,900
I'll have the OLTP numbers as well so

01:27:22,070 --> 01:27:29,360
just to show some basic numbers like

01:27:26,900 --> 01:27:31,670
hack bench pipe enemies vent pipe I mean

01:27:29,360 --> 01:27:34,880
the gains are huge and this is expected

01:27:31,670 --> 01:27:37,040
because these threads are probably not

01:27:34,880 --> 01:27:38,900
doing much they're just running for a

01:27:37,040 --> 01:27:41,300
very small amount of time and it is

01:27:38,900 --> 01:27:44,330
dominated by the other system overhead

01:27:41,300 --> 01:27:45,830
so even the same for the unit's bench

01:27:44,330 --> 01:27:49,550
bike so that's why you see it again so

01:27:45,830 --> 01:27:51,640
much on a more real workload will

01:27:49,550 --> 01:27:55,220
probably be like single-digit percentage

01:27:51,640 --> 01:27:58,520
my guess and one thing to remember is

01:27:55,220 --> 01:28:00,530
that yeah of course this is needed

01:27:58,520 --> 01:28:02,540
because like anyway they you spend

01:28:00,530 --> 01:28:05,300
sequel cycles going to sleep and then

01:28:02,540 --> 01:28:06,860
waking up so if you are working a faster

01:28:05,300 --> 01:28:09,920
than that of course it makes sense to

01:28:06,860 --> 01:28:12,230
just be on CPU in fact that will also

01:28:09,920 --> 01:28:13,460
bring down the CPU utilization in

01:28:12,230 --> 01:28:19,550
addition to giving performance

01:28:13,460 --> 01:28:23,000
improvement okay so there are a few

01:28:19,550 --> 01:28:25,910
takeaways I had like working on the

01:28:23,000 --> 01:28:28,820
scheduler and I I really found at least

01:28:25,910 --> 01:28:31,040
the part I have worked on for last few

01:28:28,820 --> 01:28:33,880
months so optimizing the scheduler for

01:28:31,040 --> 01:28:36,740
performance is really hard and it's

01:28:33,880 --> 01:28:38,750
basically for like a couple of different

01:28:36,740 --> 01:28:40,400
reasons it's always a double-edged sword

01:28:38,750 --> 01:28:42,860
because you cannot spend too much time

01:28:40,400 --> 01:28:45,440
at least on the push side you cannot

01:28:42,860 --> 01:28:47,330
spend too much time finding any idle

01:28:45,440 --> 01:28:48,650
secure it will hurt you but then you

01:28:47,330 --> 01:28:50,180
also want to make an intelligent

01:28:48,650 --> 01:28:52,550
decision and spread out those red

01:28:50,180 --> 01:28:55,340
threads evenly enough so that there is

01:28:52,550 --> 01:28:56,600
no we're not wasting CPUs so it's very

01:28:55,340 --> 01:29:00,710
difficult to strike a balance between

01:28:56,600 --> 01:29:03,440
the two secondly all workloads on the

01:29:00,710 --> 01:29:06,800
planet use the scheduler so anything you

01:29:03,440 --> 01:29:08,630
change you have to test set of

01:29:06,800 --> 01:29:09,980
exhaustive workloads on all kinds of

01:29:08,630 --> 01:29:11,570
architecture and most people probably

01:29:09,980 --> 01:29:13,280
even don't have access to all kinds of

01:29:11,570 --> 01:29:15,650
architectures and then you have to also

01:29:13,280 --> 01:29:17,630
vary the utilization levels for each of

01:29:15,650 --> 01:29:19,640
these benchmarks so just imagine that s

01:29:17,630 --> 01:29:24,050
matrix every time you have to go through

01:29:19,640 --> 01:29:27,560
when you change something and for the

01:29:24,050 --> 01:29:28,790
double-edged sword thing sorry for

01:29:27,560 --> 01:29:30,710
satisfying all were closed thing

01:29:28,790 --> 01:29:32,810
sometimes the scheduler feature will

01:29:30,710 --> 01:29:36,740
come to the rescue for example I just

01:29:32,810 --> 01:29:38,450
decided that okay we can put a Cisco who

01:29:36,740 --> 01:29:41,120
kind of visible the idle core search

01:29:38,450 --> 01:29:41,580
which might hurt certain cases but then

01:29:41,120 --> 01:29:44,430
we can

01:29:41,580 --> 01:29:48,060
a kind of a tunable thing so that might

01:29:44,430 --> 01:29:54,120
and some food for thought

01:29:48,060 --> 01:30:01,890
will the LLC domain where do we stand on

01:29:54,120 --> 01:30:05,840
schedule it enables Peter I think he

01:30:01,890 --> 01:30:08,760
likes care teachers you have used a lot

01:30:05,840 --> 01:30:12,150
so features are on the Scott debugger

01:30:08,760 --> 01:30:14,760
this is a hint it is not an API were not

01:30:12,150 --> 01:30:16,650
supporting it okay if it goes away in

01:30:14,760 --> 01:30:27,300
your database falls over it's not my

01:30:16,650 --> 01:30:30,090
problem I don't think it's practical for

01:30:27,300 --> 01:30:33,810
us to expect one scheduler to now work

01:30:30,090 --> 01:30:37,650
across this wire right here Perkins it's

01:30:33,810 --> 01:30:41,150
working at once I mean we can have some

01:30:37,650 --> 01:30:41,150
tunable but there's always

01:30:45,940 --> 01:30:51,020
how many knobs do you want to turn and

01:30:48,650 --> 01:30:52,580
then you Jun it for the one workload and

01:30:51,020 --> 01:30:54,770
then another workload comes in to the

01:30:52,580 --> 01:31:01,760
same machine and it completely falls

01:30:54,770 --> 01:31:04,219
over again the the tunable almost never

01:31:01,760 --> 01:31:07,270
work in practice the queue will maybe

01:31:04,219 --> 01:31:10,219
it's unit once for one work world but

01:31:07,270 --> 01:31:12,140
99.99 percent of user will never to unit

01:31:10,219 --> 01:31:14,210
and they will never even know that it's

01:31:12,140 --> 01:31:16,719
there yes so the only thing that matters

01:31:14,210 --> 01:31:25,489
is the default that's not true actually

01:31:16,719 --> 01:31:27,320
well so there is like 0.01 yes you put

01:31:25,489 --> 01:31:30,739
this behind a tunable and it doesn't

01:31:27,320 --> 01:31:32,780
affect them but you do make the tunable

01:31:30,739 --> 01:31:36,170
available to folks who do want to use

01:31:32,780 --> 01:31:39,739
this feature and it brings them the

01:31:36,170 --> 01:31:41,840
optimization that they want this is

01:31:39,739 --> 01:31:44,060
courting I can by default I left it as

01:31:41,840 --> 01:31:47,060
same as the baseline so I just I just do

01:31:44,060 --> 01:31:50,060
the value with workloads okay the point

01:31:47,060 --> 01:31:54,170
is that we must much more concentrate on

01:31:50,060 --> 01:31:56,030
the default so for the default we can

01:31:54,170 --> 01:31:57,980
just say that that for this bed and

01:31:56,030 --> 01:32:00,560
expect you so much that even how do you

01:31:57,980 --> 01:32:02,300
define a default do you work like how is

01:32:00,560 --> 01:32:02,570
one workload more important than the

01:32:02,300 --> 01:32:05,390
other

01:32:02,570 --> 01:32:09,020
like when I do def config whatever it

01:32:05,390 --> 01:32:13,280
gives me sorry I could you repeat what

01:32:09,020 --> 01:32:16,400
you just said and say that we must not

01:32:13,280 --> 01:32:20,150
consider that the default doesn't matter

01:32:16,400 --> 01:32:21,679
it and make it bad and accumulate no I

01:32:20,150 --> 01:32:23,989
don't think anybody's disagreeing with

01:32:21,679 --> 01:32:28,510
you the question is what is default

01:32:23,989 --> 01:32:28,510
what is the workload Linux is targeting

01:32:28,920 --> 01:32:38,490
yeah well isn't it whatever transcend it

01:32:31,890 --> 01:32:42,990
now the answer to that question is

01:32:38,490 --> 01:32:46,650
Lisa's laptop well yes that but also we

01:32:42,990 --> 01:32:52,320
shouldn't particularly suck at any one

01:32:46,650 --> 01:32:55,800
thing we can't win all the benchmarks

01:32:52,320 --> 01:33:00,060
but we stir we try not to tank on any of

01:32:55,800 --> 01:33:01,830
them you will not suck so much on

01:33:00,060 --> 01:33:04,880
anything you'll suck to some extent

01:33:01,830 --> 01:33:08,690
that's the thing you'll never suck like

01:33:04,880 --> 01:33:08,690
[Laughter]

01:33:08,750 --> 01:33:14,130
okay so I'll just the last point is like

01:33:11,520 --> 01:33:17,520
I can have the default conversation

01:33:14,130 --> 01:33:19,710
maybe after offline last point is so a

01:33:17,520 --> 01:33:21,990
lot of things are tied to the LLC domain

01:33:19,710 --> 01:33:23,400
and we are searching that will it

01:33:21,990 --> 01:33:26,580
actually continue to get bigger so I

01:33:23,400 --> 01:33:30,930
know Intel has 28 course as of now in

01:33:26,580 --> 01:33:33,060
what LLC domain an empty has two to four

01:33:30,930 --> 01:33:34,980
chips but as far as I know those are the

01:33:33,060 --> 01:33:38,460
multi chip modules so it essentially as

01:33:34,980 --> 01:33:42,060
like a four LLC inside that so they're

01:33:38,460 --> 01:33:45,980
like think eight eight cores but it is

01:33:42,060 --> 01:33:49,350
so how how big will it get and yeah I

01:33:45,980 --> 01:33:50,640
believe that Intel recommends these dice

01:33:49,350 --> 01:33:52,320
on who still works for Intel could

01:33:50,640 --> 01:33:54,350
perhaps confirm this that if you're on a

01:33:52,320 --> 01:33:58,170
28 core the socket box that you actually

01:33:54,350 --> 01:34:01,250
use a you partition it into its soft

01:33:58,170 --> 01:34:01,250
Numa kind of stuff right

01:34:01,670 --> 01:34:06,510
so I I don't know what our official

01:34:04,500 --> 01:34:08,219
recommendation is but we are we have

01:34:06,510 --> 01:34:12,320
this feature there's something new my

01:34:08,219 --> 01:34:12,320
cluster is I think the last name for it

01:34:13,489 --> 01:34:17,610
and for our moans so I know I I don't

01:34:15,929 --> 01:34:19,679
have direct access to any opera types

01:34:17,610 --> 01:34:22,739
but from the discussions I have seen

01:34:19,679 --> 01:34:24,330
like ampair chips they have kind of a

01:34:22,739 --> 01:34:26,160
non uniform cache architecture so they

01:34:24,330 --> 01:34:28,410
also have like two cache as three whole

01:34:26,160 --> 01:34:30,239
clusters in the last level so so my

01:34:28,410 --> 01:34:32,520
point here is that how big can it get

01:34:30,239 --> 01:34:35,520
like how what is the worst case some

01:34:32,520 --> 01:34:36,989
some food for thought since we are

01:34:35,520 --> 01:34:41,370
trying to solve the scalability of the

01:34:36,989 --> 01:34:43,410
LLC scanning so so I think there's also

01:34:41,370 --> 01:34:48,380
physical limits to how big they can get

01:34:43,410 --> 01:34:48,380
it and I think we're very near there

01:34:53,989 --> 01:35:05,489
yeah I know but I mean if now I mean if

01:35:01,920 --> 01:35:09,750
you build a very large ring at some

01:35:05,489 --> 01:35:13,560
point the actual interconnect costs

01:35:09,750 --> 01:35:16,890
start to get pretty terrible and and so

01:35:13,560 --> 01:35:23,090
where is the point where we'll say no

01:35:16,890 --> 01:35:23,090
more and then go to an actual new one so

01:35:27,030 --> 01:35:32,050
well so those trends are true when you

01:35:30,130 --> 01:35:33,909
end up with things like core clusters as

01:35:32,050 --> 01:35:36,099
an intermediate level and so the

01:35:33,909 --> 01:35:38,050
question for the scheduler is so now do

01:35:36,099 --> 01:35:39,579
I push and only look for candidates

01:35:38,050 --> 01:35:41,019
within the core cluster because it's a

01:35:39,579 --> 01:35:43,119
smaller and I'm ignoring a lot of other

01:35:41,019 --> 01:35:45,820
potential il CPUs or do we have to

01:35:43,119 --> 01:35:47,679
extend it to be a multi-level search and

01:35:45,820 --> 01:35:54,849
in a scalable fashion right it's

01:35:47,679 --> 01:35:56,320
something like that okay everybody we

01:35:54,849 --> 01:35:57,040
can have more questions like offline

01:35:56,320 --> 01:36:02,050
afterwards

01:35:57,040 --> 01:36:04,340
[Applause]

01:36:02,050 --> 01:36:05,749
welcome back everybody to performance

01:36:04,340 --> 01:36:06,709
and scalability Michael conference I

01:36:05,749 --> 01:36:08,179
just want to make a quick announcement

01:36:06,709 --> 01:36:09,829
before we get started again

01:36:08,179 --> 01:36:11,329
if you ask a question please state your

01:36:09,829 --> 01:36:14,119
name first so that we can credit you in

01:36:11,329 --> 01:36:15,409
the notes okay we have Christoph laminar

01:36:14,119 --> 01:36:20,959
and Mike Kravitz to talk about huge

01:36:15,409 --> 01:36:27,139
pages can you give me okay

01:36:20,959 --> 01:36:29,119
so huge pages is I was at this love-hate

01:36:27,139 --> 01:36:31,249
relationship as huge pages on the one

01:36:29,119 --> 01:36:32,749
hand we want performance on the other

01:36:31,249 --> 01:36:34,519
hand why do we have to go through all

01:36:32,749 --> 01:36:35,929
this crap to make it possible and why it

01:36:34,519 --> 01:36:39,469
doesn't this work and why doesn't that

01:36:35,929 --> 01:36:42,949
work and so a lot of what we are doing

01:36:39,469 --> 01:36:45,019
is in the attention and making huge

01:36:42,949 --> 01:36:50,559
pages available for everybody and an

01:36:45,019 --> 01:36:53,059
easy way and really one need just clean

01:36:50,559 --> 01:36:55,249
suffer just run without it right so we

01:36:53,059 --> 01:36:58,130
have transparent huge pages we have

01:36:55,249 --> 01:37:00,709
various h-2b libraries and all sorts of

01:36:58,130 --> 01:37:01,820
extra stuff and so we're gonna talk

01:37:00,709 --> 01:37:05,239
about a little bit about all these

01:37:01,820 --> 01:37:08,630
issues today and Mike has done some work

01:37:05,239 --> 01:37:12,650
on this and won't say anything about

01:37:08,630 --> 01:37:14,659
tree so if you're looking for anything

01:37:12,650 --> 01:37:16,429
new Under the Sun you're not gonna find

01:37:14,659 --> 01:37:19,249
it at this session this is a little bit

01:37:16,429 --> 01:37:21,380
kind of the state of how things are the

01:37:19,249 --> 01:37:23,860
problems that we face some potential

01:37:21,380 --> 01:37:26,389
directions people are working in but

01:37:23,860 --> 01:37:29,989
we're not really into solution space

01:37:26,389 --> 01:37:30,619
here well we have a couple of ideas and

01:37:29,989 --> 01:37:36,260
stuff of course

01:37:30,619 --> 01:37:40,309
welcome progress so now what is the

01:37:36,260 --> 01:37:43,159
state of huge pages so the interesting

01:37:40,309 --> 01:37:45,889
thing here is that the huge pages

01:37:43,159 --> 01:37:47,119
directly expand as time progresses so

01:37:45,889 --> 01:37:50,479
most of the most exciting things that I

01:37:47,119 --> 01:37:53,380
see is the the use of decks where you

01:37:50,479 --> 01:37:55,909
can memory map a non-volatile memory and

01:37:53,380 --> 01:37:58,189
I looked at the code of a sudden what to

01:37:55,909 --> 01:38:00,019
do how fast can I get and I saw it uses

01:37:58,189 --> 01:38:03,320
one gig mappings and to make mappings if

01:38:00,019 --> 01:38:05,119
possible automatically so if you can use

01:38:03,320 --> 01:38:07,400
it you can avoid all the issues with the

01:38:05,119 --> 01:38:09,480
4k page on Intel just by using decks for

01:38:07,400 --> 01:38:13,320
example

01:38:09,480 --> 01:38:14,940
yeah I think that's probably a nice

01:38:13,320 --> 01:38:17,760
direction that we're taking that would

01:38:14,940 --> 01:38:19,490
ultimately be nice to have in the core

01:38:17,760 --> 01:38:22,320
memory management to actually

01:38:19,490 --> 01:38:24,330
automatically use to make or one gig

01:38:22,320 --> 01:38:26,580
pages depending upon what would be

01:38:24,330 --> 01:38:29,370
optimal I know that there are some

01:38:26,580 --> 01:38:31,830
people working in this area no patches

01:38:29,370 --> 01:38:34,110
out yet but are at least thinking about

01:38:31,830 --> 01:38:36,270
it and considering it your other two

01:38:34,110 --> 01:38:40,230
options state of the art today or you

01:38:36,270 --> 01:38:42,000
know the old very old huge t OB FS which

01:38:40,230 --> 01:38:43,980
is you know if you want to use it you've

01:38:42,000 --> 01:38:46,860
pretty much got to configure the thing

01:38:43,980 --> 01:38:49,980
it's good for what I call single purpose

01:38:46,860 --> 01:38:52,140
use cases databases love it they set up

01:38:49,980 --> 01:38:54,540
the machine in a particular way for

01:38:52,140 --> 01:38:56,310
their workload they pre-configure

01:38:54,540 --> 01:38:59,130
pre-allocate huge pages and they're good

01:38:56,310 --> 01:39:02,130
to go the other extreme is of course as

01:38:59,130 --> 01:39:05,670
transparent huge pages you don't really

01:39:02,130 --> 01:39:07,800
know exactly what your workload is but

01:39:05,670 --> 01:39:10,260
you can actually get some benefit by

01:39:07,800 --> 01:39:12,450
using this automatically with no very

01:39:10,260 --> 01:39:16,500
little application or system

01:39:12,450 --> 01:39:18,240
configuration don't expect too much of

01:39:16,500 --> 01:39:19,740
transplant HP so you can't use it a

01:39:18,240 --> 01:39:21,210
visit with the file back mappings and so

01:39:19,740 --> 01:39:23,580
you're restricted to HIPAA know and

01:39:21,210 --> 01:39:24,840
stuff only and a lot of people whenever

01:39:23,580 --> 01:39:26,610
I say book talk about transplant use

01:39:24,840 --> 01:39:31,400
video I can use this for I oh no you

01:39:26,610 --> 01:39:31,400
can't we wish but we aren't there

01:39:50,640 --> 01:39:59,470
better okay all right so for transparent

01:39:57,880 --> 01:40:01,690
huge pages he like to map with either

01:39:59,470 --> 01:40:04,510
two Meg or one gig depending what fits

01:40:01,690 --> 01:40:07,120
and what's the line but my question is

01:40:04,510 --> 01:40:09,400
and maybe for the Intel folks here are

01:40:07,120 --> 01:40:12,400
the number of TLB entries that can

01:40:09,400 --> 01:40:14,320
support one gig to limited such that if

01:40:12,400 --> 01:40:16,060
we automatically made a lot of one gig

01:40:14,320 --> 01:40:17,980
mappings we'd end up running poorly

01:40:16,060 --> 01:40:19,930
because of TLB thrashing you certainly

01:40:17,980 --> 01:40:22,030
have used architectures in the past

01:40:19,930 --> 01:40:23,920
where that's a thing and you have to

01:40:22,030 --> 01:40:25,600
apply heuristics as to how many of these

01:40:23,920 --> 01:40:27,280
mappings you allow and evict sum and

01:40:25,600 --> 01:40:28,690
downgrade is right now you have that

01:40:27,280 --> 01:40:31,330
problem right but future generations of

01:40:28,690 --> 01:40:33,070
CPUs will have more so today it's

01:40:31,330 --> 01:40:35,020
probably advisable to use - to make huge

01:40:33,070 --> 01:40:36,400
pages on Intel but that's an

01:40:35,020 --> 01:40:39,280
interrelation various platforms have

01:40:36,400 --> 01:40:41,320
different requirements yeah thanks I was

01:40:39,280 --> 01:40:43,690
gonna say can anybody from Intel kind of

01:40:41,320 --> 01:40:51,660
speak to the latest and greatest state

01:40:43,690 --> 01:40:51,660
of the art and that respect as far as No

01:40:52,530 --> 01:41:02,920
okay yeah and again this is just kind of

01:40:58,440 --> 01:41:04,719
restating what is you know kind of the

01:41:02,920 --> 01:41:06,729
current state of the air so huge t OB FS

01:41:04,719 --> 01:41:09,070
like I said before it's usually pre

01:41:06,729 --> 01:41:12,699
allocation at boot time or early sis and

01:41:09,070 --> 01:41:14,920
at time memory is only available for you

01:41:12,699 --> 01:41:16,329
t OB FS so if you want to use huge pages

01:41:14,920 --> 01:41:19,179
in this way it's pretty much a

01:41:16,329 --> 01:41:21,929
reservation scheme you're limiting the

01:41:19,179 --> 01:41:25,510
amount of memory available to other

01:41:21,929 --> 01:41:26,019
other users in the system to actually

01:41:25,510 --> 01:41:29,380
use it

01:41:26,019 --> 01:41:31,269
applications must change open and map

01:41:29,380 --> 01:41:35,130
calls to actually get access to this

01:41:31,269 --> 01:41:37,929
stuff not all file operations are even

01:41:35,130 --> 01:41:41,889
available such as you can't write to a

01:41:37,929 --> 01:41:43,659
huge Tod FS file you can only be good

01:41:41,889 --> 01:41:45,820
thing about it is is that you can use

01:41:43,659 --> 01:41:49,059
multiple huge page sizes it's

01:41:45,820 --> 01:41:53,260
architecture independent obviously Intel

01:41:49,059 --> 01:41:55,719
the sizes are 2 Meg and one gig other

01:41:53,260 --> 01:41:59,440
architectures like arm and PowerPC

01:41:55,719 --> 01:42:04,749
you've got many many more huge page

01:41:59,440 --> 01:42:07,210
sizes you can choose from and dynamic

01:42:04,749 --> 01:42:09,219
allocation is possible with troublesome

01:42:07,210 --> 01:42:10,780
I know people using it this way today

01:42:09,219 --> 01:42:14,110
that you can actually dynamically

01:42:10,780 --> 01:42:15,760
allocate huge pages but if you can't do

01:42:14,110 --> 01:42:17,139
it you actually get a sig bus to your

01:42:15,760 --> 01:42:18,900
application you've got to catch and

01:42:17,139 --> 01:42:21,159
recover from that so it's it's very

01:42:18,900 --> 01:42:24,719
cumbersome to try to do that but people

01:42:21,159 --> 01:42:24,719
actually are doing that today

01:42:25,250 --> 01:42:29,450
okay any questions on a huge geo BFS

01:42:27,860 --> 01:42:32,300
this is the classic thing that we've

01:42:29,450 --> 01:42:33,680
done for more than a decade now it's

01:42:32,300 --> 01:42:37,010
very well-established accidental

01:42:33,680 --> 01:42:43,090
pigmentation okay everything's clear

01:42:37,010 --> 01:42:43,090
good then we have transparent huge pages

01:42:43,300 --> 01:42:49,520
this is the attempt to avoid manual

01:42:46,670 --> 01:42:53,000
configuration make it automatic however

01:42:49,520 --> 01:42:55,430
we end up with em advised cause

01:42:53,000 --> 01:42:56,690
basically that tell you which sections

01:42:55,430 --> 01:42:59,180
should be backed by huge pages in which

01:42:56,690 --> 01:43:01,010
shouldn't shouldn't be and gradually the

01:42:59,180 --> 01:43:03,050
use of transparent which basic expands

01:43:01,010 --> 01:43:06,290
first it was only anomalous memory now

01:43:03,050 --> 01:43:09,010
we have shared memory and temp FS you

01:43:06,290 --> 01:43:11,440
have no need for any changes to of the

01:43:09,010 --> 01:43:14,420
application that's the great advantage

01:43:11,440 --> 01:43:19,880
and but there's only a single huge page

01:43:14,420 --> 01:43:21,860
size supported so the problems with huge

01:43:19,880 --> 01:43:23,750
page elevations I think the same as

01:43:21,860 --> 01:43:26,420
regular huge pages because you must have

01:43:23,750 --> 01:43:28,610
them and if a system memory is different

01:43:26,420 --> 01:43:31,700
mented and then you can't automatically

01:43:28,610 --> 01:43:33,380
generate new huge pages and the attempt

01:43:31,700 --> 01:43:34,730
to do so is extremely slow because the

01:43:33,380 --> 01:43:36,740
system will scan through all of your

01:43:34,730 --> 01:43:40,790
processes in all your memory try to find

01:43:36,740 --> 01:43:42,650
linear mappings and try soon and evict

01:43:40,790 --> 01:43:45,800
or move pages around so that you can

01:43:42,650 --> 01:43:48,260
allocate it to make segments so as time

01:43:45,800 --> 01:43:50,600
progresses that effort will be higher

01:43:48,260 --> 01:43:52,490
and higher and so you end up as a

01:43:50,600 --> 01:43:54,410
similar company issue a solution a pages

01:43:52,490 --> 01:43:56,030
you have to pre configure if possible

01:43:54,410 --> 01:43:59,690
the number of huge pages you want and

01:43:56,030 --> 01:44:02,680
you need for your application yeah I

01:43:59,690 --> 01:44:04,760
don't really have much to add to that

01:44:02,680 --> 01:44:07,970
there are no application changes

01:44:04,760 --> 01:44:10,040
required but really to take advantage of

01:44:07,970 --> 01:44:13,100
this you need to make sure alignment and

01:44:10,040 --> 01:44:15,619
other type things are correct for

01:44:13,100 --> 01:44:17,770
optimal usage

01:44:15,619 --> 01:44:20,900
and as it says you only have

01:44:17,770 --> 01:44:24,159
availability for a single huge page size

01:44:20,900 --> 01:44:28,659
now which is typically PMD size which is

01:44:24,159 --> 01:44:28,659
2 Meg and Intel

01:44:32,239 --> 01:44:36,590
yeah the main problems with the huge

01:44:34,040 --> 01:44:40,300
pages is we can't release page cache we

01:44:36,590 --> 01:44:42,410
really want that before i/o throughput

01:44:40,300 --> 01:44:43,970
and if you want to reconfigure the

01:44:42,410 --> 01:44:45,770
system for a different size of huge

01:44:43,970 --> 01:44:46,280
babies you actually may have to reboot

01:44:45,770 --> 01:44:48,710
the system

01:44:46,280 --> 01:44:50,930
that's something it's an enormous pain

01:44:48,710 --> 01:44:53,630
right now for my guys and my company we

01:44:50,930 --> 01:44:55,160
have a lot of dependencies on huge pages

01:44:53,630 --> 01:44:57,560
because you can't get the performance

01:44:55,160 --> 01:44:59,930
with 4k pages and if you want to change

01:44:57,560 --> 01:45:01,400
the application you have to reboot the

01:44:59,930 --> 01:45:04,070
system because there's no contiguous

01:45:01,400 --> 01:45:05,840
memory available and so changing the

01:45:04,070 --> 01:45:07,940
applications takes about 15 minutes to

01:45:05,840 --> 01:45:09,560
get the net system down and up again and

01:45:07,940 --> 01:45:13,030
that is pretty unnerving

01:45:09,560 --> 01:45:13,030
especially on a computational cluster

01:45:14,050 --> 01:45:18,560
why do we have to deal with this crap

01:45:16,370 --> 01:45:20,360
anyways there are other platforms that

01:45:18,560 --> 01:45:22,910
have larger base page sizes and on those

01:45:20,360 --> 01:45:26,360
we don't see the problems at all we have

01:45:22,910 --> 01:45:27,710
some power systems in our lab and the

01:45:26,360 --> 01:45:30,560
same application runs without any

01:45:27,710 --> 01:45:32,480
problems at all the 64 K pages similarly

01:45:30,560 --> 01:45:35,180
we have also arm systems that have the

01:45:32,480 --> 01:45:36,860
same solution they also have 64 K pages

01:45:35,180 --> 01:45:38,540
base pages and you don't have any issues

01:45:36,860 --> 01:45:40,400
there either you don't need to go

01:45:38,540 --> 01:45:44,090
through all the huge page configurations

01:45:40,400 --> 01:45:46,400
yeah and so my biggest gripe with this

01:45:44,090 --> 01:45:47,900
is why doesn't Intel also offer us a

01:45:46,400 --> 01:45:50,800
reasonable page size so that we don't

01:45:47,900 --> 01:45:50,800
have to deal with this crap

01:45:56,900 --> 01:46:02,760
yeah so a larger base page solves some

01:46:00,420 --> 01:46:04,739
problems but not all that the problem is

01:46:02,760 --> 01:46:07,500
that our applications require an

01:46:04,739 --> 01:46:09,420
exponential range of page sizes to cover

01:46:07,500 --> 01:46:11,340
the different data usages right and and

01:46:09,420 --> 01:46:13,679
and there are some characteristics right

01:46:11,340 --> 01:46:16,830
you know stacks like to be and you know

01:46:13,679 --> 01:46:18,239
64 K and above range you know text maybe

01:46:16,830 --> 01:46:19,050
you know a couple hundred K you know a

01:46:18,239 --> 01:46:21,600
couple Meg

01:46:19,050 --> 01:46:23,520
so you know maybe to make it the upper

01:46:21,600 --> 01:46:25,230
bounds but then the the data segments

01:46:23,520 --> 01:46:27,780
the non segments you know they're huge

01:46:25,230 --> 01:46:29,790
and you can have some large memory

01:46:27,780 --> 01:46:31,409
applications that have you know

01:46:29,790 --> 01:46:33,719
terabytes of data and those need the

01:46:31,409 --> 01:46:35,610
maximum possible page size so simply

01:46:33,719 --> 01:46:37,500
bumping up the base page size isn't

01:46:35,610 --> 01:46:39,390
going to improve the TLB miss rate for

01:46:37,500 --> 01:46:41,190
that kind of access pattern well it

01:46:39,390 --> 01:46:44,460
definitely changes the behavior by a

01:46:41,190 --> 01:46:46,890
factor of ten for me but again that

01:46:44,460 --> 01:46:48,780
that's totally application yeah that's

01:46:46,890 --> 01:46:50,100
that's true so ideally you would have to

01:46:48,780 --> 01:46:57,000
have a multiple page size that we can

01:46:50,100 --> 01:47:00,780
pick on whenever we want right so yes so

01:46:57,000 --> 01:47:02,429
I used to work with SPARC and I still

01:47:00,780 --> 01:47:04,260
miss it but you know I'm moving on but

01:47:02,429 --> 01:47:06,719
one of the great things about it was

01:47:04,260 --> 01:47:10,230
that you know it had a relatively small

01:47:06,719 --> 01:47:12,120
TLB 128 entries per core but it's fully

01:47:10,230 --> 01:47:14,429
associative and he could have any page

01:47:12,120 --> 01:47:17,100
size and it offered a large range of

01:47:14,429 --> 01:47:19,409
sizes you know all the way up to a 16

01:47:17,100 --> 01:47:21,960
gig and then down to the base page 8a K

01:47:19,409 --> 01:47:24,300
so it was easy for software to pick a

01:47:21,960 --> 01:47:26,070
size that match the particular segment

01:47:24,300 --> 01:47:28,170
need without any penalty so it's very

01:47:26,070 --> 01:47:30,600
flexible I'd really encourage the other

01:47:28,170 --> 01:47:33,360
CPU designers to consider such an

01:47:30,600 --> 01:47:35,190
architecture the problems also the

01:47:33,360 --> 01:47:37,140
memory size is ever increased we have

01:47:35,190 --> 01:47:39,780
machines now with terabyte of memory and

01:47:37,140 --> 01:47:42,120
it is pretty standard to shift around

01:47:39,780 --> 01:47:44,010
for kick for gigabyte pages

01:47:42,120 --> 01:47:46,739
about the files throughout our our never

01:47:44,010 --> 01:47:51,870
infrastructure and a 4 gigabyte file

01:47:46,739 --> 01:47:55,100
requires 1 million 4k entities to manage

01:47:51,870 --> 01:47:55,100
which is not that simple

01:47:56,540 --> 01:48:02,920
so working progress

01:47:59,949 --> 01:48:05,260
this have more support for file-based

01:48:02,920 --> 01:48:06,969
mappings I think Matthew just got this

01:48:05,260 --> 01:48:08,590
x-ray in which is one of the basic

01:48:06,969 --> 01:48:13,150
points to get that going hopefully at

01:48:08,590 --> 01:48:16,090
some point then I think you are working

01:48:13,150 --> 01:48:18,699
on the page care support for huge pages

01:48:16,090 --> 01:48:33,429
so hopefully we'll get there it's pretty

01:48:18,699 --> 01:48:35,320
slow but yeah so I'm not working on that

01:48:33,429 --> 01:48:37,949
I think that was Carol shoot them off

01:48:35,320 --> 01:48:37,949
okay

01:48:40,019 --> 01:48:49,360
yes your work I think your work just to

01:48:46,440 --> 01:48:51,340
make it available yeah just to make huge

01:48:49,360 --> 01:48:53,260
pages available I mean there's the when

01:48:51,340 --> 01:48:55,329
we talk about transparent huge pages and

01:48:53,260 --> 01:48:56,949
what Christoph was talking about is when

01:48:55,329 --> 01:48:58,719
you need one you've actually got to find

01:48:56,949 --> 01:49:02,729
one somewhere and so you have this whole

01:48:58,719 --> 01:49:04,690
issue of well we've got to migrate away

01:49:02,729 --> 01:49:06,789
usages we've got to find a contiguous

01:49:04,690 --> 01:49:08,860
area or can actually get a huge page and

01:49:06,789 --> 01:49:11,829
I know that you are working in that area

01:49:08,860 --> 01:49:14,309
so there's kind of multiple ways we're

01:49:11,829 --> 01:49:16,090
kind of trying to approach this yeah

01:49:14,309 --> 01:49:17,289
everybody's tries to do a small bit

01:49:16,090 --> 01:49:20,289
hopefully at some point it all comes

01:49:17,289 --> 01:49:22,119
together I've also proposed a patch set

01:49:20,289 --> 01:49:25,209
to our reserve powers the pages never

01:49:22,119 --> 01:49:27,179
try order the forum any problem that we

01:49:25,209 --> 01:49:29,409
have if we use multiple page sizes is

01:49:27,179 --> 01:49:31,179
that memory is different mentoring and

01:49:29,409 --> 01:49:33,519
we cannot recover the continuous nature

01:49:31,179 --> 01:49:35,110
of memory and that is required actually

01:49:33,519 --> 01:49:41,249
to have a consistent and reliable

01:49:35,110 --> 01:49:44,079
allocation of varying sizes of pages so

01:49:41,249 --> 01:49:46,449
even just having huge pages and pages in

01:49:44,079 --> 01:49:52,199
their causes basic issues aren't many

01:49:46,449 --> 01:49:54,699
main management layer so with that we

01:49:52,199 --> 01:49:59,769
come to some of the esoteric things that

01:49:54,699 --> 01:50:02,519
we still that are still in progress so

01:49:59,769 --> 01:50:05,530
for example I'm trying to increase the

01:50:02,519 --> 01:50:07,360
ability of the kernel to in to be able

01:50:05,530 --> 01:50:09,969
to reformat memories by making the slab

01:50:07,360 --> 01:50:12,660
objects moveable that's one of the key

01:50:09,969 --> 01:50:16,410
things that always

01:50:12,660 --> 01:50:18,720
the metaphor about you recover to make

01:50:16,410 --> 01:50:20,880
pages for example this is a kind of a

01:50:18,720 --> 01:50:23,160
significant intrusion into the way that

01:50:20,880 --> 01:50:25,410
the system works and gets us too close

01:50:23,160 --> 01:50:27,000
to job our style Gabor's collection and

01:50:25,410 --> 01:50:34,740
the kernel I think I think it's very

01:50:27,000 --> 01:50:36,780
offensive to many somehow then this is

01:50:34,740 --> 01:50:40,950
the idea here maybe we can just on x86

01:50:36,780 --> 01:50:42,630
create a custom Linux kernel that we'll

01:50:40,950 --> 01:50:44,820
just run with a to make base base page

01:50:42,630 --> 01:50:46,170
size with that we don't wouldn't have an

01:50:44,820 --> 01:50:49,110
issue anymore with fragmentation yeah

01:50:46,170 --> 01:50:52,380
there's a separate area for Linux kernel

01:50:49,110 --> 01:50:53,790
transmits to Mac there are system

01:50:52,380 --> 01:50:59,040
components are already prepared for that

01:50:53,790 --> 01:51:00,990
like the locators so it may not be that

01:50:59,040 --> 01:51:02,730
much of an effort if somebody has some

01:51:00,990 --> 01:51:03,120
dedicated time for this to get that

01:51:02,730 --> 01:51:06,300
going

01:51:03,120 --> 01:51:08,970
I also I know that the I thought that

01:51:06,300 --> 01:51:12,450
the binary format and xe6

01:51:08,970 --> 01:51:14,700
has been prepped for to make layouts so

01:51:12,450 --> 01:51:16,890
the text segments and all the other

01:51:14,700 --> 01:51:19,710
statements aligned at to make boundaries

01:51:16,890 --> 01:51:23,400
so that binaries will just load and run

01:51:19,710 --> 01:51:24,510
on a to make VM so if somebody has some

01:51:23,400 --> 01:51:27,240
dedicated time on it we really

01:51:24,510 --> 01:51:30,960
appreciate that to get this going if

01:51:27,240 --> 01:51:32,430
possible then my guys and my company

01:51:30,960 --> 01:51:35,190
have issues with constant you have to

01:51:32,430 --> 01:51:36,930
reconfigure the systems because of very

01:51:35,190 --> 01:51:40,350
huge page sizes for various applications

01:51:36,930 --> 01:51:42,480
so there are some work in progress using

01:51:40,350 --> 01:51:45,300
ansible and trying to come up with some

01:51:42,480 --> 01:51:47,400
configuration techniques to more or less

01:51:45,300 --> 01:51:49,320
ultimate eyes these things depended

01:51:47,400 --> 01:51:50,660
based on application profiles and stuff

01:51:49,320 --> 01:51:54,000
like that so you can automatically

01:51:50,660 --> 01:51:56,190
reboot the system and we configure it

01:51:54,000 --> 01:51:58,500
for the applications you may run or want

01:51:56,190 --> 01:52:00,900
to run on this this is an awkward walk

01:51:58,500 --> 01:52:02,310
around against the main problem yeah but

01:52:00,900 --> 01:52:08,010
it seems to be the warm that may be

01:52:02,310 --> 01:52:10,350
necessary in the short term future and

01:52:08,010 --> 01:52:11,940
it's pressure in charge to increase the

01:52:10,350 --> 01:52:14,970
page size right I'm trying to do that

01:52:11,940 --> 01:52:19,440
for 15 years now not successful but

01:52:14,970 --> 01:52:22,680
maybe a good thing and then you have the

01:52:19,440 --> 01:52:27,030
approach it increasing ability of 4k

01:52:22,680 --> 01:52:30,810
pages and I want to get to is to be able

01:52:27,030 --> 01:52:34,110
to move the entries in I nodes around

01:52:30,810 --> 01:52:36,480
and main memory so that we can deal with

01:52:34,110 --> 01:52:37,740
issues where you have access to huge

01:52:36,480 --> 01:52:39,390
amount of files and then you switch the

01:52:37,740 --> 01:52:40,830
load to do something else at that point

01:52:39,390 --> 01:52:42,420
we don't have a lot of slot pages I just

01:52:40,830 --> 01:52:43,800
you have a single log entry I note in

01:52:42,420 --> 01:52:46,290
there and a lot of memory is being

01:52:43,800 --> 01:52:49,520
wasted and this memory is locked down so

01:52:46,290 --> 01:52:51,570
that you can't

01:52:49,520 --> 01:52:53,520
large contiguous seconds because yes

01:52:51,570 --> 01:52:55,980
this one object is sitting in a 4k page

01:52:53,520 --> 01:52:59,460
that blocks all the recovery of a to MEK

01:52:55,980 --> 01:53:02,640
page so that's one thing that I hope to

01:52:59,460 --> 01:53:05,010
be able to get done hopefully it next

01:53:02,640 --> 01:53:07,400
year but I've been telling you that for

01:53:05,010 --> 01:53:07,400
10 years now

01:53:08,810 --> 01:53:15,480
I don't have much more to say here but

01:53:11,750 --> 01:53:24,210
just like to open it up for issues

01:53:15,480 --> 01:53:33,590
people have suggestions anything yeah

01:53:24,210 --> 01:53:36,990
actually we were trying to machine to

01:53:33,590 --> 01:53:39,630
but we experiencing a problem in the

01:53:36,990 --> 01:53:42,570
registry because when you re-enter the

01:53:39,630 --> 01:53:47,090
PHP into registry you have to enter it

01:53:42,570 --> 01:53:51,260
as a 4k normal page and for him

01:53:47,090 --> 01:53:55,350
but so you are fixing that yeah okay

01:53:51,260 --> 01:53:57,420
yeah but for the huge ifs you just use

01:53:55,350 --> 01:54:00,090
through the page as a huge page that's

01:53:57,420 --> 01:54:07,800
one Mario Hill page so it's a problem we

01:54:00,090 --> 01:54:12,750
are experiencing yes okay so which is

01:54:07,800 --> 01:54:15,780
the conversion to the x-ray the the

01:54:12,750 --> 01:54:20,219
second step is to

01:54:15,780 --> 01:54:24,389
converts look up to the point where pate

01:54:20,219 --> 01:54:26,369
page look up so that if it sees so right

01:54:24,389 --> 01:54:29,520
now we insert when we insert a two

01:54:26,369 --> 01:54:33,539
megabyte page we put in the five hundred

01:54:29,520 --> 01:54:36,059
twelve consecutive page pointers the

01:54:33,539 --> 01:54:39,150
next step is to convert the lookup code

01:54:36,059 --> 01:54:44,070
so that when it sees a head page it

01:54:39,150 --> 01:54:48,539
knows to in it looks okay relative to

01:54:44,070 --> 01:54:51,750
512 entries for that head page where are

01:54:48,539 --> 01:54:55,139
we in in in that page and so it will

01:54:51,750 --> 01:55:00,360
insert point so to overturn a pointer to

01:54:55,139 --> 01:55:03,869
the sub page and kinda prototype this I

01:55:00,360 --> 01:55:06,690
think he's in a not very well thought

01:55:03,869 --> 01:55:11,369
out order but he knew if you look

01:55:06,690 --> 01:55:15,150
through these patches this is their so

01:55:11,369 --> 01:55:17,219
once we have that in then we can switch

01:55:15,150 --> 01:55:20,639
to instead of inserting five hundred and

01:55:17,219 --> 01:55:25,340
twelve consecutive page pointers we

01:55:20,639 --> 01:55:28,579
insert 512 pointers to the base page

01:55:25,340 --> 01:55:33,090
once that's done we can then instead

01:55:28,579 --> 01:55:35,400
inserting 512 entries we can insert what

01:55:33,090 --> 01:55:37,409
is going to appear to be one huge entry

01:55:35,400 --> 01:55:42,329
well it actually looks like in the radix

01:55:37,409 --> 01:55:45,449
tree data structure is having a single

01:55:42,329 --> 01:55:48,570
pointer at a higher level of the rate

01:55:45,449 --> 01:55:51,300
extreme and then seven consecutive

01:55:48,570 --> 01:55:57,059
entries which say don't look here look

01:55:51,300 --> 01:55:59,270
at look at the base address instead and

01:55:57,059 --> 01:56:05,520
then from the page cache point of view

01:55:59,270 --> 01:56:06,389
we're done what I am working on and I'm

01:56:05,520 --> 01:56:10,659
gonna be talking more about this

01:56:06,389 --> 01:56:12,320
probably a 4 p.m. in the kernel track

01:56:10,659 --> 01:56:15,170
what what we're going to at that point

01:56:12,320 --> 01:56:17,150
is change the underlying representation

01:56:15,170 --> 01:56:19,370
of the page cash from being a radix tree

01:56:17,150 --> 01:56:24,820
to being a b-tree we are currently

01:56:19,370 --> 01:56:27,320
working on a are see you safe B tree and

01:56:24,820 --> 01:56:29,060
then that will truly be one pointer

01:56:27,320 --> 01:56:34,250
rather than being this hack where we

01:56:29,060 --> 01:56:36,980
have seven references over to the base

01:56:34,250 --> 01:56:40,310
page and at that point we we should

01:56:36,980 --> 01:56:42,739
actually be able to put arbitrary sized

01:56:40,310 --> 01:56:44,600
pages into the page cache not not just

01:56:42,739 --> 01:56:46,850
power of two but actually arbitrary size

01:56:44,600 --> 01:56:49,010
the the page allocator is not going to

01:56:46,850 --> 01:56:51,440
be happy with the English the idea of

01:56:49,010 --> 01:56:52,969
putting arbitrary size pages in but we

01:56:51,440 --> 01:56:55,640
could do something like that if there's

01:56:52,969 --> 01:56:57,409
some on foot and so the I'm trying to

01:56:55,640 --> 01:57:01,449
get to is that the the rate the the

01:56:57,409 --> 01:57:03,679
radix tree will no longer be the problem

01:57:01,449 --> 01:57:08,449
and then maybe somebody else can do some

01:57:03,679 --> 01:57:11,750
of the work but for the short time

01:57:08,449 --> 01:57:13,489
that's the market order III can help the

01:57:11,750 --> 01:57:17,090
multi oldest ways multi order radix tree

01:57:13,489 --> 01:57:18,739
has gone it's gone I I deleted it as

01:57:17,090 --> 01:57:22,909
part of the x-ray conversion the multi

01:57:18,739 --> 01:57:26,330
order radix tree has gone but so the

01:57:22,909 --> 01:57:28,909
multi order pay the multi-holder x-ray

01:57:26,330 --> 01:57:30,230
stuff will do just about everything that

01:57:28,909 --> 01:57:34,640
not see order radix treat are just

01:57:30,230 --> 01:57:36,800
slightly better but we'll I mean it

01:57:34,640 --> 01:57:40,120
shouldn't be too much longer before we

01:57:36,800 --> 01:57:40,120
actually have the

01:57:43,100 --> 01:57:53,680
the 8 8 entries part of us should be in

01:57:47,000 --> 01:57:57,560
pretty soon this is a follow-up to

01:57:53,680 --> 01:58:01,250
Stephens questions on future Intel CPU

01:57:57,560 --> 01:58:03,890
the TLB entries so for a tenant Lake

01:58:01,250 --> 01:58:07,130
there is say unfortunately there's no no

01:58:03,890 --> 01:58:12,470
changing the number of TLB entries for a

01:58:07,130 --> 01:58:14,420
large page but there are some work in

01:58:12,470 --> 01:58:19,640
the futures for it for the follow-on

01:58:14,420 --> 01:58:26,680
CPUs but you probably have to gather NDA

01:58:19,640 --> 01:58:29,540
say to gather information support and

01:58:26,680 --> 01:58:31,790
given the recent CPU roadmap changes

01:58:29,540 --> 01:58:33,950
that have been become public the

01:58:31,790 --> 01:58:38,350
information you get under 8 NDA may not

01:58:33,950 --> 01:58:38,350
reflect what happens in the future I

01:58:45,580 --> 01:58:56,270
have a question so one thing that I was

01:58:53,150 --> 01:59:02,630
missing from useable perspective is to

01:58:56,270 --> 01:59:05,300
ask so so usually a robust program would

01:59:02,630 --> 01:59:13,000
allocate memory with with M up either an

01:59:05,300 --> 01:59:17,360
anonymous amok or even Oh a temp file

01:59:13,000 --> 01:59:21,980
open on some other run attempt or

01:59:17,360 --> 01:59:25,220
whatever and so I'm missing the feature

01:59:21,980 --> 01:59:28,340
of asking the the Linux kernel to

01:59:25,220 --> 01:59:32,840
allocate a two megabyte aligned address

01:59:28,340 --> 01:59:36,560
for me and so we do crazy stuff like ask

01:59:32,840 --> 01:59:38,720
for two megabyte more and align that

01:59:36,560 --> 01:59:43,010
pointer and it's not wasted because I'm

01:59:38,720 --> 01:59:45,889
not page faulting that extra margin but

01:59:43,010 --> 01:59:48,400
it kind of sounds kind of

01:59:45,889 --> 01:59:51,530
helpful that the colonel would have a

01:59:48,400 --> 01:59:54,409
facility for me because sometimes you

01:59:51,530 --> 01:59:56,750
know especially when we have these two

01:59:54,409 --> 01:59:59,119
megabyte features opening up for us and

01:59:56,750 --> 02:00:02,179
we know where you're aiming for them and

01:59:59,119 --> 02:00:04,789
we know where we are trying to use them

02:00:02,179 --> 02:00:09,650
it's good that we have a two megabyte

02:00:04,789 --> 02:00:12,020
aligned pointer in user space what were

02:00:09,650 --> 02:00:14,329
you gonna say Krista I mean I agree on

02:00:12,020 --> 02:00:15,650
you know if you use huge she'll be FS of

02:00:14,329 --> 02:00:17,389
course you're going to get back a too

02:00:15,650 --> 02:00:20,030
many exactly but when you're doing

02:00:17,389 --> 02:00:21,710
something like just for PHP you don't

02:00:20,030 --> 02:00:24,139
get back you get something page aligned

02:00:21,710 --> 02:00:26,510
which may not be to Meg aligned which

02:00:24,139 --> 02:00:28,880
would be optimal for your case yeah I

02:00:26,510 --> 02:00:30,889
mean Solaris just like a me Sinclair's

02:00:28,880 --> 02:00:33,070
had a map a line feature that you could

02:00:30,889 --> 02:00:35,449
just you basically used the void star

02:00:33,070 --> 02:00:38,300
argument was treated as the alignment

02:00:35,449 --> 02:00:39,980
required for the mapping and that might

02:00:38,300 --> 02:00:42,559
be a pretty straightforward thing to

02:00:39,980 --> 02:00:44,000
implement well if you want to use

02:00:42,559 --> 02:00:46,610
transparent which pages then you have a

02:00:44,000 --> 02:00:48,590
transparent right it's not really a huge

02:00:46,610 --> 02:00:51,500
basis is just trying to do as much as

02:00:48,590 --> 02:00:56,989
possible if you do request for you which

02:00:51,500 --> 02:01:00,590
page you get the proper alignment yeah

02:00:56,989 --> 02:01:09,489
but then it will fail you well you can't

02:01:00,590 --> 02:01:11,960
you can't get it do it explicitly I

02:01:09,489 --> 02:01:15,349
don't want to force I don't want to

02:01:11,960 --> 02:01:17,389
force that system that if that system if

02:01:15,349 --> 02:01:20,659
the system doesn't give me or doesn't

02:01:17,389 --> 02:01:24,050
have available huge a lot of huge pages

02:01:20,659 --> 02:01:26,929
for me I don't care that this underlying

02:01:24,050 --> 02:01:29,980
file system does what it can but it's

02:01:26,929 --> 02:01:33,260
just it just wanted hide hidden you know

02:01:29,980 --> 02:01:36,340
sometimes yeah but I met em advise just

02:01:33,260 --> 02:01:36,340
do that Christa

02:01:38,010 --> 02:01:45,230
Oh is there and I'm advised that can

02:01:42,620 --> 02:01:47,970
there is an M advised to say use

02:01:45,230 --> 02:01:49,800
transparent huge pages but after you set

02:01:47,970 --> 02:01:56,270
up the mapping so you still you don't

02:01:49,800 --> 02:02:00,210
have the alignment I think excuse me

02:01:56,270 --> 02:02:02,430
correct I'm just gonna say four decks

02:02:00,210 --> 02:02:06,690
because we do want to encourage huge

02:02:02,430 --> 02:02:08,760
page mappings we do when anybody called

02:02:06,690 --> 02:02:11,239
them map we asked for a 2 mega alignment

02:02:08,760 --> 02:02:13,680
despite a fault even if for everything

02:02:11,239 --> 02:02:18,900
but that's that's kind of a purple

02:02:13,680 --> 02:02:21,630
system policy they're a lot in the ink

02:02:18,900 --> 02:02:24,750
kernel a lot of sub systems have done

02:02:21,630 --> 02:02:27,390
that and you know just copy pasted that

02:02:24,750 --> 02:02:31,950
thing instead of just having a central

02:02:27,390 --> 02:02:34,080
place of saying yeah moppy user mapping

02:02:31,950 --> 02:02:36,390
to megabyte aligned it's not gonna waste

02:02:34,080 --> 02:02:39,960
a lot of I mean I think it would be

02:02:36,390 --> 02:02:43,830
straightforward to add it to a temp FS

02:02:39,960 --> 02:02:45,960
or SH mem file system that has actually

02:02:43,830 --> 02:02:47,970
requested to use huge pages that would

02:02:45,960 --> 02:02:50,730
probably be makes sense to align it to 2

02:02:47,970 --> 02:02:52,770
Meg we have in those cases we could just

02:02:50,730 --> 02:02:55,110
change the heuristics at the top level

02:02:52,770 --> 02:02:57,510
if the user has asked for a mapping size

02:02:55,110 --> 02:03:00,540
which is greater than 2 megabytes

02:02:57,510 --> 02:03:03,380
I then happen to return a 2 megabyte

02:03:00,540 --> 02:03:03,380
aligned address

02:03:15,650 --> 02:03:22,170
if I'm asking for a length that is two

02:03:19,050 --> 02:03:26,090
megabyte aligned then probably I'm

02:03:22,170 --> 02:03:33,660
looking for two megabyte Airlines

02:03:26,090 --> 02:03:35,250
alignment yeah no I actually disagree

02:03:33,660 --> 02:03:36,930
there's as long as you've asked for

02:03:35,250 --> 02:03:38,970
something if you've asked for a length

02:03:36,930 --> 02:03:42,150
which is more than two megabytes those

02:03:38,970 --> 02:03:44,820
two megabytes or more then you can

02:03:42,150 --> 02:03:47,940
benefit from having a THP you don't you

02:03:44,820 --> 02:03:49,920
don't need to say well it isn't you know

02:03:47,940 --> 02:03:51,870
it's it's two megabytes plus one page

02:03:49,920 --> 02:03:54,390
therefore we won't two megabyte align it

02:03:51,870 --> 02:03:55,770
because you know there's two megabyte is

02:03:54,390 --> 02:03:58,130
there where you could have had a benefit

02:03:55,770 --> 02:03:58,130
from it

02:04:04,160 --> 02:04:08,730
so I will extract some of our

02:04:06,990 --> 02:04:11,910
experiments like we found this is

02:04:08,730 --> 02:04:14,970
extremely helpful to like pooty your

02:04:11,910 --> 02:04:17,760
application tax on the huge page and we

02:04:14,970 --> 02:04:20,130
really haven't got a very easy way to do

02:04:17,760 --> 02:04:23,580
that so we have a very high key way

02:04:20,130 --> 02:04:26,010
pretty much we see this we a load we

02:04:23,580 --> 02:04:28,380
identify which is the heart attacks you

02:04:26,010 --> 02:04:33,270
can use a lot and we put them together

02:04:28,380 --> 02:04:35,640
and and try to like to final I'd like to

02:04:33,270 --> 02:04:39,030
Mike meaning my move the front and the

02:04:35,640 --> 02:04:41,460
back extend to maybe more than like two

02:04:39,030 --> 02:04:46,410
magma before Mac just gathered a line

02:04:41,460 --> 02:04:49,470
and we do a matter than advice huge and

02:04:46,410 --> 02:04:51,720
we that's pretty much zero all your

02:04:49,470 --> 02:04:54,300
instruction and you need to somehow copy

02:04:51,720 --> 02:04:57,480
your instruction back which is but that

02:04:54,300 --> 02:05:00,630
works to give us like multiple percent

02:04:57,480 --> 02:05:04,940
savings well we're looking for a better

02:05:00,630 --> 02:05:08,970
way to either with huge TLB FS or like

02:05:04,940 --> 02:05:10,980
THP but we we haven't got a really easy

02:05:08,970 --> 02:05:14,120
one as the one closest we found is to

02:05:10,980 --> 02:05:18,060
put all your like application binary in

02:05:14,120 --> 02:05:22,980
time 5s and use em advice and huge TFS

02:05:18,060 --> 02:05:25,290
which is closer to that but in our case

02:05:22,980 --> 02:05:28,470
we we cannot afford putting the whole

02:05:25,290 --> 02:05:34,290
binary in D run because that's the cost

02:05:28,470 --> 02:05:36,660
money as well so we're actually we're

02:05:34,290 --> 02:05:41,220
actively looking for solutions to to get

02:05:36,660 --> 02:05:47,000
that or to put your tax in the huge page

02:05:41,220 --> 02:05:47,000
so we get less I instruction TLB misses

02:05:48,489 --> 02:05:52,849
well if the page cache will be

02:05:51,050 --> 02:05:55,150
successful then we kind of have that

02:05:52,849 --> 02:06:06,340
right but until that time you have to

02:05:55,150 --> 02:06:06,340
suffer sorry maybe to get this done so

02:06:06,519 --> 02:06:11,929
back to application hints again really

02:06:10,099 --> 02:06:13,610
it can be fairly simple you look at the

02:06:11,929 --> 02:06:15,860
end map segment size if they're mapping

02:06:13,610 --> 02:06:18,860
something you know you know above -

02:06:15,860 --> 02:06:20,329
Megan less than you know three four gigs

02:06:18,860 --> 02:06:21,979
something like that then you lines to

02:06:20,329 --> 02:06:24,320
Meg if they're mapping something that's

02:06:21,979 --> 02:06:27,650
hundreds of gig you line to one gig and

02:06:24,320 --> 02:06:29,389
then you let the later hints or attempts

02:06:27,650 --> 02:06:31,849
use huge pages pick the largest possible

02:06:29,389 --> 02:06:33,530
page that again with other operating

02:06:31,849 --> 02:06:35,329
systems we've done this and it's work

02:06:33,530 --> 02:06:36,619
really well and you don't modify the app

02:06:35,329 --> 02:06:38,929
so the kernel does that fun animals

02:06:36,619 --> 02:06:40,459
memory already for Trent that's just me

02:06:38,929 --> 02:06:41,659
that's a transman huge page team that

02:06:40,459 --> 02:06:44,380
doesn't work for executables by now

02:06:41,659 --> 02:06:53,360
because they are page cache based yeah

02:06:44,380 --> 02:06:55,689
okay I think last question okay thanks

02:06:53,360 --> 02:06:55,689
for coming

02:06:56,040 --> 02:07:03,340
[Applause]

02:08:02,960 --> 02:08:10,970
so my name is Tom Bochy today I'm going

02:08:06,440 --> 02:08:14,090
to talk about sing I come from how and

02:08:10,970 --> 02:08:18,710
today I'm going to talk about an example

02:08:14,090 --> 02:08:21,470
of the problem we might in using working

02:08:18,710 --> 02:08:25,430
queue and we drove back to City Hall

02:08:21,470 --> 02:08:31,460
clock and a possible solution that it's

02:08:25,430 --> 02:08:34,580
basically and also there is something I

02:08:31,460 --> 02:08:40,640
think it would be good to have in the

02:08:34,580 --> 02:08:43,600
work queue API so let's begin the first

02:08:40,640 --> 02:08:46,450
thing is a background story one day that

02:08:43,600 --> 02:08:49,810
Tom McKenney decided he wanted to

02:08:46,450 --> 02:08:51,800
paralyze the expert grace period

02:08:49,810 --> 02:08:54,910
initialization of sorry

02:08:51,800 --> 02:08:59,680
of our see you so he didn't commit and

02:08:54,910 --> 02:09:05,060
there is the code for men knowledge

02:08:59,680 --> 02:09:08,840
logic so it did think that for each you

02:09:05,060 --> 02:09:12,140
know the leaf node of ICU and then in

02:09:08,840 --> 02:09:14,990
each work and put at work in delicate

02:09:12,140 --> 02:09:20,450
work you and then a set of like and then

02:09:14,990 --> 02:09:22,940
they wait for out of like if there is if

02:09:20,450 --> 02:09:27,440
the flag is side and then he flash out

02:09:22,940 --> 02:09:30,590
of work from Network you so I'm going to

02:09:27,440 --> 02:09:33,950
explain a little bit about the Arsenal

02:09:30,590 --> 02:09:38,660
so as your node is you know it's a range

02:09:33,950 --> 02:09:41,870
of CPU which is actually no it's

02:09:38,660 --> 02:09:44,630
basically smaller than Newman node but

02:09:41,870 --> 02:09:50,180
other side is a range of CPU so it's

02:09:44,630 --> 02:09:53,120
bigger than in mine CPU so why that poem

02:09:50,180 --> 02:09:56,240
makini wants to paralyze the crispier

02:09:53,120 --> 02:10:00,650
initialization is because that a large

02:09:56,240 --> 02:10:02,810
system there are many as you know so the

02:10:00,650 --> 02:10:05,780
inertia in this day sorry the

02:10:02,810 --> 02:10:09,530
initialization will take a long time if

02:10:05,780 --> 02:10:12,500
we do that sequentially

02:10:09,530 --> 02:10:17,600
and there are some people report the

02:10:12,500 --> 02:10:22,850
problem that they're synchronized as to

02:10:17,600 --> 02:10:26,630
you for expired expect it to take quite

02:10:22,850 --> 02:10:31,370
long and the major problem is that in

02:10:26,630 --> 02:10:36,290
the initial freeze so that's the purpose

02:10:31,370 --> 02:10:37,940
of this work and you know it's a so the

02:10:36,290 --> 02:10:40,880
solution is that you know goes

02:10:37,940 --> 02:10:44,390
throughout the leaf node and you know

02:10:40,880 --> 02:10:48,560
use the word killed who to paralyze

02:10:44,390 --> 02:10:54,580
their work but and then we see this

02:10:48,560 --> 02:10:59,420
problem and so it's basically the last

02:10:54,580 --> 02:11:05,990
client is the most informative is that

02:10:59,420 --> 02:11:12,470
we have a work you lock up and the log

02:11:05,990 --> 02:11:14,570
that the P topic you state is the

02:11:12,470 --> 02:11:18,020
basically means that the pw0

02:11:14,570 --> 02:11:24,410
is offline so what are these happens

02:11:18,020 --> 02:11:26,480
what are these happens to look at so if

02:11:24,410 --> 02:11:31,700
you look deeply for the look carefully

02:11:26,480 --> 02:11:34,490
for the work queue API there is a thing

02:11:31,700 --> 02:11:37,490
that the color must ensure that when a

02:11:34,490 --> 02:11:40,550
queue the work it's like it must ensure

02:11:37,490 --> 02:11:44,900
that you know the the CPU is not a

02:11:40,550 --> 02:11:47,930
flying Oh ended in a queue in the whole

02:11:44,900 --> 02:11:50,800
queue process that there is no hot black

02:11:47,930 --> 02:11:55,910
and SPU so that's the color that's the

02:11:50,800 --> 02:12:00,560
requirement for the color so like we can

02:11:55,910 --> 02:12:04,880
see in this in this code we do the

02:12:00,560 --> 02:12:09,700
weight queue the work on ListView oh by

02:12:04,880 --> 02:12:13,040
the way the group low is the like I said

02:12:09,700 --> 02:12:16,430
Arsenal is a range of CPU and the group

02:12:13,040 --> 02:12:18,110
flow is the lowest CPU and the group

02:12:16,430 --> 02:12:23,360
high is the highest view

02:12:18,110 --> 02:12:28,580
range so it just pick pick CPU in that

02:12:23,360 --> 02:12:34,869
as you know because that it gains better

02:12:28,580 --> 02:12:37,670
local because that we want work to acute

02:12:34,869 --> 02:12:40,100
locally in in the as you note

02:12:37,670 --> 02:12:44,060
so we killed so you will kill that know

02:12:40,100 --> 02:12:47,600
unless CPU and but you look at the code

02:12:44,060 --> 02:12:50,060
there is no way to ensure that the CPU

02:12:47,600 --> 02:12:53,030
is not offline at the moment when we

02:12:50,060 --> 02:12:56,480
cure work and particularly in that

02:12:53,030 --> 02:12:58,630
problem I think the other said there is

02:12:56,480 --> 02:13:02,510
Hardware back so that's abuser is

02:12:58,630 --> 02:13:08,139
offline all the time so that's where we

02:13:02,510 --> 02:13:13,670
hit them so so I come up the solution

02:13:08,139 --> 02:13:17,869
basically we need to it's no the reason

02:13:13,670 --> 02:13:20,449
why this why this problem happens it

02:13:17,869 --> 02:13:23,860
would be easy to fix that you just make

02:13:20,449 --> 02:13:27,560
sure that when we kill the work on we

02:13:23,860 --> 02:13:30,469
make sure that the CPU is not a fly so

02:13:27,560 --> 02:13:34,820
we can check that in the CPU on a mask

02:13:30,469 --> 02:13:37,760
and if we if all the CPU in the arsenal

02:13:34,820 --> 02:13:42,170
of line then we kill it

02:13:37,760 --> 02:13:45,710
in band work you which will always ask

02:13:42,170 --> 02:13:49,159
as scheduled and render work and so

02:13:45,710 --> 02:13:53,540
notice that there is a branch and

02:13:49,159 --> 02:13:57,110
disable section here so in theory we

02:13:53,540 --> 02:14:01,730
should use the CPU hospital logs to

02:13:57,110 --> 02:14:07,060
brand that two together accurate CPU on

02:14:01,730 --> 02:14:07,060
a mask but I think here we can

02:14:07,539 --> 02:14:20,800
yeah yeah yeah because that preempt is

02:14:18,400 --> 02:14:25,300
able it's really the wrong tool here

02:14:20,800 --> 02:14:26,650
because while it by accident prevents

02:14:25,300 --> 02:14:29,110
CPU hot block

02:14:26,650 --> 02:14:32,019
there's no semantically guarantee for it

02:14:29,110 --> 02:14:35,820
and I really hate it when people have

02:14:32,019 --> 02:14:35,820
make that assumption because it's broken

02:14:36,090 --> 02:14:43,300
pun yes so hammer I said one day won't

02:14:42,610 --> 02:14:47,349
be true anymore

02:14:43,300 --> 02:14:54,300
yes so we are working hard to get this

02:14:47,349 --> 02:14:58,059
stuff not done please don't try to to to

02:14:54,300 --> 02:15:00,460
rely on preempt preamp disabled for for

02:14:58,059 --> 02:15:03,730
these kinds of things we need we need

02:15:00,460 --> 02:15:07,809
better semantically defined solutions

02:15:03,730 --> 02:15:12,159
for that problem we have Thomas is that

02:15:07,809 --> 02:15:13,690
if we put a prepped a CPU hot plug

02:15:12,159 --> 02:15:17,550
disable air we deadlock because of

02:15:13,690 --> 02:15:21,579
people doing grace periods in notifiers

02:15:17,550 --> 02:15:24,280
yeah I know but but still we need to

02:15:21,579 --> 02:15:28,420
find the solution which this is the

02:15:24,280 --> 02:15:30,940
typically duct tape we removed all over

02:15:28,420 --> 02:15:33,190
the place and don't add duct tape back

02:15:30,940 --> 02:15:34,449
so I'll be talking to Sebastian this

02:15:33,190 --> 02:15:36,699
evening there's a we're having an

02:15:34,449 --> 02:15:39,909
informal buff on yeah I know like this

02:15:36,699 --> 02:15:42,550
an RC oh yeah because I mean the thing I

02:15:39,909 --> 02:15:45,039
know that I'm going tour tour Sebastian

02:15:42,550 --> 02:15:46,809
is going to deal with that patch in a

02:15:45,039 --> 02:15:50,699
minute because he's it's going to break

02:15:46,809 --> 02:15:53,710
RT yeah what what I'm doing about that

02:15:50,699 --> 02:15:55,269
in the in the meantime is there's a

02:15:53,710 --> 02:15:57,760
check you can't see this in the dot dot

02:15:55,269 --> 02:16:00,760
dot the one inside the loop right and

02:15:57,760 --> 02:16:03,940
that check looks for this is expedited

02:16:00,760 --> 02:16:05,349
if I'm not too confused still this is

02:16:03,940 --> 02:16:07,750
expedited yeah okay

02:16:05,349 --> 02:16:10,989
and so the what happens in RT you

02:16:07,750 --> 02:16:13,389
disable expedited normally except during

02:16:10,989 --> 02:16:15,070
boot when you can't and so this code

02:16:13,389 --> 02:16:15,920
doesn't do anything for RT except during

02:16:15,070 --> 02:16:18,710
boot

02:16:15,920 --> 02:16:20,510
so what I what I would do is put a check

02:16:18,710 --> 02:16:22,670
and say if it's ringing boudoir if we're

02:16:20,510 --> 02:16:23,720
I could do it now except that somebody's

02:16:22,670 --> 02:16:25,430
probably yell at me for putting a

02:16:23,720 --> 02:16:27,920
configure in thing that doesn't exist a

02:16:25,430 --> 02:16:30,740
mainline yet but if we're at boudoir if

02:16:27,920 --> 02:16:31,940
we're arty full do it the boot time way

02:16:30,740 --> 02:16:35,179
would just cycles through it calls

02:16:31,940 --> 02:16:38,140
directly and that way Artie would avoid

02:16:35,179 --> 02:16:40,610
the preamp disabled wouldn't happen so

02:16:38,140 --> 02:16:42,860
actually I rod I want to have some

02:16:40,610 --> 02:16:45,740
solution where would be smart about that

02:16:42,860 --> 02:16:48,200
and half well because the cue work on

02:16:45,740 --> 02:16:49,969
thing a lot of people tripped over that

02:16:48,200 --> 02:16:51,880
and got it wrong and we fixed a lot of

02:16:49,969 --> 02:16:56,000
places

02:16:51,880 --> 02:16:57,980
yeah there so one solution would be and

02:16:56,000 --> 02:17:04,460
that would be probably the right thing

02:16:57,980 --> 02:17:07,520
to do to have cube work on mask and hand

02:17:04,460 --> 02:17:10,310
in a possible CPU mask and do it in a

02:17:07,520 --> 02:17:12,380
central place well for me it'd be fine

02:17:10,310 --> 02:17:13,910
if that would work but it also be fine

02:17:12,380 --> 02:17:15,800
if it just silently if there's some way

02:17:13,910 --> 02:17:17,630
I could say I really I would like you to

02:17:15,800 --> 02:17:19,280
queue on the CPU but you know if that's

02:17:17,630 --> 02:17:20,630
a problem for ever reason just put it

02:17:19,280 --> 02:17:23,570
somewhere else and I don't care yeah I

02:17:20,630 --> 02:17:27,140
mean say say we have to come up with

02:17:23,570 --> 02:17:29,960
something but we really should do it at

02:17:27,140 --> 02:17:32,600
the work you in the centre of place

02:17:29,960 --> 02:17:35,690
because this breaks again in the game

02:17:32,600 --> 02:17:39,650
yeah I'm with you there there I know who

02:17:35,690 --> 02:17:43,670
is going to clean it up you or tation no

02:17:39,650 --> 02:17:49,390
I have minions for that alright thank

02:17:43,670 --> 02:17:53,530
you for volunteering them okay okay so

02:17:49,390 --> 02:17:56,120
not only just this like this lack

02:17:53,530 --> 02:17:59,780
Tomasello can we just we should not use

02:17:56,120 --> 02:18:02,270
the presentation disabled here also that

02:17:59,780 --> 02:18:04,460
i suppose that we do to it we actually

02:18:02,270 --> 02:18:08,000
we don't care about you know where's

02:18:04,460 --> 02:18:11,390
what what the cpu will excuse that work

02:18:08,000 --> 02:18:15,590
because just we just need to you know

02:18:11,390 --> 02:18:18,170
pick any any number any cpu in that city

02:18:15,590 --> 02:18:22,760
you know in an arson node will be fine

02:18:18,170 --> 02:18:28,280
to expect so excuse that work

02:18:22,760 --> 02:18:29,170
so using this way also makes also lose

02:18:28,280 --> 02:18:33,130
as the

02:18:29,170 --> 02:18:35,800
the sexy flexibility to you know use the

02:18:33,130 --> 02:18:38,889
scaler at scheduler to schedule at work

02:18:35,800 --> 02:18:42,429
in and others in their patterns view so

02:18:38,889 --> 02:18:45,969
as I think we can have a better solution

02:18:42,429 --> 02:18:49,300
so basically here we have the limitation

02:18:45,969 --> 02:18:53,080
for the work queue API if we use first

02:18:49,300 --> 02:18:57,910
view with OQ we can you know control to

02:18:53,080 --> 02:19:00,610
put item and random parallel but we need

02:18:57,910 --> 02:19:04,769
to deal with the hot black scene and if

02:19:00,610 --> 02:19:10,479
we use unbound our queue and it only has

02:19:04,769 --> 02:19:14,679
new Mileva of parallel execution level

02:19:10,479 --> 02:19:18,519
so ideally we we want the work queue API

02:19:14,679 --> 02:19:22,599
to how the functionality to either run

02:19:18,519 --> 02:19:25,750
like n works which n is greater than

02:19:22,599 --> 02:19:29,229
number of Numa node in parallel or we

02:19:25,750 --> 02:19:34,380
want the work queue API to provide a way

02:19:29,229 --> 02:19:38,170
to run the work for each Bank ruined

02:19:34,380 --> 02:19:40,210
group hops views really I mean that

02:19:38,170 --> 02:19:42,760
smaller in the new my node for example

02:19:40,210 --> 02:19:47,650
is astronaut and no need to worry about

02:19:42,760 --> 02:19:50,439
the CPU hot black and I haven't done any

02:19:47,650 --> 02:19:53,290
coding yet but by looking the

02:19:50,439 --> 02:19:57,430
working code I think there are at least

02:19:53,290 --> 02:20:01,180
three possible solution so one thing is

02:19:57,430 --> 02:20:03,130
we can allow pure work organized to you

02:20:01,180 --> 02:20:06,960
in pursue you would kill so basically

02:20:03,130 --> 02:20:11,560
means the work you has to have some

02:20:06,960 --> 02:20:13,720
mechanism to steal grab work from from

02:20:11,560 --> 02:20:22,660
another work sorry

02:20:13,720 --> 02:20:24,970
so it should be work you for so CPU p pw

02:20:22,660 --> 02:20:29,620
should be able to steal a word from

02:20:24,970 --> 02:20:33,370
another PW if that fly so that's that's

02:20:29,620 --> 02:20:36,160
we need we need to modify the working

02:20:33,370 --> 02:20:38,170
code and to make that happens and the

02:20:36,160 --> 02:20:41,560
pros is that we don't need

02:20:38,170 --> 02:20:44,880
to introduce another work you API we can

02:20:41,560 --> 02:20:49,510
choose just used Q work on and the

02:20:44,880 --> 02:20:52,330
downside is that we when we do the work

02:20:49,510 --> 02:20:54,610
heightened processing we need to read

02:20:52,330 --> 02:20:58,210
code to care about that view and grab

02:20:54,610 --> 02:21:01,000
seen in that code and also there is an

02:20:58,210 --> 02:21:04,180
another thing I think I'm concerned

02:21:01,000 --> 02:21:08,290
about is that the the world the work

02:21:04,180 --> 02:21:11,229
items do and grab may conflict with the

02:21:08,290 --> 02:21:14,110
load balance instigator because you know

02:21:11,229 --> 02:21:17,710
the the work you and scheduler may have

02:21:14,110 --> 02:21:20,020
different region different wheels of you

02:21:17,710 --> 02:21:26,410
know which video is busy and we should

02:21:20,020 --> 02:21:28,920
run my work so we need and another

02:21:26,410 --> 02:21:32,410
solution is that I look into the Numa

02:21:28,920 --> 02:21:36,189
grabber code and it's actually in a week

02:21:32,410 --> 02:21:43,120
wisdom modification we can make it

02:21:36,189 --> 02:21:45,640
worked like like per as you note PW

02:21:43,120 --> 02:21:49,120
something like that so it basically

02:21:45,640 --> 02:21:53,350
needed to modify the allocation power

02:21:49,120 --> 02:21:57,790
and also you know how the own CPU to

02:21:53,350 --> 02:22:00,610
note my ping because the because this

02:21:57,790 --> 02:22:03,340
one is burdened because currently we

02:22:00,610 --> 02:22:07,350
used the CPU to note mapping for the

02:22:03,340 --> 02:22:11,320
Manoel so if we want to have an green

02:22:07,350 --> 02:22:16,510
node we need to modify that and this

02:22:11,320 --> 02:22:19,479
should be specific to every work you if

02:22:16,510 --> 02:22:21,490
that the work you use that feature so

02:22:19,479 --> 02:22:28,689
also we need to have different way to

02:22:21,490 --> 02:22:32,200
calculate the spill mask and also the

02:22:28,689 --> 02:22:35,500
frost one I think that the new map it up

02:22:32,200 --> 02:22:39,700
you already handle some already handle

02:22:35,500 --> 02:22:41,860
CP black greatly so maybe we can use

02:22:39,700 --> 02:22:45,560
that code but maybe we need to modify

02:22:41,860 --> 02:22:48,800
that too to make sure because

02:22:45,560 --> 02:22:50,779
to make sure that we have we can handle

02:22:48,800 --> 02:22:55,189
it to be hot back case and in other

02:22:50,779 --> 02:22:58,010
societies so that in her lair so which

02:22:55,189 --> 02:23:00,260
is you know no need to modify or kill

02:22:58,010 --> 02:23:03,859
bad provide another layer based on work

02:23:00,260 --> 02:23:05,779
you to show this so yeah that's

02:23:03,859 --> 02:23:10,239
basically what I'm want to share with

02:23:05,779 --> 02:23:10,239
you so any suggestion or question

02:23:17,789 --> 02:23:27,390
okay so I will give this to Daniel okay

02:23:47,540 --> 02:23:54,829
[Applause]

02:25:11,910 --> 02:25:15,820
okay I'm gonna spend this relatively

02:25:14,230 --> 02:25:17,800
short period talking about a project

02:25:15,820 --> 02:25:20,620
I've been working on for a while now off

02:25:17,800 --> 02:25:23,230
and on called K tasks to parallelize CPU

02:25:20,620 --> 02:25:25,330
intensive work in the kernel so I'm

02:25:23,230 --> 02:25:28,150
gonna try to motivate the problem up

02:25:25,330 --> 02:25:30,010
front first and explain a little bit

02:25:28,150 --> 02:25:32,021
about what K tasks does but hopefully

02:25:30,010 --> 02:25:33,760
leave the majority of the time for some

02:25:32,021 --> 02:25:38,471
feedback that I was hoping to get from

02:25:33,760 --> 02:25:39,990
you all so a couple of times in the

02:25:38,471 --> 02:25:45,910
kernel maybe more than a couple of times

02:25:39,990 --> 02:25:48,130
when we burn a lot of CPU doing well we

02:25:45,910 --> 02:25:50,530
operate on data at a large scale so for

02:25:48,130 --> 02:25:52,420
example at boot time when we initialize

02:25:50,530 --> 02:25:53,950
the structure pages when we started

02:25:52,420 --> 02:25:59,410
guests with via file page pending and

02:25:53,950 --> 02:26:01,210
have to pen a large range of pages these

02:25:59,410 --> 02:26:03,280
things take a long time and there are

02:26:01,210 --> 02:26:05,320
other CPUs on the system that stay idle

02:26:03,280 --> 02:26:06,880
that that prevent the applications and

02:26:05,320 --> 02:26:11,110
even the kernel itself from starting or

02:26:06,880 --> 02:26:15,580
stopping more quickly so you know just

02:26:11,110 --> 02:26:17,080
as one example of the kind of speed ups

02:26:15,580 --> 02:26:19,300
this work can bring when we were

02:26:17,080 --> 02:26:22,181
starting a large you know kayvyun guest

02:26:19,300 --> 02:26:24,900
on a big server we can use threads to

02:26:22,181 --> 02:26:24,900
pretty great effect

02:26:28,149 --> 02:26:32,390
so the idea is that since there are so

02:26:31,040 --> 02:26:34,939
many different places in the kernel that

02:26:32,390 --> 02:26:36,320
that have this problem and there are so

02:26:34,939 --> 02:26:38,000
many kind of nasty things you have to

02:26:36,320 --> 02:26:40,340
worry about when you start many threads

02:26:38,000 --> 02:26:43,100
we want to encapsulate all the nastiness

02:26:40,340 --> 02:26:46,310
inside of a new API that can handle this

02:26:43,100 --> 02:26:48,200
for you so the basic idea with K task is

02:26:46,310 --> 02:26:51,080
that you pass in the size of the task

02:26:48,200 --> 02:26:52,550
some start of the range a thread

02:26:51,080 --> 02:26:53,960
function that does a part of the task

02:26:52,550 --> 02:26:57,470
and a few other parameters that I'll

02:26:53,960 --> 02:26:59,120
talk about later and K tasks will split

02:26:57,470 --> 02:27:02,060
up the work for you it will load balance

02:26:59,120 --> 02:27:04,220
between the threads so that the start

02:27:02,060 --> 02:27:05,240
and stop times don't vary too much and

02:27:04,220 --> 02:27:09,109
one thread isn't holding up the other

02:27:05,240 --> 02:27:11,890
ones que TAS can respect other load on

02:27:09,109 --> 02:27:14,600
the system so if it's very busy then

02:27:11,890 --> 02:27:17,060
multi-threading and optimizing one

02:27:14,600 --> 02:27:19,010
kernel code path won't hurt those tasks

02:27:17,060 --> 02:27:20,630
I do this by running the helper threads

02:27:19,010 --> 02:27:22,399
at the lowest priority on the system and

02:27:20,630 --> 02:27:25,070
there others and and there's some other

02:27:22,399 --> 02:27:29,060
specific details to make that work well

02:27:25,070 --> 02:27:31,910
that I can go into if you want the thing

02:27:29,060 --> 02:27:33,680
should probably be aware of C group also

02:27:31,910 --> 02:27:36,290
that's work in progress that can be done

02:27:33,680 --> 02:27:40,790
completely separately beforehand that's

02:27:36,290 --> 02:27:42,530
to do you can do a few other things too

02:27:40,790 --> 02:27:44,000
it can control where you run the helper

02:27:42,530 --> 02:27:47,420
threads on this node or that node of the

02:27:44,000 --> 02:27:49,160
entire system so so that's the basic

02:27:47,420 --> 02:27:52,660
idea of the framework are there any

02:27:49,160 --> 02:27:52,660
questions before I go on

02:27:53,720 --> 02:28:03,080
oh yes why is is it really the right

02:28:00,649 --> 02:28:08,300
policy to say run at max nice to voices

02:28:03,080 --> 02:28:11,540
some disturbance I mean if if my task is

02:28:08,300 --> 02:28:13,700
running a maximum RT priority the

02:28:11,540 --> 02:28:14,979
sysadmin has said i want this to execute

02:28:13,700 --> 02:28:18,170
with the highest possible priority

02:28:14,979 --> 02:28:21,950
shouldn't the work being done on its

02:28:18,170 --> 02:28:24,460
behalf by other cpus also run at that

02:28:21,950 --> 02:28:28,010
same priority that's a good question and

02:28:24,460 --> 02:28:29,479
we well as we it's worth some thought

02:28:28,010 --> 02:28:32,270
but I will point out that the main

02:28:29,479 --> 02:28:34,399
thread stays at the current priority so

02:28:32,270 --> 02:28:36,319
we you know we make progress in that one

02:28:34,399 --> 02:28:37,640
thread but if you have a special case

02:28:36,319 --> 02:28:39,020
like that maybe this framework isn't

02:28:37,640 --> 02:28:42,580
appropriate maybe we have some more work

02:28:39,020 --> 02:28:42,580
to do to address cases like that

02:28:51,900 --> 02:28:56,950
your answer is essentially correct with

02:28:54,400 --> 02:28:59,051
with the there's one task that has the

02:28:56,950 --> 02:29:01,570
right to run an IRT priority that's the

02:28:59,051 --> 02:29:04,120
setup here and and you answer that that

02:29:01,570 --> 02:29:06,011
one part that that one helper that one

02:29:04,120 --> 02:29:07,960
main thread runs at that RT priority

02:29:06,011 --> 02:29:09,551
that's the only right you have all the

02:29:07,960 --> 02:29:11,681
helpers run at the max nice you don't

02:29:09,551 --> 02:29:14,320
have permission to do otherwise unless

02:29:11,681 --> 02:29:17,650
the system or the user gives it to you

02:29:14,320 --> 02:29:20,830
in some way but if if I have a toaster

02:29:17,650 --> 02:29:25,570
she's running at RT priority and I spawn

02:29:20,830 --> 02:29:28,391
threads from that task other CPUs might

02:29:25,570 --> 02:29:30,250
also be running RT things you don't have

02:29:28,391 --> 02:29:33,341
a right to monopolize the whole system

02:29:30,250 --> 02:29:35,410
because your one task is RT there might

02:29:33,341 --> 02:29:36,490
be other parts I don't mean that's kind

02:29:35,410 --> 02:29:39,610
of the favorite that's quite the point

02:29:36,490 --> 02:29:41,261
of the Aussie scheduling class if you

02:29:39,610 --> 02:29:43,421
could globally look at the system know

02:29:41,261 --> 02:29:45,101
there are no other you know things of

02:29:43,421 --> 02:29:47,131
that party out there then yeah you know

02:29:45,101 --> 02:29:49,750
you don't have a ball technical system

02:29:47,131 --> 02:29:52,091
that's a step beyond what Daniels trying

02:29:49,750 --> 02:29:54,250
to do right now yeah but what I'm saying

02:29:52,091 --> 02:29:56,591
is that if I were to divide up the work

02:29:54,250 --> 02:29:58,301
that needs to be done assuming it even

02:29:56,591 --> 02:30:01,210
was away - but assuming this I can

02:29:58,301 --> 02:30:04,091
divide up the work into eight chunks and

02:30:01,210 --> 02:30:05,500
spawned seven extra threads and each one

02:30:04,091 --> 02:30:08,681
of those makes a sis call to do one

02:30:05,500 --> 02:30:11,200
eight for the work then I can do all

02:30:08,681 --> 02:30:14,171
that and all of those will run at RT

02:30:11,200 --> 02:30:16,301
priority in an ideal world yes that

02:30:14,171 --> 02:30:19,181
there is overhead to the paralyzation so

02:30:16,301 --> 02:30:20,410
you get 50 75 % so in fact you're using

02:30:19,181 --> 02:30:23,141
more resources than you would otherwise

02:30:20,410 --> 02:30:26,971
so it's not a not a perfect paralyzation

02:30:23,141 --> 02:30:29,801
in general right but things the the this

02:30:26,971 --> 02:30:32,830
chooses to use less CPU and and and that

02:30:29,801 --> 02:30:35,650
cost and that choice is being made for

02:30:32,830 --> 02:30:37,750
me by Daniel that's a good point I mean

02:30:35,650 --> 02:30:40,271
there's no reason we can't you know add

02:30:37,750 --> 02:30:42,221
a way to change this behavior if you if

02:30:40,271 --> 02:30:44,320
you have a special case like that I'd be

02:30:42,221 --> 02:30:48,820
curious to hear what we're doing at that

02:30:44,320 --> 02:30:49,931
high of a priority level that you some

02:30:48,820 --> 02:30:52,540
that much

02:30:49,931 --> 02:30:54,761
yeah I mean I absolutely we need we need

02:30:52,540 --> 02:30:57,431
users to come forward and say oh this

02:30:54,761 --> 02:30:59,200
this thing that you've done introduces a

02:30:57,431 --> 02:31:06,761
problem in my program and here's why but

02:30:59,200 --> 02:31:09,551
yeah yeah you run into issues where one

02:31:06,761 --> 02:31:11,801
of those K worker tasks had has gotten a

02:31:09,551 --> 02:31:13,960
piece of the work to do and then can't

02:31:11,801 --> 02:31:15,551
run anywhere because there's too low

02:31:13,960 --> 02:31:17,620
priority and there's no work to do and

02:31:15,551 --> 02:31:20,771
so your main thread has actually stopped

02:31:17,620 --> 02:31:22,780
by the fact that it keeps this one piece

02:31:20,771 --> 02:31:24,370
has been taken it's been basically

02:31:22,780 --> 02:31:27,690
reserved but it's not actually getting

02:31:24,370 --> 02:31:30,910
executed yeah we address starvation so

02:31:27,690 --> 02:31:32,740
all of the threads run by taking the

02:31:30,910 --> 02:31:34,330
next chunk of work to do and then when

02:31:32,740 --> 02:31:36,160
the main thread is done it waits for all

02:31:34,330 --> 02:31:38,681
the other helpers to finish and while it

02:31:36,160 --> 02:31:41,320
does that it will its own priority to

02:31:38,681 --> 02:31:44,021
the next helper one at a time and so we

02:31:41,320 --> 02:31:45,341
have at most one thread always running

02:31:44,021 --> 02:31:48,601
at the original priority of the caller

02:31:45,341 --> 02:31:48,601
to avoid that kind of a thing

02:31:56,271 --> 02:32:01,730
one thing I wonder is like you're

02:31:59,210 --> 02:32:05,030
breaking that task into chunks of words

02:32:01,730 --> 02:32:07,610
right so there cannot be dependency

02:32:05,030 --> 02:32:09,891
between the chunks of work right that's

02:32:07,610 --> 02:32:11,860
a requirement not as it's written now

02:32:09,891 --> 02:32:16,340
now No

02:32:11,860 --> 02:32:19,370
so like is it required to have like a

02:32:16,340 --> 02:32:22,490
division into chunks beforehand or like

02:32:19,370 --> 02:32:27,370
can we have for example spawn a number

02:32:22,490 --> 02:32:27,370
of workers and then they pick up work

02:32:27,431 --> 02:32:34,340
they finish up with their previous work

02:32:31,360 --> 02:32:36,590
so let's see I guess I'm missing the

02:32:34,340 --> 02:32:38,690
main thrust of the question but I mean

02:32:36,590 --> 02:32:40,040
the framework is written assuming that

02:32:38,690 --> 02:32:42,950
you know the size of the work to do

02:32:40,040 --> 02:32:46,160
beforehand right but I mean you might

02:32:42,950 --> 02:32:47,840
know let's say you have a certain number

02:32:46,160 --> 02:32:50,210
of things that you have to process right

02:32:47,840 --> 02:32:54,050
but they might not be processed in equal

02:32:50,210 --> 02:32:58,010
times so it might be that some have

02:32:54,050 --> 02:32:59,750
another sorry so I mean you might have a

02:32:58,010 --> 02:33:04,610
certain number of things that you want

02:32:59,750 --> 02:33:06,590
to process and so you know the number of

02:33:04,610 --> 02:33:10,190
keys for example but you might not

02:33:06,590 --> 02:33:12,351
necessarily know how much time each of

02:33:10,190 --> 02:33:14,931
them would take to process so maybe some

02:33:12,351 --> 02:33:16,400
part will take more to possess some

02:33:14,931 --> 02:33:19,010
other part will take less time to

02:33:16,400 --> 02:33:20,360
process so instead of providing based on

02:33:19,010 --> 02:33:23,630
and the more of items it might be better

02:33:20,360 --> 02:33:26,300
to actually pick up work as you finish

02:33:23,630 --> 02:33:30,170
off with your previous work in terms of

02:33:26,300 --> 02:33:33,141
like I suppose I addressed that

02:33:30,170 --> 02:33:35,271
partially by dividing the work into more

02:33:33,141 --> 02:33:38,210
chunks than there are threads and so if

02:33:35,271 --> 02:33:39,710
there's varying time in each chunk it

02:33:38,210 --> 02:33:42,311
can smooth it out a little bit like that

02:33:39,710 --> 02:33:42,311
yeah

02:33:47,471 --> 02:33:55,490
yeah so I think I know of an example of

02:33:51,070 --> 02:34:00,710
what he was saying there was an attempt

02:33:55,490 --> 02:34:04,070
to parallel eyes are mapping page tables

02:34:00,710 --> 02:34:06,860
of a task and one of the issues that

02:34:04,070 --> 02:34:12,771
were expected with that is you don't

02:34:06,860 --> 02:34:18,290
know beforehand the density of page

02:34:12,771 --> 02:34:24,681
tables in different areas of the virtual

02:34:18,290 --> 02:34:28,160
address space so the divider tasks based

02:34:24,681 --> 02:34:31,431
on the virtual addresses some of them

02:34:28,160 --> 02:34:35,450
might be more expensive than others so

02:34:31,431 --> 02:34:39,351
you think this could be used for for for

02:34:35,450 --> 02:34:41,030
this purpose I do and you know I'm gonna

02:34:39,351 --> 02:34:42,891
speak to that case specifically because

02:34:41,030 --> 02:34:44,330
I know I know those patches they were

02:34:42,891 --> 02:34:48,800
from air and I think like a year and

02:34:44,330 --> 02:34:50,150
change ago walking page tables you're

02:34:48,800 --> 02:34:51,710
absolutely right we don't know how

02:34:50,150 --> 02:34:53,000
sparse the mappings are but the heavy

02:34:51,710 --> 02:34:54,261
part of that operation is actually

02:34:53,000 --> 02:34:56,601
freeing the pages back to the page

02:34:54,261 --> 02:34:59,660
allocator and that's mm you gather type

02:34:56,601 --> 02:35:01,490
stuff and when we make that list up we

02:34:59,660 --> 02:35:02,840
actually do know exactly how long it is

02:35:01,490 --> 02:35:04,460
and every single one is obviously packed

02:35:02,840 --> 02:35:06,561
by page that's what the list is you know

02:35:04,460 --> 02:35:08,080
it's supposed to pages so you know

02:35:06,561 --> 02:35:12,130
that's the part that we want to optimize

02:35:08,080 --> 02:35:12,130
and I think it can work well for that

02:35:16,279 --> 02:35:21,960
so you showed some like improvement

02:35:20,159 --> 02:35:24,750
numbers in the first slide but they were

02:35:21,960 --> 02:35:26,699
like the pure paralysing the task right

02:35:24,750 --> 02:35:29,220
what is the overall improvement in any

02:35:26,699 --> 02:35:31,739
real workload like while you paralyze

02:35:29,220 --> 02:35:33,600
some parts some background tasks are

02:35:31,739 --> 02:35:37,080
been paralyzed is there intangible

02:35:33,600 --> 02:35:39,449
effect on a overall real workload on the

02:35:37,080 --> 02:35:41,069
overall which sorry I mean the initial

02:35:39,449 --> 02:35:45,300
numbers that you showed on the first

02:35:41,069 --> 02:35:47,220
slide that's just the 2x or 16x some of

02:35:45,300 --> 02:35:49,290
the numbers were there that's just

02:35:47,220 --> 02:35:51,810
paralyzing the task right you just

02:35:49,290 --> 02:35:58,140
measure the task when paralyzed how much

02:35:51,810 --> 02:36:00,949
class it takes yeah saying can go to a

02:35:58,140 --> 02:36:00,949
first light yeah

02:36:17,410 --> 02:36:25,210
yes or the speed-up so that's just the

02:36:22,780 --> 02:36:28,540
speed-up paralyzing the task right using

02:36:25,210 --> 02:36:30,430
that K tasks but what is in a real

02:36:28,540 --> 02:36:32,290
scenario certain tasks are being

02:36:30,430 --> 02:36:34,720
paralyzed in the paralyzed in the

02:36:32,290 --> 02:36:36,370
background like you're running database

02:36:34,720 --> 02:36:39,880
or some other workload so what is the

02:36:36,370 --> 02:36:42,700
overall effect there's a competing

02:36:39,880 --> 02:36:45,340
workload you're saying I mean if this

02:36:42,700 --> 02:36:47,050
paralyzing thing is freeing up some

02:36:45,340 --> 02:36:48,700
resources or something so you were doing

02:36:47,050 --> 02:36:51,939
it faster but how does that eventually

02:36:48,700 --> 02:36:53,200
affect this this Ino workload will be

02:36:51,939 --> 02:36:54,780
speed up by that right

02:36:53,200 --> 02:36:59,200
essentially no real workload just

02:36:54,780 --> 02:37:00,280
measuring a small paralyzing thing yeah

02:36:59,200 --> 02:37:02,080
I mean if there are other things

02:37:00,280 --> 02:37:05,050
happening on the system then then

02:37:02,080 --> 02:37:06,910
obviously this won't be nearly as fast

02:37:05,050 --> 02:37:10,030
but that's what we want I mean we we

02:37:06,910 --> 02:37:15,700
want to avoid disturbing other tasks

02:37:10,030 --> 02:37:22,120
just for the sake of an optimization so

02:37:15,700 --> 02:37:29,520
this is a real-world data on a system

02:37:22,120 --> 02:37:29,520
where you start like a queue you just

02:37:37,040 --> 02:37:41,700
there are probably other things that are

02:37:39,210 --> 02:37:42,870
depending on this thing right and so

02:37:41,700 --> 02:37:46,140
speeding this up you're hoping that

02:37:42,870 --> 02:37:47,250
overall things will improve what I'm

02:37:46,140 --> 02:37:50,130
saying like what is the overall

02:37:47,250 --> 02:37:52,979
improvement in any operational workload

02:37:50,130 --> 02:37:54,990
subs database is running and some tasks

02:37:52,979 --> 02:37:57,270
are being sped up using gate asks is

02:37:54,990 --> 02:38:00,840
there any visible difference in like

02:37:57,270 --> 02:38:04,560
throughput over all stuff like that well

02:38:00,840 --> 02:38:15,840
no I mean this this is about starting up

02:38:04,560 --> 02:38:17,820
a so if you are using the startup

02:38:15,840 --> 02:38:22,920
example so like what does the overall

02:38:17,820 --> 02:38:24,840
startup time reduction that's the total

02:38:22,920 --> 02:38:27,240
setup time you've got this yeah right

02:38:24,840 --> 02:38:29,820
with one thread you know it takes over a

02:38:27,240 --> 02:38:32,550
minute to startup and then you know you

02:38:29,820 --> 02:38:36,479
can see the times fall okay okay yeah so

02:38:32,550 --> 02:38:38,520
we're starting up a VM faster right but

02:38:36,479 --> 02:38:40,380
so the idea here is you you have the one

02:38:38,520 --> 02:38:44,550
VM you're starting up and that one VM

02:38:40,380 --> 02:38:46,439
starts up faster now if you had 200 VMs

02:38:44,550 --> 02:38:48,149
you're having the startup what's gonna

02:38:46,439 --> 02:38:50,370
be the penalty you're gonna take for it

02:38:48,149 --> 02:38:53,760
going and overflowing the number of CPUs

02:38:50,370 --> 02:38:55,950
you have on the system due to all the

02:38:53,760 --> 02:38:57,720
parallelization that was added right so

02:38:55,950 --> 02:38:59,970
one of the things that Kay test does is

02:38:57,720 --> 02:39:01,590
it prevents this overly optimistic sort

02:38:59,970 --> 02:39:03,030
of flooding the system with threads we

02:39:01,590 --> 02:39:04,200
really don't want to do that so in

02:39:03,030 --> 02:39:05,939
addition to running them at lower

02:39:04,200 --> 02:39:07,950
priority we also have caps on the number

02:39:05,939 --> 02:39:11,359
of threads that run per node as well as

02:39:07,950 --> 02:39:11,359
on the entire system okay

02:39:12,940 --> 02:39:18,860
I'm sorry but you are talking about a

02:39:15,951 --> 02:39:20,721
single task what if this task is issuing

02:39:18,860 --> 02:39:23,421
transactional instructions to the CPU

02:39:20,721 --> 02:39:28,190
what if you are actually splitting this

02:39:23,421 --> 02:39:29,780
transaction not two up on that area but

02:39:28,190 --> 02:39:32,030
I would imagine it would be a thing that

02:39:29,780 --> 02:39:33,501
we would point out and just say be

02:39:32,030 --> 02:39:52,730
careful if you're using it for this you

02:39:33,501 --> 02:39:54,621
know just repeat for the audio so so the

02:39:52,730 --> 02:39:56,121
client calling the framework knows how

02:39:54,621 --> 02:39:57,801
to break up the work they know whether

02:39:56,121 --> 02:39:59,601
it's safe to break up the chunk they

02:39:57,801 --> 02:40:01,641
know whether a chunk is atomic or not

02:39:59,601 --> 02:40:18,261
and can't be broken up so it's purely a

02:40:01,641 --> 02:40:20,360
decision on the client how quickly can

02:40:18,261 --> 02:40:22,400
you go over what the construct might be

02:40:20,360 --> 02:40:26,780
if I were to use if I know ahead of time

02:40:22,400 --> 02:40:29,061
let's say I have a lot of intensive work

02:40:26,780 --> 02:40:32,030
to be done in the kernel what would be

02:40:29,061 --> 02:40:34,551
the construct of such a how do I request

02:40:32,030 --> 02:40:37,011
for this is this an API would I specify

02:40:34,551 --> 02:40:39,051
the number of threads and what are the

02:40:37,011 --> 02:40:42,621
criteria I should consider in an ideal

02:40:39,051 --> 02:40:45,980
situation for such a usage right so you

02:40:42,621 --> 02:40:48,921
you know it's a new API you pass on a

02:40:45,980 --> 02:40:50,771
few parameters that explain you know the

02:40:48,921 --> 02:40:53,211
size of the task where it starts the

02:40:50,771 --> 02:40:54,261
thread function and critically there are

02:40:53,211 --> 02:40:55,701
a couple other parameters that I was

02:40:54,261 --> 02:40:59,811
hoping to talk about although I'm all

02:40:55,701 --> 02:41:01,070
out of time the minimum chunk size is

02:40:59,811 --> 02:41:03,230
very important it

02:41:01,070 --> 02:41:06,021
it describes the minimum amount of work

02:41:03,230 --> 02:41:08,001
that makes sense for one thread to do so

02:41:06,021 --> 02:41:09,130
if you're operating on a large range of

02:41:08,001 --> 02:41:10,540
memory say

02:41:09,130 --> 02:41:12,250
20 megabytes would be your chunk size

02:41:10,540 --> 02:41:13,479
rather than something like 1 megabyte so

02:41:12,250 --> 02:41:15,070
that you wouldn't start

02:41:13,479 --> 02:41:23,380
I'm team threads to operate on these

02:41:15,070 --> 02:41:25,689
tiny ranges then Numa where I mean you

02:41:23,380 --> 02:41:31,840
can have tasks which sort of are new

02:41:25,689 --> 02:41:34,060
more sensitive so can the API take yeah

02:41:31,840 --> 02:41:35,830
the there are two api's and one will

02:41:34,060 --> 02:41:37,540
allow you to do that you can open you

02:41:35,830 --> 02:41:45,010
can say which parts of the task run on

02:41:37,540 --> 02:41:47,380
which nodes was this is this specific to

02:41:45,010 --> 02:41:52,479
one type of cluster for example arm has

02:41:47,380 --> 02:41:55,540
different speed CPUs it's not worth of

02:41:52,479 --> 02:42:01,780
hardware layout specifically no no so

02:41:55,540 --> 02:42:04,960
basically the program what what you are

02:42:01,780 --> 02:42:06,970
running and is there would it be useful

02:42:04,960 --> 02:42:09,790
what I'm saying is would it be useful to

02:42:06,970 --> 02:42:12,220
give that option as well maybe provide

02:42:09,790 --> 02:42:16,510
CPU masks for these parallelized threads

02:42:12,220 --> 02:42:18,939
to run on yeah that is one possible

02:42:16,510 --> 02:42:20,680
extension sure if you wanted to be more

02:42:18,939 --> 02:42:24,780
specific about where things ran then

02:42:20,680 --> 02:42:24,780
just which node you know sure

02:42:26,770 --> 02:42:31,300
I think this is a last question I don't

02:42:29,380 --> 02:42:35,410
want to run over too much okay

02:42:31,300 --> 02:42:38,350
I think basically the the current unico

02:42:35,410 --> 02:42:42,370
or did do some parallel use new citation

02:42:38,350 --> 02:42:46,120
or memory but what just one track nope

02:42:42,370 --> 02:42:50,830
so are you going to change the way you

02:42:46,120 --> 02:42:53,860
work instead of doing that you spawned

02:42:50,830 --> 02:42:55,420
the Kate has to do order in your

02:42:53,860 --> 02:42:56,830
citation sorry I missed the beginning

02:42:55,420 --> 02:42:57,280
but you're talking about structures

02:42:56,830 --> 02:43:00,310
right

02:42:57,280 --> 02:43:02,290
it struck page yes structure P citation

02:43:00,310 --> 02:43:06,040
it's already current done in parallel

02:43:02,290 --> 02:43:08,440
but one fret and oh yes so I actually do

02:43:06,040 --> 02:43:11,260
build on top of that I don't change what

02:43:08,440 --> 02:43:13,720
Mel added because Kate s requires you to

02:43:11,260 --> 02:43:15,280
describe the work upfront and so you

02:43:13,720 --> 02:43:16,990
would have to traverse all of the ranges

02:43:15,280 --> 02:43:19,660
beforehand in order to be able to

02:43:16,990 --> 02:43:22,660
describe it to the API if you just ran

02:43:19,660 --> 02:43:24,280
each each thread in a node like we do

02:43:22,660 --> 02:43:27,690
now then you don't have to do that you

02:43:24,280 --> 02:43:27,690
can just start the K task right off

02:43:28,190 --> 02:43:34,221
I have a last question oh very very

02:43:31,311 --> 02:43:36,110
short one so I read the RFC

02:43:34,221 --> 02:43:38,900
documentation it says that these four

02:43:36,110 --> 02:43:40,311
CPU intensive work is that the only

02:43:38,900 --> 02:43:42,801
intent or can it be used for i/o

02:43:40,311 --> 02:43:45,200
intensive or a high latency I your work

02:43:42,801 --> 02:43:47,271
that we might want to do for storage in

02:43:45,200 --> 02:43:52,940
Carmel can it be replacement for work

02:43:47,271 --> 02:43:54,620
use to start moving to K work or you

02:43:52,940 --> 02:43:56,211
know honestly I I haven't thought about

02:43:54,620 --> 02:44:01,160
that at all but it is one possible

02:43:56,211 --> 02:44:03,090
extension okay thanks okay thank you

02:44:01,160 --> 02:44:09,049
very much

02:44:03,090 --> 02:44:09,049
[Applause]

02:45:28,230 --> 02:45:34,110
hello hello hello folks I'm Jung Joo

02:45:31,800 --> 02:45:40,710
from Alibaba and going to talk about

02:45:34,110 --> 02:45:44,820
something about IMF say yeah

02:45:40,710 --> 02:45:48,000
firstly I'm going to speak what our

02:45:44,820 --> 02:45:51,900
problems about mm Sam I think Polly

02:45:48,000 --> 02:45:54,811
knows about what I'm doing so protecting

02:45:51,900 --> 02:45:58,200
the address space of the process but the

02:45:54,811 --> 02:46:00,540
abuse of the misuse of the mf7 may lead

02:45:58,200 --> 02:46:04,970
to some processor stalled for a very

02:46:00,540 --> 02:46:09,210
long time to process my guests are

02:46:04,970 --> 02:46:12,061
something else and it may cost I all

02:46:09,210 --> 02:46:16,820
very worried because to a pitfall to me

02:46:12,061 --> 02:46:19,170
so I open holding the MF set and some

02:46:16,820 --> 02:46:25,230
low contention between the Northeast

02:46:19,170 --> 02:46:27,900
right process what is right asks in the

02:46:25,230 --> 02:46:31,860
past year we in the community we have

02:46:27,900 --> 02:46:37,740
done some work to mitigate the easy of

02:46:31,860 --> 02:46:40,470
the I'm upset we we have we have tons of

02:46:37,740 --> 02:46:42,150
people after something I know something

02:46:40,470 --> 02:46:46,050
are going on

02:46:42,150 --> 02:46:51,870
firstly we can use for some scenario we

02:46:46,050 --> 02:46:55,110
can use 12 lakhs to to avoid the sleep

02:46:51,870 --> 02:46:58,200
and medicine for some news case for

02:46:55,110 --> 02:47:03,210
example a huge PD because the hillbilly

02:46:58,200 --> 02:47:07,170
will memory address space at least to

02:47:03,210 --> 02:47:11,851
try to collapse PHP but it has a not to

02:47:07,170 --> 02:47:14,120
wait for the but it has not to it for

02:47:11,851 --> 02:47:18,110
the specific process it can skip the

02:47:14,120 --> 02:47:22,080
block the process and the other

02:47:18,110 --> 02:47:27,690
mitigation is there to dump grated mmm

02:47:22,080 --> 02:47:30,480
from a right to read into a map and PRK

02:47:27,690 --> 02:47:34,170
and Umrah map when it when they are

02:47:30,480 --> 02:47:36,840
shrinking the very large making to two

02:47:34,170 --> 02:47:39,810
lies the other swear to

02:47:36,840 --> 02:47:42,510
to commune in parallel and we have

02:47:39,810 --> 02:47:44,609
something else on going

02:47:42,510 --> 02:47:47,460
the first one is a speculative pitiful

02:47:44,609 --> 02:47:50,760
from Lauren but it's a the pleasure

02:47:47,460 --> 02:47:53,550
still reviewing and it tries to avoid

02:47:50,760 --> 02:47:56,600
holding the map assignment in the

02:47:53,550 --> 02:47:59,160
Pettiford pass and that the other one is

02:47:56,600 --> 02:48:05,210
chopped and understand when the doing IO

02:47:59,160 --> 02:48:08,189
into a beautiful pass from Joseph aim to

02:48:05,210 --> 02:48:11,100
why the IO priority in worrying and

02:48:08,189 --> 02:48:13,920
actually story so one thing I think

02:48:11,100 --> 02:48:17,399
there's nothing missing father using the

02:48:13,920 --> 02:48:20,780
swipe slap-ass when doing ass wipe the

02:48:17,399 --> 02:48:25,470
sweat will try to have some patreon but

02:48:20,780 --> 02:48:28,260
in this pasture is not we have not do

02:48:25,470 --> 02:48:30,870
anything in this pasture to a wider and

02:48:28,260 --> 02:48:39,170
understand - all right holding MF save a

02:48:30,870 --> 02:48:41,010
long time so I'm and actually all the

02:48:39,170 --> 02:48:43,680
approaches on the waste on the patches

02:48:41,010 --> 02:48:46,590
are trying to mitigate the problem but

02:48:43,680 --> 02:48:50,460
there's a more fundamental issues in the

02:48:46,590 --> 02:48:53,790
understand so I think the most important

02:48:50,460 --> 02:48:55,680
problem is that it's not easy to figure

02:48:53,790 --> 02:49:01,380
out what I'm Emerson really protects

02:48:55,680 --> 02:49:04,380
because we understand to protect way any

02:49:01,380 --> 02:49:07,530
arbitrary and the way maillist and when

02:49:04,380 --> 02:49:10,590
we flex and even worse that we have to

02:49:07,530 --> 02:49:15,630
hold the exclusive lock for updating the

02:49:10,590 --> 02:49:18,660
Whammy flags and some field in the mmm

02:49:15,630 --> 02:49:23,330
struggle you can see we used to use mmm

02:49:18,660 --> 02:49:26,609
to synchronize the access to ox are

02:49:23,330 --> 02:49:29,760
something in the an instructor but it

02:49:26,609 --> 02:49:35,689
has been solved in the 4.8 we introduced

02:49:29,760 --> 02:49:39,570
the dedication lock for what s tough so

02:49:35,689 --> 02:49:42,570
actually I don't have a specific idea

02:49:39,570 --> 02:49:45,740
about how to solve the fundamental issue

02:49:42,570 --> 02:49:49,380
so I would like to

02:49:45,740 --> 02:49:53,030
you know bring the repeat bring the

02:49:49,380 --> 02:49:56,150
problem to the broader audience to see

02:49:53,030 --> 02:50:00,061
anyway I also has a better end here so

02:49:56,150 --> 02:50:05,340
we may use some finer grained a lock for

02:50:00,061 --> 02:50:18,330
example green lock for we may love to to

02:50:05,340 --> 02:50:20,730
Kyoto another side so finally so the

02:50:18,330 --> 02:50:23,550
pervy ma lock thing has been suggested

02:50:20,730 --> 02:50:27,150
before in the past but most people agree

02:50:23,550 --> 02:50:29,280
that it would just transition the MSM

02:50:27,150 --> 02:50:30,750
and to that the contingent wouldn't vary

02:50:29,280 --> 02:50:32,311
that much

02:50:30,750 --> 02:50:36,750
particularly when you're mainly doing

02:50:32,311 --> 02:50:38,311
operations on the same VMA so you'd be

02:50:36,750 --> 02:50:42,061
replacing one point of contention for

02:50:38,311 --> 02:50:44,400
another for what for the pervy ma lock

02:50:42,061 --> 02:50:49,320
thing you'd be replacing one point of

02:50:44,400 --> 02:50:58,080
contention with another oh no I did

02:50:49,320 --> 02:51:00,840
enjoy that no I'm actually talking about

02:50:58,080 --> 02:51:02,340
the the per VM a lock but if you're

02:51:00,840 --> 02:51:06,540
working with just one VM a of course

02:51:02,340 --> 02:51:08,101
yeah the rain the range walking will

02:51:06,540 --> 02:51:09,420
still help but there are races right now

02:51:08,101 --> 02:51:11,780
with when you're dealing with a single

02:51:09,420 --> 02:51:11,780
VM a

02:51:18,630 --> 02:51:24,130
so could be you can all outcomes are

02:51:21,550 --> 02:51:26,740
going could we perhaps darks play think

02:51:24,130 --> 02:51:30,540
we are mice even though if they have the

02:51:26,740 --> 02:51:33,521
same flex and stuff just to avoid the

02:51:30,540 --> 02:51:36,641
contention like have some maximum via my

02:51:33,521 --> 02:51:39,721
size Rhiannon's VMA size and then we

02:51:36,641 --> 02:51:39,721
splitted it tomorrow

02:51:46,501 --> 02:51:52,030
that would actually help because a lot

02:51:49,301 --> 02:51:54,671
of times when you have a program it has

02:51:52,030 --> 02:51:58,211
a gigantic VMA where most accesses

02:51:54,671 --> 02:52:00,641
happen you don't know the different

02:51:58,211 --> 02:52:04,900
threads will just be accessing stuff

02:52:00,641 --> 02:52:08,261
randomly all over and simply splitting

02:52:04,900 --> 02:52:11,051
up the VMA into more might not actually

02:52:08,261 --> 02:52:13,061
help you yeah it's true actually for

02:52:11,051 --> 02:52:16,450
some applications when they have a very

02:52:13,061 --> 02:52:18,490
gigantic vme and some may have a very a

02:52:16,450 --> 02:52:23,740
lot of very small ways

02:52:18,490 --> 02:52:26,921
so well actually if how much the windows

02:52:23,740 --> 02:52:31,030
logon prayer can benefit for different

02:52:26,921 --> 02:52:33,851
workloads so if for maybe for the

02:52:31,030 --> 02:52:37,421
applications we saw a lot of very

02:52:33,851 --> 02:52:41,561
similar me it may benefit her a lot but

02:52:37,421 --> 02:52:45,030
for with very gigantic whammy we may not

02:52:41,561 --> 02:52:50,700
get any benefit from the window lock

02:52:45,030 --> 02:52:50,700
arguing the community too

02:52:52,350 --> 02:53:03,109
I would just like to add that thank

02:52:59,699 --> 02:53:08,330
waiting a while och - we don't may not

02:53:03,109 --> 02:53:11,310
have depend on we don't actually have

02:53:08,330 --> 02:53:14,550
the reason is because we do while on

02:53:11,310 --> 02:53:15,479
optimistic spinning but when he become a

02:53:14,550 --> 02:53:18,330
bit long

02:53:15,479 --> 02:53:18,890
although spinning writer will have to

02:53:18,330 --> 02:53:23,729
sleep

02:53:18,890 --> 02:53:26,550
actually I saw that uh regression report

02:53:23,729 --> 02:53:28,800
yeah we were done right turn with red

02:53:26,550 --> 02:53:32,040
lock from red lock it to me break

02:53:28,800 --> 02:53:33,630
something for if you enable the a type

02:53:32,040 --> 02:53:36,989
of sign spinning or owner

02:53:33,630 --> 02:53:43,560
yeah it's true yeah but it is still

02:53:36,989 --> 02:53:45,420
still can benefit for some lot of

02:53:43,560 --> 02:53:56,489
readers meeting on the understand no

02:53:45,420 --> 02:53:58,229
there's no no writer inside yes yeah

02:53:56,489 --> 02:54:00,899
that that mainly looks bad in micro

02:53:58,229 --> 02:54:02,729
benchmarks but yeah if you have a case

02:54:00,899 --> 02:54:03,869
where you can really benefit from from

02:54:02,729 --> 02:54:05,460
sharing the lock then yeah that

02:54:03,869 --> 02:54:07,140
definitely oh yeah I know it's a yeah

02:54:05,460 --> 02:54:09,810
you see actually a micro pin the map but

02:54:07,140 --> 02:54:12,739
it may have not a significant impact on

02:54:09,810 --> 02:54:12,739
real life workload

02:54:16,319 --> 02:54:22,720
Thanks one of the things with what we're

02:54:20,620 --> 02:54:25,270
looking at if you're here earlier we

02:54:22,720 --> 02:54:26,859
were talking about adding an RC you safe

02:54:25,270 --> 02:54:29,410
bee tree we're actually looking

02:54:26,859 --> 02:54:32,199
replacing this RB tree with that asks

02:54:29,410 --> 02:54:34,810
you say Petrie once once it exists so we

02:54:32,199 --> 02:54:37,330
may be able to get the contention on

02:54:34,810 --> 02:54:41,560
that our W SEM down a bit by just not

02:54:37,330 --> 02:54:44,199
even taking the read we'll see if this

02:54:41,560 --> 02:54:45,460
works out I don't know if people been

02:54:44,199 --> 02:54:47,050
trying a lot of different things over

02:54:45,460 --> 02:54:48,670
the years and maybe this one will work

02:54:47,050 --> 02:54:58,140
and maybe not who knows but we're gonna

02:54:48,670 --> 02:55:00,340
try yeah so oh sorry was there more yeah

02:54:58,140 --> 02:55:02,770
so the problem of the range lock is

02:55:00,340 --> 02:55:04,569
generally that's some kind of tree of

02:55:02,770 --> 02:55:06,010
structures you're traversing to see if a

02:55:04,569 --> 02:55:07,660
range is already there or not and you've

02:55:06,010 --> 02:55:10,120
gotta lock the root and that's still

02:55:07,660 --> 02:55:13,090
your bottleneck um an alternate approach

02:55:10,120 --> 02:55:17,080
is to use something like a hashed array

02:55:13,090 --> 02:55:20,710
of locks to cover the VMA range that

02:55:17,080 --> 02:55:22,540
wraps around so you know and you need a

02:55:20,710 --> 02:55:24,640
lot of buckets depending on you have the

02:55:22,540 --> 02:55:26,050
number buck it's roughly scaled the size

02:55:24,640 --> 02:55:27,939
the VMA but if you want to touch a

02:55:26,050 --> 02:55:29,350
certain range you you know you hash the

02:55:27,939 --> 02:55:31,000
appropriate range you take that lock to

02:55:29,350 --> 02:55:34,090
release it and that's very

02:55:31,000 --> 02:55:35,319
parallelizable the one disadvantage of

02:55:34,090 --> 02:55:36,970
that approach is that if you have to do

02:55:35,319 --> 02:55:39,640
something on a fairly large range you

02:55:36,970 --> 02:55:42,340
might have to lock a series of buckets

02:55:39,640 --> 02:55:44,649
so that's the downside but in practice

02:55:42,340 --> 02:55:46,090
I've seen that work well yeah I agree

02:55:44,649 --> 02:55:50,170
actor there's always a trade-off between

02:55:46,090 --> 02:55:53,189
the yeah so a dog so we actually have a

02:55:50,170 --> 02:55:56,170
range already it's based on our be tree

02:55:53,189 --> 02:55:58,000
and performance wise like when you can

02:55:56,170 --> 02:56:00,609
print the worst case scenario with what

02:55:58,000 --> 02:56:03,460
we have with readwrite semaphores they'd

02:56:00,609 --> 02:56:05,979
actually prefer form help center force

02:56:03,460 --> 02:56:09,699
will always be outperform it because it

02:56:05,979 --> 02:56:12,100
has the spinning thing but it's actually

02:56:09,699 --> 02:56:14,949
pretty close to the worst case scenario

02:56:12,100 --> 02:56:16,689
so has a baseline for starting for the

02:56:14,949 --> 02:56:19,989
primitive itself I think we're in a good

02:56:16,689 --> 02:56:21,580
place right now the the main pain point

02:56:19,989 --> 02:56:23,670
right now I see with range walking is

02:56:21,580 --> 02:56:26,250
how do you

02:56:23,670 --> 02:56:29,300
serialized operations concurrent

02:56:26,250 --> 02:56:29,300
operations on the same VMA

02:56:36,831 --> 02:56:41,511
so it using the area see you for

02:56:39,771 --> 02:56:43,671
protecting the ab3 likes much who is

02:56:41,511 --> 02:56:48,791
mentioning thats could be an ID and

02:56:43,671 --> 02:56:51,561
that's that's once we've done that list

02:56:48,791 --> 02:56:54,230
that's pointing to the VNA that's we're

02:56:51,561 --> 02:56:56,900
making some issues there the problem

02:56:54,230 --> 02:56:59,360
with the VNA that there is so many ways

02:56:56,900 --> 02:57:04,881
to get to the DME there is the beach

02:56:59,360 --> 02:57:07,161
weather is the DME and that's making the

02:57:04,881 --> 02:57:10,371
program more complex that's only one

02:57:07,161 --> 02:57:12,201
list and ones looking is one option but

02:57:10,371 --> 02:57:16,631
when we are getting to the damage with

02:57:12,201 --> 02:57:16,631
UVA what about the range looking there

02:57:18,671 --> 02:57:26,230
yeah III completely agree with you and

02:57:21,971 --> 02:57:28,341
one of the things that we don't

02:57:26,230 --> 02:57:31,661
distinguish really in our code base at

02:57:28,341 --> 02:57:34,820
the moment is are we protecting an

02:57:31,661 --> 02:57:37,341
individual VMA by having this lock or

02:57:34,820 --> 02:57:40,881
are we protecting the whole address

02:57:37,341 --> 02:57:43,671
space and I think once we split that

02:57:40,881 --> 02:57:45,681
those locking some assumptions apart I

02:57:43,671 --> 02:57:48,741
think we will have a better time trying

02:57:45,681 --> 02:57:54,891
to scale than we are right now yeah and

02:57:48,741 --> 02:57:58,190
we have to get and that's Maine also

02:57:54,891 --> 02:58:02,690
that going to the point that should we

02:57:58,190 --> 02:58:06,711
have too much VMS all the stuff we are

02:58:02,690 --> 02:58:09,681
doing through the VM injustice smallest

02:58:06,711 --> 02:58:18,261
is possible that's also leading to some

02:58:09,681 --> 02:58:21,650
attention because if but also when we

02:58:18,261 --> 02:58:24,141
will try to run Maps a large range of

02:58:21,650 --> 02:58:26,781
area that's covering multiple DME we

02:58:24,141 --> 02:58:29,721
will have to find a way to be sure that

02:58:26,781 --> 02:58:31,521
we are not engage into some situation

02:58:29,721 --> 02:58:34,070
because some of those rights will try to

02:58:31,521 --> 02:58:35,220
get some logging that's in this area

02:58:34,070 --> 02:58:38,500
also and we get

02:58:35,220 --> 02:58:40,300
trouble there yeah I mean what I'm

02:58:38,500 --> 02:58:42,550
really hoping to get to is having a spin

02:58:40,300 --> 02:58:45,280
lock that's protecting the entire

02:58:42,550 --> 02:58:48,939
processes address space and then having

02:58:45,280 --> 02:58:52,830
the the semaphore for each VM a but I

02:58:48,939 --> 02:58:56,790
don't know yeah the deadlock situation

02:58:52,830 --> 02:58:56,790
addressing at one time

02:59:00,351 --> 02:59:04,910
I I think there's way around it but we

02:59:03,200 --> 02:59:06,200
wish we should we should probably talk

02:59:04,910 --> 02:59:07,490
with the whiteboard in front of us to

02:59:06,200 --> 02:59:10,061
figure out what words that we're talking

02:59:07,490 --> 02:59:10,061
about the same thing

02:59:28,229 --> 02:59:32,490
okay thank you

02:59:32,510 --> 02:59:38,680

YouTube URL: https://www.youtube.com/watch?v=YBgw2PSrzyY


