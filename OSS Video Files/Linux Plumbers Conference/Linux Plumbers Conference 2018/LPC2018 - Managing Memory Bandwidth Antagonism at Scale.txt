Title: LPC2018 - Managing Memory Bandwidth Antagonism at Scale
Publication date: 2018-11-28
Playlist: Linux Plumbers Conference 2018
Description: 
	url:  https://linuxplumbersconf.org/event/2/contributions/64/
speaker:  Rohit Jnagal (Google), David Lo (Google), Dragos Sbirlea (Google)


Providing a consistent and predictable performance experience for applications is an important goal for cloud providers. Creating isolated job domains in a multi-tenant shared environment can be extremely challenging. At Google, performance isolation challenges due to memory bandwidth has been on the rise with newer workloads. This talk covers our attempt to understand and mitigate isolation issues caused by memory bandwidth saturation.

The recent Intel RDT support in Linux helps us both monitor and manage memory bandwidth use on newer platforms. However, it still leaves a large chunk of our fleet at risk of memory bandwidth issues. The talk covers three aspects of our isolation attempts:

At Google and Borg, we run all application in containers. Our first attempt was to estimate memory bandwidth utilization for each container on all supported platform by using existing performance counters. The talk will cover details on our approximation methodology and issues we identified in monitoring as well as some usage trends across different workloads.
The second part of our effort was focussed on building actuators and policies for memory bandwidth control. We will cover multiple iterations of our enforcement efforts at node and cluster level with production use-cases and lessons learnt.
For newer platforms, we attempted to use Intel RDT support via the resctrl interface. We ran into issues on both the monitoring and isolation side. Weâ€™ll discuss the fixes and workarounds we used and changes we proposed for resource-control support in Linux.
We believe the problems and trends we have observed are universally applicable. We hope to inform and initiate discussion around common solutions across the community.
Captions: 
	00:00:05,590 --> 00:00:10,360
today's presentation will be about

00:00:08,020 --> 00:00:14,799
memory bandwidth which is a specific

00:00:10,360 --> 00:00:18,730
type of noisy neighbor problem that

00:00:14,799 --> 00:00:24,760
we've run into at Google the team is

00:00:18,730 --> 00:00:26,350
David heroin me and Rohit and it's a

00:00:24,760 --> 00:00:29,860
project that's a collaboration between

00:00:26,350 --> 00:00:32,439
platforms and Google born which is the

00:00:29,860 --> 00:00:35,739
Google cluster management system at

00:00:32,439 --> 00:00:37,809
Google we like to have our clusters full

00:00:35,739 --> 00:00:40,510
of jobs as high utilization the higher

00:00:37,809 --> 00:00:43,120
the better so we end up running a mix of

00:00:40,510 --> 00:00:45,329
high and low priority jobs with various

00:00:43,120 --> 00:00:50,079
latency degrees of latency sensitive

00:00:45,329 --> 00:00:53,499
down to batch workloads and we handle

00:00:50,079 --> 00:00:56,050
the isolation part through pair metal

00:00:53,499 --> 00:00:59,170
contains OC groups and namespaces and

00:00:56,050 --> 00:01:02,789
for monitoring we use both C groups and

00:00:59,170 --> 00:01:06,370
for the perf infrastructure in Linux

00:01:02,789 --> 00:01:09,640
the same for for enforcement we use C

00:01:06,370 --> 00:01:12,700
groups and Hardware controls we also use

00:01:09,640 --> 00:01:17,409
cluster level strategies to balance the

00:01:12,700 --> 00:01:21,579
resource requirements so this talk will

00:01:17,409 --> 00:01:25,119
focus on memory bandwidth and what we

00:01:21,579 --> 00:01:27,670
see is that at when memory bandwidth

00:01:25,119 --> 00:01:29,950
usage it becomes high enough the

00:01:27,670 --> 00:01:33,969
performance of the whole system the

00:01:29,950 --> 00:01:37,119
speed of memory band memory accesses

00:01:33,969 --> 00:01:40,030
decreases and the whole system is

00:01:37,119 --> 00:01:42,909
affected in the image shown on this

00:01:40,030 --> 00:01:46,719
slide we see the latency of a sensitive

00:01:42,909 --> 00:01:50,520
job that gets basically doubled as soon

00:01:46,719 --> 00:01:53,609
as memory bandwidth intensive job starts

00:01:50,520 --> 00:01:56,189
and this kind of behavior can and

00:01:53,609 --> 00:01:58,840
periodically causes significant

00:01:56,189 --> 00:02:02,159
performance degradation in our clusters

00:01:58,840 --> 00:02:04,359
of course some workloads are more

00:02:02,159 --> 00:02:07,479
seriously affected than others

00:02:04,359 --> 00:02:09,640
and it does not it's not proportional to

00:02:07,479 --> 00:02:11,770
your memory bandwidth use so if your job

00:02:09,640 --> 00:02:14,470
uses little memory bandwidth it can

00:02:11,770 --> 00:02:17,459
still be very much affected when memory

00:02:14,470 --> 00:02:20,519
bandwidth becomes saturated

00:02:17,459 --> 00:02:24,219
not only this but the problem becomes

00:02:20,519 --> 00:02:26,950
increasingly worse over time so this

00:02:24,219 --> 00:02:31,900
chart shows just this year how much

00:02:26,950 --> 00:02:33,639
worse the problem has become the the

00:02:31,900 --> 00:02:37,569
chart counts the number of distinct

00:02:33,639 --> 00:02:40,079
machines that get affected by memory

00:02:37,569 --> 00:02:45,579
bandwidth saturation for at least one

00:02:40,079 --> 00:02:48,069
timestamp during the day we believe it's

00:02:45,579 --> 00:02:50,590
becoming a bigger issue because of two

00:02:48,069 --> 00:02:53,230
separate reasons first machines are

00:02:50,590 --> 00:02:57,159
becoming bigger and bigger which means

00:02:53,230 --> 00:02:59,109
that if we want to keep a certain level

00:02:57,159 --> 00:03:01,269
of utilization we will pack more and

00:02:59,109 --> 00:03:04,599
more jobs on the machines thus

00:03:01,269 --> 00:03:07,659
increasing the probability that one of

00:03:04,599 --> 00:03:12,250
the jobs will abuse some resource on the

00:03:07,659 --> 00:03:14,530
system and that Google a specific type

00:03:12,250 --> 00:03:19,209
of workload which we know is very memory

00:03:14,530 --> 00:03:22,810
bandwidth intensive is m/l with the

00:03:19,209 --> 00:03:25,180
increasing amount of memory of ml that

00:03:22,810 --> 00:03:28,379
happens it becomes more and more likely

00:03:25,180 --> 00:03:31,419
that we'll run into this issue

00:03:28,379 --> 00:03:33,340
all right thanks dragos for motivating

00:03:31,419 --> 00:03:35,769
the need to control the memory bandwidth

00:03:33,340 --> 00:03:38,409
in the system so the next part is talk

00:03:35,769 --> 00:03:40,000
we'll go into how we determine there's a

00:03:38,409 --> 00:03:41,620
memory bandwidth problem on the system

00:03:40,000 --> 00:03:44,799
and the mechanisms that we have

00:03:41,620 --> 00:03:46,900
available to control the issue so let me

00:03:44,799 --> 00:03:49,150
now talk to you about how we detect

00:03:46,900 --> 00:03:52,030
there's even issue in the first place so

00:03:49,150 --> 00:03:54,129
on modern CPUs in the uncor specifically

00:03:52,030 --> 00:03:56,349
an integrated memory controller and eco

00:03:54,129 --> 00:03:58,150
coherent home agent there are

00:03:56,349 --> 00:03:59,560
performance monitoring facilities that

00:03:58,150 --> 00:04:01,750
tell you how much memory bandwidth is

00:03:59,560 --> 00:04:03,310
being consumed and the local versus

00:04:01,750 --> 00:04:05,500
remote breakdown of that memory

00:04:03,310 --> 00:04:07,569
bandwidth once we started collecting

00:04:05,500 --> 00:04:09,069
this information fleet wide and we

00:04:07,569 --> 00:04:11,379
plumbed it for our infrastructure for

00:04:09,069 --> 00:04:13,719
analysis now we could start doing some

00:04:11,379 --> 00:04:15,819
performance characterizations to figure

00:04:13,719 --> 00:04:18,099
out how much memory bandwidth we can

00:04:15,819 --> 00:04:20,409
tolerate on the machine before the

00:04:18,099 --> 00:04:21,340
performance of latency sensitive tasks

00:04:20,409 --> 00:04:23,979
starts dropping

00:04:21,340 --> 00:04:27,010
so we do this characterization for every

00:04:23,979 --> 00:04:29,080
platform and in this magic number we

00:04:27,010 --> 00:04:30,470
call the saturation threshold and so

00:04:29,080 --> 00:04:33,320
this is basically you can think of it

00:04:30,470 --> 00:04:35,240
as the knee in the curve we're below

00:04:33,320 --> 00:04:37,220
that the performance is fine but as soon

00:04:35,240 --> 00:04:39,500
as you cross that threshold the

00:04:37,220 --> 00:04:42,560
performance degradation starts going off

00:04:39,500 --> 00:04:44,360
a cliff and you have a bad time and the

00:04:42,560 --> 00:04:47,030
next slide we characterized the

00:04:44,360 --> 00:04:49,670
saturation variances by platforms and

00:04:47,030 --> 00:04:51,380
clusters so due to the heterogeneous

00:04:49,670 --> 00:04:54,230
nature of Google's fleet we have

00:04:51,380 --> 00:04:57,260
multiple kinds of a hardware platforms

00:04:54,230 --> 00:04:58,490
within the fleet and within cluster this

00:04:57,260 --> 00:05:01,070
means that we actually get different

00:04:58,490 --> 00:05:03,890
kinds of saturation behavior it's not

00:05:01,070 --> 00:05:06,170
just oh you have all machines well

00:05:03,890 --> 00:05:08,240
behaved to say so depending on the

00:05:06,170 --> 00:05:10,040
memory bandwidth to the number of CPU

00:05:08,240 --> 00:05:11,870
cores ratio some platforms have less

00:05:10,040 --> 00:05:14,630
memory bandwidth in more cores other

00:05:11,870 --> 00:05:17,300
platforms have the other kind inverse

00:05:14,630 --> 00:05:19,880
ratio then you get different levels of

00:05:17,300 --> 00:05:21,230
saturation per platform and as I

00:05:19,880 --> 00:05:23,570
mentioned due to the heterogeneous

00:05:21,230 --> 00:05:25,940
nature of our clusters we actually see

00:05:23,570 --> 00:05:27,710
saturation in different clusters at

00:05:25,940 --> 00:05:30,020
different times of the day or at

00:05:27,710 --> 00:05:32,120
different days of the week or different

00:05:30,020 --> 00:05:34,160
months of the year so it's very wide

00:05:32,120 --> 00:05:38,570
variance it doesn't all happen at the

00:05:34,160 --> 00:05:40,610
same time hey so we can now get to the

00:05:38,570 --> 00:05:43,430
data per socket to determine if there's

00:05:40,610 --> 00:05:45,919
a problem the next step is to then go

00:05:43,430 --> 00:05:48,200
attribute the memory bandwidth usage to

00:05:45,919 --> 00:05:50,090
particular workloads it's not very

00:05:48,200 --> 00:05:51,650
helpful to just know that there's a

00:05:50,090 --> 00:05:53,240
problem and then try to guess who's

00:05:51,650 --> 00:05:55,280
causing the problem by looking at CPU

00:05:53,240 --> 00:05:57,350
usage because what we've seen is that

00:05:55,280 --> 00:05:59,419
there are tasks that can use lost CPU

00:05:57,350 --> 00:06:00,710
but because you and they're very cache

00:05:59,419 --> 00:06:03,140
friendly or they're not doing very much

00:06:00,710 --> 00:06:05,030
data accesses they don't consume a lot

00:06:03,140 --> 00:06:07,280
of memory bandwidth well there's other

00:06:05,030 --> 00:06:08,960
kinds of tasks that are just streaming

00:06:07,280 --> 00:06:11,450
through lots of data so they don't

00:06:08,960 --> 00:06:13,100
consume as much CPU but they can

00:06:11,450 --> 00:06:15,919
basically blow out the entire memory

00:06:13,100 --> 00:06:18,830
bandwidth of a single socket using say

00:06:15,919 --> 00:06:20,750
five CPUs right so we need to be able to

00:06:18,830 --> 00:06:23,750
accurately figure out how much memory

00:06:20,750 --> 00:06:26,690
bandwidth each task is using and we need

00:06:23,750 --> 00:06:28,729
this in order to identify both tasks our

00:06:26,690 --> 00:06:30,380
what we call abusers these are tasks

00:06:28,729 --> 00:06:33,200
that are consuming more memory bandwidth

00:06:30,380 --> 00:06:35,090
than we'd like them to last tasks that

00:06:33,200 --> 00:06:37,850
we call victims these are two tasks that

00:06:35,090 --> 00:06:40,010
one co-located on a machine with high

00:06:37,850 --> 00:06:42,830
memory bandwidth usage you get the kind

00:06:40,010 --> 00:06:43,950
of latency cliff that Drago showed

00:06:42,830 --> 00:06:46,890
earlier

00:06:43,950 --> 00:06:49,440
so a new platform specifically Broadwell

00:06:46,890 --> 00:06:51,300
and skylight from intel there's actually

00:06:49,440 --> 00:06:53,790
hardware capabilities in order to get

00:06:51,300 --> 00:06:56,250
this information per task however when

00:06:53,790 --> 00:06:57,750
we start out doing this work it was

00:06:56,250 --> 00:07:00,270
actually challenging to use these new

00:06:57,750 --> 00:07:02,580
hardware features the first reason was

00:07:00,270 --> 00:07:04,980
because the Intel our DTC group was on

00:07:02,580 --> 00:07:07,200
its way out is being deprecated and a

00:07:04,980 --> 00:07:09,390
new system in the kernel in order to

00:07:07,200 --> 00:07:11,250
interface with the hardware rest which

00:07:09,390 --> 00:07:13,770
we'll talk about a bit more later that

00:07:11,250 --> 00:07:15,930
was not ready yet for us to use and the

00:07:13,770 --> 00:07:17,430
second challenge is that again google

00:07:15,930 --> 00:07:19,410
has a very heterogeneous fleet this

00:07:17,430 --> 00:07:21,060
means a lot of legacy old hardware

00:07:19,410 --> 00:07:23,490
platforms that don't have this Hardware

00:07:21,060 --> 00:07:25,350
support and these platforms were also

00:07:23,490 --> 00:07:27,660
experiencing memory bandwidth saturation

00:07:25,350 --> 00:07:29,970
problems so we also needed some way of

00:07:27,660 --> 00:07:33,480
getting information from those older

00:07:29,970 --> 00:07:35,400
platforms so for our purposes we

00:07:33,480 --> 00:07:37,590
actually could get away with a rough

00:07:35,400 --> 00:07:40,230
estimation of memory if we didn't need

00:07:37,590 --> 00:07:42,360
to count exactly every single byte going

00:07:40,230 --> 00:07:45,060
across the memory bus typically when we

00:07:42,360 --> 00:07:46,830
saw saturation there's a couple of large

00:07:45,060 --> 00:07:48,510
abusers responsible for that memory

00:07:46,830 --> 00:07:54,810
bandwidth usage so as long as you're in

00:07:48,510 --> 00:07:58,410
D ballpark range you could I I blamed

00:07:54,810 --> 00:08:00,540
this on a memory bandwidth contention so

00:07:58,410 --> 00:08:02,430
as long as you can within the ballpark

00:08:00,540 --> 00:08:04,110
range identifies who's using most of the

00:08:02,430 --> 00:08:07,650
bandwidth that's good enough for control

00:08:04,110 --> 00:08:10,110
purposes okay so how do we do this per

00:08:07,650 --> 00:08:11,670
test memory bound estimation summary of

00:08:10,110 --> 00:08:13,470
the requirements is that you know we

00:08:11,670 --> 00:08:15,090
need this local remote breakdown so we

00:08:13,470 --> 00:08:16,740
know exactly how much traffic is being

00:08:15,090 --> 00:08:20,640
sent to which socket that's seeing the

00:08:16,740 --> 00:08:22,620
issue and because of the way we run task

00:08:20,640 --> 00:08:24,930
a Google their own containers we need

00:08:22,620 --> 00:08:27,210
this method to be compatible of the C

00:08:24,930 --> 00:08:28,590
group model so let's take a quick tour

00:08:27,210 --> 00:08:30,780
of what's available in the hardware and

00:08:28,590 --> 00:08:33,090
see which one we can make work so the

00:08:30,780 --> 00:08:34,530
most obvious Hardware feature is uncork

00:08:33,090 --> 00:08:36,600
counters these are the integrated memory

00:08:34,530 --> 00:08:37,920
controller two coherent home agent if

00:08:36,600 --> 00:08:39,390
you're not familiar of what does are in

00:08:37,920 --> 00:08:40,890
Hardware I'm showing a little cartoon or

00:08:39,390 --> 00:08:43,140
write that you know shows basically

00:08:40,890 --> 00:08:44,430
where they sit logically so you may

00:08:43,140 --> 00:08:45,990
think these things are great for

00:08:44,430 --> 00:08:47,760
measuring memory bandwidth after all

00:08:45,990 --> 00:08:50,820
these are dings are closest to your

00:08:47,760 --> 00:08:53,730
diagram in the system however it's very

00:08:50,820 --> 00:08:55,500
difficult to attribute counts on these

00:08:53,730 --> 00:08:57,540
counters back to the individual hyper

00:08:55,500 --> 00:08:59,449
thread which we need to do in or

00:08:57,540 --> 00:09:02,160
to do to see group attribution properly

00:08:59,449 --> 00:09:04,529
so uncork counters are out is there

00:09:02,160 --> 00:09:06,899
anything else we can use turns out that

00:09:04,529 --> 00:09:08,670
the CPU pmu counters or performance

00:09:06,899 --> 00:09:10,500
monitoring unit counters they're

00:09:08,670 --> 00:09:12,990
actually very well suited for this task

00:09:10,500 --> 00:09:14,459
the counters are hyper Fred local which

00:09:12,990 --> 00:09:16,319
means we can get two counts per hyper

00:09:14,459 --> 00:09:19,139
Fred that's great we can do the sieve of

00:09:16,319 --> 00:09:21,449
attribution properly and secondly using

00:09:19,139 --> 00:09:23,100
the perf subsystem the Linux kernel it

00:09:21,449 --> 00:09:25,050
actually already supports C root

00:09:23,100 --> 00:09:26,430
profiling mode we didn't have to jump

00:09:25,050 --> 00:09:30,750
through any extra hoops to make this

00:09:26,430 --> 00:09:32,490
work with the secret model okay so we're

00:09:30,750 --> 00:09:34,500
gonna use P mu counters now the question

00:09:32,490 --> 00:09:36,930
is which counter to use you look at the

00:09:34,500 --> 00:09:38,730
Intel manual it's a very long list of

00:09:36,930 --> 00:09:40,949
counters it's very difficult to tell

00:09:38,730 --> 00:09:42,360
which one is actually useful for this so

00:09:40,949 --> 00:09:44,910
I'll save you all that trouble and tell

00:09:42,360 --> 00:09:47,639
you that off core response is the

00:09:44,910 --> 00:09:49,170
counter to use so here I'll just give a

00:09:47,639 --> 00:09:51,899
very brief overview what it is it's a

00:09:49,170 --> 00:09:54,360
programmable it's a P mu with a

00:09:51,899 --> 00:09:56,130
programmable filter so you can say tell

00:09:54,360 --> 00:09:58,350
me all the memory accesses that were

00:09:56,130 --> 00:10:00,990
serviced by the local memory or by the

00:09:58,350 --> 00:10:03,000
remote memory and one very nice thing

00:10:00,990 --> 00:10:05,910
about this counter is that it can also

00:10:03,000 --> 00:10:08,160
capture on the man load and prefetch

00:10:05,910 --> 00:10:09,930
traffic as well so it can get a pretty

00:10:08,160 --> 00:10:12,959
comprehensive view of memory bandwidth

00:10:09,930 --> 00:10:15,149
used by workload so there's also a

00:10:12,959 --> 00:10:17,010
online documentation by in town with all

00:10:15,149 --> 00:10:19,350
the little bits mean you can look at

00:10:17,010 --> 00:10:22,079
that offline but the general way to

00:10:19,350 --> 00:10:24,329
interpret this counter is that you can

00:10:22,079 --> 00:10:25,709
see how many counts go by and you just

00:10:24,329 --> 00:10:28,800
multiply that by the cache line size

00:10:25,709 --> 00:10:30,839
which is 64 bytes unintel cpus and then

00:10:28,800 --> 00:10:37,290
that tells you how much how many bytes

00:10:30,839 --> 00:10:40,170
have a this c group has transferred with

00:10:37,290 --> 00:10:42,569
this approach that David presented it

00:10:40,170 --> 00:10:44,579
helped us gather some insights about the

00:10:42,569 --> 00:10:46,529
problem and how it relates to the

00:10:44,579 --> 00:10:48,389
workload on the machine first we

00:10:46,529 --> 00:10:51,930
gathered up user insights which show

00:10:48,389 --> 00:10:55,350
that a large percentage of time one or

00:10:51,930 --> 00:10:57,949
two high memory bandwidth consumers are

00:10:55,350 --> 00:11:00,689
soaking up all of the memory bandwidth

00:10:57,949 --> 00:11:03,569
an interesting thing to note is that the

00:11:00,689 --> 00:11:04,160
CPU share of these consumers does not

00:11:03,569 --> 00:11:06,980
match

00:11:04,160 --> 00:11:10,640
the proportion that we see that I am

00:11:06,980 --> 00:11:15,260
using memory bandwidth another insight

00:11:10,640 --> 00:11:16,640
is for victims we did not know how often

00:11:15,260 --> 00:11:18,920
do we care about memory when the

00:11:16,640 --> 00:11:20,600
saturation and it turns out that a large

00:11:18,920 --> 00:11:23,210
percentage of jobs are actually

00:11:20,600 --> 00:11:26,390
sensitive and Louis's performance due to

00:11:23,210 --> 00:11:30,800
it another thing that came out of this

00:11:26,390 --> 00:11:32,870
is that we found out jobs who are

00:11:30,800 --> 00:11:34,460
sensitive don't actually use a lot of

00:11:32,870 --> 00:11:38,330
memory bandwidth they could be using

00:11:34,460 --> 00:11:40,970
very little and also on based on this

00:11:38,330 --> 00:11:43,130
approach we were able to gather guidance

00:11:40,970 --> 00:11:45,800
on various enforcement options that we

00:11:43,130 --> 00:11:48,650
were considering we wanted to know how

00:11:45,800 --> 00:11:51,110
much saturation would we fix by doing X

00:11:48,650 --> 00:11:54,350
how much with ethics by doing Y which

00:11:51,110 --> 00:11:59,650
jobs would be affected in a way that we

00:11:54,350 --> 00:12:02,930
would not we would prefer not to and

00:11:59,650 --> 00:12:04,850
based on these insights we decided on

00:12:02,930 --> 00:12:08,060
the following and format reinforcement

00:12:04,850 --> 00:12:11,420
approach which is guided a lot by the

00:12:08,060 --> 00:12:14,990
needs of the jobs and ESL O's that we

00:12:11,420 --> 00:12:17,740
promised them so on the x-axis we look

00:12:14,990 --> 00:12:20,500
at the memory bandwidth usage of the job

00:12:17,740 --> 00:12:24,110
moderate meaning the job would not

00:12:20,500 --> 00:12:27,650
saturate another machine if we were to

00:12:24,110 --> 00:12:29,780
move it to another book machine heavy

00:12:27,650 --> 00:12:32,030
would mean that if we're trying to move

00:12:29,780 --> 00:12:35,660
it it's probably going to cause the same

00:12:32,030 --> 00:12:37,430
issue all over again on the y-axis we

00:12:35,660 --> 00:12:40,220
have the various priorities that the

00:12:37,430 --> 00:12:42,770
jobs could have so if we look at jobs of

00:12:40,220 --> 00:12:45,170
low priority we decided to simply

00:12:42,770 --> 00:12:47,630
throttle the jobs decreasing their

00:12:45,170 --> 00:12:50,570
throughput but also their memory

00:12:47,630 --> 00:12:53,720
bandwidth thus saving the machine for

00:12:50,570 --> 00:12:56,600
jobs of high priority we want them to

00:12:53,720 --> 00:12:58,790
never be impacted so we decided to

00:12:56,600 --> 00:13:00,800
isolate them removing any other jobs

00:12:58,790 --> 00:13:03,230
from the machine that may use memory

00:13:00,800 --> 00:13:05,770
bandwidth the interesting part is what

00:13:03,230 --> 00:13:09,410
we do for jobs of medium priority

00:13:05,770 --> 00:13:12,530
because if they use a lot of memory

00:13:09,410 --> 00:13:14,670
bandwidth moving them around with jaw

00:13:12,530 --> 00:13:17,220
just move the problem around

00:13:14,670 --> 00:13:18,960
the new machine so if they use if they

00:13:17,220 --> 00:13:22,260
are helping memory bandwidth user we

00:13:18,960 --> 00:13:24,300
will disable these jobs if instead they

00:13:22,260 --> 00:13:26,070
only use part of their memory bandwidth

00:13:24,300 --> 00:13:28,050
in the system we use this approach

00:13:26,070 --> 00:13:31,520
called reactive rescheduling that I'm

00:13:28,050 --> 00:13:34,890
going to discuss on a subsequent slide

00:13:31,520 --> 00:13:36,860
so thus there are two types high-level

00:13:34,890 --> 00:13:39,360
types of approach node based approach

00:13:36,860 --> 00:13:42,810
approaches and cluster wide approaches

00:13:39,360 --> 00:13:44,490
for node based we discussed and will

00:13:42,810 --> 00:13:48,270
discuss more about memory bandwidth

00:13:44,490 --> 00:13:51,510
allocation support in hardware and for

00:13:48,270 --> 00:13:53,370
older platform that one's what we can do

00:13:51,510 --> 00:13:55,890
is we can indirectly control memory

00:13:53,370 --> 00:13:58,200
bandwidth by CPU throttling we would

00:13:55,890 --> 00:14:01,650
limit CPU access thus decreasing the

00:13:58,200 --> 00:14:04,730
amount of memory bandwidth used cluster

00:14:01,650 --> 00:14:06,870
approaches consist of this reactive

00:14:04,730 --> 00:14:09,360
evictions and with scheduling approach

00:14:06,870 --> 00:14:11,250
that I mentioned basically the scheduler

00:14:09,360 --> 00:14:12,810
would move jobs around to lie to

00:14:11,250 --> 00:14:15,870
machines that have lower memory

00:14:12,810 --> 00:14:19,470
bandwidth utilization and the heavy

00:14:15,870 --> 00:14:24,210
hammer disabling the the jobs who really

00:14:19,470 --> 00:14:27,810
abuse the cell so if you look in detail

00:14:24,210 --> 00:14:31,160
at the CPU throttling approach because

00:14:27,810 --> 00:14:33,390
it reduces basically arbitrarily the CPU

00:14:31,160 --> 00:14:35,310
accessible to the task it can be very

00:14:33,390 --> 00:14:37,620
effective in reducing saturation and

00:14:35,310 --> 00:14:40,440
it's got the considerable advantage that

00:14:37,620 --> 00:14:42,660
it works on all of our platforms the

00:14:40,440 --> 00:14:44,700
disadvantage is it's very coercing

00:14:42,660 --> 00:14:47,880
granularity and it's indiscriminate in

00:14:44,700 --> 00:14:51,600
the sense that it slows down any thread

00:14:47,880 --> 00:14:53,820
whether it accesses memory or not it

00:14:51,600 --> 00:14:56,760
also has a big disadvantage of poor

00:14:53,820 --> 00:15:00,330
interaction with load balancing because

00:14:56,760 --> 00:15:02,370
if a load balancer looks at a job and

00:15:00,330 --> 00:15:06,540
sees that it's using very little CPU

00:15:02,370 --> 00:15:08,580
where as it requested a lot it's going

00:15:06,540 --> 00:15:11,130
to assume that oh there's very little

00:15:08,580 --> 00:15:13,590
work load let's send some more requests

00:15:11,130 --> 00:15:15,840
to it whereas in practice that job is

00:15:13,590 --> 00:15:20,040
limited and the queue of requests is

00:15:15,840 --> 00:15:24,570
climbing so how are we doing the

00:15:20,040 --> 00:15:27,190
throttling with based on the sockets and

00:15:24,570 --> 00:15:28,870
see group counters we run periodically

00:15:27,190 --> 00:15:33,550
memory saturate memory bandwidth

00:15:28,870 --> 00:15:36,279
saturation detector which will decide if

00:15:33,550 --> 00:15:39,190
the soccer memory bandwidth is over the

00:15:36,279 --> 00:15:42,480
saturation threshold that we decide if

00:15:39,190 --> 00:15:46,060
it is then we run the attribution part

00:15:42,480 --> 00:15:49,180
which is based on both socket and C

00:15:46,060 --> 00:15:52,180
group perf counters the output is a set

00:15:49,180 --> 00:15:55,329
of tasks out of them a policy filter

00:15:52,180 --> 00:15:58,029
based on a fellow will select a subset

00:15:55,329 --> 00:16:01,870
of them that are eligible for throttling

00:15:58,029 --> 00:16:06,069
and then an enforcer decreases the CPU

00:16:01,870 --> 00:16:08,560
runnable mask for this tasks on the

00:16:06,069 --> 00:16:09,579
other hand if we detect that saturation

00:16:08,560 --> 00:16:11,500
has disappeared

00:16:09,579 --> 00:16:15,069
maybe due to the throttling that we

00:16:11,500 --> 00:16:17,230
apply we may decide if the socket memory

00:16:15,069 --> 00:16:19,600
bandwidth is lower than another

00:16:17,230 --> 00:16:21,790
threshold a lower one then we can

00:16:19,600 --> 00:16:24,790
unthreat all the tasks and the enforcer

00:16:21,790 --> 00:16:26,920
will increase the CPU runnable masks the

00:16:24,790 --> 00:16:29,350
key takeaway here is that we can use

00:16:26,920 --> 00:16:33,339
this hysteresis mechanism basically a

00:16:29,350 --> 00:16:39,339
use of two different threshold to keep

00:16:33,339 --> 00:16:40,959
the approach from being too bouncy so

00:16:39,339 --> 00:16:43,899
the hardware approach is called memory

00:16:40,959 --> 00:16:46,930
bandwidth allocation and we're going to

00:16:43,899 --> 00:16:49,180
discuss a bit more about it later but

00:16:46,930 --> 00:16:51,279
the idea is that there's a request rate

00:16:49,180 --> 00:16:52,870
controller somewhere between the l2

00:16:51,279 --> 00:16:55,870
cache and the high speed interconnect

00:16:52,870 --> 00:17:01,630
that allows us to slow down the memory

00:16:55,870 --> 00:17:03,730
accesses of on a per app basis it's for

00:17:01,630 --> 00:17:07,209
a newer platform or only so it would not

00:17:03,730 --> 00:17:09,429
help order all our clusters and it also

00:17:07,209 --> 00:17:13,150
has its own its own issues that will

00:17:09,429 --> 00:17:15,689
detail later going to the cluster

00:17:13,150 --> 00:17:18,490
approaches we have reactively scheduling

00:17:15,689 --> 00:17:22,179
which is an approach that we use when

00:17:18,490 --> 00:17:24,339
the set the cluster has saturated

00:17:22,179 --> 00:17:27,880
sockets but the fraction of them is not

00:17:24,339 --> 00:17:31,270
too high and the jobs that are causing

00:17:27,880 --> 00:17:33,220
saturation don't use heavy memory

00:17:31,270 --> 00:17:34,610
bandwidth so they would not saturate the

00:17:33,220 --> 00:17:38,210
socket if you were to move

00:17:34,610 --> 00:17:40,220
around and how this works is when a host

00:17:38,210 --> 00:17:43,850
discovers that it's saturated by

00:17:40,220 --> 00:17:46,040
profiling the socket it's going to call

00:17:43,850 --> 00:17:47,929
for help to an observer and give it a

00:17:46,040 --> 00:17:50,990
list of jobs that use memory bandwidth

00:17:47,929 --> 00:17:53,480
the observer will decide to evict

00:17:50,990 --> 00:17:55,880
somebody and ask the scheduler to do so

00:17:53,480 --> 00:17:58,610
which will then Evek the job and

00:17:55,880 --> 00:18:03,710
reschedule it on a host that's lightly

00:17:58,610 --> 00:18:07,150
loaded unfortunately there is scheduling

00:18:03,710 --> 00:18:09,169
approach does not work if the cell

00:18:07,150 --> 00:18:11,750
utilization looks like this basically

00:18:09,169 --> 00:18:13,970
most of the machines being at least

00:18:11,750 --> 00:18:17,450
medium medium ly used on memory

00:18:13,970 --> 00:18:20,450
bandwidth and so for situations like the

00:18:17,450 --> 00:18:23,090
one on the bottom we have to use bigger

00:18:20,450 --> 00:18:26,690
hammers in this case disabling abusers

00:18:23,090 --> 00:18:29,870
so we look at collections that saturate

00:18:26,690 --> 00:18:32,890
a lot of machines and pick which one to

00:18:29,870 --> 00:18:35,210
do to disable based on its priority

00:18:32,890 --> 00:18:37,130
there are a few alternatives that we

00:18:35,210 --> 00:18:41,150
considered here that we can discuss

00:18:37,130 --> 00:18:43,220
later users once disabled they are can

00:18:41,150 --> 00:18:45,830
either reconfigure their service to

00:18:43,220 --> 00:18:50,120
maybe let us throttle them a different

00:18:45,830 --> 00:18:53,780
priority or switch to another cluster

00:18:50,120 --> 00:18:57,669
which is like more lightly loaded so the

00:18:53,780 --> 00:19:00,290
results so if we evaluate CPU throttling

00:18:57,669 --> 00:19:02,600
we see that at least in combination with

00:19:00,290 --> 00:19:07,490
cries for help it's helpful in reducing

00:19:02,600 --> 00:19:10,490
about 71% it's successful at removing

00:19:07,490 --> 00:19:14,330
saturation in about 71 percent of the

00:19:10,490 --> 00:19:16,130
time another 20 something percent we

00:19:14,330 --> 00:19:19,760
just have no candidates no low priority

00:19:16,130 --> 00:19:22,970
jobs that we could throttle for the

00:19:19,760 --> 00:19:27,890
rebalancing approach we have a very

00:19:22,970 --> 00:19:31,549
interesting situation where we avoid the

00:19:27,890 --> 00:19:34,970
saturation about 30% of the time there's

00:19:31,549 --> 00:19:37,520
49 40 percent of the time where we can't

00:19:34,970 --> 00:19:40,260
use this approach due to job SLO and

00:19:37,520 --> 00:19:43,560
another 30 percent of the time where

00:19:40,260 --> 00:19:48,660
we have what we call tolerated

00:19:43,560 --> 00:19:50,730
saturation or rescheduling we don't want

00:19:48,660 --> 00:19:55,440
to trigger it immediately as soon as the

00:19:50,730 --> 00:19:57,150
problem shows up because the spikes in

00:19:55,440 --> 00:19:59,370
memory bandwidth saturation like in

00:19:57,150 --> 00:20:01,170
instantaneous spikes that disappear

00:19:59,370 --> 00:20:03,240
immediately after would cause too many

00:20:01,170 --> 00:20:06,240
evictions it's too much toil in the

00:20:03,240 --> 00:20:10,950
schedule and too much work lost on the

00:20:06,240 --> 00:20:13,200
jobs moving around all right

00:20:10,950 --> 00:20:15,840
so up to this point you may be wondering

00:20:13,200 --> 00:20:17,730
okay this is you know all very nice work

00:20:15,840 --> 00:20:19,950
but where it was that to do the Linux

00:20:17,730 --> 00:20:21,270
kernel and if you're wondering that I

00:20:19,950 --> 00:20:24,030
thank you for your patience we're about

00:20:21,270 --> 00:20:25,890
to get into details of how hard work us

00:20:24,030 --> 00:20:28,530
mechanisms interact with Linux kernel

00:20:25,890 --> 00:20:32,490
our experiences with a subsystem as well

00:20:28,530 --> 00:20:34,680
as a community call for action so some

00:20:32,490 --> 00:20:36,360
audience participation question raise

00:20:34,680 --> 00:20:41,940
your hands have you heard of res control

00:20:36,360 --> 00:20:43,770
classes service ID RMIT oh wow that's

00:20:41,940 --> 00:20:47,010
actually more people than expected

00:20:43,770 --> 00:20:48,510
okay well so the vast majority people in

00:20:47,010 --> 00:20:50,490
this room haven't heard this I'll give a

00:20:48,510 --> 00:20:52,680
quick tour throughout these concepts so

00:20:50,490 --> 00:20:54,750
then you can understand later when I'm

00:20:52,680 --> 00:20:57,300
talking about experiences using the

00:20:54,750 --> 00:20:59,460
subsystem so wrest control stands for

00:20:57,300 --> 00:21:01,230
resource control it's a new unified

00:20:59,460 --> 00:21:04,110
interface in a kernel who manage

00:21:01,230 --> 00:21:06,780
Hardware quality of service features and

00:21:04,110 --> 00:21:08,940
so to us this is a big improvement over

00:21:06,780 --> 00:21:11,340
the previous non-standard RT TC group

00:21:08,940 --> 00:21:13,200
interface that one we tried to make some

00:21:11,340 --> 00:21:15,810
more changes to it upstream maintainer

00:21:13,200 --> 00:21:17,520
z' didn't like it so then they sent they

00:21:15,810 --> 00:21:18,840
sent actually the Intel folks back to

00:21:17,520 --> 00:21:22,200
the drawing board and then they came up

00:21:18,840 --> 00:21:24,030
with this res control file system so

00:21:22,200 --> 00:21:26,340
it's actually seeing a lot of adoption

00:21:24,030 --> 00:21:29,040
within different vendors and different

00:21:26,340 --> 00:21:31,260
architectures both AMD arm and Intel

00:21:29,040 --> 00:21:33,960
have patches and proposals upstream to

00:21:31,260 --> 00:21:36,300
make use of this system and the system

00:21:33,960 --> 00:21:38,730
is also very rich in features it

00:21:36,300 --> 00:21:40,770
supports a hardware memory bandwidth

00:21:38,730 --> 00:21:44,390
monitoring and isolation as well as last

00:21:40,770 --> 00:21:47,790
level cache mantri and isolation as well

00:21:44,390 --> 00:21:49,950
so res control is the software layer so

00:21:47,790 --> 00:21:51,300
now give a quick introduction to exactly

00:21:49,950 --> 00:21:52,660
you know what in the hardware is

00:21:51,300 --> 00:21:55,060
managing

00:21:52,660 --> 00:21:56,950
the Google use case for resource

00:21:55,060 --> 00:21:58,990
isolation is to have lots of different

00:21:56,950 --> 00:22:00,880
workloads running on each machine each

00:21:58,990 --> 00:22:03,700
of those were closed corresponds to one

00:22:00,880 --> 00:22:05,980
container and we may have a small number

00:22:03,700 --> 00:22:07,780
of quality-of-service configs for

00:22:05,980 --> 00:22:09,580
example maybe the high priority lanes a

00:22:07,780 --> 00:22:11,410
sensitive things to get more cash get

00:22:09,580 --> 00:22:13,150
more memory bandwidth but the low

00:22:11,410 --> 00:22:15,040
priority batch workloads which want to

00:22:13,150 --> 00:22:17,260
constrain how much cash they can use who

00:22:15,040 --> 00:22:20,350
don't want them to use too much memory

00:22:17,260 --> 00:22:22,480
bandwidth in the system and so this

00:22:20,350 --> 00:22:24,490
naturally leads to a model where you

00:22:22,480 --> 00:22:27,040
have a small number of a class of

00:22:24,490 --> 00:22:29,410
service ids these are things that maps

00:22:27,040 --> 00:22:31,690
to unique QoS configs in the hardware

00:22:29,410 --> 00:22:34,210
but at the same time we want to have

00:22:31,690 --> 00:22:36,610
very fine-grained monitoring of each

00:22:34,210 --> 00:22:38,380
workload running the system so this

00:22:36,610 --> 00:22:40,750
means that we want to have lots of work

00:22:38,380 --> 00:22:43,720
called resource monitoring IDs or arm

00:22:40,750 --> 00:22:46,150
IDs because that way we can hand out 1rm

00:22:43,720 --> 00:22:48,580
ID to every workload so that way we can

00:22:46,150 --> 00:22:50,380
exactly find out who's using how much

00:22:48,580 --> 00:22:55,600
memory bandwidth so then we can go for

00:22:50,380 --> 00:22:57,850
on all them later ok so this is a very

00:22:55,600 --> 00:23:00,100
quick overview of the rest control

00:22:57,850 --> 00:23:02,470
filesystem there's a lot more files and

00:23:00,100 --> 00:23:04,300
directories in the actual file system so

00:23:02,470 --> 00:23:07,440
if you're I'm only going to present the

00:23:04,300 --> 00:23:10,780
things that are relevant to this talk so

00:23:07,440 --> 00:23:13,780
rest control file system is organized as

00:23:10,780 --> 00:23:15,880
multiple rest control groups or resource

00:23:13,780 --> 00:23:18,340
control group in each group today it

00:23:15,880 --> 00:23:20,950
corresponds to one Hardware class of

00:23:18,340 --> 00:23:23,410
service ID so that is important detail

00:23:20,950 --> 00:23:26,140
that will come up again later then

00:23:23,410 --> 00:23:27,760
within each resident role group you have

00:23:26,140 --> 00:23:29,740
two files you have two schematic file

00:23:27,760 --> 00:23:31,660
which is a human readable file that

00:23:29,740 --> 00:23:33,310
tells you oh here's how much resources

00:23:31,660 --> 00:23:35,560
this resident true group should be

00:23:33,310 --> 00:23:37,840
getting in hardware as well as the tasks

00:23:35,560 --> 00:23:40,870
file which specifies which Fred IDs

00:23:37,840 --> 00:23:42,730
belong to this rest control group then

00:23:40,870 --> 00:23:46,000
within a res control group you also have

00:23:42,730 --> 00:23:47,890
the monitoring aspect so this is

00:23:46,000 --> 00:23:51,040
captured under naman groups

00:23:47,890 --> 00:23:55,540
so each Mon group represents one unique

00:23:51,040 --> 00:23:57,460
a Hardware RM ID and again it also has a

00:23:55,540 --> 00:23:58,870
task file which specifies which Fred's

00:23:57,460 --> 00:24:00,910
belong to that particular resource

00:23:58,870 --> 00:24:04,300
monitoring group

00:24:00,910 --> 00:24:06,010
and then there's also data associated

00:24:04,300 --> 00:24:07,600
with that resource monitoring group as

00:24:06,010 --> 00:24:08,950
well you can go in and you can get the

00:24:07,600 --> 00:24:12,280
cache occupancy or how much memory

00:24:08,950 --> 00:24:15,010
bandwidth has been used in addition

00:24:12,280 --> 00:24:16,900
there's also a top-level Mon data

00:24:15,010 --> 00:24:18,720
associated with each troll group

00:24:16,900 --> 00:24:21,730
this tells you the total amount of

00:24:18,720 --> 00:24:25,420
resources that everything in that rez

00:24:21,730 --> 00:24:26,770
control group is using okay so in this

00:24:25,420 --> 00:24:29,860
slide I'm just going to give a brief

00:24:26,770 --> 00:24:32,140
example of what you can see in the rest

00:24:29,860 --> 00:24:33,040
control file system so if you can go

00:24:32,140 --> 00:24:36,190
look at the schemata

00:24:33,040 --> 00:24:38,140
you can see here how much l3 cache a

00:24:36,190 --> 00:24:40,120
reskin true group should get how much

00:24:38,140 --> 00:24:43,360
memory bandwidth it should get and you

00:24:40,120 --> 00:24:46,630
can set them by just echoing lines into

00:24:43,360 --> 00:24:50,080
the schemata then to get a reading you

00:24:46,630 --> 00:24:51,730
can then just read out a file you can

00:24:50,080 --> 00:24:54,400
then wait a little bit of time read it

00:24:51,730 --> 00:24:56,890
out again then you compute the Delta to

00:24:54,400 --> 00:24:58,300
get the rate and so in this case if you

00:24:56,890 --> 00:25:00,190
wait one second and you can read it

00:24:58,300 --> 00:25:01,720
twice do the subtraction we're saying

00:25:00,190 --> 00:25:05,130
that this particular hmong group is

00:25:01,720 --> 00:25:08,680
consuming about 1.8 gigabytes per second

00:25:05,130 --> 00:25:10,750
okay so that's the existing res Cantrill

00:25:08,680 --> 00:25:13,150
interface in linux kernel so let's now

00:25:10,750 --> 00:25:16,500
try to fit the our container model in

00:25:13,150 --> 00:25:18,850
Bourque onto his troll here's a

00:25:16,500 --> 00:25:21,820
hint it didn't work so well the first

00:25:18,850 --> 00:25:23,800
time we tried it so our use case is to

00:25:21,820 --> 00:25:27,070
dynamically apply men where bandwidth

00:25:23,800 --> 00:25:28,780
throttling to workloads are consuming

00:25:27,070 --> 00:25:29,830
too much memory bandwidth when the

00:25:28,780 --> 00:25:32,140
machine is in trouble

00:25:29,830 --> 00:25:34,000
so to set this up first you create two

00:25:32,140 --> 00:25:36,130
different rest control groups one for no

00:25:34,000 --> 00:25:38,080
FRA tolling one with FRA tooling and

00:25:36,130 --> 00:25:40,390
then when you have a new C group that

00:25:38,080 --> 00:25:43,690
comes in then you have to first go

00:25:40,390 --> 00:25:45,430
create a Mon group in in one res control

00:25:43,690 --> 00:25:46,900
group and then when that C group starts

00:25:45,430 --> 00:25:49,120
out then you have to start putting tasks

00:25:46,900 --> 00:25:51,460
into the rest control group you have to

00:25:49,120 --> 00:25:53,950
do that one at a time then you then have

00:25:51,460 --> 00:25:57,070
to do that again the Mon group and then

00:25:53,950 --> 00:26:00,160
now your secret has started then when it

00:25:57,070 --> 00:26:01,630
comes time to you then detect a C group

00:26:00,160 --> 00:26:02,830
it's causing memory bandwidth problems

00:26:01,630 --> 00:26:04,840
you didn't want to move it to the

00:26:02,830 --> 00:26:06,760
bandwidth fraud old res control group so

00:26:04,840 --> 00:26:09,790
then you have to then move all the tits

00:26:06,760 --> 00:26:11,500
over right you know 30 second

00:26:09,790 --> 00:26:14,890
description sounds easy sounds fine

00:26:11,500 --> 00:26:17,590
what's the big deal so turns out

00:26:14,890 --> 00:26:19,540
Devils in the details and actually as

00:26:17,590 --> 00:26:21,940
currently designed it creates a lot of

00:26:19,540 --> 00:26:24,580
complexity and toil for a knowed agent

00:26:21,940 --> 00:26:27,550
software to use rez control specifically

00:26:24,580 --> 00:26:30,160
at Google there can be tasks that create

00:26:27,550 --> 00:26:32,260
thousands of Reds so recall that you

00:26:30,160 --> 00:26:34,390
have to put each Fred into the task file

00:26:32,260 --> 00:26:37,930
one at a time not very fast pretty

00:26:34,390 --> 00:26:40,830
expensive to do so and one more and a

00:26:37,930 --> 00:26:43,600
more subtle issue is that when these

00:26:40,830 --> 00:26:45,490
tasks are creating friends you can

00:26:43,600 --> 00:26:48,220
actually get a race condition where

00:26:45,490 --> 00:26:50,440
between when you started putting kids

00:26:48,220 --> 00:26:52,090
into the file and when you finish some

00:26:50,440 --> 00:26:54,250
new friends have been created at that

00:26:52,090 --> 00:26:56,050
time so you have to keep on cycling

00:26:54,250 --> 00:26:57,580
through putting threads in until you're

00:26:56,050 --> 00:27:00,190
reasonably confident that you got all

00:26:57,580 --> 00:27:02,650
the Fred's now you need to do this three

00:27:00,190 --> 00:27:04,480
times right twice at the beginning when

00:27:02,650 --> 00:27:07,740
you create it and third one you need to

00:27:04,480 --> 00:27:10,420
reassign the quality of service config

00:27:07,740 --> 00:27:13,810
another challenge that we faced which is

00:27:10,420 --> 00:27:16,960
a functionality challenge is that when

00:27:13,810 --> 00:27:18,670
you try to move a particular task into a

00:27:16,960 --> 00:27:21,100
different resource control group you

00:27:18,670 --> 00:27:23,140
can't actually move them on group with

00:27:21,100 --> 00:27:25,240
it you have to go Riaan Stan she ate

00:27:23,140 --> 00:27:27,670
that new monitoring group which causes a

00:27:25,240 --> 00:27:30,220
new RM ID to be allocated and assigned

00:27:27,670 --> 00:27:32,830
to this task and in a meanwhile on your

00:27:30,220 --> 00:27:36,310
cpu all that data you brought to the l3

00:27:32,830 --> 00:27:38,680
it's associated with that old RMIT right

00:27:36,310 --> 00:27:41,140
so then now you have this new RMIT and

00:27:38,680 --> 00:27:43,060
it's phantom old RMIT it's very

00:27:41,140 --> 00:27:45,130
difficult to then get the correct data

00:27:43,060 --> 00:27:49,900
for last level cache occupancy

00:27:45,130 --> 00:27:52,150
monitoring okay so we then asked

00:27:49,900 --> 00:27:53,800
ourselves our question you know this is

00:27:52,150 --> 00:27:56,860
not great how do we make it better and

00:27:53,800 --> 00:27:58,930
the main idea is here's what we now had

00:27:56,860 --> 00:28:01,510
the ability to have a one-to-one mapping

00:27:58,930 --> 00:28:03,610
of containers to res control groups

00:28:01,510 --> 00:28:05,770
right then this way when we want the

00:28:03,610 --> 00:28:09,430
change out to us config all we do is

00:28:05,770 --> 00:28:11,380
rewrite to schemata little done we don't

00:28:09,430 --> 00:28:13,210
have to move the tins around we could to

00:28:11,380 --> 00:28:15,670
keep the existing RMIT so we don't get

00:28:13,210 --> 00:28:19,480
the l3 occupy CD synchronization issue

00:28:15,670 --> 00:28:22,120
and it's also 100% compatible with the

00:28:19,480 --> 00:28:24,730
existing resin troll abstraction right

00:28:22,120 --> 00:28:27,230
it's an implementation detail that each

00:28:24,730 --> 00:28:30,260
resident roll group happens to

00:28:27,230 --> 00:28:33,020
require one class of service ID right so

00:28:30,260 --> 00:28:34,940
the challenge as I just alluded to is

00:28:33,020 --> 00:28:36,470
that with the existing system you're

00:28:34,940 --> 00:28:39,169
gonna run out of class of service IDs

00:28:36,470 --> 00:28:42,710
very quickly right on the skylake you

00:28:39,169 --> 00:28:44,900
try creating six actually 15 rez control

00:28:42,710 --> 00:28:47,030
groups that's it you can't create any

00:28:44,900 --> 00:28:49,640
more right but aboard we want to run

00:28:47,030 --> 00:28:52,390
potentially hundreds of containers right

00:28:49,640 --> 00:28:55,040
so the solution to this is to

00:28:52,390 --> 00:28:57,140
essentially share class of service IDs

00:28:55,040 --> 00:29:01,910
recall that from earlier we only have a

00:28:57,140 --> 00:29:04,160
small finite number of QoS configs so we

00:29:01,910 --> 00:29:05,809
essentially we want to de doop these

00:29:04,160 --> 00:29:07,610
config so as long as you have the same

00:29:05,809 --> 00:29:10,700
config you should get the same class of

00:29:07,610 --> 00:29:12,440
service ID there's no need to have ten

00:29:10,700 --> 00:29:15,919
different classes service IDs that all

00:29:12,440 --> 00:29:18,110
have the exact same configuration and so

00:29:15,919 --> 00:29:20,059
Google has developed a kernel patch to

00:29:18,110 --> 00:29:21,860
enable this functionality and it will be

00:29:20,059 --> 00:29:22,900
hopefully be released soon in the next

00:29:21,860 --> 00:29:25,160
couple of weeks

00:29:22,900 --> 00:29:28,700
Stefan and the audience here did a lot

00:29:25,160 --> 00:29:31,220
of hard work to make that happen and the

00:29:28,700 --> 00:29:32,809
main takeaway point that I want to that

00:29:31,220 --> 00:29:35,419
I want all of you to leave with is that

00:29:32,809 --> 00:29:37,549
this really demonstrates the need to

00:29:35,419 --> 00:29:40,100
make the container model a first-class

00:29:37,549 --> 00:29:42,169
consideration when designing kernel

00:29:40,100 --> 00:29:44,630
interfaces to manage hardware quality of

00:29:42,169 --> 00:29:47,570
service mechanisms right because the

00:29:44,630 --> 00:29:49,640
original design was it was honestly is

00:29:47,570 --> 00:29:51,860
designed for more HPC type of use case

00:29:49,640 --> 00:29:53,809
right the container model wasn't really

00:29:51,860 --> 00:29:55,580
dot off so then when we tried to fit

00:29:53,809 --> 00:29:58,100
containers on top of it it was like

00:29:55,580 --> 00:30:00,440
putting a square peg for a circular hole

00:29:58,100 --> 00:30:01,450
you can do it it's just not very pretty

00:30:00,440 --> 00:30:05,600
and it leads to a lot of unnecessary

00:30:01,450 --> 00:30:08,450
complications okay so to close this all

00:30:05,600 --> 00:30:10,880
out now let's see at how simple is after

00:30:08,450 --> 00:30:12,260
we make this change so again same use

00:30:10,880 --> 00:30:15,169
case you want to dynamically apply a

00:30:12,260 --> 00:30:17,960
memory valve Rawling to abusers so in

00:30:15,169 --> 00:30:19,850
this case what we do is now we just

00:30:17,960 --> 00:30:22,460
create a new resident roll group when we

00:30:19,850 --> 00:30:24,830
start to see group right into no Frawley

00:30:22,460 --> 00:30:26,690
and configure nets Kumada move the kids

00:30:24,830 --> 00:30:28,340
into the task file notice how we only

00:30:26,690 --> 00:30:31,340
have to do that once now as opposed to

00:30:28,340 --> 00:30:33,169
twice and when we need to apply fraud

00:30:31,340 --> 00:30:34,280
alene we just we might rewrite two

00:30:33,169 --> 00:30:38,470
schemata with the new Frawley

00:30:34,280 --> 00:30:38,470
configuration and that's it we're done

00:30:40,760 --> 00:30:50,490
so after we saw the final solution we do

00:30:45,810 --> 00:30:51,800
have some conclusions about interacting

00:30:50,490 --> 00:30:54,240
between cgroups

00:30:51,800 --> 00:30:57,000
container runtimes and micro

00:30:54,240 --> 00:31:00,960
architectural enforcement or monitoring

00:30:57,000 --> 00:31:03,420
tools we believe that a big reason for

00:31:00,960 --> 00:31:05,630
the success of this project is the good

00:31:03,420 --> 00:31:08,160
interaction between the two and

00:31:05,630 --> 00:31:13,380
interfaces the interfaces that we have

00:31:08,160 --> 00:31:15,930
now aren't the very well suited for this

00:31:13,380 --> 00:31:18,540
so we would love to contribute to a

00:31:15,930 --> 00:31:21,030
standard framework around performance

00:31:18,540 --> 00:31:24,390
management for container runtimes that

00:31:21,030 --> 00:31:28,560
would integrate both to both of these of

00:31:24,390 --> 00:31:32,520
these approaches so what are the

00:31:28,560 --> 00:31:36,450
takeaways from from this talk as time

00:31:32,520 --> 00:31:38,790
goes on we see bees in a name no easy

00:31:36,450 --> 00:31:41,220
neighbor and resource isolation

00:31:38,790 --> 00:31:44,730
low-level resource isolation problems

00:31:41,220 --> 00:31:47,580
become more and more frequent and the

00:31:44,730 --> 00:31:50,550
key to controlling them is continuous

00:31:47,580 --> 00:31:54,660
monitoring and enforcement especially

00:31:50,550 --> 00:31:57,110
when you want to have multi tenant hosts

00:31:54,660 --> 00:32:02,460
where the workload may not be

00:31:57,110 --> 00:32:05,070
well-behaved all the time for the work

00:32:02,460 --> 00:32:07,410
described in in this talk we plan to do

00:32:05,070 --> 00:32:10,320
to current extent to extensions to

00:32:07,410 --> 00:32:14,010
improve the success rate we also want to

00:32:10,320 --> 00:32:16,230
gather feedback about building a general

00:32:14,010 --> 00:32:20,250
framework that would help collaboration

00:32:16,230 --> 00:32:25,320
in the community for C group and rest

00:32:20,250 --> 00:32:28,800
control interaction and longer term we

00:32:25,320 --> 00:32:32,150
are looking into doing better scheduling

00:32:28,800 --> 00:32:35,880
ahead of time for memory bandwidth

00:32:32,150 --> 00:32:39,950
control based on hints that we could

00:32:35,880 --> 00:32:44,880
detect based on past job behavior and

00:32:39,950 --> 00:32:47,610
known job sensitivities in conclusion I

00:32:44,880 --> 00:32:50,160
also want to say thank you to the more

00:32:47,610 --> 00:32:51,110
than 10 people who contributed to this

00:32:50,160 --> 00:32:53,610
project that

00:32:51,110 --> 00:32:57,350
thank you very much if there are any

00:32:53,610 --> 00:32:57,350
questions we'll be glad to take them

00:33:02,330 --> 00:33:08,460
[Applause]

00:33:06,049 --> 00:33:10,679
when you sit with isolation for D main

00:33:08,460 --> 00:33:12,799
isolating it on a particular box or do

00:33:10,679 --> 00:33:18,330
you mean moving into a socket with

00:33:12,799 --> 00:33:20,940
numeric ation so in general when we talk

00:33:18,330 --> 00:33:23,090
about isolation or performance isolation

00:33:20,940 --> 00:33:26,669
we sort of want to you know give this

00:33:23,090 --> 00:33:28,679
you know it's the goal is to be able to

00:33:26,669 --> 00:33:31,019
get to a point where every workload

00:33:28,679 --> 00:33:33,539
running on the system they see the same

00:33:31,019 --> 00:33:35,399
performance as if you know there were

00:33:33,539 --> 00:33:38,610
the only ones running on that system

00:33:35,399 --> 00:33:40,230
right so that could be you know for

00:33:38,610 --> 00:33:41,909
example Numa isolation as you mentioned

00:33:40,230 --> 00:33:45,499
is potentially one mechanism in order to

00:33:41,909 --> 00:33:45,499
provide illusioned

00:34:09,270 --> 00:34:15,179
you can balance the workload between the

00:34:12,569 --> 00:34:17,520
two sockets but we saw that this

00:34:15,179 --> 00:34:19,589
frequently would not solve the issue in

00:34:17,520 --> 00:34:22,290
our case because when one socket is

00:34:19,589 --> 00:34:25,829
saturated the other one is also highly

00:34:22,290 --> 00:34:29,250
reloaded we didn't dig you know super

00:34:25,829 --> 00:34:32,190
deep in into why this happens but it may

00:34:29,250 --> 00:34:42,030
be that big jobs may be run on both

00:34:32,190 --> 00:34:44,970
sockets and so I have a question on so

00:34:42,030 --> 00:34:46,560
in case of deep you workloads like what

00:34:44,970 --> 00:34:49,710
you have in tens of floors links in

00:34:46,560 --> 00:34:52,079
there so primarily the jobs there the

00:34:49,710 --> 00:34:54,599
bandwidth is more consumed is by at the

00:34:52,079 --> 00:34:56,819
GPU network it's not the CPU thread

00:34:54,599 --> 00:34:58,890
which is actually generating bandwidth

00:34:56,819 --> 00:35:01,170
in there so there the CPU that is more

00:34:58,890 --> 00:35:04,160
of just setting up the memory locations

00:35:01,170 --> 00:35:06,200
and setting the typical to go

00:35:04,160 --> 00:35:08,750
so if you go into throttle based on the

00:35:06,200 --> 00:35:11,150
memory bandwidth for the CPU thread

00:35:08,750 --> 00:35:12,760
that's gonna hurt the gpo approach

00:35:11,150 --> 00:35:17,930
because there you are actually

00:35:12,760 --> 00:35:20,770
decreasing the the workload by not

00:35:17,930 --> 00:35:22,670
growing the primary thread to make him

00:35:20,770 --> 00:35:27,829
allocate memory bandwidth things in

00:35:22,670 --> 00:35:29,390
there so how do you handle that probably

00:35:27,829 --> 00:35:30,829
should have made it earlier that the

00:35:29,390 --> 00:35:33,799
stuff we are talking about here is

00:35:30,829 --> 00:35:36,559
running ML models on computer not on

00:35:33,799 --> 00:35:38,180
GPUs so we do something similar for GPUs

00:35:36,559 --> 00:35:39,589
but the throttling model is different

00:35:38,180 --> 00:35:42,380
and the problem is kind of easier

00:35:39,589 --> 00:35:44,450
because the machines there are optimized

00:35:42,380 --> 00:35:47,119
to run GPUs and we kind of take

00:35:44,450 --> 00:35:49,250
everything that I disturbed those jobs

00:35:47,119 --> 00:35:52,069
so this is for running like general

00:35:49,250 --> 00:35:54,880
running I got some bad jobs and machines

00:35:52,069 --> 00:35:54,880
are kind of idle

00:36:01,630 --> 00:36:05,750
are there so that raises another

00:36:04,340 --> 00:36:07,490
question are there other hardware

00:36:05,750 --> 00:36:09,590
resources when I first looked at the

00:36:07,490 --> 00:36:11,330
original description it wasn't just

00:36:09,590 --> 00:36:13,120
memory bandwidth and caching allocation

00:36:11,330 --> 00:36:16,250
you could control there were other

00:36:13,120 --> 00:36:16,700
Hardware quality of service metrics

00:36:16,250 --> 00:36:18,770
available

00:36:16,700 --> 00:36:21,920
are you planning to extend to use any of

00:36:18,770 --> 00:36:23,840
those so I think today what's available

00:36:21,920 --> 00:36:26,180
in commercial hardware's just eternally

00:36:23,840 --> 00:36:27,980
last level cache memory bandwidth and

00:36:26,180 --> 00:36:29,600
then there's some more fancy dings unit

00:36:27,980 --> 00:36:31,700
of the cache for example what's called a

00:36:29,600 --> 00:36:34,100
code data partitioning so you can say oh

00:36:31,700 --> 00:36:36,740
I allocate some portion to cache for the

00:36:34,100 --> 00:36:39,320
code versus the data but you know endo

00:36:36,740 --> 00:36:41,300
and we we definitely do you know see the

00:36:39,320 --> 00:36:42,920
need for more Hardware quality of

00:36:41,300 --> 00:36:45,680
service mechanism the future right it's

00:36:42,920 --> 00:36:49,490
for example the interconnect between

00:36:45,680 --> 00:36:51,890
sockets PCIe IO right there's a lot your

00:36:49,490 --> 00:36:53,810
your performance isolation illusion is

00:36:51,890 --> 00:36:56,180
only as strong as the weakest link in

00:36:53,810 --> 00:36:57,980
the chain right so even though you can

00:36:56,180 --> 00:36:59,390
fix memory bandwidth it can fix cache

00:36:57,980 --> 00:37:01,250
there's always going to be something

00:36:59,390 --> 00:37:03,470
else that can come up and break your

00:37:01,250 --> 00:37:06,050
illusion and performance isolation and

00:37:03,470 --> 00:37:07,430
so we really look forward to working

00:37:06,050 --> 00:37:10,100
with vendors and with the community to

00:37:07,430 --> 00:37:17,050
help enable these mechanisms and to

00:37:10,100 --> 00:37:17,050
control them in a unified easy fashion I

00:37:19,600 --> 00:37:23,720
related to the performance isolation as

00:37:22,040 --> 00:37:27,350
well while recognizing that it's so much

00:37:23,720 --> 00:37:28,760
work with Wroclaw dependent how many

00:37:27,350 --> 00:37:30,260
times if you had problems which

00:37:28,760 --> 00:37:31,670
uncontrolled memory bandwidth

00:37:30,260 --> 00:37:35,240
consumption things that happen out of

00:37:31,670 --> 00:37:36,920
bands like worker traits completing io k

00:37:35,240 --> 00:37:39,260
compacted e running in the background it

00:37:36,920 --> 00:37:42,200
requires memory bandwidth intensive VM

00:37:39,260 --> 00:37:44,470
scan in certain cases and so on

00:37:42,200 --> 00:37:47,180
have you had common problems with

00:37:44,470 --> 00:37:50,000
uncontrolled bandwidth access or this is

00:37:47,180 --> 00:37:53,960
relatively rare in your experience it's

00:37:50,000 --> 00:37:55,760
been relatively especially all the

00:37:53,960 --> 00:37:58,610
kernel that we do monitor those two and

00:37:55,760 --> 00:38:01,160
track how much they use this mostly has

00:37:58,610 --> 00:38:03,290
been like very much more intensive and

00:38:01,160 --> 00:38:05,480
continuous things we see a lot of burst

00:38:03,290 --> 00:38:06,860
from kernels will die down within a

00:38:05,480 --> 00:38:07,660
second and none of these things kind of

00:38:06,860 --> 00:38:09,460
applied

00:38:07,660 --> 00:38:11,260
they don't last that long there I can do

00:38:09,460 --> 00:38:13,000
something about it we try to isolate

00:38:11,260 --> 00:38:15,550
them different ways try to keep them

00:38:13,000 --> 00:38:17,530
away from other jobs but we don't have

00:38:15,550 --> 00:38:20,440
to go to this extent for those problems

00:38:17,530 --> 00:38:25,510
to answer that did you see any of the l1

00:38:20,440 --> 00:38:28,060
th mitigation is trying to interfere no

00:38:25,510 --> 00:38:30,760
I think as we described right we have a

00:38:28,060 --> 00:38:32,710
saturation threshold that's probably

00:38:30,760 --> 00:38:34,450
high enough that Elwyn DF and other

00:38:32,710 --> 00:38:37,240
things don't actually push the machines

00:38:34,450 --> 00:38:38,920
that high and again this one happens

00:38:37,240 --> 00:38:41,200
like not all the time it's happening

00:38:38,920 --> 00:38:43,300
like once a day in one cluster for us

00:38:41,200 --> 00:38:45,700
for a few hours it's big enough to

00:38:43,300 --> 00:38:47,320
disrupt whatever we are doing but it's

00:38:45,700 --> 00:38:48,910
not like every second on a machine and

00:38:47,320 --> 00:38:50,740
we need to kind of control it that level

00:38:48,910 --> 00:38:53,410
so that's why all our approaches here

00:38:50,740 --> 00:38:55,810
are more reactive we do something and we

00:38:53,410 --> 00:38:57,640
notice it if it was the other way around

00:38:55,810 --> 00:38:59,890
we'll have to do something right at the

00:38:57,640 --> 00:39:01,690
scheduling time be very sure what we are

00:38:59,890 --> 00:39:03,180
allocating everybody and never let them

00:39:01,690 --> 00:39:06,360
go across so we are not there yet

00:39:03,180 --> 00:39:06,360
things will change

00:39:09,780 --> 00:39:15,670
so for monitoring like how often do we

00:39:12,970 --> 00:39:17,820
need to monitor it how how intrusive is

00:39:15,670 --> 00:39:17,820
it

00:39:20,350 --> 00:39:30,880
at Google we are not yet at a stage

00:39:24,070 --> 00:39:33,340
where we where jobs jobs either are not

00:39:30,880 --> 00:39:35,560
you know affected by short bursts of

00:39:33,340 --> 00:39:37,780
saturation which is why an is the

00:39:35,560 --> 00:39:43,000
scheduling the reactive scheduling works

00:39:37,780 --> 00:39:45,460
well or they you know tolerate them we

00:39:43,000 --> 00:39:48,930
run them every couple of minutes the the

00:39:45,460 --> 00:39:53,170
checks so it's a pretty course

00:39:48,930 --> 00:39:56,610
monitoring then that and that seems to

00:39:53,170 --> 00:39:56,610
be as efficient yes

00:40:06,770 --> 00:40:11,180
so you monitor every couple of minutes

00:40:09,410 --> 00:40:13,550
what's the overhead of monitoring could

00:40:11,180 --> 00:40:15,770
it be smaller

00:40:13,550 --> 00:40:19,460
you're you're an excellent plant in the

00:40:15,770 --> 00:40:21,350
audience so cert so the sake level

00:40:19,460 --> 00:40:22,670
monitoring doubt when is fairly low

00:40:21,350 --> 00:40:24,260
overhead because we just go to

00:40:22,670 --> 00:40:26,420
essentially you know one hardware block

00:40:24,260 --> 00:40:28,880
and ask you know how much memory valve

00:40:26,420 --> 00:40:31,730
is being consumed the per task

00:40:28,880 --> 00:40:34,100
monitoring using the CPU performance

00:40:31,730 --> 00:40:36,710
counters using a perf subsystem in

00:40:34,100 --> 00:40:38,869
secret profiling mode that is very

00:40:36,710 --> 00:40:40,700
expensive right because in C group mode

00:40:38,869 --> 00:40:42,760
now at context switch time you have to

00:40:40,700 --> 00:40:45,110
go save and restore all those

00:40:42,760 --> 00:40:46,490
performance counters you have to go find

00:40:45,110 --> 00:40:49,460
the next thing is actually fairly

00:40:46,490 --> 00:40:51,440
expensive and noticeable and actually

00:40:49,460 --> 00:40:53,540
there are users within Google that have

00:40:51,440 --> 00:40:55,700
complained very loudly at us for their

00:40:53,540 --> 00:40:57,410
performance hit that they see on that so

00:40:55,700 --> 00:40:59,480
that's actually an issue that you know

00:40:57,410 --> 00:41:02,540
we hope to see get addressed how to make

00:40:59,480 --> 00:41:04,430
you know DC group of monitoring more

00:41:02,540 --> 00:41:06,290
lightweight because we get a lot of

00:41:04,430 --> 00:41:08,650
value out of it as you seen in today's

00:41:06,290 --> 00:41:08,650
talk

00:41:22,990 --> 00:41:30,290
how do we do scipio throat ho

00:41:27,470 --> 00:41:32,360
I think for CPU throttle is basically we

00:41:30,290 --> 00:41:34,010
just restrict the access for the jobs

00:41:32,360 --> 00:41:36,290
that are using a lot of memory to a

00:41:34,010 --> 00:41:38,150
fewer CPUs and they run slower and they

00:41:36,290 --> 00:41:40,280
can only access part of the available

00:41:38,150 --> 00:41:41,750
socket bandwidth and that's how we keep

00:41:40,280 --> 00:41:43,880
bringing them down it's good in Chile to

00:41:41,750 --> 00:41:46,640
see how much we can throttle and we have

00:41:43,880 --> 00:41:48,080
a minimum and we kind of do a mix of

00:41:46,640 --> 00:41:49,730
both approaches while we are throttling

00:41:48,080 --> 00:41:52,100
and if it's not solving the problem and

00:41:49,730 --> 00:41:54,260
we can't go any further down it already

00:41:52,100 --> 00:41:57,460
triggers a vixen mechanism and we'll

00:41:54,260 --> 00:41:57,460
kick somebody out but it

00:42:04,140 --> 00:42:11,730
thank you

00:42:05,890 --> 00:42:11,730

YouTube URL: https://www.youtube.com/watch?v=29b7n2rqWVM


