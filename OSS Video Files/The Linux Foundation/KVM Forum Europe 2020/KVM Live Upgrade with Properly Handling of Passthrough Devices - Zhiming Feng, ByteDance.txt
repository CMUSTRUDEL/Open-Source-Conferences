Title: KVM Live Upgrade with Properly Handling of Passthrough Devices - Zhiming Feng, ByteDance
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	KVM Live Upgrade with Properly Handling of Passthrough Devices - Zhiming Feng, ByteDance
Captions: 
	00:00:06,560 --> 00:00:11,040
hello

00:00:07,520 --> 00:00:15,280
my name is fun zumi i'm now working

00:00:11,040 --> 00:00:19,359
for pat dance and currently

00:00:15,280 --> 00:00:23,439
folks or watch like this later project

00:00:19,359 --> 00:00:27,199
today i will share my topic kvm

00:00:23,439 --> 00:00:29,679
live upgrade with properly hiding of a

00:00:27,199 --> 00:00:33,280
pass-through device

00:00:29,679 --> 00:00:37,840
next i will need to introduce it

00:00:33,280 --> 00:00:37,840
from the four aspects

00:00:38,879 --> 00:00:42,320
with the development of technology the

00:00:41,680 --> 00:00:45,600
vm

00:00:42,320 --> 00:00:50,000
has to be frequently updated and restart

00:00:45,600 --> 00:00:53,199
to add security paths and new features

00:00:50,000 --> 00:00:56,239
there are two existing live

00:00:53,199 --> 00:01:00,879
live update methods to improve the

00:00:56,239 --> 00:01:04,559
cloud availability kvm live pets

00:01:00,879 --> 00:01:07,600
and virtual machine live migration

00:01:04,559 --> 00:01:11,439
however labels has a

00:01:07,600 --> 00:01:12,000
serious drawbacks kernel live pads

00:01:11,439 --> 00:01:15,439
cannot

00:01:12,000 --> 00:01:19,600
handle complex change for example

00:01:15,439 --> 00:01:22,880
change to persistence data structure

00:01:19,600 --> 00:01:25,040
vm live migration cannot handle the

00:01:22,880 --> 00:01:28,159
pass-through devices

00:01:25,040 --> 00:01:31,439
and it may be

00:01:28,159 --> 00:01:35,119
incur unacceptable a lot less

00:01:31,439 --> 00:01:36,400
because live magazine needs to copy

00:01:35,119 --> 00:01:40,640
memory formula

00:01:36,400 --> 00:01:43,680
or cumule to new cumule

00:01:40,640 --> 00:01:47,200
in this topic i will introduce our live

00:01:43,680 --> 00:01:50,320
upgrade method we can probably

00:01:47,200 --> 00:01:55,759
update the kvm and the qmu

00:01:50,320 --> 00:02:00,159
without interrupt customer vms

00:01:55,759 --> 00:02:03,520
very difficult for vm live upgrade

00:02:00,159 --> 00:02:07,119
how to handle the password device

00:02:03,520 --> 00:02:10,160
during the live upgrade and

00:02:07,119 --> 00:02:13,920
minimized service downtime is

00:02:10,160 --> 00:02:17,040
a major concern of cloud

00:02:13,920 --> 00:02:20,319
providers in this talk

00:02:17,040 --> 00:02:24,080
we will analyze the requirements

00:02:20,319 --> 00:02:27,599
for the password device hadoop

00:02:24,080 --> 00:02:30,400
and present how we following

00:02:27,599 --> 00:02:31,280
we how we how we follow those

00:02:30,400 --> 00:02:34,959
requirements

00:02:31,280 --> 00:02:38,599
to properly handle password devices

00:02:34,959 --> 00:02:41,519
in our kvm live upgrade

00:02:38,599 --> 00:02:45,280
implementation in this

00:02:41,519 --> 00:02:49,360
we also optimize the start-up

00:02:45,280 --> 00:02:54,560
and suspend of vm to decrease

00:02:49,360 --> 00:02:54,560
the door time during the live upgrade

00:02:55,760 --> 00:03:03,519
this is the framework for live

00:02:59,040 --> 00:03:06,720
upgrade in order to live upgrade the

00:03:03,519 --> 00:03:09,920
kvm we modify kvm module

00:03:06,720 --> 00:03:14,480
and allow it to be complied

00:03:09,920 --> 00:03:17,879
multiple modules named by km1 module

00:03:14,480 --> 00:03:21,040
qmm2 module and so on

00:03:17,879 --> 00:03:24,480
specific implement

00:03:21,040 --> 00:03:27,840
implementation is that we move

00:03:24,480 --> 00:03:31,519
both most of the qm module function

00:03:27,840 --> 00:03:32,799
into qm enter module to load the

00:03:31,519 --> 00:03:37,680
multiple

00:03:32,799 --> 00:03:40,959
uh copies of kim internal modules

00:03:37,680 --> 00:03:45,519
we associate all the already

00:03:40,959 --> 00:03:48,959
all all the original global uh

00:03:45,519 --> 00:03:52,319
variables where variables

00:03:48,959 --> 00:03:55,920
in the in kvm model with a

00:03:52,319 --> 00:03:58,560
chemical model and make all the global

00:03:55,920 --> 00:04:02,799
function local

00:03:58,560 --> 00:04:06,480
in linux a device password is enabled by

00:04:02,799 --> 00:04:10,080
waffle during the live upgrade

00:04:06,480 --> 00:04:13,680
we inherit the vfl

00:04:10,080 --> 00:04:15,360
connector from the outcome to the new

00:04:13,680 --> 00:04:18,959
cumule

00:04:15,360 --> 00:04:22,800
and the vm's memory is shared by the new

00:04:18,959 --> 00:04:26,240
at cumulative process the mapping from

00:04:22,800 --> 00:04:30,000
gpa to hpa is not changed during the

00:04:26,240 --> 00:04:34,320
life upgrade at the iommu

00:04:30,000 --> 00:04:34,800
table under the i o mmu translation

00:04:34,320 --> 00:04:37,040
table

00:04:34,800 --> 00:04:37,040
is

00:04:38,240 --> 00:04:45,120
my reminder validity

00:04:41,520 --> 00:04:49,440
so device dma operation can

00:04:45,120 --> 00:04:53,360
continue executing without interruption

00:04:49,440 --> 00:04:53,360
even when the vm is stopped

00:04:53,919 --> 00:04:57,919
for password device how to install

00:04:57,040 --> 00:05:01,360
interrupt

00:04:57,919 --> 00:05:04,400
is not lost during the live upgrade

00:05:01,360 --> 00:05:05,039
this is difficult because password

00:05:04,400 --> 00:05:08,400
device

00:05:05,039 --> 00:05:12,080
is not suspended for this reason

00:05:08,400 --> 00:05:15,919
the password device interrupts can incur

00:05:12,080 --> 00:05:19,919
at any time so we cannot

00:05:15,919 --> 00:05:22,639
completely copy the password device

00:05:19,919 --> 00:05:23,520
interrupt from the old commute to the

00:05:22,639 --> 00:05:27,199
nucleus

00:05:23,520 --> 00:05:27,199
during the live upgrade

00:05:27,280 --> 00:05:35,759
there is existing solution

00:05:31,520 --> 00:05:39,600
inject inject additional virtual

00:05:35,759 --> 00:05:42,960
erq is quite there first

00:05:39,600 --> 00:05:46,400
new cumule inherits vfl event have this

00:05:42,960 --> 00:05:49,680
from the old cumule second

00:05:46,400 --> 00:05:52,160
then you accumulate the switch from

00:05:49,680 --> 00:05:53,600
entirety and receive the paint

00:05:52,160 --> 00:05:58,000
interrupts

00:05:53,600 --> 00:06:01,360
in last inject additional virtual aiq

00:05:58,000 --> 00:06:04,160
into the vm after having

00:06:01,360 --> 00:06:04,160
over device

00:06:05,919 --> 00:06:10,639
in this topic we use post interrupt

00:06:09,600 --> 00:06:14,160
technology

00:06:10,639 --> 00:06:18,639
to inject interrupt a device

00:06:14,160 --> 00:06:20,400
will set a post interrupt request speed

00:06:18,639 --> 00:06:21,840
where the device is reading and

00:06:20,400 --> 00:06:25,440
interrupt

00:06:21,840 --> 00:06:28,400
so we need only to ensure the sim

00:06:25,440 --> 00:06:29,199
peer descriptor data between the old

00:06:28,400 --> 00:06:32,560
cumule

00:06:29,199 --> 00:06:35,759
and the new communal process

00:06:32,560 --> 00:06:38,560
we allow pierre descriptor data is

00:06:35,759 --> 00:06:39,520
shared between the new cumula and the

00:06:38,560 --> 00:06:43,360
autocum

00:06:39,520 --> 00:06:43,360
during the live upgrade

00:06:44,479 --> 00:06:51,840
this picture so pidscrapter data

00:06:48,160 --> 00:06:55,039
uh initialization compared to

00:06:51,840 --> 00:06:58,479
original kvm design

00:06:55,039 --> 00:07:02,319
in order to share pi descriptor data

00:06:58,479 --> 00:07:05,599
in the new cumule we

00:07:02,319 --> 00:07:09,440
locate we allocate the memory for

00:07:05,599 --> 00:07:12,240
four ps capital data structure

00:07:09,440 --> 00:07:12,240
in the accumula

00:07:12,800 --> 00:07:18,080
there are three key points for peer

00:07:15,199 --> 00:07:21,120
descriptor structure initialization

00:07:18,080 --> 00:07:24,160
in the new cumule first pi

00:07:21,120 --> 00:07:28,240
described predictor shouldn't not

00:07:24,160 --> 00:07:31,440
be initialized in any nitride

00:07:28,240 --> 00:07:35,280
while the new cumule is neutralized

00:07:31,440 --> 00:07:38,000
second the nucleus don't need to seek

00:07:35,280 --> 00:07:39,680
post-interrupt requesting data from the

00:07:38,000 --> 00:07:42,400
outcome

00:07:39,680 --> 00:07:43,599
because pierre descriptor data is shared

00:07:42,400 --> 00:07:47,360
between

00:07:43,599 --> 00:07:51,599
the new cumule and the automobile

00:07:47,360 --> 00:07:52,319
in last the nucleus don't need to update

00:07:51,599 --> 00:07:57,120
interrupt

00:07:52,319 --> 00:07:57,120
mapping table during the live upgrade

00:07:57,360 --> 00:08:04,560
next i will introduce

00:08:00,400 --> 00:08:08,479
how to optimize the vm downtime

00:08:04,560 --> 00:08:08,479
during the live upgrade

00:08:08,960 --> 00:08:16,879
this picture saw the live upgrade

00:08:12,800 --> 00:08:19,919
flow diagram the first step

00:08:16,879 --> 00:08:23,120
we focus uh choosing process

00:08:19,919 --> 00:08:26,240
and execute the new

00:08:23,120 --> 00:08:27,680
qmu binary and the new community is in

00:08:26,240 --> 00:08:30,879
slide

00:08:27,680 --> 00:08:31,599
the second step we stopped the old

00:08:30,879 --> 00:08:34,959
commute

00:08:31,599 --> 00:08:38,719
and the silver way and the state

00:08:34,959 --> 00:08:41,599
the last stand the nuclear load

00:08:38,719 --> 00:08:43,200
state from loud cumulative and start the

00:08:41,599 --> 00:08:47,279
new commute

00:08:43,200 --> 00:08:50,720
it is obvious that vm.time

00:08:47,279 --> 00:08:54,560
contents the following feed stop

00:08:50,720 --> 00:08:57,680
the old commute save the old chemistry

00:08:54,560 --> 00:09:02,399
state load the state from the

00:08:57,680 --> 00:09:02,399
automobile and started a new premium

00:09:03,440 --> 00:09:09,760
while stopping the old cumule

00:09:06,560 --> 00:09:10,720
we find the cleaner way for the cleaner

00:09:09,760 --> 00:09:14,399
primitive

00:09:10,720 --> 00:09:17,680
is taking a long time when devices have

00:09:14,399 --> 00:09:21,040
multiple cues for example

00:09:17,680 --> 00:09:24,080
what ionic water blockers are

00:09:21,040 --> 00:09:27,680
the odd communal process will be killed

00:09:24,080 --> 00:09:30,720
after the life upgrade under

00:09:27,680 --> 00:09:34,800
under normal human initialization

00:09:30,720 --> 00:09:37,279
locket lockheed

00:09:34,800 --> 00:09:39,120
event have they will be free by queueing

00:09:37,279 --> 00:09:42,000
process

00:09:39,120 --> 00:09:42,399
however the device you might have the

00:09:42,000 --> 00:09:46,320
will

00:09:42,399 --> 00:09:49,760
be free by a by the

00:09:46,320 --> 00:09:53,040
uh by the us

00:09:49,760 --> 00:09:56,399
by the system uh if the cumulative

00:09:53,040 --> 00:09:58,800
uh free it so device you might have the

00:09:56,399 --> 00:09:59,839
needed to clean up by the outcome

00:09:58,800 --> 00:10:02,880
process

00:09:59,839 --> 00:10:06,720
while the live upgrade is

00:10:02,880 --> 00:10:10,560
successful other

00:10:06,720 --> 00:10:13,519
normal vm startup logic

00:10:10,560 --> 00:10:15,360
first even have the new slide with the

00:10:13,519 --> 00:10:19,519
devices startup

00:10:15,360 --> 00:10:22,640
then vcpu is resumed

00:10:19,519 --> 00:10:26,160
so the vm time content

00:10:22,640 --> 00:10:30,800
contents uh the initialize

00:10:26,160 --> 00:10:30,800
of devices will have the industry

00:10:30,880 --> 00:10:38,480
inspired by the all optimization of

00:10:35,279 --> 00:10:40,720
vm suspender uh we capriculate the

00:10:38,480 --> 00:10:44,240
device symmetric during

00:10:40,720 --> 00:10:47,920
the qmu initialization because

00:10:44,240 --> 00:10:51,200
the vm don't the

00:10:47,920 --> 00:10:54,560
new communal initialization we can

00:10:51,200 --> 00:10:57,680
decrease the startup

00:10:54,560 --> 00:10:57,680
time industry

00:10:59,279 --> 00:11:06,560
in last we we

00:11:02,959 --> 00:11:10,000
we use the shared memory to save

00:11:06,560 --> 00:11:10,640
the old cumulative state and the loading

00:11:10,000 --> 00:11:13,760
state

00:11:10,640 --> 00:11:17,120
in the new cumule happens

00:11:13,760 --> 00:11:19,600
concurrently with receiving state in the

00:11:17,120 --> 00:11:19,600
old queue

00:11:22,240 --> 00:11:29,040
we use the different recipient number

00:11:25,600 --> 00:11:32,240
and memory size to measure

00:11:29,040 --> 00:11:34,720
the recipient time

00:11:32,240 --> 00:11:36,480
with a development with a different

00:11:34,720 --> 00:11:39,600
workload

00:11:36,480 --> 00:11:43,279
we use the different benchmark tools

00:11:39,600 --> 00:11:45,680
to simulate the

00:11:43,279 --> 00:11:47,279
command use the case of the cloud

00:11:45,680 --> 00:11:50,320
service

00:11:47,279 --> 00:11:53,600
including computation

00:11:50,320 --> 00:11:56,800
storage and memory we use

00:11:53,600 --> 00:12:00,240
the following tools streams

00:11:56,800 --> 00:12:04,240
memory tester and file to

00:12:00,240 --> 00:12:08,560
simulate the computation memory

00:12:04,240 --> 00:12:08,560
and and storage

00:12:09,200 --> 00:12:16,000
uh this picture saw the distribution

00:12:12,959 --> 00:12:20,000
of vcpu downtime under

00:12:16,000 --> 00:12:23,920
the vmi uh idle

00:12:20,000 --> 00:12:24,480
we can see the distribution of vm.time

00:12:23,920 --> 00:12:27,519
is

00:12:24,480 --> 00:12:30,959
11 milliseconds to

00:12:27,519 --> 00:12:30,959
34 milliseconds

00:12:31,120 --> 00:12:39,120
next we use the streams tools

00:12:34,399 --> 00:12:43,920
to simulate the competition workload

00:12:39,120 --> 00:12:47,120
while running the street tools in vm

00:12:43,920 --> 00:12:50,279
we update the vm in host

00:12:47,120 --> 00:12:53,360
at we can see the distribution of

00:12:50,279 --> 00:12:59,120
vm.time is 12

00:12:53,360 --> 00:12:59,120
12 milliseconds to 34 milliseconds

00:13:00,560 --> 00:13:09,440
for the memory tester we use the

00:13:05,040 --> 00:13:13,680
memory we we use the

00:13:09,440 --> 00:13:16,720
memory memory tester tools we use

00:13:13,680 --> 00:13:20,000
we use 4 gb memory in

00:13:16,720 --> 00:13:24,000
vm and we offer upward

00:13:20,000 --> 00:13:27,440
the way all we and the way

00:13:24,000 --> 00:13:30,560
upwards the distribution of

00:13:27,440 --> 00:13:34,160
vmware downtime is 12 milliseconds

00:13:30,560 --> 00:13:37,680
to 34 milliseconds

00:13:34,160 --> 00:13:41,360
in last we use the file tools

00:13:37,680 --> 00:13:44,560
to simulate the storage workload

00:13:41,360 --> 00:13:47,680
we would write a 14 gb

00:13:44,560 --> 00:13:52,160
to disk a we

00:13:47,680 --> 00:13:55,199
observed the distribution of

00:13:52,160 --> 00:13:59,360
vm time is power

00:13:55,199 --> 00:14:03,839
millisecond to 38 milliseconds

00:13:59,360 --> 00:14:06,880
based on the above tester router

00:14:03,839 --> 00:14:08,399
we find that the relationship between

00:14:06,880 --> 00:14:14,079
the dispute on time

00:14:08,399 --> 00:14:14,079
and the vm workload will not close

00:14:14,160 --> 00:14:19,839
okay that's how

00:14:17,519 --> 00:14:21,120
thank you for your list in my topic if

00:14:19,839 --> 00:14:24,720
you any

00:14:21,120 --> 00:14:31,839
question for this topic please contact

00:14:24,720 --> 00:14:31,839
me by this email thank you

00:14:32,959 --> 00:14:35,040

YouTube URL: https://www.youtube.com/watch?v=OLs8qeVXMLc


