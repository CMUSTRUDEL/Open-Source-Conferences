Title: Comparing Performance of NVMe Hard Drives in KVM, Baremetal, and Docker
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Comparing Performance of NVMe Hard Drives in KVM, Baremetal, and Docker Using Fio and SPDK for Virtual Testbed Applications - Mauricio Tavares, RENCI
Captions: 
	00:00:00,399 --> 00:00:03,600
hello there today i will be talking

00:00:03,040 --> 00:00:05,759
about

00:00:03,600 --> 00:00:08,960
hard drive performance testing on bare

00:00:05,759 --> 00:00:11,920
metal docker and kvm

00:00:08,960 --> 00:00:13,120
however this talk will not be about

00:00:11,920 --> 00:00:16,720
comparing

00:00:13,120 --> 00:00:19,840
say two drives run in some way or form

00:00:16,720 --> 00:00:19,840
on these platforms

00:00:20,960 --> 00:00:25,359
what i am interested on is scaling

00:00:23,600 --> 00:00:27,039
things up

00:00:25,359 --> 00:00:29,760
what happens when you add multiple

00:00:27,039 --> 00:00:32,640
drives per guest or container

00:00:29,760 --> 00:00:33,120
versus multiple guests in containers

00:00:32,640 --> 00:00:36,480
with

00:00:33,120 --> 00:00:39,520
one drive each how to

00:00:36,480 --> 00:00:43,600
investigate which tweaks we need

00:00:39,520 --> 00:00:43,600
to get the best performance

00:00:44,079 --> 00:00:50,800
that will require a lot of testing

00:00:47,840 --> 00:00:51,600
i don't know about you but i don't feel

00:00:50,800 --> 00:00:55,360
like building

00:00:51,600 --> 00:00:56,960
a task test box manually running out the

00:00:55,360 --> 00:01:01,039
tests

00:00:56,960 --> 00:01:03,680
collecting the data just so

00:01:01,039 --> 00:01:04,720
fine just so after all that i find out

00:01:03,680 --> 00:01:07,280
that i forgot

00:01:04,720 --> 00:01:09,600
something and need to start all over

00:01:07,280 --> 00:01:11,680
again

00:01:09,600 --> 00:01:13,200
i have done that and it's not a pretty

00:01:11,680 --> 00:01:17,119
sight

00:01:13,200 --> 00:01:21,360
there has to be a better way so

00:01:17,119 --> 00:01:21,360
how can i be lazy in my testing

00:01:23,680 --> 00:01:28,320
let's begin by defining the problem we

00:01:26,240 --> 00:01:31,520
are trying to solve

00:01:28,320 --> 00:01:34,799
in this case how to do a ton of hard

00:01:31,520 --> 00:01:34,799
drive performance tests

00:01:35,680 --> 00:01:39,520
knowing which tests we want to run is

00:01:38,079 --> 00:01:42,399
important

00:01:39,520 --> 00:01:43,520
there is probably some standardized

00:01:42,399 --> 00:01:46,240
standard

00:01:43,520 --> 00:01:46,240
we can use

00:01:47,840 --> 00:01:54,320
spending the time to find a way not

00:01:51,280 --> 00:01:55,600
to only how to make this testing as much

00:01:54,320 --> 00:01:59,119
as possible

00:01:55,600 --> 00:02:03,360
but also make it become rather

00:01:59,119 --> 00:02:07,360
platform agnostic will go a long way

00:02:03,360 --> 00:02:10,360
in helping us save time

00:02:07,360 --> 00:02:12,319
we should be focusing on the data

00:02:10,360 --> 00:02:15,520
interpretation

00:02:12,319 --> 00:02:19,360
and coming up with questions

00:02:15,520 --> 00:02:23,840
to be answered not on the actual

00:02:19,360 --> 00:02:23,840
running of the experiment

00:02:27,760 --> 00:02:33,280
by then i hope

00:02:31,040 --> 00:02:35,200
we will have found some kind of

00:02:33,280 --> 00:02:38,800
conclusion

00:02:35,200 --> 00:02:42,640
we still have time to make something up

00:02:38,800 --> 00:02:43,840
and finally we will use the remaining

00:02:42,640 --> 00:02:47,920
time

00:02:43,840 --> 00:02:47,920
to go over any questions you may have

00:02:49,120 --> 00:02:55,200
warning my employee has no idea

00:02:52,319 --> 00:02:57,440
that i'm talking here really don't blame

00:02:55,200 --> 00:02:57,440
them

00:02:58,959 --> 00:03:04,159
this presentation is not as technical as

00:03:02,159 --> 00:03:07,680
most of the other talks

00:03:04,159 --> 00:03:11,920
we'll be focusing on the process

00:03:07,680 --> 00:03:15,920
not on the or not on the coding

00:03:11,920 --> 00:03:15,920
but there will be lots of pretty

00:03:16,840 --> 00:03:20,640
pictures

00:03:18,159 --> 00:03:21,280
over here we have researchers who need

00:03:20,640 --> 00:03:24,159
to get

00:03:21,280 --> 00:03:27,040
as close to the wire speed on devices

00:03:24,159 --> 00:03:29,680
they are working with

00:03:27,040 --> 00:03:29,680
gpus

00:03:30,720 --> 00:03:33,840
hard drives

00:03:34,480 --> 00:03:42,159
and network cards as possible

00:03:38,480 --> 00:03:46,319
the as close 2 is

00:03:42,159 --> 00:03:49,280
the name of the game ideally would

00:03:46,319 --> 00:03:50,879
provide each researcher with enough

00:03:49,280 --> 00:03:53,920
better metal servers

00:03:50,879 --> 00:03:57,519
to perform the experiment and

00:03:53,920 --> 00:04:00,159
insanely fast communication between them

00:03:57,519 --> 00:04:02,640
a group that does the enough better

00:04:00,159 --> 00:04:06,799
metal service part

00:04:02,640 --> 00:04:06,799
is the chameleon cloud

00:04:07,280 --> 00:04:14,640
but we do not have infinite resources to

00:04:11,360 --> 00:04:18,000
throw at the problem

00:04:14,640 --> 00:04:21,280
instead we can cram

00:04:18,000 --> 00:04:25,040
a bunch of these devices

00:04:21,280 --> 00:04:27,919
in as few servers as possible

00:04:25,040 --> 00:04:29,600
and then come up with some way for the

00:04:27,919 --> 00:04:33,919
researchers

00:04:29,600 --> 00:04:33,919
to fetch a device they want

00:04:35,040 --> 00:04:40,800
the problem with abstracting hardware

00:04:38,240 --> 00:04:40,800
is

00:04:41,440 --> 00:04:44,479
when using some kind of virtualization

00:04:43,520 --> 00:04:46,639
is that

00:04:44,479 --> 00:04:48,000
something is always lost in the

00:04:46,639 --> 00:04:53,199
transaction

00:04:48,000 --> 00:04:53,199
or translation how much

00:04:53,919 --> 00:05:00,880
that is the question which ties back to

00:04:57,360 --> 00:05:06,080
the as close to comment we had

00:05:00,880 --> 00:05:10,160
before to show what i mean

00:05:06,080 --> 00:05:13,759
let's talk about three different ways to

00:05:10,160 --> 00:05:18,160
pass a network card to a vm guest

00:05:13,759 --> 00:05:18,160
or a darker container

00:05:18,960 --> 00:05:25,360
we can pass a virtual or scale down

00:05:23,520 --> 00:05:27,120
how much scaled down it is really

00:05:25,360 --> 00:05:30,160
depends on the

00:05:27,120 --> 00:05:34,560
driver that's doing the emulation

00:05:30,160 --> 00:05:38,479
card to multiple guests

00:05:34,560 --> 00:05:41,759
out of the same physical card

00:05:38,479 --> 00:05:45,360
this method is called

00:05:41,759 --> 00:05:47,759
para virtualization it uses a lot of

00:05:45,360 --> 00:05:52,000
resources to abstract it

00:05:47,759 --> 00:05:55,600
at the hypervisor or

00:05:52,000 --> 00:06:00,240
container server level

00:05:55,600 --> 00:06:03,039
and is as a result

00:06:00,240 --> 00:06:03,039
the slowest

00:06:03,280 --> 00:06:10,080
and on the top of that it does not

00:06:06,319 --> 00:06:13,360
expose much of the original card

00:06:10,080 --> 00:06:16,880
most people are fine with that

00:06:13,360 --> 00:06:21,840
but since we are doing research

00:06:16,880 --> 00:06:21,840
that is not good enough

00:06:22,000 --> 00:06:30,560
other cards have srrov

00:06:26,560 --> 00:06:35,440
which allows us to create

00:06:30,560 --> 00:06:38,960
fake cards at the card itself

00:06:35,440 --> 00:06:42,160
and then hand them off as before

00:06:38,960 --> 00:06:45,440
so in this case the card is really

00:06:42,160 --> 00:06:49,280
a virtual environment

00:06:45,440 --> 00:06:55,520
a mini virtual server

00:06:49,280 --> 00:06:58,880
but it only serves partitions of itself

00:06:55,520 --> 00:06:59,919
a classical example of that is a gpu

00:06:58,880 --> 00:07:03,840
card

00:06:59,919 --> 00:07:03,840
and some network cards

00:07:04,160 --> 00:07:12,720
as such it reveals more about itself

00:07:09,360 --> 00:07:16,960
there are such cards that allow

00:07:12,720 --> 00:07:19,599
each virtual device

00:07:16,960 --> 00:07:22,479
in this mode should be programmed up to

00:07:19,599 --> 00:07:22,479
a certain point

00:07:23,520 --> 00:07:27,680
then we have pci passthrough

00:07:28,240 --> 00:07:31,680
the speed difference between pci

00:07:30,400 --> 00:07:34,960
passthrough and sr

00:07:31,680 --> 00:07:40,319
rov needs to be checked

00:07:34,960 --> 00:07:44,000
for our context but it tends to be

00:07:40,319 --> 00:07:46,720
slightly faster

00:07:44,000 --> 00:07:48,160
however we plan on completely

00:07:46,720 --> 00:07:51,360
reprogramming

00:07:48,160 --> 00:07:55,840
at the very least of network cards

00:07:51,360 --> 00:07:55,840
and that includes changing its firmware

00:07:56,000 --> 00:08:04,400
how much is that how much of that

00:07:59,599 --> 00:08:07,840
is available through srov i don't know

00:08:04,400 --> 00:08:10,840
for now it's safer to just be able to

00:08:07,840 --> 00:08:13,440
hand over the entire card to the

00:08:10,840 --> 00:08:17,919
researcher

00:08:13,440 --> 00:08:17,919
and in such a way that

00:08:18,160 --> 00:08:21,680
the researcher has full control of

00:08:20,960 --> 00:08:26,639
within

00:08:21,680 --> 00:08:29,759
the vm guest

00:08:26,639 --> 00:08:33,680
for our case we are very interested

00:08:29,759 --> 00:08:38,159
on the pci passthrough in general

00:08:33,680 --> 00:08:42,719
but we will consider investigating

00:08:38,159 --> 00:08:46,720
other abstractions models

00:08:42,719 --> 00:08:46,720
to see if you can still perform

00:08:46,880 --> 00:08:53,279
we can still achieve the same

00:08:48,640 --> 00:08:53,279
performance and configurability

00:08:53,920 --> 00:08:59,279
that we can get by just handing over the

00:08:56,480 --> 00:08:59,279
entire card

00:09:01,760 --> 00:09:09,839
with that said some of you have realized

00:09:04,720 --> 00:09:09,839
there's something missing here

00:09:10,040 --> 00:09:15,040
pcie version 4.

00:09:13,360 --> 00:09:16,720
i really would like to be talking about

00:09:15,040 --> 00:09:22,160
a pci e5

00:09:16,720 --> 00:09:25,680
but as we know not just here

00:09:22,160 --> 00:09:26,880
anyway pcie v4 can provide the same

00:09:25,680 --> 00:09:32,399
bandwidth

00:09:26,880 --> 00:09:35,760
of pci v3 using half of the lanes

00:09:32,399 --> 00:09:38,480
for the sake of compatibility many cards

00:09:35,760 --> 00:09:42,959
scale down their bandwidth usage

00:09:38,480 --> 00:09:46,160
to match they slot they are in

00:09:42,959 --> 00:09:50,000
it may not sound that important but

00:09:46,160 --> 00:09:52,399
when you see a 100 gigabit millinox

00:09:50,000 --> 00:09:52,399
card

00:09:54,160 --> 00:10:02,959
not uh uh not able to

00:09:59,440 --> 00:10:06,240
get full speed on using one pci

00:10:02,959 --> 00:10:10,079
e3 slot in fact

00:10:06,240 --> 00:10:13,600
having to use two pci 3 slots to

00:10:10,079 --> 00:10:17,040
achieve its full 100 gigabit speed

00:10:13,600 --> 00:10:20,399
when if it was connected to a pcie

00:10:17,040 --> 00:10:24,399
v4 slot we do not

00:10:20,399 --> 00:10:24,399
need this auxiliary card

00:10:24,800 --> 00:10:28,720
you know it really starts to add up

00:10:27,680 --> 00:10:31,839
especially

00:10:28,720 --> 00:10:34,399
if you have a limit on the number of

00:10:31,839 --> 00:10:34,399
slots

00:10:35,600 --> 00:10:44,320
and then there's numa

00:10:38,880 --> 00:10:44,320
which allows us to say textures

00:10:44,839 --> 00:10:50,720
choose cpu server with a few

00:10:48,240 --> 00:10:51,440
some memory and a few pci slots with

00:10:50,720 --> 00:10:54,640
some

00:10:51,440 --> 00:10:54,640
nice orange cards

00:10:56,560 --> 00:11:04,959
and then break it apart

00:11:02,240 --> 00:11:07,760
break the cpus apart to create

00:11:04,959 --> 00:11:10,880
individual computers

00:11:07,760 --> 00:11:16,079
which we really call them pneuma nodes

00:11:10,880 --> 00:11:20,240
with their own pci and memory slots

00:11:16,079 --> 00:11:20,240
and these slots can be accessed

00:11:21,519 --> 00:11:28,560
faster by the cpu you know if the cpu

00:11:25,519 --> 00:11:31,839
if they are inside the same numera node

00:11:28,560 --> 00:11:34,320
the cpu can access the

00:11:31,839 --> 00:11:35,519
those pci slots and the memory slot

00:11:34,320 --> 00:11:39,279
faster than

00:11:35,519 --> 00:11:42,880
if it tries to access

00:11:39,279 --> 00:11:44,079
a pci slot that is somewhere else in

00:11:42,880 --> 00:11:47,279
some other new

00:11:44,079 --> 00:11:50,720
node the

00:11:47,279 --> 00:11:53,839
bottom line here is that

00:11:50,720 --> 00:11:57,040
this direct access can lead to

00:11:53,839 --> 00:12:00,000
faster vm guests

00:11:57,040 --> 00:12:01,360
and by fast i mean we start getting

00:12:00,000 --> 00:12:03,760
close to bare metal

00:12:01,360 --> 00:12:03,760
speed

00:12:04,639 --> 00:12:08,480
as other talks on this conference will

00:12:07,120 --> 00:12:12,160
show

00:12:08,480 --> 00:12:15,519
there is also a lot of work in the numa

00:12:12,160 --> 00:12:20,000
domain being done by amd

00:12:15,519 --> 00:12:20,000
you really should not miss those talks

00:12:23,440 --> 00:12:30,079
now there are standards for testing uh

00:12:26,959 --> 00:12:34,320
gpus network cards hard drives

00:12:30,079 --> 00:12:34,880
and so on for this presentation we'll

00:12:34,320 --> 00:12:39,360
focus

00:12:34,880 --> 00:12:43,360
on nvme hard drives but the concepts

00:12:39,360 --> 00:12:43,360
really apply for every for the others

00:12:45,600 --> 00:12:50,720
the standard protocol you use is the

00:12:51,519 --> 00:12:59,279
storage network industry association

00:12:54,800 --> 00:13:02,560
salt network storage performance testing

00:12:59,279 --> 00:13:07,200
specification version 2.01 from

00:13:02,560 --> 00:13:11,760
february 2018. yes it's a more full

00:13:07,200 --> 00:13:14,160
but what does it mean it describes

00:13:11,760 --> 00:13:15,120
the process of testing performance

00:13:14,160 --> 00:13:17,839
testing

00:13:15,120 --> 00:13:18,880
hard drives solid state hard drives

00:13:17,839 --> 00:13:21,760
which includes

00:13:18,880 --> 00:13:22,880
how to pre-condition the hard drive so

00:13:21,760 --> 00:13:25,360
it doesn't behave

00:13:22,880 --> 00:13:26,800
like a hard drive that's fresh out of

00:13:25,360 --> 00:13:30,560
the box

00:13:26,800 --> 00:13:33,600
which should look fast and insane

00:13:30,560 --> 00:13:36,160
but that's inaccurate

00:13:33,600 --> 00:13:38,160
in real life and that would skew the

00:13:36,160 --> 00:13:41,440
results

00:13:38,160 --> 00:13:45,440
it will also deter specify

00:13:41,440 --> 00:13:48,480
which sample range to connect

00:13:45,440 --> 00:13:52,000
if you're doing the testing

00:13:48,480 --> 00:13:53,839
the in the beginning we on the region

00:13:52,000 --> 00:13:54,800
that hard drives might start still

00:13:53,839 --> 00:14:00,320
behaving

00:13:54,800 --> 00:14:00,320
as new hard drives so the

00:14:00,880 --> 00:14:07,600
built-in optimization algorithm

00:14:04,480 --> 00:14:10,880
is working full speed but

00:14:07,600 --> 00:14:14,399
then we are going to start reaching a

00:14:10,880 --> 00:14:14,399
a region where it's

00:14:15,040 --> 00:14:20,240
the it doesn't affect as much because

00:14:18,399 --> 00:14:22,079
there's so much data that had to go

00:14:20,240 --> 00:14:25,279
through it

00:14:22,079 --> 00:14:28,079
that it's now

00:14:25,279 --> 00:14:31,839
finally reaching the same behavior as a

00:14:28,079 --> 00:14:35,279
hard drive has been used

00:14:31,839 --> 00:14:39,120
24 hours every day

00:14:35,279 --> 00:14:39,120
would have

00:14:39,199 --> 00:14:46,720
and after that

00:14:43,600 --> 00:14:49,600
then there is a point that we call the

00:14:46,720 --> 00:14:49,600
steady state

00:14:49,680 --> 00:14:54,000
and that's where we actually want to

00:14:51,760 --> 00:14:57,279
take our readings

00:14:54,000 --> 00:15:01,760
and that is defined when the

00:14:57,279 --> 00:15:01,760
average readings do not change much

00:15:03,040 --> 00:15:12,320
in the experiments we have run here

00:15:07,199 --> 00:15:14,800
that use achieved close to a day of

00:15:12,320 --> 00:15:14,800
testing

00:15:15,519 --> 00:15:22,880
this standard also define which

00:15:19,519 --> 00:15:26,399
tests to perform specifically

00:15:22,880 --> 00:15:30,160
we're talking about bandwidth iops

00:15:26,399 --> 00:15:33,199
i o operations per second latency

00:15:30,160 --> 00:15:33,600
latency and each of them has specific

00:15:33,199 --> 00:15:36,639
read

00:15:33,600 --> 00:15:40,079
write ratios and block sizes

00:15:36,639 --> 00:15:42,320
for that specific test

00:15:40,079 --> 00:15:43,759
for instance iops we are going to be

00:15:42,320 --> 00:15:47,199
doing a

00:15:43,759 --> 00:15:49,519
random read write ratios that

00:15:47,199 --> 00:15:51,040
can be like a hundred to zero 100

00:15:49,519 --> 00:15:55,920
percent reads zero

00:15:51,040 --> 00:15:58,880
uh right 95 5 65 35

00:15:55,920 --> 00:15:59,759
and then and then turns back to 35 65

00:15:58,880 --> 00:16:03,519
00:15:59,759 --> 00:16:06,560
and 0 100. the block size can be

00:16:03,519 --> 00:16:11,759
from i can start from a half a k

00:16:06,560 --> 00:16:11,759
a kilobyte to 1024 kilobytes

00:16:16,880 --> 00:16:23,839
now let's talk about the tools

00:16:20,480 --> 00:16:25,759
that we're going to be using first

00:16:23,839 --> 00:16:27,440
one is going to be the flexible io

00:16:25,759 --> 00:16:30,079
tester

00:16:27,440 --> 00:16:30,480
which is a multi-threaded i o generator

00:16:30,079 --> 00:16:34,160
tool

00:16:30,480 --> 00:16:34,720
used for as its name indicate testing a

00:16:34,160 --> 00:16:39,839
give

00:16:34,720 --> 00:16:42,480
and workload it's extremely customizable

00:16:39,839 --> 00:16:44,720
and i personally consider one of the

00:16:42,480 --> 00:16:47,519
best

00:16:44,720 --> 00:16:48,079
testing tools out there then there's

00:16:47,519 --> 00:16:52,560
also

00:16:48,079 --> 00:16:54,320
the storage performance development kit

00:16:52,560 --> 00:16:56,480
which is really a set of tools and

00:16:54,320 --> 00:16:59,040
libraries to write high performance

00:16:56,480 --> 00:17:02,560
scalable

00:16:59,040 --> 00:17:05,360
user mode storage applications

00:17:02,560 --> 00:17:06,559
so what we're really talking about is

00:17:05,360 --> 00:17:09,679
doing a

00:17:06,559 --> 00:17:13,120
zero copy highly parallel access

00:17:09,679 --> 00:17:16,799
directed to a ssd

00:17:13,120 --> 00:17:16,799
at the user space level

00:17:16,839 --> 00:17:23,839
or drivers that we build

00:17:21,120 --> 00:17:25,839
using this they bought you know really

00:17:23,839 --> 00:17:28,960
they should be

00:17:25,839 --> 00:17:33,120
allowed us to get much faster

00:17:28,960 --> 00:17:36,480
storage how fast that is compared to

00:17:33,120 --> 00:17:39,440
other methods that's really of course a

00:17:36,480 --> 00:17:39,440
method of testing

00:17:39,919 --> 00:17:45,840
now spdk can be used in conjunction with

00:17:43,200 --> 00:17:49,280
fio

00:17:45,840 --> 00:17:52,640
test device using a testing device

00:17:49,280 --> 00:17:56,320
that use spdk driver

00:17:52,640 --> 00:18:00,640
or use its own

00:17:56,320 --> 00:18:04,080
performance testing tool

00:18:00,640 --> 00:18:07,440
and finally we have some custom scripts

00:18:04,080 --> 00:18:10,559
that we created to make our life easier

00:18:07,440 --> 00:18:11,440
one is just to find all the nvme hard

00:18:10,559 --> 00:18:15,440
drives

00:18:11,440 --> 00:18:20,720
that are available in a given computer

00:18:15,440 --> 00:18:24,320
and the order acts as a wrapper

00:18:20,720 --> 00:18:26,960
for fio and spdk

00:18:24,320 --> 00:18:29,360
and deals without that creating the

00:18:26,960 --> 00:18:33,440
laundry list for tests

00:18:29,360 --> 00:18:35,840
to be fed and then running the tasks

00:18:33,440 --> 00:18:36,480
saving out the output in some kind of

00:18:35,840 --> 00:18:38,799
format

00:18:36,480 --> 00:18:42,400
that's convenient for us to use in this

00:18:38,799 --> 00:18:45,760
case we chose a comma separated

00:18:42,400 --> 00:18:48,960
files which then

00:18:45,760 --> 00:18:52,000
we can pass to other scripts

00:18:48,960 --> 00:18:57,840
to look for interesting data

00:18:52,000 --> 00:19:01,280
and make pretty graphs

00:18:57,840 --> 00:19:04,880
the host that we've

00:19:01,280 --> 00:19:08,160
been using for testing this is a

00:19:04,880 --> 00:19:09,280
old super micro box which also doubles

00:19:08,160 --> 00:19:13,440
as my

00:19:09,280 --> 00:19:13,440
3d printing filament dryer

00:19:13,600 --> 00:19:20,960
it has 48 pcie v3

00:19:17,039 --> 00:19:24,400
lanes and

00:19:20,960 --> 00:19:29,039
i also do intel

00:19:24,400 --> 00:19:32,480
cpus each of those with 18 cores each

00:19:29,039 --> 00:19:36,320
and the wage setup each

00:19:32,480 --> 00:19:40,400
single cpu is a independent pneuma node

00:19:36,320 --> 00:19:44,080
so we really have only two pneuma nodes

00:19:40,400 --> 00:19:47,520
one that takes the course from 0 to 17

00:19:44,080 --> 00:19:53,840
and the order that takes

00:19:47,520 --> 00:19:53,840
18 to 35.

00:19:54,480 --> 00:20:00,240
also there we have the intel nvme hard

00:19:58,160 --> 00:20:02,320
drives

00:20:00,240 --> 00:20:03,280
and the ones we are using on this

00:20:02,320 --> 00:20:06,320
machine

00:20:03,280 --> 00:20:08,080
they go on the front that's all those

00:20:06,320 --> 00:20:11,600
drive bays are for

00:20:08,080 --> 00:20:14,240
and they are using the u.2 interface

00:20:11,600 --> 00:20:15,360
which means as the picture shows on the

00:20:14,240 --> 00:20:18,799
bottom

00:20:15,360 --> 00:20:20,640
they look like standard sata connectors

00:20:18,799 --> 00:20:23,360
but they have a few little pins

00:20:20,640 --> 00:20:23,360
in the middle

00:20:24,240 --> 00:20:30,640
the problem here is that all of those

00:20:27,440 --> 00:20:31,280
10 hard drives that we have are attached

00:20:30,640 --> 00:20:35,280
to one

00:20:31,280 --> 00:20:37,679
single controller which is in numa node

00:20:35,280 --> 00:20:37,679
zero

00:20:38,880 --> 00:20:43,520
so there will be really no performance

00:20:41,679 --> 00:20:46,960
gains to be had

00:20:43,520 --> 00:20:48,720
by running uh the different gas

00:20:46,960 --> 00:20:51,200
containers in separate

00:20:48,720 --> 00:20:55,280
numa nodes if you want any performance

00:20:51,200 --> 00:20:55,280
we have to put them on numa node zero

00:20:56,320 --> 00:21:03,120
out of curiosity jesus

00:20:59,919 --> 00:21:06,159
uh how we

00:21:03,120 --> 00:21:07,520
looked for the list you know where each

00:21:06,159 --> 00:21:10,720
of those hard drives

00:21:07,520 --> 00:21:13,120
was in which numa node it was

00:21:10,720 --> 00:21:15,120
we used a fine drive script that we

00:21:13,120 --> 00:21:19,280
mentioned before

00:21:15,120 --> 00:21:21,760
and a bit of help of

00:21:19,280 --> 00:21:21,760
verse

00:21:26,960 --> 00:21:31,840
now that we know the players let's see

00:21:29,200 --> 00:21:34,000
if we can grasp

00:21:31,840 --> 00:21:36,320
what we really want to accomplish with

00:21:34,000 --> 00:21:40,240
them

00:21:36,320 --> 00:21:40,240
what we are proposing is to compare

00:21:41,600 --> 00:21:49,600
file by itself with with a

00:21:45,280 --> 00:21:54,559
lib io maybe file in spdk

00:21:49,600 --> 00:21:54,559
and maybe even spdk it by itself

00:21:55,280 --> 00:22:00,720
and that's not that's even not counting

00:21:58,799 --> 00:22:04,799
all the optimizations we can do

00:22:00,720 --> 00:22:09,120
by just messing with

00:22:04,799 --> 00:22:11,280
uh spdk announce the optimizations we

00:22:09,120 --> 00:22:12,640
are going to be doing by using a virtual

00:22:11,280 --> 00:22:15,919
device a virtual

00:22:12,640 --> 00:22:20,159
host we also have

00:22:15,919 --> 00:22:22,320
10 hard drives they are exact the same

00:22:20,159 --> 00:22:24,159
makeup model

00:22:22,320 --> 00:22:26,640
you know be nice to have different ones

00:22:24,159 --> 00:22:31,760
but different models

00:22:26,640 --> 00:22:31,760
but the thing is how similar they are

00:22:32,080 --> 00:22:36,559
if you run the same test on each of

00:22:34,559 --> 00:22:38,840
those hard drives would i get the exact

00:22:36,559 --> 00:22:41,520
same result or it's going to be some

00:22:38,840 --> 00:22:45,440
variation

00:22:41,520 --> 00:22:48,720
and then we have if you're going to do

00:22:45,440 --> 00:22:49,840
the iops test we have eight different

00:22:48,720 --> 00:22:53,039
block size

00:22:49,840 --> 00:22:56,720
1024k 128k

00:22:53,039 --> 00:23:00,240
64k 32 16 8 4

00:22:56,720 --> 00:23:00,240
and then half a k

00:23:01,120 --> 00:23:05,840
and then we have the seven different

00:23:04,159 --> 00:23:06,960
read write ratios that we mentioned

00:23:05,840 --> 00:23:10,720
before

00:23:06,960 --> 00:23:16,000
and that's not even counting sequential

00:23:10,720 --> 00:23:16,000
uh read writes versus random read writes

00:23:17,200 --> 00:23:24,559
and then we have to uh

00:23:21,919 --> 00:23:26,480
see the effects which we really can't

00:23:24,559 --> 00:23:28,880
test on this machine but

00:23:26,480 --> 00:23:30,240
they are available they are there in

00:23:28,880 --> 00:23:33,919
principle

00:23:30,240 --> 00:23:35,840
about uh playing uh which kind of uh

00:23:33,919 --> 00:23:37,120
memory we are setting up our virtual

00:23:35,840 --> 00:23:40,720
machines

00:23:37,120 --> 00:23:43,919
the number of cpus no more nodes

00:23:40,720 --> 00:23:46,080
and so on

00:23:43,919 --> 00:23:47,679
i would like to take uh to calculate the

00:23:46,080 --> 00:23:51,760
possible permutations but

00:23:47,679 --> 00:23:54,799
it's a lot like thousands

00:23:51,760 --> 00:23:59,679
you know using the fio test

00:23:54,799 --> 00:24:01,360
we can cut down some of those tests

00:23:59,679 --> 00:24:02,799
but you still have to manually start

00:24:01,360 --> 00:24:04,799
them

00:24:02,799 --> 00:24:08,480
and they still have to be configured and

00:24:04,799 --> 00:24:08,480
you still have to process the data

00:24:08,799 --> 00:24:15,919
i tried to generate a file task

00:24:12,320 --> 00:24:17,679
config files before for a few runs was

00:24:15,919 --> 00:24:19,840
fine but

00:24:17,679 --> 00:24:21,360
it soon became very easy to make

00:24:19,840 --> 00:24:24,720
mistakes

00:24:21,360 --> 00:24:28,000
in fact i made a lot

00:24:24,720 --> 00:24:32,640
and that was bad because

00:24:28,000 --> 00:24:35,679
i had to

00:24:32,640 --> 00:24:38,960
redo them and sometimes find out what

00:24:35,679 --> 00:24:42,080
was wrong and then redo them

00:24:38,960 --> 00:24:42,400
and that means since those tests they

00:24:42,080 --> 00:24:45,760
take

00:24:42,400 --> 00:24:49,360
hours upon hours that means i lost

00:24:45,760 --> 00:24:54,080
not only ours but days in the

00:24:49,360 --> 00:24:54,080
process i think we can do better

00:24:56,559 --> 00:25:05,120
so as i mentioned before

00:25:01,919 --> 00:25:06,960
we wrote a script that takes

00:25:05,120 --> 00:25:08,640
the parameters that are used to create

00:25:06,960 --> 00:25:12,080
the config files for

00:25:08,640 --> 00:25:15,919
fions pdk creates those files

00:25:12,080 --> 00:25:19,440
run the tests and for each of

00:25:15,919 --> 00:25:19,440
uh those tests

00:25:19,760 --> 00:25:22,000
we

00:25:23,360 --> 00:25:29,840
then process uh we collect the data

00:25:26,640 --> 00:25:33,360
collect the data not only just in a

00:25:29,840 --> 00:25:36,159
csv format but also in a file name

00:25:33,360 --> 00:25:38,880
format that helps us identify which run

00:25:36,159 --> 00:25:38,880
it came from

00:25:39,039 --> 00:25:45,039
and we actually do save

00:25:42,559 --> 00:25:48,080
the config files that were created to

00:25:45,039 --> 00:25:48,080
run those tests

00:25:48,880 --> 00:25:51,279
and

00:25:52,799 --> 00:25:59,200
that means that you know we can go back

00:25:56,000 --> 00:26:00,400
to them not only to use for reference

00:25:59,200 --> 00:26:04,640
for documentation but

00:26:00,400 --> 00:26:08,400
also to reproduce any specific test

00:26:04,640 --> 00:26:12,640
because of this automation

00:26:08,400 --> 00:26:15,600
you know we really you know

00:26:12,640 --> 00:26:17,279
we decide why just running some tests

00:26:15,600 --> 00:26:17,600
for like you know bandwidth some tests

00:26:17,279 --> 00:26:22,720
for

00:26:17,600 --> 00:26:22,720
iops why not just run all of them

00:26:23,360 --> 00:26:30,080
that will cover all the snia

00:26:26,400 --> 00:26:32,480
test requirements it's easy to drop the

00:26:30,080 --> 00:26:32,480
data

00:26:32,559 --> 00:26:36,840
because you know by that i mean it's

00:26:34,960 --> 00:26:38,559
it's always easy to drop data you don't

00:26:36,840 --> 00:26:41,840
need

00:26:38,559 --> 00:26:42,720
but you collect it then it is to to try

00:26:41,840 --> 00:26:45,919
to

00:26:42,720 --> 00:26:48,400
get data that you need but do not

00:26:45,919 --> 00:26:48,400
collect

00:26:48,799 --> 00:26:56,640
you know if this takes like you know

00:26:52,320 --> 00:26:58,960
an extra day or so who cares

00:26:56,640 --> 00:26:59,919
we can automate it we can line them up

00:26:58,960 --> 00:27:02,320
as soon as one

00:26:59,919 --> 00:27:04,480
batch of tests starts uh ends another

00:27:02,320 --> 00:27:06,799
one starts and you keep doing that

00:27:04,480 --> 00:27:07,679
and why that's doing we can we can get

00:27:06,799 --> 00:27:09,760
the data

00:27:07,679 --> 00:27:10,720
we can process it and then plan the next

00:27:09,760 --> 00:27:14,640
batch

00:27:10,720 --> 00:27:14,640
you know automation is good

00:27:15,520 --> 00:27:21,840
now here's an example of uh how that

00:27:20,240 --> 00:27:25,760
torture test

00:27:21,840 --> 00:27:29,760
script creates the config file

00:27:25,760 --> 00:27:35,840
for children with fio

00:27:29,760 --> 00:27:35,840
in this case it is a

00:27:36,720 --> 00:27:42,240
you know it it's set up to run to do a

00:27:39,200 --> 00:27:46,480
to run fio with libio

00:27:42,240 --> 00:27:48,559
it's set up to run up to 24 hours

00:27:46,480 --> 00:27:51,039
but if you see that steady state

00:27:48,559 --> 00:27:54,799
statement

00:27:51,039 --> 00:27:58,000
it will uh stop

00:27:54,799 --> 00:28:01,360
running the test when the

00:27:58,000 --> 00:28:03,200
iops slope is less than three percent

00:28:01,360 --> 00:28:06,960
for a period of 1800

00:28:03,200 --> 00:28:09,360
seconds and usually so far

00:28:06,960 --> 00:28:12,159
that has always been before the 24

00:28:09,360 --> 00:28:15,279
window ended

00:28:12,159 --> 00:28:17,840
and for every single run that

00:28:15,279 --> 00:28:17,840
torture

00:28:20,640 --> 00:28:26,480
my little torture drive script will

00:28:23,200 --> 00:28:26,480
create a file like that

00:28:27,440 --> 00:28:32,080
note also the file name that is

00:28:29,840 --> 00:28:35,600
identifies that

00:28:32,080 --> 00:28:36,159
is the the name of the device which test

00:28:35,600 --> 00:28:39,200
we're doing

00:28:36,159 --> 00:28:39,200
and when we started

00:28:40,960 --> 00:28:46,880
we also use enspo to build

00:28:44,480 --> 00:28:46,880
things

00:28:47,520 --> 00:28:53,760
the playbook we have two playbooks

00:28:50,720 --> 00:28:56,799
actually one is to build the

00:28:53,760 --> 00:29:01,600
bare metal server

00:28:56,799 --> 00:29:01,600
and specifically installs docker and kvm

00:29:02,960 --> 00:29:06,000
and those little arrows that you're

00:29:05,200 --> 00:29:09,840
seeing there

00:29:06,000 --> 00:29:13,120
they actually represent task files

00:29:09,840 --> 00:29:15,440
that are run by the docker uh

00:29:13,120 --> 00:29:19,520
container they do not they're not the

00:29:15,440 --> 00:29:19,520
docker but but the ansible playbook

00:29:20,320 --> 00:29:24,240
i like to keep them separate because

00:29:22,480 --> 00:29:27,760
that allows me to use

00:29:24,240 --> 00:29:31,919
uh them for other tasks

00:29:27,760 --> 00:29:33,919
in fact the install docker

00:29:31,919 --> 00:29:35,440
task i'm going to be using later on

00:29:33,919 --> 00:29:41,039
today

00:29:35,440 --> 00:29:44,320
for another project

00:29:41,039 --> 00:29:48,000
we also have a playbook to install the

00:29:44,320 --> 00:29:48,000
testing software itself

00:29:48,399 --> 00:29:56,559
i really can use an ultry

00:29:51,679 --> 00:29:59,520
barmero kvm darker kinds of text boxes

00:29:56,559 --> 00:30:01,200
in fact i have used display book on all

00:29:59,520 --> 00:30:04,880
three before

00:30:01,200 --> 00:30:08,000
the only reason i stopped using it on

00:30:04,880 --> 00:30:12,480
primarily darker and nowadays on

00:30:08,000 --> 00:30:12,480
kvm was

00:30:13,440 --> 00:30:19,520
speed specifically that

00:30:17,440 --> 00:30:20,720
i would have the image already ready to

00:30:19,520 --> 00:30:23,120
go without the

00:30:20,720 --> 00:30:26,000
package is installed and i can just

00:30:23,120 --> 00:30:26,000
start it and run

00:30:26,399 --> 00:30:33,039
second by doing that i don't have to

00:30:30,159 --> 00:30:34,159
run ssh inside either the docker

00:30:33,039 --> 00:30:37,440
container

00:30:34,159 --> 00:30:39,840
or the kvm

00:30:37,440 --> 00:30:39,840
guest

00:30:40,720 --> 00:30:47,120
and asphalt needs ssh to

00:30:44,399 --> 00:30:47,120
do its thing

00:30:48,840 --> 00:30:51,840
and

00:30:52,320 --> 00:31:00,399
the that doesn't stop me

00:30:55,840 --> 00:31:02,880
if i want to run the playbook again

00:31:00,399 --> 00:31:04,480
because the playbook will just go

00:31:02,880 --> 00:31:05,840
through it it's going to realize oh you

00:31:04,480 --> 00:31:08,799
already installed this package

00:31:05,840 --> 00:31:10,399
you already download the gs files your

00:31:08,799 --> 00:31:11,279
red compound and build them so you're

00:31:10,399 --> 00:31:15,039
good i don't

00:31:11,279 --> 00:31:17,440
need to up to change unless it has a

00:31:15,039 --> 00:31:18,080
version that's newer than the version

00:31:17,440 --> 00:31:20,559
that's already

00:31:18,080 --> 00:31:20,559
installed

00:31:22,000 --> 00:31:29,039
then we also have to

00:31:25,279 --> 00:31:29,039
talk about our test boxes

00:31:29,519 --> 00:31:31,760
the

00:31:35,039 --> 00:31:41,440
server we really have one that

00:31:38,559 --> 00:31:42,960
right now we only have that super micro

00:31:41,440 --> 00:31:45,440
so

00:31:42,960 --> 00:31:46,000
there is really no point of building a

00:31:45,440 --> 00:31:48,399
separate

00:31:46,000 --> 00:31:50,080
custom image just for it it's much

00:31:48,399 --> 00:31:53,360
easier just to

00:31:50,080 --> 00:31:55,840
put the machine do a basic install

00:31:53,360 --> 00:31:57,279
let answer build it and off we go

00:31:55,840 --> 00:31:59,519
theoretically i could

00:31:57,279 --> 00:32:02,799
do it you know create a custom machine

00:31:59,519 --> 00:32:07,360
and run it through pixyboot

00:32:02,799 --> 00:32:07,360
but for both dock and kvm

00:32:07,679 --> 00:32:13,600
i have a image

00:32:10,720 --> 00:32:16,960
you know as i mentioned earlier without

00:32:13,600 --> 00:32:16,960
the package that we need

00:32:17,440 --> 00:32:24,080
and once you run this

00:32:21,039 --> 00:32:27,440
with this image you start the

00:32:24,080 --> 00:32:29,039
vm guest or the container you give them

00:32:27,440 --> 00:32:32,559
the locations to start

00:32:29,039 --> 00:32:33,440
to install the data that will survive

00:32:32,559 --> 00:32:37,760
the container

00:32:33,440 --> 00:32:40,840
being gone by that i mean it's a storage

00:32:37,760 --> 00:32:43,760
it's a directory that's shared between

00:32:40,840 --> 00:32:47,360
the bare metal server

00:32:43,760 --> 00:32:50,960
and the docker or the kvm

00:32:47,360 --> 00:32:53,200
guest and that's where we're going to be

00:32:50,960 --> 00:32:57,840
storing the

00:32:53,200 --> 00:32:57,840
results from the experiments

00:32:59,039 --> 00:33:02,480
and then with and then we when we start

00:33:01,279 --> 00:33:08,559
the containers

00:33:02,480 --> 00:33:11,919
we we feed the hard drive the pci

00:33:08,559 --> 00:33:13,600
the pci base the not the pc well it's on

00:33:11,919 --> 00:33:16,320
pci but the

00:33:13,600 --> 00:33:16,799
using pci passthrough the nvme hard

00:33:16,320 --> 00:33:19,919
drive

00:33:16,799 --> 00:33:22,840
is is fed to either the docker

00:33:19,919 --> 00:33:25,840
container or the vm gas at the start

00:33:22,840 --> 00:33:29,440
time

00:33:25,840 --> 00:33:31,679
if those any of those guys they need to

00:33:29,440 --> 00:33:36,159
download something or access the network

00:33:31,679 --> 00:33:36,159
both on and netball networks

00:33:36,840 --> 00:33:41,200
and that means that you know if you

00:33:40,159 --> 00:33:43,760
really need to

00:33:41,200 --> 00:33:47,120
enable ssh we can do that and do some

00:33:43,760 --> 00:33:47,120
port forward and be done

00:33:48,480 --> 00:33:51,519
this is how you use an example how to

00:33:51,039 --> 00:33:55,039
run

00:33:51,519 --> 00:33:59,640
torture disk against

00:33:55,039 --> 00:34:02,960
one against three hard drives

00:33:59,640 --> 00:34:07,200
nvme two

00:34:02,960 --> 00:34:09,440
five and nine and

00:34:07,200 --> 00:34:10,879
running at all those different block

00:34:09,440 --> 00:34:16,159
sizes

00:34:10,879 --> 00:34:19,119
the mixed reads are the read write ratio

00:34:16,159 --> 00:34:20,560
and even though it just shows like 10 to

00:34:19,119 --> 00:34:25,679
zero it's actually

00:34:20,560 --> 00:34:25,679
actually test the all the way around too

00:34:27,280 --> 00:34:32,159
and the out uh the

00:34:30,480 --> 00:34:34,079
out the director is going to create

00:34:32,159 --> 00:34:37,679
store the log files is based

00:34:34,079 --> 00:34:40,399
on the that lib io name

00:34:37,679 --> 00:34:41,599
plus the name of the hard drive and plus

00:34:40,399 --> 00:34:44,879
the date

00:34:41,599 --> 00:34:48,480
as you saw also as before

00:34:44,879 --> 00:34:50,240
the time here it says that you know

00:34:48,480 --> 00:34:53,599
that's the limit how far it can go

00:34:50,240 --> 00:34:54,399
before it stops instead state it's set

00:34:53,599 --> 00:35:00,320
up to

00:34:54,399 --> 00:35:00,320
do the iop stat state that we mentioned

00:35:00,839 --> 00:35:05,520
before

00:35:02,400 --> 00:35:07,760
now if you uh

00:35:05,520 --> 00:35:08,960
if you're going to run for instance uh

00:35:07,760 --> 00:35:12,720
start uh you know

00:35:08,960 --> 00:35:15,200
creating a bunch of docker containers

00:35:12,720 --> 00:35:15,839
we can do that uh like as in this

00:35:15,200 --> 00:35:20,800
example

00:35:15,839 --> 00:35:20,800
by listing

00:35:20,960 --> 00:35:27,599
all the nvme hard drives that are found

00:35:24,720 --> 00:35:30,480
on this server using find drive and then

00:35:27,599 --> 00:35:34,079
looping over them

00:35:30,480 --> 00:35:35,839
and then we are going to uh create the

00:35:34,079 --> 00:35:37,359
direct on the fly the director is going

00:35:35,839 --> 00:35:40,079
to store the log files

00:35:37,359 --> 00:35:42,160
and then pass this directory to the

00:35:40,079 --> 00:35:44,720
docker container

00:35:42,160 --> 00:35:46,960
and the name of the device and let it do

00:35:44,720 --> 00:35:48,960
its thing

00:35:46,960 --> 00:35:50,000
this is very similar to what we would

00:35:48,960 --> 00:35:53,440
have done

00:35:50,000 --> 00:35:56,240
in kvm you know we still have to create

00:35:53,440 --> 00:35:59,680
an image download package here

00:35:56,240 --> 00:36:03,520
and everything else

00:35:59,680 --> 00:36:08,240
it just takes a bit longer oh it takes

00:36:03,520 --> 00:36:11,359
much longer than a darker container

00:36:08,240 --> 00:36:14,480
but it really doesn't matter because

00:36:11,359 --> 00:36:14,480
it's automated

00:36:18,160 --> 00:36:24,960
the following graphs show the

00:36:21,839 --> 00:36:30,160
results for running fio tests using

00:36:24,960 --> 00:36:30,160
libio on just one single drive

00:36:30,240 --> 00:36:34,079
remember when i said before that you

00:36:32,240 --> 00:36:36,160
know there's a lot of data and there's a

00:36:34,079 --> 00:36:39,119
lot of tests that we're going to do

00:36:36,160 --> 00:36:39,520
this is just one single drive and jesus

00:36:39,119 --> 00:36:42,880
just

00:36:39,520 --> 00:36:45,920
as you can see here is just doing the

00:36:42,880 --> 00:36:47,760
fio using libio and just not even the

00:36:45,920 --> 00:36:50,880
entire list

00:36:47,760 --> 00:36:54,400
in fact you know i did not uh

00:36:50,880 --> 00:36:57,599
show the iops

00:36:54,400 --> 00:36:58,480
and bandwidth acting node that we have

00:36:57,599 --> 00:37:02,320
iops yes

00:36:58,480 --> 00:37:05,359
it's a bandwidth latency in iops

00:37:02,320 --> 00:37:09,680
but we don't have the

00:37:05,359 --> 00:37:11,520
half of the tests are not here

00:37:09,680 --> 00:37:13,599
but i really like this kind of graph

00:37:11,520 --> 00:37:15,599
because you know they

00:37:13,599 --> 00:37:17,440
really show in this case you know you're

00:37:15,599 --> 00:37:20,720
comparing bear metal

00:37:17,440 --> 00:37:23,839
dark and kvm they

00:37:20,720 --> 00:37:26,320
really show to

00:37:23,839 --> 00:37:26,320
others

00:37:27,119 --> 00:37:34,079
how the the different outcomes that

00:37:30,160 --> 00:37:35,839
can be created using different settings

00:37:34,079 --> 00:37:37,440
yes we still can do like know the f

00:37:35,839 --> 00:37:39,280
after careful considering

00:37:37,440 --> 00:37:40,720
the answers provided by bone reading we

00:37:39,280 --> 00:37:43,200
conclude the best performance is

00:37:40,720 --> 00:37:45,520
achieved by setting the block size to x

00:37:43,200 --> 00:37:46,560
who chooses and why for the rest of the

00:37:45,520 --> 00:37:50,160
week

00:37:46,560 --> 00:37:53,440
you know but

00:37:50,160 --> 00:37:53,920
this is not really a hardcore technical

00:37:53,440 --> 00:37:56,640
talk

00:37:53,920 --> 00:37:58,560
and i'm not really concerned about

00:37:56,640 --> 00:38:01,119
setting that specific drive

00:37:58,560 --> 00:38:02,400
to the best performance and let it go

00:38:01,119 --> 00:38:03,359
i'm going to be testing on the drives

00:38:02,400 --> 00:38:04,800
i'm going to be testing all the

00:38:03,359 --> 00:38:06,160
situations i'm going to be adding other

00:38:04,800 --> 00:38:07,760
things so

00:38:06,160 --> 00:38:11,920
i always going to have more varia

00:38:07,760 --> 00:38:11,920
variants that i need to deal with

00:38:12,720 --> 00:38:17,040
but if you look carefully you'll note

00:38:15,520 --> 00:38:19,359
that you know the

00:38:17,040 --> 00:38:22,000
the charts and the access titles are not

00:38:19,359 --> 00:38:24,079
really consistent

00:38:22,000 --> 00:38:25,040
this happened because you know the way i

00:38:24,079 --> 00:38:28,800
made those

00:38:25,040 --> 00:38:30,880
uh graphs i literally imported the data

00:38:28,800 --> 00:38:32,880
into libreoffice and i tried to create

00:38:30,880 --> 00:38:34,800
the graphs

00:38:32,880 --> 00:38:36,720
so i really had to rely on my lack of

00:38:34,800 --> 00:38:37,599
typing skills instead of having some

00:38:36,720 --> 00:38:40,720
kind of no

00:38:37,599 --> 00:38:41,520
smart script that take the data do the

00:38:40,720 --> 00:38:45,680
average for each

00:38:41,520 --> 00:38:47,040
run for each test box or drive or do a

00:38:45,680 --> 00:38:50,000
combination whatever you want to do and

00:38:47,040 --> 00:38:50,000
then create the graphs

00:38:50,560 --> 00:38:56,560
next time i'm going to see how to create

00:38:54,480 --> 00:38:59,200
proper nice looking 3d graphs using

00:38:56,560 --> 00:39:01,040
inuplot

00:38:59,200 --> 00:39:02,720
and they look uh i want them to look

00:39:01,040 --> 00:39:05,119
good enough so i can put in a taco

00:39:02,720 --> 00:39:05,119
journal

00:39:05,680 --> 00:39:09,440
and this one shows similar tests but we

00:39:08,960 --> 00:39:15,440
are doing

00:39:09,440 --> 00:39:19,359
fio with sf with spdk

00:39:15,440 --> 00:39:22,720
and as we mentioned before we kind of

00:39:19,359 --> 00:39:27,599
expect an increase in iops and bandwidth

00:39:22,720 --> 00:39:29,359
and if you look at the axis

00:39:27,599 --> 00:39:33,200
the scales on the axis you're going to

00:39:29,359 --> 00:39:35,839
see as we had an increase

00:39:33,200 --> 00:39:37,599
i personally i kind of don't like that

00:39:35,839 --> 00:39:39,520
you know it's out scaling but

00:39:37,599 --> 00:39:40,640
i don't know what to do about that maybe

00:39:39,520 --> 00:39:42,320
when i start using

00:39:40,640 --> 00:39:45,920
the plot i can control that so you

00:39:42,320 --> 00:39:45,920
actually can see a difference

00:39:46,560 --> 00:39:49,119
and

00:39:49,599 --> 00:39:55,680
there's by the way an intel uh

00:39:52,640 --> 00:39:57,599
article you know from intel the company

00:39:55,680 --> 00:39:58,880
and i'll put on the end of the slide act

00:39:57,599 --> 00:40:01,920
that states that the

00:39:58,880 --> 00:40:02,720
performance obtained using fio plus

00:40:01,920 --> 00:40:08,079
libio

00:40:02,720 --> 00:40:11,680
is less than that with fio plus fcp spdk

00:40:08,079 --> 00:40:15,119
and which is less than running the spdk

00:40:11,680 --> 00:40:15,119
performance tool by itself

00:40:15,200 --> 00:40:20,960
the you know i could have shown the

00:40:18,640 --> 00:40:23,520
other drives but i'm not going to

00:40:20,960 --> 00:40:24,160
because you know i have not automated

00:40:23,520 --> 00:40:26,640
the

00:40:24,160 --> 00:40:29,359
plotting part and i'm not really going

00:40:26,640 --> 00:40:32,400
to do this manually

00:40:29,359 --> 00:40:33,839
no thank you and now so you know now

00:40:32,400 --> 00:40:36,839
that jesus is stable

00:40:33,839 --> 00:40:38,319
i want you to start focusing on the

00:40:36,839 --> 00:40:40,800
effects

00:40:38,319 --> 00:40:41,680
of using different settings say huge man

00:40:40,800 --> 00:40:45,040
cpu

00:40:41,680 --> 00:40:48,079
pneuma will do

00:40:45,040 --> 00:40:48,079
to the performance

00:40:49,920 --> 00:40:54,319
but remember that you know jesus was not

00:40:52,720 --> 00:40:56,079
really about you know the data showing

00:40:54,319 --> 00:40:57,200
like oh just the results obtained and

00:40:56,079 --> 00:41:00,560
things like that

00:40:57,200 --> 00:41:01,280
this was all about setting up the

00:41:00,560 --> 00:41:04,400
experiment

00:41:01,280 --> 00:41:09,359
and how to automate the experiment

00:41:04,400 --> 00:41:09,359
to actually do some real experimenting

00:41:10,160 --> 00:41:13,839
which uh what we're talking about here

00:41:12,960 --> 00:41:18,079
you know

00:41:13,839 --> 00:41:21,280
that make your life easier

00:41:18,079 --> 00:41:24,000
make it your experience reproducible

00:41:21,280 --> 00:41:24,560
and be lazy you know create you know

00:41:24,000 --> 00:41:28,319
pre-built

00:41:24,560 --> 00:41:31,200
images for docker and kvm

00:41:28,319 --> 00:41:34,560
it's nice have a script track to do the

00:41:31,200 --> 00:41:37,599
actual health lifting

00:41:34,560 --> 00:41:38,960
and i would like

00:41:37,599 --> 00:41:41,040
you know to take the opportunity to

00:41:38,960 --> 00:41:43,280
thank the linux foundation for the

00:41:41,040 --> 00:41:45,680
opportunity to participate on the kvm

00:41:43,280 --> 00:41:45,680
forum

00:41:46,240 --> 00:41:52,480
and here are some useful links

00:41:49,760 --> 00:41:53,839
you know you can read the 102 pages on

00:41:52,480 --> 00:41:57,200
the

00:41:53,839 --> 00:41:59,920
snia uh how to tell how to properly test

00:41:57,200 --> 00:42:04,079
solid state drive

00:41:59,920 --> 00:42:06,560
there's the fios pdk links

00:42:04,079 --> 00:42:08,079
the there's the intel article i talked

00:42:06,560 --> 00:42:11,760
about

00:42:08,079 --> 00:42:12,800
the watches fabric is the link to the

00:42:11,760 --> 00:42:17,040
project

00:42:12,800 --> 00:42:21,040
that that i'm building gs test

00:42:17,040 --> 00:42:23,599
tools for and finally the

00:42:21,040 --> 00:42:26,240
the last one is the link to my find

00:42:23,599 --> 00:42:26,240
drive script

00:42:27,440 --> 00:42:32,560
and the torture disc is also available

00:42:30,800 --> 00:42:33,440
but i'm kind of ashamed of it you know

00:42:32,560 --> 00:42:35,680
the code still

00:42:33,440 --> 00:42:36,480
needs a lot of cleaning so i'm not

00:42:35,680 --> 00:42:38,400
putting it here

00:42:36,480 --> 00:42:40,000
even though you can go to my github

00:42:38,400 --> 00:42:45,839
thing and find it

00:42:40,000 --> 00:42:45,839
but just don't tell me you did

00:42:52,880 --> 00:42:54,960

YouTube URL: https://www.youtube.com/watch?v=BUuvgzoezl0


