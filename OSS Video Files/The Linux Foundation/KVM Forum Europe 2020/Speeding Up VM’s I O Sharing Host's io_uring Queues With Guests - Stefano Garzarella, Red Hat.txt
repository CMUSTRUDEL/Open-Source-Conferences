Title: Speeding Up VM’s I O Sharing Host's io_uring Queues With Guests - Stefano Garzarella, Red Hat
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Speeding Up VM’s I/O Sharing Host's io_uring Queues With Guests - Stefano Garzarella, Red Hat
Captions: 
	00:00:06,160 --> 00:00:08,880
hello everyone and thank you for

00:00:07,680 --> 00:00:10,880
attending this talk

00:00:08,880 --> 00:00:12,240
i'm stefano garzarella i'm a senior

00:00:10,880 --> 00:00:15,120
software engineer in

00:00:12,240 --> 00:00:16,960
the red dot virtualization team today

00:00:15,120 --> 00:00:17,840
we'll take a look on how to speed up the

00:00:16,960 --> 00:00:20,240
hamsa yo

00:00:17,840 --> 00:00:22,640
sharing all's iu urine queues with

00:00:20,240 --> 00:00:22,640
guests

00:00:22,880 --> 00:00:27,359
this is the agenda of the tour first of

00:00:25,439 --> 00:00:27,840
all we'll take an overview of value

00:00:27,359 --> 00:00:30,240
urine

00:00:27,840 --> 00:00:30,880
looking at the system codes how the

00:00:30,240 --> 00:00:33,840
queues are

00:00:30,880 --> 00:00:34,559
organized and some interesting features

00:00:33,840 --> 00:00:37,920
like

00:00:34,559 --> 00:00:41,040
resource registration and poly

00:00:37,920 --> 00:00:42,719
then we look at premier and how we use

00:00:41,040 --> 00:00:44,960
it are you urine

00:00:42,719 --> 00:00:46,160
at that point we'll see how to speed up

00:00:44,960 --> 00:00:48,719
brother your block

00:00:46,160 --> 00:00:50,320
sharing are you urine cues directly with

00:00:48,719 --> 00:00:52,480
a guest

00:00:50,320 --> 00:00:53,680
we'll also see some alternatives to this

00:00:52,480 --> 00:00:56,960
approach like

00:00:53,680 --> 00:01:00,000
b host block and vdpa block

00:00:56,960 --> 00:01:03,199
and we'll compare them finally we'll

00:01:00,000 --> 00:01:03,199
talk about next steps

00:01:05,119 --> 00:01:08,479
io urine is a new linux interface

00:01:08,000 --> 00:01:10,640
between

00:01:08,479 --> 00:01:12,080
user space and kernel to do a

00:01:10,640 --> 00:01:14,080
synchronous io

00:01:12,080 --> 00:01:15,439
it's not only oriented to block

00:01:14,080 --> 00:01:18,000
operation but

00:01:15,439 --> 00:01:19,360
it's evolved like a generic interface to

00:01:18,000 --> 00:01:22,560
do asynchronous

00:01:19,360 --> 00:01:25,439
system goals the interface consists

00:01:22,560 --> 00:01:26,240
of a pair of rings allocated by the

00:01:25,439 --> 00:01:29,759
kernel

00:01:26,240 --> 00:01:31,360
and shared with the user space one ring

00:01:29,759 --> 00:01:34,000
is used by the application

00:01:31,360 --> 00:01:34,560
to submit new requests to do and it's

00:01:34,000 --> 00:01:37,759
called

00:01:34,560 --> 00:01:41,040
submission queue ask you the harder ring

00:01:37,759 --> 00:01:43,280
called completion queue cq is used by

00:01:41,040 --> 00:01:46,479
the kernel to return to the user

00:01:43,280 --> 00:01:49,600
the result of the submitted request

00:01:46,479 --> 00:01:53,119
there are three system calls exposed by

00:01:49,600 --> 00:01:56,159
io urine that we are going to explore

00:01:53,119 --> 00:01:59,360
the the first one

00:01:56,159 --> 00:02:01,600
is i o urine setup it's the first system

00:01:59,360 --> 00:02:04,640
called to invoke to set up the context

00:02:01,600 --> 00:02:06,640
for performing a synchronous io

00:02:04,640 --> 00:02:09,679
several flags and parameters can be

00:02:06,640 --> 00:02:12,400
specified such as the ring size

00:02:09,679 --> 00:02:14,160
it returns a file descriptor that

00:02:12,400 --> 00:02:15,920
identifies the context

00:02:14,160 --> 00:02:18,720
and it must be used with the harder

00:02:15,920 --> 00:02:21,840
system codes

00:02:18,720 --> 00:02:23,840
the second one is iu urine register this

00:02:21,840 --> 00:02:24,800
system code is also used during the

00:02:23,840 --> 00:02:28,000
initialization

00:02:24,800 --> 00:02:29,760
phase of the rings or even afterwards to

00:02:28,000 --> 00:02:34,000
change registered stuff

00:02:29,760 --> 00:02:38,080
but it's not used in the critical part

00:02:34,000 --> 00:02:40,560
we'll talk more about it in a few slides

00:02:38,080 --> 00:02:43,040
the last one is are you during enter

00:02:40,560 --> 00:02:44,959
it's the most used system called during

00:02:43,040 --> 00:02:46,000
the life cycle of the context because

00:02:44,959 --> 00:02:49,040
it's used

00:02:46,000 --> 00:02:51,920
to initiate and or

00:02:49,040 --> 00:02:53,680
to complete a synchronous io so with a

00:02:51,920 --> 00:02:56,800
single system call

00:02:53,680 --> 00:03:00,000
we can submit new operation to do

00:02:56,800 --> 00:03:02,840
and reap operations done using the rings

00:03:00,000 --> 00:03:05,840
that we are going to see in the next

00:03:02,840 --> 00:03:05,840
slides

00:03:06,319 --> 00:03:09,440
the submission queue is used by the

00:03:08,560 --> 00:03:12,640
application

00:03:09,440 --> 00:03:13,280
to submit new request producing a new sq

00:03:12,640 --> 00:03:16,720
entry

00:03:13,280 --> 00:03:19,599
sqe that contains the operation to do

00:03:16,720 --> 00:03:21,120
and its parameters like file descriptor

00:03:19,599 --> 00:03:24,400
buffer address

00:03:21,120 --> 00:03:24,400
offset etc

00:03:24,799 --> 00:03:28,239
when the application has one or more sql

00:03:27,440 --> 00:03:30,640
id

00:03:28,239 --> 00:03:32,400
it increase the tail and calls are you

00:03:30,640 --> 00:03:34,480
hearing enter to pass control to the

00:03:32,400 --> 00:03:37,519
corner

00:03:34,480 --> 00:03:40,000
at this point the kernel consumes sqe

00:03:37,519 --> 00:03:41,680
updates the head and it schedules the

00:03:40,000 --> 00:03:44,239
work to execute the corporation

00:03:41,680 --> 00:03:44,239
requested

00:03:44,720 --> 00:03:48,159
the canada wheel processes the operation

00:03:47,519 --> 00:03:50,720
schedule

00:03:48,159 --> 00:03:51,360
and when they are done it prepares a new

00:03:50,720 --> 00:03:54,239
cq

00:03:51,360 --> 00:03:56,080
entry cqe for each submitted request

00:03:54,239 --> 00:03:59,120
that contains the result of the

00:03:56,080 --> 00:03:59,120
operation requested

00:03:59,439 --> 00:04:04,799
the cqe which also contains the same

00:04:02,640 --> 00:04:05,599
user data value specified by the

00:04:04,799 --> 00:04:08,959
application

00:04:05,599 --> 00:04:12,319
in the corresponding sqe so it's

00:04:08,959 --> 00:04:14,640
an apac value for the kernel and can be

00:04:12,319 --> 00:04:15,360
used by the application to match the

00:04:14,640 --> 00:04:18,239
result

00:04:15,360 --> 00:04:20,000
with the submitted operation the kernel

00:04:18,239 --> 00:04:20,479
increases the tail of the completion

00:04:20,000 --> 00:04:24,160
queue

00:04:20,479 --> 00:04:24,160
when it adds new cqe

00:04:24,960 --> 00:04:29,840
and the application will consume cqes

00:04:27,759 --> 00:04:31,600
moving the head and checking the result

00:04:29,840 --> 00:04:35,040
of the operation submitted

00:04:31,600 --> 00:04:35,040
with the same user data

00:04:37,520 --> 00:04:42,400
for each request the kernel must take an

00:04:40,000 --> 00:04:44,000
internal reference to the file pointed

00:04:42,400 --> 00:04:46,160
by the file descriptor

00:04:44,000 --> 00:04:47,199
and release it when the operation is

00:04:46,160 --> 00:04:50,240
done

00:04:47,199 --> 00:04:50,800
it also need to map and then map every

00:04:50,240 --> 00:04:53,360
time

00:04:50,800 --> 00:04:55,360
for each request the user buffer in the

00:04:53,360 --> 00:04:58,560
kernel virtual memory

00:04:55,360 --> 00:05:01,600
in order to reduce the overhead for

00:04:58,560 --> 00:05:04,080
each request if the application has

00:05:01,600 --> 00:05:04,800
a set of file descriptors and user

00:05:04,080 --> 00:05:07,919
buffers

00:05:04,800 --> 00:05:09,840
users very often we can pre-register

00:05:07,919 --> 00:05:10,880
them with the higher urine register

00:05:09,840 --> 00:05:13,840
system code

00:05:10,880 --> 00:05:16,400
and use an index in the sqe in this way

00:05:13,840 --> 00:05:19,840
the kernel already has the reference

00:05:16,400 --> 00:05:22,080
and it used that index to get it

00:05:19,840 --> 00:05:24,160
this system call can be used also to

00:05:22,080 --> 00:05:27,039
register other resources like an

00:05:24,160 --> 00:05:29,280
event fd to receive notification when

00:05:27,039 --> 00:05:31,919
some request is completed

00:05:29,280 --> 00:05:32,639
or it can be used to pro by urine to get

00:05:31,919 --> 00:05:35,120
information

00:05:32,639 --> 00:05:36,800
about the code supported by the running

00:05:35,120 --> 00:05:38,800
kernel

00:05:36,800 --> 00:05:41,039
it can be also used to register

00:05:38,800 --> 00:05:44,160
personality to issue sqe

00:05:41,039 --> 00:05:45,199
with certain credentials or as we can

00:05:44,160 --> 00:05:49,680
see later

00:05:45,199 --> 00:05:49,680
to register restriction and enable ring

00:05:50,840 --> 00:05:54,800
processing

00:05:52,320 --> 00:05:55,360
another good feature provided by io

00:05:54,800 --> 00:05:57,680
urine

00:05:55,360 --> 00:05:58,800
is the poly we have the possibility to

00:05:57,680 --> 00:06:01,600
enable the sq

00:05:58,800 --> 00:06:02,080
polling and the geopoly in the first

00:06:01,600 --> 00:06:04,639
case

00:06:02,080 --> 00:06:05,680
a kernel trade is created to pull the

00:06:04,639 --> 00:06:08,000
submission queue

00:06:05,680 --> 00:06:08,720
avoiding the needs of system call to

00:06:08,000 --> 00:06:12,000
pass controlled

00:06:08,720 --> 00:06:14,560
again an idle time is configurable

00:06:12,000 --> 00:06:15,759
so if the kernel trial is either for

00:06:14,560 --> 00:06:18,240
more than a configured

00:06:15,759 --> 00:06:20,400
time it goes to sleep and the

00:06:18,240 --> 00:06:22,479
application must call are you uring

00:06:20,400 --> 00:06:25,520
enter with a special flag

00:06:22,479 --> 00:06:28,000
to wake up the current trial

00:06:25,520 --> 00:06:30,080
when this feature is enabled potentially

00:06:28,000 --> 00:06:32,960
the application can submit

00:06:30,080 --> 00:06:34,880
and reap request without doing a single

00:06:32,960 --> 00:06:38,080
system code

00:06:34,880 --> 00:06:38,560
we can also enable io poly doing busy

00:06:38,080 --> 00:06:41,600
wait

00:06:38,560 --> 00:06:44,400
for your completion instead of waiting

00:06:41,600 --> 00:06:45,280
for an asynchronous notification such as

00:06:44,400 --> 00:06:48,720
an interrupt

00:06:45,280 --> 00:06:51,360
from the device this feature can be used

00:06:48,720 --> 00:06:55,280
only if the device or the file system

00:06:51,360 --> 00:06:55,280
support block io polling

00:06:57,680 --> 00:07:03,440
starting from queen 5.2 released in her

00:07:00,800 --> 00:07:06,560
previous year iu urine is available

00:07:03,440 --> 00:07:09,360
in the qimo synchronous iosum system

00:07:06,560 --> 00:07:09,759
thanks to arushi julian stefan we have a

00:07:09,360 --> 00:07:12,800
new

00:07:09,759 --> 00:07:15,039
aio engine that we can use with a dash

00:07:12,800 --> 00:07:17,840
drive option

00:07:15,039 --> 00:07:18,800
the engine will do the standard block iu

00:07:17,840 --> 00:07:22,960
operation

00:07:18,800 --> 00:07:28,160
read write f sync in the synchronous way

00:07:22,960 --> 00:07:28,160
using their urine cues and operations

00:07:29,120 --> 00:07:33,280
now let's go into the main topic of the

00:07:31,280 --> 00:07:37,919
talk

00:07:33,280 --> 00:07:39,919
so how to speed up block io in qmu

00:07:37,919 --> 00:07:41,360
this is the starting point we have a

00:07:39,919 --> 00:07:44,639
virtual block device

00:07:41,360 --> 00:07:47,199
emulated increment that uses the io

00:07:44,639 --> 00:07:49,919
urine aion joint to do

00:07:47,199 --> 00:07:50,879
block operation so we have two

00:07:49,919 --> 00:07:54,639
communication

00:07:50,879 --> 00:07:58,160
channels a vert queue between gas kernel

00:07:54,639 --> 00:07:59,280
and premier and io urine cues between

00:07:58,160 --> 00:08:03,120
qmu

00:07:59,280 --> 00:08:03,759
and the host kernel so there is a kind

00:08:03,120 --> 00:08:07,039
of

00:08:03,759 --> 00:08:08,240
translation made by kuem from word queue

00:08:07,039 --> 00:08:12,800
descriptors

00:08:08,240 --> 00:08:15,840
to io urine q hundreds and vice versa

00:08:12,800 --> 00:08:18,960
so if if we don't need the features

00:08:15,840 --> 00:08:19,599
of qima block layer for example if we

00:08:18,960 --> 00:08:23,199
are using

00:08:19,599 --> 00:08:25,599
raw files or devices we can bypass it

00:08:23,199 --> 00:08:26,560
and pass through the higher urine queues

00:08:25,599 --> 00:08:29,840
directly

00:08:26,560 --> 00:08:29,840
in the gas current memory

00:08:30,960 --> 00:08:35,760
so to realize a using posture the

00:08:34,000 --> 00:08:38,800
submission and completion

00:08:35,760 --> 00:08:39,680
cues are mapped in the guest memory and

00:08:38,800 --> 00:08:42,399
we modify

00:08:39,680 --> 00:08:43,839
the vertical block driver to use this

00:08:42,399 --> 00:08:47,279
new short path

00:08:43,839 --> 00:08:50,000
instead of instead of vert queues

00:08:47,279 --> 00:08:51,839
it will submit and reheat requests

00:08:50,000 --> 00:08:55,920
directly from the sq

00:08:51,839 --> 00:08:58,800
and securings we used a registered event

00:08:55,920 --> 00:09:00,959
of dna urine to inject interrupt

00:08:58,800 --> 00:09:01,920
in the guest when there are new security

00:09:00,959 --> 00:09:04,320
available

00:09:01,920 --> 00:09:06,839
also if we're implementing a polling

00:09:04,320 --> 00:09:09,440
strategies where we disable these

00:09:06,839 --> 00:09:10,560
notifications about polling we use the

00:09:09,440 --> 00:09:13,279
set of patches

00:09:10,560 --> 00:09:14,399
developed by stephanie noxie to enable

00:09:13,279 --> 00:09:17,839
the device polling

00:09:14,399 --> 00:09:22,480
through linux block io poll interface

00:09:17,839 --> 00:09:25,040
in the verdejo block driver

00:09:22,480 --> 00:09:27,760
we modify it to pull the completion

00:09:25,040 --> 00:09:30,959
queue in order to avoid interrupts

00:09:27,760 --> 00:09:33,040
in the host we we enable the sq

00:09:30,959 --> 00:09:35,600
polling to avoid notification from the

00:09:33,040 --> 00:09:38,399
guest reducing the vm exit

00:09:35,600 --> 00:09:40,480
and we also enable the yield poly to

00:09:38,399 --> 00:09:42,959
avoid the interrupts from the hardware

00:09:40,480 --> 00:09:42,959
device

00:09:45,120 --> 00:09:49,440
in order to share submission and

00:09:47,360 --> 00:09:52,080
completion cues with the guest

00:09:49,440 --> 00:09:52,959
we needed some changes in our your urine

00:09:52,080 --> 00:09:56,560
the first one

00:09:52,959 --> 00:09:58,880
was a way to enable and disable event fd

00:09:56,560 --> 00:10:01,200
notification at runtime

00:09:58,880 --> 00:10:01,920
we use this feature to disable interrupt

00:10:01,200 --> 00:10:06,000
in the guest

00:10:01,920 --> 00:10:06,000
when we are following the completion cue

00:10:06,959 --> 00:10:10,160
the second change are the most important

00:10:09,040 --> 00:10:12,880
part we need

00:10:10,160 --> 00:10:13,440
a way to restrict the operations allowed

00:10:12,880 --> 00:10:17,200
in an eye

00:10:13,440 --> 00:10:20,480
urine context to safely share the rings

00:10:17,200 --> 00:10:22,920
with untrusted processes or guests

00:10:20,480 --> 00:10:25,360
i put a link to a good article on

00:10:22,920 --> 00:10:28,480
lwhan.net about this feature that we

00:10:25,360 --> 00:10:30,160
will discuss in the next slides

00:10:28,480 --> 00:10:32,160
the last change concern memory

00:10:30,160 --> 00:10:34,720
translation because io urine

00:10:32,160 --> 00:10:35,279
expat hosts virtual addresses but the

00:10:34,720 --> 00:10:38,480
driver

00:10:35,279 --> 00:10:40,640
in the guest use gas fist guest

00:10:38,480 --> 00:10:43,279
physical addresses so we need a

00:10:40,640 --> 00:10:45,519
mechanism to register the memory mapping

00:10:43,279 --> 00:10:46,640
allowing io urine to translate these

00:10:45,519 --> 00:10:48,560
addresses

00:10:46,640 --> 00:10:51,040
unfortunately this feature is not yet

00:10:48,560 --> 00:10:51,040
available

00:10:52,320 --> 00:10:56,800
as we saw we had the possibility to

00:10:54,800 --> 00:10:59,120
restrict the higher urine cues

00:10:56,800 --> 00:11:01,360
to share them with the guests for

00:10:59,120 --> 00:11:04,000
example we don't want to allow

00:11:01,360 --> 00:11:05,279
a guest to use all file descriptors

00:11:04,000 --> 00:11:08,240
opened by premium

00:11:05,279 --> 00:11:10,720
or to do any kind of operations we want

00:11:08,240 --> 00:11:14,079
to enable only some operation

00:11:10,720 --> 00:11:17,040
like read write sync

00:11:14,079 --> 00:11:19,120
on a subset of file descriptor so with

00:11:17,040 --> 00:11:19,440
the higher urine restriction feature we

00:11:19,120 --> 00:11:22,160
can

00:11:19,440 --> 00:11:23,279
install an allow list or an io urine

00:11:22,160 --> 00:11:25,920
context

00:11:23,279 --> 00:11:26,399
and only the operation defined in that

00:11:25,920 --> 00:11:29,519
list

00:11:26,399 --> 00:11:30,480
can be executed this also prevents that

00:11:29,519 --> 00:11:32,880
a new

00:11:30,480 --> 00:11:35,839
urine features accidentally become

00:11:32,880 --> 00:11:38,240
available for the guest

00:11:35,839 --> 00:11:40,560
the allow list can be installed using

00:11:38,240 --> 00:11:43,040
the audio urine register system call

00:11:40,560 --> 00:11:44,640
but the rings must start disabled using

00:11:43,040 --> 00:11:47,120
the air disable flag

00:11:44,640 --> 00:11:49,519
during the setup in this state no

00:11:47,120 --> 00:11:51,920
operation can be submitted

00:11:49,519 --> 00:11:53,920
when the restrictions are installed we

00:11:51,920 --> 00:11:57,040
can enable the ring processing

00:11:53,920 --> 00:11:59,360
using the enable rings of code with our

00:11:57,040 --> 00:12:02,399
urine register system call

00:11:59,360 --> 00:12:04,800
this allow us to avoid critical races

00:12:02,399 --> 00:12:07,920
between the creation of the rings and

00:12:04,800 --> 00:12:10,320
the installation of the restrictions

00:12:07,920 --> 00:12:12,000
with the allow list we can restrict the

00:12:10,320 --> 00:12:14,800
io urine register

00:12:12,000 --> 00:12:16,160
op codes for example disabling the

00:12:14,800 --> 00:12:19,360
possibility to register

00:12:16,160 --> 00:12:20,320
new buffers or file descriptors in this

00:12:19,360 --> 00:12:22,320
way the guest

00:12:20,320 --> 00:12:25,120
can use only the file descriptors that

00:12:22,320 --> 00:12:28,720
we already registered

00:12:25,120 --> 00:12:31,519
we can also limit the sqe of codes

00:12:28,720 --> 00:12:32,639
allowing only a subset of operation and

00:12:31,519 --> 00:12:35,680
we can specify

00:12:32,639 --> 00:12:39,279
which sqe flags are allowed or

00:12:35,680 --> 00:12:42,880
required for each operation

00:12:39,279 --> 00:12:46,240
for example if we want that each sqe

00:12:42,880 --> 00:12:48,480
uses only the file descriptor registered

00:12:46,240 --> 00:12:49,279
we need to require that the fixed file

00:12:48,480 --> 00:12:53,519
flag

00:12:49,279 --> 00:12:55,760
must be set in each in each sqe

00:12:53,519 --> 00:12:56,720
with this mechanism implemented in io

00:12:55,760 --> 00:13:00,079
urine we can

00:12:56,720 --> 00:13:00,720
safely share submission and completion

00:13:00,079 --> 00:13:03,360
cues

00:13:00,720 --> 00:13:03,360
with a guest

00:13:04,800 --> 00:13:08,959
we realized a proof of concept to

00:13:06,959 --> 00:13:11,519
analyze the performance and

00:13:08,959 --> 00:13:13,920
we compared it with bare metal and

00:13:11,519 --> 00:13:16,480
virtio block device simulation in qmu

00:13:13,920 --> 00:13:21,040
that we saw some slides ago

00:13:16,480 --> 00:13:24,160
in our test we run with io urine engine

00:13:21,040 --> 00:13:26,800
and 4k block sides we

00:13:24,160 --> 00:13:27,279
measure the number of io operation per

00:13:26,800 --> 00:13:31,040
second

00:13:27,279 --> 00:13:31,040
that we put in the vertical axis

00:13:31,120 --> 00:13:38,720
so the the unit

00:13:34,959 --> 00:13:40,000
is kilojoules so thousand io operation

00:13:38,720 --> 00:13:42,000
per second

00:13:40,000 --> 00:13:43,600
and the results are really encouraging

00:13:42,000 --> 00:13:48,399
since in the worst case

00:13:43,600 --> 00:13:52,240
where i adapt is one

00:13:48,399 --> 00:13:55,600
there is only one request in fly

00:13:52,240 --> 00:13:58,399
and in this case the gap

00:13:55,600 --> 00:14:00,800
between a urine pasture and bare metal

00:13:58,399 --> 00:14:03,120
is less than 13

00:14:00,800 --> 00:14:03,839
so this gap is charactered by the fact

00:14:03,120 --> 00:14:06,320
that we

00:14:03,839 --> 00:14:06,959
we have to cross twice the linux block

00:14:06,320 --> 00:14:10,079
layer

00:14:06,959 --> 00:14:13,600
one time in the guest and another one

00:14:10,079 --> 00:14:14,480
in the host as we can see increasing the

00:14:13,600 --> 00:14:17,600
number of

00:14:14,480 --> 00:14:21,199
operation in flight

00:14:17,600 --> 00:14:22,959
the gap go to zero

00:14:21,199 --> 00:14:25,120
compared with virtio block device

00:14:22,959 --> 00:14:29,040
simulation in queen mu

00:14:25,120 --> 00:14:32,480
the first the first bar in the graph

00:14:29,040 --> 00:14:35,440
we are always faster as we skip a big

00:14:32,480 --> 00:14:36,079
piece of software stack and we avoid the

00:14:35,440 --> 00:14:39,120
trust

00:14:36,079 --> 00:14:44,399
the the translation from word queue to

00:14:39,120 --> 00:14:46,880
higher urine queues

00:14:44,399 --> 00:14:49,199
an alternative to a urine pass-through

00:14:46,880 --> 00:14:52,079
is to move the device emulation

00:14:49,199 --> 00:14:52,639
in the kernel using v-host also in this

00:14:52,079 --> 00:14:54,639
case

00:14:52,639 --> 00:14:56,399
we will have a single communication

00:14:54,639 --> 00:14:58,320
channel since the word queue

00:14:56,399 --> 00:14:59,600
is shared between guests and those

00:14:58,320 --> 00:15:01,760
kernels

00:14:59,600 --> 00:15:02,880
in the last years some v-host block

00:15:01,760 --> 00:15:06,480
implementation was

00:15:02,880 --> 00:15:10,720
published of the same but never measured

00:15:06,480 --> 00:15:14,079
the first version from isis used the bio

00:15:10,720 --> 00:15:18,000
api the lowest api just up to the block

00:15:14,079 --> 00:15:21,600
drivers the second version from vitaly

00:15:18,000 --> 00:15:24,240
moved to bfs api this allows us to

00:15:21,600 --> 00:15:25,519
use also raw files stored in a file

00:15:24,240 --> 00:15:28,720
system

00:15:25,519 --> 00:15:31,199
vfs adds some overhead of course but

00:15:28,720 --> 00:15:31,839
it's negligible and it's also the same

00:15:31,199 --> 00:15:35,279
interface

00:15:31,839 --> 00:15:37,680
used by iouri so i compared

00:15:35,279 --> 00:15:38,720
this last version with io urine

00:15:37,680 --> 00:15:40,880
passthrough

00:15:38,720 --> 00:15:42,800
improving a bit the implementation

00:15:40,880 --> 00:15:46,399
adding vert queue

00:15:42,800 --> 00:15:46,399
and block device polling

00:15:46,880 --> 00:15:51,120
i run the same fio configuration that we

00:15:49,680 --> 00:15:54,320
saw before

00:15:51,120 --> 00:15:56,160
the first bar is obtained without

00:15:54,320 --> 00:16:00,399
polling so it's the original

00:15:56,160 --> 00:16:02,639
version in the second bar i added

00:16:00,399 --> 00:16:04,000
the vertical polling in the vios block

00:16:02,639 --> 00:16:06,079
emulation

00:16:04,000 --> 00:16:07,279
so we avoided the notification from the

00:16:06,079 --> 00:16:10,399
guest

00:16:07,279 --> 00:16:10,880
in the yellow bar we enable the block io

00:16:10,399 --> 00:16:13,600
poly

00:16:10,880 --> 00:16:14,240
so we do busy weight in the host kernel

00:16:13,600 --> 00:16:17,360
avoiding

00:16:14,240 --> 00:16:20,320
interrupts from the device and

00:16:17,360 --> 00:16:20,320
in the green bar

00:16:20,480 --> 00:16:24,720
we sq pawling in a fire running in the

00:16:23,360 --> 00:16:28,800
guest

00:16:24,720 --> 00:16:31,040
as you can see also with polling

00:16:28,800 --> 00:16:33,360
there is still a gap with our urine pass

00:16:31,040 --> 00:16:36,639
through and it should be related

00:16:33,360 --> 00:16:39,759
to the rings allocation with vhost block

00:16:36,639 --> 00:16:41,279
the birth queue and descriptors are

00:16:39,759 --> 00:16:44,720
located in the guest memory

00:16:41,279 --> 00:16:45,759
so the host kernel must call copy in and

00:16:44,720 --> 00:16:49,360
copy 2

00:16:45,759 --> 00:16:50,399
for each request this is not needed

00:16:49,360 --> 00:16:52,320
without your urine

00:16:50,399 --> 00:16:53,600
posture where the submission and

00:16:52,320 --> 00:16:56,160
completion cues

00:16:53,600 --> 00:16:57,279
are allocated in the host kernel and

00:16:56,160 --> 00:17:00,240
then mapped

00:16:57,279 --> 00:17:00,240
in the guest memory

00:17:01,120 --> 00:17:05,360
as i said v-host block was never merged

00:17:04,720 --> 00:17:07,919
upstream

00:17:05,360 --> 00:17:09,280
but recently a new framework has been

00:17:07,919 --> 00:17:11,679
developed

00:17:09,280 --> 00:17:12,959
especially to upload vertical processing

00:17:11,679 --> 00:17:16,559
to the algorithm

00:17:12,959 --> 00:17:19,919
this framework is called vdpa vertio

00:17:16,559 --> 00:17:21,600
data path acceleration our idea is to

00:17:19,919 --> 00:17:24,000
implement a vdpa block

00:17:21,600 --> 00:17:24,640
software device very similar to vios

00:17:24,000 --> 00:17:27,839
block

00:17:24,640 --> 00:17:30,320
but using this new framework

00:17:27,839 --> 00:17:30,960
this allows us to unify the software

00:17:30,320 --> 00:17:33,520
stack

00:17:30,960 --> 00:17:34,240
and reuse the same code for example in

00:17:33,520 --> 00:17:35,919
premiere

00:17:34,240 --> 00:17:37,280
when hardware implementation will be

00:17:35,919 --> 00:17:40,559
available

00:17:37,280 --> 00:17:43,760
in addition with vdpa we have more

00:17:40,559 --> 00:17:46,480
control than we host on device lifecycle

00:17:43,760 --> 00:17:47,840
and the gas pages are pinned in memory

00:17:46,480 --> 00:17:51,120
so we don't need to

00:17:47,840 --> 00:17:52,160
memory map user buffers or do copy in

00:17:51,120 --> 00:17:55,200
and copy 2

00:17:52,160 --> 00:17:57,760
for each descriptors this

00:17:55,200 --> 00:17:59,200
should allow us to fill the gap with our

00:17:57,760 --> 00:18:02,080
urine posture

00:17:59,200 --> 00:18:04,880
on the other hand the pin pages don't

00:18:02,080 --> 00:18:07,840
allow us to over commit the guest memory

00:18:04,880 --> 00:18:09,039
and we also need to implement the

00:18:07,840 --> 00:18:12,840
polling strategies

00:18:09,039 --> 00:18:14,160
and bfs integration that we are already

00:18:12,840 --> 00:18:17,120
available

00:18:14,160 --> 00:18:17,120
in io urine

00:18:18,160 --> 00:18:23,200
concluding in the next months we will

00:18:20,240 --> 00:18:26,240
implement a proof of concept of vdba

00:18:23,200 --> 00:18:28,720
block software device starting from a

00:18:26,240 --> 00:18:31,840
bdba block simulator in the kernel

00:18:28,720 --> 00:18:32,320
then we will add the support of bdpa

00:18:31,840 --> 00:18:34,960
block

00:18:32,320 --> 00:18:35,600
on queen new and we will develop the

00:18:34,960 --> 00:18:38,640
linux

00:18:35,600 --> 00:18:40,720
vtpa driver with device emulation and

00:18:38,640 --> 00:18:43,520
bfs integration

00:18:40,720 --> 00:18:44,559
we will also work on the block io poll

00:18:43,520 --> 00:18:47,600
optimization

00:18:44,559 --> 00:18:49,600
for the verteio block driver and we will

00:18:47,600 --> 00:18:52,080
try to add the missing features

00:18:49,600 --> 00:18:55,360
to io urine to complete the urine

00:18:52,080 --> 00:18:55,360
passthrough implementation

00:18:55,679 --> 00:19:08,799
so thank you very much and now it's time

00:18:58,400 --> 00:19:08,799

YouTube URL: https://www.youtube.com/watch?v=MXwjMW2t0gE


