Title: Towards an Alternative Memory Architecture - Joao Martins, Oracle
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Towards an Alternative Memory Architecture - Joao Martins, Oracle
Captions: 
	00:00:05,839 --> 00:00:09,920
hello everyone

00:00:07,440 --> 00:00:11,360
my name is martinez um and i'm here to

00:00:09,920 --> 00:00:14,240
talk about memory efficiency

00:00:11,360 --> 00:00:15,280
and ways we could fix it and i'll start

00:00:14,240 --> 00:00:18,720
with the

00:00:15,280 --> 00:00:23,039
short motivation with what got us here

00:00:18,720 --> 00:00:26,320
linux keeps the a map of all physical

00:00:23,039 --> 00:00:28,720
uh memory in its page

00:00:26,320 --> 00:00:30,240
in the kernel page tables which is

00:00:28,720 --> 00:00:33,040
called the direct map

00:00:30,240 --> 00:00:34,239
this address space is modified uh in a

00:00:33,040 --> 00:00:36,410
lot of ways

00:00:34,239 --> 00:00:37,920
throughout the different os uh

00:00:36,410 --> 00:00:41,200
[Music]

00:00:37,920 --> 00:00:45,200
stages uh throughout the os lifecycle

00:00:41,200 --> 00:00:48,960
at boot when initializing

00:00:45,200 --> 00:00:52,079
the memory map and all the system ram

00:00:48,960 --> 00:00:54,160
or at hot plug when you you know

00:00:52,079 --> 00:00:55,280
hot plugging a new memory or when you

00:00:54,160 --> 00:00:59,199
try to map

00:00:55,280 --> 00:00:59,600
an io address portions of this address

00:00:59,199 --> 00:01:01,760
split

00:00:59,600 --> 00:01:03,199
may be baked by various kinds of

00:01:01,760 --> 00:01:05,600
metadata

00:01:03,199 --> 00:01:07,040
which track different different things

00:01:05,600 --> 00:01:09,840
they have different granularities we

00:01:07,040 --> 00:01:13,439
have memblog which describes

00:01:09,840 --> 00:01:17,040
blocks of memory uh

00:01:13,439 --> 00:01:17,840
you have the underlying memory model so

00:01:17,040 --> 00:01:21,119
vma map

00:01:17,840 --> 00:01:23,439
and um which looks like

00:01:21,119 --> 00:01:27,040
a single contiguous regions which in

00:01:23,439 --> 00:01:31,520
each individual

00:01:27,040 --> 00:01:34,720
address points to start pages

00:01:31,520 --> 00:01:36,640
although um at the lower end as you can

00:01:34,720 --> 00:01:39,119
see you have swept pages

00:01:36,640 --> 00:01:41,520
uh the address space though may be

00:01:39,119 --> 00:01:44,560
managed what goes in the actual

00:01:41,520 --> 00:01:46,479
page tables may be done in a

00:01:44,560 --> 00:01:47,759
bigger chunk so usually it's often the

00:01:46,479 --> 00:01:52,560
case that you use

00:01:47,759 --> 00:01:52,560
two megabyte pages for direct map

00:01:52,880 --> 00:01:56,640
so what the process may think it has

00:01:54,720 --> 00:01:58,960
mapped when it sets up

00:01:56,640 --> 00:02:00,560
a guest with the memory slots or any

00:01:58,960 --> 00:02:02,479
private vm data

00:02:00,560 --> 00:02:04,320
when it's when you enter the kernel and

00:02:02,479 --> 00:02:04,640
you switch to the kernel page tables you

00:02:04,320 --> 00:02:07,840
have

00:02:04,640 --> 00:02:11,200
all guest memory read rightly available

00:02:07,840 --> 00:02:14,560
alongside other

00:02:11,200 --> 00:02:16,959
kernel kernel allocations or anonymous

00:02:14,560 --> 00:02:16,959
memory

00:02:18,640 --> 00:02:22,480
with respect to this uh the direct map i

00:02:21,440 --> 00:02:25,280
would like to highlight

00:02:22,480 --> 00:02:25,520
one particular metadata structure that

00:02:25,280 --> 00:02:27,920
is

00:02:25,520 --> 00:02:28,560
structured which is going to be the

00:02:27,920 --> 00:02:31,840
source

00:02:28,560 --> 00:02:33,120
of most of the talk uh the data

00:02:31,840 --> 00:02:36,000
structure

00:02:33,120 --> 00:02:38,720
design uh is largely derived by the

00:02:36,000 --> 00:02:41,760
needs of page cache in anonymous memory

00:02:38,720 --> 00:02:43,920
and it's also a structure used by

00:02:41,760 --> 00:02:44,959
most kernel services in the most

00:02:43,920 --> 00:02:48,720
granular way of

00:02:44,959 --> 00:02:50,239
tracking memory the purpose of the

00:02:48,720 --> 00:02:54,640
structure the data structure

00:02:50,239 --> 00:02:57,360
is to track um references to pfm

00:02:54,640 --> 00:02:57,360
alongside

00:02:58,000 --> 00:03:03,280
file mappings uh and other

00:03:00,680 --> 00:03:06,560
subsystem-specific

00:03:03,280 --> 00:03:09,920
data usually

00:03:06,560 --> 00:03:12,159
uh you get one of those uh swept pages

00:03:09,920 --> 00:03:13,599
when you say use the body allocator with

00:03:12,159 --> 00:03:16,640
allow page

00:03:13,599 --> 00:03:19,360
or get three pages or

00:03:16,640 --> 00:03:20,640
you you grab a reference to an existing

00:03:19,360 --> 00:03:22,720
page

00:03:20,640 --> 00:03:24,560
um to a particular page with get user

00:03:22,720 --> 00:03:26,799
pages where you pin memory short or long

00:03:24,560 --> 00:03:26,799
term

00:03:26,959 --> 00:03:31,200
although the data structure has some

00:03:29,120 --> 00:03:34,319
overhead the size of the structure

00:03:31,200 --> 00:03:34,879
is about 64 bytes and it's usually

00:03:34,319 --> 00:03:37,280
tracking

00:03:34,879 --> 00:03:38,560
uh 4k although certain architectures

00:03:37,280 --> 00:03:43,040
allow this to be

00:03:38,560 --> 00:03:44,640
tracked in a bigger trend like i'm 64

00:03:43,040 --> 00:03:46,640
lets you play with

00:03:44,640 --> 00:03:49,840
what's going to be the underlying page

00:03:46,640 --> 00:03:54,080
size uh so saying 64k

00:03:49,840 --> 00:03:56,400
um in on top of the structure

00:03:54,080 --> 00:03:57,599
you have other overheads on top like you

00:03:56,400 --> 00:04:00,159
spend 8 bytes per

00:03:57,599 --> 00:04:02,000
apt entry and the process page tables

00:04:00,159 --> 00:04:04,879
you spend about eight bytes as well

00:04:02,000 --> 00:04:05,599
for hpt entry although these costs can

00:04:04,879 --> 00:04:08,720
be available

00:04:05,599 --> 00:04:12,080
when you try to use

00:04:08,720 --> 00:04:14,400
uh huge pages which to amortize that

00:04:12,080 --> 00:04:15,760
page table cost to a great extent so

00:04:14,400 --> 00:04:18,880
when you put it all together

00:04:15,760 --> 00:04:21,280
we are just talking about 1.5 to 1.75

00:04:18,880 --> 00:04:23,360
percent of forced physical memory

00:04:21,280 --> 00:04:24,320
which at the first class does not look

00:04:23,360 --> 00:04:27,680
much

00:04:24,320 --> 00:04:31,680
but let's let's uh revisit uh what

00:04:27,680 --> 00:04:34,560
that actually means in practical terms

00:04:31,680 --> 00:04:35,600
so if we extrapolate that to say 2

00:04:34,560 --> 00:04:38,240
terabytes of memory

00:04:35,600 --> 00:04:40,000
we are spending about 32 to 36 gigabytes

00:04:38,240 --> 00:04:43,360
of memory

00:04:40,000 --> 00:04:47,120
uh so for short that's roughly

00:04:43,360 --> 00:04:49,440
that's 16 gigs per terabyte

00:04:47,120 --> 00:04:50,960
and if you work for a slightly bigger

00:04:49,440 --> 00:04:51,759
machine like an 8 terabyte machine you

00:04:50,960 --> 00:04:54,800
spend about

00:04:51,759 --> 00:04:57,840
128 gigabytes of memory to 160

00:04:54,800 --> 00:05:00,800
gigabytes of memory these are not really

00:04:57,840 --> 00:05:02,960
um crazy numbers they actually numbers

00:05:00,800 --> 00:05:04,960
on machines we have problems with

00:05:02,960 --> 00:05:06,000
uh where a lot of this memory you're

00:05:04,960 --> 00:05:09,360
spending

00:05:06,000 --> 00:05:11,840
could hopefully be used to actually boot

00:05:09,360 --> 00:05:11,840
more guests

00:05:12,160 --> 00:05:18,000
if we take in consideration how uh

00:05:15,759 --> 00:05:19,840
how this where this is going and that

00:05:18,000 --> 00:05:23,280
the fact that beams are getting

00:05:19,840 --> 00:05:24,000
more dense uh if you take a 64 terabytes

00:05:23,280 --> 00:05:25,919
machine

00:05:24,000 --> 00:05:30,160
to put this overhead in perspective you

00:05:25,919 --> 00:05:33,840
would be spending about 1.2 terabytes

00:05:30,160 --> 00:05:34,720
in struct page take that into account

00:05:33,840 --> 00:05:37,840
with the recent

00:05:34,720 --> 00:05:39,680
spec uh the recent vulnerabilities in

00:05:37,840 --> 00:05:41,919
hydro and cpus

00:05:39,680 --> 00:05:44,479
where we could speculate we take

00:05:41,919 --> 00:05:48,880
advantage of these code gadgets

00:05:44,479 --> 00:05:51,199
or or you could potentially leak

00:05:48,880 --> 00:05:53,440
all the memory map by the kernel in user

00:05:51,199 --> 00:05:57,199
space

00:05:53,440 --> 00:06:00,240
uh or exploiting through more uh

00:05:57,199 --> 00:06:01,120
cpu resources uh to lower cpu resources

00:06:00,240 --> 00:06:05,360
like

00:06:01,120 --> 00:06:08,560
the l1 cache the lndita cache or

00:06:05,360 --> 00:06:10,479
microarchitectural buffers uh

00:06:08,560 --> 00:06:11,680
one thing all these have one thing in

00:06:10,479 --> 00:06:14,400
common which is

00:06:11,680 --> 00:06:15,120
given that the kernel maps everything

00:06:14,400 --> 00:06:18,560
therefore

00:06:15,120 --> 00:06:20,400
everything is lickable the one i would

00:06:18,560 --> 00:06:24,400
like to give special emphasis

00:06:20,400 --> 00:06:27,600
is spectre v1 which is hard to mitigate

00:06:24,400 --> 00:06:29,120
um and that you have just so many code

00:06:27,600 --> 00:06:33,039
gadgets you need to hunt

00:06:29,120 --> 00:06:34,240
and every new merge window adds

00:06:33,039 --> 00:06:38,240
potentially new

00:06:34,240 --> 00:06:40,160
code gadgets that you could exploit

00:06:38,240 --> 00:06:43,520
so the main premise i'm trying to raise

00:06:40,160 --> 00:06:46,000
here is can we do better for hypervisors

00:06:43,520 --> 00:06:46,560
uh the problem uh the problems you see

00:06:46,000 --> 00:06:48,400
is that

00:06:46,560 --> 00:06:49,759
a given struct page does not really

00:06:48,400 --> 00:06:52,800
reflect

00:06:49,759 --> 00:06:56,400
what goes in the page tables and if you

00:06:52,800 --> 00:06:56,400
look at more modern hypervisors

00:06:56,720 --> 00:06:59,919
we won't be needing the majority of the

00:06:59,039 --> 00:07:03,440
kernel services

00:06:59,919 --> 00:07:06,720
say if you're just doing cpu

00:07:03,440 --> 00:07:08,400
memory and pci devices

00:07:06,720 --> 00:07:10,160
on those circumstances we are

00:07:08,400 --> 00:07:14,000
essentially we

00:07:10,160 --> 00:07:16,880
essentially losing a lot of efficiency

00:07:14,000 --> 00:07:19,199
to what represents a largely idle

00:07:16,880 --> 00:07:22,240
infrastructure throughout the

00:07:19,199 --> 00:07:25,599
uh guest's lifetime or host lifetime

00:07:22,240 --> 00:07:27,360
while potentially unnecessarily mapping

00:07:25,599 --> 00:07:30,880
all customers data when we probably

00:07:27,360 --> 00:07:33,280
don't need to

00:07:30,880 --> 00:07:34,000
so the first step towards fixing some of

00:07:33,280 --> 00:07:36,479
this

00:07:34,000 --> 00:07:39,440
led us to what happens if you try to

00:07:36,479 --> 00:07:39,440
remove strutpage

00:07:39,680 --> 00:07:46,400
first let me describe what today is one

00:07:42,960 --> 00:07:48,639
way you could you can do some of this

00:07:46,400 --> 00:07:50,400
and that's through def mam and mam

00:07:48,639 --> 00:07:53,680
equals x

00:07:50,400 --> 00:07:56,960
uh what essentially you do as a user is

00:07:53,680 --> 00:08:00,080
you specify men equals and

00:07:56,960 --> 00:08:01,759
some amount and that's going to be an

00:08:00,080 --> 00:08:05,199
amount which you're limited

00:08:01,759 --> 00:08:06,720
to by the kernel and

00:08:05,199 --> 00:08:08,720
you have this special device called the

00:08:06,720 --> 00:08:12,400
fmm where you can map

00:08:08,720 --> 00:08:14,479
every uh every memory on the system

00:08:12,400 --> 00:08:15,599
um one problem there are a couple

00:08:14,479 --> 00:08:16,240
problems with this that i would like to

00:08:15,599 --> 00:08:18,639
enumerate

00:08:16,240 --> 00:08:19,680
uh first and foremost when you specify

00:08:18,639 --> 00:08:21,919
mem equals x

00:08:19,680 --> 00:08:22,720
you have no way to characterize where

00:08:21,919 --> 00:08:25,440
exactly

00:08:22,720 --> 00:08:26,080
you want to take that amount from so you

00:08:25,440 --> 00:08:29,120
potentially

00:08:26,080 --> 00:08:32,320
either restrict one to the first node

00:08:29,120 --> 00:08:35,919
or straddling all

00:08:32,320 --> 00:08:39,039
nodes uh to fulfill that parameter

00:08:35,919 --> 00:08:40,880
although you do have a mechanism

00:08:39,039 --> 00:08:43,120
that do not necessarily need to have

00:08:40,880 --> 00:08:45,279
strike pages for that memory

00:08:43,120 --> 00:08:48,080
and but you're limited to a single

00:08:45,279 --> 00:08:48,080
contiguous chunk

00:08:48,480 --> 00:08:56,000
also you can only map you can only

00:08:52,320 --> 00:08:58,640
memory map that memory in 4k

00:08:56,000 --> 00:09:02,240
page sizes you have no two megabyte huge

00:08:58,640 --> 00:09:02,240
pages or one gigabyte huge pages

00:09:02,399 --> 00:09:06,839
which then goes to my next point when

00:09:04,640 --> 00:09:11,360
you're dealing with fragmentation

00:09:06,839 --> 00:09:13,360
across hundreds thousands of um

00:09:11,360 --> 00:09:14,800
guests creations and teardowns you're

00:09:13,360 --> 00:09:15,519
potentially giving me the fragmented

00:09:14,800 --> 00:09:18,160
systems

00:09:15,519 --> 00:09:20,839
so in addition to not having huge pages

00:09:18,160 --> 00:09:24,640
which sort of amortizes some of that

00:09:20,839 --> 00:09:27,680
um uh you want the ability to

00:09:24,640 --> 00:09:29,760
pick holes uh free

00:09:27,680 --> 00:09:31,600
free holes you have within allocated

00:09:29,760 --> 00:09:33,680
chunks to accommodate the given

00:09:31,600 --> 00:09:36,560
allocation

00:09:33,680 --> 00:09:37,440
and in order to do that with devman we

00:09:36,560 --> 00:09:40,160
need to

00:09:37,440 --> 00:09:42,240
map several times devnet and space and

00:09:40,160 --> 00:09:45,600
use different page offsets

00:09:42,240 --> 00:09:47,600
um but you still need to give devman

00:09:45,600 --> 00:09:50,160
access to a given vmm

00:09:47,600 --> 00:09:51,760
which then you know breaks a little bit

00:09:50,160 --> 00:09:54,640
uh

00:09:51,760 --> 00:09:55,200
the case where vmm runs potentially the

00:09:54,640 --> 00:09:58,160
privileged

00:09:55,200 --> 00:09:59,760
environment and therefore should not

00:09:58,160 --> 00:10:02,320
have been able to map

00:09:59,760 --> 00:10:03,519
any memory on the system and finally you

00:10:02,320 --> 00:10:06,320
don't have

00:10:03,519 --> 00:10:08,079
a way to give some of that memory back

00:10:06,320 --> 00:10:09,040
to the kernel so you'll try to rescue

00:10:08,079 --> 00:10:13,200
the host from uh

00:10:09,040 --> 00:10:13,200
out of memory's situation

00:10:13,680 --> 00:10:20,720
all these problems let us look at tax

00:10:17,120 --> 00:10:22,640
which is the other mechanism which

00:10:20,720 --> 00:10:24,000
purpose is to give you direct access to

00:10:22,640 --> 00:10:26,240
memory

00:10:24,000 --> 00:10:27,360
it's bigger consumer is pmem and the

00:10:26,240 --> 00:10:29,680
interface behind

00:10:27,360 --> 00:10:30,640
that device text is very simple it's a

00:10:29,680 --> 00:10:32,880
character device

00:10:30,640 --> 00:10:34,880
which you instrument to csfs you create

00:10:32,880 --> 00:10:37,519
to csfs

00:10:34,880 --> 00:10:38,640
and which just lets you in the kernel

00:10:37,519 --> 00:10:41,680
memory map

00:10:38,640 --> 00:10:45,040
a given chunk of memory

00:10:41,680 --> 00:10:48,240
application is a a given memory map so

00:10:45,040 --> 00:10:51,040
all this metadata is uncreated

00:10:48,240 --> 00:10:51,920
and application has control over how the

00:10:51,040 --> 00:10:55,040
memory is mapped

00:10:51,920 --> 00:10:56,800
like in 4k pages 2 megabytes or 1

00:10:55,040 --> 00:10:59,920
gigabyte pages

00:10:56,800 --> 00:11:02,959
and any exception you have uh

00:10:59,920 --> 00:11:03,760
like mcs and x86 are forward back to the

00:11:02,959 --> 00:11:07,120
application

00:11:03,760 --> 00:11:10,320
uh they you get a signal

00:11:07,120 --> 00:11:12,560
and final finally you have memory ui

00:11:10,320 --> 00:11:14,880
mechanisms to return that back that

00:11:12,560 --> 00:11:18,320
memory back to the kernel say with dax

00:11:14,880 --> 00:11:21,360
canon driver uh you can

00:11:18,320 --> 00:11:22,480
emulate some of this uh with mem map

00:11:21,360 --> 00:11:24,160
option

00:11:22,480 --> 00:11:26,720
although the problem with using this

00:11:24,160 --> 00:11:30,399
option is that you give

00:11:26,720 --> 00:11:33,519
that option has a lot of um

00:11:30,399 --> 00:11:36,800
power uh uh into

00:11:33,519 --> 00:11:39,519
messing up with your memory map so users

00:11:36,800 --> 00:11:41,200
really need deep knowledge of what your

00:11:39,519 --> 00:11:44,079
hardware memory map looks like

00:11:41,200 --> 00:11:45,680
to be able to pick an actual run range

00:11:44,079 --> 00:11:48,320
one thing i'd like to clarify here

00:11:45,680 --> 00:11:49,920
uh is that usually people can fuse call

00:11:48,320 --> 00:11:52,800
decks as one thing

00:11:49,920 --> 00:11:53,839
but there are two kinds of daxes and and

00:11:52,800 --> 00:11:56,720
there's the pm

00:11:53,839 --> 00:11:58,000
so there is device tags which is this

00:11:56,720 --> 00:12:00,639
very simple

00:11:58,000 --> 00:12:02,240
uh device there's pman which is the

00:12:00,639 --> 00:12:04,560
block device and there is file system

00:12:02,240 --> 00:12:04,560
decks

00:12:04,639 --> 00:12:07,680
which purpose is to bypass the page

00:12:06,320 --> 00:12:08,959
cache so

00:12:07,680 --> 00:12:10,880
these are all three different things and

00:12:08,959 --> 00:12:14,000
the one i'm emphasizing here is dax

00:12:10,880 --> 00:12:16,720
device text as i'm sort of hinting there

00:12:14,000 --> 00:12:19,920
is a couple of problems with device text

00:12:16,720 --> 00:12:21,440
and it's largely uh derived from its

00:12:19,920 --> 00:12:25,360
biggest consumer which is

00:12:21,440 --> 00:12:26,399
pman persistent memory name spaces are

00:12:25,360 --> 00:12:28,880
not supported

00:12:26,399 --> 00:12:30,959
uh do not support these contiguous

00:12:28,880 --> 00:12:33,839
regions

00:12:30,959 --> 00:12:36,880
they support uh only you can only have a

00:12:33,839 --> 00:12:39,760
namespace with one contiguous chunk

00:12:36,880 --> 00:12:40,800
uh in addition to that uh because you

00:12:39,760 --> 00:12:43,279
need to initialize

00:12:40,800 --> 00:12:45,519
all these many extract pages you have

00:12:43,279 --> 00:12:48,320
long initialization time of your

00:12:45,519 --> 00:12:49,519
uh dex device in bringing it up online

00:12:48,320 --> 00:12:52,480
because you need to clear

00:12:49,519 --> 00:12:53,519
all that memory and to add up while you

00:12:52,480 --> 00:12:56,959
can represent

00:12:53,519 --> 00:13:00,320
huge pages uh in the page tables uh

00:12:56,959 --> 00:13:04,079
the way these page strike pages look are

00:13:00,320 --> 00:13:06,800
not uh are not the same as say

00:13:04,079 --> 00:13:07,839
transparent each pages or hp glbfs where

00:13:06,800 --> 00:13:09,920
you you would

00:13:07,839 --> 00:13:11,920
have a head page and a couple of tell

00:13:09,920 --> 00:13:15,600
pages to represent a two megabyte

00:13:11,920 --> 00:13:17,200
or one gigabyte page uh so these

00:13:15,600 --> 00:13:18,959
you gotta look at the page tables to

00:13:17,200 --> 00:13:21,680
understand whether a given struct page

00:13:18,959 --> 00:13:24,720
belongs or not to

00:13:21,680 --> 00:13:26,480
a a huge page

00:13:24,720 --> 00:13:27,839
and finally you need architectural

00:13:26,480 --> 00:13:29,839
support for

00:13:27,839 --> 00:13:31,200
devmap which is the kernel way to tell

00:13:29,839 --> 00:13:35,279
that this divide this

00:13:31,200 --> 00:13:37,760
particular page um basically a pfm

00:13:35,279 --> 00:13:38,320
belongs to a particular device map and

00:13:37,760 --> 00:13:42,639
therefore

00:13:38,320 --> 00:13:48,480
his own device special zone in the

00:13:42,639 --> 00:13:50,639
uh in the kernel so um

00:13:48,480 --> 00:13:51,600
the main question we got were we got

00:13:50,639 --> 00:13:54,320
into was

00:13:51,600 --> 00:13:56,160
how we could repurpose some of this uh

00:13:54,320 --> 00:13:59,519
device text already provides you

00:13:56,160 --> 00:14:01,360
uh everything that we need and how we

00:13:59,519 --> 00:14:03,920
could repurpose some of this

00:14:01,360 --> 00:14:06,079
while fixing making some improvements to

00:14:03,920 --> 00:14:08,959
repurpose this to volatile memory

00:14:06,079 --> 00:14:11,279
with that let us look to dax hvac which

00:14:08,959 --> 00:14:13,839
is the driver

00:14:11,279 --> 00:14:16,000
which can be used for performance

00:14:13,839 --> 00:14:19,279
differentiated ram

00:14:16,000 --> 00:14:23,279
we essentially remove this memo option

00:14:19,279 --> 00:14:26,800
from here and we instead use efi

00:14:23,279 --> 00:14:30,320
and we added the f5 memory map such that

00:14:26,800 --> 00:14:34,399
we mark ram ranges with efi

00:14:30,320 --> 00:14:37,839
specific memory so we only need to care

00:14:34,399 --> 00:14:40,959
in about ram ranges and we don't need to

00:14:37,839 --> 00:14:44,079
understand uh how exactly

00:14:40,959 --> 00:14:45,440
is firmware exposing uh everything else

00:14:44,079 --> 00:14:47,600
that is not ram

00:14:45,440 --> 00:14:49,199
and we essentially have an ability for

00:14:47,600 --> 00:14:51,199
firmware to

00:14:49,199 --> 00:14:53,360
dedicate memory to user space when

00:14:51,199 --> 00:14:55,760
memory styled with the specific purpose

00:14:53,360 --> 00:14:56,639
attribute it means that the kernel is

00:14:55,760 --> 00:14:59,680
going to create

00:14:56,639 --> 00:15:02,639
one next device and let's

00:14:59,680 --> 00:15:03,360
give that to user space as a memory

00:15:02,639 --> 00:15:06,639
mappable

00:15:03,360 --> 00:15:10,560
uh that lacks device so we

00:15:06,639 --> 00:15:12,560
essentially all have to we had to fix is

00:15:10,560 --> 00:15:14,959
we had to just support these contiguous

00:15:12,560 --> 00:15:17,360
regions where we tried to pick

00:15:14,959 --> 00:15:18,399
all three ranges to accommodate the

00:15:17,360 --> 00:15:21,519
given allocation

00:15:18,399 --> 00:15:23,760
as opposed to um

00:15:21,519 --> 00:15:25,680
uh deal with the just contiguous chunks

00:15:23,760 --> 00:15:27,440
and we and that helps tremendously with

00:15:25,680 --> 00:15:29,040
dealing with fragmentation

00:15:27,440 --> 00:15:30,880
and then the way you allocate this you

00:15:29,040 --> 00:15:33,199
can either give the application control

00:15:30,880 --> 00:15:34,720
over what ranges to peak or you can

00:15:33,199 --> 00:15:37,600
reserve to the

00:15:34,720 --> 00:15:40,399
dax mediation where there's a simple

00:15:37,600 --> 00:15:43,279
range allocated where it adjusts

00:15:40,399 --> 00:15:43,920
ranges or allocate new ones to fulfill

00:15:43,279 --> 00:15:47,040
the

00:15:43,920 --> 00:15:48,639
allocation provided by the user

00:15:47,040 --> 00:15:51,199
the fact that you specify mappings

00:15:48,639 --> 00:15:55,199
especially useful for use cases like

00:15:51,199 --> 00:15:58,480
vmm live restart a kmu live update or

00:15:55,199 --> 00:16:00,800
a kvm live update where you want to

00:15:58,480 --> 00:16:03,839
preserve the exact same ranges

00:16:00,800 --> 00:16:07,279
while not scrubbing that memory

00:16:03,839 --> 00:16:09,040
the next time you map it again and so i

00:16:07,279 --> 00:16:10,959
like to refer to jason zhang and steven

00:16:09,040 --> 00:16:11,519
cesar's presentations which cover a lot

00:16:10,959 --> 00:16:15,279
of

00:16:11,519 --> 00:16:15,279
uh what i refer here

00:16:16,480 --> 00:16:19,759
so the next step was then obviously to

00:16:18,480 --> 00:16:22,959
remove struct page from

00:16:19,759 --> 00:16:26,079
device text uh a lot of the bigger

00:16:22,959 --> 00:16:27,199
infrastructure work was for repurposing

00:16:26,079 --> 00:16:29,360
dax

00:16:27,199 --> 00:16:30,880
and so fixing this discontiguous

00:16:29,360 --> 00:16:34,320
limitation

00:16:30,880 --> 00:16:37,040
and all that left remaining was to

00:16:34,320 --> 00:16:38,959
have a pageless memory map and we still

00:16:37,040 --> 00:16:42,480
keep the same properties behind decks

00:16:38,959 --> 00:16:43,360
so uh static dfm mapping for a given va

00:16:42,480 --> 00:16:46,880
range

00:16:43,360 --> 00:16:50,639
and so uh you still know

00:16:46,880 --> 00:16:53,839
what you will know um

00:16:50,639 --> 00:16:54,399
at device creation what's a va is going

00:16:53,839 --> 00:16:57,279
to

00:16:54,399 --> 00:16:58,079
be mapped to a particular pfm and

00:16:57,279 --> 00:17:01,600
essentially

00:16:58,079 --> 00:17:04,720
the vma type is going to be essentially

00:17:01,600 --> 00:17:05,199
a pfn map which in korvian karma means

00:17:04,720 --> 00:17:08,160
that

00:17:05,199 --> 00:17:09,280
i have no struct pages we leverage a lot

00:17:08,160 --> 00:17:11,439
of the work that by

00:17:09,280 --> 00:17:12,799
kareem hashmap where it introduces an

00:17:11,439 --> 00:17:14,559
alternative

00:17:12,799 --> 00:17:16,079
guest mapping series when memory is not

00:17:14,559 --> 00:17:19,120
paid by struct pages

00:17:16,079 --> 00:17:20,559
and we had to simply fix not in kvm

00:17:19,120 --> 00:17:22,799
we'll average a lot of that works we had

00:17:20,559 --> 00:17:25,280
no changes specific to dax or anything

00:17:22,799 --> 00:17:26,079
was mostly bug fixes which are general

00:17:25,280 --> 00:17:29,600
to

00:17:26,079 --> 00:17:31,200
the usage of pfn maps um

00:17:29,600 --> 00:17:33,039
but we had to support the huge phrases

00:17:31,200 --> 00:17:34,080
for page special this special is how the

00:17:33,039 --> 00:17:37,440
kernel

00:17:34,080 --> 00:17:38,720
says uh this memory does not have a

00:17:37,440 --> 00:17:40,640
strip page

00:17:38,720 --> 00:17:42,960
and finally we have to fix out memory

00:17:40,640 --> 00:17:46,400
failure as the kernel builds out

00:17:42,960 --> 00:17:47,840
early when he has an mcu on memory it

00:17:46,400 --> 00:17:51,679
does not track

00:17:47,840 --> 00:17:54,240
and we had to uh reflect what's uh

00:17:51,679 --> 00:17:54,799
what the actual casual property uh is

00:17:54,240 --> 00:17:56,880
for ram

00:17:54,799 --> 00:17:59,120
and so be able to map it as right back

00:17:56,880 --> 00:18:00,960
as opposed to uncashable

00:17:59,120 --> 00:18:03,120
but that's nothing really different that

00:18:00,960 --> 00:18:06,320
it's not done for death man and

00:18:03,120 --> 00:18:08,799
again there was no logic specific to dex

00:18:06,320 --> 00:18:10,240
uh to make this work so i'd like to

00:18:08,799 --> 00:18:11,360
refer to the previous diagram i

00:18:10,240 --> 00:18:13,360
explained earlier

00:18:11,360 --> 00:18:14,559
where we have all the memory types in

00:18:13,360 --> 00:18:17,120
direct map and

00:18:14,559 --> 00:18:18,720
uh and we're essentially doing here by

00:18:17,120 --> 00:18:22,080
removing struct page we gain

00:18:18,720 --> 00:18:24,640
this memory efficiency back and we move

00:18:22,080 --> 00:18:26,840
other guests memory from the direct map

00:18:24,640 --> 00:18:30,799
so less subject to

00:18:26,840 --> 00:18:33,760
leakage in practice

00:18:30,799 --> 00:18:35,360
uh what we do is you would specify this

00:18:33,760 --> 00:18:39,039
efi fake map

00:18:35,360 --> 00:18:41,760
uh the option as you can tell from what

00:18:39,039 --> 00:18:44,080
it describes it's not really intuitive

00:18:41,760 --> 00:18:46,320
so they still work there to make this

00:18:44,080 --> 00:18:47,600
slightly more user friendly but what we

00:18:46,320 --> 00:18:50,720
are essentially describing

00:18:47,600 --> 00:18:51,360
here is that my hypervisor is going to

00:18:50,720 --> 00:18:55,360
have

00:18:51,360 --> 00:18:59,360
16 gigabytes per node available for

00:18:55,360 --> 00:19:03,679
user space kernel manage allocations

00:18:59,360 --> 00:19:07,120
or and you associate the rest for dax

00:19:03,679 --> 00:19:10,640
and so dex has 368

00:19:07,120 --> 00:19:11,760
gigs per node so you essentially bring

00:19:10,640 --> 00:19:15,520
up two regions

00:19:11,760 --> 00:19:16,960
one predominant on a procfs this appears

00:19:15,520 --> 00:19:19,039
less soft reserved

00:19:16,960 --> 00:19:20,320
and you then supposed to use the dax

00:19:19,039 --> 00:19:22,160
tools to

00:19:20,320 --> 00:19:24,080
instrument this region or you know you

00:19:22,160 --> 00:19:26,960
cannot also come up with the

00:19:24,080 --> 00:19:28,960
your own tools which uses the csfs api

00:19:26,960 --> 00:19:31,200
uh for the purpose

00:19:28,960 --> 00:19:33,200
and you can then create uh various

00:19:31,200 --> 00:19:34,080
devices with you know a 30 gigabytes

00:19:33,200 --> 00:19:37,360
guest with

00:19:34,080 --> 00:19:41,280
uh given huge pages and you select which

00:19:37,360 --> 00:19:42,799
region you want and then optionally what

00:19:41,280 --> 00:19:44,160
we are trying to introduce with pages

00:19:42,799 --> 00:19:45,360
memory is that you pass on this new

00:19:44,160 --> 00:19:48,480
metadata and

00:19:45,360 --> 00:19:51,600
you are not going to create struct pages

00:19:48,480 --> 00:19:53,840
for these devices which

00:19:51,600 --> 00:19:56,480
also tremendously speeds up the bring up

00:19:53,840 --> 00:19:56,480
of the device

00:19:57,679 --> 00:20:00,799
and can you you then use this like any

00:20:00,000 --> 00:20:03,760
other regular

00:20:00,799 --> 00:20:05,120
file-based memory and not there's

00:20:03,760 --> 00:20:08,320
nothing really different there and it's

00:20:05,120 --> 00:20:11,360
the same for ucldfs or any other

00:20:08,320 --> 00:20:11,360
shared memory mechanism

00:20:12,240 --> 00:20:15,440
uh use case that i see for this you

00:20:14,159 --> 00:20:17,760
could let the kvm

00:20:15,440 --> 00:20:20,159
bind to these devices similar to what we

00:20:17,760 --> 00:20:21,120
do for dax kmm where we give back memory

00:20:20,159 --> 00:20:24,960
to the kernel

00:20:21,120 --> 00:20:27,520
but here uh kvm would use it to

00:20:24,960 --> 00:20:29,440
to back some of those data structures

00:20:27,520 --> 00:20:30,240
used when doing work on behalf of the

00:20:29,440 --> 00:20:32,840
guest

00:20:30,240 --> 00:20:34,000
such as we would be hiding the vcp

00:20:32,840 --> 00:20:37,440
registers

00:20:34,000 --> 00:20:39,120
um or kvmyopic and there is a couple of

00:20:37,440 --> 00:20:39,679
call sides i'm just this is just for

00:20:39,120 --> 00:20:41,600
example

00:20:39,679 --> 00:20:43,280
finding purposes it does not need to be

00:20:41,600 --> 00:20:44,640
pageless so long as it's not part of the

00:20:43,280 --> 00:20:47,360
direct map or it's

00:20:44,640 --> 00:20:49,120
isolated in some form but this would be

00:20:47,360 --> 00:20:49,440
one way to implement a poor man version

00:20:49,120 --> 00:20:51,200
of

00:20:49,440 --> 00:20:55,919
process local memory which was at some

00:20:51,200 --> 00:20:58,320
point proposed by some of the aws faults

00:20:55,919 --> 00:20:59,840
the user space another use case could be

00:20:58,320 --> 00:21:03,039
to use this memory for

00:20:59,840 --> 00:21:04,880
any other vmm allocations

00:21:03,039 --> 00:21:06,480
and it could well serve as a memory pool

00:21:04,880 --> 00:21:09,039
as opposed to reserve to anonymous

00:21:06,480 --> 00:21:09,039
allocations

00:21:09,200 --> 00:21:13,200
to recap on some of the advantages by

00:21:11,919 --> 00:21:16,720
removing threat page

00:21:13,200 --> 00:21:20,240
you sort of kill two

00:21:16,720 --> 00:21:23,840
birds in one shot which is

00:21:20,240 --> 00:21:28,400
uh you get a ton of memory back uh

00:21:23,840 --> 00:21:31,520
that is being lost in stock page

00:21:28,400 --> 00:21:33,039
uh and fundamentally because you're the

00:21:31,520 --> 00:21:36,240
current does not map the mapping

00:21:33,039 --> 00:21:40,080
the the that same customer data

00:21:36,240 --> 00:21:42,480
is less prone to leakage by other guests

00:21:40,080 --> 00:21:43,440
you scale this by uh preserving that

00:21:42,480 --> 00:21:46,240
memory across

00:21:43,440 --> 00:21:46,960
uh hypervisor or vmware live a live

00:21:46,240 --> 00:21:49,039
update

00:21:46,960 --> 00:21:51,039
are more easily done fundamentally given

00:21:49,039 --> 00:21:54,400
that how box works and gives

00:21:51,039 --> 00:21:56,880
that control to the application um

00:21:54,400 --> 00:21:58,400
and hunting down spectrum and big assets

00:21:56,880 --> 00:22:01,200
especially those done on

00:21:58,400 --> 00:22:02,080
the context of guest memory gets a lot

00:22:01,200 --> 00:22:05,280
more easily

00:22:02,080 --> 00:22:05,840
mitigated but there are pitfalls in

00:22:05,280 --> 00:22:08,720
doing

00:22:05,840 --> 00:22:09,120
this approach as well and that means

00:22:08,720 --> 00:22:13,039
that

00:22:09,120 --> 00:22:15,600
once you remove struct page

00:22:13,039 --> 00:22:16,400
you're on your own and so subsystems

00:22:15,600 --> 00:22:19,760
don't really

00:22:16,400 --> 00:22:24,400
work well without it and you're

00:22:19,760 --> 00:22:26,000
largely losing certain kernel services

00:22:24,400 --> 00:22:27,600
given that you don't have get to your

00:22:26,000 --> 00:22:28,480
pages can use your pages and so on and

00:22:27,600 --> 00:22:30,880
so forth

00:22:28,480 --> 00:22:32,000
so for example an easy peak is that

00:22:30,880 --> 00:22:34,960
direct io

00:22:32,000 --> 00:22:35,440
and zero copy networking i o doesn't

00:22:34,960 --> 00:22:38,159
work

00:22:35,440 --> 00:22:39,440
uh for example if you send message

00:22:38,159 --> 00:22:42,720
message zero copy

00:22:39,440 --> 00:22:44,720
or if you use or direct um you given

00:22:42,720 --> 00:22:46,880
that get user pages do not return you

00:22:44,720 --> 00:22:50,159
any actual stroke pages

00:22:46,880 --> 00:22:51,919
it you know you will fade the the the i

00:22:50,159 --> 00:22:54,000
o

00:22:51,919 --> 00:22:56,400
this does work for certain specialized

00:22:54,000 --> 00:22:58,880
cases such as the case of kvm

00:22:56,400 --> 00:23:00,960
or if you do basic pci assignment but

00:22:58,880 --> 00:23:04,000
even there there are some issues

00:23:00,960 --> 00:23:04,400
uh in which you need to in addition to

00:23:04,000 --> 00:23:06,480
use

00:23:04,400 --> 00:23:09,280
fall pfn you're expected to track page

00:23:06,480 --> 00:23:11,440
table entry updates

00:23:09,280 --> 00:23:12,320
to reflect that in your secondary mmu

00:23:11,440 --> 00:23:15,600
mapping

00:23:12,320 --> 00:23:17,440
so you usually need to register some

00:23:15,600 --> 00:23:20,320
form of memory notifier

00:23:17,440 --> 00:23:21,440
uh in addition to users for pfm kvm does

00:23:20,320 --> 00:23:24,559
it right

00:23:21,440 --> 00:23:26,480
uh but other subsystems would need so so

00:23:24,559 --> 00:23:27,840
if you're mapping if you're giving a

00:23:26,480 --> 00:23:31,280
device to

00:23:27,840 --> 00:23:35,039
vfio uh it does work today but

00:23:31,280 --> 00:23:36,640
if you invalidate one given va range

00:23:35,039 --> 00:23:38,320
you may want to reflect that into the

00:23:36,640 --> 00:23:41,520
immune

00:23:38,320 --> 00:23:42,650
underlying mappings io does work but

00:23:41,520 --> 00:23:44,240
again is limited to

00:23:42,650 --> 00:23:46,880
[Music]

00:23:44,240 --> 00:23:48,159
uh copy based which is also the default

00:23:46,880 --> 00:23:50,960
in the host net

00:23:48,159 --> 00:23:52,159
vhost does work because the mm owner is

00:23:50,960 --> 00:23:54,799
the same as the vmn

00:23:52,159 --> 00:23:56,880
is the vmm so we just had to come up

00:23:54,799 --> 00:23:59,919
with a little trick with the host

00:23:56,880 --> 00:24:01,440
uh scasi for remote storage where you

00:23:59,919 --> 00:24:04,720
allocate staging buffers

00:24:01,440 --> 00:24:07,840
for um uh

00:24:04,720 --> 00:24:11,200
drive some of that um

00:24:07,840 --> 00:24:13,440
i o but

00:24:11,200 --> 00:24:15,679
there is a big there is a big uh

00:24:13,440 --> 00:24:18,159
drawback which is

00:24:15,679 --> 00:24:19,039
losing kernel services and so what are

00:24:18,159 --> 00:24:22,400
the directions

00:24:19,039 --> 00:24:25,200
we are looking at here naturally this

00:24:22,400 --> 00:24:26,400
uh goes um there are two approaches

00:24:25,200 --> 00:24:28,400
another long-term approach we are

00:24:26,400 --> 00:24:30,880
looking at which is asi which takes a

00:24:28,400 --> 00:24:33,279
safer approach into securing

00:24:30,880 --> 00:24:34,000
a greater portion of what kvm is

00:24:33,279 --> 00:24:35,679
handling

00:24:34,000 --> 00:24:37,039
versus this approach of removing

00:24:35,679 --> 00:24:39,919
structure which is

00:24:37,039 --> 00:24:40,799
the opposite which is you're trying to

00:24:39,919 --> 00:24:43,440
protect

00:24:40,799 --> 00:24:44,880
uh certain chunks of memory but i

00:24:43,440 --> 00:24:48,400
believe this could work

00:24:44,880 --> 00:24:51,600
uh in concert and so you could use

00:24:48,400 --> 00:24:54,240
this mechanism to say protect uh uh

00:24:51,600 --> 00:24:56,480
customers and using asi to protect vmm

00:24:54,240 --> 00:25:00,240
and kernel private details is a better

00:24:56,480 --> 00:25:01,279
that's a better catch-all to what's

00:25:00,240 --> 00:25:03,679
going to be

00:25:01,279 --> 00:25:05,679
um a number of allocations then on

00:25:03,679 --> 00:25:07,679
behalf of the guests

00:25:05,679 --> 00:25:09,279
but maybe the pages could also serve as

00:25:07,679 --> 00:25:11,679
a performance improvement

00:25:09,279 --> 00:25:12,559
say if you're not exiting to user space

00:25:11,679 --> 00:25:13,679
would you need

00:25:12,559 --> 00:25:15,919
could you leave some of these

00:25:13,679 --> 00:25:18,080
mitigations say the nds flash

00:25:15,919 --> 00:25:19,360
if you're not exiting to user space

00:25:18,080 --> 00:25:21,200
could that serve as the performance

00:25:19,360 --> 00:25:24,559
optimization

00:25:21,200 --> 00:25:26,799
also the larger problem at hand here is

00:25:24,559 --> 00:25:26,799
that

00:25:27,200 --> 00:25:31,840
we need to work better uh strike pages

00:25:30,480 --> 00:25:33,840
need to reflect better what the

00:25:31,840 --> 00:25:36,159
underlying size and the page table

00:25:33,840 --> 00:25:37,840
or the alternative is to have subsystems

00:25:36,159 --> 00:25:39,440
work with struct pages

00:25:37,840 --> 00:25:41,679
one good example is the large page in

00:25:39,440 --> 00:25:44,000
the page cache work is that

00:25:41,679 --> 00:25:45,279
you only look at the page head pages to

00:25:44,000 --> 00:25:48,559
compute any

00:25:45,279 --> 00:25:48,559
address uh

00:25:49,279 --> 00:25:53,200
any computations we do on a particular

00:25:51,200 --> 00:25:55,600
address and we don't need to

00:25:53,200 --> 00:25:57,360
use all those tail pages in that sort of

00:25:55,600 --> 00:26:01,039
becomes an implementation

00:25:57,360 --> 00:26:02,000
detail from subsystem or from user

00:26:01,039 --> 00:26:03,919
perspective

00:26:02,000 --> 00:26:05,600
so get user pages for example we just

00:26:03,919 --> 00:26:08,400
return you head pages

00:26:05,600 --> 00:26:09,679
and no tail pages just as a more easy

00:26:08,400 --> 00:26:10,799
example

00:26:09,679 --> 00:26:13,520
but there is also an interesting

00:26:10,799 --> 00:26:17,039
approach which i thought i would mention

00:26:13,520 --> 00:26:20,240
and that happened like a month or so ago

00:26:17,039 --> 00:26:23,120
and that is for external locators like

00:26:20,240 --> 00:26:24,880
hcl bfs and dax i would like to remember

00:26:23,120 --> 00:26:25,520
that dexter also used for pm this is so

00:26:24,880 --> 00:26:27,600
this is

00:26:25,520 --> 00:26:29,440
not only applicable for this but for

00:26:27,600 --> 00:26:31,520
also persistent memory

00:26:29,440 --> 00:26:33,440
but one interesting question raised by

00:26:31,520 --> 00:26:37,120
this bad dance folks is

00:26:33,440 --> 00:26:37,600
what happens if portions of the v-man

00:26:37,120 --> 00:26:41,200
map

00:26:37,600 --> 00:26:44,480
we use the same tail pages sorry

00:26:41,200 --> 00:26:46,799
uh what if all these tail pages

00:26:44,480 --> 00:26:47,760
could use the same big bit memory

00:26:46,799 --> 00:26:51,679
provided that

00:26:47,760 --> 00:26:53,440
you represent in a subset of unique

00:26:51,679 --> 00:26:55,760
stock pages all the information that you

00:26:53,440 --> 00:26:56,480
need for a two megabyte or one gigabyte

00:26:55,760 --> 00:26:58,880
page

00:26:56,480 --> 00:26:59,679
what happens if the remaining ones are

00:26:58,880 --> 00:27:03,520
not needed

00:26:59,679 --> 00:27:03,520
and they all point to the same memory

00:27:03,919 --> 00:27:07,760
that would mean one thing you need less

00:27:06,080 --> 00:27:09,360
memory to make the straight pages you

00:27:07,760 --> 00:27:11,600
still have those track pages

00:27:09,360 --> 00:27:12,480
uh so the it does look like that you

00:27:11,600 --> 00:27:15,600
have one

00:27:12,480 --> 00:27:17,919
uniquely to every 4k chunk the others

00:27:15,600 --> 00:27:21,120
pointing to the same memory

00:27:17,919 --> 00:27:23,840
if such a mechanism was possible uh

00:27:21,120 --> 00:27:25,520
that would be applicable for uclb fs

00:27:23,840 --> 00:27:27,760
index which pre-allocated and re

00:27:25,520 --> 00:27:29,840
pre-assigned chunks at boot or rather

00:27:27,760 --> 00:27:32,320
can pre-assign chunks of boot

00:27:29,840 --> 00:27:32,960
and if decks have support for these more

00:27:32,320 --> 00:27:36,799
page

00:27:32,960 --> 00:27:39,919
compound pages uh it would also serve um

00:27:36,799 --> 00:27:41,760
it's also could also fix other problems

00:27:39,919 --> 00:27:45,600
we have for persistent memory

00:27:41,760 --> 00:27:48,080
uh uh where we would pin faster

00:27:45,600 --> 00:27:48,640
or initialize quicker some of these

00:27:48,080 --> 00:27:51,760
lacks

00:27:48,640 --> 00:27:52,960
some of these namespaces these are also

00:27:51,760 --> 00:27:54,799
something we are looking at at the

00:27:52,960 --> 00:27:55,360
moment and hopefully we can have an

00:27:54,799 --> 00:27:58,799
update

00:27:55,360 --> 00:28:01,919
um in a few weeks

00:27:58,799 --> 00:28:02,240
and with that i'd like to conclude um

00:28:01,919 --> 00:28:04,640
for

00:28:02,240 --> 00:28:06,960
5.10 is going to have a lot of this uh

00:28:04,640 --> 00:28:09,760
repurposing of decks for volatile memory

00:28:06,960 --> 00:28:10,399
and provides a way to carve out struct

00:28:09,760 --> 00:28:14,000
page

00:28:10,399 --> 00:28:17,200
um which fills up

00:28:14,000 --> 00:28:18,000
fits a given use case when your

00:28:17,200 --> 00:28:21,360
hypervisor

00:28:18,000 --> 00:28:23,760
is not using so many this doesn't need

00:28:21,360 --> 00:28:26,559
to provide so many kernel services

00:28:23,760 --> 00:28:27,520
uh the dax huge pages proper support is

00:28:26,559 --> 00:28:29,120
going to continue

00:28:27,520 --> 00:28:31,520
and we are looking at alternatives such

00:28:29,120 --> 00:28:33,919
that we don't have such a big compromise

00:28:31,520 --> 00:28:34,559
into having giving away so many kernel

00:28:33,919 --> 00:28:36,399
services

00:28:34,559 --> 00:28:38,000
and what i was trying to propose here is

00:28:36,399 --> 00:28:39,520
to have sort of a

00:28:38,000 --> 00:28:41,279
harder boundary between what's

00:28:39,520 --> 00:28:42,880
hypervised and what's guess and what's

00:28:41,279 --> 00:28:45,120
gas or customer data

00:28:42,880 --> 00:28:46,720
and at least the lesser known for me was

00:28:45,120 --> 00:28:48,320
that

00:28:46,720 --> 00:28:51,279
when you strip away such according to

00:28:48,320 --> 00:28:54,559
structures as a web page

00:28:51,279 --> 00:28:57,600
it was interesting to note that uh

00:28:54,559 --> 00:28:59,520
not much is needed other when

00:28:57,600 --> 00:29:00,960
your hypervisor doesn't need to provide

00:28:59,520 --> 00:29:03,760
that much uh

00:29:00,960 --> 00:29:05,360
services and with that thank you for

00:29:03,760 --> 00:29:08,480
listening to me some links here

00:29:05,360 --> 00:29:11,360
of some of the work i'm talking about

00:29:08,480 --> 00:29:12,880
i'd like to thank matthew mike kravitz

00:29:11,360 --> 00:29:15,200
uh lehana law and

00:29:12,880 --> 00:29:16,000
nikita as they all were part of this

00:29:15,200 --> 00:29:26,240
work

00:29:16,000 --> 00:29:26,240

YouTube URL: https://www.youtube.com/watch?v=f-0oEc9hyxQ


