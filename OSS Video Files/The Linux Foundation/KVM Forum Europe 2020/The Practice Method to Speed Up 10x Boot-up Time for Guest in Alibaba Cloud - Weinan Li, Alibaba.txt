Title: The Practice Method to Speed Up 10x Boot-up Time for Guest in Alibaba Cloud - Weinan Li, Alibaba
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	The Practice Method to Speed Up 10x Boot-up Time for Guest in Alibaba Cloud - Weinan Li, Alibaba
Captions: 
	00:00:04,960 --> 00:00:09,200
hello everyone

00:00:06,160 --> 00:00:12,080
this is rina from alibaba cloud goten

00:00:09,200 --> 00:00:15,200
and i complete this practice

00:00:12,080 --> 00:00:16,160
i will present this session today our

00:00:15,200 --> 00:00:18,400
topic is

00:00:16,160 --> 00:00:20,000
speed up the boot time of guest in

00:00:18,400 --> 00:00:23,039
alibaba cloud

00:00:20,000 --> 00:00:25,920
this is the agenda of today's session

00:00:23,039 --> 00:00:28,720
first i will introduce the background

00:00:25,920 --> 00:00:31,760
and why do we need to do this

00:00:28,720 --> 00:00:34,559
second i will figure out our solution we

00:00:31,760 --> 00:00:37,360
call it async dme map

00:00:34,559 --> 00:00:40,239
third i will show the guest boot process

00:00:37,360 --> 00:00:42,559
with async dma

00:00:40,239 --> 00:00:43,920
and then i will list several

00:00:42,559 --> 00:00:47,120
optimization design

00:00:43,920 --> 00:00:49,520
for this practice at last

00:00:47,120 --> 00:00:50,879
i will share our achievements with this

00:00:49,520 --> 00:00:54,000
solution

00:00:50,879 --> 00:00:56,960
let's start as a background so

00:00:54,000 --> 00:00:59,440
what's the problem as you know we need

00:00:56,960 --> 00:01:02,320
to dma map all the guest memory

00:00:59,440 --> 00:01:02,719
when there is a pass-through device

00:01:02,320 --> 00:01:04,960
since

00:01:02,719 --> 00:01:06,479
the device is mapped dma to the whole

00:01:04,960 --> 00:01:09,200
gas memory

00:01:06,479 --> 00:01:11,600
and the memory cannot be swapped which

00:01:09,200 --> 00:01:14,080
is dma targeted

00:01:11,600 --> 00:01:15,600
but we don't know what memory is the dma

00:01:14,080 --> 00:01:18,560
target

00:01:15,600 --> 00:01:21,200
so one simple solution is pin map all

00:01:18,560 --> 00:01:23,360
the guest memory

00:01:21,200 --> 00:01:25,360
it might not be a problem when the gas

00:01:23,360 --> 00:01:28,479
memory is very small

00:01:25,360 --> 00:01:32,320
but as you know the memory is not a

00:01:28,479 --> 00:01:34,240
scarce resource today so a guest

00:01:32,320 --> 00:01:36,400
might have a few hundred or more

00:01:34,240 --> 00:01:39,520
gigabytes system memory

00:01:36,400 --> 00:01:41,119
and the dme map is one time consuming

00:01:39,520 --> 00:01:44,000
process

00:01:41,119 --> 00:01:46,000
since that will be a big problem there

00:01:44,000 --> 00:01:48,159
are two charts to show the impact

00:01:46,000 --> 00:01:49,600
on the guest boot and cumule

00:01:48,159 --> 00:01:52,960
initializing time

00:01:49,600 --> 00:01:55,680
along with the vm's memory size increase

00:01:52,960 --> 00:01:56,799
let's start as the left one the

00:01:55,680 --> 00:02:00,159
horizontal

00:01:56,799 --> 00:02:02,159
coordinate is the vm's memory size

00:02:00,159 --> 00:02:03,200
and the vertical one is the boot time

00:02:02,159 --> 00:02:06,240
with the unit

00:02:03,200 --> 00:02:09,759
second you can see if we

00:02:06,240 --> 00:02:14,800
then 8 gigabytes memory to the vm

00:02:09,759 --> 00:02:16,800
the whole boot time is around 20 seconds

00:02:14,800 --> 00:02:19,360
the boot time will increase along with

00:02:16,800 --> 00:02:21,920
the gas memory growing up

00:02:19,360 --> 00:02:22,879
when the gas memory reaches 300

00:02:21,920 --> 00:02:25,680
gigabytes

00:02:22,879 --> 00:02:26,720
the boot time of vm will be upon 2

00:02:25,680 --> 00:02:30,480
minutes

00:02:26,720 --> 00:02:32,319
in this time the users don't know what

00:02:30,480 --> 00:02:34,400
happened in the background

00:02:32,319 --> 00:02:36,400
and they are also not sure if the

00:02:34,400 --> 00:02:39,360
creation is still running

00:02:36,400 --> 00:02:41,680
just worry by the user experience we

00:02:39,360 --> 00:02:44,239
need to figure out the root cause

00:02:41,680 --> 00:02:45,200
then we have the right chart it shows

00:02:44,239 --> 00:02:48,640
the q mu

00:02:45,200 --> 00:02:51,040
initialization time versus memory size

00:02:48,640 --> 00:02:51,680
as you can see most of the boot time is

00:02:51,040 --> 00:02:54,920
in

00:02:51,680 --> 00:02:58,560
commute initialization and most of the

00:02:54,920 --> 00:03:01,280
initialization time is in doing dma map

00:02:58,560 --> 00:03:04,080
before we reach the solution we need to

00:03:01,280 --> 00:03:07,760
know some conditions

00:03:04,080 --> 00:03:08,720
for this problem first it means more

00:03:07,760 --> 00:03:12,000
time cost

00:03:08,720 --> 00:03:15,680
along with more memory second

00:03:12,000 --> 00:03:16,400
no dma no dma mac if there are no

00:03:15,680 --> 00:03:19,280
devices

00:03:16,400 --> 00:03:22,319
need to do dma of course you don't need

00:03:19,280 --> 00:03:22,319
to do dma map

00:03:22,560 --> 00:03:26,799
but there is one important information

00:03:27,200 --> 00:03:30,720
dma only touch a specific range of

00:03:30,159 --> 00:03:33,760
memory

00:03:30,720 --> 00:03:37,200
at a certain time it gives

00:03:33,760 --> 00:03:39,599
us a chance to make optimization

00:03:37,200 --> 00:03:41,599
maybe we don't need to pin map all the

00:03:39,599 --> 00:03:45,280
guest memory during the creation

00:03:41,599 --> 00:03:48,879
of vm based on the conditions

00:03:45,280 --> 00:03:51,599
what options can we have the first thing

00:03:48,879 --> 00:03:54,799
that comes to mind is the virtual iomu

00:03:51,599 --> 00:03:56,640
or pyro virtualization iomu it should be

00:03:54,799 --> 00:04:00,400
a good solution

00:03:56,640 --> 00:04:02,879
but the implementation is very complex

00:04:00,400 --> 00:04:04,720
and it also needs much development

00:04:02,879 --> 00:04:07,360
effort

00:04:04,720 --> 00:04:08,239
so we choose one simple solution we call

00:04:07,360 --> 00:04:12,239
it

00:04:08,239 --> 00:04:15,760
heating dma map there are two key points

00:04:12,239 --> 00:04:17,680
the first one is only map necessary

00:04:15,760 --> 00:04:20,239
memory first

00:04:17,680 --> 00:04:21,759
it ensures the gas operating system boot

00:04:20,239 --> 00:04:24,800
up

00:04:21,759 --> 00:04:27,919
then maps the other memory

00:04:24,800 --> 00:04:28,639
is synchronously in the background it

00:04:27,919 --> 00:04:31,360
might bring

00:04:28,639 --> 00:04:34,000
a little perception to the user but it

00:04:31,360 --> 00:04:36,560
gives better user experience

00:04:34,000 --> 00:04:37,120
this page shows the overview of the

00:04:36,560 --> 00:04:40,320
memory

00:04:37,120 --> 00:04:42,320
sense with a pass-through device first

00:04:40,320 --> 00:04:44,000
let's talk about the current status of

00:04:42,320 --> 00:04:47,680
kvm

00:04:44,000 --> 00:04:50,160
let's use gpu as an example

00:04:47,680 --> 00:04:53,040
the host pin map also gets the memory

00:04:50,160 --> 00:04:56,320
before the guest os boot

00:04:53,040 --> 00:04:58,400
then the gpu driver is loaded and then

00:04:56,320 --> 00:05:00,240
the application generates workloads for

00:04:58,400 --> 00:05:02,080
gpu

00:05:00,240 --> 00:05:03,600
renderable clause comes through gpu

00:05:02,080 --> 00:05:07,600
hardware

00:05:03,600 --> 00:05:10,639
it might trigger dma if the dma address

00:05:07,600 --> 00:05:12,720
hadn't been mapped hardware access error

00:05:10,639 --> 00:05:15,360
will have occurred

00:05:12,720 --> 00:05:16,880
our solution is adding the water balloon

00:05:15,360 --> 00:05:19,919
driver

00:05:16,880 --> 00:05:20,240
the water balloon and gpu driver both

00:05:19,919 --> 00:05:23,360
can

00:05:20,240 --> 00:05:25,600
allocate the system memory if the

00:05:23,360 --> 00:05:26,320
balloon driver is loaded before the tpu

00:05:25,600 --> 00:05:29,840
driver

00:05:26,320 --> 00:05:32,000
it can blow some memory ranges first

00:05:29,840 --> 00:05:34,800
since the gpu driver doesn't have chance

00:05:32,000 --> 00:05:38,960
to allocate from these renders

00:05:34,800 --> 00:05:42,320
dma won't happen in this renders 2

00:05:38,960 --> 00:05:45,520
so it's not necessary to pin map them

00:05:42,320 --> 00:05:49,360
during the creation of guests

00:05:45,520 --> 00:05:51,039
maxim map them asynchronously is fun

00:05:49,360 --> 00:05:53,759
this page shows the architecture

00:05:51,039 --> 00:05:56,800
overview the solution will touch three

00:05:53,759 --> 00:06:00,000
components of kvm motorization

00:05:56,800 --> 00:06:00,960
the first one is qmu it's responsible

00:06:00,000 --> 00:06:03,919
for triggering

00:06:00,960 --> 00:06:05,680
dma map and the balloon change it's also

00:06:03,919 --> 00:06:08,000
responsible for tracking the balloon

00:06:05,680 --> 00:06:11,120
pages

00:06:08,000 --> 00:06:13,440
the second one is the water open driver

00:06:11,120 --> 00:06:15,680
it's responsible for ballooning pages

00:06:13,440 --> 00:06:18,800
and tiring to the host

00:06:15,680 --> 00:06:23,360
the third one is the way aphro driver

00:06:18,800 --> 00:06:23,360
is responsible for doing pin dma map

00:06:23,440 --> 00:06:28,400
in the qmil initialization the backhand

00:06:26,400 --> 00:06:30,560
driver of motile balloon

00:06:28,400 --> 00:06:32,400
in the cumule initialize the balloon

00:06:30,560 --> 00:06:34,319
size of gas

00:06:32,400 --> 00:06:36,880
when the front-end driver of hotel

00:06:34,319 --> 00:06:38,800
balloon in the gas is loaded

00:06:36,880 --> 00:06:40,080
it will query the balloon size

00:06:38,800 --> 00:06:43,440
configuration

00:06:40,080 --> 00:06:46,560
and try to blend to target it needs many

00:06:43,440 --> 00:06:48,960
loops with a little change one time

00:06:46,560 --> 00:06:52,160
every time balloon it will send a

00:06:48,960 --> 00:06:52,160
notification to the host

00:06:52,319 --> 00:06:58,400
the back-end driver receives its

00:06:54,000 --> 00:07:00,400
notifications to track the balloon pages

00:06:58,400 --> 00:07:02,639
they are used for generating the balloon

00:07:00,400 --> 00:07:06,080
page table in the host

00:07:02,639 --> 00:07:09,919
the whole page table is generated

00:07:06,080 --> 00:07:12,160
after it is finished the cumule

00:07:09,919 --> 00:07:14,560
called dme map for the memory ranges

00:07:12,160 --> 00:07:16,319
beyond the pitch table first

00:07:14,560 --> 00:07:20,160
the other memory ranges in the page

00:07:16,319 --> 00:07:22,560
table can be mapped economically

00:07:20,160 --> 00:07:24,720
how about the communication channel of

00:07:22,560 --> 00:07:27,280
water balloon

00:07:24,720 --> 00:07:28,960
this page shows the related functions

00:07:27,280 --> 00:07:31,680
and struct

00:07:28,960 --> 00:07:33,280
the communication channel is ready the

00:07:31,680 --> 00:07:37,520
only thing we need to do

00:07:33,280 --> 00:07:37,520
is recording the blue pages address

00:07:37,599 --> 00:07:41,199
there are two word queues named inflate

00:07:40,400 --> 00:07:44,560
vq

00:07:41,199 --> 00:07:45,840
and deflate vq the front-end driver used

00:07:44,560 --> 00:07:47,280
the inflate vehicle and

00:07:45,840 --> 00:07:49,840
deflate vehicle to send the

00:07:47,280 --> 00:07:52,400
notifications to the host

00:07:49,840 --> 00:07:53,280
one handler is attached to these two

00:07:52,400 --> 00:07:56,240
volt cues

00:07:53,280 --> 00:07:57,199
in the backhand driver that is what i

00:07:56,240 --> 00:08:00,400
open

00:07:57,199 --> 00:08:03,440
handle output we can get

00:08:00,400 --> 00:08:04,400
what kill element in this handler it

00:08:03,440 --> 00:08:06,560
contains

00:08:04,400 --> 00:08:08,000
the guest pi frame and page number

00:08:06,560 --> 00:08:11,120
information

00:08:08,000 --> 00:08:12,879
so everything is ready just add a simple

00:08:11,120 --> 00:08:15,520
recording

00:08:12,879 --> 00:08:17,120
logic this page shows the balloon range

00:08:15,520 --> 00:08:20,080
tracking workflow

00:08:17,120 --> 00:08:21,360
it's also very simple in the input

00:08:20,080 --> 00:08:23,360
process

00:08:21,360 --> 00:08:26,319
the front-hand driver sends a input

00:08:23,360 --> 00:08:28,800
notification by inflate vq

00:08:26,319 --> 00:08:29,919
then the background driver receives it

00:08:28,800 --> 00:08:32,719
and dispatches

00:08:29,919 --> 00:08:33,200
it to the handler the backend driver

00:08:32,719 --> 00:08:36,560
tracks

00:08:33,200 --> 00:08:40,159
all the inflate pages and gets the gpa

00:08:36,560 --> 00:08:43,200
by pfn then adds them into the

00:08:40,159 --> 00:08:46,160
blonde page table

00:08:43,200 --> 00:08:47,760
in the deflate process it's similar to

00:08:46,160 --> 00:08:50,480
inflate

00:08:47,760 --> 00:08:52,480
the only difference is the back-end

00:08:50,480 --> 00:08:55,200
driver needs to remove

00:08:52,480 --> 00:08:57,040
the released pages out from the blue one

00:08:55,200 --> 00:08:59,279
page table

00:08:57,040 --> 00:09:00,720
this page shows one whole picture of the

00:08:59,279 --> 00:09:03,920
guest boot process

00:09:00,720 --> 00:09:07,120
with async dma map

00:09:03,920 --> 00:09:08,880
there are three phases first one is

00:09:07,120 --> 00:09:13,600
initialization

00:09:08,880 --> 00:09:17,120
phase two is dma map asynchronously

00:09:13,600 --> 00:09:19,920
phase three is completion first one

00:09:17,120 --> 00:09:22,640
begins with the cumula initialization

00:09:19,920 --> 00:09:23,760
the cumule initialize the water balloon

00:09:22,640 --> 00:09:27,680
size

00:09:23,760 --> 00:09:30,640
only the necessary memory for the guest

00:09:27,680 --> 00:09:33,760
then it performs dma map below 4

00:09:30,640 --> 00:09:36,320
gigabytes as euro

00:09:33,760 --> 00:09:37,440
then let's turn to the guest wheel the

00:09:36,320 --> 00:09:39,519
gas os

00:09:37,440 --> 00:09:40,800
will enable the water balloon driver

00:09:39,519 --> 00:09:42,560
first

00:09:40,800 --> 00:09:45,040
then it curious the balloon

00:09:42,560 --> 00:09:45,760
configuration and begins to inflate the

00:09:45,040 --> 00:09:50,800
balloon

00:09:45,760 --> 00:09:52,800
to the target it will call field rune to

00:09:50,800 --> 00:09:55,519
allocate pages

00:09:52,800 --> 00:09:57,279
and tell the host the pi fm of the brown

00:09:55,519 --> 00:09:58,959
pages

00:09:57,279 --> 00:10:00,399
the backend driver receives this

00:09:58,959 --> 00:10:03,200
notification and

00:10:00,399 --> 00:10:05,440
use the pfn to generate the balloon page

00:10:03,200 --> 00:10:08,640
table

00:10:05,440 --> 00:10:10,800
after the balloon process is finished

00:10:08,640 --> 00:10:12,640
the host will know all the blue memory

00:10:10,800 --> 00:10:15,360
ranges

00:10:12,640 --> 00:10:17,680
since the balloon memory won't be

00:10:15,360 --> 00:10:20,880
allocated by other devices

00:10:17,680 --> 00:10:23,920
so dma won't happen in this renders

00:10:20,880 --> 00:10:27,200
so qmio can only map the memory renders

00:10:23,920 --> 00:10:31,120
beyond the pivot table then the password

00:10:27,200 --> 00:10:31,120
device driver is loaded as europe

00:10:31,279 --> 00:10:38,320
phase two cumulative deflate

00:10:34,480 --> 00:10:39,760
balloon step by step the frontal driver

00:10:38,320 --> 00:10:42,880
of water balloon

00:10:39,760 --> 00:10:45,279
receives this event will call big

00:10:42,880 --> 00:10:48,720
balloon to deflate

00:10:45,279 --> 00:10:50,720
as same as inflating process cumule can

00:10:48,720 --> 00:10:53,920
receive the different notification

00:10:50,720 --> 00:10:56,800
and get the released pages pfm

00:10:53,920 --> 00:10:57,040
then qmill updates the blue page table

00:10:56,800 --> 00:11:00,560
and

00:10:57,040 --> 00:11:03,519
trigger dma map of the released pages

00:11:00,560 --> 00:11:05,360
after the balloon is empty everything is

00:11:03,519 --> 00:11:07,760
back to normal

00:11:05,360 --> 00:11:09,680
during the practice we met several

00:11:07,760 --> 00:11:12,320
problems

00:11:09,680 --> 00:11:14,240
then we have some optimization design

00:11:12,320 --> 00:11:17,440
for these problems

00:11:14,240 --> 00:11:20,079
the first one is auto combination during

00:11:17,440 --> 00:11:22,959
the inflating process

00:11:20,079 --> 00:11:23,680
the problem is the bloom driver only

00:11:22,959 --> 00:11:27,120
allocate

00:11:23,680 --> 00:11:28,880
one small page at a time understand

00:11:27,120 --> 00:11:32,640
notification to the host

00:11:28,880 --> 00:11:34,560
every one megabytes the cameo will get

00:11:32,640 --> 00:11:37,200
huge number pages

00:11:34,560 --> 00:11:38,480
the best method is combining the

00:11:37,200 --> 00:11:40,880
adjacent pages

00:11:38,480 --> 00:11:43,200
and create bigger memory range in the

00:11:40,880 --> 00:11:46,000
brown page table

00:11:43,200 --> 00:11:47,680
actually most of the memory ranges are

00:11:46,000 --> 00:11:49,760
adjacent

00:11:47,680 --> 00:11:50,800
since the balloon driver is loaded out

00:11:49,760 --> 00:11:54,560
very early

00:11:50,800 --> 00:11:54,560
and most of the memory is free

00:11:54,880 --> 00:12:01,120
after the fleeting process finished

00:11:58,959 --> 00:12:02,160
queue will trigger dma map of all the

00:12:01,120 --> 00:12:05,120
memory ranges

00:12:02,160 --> 00:12:06,000
beyond the pivot table it can reduce the

00:12:05,120 --> 00:12:09,519
dma map

00:12:06,000 --> 00:12:12,800
times second optimization design

00:12:09,519 --> 00:12:15,920
is increasing the balloon pitch sets

00:12:12,800 --> 00:12:19,120
here is the source code in linux kernel

00:12:15,920 --> 00:12:21,040
you can see bloom page log only locate

00:12:19,120 --> 00:12:23,839
one page at a time

00:12:21,040 --> 00:12:24,720
4k page is too small for the current

00:12:23,839 --> 00:12:29,120
virtualization

00:12:24,720 --> 00:12:32,000
environment that will import

00:12:29,120 --> 00:12:32,800
heavy but unnecessary communication

00:12:32,000 --> 00:12:36,560
between guest

00:12:32,800 --> 00:12:37,519
and host if the guest has a few hundred

00:12:36,560 --> 00:12:41,200
or more

00:12:37,519 --> 00:12:42,160
system memory just make a little change

00:12:41,200 --> 00:12:45,600
to use

00:12:42,160 --> 00:12:46,720
lock pages to unlock one large size of

00:12:45,600 --> 00:12:50,240
memory

00:12:46,720 --> 00:12:53,519
for example locating 2 megabytes

00:12:50,240 --> 00:12:53,839
inside a 4 key makes the communication

00:12:53,519 --> 00:12:58,240
much

00:12:53,839 --> 00:13:02,800
more efficient one-time volatile torque

00:12:58,240 --> 00:13:06,240
can inflate or deflate 512

00:13:02,800 --> 00:13:12,079
megabytes memory it can reduce the

00:13:06,240 --> 00:13:12,079
communication frequency significantly

00:13:13,040 --> 00:13:16,399
the third optimization design is

00:13:15,040 --> 00:13:19,360
pre-mapped

00:13:16,399 --> 00:13:20,160
the async dma map can start early

00:13:19,360 --> 00:13:23,680
independent

00:13:20,160 --> 00:13:27,040
of deflating notification qmio triggers

00:13:23,680 --> 00:13:29,760
async dma map step by step if there

00:13:27,040 --> 00:13:31,760
is a new notification from deflate vq

00:13:29,760 --> 00:13:33,200
which contains the released pages

00:13:31,760 --> 00:13:36,079
information

00:13:33,200 --> 00:13:36,800
check if they are in the mapped range if

00:13:36,079 --> 00:13:39,920
not

00:13:36,800 --> 00:13:41,519
then map these pages and give ac key to

00:13:39,920 --> 00:13:44,000
the guest

00:13:41,519 --> 00:13:45,440
this optimization design can speed up

00:13:44,000 --> 00:13:48,720
the async dmv map

00:13:45,440 --> 00:13:49,519
process last let's see what are the

00:13:48,720 --> 00:13:53,199
achievements

00:13:49,519 --> 00:13:55,920
of this practice this test result is

00:13:53,199 --> 00:13:59,600
based on the initialized balloon size

00:13:55,920 --> 00:14:01,360
it's set as 8 gigabytes you can see the

00:13:59,600 --> 00:14:05,360
cumule initialization time

00:14:01,360 --> 00:14:08,320
is still around 7 seconds although the

00:14:05,360 --> 00:14:09,519
gas has more than 300 gigabytes system

00:14:08,320 --> 00:14:11,839
memory

00:14:09,519 --> 00:14:13,519
the watch click amount can return back

00:14:11,839 --> 00:14:16,000
very quickly

00:14:13,519 --> 00:14:19,120
okay then let's see the guest boot time

00:14:16,000 --> 00:14:21,360
versus memory size

00:14:19,120 --> 00:14:25,120
the boot time of gas hasn't increased

00:14:21,360 --> 00:14:25,120
along with the memory size increase

00:14:25,199 --> 00:14:29,440
you can see the boot time keeps around

00:14:28,079 --> 00:14:32,160
20 seconds

00:14:29,440 --> 00:14:34,800
even though this memory is upon 300

00:14:32,160 --> 00:14:34,800
gigabytes

00:14:35,040 --> 00:14:41,160
so the result shows that this practice

00:14:38,720 --> 00:14:43,199
can speed up the boot time forecast

00:14:41,160 --> 00:14:59,839
significantly

00:14:43,199 --> 00:14:59,839
okay that's it thank you

00:15:00,639 --> 00:15:02,720

YouTube URL: https://www.youtube.com/watch?v=NL615LS9obY


