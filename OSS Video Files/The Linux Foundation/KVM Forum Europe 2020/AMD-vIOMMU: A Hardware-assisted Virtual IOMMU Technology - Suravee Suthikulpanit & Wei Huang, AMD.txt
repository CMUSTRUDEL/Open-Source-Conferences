Title: AMD-vIOMMU: A Hardware-assisted Virtual IOMMU Technology - Suravee Suthikulpanit & Wei Huang, AMD
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	AMD-vIOMMU: A Hardware-assisted Virtual IOMMU Technology - Suravee Suthikulpanit & Wei Huang, AMD
Captions: 
	00:00:09,040 --> 00:00:11,519
thank you for joining today's

00:00:10,559 --> 00:00:14,480
presentation

00:00:11,519 --> 00:00:16,080
my name is wei huang today suavi and i

00:00:14,480 --> 00:00:19,199
are going to talk about

00:00:16,080 --> 00:00:19,920
a new hardware-assisted v-io memory

00:00:19,199 --> 00:00:23,119
technology

00:00:19,920 --> 00:00:23,119
offered by amd

00:00:24,480 --> 00:00:27,760
here is the agenda for today's

00:00:26,080 --> 00:00:29,439
presentation

00:00:27,760 --> 00:00:30,960
at the beginning i will give a brief

00:00:29,439 --> 00:00:34,000
overview about amd

00:00:30,960 --> 00:00:38,320
iomu and how does dma remap works

00:00:34,000 --> 00:00:40,960
in this hardware for vio immunosupport

00:00:38,320 --> 00:00:43,200
inside a guest vm there are two type of

00:00:40,960 --> 00:00:43,200
the

00:00:43,440 --> 00:00:48,000
vi or menu the first one is qemu

00:00:46,320 --> 00:00:51,760
emulated vr menu

00:00:48,000 --> 00:00:53,520
we also know as software vio menu

00:00:51,760 --> 00:00:54,879
and the second one is the call of

00:00:53,520 --> 00:00:59,600
today's presentation

00:00:54,879 --> 00:01:02,239
is a hardware assisted amd vi vio memu

00:00:59,600 --> 00:01:03,280
in the rest of presentations ravi will

00:01:02,239 --> 00:01:06,159
talk

00:01:03,280 --> 00:01:07,760
in details about the hardware changes

00:01:06,159 --> 00:01:11,119
for

00:01:07,760 --> 00:01:13,119
amd vi or memo and also the software

00:01:11,119 --> 00:01:15,439
changes in different system software

00:01:13,119 --> 00:01:18,720
component we are proposing

00:01:15,439 --> 00:01:22,400
at the end we'll open up the summary

00:01:18,720 --> 00:01:22,400
and for the discussion

00:01:24,640 --> 00:01:31,680
okay so this slide shows you the amd

00:01:28,080 --> 00:01:34,960
ioma mu design as you can see

00:01:31,680 --> 00:01:38,000
on the right hand side in the diagram

00:01:34,960 --> 00:01:38,960
the hardware use a lot of different data

00:01:38,000 --> 00:01:42,320
structures

00:01:38,960 --> 00:01:46,640
for the iowa memory management

00:01:42,320 --> 00:01:51,920
and the main registers

00:01:46,640 --> 00:01:55,840
are inside for two 4kb mmr region

00:01:51,920 --> 00:01:57,360
the first 4kb contains for example base

00:01:55,840 --> 00:02:00,399
and config register

00:01:57,360 --> 00:02:02,000
those register point to the starting

00:02:00,399 --> 00:02:06,880
address of those

00:02:02,000 --> 00:02:08,959
data structure and the second 4kb

00:02:06,880 --> 00:02:10,560
mmr region contains head and tail

00:02:08,959 --> 00:02:13,680
register

00:02:10,560 --> 00:02:16,959
that point into inside those

00:02:13,680 --> 00:02:18,720
data structure for example the

00:02:16,959 --> 00:02:22,560
head and tail register into command

00:02:18,720 --> 00:02:25,360
buffer and ppr logs

00:02:22,560 --> 00:02:26,800
the main functionality for iom mmu is to

00:02:25,360 --> 00:02:30,800
do the dma remap

00:02:26,800 --> 00:02:31,760
so mdio memo support two type of io page

00:02:30,800 --> 00:02:36,319
table

00:02:31,760 --> 00:02:39,680
the first one is host io page table this

00:02:36,319 --> 00:02:43,120
is being used currently by

00:02:39,680 --> 00:02:45,599
linux dma api and vfio

00:02:43,120 --> 00:02:47,680
we call this page table v1 page table

00:02:45,599 --> 00:02:50,160
and this table does a translation from

00:02:47,680 --> 00:02:52,160
gpa to spa

00:02:50,160 --> 00:02:55,120
and the second page table is a little

00:02:52,160 --> 00:02:59,360
bit different it's called v2 table

00:02:55,120 --> 00:03:03,200
and it does support the x86

00:02:59,360 --> 00:03:07,440
cpu page table format this page table is

00:03:03,200 --> 00:03:07,440
currently used by linux kfd driver

00:03:10,879 --> 00:03:16,000
so as we know iomu offer a lot of

00:03:13,440 --> 00:03:18,239
benefits

00:03:16,000 --> 00:03:20,800
you can do device pass-through you can

00:03:18,239 --> 00:03:24,239
enhance the io security

00:03:20,800 --> 00:03:28,640
uh by using dma isolation

00:03:24,239 --> 00:03:29,280
so over time end user or customer always

00:03:28,640 --> 00:03:31,440
want

00:03:29,280 --> 00:03:33,440
to enable this feature for their guest

00:03:31,440 --> 00:03:35,920
vm

00:03:33,440 --> 00:03:36,959
to support this feature over the past

00:03:35,920 --> 00:03:39,760
several years

00:03:36,959 --> 00:03:40,560
the community has come up with different

00:03:39,760 --> 00:03:43,200
solutions

00:03:40,560 --> 00:03:44,080
based on different device model for

00:03:43,200 --> 00:03:47,280
example

00:03:44,080 --> 00:03:50,560
we do have intel io mmu

00:03:47,280 --> 00:03:53,360
support in the qemu similarly for arm

00:03:50,560 --> 00:03:54,879
smemu we have the same capability

00:03:53,360 --> 00:03:58,080
capability

00:03:54,879 --> 00:04:01,040
for mdi or mmu we do support

00:03:58,080 --> 00:04:05,120
pass-through for the emulator device

00:04:01,040 --> 00:04:08,720
but for the vfi or pci password

00:04:05,120 --> 00:04:11,200
this project is still working progress

00:04:08,720 --> 00:04:14,400
recently we just submit a series of

00:04:11,200 --> 00:04:17,440
patches to enable this feature

00:04:14,400 --> 00:04:22,079
to use this feature is very simple

00:04:17,440 --> 00:04:25,120
when you start the qemi command you just

00:04:22,079 --> 00:04:29,040
specify amd io manual device

00:04:25,120 --> 00:04:32,240
and added the vfi or pci host device

00:04:29,040 --> 00:04:35,120
and attached to this mdio menu

00:04:32,240 --> 00:04:35,120
and that should be it

00:04:37,919 --> 00:04:43,280
for parcel device inside a guest vm

00:04:41,040 --> 00:04:44,560
they can actually be classified into two

00:04:43,280 --> 00:04:46,400
categories

00:04:44,560 --> 00:04:48,560
in the next slide i will talk about why

00:04:46,400 --> 00:04:51,199
we

00:04:48,560 --> 00:04:52,400
differentiate them from each other the

00:04:51,199 --> 00:04:55,520
first category

00:04:52,400 --> 00:04:56,720
is called emulated pci device for

00:04:55,520 --> 00:05:00,160
example

00:04:56,720 --> 00:05:04,639
like intel e1000 nic

00:05:00,160 --> 00:05:08,000
right inside your guest vm

00:05:04,639 --> 00:05:08,639
and the second one is pci password

00:05:08,000 --> 00:05:12,000
device

00:05:08,639 --> 00:05:13,680
like vfio password device for example

00:05:12,000 --> 00:05:16,240
you want to pass through a 40 gig

00:05:13,680 --> 00:05:17,039
nic into your guest vm and in the guest

00:05:16,240 --> 00:05:20,080
vm you

00:05:17,039 --> 00:05:23,280
want this device be managed by amd iom

00:05:20,080 --> 00:05:26,560
wheel driver both device

00:05:23,280 --> 00:05:31,039
are supported and for the

00:05:26,560 --> 00:05:31,039
dma remapping and interrupt remapping

00:05:31,120 --> 00:05:36,880
next one but

00:05:34,320 --> 00:05:37,440
the implementation for dma remapping

00:05:36,880 --> 00:05:41,199
actually

00:05:37,440 --> 00:05:44,080
a little bit different as you can see

00:05:41,199 --> 00:05:45,840
on the right hand side for the emulated

00:05:44,080 --> 00:05:48,479
pci device

00:05:45,840 --> 00:05:50,800
all the dma coming up will be managed by

00:05:48,479 --> 00:05:53,840
emulated iomu

00:05:50,800 --> 00:05:56,319
and this immigrated irma maintained

00:05:53,840 --> 00:05:57,120
an emulated host table that does a

00:05:56,319 --> 00:06:00,560
translation

00:05:57,120 --> 00:06:04,160
from guest iova to gpa

00:06:00,560 --> 00:06:07,440
but for pci password device because

00:06:04,160 --> 00:06:10,800
this device actually is managed by host

00:06:07,440 --> 00:06:13,280
iomu the real io memory hardware

00:06:10,800 --> 00:06:14,479
the software needs to create a shutter

00:06:13,280 --> 00:06:18,840
host table

00:06:14,479 --> 00:06:20,479
that does a translation from guest io to

00:06:18,840 --> 00:06:23,199
spa

00:06:20,479 --> 00:06:26,479
obviously this shuttle host table

00:06:23,199 --> 00:06:29,039
maintenance creates some performance hit

00:06:26,479 --> 00:06:31,280
imagine that anytime the guest io memory

00:06:29,039 --> 00:06:34,319
driver updates its page table

00:06:31,280 --> 00:06:35,360
we need to reflect that change in the

00:06:34,319 --> 00:06:38,720
whole

00:06:35,360 --> 00:06:40,720
shuttle host table used by the real

00:06:38,720 --> 00:06:43,440
hardware

00:06:40,720 --> 00:06:44,240
in the meanwhile there are other

00:06:43,440 --> 00:06:48,319
performance

00:06:44,240 --> 00:06:54,000
hit for example we need to emulate

00:06:48,319 --> 00:06:54,000
guest iom driver access to sma mio

00:06:54,160 --> 00:06:57,919
on top of that uh the io memory command

00:06:56,880 --> 00:07:01,919
processing

00:06:57,919 --> 00:07:04,400
and even log event and ppr log access

00:07:01,919 --> 00:07:07,599
also needs to be emulated

00:07:04,400 --> 00:07:09,520
combining those things together we can

00:07:07,599 --> 00:07:12,720
imagine that the performance for pci

00:07:09,520 --> 00:07:12,720
password is not great

00:07:13,440 --> 00:07:20,160
next one so

00:07:16,960 --> 00:07:23,039
to solve this problem amd come up with

00:07:20,160 --> 00:07:23,919
a new hardware based approach it's

00:07:23,039 --> 00:07:27,919
called

00:07:23,919 --> 00:07:28,400
hardware vio mmu this hardware new

00:07:27,919 --> 00:07:31,120
feature

00:07:28,400 --> 00:07:31,599
tries to solve the problem we just

00:07:31,120 --> 00:07:33,919
mentioned

00:07:31,599 --> 00:07:34,960
in the last slide for example for shadow

00:07:33,919 --> 00:07:37,520
host table

00:07:34,960 --> 00:07:38,000
we want to utilize the nested io page

00:07:37,520 --> 00:07:41,599
table

00:07:38,000 --> 00:07:43,919
instead and for the mmr register access

00:07:41,599 --> 00:07:44,879
we want to allow guests to directly

00:07:43,919 --> 00:07:49,120
access those

00:07:44,879 --> 00:07:52,800
registers in the meanwhile

00:07:49,120 --> 00:07:55,919
the hardware the new feature

00:07:52,800 --> 00:07:58,400
create a private mapping allow the

00:07:55,919 --> 00:08:00,720
hardware to access guest io mmu command

00:07:58,400 --> 00:08:05,039
directory

00:08:00,720 --> 00:08:07,520
so those new hardware features

00:08:05,039 --> 00:08:08,639
will solve the problem we just mentioned

00:08:07,520 --> 00:08:12,400
in the last slides

00:08:08,639 --> 00:08:13,919
and well beneficial for end users who

00:08:12,400 --> 00:08:17,280
want to pass through the

00:08:13,919 --> 00:08:19,759
native device into the gas sphere

00:08:17,280 --> 00:08:20,479
there's ongoing development effort

00:08:19,759 --> 00:08:22,720
around

00:08:20,479 --> 00:08:23,759
different areas in the system software

00:08:22,720 --> 00:08:26,080
components

00:08:23,759 --> 00:08:28,479
which will be covered by sravi in detail

00:08:26,080 --> 00:08:28,479
later on

00:08:31,680 --> 00:08:37,360
i want to go back a little bit about

00:08:33,599 --> 00:08:41,760
nested io page table support

00:08:37,360 --> 00:08:44,080
so this nested io page support

00:08:41,760 --> 00:08:44,959
is different from shadow host page table

00:08:44,080 --> 00:08:48,000
support

00:08:44,959 --> 00:08:49,920
uh i just mentioned in the last two

00:08:48,000 --> 00:08:53,680
slides

00:08:49,920 --> 00:08:56,720
in this new design the gas

00:08:53,680 --> 00:08:59,120
iommu driver will use the

00:08:56,720 --> 00:09:03,120
v2 table the guest table that does the

00:08:59,120 --> 00:09:06,080
translation from guess iova to gpa

00:09:03,120 --> 00:09:08,000
and the host iommu actually will use a

00:09:06,080 --> 00:09:12,080
host table that does a translation

00:09:08,000 --> 00:09:12,640
from gpa and spa so you can imagine that

00:09:12,080 --> 00:09:15,120
this

00:09:12,640 --> 00:09:17,440
nested io page table is very similar to

00:09:15,120 --> 00:09:20,959
cpu side of nested paging

00:09:17,440 --> 00:09:22,480
so we expect this will be able to solve

00:09:20,959 --> 00:09:28,160
some performance

00:09:22,480 --> 00:09:28,160
hit we mentioned in the previous slides

00:09:30,240 --> 00:09:36,000
now putting these two design together we

00:09:33,200 --> 00:09:38,080
create a hybrid system model

00:09:36,000 --> 00:09:40,399
depending what kind of pass-through

00:09:38,080 --> 00:09:43,680
device you're going to use

00:09:40,399 --> 00:09:45,760
if you want to use emulated pci device

00:09:43,680 --> 00:09:47,120
we will go back to use the current

00:09:45,760 --> 00:09:50,560
software-based

00:09:47,120 --> 00:09:51,440
approach the emulated iomu that goes dma

00:09:50,560 --> 00:09:54,240
goes through the

00:09:51,440 --> 00:09:57,040
emulated host table as shown on the

00:09:54,240 --> 00:10:00,160
right hand side the top part

00:09:57,040 --> 00:10:01,040
if end user wants to pass through a pci

00:10:00,160 --> 00:10:03,519
device

00:10:01,040 --> 00:10:05,600
then they will use a nested io page

00:10:03,519 --> 00:10:10,000
table

00:10:05,600 --> 00:10:13,920
and that requires adding a new

00:10:10,000 --> 00:10:13,920
amdvi remember device model

00:10:14,000 --> 00:10:18,959
so by far i just give you a brief

00:10:16,720 --> 00:10:22,000
introduction about the amdio menu

00:10:18,959 --> 00:10:23,360
and the hardware vi or menu the rest of

00:10:22,000 --> 00:10:25,120
presentations ravi

00:10:23,360 --> 00:10:27,360
will talk about the detailed hardware

00:10:25,120 --> 00:10:29,920
design and the changes

00:10:27,360 --> 00:10:31,360
uh proposed in different system software

00:10:29,920 --> 00:10:33,839
component

00:10:31,360 --> 00:10:33,839
so are we

00:10:34,640 --> 00:10:40,720
hi so in the next section i will start

00:10:38,560 --> 00:10:44,880
discussing the detail of the changes for

00:10:40,720 --> 00:10:44,880
the hardware assisted vio menu feature

00:10:45,680 --> 00:10:48,959
starting with the hardware changes we're

00:10:47,920 --> 00:10:52,160
introducing the

00:10:48,959 --> 00:10:55,200
iomu private address space

00:10:52,160 --> 00:10:56,320
this address space is used by the io

00:10:55,200 --> 00:10:59,920
memo hardware

00:10:56,320 --> 00:11:01,440
to access per guest iomu data structures

00:10:59,920 --> 00:11:04,320
on the left hand side

00:11:01,440 --> 00:11:06,480
there's a diagram showing the layout of

00:11:04,320 --> 00:11:08,959
the private address space

00:11:06,480 --> 00:11:09,600
and you can see starting from the top we

00:11:08,959 --> 00:11:13,200
have the

00:11:09,600 --> 00:11:15,360
event and ppr lock the command buffer

00:11:13,200 --> 00:11:17,360
and the guest mmio registers at the

00:11:15,360 --> 00:11:21,600
bottom

00:11:17,360 --> 00:11:23,440
also it is used to access the viomu

00:11:21,600 --> 00:11:26,399
specific data structures

00:11:23,440 --> 00:11:28,959
which are the domain id mapping table

00:11:26,399 --> 00:11:30,320
that maps the host domain id to guest

00:11:28,959 --> 00:11:32,160
domain id

00:11:30,320 --> 00:11:34,079
and the device id mapping table that

00:11:32,160 --> 00:11:36,959
maps the host device id

00:11:34,079 --> 00:11:38,480
to the guest device id and this is for

00:11:36,959 --> 00:11:42,240
the password device

00:11:38,480 --> 00:11:46,000
the pci password that used the vfio

00:11:42,240 --> 00:11:49,279
also another data structure is the

00:11:46,000 --> 00:11:51,600
command buffer dirty status table

00:11:49,279 --> 00:11:53,360
and all the mapping here is being done

00:11:51,600 --> 00:11:57,120
using the

00:11:53,360 --> 00:11:57,680
iomu v1 page table that maps the private

00:11:57,120 --> 00:12:00,720
address

00:11:57,680 --> 00:12:01,360
to the system physical address and it

00:12:00,720 --> 00:12:05,040
can support

00:12:01,360 --> 00:12:05,040
up to 64k vms

00:12:06,480 --> 00:12:10,639
next hardware changes are the

00:12:08,720 --> 00:12:14,880
introduction of

00:12:10,639 --> 00:12:17,760
the vf and vf control mmio bars

00:12:14,880 --> 00:12:19,279
so the bars are used mainly by the

00:12:17,760 --> 00:12:22,560
hypervisor to access

00:12:19,279 --> 00:12:25,440
the per gas mmi oh registers

00:12:22,560 --> 00:12:25,920
as we mentioned earlier we have two set

00:12:25,440 --> 00:12:29,440
of

00:12:25,920 --> 00:12:32,079
mmio registers one is used for control

00:12:29,440 --> 00:12:33,680
and one is and another one is used for

00:12:32,079 --> 00:12:36,880
hit and till pointers of

00:12:33,680 --> 00:12:39,519
the data structure inside iomu

00:12:36,880 --> 00:12:42,959
so the control one is is basically

00:12:39,519 --> 00:12:46,600
accessed via the vf control mmo bar

00:12:42,959 --> 00:12:50,480
the pointers ones are accessed using the

00:12:46,600 --> 00:12:52,399
vfml bar and the diagram on the right

00:12:50,480 --> 00:12:55,600
hand side just show the breakdown

00:12:52,399 --> 00:12:58,639
of the the regions of the bar

00:12:55,600 --> 00:13:02,880
you can see that it split into different

00:12:58,639 --> 00:13:07,200
4k regions indexed by the guest id

00:13:02,880 --> 00:13:10,320
same for dbf control mmio

00:13:07,200 --> 00:13:11,920
so next hardware change are the

00:13:10,320 --> 00:13:16,240
introduction of the new

00:13:11,920 --> 00:13:16,240
iomu commands and events

00:13:16,399 --> 00:13:22,560
for the event typically when an iomu

00:13:19,600 --> 00:13:23,680
encounter an arrow it will generate

00:13:22,560 --> 00:13:28,160
event locks

00:13:23,680 --> 00:13:32,839
into the event buffer

00:13:28,160 --> 00:13:34,000
and in this case when we have the guest

00:13:32,839 --> 00:13:36,320
iomu

00:13:34,000 --> 00:13:39,440
errors inside the guest will actually be

00:13:36,320 --> 00:13:42,480
locked in the host iomu

00:13:39,440 --> 00:13:44,800
so to be able to do

00:13:42,480 --> 00:13:46,639
in order to be able to identify that the

00:13:44,800 --> 00:13:49,199
event belongs to the guest

00:13:46,639 --> 00:13:51,680
we're introducing bit fields for the

00:13:49,199 --> 00:13:54,800
existing i o mmu events

00:13:51,680 --> 00:13:54,800
and it's listed here

00:13:55,040 --> 00:14:01,279
so next will be

00:13:58,720 --> 00:14:02,880
the i o maybe host driver will process

00:14:01,279 --> 00:14:05,519
the event lock

00:14:02,880 --> 00:14:06,399
and if it wants to inject that event

00:14:05,519 --> 00:14:09,199
into the guest

00:14:06,399 --> 00:14:10,959
it can do so using the insert guest

00:14:09,199 --> 00:14:14,079
event command

00:14:10,959 --> 00:14:15,519
which will place the event into the

00:14:14,079 --> 00:14:19,040
guest

00:14:15,519 --> 00:14:22,720
event lock and

00:14:19,040 --> 00:14:25,279
another scenario is when we run into

00:14:22,720 --> 00:14:26,560
errors that related to viomu

00:14:25,279 --> 00:14:29,519
specifically

00:14:26,560 --> 00:14:31,279
the highway will lock the viomu highway

00:14:29,519 --> 00:14:34,639
error event

00:14:31,279 --> 00:14:35,360
and usually this will cause the guests

00:14:34,639 --> 00:14:38,560
to

00:14:35,360 --> 00:14:40,480
to fail and when we try to

00:14:38,560 --> 00:14:42,560
re-initialize and recover from that

00:14:40,480 --> 00:14:45,680
failure

00:14:42,560 --> 00:14:49,519
there's a command to reset the vmmio

00:14:45,680 --> 00:14:52,639
um sorry there's a command

00:14:49,519 --> 00:14:58,000
called reset vmmio that will reset the

00:14:52,639 --> 00:15:01,279
guess mmo registers

00:14:58,000 --> 00:15:05,199
next are changes to the

00:15:01,279 --> 00:15:07,199
host iommu driver starting from the boot

00:15:05,199 --> 00:15:09,680
time initialization

00:15:07,199 --> 00:15:12,160
first we add the logic to detect and

00:15:09,680 --> 00:15:13,600
enable the vio menu feature in the

00:15:12,160 --> 00:15:17,360
hardware

00:15:13,600 --> 00:15:20,880
then we set up the io menu v1 page table

00:15:17,360 --> 00:15:20,880
for the private address mapping

00:15:22,240 --> 00:15:28,240
the first part also

00:15:25,440 --> 00:15:29,120
require a locating map i'm sorry

00:15:28,240 --> 00:15:32,399
allocating

00:15:29,120 --> 00:15:34,959
and map the the tables that listed here

00:15:32,399 --> 00:15:36,800
we have the domain id table device id

00:15:34,959 --> 00:15:40,320
table and the command

00:15:36,800 --> 00:15:43,279
buffer dirty bit um status table

00:15:40,320 --> 00:15:45,920
and those those are listed um on the

00:15:43,279 --> 00:15:48,480
right hand side here for the boot time

00:15:45,920 --> 00:15:49,040
then we have the per vm initialization

00:15:48,480 --> 00:15:53,680
code

00:15:49,040 --> 00:15:57,279
that is going to be used when we

00:15:53,680 --> 00:16:00,399
launch the vm and basically it will

00:15:57,279 --> 00:16:01,360
needs to create mapping for the private

00:16:00,399 --> 00:16:04,560
address

00:16:01,360 --> 00:16:07,120
to the spa mapping and that's

00:16:04,560 --> 00:16:08,720
basically the black arrows that shows

00:16:07,120 --> 00:16:11,360
here for event

00:16:08,720 --> 00:16:14,000
ppl lock for command buffer for guest

00:16:11,360 --> 00:16:16,639
mmi registers

00:16:14,000 --> 00:16:18,000
also we need to do host to guest mapping

00:16:16,639 --> 00:16:21,040
for the device id

00:16:18,000 --> 00:16:22,320
and the domain id and we also need to

00:16:21,040 --> 00:16:24,959
trap

00:16:22,320 --> 00:16:25,680
set up for trapping when the guests try

00:16:24,959 --> 00:16:28,560
to access

00:16:25,680 --> 00:16:30,560
the first 4k of the mmo mmio region

00:16:28,560 --> 00:16:33,600
which is the control regions

00:16:30,560 --> 00:16:33,600
of the mi mio

00:16:33,759 --> 00:16:36,959
also we need to add code for supporting

00:16:36,000 --> 00:16:40,800
new i own

00:16:36,959 --> 00:16:40,800
iommu commands and events

00:16:41,839 --> 00:16:46,240
so the next part are changes to the qvm

00:16:44,639 --> 00:16:49,519
view

00:16:46,240 --> 00:16:52,880
basically the changes are introducing

00:16:49,519 --> 00:16:54,000
the new hardware vi or memory device

00:16:52,880 --> 00:16:56,160
model

00:16:54,000 --> 00:16:57,040
which is different than the current

00:16:56,160 --> 00:17:00,160
software

00:16:57,040 --> 00:17:02,240
emulated vio memory model so

00:17:00,160 --> 00:17:04,160
on the right hand side shows an example

00:17:02,240 --> 00:17:07,760
of a guest vm

00:17:04,160 --> 00:17:11,439
where we pass through three next devices

00:17:07,760 --> 00:17:15,039
um nik one and x0 is part of the

00:17:11,439 --> 00:17:17,600
i o mu0 and nix 4 is actually on

00:17:15,039 --> 00:17:20,240
i o memory 1. so when we pass through

00:17:17,600 --> 00:17:23,439
this three nicks into a same guess

00:17:20,240 --> 00:17:25,039
we actually need to create two viomu

00:17:23,439 --> 00:17:28,480
instances

00:17:25,039 --> 00:17:30,160
and associate them accordingly because

00:17:28,480 --> 00:17:32,080
the hardware has to be able to

00:17:30,160 --> 00:17:36,080
virtualize all the command buffers

00:17:32,080 --> 00:17:39,440
and all the data structure belongs to um

00:17:36,080 --> 00:17:42,799
that that nick is associated to

00:17:39,440 --> 00:17:44,400
um so this require changes to several

00:17:42,799 --> 00:17:46,880
things in qmu

00:17:44,400 --> 00:17:48,240
the first thing is we need to be able to

00:17:46,880 --> 00:17:51,280
specify more than one

00:17:48,240 --> 00:17:53,600
i o mu in a guess currently with the

00:17:51,280 --> 00:17:58,000
emulated io memo we only can support

00:17:53,600 --> 00:18:00,240
one we also need to be able to specify

00:17:58,000 --> 00:18:03,280
pci topology for the vm and

00:18:00,240 --> 00:18:05,440
associative associativity

00:18:03,280 --> 00:18:06,320
with the device for example at the

00:18:05,440 --> 00:18:09,679
bottom here

00:18:06,320 --> 00:18:12,960
shows the command for qemu to launch

00:18:09,679 --> 00:18:15,919
the guest with two vio memo

00:18:12,960 --> 00:18:20,320
the red one is for the i o menu zero and

00:18:15,919 --> 00:18:20,320
the green one is for the i o m u one

00:18:23,520 --> 00:18:26,960
and to be able to support that we also

00:18:26,080 --> 00:18:30,080
introduced

00:18:26,960 --> 00:18:33,280
new device fs and iocto interface

00:18:30,080 --> 00:18:34,720
the new device surface is called amd vio

00:18:33,280 --> 00:18:38,000
mmu

00:18:34,720 --> 00:18:42,720
and this is implemented by

00:18:38,000 --> 00:18:45,679
the iomu host driver also the new

00:18:42,720 --> 00:18:46,799
um vio menu specific iot interface

00:18:45,679 --> 00:18:48,799
listed here

00:18:46,799 --> 00:18:51,039
to support all the operations that's

00:18:48,799 --> 00:18:54,160
required for setting up vm's

00:18:51,039 --> 00:18:56,880
device domain and all the memory

00:18:54,160 --> 00:18:59,280
mmo access that's that's needed to be

00:18:56,880 --> 00:18:59,280
trapped

00:19:01,840 --> 00:19:06,960
the last part is the change for the

00:19:04,640 --> 00:19:10,799
guest io memory driver

00:19:06,960 --> 00:19:13,760
as we mentioned we now tr

00:19:10,799 --> 00:19:15,440
making use of the nested io page table

00:19:13,760 --> 00:19:19,039
which required

00:19:15,440 --> 00:19:23,039
the guest to be using the v2 table

00:19:19,039 --> 00:19:26,080
to do the g the guest iova

00:19:23,039 --> 00:19:28,640
translation to gpa

00:19:26,080 --> 00:19:29,520
because the first table of the sorry the

00:19:28,640 --> 00:19:32,320
v1 table

00:19:29,520 --> 00:19:34,080
is already being used by the vfio to do

00:19:32,320 --> 00:19:37,440
gpa to spa

00:19:34,080 --> 00:19:38,640
the problem is currently for i o memory

00:19:37,440 --> 00:19:42,880
driver

00:19:38,640 --> 00:19:46,000
we only support v1 table for dma api

00:19:42,880 --> 00:19:48,880
so we need to make some changes here

00:19:46,000 --> 00:19:50,799
for the guest driver and the changes are

00:19:48,880 --> 00:19:53,280
split into two part

00:19:50,799 --> 00:19:54,000
first is to refactor the current code to

00:19:53,280 --> 00:19:56,080
use the

00:19:54,000 --> 00:19:57,200
generic io page table framework that has

00:19:56,080 --> 00:19:59,679
been introduced

00:19:57,200 --> 00:20:01,360
and currently used by arm the code has

00:19:59,679 --> 00:20:02,559
been submitted upstream already for

00:20:01,360 --> 00:20:04,640
review

00:20:02,559 --> 00:20:06,720
next part will be adding support for the

00:20:04,640 --> 00:20:09,760
page table

00:20:06,720 --> 00:20:12,400
sorry for the v2 page table for dma api

00:20:09,760 --> 00:20:14,400
and it's also going to be using the same

00:20:12,400 --> 00:20:17,440
generic io page table framework

00:20:14,400 --> 00:20:21,280
this is work in progress

00:20:17,440 --> 00:20:24,480
so to summary so far we've been

00:20:21,280 --> 00:20:26,720
comparing the software versus the

00:20:24,480 --> 00:20:31,039
hardware v i o m u

00:20:26,720 --> 00:20:35,200
and try to show how the hardware vi menu

00:20:31,039 --> 00:20:36,080
supposed to help improve performance for

00:20:35,200 --> 00:20:37,679
the

00:20:36,080 --> 00:20:39,919
guest i o menu for the pass-through

00:20:37,679 --> 00:20:42,159
device by

00:20:39,919 --> 00:20:43,679
instead of using the host page table

00:20:42,159 --> 00:20:45,520
we're now going to be using the nested

00:20:43,679 --> 00:20:48,799
io page table

00:20:45,520 --> 00:20:49,520
and now guests can directly access mmo

00:20:48,799 --> 00:20:51,360
register

00:20:49,520 --> 00:20:53,120
instead of having to be emulated by the

00:20:51,360 --> 00:20:56,480
hypervisor

00:20:53,120 --> 00:20:58,240
um last part is all the emulation that

00:20:56,480 --> 00:21:00,880
needs to be done for command buffer

00:20:58,240 --> 00:21:05,039
event buffer and ppr buffer now it's

00:21:00,880 --> 00:21:10,320
being escalated by the hardware

00:21:05,039 --> 00:21:10,320
so for the for the end we would like to

00:21:10,400 --> 00:21:17,120
start some discussion with the audience

00:21:13,840 --> 00:21:19,679
around a few topics here first

00:21:17,120 --> 00:21:20,640
is we would like to get some feedback on

00:21:19,679 --> 00:21:22,960
the new

00:21:20,640 --> 00:21:25,679
hardware vio menu device model that we

00:21:22,960 --> 00:21:28,960
are proposing

00:21:25,679 --> 00:21:31,039
second part is the hybrid vi immune

00:21:28,960 --> 00:21:33,840
system model where we have both

00:21:31,039 --> 00:21:36,400
software and hardware via vio mu in the

00:21:33,840 --> 00:21:36,400
same guest

00:21:36,880 --> 00:21:43,919
how that will scale and how would that

00:21:41,039 --> 00:21:45,039
be included in the qmu are there is pros

00:21:43,919 --> 00:21:46,559
and cons we would like to get some

00:21:45,039 --> 00:21:49,760
feedback on that one

00:21:46,559 --> 00:21:51,600
next one is the iautos interface design

00:21:49,760 --> 00:21:54,240
because we're actually using a new a

00:21:51,600 --> 00:21:56,400
brand new iot interface

00:21:54,240 --> 00:21:57,760
there were some discussion on whether we

00:21:56,400 --> 00:22:02,159
should extending the

00:21:57,760 --> 00:22:04,000
existing vfio ioto interface or not

00:22:02,159 --> 00:22:06,000
and for the last part we would like to

00:22:04,000 --> 00:22:06,799
get some feedback on additional usage

00:22:06,000 --> 00:22:08,880
models

00:22:06,799 --> 00:22:10,799
based on the proposed design that we are

00:22:08,880 --> 00:22:13,840
that we have presented

00:22:10,799 --> 00:22:13,840

YouTube URL: https://www.youtube.com/watch?v=7-KlLMpbb_g


