Title: Evaluate Implementation Options of KVM-based Type1 (or 1.5) Hypervisor - Jun Nakajima, Intel
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Evaluate Implementation Options of KVM-based Type1 (or 1.5) Hypervisor - Jun Nakajima, Intel
Captions: 
	00:00:00,320 --> 00:00:06,560
hi i'm jun nakajima from intel

00:00:04,319 --> 00:00:08,480
today i'm going to talk about

00:00:06,560 --> 00:00:12,000
implementation options

00:00:08,480 --> 00:00:15,200
of kbm based type 1

00:00:12,000 --> 00:00:19,119
or 1.5 hypervisor

00:00:15,200 --> 00:00:22,400
before that i'd like to appreciate

00:00:19,119 --> 00:00:24,800
chan xiao and anthony

00:00:22,400 --> 00:00:26,960
for their effort and contribution to

00:00:24,800 --> 00:00:30,720
this project

00:00:26,960 --> 00:00:34,239
they implemented pocs

00:00:30,720 --> 00:00:37,440
and also measures measured a lot of

00:00:34,239 --> 00:00:39,040
data for this project anyway thank you

00:00:37,440 --> 00:00:42,879
very much

00:00:39,040 --> 00:00:42,879
uh chancellor nancy

00:00:43,280 --> 00:00:46,320
so as your usual

00:00:47,120 --> 00:00:51,440
now that danger i'm start i'm starting

00:00:49,840 --> 00:00:55,199
from the multiplication

00:00:51,440 --> 00:00:58,719
and show some

00:00:55,199 --> 00:01:01,840
implementation options and then poc

00:00:58,719 --> 00:01:04,640
performance data and

00:01:01,840 --> 00:01:06,640
followed by our conclusion next step

00:01:04,640 --> 00:01:10,560
okay

00:01:06,640 --> 00:01:13,600
i presented this at the last kvm forum

00:01:10,560 --> 00:01:16,720
but at that time i pointed out

00:01:13,600 --> 00:01:19,920
the security risk of a

00:01:16,720 --> 00:01:23,759
guest on linux kvm

00:01:19,920 --> 00:01:27,680
basically kbn piggybacks

00:01:23,759 --> 00:01:30,960
depend on the linux the host

00:01:27,680 --> 00:01:33,840
and linux is as you know

00:01:30,960 --> 00:01:35,920
needs to run various operating system

00:01:33,840 --> 00:01:39,040
like loads

00:01:35,920 --> 00:01:42,320
and it's much larger than

00:01:39,040 --> 00:01:45,520
kvm itself or hypervisor

00:01:42,320 --> 00:01:49,600
so it has more attack surfaces and

00:01:45,520 --> 00:01:53,600
making the guests more exposed

00:01:49,600 --> 00:01:56,960
also use a space bmm like a qmu

00:01:53,600 --> 00:02:00,799
has full access to the guest

00:01:56,960 --> 00:02:03,520
memory also against

00:02:00,799 --> 00:02:03,520
cp state

00:02:04,000 --> 00:02:12,640
and then the kernel has a full access to

00:02:08,800 --> 00:02:14,879
any guest memory or cpu state so

00:02:12,640 --> 00:02:16,160
that's the kind of security risk of

00:02:14,879 --> 00:02:20,720
existing

00:02:16,160 --> 00:02:20,720
uh linux kbm implementation

00:02:20,840 --> 00:02:28,400
okay

00:02:23,599 --> 00:02:30,319
so the motivation of a type 1 hypervisor

00:02:28,400 --> 00:02:31,440
this is not the new for example if you

00:02:30,319 --> 00:02:35,840
take a zen

00:02:31,440 --> 00:02:37,760
it's kind of a 1.5 and then also hyper-v

00:02:35,840 --> 00:02:41,599
and another hypervisor

00:02:37,760 --> 00:02:45,280
has a similar architecture

00:02:41,599 --> 00:02:46,080
the benefits of the type 1 or type 1

00:02:45,280 --> 00:02:49,519
point

00:02:46,080 --> 00:02:50,560
hypervisor is it separates the

00:02:49,519 --> 00:02:54,000
hypervisor

00:02:50,560 --> 00:02:57,280
from uh

00:02:54,000 --> 00:03:02,000
linux which

00:02:57,280 --> 00:03:06,560
handles more complex operations and

00:03:02,000 --> 00:03:10,800
for example io which is

00:03:06,560 --> 00:03:14,080
using various uh device drivers

00:03:10,800 --> 00:03:14,959
from various parties and process

00:03:14,080 --> 00:03:18,480
management

00:03:14,959 --> 00:03:21,920
you know user process entering

00:03:18,480 --> 00:03:26,239
whereas the hypervisor will be

00:03:21,920 --> 00:03:26,239
responsible for isolation

00:03:26,319 --> 00:03:30,319
and also if trusted

00:03:30,799 --> 00:03:37,840
hypervisor can create secure environment

00:03:34,239 --> 00:03:42,720
like trusted execution environment well

00:03:37,840 --> 00:03:45,760
trusted bmm on top of a hypervisor

00:03:42,720 --> 00:03:47,920
now take a look at how we connect how we

00:03:45,760 --> 00:03:51,519
can convert kbm to type

00:03:47,920 --> 00:03:56,959
1.5 okay so we take a

00:03:51,519 --> 00:03:56,959
standard linux plus kvm system and then

00:03:58,159 --> 00:04:06,080
insert hypervisor as l0

00:04:01,360 --> 00:04:09,439
now the kbm becomes l1 and then

00:04:06,080 --> 00:04:13,599
the existing vms

00:04:09,439 --> 00:04:16,160
become like a nested so l2 okay

00:04:13,599 --> 00:04:17,040
but we still want to keep our path

00:04:16,160 --> 00:04:20,079
through

00:04:17,040 --> 00:04:23,120
um so the hypervisor

00:04:20,079 --> 00:04:26,840
itself uh basically doesn't have any

00:04:23,120 --> 00:04:29,360
device driver io device driver

00:04:26,840 --> 00:04:31,759
okay

00:04:29,360 --> 00:04:32,479
and then we call dom zero like zen guys

00:04:31,759 --> 00:04:35,360
did

00:04:32,479 --> 00:04:37,600
essentially this is very similar to the

00:04:35,360 --> 00:04:40,800
zen architecture

00:04:37,600 --> 00:04:44,720
the difference would be the

00:04:40,800 --> 00:04:49,040
as a dom zero it uses uh basically

00:04:44,720 --> 00:04:49,040
unmodified linux okay

00:04:49,120 --> 00:04:56,479
but there are more uh differences

00:04:52,880 --> 00:04:58,400
i'll talk about

00:04:56,479 --> 00:04:59,680
and in terms of implementation we have

00:04:58,400 --> 00:05:02,960
two extremes

00:04:59,680 --> 00:05:04,160
one extreme is as hyper

00:05:02,960 --> 00:05:06,080
you know i've been talking about the

00:05:04,160 --> 00:05:10,160
hypervisor implementation

00:05:06,080 --> 00:05:14,000
one extreme is to use uh

00:05:10,160 --> 00:05:17,120
minimally configured linux okay

00:05:14,000 --> 00:05:20,160
plus kbm

00:05:17,120 --> 00:05:23,120
and since it's a linux although it

00:05:20,160 --> 00:05:26,479
doesn't have a i o device driver

00:05:23,120 --> 00:05:27,680
it's a basically operating system it has

00:05:26,479 --> 00:05:31,280
a scheduler

00:05:27,680 --> 00:05:32,560
memory management so forth okay and then

00:05:31,280 --> 00:05:35,759
it boots first

00:05:32,560 --> 00:05:39,440
and separately from the

00:05:35,759 --> 00:05:43,360
dom zero okay as a linux

00:05:39,440 --> 00:05:47,759
it can run user processes and

00:05:43,360 --> 00:05:50,880
it can run user level bmms uh

00:05:47,759 --> 00:05:52,000
for dom zero you cannot use the qmu per

00:05:50,880 --> 00:05:54,400
se

00:05:52,000 --> 00:05:55,360
mainly because what doesn't need to it

00:05:54,400 --> 00:05:59,440
doesn't need to

00:05:55,360 --> 00:06:01,919
you know now use a qmu because uh

00:05:59,440 --> 00:06:03,120
i o virtualization is not really

00:06:01,919 --> 00:06:06,960
required there

00:06:03,120 --> 00:06:09,759
okay for other vms we probably need to

00:06:06,960 --> 00:06:09,759
use qmu

00:06:10,800 --> 00:06:16,960
this is one implementation and

00:06:14,080 --> 00:06:18,960
the other side is they say the

00:06:16,960 --> 00:06:22,400
lightweight hypervisor

00:06:18,960 --> 00:06:23,039
the is simply just a deep privilege the

00:06:22,400 --> 00:06:27,440
linux

00:06:23,039 --> 00:06:31,280
for isolation and it's just a reactive

00:06:27,440 --> 00:06:34,319
so as long as the host linux behaves

00:06:31,280 --> 00:06:38,400
the gastrinax behaves

00:06:34,319 --> 00:06:41,520
well it doesn't generate the vmx

00:06:38,400 --> 00:06:45,360
okay and this can be

00:06:41,520 --> 00:06:48,479
loaded by linux at the already put time

00:06:45,360 --> 00:06:51,520
as long as the

00:06:48,479 --> 00:06:55,919
linux was

00:06:51,520 --> 00:06:55,919
the healthy at that time okay

00:06:56,960 --> 00:07:02,720
and then common thing is again io pass

00:07:00,840 --> 00:07:05,840
through

00:07:02,720 --> 00:07:09,039
so let's talk about pros and cons

00:07:05,840 --> 00:07:12,160
for each implementation

00:07:09,039 --> 00:07:15,120
option okay one is

00:07:12,160 --> 00:07:17,360
linux kvm hypervisor so again the

00:07:15,120 --> 00:07:20,960
hypervisor is minimally

00:07:17,360 --> 00:07:24,240
configured linux plus kbm okay

00:07:20,960 --> 00:07:28,319
so it has a user label as well it's

00:07:24,240 --> 00:07:33,280
user support like the dom

00:07:28,319 --> 00:07:37,039
the user level bmm and then the qmu

00:07:33,280 --> 00:07:40,560
the benefits as a hypervisor

00:07:37,039 --> 00:07:44,800
uh you can run unmodified

00:07:40,560 --> 00:07:49,599
guests on n1 here

00:07:44,800 --> 00:07:53,199
and since the hypervisor is a linux

00:07:49,599 --> 00:07:57,039
kbm you can benefit from a linux kbm

00:07:53,199 --> 00:08:00,639
also continue to benefit right

00:07:57,039 --> 00:08:01,440
but there are some uh disadvantages of

00:08:00,639 --> 00:08:04,240
this uh

00:08:01,440 --> 00:08:04,639
approach first of all i'll talk about

00:08:04,240 --> 00:08:08,160
this

00:08:04,639 --> 00:08:10,479
later but the higher latency to dom zero

00:08:08,160 --> 00:08:12,960
uh because of the scheduling double

00:08:10,479 --> 00:08:16,319
scheduling and then bm exit

00:08:12,960 --> 00:08:19,360
and it's still big so it may have

00:08:16,319 --> 00:08:23,039
issue as a tcb or we

00:08:19,360 --> 00:08:26,160
may be able to handle that okay and then

00:08:23,039 --> 00:08:29,039
barter io for the guest as long as

00:08:26,160 --> 00:08:30,080
you run the guest within a dom 0 it

00:08:29,039 --> 00:08:33,440
switches just

00:08:30,080 --> 00:08:37,919
linux typical linux system but

00:08:33,440 --> 00:08:43,279
this guy this this uh n1

00:08:37,919 --> 00:08:43,279
is outside dom zero and

00:08:43,680 --> 00:08:47,040
there are some issues with handling vert

00:08:46,480 --> 00:08:51,200
io

00:08:47,040 --> 00:08:54,720
for this for you know these vms

00:08:51,200 --> 00:08:58,959
also power management is

00:08:54,720 --> 00:09:02,399
essentially who should manage the power

00:08:58,959 --> 00:09:06,080
for the cpus and the platform

00:09:02,399 --> 00:09:06,480
okay so i'll talk about some scheduling

00:09:06,080 --> 00:09:09,760
and

00:09:06,480 --> 00:09:13,279
power management issues here

00:09:09,760 --> 00:09:16,800
as hypervisor will linux uh

00:09:13,279 --> 00:09:19,920
initiate own the vm scheduling bm

00:09:16,800 --> 00:09:19,920
scheduling okay

00:09:21,279 --> 00:09:28,480
to that end we need to intercept halt

00:09:25,040 --> 00:09:28,480
emulate in dom zero

00:09:31,519 --> 00:09:36,640
but this causes uh inefficiency for

00:09:34,640 --> 00:09:38,800
especially for the client

00:09:36,640 --> 00:09:40,320
because of the two-level scheduling so

00:09:38,800 --> 00:09:43,040
now we have a

00:09:40,320 --> 00:09:44,880
scheduling of the two labels in the

00:09:43,040 --> 00:09:47,920
hypervisor linux and

00:09:44,880 --> 00:09:52,160
also within the

00:09:47,920 --> 00:09:55,600
doms linux kernel and

00:09:52,160 --> 00:09:58,959
because of those we see

00:09:55,600 --> 00:10:04,240
unexpected latency in bms especially in

00:09:58,959 --> 00:10:07,360
a dom zero

00:10:04,240 --> 00:10:08,240
now other issue is how we're going to

00:10:07,360 --> 00:10:10,959
create

00:10:08,240 --> 00:10:10,959
the vms

00:10:13,440 --> 00:10:20,320
we need to invoke the qmu

00:10:16,640 --> 00:10:25,440
process here to create the

00:10:20,320 --> 00:10:25,440
n1 bmm sorry 81 bm

00:10:25,839 --> 00:10:33,120
but from a user point of view

00:10:29,519 --> 00:10:36,240
this you use a process

00:10:33,120 --> 00:10:39,360
is available

00:10:36,240 --> 00:10:42,720
you cannot really access to

00:10:39,360 --> 00:10:46,880
hypervisor level of a

00:10:42,720 --> 00:10:51,519
user space okay so

00:10:46,880 --> 00:10:54,240
we need help some uh uh communication

00:10:51,519 --> 00:10:54,720
safe communication secure communication

00:10:54,240 --> 00:10:58,000
to

00:10:54,720 --> 00:11:02,640
from a user level in dom zero

00:10:58,000 --> 00:11:05,839
to hypervisor or the qmu or some process

00:11:02,640 --> 00:11:07,920
on the hypervisor side berta you is also

00:11:05,839 --> 00:11:11,040
a problem

00:11:07,920 --> 00:11:15,200
because from the qmu

00:11:11,040 --> 00:11:16,000
process point he doesn't have a device

00:11:15,200 --> 00:11:19,120
driver

00:11:16,000 --> 00:11:22,000
available in the hypervisor on the

00:11:19,120 --> 00:11:25,360
memory file system be available

00:11:22,000 --> 00:11:29,120
so qmu can

00:11:25,360 --> 00:11:33,519
in hypervisor run

00:11:29,120 --> 00:11:36,880
vm without protein

00:11:33,519 --> 00:11:40,720
so it's again it's limited there are

00:11:36,880 --> 00:11:44,079
probably ways to allow that

00:11:40,720 --> 00:11:47,440
qmu to access the

00:11:44,079 --> 00:11:50,399
the dlio devices but uh

00:11:47,440 --> 00:11:51,440
we don't know how at this point okay

00:11:50,399 --> 00:11:54,160
probably we need

00:11:51,440 --> 00:11:56,240
something like a zen or a hyper-v kind

00:11:54,160 --> 00:12:00,480
of a solution here here

00:11:56,240 --> 00:12:04,000
so switch to the lightweight hypervisor

00:12:00,480 --> 00:12:05,680
pros is like i said uh codepath if you

00:12:04,000 --> 00:12:08,800
look at the code paths of

00:12:05,680 --> 00:12:11,839
this doms the code pass is

00:12:08,800 --> 00:12:12,880
basically i'd almost probably identical

00:12:11,839 --> 00:12:16,639
to the bare metal

00:12:12,880 --> 00:12:17,360
same code to the the bare metal linux

00:12:16,639 --> 00:12:20,639
kvm

00:12:17,360 --> 00:12:24,320
and that's a low overhead

00:12:20,639 --> 00:12:27,839
and then the low latency and then it's

00:12:24,320 --> 00:12:31,760
small tcb okay

00:12:27,839 --> 00:12:32,560
the sub disadvantage is if you want to

00:12:31,760 --> 00:12:35,760
run

00:12:32,560 --> 00:12:38,639
the l1 vm for example in

00:12:35,760 --> 00:12:39,839
enclave or secure environment for

00:12:38,639 --> 00:12:42,399
purpose

00:12:39,839 --> 00:12:43,600
then we have kind of a similar problem

00:12:42,399 --> 00:12:46,480
basically

00:12:43,600 --> 00:12:46,880
the this lightweight hypervisor doesn't

00:12:46,480 --> 00:12:50,240
have

00:12:46,880 --> 00:12:53,680
any device drivers so and then

00:12:50,240 --> 00:12:57,040
also no user process available so it's

00:12:53,680 --> 00:12:59,279
not really not so easy to support

00:12:57,040 --> 00:13:03,120
virtual devices

00:12:59,279 --> 00:13:07,200
in the 81 bm n1 vms

00:13:03,120 --> 00:13:10,320
you can run the unmodified guest

00:13:07,200 --> 00:13:13,839
but that's the case in that case

00:13:10,320 --> 00:13:17,279
the vms uh will be learned

00:13:13,839 --> 00:13:21,040
as l2 as a

00:13:17,279 --> 00:13:26,079
kbm guest which is l1

00:13:21,040 --> 00:13:29,120
nested okay so there may be some uh

00:13:26,079 --> 00:13:29,800
performance concern with that there are

00:13:29,120 --> 00:13:33,120
some uh

00:13:29,800 --> 00:13:36,240
optimization uh when learned

00:13:33,120 --> 00:13:39,360
when running a cabin gas on top of

00:13:36,240 --> 00:13:42,560
lightweight hypervisor

00:13:39,360 --> 00:13:45,760
so one is also called optimized

00:13:42,560 --> 00:13:46,839
nesting i'll talk about more details on

00:13:45,760 --> 00:13:50,720
next

00:13:46,839 --> 00:13:53,920
page but we can pass through

00:13:50,720 --> 00:13:58,320
most of the shadow

00:13:53,920 --> 00:13:58,320
diffuse of the shadow bmcs

00:13:58,399 --> 00:14:04,480
and also we can convert

00:14:01,440 --> 00:14:05,040
the shadow bmcs to real vmcs very

00:14:04,480 --> 00:14:07,760
quickly

00:14:05,040 --> 00:14:08,320
just flipping one bit anyway i'll talk

00:14:07,760 --> 00:14:11,600
about

00:14:08,320 --> 00:14:15,040
more detail nate on next page

00:14:11,600 --> 00:14:18,399
the other technique we have is

00:14:15,040 --> 00:14:21,600
we we can have the kvm

00:14:18,399 --> 00:14:25,680
the first level entry point

00:14:21,600 --> 00:14:29,440
inside the lightweight hypervisor

00:14:25,680 --> 00:14:33,920
then handle the vmx is

00:14:29,440 --> 00:14:36,560
immediately inside l0 hypervisor and

00:14:33,920 --> 00:14:39,600
quickly go back

00:14:36,560 --> 00:14:43,680
for the for more complex

00:14:39,600 --> 00:14:48,320
handling we need to go back to

00:14:43,680 --> 00:14:51,440
kbm so in that case we need to enter the

00:14:48,320 --> 00:14:56,000
kbm or the dom 0

00:14:51,440 --> 00:15:00,399
for that part but still it's

00:14:56,000 --> 00:15:04,240
faster as long as we can handle within

00:15:00,399 --> 00:15:07,440
the l0 hypervisor in that case

00:15:04,240 --> 00:15:12,320
the l2vm looks like

00:15:07,440 --> 00:15:15,440
kind of a l1 vm okay

00:15:12,320 --> 00:15:18,399
so now i'll talk about more details on

00:15:15,440 --> 00:15:21,839
nested virtualization

00:15:18,399 --> 00:15:26,880
for nested virtualization in the kvm

00:15:21,839 --> 00:15:30,000
we use the so-called vmcs shadowing

00:15:26,880 --> 00:15:33,920
and the benefits of bm shadowing

00:15:30,000 --> 00:15:37,120
is uh it doesn't need to cause the

00:15:33,920 --> 00:15:40,560
bm exit upon uh vm

00:15:37,120 --> 00:15:43,759
lead or vm right in l1 bmm

00:15:40,560 --> 00:15:47,839
so if the hypervisor

00:15:43,759 --> 00:15:51,680
allow the bm

00:15:47,839 --> 00:15:55,680
the guest bmm to directly access the

00:15:51,680 --> 00:15:58,720
vmcs then it doesn't need to generate

00:15:55,680 --> 00:16:02,480
the bm exit it just access

00:15:58,720 --> 00:16:04,160
the bmcs the hypervisor can set the

00:16:02,480 --> 00:16:07,279
bitmap

00:16:04,160 --> 00:16:10,240
for the bmcsps

00:16:07,279 --> 00:16:12,560
read on the right the separate bitmaps

00:16:10,240 --> 00:16:15,759
will read and the write operation

00:16:12,560 --> 00:16:19,839
so as long as they are

00:16:15,759 --> 00:16:23,759
not in a bitmap then

00:16:19,839 --> 00:16:25,759
the l1 bmm can just simply

00:16:23,759 --> 00:16:28,399
do the upper hp and read the vm write

00:16:25,759 --> 00:16:30,000
operation without causing vm exit to the

00:16:28,399 --> 00:16:33,199
l0 vmm

00:16:30,000 --> 00:16:37,600
but sometimes uh

00:16:33,199 --> 00:16:41,839
it needs to generate the br the l0 bmm

00:16:37,600 --> 00:16:44,880
need to intercept and even still the vr

00:16:41,839 --> 00:16:49,519
we use the bmcs shadowing

00:16:44,880 --> 00:16:49,519
certain lights will lead to the

00:16:49,680 --> 00:16:56,160
field causes a bmx also

00:16:52,959 --> 00:16:59,199
the biggest problem with the bmcs

00:16:56,160 --> 00:17:02,720
shadowing is this

00:16:59,199 --> 00:17:06,720
shadow bmcs itself cannot be

00:17:02,720 --> 00:17:11,520
used for vm entry so

00:17:06,720 --> 00:17:16,079
when we go back to the l2 vm

00:17:11,520 --> 00:17:19,199
we need to copy or sync some of

00:17:16,079 --> 00:17:23,039
the vmcs field so that we can make a

00:17:19,199 --> 00:17:25,919
real bmcs for the vm entry back to the

00:17:23,039 --> 00:17:25,919
ltvm

00:17:26,319 --> 00:17:33,360
now the optimization

00:17:30,240 --> 00:17:37,200
we have we have

00:17:33,360 --> 00:17:40,960
doesn't require a copy of sync because

00:17:37,200 --> 00:17:44,559
the vmcs

00:17:40,960 --> 00:17:47,880
shadow or shadow bmcs is

00:17:44,559 --> 00:17:52,240
it's identical to the

00:17:47,880 --> 00:17:57,520
dlvmcs we still need to intercept

00:17:52,240 --> 00:18:00,799
the some limited the bmcs field

00:17:57,520 --> 00:18:03,919
by the l1 bmm for

00:18:00,799 --> 00:18:07,120
security purpose but

00:18:03,919 --> 00:18:10,320
the contents are basically same

00:18:07,120 --> 00:18:15,840
so we can just flip the bit now

00:18:10,320 --> 00:18:19,919
we took now let's talk about uh poc

00:18:15,840 --> 00:18:23,039
so we extended vbh

00:18:19,919 --> 00:18:27,679
to create the lightweight

00:18:23,039 --> 00:18:30,720
hypervisor poc the original vvh

00:18:27,679 --> 00:18:35,280
is a virtualization based hardening

00:18:30,720 --> 00:18:38,000
it simply deprives the linux scanner

00:18:35,280 --> 00:18:38,720
to held in the kernel using

00:18:38,000 --> 00:18:42,400
hardware-based

00:18:38,720 --> 00:18:45,840
virtualization features

00:18:42,400 --> 00:18:48,960
we pass through all i o and also

00:18:45,840 --> 00:18:52,480
epic and in the poc from

00:18:48,960 --> 00:18:55,200
the vbh uh

00:18:52,480 --> 00:18:56,320
you know beyond the bbh we added the

00:18:55,200 --> 00:19:00,320
simple nesting

00:18:56,320 --> 00:19:04,240
support this only works for the

00:19:00,320 --> 00:19:07,520
l1vm bmm like bml

00:19:04,240 --> 00:19:10,640
vm where the gpa is

00:19:07,520 --> 00:19:12,799
identical to hpa like i showed we

00:19:10,640 --> 00:19:16,720
implemented the optimized

00:19:12,799 --> 00:19:20,480
bmcs shading and also virtual ebt

00:19:16,720 --> 00:19:24,000
to make sure i isolation

00:19:20,480 --> 00:19:27,760
also added a a feature to

00:19:24,000 --> 00:19:31,679
run a simple l1bm in

00:19:27,760 --> 00:19:34,799
trusted environment we can run

00:19:31,679 --> 00:19:38,320
opti os uh

00:19:34,799 --> 00:19:41,360
please look at the pointer below

00:19:38,320 --> 00:19:43,280
and we are working on the virtual mmu

00:19:41,360 --> 00:19:45,440
iomu support

00:19:43,280 --> 00:19:46,880
okay now let's take a look at the

00:19:45,440 --> 00:19:50,000
performance data

00:19:46,880 --> 00:19:51,200
what kind of performance data we have by

00:19:50,000 --> 00:19:54,240
comparing

00:19:51,200 --> 00:19:58,799
the the vm l1

00:19:54,240 --> 00:20:01,440
or l2vm performance on

00:19:58,799 --> 00:20:02,080
kvm based hypervisor versus the

00:20:01,440 --> 00:20:06,760
lightweight

00:20:02,080 --> 00:20:09,760
hypervisor so we are comparing the bm

00:20:06,760 --> 00:20:13,200
ltbms on top of

00:20:09,760 --> 00:20:16,320
linux kbm hypervisor

00:20:13,200 --> 00:20:20,799
versus lightweight hypervisor

00:20:16,320 --> 00:20:21,360
so the first performance measurement is

00:20:20,799 --> 00:20:24,480
the

00:20:21,360 --> 00:20:27,919
compression of l2 vms

00:20:24,480 --> 00:20:31,200
okay so this is kind of result

00:20:27,919 --> 00:20:34,080
and this one shows the uh

00:20:31,200 --> 00:20:34,480
improvements from flipping uh you know

00:20:34,080 --> 00:20:37,919
the

00:20:34,480 --> 00:20:42,000
shadow bmcs indicator that the one bit

00:20:37,919 --> 00:20:45,280
okay so if you look at the bm exit

00:20:42,000 --> 00:20:49,120
uh cost initially uh via

00:20:45,280 --> 00:20:51,440
uh kbm had this much but

00:20:49,120 --> 00:20:52,960
now it's a more than like a one-tenth

00:20:51,440 --> 00:20:56,080
okay less than one tenth

00:20:52,960 --> 00:20:59,200
so you see a consistent

00:20:56,080 --> 00:21:02,240
uh you know results

00:20:59,200 --> 00:21:05,360
in terms of the vm exit

00:21:02,240 --> 00:21:09,440
uh latency reduction like

00:21:05,360 --> 00:21:12,559
about 10 x or more okay also

00:21:09,440 --> 00:21:16,240
at the time of a vm entry

00:21:12,559 --> 00:21:19,280
uh because of a fast switching uh

00:21:16,240 --> 00:21:22,799
you also see like uh almost a

00:21:19,280 --> 00:21:27,600
10x of improvement or more than 10x

00:21:22,799 --> 00:21:31,840
improvement so we have a very good

00:21:27,600 --> 00:21:35,120
l2 performance okay compared with

00:21:31,840 --> 00:21:38,640
the kvm l2 if we

00:21:35,120 --> 00:21:42,159
compare kbm

00:21:38,640 --> 00:21:45,679
a1 okay versus

00:21:42,159 --> 00:21:48,799
that l2 on lightweight hypervisor

00:21:45,679 --> 00:21:52,240
okay so this is a result okay

00:21:48,799 --> 00:21:55,440
result is it's almost same

00:21:52,240 --> 00:21:58,720
we have some uh uh

00:21:55,440 --> 00:21:59,280
regression in uh io area at this point

00:21:58,720 --> 00:22:03,440
but this

00:21:59,280 --> 00:22:06,080
is kind of uh first implementation so we

00:22:03,440 --> 00:22:06,880
don't have any optimization for io at

00:22:06,080 --> 00:22:10,159
this point

00:22:06,880 --> 00:22:12,000
but so the performance looks good and

00:22:10,159 --> 00:22:15,360
then we don't have a

00:22:12,000 --> 00:22:17,760
the another optimization like a kb and

00:22:15,360 --> 00:22:20,880
faster entry point handling

00:22:17,760 --> 00:22:26,480
okay so today the

00:22:20,880 --> 00:22:29,919
l2 vms are lightweight hypervisor runs

00:22:26,480 --> 00:22:33,600
purely in that nested way

00:22:29,919 --> 00:22:36,720
so from uh pocs

00:22:33,600 --> 00:22:40,799
we found a couple of things so

00:22:36,720 --> 00:22:42,480
we also have a poc for the linux kbm

00:22:40,799 --> 00:22:46,240
hypervisor

00:22:42,480 --> 00:22:49,840
it actually has structural impact

00:22:46,240 --> 00:22:50,080
okay the structural impact in the sense

00:22:49,840 --> 00:22:54,000
of

00:22:50,080 --> 00:22:54,880
resource management okay scheduling like

00:22:54,000 --> 00:22:57,840
i said the

00:22:54,880 --> 00:23:00,159
the double scheduling and then also

00:22:57,840 --> 00:23:03,280
power management

00:23:00,159 --> 00:23:07,280
the linux as a hypervisor

00:23:03,280 --> 00:23:11,840
needs to have the power management

00:23:07,280 --> 00:23:15,280
although hypervisor doesn't have a

00:23:11,840 --> 00:23:18,559
uh any io devices it needs to

00:23:15,280 --> 00:23:21,760
handle still needs to handle the

00:23:18,559 --> 00:23:22,159
vm management but from a user point of

00:23:21,760 --> 00:23:27,600
view

00:23:22,159 --> 00:23:30,880
i mean the use of l0 dom 0

00:23:27,600 --> 00:23:34,000
user level it doesn't allow

00:23:30,880 --> 00:23:37,440
to manage the vm directory

00:23:34,000 --> 00:23:40,480
because the qmu on the dom

00:23:37,440 --> 00:23:43,679
zero just handles the vms on the dom

00:23:40,480 --> 00:23:47,279
zero okay vertical implementation

00:23:43,679 --> 00:23:48,480
will be a bit tricky we need a similar

00:23:47,279 --> 00:23:51,679
solution like

00:23:48,480 --> 00:23:53,919
zen or hyper v provides like uh

00:23:51,679 --> 00:23:54,799
you know zen bus and the back end and

00:23:53,919 --> 00:23:57,919
the front end

00:23:54,799 --> 00:24:01,279
sort of you know solution the

00:23:57,919 --> 00:24:05,760
bigger challenge is to kind of redo

00:24:01,279 --> 00:24:09,039
all the performance or validation work

00:24:05,760 --> 00:24:10,320
if we switch to this one because you you

00:24:09,039 --> 00:24:13,600
have a

00:24:10,320 --> 00:24:14,640
different uh resource management

00:24:13,600 --> 00:24:18,240
structure right

00:24:14,640 --> 00:24:19,440
large difference here in terms of

00:24:18,240 --> 00:24:23,200
performance or

00:24:19,440 --> 00:24:26,000
tuning um you cannot really

00:24:23,200 --> 00:24:27,200
get same results as a bare metal once

00:24:26,000 --> 00:24:30,080
you have

00:24:27,200 --> 00:24:31,279
this hypervisor this architecture you

00:24:30,080 --> 00:24:34,960
need to redo

00:24:31,279 --> 00:24:37,120
the optimization tuning

00:24:34,960 --> 00:24:38,720
on the light hands lightweight

00:24:37,120 --> 00:24:41,840
hypervisor side

00:24:38,720 --> 00:24:46,000
we had some concern with

00:24:41,840 --> 00:24:48,000
nesting and at this point the

00:24:46,000 --> 00:24:50,320
performance about l2

00:24:48,000 --> 00:24:52,240
on the hype a lightweight hypervisor and

00:24:50,320 --> 00:24:56,559
then the kbm l1

00:24:52,240 --> 00:25:00,159
is almost the same except uh

00:24:56,559 --> 00:25:03,919
i o i o is about 90 percent of

00:25:00,159 --> 00:25:05,760
l1 kbm this point so we still need more

00:25:03,919 --> 00:25:09,279
optimization for ios

00:25:05,760 --> 00:25:10,640
here's our conclusion the lightweight or

00:25:09,279 --> 00:25:12,799
reactive hypervisor

00:25:10,640 --> 00:25:16,000
approach is more suitable for the

00:25:12,799 --> 00:25:19,360
existing linux kvm

00:25:16,000 --> 00:25:22,720
when making it more

00:25:19,360 --> 00:25:26,000
secure i type 1

00:25:22,720 --> 00:25:28,320
or 1.5 vm

00:25:26,000 --> 00:25:29,039
because we can maintain the same code

00:25:28,320 --> 00:25:33,520
paths as

00:25:29,039 --> 00:25:35,120
a bare metal linux kbm

00:25:33,520 --> 00:25:37,520
including scheduling and power

00:25:35,120 --> 00:25:39,440
management so forth

00:25:37,520 --> 00:25:41,360
it also provides low latency and

00:25:39,440 --> 00:25:45,120
overhead

00:25:41,360 --> 00:25:47,120
in addition the bvh-based hypervisor can

00:25:45,120 --> 00:25:49,919
harden the dom zero kernel and guest

00:25:47,120 --> 00:25:53,279
kernel as well

00:25:49,919 --> 00:25:56,400
kvm guests also

00:25:53,279 --> 00:25:59,840
run with minimal overhead

00:25:56,400 --> 00:26:02,880
even though they run

00:25:59,840 --> 00:26:06,000
in l2 environment we also found

00:26:02,880 --> 00:26:08,320
advantage when implementing trusted

00:26:06,000 --> 00:26:12,240
execution environment

00:26:08,320 --> 00:26:12,240
because of a small tcv

00:26:12,799 --> 00:26:21,520
so this is a next step so

00:26:16,880 --> 00:26:26,240
we want to finish the vba based poc

00:26:21,520 --> 00:26:29,600
especially complete ioma virtualization

00:26:26,240 --> 00:26:33,360
and optimize the kvm guests more

00:26:29,600 --> 00:26:37,279
especially in before io areas

00:26:33,360 --> 00:26:41,279
especially io write operations

00:26:37,279 --> 00:26:41,279
also add we also add the

00:26:41,919 --> 00:26:49,200
first level kvm entry point in vvh

00:26:46,080 --> 00:26:51,360
and then after that we share the source

00:26:49,200 --> 00:26:54,080
code

00:26:51,360 --> 00:26:55,279
with that this is a end of my

00:26:54,080 --> 00:27:00,320
presentation

00:26:55,279 --> 00:27:00,320

YouTube URL: https://www.youtube.com/watch?v=y6BfxDZ-1SI


