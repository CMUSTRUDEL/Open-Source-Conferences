Title: Optimizing for NVMe Drives: The 10 Microsecond Challenge - Stefan Hajnoczi, Red Hat
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Optimizing for NVMe Drives: The 10 Microsecond Challenge - Stefan Hajnoczi, Red Hat
Captions: 
	00:00:05,759 --> 00:00:08,160
hi

00:00:06,160 --> 00:00:10,559
my name is stephan hoynitzy and i'm

00:00:08,160 --> 00:00:15,200
going to talk about optimizing kvm

00:00:10,559 --> 00:00:18,320
for nvme drives so what are nvme drives

00:00:15,200 --> 00:00:21,199
nvme is a standard interface for

00:00:18,320 --> 00:00:22,400
solid state disks so it's a pci storage

00:00:21,199 --> 00:00:25,359
controller interface

00:00:22,400 --> 00:00:26,880
and there is an open specification and a

00:00:25,359 --> 00:00:30,880
standard linux driver

00:00:26,880 --> 00:00:31,439
that can talk to any nvme compliant

00:00:30,880 --> 00:00:34,480
device

00:00:31,439 --> 00:00:37,600
and devices are made by multiple vendors

00:00:34,480 --> 00:00:38,160
what's interesting about nvme drives is

00:00:37,600 --> 00:00:40,719
that

00:00:38,160 --> 00:00:42,640
some of them have extremely good latency

00:00:40,719 --> 00:00:46,399
much much lower latencies

00:00:42,640 --> 00:00:48,960
than we've had in the past so

00:00:46,399 --> 00:00:50,160
i quoted two figures here one is for an

00:00:48,960 --> 00:00:52,160
enterprise ssd

00:00:50,160 --> 00:00:54,239
so that would be something for for data

00:00:52,160 --> 00:00:56,559
centers and servers and so on

00:00:54,239 --> 00:00:57,440
this is an intel optane drive and it's

00:00:56,559 --> 00:01:00,960
quoted at

00:00:57,440 --> 00:01:04,720
10 microseconds for reads and writes

00:01:00,960 --> 00:01:09,200
but even the consumer ssds can be very

00:01:04,720 --> 00:01:11,680
low latency so the samsung 970 evo plus

00:01:09,200 --> 00:01:13,520
is quoted at around 17 microseconds

00:01:11,680 --> 00:01:15,040
right latency

00:01:13,520 --> 00:01:17,680
now just because the drive says it's

00:01:15,040 --> 00:01:19,520
nvme just because it

00:01:17,680 --> 00:01:20,960
uses that standard doesn't necessarily

00:01:19,520 --> 00:01:21,439
mean it's one of these low latency

00:01:20,960 --> 00:01:23,280
drives

00:01:21,439 --> 00:01:26,400
there are significant differences so you

00:01:23,280 --> 00:01:29,920
should check the data sheet before

00:01:26,400 --> 00:01:32,640
purchasing drives now in 2012

00:01:29,920 --> 00:01:33,520
jeff dean published a slide that became

00:01:32,640 --> 00:01:35,680
very popular

00:01:33,520 --> 00:01:36,560
called latency numbers every programmer

00:01:35,680 --> 00:01:38,320
should know

00:01:36,560 --> 00:01:40,079
and it's probably became popular because

00:01:38,320 --> 00:01:42,079
it's really interesting to see

00:01:40,079 --> 00:01:44,000
how long different operations in

00:01:42,079 --> 00:01:45,680
computer systems take

00:01:44,000 --> 00:01:46,640
now the reason i'm showing this table

00:01:45,680 --> 00:01:48,320
and the reason it's interesting is

00:01:46,640 --> 00:01:51,040
because if you look at

00:01:48,320 --> 00:01:52,079
other storage if you look at spinning

00:01:51,040 --> 00:01:55,439
disks

00:01:52,079 --> 00:01:57,520
hard disks that have platters

00:01:55,439 --> 00:01:59,119
that store the data magnetically and

00:01:57,520 --> 00:02:02,000
they have drive heads that need to

00:01:59,119 --> 00:02:02,719
move to the correct location in order to

00:02:02,000 --> 00:02:04,240
be able to

00:02:02,719 --> 00:02:06,320
read those blocks of data from the

00:02:04,240 --> 00:02:09,039
platter the

00:02:06,320 --> 00:02:10,720
they are the spinning discs have seek

00:02:09,039 --> 00:02:12,959
times in order to perform random

00:02:10,720 --> 00:02:15,040
accesses if we do the same random read

00:02:12,959 --> 00:02:16,560
um there's movement involved and that

00:02:15,040 --> 00:02:19,599
takes time

00:02:16,560 --> 00:02:23,920
so the time for that is quoted as

00:02:19,599 --> 00:02:26,959
two so comparing these two things

00:02:23,920 --> 00:02:28,080
hard disk versus an nvme solid state

00:02:26,959 --> 00:02:32,879
disk

00:02:28,080 --> 00:02:36,239
we have 2 000 microseconds for accessing

00:02:32,879 --> 00:02:37,920
a random read and

00:02:36,239 --> 00:02:40,000
10 microseconds so there's a huge

00:02:37,920 --> 00:02:43,760
difference here and and and you can

00:02:40,000 --> 00:02:45,200
see why um optimizing for nvme drives is

00:02:43,760 --> 00:02:45,920
very different than from traditional

00:02:45,200 --> 00:02:48,319
disks

00:02:45,920 --> 00:02:49,680
so let's have a look at a relationship

00:02:48,319 --> 00:02:51,680
between iops

00:02:49,680 --> 00:02:52,800
and latency this is an important one in

00:02:51,680 --> 00:02:55,360
fact this is

00:02:52,800 --> 00:02:56,959
the critical thing that drives the rest

00:02:55,360 --> 00:02:58,800
of this presentation

00:02:56,959 --> 00:03:00,879
and the optimization work that we're

00:02:58,800 --> 00:03:02,159
going to look into it's the relationship

00:03:00,879 --> 00:03:04,480
between

00:03:02,159 --> 00:03:06,879
iops and latency iops is the number of

00:03:04,480 --> 00:03:10,080
operations per second

00:03:06,879 --> 00:03:12,879
and here i'm showing a a simple uh

00:03:10,080 --> 00:03:13,760
a model of of doing one operation at a

00:03:12,879 --> 00:03:15,519
time

00:03:13,760 --> 00:03:17,360
and it's and the relationship is just

00:03:15,519 --> 00:03:18,000
the total run time divided by the

00:03:17,360 --> 00:03:19,920
latency that

00:03:18,000 --> 00:03:22,720
gives us the number of operations we can

00:03:19,920 --> 00:03:25,920
complete in that runtime

00:03:22,720 --> 00:03:28,000
so the first thing you notice is that

00:03:25,920 --> 00:03:29,760
this relationship is non-linear

00:03:28,000 --> 00:03:31,680
right it's not just a straight line it

00:03:29,760 --> 00:03:35,200
is a curve

00:03:31,680 --> 00:03:37,440
and to start investigating it

00:03:35,200 --> 00:03:39,120
imagine that we are at the right hand

00:03:37,440 --> 00:03:41,360
side of this curve we're at

00:03:39,120 --> 00:03:43,440
20 microseconds latency that means it

00:03:41,360 --> 00:03:44,879
takes us 20 microseconds to complete our

00:03:43,440 --> 00:03:47,360
operation

00:03:44,879 --> 00:03:48,560
if we identify an optimization we could

00:03:47,360 --> 00:03:51,840
make to make the system

00:03:48,560 --> 00:03:53,439
faster say it shaves off two

00:03:51,840 --> 00:03:53,840
microseconds then we can bring it down

00:03:53,439 --> 00:03:56,560
to

00:03:53,840 --> 00:03:58,239
18 microseconds latency and the graph

00:03:56,560 --> 00:04:00,000
shows us how many iops

00:03:58,239 --> 00:04:02,799
we are going to get if we move it from

00:04:00,000 --> 00:04:05,680
20 to 18 microseconds latency

00:04:02,799 --> 00:04:06,080
well that slope is pretty flat over

00:04:05,680 --> 00:04:08,080
there

00:04:06,080 --> 00:04:09,680
at 20 microseconds so actually the

00:04:08,080 --> 00:04:12,560
number of iops doesn't

00:04:09,680 --> 00:04:14,000
increase that much but if you imagine

00:04:12,560 --> 00:04:16,400
for a second that we were at

00:04:14,000 --> 00:04:18,639
four microseconds of latency on the left

00:04:16,400 --> 00:04:20,959
hand side of that graph

00:04:18,639 --> 00:04:22,320
and then we shave off two microseconds

00:04:20,959 --> 00:04:24,080
so again here we're shaped we're just

00:04:22,320 --> 00:04:24,720
shaving off two microseconds the same

00:04:24,080 --> 00:04:27,280
adjustment

00:04:24,720 --> 00:04:29,440
in both cases but now it brings us down

00:04:27,280 --> 00:04:33,360
to two microseconds and we go from

00:04:29,440 --> 00:04:34,800
250k iops to 500k iops

00:04:33,360 --> 00:04:37,600
and this might be an obvious

00:04:34,800 --> 00:04:39,919
relationship because when we were at 20

00:04:37,600 --> 00:04:41,040
microseconds and we removed two we only

00:04:39,919 --> 00:04:42,479
optimized the way

00:04:41,040 --> 00:04:45,120
percent of the total latency but when we

00:04:42,479 --> 00:04:46,800
were at four and we went down to two

00:04:45,120 --> 00:04:48,560
we optimized away fifty percent of

00:04:46,800 --> 00:04:49,840
latency so it does make sense that the

00:04:48,560 --> 00:04:51,440
the jump is going to be bigger here

00:04:49,840 --> 00:04:52,560
because it is a bigger proportional

00:04:51,440 --> 00:04:54,720
improvement

00:04:52,560 --> 00:04:56,080
but still it leads to interesting things

00:04:54,720 --> 00:04:58,560
first of all

00:04:56,080 --> 00:04:59,520
think about having two independent

00:04:58,560 --> 00:05:01,919
optimizations

00:04:59,520 --> 00:05:03,039
a and b that you found and that you want

00:05:01,919 --> 00:05:04,639
to make well

00:05:03,039 --> 00:05:06,160
one of the misleading things here is

00:05:04,639 --> 00:05:09,440
that if you

00:05:06,160 --> 00:05:12,400
apply optimization a

00:05:09,440 --> 00:05:14,320
and then you apply optimization b say

00:05:12,400 --> 00:05:16,240
both of them

00:05:14,320 --> 00:05:17,360
reduce the total latency by two

00:05:16,240 --> 00:05:20,800
microseconds

00:05:17,360 --> 00:05:24,080
then if optimization a gains you 10k

00:05:20,800 --> 00:05:26,479
iops optimization b will gain you

00:05:24,080 --> 00:05:28,160
more than 10k iops because it was

00:05:26,479 --> 00:05:29,680
applied afterwards after we had already

00:05:28,160 --> 00:05:31,199
reduced the total latency

00:05:29,680 --> 00:05:33,360
and because they're independent if we do

00:05:31,199 --> 00:05:34,080
the other order the other way around if

00:05:33,360 --> 00:05:36,400
we do

00:05:34,080 --> 00:05:38,479
optimization b first and then

00:05:36,400 --> 00:05:40,960
optimization a

00:05:38,479 --> 00:05:42,160
then we'll have the reverse optimization

00:05:40,960 --> 00:05:45,039
b will give us

00:05:42,160 --> 00:05:46,400
10 iops improvement 10k iops improvement

00:05:45,039 --> 00:05:47,120
and optimization b will give us more

00:05:46,400 --> 00:05:48,720
than that

00:05:47,120 --> 00:05:50,240
so this can be misleading right it can

00:05:48,720 --> 00:05:51,520
be tricky in the order in which we apply

00:05:50,240 --> 00:05:52,880
optimization we can kind of fool

00:05:51,520 --> 00:05:55,840
ourselves thinking that one

00:05:52,880 --> 00:05:56,479
boosts iops by a lot and the other one

00:05:55,840 --> 00:05:59,039
doesn't

00:05:56,479 --> 00:06:00,639
but in terms of total latency uh it

00:05:59,039 --> 00:06:03,199
doesn't necessarily tell us

00:06:00,639 --> 00:06:04,560
how much time we've reduced and that's

00:06:03,199 --> 00:06:07,520
why saying something like

00:06:04,560 --> 00:06:08,080
iops increased by 10k doesn't really

00:06:07,520 --> 00:06:10,160
convey

00:06:08,080 --> 00:06:11,280
enough information to know what is going

00:06:10,160 --> 00:06:12,960
on

00:06:11,280 --> 00:06:14,479
you need to know at least the basic iops

00:06:12,960 --> 00:06:16,720
level you're at before

00:06:14,479 --> 00:06:19,360
in order to understand how the latency

00:06:16,720 --> 00:06:20,800
came into play

00:06:19,360 --> 00:06:22,400
the other thing that's important about

00:06:20,800 --> 00:06:25,680
this graph

00:06:22,400 --> 00:06:27,440
is that nvme drives being at 10

00:06:25,680 --> 00:06:30,720
microseconds or less

00:06:27,440 --> 00:06:32,479
latency if we focus on on that

00:06:30,720 --> 00:06:34,639
that is the left hand side of this graph

00:06:32,479 --> 00:06:36,720
that's where the graph becomes

00:06:34,639 --> 00:06:38,720
very non-linear that's where the slope

00:06:36,720 --> 00:06:40,960
gets very steep

00:06:38,720 --> 00:06:42,240
so we need to we need to keep that in

00:06:40,960 --> 00:06:45,759
mind that any small

00:06:42,240 --> 00:06:47,199
change to latency increasing it or

00:06:45,759 --> 00:06:49,759
reducing it on the left hand side of the

00:06:47,199 --> 00:06:52,479
graph can have a big effect on iops

00:06:49,759 --> 00:06:52,960
we need to re-examine the guest and the

00:06:52,479 --> 00:06:55,840
host

00:06:52,960 --> 00:06:57,440
software stack because the hardware is

00:06:55,840 --> 00:06:59,280
faster so now we need to

00:06:57,440 --> 00:07:01,360
re-examine the guest and host software

00:06:59,280 --> 00:07:03,039
stack and we need to rethink the

00:07:01,360 --> 00:07:07,440
architecture

00:07:03,039 --> 00:07:08,800
of qmu of kvm and of the linux drivers

00:07:07,440 --> 00:07:10,400
and so on

00:07:08,800 --> 00:07:11,840
because the hardware has some gotten so

00:07:10,400 --> 00:07:13,199
much faster and when i say rethink the

00:07:11,840 --> 00:07:14,639
architecture that's that's not a

00:07:13,199 --> 00:07:17,039
buzzword what i mean by that

00:07:14,639 --> 00:07:18,960
is think back to that curve that we saw

00:07:17,039 --> 00:07:21,840
the ios versus latency

00:07:18,960 --> 00:07:22,880
the optimizations that we considered in

00:07:21,840 --> 00:07:26,319
the past

00:07:22,880 --> 00:07:27,840
and that had no measurable effect

00:07:26,319 --> 00:07:30,080
you know they were not significant in

00:07:27,840 --> 00:07:32,319
the past we decided they were too

00:07:30,080 --> 00:07:34,240
complex or for whatever reason we didn't

00:07:32,319 --> 00:07:36,560
implement them in the past

00:07:34,240 --> 00:07:38,479
those optimizations might now be

00:07:36,560 --> 00:07:40,080
relevant again because even shaving off

00:07:38,479 --> 00:07:41,360
a little bit of time on the left hand

00:07:40,080 --> 00:07:43,840
side of that curve

00:07:41,360 --> 00:07:44,960
can boost iops a lot and can allow the

00:07:43,840 --> 00:07:47,120
application to get

00:07:44,960 --> 00:07:48,720
better performance so that's why i mean

00:07:47,120 --> 00:07:51,440
rethinking the architecture because we

00:07:48,720 --> 00:07:53,599
really can reconsider things

00:07:51,440 --> 00:07:55,599
that didn't make sense in the past and

00:07:53,599 --> 00:07:59,199
so that's the 10 microsecond challenge

00:07:55,599 --> 00:08:03,199
that we're going to talk about

00:07:59,199 --> 00:08:03,520
okay so let's begin by looking at the i

00:08:03,199 --> 00:08:06,080
o

00:08:03,520 --> 00:08:06,879
request lifecycle the i o request

00:08:06,080 --> 00:08:08,160
lifecycle

00:08:06,879 --> 00:08:10,960
is the core of what we need to

00:08:08,160 --> 00:08:13,280
understand in order to optimize this

00:08:10,960 --> 00:08:14,639
here is a high-level model it doesn't

00:08:13,280 --> 00:08:17,759
show the specifics

00:08:14,639 --> 00:08:20,879
of virtualization right you don't see

00:08:17,759 --> 00:08:23,039
qmu you don't see kvm in here and it

00:08:20,879 --> 00:08:24,400
or any emulated devices or the operating

00:08:23,039 --> 00:08:25,039
system or anything this is just a

00:08:24,400 --> 00:08:28,240
high-level

00:08:25,039 --> 00:08:30,080
model what it shows you is that an

00:08:28,240 --> 00:08:33,200
application that is running

00:08:30,080 --> 00:08:33,839
on the vcpu and decides to submit an i o

00:08:33,200 --> 00:08:35,680
request

00:08:33,839 --> 00:08:37,599
in this case it's a read request that is

00:08:35,680 --> 00:08:40,959
being submitted

00:08:37,599 --> 00:08:43,120
that request needs to be prepared and

00:08:40,959 --> 00:08:44,000
then it will be a message will be sent

00:08:43,120 --> 00:08:47,360
to the device

00:08:44,000 --> 00:08:48,399
notifying it that there's a request

00:08:47,360 --> 00:08:50,560
available

00:08:48,399 --> 00:08:51,680
and then the device can process that

00:08:50,560 --> 00:08:54,399
request

00:08:51,680 --> 00:08:55,920
now when that message has been sent when

00:08:54,399 --> 00:08:59,279
we've submitted our i o

00:08:55,920 --> 00:09:00,560
the vcpu might have no more work to do

00:08:59,279 --> 00:09:02,160
or it might have some other tasks that

00:09:00,560 --> 00:09:04,480
it can run in the meantime while we're

00:09:02,160 --> 00:09:06,240
waiting for the request to finish

00:09:04,480 --> 00:09:08,000
but either way at this point the

00:09:06,240 --> 00:09:10,800
critical path is in the hardware because

00:09:08,000 --> 00:09:12,399
now the device with its latency of say

00:09:10,800 --> 00:09:14,720
10 microseconds

00:09:12,399 --> 00:09:16,160
is going to be processing that request

00:09:14,720 --> 00:09:18,320
and when it finishes

00:09:16,160 --> 00:09:20,000
it sends back a completion notification

00:09:18,320 --> 00:09:23,120
to the vcpu

00:09:20,000 --> 00:09:25,440
the vcpu is going to then

00:09:23,120 --> 00:09:28,000
process that completion and resume the

00:09:25,440 --> 00:09:30,160
application because the i o has finished

00:09:28,000 --> 00:09:34,320
and so that's the life cycle of a single

00:09:30,160 --> 00:09:35,600
request of just one request in isolation

00:09:34,320 --> 00:09:38,080
and the important thing to understand

00:09:35,600 --> 00:09:40,560
here is that if we're trying to optimize

00:09:38,080 --> 00:09:43,360
the software layers we need to study

00:09:40,560 --> 00:09:44,399
the mechanism through which requests are

00:09:43,360 --> 00:09:46,320
submitted

00:09:44,399 --> 00:09:48,000
the mechanism through which requests are

00:09:46,320 --> 00:09:50,000
completed and of course

00:09:48,000 --> 00:09:51,519
the code path and the layers of code

00:09:50,000 --> 00:09:54,480
that that are

00:09:51,519 --> 00:09:55,120
parsing requests or or or creating them

00:09:54,480 --> 00:09:57,279
in

00:09:55,120 --> 00:09:58,720
in in memory and so on because that's

00:09:57,279 --> 00:10:01,120
what we can optimize that's under our

00:09:58,720 --> 00:10:01,120
control

00:10:02,079 --> 00:10:06,320
so you might have noticed that i've been

00:10:03,519 --> 00:10:07,920
mentioning latency all the time

00:10:06,320 --> 00:10:10,959
and in this presentation we are going to

00:10:07,920 --> 00:10:14,560
focus on latency but latency is just one

00:10:10,959 --> 00:10:16,800
performance factor there are others

00:10:14,560 --> 00:10:18,000
so request parallelism is another thing

00:10:16,800 --> 00:10:20,000
that that really boosts

00:10:18,000 --> 00:10:21,440
performance and batching is one of the

00:10:20,000 --> 00:10:24,320
techniques to to

00:10:21,440 --> 00:10:26,160
to to also benefit from that what they

00:10:24,320 --> 00:10:27,279
can do is they can hide poor latency

00:10:26,160 --> 00:10:28,560
because you're able to do a lot of

00:10:27,279 --> 00:10:30,959
requests at once

00:10:28,560 --> 00:10:32,560
and that way you're able to get a lot of

00:10:30,959 --> 00:10:33,839
work done even if the latency for one

00:10:32,560 --> 00:10:36,000
request is relatively

00:10:33,839 --> 00:10:37,360
long but that's not what we're looking

00:10:36,000 --> 00:10:38,959
at here the reason why we're looking

00:10:37,360 --> 00:10:41,519
purely at latency

00:10:38,959 --> 00:10:43,680
is because it's a fundamental thing and

00:10:41,519 --> 00:10:46,640
if we optimize latency first

00:10:43,680 --> 00:10:47,440
then we can consider those factors later

00:10:46,640 --> 00:10:50,640
now

00:10:47,440 --> 00:10:53,040
there are latency sensitive applications

00:10:50,640 --> 00:10:54,320
and you can't hide poor latency from

00:10:53,040 --> 00:10:57,279
them and the reason why

00:10:54,320 --> 00:10:59,920
is because they need a specific request

00:10:57,279 --> 00:11:01,519
to complete before they can continue

00:10:59,920 --> 00:11:03,040
even if there's a lot of parallelism

00:11:01,519 --> 00:11:04,720
available to them

00:11:03,040 --> 00:11:06,240
because they need to wait for one

00:11:04,720 --> 00:11:08,640
specific request they

00:11:06,240 --> 00:11:09,680
are bounded by the latency of that one

00:11:08,640 --> 00:11:11,200
request

00:11:09,680 --> 00:11:13,519
and that's what we're trying to optimize

00:11:11,200 --> 00:11:15,440
here and that's what's most obvious

00:11:13,519 --> 00:11:17,279
those types of applications suffer the

00:11:15,440 --> 00:11:19,839
most when

00:11:17,279 --> 00:11:21,519
they're run on on on virtualization on

00:11:19,839 --> 00:11:21,920
extremely fast hardware because that's

00:11:21,519 --> 00:11:25,200
where

00:11:21,920 --> 00:11:26,560
the overheads become apparent so what

00:11:25,200 --> 00:11:27,279
we're going to do is we're going to look

00:11:26,560 --> 00:11:29,680
at

00:11:27,279 --> 00:11:32,240
cue depth one benchmarks and that means

00:11:29,680 --> 00:11:34,240
only submitting one request at a time

00:11:32,240 --> 00:11:35,600
we're also going to focus primarily on

00:11:34,240 --> 00:11:37,519
small block sizes

00:11:35,600 --> 00:11:38,959
i will be showing graphs that have the

00:11:37,519 --> 00:11:41,040
larger block sizes

00:11:38,959 --> 00:11:42,320
just as a reference but what we'll see

00:11:41,040 --> 00:11:44,000
is that

00:11:42,320 --> 00:11:45,920
other factors come into play there maybe

00:11:44,000 --> 00:11:48,000
the data transfer time and so on

00:11:45,920 --> 00:11:50,240
become more relevant and they dominate

00:11:48,000 --> 00:11:52,000
and we're seeing less of the

00:11:50,240 --> 00:11:53,440
completion and submission latency which

00:11:52,000 --> 00:11:56,000
affects applications

00:11:53,440 --> 00:11:58,000
with small block sizes and if you think

00:11:56,000 --> 00:12:00,480
about networking performance the kind of

00:11:58,000 --> 00:12:02,399
analogy or similar thing there is

00:12:00,480 --> 00:12:04,320
benchmarking small packet sizes that

00:12:02,399 --> 00:12:06,560
tends to show

00:12:04,320 --> 00:12:08,079
the cost the per packet cost and it's

00:12:06,560 --> 00:12:10,720
the same thing here that's what we're

00:12:08,079 --> 00:12:13,680
focusing on

00:12:10,720 --> 00:12:14,880
but you can get other perspectives if if

00:12:13,680 --> 00:12:15,600
latency isn't the only thing you're

00:12:14,880 --> 00:12:17,920
interested in

00:12:15,600 --> 00:12:21,040
then take a look at these talks there is

00:12:17,920 --> 00:12:24,160
another talk this year at kvm forum

00:12:21,040 --> 00:12:26,160
which investigates nvme performance and

00:12:24,160 --> 00:12:27,279
last year there was a great talk at kvm

00:12:26,160 --> 00:12:29,200
forum

00:12:27,279 --> 00:12:30,800
that compared storage performance

00:12:29,200 --> 00:12:34,560
between hypervisors

00:12:30,800 --> 00:12:34,560
nvme and various other things

00:12:36,320 --> 00:12:40,720
okay now i mentioned that the mechanisms

00:12:39,920 --> 00:12:43,200
through which we

00:12:40,720 --> 00:12:43,920
complete and submit requests are

00:12:43,200 --> 00:12:45,279
important

00:12:43,920 --> 00:12:47,600
because they can determine the

00:12:45,279 --> 00:12:48,560
performance or they can determine the

00:12:47,600 --> 00:12:50,240
latency

00:12:48,560 --> 00:12:52,000
so let's have a look at the mechanisms

00:12:50,240 --> 00:12:54,880
that are used in

00:12:52,000 --> 00:12:56,240
linux and in kvm today these aren't the

00:12:54,880 --> 00:12:57,360
only mechanisms but they're the main

00:12:56,240 --> 00:12:59,680
ones

00:12:57,360 --> 00:13:00,800
first there is eventfd which is a

00:12:59,680 --> 00:13:03,920
counter

00:13:00,800 --> 00:13:06,560
and it's a file descriptor so

00:13:03,920 --> 00:13:08,399
when you read from this file descriptor

00:13:06,560 --> 00:13:11,200
the counter is reset to zero

00:13:08,399 --> 00:13:12,639
if it's already zero then the read would

00:13:11,200 --> 00:13:14,000
block

00:13:12,639 --> 00:13:15,440
now if the counter is incremented

00:13:14,000 --> 00:13:16,320
multiple times you don't need to read it

00:13:15,440 --> 00:13:17,920
multiple times

00:13:16,320 --> 00:13:19,920
because the single read already resets

00:13:17,920 --> 00:13:21,839
it to zero so what this means is that

00:13:19,920 --> 00:13:24,399
multiple notifications will be

00:13:21,839 --> 00:13:27,120
coalesced into a single read which can

00:13:24,399 --> 00:13:29,519
be nice for performance that helps

00:13:27,120 --> 00:13:31,519
now because it's an event because it's a

00:13:29,519 --> 00:13:34,240
file descriptor

00:13:31,519 --> 00:13:35,760
it relies on the kernel scheduler to

00:13:34,240 --> 00:13:36,480
wake threads because if you're trying to

00:13:35,760 --> 00:13:38,880
read from it

00:13:36,480 --> 00:13:40,399
or you're in a select style system call

00:13:38,880 --> 00:13:41,680
waiting for that file descriptor to

00:13:40,399 --> 00:13:45,120
become ready

00:13:41,680 --> 00:13:47,040
then your your thread may be descheduled

00:13:45,120 --> 00:13:49,760
maybe the physical cpu will even be

00:13:47,040 --> 00:13:53,120
halted and put into a low power state

00:13:49,760 --> 00:13:56,639
and waking up again from a halted cpu

00:13:53,120 --> 00:13:59,600
and resuming the the application thread

00:13:56,639 --> 00:14:01,279
that may have been descheduled has some

00:13:59,600 --> 00:14:02,959
latency

00:14:01,279 --> 00:14:07,040
and so this is not necessarily the the

00:14:02,959 --> 00:14:10,639
lowest latency approach the event fd

00:14:07,040 --> 00:14:13,440
but it is widely used it's used by vfio

00:14:10,639 --> 00:14:14,320
interrupts by kvm's io of nfd and irq

00:14:13,440 --> 00:14:16,320
mechanism

00:14:14,320 --> 00:14:18,639
and by linux aio and iou rings

00:14:16,320 --> 00:14:21,279
completion mechanisms

00:14:18,639 --> 00:14:22,079
so the other approach polling or busy

00:14:21,279 --> 00:14:26,959
waiting

00:14:22,079 --> 00:14:29,040
is also popular so this simply means

00:14:26,959 --> 00:14:31,360
looping and continuously checking for

00:14:29,040 --> 00:14:33,839
the event that you're waiting for

00:14:31,360 --> 00:14:36,160
of course if you keep if you do

00:14:33,839 --> 00:14:38,160
something from a from a tight loop

00:14:36,160 --> 00:14:40,240
that consumes cpu cycles and no other

00:14:38,160 --> 00:14:43,120
tasks can run while you're running

00:14:40,240 --> 00:14:45,040
so it's not very power efficient it is

00:14:43,120 --> 00:14:47,440
it does hog the cpu

00:14:45,040 --> 00:14:49,040
but the advantage of this is when i when

00:14:47,440 --> 00:14:51,120
i mentioned how even fd

00:14:49,040 --> 00:14:52,800
relies on the scheduler and the cpu

00:14:51,120 --> 00:14:53,839
could be halted and there's a latency

00:14:52,800 --> 00:14:56,160
associated with that

00:14:53,839 --> 00:14:57,600
well polling does not have that problem

00:14:56,160 --> 00:15:00,320
because when you're polling

00:14:57,600 --> 00:15:01,199
you are running on the cpu you have

00:15:00,320 --> 00:15:03,120
control

00:15:01,199 --> 00:15:05,199
there is no latency you're just going to

00:15:03,120 --> 00:15:08,800
read the completion value

00:15:05,199 --> 00:15:11,279
and see that the request is now ready

00:15:08,800 --> 00:15:13,040
and so it has great latency and that's

00:15:11,279 --> 00:15:16,480
why it's used it's used by

00:15:13,040 --> 00:15:19,040
qmu's aio context by kvm's hotpoll on s

00:15:16,480 --> 00:15:21,279
by cpu idle hall poll and linux io poll

00:15:19,040 --> 00:15:22,959
and dpdk and spdk all these things

00:15:21,279 --> 00:15:24,880
they use it in different ways in various

00:15:22,959 --> 00:15:27,680
flavors but but busy waiting

00:15:24,880 --> 00:15:29,279
is used in order to reduce latency the

00:15:27,680 --> 00:15:30,000
reason i covered these mechanisms is

00:15:29,279 --> 00:15:32,000
because

00:15:30,000 --> 00:15:33,440
later on when we look at the different

00:15:32,000 --> 00:15:34,800
optimizations available

00:15:33,440 --> 00:15:36,720
and the ones that i've tried out and

00:15:34,800 --> 00:15:38,839
that i'm presenting here they are

00:15:36,720 --> 00:15:41,360
related to these on how to use them

00:15:38,839 --> 00:15:44,160
effectively

00:15:41,360 --> 00:15:45,440
okay so the starting point for nvme

00:15:44,160 --> 00:15:47,120
performance for achieving good

00:15:45,440 --> 00:15:50,480
performance in vms

00:15:47,120 --> 00:15:51,920
is device pci device assignment pci

00:15:50,480 --> 00:15:54,480
device assignment is a way

00:15:51,920 --> 00:15:56,320
that achieves the highest performance

00:15:54,480 --> 00:15:57,680
you can get bare metal performance by

00:15:56,320 --> 00:15:59,920
doing this

00:15:57,680 --> 00:16:02,320
and the reason why this is so efficient

00:15:59,920 --> 00:16:04,720
is because when you take a pci device

00:16:02,320 --> 00:16:07,279
and you pass it through to the guest

00:16:04,720 --> 00:16:08,079
then the host is actually not involved

00:16:07,279 --> 00:16:11,759
in the critical

00:16:08,079 --> 00:16:14,560
path of processing i o requests

00:16:11,759 --> 00:16:17,199
this works because the pci device the

00:16:14,560 --> 00:16:20,079
hardware registers of the nvme drive

00:16:17,199 --> 00:16:20,560
will be memory mapped into the guest so

00:16:20,079 --> 00:16:21,920
that

00:16:20,560 --> 00:16:24,240
accessing them doesn't require the

00:16:21,920 --> 00:16:26,560
hypervisor to be involved

00:16:24,240 --> 00:16:28,880
and the irqs the interrupts that are

00:16:26,560 --> 00:16:31,120
raised by the nvme drive

00:16:28,880 --> 00:16:32,880
they can be directly injected into

00:16:31,120 --> 00:16:34,000
running guests if they are currently

00:16:32,880 --> 00:16:37,040
scheduled on a

00:16:34,000 --> 00:16:38,639
vcp on a cpu and a physical cpu and

00:16:37,040 --> 00:16:40,959
that's thanks to posted interrupts the

00:16:38,639 --> 00:16:44,160
cpu feature that's available

00:16:40,959 --> 00:16:45,920
on some cpus so again there

00:16:44,160 --> 00:16:47,199
the hypervisor doesn't need to have an

00:16:45,920 --> 00:16:48,720
interrupt handler it doesn't need to

00:16:47,199 --> 00:16:50,240
forward that interrupt in the vm

00:16:48,720 --> 00:16:53,120
instead the hardware is able to do that

00:16:50,240 --> 00:16:56,880
directly which is good for latency

00:16:53,120 --> 00:16:59,680
and then finally for guest ram access

00:16:56,880 --> 00:17:00,959
the i o mmu the i o memory management

00:16:59,680 --> 00:17:04,400
unit

00:17:00,959 --> 00:17:05,199
allows the physical pci device the nvme

00:17:04,400 --> 00:17:07,520
drive

00:17:05,199 --> 00:17:08,959
to directly access guest memory so again

00:17:07,520 --> 00:17:11,120
there you're also

00:17:08,959 --> 00:17:12,640
not involving a software layer in the

00:17:11,120 --> 00:17:15,839
hypervisor in order to

00:17:12,640 --> 00:17:17,360
perform any io so that's why it's fast

00:17:15,839 --> 00:17:20,480
and it's a great approach

00:17:17,360 --> 00:17:23,360
for high performance it does have

00:17:20,480 --> 00:17:23,679
some limitations which cause it to not

00:17:23,360 --> 00:17:26,720
be

00:17:23,679 --> 00:17:28,960
as widely deployed um as as

00:17:26,720 --> 00:17:30,640
you know as it would would be great for

00:17:28,960 --> 00:17:33,440
performance but

00:17:30,640 --> 00:17:34,720
it does not support a live migration in

00:17:33,440 --> 00:17:36,799
most cases

00:17:34,720 --> 00:17:38,880
because that device is actually a black

00:17:36,799 --> 00:17:39,360
box to the hypervisor and qmu doesn't

00:17:38,880 --> 00:17:40,880
know

00:17:39,360 --> 00:17:43,039
about what's going on it's unable to

00:17:40,880 --> 00:17:46,000
live migrate it because only the guest

00:17:43,039 --> 00:17:46,880
is using that device and and has the

00:17:46,000 --> 00:17:50,240
driver and the state

00:17:46,880 --> 00:17:50,799
associated with it software features

00:17:50,240 --> 00:17:53,039
like

00:17:50,799 --> 00:17:54,960
backup and snapshots and so that qmu can

00:17:53,039 --> 00:17:56,559
offer when you're using disk images are

00:17:54,960 --> 00:17:59,840
also not available again because the

00:17:56,559 --> 00:18:02,559
hypervisor is bypassed

00:17:59,840 --> 00:18:04,320
thing to consider is that exposing pci

00:18:02,559 --> 00:18:06,880
devices to your guests

00:18:04,320 --> 00:18:08,640
may be inconvenient or may have security

00:18:06,880 --> 00:18:11,360
implications and this is also why

00:18:08,640 --> 00:18:13,520
sometimes it cannot be used

00:18:11,360 --> 00:18:15,360
the issue here is that of course if you

00:18:13,520 --> 00:18:17,440
have varying hardware

00:18:15,360 --> 00:18:19,360
and you want to live my grass live

00:18:17,440 --> 00:18:21,120
migrate your guests around

00:18:19,360 --> 00:18:22,480
or if you just want to upgrade some of

00:18:21,120 --> 00:18:24,720
the hardware in

00:18:22,480 --> 00:18:26,080
in your infrastructure then the guests

00:18:24,720 --> 00:18:27,679
will actually see

00:18:26,080 --> 00:18:30,400
those changes they will see the new

00:18:27,679 --> 00:18:32,400
hardware so they need driver support

00:18:30,400 --> 00:18:33,840
and they might need reconfiguration if

00:18:32,400 --> 00:18:36,000
it's a different

00:18:33,840 --> 00:18:37,280
device that requires different setup and

00:18:36,000 --> 00:18:38,640
so that can be prohibitive

00:18:37,280 --> 00:18:40,480
because in some environments you don't

00:18:38,640 --> 00:18:41,840
control the guests they may be very old

00:18:40,480 --> 00:18:44,400
and so on and so then you don't have the

00:18:41,840 --> 00:18:46,240
freedom to change the hardware

00:18:44,400 --> 00:18:47,679
you might also be concerned about the

00:18:46,240 --> 00:18:50,000
guest being able to

00:18:47,679 --> 00:18:51,840
say do a firmware update on the device

00:18:50,000 --> 00:18:53,520
and so on so there are some issues there

00:18:51,840 --> 00:18:56,080
with exposing those devices so that's

00:18:53,520 --> 00:18:58,240
another thing to watch out for and the

00:18:56,080 --> 00:19:02,000
final thing is simply the cost because

00:18:58,240 --> 00:19:04,799
you do need to dedicate one pci device

00:19:02,000 --> 00:19:06,400
to a particular guest so other guests

00:19:04,799 --> 00:19:07,760
won't be able to use that same device

00:19:06,400 --> 00:19:10,320
that cannot share it

00:19:07,760 --> 00:19:13,200
because only one guest can be the the

00:19:10,320 --> 00:19:16,320
one that's running the driver at a time

00:19:13,200 --> 00:19:17,919
now you can use sriv some devices allow

00:19:16,320 --> 00:19:20,000
you to virtualize them and split them up

00:19:17,919 --> 00:19:24,160
into virtual pci devices

00:19:20,000 --> 00:19:25,919
but that also has has its limits

00:19:24,160 --> 00:19:27,360
so here's the configuration i'm not

00:19:25,919 --> 00:19:29,919
going to go into libvert

00:19:27,360 --> 00:19:30,720
domain xml details in this presentation

00:19:29,919 --> 00:19:32,880
i just want to

00:19:30,720 --> 00:19:35,120
show you the slides so that if you are

00:19:32,880 --> 00:19:37,039
watching this or reading this later on

00:19:35,120 --> 00:19:39,360
you could have links to the

00:19:37,039 --> 00:19:40,160
documentation and and find the keywords

00:19:39,360 --> 00:19:42,080
and things to

00:19:40,160 --> 00:19:43,679
to to look up in order to apply this

00:19:42,080 --> 00:19:47,360
configuration

00:19:43,679 --> 00:19:49,919
okay so we've mentioned that

00:19:47,360 --> 00:19:52,799
for starters pci device assignment is a

00:19:49,919 --> 00:19:54,960
way to get good performance

00:19:52,799 --> 00:19:57,039
it you don't necessarily have the best

00:19:54,960 --> 00:19:58,960
performance right away with pci device

00:19:57,039 --> 00:20:02,480
assignment unless you also consider

00:19:58,960 --> 00:20:05,280
the pneuma topology of your host

00:20:02,480 --> 00:20:07,600
and what this means is that on modern

00:20:05,280 --> 00:20:10,720
systems typically the system is divided

00:20:07,600 --> 00:20:13,919
into multiple domains that are called

00:20:10,720 --> 00:20:17,039
pneuma nodes and a processor and

00:20:13,919 --> 00:20:19,919
memory and pci devices are associated

00:20:17,039 --> 00:20:22,400
with a particular pneuma node accesses

00:20:19,919 --> 00:20:24,799
to the resources within that node

00:20:22,400 --> 00:20:25,520
are local and they are cheaper than

00:20:24,799 --> 00:20:28,640
accesses

00:20:25,520 --> 00:20:30,480
to resources in other nodes and so for

00:20:28,640 --> 00:20:34,240
performance it is important

00:20:30,480 --> 00:20:34,960
that we keep be aware of locality and we

00:20:34,240 --> 00:20:37,360
make sure

00:20:34,960 --> 00:20:39,520
that our operations are within a pneuma

00:20:37,360 --> 00:20:40,960
node where possible

00:20:39,520 --> 00:20:43,280
so the tools that you can use to

00:20:40,960 --> 00:20:44,080
investigate this are pneuma ctl and

00:20:43,280 --> 00:20:46,240
elastopol

00:20:44,080 --> 00:20:48,720
and then you can get an overview of how

00:20:46,240 --> 00:20:50,559
of the topology of your machine

00:20:48,720 --> 00:20:52,240
there are also performance counters that

00:20:50,559 --> 00:20:53,760
you can use if you suspect your

00:20:52,240 --> 00:20:55,919
application might be making

00:20:53,760 --> 00:20:57,440
cross-pneuma node memory accesses and so

00:20:55,919 --> 00:21:00,960
on

00:20:57,440 --> 00:21:03,200
for more information check out this this

00:21:00,960 --> 00:21:04,240
talk that daario is giving at kvm forum

00:21:03,200 --> 00:21:08,960
this year

00:21:04,240 --> 00:21:11,600
um about topology and pneuma

00:21:08,960 --> 00:21:12,000
okay so in terms of how you set things

00:21:11,600 --> 00:21:15,120
up

00:21:12,000 --> 00:21:18,640
libvert has the uh gives you the ability

00:21:15,120 --> 00:21:21,039
to pin vcpu threads to physical cpus

00:21:18,640 --> 00:21:23,039
you can also control qmu's own threads

00:21:21,039 --> 00:21:26,320
the i o threads and the emulator thread

00:21:23,039 --> 00:21:30,400
they can be pinned to physical cpus

00:21:26,320 --> 00:21:32,400
in addition you can control which host

00:21:30,400 --> 00:21:34,880
pneuma node to allocate

00:21:32,400 --> 00:21:36,000
portions of guest ram for and you can

00:21:34,880 --> 00:21:38,320
also expose

00:21:36,000 --> 00:21:40,400
a virtual numa topology you can describe

00:21:38,320 --> 00:21:41,440
a pneuma topology that the guest will

00:21:40,400 --> 00:21:44,320
see

00:21:41,440 --> 00:21:46,559
and the goal there is we want to align

00:21:44,320 --> 00:21:48,640
the guest's virtual numa topology

00:21:46,559 --> 00:21:49,840
with the host's pneuma topology it

00:21:48,640 --> 00:21:52,000
should reflect

00:21:49,840 --> 00:21:54,400
what those resources on the host look

00:21:52,000 --> 00:21:57,280
like and by doing that

00:21:54,400 --> 00:21:59,600
the guest kernel as well as numa aware

00:21:57,280 --> 00:22:02,400
applications running inside the guest

00:21:59,600 --> 00:22:04,159
will be able to make good scheduling and

00:22:02,400 --> 00:22:06,159
allocation decisions because they'll

00:22:04,159 --> 00:22:06,720
have that locality information they'll

00:22:06,159 --> 00:22:08,559
know

00:22:06,720 --> 00:22:10,000
what is cheap and what is expensive so

00:22:08,559 --> 00:22:13,840
they can choose

00:22:10,000 --> 00:22:13,840
um the the best configuration

00:22:14,960 --> 00:22:19,039
now here's a little example this is

00:22:17,200 --> 00:22:21,120
trivial but what's interesting is that

00:22:19,039 --> 00:22:23,520
we can also demonstrate how quickly we

00:22:21,120 --> 00:22:24,159
hit limitations and trade-offs here when

00:22:23,520 --> 00:22:26,960
we do

00:22:24,159 --> 00:22:29,440
pneuma tuning so let's say we have a

00:22:26,960 --> 00:22:33,120
1vcpu guest and what we want to do

00:22:29,440 --> 00:22:37,120
is we want this vm to do i o

00:22:33,120 --> 00:22:38,880
so we have an nvme pci adapter

00:22:37,120 --> 00:22:42,559
and you can see the topology here in the

00:22:38,880 --> 00:22:46,559
diagram on the left side of the slide

00:22:42,559 --> 00:22:49,120
now where should we put the vcpu

00:22:46,559 --> 00:22:50,880
on which node should it run since it's

00:22:49,120 --> 00:22:51,600
going to be doing i o and using the nvme

00:22:50,880 --> 00:22:54,080
drive

00:22:51,600 --> 00:22:56,320
let's place it on node 0 because that's

00:22:54,080 --> 00:22:57,919
where the nvme drive is local

00:22:56,320 --> 00:22:59,280
right so instead of placing on node 1

00:22:57,919 --> 00:23:01,039
where it would have to cross the node we

00:22:59,280 --> 00:23:02,799
put it on node 0. so that's

00:23:01,039 --> 00:23:06,159
that's the starting point so let's pin

00:23:02,799 --> 00:23:08,960
the vcpu thread onto processor 0.

00:23:06,159 --> 00:23:09,360
now if we're using an i o thread in qmu

00:23:08,960 --> 00:23:12,159
which

00:23:09,360 --> 00:23:13,200
we will also get into later on why why

00:23:12,159 --> 00:23:16,480
using i o thread can

00:23:13,200 --> 00:23:17,280
can can be advantageous that is going to

00:23:16,480 --> 00:23:20,320
be doing i o

00:23:17,280 --> 00:23:22,880
on behalf of the guest and so that also

00:23:20,320 --> 00:23:25,919
needs to be where the nvme

00:23:22,880 --> 00:23:27,360
pci adapter is and so we will pin it to

00:23:25,919 --> 00:23:29,840
processor 1

00:23:27,360 --> 00:23:32,000
and of course we're on node 0 so we want

00:23:29,840 --> 00:23:35,520
to be using ram 0. so hopefully

00:23:32,000 --> 00:23:37,200
guest ram fits into ram 0's

00:23:35,520 --> 00:23:39,360
range so there's enough memory there for

00:23:37,200 --> 00:23:41,039
our entire guest that would be great

00:23:39,360 --> 00:23:42,720
and that's the setup but you can already

00:23:41,039 --> 00:23:44,880
start to see some of the the challenges

00:23:42,720 --> 00:23:46,559
what if we wanted a guess that had more

00:23:44,880 --> 00:23:49,520
ram than was available in ram

00:23:46,559 --> 00:23:51,440
0. then maybe we would have to define a

00:23:49,520 --> 00:23:53,600
virtual numa node and use some

00:23:51,440 --> 00:23:55,120
of the memory from ram 1 as well and

00:23:53,600 --> 00:23:56,640
hopefully the guest will then be able to

00:23:55,120 --> 00:23:59,039
make smart decisions about

00:23:56,640 --> 00:24:01,039
what to place into which of these two

00:23:59,039 --> 00:24:02,799
virtual pneuma nodes

00:24:01,039 --> 00:24:04,720
now if we add more guests to this

00:24:02,799 --> 00:24:06,000
picture it becomes even harder because

00:24:04,720 --> 00:24:08,559
at that point we we need to

00:24:06,000 --> 00:24:10,480
maybe make sacrifices decide whether to

00:24:08,559 --> 00:24:13,840
share resources like

00:24:10,480 --> 00:24:17,840
processors across guests or whether to

00:24:13,840 --> 00:24:19,200
assign vcpus to processors that are on

00:24:17,840 --> 00:24:20,480
what is going to turn out to be the

00:24:19,200 --> 00:24:23,919
wrong node

00:24:20,480 --> 00:24:26,640
now today pneuma tuning is something

00:24:23,919 --> 00:24:27,600
that pays off for performance critical

00:24:26,640 --> 00:24:30,640
vms

00:24:27,600 --> 00:24:32,480
doing this manually pays off

00:24:30,640 --> 00:24:34,080
and hopefully in the future we'll see

00:24:32,480 --> 00:24:36,159
more automatic

00:24:34,080 --> 00:24:38,320
pneuma tuning support in the management

00:24:36,159 --> 00:24:40,080
tools that use kvm

00:24:38,320 --> 00:24:42,240
so that they can automatically set these

00:24:40,080 --> 00:24:44,159
things up and we don't need to

00:24:42,240 --> 00:24:45,679
manually tune it it gets especially hard

00:24:44,159 --> 00:24:47,360
when we have a lot of vms

00:24:45,679 --> 00:24:49,120
or when we do live migration so the

00:24:47,360 --> 00:24:50,159
situation is dynamic and it's no longer

00:24:49,120 --> 00:24:53,120
so easy to

00:24:50,159 --> 00:24:54,400
come up with a static pinning that makes

00:24:53,120 --> 00:24:58,240
sense

00:24:54,400 --> 00:25:00,559
okay so we covered

00:24:58,240 --> 00:25:02,799
the importance of pneuma and a bit about

00:25:00,559 --> 00:25:06,240
how to tune it and where to look

00:25:02,799 --> 00:25:08,400
so next up is cpu idle halt poll

00:25:06,240 --> 00:25:10,320
this is going back to the i o request

00:25:08,400 --> 00:25:14,159
life cycle that we looked at

00:25:10,320 --> 00:25:16,320
so what we saw was that the two

00:25:14,159 --> 00:25:18,880
important mechanisms that we have are

00:25:16,320 --> 00:25:22,159
submitting requests and completing them

00:25:18,880 --> 00:25:26,159
well if you have passed through an nvme

00:25:22,159 --> 00:25:26,799
pci device there are interrupts for the

00:25:26,159 --> 00:25:29,120
completion

00:25:26,799 --> 00:25:30,510
when requests complete there there is an

00:25:29,120 --> 00:25:31,679
interrupt

00:25:30,510 --> 00:25:34,720
[Music]

00:25:31,679 --> 00:25:37,600
now halting a vcpu

00:25:34,720 --> 00:25:39,520
involves a vm exit and if there's no

00:25:37,600 --> 00:25:41,520
further work to do even on the host then

00:25:39,520 --> 00:25:44,240
maybe the physical cpu will halt

00:25:41,520 --> 00:25:45,360
two it will go into low power state and

00:25:44,240 --> 00:25:48,880
then when the

00:25:45,360 --> 00:25:51,679
nvme drive completes the request it will

00:25:48,880 --> 00:25:53,120
fire the interrupt the cpu will come out

00:25:51,679 --> 00:25:54,799
of that low power state

00:25:53,120 --> 00:25:57,200
and there's the latency cost associated

00:25:54,799 --> 00:26:00,159
with that and then we can re-enter

00:25:57,200 --> 00:26:02,000
that vcpu can vm enter and you can see

00:26:00,159 --> 00:26:03,279
that this is this becomes a chain of

00:26:02,000 --> 00:26:05,279
several steps

00:26:03,279 --> 00:26:06,960
and it has a latency cost so we want to

00:26:05,279 --> 00:26:09,919
avoid that

00:26:06,960 --> 00:26:11,440
what the v what the cpu idle halt pole

00:26:09,919 --> 00:26:14,480
driver does

00:26:11,440 --> 00:26:17,840
is it runs a busy weight loop

00:26:14,480 --> 00:26:19,360
inside the guest so on that vcpu

00:26:17,840 --> 00:26:21,279
at the point where it decides oh i have

00:26:19,360 --> 00:26:23,120
no more work to do

00:26:21,279 --> 00:26:25,039
so it has a timeout and it says well i'm

00:26:23,120 --> 00:26:26,559
going to try running a busy weight loop

00:26:25,039 --> 00:26:29,760
for a little while at least

00:26:26,559 --> 00:26:32,480
to see if something still becomes

00:26:29,760 --> 00:26:32,960
schedulable and becomes ready to run and

00:26:32,480 --> 00:26:35,360
so

00:26:32,960 --> 00:26:36,960
that way the vcpu is actually running is

00:26:35,360 --> 00:26:38,880
actually active when that interrupt

00:26:36,960 --> 00:26:39,840
comes in and so when that interrupt is

00:26:38,880 --> 00:26:42,640
delivered

00:26:39,840 --> 00:26:43,120
we can quickly schedule the application

00:26:42,640 --> 00:26:44,559
again

00:26:43,120 --> 00:26:46,240
after completing the request and we

00:26:44,559 --> 00:26:46,720
don't need to go through this long path

00:26:46,240 --> 00:26:48,559
of

00:26:46,720 --> 00:26:50,080
going down all the way to a halted

00:26:48,559 --> 00:26:53,120
physical cpu and coming back

00:26:50,080 --> 00:26:54,720
out that decreases latency now this

00:26:53,120 --> 00:26:55,279
mechanism makes sense when you are

00:26:54,720 --> 00:26:58,159
pinning

00:26:55,279 --> 00:26:58,720
vcpus because when you're pulling you're

00:26:58,159 --> 00:27:01,279
you're

00:26:58,720 --> 00:27:02,880
you're wasting those cpu cycles until

00:27:01,279 --> 00:27:05,120
there's some work to do

00:27:02,880 --> 00:27:06,720
and of course if you had lots of vms and

00:27:05,120 --> 00:27:08,080
if they were sharing cpus you wouldn't

00:27:06,720 --> 00:27:08,880
want them all to pull so this is

00:27:08,080 --> 00:27:10,720
something to do

00:27:08,880 --> 00:27:12,159
when you have a high performance or

00:27:10,720 --> 00:27:14,799
performance critical

00:27:12,159 --> 00:27:15,440
vm and you have assigned it a dedicated

00:27:14,799 --> 00:27:17,600
cpu

00:27:15,440 --> 00:27:21,120
okay now here's the tuning this is just

00:27:17,600 --> 00:27:24,399
for the syntax i won't go into detail

00:27:21,120 --> 00:27:26,240
all right so now here's our first graph

00:27:24,399 --> 00:27:28,720
so these are the results for doing

00:27:26,240 --> 00:27:32,320
random reads and random writes

00:27:28,720 --> 00:27:34,960
q depth one on an nvme drive

00:27:32,320 --> 00:27:36,399
and what we see here is the blue bar the

00:27:34,960 --> 00:27:39,840
leftmost one

00:27:36,399 --> 00:27:43,520
is the bare metal result

00:27:39,840 --> 00:27:44,640
with the nvme drive the red bar the one

00:27:43,520 --> 00:27:47,679
in the middle

00:27:44,640 --> 00:27:50,880
is the vfio

00:27:47,679 --> 00:27:53,279
so that is a virtual machine with a pci

00:27:50,880 --> 00:27:55,039
device assigned to it and we can see

00:27:53,279 --> 00:27:56,080
that that's typically lower not in all

00:27:55,039 --> 00:27:58,240
cases but but it's

00:27:56,080 --> 00:27:59,600
it's often lower and even by a

00:27:58,240 --> 00:28:01,679
significant amount

00:27:59,600 --> 00:28:04,000
and then the final result is the vfio

00:28:01,679 --> 00:28:06,799
with that cpu idle halt pole

00:28:04,000 --> 00:28:07,120
driver enabled and you can see that that

00:28:06,799 --> 00:28:10,399
one

00:28:07,120 --> 00:28:11,120
performs very well why is it performing

00:28:10,399 --> 00:28:13,440
better than

00:28:11,120 --> 00:28:16,640
bare metal right how is that possible

00:28:13,440 --> 00:28:19,039
well it's because bare metal

00:28:16,640 --> 00:28:20,480
isn't doing polling and so bare metal

00:28:19,039 --> 00:28:22,720
might halt the cpu

00:28:20,480 --> 00:28:24,559
it will save more power but then it will

00:28:22,720 --> 00:28:27,919
also have higher latency

00:28:24,559 --> 00:28:28,799
and the vm is staying active the cpu is

00:28:27,919 --> 00:28:30,240
still running

00:28:28,799 --> 00:28:31,840
and so it's able to take those

00:28:30,240 --> 00:28:33,200
completion interrupts with lower latency

00:28:31,840 --> 00:28:35,760
so actually in this benchmark it

00:28:33,200 --> 00:28:38,640
achieves higher performance

00:28:35,760 --> 00:28:40,559
so that's what we see here and we're

00:28:38,640 --> 00:28:42,240
going to take this a step further

00:28:40,559 --> 00:28:44,080
there's another polling approach we can

00:28:42,240 --> 00:28:45,520
use and it that will actually make the

00:28:44,080 --> 00:28:48,159
bare metal versus

00:28:45,520 --> 00:28:50,080
virtual machines comparison fairer and

00:28:48,159 --> 00:28:54,960
it will show us the final picture

00:28:50,080 --> 00:28:57,279
so here we go the linux nvme driver

00:28:54,960 --> 00:28:58,799
allows you to allocate cues for a

00:28:57,279 --> 00:29:01,520
specific usage

00:28:58,799 --> 00:29:02,799
so it's possible to actually reserve

00:29:01,520 --> 00:29:05,919
polling cues

00:29:02,799 --> 00:29:08,799
and what those pull mode cues do

00:29:05,919 --> 00:29:11,200
is when an application sets the high

00:29:08,799 --> 00:29:14,240
priority request flag

00:29:11,200 --> 00:29:16,320
then the kernel will busy wait for those

00:29:14,240 --> 00:29:19,440
requests to finish and it just calls

00:29:16,320 --> 00:29:21,840
the poll function in that driver

00:29:19,440 --> 00:29:22,559
in the nvme driver and that function

00:29:21,840 --> 00:29:24,000
will just check

00:29:22,559 --> 00:29:26,320
it will just check memory and have a

00:29:24,000 --> 00:29:28,000
look to see if the request has completed

00:29:26,320 --> 00:29:29,520
yet

00:29:28,000 --> 00:29:31,919
so the kernel can do polling for us and

00:29:29,520 --> 00:29:34,480
this is called io poll

00:29:31,919 --> 00:29:35,120
and so this improves completion latency

00:29:34,480 --> 00:29:37,360
and actually

00:29:35,120 --> 00:29:38,960
more than cpu idle halt paul because

00:29:37,360 --> 00:29:42,640
here we're guaranteed to

00:29:38,960 --> 00:29:45,360
be spinning and it doesn't give up

00:29:42,640 --> 00:29:46,399
it it keeps running in its default mode

00:29:45,360 --> 00:29:48,000
at least

00:29:46,399 --> 00:29:49,679
and this allows us to make a fair

00:29:48,000 --> 00:29:52,960
comparison because this is

00:29:49,679 --> 00:29:56,320
done by the driver in both bare metal

00:29:52,960 --> 00:29:57,600
and in in the virtual machine case

00:29:56,320 --> 00:29:59,039
so at the bottom of the slide here is

00:29:57,600 --> 00:30:00,960
the syntax in case you want to see how

00:29:59,039 --> 00:30:02,960
to enable it

00:30:00,960 --> 00:30:04,640
and let's look at the performance

00:30:02,960 --> 00:30:06,880
numbers

00:30:04,640 --> 00:30:09,039
so as you can see on this graph at this

00:30:06,880 --> 00:30:12,159
point when we enable that feature

00:30:09,039 --> 00:30:14,080
first of all the absolute number of iops

00:30:12,159 --> 00:30:15,360
the number of i o operations per second

00:30:14,080 --> 00:30:18,640
that we can

00:30:15,360 --> 00:30:19,840
achieve have jumped the max we were at

00:30:18,640 --> 00:30:22,000
was around 80k

00:30:19,840 --> 00:30:23,520
before we used iopul now that we're

00:30:22,000 --> 00:30:26,880
polling all the time

00:30:23,520 --> 00:30:29,919
we get up to over 120k iops

00:30:26,880 --> 00:30:32,799
for qdep1 4 kilobyte

00:30:29,919 --> 00:30:34,480
requests so you can see that's a

00:30:32,799 --> 00:30:37,360
significant performance improvement

00:30:34,480 --> 00:30:40,159
enabling iopal

00:30:37,360 --> 00:30:40,640
the other thing we see here is that the

00:30:40,159 --> 00:30:44,720
gap

00:30:40,640 --> 00:30:47,600
between bare metal and vms has closed

00:30:44,720 --> 00:30:48,000
so at this point they're very similar

00:30:47,600 --> 00:30:49,840
and

00:30:48,000 --> 00:30:51,200
you can say that using pci device

00:30:49,840 --> 00:30:53,760
passthrough you can get bare metal

00:30:51,200 --> 00:30:53,760
performance

00:30:54,080 --> 00:30:58,320
so that's great but what about

00:30:56,159 --> 00:31:00,000
situations where you cannot use

00:30:58,320 --> 00:31:01,440
pci device assignment right i had this

00:31:00,000 --> 00:31:03,440
big disclaimer i

00:31:01,440 --> 00:31:04,960
i covered all the cons and why why you

00:31:03,440 --> 00:31:07,600
sometimes cannot use it

00:31:04,960 --> 00:31:08,080
well in that case you can use virtio

00:31:07,600 --> 00:31:10,080
block

00:31:08,080 --> 00:31:12,000
and virtio block is an emulated storage

00:31:10,080 --> 00:31:14,240
controller it's a para virtualized

00:31:12,000 --> 00:31:15,760
device that was designed specifically

00:31:14,240 --> 00:31:17,360
for virtualization so it's been

00:31:15,760 --> 00:31:19,600
optimized over the years

00:31:17,360 --> 00:31:21,519
and has evolved and so it's a good

00:31:19,600 --> 00:31:23,679
storage controller to choose

00:31:21,519 --> 00:31:25,600
if you want to get good performance with

00:31:23,679 --> 00:31:27,120
kvm

00:31:25,600 --> 00:31:28,880
there are two settings though that i

00:31:27,120 --> 00:31:30,399
want to discuss here because they're not

00:31:28,880 --> 00:31:32,080
enabled by default yet

00:31:30,399 --> 00:31:33,679
and they do boost performance so they're

00:31:32,080 --> 00:31:36,559
worth considering

00:31:33,679 --> 00:31:38,240
the first one is multi-q although the

00:31:36,559 --> 00:31:41,519
feature has been there for years

00:31:38,240 --> 00:31:44,399
it hasn't really been used

00:31:41,519 --> 00:31:46,000
and in qmu 5.2 that's going to change in

00:31:44,399 --> 00:31:47,760
qmi 5.2 the number

00:31:46,000 --> 00:31:49,120
of queues is going to default to the

00:31:47,760 --> 00:31:51,679
number of vcpus

00:31:49,120 --> 00:31:53,919
in other words multi-q will be enabled

00:31:51,679 --> 00:31:56,559
by default on virtio block and also on

00:31:53,919 --> 00:31:58,480
video scuzzy devices

00:31:56,559 --> 00:32:00,799
the reason why this is a win the reason

00:31:58,480 --> 00:32:04,080
why this improves performance

00:32:00,799 --> 00:32:07,279
is because giving every

00:32:04,080 --> 00:32:11,600
cpu every vcpu a dedicated queue

00:32:07,279 --> 00:32:14,720
means that now the completion interrupts

00:32:11,600 --> 00:32:16,159
they can be directed at the cpu that

00:32:14,720 --> 00:32:17,919
submitted the i o

00:32:16,159 --> 00:32:19,360
and that's the one where the task is

00:32:17,919 --> 00:32:20,000
scheduled and where we want to do our

00:32:19,360 --> 00:32:22,000
completion

00:32:20,000 --> 00:32:24,240
we don't want that interrupt to go to

00:32:22,000 --> 00:32:27,120
some other vcpu that then says oh

00:32:24,240 --> 00:32:28,320
okay i'd better wake up that task that's

00:32:27,120 --> 00:32:30,720
ready to run on

00:32:28,320 --> 00:32:33,760
on another cpu and i'll send a message

00:32:30,720 --> 00:32:37,600
we don't want inter processor interrupts

00:32:33,760 --> 00:32:40,399
so by giving every vcpu its own cue

00:32:37,600 --> 00:32:42,159
we can eliminate that and we can improve

00:32:40,399 --> 00:32:44,720
interrupt completion latency

00:32:42,159 --> 00:32:47,039
so that's why multicue helps in addition

00:32:44,720 --> 00:32:49,679
to this the linux block layer also has

00:32:47,039 --> 00:32:51,279
multi-cue support and there are some

00:32:49,679 --> 00:32:52,640
code paths in the linux block layer that

00:32:51,279 --> 00:32:55,519
take advantage of it

00:32:52,640 --> 00:32:56,320
when the driver only allocates one queue

00:32:55,519 --> 00:32:59,519
we don't

00:32:56,320 --> 00:33:01,600
take those code paths and the most

00:32:59,519 --> 00:33:03,200
the most obvious user visible effect of

00:33:01,600 --> 00:33:05,120
this for example is that the i o

00:33:03,200 --> 00:33:08,720
scheduler is different

00:33:05,120 --> 00:33:10,080
for devices that have more than one

00:33:08,720 --> 00:33:11,760
queue and that are multi-cue block

00:33:10,080 --> 00:33:14,559
drivers so

00:33:11,760 --> 00:33:18,000
that affects latency too and so it's

00:33:14,559 --> 00:33:18,000
it's best to enable multi-cue

00:33:18,159 --> 00:33:23,679
next there's the virtio 1.1 packed vert

00:33:20,880 --> 00:33:26,320
q layout so this is a new memory layout

00:33:23,679 --> 00:33:28,320
for the cues that virtio devices use and

00:33:26,320 --> 00:33:30,960
this layout is more efficient

00:33:28,320 --> 00:33:32,960
in the benchmarks that i've run i've

00:33:30,960 --> 00:33:34,080
seen that it improves video block

00:33:32,960 --> 00:33:36,840
performance

00:33:34,080 --> 00:33:39,360
so this is also worth taking into

00:33:36,840 --> 00:33:40,559
account it's not a huge win but it is a

00:33:39,360 --> 00:33:42,480
small win and

00:33:40,559 --> 00:33:43,679
and and when devices are so low latency

00:33:42,480 --> 00:33:46,960
then every small

00:33:43,679 --> 00:33:46,960
win is worth taking

00:33:48,000 --> 00:33:54,159
okay now here's the syntax we'll move on

00:33:52,399 --> 00:33:56,080
and what we're going to do is i'll i'll

00:33:54,159 --> 00:33:56,880
walk through several more configuration

00:33:56,080 --> 00:33:58,399
topics

00:33:56,880 --> 00:34:00,320
and then i'll walk through some

00:33:58,399 --> 00:34:01,200
optimizations that i've implemented some

00:34:00,320 --> 00:34:03,760
prototypes

00:34:01,200 --> 00:34:05,360
some new stuff and finally at the end

00:34:03,760 --> 00:34:07,519
we'll look at a graph that

00:34:05,360 --> 00:34:08,399
stacks them all up and we can see how

00:34:07,519 --> 00:34:11,599
incrementally

00:34:08,399 --> 00:34:13,760
by combining them we can increase iops

00:34:11,599 --> 00:34:15,839
significantly

00:34:13,760 --> 00:34:17,760
so here we are i o threads is the next

00:34:15,839 --> 00:34:18,839
feature if you're using vertical block

00:34:17,760 --> 00:34:22,240
that is

00:34:18,839 --> 00:34:25,280
critical i o threads are a way

00:34:22,240 --> 00:34:27,040
for defining threads and assigning

00:34:25,280 --> 00:34:31,440
devices to them so it gives

00:34:27,040 --> 00:34:35,520
users control over which physical cpu

00:34:31,440 --> 00:34:37,679
device emulation and i o will run on

00:34:35,520 --> 00:34:38,639
so there's an end to one mapping there

00:34:37,679 --> 00:34:41,839
you can assign

00:34:38,639 --> 00:34:43,760
multiple devices to a single i o thread

00:34:41,839 --> 00:34:44,800
and you can define multiple i o threads

00:34:43,760 --> 00:34:46,079
as well so it gives you a lot of

00:34:44,800 --> 00:34:47,839
flexibility

00:34:46,079 --> 00:34:49,839
and this is great because it allows us

00:34:47,839 --> 00:34:50,879
to reflect the new topology in our

00:34:49,839 --> 00:34:52,879
system

00:34:50,879 --> 00:34:54,480
it is also good for scalability because

00:34:52,879 --> 00:34:57,440
when you have vms with

00:34:54,480 --> 00:34:58,720
many devices that are doing heavy io you

00:34:57,440 --> 00:35:00,720
may want to

00:34:58,720 --> 00:35:02,400
put them into separate io threads and

00:35:00,720 --> 00:35:03,520
run them in a separate cpu so that

00:35:02,400 --> 00:35:05,760
there's

00:35:03,520 --> 00:35:07,920
enough resources for both of them and no

00:35:05,760 --> 00:35:09,440
interference between them

00:35:07,920 --> 00:35:11,920
the final thing i want to mention about

00:35:09,440 --> 00:35:13,760
i o threads is that

00:35:11,920 --> 00:35:16,160
the i o threads feature when it's

00:35:13,760 --> 00:35:17,119
enabled that device will be able to take

00:35:16,160 --> 00:35:20,240
advantage of an

00:35:17,119 --> 00:35:22,160
adaptive polling event loop in qemu it's

00:35:20,240 --> 00:35:24,400
a different code path from cumi's main

00:35:22,160 --> 00:35:25,280
loop and it has lower latency because

00:35:24,400 --> 00:35:28,320
it's able to do

00:35:25,280 --> 00:35:31,280
polling instead of always

00:35:28,320 --> 00:35:34,079
yielding when it waits for file

00:35:31,280 --> 00:35:36,000
descriptors to become ready

00:35:34,079 --> 00:35:37,280
so that's another reason why it's faster

00:35:36,000 --> 00:35:40,640
and we'll see the numbers

00:35:37,280 --> 00:35:40,640
later on when i show the graph

00:35:40,720 --> 00:35:44,320
so here's the configuration the the

00:35:43,520 --> 00:35:46,960
things you can do

00:35:44,320 --> 00:35:48,320
for defining i o threads pinning them on

00:35:46,960 --> 00:35:51,680
the host and then

00:35:48,320 --> 00:35:55,280
assigning devices to i o threads

00:35:51,680 --> 00:35:55,280
in libvard xml syntax

00:35:55,520 --> 00:35:59,839
okay so now we're going to move on to

00:35:57,760 --> 00:36:01,599
things that aren't as standard yet

00:35:59,839 --> 00:36:03,040
things that aren't as widely known and

00:36:01,599 --> 00:36:05,280
as widely used

00:36:03,040 --> 00:36:07,359
there has been a user space nvme driver

00:36:05,280 --> 00:36:08,720
in qmu for some time now

00:36:07,359 --> 00:36:10,480
it's been there for a long time but it

00:36:08,720 --> 00:36:13,760
hasn't been used

00:36:10,480 --> 00:36:16,000
very widely yet what it is is

00:36:13,760 --> 00:36:17,359
it's somewhat similar to pci device

00:36:16,000 --> 00:36:20,079
assignment in that

00:36:17,359 --> 00:36:22,240
that pci device that nvme drive can be

00:36:20,079 --> 00:36:24,400
assigned to a particular vm

00:36:22,240 --> 00:36:26,800
however instead of passing the device

00:36:24,400 --> 00:36:28,640
through into the guest and exposing that

00:36:26,800 --> 00:36:30,560
physical device to the guest

00:36:28,640 --> 00:36:33,599
we still have an emulated virtio block

00:36:30,560 --> 00:36:36,480
device that the guest uses and sees

00:36:33,599 --> 00:36:37,760
and in qmu we have the driver so it's in

00:36:36,480 --> 00:36:40,800
qmu user space

00:36:37,760 --> 00:36:41,680
in the host and it's an nvme driver and

00:36:40,800 --> 00:36:44,880
so

00:36:41,680 --> 00:36:48,240
what this means is that we're able to

00:36:44,880 --> 00:36:52,079
get some performance benefits from

00:36:48,240 --> 00:36:53,680
having a user space driver that bypasses

00:36:52,079 --> 00:36:55,839
the kernel that means no system calls

00:36:53,680 --> 00:36:57,200
are necessary and and there is a shorter

00:36:55,839 --> 00:36:59,119
code path that's completely under the

00:36:57,200 --> 00:37:02,880
control of qmu

00:36:59,119 --> 00:37:04,480
um while still offering qmu's block

00:37:02,880 --> 00:37:07,119
layer features things like

00:37:04,480 --> 00:37:08,880
live migration or snapshots and so on

00:37:07,119 --> 00:37:11,520
even image formats can

00:37:08,880 --> 00:37:13,599
work on top of the user space driver so

00:37:11,520 --> 00:37:14,240
this solves some of the limitations of

00:37:13,599 --> 00:37:17,920
pci

00:37:14,240 --> 00:37:19,200
device pass-through and right now

00:37:17,920 --> 00:37:21,359
some improvements are being made

00:37:19,200 --> 00:37:23,119
upstream and activity has

00:37:21,359 --> 00:37:26,160
started up again around around this

00:37:23,119 --> 00:37:27,040
driver non-x86 architecture support is

00:37:26,160 --> 00:37:30,320
being added

00:37:27,040 --> 00:37:36,079
multi-cue is being added and more

00:37:30,320 --> 00:37:38,560
so this is the syntax for configuring it

00:37:36,079 --> 00:37:40,640
one thing that is missing from the nvme

00:37:38,560 --> 00:37:42,640
user space driver in qmu

00:37:40,640 --> 00:37:44,160
that i wanted to add an optimization i

00:37:42,640 --> 00:37:46,720
wanted to try out

00:37:44,160 --> 00:37:47,920
is polled cues because in nvme when you

00:37:46,720 --> 00:37:51,040
create a queue

00:37:47,920 --> 00:37:52,960
you assign to it an interrupt um

00:37:51,040 --> 00:37:55,119
the completion queue has an interrupt

00:37:52,960 --> 00:37:58,320
and you can turn that off completely

00:37:55,119 --> 00:37:59,680
while creating the queue and

00:37:58,320 --> 00:38:02,079
when you have a queue that doesn't have

00:37:59,680 --> 00:38:04,240
an interrupt you can just poll

00:38:02,079 --> 00:38:05,920
for completions you can just look at the

00:38:04,240 --> 00:38:08,240
memory and see when those requests

00:38:05,920 --> 00:38:10,640
become ready

00:38:08,240 --> 00:38:11,440
and so doing so is an alternative to

00:38:10,640 --> 00:38:12,880
interrupts

00:38:11,440 --> 00:38:14,560
effectively this is kind of like

00:38:12,880 --> 00:38:17,599
switching from

00:38:14,560 --> 00:38:19,839
the eventfd style mechanism to

00:38:17,599 --> 00:38:20,880
a polling mechanism and so we hope that

00:38:19,839 --> 00:38:23,599
it will reduce

00:38:20,880 --> 00:38:24,640
latency one interesting thing about

00:38:23,599 --> 00:38:27,440
doing this though

00:38:24,640 --> 00:38:29,040
was that it requires changes to qmu's

00:38:27,440 --> 00:38:31,040
event loop itself

00:38:29,040 --> 00:38:33,040
because qmu's event loop is really

00:38:31,040 --> 00:38:34,720
fundamentally designed for file

00:38:33,040 --> 00:38:36,560
descriptor monitoring

00:38:34,720 --> 00:38:38,880
the adaptive polling has been added to

00:38:36,560 --> 00:38:40,880
it but the whole premise is that we only

00:38:38,880 --> 00:38:43,359
poll for short amounts of time

00:38:40,880 --> 00:38:44,640
now when we have a pull mode queue in

00:38:43,359 --> 00:38:47,119
the nvme driver

00:38:44,640 --> 00:38:48,640
we need to pull all the time but if we

00:38:47,119 --> 00:38:50,560
pull all the time we will be starving

00:38:48,640 --> 00:38:53,520
the file descriptors right because

00:38:50,560 --> 00:38:54,480
we're just spinning in our busy loop but

00:38:53,520 --> 00:38:55,680
we're not looking at the file

00:38:54,480 --> 00:38:59,040
descriptors

00:38:55,680 --> 00:39:00,880
so um i've i have some patches that i'm

00:38:59,040 --> 00:39:02,480
going to send up stream that extend the

00:39:00,880 --> 00:39:04,079
the event loop and in fact some of the

00:39:02,480 --> 00:39:07,440
iou ring work has

00:39:04,079 --> 00:39:09,440
already gone upstream and i found that

00:39:07,440 --> 00:39:10,560
using i o u ring we're able to do this

00:39:09,440 --> 00:39:12,320
efficiently

00:39:10,560 --> 00:39:16,079
and integrate it into the busy weight

00:39:12,320 --> 00:39:18,320
loop without using syscalls

00:39:16,079 --> 00:39:20,160
okay so we'll look at the numbers for

00:39:18,320 --> 00:39:22,720
that in the end

00:39:20,160 --> 00:39:24,560
the next thing i want to share is an

00:39:22,720 --> 00:39:26,720
idea that in one form or another has

00:39:24,560 --> 00:39:29,520
already been around for a long time

00:39:26,720 --> 00:39:31,200
in 2014 when we introduced co-routines

00:39:29,520 --> 00:39:32,400
into the core block layer and started

00:39:31,200 --> 00:39:35,280
using them for request

00:39:32,400 --> 00:39:37,040
processing uh that was very useful

00:39:35,280 --> 00:39:39,839
because we needed them for

00:39:37,040 --> 00:39:40,720
for things like i o throttling and and

00:39:39,839 --> 00:39:42,079
some of the

00:39:40,720 --> 00:39:44,400
the operations that were just getting

00:39:42,079 --> 00:39:47,680
really really complex and difficult

00:39:44,400 --> 00:39:49,839
to write in an asynchronous style so now

00:39:47,680 --> 00:39:52,640
there's request cueing and so on in in

00:39:49,839 --> 00:39:54,640
the core block layer in qmu

00:39:52,640 --> 00:39:56,400
but there was concerns even back then

00:39:54,640 --> 00:39:59,119
that maybe this overhead

00:39:56,400 --> 00:39:59,920
will become a problem and so there have

00:39:59,119 --> 00:40:02,640
been discussions

00:39:59,920 --> 00:40:03,680
in the past about can we optimize it

00:40:02,640 --> 00:40:06,160
away

00:40:03,680 --> 00:40:06,720
and really the thing is when you are not

00:40:06,160 --> 00:40:09,119
using

00:40:06,720 --> 00:40:11,359
certain qmu features like disk image

00:40:09,119 --> 00:40:12,319
formats or io throttling or storage

00:40:11,359 --> 00:40:14,319
migration

00:40:12,319 --> 00:40:16,240
while those things are inactive you

00:40:14,319 --> 00:40:18,560
don't really need to do the full request

00:40:16,240 --> 00:40:20,240
processing all that machinery that

00:40:18,560 --> 00:40:21,200
infrastructure is only needed to support

00:40:20,240 --> 00:40:22,880
those features

00:40:21,200 --> 00:40:25,839
so wouldn't it be great if there was a

00:40:22,880 --> 00:40:27,599
way to bypass it when it's not needed

00:40:25,839 --> 00:40:29,200
so as a prototype i've tried

00:40:27,599 --> 00:40:31,359
implementing this i've tried

00:40:29,200 --> 00:40:34,079
implementing an aio fast path

00:40:31,359 --> 00:40:35,119
what it does is it introduces an aio

00:40:34,079 --> 00:40:37,520
interface

00:40:35,119 --> 00:40:38,400
to the block drivers in qmu because

00:40:37,520 --> 00:40:40,160
currently

00:40:38,400 --> 00:40:42,000
they have a co-routine interface which

00:40:40,160 --> 00:40:45,920
kind of assumes that you're in this

00:40:42,000 --> 00:40:45,920
full request processing mode

00:40:46,079 --> 00:40:49,599
and that allows the virtio block

00:40:48,560 --> 00:40:52,960
emulation

00:40:49,599 --> 00:40:54,960
to call the nvme user space driver with

00:40:52,960 --> 00:40:56,880
with relatively little overhead and we

00:40:54,960 --> 00:40:59,040
can skip the full request processing

00:40:56,880 --> 00:41:01,839
step

00:40:59,040 --> 00:41:03,200
so we'll see those numbers the next

00:41:01,839 --> 00:41:06,400
thing i want to mention

00:41:03,200 --> 00:41:08,720
is that when we looked at pci device

00:41:06,400 --> 00:41:12,160
assignment we saw how beneficial

00:41:08,720 --> 00:41:15,760
linux i o poll is we saw that

00:41:12,160 --> 00:41:18,800
polling for the requests from the

00:41:15,760 --> 00:41:20,800
nvme driver is

00:41:18,800 --> 00:41:23,280
it reduces latency and it got us the

00:41:20,800 --> 00:41:25,680
highest the highest iops that that we've

00:41:23,280 --> 00:41:29,920
achieved so far we had this 120k

00:41:25,680 --> 00:41:31,040
iops bare metal so virtio block today

00:41:29,920 --> 00:41:33,200
the guest driver

00:41:31,040 --> 00:41:34,560
for virtio block does not implement this

00:41:33,200 --> 00:41:35,520
interface yet but it's a driver

00:41:34,560 --> 00:41:36,880
interface in fact

00:41:35,520 --> 00:41:39,280
it's this one function that we need to

00:41:36,880 --> 00:41:42,240
implement and so

00:41:39,280 --> 00:41:42,880
i have also written a prototype for that

00:41:42,240 --> 00:41:45,040
um

00:41:42,880 --> 00:41:46,720
it only supports qdep1 because that's

00:41:45,040 --> 00:41:48,720
what i was benchmarking it's not a

00:41:46,720 --> 00:41:50,000
full implementation it's a prototype to

00:41:48,720 --> 00:41:51,200
check

00:41:50,000 --> 00:41:53,359
what kind of effect it has on

00:41:51,200 --> 00:41:54,000
performance and the link to the get

00:41:53,359 --> 00:41:57,520
branches

00:41:54,000 --> 00:42:00,000
is on the slide so we'll look at that

00:41:57,520 --> 00:42:01,119
here we are so this is the final

00:42:00,000 --> 00:42:02,960
incremental

00:42:01,119 --> 00:42:05,599
applying all of these optimizations on

00:42:02,960 --> 00:42:07,520
top of each other and how far it gets us

00:42:05,599 --> 00:42:09,760
on the left hand side the starting

00:42:07,520 --> 00:42:10,640
position we want to look at bare metal

00:42:09,760 --> 00:42:14,400
and without

00:42:10,640 --> 00:42:18,079
i o pull bare metal is at 78k iops

00:42:14,400 --> 00:42:21,280
now when we configure qmu with a file

00:42:18,079 --> 00:42:24,319
um aio equals native so this is a

00:42:21,280 --> 00:42:27,119
a standard non-optimized setup

00:42:24,319 --> 00:42:27,839
and we don't use the i o thread then we

00:42:27,119 --> 00:42:30,160
start at

00:42:27,839 --> 00:42:32,319
21k iops so that's extremely low we can

00:42:30,160 --> 00:42:34,000
see there's a lot of overhead

00:42:32,319 --> 00:42:36,960
i wouldn't necessarily say that this is

00:42:34,000 --> 00:42:38,880
what most qmu users experience today

00:42:36,960 --> 00:42:40,480
because io threads is recommended and

00:42:38,880 --> 00:42:43,680
more and more of the management

00:42:40,480 --> 00:42:44,720
tools built on top of kvm and qmu have

00:42:43,680 --> 00:42:48,560
been using it

00:42:44,720 --> 00:42:50,880
by default so hopefully more

00:42:48,560 --> 00:42:51,760
hopefully most users today are around

00:42:50,880 --> 00:42:54,000
the second

00:42:51,760 --> 00:42:56,560
blue bar the i o thread bar so when we

00:42:54,000 --> 00:42:59,760
add i o threads

00:42:56,560 --> 00:43:02,000
then we are at around 46k iops there's

00:42:59,760 --> 00:43:05,599
still significant overhead right

00:43:02,000 --> 00:43:08,000
it's still pretty bad so next up we can

00:43:05,599 --> 00:43:09,680
enable virtio block multi queue

00:43:08,000 --> 00:43:11,040
now doing it at this stage actually

00:43:09,680 --> 00:43:13,599
turned out not to be

00:43:11,040 --> 00:43:14,800
very instructive although it slightly

00:43:13,599 --> 00:43:16,640
improved performance

00:43:14,800 --> 00:43:17,920
it wasn't it wasn't very significant in

00:43:16,640 --> 00:43:20,560
this graph

00:43:17,920 --> 00:43:22,400
but it's still essential part of the

00:43:20,560 --> 00:43:23,760
reason why it didn't improve performance

00:43:22,400 --> 00:43:25,280
very much in this graph is because i was

00:43:23,760 --> 00:43:27,280
already using pinning both inside the

00:43:25,280 --> 00:43:28,960
guest and on the host and so on so

00:43:27,280 --> 00:43:30,720
everything was already set up optimally

00:43:28,960 --> 00:43:32,160
adding the cues didn't help the i o

00:43:30,720 --> 00:43:34,960
scheduler was already

00:43:32,160 --> 00:43:36,480
none so that didn't help and so on but

00:43:34,960 --> 00:43:38,079
it is an essential part

00:43:36,480 --> 00:43:41,520
of making things scale and making things

00:43:38,079 --> 00:43:45,119
work so keep multi-cue

00:43:41,520 --> 00:43:46,880
next up we introduce the user space nvme

00:43:45,119 --> 00:43:48,319
driver in qmu and that boosts

00:43:46,880 --> 00:43:51,599
performance so we jump

00:43:48,319 --> 00:43:55,119
almost uh 10k from 46k

00:43:51,599 --> 00:43:58,240
to 55k so that's that's a nice boost

00:43:55,119 --> 00:43:58,800
getting us closer to bare metal now what

00:43:58,240 --> 00:44:01,520
happens

00:43:58,800 --> 00:44:02,960
when we try the vertio block guest

00:44:01,520 --> 00:44:06,240
drivers iopal

00:44:02,960 --> 00:44:09,440
prototype so adding that on

00:44:06,240 --> 00:44:11,520
brings us up to the above the initial

00:44:09,440 --> 00:44:14,319
bare metal number that we collected

00:44:11,520 --> 00:44:16,960
without iopol on the host side

00:44:14,319 --> 00:44:18,480
so what this is doing is um now that

00:44:16,960 --> 00:44:19,520
we're we're polling and we're using more

00:44:18,480 --> 00:44:22,079
cpu cycles

00:44:19,520 --> 00:44:23,599
we are able to make some ground there

00:44:22,079 --> 00:44:26,800
we're able to

00:44:23,599 --> 00:44:28,560
reduce the latency that we had so this

00:44:26,800 --> 00:44:29,760
is looking good but it's also unfair

00:44:28,560 --> 00:44:30,960
because now really we should be

00:44:29,760 --> 00:44:33,520
comparing against

00:44:30,960 --> 00:44:34,720
a bare metal that is also using io poll

00:44:33,520 --> 00:44:36,240
so let's do that

00:44:34,720 --> 00:44:39,040
on the right hand side of the graph you

00:44:36,240 --> 00:44:42,160
see the gray bar that is 121

00:44:39,040 --> 00:44:45,359
120 k iops that's bare metal with

00:44:42,160 --> 00:44:46,640
nvme i o pol so we're still behind we

00:44:45,359 --> 00:44:48,079
still have overhead but

00:44:46,640 --> 00:44:50,079
our absolute number of iops has

00:44:48,079 --> 00:44:53,040
increased um

00:44:50,079 --> 00:44:53,680
quite quite well and we're not done yet

00:44:53,040 --> 00:44:56,800
so next

00:44:53,680 --> 00:44:59,920
up we can try the nvme user space

00:44:56,800 --> 00:45:01,280
drivers polling queues where we are

00:44:59,920 --> 00:45:04,480
polling in qemu

00:45:01,280 --> 00:45:07,359
in the i o thread and

00:45:04,480 --> 00:45:08,800
this brings us up to 94k iops so that's

00:45:07,359 --> 00:45:10,319
definitely a worthwhile improvement a

00:45:08,800 --> 00:45:12,960
nice jump there

00:45:10,319 --> 00:45:14,160
and then finally the aio fast path that

00:45:12,960 --> 00:45:16,640
i just mentioned

00:45:14,160 --> 00:45:17,839
so this is the final optimization that

00:45:16,640 --> 00:45:18,560
i'm going to show today the final

00:45:17,839 --> 00:45:21,040
prototype

00:45:18,560 --> 00:45:23,440
that that i wanted to share and that

00:45:21,040 --> 00:45:25,680
bypasses the the full request processing

00:45:23,440 --> 00:45:28,960
in qmu and as you can see so that that

00:45:25,680 --> 00:45:31,520
that is another 10k uh

00:45:28,960 --> 00:45:32,800
further towards closing the gap to bare

00:45:31,520 --> 00:45:34,079
metal

00:45:32,800 --> 00:45:35,839
so that's the status that's what i

00:45:34,079 --> 00:45:38,640
wanted to share with you

00:45:35,839 --> 00:45:42,480
and i am working on upstreaming these

00:45:38,640 --> 00:45:42,480
optimizations so that they can be used

00:45:43,119 --> 00:45:46,960
but the entire vertical block and nvme

00:45:46,240 --> 00:45:50,160
approach

00:45:46,960 --> 00:45:51,920
with the nvme driver nvme user space

00:45:50,160 --> 00:45:54,319
driver has still left us with

00:45:51,920 --> 00:45:55,200
a similar limitation as pci passed to we

00:45:54,319 --> 00:45:59,520
still need

00:45:55,200 --> 00:46:02,079
one device per guest luckily this year

00:45:59,520 --> 00:46:04,240
a new tool has been added to qmu called

00:46:02,079 --> 00:46:06,560
qmu storage daemon

00:46:04,240 --> 00:46:09,760
and this is a separate program that has

00:46:06,560 --> 00:46:11,680
qmu storage related functionality

00:46:09,760 --> 00:46:13,920
in addition to that a vhost user block

00:46:11,680 --> 00:46:16,240
server has also been added to qmu which

00:46:13,920 --> 00:46:18,720
is very convenient it means that now

00:46:16,240 --> 00:46:19,359
we can use the storage daemon we can

00:46:18,720 --> 00:46:23,040
host

00:46:19,359 --> 00:46:25,839
the nvme user space driver inside it

00:46:23,040 --> 00:46:26,880
and that one daemon can serve multiple

00:46:25,839 --> 00:46:29,760
guests

00:46:26,880 --> 00:46:30,480
so we now have the ability to share a

00:46:29,760 --> 00:46:32,800
single

00:46:30,480 --> 00:46:34,720
pci device and use the user space driver

00:46:32,800 --> 00:46:36,800
and have multiple guests so that solves

00:46:34,720 --> 00:46:39,520
that limitation

00:46:36,800 --> 00:46:40,560
it's already available in cumu.kit but

00:46:39,520 --> 00:46:42,400
the code path

00:46:40,560 --> 00:46:44,079
is different from the vertical block

00:46:42,400 --> 00:46:45,920
results that i presented to you

00:46:44,079 --> 00:46:47,599
so those optimizations don't apply it

00:46:45,920 --> 00:46:48,319
some of them still need to be ported to

00:46:47,599 --> 00:46:51,200
this

00:46:48,319 --> 00:46:51,520
so over time we can expect this to equal

00:46:51,200 --> 00:46:53,760
the

00:46:51,520 --> 00:46:55,119
the results that i just showed and then

00:46:53,760 --> 00:46:57,680
this will be an excellent way

00:46:55,119 --> 00:46:58,160
for if you need to share drives on top

00:46:57,680 --> 00:47:00,079
of this

00:46:58,160 --> 00:47:01,200
the qm storage demon offers a lot of

00:47:00,079 --> 00:47:04,400
other functionality

00:47:01,200 --> 00:47:05,839
some of the cool things are nbd exports

00:47:04,400 --> 00:47:10,400
that would allow you to

00:47:05,839 --> 00:47:13,040
also attach those drives on the host or

00:47:10,400 --> 00:47:15,520
applications can use them and infuse

00:47:13,040 --> 00:47:16,880
exports are also in development

00:47:15,520 --> 00:47:18,560
there the block jobs features are

00:47:16,880 --> 00:47:20,079
available so human storage human will be

00:47:18,560 --> 00:47:20,480
a nice utility and i think we're going

00:47:20,079 --> 00:47:23,599
to see

00:47:20,480 --> 00:47:25,359
more use of it in the future okay

00:47:23,599 --> 00:47:26,480
now if you saw the qe storage demon site

00:47:25,359 --> 00:47:28,240
you might have thought wait a second

00:47:26,480 --> 00:47:29,359
this is a familiar architecture we know

00:47:28,240 --> 00:47:31,440
this approach yes

00:47:29,359 --> 00:47:33,200
it's very similar to spdk storage

00:47:31,440 --> 00:47:34,800
performance development kit

00:47:33,200 --> 00:47:36,240
and that also uses a polling

00:47:34,800 --> 00:47:36,720
architecture and it's been around for

00:47:36,240 --> 00:47:39,760
years

00:47:36,720 --> 00:47:42,880
in fact the v host user block

00:47:39,760 --> 00:47:45,280
interface was created in order to

00:47:42,880 --> 00:47:47,119
connect qmu and spdk

00:47:45,280 --> 00:47:48,640
so we're very thankful that that already

00:47:47,119 --> 00:47:51,119
exists and we can reuse it

00:47:48,640 --> 00:47:51,760
so i wanted to mention spdk because

00:47:51,119 --> 00:47:53,359
obviously

00:47:51,760 --> 00:47:55,440
this has some influence and it's great

00:47:53,359 --> 00:47:56,079
project to check out if you want to find

00:47:55,440 --> 00:47:59,280
out more

00:47:56,079 --> 00:48:02,480
about what's going on in improving

00:47:59,280 --> 00:48:05,520
the general non-nvme case please

00:48:02,480 --> 00:48:06,000
check out stefano gazzarella's talk this

00:48:05,520 --> 00:48:08,720
year

00:48:06,000 --> 00:48:10,559
at kvm forum he'll be going into what

00:48:08,720 --> 00:48:13,599
he's done with iouring and some of the

00:48:10,559 --> 00:48:16,559
new stuff that he's working on

00:48:13,599 --> 00:48:17,760
finally the future direction so in the

00:48:16,559 --> 00:48:20,400
short term

00:48:17,760 --> 00:48:23,599
it's time to get these prototypes into a

00:48:20,400 --> 00:48:25,280
polished state get them upstream

00:48:23,599 --> 00:48:26,880
that will allow us to reach the

00:48:25,280 --> 00:48:28,480
performance that i've shown you here on

00:48:26,880 --> 00:48:31,359
these slides

00:48:28,480 --> 00:48:31,920
and in the longer term i think what's

00:48:31,359 --> 00:48:35,200
clear

00:48:31,920 --> 00:48:35,760
is that pci device assignment because it

00:48:35,200 --> 00:48:39,280
gives us

00:48:35,760 --> 00:48:41,280
bare metal performance it's important

00:48:39,280 --> 00:48:43,040
to find more ways to pass through

00:48:41,280 --> 00:48:44,640
devices because when the hypervisor is

00:48:43,040 --> 00:48:45,520
not involved when there's no software

00:48:44,640 --> 00:48:47,680
path

00:48:45,520 --> 00:48:50,240
that's how we get the best performance

00:48:47,680 --> 00:48:52,559
so summary

00:48:50,240 --> 00:48:54,559
what have we looked at well there's the

00:48:52,559 --> 00:48:55,280
basic configuration and tuning that is

00:48:54,559 --> 00:48:57,839
essential

00:48:55,280 --> 00:48:58,880
the pneuma cpu idle helpful and i o

00:48:57,839 --> 00:49:01,280
thread setup

00:48:58,880 --> 00:49:02,800
that gives you a basic performant

00:49:01,280 --> 00:49:04,559
starting point

00:49:02,800 --> 00:49:06,640
and then you have the big choice do you

00:49:04,559 --> 00:49:08,079
want to use pci device assignment

00:49:06,640 --> 00:49:09,040
because that way you'll have the minimal

00:49:08,079 --> 00:49:11,040
overhead

00:49:09,040 --> 00:49:12,240
that's the best way to go if performance

00:49:11,040 --> 00:49:13,680
is critical

00:49:12,240 --> 00:49:16,400
but you need to keep in mind the

00:49:13,680 --> 00:49:19,520
limitations of that feature

00:49:16,400 --> 00:49:20,800
and if you decide that you can't use pci

00:49:19,520 --> 00:49:23,119
device assignment then you can use

00:49:20,800 --> 00:49:23,599
virtio block with the user space nvme

00:49:23,119 --> 00:49:25,760
driver

00:49:23,599 --> 00:49:27,520
that will boost performance and finally

00:49:25,760 --> 00:49:30,319
the qmu storage daemon

00:49:27,520 --> 00:49:32,000
now allows the sharing of user space

00:49:30,319 --> 00:49:35,440
nvme drives

00:49:32,000 --> 00:49:39,920
with multiple guests

00:49:35,440 --> 00:49:42,559
thank you i also published the

00:49:39,920 --> 00:49:43,359
ansible playbooks that i used to collect

00:49:42,559 --> 00:49:44,640
the data

00:49:43,359 --> 00:49:46,480
if you want to go and look at the

00:49:44,640 --> 00:49:48,480
specifics of the benchmarks

00:49:46,480 --> 00:49:57,599
there's a url in the slide thank you

00:49:48,480 --> 00:49:57,599

YouTube URL: https://www.youtube.com/watch?v=EJa6RatD_yo


