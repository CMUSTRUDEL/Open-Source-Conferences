Title: Live Migration With Hardware Acceleration - Wei Wang, Intel
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Live Migration With Hardware Acceleration - Wei Wang, Intel
Captions: 
	00:00:01,920 --> 00:00:05,600
good afternoon my name is

00:00:03,280 --> 00:00:07,359
weiwang and i'm from intel in this

00:00:05,600 --> 00:00:10,000
presentation i'm going to introduce

00:00:07,359 --> 00:00:10,880
using hardware x emulators to accentuate

00:00:10,000 --> 00:00:13,519
the

00:00:10,880 --> 00:00:15,360
migration of virtual machines those guys

00:00:13,519 --> 00:00:16,720
here are also from intel and they are

00:00:15,360 --> 00:00:19,199
contributors to this

00:00:16,720 --> 00:00:19,199
project

00:00:20,720 --> 00:00:24,960
let's have a look at the agenda of this

00:00:22,960 --> 00:00:27,279
presentation so in total i have

00:00:24,960 --> 00:00:30,160
five parts in the first part i'm going

00:00:27,279 --> 00:00:32,399
to introduce the goals of this project

00:00:30,160 --> 00:00:34,800
and in the second part i will give a

00:00:32,399 --> 00:00:36,800
high level

00:00:34,800 --> 00:00:38,800
give an introduction of the high level

00:00:36,800 --> 00:00:40,800
architecture of this solution

00:00:38,800 --> 00:00:42,640
and in the third part i will introduce

00:00:40,800 --> 00:00:45,200
some important features that are

00:00:42,640 --> 00:00:46,719
used in this solution and in the fourth

00:00:45,200 --> 00:00:49,120
part i will show some test

00:00:46,719 --> 00:00:51,199
results and in the last part i will

00:00:49,120 --> 00:00:53,520
introduce some future works that we plan

00:00:51,199 --> 00:00:57,199
to do

00:00:53,520 --> 00:01:00,559
okay let's start from the project goals

00:00:57,199 --> 00:01:03,120
so um today there are some 10 points

00:01:00,559 --> 00:01:05,199
in now migration so the first one is

00:01:03,120 --> 00:01:05,920
that the virtual machines with memory

00:01:05,199 --> 00:01:07,920
intensive

00:01:05,920 --> 00:01:09,200
light intensive workloads are difficult

00:01:07,920 --> 00:01:11,760
to migrate

00:01:09,200 --> 00:01:12,960
so this is because the guest rights to

00:01:11,760 --> 00:01:15,040
the memory they dirty

00:01:12,960 --> 00:01:17,119
more pages than the pages that can be

00:01:15,040 --> 00:01:19,520
transferred to migration

00:01:17,119 --> 00:01:20,320
so the second one is vms with large

00:01:19,520 --> 00:01:23,119
memory size

00:01:20,320 --> 00:01:24,640
usually takes long time to migrate the

00:01:23,119 --> 00:01:27,360
third one is

00:01:24,640 --> 00:01:29,600
the vm migration may consume large

00:01:27,360 --> 00:01:32,159
network bandwidth

00:01:29,600 --> 00:01:33,520
so there are some existing solutions in

00:01:32,159 --> 00:01:36,960
the current

00:01:33,520 --> 00:01:39,680
queueing for example

00:01:36,960 --> 00:01:41,200
people may choose to use the cpus due to

00:01:39,680 --> 00:01:43,040
compression like a

00:01:41,200 --> 00:01:45,360
multi-threaded compression with the

00:01:43,040 --> 00:01:46,399
z-lip and the problem is that it's a

00:01:45,360 --> 00:01:49,840
snow based on

00:01:46,399 --> 00:01:53,040
our experiments and the second issue is

00:01:49,840 --> 00:01:56,399
it consumes too many cpus from the host

00:01:53,040 --> 00:01:59,759
so this is not expected from the

00:01:56,399 --> 00:02:01,920
cloud cloud vendors so

00:01:59,759 --> 00:02:04,399
our solution is to offload the

00:02:01,920 --> 00:02:07,680
compression part to interactivity

00:02:04,399 --> 00:02:09,280
quicker assistance in technology with

00:02:07,680 --> 00:02:11,920
efficient approaches

00:02:09,280 --> 00:02:12,959
so by efficient i mean higher migration

00:02:11,920 --> 00:02:15,599
throughput

00:02:12,959 --> 00:02:17,520
so this is measured by how many pages we

00:02:15,599 --> 00:02:20,720
can transfer the

00:02:17,520 --> 00:02:23,360
phone source to destination 3 migration

00:02:20,720 --> 00:02:23,920
so the higher the better and the second

00:02:23,360 --> 00:02:25,920
one is

00:02:23,920 --> 00:02:27,200
lower secure butanization so we don't

00:02:25,920 --> 00:02:30,480
want to waste

00:02:27,200 --> 00:02:32,560
more cpus on the horse so the

00:02:30,480 --> 00:02:34,000
second goal we have here is to have a

00:02:32,560 --> 00:02:37,040
comedy line ready for

00:02:34,000 --> 00:02:39,360
future more accelerators to draw in so

00:02:37,040 --> 00:02:40,720
on the upcoming interest of rapid cpus

00:02:39,360 --> 00:02:43,440
we will have

00:02:40,720 --> 00:02:44,000
data streaming accelerator short for dlc

00:02:43,440 --> 00:02:46,959
and

00:02:44,000 --> 00:02:47,599
intel analytics accelerator short for

00:02:46,959 --> 00:02:50,000
iago

00:02:47,599 --> 00:02:51,920
iex so we have these two more

00:02:50,000 --> 00:02:54,800
accelerators to be integrated

00:02:51,920 --> 00:02:56,800
into the cpu chip and we want to take

00:02:54,800 --> 00:02:58,800
advantage of them to

00:02:56,800 --> 00:03:00,720
process the guest memory during

00:02:58,800 --> 00:03:03,040
migration as well

00:03:00,720 --> 00:03:04,159
and also we wanted to have a smarter

00:03:03,040 --> 00:03:07,040
selection technique

00:03:04,159 --> 00:03:09,360
which is smartly and dynamically select

00:03:07,040 --> 00:03:12,080
an appropriate examinator

00:03:09,360 --> 00:03:13,280
to accent return on migration so i will

00:03:12,080 --> 00:03:18,640
introduce this

00:03:13,280 --> 00:03:21,680
a little bit more in later slides

00:03:18,640 --> 00:03:23,120
let's have a look at the architecture of

00:03:21,680 --> 00:03:25,599
this solution

00:03:23,120 --> 00:03:27,360
so on the source machine so on the

00:03:25,599 --> 00:03:30,159
bottom here you can see that

00:03:27,360 --> 00:03:31,360
we could have multiple accelerators here

00:03:30,159 --> 00:03:34,159
and

00:03:31,360 --> 00:03:34,640
the each of them have their own software

00:03:34,159 --> 00:03:40,080
stack

00:03:34,640 --> 00:03:42,640
like the qt library and the driver

00:03:40,080 --> 00:03:43,599
and on the on the top there are two

00:03:42,640 --> 00:03:46,720
threads here

00:03:43,599 --> 00:03:50,159
so the migration migration threat is

00:03:46,720 --> 00:03:52,720
the one that's already existing

00:03:50,159 --> 00:03:53,599
so in the migration thread there i

00:03:52,720 --> 00:03:56,959
conclude

00:03:53,599 --> 00:04:00,000
it into four steps the first step is the

00:03:56,959 --> 00:04:02,400
migration setup so basically it will do

00:04:00,000 --> 00:04:04,239
some preparation for migration including

00:04:02,400 --> 00:04:07,040
the accelerator device in this

00:04:04,239 --> 00:04:08,640
initialization and the device and the

00:04:07,040 --> 00:04:12,480
device supporting throughout the

00:04:08,640 --> 00:04:13,680
creation so in the second step of the

00:04:12,480 --> 00:04:16,000
microphone

00:04:13,680 --> 00:04:16,880
the migrations that will do the page

00:04:16,000 --> 00:04:19,919
searching so it

00:04:16,880 --> 00:04:20,400
searches for 30 pages to process and

00:04:19,919 --> 00:04:23,440
just

00:04:20,400 --> 00:04:24,479
send it to the destination so in the

00:04:23,440 --> 00:04:27,520
third part you know

00:04:24,479 --> 00:04:30,320
in the third step we will do a

00:04:27,520 --> 00:04:31,440
a smarter selection so the migration

00:04:30,320 --> 00:04:34,479
thread will select

00:04:31,440 --> 00:04:38,320
an appropriate accelerator based on the

00:04:34,479 --> 00:04:40,960
history of the acceleration efficiency

00:04:38,320 --> 00:04:42,000
so once it decides that for example

00:04:40,960 --> 00:04:43,680
using acuity is

00:04:42,000 --> 00:04:46,080
more efficient so it will choose

00:04:43,680 --> 00:04:47,280
security to process memory for the

00:04:46,080 --> 00:04:50,880
following

00:04:47,280 --> 00:04:54,320
pages and the the fourth

00:04:50,880 --> 00:04:57,040
step is to dispatch requests so once the

00:04:54,320 --> 00:04:58,560
accelerator is selected it will compose

00:04:57,040 --> 00:05:01,360
a request

00:04:58,560 --> 00:05:03,199
data structure and submit data to the

00:05:01,360 --> 00:05:06,240
device to due process

00:05:03,199 --> 00:05:06,720
for example if it's using qet and there

00:05:06,240 --> 00:05:09,759
may

00:05:06,720 --> 00:05:10,000
have multiple qt compression engines so

00:05:09,759 --> 00:05:13,280
it

00:05:10,000 --> 00:05:15,840
may deliver dispatch the requests to the

00:05:13,280 --> 00:05:18,240
those compression engines in one roping

00:05:15,840 --> 00:05:18,240
fashion

00:05:18,960 --> 00:05:23,520
and for the device including that its

00:05:21,039 --> 00:05:24,479
main job is to pull for responses from

00:05:23,520 --> 00:05:27,039
the device

00:05:24,479 --> 00:05:28,400
so the response here means that the

00:05:27,039 --> 00:05:31,680
previously submitted

00:05:28,400 --> 00:05:32,560
request has been processed and once the

00:05:31,680 --> 00:05:35,600
response

00:05:32,560 --> 00:05:38,800
is is got is obtained by the

00:05:35,600 --> 00:05:41,680
polling site it will release the the

00:05:38,800 --> 00:05:44,639
request data structure

00:05:41,680 --> 00:05:46,960
and it blocks when there is no response

00:05:44,639 --> 00:05:46,960
ready

00:05:47,520 --> 00:05:53,280
so the second thing the polling slot

00:05:50,639 --> 00:05:54,320
doing here is to send the compressed

00:05:53,280 --> 00:05:56,319
data

00:05:54,320 --> 00:05:59,120
along with the related header to the

00:05:56,319 --> 00:06:01,360
destination through the network

00:05:59,120 --> 00:06:03,199
so in this model we basically have a

00:06:01,360 --> 00:06:05,120
split of the

00:06:03,199 --> 00:06:07,360
migration flow so in the current

00:06:05,120 --> 00:06:10,000
migration flow

00:06:07,360 --> 00:06:11,600
the migratory thread is responsible for

00:06:10,000 --> 00:06:14,639
searching for 30 pages

00:06:11,600 --> 00:06:18,880
and then send it to the to the network

00:06:14,639 --> 00:06:23,280
so in this model we the data transfer

00:06:18,880 --> 00:06:25,759
is is given to the polling thread that

00:06:23,280 --> 00:06:25,759
you send

00:06:26,479 --> 00:06:30,000
so here is the picture on the

00:06:28,000 --> 00:06:31,840
destination side it's

00:06:30,000 --> 00:06:33,199
similar to the one that we saw on the

00:06:31,840 --> 00:06:35,600
south side

00:06:33,199 --> 00:06:36,639
so on the bottom here you also have

00:06:35,600 --> 00:06:39,759
those

00:06:36,639 --> 00:06:40,800
accent litters and on the top it has the

00:06:39,759 --> 00:06:44,720
two

00:06:40,800 --> 00:06:47,199
threads as before and

00:06:44,720 --> 00:06:48,000
the migration setup is similar as the

00:06:47,199 --> 00:06:49,919
source side so

00:06:48,000 --> 00:06:51,919
it will do some initialization work so

00:06:49,919 --> 00:06:54,240
the difference here is

00:06:51,919 --> 00:06:55,759
it has a page receiving step so this

00:06:54,240 --> 00:06:58,960
step is

00:06:55,759 --> 00:07:01,919
it receives the data from the network

00:06:58,960 --> 00:07:03,599
and then the migration sliding slide

00:07:01,919 --> 00:07:05,680
passes the

00:07:03,599 --> 00:07:08,479
the migration protocol for example we

00:07:05,680 --> 00:07:11,039
added a multi-page protocol so there is

00:07:08,479 --> 00:07:14,479
a multi-agent header

00:07:11,039 --> 00:07:19,039
and also if we are select an accelerator

00:07:14,479 --> 00:07:21,919
to to process the memory so

00:07:19,039 --> 00:07:22,560
the the data the the header like the

00:07:21,919 --> 00:07:24,800
header

00:07:22,560 --> 00:07:25,759
tells the migration thread on the

00:07:24,800 --> 00:07:27,680
desktop side

00:07:25,759 --> 00:07:29,120
that which accelerator to use for

00:07:27,680 --> 00:07:31,520
example if the source side

00:07:29,120 --> 00:07:33,039
use the activity to do compression then

00:07:31,520 --> 00:07:36,880
on the destination side they need to be

00:07:33,039 --> 00:07:40,800
a selected acuity to do decompression

00:07:36,880 --> 00:07:43,919
so for the device supporting thread and

00:07:40,800 --> 00:07:45,120
on the destiny side its job is simple so

00:07:43,919 --> 00:07:48,240
it adjusts it's

00:07:45,120 --> 00:07:51,520
just a pose for responses

00:07:48,240 --> 00:07:52,720
so once a response is obtained then it

00:07:51,520 --> 00:07:59,440
will

00:07:52,720 --> 00:08:01,840
release the request data structure

00:07:59,440 --> 00:08:04,160
it also blocks when there is no response

00:08:01,840 --> 00:08:04,160
ready

00:08:04,639 --> 00:08:11,440
and the for the device the decompress

00:08:09,039 --> 00:08:13,840
the data idea made to the premium memory

00:08:11,440 --> 00:08:13,840
directly

00:08:14,479 --> 00:08:20,639
okay let's go to the the third part

00:08:17,759 --> 00:08:21,280
um so i will introduce some features

00:08:20,639 --> 00:08:24,639
that

00:08:21,280 --> 00:08:27,680
i used in this solution so

00:08:24,639 --> 00:08:30,240
the first feature we use is zero copy so

00:08:27,680 --> 00:08:33,519
it allows with this feature it allows

00:08:30,240 --> 00:08:37,200
the accelerator device to direct me

00:08:33,519 --> 00:08:37,919
access to the guest memory so the second

00:08:37,200 --> 00:08:41,360
one we use

00:08:37,919 --> 00:08:41,360
is the multi-page process

00:08:41,760 --> 00:08:44,959
so the current migration flow only

00:08:43,760 --> 00:08:47,200
supports the

00:08:44,959 --> 00:08:49,680
single page processing meaning that the

00:08:47,200 --> 00:08:52,480
ether finds the only one

00:08:49,680 --> 00:08:52,720
dirty page and then compress this page

00:08:52,480 --> 00:08:55,600
and

00:08:52,720 --> 00:08:56,640
then send it to the migration to the

00:08:55,600 --> 00:08:58,959
network

00:08:56,640 --> 00:09:00,399
so with this feature the microsoft

00:08:58,959 --> 00:09:03,519
follow is able to

00:09:00,399 --> 00:09:05,440
process multiple pages each time and the

00:09:03,519 --> 00:09:06,320
third feature is the accenture and

00:09:05,440 --> 00:09:09,760
request of

00:09:06,320 --> 00:09:13,200
caching so this feature just caches the

00:09:09,760 --> 00:09:16,240
acceleration request the data structure

00:09:13,200 --> 00:09:18,800
for efficient memory allocation so i

00:09:16,240 --> 00:09:22,000
will introduce more details about

00:09:18,800 --> 00:09:24,959
this those three features so

00:09:22,000 --> 00:09:26,080
for zero copy so at the migration setup

00:09:24,959 --> 00:09:28,399
step

00:09:26,080 --> 00:09:30,880
uh the migration strategy is to

00:09:28,399 --> 00:09:32,399
pre-allocate and the pin

00:09:30,880 --> 00:09:34,000
the queuing memory is to be

00:09:32,399 --> 00:09:37,279
pre-allocated and

00:09:34,000 --> 00:09:40,480
pinned so this is uh to prevent

00:09:37,279 --> 00:09:42,880
the memory to be swept out during

00:09:40,480 --> 00:09:46,640
migration

00:09:42,880 --> 00:09:49,680
and so on the destination side

00:09:46,640 --> 00:09:51,920
the memory will be amping the random is

00:09:49,680 --> 00:09:51,920
done

00:09:54,560 --> 00:09:59,040
but this is not indeed the in the future

00:09:56,800 --> 00:10:02,560
when we have vfi obviously

00:09:59,040 --> 00:10:05,680
this is the driver so i will

00:10:02,560 --> 00:10:09,519
introduce in our future work

00:10:05,680 --> 00:10:12,720
so for the request composing

00:10:09,519 --> 00:10:14,399
uh on the source side the dma will set

00:10:12,720 --> 00:10:18,000
up a dma buffer for the

00:10:14,399 --> 00:10:19,120
for the source device to to process the

00:10:18,000 --> 00:10:22,000
memory

00:10:19,120 --> 00:10:24,320
so the the dna read the buffer uh it

00:10:22,000 --> 00:10:27,440
points to the premium memory so like uh

00:10:24,320 --> 00:10:29,600
the the creative device can directly you

00:10:27,440 --> 00:10:32,399
dma reader from the cumulative memory

00:10:29,600 --> 00:10:34,240
and then compress compress the guest

00:10:32,399 --> 00:10:36,480
data

00:10:34,240 --> 00:10:37,920
and for the dma right buffer it's

00:10:36,480 --> 00:10:40,480
allocated by the

00:10:37,920 --> 00:10:41,519
library for example the qet library so

00:10:40,480 --> 00:10:43,760
when the

00:10:41,519 --> 00:10:45,200
the compressed when the compression is

00:10:43,760 --> 00:10:48,000
ready it is done

00:10:45,200 --> 00:10:48,800
the data idea made the idea made right

00:10:48,000 --> 00:10:52,320
to the

00:10:48,800 --> 00:10:55,440
to the to the to the buffer

00:10:52,320 --> 00:10:56,720
and on the destination side the dma grid

00:10:55,440 --> 00:10:59,120
buffer is allocated

00:10:56,720 --> 00:11:01,120
by the by the library so there is a

00:10:59,120 --> 00:11:03,600
piece of buffer

00:11:01,120 --> 00:11:07,360
and the dma right buffer points to the

00:11:03,600 --> 00:11:10,399
extremely memory so the device

00:11:07,360 --> 00:11:12,880
reads the compress the data from this

00:11:10,399 --> 00:11:14,640
library buffer and then do decompression

00:11:12,880 --> 00:11:15,920
so once the compression is done the

00:11:14,640 --> 00:11:18,160
device

00:11:15,920 --> 00:11:19,360
do dm right to the cream in the memory

00:11:18,160 --> 00:11:23,200
directly

00:11:19,360 --> 00:11:25,760
so this achieves the zero recording

00:11:23,200 --> 00:11:27,440
so for the multi-page processing so here

00:11:25,760 --> 00:11:31,120
is an example that we have

00:11:27,440 --> 00:11:34,320
12 pages so the migration thread will

00:11:31,120 --> 00:11:36,000
find the multiple dirty pages one time

00:11:34,320 --> 00:11:38,160
for example here it defines that the

00:11:36,000 --> 00:11:42,399
page 0 1 is 30

00:11:38,160 --> 00:11:45,120
and it it composes a data structure that

00:11:42,399 --> 00:11:46,000
the data page starts from zero and size

00:11:45,120 --> 00:11:49,279
two meaning that

00:11:46,000 --> 00:11:52,240
there are two pages consecutive pages

00:11:49,279 --> 00:11:54,480
and the second group of dirty pages

00:11:52,240 --> 00:11:56,800
starts from page three and it has the

00:11:54,480 --> 00:11:59,760
four pages and the third group

00:11:56,800 --> 00:12:00,240
starts from page 8 and page 9 and it has

00:11:59,760 --> 00:12:03,600
three

00:12:00,240 --> 00:12:05,519
30 pages and then

00:12:03,600 --> 00:12:07,120
the migration thread do the request

00:12:05,519 --> 00:12:11,440
composing conclusion

00:12:07,120 --> 00:12:14,639
and it compose the request

00:12:11,440 --> 00:12:17,040
which is submittable to the device and

00:12:14,639 --> 00:12:19,760
it also needs to set up the dma

00:12:17,040 --> 00:12:21,040
the dma dma buffer so it's a scale

00:12:19,760 --> 00:12:24,160
together buffer

00:12:21,040 --> 00:12:25,200
so like here it has live buffers so the

00:12:24,160 --> 00:12:28,079
dna

00:12:25,200 --> 00:12:29,440
the dna buffer points to these nine

00:12:28,079 --> 00:12:32,480
pages

00:12:29,440 --> 00:12:36,000
and it's joined together

00:12:32,480 --> 00:12:37,680
and the the orange box here is the

00:12:36,000 --> 00:12:41,440
buffer allocated by the

00:12:37,680 --> 00:12:44,480
biosecurity library so the qut device

00:12:41,440 --> 00:12:48,480
factory is data from the

00:12:44,480 --> 00:12:51,519
memory and then you do compression so

00:12:48,480 --> 00:12:54,639
once the compression is done the data

00:12:51,519 --> 00:12:56,720
those night pages and

00:12:54,639 --> 00:12:57,680
the compression of these large pages uh

00:12:56,720 --> 00:13:00,480
the result is

00:12:57,680 --> 00:13:02,720
written into the orange box which is

00:13:00,480 --> 00:13:04,880
allocated by the create library

00:13:02,720 --> 00:13:06,079
and then the device opponents that will

00:13:04,880 --> 00:13:08,399
transfer those

00:13:06,079 --> 00:13:09,839
compressed data as a whole to the

00:13:08,399 --> 00:13:12,240
desktop side

00:13:09,839 --> 00:13:14,240
and for the data either each one is

00:13:12,240 --> 00:13:14,560
associated with the multi-page header

00:13:14,240 --> 00:13:17,839
which

00:13:14,560 --> 00:13:21,839
pairs the address of the

00:13:17,839 --> 00:13:21,839
address of the premium page

00:13:22,560 --> 00:13:26,639
like on the destination side when the

00:13:24,480 --> 00:13:29,440
data is received here

00:13:26,639 --> 00:13:30,560
and so it it knows that there is

00:13:29,440 --> 00:13:33,760
multi-page header

00:13:30,560 --> 00:13:36,800
and and the payload is the

00:13:33,760 --> 00:13:38,560
compress the data and then either the

00:13:36,800 --> 00:13:40,800
migration thread on the destination side

00:13:38,560 --> 00:13:45,040
will compose the request

00:13:40,800 --> 00:13:48,240
and it defines uh for the dma buffer

00:13:45,040 --> 00:13:51,360
the destination side address is

00:13:48,240 --> 00:13:52,720
is a uncalculated by the multi-page

00:13:51,360 --> 00:13:55,040
header which pairs

00:13:52,720 --> 00:13:56,160
where is the communication that should

00:13:55,040 --> 00:13:59,040
hold those

00:13:56,160 --> 00:14:01,360
decompress the data so the device reads

00:13:59,040 --> 00:14:04,079
data from the compressor data and

00:14:01,360 --> 00:14:05,199
decompression and then write those data

00:14:04,079 --> 00:14:07,519
to the

00:14:05,199 --> 00:14:10,320
to the to the destination side the

00:14:07,519 --> 00:14:10,320
premium memory

00:14:11,920 --> 00:14:15,199
so for examination request attraction

00:14:14,880 --> 00:14:17,680
this

00:14:15,199 --> 00:14:18,320
is a common technique so during the

00:14:17,680 --> 00:14:22,560
device

00:14:18,320 --> 00:14:24,480
setup stage the migration thread

00:14:22,560 --> 00:14:26,399
pre-allocates some amount of

00:14:24,480 --> 00:14:28,560
acceleration requests

00:14:26,399 --> 00:14:30,160
data structure and the fuse them into

00:14:28,560 --> 00:14:32,720
the cache pool

00:14:30,160 --> 00:14:33,839
and during the request composing stage

00:14:32,720 --> 00:14:36,880
the

00:14:33,839 --> 00:14:39,440
the migration thread will allocate the

00:14:36,880 --> 00:14:42,720
request

00:14:39,440 --> 00:14:44,800
so instead of doing the malloc either

00:14:42,720 --> 00:14:46,480
directly take a request from the cash

00:14:44,800 --> 00:14:49,680
pool once the

00:14:46,480 --> 00:14:51,920
requests that are used up from the cash

00:14:49,680 --> 00:14:54,639
code it will do open up

00:14:51,920 --> 00:14:56,160
so and then it initialize the request

00:14:54,639 --> 00:14:59,760
based on the new pages

00:14:56,160 --> 00:15:02,480
with them so for the response

00:14:59,760 --> 00:15:03,519
pulling that well when a response is

00:15:02,480 --> 00:15:06,320
obtained

00:15:03,519 --> 00:15:07,440
and the threat frees the request to the

00:15:06,320 --> 00:15:10,839
cash pool

00:15:07,440 --> 00:15:13,040
so instead of doing the coin the free

00:15:10,839 --> 00:15:15,199
system

00:15:13,040 --> 00:15:16,240
okay let's have a look at the test

00:15:15,199 --> 00:15:19,120
results

00:15:16,240 --> 00:15:20,160
so for the tests we tested on the inter

00:15:19,120 --> 00:15:23,800
xen

00:15:20,160 --> 00:15:26,639
cpu e5 26 99 before running at

00:15:23,800 --> 00:15:27,440
2.2 gigahertz and this is the broadway

00:15:26,639 --> 00:15:31,440
cpu

00:15:27,440 --> 00:15:32,160
and for qt we use the pcie gen3qt card

00:15:31,440 --> 00:15:35,519
so it's

00:15:32,160 --> 00:15:38,560
a pcie card plugged into the

00:15:35,519 --> 00:15:41,440
pcie slot but in all

00:15:38,560 --> 00:15:42,560
separate cpus we will have qt integrated

00:15:41,440 --> 00:15:46,480
into cpu and

00:15:42,560 --> 00:15:49,519
it uses pcie gen4 so the the speed will

00:15:46,480 --> 00:15:51,199
be much faster on the upcoming in terms

00:15:49,519 --> 00:15:55,120
of rocket cpu

00:15:51,199 --> 00:15:59,360
so for the dram and we use the ddr4

00:15:55,120 --> 00:15:59,360
and the running at uh

00:16:01,079 --> 00:16:05,440
00:16:02,399 --> 00:16:06,079
megahertz and for the network card we

00:16:05,440 --> 00:16:09,040
use the

00:16:06,079 --> 00:16:10,160
40 giga network card so for the

00:16:09,040 --> 00:16:13,680
migration setup

00:16:10,160 --> 00:16:16,320
the downtime we use is 300 microseconds

00:16:13,680 --> 00:16:17,120
so it's the default uh downtime using

00:16:16,320 --> 00:16:19,199
qmu

00:16:17,120 --> 00:16:21,120
and for the level of bandwidth we didn't

00:16:19,199 --> 00:16:24,240
set at a limit so it can use

00:16:21,120 --> 00:16:24,639
up to 40 gigahertz but in reality it

00:16:24,240 --> 00:16:27,920
won't

00:16:24,639 --> 00:16:30,000
consume those much boundaries

00:16:27,920 --> 00:16:30,959
for the compressed level we set it to

00:16:30,000 --> 00:16:34,399
one so this

00:16:30,959 --> 00:16:36,240
is the fastest speed to compress

00:16:34,399 --> 00:16:37,680
for the multi-page we added a new

00:16:36,240 --> 00:16:40,560
parameter called multipage

00:16:37,680 --> 00:16:42,320
so meaning that the migration so that

00:16:40,560 --> 00:16:45,600
they can

00:16:42,320 --> 00:16:47,839
can can process multiple pages each time

00:16:45,600 --> 00:16:50,639
so we set the value to 63.

00:16:47,839 --> 00:16:51,279
this is the maximum value can be

00:16:50,639 --> 00:16:53,920
supported

00:16:51,279 --> 00:16:55,199
currently so for the guests we have

00:16:53,920 --> 00:16:58,639
three types of gas

00:16:55,199 --> 00:17:02,320
to test the first type in of gaster has

00:16:58,639 --> 00:17:03,360
the four vcpus and 32gb ram and it runs

00:17:02,320 --> 00:17:05,199
the workload

00:17:03,360 --> 00:17:07,039
with the writing compression friendly

00:17:05,199 --> 00:17:10,240
data and the second

00:17:07,039 --> 00:17:13,520
type of guest has the four vcpus and

00:17:10,240 --> 00:17:15,600
it has a 32gb ram but it runs the

00:17:13,520 --> 00:17:17,839
workload which provides

00:17:15,600 --> 00:17:18,799
sequence numbers to memory so sequence

00:17:17,839 --> 00:17:22,240
number is not

00:17:18,799 --> 00:17:24,240
a very compression friendly but it is

00:17:22,240 --> 00:17:25,839
still locative components not difficult

00:17:24,240 --> 00:17:28,880
also

00:17:25,839 --> 00:17:32,000
so the for the third type of gas it has

00:17:28,880 --> 00:17:35,200
the eight of eight of the cpus and

00:17:32,000 --> 00:17:38,000
uh 128 gig ram and

00:17:35,200 --> 00:17:38,720
either runs a memory cache workload with

00:17:38,000 --> 00:17:41,280
the right

00:17:38,720 --> 00:17:42,160
reading and writing random numbers so

00:17:41,280 --> 00:17:45,679
random numbers

00:17:42,160 --> 00:17:50,799
are relatively difficult to compress

00:17:45,679 --> 00:17:53,280
we will see the conversion ratio later

00:17:50,799 --> 00:17:54,320
so let's have a look at the first uh

00:17:53,280 --> 00:17:56,480
test

00:17:54,320 --> 00:17:57,760
so we want this vertical workload inside

00:17:56,480 --> 00:18:01,360
of the guest which

00:17:57,760 --> 00:18:01,919
writes data to the to the guest the

00:18:01,360 --> 00:18:05,360
memory

00:18:01,919 --> 00:18:06,160
in a specified dirty grid and like here

00:18:05,360 --> 00:18:09,280
we can set it

00:18:06,160 --> 00:18:14,720
to write uh 100 megabytes

00:18:09,280 --> 00:18:16,559
per second and for the data so

00:18:14,720 --> 00:18:18,080
there is something probably you need to

00:18:16,559 --> 00:18:20,320
understand first so for the

00:18:18,080 --> 00:18:21,760
stroke it's the migration throughput and

00:18:20,320 --> 00:18:24,000
it's made by

00:18:21,760 --> 00:18:26,000
how many pages we are transferred from

00:18:24,000 --> 00:18:27,120
the source to destination so the higher

00:18:26,000 --> 00:18:29,760
the better

00:18:27,120 --> 00:18:31,039
and to make the numbers simple so i put

00:18:29,760 --> 00:18:34,720
the multiplier

00:18:31,039 --> 00:18:35,679
here so for the num compression case it

00:18:34,720 --> 00:18:39,840
can

00:18:35,679 --> 00:18:44,000
send like a um 17 multiplier

00:18:39,840 --> 00:18:47,760
ten thousands pages per second to 29

00:18:44,000 --> 00:18:50,799
multiply this number so for the 16

00:18:47,760 --> 00:18:54,000
cpu case so it's a little bit

00:18:50,799 --> 00:18:56,960
it's almost two times larger

00:18:54,000 --> 00:18:58,320
more than two times nine for the qet so

00:18:56,960 --> 00:19:00,480
it's uh

00:18:58,320 --> 00:19:01,440
around the five times larger so we can

00:19:00,480 --> 00:19:04,400
see the

00:19:01,440 --> 00:19:06,160
normalized throughput here so the

00:19:04,400 --> 00:19:08,799
queueing case has

00:19:06,160 --> 00:19:11,360
much larger migration slope than the

00:19:08,799 --> 00:19:13,919
lung compression case

00:19:11,360 --> 00:19:14,559
so for the largest migrateable data

00:19:13,919 --> 00:19:17,280
reader

00:19:14,559 --> 00:19:17,760
it means that we want we turn this

00:19:17,280 --> 00:19:20,720
number

00:19:17,760 --> 00:19:21,039
this dirty rate inside of the gas and we

00:19:20,720 --> 00:19:24,160
get

00:19:21,039 --> 00:19:28,160
that uh if we turn the number to

00:19:24,160 --> 00:19:30,559
like 1200 then this vm cannot be

00:19:28,160 --> 00:19:31,919
migrated with uh with without the

00:19:30,559 --> 00:19:36,799
compression

00:19:31,919 --> 00:19:39,840
so this one um 130 read

00:19:36,799 --> 00:19:40,640
inside of the gas is the largest w lead

00:19:39,840 --> 00:19:44,320
that the guest

00:19:40,640 --> 00:19:47,600
can have to ensure it can be migrated

00:19:44,320 --> 00:19:50,000
so for the 16 cpu case it supports this

00:19:47,600 --> 00:19:53,760
number and for the 3d case it's much

00:19:50,000 --> 00:19:55,760
larger than the low compression piece

00:19:53,760 --> 00:19:58,080
and for the extra cpu you know as usual

00:19:55,760 --> 00:20:00,880
it means how many cpus are used

00:19:58,080 --> 00:20:02,480
in addition to the migration slot so for

00:20:00,880 --> 00:20:06,159
the long compression case

00:20:02,480 --> 00:20:09,600
there is no extra thread so it doesn't

00:20:06,159 --> 00:20:11,640
consume extra cpu so for the 16 cpu case

00:20:09,600 --> 00:20:13,520
it consumes

00:20:11,640 --> 00:20:16,880
00:20:13,520 --> 00:20:19,760
cpu percentage cpu it doesn't consume

00:20:16,880 --> 00:20:21,120
all the 16 cpus because this data

00:20:19,760 --> 00:20:24,320
pattern is easy to

00:20:21,120 --> 00:20:25,200
compress so for the qt case it's less

00:20:24,320 --> 00:20:28,559
than 40

00:20:25,200 --> 00:20:30,960
each so it's much less than the

00:20:28,559 --> 00:20:32,080
cpu compression case so for the

00:20:30,960 --> 00:20:35,520
compaction visual

00:20:32,080 --> 00:20:37,679
you can see that uh the qt compaction

00:20:35,520 --> 00:20:40,159
has a higher ratio than the

00:20:37,679 --> 00:20:41,760
the cpu zdb compression so the

00:20:40,159 --> 00:20:44,880
compression algorithm we use

00:20:41,760 --> 00:20:46,880
here is similar its same is the same but

00:20:44,880 --> 00:20:48,240
the comparation is different this is

00:20:46,880 --> 00:20:51,120
because the multi-page

00:20:48,240 --> 00:20:52,320
comparison so when we have multiple

00:20:51,120 --> 00:20:55,039
pages because the

00:20:52,320 --> 00:20:55,919
for this workload it writes the all ones

00:20:55,039 --> 00:20:59,679
through

00:20:55,919 --> 00:21:03,120
to to the memory so this repeated

00:20:59,679 --> 00:21:06,000
one can be can generate

00:21:03,120 --> 00:21:07,440
only for example one or single one as

00:21:06,000 --> 00:21:10,799
the compression payload

00:21:07,440 --> 00:21:13,600
so in this case multiple pages can have

00:21:10,799 --> 00:21:14,080
the higher compaction ratio if we don't

00:21:13,600 --> 00:21:16,159
have this

00:21:14,080 --> 00:21:18,159
repeated numbers the compression ratio

00:21:16,159 --> 00:21:19,039
between the 2d and the cpu compression

00:21:18,159 --> 00:21:22,159
will be similar

00:21:19,039 --> 00:21:22,480
you can see from the next slide here for

00:21:22,159 --> 00:21:24,480
this

00:21:22,480 --> 00:21:26,960
sequence number so it's not repeated

00:21:24,480 --> 00:21:31,120
it's just a number from 0

00:21:26,960 --> 00:21:33,200
1 2 3 so

00:21:31,120 --> 00:21:34,320
so we run this workload inside of the

00:21:33,200 --> 00:21:36,240
gas and uh

00:21:34,320 --> 00:21:38,640
for the migration of the locator we can

00:21:36,240 --> 00:21:38,640
see that

00:21:38,720 --> 00:21:42,960
the qd case is here much larger than the

00:21:41,760 --> 00:21:45,600
conversion piece

00:21:42,960 --> 00:21:46,000
so interesting here is that we find the

00:21:45,600 --> 00:21:48,480
with

00:21:46,000 --> 00:21:49,120
16 cpu compression the migration

00:21:48,480 --> 00:21:51,760
throughput

00:21:49,120 --> 00:21:53,120
is even lower than the low compaction

00:21:51,760 --> 00:21:56,240
piece this is because

00:21:53,120 --> 00:21:58,880
the compression isn't efficient with cpu

00:21:56,240 --> 00:22:02,880
compression so

00:21:58,880 --> 00:22:05,200
it actually de-accelerated the migration

00:22:02,880 --> 00:22:06,400
so for the lattice the migrator for data

00:22:05,200 --> 00:22:08,559
read so

00:22:06,400 --> 00:22:10,159
for the qd case it's still larger than

00:22:08,559 --> 00:22:12,559
the low compression case

00:22:10,159 --> 00:22:14,400
and as for the cpu utilization the cpu

00:22:12,559 --> 00:22:17,520
compression keys

00:22:14,400 --> 00:22:18,400
consumes all the 16 cpu and for the qt

00:22:17,520 --> 00:22:21,520
conversion case

00:22:18,400 --> 00:22:23,600
it's less than 70 each so the compaction

00:22:21,520 --> 00:22:26,240
ratio between the cpu and the

00:22:23,600 --> 00:22:27,440
liquidity is a signal here because they

00:22:26,240 --> 00:22:30,960
use the same

00:22:27,440 --> 00:22:33,360
compression algorithm so for the third

00:22:30,960 --> 00:22:35,440
test we use the

00:22:33,360 --> 00:22:36,880
we set up the memory cache environment

00:22:35,440 --> 00:22:40,000
using this

00:22:36,880 --> 00:22:42,000
so basically it's a client memory

00:22:40,000 --> 00:22:44,159
cache decliner called the mem step kind

00:22:42,000 --> 00:22:47,200
and it divides random numbers to these

00:22:44,159 --> 00:22:50,240
memory cache memory pages

00:22:47,200 --> 00:22:50,799
so the random number is much more

00:22:50,240 --> 00:22:52,880
difficult

00:22:50,799 --> 00:22:54,480
to compress than the previous data

00:22:52,880 --> 00:22:57,120
pattern we can see from the compression

00:22:54,480 --> 00:22:59,760
video here it's only 1.6

00:22:57,120 --> 00:23:01,039
but the 3d comparison cases here has

00:22:59,760 --> 00:23:04,559
advantage over

00:23:01,039 --> 00:23:07,679
those two cases so

00:23:04,559 --> 00:23:10,640
it's like a more than two times faster

00:23:07,679 --> 00:23:12,720
for the migration circuit

00:23:10,640 --> 00:23:13,840
and the migration time here infinite

00:23:12,720 --> 00:23:16,880
means that uh

00:23:13,840 --> 00:23:18,159
the mic the vm cannot be migrated like

00:23:16,880 --> 00:23:20,880
in the

00:23:18,159 --> 00:23:22,080
in the low conversion case so or the cpu

00:23:20,880 --> 00:23:24,799
comparison case

00:23:22,080 --> 00:23:26,320
the vm cannot be migrated but with

00:23:24,799 --> 00:23:29,440
security case it takes

00:23:26,320 --> 00:23:32,400
around 60 seconds to successfully

00:23:29,440 --> 00:23:32,400
migrate the vm

00:23:34,159 --> 00:23:38,159
ok let's have a look at some future

00:23:36,400 --> 00:23:41,440
works that we plan to do

00:23:38,159 --> 00:23:44,320
so the first one is a vfio driver based

00:23:41,440 --> 00:23:46,240
zero copy so the current vehicle zero

00:23:44,320 --> 00:23:48,159
copy is implemented based on the

00:23:46,240 --> 00:23:50,559
uio-based security driver

00:23:48,159 --> 00:23:52,480
so this requires the chimney to be root

00:23:50,559 --> 00:23:54,159
privileged to gather the virtual grass

00:23:52,480 --> 00:23:55,520
to physical drastic mapping via the

00:23:54,159 --> 00:23:58,240
pitch map

00:23:55,520 --> 00:23:59,200
and this may be difficult for some cloud

00:23:58,240 --> 00:24:00,720
vendors because

00:23:59,200 --> 00:24:02,400
their community doesn't have root

00:24:00,720 --> 00:24:05,200
performance

00:24:02,400 --> 00:24:06,000
and this also requires a cumulative pin

00:24:05,200 --> 00:24:08,159
it's memory

00:24:06,000 --> 00:24:09,760
with the vf but io based driver will

00:24:08,159 --> 00:24:13,600
actually have the

00:24:09,760 --> 00:24:13,600
the rare file support to do this

00:24:17,120 --> 00:24:20,640
so for the communities where firebase

00:24:19,360 --> 00:24:22,799
the user destroyer is

00:24:20,640 --> 00:24:24,080
still working progress and we will be

00:24:22,799 --> 00:24:28,080
able to switch to that

00:24:24,080 --> 00:24:30,559
later and the second uh

00:24:28,080 --> 00:24:32,159
the second work we plan to do is smart

00:24:30,559 --> 00:24:35,679
acceleration support

00:24:32,159 --> 00:24:38,960
so this comes with the idea that uh dsa

00:24:35,679 --> 00:24:41,960
can do a direction of the dirty memory

00:24:38,960 --> 00:24:43,760
so the functionality is the same as the

00:24:41,960 --> 00:24:46,640
xbre in the current

00:24:43,760 --> 00:24:47,200
preview so it finds out uh the exact

00:24:46,640 --> 00:24:49,760
page

00:24:47,200 --> 00:24:51,200
that the guest is the elector buys that

00:24:49,760 --> 00:24:53,679
the guests the dirtiest

00:24:51,200 --> 00:24:55,360
for example if the if the guest only

00:24:53,679 --> 00:24:57,760
writes one byte

00:24:55,360 --> 00:25:00,480
so there is no need to send the entire

00:24:57,760 --> 00:25:02,400
4kb page to the destination it can

00:25:00,480 --> 00:25:04,080
the essay can help to find out the one

00:25:02,400 --> 00:25:06,880
byte

00:25:04,080 --> 00:25:08,480
um so this works efficiently when the

00:25:06,880 --> 00:25:12,400
guest only modifies a small

00:25:08,480 --> 00:25:15,919
part of page but we know if the guest

00:25:12,400 --> 00:25:16,480
writes the entire 40 pages of the the 4k

00:25:15,919 --> 00:25:20,400
bytes

00:25:16,480 --> 00:25:23,919
each time then the bsc

00:25:20,400 --> 00:25:27,200
galaxy encoding might not be efficient

00:25:23,919 --> 00:25:29,440
so in that case we can use security to

00:25:27,200 --> 00:25:31,440
compression the foot device instead of

00:25:29,440 --> 00:25:34,799
instead of doing the dirt

00:25:31,440 --> 00:25:37,120
processing so we wanted to have a smart

00:25:34,799 --> 00:25:39,600
acceleration here

00:25:37,120 --> 00:25:41,440
so with this technical the migrants

00:25:39,600 --> 00:25:44,480
migrations that will be able to

00:25:41,440 --> 00:25:46,960
dynamically switch to your security

00:25:44,480 --> 00:25:48,400
ix ix is also accelerator that can be

00:25:46,960 --> 00:25:51,760
used to compression

00:25:48,400 --> 00:25:55,120
so it can select either qt or is

00:25:51,760 --> 00:25:56,159
to do compression or use dsc device to

00:25:55,120 --> 00:25:58,880
do a direct

00:25:56,159 --> 00:25:59,679
processing during non-migration so this

00:25:58,880 --> 00:26:02,960
will

00:25:59,679 --> 00:26:05,520
rely on a prediction based on the

00:26:02,960 --> 00:26:07,039
compression visual history and the data

00:26:05,520 --> 00:26:10,480
encoding

00:26:07,039 --> 00:26:13,440
history so for example we can choose to

00:26:10,480 --> 00:26:14,080
do uh to compress the 10 requests at the

00:26:13,440 --> 00:26:16,880
beginning

00:26:14,080 --> 00:26:17,520
and find out what's the compression

00:26:16,880 --> 00:26:20,880
ratio

00:26:17,520 --> 00:26:24,640
and also do using the essay to

00:26:20,880 --> 00:26:26,880
to find out the encoding read

00:26:24,640 --> 00:26:28,159
if the encoding rate is higher than for

00:26:26,880 --> 00:26:30,400
the upcoming

00:26:28,159 --> 00:26:32,480
pages we can choose dnc if the

00:26:30,400 --> 00:26:34,960
compression rate is higher we can

00:26:32,480 --> 00:26:37,360
use security for the following

00:26:34,960 --> 00:26:45,919
processing

00:26:37,360 --> 00:26:45,919

YouTube URL: https://www.youtube.com/watch?v=X91nQ_Zx5Z0


