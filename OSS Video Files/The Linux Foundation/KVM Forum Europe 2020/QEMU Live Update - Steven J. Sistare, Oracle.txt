Title: QEMU Live Update - Steven J. Sistare, Oracle
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	QEMU Live Update - Steven J. Sistare, Oracle
Captions: 
	00:00:07,440 --> 00:00:10,400
hi folks

00:00:08,240 --> 00:00:12,160
i'm steve cystere and i work on linux

00:00:10,400 --> 00:00:14,160
kernel and virtualization features at

00:00:12,160 --> 00:00:16,560
oracle with my colleagues anthony

00:00:14,160 --> 00:00:20,560
snaga and mark conda welcome to our

00:00:16,560 --> 00:00:20,560
presentation of qmu live update

00:00:23,359 --> 00:00:26,560
live update is a method to update qmu to

00:00:25,680 --> 00:00:28,720
a new version

00:00:26,560 --> 00:00:30,480
while keeping the guest alive has

00:00:28,720 --> 00:00:32,320
minimal impact on the guest

00:00:30,480 --> 00:00:34,320
the guest pauses briefly for about a

00:00:32,320 --> 00:00:36,719
hundred milliseconds

00:00:34,320 --> 00:00:38,559
update is transparent to guest clients

00:00:36,719 --> 00:00:39,280
they suffer no loss of connectivity to

00:00:38,559 --> 00:00:42,640
the guest

00:00:39,280 --> 00:00:44,960
and only experience the brief pause

00:00:42,640 --> 00:00:46,000
the method supports sriov without guest

00:00:44,960 --> 00:00:47,600
cooperation

00:00:46,000 --> 00:00:49,760
so there's no restriction on the guest

00:00:47,600 --> 00:00:51,840
operating system

00:00:49,760 --> 00:00:53,360
we do this to enable critical bug fixes

00:00:51,840 --> 00:00:55,760
and security mitigations

00:00:53,360 --> 00:00:58,879
in a timely manner to keep our guests

00:00:55,760 --> 00:01:01,039
safe without requiring them to reboot

00:00:58,879 --> 00:01:02,640
however because we update to an entirely

00:01:01,039 --> 00:01:05,920
new version of qmu

00:01:02,640 --> 00:01:07,760
we enable new features as well

00:01:05,920 --> 00:01:09,680
live migration can be used to achieve

00:01:07,760 --> 00:01:10,960
the same results but is more resource

00:01:09,680 --> 00:01:12,960
intensive

00:01:10,960 --> 00:01:15,600
it ties up the source and target hosts

00:01:12,960 --> 00:01:17,840
decreasing fleet utilization

00:01:15,600 --> 00:01:19,280
it consumes memory network bandwidth

00:01:17,840 --> 00:01:21,680
impacting the performance of other

00:01:19,280 --> 00:01:23,200
processes on the guest and the host

00:01:21,680 --> 00:01:25,280
the duration of the impact is

00:01:23,200 --> 00:01:26,000
indeterminate as it depends on when the

00:01:25,280 --> 00:01:29,280
copy phase

00:01:26,000 --> 00:01:30,720
converges lastly live migration is

00:01:29,280 --> 00:01:32,640
prohibitively expensive

00:01:30,720 --> 00:01:40,400
if large local storage must be copied

00:01:32,640 --> 00:01:42,079
across the network to the target

00:01:40,400 --> 00:01:43,920
live update is based on the following

00:01:42,079 --> 00:01:46,399
design elements

00:01:43,920 --> 00:01:47,119
the old qmu process execs the new qmu

00:01:46,399 --> 00:01:48,799
binary

00:01:47,119 --> 00:01:50,479
allowing various aspects of the

00:01:48,799 --> 00:01:53,280
execution environment to be carried

00:01:50,479 --> 00:01:55,200
forward into the new

00:01:53,280 --> 00:01:57,360
guest memory is preserved in place in

00:01:55,200 --> 00:01:59,520
ram so dma operations may safely

00:01:57,360 --> 00:02:01,200
continue

00:01:59,520 --> 00:02:04,640
external descriptors are kept open

00:02:01,200 --> 00:02:06,560
across the exact preserving connectivity

00:02:04,640 --> 00:02:08,640
this includes for example serial

00:02:06,560 --> 00:02:11,360
consoles qmu monitor

00:02:08,640 --> 00:02:13,440
vnc sessions pseudoterminals and vhost

00:02:11,360 --> 00:02:15,680
devices

00:02:13,440 --> 00:02:17,520
vfio device descriptors are preserved

00:02:15,680 --> 00:02:20,000
which keeps them alive

00:02:17,520 --> 00:02:22,319
however the kvm descriptor is closed

00:02:20,000 --> 00:02:23,599
which destroys the instance cutting the

00:02:22,319 --> 00:02:26,959
cord between kvm

00:02:23,599 --> 00:02:29,360
and the vfio kernel state

00:02:26,959 --> 00:02:32,480
lastly the qmu back-end device state is

00:02:29,360 --> 00:02:34,640
serialized and saved to a file

00:02:32,480 --> 00:02:36,480
these elements are executed by two new

00:02:34,640 --> 00:02:39,519
qmu monitor interfaces

00:02:36,480 --> 00:02:42,800
cpr save and cpr load where cpr

00:02:39,519 --> 00:02:46,879
stands for checkpoint and restart qmp

00:02:42,800 --> 00:02:46,879
and hmp versions of each are provided

00:02:47,680 --> 00:02:51,440
live update has been a hot topic this

00:02:49,440 --> 00:02:53,519
year and you may notice some overlap

00:02:51,440 --> 00:02:55,599
between our work and others

00:02:53,519 --> 00:02:57,599
however we've been working independently

00:02:55,599 --> 00:02:59,360
in this area for quite some time

00:02:57,599 --> 00:03:05,840
i believe we are the first to submit our

00:02:59,360 --> 00:03:05,840
patches to the community

00:03:07,120 --> 00:03:11,280
to preserve guest memory in place we

00:03:09,440 --> 00:03:12,080
propose an extension to the m advised

00:03:11,280 --> 00:03:15,360
system called

00:03:12,080 --> 00:03:16,640
m advise do exec this preserves mappings

00:03:15,360 --> 00:03:19,760
in an address range across

00:03:16,640 --> 00:03:21,599
exec the same virtual address it works

00:03:19,760 --> 00:03:23,280
for memory created with map and on

00:03:21,599 --> 00:03:25,680
which otherwise would disappear after

00:03:23,280 --> 00:03:25,680
exec

00:03:26,080 --> 00:03:29,680
the executed binary must explicitly

00:03:28,159 --> 00:03:32,239
allow incoming mappings

00:03:29,680 --> 00:03:35,280
via an elf node this prevents unexpected

00:03:32,239 --> 00:03:37,280
sharing of content across the exec

00:03:35,280 --> 00:03:39,599
the implementation is straightforward

00:03:37,280 --> 00:03:41,120
about 300 lines of kernel code

00:03:39,599 --> 00:03:43,440
most of that is for reading and checking

00:03:41,120 --> 00:03:45,760
the elf note

00:03:43,440 --> 00:03:47,840
the m advise call sets a new keep flag

00:03:45,760 --> 00:03:51,120
on the vmas that span the range

00:03:47,840 --> 00:03:53,439
splitting them if necessary exec

00:03:51,120 --> 00:03:54,159
copies the mark vmas from the old mm to

00:03:53,439 --> 00:03:55,920
the new

00:03:54,159 --> 00:03:58,239
almost exactly like the vm dupe

00:03:55,920 --> 00:04:00,560
operation in fork

00:03:58,239 --> 00:04:02,159
for details see the m advise do exec

00:04:00,560 --> 00:04:04,640
kernel patches that anthony and i

00:04:02,159 --> 00:04:04,640
submitted

00:04:11,120 --> 00:04:16,799
when m buys is used for qmu the dma

00:04:13,920 --> 00:04:19,120
mappings remain valid at all times

00:04:16,799 --> 00:04:19,919
dma activity from post requests

00:04:19,120 --> 00:04:23,120
continues

00:04:19,919 --> 00:04:25,600
even while the guest is paused

00:04:23,120 --> 00:04:26,240
it is safe to translate iova to virtual

00:04:25,600 --> 00:04:28,639
address

00:04:26,240 --> 00:04:30,479
and page throughout the transition so

00:04:28,639 --> 00:04:34,560
asynchronous kernel threads may safely

00:04:30,479 --> 00:04:34,560
create and access the dma regions

00:04:35,120 --> 00:04:38,720
qmu saves the address and length of the

00:04:37,360 --> 00:04:40,720
preserved memory regions

00:04:38,720 --> 00:04:42,160
in environment variables tagged with the

00:04:40,720 --> 00:04:44,400
name of the region

00:04:42,160 --> 00:04:46,400
in the example at right the pc.ram

00:04:44,400 --> 00:04:48,160
region is remembered in the environment

00:04:46,400 --> 00:04:50,479
with values for both the address and the

00:04:48,160 --> 00:04:50,479
length

00:04:50,560 --> 00:04:55,199
after exec qmu looks for variables of

00:04:52,720 --> 00:04:57,280
this form and retrieves the address

00:04:55,199 --> 00:04:59,040
the address is attached to the new kvm

00:04:57,280 --> 00:05:01,600
instance via the set memory region

00:04:59,040 --> 00:05:01,600
ioctyl

00:05:01,759 --> 00:05:05,039
the first time we did this we were

00:05:03,120 --> 00:05:07,039
surprised to find that the iactal time

00:05:05,039 --> 00:05:08,720
rose linearly with page count

00:05:07,039 --> 00:05:10,960
adding hundreds of milliseconds for

00:05:08,720 --> 00:05:13,039
larger memories

00:05:10,960 --> 00:05:14,800
anthony investigated and found it to be

00:05:13,039 --> 00:05:17,759
an accident of the implementation

00:05:14,800 --> 00:05:18,240
easily fixed he eliminated the linear

00:05:17,759 --> 00:05:20,160
cost

00:05:18,240 --> 00:05:25,840
with the following kernel patch which is

00:05:20,160 --> 00:05:25,840
available in kernel 5.8

00:05:28,080 --> 00:05:32,479
to support vfio devices we preserve

00:05:30,720 --> 00:05:34,080
their descriptors across exec

00:05:32,479 --> 00:05:36,320
which preserves the kernel state of the

00:05:34,080 --> 00:05:38,720
device after exec

00:05:36,320 --> 00:05:40,320
qmu finds the descriptors and rebuilds

00:05:38,720 --> 00:05:42,320
the data structures that represent the

00:05:40,320 --> 00:05:44,880
device

00:05:42,320 --> 00:05:47,199
the pci bar and config memory regions

00:05:44,880 --> 00:05:50,639
are accessible via the vfio device

00:05:47,199 --> 00:05:53,840
fd after exec qmu maps the bars

00:05:50,639 --> 00:05:55,840
and re-reads the config

00:05:53,840 --> 00:05:58,000
the dma mappings are kept alive by

00:05:55,840 --> 00:06:01,840
preserving the i o mmu group fd

00:05:58,000 --> 00:06:03,840
in the container fd the interrupt state

00:06:01,840 --> 00:06:06,080
is captured by the event fds and the

00:06:03,840 --> 00:06:08,400
msix data

00:06:06,080 --> 00:06:10,160
event fts are created and preserved for

00:06:08,400 --> 00:06:14,080
the error and request irqs

00:06:10,160 --> 00:06:16,240
in an msix irq per vector

00:06:14,080 --> 00:06:20,560
the msix table and pending bit are saved

00:06:16,240 --> 00:06:22,400
to when restored from the vm state file

00:06:20,560 --> 00:06:24,000
the values of the descriptors are saved

00:06:22,400 --> 00:06:26,560
in the environment

00:06:24,000 --> 00:06:27,600
the box on the right shows all the fds

00:06:26,560 --> 00:06:30,800
saved for one

00:06:27,600 --> 00:06:31,280
vfio device the name is not pretty but

00:06:30,800 --> 00:06:33,120
they

00:06:31,280 --> 00:06:35,039
completely describe and identify the

00:06:33,120 --> 00:06:37,280
descriptor

00:06:35,039 --> 00:06:39,600
for example the highlighted entry is the

00:06:37,280 --> 00:06:42,479
kvm irq chip notifier

00:06:39,600 --> 00:06:46,560
for vector zero device three colon 10

00:06:42,479 --> 00:06:46,560
descriptor number 163

00:06:46,720 --> 00:06:50,080
after exec cue mu attaches the vfi

00:06:49,280 --> 00:06:52,000
descriptors

00:06:50,080 --> 00:06:53,199
to the new kvm instance using the

00:06:52,000 --> 00:06:56,479
appropriate eye octals

00:06:53,199 --> 00:06:59,440
shown here the required code changes to

00:06:56,479 --> 00:07:02,080
achieve all this are surprisingly small

00:06:59,440 --> 00:07:03,120
wherever a vfi descriptor is created we

00:07:02,080 --> 00:07:05,680
check the environment

00:07:03,120 --> 00:07:08,160
and use that value instead then execute

00:07:05,680 --> 00:07:10,560
the existing code paths

00:07:08,160 --> 00:07:11,199
we remember that the fd is reused and

00:07:10,560 --> 00:07:13,199
skip in

00:07:11,199 --> 00:07:15,599
any ioctals that would reconfigure the

00:07:13,199 --> 00:07:15,599
device

00:07:15,759 --> 00:07:19,280
we have tested this with interrupts

00:07:17,599 --> 00:07:22,080
delivered to qmu

00:07:19,280 --> 00:07:23,919
to the kernel kvm irq chip and posted

00:07:22,080 --> 00:07:27,759
directly to the guest

00:07:23,919 --> 00:07:27,759
all work robustly across the update

00:07:28,840 --> 00:07:31,840
operation

00:07:34,400 --> 00:07:38,319
to handle other qmu device state we

00:07:36,639 --> 00:07:40,160
leverage the vm state framework that

00:07:38,319 --> 00:07:42,080
live migration uses

00:07:40,160 --> 00:07:44,000
we modify the code so that the save and

00:07:42,080 --> 00:07:46,560
restore handlers can be selected based

00:07:44,000 --> 00:07:49,520
on the operation such as cpr versus

00:07:46,560 --> 00:07:51,599
snapshot versus migration

00:07:49,520 --> 00:07:52,720
objects are serialized to an ordinary

00:07:51,599 --> 00:07:54,879
file

00:07:52,720 --> 00:07:57,199
not a socket like live migration and not

00:07:54,879 --> 00:07:59,120
to a qcow snapshot

00:07:57,199 --> 00:08:02,400
this allows us to support a variety of

00:07:59,120 --> 00:08:04,240
image formats and guest boot devices

00:08:02,400 --> 00:08:05,440
however because the block devices are

00:08:04,240 --> 00:08:07,280
not snapshotted

00:08:05,440 --> 00:08:10,240
one must not modify the blocks between

00:08:07,280 --> 00:08:12,319
the save and the restore

00:08:10,240 --> 00:08:13,680
the save file is small less than one

00:08:12,319 --> 00:08:15,520
megabyte

00:08:13,680 --> 00:08:19,039
writing the file is very fast adding

00:08:15,520 --> 00:08:19,039
little to the guest pause time

00:08:22,720 --> 00:08:27,280
the cpr save command puts it all

00:08:24,840 --> 00:08:29,520
together you specify the file for saving

00:08:27,280 --> 00:08:31,440
state and a mode argument

00:08:29,520 --> 00:08:34,880
the mode is the keyword restart for live

00:08:31,440 --> 00:08:37,760
update i'll show another mode shortly

00:08:34,880 --> 00:08:40,560
qmu pauses the guest fee cpus and saves

00:08:37,760 --> 00:08:42,719
device state to the file it calls and

00:08:40,560 --> 00:08:46,959
advise due exec for all the ram segments

00:08:42,719 --> 00:08:48,800
such as main memory video ram and others

00:08:46,959 --> 00:08:50,880
it clears the close on exec flag for

00:08:48,800 --> 00:08:53,440
vfio and other descriptors and remembers

00:08:50,880 --> 00:08:55,360
their values in the environment

00:08:53,440 --> 00:08:58,560
it destroys the old kvm instance and

00:08:55,360 --> 00:08:58,560
exacts the new qmu

00:08:58,640 --> 00:09:04,560
however if user bin qmu exec exists

00:09:01,680 --> 00:09:06,480
we call that instead a site may provide

00:09:04,560 --> 00:09:07,440
this binary to customize the update

00:09:06,480 --> 00:09:10,080
procedure

00:09:07,440 --> 00:09:11,040
by changing the qmu binary path changing

00:09:10,080 --> 00:09:14,080
the rv

00:09:11,040 --> 00:09:16,080
or modifying the execution environment

00:09:14,080 --> 00:09:18,480
we use it to run qmu in a container

00:09:16,080 --> 00:09:21,200
environment for example

00:09:18,480 --> 00:09:24,240
qmu exec finishes off by executing the

00:09:21,200 --> 00:09:26,560
new qmu binary

00:09:24,240 --> 00:09:27,680
new qmu starts and creates a new kvm

00:09:26,560 --> 00:09:30,320
instance

00:09:27,680 --> 00:09:32,880
it finds and reuses ram segments finds

00:09:30,320 --> 00:09:35,120
and reuses vfio and other descriptors

00:09:32,880 --> 00:09:37,839
and attaches vfio to the new kvm

00:09:35,120 --> 00:09:37,839
instance

00:09:41,440 --> 00:09:46,000
qmu is now in the pre-launch state the

00:09:44,080 --> 00:09:47,680
management layer now has the opportunity

00:09:46,000 --> 00:09:49,600
to send device ad commands

00:09:47,680 --> 00:09:51,040
that supplement the devices defined by

00:09:49,600 --> 00:09:53,360
the rgv

00:09:51,040 --> 00:09:56,320
this is why update is divided into cpr

00:09:53,360 --> 00:09:58,800
save and cpr load phases

00:09:56,320 --> 00:10:00,720
cpr load is fairly simple it loads

00:09:58,800 --> 00:10:02,720
device state from the file and continues

00:10:00,720 --> 00:10:04,560
the vcpus

00:10:02,720 --> 00:10:06,640
the guest is running again controlled by

00:10:04,560 --> 00:10:08,959
a new version of qmu

00:10:06,640 --> 00:10:10,480
the pause time is about 100 milliseconds

00:10:08,959 --> 00:10:11,839
and this on a four-year-old xenon

00:10:10,480 --> 00:10:14,160
processor

00:10:11,839 --> 00:10:15,839
we've not spent any time profiling this

00:10:14,160 --> 00:10:17,040
and i expect with optimizations and a

00:10:15,839 --> 00:10:20,399
recent processor

00:10:17,040 --> 00:10:20,399
this can be quite a bit faster

00:10:24,560 --> 00:10:28,399
here's an example using the interactive

00:10:30,839 --> 00:10:34,880
monitor

00:10:32,000 --> 00:10:36,560
window one on the left we start qmu the

00:10:34,880 --> 00:10:37,680
status command shows that the guest is

00:10:36,560 --> 00:10:39,920
running

00:10:37,680 --> 00:10:42,320
in window 2 on the right we use yum to

00:10:39,920 --> 00:10:44,079
update qmu on disk

00:10:42,320 --> 00:10:45,920
this does not affect the running qmu

00:10:44,079 --> 00:10:47,360
process and the guest is still live and

00:10:45,920 --> 00:10:49,279
running

00:10:47,360 --> 00:10:52,079
on the left we issue the cpr save

00:10:49,279 --> 00:10:53,760
command it execs the new qmu binary and

00:10:52,079 --> 00:10:55,839
returns

00:10:53,760 --> 00:10:56,880
status shows the vm is in the pre-launch

00:10:55,839 --> 00:10:59,440
state

00:10:56,880 --> 00:11:03,519
to finish off we issue cpr load the

00:10:59,440 --> 00:11:03,519
guest resumes and the update is complete

00:11:08,240 --> 00:11:15,839
now for a short demonstration

00:11:17,839 --> 00:11:22,240
in this demo i run a script that updates

00:11:20,079 --> 00:11:24,240
qmu in our container environment and

00:11:22,240 --> 00:11:26,079
issues the monitor commands

00:11:24,240 --> 00:11:28,560
the host is on the left and is running a

00:11:26,079 --> 00:11:29,200
single instance with qmu version dash

00:11:28,560 --> 00:11:33,040
one

00:11:29,200 --> 00:11:33,040
which is shown by querying the monitor

00:11:33,279 --> 00:11:36,480
the guest is on the right and i start a

00:11:35,040 --> 00:11:38,079
counting program there

00:11:36,480 --> 00:11:41,200
this will show that the guest is live

00:11:38,079 --> 00:11:41,200
throughout the demonstration

00:11:41,360 --> 00:11:44,880
now i type update commands on the host

00:11:43,760 --> 00:11:48,480
the prepare command

00:11:44,880 --> 00:11:50,720
mounts the new qmu version

00:11:48,480 --> 00:11:51,760
suspend stops the guest and execs the

00:11:50,720 --> 00:11:55,279
new version

00:11:51,760 --> 00:11:56,000
our count stops resume continues the

00:11:55,279 --> 00:11:59,040
guest

00:11:56,000 --> 00:11:59,040
and our account resumes

00:11:59,120 --> 00:12:04,320
now we are running the qmu dash version

00:12:01,279 --> 00:12:04,320
the update is complete

00:12:04,399 --> 00:12:07,440
now let's do it all with a single

00:12:05,839 --> 00:12:09,600
restart command

00:12:07,440 --> 00:12:12,079
watch the count it barely stops barely

00:12:09,600 --> 00:12:12,079
hiccups

00:12:14,639 --> 00:12:18,240
now let's go back and forth between the

00:12:16,480 --> 00:12:21,040
old new versions repeatedly

00:12:18,240 --> 00:12:23,200
show how robust the update feature is

00:12:21,040 --> 00:12:24,320
for each update the guest pause time is

00:12:23,200 --> 00:12:27,920
measured and printed

00:12:24,320 --> 00:12:27,920
and is about 100 milliseconds

00:12:28,720 --> 00:12:39,839
we are updating back and forth

00:12:30,079 --> 00:12:39,839
continuously but the guest marches on

00:12:46,079 --> 00:12:49,279
i've shown you how to update qmu using

00:12:48,160 --> 00:12:50,800
cpr save

00:12:49,279 --> 00:12:53,600
which implies you must start with a

00:12:50,800 --> 00:12:55,600
recent version that supports the command

00:12:53,600 --> 00:12:57,920
however we can also update a legacy

00:12:55,600 --> 00:13:01,839
version by dynamically injecting code

00:12:57,920 --> 00:13:03,839
that performs the equivalent of cpr save

00:13:01,839 --> 00:13:05,200
the vm save shared object provides the

00:13:03,839 --> 00:13:07,360
injected code

00:13:05,200 --> 00:13:10,160
it accesses qmu data structures and

00:13:07,360 --> 00:13:12,560
globals such as the list of ram handlers

00:13:10,160 --> 00:13:13,839
the list of vm state handlers character

00:13:12,560 --> 00:13:17,360
devices and v-host

00:13:13,839 --> 00:13:19,680
devices just to name a few

00:13:17,360 --> 00:13:21,680
however deal open does not resolve the

00:13:19,680 --> 00:13:22,320
address of these globals when vmsave is

00:13:21,680 --> 00:13:24,720
loaded

00:13:22,320 --> 00:13:26,480
because qmu is loaded with the rtld

00:13:24,720 --> 00:13:29,040
local flag

00:13:26,480 --> 00:13:32,480
so we wrote code to find the addresses

00:13:29,040 --> 00:13:34,560
by looking them up in the symbol table

00:13:32,480 --> 00:13:36,240
the code deletes some vm state handlers

00:13:34,560 --> 00:13:37,200
such as those specifically for live

00:13:36,240 --> 00:13:40,639
migration

00:13:37,200 --> 00:13:42,399
and registers a new handler for vfio

00:13:40,639 --> 00:13:45,199
it calls m advise to exec and guest

00:13:42,399 --> 00:13:49,839
memory it finds devices and lists and

00:13:45,199 --> 00:13:49,839
preserves their descriptors

00:13:51,519 --> 00:13:55,839
to invoke the code we hot patch over the

00:13:53,519 --> 00:13:58,639
text of a qmu monitor function

00:13:55,839 --> 00:13:59,600
by writing to proc mem we then call the

00:13:58,639 --> 00:14:02,480
monitor function

00:13:59,600 --> 00:14:03,920
using an already created monitor socket

00:14:02,480 --> 00:14:06,160
the code patch is small

00:14:03,920 --> 00:14:07,839
the deal opens vm save and then calls

00:14:06,160 --> 00:14:10,560
its entry point

00:14:07,839 --> 00:14:12,639
vmsave runs saves state and execs the

00:14:10,560 --> 00:14:15,440
new qmu

00:14:12,639 --> 00:14:16,079
the new qmu includes live update so we

00:14:15,440 --> 00:14:18,480
simply call

00:14:16,079 --> 00:14:20,959
cprload to finish the update and resume

00:14:18,480 --> 00:14:20,959
the guest

00:14:21,199 --> 00:14:25,519
the vm save library has binary

00:14:23,120 --> 00:14:26,480
dependencies on qmu data structures and

00:14:25,519 --> 00:14:28,639
variables

00:14:26,480 --> 00:14:30,720
so we build a separate vmsave library

00:14:28,639 --> 00:14:34,240
for each legacy qmu version

00:14:30,720 --> 00:14:36,480
indexed by gcc build id we extract the

00:14:34,240 --> 00:14:40,079
build id from the running qmu process

00:14:36,480 --> 00:14:42,480
and inject the matching vm save object

00:14:40,079 --> 00:14:44,000
the technique is fast and reliable the

00:14:42,480 --> 00:14:46,320
guest pause time is roughly

00:14:44,000 --> 00:14:48,120
the same as for cpr save and we've

00:14:46,320 --> 00:14:52,399
successfully tested updates from

00:14:48,120 --> 00:14:59,839
qmu2.x to 3.x and 3.x to 4.x

00:14:52,399 --> 00:14:59,839
it's pretty cool

00:15:00,720 --> 00:15:04,399
many critical fixes can be applied by

00:15:02,720 --> 00:15:05,920
updating only qmu

00:15:04,399 --> 00:15:08,079
but if you need to update the host

00:15:05,920 --> 00:15:11,279
kernel we have a method for doing so

00:15:08,079 --> 00:15:13,760
in the cpr framework the mode argument

00:15:11,279 --> 00:15:16,720
in this case is reboot

00:15:13,760 --> 00:15:18,959
after cpr save uk exec boot the kernel

00:15:16,720 --> 00:15:20,959
and issue cpr load

00:15:18,959 --> 00:15:22,639
the guest pause time is longer but

00:15:20,959 --> 00:15:23,920
connections from the guest kernel to the

00:15:22,639 --> 00:15:27,120
outside world

00:15:23,920 --> 00:15:28,959
survive the reboot

00:15:27,120 --> 00:15:30,320
the guest ram must be backed by a shared

00:15:28,959 --> 00:15:32,320
memory segment

00:15:30,320 --> 00:15:34,399
the segment is preserved across k exec

00:15:32,320 --> 00:15:36,639
reboot by anthony's pk ram kernel

00:15:34,399 --> 00:15:38,720
patches

00:15:36,639 --> 00:15:40,000
the pages of the segment are visited to

00:15:38,720 --> 00:15:43,040
find the pfns that must be

00:15:40,000 --> 00:15:44,160
preserved the 8 by pfns are packed onto

00:15:43,040 --> 00:15:46,240
free pages

00:15:44,160 --> 00:15:49,920
those pages are linked together and the

00:15:46,240 --> 00:15:52,000
head is passed across the k exec

00:15:49,920 --> 00:15:54,320
early in reboot the pfns are mapped to

00:15:52,000 --> 00:15:55,600
pages and those pages are removed from

00:15:54,320 --> 00:15:58,160
the free list

00:15:55,600 --> 00:16:01,839
the schmema node is recreated and pages

00:15:58,160 --> 00:16:01,839
are attached to the file's address space

00:16:04,399 --> 00:16:08,480
page visitation and reclaim are

00:16:06,240 --> 00:16:10,240
parallelized for speed and hundreds of

00:16:08,480 --> 00:16:11,360
gigabytes can be preserved in less than

00:16:10,240 --> 00:16:13,360
one second

00:16:11,360 --> 00:16:15,839
see the pk ram patch series for more

00:16:13,360 --> 00:16:15,839
details

00:16:16,639 --> 00:16:20,800
we support sri ov devices if the guest

00:16:19,199 --> 00:16:24,639
provides an agent that implements

00:16:20,800 --> 00:16:26,079
suspender ram such as qmuga

00:16:24,639 --> 00:16:29,519
the update sequence starts with

00:16:26,079 --> 00:16:32,959
suspender ram followed by cpr save

00:16:29,519 --> 00:16:35,440
k exec reboot cpr load and finishes with

00:16:32,959 --> 00:16:37,759
a system wake up

00:16:35,440 --> 00:16:39,600
on suspender ram the guest device

00:16:37,759 --> 00:16:42,160
drivers flush posted requests

00:16:39,600 --> 00:16:43,920
and reinitialized to a reset state the

00:16:42,160 --> 00:16:45,440
same state reached after the host

00:16:43,920 --> 00:16:47,360
reboots

00:16:45,440 --> 00:16:51,839
thus when the guest resumes the guests

00:16:47,360 --> 00:16:51,839
and the host agree on the state

00:17:02,240 --> 00:17:06,079
that's a wrap on a work to date in the

00:17:04,640 --> 00:17:08,400
future we would like to merge with

00:17:06,079 --> 00:17:10,559
intel's of emm fast restart work which

00:17:08,400 --> 00:17:13,199
keeps sri iov devices alive

00:17:10,559 --> 00:17:14,880
and configured across reboot this is

00:17:13,199 --> 00:17:18,160
faster than suspender ram

00:17:14,880 --> 00:17:21,600
and eliminates the guest agent current

00:17:18,160 --> 00:17:24,400
kvm forum presentation

00:17:21,600 --> 00:17:25,039
the m advise do do exec extension works

00:17:24,400 --> 00:17:26,799
great

00:17:25,039 --> 00:17:28,559
but our upstream reviewers are not keen

00:17:26,799 --> 00:17:30,720
on accepting it so i'm pondering

00:17:28,559 --> 00:17:32,960
alternate solutions

00:17:30,720 --> 00:17:35,679
i'm considering standard system calls to

00:17:32,960 --> 00:17:38,880
reattach to guest memory after exec

00:17:35,679 --> 00:17:42,000
such as schmatt or map of a mfd

00:17:38,880 --> 00:17:45,520
plus nui octals to tell vfio that the va

00:17:42,000 --> 00:17:47,520
is changing stay tuned on that

00:17:45,520 --> 00:17:50,000
lastly we received great feedback from

00:17:47,520 --> 00:17:51,679
our qmu reviewers in version one

00:17:50,000 --> 00:17:55,200
of the live update patches and we're

00:17:51,679 --> 00:17:55,200
busily working on version two

00:17:55,280 --> 00:17:58,799
we very much look forward to making live

00:17:57,039 --> 00:18:00,320
update available to the community

00:17:58,799 --> 00:18:02,799
here are some references to follow our

00:18:00,320 --> 00:18:05,120
work if you're interested

00:18:02,799 --> 00:18:06,720
i hope you've enjoyed this chalk talk

00:18:05,120 --> 00:18:15,600
and thanks for attending

00:18:06,720 --> 00:18:17,679
i'll now take your questions over chad

00:18:15,600 --> 00:18:17,679

YouTube URL: https://www.youtube.com/watch?v=1gbA4LDjXDM


