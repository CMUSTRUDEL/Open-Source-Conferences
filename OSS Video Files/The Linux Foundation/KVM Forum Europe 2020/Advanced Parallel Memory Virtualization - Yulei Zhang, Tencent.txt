Title: Advanced Parallel Memory Virtualization - Yulei Zhang, Tencent
Publication date: 2020-11-12
Playlist: KVM Forum Europe 2020
Description: 
	Advanced Parallel Memory Virtualization - Yulei Zhang, Tencent
Captions: 
	00:00:05,920 --> 00:00:11,920
hello everyone welcome to the

00:00:08,280 --> 00:00:14,719
2020 kvm forum my name is

00:00:11,920 --> 00:00:16,960
zhang yule i'm working for turns on the

00:00:14,719 --> 00:00:19,840
cloud which is the

00:00:16,960 --> 00:00:21,199
leading public cloud provider in china

00:00:19,840 --> 00:00:23,519
this session's topic

00:00:21,199 --> 00:00:24,800
is advanced parallel memory

00:00:23,519 --> 00:00:27,519
virtualization

00:00:24,800 --> 00:00:28,080
we would like to use this opportunity to

00:00:27,519 --> 00:00:31,599
introduce

00:00:28,080 --> 00:00:35,120
idea about how to mitigate the burden of

00:00:31,599 --> 00:00:38,079
mmu log to boosted forecast performance

00:00:35,120 --> 00:00:38,960
in virtualization environment after this

00:00:38,079 --> 00:00:40,399
session

00:00:38,960 --> 00:00:42,879
you can reach me through the email

00:00:40,399 --> 00:00:45,039
address if you are interested in it

00:00:42,879 --> 00:00:46,879
and any questions and suggestions are

00:00:45,039 --> 00:00:49,360
welcome

00:00:46,879 --> 00:00:51,199
this is today's agenda for today's topic

00:00:49,360 --> 00:00:54,079
we will start with the issues that

00:00:51,199 --> 00:00:55,760
inspire us to create this solution and

00:00:54,079 --> 00:00:57,520
go through the details about the

00:00:55,760 --> 00:01:00,320
implementation

00:00:57,520 --> 00:01:01,199
at the last we will talk about the plans

00:01:00,320 --> 00:01:04,000
to continue

00:01:01,199 --> 00:01:07,119
improving this and hopefully it can be

00:01:04,000 --> 00:01:07,119
upstream in the future

00:01:07,920 --> 00:01:11,280
in our cloud environment we got an issue

00:01:10,240 --> 00:01:13,439
that

00:01:11,280 --> 00:01:14,560
there is a significant significant

00:01:13,439 --> 00:01:18,479
performance drop

00:01:14,560 --> 00:01:21,600
in the gaster afterlife migration

00:01:18,479 --> 00:01:25,119
the commonalities for these guests

00:01:21,600 --> 00:01:28,080
are they have multiple vcpus usually

00:01:25,119 --> 00:01:30,320
have numerous memories and with a huge

00:01:28,080 --> 00:01:32,479
page table enabled

00:01:30,320 --> 00:01:34,560
the workload in the guest becomes really

00:01:32,479 --> 00:01:37,040
slow after migration

00:01:34,560 --> 00:01:37,600
which made us thinking about what

00:01:37,040 --> 00:01:41,280
happened

00:01:37,600 --> 00:01:43,759
behind the scene

00:01:41,280 --> 00:01:44,799
so we will try to debug this issue after

00:01:43,759 --> 00:01:48,079
debugging

00:01:44,799 --> 00:01:50,079
we find the following phenomenons

00:01:48,079 --> 00:01:51,680
from the trace log we find that there

00:01:50,079 --> 00:01:54,240
are a bunch of page fault

00:01:51,680 --> 00:01:54,960
after the migration because after

00:01:54,240 --> 00:01:57,840
migration

00:01:54,960 --> 00:01:58,399
when used when the guests try to access

00:01:57,840 --> 00:02:01,759
memory

00:01:58,399 --> 00:02:04,560
it has to set up the page table at first

00:02:01,759 --> 00:02:05,200
and also performance job getting worse

00:02:04,560 --> 00:02:08,479
if we

00:02:05,200 --> 00:02:11,440
change or enlarge the guest memory size

00:02:08,479 --> 00:02:13,360
so what is happening let's look at the

00:02:11,440 --> 00:02:17,040
page table setup procedure

00:02:13,360 --> 00:02:19,280
now this is the diagram

00:02:17,040 --> 00:02:20,720
about the epd setup process in current

00:02:19,280 --> 00:02:22,800
kvm

00:02:20,720 --> 00:02:24,080
the npt valuation will cause the vm

00:02:22,800 --> 00:02:26,319
exists

00:02:24,080 --> 00:02:28,080
and the epd valuation handler will check

00:02:26,319 --> 00:02:31,440
if it is the

00:02:28,080 --> 00:02:32,400
mmio address if not it will check if it

00:02:31,440 --> 00:02:35,760
can be

00:02:32,400 --> 00:02:38,480
handled by the first page fault pass

00:02:35,760 --> 00:02:39,440
which is used for dirty log and access

00:02:38,480 --> 00:02:42,080
tracking

00:02:39,440 --> 00:02:43,760
if not either then it will go through

00:02:42,080 --> 00:02:46,239
the rail page fault handler

00:02:43,760 --> 00:02:48,480
to ping the guest memory at first and

00:02:46,239 --> 00:02:51,519
then before updating the page table

00:02:48,480 --> 00:02:54,640
it need acquire the mmu lock and hold

00:02:51,519 --> 00:02:57,200
the lock until it finishes the work

00:02:54,640 --> 00:02:59,120
so that you can see the mmu lock will

00:02:57,200 --> 00:03:01,840
block other vcpus from

00:02:59,120 --> 00:03:04,159
touch the page table and force them to

00:03:01,840 --> 00:03:07,120
become a sequential operation

00:03:04,159 --> 00:03:08,319
if there are multiple vcpus have to

00:03:07,120 --> 00:03:11,840
update the page table

00:03:08,319 --> 00:03:12,720
at the same time they will have to wait

00:03:11,840 --> 00:03:16,159
in line

00:03:12,720 --> 00:03:18,640
thus they cannot return to the guest

00:03:16,159 --> 00:03:20,959
to work on their rail tasks until they

00:03:18,640 --> 00:03:22,959
have the pay table ready

00:03:20,959 --> 00:03:24,239
so the performance drop in the guest is

00:03:22,959 --> 00:03:26,720
expected

00:03:24,239 --> 00:03:27,440
now we know the bottleneck is the mmu

00:03:26,720 --> 00:03:30,400
lock

00:03:27,440 --> 00:03:33,120
on the page fault handler pass but the

00:03:30,400 --> 00:03:36,080
lock is used to protect the page table

00:03:33,120 --> 00:03:38,640
and the synchronized updating can we get

00:03:36,080 --> 00:03:38,640
rid of it

00:03:39,040 --> 00:03:43,760
in order to solve this problem and

00:03:41,440 --> 00:03:47,120
invade the burden of mmu log

00:03:43,760 --> 00:03:48,799
here comes our proposal for it we will

00:03:47,120 --> 00:03:49,599
pre-construct the pay table for the

00:03:48,799 --> 00:03:52,159
guest

00:03:49,599 --> 00:03:53,120
so that it won't be necessary to vm

00:03:52,159 --> 00:03:55,040
exist

00:03:53,120 --> 00:03:56,720
to the host to set up the page mapping

00:03:55,040 --> 00:04:00,159
again during the

00:03:56,720 --> 00:04:02,400
guest life cycle and it is also able to

00:04:00,159 --> 00:04:03,680
locklessly update the read write status

00:04:02,400 --> 00:04:07,599
in the page table

00:04:03,680 --> 00:04:07,599
for the dirty log and page tracking

00:04:09,360 --> 00:04:13,040
this is the overview diagram for our

00:04:11,519 --> 00:04:16,239
implementation

00:04:13,040 --> 00:04:19,359
as the guest when the guest boot up into

00:04:16,239 --> 00:04:22,560
a boot up to separate the memory region

00:04:19,359 --> 00:04:23,360
without control like kvm set memory

00:04:22,560 --> 00:04:25,120
region

00:04:23,360 --> 00:04:27,680
we will construct the page table

00:04:25,120 --> 00:04:32,400
according to the memory source change

00:04:27,680 --> 00:04:37,280
and it is protected by the kevin memory

00:04:32,400 --> 00:04:37,280
slot lock after that it is also

00:04:38,080 --> 00:04:42,080
support dirty logging as our control is

00:04:40,880 --> 00:04:45,120
also protected

00:04:42,080 --> 00:04:47,440
by the same slot lock

00:04:45,120 --> 00:04:48,639
and the updated to read write and dirty

00:04:47,440 --> 00:04:56,160
bit in the

00:04:48,639 --> 00:04:58,800
pte is atomic and lockless

00:04:56,160 --> 00:05:00,479
here's the detail about the page table

00:04:58,800 --> 00:05:02,479
pre-construction

00:05:00,479 --> 00:05:03,840
after getting io control about the

00:05:02,479 --> 00:05:06,160
memory region change

00:05:03,840 --> 00:05:07,039
we will iterate the ping the guest

00:05:06,160 --> 00:05:09,520
memory and

00:05:07,039 --> 00:05:10,479
set up the page mapping with different

00:05:09,520 --> 00:05:13,919
page size

00:05:10,479 --> 00:05:15,759
granularity the page label is stored in

00:05:13,919 --> 00:05:20,000
the root pointer named

00:05:15,759 --> 00:05:20,880
global root hpa so before the vcpus

00:05:20,000 --> 00:05:23,919
enter the guest

00:05:20,880 --> 00:05:24,560
it will invoke the mme log to load the

00:05:23,919 --> 00:05:28,720
our

00:05:24,560 --> 00:05:31,199
load into load our root point into cr3

00:05:28,720 --> 00:05:33,440
to use the pre-populated page table

00:05:31,199 --> 00:05:36,400
which will help them get rid of the page

00:05:33,440 --> 00:05:36,400
folder exception

00:05:36,960 --> 00:05:40,160
and in addition we want to support the

00:05:39,280 --> 00:05:43,199
migration

00:05:40,160 --> 00:05:43,759
for the guest in order to do so we have

00:05:43,199 --> 00:05:46,000
to

00:05:43,759 --> 00:05:47,280
break down the page table into 4k

00:05:46,000 --> 00:05:49,680
granularity

00:05:47,280 --> 00:05:50,479
when it starts to do the page dirty

00:05:49,680 --> 00:05:55,280
logging if

00:05:50,479 --> 00:05:58,720
there is a huge page enabled and if

00:05:55,280 --> 00:05:59,600
hardware support dirty log we will clear

00:05:58,720 --> 00:06:02,319
the dirty bit

00:05:59,600 --> 00:06:03,120
on the page table entry otherwise we

00:06:02,319 --> 00:06:05,840
will set up

00:06:03,120 --> 00:06:06,400
write protect on the page table entry

00:06:05,840 --> 00:06:08,800
and

00:06:06,400 --> 00:06:10,240
if the migration fails we will restore

00:06:08,800 --> 00:06:13,680
the table as well

00:06:10,240 --> 00:06:13,680
with a huge page enabled

00:06:14,880 --> 00:06:21,360
thus you will find the original

00:06:18,319 --> 00:06:22,639
scenarios that vcpu handled together to

00:06:21,360 --> 00:06:25,680
update the pay table

00:06:22,639 --> 00:06:28,000
is gone now that vcpu can update

00:06:25,680 --> 00:06:29,759
the page table in parallel which can

00:06:28,000 --> 00:06:35,840
improve the guest performance

00:06:29,759 --> 00:06:35,840
and get rid of the burden of mmu log

00:06:36,479 --> 00:06:40,400
this is the initial performance data we

00:06:38,479 --> 00:06:44,080
got with the initial patch

00:06:40,400 --> 00:06:47,120
we create a guest with 32 vcpus and

00:06:44,080 --> 00:06:50,160
64 giga memories and

00:06:47,120 --> 00:06:54,639
let vcpu dirty the entire memory region

00:06:50,160 --> 00:06:57,039
concurrently so each vcpu thread

00:06:54,639 --> 00:06:59,360
will dirty 2 gigahertz memories we

00:06:57,039 --> 00:07:00,960
compare the time for each vcpu threader

00:06:59,360 --> 00:07:04,080
to finish the dirty job

00:07:00,960 --> 00:07:04,720
you can see in 4k granularity the

00:07:04,080 --> 00:07:08,400
improvement

00:07:04,720 --> 00:07:09,599
is huge with the normal page photo

00:07:08,400 --> 00:07:13,759
process

00:07:09,599 --> 00:07:15,120
each view takes about 18 to 21 seconds

00:07:13,759 --> 00:07:17,840
to finish the job

00:07:15,120 --> 00:07:18,639
which you with the pre-populated method

00:07:17,840 --> 00:07:22,400
it only takes

00:07:18,639 --> 00:07:26,080
2 to 2.5 seconds and if we enable the

00:07:22,400 --> 00:07:29,039
huge page for example about the 2 mega

00:07:26,080 --> 00:07:30,960
granularity the normal code path will

00:07:29,039 --> 00:07:34,960
take about

00:07:30,960 --> 00:07:37,680
32 point or to three to six seconds

00:07:34,960 --> 00:07:40,160
which is about 1.5 times longer than our

00:07:37,680 --> 00:07:40,160
solution

00:07:41,599 --> 00:07:46,240
so after all you can see the benefit we

00:07:43,759 --> 00:07:49,599
got from the new app implementation

00:07:46,240 --> 00:07:51,680
as we got logic access to the page table

00:07:49,599 --> 00:07:54,319
guest will get performance improved as

00:07:51,680 --> 00:07:57,919
it doesn't need page folder to set up

00:07:54,319 --> 00:08:00,160
the page table and also we can save some

00:07:57,919 --> 00:08:01,919
memory system resources as we do not

00:08:00,160 --> 00:08:04,800
need the

00:08:01,919 --> 00:08:06,879
mmu notification so far and we can also

00:08:04,800 --> 00:08:10,879
drop the shadow page cache

00:08:06,879 --> 00:08:10,879
and parent reserve the mapping

00:08:11,280 --> 00:08:16,800
but still there are some limitations for

00:08:13,199 --> 00:08:19,919
us uh currently we only support the

00:08:16,800 --> 00:08:23,520
future with mma virtualization enabled

00:08:19,919 --> 00:08:26,240
and so far the system management

00:08:23,520 --> 00:08:28,240
mode is not supported and we need a

00:08:26,240 --> 00:08:30,400
pre-pin the guest memory

00:08:28,240 --> 00:08:32,560
if we have to support the system

00:08:30,400 --> 00:08:34,399
management mode we have to ping the same

00:08:32,560 --> 00:08:37,200
memory slot for them

00:08:34,399 --> 00:08:38,560
which requires extra memory to use so

00:08:37,200 --> 00:08:40,800
probably

00:08:38,560 --> 00:08:42,880
we will fall back to original page photo

00:08:40,800 --> 00:08:43,519
mode when it enter the system management

00:08:42,880 --> 00:08:45,920
mode

00:08:43,519 --> 00:08:47,760
to only set up the page mapping on the

00:08:45,920 --> 00:08:50,800
mounted

00:08:47,760 --> 00:08:53,440
again we as we have to ping the

00:08:50,800 --> 00:08:54,800
whole guest memory in advance so the

00:08:53,440 --> 00:08:58,080
memory over commitment

00:08:54,800 --> 00:08:58,080
is not supported either

00:09:00,640 --> 00:09:05,200
at last let's talk about something we

00:09:02,880 --> 00:09:07,440
are working on right now

00:09:05,200 --> 00:09:08,240
as ben gordon pointed out in the mail

00:09:07,440 --> 00:09:11,920
list

00:09:08,240 --> 00:09:14,959
we do not support the post copy in

00:09:11,920 --> 00:09:17,040
migration yet as we know post copy rely

00:09:14,959 --> 00:09:19,120
on the uniform fd to handle the page

00:09:17,040 --> 00:09:22,000
fault in user space

00:09:19,120 --> 00:09:23,519
in the post copy live migration context

00:09:22,000 --> 00:09:27,040
the newly

00:09:23,519 --> 00:09:29,600
spawned vm try to access a

00:09:27,040 --> 00:09:32,480
memory page and if it fails the vm

00:09:29,600 --> 00:09:35,480
fetched the memory page from original vm

00:09:32,480 --> 00:09:37,120
memory over the network but in

00:09:35,480 --> 00:09:39,360
pre-population method

00:09:37,120 --> 00:09:40,560
we already set up the page table mapping

00:09:39,360 --> 00:09:43,040
on the target site

00:09:40,560 --> 00:09:45,360
so there's no trigger to help attack the

00:09:43,040 --> 00:09:49,120
vm to fetch memory

00:09:45,360 --> 00:09:51,040
from the source side as we plan to make

00:09:49,120 --> 00:09:54,800
this implementation a common solution

00:09:51,040 --> 00:09:58,740
and a support to support the post copy

00:09:54,800 --> 00:10:00,640
as this diagram shows our idea is to

00:09:58,740 --> 00:10:02,959
[Music]

00:10:00,640 --> 00:10:04,079
partially invalidate the page table

00:10:02,959 --> 00:10:07,279
mapping with

00:10:04,079 --> 00:10:09,839
m advice system call so that it could

00:10:07,279 --> 00:10:12,800
generate page fault for the user fd on

00:10:09,839 --> 00:10:15,279
target site to handle the post copy

00:10:12,800 --> 00:10:16,839
this work is ongoing we will send our

00:10:15,279 --> 00:10:19,600
office again when we

00:10:16,839 --> 00:10:22,000
finished

00:10:19,600 --> 00:10:25,200
below is the link on this page this is

00:10:22,000 --> 00:10:27,600
the link to the page that we send out

00:10:25,200 --> 00:10:29,839
you're welcome to have a try with it and

00:10:27,600 --> 00:10:33,440
bring us any feedback if it

00:10:29,839 --> 00:10:35,200
is helpful thanks i think that's all for

00:10:33,440 --> 00:10:37,600
today's sharing

00:10:35,200 --> 00:10:37,600
thank you

00:10:39,040 --> 00:10:47,600

YouTube URL: https://www.youtube.com/watch?v=aagwc0Tda4c


