Title: Faster and Smaller qcow2 Files With Subcluster-based Allocation - Alberto Garcia, Igalia
Publication date: 2020-11-10
Playlist: KVM Forum Europe 2020
Description: 
	Faster and Smaller qcow2 Files With Subcluster-based Allocation - Alberto Garcia, Igalia
Captions: 
	00:00:05,120 --> 00:00:08,800
hello and

00:00:06,160 --> 00:00:09,599
thanks for listening my name is alberto

00:00:08,800 --> 00:00:11,679
garcia

00:00:09,599 --> 00:00:12,799
i work for regalia on the qmi project

00:00:11,679 --> 00:00:14,639
and in this presentation

00:00:12,799 --> 00:00:16,560
i'm going to talk about the work that i

00:00:14,639 --> 00:00:18,000
have been doing lately and this is

00:00:16,560 --> 00:00:19,680
related to the qco2

00:00:18,000 --> 00:00:21,840
file format as you know this is the

00:00:19,680 --> 00:00:22,960
native file format used by qmu and it

00:00:21,840 --> 00:00:26,640
supports many features

00:00:22,960 --> 00:00:28,080
has encryption compression backing files

00:00:26,640 --> 00:00:30,160
etc

00:00:28,080 --> 00:00:31,519
but the the question that i'm going to

00:00:30,160 --> 00:00:33,840
try to answer today is

00:00:31,519 --> 00:00:35,440
why is it that sometimes this uh slower

00:00:33,840 --> 00:00:36,880
than a raw file

00:00:35,440 --> 00:00:38,399
there's many reasons for that it can be

00:00:36,880 --> 00:00:39,760
because it hasn't been configured

00:00:38,399 --> 00:00:42,239
correctly we're not using the right

00:00:39,760 --> 00:00:44,399
options for our setup

00:00:42,239 --> 00:00:45,360
of course it can be that the driver can

00:00:44,399 --> 00:00:48,239
be improved and

00:00:45,360 --> 00:00:48,800
there's still room for improvement there

00:00:48,239 --> 00:00:50,960
um

00:00:48,800 --> 00:00:52,160
three years ago in the kvm forums 2017 i

00:00:50,960 --> 00:00:54,000
was uh

00:00:52,160 --> 00:00:55,680
talking about these things so you can

00:00:54,000 --> 00:00:57,760
check the talk it's in youtube it's

00:00:55,680 --> 00:00:59,920
available

00:00:57,760 --> 00:01:01,359
today i want to focus on the problems

00:00:59,920 --> 00:01:05,760
that are a result of the

00:01:01,359 --> 00:01:08,000
very design of the qco2 file format

00:01:05,760 --> 00:01:09,040
so let's start with the format itself

00:01:08,000 --> 00:01:10,880
how it works

00:01:09,040 --> 00:01:12,080
uh the basic idea of the the quicker

00:01:10,880 --> 00:01:13,920
qco2 file it is

00:01:12,080 --> 00:01:17,439
it is divided into clusters of the same

00:01:13,920 --> 00:01:18,880
size 64k by default but

00:01:17,439 --> 00:01:21,920
it can be changed when the file is

00:01:18,880 --> 00:01:24,080
created from 512 bytes to

00:01:21,920 --> 00:01:25,680
up to 2 megabytes there are different

00:01:24,080 --> 00:01:26,080
cluster size i'm not going to go into

00:01:25,680 --> 00:01:29,200
the

00:01:26,080 --> 00:01:31,280
details now but let's focus on the data

00:01:29,200 --> 00:01:34,000
cluster which contains the

00:01:31,280 --> 00:01:35,600
data that the guest can see so every

00:01:34,000 --> 00:01:38,479
time the guest

00:01:35,600 --> 00:01:39,600
needs to read data it goes to the qcoto

00:01:38,479 --> 00:01:41,520
file is the

00:01:39,600 --> 00:01:43,360
the data the cluster has been allocated

00:01:41,520 --> 00:01:44,799
there then it just reads the data from

00:01:43,360 --> 00:01:46,240
the cluster

00:01:44,799 --> 00:01:48,320
but if the cluster hasn't been allocated

00:01:46,240 --> 00:01:49,600
then the it contains zeros

00:01:48,320 --> 00:01:51,119
or if there's a working file it goes to

00:01:49,600 --> 00:01:52,960
the backend file and checks the data is

00:01:51,119 --> 00:01:55,680
there

00:01:52,960 --> 00:01:57,280
so the cluster is the smallest unit of

00:01:55,680 --> 00:01:58,640
allocation so every time we allocate a

00:01:57,280 --> 00:02:00,399
new cluster

00:01:58,640 --> 00:02:02,240
if the write request is smaller than the

00:02:00,399 --> 00:02:03,520
cluster size then we need to fill the

00:02:02,240 --> 00:02:06,000
rest with data

00:02:03,520 --> 00:02:07,759
and the data means we either go to the

00:02:06,000 --> 00:02:10,239
backing file and get it from there

00:02:07,759 --> 00:02:10,879
so in the case of in this image that you

00:02:10,239 --> 00:02:13,440
see

00:02:10,879 --> 00:02:14,879
we write to the area in pink we will

00:02:13,440 --> 00:02:15,520
need to go to the backing file and read

00:02:14,879 --> 00:02:17,200
the

00:02:15,520 --> 00:02:19,040
the areas in dark blue and copy them to

00:02:17,200 --> 00:02:20,640
the active file

00:02:19,040 --> 00:02:22,080
or if there's no viking file we just

00:02:20,640 --> 00:02:23,440
fill it with zeros but we still need to

00:02:22,080 --> 00:02:25,360
feel it

00:02:23,440 --> 00:02:26,959
uh so the problem is that of course q i

00:02:25,360 --> 00:02:28,560
mean is to perform additional i o to

00:02:26,959 --> 00:02:29,360
copy the rest of the data so the copy

00:02:28,560 --> 00:02:32,160
and write this and

00:02:29,360 --> 00:02:33,360
can be an expensive operation as you can

00:02:32,160 --> 00:02:35,040
imagine the

00:02:33,360 --> 00:02:36,480
when we increase the cluster size we

00:02:35,040 --> 00:02:38,000
have to do more copy and write so the

00:02:36,480 --> 00:02:39,440
performance goes down you can see that

00:02:38,000 --> 00:02:40,800
in the table

00:02:39,440 --> 00:02:42,720
i had to mention though that if you

00:02:40,800 --> 00:02:46,239
don't have a backing file then

00:02:42,720 --> 00:02:47,440
as i said we should fill the the cluster

00:02:46,239 --> 00:02:50,879
with zeros but um

00:02:47,440 --> 00:02:52,800
nowadays uses f allocated try to to

00:02:50,879 --> 00:02:54,319
fill it in a more efficient way so if

00:02:52,800 --> 00:02:55,680
that works the file system supports it

00:02:54,319 --> 00:02:57,120
and the operating system supported this

00:02:55,680 --> 00:02:58,400
is very fast and then the cluster size

00:02:57,120 --> 00:02:59,920
doesn't have any effect

00:02:58,400 --> 00:03:02,400
but if that doesn't work then it goes to

00:02:59,920 --> 00:03:02,800
the slow path of writing actual zeros to

00:03:02,400 --> 00:03:04,239
disk

00:03:02,800 --> 00:03:06,560
and then you can see the numbers that

00:03:04,239 --> 00:03:08,159
i'm showing in the table

00:03:06,560 --> 00:03:10,959
if there's a vacuum file however there's

00:03:08,159 --> 00:03:12,800
no no alternative we need to go to the

00:03:10,959 --> 00:03:16,640
backing file and get the data from there

00:03:12,800 --> 00:03:19,760
so that's that's where the the cluster

00:03:16,640 --> 00:03:21,440
has an effect the other

00:03:19,760 --> 00:03:22,800
consequence of this is of course the

00:03:21,440 --> 00:03:25,360
larger the cluster size

00:03:22,800 --> 00:03:27,680
we do more io we do more copy and write

00:03:25,360 --> 00:03:30,560
and then the image

00:03:27,680 --> 00:03:32,000
is bigger you write the same data but

00:03:30,560 --> 00:03:33,280
you get a bigger image and result

00:03:32,000 --> 00:03:34,799
because you are duplicating data from

00:03:33,280 --> 00:03:36,959
the backing file

00:03:34,799 --> 00:03:39,519
how much well this depends a lot on the

00:03:36,959 --> 00:03:42,560
use case here reports that

00:03:39,519 --> 00:03:44,560
can be 30 larger or 40

00:03:42,560 --> 00:03:46,080
larger but it depends a lot on your use

00:03:44,560 --> 00:03:48,239
case i was just doing a

00:03:46,080 --> 00:03:49,120
couple of tests for this presentation

00:03:48,239 --> 00:03:50,720
and

00:03:49,120 --> 00:03:52,319
you can see that if we have an empty

00:03:50,720 --> 00:03:55,840
image and we write

00:03:52,319 --> 00:03:59,280
100 megabytes worth of

00:03:55,840 --> 00:04:02,400
random 4k requests the the impact of

00:03:59,280 --> 00:04:02,720
having a a larger cluster size is very

00:04:02,400 --> 00:04:06,319
big

00:04:02,720 --> 00:04:07,439
we have a default 64k we get a 1.6

00:04:06,319 --> 00:04:09,519
gigabyte

00:04:07,439 --> 00:04:11,439
image which is more than 10 times the

00:04:09,519 --> 00:04:13,280
what we were trying to write

00:04:11,439 --> 00:04:14,799
which is a lot but if we look at the

00:04:13,280 --> 00:04:17,759
maximum cluster size

00:04:14,799 --> 00:04:18,320
we get 29 gigabytes which is 300 times

00:04:17,759 --> 00:04:20,239
the

00:04:18,320 --> 00:04:21,759
the initial the amount of data that we

00:04:20,239 --> 00:04:22,639
want to write which is very big of

00:04:21,759 --> 00:04:26,000
course this is a

00:04:22,639 --> 00:04:28,479
extreme case normally in a real world

00:04:26,000 --> 00:04:30,240
scenario we don't just write random

00:04:28,479 --> 00:04:31,919
write request

00:04:30,240 --> 00:04:33,680
but it gives an idea of what the problem

00:04:31,919 --> 00:04:36,320
is um

00:04:33,680 --> 00:04:37,919
then i did a second test i took an empty

00:04:36,320 --> 00:04:38,880
one terabyte image and i created the

00:04:37,919 --> 00:04:41,360
file system there

00:04:38,880 --> 00:04:42,000
and you can see the file system itself

00:04:41,360 --> 00:04:44,080
the metadata

00:04:42,000 --> 00:04:47,120
used by the file system which is just

00:04:44,080 --> 00:04:48,960
well is 1.1.1 gigabyte

00:04:47,120 --> 00:04:50,800
but if you increase the cluster size and

00:04:48,960 --> 00:04:52,320
you take it to the maximum then you use

00:04:50,800 --> 00:04:54,160
one more gigabyte

00:04:52,320 --> 00:04:56,880
just for creating a file system an empty

00:04:54,160 --> 00:05:00,400
file system with nothing else in it

00:04:56,880 --> 00:05:01,600
so so in summary if we increase the

00:05:00,400 --> 00:05:03,520
cluster size

00:05:01,600 --> 00:05:06,479
we get less performance because there's

00:05:03,520 --> 00:05:08,560
additional i o that needs to be done

00:05:06,479 --> 00:05:10,560
and we also get larger larger images and

00:05:08,560 --> 00:05:12,479
duplicate data so

00:05:10,560 --> 00:05:15,199
things clear then we don't just just

00:05:12,479 --> 00:05:17,280
reduce the cluster size no

00:05:15,199 --> 00:05:19,360
problem is that is not so easy because

00:05:17,280 --> 00:05:21,919
smaller clusters means more metadata

00:05:19,360 --> 00:05:23,759
and more cluster source so what does

00:05:21,919 --> 00:05:26,560
this

00:05:23,759 --> 00:05:28,400
apart from the guest data itself qco2

00:05:26,560 --> 00:05:29,280
images also need to store metadata about

00:05:28,400 --> 00:05:31,600
the the

00:05:29,280 --> 00:05:32,800
cluster so important things are the

00:05:31,600 --> 00:05:35,919
cluster mappings which

00:05:32,800 --> 00:05:37,360
map the guest addresses to the host

00:05:35,919 --> 00:05:40,320
addresses

00:05:37,360 --> 00:05:41,759
and the reference count all clusters in

00:05:40,320 --> 00:05:43,440
key code to have reference counts how

00:05:41,759 --> 00:05:46,000
we're going to see later

00:05:43,440 --> 00:05:47,199
so if we're going to have more clusters

00:05:46,000 --> 00:05:51,039
we're going to have more of them

00:05:47,199 --> 00:05:51,039
so it means more metadata

00:05:51,280 --> 00:05:55,199
so the the mapping from the guest

00:05:53,360 --> 00:05:56,880
clusters to the the guest offices to the

00:05:55,199 --> 00:05:58,960
host offices

00:05:56,880 --> 00:05:59,919
uh is done using this structure that we

00:05:58,960 --> 00:06:02,960
call l21

00:05:59,919 --> 00:06:06,160
uh l1 and l2 tables this is a simple

00:06:02,960 --> 00:06:09,280
structure uh that maps

00:06:06,160 --> 00:06:11,840
virtual offsets into host offset you can

00:06:09,280 --> 00:06:14,880
see our example here in this graphic

00:06:11,840 --> 00:06:16,880
the l1 table is just one per image

00:06:14,880 --> 00:06:18,800
for snapshot actually because the qgo2

00:06:16,880 --> 00:06:22,160
format can have several snapshots but

00:06:18,800 --> 00:06:24,960
we're not going to go into that now

00:06:22,160 --> 00:06:26,880
but the table itself is very small it's

00:06:24,960 --> 00:06:30,319
for a one terabyte image it's just

00:06:26,880 --> 00:06:33,360
16k so it's nothing it's stored

00:06:30,319 --> 00:06:34,800
continuously in the image file and um

00:06:33,360 --> 00:06:36,080
always keeps it in memory because it's

00:06:34,800 --> 00:06:37,520
very small so it's not there's no

00:06:36,080 --> 00:06:39,360
problem with that

00:06:37,520 --> 00:06:42,160
and basically the table just contains

00:06:39,360 --> 00:06:44,000
pointers to the l2 tables

00:06:42,160 --> 00:06:45,759
digital tables there can be many of them

00:06:44,000 --> 00:06:47,280
and initially there's none but they

00:06:45,759 --> 00:06:49,199
they are allocated in demand as the

00:06:47,280 --> 00:06:51,120
image grows

00:06:49,199 --> 00:06:53,840
delta tables are always one cluster in

00:06:51,120 --> 00:06:55,759
size never more nevertheless

00:06:53,840 --> 00:06:57,440
and they also contain basically good

00:06:55,759 --> 00:06:59,840
pointers to the to the

00:06:57,440 --> 00:07:02,080
data clusters plus medicinal information

00:06:59,840 --> 00:07:03,840
that we're going to see later

00:07:02,080 --> 00:07:05,680
thing is that of course if we reduce the

00:07:03,840 --> 00:07:07,440
cluster size then we need

00:07:05,680 --> 00:07:08,800
more entries so graphically we have two

00:07:07,440 --> 00:07:11,440
clusters and we

00:07:08,800 --> 00:07:12,880
make the clusters twice as small then

00:07:11,440 --> 00:07:13,840
we're going to have four clusters and we

00:07:12,880 --> 00:07:17,120
need four

00:07:13,840 --> 00:07:18,720
entries this time so half the cluster

00:07:17,120 --> 00:07:21,680
size twice the metadata

00:07:18,720 --> 00:07:21,680
that's the basic idea

00:07:21,840 --> 00:07:25,599
here we see the table what's the the

00:07:23,759 --> 00:07:26,160
maximum metadata for a one terabyte

00:07:25,599 --> 00:07:28,639
image

00:07:26,160 --> 00:07:30,720
as you see if you reduce the cluster by

00:07:28,639 --> 00:07:32,479
size by half you increase metadata by

00:07:30,720 --> 00:07:34,639
two

00:07:32,479 --> 00:07:35,840
which is a very big uh difference of

00:07:34,639 --> 00:07:37,520
course so

00:07:35,840 --> 00:07:38,639
choosing the right cluster size has a

00:07:37,520 --> 00:07:39,599
very big impact on the amount of

00:07:38,639 --> 00:07:42,400
metadata that

00:07:39,599 --> 00:07:42,400
you have in the image

00:07:42,880 --> 00:07:46,160
so what does this mean every time we

00:07:45,039 --> 00:07:50,720
need to uh

00:07:46,160 --> 00:07:53,520
do a i o request in the from the guest

00:07:50,720 --> 00:07:54,319
curator needs to go to the l2 table and

00:07:53,520 --> 00:07:56,479
get the

00:07:54,319 --> 00:07:57,919
host offset let's transform the guest

00:07:56,479 --> 00:07:59,759
offset into the host offset

00:07:57,919 --> 00:08:00,960
so that's one additional operation per

00:07:59,759 --> 00:08:02,720
request and this is a

00:08:00,960 --> 00:08:04,240
has a very big import impacting

00:08:02,720 --> 00:08:07,280
performance

00:08:04,240 --> 00:08:07,520
so what qmu does in order to minimize it

00:08:07,280 --> 00:08:09,360
is

00:08:07,520 --> 00:08:12,560
it keeps the f2 tables in memory there's

00:08:09,360 --> 00:08:14,400
a cubecode to cache the l2 cache for

00:08:12,560 --> 00:08:16,240
that purpose

00:08:14,400 --> 00:08:17,599
i was talking about about it in more

00:08:16,240 --> 00:08:19,759
detail in the previous presentation that

00:08:17,599 --> 00:08:21,360
i mentioned earlier

00:08:19,759 --> 00:08:22,879
and it has a very big impact if we

00:08:21,360 --> 00:08:24,800
increase the cluster size we get

00:08:22,879 --> 00:08:26,319
much more performance so in the case of

00:08:24,800 --> 00:08:28,879
in this example that you see here

00:08:26,319 --> 00:08:30,879
the maximum cache need is 5 megabytes

00:08:28,879 --> 00:08:33,680
and we get

00:08:30,879 --> 00:08:35,120
40 000 operations per second but we we

00:08:33,680 --> 00:08:37,440
reduce the cluster size

00:08:35,120 --> 00:08:39,279
the cache size sorry that the

00:08:37,440 --> 00:08:41,039
performance code goes down very quickly

00:08:39,279 --> 00:08:43,200
because it means that

00:08:41,039 --> 00:08:44,240
there's no we need to go to disk to get

00:08:43,200 --> 00:08:47,839
the l2

00:08:44,240 --> 00:08:50,640
metadata more often so

00:08:47,839 --> 00:08:51,839
reducing the cluster size means we have

00:08:50,640 --> 00:08:54,399
much more metadata

00:08:51,839 --> 00:08:58,000
and we have much more ram than we need

00:08:54,399 --> 00:08:58,000
to keep that metadata memory

00:08:58,560 --> 00:09:01,600
then there's the reference count every

00:09:00,160 --> 00:09:02,800
cluster in a quick code image has a

00:09:01,600 --> 00:09:04,240
reference count

00:09:02,800 --> 00:09:05,600
all types of cluster not just data

00:09:04,240 --> 00:09:06,560
cluster these are used for example for

00:09:05,600 --> 00:09:08,560
snapshots because

00:09:06,560 --> 00:09:10,160
you need to know who is using each one

00:09:08,560 --> 00:09:11,360
of the clusters

00:09:10,160 --> 00:09:13,279
and they are storing a two-level

00:09:11,360 --> 00:09:15,279
structure called reference table and

00:09:13,279 --> 00:09:17,360
reference block it's very similar to the

00:09:15,279 --> 00:09:18,880
the one and the two tables that we just

00:09:17,360 --> 00:09:20,880
described

00:09:18,880 --> 00:09:22,640
so of course allocating new cluster has

00:09:20,880 --> 00:09:24,160
additional overhead

00:09:22,640 --> 00:09:26,320
because you need to update the reference

00:09:24,160 --> 00:09:28,000
counts so with more cluster we need to

00:09:26,320 --> 00:09:29,440
allocate more of them

00:09:28,000 --> 00:09:31,040
so in general we have a lot of small

00:09:29,440 --> 00:09:32,640
clusters we need to allocate first more

00:09:31,040 --> 00:09:34,399
clusters we need to locate more l2

00:09:32,640 --> 00:09:37,200
tables we need to allocate more

00:09:34,399 --> 00:09:37,600
reference blocks and all that together

00:09:37,200 --> 00:09:40,959
means

00:09:37,600 --> 00:09:41,839
that the although normally reducing the

00:09:40,959 --> 00:09:43,680
cluster size

00:09:41,839 --> 00:09:45,760
increases the performance because

00:09:43,680 --> 00:09:48,560
there's less copy and write involved

00:09:45,760 --> 00:09:50,720
once we go uh under a certain limit in

00:09:48,560 --> 00:09:53,120
this example is less than 16k

00:09:50,720 --> 00:09:54,560
the performance goes down very quickly

00:09:53,120 --> 00:09:55,279
as you can see the performance when we

00:09:54,560 --> 00:09:58,880
have

00:09:55,279 --> 00:10:00,480
4k clusters is horrible even though

00:09:58,880 --> 00:10:02,240
with 4k clusters there is no copy on

00:10:00,480 --> 00:10:03,360
right but there's we have to allocate so

00:10:02,240 --> 00:10:05,600
many clusters how many

00:10:03,360 --> 00:10:06,480
l2 tables so many reference blocks that

00:10:05,600 --> 00:10:09,440
is the

00:10:06,480 --> 00:10:09,440
performance is very bad

00:10:10,000 --> 00:10:14,160
so the situation so far is that we

00:10:12,240 --> 00:10:17,200
cannot have two big clusters because

00:10:14,160 --> 00:10:18,880
they waste too much space and there's

00:10:17,200 --> 00:10:20,480
additional i o needed for a copy and

00:10:18,880 --> 00:10:21,600
write and we cannot have two small

00:10:20,480 --> 00:10:22,880
clusters because they increase the

00:10:21,600 --> 00:10:25,200
amount of metadata and

00:10:22,880 --> 00:10:26,160
if we decrease it too much then it's

00:10:25,200 --> 00:10:28,560
also very bad

00:10:26,160 --> 00:10:29,839
performance and this is a direct

00:10:28,560 --> 00:10:31,279
consequence of the

00:10:29,839 --> 00:10:33,200
the format itself is not something that

00:10:31,279 --> 00:10:36,480
you can fix in the driver so

00:10:33,200 --> 00:10:38,320
what can we do about it so

00:10:36,480 --> 00:10:39,839
the solution that i'm describing in this

00:10:38,320 --> 00:10:41,680
presentation is

00:10:39,839 --> 00:10:43,600
called sub-cluster allocation and the

00:10:41,680 --> 00:10:44,839
basic idea is that we have big clusters

00:10:43,600 --> 00:10:46,959
in order to reduce the amount of

00:10:44,839 --> 00:10:48,320
metadata but it's one of them is divided

00:10:46,959 --> 00:10:51,040
into sub-clusters

00:10:48,320 --> 00:10:52,480
that can be allocated separately so we

00:10:51,040 --> 00:10:55,600
have faster allocations and

00:10:52,480 --> 00:10:58,800
less disk usage so graphically

00:10:55,600 --> 00:11:00,410
a normal l2 table looks like this we

00:10:58,800 --> 00:11:01,600
have two data clusters as you can see

00:11:00,410 --> 00:11:03,279
[Music]

00:11:01,600 --> 00:11:05,200
with some clusters each one of the data

00:11:03,279 --> 00:11:08,880
clusters is divided into

00:11:05,200 --> 00:11:11,120
32 sub-clusters of the same size

00:11:08,880 --> 00:11:12,880
and they are allocated separately so in

00:11:11,120 --> 00:11:14,480
this case only the rs and blue

00:11:12,880 --> 00:11:16,800
are actually allocated and using space

00:11:14,480 --> 00:11:16,800
and disk

00:11:17,839 --> 00:11:20,959
uh internally the l2 table contains as i

00:11:20,240 --> 00:11:22,959
said earlier

00:11:20,959 --> 00:11:24,800
pointers to the data clusters basically

00:11:22,959 --> 00:11:26,320
it looks like this is the cluster offset

00:11:24,800 --> 00:11:27,680
plus a few more bits that indicate

00:11:26,320 --> 00:11:30,079
whether the cluster is

00:11:27,680 --> 00:11:31,600
allocated or not is compressed or not or

00:11:30,079 --> 00:11:33,839
it contains zeros

00:11:31,600 --> 00:11:35,040
contain zeros is a feature from qco2

00:11:33,839 --> 00:11:36,640
that means that the

00:11:35,040 --> 00:11:38,240
cluster doesn't have any other data

00:11:36,640 --> 00:11:40,160
other than zero so there's no need to go

00:11:38,240 --> 00:11:41,760
to the data cluster and read from there

00:11:40,160 --> 00:11:43,440
we just we just know that it's zeros and

00:11:41,760 --> 00:11:46,640
we can return zeros without

00:11:43,440 --> 00:11:48,079
doing the i o so we

00:11:46,640 --> 00:11:49,519
have some clusters we need to store

00:11:48,079 --> 00:11:51,040
additional information for that and

00:11:49,519 --> 00:11:54,079
there's no space here

00:11:51,040 --> 00:11:55,120
so we have uh we added this extended l2

00:11:54,079 --> 00:11:56,639
entries which is

00:11:55,120 --> 00:11:58,480
basically very similar to the ones that

00:11:56,639 --> 00:12:00,240
we had before but they contain an

00:11:58,480 --> 00:12:01,760
additional bitmap indicating the status

00:12:00,240 --> 00:12:03,519
of each sub-cluster

00:12:01,760 --> 00:12:05,920
so with this each one of the individuals

00:12:03,519 --> 00:12:06,560
of clusters can be allocated unallocated

00:12:05,920 --> 00:12:09,600
or

00:12:06,560 --> 00:12:11,360
can be all zeros also

00:12:09,600 --> 00:12:13,680
um compressed clusters don't have this

00:12:11,360 --> 00:12:15,600
however complex clusters

00:12:13,680 --> 00:12:18,560
they cannot be divided into sub-clusters

00:12:15,600 --> 00:12:20,320
and any way complex clusters

00:12:18,560 --> 00:12:22,480
there doesn't really make so much sense

00:12:20,320 --> 00:12:24,320
to to use compression with the

00:12:22,480 --> 00:12:27,200
extended l2 entries because there are

00:12:24,320 --> 00:12:27,200
different use cases

00:12:27,519 --> 00:12:31,360
so these cases that i see for the

00:12:28,959 --> 00:12:32,639
subclass relation are two

00:12:31,360 --> 00:12:34,079
one of them is having very large

00:12:32,639 --> 00:12:34,800
clusters because we want to minimize the

00:12:34,079 --> 00:12:36,399
amount of

00:12:34,800 --> 00:12:38,320
metadata and the amount of memory that

00:12:36,399 --> 00:12:41,519
we that we need

00:12:38,320 --> 00:12:44,480
but still want to have good io and

00:12:41,519 --> 00:12:44,480
have smaller images

00:12:44,560 --> 00:12:47,440
and the other use case is that we want

00:12:45,839 --> 00:12:48,160
to maximize the performance so we want

00:12:47,440 --> 00:12:49,760
to have the

00:12:48,160 --> 00:12:53,839
keep the location unit as close as

00:12:49,760 --> 00:12:53,839
possible to the to the

00:12:54,399 --> 00:13:00,000
block guest block size

00:12:58,480 --> 00:13:01,440
so we want to minimize the amount of

00:13:00,000 --> 00:13:02,800
copy and write and get the maximum

00:13:01,440 --> 00:13:05,200
performance

00:13:02,800 --> 00:13:07,040
so what does this mean as i said if we

00:13:05,200 --> 00:13:08,160
make the sub cluster size equal to the

00:13:07,040 --> 00:13:10,240
request size

00:13:08,160 --> 00:13:11,920
it means that the files have the file

00:13:10,240 --> 00:13:13,040
system block size then we get there's no

00:13:11,920 --> 00:13:14,160
copy and write at all and we get the

00:13:13,040 --> 00:13:16,160
maximum performance

00:13:14,160 --> 00:13:18,320
we can see here that compared to the

00:13:16,160 --> 00:13:20,480
previous to default

00:13:18,320 --> 00:13:21,680
setup without some clusters in some

00:13:20,480 --> 00:13:25,839
cases we get

00:13:21,680 --> 00:13:28,880
10 times more io operation per second

00:13:25,839 --> 00:13:31,200
and if we go to the cases where

00:13:28,880 --> 00:13:32,800
the sub-cluster is 4k or less which is

00:13:31,200 --> 00:13:33,680
the the size of the request in this

00:13:32,800 --> 00:13:35,040
example

00:13:33,680 --> 00:13:37,680
then we get the maximum performance

00:13:35,040 --> 00:13:41,199
which is 12 13

00:13:37,680 --> 00:13:41,199
k i operation per second

00:13:42,320 --> 00:13:45,279
without the backing file the relative

00:13:43,760 --> 00:13:46,720
differences are the same of course it is

00:13:45,279 --> 00:13:48,720
faster because we don't need to go to

00:13:46,720 --> 00:13:50,880
the backing file to read the data

00:13:48,720 --> 00:13:53,040
and again i want to mention that if the

00:13:50,880 --> 00:13:54,959
file system supports

00:13:53,040 --> 00:13:56,480
empty in the cluster with f allocate

00:13:54,959 --> 00:13:57,920
then this

00:13:56,480 --> 00:13:59,440
is going to be much faster than this and

00:13:57,920 --> 00:14:00,079
then actually using some cluster doesn't

00:13:59,440 --> 00:14:01,760
really

00:14:00,079 --> 00:14:04,399
make a difference it's not going to be

00:14:01,760 --> 00:14:07,519
faster with that

00:14:04,399 --> 00:14:10,079
so you have to consider that

00:14:07,519 --> 00:14:12,160
about the the space of course if we have

00:14:10,079 --> 00:14:15,519
now the smaller allocation

00:14:12,160 --> 00:14:18,240
units then the images grow

00:14:15,519 --> 00:14:18,880
much less so we compared the random

00:14:18,240 --> 00:14:20,800
writes that i

00:14:18,880 --> 00:14:22,800
that i mentioned before we write 100

00:14:20,800 --> 00:14:25,519
megabytes in 4k

00:14:22,800 --> 00:14:27,040
write requests the end result is much

00:14:25,519 --> 00:14:31,360
much smaller as you can see in the

00:14:27,040 --> 00:14:36,399
example so so of course this is a

00:14:31,360 --> 00:14:40,000
improves a lot how we use the disk

00:14:36,399 --> 00:14:43,760
and although l2 extent delta entries are

00:14:40,000 --> 00:14:47,040
twice as large as normal to entries

00:14:43,760 --> 00:14:49,040
we would uh in principle it would use uh

00:14:47,040 --> 00:14:50,320
more metadata however since each one of

00:14:49,040 --> 00:14:54,800
the l2

00:14:50,320 --> 00:14:57,199
entries now points to 60 32 subclusters

00:14:54,800 --> 00:14:58,800
the end result is that we have 16 times

00:14:57,199 --> 00:15:00,399
less metadata for the same unit of

00:14:58,800 --> 00:15:01,680
allocation so we compared units of

00:15:00,399 --> 00:15:04,800
allocation

00:15:01,680 --> 00:15:08,160
clusters in traditional l2 entries and

00:15:04,800 --> 00:15:11,120
sub-clusters in extendable to entries

00:15:08,160 --> 00:15:13,279
we see that for a 64k cluster size we

00:15:11,120 --> 00:15:16,000
would need 128

00:15:13,279 --> 00:15:17,360
megabytes of cache for a one terabyte

00:15:16,000 --> 00:15:19,199
image

00:15:17,360 --> 00:15:20,560
but with the standalone two entries if

00:15:19,199 --> 00:15:22,079
we have 64k

00:15:20,560 --> 00:15:23,680
sub clusters we only need eight

00:15:22,079 --> 00:15:27,839
megabytes which is much less

00:15:23,680 --> 00:15:30,000
so we can have much larger clusters and

00:15:27,839 --> 00:15:30,880
keep good performance and without

00:15:30,000 --> 00:15:34,000
needing so much

00:15:30,880 --> 00:15:35,279
memory for the cache so

00:15:34,000 --> 00:15:36,800
things that need to be taken into

00:15:35,279 --> 00:15:39,040
account all this looks good but this is

00:15:36,800 --> 00:15:40,800
not magic so

00:15:39,040 --> 00:15:42,720
this feature is useful during allocation

00:15:40,800 --> 00:15:47,120
once the cluster is allocated

00:15:42,720 --> 00:15:48,639
there's no uh qco2 works just fine

00:15:47,120 --> 00:15:50,959
with or without sub-clusters and you get

00:15:48,639 --> 00:15:52,720
good performance so once the image is

00:15:50,959 --> 00:15:55,519
located this is not going to really help

00:15:52,720 --> 00:15:57,199
so much more

00:15:55,519 --> 00:15:58,639
again with compressed with compressed

00:15:57,199 --> 00:15:59,040
images it doesn't make sense i think

00:15:58,639 --> 00:16:01,519
it's a

00:15:59,040 --> 00:16:02,800
completely different use case so this is

00:16:01,519 --> 00:16:05,040
not going to give you any benefit you're

00:16:02,800 --> 00:16:07,360
going to have

00:16:05,040 --> 00:16:08,480
twice as much metadata and you're not

00:16:07,360 --> 00:16:11,120
going to see anything

00:16:08,480 --> 00:16:11,120
any benefit

00:16:11,680 --> 00:16:15,600
and if your image doesn't have any

00:16:14,240 --> 00:16:16,720
backing file maybe you don't see any

00:16:15,600 --> 00:16:20,959
speed up as i said

00:16:16,720 --> 00:16:23,600
qmu tries first to use flocate to

00:16:20,959 --> 00:16:24,959
allocate clusters efficiently so you

00:16:23,600 --> 00:16:26,399
have to try that first to see if it

00:16:24,959 --> 00:16:26,959
helps in your scenario but if you are

00:16:26,399 --> 00:16:28,399
using

00:16:26,959 --> 00:16:30,240
backing files then it will help in any

00:16:28,399 --> 00:16:32,639
case

00:16:30,240 --> 00:16:34,480
and then of course images created with

00:16:32,639 --> 00:16:35,680
this extended l2 entries are not going

00:16:34,480 --> 00:16:37,920
to be

00:16:35,680 --> 00:16:40,160
possible you are not going to be able to

00:16:37,920 --> 00:16:40,399
read them with other virtues of qmu and

00:16:40,160 --> 00:16:41,839
i

00:16:40,399 --> 00:16:43,839
don't expect that this feature can be

00:16:41,839 --> 00:16:47,440
backported easily so

00:16:43,839 --> 00:16:50,079
you will need the latest versions of qmu

00:16:47,440 --> 00:16:51,759
so how do i try this this is not

00:16:50,079 --> 00:16:54,800
available in camera yet it's not

00:16:51,759 --> 00:16:57,360
any release it will probably be

00:16:54,800 --> 00:16:58,720
available in game of 5.2 but the feature

00:16:57,360 --> 00:16:59,839
is complete and it's already in the

00:16:58,720 --> 00:17:01,519
repository and

00:16:59,839 --> 00:17:05,360
you can test it already so you just

00:17:01,519 --> 00:17:07,199
download the latest version from kit

00:17:05,360 --> 00:17:09,600
you compile it and you create an image

00:17:07,199 --> 00:17:12,559
with the option extended l2

00:17:09,600 --> 00:17:13,919
enabled and that's all you also probably

00:17:12,559 --> 00:17:15,600
want to have a

00:17:13,919 --> 00:17:17,360
larger cluster size the default cluster

00:17:15,600 --> 00:17:19,120
size is 64k but

00:17:17,360 --> 00:17:20,400
with this feature it makes sense to use

00:17:19,120 --> 00:17:23,679
larger clusters so

00:17:20,400 --> 00:17:26,480
be sure to try those but that's all that

00:17:23,679 --> 00:17:28,880
you need to do there's no nothing else

00:17:26,480 --> 00:17:30,080
again feedback back reports etc are very

00:17:28,880 --> 00:17:32,720
much appreciated this feature is

00:17:30,080 --> 00:17:33,760
complete but it's new so any testing

00:17:32,720 --> 00:17:36,000
anything that you

00:17:33,760 --> 00:17:37,280
try suggestions etc we will be happy to

00:17:36,000 --> 00:17:38,160
hear about them you can write to the

00:17:37,280 --> 00:17:40,799
mailing list

00:17:38,160 --> 00:17:42,160
or you can contact me directly and

00:17:40,799 --> 00:17:43,679
that's basically it

00:17:42,160 --> 00:17:45,360
i would also like to take the

00:17:43,679 --> 00:17:46,160
opportunity to thank outscale which is

00:17:45,360 --> 00:17:48,400
the company that is

00:17:46,160 --> 00:17:50,160
sponsoring all my work in qmu and in

00:17:48,400 --> 00:17:51,679
this feature in particular so everything

00:17:50,160 --> 00:17:54,160
that you are seeing here today is

00:17:51,679 --> 00:17:55,760
thanks to their sponsoring and this is

00:17:54,160 --> 00:17:57,200
all

00:17:55,760 --> 00:17:59,280
i hope that you enjoyed the presentation

00:17:57,200 --> 00:18:01,840
and i'm open to any questions

00:17:59,280 --> 00:18:01,840

YouTube URL: https://www.youtube.com/watch?v=clmbHp81GAc


