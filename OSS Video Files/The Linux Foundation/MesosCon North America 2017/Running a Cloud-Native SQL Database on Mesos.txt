Title: Running a Cloud-Native SQL Database on Mesos
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Running a Cloud-Native SQL Database on Mesos - Ben Darnell, Cockroach Labs

Compute management solutions like Mesos have rapidly advanced in recent years, but storage options have been left behind. Conventional SQL databases still require mountains of development and operational overhead to keep them functional as scale and availability requirements grow. What could be possible if a strongly-consistent database built specifically for cloud-native deployments paired with a powerful orchestration system like Mesos? 

This talk will look at how to deploy and manage CockroachDB, a scalable and strongly-consistent database, using sophisticated orchestration tools like Mesos. We will explore the ease with which CockroachDB was configured to run on Mesos using the new DC/OS SDK for stateful services, which saved days of writing code. We will then explore how you can deploy CockroachDB on Mesos to ship scalable, resilient applications on any cloud infrastructure.

About


Ben Darnell
CTO, Cockroach Labs
Captions: 
	00:00:00,650 --> 00:00:09,269
okay let's go ahead and get started my

00:00:06,180 --> 00:00:11,429
name is Ben Darnell I'm the CTO and

00:00:09,269 --> 00:00:12,509
co-founder of cockroach labs I'm here

00:00:11,429 --> 00:00:17,400
today to talk to you about running

00:00:12,509 --> 00:00:19,260
cockroach DB on DCOs so today I'm going

00:00:17,400 --> 00:00:20,970
to talk about this concept of a cloud

00:00:19,260 --> 00:00:22,859
native database what that means and why

00:00:20,970 --> 00:00:24,750
you what why you should be interested in

00:00:22,859 --> 00:00:26,550
in using one and then also why you

00:00:24,750 --> 00:00:28,529
should be looking at cockroach DB

00:00:26,550 --> 00:00:30,599
specifically as your cloud native

00:00:28,529 --> 00:00:32,610
database and why this kind of database

00:00:30,599 --> 00:00:35,239
is a natural fit for advanced

00:00:32,610 --> 00:00:38,010
orchestration platforms like like DCOs

00:00:35,239 --> 00:00:40,770
and then I'll be giving you a

00:00:38,010 --> 00:00:44,730
demonstration of the new DC OS package

00:00:40,770 --> 00:00:46,860
that we're announcing today so what is a

00:00:44,730 --> 00:00:48,780
cloud native database

00:00:46,860 --> 00:00:52,800
well cloud native we think of it as

00:00:48,780 --> 00:00:54,390
being a collection of features including

00:00:52,800 --> 00:00:56,010
horizontal scalability where you can

00:00:54,390 --> 00:00:56,570
just add to add new machines easily

00:00:56,010 --> 00:00:58,440
easily

00:00:56,570 --> 00:00:59,969
individual machines and the cluster

00:00:58,440 --> 00:01:01,649
aren't really special you don't have

00:00:59,969 --> 00:01:05,339
primaries and secondaries you know have

00:01:01,649 --> 00:01:08,189
different kinds of of replicas and the

00:01:05,339 --> 00:01:09,990
the entire system is is built to handle

00:01:08,189 --> 00:01:12,740
handle failures transparently and

00:01:09,990 --> 00:01:14,790
provide you with continuous availability

00:01:12,740 --> 00:01:15,630
so why is this important

00:01:14,790 --> 00:01:18,030
well because it helps your business

00:01:15,630 --> 00:01:19,950
adapt to change your database can grow

00:01:18,030 --> 00:01:23,549
and shrink based on based on demands

00:01:19,950 --> 00:01:24,810
both of storage and and query traffic it

00:01:23,549 --> 00:01:26,340
gives you a very easy and rapid

00:01:24,810 --> 00:01:27,869
development so that you're your

00:01:26,340 --> 00:01:29,729
developers can make make progress

00:01:27,869 --> 00:01:32,630
quickly and get get changes into

00:01:29,729 --> 00:01:35,490
production easily and the cluster can be

00:01:32,630 --> 00:01:37,530
self-organizing to reduce to balance

00:01:35,490 --> 00:01:39,119
load and reduce latency this is

00:01:37,530 --> 00:01:40,860
especially important in global

00:01:39,119 --> 00:01:42,329
businesses where you typically have

00:01:40,860 --> 00:01:45,600
traffic patterns where the load follows

00:01:42,329 --> 00:01:46,710
the Sun traditional databases have

00:01:45,600 --> 00:01:48,750
trouble with these with these

00:01:46,710 --> 00:01:50,070
characteristics traditional databases

00:01:48,750 --> 00:01:51,930
are more comfortable scale scaling

00:01:50,070 --> 00:01:52,979
vertically than horizontally

00:01:51,930 --> 00:01:54,689
they make you distinguish between

00:01:52,979 --> 00:01:56,939
primaries and secondaries and read-only

00:01:54,689 --> 00:01:59,189
replicas and on and on

00:01:56,939 --> 00:02:00,810
and frequently this varies depending on

00:01:59,189 --> 00:02:03,390
the database you have a manual and

00:02:00,810 --> 00:02:04,920
error-prone failure process often

00:02:03,390 --> 00:02:06,810
involving asynchronous replication that

00:02:04,920 --> 00:02:10,440
may that may lose data when there's a

00:02:06,810 --> 00:02:11,420
failover in cockroach DB on the other

00:02:10,440 --> 00:02:13,280
hand

00:02:11,420 --> 00:02:14,870
you can add nodes to the cluster at any

00:02:13,280 --> 00:02:17,330
time and the data automatically

00:02:14,870 --> 00:02:18,950
rebalances across them any node can

00:02:17,330 --> 00:02:20,780
serve any role there's no distinguished

00:02:18,950 --> 00:02:23,989
primary secondaries or anything like

00:02:20,780 --> 00:02:25,849
that and failover is automatic and

00:02:23,989 --> 00:02:32,270
everything is handled using consistent

00:02:25,849 --> 00:02:34,160
consensus based replication and so it's

00:02:32,270 --> 00:02:37,250
so a cloud native database is a is a

00:02:34,160 --> 00:02:39,260
good fit for for an advanced

00:02:37,250 --> 00:02:41,720
orchestration platform like like DCOs

00:02:39,260 --> 00:02:43,610
because it provides a lot of a lot of

00:02:41,720 --> 00:02:45,410
functionality that that you need to

00:02:43,610 --> 00:02:49,250
provide this kind of this level of

00:02:45,410 --> 00:02:52,190
service so the first of all DCOs like a

00:02:49,250 --> 00:02:54,260
lot of a lot of container platforms

00:02:52,190 --> 00:02:57,230
provides elastic allocation and

00:02:54,260 --> 00:02:59,480
scheduling so it can you can say I need

00:02:57,230 --> 00:03:02,269
I need ten copies of this of this

00:02:59,480 --> 00:03:05,690
process and it will it'll find space on

00:03:02,269 --> 00:03:08,180
your on your hardware to run that it

00:03:05,690 --> 00:03:10,760
provides service discovery Network

00:03:08,180 --> 00:03:11,959
virtual IP addresses load balancing that

00:03:10,760 --> 00:03:15,140
sort of thing for managing communication

00:03:11,959 --> 00:03:17,120
between between both within the database

00:03:15,140 --> 00:03:19,160
and the database and your application

00:03:17,120 --> 00:03:23,450
and then one of the most interesting

00:03:19,160 --> 00:03:25,070
things that DCOs provides now is what we

00:03:23,450 --> 00:03:28,400
think of as weakly persistent local

00:03:25,070 --> 00:03:30,590
storage which solves a dilemma that that

00:03:28,400 --> 00:03:34,700
you tend to face in container based

00:03:30,590 --> 00:03:36,140
deployments of databases in in sort of

00:03:34,700 --> 00:03:38,450
the first wave of container based

00:03:36,140 --> 00:03:39,739
deployments you have you have a couple

00:03:38,450 --> 00:03:43,400
of different options for how to manage

00:03:39,739 --> 00:03:46,400
your your actual data one way is to just

00:03:43,400 --> 00:03:48,620
use a raid array or some sort of network

00:03:46,400 --> 00:03:51,530
attached storage that will store your

00:03:48,620 --> 00:03:54,350
data persistently and and reliably and

00:03:51,530 --> 00:03:58,040
redundantly but this is this is

00:03:54,350 --> 00:03:59,980
expensive it's you can you know spend

00:03:58,040 --> 00:04:01,280
spend tons of money on a

00:03:59,980 --> 00:04:03,709
enterprise-grade

00:04:01,280 --> 00:04:05,780
network attached storage box but you

00:04:03,709 --> 00:04:07,870
don't really need that when cockroach DB

00:04:05,780 --> 00:04:10,579
provides its own redundancy internally

00:04:07,870 --> 00:04:11,959
on the opposite extreme container

00:04:10,579 --> 00:04:13,700
platforms will often provide you with

00:04:11,959 --> 00:04:15,350
just ephemeral disk that gets wiped

00:04:13,700 --> 00:04:16,010
whenever your whenever your job gets

00:04:15,350 --> 00:04:17,989
rescheduled

00:04:16,010 --> 00:04:20,450
but that's no good for a database either

00:04:17,989 --> 00:04:21,890
because if you if you lose all your if

00:04:20,450 --> 00:04:24,410
you lose too many nodes at once then you

00:04:21,890 --> 00:04:29,120
may lose all the other replicas of your

00:04:24,410 --> 00:04:30,740
and so using the new DCOs SDK we can

00:04:29,120 --> 00:04:33,650
take more control of the scheduling

00:04:30,740 --> 00:04:35,780
process and give you give you more

00:04:33,650 --> 00:04:38,150
stronger degree of association with the

00:04:35,780 --> 00:04:40,640
data that's on your nodes local discs so

00:04:38,150 --> 00:04:43,850
that you can have a good degree of Rai

00:04:40,640 --> 00:04:44,900
ability with with data that's not that

00:04:43,850 --> 00:04:50,420
were you're not paying for multiple

00:04:44,900 --> 00:04:51,890
levels of redundancy and so today here

00:04:50,420 --> 00:04:56,150
at this conference we're announcing the

00:04:51,890 --> 00:04:57,890
release of our of our DCOs package which

00:04:56,150 --> 00:05:00,050
you can install with a single command

00:04:57,890 --> 00:05:02,480
DCOs packard package install cockroach

00:05:00,050 --> 00:05:06,230
DB and this will start up a three node

00:05:02,480 --> 00:05:08,120
cluster for you and this you can change

00:05:06,230 --> 00:05:10,460
this change this later by going going

00:05:08,120 --> 00:05:12,140
back into the DCOs interface and

00:05:10,460 --> 00:05:14,330
changing the changing the node count

00:05:12,140 --> 00:05:16,130
variable you can also do all of this

00:05:14,330 --> 00:05:17,750
both through the through the command

00:05:16,130 --> 00:05:24,860
line or or through the web-based

00:05:17,750 --> 00:05:26,750
interface alright so before we go into

00:05:24,860 --> 00:05:27,860
demonstrating this package let me tell

00:05:26,750 --> 00:05:29,870
you a little bit more about about

00:05:27,860 --> 00:05:32,090
cockroach DB and and what it can do for

00:05:29,870 --> 00:05:34,100
you now cartridge DB is an open-source

00:05:32,090 --> 00:05:37,400
sequel database for global cloud

00:05:34,100 --> 00:05:40,010
services so the key things that it that

00:05:37,400 --> 00:05:42,140
provides our distributed sequel so you

00:05:40,010 --> 00:05:44,180
can use it use the sequel a sequel

00:05:42,140 --> 00:05:47,570
language and your existing existing

00:05:44,180 --> 00:05:50,840
tools across across a large pool of

00:05:47,570 --> 00:05:53,420
resources and provides data integrity at

00:05:50,840 --> 00:05:54,770
global scale which means high

00:05:53,420 --> 00:05:57,350
availability or multi active

00:05:54,770 --> 00:05:59,000
availability as we call it and the

00:05:57,350 --> 00:06:01,010
entire system is built on a system of

00:05:59,000 --> 00:06:04,400
consistent transactions and consistent

00:06:01,010 --> 00:06:07,160
replication so distributed sequel makes

00:06:04,400 --> 00:06:09,110
it possible for cockroach DB as you as a

00:06:07,160 --> 00:06:11,690
database to grow grow along with your

00:06:09,110 --> 00:06:14,480
application this is full full-fledged

00:06:11,690 --> 00:06:17,900
sequel with acid semantics and indexing

00:06:14,480 --> 00:06:20,540
joins the whole deal and it runs across

00:06:17,900 --> 00:06:22,160
across multiple database servers and so

00:06:20,540 --> 00:06:24,170
this lets you take advantage of both

00:06:22,160 --> 00:06:26,000
distributed storage across your across

00:06:24,170 --> 00:06:28,460
your pool of resources and also the

00:06:26,000 --> 00:06:31,730
distributed computational resources on

00:06:28,460 --> 00:06:33,530
these nodes so in cockroach DB scaling

00:06:31,730 --> 00:06:36,080
is always transparent your tables can

00:06:33,530 --> 00:06:37,599
grow to any size there's no there's no

00:06:36,080 --> 00:06:40,430
manual partitioning or

00:06:37,599 --> 00:06:43,009
or configuration needed as your as your

00:06:40,430 --> 00:06:44,750
data grows you can just add add more

00:06:43,009 --> 00:06:46,870
machines at any time and the data

00:06:44,750 --> 00:06:48,919
rebalances automatically across the

00:06:46,870 --> 00:06:50,330
across the pool within within

00:06:48,919 --> 00:06:53,120
constraints that you can configure if

00:06:50,330 --> 00:06:54,860
you if you need to also support

00:06:53,120 --> 00:06:56,930
distributed execution of your sequel

00:06:54,860 --> 00:06:58,879
queries so when you when you do an

00:06:56,930 --> 00:07:01,490
aggregate query like select sum of

00:06:58,879 --> 00:07:03,229
duration from sessions this query is

00:07:01,490 --> 00:07:04,729
actually going to be farmed out to all

00:07:03,229 --> 00:07:06,830
of the nodes in the cluster that contain

00:07:04,729 --> 00:07:08,629
data for the sessions table and it will

00:07:06,830 --> 00:07:10,729
each of those nodes will compute their

00:07:08,629 --> 00:07:12,530
partial sum and then send the same those

00:07:10,729 --> 00:07:14,960
enum intermediate results back up to the

00:07:12,530 --> 00:07:18,680
back up to the Gateway node for for the

00:07:14,960 --> 00:07:20,060
final final sum and so this gives you it

00:07:18,680 --> 00:07:22,310
gives you a very efficient way to

00:07:20,060 --> 00:07:24,500
operate on large amounts of data

00:07:22,310 --> 00:07:28,580
spanning spanning large numbers of

00:07:24,500 --> 00:07:31,130
machines so DCOs

00:07:28,580 --> 00:07:33,289
gives us flexible scheduling for these

00:07:31,130 --> 00:07:35,650
for these different database nodes and

00:07:33,289 --> 00:07:38,539
so you can you can schedule your

00:07:35,650 --> 00:07:41,539
cockroach DB nodes near your application

00:07:38,539 --> 00:07:43,490
servers for for lower latency you can

00:07:41,539 --> 00:07:45,289
even schedule them on your application

00:07:43,490 --> 00:07:47,150
servers if you don't you know if you end

00:07:45,289 --> 00:07:49,250
up with unused disk capacity then you

00:07:47,150 --> 00:07:50,960
can just start up a cockroach node to

00:07:49,250 --> 00:07:54,289
kind of make that make that disk space

00:07:50,960 --> 00:07:58,250
available to as a part of your database

00:07:54,289 --> 00:07:59,509
to the rest of your cluster and you also

00:07:58,250 --> 00:08:01,310
have a lot of flexibility in terms of

00:07:59,509 --> 00:08:04,370
where you place these these resources

00:08:01,310 --> 00:08:06,380
you can you can span your expand your

00:08:04,370 --> 00:08:08,029
cluster across the globe you can if you

00:08:06,380 --> 00:08:09,919
have at least at least three data

00:08:08,029 --> 00:08:12,080
centers then you can survive the outage

00:08:09,919 --> 00:08:14,779
of any of any one data center you can

00:08:12,080 --> 00:08:17,150
also survive the loss of loss of

00:08:14,779 --> 00:08:18,860
machines within within these data

00:08:17,150 --> 00:08:20,389
centers so this gives you the

00:08:18,860 --> 00:08:23,150
flexibility to put your put your data

00:08:20,389 --> 00:08:28,069
close to your customers and and provide

00:08:23,150 --> 00:08:30,139
them with with low latency access so

00:08:28,069 --> 00:08:33,890
talked about I mentioned multi active

00:08:30,139 --> 00:08:36,110
availability already this is our our way

00:08:33,890 --> 00:08:39,320
to the way that we provide to survive

00:08:36,110 --> 00:08:41,120
survive disasters and this is sort of a

00:08:39,320 --> 00:08:43,820
new term so explain what I what I mean

00:08:41,120 --> 00:08:46,399
by that you can think of disaster

00:08:43,820 --> 00:08:48,220
recovery originally as being about about

00:08:46,399 --> 00:08:50,270
backup and restore so you would have a

00:08:48,220 --> 00:08:51,220
have a database and a bunch of services

00:08:50,270 --> 00:08:53,019
talking to it

00:08:51,220 --> 00:08:55,120
and then you would make a make a backup

00:08:53,019 --> 00:08:58,329
and store that remotely so that in the

00:08:55,120 --> 00:09:00,069
event of a data center outage you'd have

00:08:58,329 --> 00:09:01,360
something that you could you used to get

00:09:00,069 --> 00:09:03,339
back online and recover

00:09:01,360 --> 00:09:05,019
of course this that this was pretty

00:09:03,339 --> 00:09:08,019
painful because it was a very manual

00:09:05,019 --> 00:09:10,540
process and and took a long time and so

00:09:08,019 --> 00:09:13,000
in the event of an outage you would you

00:09:10,540 --> 00:09:16,930
would have a lot of a lot of downtime so

00:09:13,000 --> 00:09:20,439
this is evolved and we move towards a

00:09:16,930 --> 00:09:24,959
towards a more efficient failover model

00:09:20,439 --> 00:09:28,180
where you would have a hot spare

00:09:24,959 --> 00:09:29,829
database so you'd have a priority and a

00:09:28,180 --> 00:09:34,060
secondary with asynchronous replication

00:09:29,829 --> 00:09:36,040
between them which means that you really

00:09:34,060 --> 00:09:38,050
have much less much less downtime when

00:09:36,040 --> 00:09:40,360
you're when there's a failover but

00:09:38,050 --> 00:09:42,009
you've got it's fairly expensive because

00:09:40,360 --> 00:09:43,660
you've got this huge pool of resources

00:09:42,009 --> 00:09:46,600
in your secondary data center that's

00:09:43,660 --> 00:09:49,120
just sitting idle most of the time and

00:09:46,600 --> 00:09:51,910
so then the next step in this in this

00:09:49,120 --> 00:09:54,699
evolution is is active active where both

00:09:51,910 --> 00:09:57,209
both sides of the of the link are our

00:09:54,699 --> 00:09:59,199
serving traffic at the same time this

00:09:57,209 --> 00:10:01,540
doesn't actually get you a huge amount

00:09:59,199 --> 00:10:03,850
more efficiency because even though

00:10:01,540 --> 00:10:06,250
neither side of the of the connection is

00:10:03,850 --> 00:10:08,680
is sitting completely idle you don't

00:10:06,250 --> 00:10:12,009
want to work work this cluster too hard

00:10:08,680 --> 00:10:14,889
because it will if you are using both

00:10:12,009 --> 00:10:16,629
sides at more than 50% then one one goes

00:10:14,889 --> 00:10:18,250
down and you're you've exceeded the

00:10:16,629 --> 00:10:20,139
capacity on your remaining data center

00:10:18,250 --> 00:10:22,089
and so you still have to keep keep

00:10:20,139 --> 00:10:26,079
utilization on these on these clusters

00:10:22,089 --> 00:10:29,110
low there's also a problem that this

00:10:26,079 --> 00:10:30,910
replication can either be consistent can

00:10:29,110 --> 00:10:34,120
either be synchronous or asynchronous so

00:10:30,910 --> 00:10:35,649
consensus consistent replication is is

00:10:34,120 --> 00:10:38,410
going to be going to be synchronous it's

00:10:35,649 --> 00:10:39,850
going to impact your your latency and

00:10:38,410 --> 00:10:41,980
it's also going to hurt availability

00:10:39,850 --> 00:10:44,199
because when the second data center goes

00:10:41,980 --> 00:10:46,540
down the first one actually gets gets

00:10:44,199 --> 00:10:48,579
blocked because it can't commit its its

00:10:46,540 --> 00:10:50,680
two-phase transactions across across

00:10:48,579 --> 00:10:53,290
both replicas if you use async

00:10:50,680 --> 00:10:55,149
replication on the other hand then then

00:10:53,290 --> 00:10:57,879
you've given up on consistency of your

00:10:55,149 --> 00:10:59,769
of your data and there's there's always

00:10:57,879 --> 00:11:02,769
an opportunity for conflicts between the

00:10:59,769 --> 00:11:04,389
between the two data centers and so the

00:11:02,769 --> 00:11:04,810
cockroach DB model which we call multi

00:11:04,389 --> 00:11:07,150
active

00:11:04,810 --> 00:11:09,250
availability is an extension of active

00:11:07,150 --> 00:11:11,860
active to more than two more than tuna

00:11:09,250 --> 00:11:13,210
took more than two data centers and this

00:11:11,860 --> 00:11:15,370
sounds like a small thing but it's

00:11:13,210 --> 00:11:15,880
really it really kind is kind of a game

00:11:15,370 --> 00:11:17,730
changer

00:11:15,880 --> 00:11:20,410
because it makes it possible to use

00:11:17,730 --> 00:11:22,750
consensus based replication instead of

00:11:20,410 --> 00:11:25,540
being stuck in the in the asynchronous

00:11:22,750 --> 00:11:27,220
log for reading model or the or the

00:11:25,540 --> 00:11:31,600
synchronous two-phase commit model and

00:11:27,220 --> 00:11:33,779
so in this mode you have you have at

00:11:31,600 --> 00:11:36,430
least three replicas of everything and

00:11:33,779 --> 00:11:38,589
whenever you go to a whenever you go to

00:11:36,430 --> 00:11:40,240
commit a change that goes to that it

00:11:38,589 --> 00:11:42,370
gets broadcast to all of the replicas

00:11:40,240 --> 00:11:44,500
and two out of three or three out of

00:11:42,370 --> 00:11:46,630
five need to need to acknowledge the

00:11:44,500 --> 00:11:48,190
change in order for it to to be

00:11:46,630 --> 00:11:50,350
considered committed and so you don't

00:11:48,190 --> 00:11:52,779
need to rely on every on every data

00:11:50,350 --> 00:11:54,130
center or every replica beam up but you

00:11:52,779 --> 00:11:56,890
just need you just need two out of the

00:11:54,130 --> 00:11:59,080
three so that you have the that the

00:11:56,890 --> 00:12:00,580
better the better of the two latency

00:11:59,080 --> 00:12:03,060
wise and you still have high

00:12:00,580 --> 00:12:06,520
availability and the ability to tolerate

00:12:03,060 --> 00:12:08,560
data center failures and the same the

00:12:06,520 --> 00:12:10,150
same pattern plays out on a smaller

00:12:08,560 --> 00:12:12,460
scale even if you're in a single data

00:12:10,150 --> 00:12:15,310
center because everything is replicated

00:12:12,460 --> 00:12:17,860
three ways and and can survive the loss

00:12:15,310 --> 00:12:20,830
of individual machines in the data

00:12:17,860 --> 00:12:22,870
center whenever whenever a machine goes

00:12:20,830 --> 00:12:26,230
down that kicks off an automatic repair

00:12:22,870 --> 00:12:28,270
process that will go and and rear epic

00:12:26,230 --> 00:12:32,980
eight that nodes data onto on to the

00:12:28,270 --> 00:12:36,360
remaining nodes and so on top of all

00:12:32,980 --> 00:12:39,760
this and on top of this I we available

00:12:36,360 --> 00:12:42,940
because replication we also support

00:12:39,760 --> 00:12:43,390
distributed transactions these are full

00:12:42,940 --> 00:12:45,820
fledged

00:12:43,390 --> 00:12:48,640
acid transactions that give you all of

00:12:45,820 --> 00:12:50,800
the other guarantees you expect from a

00:12:48,640 --> 00:12:52,390
relational database and like I said it's

00:12:50,800 --> 00:12:54,940
built on this consensus replication

00:12:52,390 --> 00:12:59,020
model using the raft consensus algorithm

00:12:54,940 --> 00:13:01,660
so all changes go to a majority of of

00:12:59,020 --> 00:13:03,610
the replicas and you can never you can

00:13:01,660 --> 00:13:06,100
never lose committed data as long as you

00:13:03,610 --> 00:13:09,640
as long as you don't lose a half of your

00:13:06,100 --> 00:13:11,709
half of your replicas we have

00:13:09,640 --> 00:13:12,850
distributed transactions which can span

00:13:11,709 --> 00:13:14,620
rosed

00:13:12,850 --> 00:13:17,140
tables even databases there's no

00:13:14,620 --> 00:13:19,290
restrictions on what can go into a into

00:13:17,140 --> 00:13:21,690
a transaction in cockroach

00:13:19,290 --> 00:13:24,840
there is a limitation on the size of a

00:13:21,690 --> 00:13:27,210
right transaction but you can there's no

00:13:24,840 --> 00:13:30,240
limitation on what what tables or other

00:13:27,210 --> 00:13:31,860
objects can be can be included in the in

00:13:30,240 --> 00:13:34,620
the transaction and if you're familiar

00:13:31,860 --> 00:13:36,060
with the concept of sequel isolation

00:13:34,620 --> 00:13:38,100
levels you know that databases give you

00:13:36,060 --> 00:13:39,720
a give you a number of different

00:13:38,100 --> 00:13:41,550
configurable settings that you can use

00:13:39,720 --> 00:13:44,070
for the isolation of your transaction

00:13:41,550 --> 00:13:46,080
cockroach defaults to serializability

00:13:44,070 --> 00:13:48,960
which is the highest of the for standard

00:13:46,080 --> 00:13:50,550
sequel isolation levels because we we

00:13:48,960 --> 00:13:53,430
think it's important that your database

00:13:50,550 --> 00:13:54,600
provide provide the maximum amount of

00:13:53,430 --> 00:13:56,460
consistency because it turns out that

00:13:54,600 --> 00:13:57,570
actually trying to think about all the

00:13:56,460 --> 00:14:00,600
different ways that things can go wrong

00:13:57,570 --> 00:14:02,940
in lesser isolation levels is really is

00:14:00,600 --> 00:14:04,920
really difficult and we don't think

00:14:02,940 --> 00:14:10,140
that's a good a good trade-off for for

00:14:04,920 --> 00:14:14,160
developers to have to make and so now

00:14:10,140 --> 00:14:19,560
going to give you a demo as long as the

00:14:14,160 --> 00:14:21,720
Wi-Fi is cooperating so this is a brand

00:14:19,560 --> 00:14:25,890
new DCOs cluster that I set up this

00:14:21,720 --> 00:14:29,070
morning on AWS using the default

00:14:25,890 --> 00:14:32,270
installation instructions and so I can

00:14:29,070 --> 00:14:38,430
go into the the package catalog and find

00:14:32,270 --> 00:14:40,740
cockroach DB and click deploy and so

00:14:38,430 --> 00:14:44,220
it's a it's a one-click process this is

00:14:40,740 --> 00:14:46,320
going to start up five tasks in Indi cos

00:14:44,220 --> 00:14:48,540
the first task which is starting up

00:14:46,320 --> 00:14:52,410
right now is the scheduler so this is

00:14:48,540 --> 00:14:54,870
this is using the DC OS SDK instead of

00:14:52,410 --> 00:14:58,080
instead of Marathon and so it takes on

00:14:54,870 --> 00:15:00,420
some of the scheduling work itself and

00:14:58,080 --> 00:15:04,640
so this is this is the first task that

00:15:00,420 --> 00:15:07,200
gets get started up there's very little

00:15:04,640 --> 00:15:09,570
very little custom code in this package

00:15:07,200 --> 00:15:11,400
and actually in cockroach DB 1.1 which

00:15:09,570 --> 00:15:13,830
is coming up pretty soon even that

00:15:11,400 --> 00:15:15,480
custom code is going away okay and so

00:15:13,830 --> 00:15:18,570
now we can see that the other the other

00:15:15,480 --> 00:15:21,360
four tasks have started up and so what's

00:15:18,570 --> 00:15:23,730
running here is the scheduler a metric

00:15:21,360 --> 00:15:26,790
server which translates cockroaches

00:15:23,730 --> 00:15:28,950
exported metrics from the Prometheus

00:15:26,790 --> 00:15:31,200
format which we which we've supported as

00:15:28,950 --> 00:15:32,200
our first monitoring integration to the

00:15:31,200 --> 00:15:35,140
stats D

00:15:32,200 --> 00:15:37,330
service that that DCOs provides forest

00:15:35,140 --> 00:15:41,230
and then we've started up three 3d

00:15:37,330 --> 00:15:44,230
cockroach nodes and we can actually go

00:15:41,230 --> 00:15:48,250
over to another tab and see this is the

00:15:44,230 --> 00:15:51,040
is the built in admin UI on one of these

00:15:48,250 --> 00:15:52,540
nodes but even though if you can see the

00:15:51,040 --> 00:15:54,580
address bar this it says localhost

00:15:52,540 --> 00:15:58,630
that's this is actually running in that

00:15:54,580 --> 00:16:02,830
DCOs cluster I just set up a SSH port

00:15:58,630 --> 00:16:04,870
forward as a as a shortcut there so a

00:16:02,830 --> 00:16:07,030
database that's not doing anything is

00:16:04,870 --> 00:16:11,230
not very interesting and so I'm going to

00:16:07,030 --> 00:16:13,390
launch another another application just

00:16:11,230 --> 00:16:16,980
to put some put some load on it so this

00:16:13,390 --> 00:16:18,550
is a simple JSON file specifying a

00:16:16,980 --> 00:16:21,490
docker container

00:16:18,550 --> 00:16:23,770
this is just a docker container that

00:16:21,490 --> 00:16:25,540
we've we built for purposes of just

00:16:23,770 --> 00:16:30,700
generating load on a on a cockroach

00:16:25,540 --> 00:16:32,050
cluster and so it's going to going to

00:16:30,700 --> 00:16:34,540
start up I started the cluster with that

00:16:32,050 --> 00:16:39,430
parameter pointing to the pointing to

00:16:34,540 --> 00:16:45,370
the first node in the in the cluster and

00:16:39,430 --> 00:16:49,270
with one one command that is started and

00:16:45,370 --> 00:16:51,760
we should be able to watch in just a

00:16:49,270 --> 00:17:01,890
minute and see the see the sequel query

00:16:51,760 --> 00:17:03,910
graph spike up and there it goes so this

00:17:01,890 --> 00:17:05,890
I want to call your attention to this

00:17:03,910 --> 00:17:10,740
the third graph on this page the

00:17:05,890 --> 00:17:14,470
replicas per node so we see here that

00:17:10,740 --> 00:17:17,860
the so all the data in cockroach DB is

00:17:14,470 --> 00:17:20,050
broken up in two ranges each range is is

00:17:17,860 --> 00:17:23,860
a contiguous chunk of data in the

00:17:20,050 --> 00:17:26,800
database usually 64 Meg's by default and

00:17:23,860 --> 00:17:30,070
so we can see here that that the the

00:17:26,800 --> 00:17:35,860
number of replicas on each on each node

00:17:30,070 --> 00:17:37,900
is is the same and as the as the the

00:17:35,860 --> 00:17:40,380
load generator is running we're seeing

00:17:37,900 --> 00:17:43,830
that this this number is is increasing

00:17:40,380 --> 00:17:47,039
over time as new as new

00:17:43,830 --> 00:17:50,370
Rangers get get split off from the from

00:17:47,039 --> 00:17:52,679
the data as it grows we can see see some

00:17:50,370 --> 00:17:56,130
other other variables here we can see

00:17:52,679 --> 00:17:59,580
the the rate at which the the data is is

00:17:56,130 --> 00:18:02,580
growing but let's go back to here to see

00:17:59,580 --> 00:18:07,260
the replicas per node and I'm going to

00:18:02,580 --> 00:18:10,260
go to the DCOs web interface on my

00:18:07,260 --> 00:18:13,529
cockroach DB service and I'm going to go

00:18:10,260 --> 00:18:16,260
in here and edit it so here in the

00:18:13,529 --> 00:18:18,299
environment tab we have a bunch of

00:18:16,260 --> 00:18:22,130
variables including node count so I'm

00:18:18,299 --> 00:18:22,130
going to change this from three to five

00:18:22,940 --> 00:18:32,399
okay and so now we'll see two more tasks

00:18:27,269 --> 00:18:35,929
showing up on this on this list once

00:18:32,399 --> 00:18:35,929
they get once they get started up

00:18:46,809 --> 00:18:56,799
and we're just just waiting a minute for

00:18:52,809 --> 00:18:56,799
for those tasks and start up

00:19:08,809 --> 00:19:15,890
and here goes so here's there's the

00:19:12,480 --> 00:19:15,890
first of the two of the two new nodes

00:19:21,860 --> 00:19:29,049
and we should be able to see here in

00:19:24,019 --> 00:19:32,559
just a minute that a new node appears in

00:19:29,049 --> 00:19:32,559
in this graph

00:19:45,940 --> 00:19:51,700
all right so it didn't didn't refresh

00:19:48,100 --> 00:19:53,110
automatically but I had to manually

00:19:51,700 --> 00:19:55,390
refresh it to get it to show up and so

00:19:53,110 --> 00:19:58,510
now you can see that the the replicas

00:19:55,390 --> 00:20:02,500
per node is going down because the new

00:19:58,510 --> 00:20:05,080
node is available and it's and it's

00:20:02,500 --> 00:20:06,760
rebalancing on to these other nodes and

00:20:05,080 --> 00:20:16,290
the sequel queries is also going down

00:20:06,760 --> 00:20:16,290
that's not good the joys of live demos I

00:20:19,980 --> 00:20:25,480
think this is actually a yeah this is a

00:20:22,810 --> 00:20:27,580
this is a UI bug it's it's miss

00:20:25,480 --> 00:20:29,590
rendering the glass data point as a zero

00:20:27,580 --> 00:20:31,870
but anyway you can see here that over

00:20:29,590 --> 00:20:35,470
the course of a couple of data points

00:20:31,870 --> 00:20:37,030
that the that the number of the there

00:20:35,470 --> 00:20:38,290
were twelve replicas per node when there

00:20:37,030 --> 00:20:41,830
were three nodes and then once it got up

00:20:38,290 --> 00:20:44,440
to what once the additional node started

00:20:41,830 --> 00:20:46,420
up that that the other other replicas

00:20:44,440 --> 00:20:47,830
got got rebalanced on to the under the

00:20:46,420 --> 00:20:53,440
new nodes and the number of replicas per

00:20:47,830 --> 00:20:56,350
node is is going down so that's that's

00:20:53,440 --> 00:20:59,620
an example of of how easy it is to run

00:20:56,350 --> 00:21:05,200
run cockroach on on DCOs

00:20:59,620 --> 00:21:07,810
and so now back to the the presentation

00:21:05,200 --> 00:21:11,860
so i'm gonna tell you about the the

00:21:07,810 --> 00:21:13,180
current status of cockroach DB current

00:21:11,860 --> 00:21:16,960
version which was just released

00:21:13,180 --> 00:21:19,110
yesterday is 1.0 point six our first

00:21:16,960 --> 00:21:23,230
production ready release was in May of

00:21:19,110 --> 00:21:24,850
May of this year so we've been doing six

00:21:23,230 --> 00:21:28,290
this is our six that patch released

00:21:24,850 --> 00:21:30,010
since then and our 1.0 version was

00:21:28,290 --> 00:21:32,320
provided all the all the core benefits

00:21:30,010 --> 00:21:35,920
of cockroach DB the distributed sequel

00:21:32,320 --> 00:21:37,900
multi active availability it also comes

00:21:35,920 --> 00:21:40,570
in both both an open source and an

00:21:37,900 --> 00:21:43,540
Enterprise Edition and in the Enterprise

00:21:40,570 --> 00:21:45,190
Edition the first the first feature we

00:21:43,540 --> 00:21:47,650
have there is distributed and

00:21:45,190 --> 00:21:50,080
incremental backup and restore we do

00:21:47,650 --> 00:21:50,950
have a backup option for the for the

00:21:50,080 --> 00:21:54,390
open source edition

00:21:50,950 --> 00:21:57,760
using kind of a sequel dump format which

00:21:54,390 --> 00:21:58,960
produces a file of file full insert

00:21:57,760 --> 00:21:59,860
statements that can be used to recreate

00:21:58,960 --> 00:22:03,190
your two

00:21:59,860 --> 00:22:04,780
so you do have a backup option in the in

00:22:03,190 --> 00:22:08,590
the free edition but the Enterprise

00:22:04,780 --> 00:22:10,570
Edition has a has a implementation of

00:22:08,590 --> 00:22:15,340
backup and restore that is much faster

00:22:10,570 --> 00:22:17,890
both we back up and restore sides and

00:22:15,340 --> 00:22:22,720
then very soon probably within a month

00:22:17,890 --> 00:22:24,580
we'll have version 1.1 the key the key

00:22:22,720 --> 00:22:26,440
theme of this of this next release is

00:22:24,580 --> 00:22:29,020
what we're calling it rugged ization

00:22:26,440 --> 00:22:32,290
which is just trying to make make the

00:22:29,020 --> 00:22:35,680
database more more robust in in

00:22:32,290 --> 00:22:37,600
production giving database operators the

00:22:35,680 --> 00:22:39,700
tools that they need to inspect the

00:22:37,600 --> 00:22:42,880
inspect the cluster see see what's

00:22:39,700 --> 00:22:46,230
running what what queries are taking a

00:22:42,880 --> 00:22:48,600
long time how to how to adjust adjust

00:22:46,230 --> 00:22:50,950
queries to improve their performance and

00:22:48,600 --> 00:22:55,000
cancel long running queries and things

00:22:50,950 --> 00:22:57,730
like that and of course all every

00:22:55,000 --> 00:22:59,530
cockroach in DB release includes a you

00:22:57,730 --> 00:23:02,080
know ongoing work on performance and

00:22:59,530 --> 00:23:05,770
sequel feature coverage and bug fixing

00:23:02,080 --> 00:23:08,050
all of the all the usual things so most

00:23:05,770 --> 00:23:11,050
of that most of the new features in 1.1

00:23:08,050 --> 00:23:13,420
are related to operational things there

00:23:11,050 --> 00:23:15,520
are more administrative tools than so

00:23:13,420 --> 00:23:17,170
it's sort of high profile features but

00:23:15,520 --> 00:23:21,970
we do have one one big one which is a

00:23:17,170 --> 00:23:25,030
fast CSV importer which uses uses the

00:23:21,970 --> 00:23:26,380
same framework as the enterprise as the

00:23:25,030 --> 00:23:29,020
enterprise backup and restore

00:23:26,380 --> 00:23:31,150
functionality but we're making this

00:23:29,020 --> 00:23:33,850
available in the indie free version of

00:23:31,150 --> 00:23:36,040
the of the product because we know that

00:23:33,850 --> 00:23:38,590
it's important for everyone to be able

00:23:36,040 --> 00:23:41,590
to get get their data into cockroach DB

00:23:38,590 --> 00:23:45,640
to be able to even start start trying it

00:23:41,590 --> 00:23:48,520
out in the in the longer term

00:23:45,640 --> 00:23:52,090
this is maintenance that are slated for

00:23:48,520 --> 00:23:55,360
our next release which were slated for

00:23:52,090 --> 00:23:57,910
next next spring of course the

00:23:55,360 --> 00:24:02,200
ever-present performance and sequel

00:23:57,910 --> 00:24:04,060
feature categories are top one of the

00:24:02,200 --> 00:24:05,590
but when our top requested sequel

00:24:04,060 --> 00:24:09,040
feature is going to be coming in this

00:24:05,590 --> 00:24:11,770
next next release which is JSON column

00:24:09,040 --> 00:24:13,450
types inspired by the the Postgres JSON

00:24:11,770 --> 00:24:15,249
B format

00:24:13,450 --> 00:24:18,639
we're also building change data capture

00:24:15,249 --> 00:24:21,879
so that you can get a table or database

00:24:18,639 --> 00:24:24,789
level log of all changes emitted out to

00:24:21,879 --> 00:24:28,179
to Kafka so that you can stream those

00:24:24,789 --> 00:24:32,830
that stream that data into other into

00:24:28,179 --> 00:24:34,989
other data storage systems for later

00:24:32,830 --> 00:24:36,999
analysis and we're working on on

00:24:34,989 --> 00:24:39,700
improving our support for global data

00:24:36,999 --> 00:24:42,609
architecture and and especially giving

00:24:39,700 --> 00:24:44,589
it giving you better tools for for

00:24:42,609 --> 00:24:46,929
managing a cluster spread out over a

00:24:44,589 --> 00:24:50,079
large large area with high latency

00:24:46,929 --> 00:24:51,849
network links part of this is going to

00:24:50,079 --> 00:24:54,789
be in the form of a new enterprise

00:24:51,849 --> 00:24:57,789
feature for row level data partitioning

00:24:54,789 --> 00:25:00,159
which gives an admin control over the

00:24:57,789 --> 00:25:02,049
over data placement at a a sub table

00:25:00,159 --> 00:25:03,849
level as opposed to the table

00:25:02,049 --> 00:25:10,529
granularity controls that you that you

00:25:03,849 --> 00:25:12,729
get in the open source edition so that's

00:25:10,529 --> 00:25:15,070
that that's pretty much pretty much it

00:25:12,729 --> 00:25:17,499
so here are a bunch of links for where

00:25:15,070 --> 00:25:20,469
you can find out more about us our main

00:25:17,499 --> 00:25:21,909
website is cockroach labs comm all of

00:25:20,469 --> 00:25:24,309
our source code is on github at

00:25:21,909 --> 00:25:26,409
cockroach DB slash cockroach if you want

00:25:24,309 --> 00:25:29,109
instructions for running cockroach on

00:25:26,409 --> 00:25:31,089
DCOs the best document for that is

00:25:29,109 --> 00:25:33,999
currently in the on github.com slash

00:25:31,089 --> 00:25:37,419
DCOs slash examples there's a link on

00:25:33,999 --> 00:25:40,809
that page for for cockroach DB and we're

00:25:37,419 --> 00:25:42,579
also active on the git er chat system so

00:25:40,809 --> 00:25:45,219
if you want to come chat with us in real

00:25:42,579 --> 00:25:46,929
time then that would be the be the best

00:25:45,219 --> 00:25:49,419
place to do it

00:25:46,929 --> 00:25:56,609
and that would be happy to answer any

00:25:49,419 --> 00:25:56,609
questions you have yes

00:26:38,820 --> 00:26:43,720
right so the question is about how

00:26:41,500 --> 00:26:46,270
sensitive cockroach gdb is to clock

00:26:43,720 --> 00:26:48,910
clock synchronization problems this was

00:26:46,270 --> 00:26:51,460
something that Kyle Kingsbury referred

00:26:48,910 --> 00:26:52,780
to in his in his Jepsen report on on

00:26:51,460 --> 00:26:57,820
analyzing the consistency of cockroach

00:26:52,780 --> 00:26:59,860
DB so this is a that this is something

00:26:57,820 --> 00:27:03,400
that we're of course paying and paying

00:26:59,860 --> 00:27:05,110
attention to very closely we do most of

00:27:03,400 --> 00:27:08,860
our testing on the public cloud

00:27:05,110 --> 00:27:13,780
platforms and we find that in general

00:27:08,860 --> 00:27:17,110
NTP works works very well so if as long

00:27:13,780 --> 00:27:20,560
as you run run NTP on on your on your

00:27:17,110 --> 00:27:22,180
nodes that then that then this hasn't

00:27:20,560 --> 00:27:26,020
really been a been a problem in practice

00:27:22,180 --> 00:27:28,870
we see sub-sub ten millisecond clock

00:27:26,020 --> 00:27:32,560
offsets and the default default

00:27:28,870 --> 00:27:37,450
configuration for cockroach DB is is to

00:27:32,560 --> 00:27:40,570
is to have a fruit for the nodes to to

00:27:37,450 --> 00:27:43,960
crash and and die if they detect a clock

00:27:40,570 --> 00:27:45,640
clock offset of 250 milliseconds and

00:27:43,960 --> 00:27:47,440
then to actually have a consistency

00:27:45,640 --> 00:27:51,070
problem you would need a clock offset of

00:27:47,440 --> 00:27:54,150
half a second and we we run in this

00:27:51,070 --> 00:27:57,160
configuration on you know all the major

00:27:54,150 --> 00:27:58,810
virtualized cloud platforms and we don't

00:27:57,160 --> 00:28:03,160
we don't really have any trouble keeping

00:27:58,810 --> 00:28:06,990
clock offsets well you know 50 times

00:28:03,160 --> 00:28:08,980
below the maximum limit there so this is

00:28:06,990 --> 00:28:10,120
this is something that you do have to

00:28:08,980 --> 00:28:11,560
watch out for you've got to be sure that

00:28:10,120 --> 00:28:13,780
you are running in

00:28:11,560 --> 00:28:17,020
you can't just count on the cloud

00:28:13,780 --> 00:28:19,450
platform doing it for you but as long as

00:28:17,020 --> 00:28:21,130
you're running NTP this is that this has

00:28:19,450 --> 00:28:46,810
not been not proven to be a problem in

00:28:21,130 --> 00:28:48,910
practice yes so how does the performance

00:28:46,810 --> 00:28:52,240
compare to other other sequel based

00:28:48,910 --> 00:28:54,310
databases well it's a it's it's a tricky

00:28:52,240 --> 00:28:56,310
question to answer because it's tricky

00:28:54,310 --> 00:29:00,310
to get an apples to apples comparison

00:28:56,310 --> 00:29:02,830
and so one way to look at it is to just

00:29:00,310 --> 00:29:04,930
compare single node performance so a

00:29:02,830 --> 00:29:09,630
single node of cockroach compared to a

00:29:04,930 --> 00:29:13,030
single node of Postgres for example

00:29:09,630 --> 00:29:14,410
where you know that there's no that

00:29:13,030 --> 00:29:16,120
there's no fundamental yet you know

00:29:14,410 --> 00:29:17,230
reason why that those are those are

00:29:16,120 --> 00:29:20,020
different different sorts of

00:29:17,230 --> 00:29:22,990
environments and so you can sort of get

00:29:20,020 --> 00:29:25,270
a baseline baseline number there and for

00:29:22,990 --> 00:29:27,580
most operations we're we're within a

00:29:25,270 --> 00:29:30,160
factor of 2 of Postgres single node

00:29:27,580 --> 00:29:33,280
performance as for the impact of

00:29:30,160 --> 00:29:37,870
synchronous replication this is going to

00:29:33,280 --> 00:29:41,020
depend on both the layout of your of

00:29:37,870 --> 00:29:45,190
your nodes for in terms of geographic

00:29:41,020 --> 00:29:47,560
distribution and and the distribution of

00:29:45,190 --> 00:29:49,930
your query traffic if your queries are

00:29:47,560 --> 00:29:52,510
well distributed across the across the

00:29:49,930 --> 00:29:54,670
key space then you can get a lot of

00:29:52,510 --> 00:29:57,790
parallelization even though the latency

00:29:54,670 --> 00:30:00,730
is high and so that that also helps

00:29:57,790 --> 00:30:02,020
mitigate it but if you have a lot of

00:30:00,730 --> 00:30:04,390
contention in your queries and they're

00:30:02,020 --> 00:30:06,910
all hitting the same key then that then

00:30:04,390 --> 00:30:08,160
your performance is going to suffer in

00:30:06,910 --> 00:30:11,590
proportion to the latency between

00:30:08,160 --> 00:30:13,990
between your nodes and so for the most

00:30:11,590 --> 00:30:16,870
part we do we do recommend that unless

00:30:13,990 --> 00:30:18,250
you really need sort of a globe spanning

00:30:16,870 --> 00:30:21,100
architecture that you probably want to

00:30:18,250 --> 00:30:22,669
keep you know three availability zones

00:30:21,100 --> 00:30:24,559
in one region

00:30:22,669 --> 00:30:28,100
or the equivalent you know terminology

00:30:24,559 --> 00:30:31,399
across across other other hosting

00:30:28,100 --> 00:30:33,679
providers because you are going to be

00:30:31,399 --> 00:30:35,869
paying that that wait-and-see hit on all

00:30:33,679 --> 00:30:39,759
of your all of your rights to go across

00:30:35,869 --> 00:30:42,320
to the to the other replicas for the

00:30:39,759 --> 00:30:43,759
four-year reads of course that reads

00:30:42,320 --> 00:30:46,489
don't have to go through the consensus

00:30:43,759 --> 00:30:50,119
layer and so you have to talk to the to

00:30:46,489 --> 00:30:53,509
the leader of a range but that doesn't

00:30:50,119 --> 00:30:55,009
it doesn't necessarily mean going going

00:30:53,509 --> 00:30:58,190
across the network if the leader is

00:30:55,009 --> 00:31:00,350
local and we do when query patterns

00:30:58,190 --> 00:31:01,970
allow us to optimize things by assigning

00:31:00,350 --> 00:31:04,159
a weed or in the place where the query

00:31:01,970 --> 00:31:06,889
is coming from we take advantage of that

00:31:04,159 --> 00:31:09,970
so that so that you can have as as good

00:31:06,889 --> 00:31:09,970
a performance as possible

00:31:29,920 --> 00:31:33,059
[Music]

00:31:38,060 --> 00:31:42,740
yeah so the question is do we get when

00:31:40,700 --> 00:31:44,660
you run sort of complicated sequel

00:31:42,740 --> 00:31:46,940
queries with joins and where clauses and

00:31:44,660 --> 00:31:47,900
things like that do you get good

00:31:46,940 --> 00:31:50,810
performance in the distributed

00:31:47,900 --> 00:31:55,100
environment and that's a complicated

00:31:50,810 --> 00:31:58,910
question to to answer but we do

00:31:55,100 --> 00:32:01,970
generally get get good results in in

00:31:58,910 --> 00:32:04,360
terms of being able to split up a split

00:32:01,970 --> 00:32:06,800
of a queries in such a way that it can

00:32:04,360 --> 00:32:09,500
that it can run as efficiently as

00:32:06,800 --> 00:32:13,280
possible across across these clusters so

00:32:09,500 --> 00:32:14,660
our the biggest limitation for that that

00:32:13,280 --> 00:32:16,940
sort of thing right now is that the

00:32:14,660 --> 00:32:19,370
query planner is is kind of stupid and

00:32:16,940 --> 00:32:23,030
it doesn't know it doesn't know a lot

00:32:19,370 --> 00:32:24,680
about about query that the table

00:32:23,030 --> 00:32:26,900
statistics how to take advantage of the

00:32:24,680 --> 00:32:27,950
fact like it doesn't know how to take

00:32:26,900 --> 00:32:29,330
advantage of the fact that maybe one

00:32:27,950 --> 00:32:32,540
table is a lot smaller than the other

00:32:29,330 --> 00:32:34,810
and so sometimes it will it will produce

00:32:32,540 --> 00:32:37,580
a very inefficient query plan for joins

00:32:34,810 --> 00:32:40,910
and you have to kind of give it all its

00:32:37,580 --> 00:32:46,450
hand and tell it exactly how to how to

00:32:40,910 --> 00:32:49,040
join things together but yeah we have a

00:32:46,450 --> 00:32:51,880
yeah we have a pretty good pretty good

00:32:49,040 --> 00:32:55,190
ability to to take take queries that are

00:32:51,880 --> 00:32:57,080
sort sort of well indexed and and turn

00:32:55,190 --> 00:33:02,080
those into efficient distributed query

00:32:57,080 --> 00:33:02,080
plans yes

00:33:05,509 --> 00:33:11,489
no we don't we don't currently support

00:33:07,980 --> 00:33:17,759
the geometry or spatial indexing formats

00:33:11,489 --> 00:33:20,039
that that Postgres does no no user

00:33:17,759 --> 00:33:22,499
defined types either the best way to

00:33:20,039 --> 00:33:24,899
think of our of our sequel support at

00:33:22,499 --> 00:33:27,059
this point is that we implement a very

00:33:24,899 --> 00:33:30,169
large fraction of the common subset of

00:33:27,059 --> 00:33:32,850
sequel across across all major databases

00:33:30,169 --> 00:33:34,859
we don't support a lot of things that

00:33:32,850 --> 00:33:59,909
are that are unique to any particular

00:33:34,859 --> 00:34:01,409
database yes yeah so we have made a lot

00:33:59,909 --> 00:34:04,679
of improvements and our joint support

00:34:01,409 --> 00:34:08,069
over the last six to nine months so we

00:34:04,679 --> 00:34:09,629
we did a blog post about a year ago I

00:34:08,069 --> 00:34:10,679
think which is probably what you're what

00:34:09,629 --> 00:34:13,559
you're thinking of where we talked about

00:34:10,679 --> 00:34:17,280
our you know our initial version of

00:34:13,559 --> 00:34:19,799
joins being kind of kind of limited I

00:34:17,280 --> 00:34:22,649
think that we've made made improvements

00:34:19,799 --> 00:34:24,869
in in a number of areas for one thing we

00:34:22,649 --> 00:34:27,480
implement merge joins now and not just

00:34:24,869 --> 00:34:30,000
hash joins we the query planner has

00:34:27,480 --> 00:34:32,579
gotten has gotten smarter we support

00:34:30,000 --> 00:34:35,309
temporary on disk spooling of

00:34:32,579 --> 00:34:40,589
intermediate results so it doesn't have

00:34:35,309 --> 00:34:42,329
to all all fit in in memory and so yeah

00:34:40,589 --> 00:34:46,020
we've we've made made a lot of

00:34:42,329 --> 00:34:50,059
improvements in our in our ability to to

00:34:46,020 --> 00:34:53,339
handle joins I think that we're still

00:34:50,059 --> 00:34:55,500
you know I would say if we're still not

00:34:53,339 --> 00:34:57,990
not great at dealing with joins that

00:34:55,500 --> 00:35:01,230
handle very very large amounts of data

00:34:57,990 --> 00:35:03,780
but if you're if the index is in your in

00:35:01,230 --> 00:35:06,210
your tables are such that you know the

00:35:03,780 --> 00:35:07,890
joint can be satisfied without what

00:35:06,210 --> 00:35:09,900
without having to do a big a big cross

00:35:07,890 --> 00:35:12,089
join that multiplies out a lot of data

00:35:09,900 --> 00:35:14,809
then you know it's it I think it works

00:35:12,089 --> 00:35:14,809
that works pretty well

00:35:17,550 --> 00:36:12,160
yes yes so can we take advantage of

00:36:08,730 --> 00:36:14,500
operator supplied information to to help

00:36:12,160 --> 00:36:17,080
optimize data placements like to put

00:36:14,500 --> 00:36:18,850
some some parts of the data on on fast

00:36:17,080 --> 00:36:22,990
storage and some on on slower storage

00:36:18,850 --> 00:36:26,080
yes so that that's currently currently

00:36:22,990 --> 00:36:27,940
fairly fairly coarse grained that it's

00:36:26,080 --> 00:36:30,000
you can you can configure these things

00:36:27,940 --> 00:36:33,280
at the table level and so you can have

00:36:30,000 --> 00:36:35,730
have a table that's that's stored on on

00:36:33,280 --> 00:36:38,530
SSD and another stored on spinning disk

00:36:35,730 --> 00:36:40,420
in one point two with the row level

00:36:38,530 --> 00:36:43,090
partitioning enterprise feature that I

00:36:40,420 --> 00:36:46,450
talked about we want to be able to let

00:36:43,090 --> 00:36:48,690
you specify that on a on a on a sub

00:36:46,450 --> 00:36:51,510
table granularity which could include

00:36:48,690 --> 00:36:56,110
doing things like designating a

00:36:51,510 --> 00:36:59,260
timestamp column as your as your region

00:36:56,110 --> 00:37:02,170
as your region key and then say set say

00:36:59,260 --> 00:37:03,970
that you know you you'd have like a cron

00:37:02,170 --> 00:37:07,000
job shifting shifting boundary so that

00:37:03,970 --> 00:37:10,320
data would age out into into cheaper

00:37:07,000 --> 00:37:10,320
cheaper storage over time

00:37:16,690 --> 00:37:20,710
all right thank you very much

00:37:20,790 --> 00:37:23,929

YouTube URL: https://www.youtube.com/watch?v=28e54qRvcyY


