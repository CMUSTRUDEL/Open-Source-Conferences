Title: Continuous Delivery for DC OS with Spinnaker
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Continuous Delivery for DC/OS with Spinnaker - Will Gorman, Cerner

In this presentation, Will Gorman will provide an overview of the Spinnaker continuous delivery platform, and discuss how extending it to support DC/OS as a deployment target is helping Cerner to manage the challenges of deploying its healthcare services architecture safely and reliably.

About

Will Gorman
Cerner
Software Architect
Kansas City, MO
Twitter Tweet
Will Gorman is a software architect at Cerner where he has been building health care solutions for over a decade. More recently he has been working on transforming the deployment of those solutions with DC/OS. He loves to create tools to help other developers to build and deploy their ideas quickly and safely.
Captions: 
	00:00:00,030 --> 00:00:04,170
today I want to talk to you about our

00:00:02,520 --> 00:00:05,100
approach for doing continuous delivery

00:00:04,170 --> 00:00:07,290
with DCOs

00:00:05,100 --> 00:00:09,090
I'll explain a little bit about what our

00:00:07,290 --> 00:00:10,830
motivations and goals were and then

00:00:09,090 --> 00:00:12,780
introduce spinnaker the system that

00:00:10,830 --> 00:00:15,509
we're using as well as get into some of

00:00:12,780 --> 00:00:16,890
the details about how we use it my name

00:00:15,509 --> 00:00:18,539
is will Gorman and I work on a team

00:00:16,890 --> 00:00:20,609
developing deployment solutions at

00:00:18,539 --> 00:00:23,519
cerner we're a health care technology

00:00:20,609 --> 00:00:25,170
company we've been using mesas for a few

00:00:23,519 --> 00:00:26,130
years now and if more recently started

00:00:25,170 --> 00:00:27,930
to adopt DCOs

00:00:26,130 --> 00:00:30,900
as we look towards container izing our

00:00:27,930 --> 00:00:34,050
service workloads one thing that we

00:00:30,900 --> 00:00:36,120
found is that deploying software is

00:00:34,050 --> 00:00:37,550
challenging I'm going to guess this is

00:00:36,120 --> 00:00:40,050
probably not a controversial statement

00:00:37,550 --> 00:00:42,090
if anybody disagrees I'd like to talk to

00:00:40,050 --> 00:00:44,010
you afterwards so we can do what you're

00:00:42,090 --> 00:00:46,190
doing but whenever we're doing a

00:00:44,010 --> 00:00:48,930
deployment this is a time when our

00:00:46,190 --> 00:00:52,100
systems are at a greater risk of

00:00:48,930 --> 00:00:54,899
downtime or instability due to a problem

00:00:52,100 --> 00:00:57,510
with the new code being deployed or the

00:00:54,899 --> 00:01:00,210
configuration change or maybe some steps

00:00:57,510 --> 00:01:04,049
in a manual work plan being performed in

00:01:00,210 --> 00:01:05,670
the wrong order or omitted so one thing

00:01:04,049 --> 00:01:07,170
that we're constantly evaluating is how

00:01:05,670 --> 00:01:09,330
we can reduce the risks of our

00:01:07,170 --> 00:01:13,409
deployments and make them more reliable

00:01:09,330 --> 00:01:15,299
and consistent continuous delivery is

00:01:13,409 --> 00:01:17,280
something that we believe can help us in

00:01:15,299 --> 00:01:20,040
this regard with continuous delivery

00:01:17,280 --> 00:01:22,680
we're striving to release smaller

00:01:20,040 --> 00:01:25,229
changes more frequently and avoid the

00:01:22,680 --> 00:01:27,540
situations where by deploying a large

00:01:25,229 --> 00:01:30,060
set of unrelated features together there

00:01:27,540 --> 00:01:33,030
is some unexpected interaction between

00:01:30,060 --> 00:01:34,560
those when they're deployed or if there

00:01:33,030 --> 00:01:36,360
is an issue may be more difficult to

00:01:34,560 --> 00:01:39,420
pinpoint which of those features is

00:01:36,360 --> 00:01:41,189
responsible for it but continuous

00:01:39,420 --> 00:01:43,710
delivery can also help us decrease the

00:01:41,189 --> 00:01:46,229
cost of our deployments by requiring us

00:01:43,710 --> 00:01:48,030
to address any points of friction or

00:01:46,229 --> 00:01:50,850
inefficiencies that we might be able to

00:01:48,030 --> 00:01:54,619
otherwise tolerate or overlook when we

00:01:50,850 --> 00:01:56,880
deploy less frequently and finally by

00:01:54,619 --> 00:01:59,009
decreasing the delay between the

00:01:56,880 --> 00:02:00,360
development of a feature and getting

00:01:59,009 --> 00:02:02,520
that into the hands of our users were

00:02:00,360 --> 00:02:07,439
able to provide value to them more

00:02:02,520 --> 00:02:10,800
quickly so as we started to consider

00:02:07,439 --> 00:02:12,569
what kind of attributes a system that we

00:02:10,800 --> 00:02:13,860
would use for continuous delivery that

00:02:12,569 --> 00:02:16,230
Cerner would need to provide

00:02:13,860 --> 00:02:19,020
there are really three key ones that we

00:02:16,230 --> 00:02:20,040
landed on and that's what those are that

00:02:19,020 --> 00:02:23,250
it would need to help make our

00:02:20,040 --> 00:02:25,650
deployments safe automated and flexible

00:02:23,250 --> 00:02:27,330
so I'd like to spend just a minute on

00:02:25,650 --> 00:02:30,930
each of those and get into some more

00:02:27,330 --> 00:02:32,780
detail at cerner the safety of our

00:02:30,930 --> 00:02:35,160
deployments is of the utmost concern

00:02:32,780 --> 00:02:36,930
since we work in healthcare the

00:02:35,160 --> 00:02:40,080
reliability of our services can really

00:02:36,930 --> 00:02:42,209
have an impact on people's lives if you

00:02:40,080 --> 00:02:44,310
know we have a downtime or some other

00:02:42,209 --> 00:02:46,290
interruption of service that means that

00:02:44,310 --> 00:02:47,820
during that time a clinician may not

00:02:46,290 --> 00:02:49,410
have access to all the information that

00:02:47,820 --> 00:02:51,540
they need to provide the best possible

00:02:49,410 --> 00:02:54,360
care so we have to be doing everything

00:02:51,540 --> 00:02:56,520
we can to prevent issues as a result of

00:02:54,360 --> 00:02:58,770
deployments it's certainly true that

00:02:56,520 --> 00:03:00,510
with DC US we have the ability to do

00:02:58,770 --> 00:03:02,520
things like roll back to an earlier

00:03:00,510 --> 00:03:04,890
version much more quickly and easily

00:03:02,520 --> 00:03:07,050
than we could with our traditional VM

00:03:04,890 --> 00:03:08,700
based deployments but that alone isn't

00:03:07,050 --> 00:03:10,650
necessarily enough to get us to the

00:03:08,700 --> 00:03:12,900
level of quality and assurance that we

00:03:10,650 --> 00:03:14,160
need we want to make sure that we're

00:03:12,900 --> 00:03:16,230
doing everything possible to catch those

00:03:14,160 --> 00:03:18,420
types of issues before they reach a

00:03:16,230 --> 00:03:19,410
point where they can affect our users so

00:03:18,420 --> 00:03:21,480
we have to make sure that we're doing

00:03:19,410 --> 00:03:24,750
certain things consistently and reliably

00:03:21,480 --> 00:03:26,070
during every deployment so some of those

00:03:24,750 --> 00:03:27,720
things we want to do on every deployment

00:03:26,070 --> 00:03:29,160
are make sure we're running unit tests

00:03:27,720 --> 00:03:31,890
or integration tests as part of our

00:03:29,160 --> 00:03:33,060
builds deploying first to a sandbox or

00:03:31,890 --> 00:03:35,640
non production environment before

00:03:33,060 --> 00:03:38,070
production after we deploy we want to

00:03:35,640 --> 00:03:39,810
execute smoke tests to send some kind of

00:03:38,070 --> 00:03:41,220
you know synthetic traffic to services

00:03:39,810 --> 00:03:43,350
just to make sure that they're doing

00:03:41,220 --> 00:03:45,390
what we expect after the deployment and

00:03:43,350 --> 00:03:49,769
of course we have to deploy without

00:03:45,390 --> 00:03:51,780
introducing any kind of downtime finally

00:03:49,769 --> 00:03:53,459
we want to be able to adopt deployment

00:03:51,780 --> 00:03:56,400
strategies like canari deployments where

00:03:53,459 --> 00:03:58,650
we can deploy a new version of our

00:03:56,400 --> 00:04:00,180
service and send a small percentage of

00:03:58,650 --> 00:04:02,610
our traffic to it so that we can analyze

00:04:00,180 --> 00:04:04,560
and observe the characteristics and

00:04:02,610 --> 00:04:06,330
behavior of that change before it gets

00:04:04,560 --> 00:04:07,799
to the point where it can impact a large

00:04:06,330 --> 00:04:12,239
portion of our users if there is

00:04:07,799 --> 00:04:14,220
something that doesn't work right so

00:04:12,239 --> 00:04:15,810
with all these additional steps that we

00:04:14,220 --> 00:04:18,150
want to perform on every deployment

00:04:15,810 --> 00:04:20,729
we're really saying if these are going

00:04:18,150 --> 00:04:22,440
to be performed by humans manually now

00:04:20,729 --> 00:04:24,180
we're actually adding more complexity to

00:04:22,440 --> 00:04:26,520
our deployments and introducing more

00:04:24,180 --> 00:04:27,430
opportunities for human error so when we

00:04:26,520 --> 00:04:30,070
look at automate

00:04:27,430 --> 00:04:32,889
of our continuous delivery this is

00:04:30,070 --> 00:04:35,560
important not just so that we can go

00:04:32,889 --> 00:04:36,370
more quickly or reduce drudgery although

00:04:35,560 --> 00:04:38,380
it does do that

00:04:36,370 --> 00:04:41,740
Automation is also an important enabler

00:04:38,380 --> 00:04:43,270
of safety for example if we think about

00:04:41,740 --> 00:04:44,860
the parts of our deployment plans that

00:04:43,270 --> 00:04:46,570
hopefully we're not executing that

00:04:44,860 --> 00:04:48,550
frequently those are probably the parts

00:04:46,570 --> 00:04:51,039
that deal with errors and recovering

00:04:48,550 --> 00:04:53,800
prepares if we're automating we're able

00:04:51,039 --> 00:04:55,780
to define not only what should happen

00:04:53,800 --> 00:04:57,639
when a deployment works successfully but

00:04:55,780 --> 00:04:59,259
also when something goes wrong and we

00:04:57,639 --> 00:05:00,789
could be testing those steps of

00:04:59,259 --> 00:05:02,550
operations to make sure that they are

00:05:00,789 --> 00:05:05,410
accurate and correct

00:05:02,550 --> 00:05:06,639
finally Automation is important to get

00:05:05,410 --> 00:05:09,009
us to the place where we are actually

00:05:06,639 --> 00:05:12,449
physically able to deploy those changes

00:05:09,009 --> 00:05:12,449
frequently enough to get the benefits

00:05:12,900 --> 00:05:18,160
for flexibility we have a need to manage

00:05:16,180 --> 00:05:20,050
and automate the deployments for many

00:05:18,160 --> 00:05:22,960
different teams and groups within Cerner

00:05:20,050 --> 00:05:25,120
that are using DCOs a lot of these teams

00:05:22,960 --> 00:05:27,220
have very similar needs for their

00:05:25,120 --> 00:05:28,990
deployment patterns but some variations

00:05:27,220 --> 00:05:30,669
here and there however can be

00:05:28,990 --> 00:05:33,789
organizationally challenging to kind of

00:05:30,669 --> 00:05:37,599
get everyone to all agree to adopt the

00:05:33,789 --> 00:05:38,949
same fixed approach Engineers kind of

00:05:37,599 --> 00:05:40,389
left to their own devices naturally like

00:05:38,949 --> 00:05:41,650
to automate things so we could

00:05:40,389 --> 00:05:44,080
anticipate if we just kind of turned

00:05:41,650 --> 00:05:46,780
everybody loose on DCOs and said have at

00:05:44,080 --> 00:05:48,490
it we would probably see an explosion of

00:05:46,780 --> 00:05:50,110
you know hand-built and deployment

00:05:48,490 --> 00:05:52,630
automation tools from these different

00:05:50,110 --> 00:05:54,039
teams this would undercut the kind of

00:05:52,630 --> 00:05:57,099
efficiency gains that we're also hoping

00:05:54,039 --> 00:05:59,740
to get from continuous delivery so what

00:05:57,099 --> 00:06:01,659
we really want to do is make it easy for

00:05:59,740 --> 00:06:03,909
teams to define deployment pipelines

00:06:01,659 --> 00:06:05,919
that fit their needs but also to be able

00:06:03,909 --> 00:06:09,099
to share and reuse those common patterns

00:06:05,919 --> 00:06:12,909
and practices that are common across all

00:06:09,099 --> 00:06:14,740
of them ideally we could visually

00:06:12,909 --> 00:06:16,840
represent these pipelines in a way that

00:06:14,740 --> 00:06:22,479
would make it easier to build and reason

00:06:16,840 --> 00:06:23,919
about so if we look at this set of

00:06:22,479 --> 00:06:26,050
requirements we can see we're describing

00:06:23,919 --> 00:06:28,479
a system that's not exactly trivial and

00:06:26,050 --> 00:06:29,830
the question that we were faced with at

00:06:28,479 --> 00:06:32,020
this point was okay how are we actually

00:06:29,830 --> 00:06:34,030
going to put a system like this into

00:06:32,020 --> 00:06:36,099
practice and as you've probably guessed

00:06:34,030 --> 00:06:37,260
from the title of this presentation we

00:06:36,099 --> 00:06:40,900
didn't have to build something ourselves

00:06:37,260 --> 00:06:43,600
we were able to leverage spinnaker

00:06:40,900 --> 00:06:45,770
so spinnaker is an open-source

00:06:43,600 --> 00:06:48,380
multi-cloud continuous delivery tool

00:06:45,770 --> 00:06:50,240
that originated at Netflix as the way

00:06:48,380 --> 00:06:51,470
that they would perform continuous

00:06:50,240 --> 00:06:54,320
delivery and orchestrate their

00:06:51,470 --> 00:06:56,420
deployments to AWS however since being

00:06:54,320 --> 00:06:58,670
open sourced it's been expanded by the

00:06:56,420 --> 00:07:01,450
community to be able to manage

00:06:58,670 --> 00:07:04,970
deployments to Google cloud platform

00:07:01,450 --> 00:07:08,090
Azure and even container schedulers like

00:07:04,970 --> 00:07:10,100
kubernetes at the time we first started

00:07:08,090 --> 00:07:11,060
looking at spinnaker it didn't support

00:07:10,100 --> 00:07:12,950
deploying to DCOs

00:07:11,060 --> 00:07:14,960
but it quickly became clear to us that

00:07:12,950 --> 00:07:17,390
if we were to build that bit of

00:07:14,960 --> 00:07:19,220
integration ourselves it would be a big

00:07:17,390 --> 00:07:20,270
win and get us much more quickly to the

00:07:19,220 --> 00:07:22,760
place where we wanted to be with

00:07:20,270 --> 00:07:24,890
continuous delivery not only would it

00:07:22,760 --> 00:07:27,590
meet or exceed all of our requirements

00:07:24,890 --> 00:07:29,540
for safety automation and flexibility

00:07:27,590 --> 00:07:31,490
but we could take advantage of the same

00:07:29,540 --> 00:07:35,840
proven methods that power Netflix is

00:07:31,490 --> 00:07:37,940
prolific deployments so some of those

00:07:35,840 --> 00:07:39,380
features that spinnaker offers to enable

00:07:37,940 --> 00:07:42,560
continuous delivery are the ability to

00:07:39,380 --> 00:07:44,620
define pipelines that take individual

00:07:42,560 --> 00:07:47,420
commits all the way through production

00:07:44,620 --> 00:07:50,650
along the way they can run continuous

00:07:47,420 --> 00:07:53,480
integration builds and tests for

00:07:50,650 --> 00:07:56,300
deployments to providers like AWS you

00:07:53,480 --> 00:07:58,700
can bake virtual machine images from

00:07:56,300 --> 00:08:00,740
your builds and then at the time to

00:07:58,700 --> 00:08:03,200
deploy you can define deployment

00:08:00,740 --> 00:08:04,970
strategies like blue green or canary and

00:08:03,200 --> 00:08:07,340
validate the results of those

00:08:04,970 --> 00:08:09,140
deployments through spinnaker finally

00:08:07,340 --> 00:08:10,760
you're able to promote builds across

00:08:09,140 --> 00:08:13,010
environments so you can ensure that

00:08:10,760 --> 00:08:15,080
every release is going through that

00:08:13,010 --> 00:08:19,910
defined set of steps in the correct

00:08:15,080 --> 00:08:22,220
order that you expect the multi cloud

00:08:19,910 --> 00:08:24,290
capabilities of spinnaker are a great

00:08:22,220 --> 00:08:25,940
way to keep our deployment process from

00:08:24,290 --> 00:08:28,280
getting too tied to a single cloud

00:08:25,940 --> 00:08:31,640
provider we know that today we want to

00:08:28,280 --> 00:08:33,560
use marathon on DCOs but if in a year or

00:08:31,640 --> 00:08:35,540
two we decide that we want to do

00:08:33,560 --> 00:08:37,040
kubernetes instead we don't want to be

00:08:35,540 --> 00:08:39,200
in a position of kind of yanking the rug

00:08:37,040 --> 00:08:41,660
out from underneath all the developers

00:08:39,200 --> 00:08:43,340
that have come to rely on automating

00:08:41,660 --> 00:08:45,440
their deployments through the DC OSAP is

00:08:43,340 --> 00:08:47,510
so we want to have kind of a layer in

00:08:45,440 --> 00:08:50,620
place that can serve to translate and

00:08:47,510 --> 00:08:50,620
spinnaker meets that need

00:08:51,700 --> 00:08:57,050
spinnaker and being multi-cloud is not

00:08:54,980 --> 00:08:59,960
aiming to be this perfect abstraction

00:08:57,050 --> 00:09:01,520
that homogenizes and provides least

00:08:59,960 --> 00:09:04,160
common denominator set of features

00:09:01,520 --> 00:09:05,930
across the different providers there are

00:09:04,160 --> 00:09:07,760
different definitely differences exposed

00:09:05,930 --> 00:09:10,190
and you can't expect to just write a

00:09:07,760 --> 00:09:11,930
pipeline once to deploy to AWS and then

00:09:10,190 --> 00:09:15,260
automatically apply that same pipeline

00:09:11,930 --> 00:09:16,900
to kubernetes however the fact that we

00:09:15,260 --> 00:09:19,910
have our pipeline's defined

00:09:16,900 --> 00:09:22,250
declaratively as JSON to spinnaker means

00:09:19,910 --> 00:09:24,800
that the process of moving from one

00:09:22,250 --> 00:09:27,140
provider to another is now simply a data

00:09:24,800 --> 00:09:28,670
transformation since spinnaker can

00:09:27,140 --> 00:09:29,990
handle the mechanisms of actually

00:09:28,670 --> 00:09:35,000
interacting with those different

00:09:29,990 --> 00:09:36,950
providers to to run the deployment so I

00:09:35,000 --> 00:09:39,650
want to look next at some of the kind of

00:09:36,950 --> 00:09:42,140
concepts and ideas that are central to

00:09:39,650 --> 00:09:43,670
spinnaker in the way that it has an

00:09:42,140 --> 00:09:46,730
opinionated model of how cloud

00:09:43,670 --> 00:09:50,420
deployments should work the first of

00:09:46,730 --> 00:09:51,320
these is the application in spinnaker

00:09:50,420 --> 00:09:53,300
deployments are centered around

00:09:51,320 --> 00:09:55,490
applications and these are the way that

00:09:53,300 --> 00:09:56,810
it logically groups all of the

00:09:55,490 --> 00:09:58,730
deployments of some individual

00:09:56,810 --> 00:10:01,160
deployable artifact so a spinnaker

00:09:58,730 --> 00:10:02,930
application is not equivalent to a

00:10:01,160 --> 00:10:05,780
marathon application it's kind of a

00:10:02,930 --> 00:10:07,940
higher-level grouping that can encompass

00:10:05,780 --> 00:10:09,680
the multiple deployments of that

00:10:07,940 --> 00:10:12,080
application like a cross dev and

00:10:09,680 --> 00:10:14,090
production so here we can see in

00:10:12,080 --> 00:10:16,370
spinnaker this has our mesas con

00:10:14,090 --> 00:10:18,230
application and digging a little further

00:10:16,370 --> 00:10:20,900
down we can see it's comprised of

00:10:18,230 --> 00:10:22,810
clusters so we have a cluster for dev

00:10:20,900 --> 00:10:25,160
and a cluster for production in

00:10:22,810 --> 00:10:27,050
spinnaker a cluster is confusingly

00:10:25,160 --> 00:10:29,210
enough not related to a DCOs cluster

00:10:27,050 --> 00:10:31,370
it's just that kind of sub level of

00:10:29,210 --> 00:10:37,640
grouping in an application for

00:10:31,370 --> 00:10:39,080
deployment for a specific purpose going

00:10:37,640 --> 00:10:40,640
down one more level we get to the

00:10:39,080 --> 00:10:43,010
fundamental unit of deployment in

00:10:40,640 --> 00:10:44,930
spinnaker which is a server group this

00:10:43,010 --> 00:10:46,760
represents a set of instances that are

00:10:44,930 --> 00:10:49,580
all running the same version of some

00:10:46,760 --> 00:10:51,320
deployable artifact so in AWS with

00:10:49,580 --> 00:10:53,180
spinnaker a server group would be like

00:10:51,320 --> 00:10:56,420
an auto scaling group of VMs all running

00:10:53,180 --> 00:10:58,520
from the same ami in d cos a server

00:10:56,420 --> 00:11:01,670
group is the equivalent of a marathon

00:10:58,520 --> 00:11:03,459
application so here what we see two

00:11:01,670 --> 00:11:05,980
different versions of our

00:11:03,459 --> 00:11:06,819
service version zero zero zero and zero

00:11:05,980 --> 00:11:08,290
zero one

00:11:06,819 --> 00:11:10,629
each backed by a different marathon

00:11:08,290 --> 00:11:13,959
application the primary difference

00:11:10,629 --> 00:11:15,249
between how spinnaker thinks of server

00:11:13,959 --> 00:11:17,589
groups americathon thinks of

00:11:15,249 --> 00:11:18,550
applications is in marathon if we want

00:11:17,589 --> 00:11:20,889
to change something about our

00:11:18,550 --> 00:11:23,529
application like deploy a new tag of our

00:11:20,889 --> 00:11:24,730
docker image or change some environment

00:11:23,529 --> 00:11:26,800
variables we can just update that

00:11:24,730 --> 00:11:29,139
application definition restart the

00:11:26,800 --> 00:11:30,939
service to pick them up in spinnaker

00:11:29,139 --> 00:11:33,069
just about the only change that we can

00:11:30,939 --> 00:11:35,829
make to a server group after it's been

00:11:33,069 --> 00:11:38,019
created is to resize it and scale it up

00:11:35,829 --> 00:11:40,329
or down anytime we want to actually make

00:11:38,019 --> 00:11:42,910
some other more substantial change we

00:11:40,329 --> 00:11:46,929
have to create a new server group as we

00:11:42,910 --> 00:11:49,959
see here so this is kind of showing

00:11:46,929 --> 00:11:52,629
side-by-side comparison between the

00:11:49,959 --> 00:11:54,429
spinnaker UI and the dcs UI where we can

00:11:52,629 --> 00:11:56,319
see in spinnaker several clusters

00:11:54,429 --> 00:11:58,720
containing server groups those are each

00:11:56,319 --> 00:12:01,360
map to their backing marathon

00:11:58,720 --> 00:12:03,759
applications by means of Spinnaker's

00:12:01,360 --> 00:12:06,100
naming convention so it kind of controls

00:12:03,759 --> 00:12:07,119
those marathon application names and we

00:12:06,100 --> 00:12:09,369
see the different pieces of the

00:12:07,119 --> 00:12:12,639
spinnaker organizational structure

00:12:09,369 --> 00:12:14,619
reflected in those names as you might

00:12:12,639 --> 00:12:16,029
imagine if we're treating applications

00:12:14,619 --> 00:12:18,790
this differently in creating new

00:12:16,029 --> 00:12:20,049
marathon applications to deploy new

00:12:18,790 --> 00:12:21,459
changes this probably has some

00:12:20,049 --> 00:12:23,829
implications for how we interact with

00:12:21,459 --> 00:12:25,629
things like load balancers in DC OS and

00:12:23,829 --> 00:12:30,069
that's something that we'll see a little

00:12:25,629 --> 00:12:33,009
bit later on so when we want to create a

00:12:30,069 --> 00:12:35,769
new server server group we have a wizard

00:12:33,009 --> 00:12:37,509
here that exposes pretty much all of the

00:12:35,769 --> 00:12:40,839
same attributes that you see in the DC

00:12:37,509 --> 00:12:43,509
OS UI so we can set the memory and CPU

00:12:40,839 --> 00:12:45,850
limits for our containers choose which

00:12:43,509 --> 00:12:48,689
docker image to run set network

00:12:45,850 --> 00:12:51,579
endpoints environment variables secrets

00:12:48,689 --> 00:12:53,019
labels health checks all of that stuff

00:12:51,579 --> 00:12:55,179
the only difference really is instead of

00:12:53,019 --> 00:12:57,610
just setting a name for marathon

00:12:55,179 --> 00:12:59,470
application we're going to fill out some

00:12:57,610 --> 00:13:01,629
spinnaker specific values like the stack

00:12:59,470 --> 00:13:05,290
and the detail and let it derive the

00:13:01,629 --> 00:13:07,059
name it also automatically manages that

00:13:05,290 --> 00:13:09,249
version suffix that we see on our

00:13:07,059 --> 00:13:11,559
applications so if we tell spinnaker

00:13:09,249 --> 00:13:13,809
create a server group that's part of a

00:13:11,559 --> 00:13:16,450
cluster that already exists here in this

00:13:13,809 --> 00:13:18,550
case the meso is kind of cluster

00:13:16,450 --> 00:13:23,650
it will automatically look and see what

00:13:18,550 --> 00:13:25,300
version it should be applied to next in

00:13:23,650 --> 00:13:28,270
spinnaker we can also allow server

00:13:25,300 --> 00:13:29,650
groups to our clusters rather to contain

00:13:28,270 --> 00:13:33,010
server groups running in different

00:13:29,650 --> 00:13:34,210
regions so in AWS or another

00:13:33,010 --> 00:13:35,710
infrastructure provider like that a

00:13:34,210 --> 00:13:39,250
region is just what you would expect it

00:13:35,710 --> 00:13:41,830
to be for DCOs we have regions mapped to

00:13:39,250 --> 00:13:44,230
separate DC OS clusters so in this

00:13:41,830 --> 00:13:46,840
example we have marathon application

00:13:44,230 --> 00:13:48,400
running in this cluster named us East

00:13:46,840 --> 00:13:50,770
one and another one running in a cluster

00:13:48,400 --> 00:13:52,900
named us West one so we're able to use

00:13:50,770 --> 00:13:55,750
spinnaker to perfor to provide kind of a

00:13:52,900 --> 00:13:58,240
lightweight form of Federation across

00:13:55,750 --> 00:13:59,830
these multiple DC OS clusters and have a

00:13:58,240 --> 00:14:01,210
single place where we can view the

00:13:59,830 --> 00:14:03,640
status of our deployments across those

00:14:01,210 --> 00:14:05,410
clusters and manage things like

00:14:03,640 --> 00:14:10,930
migrating applications from one cluster

00:14:05,410 --> 00:14:12,700
to another so now we get to pipelines

00:14:10,930 --> 00:14:15,790
which are really the heart of spinnaker

00:14:12,700 --> 00:14:17,290
and spinnaker a pipeline is mechanism

00:14:15,790 --> 00:14:19,450
that lets you describe and automate

00:14:17,290 --> 00:14:21,910
complicated deployments and can interact

00:14:19,450 --> 00:14:25,120
with other parts of your CI like Jenkins

00:14:21,910 --> 00:14:27,130
Travis github and bitbucket pipelines

00:14:25,120 --> 00:14:29,470
are composed of a series of stages where

00:14:27,130 --> 00:14:32,020
each stage represents some kind of

00:14:29,470 --> 00:14:35,350
common operation that happens in a cloud

00:14:32,020 --> 00:14:37,630
deployment pipelines support taking

00:14:35,350 --> 00:14:40,060
input parameters and can also produce

00:14:37,630 --> 00:14:42,250
output that's added into a pipeline

00:14:40,060 --> 00:14:44,410
context that gets passed along for

00:14:42,250 --> 00:14:46,720
subsequent subsequent stages to take

00:14:44,410 --> 00:14:49,240
advantage of where they can do things

00:14:46,720 --> 00:14:52,600
like make decisions about what branch of

00:14:49,240 --> 00:14:54,820
the pipeline to execute or conditionally

00:14:52,600 --> 00:15:00,370
enable or disable certain stages and run

00:14:54,820 --> 00:15:02,020
stages in parallel to start our

00:15:00,370 --> 00:15:04,870
pipelines they can be triggered manually

00:15:02,020 --> 00:15:06,280
or by a number of different options that

00:15:04,870 --> 00:15:07,660
give you some flexibility with how you

00:15:06,280 --> 00:15:09,850
actually want to plug this into your

00:15:07,660 --> 00:15:11,320
existing CI setup so we can have a

00:15:09,850 --> 00:15:14,110
pipeline be triggered by a web hook from

00:15:11,320 --> 00:15:16,180
a github repository or by watching for

00:15:14,110 --> 00:15:19,360
the successful completion of a job in

00:15:16,180 --> 00:15:21,640
Jenkins or Travis the pipeline's can

00:15:19,360 --> 00:15:23,950
watch for a docker registry and look for

00:15:21,640 --> 00:15:26,140
a new tag to be pushed for an image and

00:15:23,950 --> 00:15:28,150
then pipelines can also trigger other

00:15:26,140 --> 00:15:29,130
pipelines within spinnaker allowing us

00:15:28,150 --> 00:15:31,980
to build up kind of a graph

00:15:29,130 --> 00:15:33,540
of pipeline executions to separate

00:15:31,980 --> 00:15:36,420
things out and make pieces rip more

00:15:33,540 --> 00:15:38,730
reusable and then finally we can just

00:15:36,420 --> 00:15:40,230
have a kind of cron like schedule for

00:15:38,730 --> 00:15:45,480
running our pipelines and run whatever

00:15:40,230 --> 00:15:47,010
some scheduled interval expires in d cos

00:15:45,480 --> 00:15:48,630
the trigger that may be the most

00:15:47,010 --> 00:15:50,790
interesting for us is that docker

00:15:48,630 --> 00:15:53,160
registry trigger since this is kind of

00:15:50,790 --> 00:15:55,200
an easy way to use just whatever your

00:15:53,160 --> 00:15:58,020
existing setup is for building images

00:15:55,200 --> 00:15:59,570
and let spinnaker just take over from

00:15:58,020 --> 00:16:02,100
the point where that image gets pushed

00:15:59,570 --> 00:16:04,440
so we can define what image we want and

00:16:02,100 --> 00:16:06,540
then some criteria about the tags that

00:16:04,440 --> 00:16:09,090
we want to look for it's important to

00:16:06,540 --> 00:16:11,250
note that by default spinnaker is not

00:16:09,090 --> 00:16:13,460
looking it's only looking for new tags

00:16:11,250 --> 00:16:16,050
so if you're doing something like

00:16:13,460 --> 00:16:17,960
reusing a tag to point to a new image as

00:16:16,050 --> 00:16:20,070
with frequently done with latest

00:16:17,960 --> 00:16:21,630
spinnaker is not picking those up and

00:16:20,070 --> 00:16:24,300
the reason is that we want to make it

00:16:21,630 --> 00:16:26,730
easy to rollback within our pipelines so

00:16:24,300 --> 00:16:31,050
we always want to have a way to easily

00:16:26,730 --> 00:16:32,970
refer to those previous versions it in

00:16:31,050 --> 00:16:35,910
our case what we do usually is apply

00:16:32,970 --> 00:16:37,710
kind of a build timestamp to our docker

00:16:35,910 --> 00:16:39,450
image tags and then use a regular

00:16:37,710 --> 00:16:43,470
expression to match on only the part of

00:16:39,450 --> 00:16:45,180
the tag that we're interested in so we

00:16:43,470 --> 00:16:47,310
have a number of different kind of

00:16:45,180 --> 00:16:49,410
stages and operations that can be used

00:16:47,310 --> 00:16:51,540
in spinnaker for interacting with DCOs

00:16:49,410 --> 00:16:53,070
and rather than go through all of those

00:16:51,540 --> 00:16:54,360
in detail I just want to touch on a few

00:16:53,070 --> 00:16:58,680
of the ones that are used most

00:16:54,360 --> 00:17:01,220
frequently unsurprisingly the deploy

00:16:58,680 --> 00:17:04,439
stage gets used a lot during deployments

00:17:01,220 --> 00:17:06,240
in the deploy stage were able to provide

00:17:04,439 --> 00:17:09,480
that configuration like we saw on that

00:17:06,240 --> 00:17:10,829
create server group wizard earlier but

00:17:09,480 --> 00:17:12,660
we can also do things like deploy

00:17:10,829 --> 00:17:14,660
multiple server groups at the same time

00:17:12,660 --> 00:17:17,100
so in this example we set it up to

00:17:14,660 --> 00:17:18,870
deploy the same server group

00:17:17,100 --> 00:17:24,270
configuration into multiple DCOs

00:17:18,870 --> 00:17:26,370
clusters most of the time in a pipeline

00:17:24,270 --> 00:17:28,770
after we've deployed something we're

00:17:26,370 --> 00:17:30,690
also going to need to destroy something

00:17:28,770 --> 00:17:32,370
and remove something hopefully that's

00:17:30,690 --> 00:17:34,470
because the new version of our

00:17:32,370 --> 00:17:36,000
application is working great and we're

00:17:34,470 --> 00:17:38,730
able to tell the destroy server group

00:17:36,000 --> 00:17:41,490
stage to target that previous server

00:17:38,730 --> 00:17:42,340
group and remove that older version but

00:17:41,490 --> 00:17:43,840
if something goes

00:17:42,340 --> 00:17:45,610
and the new version doesn't work out we

00:17:43,840 --> 00:17:47,799
can also tell it to target that newest

00:17:45,610 --> 00:17:51,330
server group and remove that to get us

00:17:47,799 --> 00:17:51,330
back to the state we were in previously

00:17:52,559 --> 00:17:57,549
sometimes in pipelines we want to have a

00:17:55,419 --> 00:17:59,350
place to kind of just execute a script

00:17:57,549 --> 00:18:02,110
or some other kind of logic to do

00:17:59,350 --> 00:18:05,110
something like run some smoke tests or

00:18:02,110 --> 00:18:06,490
perform a database migration in

00:18:05,110 --> 00:18:08,860
spinnaker we have the ability to

00:18:06,490 --> 00:18:10,480
initiate Jenkins jobs from a pipeline

00:18:08,860 --> 00:18:12,150
and that's one way that we can have kind

00:18:10,480 --> 00:18:14,919
of an execution environment for

00:18:12,150 --> 00:18:17,500
general-purpose tasks like these but

00:18:14,919 --> 00:18:19,720
since in DCOs we have a full container

00:18:17,500 --> 00:18:21,820
scheduler at our disposal with the

00:18:19,720 --> 00:18:24,700
metronome framework we also have this

00:18:21,820 --> 00:18:27,010
run job stage that lets us define the

00:18:24,700 --> 00:18:28,510
configuration for a metronome job what

00:18:27,010 --> 00:18:30,909
container we want and what command to

00:18:28,510 --> 00:18:34,600
run in it and then we can have that

00:18:30,909 --> 00:18:36,490
execute during our pipeline as part of

00:18:34,600 --> 00:18:39,460
this we can also have this task choose

00:18:36,490 --> 00:18:42,070
to write out a JSON or properties file

00:18:39,460 --> 00:18:44,020
into the meso sandbox for the task and

00:18:42,070 --> 00:18:46,809
spinnaker can read the contents of that

00:18:44,020 --> 00:18:48,669
file in and make that available as part

00:18:46,809 --> 00:18:52,600
of the pipeline context as information

00:18:48,669 --> 00:18:54,520
that subsequent stages can use the way

00:18:52,600 --> 00:18:56,350
that stages can make use of that context

00:18:54,520 --> 00:18:59,320
is through a really powerful and

00:18:56,350 --> 00:19:01,120
flexible feature maker that enables a

00:18:59,320 --> 00:19:03,700
lot of dynamism and our pipelines and

00:19:01,120 --> 00:19:05,440
that is pipeline expressions so here we

00:19:03,700 --> 00:19:07,090
have it's using the spring expression

00:19:05,440 --> 00:19:08,799
language to let us define little kind of

00:19:07,090 --> 00:19:11,080
snippets of code that can be placed in

00:19:08,799 --> 00:19:12,940
just about any attribute of those fields

00:19:11,080 --> 00:19:14,409
when we're building a pipeline and with

00:19:12,940 --> 00:19:16,029
pipeline expressions we can do things

00:19:14,409 --> 00:19:17,770
like generate attributes of a pipeline

00:19:16,029 --> 00:19:21,279
at runtime for things that we don't know

00:19:17,770 --> 00:19:22,870
when we build it or evaluate tests for

00:19:21,279 --> 00:19:25,360
certain conditions to determine what

00:19:22,870 --> 00:19:27,970
branches to take so we can see we have

00:19:25,360 --> 00:19:30,429
kind of a nice autocomplete UI that pops

00:19:27,970 --> 00:19:32,860
up here when you're using a pipeline

00:19:30,429 --> 00:19:34,929
expression to show you what values are

00:19:32,860 --> 00:19:36,669
available in that pipeline context using

00:19:34,929 --> 00:19:40,210
values from previous runs of that

00:19:36,669 --> 00:19:42,340
pipeline and in order to use a pipeline

00:19:40,210 --> 00:19:44,799
expression we can do something as one

00:19:42,340 --> 00:19:46,799
example if we have an input parameter

00:19:44,799 --> 00:19:49,390
for our pipeline that we want to set as

00:19:46,799 --> 00:19:52,929
like a label on our marathon application

00:19:49,390 --> 00:19:55,030
in inside of that label field we can

00:19:52,929 --> 00:19:56,560
refer to that input parameter through

00:19:55,030 --> 00:19:58,660
I apply an expression and at the time

00:19:56,560 --> 00:20:00,370
the pipeline is run we'll get a prompt

00:19:58,660 --> 00:20:03,070
to and put that parameter and it will

00:20:00,370 --> 00:20:04,480
propagate through into that label so

00:20:03,070 --> 00:20:06,280
we'll see another number of other

00:20:04,480 --> 00:20:11,410
examples of pipeline expressions later

00:20:06,280 --> 00:20:13,480
on so spinnaker also has a number of

00:20:11,410 --> 00:20:17,050
features that it provides as a way to

00:20:13,480 --> 00:20:20,040
make deployments safer and less risky so

00:20:17,050 --> 00:20:22,930
I want to touch on some of those next

00:20:20,040 --> 00:20:24,850
so with execution windows in spinnaker

00:20:22,930 --> 00:20:27,130
we want to make it possible for a commit

00:20:24,850 --> 00:20:29,290
to get built and go all the way into

00:20:27,130 --> 00:20:31,270
production automatically but we don't

00:20:29,290 --> 00:20:33,400
necessarily want that to happen as soon

00:20:31,270 --> 00:20:36,070
as the commit is pushed we may only want

00:20:33,400 --> 00:20:38,080
to allow our deployment to happen during

00:20:36,070 --> 00:20:39,670
off-peak load times if our service

00:20:38,080 --> 00:20:42,130
follows some predictable pattern for

00:20:39,670 --> 00:20:43,720
load or maybe we only want to deploy at

00:20:42,130 --> 00:20:45,370
times when we know we have enough people

00:20:43,720 --> 00:20:48,760
available to deal with anything

00:20:45,370 --> 00:20:50,350
unexpected that happens in spinnaker for

00:20:48,760 --> 00:20:53,020
any stage in a pipeline we can define an

00:20:50,350 --> 00:20:54,550
execution window that specifies a range

00:20:53,020 --> 00:20:57,460
of time during which that stage is

00:20:54,550 --> 00:20:59,500
allowed to execute and if the pipeline

00:20:57,460 --> 00:21:01,840
were to run outside of that time it will

00:20:59,500 --> 00:21:04,330
be held at a suspended state waiting for

00:21:01,840 --> 00:21:06,760
the time to begin or waiting for this

00:21:04,330 --> 00:21:11,740
manual override to be provided and allow

00:21:06,760 --> 00:21:14,070
the pipeline to continue since pipelines

00:21:11,740 --> 00:21:16,540
are very powerful and flexible

00:21:14,070 --> 00:21:19,930
unfortunately that also means that they

00:21:16,540 --> 00:21:21,790
can contain errors or things that caused

00:21:19,930 --> 00:21:23,680
something unintended to happen when the

00:21:21,790 --> 00:21:26,380
pipeline runs the last thing that we

00:21:23,680 --> 00:21:27,970
want to find out that has happened is

00:21:26,380 --> 00:21:30,040
that a pipeline had some flaw that

00:21:27,970 --> 00:21:32,980
caused us to shut down all of our active

00:21:30,040 --> 00:21:34,570
instances in production in spinnaker to

00:21:32,980 --> 00:21:36,700
make it harder to do something like that

00:21:34,570 --> 00:21:39,490
there are traffic guards that we can use

00:21:36,700 --> 00:21:41,560
to specify that certain clusters of our

00:21:39,490 --> 00:21:43,750
application are critical and if any

00:21:41,560 --> 00:21:45,520
pipeline operation or manual operation

00:21:43,750 --> 00:21:48,130
would leave those in a state where there

00:21:45,520 --> 00:21:50,230
are no active instances spinnaker will

00:21:48,130 --> 00:21:56,230
stop that operation from occurring and

00:21:50,230 --> 00:21:58,450
leave leave the cluster as it is finally

00:21:56,230 --> 00:22:01,630
with our applications we want to make

00:21:58,450 --> 00:22:03,550
sure that they are really resilient and

00:22:01,630 --> 00:22:05,740
able to be highly available even in the

00:22:03,550 --> 00:22:08,730
face of failures or the loss of

00:22:05,740 --> 00:22:10,380
individual instances of our applique

00:22:08,730 --> 00:22:12,570
and this is something that Netflix has

00:22:10,380 --> 00:22:15,419
really pioneered with their kind of

00:22:12,570 --> 00:22:17,850
chaos testing suite of tools that began

00:22:15,419 --> 00:22:20,730
with the original chaos monkey that was

00:22:17,850 --> 00:22:24,600
used to randomly terminate instances in

00:22:20,730 --> 00:22:26,250
AWS and make sure that the services that

00:22:24,600 --> 00:22:28,740
those instances belong to still remained

00:22:26,250 --> 00:22:30,590
available in the most recent version of

00:22:28,740 --> 00:22:33,390
chaos monkey it's been rewritten to use

00:22:30,590 --> 00:22:35,340
spinnaker api's instead of AWS api's

00:22:33,390 --> 00:22:37,919
directly so this means that now we're

00:22:35,340 --> 00:22:39,390
able to use chaos monkey with any of the

00:22:37,919 --> 00:22:41,330
providers that spinnaker supports

00:22:39,390 --> 00:22:43,919
deploying - including DCOs

00:22:41,330 --> 00:22:47,010
so now at an application by application

00:22:43,919 --> 00:22:49,140
level we can define a schedule our

00:22:47,010 --> 00:22:51,870
frequency at which we want chaos monkey

00:22:49,140 --> 00:22:54,000
to possibly randomly terminate an

00:22:51,870 --> 00:22:56,399
application but also mark certain

00:22:54,000 --> 00:23:03,539
clusters as protected and prevent that

00:22:56,399 --> 00:23:05,039
from applying to them so next I'd like

00:23:03,539 --> 00:23:07,799
to go over some of the ways that we've

00:23:05,039 --> 00:23:09,029
put these capabilities to use in our

00:23:07,799 --> 00:23:13,710
deployments and some of the patterns

00:23:09,029 --> 00:23:16,110
that we're using with spinnaker today so

00:23:13,710 --> 00:23:19,860
one thing we can do is provide a way to

00:23:16,110 --> 00:23:22,289
enforce timeouts for deployments so

00:23:19,860 --> 00:23:24,299
marathon if we have something going

00:23:22,289 --> 00:23:26,039
wrong with our deployment like a health

00:23:24,299 --> 00:23:28,950
check is constantly failing its

00:23:26,039 --> 00:23:29,909
restarting the services or we have

00:23:28,950 --> 00:23:32,490
something that's causing those

00:23:29,909 --> 00:23:34,049
containers to crash immediately marathon

00:23:32,490 --> 00:23:36,029
is just really determined that

00:23:34,049 --> 00:23:37,890
eventually it's going to work so it

00:23:36,029 --> 00:23:40,549
keeps retrying that over and over

00:23:37,890 --> 00:23:42,419
despite all evidence to the contrary

00:23:40,549 --> 00:23:43,559
eventually we're gonna realize you know

00:23:42,419 --> 00:23:45,510
what no I don't think this is going to

00:23:43,559 --> 00:23:47,190
work and have to go in and say ok let's

00:23:45,510 --> 00:23:49,700
go and manually let's stop that

00:23:47,190 --> 00:23:53,309
deployment and figure out what happened

00:23:49,700 --> 00:23:54,630
with spinnaker instead we can define a

00:23:53,309 --> 00:23:56,549
pipeline that cleans these up

00:23:54,630 --> 00:24:00,000
automatically so if we did something

00:23:56,549 --> 00:24:01,460
silly like trying to deploy a image with

00:24:00,000 --> 00:24:03,750
a tag that doesn't exist

00:24:01,460 --> 00:24:05,970
marathons gonna just keep trying to pull

00:24:03,750 --> 00:24:07,590
that over and over but in our deploy

00:24:05,970 --> 00:24:10,559
stage in spinnaker we can override that

00:24:07,590 --> 00:24:13,320
default timeout of an hour with a lower

00:24:10,559 --> 00:24:17,100
value and then follow this deploy stage

00:24:13,320 --> 00:24:20,159
with a destroy server group that is only

00:24:17,100 --> 00:24:21,419
going to be conditionally enabled based

00:24:20,159 --> 00:24:23,700
on the value of this pipe

00:24:21,419 --> 00:24:26,700
expression down here this pipeline

00:24:23,700 --> 00:24:28,829
expression is looking at the results of

00:24:26,700 --> 00:24:31,709
that deploy stage to see if it contains

00:24:28,829 --> 00:24:35,339
a time out exception and if it does then

00:24:31,709 --> 00:24:37,139
this stage is going to be enabled so

00:24:35,339 --> 00:24:39,989
when the pipeline runs and that deploy

00:24:37,139 --> 00:24:42,269
stage times out we go and forcefully

00:24:39,989 --> 00:24:43,979
stop that running deployment and can

00:24:42,269 --> 00:24:45,929
then fail the pipeline and send a

00:24:43,979 --> 00:24:49,739
notification so someone can go in and

00:24:45,929 --> 00:24:51,419
see what happened it's also important to

00:24:49,739 --> 00:24:53,459
note that since when we're deploying

00:24:51,419 --> 00:24:55,200
with spinnaker we're deploying a new

00:24:53,459 --> 00:24:56,779
server group a new marathon application

00:24:55,200 --> 00:24:58,950
at this point we haven't actually

00:24:56,779 --> 00:25:01,289
effected any of the instances of our

00:24:58,950 --> 00:25:03,179
running application so when we stop that

00:25:01,289 --> 00:25:04,440
new deployment that's failing we're just

00:25:03,179 --> 00:25:09,599
back to the state where we were at

00:25:04,440 --> 00:25:12,629
before another concern that we have is

00:25:09,599 --> 00:25:15,779
since we work in healthcare regulated

00:25:12,629 --> 00:25:17,639
industry we have things and processes

00:25:15,779 --> 00:25:19,889
that have to be done as part of our

00:25:17,639 --> 00:25:22,519
release and deployment process and not

00:25:19,889 --> 00:25:26,190
all of these are very easy to automate

00:25:22,519 --> 00:25:27,749
so we don't want to put off doing

00:25:26,190 --> 00:25:29,609
continuous delivery until we've figured

00:25:27,749 --> 00:25:30,119
out a way to automate every single one

00:25:29,609 --> 00:25:33,149
of these

00:25:30,119 --> 00:25:35,879
so in spinnaker instead of having to do

00:25:33,149 --> 00:25:38,399
that we can use manual judgment stages

00:25:35,879 --> 00:25:40,469
in our pipeline so a manual judgment

00:25:38,399 --> 00:25:43,259
stage is a point where the pipeline is

00:25:40,469 --> 00:25:46,259
going to pause execution and then can

00:25:43,259 --> 00:25:47,940
send a notification to a user or an

00:25:46,259 --> 00:25:49,200
authorized user really because we have a

00:25:47,940 --> 00:25:51,119
permission model so we can make sure

00:25:49,200 --> 00:25:53,459
that we're only allowing certain users

00:25:51,119 --> 00:25:55,619
to approve those manual judgments and

00:25:53,459 --> 00:25:59,190
allow the pipeline to continue so the

00:25:55,619 --> 00:26:00,690
user can come in and provide an input to

00:25:59,190 --> 00:26:03,269
that judgment to determine whether it

00:26:00,690 --> 00:26:05,609
should allow the pipeline to continue or

00:26:03,269 --> 00:26:08,009
to stop or that input can also be used

00:26:05,609 --> 00:26:14,879
as a way to control branching within the

00:26:08,009 --> 00:26:16,799
pipeline so we've seen with the doctor

00:26:14,879 --> 00:26:19,289
registry trigger that we have this nice

00:26:16,799 --> 00:26:22,379
way to be able to initiate a pipeline

00:26:19,289 --> 00:26:24,139
whenever we have a new version of our

00:26:22,379 --> 00:26:26,099
doctor image that we want to deploy

00:26:24,139 --> 00:26:28,160
however sometimes we just want to deploy

00:26:26,099 --> 00:26:29,990
a configuration change

00:26:28,160 --> 00:26:32,840
like updating the environment variables

00:26:29,990 --> 00:26:35,570
for our application we would prefer to

00:26:32,840 --> 00:26:38,360
not have to go into the spinnaker UI and

00:26:35,570 --> 00:26:39,800
make those changes and then manually run

00:26:38,360 --> 00:26:42,290
the pipeline's in order to deploy

00:26:39,800 --> 00:26:43,700
configuration changes for one thing if

00:26:42,290 --> 00:26:45,380
we have configuration changes that

00:26:43,700 --> 00:26:47,990
affect a number of applications that's

00:26:45,380 --> 00:26:49,550
going to be a fairly tedious process but

00:26:47,990 --> 00:26:51,470
also we'd really like to keep that

00:26:49,550 --> 00:26:54,290
configuration externalized from

00:26:51,470 --> 00:26:57,080
spinnaker in version control so this way

00:26:54,290 --> 00:26:59,090
we can you know have pull requests to

00:26:57,080 --> 00:27:01,040
review configuration changes to make

00:26:59,090 --> 00:27:03,770
sure that they look okay and then also

00:27:01,040 --> 00:27:06,590
have access to the history of version

00:27:03,770 --> 00:27:09,860
changes so we can see how configuration

00:27:06,590 --> 00:27:13,250
has changed over time in spinnaker if we

00:27:09,860 --> 00:27:15,110
keep our configuration as JSON and a

00:27:13,250 --> 00:27:17,720
github repository and then use that

00:27:15,110 --> 00:27:21,620
github repository as a web hook trigger

00:27:17,720 --> 00:27:24,350
for our pipeline's when we get to the

00:27:21,620 --> 00:27:26,390
deploy stage we can edit the JSON of the

00:27:24,350 --> 00:27:30,200
pipeline directly instead of using that

00:27:26,390 --> 00:27:33,110
you know the form and in that we can

00:27:30,200 --> 00:27:35,450
kind of replace any arbitrary section of

00:27:33,110 --> 00:27:38,060
that JSON structure with the results of

00:27:35,450 --> 00:27:40,940
a pipeline execution so what we're doing

00:27:38,060 --> 00:27:42,230
here is in a deploy stage for the

00:27:40,940 --> 00:27:43,700
environment variables that we're going

00:27:42,230 --> 00:27:46,070
to use for our marathon application

00:27:43,700 --> 00:27:48,080
we're going to use this JSON from URL

00:27:46,070 --> 00:27:49,370
helper function but are a number of

00:27:48,080 --> 00:27:50,870
different helper functions that are

00:27:49,370 --> 00:27:54,410
available for pipeline expressions to

00:27:50,870 --> 00:27:56,660
use that downloads that JSON file from

00:27:54,410 --> 00:27:58,520
our github repository that was the one

00:27:56,660 --> 00:28:00,980
that triggered the pipeline and then

00:27:58,520 --> 00:28:03,170
inserts those values as the environment

00:28:00,980 --> 00:28:05,660
variables so we can really have kind of

00:28:03,170 --> 00:28:07,760
any arbitrary piece of our applications

00:28:05,660 --> 00:28:12,710
configuration come from our version

00:28:07,760 --> 00:28:15,830
control now so now I want to get back to

00:28:12,710 --> 00:28:18,770
the item that I raised earlier about how

00:28:15,830 --> 00:28:21,130
do we deal with load balancing in DCOs

00:28:18,770 --> 00:28:23,420
when we are using spinnaker to deploy

00:28:21,130 --> 00:28:24,980
separate marathon applications for every

00:28:23,420 --> 00:28:27,140
change

00:28:24,980 --> 00:28:29,570
so in spinnaker it really expects load

00:28:27,140 --> 00:28:31,220
balancers for providers to have a

00:28:29,570 --> 00:28:34,550
programmatic interface that it can

00:28:31,220 --> 00:28:36,560
control so think of lbs for AWS or

00:28:34,550 --> 00:28:39,650
services and ingress controllers for

00:28:36,560 --> 00:28:41,410
kubernetes however in DC us if we're

00:28:39,650 --> 00:28:43,450
using marathon lb we

00:28:41,410 --> 00:28:44,920
assign labels to our applications and

00:28:43,450 --> 00:28:46,780
let marathon I'll be automatically

00:28:44,920 --> 00:28:48,520
figure out the routing rules so it

00:28:46,780 --> 00:28:51,220
doesn't really work with that model that

00:28:48,520 --> 00:28:52,900
spinnaker wants to use to control load

00:28:51,220 --> 00:28:55,240
balancing during deployment strategies

00:28:52,900 --> 00:28:56,950
so this is somewhat inconvenient but we

00:28:55,240 --> 00:28:58,660
can kind of work around it by building

00:28:56,950 --> 00:29:01,180
additional stages into our pipelines

00:28:58,660 --> 00:29:02,410
that you know restart applications with

00:29:01,180 --> 00:29:04,900
different labels to change load

00:29:02,410 --> 00:29:07,450
balancing but we have a bigger problem

00:29:04,900 --> 00:29:10,150
with Marathon lb and that is that it

00:29:07,450 --> 00:29:12,130
doesn't support putting multiple

00:29:10,150 --> 00:29:15,160
marathon applications in the same

00:29:12,130 --> 00:29:16,750
back-end so if we had this old version

00:29:15,160 --> 00:29:18,790
and new version of our application

00:29:16,750 --> 00:29:20,710
running as two separate server groups

00:29:18,790 --> 00:29:22,870
deployed by spinnaker both of them

00:29:20,710 --> 00:29:24,730
trying to use the H a proxy V host label

00:29:22,870 --> 00:29:28,060
to say that they want traffic for the

00:29:24,730 --> 00:29:30,360
food example.com host marathon lb is

00:29:28,060 --> 00:29:32,410
only going to route to one of them

00:29:30,360 --> 00:29:34,300
unfortunately it's deterministic it's

00:29:32,410 --> 00:29:37,240
based on the sort order of the name of

00:29:34,300 --> 00:29:39,670
the application but when we actually

00:29:37,240 --> 00:29:42,250
want to flip over to the new application

00:29:39,670 --> 00:29:44,290
when we shut down that last instance of

00:29:42,250 --> 00:29:47,290
the old application there's this period

00:29:44,290 --> 00:29:49,390
of time where there's no instances there

00:29:47,290 --> 00:29:51,190
for us to be able to handle requests but

00:29:49,390 --> 00:29:52,870
Marathon lb hasn't yet gotten the event

00:29:51,190 --> 00:29:56,080
from the marathon event stream to

00:29:52,870 --> 00:29:58,330
reconfigure and point over to that new

00:29:56,080 --> 00:30:00,790
version so we're going to be dropping

00:29:58,330 --> 00:30:04,300
requests on every deployment that

00:30:00,790 --> 00:30:06,640
doesn't sound great so marathon lb is

00:30:04,300 --> 00:30:10,120
not not going to be a viable option for

00:30:06,640 --> 00:30:12,390
us to use with spinnaker fortunately we

00:30:10,120 --> 00:30:15,940
were able to find an alternative in

00:30:12,390 --> 00:30:17,860
traffic so traffic is another load

00:30:15,940 --> 00:30:20,320
balancer built with micro services in

00:30:17,860 --> 00:30:22,060
mind that supports getting routing

00:30:20,320 --> 00:30:24,640
configuration from a number of different

00:30:22,060 --> 00:30:29,250
dynamic backends like kubernetes and

00:30:24,640 --> 00:30:33,490
docker swarm and fortunately for us DCOs

00:30:29,250 --> 00:30:36,070
traffic uses the same kind of label

00:30:33,490 --> 00:30:38,500
based approach for determining routing

00:30:36,070 --> 00:30:40,540
rules as marathon lb and in fact also

00:30:38,500 --> 00:30:44,440
supports a subset of the marathon lb

00:30:40,540 --> 00:30:46,210
labels for compatibility but it also has

00:30:44,440 --> 00:30:48,610
its own additional set of labels like

00:30:46,210 --> 00:30:50,650
this traffic back-end label that if we

00:30:48,610 --> 00:30:51,580
put the same traffic back-end on two

00:30:50,650 --> 00:30:53,890
different marathon

00:30:51,580 --> 00:30:56,500
Asians traffic will round-robin request

00:30:53,890 --> 00:30:58,000
between both of those so now by using

00:30:56,500 --> 00:31:01,920
traffic with spinnaker we can get back

00:30:58,000 --> 00:31:01,920
to having zero downtime deployments

00:31:02,640 --> 00:31:07,150
unfortunately we still have that problem

00:31:04,630 --> 00:31:09,250
that we can't really provide exactly the

00:31:07,150 --> 00:31:11,400
same kind of baked in deployment

00:31:09,250 --> 00:31:14,590
strategies as other providers do through

00:31:11,400 --> 00:31:16,450
spinnaker load balancer interface since

00:31:14,590 --> 00:31:21,150
we're doing this with labels however

00:31:16,450 --> 00:31:24,130
there's hope in DCOs 110 the new edge lb

00:31:21,150 --> 00:31:27,790
external load balancing component does

00:31:24,130 --> 00:31:30,190
have a API that we can use and so this

00:31:27,790 --> 00:31:31,870
has just been released had to update my

00:31:30,190 --> 00:31:35,100
slides today when I realized it was no

00:31:31,870 --> 00:31:37,620
longer in beta but we are planning to

00:31:35,100 --> 00:31:41,710
very soon take on the work of

00:31:37,620 --> 00:31:43,450
integrating the edge lb as as the

00:31:41,710 --> 00:31:45,220
spinnaker load balancer and when we do

00:31:43,450 --> 00:31:47,740
that we can start providing deployment

00:31:45,220 --> 00:31:49,090
strategies that are kind of built in on

00:31:47,740 --> 00:31:54,490
par with some of the other providers

00:31:49,090 --> 00:31:57,340
that spinnaker works with so next I want

00:31:54,490 --> 00:31:59,080
to show kind of an example of putting a

00:31:57,340 --> 00:32:01,390
number of these pieces together and

00:31:59,080 --> 00:32:03,670
running a deployment for an application

00:32:01,390 --> 00:32:05,650
that starts with a commit to github and

00:32:03,670 --> 00:32:08,610
takes it all the way through a dev and

00:32:05,650 --> 00:32:11,950
production environment so here we have

00:32:08,610 --> 00:32:13,570
an application running a dev and a

00:32:11,950 --> 00:32:19,090
production cluster with a number of intz

00:32:13,570 --> 00:32:20,830
each of those so we can go and look at

00:32:19,090 --> 00:32:22,660
those dev and production endpoints we

00:32:20,830 --> 00:32:24,550
see this very fancy modern web

00:32:22,660 --> 00:32:28,990
application displaying an incredibly

00:32:24,550 --> 00:32:31,780
creative hello world message so when we

00:32:28,990 --> 00:32:34,170
want to push an update we'll go and look

00:32:31,780 --> 00:32:38,230
at that first pipeline this is our

00:32:34,170 --> 00:32:40,570
pipeline which is configured to start

00:32:38,230 --> 00:32:42,550
with a doctor registry push so we're

00:32:40,570 --> 00:32:45,700
defining what image we want to look at

00:32:42,550 --> 00:32:47,290
and telling it to watch doctor hub so

00:32:45,700 --> 00:32:48,610
since we've left that tag field blank

00:32:47,290 --> 00:32:52,930
it's going to be looking for just any

00:32:48,610 --> 00:32:54,250
new tag to get things started we can go

00:32:52,930 --> 00:32:56,590
and make a change

00:32:54,250 --> 00:32:58,720
in our code to replace hello world with

00:32:56,590 --> 00:33:01,870
a much more appropriate hello mesas con

00:32:58,720 --> 00:33:06,490
and then commit

00:33:01,870 --> 00:33:06,880
and tag this and what's going to happen

00:33:06,490 --> 00:33:09,700
next

00:33:06,880 --> 00:33:11,770
after we push is that we have docker hub

00:33:09,700 --> 00:33:13,630
set up to receive a web hook from this

00:33:11,770 --> 00:33:16,210
github repository and it's going to

00:33:13,630 --> 00:33:18,909
start building our new image using the

00:33:16,210 --> 00:33:22,870
same tag that was the tag that we pushed

00:33:18,909 --> 00:33:25,210
to github so dot 18 in this case so now

00:33:22,870 --> 00:33:26,650
it's building and while we wait for it

00:33:25,210 --> 00:33:29,230
to start we can go and see what's going

00:33:26,650 --> 00:33:31,840
to happen first and that is we're going

00:33:29,230 --> 00:33:33,720
to deploy this new version to dev since

00:33:31,840 --> 00:33:36,610
we didn't specify the tag ahead of time

00:33:33,720 --> 00:33:39,250
since we didn't know we can see here

00:33:36,610 --> 00:33:40,899
we're able to for the deploy stage tell

00:33:39,250 --> 00:33:44,220
it to use the image from trigger and let

00:33:40,899 --> 00:33:44,220
it resolve that tag at runtime

00:33:52,870 --> 00:33:57,610
so as the build of the image finishes

00:33:55,390 --> 00:33:59,830
our pipeline starts running and we're

00:33:57,610 --> 00:34:02,230
now spinnaker is now making calls to DC

00:33:59,830 --> 00:34:04,929
us to tell it to start deploying our new

00:34:02,230 --> 00:34:06,970
server group so we can see our dev

00:34:04,929 --> 00:34:09,520
version now has picked up that change

00:34:06,970 --> 00:34:11,950
and is showing the new version of our

00:34:09,520 --> 00:34:13,450
application and the next thing in our

00:34:11,950 --> 00:34:16,060
pipeline is going to be to execute a

00:34:13,450 --> 00:34:17,649
smoke test where we're using a metronome

00:34:16,060 --> 00:34:20,200
job where we're just going to run a

00:34:17,649 --> 00:34:29,859
simple curl against that dev endpoint to

00:34:20,200 --> 00:34:31,149
look for the appropriate response so we

00:34:29,859 --> 00:34:33,280
can see this is set up to either pass

00:34:31,149 --> 00:34:36,550
and continue on or fail and initiate a

00:34:33,280 --> 00:34:39,010
rollback if we look in dev we can see

00:34:36,550 --> 00:34:42,280
both of those server groups running with

00:34:39,010 --> 00:34:44,679
the old and new version and then if we

00:34:42,280 --> 00:34:46,780
look at how that pass and fail check

00:34:44,679 --> 00:34:49,149
actually works we can see we have

00:34:46,780 --> 00:34:51,669
another pipeline expression here that is

00:34:49,149 --> 00:34:55,089
looking at the results the status of

00:34:51,669 --> 00:34:56,830
that smoke test stage and determining if

00:34:55,089 --> 00:35:04,030
it's exceeded and allowing the pipeline

00:34:56,830 --> 00:35:05,619
to continue on so our smoke test has

00:35:04,030 --> 00:35:07,450
passed we're now on to destroying that

00:35:05,619 --> 00:35:09,280
old version we're gonna be shutting down

00:35:07,450 --> 00:35:12,910
the the hello world version that was

00:35:09,280 --> 00:35:14,589
running previously which in our cluster

00:35:12,910 --> 00:35:18,190
view we can see that that's just shut

00:35:14,589 --> 00:35:20,950
down and then the last thing that our

00:35:18,190 --> 00:35:22,720
dev pipeline is going to do after that

00:35:20,950 --> 00:35:25,420
finishes up is we're going to have a

00:35:22,720 --> 00:35:27,670
pipeline stage at the end that is our

00:35:25,420 --> 00:35:32,710
deployed to prod pipeline so we're going

00:35:27,670 --> 00:35:34,510
to now be initiating another pipeline in

00:35:32,710 --> 00:35:35,650
our production pipeline the first thing

00:35:34,510 --> 00:35:37,780
that we need to figure out is how to

00:35:35,650 --> 00:35:40,500
tell it what image to deploy since this

00:35:37,780 --> 00:35:43,150
wasn't triggered by doctor registry push

00:35:40,500 --> 00:35:44,770
so for that purpose we have a find image

00:35:43,150 --> 00:35:46,720
from cluster stage where we can tell

00:35:44,770 --> 00:35:48,550
spinnaker to look at a server group and

00:35:46,720 --> 00:35:51,339
another running cluster in this case our

00:35:48,550 --> 00:35:52,450
dev cluster and pick up that image that

00:35:51,339 --> 00:35:53,890
was running there so we can know that

00:35:52,450 --> 00:35:55,240
we're taking the version that we've just

00:35:53,890 --> 00:35:56,609
deployed to dev and taking that into

00:35:55,240 --> 00:36:00,250
production

00:35:56,609 --> 00:36:02,710
the first thing we do now is one final

00:36:00,250 --> 00:36:04,140
check before we deploy this live behind

00:36:02,710 --> 00:36:07,030
our production endpoint we're going to

00:36:04,140 --> 00:36:08,410
deploy at this latest

00:36:07,030 --> 00:36:09,670
in point instead of just a pride in

00:36:08,410 --> 00:36:13,900
point to kind of give us a chance to

00:36:09,670 --> 00:36:15,760
make sure everything looks good and the

00:36:13,900 --> 00:36:19,030
way that that has been set up to work is

00:36:15,760 --> 00:36:22,870
through a traffic label on our marathon

00:36:19,030 --> 00:36:24,790
application or we're telling traffic to

00:36:22,870 --> 00:36:29,190
take requests for this hostname and

00:36:24,790 --> 00:36:29,190
route it to this kind of new version

00:36:30,930 --> 00:36:36,790
after that runs we're now going to have

00:36:33,070 --> 00:36:39,100
a manual judgment stage in our pipeline

00:36:36,790 --> 00:36:40,630
so now we have to go and say I get an

00:36:39,100 --> 00:36:42,400
email that says I need to go verify

00:36:40,630 --> 00:36:44,110
something when I click on the email I

00:36:42,400 --> 00:36:47,170
get a link that will take me directly to

00:36:44,110 --> 00:36:50,230
the point in the pipeline that I need to

00:36:47,170 --> 00:36:52,480
provide my judgment on so since we saw

00:36:50,230 --> 00:36:54,010
earlier that everything looked good the

00:36:52,480 --> 00:36:59,140
hello mrs. Kahn will let that pass and

00:36:54,010 --> 00:37:00,810
continue on so as that continues now

00:36:59,140 --> 00:37:02,950
we're going to actually deploy our first

00:37:00,810 --> 00:37:05,860
instance behind the production load

00:37:02,950 --> 00:37:07,480
balancer we're all going to deploy a

00:37:05,860 --> 00:37:09,850
single instance rather than at full

00:37:07,480 --> 00:37:11,260
capacity and the way that we make sure

00:37:09,850 --> 00:37:12,910
to put it behind that production

00:37:11,260 --> 00:37:15,340
endpoint is with that traffic back-end

00:37:12,910 --> 00:37:17,500
label Broadway shows con that matches

00:37:15,340 --> 00:37:21,430
the label on that version that's already

00:37:17,500 --> 00:37:22,780
running so as that deployment proceeds

00:37:21,430 --> 00:37:24,430
we can then start to see as this

00:37:22,780 --> 00:37:26,410
refreshes that now requests are being

00:37:24,430 --> 00:37:32,140
round-robin between the old and new

00:37:26,410 --> 00:37:34,420
version of the application and then we

00:37:32,140 --> 00:37:36,130
had another run job staged to kind of

00:37:34,420 --> 00:37:38,680
validate that everything looks good and

00:37:36,130 --> 00:37:41,380
after that passes now we start scaling

00:37:38,680 --> 00:37:43,960
up our new version from one instance to

00:37:41,380 --> 00:37:46,090
five to get it to full capacity and then

00:37:43,960 --> 00:37:48,520
after that completes we start shutting

00:37:46,090 --> 00:37:50,230
down the old version until now

00:37:48,520 --> 00:37:55,540
production is running entirely on the

00:37:50,230 --> 00:37:57,400
new version of our application so if

00:37:55,540 --> 00:37:59,500
this has been interesting and you'd like

00:37:57,400 --> 00:38:01,210
to try it out the DC OS integration is

00:37:59,500 --> 00:38:04,210
available in the latest release of

00:38:01,210 --> 00:38:06,670
spinnaker 1.3 release just kind of a

00:38:04,210 --> 00:38:09,180
caveat about differences between open

00:38:06,670 --> 00:38:12,850
source and Enterprise DCOs at cerner we

00:38:09,180 --> 00:38:14,140
DCOs and so that's kind of been the

00:38:12,850 --> 00:38:16,870
version that we focused on supporting

00:38:14,140 --> 00:38:18,280
features for so right now with open

00:38:16,870 --> 00:38:20,020
source the difference is that the

00:38:18,280 --> 00:38:23,020
authentication methods and open source

00:38:20,020 --> 00:38:24,100
gcos are not supported so for now in

00:38:23,020 --> 00:38:26,230
order to try it out with open-source

00:38:24,100 --> 00:38:28,510
DCOs it will only work on a cluster with

00:38:26,230 --> 00:38:30,130
authentication disabled so not

00:38:28,510 --> 00:38:35,310
necessarily a production ready solution

00:38:30,130 --> 00:38:35,310
there but ready to try out at any rate I

00:38:35,670 --> 00:38:39,310
hope this has been a good demonstration

00:38:37,540 --> 00:38:42,030
of why we think spinnaker is so valuable

00:38:39,310 --> 00:38:44,590
for bringing safe automated and flexible

00:38:42,030 --> 00:38:46,330
continuous delivery pipe lines to DCOs

00:38:44,590 --> 00:38:49,930
if you want to learn more about getting

00:38:46,330 --> 00:38:51,580
started the spinnaker IO site has a

00:38:49,930 --> 00:38:53,860
number of resources including kind of a

00:38:51,580 --> 00:38:56,230
QuickStart and a code lab for getting

00:38:53,860 --> 00:38:58,390
things running on DCOs and then

00:38:56,230 --> 00:39:00,790
questions can be directed to the slack

00:38:58,390 --> 00:39:02,560
channel as well also if you're

00:39:00,790 --> 00:39:04,720
interested in solving challenging

00:39:02,560 --> 00:39:06,880
healthcare problems that can improve

00:39:04,720 --> 00:39:07,900
health care for millions of people you

00:39:06,880 --> 00:39:12,010
should come work at Cerner

00:39:07,900 --> 00:39:13,930
a shameless plug there but other than

00:39:12,010 --> 00:39:23,140
that that's what I've got so thank you

00:39:13,930 --> 00:39:24,820
and enjoy the rest of May so skon we

00:39:23,140 --> 00:39:30,760
have time for one question or two

00:39:24,820 --> 00:39:33,940
question maybe yeah hi I saw that you

00:39:30,760 --> 00:39:36,310
talked a lot about docker containers in

00:39:33,940 --> 00:39:38,590
high level applications do you how do

00:39:36,310 --> 00:39:41,440
you manage the underlying infrastructure

00:39:38,590 --> 00:39:45,550
do you manage your own servers or is

00:39:41,440 --> 00:39:47,170
this all cloud base so we have a couple

00:39:45,550 --> 00:39:48,760
of different we have some deployments in

00:39:47,170 --> 00:39:51,430
the cloud and then also deployments in

00:39:48,760 --> 00:39:54,040
our data center using OpenStack right

00:39:51,430 --> 00:39:59,200
now we are looking we're trying to get

00:39:54,040 --> 00:40:01,830
to the point ploy but yeah that's how

00:39:59,200 --> 00:40:01,830
we've been running it

00:40:07,880 --> 00:40:13,740
how do you handle disaster recovery do

00:40:10,319 --> 00:40:16,170
you have a dedicated cloth DCOs cluster

00:40:13,740 --> 00:40:19,589
in a dr facility and if so do you use

00:40:16,170 --> 00:40:22,980
this to deploy to it say yeah that's our

00:40:19,589 --> 00:40:25,020
plan with the you know ability for one

00:40:22,980 --> 00:40:26,700
spinnaker to controlled appointments to

00:40:25,020 --> 00:40:28,680
multiple dcs clusters we want to have

00:40:26,700 --> 00:40:29,940
have that backup cluster available and

00:40:28,680 --> 00:40:32,069
ready with nothing running in it and

00:40:29,940 --> 00:40:33,960
then just be able to update the

00:40:32,069 --> 00:40:36,000
spinnaker pipeline configuration to say

00:40:33,960 --> 00:40:37,559
nope go to go to this cluster now and

00:40:36,000 --> 00:40:43,920
then bring the applications up in the

00:40:37,559 --> 00:40:44,460
new one less question yeah this looks

00:40:43,920 --> 00:40:46,740
great

00:40:44,460 --> 00:40:49,950
I didn't historically ignored Netflix

00:40:46,740 --> 00:40:52,440
stuff cuz it was all ami based are you

00:40:49,950 --> 00:40:55,470
guys still the sole contributor or is

00:40:52,440 --> 00:40:59,460
the community adopting this as a tool

00:40:55,470 --> 00:41:00,980
for DCOs and me so us right now we're

00:40:59,460 --> 00:41:04,049
the sole contributor for the TCOs

00:41:00,980 --> 00:41:06,299
feature in spinnaker it just it was just

00:41:04,049 --> 00:41:08,069
released a few days ago but hopefully

00:41:06,299 --> 00:41:09,630
yeah we're we're wanting to get more

00:41:08,069 --> 00:41:11,549
involvement and you know it'd be great

00:41:09,630 --> 00:41:14,839
to have more support for better support

00:41:11,549 --> 00:41:14,839
for open source TCS as well

00:41:17,660 --> 00:41:23,010

YouTube URL: https://www.youtube.com/watch?v=F3fArY9CxEE


