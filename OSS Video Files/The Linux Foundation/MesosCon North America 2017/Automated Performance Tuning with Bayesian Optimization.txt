Title: Automated Performance Tuning with Bayesian Optimization
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Automated Performance Tuning with Bayesian Optimization - Joshua Cohen & Ramki Ramakrishna, Twitter

Managing resource utilization is one of the hardest aspects of operating Twitterâ€™s Mesos clusters. As the number of services grows and their resource shapes diversify, the bin packing problem becomes increasingly difficult. Tuning for optimal performance would reduce resource usage, and ease the bin packing burden. However, the multitude of available knobs, heterogeneous hardware, the large number of services, and software and hardware upgrades together make the tuning problem combinatorially intractable.

At Twitter we are developing a system that continuously performs automated tuning of services running in our Mesos clusters, using a machine learning technique called Bayesian optimization. This technique allows us to efficiently search very large parameter spaces to optimize specific performance metrics. We describe our system and share initial results.

About 

Joshua Cohen
Twitter
Twitter
Joshua Cohen is a Senior Software Engineer at Twitter on the VM Team, working on performance optimization of JVM services. He is also a committer and PMC member for the Apache Aurora project where he has focused on deploy tooling and filesystem isolation. Previously, amongst other places, he has worked at Flickr on notifications infrastructure and the image processing pipeline.

Ramki Ramakrishna
Twitter
Staff Software Engineer
San Francisco, CA
Ramki Ramakrishna is a staff software engineer in the Infrastructure Engineering Division of Twitter. He is a member of the JVM Platform team and of the Twitter Architecture Group. Ramki has worked with several generations of the JVM, at Sun and Oracle, before Twitter. He has been a committer and reviewer for the HotSpot group in OpenJDK. His principal contributions have been in the areas of performance analysis, tuning and adaptive optimization, parallel and concurrent garbage collection, and the synchronization infrastructure within the JVM. Before joining industry, Ramki worked at SUNY Stony Brook, the Tata Institute of Fundamental Research in India, and Aalborg University in Denmark, dividing time between teaching and research into the formal verification of concurrent systems, using process algebras, temporal logics and automatic theorem-proving. Ramki holds a Ph.D. in Electrical and Computer Engineering from the University of California at Santa Barbara, and a B.Tech. in Electrical Engineering from IIT Kanpur in India.
Captions: 
	00:00:00,179 --> 00:00:05,179
good afternoon everyone thanks for

00:00:01,949 --> 00:00:05,179
coming for this session

00:00:05,520 --> 00:00:10,980
my name is romkey ramakrishna and this

00:00:07,680 --> 00:00:13,320
is Joshua Cohen we are at Twitter with

00:00:10,980 --> 00:00:16,590
platform engineering and today we're

00:00:13,320 --> 00:00:19,400
going to talk about how you you might

00:00:16,590 --> 00:00:23,070
apply some neat machine learning

00:00:19,400 --> 00:00:26,609
algorithm what's called Bayesian

00:00:23,070 --> 00:00:28,680
optimization to the problem of tuning

00:00:26,609 --> 00:00:39,930
performance in an automated manner in

00:00:28,680 --> 00:00:43,950
the data center right so let's see the

00:00:39,930 --> 00:00:45,600
slides a bit light over here but this

00:00:43,950 --> 00:00:48,780
represents the service graph of twitter

00:00:45,600 --> 00:00:52,800
twitter runs on microservices and along

00:00:48,780 --> 00:00:55,320
the edge of this circle is the set of

00:00:52,800 --> 00:00:59,309
micro services that together make up the

00:00:55,320 --> 00:01:01,379
Twitter experience the edges over there

00:00:59,309 --> 00:01:05,060
are the edges of the service graph so

00:01:01,379 --> 00:01:07,290
that for example in in the with the

00:01:05,060 --> 00:01:09,930
magnified portion of over there showing

00:01:07,290 --> 00:01:12,150
the TFE router edges going from the TFE

00:01:09,930 --> 00:01:15,360
router to the other micro services

00:01:12,150 --> 00:01:17,520
represent the calls that the that that

00:01:15,360 --> 00:01:22,590
micro service might make to its

00:01:17,520 --> 00:01:24,509
downstream downstream services and in

00:01:22,590 --> 00:01:27,420
turn those might make further calls to

00:01:24,509 --> 00:01:30,090
other micro services and and thus

00:01:27,420 --> 00:01:34,259
together satisfy the requests that came

00:01:30,090 --> 00:01:37,710
in at the at the front end it turns out

00:01:34,259 --> 00:01:39,119
that as you can tell from the you know

00:01:37,710 --> 00:01:40,770
from the number of services that are

00:01:39,119 --> 00:01:42,630
listed over here there's a very large

00:01:40,770 --> 00:01:47,009
number of services that together make up

00:01:42,630 --> 00:01:49,729
twitter and it's about a thousand you

00:01:47,009 --> 00:01:52,799
know several thousand service services

00:01:49,729 --> 00:01:56,210
microcircuits mic services that together

00:01:52,799 --> 00:02:00,750
make up twitter and for each of these

00:01:56,210 --> 00:02:03,630
micro services they might be several

00:02:00,750 --> 00:02:06,840
instances horizontally scaled instances

00:02:03,630 --> 00:02:08,879
that together make up each service and

00:02:06,840 --> 00:02:10,830
of course these services might come in

00:02:08,879 --> 00:02:12,500
different sizes depending on how much

00:02:10,830 --> 00:02:15,900
load they have to

00:02:12,500 --> 00:02:18,900
how many requests they need to service

00:02:15,900 --> 00:02:20,790
and based upon that there's if you look

00:02:18,900 --> 00:02:22,920
at all of the service instances running

00:02:20,790 --> 00:02:25,189
in the data center at any time there

00:02:22,920 --> 00:02:29,510
might be upwards of several hundred

00:02:25,189 --> 00:02:34,019
thousand such service instances running

00:02:29,510 --> 00:02:39,689
think of a service instance as a as a as

00:02:34,019 --> 00:02:43,730
a unix process for example now many of

00:02:39,689 --> 00:02:45,930
our services are actually built out of

00:02:43,730 --> 00:02:48,090
they run atop the java virtual machine

00:02:45,930 --> 00:02:52,980
so one way of thinking of the service

00:02:48,090 --> 00:02:54,780
instance is as a JVM there are of course

00:02:52,980 --> 00:02:57,180
non JVM service instances as well

00:02:54,780 --> 00:03:00,269
running as well but the JVM makes up a

00:02:57,180 --> 00:03:03,510
large percentage of the of the services

00:03:00,269 --> 00:03:05,970
that we run these services run on

00:03:03,510 --> 00:03:09,120
heterogeneous hardware there could be

00:03:05,970 --> 00:03:11,909
several generations of servers in our

00:03:09,120 --> 00:03:13,680
data center and they're all taking there

00:03:11,909 --> 00:03:16,799
all our service instances might be

00:03:13,680 --> 00:03:18,870
scheduled on what different kinds of

00:03:16,799 --> 00:03:23,430
hardware so at anytime a service

00:03:18,870 --> 00:03:27,049
instance has no idea which which piece

00:03:23,430 --> 00:03:30,060
of hardware it might be sheduled on and

00:03:27,049 --> 00:03:32,879
in addition to that depending on what

00:03:30,060 --> 00:03:34,909
the service does it might have different

00:03:32,879 --> 00:03:38,489
resource requirements it might require

00:03:34,909 --> 00:03:41,400
either large large amount of memory or a

00:03:38,489 --> 00:03:44,609
small memory depending on how much load

00:03:41,400 --> 00:03:48,209
it serves it might make use of several

00:03:44,609 --> 00:03:51,060
cores or only you know a few cores

00:03:48,209 --> 00:03:54,150
depending on how much parallelism how

00:03:51,060 --> 00:03:56,250
much concurrency it supports and so the

00:03:54,150 --> 00:04:00,540
data center actually is composed of many

00:03:56,250 --> 00:04:06,299
many different service instances running

00:04:00,540 --> 00:04:10,530
different flavors of applications here

00:04:06,299 --> 00:04:13,079
is a typical performance stack of any

00:04:10,530 --> 00:04:15,959
particular instance so if you if I look

00:04:13,079 --> 00:04:18,959
at this stack over here at the bottom is

00:04:15,959 --> 00:04:21,090
the hardware of the host and the kernel

00:04:18,959 --> 00:04:24,599
and OS running on top of the hardware

00:04:21,090 --> 00:04:26,430
and in this particular case we have 2

00:04:24,599 --> 00:04:29,970
meses containers

00:04:26,430 --> 00:04:32,330
running on that host and each of these

00:04:29,970 --> 00:04:35,190
containers in turns running a JVM

00:04:32,330 --> 00:04:38,190
process and the Micra service is

00:04:35,190 --> 00:04:41,940
executing on top of the JVM so it's a

00:04:38,190 --> 00:04:45,300
layered system and each layer of this of

00:04:41,940 --> 00:04:47,160
this service stack might expose a set of

00:04:45,300 --> 00:04:49,790
parameters which you might tune for

00:04:47,160 --> 00:04:53,550
optimal performance of the micro service

00:04:49,790 --> 00:04:56,280
here I list for example h1 h2 as being

00:04:53,550 --> 00:04:58,710
the hardware parameters that might be

00:04:56,280 --> 00:05:00,060
tunable of course that tuning may not be

00:04:58,710 --> 00:05:01,500
done dynamically maybe it's something

00:05:00,060 --> 00:05:04,500
that you set when you release the

00:05:01,500 --> 00:05:06,120
machine into the data center the kernel

00:05:04,500 --> 00:05:09,240
might be - might have several chip might

00:05:06,120 --> 00:05:10,650
expose certain tunable x' there the

00:05:09,240 --> 00:05:13,350
maysa container might have some

00:05:10,650 --> 00:05:17,010
parameters set things such as the number

00:05:13,350 --> 00:05:18,480
of cores that the container can use the

00:05:17,010 --> 00:05:20,930
amount of memory that the container has

00:05:18,480 --> 00:05:23,280
the amount of network bandwidth that it

00:05:20,930 --> 00:05:27,930
that that the container is allowed to

00:05:23,280 --> 00:05:29,790
use and then the JVM itself has a whole

00:05:27,930 --> 00:05:33,060
lot of parameters and many of us are

00:05:29,790 --> 00:05:36,780
familiar with you know tuning various

00:05:33,060 --> 00:05:38,340
things such as the heap size and various

00:05:36,780 --> 00:05:40,830
other heap shaping parameters and so

00:05:38,340 --> 00:05:42,900
forth in order to maximize the

00:05:40,830 --> 00:05:45,450
performance of microservice and finally

00:05:42,900 --> 00:05:49,760
the micro service itself might be

00:05:45,450 --> 00:05:52,230
tunable in terms of how you know various

00:05:49,760 --> 00:05:55,620
service level parameters that you set in

00:05:52,230 --> 00:05:59,520
order to change the size or the user

00:05:55,620 --> 00:06:02,040
experience so the the performance of the

00:05:59,520 --> 00:06:05,340
microservice itself is can be considered

00:06:02,040 --> 00:06:07,860
a function f of the parameters lower

00:06:05,340 --> 00:06:12,870
down in the stack which I list here as h

00:06:07,860 --> 00:06:15,780
KJ m m and s and turning first to the

00:06:12,870 --> 00:06:19,650
JVM which is what we we are initially

00:06:15,780 --> 00:06:21,690
targeting the hotspot JVM has a has

00:06:19,650 --> 00:06:23,550
hundreds of parameters in this case I'm

00:06:21,690 --> 00:06:25,800
listing a few of them over here and if I

00:06:23,550 --> 00:06:28,580
look at the count of a version the JVM

00:06:25,800 --> 00:06:31,440
that's about probably a year old

00:06:28,580 --> 00:06:33,600
we find out they're about seven and 57

00:06:31,440 --> 00:06:34,890
tunable parameters now not all these

00:06:33,600 --> 00:06:36,780
parameters actually affect performance

00:06:34,890 --> 00:06:39,560
but there's a large percentage of them

00:06:36,780 --> 00:06:41,370
that they do so

00:06:39,560 --> 00:06:43,740
you could say that there are several

00:06:41,370 --> 00:06:45,000
hundred parameters that can be tuned and

00:06:43,740 --> 00:06:48,949
that might affect the performance of

00:06:45,000 --> 00:06:51,599
your of your application these

00:06:48,949 --> 00:06:53,930
parameters just even focusing on the JVM

00:06:51,599 --> 00:06:58,349
these parameters are of a you know

00:06:53,930 --> 00:07:00,539
there's lots of these parameters and the

00:06:58,349 --> 00:07:02,479
performance of your service might be

00:07:00,539 --> 00:07:06,389
sensitive to some but not to other

00:07:02,479 --> 00:07:08,460
parameters we don't a priori might well

00:07:06,389 --> 00:07:10,889
we the APR you may not know the

00:07:08,460 --> 00:07:12,840
performance sensitivity of your service

00:07:10,889 --> 00:07:15,360
to the settings of each of these

00:07:12,840 --> 00:07:17,940
parameters some of the parameters

00:07:15,360 --> 00:07:21,500
themselves are Hardware dependent so for

00:07:17,940 --> 00:07:24,719
example there might be something that

00:07:21,500 --> 00:07:26,990
tweaks the digestion time the JIT

00:07:24,719 --> 00:07:30,719
compiler and the JIT compiler might emit

00:07:26,990 --> 00:07:35,310
code that might be better suited to

00:07:30,719 --> 00:07:37,620
certain certain CPU instructions it

00:07:35,310 --> 00:07:39,719
might be able to make use of CP

00:07:37,620 --> 00:07:42,030
instructions for specific hardware

00:07:39,719 --> 00:07:43,860
better than with older hardware for

00:07:42,030 --> 00:07:45,659
example they might also be mutual

00:07:43,860 --> 00:07:47,669
interdependence amongst the parameters

00:07:45,659 --> 00:07:50,159
so for example I might say that oh I

00:07:47,669 --> 00:07:52,800
need a heap that is this big and then

00:07:50,159 --> 00:07:56,610
once I have fixed that it might be that

00:07:52,800 --> 00:07:58,380
the the young generation has to be tuned

00:07:56,610 --> 00:08:00,300
after you figured out what the size of

00:07:58,380 --> 00:08:02,069
the overall heap might be so there's

00:08:00,300 --> 00:08:07,919
lots of interdependencies amongst the

00:08:02,069 --> 00:08:10,440
parameters clearly hand tuning something

00:08:07,919 --> 00:08:13,949
of the scale of Twitter with hundreds of

00:08:10,440 --> 00:08:15,810
thousands of thousands of services each

00:08:13,949 --> 00:08:18,539
with different requirements and

00:08:15,810 --> 00:08:20,789
different performance characteristics is

00:08:18,539 --> 00:08:22,830
is a difficult exercise it's something

00:08:20,789 --> 00:08:26,639
that's impossible for any small team of

00:08:22,830 --> 00:08:28,289
engineers to do if what you can how you

00:08:26,639 --> 00:08:31,680
can scale down the problem as you say

00:08:28,289 --> 00:08:33,659
okay maybe the the top few parameters

00:08:31,680 --> 00:08:35,630
that that affect performance are these

00:08:33,659 --> 00:08:38,310
and then you kind of focus on those

00:08:35,630 --> 00:08:42,209
specific set of parameters and try to

00:08:38,310 --> 00:08:43,740
tune tune those manually but even if you

00:08:42,209 --> 00:08:46,140
were to do that something a manual

00:08:43,740 --> 00:08:48,959
exercise would be time consuming it

00:08:46,140 --> 00:08:50,940
would be labor-intensive and very likely

00:08:48,959 --> 00:08:51,730
error-prone especially as the number of

00:08:50,940 --> 00:08:53,800
parameters

00:08:51,730 --> 00:08:56,050
increases you have to keep track of what

00:08:53,800 --> 00:08:59,949
experiments you've run and try and plot

00:08:56,050 --> 00:09:05,130
a highly multi-dimensional performance

00:08:59,949 --> 00:09:08,230
surface as a result what happens is when

00:09:05,130 --> 00:09:09,880
new might reverse or ittin it is often

00:09:08,230 --> 00:09:13,029
the case that you look at some

00:09:09,880 --> 00:09:15,040
micro-service that resembles one that

00:09:13,029 --> 00:09:17,019
has already been written and so you say

00:09:15,040 --> 00:09:19,209
hey this set of parameters worked well

00:09:17,019 --> 00:09:21,399
for this service and my service kind of

00:09:19,209 --> 00:09:24,220
looks like like the other one and so I

00:09:21,399 --> 00:09:27,630
might be able to set my jaebeum's

00:09:24,220 --> 00:09:29,740
parameters like they said set theirs and

00:09:27,630 --> 00:09:32,470
as a result of this there's a lot of

00:09:29,740 --> 00:09:38,050
cargo counting of configurations that

00:09:32,470 --> 00:09:40,480
that that goes on and perhaps you have

00:09:38,050 --> 00:09:41,889
the time to tune this cargo calc ultd

00:09:40,480 --> 00:09:43,420
configuration before you launch your

00:09:41,889 --> 00:09:44,889
might service perhaps you don't you find

00:09:43,420 --> 00:09:50,350
that the performance is reasonable so

00:09:44,889 --> 00:09:52,170
I'm going to just make use of it and you

00:09:50,350 --> 00:09:54,970
don't have time to tune it for example

00:09:52,170 --> 00:09:56,740
but even if you spent a lot of time

00:09:54,970 --> 00:09:59,250
tuning something like this and said that

00:09:56,740 --> 00:10:02,410
ok I'll pick these five parameters tune

00:09:59,250 --> 00:10:04,120
micro-service for these parameters do a

00:10:02,410 --> 00:10:07,810
whole lot of experiments maybe I spend

00:10:04,120 --> 00:10:11,470
two or three weeks performance tuning

00:10:07,810 --> 00:10:15,579
this service by the time it's next week

00:10:11,470 --> 00:10:16,990
and the the service owners start rolling

00:10:15,579 --> 00:10:18,819
out there the next version of the

00:10:16,990 --> 00:10:23,110
service the performance characteristics

00:10:18,819 --> 00:10:25,949
have changed and so what was optimal two

00:10:23,110 --> 00:10:28,209
weeks ago may no longer be optimal now

00:10:25,949 --> 00:10:29,920
in addition to that because of all

00:10:28,209 --> 00:10:32,440
because of the layering of our stack and

00:10:29,920 --> 00:10:35,500
because of the fact that in a large data

00:10:32,440 --> 00:10:37,810
center we don't know which which

00:10:35,500 --> 00:10:40,149
hardware instance our service might be

00:10:37,810 --> 00:10:42,190
scheduled on and because the fact that

00:10:40,149 --> 00:10:43,449
they might be upgrades going on at

00:10:42,190 --> 00:10:45,760
various levels of the stack so for

00:10:43,449 --> 00:10:48,910
example I might be changing my version

00:10:45,760 --> 00:10:51,519
of Linux on in my data center from one

00:10:48,910 --> 00:10:55,240
version to the next I don't know a

00:10:51,519 --> 00:10:57,069
priori which of these two platforms my

00:10:55,240 --> 00:10:58,510
service will get scheduled on so there's

00:10:57,069 --> 00:11:00,220
a little bit of this there's quite a bit

00:10:58,510 --> 00:11:01,630
of you know there's a lot of churn going

00:11:00,220 --> 00:11:04,660
on in the data center there's a lot of

00:11:01,630 --> 00:11:05,350
heterogeneity and trying to get optimal

00:11:04,660 --> 00:11:07,600
performance

00:11:05,350 --> 00:11:11,380
cross the entire spectrum and making

00:11:07,600 --> 00:11:14,650
that be optimal over time is a difficult

00:11:11,380 --> 00:11:17,830
problem that know no amount of manual

00:11:14,650 --> 00:11:21,100
tuning will will will will address and

00:11:17,830 --> 00:11:23,770
so it turns out that if we follow the

00:11:21,100 --> 00:11:25,240
manual approach then it'll it it it goes

00:11:23,770 --> 00:11:26,950
without saying that many mike reverses

00:11:25,240 --> 00:11:32,410
will in fact be operating below

00:11:26,950 --> 00:11:34,830
optimality and so the question is how

00:11:32,410 --> 00:11:37,600
can we get a handle on this on this

00:11:34,830 --> 00:11:40,990
apparently intractable problem and make

00:11:37,600 --> 00:11:44,950
it automated so our approach is to

00:11:40,990 --> 00:11:48,130
actually you leverage some of the

00:11:44,950 --> 00:11:50,320
literature in industrial engineering and

00:11:48,130 --> 00:11:53,050
operations research which have looked at

00:11:50,320 --> 00:11:55,120
problems of optimizing engineering

00:11:53,050 --> 00:11:57,460
devices so for example where you have a

00:11:55,120 --> 00:12:00,100
device that exposes a set of knobs and

00:11:57,460 --> 00:12:01,930
by tuning the knobs you want to find the

00:12:00,100 --> 00:12:03,880
optimal operation set operational

00:12:01,930 --> 00:12:05,170
setting for this device it's an old

00:12:03,880 --> 00:12:08,950
problem that has been addressed in

00:12:05,170 --> 00:12:11,700
various engineering fields and the way

00:12:08,950 --> 00:12:14,110
to do it is to say hey the function f

00:12:11,700 --> 00:12:18,160
which is my which is the performance

00:12:14,110 --> 00:12:20,890
metric that I'm trying to optimize is is

00:12:18,160 --> 00:12:23,230
a function of x1 through xn which are

00:12:20,890 --> 00:12:26,080
the knobs that I'm going to tune and to

00:12:23,230 --> 00:12:29,200
turn and what my objective is is to find

00:12:26,080 --> 00:12:31,510
a configuration a which is the settings

00:12:29,200 --> 00:12:33,910
of x1 through xn to the values a 1

00:12:31,510 --> 00:12:37,990
through a n which is the knob settings

00:12:33,910 --> 00:12:43,990
such that my performance metric F is

00:12:37,990 --> 00:12:45,820
maximized but there might be constraints

00:12:43,990 --> 00:12:49,660
on this optimization problem so for

00:12:45,820 --> 00:12:53,140
example I even supposing I want max to

00:12:49,660 --> 00:12:55,750
maximize throughput of my of my service

00:12:53,140 --> 00:12:58,360
I might still have some constraints on

00:12:55,750 --> 00:13:01,120
the latency that that the service

00:12:58,360 --> 00:13:03,010
responses might might have to satisfy so

00:13:01,120 --> 00:13:05,230
there they could possibly be a

00:13:03,010 --> 00:13:07,120
constraint predicate G they could be a

00:13:05,230 --> 00:13:09,610
bunch of constraint predicates and I can

00:13:07,120 --> 00:13:12,690
actually represent it as the conjunction

00:13:09,610 --> 00:13:17,779
of a set of individual predicates that

00:13:12,690 --> 00:13:19,689
that must be satisfied for the for the

00:13:17,779 --> 00:13:23,680
optimal configuration to be acceptable

00:13:19,689 --> 00:13:28,100
so that's how we would model this

00:13:23,680 --> 00:13:30,560
performance optimization problem and

00:13:28,100 --> 00:13:31,999
then just to kind of give you a little

00:13:30,560 --> 00:13:34,100
bit more of a flavor of the kinds of

00:13:31,999 --> 00:13:37,970
constraints that we might want a model I

00:13:34,100 --> 00:13:40,910
might say that I this the the parameters

00:13:37,970 --> 00:13:43,220
X 1 and X 2 which are parameters to my

00:13:40,910 --> 00:13:45,740
performance function might should be in

00:13:43,220 --> 00:13:47,569
the relationship X 1 is less than X 2 so

00:13:45,740 --> 00:13:50,809
that kind of constraints constrains the

00:13:47,569 --> 00:13:53,329
the set of configurations that are

00:13:50,809 --> 00:13:56,629
acceptable and that reduces the space

00:13:53,329 --> 00:13:58,639
within which we must optimize in the set

00:13:56,629 --> 00:14:01,610
in the context of the JVM that might

00:13:58,639 --> 00:14:03,259
mean for example that size of the new

00:14:01,610 --> 00:14:05,660
generation of the young generation is

00:14:03,259 --> 00:14:07,790
smaller than the size of the heap or

00:14:05,660 --> 00:14:09,319
that for example there's something

00:14:07,790 --> 00:14:13,839
called the max tenuring threshold which

00:14:09,319 --> 00:14:16,879
determines how many times GC will will

00:14:13,839 --> 00:14:18,559
how many G sees an object has to survive

00:14:16,879 --> 00:14:21,800
before it gets promoted in the old

00:14:18,559 --> 00:14:24,649
generation and the JVM itself might set

00:14:21,800 --> 00:14:26,899
some limits on what the value of that

00:14:24,649 --> 00:14:29,839
setting might be there could be more

00:14:26,899 --> 00:14:32,269
complex constraints that that relate

00:14:29,839 --> 00:14:34,160
some complex function of one or two

00:14:32,269 --> 00:14:36,379
variables with another function of other

00:14:34,160 --> 00:14:37,850
variables and then the last one the

00:14:36,379 --> 00:14:42,889
constraints in the behavior of already

00:14:37,850 --> 00:14:45,439
talked about a little bit before so it

00:14:42,889 --> 00:14:47,930
but but further making thing prop the

00:14:45,439 --> 00:14:52,490
problem more difficult is that there are

00:14:47,930 --> 00:14:54,290
sets of of parameters that affect the

00:14:52,490 --> 00:14:56,899
performance of the function at the

00:14:54,290 --> 00:14:59,959
performance of my device there in other

00:14:56,899 --> 00:15:03,709
words the function f which we may have

00:14:59,959 --> 00:15:05,509
no control over so for example my

00:15:03,709 --> 00:15:07,550
service doesn't know which other service

00:15:05,509 --> 00:15:10,250
or which other container it might be

00:15:07,550 --> 00:15:11,929
co-hosted with on a specific host and

00:15:10,250 --> 00:15:13,639
they might be in to contain a crosstalk

00:15:11,929 --> 00:15:16,790
that I don't have any control explicit

00:15:13,639 --> 00:15:21,199
control over there might be variation in

00:15:16,790 --> 00:15:22,730
the in the load across time which I

00:15:21,199 --> 00:15:24,949
which I may not be able to control and

00:15:22,730 --> 00:15:26,750
it is possible that the optimal settings

00:15:24,949 --> 00:15:28,819
for one load are different from the

00:15:26,750 --> 00:15:30,650
optimal settings for another load and

00:15:28,819 --> 00:15:32,810
then I've already talked about

00:15:30,650 --> 00:15:34,960
not having not necessarily having

00:15:32,810 --> 00:15:38,180
control over the hardware where I'll be

00:15:34,960 --> 00:15:40,040
I'll be scheduled and so when there is

00:15:38,180 --> 00:15:43,400
uncertainty in these hidden parameters

00:15:40,040 --> 00:15:47,270
that appears as noise in your objective

00:15:43,400 --> 00:15:48,830
function so the performance tuning

00:15:47,270 --> 00:15:50,570
exercise itself would consist of

00:15:48,830 --> 00:15:54,560
designing a suitable performance metric

00:15:50,570 --> 00:15:56,870
and then decide deciding on and refining

00:15:54,560 --> 00:15:58,580
the set of knobs to tune and then using

00:15:56,870 --> 00:16:01,160
an iterative strategy to do these knobs

00:15:58,580 --> 00:16:04,730
and of course not all knobs are visible

00:16:01,160 --> 00:16:06,140
to us so pictorially that might look

00:16:04,730 --> 00:16:09,110
like this the performance engineer picks

00:16:06,140 --> 00:16:10,760
a set of knobs and picks settings for

00:16:09,110 --> 00:16:13,040
those knobs and then feeds and then

00:16:10,760 --> 00:16:14,630
starts up the system that he's trying to

00:16:13,040 --> 00:16:17,660
optimize he makes some performance

00:16:14,630 --> 00:16:21,410
measurements of the metric F that he's

00:16:17,660 --> 00:16:22,580
interested in optimizing and he makes a

00:16:21,410 --> 00:16:24,080
couple of these measurements with

00:16:22,580 --> 00:16:26,450
different settings and tries to figure

00:16:24,080 --> 00:16:32,690
out the shape of his of the performance

00:16:26,450 --> 00:16:35,270
surface but I can the question is can we

00:16:32,690 --> 00:16:37,000
automate this by having a back black box

00:16:35,270 --> 00:16:39,530
tuning assistance something that will

00:16:37,000 --> 00:16:41,510
that'll you know try different

00:16:39,530 --> 00:16:43,280
parameters look at the shape of the

00:16:41,510 --> 00:16:45,350
function and maybe learn what the

00:16:43,280 --> 00:16:46,970
function shape might be like and for

00:16:45,350 --> 00:16:50,350
complex systems the shape might be

00:16:46,970 --> 00:16:54,530
extremely complex it might be a highly

00:16:50,350 --> 00:16:58,730
multimodal nonlinear system as we as we

00:16:54,530 --> 00:17:01,490
know most of our customs are we make use

00:16:58,730 --> 00:17:03,620
of a of a technique called Bayesian

00:17:01,490 --> 00:17:06,800
optimization from the machine learning

00:17:03,620 --> 00:17:10,010
literature it's a statistical technique

00:17:06,800 --> 00:17:13,280
that is can be used to learn potentially

00:17:10,010 --> 00:17:16,340
noisy objective functions and we've

00:17:13,280 --> 00:17:19,670
already talked about how we might have

00:17:16,340 --> 00:17:21,860
noise in our system iteratively and

00:17:19,670 --> 00:17:24,680
efficiently and by efficiently what I

00:17:21,860 --> 00:17:28,640
mean here is that through a small number

00:17:24,680 --> 00:17:31,250
of of evaluations or experiments of the

00:17:28,640 --> 00:17:36,080
response surface we can quickly find our

00:17:31,250 --> 00:17:37,250
way to an optimum configuration and and

00:17:36,080 --> 00:17:39,560
that's what we are saying here that then

00:17:37,250 --> 00:17:41,780
that in a very small number of

00:17:39,560 --> 00:17:44,880
iterations we should be able to find an

00:17:41,780 --> 00:17:46,950
optimum setting for our parameter

00:17:44,880 --> 00:17:49,410
this technique has existed for a while

00:17:46,950 --> 00:17:52,289
it was first talked about in the 80s and

00:17:49,410 --> 00:17:56,690
by moccasined the former Soviet Union

00:17:52,289 --> 00:17:59,970
and then there's been more work recently

00:17:56,690 --> 00:18:01,559
and we are going to be leveraging work

00:17:59,970 --> 00:18:04,740
that was done at Harvard in Toronto a

00:18:01,559 --> 00:18:08,370
couple of years ago and which we have

00:18:04,740 --> 00:18:09,990
in-house today and this technique works

00:18:08,370 --> 00:18:12,360
well with nonlinear multimodal and

00:18:09,990 --> 00:18:16,740
highly dimensional functions so it won't

00:18:12,360 --> 00:18:19,169
get caught and local local maxima let me

00:18:16,740 --> 00:18:21,179
walk through a quick example to show how

00:18:19,169 --> 00:18:22,980
Bayes and optimization works in practice

00:18:21,179 --> 00:18:24,510
supposing we have taken three

00:18:22,980 --> 00:18:26,669
measurements of the system and those are

00:18:24,510 --> 00:18:29,000
the three so I'm here looking at a

00:18:26,669 --> 00:18:31,200
single single variable system

00:18:29,000 --> 00:18:33,900
performance shown on the y-axis over

00:18:31,200 --> 00:18:36,080
there and the the knob settings shown in

00:18:33,900 --> 00:18:39,480
the x-axis over here and we've had three

00:18:36,080 --> 00:18:42,809
evaluations that give us that that set

00:18:39,480 --> 00:18:44,490
of points we might look at this and say

00:18:42,809 --> 00:18:47,460
that maybe it's it's worthwhile to look

00:18:44,490 --> 00:18:50,909
at something that's of setting the value

00:18:47,460 --> 00:18:53,429
at minus 4 because presumably that that

00:18:50,909 --> 00:18:56,340
setting might be a might result in

00:18:53,429 --> 00:18:58,230
higher performance now it turns out and

00:18:56,340 --> 00:18:59,700
we do not know this it turns out that

00:18:58,230 --> 00:19:00,179
the actual function might have this

00:18:59,700 --> 00:19:02,520
shape

00:19:00,179 --> 00:19:05,460
which would still produce the same set

00:19:02,520 --> 00:19:08,370
of settings so here I'll walk through an

00:19:05,460 --> 00:19:10,289
example that that where Basin

00:19:08,370 --> 00:19:12,840
optimization will try and learn the

00:19:10,289 --> 00:19:14,580
shape of the surface as we go through so

00:19:12,840 --> 00:19:16,230
the way Basin optimisation learns is

00:19:14,580 --> 00:19:20,070
that it models the function the unknown

00:19:16,230 --> 00:19:21,419
function as a stochastic process as it's

00:19:20,070 --> 00:19:24,780
what's called a Gaussian process and

00:19:21,419 --> 00:19:27,980
this is a distribution it's a it's a

00:19:24,780 --> 00:19:32,100
probability distribution over functions

00:19:27,980 --> 00:19:34,200
in it one way to understand to to kind

00:19:32,100 --> 00:19:38,039
of think about this is that at any point

00:19:34,200 --> 00:19:42,120
of of the of this space of the settings

00:19:38,039 --> 00:19:46,020
of the input parameter there's there's a

00:19:42,120 --> 00:19:50,309
there's a distribution of values that

00:19:46,020 --> 00:19:54,720
that that the function might take so for

00:19:50,309 --> 00:19:57,360
example if I pick the setting to we are

00:19:54,720 --> 00:20:00,659
we we have some uncertainty as to where

00:19:57,360 --> 00:20:03,389
the the value of f2 would be and so that

00:20:00,659 --> 00:20:06,720
uncertainty is represented by this this

00:20:03,389 --> 00:20:09,090
cloud which is a distribution it's a

00:20:06,720 --> 00:20:11,489
normal distribution of it with a given

00:20:09,090 --> 00:20:14,070
mean and as we make more measurements

00:20:11,489 --> 00:20:15,720
this a probability distribution will get

00:20:14,070 --> 00:20:18,210
modified so we start with a certain

00:20:15,720 --> 00:20:20,489
prior distribution and as we make

00:20:18,210 --> 00:20:22,259
observations it gives us a posterior

00:20:20,489 --> 00:20:28,230
distribution and we does iteratively

00:20:22,259 --> 00:20:30,570
define and go forward so there's our so

00:20:28,230 --> 00:20:34,259
the way we we we figure out what the

00:20:30,570 --> 00:20:37,649
next point at which we should look to to

00:20:34,259 --> 00:20:39,419
make a measurement is that think of the

00:20:37,649 --> 00:20:42,659
best point that we've got so far shown

00:20:39,419 --> 00:20:45,659
over there at the top and we we slice

00:20:42,659 --> 00:20:49,559
the probability distribution and look at

00:20:45,659 --> 00:20:50,909
the the mass of the of the of the

00:20:49,559 --> 00:20:53,609
probability distribution that's above it

00:20:50,909 --> 00:20:56,460
and so if I take any specific point over

00:20:53,609 --> 00:20:59,129
here and think of the probability of an

00:20:56,460 --> 00:21:00,629
improvement it would be the sum the the

00:20:59,129 --> 00:21:03,450
integral of the probability distribution

00:21:00,629 --> 00:21:05,789
that lies above the curve and that's

00:21:03,450 --> 00:21:07,619
true of each each point now I could use

00:21:05,789 --> 00:21:10,320
the probability of improvement as

00:21:07,619 --> 00:21:12,720
something that would drive the choice of

00:21:10,320 --> 00:21:15,299
the next point it turns out that people

00:21:12,720 --> 00:21:16,980
have have looked at this and found that

00:21:15,299 --> 00:21:20,549
expected improvement might be a better

00:21:16,980 --> 00:21:23,159
measure so for example I might say that

00:21:20,549 --> 00:21:25,769
the value of the improvement times the

00:21:23,159 --> 00:21:27,389
probability integrated over the slice

00:21:25,769 --> 00:21:30,149
that the part that lies above it would

00:21:27,389 --> 00:21:31,619
would give us a much better measure so

00:21:30,149 --> 00:21:39,090
here we've plotted the expected

00:21:31,619 --> 00:21:41,789
improvement of that of this of the

00:21:39,090 --> 00:21:43,710
Gaussian process and given the value

00:21:41,789 --> 00:21:47,159
that that the best value that we have so

00:21:43,710 --> 00:21:48,809
far and then there's a there's a there's

00:21:47,159 --> 00:21:51,330
it turns out that it's mathematically

00:21:48,809 --> 00:21:52,470
easy to optimize this function which is

00:21:51,330 --> 00:21:55,169
called a surrogate function the

00:21:52,470 --> 00:21:58,440
acquisition function and so it turns out

00:21:55,169 --> 00:22:00,090
that this point would maximize the

00:21:58,440 --> 00:22:03,509
expected improvement and so we pick that

00:22:00,090 --> 00:22:04,980
point and proceed from there we make a

00:22:03,509 --> 00:22:06,899
measurement over there and we find that

00:22:04,980 --> 00:22:10,259
it actually doesn't mean to improve our

00:22:06,899 --> 00:22:10,840
performance much but we keep doing this

00:22:10,259 --> 00:22:13,180
at

00:22:10,840 --> 00:22:15,220
every point refining the probability

00:22:13,180 --> 00:22:19,380
distribution the Gaussian process that

00:22:15,220 --> 00:22:22,630
we have and over a couple of such

00:22:19,380 --> 00:22:26,350
iterations every time picking the best

00:22:22,630 --> 00:22:29,400
point that we that the optimum optimum

00:22:26,350 --> 00:22:31,720
optimization of the expected improvement

00:22:29,400 --> 00:22:36,400
acquisition function gives us we

00:22:31,720 --> 00:22:40,030
eventually find our way to the to

00:22:36,400 --> 00:22:42,190
somewhere near the optimum one thing

00:22:40,030 --> 00:22:44,050
that I want you to note over here is

00:22:42,190 --> 00:22:47,020
that there are points at which we

00:22:44,050 --> 00:22:48,580
actually pick up and do and evaluate the

00:22:47,020 --> 00:22:50,500
function at points where it actually

00:22:48,580 --> 00:22:53,430
performs much worse than then the best

00:22:50,500 --> 00:22:53,430
point that we had so far

00:22:54,510 --> 00:22:58,000
there are several alternate approaches I

00:22:56,830 --> 00:23:00,010
won't go into the details of this

00:22:58,000 --> 00:23:02,680
because I'll probably run out of time

00:23:00,010 --> 00:23:04,810
but a Bayesian optimization has been

00:23:02,680 --> 00:23:08,110
used at Twitter for a number of

00:23:04,810 --> 00:23:09,910
optimization problems I list a few of

00:23:08,110 --> 00:23:12,280
them over here and we are applying it

00:23:09,910 --> 00:23:16,210
now to JVM performance tuning in the

00:23:12,280 --> 00:23:17,770
data center note that this kind of

00:23:16,210 --> 00:23:20,170
technique could be applied at pretty

00:23:17,770 --> 00:23:22,870
much every any layer of the stack we are

00:23:20,170 --> 00:23:25,420
now focusing mainly on the JVM and so

00:23:22,870 --> 00:23:27,100
that's all that I'm going to focus on

00:23:25,420 --> 00:23:28,570
now but there's no reason why the same

00:23:27,100 --> 00:23:32,530
technique would not extend and be

00:23:28,570 --> 00:23:34,240
applicable to almost anything on the in

00:23:32,530 --> 00:23:37,120
the performance stack including for

00:23:34,240 --> 00:23:41,220
example the the mesas the container

00:23:37,120 --> 00:23:44,080
level somehow got skipped in this slide

00:23:41,220 --> 00:23:47,260
so I'll actually very quickly go through

00:23:44,080 --> 00:23:50,500
a proof of concept that we did where we

00:23:47,260 --> 00:23:52,750
picked a bunch of JVM parameters some of

00:23:50,500 --> 00:23:55,390
which I've listed over here for a total

00:23:52,750 --> 00:23:57,700
of about 30 30 parameters so taking

00:23:55,390 --> 00:24:01,990
these 30 parameters we ran B is an

00:23:57,700 --> 00:24:03,670
optimization over it and the set up was

00:24:01,990 --> 00:24:06,460
of a large production service to which

00:24:03,670 --> 00:24:08,910
we applied this but we didn't apply it

00:24:06,460 --> 00:24:12,310
in production we applied it in a in a

00:24:08,910 --> 00:24:14,200
staging environment and the performance

00:24:12,310 --> 00:24:15,820
metric that we were optimizing for is

00:24:14,200 --> 00:24:18,670
listed over here it's the number of

00:24:15,820 --> 00:24:22,120
requests per second divided by the GC

00:24:18,670 --> 00:24:23,830
cost and so intuitively we want to

00:24:22,120 --> 00:24:24,190
increase the throughput of the system

00:24:23,830 --> 00:24:25,899
and

00:24:24,190 --> 00:24:27,909
want to reduce the garbage collection

00:24:25,899 --> 00:24:32,379
cost and intuitively what that means is

00:24:27,909 --> 00:24:35,970
the latency of your requests would be

00:24:32,379 --> 00:24:40,240
minimized while throughput is increased

00:24:35,970 --> 00:24:42,490
this shows how the you know this on the

00:24:40,240 --> 00:24:44,019
x-axis is the iteration number and on

00:24:42,490 --> 00:24:47,830
the y-axis is the relative performance

00:24:44,019 --> 00:24:49,720
improvement with respect to the original

00:24:47,830 --> 00:24:53,559
settings that the service was running

00:24:49,720 --> 00:24:58,269
with and we can see that by about the

00:24:53,559 --> 00:25:00,070
probably the 20th or 25th iteration even

00:24:58,269 --> 00:25:01,929
though we are we are trying to tune in a

00:25:00,070 --> 00:25:03,190
system with 30 parameters each of which

00:25:01,929 --> 00:25:07,389
might take up to a hundred different

00:25:03,190 --> 00:25:10,690
values for a huge space like a hundred

00:25:07,389 --> 00:25:12,009
to the power of 30 even though we were

00:25:10,690 --> 00:25:13,899
searching over such a huge space and

00:25:12,009 --> 00:25:16,330
this could possibly be a highly

00:25:13,899 --> 00:25:18,669
nonlinear multimodal system within

00:25:16,330 --> 00:25:20,259
twenty iterations we had found our way

00:25:18,669 --> 00:25:23,440
to something that doubled the

00:25:20,259 --> 00:25:27,600
performance of the of the service and by

00:25:23,440 --> 00:25:29,649
this 70 whatever 78 iteration we had

00:25:27,600 --> 00:25:31,299
performance that was to point to you

00:25:29,649 --> 00:25:35,049
know to point to of the original

00:25:31,299 --> 00:25:37,929
performance so that's the optimum over

00:25:35,049 --> 00:25:41,980
there and I won't go into the details

00:25:37,929 --> 00:25:44,590
but basically it was the metric as you

00:25:41,980 --> 00:25:48,220
know over time the blue line represents

00:25:44,590 --> 00:25:49,659
the the tuned system the optimized

00:25:48,220 --> 00:25:53,350
system and the yellow line represents

00:25:49,659 --> 00:25:55,990
the unoptimized system and looking at

00:25:53,350 --> 00:25:57,700
the two parts of the optimization

00:25:55,990 --> 00:25:59,500
function one was the requests per second

00:25:57,700 --> 00:26:02,950
that was the incoming ambient traffic

00:25:59,500 --> 00:26:04,809
that is the same for both the optimized

00:26:02,950 --> 00:26:07,210
system and the non optimized system and

00:26:04,809 --> 00:26:11,830
the improvement was entirely because GC

00:26:07,210 --> 00:26:13,539
cost was reduced dramatically I won't

00:26:11,830 --> 00:26:15,759
actually go into the optimized settings

00:26:13,539 --> 00:26:19,149
I'll let you look over them in the end

00:26:15,759 --> 00:26:21,820
in the slides that we have but basically

00:26:19,149 --> 00:26:25,570
what one can take away from this is that

00:26:21,820 --> 00:26:27,190
if we can optimize if this optim

00:26:25,570 --> 00:26:28,750
optimization carried over well to

00:26:27,190 --> 00:26:30,519
production then we might be able to

00:26:28,750 --> 00:26:32,110
extract a data center footprint

00:26:30,519 --> 00:26:34,720
reduction and therefore a cost

00:26:32,110 --> 00:26:36,669
improvement in the data center and so

00:26:34,720 --> 00:26:38,850
that's really what we are trying to get

00:26:36,669 --> 00:26:38,850
at

00:26:39,190 --> 00:26:44,259
the key takeaways is that you have to be

00:26:41,379 --> 00:26:45,969
aware of Hardware heterogeneity you have

00:26:44,259 --> 00:26:47,950
to factor out Hardware effects when

00:26:45,969 --> 00:26:49,570
you're doing these experiments there

00:26:47,950 --> 00:26:51,009
might be load spikes and seasonality and

00:26:49,570 --> 00:26:52,839
your to factor those out by running

00:26:51,009 --> 00:26:56,859
sufficiently long experiments that will

00:26:52,839 --> 00:26:58,629
average that out you have to have

00:26:56,859 --> 00:27:00,039
baseline configurations so that you can

00:26:58,629 --> 00:27:01,269
normalize your evaluations this is

00:27:00,039 --> 00:27:04,629
something that I kind of mentioned a

00:27:01,269 --> 00:27:06,489
little bit before you have to run

00:27:04,629 --> 00:27:08,979
experiments long enough that long range

00:27:06,489 --> 00:27:11,349
effects appear so if you don't run your

00:27:08,979 --> 00:27:13,179
experiment sufficiently long then the

00:27:11,349 --> 00:27:14,950
long range effects may not be visible

00:27:13,179 --> 00:27:16,869
and you might what you thought was an

00:27:14,950 --> 00:27:19,109
optimum turns out not to be an optimum

00:27:16,869 --> 00:27:23,200
after you've run for about a day or more

00:27:19,109 --> 00:27:24,969
in order to increase convergence speed

00:27:23,200 --> 00:27:28,299
we want to reduce the amount of noise as

00:27:24,969 --> 00:27:32,379
well as to increase the quality of the

00:27:28,299 --> 00:27:37,419
optimum we want to make it make it

00:27:32,379 --> 00:27:39,759
robust with respect to to noise we can

00:27:37,419 --> 00:27:41,499
actually the technique that I'm you that

00:27:39,759 --> 00:27:44,099
we've we are using and in fact we

00:27:41,499 --> 00:27:46,839
paralyze so you could have several

00:27:44,099 --> 00:27:50,609
experiments running concurrently to

00:27:46,839 --> 00:27:53,109
speed up the convergence and finally

00:27:50,609 --> 00:27:55,389
suboptimal suggestions such as the one

00:27:53,109 --> 00:27:57,609
that I showed pointed out earlier you

00:27:55,389 --> 00:28:00,940
need to be able to detect those and and

00:27:57,609 --> 00:28:02,289
stop them early so that you don't affect

00:28:00,940 --> 00:28:03,549
the overall quality of service

00:28:02,289 --> 00:28:05,589
especially if you're running the

00:28:03,549 --> 00:28:07,479
experiment in production and the last

00:28:05,589 --> 00:28:08,919
bullet basically says that staging is

00:28:07,479 --> 00:28:11,529
not production if you optimize something

00:28:08,919 --> 00:28:14,739
in staging as we found out the hardware

00:28:11,529 --> 00:28:16,209
it doesn't translate automatically to

00:28:14,739 --> 00:28:18,639
production so it makes sense to run

00:28:16,209 --> 00:28:25,479
these evaluation experiments in

00:28:18,639 --> 00:28:28,419
production so I'll actually let Joshua

00:28:25,479 --> 00:28:29,979
now take over the talk and talk about an

00:28:28,419 --> 00:28:30,659
implementation of this that he's been

00:28:29,979 --> 00:28:37,119
doing

00:28:30,659 --> 00:28:38,379
thanks romkey so yeah so based on what

00:28:37,119 --> 00:28:41,559
we learned from the original prototype

00:28:38,379 --> 00:28:43,690
that was built we've started building a

00:28:41,559 --> 00:28:45,940
system that we call auto-tune it's a

00:28:43,690 --> 00:28:47,580
Bayesian optimization automated tuning

00:28:45,940 --> 00:28:50,909
service

00:28:47,580 --> 00:28:54,120
if you put that all together and it

00:28:50,909 --> 00:28:59,970
spells out boat so we like to hit these

00:28:54,120 --> 00:29:01,169
memes at the peak of their popularity we

00:28:59,970 --> 00:29:05,549
can also just call it an auto-tune as a

00:29:01,169 --> 00:29:07,409
service we take what we learned from our

00:29:05,549 --> 00:29:09,149
prototype and apply it to the general

00:29:07,409 --> 00:29:11,700
serve into a general service that anyone

00:29:09,149 --> 00:29:12,840
can use the key goal is that no coding

00:29:11,700 --> 00:29:15,600
is required it should just be

00:29:12,840 --> 00:29:17,580
configuration it should support any type

00:29:15,600 --> 00:29:22,649
of service and it should be running

00:29:17,580 --> 00:29:24,870
continuously or on demand and yeah it

00:29:22,649 --> 00:29:26,520
should just basically as you would want

00:29:24,870 --> 00:29:30,840
from any service be really easy for our

00:29:26,520 --> 00:29:32,190
engineers and SRE Strunz so this is an

00:29:30,840 --> 00:29:33,529
example of what an auto-tune

00:29:32,190 --> 00:29:36,240
configuration file looks like

00:29:33,529 --> 00:29:37,770
essentially you give it some parameters

00:29:36,240 --> 00:29:39,450
that you want to optimize for I'm just

00:29:37,770 --> 00:29:40,470
showing one here for the sake of space

00:29:39,450 --> 00:29:44,850
but it could be any number of parameters

00:29:40,470 --> 00:29:46,230
you give it an objective query which is

00:29:44,850 --> 00:29:49,740
essentially the function that we're

00:29:46,230 --> 00:29:53,220
trying to optimize a job key to identify

00:29:49,740 --> 00:29:55,350
this in our Arora mesas cluster the

00:29:53,220 --> 00:29:58,130
range of instances you want to

00:29:55,350 --> 00:30:01,549
experiment on and how long each

00:29:58,130 --> 00:30:04,649
experimental evaluation should run for

00:30:01,549 --> 00:30:07,860
this is just a brief systems diagram

00:30:04,649 --> 00:30:09,450
which isn't super interesting so so

00:30:07,860 --> 00:30:11,760
let's talk about how Aurora and Mesa

00:30:09,450 --> 00:30:15,750
sauce actually help us to build this

00:30:11,760 --> 00:30:18,390
auto-tune surface I assume everyone's

00:30:15,750 --> 00:30:19,770
familiar with mezzos for those who are

00:30:18,390 --> 00:30:21,929
not super familiar with Aurora

00:30:19,770 --> 00:30:23,429
it's amazing solar initially developed a

00:30:21,929 --> 00:30:26,850
Twitter later open sourced

00:30:23,429 --> 00:30:30,960
Pachi project it was designed for micro

00:30:26,850 --> 00:30:34,610
services so what does Aurora bring to

00:30:30,960 --> 00:30:38,580
the table that lets us build this system

00:30:34,610 --> 00:30:40,350
it's got homogeneous jobs instance level

00:30:38,580 --> 00:30:42,720
scheduling constraints programmatic

00:30:40,350 --> 00:30:44,610
access via the API fault tolerance

00:30:42,720 --> 00:30:46,830
automated deploys these are all things

00:30:44,610 --> 00:30:48,929
that make it easy to build our

00:30:46,830 --> 00:30:53,130
auto-tuned service on top of Aurora and

00:30:48,929 --> 00:30:55,559
mesas before we definitely deeper just a

00:30:53,130 --> 00:30:58,260
brief primer on how jobs are configured

00:30:55,559 --> 00:30:59,879
and executed by Aurora jobs are a

00:30:58,260 --> 00:31:02,969
top-level construct in Aurora

00:30:59,879 --> 00:31:05,779
posed of n identical tasks tasks

00:31:02,969 --> 00:31:08,489
themselves are composed of processes

00:31:05,779 --> 00:31:10,549
individual task execution is managed by

00:31:08,489 --> 00:31:14,849
roars executor which is called thermos

00:31:10,549 --> 00:31:16,499
Aurora itself is responsible for tasks

00:31:14,849 --> 00:31:18,599
being resilient to any failures either

00:31:16,499 --> 00:31:21,959
of the tasks themselves or host failures

00:31:18,599 --> 00:31:23,909
and Warren thermos together facilitate

00:31:21,959 --> 00:31:26,009
service discovery so other services in

00:31:23,909 --> 00:31:27,449
the data center can find those and talk

00:31:26,009 --> 00:31:30,079
to them which will be important when we

00:31:27,449 --> 00:31:32,789
want to send them production traffic

00:31:30,079 --> 00:31:36,690
Aurora Aurora also provides a thrift

00:31:32,789 --> 00:31:38,669
thrift RPC API that lets us have full

00:31:36,690 --> 00:31:44,789
control over anything we need to do when

00:31:38,669 --> 00:31:46,440
scheduling our experimental instances so

00:31:44,789 --> 00:31:49,469
this is sort of the overview of the

00:31:46,440 --> 00:31:53,190
process that we need to take when we go

00:31:49,469 --> 00:31:54,539
to launch experiments Aurora by the time

00:31:53,190 --> 00:31:57,599
we're talking to Aurora we already have

00:31:54,539 --> 00:32:00,959
our suggestions from Twitter's BAE's off

00:31:57,599 --> 00:32:02,749
service so effectively we need to find

00:32:00,959 --> 00:32:05,069
the primary task config from Aurora

00:32:02,749 --> 00:32:07,639
inject our suggestions from the base ops

00:32:05,069 --> 00:32:09,569
service make sure that we're

00:32:07,639 --> 00:32:12,149
consistently running it on the same

00:32:09,569 --> 00:32:15,559
hardware platform and then finally pick

00:32:12,149 --> 00:32:15,559
an instance to run it on

00:32:15,889 --> 00:32:20,339
ideally Aurora jobs or homogeneous but

00:32:18,869 --> 00:32:24,479
during Canaries and upgrades

00:32:20,339 --> 00:32:27,869
updates they might not be so we need to

00:32:24,479 --> 00:32:29,489
pick a task config that identifies sort

00:32:27,869 --> 00:32:32,549
of the base level task config that we

00:32:29,489 --> 00:32:35,940
can make all of our experimental changes

00:32:32,549 --> 00:32:37,649
to we use a simple heuristic for this

00:32:35,940 --> 00:32:40,139
which is just picking the most common

00:32:37,649 --> 00:32:43,289
task config among all instances but if

00:32:40,139 --> 00:32:45,269
we needed to we could do that

00:32:43,289 --> 00:32:46,619
differently so I'm talking about a

00:32:45,269 --> 00:32:49,139
little bit about tasks configs

00:32:46,619 --> 00:32:52,379
this is what an abridged or a task

00:32:49,139 --> 00:32:54,659
config looks like the key things to

00:32:52,379 --> 00:32:58,019
notice here on top of the obvious like

00:32:54,659 --> 00:32:59,519
these are the resources to use are and

00:32:58,019 --> 00:33:02,519
you know it allows specifying task level

00:32:59,519 --> 00:33:04,619
constraints it's got some data for

00:33:02,519 --> 00:33:09,539
Aurora's executor and it allows us to

00:33:04,619 --> 00:33:11,819
specify metadata and so this is what

00:33:09,539 --> 00:33:12,840
Aurora thermoses executor config looks

00:33:11,819 --> 00:33:16,080
like

00:33:12,840 --> 00:33:18,390
we have a list of processes and there's

00:33:16,080 --> 00:33:20,370
constraints among those processes that

00:33:18,390 --> 00:33:23,669
define the order in which they should

00:33:20,370 --> 00:33:25,679
run so this constraint here says that

00:33:23,669 --> 00:33:30,360
the stage process must run to completion

00:33:25,679 --> 00:33:32,159
before the run process completes you

00:33:30,360 --> 00:33:34,200
might be asking so if we look back here

00:33:32,159 --> 00:33:35,549
we have you can see in the command line

00:33:34,200 --> 00:33:39,480
there's a little bit of auto tune

00:33:35,549 --> 00:33:40,799
specific stuff here so you might be

00:33:39,480 --> 00:33:43,169
asking yourself how can we guarantee

00:33:40,799 --> 00:33:47,970
that that's all in there for the sake of

00:33:43,169 --> 00:33:50,850
time we have a helper built into our

00:33:47,970 --> 00:33:53,370
Aurora install at Twitter that all JVM

00:33:50,850 --> 00:33:55,260
processes run through which gives us a

00:33:53,370 --> 00:34:00,630
convenient point to hook into to add

00:33:55,260 --> 00:34:06,179
those necessary flags if you'll recall

00:34:00,630 --> 00:34:08,240
from that command line we are looking

00:34:06,179 --> 00:34:11,220
for this Auto Tune config file and

00:34:08,240 --> 00:34:13,290
effectively what we do in the auto tune

00:34:11,220 --> 00:34:16,349
service is inject a new process that

00:34:13,290 --> 00:34:19,109
generates this file for us which

00:34:16,349 --> 00:34:20,550
effectively exports a batch variable

00:34:19,109 --> 00:34:23,550
that contains all those command line

00:34:20,550 --> 00:34:28,109
flags that we want to pass to the

00:34:23,550 --> 00:34:29,909
experimental instance but the process by

00:34:28,109 --> 00:34:31,619
itself is not enough because we need to

00:34:29,909 --> 00:34:35,490
make sure that that runs before all of

00:34:31,619 --> 00:34:36,839
our other processes otherwise we won't

00:34:35,490 --> 00:34:38,790
know that they've picked up those

00:34:36,839 --> 00:34:41,159
settings so we also inject some

00:34:38,790 --> 00:34:44,849
constraints that guarantees that our

00:34:41,159 --> 00:34:46,349
process runs first as rompe you

00:34:44,849 --> 00:34:48,510
mentioned one of the lessons we learned

00:34:46,349 --> 00:34:50,220
from our prototype is that hardware

00:34:48,510 --> 00:34:54,540
differences definitely impacted the

00:34:50,220 --> 00:34:56,879
optimal settings so thankfully Aurora

00:34:54,540 --> 00:35:00,030
allows for us to make scheduling

00:34:56,879 --> 00:35:02,099
decisions based on those attributes so

00:35:00,030 --> 00:35:05,670
we just configure our maze those

00:35:02,099 --> 00:35:07,109
attributes with our Maysles agent with

00:35:05,670 --> 00:35:09,630
an attribute specifying the hardware

00:35:07,109 --> 00:35:15,630
platform and then Aurora can schedule on

00:35:09,630 --> 00:35:17,720
that constraint last thing we need to do

00:35:15,630 --> 00:35:21,510
before we can launch an experiment is

00:35:17,720 --> 00:35:23,970
pick an instance to run it on we want to

00:35:21,510 --> 00:35:26,250
make sure that we're not clobbering an

00:35:23,970 --> 00:35:26,660
existing experiment so we inject a

00:35:26,250 --> 00:35:29,359
little bit

00:35:26,660 --> 00:35:31,640
metadata so we can tell our auto-tune

00:35:29,359 --> 00:35:35,450
experiments from normal instances of a

00:35:31,640 --> 00:35:38,299
production service so we finally have

00:35:35,450 --> 00:35:40,039
modified aurora task config this is

00:35:38,299 --> 00:35:43,609
where aurora really helps us out because

00:35:40,039 --> 00:35:45,799
of its built-in job update support we

00:35:43,609 --> 00:35:49,579
just initiated neuroid job update with

00:35:45,799 --> 00:35:51,859
this task config if the suggestions are

00:35:49,579 --> 00:35:54,859
bad or automatically rolls that update

00:35:51,859 --> 00:35:56,329
back Twitter doesn't go down as a result

00:35:54,859 --> 00:35:58,000
of this because we are experimenting on

00:35:56,329 --> 00:36:00,460
production instances of these services

00:35:58,000 --> 00:36:03,980
auto tune then detects the rollback and

00:36:00,460 --> 00:36:06,280
updates our Bayes optimal service to let

00:36:03,980 --> 00:36:09,049
know that those settings were no good

00:36:06,280 --> 00:36:11,030
once the experiment is running Aurora's

00:36:09,049 --> 00:36:14,539
built-in support for service discovery

00:36:11,030 --> 00:36:15,950
means that this instance is taking real

00:36:14,539 --> 00:36:17,869
production traffic just like any other

00:36:15,950 --> 00:36:21,349
instance of the service if the

00:36:17,869 --> 00:36:24,710
suggestions are bad and the service

00:36:21,349 --> 00:36:27,950
fails Aurora will restart it will detect

00:36:24,710 --> 00:36:30,880
if there's too many restarts and market

00:36:27,950 --> 00:36:33,200
is bad if we're also monitoring metrics

00:36:30,880 --> 00:36:38,270
while these are running if the metrics

00:36:33,200 --> 00:36:40,549
are bad we mark it as bad again

00:36:38,270 --> 00:36:43,910
so finally experiments will run for

00:36:40,549 --> 00:36:47,390
whatever the service owner iteration was

00:36:43,910 --> 00:36:50,829
specified as once the evaluation is

00:36:47,390 --> 00:36:53,450
complete we check the objective query

00:36:50,829 --> 00:36:54,920
feed those results back into the base

00:36:53,450 --> 00:36:57,799
ops service for the next round of

00:36:54,920 --> 00:36:59,510
experiment and repeat the thing over and

00:36:57,799 --> 00:37:01,490
over again until it has romkey

00:36:59,510 --> 00:37:06,829
demonstrated we eventually find the

00:37:01,490 --> 00:37:09,160
global optimum so eventually experiments

00:37:06,829 --> 00:37:12,410
will converge on an optimal setting and

00:37:09,160 --> 00:37:14,720
this part is you know currently manual

00:37:12,410 --> 00:37:16,609
in that SOAs will take those settings

00:37:14,720 --> 00:37:18,109
and apply them to a service but

00:37:16,609 --> 00:37:19,880
hopefully one day it will just be

00:37:18,109 --> 00:37:22,789
automatic and we'll just keep doing this

00:37:19,880 --> 00:37:24,770
over and over again continuing to feed

00:37:22,789 --> 00:37:28,990
those settings back in to production

00:37:24,770 --> 00:37:33,049
services and so looking back at the

00:37:28,990 --> 00:37:35,210
diagram that we saw earlier we replace

00:37:33,049 --> 00:37:36,760
this you know blackbox tuning assistant

00:37:35,210 --> 00:37:38,710
with auto-tune

00:37:36,760 --> 00:37:41,020
we replace the performance engineer with

00:37:38,710 --> 00:37:42,790
the bass op service we have Twitter's

00:37:41,020 --> 00:37:47,339
existing monitoring infrastructure and

00:37:42,790 --> 00:37:51,160
it just keeps running and running so

00:37:47,339 --> 00:37:53,130
this is great for the JVM but what can

00:37:51,160 --> 00:37:55,750
Auto Tune do for us outside of the JVM

00:37:53,130 --> 00:37:58,329
one area that I'm super excited to

00:37:55,750 --> 00:38:01,480
explore is automatically sizing Aurora

00:37:58,329 --> 00:38:03,640
instances so instead of engineers having

00:38:01,480 --> 00:38:05,500
to figure out that I need four CPUs and

00:38:03,640 --> 00:38:10,800
twelve gigs ram auto-tune will just

00:38:05,500 --> 00:38:12,849
figure it out for you so in conclusion

00:38:10,800 --> 00:38:15,550
given the scale of the problem and the

00:38:12,849 --> 00:38:17,980
possible gains it seems clear to us that

00:38:15,550 --> 00:38:22,990
some form of automation is desirable in

00:38:17,980 --> 00:38:24,460
this space like we do for QA continuous

00:38:22,990 --> 00:38:26,650
performance optimization appears

00:38:24,460 --> 00:38:30,820
inevitable for efficient operation of

00:38:26,650 --> 00:38:34,329
micro services bays opt because it

00:38:30,820 --> 00:38:36,190
drastically reduces the cost of search

00:38:34,329 --> 00:38:38,800
for an optimum appears well-suited to

00:38:36,190 --> 00:38:49,030
the B as the technical basis for this

00:38:38,800 --> 00:38:51,400
kind of work yeah and yeah so at Twitter

00:38:49,030 --> 00:38:53,800
we think that because of the current

00:38:51,400 --> 00:38:55,540
state we believe that the current state

00:38:53,800 --> 00:38:58,050
of the art and containerization combined

00:38:55,540 --> 00:39:00,310
with command is the commoditization of

00:38:58,050 --> 00:39:02,829
machine learning technologies opens up

00:39:00,310 --> 00:39:04,960
new frontiers and operations scalability

00:39:02,829 --> 00:39:07,270
and performance engineering things that

00:39:04,960 --> 00:39:10,390
were previously unavailable are now

00:39:07,270 --> 00:39:12,130
automatable our work on auto-tune

00:39:10,390 --> 00:39:14,079
focuses on a small piece of the stack

00:39:12,130 --> 00:39:15,640
and uses a tiny subset of what's

00:39:14,079 --> 00:39:17,890
available in machine learning today as

00:39:15,640 --> 00:39:20,680
you move your platform and your

00:39:17,890 --> 00:39:22,260
infrastructure onto services like mesos

00:39:20,680 --> 00:39:24,160
you know we encourage everyone to

00:39:22,260 --> 00:39:26,170
examine what opportunities for

00:39:24,160 --> 00:39:29,290
organization autumn automation and

00:39:26,170 --> 00:39:31,720
optimization are available to you and in

00:39:29,290 --> 00:39:35,250
the negative 15 seconds I've got less

00:39:31,720 --> 00:39:35,250
time happy to answer any questions

00:39:48,050 --> 00:40:04,190
not at this time I'm great so yeah so I

00:40:01,310 --> 00:40:06,740
mean the underlying machine learning the

00:40:04,190 --> 00:40:10,970
basic optimization system is available

00:40:06,740 --> 00:40:14,320
open source that's called spearmint so

00:40:10,970 --> 00:40:18,230
if you look on github you can find that

00:40:14,320 --> 00:40:22,340
but the the orchestration piece that is

00:40:18,230 --> 00:40:26,380
auto-tune itself maybe one day but at

00:40:22,340 --> 00:40:26,380
this right

00:40:31,630 --> 00:40:38,809
yeah and I mean you know come to Aurora

00:40:37,249 --> 00:40:40,400
slack talked to us about it

00:40:38,809 --> 00:40:41,329
you know we're I don't think there's

00:40:40,400 --> 00:40:42,949
anything here that's particularly

00:40:41,329 --> 00:40:44,979
proprietary so we're not opposed to the

00:40:42,949 --> 00:40:44,979
idea

00:40:46,540 --> 00:40:51,679

YouTube URL: https://www.youtube.com/watch?v=X-fZZyw5HAQ


