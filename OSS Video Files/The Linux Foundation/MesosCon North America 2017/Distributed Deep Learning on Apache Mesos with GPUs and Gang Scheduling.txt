Title: Distributed Deep Learning on Apache Mesos with GPUs and Gang Scheduling
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Distributed Deep Learning on Apache Mesos with GPUs and Gang Scheduling - Min Cai, Alex Sergeev, Paul Mikesell & Anne Holler, UBER

Distributed deep learning is essential to speed up complex model training, scale out to hundreds of GPUs, and shard models that can not be fit into a single machine. With recent advance on deep learning models in self-driving car areas such as lane-detection, perception and so on, it is important to enable distributed deep learning with large-scale GPU clusters. 

This presentation will discuss our design and implementation of running distributed TensorFlow on top of Mesos clusters with hundreds of GPUs. It leverages several key features offered by Mesos such as GPU isolation and nested containers. We also implement several features in our scheduler to support GPU and Gang scheduling, task discovery and dynamic port allocation. Finally, we will show the speed up of distributed training on Mesos using an example TensorFlow model for image classification.

About

Min Cai
Staff Engineer, UBER
Min Cai is a Staff Engineer at UBER working on cluster management. He received his Ph.D. degree in Computer Science from USC. Before joining Uber, he was a Sr. Staff Engineer at VMware working on vMotion and vSphere.

Alex Sergeev
Senior Engineer, UBER
Alex Sergeev is a Senior Engineer at UBER working on scalable Deep Learning. He recived his MS. degree in Computer Science from MEPhI. Before joining UBER, he was Senior Engineer at Microsoft working on Big Data Mining.
Captions: 
	00:00:00,269 --> 00:00:09,120
good afternoon everybody today Alex

00:00:06,390 --> 00:00:11,820
enemy cannot present distributed tip

00:00:09,120 --> 00:00:16,640
landing on messes with GPU and the gang

00:00:11,820 --> 00:00:21,300
scheduling this is a project we

00:00:16,640 --> 00:00:23,460
delivered at uber with - with the

00:00:21,300 --> 00:00:28,380
collaboration between computer team as

00:00:23,460 --> 00:00:31,920
well as a Mac angular team so we have my

00:00:28,380 --> 00:00:36,690
name is Ming hi this is Alex and we have

00:00:31,920 --> 00:00:43,379
a holiday or two then also poor Mac so

00:00:36,690 --> 00:00:46,700
so first of all let's talk about why we

00:00:43,379 --> 00:00:49,710
need our Alex even introduce yourself

00:00:46,700 --> 00:00:54,170
I'm Alex I'm from deep learning team and

00:00:49,710 --> 00:00:54,170
then main is from computer platform team

00:00:56,059 --> 00:01:03,570
so first let's talk about like why we

00:01:00,539 --> 00:01:07,439
need a deep learning at uber so as all

00:01:03,570 --> 00:01:10,189
you guys probably know always grow very

00:01:07,439 --> 00:01:13,680
fast in the last few years from

00:01:10,189 --> 00:01:19,080
initially a ride-sharing company now we

00:01:13,680 --> 00:01:21,810
have our self-driving we business unit

00:01:19,080 --> 00:01:26,580
so basically self-driving cars are a

00:01:21,810 --> 00:01:30,270
huge source of workload for especially

00:01:26,580 --> 00:01:32,939
for deep learning and machine learnings

00:01:30,270 --> 00:01:35,549
then next to use case is of course at

00:01:32,939 --> 00:01:40,049
the ready sharing company we would like

00:01:35,549 --> 00:01:42,509
to predict what is the trip and how much

00:01:40,049 --> 00:01:46,009
trip we're gonna see in the next coming

00:01:42,509 --> 00:01:48,979
months or year and then third of our is

00:01:46,009 --> 00:01:52,079
because always completely based on

00:01:48,979 --> 00:01:53,880
mobile payment and of course people

00:01:52,079 --> 00:01:56,210
gonna take advantage of that and make

00:01:53,880 --> 00:02:00,259
some money out of the out of all was

00:01:56,210 --> 00:02:03,240
success so first for the self-driving

00:02:00,259 --> 00:02:06,270
vehicles right now we have both

00:02:03,240 --> 00:02:09,800
self-driving cars as well as self

00:02:06,270 --> 00:02:12,180
driving trucks and the self-driving

00:02:09,800 --> 00:02:14,099
vehicle problem is actually

00:02:12,180 --> 00:02:17,849
very interesting as you guys probably

00:02:14,099 --> 00:02:23,010
already know it's largely computer

00:02:17,849 --> 00:02:27,150
vision as well as image recognization

00:02:23,010 --> 00:02:30,480
and physical perception issues as well

00:02:27,150 --> 00:02:32,970
as like how do you build a precise 3d

00:02:30,480 --> 00:02:36,750
maps so all those need a lots of like

00:02:32,970 --> 00:02:38,730
deep learning computation Nepal as well

00:02:36,750 --> 00:02:42,629
as like how you change those distribute

00:02:38,730 --> 00:02:46,560
deep learning models quick enough so we

00:02:42,629 --> 00:02:49,739
can do quick turnarounds then for the

00:02:46,560 --> 00:02:51,569
trip forecasting as we see here this is

00:02:49,739 --> 00:02:55,709
actually we have a engineering block

00:02:51,569 --> 00:03:00,299
talk about how we do chip forecasting

00:02:55,709 --> 00:03:02,639
using deep learning networks based on

00:03:00,299 --> 00:03:04,739
lots of time mysterious data as well as

00:03:02,639 --> 00:03:07,889
other informations potentially like

00:03:04,739 --> 00:03:10,290
weather information events for example a

00:03:07,889 --> 00:03:14,129
Super Bowl coming or all those things so

00:03:10,290 --> 00:03:16,260
as we can see here you know tipping is

00:03:14,129 --> 00:03:18,859
very powerful actually so the prediction

00:03:16,260 --> 00:03:26,310
is pretty accurate

00:03:18,859 --> 00:03:30,569
that gives us lots of nice advantage or

00:03:26,310 --> 00:03:35,310
a nice lead time to notify drivers say

00:03:30,569 --> 00:03:38,459
hey there are lots of trips at the

00:03:35,310 --> 00:03:40,739
amount coming do you want to turn on

00:03:38,459 --> 00:03:45,659
your app and then start like driving

00:03:40,739 --> 00:03:49,049
over then the third use case is a fourth

00:03:45,659 --> 00:03:52,440
detection so this is for example one a

00:03:49,049 --> 00:03:55,109
typical use case for how people can cash

00:03:52,440 --> 00:03:56,909
up some money from uber especially like

00:03:55,109 --> 00:04:00,989
in early days over has lots of

00:03:56,909 --> 00:04:05,639
incentives or referral code and

00:04:00,989 --> 00:04:08,129
basically some first just go through spy

00:04:05,639 --> 00:04:11,459
and referral code through their friends

00:04:08,129 --> 00:04:15,199
and then putting up with friends as the

00:04:11,459 --> 00:04:19,470
driver and then cash out over credits so

00:04:15,199 --> 00:04:21,630
this is is actually a huge problem when

00:04:19,470 --> 00:04:22,880
we have the China business because

00:04:21,630 --> 00:04:27,290
people just like

00:04:22,880 --> 00:04:32,030
lots of those so deep learning is

00:04:27,290 --> 00:04:34,370
actually a very good approach to

00:04:32,030 --> 00:04:36,580
basically use different signals to

00:04:34,370 --> 00:04:38,530
figure out who might be a potential

00:04:36,580 --> 00:04:42,260
florist

00:04:38,530 --> 00:04:44,600
so now let's talk about why distributed

00:04:42,260 --> 00:04:48,620
deep learning alright so as you guys

00:04:44,600 --> 00:04:50,330
probably know the success of recent deep

00:04:48,620 --> 00:04:53,720
learning is because of two things

00:04:50,330 --> 00:04:55,820
why is big data so we have lots of lots

00:04:53,720 --> 00:04:59,210
of data and the potential I hope for a

00:04:55,820 --> 00:05:02,270
label data the second part is you have a

00:04:59,210 --> 00:05:04,730
huge cluster as well as the cheap

00:05:02,270 --> 00:05:06,470
advance of GPUs so basically you just

00:05:04,730 --> 00:05:10,910
have like lots of computation apause

00:05:06,470 --> 00:05:16,730
with lots of data that's actually a is

00:05:10,910 --> 00:05:18,560
kind of the main contributing underneath

00:05:16,730 --> 00:05:21,110
reason why did you should be deepening

00:05:18,560 --> 00:05:24,410
our deep learning itself become a

00:05:21,110 --> 00:05:27,020
success in the recent yes so for

00:05:24,410 --> 00:05:30,680
distribute deep learning is critical for

00:05:27,020 --> 00:05:33,350
that for three reasons why is you want

00:05:30,680 --> 00:05:36,020
to speed up model training because some

00:05:33,350 --> 00:05:39,890
large deep learning models would take

00:05:36,020 --> 00:05:42,230
like weeks or months to train with huge

00:05:39,890 --> 00:05:45,410
amount of data with this really deep

00:05:42,230 --> 00:05:47,600
learning you can basically speed up that

00:05:45,410 --> 00:05:50,990
stick originally and of course you can

00:05:47,600 --> 00:05:54,770
scale out those hundreds of GPUs the

00:05:50,990 --> 00:05:56,690
third reason is some large deep learning

00:05:54,770 --> 00:06:00,020
model they cannot really fit into a

00:05:56,690 --> 00:06:00,470
single machine so with this route deep

00:06:00,020 --> 00:06:02,720
learning

00:06:00,470 --> 00:06:07,010
you can basically partition those models

00:06:02,720 --> 00:06:10,340
and then doing training as well as

00:06:07,010 --> 00:06:13,790
prediction for that so this diagram

00:06:10,340 --> 00:06:14,660
shows how distribute deep learning works

00:06:13,790 --> 00:06:17,570
in general

00:06:14,660 --> 00:06:20,600
this is like a very high level so the

00:06:17,570 --> 00:06:23,270
idea is you have lots of data on your

00:06:20,600 --> 00:06:26,120
data store which could be HDFS or any

00:06:23,270 --> 00:06:28,610
other like file system and then you have

00:06:26,120 --> 00:06:31,490
different training process which is

00:06:28,610 --> 00:06:35,600
normally running in containers and they

00:06:31,490 --> 00:06:36,530
will retrieve data basically ingest data

00:06:35,600 --> 00:06:39,290
from data

00:06:36,530 --> 00:06:44,060
and then we have you know data

00:06:39,290 --> 00:06:45,620
scientists or depending experts will

00:06:44,060 --> 00:06:47,450
come up with the dip learning models

00:06:45,620 --> 00:06:49,910
with different layers and parameter

00:06:47,450 --> 00:06:54,740
initial parameters and those models will

00:06:49,910 --> 00:07:00,590
in each training process and then each

00:06:54,740 --> 00:07:04,100
shooting process where compute the model

00:07:00,590 --> 00:07:08,420
updates which are Greenlees so this

00:07:04,100 --> 00:07:09,919
process this step is a kind of parallel

00:07:08,420 --> 00:07:14,600
or independent of each other

00:07:09,919 --> 00:07:17,150
however after the Modoc updates all

00:07:14,600 --> 00:07:19,550
those training process need to talk to

00:07:17,150 --> 00:07:20,270
each other and then computer the

00:07:19,550 --> 00:07:24,050
averaged

00:07:20,270 --> 00:07:25,970
grindings so that's the keep actually

00:07:24,050 --> 00:07:28,940
one very unique property of this kind of

00:07:25,970 --> 00:07:33,590
workload because it's not like as a

00:07:28,940 --> 00:07:35,419
spark or MapReduce or streaming process

00:07:33,590 --> 00:07:36,980
you know those different processes don't

00:07:35,419 --> 00:07:39,080
necessarily need to communicate to each

00:07:36,980 --> 00:07:43,340
other so this is kind of very tightly

00:07:39,080 --> 00:07:46,130
coupled distributed application in a

00:07:43,340 --> 00:07:48,520
while kind of similar application is

00:07:46,130 --> 00:07:51,890
like a traditional MPI kind of

00:07:48,520 --> 00:07:53,180
applications in supercomputing world so

00:07:51,890 --> 00:07:57,110
this has lots of kind of a unique

00:07:53,180 --> 00:08:02,840
challenge to how to sort of make this

00:07:57,110 --> 00:08:06,560
work efficiently at UM like custom

00:08:02,840 --> 00:08:08,240
management system like missiles so first

00:08:06,560 --> 00:08:10,790
we're going to talk about a wire we

00:08:08,240 --> 00:08:13,220
choose my sauce of course because it's

00:08:10,790 --> 00:08:17,810
the missiles con so that's the reason we

00:08:13,220 --> 00:08:19,640
are here but beyond that the reason we

00:08:17,810 --> 00:08:21,770
decide to use missiles is first it's

00:08:19,640 --> 00:08:25,430
wider they adopt like as we know like

00:08:21,770 --> 00:08:28,130
lots of good companies upon it 4x over

00:08:25,430 --> 00:08:31,400
twitter is using missiles in production

00:08:28,130 --> 00:08:36,320
for quite a long time and also my sauce

00:08:31,400 --> 00:08:40,460
has a very good GPU support especially

00:08:36,320 --> 00:08:42,979
with the latest Nvidia Isolators

00:08:40,460 --> 00:08:45,170
everything so that is kind of perfect

00:08:42,979 --> 00:08:48,470
for this use case the sort of our is

00:08:45,170 --> 00:08:51,500
with the newly introduced

00:08:48,470 --> 00:08:55,160
nested containers which enables us to do

00:08:51,500 --> 00:08:59,150
lots of things for example like separate

00:08:55,160 --> 00:09:01,820
the management code from users own

00:08:59,150 --> 00:09:03,860
docker image so basically we that

00:09:01,820 --> 00:09:06,320
provides us lots of good isolations

00:09:03,860 --> 00:09:11,480
between the machine learning platform

00:09:06,320 --> 00:09:13,420
and you use a docker image of course

00:09:11,480 --> 00:09:16,280
missus is highly customizable and

00:09:13,420 --> 00:09:17,540
reliable and scalable so that's the key

00:09:16,280 --> 00:09:22,070
reasons we're using missus

00:09:17,540 --> 00:09:29,000
so I'm not sure how many you guys

00:09:22,070 --> 00:09:31,670
already run GPUs on my sauce so anyway

00:09:29,000 --> 00:09:34,370
raise your hand cool so we have like

00:09:31,670 --> 00:09:36,590
actually pretty good so how many of you

00:09:34,370 --> 00:09:38,360
are using my sauce canned analyzer or

00:09:36,590 --> 00:09:42,080
unified irken denizer instead of

00:09:38,360 --> 00:09:44,360
tokenizer very good so as you guys

00:09:42,080 --> 00:09:47,480
probably already know you know missiles

00:09:44,360 --> 00:09:51,230
have this nice API is for containerized

00:09:47,480 --> 00:09:53,330
as well as a ps4 isolator actually for

00:09:51,230 --> 00:09:54,380
us we also use a unified missus Clinton

00:09:53,330 --> 00:09:57,380
riser because that's the only thing

00:09:54,380 --> 00:10:00,800
works upstream right now for tokens

00:09:57,380 --> 00:10:05,390
analyzer you have to take some upstream

00:10:00,800 --> 00:10:09,110
patch and you know deal with that

00:10:05,390 --> 00:10:12,050
yourself but the key part here is Nvidia

00:10:09,110 --> 00:10:15,050
actually contributed their GPU a locator

00:10:12,050 --> 00:10:17,660
as well as isolator

00:10:15,050 --> 00:10:21,620
so basically you can pretty much from

00:10:17,660 --> 00:10:23,780
the mesas framework perspective we can

00:10:21,620 --> 00:10:26,860
manage the GPU resource almost the same

00:10:23,780 --> 00:10:29,840
as other like CPU or memory resource

00:10:26,860 --> 00:10:33,370
then of course you have to prepare your

00:10:29,840 --> 00:10:36,350
missus aging with correctable cheap

00:10:33,370 --> 00:10:38,060
immediate driver as well as like

00:10:36,350 --> 00:10:39,620
programmer in the talk image you have to

00:10:38,060 --> 00:10:43,670
have the correct like CUDA Bodines

00:10:39,620 --> 00:10:50,390
everything so those are kind of GPUs

00:10:43,670 --> 00:10:52,280
labor tricky from that aspect now the

00:10:50,390 --> 00:10:56,480
second one I would like to discuss the

00:10:52,280 --> 00:10:57,710
weather is a Mesa Sonesta container so

00:10:56,480 --> 00:10:59,030
just won't get some idea like how many

00:10:57,710 --> 00:11:00,589
of the you guys are using nested

00:10:59,030 --> 00:11:05,069
containers

00:11:00,589 --> 00:11:06,989
okay that's much less people but this is

00:11:05,069 --> 00:11:09,779
a great feature right so basically the

00:11:06,989 --> 00:11:11,429
idea for next contender is you won't for

00:11:09,779 --> 00:11:13,709
example let's say we build some children

00:11:11,429 --> 00:11:19,169
or machine learning platform which of

00:11:13,709 --> 00:11:22,339
course we have some control code inside

00:11:19,169 --> 00:11:25,019
of the container but a data control code

00:11:22,339 --> 00:11:27,329
needs to be kind of separate or isolate

00:11:25,019 --> 00:11:30,089
the carpod away from the user docker

00:11:27,329 --> 00:11:33,389
image because in the machine learning or

00:11:30,089 --> 00:11:36,179
deep learning space different teams are

00:11:33,389 --> 00:11:38,279
different like data scientists they want

00:11:36,179 --> 00:11:40,589
to like different toolings in their taka

00:11:38,279 --> 00:11:44,489
image for example they won't install

00:11:40,589 --> 00:11:46,619
some different Python libraries those

00:11:44,489 --> 00:11:49,609
who this would you know have like issues

00:11:46,619 --> 00:11:52,379
if you put them in the same talk image

00:11:49,609 --> 00:11:54,749
so with next contender basically what we

00:11:52,379 --> 00:11:56,999
are doing is we have a parent task which

00:11:54,749 --> 00:12:02,179
is running the management code and then

00:11:56,999 --> 00:12:05,039
then the management code where oops

00:12:02,179 --> 00:12:07,979
the management code will then go back to

00:12:05,039 --> 00:12:09,809
missus agent and to pull the Nesta

00:12:07,979 --> 00:12:13,019
containers which is a different image in

00:12:09,809 --> 00:12:14,309
a run in that way and then basically for

00:12:13,019 --> 00:12:17,699
us we're gonna probably have tens of

00:12:14,309 --> 00:12:22,229
flows the model or the trainings is in

00:12:17,699 --> 00:12:26,119
the nest egg container so yeah missus is

00:12:22,229 --> 00:12:30,359
cool when a question is what is missing

00:12:26,119 --> 00:12:34,319
so there are a few key items like we

00:12:30,359 --> 00:12:35,819
need to kind of build ourselves on top

00:12:34,319 --> 00:12:36,659
of my sauce so we can make this a

00:12:35,819 --> 00:12:39,449
production ready

00:12:36,659 --> 00:12:41,970
the first thing or most important one is

00:12:39,449 --> 00:12:44,249
record an elastic GPU resource our

00:12:41,970 --> 00:12:45,929
management because GPU is kind of a

00:12:44,249 --> 00:12:48,349
resource which is very expensive you

00:12:45,929 --> 00:12:51,269
know as you guys probably know like the

00:12:48,349 --> 00:12:54,569
enterprise squared GPU is like five

00:12:51,269 --> 00:12:57,329
grand one piece and you put like four of

00:12:54,569 --> 00:12:59,489
them in one machine that's like 20 grand

00:12:57,329 --> 00:13:02,069
right there and the part of the power

00:12:59,489 --> 00:13:03,529
support and a rack everything so GPU is

00:13:02,069 --> 00:13:06,239
very very expensive

00:13:03,529 --> 00:13:09,479
of course you don't really want the GPU

00:13:06,239 --> 00:13:11,249
resource to the setter they are idle so

00:13:09,479 --> 00:13:13,139
that's why I like but you know we have

00:13:11,249 --> 00:13:16,769
like at least at uber we have different

00:13:13,139 --> 00:13:20,729
organizations they all want like a used

00:13:16,769 --> 00:13:22,979
GPU but you you cannot really give them

00:13:20,729 --> 00:13:24,659
a static quota because if we give them a

00:13:22,979 --> 00:13:28,529
static quota if they are not using it

00:13:24,659 --> 00:13:30,149
then your GPU is at idle the second part

00:13:28,529 --> 00:13:34,259
is as we we talked about it before

00:13:30,149 --> 00:13:38,009
because of the averaging grindings

00:13:34,259 --> 00:13:42,600
computation we need lots of network

00:13:38,009 --> 00:13:44,459
bandwidth between different workers to

00:13:42,600 --> 00:13:48,919
communicate to each other so they can

00:13:44,459 --> 00:13:52,350
compute the final model so this will

00:13:48,919 --> 00:13:55,229
this will need us to support a locality

00:13:52,350 --> 00:13:57,919
and the network aware placement the idea

00:13:55,229 --> 00:14:00,720
is like you would like to place all the

00:13:57,919 --> 00:14:05,069
containers of the same training job as

00:14:00,720 --> 00:14:07,639
close to each other as possible then the

00:14:05,069 --> 00:14:10,439
next one is the kind scheduling because

00:14:07,639 --> 00:14:14,069
typically in the distributed deplaning

00:14:10,439 --> 00:14:16,229
training job you will need all the

00:14:14,069 --> 00:14:18,989
workers to be up so you can make

00:14:16,229 --> 00:14:21,509
progress together so that means like if

00:14:18,989 --> 00:14:23,759
we only allocate half of the workers or

00:14:21,509 --> 00:14:26,339
you're only allocated resources for half

00:14:23,759 --> 00:14:29,489
of the workers then they we have to

00:14:26,339 --> 00:14:31,829
stage as idols there and wait for the

00:14:29,489 --> 00:14:33,779
remaining resource so this would easily

00:14:31,829 --> 00:14:35,699
introduce a deadlock situation for

00:14:33,779 --> 00:14:39,509
example if you have only have like 10

00:14:35,699 --> 00:14:41,819
GPU and then each job need 8 GPU then if

00:14:39,509 --> 00:14:45,239
you don't have any way to kind of do

00:14:41,819 --> 00:14:48,059
cash scheduling of those 8 GPU workers

00:14:45,239 --> 00:14:52,319
then the way I put like each grab like 5

00:14:48,059 --> 00:14:54,479
or anything waiting 0 also like for task

00:14:52,319 --> 00:14:55,979
discovery because this is at issue this

00:14:54,479 --> 00:14:59,039
really training applications they need

00:14:55,979 --> 00:15:00,779
to know each other's I like which

00:14:59,039 --> 00:15:02,549
machines they are running on and which

00:15:00,779 --> 00:15:06,509
dynamic reports they are listening on

00:15:02,549 --> 00:15:09,659
the connection so this we have to solve

00:15:06,509 --> 00:15:11,069
that that this discovery problem but

00:15:09,659 --> 00:15:13,289
that discovery is a little bit different

00:15:11,069 --> 00:15:15,629
than the service discovery you know we

00:15:13,289 --> 00:15:18,600
have talked about last yesterday because

00:15:15,629 --> 00:15:22,079
this discovery is only within the same

00:15:18,600 --> 00:15:24,840
job scope finally of course we have

00:15:22,079 --> 00:15:29,160
handles a failure failure failure

00:15:24,840 --> 00:15:32,820
so this this is like how to address this

00:15:29,160 --> 00:15:36,210
issues we actually implement a system

00:15:32,820 --> 00:15:38,820
called peloton which is a cluster of

00:15:36,210 --> 00:15:40,650
bicycles the idea was and that name is

00:15:38,820 --> 00:15:44,580
you know if you cluster together you

00:15:40,650 --> 00:15:47,550
move faster together so basically you

00:15:44,580 --> 00:15:51,360
know as we can see here partner is

00:15:47,550 --> 00:15:53,310
trying to bridge the gap between more

00:15:51,360 --> 00:15:54,900
general-purpose scheduler and the

00:15:53,310 --> 00:15:57,420
Maysles because the missile's provided

00:15:54,900 --> 00:16:00,000
lots of great tasks

00:15:57,420 --> 00:16:01,860
shushing resource allocation as well but

00:16:00,000 --> 00:16:06,660
then we still missing the preemption

00:16:01,860 --> 00:16:09,000
aspect the the placement and finally the

00:16:06,660 --> 00:16:11,880
job and Haskell life cycle so the parent

00:16:09,000 --> 00:16:14,430
is trying to be a scheduler which we are

00:16:11,880 --> 00:16:17,160
provided a common implementation for all

00:16:14,430 --> 00:16:20,040
those things and of course it tried to

00:16:17,160 --> 00:16:24,000
it's going to support a GPU as well as a

00:16:20,040 --> 00:16:26,070
mixed workloads so this is a high-level

00:16:24,000 --> 00:16:27,870
architecture of peloton

00:16:26,070 --> 00:16:31,350
it's basically sits on top of my sauce

00:16:27,870 --> 00:16:35,370
master and we are using HTTP as a latest

00:16:31,350 --> 00:16:38,490
missus HTTP API and then it's self has

00:16:35,370 --> 00:16:41,640
its own API exposed in a protocol buff

00:16:38,490 --> 00:16:45,660
and right now we we have our own

00:16:41,640 --> 00:16:47,160
homegrown G a RPC implementation but

00:16:45,660 --> 00:16:50,100
it's hundred percent backwards

00:16:47,160 --> 00:16:51,750
compatible with NHGRI C client so that's

00:16:50,100 --> 00:16:54,030
actually give a give us a very good at

00:16:51,750 --> 00:16:58,980
like for accessibility for example all

00:16:54,030 --> 00:17:03,390
UI is in nodejs we have like python

00:16:58,980 --> 00:17:06,470
current Java current which can all talk

00:17:03,390 --> 00:17:09,300
to the back-end using the same protocol

00:17:06,470 --> 00:17:11,459
then the party itself actually has break

00:17:09,300 --> 00:17:14,490
it down into like four big components

00:17:11,459 --> 00:17:15,750
why is the wholesome manager which kind

00:17:14,490 --> 00:17:18,000
of a buried all the resource from

00:17:15,750 --> 00:17:20,400
missiles and present that to resource

00:17:18,000 --> 00:17:22,110
manager the resource manager safe

00:17:20,400 --> 00:17:24,329
basically where do this elastic resource

00:17:22,110 --> 00:17:28,410
sharing and then we in from different

00:17:24,329 --> 00:17:30,240
favor of Prisma engines one of them is

00:17:28,410 --> 00:17:34,440
kind of for deeper learning so which

00:17:30,240 --> 00:17:36,450
kind of support locality which we are

00:17:34,440 --> 00:17:38,160
support actually locally where we are

00:17:36,450 --> 00:17:41,820
right now it's more like constrained by

00:17:38,160 --> 00:17:44,630
and also the important manager we were

00:17:41,820 --> 00:17:50,010
to basically manages a lifecycle of the

00:17:44,630 --> 00:17:51,299
job and tasks and all those data all all

00:17:50,010 --> 00:17:53,460
those metadata like the jobs

00:17:51,299 --> 00:17:55,020
configuration or task configuration as

00:17:53,460 --> 00:17:59,030
well as the wrong time

00:17:55,020 --> 00:18:04,850
saved in Cassandra so you can basically

00:17:59,030 --> 00:18:04,850
do lots of like search and index on that

00:18:06,920 --> 00:18:12,660
so this this slide shows like how we

00:18:09,770 --> 00:18:15,600
what's our solution for the elastic GPU

00:18:12,660 --> 00:18:17,460
resource management the idea is

00:18:15,600 --> 00:18:19,080
basically we take this very similar idea

00:18:17,460 --> 00:18:21,630
as what my source is also doing right

00:18:19,080 --> 00:18:24,990
now but we kind of we cannot wait so we

00:18:21,630 --> 00:18:27,630
did that in our schedule itself to begin

00:18:24,990 --> 00:18:29,240
with what missus is a variable we came

00:18:27,630 --> 00:18:32,250
basically offloaded that to my source

00:18:29,240 --> 00:18:33,690
but anyway the high-level idea is we

00:18:32,250 --> 00:18:35,940
have something called a reservation

00:18:33,690 --> 00:18:39,030
which every organization can say how

00:18:35,940 --> 00:18:40,770
much GPU I want to reserve but if they

00:18:39,030 --> 00:18:46,290
don't use those resources will be used

00:18:40,770 --> 00:18:49,530
for other organizations as like best

00:18:46,290 --> 00:18:51,570
effort or fair share resource and then

00:18:49,530 --> 00:18:56,400
we have job priorities which is local

00:18:51,570 --> 00:18:58,080
local to individual organization and we

00:18:56,400 --> 00:19:02,040
have a concept called the resource pool

00:18:58,080 --> 00:19:04,200
which is very similar to the raw concept

00:19:02,040 --> 00:19:06,240
in basis so basically for this is a

00:19:04,200 --> 00:19:08,610
hierarchical resource pool and you can

00:19:06,240 --> 00:19:10,710
define the what's the reserve the

00:19:08,610 --> 00:19:14,220
resource for this pool and what's the

00:19:10,710 --> 00:19:18,299
fair share if for those idle resources

00:19:14,220 --> 00:19:20,820
and then dynamically we can compute

00:19:18,299 --> 00:19:25,890
what's the current resource this

00:19:20,820 --> 00:19:29,270
resource pool is entitled to wrong so

00:19:25,890 --> 00:19:32,910
then put gas scheduling basically we

00:19:29,270 --> 00:19:36,120
solve this problem in this also in the

00:19:32,910 --> 00:19:39,830
part and scheduler layer by define a

00:19:36,120 --> 00:19:43,080
subset of tasks in a job which was

00:19:39,830 --> 00:19:45,299
basically scheduled visually it all

00:19:43,080 --> 00:19:47,600
ganged tasks will be a single scheduling

00:19:45,299 --> 00:19:51,150
unit that means it will be a damaged

00:19:47,600 --> 00:19:53,820
placed a preempt and cured as a group

00:19:51,150 --> 00:19:57,330
and but the those can tasks though is a

00:19:53,820 --> 00:19:59,120
independent excusing unit because they

00:19:57,330 --> 00:20:02,160
are running in separate containers and

00:19:59,120 --> 00:20:03,660
mayfair independently so that actually

00:20:02,160 --> 00:20:05,880
requires us to do lots of lifecycle

00:20:03,660 --> 00:20:08,040
management on the Gantt side because if

00:20:05,880 --> 00:20:11,880
some tasks cannot be restarted we have

00:20:08,040 --> 00:20:14,490
to cure the whole game so fine then the

00:20:11,880 --> 00:20:16,140
next interesting problems like you know

00:20:14,490 --> 00:20:18,780
we have to improve some spatial

00:20:16,140 --> 00:20:22,530
appraisement strategies for this

00:20:18,780 --> 00:20:24,390
particular use case why is like we would

00:20:22,530 --> 00:20:27,060
like to place as many as containers of

00:20:24,390 --> 00:20:31,580
the same job into the same host or rack

00:20:27,060 --> 00:20:35,730
so we can minimize the network traffic

00:20:31,580 --> 00:20:37,680
across rack top-of-rack switch and also

00:20:35,730 --> 00:20:39,300
we would like to use some kind of base

00:20:37,680 --> 00:20:41,790
the fit algorithm into the wide

00:20:39,300 --> 00:20:45,060
spreading algorithm to title a package

00:20:41,790 --> 00:20:48,060
GPO containers finally we have to

00:20:45,060 --> 00:20:51,960
support a constraint based placement so

00:20:48,060 --> 00:20:53,760
that the user can place all they are for

00:20:51,960 --> 00:20:56,250
all the containers of the same job using

00:20:53,760 --> 00:20:57,680
the same generation abhi GPU so this is

00:20:56,250 --> 00:21:01,410
actually also very important because

00:20:57,680 --> 00:21:03,390
unlike CPU different generation of GPU

00:21:01,410 --> 00:21:05,610
has huge difference in terms of

00:21:03,390 --> 00:21:09,170
performance so we don't really want to

00:21:05,610 --> 00:21:12,540
mix different GPU models in the same job

00:21:09,170 --> 00:21:14,400
now I'm going to get to Eric's talk

00:21:12,540 --> 00:21:16,110
about how tensorflow and how the

00:21:14,400 --> 00:21:16,950
performance never looked like on top of

00:21:16,110 --> 00:21:21,150
my sauce

00:21:16,950 --> 00:21:24,090
thanks man alright so as we grew as a

00:21:21,150 --> 00:21:26,220
company as we did more and more of

00:21:24,090 --> 00:21:27,900
machine learning we realized that we

00:21:26,220 --> 00:21:30,000
need to invest in our deep learning

00:21:27,900 --> 00:21:31,650
infrastructure and the framework we

00:21:30,000 --> 00:21:34,080
chose for majority of our model is

00:21:31,650 --> 00:21:36,120
tensorflow so why did we do that

00:21:34,080 --> 00:21:37,500
desert flow is the most popular open

00:21:36,120 --> 00:21:39,390
source framework for deep learning at

00:21:37,500 --> 00:21:41,220
this moment when I was preparing these

00:21:39,390 --> 00:21:43,680
slides I looked at the github page and

00:21:41,220 --> 00:21:48,030
it's basically showing 69 thousand stars

00:21:43,680 --> 00:21:52,380
and 60 34,000 Forks it's huge number by

00:21:48,030 --> 00:21:55,140
the compared to other frameworks so that

00:21:52,380 --> 00:21:57,270
as users give us ability to ramp up

00:21:55,140 --> 00:21:59,250
people much easier because there is a

00:21:57,270 --> 00:21:59,820
lot more examples for how to use

00:21:59,250 --> 00:22:02,710
tensorflow

00:21:59,820 --> 00:22:04,570
how to deploy it and all that stuff

00:22:02,710 --> 00:22:06,520
Delta flow also combines how

00:22:04,570 --> 00:22:08,559
high-performance with ability to tinker

00:22:06,520 --> 00:22:10,809
with low-level model details so what

00:22:08,559 --> 00:22:12,580
that means is you can take some of the

00:22:10,809 --> 00:22:15,490
high-level abstractions and they work

00:22:12,580 --> 00:22:17,289
very well at the same time if you want

00:22:15,490 --> 00:22:19,779
to write your own implementation for

00:22:17,289 --> 00:22:22,600
some new operation that you came up with

00:22:19,779 --> 00:22:24,159
you can do it in CUDA code and you can

00:22:22,600 --> 00:22:25,960
integrate it with tensor flow without

00:22:24,159 --> 00:22:28,750
any issues they have nice plug-in model

00:22:25,960 --> 00:22:30,909
for that and then Tessa focus on

00:22:28,750 --> 00:22:34,059
transport from research to production so

00:22:30,909 --> 00:22:35,830
you can do research real easy because of

00:22:34,059 --> 00:22:37,779
that and you can also export your model

00:22:35,830 --> 00:22:42,850
and then deploy it all within the same

00:22:37,779 --> 00:22:44,529
framework so min mentioned about how we

00:22:42,850 --> 00:22:46,870
do distributed deep learning so in

00:22:44,529 --> 00:22:49,480
general you have basically have

00:22:46,870 --> 00:22:52,809
MapReduce except MapReduce whole phase

00:22:49,480 --> 00:22:55,750
happens every second and your model

00:22:52,809 --> 00:22:58,450
weights are about one or multiple

00:22:55,750 --> 00:23:00,250
gigabytes so every second you exchange

00:22:58,450 --> 00:23:02,890
multiple gigabytes that every worker

00:23:00,250 --> 00:23:05,559
possesses and they all need to arrive to

00:23:02,890 --> 00:23:07,179
average number so how does distribute

00:23:05,559 --> 00:23:09,610
tensor flow handle that there is

00:23:07,179 --> 00:23:12,429
typically one of the two patterns either

00:23:09,610 --> 00:23:14,409
you have one parameter server which is

00:23:12,429 --> 00:23:15,820
special kind of worker and all the

00:23:14,409 --> 00:23:19,299
workers communicate with that parameter

00:23:15,820 --> 00:23:21,669
server to upload those weights another

00:23:19,299 --> 00:23:23,200
option that people do is instead of

00:23:21,669 --> 00:23:25,000
having one parameter server you can have

00:23:23,200 --> 00:23:27,039
multiple or in extreme case you have

00:23:25,000 --> 00:23:30,250
same amount of parameter service workers

00:23:27,039 --> 00:23:32,710
so what that gives is it allows you to

00:23:30,250 --> 00:23:33,940
offload the load from the single

00:23:32,710 --> 00:23:35,860
parameter server so you don't have a

00:23:33,940 --> 00:23:37,630
bottleneck anymore but at the same time

00:23:35,860 --> 00:23:39,460
that gives you basically all - all

00:23:37,630 --> 00:23:41,020
communication pattern that every host

00:23:39,460 --> 00:23:44,890
has to communicate with every other host

00:23:41,020 --> 00:23:47,649
which is also not ideal but it works so

00:23:44,890 --> 00:23:49,510
when we deployed traditional distribute

00:23:47,649 --> 00:23:51,760
tons of flow on the misses that's how we

00:23:49,510 --> 00:23:53,679
did it basically at where we have this

00:23:51,760 --> 00:23:56,260
Michaelangelo deploying platform or

00:23:53,679 --> 00:23:59,440
service which talks to peloton and then

00:23:56,260 --> 00:24:02,679
peloton schedules were distributed tens

00:23:59,440 --> 00:24:04,840
of flow jobs on misses agents so we get

00:24:02,679 --> 00:24:07,720
containers provisioned with our

00:24:04,840 --> 00:24:09,760
management code and then we start nested

00:24:07,720 --> 00:24:12,610
containers with user code which may

00:24:09,760 --> 00:24:15,490
views variety of versions of tensorflow

00:24:12,610 --> 00:24:16,500
and may use variety of like native

00:24:15,490 --> 00:24:18,690
libraries or whatever

00:24:16,500 --> 00:24:22,140
which could conflict with our management

00:24:18,690 --> 00:24:24,660
container otherwise so typically what

00:24:22,140 --> 00:24:26,610
happens is in case of the first pattern

00:24:24,660 --> 00:24:28,260
you would have one parameter server and

00:24:26,610 --> 00:24:31,110
then on every host you would typically

00:24:28,260 --> 00:24:33,180
have one container with one worker

00:24:31,110 --> 00:24:35,550
container and then those containers

00:24:33,180 --> 00:24:38,480
would use as many GPUs as possible

00:24:35,550 --> 00:24:43,470
within the same same container instance

00:24:38,480 --> 00:24:47,100
so we deployed that and then this is we

00:24:43,470 --> 00:24:49,350
did the benchmark so we tried standard

00:24:47,100 --> 00:24:51,390
tons of flow benchmark which is very

00:24:49,350 --> 00:24:53,580
popular now and this is a numbers that

00:24:51,390 --> 00:24:57,540
we get it's actually pretty good number

00:24:53,580 --> 00:24:59,700
so we are able to Train image model and

00:24:57,540 --> 00:25:02,490
image net data set which is about a

00:24:59,700 --> 00:25:05,040
little over 1 million images we're able

00:25:02,490 --> 00:25:07,380
to train at again 3 or 4 minutes because

00:25:05,040 --> 00:25:09,890
you processing rate is 5,000 images per

00:25:07,380 --> 00:25:13,020
second so that's pretty good but is it

00:25:09,890 --> 00:25:15,120
best we can do so if you look at scaling

00:25:13,020 --> 00:25:17,730
factor which is basically a number of

00:25:15,120 --> 00:25:20,040
images per second you get on major GPUs

00:25:17,730 --> 00:25:23,490
compared to 1 GPU this is what we get we

00:25:20,040 --> 00:25:25,800
see that for different networks numbers

00:25:23,490 --> 00:25:28,920
are different but major team is that

00:25:25,800 --> 00:25:31,920
will lose 50% of your can power computer

00:25:28,920 --> 00:25:35,310
resource so on rest net which is one of

00:25:31,920 --> 00:25:39,180
the popular networks we get 31 GPU

00:25:35,310 --> 00:25:41,940
basically 31 X 1 64 GPUs and then on

00:25:39,180 --> 00:25:46,920
inception we get 35 these are two very

00:25:41,940 --> 00:25:49,410
popular models how can we do better so

00:25:46,920 --> 00:25:51,900
what's what's wrong with this model we

00:25:49,410 --> 00:25:53,430
can do two things first we can change

00:25:51,900 --> 00:25:55,200
communication algorithms so instead of

00:25:53,430 --> 00:25:57,480
doing all two all over all two one maybe

00:25:55,200 --> 00:25:59,970
we can do something else second we can

00:25:57,480 --> 00:26:02,790
try to use different kind of networking

00:25:59,970 --> 00:26:06,060
because we exchange gigabytes of data

00:26:02,790 --> 00:26:12,960
every second maybe we can upload doing

00:26:06,060 --> 00:26:15,660
so from the CPU to NICs themselves so we

00:26:12,960 --> 00:26:17,940
invested in this and we came up with a

00:26:15,660 --> 00:26:20,070
framework to do that called horowitz so

00:26:17,940 --> 00:26:21,930
what does horrible do Herat is a

00:26:20,070 --> 00:26:25,200
distributed training framework for

00:26:21,930 --> 00:26:27,179
tensorflow it uses different algorithm

00:26:25,200 --> 00:26:29,070
its users bandwidth up optimal algorithm

00:26:27,179 --> 00:26:29,770
to do this exchange of gradients which I

00:26:29,070 --> 00:26:33,640
will

00:26:29,770 --> 00:26:35,320
deep in it uses rocky or infinite bent

00:26:33,640 --> 00:26:38,350
if it's available where otherwise it can

00:26:35,320 --> 00:26:41,110
also work on playing TCP and then it

00:26:38,350 --> 00:26:44,140
installs on top of desert flow just to

00:26:41,110 --> 00:26:46,930
using Python package installation so as

00:26:44,140 --> 00:26:48,640
a benefit you don't rebuild the whole

00:26:46,930 --> 00:26:50,530
tensor flow which takes like a half an

00:26:48,640 --> 00:26:52,000
hour and experience of doing so you can

00:26:50,530 --> 00:26:55,690
just do people install anything stalls

00:26:52,000 --> 00:26:58,540
in few seconds so how does it work it's

00:26:55,690 --> 00:27:00,280
using very different methods than your

00:26:58,540 --> 00:27:02,710
regular parameter server so what happens

00:27:00,280 --> 00:27:06,550
is it's using so-called ring or reduce

00:27:02,710 --> 00:27:09,520
which is something that was coming from

00:27:06,550 --> 00:27:11,410
the HPC world high performance computing

00:27:09,520 --> 00:27:13,780
world where they do NP III they do

00:27:11,410 --> 00:27:16,300
weather predictions these kind of things

00:27:13,780 --> 00:27:18,760
and there is a paper which is showing

00:27:16,300 --> 00:27:20,710
that this is most optimal algorithm and

00:27:18,760 --> 00:27:22,540
the high-level idea is this let's say

00:27:20,710 --> 00:27:26,620
you have three workers each of them have

00:27:22,540 --> 00:27:29,440
some amount of numbers so they split

00:27:26,620 --> 00:27:31,210
their arrays of numbers into same amount

00:27:29,440 --> 00:27:35,020
s how there is workers so in this

00:27:31,210 --> 00:27:38,110
example three and then they do they send

00:27:35,020 --> 00:27:40,290
each other those numbers and they add

00:27:38,110 --> 00:27:43,530
the numbers together so in this case

00:27:40,290 --> 00:27:46,660
worker first worker will send its first

00:27:43,530 --> 00:27:48,190
chunk to the second worker second worker

00:27:46,660 --> 00:27:50,950
will send second chunk the third worker

00:27:48,190 --> 00:27:53,680
and so on and so forth so they have to

00:27:50,950 --> 00:27:56,080
do this operation same amount of times

00:27:53,680 --> 00:27:59,860
as there is workers minus one and what

00:27:56,080 --> 00:28:01,510
happens is during that time they own

00:27:59,860 --> 00:28:03,490
each worker only communicates with two

00:28:01,510 --> 00:28:05,020
adjacent workers which is good because

00:28:03,490 --> 00:28:07,210
it saves you bandwidth you don't have

00:28:05,020 --> 00:28:10,630
this whole communication pattern so your

00:28:07,210 --> 00:28:14,380
network switches are pretty happy at the

00:28:10,630 --> 00:28:16,900
same time what happens next is after you

00:28:14,380 --> 00:28:19,600
do these N minus 1 iterations you will

00:28:16,900 --> 00:28:21,010
have correct answer for every chunk but

00:28:19,600 --> 00:28:22,690
it will be scattered across all the

00:28:21,010 --> 00:28:25,090
workers so what you do is you just send

00:28:22,690 --> 00:28:28,510
the data around in the similar circular

00:28:25,090 --> 00:28:30,340
pattern and then after another n minus 1

00:28:28,510 --> 00:28:34,840
iterations everybody will have the right

00:28:30,340 --> 00:28:38,410
answer so this algorithm it's as I said

00:28:34,840 --> 00:28:42,520
more optimal and it's only talking to

00:28:38,410 --> 00:28:44,529
two peers so how do we run this on me so

00:28:42,520 --> 00:28:47,950
this is very different right

00:28:44,529 --> 00:28:50,650
we leverage NPI to provide us with

00:28:47,950 --> 00:28:53,740
infrastructure to kick off all the jobs

00:28:50,650 --> 00:28:56,620
and make them aware of each other and we

00:28:53,740 --> 00:28:59,500
use NVDA a library called Nichol to

00:28:56,620 --> 00:29:02,409
actually do all reduce from GPUs so the

00:28:59,500 --> 00:29:05,130
way we deploy this is we provision one

00:29:02,409 --> 00:29:07,690
container on one server and that

00:29:05,130 --> 00:29:10,600
container regardless of how many GPUs

00:29:07,690 --> 00:29:13,990
and that container will launch n copies

00:29:10,600 --> 00:29:15,940
of the trading script where n is number

00:29:13,990 --> 00:29:17,860
of GPUs you have so in this case you

00:29:15,940 --> 00:29:21,669
have two GPUs per server so it's not

00:29:17,860 --> 00:29:24,580
just two processes so what it gives you

00:29:21,669 --> 00:29:29,409
is that you write your code as a single

00:29:24,580 --> 00:29:35,110
GPU code and MPI will launch n number of

00:29:29,409 --> 00:29:39,490
copies per container so in this case we

00:29:35,110 --> 00:29:41,950
have MPI here and we use SSH as a

00:29:39,490 --> 00:29:44,500
communication agent so when MPI launches

00:29:41,950 --> 00:29:49,090
the script it will use Asus age to just

00:29:44,500 --> 00:29:53,169
launch the program on every host so how

00:29:49,090 --> 00:29:55,570
does this compare performance wise we

00:29:53,169 --> 00:29:57,399
see that so these are numbers where we

00:29:55,570 --> 00:30:00,070
see both distribute transfer flow and

00:29:57,399 --> 00:30:03,700
hora what we see the numbers are much

00:30:00,070 --> 00:30:05,980
better so with harm what we get almost

00:30:03,700 --> 00:30:09,760
8000 images per second and that means

00:30:05,980 --> 00:30:13,720
that for the same image net we can do

00:30:09,760 --> 00:30:15,850
one epoch in about two minutes and then

00:30:13,720 --> 00:30:20,230
we see that scaling factors are actually

00:30:15,850 --> 00:30:23,679
very good we see that we get about 33 X

00:30:20,230 --> 00:30:29,620
on Inception y3 and we get about 30 57

00:30:23,679 --> 00:30:32,830
ex on ResNet 101 so these numbers were

00:30:29,620 --> 00:30:35,649
obtained on TCP network we strongly

00:30:32,830 --> 00:30:37,840
believe that on rocky capable Network we

00:30:35,649 --> 00:30:39,340
would get even better numbers but at the

00:30:37,840 --> 00:30:43,480
time of doing this test we didn't have

00:30:39,340 --> 00:30:46,299
correct setup so that's all cool right

00:30:43,480 --> 00:30:48,100
sounds good what about developer

00:30:46,299 --> 00:30:50,320
usability our developer is going to be

00:30:48,100 --> 00:30:52,480
happy to use this not everybody cares

00:30:50,320 --> 00:30:53,710
about squeezing every inch of

00:30:52,480 --> 00:30:56,860
performance they just want have trainer

00:30:53,710 --> 00:30:58,900
model faster so what we found is

00:30:56,860 --> 00:31:01,450
this approach of doing development of

00:30:58,900 --> 00:31:03,160
single GPU tensorflow script and then

00:31:01,450 --> 00:31:06,970
having a distribute actually it's much

00:31:03,160 --> 00:31:08,799
easier than standard hazard flow so in

00:31:06,970 --> 00:31:10,660
this slide there is an example of

00:31:08,799 --> 00:31:13,960
distribute as a flow script from the

00:31:10,660 --> 00:31:16,090
Google website and it doesn't fit you

00:31:13,960 --> 00:31:18,100
cannot read anything because it's it's

00:31:16,090 --> 00:31:20,049
pretty long example and the problem with

00:31:18,100 --> 00:31:22,179
that is there is a lot of concepts that

00:31:20,049 --> 00:31:24,730
with distributors of flow you have to

00:31:22,179 --> 00:31:26,830
learn you have to create occult hours

00:31:24,730 --> 00:31:28,900
average gradients among them within the

00:31:26,830 --> 00:31:31,120
worker then you have to create these

00:31:28,900 --> 00:31:32,440
parameter servers workers and the

00:31:31,120 --> 00:31:34,390
problem is all this code bleeds into

00:31:32,440 --> 00:31:35,650
your model and you have to insert

00:31:34,390 --> 00:31:39,220
different portions of the code in

00:31:35,650 --> 00:31:41,169
different places so with hora what user

00:31:39,220 --> 00:31:43,720
experience is arguably much better

00:31:41,169 --> 00:31:47,100
because so this is the equivalent

00:31:43,720 --> 00:31:49,960
program but in horrid and basically the

00:31:47,100 --> 00:31:51,850
highlighted both parts are there things

00:31:49,960 --> 00:31:54,760
that you need to add to make your single

00:31:51,850 --> 00:31:57,850
GPU program distributed and there is

00:31:54,760 --> 00:31:58,900
basically four major components and one

00:31:57,850 --> 00:32:01,270
of them you need to initialize it

00:31:58,900 --> 00:32:02,830
another is you need to tell it which GPU

00:32:01,270 --> 00:32:05,500
is going to use which is going to be

00:32:02,830 --> 00:32:09,220
same as program original number in your

00:32:05,500 --> 00:32:10,210
container then you wrap your optimizer

00:32:09,220 --> 00:32:12,030
that you use with the stability

00:32:10,210 --> 00:32:13,809
optimizer which is going to take care of

00:32:12,030 --> 00:32:16,240
collecting the gradients and then

00:32:13,809 --> 00:32:18,760
averaging them and the last part is to

00:32:16,240 --> 00:32:20,679
do initialization when everybody does

00:32:18,760 --> 00:32:23,020
initialization let's make sure that

00:32:20,679 --> 00:32:26,049
makes sure that all of them start

00:32:23,020 --> 00:32:29,500
initialization with the same values so

00:32:26,049 --> 00:32:31,690
that we did internally a lot of

00:32:29,500 --> 00:32:33,700
deployment with this model and we found

00:32:31,690 --> 00:32:36,220
that users are much much more happy to

00:32:33,700 --> 00:32:38,140
embrace this model rather than convert

00:32:36,220 --> 00:32:42,820
their program to standard receiver

00:32:38,140 --> 00:32:44,890
telephone and then the best part is this

00:32:42,820 --> 00:32:47,770
 what is available today only

00:32:44,890 --> 00:32:50,620
github if you or your teams or your

00:32:47,770 --> 00:32:53,080
companies are doing deep learning you

00:32:50,620 --> 00:32:54,760
may find it useful so we're super

00:32:53,080 --> 00:32:57,250
welcome for you to use it leave us

00:32:54,760 --> 00:32:59,380
feedback if you find usability some any

00:32:57,250 --> 00:33:03,010
issues or any slicing problems or

00:32:59,380 --> 00:33:05,290
anything give us a shout if you do pinch

00:33:03,010 --> 00:33:06,790
performance benchmark we're super we're

00:33:05,290 --> 00:33:10,509
super interested what numbers you're

00:33:06,790 --> 00:33:12,820
gonna get and we are willing to help you

00:33:10,509 --> 00:33:16,049
you have the best numbers so thank you

00:33:12,820 --> 00:33:16,049
do you have any questions

00:33:38,760 --> 00:33:44,740
so so you were asking do we have a

00:33:42,159 --> 00:33:51,490
training and whether we have information

00:33:44,740 --> 00:33:55,000
about how the full dataset runs or yes

00:33:51,490 --> 00:33:57,880
so we did apply this to actual deep

00:33:55,000 --> 00:34:00,220
learning problems that we have in we I

00:33:57,880 --> 00:34:01,899
can give you one example we had models

00:34:00,220 --> 00:34:05,380
which used to Train in two weeks and we

00:34:01,899 --> 00:34:07,210
made it trained in seven hours so

00:34:05,380 --> 00:34:10,480
Facebook actually recently published the

00:34:07,210 --> 00:34:13,450
paper how to Train imagenet on rest net

00:34:10,480 --> 00:34:15,550
101 on image net in one hour one 256

00:34:13,450 --> 00:34:17,889
GPUs and they're using exactly the same

00:34:15,550 --> 00:34:20,550
approach but they use cafe too so if you

00:34:17,889 --> 00:34:23,589
use cafe two you can use Facebook

00:34:20,550 --> 00:34:33,070
approach if you use tensorflow you can

00:34:23,589 --> 00:34:34,330
use this know if you use so this is

00:34:33,070 --> 00:34:36,970
actually very interesting question when

00:34:34,330 --> 00:34:40,419
you scale distributed training what

00:34:36,970 --> 00:34:44,109
happens is you were you effectively

00:34:40,419 --> 00:34:45,820
increase how many examples per second or

00:34:44,109 --> 00:34:48,369
per batch you process so when you do

00:34:45,820 --> 00:34:50,589
let's say you have single GPU training

00:34:48,369 --> 00:34:52,780
you look at 64 examples you compute

00:34:50,589 --> 00:34:55,419
model update based on 64 examples and

00:34:52,780 --> 00:34:58,599
you basically adjust your model to that

00:34:55,419 --> 00:35:00,880
if you go to super large batch instead

00:34:58,599 --> 00:35:03,280
of doing these small updates you kind of

00:35:00,880 --> 00:35:07,510
do bigger update and the tricky part

00:35:03,280 --> 00:35:10,359
with that is the space of the model is

00:35:07,510 --> 00:35:14,380
not convex so you may fall into some

00:35:10,359 --> 00:35:16,540
trap so if you have huge set that you

00:35:14,380 --> 00:35:19,320
process per one step then it's easier

00:35:16,540 --> 00:35:21,820
for you to fall into most local trap

00:35:19,320 --> 00:35:24,160
Facebook introduced this approach of

00:35:21,820 --> 00:35:27,490
changing learning rate as you scale and

00:35:24,160 --> 00:35:30,130
they demonstrated scaling on 256 GPUs so

00:35:27,490 --> 00:35:31,390
what they recommend doing is you

00:35:30,130 --> 00:35:33,310
basically need to increase your learning

00:35:31,390 --> 00:35:35,349
rate but don't start with the learning

00:35:33,310 --> 00:35:37,119
rate which is equivalent to your

00:35:35,349 --> 00:35:39,220
original learning rate multiplied by

00:35:37,119 --> 00:35:41,140
number of GPUs because that's it reduced

00:35:39,220 --> 00:35:43,780
to moisture so what they did is they

00:35:41,140 --> 00:35:46,210
scale the learning rate during first M

00:35:43,780 --> 00:35:48,540
epoch which is hyper parameter which

00:35:46,210 --> 00:35:51,940
needs tweaked and then you do your

00:35:48,540 --> 00:35:54,820
reduction of learning rate this is the

00:35:51,940 --> 00:35:57,640
open area for research we found that

00:35:54,820 --> 00:36:00,990
they found that it doesn't scale more

00:35:57,640 --> 00:36:03,910
than 8,000 images or eight thousand

00:36:00,990 --> 00:36:05,530
examples per batch so there is

00:36:03,910 --> 00:36:08,560
definitely limit how much you can go

00:36:05,530 --> 00:36:10,480
higher and it's actually very active

00:36:08,560 --> 00:36:12,160
area of research how can you go higher

00:36:10,480 --> 00:36:16,180
if you have thousands of GPUs how do you

00:36:12,160 --> 00:36:19,980
train and it's I think next paper about

00:36:16,180 --> 00:36:19,980
that is should be coming from somebody

00:36:34,140 --> 00:36:39,400
so MPI doesn't have great handle of

00:36:37,930 --> 00:36:42,100
failure recovery it basically crashes

00:36:39,400 --> 00:36:43,930
everything so what we do is we ensure

00:36:42,100 --> 00:36:45,940
that users do checkpoints on fairly

00:36:43,930 --> 00:36:48,730
regular basis and then we just recover

00:36:45,940 --> 00:36:52,000
from us checkpoint on in practice we

00:36:48,730 --> 00:36:56,280
found that we seldom see errors in

00:36:52,000 --> 00:36:56,280
flight hardware is okay

00:37:16,810 --> 00:37:25,770
any more questions last chance all right

00:37:23,710 --> 00:37:29,360
thank you

00:37:25,770 --> 00:37:29,360

YouTube URL: https://www.youtube.com/watch?v=90JAQLRhQRo


