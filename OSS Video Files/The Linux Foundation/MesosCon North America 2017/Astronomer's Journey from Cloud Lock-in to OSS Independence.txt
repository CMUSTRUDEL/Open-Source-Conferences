Title: Astronomer's Journey from Cloud Lock-in to OSS Independence
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Astronomer's Journey from Cloud Lock-in to OSS Independence - Aaron Brongersma, Astronomer

Astronomer is a data engineering platform that collects, processes and unifies users' data so that they can get straight to analytics and data science. Initially, Astronomer built their pipeline out of AWS-managed services. But, when they ran into the limitations of these services, they realized that they had committed to proprietary components, which prevented them from changing cloud providers. 

In this talk, Astronomer outlines their transition to open source services running on Apache Mesos and DC/OS. They started by using Apache Airflow instead of AWS Simple Workflow Service and went on to replace their Kinesis streams with Apache Kafka, and deploy their stream processing applications with Marathon. They will review their remaining challenges and the solutions they are considering. It took some time and effort to for Astronomer decouple from AWS, but they are glad they did.

About

Aaron Brongersma
VP of Engineering, Astronomer
Captions: 
	00:00:00,030 --> 00:00:04,259
so this is astronomers journey from

00:00:02,580 --> 00:00:07,859
cloud lock-in to open source software

00:00:04,259 --> 00:00:10,139
independence this is the obligatory

00:00:07,859 --> 00:00:12,990
slide where you can verify that I am who

00:00:10,139 --> 00:00:15,660
I say I am so Aaron Berger's my VP of

00:00:12,990 --> 00:00:19,680
engineering at astronomer and I'm going

00:00:15,660 --> 00:00:22,650
a little bit here they're at astronomer

00:00:19,680 --> 00:00:24,720
we focus on two major core competencies

00:00:22,650 --> 00:00:28,230
one is streaming data and the other is

00:00:24,720 --> 00:00:31,289
batch style ETL we have a very code

00:00:28,230 --> 00:00:33,480
first approach that we bring we think

00:00:31,289 --> 00:00:34,890
that you know to empower data engineers

00:00:33,480 --> 00:00:37,200
and data scientists we need to be able

00:00:34,890 --> 00:00:39,540
to run native Python code as quickly as

00:00:37,200 --> 00:00:40,920
possible and allow them to really

00:00:39,540 --> 00:00:44,550
understand their business logic better

00:00:40,920 --> 00:00:48,420
than any kind of predictive wizzy gig or

00:00:44,550 --> 00:00:50,219
whatever they're so astronomer it all

00:00:48,420 --> 00:00:53,399
started out where we were chained to its

00:00:50,219 --> 00:00:56,850
cloud our cloud provider but one way or

00:00:53,399 --> 00:00:59,579
one day we got away so early priorities

00:00:56,850 --> 00:01:01,949
at astronomer was getting the market

00:00:59,579 --> 00:01:04,739
quickly for validation so we focused

00:01:01,949 --> 00:01:06,270
really heavily on quick prototyping the

00:01:04,739 --> 00:01:09,060
next goal for us was to get data just in

00:01:06,270 --> 00:01:12,930
motion and then we were always doubling

00:01:09,060 --> 00:01:15,900
down on exploration so v1 of astronomer

00:01:12,930 --> 00:01:18,570
looked looked very different than it is

00:01:15,900 --> 00:01:21,659
today but it started off with a heavy

00:01:18,570 --> 00:01:24,119
utilization in AWS lambda and then we

00:01:21,659 --> 00:01:27,000
set a TPI gateway in front of that to

00:01:24,119 --> 00:01:28,619
kind of act as our API gateway layer we

00:01:27,000 --> 00:01:31,710
used cloud watch for every bit of our

00:01:28,619 --> 00:01:33,869
metrics gathering and we bolted into

00:01:31,710 --> 00:01:36,180
Amazon's Kinesis and elastic Beanstalk

00:01:33,869 --> 00:01:38,159
to kind of do our auto scaling now this

00:01:36,180 --> 00:01:39,750
works when you have a team of between

00:01:38,159 --> 00:01:41,700
three to five it worked out well for

00:01:39,750 --> 00:01:44,250
start-up it got us to market quickly it

00:01:41,700 --> 00:01:46,590
it got our customers on version 1 of our

00:01:44,250 --> 00:01:48,049
application but there was trouble in

00:01:46,590 --> 00:01:50,549
paradise

00:01:48,049 --> 00:01:53,189
we had some engineering obstacles which

00:01:50,549 --> 00:01:55,409
were that as we were scaling our user

00:01:53,189 --> 00:01:57,000
base be a number of events that we were

00:01:55,409 --> 00:01:59,820
sending through API gateway was becoming

00:01:57,000 --> 00:02:01,590
cost prohibitive the next thing we

00:01:59,820 --> 00:02:04,560
learned and we learned this the hard way

00:02:01,590 --> 00:02:05,490
was that Kinesis isn't Kafka so as you

00:02:04,560 --> 00:02:06,780
start seeing they're really awesome

00:02:05,490 --> 00:02:09,300
things that happen in the Kafka

00:02:06,780 --> 00:02:10,319
ecosystem you it leaves you wanting for

00:02:09,300 --> 00:02:12,150
more when you're

00:02:10,319 --> 00:02:14,879
you're running your data through through

00:02:12,150 --> 00:02:16,890
only a part of the potential of what

00:02:14,879 --> 00:02:19,019
Kafka is and we also found that

00:02:16,890 --> 00:02:21,900
provisioning with elastic beanstalk was

00:02:19,019 --> 00:02:23,310
very slow and antiquated it was one of

00:02:21,900 --> 00:02:26,970
the first ways that we started deploying

00:02:23,310 --> 00:02:28,379
containers on AWS moving forward it it

00:02:26,970 --> 00:02:31,470
seriously left us lacking in that

00:02:28,379 --> 00:02:33,060
department this is my face when I saw

00:02:31,470 --> 00:02:35,220
the Amazon bill the very first time we

00:02:33,060 --> 00:02:38,220
ran a billion events through our API

00:02:35,220 --> 00:02:41,269
gateway so that's my thoughts on API

00:02:38,220 --> 00:02:44,160
gateway we also have product obstacles

00:02:41,269 --> 00:02:47,120
so our customers had issues accessing

00:02:44,160 --> 00:02:49,319
all of their data and so if you're a SAS

00:02:47,120 --> 00:02:51,480
provider and you want to operationalize

00:02:49,319 --> 00:02:54,180
someone else's data warehouse that it

00:02:51,480 --> 00:02:55,950
becomes a very you know custom VPN

00:02:54,180 --> 00:02:57,750
solution that had open ports to the

00:02:55,950 --> 00:02:59,340
public they had a whitelist IP addresses

00:02:57,750 --> 00:03:01,560
so what we found is we really needed to

00:02:59,340 --> 00:03:04,879
start doing our work inside of a

00:03:01,560 --> 00:03:09,510
firewall or inside of a private network

00:03:04,879 --> 00:03:10,709
so quoted from our CTO we basically need

00:03:09,510 --> 00:03:12,540
a stack that could run our data

00:03:10,709 --> 00:03:14,760
engineering tools anywhere we wanted to

00:03:12,540 --> 00:03:16,079
be anywhere our customers were and we

00:03:14,760 --> 00:03:18,840
didn't want to be limited by any one

00:03:16,079 --> 00:03:20,120
cloud service provider so that landed us

00:03:18,840 --> 00:03:22,260
on DCOs

00:03:20,120 --> 00:03:23,910
we knew that a lot of the components

00:03:22,260 --> 00:03:26,700
that we were using at astronomer things

00:03:23,910 --> 00:03:28,530
like Cassandra and SPARC were first

00:03:26,700 --> 00:03:30,000
class citizens and mezzos but what we

00:03:28,530 --> 00:03:31,530
didn't know is how to get mezzos up and

00:03:30,000 --> 00:03:33,930
running quickly so we were a small

00:03:31,530 --> 00:03:36,329
engineering team we'd never really

00:03:33,930 --> 00:03:37,799
worked on the smack stack before so DCOs

00:03:36,329 --> 00:03:39,500
was a really quick way for us to get up

00:03:37,799 --> 00:03:43,470
and running and have best practices

00:03:39,500 --> 00:03:46,169
rolled out throughout our fleet so DCOs

00:03:43,470 --> 00:03:49,230
an astronomer we used eCos to run all of

00:03:46,169 --> 00:03:51,510
our Apache airflow tasks against so

00:03:49,230 --> 00:03:53,519
Apache airflow is our tool that we use

00:03:51,510 --> 00:03:56,910
for all of our batch ETL it came out of

00:03:53,519 --> 00:03:58,709
Airbnb it has a mezzos executor that is

00:03:56,910 --> 00:04:00,660
pretty awesome so we can actually scale

00:03:58,709 --> 00:04:02,669
out all of our ETL jobs inside of docker

00:04:00,660 --> 00:04:06,389
containers and scale that up across our

00:04:02,669 --> 00:04:08,370
entire DCO sms's cluster that we have we

00:04:06,389 --> 00:04:10,079
moved marathon in to replace elastic

00:04:08,370 --> 00:04:11,489
Beanstalk that may be one of the the

00:04:10,079 --> 00:04:14,849
greatest things that we did I was like a

00:04:11,489 --> 00:04:17,160
night day change and then we really

00:04:14,849 --> 00:04:19,769
focused on task scheduling at first we

00:04:17,160 --> 00:04:20,269
were doing nightly batches into Amazon

00:04:19,769 --> 00:04:22,069
Reggie

00:04:20,269 --> 00:04:23,780
and we were finding that you know either

00:04:22,069 --> 00:04:25,580
our customers created too many events or

00:04:23,780 --> 00:04:27,229
it wasn't real time enough for them so

00:04:25,580 --> 00:04:29,240
we've started moving more to a spark

00:04:27,229 --> 00:04:31,550
streaming technology which allows us to

00:04:29,240 --> 00:04:34,190
micro batches and between anytime

00:04:31,550 --> 00:04:35,870
between a attend to two hour window we

00:04:34,190 --> 00:04:39,410
can micro batch into redshift using

00:04:35,870 --> 00:04:41,449
spark streaming so Apache airflow the

00:04:39,410 --> 00:04:43,669
reason why we've stumbled upon Apache

00:04:41,449 --> 00:04:45,590
airflow is we were running batch ETL

00:04:43,669 --> 00:04:47,569
jobs but they weren't running in a

00:04:45,590 --> 00:04:48,979
dependency driven manner so one of the

00:04:47,569 --> 00:04:51,380
things that would happen is that our

00:04:48,979 --> 00:04:52,940
nightly ETL task that was supposed to

00:04:51,380 --> 00:04:55,400
populate our data warehouse didn't

00:04:52,940 --> 00:04:56,840
trigger so traditionally with cron you

00:04:55,400 --> 00:04:58,819
would see job one would run every night

00:04:56,840 --> 00:05:00,590
at midnight and job two might run at

00:04:58,819 --> 00:05:03,590
1:00 in the morning there was no nothing

00:05:00,590 --> 00:05:06,349
blocking job two when one failed so

00:05:03,590 --> 00:05:08,449
bringing in a patchy airflow tasks are

00:05:06,349 --> 00:05:10,520
laid out in what's considered a dag

00:05:08,449 --> 00:05:12,500
which is the distributed ace acrylic

00:05:10,520 --> 00:05:15,349
graph and it's just a graph of tasks

00:05:12,500 --> 00:05:17,000
like start here do this do all of these

00:05:15,349 --> 00:05:18,770
and then report back when you're done

00:05:17,000 --> 00:05:20,180
now this is really important when you're

00:05:18,770 --> 00:05:21,889
doing transforms or you're actually

00:05:20,180 --> 00:05:23,210
trying to do enrichments so when you're

00:05:21,889 --> 00:05:25,310
bringing in data from a number of

00:05:23,210 --> 00:05:27,169
different data sources you can make sure

00:05:25,310 --> 00:05:29,150
that you know if your connection timed

00:05:27,169 --> 00:05:32,830
out to your MongoDB database you can

00:05:29,150 --> 00:05:35,960
still rerun that one particular task

00:05:32,830 --> 00:05:37,340
so why airflow on mezzos the first thing

00:05:35,960 --> 00:05:41,509
it was that we were able to use the

00:05:37,340 --> 00:05:42,560
mezzo scheduler which was robust one of

00:05:41,509 --> 00:05:44,990
the greatest things that we really love

00:05:42,560 --> 00:05:46,490
is that it's the the actual task

00:05:44,990 --> 00:05:48,830
scheduling is one of the least painful

00:05:46,490 --> 00:05:50,630
parts for our entire fleet we spent a

00:05:48,830 --> 00:05:52,219
lot more time tuning the docker images

00:05:50,630 --> 00:05:54,620
or making sure that the actual Python

00:05:52,219 --> 00:05:56,120
tags are written correctly but when it

00:05:54,620 --> 00:05:58,130
comes to scaling out elastically we're

00:05:56,120 --> 00:06:00,710
easily able to do millions of events an

00:05:58,130 --> 00:06:02,690
hour we were up and running quickly so

00:06:00,710 --> 00:06:04,940
as soon as we had DC US we had air flow

00:06:02,690 --> 00:06:06,590
with our new custom executors we were

00:06:04,940 --> 00:06:09,349
able to start distributing tasks out

00:06:06,590 --> 00:06:11,659
within an hour of scaling up a brand new

00:06:09,349 --> 00:06:13,310
cluster and like I said millions of

00:06:11,659 --> 00:06:14,449
tasks a day and that was really

00:06:13,310 --> 00:06:16,490
important for a lot of the customers

00:06:14,449 --> 00:06:18,650
where they were used to only having

00:06:16,490 --> 00:06:20,479
nightly ETL jobs happen and so now we're

00:06:18,650 --> 00:06:22,310
able to give them insights up to the up

00:06:20,479 --> 00:06:25,639
to the hour or maybe even less depending

00:06:22,310 --> 00:06:27,590
on where the data sources are so more on

00:06:25,639 --> 00:06:30,110
a airflow at astronomer we use it for

00:06:27,590 --> 00:06:32,180
all our data pipelines originally we had

00:06:30,110 --> 00:06:34,250
a tool kit called Aires and

00:06:32,180 --> 00:06:36,370
was a JavaScript operator that'll

00:06:34,250 --> 00:06:39,530
actually allowed you to use nodejs

00:06:36,370 --> 00:06:42,050
inside of your airflow tasks that's

00:06:39,530 --> 00:06:43,490
absolutely insane so it's really kind of

00:06:42,050 --> 00:06:45,139
went against the original principles of

00:06:43,490 --> 00:06:46,699
airflow and being dependency driven we

00:06:45,139 --> 00:06:48,650
were very much focused on streaming in

00:06:46,699 --> 00:06:51,229
the beginning so we've kind of dialed

00:06:48,650 --> 00:06:53,750
the back the JavaScript and have doubled

00:06:51,229 --> 00:06:55,669
down now on Python the next thing that

00:06:53,750 --> 00:06:57,380
we really needed airflow for is our

00:06:55,669 --> 00:06:59,600
ability to our intelligent redshift

00:06:57,380 --> 00:07:01,940
loading so we guarantee that the data

00:06:59,600 --> 00:07:04,160
comes in that also triggers a task that

00:07:01,940 --> 00:07:06,110
executes a spark streaming job and then

00:07:04,160 --> 00:07:08,300
we're able to make sure that we can do

00:07:06,110 --> 00:07:10,340
our schema inference so we get much

00:07:08,300 --> 00:07:12,860
better to find tables when we go inside

00:07:10,340 --> 00:07:14,690
of a data warehouse and then once again

00:07:12,860 --> 00:07:18,289
just hitting on dependency driven tasks

00:07:14,690 --> 00:07:20,210
running so next thing we had to kill the

00:07:18,289 --> 00:07:22,280
astronomer was we needed to get from

00:07:20,210 --> 00:07:23,630
Kinesis to Kafka more and more customers

00:07:22,280 --> 00:07:24,800
that we were working with we're sending

00:07:23,630 --> 00:07:27,349
more and more events through our

00:07:24,800 --> 00:07:29,690
pipeline we went from 500 million and

00:07:27,349 --> 00:07:31,190
now we're up to three billion so what

00:07:29,690 --> 00:07:32,960
that starts you start feeling the pain

00:07:31,190 --> 00:07:34,430
with starting out Kinesis you started

00:07:32,960 --> 00:07:37,460
feeling the pain when you're using api

00:07:34,430 --> 00:07:38,870
gateway so some things that jumped out

00:07:37,460 --> 00:07:41,000
to us the first thing that we really

00:07:38,870 --> 00:07:41,599
struggled with was the Kinesis client

00:07:41,000 --> 00:07:43,580
library

00:07:41,599 --> 00:07:44,900
now the Kinesis client library looks

00:07:43,580 --> 00:07:46,220
pretty cool at first because it allows

00:07:44,900 --> 00:07:47,690
you use any kind of language you want

00:07:46,220 --> 00:07:49,490
and as long as you're communicating

00:07:47,690 --> 00:07:51,260
through standard out you can actually

00:07:49,490 --> 00:07:53,780
process a stream and it'll do all your

00:07:51,260 --> 00:07:56,570
check pointing the things that we run

00:07:53,780 --> 00:07:58,280
into is that since it logs directly to

00:07:56,570 --> 00:07:59,870
standard out and it's very sensitive

00:07:58,280 --> 00:08:01,190
about standard error it becomes really

00:07:59,870 --> 00:08:03,169
difficult to troubleshoot when things

00:08:01,190 --> 00:08:04,550
actually go wrong so we've had a quite a

00:08:03,169 --> 00:08:06,259
few race conditions where we have a

00:08:04,550 --> 00:08:09,590
really difficult time trying to pull

00:08:06,259 --> 00:08:11,509
those out of our elk stack logs not

00:08:09,590 --> 00:08:12,710
available everywhere more and more of

00:08:11,509 --> 00:08:16,130
the companies that we're doing business

00:08:12,710 --> 00:08:18,259
with are not running on Amazon they're

00:08:16,130 --> 00:08:19,699
running within whether it's a sure

00:08:18,259 --> 00:08:21,440
whether they're running inside of Google

00:08:19,699 --> 00:08:24,289
compute or they're running on their own

00:08:21,440 --> 00:08:25,849
data center still the seems like back

00:08:24,289 --> 00:08:27,169
home in Cincinnati the bigger the tower

00:08:25,849 --> 00:08:28,669
the more people that are still running

00:08:27,169 --> 00:08:31,940
inside of their data centers they're not

00:08:28,669 --> 00:08:34,430
necessarily adopting the cloud so the

00:08:31,940 --> 00:08:36,409
road to Kafka for us first was a rewrite

00:08:34,430 --> 00:08:38,000
of our API so we moved off our

00:08:36,409 --> 00:08:40,159
JavaScript API that was running behind

00:08:38,000 --> 00:08:42,500
API gateway and it was using lambda

00:08:40,159 --> 00:08:44,030
functions and we rewrote that and go we

00:08:42,500 --> 00:08:46,190
got a 10x improvement and

00:08:44,030 --> 00:08:48,320
and performance out of that and when we

00:08:46,190 --> 00:08:49,580
were rewriting the API and go we started

00:08:48,320 --> 00:08:50,630
working on the abstractions that would

00:08:49,580 --> 00:08:51,620
allow us to switch our underlying

00:08:50,630 --> 00:08:54,680
message service

00:08:51,620 --> 00:08:56,960
so we abstract it away just just generic

00:08:54,680 --> 00:09:00,110
messages and those generic messages can

00:08:56,960 --> 00:09:02,000
either go into Kafka Kinesis we improved

00:09:00,110 --> 00:09:04,100
our provisioning monitoring and testing

00:09:02,000 --> 00:09:05,870
so this is really important that it's

00:09:04,100 --> 00:09:07,910
it's one thing to go do a docker compose

00:09:05,870 --> 00:09:09,560
up and have a cough gets another one you

00:09:07,910 --> 00:09:11,420
roll it out in production so there's a

00:09:09,560 --> 00:09:12,770
lot of tuning and micro adjustments that

00:09:11,420 --> 00:09:15,140
you need to make before you actually run

00:09:12,770 --> 00:09:16,580
over production client loads so we've

00:09:15,140 --> 00:09:18,770
been spending quite a bit more time on

00:09:16,580 --> 00:09:20,420
that and that's one of the bigger pain

00:09:18,770 --> 00:09:23,330
points of running your own Kafka versus

00:09:20,420 --> 00:09:24,410
using something like Kinesis and then

00:09:23,330 --> 00:09:26,930
one of the other things that we needed

00:09:24,410 --> 00:09:28,970
to do is run all run both of our systems

00:09:26,930 --> 00:09:31,730
in parallel so what we have now is we're

00:09:28,970 --> 00:09:33,650
running 99% of our traffic right through

00:09:31,730 --> 00:09:36,920
Kinesis still and then we're running our

00:09:33,650 --> 00:09:38,450
new canary version of our kinesio of our

00:09:36,920 --> 00:09:40,820
stream processing library that's written

00:09:38,450 --> 00:09:42,440
in go and it uses Kafka and once we're

00:09:40,820 --> 00:09:45,320
through the final QA checks there will

00:09:42,440 --> 00:09:46,490
be on that production so one of the

00:09:45,320 --> 00:09:49,130
things that jumps out to me though is

00:09:46,490 --> 00:09:50,630
that not all AWS tools are created equal

00:09:49,130 --> 00:09:52,430
and this is something that you forget

00:09:50,630 --> 00:09:54,770
when you start working with tools like

00:09:52,430 --> 00:09:56,810
Amazon s3 would actually had an incident

00:09:54,770 --> 00:09:58,970
yesterday that went down but tools that

00:09:56,810 --> 00:10:01,810
tend to be very durable so RDS tends to

00:09:58,970 --> 00:10:03,680
be very durable s3 is durable but

00:10:01,810 --> 00:10:05,450
oftentimes you start looking for new

00:10:03,680 --> 00:10:06,890
tools like the simple workflow service

00:10:05,450 --> 00:10:08,420
that came out of Amazon those tools

00:10:06,890 --> 00:10:10,850
aren't necessarily as battle-hardened

00:10:08,420 --> 00:10:11,960
they might be more in a beta state even

00:10:10,850 --> 00:10:14,540
though it's been released to the public

00:10:11,960 --> 00:10:16,610
so a lot of the tools we tried to use

00:10:14,540 --> 00:10:18,170
that were more bleeding-edge definitely

00:10:16,610 --> 00:10:20,830
didn't hold up to any of the workloads

00:10:18,170 --> 00:10:23,060
that we were putting against them

00:10:20,830 --> 00:10:24,770
exploration so this is one of the key

00:10:23,060 --> 00:10:27,050
philosophies we have at astronomer and

00:10:24,770 --> 00:10:28,760
through our exploration we've really

00:10:27,050 --> 00:10:31,300
made some really awesome improvements to

00:10:28,760 --> 00:10:34,160
how we deploy DCOs and our fleet

00:10:31,300 --> 00:10:36,320
CloudFormation to terraform so when my

00:10:34,160 --> 00:10:37,700
team first started astronomer it was the

00:10:36,320 --> 00:10:39,740
first time we'd ever operationalize

00:10:37,700 --> 00:10:40,820
mezzos anywhere so we had a lot of work

00:10:39,740 --> 00:10:42,410
to do to get up to speed on

00:10:40,820 --> 00:10:44,120
understanding what we were working with

00:10:42,410 --> 00:10:45,860
and how to support it when it goes down

00:10:44,120 --> 00:10:47,660
or when nodes go down and how do we do

00:10:45,860 --> 00:10:49,910
healthy health housekeeping across that

00:10:47,660 --> 00:10:51,440
fleet so one of the first steps we did

00:10:49,910 --> 00:10:53,150
was reverse engineered our cloud

00:10:51,440 --> 00:10:55,550
formation templates and brought those in

00:10:53,150 --> 00:10:57,020
to terraform so what that did was it

00:10:55,550 --> 00:10:57,440
gave us a really easy way for any of our

00:10:57,020 --> 00:10:58,970
develop

00:10:57,440 --> 00:11:00,980
to bring up their test environments on

00:10:58,970 --> 00:11:03,050
the fly so if we needed to test a new

00:11:00,980 --> 00:11:04,880
version of D cos it was really easy for

00:11:03,050 --> 00:11:07,190
us to make my code micro adjustments and

00:11:04,880 --> 00:11:09,050
our config this also allowed us to have

00:11:07,190 --> 00:11:11,210
an ability to scale out different node

00:11:09,050 --> 00:11:13,880
types fairly easily so when we wanted to

00:11:11,210 --> 00:11:16,010
roll out new high CPU or high mem or

00:11:13,880 --> 00:11:18,250
change an underlying instance type it

00:11:16,010 --> 00:11:20,810
was fairly easy for us to add those in

00:11:18,250 --> 00:11:22,610
so this really set forth the

00:11:20,810 --> 00:11:24,140
infrastructure is code so all of our

00:11:22,610 --> 00:11:25,610
code changes are stored inside of our

00:11:24,140 --> 00:11:28,520
terraform configs that are running

00:11:25,610 --> 00:11:29,690
inside a console so any one of our

00:11:28,520 --> 00:11:31,130
engineers can be on any different

00:11:29,690 --> 00:11:32,870
machine and we all have the same shared

00:11:31,130 --> 00:11:35,630
state up inside a console so that was

00:11:32,870 --> 00:11:37,160
really good for us for change control we

00:11:35,630 --> 00:11:38,870
focused on a hundred percent repeatable

00:11:37,160 --> 00:11:41,090
installs without having to do any under

00:11:38,870 --> 00:11:42,530
the hood manual configuration so this

00:11:41,090 --> 00:11:43,940
was really big for us for bringing up

00:11:42,530 --> 00:11:46,340
some of the underlying components like

00:11:43,940 --> 00:11:48,800
Postgres on RDS all of our networking

00:11:46,340 --> 00:11:50,540
inside of amazon inside of our DPC all

00:11:48,800 --> 00:11:51,740
of that is done in one click it

00:11:50,540 --> 00:11:53,600
provisions everything from your

00:11:51,740 --> 00:11:54,860
bootstrap all of your masters in any of

00:11:53,600 --> 00:11:56,960
the worker nodes and any number of

00:11:54,860 --> 00:11:58,820
availability zones we've scaled this up

00:11:56,960 --> 00:12:01,610
to over a couple of hundred VMs right

00:11:58,820 --> 00:12:04,070
now but we do have issues along the way

00:12:01,610 --> 00:12:06,260
on our way to scale up it feels more

00:12:04,070 --> 00:12:08,930
like a pet than a cattle situation and

00:12:06,260 --> 00:12:10,640
as we start scaling out terraform larger

00:12:08,930 --> 00:12:12,260
and larger in our organization we

00:12:10,640 --> 00:12:13,970
definitely have more pockets of very

00:12:12,260 --> 00:12:16,460
sensitive servers that need to be kept

00:12:13,970 --> 00:12:18,170
more like pets that seems like a bit of

00:12:16,460 --> 00:12:19,340
an anti-pattern to me so my team has

00:12:18,170 --> 00:12:21,260
been doing a quite a bit more work to

00:12:19,340 --> 00:12:22,780
understand how we're we're gonna make

00:12:21,260 --> 00:12:25,790
sure we have healthy scheduling of

00:12:22,780 --> 00:12:26,840
rescheduling of tasks that die but one

00:12:25,790 --> 00:12:29,810
of the things that's really hurt us

00:12:26,840 --> 00:12:31,070
lately here is the fact that every one

00:12:29,810 --> 00:12:32,600
of our servers we have to be very

00:12:31,070 --> 00:12:37,040
careful when we're detaching like R

00:12:32,600 --> 00:12:38,570
x-ray volumes and whatnot the next thing

00:12:37,040 --> 00:12:40,010
that we focused on and one of the tools

00:12:38,570 --> 00:12:42,380
we've had one of the best pieces of

00:12:40,010 --> 00:12:43,970
success with this prometheus so we went

00:12:42,380 --> 00:12:45,920
right after the throat of our cloud

00:12:43,970 --> 00:12:47,510
watch metrics and decided that we wanted

00:12:45,920 --> 00:12:49,610
to try to keep all of that data in-house

00:12:47,510 --> 00:12:52,550
that meant that we had to write actual

00:12:49,610 --> 00:12:54,140
custom exporters for Prometheus so we

00:12:52,550 --> 00:12:55,400
write those in go and we contribute

00:12:54,140 --> 00:12:57,230
those back-end to the open source

00:12:55,400 --> 00:12:58,610
community but it's pretty awesome to be

00:12:57,230 --> 00:13:01,540
able to change any of the micro

00:12:58,610 --> 00:13:03,890
adjustments that you want to monitor for

00:13:01,540 --> 00:13:05,510
all of our systems are monitored out of

00:13:03,890 --> 00:13:07,100
the box so every machine that comes up

00:13:05,510 --> 00:13:08,930
automatically subscribes with the

00:13:07,100 --> 00:13:09,820
correct tags so we never really worried

00:13:08,930 --> 00:13:10,960
about having that one

00:13:09,820 --> 00:13:13,750
since that came up that didn't have the

00:13:10,960 --> 00:13:14,830
monitoring agents on it I already

00:13:13,750 --> 00:13:16,660
touched on the fact that we write our

00:13:14,830 --> 00:13:19,210
own exporters which is actually really

00:13:16,660 --> 00:13:21,520
easy but polling makes alerting really

00:13:19,210 --> 00:13:24,010
easy so one of the differences that we

00:13:21,520 --> 00:13:26,320
have is that real-time logs actually

00:13:24,010 --> 00:13:28,510
flow into elasticsearch and so we can do

00:13:26,320 --> 00:13:30,400
pattern matching and you know regular

00:13:28,510 --> 00:13:32,110
expression pattern matching there but

00:13:30,400 --> 00:13:34,360
for Prometheus Prometheus looks at the

00:13:32,110 --> 00:13:35,650
health of the service if the numbers are

00:13:34,360 --> 00:13:37,240
out of whack if you're using too much

00:13:35,650 --> 00:13:41,290
CPU so we really have two different

00:13:37,240 --> 00:13:43,090
types of alerting in our fleet Kong and

00:13:41,290 --> 00:13:48,160
the inevitable the demise of the API

00:13:43,090 --> 00:13:49,870
gateway so we found Kong almost by

00:13:48,160 --> 00:13:51,970
chance we weren't looking for it in the

00:13:49,870 --> 00:13:56,050
beginning we were a heavy engine X shop

00:13:51,970 --> 00:13:57,490
and so early on we needed to start

00:13:56,050 --> 00:13:59,800
replacing some of our graph QL

00:13:57,490 --> 00:14:02,700
authentication which got us down the

00:13:59,800 --> 00:14:05,050
path of looking at Kong we already run

00:14:02,700 --> 00:14:06,400
cassandra throughout our fleet so it

00:14:05,050 --> 00:14:07,870
made it really nice to run all of your

00:14:06,400 --> 00:14:09,400
different Kong nodes across all the

00:14:07,870 --> 00:14:11,260
availability zones and then still have

00:14:09,400 --> 00:14:13,840
your Cassandra nodes across availability

00:14:11,260 --> 00:14:15,520
zones as well so we have much more fault

00:14:13,840 --> 00:14:17,500
tolerance there with our API gateway

00:14:15,520 --> 00:14:20,020
than we had before plus we have we were

00:14:17,500 --> 00:14:21,580
able to run it on fixed costs so three

00:14:20,020 --> 00:14:23,200
instances handles just as much traffic

00:14:21,580 --> 00:14:26,320
as we were running through API gateway

00:14:23,200 --> 00:14:28,390
and we haven't seen a hit there it's got

00:14:26,320 --> 00:14:29,740
built-in authentication and it handles

00:14:28,390 --> 00:14:32,440
all of our rate limiting

00:14:29,740 --> 00:14:35,170
it'll also invoke some of our old API

00:14:32,440 --> 00:14:37,960
lambda functions which is really nice as

00:14:35,170 --> 00:14:41,560
we were doing our transition to go and

00:14:37,960 --> 00:14:44,940
then also it was backed by Cassandra so

00:14:41,560 --> 00:14:47,740
time series databases for fun and profit

00:14:44,940 --> 00:14:50,320
kairos DB didn't seem like a first

00:14:47,740 --> 00:14:52,060
choice for us our engineering team had

00:14:50,320 --> 00:14:54,220
done a lot of work with influx DB in the

00:14:52,060 --> 00:14:55,510
past and we have had great success but

00:14:54,220 --> 00:14:57,400
one of the things that happened is we

00:14:55,510 --> 00:15:00,130
were start sending more and more as we

00:14:57,400 --> 00:15:01,540
passed the 500 million events we really

00:15:00,130 --> 00:15:03,970
wanted to have better durability of the

00:15:01,540 --> 00:15:05,980
data that we have for influx that

00:15:03,970 --> 00:15:08,410
started us down the path of Kairos so

00:15:05,980 --> 00:15:10,150
Kairos sits on top of Cassandra and it

00:15:08,410 --> 00:15:11,920
works as an abstraction layer it gives

00:15:10,150 --> 00:15:13,600
you a time series database it's highly

00:15:11,920 --> 00:15:15,700
opinionated on how you insert the data

00:15:13,600 --> 00:15:17,880
in but it's very performant for writes

00:15:15,700 --> 00:15:20,340
and we haven't do automatic

00:15:17,880 --> 00:15:21,780
roll ups and aggregations and our UI and

00:15:20,340 --> 00:15:23,730
all of our billing services are backed

00:15:21,780 --> 00:15:25,680
by this so whenever we want to pull up

00:15:23,730 --> 00:15:28,170
any of our uses metrics all of us backed

00:15:25,680 --> 00:15:30,780
inside of Cassandra we've seen extreme

00:15:28,170 --> 00:15:31,860
durability even when you do have nodes

00:15:30,780 --> 00:15:33,870
that go down

00:15:31,860 --> 00:15:36,570
Kairos does a really great job of doing

00:15:33,870 --> 00:15:38,100
the housekeeping and re-indexing like I

00:15:36,570 --> 00:15:39,590
said it really came as an underdog as a

00:15:38,100 --> 00:15:41,550
selection of a tool and our stack

00:15:39,590 --> 00:15:44,280
because there are definitely some more

00:15:41,550 --> 00:15:46,740
more up-and-coming technologies but we

00:15:44,280 --> 00:15:50,600
haven't been burned by Kairos yet it's

00:15:46,740 --> 00:15:53,280
been awesome so what's next

00:15:50,600 --> 00:15:54,810
so under development we've been spending

00:15:53,280 --> 00:15:57,690
quite a bit more time around a patchy

00:15:54,810 --> 00:15:59,610
druid a lot of the queries that we do

00:15:57,690 --> 00:16:02,160
tend to have backfill of thought

00:15:59,610 --> 00:16:04,170
required as well so maybe we get a task

00:16:02,160 --> 00:16:05,970
up to date within the the last hour or

00:16:04,170 --> 00:16:07,290
two hours but sometimes our customers

00:16:05,970 --> 00:16:09,840
will last up for us to go back and

00:16:07,290 --> 00:16:11,160
backfill for the last month so one of

00:16:09,840 --> 00:16:12,620
the things that we would love to do is

00:16:11,160 --> 00:16:15,450
make sure that we have a better set of

00:16:12,620 --> 00:16:17,460
tooling around pulling that data out of

00:16:15,450 --> 00:16:19,560
RAM instead of pulling that back off of

00:16:17,460 --> 00:16:21,390
a disk so we want to start powering some

00:16:19,560 --> 00:16:24,870
of our back load technology directly off

00:16:21,390 --> 00:16:26,820
of druid we're super pumped about kafka

00:16:24,870 --> 00:16:29,100
connect so that's one of the reasons why

00:16:26,820 --> 00:16:31,500
we've really pushed hard to move off of

00:16:29,100 --> 00:16:34,980
Kinesis as well is that there's

00:16:31,500 --> 00:16:36,900
definitely more of a need in the in the

00:16:34,980 --> 00:16:39,780
ecosystem that we're selling into for

00:16:36,900 --> 00:16:42,000
more real-time you know transactions of

00:16:39,780 --> 00:16:45,450
more traditional relational database

00:16:42,000 --> 00:16:47,940
systems so it's not all of the data that

00:16:45,450 --> 00:16:49,260
we want to replicate across but once you

00:16:47,940 --> 00:16:50,460
start realizing what patterns you're

00:16:49,260 --> 00:16:52,410
looking for and you're in your

00:16:50,460 --> 00:16:53,490
transactions of your databases these can

00:16:52,410 --> 00:16:56,370
actually drive some really awesome

00:16:53,490 --> 00:16:57,810
business insights it's also stopped us

00:16:56,370 --> 00:17:00,420
from having to run more traditional

00:16:57,810 --> 00:17:02,430
batch ETL jobs and we've been able to

00:17:00,420 --> 00:17:05,790
just replace them straight out with with

00:17:02,430 --> 00:17:07,290
kafka connect one component that we

00:17:05,790 --> 00:17:09,959
still struggle with moving off of is

00:17:07,290 --> 00:17:11,339
Amazon s3 the fact that it's just

00:17:09,959 --> 00:17:12,900
durable and it's most of the time it's

00:17:11,339 --> 00:17:16,170
up I'm gonna knock on wood here real

00:17:12,900 --> 00:17:17,760
quick because it wasn't yesterday but it

00:17:16,170 --> 00:17:18,990
makes it really difficult for us to go

00:17:17,760 --> 00:17:21,720
and try to make an investment in

00:17:18,990 --> 00:17:22,829
something like SEF or Mineo we use Mineo

00:17:21,720 --> 00:17:25,860
a lot more in our development

00:17:22,829 --> 00:17:26,940
environments but stuff is certainly a

00:17:25,860 --> 00:17:28,620
lot more

00:17:26,940 --> 00:17:30,960
you know technical debt for our team to

00:17:28,620 --> 00:17:33,210
bring up we're looking to have that in

00:17:30,960 --> 00:17:35,010
like q1 next year of heavens stuff kind

00:17:33,210 --> 00:17:37,110
of figured out and and operationalized

00:17:35,010 --> 00:17:38,430
now one of the things that when we start

00:17:37,110 --> 00:17:40,320
focused on having a software as a

00:17:38,430 --> 00:17:41,940
service platform interconnectivity

00:17:40,320 --> 00:17:44,310
between our services is really kind of

00:17:41,940 --> 00:17:45,870
starts to get dangerous in the beginning

00:17:44,310 --> 00:17:47,280
you can firewall and reject almost

00:17:45,870 --> 00:17:49,230
everything in and I you know in an

00:17:47,280 --> 00:17:50,580
almost like in an IP table rule but that

00:17:49,230 --> 00:17:52,830
doesn't scale as you start adding

00:17:50,580 --> 00:17:54,810
hundreds and hundreds of new containers

00:17:52,830 --> 00:17:57,830
into your fleet so we're starting to do

00:17:54,810 --> 00:18:01,170
a lot more rd into tools like sto we've

00:17:57,830 --> 00:18:02,610
also linked or D like very important for

00:18:01,170 --> 00:18:04,140
us to figure out that service mesh

00:18:02,610 --> 00:18:07,950
fabric therefore multi-tenant

00:18:04,140 --> 00:18:09,510
environments we've got a we kind of

00:18:07,950 --> 00:18:11,640
danced a jig when we solve kubernetes

00:18:09,510 --> 00:18:13,140
available because there's some lighter

00:18:11,640 --> 00:18:15,780
weight workloads that we're looking into

00:18:13,140 --> 00:18:17,460
running tasks on that don't necessarily

00:18:15,780 --> 00:18:19,290
need all of the durability or the

00:18:17,460 --> 00:18:22,680
ability to roll out more of the JVM

00:18:19,290 --> 00:18:24,360
based tasks so the thought of having you

00:18:22,680 --> 00:18:27,600
know these lighter weight and kubernetes

00:18:24,360 --> 00:18:29,640
based pods out there that we can run for

00:18:27,600 --> 00:18:30,720
some of our stateless services really

00:18:29,640 --> 00:18:37,290
looks to give us a pretty awesome

00:18:30,720 --> 00:18:39,210
competitive advantage so that's that's

00:18:37,290 --> 00:18:42,780
who I am you can look me up on Twitter

00:18:39,210 --> 00:18:44,400
or LinkedIn and I'm more than excited to

00:18:42,780 --> 00:18:45,660
ask a little more questions and make

00:18:44,400 --> 00:18:47,820
this more interactive I know there's not

00:18:45,660 --> 00:18:49,200
many of us in here but would love to

00:18:47,820 --> 00:18:51,090
hear if you had any interesting

00:18:49,200 --> 00:18:52,800
challenges along your journey to move

00:18:51,090 --> 00:18:56,130
more to an open source environment I

00:18:52,800 --> 00:18:58,050
just had a question regarding your

00:18:56,130 --> 00:19:01,440
decision for stores like I understand

00:18:58,050 --> 00:19:04,740
it's up most of the time but like were

00:19:01,440 --> 00:19:06,600
you looking for maybe you know you

00:19:04,740 --> 00:19:09,330
always like redundancy so I'm wondering

00:19:06,600 --> 00:19:12,000
like why not consider either like an on

00:19:09,330 --> 00:19:14,460
premise or like another provider or

00:19:12,000 --> 00:19:15,960
something like that no I I think we're

00:19:14,460 --> 00:19:18,540
definitely looking for more providers

00:19:15,960 --> 00:19:19,860
and a more redundancy the interesting

00:19:18,540 --> 00:19:22,290
thing is where we're at in our startup

00:19:19,860 --> 00:19:24,630
growth that you're starting to have that

00:19:22,290 --> 00:19:26,640
determining the layer of durability

00:19:24,630 --> 00:19:29,300
across all our services so we have some

00:19:26,640 --> 00:19:31,830
particular tools that we run like our UI

00:19:29,300 --> 00:19:33,540
our user interface isn't our primary

00:19:31,830 --> 00:19:36,570
interaction with our platform so it has

00:19:33,540 --> 00:19:38,279
a much level lower level service sli

00:19:36,570 --> 00:19:40,080
measurement that we have

00:19:38,279 --> 00:19:42,059
so there are certain particular things

00:19:40,080 --> 00:19:43,950
like Amazon s3 for example is not a

00:19:42,059 --> 00:19:46,200
persistent long-term storage solution

00:19:43,950 --> 00:19:48,179
for us it's more of an intermediate so

00:19:46,200 --> 00:19:50,700
when we run a particular task on an

00:19:48,179 --> 00:19:52,679
Apache airflow it will go and you know

00:19:50,700 --> 00:19:54,929
gather all the data for a time range and

00:19:52,679 --> 00:19:56,340
dump that into the file in s3 and then

00:19:54,929 --> 00:19:58,259
we were able to like reload that into

00:19:56,340 --> 00:19:59,700
spark do our transforms and it really

00:19:58,259 --> 00:20:02,249
becomes like more of an intermediary

00:19:59,700 --> 00:20:04,859
space so knowing that we have the

00:20:02,249 --> 00:20:06,570
ability to operationalize that and bring

00:20:04,859 --> 00:20:08,519
that on Prem if we need to for a

00:20:06,570 --> 00:20:10,919
particular customer like that's when

00:20:08,519 --> 00:20:12,899
Seth would come in to play for us but

00:20:10,919 --> 00:20:14,820
the you know I think that there's a lot

00:20:12,899 --> 00:20:17,669
of pain removed just having the ability

00:20:14,820 --> 00:20:20,460
to just use Amazon s3 at this time for

00:20:17,669 --> 00:20:21,749
us like we have a smaller DevOps team so

00:20:20,460 --> 00:20:23,369
just like I said like adding one more

00:20:21,749 --> 00:20:25,320
component to the stack that we would

00:20:23,369 --> 00:20:27,119
have to master it's always easy to like

00:20:25,320 --> 00:20:28,889
bring them up write all the services but

00:20:27,119 --> 00:20:30,539
when something actually goes wrong and

00:20:28,889 --> 00:20:32,849
you don't have that technical depth to

00:20:30,539 --> 00:20:34,320
go and deep dive it I always kind of get

00:20:32,849 --> 00:20:36,299
a little uncomfortable trying to roll my

00:20:34,320 --> 00:20:37,979
team into supporting something like and

00:20:36,299 --> 00:20:41,029
that we've only you know run in

00:20:37,979 --> 00:20:41,029
development for a short time

00:20:46,149 --> 00:20:50,379
so it sounds like you have become a

00:20:48,219 --> 00:20:52,539
small team you're moving pretty fast and

00:20:50,379 --> 00:20:55,359
he looks like you have a pretty good

00:20:52,539 --> 00:20:57,039
stack here so I'm kind of impressed by

00:20:55,359 --> 00:20:58,539
what you're able to do here if you talk

00:20:57,039 --> 00:20:59,739
a little bit about like the composition

00:20:58,539 --> 00:21:02,199
of the team that worked on this and

00:20:59,739 --> 00:21:03,909
where their skillsets are and you know

00:21:02,199 --> 00:21:07,449
timeline you've had moving from Amazon

00:21:03,909 --> 00:21:09,399
to DC US and so on sure so the size of

00:21:07,449 --> 00:21:12,699
our team now is about ten product

00:21:09,399 --> 00:21:15,039
engineers it's heavily focused more on

00:21:12,699 --> 00:21:17,319
the DevOps side so I came in and

00:21:15,039 --> 00:21:18,699
originally as head of infrastructure and

00:21:17,319 --> 00:21:20,649
came in with an infrastructure team

00:21:18,699 --> 00:21:22,959
we've kind of done a shake-up on our

00:21:20,649 --> 00:21:24,459
engineering organization where we have

00:21:22,959 --> 00:21:25,809
two teams for our two different product

00:21:24,459 --> 00:21:28,329
lines we have a stream team and we have

00:21:25,809 --> 00:21:30,729
a batch team those teams are composed

00:21:28,329 --> 00:21:32,249
with front-end you know front-end

00:21:30,729 --> 00:21:35,319
engineers for each of the teams a

00:21:32,249 --> 00:21:37,419
product manager and then DevOps and

00:21:35,319 --> 00:21:40,149
product development engineers that kind

00:21:37,419 --> 00:21:42,339
of sit in the the two flexible roles we

00:21:40,149 --> 00:21:43,809
share a lot of the information so some

00:21:42,339 --> 00:21:46,029
of our issues are actually cross team

00:21:43,809 --> 00:21:47,589
like so working on things like Apache

00:21:46,029 --> 00:21:49,479
airflow that would be one of those like

00:21:47,589 --> 00:21:50,889
cross-functional team requirements but

00:21:49,479 --> 00:21:52,149
most of the time people snap back into

00:21:50,889 --> 00:21:56,649
their functional role whether it's

00:21:52,149 --> 00:21:59,469
streaming or or badge as far as getting

00:21:56,649 --> 00:22:01,209
up to speed for for most of the team it

00:21:59,469 --> 00:22:02,529
took us about six months to really get

00:22:01,209 --> 00:22:03,879
our heads around it and start running

00:22:02,529 --> 00:22:05,919
more and more of our production data

00:22:03,879 --> 00:22:08,199
through one of the first things I did

00:22:05,919 --> 00:22:11,649
when I started was moved a lot of our

00:22:08,199 --> 00:22:13,809
production systems of record off of DCs

00:22:11,649 --> 00:22:15,969
to actually figure out how all of the

00:22:13,809 --> 00:22:17,619
recs are a volume mounting worked what

00:22:15,969 --> 00:22:19,029
actually happened when you restarted a

00:22:17,619 --> 00:22:21,399
node or a node got scheduled for

00:22:19,029 --> 00:22:22,599
termination in Amazon we really wanted

00:22:21,399 --> 00:22:24,389
to figure that out before we just

00:22:22,599 --> 00:22:26,649
trusted the magic recs Reeboks

00:22:24,389 --> 00:22:27,669
since then we've brought it all back in

00:22:26,649 --> 00:22:29,169
but we really feel like we've

00:22:27,669 --> 00:22:30,429
operationalized and figured out how

00:22:29,169 --> 00:22:32,859
those components work and we have a

00:22:30,429 --> 00:22:34,959
really good set of checklists on how to

00:22:32,859 --> 00:22:36,879
handle you know when a node fails and

00:22:34,959 --> 00:22:38,889
you don't have your X ray volume removed

00:22:36,879 --> 00:22:40,059
and how we migrate those over but I'm

00:22:38,889 --> 00:22:41,559
really glad that we went and did the

00:22:40,059 --> 00:22:44,199
extra diligence to make sure that we

00:22:41,559 --> 00:22:45,909
could figure those out as far as you

00:22:44,199 --> 00:22:47,799
know moving forward for our team there's

00:22:45,909 --> 00:22:49,089
still certainly a high overhead for

00:22:47,799 --> 00:22:52,179
getting people up and running on it

00:22:49,089 --> 00:22:54,039
especially when a lot of our talent and

00:22:52,179 --> 00:22:55,959
Cincinnati comes from more of the docker

00:22:54,039 --> 00:22:57,399
community so looking at things like

00:22:55,959 --> 00:22:58,970
mezzos tend to be a much more mature

00:22:57,399 --> 00:23:00,500
framework that

00:22:58,970 --> 00:23:03,799
a lot of her up-and-comers don't get a

00:23:00,500 --> 00:23:05,419
direct access to but it but that minor

00:23:03,799 --> 00:23:07,940
challenge there if trying to find some

00:23:05,419 --> 00:23:09,740
new talent that you know that is there

00:23:07,940 --> 00:23:11,779
you know pre-established mezzos people

00:23:09,740 --> 00:23:13,159
we've been able to get a lot of our

00:23:11,779 --> 00:23:15,289
engineering team up and running quickly

00:23:13,159 --> 00:23:16,519
so a lot of that is just the ability to

00:23:15,289 --> 00:23:18,409
like bring up an environment and

00:23:16,519 --> 00:23:20,059
terraform and about you know five to ten

00:23:18,409 --> 00:23:21,289
minutes and then be able to tear that

00:23:20,059 --> 00:23:23,740
down at the end of the day if you have

00:23:21,289 --> 00:23:23,740
problems

00:23:31,190 --> 00:23:35,270
cool alright everybody thanks for coming

00:23:33,590 --> 00:23:37,490
out

00:23:35,270 --> 00:23:37,490

YouTube URL: https://www.youtube.com/watch?v=Lwn1UA6K0co


