Title: Using External Persistent Volumes to Reduce Recovery Times and Achieve High Availability on DC OS
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Using External Persistent Volumes to Reduce Recovery Times and Achieve High Availability on DC/OS - Dinesh Israni, Portworx Inc.

Most modern distributed applications like Cassandra and HDFS provide replication of data across nodes and failure zones to be able to deal with failures. But the time taken to recover to a pre-failure level of redundancy in cases of permanent node failures can be large, since a lot of data needs to be copied over to the new node. Also, some of these applications cannot accept new writes on the nodes being bootstrapped, further increasing the recovery time.

About 


Dinesh Israni
Portworx Inc
Senior Software Engineer
San Francisco Bay Area
Twitter Tweet
Dinesh Israni is a Senior Software Engineer at Portworx with over 7 years of experience building Distributed Storage solutions. Prior to Portworx, Dinesh was at Microsoft, through their acquisition of StorSimple, working on their Hybrid Cloud Storage solution. Recently, he has been involved in the DC/OS commons frameworks project.
Captions: 
	00:00:00,030 --> 00:00:04,259
welcome everyone my name is Denise

00:00:01,829 --> 00:00:06,600
Ronnie I'm a software engineer at port

00:00:04,259 --> 00:00:08,700
works and today I'm going to be talking

00:00:06,600 --> 00:00:10,530
about how you can use external

00:00:08,700 --> 00:00:12,780
persistent volumes to achieve high

00:00:10,530 --> 00:00:16,130
availability and reduce recovery times

00:00:12,780 --> 00:00:19,710
when using stateful services on DCOs

00:00:16,130 --> 00:00:22,199
so let's jump right on so here are the

00:00:19,710 --> 00:00:23,730
topics I'm going to cover today we will

00:00:22,199 --> 00:00:26,340
talk about the different types of

00:00:23,730 --> 00:00:27,869
stateful services then I'll go through

00:00:26,340 --> 00:00:30,390
the advantages of using external

00:00:27,869 --> 00:00:32,040
persistent volumes I'll also give you an

00:00:30,390 --> 00:00:34,110
introduction about port works and how

00:00:32,040 --> 00:00:36,140
you can deploy services on DCOs

00:00:34,110 --> 00:00:38,520
to take advantage of port works volumes

00:00:36,140 --> 00:00:40,379
then I'll do a demo showing how you can

00:00:38,520 --> 00:00:41,820
install port works as well as Cassandra

00:00:40,379 --> 00:00:43,829
to use port works volumes and

00:00:41,820 --> 00:00:49,410
demonstrate failover and some other

00:00:43,829 --> 00:00:52,199
useful scenarios so let's talk about

00:00:49,410 --> 00:00:54,270
stateful services there are basically

00:00:52,199 --> 00:00:57,360
two types of stateful services when it

00:00:54,270 --> 00:00:59,070
comes to persisting data the first are

00:00:57,360 --> 00:01:01,739
simple applications which don't do their

00:00:59,070 --> 00:01:03,300
own replication they rely on the

00:01:01,739 --> 00:01:06,990
underneath storage layer to be always

00:01:03,300 --> 00:01:09,950
available WordPress or MySQL are

00:01:06,990 --> 00:01:13,560
examples of such kind of applications

00:01:09,950 --> 00:01:15,659
now the second type of stateful services

00:01:13,560 --> 00:01:18,150
are basically applications that do their

00:01:15,659 --> 00:01:20,189
own application across nodes so in case

00:01:18,150 --> 00:01:21,840
of node dies or fields there is always

00:01:20,189 --> 00:01:23,820
either another copy of the data

00:01:21,840 --> 00:01:27,780
available somewhere or somebody is able

00:01:23,820 --> 00:01:30,299
to replicate data on to that node this

00:01:27,780 --> 00:01:32,189
can be so so if a node that has crashed

00:01:30,299 --> 00:01:34,409
comes back online then the application

00:01:32,189 --> 00:01:38,549
takes care of repairing data on to that

00:01:34,409 --> 00:01:40,259
node this repair can be done either

00:01:38,549 --> 00:01:42,030
manually or it can be automatic

00:01:40,259 --> 00:01:44,880
depending on the application that is

00:01:42,030 --> 00:01:46,979
using the data some of the common

00:01:44,880 --> 00:01:49,320
applications use nowadays like Cassandra

00:01:46,979 --> 00:01:56,369
and as DFS are examples of such kind of

00:01:49,320 --> 00:01:58,110
stateful services so now you might be

00:01:56,369 --> 00:02:01,259
asking why is this replication strategy

00:01:58,110 --> 00:02:02,700
important well it's because bad things

00:02:01,259 --> 00:02:04,229
happen all the time right your notes

00:02:02,700 --> 00:02:06,270
would could crash your network could

00:02:04,229 --> 00:02:08,580
have issues disks on your nodes could

00:02:06,270 --> 00:02:10,200
fail you might also have power outages

00:02:08,580 --> 00:02:10,920
which might which might take down

00:02:10,200 --> 00:02:14,250
complete

00:02:10,920 --> 00:02:15,810
of your notes for applications that do

00:02:14,250 --> 00:02:17,790
their own application there's always

00:02:15,810 --> 00:02:19,680
another copy on one of the other nodes

00:02:17,790 --> 00:02:22,800
in the cluster so they can continue to

00:02:19,680 --> 00:02:24,870
serve iOS and if you have to replace a

00:02:22,800 --> 00:02:27,290
data you can just bootstrap it and

00:02:24,870 --> 00:02:29,580
repair all the necessary data back to it

00:02:27,290 --> 00:02:31,170
this does end up taking a long time

00:02:29,580 --> 00:02:33,840
depending on how much data you had on

00:02:31,170 --> 00:02:35,850
that node that field for non-clustered

00:02:33,840 --> 00:02:38,040
applications though if you had no backup

00:02:35,850 --> 00:02:40,590
and they were using local storage your

00:02:38,040 --> 00:02:42,300
application is doomed you will not be

00:02:40,590 --> 00:02:45,239
able to bring it back up unless you can

00:02:42,300 --> 00:02:49,470
either move your data this to another

00:02:45,239 --> 00:02:51,090
node or restore from a backup and if

00:02:49,470 --> 00:02:52,440
your disses actually fail you will end

00:02:51,090 --> 00:03:01,440
up losing your data and you will not be

00:02:52,440 --> 00:03:03,390
able to bring your service back up so

00:03:01,440 --> 00:03:05,640
how can persistent storage help in all

00:03:03,390 --> 00:03:07,920
of this well for the case of

00:03:05,640 --> 00:03:09,900
applications like MySQL and WordPress

00:03:07,920 --> 00:03:11,040
which don't do their own replication it

00:03:09,900 --> 00:03:13,530
can help to provide high availability

00:03:11,040 --> 00:03:16,590
for your services and make sure that

00:03:13,530 --> 00:03:18,780
your downtime is eliminated and for

00:03:16,590 --> 00:03:20,880
services that do their own application

00:03:18,780 --> 00:03:23,070
it can help reduce recovery times by a

00:03:20,880 --> 00:03:24,840
large amount this is because you don't

00:03:23,070 --> 00:03:26,940
have to bootstrap a new node every time

00:03:24,840 --> 00:03:29,310
a node fails all you would have to do is

00:03:26,940 --> 00:03:31,769
basically start the same task on another

00:03:29,310 --> 00:03:33,930
node and basically the external storage

00:03:31,769 --> 00:03:35,280
would be able to be mounted on the new

00:03:33,930 --> 00:03:37,709
node and you would only have to run a

00:03:35,280 --> 00:03:39,060
repair to restore data for the rights

00:03:37,709 --> 00:03:46,799
that you hadn't missed while the node

00:03:39,060 --> 00:03:50,299
was down so I'll talk a little more

00:03:46,799 --> 00:03:50,299
about this in the next slides

00:03:56,730 --> 00:04:00,519
so before we talk about the scenarios I

00:03:58,959 --> 00:04:01,959
would like to give a brief introduction

00:04:00,519 --> 00:04:03,609
about port works because a lot of the

00:04:01,959 --> 00:04:05,260
scenarios that you talk about are based

00:04:03,609 --> 00:04:08,230
on software-defined storage solutions

00:04:05,260 --> 00:04:10,209
like port works so portraits is the

00:04:08,230 --> 00:04:13,599
first production-ready software-defined

00:04:10,209 --> 00:04:15,430
storage solution from the design from

00:04:13,599 --> 00:04:16,109
the ground up with micro services in

00:04:15,430 --> 00:04:19,090
mind

00:04:16,109 --> 00:04:20,919
so using port works you can provision

00:04:19,090 --> 00:04:23,169
and manage container granular virtual

00:04:20,919 --> 00:04:26,620
devices and a tight integration with

00:04:23,169 --> 00:04:28,720
schedulers like DCOs and communities and

00:04:26,620 --> 00:04:30,940
container orchestrators helps run your

00:04:28,720 --> 00:04:32,530
workload local to where the data is so

00:04:30,940 --> 00:04:34,419
you don't spend a lot of network

00:04:32,530 --> 00:04:39,970
bandwidth basically traversing your data

00:04:34,419 --> 00:04:42,370
between nodes we also have features like

00:04:39,970 --> 00:04:47,590
snapshots and cloud snaps for backup and

00:04:42,370 --> 00:04:49,570
dr you we also have support for for

00:04:47,590 --> 00:04:51,370
encryption so you can actually integrate

00:04:49,570 --> 00:04:54,099
with vault as well as the issuer's wall

00:04:51,370 --> 00:04:58,570
to provide keys to encrypt your volumes

00:04:54,099 --> 00:05:00,820
and all of this is done using automated

00:04:58,570 --> 00:05:02,199
provisioning and control so you can

00:05:00,820 --> 00:05:05,639
basically repeat this entire

00:05:02,199 --> 00:05:07,960
provisioning using restful api s-- and

00:05:05,639 --> 00:05:12,130
port works itself actually runs as a

00:05:07,960 --> 00:05:15,099
container we are also working towards

00:05:12,130 --> 00:05:17,349
integration with CSI so when DCA when DC

00:05:15,099 --> 00:05:22,389
OS has support for CSI we will be able

00:05:17,349 --> 00:05:24,460
to support CSI too so this is how port

00:05:22,389 --> 00:05:27,599
folks the port works architecture looks

00:05:24,460 --> 00:05:29,440
when it's deployed port works basically

00:05:27,599 --> 00:05:30,970
consolidates all your storage

00:05:29,440 --> 00:05:33,610
infrastructure whether it's bare metal

00:05:30,970 --> 00:05:36,130
servers existing mass or sans clouds

00:05:33,610 --> 00:05:38,020
clouds or hybrid clouds into unified

00:05:36,130 --> 00:05:41,560
data layer which is specified by the

00:05:38,020 --> 00:05:43,419
orange part of this diagram so by

00:05:41,560 --> 00:05:45,159
deploying port works you have persistent

00:05:43,419 --> 00:05:47,919
storage ready to be provisioned for all

00:05:45,159 --> 00:05:54,610
your stateful containers and your apps

00:05:47,919 --> 00:05:56,710
driven by scheduler of your choice so

00:05:54,610 --> 00:05:59,110
the block level layer application

00:05:56,710 --> 00:06:01,720
performed by port works is synchronous

00:05:59,110 --> 00:06:04,539
so all the replicas will always have the

00:06:01,720 --> 00:06:06,400
same data this ensures that your

00:06:04,539 --> 00:06:08,289
application is always able to come up on

00:06:06,400 --> 00:06:09,430
another load in case one of the nodes

00:06:08,289 --> 00:06:12,669
where your data is placed

00:06:09,430 --> 00:06:14,560
crashes so once the node comes back up

00:06:12,669 --> 00:06:16,120
port works will automatically repair all

00:06:14,560 --> 00:06:17,800
the data that has missed on that node

00:06:16,120 --> 00:06:20,889
and bring it back in sync with the

00:06:17,800 --> 00:06:23,169
current state of that volume so if the

00:06:20,889 --> 00:06:24,610
node goes away permanently though port

00:06:23,169 --> 00:06:27,460
Works will be able to replicate the

00:06:24,610 --> 00:06:31,659
volume onto another node completely so

00:06:27,460 --> 00:06:33,210
the and this ensures that that port

00:06:31,659 --> 00:06:36,760
works is able to deal with any further

00:06:33,210 --> 00:06:39,360
crashes so that you always have data for

00:06:36,760 --> 00:06:39,360
your volume available

00:06:48,030 --> 00:06:53,490
so now let's take a look at what

00:06:50,960 --> 00:06:55,020
recovery times with local volumes and

00:06:53,490 --> 00:06:57,720
external persistent volumes are

00:06:55,020 --> 00:07:00,270
different with local volumes your data

00:06:57,720 --> 00:07:02,610
is basically pin to a particular node

00:07:00,270 --> 00:07:05,040
so if our node crashes the faster it

00:07:02,610 --> 00:07:06,450
comes back online is is the best for

00:07:05,040 --> 00:07:09,720
your application because you would have

00:07:06,450 --> 00:07:11,880
to repair less data but in reality that

00:07:09,720 --> 00:07:13,290
is very hard to enforce because if you

00:07:11,880 --> 00:07:15,330
have any kind of maintenance on your

00:07:13,290 --> 00:07:16,860
servers it could end up taking some time

00:07:15,330 --> 00:07:18,990
and basically when the node does come

00:07:16,860 --> 00:07:20,730
back up online you would have to repair

00:07:18,990 --> 00:07:24,600
a lot of data to get it back in sync

00:07:20,730 --> 00:07:26,640
with the rest of your cluster also

00:07:24,600 --> 00:07:28,440
service take some time to reboot where

00:07:26,640 --> 00:07:31,500
it is bare metal or just the VM in the

00:07:28,440 --> 00:07:33,270
cloud and in case your note fields

00:07:31,500 --> 00:07:35,070
permanently this recovery time will be

00:07:33,270 --> 00:07:36,780
even longer because what you would have

00:07:35,070 --> 00:07:38,669
to do is you would have to provision a

00:07:36,780 --> 00:07:41,010
new node add it back to your application

00:07:38,669 --> 00:07:43,140
cluster and you would have to bootstrap

00:07:41,010 --> 00:07:45,990
it with all the data that was that is

00:07:43,140 --> 00:07:47,910
that was missed while the while the node

00:07:45,990 --> 00:07:50,780
was down and this involves a two-step

00:07:47,910 --> 00:07:52,710
thing you have two examples like

00:07:50,780 --> 00:07:54,630
Cassandra you would actually have to

00:07:52,710 --> 00:07:56,669
bootstrap the node and then repair all

00:07:54,630 --> 00:07:58,440
the data back to the node to make sure

00:07:56,669 --> 00:08:06,030
that it is in sync with the rest of your

00:07:58,440 --> 00:08:08,040
Cassandra cluster comparing this to

00:08:06,030 --> 00:08:09,860
external storage your volumes will be

00:08:08,040 --> 00:08:12,540
accessible from any node in the cluster

00:08:09,860 --> 00:08:14,430
so if a node goes down you don't need

00:08:12,540 --> 00:08:15,960
for it to be able to come back up for

00:08:14,430 --> 00:08:18,090
the node to join back into your

00:08:15,960 --> 00:08:19,770
application cluster your sherrilyn would

00:08:18,090 --> 00:08:21,720
just need to start the same task on

00:08:19,770 --> 00:08:27,570
another node and use the same volume

00:08:21,720 --> 00:08:30,419
from the node that went down and I think

00:08:27,570 --> 00:08:32,789
in case a node fields permanently this

00:08:30,419 --> 00:08:34,200
is the same to over there because you

00:08:32,789 --> 00:08:35,880
would be able to use the same day or

00:08:34,200 --> 00:08:40,740
same volume to basically bring it back

00:08:35,880 --> 00:08:42,150
up online so in this case for example if

00:08:40,740 --> 00:08:45,060
you were running Cassandra you would not

00:08:42,150 --> 00:08:48,120
you would not have to use the you will

00:08:45,060 --> 00:08:50,339
not have to run the bootstrap step you

00:08:48,120 --> 00:08:51,960
would only have to repair data to that

00:08:50,339 --> 00:08:54,170
node that was missed while the node was

00:08:51,960 --> 00:08:54,170
done

00:08:57,410 --> 00:09:03,529
so here are some of the advantages of

00:08:59,839 --> 00:09:06,769
using port works so you might think why

00:09:03,529 --> 00:09:08,989
can't you just use a sign or an ass so

00:09:06,769 --> 00:09:11,449
the fact is that's using a sign on Nass

00:09:08,989 --> 00:09:13,489
is its kind of an anti-pattern for

00:09:11,449 --> 00:09:16,189
container workloads because they involve

00:09:13,489 --> 00:09:17,959
static out of out-of-band provisioning

00:09:16,189 --> 00:09:20,869
which is not something that fits in very

00:09:17,959 --> 00:09:24,619
well with the current DevOps flow for

00:09:20,869 --> 00:09:27,019
the for new generation apps also using

00:09:24,619 --> 00:09:29,119
San and Nas introduces a lot of latency

00:09:27,019 --> 00:09:31,909
in your apps since you are you actually

00:09:29,119 --> 00:09:35,059
have to basically go over the network to

00:09:31,909 --> 00:09:36,709
get storage which is which is different

00:09:35,059 --> 00:09:41,959
from what you would actually want for

00:09:36,709 --> 00:09:46,039
your high-performing apps and and if you

00:09:41,959 --> 00:09:47,299
actually use any of sign on as appliques

00:09:46,039 --> 00:09:49,699
you would know that there are a lot of

00:09:47,299 --> 00:09:51,499
issues just around automating the entire

00:09:49,699 --> 00:09:54,139
workflow with containers and schedulers

00:09:51,499 --> 00:09:56,419
you will you will you will frequently

00:09:54,139 --> 00:09:59,089
run into issues where you where you are

00:09:56,419 --> 00:10:03,139
not able to use the same volume if if a

00:09:59,089 --> 00:10:05,229
node had failed onto another node and in

00:10:03,139 --> 00:10:07,999
fact if you actually hit network issues

00:10:05,229 --> 00:10:09,499
contacting your Nasus and your entire

00:10:07,999 --> 00:10:11,299
service will be down you will not be

00:10:09,499 --> 00:10:13,249
able to provision anything just because

00:10:11,299 --> 00:10:17,659
that one link to your Nass or sign is

00:10:13,249 --> 00:10:20,539
down so the advantage of port works here

00:10:17,659 --> 00:10:22,369
is that it is it is Bill ground up with

00:10:20,539 --> 00:10:25,309
micro services in mind with four

00:10:22,369 --> 00:10:27,199
containers and our goal has been to

00:10:25,309 --> 00:10:30,529
basically have a tight integration with

00:10:27,199 --> 00:10:33,470
schedulers to to basically avoid the

00:10:30,529 --> 00:10:38,569
pitfalls that you would see while during

00:10:33,470 --> 00:10:40,339
failover scenarios and with forfox you

00:10:38,569 --> 00:10:42,949
can actually have a unified solution for

00:10:40,339 --> 00:10:45,139
hybrid deployments so in case you have

00:10:42,949 --> 00:10:46,609
an on-prem deployment as well as a cloud

00:10:45,139 --> 00:10:50,029
deployment you don't have to have two

00:10:46,609 --> 00:10:51,979
strategies to deploy your apps to deploy

00:10:50,029 --> 00:10:53,929
your apps you would basically be using

00:10:51,979 --> 00:10:56,059
the same port works solution in the

00:10:53,929 --> 00:10:58,699
cloud as well as on-prem so all your

00:10:56,059 --> 00:11:04,249
automation can be can be consolidated to

00:10:58,699 --> 00:11:06,289
use the same automation so you might

00:11:04,249 --> 00:11:09,619
wonder why can't you just use EBS

00:11:06,289 --> 00:11:10,780
directly right so EBS doesn't work for

00:11:09,619 --> 00:11:12,250
hybrid deployments

00:11:10,780 --> 00:11:14,350
it's fine if you want to use it in the

00:11:12,250 --> 00:11:15,400
cloud but then if you have an on-prem

00:11:14,350 --> 00:11:18,940
deployment you would need to have

00:11:15,400 --> 00:11:21,580
different strategies for that also ec2

00:11:18,940 --> 00:11:24,340
instances have a limit of 16 OBS volumes

00:11:21,580 --> 00:11:26,080
per ec2 instance so you will not be able

00:11:24,340 --> 00:11:29,530
to deploy a lot of apps if you are using

00:11:26,080 --> 00:11:31,570
something like EBS and if you have

00:11:29,530 --> 00:11:34,240
actually used EBS you would know that a

00:11:31,570 --> 00:11:36,760
lot of the times EBS volumes get stuck

00:11:34,240 --> 00:11:39,430
in detaching and attaching state so when

00:11:36,760 --> 00:11:41,260
your ec2 instances fail you will you

00:11:39,430 --> 00:11:44,740
will basically not be able to use the

00:11:41,260 --> 00:11:46,660
EBS instance EBS volumes on other

00:11:44,740 --> 00:11:49,060
instances you would basically have to go

00:11:46,660 --> 00:11:51,040
in and manually force it to be attached

00:11:49,060 --> 00:11:55,030
or detached that's usable somewhere else

00:11:51,040 --> 00:11:57,190
and performance is also not very good

00:11:55,030 --> 00:11:59,110
when you're using EBS volumes unless

00:11:57,190 --> 00:12:01,150
you're using provision I ops but then

00:11:59,110 --> 00:12:05,380
you end up paying a large cost for using

00:12:01,150 --> 00:12:08,020
that and if you're using EBS your

00:12:05,380 --> 00:12:12,520
failure is is slow because that that's

00:12:08,020 --> 00:12:13,900
that's how it's it is designed so AWS

00:12:12,520 --> 00:12:16,780
would first have to realize that your

00:12:13,900 --> 00:12:18,760
ec2 instances down then it would need to

00:12:16,780 --> 00:12:21,700
make sure that the EBS instance is not

00:12:18,760 --> 00:12:23,950
being used on the Oh Lord spin up a new

00:12:21,700 --> 00:12:31,720
ec2 instance and then attach the EBS

00:12:23,950 --> 00:12:34,060
volume there so basically using port

00:12:31,720 --> 00:12:35,560
works with stateful services there are

00:12:34,060 --> 00:12:38,800
three ways to use port works with

00:12:35,560 --> 00:12:40,870
stateful services or nadis us you could

00:12:38,800 --> 00:12:44,830
use marathon to basically deploy our

00:12:40,870 --> 00:12:47,230
services to use port works volumes we

00:12:44,830 --> 00:12:49,600
also have a couple of services in the

00:12:47,230 --> 00:12:52,120
DCOs universe based on the DC us Commons

00:12:49,600 --> 00:12:53,950
SDK which has added support for port Vox

00:12:52,120 --> 00:12:56,050
volumes so they are available in the

00:12:53,950 --> 00:12:59,440
universe and you can just deploy them

00:12:56,050 --> 00:13:01,180
through that the changes that we've made

00:12:59,440 --> 00:13:04,360
to the dcs Commons framework is also

00:13:01,180 --> 00:13:05,920
available on github so you can actually

00:13:04,360 --> 00:13:08,290
write your own services to take

00:13:05,920 --> 00:13:13,900
advantage of this framework in case you

00:13:08,290 --> 00:13:15,280
want to use port works volumes so this

00:13:13,900 --> 00:13:18,940
is an example of how you can use

00:13:15,280 --> 00:13:21,940
marathon to deploy deploy apps to use

00:13:18,940 --> 00:13:24,950
port Vox volumes underneath so if you're

00:13:21,940 --> 00:13:27,140
familiar with deploying apps through

00:13:24,950 --> 00:13:29,750
you basically have to specify a JSON

00:13:27,140 --> 00:13:32,360
format for the other apps to specify

00:13:29,750 --> 00:13:33,740
what container you want to use and the

00:13:32,360 --> 00:13:36,800
volumes you are gonna use as well as

00:13:33,740 --> 00:13:38,120
your port specification so to use port

00:13:36,800 --> 00:13:42,650
works all you would have to do is

00:13:38,120 --> 00:13:44,780
specify pxd as the volume driver and you

00:13:42,650 --> 00:13:46,670
can and you would have to specify the

00:13:44,780 --> 00:13:48,800
options for the for the volume that you

00:13:46,670 --> 00:13:50,750
want to use so there would be no out of

00:13:48,800 --> 00:13:52,700
bank provisioning as soon as you start

00:13:50,750 --> 00:13:54,800
the app and you specify the volume

00:13:52,700 --> 00:13:56,720
options that you want to ask started

00:13:54,800 --> 00:14:00,620
with you would be able to still use the

00:13:56,720 --> 00:14:03,740
app and this the same method can be used

00:14:00,620 --> 00:14:10,430
to deploy apps using docker or UCR on

00:14:03,740 --> 00:14:12,920
DCOs so the second way is to deploy apps

00:14:10,430 --> 00:14:16,100
through the universe so we have a couple

00:14:12,920 --> 00:14:18,620
of apps published in the universe based

00:14:16,100 --> 00:14:21,260
on the DCOs commons framework to use

00:14:18,620 --> 00:14:24,080
port works volumes those are the

00:14:21,260 --> 00:14:26,990
cassandra' hadoop elasticsearch as well

00:14:24,080 --> 00:14:29,960
as Kafka so apart from adding support

00:14:26,990 --> 00:14:32,900
for port Vox volumes the changes that

00:14:29,960 --> 00:14:35,240
we've made also support failure of tasks

00:14:32,900 --> 00:14:38,150
when nodes crashed so this ensures that

00:14:35,240 --> 00:14:41,420
your services have a high of a lower

00:14:38,150 --> 00:14:43,130
higher higher uptime and it also reduces

00:14:41,420 --> 00:14:44,690
recovery times because you don't have to

00:14:43,130 --> 00:14:48,350
wait for your note to come back up as

00:14:44,690 --> 00:14:50,600
soon as DCOs recognizes that a node is

00:14:48,350 --> 00:14:52,700
down the the framework would know that

00:14:50,600 --> 00:14:54,140
would would get a notification and it

00:14:52,700 --> 00:14:58,330
would be able to launch the same task

00:14:54,140 --> 00:14:58,330
onto another node using the same volume

00:14:59,140 --> 00:15:03,260
so we've also actually updated the

00:15:01,520 --> 00:15:06,650
framework to be able to co-locate your

00:15:03,260 --> 00:15:08,600
data with your nodes so because because

00:15:06,650 --> 00:15:10,400
we have control over the framework what

00:15:08,600 --> 00:15:11,750
we can do is we can actually query port

00:15:10,400 --> 00:15:14,260
works and figure out where the data is

00:15:11,750 --> 00:15:17,450
and launch your tasks local to that node

00:15:14,260 --> 00:15:19,670
so this reduces latency as well as

00:15:17,450 --> 00:15:26,000
reducing network congestion on your

00:15:19,670 --> 00:15:28,430
network so this is an example of how you

00:15:26,000 --> 00:15:31,460
can use the updated Decius commons

00:15:28,430 --> 00:15:33,650
framework to use port works volumes so

00:15:31,460 --> 00:15:36,350
Decius commons makes it very simple to

00:15:33,650 --> 00:15:37,050
start stateful services but it only has

00:15:36,350 --> 00:15:40,410
support for

00:15:37,050 --> 00:15:41,430
rude and bound volumes which is not very

00:15:40,410 --> 00:15:43,320
efficient when you're dealing with

00:15:41,430 --> 00:15:46,140
failures which will happen in large

00:15:43,320 --> 00:15:47,580
clusters so what we've done is we've

00:15:46,140 --> 00:15:50,399
basically added support for products

00:15:47,580 --> 00:15:53,250
volumes in the SDK and all you need to

00:15:50,399 --> 00:15:55,200
do is if you if you're writing as a an

00:15:53,250 --> 00:15:58,500
application for Decius comments all you

00:15:55,200 --> 00:16:01,430
will need to do is specify the volume

00:15:58,500 --> 00:16:03,690
name and volume options in the amethyst

00:16:01,430 --> 00:16:06,200
specification for Decius comments and

00:16:03,690 --> 00:16:08,519
you will automatically end up starting

00:16:06,200 --> 00:16:11,940
using port works volumes when your

00:16:08,519 --> 00:16:13,500
application starts up so once you have

00:16:11,940 --> 00:16:15,450
your application specified all you will

00:16:13,500 --> 00:16:16,649
need to do is build it and deploy it and

00:16:15,450 --> 00:16:20,040
you would be able to use port works

00:16:16,649 --> 00:16:22,079
volumes and again same as launching

00:16:20,040 --> 00:16:23,730
tasks to marathon all your volumes will

00:16:22,079 --> 00:16:25,829
be automatically provisioned so you

00:16:23,730 --> 00:16:27,630
don't need to wait for you for a storage

00:16:25,829 --> 00:16:31,350
admin to basically come in and provision

00:16:27,630 --> 00:16:33,329
volumes for you so the changes that we

00:16:31,350 --> 00:16:36,329
made to the dizziest Commons is

00:16:33,329 --> 00:16:44,149
available on github at Garib comm slash

00:16:36,329 --> 00:16:46,740
port works /d shears comments okay so

00:16:44,149 --> 00:16:48,810
I'm going to show you a demo of how easy

00:16:46,740 --> 00:16:50,420
it is to install port works and on top

00:16:48,810 --> 00:16:52,860
of that how easy it is to install

00:16:50,420 --> 00:16:54,750
Cassandra services to use the sport

00:16:52,860 --> 00:16:56,550
works volumes and then I'm also going to

00:16:54,750 --> 00:17:01,610
demonstrate what happens when a node

00:16:56,550 --> 00:17:01,610
fails and and some other scenarios do

00:17:16,390 --> 00:17:19,049
okay

00:17:19,240 --> 00:17:23,260
so like I mentioned all these services

00:17:21,490 --> 00:17:25,539
actually available in the DCOs universe

00:17:23,260 --> 00:17:29,950
so first I'm gonna go ahead and install

00:17:25,539 --> 00:17:32,620
the port works service I'm just going to

00:17:29,950 --> 00:17:34,299
change the the image name to be to use

00:17:32,620 --> 00:17:37,809
the enterprise image and I have a finer

00:17:34,299 --> 00:17:39,250
DCOs cluster so I'm just going to change

00:17:37,809 --> 00:17:48,750
that to install it on all the five

00:17:39,250 --> 00:17:48,750
private agents I don't think so actually

00:17:53,370 --> 00:17:58,659
so I have actually sped up the video a

00:17:56,380 --> 00:18:00,419
little because it takes like eight

00:17:58,659 --> 00:18:02,529
minutes to start the entire thing but

00:18:00,419 --> 00:18:05,020
I'm just gonna pause it here so what

00:18:02,529 --> 00:18:06,730
will what will happen is will we launch

00:18:05,020 --> 00:18:08,260
the entire stack so first we will go

00:18:06,730 --> 00:18:10,120
ahead and launch a three node HDD

00:18:08,260 --> 00:18:13,299
cluster then we'll start a naturally

00:18:10,120 --> 00:18:14,860
proxy then we'll start an influx DV node

00:18:13,299 --> 00:18:18,190
which is basically used to store the

00:18:14,860 --> 00:18:20,710
stats for our UI then I'll go ahead and

00:18:18,190 --> 00:18:22,929
start a lighthouse task which is

00:18:20,710 --> 00:18:24,549
basically the port works UI and after

00:18:22,929 --> 00:18:26,350
all that is done it will basically go

00:18:24,549 --> 00:18:30,850
ahead and install port works on all the

00:18:26,350 --> 00:18:33,909
five private agents so as you can see

00:18:30,850 --> 00:18:35,770
your HDD the athili three nodes at CVS

00:18:33,909 --> 00:18:37,809
started up in slugsy we started up and

00:18:35,770 --> 00:18:40,899
then the lighthouse is basically staging

00:18:37,809 --> 00:18:42,760
at this point so since lighthouse is

00:18:40,899 --> 00:18:45,070
installed on a private agent it's not

00:18:42,760 --> 00:18:47,080
accessible from outside the dcs cluster

00:18:45,070 --> 00:18:49,270
so what I'm going to go ahead and do now

00:18:47,080 --> 00:18:51,730
is it will basically go ahead and start

00:18:49,270 --> 00:18:54,940
our epoxy service which is which is kind

00:18:51,730 --> 00:18:59,169
of a proxy from a public agent onto the

00:18:54,940 --> 00:19:00,399
private tasks that's running so all of

00:18:59,169 --> 00:19:02,860
this information is actually available

00:19:00,399 --> 00:19:04,029
on the docks website and is also linked

00:19:02,860 --> 00:19:07,990
when you're starting the port work

00:19:04,029 --> 00:19:10,120
service so this is just starting a

00:19:07,990 --> 00:19:13,330
marathon app to start the epoxy service

00:19:10,120 --> 00:19:16,840
and once that comes up you can basically

00:19:13,330 --> 00:19:18,309
go to your private agent port nine nine

00:19:16,840 --> 00:19:22,380
nine nine and you should be able to

00:19:18,309 --> 00:19:22,380
access the lighthouse UI

00:19:29,690 --> 00:19:33,590
so once this comes up what we'll see is

00:19:32,030 --> 00:19:36,110
basically port works install is

00:19:33,590 --> 00:19:39,080
happening on the on the private agents

00:19:36,110 --> 00:19:43,190
so we'll start seeing these nodes come

00:19:39,080 --> 00:19:44,660
up on the dashboard so as as soon as

00:19:43,190 --> 00:19:46,040
they come up they basically contact

00:19:44,660 --> 00:19:47,720
lighthouse and tell them they're up and

00:19:46,040 --> 00:19:50,900
as you can see five nodes are installed

00:19:47,720 --> 00:19:53,120
and it's a little bloody in in the video

00:19:50,900 --> 00:19:55,250
but all this happened in basically eight

00:19:53,120 --> 00:19:57,530
minutes so we basically stood up an HDD

00:19:55,250 --> 00:19:59,540
cluster the UI as well as port works and

00:19:57,530 --> 00:20:06,140
it's all ready to be provisioned in

00:19:59,540 --> 00:20:09,800
under fiber in under eight minutes so

00:20:06,140 --> 00:20:12,260
now that this is done so you can

00:20:09,800 --> 00:20:19,340
actually also look at the status of your

00:20:12,260 --> 00:20:21,320
port works cluster by by running by

00:20:19,340 --> 00:20:28,340
divided by running the port works

00:20:21,320 --> 00:20:30,140
control CLI so all you would have to do

00:20:28,340 --> 00:20:32,570
is log into one of the private agents

00:20:30,140 --> 00:20:35,330
and pick secret status and I guess the

00:20:32,570 --> 00:20:36,530
color is not very visible but as you can

00:20:35,330 --> 00:20:38,120
see in the green there are like five

00:20:36,530 --> 00:20:40,900
there are five nodes that are now online

00:20:38,120 --> 00:20:43,100
and ready to be provisioned four volumes

00:20:40,900 --> 00:20:45,260
so now that code box is set up on all

00:20:43,100 --> 00:20:47,570
the private agents I'm going to go ahead

00:20:45,260 --> 00:20:49,820
and install Cassandra which is available

00:20:47,570 --> 00:20:51,590
in the DAO and the DC as you know as -

00:20:49,820 --> 00:20:54,080
so this Cassandra service has basically

00:20:51,590 --> 00:20:57,790
been updated to automatically use port

00:20:54,080 --> 00:20:57,790
works volumes when it gets provisioned

00:21:00,880 --> 00:21:08,000
so the defaults are said to use the port

00:21:05,000 --> 00:21:10,130
works it's it's been said to use port

00:21:08,000 --> 00:21:12,320
works volumes but and we can also pass

00:21:10,130 --> 00:21:14,060
in our additional options for the port

00:21:12,320 --> 00:21:17,030
Vox volumes so what I'm going to do here

00:21:14,060 --> 00:21:18,530
is I'm going to set up the the port

00:21:17,030 --> 00:21:20,870
works volumes to use a replica

00:21:18,530 --> 00:21:23,780
replication factor of three so in case a

00:21:20,870 --> 00:21:25,220
node that node where the volume is is

00:21:23,780 --> 00:21:27,740
provision goes down we will still be

00:21:25,220 --> 00:21:30,340
able to basically use the same volume on

00:21:27,740 --> 00:21:30,340
another node

00:21:34,220 --> 00:21:38,970
so basically here is the port works

00:21:37,500 --> 00:21:41,070
volume name and here is the port works

00:21:38,970 --> 00:21:43,350
volume options and I'm just going to

00:21:41,070 --> 00:21:45,059
specify a level equal to three and you

00:21:43,350 --> 00:21:46,470
can basically specify multiple options

00:21:45,059 --> 00:21:48,990
over here which will be which are comma

00:21:46,470 --> 00:21:51,210
separated so if you wanted this to be an

00:21:48,990 --> 00:21:53,550
encrypted encrypted volume you would

00:21:51,210 --> 00:21:55,680
just say secure equal to two you would

00:21:53,550 --> 00:21:59,070
need to make sure that both hooks is set

00:21:55,680 --> 00:22:07,190
up to basically pulling keys from a

00:21:59,070 --> 00:22:09,720
secure vault so once once I said this

00:22:07,190 --> 00:22:11,070
I'm just you can also specify the size

00:22:09,720 --> 00:22:13,650
of the volume I've currently set it to

00:22:11,070 --> 00:22:23,130
10 GB and I'm just going to click review

00:22:13,650 --> 00:22:25,320
and install and install so just looking

00:22:23,130 --> 00:22:27,300
at the volume list I'm confirming there

00:22:25,320 --> 00:22:29,220
are no volumes available and I'm just

00:22:27,300 --> 00:22:30,870
going to keep a lookout for the volumes

00:22:29,220 --> 00:22:32,910
that get automatically provisioned and

00:22:30,870 --> 00:22:35,490
I'm going to go back to DCOs

00:22:32,910 --> 00:22:36,630
and from the lighthouse also you can see

00:22:35,490 --> 00:22:42,920
that there are no volumes currently

00:22:36,630 --> 00:22:45,500
provision and we'll see as soon as the

00:22:42,920 --> 00:22:47,670
Cassandra know it's come up you know the

00:22:45,500 --> 00:22:49,710
volumes get automatically provisioned

00:22:47,670 --> 00:22:51,660
and they're visible both through the CLI

00:22:49,710 --> 00:22:53,040
as well as lighthouse I sped up the

00:22:51,660 --> 00:22:55,890
video a little because it takes time for

00:22:53,040 --> 00:22:58,800
the notes to come up but at this point

00:22:55,890 --> 00:23:01,830
the three node Cassandra cluster is up

00:22:58,800 --> 00:23:04,650
and as you can see three volumes were

00:23:01,830 --> 00:23:10,650
created all with a replication factor of

00:23:04,650 --> 00:23:14,880
three and a size of 10 GB so now that

00:23:10,650 --> 00:23:18,150
this is up what we are going to do is

00:23:14,880 --> 00:23:21,270
we're gonna basically go ahead and power

00:23:18,150 --> 00:23:25,410
of one of the nodes now what would

00:23:21,270 --> 00:23:27,420
happen in this with just the the regular

00:23:25,410 --> 00:23:29,130
Cassandra framework was if you power of

00:23:27,420 --> 00:23:30,929
this node the framework would not bring

00:23:29,130 --> 00:23:32,970
up the same task on another node because

00:23:30,929 --> 00:23:35,790
all your data would be local to the node

00:23:32,970 --> 00:23:37,920
that was powered off but what we'll see

00:23:35,790 --> 00:23:40,370
here is since these volumes are backed

00:23:37,920 --> 00:23:43,370
up by port works and there is

00:23:40,370 --> 00:23:45,050
and the replication factor is set to 3d

00:23:43,370 --> 00:23:47,420
cos will actually be able to bring this

00:23:45,050 --> 00:23:52,690
node this same Cassandra node up on

00:23:47,420 --> 00:23:56,450
another physical node so I am going to

00:23:52,690 --> 00:24:08,090
basically as such into one of the nodes

00:23:56,450 --> 00:24:11,330
and I'm going to just power it off first

00:24:08,090 --> 00:24:12,740
I'm confirming that the volume was mount

00:24:11,330 --> 00:24:21,380
area and the task was running here and

00:24:12,740 --> 00:24:23,210
then I power the node off and going back

00:24:21,380 --> 00:24:25,340
to the UI you will see that this task

00:24:23,210 --> 00:24:27,500
will disappear so the D shows already

00:24:25,340 --> 00:24:31,700
sees that the that the node is offline

00:24:27,500 --> 00:24:33,380
and in some time you'll see that the the

00:24:31,700 --> 00:24:37,610
node disappears from the list and

00:24:33,380 --> 00:24:39,560
another one gets spun up it's not very

00:24:37,610 --> 00:24:41,690
clear but the IP is actually one of the

00:24:39,560 --> 00:24:45,170
other nodes where the replica was for

00:24:41,690 --> 00:24:46,940
that data for that volume so it'll

00:24:45,170 --> 00:24:50,830
basically be able to join as the same

00:24:46,940 --> 00:24:50,830
identity as node that went down

00:24:59,820 --> 00:25:04,289
yeah and we can see that the poor folks

00:25:01,889 --> 00:25:06,389
also sees the node that was powered off

00:25:04,289 --> 00:25:08,220
is off so it it actually knows that this

00:25:06,389 --> 00:25:11,789
volume is safe to be attached on any

00:25:08,220 --> 00:25:14,100
other node scenarios like this you were

00:25:11,789 --> 00:25:17,159
if you were using EBS or a SAN or an ass

00:25:14,100 --> 00:25:19,559
you would run into issues where where

00:25:17,159 --> 00:25:21,179
the where the software solution is not

00:25:19,559 --> 00:25:23,429
the storage solution is not able to

00:25:21,179 --> 00:25:26,009
determine whether whether a volume is

00:25:23,429 --> 00:25:27,629
still being used by another being used

00:25:26,009 --> 00:25:29,789
by another node or not and whether it's

00:25:27,629 --> 00:25:36,029
actually safe to attach it to another

00:25:29,789 --> 00:25:37,889
node when one more interesting scenario

00:25:36,029 --> 00:25:40,259
I'm going to show you is so as I pointed

00:25:37,889 --> 00:25:43,759
out I had actually started these these

00:25:40,259 --> 00:25:46,229
Cassandra nodes with 10 GB of storage

00:25:43,759 --> 00:25:47,909
now if you are running any any

00:25:46,229 --> 00:25:49,679
application in production 10 GB of

00:25:47,909 --> 00:25:52,139
storage for each Cassandra node is is

00:25:49,679 --> 00:25:55,649
very low right and if you were actually

00:25:52,139 --> 00:25:57,899
using this with route all local route of

00:25:55,649 --> 00:25:59,789
mount volumes you wouldn't have any way

00:25:57,899 --> 00:26:02,039
of actually increasing the size of these

00:25:59,789 --> 00:26:04,289
volumes you will have to destroy the

00:26:02,039 --> 00:26:06,749
service start a new service and then

00:26:04,289 --> 00:26:09,450
copy all the data on start a new service

00:26:06,749 --> 00:26:11,970
with increased files volume size and

00:26:09,450 --> 00:26:13,830
copy all the data onto that what I'm

00:26:11,970 --> 00:26:16,379
going to show you here is you can

00:26:13,830 --> 00:26:18,539
basically resize the volume online

00:26:16,379 --> 00:26:25,769
without actually taking your service

00:26:18,539 --> 00:26:27,359
offline so if it's very clear but yeah

00:26:25,769 --> 00:26:36,929
this shows that the volume was actually

00:26:27,359 --> 00:26:39,239
mounted on that node so and resizing the

00:26:36,929 --> 00:26:41,070
volume as simple as running the CLI all

00:26:39,239 --> 00:26:43,799
you would need to do is run pick C curl

00:26:41,070 --> 00:26:45,809
a volume update specify the name of the

00:26:43,799 --> 00:26:47,429
volume and specify the new size of the

00:26:45,809 --> 00:26:51,139
volume and it would automatically go

00:26:47,429 --> 00:26:51,139
ahead and update the size of the volume

00:26:53,029 --> 00:26:57,599
so basically over here I'm increasing

00:26:55,259 --> 00:27:00,659
the size of the volume from 10gb to

00:26:57,599 --> 00:27:03,619
100gb and as you can see it it went

00:27:00,659 --> 00:27:03,619
ahead successfully

00:27:10,670 --> 00:27:15,650
and if I do a volume list again you can

00:27:13,250 --> 00:27:17,450
see it's not very clear but over here

00:27:15,650 --> 00:27:24,860
you you can see that the size has been

00:27:17,450 --> 00:27:26,210
increased to 100 GB and the application

00:27:24,860 --> 00:27:28,040
will actually see the size increase

00:27:26,210 --> 00:27:29,930
right away so you don't have enough to

00:27:28,040 --> 00:27:32,830
restart your application to be able to

00:27:29,930 --> 00:27:32,830
see the increased size

00:27:41,770 --> 00:27:48,400
okay I guess that's it for for the

00:27:46,750 --> 00:27:49,380
presentation does anybody have any

00:27:48,400 --> 00:27:58,360
questions

00:27:49,380 --> 00:28:00,970
sure yes so you can start start a

00:27:58,360 --> 00:28:03,070
service through marathon to use port

00:28:00,970 --> 00:28:04,960
works volumes so for the example that I

00:28:03,070 --> 00:28:07,030
gave for MySQL the only thing you would

00:28:04,960 --> 00:28:09,940
need to do is use a Postgres container

00:28:07,030 --> 00:28:11,530
specify the same parameters and it would

00:28:09,940 --> 00:28:14,350
basically be able to start a post class

00:28:11,530 --> 00:28:15,940
service so any any container that any

00:28:14,350 --> 00:28:18,070
service that you can start using

00:28:15,940 --> 00:28:20,650
marathon you can use port works volumes

00:28:18,070 --> 00:28:23,970
with that for persistence and you can

00:28:20,650 --> 00:28:23,970
use docker as well as UCR

00:28:36,670 --> 00:28:41,620
so so all the replication is synchronous

00:28:42,460 --> 00:28:47,360
no so so there'll be a total of three

00:28:45,650 --> 00:28:49,670
replicas one will be local and two will

00:28:47,360 --> 00:28:52,010
be remote so we do wait for all the I

00:28:49,670 --> 00:28:53,780
ought to be persisted before it's

00:28:52,010 --> 00:28:56,840
acknowledged so there is a slight

00:28:53,780 --> 00:28:58,550
overhead but it comes I mean it provides

00:28:56,840 --> 00:29:03,160
an advantage of providing high

00:28:58,550 --> 00:29:03,160
availability in case the don't goes down

00:29:05,380 --> 00:29:10,280
yes yes

00:29:07,670 --> 00:29:12,200
but but we see in most customer

00:29:10,280 --> 00:29:15,080
deployments that they have I mean most

00:29:12,200 --> 00:29:16,880
of the customers have high high

00:29:15,080 --> 00:29:19,750
throughput and low latency backbone

00:29:16,880 --> 00:29:19,750
connecting the nodes

00:29:30,309 --> 00:29:35,289
so the the the changes actually are very

00:29:33,099 --> 00:29:37,269
minimum so it's it's all open source

00:29:35,289 --> 00:29:39,159
like a parent or you can go ahead and

00:29:37,269 --> 00:29:42,099
look at the changes to but the changes

00:29:39,159 --> 00:29:44,619
are very minimum so the changes involve

00:29:42,099 --> 00:29:47,859
adding support for daca volume drivers

00:29:44,619 --> 00:29:49,690
where we support port works and the

00:29:47,859 --> 00:29:53,259
second major change is to basically

00:29:49,690 --> 00:29:55,840
support failover of tasks so the the

00:29:53,259 --> 00:29:58,419
upstream DCOs commons package even for

00:29:55,840 --> 00:30:00,279
stateless apps your tasks cannot fail

00:29:58,419 --> 00:30:02,139
over between nodes even if the node goes

00:30:00,279 --> 00:30:04,719
down so that is something that was

00:30:02,139 --> 00:30:06,489
basically added as support so there are

00:30:04,719 --> 00:30:08,799
these two major changes but it does not

00:30:06,489 --> 00:30:10,690
diverge a lot from Decius comments and

00:30:08,799 --> 00:30:13,179
we basically try to keep up-to-date with

00:30:10,690 --> 00:30:15,940
the DC u.s. commons releases so we we

00:30:13,179 --> 00:30:17,469
will basically try to push changes into

00:30:15,940 --> 00:30:22,989
the universe as soon as these shows

00:30:17,469 --> 00:30:24,489
comments make changes to yeah hi so I

00:30:22,989 --> 00:30:28,779
have two questions

00:30:24,489 --> 00:30:30,999
can you talk to running port works in an

00:30:28,779 --> 00:30:34,269
environment where only some of the port

00:30:30,999 --> 00:30:37,359
works services have volumes that are

00:30:34,269 --> 00:30:40,839
shared to the overall port works

00:30:37,359 --> 00:30:44,529
environment and making other nodes in

00:30:40,839 --> 00:30:46,570
your cluster available to that storage

00:30:44,529 --> 00:30:50,259
for I guess headless is what you guys

00:30:46,570 --> 00:30:53,379
call that and then also talk to your

00:30:50,259 --> 00:30:56,440
roadmap about removing at CD from your

00:30:53,379 --> 00:30:59,909
dependency chain and and one thing that

00:30:56,440 --> 00:31:03,940
we found was that running at CD within

00:30:59,909 --> 00:31:06,129
me so sphere is not fast enough and it

00:31:03,940 --> 00:31:08,830
fails often so we extracted RS CD

00:31:06,129 --> 00:31:09,789
outside of the cluster all together so

00:31:08,830 --> 00:31:12,549
you could talk to those two things

00:31:09,789 --> 00:31:16,179
that'd be great yeah so two things the

00:31:12,549 --> 00:31:19,119
first thing is basically if I got your

00:31:16,179 --> 00:31:20,589
question was how do you specify port

00:31:19,119 --> 00:31:23,879
works to be installed only a set of

00:31:20,589 --> 00:31:23,879
nodes and not on the other set

00:31:45,000 --> 00:31:48,840
okay so port works can be installed on

00:31:47,159 --> 00:31:50,909
nodes which have stories as well as

00:31:48,840 --> 00:31:52,679
which don't have stories so in case you

00:31:50,909 --> 00:31:54,510
install port works on nodes that don't

00:31:52,679 --> 00:31:57,179
have storage you will still be able to

00:31:54,510 --> 00:31:58,919
mount the volumes that are that are

00:31:57,179 --> 00:32:03,210
basically placed on the other port works

00:31:58,919 --> 00:32:07,620
nodes so that all works seamlessly if

00:32:03,210 --> 00:32:09,600
you want to specify DCOs to run your

00:32:07,620 --> 00:32:11,010
jobs only on the nodes which don't have

00:32:09,600 --> 00:32:13,350
stories because they have a lot more

00:32:11,010 --> 00:32:15,630
compute power you can always specify

00:32:13,350 --> 00:32:17,549
constraints so you can place can you can

00:32:15,630 --> 00:32:19,110
place labels on your headless nodes and

00:32:17,549 --> 00:32:21,150
when you start up these services like

00:32:19,110 --> 00:32:23,760
Cassandra or Hadoop you can basically

00:32:21,150 --> 00:32:25,860
say that you want to start these tasks

00:32:23,760 --> 00:32:28,130
only on the headless nodes so what will

00:32:25,860 --> 00:32:31,380
it end up happening is they'll remotely

00:32:28,130 --> 00:32:33,059
you will end up mounting attaching and

00:32:31,380 --> 00:32:34,770
mounting these block devices on your

00:32:33,059 --> 00:32:43,919
headless nodes but your data will lie on

00:32:34,770 --> 00:32:45,840
your other storage nodes yes they all

00:32:43,919 --> 00:32:49,830
have the same licensing yes

00:32:45,840 --> 00:32:52,260
and about your question for at CD so yes

00:32:49,830 --> 00:32:54,750
we have also realized that running at CD

00:32:52,260 --> 00:32:57,120
within this entire framework causes

00:32:54,750 --> 00:33:00,000
issues because what happens is in case

00:32:57,120 --> 00:33:02,220
the you had issues with just DC us in

00:33:00,000 --> 00:33:04,950
general what happens is you lose you

00:33:02,220 --> 00:33:09,150
lose state for those root and mount

00:33:04,950 --> 00:33:11,909
volumes for that CD so it is for for a

00:33:09,150 --> 00:33:13,919
test scenario this is you using the

00:33:11,909 --> 00:33:16,919
entire stack to start at CD as well as

00:33:13,919 --> 00:33:18,570
port works with is the same thing is is

00:33:16,919 --> 00:33:21,179
is a good way to basically make sure

00:33:18,570 --> 00:33:24,380
that you are it fits you needs and stuff

00:33:21,179 --> 00:33:26,909
but for production it is advisable to at

00:33:24,380 --> 00:33:28,740
provision HDD separately or on a

00:33:26,909 --> 00:33:30,480
different cluster completely so that in

00:33:28,740 --> 00:33:31,200
case your DCOs cluster goes down

00:33:30,480 --> 00:33:33,210
completely

00:33:31,200 --> 00:33:35,850
you're still able to bring port works up

00:33:33,210 --> 00:33:37,380
because your HDD is lying externally so

00:33:35,850 --> 00:33:40,409
port works doesn't have any dependency

00:33:37,380 --> 00:33:43,440
on DCOs all it really depends on is that

00:33:40,409 --> 00:33:44,880
your data discs are not touched so even

00:33:43,440 --> 00:33:46,500
if DC OS goes down and you have to

00:33:44,880 --> 00:33:48,990
reinstall this u.s. you will still be

00:33:46,500 --> 00:33:54,840
able to start up port works and she was

00:33:48,990 --> 00:33:57,080
the same volumes yes and in the next

00:33:54,840 --> 00:33:59,840
release we are basically planning to

00:33:57,080 --> 00:34:02,360
the kbdb into a into port works itself

00:33:59,840 --> 00:34:10,250
so you not have a need to even provision

00:34:02,360 --> 00:34:12,620
an external HDD so so we will not be

00:34:10,250 --> 00:34:14,900
using the route the the route or the

00:34:12,620 --> 00:34:16,760
budist will basically be using we will

00:34:14,900 --> 00:34:18,860
create a partition on one of the data

00:34:16,760 --> 00:34:21,770
discs and store a CD information there

00:34:18,860 --> 00:34:23,840
too so again in that case - if the issue

00:34:21,770 --> 00:34:26,030
is goes away if dcs basic you have to

00:34:23,840 --> 00:34:27,740
install d cos you'll still be able to

00:34:26,030 --> 00:34:29,510
bring up port works with exactly the

00:34:27,740 --> 00:34:31,190
same state and all you would need to do

00:34:29,510 --> 00:34:33,790
is basically redeploy your apps and you

00:34:31,190 --> 00:34:41,120
would you would be back up in production

00:34:33,790 --> 00:34:42,350
yep so from some of the answers you've

00:34:41,120 --> 00:34:44,150
given to the questions here it sounds

00:34:42,350 --> 00:34:46,010
like that there's always the potential

00:34:44,150 --> 00:34:47,990
that you could be crossing network

00:34:46,010 --> 00:34:49,520
boundaries with addressing storage and

00:34:47,990 --> 00:34:51,440
obviously that's incurred in synchronous

00:34:49,520 --> 00:34:54,830
there's some right overhead is there a

00:34:51,440 --> 00:34:56,720
capability to get additional read

00:34:54,830 --> 00:34:59,120
performance by having more replicas does

00:34:56,720 --> 00:35:01,070
it have the intelligence to distribute

00:34:59,120 --> 00:35:03,200
reads among the replicas or is always

00:35:01,070 --> 00:35:05,300
focused only on the local copy even if

00:35:03,200 --> 00:35:07,100
it's sale across the network you're

00:35:05,300 --> 00:35:10,340
running headless compute against storage

00:35:07,100 --> 00:35:12,770
nodes so I read optimize in the sense

00:35:10,340 --> 00:35:14,990
that we would be basically send out read

00:35:12,770 --> 00:35:17,330
we do read from all the replicas and

00:35:14,990 --> 00:35:19,790
basically we try to send out more reads

00:35:17,330 --> 00:35:21,260
to nodes that are replying faster so in

00:35:19,790 --> 00:35:24,860
that case we are able to basically

00:35:21,260 --> 00:35:29,360
utilize a reads utilize all the replicas

00:35:24,860 --> 00:35:32,360
to perform reads and and this is all

00:35:29,360 --> 00:35:33,980
consistent so this is all consistent so

00:35:32,360 --> 00:35:37,460
each can come from any data and they

00:35:33,980 --> 00:35:39,590
will always be see second question since

00:35:37,460 --> 00:35:41,180
since this causes storage to cross

00:35:39,590 --> 00:35:43,580
network boundaries as well do you do

00:35:41,180 --> 00:35:45,350
like storage pooling across network

00:35:43,580 --> 00:35:46,880
boundaries so let's say a single storage

00:35:45,350 --> 00:35:50,600
node does not have the capacity to

00:35:46,880 --> 00:35:51,590
service what you need and say a stateful

00:35:50,600 --> 00:35:54,500
application that doesn't support

00:35:51,590 --> 00:35:55,700
shardene would you be able to use port

00:35:54,500 --> 00:35:57,400
works yes if you have support for

00:35:55,700 --> 00:35:59,720
aggregated volumes so you can basically

00:35:57,400 --> 00:36:02,690
apart from doing the application you can

00:35:59,720 --> 00:36:04,730
basically split out a volume up to three

00:36:02,690 --> 00:36:09,190
aggregates so basically you can split

00:36:04,730 --> 00:36:09,190
out a bit amongst three nodes yeah

00:36:09,309 --> 00:36:13,430
and this has an advantage right so if

00:36:11,900 --> 00:36:15,410
you basically want a scale you can

00:36:13,430 --> 00:36:17,599
basically the scale up your nodes or you

00:36:15,410 --> 00:36:21,650
can scale scale out your notes right so

00:36:17,599 --> 00:36:23,300
you cannot actually add more disks to

00:36:21,650 --> 00:36:24,829
your existing nodes or if you don't have

00:36:23,300 --> 00:36:26,510
enough capacity or slot you can

00:36:24,829 --> 00:36:28,280
basically add more nodes and you'll be

00:36:26,510 --> 00:36:35,180
able to basically resize your volumes

00:36:28,280 --> 00:36:37,569
across nodes in that case - any more

00:36:35,180 --> 00:36:37,569
questions

00:36:57,110 --> 00:37:01,280
yes so if it's not the same size you

00:36:59,900 --> 00:37:02,990
will basically end up creating a new

00:37:01,280 --> 00:37:04,390
pool but it'll still be accessible for

00:37:02,990 --> 00:37:05,570
any new volumes that you want to create

00:37:04,390 --> 00:37:07,940
yeah

00:37:05,570 --> 00:37:09,650
and and what we basically do is we are

00:37:07,940 --> 00:37:11,630
able to partition we are able to create

00:37:09,650 --> 00:37:14,090
different pools for different types of

00:37:11,630 --> 00:37:15,710
discs so if you have SSDs as well as as

00:37:14,090 --> 00:37:18,050
CDs will basically end up creating two

00:37:15,710 --> 00:37:19,790
pools of these discs and depending on

00:37:18,050 --> 00:37:21,710
your application for example if it's

00:37:19,790 --> 00:37:23,810
Cassandra and you want low latency you

00:37:21,710 --> 00:37:25,010
can basically specify the class of

00:37:23,810 --> 00:37:28,310
service that you want and you will be

00:37:25,010 --> 00:37:30,050
able to use the SSD for your Cassandra

00:37:28,310 --> 00:37:32,510
sources but if you have something like

00:37:30,050 --> 00:37:34,610
Hadoop very well you want more

00:37:32,510 --> 00:37:36,650
throughput you can basically specify the

00:37:34,610 --> 00:37:38,810
as you need to be used instead of that

00:37:36,650 --> 00:37:40,730
and it will automatically place the

00:37:38,810 --> 00:37:43,750
replicas on all the different nodes on

00:37:40,730 --> 00:37:43,750
the same class for service

00:38:25,809 --> 00:38:32,059
so do you mean replication across

00:38:28,779 --> 00:38:37,400
availability zones or I sorry I didn't

00:38:32,059 --> 00:38:38,989
get your question so these would be two

00:38:37,400 --> 00:38:41,239
different clusters completely or would

00:38:38,989 --> 00:38:44,089
it be a part of the same cluster or

00:38:41,239 --> 00:38:52,839
would it be two different nodes in two

00:38:44,089 --> 00:38:52,839
different racks is what you mean okay

00:38:55,210 --> 00:39:01,309
okay so so right now we don't have

00:38:58,279 --> 00:39:03,559
support for replication across clusters

00:39:01,309 --> 00:39:04,759
because if I'm understanding caring this

00:39:03,559 --> 00:39:15,140
would be two different port works

00:39:04,759 --> 00:39:16,789
clusters completely right yes you can do

00:39:15,140 --> 00:39:18,140
that but obviously you would have to

00:39:16,789 --> 00:39:20,239
realize that there would be a latency

00:39:18,140 --> 00:39:22,819
involved when you're doing your

00:39:20,239 --> 00:39:25,150
application across like a vine or

00:39:22,819 --> 00:39:25,150
something

00:39:32,690 --> 00:39:36,290
I I guess that would all depend on your

00:39:34,670 --> 00:39:38,030
application needs and what your

00:39:36,290 --> 00:39:40,310
application requires for example if you

00:39:38,030 --> 00:39:41,960
are running Cassandra is that I mean

00:39:40,310 --> 00:39:45,350
that really won't help your application

00:39:41,960 --> 00:39:48,160
right so it it can be done yeah there is

00:39:45,350 --> 00:39:51,160
there is no limitation from our side

00:39:48,160 --> 00:39:51,160
yeah

00:39:55,700 --> 00:40:02,370
any more questions if you want more

00:40:00,720 --> 00:40:06,840
information you can visit us at our

00:40:02,370 --> 00:40:08,250
booth you can visit us at our booth you

00:40:06,840 --> 00:40:10,440
can also visit our website at port

00:40:08,250 --> 00:40:12,300
voxcom like i mentioned all our services

00:40:10,440 --> 00:40:14,130
are in the DC Universe so you can just

00:40:12,300 --> 00:40:17,340
show port works and they will all show

00:40:14,130 --> 00:40:19,560
up and if you want to request the demo

00:40:17,340 --> 00:40:22,010
you can always contact us at info at

00:40:19,560 --> 00:40:25,100
Port voxcom

00:40:22,010 --> 00:40:25,100

YouTube URL: https://www.youtube.com/watch?v=3xZ4GzBhGc0


