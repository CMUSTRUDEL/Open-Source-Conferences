Title: Deploying Kafka on DC OS
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Deploying Kafka on DC/OS - Kaufman Ng, Confluent

Apache Kafka is increasingly popular as the streaming platform of choice for real-time data pipelines. In addition, Kafka and microservices are deployed together in DC/OS. In this presentation, Kaufman Ng will discuss the best practices on deploying Kafka to DC/OS, the challenges, and lessons learned from customer deployments.

About


Kaufman Ng
Solutions Architect, Confluent
Solutions Architect at Confluent. Contributor to Apache Kafka.
Captions: 
	00:00:00,800 --> 00:00:06,080
hello everybody so we're gonna get

00:00:02,970 --> 00:00:08,370
started we have Kaufman Angie here from

00:00:06,080 --> 00:00:15,750
confluent who will talk to us about

00:00:08,370 --> 00:00:18,840
deploying Kafka on DCOs hi I'm Coffman

00:00:15,750 --> 00:00:23,430
and so so what we're going to talk about

00:00:18,840 --> 00:00:26,609
is basically oops basically the kind of

00:00:23,430 --> 00:00:29,970
how like the Gauchos how in fact how to

00:00:26,609 --> 00:00:31,949
effectively deploy cough currentc OS so

00:00:29,970 --> 00:00:34,230
with TC where everyone knows there's

00:00:31,949 --> 00:00:37,140
comfortable a lot you need you know a

00:00:34,230 --> 00:00:39,480
universe or packages right so but

00:00:37,140 --> 00:00:41,760
there's like some nuances you might want

00:00:39,480 --> 00:00:44,520
to count watch out for so the target

00:00:41,760 --> 00:00:46,649
audience it's mostly for the for the Ops

00:00:44,520 --> 00:00:52,289
guys administrators who ever a means

00:00:46,649 --> 00:00:57,059
administering your dcs clusters so about

00:00:52,289 --> 00:00:59,760
myself this is the agenda about myself

00:00:57,059 --> 00:01:03,059
I'm a Solutions Architect from confluent

00:00:59,760 --> 00:01:04,920
so and beaver Slee I've been to a

00:01:03,059 --> 00:01:07,470
company for over one year now and

00:01:04,920 --> 00:01:12,420
previously I was a I was a clogger doing

00:01:07,470 --> 00:01:14,880
similar role and I've contributed things

00:01:12,420 --> 00:01:20,939
patches fixes here and there to mostly

00:01:14,880 --> 00:01:23,220
to Kafka and park' so about company just

00:01:20,939 --> 00:01:25,970
a one slide pretty short so basically

00:01:23,220 --> 00:01:30,439
the company is exactly three years old

00:01:25,970 --> 00:01:33,960
so it's founded by the creators of Kafka

00:01:30,439 --> 00:01:37,680
so there are three founders Jai Neha and

00:01:33,960 --> 00:01:40,380
Joon they are the original inventors of

00:01:37,680 --> 00:01:44,960
Kafka and they came out they came Elissa

00:01:40,380 --> 00:01:48,030
make LinkedIn engineers and and

00:01:44,960 --> 00:01:50,750
currently we have about like hunter

00:01:48,030 --> 00:01:52,860
something people now 140 ish people and

00:01:50,750 --> 00:01:56,130
with a good portion of them are

00:01:52,860 --> 00:01:59,640
engineers and we think we are the

00:01:56,130 --> 00:02:04,140
largest contributor to Kafka to the

00:01:59,640 --> 00:02:07,920
Apache Kafka project so below down below

00:02:04,140 --> 00:02:10,039
basically those are the VC firms packing

00:02:07,920 --> 00:02:10,039
us

00:02:11,340 --> 00:02:17,220
so what's the relationship of comp the

00:02:15,540 --> 00:02:20,160
confluent platform which is our our

00:02:17,220 --> 00:02:22,620
product versus apache Kafka so you can

00:02:20,160 --> 00:02:24,720
see kind of so basically we we are

00:02:22,620 --> 00:02:26,610
building the enterprise offering on top

00:02:24,720 --> 00:02:31,470
of the open-source project and you can

00:02:26,610 --> 00:02:34,650
see the core the core Kafka logo here is

00:02:31,470 --> 00:02:38,160
the open source component and then what

00:02:34,650 --> 00:02:41,150
the confluent platform is it it's being

00:02:38,160 --> 00:02:43,739
offered in two versions one is the

00:02:41,150 --> 00:02:45,420
community or the open-source version so

00:02:43,739 --> 00:02:49,700
which is what you see in the blue circle

00:02:45,420 --> 00:02:54,660
there it comes with a bunch of add-ons

00:02:49,700 --> 00:02:56,549
connectors for Kafka connect some schema

00:02:54,660 --> 00:02:58,830
registry things for looking at the

00:02:56,549 --> 00:03:01,739
schemas and other things and other non

00:02:58,830 --> 00:03:02,880
Java clients - Kafka and then on top of

00:03:01,739 --> 00:03:05,940
that we have the Enterprise version

00:03:02,880 --> 00:03:09,380
which is what pushes of our commercial

00:03:05,940 --> 00:03:11,459
offering and from that we provide a

00:03:09,380 --> 00:03:17,060
control center which is our management

00:03:11,459 --> 00:03:17,060
tool some operational tools as well

00:03:19,340 --> 00:03:28,470
anyone from move anyone nothing moved up

00:03:22,500 --> 00:03:30,120
apache Kafka okay so everyone knows so I

00:03:28,470 --> 00:03:34,019
can kind of fly by this very quickly

00:03:30,120 --> 00:03:37,049
so basically Kafka is a basically a pops

00:03:34,019 --> 00:03:40,380
up paradigm distributed message message

00:03:37,049 --> 00:03:44,220
bus and a lot of people kind of think

00:03:40,380 --> 00:03:48,299
about it as like a message queue JMS MQ

00:03:44,220 --> 00:03:50,370
and the like and it's the main kind of

00:03:48,299 --> 00:03:52,200
distinguishing feature of Kafka is it is

00:03:50,370 --> 00:03:56,940
highly fault tolerant is highly

00:03:52,200 --> 00:03:58,650
performant it also allows stream

00:03:56,940 --> 00:04:04,079
processing I'm gonna talk about what it

00:03:58,650 --> 00:04:07,400
means in a bit so I'm gonna apply by

00:04:04,079 --> 00:04:09,920
this license you guys know Kafka already

00:04:07,400 --> 00:04:12,860
so

00:04:09,920 --> 00:04:17,510
yeah so basically to interact with Kafka

00:04:12,860 --> 00:04:19,790
you basically need to two types of

00:04:17,510 --> 00:04:21,620
clients so one is the producer one is

00:04:19,790 --> 00:04:23,630
the consumer and the names are obvious

00:04:21,620 --> 00:04:26,180
so one is basically for writing out

00:04:23,630 --> 00:04:30,440
messages to Kafka Kafka is in the middle

00:04:26,180 --> 00:04:33,740
one here and then and to read messages

00:04:30,440 --> 00:04:36,320
you need consumers as simple as that

00:04:33,740 --> 00:04:39,610
kind of same as a lot of the message

00:04:36,320 --> 00:04:43,220
queue systems

00:04:39,610 --> 00:04:47,210
so why was Kafka invented and at

00:04:43,220 --> 00:04:49,280
LinkedIn a couple years ago what what

00:04:47,210 --> 00:04:51,320
they experienced was they have a number

00:04:49,280 --> 00:04:55,660
of systems they need to have systems

00:04:51,320 --> 00:04:58,400
talking to each other so over time you

00:04:55,660 --> 00:05:00,169
need to in order talk from system one

00:04:58,400 --> 00:05:03,860
day system two you need to build custom

00:05:00,169 --> 00:05:05,870
integration points and then it gets you

00:05:03,860 --> 00:05:09,950
can see them the diagram here it gets

00:05:05,870 --> 00:05:13,060
messy right so that's why Kafka was

00:05:09,950 --> 00:05:16,310
invented as a centralized message bus

00:05:13,060 --> 00:05:18,770
cannot adding acid as a message hub for

00:05:16,310 --> 00:05:21,250
all the systems and because it's

00:05:18,770 --> 00:05:23,570
centralized you don't have to get the

00:05:21,250 --> 00:05:25,750
you don't get the duplication of

00:05:23,570 --> 00:05:28,550
messages you get a consistent interface

00:05:25,750 --> 00:05:34,130
to us to a centralized message message

00:05:28,550 --> 00:05:36,710
bus so a typical Kafka on the

00:05:34,130 --> 00:05:40,190
infrastructure side typical cluster

00:05:36,710 --> 00:05:44,090
consists of a couple things here so the

00:05:40,190 --> 00:05:45,800
broker is basically the the main server

00:05:44,090 --> 00:05:47,900
component of Kafka

00:05:45,800 --> 00:05:50,240
so basically coworker is basically

00:05:47,900 --> 00:05:52,190
stores and messengers and also serves

00:05:50,240 --> 00:05:55,479
search your messages down to the clients

00:05:52,190 --> 00:05:59,570
and zookeeper is basically used for

00:05:55,479 --> 00:06:01,850
coordination mechanism so in in a lot of

00:05:59,570 --> 00:06:04,729
distributed systems like Hadoop and

00:06:01,850 --> 00:06:07,280
Kafka they they rely on zookeeper as

00:06:04,729 --> 00:06:10,360
their coordination mechanisms for

00:06:07,280 --> 00:06:12,729
functionalities such as leader election

00:06:10,360 --> 00:06:15,229
finding out who the active guy is

00:06:12,729 --> 00:06:17,570
there's a special role in a Kafka

00:06:15,229 --> 00:06:21,550
cluster called controller so those kind

00:06:17,570 --> 00:06:24,430
of state management functionality is

00:06:21,550 --> 00:06:27,700
notify zookeeper so that's a requirement

00:06:24,430 --> 00:06:30,730
and the next two things are pretty more

00:06:27,700 --> 00:06:34,090
like optional things you can use with

00:06:30,730 --> 00:06:36,310
Kafka Custer Connect connect is

00:06:34,090 --> 00:06:38,200
basically basically like a like a

00:06:36,310 --> 00:06:40,230
mechanism for importing data and

00:06:38,200 --> 00:06:42,970
exporting data to and from Kafka and

00:06:40,230 --> 00:06:45,820
Kafka streams is the extremes processing

00:06:42,970 --> 00:06:47,860
framework when events come in to your

00:06:45,820 --> 00:06:48,400
message bus if you want to do something

00:06:47,860 --> 00:06:51,220
with it

00:06:48,400 --> 00:06:53,500
you can process it aggregate it run your

00:06:51,220 --> 00:06:55,570
business logic on top of it and so on so

00:06:53,500 --> 00:06:57,640
you would then use Kafka streams to

00:06:55,570 --> 00:07:01,720
process your data in while they are in

00:06:57,640 --> 00:07:03,430
Kafka so on top of that so the first

00:07:01,720 --> 00:07:09,070
four things here you can see they are

00:07:03,430 --> 00:07:10,120
the pericardium hacci Kafka project so

00:07:09,070 --> 00:07:13,720
the next couple of things is what

00:07:10,120 --> 00:07:18,550
content platform offers is the schema

00:07:13,720 --> 00:07:22,240
registry which can provide you with like

00:07:18,550 --> 00:07:26,020
a like a like a registry of data schemas

00:07:22,240 --> 00:07:28,540
so your fault your your message format

00:07:26,020 --> 00:07:30,670
if you if you enable with scheme and

00:07:28,540 --> 00:07:33,150
registry screaming registry will help

00:07:30,670 --> 00:07:37,030
you to make sure your data is consistent

00:07:33,150 --> 00:07:42,340
kind of like a database DDL from a

00:07:37,030 --> 00:07:44,020
relational database tables okay oops for

00:07:42,340 --> 00:07:47,140
ones that get lost in a screen there and

00:07:44,020 --> 00:07:51,070
then Rex proxy is basically a restful

00:07:47,140 --> 00:07:52,330
simple lightweight restful server for if

00:07:51,070 --> 00:07:54,940
you don't want to deal with the capital

00:07:52,330 --> 00:07:56,140
API you could use the rest for out to

00:07:54,940 --> 00:07:58,090
talk to Kafka it that way

00:07:56,140 --> 00:08:00,130
so basically it's provide to you that

00:07:58,090 --> 00:08:02,680
restful api to interact with Kafka and

00:08:00,130 --> 00:08:04,690
you can do similar things as the normal

00:08:02,680 --> 00:08:07,690
Kafka clients you can consume you can

00:08:04,690 --> 00:08:09,910
compute use there's other tools in a

00:08:07,690 --> 00:08:11,890
complan platform mostly into enterprise

00:08:09,910 --> 00:08:18,130
features Enterprise Edition on the

00:08:11,890 --> 00:08:23,080
platform so why Kafka on DCOs

00:08:18,130 --> 00:08:27,040
so as I mentioned Kafka cluster consists

00:08:23,080 --> 00:08:30,820
of multiple components here in order to

00:08:27,040 --> 00:08:33,280
provision position and new of while

00:08:30,820 --> 00:08:34,850
fully working cluster you probably need

00:08:33,280 --> 00:08:37,220
a couple machines

00:08:34,850 --> 00:08:39,620
typical footprint would be at least like

00:08:37,220 --> 00:08:42,080
six machines and so on for the brokers

00:08:39,620 --> 00:08:47,210
for the zookeeper and for other things

00:08:42,080 --> 00:08:49,430
so you need a management layer if you

00:08:47,210 --> 00:08:51,550
deploy on tram maybe that's not a big

00:08:49,430 --> 00:08:54,340
problem a lot of people do it with

00:08:51,550 --> 00:08:57,800
DevOps tools provisioning tools like

00:08:54,340 --> 00:09:01,880
chef puppet ansible that can be done as

00:08:57,800 --> 00:09:03,080
well but what if I want to run some of

00:09:01,880 --> 00:09:06,890
the things there's a lightweight

00:09:03,080 --> 00:09:09,110
containers right I want to deploy this

00:09:06,890 --> 00:09:11,960
schema registry I wanted qui zookeeper

00:09:09,110 --> 00:09:14,450
as a as a docker container for example

00:09:11,960 --> 00:09:17,720
right so you need a tool for actually

00:09:14,450 --> 00:09:19,400
managing all these right and some of the

00:09:17,720 --> 00:09:22,190
things some of the components in Kafka

00:09:19,400 --> 00:09:24,320
are stateful and some are stateless and

00:09:22,190 --> 00:09:27,110
how do you manage them we're going to

00:09:24,320 --> 00:09:29,150
talk about those nuances about how do

00:09:27,110 --> 00:09:33,440
how to deal with the staple services in

00:09:29,150 --> 00:09:35,480
general and also by provisioning of

00:09:33,440 --> 00:09:38,180
docker containers right you need service

00:09:35,480 --> 00:09:40,430
discovery and routing addressing all the

00:09:38,180 --> 00:09:44,570
all these things right so DCOs naturally

00:09:40,430 --> 00:09:47,060
provides all these for you so if you

00:09:44,570 --> 00:09:49,670
look at the diagram here basically this

00:09:47,060 --> 00:09:53,560
these are the typical Kopke coaster

00:09:49,670 --> 00:09:56,210
components and you can see from from

00:09:53,560 --> 00:09:58,430
from the icon here you can see there's

00:09:56,210 --> 00:10:01,040
like a I put in a disk so basically

00:09:58,430 --> 00:10:07,420
those things are the stateful services

00:10:01,040 --> 00:10:07,420
they usually require the local storage

00:10:07,630 --> 00:10:12,290
so you can have multiple instances of

00:10:10,010 --> 00:10:14,510
things for examples you can see we have

00:10:12,290 --> 00:10:16,600
now have three instances of Kafka

00:10:14,510 --> 00:10:18,830
Brokers the three zookeepers and

00:10:16,600 --> 00:10:20,720
multiple three instances of caca streams

00:10:18,830 --> 00:10:25,370
and so on so the other things down here

00:10:20,720 --> 00:10:28,720
they are more they're stateless and they

00:10:25,370 --> 00:10:33,460
tend to be light away oops

00:10:28,720 --> 00:10:38,840
the connection is getting flaky sorry

00:10:33,460 --> 00:10:42,590
so if you look at what the the actual

00:10:38,840 --> 00:10:46,340
state what they store so brokers pay

00:10:42,590 --> 00:10:48,200
basically you go if you look at the top

00:10:46,340 --> 00:10:51,260
layer it says Kafka streams

00:10:48,200 --> 00:10:53,090
so basically Kafka streams is the what

00:10:51,260 --> 00:10:55,940
you built in your what you built your

00:10:53,090 --> 00:10:58,640
app and chopper applications in so with

00:10:55,940 --> 00:11:00,470
streams it requires your local state

00:10:58,640 --> 00:11:03,560
store I'm gonna talk about what it means

00:11:00,470 --> 00:11:04,010
later on with capital brokers it stores

00:11:03,560 --> 00:11:05,570
data

00:11:04,010 --> 00:11:08,840
it stores and messengers so it also

00:11:05,570 --> 00:11:11,450
meets its own storage for the messages

00:11:08,840 --> 00:11:13,130
for the events coming in and we've

00:11:11,450 --> 00:11:16,820
zookeeper we all know that zookeeper has

00:11:13,130 --> 00:11:19,580
its own local storage so as well for

00:11:16,820 --> 00:11:22,460
transaction locks and snapshots and so

00:11:19,580 --> 00:11:24,230
on so these are you look at these things

00:11:22,460 --> 00:11:27,970
here there's at least three types of

00:11:24,230 --> 00:11:27,970
components now there are more stateful

00:11:28,030 --> 00:11:36,350
so how do you manage all these and also

00:11:32,360 --> 00:11:38,450
to work Kafka cluster you typically you

00:11:36,350 --> 00:11:40,190
would include you will put on other

00:11:38,450 --> 00:11:41,810
systems as well to talk to

00:11:40,190 --> 00:11:44,360
Kafka RA you need your custom

00:11:41,810 --> 00:11:47,000
applications you may have connect you

00:11:44,360 --> 00:11:49,670
may have you may have a relational

00:11:47,000 --> 00:11:52,790
database then step that's piping data

00:11:49,670 --> 00:11:55,040
into Kafka so so there's a lot more

00:11:52,790 --> 00:11:58,250
things going on than just calculate self

00:11:55,040 --> 00:12:02,120
and then you have to kind of address

00:11:58,250 --> 00:12:03,820
services discovery where those where

00:12:02,120 --> 00:12:08,480
those containers are located right

00:12:03,820 --> 00:12:14,270
addressing right and low balancing as

00:12:08,480 --> 00:12:21,550
well so weird where do where do we place

00:12:14,270 --> 00:12:22,700
these guys so you so brokers are

00:12:21,550 --> 00:12:26,030
stateful

00:12:22,700 --> 00:12:30,890
as I mentioned so because they are safe

00:12:26,030 --> 00:12:31,400
oh and and and they are relatively of a

00:12:30,890 --> 00:12:33,860
pickup

00:12:31,400 --> 00:12:35,900
they usually have a bigger footprint so

00:12:33,860 --> 00:12:39,800
we don't recommend them to be co-located

00:12:35,900 --> 00:12:42,260
so meaning that you should never place

00:12:39,800 --> 00:12:45,750
to Booker's on the same host or the same

00:12:42,260 --> 00:12:48,120
slave on DCOs and

00:12:45,750 --> 00:12:49,920
because brokers also have dedicated

00:12:48,120 --> 00:12:53,670
discs they should have that is a

00:12:49,920 --> 00:12:55,529
dedicated discs so so what what that

00:12:53,670 --> 00:12:58,529
means is when you when you're writing

00:12:55,529 --> 00:13:00,000
cat data to Kafka eventually those data

00:12:58,529 --> 00:13:03,569
will be actually written to the disk

00:13:00,000 --> 00:13:06,360
flushed flush to disk and when brokers

00:13:03,569 --> 00:13:09,769
start up they need to be they need to

00:13:06,360 --> 00:13:13,889
read the data from disks as well so

00:13:09,769 --> 00:13:16,500
that's why they should be dedicated for

00:13:13,889 --> 00:13:20,129
performance reasons and same force

00:13:16,500 --> 00:13:24,269
through keepers and should they be

00:13:20,129 --> 00:13:25,740
co-located probably not because even

00:13:24,269 --> 00:13:29,480
though zookeepers are more light away

00:13:25,740 --> 00:13:31,769
mechanisms brokers themselves are more

00:13:29,480 --> 00:13:34,769
heavier weight when it comes to memory

00:13:31,769 --> 00:13:39,029
i/o network and zoo camp zookeeper is

00:13:34,769 --> 00:13:42,600
more like a IO sensitive system and that

00:13:39,029 --> 00:13:44,490
were sensitive rather so you would we

00:13:42,600 --> 00:13:48,089
put them together you're likely getting

00:13:44,490 --> 00:13:50,370
to contention issues and then you have

00:13:48,089 --> 00:13:53,660
the thing about which container should I

00:13:50,370 --> 00:13:57,300
use right for each of these services

00:13:53,660 --> 00:13:59,670
could it typically would the most common

00:13:57,300 --> 00:14:00,959
options would be a talker and with meso

00:13:59,670 --> 00:14:09,629
space it also comes with its own

00:14:00,959 --> 00:14:10,139
container riser as well so for so with

00:14:09,629 --> 00:14:13,170
DCOs

00:14:10,139 --> 00:14:15,809
it comes with a lot of like the niceties

00:14:13,170 --> 00:14:18,899
features built in you can easily

00:14:15,809 --> 00:14:20,910
configure your service your footprint

00:14:18,899 --> 00:14:23,610
your system requirements and so on for

00:14:20,910 --> 00:14:26,819
the service and then you can actually

00:14:23,610 --> 00:14:29,399
also place these quality coil

00:14:26,819 --> 00:14:31,680
constraints meaning that you can play

00:14:29,399 --> 00:14:34,079
service certain things of services on a

00:14:31,680 --> 00:14:37,980
certain note or maybe pin down to

00:14:34,079 --> 00:14:40,139
certain certain notes if needed and you

00:14:37,980 --> 00:14:44,550
can handle these OS can hand out staple

00:14:40,139 --> 00:14:46,259
services once they're pinned down the

00:14:44,550 --> 00:14:53,189
services will stick on that note well

00:14:46,259 --> 00:14:56,279
regardless of how you restart it so a

00:14:53,189 --> 00:14:58,680
typical clock kafka cluster size how

00:14:56,279 --> 00:15:03,090
many of them should I use

00:14:58,680 --> 00:15:03,090
typically we recommend three brokers

00:15:03,150 --> 00:15:09,280
three is a good start because it does a

00:15:06,910 --> 00:15:11,350
hatchery it doesn't have to be three it

00:15:09,280 --> 00:15:14,140
could be two it doesn't have to be like

00:15:11,350 --> 00:15:16,600
odd number even a lot of people who use

00:15:14,140 --> 00:15:17,670
free because they like the replication

00:15:16,600 --> 00:15:21,010
factor to be free

00:15:17,670 --> 00:15:23,020
so what replication factor is basically

00:15:21,010 --> 00:15:25,510
for every single message that puts into

00:15:23,020 --> 00:15:27,850
the topic it will get replicated but

00:15:25,510 --> 00:15:32,550
that number of times and three times

00:15:27,850 --> 00:15:36,100
it's usually good for for tolerance and

00:15:32,550 --> 00:15:39,430
Kafka brokers themselves they don't need

00:15:36,100 --> 00:15:42,060
a lot of memory they are like they are

00:15:39,430 --> 00:15:45,310
Java processes but they unlike unlike

00:15:42,060 --> 00:15:47,590
j2ee applications they don't need a lot

00:15:45,310 --> 00:15:49,660
of heap a lot of the memory requirements

00:15:47,590 --> 00:15:53,950
it's actually all heap so the JVM

00:15:49,660 --> 00:15:56,890
process itself is relatively small one

00:15:53,950 --> 00:16:00,490
to two gigs four or five gigs is

00:15:56,890 --> 00:16:03,460
probably on the high end but but people

00:16:00,490 --> 00:16:05,950
have done it before and it's not a CPU

00:16:03,460 --> 00:16:10,690
heavy system either so a few course is

00:16:05,950 --> 00:16:12,910
okay it's not it's multi-threaded but

00:16:10,690 --> 00:16:16,180
it's not like very highly multi-threaded

00:16:12,910 --> 00:16:21,660
so so a relatively low number of course

00:16:16,180 --> 00:16:24,430
is fine and I'm not on the TCOs side

00:16:21,660 --> 00:16:26,140
typically on a marathon configuration

00:16:24,430 --> 00:16:28,030
you would put something like this in

00:16:26,140 --> 00:16:30,430
order to do your placement constraints

00:16:28,030 --> 00:16:36,430
so you guys know what that means

00:16:30,430 --> 00:16:40,750
deadlines here so basically what that

00:16:36,430 --> 00:16:44,160
means is I'm placing one max per host

00:16:40,750 --> 00:16:48,460
name it actually is pretty simple

00:16:44,160 --> 00:16:50,380
alternatively in the in the in the DCOs

00:16:48,460 --> 00:16:52,900
service configuration there's also

00:16:50,380 --> 00:16:56,170
something called placement strategy so

00:16:52,900 --> 00:16:58,780
in this case basically you're pinning

00:16:56,170 --> 00:17:02,170
down to placement onto a particular note

00:16:58,780 --> 00:17:04,590
and you can combine these two to the

00:17:02,170 --> 00:17:04,590
other by the way

00:17:05,890 --> 00:17:13,260
so this is how the the broker service

00:17:10,439 --> 00:17:17,410
convict our JSON looks like in DCOs

00:17:13,260 --> 00:17:20,589
so this is just a simple just a sample

00:17:17,410 --> 00:17:23,829
of it and so this thing we gonna we just

00:17:20,589 --> 00:17:26,679
taught about max per node you you only

00:17:23,829 --> 00:17:28,750
place one per note array and the next

00:17:26,679 --> 00:17:30,970
thing is that you deploy strategy so

00:17:28,750 --> 00:17:36,130
this is a DC wrestling how how you

00:17:30,970 --> 00:17:39,700
deploy instances or tasks of the service

00:17:36,130 --> 00:17:41,890
so remember we talked about Kafka broker

00:17:39,700 --> 00:17:44,320
has typically has three notes or more

00:17:41,890 --> 00:17:46,890
right so this kind of controls how you

00:17:44,320 --> 00:17:51,120
deploy the individual individual tasks

00:17:46,890 --> 00:17:53,500
one broker after another serially

00:17:51,120 --> 00:17:59,860
so you have to make sure this is in

00:17:53,500 --> 00:18:03,820
place and we also talked about brokers

00:17:59,860 --> 00:18:06,760
should have dedicated disks so what that

00:18:03,820 --> 00:18:09,910
means is if you could if you could then

00:18:06,760 --> 00:18:11,980
you should explicitly put a mount put a

00:18:09,910 --> 00:18:15,010
volume dedicated volume for the data for

00:18:11,980 --> 00:18:16,840
the comfort appropriate data so on a DCI

00:18:15,010 --> 00:18:18,880
side basically on the map on come fake

00:18:16,840 --> 00:18:37,570
you'll do something like this this type

00:18:18,880 --> 00:18:40,410
equals this type is mount yeah so so I

00:18:37,570 --> 00:18:43,090
can't talked about before Kafka messages

00:18:40,410 --> 00:18:46,870
they are flushed they're not immediately

00:18:43,090 --> 00:18:48,460
written so the reason is actually Kafka

00:18:46,870 --> 00:18:51,280
brokers actually caches the

00:18:48,460 --> 00:18:52,960
messages before it gets washed so that's

00:18:51,280 --> 00:18:55,559
how Kafka brokers achieve high

00:18:52,960 --> 00:18:58,240
performance so on the storage side

00:18:55,559 --> 00:19:01,210
because they are delayed fresh flushes

00:18:58,240 --> 00:19:04,840
we recommend in general hard drives are

00:19:01,210 --> 00:19:07,030
better than 80 than SSDs so SSD is good

00:19:04,840 --> 00:19:09,610
for random right but for like a big data

00:19:07,030 --> 00:19:12,480
systems usually standard hard drives is

00:19:09,610 --> 00:19:12,480
better Oh

00:19:15,100 --> 00:19:21,409
yeah I'm talking about coconuts here

00:19:17,419 --> 00:19:23,840
yeah yeah so but that doesn't mean you

00:19:21,409 --> 00:19:26,059
don't you can have it SSDs in

00:19:23,840 --> 00:19:29,899
combination right so a typical server

00:19:26,059 --> 00:19:31,490
node could have multiple disks

00:19:29,899 --> 00:19:34,039
you could probably could have like a

00:19:31,490 --> 00:19:38,929
SSDs for your boot volume for example

00:19:34,039 --> 00:19:41,360
and if you want to do this is more maybe

00:19:38,929 --> 00:19:43,940
maybe maybe not very commonly done in

00:19:41,360 --> 00:19:47,119
the TCOs world is on a lot on Prem

00:19:43,940 --> 00:19:50,240
systems like your standard on on Prem

00:19:47,119 --> 00:19:51,950
server great hardware a lot of people do

00:19:50,240 --> 00:19:55,399
multiple discs on server

00:19:51,950 --> 00:19:56,629
chassés right you can mount easily 1224

00:19:55,399 --> 00:20:00,049
disks right

00:19:56,629 --> 00:20:01,970
so in that case right a lot for lot big

00:20:00,049 --> 00:20:05,720
data systems you can set up as a more

00:20:01,970 --> 00:20:08,210
like a j-pod set up so basically what

00:20:05,720 --> 00:20:11,299
that means is those twelve this would

00:20:08,210 --> 00:20:15,860
appear as as short as 12 individual

00:20:11,299 --> 00:20:17,960
individual disks or alternatively you

00:20:15,860 --> 00:20:20,659
can do something like a rate you can put

00:20:17,960 --> 00:20:24,860
it in rate on it enable gray 5 rate 10

00:20:20,659 --> 00:20:27,379
then then with the rate of volume it's a

00:20:24,860 --> 00:20:29,419
it's a gigantic virtual volume so

00:20:27,379 --> 00:20:32,179
there's a protein constant on each one

00:20:29,419 --> 00:20:36,710
usually rate this rate is better for

00:20:32,179 --> 00:20:39,740
Kafka there's a couple reasons for it so

00:20:36,710 --> 00:20:42,649
one of the main reasons is Kafka brokers

00:20:39,740 --> 00:20:45,559
themselves they they handle one single

00:20:42,649 --> 00:20:48,320
volume better than j-pod so if you have

00:20:45,559 --> 00:20:54,889
multiple disks then it's probably you

00:20:48,320 --> 00:20:59,299
usually use rate mount we saw before in

00:20:54,889 --> 00:21:02,119
the config so good so basically mount in

00:20:59,299 --> 00:21:04,720
VCS means you are actually writing data

00:21:02,119 --> 00:21:08,769
in a dedicated volume rather than

00:21:04,720 --> 00:21:08,769
separate from the boot volume

00:21:11,809 --> 00:21:20,070
so let's say I have in my cluster in my

00:21:16,470 --> 00:21:22,500
DC RS cluster I have maybe maybe I

00:21:20,070 --> 00:21:24,660
wanted to have dedicate this but I can't

00:21:22,500 --> 00:21:28,020
afford to have dedicated discs on all

00:21:24,660 --> 00:21:31,020
the slaves right I want maybe I I might

00:21:28,020 --> 00:21:33,840
have if three powerful three slave nodes

00:21:31,020 --> 00:21:35,790
there they have more capacity right so

00:21:33,840 --> 00:21:38,250
how do I pin them onto the slave nodes

00:21:35,790 --> 00:21:41,730
onto those notes right so you could do

00:21:38,250 --> 00:21:43,890
something like an explicit IP addressing

00:21:41,730 --> 00:21:46,530
so in this case you can put a placement

00:21:43,890 --> 00:21:51,630
constraint with the pipe pipe litter

00:21:46,530 --> 00:21:53,490
number that list like this so this is

00:21:51,630 --> 00:21:58,440
the alternative to what we saw before

00:21:53,490 --> 00:22:00,000
earlier minutes ago so mins ago we saw

00:21:58,440 --> 00:22:02,610
the placement constraints will be

00:22:00,000 --> 00:22:04,650
maximum for maximum one instance per

00:22:02,610 --> 00:22:10,020
host and this is more explicit this is

00:22:04,650 --> 00:22:13,350
more narrow right so DCOs

00:22:10,020 --> 00:22:15,990
as we know comes with zookeeper already

00:22:13,350 --> 00:22:19,260
these z/os comes with a exhibitor tool

00:22:15,990 --> 00:22:22,040
it has the master node to use zookeeper

00:22:19,260 --> 00:22:25,530
right should we use that and by default

00:22:22,040 --> 00:22:28,440
so this is kind of the this is this is

00:22:25,530 --> 00:22:31,050
our this is somewhat debatable in some

00:22:28,440 --> 00:22:36,179
it depends on your use case if you have

00:22:31,050 --> 00:22:39,660
a small cluster maybe if you have a

00:22:36,179 --> 00:22:41,730
large cluster probably not and keep in

00:22:39,660 --> 00:22:44,940
mind if you have other services that use

00:22:41,730 --> 00:22:46,800
they also use zookeeper you probably

00:22:44,940 --> 00:22:49,470
should not should think about having a

00:22:46,800 --> 00:22:52,260
dedicated zookeeper quorum as far as I

00:22:49,470 --> 00:22:56,100
know for for master for the mesos master

00:22:52,260 --> 00:22:59,220
they already write a lot data to

00:22:56,100 --> 00:23:01,080
zookeeper right so if you have multiple

00:22:59,220 --> 00:23:03,030
user what they call user land

00:23:01,080 --> 00:23:05,370
applications that you also use the

00:23:03,030 --> 00:23:10,170
zookeeper you should consider setting up

00:23:05,370 --> 00:23:12,600
your own dedicated zookeeper but for

00:23:10,170 --> 00:23:15,110
definite deaf environment its this is

00:23:12,600 --> 00:23:15,110
popular okay

00:23:17,340 --> 00:23:28,090
so next we're going to talk about some

00:23:19,990 --> 00:23:30,100
some of the caveats contracts oops so

00:23:28,090 --> 00:23:33,670
you can restart services as we all know

00:23:30,100 --> 00:23:37,870
you move the DC US web UI that's

00:23:33,670 --> 00:23:43,750
probably the easiest sorry this

00:23:37,870 --> 00:23:47,170
flickering so but and alternatively you

00:23:43,750 --> 00:23:51,160
could also use the DC us CLI what the

00:23:47,170 --> 00:23:53,500
CLI offers you today is the CEO I can

00:23:51,160 --> 00:23:57,850
give you the far more finer control of

00:23:53,500 --> 00:23:59,170
what to restart so we're broker's is a

00:23:57,850 --> 00:24:02,170
little bit more trickier because

00:23:59,170 --> 00:24:05,080
Booker's they appear as one surface but

00:24:02,170 --> 00:24:08,350
underlying they are deployed as multiple

00:24:05,080 --> 00:24:10,720
instances multiple tasks right and we've

00:24:08,350 --> 00:24:12,850
capped a broker with a distributor

00:24:10,720 --> 00:24:15,550
system like that you should be called

00:24:12,850 --> 00:24:16,420
you should be cautious about restarting

00:24:15,550 --> 00:24:19,870
everything at once

00:24:16,420 --> 00:24:23,620
all right right right right right at the

00:24:19,870 --> 00:24:25,240
Oh everything after each other so what

00:24:23,620 --> 00:24:26,950
we recommend people doing is when you do

00:24:25,240 --> 00:24:29,650
a rope we should do a rolling restart

00:24:26,950 --> 00:24:32,280
and also keep some time between the

00:24:29,650 --> 00:24:35,320
rolling restarts of each int instances

00:24:32,280 --> 00:24:37,660
so right now we can recommend people

00:24:35,320 --> 00:24:40,360
during the sea to see how I restart way

00:24:37,660 --> 00:24:44,920
so the see how I restart you can achieve

00:24:40,360 --> 00:24:50,050
specified the broker ID to restart with

00:24:44,920 --> 00:24:52,720
the with the web you are you can't we're

00:24:50,050 --> 00:24:57,970
gonna jump into some details about Kafka

00:24:52,720 --> 00:25:01,090
streams and other failures yeah so this

00:24:57,970 --> 00:25:03,310
is what I just taught about no rolling

00:25:01,090 --> 00:25:05,230
restarts in a web UI and then it's

00:25:03,310 --> 00:25:07,750
barely do they see how I restart like

00:25:05,230 --> 00:25:10,180
that this is the example the broker ID

00:25:07,750 --> 00:25:13,480
is usually it's pretty simple is you

00:25:10,180 --> 00:25:15,250
start from 0 1 2 and so on and then you

00:25:13,480 --> 00:25:17,520
should also also check the logs of

00:25:15,250 --> 00:25:17,520
course

00:25:20,340 --> 00:25:25,550
the reason why you want to check the

00:25:22,020 --> 00:25:29,010
locks here is even the broker has

00:25:25,550 --> 00:25:31,110
restarted successfully it takes some

00:25:29,010 --> 00:25:32,550
time to catch up to the other remaining

00:25:31,110 --> 00:25:36,420
running instance in the distributed

00:25:32,550 --> 00:25:39,060
system SPRO kurz have to replicate the

00:25:36,420 --> 00:25:41,040
data from the from the running instances

00:25:39,060 --> 00:25:45,450
so that might take some time depends on

00:25:41,040 --> 00:25:47,730
how much data you have so the next thing

00:25:45,450 --> 00:25:51,870
we're going to talk about the catches on

00:25:47,730 --> 00:25:53,820
with Kafka streams so this is kind of a

00:25:51,870 --> 00:25:56,310
high level of what capita streams is

00:25:53,820 --> 00:25:58,800
basically it's a it's a application

00:25:56,310 --> 00:26:01,410
custom built application using a library

00:25:58,800 --> 00:26:06,060
to talk to Kafka to do event based

00:26:01,410 --> 00:26:07,800
processing that's all it is so so

00:26:06,060 --> 00:26:09,960
basically your deployment from a

00:26:07,800 --> 00:26:12,000
deployment standpoint you would be

00:26:09,960 --> 00:26:16,020
writing your own applications down here

00:26:12,000 --> 00:26:18,360
down in the boxes there and because the

00:26:16,020 --> 00:26:21,680
because with Kafka streams you're doing

00:26:18,360 --> 00:26:25,590
your own business logic calculations

00:26:21,680 --> 00:26:28,910
applications and so on so usually in the

00:26:25,590 --> 00:26:31,980
with API itself it comes with States

00:26:28,910 --> 00:26:33,080
through the way the way Kafka streams

00:26:31,980 --> 00:26:35,790
API

00:26:33,080 --> 00:26:37,650
keep-keep States is using something

00:26:35,790 --> 00:26:40,200
called state stores which is embedded

00:26:37,650 --> 00:26:45,350
databases you see in orange thing is

00:26:40,200 --> 00:26:49,170
there so let's say I have an application

00:26:45,350 --> 00:26:55,920
three instances so each one has its own

00:26:49,170 --> 00:26:59,640
States if one of them fails what happens

00:26:55,920 --> 00:27:03,660
is Kalka streams will actually we

00:26:59,640 --> 00:27:07,140
balance the workload you see the route

00:27:03,660 --> 00:27:11,310
of ret embedded a store cap gap

00:27:07,140 --> 00:27:13,170
replicated over so this is what this is

00:27:11,310 --> 00:27:15,780
what built in in the Kafka streams if

00:27:13,170 --> 00:27:18,120
one one of the instances dies you get

00:27:15,780 --> 00:27:20,070
the you get the low you get the

00:27:18,120 --> 00:27:22,950
balancing of the workload on to the

00:27:20,070 --> 00:27:26,400
remaining instances so with that set

00:27:22,950 --> 00:27:29,110
right this is stateful this this is a

00:27:26,400 --> 00:27:33,010
stateful application

00:27:29,110 --> 00:27:34,690
and in order to start the state to

00:27:33,010 --> 00:27:38,230
recover the state from the incense tree

00:27:34,690 --> 00:27:40,390
on to the other instances I need to

00:27:38,230 --> 00:27:44,470
catch up on the state the wrap to wrap

00:27:40,390 --> 00:27:46,110
thingy there right so naturally you can

00:27:44,470 --> 00:27:49,360
think imagine that might take some time

00:27:46,110 --> 00:27:52,150
I need to replicate a state from the

00:27:49,360 --> 00:27:54,880
Kafka Brooklands actually Republic we

00:27:52,150 --> 00:27:58,050
populated state and start the new tasks

00:27:54,880 --> 00:27:58,050
and so on right

00:27:59,310 --> 00:28:04,690
so I mentioned already every every

00:28:03,340 --> 00:28:07,600
copper streams application has its own

00:28:04,690 --> 00:28:10,900
state store state stores have to be

00:28:07,600 --> 00:28:12,760
synchronized and assigned to their

00:28:10,900 --> 00:28:16,330
instances and that could take time

00:28:12,760 --> 00:28:19,420
depends on how much data you have how

00:28:16,330 --> 00:28:22,390
much applications you have depends on

00:28:19,420 --> 00:28:24,220
how long you've been running as well so

00:28:22,390 --> 00:28:26,680
when they when when they are restarted

00:28:24,220 --> 00:28:29,860
if they can restart it they will be

00:28:26,680 --> 00:28:31,930
spawned on different notes and they will

00:28:29,860 --> 00:28:33,580
be started from the empty state store

00:28:31,930 --> 00:28:35,590
meaning that I have nothing to begin

00:28:33,580 --> 00:28:41,950
with right then I need to populate the

00:28:35,590 --> 00:28:45,400
state so there's some some other types

00:28:41,950 --> 00:28:48,730
of catchers to watch out for so this is

00:28:45,400 --> 00:28:51,370
Kathy the more like the failure in case

00:28:48,730 --> 00:28:54,820
of failures how do you how what's a

00:28:51,370 --> 00:28:57,700
proper it do it in DCOs so normally with

00:28:54,820 --> 00:29:02,980
Marathon Ray Marbury marathon will

00:28:57,700 --> 00:29:06,310
restart restart the tasks a new note

00:29:02,980 --> 00:29:09,070
possibly if they fail or if the health

00:29:06,310 --> 00:29:12,490
check fails right so oftentimes you see

00:29:09,070 --> 00:29:16,120
you may see services being killed or and

00:29:12,490 --> 00:29:19,570
restarted somewhere else so for for

00:29:16,120 --> 00:29:21,040
stateful applications in general this is

00:29:19,570 --> 00:29:24,040
something that you may want to watch out

00:29:21,040 --> 00:29:28,300
for is if it happens maybe it's okay but

00:29:24,040 --> 00:29:32,500
you you need you need to watch out for

00:29:28,300 --> 00:29:34,480
whether these things are started

00:29:32,500 --> 00:29:37,000
properly and working properly with

00:29:34,480 --> 00:29:39,460
kocher streams as I mentioned you need

00:29:37,000 --> 00:29:42,430
to allocate more time an administrator

00:29:39,460 --> 00:29:44,140
someone should look into it and also

00:29:42,430 --> 00:29:49,210
what what we recommend people doing is

00:29:44,140 --> 00:29:59,500
to alerting when I mean alerting is use

00:29:49,210 --> 00:30:03,040
Pedro de na gills and alike so these are

00:29:59,500 --> 00:30:05,530
currently the the information about the

00:30:03,040 --> 00:30:09,310
Kafka's services if we cannot draw down

00:30:05,530 --> 00:30:11,140
on one of these guys so so what

00:30:09,310 --> 00:30:13,030
so confluent and let me give you a

00:30:11,140 --> 00:30:15,280
little background here so confident meso

00:30:13,030 --> 00:30:18,670
sphere our partners so what we did was

00:30:15,280 --> 00:30:21,460
we kind of called about some dtcs

00:30:18,670 --> 00:30:24,700
packages and we've input from both sigh

00:30:21,460 --> 00:30:28,240
both companies and that gets deployed in

00:30:24,700 --> 00:30:31,690
as count ADC into the DCOs universe so

00:30:28,240 --> 00:30:34,180
the source of those packages are right

00:30:31,690 --> 00:30:37,720
here they are all open source so if you

00:30:34,180 --> 00:30:41,280
click on one of these guys here you can

00:30:37,720 --> 00:30:45,310
see a typical TCOs package deployment

00:30:41,280 --> 00:30:47,950
call definition rather so we can see the

00:30:45,310 --> 00:30:49,120
standard oh I need a escape

00:30:47,950 --> 00:30:56,020
curse screen here

00:30:49,120 --> 00:30:56,880
sorry you can't see mine can you guys

00:30:56,020 --> 00:30:59,920
see it

00:30:56,880 --> 00:31:03,040
yeah so this is no different from any

00:30:59,920 --> 00:31:05,710
other most other DCI services you can

00:31:03,040 --> 00:31:12,270
see the same bunch of config package

00:31:05,710 --> 00:31:12,270
resource JSON files so here if I go to

00:31:22,910 --> 00:31:29,510
so this is get you the documentation

00:31:25,290 --> 00:31:32,010
about the confluent kafka package so

00:31:29,510 --> 00:31:35,220
recently the diversion names may be a

00:31:32,010 --> 00:31:38,330
little bit complicated so right now the

00:31:35,220 --> 00:31:44,580
latest confluent platform version is

00:31:38,330 --> 00:31:46,380
33.0 as you can see in the later part of

00:31:44,580 --> 00:31:48,870
them that number numbering schemes here

00:31:46,380 --> 00:31:52,380
the first number is actually the kind of

00:31:48,870 --> 00:31:54,470
the framework version the DCOs framework

00:31:52,380 --> 00:31:54,470
version

00:32:02,409 --> 00:32:10,070
and also there's a couple other tools we

00:32:06,049 --> 00:32:12,320
recommend people doing installing so for

00:32:10,070 --> 00:32:17,210
mesosphere they come with they offered

00:32:12,320 --> 00:32:19,399
the Kafka client docker image so but

00:32:17,210 --> 00:32:21,019
what that is is basically a bunch of

00:32:19,399 --> 00:32:24,940
command line in the standard pouchy

00:32:21,019 --> 00:32:28,129
Kafka with no services running and

00:32:24,940 --> 00:32:31,779
there's also the confluent platform see

00:32:28,129 --> 00:32:33,769
how I so convent platform basically is a

00:32:31,779 --> 00:32:36,169
confluent packaged version of Apache

00:32:33,769 --> 00:32:40,820
Kafka binaries so it comes with standard

00:32:36,169 --> 00:32:43,940
Apache Kafka by command line as well and

00:32:40,820 --> 00:32:50,989
you can use any combination of tools if

00:32:43,940 --> 00:32:59,889
you like so I'm gonna show what it looks

00:32:50,989 --> 00:32:59,889
like here you see my screen okay

00:33:04,110 --> 00:33:11,770
so here I'm entering DCOs confluent

00:33:09,220 --> 00:33:17,440
Kafka so these are it comes with a bunch

00:33:11,770 --> 00:33:18,670
of sub comment and you can see I yeah

00:33:17,440 --> 00:33:20,650
we're not gonna go through that what

00:33:18,670 --> 00:33:23,260
that what those are but most of them are

00:33:20,650 --> 00:33:25,390
kind of the some of them are like

00:33:23,260 --> 00:33:28,240
administrative command some of them are

00:33:25,390 --> 00:33:30,460
for locking up things you can see like

00:33:28,240 --> 00:33:33,520
you can restart things if you want and

00:33:30,460 --> 00:33:36,520
you can look at the broker list and I

00:33:33,520 --> 00:33:39,850
have I think I have a I've configured to

00:33:36,520 --> 00:33:40,780
point to a running DC West coaster let's

00:33:39,850 --> 00:33:43,840
see yeah

00:33:40,780 --> 00:33:48,580
so these for example this one gives you

00:33:43,840 --> 00:33:51,850
the other broker poker IDs and another

00:33:48,580 --> 00:33:57,460
simple command I can run here a topic

00:33:51,850 --> 00:34:02,290
list here you can see these are the

00:33:57,460 --> 00:34:06,010
predefined kafka topics on my DC or s

00:34:02,290 --> 00:34:09,070
cluster so if your depends on the tool

00:34:06,010 --> 00:34:12,550
sets so the DC hours to set gave you

00:34:09,070 --> 00:34:15,250
this CI gives you this these output in a

00:34:12,550 --> 00:34:20,580
JSON like format and alternately you can

00:34:15,250 --> 00:34:20,580
do something like the standard Apache

00:34:20,610 --> 00:34:26,820
Kafka command line as well for example

00:34:23,649 --> 00:34:26,820
if I go to

00:34:32,940 --> 00:34:44,800
so now I'm on the and on the master note

00:34:41,040 --> 00:34:46,899
so what I purse they already have the

00:34:44,800 --> 00:34:48,940
talker the Kafka Klein talker image

00:34:46,899 --> 00:34:51,129
install pulldown already so on the

00:34:48,940 --> 00:34:57,060
master I already have it so I just

00:34:51,129 --> 00:34:57,060
simply run it and inside a container you

00:34:59,580 --> 00:35:11,530
get a bunch of comments like this so

00:35:08,830 --> 00:35:13,359
basically I'm not gonna run them all so

00:35:11,530 --> 00:35:16,630
basically there's list of commences so

00:35:13,359 --> 00:35:26,800
standard comments as in the Apache Kafka

00:35:16,630 --> 00:35:29,430
project alright I'm done here

00:35:26,800 --> 00:35:29,430
any questions

00:35:57,100 --> 00:36:01,070
right right

00:35:58,520 --> 00:36:05,870
you'll get used to you get leverage so

00:36:01,070 --> 00:36:07,310
basically the in the container you

00:36:05,870 --> 00:36:10,100
probably won't you I talked about

00:36:07,310 --> 00:36:12,710
earlier is the jvm process itself

00:36:10,100 --> 00:36:14,540
doesn't occupy a lot of memory right I

00:36:12,710 --> 00:36:17,390
mentioned about one to two gigs right

00:36:14,540 --> 00:36:19,460
but but you may want to allocate more

00:36:17,390 --> 00:36:22,430
free memory for the container in this

00:36:19,460 --> 00:36:24,530
case for the page cache so this is how

00:36:22,430 --> 00:36:27,050
Kafka actually caches your data before

00:36:24,530 --> 00:36:29,210
it gets flushed to disk so before they

00:36:27,050 --> 00:36:31,130
get first the data messages being

00:36:29,210 --> 00:36:33,650
freshly produced they will be sitting on

00:36:31,130 --> 00:36:35,990
the page cache and when consumers read

00:36:33,650 --> 00:36:37,880
from them they are actually Kafka

00:36:35,990 --> 00:36:39,590
brokers will serve them directly from

00:36:37,880 --> 00:36:41,930
the page cache instead of copying

00:36:39,590 --> 00:36:44,000
copying over to the JVM and then service

00:36:41,930 --> 00:36:48,470
out to the clients so this is what they

00:36:44,000 --> 00:36:51,110
call this zero copy mechanism so the

00:36:48,470 --> 00:36:54,230
more page cache you have the better in

00:36:51,110 --> 00:36:55,490
general it depends on how much data how

00:36:54,230 --> 00:36:57,590
much data volume

00:36:55,490 --> 00:37:00,770
it's very common people have playing it

00:36:57,590 --> 00:37:04,790
maybe at least at 32 gigs yeah good

00:37:00,770 --> 00:37:18,530
questions maybe give a chance to someone

00:37:04,790 --> 00:37:21,920
else oh sorry yeah yeah he does

00:37:18,530 --> 00:37:23,750
so in general I mentioned confluent and

00:37:21,920 --> 00:37:25,930
mesosphere we kind of have partnerships

00:37:23,750 --> 00:37:28,520
together so we kind of work make sure

00:37:25,930 --> 00:37:30,560
whatever are offering our latest version

00:37:28,520 --> 00:37:33,410
of the Kafka confluent platform

00:37:30,560 --> 00:37:40,220
particular gets updated on to the DCOs

00:37:33,410 --> 00:37:43,870
universe so yeah so the 3/3 version is

00:37:40,220 --> 00:37:43,870
what what you see in the latest

00:37:52,650 --> 00:37:58,359
yeah graceful is a little bit um

00:37:55,560 --> 00:38:02,680
somewhat of a loop like very loosely

00:37:58,359 --> 00:38:05,470
defined term because because as I

00:38:02,680 --> 00:38:08,470
mentioned how long it takes actually

00:38:05,470 --> 00:38:12,369
varies depends on how much data you have

00:38:08,470 --> 00:38:14,650
and how how behind you are on the

00:38:12,369 --> 00:38:17,020
brokers too so let's say my brokers was

00:38:14,650 --> 00:38:20,410
down for a day I have one day's worth of

00:38:17,020 --> 00:38:22,750
data to catch up on so that could be a

00:38:20,410 --> 00:38:25,300
lot I don't know depends on your use

00:38:22,750 --> 00:38:28,210
case so that's why we don't we don't

00:38:25,300 --> 00:38:31,000
really have like have a built in way for

00:38:28,210 --> 00:38:34,119
way for one minute our way for one hour

00:38:31,000 --> 00:38:36,190
someone should actually monitor so for

00:38:34,119 --> 00:38:38,320
so for now respond copper broker's is

00:38:36,190 --> 00:38:43,380
still somewhat manual even draws human

00:38:38,320 --> 00:38:43,380
supervision yeah

00:38:54,230 --> 00:38:59,700
yeah so the mesos Kafka I think I think

00:38:58,050 --> 00:39:04,590
you're talking about these just without

00:38:59,700 --> 00:39:06,900
a confluent prefix right yeah yeah so

00:39:04,590 --> 00:39:11,850
those are those sorts I know is based on

00:39:06,900 --> 00:39:13,740
the standard Apache Kafka project and so

00:39:11,850 --> 00:39:18,560
the difference is the confluent platform

00:39:13,740 --> 00:39:21,180
versus the Apache Kafka package so the

00:39:18,560 --> 00:39:23,790
it's pretty clear pretty compatible in

00:39:21,180 --> 00:39:26,370
general the confluent that form halves

00:39:23,790 --> 00:39:28,410
has more tools built in command-line

00:39:26,370 --> 00:39:30,780
tools built in and some enterprise

00:39:28,410 --> 00:39:48,120
features built in as I mentioned that's

00:39:30,780 --> 00:39:59,760
about it one more question the broker

00:39:48,120 --> 00:40:03,090
replacement oh yeah we recommend in

00:39:59,760 --> 00:40:08,340
general you should try to manually do it

00:40:03,090 --> 00:40:10,200
manually replace cuz if you use the

00:40:08,340 --> 00:40:13,020
amount of volume as I recommend before

00:40:10,200 --> 00:40:15,450
right you chances are you probably need

00:40:13,020 --> 00:40:17,910
to in the cloud environment you may have

00:40:15,450 --> 00:40:20,070
to detach the EBS volume or something

00:40:17,910 --> 00:40:23,520
right and then reattach the volume to a

00:40:20,070 --> 00:40:25,290
new wave placement so right now as far

00:40:23,520 --> 00:40:27,450
as I know I mean I think you need to

00:40:25,290 --> 00:40:29,610
build on your own even these yours

00:40:27,450 --> 00:40:32,220
doesn't have the capability to do that

00:40:29,610 --> 00:40:34,140
for you right remounting a disk this is

00:40:32,220 --> 00:40:37,470
something more on the awl side of things

00:40:34,140 --> 00:40:38,940
so yeah I think you have to

00:40:37,470 --> 00:40:42,030
administrators have to illustrate that

00:40:38,940 --> 00:40:47,200
to make it happen so it's still somewhat

00:40:42,030 --> 00:40:48,450
more manual yeah alright

00:40:47,200 --> 00:40:54,460
thanks guys

00:40:48,450 --> 00:40:54,460

YouTube URL: https://www.youtube.com/watch?v=iRdLwxorrh8


