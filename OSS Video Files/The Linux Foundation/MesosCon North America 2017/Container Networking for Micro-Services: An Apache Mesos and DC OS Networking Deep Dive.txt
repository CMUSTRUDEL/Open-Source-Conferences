Title: Container Networking for Micro-Services: An Apache Mesos and DC OS Networking Deep Dive
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Container Networking for Micro-Services: An Apache Mesos and DC/OS Networking Deep Dive - Deepak Goel & Jörg Schad, Mesosphere

Container Networking for Micro-Services: An Apache Mesos and DC/OS Networking Deep Dive - Deepak Goel & Jörg Schad, Mesosphere

About 

Jörg Schad
Software Engineer, Mesosphere
Jörg is a software engineer at Mesosphere in Hamburg. In his previous life he implemented distributed and in memory databases and conducted research in the Hadoop and Cloud area. His speaking experience includes various Meetups, international conferences, and lecture halls.

Deepak Goel
Software Engineer, Mesosphere Inc.
Captions: 
	00:00:00,469 --> 00:00:05,910
micro-services has emerged as the

00:00:03,600 --> 00:00:08,970
desired architecture for modern day

00:00:05,910 --> 00:00:10,740
application they are flexible because

00:00:08,970 --> 00:00:12,420
you can divide your monolithic

00:00:10,740 --> 00:00:14,580
application into smaller modular

00:00:12,420 --> 00:00:16,680
components and you can design develop

00:00:14,580 --> 00:00:18,960
and deploy these component independently

00:00:16,680 --> 00:00:21,650
they are scalable because you can run

00:00:18,960 --> 00:00:23,939
multiple instances of these components

00:00:21,650 --> 00:00:27,119
multiple instances of this component as

00:00:23,939 --> 00:00:29,490
desired in the cluster right but this

00:00:27,119 --> 00:00:32,640
flexibility and scalability comes at a

00:00:29,490 --> 00:00:34,710
cost of deployment complexity imagine

00:00:32,640 --> 00:00:37,079
that you have these components with

00:00:34,710 --> 00:00:39,510
their dependency and if they happens to

00:00:37,079 --> 00:00:42,030
run on same host then these dependency

00:00:39,510 --> 00:00:44,329
my conflict with each other well the

00:00:42,030 --> 00:00:47,309
answer is run everything in containers

00:00:44,329 --> 00:00:50,789
container is this nice construct where

00:00:47,309 --> 00:00:53,550
you pack your dependency as well as your

00:00:50,789 --> 00:00:55,890
component and deploy them together in an

00:00:53,550 --> 00:00:57,870
isolation in such a way that even if two

00:00:55,890 --> 00:00:59,760
containers land up on the same host

00:00:57,870 --> 00:01:02,489
they will not conflict with each other

00:00:59,760 --> 00:01:04,500
but I'm sure those who have enough

00:01:02,489 --> 00:01:06,689
played enough with containers would have

00:01:04,500 --> 00:01:08,850
realized that container themselves are

00:01:06,689 --> 00:01:10,640
not enough especially when you are

00:01:08,850 --> 00:01:12,900
planning to deploy them in production

00:01:10,640 --> 00:01:15,900
one they are mortal

00:01:12,900 --> 00:01:17,670
so they die you need a system that can

00:01:15,900 --> 00:01:21,150
monitor them continuously and can

00:01:17,670 --> 00:01:23,369
schedule them if they die second each

00:01:21,150 --> 00:01:25,350
container consumes certain amount of

00:01:23,369 --> 00:01:28,110
system resources such as CPU memory and

00:01:25,350 --> 00:01:30,090
disk you need a system that can do

00:01:28,110 --> 00:01:33,030
resource management for these containers

00:01:30,090 --> 00:01:35,100
and finally services that are running in

00:01:33,030 --> 00:01:37,170
these containers needs to be able to

00:01:35,100 --> 00:01:39,060
talk to each other or find each other so

00:01:37,170 --> 00:01:42,270
you need some kind of service management

00:01:39,060 --> 00:01:44,460
and this is where DCOs comes at these

00:01:42,270 --> 00:01:46,950
verses this container orchestration

00:01:44,460 --> 00:01:49,229
platform which has misses at your score

00:01:46,950 --> 00:01:51,420
and it with along with its frameworks

00:01:49,229 --> 00:01:53,490
such as marathon can dual container

00:01:51,420 --> 00:01:55,680
scheduling and resource management it

00:01:53,490 --> 00:01:57,329
also provides service management through

00:01:55,680 --> 00:01:59,340
service discovery and load balancing

00:01:57,329 --> 00:02:02,670
which is also the topic for my talk

00:01:59,340 --> 00:02:06,630
today and will go deep in that but

00:02:02,670 --> 00:02:08,849
before we take a specific DCOs

00:02:06,630 --> 00:02:10,709
networking stack we want to understand

00:02:08,849 --> 00:02:12,629
what are the challenges in container

00:02:10,709 --> 00:02:13,740
networking say you have container

00:02:12,629 --> 00:02:16,410
orchestration layer

00:02:13,740 --> 00:02:18,360
running with bunch of containers the

00:02:16,410 --> 00:02:20,660
very first challenge is to provide

00:02:18,360 --> 00:02:23,430
connectivity to these containers and

00:02:20,660 --> 00:02:25,440
these containers are different because

00:02:23,430 --> 00:02:29,580
they are not hosts or they are not VMs

00:02:25,440 --> 00:02:31,890
right second once you resolve that

00:02:29,580 --> 00:02:34,260
challenge of connectivity you would want

00:02:31,890 --> 00:02:36,480
the services that are running in these

00:02:34,260 --> 00:02:38,490
containers to be able to talk to each

00:02:36,480 --> 00:02:40,560
other so you need service discovery and

00:02:38,490 --> 00:02:42,990
now imagine as I said these containers

00:02:40,560 --> 00:02:44,910
keeps dying and keep rescheduling on

00:02:42,990 --> 00:02:47,850
different hosts so a service discovery

00:02:44,910 --> 00:02:50,370
mechanism should be able to update and

00:02:47,850 --> 00:02:54,060
reflect those changes in the cluster and

00:02:50,370 --> 00:02:56,520
finally you would want your container to

00:02:54,060 --> 00:02:59,790
have multiple instances and sitting

00:02:56,520 --> 00:03:01,350
behind a load balancer and load balancer

00:02:59,790 --> 00:03:03,120
would have exactly the same challenges

00:03:01,350 --> 00:03:05,610
service discovery it needs to reflect

00:03:03,120 --> 00:03:07,260
the dying and coming up of these

00:03:05,610 --> 00:03:11,820
containers in a cluster in a dynamic

00:03:07,260 --> 00:03:13,770
fashion so today there's bring us to our

00:03:11,820 --> 00:03:17,250
talk today so we will go through each of

00:03:13,770 --> 00:03:19,200
these topics in much detail but before

00:03:17,250 --> 00:03:21,110
we go there I want to give you a

00:03:19,200 --> 00:03:24,270
high-level view of how these different

00:03:21,110 --> 00:03:26,550
components in Decius network stack fits

00:03:24,270 --> 00:03:28,590
together to complete container

00:03:26,550 --> 00:03:32,700
networking so imagine that you have a

00:03:28,590 --> 00:03:35,010
master and bunch of missiles agents you

00:03:32,700 --> 00:03:38,520
could either use docker run time to

00:03:35,010 --> 00:03:39,930
launch docker containers or you can

00:03:38,520 --> 00:03:42,960
reduce something called universal

00:03:39,930 --> 00:03:44,670
container runtime which can run both

00:03:42,960 --> 00:03:48,540
docker containers as well as miz's

00:03:44,670 --> 00:03:50,250
container UCR has has a native support

00:03:48,540 --> 00:03:52,050
for something called CNI which is

00:03:50,250 --> 00:03:55,980
container networking interface and we

00:03:52,050 --> 00:03:58,640
will see what it is incoming slide but

00:03:55,980 --> 00:04:01,170
you see ER CNI gives this ability we're

00:03:58,640 --> 00:04:05,070
plugging of a third-party network

00:04:01,170 --> 00:04:07,350
becomes really easy docker on the other

00:04:05,070 --> 00:04:10,709
hand use something called CNM container

00:04:07,350 --> 00:04:13,140
network model so this provides the

00:04:10,709 --> 00:04:16,260
connectivity now service discovery is

00:04:13,140 --> 00:04:19,680
done through networking component called

00:04:16,260 --> 00:04:21,930
Spartan and misogynous Spartan is this

00:04:19,680 --> 00:04:23,790
component that runs on all the agents as

00:04:21,930 --> 00:04:25,350
well as master in a distributed fashion

00:04:23,790 --> 00:04:27,990
and we will see

00:04:25,350 --> 00:04:30,330
the benefit that it gives us being

00:04:27,990 --> 00:04:32,100
distributed and they gossip around to

00:04:30,330 --> 00:04:34,740
get the global state of the cluster

00:04:32,100 --> 00:04:37,170
similarly load-balancing is achieved

00:04:34,740 --> 00:04:39,750
through a component called Minutemen

00:04:37,170 --> 00:04:41,730
again like Spartan it also runs on all

00:04:39,750 --> 00:04:43,980
the agents as well as on the master and

00:04:41,730 --> 00:04:46,590
they gossip around to get the complete

00:04:43,980 --> 00:04:48,210
global state of the cluster just keep

00:04:46,590 --> 00:04:49,800
this picture in mind when we are

00:04:48,210 --> 00:04:52,950
focusing on each individual component

00:04:49,800 --> 00:04:54,720
this will give you a context as where

00:04:52,950 --> 00:04:57,390
this particular component fits in the

00:04:54,720 --> 00:04:59,460
entire picture so let's start our

00:04:57,390 --> 00:05:01,620
journey so first one is Container

00:04:59,460 --> 00:05:04,770
networking interface it is something

00:05:01,620 --> 00:05:09,630
proposed by core OS and now adopted by

00:05:04,770 --> 00:05:11,210
CN CF organisation body use here as I

00:05:09,630 --> 00:05:14,190
said earlier has a native support for

00:05:11,210 --> 00:05:17,490
CNI the way it works is there is a

00:05:14,190 --> 00:05:19,290
network CNI isolator in missus which is

00:05:17,490 --> 00:05:22,170
responsible for creating network

00:05:19,290 --> 00:05:24,900
namespace and then it hangs over this

00:05:22,170 --> 00:05:27,630
network namespace to plug in which is a

00:05:24,900 --> 00:05:29,280
CNI plugin and the CNI plugin is there

00:05:27,630 --> 00:05:31,530
is responsible for connecting the

00:05:29,280 --> 00:05:33,110
container to the host network or any

00:05:31,530 --> 00:05:38,160
network

00:05:33,110 --> 00:05:40,140
so each CNI plug-in comes with a

00:05:38,160 --> 00:05:42,180
conjugation or you can say each virtual

00:05:40,140 --> 00:05:43,890
network that is created in container

00:05:42,180 --> 00:05:45,420
networking comes with our configuration

00:05:43,890 --> 00:05:47,240
which is nothing but a JSON

00:05:45,420 --> 00:05:51,060
configuration with a bunch of key value

00:05:47,240 --> 00:05:53,760
pair specific to a particular plugin but

00:05:51,060 --> 00:05:56,490
it also has two important fields one the

00:05:53,760 --> 00:05:59,340
name of the network which defines what

00:05:56,490 --> 00:06:01,320
virtual network will be called and the

00:05:59,340 --> 00:06:02,820
second is the type of plug-in so there

00:06:01,320 --> 00:06:05,370
are different type of plugins like host

00:06:02,820 --> 00:06:08,190
plug-in bridge plug-in ipam port mapper

00:06:05,370 --> 00:06:11,250
plugin right and this configuration sits

00:06:08,190 --> 00:06:13,230
on each agent in DCOs cluster at up

00:06:11,250 --> 00:06:16,230
along with the confer long with the

00:06:13,230 --> 00:06:19,100
plugin at a predefined location

00:06:16,230 --> 00:06:22,860
now when the framework wants to launch a

00:06:19,100 --> 00:06:25,260
task on a particular virtual network it

00:06:22,860 --> 00:06:28,920
has to fill in networking foreign

00:06:25,260 --> 00:06:30,450
missiles wrote above the thing that it

00:06:28,920 --> 00:06:31,860
has to fill is the name of the new

00:06:30,450 --> 00:06:33,630
virtual network and that's why name is

00:06:31,860 --> 00:06:35,610
very important in the configuration so

00:06:33,630 --> 00:06:37,860
let's say such a task is triggered and

00:06:35,610 --> 00:06:38,400
it has launched on a particular agent

00:06:37,860 --> 00:06:41,040
the

00:06:38,400 --> 00:06:43,139
we'll go ahead and create the network

00:06:41,040 --> 00:06:45,570
namespace along with the rest of the

00:06:43,139 --> 00:06:47,729
container isolation and then it will

00:06:45,570 --> 00:06:49,560
take that network namespace give it to

00:06:47,729 --> 00:06:51,360
plugin in this particular example it is

00:06:49,560 --> 00:06:53,130
bridge plugin because the type in the

00:06:51,360 --> 00:06:56,070
configuration is bridge and the bridge

00:06:53,130 --> 00:06:57,810
plug-in will then make sure that this

00:06:56,070 --> 00:07:00,270
container is connected to the host

00:06:57,810 --> 00:07:03,720
network right that's how the cni

00:07:00,270 --> 00:07:05,580
machinery is working in d.c us now one

00:07:03,720 --> 00:07:08,039
of that implementation of that sienna is

00:07:05,580 --> 00:07:10,650
IP per container when we say IP per

00:07:08,039 --> 00:07:12,419
container a container may have IP but it

00:07:10,650 --> 00:07:15,240
it may not be routable like for example

00:07:12,419 --> 00:07:17,580
in our bridge setup where everything

00:07:15,240 --> 00:07:20,370
comes to the host and then port map to

00:07:17,580 --> 00:07:22,199
the container network right but when we

00:07:20,370 --> 00:07:24,570
say IP power container these are

00:07:22,199 --> 00:07:26,070
routable IP address you can access a

00:07:24,570 --> 00:07:29,039
container through an IP address and

00:07:26,070 --> 00:07:32,160
let's see how it is a cheap so it is it

00:07:29,039 --> 00:07:34,560
has dependent on my sauce module plus c

00:07:32,160 --> 00:07:37,080
and i as i said it it uses britt cni

00:07:34,560 --> 00:07:39,560
plugin and then it is it uses for

00:07:37,080 --> 00:07:43,530
encapsulation it uses something called

00:07:39,560 --> 00:07:48,660
VX lan which is which is very much in in

00:07:43,530 --> 00:07:51,270
linux kernel the way you configure any

00:07:48,660 --> 00:07:54,240
overlay in d.c us is through config dot

00:07:51,270 --> 00:07:56,250
EML i'm sure those who have launched dcs

00:07:54,240 --> 00:07:59,460
cluster would have encountered config

00:07:56,250 --> 00:08:01,349
dot ml so it is like a json which or ml

00:07:59,460 --> 00:08:04,530
file which has a key value pair which

00:08:01,349 --> 00:08:06,990
defines your cluster this the value that

00:08:04,530 --> 00:08:12,270
you see on the screen is the default

00:08:06,990 --> 00:08:13,979
default overlay network which comes out

00:08:12,270 --> 00:08:15,810
of the box you don't have to do it but

00:08:13,979 --> 00:08:18,050
you can change this configuration to

00:08:15,810 --> 00:08:23,010
change either the subnet for the overlay

00:08:18,050 --> 00:08:25,620
and or any other any other setting but

00:08:23,010 --> 00:08:28,169
you can also add multiple overlays one

00:08:25,620 --> 00:08:29,909
thing I didn't mention here how overlay

00:08:28,169 --> 00:08:34,200
is connected to IP per network IP per

00:08:29,909 --> 00:08:36,419
container so the IP that you get okay

00:08:34,200 --> 00:08:38,760
let me let me pause myself here and I'll

00:08:36,419 --> 00:08:40,890
explain how IP power network is kind is

00:08:38,760 --> 00:08:42,990
connected to overlay but imagine there

00:08:40,890 --> 00:08:44,459
is an overlay network and I will tell

00:08:42,990 --> 00:08:46,200
the reasoning behind why we need an

00:08:44,459 --> 00:08:48,150
overlay network or it will become more

00:08:46,200 --> 00:08:51,000
clear in coming slides so at a high

00:08:48,150 --> 00:08:53,160
level there is an overlay

00:08:51,000 --> 00:08:55,740
mrs. module that is running on master as

00:08:53,160 --> 00:08:58,440
well as on all the agents so at a boot

00:08:55,740 --> 00:09:00,450
up time the module that is running on

00:08:58,440 --> 00:09:02,880
the agents register with the master as

00:09:00,450 --> 00:09:05,880
part of this registration master takes

00:09:02,880 --> 00:09:08,850
the subnet configuration and break down

00:09:05,880 --> 00:09:10,560
the subnet into equal chunks of subnets

00:09:08,850 --> 00:09:12,860
so this is done statically at the time

00:09:10,560 --> 00:09:16,170
of registration and it hands over these

00:09:12,860 --> 00:09:18,360
chunks to each individual agent now the

00:09:16,170 --> 00:09:20,460
there is a helper module that is running

00:09:18,360 --> 00:09:23,100
on each agent called have star which

00:09:20,460 --> 00:09:25,560
continuously polls local agent state

00:09:23,100 --> 00:09:28,260
right for overlay as soon as it gets the

00:09:25,560 --> 00:09:31,710
overlay configuration it routes the map

00:09:28,260 --> 00:09:34,620
it it configures the route in the kernel

00:09:31,710 --> 00:09:37,560
right for a particular overlay and it

00:09:34,620 --> 00:09:39,420
also inform other neighboring Neph star

00:09:37,560 --> 00:09:42,930
about this configuration and that saw

00:09:39,420 --> 00:09:45,200
each Neve star is able to do a global

00:09:42,930 --> 00:09:47,940
connectivity of this over there

00:09:45,200 --> 00:09:49,920
now when marathon wants to launch a task

00:09:47,940 --> 00:09:51,870
it picks up one of the agent and the

00:09:49,920 --> 00:09:55,400
task will have an IP address that is

00:09:51,870 --> 00:09:57,960
routable right

00:09:55,400 --> 00:09:59,730
this light is the one that gives you the

00:09:57,960 --> 00:10:02,850
connectivity between overlay an IP per

00:09:59,730 --> 00:10:05,940
container so imagine that container has

00:10:02,850 --> 00:10:07,560
an IP address if and the subnet will

00:10:05,940 --> 00:10:09,620
definitely be different from the host

00:10:07,560 --> 00:10:12,330
one right so you need some kind of

00:10:09,620 --> 00:10:14,280
encapsulation on the host network to be

00:10:12,330 --> 00:10:16,260
able to route the traffic so imagine

00:10:14,280 --> 00:10:18,750
that container once wants to talk to

00:10:16,260 --> 00:10:22,740
container two it will send a packet and

00:10:18,750 --> 00:10:24,540
the routing entries on on agent one will

00:10:22,740 --> 00:10:27,810
make sure that all the packets go to V

00:10:24,540 --> 00:10:29,730
tap one we tap one has this VX land and

00:10:27,810 --> 00:10:31,430
capsulation it will encapsulate and send

00:10:29,730 --> 00:10:33,300
it to the destination we tap to

00:10:31,430 --> 00:10:35,370
destination we tap - we'll do a

00:10:33,300 --> 00:10:37,260
decapsulation and that's how the inner

00:10:35,370 --> 00:10:39,120
packet which is actually the packet

00:10:37,260 --> 00:10:41,190
destined to container - will reach there

00:10:39,120 --> 00:10:42,570
so that's why we need overland how

00:10:41,190 --> 00:10:45,990
overlay is connected with IP per

00:10:42,570 --> 00:10:49,230
container now service discovery is

00:10:45,990 --> 00:10:51,960
through spot and in massage DNS both of

00:10:49,230 --> 00:10:56,370
them are open source project both of

00:10:51,960 --> 00:10:58,170
them monitors the tasks that are getting

00:10:56,370 --> 00:11:00,420
launched in the container and create

00:10:58,170 --> 00:11:02,880
appropriate SRV and a records for

00:11:00,420 --> 00:11:04,310
service discovery at a high level the

00:11:02,880 --> 00:11:07,470
way it works is the

00:11:04,310 --> 00:11:08,970
both missus DNS and Spartan both of

00:11:07,470 --> 00:11:12,959
these components are running on master

00:11:08,970 --> 00:11:14,579
and they pole master state and to see if

00:11:12,959 --> 00:11:17,610
there is any new toss that has come in

00:11:14,579 --> 00:11:20,100
and create new era cars on SRV records

00:11:17,610 --> 00:11:22,079
or to see if there is some task that has

00:11:20,100 --> 00:11:26,880
died and you need to remove those

00:11:22,079 --> 00:11:29,220
records from DNS Spartan also

00:11:26,880 --> 00:11:31,470
communicate this information to all the

00:11:29,220 --> 00:11:33,570
other Spartan neighboring Spartans which

00:11:31,470 --> 00:11:36,779
are running on the agents and that saw

00:11:33,570 --> 00:11:39,690
each of the agent gets the entire DNS

00:11:36,779 --> 00:11:42,630
record for the for the whole cluster the

00:11:39,690 --> 00:11:44,339
benefit of that is if there is a task

00:11:42,630 --> 00:11:46,470
that is running on a particular agent

00:11:44,339 --> 00:11:48,390
and it issues a query that query is

00:11:46,470 --> 00:11:52,829
locally intercepted by Spartan and

00:11:48,390 --> 00:11:55,680
respond right so the DNS as long as it

00:11:52,829 --> 00:11:57,630
is within the cluster like the DNS and a

00:11:55,680 --> 00:11:59,610
addresses such that it can be resolved

00:11:57,630 --> 00:12:02,310
within the cluster the DNS query doesn't

00:11:59,610 --> 00:12:04,470
leave even the agent which gives a lot

00:12:02,310 --> 00:12:07,829
of scalability so Spartan is really this

00:12:04,470 --> 00:12:11,519
the distributed DNS proxy which reduces

00:12:07,829 --> 00:12:14,010
the latency along with being fact that

00:12:11,519 --> 00:12:15,660
it is running on each agent it also does

00:12:14,010 --> 00:12:17,880
something clever which is called dual

00:12:15,660 --> 00:12:20,310
dispatch and we will see how dual

00:12:17,880 --> 00:12:22,230
dispatch helps in speeding up the DNS

00:12:20,310 --> 00:12:25,079
resolution and then finally it also

00:12:22,230 --> 00:12:29,190
support upstream configuring upstream

00:12:25,079 --> 00:12:31,680
per domain so you can have dot-com TLD

00:12:29,190 --> 00:12:35,250
going towards one upstream and dot-org

00:12:31,680 --> 00:12:37,260
tailed it going to another upstream so

00:12:35,250 --> 00:12:39,959
coming to dual dispatch dual dispatch is

00:12:37,260 --> 00:12:42,390
this optimizations pattern so usually if

00:12:39,959 --> 00:12:44,850
you know if you have dealt with DNS the

00:12:42,390 --> 00:12:46,949
way the dns resolution works is one of

00:12:44,850 --> 00:12:49,170
the system will pick one of the upstream

00:12:46,949 --> 00:12:51,180
and send the DNS query to that upstream

00:12:49,170 --> 00:12:54,329
it will wait for the DNS query to fail

00:12:51,180 --> 00:12:56,699
or pass and when the timeout happens its

00:12:54,329 --> 00:12:59,850
picks up another upstream and sensit so

00:12:56,699 --> 00:13:01,949
that's that can kinds of add latency to

00:12:59,850 --> 00:13:03,959
the DNS resolution what Spartan does

00:13:01,949 --> 00:13:07,350
whenever there is a query from a misses

00:13:03,959 --> 00:13:09,480
agent it picks up two up streams and do

00:13:07,350 --> 00:13:12,000
a dual dispatch it simultaneously send

00:13:09,480 --> 00:13:13,860
queries to both the upstream then one of

00:13:12,000 --> 00:13:16,860
the stream would respond whichever it is

00:13:13,860 --> 00:13:18,810
responds first it sends that response

00:13:16,860 --> 00:13:21,570
to the to the task that was querying it

00:13:18,810 --> 00:13:23,610
and the second response that concerned

00:13:21,570 --> 00:13:25,740
it just note down the matters for that

00:13:23,610 --> 00:13:29,160
upstream so that in future if it has to

00:13:25,740 --> 00:13:30,510
pick the upstream it will kind of deny

00:13:29,160 --> 00:13:32,870
it will not pick up that particular

00:13:30,510 --> 00:13:35,670
upstream because it was low last time

00:13:32,870 --> 00:13:37,830
right now to give you a picture how it

00:13:35,670 --> 00:13:40,620
all fits let's say we have a cluster we

00:13:37,830 --> 00:13:43,530
have bunch of masters and an agent nodes

00:13:40,620 --> 00:13:46,890
that are running and we have an upstream

00:13:43,530 --> 00:13:48,960
so if there is a task that queries as I

00:13:46,890 --> 00:13:51,000
said earlier that only queries for the

00:13:48,960 --> 00:13:53,460
local DNS then that dns resolution

00:13:51,000 --> 00:13:56,070
happens locally on that particular agent

00:13:53,460 --> 00:13:58,320
if there is something which is external

00:13:56,070 --> 00:13:59,310
to the cluster such as comm kind of a

00:13:58,320 --> 00:14:01,200
record

00:13:59,310 --> 00:14:04,080
then Spartan will send it to the

00:14:01,200 --> 00:14:06,600
upstream and all dot missus goes to the

00:14:04,080 --> 00:14:10,110
massage DNS that's how the the service

00:14:06,600 --> 00:14:12,360
discovery happens in dcs now coming to

00:14:10,110 --> 00:14:14,490
load balancing node balancing is done

00:14:12,360 --> 00:14:17,430
through Minutemen and Marathon lb

00:14:14,490 --> 00:14:20,730
Minutemen is this layer for load

00:14:17,430 --> 00:14:23,070
balancer it is based on TCP and it uses

00:14:20,730 --> 00:14:26,460
something which is there in Linux kernel

00:14:23,070 --> 00:14:30,630
the the load LVS which is load balance

00:14:26,460 --> 00:14:32,970
virtual server and it so that so the

00:14:30,630 --> 00:14:35,760
benefit of that is the entire data plane

00:14:32,970 --> 00:14:38,220
is inside the condom and minutemen only

00:14:35,760 --> 00:14:41,910
do the control plane handling of the

00:14:38,220 --> 00:14:45,810
control plane the way you use whip in DC

00:14:41,910 --> 00:14:48,390
OS is through this app definition those

00:14:45,810 --> 00:14:50,010
who have interacted with marathon you

00:14:48,390 --> 00:14:51,600
need to submit this app definition in

00:14:50,010 --> 00:14:52,530
order to launch your task right so this

00:14:51,600 --> 00:14:54,060
is a particular example

00:14:52,530 --> 00:14:56,580
and it's through label so you need to

00:14:54,060 --> 00:14:59,010
specify a label called whip along with

00:14:56,580 --> 00:15:01,080
the name of the whip so these are named

00:14:59,010 --> 00:15:02,730
whip but you can also use something

00:15:01,080 --> 00:15:05,490
called IP whip where you can directly

00:15:02,730 --> 00:15:07,590
give the IP address and the port if you

00:15:05,490 --> 00:15:09,210
give name that name is translated to the

00:15:07,590 --> 00:15:10,800
actual whip which is at the bottom of

00:15:09,210 --> 00:15:12,180
the screen so if you give a web server

00:15:10,800 --> 00:15:14,520
then it will become web server dot

00:15:12,180 --> 00:15:18,090
marathon alpha will be this Decius

00:15:14,520 --> 00:15:21,720
directory at a high level let's say

00:15:18,090 --> 00:15:24,510
marathon request a task to be launched

00:15:21,720 --> 00:15:28,050
with a label food dot five thousand foo

00:15:24,510 --> 00:15:29,670
is the name whip master will pick up say

00:15:28,050 --> 00:15:30,570
one of the agent agent one to launch

00:15:29,670 --> 00:15:32,760
this task

00:15:30,570 --> 00:15:35,340
then the stars launched but the actual

00:15:32,760 --> 00:15:38,910
port the task is running on is say six

00:15:35,340 --> 00:15:40,950
seven eight nine now Minuteman that is

00:15:38,910 --> 00:15:42,810
locally running on this agent where the

00:15:40,950 --> 00:15:44,880
task was launched is continuously polled

00:15:42,810 --> 00:15:46,950
for a state like every two second and

00:15:44,880 --> 00:15:50,580
then it gets as soon as the task is not

00:15:46,950 --> 00:15:54,480
it gets this mapping between whip : port

00:15:50,580 --> 00:15:58,260
to the actual port of that task then

00:15:54,480 --> 00:15:59,910
what it does is it it gossip this

00:15:58,260 --> 00:16:03,000
information to the entire cluster

00:15:59,910 --> 00:16:05,130
through other neighboring Minutemen all

00:16:03,000 --> 00:16:06,660
the minute minds along with master or

00:16:05,130 --> 00:16:08,130
agent wherever they are running what

00:16:06,660 --> 00:16:12,060
they will do they will pick an IP

00:16:08,130 --> 00:16:14,520
independently right the reason why an IP

00:16:12,060 --> 00:16:16,770
address is not communicated because if

00:16:14,520 --> 00:16:19,380
you imagine IP address as a state so if

00:16:16,770 --> 00:16:20,700
if they have to communicate IP address

00:16:19,380 --> 00:16:23,040
instead of the name then there might be

00:16:20,700 --> 00:16:24,960
a conflict between two minutes meant

00:16:23,040 --> 00:16:26,790
picking up the same IP address so they

00:16:24,960 --> 00:16:31,590
just communicate that hey there is a

00:16:26,790 --> 00:16:33,630
name for a task within a whip foo was

00:16:31,590 --> 00:16:36,470
launched on a particular agent and each

00:16:33,630 --> 00:16:40,380
Minutemen chose their own IP address

00:16:36,470 --> 00:16:42,420
then Minutemen creates a local a record

00:16:40,380 --> 00:16:44,160
in Sparta each Minutemen will do that so

00:16:42,420 --> 00:16:47,970
all Sparta will have this a record

00:16:44,160 --> 00:16:50,790
mapping the whip to an actual IP it also

00:16:47,970 --> 00:16:52,830
programs the kernel with the appropriate

00:16:50,790 --> 00:16:55,020
front end and the back end so the IP

00:16:52,830 --> 00:16:56,610
that is pegged for this particular whip

00:16:55,020 --> 00:16:58,890
in this case it is one two three four

00:16:56,610 --> 00:17:03,090
five cows and the back end is the actual

00:16:58,890 --> 00:17:06,510
server IP with the port so now let's say

00:17:03,090 --> 00:17:08,790
when the task two wants to connect to

00:17:06,510 --> 00:17:11,640
task one through load balanced whip it

00:17:08,790 --> 00:17:14,910
will query the local Spartan with within

00:17:11,640 --> 00:17:17,160
with the DNS NT Spartan would give the

00:17:14,910 --> 00:17:20,250
local IP address that was picked by

00:17:17,160 --> 00:17:21,870
minute men and then it will try to do

00:17:20,250 --> 00:17:24,570
connect which will be intercepted by the

00:17:21,870 --> 00:17:26,699
colonel locally on this agent but

00:17:24,570 --> 00:17:28,439
ultimately because the IPPS entry has

00:17:26,699 --> 00:17:30,570
the backend information it will connect

00:17:28,439 --> 00:17:33,270
to the task one that is connect running

00:17:30,570 --> 00:17:35,730
on agent one that's how task two is able

00:17:33,270 --> 00:17:37,710
to connect to task one

00:17:35,730 --> 00:17:39,900
now coming to marathon I'll be marathon

00:17:37,710 --> 00:17:42,210
will be something that is layer 7 load

00:17:39,900 --> 00:17:45,440
balancer so when we were talking about

00:17:42,210 --> 00:17:47,940
Minutemen it was just layer 4 but

00:17:45,440 --> 00:17:50,610
marathon is layer 7 and it is a wrapper

00:17:47,940 --> 00:17:54,120
around a chip Roxy it the way it works

00:17:50,610 --> 00:17:56,010
is at a high level there is a concept

00:17:54,120 --> 00:17:58,410
called public agent public agents are

00:17:56,010 --> 00:18:00,750
those agents which say have an IP

00:17:58,410 --> 00:18:03,300
address or an interface which is exposed

00:18:00,750 --> 00:18:05,430
to the outside of the cluster so that's

00:18:03,300 --> 00:18:07,290
why you those agents are public agents

00:18:05,430 --> 00:18:10,110
so marathoner I will be say running on a

00:18:07,290 --> 00:18:13,200
public agent it continuously listen on

00:18:10,110 --> 00:18:17,130
marathon event bus for any new task

00:18:13,200 --> 00:18:19,470
right and as soon as a new task is

00:18:17,130 --> 00:18:21,810
launched or it dies it updates the HT

00:18:19,470 --> 00:18:23,160
proxy configuration internally then

00:18:21,810 --> 00:18:25,950
let's say when an external client is

00:18:23,160 --> 00:18:29,040
trying to connect to this H a proxy it

00:18:25,950 --> 00:18:31,350
load balances on the task and that's how

00:18:29,040 --> 00:18:35,700
you expose your internal running

00:18:31,350 --> 00:18:37,770
services to outside the cluster it was

00:18:35,700 --> 00:18:40,170
pretty much the same way as minutemen

00:18:37,770 --> 00:18:43,650
with there's a label on in the app

00:18:40,170 --> 00:18:46,050
definition there like tons of label if

00:18:43,650 --> 00:18:48,840
you've seen that repo man Athan lb lever

00:18:46,050 --> 00:18:50,700
wrapper but a to of like in this example

00:18:48,840 --> 00:18:52,380
or of the most important one is the

00:18:50,700 --> 00:18:54,990
external is defined that you are

00:18:52,380 --> 00:19:02,130
exposing this service externally and

00:18:54,990 --> 00:19:04,830
then what dienes it should be having ok

00:19:02,130 --> 00:19:06,930
that brings up to the things that we are

00:19:04,830 --> 00:19:09,450
currently working on so very first thing

00:19:06,930 --> 00:19:12,840
that you would see in future is is the

00:19:09,450 --> 00:19:16,140
ipv6 support in dcs networking stack

00:19:12,840 --> 00:19:18,990
then there is something called CNI spec

00:19:16,140 --> 00:19:22,080
versions so right now we support 0.2 dot

00:19:18,990 --> 00:19:25,260
0 we want to support 0.3 going forward

00:19:22,080 --> 00:19:28,260
we want to support 0.001 of the main

00:19:25,260 --> 00:19:31,830
feature in 0.3 is service chaining

00:19:28,260 --> 00:19:35,700
so those of you who have some experience

00:19:31,830 --> 00:19:39,150
with the CNI would know that this many

00:19:35,700 --> 00:19:41,280
times you need different services and by

00:19:39,150 --> 00:19:43,200
service here I mean like load balancing

00:19:41,280 --> 00:19:44,880
service DNS service or IP connectivity

00:19:43,200 --> 00:19:47,130
is also service there's an IBM service

00:19:44,880 --> 00:19:50,670
there's a port mapper service so these

00:19:47,130 --> 00:19:54,540
are various services today in expect 2.0

00:19:50,670 --> 00:19:56,940
you really cannot mix and match these

00:19:54,540 --> 00:19:59,400
services in a way that in a pluggable

00:19:56,940 --> 00:20:01,650
way but with 0.3 it has a service

00:19:59,400 --> 00:20:04,140
chaining which will allow you to pick

00:20:01,650 --> 00:20:06,720
any of these different services and

00:20:04,140 --> 00:20:09,240
create a chain of these services so it

00:20:06,720 --> 00:20:10,710
can dynamically call different virtual

00:20:09,240 --> 00:20:13,140
network can have different services

00:20:10,710 --> 00:20:14,220
running for the same functionality so

00:20:13,140 --> 00:20:17,880
that's pretty powerful

00:20:14,220 --> 00:20:20,460
and then today in DCOs if you want to

00:20:17,880 --> 00:20:22,260
use certain CNI plugin the steps are

00:20:20,460 --> 00:20:23,610
very manual you have to deploy the

00:20:22,260 --> 00:20:26,310
plug-in as well as the configuration

00:20:23,610 --> 00:20:29,130
manually on each agent we want to take

00:20:26,310 --> 00:20:31,230
it out and make it more streamlined and

00:20:29,130 --> 00:20:33,330
so we are working on something called

00:20:31,230 --> 00:20:36,480
cocina configuration service which will

00:20:33,330 --> 00:20:39,990
have an effect of like clicking few

00:20:36,480 --> 00:20:43,470
buttons on the UI and your CNI plugin is

00:20:39,990 --> 00:20:45,390
ready to be used in a network and then

00:20:43,470 --> 00:20:47,490
there is a demand for having

00:20:45,390 --> 00:20:49,530
multi-tenancy in network like certain

00:20:47,490 --> 00:20:51,390
users or certain organization should not

00:20:49,530 --> 00:20:53,700
be allowed to launch containers or

00:20:51,390 --> 00:20:54,810
certain networks so some kind of

00:20:53,700 --> 00:20:56,280
multi-tenancy

00:20:54,810 --> 00:20:58,140
so we are working on something called

00:20:56,280 --> 00:21:02,010
authorization and authentication with

00:20:58,140 --> 00:21:03,540
Network and also a security policy and a

00:21:02,010 --> 00:21:06,240
security policy when we talk about

00:21:03,540 --> 00:21:09,840
security policy it's at two layers one

00:21:06,240 --> 00:21:11,220
is per virtual network like whether a

00:21:09,840 --> 00:21:12,870
virtual network should be allowed to

00:21:11,220 --> 00:21:15,030
communicate with another virtual network

00:21:12,870 --> 00:21:17,520
what kind of isolation we need among

00:21:15,030 --> 00:21:19,620
virtual networks and then within a

00:21:17,520 --> 00:21:21,420
virtual network they are ACS like what

00:21:19,620 --> 00:21:23,370
port and IP should be allowed to connect

00:21:21,420 --> 00:21:26,820
to watch port and server this will give

00:21:23,370 --> 00:21:28,230
us the flexibility of say which services

00:21:26,820 --> 00:21:31,010
are allowed to connect to which all

00:21:28,230 --> 00:21:34,050
services so we are working on that and

00:21:31,010 --> 00:21:34,710
finally the big picture that we started

00:21:34,050 --> 00:21:37,500
our journey from

00:21:34,710 --> 00:21:40,760
that also concludes my talk for today

00:21:37,500 --> 00:21:40,760
any questions

00:21:54,550 --> 00:22:04,030
mostly I've seen people using calico

00:21:59,070 --> 00:22:06,610
pardon me I think it whatnot

00:22:04,030 --> 00:22:08,050
we've networked no no no I mostly I've

00:22:06,610 --> 00:22:16,090
heard about calico

00:22:08,050 --> 00:22:18,179
but maybe yeah actually when you say

00:22:16,090 --> 00:22:21,580
solution you also need to think about

00:22:18,179 --> 00:22:23,290
what you need from solution like what

00:22:21,580 --> 00:22:29,620
kind of functionality you are expecting

00:22:23,290 --> 00:22:32,050
from a solution yeah then it's fine then

00:22:29,620 --> 00:22:36,330
it's fine but there are virtual networks

00:22:32,050 --> 00:22:36,330
that provide more functionality

00:22:54,269 --> 00:22:58,339
sorry I could you repeat that question

00:23:17,630 --> 00:23:23,940
right so such it doesn't turn on every

00:23:21,120 --> 00:23:27,710
slave it just runs on the public slaves

00:23:23,940 --> 00:23:30,960
that you have right and you depending on

00:23:27,710 --> 00:23:33,060
requirement you so each marathon I'll be

00:23:30,960 --> 00:23:37,410
instance is in a chip Roxie instance

00:23:33,060 --> 00:23:39,090
right so in certain cases just just to

00:23:37,410 --> 00:23:41,730
give you an example let's say you want

00:23:39,090 --> 00:23:44,310
to have an H a proxy for your HTTP

00:23:41,730 --> 00:23:47,430
traffic and you want to have a proxy for

00:23:44,310 --> 00:23:49,620
HTTP traffic right in that case you may

00:23:47,430 --> 00:23:51,870
want to run marathon two instances of

00:23:49,620 --> 00:23:54,840
marathon I'll be on to public slave so

00:23:51,870 --> 00:23:56,910
it really depends how what functionality

00:23:54,840 --> 00:23:59,220
you need from a cheap proxy and that's

00:23:56,910 --> 00:24:02,390
define how many instances of marathon lb

00:23:59,220 --> 00:24:02,390
you would like to use

00:24:14,500 --> 00:24:20,750
Minutemen is heavily dependent on the

00:24:17,420 --> 00:24:23,090
missile state so that part so if you are

00:24:20,750 --> 00:24:24,380
saying it can it be run without DC us

00:24:23,090 --> 00:24:26,360
yes it can be

00:24:24,380 --> 00:24:30,760
so it is independent of DCs but it is

00:24:26,360 --> 00:24:30,760
dependent on missiles for its state

00:24:32,800 --> 00:24:44,110
estimate probably by the end of this

00:24:37,430 --> 00:24:44,110
year yeah any other questions

00:24:58,260 --> 00:25:03,300
no no currently no they all have so all

00:25:01,830 --> 00:25:05,660
the records that are created today have

00:25:03,300 --> 00:25:08,660
a default time to limit as five seconds

00:25:05,660 --> 00:25:08,660
but

00:25:15,220 --> 00:25:18,000
right

00:25:18,620 --> 00:25:24,360
okay that's not TTL so that's something

00:25:21,660 --> 00:25:26,370
how fast the system is reacting to a

00:25:24,360 --> 00:25:28,350
change that depends on the polling

00:25:26,370 --> 00:25:32,790
period so the polling period for DNS is

00:25:28,350 --> 00:25:35,220
30 seconds right so if if if let's say

00:25:32,790 --> 00:25:38,280
some tasks came in when we last fold

00:25:35,220 --> 00:25:41,610
right so you can expect that entry to be

00:25:38,280 --> 00:25:44,100
there after 30 seconds right but that is

00:25:41,610 --> 00:25:46,290
not heated TTL is like if you have a

00:25:44,100 --> 00:25:48,360
cache sitting somewhere how long it

00:25:46,290 --> 00:25:50,630
should keep that DNS before refreshing

00:25:48,360 --> 00:25:58,800
it again

00:25:50,630 --> 00:25:59,160
yeah that's sure that that's a good

00:25:58,800 --> 00:26:07,700
point

00:25:59,160 --> 00:26:07,700
yes yeah yeah right

00:26:12,750 --> 00:26:19,740
yes yes so that's good that's a good

00:26:15,630 --> 00:26:21,570
point so I don't know your name but but

00:26:19,740 --> 00:26:24,120
he made a good point so I was telling

00:26:21,570 --> 00:26:26,520
that Spartan is a DNS proxy but Spartan

00:26:24,120 --> 00:26:28,260
is DNS proxy as well as a resolver so

00:26:26,520 --> 00:26:30,960
there is a resolver or sitting inside

00:26:28,260 --> 00:26:32,940
this pattern that do DNS resolution and

00:26:30,960 --> 00:26:37,620
that's how the local DNS queries are

00:26:32,940 --> 00:26:43,760
resolved by Spartan any other question

00:26:37,620 --> 00:26:43,760
anything which was not clear yes

00:26:47,660 --> 00:26:50,660
yeah

00:26:53,080 --> 00:26:59,480
yeah that's a good question

00:26:54,920 --> 00:27:02,480
so in the conflict or tml yeah so the

00:26:59,480 --> 00:27:04,880
question is I said in an overlay that

00:27:02,480 --> 00:27:08,450
master statically divides the subnet

00:27:04,880 --> 00:27:10,730
into agents equally right he is asking

00:27:08,450 --> 00:27:13,250
then how do you support adding new

00:27:10,730 --> 00:27:15,200
agents right because in de cos you can

00:27:13,250 --> 00:27:17,240
add new agents would they get because

00:27:15,200 --> 00:27:19,760
the subnet would have already be divided

00:27:17,240 --> 00:27:22,280
how would you get news a subnet right

00:27:19,760 --> 00:27:25,550
for a new agent will get any subnet the

00:27:22,280 --> 00:27:27,830
thing is the thing that that is part of

00:27:25,550 --> 00:27:30,440
config dot EML is also the prefix of

00:27:27,830 --> 00:27:33,980
each subnet that the agent is supposed

00:27:30,440 --> 00:27:36,290
to get that defines how many slices we

00:27:33,980 --> 00:27:38,450
will be cutting from the global subnet

00:27:36,290 --> 00:27:41,590
so let me I don't know if it is easy for

00:27:38,450 --> 00:27:41,590
me to jump to that slide

00:27:50,660 --> 00:27:57,290
yeah so if you see there is a third

00:27:54,530 --> 00:27:59,420
parameter called prefix which is after

00:27:57,290 --> 00:28:02,300
name subnet so subnet when you say it's

00:27:59,420 --> 00:28:04,040
a whole subnet the subnet for the entire

00:28:02,300 --> 00:28:06,740
Decius cluster but then there is a

00:28:04,040 --> 00:28:09,440
prefix parameter which decide the slices

00:28:06,740 --> 00:28:12,740
out of this subnet so master will just

00:28:09,440 --> 00:28:15,050
take this prefix 26 and will divide the

00:28:12,740 --> 00:28:17,900
entire subnet into slash 26 networks and

00:28:15,050 --> 00:28:20,930
then the operator has to make sure that

00:28:17,900 --> 00:28:23,330
the number of agents numb so this subnet

00:28:20,930 --> 00:28:25,370
should be big enough that it is a it is

00:28:23,330 --> 00:28:27,200
able to be given to all the agents right

00:28:25,370 --> 00:28:31,120
if if number of agent increases then

00:28:27,200 --> 00:28:31,120
these these entries have to be modified

00:28:40,310 --> 00:28:44,800
anything else yes

00:28:49,910 --> 00:28:52,600
right

00:28:57,800 --> 00:29:05,180
right right yeah so good point and and

00:29:03,770 --> 00:29:08,810
and a good eye as well

00:29:05,180 --> 00:29:11,060
so I mentioned that CNI is the one that

00:29:08,810 --> 00:29:14,780
is invoking a scene I plug in and that

00:29:11,060 --> 00:29:17,510
is only possible with UCR but docker on

00:29:14,780 --> 00:29:19,460
the other hand can have this IP per

00:29:17,510 --> 00:29:22,280
container and we support that in DC ways

00:29:19,460 --> 00:29:24,770
so how it is working when docker doesn't

00:29:22,280 --> 00:29:26,810
support CNI right the way we do it is

00:29:24,770 --> 00:29:29,750
custom for this particular case because

00:29:26,810 --> 00:29:31,850
it is supported out of box from DCs so

00:29:29,750 --> 00:29:34,370
we take the path of something called

00:29:31,850 --> 00:29:37,340
user networking docker so we create a

00:29:34,370 --> 00:29:40,250
user network using the cni plugin and

00:29:37,340 --> 00:29:42,350
launch the docker container on top of

00:29:40,250 --> 00:29:44,960
that so docker is able to launch

00:29:42,350 --> 00:29:47,540
containers if you can pre create a

00:29:44,960 --> 00:29:50,810
network for docker right and add into

00:29:47,540 --> 00:29:53,000
the docker network list so we do that in

00:29:50,810 --> 00:29:55,070
the Missis module when when somebody

00:29:53,000 --> 00:29:57,470
configures an oval-8 we go ahead and

00:29:55,070 --> 00:29:59,930
creates appropriate dock and networks

00:29:57,470 --> 00:30:00,860
and attach docker container on that so

00:29:59,930 --> 00:30:03,130
that's how it is working

00:30:00,860 --> 00:30:03,130
yeah

00:30:09,330 --> 00:30:12,200
I think

00:30:12,340 --> 00:30:18,590
okay well thank you

00:30:15,600 --> 00:30:18,590

YouTube URL: https://www.youtube.com/watch?v=MvuE5HzA_qQ


