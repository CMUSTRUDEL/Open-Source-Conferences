Title: Accelerating Spark Workloads in an Apache Mesos Environment with Alluxio
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Accelerating Spark Workloads in an Apache Mesos Environment with Alluxio - Adit Madan, Alluxio, Inc.

Organizations Mesos and Apache Spark together to gain insight from large amounts of data. It is common for Spark to process data stored in disparate public cloud storage, such as Amazon S3, Microsoft Azure Blob Storage, or Google Cloud Storage as well as on-premise data on HDFS, Ceph or ECS. This architecture results in sub-optimal performance as data and compute are not co-located.

Using Alluxio, an open-source memory speed virtual distributed storage system, deployed on Mesos enables connecting any compute framework, such as Apache Spark, to storage systems via a unified namespace. Alluxio enables applications to interact with any data at memory speed. Alluxio can eliminate the pains of ETL and data duplication, and enable new workloads across all data. Adit will discuss the architecture of Mesos, Spark and Alluxio to achieve an optimal architecture for enterprises.
Captions: 
	00:00:00,030 --> 00:00:05,520
alright let's get started so we've got a

00:00:02,760 --> 00:00:07,799
debt Madan from Alexia that is a

00:00:05,520 --> 00:00:10,889
software engineer there and has been

00:00:07,799 --> 00:00:11,550
working on the Alexia integration for

00:00:10,889 --> 00:00:14,219
DCOs

00:00:11,550 --> 00:00:16,199
and mesosphere and Lexi Oh work together

00:00:14,219 --> 00:00:17,490
on this partnership to make this happen

00:00:16,199 --> 00:00:19,260
that's what other that's going to talk

00:00:17,490 --> 00:00:26,010
about and then demo the integration as

00:00:19,260 --> 00:00:28,619
well thanks Robbie for the introduction

00:00:26,010 --> 00:00:31,260
today I'm going to talk about how a

00:00:28,619 --> 00:00:35,000
Luxio can be used to accelerate spark

00:00:31,260 --> 00:00:37,890
workloads on a maize hose cluster I

00:00:35,000 --> 00:00:39,780
start with the brief overview of a Luxio

00:00:37,890 --> 00:00:43,530
for those who are not familiar with it

00:00:39,780 --> 00:00:45,559
I'll move to two use cases of this

00:00:43,530 --> 00:00:47,879
solution being used in production and

00:00:45,559 --> 00:00:49,910
then I will talk about the architecture

00:00:47,879 --> 00:00:53,039
of the solution with some details

00:00:49,910 --> 00:00:57,770
followed by the deployment of a Luxio on

00:00:53,039 --> 00:00:57,770
DCOs and a demo with performance numbers

00:00:58,789 --> 00:01:04,500
to begin with the overview if we look at

00:01:02,309 --> 00:01:07,080
the big data ecosystem of yesterday

00:01:04,500 --> 00:01:09,930
there was only one compute framework

00:01:07,080 --> 00:01:13,380
which was Hadoop MapReduce and it had

00:01:09,930 --> 00:01:15,930
only one storage system which was the

00:01:13,380 --> 00:01:20,549
Hadoop distributed file system the

00:01:15,930 --> 00:01:22,920
problem with this was that compute and

00:01:20,549 --> 00:01:25,500
storage was always co-located and if you

00:01:22,920 --> 00:01:28,619
had to schedule if you had to scale out

00:01:25,500 --> 00:01:34,380
the storage you would also scale out the

00:01:28,619 --> 00:01:37,049
compute resources and vice-versa however

00:01:34,380 --> 00:01:39,060
if you look at the ecosystem today there

00:01:37,049 --> 00:01:41,720
are a lot more compute frameworks for

00:01:39,060 --> 00:01:45,270
both streaming and batch workloads and

00:01:41,720 --> 00:01:48,360
there are an equal number or even more

00:01:45,270 --> 00:01:52,200
storage systems each with its own pros

00:01:48,360 --> 00:01:53,909
and cons and in most cases these compute

00:01:52,200 --> 00:01:57,719
frameworks and storage systems are not

00:01:53,909 --> 00:01:59,939
co-located the part about being not

00:01:57,719 --> 00:02:02,040
co-located gives you the flexibility of

00:01:59,939 --> 00:02:07,170
expanding storage and compute

00:02:02,040 --> 00:02:10,050
independently the issue is that each

00:02:07,170 --> 00:02:12,200
application manages connections to the

00:02:10,050 --> 00:02:13,740
storage systems individually and

00:02:12,200 --> 00:02:15,630
optimising force

00:02:13,740 --> 00:02:18,810
storage access requires application

00:02:15,630 --> 00:02:21,410
level changes and there is no sharing

00:02:18,810 --> 00:02:24,390
between different applications so if an

00:02:21,410 --> 00:02:26,010
up to applications which were sharing

00:02:24,390 --> 00:02:29,310
the same data they would cache it

00:02:26,010 --> 00:02:34,410
individually duplicating the data in

00:02:29,310 --> 00:02:38,640
memory and all and make and making sure

00:02:34,410 --> 00:02:45,540
that when and there would be no there

00:02:38,640 --> 00:02:48,180
would be duplication of data this is

00:02:45,540 --> 00:02:50,340
where a Luxio comes in a Luxio provides

00:02:48,180 --> 00:02:52,020
the storage abstraction for all of the

00:02:50,340 --> 00:02:55,620
different storage systems that you have

00:02:52,020 --> 00:02:57,900
be it on premise or in the cloud across

00:02:55,620 --> 00:03:00,420
different file systems as well as object

00:02:57,900 --> 00:03:03,210
storage systems and you can access the

00:03:00,420 --> 00:03:06,750
data using typically using a file system

00:03:03,210 --> 00:03:08,580
API and this can be done without any

00:03:06,750 --> 00:03:11,670
code changes at the application level

00:03:08,580 --> 00:03:14,580
and you can continue to use your spark

00:03:11,670 --> 00:03:17,490
applications presto flink application

00:03:14,580 --> 00:03:20,250
name it and without any application

00:03:17,490 --> 00:03:23,700
changes the highest performance is

00:03:20,250 --> 00:03:25,740
guaranteed by storing all the managing

00:03:23,700 --> 00:03:27,360
all of the data in memory and providing

00:03:25,740 --> 00:03:33,050
shared access across the different

00:03:27,360 --> 00:03:34,320
applications that you have to summarize

00:03:33,050 --> 00:03:37,800
up

00:03:34,320 --> 00:03:41,040
alexia has been used for a variety of

00:03:37,800 --> 00:03:43,050
applications ranging from big data to

00:03:41,040 --> 00:03:53,060
deep learning as I'll talk about in the

00:03:43,050 --> 00:03:56,340
next few slides I would to summarize

00:03:53,060 --> 00:03:59,520
alexia unifies all of the data in your

00:03:56,340 --> 00:04:02,430
cluster it provides high performance by

00:03:59,520 --> 00:04:04,710
in-memory data management it provides

00:04:02,430 --> 00:04:09,320
cost saving and you have no vendor

00:04:04,710 --> 00:04:11,820
lock-in as from an end-user perspective

00:04:09,320 --> 00:04:14,420
migrating data across storage systems is

00:04:11,820 --> 00:04:14,420
transparent

00:04:18,410 --> 00:04:23,000
I would also like to mention that a

00:04:20,090 --> 00:04:25,370
Luxio is one of the fastest-growing Big

00:04:23,000 --> 00:04:28,430
Data open source projects the graph that

00:04:25,370 --> 00:04:32,500
we are looking at is the number of

00:04:28,430 --> 00:04:36,650
contributors for different popular

00:04:32,500 --> 00:04:38,690
frameworks in their early months the get

00:04:36,650 --> 00:04:41,450
left the y-axis is the number of

00:04:38,690 --> 00:04:45,490
contributors and as you can see a Luxio

00:04:41,450 --> 00:04:45,490
is doing pretty well

00:04:47,830 --> 00:04:58,150
moving on to use cases of the given

00:04:52,160 --> 00:05:00,500
solution in production by existing today

00:04:58,150 --> 00:05:04,580
now the first one I'm going to talk

00:05:00,500 --> 00:05:07,340
about is tuna tuna is China's biggest

00:05:04,580 --> 00:05:10,630
travel search portal they were using

00:05:07,340 --> 00:05:15,470
spark and company and flink in

00:05:10,630 --> 00:05:19,610
combination to tackle an incoming stream

00:05:15,470 --> 00:05:22,910
of click data from their website the

00:05:19,610 --> 00:05:26,990
storage systems that they used for HDFS

00:05:22,910 --> 00:05:29,750
and SEF they used a Luxio as a storage

00:05:26,990 --> 00:05:32,750
abstraction layer and also as a

00:05:29,750 --> 00:05:35,720
mechanism to share data from a pipeline

00:05:32,750 --> 00:05:40,190
of data processing flowing between spark

00:05:35,720 --> 00:05:42,350
and link on average this for the

00:05:40,190 --> 00:05:45,410
workloads they saw about 15 X

00:05:42,350 --> 00:05:48,620
performance improvements and on a peak

00:05:45,410 --> 00:05:52,730
workload they see up to 300 X

00:05:48,620 --> 00:05:55,490
performance improvements the link that I

00:05:52,730 --> 00:05:59,200
have on screen gives you more

00:05:55,490 --> 00:05:59,200
information about the solution

00:06:03,940 --> 00:06:10,190
the second use case that I would like to

00:06:07,520 --> 00:06:16,030
talk about is guardant Health Garden

00:06:10,190 --> 00:06:20,270
health is genomics data processing does

00:06:16,030 --> 00:06:24,050
some data analysis on genomic data for

00:06:20,270 --> 00:06:26,510
cancer patients they used part to

00:06:24,050 --> 00:06:28,310
process data across different storage

00:06:26,510 --> 00:06:33,820
systems that they have both on-premise

00:06:28,310 --> 00:06:37,280
and in the cloud and they scaled up to

00:06:33,820 --> 00:06:40,940
exabytes of data they moved to a

00:06:37,280 --> 00:06:43,610
solution using a Luxio and mesos and an

00:06:40,940 --> 00:06:46,490
object store called Minya Minya gave

00:06:43,610 --> 00:06:49,450
them a minimum was used for coal data

00:06:46,490 --> 00:06:53,150
with backups to Amazon s3

00:06:49,450 --> 00:06:56,150
they also saw orders of magnitude of

00:06:53,150 --> 00:07:00,080
performance gains when they used a Luxio

00:06:56,150 --> 00:07:02,000
in combination with mesos the link that

00:07:00,080 --> 00:07:05,480
I have on the screen has more

00:07:02,000 --> 00:07:08,540
information in the words of our friend

00:07:05,480 --> 00:07:11,150
that gardened health the benefits that

00:07:08,540 --> 00:07:14,540
they saw from alexia with performance

00:07:11,150 --> 00:07:16,990
could literally be a lifesaver for their

00:07:14,540 --> 00:07:16,990
patients

00:07:23,130 --> 00:07:27,870
okay so let's talk about the

00:07:25,380 --> 00:07:35,580
architecture of the solution and some of

00:07:27,870 --> 00:07:37,800
the details now let's say we have two

00:07:35,580 --> 00:07:41,370
spark applications running on a mesas

00:07:37,800 --> 00:07:43,650
cluster both spark applications would

00:07:41,370 --> 00:07:48,000
have their own context with their own

00:07:43,650 --> 00:07:49,980
caching of any data that they access in

00:07:48,000 --> 00:07:52,650
this picture we have these two

00:07:49,980 --> 00:07:56,130
applications accessing data from HDFS

00:07:52,650 --> 00:07:59,310
and Amazon s3 or it could be any other

00:07:56,130 --> 00:08:01,860
storage system that so whenever an

00:07:59,310 --> 00:08:06,930
application accesses the data they would

00:08:01,860 --> 00:08:09,810
maintain its own copy so in this diagram

00:08:06,930 --> 00:08:12,990
we see that the two applications are

00:08:09,810 --> 00:08:15,450
have their own copy of blocks one and

00:08:12,990 --> 00:08:18,260
three and there there is no sharing of

00:08:15,450 --> 00:08:20,970
the data between these two applications

00:08:18,260 --> 00:08:24,180
the other thing that we see in this

00:08:20,970 --> 00:08:27,150
picture is that the lifetime of the

00:08:24,180 --> 00:08:35,010
cached data is tied to the lifetime of

00:08:27,150 --> 00:08:38,940
the application itself now if we look at

00:08:35,010 --> 00:08:42,330
the same picture with the Luxio both

00:08:38,940 --> 00:08:46,230
applications can talk to a Luxio without

00:08:42,330 --> 00:08:49,230
any code changes any data access from

00:08:46,230 --> 00:08:53,120
slow remote storage such as HDFS and

00:08:49,230 --> 00:08:56,160
Amazon s3 would be cached in a Luxio

00:08:53,120 --> 00:08:57,750
typically a Luxio is close to the

00:08:56,160 --> 00:09:01,500
compute cluster which is the spark

00:08:57,750 --> 00:09:05,010
cluster in this case and the HDFS or

00:09:01,500 --> 00:09:07,710
storage system could be could be remote

00:09:05,010 --> 00:09:09,780
it could be co-located a Luxio gives you

00:09:07,710 --> 00:09:13,050
the flexibility of maintaining

00:09:09,780 --> 00:09:15,120
performance even even regardless of the

00:09:13,050 --> 00:09:19,320
location of the story that you're

00:09:15,120 --> 00:09:22,290
accessing so in this case if the same

00:09:19,320 --> 00:09:25,710
two applications access blocks one and

00:09:22,290 --> 00:09:30,050
three you will see that there's only one

00:09:25,710 --> 00:09:32,959
copy of these data in in a Luxio

00:09:30,050 --> 00:09:35,680
this means that the memory which is

00:09:32,959 --> 00:09:37,910
expensive in your compute cluster is

00:09:35,680 --> 00:09:40,190
being well utilized there's no

00:09:37,910 --> 00:09:44,450
duplication of data and this will in

00:09:40,190 --> 00:09:51,890
turn lead to performance gains when the

00:09:44,450 --> 00:09:57,140
memory is over utilized like I mentioned

00:09:51,890 --> 00:10:00,860
before if the storage and the compute in

00:09:57,140 --> 00:10:05,990
case of vanilla spark are tightly

00:10:00,860 --> 00:10:10,910
associated so in case there is and they

00:10:05,990 --> 00:10:14,600
lie in the same JVM so let's look at

00:10:10,910 --> 00:10:17,209
what happen when the application goes

00:10:14,600 --> 00:10:18,860
away and particularly when the spark

00:10:17,209 --> 00:10:22,370
context that is running the application

00:10:18,860 --> 00:10:27,410
is no longer accessible which could be

00:10:22,370 --> 00:10:33,950
when you have a crash or you just decide

00:10:27,410 --> 00:10:37,160
to close it now once the application

00:10:33,950 --> 00:10:38,600
goes away the storage that is associated

00:10:37,160 --> 00:10:42,620
with that application is also

00:10:38,600 --> 00:10:45,079
inaccessible which means that when the

00:10:42,620 --> 00:10:47,360
same blocks are accessed again they will

00:10:45,079 --> 00:10:50,050
have to be fetched from the remote

00:10:47,360 --> 00:10:54,620
storage cluster that you have and this

00:10:50,050 --> 00:10:58,120
access is typically bound by network or

00:10:54,620 --> 00:10:58,120
slow disk IO

00:11:05,020 --> 00:11:13,270
the same picture spark accesses data

00:11:09,070 --> 00:11:16,270
from a Luxio in this case if you have a

00:11:13,270 --> 00:11:19,029
crash the data is still accessible in

00:11:16,270 --> 00:11:21,850
Alexia and Alexia would manage this data

00:11:19,029 --> 00:11:25,990
in memory or across different tiers of

00:11:21,850 --> 00:11:28,029
storage on the compute cluster which

00:11:25,990 --> 00:11:32,110
means that once the same data is

00:11:28,029 --> 00:11:34,480
accessed by the application that died or

00:11:32,110 --> 00:11:40,180
some other application you still have

00:11:34,480 --> 00:11:42,670
that data in memory to summarize the

00:11:40,180 --> 00:11:44,830
lifetime of the data is disassociated

00:11:42,670 --> 00:11:48,180
from the lifetime of the end application

00:11:44,830 --> 00:11:51,850
which is using it this provides both

00:11:48,180 --> 00:11:53,649
this provides performance when the data

00:11:51,850 --> 00:11:57,310
is being shared across different lab

00:11:53,649 --> 00:12:01,290
applications and also during scenarios

00:11:57,310 --> 00:12:01,290
in which we have failures

00:12:07,290 --> 00:12:14,800
now Alexia has been integrated with DCOs

00:12:12,450 --> 00:12:19,180
Alexia is available as one of the

00:12:14,800 --> 00:12:23,080
packages in the DC US universe you could

00:12:19,180 --> 00:12:28,060
use a Luxio you could use these heroes

00:12:23,080 --> 00:12:35,980
to deploy and manage Alexia energy cos

00:12:28,060 --> 00:12:39,400
cluster now on a DC OS cluster Alexia

00:12:35,980 --> 00:12:42,250
brings a unified view of all of the data

00:12:39,400 --> 00:12:45,089
that you have be it on a DC OS cluster

00:12:42,250 --> 00:12:47,620
or from outside it brings

00:12:45,089 --> 00:12:52,360
high-performance and predictable SLA for

00:12:47,620 --> 00:12:56,130
your workloads DCOs makes provisioning

00:12:52,360 --> 00:12:59,050
or Velux your easy it ought it manages

00:12:56,130 --> 00:13:04,300
elasticity both scaling up and scaling

00:12:59,050 --> 00:13:06,940
out and together the solution enables

00:13:04,300 --> 00:13:08,800
faster analytics with SPARC and other

00:13:06,940 --> 00:13:13,029
workflow and other frameworks which are

00:13:08,800 --> 00:13:15,279
running on the DC OS cluster it also

00:13:13,029 --> 00:13:19,380
enables you to access data from

00:13:15,279 --> 00:13:24,190
disparate disparate storage systems both

00:13:19,380 --> 00:13:28,510
for example HDFS and s3 and in the demo

00:13:24,190 --> 00:13:33,400
that I'm going to show you next we will

00:13:28,510 --> 00:13:36,490
see HDFS being mounted as the root Maya

00:13:33,400 --> 00:13:38,589
root on destroyed system in Alexia what

00:13:36,490 --> 00:13:41,860
I mean by an under storage system is

00:13:38,589 --> 00:13:46,000
just one of the storage systems that is

00:13:41,860 --> 00:13:48,690
backing Alexia and you will be able to

00:13:46,000 --> 00:13:53,250
access Amazon s3 in the same namespace

00:13:48,690 --> 00:13:53,250
which is a file system like namespace

00:14:02,399 --> 00:14:10,550
the demo is based on an Amazon ec2

00:14:06,990 --> 00:14:15,240
cluster will have spark and a Luxio

00:14:10,550 --> 00:14:19,199
running on mesos on a D&D cos we will

00:14:15,240 --> 00:14:22,439
access data from Amazon s3 by running a

00:14:19,199 --> 00:14:24,209
simple spark count application and we

00:14:22,439 --> 00:14:32,129
will see some of the performance numbers

00:14:24,209 --> 00:14:36,360
of the given solution the versions used

00:14:32,129 --> 00:14:40,620
in the demo we have will we have used

00:14:36,360 --> 00:14:43,620
the last release of a Luxio Luxio 1.5 we

00:14:40,620 --> 00:14:47,100
use the previous release of d cos 194

00:14:43,620 --> 00:14:53,509
and with spark to or two or two running

00:14:47,100 --> 00:14:53,509
on amazon m3 dot extra large instances

00:15:10,899 --> 00:15:23,929
for the demo we have a pre deployed DCOs

00:15:14,209 --> 00:15:26,419
cluster it will access data from HDFS we

00:15:23,929 --> 00:15:30,339
see that in HDFS we have a single file

00:15:26,419 --> 00:15:33,470
called licensed txt which will appear in

00:15:30,339 --> 00:15:41,539
the elecciÃ³n namespace once we mount

00:15:33,470 --> 00:15:44,779
this HDFS location into a Luxio we will

00:15:41,539 --> 00:15:48,799
also access data from an s3 cluster in

00:15:44,779 --> 00:15:51,979
s3 we have two files readme and a sample

00:15:48,799 --> 00:15:55,069
one gigabyte file the one gigabyte file

00:15:51,979 --> 00:16:01,220
is what will be used for the performance

00:15:55,069 --> 00:16:03,799
numbers for a Luxio on DCOs we have a

00:16:01,220 --> 00:16:06,259
docker registry setup the docker

00:16:03,799 --> 00:16:08,689
registry will be used to hold the a

00:16:06,259 --> 00:16:12,109
Luxio client docker image which is built

00:16:08,689 --> 00:16:15,319
as part of the deployment process and

00:16:12,109 --> 00:16:18,289
the client docker image has the Luxio

00:16:15,319 --> 00:16:22,789
CLI in it which will be used to access

00:16:18,289 --> 00:16:25,129
the alexia filesystem in addition during

00:16:22,789 --> 00:16:27,799
the deployment process we build a docker

00:16:25,129 --> 00:16:30,169
image for running SPARC on top of Luxio

00:16:27,799 --> 00:16:32,600
the docker image will have this park

00:16:30,169 --> 00:16:35,829
shell and is also the docker image used

00:16:32,600 --> 00:16:35,829
for SPARC executors

00:16:41,630 --> 00:16:50,250
to install a Luxio on DCOs locate the

00:16:45,600 --> 00:16:53,670
package in the universe the first thing

00:16:50,250 --> 00:16:59,640
that you need to do is obtain a license

00:16:53,670 --> 00:17:02,060
from Alexia and base64 encode the

00:16:59,640 --> 00:17:02,060
license

00:17:14,510 --> 00:17:20,490
don't try to use this license this

00:17:16,949 --> 00:17:24,569
license no longer works so the next

00:17:20,490 --> 00:17:27,990
thing that we do is have HDFS as the

00:17:24,569 --> 00:17:30,390
route on under storage for a Luxio like

00:17:27,990 --> 00:17:34,020
I mentioned before any rights that you

00:17:30,390 --> 00:17:37,530
do to a Luxio or any reads that you do

00:17:34,020 --> 00:17:41,660
from HDFS will be available at the root

00:17:37,530 --> 00:17:41,660
of the Luxio filesystem namespace

00:17:48,430 --> 00:17:55,560
now we just specified the under storage

00:17:52,300 --> 00:17:59,710
system for Alexia and the license and

00:17:55,560 --> 00:18:09,430
that's all that is needed to deploy a

00:17:59,710 --> 00:18:11,290
Luxio with the default configuration now

00:18:09,430 --> 00:18:15,300
you can monitor the progress of the

00:18:11,290 --> 00:18:18,400
installation in the services tab in DCOs

00:18:15,300 --> 00:18:21,970
after waiting for a few minutes you'll

00:18:18,400 --> 00:18:24,970
see that all of the processes for Alexia

00:18:21,970 --> 00:18:28,360
come up which includes the Luxio master

00:18:24,970 --> 00:18:30,850
process and Alexia workers for the Luxio

00:18:28,360 --> 00:18:36,880
distributed file system as well as other

00:18:30,850 --> 00:18:39,580
auxiliary processes once the

00:18:36,880 --> 00:18:42,130
installation finishes you'll have access

00:18:39,580 --> 00:18:47,340
to the Alexia client image that I

00:18:42,130 --> 00:18:51,640
mentioned before so we just logged into

00:18:47,340 --> 00:18:56,260
the master node in DCOs to access the

00:18:51,640 --> 00:19:00,370
CLI and once we're in the master node we

00:18:56,260 --> 00:19:08,170
can pull the the client image that I

00:19:00,370 --> 00:19:10,750
mentioned earlier now the client image

00:19:08,170 --> 00:19:13,810
that was built as part of the deployment

00:19:10,750 --> 00:19:16,090
has all of the configuration that is

00:19:13,810 --> 00:19:21,700
required to connect a client to Luxio

00:19:16,090 --> 00:19:25,750
set up already so once you have access

00:19:21,700 --> 00:19:29,110
to the image you can run the Alexia CLI

00:19:25,750 --> 00:19:33,970
which is little hidden on top which has

00:19:29,110 --> 00:19:36,580
been a Luxio FSL s so the file that we

00:19:33,970 --> 00:19:39,790
see over here license dot txt is being

00:19:36,580 --> 00:19:42,820
fetched from HDFS which we configured as

00:19:39,790 --> 00:19:44,860
the storage system backing Alexia the

00:19:42,820 --> 00:19:48,640
not in memory annotation over there

00:19:44,860 --> 00:19:52,660
specified that only the metadata from

00:19:48,640 --> 00:19:55,240
HDFS has been fetched into Alexia the

00:19:52,660 --> 00:19:56,480
data itself will be fetched into Alexia

00:19:55,240 --> 00:20:00,669
once we access

00:19:56,480 --> 00:20:00,669
except the the file

00:20:16,140 --> 00:20:22,740
so the next thing that we do is we mount

00:20:18,299 --> 00:20:24,990
an s3 bucket into a Luxio we specify the

00:20:22,740 --> 00:20:28,620
credentials that are required to access

00:20:24,990 --> 00:20:33,299
the s3 bucket in this case we mount s3

00:20:28,620 --> 00:20:37,170
at the location / s3 a into the Luxio

00:20:33,299 --> 00:20:40,080
file system and DCOs demo is the name of

00:20:37,170 --> 00:20:43,590
the bucket in s3 so you'll be able to

00:20:40,080 --> 00:20:45,840
access both s3 and HDFS in a unified

00:20:43,590 --> 00:20:49,350
namespace and you can easily migrate

00:20:45,840 --> 00:20:52,950
your data between the two two storage

00:20:49,350 --> 00:20:56,070
systems the end application only talks

00:20:52,950 --> 00:20:59,190
to a Luxio and it is in it is unaware of

00:20:56,070 --> 00:21:01,440
where the data is being accessed from so

00:20:59,190 --> 00:21:04,020
you just specify an alert Co part and

00:21:01,440 --> 00:21:08,400
that's all that is needed for your

00:21:04,020 --> 00:21:11,090
application to talk to one or more under

00:21:08,400 --> 00:21:11,090
storage systems

00:21:17,490 --> 00:21:25,420
as you can see s3 was mounted at that

00:21:22,210 --> 00:21:31,120
location and once we list the contents

00:21:25,420 --> 00:21:33,730
at the location s3 a in Alexia will see

00:21:31,120 --> 00:21:36,880
the same two files that we had in our

00:21:33,730 --> 00:21:39,310
Amazon bucket readme and sample one

00:21:36,880 --> 00:21:42,100
gigabyte sample one gigabyte like I

00:21:39,310 --> 00:21:45,040
mentioned before is the file that we'll

00:21:42,100 --> 00:21:48,580
use for the performance numbers in the

00:21:45,040 --> 00:21:50,050
count spark job that I'll run like I

00:21:48,580 --> 00:21:52,690
mentioned before the not in memory

00:21:50,050 --> 00:21:54,610
annotation only means that the metadata

00:21:52,690 --> 00:21:56,470
has been fetched from the storage

00:21:54,610 --> 00:22:12,100
systems back in galaxy and not the data

00:21:56,470 --> 00:22:15,210
yet so in the terminal that I have

00:22:12,100 --> 00:22:18,370
opened below I'll run a spark

00:22:15,210 --> 00:22:21,190
application which is the count job I log

00:22:18,370 --> 00:22:25,090
into the spark into the DCOs master node

00:22:21,190 --> 00:22:33,790
and pull the spark docker image that was

00:22:25,090 --> 00:22:35,890
built as part of the deployment now now

00:22:33,790 --> 00:22:38,910
that we have the we have pulled the

00:22:35,890 --> 00:22:42,070
docker image we'll start a spark shell

00:22:38,910 --> 00:22:44,940
running this pod job on mesos on the

00:22:42,070 --> 00:22:44,940
DCOs cluster

00:22:51,270 --> 00:22:58,960
once all of the executors for the spark

00:22:55,870 --> 00:23:02,440
job have come up we'll the first thing

00:22:58,960 --> 00:23:04,840
we will do is set the log level in spark

00:23:02,440 --> 00:23:07,440
to info to monitor the timing

00:23:04,840 --> 00:23:07,440
information

00:23:20,240 --> 00:23:25,910
as you can see the spark application

00:23:23,030 --> 00:23:28,550
talking to a lot CEO you specify the

00:23:25,910 --> 00:23:31,130
scheme alexia followed by the master

00:23:28,550 --> 00:23:33,040
node of Alexia and then you specify the

00:23:31,130 --> 00:23:35,990
path of the file that you want to access

00:23:33,040 --> 00:23:39,890
like I mentioned before the sample 1g

00:23:35,990 --> 00:23:42,170
file is not in a Luxio memory yet but

00:23:39,890 --> 00:23:44,929
when we run the spark application and

00:23:42,170 --> 00:23:47,300
the data is being accessed from Alexio

00:23:44,929 --> 00:23:50,630
that's the time when you pull the data

00:23:47,300 --> 00:23:53,780
from s3 into Alexia and all repeated

00:23:50,630 --> 00:23:56,150
accesses of that data will benefit from

00:23:53,780 --> 00:23:59,410
the acceleration that is provided by

00:23:56,150 --> 00:23:59,410
managing data in memory

00:24:11,000 --> 00:24:16,610
so we run the count job which counts as

00:24:14,659 --> 00:24:20,659
a number of lines in the one gigabyte

00:24:16,610 --> 00:24:24,200
file we noticed that the locality level

00:24:20,659 --> 00:24:27,590
was processed local for this job what

00:24:24,200 --> 00:24:30,950
this means is that Alexia does not have

00:24:27,590 --> 00:24:33,830
the information in memory and there is

00:24:30,950 --> 00:24:40,970
no locality from spark spots point of

00:24:33,830 --> 00:24:44,120
view as you can see this job took about

00:24:40,970 --> 00:24:48,460
31 seconds to finish and this is when

00:24:44,120 --> 00:24:48,460
the data was actually pulled into Alexia

00:24:51,880 --> 00:24:58,070
now if you exit the spark shell and

00:24:55,750 --> 00:25:00,230
repeat the same job let's see what

00:24:58,070 --> 00:25:02,570
happens in the top shell I just showed

00:25:00,230 --> 00:25:05,750
you that the annotation for the one

00:25:02,570 --> 00:25:08,360
gigabyte file which it changed from not

00:25:05,750 --> 00:25:10,970
in memory to in memory which means that

00:25:08,360 --> 00:25:13,520
now Alexia has the blocks for this file

00:25:10,970 --> 00:25:16,460
and in this case in the default block

00:25:13,520 --> 00:25:26,630
size was 500 megabytes we have two

00:25:16,460 --> 00:25:29,000
blocks for the file in Alexia so we

00:25:26,630 --> 00:25:34,370
restart the spark shell and redo the

00:25:29,000 --> 00:25:44,510
same count job set the log level to info

00:25:34,370 --> 00:25:46,840
again reevaluate the file and it should

00:25:44,510 --> 00:25:46,840
account

00:25:48,559 --> 00:25:56,990
so this time we can see that the

00:25:53,629 --> 00:26:00,799
locality level was node local which

00:25:56,990 --> 00:26:03,919
means that the executives the tasks had

00:26:00,799 --> 00:26:07,639
complete locality and that's why we saw

00:26:03,919 --> 00:26:10,870
the performance improved from 31 seconds

00:26:07,639 --> 00:26:10,870
to 3.6 seconds

00:26:27,120 --> 00:26:32,710
so the results for the performance

00:26:29,560 --> 00:26:35,320
experiment that we saw in the demo the

00:26:32,710 --> 00:26:38,050
lighter blue bar if the initial count

00:26:35,320 --> 00:26:40,420
that we did and the darker blue bar is

00:26:38,050 --> 00:26:44,470
when you exited the spark shell and we

00:26:40,420 --> 00:26:48,730
redid the same experiment again so if in

00:26:44,470 --> 00:26:51,450
case of a Luxio you notice that we got

00:26:48,730 --> 00:26:54,070
an 8x improvement for repeated accesses

00:26:51,450 --> 00:26:56,800
independent of the spark shell spark

00:26:54,070 --> 00:27:01,150
context that you're using for the data

00:26:56,800 --> 00:27:05,530
access however if you access the data

00:27:01,150 --> 00:27:07,780
directly from s3 the initial count and

00:27:05,530 --> 00:27:10,570
any repeated counts after the spark

00:27:07,780 --> 00:27:13,930
context has been restarted they have the

00:27:10,570 --> 00:27:15,850
same performance so the gist of this is

00:27:13,930 --> 00:27:19,540
that when you have repeated four

00:27:15,850 --> 00:27:21,940
accesses or when you have data being

00:27:19,540 --> 00:27:23,620
shared across different applications you

00:27:21,940 --> 00:27:34,090
will see a tremendous performance gains

00:27:23,620 --> 00:27:37,480
from alexia to conclude a Luxio is easy

00:27:34,090 --> 00:27:39,370
to use in a May source environment DCOs

00:27:37,480 --> 00:27:42,430
brings you ease of deployment and

00:27:39,370 --> 00:27:44,460
management of the application with the

00:27:42,430 --> 00:27:49,750
luckier you get predictable performance

00:27:44,460 --> 00:27:53,890
with a controlled caching system and you

00:27:49,750 --> 00:27:57,730
get improved performance as well Alex

00:27:53,890 --> 00:27:59,320
you is Alex you easily connect to the

00:27:57,730 --> 00:28:06,490
different storage systems that you may

00:27:59,320 --> 00:28:09,430
have be a HDFS s3 SEF you name it there

00:28:06,490 --> 00:28:12,820
are a number of connections that you can

00:28:09,430 --> 00:28:19,240
do from a Luxio to the storage system so

00:28:12,820 --> 00:28:21,460
go here's Alexei oh it's awesome so

00:28:19,240 --> 00:28:24,670
that's it from my side thank you for

00:28:21,460 --> 00:28:27,010
listening to the talk you can reach me

00:28:24,670 --> 00:28:28,990
at other that Alexio comm for any

00:28:27,010 --> 00:28:32,020
further questions that you may have

00:28:28,990 --> 00:28:35,520
after after the conference and I'll be

00:28:32,020 --> 00:28:35,520
happy to take any questions down

00:28:40,350 --> 00:28:46,210
so the question was that how long is the

00:28:43,960 --> 00:28:48,880
caching available so you have do have

00:28:46,210 --> 00:28:52,360
control over how long the data stays in

00:28:48,880 --> 00:29:15,370
Alexia say by default there will be no

00:28:52,360 --> 00:29:17,080
eviction until the memory is full so so

00:29:15,370 --> 00:29:19,930
the question was how do you handle write

00:29:17,080 --> 00:29:22,600
workloads for writes we have different

00:29:19,930 --> 00:29:25,390
policies for writes so the default

00:29:22,600 --> 00:29:27,490
policy we have a policy called cache

00:29:25,390 --> 00:29:29,260
through which means that writes will be

00:29:27,490 --> 00:29:31,630
cached in Luxio memory as well as

00:29:29,260 --> 00:29:34,480
propagated to the storage system which

00:29:31,630 --> 00:29:36,280
is backing the path you can have multi

00:29:34,480 --> 00:29:38,950
you can have different options such as

00:29:36,280 --> 00:29:43,030
something a feature called fast durable

00:29:38,950 --> 00:29:46,000
writes which means that we will make

00:29:43,030 --> 00:29:48,880
sure that the writes go into a Luxio we

00:29:46,000 --> 00:29:51,640
replicate the data in a Luxio and we a

00:29:48,880 --> 00:29:53,560
synchronously propagate update down to

00:29:51,640 --> 00:29:55,720
the storage systems which means that

00:29:53,560 --> 00:29:57,370
when you do a right to a Luxio your

00:29:55,720 --> 00:30:00,190
guarantee that your data will not be

00:29:57,370 --> 00:30:02,500
lost and we will eventually persist the

00:30:00,190 --> 00:30:04,980
data to the backing storage system did

00:30:02,500 --> 00:30:04,980
that answer your question

00:30:10,050 --> 00:30:30,570
I believe me only after unfortunately

00:30:29,100 --> 00:30:32,550
I'm not in a good position to answer

00:30:30,570 --> 00:30:35,750
that question since I'm not sure I

00:30:32,550 --> 00:30:38,340
completely understand it but we can talk

00:30:35,750 --> 00:30:51,870
afterwards to make for a longer

00:30:38,340 --> 00:30:55,910
conversation any more questions okay

00:30:51,870 --> 00:30:55,910

YouTube URL: https://www.youtube.com/watch?v=3xfy0XHxhJc


