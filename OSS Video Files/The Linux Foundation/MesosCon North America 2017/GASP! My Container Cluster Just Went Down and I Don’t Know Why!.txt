Title: GASP! My Container Cluster Just Went Down and I Don’t Know Why!
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	GASP! My Container Cluster Just Went Down and I Don’t Know Why! - Kamalakannan Muralidharan, PayPal & Ranga Rajagopalan, Avi Networks

Traffic management in a container environment, especially for microservices involves more than just load balancing. Load balancing traffic from client to core service (north-south) along with inter-service and inter-cluster interactions (east-west) is key.

There are different ways of load balancing methods in a container architecture:
North-south (Fronting LB)
East-West (Client side)

The primary challenges with load balancing in a dynamic environment (like cluster manager environment) are the consequences of a container outage. 
How do I do IP per POD vs Port Mapping?
How can I enforce dynamic registration as part of the container lifecycle?

About 

Kamalakannan Muralidharan
PayPal
Head of Product Marketing
Kamalakannan Muralidharan, Sr. Member of Technical Staff, PayPal
I have 13 years of experience in Software Engineering. We have been using Mesos and its framework in our company for the past 3 years. I have presented about OSGi in JavaOne and also about Docker and distributed cluster managers in internal talks. I am currently building a massive distributed PaaS using Mesos for our hybrid cloud strategy.

Ranga Rajagopalan
Avi Networks
CTO
Ranga Rajagopalan, CTO, Avi Networks
Over the last 15 years prior to co-founding Avi Networks, Ranga has been an architect and developer of several high-performance distributed operating systems as well as networking and storage data center products. Before his current role as CTO, he was the Senior Director at Cisco, responsible for platform software on the Nexus 7000 product line.
Captions: 
	00:00:00,030 --> 00:00:10,530
okay hello everyone good afternoon so

00:00:03,870 --> 00:00:12,630
today's talk is about how do we what are

00:00:10,530 --> 00:00:14,040
the patterns we see in running micro

00:00:12,630 --> 00:00:15,809
services in a cluster management

00:00:14,040 --> 00:00:17,730
environment and what are the load

00:00:15,809 --> 00:00:21,449
balancer techniques which we are seeing

00:00:17,730 --> 00:00:23,730
and what's our during our learning what

00:00:21,449 --> 00:00:24,960
is the experience and the insight which

00:00:23,730 --> 00:00:31,289
we got that's what we are going to share

00:00:24,960 --> 00:00:32,759
and so the the agenda is basically go

00:00:31,289 --> 00:00:34,500
with what is the use case we are trying

00:00:32,759 --> 00:00:36,600
to solve and then what are the

00:00:34,500 --> 00:00:38,820
challenges we face and how we are trying

00:00:36,600 --> 00:00:42,149
to solve it and what's the architecture

00:00:38,820 --> 00:00:44,760
we are using it and then some of the

00:00:42,149 --> 00:00:46,079
conclusions we came from it and if you

00:00:44,760 --> 00:00:49,050
have any questions we can cover with

00:00:46,079 --> 00:00:52,110
that so this is a talk jointly between

00:00:49,050 --> 00:00:54,360
me center action arm khamel am part of

00:00:52,110 --> 00:00:56,460
the platform as a service team in PayPal

00:00:54,360 --> 00:00:58,469
and we are working towards building the

00:00:56,460 --> 00:01:01,920
next generation deployment platform

00:00:58,469 --> 00:01:05,220
based on me source and along with me is

00:01:01,920 --> 00:01:07,500
Rangga who is a CTO from hobby networks

00:01:05,220 --> 00:01:12,049
and he will be talking about a beast

00:01:07,500 --> 00:01:12,049
load balancer and how it is architected

00:01:12,740 --> 00:01:18,240
so when you typically look at any

00:01:15,750 --> 00:01:21,000
applications there are the way we

00:01:18,240 --> 00:01:22,740
classify it as an application which

00:01:21,000 --> 00:01:25,380
needs an endpoint which means which is

00:01:22,740 --> 00:01:27,030
user facing or services which are

00:01:25,380 --> 00:01:28,170
accessed by other application which

00:01:27,030 --> 00:01:30,840
means they need to come through a

00:01:28,170 --> 00:01:33,150
endpoint endpoint could be an IP colon

00:01:30,840 --> 00:01:35,310
port or it could be a DNS which are

00:01:33,150 --> 00:01:37,650
accessed through any protocol right that

00:01:35,310 --> 00:01:40,170
means that you enter through a single

00:01:37,650 --> 00:01:42,180
endpoint but then the traffic gets load

00:01:40,170 --> 00:01:44,159
balance between multiple the other type

00:01:42,180 --> 00:01:47,100
of application doesn't have an entry

00:01:44,159 --> 00:01:49,500
point mostly like demons batches which

00:01:47,100 --> 00:01:51,750
runs in your VM but doesn't need an

00:01:49,500 --> 00:01:53,820
entry point it mostly a synchronous it

00:01:51,750 --> 00:01:55,770
takes take a data from behind processes

00:01:53,820 --> 00:01:57,719
and do something with that so today's

00:01:55,770 --> 00:01:59,520
talk mostly is about the first section

00:01:57,719 --> 00:02:01,380
which is about application which needs

00:01:59,520 --> 00:02:03,450
an access point or end point through a

00:02:01,380 --> 00:02:06,780
load balancer and what are the

00:02:03,450 --> 00:02:08,550
approaches we are looking at so before

00:02:06,780 --> 00:02:10,890
going into it I'll give a context of

00:02:08,550 --> 00:02:13,709
from a PayPal use case what we are doing

00:02:10,890 --> 00:02:16,409
so people is running a

00:02:13,709 --> 00:02:18,629
our own private data center and we have

00:02:16,409 --> 00:02:21,359
our own proprietary pass for deploying

00:02:18,629 --> 00:02:23,849
PayPal applications so we are a VM based

00:02:21,359 --> 00:02:26,189
deployment right now so which means that

00:02:23,849 --> 00:02:28,469
we take an application your application

00:02:26,189 --> 00:02:30,299
have n number of missions and this n

00:02:28,469 --> 00:02:32,519
number of missions will be fronted by a

00:02:30,299 --> 00:02:34,469
load balancer and the traffic gets load

00:02:32,519 --> 00:02:36,359
balance between that so when we started

00:02:34,469 --> 00:02:38,489
looking into cluster management

00:02:36,359 --> 00:02:40,109
environment what are the challenges so

00:02:38,489 --> 00:02:42,420
we started writing down what are the

00:02:40,109 --> 00:02:44,099
challenges with that environment when it

00:02:42,420 --> 00:02:45,720
comes to cluster manager so because in a

00:02:44,099 --> 00:02:47,579
cluster management environment every

00:02:45,720 --> 00:02:49,260
container comes and goes more

00:02:47,579 --> 00:02:51,030
dynamically down a VM environment

00:02:49,260 --> 00:02:52,739
because in a VM environment you will say

00:02:51,030 --> 00:02:54,329
that I need 10 instance of my

00:02:52,739 --> 00:02:56,549
application running what we do is we

00:02:54,329 --> 00:02:59,250
create 10 VMs map it to a load balancer

00:02:56,549 --> 00:03:01,680
and most of the time the 10 games are

00:02:59,250 --> 00:03:03,510
static right you are not deleting

00:03:01,680 --> 00:03:05,430
recreating every time unless you have

00:03:03,510 --> 00:03:07,709
any immutable way of doing it mostly if

00:03:05,430 --> 00:03:09,750
you are a code deployment is more like

00:03:07,709 --> 00:03:11,579
an overwrite then you have the same set

00:03:09,750 --> 00:03:13,469
so there is not much of a dynamic sum

00:03:11,579 --> 00:03:15,209
involved in a VM based work but in a

00:03:13,469 --> 00:03:17,430
cluster management world there's a lot

00:03:15,209 --> 00:03:19,530
of dynamism involved so we started

00:03:17,430 --> 00:03:21,389
exploring what are the different pattern

00:03:19,530 --> 00:03:23,819
which exists what works what doesn't

00:03:21,389 --> 00:03:26,370
work for our use cases and that's a

00:03:23,819 --> 00:03:27,930
sharing here the first thing we looked

00:03:26,370 --> 00:03:30,269
at when we wanted to move from a

00:03:27,930 --> 00:03:32,099
traditional VM based deployment into a

00:03:30,269 --> 00:03:33,900
cluster management like me source of

00:03:32,099 --> 00:03:35,519
cognitive you take anything the first

00:03:33,900 --> 00:03:38,489
thing you see is that whether you want

00:03:35,519 --> 00:03:40,260
have a load balancer in front of your

00:03:38,489 --> 00:03:42,959
applications or you want to do it like

00:03:40,260 --> 00:03:44,250
what this new concept of service mesh or

00:03:42,959 --> 00:03:46,560
a proxy is trying to do with our

00:03:44,250 --> 00:03:48,989
east-west load balancing so that's what

00:03:46,560 --> 00:03:50,909
these two models are one is a knots out

00:03:48,989 --> 00:03:52,979
and others are East we're just to give a

00:03:50,909 --> 00:03:54,780
detail and not so does something like

00:03:52,979 --> 00:03:56,970
you have a load balancer in front of

00:03:54,780 --> 00:03:58,799
your application and every request comes

00:03:56,970 --> 00:04:00,930
to the load balancer that means it's the

00:03:58,799 --> 00:04:02,280
single point of entry and then the data

00:04:00,930 --> 00:04:04,049
traffic gets load balance between

00:04:02,280 --> 00:04:06,540
different instances in the east-west

00:04:04,049 --> 00:04:09,209
model your services know about each

00:04:06,540 --> 00:04:11,759
other and there is no one more op it

00:04:09,209 --> 00:04:13,349
automatically have a know which end

00:04:11,759 --> 00:04:15,329
point to collect and there is a

00:04:13,349 --> 00:04:18,389
client-side load balancing happens there

00:04:15,329 --> 00:04:20,039
so here what we are trying to do is that

00:04:18,389 --> 00:04:21,930
figure out what are the pros and cons

00:04:20,039 --> 00:04:24,419
each of these approaches has its pros

00:04:21,930 --> 00:04:25,350
and cons and but we wanted to see what

00:04:24,419 --> 00:04:27,960
are them and then

00:04:25,350 --> 00:04:29,880
map it to our use case and really see

00:04:27,960 --> 00:04:32,820
which one will work so when we did that

00:04:29,880 --> 00:04:34,440
these are the differences we found so as

00:04:32,820 --> 00:04:36,620
a traditionally we are we were on

00:04:34,440 --> 00:04:39,390
north-south so we wanted to see whether

00:04:36,620 --> 00:04:41,850
eastward will work for us but there were

00:04:39,390 --> 00:04:44,280
certain things which we decided to stay

00:04:41,850 --> 00:04:46,950
with not so for now and I'll talk about

00:04:44,280 --> 00:04:48,810
it so these are general things which

00:04:46,950 --> 00:04:50,220
happens the one is in an orchard it's a

00:04:48,810 --> 00:04:51,660
central load balancing which means it's

00:04:50,220 --> 00:04:53,310
a single entry point you have full

00:04:51,660 --> 00:04:54,930
control that is one of the biggest

00:04:53,310 --> 00:04:56,730
aspect of it that if you want to take

00:04:54,930 --> 00:04:58,860
traffic out you can do it at a single

00:04:56,730 --> 00:05:01,320
point you don't need to have that data

00:04:58,860 --> 00:05:03,030
distributor to every VM where in case of

00:05:01,320 --> 00:05:04,860
a service mesh you need to have a proxy

00:05:03,030 --> 00:05:06,540
aware of this and if let's say there is

00:05:04,860 --> 00:05:08,460
a network partition happens and if the

00:05:06,540 --> 00:05:10,440
proxy doesn't get that you still be

00:05:08,460 --> 00:05:12,270
sending traffic to it so from a

00:05:10,440 --> 00:05:13,890
enterprise level one of the key

00:05:12,270 --> 00:05:15,840
decisions was made to go with not so

00:05:13,890 --> 00:05:17,460
because we want a single entry where you

00:05:15,840 --> 00:05:19,230
can control traffic if something is

00:05:17,460 --> 00:05:21,600
wrong we don't want any VM to get

00:05:19,230 --> 00:05:23,520
traffic then you need a simple knob to

00:05:21,600 --> 00:05:25,530
say don't send traffic that was possible

00:05:23,520 --> 00:05:28,860
only with not so we were not able to do

00:05:25,530 --> 00:05:30,840
it with the east-west model and the

00:05:28,860 --> 00:05:33,870
other thing was that when you have a

00:05:30,840 --> 00:05:36,150
central load balancer do two things what

00:05:33,870 --> 00:05:37,440
happens is that you have a the it's a

00:05:36,150 --> 00:05:39,390
cluster of air which means that if you

00:05:37,440 --> 00:05:41,070
have 10 dreams it understands what's the

00:05:39,390 --> 00:05:42,450
load on each of this VM when I send

00:05:41,070 --> 00:05:44,550
traffic whether it's a block rolling

00:05:42,450 --> 00:05:46,560
uplink a lot of information can be found

00:05:44,550 --> 00:05:48,240
but when you are doing it on the client

00:05:46,560 --> 00:05:50,190
side load balancing that all this

00:05:48,240 --> 00:05:53,130
information are little tricky because

00:05:50,190 --> 00:05:54,810
there is a each VM just finds one round

00:05:53,130 --> 00:05:56,370
of hosts and sends data now there are

00:05:54,810 --> 00:05:58,380
new things coming up where they are

00:05:56,370 --> 00:06:00,420
becoming aware of traffic awareness but

00:05:58,380 --> 00:06:06,570
still it's an evolution it's not there

00:06:00,420 --> 00:06:09,810
where it should be so this is what I was

00:06:06,570 --> 00:06:12,900
talking about so we if you have a if you

00:06:09,810 --> 00:06:14,220
are working on any typical cloud you

00:06:12,900 --> 00:06:16,140
will have a load balancer and you'll

00:06:14,220 --> 00:06:18,600
have a set of nodes you create a lb whip

00:06:16,140 --> 00:06:20,640
and it'll be pool and you have a monitor

00:06:18,600 --> 00:06:22,710
a monitor is nothing but to say whether

00:06:20,640 --> 00:06:24,840
this instance of application is good or

00:06:22,710 --> 00:06:26,490
not and then if that is good then it

00:06:24,840 --> 00:06:28,320
will start sending traffic and you have

00:06:26,490 --> 00:06:30,450
an access point which is the lb whip

00:06:28,320 --> 00:06:32,850
through which you access the service and

00:06:30,450 --> 00:06:35,010
then the load balancer based on the

00:06:32,850 --> 00:06:36,810
algorithm we have selected it sends

00:06:35,010 --> 00:06:38,430
traffic's to all this node so if it's a

00:06:36,810 --> 00:06:38,889
round-robin it keeps around robbing if

00:06:38,430 --> 00:06:40,509
it's a

00:06:38,889 --> 00:06:42,009
situated then it'll send to the one

00:06:40,509 --> 00:06:43,960
which has taken less connection so it

00:06:42,009 --> 00:06:45,969
depends upon the algorithm we do it now

00:06:43,960 --> 00:06:48,550
this is what I was talking about is more

00:06:45,969 --> 00:06:51,340
static right in this world you have VMs

00:06:48,550 --> 00:06:53,949
once the VM is created mostly the elbe

00:06:51,340 --> 00:06:55,990
side of the there's no change it almost

00:06:53,949 --> 00:06:58,629
remains static for the lifetime of these

00:06:55,990 --> 00:07:00,819
VMs only if there is a change in new VM

00:06:58,629 --> 00:07:01,990
comes in or you replace a VM you need to

00:07:00,819 --> 00:07:03,759
go and touch their load balancer

00:07:01,990 --> 00:07:10,029
configuration otherwise they statically

00:07:03,759 --> 00:07:11,500
they're but the same let's take a

00:07:10,029 --> 00:07:14,379
coastal management environment let's

00:07:11,500 --> 00:07:16,150
talk about missus or any other cluster

00:07:14,379 --> 00:07:17,949
manager now when you bring a custom

00:07:16,150 --> 00:07:19,750
management the first thing is happens is

00:07:17,949 --> 00:07:21,909
the containers are dynamic which means

00:07:19,750 --> 00:07:23,409
that it can die it can come up somewhere

00:07:21,909 --> 00:07:25,000
or when you do or the next version of

00:07:23,409 --> 00:07:26,650
the deployment your whole container what

00:07:25,000 --> 00:07:28,509
is running was gone a new set of

00:07:26,650 --> 00:07:30,400
container comes that means that there's

00:07:28,509 --> 00:07:33,039
a lot more dynamism and there is no

00:07:30,400 --> 00:07:34,990
in-place code replacement now you are

00:07:33,039 --> 00:07:37,270
going into a more of a immutable

00:07:34,990 --> 00:07:38,560
container which gets deployed and when

00:07:37,270 --> 00:07:40,150
the container dies you are another

00:07:38,560 --> 00:07:42,279
container comes up somewhere else which

00:07:40,150 --> 00:07:43,719
means there's a lot more dynamism in the

00:07:42,279 --> 00:07:45,520
cluster when there's a lot of dynamism

00:07:43,719 --> 00:07:47,139
in the cluster your load balancer

00:07:45,520 --> 00:07:49,180
configuration halls also has to be

00:07:47,139 --> 00:07:51,039
dynamic which means that every time a

00:07:49,180 --> 00:07:52,479
container dies and comes up somewhere

00:07:51,039 --> 00:07:54,759
you should have taken out the previous

00:07:52,479 --> 00:07:57,719
container and put the new container up

00:07:54,759 --> 00:08:00,699
in that way the configurations has to be

00:07:57,719 --> 00:08:03,189
generated in a more dynamic fashion so

00:08:00,699 --> 00:08:05,919
what we figured out is that even if you

00:08:03,189 --> 00:08:08,589
go to a model where in a cluster manager

00:08:05,919 --> 00:08:11,139
let's say everybody I'm using the term

00:08:08,589 --> 00:08:14,080
pod to represent a set of containers if

00:08:11,139 --> 00:08:16,389
you take your application as a pod then

00:08:14,080 --> 00:08:18,789
if you deploy that part each part should

00:08:16,389 --> 00:08:21,520
be represented by an IP in a typical

00:08:18,789 --> 00:08:22,899
world if you want to take that form from

00:08:21,520 --> 00:08:24,819
if you don't want to do port mapping

00:08:22,899 --> 00:08:27,310
which is another layer of complexity it

00:08:24,819 --> 00:08:29,259
is better to do a part where a part

00:08:27,310 --> 00:08:31,689
represents a set of containers and each

00:08:29,259 --> 00:08:34,659
part will have a unique identifier which

00:08:31,689 --> 00:08:37,719
is your IP once you have an IP then this

00:08:34,659 --> 00:08:40,419
pod that IP colon port has to be mapped

00:08:37,719 --> 00:08:42,219
to your load balancer now if you delete

00:08:40,419 --> 00:08:43,930
this container and the same container

00:08:42,219 --> 00:08:46,630
comes somewhere else it won't get the

00:08:43,930 --> 00:08:49,029
same IP because transferring IP is an

00:08:46,630 --> 00:08:50,709
even more difficult problem and in a

00:08:49,029 --> 00:08:52,750
stateless world it doesn't mix to have

00:08:50,709 --> 00:08:54,519
this IP portability

00:08:52,750 --> 00:08:57,819
so in that world that means that a

00:08:54,519 --> 00:09:00,310
container a running on a box a if it

00:08:57,819 --> 00:09:01,899
dies and comes back in a box B it'll get

00:09:00,310 --> 00:09:04,120
a new IP that means your load balancer

00:09:01,899 --> 00:09:06,220
should know now oh now I have a new

00:09:04,120 --> 00:09:07,779
member I need to send traffic there and

00:09:06,220 --> 00:09:09,430
also it should have removed the old one

00:09:07,779 --> 00:09:12,129
so that it should not send any traffic

00:09:09,430 --> 00:09:14,500
so there is a lot more dynamism involved

00:09:12,129 --> 00:09:16,029
and now the load balancer configuration

00:09:14,500 --> 00:09:17,740
is tied to the lifecycle of the

00:09:16,029 --> 00:09:19,089
container every time the container dies

00:09:17,740 --> 00:09:25,029
and comes the load balancer

00:09:19,089 --> 00:09:29,009
configuration has to be updated so just

00:09:25,029 --> 00:09:31,329
from these certain categories of

00:09:29,009 --> 00:09:34,029
patterns what we looked at these are the

00:09:31,329 --> 00:09:36,129
challenges we looked at one is that the

00:09:34,029 --> 00:09:38,769
lifespan of the elbe configuration is

00:09:36,129 --> 00:09:40,660
very short anytime a pod comes and pod

00:09:38,769 --> 00:09:43,089
goes a configuration and load balancer

00:09:40,660 --> 00:09:45,220
to be updated which means that the load

00:09:43,089 --> 00:09:47,529
balancers control flame which is nothing

00:09:45,220 --> 00:09:49,449
but the API layer should be very

00:09:47,529 --> 00:09:51,699
responsive that means I should be able

00:09:49,449 --> 00:09:54,189
to go and say if I bring up 10,000

00:09:51,699 --> 00:09:56,290
containers by deleting a 10,000 that

00:09:54,189 --> 00:09:58,029
means 10,000 operation to say remove and

00:09:56,290 --> 00:09:59,740
then out of the 10,000 operation to say

00:09:58,029 --> 00:10:02,259
add which means it should be scalable

00:09:59,740 --> 00:10:04,240
and it should be very performant if it

00:10:02,259 --> 00:10:06,399
is not performing then what will happen

00:10:04,240 --> 00:10:08,079
is that your operations will get stucked

00:10:06,399 --> 00:10:10,180
and queued up that means new containers

00:10:08,079 --> 00:10:12,550
cannot take traffic that is one of the

00:10:10,180 --> 00:10:15,220
main things second thing is that there

00:10:12,550 --> 00:10:17,139
is one drawback with this start whenever

00:10:15,220 --> 00:10:19,059
container dies and comes you have a

00:10:17,139 --> 00:10:20,500
mechanism to detect update but let's say

00:10:19,059 --> 00:10:23,110
the whole VM went down for some reason

00:10:20,500 --> 00:10:24,579
there may be some residuals in the load

00:10:23,110 --> 00:10:26,649
balancer because you then have a

00:10:24,579 --> 00:10:27,850
mechanism so you need an external system

00:10:26,649 --> 00:10:29,860
to sorry

00:10:27,850 --> 00:10:32,199
detect that and also add and remove

00:10:29,860 --> 00:10:35,110
those entries so that there's no residue

00:10:32,199 --> 00:10:37,240
left in the load balancer and I already

00:10:35,110 --> 00:10:38,769
talked about the IP pipe quad so when

00:10:37,240 --> 00:10:41,379
you have an IP per pod that means that

00:10:38,769 --> 00:10:43,449
every time a pod dies and it comes back

00:10:41,379 --> 00:10:45,069
it gets a new IP which means the load

00:10:43,449 --> 00:10:48,399
balancer configuration has to be updated

00:10:45,069 --> 00:10:51,309
and again containers keeps moving

00:10:48,399 --> 00:10:53,439
especially with the cluster manager it

00:10:51,309 --> 00:10:55,120
can move any point of time this various

00:10:53,439 --> 00:10:57,009
scenarios containers can move so that

00:10:55,120 --> 00:10:59,620
means the system whichever is going to

00:10:57,009 --> 00:11:03,790
do this update should also be aware of

00:10:59,620 --> 00:11:06,100
those things now with all this

00:11:03,790 --> 00:11:08,860
we looked at it and then said okay these

00:11:06,100 --> 00:11:10,510
are the patterns for the control plane

00:11:08,860 --> 00:11:13,480
sign of it but from the application

00:11:10,510 --> 00:11:15,730
traffic perspective what we can do so in

00:11:13,480 --> 00:11:17,830
a traditional deployment we talked about

00:11:15,730 --> 00:11:21,310
in place code deployment let's say you

00:11:17,830 --> 00:11:24,010
have 10 VMs and then a load balancer is

00:11:21,310 --> 00:11:25,840
sending traffic when you deploy code a

00:11:24,010 --> 00:11:29,020
typical word what you do is that you

00:11:25,840 --> 00:11:31,660
take one VM take it out of traffic and

00:11:29,020 --> 00:11:33,070
then replace the code and put it back in

00:11:31,660 --> 00:11:35,590
the cloud that means that one VM will

00:11:33,070 --> 00:11:37,600
have new code nine VM will have old code

00:11:35,590 --> 00:11:39,790
that's a rolling deployment right so you

00:11:37,600 --> 00:11:41,530
typically roll code in a staggered

00:11:39,790 --> 00:11:43,540
fashion so that you use the same

00:11:41,530 --> 00:11:46,360
capacity you don't have a new capacity

00:11:43,540 --> 00:11:47,800
but you replace code and then take it

00:11:46,360 --> 00:11:51,490
from one version to another version

00:11:47,800 --> 00:11:53,380
which is what is a traditional VM based

00:11:51,490 --> 00:11:54,970
capacity where you don't add any

00:11:53,380 --> 00:11:58,570
additional capacity you would deal with

00:11:54,970 --> 00:12:00,220
the capacity what you have now down the

00:11:58,570 --> 00:12:01,570
other side on a cluster management since

00:12:00,220 --> 00:12:03,490
you are running on a cluster you have

00:12:01,570 --> 00:12:05,440
containers you can do more different

00:12:03,490 --> 00:12:07,540
strategies it is possible to do the same

00:12:05,440 --> 00:12:09,520
with VMs like green blue for example

00:12:07,540 --> 00:12:12,430
Netflix has been operating with the VM

00:12:09,520 --> 00:12:13,540
but let's say if your is is not support

00:12:12,430 --> 00:12:15,850
a or you don't want to pay the

00:12:13,540 --> 00:12:18,010
additional cost of that or you want to

00:12:15,850 --> 00:12:19,510
have a faster deployments and the VM

00:12:18,010 --> 00:12:21,640
side is very difficult to do green blue

00:12:19,510 --> 00:12:24,400
if you don't have a fast is provider if

00:12:21,640 --> 00:12:25,690
you run being VMs faster then you cannot

00:12:24,400 --> 00:12:27,340
do it in a traditional let's say your

00:12:25,690 --> 00:12:29,350
own private data center and you have

00:12:27,340 --> 00:12:31,330
issues with that and you don't have

00:12:29,350 --> 00:12:33,760
additional capacity doing a green blue

00:12:31,330 --> 00:12:35,950
on a VM based deployments is very hard

00:12:33,760 --> 00:12:37,450
that's why on a cluster manager is very

00:12:35,950 --> 00:12:39,460
simple because what you're talking about

00:12:37,450 --> 00:12:41,080
is only containers if you have ten

00:12:39,460 --> 00:12:42,970
containers and you want to deploy a new

00:12:41,080 --> 00:12:45,100
version of a container then it's easy

00:12:42,970 --> 00:12:46,900
very easy to bring n plus one version of

00:12:45,100 --> 00:12:49,570
the container without affecting that ten

00:12:46,900 --> 00:12:52,420
and then slowly ramp it up or you can

00:12:49,570 --> 00:12:54,700
even bring ten new deployments and then

00:12:52,420 --> 00:12:57,340
take down the old one so it is more

00:12:54,700 --> 00:12:58,870
flexible to do these kind of deployment

00:12:57,340 --> 00:13:01,980
models with cluster management and

00:12:58,870 --> 00:13:04,570
that's where it is more effective and

00:13:01,980 --> 00:13:06,220
the third point is basically what I

00:13:04,570 --> 00:13:07,780
talked about depletion of capacity

00:13:06,220 --> 00:13:10,240
because let's say you have 10 nodes and

00:13:07,780 --> 00:13:11,860
then you if you are doing a staggered

00:13:10,240 --> 00:13:13,270
rollout you have to take two down which

00:13:11,860 --> 00:13:15,700
means you are depleting your capacity

00:13:13,270 --> 00:13:16,750
from 10 to 8 where in the other side you

00:13:15,700 --> 00:13:19,060
are not depleting your car

00:13:16,750 --> 00:13:22,750
capacity or adding more and then taking

00:13:19,060 --> 00:13:24,970
down in a way and again the roll backs

00:13:22,750 --> 00:13:26,830
are faster because in a green blow

00:13:24,970 --> 00:13:28,480
deployment let's say of 1010 new version

00:13:26,830 --> 00:13:29,680
got deployed and when you switch the

00:13:28,480 --> 00:13:31,390
traffic if you find there is an issue

00:13:29,680 --> 00:13:33,190
you can just switch it back the load

00:13:31,390 --> 00:13:34,720
balancer to the old version so there's

00:13:33,190 --> 00:13:36,760
no code deployment so that means you can

00:13:34,720 --> 00:13:38,650
roll back to a fast early - older

00:13:36,760 --> 00:13:40,420
version which is very good very very

00:13:38,650 --> 00:13:41,920
important for a business because you

00:13:40,420 --> 00:13:44,290
don't want your customers to face issues

00:13:41,920 --> 00:13:49,840
and you want to go back to the good

00:13:44,290 --> 00:13:51,430
known version as soon as possible so

00:13:49,840 --> 00:13:53,320
this is a very simple representation of

00:13:51,430 --> 00:13:56,560
what we are doing so this is a very

00:13:53,320 --> 00:13:58,240
common pattern where anytime a pod comes

00:13:56,560 --> 00:14:00,220
up you have something called as a

00:13:58,240 --> 00:14:01,210
register which is on the box which

00:14:00,220 --> 00:14:03,790
listens to docker

00:14:01,210 --> 00:14:06,250
even's and whenever we see that or that

00:14:03,790 --> 00:14:08,380
particular part is up we get all the

00:14:06,250 --> 00:14:10,180
metadata about that pod and then we go

00:14:08,380 --> 00:14:12,430
and register in the load balancer saying

00:14:10,180 --> 00:14:14,890
that there's a new load balancer member

00:14:12,430 --> 00:14:16,810
and start sending traffic to that then

00:14:14,890 --> 00:14:19,000
similarly when the container dies we

00:14:16,810 --> 00:14:20,680
similarly track that event and then we

00:14:19,000 --> 00:14:22,300
figure out this container has gone down

00:14:20,680 --> 00:14:24,640
and we go to the load balancer and say

00:14:22,300 --> 00:14:27,280
remove this Center so it's all localized

00:14:24,640 --> 00:14:29,800
to a VM for two reasons one is that it

00:14:27,280 --> 00:14:31,080
has tied to a local pace rather than a

00:14:29,800 --> 00:14:34,210
central place to do this operation

00:14:31,080 --> 00:14:36,070
because if you keep it in Central when

00:14:34,210 --> 00:14:39,550
the container moves because of reasons

00:14:36,070 --> 00:14:41,440
like a VM dying or the scheduler moves

00:14:39,550 --> 00:14:43,810
it you cannot track it but if you keep

00:14:41,440 --> 00:14:46,089
it to the commerce sorry common

00:14:43,810 --> 00:14:48,010
denominator which is your host then it's

00:14:46,089 --> 00:14:49,420
easy to track because on the host is

00:14:48,010 --> 00:14:51,160
your source of truth weather which

00:14:49,420 --> 00:14:53,530
containers are running so you can always

00:14:51,160 --> 00:15:01,690
do a reconciliation of that and update a

00:14:53,530 --> 00:15:05,170
load balancer with the right details so

00:15:01,690 --> 00:15:07,360
with all this different criterias like

00:15:05,170 --> 00:15:10,420
we started looking at different load

00:15:07,360 --> 00:15:12,400
balancers which can support us so we

00:15:10,420 --> 00:15:13,870
talked about green blow deployments

00:15:12,400 --> 00:15:15,970
right so if you want to do green glow

00:15:13,870 --> 00:15:17,440
deployments we need the load balancers

00:15:15,970 --> 00:15:19,720
supported so a load balancer should have

00:15:17,440 --> 00:15:23,380
a mechanism to say okay I can have the

00:15:19,720 --> 00:15:25,540
same set of pool having to things like

00:15:23,380 --> 00:15:27,459
pool a and Pool B and then has an

00:15:25,540 --> 00:15:29,380
ability to transfer traffic between them

00:15:27,459 --> 00:15:30,080
so that means that we should be able to

00:15:29,380 --> 00:15:31,850
do

00:15:30,080 --> 00:15:33,950
dynamic configuration and move traffic

00:15:31,850 --> 00:15:35,330
that is one second thing is we needed a

00:15:33,950 --> 00:15:37,700
load balancer which has a very strong

00:15:35,330 --> 00:15:40,220
control plane which means API layer so

00:15:37,700 --> 00:15:41,839
that we can hit that very hard like if a

00:15:40,220 --> 00:15:43,220
thousand containers has to be brought up

00:15:41,839 --> 00:15:44,420
and thousand containers has to be

00:15:43,220 --> 00:15:46,700
brought down we should be able to make

00:15:44,420 --> 00:15:48,649
the call and get responses as soon as

00:15:46,700 --> 00:15:50,540
possible that means a strong API layer

00:15:48,649 --> 00:15:52,730
the third thing is that if this has to

00:15:50,540 --> 00:15:54,470
work if all this has to work then it has

00:15:52,730 --> 00:15:56,510
also be Horizonte scalable which means

00:15:54,470 --> 00:15:58,760
it should be fault tolerant when it

00:15:56,510 --> 00:16:01,550
something fails it shouldn't be totally

00:15:58,760 --> 00:16:05,180
collapsing the system and also the main

00:16:01,550 --> 00:16:07,010
point was metrics and monitoring so when

00:16:05,180 --> 00:16:08,839
when you have this kind of dynamic

00:16:07,010 --> 00:16:11,000
environment where container comes and

00:16:08,839 --> 00:16:13,850
goes or it take traffic and where it

00:16:11,000 --> 00:16:15,769
runs in this cluster you need high level

00:16:13,850 --> 00:16:17,899
of metrics to figure out where there is

00:16:15,769 --> 00:16:20,540
issue let's say there's a slowness or

00:16:17,899 --> 00:16:22,130
you see that there is a lot of fire and

00:16:20,540 --> 00:16:23,630
happening on one most where it is not

00:16:22,130 --> 00:16:25,250
happening another you need to have a

00:16:23,630 --> 00:16:26,540
visibility to figure out where is it

00:16:25,250 --> 00:16:28,190
happening and why is it happening that

00:16:26,540 --> 00:16:29,930
means the load balancer since we have

00:16:28,190 --> 00:16:31,339
gone with the north-south model there's

00:16:29,930 --> 00:16:33,260
a single point of entry through which

00:16:31,339 --> 00:16:34,820
all these interactions are happening so

00:16:33,260 --> 00:16:36,800
it's very easy to figure out with the

00:16:34,820 --> 00:16:38,120
matrix what we get back that hey what is

00:16:36,800 --> 00:16:40,160
happening so that means that there

00:16:38,120 --> 00:16:41,660
should be a very high level of metrics

00:16:40,160 --> 00:16:44,890
available at this load balancing system

00:16:41,660 --> 00:16:47,149
to do any kind of analysis and

00:16:44,890 --> 00:16:49,370
definitely since we are a FinTech

00:16:47,149 --> 00:16:51,020
company we use TLS everywhere from node

00:16:49,370 --> 00:16:52,520
to node which means that we should be

00:16:51,020 --> 00:16:55,790
able to get a high throughput even with

00:16:52,520 --> 00:17:00,709
TLS traffic so with that I'll hand it

00:16:55,790 --> 00:17:03,770
over to Rangga to continue on Thanks my

00:17:00,709 --> 00:17:06,410
name is Rangga CTO Davi networks we have

00:17:03,770 --> 00:17:09,949
been working closely with PayPal for

00:17:06,410 --> 00:17:14,179
over a year and and let's see what we

00:17:09,949 --> 00:17:17,179
provide for high availability for

00:17:14,179 --> 00:17:20,569
security and for monitoring that makes

00:17:17,179 --> 00:17:23,510
this deployment possible so in this

00:17:20,569 --> 00:17:25,699
picture I want to walk over what a

00:17:23,510 --> 00:17:28,550
traditional load balancer looks like

00:17:25,699 --> 00:17:32,120
this is how load balancers have been

00:17:28,550 --> 00:17:33,320
deployed for 20 years now for most of us

00:17:32,120 --> 00:17:36,650
who have been through traditional

00:17:33,320 --> 00:17:39,110
deployments these are usually Hardware

00:17:36,650 --> 00:17:41,540
specialized Hardware boxes and they have

00:17:39,110 --> 00:17:42,500
both the control of the data plane in

00:17:41,540 --> 00:17:43,880
them

00:17:42,500 --> 00:17:46,460
it's it's

00:17:43,880 --> 00:17:49,220
early software but it is individually

00:17:46,460 --> 00:17:52,310
managed and you have to talk to each one

00:17:49,220 --> 00:17:55,720
to configure them and and monitor them

00:17:52,310 --> 00:18:00,590
and so on so forth what we have done

00:17:55,720 --> 00:18:03,320
Madhavi is disaggregated much like a

00:18:00,590 --> 00:18:05,800
cluster manager disaggregated in the

00:18:03,320 --> 00:18:08,450
control plane from the data plane and

00:18:05,800 --> 00:18:12,050
then the data plane essentially becomes

00:18:08,450 --> 00:18:15,140
software proxies that you deploy as a

00:18:12,050 --> 00:18:19,550
distributed fabric across your clusters

00:18:15,140 --> 00:18:22,400
all the control the maintenance the

00:18:19,550 --> 00:18:25,280
management and all of this happens

00:18:22,400 --> 00:18:29,690
through the central controller much like

00:18:25,280 --> 00:18:35,870
a mezzos master or a community's master

00:18:29,690 --> 00:18:38,330
and once you deployed this solution you

00:18:35,870 --> 00:18:39,940
can take these proxies and you can

00:18:38,330 --> 00:18:43,340
fundamentally run them on bare metal

00:18:39,940 --> 00:18:45,710
which gives you performance very high

00:18:43,340 --> 00:18:48,290
throughput and performance you can

00:18:45,710 --> 00:18:51,500
deploy them as virtual machines either

00:18:48,290 --> 00:18:54,170
vSphere or KBM you can also deploy them

00:18:51,500 --> 00:18:57,410
as containers docker containers in a

00:18:54,170 --> 00:19:00,050
cluster like DCOs

00:18:57,410 --> 00:19:02,510
or kubernetes and all of this is

00:19:00,050 --> 00:19:05,990
possible in the public cloud as well and

00:19:02,510 --> 00:19:08,810
this allows a single central point of

00:19:05,990 --> 00:19:15,710
management for the proxies and also for

00:19:08,810 --> 00:19:17,120
all your services and the controller not

00:19:15,710 --> 00:19:19,880
just provides a control in the

00:19:17,120 --> 00:19:23,180
management plane it also provides a lot

00:19:19,880 --> 00:19:26,030
of analytics a key part of what a load

00:19:23,180 --> 00:19:28,370
balancer does is it proxies the

00:19:26,030 --> 00:19:32,990
connections between the clients and the

00:19:28,370 --> 00:19:34,910
actual pods so it is aware of all the

00:19:32,990 --> 00:19:36,140
traffic that goes through it it's aware

00:19:34,910 --> 00:19:39,080
of the performance of the application

00:19:36,140 --> 00:19:43,280
it's aware of the response times for

00:19:39,080 --> 00:19:47,120
example from the backend part and so it

00:19:43,280 --> 00:19:50,270
has all this information that lets you

00:19:47,120 --> 00:19:53,300
monitor so when something goes wrong not

00:19:50,270 --> 00:19:54,830
only does it take some action it does

00:19:53,300 --> 00:19:57,530
health monitoring and all the other good

00:19:54,830 --> 00:20:00,800
stuff to remove sick instances

00:19:57,530 --> 00:20:04,130
but it also lets you debug brow notes

00:20:00,800 --> 00:20:06,980
for example if out of four parts or four

00:20:04,130 --> 00:20:11,090
containers one of them is misbehaving or

00:20:06,980 --> 00:20:13,340
is low or 5% of your users complain of

00:20:11,090 --> 00:20:15,560
slow responses at to it between 2:00

00:20:13,340 --> 00:20:17,810
a.m. and 3:00 a.m. you need to know

00:20:15,560 --> 00:20:20,210
what's happening to those specific users

00:20:17,810 --> 00:20:22,310
and that's really what it lets you do

00:20:20,210 --> 00:20:26,590
and we'll see some of how it does and

00:20:22,310 --> 00:20:29,690
what it does and the control law also

00:20:26,590 --> 00:20:31,970
works with other orchestration systems

00:20:29,690 --> 00:20:34,760
with DCOs for example so when you create

00:20:31,970 --> 00:20:37,100
an application in DCOs it automatically

00:20:34,760 --> 00:20:40,880
creates a corresponding service for it

00:20:37,100 --> 00:20:44,780
or it also provides API endpoints for

00:20:40,880 --> 00:20:47,090
you to natively control for example the

00:20:44,780 --> 00:20:51,170
services so in the case which come out

00:20:47,090 --> 00:20:53,480
just outlined when a node new node comes

00:20:51,170 --> 00:20:58,490
up and there are containers running on

00:20:53,480 --> 00:21:01,490
that node an agent can call a an API at

00:20:58,490 --> 00:21:03,590
a central point and immediately register

00:21:01,490 --> 00:21:06,860
all the containers running on that node

00:21:03,590 --> 00:21:10,400
right so so this central API endpoint

00:21:06,860 --> 00:21:14,060
allows you to centrally control and

00:21:10,400 --> 00:21:19,400
manage the life cycle of these proxies

00:21:14,060 --> 00:21:24,340
and also these services themselves so

00:21:19,400 --> 00:21:31,400
let's take a look at a live demo system

00:21:24,340 --> 00:21:32,660
so this is a demo system it shows so

00:21:31,400 --> 00:21:34,970
this is the controller that you're

00:21:32,660 --> 00:21:38,950
talking to right now and what it shows

00:21:34,970 --> 00:21:42,560
for every service is this dashboard and

00:21:38,950 --> 00:21:45,160
the first thing you see here is a end to

00:21:42,560 --> 00:21:49,400
end timing this shows on an aggregate

00:21:45,160 --> 00:21:51,400
how far your clients are what is the

00:21:49,400 --> 00:21:53,930
latency between the proxy and the

00:21:51,400 --> 00:21:57,140
instances what's the application

00:21:53,930 --> 00:21:59,480
response time so this is the time when

00:21:57,140 --> 00:22:01,700
the request was sent to the backend to

00:21:59,480 --> 00:22:03,620
when it started responding to the

00:22:01,700 --> 00:22:07,880
request and then finally the data

00:22:03,620 --> 00:22:10,809
transfer time now each metric here tells

00:22:07,880 --> 00:22:13,480
it here if the client latency is high we

00:22:10,809 --> 00:22:15,669
it can if you have clients coming in

00:22:13,480 --> 00:22:17,590
from say across the continent

00:22:15,669 --> 00:22:19,059
then essentially it means that you need

00:22:17,590 --> 00:22:21,070
to reduce that maybe you need a

00:22:19,059 --> 00:22:22,720
distributed application across multiple

00:22:21,070 --> 00:22:25,570
data centers need to move your

00:22:22,720 --> 00:22:28,360
application closer to your client if the

00:22:25,570 --> 00:22:30,700
server latency is high that means the

00:22:28,360 --> 00:22:33,309
server is being saturated I mean it's

00:22:30,700 --> 00:22:36,100
it's it's either far away which is

00:22:33,309 --> 00:22:39,789
somewhat unlikely or that it is just

00:22:36,100 --> 00:22:42,309
slow if the application response time is

00:22:39,789 --> 00:22:44,440
high that really means the server is

00:22:42,309 --> 00:22:46,419
just being slow in responding to the

00:22:44,440 --> 00:22:48,429
application and finally the data

00:22:46,419 --> 00:22:50,559
transfer time is high either response is

00:22:48,429 --> 00:22:52,629
big or your network has throughput

00:22:50,559 --> 00:22:55,059
issues so it's just taking a long time

00:22:52,629 --> 00:22:58,269
for the response to stream back so this

00:22:55,059 --> 00:23:01,419
lets you in one glance identify where

00:22:58,269 --> 00:23:03,669
the potential bottleneck could be but

00:23:01,419 --> 00:23:07,539
this is an aggregate picture if you want

00:23:03,669 --> 00:23:12,610
to go a little bit more granular it lets

00:23:07,539 --> 00:23:15,369
you do that so each entry here is a log

00:23:12,610 --> 00:23:20,049
entry that shows the information for a

00:23:15,369 --> 00:23:23,860
specific HTTP request and so if you want

00:23:20,049 --> 00:23:28,029
to look at all users whose response time

00:23:23,860 --> 00:23:30,039
for example was more than nine seconds

00:23:28,029 --> 00:23:33,369
you can simply do that you can say I

00:23:30,039 --> 00:23:35,409
want to look at all users whose response

00:23:33,369 --> 00:23:38,409
time was more than nine again and well

00:23:35,409 --> 00:23:41,529
you'll see who these users are you can

00:23:38,409 --> 00:23:42,820
see is there a specific pattern for

00:23:41,529 --> 00:23:44,529
these users are they coming from a

00:23:42,820 --> 00:23:46,749
specific location or they using a

00:23:44,529 --> 00:23:49,720
specific type of browser and other

00:23:46,749 --> 00:23:54,369
things that will let you get to the

00:23:49,720 --> 00:23:56,529
problem as soon as possible and the

00:23:54,369 --> 00:23:58,119
other thing that this fabric does in

00:23:56,529 --> 00:24:02,549
addition to monitoring it also uses this

00:23:58,119 --> 00:24:05,169
information for our scaling instances so

00:24:02,549 --> 00:24:08,230
it is constantly monitoring the

00:24:05,169 --> 00:24:09,820
application latency it's looking at for

00:24:08,230 --> 00:24:11,350
example a number of connections that are

00:24:09,820 --> 00:24:12,940
open to the application it can also look

00:24:11,350 --> 00:24:16,149
at other things like the CPU memory

00:24:12,940 --> 00:24:17,889
utilization often instances so if it

00:24:16,149 --> 00:24:20,559
starts seeing that your application

00:24:17,889 --> 00:24:23,330
performance is slowing down it's able to

00:24:20,559 --> 00:24:26,220
auto scale that by for example

00:24:23,330 --> 00:24:27,960
increasing the number of replicas in

00:24:26,220 --> 00:24:31,470
Indies us

00:24:27,960 --> 00:24:33,720
so this analytics allows you to build

00:24:31,470 --> 00:24:36,240
dashboards and and and consume the

00:24:33,720 --> 00:24:38,760
information but it's also internally

00:24:36,240 --> 00:24:41,160
consumed for improving the application

00:24:38,760 --> 00:24:45,000
performance the other thing is about

00:24:41,160 --> 00:24:47,310
resiliency so if you have any instance

00:24:45,000 --> 00:24:51,060
specific instance that's going through

00:24:47,310 --> 00:24:52,740
for example a brownout maybe it's out of

00:24:51,060 --> 00:24:55,320
memory and it's was beginning to drop

00:24:52,740 --> 00:24:57,120
request there's health monitors that are

00:24:55,320 --> 00:24:58,620
constantly monitoring them and the

00:24:57,120 --> 00:25:03,630
health monitors will make sure that the

00:24:58,620 --> 00:25:06,770
instances are really not being used in

00:25:03,630 --> 00:25:10,200
an active way for load balancing and

00:25:06,770 --> 00:25:11,970
that's just not all because if the

00:25:10,200 --> 00:25:13,590
health monitors are indicating or

00:25:11,970 --> 00:25:17,280
falling below a certain threshold and

00:25:13,590 --> 00:25:21,420
you won't have a policy that says if I

00:25:17,280 --> 00:25:23,820
just have 75% of capacity out of all my

00:25:21,420 --> 00:25:25,200
instances then you can for example or a

00:25:23,820 --> 00:25:27,570
scale increase the number of instances

00:25:25,200 --> 00:25:29,160
you need and it works the opposite way

00:25:27,570 --> 00:25:32,280
to if you see if you have excess

00:25:29,160 --> 00:25:35,460
capacity and and all the instances are

00:25:32,280 --> 00:25:39,150
being utilized very less then you can do

00:25:35,460 --> 00:25:41,610
the opposite and auto scale it Bluegreen

00:25:39,150 --> 00:25:43,620
deployments blue green deployment is a

00:25:41,610 --> 00:25:46,080
case where you really want everything to

00:25:43,620 --> 00:25:49,860
be graceful you you want to send let's

00:25:46,080 --> 00:25:52,590
say a percentage of traffic to some new

00:25:49,860 --> 00:25:54,840
version of the application while not

00:25:52,590 --> 00:25:56,460
affecting your existing clients so you

00:25:54,840 --> 00:25:59,040
want existing client connections to be

00:25:56,460 --> 00:26:03,300
preserved send a percentage of traffic

00:25:59,040 --> 00:26:05,760
for new clients to a new version monitor

00:26:03,300 --> 00:26:07,790
that if everything looks good then you

00:26:05,760 --> 00:26:11,340
want to completely cut over to the new

00:26:07,790 --> 00:26:13,890
version of the application without again

00:26:11,340 --> 00:26:17,550
affecting existing clients so it lets

00:26:13,890 --> 00:26:21,140
you do that so it's it's a range of

00:26:17,550 --> 00:26:24,750
functions including central management

00:26:21,140 --> 00:26:27,140
elasticity of not just your applications

00:26:24,750 --> 00:26:29,870
but also the proxy stemcells

00:26:27,140 --> 00:26:34,640
software-based that you can run anywhere

00:26:29,870 --> 00:26:36,290
high performance in terms of TLS or SSL

00:26:34,640 --> 00:26:40,850
of

00:26:36,290 --> 00:26:43,000
load balancing itself throughput a lot

00:26:40,850 --> 00:26:45,620
of monitoring and analytics information

00:26:43,000 --> 00:26:49,730
that you can use and the system itself

00:26:45,620 --> 00:26:52,790
can use and finally enterprise class

00:26:49,730 --> 00:26:55,540
security features you want security

00:26:52,790 --> 00:26:59,800
policies let's say blacklist whitelist

00:26:55,540 --> 00:27:02,809
rate limiters you want for example

00:26:59,800 --> 00:27:04,250
anything that is necessary to protect

00:27:02,809 --> 00:27:07,850
your application let's say a laugh

00:27:04,250 --> 00:27:09,650
that's what it provides and so the past

00:27:07,850 --> 00:27:12,950
year or so we have been partnering

00:27:09,650 --> 00:27:15,410
closely with PayPal and and and working

00:27:12,950 --> 00:27:19,880
with them on this journey in making a

00:27:15,410 --> 00:27:23,809
container cluster based application at a

00:27:19,880 --> 00:27:25,850
large scale successful and and we have

00:27:23,809 --> 00:27:28,730
learned a lot in that process enhance

00:27:25,850 --> 00:27:34,520
the capabilities of some of this a lot

00:27:28,730 --> 00:27:37,600
and you know we have built this to work

00:27:34,520 --> 00:27:39,710
for a production quality large-scale

00:27:37,600 --> 00:27:43,809
clustered application in a container

00:27:39,710 --> 00:27:43,809
cluster thank you very much

00:27:45,940 --> 00:27:53,750
yes was the question about

00:27:48,669 --> 00:27:57,110
disintegration yeah yeah so so this runs

00:27:53,750 --> 00:27:58,909
as a docker container inside DC US and

00:27:57,110 --> 00:28:01,370
at the beginning of time you can you can

00:27:58,909 --> 00:28:03,860
start using the DC Universe or you can

00:28:01,370 --> 00:28:07,400
just do a standalone install but what it

00:28:03,860 --> 00:28:13,340
does is it creates a fabric of proxies

00:28:07,400 --> 00:28:15,620
one proxy per node and it performs both

00:28:13,340 --> 00:28:20,539
ingress as well as internal load

00:28:15,620 --> 00:28:25,159
balancing and proxy things like security

00:28:20,539 --> 00:28:29,630
and policy enforcement and and you can

00:28:25,159 --> 00:28:31,970
use that for both load balancing within

00:28:29,630 --> 00:28:34,090
one cluster as well as across multiple

00:28:31,970 --> 00:28:37,400
clusters for a global load balancing so

00:28:34,090 --> 00:28:39,530
fundamentally runs as another docker

00:28:37,400 --> 00:28:46,000
container on every node and that's

00:28:39,530 --> 00:28:46,000
really how its deployed yes please

00:28:50,220 --> 00:28:56,310
that's right both layer seven and layer

00:28:52,470 --> 00:28:58,980
four yes layer seven for HTTP HTTP

00:28:56,310 --> 00:29:02,870
traffic and filters like DNS layer for

00:28:58,980 --> 00:29:02,870
foreign to standard this Piedra

00:29:09,340 --> 00:29:12,120
questions

00:29:14,980 --> 00:29:18,480
yes please one more question

00:29:28,780 --> 00:29:38,140
yeah don't you take that come on I think

00:29:32,320 --> 00:29:40,930
it was about a full Bluegreen the way we

00:29:38,140 --> 00:29:43,180
have implemented us more like if you

00:29:40,930 --> 00:29:47,230
have an instance of containers running

00:29:43,180 --> 00:29:49,660
there so there is a new version comes in

00:29:47,230 --> 00:29:52,000
so in avi it supports a pool and pool B

00:29:49,660 --> 00:29:54,370
concept so within a virtual service so

00:29:52,000 --> 00:29:56,650
basically we add the new members then n

00:29:54,370 --> 00:30:00,130
plus 1 numbers into the pool B first and

00:29:56,650 --> 00:30:02,500
then we are we as a API to transfer

00:30:00,130 --> 00:30:04,360
traffic so you'll say send 10% of the

00:30:02,500 --> 00:30:06,550
traffic to the new ones and keep the

00:30:04,360 --> 00:30:09,430
remaining 90% here so in that way we

00:30:06,550 --> 00:30:11,530
start looking at the new 10% how it is

00:30:09,430 --> 00:30:13,060
taking traffic and everything and we

00:30:11,530 --> 00:30:15,790
support two models this is something we

00:30:13,060 --> 00:30:17,230
built on top of a v1 is the complete

00:30:15,790 --> 00:30:19,510
let's say you have 10 instances we can

00:30:17,230 --> 00:30:21,670
set up the end 10 instances again and

00:30:19,510 --> 00:30:23,770
then just play with the traffic numbers

00:30:21,670 --> 00:30:26,620
or rather than that because it's a

00:30:23,770 --> 00:30:28,720
capacity player there we can do 10 and

00:30:26,620 --> 00:30:31,570
then maybe we can do only 20% of it

00:30:28,720 --> 00:30:33,820
which could be 2 nodes and then see

00:30:31,570 --> 00:30:35,410
these two nodes how it is going and then

00:30:33,820 --> 00:30:37,450
take down the old two nodes and then

00:30:35,410 --> 00:30:38,800
increase it so it's a slow ramp up so we

00:30:37,450 --> 00:30:40,330
will go that so these are two

00:30:38,800 --> 00:30:43,710
implementation through which you can do

00:30:40,330 --> 00:30:43,710
Bluegreen deployments

00:30:55,890 --> 00:30:59,270
oh sorry sorry

00:31:06,650 --> 00:31:14,849
okay all right okay thank you

00:31:11,550 --> 00:31:14,849

YouTube URL: https://www.youtube.com/watch?v=QDAebLfW1kU


