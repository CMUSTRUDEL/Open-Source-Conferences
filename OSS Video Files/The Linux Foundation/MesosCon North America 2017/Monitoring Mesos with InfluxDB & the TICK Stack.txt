Title: Monitoring Mesos with InfluxDB & the TICK Stack
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Monitoring Mesos with InfluxDB & the TICK Stack - Paul Dix, InfluxData & Tehmasp Chaudhri, Oracle Data Cloud

In this presentation, Paul Dix will introduce the open source TICK stack, a complete solution for monitoring infrastructure, applications, network, and services. Based on InfluxDB, the open source time series database, the other components of the stack provide everything a developer or operator needs to collect, store, monitor, and visualize monitoring and telemetry data from DC/OS and services and applications deployed within it. Paul will go through an end-to-end example showing how to get full visibility in your infrastructure using nothing but open source tools and software. 

About 

Tehmasp Chaudhri
Cloud Architect, Oracle Data Cloud
Cloud Architect at Oracle Data Cloud

Paul Dix
Paul is CTO and founder of InfluxData, the company behind the open source time series database InfluxDB. He has helped build software for startups, large companies and organizations like Microsoft, Google, McAfee, Thomson Reuters, and Air Force Space Command. He is the series editor for Addison Wesleyâ€™s Data & Analytics book and video series. In 2010 Paul wrote the book Service Oriented Design with Ruby and Rails. In 2009 he started the NYC Machine Learning Meetup, which now has over 10,000 members. Paul holds a degree in computer science from Columbia University.
Captions: 
	00:00:00,030 --> 00:00:05,460
so I'm Paul vixx the talk is about

00:00:03,300 --> 00:00:07,710
monitoring mezzos with influx data

00:00:05,460 --> 00:00:09,990
specifically the different parts of the

00:00:07,710 --> 00:00:12,330
stack that we make I'm gonna give an

00:00:09,990 --> 00:00:14,610
introduction to the stack itself and

00:00:12,330 --> 00:00:18,210
then Tomas is gonna come up and talk

00:00:14,610 --> 00:00:23,400
about how he's how they're using in flux

00:00:18,210 --> 00:00:27,330
to monitor Oracle data cloud so I'm the

00:00:23,400 --> 00:00:29,130
founder and CTO of in flux data which is

00:00:27,330 --> 00:00:34,559
what the makers have been flux DB among

00:00:29,130 --> 00:00:36,120
other things so in flux data we call we

00:00:34,559 --> 00:00:38,489
call yourselves a modern engine for

00:00:36,120 --> 00:00:40,770
metrics and events specific I I think of

00:00:38,489 --> 00:00:44,520
it as time series data in my mind time

00:00:40,770 --> 00:00:46,469
series data is it's not just metrics its

00:00:44,520 --> 00:00:48,149
metrics are you know things at regular

00:00:46,469 --> 00:00:52,379
intervals but it's also events

00:00:48,149 --> 00:00:54,360
underneath so we built different

00:00:52,379 --> 00:00:55,890
components for for dealing with the

00:00:54,360 --> 00:00:57,840
different challenges people have when

00:00:55,890 --> 00:01:00,239
working with time series data obviously

00:00:57,840 --> 00:01:03,719
they need to store it so we built in

00:01:00,239 --> 00:01:07,140
flux DB it's a time series database it's

00:01:03,719 --> 00:01:09,540
open source it's MIT licensed it's

00:01:07,140 --> 00:01:13,680
written in go we started the project

00:01:09,540 --> 00:01:16,619
back in 2013 but the kind of the the

00:01:13,680 --> 00:01:19,710
precursors to it was code that that I

00:01:16,619 --> 00:01:23,280
had started back in the fall of 2012 so

00:01:19,710 --> 00:01:25,049
pretty early on in in goes life cycle it

00:01:23,280 --> 00:01:27,479
has a query language that looks kind of

00:01:25,049 --> 00:01:29,070
like SQL we added a little bit of sugar

00:01:27,479 --> 00:01:31,229
in there to make it easier to do queries

00:01:29,070 --> 00:01:34,770
for time series stuff which I'll show in

00:01:31,229 --> 00:01:36,810
a little bit a couple of years ago we

00:01:34,770 --> 00:01:38,549
wrote our own storage engine from

00:01:36,810 --> 00:01:41,250
scratch which some people thought was

00:01:38,549 --> 00:01:43,759
kind of an insane move but it's actually

00:01:41,250 --> 00:01:48,450
turned out really well we call it a time

00:01:43,759 --> 00:01:50,280
time structured merge tree so it's a

00:01:48,450 --> 00:01:55,200
storage engine that's very very similar

00:01:50,280 --> 00:01:57,329
to LSM trees if you know those and it

00:01:55,200 --> 00:01:59,369
has built in stuff for creating

00:01:57,329 --> 00:02:01,140
retention policies right so you have

00:01:59,369 --> 00:02:03,360
high precision data that you keep around

00:02:01,140 --> 00:02:04,860
say for seven days then you have medium

00:02:03,360 --> 00:02:08,840
precision data that you keep around for

00:02:04,860 --> 00:02:11,430
longer all that kind of stuff and it has

00:02:08,840 --> 00:02:13,560
stuff in it for basically doing what we

00:02:11,430 --> 00:02:13,980
call continuous queries so basically you

00:02:13,560 --> 00:02:16,590
can

00:02:13,980 --> 00:02:18,480
a query to create summaries or down

00:02:16,590 --> 00:02:20,190
samples of your time series data and

00:02:18,480 --> 00:02:22,319
right into another attention policy and

00:02:20,190 --> 00:02:24,330
that'll just run continuously in the

00:02:22,319 --> 00:02:26,430
background on the database you don't

00:02:24,330 --> 00:02:31,260
have to create separate stuff separate

00:02:26,430 --> 00:02:34,290
like downsampling logic so what is time

00:02:31,260 --> 00:02:36,209
series data when I think about time

00:02:34,290 --> 00:02:39,840
series data I think about stock trades

00:02:36,209 --> 00:02:41,610
and quotes right eat each trade and a

00:02:39,840 --> 00:02:43,890
stock market is you know is obviously

00:02:41,610 --> 00:02:45,959
that's a time series this is we were

00:02:43,890 --> 00:02:47,760
looking at Apple stock price so what

00:02:45,959 --> 00:02:49,650
you're actually looking at there is a

00:02:47,760 --> 00:02:51,480
summarization of an underlying time

00:02:49,650 --> 00:02:53,310
series there are too many trades of

00:02:51,480 --> 00:02:54,930
Apple stock in a day to visualize it on

00:02:53,310 --> 00:02:57,209
a single graph right there aren't enough

00:02:54,930 --> 00:03:00,180
pixels there so this is a summarization

00:02:57,209 --> 00:03:01,620
of it we also think about metrics right

00:03:00,180 --> 00:03:03,150
this is like server metrics server

00:03:01,620 --> 00:03:07,530
monitoring application performance

00:03:03,150 --> 00:03:10,260
monitoring user analytics events this is

00:03:07,530 --> 00:03:11,730
a log from Apache when I see this I see

00:03:10,260 --> 00:03:15,329
a bunch of different time series right

00:03:11,730 --> 00:03:18,769
200 requests overtime requests to a

00:03:15,329 --> 00:03:23,069
specific page 404 Zoar errors and

00:03:18,769 --> 00:03:24,510
finally sensor data so that's physical

00:03:23,069 --> 00:03:27,269
sensors out there in the world this is

00:03:24,510 --> 00:03:28,829
like the IOT use case so as I mentioned

00:03:27,269 --> 00:03:31,650
there are two different kinds of time

00:03:28,829 --> 00:03:34,650
series data there's regular time series

00:03:31,650 --> 00:03:37,919
which is samples at fixed intervals of

00:03:34,650 --> 00:03:40,169
time and then there are irregular time

00:03:37,919 --> 00:03:41,400
series which is event-driven stuff so

00:03:40,169 --> 00:03:43,919
this could be trades in a stock market

00:03:41,400 --> 00:03:46,590
it could be response times for

00:03:43,919 --> 00:03:48,239
individual requests to an API and the

00:03:46,590 --> 00:03:50,910
thing about irregular time series is

00:03:48,239 --> 00:03:53,760
that you can induce a regular time

00:03:50,910 --> 00:03:56,459
series from an irregular one right if

00:03:53,760 --> 00:03:58,680
you have a bunch of individual API

00:03:56,459 --> 00:04:00,900
requests that you're timing what if you

00:03:58,680 --> 00:04:03,329
say you you say like I want to look at

00:04:00,900 --> 00:04:06,120
the last four hours of data at one

00:04:03,329 --> 00:04:08,879
minute summaries of the men the max and

00:04:06,120 --> 00:04:10,829
the mean response times you basically

00:04:08,879 --> 00:04:13,769
just created a regular time series out

00:04:10,829 --> 00:04:18,209
of that underlying event stream so in

00:04:13,769 --> 00:04:19,260
flux TB has has a pretty simple API it

00:04:18,209 --> 00:04:22,620
has two endpoints

00:04:19,260 --> 00:04:24,719
it's an HTTP API there's one to write

00:04:22,620 --> 00:04:26,849
data right you just do a post to here

00:04:24,719 --> 00:04:27,810
you specify the database you're writing

00:04:26,849 --> 00:04:30,960
to

00:04:27,810 --> 00:04:34,530
and your username/password there's a get

00:04:30,960 --> 00:04:36,180
so you can send a query here and every

00:04:34,530 --> 00:04:37,560
everything else is done through the

00:04:36,180 --> 00:04:40,290
query language which I'll show off in a

00:04:37,560 --> 00:04:42,510
second here's what the the post the

00:04:40,290 --> 00:04:44,820
right looks like so we created a like a

00:04:42,510 --> 00:04:48,390
line protocol to represent time series

00:04:44,820 --> 00:04:50,160
data that's this so the structure of the

00:04:48,390 --> 00:04:52,770
data is you feed it in is you have a

00:04:50,160 --> 00:04:54,150
measurement name which is a string you

00:04:52,770 --> 00:04:55,680
have tags which are key value pairs

00:04:54,150 --> 00:05:00,480
where the values are strings and this is

00:04:55,680 --> 00:05:04,110
metadata that we index and then you have

00:05:00,480 --> 00:05:05,970
fields technically you can have an

00:05:04,110 --> 00:05:08,490
unlimited number of fields so fields are

00:05:05,970 --> 00:05:11,610
key value pairs where the value can be a

00:05:08,490 --> 00:05:14,760
bunch of different types you can have in

00:05:11,610 --> 00:05:17,190
64 float64 strings or bools

00:05:14,760 --> 00:05:18,560
so as I mentioned you can technically

00:05:17,190 --> 00:05:20,640
you can have unlimited fields but

00:05:18,560 --> 00:05:22,590
realistically you probably would want to

00:05:20,640 --> 00:05:26,550
have fewer than you know say a thousand

00:05:22,590 --> 00:05:28,980
so our collection agent will collect say

00:05:26,550 --> 00:05:31,080
for example CPU statistics and every

00:05:28,980 --> 00:05:34,410
unique measurement under CPU will be a

00:05:31,080 --> 00:05:37,680
different field assuming the line

00:05:34,410 --> 00:05:39,360
protocol floats must have a decimal and

00:05:37,680 --> 00:05:42,180
the types must remain consistent over

00:05:39,360 --> 00:05:45,720
time and finally the time stamp is a

00:05:42,180 --> 00:05:48,660
nanosecond epoch which we surprisingly

00:05:45,720 --> 00:05:50,780
we have people who use nanosecond scale

00:05:48,660 --> 00:05:55,200
time stamps largely we've seen this in

00:05:50,780 --> 00:05:57,570
quantum computing use cases and also for

00:05:55,200 --> 00:06:00,390
some high frequency trading firms they

00:05:57,570 --> 00:06:02,610
track their network gear at that level

00:06:00,390 --> 00:06:04,080
of granularity because they have atomic

00:06:02,610 --> 00:06:07,170
clocks that make sure everything is in

00:06:04,080 --> 00:06:08,910
sync globally so we have a command-line

00:06:07,170 --> 00:06:11,640
interface where you can just spin it up

00:06:08,910 --> 00:06:13,860
and with row queries at it so let's walk

00:06:11,640 --> 00:06:15,570
through a few of the queries so you have

00:06:13,860 --> 00:06:18,060
a concept of a database you can create a

00:06:15,570 --> 00:06:20,490
database with the query language you can

00:06:18,060 --> 00:06:25,860
create a retention policy right so it

00:06:20,490 --> 00:06:29,520
has a name it applies to a database and

00:06:25,860 --> 00:06:31,920
it has a duration and this default part

00:06:29,520 --> 00:06:33,960
says by default all rights and queries

00:06:31,920 --> 00:06:36,000
will hit this retention policy you can

00:06:33,960 --> 00:06:38,520
change that at the time of a write or at

00:06:36,000 --> 00:06:39,520
the time of a query the other thing to

00:06:38,520 --> 00:06:43,270
note is that

00:06:39,520 --> 00:06:44,800
the the when you write data in like

00:06:43,270 --> 00:06:46,930
other than creating a database in a

00:06:44,800 --> 00:06:48,610
retention policy there's no other like

00:06:46,930 --> 00:06:50,110
formal set up that you have to do it's

00:06:48,610 --> 00:06:51,460
not like a sequel database where you

00:06:50,110 --> 00:06:53,620
have to create tables and they have a

00:06:51,460 --> 00:06:55,210
schema and all this other stuff once

00:06:53,620 --> 00:06:57,430
you've created a database in retention

00:06:55,210 --> 00:06:59,319
policy you just throw the data at it and

00:06:57,430 --> 00:07:04,360
it will like create the schema for you

00:06:59,319 --> 00:07:05,830
on the fly so with the this use case

00:07:04,360 --> 00:07:07,539
with the time series use case we found

00:07:05,830 --> 00:07:09,580
that you know in addition to actually

00:07:07,539 --> 00:07:10,840
querying the raw time series data and

00:07:09,580 --> 00:07:12,880
getting summaries and all this other

00:07:10,840 --> 00:07:14,860
stuff discovery actually becomes pretty

00:07:12,880 --> 00:07:16,690
important especially in infrastructure

00:07:14,860 --> 00:07:18,280
monitoring where you could have

00:07:16,690 --> 00:07:20,110
thousands of servers and a bunch of

00:07:18,280 --> 00:07:22,900
different services and you may not even

00:07:20,110 --> 00:07:25,539
know what data is available for you to

00:07:22,900 --> 00:07:27,669
query right so we wanted to make sure

00:07:25,539 --> 00:07:30,699
that people could do discovery on what

00:07:27,669 --> 00:07:33,430
data exists that I can work with so for

00:07:30,699 --> 00:07:34,990
that we basically have a separate part

00:07:33,430 --> 00:07:36,970
of the database which is essentially an

00:07:34,990 --> 00:07:38,830
inverted index now when people think

00:07:36,970 --> 00:07:40,750
about inverted indexes normally they

00:07:38,830 --> 00:07:43,060
think about using it for full-text

00:07:40,750 --> 00:07:44,740
search where you're mapping terms that

00:07:43,060 --> 00:07:47,620
appear in documents to those document

00:07:44,740 --> 00:07:50,440
IDs in our case what we're doing is

00:07:47,620 --> 00:07:53,139
we're mapping measurement names and tag

00:07:50,440 --> 00:07:55,870
key value pairs to the series that they

00:07:53,139 --> 00:07:57,340
appear in so I'll show some of these

00:07:55,870 --> 00:07:58,659
discovery queries and then I'll show

00:07:57,340 --> 00:08:00,520
some of the actual queries where we're

00:07:58,659 --> 00:08:02,740
you know doing computations on the time

00:08:00,520 --> 00:08:05,770
series data so here we can see what

00:08:02,740 --> 00:08:07,690
measurements exist we can see what

00:08:05,770 --> 00:08:11,409
measurements do we have for one specific

00:08:07,690 --> 00:08:15,400
host if host where host is a tag and

00:08:11,409 --> 00:08:17,889
server a is a tag value we can see what

00:08:15,400 --> 00:08:20,370
tag keys we have we can see what tag

00:08:17,889 --> 00:08:23,710
Keys we have on a specific measurement

00:08:20,370 --> 00:08:26,080
we can also see what tag values right so

00:08:23,710 --> 00:08:29,469
here show tag values from CPU with key

00:08:26,080 --> 00:08:31,930
region this will show us what regions

00:08:29,469 --> 00:08:33,969
were actually collecting CPU values for

00:08:31,930 --> 00:08:36,310
or it could it's useful like if you said

00:08:33,969 --> 00:08:38,440
show tag values from CPU with key equals

00:08:36,310 --> 00:08:41,310
host right that shows you which host

00:08:38,440 --> 00:08:43,990
you're actually collecting CPU stats for

00:08:41,310 --> 00:08:47,589
show series which are all the underlying

00:08:43,990 --> 00:08:50,500
series and then you can filter down the

00:08:47,589 --> 00:08:52,540
series by tag key value pairs and with

00:08:50,500 --> 00:08:54,910
the you know the where

00:08:52,540 --> 00:08:56,950
equals blah you can also do predicates

00:08:54,910 --> 00:08:59,290
and stuff like that and some other key

00:08:56,950 --> 00:08:59,880
equals some other value or all that kind

00:08:59,290 --> 00:09:03,430
of stuff

00:08:59,880 --> 00:09:06,700
alright so let's jump into some queries

00:09:03,430 --> 00:09:08,470
and show this show off some of that so

00:09:06,700 --> 00:09:10,150
like I said it looks kind of like SQL

00:09:08,470 --> 00:09:12,250
you know should feel somewhat familiar

00:09:10,150 --> 00:09:14,110
but it's a little bit different so in

00:09:12,250 --> 00:09:16,930
this case we're just getting all of the

00:09:14,110 --> 00:09:18,460
fields from some series for the last

00:09:16,930 --> 00:09:21,070
hour basically just give me the last

00:09:18,460 --> 00:09:25,900
hour of time series data from from that

00:09:21,070 --> 00:09:28,840
measurement here what we're doing is

00:09:25,900 --> 00:09:31,930
we're saying give me the 90th percentile

00:09:28,840 --> 00:09:34,960
of value from the CPU measurement for

00:09:31,930 --> 00:09:39,700
the last day in ten minute windows of

00:09:34,960 --> 00:09:42,250
time another thing we could do is we

00:09:39,700 --> 00:09:44,770
could add a group I say region or a

00:09:42,250 --> 00:09:48,070
group I host and we did a group by host

00:09:44,770 --> 00:09:50,860
we would get a separate time series for

00:09:48,070 --> 00:09:53,740
each individual host that we have right

00:09:50,860 --> 00:09:57,910
so if it's ten minutes of time one day

00:09:53,740 --> 00:10:03,990
that's a hundred and forty-four data

00:09:57,910 --> 00:10:07,960
points that you get back per host so as

00:10:03,990 --> 00:10:09,460
I mentioned our field types who can

00:10:07,960 --> 00:10:11,710
support different kinds so this is

00:10:09,460 --> 00:10:13,480
actually pretty unique to us as a time

00:10:11,710 --> 00:10:16,540
series database most time series

00:10:13,480 --> 00:10:18,970
databases only support either in 64 or

00:10:16,540 --> 00:10:20,800
float 64 but we actually support strings

00:10:18,970 --> 00:10:22,270
and bullying's as well and what that

00:10:20,800 --> 00:10:25,630
means is you can do interesting things

00:10:22,270 --> 00:10:27,700
with the string fields right you could

00:10:25,630 --> 00:10:30,610
in addition to your metrics data you can

00:10:27,700 --> 00:10:33,000
write in log and annotation data to give

00:10:30,610 --> 00:10:36,130
you more information and you can match

00:10:33,000 --> 00:10:38,590
against a regex for example so for

00:10:36,130 --> 00:10:41,830
writing a bunch of log values log lines

00:10:38,590 --> 00:10:44,620
into this we can say oh look look at

00:10:41,830 --> 00:10:46,540
that and the thing is because we have a

00:10:44,620 --> 00:10:48,250
time in there that actually is a pretty

00:10:46,540 --> 00:10:50,650
efficient query right normally you don't

00:10:48,250 --> 00:10:53,050
want to grab through your entire log but

00:10:50,650 --> 00:10:55,540
if you add time filtering and

00:10:53,050 --> 00:10:58,450
potentially also say filtering by a host

00:10:55,540 --> 00:11:01,630
or a specific service name this can be

00:10:58,450 --> 00:11:04,840
quite fast and you can also match

00:11:01,630 --> 00:11:07,480
against a regex on a tag right

00:11:04,840 --> 00:11:09,400
you can say Oh give me you know give me

00:11:07,480 --> 00:11:12,460
the time series data for any host that

00:11:09,400 --> 00:11:16,930
matches this this regex for this window

00:11:12,460 --> 00:11:18,700
of time so we have a bunch like here you

00:11:16,930 --> 00:11:20,650
see percentiles so that's like one of

00:11:18,700 --> 00:11:22,750
the functions we have we have a bunch of

00:11:20,650 --> 00:11:24,790
different functions in the language min

00:11:22,750 --> 00:11:26,020
max percentile first lasts all these

00:11:24,790 --> 00:11:29,440
different things we're actually adding

00:11:26,020 --> 00:11:30,880
to these quite a bit we're doing a lot

00:11:29,440 --> 00:11:34,510
of work on the query language right now

00:11:30,880 --> 00:11:37,110
to expose a lot more functionality so

00:11:34,510 --> 00:11:40,240
really quickly here's what a times

00:11:37,110 --> 00:11:41,350
continuous query looks like so the the

00:11:40,240 --> 00:11:44,140
new stuff is basically we have they

00:11:41,350 --> 00:11:46,630
create continuous query we name it apply

00:11:44,140 --> 00:11:49,240
to specific database write the Select

00:11:46,630 --> 00:11:51,610
count looks familiar and then a new

00:11:49,240 --> 00:11:56,140
thing is into so we're basically feeding

00:11:51,610 --> 00:12:02,830
it into a specific database and

00:11:56,140 --> 00:12:04,360
measurement so that's big so all that

00:12:02,830 --> 00:12:06,730
stuff's about that the actual database

00:12:04,360 --> 00:12:09,340
itself but as I mentioned we saw that

00:12:06,730 --> 00:12:11,830
people were facing common problems in

00:12:09,340 --> 00:12:14,380
time series data right they had to

00:12:11,830 --> 00:12:16,030
collect it they have to store it they

00:12:14,380 --> 00:12:17,920
have to visualize it and they have to

00:12:16,030 --> 00:12:20,410
process it so that they can monitor it

00:12:17,920 --> 00:12:22,960
so we built basically a bunch of

00:12:20,410 --> 00:12:24,640
different components for this right we

00:12:22,960 --> 00:12:27,250
have a collection agent called telegraph

00:12:24,640 --> 00:12:29,470
we have for visualization for

00:12:27,250 --> 00:12:30,640
dashboarding or basically drilling down

00:12:29,470 --> 00:12:34,210
in things we have cronograph

00:12:30,640 --> 00:12:36,430
for our storage we have influx DB the

00:12:34,210 --> 00:12:37,720
database and then finally for monitoring

00:12:36,430 --> 00:12:40,270
and processing the data we have

00:12:37,720 --> 00:12:42,300
capacitor so really quickly I'll cover

00:12:40,270 --> 00:12:44,800
each each one of these pieces so

00:12:42,300 --> 00:12:48,090
Telegraph is the agent it's also open

00:12:44,800 --> 00:12:50,110
source MIT licensed it's a written Ingo

00:12:48,090 --> 00:12:51,130
and it's basically an agent that you

00:12:50,110 --> 00:12:52,690
would deploy it across your entire

00:12:51,130 --> 00:12:55,000
infrastructure right you deploy it on

00:12:52,690 --> 00:12:57,370
every single host and it can do things

00:12:55,000 --> 00:13:00,250
like collect system metrics but it can

00:12:57,370 --> 00:13:02,830
also collect you know metrics for

00:13:00,250 --> 00:13:05,440
well-known services all that kind of

00:13:02,830 --> 00:13:07,600
stuff we have a bunch of different we

00:13:05,440 --> 00:13:09,130
call those input plugins so I think we

00:13:07,600 --> 00:13:10,930
have about a hundred input plugins at

00:13:09,130 --> 00:13:13,890
this point most of those are actually

00:13:10,930 --> 00:13:17,380
contributed by the open source community

00:13:13,890 --> 00:13:17,720
we we wrote like a small number of them

00:13:17,380 --> 00:13:19,430
in the

00:13:17,720 --> 00:13:21,889
we have people continually contributing

00:13:19,430 --> 00:13:24,110
new open-source plugins which is good

00:13:21,889 --> 00:13:25,610
because as the agent pictures it just

00:13:24,110 --> 00:13:27,019
means there's more pieces of your

00:13:25,610 --> 00:13:29,899
infrastructure you can get visibility

00:13:27,019 --> 00:13:31,160
into for instance like we have a bunch

00:13:29,899 --> 00:13:33,439
of plugins for collecting stuff on

00:13:31,160 --> 00:13:35,269
windows boxes and I don't think a single

00:13:33,439 --> 00:13:37,730
developer in our company has a Windows

00:13:35,269 --> 00:13:41,269
box so we didn't write any of those

00:13:37,730 --> 00:13:43,310
things and then there are output plugins

00:13:41,269 --> 00:13:44,870
right we wanted Telegraph to be useful

00:13:43,310 --> 00:13:46,759
as an agent whether or not you're

00:13:44,870 --> 00:13:48,350
running other parts of our stack so you

00:13:46,759 --> 00:13:49,699
can actually output the data to other

00:13:48,350 --> 00:13:53,600
things like influx dB

00:13:49,699 --> 00:13:55,490
graphite Kafka I saw signal effects is

00:13:53,600 --> 00:13:59,269
here like they also have an output

00:13:55,490 --> 00:14:01,610
plugin for Telegraph so for

00:13:59,269 --> 00:14:04,430
visualization we have cronograph it's

00:14:01,610 --> 00:14:06,500
open source but it's a GPL licensed it's

00:14:04,430 --> 00:14:08,870
written in go and for the JavaScript

00:14:06,500 --> 00:14:11,959
portions of it we use reacts and

00:14:08,870 --> 00:14:14,449
digraphs for the visualization we think

00:14:11,959 --> 00:14:17,209
of it as a UI for administering the the

00:14:14,449 --> 00:14:19,939
tick stack and for doing ad hoc data

00:14:17,209 --> 00:14:22,009
exploration and visualization and you

00:14:19,939 --> 00:14:24,050
can also do the ash boarding as well and

00:14:22,009 --> 00:14:25,459
the other thing you can do is you can

00:14:24,050 --> 00:14:27,740
create monitoring and learning rules

00:14:25,459 --> 00:14:29,300
that get injected into capacitor so

00:14:27,740 --> 00:14:31,040
there's like a point-and-click UI where

00:14:29,300 --> 00:14:33,500
you can say you know monitor the server

00:14:31,040 --> 00:14:37,040
and if the CPU goes above this for this

00:14:33,500 --> 00:14:38,930
long then trigger an alert and in

00:14:37,040 --> 00:14:42,559
addition to all that it has like a query

00:14:38,930 --> 00:14:44,059
builder it has a tick script editor tick

00:14:42,559 --> 00:14:47,779
script is the scripting language we

00:14:44,059 --> 00:14:49,279
created in capacitor so this is what

00:14:47,779 --> 00:14:50,959
cronograph looks like this is like a

00:14:49,279 --> 00:14:53,569
dashboard that we had created like an

00:14:50,959 --> 00:14:55,339
example dashboard there's also a screen

00:14:53,569 --> 00:14:56,750
where if you're running telegraph and

00:14:55,339 --> 00:14:58,730
influx and cronograph

00:14:56,750 --> 00:15:00,529
then by default you get this view where

00:14:58,730 --> 00:15:02,149
you can see all the hosts in your

00:15:00,529 --> 00:15:03,949
infrastructure and then here on the

00:15:02,149 --> 00:15:06,860
right we have links to the different

00:15:03,949 --> 00:15:09,259
kind services that we're monitoring that

00:15:06,860 --> 00:15:11,089
we have built-in dashboards for but you

00:15:09,259 --> 00:15:13,220
can also create your own dashboards our

00:15:11,089 --> 00:15:16,399
goal is over time we're going to have

00:15:13,220 --> 00:15:17,990
dashboards like pre-canned dashboards

00:15:16,399 --> 00:15:23,720
for basically any well-known service

00:15:17,990 --> 00:15:26,209
basically any Telegraph input plug-in so

00:15:23,720 --> 00:15:27,920
this is the screen where this is like

00:15:26,209 --> 00:15:30,079
screen where we're creating an alert so

00:15:27,920 --> 00:15:31,220
you can create an alerting rule here we

00:15:30,079 --> 00:15:32,630
can drill down to

00:15:31,220 --> 00:15:34,070
like the different measurements and

00:15:32,630 --> 00:15:36,080
filtered by the tags and we see an

00:15:34,070 --> 00:15:38,060
example of what the data is that it's

00:15:36,080 --> 00:15:39,710
looking at so you can create different

00:15:38,060 --> 00:15:43,100
kinds of alerting rules there are

00:15:39,710 --> 00:15:45,260
absolute thresholds relative thresholds

00:15:43,100 --> 00:15:47,720
and then what we call a Deadman switch

00:15:45,260 --> 00:15:50,870
so basically if something stops sending

00:15:47,720 --> 00:15:53,390
data it'll trigger an alert you can do a

00:15:50,870 --> 00:15:55,790
lot more than this in like raw capacitor

00:15:53,390 --> 00:15:57,200
land with tick script but this is

00:15:55,790 --> 00:15:59,750
something we created so that you

00:15:57,200 --> 00:16:01,010
wouldn't have to know that to actually

00:15:59,750 --> 00:16:03,740
be able to create monitoring and

00:16:01,010 --> 00:16:05,840
learning rules yourself if you wanted to

00:16:03,740 --> 00:16:07,520
go more like DevOps e style on it you'd

00:16:05,840 --> 00:16:10,160
probably want to have your tick scripts

00:16:07,520 --> 00:16:12,800
there actually checked into code repos

00:16:10,160 --> 00:16:15,520
and managed you know through your normal

00:16:12,800 --> 00:16:18,440
like DevOps infrastructure management

00:16:15,520 --> 00:16:20,240
and then this is the data exploration

00:16:18,440 --> 00:16:22,190
screen so you can see the data either in

00:16:20,240 --> 00:16:23,630
a graph which we saw already in the

00:16:22,190 --> 00:16:25,730
dashboard stuff the other interesting

00:16:23,630 --> 00:16:27,590
thing I think is that you can show it

00:16:25,730 --> 00:16:28,790
just as like a table right so you can

00:16:27,590 --> 00:16:32,000
show a table of the data that you're

00:16:28,790 --> 00:16:34,580
getting back all right the last last

00:16:32,000 --> 00:16:38,210
piece the the processing piece which is

00:16:34,580 --> 00:16:41,810
capacitor so it's open source and MIT

00:16:38,210 --> 00:16:44,330
license also it's written in go and it's

00:16:41,810 --> 00:16:46,700
there to process monitor and alert on

00:16:44,330 --> 00:16:48,440
the data so you can you can alert on it

00:16:46,700 --> 00:16:51,050
you can act and execute on something

00:16:48,440 --> 00:16:53,360
that comes in like I mentioned we

00:16:51,050 --> 00:16:55,460
created a DSL for capacitor called tick

00:16:53,360 --> 00:16:57,470
script which is basically it's a

00:16:55,460 --> 00:17:00,110
declarative language that allows you to

00:16:57,470 --> 00:17:02,210
create complex rules around either

00:17:00,110 --> 00:17:05,660
transforming your data or monitoring for

00:17:02,210 --> 00:17:09,140
things and triggering things so it works

00:17:05,660 --> 00:17:11,000
both for streaming and batch you can

00:17:09,140 --> 00:17:12,470
have so it can actually subscribe to the

00:17:11,000 --> 00:17:14,900
entire data feed of what's going into

00:17:12,470 --> 00:17:17,959
influx DB and it can process that as a

00:17:14,900 --> 00:17:19,910
real live you know stream or it can work

00:17:17,959 --> 00:17:21,920
in batch mode where periodically you

00:17:19,910 --> 00:17:24,079
will say like Oh query the database and

00:17:21,920 --> 00:17:28,040
pull back like the last hour of data or

00:17:24,079 --> 00:17:30,110
whatever and the other thing is it has

00:17:28,040 --> 00:17:32,330
the ability to store data back in diem

00:17:30,110 --> 00:17:34,190
flux DB so that's the part where you can

00:17:32,330 --> 00:17:37,400
essentially use it to do transforms on

00:17:34,190 --> 00:17:40,370
your data or say if your trigging alerts

00:17:37,400 --> 00:17:42,620
and events those can get stored also as

00:17:40,370 --> 00:17:43,880
time series back into the database so

00:17:42,620 --> 00:17:45,970
later on when you're doing

00:17:43,880 --> 00:17:48,260
problem investigation are you doing like

00:17:45,970 --> 00:17:51,290
you know review of all these things you

00:17:48,260 --> 00:17:55,430
have the actual alerts as time series

00:17:51,290 --> 00:17:57,680
data as well and there's something we

00:17:55,430 --> 00:17:59,360
call user-defined functions so our goal

00:17:57,680 --> 00:18:02,030
with capacitor is to provide as much as

00:17:59,360 --> 00:18:03,710
we can out of the box via tick script so

00:18:02,030 --> 00:18:05,390
that you can do all sorts of custom

00:18:03,710 --> 00:18:07,430
stuff but we know we're not going to hit

00:18:05,390 --> 00:18:09,230
every single use case so we wanted

00:18:07,430 --> 00:18:11,630
people to be able to write their own

00:18:09,230 --> 00:18:14,270
code to execute so we have examples in

00:18:11,630 --> 00:18:16,550
go and in Python and basically what it

00:18:14,270 --> 00:18:18,380
is is you can write your own code and as

00:18:16,550 --> 00:18:21,230
long as it can communicate over a socket

00:18:18,380 --> 00:18:22,640
and do deal with protobufs then you can

00:18:21,230 --> 00:18:24,950
write custom code to do whatever you

00:18:22,640 --> 00:18:27,160
want so we have an example up on our

00:18:24,950 --> 00:18:28,400
documentation where we're using

00:18:27,160 --> 00:18:30,290
tensorflow

00:18:28,400 --> 00:18:32,480
to do anomaly detection of the time

00:18:30,290 --> 00:18:36,220
series data and that's run as a

00:18:32,480 --> 00:18:38,900
user-defined function in capacitor and

00:18:36,220 --> 00:18:41,750
then the other thing is we add we also

00:18:38,900 --> 00:18:43,610
what so most of our historically our

00:18:41,750 --> 00:18:45,110
system has been push based it's

00:18:43,610 --> 00:18:47,240
basically push based monitoring system

00:18:45,110 --> 00:18:49,130
but we've seen all the work that's been

00:18:47,240 --> 00:18:51,020
happening with Prometheus and the people

00:18:49,130 --> 00:18:53,240
who are you know fans of Boardman and

00:18:51,020 --> 00:18:55,010
Google so we wanted to start adding more

00:18:53,240 --> 00:18:56,750
and more support for the pull based

00:18:55,010 --> 00:19:00,080
mechanism as well so you could do both

00:18:56,750 --> 00:19:01,760
so we think capacitor we added code we

00:19:00,080 --> 00:19:04,430
actually pulled in the Prometheus code

00:19:01,760 --> 00:19:05,660
for doing service discovery so that can

00:19:04,430 --> 00:19:08,330
connect with your service discovery

00:19:05,660 --> 00:19:10,280
mechanism and actually scrape targets

00:19:08,330 --> 00:19:14,120
that are actually exposing metrics

00:19:10,280 --> 00:19:17,840
endpoints so work for both alright

00:19:14,120 --> 00:19:22,010
that's my end of the talk I'll hand it

00:19:17,840 --> 00:19:24,980
over to Tomas everyone I'm Tomas

00:19:22,010 --> 00:19:27,590
Chaudhary and I work at Oracle data

00:19:24,980 --> 00:19:30,890
cloud but specifically the data logic

00:19:27,590 --> 00:19:32,450
side of Oracle data cloud and I just

00:19:30,890 --> 00:19:35,690
wanted to talk to you about how we've

00:19:32,450 --> 00:19:39,410
been using the TIC stack everything from

00:19:35,690 --> 00:19:42,560
influx data for quite a while now in

00:19:39,410 --> 00:19:46,970
order to monitor our infrastructure but

00:19:42,560 --> 00:19:51,890
namely the mesas stacks that we give to

00:19:46,970 --> 00:19:53,390
dev teams to do their workloads so I

00:19:51,890 --> 00:19:56,360
wanted to start off with just talking

00:19:53,390 --> 00:19:58,410
about what really you know my team and I

00:19:56,360 --> 00:20:01,460
kind of like focus

00:19:58,410 --> 00:20:03,990
so you know our our job is really to

00:20:01,460 --> 00:20:06,000
lessen the infrastructure burden as much

00:20:03,990 --> 00:20:13,230
as possible on dev teams you know the

00:20:06,000 --> 00:20:17,250
traditional DevOps kind of focus so when

00:20:13,230 --> 00:20:19,280
I got to OTC we definitely had a lot of

00:20:17,250 --> 00:20:21,570
work to do in order to make that

00:20:19,280 --> 00:20:24,210
infrastructure services us turnkey as

00:20:21,570 --> 00:20:28,590
possible we also didn't have all the

00:20:24,210 --> 00:20:30,260
teams using containerization as as their

00:20:28,590 --> 00:20:34,230
there either they didn't have any

00:20:30,260 --> 00:20:38,850
containerization ability to containerize

00:20:34,230 --> 00:20:40,650
their workloads or some teams had been

00:20:38,850 --> 00:20:42,660
doing it and taking on the burden of

00:20:40,650 --> 00:20:45,950
managing their own infrastructure so

00:20:42,660 --> 00:20:49,500
what we really really try to do is

00:20:45,950 --> 00:20:53,310
provide those turnkey components for the

00:20:49,500 --> 00:20:54,660
business and then with that you know the

00:20:53,310 --> 00:20:56,970
best thing you want to do is try to

00:20:54,660 --> 00:21:00,150
integrate your metrics and monitoring

00:20:56,970 --> 00:21:03,350
right into the stack so for the the meso

00:21:00,150 --> 00:21:06,450
side of things just like the entire

00:21:03,350 --> 00:21:10,230
parts of our stack we have monitoring

00:21:06,450 --> 00:21:17,300
hooked in with influx data from the

00:21:10,230 --> 00:21:21,090
get-go our containerization stack so

00:21:17,300 --> 00:21:24,180
right now we are using a template isin

00:21:21,090 --> 00:21:26,370
mesas cluster and we provide to the

00:21:24,180 --> 00:21:30,000
teams a way to do heterogeneous

00:21:26,370 --> 00:21:34,170
workloads each team gets their own

00:21:30,000 --> 00:21:37,260
cluster per cloud account originally we

00:21:34,170 --> 00:21:39,120
kind of had the the issue where we

00:21:37,260 --> 00:21:43,080
didn't support heterogenous workloads

00:21:39,120 --> 00:21:45,270
and it was kind of kind of a lot of

00:21:43,080 --> 00:21:48,420
effort on our part to get to a place

00:21:45,270 --> 00:21:52,710
where teams could deploy different kind

00:21:48,420 --> 00:21:56,250
of big data applications so now it's

00:21:52,710 --> 00:21:58,680
it's nice to know that we can afford for

00:21:56,250 --> 00:22:00,990
this kind of workload to dev teams and

00:21:58,680 --> 00:22:03,360
and we wouldn't have been able to do

00:22:00,990 --> 00:22:06,270
that have we not had the the monitoring

00:22:03,360 --> 00:22:09,720
baked in to see like how teams are

00:22:06,270 --> 00:22:11,010
utilizing the infrastructure seeing

00:22:09,720 --> 00:22:12,900
where we can

00:22:11,010 --> 00:22:15,990
if make them the meso stack is

00:22:12,900 --> 00:22:19,170
performant as possible I forgot to

00:22:15,990 --> 00:22:22,100
mention earlier our teens like we focus

00:22:19,170 --> 00:22:24,840
on providing then marathon and

00:22:22,100 --> 00:22:29,130
singularity on top of mesas and that's

00:22:24,840 --> 00:22:32,580
kind of like our focus to keep that's

00:22:29,130 --> 00:22:34,830
that's how we basically allow dev teams

00:22:32,580 --> 00:22:42,630
to containerize their workloads onto on

00:22:34,830 --> 00:22:45,570
top of mesas we we now have we have an

00:22:42,630 --> 00:22:49,190
internal watchdog service that monitors

00:22:45,570 --> 00:22:52,380
the each individual mesas cluster and it

00:22:49,190 --> 00:22:54,540
it looks for opportunities to scale up

00:22:52,380 --> 00:22:59,030
and down the cluster so that's really

00:22:54,540 --> 00:23:01,830
important for us just to control costs

00:22:59,030 --> 00:23:04,140
we take all of that event data and we

00:23:01,830 --> 00:23:07,050
actually persist it back to influx DB so

00:23:04,140 --> 00:23:10,680
that we can graph to the dev teams a way

00:23:07,050 --> 00:23:12,870
to see oh look at this point in my

00:23:10,680 --> 00:23:14,730
cluster the utilization went up so

00:23:12,870 --> 00:23:21,290
definitely the watchdog service went

00:23:14,730 --> 00:23:21,290
ahead and scaled up my my my stack

00:23:22,410 --> 00:23:29,610
additionally so we we capture as much

00:23:26,670 --> 00:23:32,970
metrics as as we have on as possible

00:23:29,610 --> 00:23:36,180
through Telegraph and we persisted back

00:23:32,970 --> 00:23:41,190
to influx DB I'll talk a little bit more

00:23:36,180 --> 00:23:43,230
about that Mesa R this is the stack

00:23:41,190 --> 00:23:46,200
basically right now in its current

00:23:43,230 --> 00:23:48,750
incarnation we're running a 1.3 mesas

00:23:46,200 --> 00:23:51,840
the latest marathon in singularity

00:23:48,750 --> 00:23:53,910
marathon lb and try to manage zookeeper

00:23:51,840 --> 00:24:00,390
as much as best as possible with

00:23:53,910 --> 00:24:03,960
exhibitor so a little bit more about the

00:24:00,390 --> 00:24:08,220
actual stack we're running a single very

00:24:03,960 --> 00:24:10,140
large influx DB instance that's per

00:24:08,220 --> 00:24:15,120
cloud account right now we're running at

00:24:10,140 --> 00:24:17,760
8 VP use and 32 gigs of ram we have a

00:24:15,120 --> 00:24:21,000
default retention policy that's 90 days

00:24:17,760 --> 00:24:23,700
but dev teams can kind of request to

00:24:21,000 --> 00:24:24,390
have larger retention policies for their

00:24:23,700 --> 00:24:26,970
own into

00:24:24,390 --> 00:24:30,000
visual databases that gives them the

00:24:26,970 --> 00:24:33,000
ability to since we make the the

00:24:30,000 --> 00:24:34,890
database instance multi-tenant dev teams

00:24:33,000 --> 00:24:36,000
can send their application metrics and

00:24:34,890 --> 00:24:37,590
then of course if they want to keep

00:24:36,000 --> 00:24:41,880
their metrics longer that there are four

00:24:37,590 --> 00:24:45,150
to that ability to do so and right now

00:24:41,880 --> 00:24:48,660
we are working on migrating to

00:24:45,150 --> 00:24:52,460
enterprise so that we can have a more

00:24:48,660 --> 00:24:54,990
spread-out workload a more sharded setup

00:24:52,460 --> 00:24:57,120
so on the telegraph side of things we

00:24:54,990 --> 00:25:00,870
have Telegraph installed by default in

00:24:57,120 --> 00:25:04,140
every node when I first started building

00:25:00,870 --> 00:25:06,480
this stack at data logics initially we

00:25:04,140 --> 00:25:08,970
were using collecti that's what I just

00:25:06,480 --> 00:25:11,070
had most experience with and at that

00:25:08,970 --> 00:25:13,290
time Telegraph didn't have the plugins

00:25:11,070 --> 00:25:15,720
that we kind of needed but it was nice

00:25:13,290 --> 00:25:18,690
to see that very quickly you know influx

00:25:15,720 --> 00:25:21,120
data was able to get a lot of the the

00:25:18,690 --> 00:25:22,650
plugins that we needed going forward to

00:25:21,120 --> 00:25:25,560
kind of appease all the different

00:25:22,650 --> 00:25:27,510
infrastructure use cases at data logics

00:25:25,560 --> 00:25:30,120
so it's very easier for us to kind of

00:25:27,510 --> 00:25:34,680
like remove collected and just go back

00:25:30,120 --> 00:25:37,170
to tell and go to Telegraph that gave us

00:25:34,680 --> 00:25:39,270
many of the plugins and with maysa and

00:25:37,170 --> 00:25:42,930
doc docker and like paul was saying

00:25:39,270 --> 00:25:47,070
earlier we have dev teams that do deploy

00:25:42,930 --> 00:25:48,960
onto windows infrastructure and it was

00:25:47,070 --> 00:25:55,680
nice to kind of like have that dual

00:25:48,960 --> 00:25:58,080
purpose so yeah with that the the

00:25:55,680 --> 00:26:01,110
ecosystem itself is also all the plugins

00:25:58,080 --> 00:26:03,360
we've never had seen any issues they're

00:26:01,110 --> 00:26:06,420
very performant everything is written in

00:26:03,360 --> 00:26:08,460
go for our meso stacks these are our key

00:26:06,420 --> 00:26:12,660
plugins that we currently use maces

00:26:08,460 --> 00:26:15,480
docker zookeeper and HTTP response it's

00:26:12,660 --> 00:26:17,670
probably not worth it's probably worth

00:26:15,480 --> 00:26:21,420
your time to go into github and just see

00:26:17,670 --> 00:26:23,730
the list of all the metrics that the

00:26:21,420 --> 00:26:27,380
Mesa and docker plugins alone pull for

00:26:23,730 --> 00:26:27,380
you because there is a lot

00:26:29,870 --> 00:26:34,880
going forward with that I mean we have

00:26:32,120 --> 00:26:37,880
for our learning we have capacitor

00:26:34,880 --> 00:26:40,490
obviously we we template eyes all of our

00:26:37,880 --> 00:26:43,670
tick scripts of dev teams can very

00:26:40,490 --> 00:26:45,920
quickly get some base alerting and then

00:26:43,670 --> 00:26:48,830
we try to work with them to figure out

00:26:45,920 --> 00:26:52,940
what kind of other alerting paradigms

00:26:48,830 --> 00:26:57,250
they need so we try to add more tick

00:26:52,940 --> 00:26:57,250
scripts into our capacitor ecosystem

00:26:57,550 --> 00:27:06,110
let's see we have also provided dev

00:27:04,820 --> 00:27:08,480
teams and ability to run their own

00:27:06,110 --> 00:27:10,280
capacitor containers so that way they

00:27:08,480 --> 00:27:12,230
can manage to their own tip tick scripts

00:27:10,280 --> 00:27:15,200
so that's been a kind of a nice unique

00:27:12,230 --> 00:27:17,050
use case for us where we can like make

00:27:15,200 --> 00:27:20,780
even the capacitor side of things

00:27:17,050 --> 00:27:23,090
self-service in addition we we integrate

00:27:20,780 --> 00:27:26,059
all of that alerting with capacitor into

00:27:23,090 --> 00:27:29,300
our chat and on-call systems so kind of

00:27:26,059 --> 00:27:31,820
have a end-to-end workflow from the

00:27:29,300 --> 00:27:34,760
metrics gathering all the way to the the

00:27:31,820 --> 00:27:37,610
inspection into the database via a

00:27:34,760 --> 00:27:41,270
capacitor and then sending any alerts

00:27:37,610 --> 00:27:44,450
that gets triggered into any on-call

00:27:41,270 --> 00:27:46,370
system that we have set up so for me

00:27:44,450 --> 00:27:48,140
like capacitor has been a very good

00:27:46,370 --> 00:27:50,920
Swiss Army tool I mean there's so many

00:27:48,140 --> 00:27:53,270
use cases for it it's like probably

00:27:50,920 --> 00:27:55,940
creativity is probably the only limiting

00:27:53,270 --> 00:28:00,110
factor I've been really happy to see

00:27:55,940 --> 00:28:02,440
that influx data has been even you know

00:28:00,110 --> 00:28:05,929
staying abreast with the industry and

00:28:02,440 --> 00:28:08,300
giving us the ability when we we

00:28:05,929 --> 00:28:10,880
potentially look at deploying with you

00:28:08,300 --> 00:28:13,010
know kubernetes in the future that we

00:28:10,880 --> 00:28:15,590
will have an ability to do some of the

00:28:13,010 --> 00:28:17,770
things that Prometheus is also doing

00:28:15,590 --> 00:28:17,770
currently

00:28:17,980 --> 00:28:24,530
so coronagraph there's there's been a

00:28:21,290 --> 00:28:27,230
lot of work in that area about it by

00:28:24,530 --> 00:28:28,670
influx data that's not the the

00:28:27,230 --> 00:28:31,250
visualization tool that we first

00:28:28,670 --> 00:28:33,500
selected when we started this work but

00:28:31,250 --> 00:28:34,010
we're keeping it kind of on the back

00:28:33,500 --> 00:28:36,320
burner

00:28:34,010 --> 00:28:39,140
we're really waiting for some some way

00:28:36,320 --> 00:28:41,750
to lock down the the GUI so that we have

00:28:39,140 --> 00:28:43,740
a better administrative control but I'm

00:28:41,750 --> 00:28:47,820
pretty happy to see what is coming

00:28:43,740 --> 00:28:49,559
down the pipe with that tool grrr fauna

00:28:47,820 --> 00:28:54,720
is the tool that we're using for all of

00:28:49,559 --> 00:28:59,850
our visualization we tried to pre build

00:28:54,720 --> 00:29:02,160
all of our graphs in Griffon oh so in in

00:28:59,850 --> 00:29:07,050
the case of the mesas clusters that we

00:29:02,160 --> 00:29:09,270
provide to dev teams all the graphs are

00:29:07,050 --> 00:29:11,910
already pre-built in grief on them we

00:29:09,270 --> 00:29:15,000
bring up the systems with our

00:29:11,910 --> 00:29:17,220
configuration management tools and based

00:29:15,000 --> 00:29:20,580
on the cluster names that are

00:29:17,220 --> 00:29:22,320
represented to the dev teams the dev

00:29:20,580 --> 00:29:25,200
teams are very quickly able to see oh

00:29:22,320 --> 00:29:27,720
these are all my metrics for my

00:29:25,200 --> 00:29:30,000
particular meso stack here are the the

00:29:27,720 --> 00:29:32,210
docker container metrics for the

00:29:30,000 --> 00:29:36,929
specific containers running on our stack

00:29:32,210 --> 00:29:38,850
and that that definitely has been very

00:29:36,929 --> 00:29:47,100
helpful in making that self-service as

00:29:38,850 --> 00:29:50,700
well so beyond just collecting metrics

00:29:47,100 --> 00:29:54,360
like the big like journey for us like I

00:29:50,700 --> 00:29:57,590
was saying earlier was not just you know

00:29:54,360 --> 00:30:00,090
giving dev teams the ability to use

00:29:57,590 --> 00:30:03,590
containerization but really to give then

00:30:00,090 --> 00:30:07,980
a new platform for doing deployment of

00:30:03,590 --> 00:30:12,179
business services so I always like to

00:30:07,980 --> 00:30:16,230
answer or try to answer you know let me

00:30:12,179 --> 00:30:18,600
just build a slide out a larger business

00:30:16,230 --> 00:30:21,000
questions so a lot of interesting

00:30:18,600 --> 00:30:23,880
questions that we had initially was okay

00:30:21,000 --> 00:30:26,370
if we do give dev teams an ASOS cluster

00:30:23,880 --> 00:30:29,040
are they going to be able to see a good

00:30:26,370 --> 00:30:33,870
clustered utilization for each one of

00:30:29,040 --> 00:30:35,610
those stacks what are we as an ops team

00:30:33,870 --> 00:30:38,460
when we're providing all of these

00:30:35,610 --> 00:30:41,340
clusters out to the different dev teams

00:30:38,460 --> 00:30:44,010
in the organization how do we see across

00:30:41,340 --> 00:30:45,480
the board the total utilization for all

00:30:44,010 --> 00:30:46,679
those mase those clusters that we're

00:30:45,480 --> 00:30:51,240
providing right because we're thinking

00:30:46,679 --> 00:30:53,370
of ourselves as a service team what what

00:30:51,240 --> 00:30:56,040
happened or how we capturing those

00:30:53,370 --> 00:30:57,330
cluster scaling events are we actually

00:30:56,040 --> 00:30:59,850
graphing that

00:30:57,330 --> 00:31:02,550
correctly and are we bubbling that up to

00:30:59,850 --> 00:31:05,550
the dev teams so that we can expose that

00:31:02,550 --> 00:31:07,620
when when you are running your services

00:31:05,550 --> 00:31:10,260
on am a so's platform you need to be

00:31:07,620 --> 00:31:14,190
able to see that are you using to it

00:31:10,260 --> 00:31:16,800
does that cluster have too many too many

00:31:14,190 --> 00:31:20,180
mesas agents running or not how do we

00:31:16,800 --> 00:31:23,750
basically try to reduce our cost Bend

00:31:20,180 --> 00:31:28,140
because that's very important to us

00:31:23,750 --> 00:31:30,510
sometimes we see that Oh even with our

00:31:28,140 --> 00:31:36,840
watchdog service that we are scaling the

00:31:30,510 --> 00:31:43,740
the cluster and sorry I lost my train of

00:31:36,840 --> 00:31:47,370
thought so to backtrack so that the the

00:31:43,740 --> 00:31:49,140
the big kind of onus on us like I was

00:31:47,370 --> 00:31:52,500
saying is to provide these mesas

00:31:49,140 --> 00:31:56,970
clusters out but the nature of our

00:31:52,500 --> 00:31:58,890
workloads are so are so disparate that

00:31:56,970 --> 00:32:02,070
it's been quite a bit of a challenge for

00:31:58,890 --> 00:32:04,530
us to kind of find like the best generic

00:32:02,070 --> 00:32:07,140
use case so as we've been building out

00:32:04,530 --> 00:32:09,090
these clusters through all the metrics

00:32:07,140 --> 00:32:10,920
that we've been gathering we've been

00:32:09,090 --> 00:32:13,710
learning a lot ourselves onto how to

00:32:10,920 --> 00:32:17,430
make these clusters more performance so

00:32:13,710 --> 00:32:19,620
that it can basically serve many dev

00:32:17,430 --> 00:32:21,090
teams and now it's like we have two

00:32:19,620 --> 00:32:26,250
dozen dev teams that were able to

00:32:21,090 --> 00:32:28,680
support at data logics but initially

00:32:26,250 --> 00:32:30,930
like with it was it was really hard

00:32:28,680 --> 00:32:33,150
until we added all those plugins with

00:32:30,930 --> 00:32:34,920
Telegraph to see exactly the visibility

00:32:33,150 --> 00:32:39,840
that was happening in each of these

00:32:34,920 --> 00:32:42,870
those clusters and then probably the the

00:32:39,840 --> 00:32:44,580
one big question we always ask is are we

00:32:42,870 --> 00:32:48,180
being successful with moving these dev

00:32:44,580 --> 00:32:51,210
teams to containerization how many how

00:32:48,180 --> 00:32:53,520
many services at Oracle data logics do

00:32:51,210 --> 00:32:55,830
we really have now at moved over to

00:32:53,520 --> 00:32:59,190
containers right answering that business

00:32:55,830 --> 00:33:01,680
question of are we actually making a

00:32:59,190 --> 00:33:05,840
good headway into moving into containers

00:33:01,680 --> 00:33:05,840
has been really important to us as well

00:33:05,990 --> 00:33:10,710
so here are some screenshots with taken

00:33:09,450 --> 00:33:14,640
from our productions

00:33:10,710 --> 00:33:18,890
of our Gravano hopefully you can see

00:33:14,640 --> 00:33:22,020
that but you can see that telegraph

00:33:18,890 --> 00:33:23,669
gives you quite a bit of metrics here

00:33:22,020 --> 00:33:25,950
we're looking at that's a little hard to

00:33:23,669 --> 00:33:28,950
see but we're looking at the meso stack

00:33:25,950 --> 00:33:30,299
so we're looking at the cluster a lot of

00:33:28,950 --> 00:33:32,610
the stuff you've seen in the meso is

00:33:30,299 --> 00:33:35,370
admin GUI is basically being pulled off

00:33:32,610 --> 00:33:39,080
and in in this slide you can see the

00:33:35,370 --> 00:33:41,669
number of killed lost your error rates

00:33:39,080 --> 00:33:43,679
and then in totality you can see how

00:33:41,669 --> 00:33:48,600
many tasks have been running on this

00:33:43,679 --> 00:33:52,890
cluster in this screenshot we're looking

00:33:48,600 --> 00:33:55,289
at our docker stats so this is a screen

00:33:52,890 --> 00:33:57,270
grab of all the different containers

00:33:55,289 --> 00:34:02,880
that are running on this particular

00:33:57,270 --> 00:34:05,159
cluster this you know this makes it very

00:34:02,880 --> 00:34:09,270
easy for any dev team to see okay I am

00:34:05,159 --> 00:34:12,990
running these these particular data

00:34:09,270 --> 00:34:15,810
logic services on my cluster so that

00:34:12,990 --> 00:34:17,280
definitely has been the the number of

00:34:15,810 --> 00:34:19,530
metrics that we get from the Telegraph

00:34:17,280 --> 00:34:23,730
plugin on the darker side of things has

00:34:19,530 --> 00:34:25,619
been very very useful as well here's a

00:34:23,730 --> 00:34:33,089
graph here's a dashboard that we built

00:34:25,619 --> 00:34:35,010
for bubbling up the marathon service

00:34:33,089 --> 00:34:37,530
health checks so you can see the three

00:34:35,010 --> 00:34:40,320
green boxes that shows us that marathon

00:34:37,530 --> 00:34:47,639
our front door is operating very well we

00:34:40,320 --> 00:34:49,859
have CPU metrics graphed below and so

00:34:47,639 --> 00:34:51,869
again to show you show KC if you look at

00:34:49,859 --> 00:34:54,659
the top left corner there's a cluster

00:34:51,869 --> 00:34:56,879
drop-down every every dev team can just

00:34:54,659 --> 00:34:59,790
very easily pop in to their cluster and

00:34:56,879 --> 00:35:03,990
see what is the state of their their

00:34:59,790 --> 00:35:07,170
containerization stack so additionally

00:35:03,990 --> 00:35:09,450
outside of of telegraph we we have our

00:35:07,170 --> 00:35:16,320
own services running and feeding back

00:35:09,450 --> 00:35:20,190
data into into influx in this case this

00:35:16,320 --> 00:35:22,800
is our own internal tool that we've been

00:35:20,190 --> 00:35:27,570
using to track all the mayor

00:35:22,800 --> 00:35:29,280
applications running in at ODC so here

00:35:27,570 --> 00:35:31,440
we're just starting to build out a

00:35:29,280 --> 00:35:33,420
little cleaner business intelligence

00:35:31,440 --> 00:35:35,820
dashboard where we can you know

00:35:33,420 --> 00:35:38,670
hopefully provide to the leadership team

00:35:35,820 --> 00:35:40,530
a very easy place to see how many how

00:35:38,670 --> 00:35:43,620
many business services are running in

00:35:40,530 --> 00:35:46,200
containers and then lastly I have a

00:35:43,620 --> 00:35:49,920
couple of screenshots here this is this

00:35:46,200 --> 00:35:53,910
is a at least the top three graphs you

00:35:49,920 --> 00:35:57,000
can see this is a view of all the the

00:35:53,910 --> 00:35:59,400
the latest incarnation of our mesas

00:35:57,000 --> 00:36:02,160
clusters and in production and you can

00:35:59,400 --> 00:36:04,530
see in the top and the totality of all

00:36:02,160 --> 00:36:08,340
the clusters that we provided out most

00:36:04,530 --> 00:36:12,900
recently we are only utilizing those

00:36:08,340 --> 00:36:14,700
clusters globally at about 25 percent so

00:36:12,900 --> 00:36:17,010
we have more work to do to try to get to

00:36:14,700 --> 00:36:20,070
a higher cluster utilization across the

00:36:17,010 --> 00:36:24,960
board in our dev workloads since we kind

00:36:20,070 --> 00:36:28,650
of have more ability to do more you know

00:36:24,960 --> 00:36:31,940
more quicker jobs we basically we do see

00:36:28,650 --> 00:36:34,980
a higher cluster utilization currently

00:36:31,940 --> 00:36:36,780
but even then we do have some dev teams

00:36:34,980 --> 00:36:38,970
because of the nature of their workload

00:36:36,780 --> 00:36:41,210
they're able to hit in this case like

00:36:38,970 --> 00:36:44,840
almost like a 60 percent utilization

00:36:41,210 --> 00:36:49,020
which is very nice to see

00:36:44,840 --> 00:36:51,540
so in summary so why influx data I mean

00:36:49,020 --> 00:36:53,760
overall it's been a great metrics tool

00:36:51,540 --> 00:36:56,820
and ecosystem for us at Oracle data

00:36:53,760 --> 00:36:59,550
logics and it really just helps us to

00:36:56,820 --> 00:37:02,400
you know keep fostering and providing

00:36:59,550 --> 00:37:07,140
end-to-end infrastructure services for

00:37:02,400 --> 00:37:10,080
the organization it's also allowed us to

00:37:07,140 --> 00:37:12,630
capture our own custom metrics and to

00:37:10,080 --> 00:37:15,090
keep pushing the needle in moving our

00:37:12,630 --> 00:37:17,550
dev teams over to containers

00:37:15,090 --> 00:37:19,500
lastly the the team at influx data

00:37:17,550 --> 00:37:22,920
they've been a great partner and the

00:37:19,500 --> 00:37:25,710
community has has also been very

00:37:22,920 --> 00:37:29,250
collaborative we've been able to give

00:37:25,710 --> 00:37:31,260
feedback and kind of help them kind of

00:37:29,250 --> 00:37:34,870
grow the ecosystem which in turn helps

00:37:31,260 --> 00:37:38,800
us move forward as well

00:37:34,870 --> 00:37:40,510
so thank you for your time if you're

00:37:38,800 --> 00:37:42,220
doing service discovery and you're

00:37:40,510 --> 00:37:45,690
running workloads in containers how do

00:37:42,220 --> 00:37:48,520
you tell Telegraph where to find those

00:37:45,690 --> 00:37:49,990
so-so Telegraph you is done through

00:37:48,520 --> 00:37:52,000
configuration it doesn't do service

00:37:49,990 --> 00:37:54,370
discovery so essentially you have to

00:37:52,000 --> 00:37:55,870
make sure that your you know deployment

00:37:54,370 --> 00:37:57,370
scripts and all that other stuff deploy

00:37:55,870 --> 00:38:00,160
it and maybe you can talk about what you

00:37:57,370 --> 00:38:02,830
guys do for Telegraph the service

00:38:00,160 --> 00:38:05,740
discovery stuff is really for pull based

00:38:02,830 --> 00:38:07,410
scrape targets so that you don't need to

00:38:05,740 --> 00:38:10,720
have a telegraph agent for like

00:38:07,410 --> 00:38:12,640
capacitor can do that and it integrates

00:38:10,720 --> 00:38:14,650
with it uses the exact same code that

00:38:12,640 --> 00:38:16,990
Prometheus uses for service discovery so

00:38:14,650 --> 00:38:19,420
any target service discovery target they

00:38:16,990 --> 00:38:21,820
support capacitor also supports and it

00:38:19,420 --> 00:38:24,070
will actually do the job of scraping the

00:38:21,820 --> 00:38:26,440
data and then because it's capacitor you

00:38:24,070 --> 00:38:28,300
can scrape it and either alike process

00:38:26,440 --> 00:38:29,830
it and alert on it right away or you can

00:38:28,300 --> 00:38:34,270
just board it to the database or you can

00:38:29,830 --> 00:38:35,920
do both yeah like I like why I was

00:38:34,270 --> 00:38:38,680
saying we installed Telegraph in every

00:38:35,920 --> 00:38:41,710
agent but for some of those nodes like a

00:38:38,680 --> 00:38:43,300
meso slave or a meso s-- master the way

00:38:41,710 --> 00:38:46,000
that we have our naming convention for

00:38:43,300 --> 00:38:48,040
all of our infrastructure and through

00:38:46,000 --> 00:38:49,600
configuration management management we

00:38:48,040 --> 00:38:51,670
can just do discovery and then inject

00:38:49,600 --> 00:38:54,640
the particular plugins that we want like

00:38:51,670 --> 00:38:57,670
docker or or mesas on to those

00:38:54,640 --> 00:39:00,100
particular nodes themselves so then we

00:38:57,670 --> 00:39:07,410
basically additively add more plugins

00:39:00,100 --> 00:39:07,410
depending on the node type yes

00:39:33,900 --> 00:39:39,750
so the question was can can tell a Graf

00:39:37,150 --> 00:39:43,780
send data to do two different locations

00:39:39,750 --> 00:39:45,820
or more specifically can you have one

00:39:43,780 --> 00:39:47,590
input plug-in send data to one place and

00:39:45,820 --> 00:39:50,470
another input plug-in send data to

00:39:47,590 --> 00:39:53,170
another place so that currently isn't

00:39:50,470 --> 00:39:55,330
possible it's on the roadmap right now

00:39:53,170 --> 00:39:57,190
Telegraph can send data to multiple

00:39:55,330 --> 00:40:01,450
locations but it sends data for all

00:39:57,190 --> 00:40:03,940
input plugins to the both locations or

00:40:01,450 --> 00:40:05,470
whatever it is so the app that's good to

00:40:03,940 --> 00:40:10,990
know we can we can definitely prioritize

00:40:05,470 --> 00:40:12,850
input plug-in specific yeah you can

00:40:10,990 --> 00:40:14,800
absolutely run n number of Telegraph

00:40:12,850 --> 00:40:16,330
instances for sure like that's what

00:40:14,800 --> 00:40:17,710
we've four people have asked for that

00:40:16,330 --> 00:40:20,460
kind of thing for now that's what we're

00:40:17,710 --> 00:40:20,460
telling them to do

00:40:30,430 --> 00:40:34,599
right so the the question was I said we

00:40:33,549 --> 00:40:36,369
were gonna add more functions to the

00:40:34,599 --> 00:40:41,770
query language what are those was it

00:40:36,369 --> 00:40:42,910
looked like so yeah so the big effort

00:40:41,770 --> 00:40:45,220
we're doing right now is we're actually

00:40:42,910 --> 00:40:47,380
creating a new query language that look

00:40:45,220 --> 00:40:48,549
that is functional it looks like a

00:40:47,380 --> 00:40:50,589
functional query language so the

00:40:48,549 --> 00:40:54,940
structure of it is it looks very much

00:40:50,589 --> 00:40:56,650
like like d3 or jQuery or something like

00:40:54,940 --> 00:40:59,170
that so basically there you can chain

00:40:56,650 --> 00:41:01,270
functions so basically the idea is you

00:40:59,170 --> 00:41:02,799
can view your time series data as a data

00:41:01,270 --> 00:41:04,900
frame and the functions are just

00:41:02,799 --> 00:41:07,510
transformations that you do on it right

00:41:04,900 --> 00:41:09,460
selecting arranged windowing it into

00:41:07,510 --> 00:41:12,970
5-minute intervals and then computing

00:41:09,460 --> 00:41:14,740
summaries on it so as we move to that

00:41:12,970 --> 00:41:17,950
we're gonna be adding more things so

00:41:14,740 --> 00:41:19,839
we'll be adding histograms as a function

00:41:17,950 --> 00:41:21,250
that you can do I'm interested in

00:41:19,839 --> 00:41:23,260
bringing in more workloads that

00:41:21,250 --> 00:41:27,640
currently you can only do in like pandas

00:41:23,260 --> 00:41:30,099
or R so things like doing you know K

00:41:27,640 --> 00:41:32,289
nearest neighbors on on a matrix of

00:41:30,099 --> 00:41:35,049
things to find what series of all these

00:41:32,289 --> 00:41:37,770
are like similar to each other so we'll

00:41:35,049 --> 00:41:41,049
be doing more and more of that actually

00:41:37,770 --> 00:41:42,549
quick plug we're having a in flex days

00:41:41,049 --> 00:41:45,039
conference in San Francisco on November

00:41:42,549 --> 00:41:46,650
14th and I'm giving a talk there which

00:41:45,039 --> 00:41:48,970
is gonna be like the first like

00:41:46,650 --> 00:41:50,849
unveiling in a talk of the new query

00:41:48,970 --> 00:41:53,849
language and the new functionality so

00:41:50,849 --> 00:41:53,849
yeah

00:41:58,599 --> 00:42:04,580
cool yeah thanks for one

00:42:01,410 --> 00:42:04,580

YouTube URL: https://www.youtube.com/watch?v=De8-M3_49zQ


