Title: Doing “DropBox” the Cloud Native Way
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Doing “DropBox” the Cloud Native Way - Jorg Schad, Mesosphere

Cloud Native architecture has slowly become the default way to build robust, scalable applications. How would you rebuild a large scale storage service such as Dropbox (please note Dropbox is just an example here and we have no plans of actually competing with Dropbox), using the Cloud Native architecture? In this presentation, Joerg and Nitish will discuss the Cloud Native architecture, its advantages, and then explain how to build a scalable, multi-tenant, Dropbox like cloud storage system using modern, containerized applications like Minio as the unstructured data/blob store, MongoDB for structured data, Redis for key value pairs etc, all orchestrated on DC/OS. The presentation will also include a live demo of the Dropbox equivalent product.

About 

Jörg Schad
Software Engineer, Mesosphere
Jörg is a software engineer at Mesosphere in Hamburg. In his previous life he implemented distributed and in memory databases and conducted research in the Hadoop and Cloud area. His speaking experience includes various Meetups, international conferences, and lecture halls.
Captions: 
	00:00:00,480 --> 00:00:06,960
welcome to my talk so this actually

00:00:03,750 --> 00:00:10,170
originally it was a pleasure talk with

00:00:06,960 --> 00:00:11,670
me neo and myself and unfortunately they

00:00:10,170 --> 00:00:14,670
couldn't make it so you just have to

00:00:11,670 --> 00:00:16,800
deal with myself today the other person

00:00:14,670 --> 00:00:19,560
who was supposed to jump beat joining

00:00:16,800 --> 00:00:20,670
him here on stage is Krishna and they

00:00:19,560 --> 00:00:22,380
are unfortunately I think they're in

00:00:20,670 --> 00:00:25,189
some kind of retreat all having fun

00:00:22,380 --> 00:00:27,300
while we're having fun at mrs. Kahn

00:00:25,189 --> 00:00:30,810
unfortunately what they are missing out

00:00:27,300 --> 00:00:32,520
so just to jump back into the

00:00:30,810 --> 00:00:34,649
presentation in the beginning all

00:00:32,520 --> 00:00:37,440
software was actually rather simple we

00:00:34,649 --> 00:00:39,450
were just having like a big monolith if

00:00:37,440 --> 00:00:41,489
we're looking at like Z stacked we had

00:00:39,450 --> 00:00:44,570
some hardware operating system on top

00:00:41,489 --> 00:00:48,600
and then our application all very simple

00:00:44,570 --> 00:00:50,969
then came this new like very fancy way

00:00:48,600 --> 00:00:52,949
of saying we split our application into

00:00:50,969 --> 00:00:55,559
many smaller parts called micro services

00:00:52,949 --> 00:00:58,500
so for me there exists a lot of

00:00:55,559 --> 00:01:00,809
definition of micro services and I have

00:00:58,500 --> 00:01:03,239
the feeling everyone has his or her own

00:01:00,809 --> 00:01:07,110
interpretation of it what's kind of the

00:01:03,239 --> 00:01:09,030
important place or important parts for

00:01:07,110 --> 00:01:11,790
me is that Microsoft's are designed to

00:01:09,030 --> 00:01:13,470
be flexible and resilient meaning it's

00:01:11,790 --> 00:01:15,560
important that I have those well-defined

00:01:13,470 --> 00:01:17,430
interfaces between them but the rest

00:01:15,560 --> 00:01:20,310
implementation doesn't matter there I'm

00:01:17,430 --> 00:01:23,090
flexible and this an effect actually

00:01:20,310 --> 00:01:25,619
gives me some nice results that I can be

00:01:23,090 --> 00:01:28,229
individually scalable for example which

00:01:25,619 --> 00:01:31,619
we're going to see here also in this

00:01:28,229 --> 00:01:33,540
presentation and those changes they

00:01:31,619 --> 00:01:36,180
actually also have an impact on the

00:01:33,540 --> 00:01:38,610
application stack now and then back then

00:01:36,180 --> 00:01:40,860
we had kind of like a lamp stack today

00:01:38,610 --> 00:01:43,110
we actually we have multiple stacks if

00:01:40,860 --> 00:01:45,960
we're talking about micro services it's

00:01:43,110 --> 00:01:48,090
usually like some Linux on top XANA

00:01:45,960 --> 00:01:50,040
container runtime then container

00:01:48,090 --> 00:01:52,640
orchestration and then a persistence

00:01:50,040 --> 00:01:55,649
layer where we actually store stuff

00:01:52,640 --> 00:02:00,329
replacing our ancient my sequel for

00:01:55,649 --> 00:02:03,000
example and for the first friend I guess

00:02:00,329 --> 00:02:05,700
all of us know here and have used docker

00:02:03,000 --> 00:02:11,580
or containers before or is there anyone

00:02:05,700 --> 00:02:13,770
who hasn't great but so containers are

00:02:11,580 --> 00:02:17,040
actually driving the stack and

00:02:13,770 --> 00:02:19,230
two types of containers which especially

00:02:17,040 --> 00:02:22,140
from a deployment perspective we need to

00:02:19,230 --> 00:02:24,240
distinguish I stateless containers where

00:02:22,140 --> 00:02:27,420
we usually where we try to keep most of

00:02:24,240 --> 00:02:29,340
our application logic and then stateful

00:02:27,420 --> 00:02:32,340
containers which is just there to store

00:02:29,340 --> 00:02:34,520
all the detail the benefits of this

00:02:32,340 --> 00:02:36,810
division is actually like the

00:02:34,520 --> 00:02:39,060
scalability and deployment mode so

00:02:36,810 --> 00:02:41,630
stateless containers first of all they

00:02:39,060 --> 00:02:43,590
can very easily fail if a marathon

00:02:41,630 --> 00:02:45,870
detects a failing container it will

00:02:43,590 --> 00:02:47,360
simply restart it if we were doing the

00:02:45,870 --> 00:02:50,250
same with the stateful container

00:02:47,360 --> 00:02:52,470
marathon kubernetes all the container

00:02:50,250 --> 00:02:55,740
orchestrators they can do the same but

00:02:52,470 --> 00:02:57,900
it's actually it's more it has more

00:02:55,740 --> 00:03:01,560
pitfalls because actually I have to

00:02:57,900 --> 00:03:03,300
check that I'm restarting on the same

00:03:01,560 --> 00:03:05,400
node where the data is that I still have

00:03:03,300 --> 00:03:07,140
access to my data so that it don't have

00:03:05,400 --> 00:03:09,630
write conflicts to the underlying

00:03:07,140 --> 00:03:12,120
datastore so it simply becomes some

00:03:09,630 --> 00:03:14,550
different game so kind of like the rule

00:03:12,120 --> 00:03:16,980
of some try to get as many stateless

00:03:14,550 --> 00:03:18,720
containers and then move something into

00:03:16,980 --> 00:03:22,920
stateful containers where you actually

00:03:18,720 --> 00:03:25,590
need it so looking at this stacks and in

00:03:22,920 --> 00:03:27,450
comparison to before we have some

00:03:25,590 --> 00:03:31,440
infrastructure underneath this could be

00:03:27,450 --> 00:03:33,959
AWS this could be packaged but someone

00:03:31,440 --> 00:03:36,510
who knows how to either provision bare

00:03:33,959 --> 00:03:38,610
metal machines or knows how to provision

00:03:36,510 --> 00:03:41,220
virtual machines something where it can

00:03:38,610 --> 00:03:43,200
run an OS on top on each of those

00:03:41,220 --> 00:03:45,150
operating systems are usually that have

00:03:43,200 --> 00:03:48,180
to contain a runtime and then I'm

00:03:45,150 --> 00:03:50,670
running my services on top container

00:03:48,180 --> 00:03:52,770
runtime just for those who might not be

00:03:50,670 --> 00:03:55,320
aware I suppose most of us are is

00:03:52,770 --> 00:03:58,470
actually there's much more beyond docker

00:03:55,320 --> 00:04:01,709
so it's not just darker DCOs as you

00:03:58,470 --> 00:04:04,350
learned through the last two days has

00:04:01,709 --> 00:04:09,750
actually an inbuilt container runtime

00:04:04,350 --> 00:04:11,610
there's record so there's actually a lot

00:04:09,750 --> 00:04:14,459
of choice what I pick is the container

00:04:11,610 --> 00:04:16,620
runtime here it's the problem if I

00:04:14,459 --> 00:04:18,900
arrive at this kind of stack is that

00:04:16,620 --> 00:04:21,270
actually I need more so this is the

00:04:18,900 --> 00:04:23,580
early stack at Twitter and what happened

00:04:21,270 --> 00:04:25,770
actually at Twitter was every Friday

00:04:23,580 --> 00:04:27,300
those guys would meet they would check

00:04:25,770 --> 00:04:29,669
oh how many contain

00:04:27,300 --> 00:04:31,560
have failed how many of my services ups

00:04:29,669 --> 00:04:34,199
are still running how many do I have to

00:04:31,560 --> 00:04:36,330
restart so this is what I usually try to

00:04:34,199 --> 00:04:37,979
refer to as spreadsheet scheduling

00:04:36,330 --> 00:04:40,889
Pickers I have a spreadsheet of kind of

00:04:37,979 --> 00:04:43,470
like the target state and then operators

00:04:40,889 --> 00:04:45,720
are going in on regular basis and trying

00:04:43,470 --> 00:04:47,009
to achieve this and this is actually why

00:04:45,720 --> 00:04:49,919
we need something like container

00:04:47,009 --> 00:04:52,710
orchestration something like marathon

00:04:49,919 --> 00:04:54,990
something like kubernetes to basically

00:04:52,710 --> 00:05:00,620
automate all of this and there's three

00:04:54,990 --> 00:05:03,509
main kind of like ya operations or

00:05:00,620 --> 00:05:05,639
fields where container Orchestrator has

00:05:03,509 --> 00:05:07,979
to be aware of has to take care of the

00:05:05,639 --> 00:05:11,159
first one is container scheduling so how

00:05:07,979 --> 00:05:13,710
to fit as many containers onto your ship

00:05:11,159 --> 00:05:15,690
how to restart those containers if they

00:05:13,710 --> 00:05:18,690
are failing and how to keep them running

00:05:15,690 --> 00:05:21,030
how to scale them then a little on a

00:05:18,690 --> 00:05:24,479
lower level resource management how to

00:05:21,030 --> 00:05:26,759
fix the right resources where to put

00:05:24,479 --> 00:05:29,039
your container on top babies a small

00:05:26,759 --> 00:05:32,340
ship isn't the best idea and then

00:05:29,039 --> 00:05:34,020
service management where we care about

00:05:32,340 --> 00:05:36,240
how can we actually connect multiple

00:05:34,020 --> 00:05:40,500
running containers how can we identify

00:05:36,240 --> 00:05:42,539
where they've run in the cluster so just

00:05:40,500 --> 00:05:44,400
if you care it's going to be in the

00:05:42,539 --> 00:05:47,430
slides and some more detail as I just

00:05:44,400 --> 00:05:49,710
explained it and yeah that actually

00:05:47,430 --> 00:05:52,229
gives us this stack so just on top of

00:05:49,710 --> 00:05:54,300
the container runtimes we have our

00:05:52,229 --> 00:05:59,250
container orchestration layer which is

00:05:54,300 --> 00:06:01,710
taking care of all this stuff and now

00:05:59,250 --> 00:06:06,150
let's actually the talk is called cloud

00:06:01,710 --> 00:06:08,430
native way or maybe just first as a

00:06:06,150 --> 00:06:10,560
little reminder we are not try really

00:06:08,430 --> 00:06:14,190
trying to replace Dropbox here in this

00:06:10,560 --> 00:06:16,469
talk what we are trying to look at is

00:06:14,190 --> 00:06:19,650
how would we reimplemented if we would

00:06:16,469 --> 00:06:22,259
start a company all over and so looking

00:06:19,650 --> 00:06:24,060
at this chart from the CN CF actually

00:06:22,259 --> 00:06:26,490
they're trying to list up all the

00:06:24,060 --> 00:06:28,889
different projects in this cloud native

00:06:26,490 --> 00:06:32,849
landscape and you can see they're like

00:06:28,889 --> 00:06:34,800
tons and large number of projects so I

00:06:32,849 --> 00:06:37,289
would actually rather look at the

00:06:34,800 --> 00:06:38,550
different layers which we have on the

00:06:37,289 --> 00:06:40,230
lower bottom we have like the

00:06:38,550 --> 00:06:41,820
infrastructure layer

00:06:40,230 --> 00:06:44,610
this mentions this can be someone like

00:06:41,820 --> 00:06:49,260
package who's actually there it can be

00:06:44,610 --> 00:06:51,360
something like AWS GCE a sure which you

00:06:49,260 --> 00:06:52,710
also heard about this morning so those

00:06:51,360 --> 00:06:55,920
are basically the infrastructure

00:06:52,710 --> 00:06:57,840
providers then I might have some tools

00:06:55,920 --> 00:07:01,920
of actually provisioning those servers

00:06:57,840 --> 00:07:04,080
so they're puppet chef all those tools

00:07:01,920 --> 00:07:08,040
which actually help me to spin up a

00:07:04,080 --> 00:07:09,240
platform on top next then is the actual

00:07:08,040 --> 00:07:11,250
container of runtimes

00:07:09,240 --> 00:07:14,730
we saw under layer and as you can see

00:07:11,250 --> 00:07:16,860
there are a lot more than just darker

00:07:14,730 --> 00:07:19,470
which is somewhere in there in the

00:07:16,860 --> 00:07:21,870
middle actually so we actually we have

00:07:19,470 --> 00:07:23,160
way more choices and simply sticking

00:07:21,870 --> 00:07:25,230
with stalker

00:07:23,160 --> 00:07:28,260
I'm very thankful to because I made it

00:07:25,230 --> 00:07:30,090
popular but I also believe that choice

00:07:28,260 --> 00:07:32,940
and open source standard is actually a

00:07:30,090 --> 00:07:36,090
very good thing here on top we then have

00:07:32,940 --> 00:07:38,010
our orchestration management so this can

00:07:36,090 --> 00:07:40,530
be kubernetes this can be marathon this

00:07:38,010 --> 00:07:43,350
can be darker swarm so again I have like

00:07:40,530 --> 00:07:45,360
a large number of choices here and then

00:07:43,350 --> 00:07:48,210
actually I've run my applications on top

00:07:45,360 --> 00:07:49,980
and this is even where I have so many

00:07:48,210 --> 00:07:52,020
more choices all the traditional

00:07:49,980 --> 00:07:55,200
applications like Postgres like my

00:07:52,020 --> 00:07:57,990
sequel and then all the actually cloud

00:07:55,200 --> 00:08:00,960
native applications which have been

00:07:57,990 --> 00:08:02,880
written for this kind of stack and this

00:08:00,960 --> 00:08:05,250
is also what for me kind of defines

00:08:02,880 --> 00:08:06,720
cloud native applications which have

00:08:05,250 --> 00:08:09,870
been written for such kind of

00:08:06,720 --> 00:08:12,180
environment where I'm conscious about

00:08:09,870 --> 00:08:13,830
that there's a potentially failing

00:08:12,180 --> 00:08:18,290
infrastructure underneath which will

00:08:13,830 --> 00:08:21,870
automatically reprovision potentially so

00:08:18,290 --> 00:08:24,240
from this very filled picture let's go

00:08:21,870 --> 00:08:26,790
to a little more concrete example and

00:08:24,240 --> 00:08:31,010
this is kind of going in the direction

00:08:26,790 --> 00:08:33,750
how we would implement we implement our

00:08:31,010 --> 00:08:36,000
Dropbox nowadays we have our

00:08:33,750 --> 00:08:38,970
infrastructure layer again we have the

00:08:36,000 --> 00:08:41,040
orchestration layer on top and then we

00:08:38,970 --> 00:08:43,200
actually have our different services and

00:08:41,040 --> 00:08:45,210
this is the second aspect of such kind

00:08:43,200 --> 00:08:47,790
of cloud native architecture that I'm

00:08:45,210 --> 00:08:50,070
not trying to build this big blob we saw

00:08:47,790 --> 00:08:51,600
on the second slide I believe but I'm

00:08:50,070 --> 00:08:52,990
really trying to break it up into

00:08:51,600 --> 00:08:55,240
several functionality

00:08:52,990 --> 00:08:58,540
which I can then provision on like such

00:08:55,240 --> 00:09:00,160
kind of cluster and so in this case we

00:08:58,540 --> 00:09:02,170
actually we have a lot of stuff running

00:09:00,160 --> 00:09:04,150
in docker containers we have Mineo

00:09:02,170 --> 00:09:05,770
running in docker containers we have my

00:09:04,150 --> 00:09:09,490
sequel running in docker containers

00:09:05,770 --> 00:09:14,230
Redis and then nodejs for the front-end

00:09:09,490 --> 00:09:16,540
part and you remember me saying that we

00:09:14,230 --> 00:09:19,060
should try to avoid stateful containers

00:09:16,540 --> 00:09:21,430
as you can see here we actually we end

00:09:19,060 --> 00:09:24,130
up with mostly stateful containers so

00:09:21,430 --> 00:09:27,790
often in practice it's rather hard to

00:09:24,130 --> 00:09:30,040
actually get to state stateless

00:09:27,790 --> 00:09:32,170
containers and often we unfortunately

00:09:30,040 --> 00:09:34,630
still up with an large number of

00:09:32,170 --> 00:09:37,120
stateful containers it's a good thing is

00:09:34,630 --> 00:09:39,760
in this picture like this thing we can

00:09:37,120 --> 00:09:42,160
scale is actually easy no js' this is

00:09:39,760 --> 00:09:44,680
what the customers will see and this is

00:09:42,160 --> 00:09:47,650
what the first week requests will go to

00:09:44,680 --> 00:09:49,240
so this is rather easy to scale so we

00:09:47,650 --> 00:09:51,910
can still keep like Layton sees

00:09:49,240 --> 00:09:54,040
hopefully down maybe not so much for the

00:09:51,910 --> 00:09:57,660
riots but at least for the UI which is

00:09:54,040 --> 00:09:57,660
like this thing I feel as a user

00:09:58,080 --> 00:10:05,320
persistence so as we were just talking

00:10:01,630 --> 00:10:08,050
about stateful containers I actually

00:10:05,320 --> 00:10:10,210
took a look at a docker hub and its top

00:10:08,050 --> 00:10:13,420
10 images and it's really interesting

00:10:10,210 --> 00:10:15,310
that depending on what time I checked it

00:10:13,420 --> 00:10:19,780
was a little vary but somewhere between

00:10:15,310 --> 00:10:24,640
70 and 80% in the top 10 and top 20 were

00:10:19,780 --> 00:10:26,350
actually stateful containers and so this

00:10:24,640 --> 00:10:28,030
already tells us that it's really hard

00:10:26,350 --> 00:10:31,690
of getting rid of the stateful

00:10:28,030 --> 00:10:34,330
containers and if we were choosing a

00:10:31,690 --> 00:10:36,400
stateful container it's it's a tough

00:10:34,330 --> 00:10:38,740
decision and this is actually as we have

00:10:36,400 --> 00:10:40,840
so many choices how we can store data

00:10:38,740 --> 00:10:43,450
nowadays it becomes even much much

00:10:40,840 --> 00:10:45,610
harder so what we should consider when

00:10:43,450 --> 00:10:48,520
choosing one of those containers is what

00:10:45,610 --> 00:10:51,580
kind of data model do we have and how do

00:10:48,520 --> 00:10:53,950
I actually access the data do I mostly

00:10:51,580 --> 00:10:55,450
store JSON data then probably mom would

00:10:53,950 --> 00:10:57,970
you be is a good choice do I have

00:10:55,450 --> 00:11:00,940
simpler key value pairs Redis might be a

00:10:57,970 --> 00:11:04,420
good choice and if I have just have blob

00:11:00,940 --> 00:11:06,010
data so like a special case of key value

00:11:04,420 --> 00:11:06,880
actually where I just want to store like

00:11:06,010 --> 00:11:09,280
a big

00:11:06,880 --> 00:11:11,350
then actually likes those lobsters or

00:11:09,280 --> 00:11:14,820
object stalls as they're called like

00:11:11,350 --> 00:11:16,840
mini or s3 might be a good choice and

00:11:14,820 --> 00:11:20,940
and so on and so on

00:11:16,840 --> 00:11:23,590
sequel post Chris my sequel text I can

00:11:20,940 --> 00:11:26,140
store an elastic search as kind of a

00:11:23,590 --> 00:11:29,170
search engine so just try to come up

00:11:26,140 --> 00:11:31,450
with what is the requirement for storing

00:11:29,170 --> 00:11:33,190
my data and then decide which of the

00:11:31,450 --> 00:11:35,410
data stores might be a good fit for that

00:11:33,190 --> 00:11:37,090
the good news is that in this

00:11:35,410 --> 00:11:40,180
architecture you don't have to pick just

00:11:37,090 --> 00:11:41,530
one as we all have those as we ending up

00:11:40,180 --> 00:11:43,450
with this micro service like

00:11:41,530 --> 00:11:46,480
architecture you can actually pick

00:11:43,450 --> 00:11:47,860
multiple so one for each use case but

00:11:46,480 --> 00:11:50,350
you should keep in mind that of course

00:11:47,860 --> 00:11:52,690
this increases the administration

00:11:50,350 --> 00:11:56,320
overhead but it's much simpler as it

00:11:52,690 --> 00:11:57,700
used to be in the pre orchestrated world

00:11:56,320 --> 00:12:00,520
where actually then how to deploy

00:11:57,700 --> 00:12:02,710
manually on nodes I can scale that

00:12:00,520 --> 00:12:04,390
rather easily automatically on a cluster

00:12:02,710 --> 00:12:06,580
but of course they're still like an

00:12:04,390 --> 00:12:10,840
Operations overhead for each project I

00:12:06,580 --> 00:12:14,080
install if we look into object store as

00:12:10,840 --> 00:12:18,280
we talked about many over here we ended

00:12:14,080 --> 00:12:20,110
up with object stores so like all big

00:12:18,280 --> 00:12:22,720
cloud providers actually have a solution

00:12:20,110 --> 00:12:25,270
and this is why I personally like Mineo

00:12:22,720 --> 00:12:28,750
quite a lot because it's actually its

00:12:25,270 --> 00:12:32,140
API compatible both to GCE industry so I

00:12:28,750 --> 00:12:35,680
can use the same programs I use the same

00:12:32,140 --> 00:12:38,020
API to write my data onto s3 I can use

00:12:35,680 --> 00:12:41,110
on my local cluster to write the data

00:12:38,020 --> 00:12:43,060
into an equivalent of s3 and we're going

00:12:41,110 --> 00:12:46,480
to see that in some more detail on the

00:12:43,060 --> 00:12:49,090
next slides so meaning would actually

00:12:46,480 --> 00:12:51,190
consists of the actual service which

00:12:49,090 --> 00:12:52,870
you're going to store is a data a mini

00:12:51,190 --> 00:12:54,760
oak line which can write the data

00:12:52,870 --> 00:12:58,020
similar to like in a sweet client

00:12:54,760 --> 00:13:01,740
writing the data and then the SDK for

00:12:58,020 --> 00:13:03,880
writing applications against it and

00:13:01,740 --> 00:13:06,880
Mineo it's actually it gives me

00:13:03,880 --> 00:13:09,040
different backends so I have to decide

00:13:06,880 --> 00:13:11,620
when installing or starting up two

00:13:09,040 --> 00:13:13,720
servers how I want to store the data

00:13:11,620 --> 00:13:15,400
this simple option and this is what I

00:13:13,720 --> 00:13:17,980
would use if I'm running it on my local

00:13:15,400 --> 00:13:19,980
nap laptop will be just mini or server

00:13:17,980 --> 00:13:22,440
and then some directory and

00:13:19,980 --> 00:13:24,870
would use that directory to as a

00:13:22,440 --> 00:13:26,820
back-end so stores of data but the

00:13:24,870 --> 00:13:29,940
interesting part actually comes and if I

00:13:26,820 --> 00:13:32,790
use multiple backends because I can even

00:13:29,940 --> 00:13:35,930
do that on a my single laptop so once I

00:13:32,790 --> 00:13:38,850
start specifying multiple disk what

00:13:35,930 --> 00:13:41,100
Mineo will actually do it will start

00:13:38,850 --> 00:13:43,590
distributing it across those different

00:13:41,100 --> 00:13:46,440
disks and then also add Eurasia coding

00:13:43,590 --> 00:13:49,350
so I can be sure that not a single of

00:13:46,440 --> 00:13:51,660
those a disk will be corrupt Mineo would

00:13:49,350 --> 00:13:55,530
detect that and automatically removes

00:13:51,660 --> 00:13:58,470
that disk from the from the yeah serving

00:13:55,530 --> 00:14:00,120
of data and I can do actually the same

00:13:58,470 --> 00:14:02,370
so this don't have to be on a single

00:14:00,120 --> 00:14:04,740
machine on a single host I can actually

00:14:02,370 --> 00:14:07,470
also spread that out across multiple

00:14:04,740 --> 00:14:10,230
servers and as I'm not with many oh I

00:14:07,470 --> 00:14:12,570
can also save what I don't necessarily

00:14:10,230 --> 00:14:15,270
like about this that it's actually it's

00:14:12,570 --> 00:14:17,040
limited up by 16 servers and that means

00:14:15,270 --> 00:14:19,590
if I have for really large deployments

00:14:17,040 --> 00:14:22,680
what I usually end up doing I have like

00:14:19,590 --> 00:14:25,140
a proxy on top which can then use

00:14:22,680 --> 00:14:29,190
multiple of those 16 servers underneath

00:14:25,140 --> 00:14:32,070
so by just having this one proxy on top

00:14:29,190 --> 00:14:37,590
I can then actually utilize multiple

00:14:32,070 --> 00:14:40,290
Mineo servers on top for that as

00:14:37,590 --> 00:14:43,560
mentioned they we have a ratio coding so

00:14:40,290 --> 00:14:46,830
actually when data is being split so

00:14:43,560 --> 00:14:49,410
here we have 16 nodes and for each block

00:14:46,830 --> 00:14:51,390
we're going to split it into eight data

00:14:49,410 --> 00:14:53,280
blocks and then we're gonna have eight

00:14:51,390 --> 00:14:55,380
parity blocks which are all spread out

00:14:53,280 --> 00:14:57,900
across different notes and that means

00:14:55,380 --> 00:15:00,030
that we can actually survive up to eight

00:14:57,900 --> 00:15:01,890
node failures in the system

00:15:00,030 --> 00:15:03,870
because we can always reconstruct the

00:15:01,890 --> 00:15:05,430
data from the other nodes which are

00:15:03,870 --> 00:15:07,320
still running in the cluster and it

00:15:05,430 --> 00:15:09,180
doesn't matter because it's arranger

00:15:07,320 --> 00:15:11,940
coding which if the notes are failing as

00:15:09,180 --> 00:15:16,740
long as it's up to eight we can always

00:15:11,940 --> 00:15:19,800
reconstruct our initial data and this is

00:15:16,740 --> 00:15:22,410
actually giving me a pretty nice set up

00:15:19,800 --> 00:15:26,250
if if I don't have too much data I mean

00:15:22,410 --> 00:15:29,370
16 16 disks it's already quite quite a

00:15:26,250 --> 00:15:32,940
large chunk of data which I can store in

00:15:29,370 --> 00:15:35,190
there and as long as this is good

00:15:32,940 --> 00:15:40,770
set up for my use case I can be happy

00:15:35,190 --> 00:15:43,710
with using minier for that so as

00:15:40,770 --> 00:15:46,560
mentioned up to eight drives can

00:15:43,710 --> 00:15:49,830
actually fail on on the server and what

00:15:46,560 --> 00:15:52,110
kind of their view is on them is that

00:15:49,830 --> 00:15:54,480
you migrate a data to new service a

00:15:52,110 --> 00:15:56,910
service age so you spin up this cluster

00:15:54,480 --> 00:15:59,400
with 16 deaths in three to four years

00:15:56,910 --> 00:16:01,080
when that's actually filled up you're

00:15:59,400 --> 00:16:02,850
gonna have new servers somewhere in the

00:16:01,080 --> 00:16:04,740
other corner of your datacenter anyhow

00:16:02,850 --> 00:16:07,920
and then you're going to migrate the

00:16:04,740 --> 00:16:11,250
data over to a new cluster and this is

00:16:07,920 --> 00:16:14,250
why they also have this view of spinning

00:16:11,250 --> 00:16:16,230
up multiple Mineo instances so they

00:16:14,250 --> 00:16:18,180
don't expect actually to scale more to

00:16:16,230 --> 00:16:20,280
sixteen servers from Mineola cluster

00:16:18,180 --> 00:16:22,340
because it's so easy to spin up like in

00:16:20,280 --> 00:16:25,410
next the next cluster next to it and

00:16:22,340 --> 00:16:27,540
this is the other kind of notion in this

00:16:25,410 --> 00:16:30,030
cloud native world that I can easily

00:16:27,540 --> 00:16:32,310
scale I don't necessarily I'm restricted

00:16:30,030 --> 00:16:34,020
to a big monolith if I need more power

00:16:32,310 --> 00:16:36,180
I simply going to deploy a second one

00:16:34,020 --> 00:16:38,060
and maybe half a load balancer up front

00:16:36,180 --> 00:16:40,890
so when you look at all the different

00:16:38,060 --> 00:16:42,870
cloud native deployments this is very

00:16:40,890 --> 00:16:45,750
common pattern to have multiple

00:16:42,870 --> 00:16:51,360
instances scale out instead of scaling

00:16:45,750 --> 00:16:55,830
up there are a number of companies using

00:16:51,360 --> 00:16:58,380
it I can only go into detail into the

00:16:55,830 --> 00:17:01,590
mists of mesosphere use case but as you

00:16:58,380 --> 00:17:05,189
see is also supported by package so is

00:17:01,590 --> 00:17:09,150
actually fairly widely used by different

00:17:05,189 --> 00:17:11,130
companies now we come to the problem

00:17:09,150 --> 00:17:14,280
where actually which probably all of us

00:17:11,130 --> 00:17:16,560
know is what is the challenge then for

00:17:14,280 --> 00:17:18,810
the datacenter so we've seen how would

00:17:16,560 --> 00:17:21,329
we construct such kind of cloud native

00:17:18,810 --> 00:17:24,030
app but what's now the challenge to my

00:17:21,329 --> 00:17:26,069
data center and this is that we often as

00:17:24,030 --> 00:17:28,470
we have so many different services we

00:17:26,069 --> 00:17:31,860
have mini our way of my sequel we have

00:17:28,470 --> 00:17:33,510
spark as we have all those choices we

00:17:31,860 --> 00:17:37,380
actually we end up subdividing our

00:17:33,510 --> 00:17:39,600
cluster into many sub clusters and this

00:17:37,380 --> 00:17:40,860
of course as you heard in the keynotes

00:17:39,600 --> 00:17:43,620
and probably heard multiple times

00:17:40,860 --> 00:17:46,120
throughout the last days is leading to a

00:17:43,620 --> 00:17:49,059
very bad utilization rates

00:17:46,120 --> 00:17:52,360
this is exactly why missiles and TCOs

00:17:49,059 --> 00:17:55,840
spring in to view your entire data

00:17:52,360 --> 00:17:57,850
center as like a single machine and then

00:17:55,840 --> 00:18:03,880
also be able to address it as a single

00:17:57,850 --> 00:18:05,920
machine and now just to kind of a more

00:18:03,880 --> 00:18:09,390
concrete architecture how we actually

00:18:05,920 --> 00:18:12,640
implemented a demo of like a mini or

00:18:09,390 --> 00:18:15,220
cloud Dropbox clone and so we have

00:18:12,640 --> 00:18:17,710
chosen so what parts do we actually need

00:18:15,220 --> 00:18:19,809
we need a web server where I as a user

00:18:17,710 --> 00:18:21,760
can see what data do I currently have

00:18:19,809 --> 00:18:25,900
where I can upload data and just

00:18:21,760 --> 00:18:28,030
interact with my Dropbox account then we

00:18:25,900 --> 00:18:31,570
have an API server Chris also I need to

00:18:28,030 --> 00:18:33,850
do that from an API way so that's the

00:18:31,570 --> 00:18:35,620
second part and then we actually need to

00:18:33,850 --> 00:18:38,260
store things and we have to store

00:18:35,620 --> 00:18:39,670
different kinds of things and so first

00:18:38,260 --> 00:18:42,550
of all we need to store some kind of

00:18:39,670 --> 00:18:44,710
user data that users can log in we can

00:18:42,550 --> 00:18:48,040
see what their quota is how much data

00:18:44,710 --> 00:18:50,890
can they actually store and this is like

00:18:48,040 --> 00:18:53,740
very relational data so there my sequel

00:18:50,890 --> 00:18:56,050
might be a good choice then we might

00:18:53,740 --> 00:18:59,220
actually want to offer like a free text

00:18:56,050 --> 00:19:02,470
search index that I can search across my

00:18:59,220 --> 00:19:04,660
Dropbox and find documents which I might

00:19:02,470 --> 00:19:07,300
be looking for your elastic search is a

00:19:04,660 --> 00:19:12,370
really good option for storing such kind

00:19:07,300 --> 00:19:14,679
of text data we might then have every

00:19:12,370 --> 00:19:16,179
storing blob data we actually we have to

00:19:14,679 --> 00:19:18,070
store the blob somewhere and there's

00:19:16,179 --> 00:19:20,710
those object stores actually a really

00:19:18,070 --> 00:19:23,200
good choice but we also we need to be

00:19:20,710 --> 00:19:26,260
able to retrieve them so we need to have

00:19:23,200 --> 00:19:29,500
this kind of metadata which blocks do

00:19:26,260 --> 00:19:31,360
are connected in which way and how what

00:19:29,500 --> 00:19:34,420
do I need to retrieve them so we need

00:19:31,360 --> 00:19:37,360
some kind of metadata index and they're

00:19:34,420 --> 00:19:39,010
actually Redis might probably be a good

00:19:37,360 --> 00:19:42,070
choice to storage because it's simply

00:19:39,010 --> 00:19:46,020
key value pairs for each blob we have

00:19:42,070 --> 00:19:53,020
some metadata which we need to store and

00:19:46,020 --> 00:19:56,560
that already leads up to the demo so I

00:19:53,020 --> 00:19:58,420
spun up a DCOs cluster up front so very

00:19:56,560 --> 00:19:59,470
simple not sure how many people have

00:19:58,420 --> 00:20:01,090
seen it what

00:19:59,470 --> 00:20:03,220
I like about it it's basically this

00:20:01,090 --> 00:20:05,020
focus on just house my huge realization

00:20:03,220 --> 00:20:09,039
rate which of course is zero right now

00:20:05,020 --> 00:20:12,460
as we have nothing deployed so far if we

00:20:09,039 --> 00:20:15,720
start deploying things we said we're

00:20:12,460 --> 00:20:20,880
gonna do a stripped-down version of this

00:20:15,720 --> 00:20:20,880
drop box so we first kind of pick Redis

00:20:21,750 --> 00:20:29,530
great Redis is being installed we see

00:20:25,120 --> 00:20:32,070
it's staging and next let's actually

00:20:29,530 --> 00:20:32,070
pick Mineo

00:20:44,160 --> 00:20:49,950
so many a well and it's already up and

00:20:46,559 --> 00:20:51,809
running and as a last thing we actually

00:20:49,950 --> 00:20:53,760
we need to expose many oh so many Oh

00:20:51,809 --> 00:20:57,150
would now just be running in our cluster

00:20:53,760 --> 00:20:59,370
as I actually care about the UI I need

00:20:57,150 --> 00:21:04,049
like my load balancer

00:20:59,370 --> 00:21:06,240
to expose it and therefore we going to

00:21:04,049 --> 00:21:09,240
choose my arsenal be how many of you

00:21:06,240 --> 00:21:11,460
have used Maris will not be before so

00:21:09,240 --> 00:21:13,620
marathon LV is basically an H a proxy

00:21:11,460 --> 00:21:15,720
which is running on like the external

00:21:13,620 --> 00:21:18,630
nodes see external will reachable notes

00:21:15,720 --> 00:21:20,700
and a DC OS cluster and hence they can

00:21:18,630 --> 00:21:22,799
actually expose services to the outside

00:21:20,700 --> 00:21:27,480
in this case we're just going to use it

00:21:22,799 --> 00:21:30,330
to expose the Mineo to like an external

00:21:27,480 --> 00:21:33,390
will resolve but address which in this

00:21:30,330 --> 00:21:36,990
case is our public agent and here we

00:21:33,390 --> 00:21:40,289
already go and the access key and secret

00:21:36,990 --> 00:21:42,030
keys it's the same same as you would see

00:21:40,289 --> 00:21:44,789
with amazon where you also have a secret

00:21:42,030 --> 00:21:46,890
key and access key and this is the same

00:21:44,789 --> 00:21:49,380
you would use when connecting from your

00:21:46,890 --> 00:21:58,679
AP ice against it the default ones are

00:21:49,380 --> 00:22:02,909
very simple it's Mineo ministry menu one

00:21:58,679 --> 00:22:05,730
two three yes we don't want to store

00:22:02,909 --> 00:22:08,669
that and now we can actually symbolize

00:22:05,730 --> 00:22:16,590
with an s3 we can start creating buckets

00:22:08,669 --> 00:22:24,350
so first test bucket now we have the

00:22:16,590 --> 00:22:27,419
first bucket here tests y test two and

00:22:24,350 --> 00:22:29,100
we can do the same from like any s3

00:22:27,419 --> 00:22:31,590
browsers which where they're actually a

00:22:29,100 --> 00:22:33,960
lot off and simply connect to this

00:22:31,590 --> 00:22:36,030
cluster who's now exposed why i like

00:22:33,960 --> 00:22:38,700
also the public IP occurs we installed

00:22:36,030 --> 00:22:40,710
Marisa know be the last piece missing

00:22:38,700 --> 00:22:44,370
which we're going to use in this demo is

00:22:40,710 --> 00:22:48,110
the api server and this I will actually

00:22:44,370 --> 00:22:48,110
deploy from my other child

00:22:51,230 --> 00:22:57,580
yeah it's the other one yes

00:23:00,840 --> 00:23:08,940
so what we can also do we can simply

00:23:04,679 --> 00:23:10,379
simply deploy stuff from from the CLI so

00:23:08,940 --> 00:23:13,289
this would be my local development

00:23:10,379 --> 00:23:15,059
environment I have this very simple app

00:23:13,289 --> 00:23:16,220
definition which we also going to look

00:23:15,059 --> 00:23:24,830
at in a second

00:23:16,220 --> 00:23:24,830
Marshall habit so every to die Jason

00:23:30,070 --> 00:23:36,550
and now we we have another deployment

00:23:32,520 --> 00:23:39,340
what I wanted to show you just for very

00:23:36,550 --> 00:23:43,810
brief reason is how the such kind of

00:23:39,340 --> 00:23:45,580
file looked like so all it does is

00:23:43,810 --> 00:23:48,790
actually it's going to retrieve a go

00:23:45,580 --> 00:23:50,890
binary because we said that our API our

00:23:48,790 --> 00:23:53,230
app server is written and go and the

00:23:50,890 --> 00:23:56,050
nice thing about go is that in most

00:23:53,230 --> 00:23:58,050
cases not all it's actually totally

00:23:56,050 --> 00:24:00,850
sufficient to just have this binary

00:23:58,050 --> 00:24:03,190
compiled for the right architecture so

00:24:00,850 --> 00:24:06,160
in this case it's totally sufficient to

00:24:03,190 --> 00:24:08,440
just pull down the artifact and then run

00:24:06,160 --> 00:24:11,890
it so you see like the command is just

00:24:08,440 --> 00:24:13,990
run the app server and we don't need to

00:24:11,890 --> 00:24:16,000
have to deploy a full docker container

00:24:13,990 --> 00:24:19,150
we don't have to download an entire

00:24:16,000 --> 00:24:21,940
image and you can if you just compare

00:24:19,150 --> 00:24:25,180
sizes like a typical docker image across

00:24:21,940 --> 00:24:30,280
all its image layers usually ends up

00:24:25,180 --> 00:24:32,350
somewhere between 130 to maybe up to 500

00:24:30,280 --> 00:24:34,660
megabytes or even more depending on

00:24:32,350 --> 00:24:39,610
what's inside and this is actually just

00:24:34,660 --> 00:24:41,950
I believe it's 2 megabyte or so so if I

00:24:39,610 --> 00:24:43,870
don't have to rely on a container image

00:24:41,950 --> 00:24:46,540
simply because I don't need it and I can

00:24:43,870 --> 00:24:48,280
only pull the binary or the pieces I

00:24:46,540 --> 00:24:53,380
need I can actually save a lot of

00:24:48,280 --> 00:24:56,020
bandwidth the other saying which we see

00:24:53,380 --> 00:24:57,880
and this is also very crucial if we are

00:24:56,020 --> 00:24:59,680
coming up with such kind of cloud native

00:24:57,880 --> 00:25:01,810
infrastructure we want the

00:24:59,680 --> 00:25:04,510
infrastructure to actually be aware of

00:25:01,810 --> 00:25:06,910
whatever is considered healthy and

00:25:04,510 --> 00:25:09,520
therefore really fine so-called health

00:25:06,910 --> 00:25:12,250
checks in this case this is an HTTP a

00:25:09,520 --> 00:25:13,900
health check so what it translate to is

00:25:12,250 --> 00:25:16,840
actually that from time to time we're

00:25:13,900 --> 00:25:19,600
simply gonna pull against the slash

00:25:16,840 --> 00:25:22,060
endpoint so just simply against the port

00:25:19,600 --> 00:25:25,300
on which it's running and then we're

00:25:22,060 --> 00:25:28,090
gonna see whether we get an HTTP return

00:25:25,300 --> 00:25:30,550
code in the 200 range if we if it's in

00:25:28,090 --> 00:25:33,370
200 range we assume this is healthy if

00:25:30,550 --> 00:25:35,680
it's somewhere like 400 or some other

00:25:33,370 --> 00:25:38,710
range we would actually say it's

00:25:35,680 --> 00:25:42,370
unhealthy and we can even see that from

00:25:38,710 --> 00:25:43,779
our UI here if we look at services we

00:25:42,370 --> 00:25:48,340
see here our app server

00:25:43,779 --> 00:25:51,700
- it's here it's one task healthy we can

00:25:48,340 --> 00:25:54,279
now go in and scale it as we said this

00:25:51,700 --> 00:25:57,549
is also like an artifact or a crucial

00:25:54,279 --> 00:25:59,349
definite or a defining part of the cloud

00:25:57,549 --> 00:26:02,979
native architectures if we can easily

00:25:59,349 --> 00:26:05,979
scale so I can now scale it to two

00:26:02,979 --> 00:26:11,739
instances and we see one more is

00:26:05,979 --> 00:26:15,009
deploying and now we have to running if

00:26:11,739 --> 00:26:16,989
one of them would be unhealthy it would

00:26:15,009 --> 00:26:18,759
actually be automatically restarted by

00:26:16,989 --> 00:26:20,649
the container orchestration layer and

00:26:18,759 --> 00:26:22,899
this is why those health checks are also

00:26:20,649 --> 00:26:25,570
so important because they simply allow

00:26:22,899 --> 00:26:27,669
me or they allow the infrastructure to

00:26:25,570 --> 00:26:30,369
be self-healing if something is going

00:26:27,669 --> 00:26:32,499
wrong if my server is crashing I can

00:26:30,369 --> 00:26:34,719
also have more complex health checks so

00:26:32,499 --> 00:26:37,059
for example with commands where I'm

00:26:34,719 --> 00:26:40,049
checking some database stage to just see

00:26:37,059 --> 00:26:46,089
whether the databases in a healthy state

00:26:40,049 --> 00:26:51,549
all right so this was like the quick way

00:26:46,089 --> 00:26:54,009
of rolling out our demo what I would

00:26:51,549 --> 00:26:55,539
briefly like to talk about because demo

00:26:54,009 --> 00:26:57,729
is they are actually they're very simple

00:26:55,539 --> 00:26:59,619
and they're cheating rolling out the

00:26:57,729 --> 00:27:01,839
demo is something very simple I just

00:26:59,619 --> 00:27:03,429
click install you see it up and running

00:27:01,839 --> 00:27:04,869
you see it green and healthy and you're

00:27:03,429 --> 00:27:07,239
like yeah cool

00:27:04,869 --> 00:27:10,690
but the hard thing is actually how do we

00:27:07,239 --> 00:27:12,969
keep it up and running afterwards and so

00:27:10,690 --> 00:27:15,159
these things if you're coming up with

00:27:12,969 --> 00:27:17,259
your cloud native infrastructure your

00:27:15,159 --> 00:27:18,999
cloud native architecture is you should

00:27:17,259 --> 00:27:21,159
consider those operations which are

00:27:18,999 --> 00:27:23,139
happening after we just deployed and

00:27:21,159 --> 00:27:25,419
it's up and green and those are for

00:27:23,139 --> 00:27:27,940
example configuration updates upgrades

00:27:25,419 --> 00:27:30,009
if we're scaling we just scales that

00:27:27,940 --> 00:27:31,659
application that was working but in

00:27:30,009 --> 00:27:33,820
production we probably want an outer

00:27:31,659 --> 00:27:35,769
scaling component which will if there

00:27:33,820 --> 00:27:37,719
are more users hitting the API server

00:27:35,769 --> 00:27:40,269
which will automatically upgrade the API

00:27:37,719 --> 00:27:43,749
server and potentially like sit back in

00:27:40,269 --> 00:27:45,669
other services binary upgrades of both

00:27:43,749 --> 00:27:48,009
the application I want a new Redis

00:27:45,669 --> 00:27:50,529
instance how can I do that without

00:27:48,009 --> 00:27:53,289
affecting the rest of my cluster cluster

00:27:50,529 --> 00:27:56,529
maintenance backup restore the cluster

00:27:53,289 --> 00:27:57,730
failing itself and what's very crucial

00:27:56,529 --> 00:28:00,039
to all of it

00:27:57,730 --> 00:28:01,600
whenever you're running anything in any

00:28:00,039 --> 00:28:03,909
cloud environment it doesn't even have

00:28:01,600 --> 00:28:05,620
to be cloud native it's just monitoring

00:28:03,909 --> 00:28:07,389
they're always going to be failures

00:28:05,620 --> 00:28:09,340
there might be network congestion and

00:28:07,389 --> 00:28:12,309
you want to be aware of before your

00:28:09,340 --> 00:28:15,429
users are getting unfortunately aware of

00:28:12,309 --> 00:28:17,110
it and then after you've gotten aware of

00:28:15,429 --> 00:28:19,600
it the next step is you also need to be

00:28:17,110 --> 00:28:21,399
able to debug it so you need to think of

00:28:19,600 --> 00:28:23,620
scalable ways of how you can actually

00:28:21,399 --> 00:28:26,320
deep dive into your infrastructure and

00:28:23,620 --> 00:28:28,450
debug such kind of runtime blockages for

00:28:26,320 --> 00:28:31,510
which we actually had a workshop here

00:28:28,450 --> 00:28:33,580
which Johannes was part of leading so

00:28:31,510 --> 00:28:35,830
the kind of day to operations where we

00:28:33,580 --> 00:28:38,799
looked at how can we actually debug such

00:28:35,830 --> 00:28:42,970
kind of runtime blockages this slides

00:28:38,799 --> 00:28:45,159
are online nodding yes so feel free to

00:28:42,970 --> 00:28:50,169
check out the missus university session

00:28:45,159 --> 00:28:52,960
on those data operations furthermore I

00:28:50,169 --> 00:28:55,510
said monitoring so collecting metrics of

00:28:52,960 --> 00:28:57,399
post C user apps of the cluster of the

00:28:55,510 --> 00:28:59,440
containers running in your clusters so

00:28:57,399 --> 00:29:01,210
this is something very vital also

00:28:59,440 --> 00:29:03,789
collecting logging information which

00:29:01,210 --> 00:29:07,059
makes it easier to debug as well and

00:29:03,789 --> 00:29:08,649
might also have some you might have

00:29:07,059 --> 00:29:11,830
requirements to actually keep locked

00:29:08,649 --> 00:29:14,230
like access locks depending on what the

00:29:11,830 --> 00:29:17,320
also government requirements are for

00:29:14,230 --> 00:29:19,740
your kind of business maintenance so

00:29:17,320 --> 00:29:22,929
cluster upgrades cluster resizing

00:29:19,740 --> 00:29:24,850
changing Network policies and then

00:29:22,929 --> 00:29:27,940
troubleshooting which unfortunately if

00:29:24,850 --> 00:29:31,960
we probably all had to do unvoluntary

00:29:27,940 --> 00:29:34,029
early so debugging tracing and just to

00:29:31,960 --> 00:29:36,399
make it easier to get acquainted to it

00:29:34,029 --> 00:29:39,220
we would really recommend doing chaos

00:29:36,399 --> 00:29:41,289
engineering so having like a chaos

00:29:39,220 --> 00:29:43,450
monkey introducing failures so you

00:29:41,289 --> 00:29:45,639
actually you get used to and he also get

00:29:43,450 --> 00:29:47,230
confidence in your infrastructure that

00:29:45,639 --> 00:29:49,330
it doesn't fail if there are simple

00:29:47,230 --> 00:29:54,159
failures because you have tested it

00:29:49,330 --> 00:29:56,529
using your chaos monkey that's already

00:29:54,159 --> 00:29:59,139
all of what I wanted to talk about

00:29:56,529 --> 00:30:01,510
because it's the last session and I

00:29:59,139 --> 00:30:04,029
suppose it's seeing how empty the room

00:30:01,510 --> 00:30:05,769
is people also want to go home but still

00:30:04,029 --> 00:30:08,440
if you have questions feel free to ask

00:30:05,769 --> 00:30:10,330
them feel free to also ask them later on

00:30:08,440 --> 00:30:19,330
why is it very slack channels

00:30:10,330 --> 00:30:27,130
but right now is also a good time Judas

00:30:19,330 --> 00:30:30,640
has Christians then thank you very much

00:30:27,130 --> 00:30:31,360
for listening I said like try out if you

00:30:30,640 --> 00:30:33,970
haven't

00:30:31,360 --> 00:30:37,600
Minaya I really like it but just be

00:30:33,970 --> 00:30:40,540
aware of so scaling limitations for now

00:30:37,600 --> 00:30:42,730
and you can architect around it but you

00:30:40,540 --> 00:30:45,090
just have to be aware of it thank you

00:30:42,730 --> 00:30:45,090

YouTube URL: https://www.youtube.com/watch?v=XJErXu7K4lc


