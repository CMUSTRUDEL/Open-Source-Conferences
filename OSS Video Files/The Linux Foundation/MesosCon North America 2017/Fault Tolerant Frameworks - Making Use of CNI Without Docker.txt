Title: Fault Tolerant Frameworks - Making Use of CNI Without Docker
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Fault Tolerant Frameworks - Making Use of CNI Without Docker - Aaron Wood & Tim Hansen, Verizon

While most people use Docker for their containerization efforts, it is in no way a requirement for Mesos frameworks. This presentation will cover how Verizon built their own highly available, performant framework utilizing the universal containerizer and the container network interface. Specific topics of interest include how high availability was built, the reasons behind Go as the language of choice, the benefits that CNI provides, pros and cons of using the universal containerizer, and a quick overview of the accompanying SDK. Usage of the new V1 streaming API will also be covered.

About 

Aaron Wood
Principal Software Engineer, Verizon
Aaron, a passionate software engineer with over 10 years experience, works on distributed services and cloud architecture at Verizon Labs. He has an affinity for Linux and security and works with Go, C++, Java, and Python in his current role.

Timothy Hansen
Tim has been writing software professionally for 4 years, and currently works with the research and development team at Verizon Labs. Topics of professional interest include machine learning and distributed computing. Languages primarily utilized at work are Go, Java, Python, and C++.
Captions: 
	00:00:00,030 --> 00:00:05,460
my name is Aaron wood I work for a

00:00:01,709 --> 00:00:07,799
Verizon labs Tim Hanson senior software

00:00:05,460 --> 00:00:10,110
engineer also at Verizon Labs this is

00:00:07,799 --> 00:00:13,469
presentation about fault tolerant

00:00:10,110 --> 00:00:15,680
framework we wrote our own scheduler

00:00:13,469 --> 00:00:18,720
base right on top of the open source

00:00:15,680 --> 00:00:21,689
protobufs form ASOS and we're going to

00:00:18,720 --> 00:00:24,390
talk a bit about our scheduler and

00:00:21,689 --> 00:00:30,990
framework as well as how we integrated C

00:00:24,390 --> 00:00:32,850
and I without docker so in the very

00:00:30,990 --> 00:00:35,820
beginning we had kind of a easy choice

00:00:32,850 --> 00:00:37,920
to make I would say so a lot of the

00:00:35,820 --> 00:00:40,680
frameworks I've seen so far are still

00:00:37,920 --> 00:00:43,140
using the older v-0 API that require

00:00:40,680 --> 00:00:44,489
bindings and has a bit of a different

00:00:43,140 --> 00:00:46,110
architecture underneath for how the

00:00:44,489 --> 00:00:50,449
framework communicates with me so sand

00:00:46,110 --> 00:00:52,350
I'm you suppose communicates back so I

00:00:50,449 --> 00:00:54,449
mean as you can kind of see it was

00:00:52,350 --> 00:00:57,750
pretty easy to pick the v1 API because

00:00:54,449 --> 00:01:00,239
it's just a very easy stream that we can

00:00:57,750 --> 00:01:01,710
subscribe to we can compress it we don't

00:01:00,239 --> 00:01:05,100
need any bindings so we could pick any

00:01:01,710 --> 00:01:06,299
language we want I mean I guess

00:01:05,100 --> 00:01:07,890
technically you could do the same with a

00:01:06,299 --> 00:01:11,520
v-0 API you just have to write your own

00:01:07,890 --> 00:01:13,560
bindings but for this this was really

00:01:11,520 --> 00:01:18,900
easy for us because we could just kind

00:01:13,560 --> 00:01:21,659
of pick it up and go we we picked we

00:01:18,900 --> 00:01:24,090
decided to stick with the protobuf palos

00:01:21,659 --> 00:01:27,180
and instead of supporting both Perloff

00:01:24,090 --> 00:01:28,920
and json frost internally we didn't

00:01:27,180 --> 00:01:30,840
really have a use case to support both

00:01:28,920 --> 00:01:32,970
so we just did be one with the

00:01:30,840 --> 00:01:39,060
compressed protobuf payloads to be the

00:01:32,970 --> 00:01:41,040
most efficient so we use go as our

00:01:39,060 --> 00:01:44,189
language of choice for our framework in

00:01:41,040 --> 00:01:47,220
our SDK so a Verizon Labs is really big

00:01:44,189 --> 00:01:49,860
on go a lot of the the new developments

00:01:47,220 --> 00:01:53,610
going on there are using go

00:01:49,860 --> 00:01:55,680
we kind of have bested interest in it a

00:01:53,610 --> 00:01:58,200
lot of things are a lot of things

00:01:55,680 --> 00:02:00,990
started in it and a lot of people that

00:01:58,200 --> 00:02:02,909
were brought in kind of learned it from

00:02:00,990 --> 00:02:04,170
the ground up at Verizon Labs not

00:02:02,909 --> 00:02:06,450
everyone but a lot of people was like

00:02:04,170 --> 00:02:09,349
there it was a new language for them

00:02:06,450 --> 00:02:11,430
so what goal gives us specifically is

00:02:09,349 --> 00:02:13,080
increased developer speed it's a

00:02:11,430 --> 00:02:15,450
higher-level language but it's

00:02:13,080 --> 00:02:18,720
still really fast and doesn't take up

00:02:15,450 --> 00:02:20,400
much memory and since like schedulers

00:02:18,720 --> 00:02:22,260
and any kind of scheduler and me so so

00:02:20,400 --> 00:02:23,970
is mostly just IO since you're

00:02:22,260 --> 00:02:26,730
communicating with me so and kind of

00:02:23,970 --> 00:02:28,320
coordinating tasks and resources and

00:02:26,730 --> 00:02:31,800
scheduling it's it we don't really need

00:02:28,320 --> 00:02:33,990
like anything lower that you know we're

00:02:31,800 --> 00:02:36,930
not like crunching any numbers so this

00:02:33,990 --> 00:02:39,480
was perfect for us the concurrency

00:02:36,930 --> 00:02:42,240
primitives and go are great so makes it

00:02:39,480 --> 00:02:45,420
really easy to do threading and

00:02:42,240 --> 00:02:48,840
concurrency and all that good stuff and

00:02:45,420 --> 00:02:51,300
we get single binaries in the end for

00:02:48,840 --> 00:02:53,280
both our scheduler and executor so it

00:02:51,300 --> 00:02:55,770
makes it really easy to deploy and if

00:02:53,280 --> 00:02:58,050
for some reason we ever wanted to deploy

00:02:55,770 --> 00:03:00,030
these in a container we could just take

00:02:58,050 --> 00:03:01,770
like a scratch or busybox or Alpine

00:03:00,030 --> 00:03:04,440
image which is like two or five

00:03:01,770 --> 00:03:12,570
megabytes just drop it in and deploy it

00:03:04,440 --> 00:03:14,280
that way so as we started our

00:03:12,570 --> 00:03:17,220
development on the scheduler we noticed

00:03:14,280 --> 00:03:19,370
a lot of common functionality so we said

00:03:17,220 --> 00:03:22,800
hey we should just make an sdk for go

00:03:19,370 --> 00:03:24,870
there is another go SDK out there that

00:03:22,800 --> 00:03:27,170
was written by one fellow from Mesa

00:03:24,870 --> 00:03:32,100
sphere-like on its name right now but a

00:03:27,170 --> 00:03:33,780
lot of what they had done was customized

00:03:32,100 --> 00:03:36,500
their protobufs so it was hard to take

00:03:33,780 --> 00:03:40,410
the the protobuf definitions that we had

00:03:36,500 --> 00:03:44,630
from asos and then compile them and then

00:03:40,410 --> 00:03:44,630
what we ended up getting was some custom

00:03:45,050 --> 00:03:50,640
protobufs that they had utilized so it

00:03:47,340 --> 00:03:52,530
wasn't easy to transfer over and define

00:03:50,640 --> 00:03:54,390
the protobufs from the open-source

00:03:52,530 --> 00:03:56,489
version compared to the one that we had

00:03:54,390 --> 00:03:58,590
found online that was already out there

00:03:56,489 --> 00:04:00,750
so we ended up just using our own sdk

00:03:58,590 --> 00:04:01,380
cuz anytime there's a version update and

00:04:00,750 --> 00:04:03,900
the protobufs

00:04:01,380 --> 00:04:06,239
get updated we want to just run proto c

00:04:03,900 --> 00:04:08,519
create the the go bindings and then just

00:04:06,239 --> 00:04:10,860
go right we'll be done with it it's a

00:04:08,519 --> 00:04:15,000
lot easier some of the common patterns

00:04:10,860 --> 00:04:17,010
we had is you know this scheduler

00:04:15,000 --> 00:04:19,169
framework what that will look like tasks

00:04:17,010 --> 00:04:21,239
lifecycle management lifecycle

00:04:19,169 --> 00:04:25,020
management of offers resources that are

00:04:21,239 --> 00:04:26,490
coming in from the cluster also decoding

00:04:25,020 --> 00:04:29,310
the record IO of

00:04:26,490 --> 00:04:33,389
decoder so if anybody's familiar with

00:04:29,310 --> 00:04:37,500
writing frameworks when you authorize

00:04:33,389 --> 00:04:40,020
and connect to the master and you

00:04:37,500 --> 00:04:41,220
connect it gives you back a data

00:04:40,020 --> 00:04:43,289
structure essentially in the formative

00:04:41,220 --> 00:04:44,610
stream called record i/o tells you how

00:04:43,289 --> 00:04:46,889
many bytes you have and then gives you a

00:04:44,610 --> 00:04:49,650
payload message so all that decoding is

00:04:46,889 --> 00:04:51,720
done in the SDK as well as we have a

00:04:49,650 --> 00:04:54,060
struction for our persistence storage

00:04:51,720 --> 00:04:55,979
backends we use sed but we didn't want

00:04:54,060 --> 00:04:57,690
to make any decisions for anybody else

00:04:55,979 --> 00:04:58,550
so if people want to see zookeeper or

00:04:57,690 --> 00:05:01,020
anything

00:04:58,550 --> 00:05:04,020
they can choose their own back-end for

00:05:01,020 --> 00:05:05,039
that and we have our own proto bus as

00:05:04,020 --> 00:05:13,919
well that we've defined for the

00:05:05,039 --> 00:05:15,990
scheduler an executor so containers are

00:05:13,919 --> 00:05:18,030
definitely just containers it's nothing

00:05:15,990 --> 00:05:20,520
really too special about them part of

00:05:18,030 --> 00:05:24,030
the stock was you know we started using

00:05:20,520 --> 00:05:27,240
docker and we used marathon it worked

00:05:24,030 --> 00:05:29,069
fine for most of our use cases and we

00:05:27,240 --> 00:05:31,289
ran into a lot of issues once we started

00:05:29,069 --> 00:05:32,880
optimizing things and wanting additional

00:05:31,289 --> 00:05:35,550
features that docker didn't give us like

00:05:32,880 --> 00:05:39,000
an additional network interface which

00:05:35,550 --> 00:05:41,250
where CNI comes in so we we had a real

00:05:39,000 --> 00:05:43,620
big push to go towards the UCR and also

00:05:41,250 --> 00:05:45,240
get away from vendor lock-in we didn't

00:05:43,620 --> 00:05:47,490
want to be stuck with docker we wanted

00:05:45,240 --> 00:05:54,990
to be able to do run C or any other type

00:05:47,490 --> 00:05:56,669
of container runtime so some of the

00:05:54,990 --> 00:05:59,310
reasons why we went towards the UCR

00:05:56,669 --> 00:06:01,400
again like I said we didn't need docker

00:05:59,310 --> 00:06:06,030
and all the dependencies to be installed

00:06:01,400 --> 00:06:08,729
we noticed a lot that operationally when

00:06:06,030 --> 00:06:11,849
we had customers using our cluster that

00:06:08,729 --> 00:06:14,069
docker daemon would get stuck or crash

00:06:11,849 --> 00:06:16,169
or system D would cause some issue and

00:06:14,069 --> 00:06:17,430
that would go up to date the docker game

00:06:16,169 --> 00:06:19,610
and it gets stuck and it was causing a

00:06:17,430 --> 00:06:22,229
lot of pain points for our customers

00:06:19,610 --> 00:06:24,780
with the UCR we haven't had as many

00:06:22,229 --> 00:06:27,090
issues in terms of bugs during

00:06:24,780 --> 00:06:30,289
production there's a reduced attack

00:06:27,090 --> 00:06:34,680
surface we can control the types of

00:06:30,289 --> 00:06:36,750
issues that are happening with the UCR

00:06:34,680 --> 00:06:39,330
you can be very specific about exactly

00:06:36,750 --> 00:06:41,800
what you wanted to run

00:06:39,330 --> 00:06:45,009
and there's also no need to manage

00:06:41,800 --> 00:06:46,479
secrets this a little while ago I can't

00:06:45,009 --> 00:06:48,879
remember what release it was from ASOS

00:06:46,479 --> 00:06:51,280
you had to actually use kind of a hack

00:06:48,879 --> 00:06:54,219
and use the fetcher to grab the docker

00:06:51,280 --> 00:06:56,080
JSON with the encoded registry

00:06:54,219 --> 00:06:58,300
information loaded into your container

00:06:56,080 --> 00:06:59,680
and then use that to authenticate to a

00:06:58,300 --> 00:07:01,629
registry and then pull down your image

00:06:59,680 --> 00:07:05,139
now you can actually just send it right

00:07:01,629 --> 00:07:08,229
into the protobuf and now we also have

00:07:05,139 --> 00:07:09,759
OC I support docker has that as well but

00:07:08,229 --> 00:07:11,830
we have broader support for container

00:07:09,759 --> 00:07:19,629
image specifications with the UCR

00:07:11,830 --> 00:07:26,560
compared to dagger so the UCR is emits

00:07:19,629 --> 00:07:28,000
false so we hid a lot of issues on any

00:07:26,560 --> 00:07:30,340
version of me so it's less than one

00:07:28,000 --> 00:07:32,710
point two so if anyone's looking to use

00:07:30,340 --> 00:07:35,620
the UCR I would recommend sticking with

00:07:32,710 --> 00:07:38,080
one point two and up so I just put up

00:07:35,620 --> 00:07:39,279
some of the jeers here if anyone's

00:07:38,080 --> 00:07:43,089
interested in looking them up you know

00:07:39,279 --> 00:07:45,969
feel free but um generally the the

00:07:43,089 --> 00:07:49,930
bottom section is it describes a bunch

00:07:45,969 --> 00:07:53,199
of issues around the overlay handling in

00:07:49,930 --> 00:07:56,740
the whiteout files and white house not

00:07:53,199 --> 00:07:58,659
being applied properly and some of the

00:07:56,740 --> 00:08:01,240
image backend problems we hit were I

00:07:58,659 --> 00:08:03,039
think one of them was the older versions

00:08:01,240 --> 00:08:05,440
of me so Steve faulted to use the copy

00:08:03,039 --> 00:08:06,909
back-end so when you pull down on the

00:08:05,440 --> 00:08:10,029
docker image it will use this copy

00:08:06,909 --> 00:08:11,169
back-end and it it failed on something

00:08:10,029 --> 00:08:14,229
when it was on tarring and there

00:08:11,169 --> 00:08:16,419
symlinks so it just exploded there are a

00:08:14,229 --> 00:08:18,460
couple of other issues surrounding that

00:08:16,419 --> 00:08:20,199
I think there are even some issues with

00:08:18,460 --> 00:08:23,199
their other backends and the older

00:08:20,199 --> 00:08:24,819
versions so we so there's a little bit

00:08:23,199 --> 00:08:27,339
more than this but generally like the

00:08:24,819 --> 00:08:30,610
issues here kind of affect 101.1

00:08:27,339 --> 00:08:33,459
and a lot of the minor versions in

00:08:30,610 --> 00:08:35,979
between so we just totally skipped those

00:08:33,459 --> 00:08:40,990
versions thankfully we had the the

00:08:35,979 --> 00:08:43,120
luxury to do that and we would really

00:08:40,990 --> 00:08:45,040
like to see user namespace support in

00:08:43,120 --> 00:08:46,870
the UCR I know this is something that

00:08:45,040 --> 00:08:50,740
they're they're thinking about working

00:08:46,870 --> 00:08:52,420
on there are some some issues right now

00:08:50,740 --> 00:08:52,660
that make it really difficult for them

00:08:52,420 --> 00:08:54,400
to

00:08:52,660 --> 00:08:58,870
with this I think they're hoping to wait

00:08:54,400 --> 00:09:01,060
for dust to settle on uid shifts when

00:08:58,870 --> 00:09:02,860
you amount so like so when you have two

00:09:01,060 --> 00:09:05,050
containers and they share a volume and

00:09:02,860 --> 00:09:06,880
they're both they're both pointing in

00:09:05,050 --> 00:09:08,410
this volume it doesn't turn into a total

00:09:06,880 --> 00:09:10,900
disaster with different you IDs and

00:09:08,410 --> 00:09:12,730
everything I think SEC comp they're

00:09:10,900 --> 00:09:15,390
moving pretty quickly towards it's a

00:09:12,730 --> 00:09:17,650
much easier thing to implement so

00:09:15,390 --> 00:09:19,780
initially we were thinking of doing this

00:09:17,650 --> 00:09:21,430
stuff in our custom executor it's part

00:09:19,780 --> 00:09:22,900
of the framework but I think it makes

00:09:21,430 --> 00:09:24,580
more sense to have in me sauce in the

00:09:22,900 --> 00:09:26,020
long run because then everyone can

00:09:24,580 --> 00:09:27,340
benefit from it and you don't have to

00:09:26,020 --> 00:09:28,510
have your own custom executor if you

00:09:27,340 --> 00:09:32,530
don't want you can just use the default

00:09:28,510 --> 00:09:33,940
executor so definitely not a smooth path

00:09:32,530 --> 00:09:36,400
but once we like got through all these

00:09:33,940 --> 00:09:39,790
issues it's been it's been good it's

00:09:36,400 --> 00:09:42,370
been really good so I had recorded a

00:09:39,790 --> 00:09:45,730
demo initially I want to do a live demo

00:09:42,370 --> 00:09:49,390
instead take the risk so why not so I

00:09:45,730 --> 00:09:53,050
just want to show launching maybe 500

00:09:49,390 --> 00:09:56,110
tasks on our framework in our cluster

00:09:53,050 --> 00:09:58,450
and just kind of give you an idea of

00:09:56,110 --> 00:09:59,980
like how easy it is to use and just like

00:09:58,450 --> 00:10:03,390
show you what it would be like to do

00:09:59,980 --> 00:10:03,390
this with with a custom framework

00:10:26,520 --> 00:10:31,660
you know I can switch this over to get

00:10:28,840 --> 00:10:34,440
this green do you know I can switch this

00:10:31,660 --> 00:10:34,440
over to the other screen

00:10:38,950 --> 00:10:42,770
can write a framework where we can't

00:10:41,150 --> 00:10:48,730
operate at Mack it's a lot of

00:10:42,770 --> 00:11:06,710
specialties yeah sorry guys here we go

00:10:48,730 --> 00:11:09,980
mirrored displays that's good this is

00:11:06,710 --> 00:11:12,500
the miso CY for the cluster that's

00:11:09,980 --> 00:11:14,570
running in our our our cloud that we

00:11:12,500 --> 00:11:16,550
actually are working on in addition to

00:11:14,570 --> 00:11:18,020
our framework so we we're building out

00:11:16,550 --> 00:11:20,300
our own cloud and the frameworks running

00:11:18,020 --> 00:11:21,710
on top of it and I just want to show you

00:11:20,300 --> 00:11:30,530
like how all the tasks will be running

00:11:21,710 --> 00:11:31,730
and how smooth of a process it is so

00:11:30,530 --> 00:11:33,940
we've basically tunneled into our

00:11:31,730 --> 00:11:36,920
cluster we're just going to run

00:11:33,940 --> 00:11:43,520
basically a load tester that we have 500

00:11:36,920 --> 00:11:45,950
tests go forth so what this is doing is

00:11:43,520 --> 00:11:47,690
hitting our endpoint our API is just

00:11:45,950 --> 00:11:49,310
returning this simple little JSON

00:11:47,690 --> 00:11:51,650
payload back telling you that it was

00:11:49,310 --> 00:11:52,850
successfully queued it will just keep

00:11:51,650 --> 00:11:55,340
going until it's done and you'll

00:11:52,850 --> 00:11:57,770
sexually see it appear in the web UI for

00:11:55,340 --> 00:12:02,450
the master so we can see all the tasks

00:11:57,770 --> 00:12:03,920
here running it's very simple it's

00:12:02,450 --> 00:12:07,460
extremely fast we've done some other

00:12:03,920 --> 00:12:08,690
tests I launched 50,000 tasks because it

00:12:07,460 --> 00:12:10,550
was awesome there was really no reason

00:12:08,690 --> 00:12:13,580
to do it but it was able to launch

00:12:10,550 --> 00:12:15,500
50,000 tasks and roughly I think it was

00:12:13,580 --> 00:12:21,980
like 40 something seconds so it's quite

00:12:15,500 --> 00:12:24,230
quick I believe a demo we had done two

00:12:21,980 --> 00:12:26,420
years ago I think it was two years ago

00:12:24,230 --> 00:12:28,970
that Larry did a demo on marathon and

00:12:26,420 --> 00:12:30,920
marathon did it in about 70 seconds were

00:12:28,970 --> 00:12:34,630
actually faster than marathon in terms

00:12:30,920 --> 00:12:34,630
of our tasks deployment

00:12:36,210 --> 00:12:41,520
so the tasks on launching are kind of

00:12:39,570 --> 00:12:43,560
basic they're just like a very general

00:12:41,520 --> 00:12:46,529
like you know they might echo something

00:12:43,560 --> 00:12:50,190
and sleep for some random period with a

00:12:46,529 --> 00:12:51,390
range and then just finish so one of the

00:12:50,190 --> 00:12:52,350
things that's a little bit unique about

00:12:51,390 --> 00:12:55,080
our framework because we made it

00:12:52,350 --> 00:12:58,260
flexible to not only run a long-running

00:12:55,080 --> 00:13:01,200
task but also like jobs so when things I

00:12:58,260 --> 00:13:02,970
mean when things fail and they crash or

00:13:01,200 --> 00:13:04,680
fail hard will actually get like failure

00:13:02,970 --> 00:13:07,649
statuses but if you have something that

00:13:04,680 --> 00:13:09,600
runs and completes an exit zero it will

00:13:07,649 --> 00:13:10,709
be finished you know it's not it's some

00:13:09,600 --> 00:13:13,110
of the other frameworks when they see

00:13:10,709 --> 00:13:14,910
something stop or go down regardless of

00:13:13,110 --> 00:13:15,330
how they exit they'll try to reschedule

00:13:14,910 --> 00:13:17,279
it

00:13:15,330 --> 00:13:19,080
so we'll just we just regard it as

00:13:17,279 --> 00:13:24,870
finished so the our long-running process

00:13:19,080 --> 00:13:26,190
just we assume that if you crash or if

00:13:24,870 --> 00:13:28,770
you stop running then something's gone

00:13:26,190 --> 00:13:31,230
wrong and you've crashed otherwise if

00:13:28,770 --> 00:13:34,529
you exit successfully we assume that's

00:13:31,230 --> 00:13:36,510
okay and this is kind of interesting so

00:13:34,529 --> 00:13:38,550
kind of like a side note but the I found

00:13:36,510 --> 00:13:41,870
the new versions of me sews for the web

00:13:38,550 --> 00:13:44,339
UI anyway if you view it over a tunnel

00:13:41,870 --> 00:13:47,190
it's always trying to like hit the

00:13:44,339 --> 00:13:49,080
internal IPS of the new leader that's

00:13:47,190 --> 00:13:51,240
trying to detect or any agents that you

00:13:49,080 --> 00:13:53,339
go to so you might just see some of the

00:13:51,240 --> 00:13:56,540
these errors pop up so I have to keep

00:13:53,339 --> 00:14:01,589
refreshing manually to get around that

00:13:56,540 --> 00:14:03,390
so I think most of these should be done

00:14:01,589 --> 00:14:07,740
yeah so everything is completed

00:14:03,390 --> 00:14:09,120
everything's finished you can see it's

00:14:07,740 --> 00:14:11,160
really pretty simple you know once you

00:14:09,120 --> 00:14:14,040
have the basics of a framework up and

00:14:11,160 --> 00:14:15,930
running to however you want to accept

00:14:14,040 --> 00:14:18,360
data to launch tasks and actually run it

00:14:15,930 --> 00:14:20,010
it's it's it's really quick go has

00:14:18,360 --> 00:14:23,040
helped us to be really efficient so this

00:14:20,010 --> 00:14:24,450
takes like practically no CPU even like

00:14:23,040 --> 00:14:26,040
heavily loading it's not that much

00:14:24,450 --> 00:14:28,140
there's not that much memory so be like

00:14:26,040 --> 00:14:29,760
are we measure like 15 megabytes of

00:14:28,140 --> 00:14:32,250
memory when he when he was hitting it

00:14:29,760 --> 00:14:37,760
with 50,000 tasks at the most it's like

00:14:32,250 --> 00:14:37,760
nothing absolutely nothing so

00:14:42,940 --> 00:14:47,440
we also open sourced our framework so if

00:14:45,700 --> 00:14:49,570
you'd like to check it out horizon it's

00:14:47,440 --> 00:14:52,390
a github.com slash verizon labs slash

00:14:49,570 --> 00:14:53,770
hydrogen we'd love to take pull requests

00:14:52,390 --> 00:14:54,750
and interviews you can tell us our code

00:14:53,770 --> 00:14:56,620
sucks

00:14:54,750 --> 00:14:59,080
anything would be awesome and we'd

00:14:56,620 --> 00:15:00,970
really appreciate it and if you run it -

00:14:59,080 --> 00:15:02,320
just for fun you know at home on some

00:15:00,970 --> 00:15:04,600
raspberry PI's or whatever that'd be

00:15:02,320 --> 00:15:08,500
cool - we want to support all use cases

00:15:04,600 --> 00:15:09,850
we really tried to make make the SDK

00:15:08,500 --> 00:15:11,670
because we wanted more people to utilize

00:15:09,850 --> 00:15:14,680
it and not just make it specific for

00:15:11,670 --> 00:15:17,230
Verizon so it'd be great to see what

00:15:14,680 --> 00:15:19,090
other people use it for and our focus

00:15:17,230 --> 00:15:21,100
was really smaller clusters and

00:15:19,090 --> 00:15:25,570
workloads marathons very good at using

00:15:21,100 --> 00:15:27,190
or for marathons better for larger

00:15:25,570 --> 00:15:29,440
clusters so like a thousand plus nodes

00:15:27,190 --> 00:15:31,510
but some companies or I should say most

00:15:29,440 --> 00:15:33,670
companies probably run maybe anywhere

00:15:31,510 --> 00:15:36,340
from the fifty to a hundred or a few

00:15:33,670 --> 00:15:39,820
hundred node range right for their

00:15:36,340 --> 00:15:41,380
workloads so another aspect we want to

00:15:39,820 --> 00:15:43,560
talk about is container networking

00:15:41,380 --> 00:15:46,000
interface like I mentioned earlier the

00:15:43,560 --> 00:15:47,890
main issue that we had with docker was

00:15:46,000 --> 00:15:50,230
we couldn't use multiple networking

00:15:47,890 --> 00:15:52,180
interfaces this was a big problem for us

00:15:50,230 --> 00:15:53,890
on her front end we wanted a public IP

00:15:52,180 --> 00:15:55,660
in our container and we wanted a back in

00:15:53,890 --> 00:15:58,510
IP we also wanted to be able to plug in

00:15:55,660 --> 00:16:00,040
say a storage network whatever we wanted

00:15:58,510 --> 00:16:01,390
to give it multiple interfaces we

00:16:00,040 --> 00:16:03,090
couldn't do that we also wanted to

00:16:01,390 --> 00:16:06,070
change the way that we were networking

00:16:03,090 --> 00:16:08,080
docker just gave us pretty much just the

00:16:06,070 --> 00:16:12,070
bridge in the the net right you get your

00:16:08,080 --> 00:16:13,930
172 dot 117 0.1 slash 16 it hands it out

00:16:12,070 --> 00:16:15,940
and that's it and that was problematic

00:16:13,930 --> 00:16:17,710
for some of our applications that are

00:16:15,940 --> 00:16:20,500
very networking heavy or and/or doing

00:16:17,710 --> 00:16:21,850
long-lived TCP connections so you can

00:16:20,500 --> 00:16:23,980
see to support the type of plugins here

00:16:21,850 --> 00:16:26,470
for container network interface I don't

00:16:23,980 --> 00:16:27,880
know if how many people here I've heard

00:16:26,470 --> 00:16:29,530
of see and I know what it is just

00:16:27,880 --> 00:16:33,190
raising hand okay you guys already know

00:16:29,530 --> 00:16:36,250
then I won't go over it so you're well

00:16:33,190 --> 00:16:38,590
aware of you know how CNI works and all

00:16:36,250 --> 00:16:40,750
the things that it supports the good

00:16:38,590 --> 00:16:44,080
thing is since it's a standard and the

00:16:40,750 --> 00:16:46,150
UCR supports it we can then leverage

00:16:44,080 --> 00:16:48,220
that make our own plugin if we wanted to

00:16:46,150 --> 00:16:50,500
and we can also hook in as many

00:16:48,220 --> 00:16:51,820
interfaces that we want to each

00:16:50,500 --> 00:16:56,650
container

00:16:51,820 --> 00:16:58,870
so what we've done is go back one second

00:16:56,650 --> 00:17:01,510
sorry what we've done is we've stuck

00:16:58,870 --> 00:17:04,690
with bridge for now but we use a host

00:17:01,510 --> 00:17:09,850
local eye pin so we run on a 10/8

00:17:04,690 --> 00:17:11,560
network for ipv4 and what we end up

00:17:09,850 --> 00:17:13,510
doing is giving each host the dot one

00:17:11,560 --> 00:17:17,800
acts as a router we're out on the hosts

00:17:13,510 --> 00:17:19,990
in each slash 24 gives us 254 useable

00:17:17,800 --> 00:17:22,870
addresses for containers per host we

00:17:19,990 --> 00:17:29,230
actually don't need a DHCP server

00:17:22,870 --> 00:17:31,690
because each host has a specified format

00:17:29,230 --> 00:17:34,150
right so it's ten dot dot slot on our

00:17:31,690 --> 00:17:36,820
chassis in the physical data center and

00:17:34,150 --> 00:17:38,230
what that allows us to do is we don't

00:17:36,820 --> 00:17:40,270
need to really keep track of things the

00:17:38,230 --> 00:17:41,800
house just needs to know about that / 24

00:17:40,270 --> 00:17:43,240
and it's all unique across every host

00:17:41,800 --> 00:17:45,130
and we're all good so we actually

00:17:43,240 --> 00:17:47,830
reduced the need for DHCP right there

00:17:45,130 --> 00:17:50,340
which was pretty cool because even

00:17:47,830 --> 00:17:52,510
managing DHCP got a little bit annoying

00:17:50,340 --> 00:17:54,370
when things would go down when other

00:17:52,510 --> 00:17:57,670
tasks would go down and it sort of

00:17:54,370 --> 00:18:03,100
escalates from there we do in our lab

00:17:57,670 --> 00:18:04,870
use mac VLAN we've also done IP VLAN

00:18:03,100 --> 00:18:06,340
tests a lot of different things we've

00:18:04,870 --> 00:18:08,290
written our own plugins as well we're

00:18:06,340 --> 00:18:10,810
trying to accomplish a VPN which would

00:18:08,290 --> 00:18:13,210
be really cool to do 4vx LAN for a

00:18:10,810 --> 00:18:14,260
multi-tenancy inside of our clusters so

00:18:13,210 --> 00:18:16,600
there's other things that we're looking

00:18:14,260 --> 00:18:18,790
into and CNI is definitely really

00:18:16,600 --> 00:18:20,500
exciting to enable us to be able to do

00:18:18,790 --> 00:18:25,150
all these different networking types and

00:18:20,500 --> 00:18:26,980
the in the data center so since you guys

00:18:25,150 --> 00:18:28,720
already know about CNI how it works I

00:18:26,980 --> 00:18:30,580
won't go over this this is just a bland

00:18:28,720 --> 00:18:35,740
you know where is everything go how does

00:18:30,580 --> 00:18:38,080
it actually interface with maysa so all

00:18:35,740 --> 00:18:39,610
all you need to do now in our definition

00:18:38,080 --> 00:18:41,650
you can see this on github and the

00:18:39,610 --> 00:18:43,920
readme about how to launch a task on our

00:18:41,650 --> 00:18:48,130
framework really just need to define a

00:18:43,920 --> 00:18:50,380
network just like CNI tell it okay it's

00:18:48,130 --> 00:18:50,800
a bridge it's mac VLAN whatever it might

00:18:50,380 --> 00:18:54,310
be

00:18:50,800 --> 00:18:57,490
give it the name data net my network

00:18:54,310 --> 00:19:00,160
whatever it might want to be and then

00:18:57,490 --> 00:19:02,740
the network protobuf for us underneath

00:19:00,160 --> 00:19:04,420
the covers we just say this name you

00:19:02,740 --> 00:19:05,680
don't even have to say CNI you just say

00:19:04,420 --> 00:19:08,110
my network and

00:19:05,680 --> 00:19:09,850
your task and you can put multiple of

00:19:08,110 --> 00:19:11,980
them it's a list that it takes in and

00:19:09,850 --> 00:19:15,040
when you do that you can end up getting

00:19:11,980 --> 00:19:16,510
any number or any end number of network

00:19:15,040 --> 00:19:18,640
interfaces and they can be each unique

00:19:16,510 --> 00:19:20,530
type right so one could be a bridge one

00:19:18,640 --> 00:19:21,850
could be McFeely and one could be VX LAN

00:19:20,530 --> 00:19:24,220
I mean you can do whatever you want with

00:19:21,850 --> 00:19:27,850
it which is pretty awesome and we've

00:19:24,220 --> 00:19:32,050
done that in our cluster for segregating

00:19:27,850 --> 00:19:33,490
our storage and we also have one for our

00:19:32,050 --> 00:19:35,590
front-end so we can define our front-end

00:19:33,490 --> 00:19:37,390
network with public IPs and pick right

00:19:35,590 --> 00:19:39,970
from that pool so we no longer have to

00:19:37,390 --> 00:19:42,430
manage state of who has what IP are here

00:19:39,970 --> 00:19:44,230
there some people can also pick static

00:19:42,430 --> 00:19:46,450
IPS that are reserved and it'll always

00:19:44,230 --> 00:19:48,550
give them that static IP so you can use

00:19:46,450 --> 00:19:50,530
any caste on the front end so we can

00:19:48,550 --> 00:19:52,150
have six or seven eight whatever one

00:19:50,530 --> 00:19:54,400
hundred instances of some application

00:19:52,150 --> 00:19:56,980
uses any caste and whatever is closest

00:19:54,400 --> 00:20:01,210
it just gets routed via BBC being gets

00:19:56,980 --> 00:20:02,290
hit any questions about CNI I just want

00:20:01,210 --> 00:20:05,260
to make sure everybody knows what it is

00:20:02,290 --> 00:20:10,090
and I'm not scheming over anything okay

00:20:05,260 --> 00:20:13,180
cool I just want to touch on one other

00:20:10,090 --> 00:20:15,370
thing too before he moved on we didn't

00:20:13,180 --> 00:20:17,410
put too much about this in our our talk

00:20:15,370 --> 00:20:20,380
because it's a little bit more specific

00:20:17,410 --> 00:20:22,900
but I want to say that we did make our

00:20:20,380 --> 00:20:24,760
framework a che so if anyone's thinking

00:20:22,900 --> 00:20:27,370
about making a framework I would say

00:20:24,760 --> 00:20:29,380
that in general it's very easy to do if

00:20:27,370 --> 00:20:31,600
you're using like a distributed

00:20:29,380 --> 00:20:33,010
key-value store underneath or if you're

00:20:31,600 --> 00:20:34,960
using zookeeper it's even easier because

00:20:33,010 --> 00:20:40,000
they have more primitives but with that

00:20:34,960 --> 00:20:41,140
CD all we really needed to do was I mean

00:20:40,000 --> 00:20:43,330
it's it's a little bit more complicated

00:20:41,140 --> 00:20:45,820
than this but generally like we have all

00:20:43,330 --> 00:20:48,910
these frameworks come up one wins is the

00:20:45,820 --> 00:20:51,160
leader then the the ones that don't win

00:20:48,910 --> 00:20:54,100
connect to the the leader and then when

00:20:51,160 --> 00:20:56,680
if that framework ever comes down the

00:20:54,100 --> 00:20:58,690
another one they'll all basically fight

00:20:56,680 --> 00:21:00,280
for the leader position again and one of

00:20:58,690 --> 00:21:02,140
them will win and we'll go back to this

00:21:00,280 --> 00:21:03,880
connection model so the reason why we

00:21:02,140 --> 00:21:06,360
had this like TCP connection across all

00:21:03,880 --> 00:21:09,610
of our frameworks is because zookeeper

00:21:06,360 --> 00:21:11,980
internally has this ephemeral z node

00:21:09,610 --> 00:21:13,870
concept and I believe underneath it's

00:21:11,980 --> 00:21:17,410
just a TCP connection so like when you

00:21:13,870 --> 00:21:19,810
die your connection gets cut your your

00:21:17,410 --> 00:21:23,140
key goes away so we've kind of emulated

00:21:19,810 --> 00:21:26,410
so after after getting that part solved

00:21:23,140 --> 00:21:29,380
it was very easy so I just want to touch

00:21:26,410 --> 00:21:31,120
that very quickly one quick note about

00:21:29,380 --> 00:21:32,770
that as well as we have NCD as our

00:21:31,120 --> 00:21:36,100
backing store and that already runs the

00:21:32,770 --> 00:21:37,720
raft consensus algorithm so there is

00:21:36,100 --> 00:21:39,580
really no need for us to implement

00:21:37,720 --> 00:21:41,380
another consensus algorithm a sort of

00:21:39,580 --> 00:21:43,570
waste of time so basically what he's

00:21:41,380 --> 00:21:46,540
describing is we just have each leader

00:21:43,570 --> 00:21:49,000
leader tried to do a TTL key basically

00:21:46,540 --> 00:21:51,310
and that's IDI whatever gets there first

00:21:49,000 --> 00:21:53,830
becomes the leader and SCD takes care of

00:21:51,310 --> 00:21:55,180
distributing in that state so once that

00:21:53,830 --> 00:21:56,980
key goes away and it hasn't been

00:21:55,180 --> 00:21:58,930
refreshed for some an amount of

00:21:56,980 --> 00:22:00,460
configurable time we can pick a new

00:21:58,930 --> 00:22:04,930
leader and we assume that's a network

00:22:00,460 --> 00:22:07,960
partition but back to CNI I mentioned a

00:22:04,930 --> 00:22:09,550
lot of this as well one of the biggest

00:22:07,960 --> 00:22:11,530
things for us was end-user visibility in

00:22:09,550 --> 00:22:14,830
a multi-tenant environment when somebody

00:22:11,530 --> 00:22:15,880
looks at their application you don't

00:22:14,830 --> 00:22:17,410
want them to be able to see other

00:22:15,880 --> 00:22:18,610
people's networks you know you don't

00:22:17,410 --> 00:22:21,700
want them to be able to connect to it

00:22:18,610 --> 00:22:25,120
ruin it send crazy amounts of broadcasts

00:22:21,700 --> 00:22:27,790
over it do whatever they might do so the

00:22:25,120 --> 00:22:31,300
isolation is really key for us and it

00:22:27,790 --> 00:22:33,220
allowed us to basically you know like a

00:22:31,300 --> 00:22:34,900
VMware you it looks like you have your

00:22:33,220 --> 00:22:36,220
own private bridge networker and I'll to

00:22:34,900 --> 00:22:39,190
network you can do the same thing with

00:22:36,220 --> 00:22:40,600
containers and have multiple tenants on

00:22:39,190 --> 00:22:44,740
it and you can actually manage each

00:22:40,600 --> 00:22:48,190
network from CNI per service or customer

00:22:44,740 --> 00:22:49,060
however you want to put that and all the

00:22:48,190 --> 00:22:50,800
other benefits that come with

00:22:49,060 --> 00:22:53,110
standardization right there's no vendor

00:22:50,800 --> 00:22:56,590
lock-in we can move to different

00:22:53,110 --> 00:22:59,230
standards above that I mentioned earlier

00:22:56,590 --> 00:23:01,120
we can change to an overlay mechanism

00:22:59,230 --> 00:23:04,120
like the excellent or envy GRE without

00:23:01,120 --> 00:23:05,440
the end-user noticing so see if we

00:23:04,120 --> 00:23:07,500
originally used a bridge for this

00:23:05,440 --> 00:23:09,580
customer we can say hey we're gonna do

00:23:07,500 --> 00:23:10,990
VX land they don't even have to know

00:23:09,580 --> 00:23:13,600
what that is we can just do it

00:23:10,990 --> 00:23:16,510
phenomenal work underneath it also gives

00:23:13,600 --> 00:23:18,550
you decent ipm address management like I

00:23:16,510 --> 00:23:21,010
said CNI doesn't have DHCP on its own

00:23:18,550 --> 00:23:25,090
what it will do is basically make a call

00:23:21,010 --> 00:23:28,300
to any other DHCP management system so

00:23:25,090 --> 00:23:30,490
you can run like ISDN DHCP server and

00:23:28,300 --> 00:23:31,870
that'll take care of it for you and D

00:23:30,490 --> 00:23:32,429
and I basically call some hooks into

00:23:31,870 --> 00:23:37,559
there

00:23:32,429 --> 00:23:40,470
for that so sorry for the tiny text here

00:23:37,559 --> 00:23:43,080
but this is some logs for my agents you

00:23:40,470 --> 00:23:47,220
can see there's two networks dev net and

00:23:43,080 --> 00:23:50,429
data net this container got 1150 11.2

00:23:47,220 --> 00:23:52,889
and the other one got 1050 11 . 12/24

00:23:50,429 --> 00:23:56,159
these are just two networks I made just

00:23:52,889 --> 00:23:57,600
just as a face of example but there's

00:23:56,159 --> 00:24:00,119
some logs here just to show that there's

00:23:57,600 --> 00:24:02,190
some IPS coming there and this is the

00:24:00,119 --> 00:24:03,809
host local with a bridge so these are

00:24:02,190 --> 00:24:07,230
just two bridge networks on the host

00:24:03,809 --> 00:24:10,019
they got attached into the UCR and you

00:24:07,230 --> 00:24:12,480
can see this is uh I just used NS enter

00:24:10,019 --> 00:24:15,860
got into the namespace did an IP address

00:24:12,480 --> 00:24:17,999
show and you can see the two V's there

00:24:15,860 --> 00:24:19,980
they're both flower up you know you

00:24:17,999 --> 00:24:22,259
could talk over everything the routes

00:24:19,980 --> 00:24:23,909
are there if you want to get out one

00:24:22,259 --> 00:24:27,840
thing that we did run into is that

00:24:23,909 --> 00:24:30,330
whatever network comes up this default

00:24:27,840 --> 00:24:32,399
route sometimes goes out the wrong

00:24:30,330 --> 00:24:34,590
interface so sometimes we need to make

00:24:32,399 --> 00:24:37,559
sure that the default route is correct

00:24:34,590 --> 00:24:39,720
for that application namely if you have

00:24:37,559 --> 00:24:40,889
a front-end network someone's trying to

00:24:39,720 --> 00:24:43,320
get out to the Internet but the default

00:24:40,889 --> 00:24:45,509
route is out the the internal network

00:24:43,320 --> 00:24:46,830
it's not going to work so there's some

00:24:45,509 --> 00:24:48,869
ordering that has to go there but this

00:24:46,830 --> 00:24:54,350
is just an example I want to explain

00:24:48,869 --> 00:24:57,269
that caveat is anybody else enabled any

00:24:54,350 --> 00:24:58,950
multiple network interfaces in the UCR I

00:24:57,269 --> 00:25:00,809
haven't heard of this in the community

00:24:58,950 --> 00:25:03,210
I'm really curious if somebody else's

00:25:00,809 --> 00:25:05,820
gotten this to work or done anything

00:25:03,210 --> 00:25:09,629
with it no okay

00:25:05,820 --> 00:25:12,179
I'm really excited about it right

00:25:09,629 --> 00:25:14,789
there's no ipv6 support in May so so

00:25:12,179 --> 00:25:19,649
right now it's being actively worked on

00:25:14,789 --> 00:25:21,419
and see and I just got some preliminary

00:25:19,649 --> 00:25:24,029
features for that but it'd be really

00:25:21,419 --> 00:25:26,850
interested in getting that done with

00:25:24,029 --> 00:25:28,289
ipv6 IP per container becomes extremely

00:25:26,850 --> 00:25:32,070
easy you don't have to worry about

00:25:28,289 --> 00:25:34,169
stepping on toes we don't have support

00:25:32,070 --> 00:25:36,240
for dynamic traffic policy filtering

00:25:34,169 --> 00:25:38,220
this can be done in other ways but it'd

00:25:36,240 --> 00:25:40,440
be really nice to have a hook inside of

00:25:38,220 --> 00:25:43,950
CNI it could just be another plugin but

00:25:40,440 --> 00:25:46,940
that's not really there yet and support

00:25:43,950 --> 00:25:50,360
for dynamic updates to existing network

00:25:46,940 --> 00:25:52,610
I believe it was before me so so 1.2 the

00:25:50,360 --> 00:25:54,260
CNI configs you actually had a restart

00:25:52,610 --> 00:25:56,210
the agent to read the configs into

00:25:54,260 --> 00:25:57,560
memory but now you can change the

00:25:56,210 --> 00:26:00,200
network configs and it reads it

00:25:57,560 --> 00:26:01,150
on-demand so if you change the existing

00:26:00,200 --> 00:26:03,500
network configure

00:26:01,150 --> 00:26:05,000
it should just update and you don't have

00:26:03,500 --> 00:26:06,470
to restart the agent and you know

00:26:05,000 --> 00:26:14,390
restart all these containers and cause a

00:26:06,470 --> 00:26:16,340
lot of a lot of issues so so before we

00:26:14,390 --> 00:26:18,410
wrap up for questions I just want to say

00:26:16,340 --> 00:26:20,180
that we get asked a lot why we made our

00:26:18,410 --> 00:26:22,760
own framework you know it's it's generic

00:26:20,180 --> 00:26:24,320
it's sort of a I mean marathon obviously

00:26:22,760 --> 00:26:26,420
has more features it's been a lot longer

00:26:24,320 --> 00:26:28,610
but we it's it's kind of a generic

00:26:26,420 --> 00:26:30,680
framework that anyone could use but I

00:26:28,610 --> 00:26:33,260
think the real power from this comes

00:26:30,680 --> 00:26:35,720
from some future work that we're going

00:26:33,260 --> 00:26:39,110
to do but also a lot of the the CNI and

00:26:35,720 --> 00:26:41,390
storage stuff that we have going on in

00:26:39,110 --> 00:26:43,040
the background too so one of the things

00:26:41,390 --> 00:26:45,050
that we had talked about doing on the

00:26:43,040 --> 00:26:47,060
near future is kind of tying the

00:26:45,050 --> 00:26:49,370
framework into some some more advanced

00:26:47,060 --> 00:26:54,440
scheduling algorithms so we want to

00:26:49,370 --> 00:26:58,820
basically efficiently schedule tasks and

00:26:54,440 --> 00:27:00,950
to nodes across fault domains and shut

00:26:58,820 --> 00:27:02,570
down everything that we're not using so

00:27:00,950 --> 00:27:04,520
when we do need to scale up we can

00:27:02,570 --> 00:27:06,890
actually like integrate our framework

00:27:04,520 --> 00:27:09,170
with IPMI tool or make a call out to

00:27:06,890 --> 00:27:11,720
IPMI ourselves or however we want to do

00:27:09,170 --> 00:27:15,530
it and power things on dynamically as we

00:27:11,720 --> 00:27:17,930
need them we can also do a lot of fancy

00:27:15,530 --> 00:27:19,010
stuff in our custom executor so we were

00:27:17,930 --> 00:27:20,540
actually like I said earlier we were

00:27:19,010 --> 00:27:23,300
thinking of doing username spaces and

00:27:20,540 --> 00:27:24,950
set comp and there might be better to

00:27:23,300 --> 00:27:28,100
have in me so so and focus on doing at

00:27:24,950 --> 00:27:29,900
musso's instead but you know we had some

00:27:28,100 --> 00:27:32,060
security requirements where we wanted to

00:27:29,900 --> 00:27:35,150
make use of keys and the TPM we can

00:27:32,060 --> 00:27:37,670
integrate with that we can we can do a

00:27:35,150 --> 00:27:39,350
lot of stuff that is very unique to our

00:27:37,670 --> 00:27:41,840
platform and since we're building a

00:27:39,350 --> 00:27:44,330
platform and the framework we can we can

00:27:41,840 --> 00:27:46,750
be very efficient and and and kind of

00:27:44,330 --> 00:27:50,150
control everything from start to end so

00:27:46,750 --> 00:27:52,520
while it might be generic it gives us a

00:27:50,150 --> 00:27:54,110
lot of flexibility there really a lot of

00:27:52,520 --> 00:27:57,130
flexibility and we can move very fast

00:27:54,110 --> 00:27:58,520
and we can fix things very fast and

00:27:57,130 --> 00:28:01,520
[Music]

00:27:58,520 --> 00:28:03,020
have to add that you know it was vention

00:28:01,520 --> 00:28:06,890
Durley er today just getting away from

00:28:03,020 --> 00:28:08,630
zookeeper has been like excellent Etsy

00:28:06,890 --> 00:28:11,480
Diaz has been really good for us so far

00:28:08,630 --> 00:28:14,870
and one of the things we're doing with

00:28:11,480 --> 00:28:17,870
me sews is we're evaluating Z at CD

00:28:14,870 --> 00:28:20,420
which I believe is from core OS so it's

00:28:17,870 --> 00:28:22,090
basically like a layer that sits in the

00:28:20,420 --> 00:28:25,340
middle so you point me sasa at it and

00:28:22,090 --> 00:28:27,140
accepts like an incoming zookeeper

00:28:25,340 --> 00:28:29,060
interface so it connects like it thinks

00:28:27,140 --> 00:28:32,270
it's zookeeper but it goes back to at CD

00:28:29,060 --> 00:28:33,710
so the the 500 tasks that I launched

00:28:32,270 --> 00:28:35,360
earlier on to our cluster and showed you

00:28:33,710 --> 00:28:36,890
how they're all running that is that's

00:28:35,360 --> 00:28:39,590
actually a net city back-end and we have

00:28:36,890 --> 00:28:41,390
we have five masters and I think we have

00:28:39,590 --> 00:28:44,950
five instances of our framework and and

00:28:41,390 --> 00:28:48,350
three or five instances of Etsy D so

00:28:44,950 --> 00:28:50,810
it's I think things are changing and

00:28:48,350 --> 00:28:53,360
getting easier and people are gonna have

00:28:50,810 --> 00:28:57,740
more options and it just gives a lot of

00:28:53,360 --> 00:29:09,050
flexibility so we're just gonna open up

00:28:57,740 --> 00:29:10,550
to questions any questions yes hi

00:29:09,050 --> 00:29:13,130
you mentioned the framework that you

00:29:10,550 --> 00:29:15,110
guys wrote is intended for smaller

00:29:13,130 --> 00:29:17,720
clusters what's limiting what is

00:29:15,110 --> 00:29:22,400
limiting it from scaling to clusters of

00:29:17,720 --> 00:29:25,070
larger sizes nothing right now we can

00:29:22,400 --> 00:29:26,840
test on larger clusters but we haven't

00:29:25,070 --> 00:29:29,240
done that the use case internal it was

00:29:26,840 --> 00:29:30,470
for smaller clusters but you can

00:29:29,240 --> 00:29:33,800
certainly try and use it on larger

00:29:30,470 --> 00:29:36,730
clusters one of the performance issues

00:29:33,800 --> 00:29:39,890
we had was scaling past a thousand nodes

00:29:36,730 --> 00:29:42,470
even with marathon and what ends up

00:29:39,890 --> 00:29:44,120
happening is I think zookeeper itself

00:29:42,470 --> 00:29:45,470
struggles to keep up with all the state

00:29:44,120 --> 00:29:47,840
especially if you have a lot of tasks

00:29:45,470 --> 00:29:49,730
and the policy management therefore or I

00:29:47,840 --> 00:29:52,490
should say lifecycle management of tasks

00:29:49,730 --> 00:29:54,950
becomes very difficult because with

00:29:52,490 --> 00:29:56,480
marathon attacks will try forever and

00:29:54,950 --> 00:29:58,820
you know application owners will say I

00:29:56,480 --> 00:30:00,170
wanted to start as fast as possible so

00:29:58,820 --> 00:30:02,450
if you have a task that's just dying

00:30:00,170 --> 00:30:05,300
it'll just flap forever and it never

00:30:02,450 --> 00:30:06,620
dies we're a little bit opinionated

00:30:05,300 --> 00:30:08,930
about that in our framework where we

00:30:06,620 --> 00:30:11,270
will kill the task after so many retries

00:30:08,930 --> 00:30:14,150
and say hey like your stuff is broken

00:30:11,270 --> 00:30:16,190
you should probably go fix it right so

00:30:14,150 --> 00:30:18,080
some of those little tweaks like that

00:30:16,190 --> 00:30:21,110
about policy management

00:30:18,080 --> 00:30:23,930
I guess end-user education on how better

00:30:21,110 --> 00:30:26,360
to use the cluster and not just throw up

00:30:23,930 --> 00:30:32,210
a bunch of broken tasks on their help

00:30:26,360 --> 00:30:35,540
scale a little bit as well as health

00:30:32,210 --> 00:30:37,820
checks I know now the health checks are

00:30:35,540 --> 00:30:41,450
out of experimental right the the ones

00:30:37,820 --> 00:30:42,710
that go from yeah so may so finally has

00:30:41,450 --> 00:30:44,810
health checks that go back to the

00:30:42,710 --> 00:30:48,260
executor instead of the scheduler that

00:30:44,810 --> 00:30:50,510
helps scale as well and some other

00:30:48,260 --> 00:30:53,930
issues as well I know the design of meso

00:30:50,510 --> 00:30:55,760
CNS were starting to design our own DNS

00:30:53,930 --> 00:30:57,800
because meso s-- DNS right now actually

00:30:55,760 --> 00:30:59,750
hits the master and just gets a huge

00:30:57,800 --> 00:31:02,030
list of stay from the master and does it

00:30:59,750 --> 00:31:04,310
very frequently so we're basically

00:31:02,030 --> 00:31:06,830
trying to leave the master with just as

00:31:04,310 --> 00:31:08,660
core responsibilities in our framework

00:31:06,830 --> 00:31:10,310
and we're hoping that will allow us to

00:31:08,660 --> 00:31:12,790
scale a lot more if we distribute the

00:31:10,310 --> 00:31:16,140
workload of cluster management across

00:31:12,790 --> 00:31:19,830
the executor and our own

00:31:16,140 --> 00:31:22,530
our own framework so to answer your

00:31:19,830 --> 00:31:23,820
question rap full full circle I don't

00:31:22,530 --> 00:31:25,950
think there's anything necessarily

00:31:23,820 --> 00:31:29,610
limiting it we just haven't tested on

00:31:25,950 --> 00:31:31,830
anything really larger than I think like

00:31:29,610 --> 00:31:33,720
100 nodes or somewhere around there but

00:31:31,830 --> 00:31:37,410
we're actually going to take over a lab

00:31:33,720 --> 00:31:39,330
and I think it's about 600 nodes so

00:31:37,410 --> 00:31:40,470
we'll see how it runs there but if you

00:31:39,330 --> 00:31:41,370
want to run it on a thousand nodes

00:31:40,470 --> 00:31:44,370
that'd be awesome

00:31:41,370 --> 00:31:48,240
talk to me look I'd love to know how

00:31:44,370 --> 00:31:49,620
that goes just to quickly add we one of

00:31:48,240 --> 00:31:52,260
the things we focus on around that was

00:31:49,620 --> 00:31:54,510
was the how restoring state and SCD and

00:31:52,260 --> 00:31:57,299
how we're retrieving it and how much we

00:31:54,510 --> 00:32:00,390
store because we have hit a lot of

00:31:57,299 --> 00:32:03,660
issues with I mean I don't know if it's

00:32:00,390 --> 00:32:06,059
so much zookeeper Mathon it's just how

00:32:03,660 --> 00:32:07,590
much state marathon stores and the

00:32:06,059 --> 00:32:09,360
limits of zookeeper and the Z knows and

00:32:07,590 --> 00:32:11,190
the older versions of marathons stored

00:32:09,360 --> 00:32:14,130
you know different amounts and maybe the

00:32:11,190 --> 00:32:17,690
newer versions and that's um we took a

00:32:14,130 --> 00:32:17,690
lot of time and iterated a lot on that

00:32:20,150 --> 00:32:24,919
sure there's a question of two questions

00:32:30,290 --> 00:32:37,320
what was the reason so you clearly your

00:32:32,940 --> 00:32:39,030
own this SDK are you using DC OS SDK no

00:32:37,320 --> 00:32:41,669
this is our own SDK we didn't want

00:32:39,030 --> 00:32:43,380
anybody I'm sorry any particular use it

00:32:41,669 --> 00:32:46,559
a reason for doing that compared to

00:32:43,380 --> 00:32:50,669
using SDK from these us yes

00:32:46,559 --> 00:32:53,820
so we didn't want to use yeah yeah we

00:32:50,669 --> 00:32:56,880
didn't feel like using their SDK because

00:32:53,820 --> 00:32:58,380
there was it limits us right we wanted a

00:32:56,880 --> 00:33:01,980
lot of customization we're really after

00:32:58,380 --> 00:33:03,870
performance in our clusters while having

00:33:01,980 --> 00:33:06,150
a ya know format is cool to make your

00:33:03,870 --> 00:33:11,970
your framework that's really good I

00:33:06,150 --> 00:33:14,040
think for basic tasks or and or very a

00:33:11,970 --> 00:33:16,530
cookie cutter type of things but if you

00:33:14,040 --> 00:33:17,970
want to get really extensible really

00:33:16,530 --> 00:33:22,590
squeeze performance out of your cluster

00:33:17,970 --> 00:33:27,399
and get rid of a JVM which makes my boss

00:33:22,590 --> 00:33:29,979
happy then that's why we do it right

00:33:27,399 --> 00:33:32,229
it has its place don't get me wrong if

00:33:29,979 --> 00:33:34,509
you want to do that absolutely you can

00:33:32,229 --> 00:33:38,649
use it but we just it wasn't for our use

00:33:34,509 --> 00:33:40,749
case you know we kind of have like two

00:33:38,649 --> 00:33:42,219
almost two levels in SDK at the very

00:33:40,749 --> 00:33:45,580
basic level we have like the protobuf

00:33:42,219 --> 00:33:47,019
bindings which I think me sews just now

00:33:45,580 --> 00:33:50,349
brought in to their official like Apache

00:33:47,019 --> 00:33:52,389
repos but we have an opinion more

00:33:50,349 --> 00:33:54,039
opinionated later on top which kind of

00:33:52,389 --> 00:33:56,499
provides like you know a default task

00:33:54,039 --> 00:33:59,559
manager and default resource manager and

00:33:56,499 --> 00:34:01,179
things that every scheduled scheduler

00:33:59,559 --> 00:34:03,399
will have to handle on their own no

00:34:01,179 --> 00:34:05,589
matter what so we tried to like just

00:34:03,399 --> 00:34:08,230
make that a write-once type thing

00:34:05,589 --> 00:34:10,510
we're also go so a lot of the existing

00:34:08,230 --> 00:34:13,240
stuff and from you so sphere and indie

00:34:10,510 --> 00:34:20,730
cos is is Java just it's fine it's just

00:34:13,240 --> 00:34:20,730
we went with go any other questions

00:34:23,760 --> 00:34:31,280
thanks guys thank you

00:34:26,170 --> 00:34:31,280

YouTube URL: https://www.youtube.com/watch?v=wl6qaronlvk


