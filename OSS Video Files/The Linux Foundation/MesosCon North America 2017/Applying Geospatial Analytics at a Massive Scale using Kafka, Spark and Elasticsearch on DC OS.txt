Title: Applying Geospatial Analytics at a Massive Scale using Kafka, Spark and Elasticsearch on DC OS
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Applying Geospatial Analytics at a Massive Scale using Kafka, Spark and Elasticsearch on DC/OS - Adam Mollenkopf, Esri

This session will explore how DC/OS and Mesos are being used at Esri to establish a foundational operating environment to enable the consumption of high velocity IoT data using Apache Kafka, streaming analytics using Apache Spark, high-volume storage and querying of spatiotemporal data using Elasticsearch, and recurring batch analytics using Apache Spark & Metronome. Additionally, Esri will share their experience in making their application for DC/OS portable so that it can easily be deployed amongst public cloud providers (Microsoft Azure, Amazon EC2), private cloud providers and on-premise environments. Demonstrations will be performed throughout the presentation to cement these concepts for the attendees.

About

Adam Mollenkopf
Esri
Real-Time & Big Data GIS Capability Lead
Redlands, CA
Twitter Tweet  Websiteesri.com
Adam Mollenkopf is responsible for the strategic direction Esri takes towards enabling real-time and big data capabilities in the ArcGIS platform. This includes having the ability to ingest real-time data streams from a wide variety of sources, performing continuous and recurring spatiotemporal analytics on data as it is received & disseminating analytic results to communities of interest. He leads a team of experienced individuals in the area of stream processing and big data analytics.
Captions: 
	00:00:00,000 --> 00:00:06,720
all right so we're gonna get started

00:00:02,310 --> 00:00:08,639
we've got Adam one curve Ezri he's gonna

00:00:06,720 --> 00:00:10,559
talk about applying geospatial analytics

00:00:08,639 --> 00:00:14,660
at a massive scale using Kafka spark

00:00:10,559 --> 00:00:14,660
elastic search on DCOs

00:00:14,690 --> 00:00:19,320
all right I think Thank You Robby can

00:00:17,220 --> 00:00:21,150
you guys hear me okay we're all good all

00:00:19,320 --> 00:00:23,460
right so I'm out of Mahlon cough I work

00:00:21,150 --> 00:00:25,320
at our company called ESRI we're based

00:00:23,460 --> 00:00:26,789
out of Redlands California which is just

00:00:25,320 --> 00:00:28,140
like an hour and a half east of here or

00:00:26,789 --> 00:00:31,309
if you traveled yesterday morning three

00:00:28,140 --> 00:00:33,840
hours to get here in the traffic so

00:00:31,309 --> 00:00:36,239
we're here local I'm going to be talking

00:00:33,840 --> 00:00:38,489
about applying geospatial analytics at

00:00:36,239 --> 00:00:40,410
massive scale talking about some of the

00:00:38,489 --> 00:00:42,780
things we've done at ESRI and I've got a

00:00:40,410 --> 00:00:43,950
really cool companion github site along

00:00:42,780 --> 00:00:45,510
with some of the things that you'll see

00:00:43,950 --> 00:00:46,680
here where you can go to learn a lot

00:00:45,510 --> 00:00:48,090
more than what we'll be able to cover

00:00:46,680 --> 00:00:50,550
just in this 40 minutes that we have

00:00:48,090 --> 00:00:52,199
today so I'm responsible for the real

00:00:50,550 --> 00:00:54,300
time and the big data capabilities of

00:00:52,199 --> 00:00:56,309
ESRI so this basically means the IOT

00:00:54,300 --> 00:00:59,940
strategy we have as a company bringing

00:00:56,309 --> 00:01:01,440
in sensor data and other things ESRI as

00:00:59,940 --> 00:01:04,379
a company in case you're not familiar

00:01:01,440 --> 00:01:06,630
with us we've been around since 69 we're

00:01:04,379 --> 00:01:09,270
one of the largest companies that do

00:01:06,630 --> 00:01:11,580
Geographic software or spatial software

00:01:09,270 --> 00:01:14,729
if you want to call it that we pretty

00:01:11,580 --> 00:01:18,330
much invented geospatial information

00:01:14,729 --> 00:01:20,100
systems back in the late 60s a lot of

00:01:18,330 --> 00:01:22,290
the original folks that did that are

00:01:20,100 --> 00:01:24,630
still with a company we license our

00:01:22,290 --> 00:01:27,840
software to over 350,000 user

00:01:24,630 --> 00:01:29,340
organizations around the world so we

00:01:27,840 --> 00:01:30,840
make geospatial software this isn't

00:01:29,340 --> 00:01:31,650
really a session about ESRI I just want

00:01:30,840 --> 00:01:34,350
to give you a little bit of background

00:01:31,650 --> 00:01:36,210
in case you didn't know who we are as

00:01:34,350 --> 00:01:37,530
far as the agenda for this discussion

00:01:36,210 --> 00:01:39,060
today I'm going to talk about the

00:01:37,530 --> 00:01:40,950
emergence of the new class a problem

00:01:39,060 --> 00:01:42,960
that's driving us to have to take a new

00:01:40,950 --> 00:01:45,420
architectural approach from what we've

00:01:42,960 --> 00:01:47,100
done in the past with geospatial we'll

00:01:45,420 --> 00:01:49,439
talk about our approach towards massive

00:01:47,100 --> 00:01:51,030
scale which is obviously using DCOs and

00:01:49,439 --> 00:01:54,030
a number of the packages in the

00:01:51,030 --> 00:01:55,710
ecosystem of mesosphere we'll talk about

00:01:54,030 --> 00:01:57,060
geospatial Analytics I'll give you a few

00:01:55,710 --> 00:01:59,549
samples of some of the types of

00:01:57,060 --> 00:02:01,259
analytics that we perform on this for

00:01:59,549 --> 00:02:03,420
our customers and then we'll talk about

00:02:01,259 --> 00:02:04,740
an important topic which is writing our

00:02:03,420 --> 00:02:06,360
applications in a way that they can be

00:02:04,740 --> 00:02:08,069
portable across multiple environments

00:02:06,360 --> 00:02:11,009
that's public cloud private cloud

00:02:08,069 --> 00:02:12,830
on-premise and how we can use DCOs as

00:02:11,009 --> 00:02:18,050
the way to deliver those and

00:02:12,830 --> 00:02:20,000
consistent way so this is a slide that

00:02:18,050 --> 00:02:21,860
kind of captures the essence of what

00:02:20,000 --> 00:02:24,230
we're kind of seeing in this piece over

00:02:21,860 --> 00:02:25,850
the last few years and so we have

00:02:24,230 --> 00:02:28,550
traditional customers that have been

00:02:25,850 --> 00:02:30,500
doing what we call real-time GIS where

00:02:28,550 --> 00:02:32,090
they might track their snow plows their

00:02:30,500 --> 00:02:34,880
police vehicles their fire equipment

00:02:32,090 --> 00:02:36,500
other things that are out there but they

00:02:34,880 --> 00:02:38,960
typically a few years ago we're only

00:02:36,500 --> 00:02:40,490
tracking you know one or two assets and

00:02:38,960 --> 00:02:42,620
what we're seeing now is we have cities

00:02:40,490 --> 00:02:44,300
like to buy and Singapore and and other

00:02:42,620 --> 00:02:46,220
places around the world that are pretty

00:02:44,300 --> 00:02:47,990
much tracking everything and so these

00:02:46,220 --> 00:02:50,810
are a sampling of some of the things

00:02:47,990 --> 00:02:53,840
that our customers track on a continuous

00:02:50,810 --> 00:02:55,370
basis each of these signals comes from

00:02:53,840 --> 00:02:57,740
different devices that are out in the

00:02:55,370 --> 00:02:59,810
field and they report lots of different

00:02:57,740 --> 00:03:02,090
information be it from an environmental

00:02:59,810 --> 00:03:04,820
sensor reporting air quality or water

00:03:02,090 --> 00:03:07,130
quality be it weather events and trying

00:03:04,820 --> 00:03:09,170
to correlate that to things that are

00:03:07,130 --> 00:03:11,480
happening inside of a company and

00:03:09,170 --> 00:03:13,340
basically they want to be able to bring

00:03:11,480 --> 00:03:15,320
all of this information in in near

00:03:13,340 --> 00:03:17,450
real-time as quickly as possible

00:03:15,320 --> 00:03:20,209
visualize this and have a situational

00:03:17,450 --> 00:03:21,320
awareness display of what's going on and

00:03:20,209 --> 00:03:24,110
then they can use that as a common

00:03:21,320 --> 00:03:26,269
operational picture to affect different

00:03:24,110 --> 00:03:27,650
things so more recently in the last

00:03:26,269 --> 00:03:29,890
couple weeks some of the hurricanes

00:03:27,650 --> 00:03:32,060
we've had people fielded down on site

00:03:29,890 --> 00:03:33,650
tracking the water levels tracking all

00:03:32,060 --> 00:03:36,140
kinds of other things helping FEMA and

00:03:33,650 --> 00:03:39,530
other organizations to respond to those

00:03:36,140 --> 00:03:42,110
situations so with this as bringing out

00:03:39,530 --> 00:03:44,060
new velocity of data it's not just one

00:03:42,110 --> 00:03:46,610
feed of vehicles coming in it's all

00:03:44,060 --> 00:03:48,200
these different feeds providing that and

00:03:46,610 --> 00:03:51,650
what we've traditionally done for

00:03:48,200 --> 00:03:54,380
customers is deliver a multi machine

00:03:51,650 --> 00:03:56,300
system for these customers and this is

00:03:54,380 --> 00:03:58,610
kind of our old approach before DC LS

00:03:56,300 --> 00:04:00,830
and so we would recommend an environment

00:03:58,610 --> 00:04:04,550
where people bring in and ingest this

00:04:00,830 --> 00:04:06,290
real-time data using our software store

00:04:04,550 --> 00:04:08,090
this data in a spatial temporal way

00:04:06,290 --> 00:04:10,160
spatial temporal being recording that

00:04:08,090 --> 00:04:11,690
data based on space and time and having

00:04:10,160 --> 00:04:13,670
it optimize to be able to access that

00:04:11,690 --> 00:04:15,890
data that way and then being able to run

00:04:13,670 --> 00:04:17,479
ad hoc and scheduled analytics on that

00:04:15,890 --> 00:04:19,280
data or those observations that are

00:04:17,479 --> 00:04:21,229
stored and then finally being able to

00:04:19,280 --> 00:04:24,289
visualize that information either is a

00:04:21,229 --> 00:04:26,660
live display or a kind of DVR you know

00:04:24,289 --> 00:04:28,520
go rewind what happened an hour ago

00:04:26,660 --> 00:04:31,640
two years ago will happen a year ago and

00:04:28,520 --> 00:04:33,740
so this is a traditional deployment that

00:04:31,640 --> 00:04:36,290
we've had and what we're seeing now is

00:04:33,740 --> 00:04:37,520
with this emergent emergence of IOT is

00:04:36,290 --> 00:04:39,950
that we're seeing a new class of

00:04:37,520 --> 00:04:42,830
customers that go well beyond that so

00:04:39,950 --> 00:04:44,840
they can't just use a few machines and

00:04:42,830 --> 00:04:46,370
process thousands of events per second

00:04:44,840 --> 00:04:47,750
we have customers that need to do

00:04:46,370 --> 00:04:49,400
millions of events per second

00:04:47,750 --> 00:04:51,140
potentially depending on all the

00:04:49,400 --> 00:04:53,600
different feeds that they have and just

00:04:51,140 --> 00:04:55,460
taking our traditional architecture and

00:04:53,600 --> 00:04:57,140
trying to deploy that across tens or

00:04:55,460 --> 00:04:59,060
hundreds or thousands of machines is not

00:04:57,140 --> 00:05:02,120
really a reasonable or tenable thing to

00:04:59,060 --> 00:05:04,190
do and so this requires a new approach

00:05:02,120 --> 00:05:05,780
for this and our massive scale approach

00:05:04,190 --> 00:05:08,570
is DCOs

00:05:05,780 --> 00:05:10,520
so if you took the concept of a data

00:05:08,570 --> 00:05:12,830
center or a rack of machines and a data

00:05:10,520 --> 00:05:15,260
center and treated that as one logical

00:05:12,830 --> 00:05:17,180
unit we treat this as one operating

00:05:15,260 --> 00:05:19,520
system thus the data center operating

00:05:17,180 --> 00:05:22,010
system and what we look at is instead of

00:05:19,520 --> 00:05:23,120
scheduling our software to run on eleven

00:05:22,010 --> 00:05:25,070
different machines and we're very

00:05:23,120 --> 00:05:26,510
specific about this machine is for

00:05:25,070 --> 00:05:28,820
storage and this machine is for

00:05:26,510 --> 00:05:30,620
ingestion we just treat it as one

00:05:28,820 --> 00:05:33,440
operating system where we schedule work

00:05:30,620 --> 00:05:35,570
to run and so we don't look at it as 33

00:05:33,440 --> 00:05:37,250
machines we look at it as a whole bunch

00:05:35,570 --> 00:05:39,740
of resources that are available that we

00:05:37,250 --> 00:05:41,090
can schedule work to run on and so we

00:05:39,740 --> 00:05:43,400
have a lot of RAM we have a lot of

00:05:41,090 --> 00:05:45,500
storage we have a lot of CPU resources

00:05:43,400 --> 00:05:47,120
that we can make use of and then we

00:05:45,500 --> 00:05:49,220
schedule work to run on that so this is

00:05:47,120 --> 00:05:51,290
basically what DC OS is all about and

00:05:49,220 --> 00:05:53,090
this has given us a different starting

00:05:51,290 --> 00:05:56,390
point for us to deploy these

00:05:53,090 --> 00:05:57,950
applications for customers and treating

00:05:56,390 --> 00:06:00,440
the entire data center not just the 30

00:05:57,950 --> 00:06:03,470
nodes as one operating system is it's

00:06:00,440 --> 00:06:04,940
quite useful this is not new this is

00:06:03,470 --> 00:06:06,680
something that's been proven and out in

00:06:04,940 --> 00:06:07,910
the industry for some time these are a

00:06:06,680 --> 00:06:09,560
listing of some of the companies that

00:06:07,910 --> 00:06:11,930
have been using the underlying

00:06:09,560 --> 00:06:13,460
technologies and then DCOs as well as

00:06:11,930 --> 00:06:15,380
it's emerged over the last year and a

00:06:13,460 --> 00:06:16,730
half or so so we're standing on the

00:06:15,380 --> 00:06:18,650
shoulders of giants that have proven

00:06:16,730 --> 00:06:20,240
this architecture and we're basically

00:06:18,650 --> 00:06:23,390
applying our software to it and we're

00:06:20,240 --> 00:06:25,700
adding ESRI to to this logo as well so

00:06:23,390 --> 00:06:27,440
we have a project within ESRI that we

00:06:25,700 --> 00:06:29,690
call Trinity it's Trinity like the

00:06:27,440 --> 00:06:31,160
matrix character I have the the fortune

00:06:29,690 --> 00:06:33,350
of traveling all around the world as

00:06:31,160 --> 00:06:35,060
part of my job and when I talked about

00:06:33,350 --> 00:06:36,830
Trinity and Dubai they were like cool

00:06:35,060 --> 00:06:38,960
that Christianity no no

00:06:36,830 --> 00:06:40,280
not Christianity and then I went to

00:06:38,960 --> 00:06:41,389
Germany and they were like oh like the

00:06:40,280 --> 00:06:44,120
atomic bomb project

00:06:41,389 --> 00:06:46,400
oh no no no so Trinity has turned out to

00:06:44,120 --> 00:06:47,990
be the worst name for a project ever but

00:06:46,400 --> 00:06:50,150
everybody knows it by that name

00:06:47,990 --> 00:06:52,069
so I always qualify it with the matrix

00:06:50,150 --> 00:06:54,440
character now so that's why we have

00:06:52,069 --> 00:06:56,659
trainee from from the matrix here we

00:06:54,440 --> 00:06:59,210
entered a partnership with miso spear

00:06:56,659 --> 00:07:02,330
about a year and a half two years ago to

00:06:59,210 --> 00:07:04,340
work on re-architecting ESRI software in

00:07:02,330 --> 00:07:06,379
a way that we can span this across a

00:07:04,340 --> 00:07:08,539
massive scale and so that's what our

00:07:06,379 --> 00:07:10,250
project Trinity is about so our before

00:07:08,539 --> 00:07:12,289
picture looks something like this where

00:07:10,250 --> 00:07:14,419
we had this triangle or the Trinity if

00:07:12,289 --> 00:07:16,789
you want to call it that of ingesting

00:07:14,419 --> 00:07:19,069
real-time data from sensors storing that

00:07:16,789 --> 00:07:20,840
data and then being able to run ad hoc

00:07:19,069 --> 00:07:22,819
analysis or scheduled analysis on that

00:07:20,840 --> 00:07:25,569
after the fact and then being able to

00:07:22,819 --> 00:07:28,280
visualize that information over time our

00:07:25,569 --> 00:07:29,990
before picture and our scorecard for

00:07:28,280 --> 00:07:32,509
what we could do with our traditional

00:07:29,990 --> 00:07:34,009
software stack was in this range here

00:07:32,509 --> 00:07:37,099
where we can ingest thousands of events

00:07:34,009 --> 00:07:39,650
per second which Matt probably 95% of

00:07:37,099 --> 00:07:41,270
our users use cases but we have that new

00:07:39,650 --> 00:07:42,800
class of customer that's in the hundreds

00:07:41,270 --> 00:07:44,840
of thousands or millions of events per

00:07:42,800 --> 00:07:47,060
second do we need to handle and then we

00:07:44,840 --> 00:07:48,800
also from a streaming analysis and from

00:07:47,060 --> 00:07:50,719
a batch analysis perspective need to be

00:07:48,800 --> 00:07:53,330
able to handle lots more velocity lots

00:07:50,719 --> 00:07:55,580
more volume and store many more than

00:07:53,330 --> 00:07:57,710
millions of events per second so our

00:07:55,580 --> 00:07:59,210
after picture is we needed to move to

00:07:57,710 --> 00:08:00,740
something that looked like this where

00:07:59,210 --> 00:08:02,960
we're satisfying that new class of

00:08:00,740 --> 00:08:05,090
customer and for us to do that we had to

00:08:02,960 --> 00:08:07,460
react attack to our approach it wasn't

00:08:05,090 --> 00:08:08,779
just take our existing software put a

00:08:07,460 --> 00:08:10,340
docker container around it and deploy a

00:08:08,779 --> 00:08:12,919
whole bunch of them but that wouldn't

00:08:10,340 --> 00:08:15,259
really work too well so we had to react

00:08:12,919 --> 00:08:17,509
our software and what we did is we went

00:08:15,259 --> 00:08:19,879
through a containerization process and

00:08:17,509 --> 00:08:21,979
so instead of having a monolithic

00:08:19,879 --> 00:08:24,949
application we broke it into small micro

00:08:21,979 --> 00:08:27,110
services and so what was our ingestion

00:08:24,949 --> 00:08:28,639
path has been broken into things that

00:08:27,110 --> 00:08:31,520
sit there at the edge and collect data

00:08:28,639 --> 00:08:33,199
we have Kafka that sits in between we

00:08:31,520 --> 00:08:35,839
have SPARC streaming jobs which are the

00:08:33,199 --> 00:08:37,760
purple items that write to storage

00:08:35,839 --> 00:08:39,800
systems which is elasticsearch and then

00:08:37,760 --> 00:08:41,839
we have visualization techniques to do

00:08:39,800 --> 00:08:44,360
that what this allows us to do is to

00:08:41,839 --> 00:08:46,279
scale up any aspect of the system based

00:08:44,360 --> 00:08:48,410
on a customer's need so if they're a

00:08:46,279 --> 00:08:49,710
connected car customer like you've seen

00:08:48,410 --> 00:08:51,600
on stage

00:08:49,710 --> 00:08:53,100
they have millions of events per second

00:08:51,600 --> 00:08:55,860
coming in so they're gonna need lots of

00:08:53,100 --> 00:08:57,690
black sources to sit at the edge and

00:08:55,860 --> 00:08:59,580
collect these things and I have lots of

00:08:57,690 --> 00:09:02,100
redundancy lots of availability in those

00:08:59,580 --> 00:09:04,100
services and then lots of Kafka brokers

00:09:02,100 --> 00:09:06,240
and lots of other things running as well

00:09:04,100 --> 00:09:08,160
so we take our before picture

00:09:06,240 --> 00:09:10,860
re-architect that into these containers

00:09:08,160 --> 00:09:12,900
and we have our project Trinity which

00:09:10,860 --> 00:09:16,230
has broken that out into two nice micro

00:09:12,900 --> 00:09:18,990
services so these are a listing of some

00:09:16,230 --> 00:09:20,670
of the container types that we have and

00:09:18,990 --> 00:09:23,070
the underlying technology underneath all

00:09:20,670 --> 00:09:26,160
those as I mentioned before our sources

00:09:23,070 --> 00:09:28,620
sit at the edge and they are responsible

00:09:26,160 --> 00:09:31,230
for connecting to some device out in the

00:09:28,620 --> 00:09:33,090
field so we have probably 50 or 60

00:09:31,230 --> 00:09:35,910
different sources you know everything

00:09:33,090 --> 00:09:38,370
from listen to a live weather feed to

00:09:35,910 --> 00:09:42,780
listen to connected cars or streaming in

00:09:38,370 --> 00:09:45,060
to listening to a fleet of you know

00:09:42,780 --> 00:09:46,860
FedEx or UPS vehicles or things to

00:09:45,060 --> 00:09:49,590
connect to be able to bring in all those

00:09:46,860 --> 00:09:51,630
different types of feeds so those are

00:09:49,590 --> 00:09:54,330
connectors or adapters for different

00:09:51,630 --> 00:09:55,980
device types those we write in Scala and

00:09:54,330 --> 00:09:58,950
we deliver those so this is kind of the

00:09:55,980 --> 00:10:01,140
the reactive application style approach

00:09:58,950 --> 00:10:03,990
we typically write those events to a

00:10:01,140 --> 00:10:07,320
gateway that is underlying Kafka brokers

00:10:03,990 --> 00:10:09,780
and those topics are available for spark

00:10:07,320 --> 00:10:11,340
streaming jobs to consume and then we

00:10:09,780 --> 00:10:12,630
have our geospatial analysis

00:10:11,340 --> 00:10:15,510
capabilities within those real-time

00:10:12,630 --> 00:10:17,490
analytics and this geospatial analysis

00:10:15,510 --> 00:10:19,200
is stuff that we've added to spark so we

00:10:17,490 --> 00:10:21,090
spent a lot of time over the last two

00:10:19,200 --> 00:10:23,700
and a half years to add user-defined

00:10:21,090 --> 00:10:25,770
types for geometry to add user-defined

00:10:23,700 --> 00:10:27,120
functions for different operations for

00:10:25,770 --> 00:10:29,730
free geospatial which we'll go through

00:10:27,120 --> 00:10:31,740
in a second and then our batch analysis

00:10:29,730 --> 00:10:33,360
is the same story with user-defined

00:10:31,740 --> 00:10:35,820
types and user-defined functions and

00:10:33,360 --> 00:10:37,770
spark we write that data to elastic

00:10:35,820 --> 00:10:40,290
search elastic is really good at storing

00:10:37,770 --> 00:10:41,910
geospatial data but we've extended it

00:10:40,290 --> 00:10:44,520
even further to do additional

00:10:41,910 --> 00:10:47,040
capabilities and then we expose all of

00:10:44,520 --> 00:10:49,470
our data in our spatial temporal store

00:10:47,040 --> 00:10:51,480
with a Play app that's a reactive base

00:10:49,470 --> 00:10:53,700
application a web app that people can

00:10:51,480 --> 00:10:55,350
access and generate map images and

00:10:53,700 --> 00:10:58,590
visualize things which we'll see a cool

00:10:55,350 --> 00:11:00,690
demo of in a minute our geospatial

00:10:58,590 --> 00:11:02,910
Analytics just to give you a quick

00:11:00,690 --> 00:11:03,420
sampling we're going to deep dive into

00:11:02,910 --> 00:11:06,420
these

00:11:03,420 --> 00:11:08,100
- container types is those spark things

00:11:06,420 --> 00:11:10,470
that we're talking about so it's it

00:11:08,100 --> 00:11:12,900
sparked with ESRI extensions that we've

00:11:10,470 --> 00:11:15,180
written these are a sampling of some of

00:11:12,900 --> 00:11:17,550
the different analytics that we have so

00:11:15,180 --> 00:11:19,320
I'm not going to cover all these but we

00:11:17,550 --> 00:11:21,570
have you know capabilities such as

00:11:19,320 --> 00:11:24,060
aggregate points where I might want to

00:11:21,570 --> 00:11:26,250
take observational data from sensors and

00:11:24,060 --> 00:11:28,260
they may be moving sensors and I want to

00:11:26,250 --> 00:11:29,910
record you know I don't want to put a

00:11:28,260 --> 00:11:31,650
million dots on the map I want to be

00:11:29,910 --> 00:11:33,150
able to see the density of those things

00:11:31,650 --> 00:11:35,220
and I want to see that in near real-time

00:11:33,150 --> 00:11:37,800
as it's moving around so the aggregate

00:11:35,220 --> 00:11:40,470
points would allow us to do that we can

00:11:37,800 --> 00:11:42,060
also do things like join features and so

00:11:40,470 --> 00:11:44,400
when we join features it's kind of like

00:11:42,060 --> 00:11:47,000
a relational database join but it's

00:11:44,400 --> 00:11:49,470
joining with space and time as well so

00:11:47,000 --> 00:11:51,690
which of these dots are in these

00:11:49,470 --> 00:11:53,850
polygons and which of these dots are in

00:11:51,690 --> 00:11:55,770
these polygons within this time window

00:11:53,850 --> 00:11:58,500
so being able to do spatial temporal

00:11:55,770 --> 00:11:59,550
joins there's a lot of other analytic

00:11:58,500 --> 00:12:01,590
capabilities that we're not going to go

00:11:59,550 --> 00:12:04,440
through in this session but this is just

00:12:01,590 --> 00:12:06,750
a sampling of a few of those so a couple

00:12:04,440 --> 00:12:09,300
of real-world concrete examples of these

00:12:06,750 --> 00:12:11,250
analytics are like which cron events

00:12:09,300 --> 00:12:13,110
occurred near sporting events spatially

00:12:11,250 --> 00:12:15,030
and temporally so that might be a

00:12:13,110 --> 00:12:16,680
question that somebody wants to ask to

00:12:15,030 --> 00:12:18,450
do that or what bodies of water

00:12:16,680 --> 00:12:20,400
intersects cities with populations

00:12:18,450 --> 00:12:22,110
greater than a million people and that

00:12:20,400 --> 00:12:24,540
million people could actually be not

00:12:22,110 --> 00:12:26,550
demographic data but real-time cellular

00:12:24,540 --> 00:12:28,830
phone data that's coming in so I have a

00:12:26,550 --> 00:12:30,540
million people here right now because I

00:12:28,830 --> 00:12:32,940
have anonymized cell phone data feeds

00:12:30,540 --> 00:12:34,830
that are coming in so that can work with

00:12:32,940 --> 00:12:36,780
real-time data or static data or data

00:12:34,830 --> 00:12:39,210
that's changing on on a continuous basis

00:12:36,780 --> 00:12:41,160
or another example is what traffic jams

00:12:39,210 --> 00:12:43,200
occurred because of car accidents so how

00:12:41,160 --> 00:12:44,550
can I correlate these things what was

00:12:43,200 --> 00:12:46,470
the weather like when those car

00:12:44,550 --> 00:12:49,170
accidents happen so how do I correlate

00:12:46,470 --> 00:12:51,000
all these things together to do that so

00:12:49,170 --> 00:12:53,940
this is if we go into that tool a little

00:12:51,000 --> 00:12:56,820
bit more what a user specifies is what

00:12:53,940 --> 00:12:59,400
type of input they have so it might be a

00:12:56,820 --> 00:13:02,490
cell phone data and that cell phone data

00:12:59,400 --> 00:13:04,380
I want to I want to join in to maybe zip

00:13:02,490 --> 00:13:06,110
codes or province boundaries or things

00:13:04,380 --> 00:13:08,130
of that nature or maybe operational

00:13:06,110 --> 00:13:10,710
boundaries for my company or my

00:13:08,130 --> 00:13:13,560
organization the output that I want to

00:13:10,710 --> 00:13:16,230
get is a pullet polygon that represents

00:13:13,560 --> 00:13:18,960
here's the count of those things

00:13:16,230 --> 00:13:21,720
and so we can do this by any combination

00:13:18,960 --> 00:13:23,430
of space-time or attribute and these are

00:13:21,720 --> 00:13:25,110
part of the user-defined functions that

00:13:23,430 --> 00:13:28,290
we've added to spark so that's why it

00:13:25,110 --> 00:13:31,170
says powered by by Hazari and so we have

00:13:28,290 --> 00:13:33,300
temporal operators so there's 13 or so

00:13:31,170 --> 00:13:35,160
of these we took these from the David

00:13:33,300 --> 00:13:37,290
welcome book if you guys are into those

00:13:35,160 --> 00:13:39,000
things but these are the temporal

00:13:37,290 --> 00:13:40,830
operators that we've added and we also

00:13:39,000 --> 00:13:42,540
have a bunch of spatial operators and

00:13:40,830 --> 00:13:43,980
you can combine these in any form or

00:13:42,540 --> 00:13:46,890
fashion that you like to do pretty

00:13:43,980 --> 00:13:48,420
sophisticated analysis to expand on the

00:13:46,890 --> 00:13:50,280
temporal operators a bit more this is

00:13:48,420 --> 00:13:53,280
kind of a visual depiction of it so I

00:13:50,280 --> 00:13:55,890
can ask did this weather event the Curt

00:13:53,280 --> 00:14:00,300
did it begin a car accident or did it

00:13:55,890 --> 00:14:01,860
intersect or did it coincide with a car

00:14:00,300 --> 00:14:03,330
accident that happen and so you can

00:14:01,860 --> 00:14:07,110
start to really do sophisticated

00:14:03,330 --> 00:14:09,600
analysis with these things another tool

00:14:07,110 --> 00:14:11,580
that we have is aggregate points and so

00:14:09,600 --> 00:14:13,260
this is what a lot of people like to

00:14:11,580 --> 00:14:15,930
start with just to understand their data

00:14:13,260 --> 00:14:17,970
so I have all this you know billions of

00:14:15,930 --> 00:14:19,500
observations of sensors that are out

00:14:17,970 --> 00:14:22,320
there maybe a cell phone data maybe it's

00:14:19,500 --> 00:14:22,830
car movement and so what does my data

00:14:22,320 --> 00:14:25,440
look like

00:14:22,830 --> 00:14:28,140
I just want to plot it against zip codes

00:14:25,440 --> 00:14:29,760
or other boundaries that I have and so

00:14:28,140 --> 00:14:32,040
or I could do things like where are the

00:14:29,760 --> 00:14:34,050
most power outages occurring because of

00:14:32,040 --> 00:14:36,030
the hurricane so if I have a feed of

00:14:34,050 --> 00:14:38,310
that information I can look at that or

00:14:36,030 --> 00:14:40,080
what zip codes have the highest count of

00:14:38,310 --> 00:14:42,870
crimes and incidents and again that

00:14:40,080 --> 00:14:44,850
could be a real-time feed coming in so

00:14:42,870 --> 00:14:47,910
those are some of the examples to

00:14:44,850 --> 00:14:49,470
aggregate points we pretty much take

00:14:47,910 --> 00:14:51,270
point data or observational data

00:14:49,470 --> 00:14:54,060
I apologize for why that's flickering I

00:14:51,270 --> 00:14:56,550
have no idea but the aggregate points

00:14:54,060 --> 00:14:59,220
takes observation data and it joins it

00:14:56,550 --> 00:15:01,260
into polygons or other features that you

00:14:59,220 --> 00:15:04,260
have and then it results with a polygon

00:15:01,260 --> 00:15:06,540
that's densified so you can see the

00:15:04,260 --> 00:15:08,490
counts of those things being you know

00:15:06,540 --> 00:15:11,850
red being more incidents or blue being

00:15:08,490 --> 00:15:14,310
less incidence of that this can be done

00:15:11,850 --> 00:15:16,410
on a two-dimensional basis which is kind

00:15:14,310 --> 00:15:17,850
of described here if you don't have

00:15:16,410 --> 00:15:20,040
polygons that you want to join with you

00:15:17,850 --> 00:15:22,890
can join this into just bins so I could

00:15:20,040 --> 00:15:26,040
say aggregate this cellphone movement in

00:15:22,890 --> 00:15:28,110
this mall to a 10 meter resolution and

00:15:26,040 --> 00:15:30,030
so I could say I want 10 meter bins and

00:15:28,110 --> 00:15:31,560
that's what the top picture is showing

00:15:30,030 --> 00:15:33,690
so I can do that in a two dimensional

00:15:31,560 --> 00:15:34,860
space or what's more interesting as I

00:15:33,690 --> 00:15:36,330
can actually do this in a three

00:15:34,860 --> 00:15:38,790
dimensional space where the third

00:15:36,330 --> 00:15:41,640
dimension is time and so we call these

00:15:38,790 --> 00:15:44,610
things space-time cubes and so you can

00:15:41,640 --> 00:15:47,040
take this data and add a time dimension

00:15:44,610 --> 00:15:49,740
to it and say for every hour of the day

00:15:47,040 --> 00:15:52,110
so 24 different vertical dimensions I

00:15:49,740 --> 00:15:54,180
want to see the distribution of data and

00:15:52,110 --> 00:15:56,340
Alice occurred over time so this might

00:15:54,180 --> 00:15:57,930
be population movement it might be other

00:15:56,340 --> 00:16:00,600
things that you want to want to consider

00:15:57,930 --> 00:16:02,250
and to blow your mind a little bit

00:16:00,600 --> 00:16:05,520
further you could even do this not just

00:16:02,250 --> 00:16:07,800
into cubes but into volumes so volumes

00:16:05,520 --> 00:16:09,390
being polygons or other things so these

00:16:07,800 --> 00:16:10,980
could be zip code boundaries or some

00:16:09,390 --> 00:16:14,250
other kind of shape that you you draw

00:16:10,980 --> 00:16:15,900
and you want to do some analysis on so

00:16:14,250 --> 00:16:16,920
to kind of explain this a little bit

00:16:15,900 --> 00:16:20,130
more I'm going to show you a quick

00:16:16,920 --> 00:16:23,070
little video demo of a project that we

00:16:20,130 --> 00:16:24,690
have this is data from a business

00:16:23,070 --> 00:16:27,210
partner of ours called safe graph it's

00:16:24,690 --> 00:16:29,000
anonymized cell phone data in new york

00:16:27,210 --> 00:16:32,280
city and what we're looking at here is

00:16:29,000 --> 00:16:34,230
some analysis that we pre computed and

00:16:32,280 --> 00:16:36,570
so these mobile phones this is mobile

00:16:34,230 --> 00:16:38,010
phone activity in Manhattan and we can

00:16:36,570 --> 00:16:40,140
slide through time through this little

00:16:38,010 --> 00:16:41,610
time slider on the bottom here and you

00:16:40,140 --> 00:16:44,220
can see the density of things so we're

00:16:41,610 --> 00:16:46,380
extruding the values based on the counts

00:16:44,220 --> 00:16:48,720
of mobile phones that are being counted

00:16:46,380 --> 00:16:51,450
at that particular time we can do this

00:16:48,720 --> 00:16:55,110
by aggregating the data into bins or

00:16:51,450 --> 00:16:57,090
into triangles or hexagons or even

00:16:55,110 --> 00:16:58,560
cylinders and so these are some of the

00:16:57,090 --> 00:17:00,990
extensions that we've done on top of

00:16:58,560 --> 00:17:02,060
elasticsearch to be able to aggregate by

00:17:00,990 --> 00:17:04,440
other types of sheets

00:17:02,060 --> 00:17:06,930
what's another interesting view of this

00:17:04,440 --> 00:17:09,240
is viewing this and kind of a what we

00:17:06,930 --> 00:17:12,360
call a spacetime ripple surface and so

00:17:09,240 --> 00:17:13,920
the space-time ripples surfaced is is

00:17:12,360 --> 00:17:17,430
exactly kind of like what it describes

00:17:13,920 --> 00:17:20,430
we're kind of depicting the the volume

00:17:17,430 --> 00:17:21,960
of the data based on where it is shoot I

00:17:20,430 --> 00:17:24,150
don't know whether it's flickering but

00:17:21,960 --> 00:17:26,070
as I span through time here I can see

00:17:24,150 --> 00:17:27,960
you know vertically where most of the

00:17:26,070 --> 00:17:29,580
density is occurring in this so these

00:17:27,960 --> 00:17:32,010
this is just another nice way to

00:17:29,580 --> 00:17:33,600
visualize this information and see where

00:17:32,010 --> 00:17:35,610
the distribution of this data was I

00:17:33,600 --> 00:17:37,860
could further drill into this and say

00:17:35,610 --> 00:17:39,870
just show me Android users or show me

00:17:37,860 --> 00:17:42,090
iPhone users or show me users that are

00:17:39,870 --> 00:17:43,800
greater than 30 years old so like a be

00:17:42,090 --> 00:17:47,040
interactive with us and she

00:17:43,800 --> 00:17:48,060
just on the fly to add that temporal

00:17:47,040 --> 00:17:50,970
dimension that we're talking about

00:17:48,060 --> 00:17:53,640
before we can go into a small little

00:17:50,970 --> 00:17:56,450
study area here and so we might want to

00:17:53,640 --> 00:17:59,670
know over time what was the mobile phone

00:17:56,450 --> 00:18:00,900
population in this specific area so as I

00:17:59,670 --> 00:18:02,460
slide through time here what I'm going

00:18:00,900 --> 00:18:04,290
to see is vertically I'm seeing the

00:18:02,460 --> 00:18:06,180
different time dimensions so these are

00:18:04,290 --> 00:18:08,250
different hours of the day or different

00:18:06,180 --> 00:18:10,290
days of the week based on what's

00:18:08,250 --> 00:18:12,270
occurring and so we can kind of pan

00:18:10,290 --> 00:18:14,280
around this area and interact with this

00:18:12,270 --> 00:18:16,200
and then we can also change the the

00:18:14,280 --> 00:18:18,450
visualization into the different shapes

00:18:16,200 --> 00:18:20,700
that we've indexed inside of our storage

00:18:18,450 --> 00:18:22,860
so this is just a nice cool way to

00:18:20,700 --> 00:18:25,350
visualize the data it's very interactive

00:18:22,860 --> 00:18:27,630
people can do queries against this stuff

00:18:25,350 --> 00:18:31,710
and do kind of what-if scenarios against

00:18:27,630 --> 00:18:34,170
it so that one's not open-source I sorry

00:18:31,710 --> 00:18:39,120
for that I wish I could give that one

00:18:34,170 --> 00:18:41,070
away so I'm gonna shift into a slightly

00:18:39,120 --> 00:18:43,590
different topic now which is deployment

00:18:41,070 --> 00:18:45,810
Portability and so this deployment

00:18:43,590 --> 00:18:47,520
portability is this is what a cluster

00:18:45,810 --> 00:18:50,340
ends up looking like when we deploy this

00:18:47,520 --> 00:18:52,290
for for a 30 user and so we take our

00:18:50,340 --> 00:18:54,690
containers and then based on that users

00:18:52,290 --> 00:18:57,150
need we deploy different number of

00:18:54,690 --> 00:18:59,550
instances of these things out and so we

00:18:57,150 --> 00:19:01,410
can we can define you know each of these

00:18:59,550 --> 00:19:03,180
SPARC streaming jobs is going to consume

00:19:01,410 --> 00:19:05,820
two or three cores and it's going to

00:19:03,180 --> 00:19:07,320
have X amount of RAM but I want 20 of

00:19:05,820 --> 00:19:08,910
those to run in this cluster because

00:19:07,320 --> 00:19:11,220
it's a connected car scenario and

00:19:08,910 --> 00:19:13,260
there's lots of data feeding in and I

00:19:11,220 --> 00:19:14,700
need lots of black sources to sit at the

00:19:13,260 --> 00:19:16,380
edge and be able to handle the events

00:19:14,700 --> 00:19:18,960
that are coming through and then I need

00:19:16,380 --> 00:19:21,300
lots of storage data nodes so that I can

00:19:18,960 --> 00:19:23,370
store this data and then finally I need

00:19:21,300 --> 00:19:25,110
visualization tasks that are running so

00:19:23,370 --> 00:19:27,180
that people can query a web app and

00:19:25,110 --> 00:19:28,860
generate the the rendering that I was

00:19:27,180 --> 00:19:31,050
showing in the thing that we just looked

00:19:28,860 --> 00:19:33,930
at a minute ago and so this is what an

00:19:31,050 --> 00:19:36,180
ant cluster looks like so this is with

00:19:33,930 --> 00:19:37,890
our ESRI software working I wanted to

00:19:36,180 --> 00:19:39,420
give you guys something that you could

00:19:37,890 --> 00:19:42,540
play around with so what I did is I

00:19:39,420 --> 00:19:45,600
created this project called DCOs IOT

00:19:42,540 --> 00:19:47,640
demo that's the github link up above and

00:19:45,600 --> 00:19:50,010
the pictures that are there or what I

00:19:47,640 --> 00:19:52,620
could give away is part of open source

00:19:50,010 --> 00:19:54,480
and I basically abstracted away all of

00:19:52,620 --> 00:19:57,490
our ESRI proprietary stuff out of this

00:19:54,480 --> 00:19:59,320
and said what if I just took scala Kafka

00:19:57,490 --> 00:20:01,090
spark elasticsearch and play I don't

00:19:59,320 --> 00:20:02,410
have a cool smack acronym I don't know

00:20:01,090 --> 00:20:06,100
maybe you could come up with an acronym

00:20:02,410 --> 00:20:07,660
for those things but this is basically

00:20:06,100 --> 00:20:10,150
using these underlying technologies

00:20:07,660 --> 00:20:11,590
without any kind of proprietary every

00:20:10,150 --> 00:20:13,600
stuff in it just to give you an example

00:20:11,590 --> 00:20:14,890
of here's how you can connect all these

00:20:13,600 --> 00:20:16,720
things together to build an application

00:20:14,890 --> 00:20:20,500
on top of DCOs

00:20:16,720 --> 00:20:22,390
and so I don't have enough time in this

00:20:20,500 --> 00:20:25,120
session actually go through the the demo

00:20:22,390 --> 00:20:26,650
to do this but there's a video and a

00:20:25,120 --> 00:20:27,880
number of other things up on that repo

00:20:26,650 --> 00:20:29,860
that you can kind of walk through this

00:20:27,880 --> 00:20:32,020
but what I wanted to describe is that

00:20:29,860 --> 00:20:34,090
when we ship this to a user our users

00:20:32,020 --> 00:20:36,070
kind of dictate I want this to run on

00:20:34,090 --> 00:20:37,990
Amazon or I want this to run on Azure or

00:20:36,070 --> 00:20:41,260
I want this to run on the gov cloud on a

00:20:37,990 --> 00:20:43,540
c2s environment for for the government

00:20:41,260 --> 00:20:45,370
or I want it to run even on-premise

00:20:43,540 --> 00:20:47,290
because I have my own hardware my own

00:20:45,370 --> 00:20:49,809
data center to do this so we don't

00:20:47,290 --> 00:20:51,550
really have the luxury to say we deliver

00:20:49,809 --> 00:20:52,720
the software and we deliver it on Amazon

00:20:51,550 --> 00:20:54,940
and that's it that's the only

00:20:52,720 --> 00:20:57,370
environment we have to support so one of

00:20:54,940 --> 00:20:59,140
the the key benefits of using DCOs is

00:20:57,370 --> 00:21:00,700
selecting that as a there's a way to

00:20:59,140 --> 00:21:02,890
package our software and deliver it to

00:21:00,700 --> 00:21:05,020
customers is that it makes it so that we

00:21:02,890 --> 00:21:08,380
can make this portable across multiple

00:21:05,020 --> 00:21:09,429
different infrastructures and so as Toby

00:21:08,380 --> 00:21:11,830
and other folks we're kind of pointing

00:21:09,429 --> 00:21:14,050
out this morning it's for real and this

00:21:11,830 --> 00:21:15,970
this DC OS IOT demo kind of walks

00:21:14,050 --> 00:21:16,900
through the installation steps on how to

00:21:15,970 --> 00:21:18,880
do that in various different

00:21:16,900 --> 00:21:21,760
environments so if you go to that

00:21:18,880 --> 00:21:24,070
project what you'll see is a tour so

00:21:21,760 --> 00:21:25,870
steps on how to get this environment

00:21:24,070 --> 00:21:27,160
together and number one is you know how

00:21:25,870 --> 00:21:29,620
to deploy this across different

00:21:27,160 --> 00:21:31,620
environments so the azure amazon and

00:21:29,620 --> 00:21:34,570
on-premise has some pretty decent

00:21:31,620 --> 00:21:37,059
documentation around it c2s is coming

00:21:34,570 --> 00:21:39,040
soon but that's you know a very

00:21:37,059 --> 00:21:41,860
specialized thing so if you want to talk

00:21:39,040 --> 00:21:43,420
more about that let me know but we we

00:21:41,860 --> 00:21:45,370
have very good documentation up here

00:21:43,420 --> 00:21:46,929
it's step by step you know it probably

00:21:45,370 --> 00:21:48,520
take you a couple days to walk through

00:21:46,929 --> 00:21:50,770
this tutorial and stand up the

00:21:48,520 --> 00:21:53,170
environment for yourself but it's it's

00:21:50,770 --> 00:21:54,400
all available there it goes through all

00:21:53,170 --> 00:21:56,530
the stuffs and I'm just going to

00:21:54,400 --> 00:21:59,080
visually kind of walk through what what

00:21:56,530 --> 00:22:01,570
happens and so two provision this

00:21:59,080 --> 00:22:02,980
environment - any of these different

00:22:01,570 --> 00:22:05,440
providers really the only thing that's

00:22:02,980 --> 00:22:07,360
cloud specific or provider specific is

00:22:05,440 --> 00:22:09,700
the first step which is provisioning

00:22:07,360 --> 00:22:10,900
your actual resources so when you go to

00:22:09,700 --> 00:22:12,750
AZ or Amazon

00:22:10,900 --> 00:22:15,280
different ways to do that for a sure

00:22:12,750 --> 00:22:17,440
there's a thing called armed templates

00:22:15,280 --> 00:22:19,570
so those are like the cloud formation

00:22:17,440 --> 00:22:21,970
templates inside Amazon so you would

00:22:19,570 --> 00:22:23,830
define an azure template we actually

00:22:21,970 --> 00:22:25,810
created an arm template David in the

00:22:23,830 --> 00:22:27,310
audience or my team created this really

00:22:25,810 --> 00:22:30,270
cool arm template that'll allow you to

00:22:27,310 --> 00:22:32,740
just type in I want three masters 30

00:22:30,270 --> 00:22:34,930
private agents and three public agents

00:22:32,740 --> 00:22:37,930
and boom you've got a resource that

00:22:34,930 --> 00:22:39,700
looks like this if you do it on Amazon

00:22:37,930 --> 00:22:42,130
there's cloud formation templates if you

00:22:39,700 --> 00:22:43,300
do it on c2s it's cloud formation

00:22:42,130 --> 00:22:45,460
templates but it's an awful awful

00:22:43,300 --> 00:22:47,410
offline system and a disconnected

00:22:45,460 --> 00:22:49,000
environment and then if you want to do

00:22:47,410 --> 00:22:50,320
it on premise it's basically IT

00:22:49,000 --> 00:22:52,930
administrators setting up this

00:22:50,320 --> 00:22:54,970
environment and so there's a lot of

00:22:52,930 --> 00:22:56,590
steps to do it on premise if you want to

00:22:54,970 --> 00:22:59,020
do that and we've taken a first attempt

00:22:56,590 --> 00:23:00,520
at documenting what those are but the

00:22:59,020 --> 00:23:02,890
mesosphere documentation has

00:23:00,520 --> 00:23:05,200
comprehensive information about that as

00:23:02,890 --> 00:23:06,490
well but basically this is the only step

00:23:05,200 --> 00:23:09,010
that's different depending on the

00:23:06,490 --> 00:23:11,050
infrastructure and so once you have this

00:23:09,010 --> 00:23:12,820
resources in place and all the

00:23:11,050 --> 00:23:14,530
prerequisites a place everything is the

00:23:12,820 --> 00:23:16,990
same regardless of what infrastructure

00:23:14,530 --> 00:23:19,990
you're using so to install DCOs

00:23:16,990 --> 00:23:21,490
we took the meso spear installation

00:23:19,990 --> 00:23:24,670
templates and kind of generalize them a

00:23:21,490 --> 00:23:26,530
bit more and basically we copy up those

00:23:24,670 --> 00:23:29,350
up these come from that github site so

00:23:26,530 --> 00:23:32,020
I'm just copying up my private key and

00:23:29,350 --> 00:23:33,490
my my credentials and I'm copying up the

00:23:32,020 --> 00:23:35,440
installation script and this

00:23:33,490 --> 00:23:37,600
installation script as part of the the

00:23:35,440 --> 00:23:41,350
project that's the repo that we shared

00:23:37,600 --> 00:23:44,230
and so once I have that I SSH into the

00:23:41,350 --> 00:23:45,910
buta node the buta node is one of the

00:23:44,230 --> 00:23:48,430
administrative nodes that allows me to

00:23:45,910 --> 00:23:51,100
actually install DCOs and and manage the

00:23:48,430 --> 00:23:52,930
infrastructure and so once I do that

00:23:51,100 --> 00:23:54,820
then I just basically run a an

00:23:52,930 --> 00:23:57,400
installation script and I give it a

00:23:54,820 --> 00:23:58,660
argument of how many masters I want how

00:23:57,400 --> 00:24:01,330
many private agents I want how many

00:23:58,660 --> 00:24:04,180
public agents that I want to be in my

00:24:01,330 --> 00:24:06,460
environment and when I do that it asked

00:24:04,180 --> 00:24:09,190
me what type of DCOs installation do I

00:24:06,460 --> 00:24:11,260
want do you want the latest open-source

00:24:09,190 --> 00:24:13,780
version do you want an enterprise

00:24:11,260 --> 00:24:15,580
version which version do you want and so

00:24:13,780 --> 00:24:17,620
you can type in a URL if you want a

00:24:15,580 --> 00:24:19,990
specific Enterprise version and then you

00:24:17,620 --> 00:24:23,200
put your credentials in and then voila

00:24:19,990 --> 00:24:25,299
it installs this so to provision the

00:24:23,200 --> 00:24:27,429
to provision the actual machines on

00:24:25,299 --> 00:24:29,710
Amazon or Azure it takes about three to

00:24:27,429 --> 00:24:31,929
four minutes to spin out 30 nodes so it

00:24:29,710 --> 00:24:33,370
doesn't really take too long and to run

00:24:31,929 --> 00:24:36,279
this installation script and lay down

00:24:33,370 --> 00:24:37,809
bare-bones DCOs mesas marathon

00:24:36,279 --> 00:24:40,450
everything else that comes along with it

00:24:37,809 --> 00:24:42,159
takes another six or seven minutes so

00:24:40,450 --> 00:24:45,340
within ten minutes you can have a thirty

00:24:42,159 --> 00:24:49,510
node DCOs cluster at your disposal to

00:24:45,340 --> 00:24:51,399
start doing things with after this

00:24:49,510 --> 00:24:53,740
happens it runs through and it tells you

00:24:51,399 --> 00:24:55,360
what your masters are and then it tells

00:24:53,740 --> 00:24:57,519
you some stats about how long it took

00:24:55,360 --> 00:25:01,929
and so this one took about seven minutes

00:24:57,519 --> 00:25:04,149
or so at that point we have a DCOs

00:25:01,929 --> 00:25:05,830
environment that's ready to go and we

00:25:04,149 --> 00:25:07,899
don't really have any services deployed

00:25:05,830 --> 00:25:10,210
to that so what's running is all the

00:25:07,899 --> 00:25:11,409
agents have mesas loaded on them all the

00:25:10,210 --> 00:25:13,630
Masters are ready to go they've

00:25:11,409 --> 00:25:15,370
participating in a quorum and then

00:25:13,630 --> 00:25:17,350
marathon is available and all the other

00:25:15,370 --> 00:25:19,299
frameworks that are in the universe are

00:25:17,350 --> 00:25:22,659
available for use within this

00:25:19,299 --> 00:25:24,880
environment and so the next step as far

00:25:22,659 --> 00:25:27,279
as part of our demo app that we have is

00:25:24,880 --> 00:25:29,440
to install Kafka and so you can select

00:25:27,279 --> 00:25:31,000
you go to universe and install Kafka and

00:25:29,440 --> 00:25:33,399
you say I want to have a five broker

00:25:31,000 --> 00:25:36,490
system you specify the cores the RAM the

00:25:33,399 --> 00:25:38,350
etc that you want and then a few seconds

00:25:36,490 --> 00:25:40,779
couple minutes later you have a five

00:25:38,350 --> 00:25:44,019
broker system in that environment

00:25:40,779 --> 00:25:47,110
similarly elasticsearch as part of the

00:25:44,019 --> 00:25:49,450
elastic package so you can go to the

00:25:47,110 --> 00:25:51,370
universe and install elastic and you'll

00:25:49,450 --> 00:25:53,679
get the whole elke stack you'll get the

00:25:51,370 --> 00:25:55,059
log stash you get kibana if you want to

00:25:53,679 --> 00:25:57,760
install that you'll get all the other

00:25:55,059 --> 00:26:00,010
parts of the ELQ stack as well but you

00:25:57,760 --> 00:26:01,840
get data nodes of elastic search and you

00:26:00,010 --> 00:26:03,940
can specify how many data nodes you want

00:26:01,840 --> 00:26:05,830
what size of allocation you want to give

00:26:03,940 --> 00:26:07,870
to each of those as far as RAM and

00:26:05,830 --> 00:26:09,970
storage and everything else and then you

00:26:07,870 --> 00:26:11,710
can spin that up so this is you know

00:26:09,970 --> 00:26:13,870
another couple minutes later you have a

00:26:11,710 --> 00:26:16,000
10 dude elasticsearch cluster it's a now

00:26:13,870 --> 00:26:17,679
we're at about 15 minutes so we're doing

00:26:16,000 --> 00:26:20,950
pretty good so if you try to do this by

00:26:17,679 --> 00:26:23,500
hand it would take you days probably and

00:26:20,950 --> 00:26:26,139
then we can deploy our reactive web app

00:26:23,500 --> 00:26:27,669
so once a developer is kind of written

00:26:26,139 --> 00:26:29,440
this app it's pretty easy all the source

00:26:27,669 --> 00:26:31,659
codes available on that repo and you can

00:26:29,440 --> 00:26:33,460
deploy this reactive Play app out there

00:26:31,659 --> 00:26:35,500
and this is the app that's going to

00:26:33,460 --> 00:26:36,390
allow people to query in and generate

00:26:35,500 --> 00:26:39,030
the map there

00:26:36,390 --> 00:26:40,650
Colon's that we did before and then we

00:26:39,030 --> 00:26:42,450
want to run some spark streaming jobs

00:26:40,650 --> 00:26:44,490
and again that spark streaming job could

00:26:42,450 --> 00:26:45,630
be sized to have as many instances as

00:26:44,490 --> 00:26:47,940
you want in this case it's five

00:26:45,630 --> 00:26:49,560
instances and that spark streaming job

00:26:47,940 --> 00:26:51,840
is going to be listening to topics on

00:26:49,560 --> 00:26:54,000
Kafka and then we finally need something

00:26:51,840 --> 00:26:56,280
to actually write data into Kafka and so

00:26:54,000 --> 00:26:59,100
we have a Kafka producer application

00:26:56,280 --> 00:27:02,220
what we call source and those sources

00:26:59,100 --> 00:27:05,160
are written in scala the source that we

00:27:02,220 --> 00:27:08,430
give with this repo is just querying a

00:27:05,160 --> 00:27:10,680
big I think it's a gig and a half CSV

00:27:08,430 --> 00:27:12,510
file that's on s3 loads it into memory

00:27:10,680 --> 00:27:15,780
and just starts pushing taxi cab

00:27:12,510 --> 00:27:18,330
movement events into the Kafka system

00:27:15,780 --> 00:27:20,360
and so it's a fairly simple source and

00:27:18,330 --> 00:27:23,040
at that point we have our application

00:27:20,360 --> 00:27:25,110
running and then we can query this

00:27:23,040 --> 00:27:27,390
through through the map interface

00:27:25,110 --> 00:27:29,220
hitting the play app and then we can

00:27:27,390 --> 00:27:30,990
start to visualize the information and

00:27:29,220 --> 00:27:32,130
you get these time sliders you get all

00:27:30,990 --> 00:27:33,990
kinds of other things that you can play

00:27:32,130 --> 00:27:37,260
around with the data so it's a pretty

00:27:33,990 --> 00:27:39,180
cool sample app if you saw the mesas con

00:27:37,260 --> 00:27:41,810
keno last year we showed a very early

00:27:39,180 --> 00:27:46,140
form of this back then we've since

00:27:41,810 --> 00:27:48,450
updated this to the latest specs DCOs

00:27:46,140 --> 00:27:49,920
110 came out last week so I haven't

00:27:48,450 --> 00:27:52,040
tested it against that but I'll do that

00:27:49,920 --> 00:27:54,870
shortly and make sure it works with 110

00:27:52,040 --> 00:27:56,870
and then you should have an environment

00:27:54,870 --> 00:28:00,000
to play around with and do things with

00:27:56,870 --> 00:28:01,710
so the end result is that you get an

00:28:00,000 --> 00:28:03,270
interface that looks something like this

00:28:01,710 --> 00:28:07,080
you can kind of see a time slider at the

00:28:03,270 --> 00:28:09,240
bottom you can render things in the geo

00:28:07,080 --> 00:28:11,460
hash aggregations is what's on the left

00:28:09,240 --> 00:28:13,530
and you can kind of see the density of

00:28:11,460 --> 00:28:14,820
where it's red is where a more taxicabs

00:28:13,530 --> 00:28:17,460
are moving around at that point in time

00:28:14,820 --> 00:28:20,430
and then the yellow is kind of less

00:28:17,460 --> 00:28:22,500
density and so you can zoom and zoom out

00:28:20,430 --> 00:28:23,790
it'll it'll change the shapes based on

00:28:22,500 --> 00:28:25,470
that and you'll see different levels of

00:28:23,790 --> 00:28:27,840
aggregation as you zoom in and zoom out

00:28:25,470 --> 00:28:29,910
and then you can pan through time as

00:28:27,840 --> 00:28:32,250
well and if you want to visualize it as

00:28:29,910 --> 00:28:34,200
a heat map what actually happens is the

00:28:32,250 --> 00:28:36,090
data comes back from elasticsearch into

00:28:34,200 --> 00:28:37,680
the client and then the client generates

00:28:36,090 --> 00:28:39,840
these heat maps on the client side and

00:28:37,680 --> 00:28:41,010
these heat maps can be used to to

00:28:39,840 --> 00:28:43,620
visualize the data in a slightly

00:28:41,010 --> 00:28:47,770
different way I wish I could give away

00:28:43,620 --> 00:28:49,960
the cool 3d thing that we showed before

00:28:47,770 --> 00:28:51,190
so that's pretty much what I wanted to

00:28:49,960 --> 00:28:59,610
go through in this session I want to

00:28:51,190 --> 00:28:59,610
leave good time for 10 minutes for ya

00:30:21,260 --> 00:30:26,970
so the question is about how do we do

00:30:24,600 --> 00:30:30,510
simulations it's kind of a concise way I

00:30:26,970 --> 00:30:31,560
think to summarize that so simulations

00:30:30,510 --> 00:30:33,240
can be done as part of the batch

00:30:31,560 --> 00:30:35,460
analysis the green things that we have

00:30:33,240 --> 00:30:37,530
there and so we we oftentimes have

00:30:35,460 --> 00:30:40,200
customers specifically in the military

00:30:37,530 --> 00:30:42,360
that want to do like what if we go here

00:30:40,200 --> 00:30:44,520
and do X you know what would happen in

00:30:42,360 --> 00:30:46,050
that scenario and so they can run kind

00:30:44,520 --> 00:30:48,120
of these analytics that we have this

00:30:46,050 --> 00:30:50,040
capabilities and then get a result from

00:30:48,120 --> 00:30:52,200
that and then if they want to play back

00:30:50,040 --> 00:30:54,000
that data and kind of see it occur over

00:30:52,200 --> 00:30:55,680
time they can feed that data back in

00:30:54,000 --> 00:30:57,720
through this loop it's not necessarily

00:30:55,680 --> 00:31:00,090
real time but it's simulated data and

00:30:57,720 --> 00:31:01,950
they can actually control the velocity

00:31:00,090 --> 00:31:03,600
of that so play it back ten times normal

00:31:01,950 --> 00:31:05,610
speed play it back you know less than

00:31:03,600 --> 00:31:07,410
normal speed and so that's typically how

00:31:05,610 --> 00:31:09,480
folks do that simulation is a very big

00:31:07,410 --> 00:31:12,800
part of of what we do as wells of

00:31:09,480 --> 00:31:14,850
what-if analysis what if this

00:31:12,800 --> 00:31:16,470
circumstance that we actually recorded

00:31:14,850 --> 00:31:17,910
was different what if we responded

00:31:16,470 --> 00:31:20,550
sooner what might have happened with

00:31:17,910 --> 00:31:23,460
that what if the water levels rise X

00:31:20,550 --> 00:31:24,690
amount in the next X minutes what will

00:31:23,460 --> 00:31:26,430
that do to effect things and that's

00:31:24,690 --> 00:31:28,770
where you see like here's the floodplain

00:31:26,430 --> 00:31:31,470
of New York City or something if the

00:31:28,770 --> 00:31:33,360
water level rises to this level so yeah

00:31:31,470 --> 00:31:35,850
ESRI definitely has tools to help with

00:31:33,360 --> 00:31:38,010
that but it's done now say I have to be

00:31:35,850 --> 00:31:45,720
real-time it's just a feed of data that

00:31:38,010 --> 00:31:48,180
you can control back in hopefully if

00:31:45,720 --> 00:31:49,950
it's not real-time you can go back and

00:31:48,180 --> 00:31:51,330
inside of our tools and actually rewind

00:31:49,950 --> 00:31:53,190
it through the time sliders and the

00:31:51,330 --> 00:31:55,560
other things that we have but if you

00:31:53,190 --> 00:31:56,940
want to kind of simulate it as it's

00:31:55,560 --> 00:31:58,170
happening now and it looks like it's

00:31:56,940 --> 00:31:59,520
happening now you would feed it back

00:31:58,170 --> 00:32:03,090
through this loop and then you could

00:31:59,520 --> 00:32:05,210
kind of see it over time question in the

00:32:03,090 --> 00:32:05,210
back

00:32:16,090 --> 00:32:19,610
so the question is do we have any

00:32:18,050 --> 00:32:21,920
challenges deploying Kafka or other

00:32:19,610 --> 00:32:23,900
things other than just doing it outside

00:32:21,920 --> 00:32:26,270
of DC us now it's much easier with in DC

00:32:23,900 --> 00:32:29,840
OS who it gives us a very deterministic

00:32:26,270 --> 00:32:31,550
way to deliver these brokers and it does

00:32:29,840 --> 00:32:34,370
that through the scheduling system of

00:32:31,550 --> 00:32:36,590
DCs and mesas where it'll actually land

00:32:34,370 --> 00:32:38,780
on nodes that have availability and so

00:32:36,590 --> 00:32:40,640
we don't really have to think about this

00:32:38,780 --> 00:32:42,380
machine is for Kafka or this machine is

00:32:40,640 --> 00:32:43,760
for elasticsearch we can have a general

00:32:42,380 --> 00:32:45,860
framework and get much better

00:32:43,760 --> 00:32:48,830
utilization out of the systems if we

00:32:45,860 --> 00:32:51,230
want to to give hence to the scheduler

00:32:48,830 --> 00:32:53,090
to say I actually want this to be a data

00:32:51,230 --> 00:32:54,950
node in this to be an ingestion node you

00:32:53,090 --> 00:32:56,360
can do that as well through through DC

00:32:54,950 --> 00:32:59,360
OS and Mace's through what's called a

00:32:56,360 --> 00:33:01,550
placement constraint so I can say or if

00:32:59,360 --> 00:33:03,170
I don't want to Kafka brokers to land on

00:33:01,550 --> 00:33:05,570
the same machine I can give it unique

00:33:03,170 --> 00:33:07,490
host constraints so only run this on one

00:33:05,570 --> 00:33:09,410
machine so you have a lot of flexibility

00:33:07,490 --> 00:33:11,930
in how you do that but it's much easier

00:33:09,410 --> 00:33:21,890
to deploy within DC US than it is to try

00:33:11,930 --> 00:33:23,810
to do it on your yes sir so the question

00:33:21,890 --> 00:33:25,130
is are the extensions we wrote for SPARC

00:33:23,810 --> 00:33:27,860
open source so they're not so

00:33:25,130 --> 00:33:30,710
unfortunately there is some a couple

00:33:27,860 --> 00:33:32,570
samples in the the repo that talk about

00:33:30,710 --> 00:33:34,790
how to use our open source we do have an

00:33:32,570 --> 00:33:36,680
open source library that's it's called

00:33:34,790 --> 00:33:38,660
GIS tools for Hadoop but it also works

00:33:36,680 --> 00:33:39,440
with SPARC and so we have a couple

00:33:38,660 --> 00:33:40,940
examples

00:33:39,440 --> 00:33:43,220
I think there's five or six operators

00:33:40,940 --> 00:33:45,470
that do geospatial things with SPARC

00:33:43,220 --> 00:33:47,990
there's no user defined types it's only

00:33:45,470 --> 00:33:49,760
user defined functions but that's a good

00:33:47,990 --> 00:33:51,620
starting point and there's there is an

00:33:49,760 --> 00:33:54,200
app and this repo that uses that to do

00:33:51,620 --> 00:33:56,900
like a geofence detection so tell me

00:33:54,200 --> 00:33:58,640
when this enters this area so if you

00:33:56,900 --> 00:34:00,050
want to get a basic example of that that

00:33:58,640 --> 00:34:04,240
you could expand further you can take a

00:34:00,050 --> 00:34:04,240
look there yes

00:34:05,520 --> 00:34:09,580
so the question is have you done

00:34:07,690 --> 00:34:12,100
anything with auto-scaling so we haven't

00:34:09,580 --> 00:34:13,870
yet so the way we deliver these things

00:34:12,100 --> 00:34:15,250
right now is a managed service so we

00:34:13,870 --> 00:34:16,960
deliver it for customer and we kind of

00:34:15,250 --> 00:34:19,360
manage it for them and so we monitor it

00:34:16,960 --> 00:34:21,790
and have humans involved to do scaling

00:34:19,360 --> 00:34:23,110
and things it wouldn't be hard to add

00:34:21,790 --> 00:34:24,850
auto scaling and there's some new

00:34:23,110 --> 00:34:28,030
capabilities coming in DCs that'll help

00:34:24,850 --> 00:34:29,980
with that but having good

00:34:28,030 --> 00:34:32,710
instrumentation is super important for

00:34:29,980 --> 00:34:36,130
all this so a lot of times when you see

00:34:32,710 --> 00:34:38,650
talks about PCOS and and monitoring it's

00:34:36,130 --> 00:34:40,720
all about what's the container CPU and

00:34:38,650 --> 00:34:42,550
what's the RAM doing but when you build

00:34:40,720 --> 00:34:43,570
a higher order application like this

00:34:42,550 --> 00:34:45,340
it's really important to think about

00:34:43,570 --> 00:34:47,500
what are the application metrics that I

00:34:45,340 --> 00:34:49,540
want to do so it's more about how many

00:34:47,500 --> 00:34:52,650
bytes per second am i streaming through

00:34:49,540 --> 00:34:54,730
the system or how many how many car

00:34:52,650 --> 00:34:57,070
observations are I received and I know

00:34:54,730 --> 00:34:59,320
that I'm supposed to expect 100,000 to

00:34:57,070 --> 00:35:00,460
200,000 events per second and if I get

00:34:59,320 --> 00:35:02,410
something drastically different from

00:35:00,460 --> 00:35:04,300
that I need to sound an alarm and see if

00:35:02,410 --> 00:35:07,060
somebody can do something about that or

00:35:04,300 --> 00:35:09,310
if I'm supposed to have a response time

00:35:07,060 --> 00:35:11,290
of my map visualization generation

00:35:09,310 --> 00:35:13,240
sub-second and I'm not getting that I

00:35:11,290 --> 00:35:15,720
need to get those stats in so there's

00:35:13,240 --> 00:35:19,510
some really cool DCOs metrics

00:35:15,720 --> 00:35:21,700
capabilities in DCU s19 and liter and

00:35:19,510 --> 00:35:23,740
those metrics we actually use to add

00:35:21,700 --> 00:35:25,780
application level metrics in so our

00:35:23,740 --> 00:35:27,370
containers write application metrics

00:35:25,780 --> 00:35:30,160
into that and then we can hook on to the

00:35:27,370 --> 00:35:32,920
event bus to receive those around so

00:35:30,160 --> 00:35:33,850
we've we've built our own kind of things

00:35:32,920 --> 00:35:35,740
in that and you could use those

00:35:33,850 --> 00:35:38,650
application metrics for example to say

00:35:35,740 --> 00:35:40,990
if I've got a connected car feed coming

00:35:38,650 --> 00:35:43,750
in and there's a topic for outies or

00:35:40,990 --> 00:35:45,850
whatever it is then if the audi topic is

00:35:43,750 --> 00:35:47,650
growing and my spark streaming jobs

00:35:45,850 --> 00:35:49,120
aren't keeping up i want to auto scale

00:35:47,650 --> 00:35:50,890
more instances of the spark streaming

00:35:49,120 --> 00:35:53,260
job so you could have the infrastructure

00:35:50,890 --> 00:35:54,370
in place to do that we haven't gone to

00:35:53,260 --> 00:35:58,050
that level yet but it's certainly

00:35:54,370 --> 00:35:58,050
possible to do yeah

00:36:03,369 --> 00:36:09,049
the question is do you have stats or

00:36:06,140 --> 00:36:11,239
metrics on how many megabytes or points

00:36:09,049 --> 00:36:14,660
per core you can do that I don't think

00:36:11,239 --> 00:36:16,880
we have it by core but we have it based

00:36:14,660 --> 00:36:18,949
on we've been able to scale up to

00:36:16,880 --> 00:36:20,630
millions of events per second on the

00:36:18,949 --> 00:36:22,489
sources and the kafka brokers and

00:36:20,630 --> 00:36:23,989
depending on your analytics you're

00:36:22,489 --> 00:36:26,299
performing it'll drastically be

00:36:23,989 --> 00:36:29,420
different so if you're doing analysis

00:36:26,299 --> 00:36:31,039
across geofences and there's 800,000

00:36:29,420 --> 00:36:33,019
geofences that's going to be different

00:36:31,039 --> 00:36:35,569
than if there's and those geofences are

00:36:33,019 --> 00:36:36,829
to have 300 edges on them then that's

00:36:35,569 --> 00:36:38,809
going to be drastically different than

00:36:36,829 --> 00:36:40,759
if it's just rectangles and there's ten

00:36:38,809 --> 00:36:42,410
of them so it kind of depends on that

00:36:40,759 --> 00:36:44,689
alysus that you're doing and it's kind

00:36:42,410 --> 00:36:46,789
of dependent on the application but we

00:36:44,689 --> 00:36:48,799
certainly do capacity plan and help

00:36:46,789 --> 00:36:51,229
customers with their use cases to do

00:36:48,799 --> 00:36:54,589
that the geofence has also can be

00:36:51,229 --> 00:36:56,509
dynamic as well so it could be ricardo's

00:36:54,589 --> 00:36:58,459
walking around and i want to generate

00:36:56,509 --> 00:37:00,499
one of our analytics tools can create a

00:36:58,459 --> 00:37:03,380
service area where the service is where

00:37:00,499 --> 00:37:05,749
can Ricardo walk in one minute and so he

00:37:03,380 --> 00:37:07,459
gets a polygon around him and I have a

00:37:05,749 --> 00:37:09,619
polygon around me and I want to know if

00:37:07,459 --> 00:37:11,749
those two polygons ever intersect so in

00:37:09,619 --> 00:37:13,369
that case the polygons are drastically

00:37:11,749 --> 00:37:15,769
changing all the time because every time

00:37:13,369 --> 00:37:17,630
I move I get a new polygon so with those

00:37:15,769 --> 00:37:19,249
types of scenarios that you get less

00:37:17,630 --> 00:37:21,170
events per second because the polygons

00:37:19,249 --> 00:37:22,910
are constantly moving around and things

00:37:21,170 --> 00:37:24,979
of that nature so it really depends on

00:37:22,910 --> 00:37:26,569
the the use case but we've been able to

00:37:24,979 --> 00:37:28,819
scale this up to millions of events per

00:37:26,569 --> 00:37:32,779
second and it takes you know tens or

00:37:28,819 --> 00:37:34,339
Sonos to do that it took like ten kafka

00:37:32,779 --> 00:37:36,640
brokers to get to millions of events per

00:37:34,339 --> 00:37:36,640
second

00:37:44,880 --> 00:37:50,740
yeah that's a great question great

00:37:47,710 --> 00:37:52,809
comment so the question is what are you

00:37:50,740 --> 00:37:55,450
doing are you doing stateful processing

00:37:52,809 --> 00:37:56,950
in your real-time analytics and if you

00:37:55,450 --> 00:37:58,299
are what are you doing for that so

00:37:56,950 --> 00:38:01,030
that's a problem we haven't yet solved

00:37:58,299 --> 00:38:03,579
so we in our traditional product we do

00:38:01,030 --> 00:38:06,549
stateful streaming analysis so we want

00:38:03,579 --> 00:38:08,049
to know when Adam enters this room if I

00:38:06,549 --> 00:38:09,760
need to know when Adam enters this room

00:38:08,049 --> 00:38:12,880
I need to know his last position in his

00:38:09,760 --> 00:38:14,859
current position and so being able to do

00:38:12,880 --> 00:38:17,589
stateful processing is relatively easy

00:38:14,859 --> 00:38:19,329
to do being able to do reliable stateful

00:38:17,589 --> 00:38:21,369
processing is very difficult to do and

00:38:19,329 --> 00:38:23,490
so that's a problem we're still figuring

00:38:21,369 --> 00:38:26,049
out is how we're gonna do that with

00:38:23,490 --> 00:38:28,119
reliability on that because we need to

00:38:26,049 --> 00:38:30,309
there's there's a number of factors like

00:38:28,119 --> 00:38:32,710
if you do that in a spark streaming job

00:38:30,309 --> 00:38:34,720
you need to actually have the partitions

00:38:32,710 --> 00:38:36,970
and Kafka's such that you get the same

00:38:34,720 --> 00:38:39,010
track ID on the same spark streaming job

00:38:36,970 --> 00:38:41,289
so that I know that and so that's one

00:38:39,010 --> 00:38:43,420
way to start with that but if that spark

00:38:41,289 --> 00:38:45,520
streaming job fails you want to be able

00:38:43,420 --> 00:38:46,599
to have that state on the other spark

00:38:45,520 --> 00:38:48,819
straining job that picks up that

00:38:46,599 --> 00:38:49,660
partition after that so that's a harder

00:38:48,819 --> 00:38:52,029
problem of how do you actually

00:38:49,660 --> 00:38:53,559
synchronize that and if you do that how

00:38:52,029 --> 00:38:55,079
does that affect your throughput and so

00:38:53,559 --> 00:38:57,849
there's a number of factors to consider

00:38:55,079 --> 00:39:00,640
given that we have we've prioritized our

00:38:57,849 --> 00:39:02,289
stateful streaming stuff less and our

00:39:00,640 --> 00:39:04,240
customers have actually been accepting

00:39:02,289 --> 00:39:06,279
of that the way that we've kind of

00:39:04,240 --> 00:39:08,260
compensated for that is our batch

00:39:06,279 --> 00:39:10,569
analysis capabilities can be scheduled

00:39:08,260 --> 00:39:12,130
on a regular basis so if they want to do

00:39:10,569 --> 00:39:14,829
stateful processing they actually do

00:39:12,130 --> 00:39:18,150
that every 30 seconds every minute we

00:39:14,829 --> 00:39:20,200
run a batch analytic or a spark job and

00:39:18,150 --> 00:39:21,849
calculate and detect those things and

00:39:20,200 --> 00:39:23,380
send that out so it's typically

00:39:21,849 --> 00:39:25,569
acceptable that there's a 30 second

00:39:23,380 --> 00:39:27,520
latency between those alerts happening

00:39:25,569 --> 00:39:29,109
so that's how we've compensated and how

00:39:27,520 --> 00:39:30,970
we've avoided having to solve that

00:39:29,109 --> 00:39:34,650
problem so it's a great question on a

00:39:30,970 --> 00:39:34,650
very difficult problem we spent years on

00:39:38,790 --> 00:39:44,290
yeah yeah so the batch analysis so spark

00:39:42,850 --> 00:39:45,880
stream jobs right the data into

00:39:44,290 --> 00:39:47,980
elasticsearch and then our batch jobs

00:39:45,880 --> 00:39:49,810
use metronome and they basically

00:39:47,980 --> 00:39:52,270
scheduled themselves every 30 seconds

00:39:49,810 --> 00:39:54,310
every minute run detect when Adams

00:39:52,270 --> 00:39:56,080
entered something and so that way it

00:39:54,310 --> 00:39:58,390
doesn't really matter which spark

00:39:56,080 --> 00:40:00,130
streaming job process to and so that's

00:39:58,390 --> 00:40:02,110
the reliable way that we can do that and

00:40:00,130 --> 00:40:03,070
if data comes in late we can compensate

00:40:02,110 --> 00:40:05,800
for that as well

00:40:03,070 --> 00:40:08,640
which is another problem so when you're

00:40:05,800 --> 00:40:11,770
dealing with streaming state in a stream

00:40:08,640 --> 00:40:25,810
we got time for one last question all

00:40:11,770 --> 00:40:27,460
right who wants the last question so the

00:40:25,810 --> 00:40:29,350
question is how do you deal with out of

00:40:27,460 --> 00:40:31,510
sequence things and and something goes

00:40:29,350 --> 00:40:33,100
down so that's another reason why we've

00:40:31,510 --> 00:40:34,930
kind of compensated with a recurring

00:40:33,100 --> 00:40:36,820
batch analysis capability and so

00:40:34,930 --> 00:40:39,850
somebody needs to be thoughtful about

00:40:36,820 --> 00:40:42,490
how latent can something be and so if

00:40:39,850 --> 00:40:44,800
it's gonna be you know a 50-minute

00:40:42,490 --> 00:40:46,690
latency potential then you want to run

00:40:44,800 --> 00:40:48,160
that every hour as opposed to that so

00:40:46,690 --> 00:40:49,359
you can try to fit it in so there's

00:40:48,160 --> 00:40:51,340
always going to be circumstances where

00:40:49,359 --> 00:40:53,230
something comes out of order so maybe

00:40:51,340 --> 00:40:54,940
somebody turns their phone off and it

00:40:53,230 --> 00:40:56,170
kind of queues up the events so that

00:40:54,940 --> 00:40:58,180
when they get connected again it

00:40:56,170 --> 00:40:59,380
distributes everything and all of those

00:40:58,180 --> 00:41:01,900
come out of sequence because they get

00:40:59,380 --> 00:41:03,670
processed in parallel so that's a better

00:41:01,900 --> 00:41:06,310
way to do it is to actually do recurring

00:41:03,670 --> 00:41:08,140
analytics through a batch recurring

00:41:06,310 --> 00:41:10,390
analytic test that's running and so

00:41:08,140 --> 00:41:12,550
metronome at or Chronos are very good at

00:41:10,390 --> 00:41:13,960
doing that type of thing all right I

00:41:12,550 --> 00:41:15,520
think that's all the time we had - hey

00:41:13,960 --> 00:41:17,230
thanks Adam I'll stick around on the

00:41:15,520 --> 00:41:21,679
hallway thank you guys

00:41:17,230 --> 00:41:21,679

YouTube URL: https://www.youtube.com/watch?v=sa4RiH1RXEA


