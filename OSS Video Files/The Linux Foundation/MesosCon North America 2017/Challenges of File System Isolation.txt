Title: Challenges of File System Isolation
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Challenges of File System Isolation - Santhosh Kumar Shanmugham, Twitter

Twitter use Apache Aurora on Apache Mesos to provide a scalable cluster that is used by the engineers to run their microservices. The platform has seen dramatic adoption due to the deploy features provided by Aurora, which is appealing to the engineers and the resource isolation features provided by Mesos, which improves the cluster utilization. At Twitter we use isolation of CPU, Memory, Disk and Network bandwidth to provide guaranteed access to resources for application running inside a container. One of the glaring exceptions of the container isolation is the lack of file system isolation. To this end we experimented with enabling Docker containers inside Twitter to provide an isolation layer between the host and the container. In this talk we will share our experiences at an attempt to using Docker at scale and the lessons we have learned along the journey.

About

Santhosh Kumar Shanmugham
Sr. Software Engineer, Twitter
I am a Software Engineer at Twitter working on Apache Aurora and Apache Mesos projects that provides Twitter's Compute Platform. Recently I have been working on enabling filesystem isolation via Docker containers at Twitter.
Captions: 
	00:00:00,000 --> 00:00:11,730
Oh guys come on in cycle down I'm gonna

00:00:03,120 --> 00:00:14,759
start cool I know you have a talk from

00:00:11,730 --> 00:00:17,850
Santosh talking about challenges of

00:00:14,759 --> 00:00:20,820
filesystem isolation at Twitter it's a

00:00:17,850 --> 00:00:22,800
pretty exciting topic so hopefully we'll

00:00:20,820 --> 00:00:27,570
have lot of lessons that we can learn

00:00:22,800 --> 00:00:29,880
for production usage oh yeah Santosh is

00:00:27,570 --> 00:00:36,930
an apache Eroica meter and engineer

00:00:29,880 --> 00:00:42,329
twitter thanks for the inflection hello

00:00:36,930 --> 00:00:45,320
everybody so during the last during the

00:00:42,329 --> 00:00:47,820
early part of the year I was working on

00:00:45,320 --> 00:00:50,370
trying to enable phone system isolation

00:00:47,820 --> 00:00:52,199
in the mix of clusters at Twitter as a

00:00:50,370 --> 00:00:56,090
way to improve our operational

00:00:52,199 --> 00:01:00,660
efficiency and in this talk I'm gonna

00:00:56,090 --> 00:01:03,660
share my experience about the challenges

00:01:00,660 --> 00:01:07,200
that we faced and the lessons that we

00:01:03,660 --> 00:01:10,409
learned along the way so first off let

00:01:07,200 --> 00:01:14,189
me tell how it all began during the

00:01:10,409 --> 00:01:17,970
beginning of the year the mixes hosts

00:01:14,189 --> 00:01:20,909
had been running CentOS file and these

00:01:17,970 --> 00:01:23,070
were going to its end of life which

00:01:20,909 --> 00:01:26,909
meant we had to upgrade the operating

00:01:23,070 --> 00:01:30,420
system on all of our hosts and move on

00:01:26,909 --> 00:01:32,369
to CentOS 7 and we had to do this with a

00:01:30,420 --> 00:01:34,920
couple of constraints to make sure that

00:01:32,369 --> 00:01:38,549
there was zero impact to any running

00:01:34,920 --> 00:01:40,619
services and we also had to do it in a

00:01:38,549 --> 00:01:43,829
short period of time less than three

00:01:40,619 --> 00:01:45,780
months and to put things into

00:01:43,829 --> 00:01:47,909
perspective as to where the challenges

00:01:45,780 --> 00:01:51,720
actually come from it's mostly due to

00:01:47,909 --> 00:01:55,259
the scale of Twitter's clusters our

00:01:51,720 --> 00:01:57,240
mesas cluster runs about tens of

00:01:55,259 --> 00:02:00,360
hundreds of thousands of containers on

00:01:57,240 --> 00:02:03,240
tens of thousands of machines belonging

00:02:00,360 --> 00:02:06,509
to thousands of different services and

00:02:03,240 --> 00:02:09,479
we all we manage all of these with a

00:02:06,509 --> 00:02:11,650
team of a handful of engineers less than

00:02:09,479 --> 00:02:15,610
00:02:11,650 --> 00:02:17,980
let me talk about how typical container

00:02:15,610 --> 00:02:20,470
infrastructure is usually set up it

00:02:17,980 --> 00:02:23,860
usually contains both a container image

00:02:20,470 --> 00:02:26,019
and a container runtime and any

00:02:23,860 --> 00:02:28,209
infrastructure that uses a container

00:02:26,019 --> 00:02:31,840
image gates file system isolation for

00:02:28,209 --> 00:02:35,530
free and there are several types of

00:02:31,840 --> 00:02:38,349
image formats there are available out in

00:02:35,530 --> 00:02:41,379
the community right now namely darker

00:02:38,349 --> 00:02:44,730
obscene and OCI on their corresponding

00:02:41,379 --> 00:02:49,450
runtimes or darker rocket and Runcie

00:02:44,730 --> 00:02:51,910
Howard so yeah using container images

00:02:49,450 --> 00:02:58,030
provides file system installation I just

00:02:51,910 --> 00:03:00,959
want to call this out the atwitter we

00:02:58,030 --> 00:03:03,670
have some peculiarities in the baby use

00:03:00,959 --> 00:03:06,040
our basis closer aren't the way we set

00:03:03,670 --> 00:03:09,790
up our messes clusters on these

00:03:06,040 --> 00:03:12,340
peculiarities are that we don't use a

00:03:09,790 --> 00:03:15,639
container runtime and sorry we don't use

00:03:12,340 --> 00:03:17,950
a container image and for the container

00:03:15,639 --> 00:03:20,950
runtime itself we use the mixes

00:03:17,950 --> 00:03:23,739
container Iser the mixes container Iser

00:03:20,950 --> 00:03:27,430
is very modular and it allows to work

00:03:23,739 --> 00:03:29,260
with or without container images since

00:03:27,430 --> 00:03:31,419
we don't use the container image we

00:03:29,260 --> 00:03:35,319
don't essentially get any file system

00:03:31,419 --> 00:03:37,870
isolation on our containers so how is

00:03:35,319 --> 00:03:42,160
life without container images at Twitter

00:03:37,870 --> 00:03:45,430
so become developers workflow involves

00:03:42,160 --> 00:03:48,760
building their binaries uploading them

00:03:45,430 --> 00:03:52,750
to our internal binary store and when he

00:03:48,760 --> 00:03:54,970
wants to launch application you fetch

00:03:52,750 --> 00:03:58,900
the binary from into the container and

00:03:54,970 --> 00:04:01,090
start running the application so since

00:03:58,900 --> 00:04:03,810
we don't have a container image where

00:04:01,090 --> 00:04:06,280
you can package shear dependencies

00:04:03,810 --> 00:04:09,519
service owners get a couple of building

00:04:06,280 --> 00:04:12,010
blocks to build on and out of the box we

00:04:09,519 --> 00:04:16,599
provide both the JVM and the Python

00:04:12,010 --> 00:04:18,190
runtimes on our resource hosts you know

00:04:16,599 --> 00:04:21,250
that it's a limited amount of choices

00:04:18,190 --> 00:04:23,409
but the combination of both Python and

00:04:21,250 --> 00:04:25,120
Java together gives a very powerful

00:04:23,409 --> 00:04:27,450
system

00:04:25,120 --> 00:04:30,850
very powerful platform very can build

00:04:27,450 --> 00:04:33,280
complex and extensive systems as a

00:04:30,850 --> 00:04:39,100
matter of fact entire Twitter it's based

00:04:33,280 --> 00:04:41,380
off of just Java Python what this

00:04:39,100 --> 00:04:43,270
results in is that the artifacts that

00:04:41,380 --> 00:04:47,200
get built for the applications are

00:04:43,270 --> 00:04:49,150
either a jar or FX and these artifacts

00:04:47,200 --> 00:04:51,580
have this nice property that they are

00:04:49,150 --> 00:04:56,830
usually self-contained in terms of

00:04:51,580 --> 00:05:00,340
dependencies so now since the platform

00:04:56,830 --> 00:05:02,220
provides both the JVM and Python box I

00:05:00,340 --> 00:05:05,020
becomes the platform owners

00:05:02,220 --> 00:05:08,979
responsibility to maintain Java and

00:05:05,020 --> 00:05:10,750
Python runtime on these machines so we

00:05:08,979 --> 00:05:15,040
can view the infrastructure at Twitter

00:05:10,750 --> 00:05:18,820
and its ownership model like this

00:05:15,040 --> 00:05:21,729
diagram anything above the line is owned

00:05:18,820 --> 00:05:25,419
by service owners and anything below is

00:05:21,729 --> 00:05:27,210
owned by the platform owners so when an

00:05:25,419 --> 00:05:31,050
application needs to be updated

00:05:27,210 --> 00:05:34,300
we used our odd to push team built

00:05:31,050 --> 00:05:37,210
artifacts such as the jars are the exes

00:05:34,300 --> 00:05:39,910
into the containers however when it

00:05:37,210 --> 00:05:42,490
comes to the hole is to vary the if the

00:05:39,910 --> 00:05:45,370
requirement to update the JVM or the

00:05:42,490 --> 00:05:47,229
Python we have a separate puppet in the

00:05:45,370 --> 00:05:49,180
puppet infrastructure which actually

00:05:47,229 --> 00:05:53,260
position pushes the libraries into the

00:05:49,180 --> 00:05:54,820
host and get stuff installed so due to

00:05:53,260 --> 00:05:57,340
the fact that we don't have any file

00:05:54,820 --> 00:06:00,389
system isolation in our clusters it can

00:05:57,340 --> 00:06:04,510
lead to bad behavior from service owners

00:06:00,389 --> 00:06:06,610
where services start depending directly

00:06:04,510 --> 00:06:09,910
on libraries that are available on the

00:06:06,610 --> 00:06:11,620
mesas hosts so the service the

00:06:09,910 --> 00:06:13,930
containers or the service is running

00:06:11,620 --> 00:06:16,780
inside the container starts leaking

00:06:13,930 --> 00:06:18,610
below the line of separation and creates

00:06:16,780 --> 00:06:21,970
coupling between the host and the

00:06:18,610 --> 00:06:24,700
container and this makes dependency

00:06:21,970 --> 00:06:28,240
management much more harder since

00:06:24,700 --> 00:06:30,789
there's no file system isolation just to

00:06:28,240 --> 00:06:35,590
give an example of what really happened

00:06:30,789 --> 00:06:38,590
we had we were rolling out

00:06:35,590 --> 00:06:41,250
defects and we had a certain service

00:06:38,590 --> 00:06:44,020
which actually dependent on a particular

00:06:41,250 --> 00:06:48,639
my sequel client that was available on a

00:06:44,020 --> 00:06:51,040
host on our mesas hosts and ruling out

00:06:48,639 --> 00:06:53,320
the fix ended up breaking this

00:06:51,040 --> 00:06:56,230
particular service so we were forced to

00:06:53,320 --> 00:07:01,570
roll back the entire cluster since we

00:06:56,230 --> 00:07:03,810
were affecting this single service so we

00:07:01,570 --> 00:07:07,000
learned some lessons from it maybe that

00:07:03,810 --> 00:07:09,729
snowflakes are dangerous that is don't

00:07:07,000 --> 00:07:12,610
have containers or any applications

00:07:09,729 --> 00:07:16,000
having unique configurations it's also

00:07:12,610 --> 00:07:20,800
really hard to test our canary any

00:07:16,000 --> 00:07:23,470
changes at platform wide scale what this

00:07:20,800 --> 00:07:26,380
means is that in order to gain more

00:07:23,470 --> 00:07:30,310
operational agility we needed a better

00:07:26,380 --> 00:07:32,950
dependency management system and what it

00:07:30,310 --> 00:07:36,880
meant is to basically break the coupling

00:07:32,950 --> 00:07:41,860
between the service and the host by

00:07:36,880 --> 00:07:47,229
enabling fan system isolation so in the

00:07:41,860 --> 00:07:50,470
end we ended up actually upgrading all

00:07:47,229 --> 00:07:54,039
of our hosts of upwards of

00:07:50,470 --> 00:07:59,110
30,000 hosts which correspond to almost

00:07:54,039 --> 00:08:02,229
99% of her fleet the remaining 1% is due

00:07:59,110 --> 00:08:04,270
to some snowflakes services which had

00:08:02,229 --> 00:08:09,220
some really tight coupling which had to

00:08:04,270 --> 00:08:12,639
be handled and special cased and we did

00:08:09,220 --> 00:08:16,600
all this with very minimal orling zero

00:08:12,639 --> 00:08:18,340
impact to any of Orion services all of

00:08:16,600 --> 00:08:21,820
this we did without even turning on

00:08:18,340 --> 00:08:25,810
filesystem isolation so why don't we

00:08:21,820 --> 00:08:27,070
turn it on but before we see why we

00:08:25,810 --> 00:08:30,729
didn't turn on let me give you some

00:08:27,070 --> 00:08:34,150
background first off what is fall system

00:08:30,729 --> 00:08:37,000
isolation right Francis of isolation is

00:08:34,150 --> 00:08:39,820
a way to create an isolated environment

00:08:37,000 --> 00:08:42,450
for each container on Linux

00:08:39,820 --> 00:08:44,210
it is achieved with the combination of

00:08:42,450 --> 00:08:49,350
chroot

00:08:44,210 --> 00:08:52,140
subtrees Giroud is just change route and

00:08:49,350 --> 00:08:54,270
it helps to create environments where

00:08:52,140 --> 00:08:57,510
the dependencies could be self-contained

00:08:54,270 --> 00:08:59,640
so here we have an example there we have

00:08:57,510 --> 00:09:03,210
a host with a couple of containers on

00:08:59,640 --> 00:09:06,060
them and as you can see the containers

00:09:03,210 --> 00:09:08,790
have their own file system subtrees and

00:09:06,060 --> 00:09:14,070
it is separated from each other and from

00:09:08,790 --> 00:09:18,330
the host so what what how does using

00:09:14,070 --> 00:09:24,270
filesystem isolation change or

00:09:18,330 --> 00:09:26,670
day-to-day workflow before without can

00:09:24,270 --> 00:09:28,710
without any file system isolation we did

00:09:26,670 --> 00:09:30,990
not have any container images that we

00:09:28,710 --> 00:09:33,060
have to fetch now that we are

00:09:30,990 --> 00:09:35,160
introducing we had to have this extra

00:09:33,060 --> 00:09:38,040
step before actually creating a

00:09:35,160 --> 00:09:39,750
container which is to fetch damage we

00:09:38,040 --> 00:09:42,630
have to fetch the image before we could

00:09:39,750 --> 00:09:46,320
create the container since the image

00:09:42,630 --> 00:09:48,930
actually contains the file system that

00:09:46,320 --> 00:09:55,350
would be unpacked and created that one

00:09:48,930 --> 00:09:58,200
gets mounted into the container so we

00:09:55,350 --> 00:10:00,900
had a couple of requirements for the

00:09:58,200 --> 00:10:03,420
solution for enabling file system

00:10:00,900 --> 00:10:06,600
isolation first off we had to ensure

00:10:03,420 --> 00:10:10,050
that our container launch time would not

00:10:06,600 --> 00:10:13,320
change and second off we had to ensure

00:10:10,050 --> 00:10:15,210
that we get full adoption we needed full

00:10:13,320 --> 00:10:19,050
adoption which is to basically lift and

00:10:15,210 --> 00:10:21,240
shift all the services into container

00:10:19,050 --> 00:10:25,350
images because we had to pull this off

00:10:21,240 --> 00:10:27,540
in a short time of three months file

00:10:25,350 --> 00:10:31,170
system isolation could be achieved in

00:10:27,540 --> 00:10:33,660
different levels first is the most

00:10:31,170 --> 00:10:35,850
obvious there's no isolation we're above

00:10:33,660 --> 00:10:39,480
the container and the whole share the

00:10:35,850 --> 00:10:42,900
file system and at the other end we have

00:10:39,480 --> 00:10:44,430
full isolation where there is no both

00:10:42,900 --> 00:10:47,460
the host and the container have separate

00:10:44,430 --> 00:10:49,530
file system there is a middle ground

00:10:47,460 --> 00:10:53,000
where there could be partial sharing of

00:10:49,530 --> 00:10:55,400
any some parts of the filesystem subtree

00:10:53,000 --> 00:11:00,230
that is shared between the hosts

00:10:55,400 --> 00:11:02,480
container that's we're ignoring that

00:11:00,230 --> 00:11:04,730
since we are to lift and shipped

00:11:02,480 --> 00:11:06,770
services we have to package all the

00:11:04,730 --> 00:11:08,810
dependencies that are currently

00:11:06,770 --> 00:11:11,810
available on the mesas horse into the

00:11:08,810 --> 00:11:14,030
image and this meant we had to get full

00:11:11,810 --> 00:11:16,490
file system isolation to ensure that we

00:11:14,030 --> 00:11:20,930
completely decoupled the hosts from the

00:11:16,490 --> 00:11:26,630
services at this point it's worth noting

00:11:20,930 --> 00:11:28,160
that the way containers work today when

00:11:26,630 --> 00:11:30,410
a container actually executes the

00:11:28,160 --> 00:11:33,290
container runtime depends on the host

00:11:30,410 --> 00:11:35,900
kernel for execution why is this

00:11:33,290 --> 00:11:40,040
important it's because if we even if

00:11:35,900 --> 00:11:42,920
container file system image has specific

00:11:40,040 --> 00:11:44,870
kernel patches these would not be in

00:11:42,920 --> 00:11:49,490
effect when the container is actually

00:11:44,870 --> 00:11:52,700
executing so next let's look at some of

00:11:49,490 --> 00:11:55,430
the technical details of the choices

00:11:52,700 --> 00:12:00,230
that we made for enabling full full

00:11:55,430 --> 00:12:02,570
isolation since we had containers that

00:12:00,230 --> 00:12:06,020
were reading into the host and had

00:12:02,570 --> 00:12:08,690
coupling with the host itself we had to

00:12:06,020 --> 00:12:10,190
package all the dependencies all the

00:12:08,690 --> 00:12:12,650
possible dependencies that were

00:12:10,190 --> 00:12:14,990
available on the host and create an

00:12:12,650 --> 00:12:17,270
image that would satisfy every service

00:12:14,990 --> 00:12:21,200
this meant that we had to create a

00:12:17,270 --> 00:12:26,410
really big and fat and this image

00:12:21,200 --> 00:12:29,810
turned out to be close to 5 gigs in size

00:12:26,410 --> 00:12:33,440
now looking back at our requirement that

00:12:29,810 --> 00:12:37,430
we do not change any container launch

00:12:33,440 --> 00:12:41,120
time since we have this extra extra step

00:12:37,430 --> 00:12:42,800
to actually fetch the image we had to

00:12:41,120 --> 00:12:46,280
look at the options that mace was

00:12:42,800 --> 00:12:48,620
provided for fetching the image my

00:12:46,280 --> 00:12:50,900
association provides a couple of ways of

00:12:48,620 --> 00:12:53,480
fetching the image before launching a

00:12:50,900 --> 00:12:56,540
container first one is the registry

00:12:53,480 --> 00:13:00,980
puller which can talk to any container

00:12:56,540 --> 00:13:04,490
registry and fetch the image one example

00:13:00,980 --> 00:13:05,810
would be the docker registry and the

00:13:04,490 --> 00:13:09,320
other and we have

00:13:05,810 --> 00:13:10,430
local blur which would just copy the

00:13:09,320 --> 00:13:13,420
container image are reading the

00:13:10,430 --> 00:13:17,450
container image from the local host

00:13:13,420 --> 00:13:22,040
itself using the registry meant like we

00:13:17,450 --> 00:13:26,450
had to have an extra step that is

00:13:22,040 --> 00:13:29,240
fetching the five gig image on demand

00:13:26,450 --> 00:13:31,460
when the containers being launched and

00:13:29,240 --> 00:13:34,850
this will definitely affect our

00:13:31,460 --> 00:13:36,950
container launch time so we had to make

00:13:34,850 --> 00:13:39,290
sure that we had the image already on

00:13:36,950 --> 00:13:41,690
the host before we started launching it

00:13:39,290 --> 00:13:45,170
and the local Buller was the option that

00:13:41,690 --> 00:13:47,120
best suited us so we had of me too

00:13:45,170 --> 00:13:49,940
really needed a way to prefetch the

00:13:47,120 --> 00:13:51,350
image since we had to prefetch the image

00:13:49,940 --> 00:13:54,529
we needed to now think about

00:13:51,350 --> 00:13:58,100
distributing this image to upwards of

00:13:54,529 --> 00:14:01,850
30,000 horse and this image was close to

00:13:58,100 --> 00:14:03,860
five gigs in size so we looked at the

00:14:01,850 --> 00:14:05,779
first option which is our default which

00:14:03,860 --> 00:14:09,680
is our the binary store that actually

00:14:05,779 --> 00:14:14,570
hosts the binaries that get created as

00:14:09,680 --> 00:14:19,790
part of the CI it was not designed for

00:14:14,570 --> 00:14:22,310
pushing gigs of data to tens of

00:14:19,790 --> 00:14:24,140
thousands of nodes however it would work

00:14:22,310 --> 00:14:25,940
for which is pushing application

00:14:24,140 --> 00:14:29,089
binaries to thousands of containers and

00:14:25,940 --> 00:14:32,930
it reaches work fine so next we looked

00:14:29,089 --> 00:14:36,710
at CERN VMFS which is a distributed file

00:14:32,930 --> 00:14:40,370
system project from CERN labs in

00:14:36,710 --> 00:14:42,710
Switzerland this would have the scale

00:14:40,370 --> 00:14:46,540
but it did not have the ability to

00:14:42,710 --> 00:14:49,300
prefetch it was heavily cached

00:14:46,540 --> 00:14:52,190
distributions a distribution service

00:14:49,300 --> 00:14:54,650
which meant like the file is actually

00:14:52,190 --> 00:14:58,160
fetched on demand when you try to access

00:14:54,650 --> 00:15:00,440
it this had the implication that the

00:14:58,160 --> 00:15:03,890
container execution would have

00:15:00,440 --> 00:15:05,900
unpredictable performance since fetching

00:15:03,890 --> 00:15:08,930
the file on demand

00:15:05,900 --> 00:15:12,020
it was totally reliant on the network's

00:15:08,930 --> 00:15:14,540
performance at that point the third

00:15:12,020 --> 00:15:17,840
option and the obvious one is to use the

00:15:14,540 --> 00:15:20,480
appropriate registry for example

00:15:17,840 --> 00:15:23,720
we big darker image so docker registry

00:15:20,480 --> 00:15:26,780
could have been used but it did not

00:15:23,720 --> 00:15:30,380
allow us to prefetch so we have this

00:15:26,780 --> 00:15:33,860
same problem that we could we had to

00:15:30,380 --> 00:15:36,320
elongate the container launch time so we

00:15:33,860 --> 00:15:37,940
ended up rolling our own distribution

00:15:36,320 --> 00:15:41,690
mechanism based on the BitTorrent

00:15:37,940 --> 00:15:43,730
protocol and remain slight changes to

00:15:41,690 --> 00:15:46,640
the protocol to make sure that it was

00:15:43,730 --> 00:15:50,720
more efficient to avoid unnecessary up

00:15:46,640 --> 00:15:53,720
frac traffic so the protocol essentially

00:15:50,720 --> 00:15:58,150
prefers to fetch pieces of files that

00:15:53,720 --> 00:15:58,150
are present locally in the same rack

00:15:58,330 --> 00:16:04,520
it's worth calling out now that if we

00:16:01,760 --> 00:16:06,800
had had a object for a block store I

00:16:04,520 --> 00:16:10,760
could have totally worked and we would

00:16:06,800 --> 00:16:14,060
have used it since our since Twitter

00:16:10,760 --> 00:16:15,220
uses a private cloud we don't have those

00:16:14,060 --> 00:16:17,930
options

00:16:15,220 --> 00:16:20,960
so we built a mid torrent based

00:16:17,930 --> 00:16:23,800
distribution system and the system's

00:16:20,960 --> 00:16:27,020
installation looks roughly like this

00:16:23,800 --> 00:16:30,980
where the binary store hosted the actual

00:16:27,020 --> 00:16:33,400
built filesystem image and we had a

00:16:30,980 --> 00:16:35,950
layer of peers called as the cedars

00:16:33,400 --> 00:16:38,870
which you are responsible for

00:16:35,950 --> 00:16:42,470
downloading this binary downloading the

00:16:38,870 --> 00:16:44,960
image and making it available for the

00:16:42,470 --> 00:16:47,780
maysa agents to download so it makes us

00:16:44,960 --> 00:16:50,270
host to download this was necessary

00:16:47,780 --> 00:16:53,830
since the binary store did not have

00:16:50,270 --> 00:16:57,220
enough throughput to match the

00:16:53,830 --> 00:17:00,380
requirements of all the high number of

00:16:57,220 --> 00:17:03,350
basis agents that we have mesas hosts

00:17:00,380 --> 00:17:05,510
that we have so the last layer is the

00:17:03,350 --> 00:17:08,320
leecher which are peers that essentially

00:17:05,510 --> 00:17:11,690
talk to the cedars and as you can see

00:17:08,320 --> 00:17:14,390
the modified BitTorrent protocol prefers

00:17:11,690 --> 00:17:18,650
fetching from a cedar that is in the

00:17:14,390 --> 00:17:21,440
local rack and if there is no such cedar

00:17:18,650 --> 00:17:26,390
in the same rack it then it then goes

00:17:21,440 --> 00:17:29,330
over the rack so we built it and then we

00:17:26,390 --> 00:17:33,019
tried enabling and started distributing

00:17:29,330 --> 00:17:36,980
our first image and we had a bad

00:17:33,019 --> 00:17:39,230
situation so what happened was the Turin

00:17:36,980 --> 00:17:42,159
traffic ended up forming the host Knicks

00:17:39,230 --> 00:17:45,409
and I blocked all the traffic that were

00:17:42,159 --> 00:17:47,750
moving out of the host which also

00:17:45,409 --> 00:17:51,769
blocked the heartbeats from the agent to

00:17:47,750 --> 00:17:55,850
the master which led to lost leaves and

00:17:51,769 --> 00:18:00,710
moss tasks so we had to restrict the

00:17:55,850 --> 00:18:02,000
resource usage of BitTorrent peers so

00:18:00,710 --> 00:18:04,039
some of the challenges that we faced

00:18:02,000 --> 00:18:07,700
here when we actually tried to isolate

00:18:04,039 --> 00:18:09,139
it firstly isolating the Cedars fare

00:18:07,700 --> 00:18:10,700
really easy because these fare

00:18:09,139 --> 00:18:13,250
essentially our aura jobs they were

00:18:10,700 --> 00:18:15,039
inside containers and the container

00:18:13,250 --> 00:18:17,600
isolation automatically took care of

00:18:15,039 --> 00:18:20,779
restricting restricting the resource

00:18:17,600 --> 00:18:22,970
usage how we're on the other hand the

00:18:20,779 --> 00:18:26,299
leaders were not running inside a

00:18:22,970 --> 00:18:30,559
container and we had to do this because

00:18:26,299 --> 00:18:32,929
the leechers had to have access are like

00:18:30,559 --> 00:18:36,799
root privileges to access the maysa

00:18:32,929 --> 00:18:39,260
agents image cache so the lee-char ended

00:18:36,799 --> 00:18:40,880
up being demon running alongside the

00:18:39,260 --> 00:18:45,279
main source agent on every mace was a

00:18:40,880 --> 00:18:49,970
host so we had to manually isolate

00:18:45,279 --> 00:18:53,059
resource usage for features isolating

00:18:49,970 --> 00:18:55,669
CPU memory and disk was easy it was just

00:18:53,059 --> 00:18:57,769
setting up appropriate C groups however

00:18:55,669 --> 00:19:00,230
it was not the same case when it came to

00:18:57,769 --> 00:19:02,620
isolating the network this is because

00:19:00,230 --> 00:19:04,220
and Twitter reviews a non-standard

00:19:02,620 --> 00:19:06,019
network isolator

00:19:04,220 --> 00:19:09,559
called as the port mapping isolator that

00:19:06,019 --> 00:19:12,190
is present in May sauce which is

00:19:09,559 --> 00:19:16,220
complicated and makes it harder for

00:19:12,190 --> 00:19:18,799
changing it so let me talk a little bit

00:19:16,220 --> 00:19:22,309
to explain how the port mapping isolator

00:19:18,799 --> 00:19:25,370
actually works a pod mapping isolator

00:19:22,309 --> 00:19:27,740
tries to divide the range of ports that

00:19:25,370 --> 00:19:33,080
are available on hosts and assign them

00:19:27,740 --> 00:19:36,260
to containers each container then gets

00:19:33,080 --> 00:19:40,730
its own network namespace and it also

00:19:36,260 --> 00:19:41,549
has a virtual Ethernet pair where one

00:19:40,730 --> 00:19:43,799
end of the pair

00:19:41,549 --> 00:19:46,980
pushed into the hedgerow game space and

00:19:43,799 --> 00:19:50,070
the other end stays in the host once we

00:19:46,980 --> 00:19:51,929
have done this appropriate routing from

00:19:50,070 --> 00:19:54,149
the host Internet to the virtual

00:19:51,929 --> 00:19:56,220
Ethernet for the container that's

00:19:54,149 --> 00:19:59,100
present in the host is created so that

00:19:56,220 --> 00:20:01,559
any traffic that is this time for a

00:19:59,100 --> 00:20:06,509
particular port range is routed to that

00:20:01,559 --> 00:20:10,200
container and vice versa and it's worth

00:20:06,509 --> 00:20:13,100
noting that we have we also have a rate

00:20:10,200 --> 00:20:15,539
limit or a hierarchical token bucket

00:20:13,100 --> 00:20:19,470
regulator that's installed on the

00:20:15,539 --> 00:20:24,059
containers virtual Ethernet to limit any

00:20:19,470 --> 00:20:27,600
us traffic this way make sure that

00:20:24,059 --> 00:20:32,970
containers have isolated network access

00:20:27,600 --> 00:20:35,700
and we do not have starvation now we had

00:20:32,970 --> 00:20:38,369
to add that Christ elation for a leecher

00:20:35,700 --> 00:20:41,850
which meant we had to do the same or

00:20:38,369 --> 00:20:45,029
similar kind of setup for the leecher by

00:20:41,850 --> 00:20:47,519
hand which meant we had to create its

00:20:45,029 --> 00:20:51,029
own network namespace have a virtual

00:20:47,519 --> 00:20:54,299
Ethernet there and it's all its own

00:20:51,029 --> 00:20:58,379
shibi rates of it the point worth noting

00:20:54,299 --> 00:21:01,759
here is that we have an extra TC filter

00:20:58,379 --> 00:21:04,739
that we install on the virtual Ethernet

00:21:01,759 --> 00:21:07,619
for the bleachers container that's

00:21:04,739 --> 00:21:11,519
present in the host which is the TC

00:21:07,619 --> 00:21:14,249
police filter and we do this to limit

00:21:11,519 --> 00:21:16,169
inverse traffic as well the reason we

00:21:14,249 --> 00:21:20,749
had to do this is because we could have

00:21:16,169 --> 00:21:23,999
a misbehaving Turin peer which could

00:21:20,749 --> 00:21:25,679
start flooding which we start sending

00:21:23,999 --> 00:21:28,980
traffic to the leecher

00:21:25,679 --> 00:21:32,070
which could end up flooding we Ethernet

00:21:28,980 --> 00:21:34,049
and Ethernet on the host and browning

00:21:32,070 --> 00:21:36,720
out the host it all together which would

00:21:34,049 --> 00:21:38,820
affect rest of the containers and we

00:21:36,720 --> 00:21:42,359
don't do this on our production

00:21:38,820 --> 00:21:48,239
containers since those are tier 1 and

00:21:42,359 --> 00:21:51,119
tier 0 services because TC police is

00:21:48,239 --> 00:21:54,659
very aggressive and tries to drop

00:21:51,119 --> 00:21:57,629
packages and forces tcp to

00:21:54,659 --> 00:22:02,309
reduce its window size doing this for

00:21:57,629 --> 00:22:03,779
this tier 2 service which is only used

00:22:02,309 --> 00:22:06,629
for distributing the images seemed

00:22:03,779 --> 00:22:08,789
appropriate sent me one since we wanted

00:22:06,629 --> 00:22:12,210
to minimize any effect to running

00:22:08,789 --> 00:22:14,450
production containers so we had a

00:22:12,210 --> 00:22:17,099
working distribution mechanism and

00:22:14,450 --> 00:22:18,529
before we could take it to production we

00:22:17,099 --> 00:22:20,970
wanted to have a good story around

00:22:18,529 --> 00:22:24,239
versioning these images that we were

00:22:20,970 --> 00:22:27,239
using so we had a couple of we looked at

00:22:24,239 --> 00:22:29,759
it and we came up with a couple of

00:22:27,239 --> 00:22:32,399
requirements first off we had to have

00:22:29,759 --> 00:22:34,349
full adoption which meant like we had to

00:22:32,399 --> 00:22:38,759
use the big and fat image that we

00:22:34,349 --> 00:22:42,029
created before and we also had to make

00:22:38,759 --> 00:22:44,129
sure that he had multiple versions we we

00:22:42,029 --> 00:22:47,009
could maintain multiple versions of this

00:22:44,129 --> 00:22:50,759
image in case we ever run into a

00:22:47,009 --> 00:22:54,269
vulnerability so we needed to have

00:22:50,759 --> 00:22:56,190
multiple versions since it usually takes

00:22:54,269 --> 00:22:58,470
service owners longer to update their

00:22:56,190 --> 00:23:01,470
services and we had to make sure that we

00:22:58,470 --> 00:23:05,970
don't break running services so the

00:23:01,470 --> 00:23:10,409
challenges in this part we're quite bad

00:23:05,970 --> 00:23:12,869
in that we had certain hardware profiles

00:23:10,409 --> 00:23:15,059
or hardware specification that were not

00:23:12,869 --> 00:23:18,090
really designed for this use case and

00:23:15,059 --> 00:23:22,009
and this turned out to be a significant

00:23:18,090 --> 00:23:24,029
portion of our fleet which was

00:23:22,009 --> 00:23:27,330
practically old hardware that we were

00:23:24,029 --> 00:23:30,509
still continuing to run what it meant is

00:23:27,330 --> 00:23:32,789
like these holes had barely 100 gig

00:23:30,509 --> 00:23:37,049
disks and even after carving out like

00:23:32,789 --> 00:23:39,059
10% of these disks away from container

00:23:37,049 --> 00:23:43,259
usage we could only support a bare

00:23:39,059 --> 00:23:45,779
minimum of two versions so we tried a

00:23:43,259 --> 00:23:50,340
bunch of things to try to reduce the

00:23:45,779 --> 00:23:54,599
image size and so we had picked darker

00:23:50,340 --> 00:23:57,299
image as our image format and generally

00:23:54,599 --> 00:24:00,720
most of the container image formats out

00:23:57,299 --> 00:24:02,849
there are support layering and these

00:24:00,720 --> 00:24:03,670
layers are content addressable as well

00:24:02,849 --> 00:24:06,490
so

00:24:03,670 --> 00:24:08,770
it was possible to D do players that

00:24:06,490 --> 00:24:12,460
were share across different images so

00:24:08,770 --> 00:24:15,250
you looked at it however it turned out

00:24:12,460 --> 00:24:18,010
that due to the very darker image

00:24:15,250 --> 00:24:21,400
creation works a little darker image is

00:24:18,010 --> 00:24:23,740
layering works darker image tended to

00:24:21,400 --> 00:24:26,440
explode in size due to the presence of

00:24:23,740 --> 00:24:30,550
whiteout files these are files that get

00:24:26,440 --> 00:24:33,580
created when a new file gets installed

00:24:30,550 --> 00:24:35,950
on to the file system and immediately in

00:24:33,580 --> 00:24:37,870
the next layer it gets deleted so

00:24:35,950 --> 00:24:41,950
essentially we could have multiple

00:24:37,870 --> 00:24:44,350
versions for the same file in the

00:24:41,950 --> 00:24:46,720
container image but when we actually

00:24:44,350 --> 00:24:49,810
unpack the image and recreate the file

00:24:46,720 --> 00:24:51,760
system it may not even God be present in

00:24:49,810 --> 00:24:55,780
the eventual file system that gets

00:24:51,760 --> 00:24:58,780
created and we also found that the

00:24:55,780 --> 00:25:03,820
darker file format and the command

00:24:58,780 --> 00:25:05,410
sequence that we had to use to make sure

00:25:03,820 --> 00:25:08,050
that we got the maximum amount of

00:25:05,410 --> 00:25:12,730
deduplication turned out to be quite

00:25:08,050 --> 00:25:15,210
tricky so we had to tweak the docker

00:25:12,730 --> 00:25:18,070
file quite a bit to get the correct

00:25:15,210 --> 00:25:20,980
deduplication that we want it so this

00:25:18,070 --> 00:25:25,720
led to a really unpredictable process

00:25:20,980 --> 00:25:27,010
which was not favorable so it since you

00:25:25,720 --> 00:25:29,710
take away is that

00:25:27,010 --> 00:25:32,110
phoo file system isolation at scale is

00:25:29,710 --> 00:25:36,910
really hard just because of the scale

00:25:32,110 --> 00:25:38,590
part so going back to the question how

00:25:36,910 --> 00:25:42,220
did we actually pull out the upgrade

00:25:38,590 --> 00:25:46,690
turns out our peculiar infrastructure

00:25:42,220 --> 00:25:50,350
actually had certain advantages and it

00:25:46,690 --> 00:25:52,480
comes out as these two points where the

00:25:50,350 --> 00:25:54,910
chars and exes which were used for our

00:25:52,480 --> 00:25:56,730
artifact formats student for the

00:25:54,910 --> 00:25:59,920
isolation since these were

00:25:56,730 --> 00:26:03,540
self-contained in terms of dependencies

00:25:59,920 --> 00:26:05,890
and since we had very opinionated

00:26:03,540 --> 00:26:08,290
infrastructure which limited the number

00:26:05,890 --> 00:26:11,620
of options that we actually provided to

00:26:08,290 --> 00:26:13,059
our customers we thereby limited the

00:26:11,620 --> 00:26:15,940
coupling that could be

00:26:13,059 --> 00:26:20,200
happening in between services and the

00:26:15,940 --> 00:26:24,129
host which meant like even after even if

00:26:20,200 --> 00:26:28,480
a Python library had native bindings to

00:26:24,129 --> 00:26:30,700
a particular host OS if we just saw all

00:26:28,480 --> 00:26:32,679
the fix the problem for one service we

00:26:30,700 --> 00:26:38,019
could we essentially fix the problem for

00:26:32,679 --> 00:26:41,019
rest of the fleet as well so on the

00:26:38,019 --> 00:26:43,120
whole now comparing the different styles

00:26:41,019 --> 00:26:49,840
of different levels of filesystem

00:26:43,120 --> 00:26:51,970
isolation we found that we compared all

00:26:49,840 --> 00:26:56,080
along different dimensions namely the

00:26:51,970 --> 00:26:57,309
container launch delay the time at the

00:26:56,080 --> 00:27:00,820
extra time that would be needed to

00:26:57,309 --> 00:27:02,830
launch our containers in which case if

00:27:00,820 --> 00:27:05,740
you don't have any image at all that

00:27:02,830 --> 00:27:08,820
would be the best case there we look at

00:27:05,740 --> 00:27:11,230
service debugging where we get the same

00:27:08,820 --> 00:27:13,330
filesystem environment that could be

00:27:11,230 --> 00:27:15,580
used that would be used in production

00:27:13,330 --> 00:27:19,299
and could be replicated in the

00:27:15,580 --> 00:27:22,929
development environment service agility

00:27:19,299 --> 00:27:25,629
and operational agility would be really

00:27:22,929 --> 00:27:28,600
high if there is full isolation over

00:27:25,629 --> 00:27:30,779
having just partial isolation just makes

00:27:28,600 --> 00:27:33,460
it work

00:27:30,779 --> 00:27:38,259
I've been we had vulnerability updates

00:27:33,460 --> 00:27:41,289
and we had role of fixes isolation did

00:27:38,259 --> 00:27:43,720
if having no isolation meant that you

00:27:41,289 --> 00:27:46,509
could roll out of fix to all service at

00:27:43,720 --> 00:27:48,639
once but it had the other problems of

00:27:46,509 --> 00:27:52,149
not being able to properly canary

00:27:48,639 --> 00:27:55,779
changes when it came to the number of

00:27:52,149 --> 00:27:57,399
images that people could have if be

00:27:55,779 --> 00:28:02,049
virtual of service owners to create

00:27:57,399 --> 00:28:05,139
their own images if he had chosen full

00:28:02,049 --> 00:28:07,299
isolation we would hand up with huge

00:28:05,139 --> 00:28:09,340
images and essentially limiting the

00:28:07,299 --> 00:28:11,769
number of images that the system can

00:28:09,340 --> 00:28:14,639
support at any point in time so in this

00:28:11,769 --> 00:28:17,139
case a partial isolated

00:28:14,639 --> 00:28:21,990
application-specific image would have

00:28:17,139 --> 00:28:24,519
been more appropriate and lastly we have

00:28:21,990 --> 00:28:25,960
service service owners to own their own

00:28:24,519 --> 00:28:29,080
dependencies

00:28:25,960 --> 00:28:29,559
case full isolation is the best person

00:28:29,080 --> 00:28:31,630
are you

00:28:29,559 --> 00:28:34,990
however it comes with the drawback of

00:28:31,630 --> 00:28:36,399
being loaded in terms of image size so

00:28:34,990 --> 00:28:38,409
even here

00:28:36,399 --> 00:28:41,350
a partial isolation seems to be the

00:28:38,409 --> 00:28:44,260
middle ground so the key takeaway that I

00:28:41,350 --> 00:28:46,299
want to enforce is that application

00:28:44,260 --> 00:28:51,429
level filesystem isolation has the

00:28:46,299 --> 00:29:17,500
better of all worlds and full isolation

00:28:51,429 --> 00:29:20,230
seems to be the honest I was wondering

00:29:17,500 --> 00:29:22,419
how many container base images are you

00:29:20,230 --> 00:29:23,770
are you deploying because it sounds like

00:29:22,419 --> 00:29:27,460
you put quite a lot of effort into

00:29:23,770 --> 00:29:31,090
prefetching do you change those images a

00:29:27,460 --> 00:29:33,039
lot or you're just playing for the

00:29:31,090 --> 00:29:34,960
future because if you have one image the

00:29:33,039 --> 00:29:37,210
prefetch impact perhaps matters less

00:29:34,960 --> 00:29:38,770
because once you've got one one task

00:29:37,210 --> 00:29:40,000
running on each machine it's it's there

00:29:38,770 --> 00:29:44,320
and it's gonna stay there for quite a

00:29:40,000 --> 00:29:46,980
long time so we actually like I

00:29:44,320 --> 00:29:50,289
mentioned we provide both the JVM and

00:29:46,980 --> 00:29:53,890
Python runtimes directly out of the box

00:29:50,289 --> 00:29:56,740
so we had to maintain it and we do how

00:29:53,890 --> 00:29:59,049
multiple versions of JVM that we provide

00:29:56,740 --> 00:30:00,640
at any point in time because the JVM is

00:29:59,049 --> 00:30:02,500
also owned by a separate team and they

00:30:00,640 --> 00:30:06,340
have kinetic versions and other versions

00:30:02,500 --> 00:30:08,529
so similarly for the case for Python as

00:30:06,340 --> 00:30:10,299
well so we have multiple layers of these

00:30:08,529 --> 00:30:12,940
things are multiple versions of these

00:30:10,299 --> 00:30:15,240
libraries which essentially load sub

00:30:12,940 --> 00:30:15,240
sandwich

00:30:20,899 --> 00:30:32,399
any other questions I'm just curious how

00:30:29,190 --> 00:30:38,100
long does it take to to prefetch an

00:30:32,399 --> 00:30:39,510
image so we were able to push a 5 gig

00:30:38,100 --> 00:30:43,350
image

00:30:39,510 --> 00:30:45,090
listen to ours to two hours and to the

00:30:43,350 --> 00:30:48,080
entire cluster to the entire cluster

00:30:45,090 --> 00:30:48,080
okay thanks

00:30:53,429 --> 00:30:59,789
one thing I noticed that I didn't see

00:30:57,409 --> 00:31:02,880
mentioned maybe I missed it I'm

00:30:59,789 --> 00:31:05,309
wondering about is with this partial

00:31:02,880 --> 00:31:08,880
isolation approach that you were

00:31:05,309 --> 00:31:15,960
favoring I think if I were ready

00:31:08,880 --> 00:31:18,659
abruptly do you have some other means

00:31:15,960 --> 00:31:22,049
you're peculiar professor for managing

00:31:18,659 --> 00:31:24,750
like subversion of one micro service on

00:31:22,049 --> 00:31:26,760
a box letting you bleep others

00:31:24,750 --> 00:31:29,760
with a partial isolation thing more

00:31:26,760 --> 00:31:32,669
easily or is that not as a concern for

00:31:29,760 --> 00:31:36,860
the reasons really I didn't understand

00:31:32,669 --> 00:31:39,539
the second part the filesystem isolation

00:31:36,860 --> 00:31:40,860
one of reasons that like my team's been

00:31:39,539 --> 00:31:43,010
investigating file system isolation as

00:31:40,860 --> 00:31:47,610
part of a cohesive security boundary

00:31:43,010 --> 00:31:50,070
between multi-tenant applications owned

00:31:47,610 --> 00:31:51,659
by different service owners that might

00:31:50,070 --> 00:31:53,850
have different security stances because

00:31:51,659 --> 00:31:56,340
of their target audience or maturity or

00:31:53,850 --> 00:31:59,730
whatever and so like how do you guys

00:31:56,340 --> 00:32:01,980
handle that conflict yeah personal

00:31:59,730 --> 00:32:03,929
isolation thing with the partially so

00:32:01,980 --> 00:32:06,809
with the partial isolation you are

00:32:03,929 --> 00:32:09,210
expected to it's very similar to

00:32:06,809 --> 00:32:10,919
application containers there anything

00:32:09,210 --> 00:32:13,230
that's specific to the application gets

00:32:10,919 --> 00:32:15,179
bundled into the container image and

00:32:13,230 --> 00:32:19,140
anything that's provided by the platform

00:32:15,179 --> 00:32:22,440
is then mounted are like shared or laid

00:32:19,140 --> 00:32:23,700
on top of this so it's a completely up

00:32:22,440 --> 00:32:25,350
to the service owners to package

00:32:23,700 --> 00:32:27,149
whichever way they want and they would

00:32:25,350 --> 00:32:29,779
have to deal with it it's not depend on

00:32:27,149 --> 00:32:29,779
the platform install

00:32:31,560 --> 00:32:35,040
any other questions

00:32:41,610 --> 00:32:51,089
okay cool thank you thank Santos

00:32:45,580 --> 00:32:51,089

YouTube URL: https://www.youtube.com/watch?v=JQNw6Utts-c


