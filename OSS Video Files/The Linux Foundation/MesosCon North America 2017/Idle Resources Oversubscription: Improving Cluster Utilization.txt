Title: Idle Resources Oversubscription: Improving Cluster Utilization
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	Idle Resources Oversubscription: Improving Cluster Utilization - Dmitry Zhuk, Twitter

In this talk we will focus on effective Mesos cluster utilization for non-production jobs, which are commonly used for development and testing. While many of such jobs are idle, they consume cluster resources, leaving a huge gap between allocated and actually utilized resources.
We will present our approach for detecting idle resources, offering them to other non-production jobs using Mesos oversubscription model, and isolating non-production jobs from production jobs. We will also highlight issues in Mesosâ€™ approach to oversubscription and discuss possible solutions and workarounds.

About 

Dmitry Zhuk
Software Engineer, Twitter
Dmitry Zhuk is Software Engineer at Twitter, working in Mesos/Aurora Team.
Captions: 
	00:00:00,650 --> 00:00:08,849
all right let's start last for the day

00:00:04,850 --> 00:00:11,219
we have a talk from Dmitry senior

00:00:08,849 --> 00:00:15,420
engineer at Twitter it's gonna talk

00:00:11,219 --> 00:00:18,630
about our subscription and now the how

00:00:15,420 --> 00:00:23,960
they did it in a production quality via

00:00:18,630 --> 00:00:27,359
Twitter so let's start thank you

00:00:23,960 --> 00:00:29,880
so I work at Twitter compute platform

00:00:27,359 --> 00:00:32,399
team who is basically responsible for

00:00:29,880 --> 00:00:37,380
running Aurora and mazes on Twitter

00:00:32,399 --> 00:00:39,899
requesters I'm going to introduce our

00:00:37,380 --> 00:00:44,760
solution to idle resources of a

00:00:39,899 --> 00:00:46,110
subscription in which we which we used

00:00:44,760 --> 00:00:48,809
to try to improve our cluster

00:00:46,110 --> 00:00:51,180
utilization this is still

00:00:48,809 --> 00:00:53,640
work-in-progress it it's not deployed in

00:00:51,180 --> 00:00:54,989
production but we thought that this

00:00:53,640 --> 00:00:58,890
would be interesting to the community

00:00:54,989 --> 00:01:04,680
that we share our challenges and the

00:00:58,890 --> 00:01:08,640
solutions so a little bit of background

00:01:04,680 --> 00:01:12,510
on Twitter clusters we have a cluster of

00:01:08,640 --> 00:01:15,960
more than 30,000 notes that's a really

00:01:12,510 --> 00:01:18,000
huge cluster and it basically runs two

00:01:15,960 --> 00:01:22,170
types of jobs that's production and long

00:01:18,000 --> 00:01:25,170
production jobs most of jobs are of

00:01:22,170 --> 00:01:28,229
course production and they have about 80

00:01:25,170 --> 00:01:30,750
percent of allocated CPUs while non

00:01:28,229 --> 00:01:35,640
production jobs at the moment are about

00:01:30,750 --> 00:01:37,560
10% and cluster and p95 of CP

00:01:35,640 --> 00:01:41,009
utilization is also different for those

00:01:37,560 --> 00:01:43,350
for those jobs our productions jobs are

00:01:41,009 --> 00:01:49,409
more effective like utilizing more than

00:01:43,350 --> 00:01:51,659
30% and non production just 20 non

00:01:49,409 --> 00:01:56,610
production jobs and production jobs have

00:01:51,659 --> 00:01:58,439
different delays for we we are more

00:01:56,610 --> 00:02:03,960
flexible about non production jobs like

00:01:58,439 --> 00:02:07,649
we can preamp them and do some other

00:02:03,960 --> 00:02:11,390
things while production jobs are more

00:02:07,649 --> 00:02:16,160
critical so we treat them

00:02:11,390 --> 00:02:18,640
in the proper way so we're focusing here

00:02:16,160 --> 00:02:23,450
on non production jobs we want

00:02:18,640 --> 00:02:27,910
oversubscribe production jobs so

00:02:23,450 --> 00:02:31,490
although it's just 10% of CPUs allocated

00:02:27,910 --> 00:02:35,000
well given the cluster size even if we

00:02:31,490 --> 00:02:41,390
managed to get a few percents from that

00:02:35,000 --> 00:02:46,310
number it's really lots of hosts our

00:02:41,390 --> 00:02:48,680
most limited resources CPUs we're not so

00:02:46,310 --> 00:02:56,990
much bound by a network or disk space or

00:02:48,680 --> 00:02:58,490
memory and the problem is that both non

00:02:56,990 --> 00:03:01,190
production and production and resource

00:02:58,490 --> 00:03:03,350
allocation is growing so in order to

00:03:01,190 --> 00:03:06,170
keep up with these groves we growth

00:03:03,350 --> 00:03:08,780
growth we either have to add new

00:03:06,170 --> 00:03:10,240
machines to the cluster or try to find

00:03:08,780 --> 00:03:14,510
some other solutions like

00:03:10,240 --> 00:03:15,440
oversubscription we also have an observe

00:03:14,510 --> 00:03:17,510
a Shinto

00:03:15,440 --> 00:03:21,530
many non production jobs are actually

00:03:17,510 --> 00:03:27,530
idle like CPU utilization is really

00:03:21,530 --> 00:03:29,120
really low somewhere near zero so what

00:03:27,530 --> 00:03:33,080
we did we collected some statistics

00:03:29,120 --> 00:03:35,300
about the jobs non production jobs and

00:03:33,080 --> 00:03:38,870
found that many of the jobs are not

00:03:35,300 --> 00:03:40,760
using CPU and there are some jobs which

00:03:38,870 --> 00:03:44,900
have occasional bursts in CPU

00:03:40,760 --> 00:03:46,459
utilization but basically almost any of

00:03:44,900 --> 00:03:49,820
those no conduction jobs to die at all

00:03:46,459 --> 00:03:53,239
this kind of jobs is development testing

00:03:49,820 --> 00:03:57,370
or some batch processing which is not so

00:03:53,239 --> 00:04:00,739
much critical we also found some

00:03:57,370 --> 00:04:05,090
utilization metrics for all the jobs I

00:04:00,739 --> 00:04:08,299
didn't we did include short leave jobs

00:04:05,090 --> 00:04:12,350
because well we cannot win much from

00:04:08,299 --> 00:04:15,950
them we're just focusing on long live in

00:04:12,350 --> 00:04:17,959
non production jobs and that's about 80%

00:04:15,950 --> 00:04:21,970
of CPUs allocated for non production

00:04:17,959 --> 00:04:23,520
jobs here's an example of idle tasks

00:04:21,970 --> 00:04:26,550
this

00:04:23,520 --> 00:04:29,849
so this graph shows CPU utilization of

00:04:26,550 --> 00:04:33,659
some service which has four CPUs

00:04:29,849 --> 00:04:39,870
allocated but in reality CPU utilization

00:04:33,659 --> 00:04:42,990
is like less than 10% of that you can

00:04:39,870 --> 00:04:46,470
see some occasional bursts but even with

00:04:42,990 --> 00:04:51,440
this bursts it doesn't achieve the

00:04:46,470 --> 00:04:54,960
really allocated number of CPUs

00:04:51,440 --> 00:04:59,270
here's another statistics this shows

00:04:54,960 --> 00:05:01,800
cumulative distribution of road jobs and

00:04:59,270 --> 00:05:12,180
CPU utilization so basically this means

00:05:01,800 --> 00:05:15,360
that 60% of all jobs with use only 25%

00:05:12,180 --> 00:05:24,419
of CPU when we use different metrics

00:05:15,360 --> 00:05:28,580
like P be 95 here so it means that 95

00:05:24,419 --> 00:05:35,880
percent of time drop is basically using

00:05:28,580 --> 00:05:39,659
CPU less than 25 percent so we can we

00:05:35,880 --> 00:05:42,180
can see small spike near 100% this just

00:05:39,659 --> 00:05:44,880
means that there are really really very

00:05:42,180 --> 00:05:49,800
few jobs which ever reach of the

00:05:44,880 --> 00:05:52,889
allocation of CPUs so if we zoom in to

00:05:49,800 --> 00:06:01,500
the left side of the chart we could find

00:05:52,889 --> 00:06:05,630
some interesting insights so P 95 CPU

00:06:01,500 --> 00:06:10,130
utilization of 10% is actually about

00:06:05,630 --> 00:06:14,699
reached is actually reached by less than

00:06:10,130 --> 00:06:18,150
by 50% of jobs so basically if we use

00:06:14,699 --> 00:06:23,000
such metrics and detect idle jobs based

00:06:18,150 --> 00:06:25,800
on these metrics we can save we can win

00:06:23,000 --> 00:06:28,710
50% we can win a large amount of

00:06:25,800 --> 00:06:31,529
machines that can be used to run another

00:06:28,710 --> 00:06:34,949
non production jobs by using our

00:06:31,529 --> 00:06:36,899
subscription so what we try to do is to

00:06:34,949 --> 00:06:37,700
detect under if Phillip utilized

00:06:36,899 --> 00:06:40,610
superiors

00:06:37,700 --> 00:06:44,010
based on the metrics that we collect

00:06:40,610 --> 00:06:49,590
then we offer this resources for of a

00:06:44,010 --> 00:06:52,020
subscription to non production jobs we

00:06:49,590 --> 00:06:54,120
also separate how we run production and

00:06:52,020 --> 00:06:56,490
non production jobs so the production

00:06:54,120 --> 00:06:59,010
jobs will use non revocable resources

00:06:56,490 --> 00:07:01,919
while non production jobs are going to

00:06:59,010 --> 00:07:04,290
use just revocable resources and we need

00:07:01,919 --> 00:07:08,130
some kind of intermediate stage when we

00:07:04,290 --> 00:07:09,990
are moving from current state when non

00:07:08,130 --> 00:07:12,720
production jobs are using non revocable

00:07:09,990 --> 00:07:15,180
resources so potentially we could have

00:07:12,720 --> 00:07:22,320
non production jobs which are using both

00:07:15,180 --> 00:07:24,060
revocable and non revocable resources we

00:07:22,320 --> 00:07:29,370
also have some limitations and

00:07:24,060 --> 00:07:31,919
constraints so production non production

00:07:29,370 --> 00:07:34,260
jobs must not affect affect production

00:07:31,919 --> 00:07:36,900
jobs because our production jobs and

00:07:34,260 --> 00:07:43,650
mission critical we cannot preempt

00:07:36,900 --> 00:07:46,200
production jobs however we can preempt

00:07:43,650 --> 00:07:49,020
non production jobs like what means that

00:07:46,200 --> 00:07:54,210
we can kill a job move it to some other

00:07:49,020 --> 00:07:56,370
node and relaunch it there we also

00:07:54,210 --> 00:08:00,120
expect some impact on performance of non

00:07:56,370 --> 00:08:01,620
production jobs because we need some

00:08:00,120 --> 00:08:03,390
time to detect that there is a

00:08:01,620 --> 00:08:06,990
contention between non production jobs

00:08:03,390 --> 00:08:08,760
and this could take some time to collect

00:08:06,990 --> 00:08:12,570
the metrics see that there is a

00:08:08,760 --> 00:08:14,340
contention and during this time we can

00:08:12,570 --> 00:08:18,030
have an impact on non production jobs

00:08:14,340 --> 00:08:25,919
but non production job should must not

00:08:18,030 --> 00:08:29,039
impact production job so missus have for

00:08:25,919 --> 00:08:31,500
quite a while let us have solution to

00:08:29,039 --> 00:08:35,820
over subscribe resources which is

00:08:31,500 --> 00:08:39,120
basically represented here this is taken

00:08:35,820 --> 00:08:41,010
from message documentation so the two

00:08:39,120 --> 00:08:43,950
major components here are resources

00:08:41,010 --> 00:08:46,410
tomato and Q s controller our resources

00:08:43,950 --> 00:08:48,600
to meter is responsible for estimating

00:08:46,410 --> 00:08:49,730
the amount of available resources for

00:08:48,600 --> 00:08:52,960
hours

00:08:49,730 --> 00:08:55,640
and cruise controller is basically

00:08:52,960 --> 00:08:59,210
monitoring the state of the jobs of the

00:08:55,640 --> 00:09:01,820
processes and decides that needs to take

00:08:59,210 --> 00:09:03,860
some action to ensure the quality of

00:09:01,820 --> 00:09:05,870
service for example currently there is

00:09:03,860 --> 00:09:10,730
just one action that supports it is

00:09:05,870 --> 00:09:13,310
killing a job both of them can consult a

00:09:10,730 --> 00:09:16,010
resource monitor to obtain statistics

00:09:13,310 --> 00:09:20,230
from the from the container like CPU

00:09:16,010 --> 00:09:24,580
usage memory usage and all that stuff

00:09:20,230 --> 00:09:28,279
when resource estimator contains some

00:09:24,580 --> 00:09:30,950
value of available resources it send

00:09:28,279 --> 00:09:32,570
this sense it to agent then it goes to

00:09:30,950 --> 00:09:34,400
master and master can offer this

00:09:32,570 --> 00:09:41,110
resources to framework as revocable

00:09:34,400 --> 00:09:44,020
resources this this is the interface of

00:09:41,110 --> 00:09:46,460
resources tomaters is defined in meses

00:09:44,020 --> 00:09:54,140
so basically there is an initialize

00:09:46,460 --> 00:09:57,470
function which accepts a function which

00:09:54,140 --> 00:09:59,510
can be used to obtain resource usage and

00:09:57,470 --> 00:10:04,250
also there is our subscribable function

00:09:59,510 --> 00:10:06,200
which returns available resources this

00:10:04,250 --> 00:10:10,220
controller is very similar same

00:10:06,200 --> 00:10:12,230
initialize function and corrections

00:10:10,220 --> 00:10:17,089
function which should return the list of

00:10:12,230 --> 00:10:21,860
Corrections basically this action to

00:10:17,089 --> 00:10:23,870
take on which container also frameworks

00:10:21,860 --> 00:10:25,970
must opt in to receive revocable

00:10:23,870 --> 00:10:30,230
resources with declaring capability

00:10:25,970 --> 00:10:33,220
irrevocable resources by default the

00:10:30,230 --> 00:10:37,270
framework will not receive it

00:10:33,220 --> 00:10:39,980
there are also some parameters on agent

00:10:37,270 --> 00:10:42,770
which should be used to specify resource

00:10:39,980 --> 00:10:45,140
estimate or grass controller and two

00:10:42,770 --> 00:10:48,320
parameters to define intervals which

00:10:45,140 --> 00:10:51,740
measures uses to query resources tomater

00:10:48,320 --> 00:10:56,900
and Kos control which are by default

00:10:51,740 --> 00:10:59,089
it's 15 seconds so we came up with the

00:10:56,900 --> 00:11:02,300
idea of resource idle resources for

00:10:59,089 --> 00:11:05,420
subscription which is in

00:11:02,300 --> 00:11:08,560
this model can be presented like this so

00:11:05,420 --> 00:11:12,380
for resource estimation we detect

00:11:08,560 --> 00:11:15,620
initialized CPUs from non production

00:11:12,380 --> 00:11:18,410
jobs that's basically allocation slack

00:11:15,620 --> 00:11:25,450
like resources which are not allocated

00:11:18,410 --> 00:11:27,830
to any type of of job and we add

00:11:25,450 --> 00:11:32,660
underutilized allocated resources used

00:11:27,830 --> 00:11:36,170
by non production jobs to that case

00:11:32,660 --> 00:11:38,060
control is responsible for killing non

00:11:36,170 --> 00:11:40,010
production jobs if there are not

00:11:38,060 --> 00:11:44,180
available not enough available resources

00:11:40,010 --> 00:11:47,150
this can be due to different cases for

00:11:44,180 --> 00:11:50,840
example we detected that non production

00:11:47,150 --> 00:11:54,310
job is idle and sometime after that it

00:11:50,840 --> 00:11:58,610
became active like it started using CPU

00:11:54,310 --> 00:12:00,560
proactively so there could be a case

00:11:58,610 --> 00:12:04,430
then that there are not enough resources

00:12:00,560 --> 00:12:07,330
now so we should kill some of the jobs

00:12:04,430 --> 00:12:11,900
to make sure that there is no contention

00:12:07,330 --> 00:12:16,690
and also since we have important job to

00:12:11,900 --> 00:12:19,940
keep production jobs safe from

00:12:16,690 --> 00:12:23,120
contention we must use these elation and

00:12:19,940 --> 00:12:27,620
isolate all non production jobs in a

00:12:23,120 --> 00:12:31,010
separate group so the basic rule for

00:12:27,620 --> 00:12:33,800
detecting idleness is like this so

00:12:31,010 --> 00:12:38,390
containers idle if P percent of samples

00:12:33,800 --> 00:12:41,270
over some window with duration shows CPU

00:12:38,390 --> 00:12:45,170
usage less than threshold in other words

00:12:41,270 --> 00:12:49,100
if we see that during a window of for

00:12:45,170 --> 00:12:54,650
example one day some of some containers

00:12:49,100 --> 00:12:59,030
shows that it's only using in ninety

00:12:54,650 --> 00:13:02,300
five metric of CPU usage less than 10

00:12:59,030 --> 00:13:05,330
percent then the job is idle it can have

00:13:02,300 --> 00:13:10,700
some births but basically in ninety five

00:13:05,330 --> 00:13:13,610
covers this based on our data we made

00:13:10,700 --> 00:13:14,670
some estimations so using different

00:13:13,610 --> 00:13:19,649
metrics

00:13:14,670 --> 00:13:25,430
we found that we have more than 50% of

00:13:19,649 --> 00:13:29,339
tasks idle and using different

00:13:25,430 --> 00:13:34,019
methodologies to estimate idle CPUs we

00:13:29,339 --> 00:13:38,700
came up this 30 to 50 percent of idle

00:13:34,019 --> 00:13:44,070
CPUs so the lower bound is based on

00:13:38,700 --> 00:13:47,240
Simon integer rounded to integer number

00:13:44,070 --> 00:13:51,750
of CPUs because basically if we can have

00:13:47,240 --> 00:13:54,120
zero point eight CPUs available um it's

00:13:51,750 --> 00:13:56,269
almost likely that no job will be able

00:13:54,120 --> 00:14:00,269
to use it because people tend to use

00:13:56,269 --> 00:14:04,170
integer numbers like 1 or 2 CPUs so

00:14:00,269 --> 00:14:08,300
that's the lower bound and the upper

00:14:04,170 --> 00:14:14,040
bound is just sum of all CPUs available

00:14:08,300 --> 00:14:17,310
so let's see how we detect available

00:14:14,040 --> 00:14:20,790
resources so first of all we always

00:14:17,310 --> 00:14:23,160
ignore resources allocated to production

00:14:20,790 --> 00:14:27,899
jobs they are not available for our

00:14:23,160 --> 00:14:29,910
subscription if there are no non

00:14:27,899 --> 00:14:33,180
production resource jobs running then

00:14:29,910 --> 00:14:36,180
all available our subscriber resources

00:14:33,180 --> 00:14:39,570
are equal to allocation slack means not

00:14:36,180 --> 00:14:42,360
allocated resources then if we launch

00:14:39,570 --> 00:14:46,050
non production job allocations flock is

00:14:42,360 --> 00:14:50,310
reduced and we make this available as

00:14:46,050 --> 00:14:55,940
our subscribable resources if we detect

00:14:50,310 --> 00:14:58,290
that the job is idle then we can offer

00:14:55,940 --> 00:15:00,630
unutilized resources as oversubscribe

00:14:58,290 --> 00:15:07,260
also we just increase that amount of

00:15:00,630 --> 00:15:10,829
resources let's see how Corrections work

00:15:07,260 --> 00:15:13,290
in this case so suppose you have an idle

00:15:10,829 --> 00:15:18,500
non production job running using

00:15:13,290 --> 00:15:21,420
revocable resources and we launch

00:15:18,500 --> 00:15:24,079
another production job so that means

00:15:21,420 --> 00:15:26,310
that our allocation slack is reduced and

00:15:24,079 --> 00:15:28,550
we have to reduce the amount of our

00:15:26,310 --> 00:15:31,730
subscribers

00:15:28,550 --> 00:15:33,350
so in this case we see that no

00:15:31,730 --> 00:15:37,250
production job is no longer having

00:15:33,350 --> 00:15:40,760
enough resources basically it's starving

00:15:37,250 --> 00:15:45,310
so we need to kill it Andrew lunch

00:15:40,760 --> 00:15:50,050
somewhere else another use case is when

00:15:45,310 --> 00:15:53,150
non production job becomes non idle so

00:15:50,050 --> 00:15:56,780
we have two jobs one is idle one is not

00:15:53,150 --> 00:15:59,090
and then that first job becomes non idle

00:15:56,780 --> 00:16:01,880
again we know we now don't have enough

00:15:59,090 --> 00:16:08,540
resources to have them both on the same

00:16:01,880 --> 00:16:11,600
post so we should give one of them our

00:16:08,540 --> 00:16:15,080
case correction strategy is to kill non

00:16:11,600 --> 00:16:17,570
idle jobs and preserve for idleness

00:16:15,080 --> 00:16:20,330
this might seem counterintuitive because

00:16:17,570 --> 00:16:24,470
why would we kill normal job is doing

00:16:20,330 --> 00:16:28,370
something right the reason is that if we

00:16:24,470 --> 00:16:32,510
kill idle job it will be rescheduled

00:16:28,370 --> 00:16:34,570
somewhere else as non idle so in the end

00:16:32,510 --> 00:16:40,880
we will have two non idle jobs running

00:16:34,570 --> 00:16:43,310
but if we kill a non idle job then we

00:16:40,880 --> 00:16:49,760
will have one idle job and we will

00:16:43,310 --> 00:16:56,720
rewind entitled job somewhere else this

00:16:49,760 --> 00:17:00,140
also requires some some taking care of

00:16:56,720 --> 00:17:03,260
recovery to preserve idleness because we

00:17:00,140 --> 00:17:07,520
wanted to start with I don't win dough

00:17:03,260 --> 00:17:09,410
of one day or maybe one week and that's

00:17:07,520 --> 00:17:12,410
a long time like we need to update

00:17:09,410 --> 00:17:15,320
agents on the hosts many different

00:17:12,410 --> 00:17:19,460
things can happen so if we don't

00:17:15,320 --> 00:17:22,190
preserve this I information then each

00:17:19,460 --> 00:17:24,800
restart of the agents will cause restart

00:17:22,190 --> 00:17:27,110
of the jobs possibly so we checkpoint

00:17:24,800 --> 00:17:29,270
instead of chicken to point in all the

00:17:27,110 --> 00:17:31,580
samples obtained from the job during one

00:17:29,270 --> 00:17:33,770
win during one day for example in one

00:17:31,580 --> 00:17:38,750
week this could be a huge amount of data

00:17:33,770 --> 00:17:42,890
so instead we check point statistics and

00:17:38,750 --> 00:17:46,760
a few times temps when agent restarts we

00:17:42,890 --> 00:17:50,300
recover the samples so this can

00:17:46,760 --> 00:17:53,690
demonstrate it on this graph supposed

00:17:50,300 --> 00:17:57,770
it's CPU utilization overtime during

00:17:53,690 --> 00:18:00,350
some period of time how statistics is

00:17:57,770 --> 00:18:05,960
obtained basically we sort or samples by

00:18:00,350 --> 00:18:12,020
value and then take the metrics like P

00:18:05,960 --> 00:18:15,230
50 p90 P 95 maximum when we try to

00:18:12,020 --> 00:18:17,480
reconstruct we do this conservatively so

00:18:15,230 --> 00:18:21,340
we don't have any data about samples

00:18:17,480 --> 00:18:27,370
between metrics so we just assume that

00:18:21,340 --> 00:18:27,370
the the CPU usage was a possible maximum

00:18:27,940 --> 00:18:35,930
then we just convert from statistics

00:18:31,790 --> 00:18:39,860
space to time space to receive samples

00:18:35,930 --> 00:18:43,480
and there is one more thing that during

00:18:39,860 --> 00:18:47,330
aging downtime we need to shift the

00:18:43,480 --> 00:18:53,090
samples and by this time here we just

00:18:47,330 --> 00:18:56,060
assume that while agent was down the CPU

00:18:53,090 --> 00:18:59,620
usage remained constant and it was our

00:18:56,060 --> 00:18:59,620
maximum obtained sample

00:19:00,560 --> 00:19:05,060
so this overestimates tends to

00:19:02,780 --> 00:19:08,420
overestimate CPU utilization but it's

00:19:05,060 --> 00:19:12,800
conservative it means that non idle jobs

00:19:08,420 --> 00:19:15,380
will remain non idle due to the way it's

00:19:12,800 --> 00:19:19,640
reconstructed idle jobs may become non

00:19:15,380 --> 00:19:21,140
idle but that's okay we will just if

00:19:19,640 --> 00:19:23,120
there are enough resources they will

00:19:21,140 --> 00:19:24,800
keep running if not then we will kill

00:19:23,120 --> 00:19:28,520
the job and we launch it somewhere else

00:19:24,800 --> 00:19:31,280
in the cluster and also after this

00:19:28,520 --> 00:19:34,640
reconstruction because we rearranged

00:19:31,280 --> 00:19:37,580
samples from lowest value to the maximum

00:19:34,640 --> 00:19:39,890
value it means that the job can remain

00:19:37,580 --> 00:19:45,290
on idle for more time of such

00:19:39,890 --> 00:19:48,530
reconstruction that's also fine so we

00:19:45,290 --> 00:19:51,049
implemented this using lasers model and

00:19:48,530 --> 00:19:55,100
found some difficulties

00:19:51,049 --> 00:19:57,139
that it didn't work well so first of all

00:19:55,100 --> 00:20:00,710
there was an issue with decreasing their

00:19:57,139 --> 00:20:02,330
subscribable resources this happens

00:20:00,710 --> 00:20:05,899
between because there is a delay between

00:20:02,330 --> 00:20:09,320
killing on production job and update of

00:20:05,899 --> 00:20:11,600
the resources like there was an interval

00:20:09,320 --> 00:20:14,659
there is an interval in misses which is

00:20:11,600 --> 00:20:18,830
used for case Corrections and for over

00:20:14,659 --> 00:20:21,980
subscription so if if scheduler can

00:20:18,830 --> 00:20:25,879
manages to get into this between those

00:20:21,980 --> 00:20:29,269
two actions it can launch a task using

00:20:25,879 --> 00:20:33,830
old amount of resources so more details

00:20:29,269 --> 00:20:36,049
on this so resources tomater when

00:20:33,830 --> 00:20:37,609
resource estimate is called its it must

00:20:36,049 --> 00:20:40,549
return additional amount of or

00:20:37,609 --> 00:20:45,499
subscriber resources not the total

00:20:40,549 --> 00:20:47,149
amount that's one of the issues so here

00:20:45,499 --> 00:20:49,999
in this case we have production on

00:20:47,149 --> 00:20:51,739
production job and we use a location

00:20:49,999 --> 00:20:54,859
slab to return a list of tribal

00:20:51,739 --> 00:20:58,580
resources when we launch another job so

00:20:54,859 --> 00:21:01,190
basically here we amount of our

00:20:58,580 --> 00:21:04,909
subscriber resources is negative because

00:21:01,190 --> 00:21:07,309
those containers CP usage over severe

00:21:04,909 --> 00:21:09,320
location overlaps but we cannot return

00:21:07,309 --> 00:21:12,499
negative value from our subscribable

00:21:09,320 --> 00:21:15,950
function so the best we can do is return

00:21:12,499 --> 00:21:18,830
zero meaning we don't have our

00:21:15,950 --> 00:21:21,440
subscriber resources anymore then if

00:21:18,830 --> 00:21:24,399
make a case correction to kill a

00:21:21,440 --> 00:21:27,730
container but now there is a delay

00:21:24,399 --> 00:21:30,409
between killing the container and

00:21:27,730 --> 00:21:35,299
measures calling over subscribable

00:21:30,409 --> 00:21:37,970
method during this time master is not

00:21:35,299 --> 00:21:40,850
aware of change of resources and it can

00:21:37,970 --> 00:21:44,049
offer it to framework framework kind of

00:21:40,850 --> 00:21:46,789
set resources and launch another job and

00:21:44,049 --> 00:21:50,119
basically we end in the same situation

00:21:46,789 --> 00:21:54,019
and we we make a loop when we should

00:21:50,119 --> 00:21:57,619
kill job hope that runs resources tomato

00:21:54,019 --> 00:22:00,970
runs before the framework and so on and

00:21:57,619 --> 00:22:04,330
so forth so

00:22:00,970 --> 00:22:06,789
were crowned that we came with games

00:22:04,330 --> 00:22:10,330
from a resources tomato and cruise

00:22:06,789 --> 00:22:14,770
control interfaces so they reserved they

00:22:10,330 --> 00:22:18,610
returned futures of resources and grace

00:22:14,770 --> 00:22:21,250
Corrections we don't have to act

00:22:18,610 --> 00:22:23,230
immediately like if we requested to

00:22:21,250 --> 00:22:26,169
return every subscriber resources we

00:22:23,230 --> 00:22:28,750
don't have to go estimate it right now

00:22:26,169 --> 00:22:31,390
and return so what instead with what we

00:22:28,750 --> 00:22:34,510
do instead is we make a promise to

00:22:31,390 --> 00:22:38,440
return resources and basically do

00:22:34,510 --> 00:22:41,950
nothing when we see that there is a

00:22:38,440 --> 00:22:44,770
change that we must react to we then

00:22:41,950 --> 00:22:47,409
complete these fusions so our reaction

00:22:44,770 --> 00:22:49,419
is immediately as soon as we detect the

00:22:47,409 --> 00:22:55,840
change we don't have to wait another

00:22:49,419 --> 00:22:58,419
cycle of requests from messes so this

00:22:55,840 --> 00:23:00,850
way we control resource estimating and

00:22:58,419 --> 00:23:03,280
curse correction resources estimation

00:23:00,850 --> 00:23:07,480
and corrections by completing food

00:23:03,280 --> 00:23:10,600
future when we at the right time when we

00:23:07,480 --> 00:23:13,750
detect the change to detect the changes

00:23:10,600 --> 00:23:17,380
we need to hook executor and task

00:23:13,750 --> 00:23:22,419
lifecycle events to direct those changes

00:23:17,380 --> 00:23:25,510
and we must also adjust measures how

00:23:22,419 --> 00:23:28,990
Malthus works by setting intervals to

00:23:25,510 --> 00:23:31,330
zeros this basically means that when we

00:23:28,990 --> 00:23:35,140
complete the future measures immediately

00:23:31,330 --> 00:23:40,950
requests the new value which we can hold

00:23:35,140 --> 00:23:44,350
on hold on till we detect another change

00:23:40,950 --> 00:23:46,690
we also found that there was duplicate

00:23:44,350 --> 00:23:51,340
work between resources tomato increase

00:23:46,690 --> 00:23:55,809
controller and really high coupling

00:23:51,340 --> 00:23:57,580
between them so instead of having two

00:23:55,809 --> 00:24:00,700
different modules which run

00:23:57,580 --> 00:24:04,120
independently we came up with this

00:24:00,700 --> 00:24:06,309
structure where our subscriber is

00:24:04,120 --> 00:24:09,760
basically a combination of resources

00:24:06,309 --> 00:24:11,860
tomato and Coast controller and combines

00:24:09,760 --> 00:24:14,350
both of them

00:24:11,860 --> 00:24:16,630
there is of a subscriber factory factory

00:24:14,350 --> 00:24:19,240
to ensure that we create only one or

00:24:16,630 --> 00:24:22,150
subscriber and that it's this one

00:24:19,240 --> 00:24:26,410
instance is used by both resources

00:24:22,150 --> 00:24:29,020
tomater and case controller modules from

00:24:26,410 --> 00:24:31,870
this point it's pretty much standard for

00:24:29,020 --> 00:24:34,270
measures when we have an interface in

00:24:31,870 --> 00:24:37,990
the process the process process which is

00:24:34,270 --> 00:24:41,040
responsible for handling the requests so

00:24:37,990 --> 00:24:43,030
here idle ops over subscriber just

00:24:41,040 --> 00:24:45,880
dispatches requests

00:24:43,030 --> 00:24:49,210
- I love subscriber process and we also

00:24:45,880 --> 00:24:51,780
added hooks implementation to page task

00:24:49,210 --> 00:24:51,780
transitions

00:24:52,390 --> 00:24:57,220
this makes configuration a little bit

00:24:54,580 --> 00:25:01,440
weird because we have to collect all

00:24:57,220 --> 00:25:06,490
parameters in a hook module just because

00:25:01,440 --> 00:25:08,740
it's created first and then just have a

00:25:06,490 --> 00:25:10,810
names for us control and resources

00:25:08,740 --> 00:25:12,700
estimator without any parameters but

00:25:10,810 --> 00:25:18,190
actually they parameter their parameters

00:25:12,700 --> 00:25:19,480
are in hook parameters we also

00:25:18,190 --> 00:25:22,840
implemented distillation

00:25:19,480 --> 00:25:26,820
this picture shows current

00:25:22,840 --> 00:25:31,210
implementation of isolation in measures

00:25:26,820 --> 00:25:34,840
it doesn't differentiate between jobs

00:25:31,210 --> 00:25:37,270
running on production on revocable and

00:25:34,840 --> 00:25:41,530
non revocable resources so in this case

00:25:37,270 --> 00:25:44,920
when we run too many too many tasks

00:25:41,530 --> 00:25:47,130
right using revocable resources there is

00:25:44,920 --> 00:25:52,390
a potential for contention between

00:25:47,130 --> 00:25:55,270
production and non production tasks we

00:25:52,390 --> 00:26:00,220
wanted to avoid this so we introduced a

00:25:55,270 --> 00:26:04,270
revocable see group we just groups all

00:26:00,220 --> 00:26:06,940
non production containers underneath so

00:26:04,270 --> 00:26:10,720
even if all of those non production

00:26:06,940 --> 00:26:13,270
containers become non idle due to the

00:26:10,720 --> 00:26:19,870
group limitations it won't be able to to

00:26:13,270 --> 00:26:20,650
affect production containers we limit

00:26:19,870 --> 00:26:23,320
revoke

00:26:20,650 --> 00:26:26,460
see group by location slack so this

00:26:23,320 --> 00:26:32,200
ensures that production containers have

00:26:26,460 --> 00:26:36,520
their share of CPU time this has some

00:26:32,200 --> 00:26:39,190
issues because if we have too many we

00:26:36,520 --> 00:26:42,580
have too many non production containers

00:26:39,190 --> 00:26:46,210
running there could be a contention but

00:26:42,580 --> 00:26:47,560
we might not be able to detect it just

00:26:46,210 --> 00:26:51,610
because there are not enough resources

00:26:47,560 --> 00:26:54,760
for all of them to become non idle this

00:26:51,610 --> 00:26:58,090
can be solved by using some techniques

00:26:54,760 --> 00:27:02,350
to detect contention basing based on

00:26:58,090 --> 00:27:05,830
like how much CPU is throttled and so on

00:27:02,350 --> 00:27:09,190
or just ensure that we don't arch too

00:27:05,830 --> 00:27:13,870
much containers non production

00:27:09,190 --> 00:27:17,020
containers there's also limitation that

00:27:13,870 --> 00:27:23,650
enable enabling revocable isolation will

00:27:17,020 --> 00:27:25,630
take effect only on on new tasks but so

00:27:23,650 --> 00:27:29,020
we would have to restart

00:27:25,630 --> 00:27:32,350
non non production tasks to make sure

00:27:29,020 --> 00:27:37,150
that there is alai isolated that's fine

00:27:32,350 --> 00:27:40,030
because currently our tasks that are run

00:27:37,150 --> 00:27:43,390
in on our non production jobs that are

00:27:40,030 --> 00:27:49,870
running on non revocable resources they

00:27:43,390 --> 00:27:53,290
just cannot be oversubscribed so another

00:27:49,870 --> 00:27:56,470
issue is disabling or downgrading we

00:27:53,290 --> 00:28:00,280
basically have to restart all tasks to

00:27:56,470 --> 00:28:01,830
make sure that they cannot affect each

00:28:00,280 --> 00:28:05,020
other

00:28:01,830 --> 00:28:09,220
we also made scale tests of the

00:28:05,020 --> 00:28:10,630
implementation and found some issues the

00:28:09,220 --> 00:28:13,210
one of one of them was increased

00:28:10,630 --> 00:28:16,410
frequency update of agent resources some

00:28:13,210 --> 00:28:20,200
days like resources tomater monitors

00:28:16,410 --> 00:28:22,420
active tasks idle tasks and race

00:28:20,200 --> 00:28:26,020
teammates amount of available resources

00:28:22,420 --> 00:28:27,640
this causes agent update which is

00:28:26,020 --> 00:28:30,970
reported to master and master than

00:28:27,640 --> 00:28:32,419
Racine's offer and sends another offer

00:28:30,970 --> 00:28:38,509
to the framework

00:28:32,419 --> 00:28:43,690
and it happened that Aurora which we use

00:28:38,509 --> 00:28:48,350
as a scheduler received too many

00:28:43,690 --> 00:28:50,869
rescinded offers and was using old

00:28:48,350 --> 00:28:55,730
offers to launch another task so with so

00:28:50,869 --> 00:29:04,279
many issues like schedule is trying to

00:28:55,730 --> 00:29:09,289
launch a task on old offer so what we

00:29:04,279 --> 00:29:11,690
did is we just edit tolerance if change

00:29:09,289 --> 00:29:13,609
his minor in changing available

00:29:11,690 --> 00:29:16,309
resources his minor then we just ignore

00:29:13,609 --> 00:29:19,639
it and do not report it to must to the

00:29:16,309 --> 00:29:23,600
agent in the master this significantly

00:29:19,639 --> 00:29:32,359
reduced the traffic and amount of lost

00:29:23,600 --> 00:29:34,249
tasks so we also changed our order to

00:29:32,359 --> 00:29:37,549
make sure that we sinned it offers a

00:29:34,249 --> 00:29:41,980
handled as soon as possible like before

00:29:37,549 --> 00:29:45,369
that there was one single queue for

00:29:41,980 --> 00:29:48,379
processing offers and rescinding offers

00:29:45,369 --> 00:29:51,109
now it's out of order if there is a

00:29:48,379 --> 00:29:55,279
rescind its processed with highest

00:29:51,109 --> 00:29:57,759
priority another issue is that scheduler

00:29:55,279 --> 00:30:00,980
is unaware of revocable and irrevocable

00:29:57,759 --> 00:30:05,179
resources relation it basically means

00:30:00,980 --> 00:30:09,499
that if we launch some tasks with for

00:30:05,179 --> 00:30:12,139
example production tasks we know that

00:30:09,499 --> 00:30:14,450
this would reduce location slack and

00:30:12,139 --> 00:30:17,679
therefore it will reduce the amount of

00:30:14,450 --> 00:30:19,850
our subscribable resources available

00:30:17,679 --> 00:30:22,639
scheduler does it know this and

00:30:19,850 --> 00:30:26,600
potentially it could launch production

00:30:22,639 --> 00:30:29,480
tasks and non production tasks on on an

00:30:26,600 --> 00:30:31,909
offer and just non production tasks will

00:30:29,480 --> 00:30:36,889
be killed because allocations like was

00:30:31,909 --> 00:30:40,639
reduced so the current state that we

00:30:36,889 --> 00:30:42,090
might test some product on our scale

00:30:40,639 --> 00:30:46,380
test faster

00:30:42,090 --> 00:30:50,010
which somewhat simulates our production

00:30:46,380 --> 00:30:52,650
cluster it's not in production yet yet

00:30:50,010 --> 00:30:56,600
we still have to fix few issues for

00:30:52,650 --> 00:30:58,830
example that scheduler and awareness of

00:30:56,600 --> 00:31:03,690
revocable and on revocable resources

00:30:58,830 --> 00:31:06,000
relation and our deployment plan is to

00:31:03,690 --> 00:31:08,280
fix the issues and then migrate non

00:31:06,000 --> 00:31:11,940
productions or jobs to revocable

00:31:08,280 --> 00:31:14,250
resources it's pretty easy to do with

00:31:11,940 --> 00:31:18,750
Aurora because we just have to do is to

00:31:14,250 --> 00:31:21,060
change tier configuration and just

00:31:18,750 --> 00:31:23,850
configure it to use revocable resources

00:31:21,060 --> 00:31:25,710
so new tasks will often automatically

00:31:23,850 --> 00:31:30,150
launch on revocable on revocable

00:31:25,710 --> 00:31:36,120
resources the tasks that are running we

00:31:30,150 --> 00:31:47,300
basically have to restart them so that's

00:31:36,120 --> 00:31:47,300
it any questions

00:31:56,980 --> 00:32:03,020
production versus non production that's

00:31:59,690 --> 00:32:04,610
a pretty broad brush to categorize and

00:32:03,020 --> 00:32:07,610
which may be applicable which is fine

00:32:04,610 --> 00:32:11,570
but I'm curious have you felt the need

00:32:07,610 --> 00:32:14,510
to have even more finer granularity x'

00:32:11,570 --> 00:32:16,220
or do you think that adds complexity and

00:32:14,510 --> 00:32:21,110
that's the reason you're not looking at

00:32:16,220 --> 00:32:24,380
a finer granularity well I don't think

00:32:21,110 --> 00:32:26,690
that we need finer granularity basically

00:32:24,380 --> 00:32:30,200
production jobs mission-critical jobs

00:32:26,690 --> 00:32:34,960
which have quotas and all that stuff

00:32:30,200 --> 00:32:40,090
non production jobs it's usually

00:32:34,960 --> 00:32:45,230
development or testing jobs or some

00:32:40,090 --> 00:32:47,360
really some jobs that do not require

00:32:45,230 --> 00:32:48,740
that high level of SLA which are

00:32:47,360 --> 00:32:52,700
required which is required for

00:32:48,740 --> 00:32:54,290
production jobs so maybe when we start

00:32:52,700 --> 00:32:58,870
over subscribing we will find something

00:32:54,290 --> 00:33:05,679
different something else but so far

00:32:58,870 --> 00:33:05,679
thanks any questions

00:33:14,040 --> 00:33:22,360
so if you ever observed something that

00:33:20,440 --> 00:33:24,010
made all the idle jobs spike at the same

00:33:22,360 --> 00:33:25,660
time and then maybe even fail because

00:33:24,010 --> 00:33:27,880
they weren't able to get the resources

00:33:25,660 --> 00:33:29,710
they needed in a short period of time

00:33:27,880 --> 00:33:31,120
just because there was some external

00:33:29,710 --> 00:33:33,910
thing that makes them all spike at the

00:33:31,120 --> 00:33:36,070
same time well this shouldn't be an

00:33:33,910 --> 00:33:39,370
issue because of these either isolation

00:33:36,070 --> 00:33:41,080
so we grouped all revocable of all jobs

00:33:39,370 --> 00:33:45,190
running on revocable resources in the

00:33:41,080 --> 00:33:47,770
same C group so they have limited amount

00:33:45,190 --> 00:33:51,970
of time and they cannot impact the

00:33:47,770 --> 00:33:54,760
production jobs but if they all spike at

00:33:51,970 --> 00:33:57,760
the same time well there will be

00:33:54,760 --> 00:34:03,130
contention we're fine with with this

00:33:57,760 --> 00:34:05,320
contention for some period of time so

00:34:03,130 --> 00:34:08,350
it's acceptable to have this contention

00:34:05,320 --> 00:34:11,740
but when we detect this we will

00:34:08,350 --> 00:34:14,350
reschedule one of the or some of the non

00:34:11,740 --> 00:34:18,510
production jobs on the other host when

00:34:14,350 --> 00:34:18,510
we'll have probably better situation

00:34:23,220 --> 00:34:28,810
how's the experience with the qsr

00:34:25,690 --> 00:34:32,860
controller like do you see the kill rate

00:34:28,810 --> 00:34:35,169
is too high or do you see like the kill

00:34:32,860 --> 00:34:38,409
happening when the system is actually

00:34:35,169 --> 00:34:42,340
not under kind of heavy load like have

00:34:38,409 --> 00:34:45,100
you looked into that aspect sir could

00:34:42,340 --> 00:34:47,260
your business like the QoS controller

00:34:45,100 --> 00:34:49,629
like the killing of the job they caused

00:34:47,260 --> 00:34:52,330
your experience like do you see that

00:34:49,629 --> 00:34:57,220
kill rate is too high like is it killing

00:34:52,330 --> 00:35:00,310
too many kind of containers well based

00:34:57,220 --> 00:35:03,540
on the experiments I don't think that

00:35:00,310 --> 00:35:08,110
the rate would be too high because well

00:35:03,540 --> 00:35:11,680
the statistics shows that really many

00:35:08,110 --> 00:35:16,000
jobs are idle and this keeps for a very

00:35:11,680 --> 00:35:19,690
long window so the kill only can happen

00:35:16,000 --> 00:35:21,830
if we launch a production job that's the

00:35:19,690 --> 00:35:24,260
issue is that we have with

00:35:21,830 --> 00:35:27,050
the scheduler which it's that it's not

00:35:24,260 --> 00:35:30,230
aware of relation between revocable and

00:35:27,050 --> 00:35:32,480
on revocable resources so this can be

00:35:30,230 --> 00:35:36,380
fixed on scheduler level that it would

00:35:32,480 --> 00:35:41,290
prefer to launch production jobs and non

00:35:36,380 --> 00:35:41,290
production jobs in different hosts like

00:35:42,730 --> 00:35:51,800
like drying basically it's trying to to

00:35:48,770 --> 00:35:53,240
launch jobs from different directions if

00:35:51,800 --> 00:35:58,550
you see what I mean

00:35:53,240 --> 00:36:02,240
so another potential is to kill the job

00:35:58,550 --> 00:36:06,670
if non lie if idle John job becomes non

00:36:02,240 --> 00:36:12,230
idle but that doesn't happen frequently

00:36:06,670 --> 00:36:14,480
so I don't think that given the current

00:36:12,230 --> 00:36:17,960
situation we would have too much

00:36:14,480 --> 00:36:20,690
preempted jobs how do you take like

00:36:17,960 --> 00:36:22,310
these big upside as non-ideal like how

00:36:20,690 --> 00:36:25,220
do you detect what is the your detection

00:36:22,310 --> 00:36:26,530
mechanism is it like per container sorry

00:36:25,220 --> 00:36:29,930
I'm sorry

00:36:26,530 --> 00:36:31,670
so for like how do you detect system is

00:36:29,930 --> 00:36:34,850
like the container is idle versus non

00:36:31,670 --> 00:36:37,520
idle so do you have per container

00:36:34,850 --> 00:36:40,730
kind of you are looking to the SLA for

00:36:37,520 --> 00:36:46,480
that per container or is it based on the

00:36:40,730 --> 00:36:49,630
complete system level statistics so I

00:36:46,480 --> 00:36:49,630
don't necessarily

00:36:57,509 --> 00:37:05,419
is this rule so we collect metrics for

00:37:01,139 --> 00:37:07,619
each container meses API allows from the

00:37:05,419 --> 00:37:10,919
collection of such metrics on container

00:37:07,619 --> 00:37:13,439
level not on each individual process so

00:37:10,919 --> 00:37:17,069
we collect this on container level and

00:37:13,439 --> 00:37:21,799
the parameters PD and threshold are the

00:37:17,069 --> 00:37:21,799
same for all for all non production jobs

00:37:25,819 --> 00:37:33,329
any other questions

00:37:28,819 --> 00:37:35,729
so you apply this only for CPUs as a

00:37:33,329 --> 00:37:38,369
resource or have you applied it to other

00:37:35,729 --> 00:37:39,869
resources as well and would the strategy

00:37:38,369 --> 00:37:41,969
if you've not applied it would it work

00:37:39,869 --> 00:37:46,369
as well or would you need to tweak it

00:37:41,969 --> 00:37:51,349
for oversubscription of other resources

00:37:46,369 --> 00:37:51,349
so currently we only apply this to CPUs

00:37:51,769 --> 00:37:58,829
potentially there is an issue with

00:37:53,939 --> 00:38:02,549
network bandwidth like if we started

00:37:58,829 --> 00:38:07,169
with subscribing CPUs then we would also

00:38:02,549 --> 00:38:09,599
over subscribe the network but since we

00:38:07,169 --> 00:38:12,299
are not currently network bound for most

00:38:09,599 --> 00:38:15,439
of the jobs we believe that this

00:38:12,299 --> 00:38:19,169
wouldn't be the this wouldn't affect

00:38:15,439 --> 00:38:22,169
jobs in any way for the memory there is

00:38:19,169 --> 00:38:25,499
no of subscription obviously and if

00:38:22,169 --> 00:38:27,869
there are not if memory if there's

00:38:25,499 --> 00:38:29,819
memory shortage then we just wouldn't

00:38:27,869 --> 00:38:32,149
launch launch non production job on this

00:38:29,819 --> 00:38:32,149
host

00:38:37,540 --> 00:38:43,760
and then maybe I missed it um givenness

00:38:40,610 --> 00:38:46,040
new method that you use now what kind of

00:38:43,760 --> 00:38:49,220
CPU cluster utilization of references

00:38:46,040 --> 00:38:51,410
you achieve we don't have this in

00:38:49,220 --> 00:38:59,810
production yet we had our estimate is

00:38:51,410 --> 00:39:05,210
that we would be able to to reclaim at

00:38:59,810 --> 00:39:09,920
least 50% of CPUs used yeah like not 50

00:39:05,210 --> 00:39:16,900
but 30 to 40-percent CPUs used okay cool

00:39:09,920 --> 00:39:16,900
thank you any other questions

00:39:19,870 --> 00:39:26,480
so going back to the memory of a

00:39:22,910 --> 00:39:28,640
subscription like the controller right

00:39:26,480 --> 00:39:31,130
the Linux home controller provides

00:39:28,640 --> 00:39:35,000
mechanisms to kill kind of particular

00:39:31,130 --> 00:39:36,260
tasks like have you looked into MIT's

00:39:35,000 --> 00:39:40,100
have you started looking into the memory

00:39:36,260 --> 00:39:41,480
of a subscription ax or that you don't

00:39:40,100 --> 00:39:48,680
have in a plan the memory of a

00:39:41,480 --> 00:39:51,250
subscription so my memory of

00:39:48,680 --> 00:39:53,390
subscription we do not consider this

00:39:51,250 --> 00:39:55,550
you're not blind you don't have in the

00:39:53,390 --> 00:39:58,520
future planner something no because

00:39:55,550 --> 00:40:02,090
we're not memory bound right now like

00:39:58,520 --> 00:40:06,110
our main issue is that CPU allocation is

00:40:02,090 --> 00:40:08,540
quite high and there are still idle

00:40:06,110 --> 00:40:12,020
resources that so the we can postpone

00:40:08,540 --> 00:40:17,660
Hardware a preview buying more hardware

00:40:12,020 --> 00:40:20,650
to run the cluster but for memory there

00:40:17,660 --> 00:40:20,650
is just no need for now

00:40:29,070 --> 00:40:36,330
any other questions cool all right

00:40:32,710 --> 00:40:41,949
thanks symmetry thank you

00:40:36,330 --> 00:40:41,949

YouTube URL: https://www.youtube.com/watch?v=qrulRFp3UEY


