Title: My Application is Ready for Production... Now What?
Publication date: 2017-09-18
Playlist: MesosCon North America 2017
Description: 
	My Application is Ready for Production... Now What? - Gastón Kleiman, Johannes Unterstein, Kevin Klues, & Philip Norman, Mesosphere (limited spots, pre-registration suggested)

We'll explore the options offered by Mesos, Marathon, and DC/OS to try to answer all these questions and more: 
How do I deploy my application on a Mesos cluster? 
How can I monitor my application and make sure that everything stays running happily?
How can I monitor the underlying infrastructure? 
Help! Something strange is going on with my tasks! How do I debug them? 
Uhm, my tasks are now spread across my entire cluster… how do I deal with all the logs?
Wait, how do my services know how to talk to one another? 
How can I prevent downtime during deployments?

About 

Philip Norman
Software Engineer, Mesosphere
Philip runs the DC/OS day 2 operations working group, and is responsible for the DC/OS metrics project. Philip says: "Come talk to me about monitoring containers!"

Johannes Unterstein
Distributed Applications Engineer, Mesosphere, Inc.
Johannes Unterstein is a Distributed Applications Engineer at Mesosphere in Hamburg where he works on Mesosphere's Marathon project.

Gastón Kleiman
Mesosphere
Distributed Systems Engineer
San Francisco
Gastón Kleiman is a Senior Distributed Software Engineer at Mesosphere - he used to work on Marathon, but is focusing on Apache Mesos these days. Gastón fell in love with distributed systems while contracting for Google and working with Borg, MapReduce and other cool technology. That led him to work at Amazon Web Services on AWS OpsWorks, and on a real-time analytics system at LinkedIn. Speaking experience: MesosCon EU 2016: Securing a Marathon installation.

Kevin Klues
Mesosphere
Tech Lead Manager
San Francisco
Kevin Klues is a Tech Lead Manager at Mesosphere running the DC/OS ClusterOps team. Since joining Mesosphere, Kevin has been involved in the design and implementation of a number of Mesos’s core subsystems, including GPU isolation, Pods, the Mesos CLI and Attach/Exec support. He now leads a team of 10 Engineers working on everything from the DC/OS CLI to the installation, logging, backup/restore, and gathering / reporting of metrics and diagnostics of a running DC/OS cluster. When not working, you can usually find Kevin on a snowboard or up in the mountains in some capacity or another.
Captions: 
	00:00:00,030 --> 00:00:05,310
so yeah so my name is Kevin Clues I my

00:00:03,240 --> 00:00:06,359
background when I work at Mesa sphere so

00:00:05,310 --> 00:00:08,610
most of this stuff we're gonna be

00:00:06,359 --> 00:00:10,889
showing in this tutorial is around DCOs

00:00:08,610 --> 00:00:13,139
not necessarily just meso sand meso

00:00:10,889 --> 00:00:15,120
specific we're all employees in

00:00:13,139 --> 00:00:18,060
mesosphere when I first joined the

00:00:15,120 --> 00:00:21,300
company I was working on the Mesa core

00:00:18,060 --> 00:00:23,369
subsystem the the mesas you know makes

00:00:21,300 --> 00:00:25,230
its container Iser I added some GPU

00:00:23,369 --> 00:00:25,710
support tomatoes and things along those

00:00:25,230 --> 00:00:27,359
lines

00:00:25,710 --> 00:00:30,000
what I'm going to be talking about today

00:00:27,359 --> 00:00:31,650
though is the the task exec support that

00:00:30,000 --> 00:00:33,510
I added so the ability to sort of mimic

00:00:31,650 --> 00:00:35,160
the same functionality you get from

00:00:33,510 --> 00:00:37,320
something like docker attached or dr.

00:00:35,160 --> 00:00:39,570
exec but adding that capability to meso

00:00:37,320 --> 00:00:41,239
so you can go ahead and debug your

00:00:39,570 --> 00:00:43,770
applications once when she launched them

00:00:41,239 --> 00:00:45,450
so why don't I let these guys introduce

00:00:43,770 --> 00:00:49,170
themselves as well and then we'll jump

00:00:45,450 --> 00:00:50,760
into the presentation thank you my name

00:00:49,170 --> 00:00:54,289
is Johannes I'm also working for mrs.

00:00:50,760 --> 00:00:54,289
fear I'm working in the marathon project

00:00:56,719 --> 00:01:02,430
yeah and I'm really happy to show you

00:00:59,370 --> 00:01:04,320
today all about marathon and networking

00:01:02,430 --> 00:01:09,119
options you have when you use real phone

00:01:04,320 --> 00:01:11,909
especially when you use DC on top hello

00:01:09,119 --> 00:01:12,869
my name is Kirsten when I joined

00:01:11,909 --> 00:01:15,210
mesosphere

00:01:12,869 --> 00:01:17,729
I worked first on marathon and rid of

00:01:15,210 --> 00:01:20,220
Chronos and nowadays a mostly focused on

00:01:17,729 --> 00:01:22,170
I mean the core team so mostly on meso

00:01:20,220 --> 00:01:24,869
sandy for the executor has checks and

00:01:22,170 --> 00:01:28,500
stuff like that and that's why I'm going

00:01:24,869 --> 00:01:31,170
to talk about hi everyone my name is

00:01:28,500 --> 00:01:33,329
salad Norman I work on the metrics

00:01:31,170 --> 00:01:35,070
project in DC OS and I also run the day

00:01:33,329 --> 00:01:36,360
to ops working group I don't know if

00:01:35,070 --> 00:01:41,729
anyone's here from that but if you are

00:01:36,360 --> 00:01:43,020
hi okay back to you Kevin I think the

00:01:41,729 --> 00:01:44,880
feedback was coming from that other

00:01:43,020 --> 00:01:48,659
microphone so I'm gonna see how I do

00:01:44,880 --> 00:01:50,640
with with this one so just a quick sort

00:01:48,659 --> 00:01:52,710
of intro before we jump into the the

00:01:50,640 --> 00:01:55,200
main topics as most of you probably know

00:01:52,710 --> 00:01:56,549
deploying to production can be pretty

00:01:55,200 --> 00:01:59,250
hard when you're trying to run something

00:01:56,549 --> 00:02:01,170
on sophisticated sophisticated of a

00:01:59,250 --> 00:02:02,820
system as maysa and all the other moving

00:02:01,170 --> 00:02:03,810
parts that come along with that but it

00:02:02,820 --> 00:02:05,850
really doesn't have to be and you know

00:02:03,810 --> 00:02:07,380
when you do finally get to the point of

00:02:05,850 --> 00:02:08,940
deploying something into production if

00:02:07,380 --> 00:02:10,860
things go wrong it's not always obvious

00:02:08,940 --> 00:02:12,780
to figure out how to deal with those

00:02:10,860 --> 00:02:13,830
problems right so our goal of this

00:02:12,780 --> 00:02:15,030
tutorial is really to

00:02:13,830 --> 00:02:16,530
we want to teach you how to get your

00:02:15,030 --> 00:02:18,150
applications ready for production how do

00:02:16,530 --> 00:02:19,980
you configure them so that you can

00:02:18,150 --> 00:02:22,620
actually deploy them on a meso CCOs

00:02:19,980 --> 00:02:24,270
cluster so that they can run in a

00:02:22,620 --> 00:02:25,860
production environment and then show you

00:02:24,270 --> 00:02:27,660
some of the best practices for debugging

00:02:25,860 --> 00:02:29,190
your applications you know when things

00:02:27,660 --> 00:02:31,290
in everly go wrong during that process

00:02:29,190 --> 00:02:32,730
right so you know we want to move you

00:02:31,290 --> 00:02:34,260
from this from this point of things

00:02:32,730 --> 00:02:35,580
being hard to move you to the point of

00:02:34,260 --> 00:02:38,520
things being at least a little bit

00:02:35,580 --> 00:02:39,540
easier just a quick rundown or the

00:02:38,520 --> 00:02:41,820
different topics that we're gonna go

00:02:39,540 --> 00:02:43,620
through in this so yo.hannes is gonna

00:02:41,820 --> 00:02:45,180
start off talking about container

00:02:43,620 --> 00:02:47,340
networking how do you configure the

00:02:45,180 --> 00:02:49,260
different settings in your containers so

00:02:47,340 --> 00:02:50,580
that you can launch them and have you

00:02:49,260 --> 00:02:52,260
know the right neck networking between

00:02:50,580 --> 00:02:52,770
them for service discovery and things

00:02:52,260 --> 00:02:55,020
like this

00:02:52,770 --> 00:02:56,400
Gaston is then going to jump in to some

00:02:55,020 --> 00:02:58,260
of the health check work that we've that

00:02:56,400 --> 00:02:59,970
we've put together recently and

00:02:58,260 --> 00:03:01,350
including readiness checks to you know

00:02:59,970 --> 00:03:03,480
check the health of your applications to

00:03:01,350 --> 00:03:04,800
figure out if there's something wrong so

00:03:03,480 --> 00:03:07,200
that you can eventually then go in and

00:03:04,800 --> 00:03:09,090
try and debug that yo.hannes is then

00:03:07,200 --> 00:03:11,340
gonna talk again about debugging

00:03:09,090 --> 00:03:12,480
deployments so if you have problems when

00:03:11,340 --> 00:03:14,670
you actually go through the deployment

00:03:12,480 --> 00:03:16,530
phase of launching it by Avaya marathon

00:03:14,670 --> 00:03:18,330
what goes wrong how can you figure out

00:03:16,530 --> 00:03:19,739
what's wrong and then fix it then

00:03:18,330 --> 00:03:21,810
Philips gonna jump in and talk about

00:03:19,739 --> 00:03:24,510
some of the best pack best practices

00:03:21,810 --> 00:03:27,600
around using our metrics and logging

00:03:24,510 --> 00:03:28,650
services that we have in DC OS and how

00:03:27,600 --> 00:03:30,690
to actually get the most useful

00:03:28,650 --> 00:03:32,190
information about from those logs and

00:03:30,690 --> 00:03:34,650
metrics in order to debug things that

00:03:32,190 --> 00:03:38,459
are going wrong and then I'll conclude

00:03:34,650 --> 00:03:40,320
with an overview of how to use this new

00:03:38,459 --> 00:03:41,850
attach exec support that we built in so

00:03:40,320 --> 00:03:43,620
that you can you know launch a remote

00:03:41,850 --> 00:03:45,360
shell inside a container figure out

00:03:43,620 --> 00:03:46,860
things that are going wrong do you bug

00:03:45,360 --> 00:03:49,739
that and then you know get the thing

00:03:46,860 --> 00:03:51,269
running again if you know after after

00:03:49,739 --> 00:03:52,890
you've diagnosed what was wrong in the

00:03:51,269 --> 00:03:55,050
first place and then after that we'll

00:03:52,890 --> 00:03:57,480
jump into a interactive session with you

00:03:55,050 --> 00:03:59,489
guys so we've set up a DC u.s. cluster

00:03:57,480 --> 00:04:00,930
we've given we've created a bunch of

00:03:59,489 --> 00:04:02,610
user accounts on that and all of the

00:04:00,930 --> 00:04:04,830
tutorial all of the scripts and all of

00:04:02,610 --> 00:04:06,750
the configuration files that we that we

00:04:04,830 --> 00:04:08,250
put together that will be showing during

00:04:06,750 --> 00:04:10,470
these the different parts of this talk

00:04:08,250 --> 00:04:11,820
we'll make available via gist so that

00:04:10,470 --> 00:04:12,900
you can try and deploy them and play

00:04:11,820 --> 00:04:14,730
around with them and use some of the

00:04:12,900 --> 00:04:15,930
debugging tools that we've that we

00:04:14,730 --> 00:04:16,769
demonstrate to you you can play around

00:04:15,930 --> 00:04:21,120
with that yourself

00:04:16,769 --> 00:04:23,760
right okay so here's your harness yeah

00:04:21,120 --> 00:04:26,190
so before we're talking about production

00:04:23,760 --> 00:04:27,090
we need to make sure that our services

00:04:26,190 --> 00:04:30,090
like

00:04:27,090 --> 00:04:33,060
knows each other so when you're having a

00:04:30,090 --> 00:04:35,639
service which don't expose the expose I

00:04:33,060 --> 00:04:37,710
saw a port or don't consumer port it's

00:04:35,639 --> 00:04:40,889
basically something that's runs but

00:04:37,710 --> 00:04:42,650
don't have interaction with others but

00:04:40,889 --> 00:04:45,000
usually you're building like a

00:04:42,650 --> 00:04:47,220
service-oriented architecture with some

00:04:45,000 --> 00:04:48,960
micro services on data services and you

00:04:47,220 --> 00:04:51,900
need to make sure that you have

00:04:48,960 --> 00:04:54,720
producers offering services and they

00:04:51,900 --> 00:04:57,150
need to define a name or whatever to

00:04:54,720 --> 00:04:58,830
expose those services and on the other

00:04:57,150 --> 00:05:03,300
hand you have consumers who wants to

00:04:58,830 --> 00:05:06,960
talk to those services and the first

00:05:03,300 --> 00:05:09,270
thing or what what I saw when I was a

00:05:06,960 --> 00:05:12,000
user of these kind of services was that

00:05:09,270 --> 00:05:14,639
you have like hard-coded dependencies or

00:05:12,000 --> 00:05:16,770
names where you want to connect to but

00:05:14,639 --> 00:05:19,710
when you have a highly distributed and

00:05:16,770 --> 00:05:21,780
highly dynamic system like the CRS and

00:05:19,710 --> 00:05:24,120
all the services on top you need to make

00:05:21,780 --> 00:05:26,450
sure that you are able to dynamically

00:05:24,120 --> 00:05:29,550
change those configurations and make it

00:05:26,450 --> 00:05:31,320
configure it from the outside so again

00:05:29,550 --> 00:05:33,270
I'm from the marathon team so everything

00:05:31,320 --> 00:05:35,820
I show you today is basically this

00:05:33,270 --> 00:05:37,350
marathon JSON configuration and a

00:05:35,820 --> 00:05:39,090
marathon for an application this would

00:05:37,350 --> 00:05:41,010
look like something like this you have

00:05:39,090 --> 00:05:43,560
an environment property in your up

00:05:41,010 --> 00:05:46,320
definition and you say my database your

00:05:43,560 --> 00:05:48,539
I should be something like this and you

00:05:46,320 --> 00:05:51,270
see that here's a symbolic name

00:05:48,539 --> 00:05:53,789
configured like database on some random

00:05:51,270 --> 00:05:56,460
port for the my sequel database but how

00:05:53,789 --> 00:05:58,229
do we get there that we have those kind

00:05:56,460 --> 00:06:01,590
of symbolic names where I can connect it

00:05:58,229 --> 00:06:03,120
so here using missus marathon and

00:06:01,590 --> 00:06:06,060
especially DC s you have a couple of

00:06:03,120 --> 00:06:10,440
options to to solve this problem so the

00:06:06,060 --> 00:06:12,450
the first and probably all this an

00:06:10,440 --> 00:06:17,250
established solution was to me sustain

00:06:12,450 --> 00:06:20,130
us so when you launch a task in marathon

00:06:17,250 --> 00:06:22,800
you get a proper DNS name for the task

00:06:20,130 --> 00:06:25,169
and all running services for this but

00:06:22,800 --> 00:06:28,950
this is done via surf racket so DNS surf

00:06:25,169 --> 00:06:30,990
records so this is really really easy

00:06:28,950 --> 00:06:33,570
for the uh producer for service because

00:06:30,990 --> 00:06:35,130
you don't care about configuring

00:06:33,570 --> 00:06:37,229
something in your marathon applications

00:06:35,130 --> 00:06:38,360
it's just there one may sustain SS

00:06:37,229 --> 00:06:41,599
running

00:06:38,360 --> 00:06:43,430
but the downside is that only a few

00:06:41,599 --> 00:06:46,400
applications can handle those surf

00:06:43,430 --> 00:06:48,050
records and it's like not not really low

00:06:46,400 --> 00:06:49,849
balance so you need to connect all those

00:06:48,050 --> 00:06:53,449
together and it's done by your own and

00:06:49,849 --> 00:06:56,000
if we go to what is exposed it's

00:06:53,449 --> 00:06:58,520
probably like this so our application is

00:06:56,000 --> 00:07:01,310
service one it's the app ID in Marathon

00:06:58,520 --> 00:07:02,689
and if you're using this you get the DNS

00:07:01,310 --> 00:07:05,389
name service one dot Marathon that

00:07:02,689 --> 00:07:08,150
misses and then if you do a simple dig

00:07:05,389 --> 00:07:11,509
on this you get those one IP address

00:07:08,150 --> 00:07:13,520
back but if you have two services on the

00:07:11,509 --> 00:07:16,580
same host name running you get only one

00:07:13,520 --> 00:07:18,560
entry so you can do a dig for surf

00:07:16,580 --> 00:07:21,080
records and then you get additionally

00:07:18,560 --> 00:07:22,669
the hosts poll of this but again so you

00:07:21,080 --> 00:07:24,800
don't have load balancing so no

00:07:22,669 --> 00:07:26,509
automatic load balancing on this and in

00:07:24,800 --> 00:07:30,050
most system don't understand surf

00:07:26,509 --> 00:07:33,830
records so we can use a second version

00:07:30,050 --> 00:07:36,199
of this so we can use virtual IPS on the

00:07:33,830 --> 00:07:39,439
underlying technology we use in the USS

00:07:36,199 --> 00:07:41,210
Minuteman so what do you do with DC

00:07:39,439 --> 00:07:45,020
Earth's you say on the producer side

00:07:41,210 --> 00:07:48,879
that you want to export the service like

00:07:45,020 --> 00:07:51,500
a hostname and port combination and

00:07:48,879 --> 00:07:54,020
minutemen make sure without their self

00:07:51,500 --> 00:07:57,319
balancer in place and you can connect to

00:07:54,020 --> 00:08:00,860
those symbolic name and port combination

00:07:57,319 --> 00:08:02,509
and you you not only restrict it to

00:08:00,860 --> 00:08:06,349
symbolic names you could also use IP

00:08:02,509 --> 00:08:08,360
patterns for this but the downside is

00:08:06,349 --> 00:08:10,879
you need to install Minuteman all like

00:08:08,360 --> 00:08:12,139
used easier so it's prepackaged in DC RC

00:08:10,879 --> 00:08:14,449
if you're using these years you don't

00:08:12,139 --> 00:08:16,819
worry about anything and you have some

00:08:14,449 --> 00:08:19,279
DNS suffix so I show you what I mean

00:08:16,819 --> 00:08:20,860
with this so on the producer side you go

00:08:19,279 --> 00:08:23,060
to a marathon app definition and

00:08:20,860 --> 00:08:25,550
configure a port mapping you say you

00:08:23,060 --> 00:08:28,759
want to export you container port three

00:08:25,550 --> 00:08:31,819
Theo things and you can define labels so

00:08:28,759 --> 00:08:33,740
the first label VIP zero is a symbolic

00:08:31,819 --> 00:08:37,039
name you can say I want to export

00:08:33,740 --> 00:08:39,680
database on that port and every traffic

00:08:37,039 --> 00:08:42,219
in this DCOs cluster on this hostname

00:08:39,680 --> 00:08:44,540
port combination is mapped to a

00:08:42,219 --> 00:08:47,240
load-balanced a version of those

00:08:44,540 --> 00:08:49,250
marathon applications and the same is

00:08:47,240 --> 00:08:52,220
true for these IP patterns so you can

00:08:49,250 --> 00:08:54,139
define a virtual IP but were using which

00:08:52,220 --> 00:08:56,060
Pease you like you need to talk to other

00:08:54,139 --> 00:08:58,790
people in your team making sure that

00:08:56,060 --> 00:09:01,220
this IP is not used by anyone else and

00:08:58,790 --> 00:09:07,009
stuff like this so you I would use like

00:09:01,220 --> 00:09:09,769
this symbolic symbolic version and then

00:09:07,009 --> 00:09:12,170
as consumer you can use something like

00:09:09,769 --> 00:09:14,180
this you can feel having like want to

00:09:12,170 --> 00:09:18,529
connect your database just configure

00:09:14,180 --> 00:09:20,360
your JDBC string us3 306 on par 3 306

00:09:18,529 --> 00:09:22,459
and you're done

00:09:20,360 --> 00:09:25,490
if you have a database who supports load

00:09:22,459 --> 00:09:28,670
balancing if not you're only able to

00:09:25,490 --> 00:09:31,579
start one application basically and on

00:09:28,670 --> 00:09:35,389
the top you see what I saw what I named

00:09:31,579 --> 00:09:38,449
the DNS suffix so if you exposing the

00:09:35,389 --> 00:09:40,879
database symbolic name you get the PO

00:09:38,449 --> 00:09:44,120
suffix Marathon dot l4b dot this

00:09:40,879 --> 00:09:45,949
disaster directory this is something you

00:09:44,120 --> 00:09:49,100
need to know but when you're used to it

00:09:45,949 --> 00:09:51,500
you can define between this which will

00:09:49,100 --> 00:09:56,120
IP our services corruption and the

00:09:51,500 --> 00:09:59,870
others and the I show you now so the

00:09:56,120 --> 00:10:01,579
last one is what your networks and in

00:09:59,870 --> 00:10:03,949
this verse we use Napster for this and

00:10:01,579 --> 00:10:05,420
which one networks basically is you

00:10:03,949 --> 00:10:07,639
configure that your container should

00:10:05,420 --> 00:10:09,290
join an overlay network and this

00:10:07,639 --> 00:10:13,189
container will join this network and

00:10:09,290 --> 00:10:15,410
will have every I will have a IP per

00:10:13,189 --> 00:10:18,920
container so a dedicated IP for this

00:10:15,410 --> 00:10:22,399
container and every port is exposed so

00:10:18,920 --> 00:10:23,839
you don't need to do some magic port

00:10:22,399 --> 00:10:27,290
mapping so stuff like this

00:10:23,839 --> 00:10:30,199
everything is open for this so if you

00:10:27,290 --> 00:10:32,930
have technologies who rely on starting a

00:10:30,199 --> 00:10:35,209
service on a certain port let's say 8080

00:10:32,930 --> 00:10:37,339
for some traditional Java applications

00:10:35,209 --> 00:10:40,670
and you are not able to change this port

00:10:37,339 --> 00:10:42,829
for this application because like the

00:10:40,670 --> 00:10:45,110
application is crashes otherwise this is

00:10:42,829 --> 00:10:47,540
a good way to like have an own MP for

00:10:45,110 --> 00:10:48,019
your service exposing 8080 and you're

00:10:47,540 --> 00:10:51,319
done

00:10:48,019 --> 00:10:53,990
and again like you need to have enough

00:10:51,319 --> 00:10:55,939
store or DCs and also have a different

00:10:53,990 --> 00:10:59,899
DNS suffix for this kind of things

00:10:55,939 --> 00:11:02,630
ok this would be look like in Marathon

00:10:59,899 --> 00:11:06,170
so we configure a application with the

00:11:02,630 --> 00:11:08,720
ID database and say I had this couldn't

00:11:06,170 --> 00:11:11,769
should have an IP address and it coming

00:11:08,720 --> 00:11:14,540
from the overlay Network deceive us and

00:11:11,769 --> 00:11:16,250
I want to have two instances now because

00:11:14,540 --> 00:11:20,089
I want to show you how this would look

00:11:16,250 --> 00:11:23,060
like when I do a dick on this so if I

00:11:20,089 --> 00:11:26,329
now do a dick so now I have data based

00:11:23,060 --> 00:11:28,850
at marathon dot container a P dot d CS

00:11:26,329 --> 00:11:31,339
pong this D cos home directory so it

00:11:28,850 --> 00:11:33,560
gets a little bit longer now you would

00:11:31,339 --> 00:11:37,100
get the following answer you could would

00:11:33,560 --> 00:11:41,420
get two answers for an a record DNS a

00:11:37,100 --> 00:11:44,209
record again with the whole like host

00:11:41,420 --> 00:11:47,149
name over there it's too long for the

00:11:44,209 --> 00:11:48,889
slides so I like added three dots you

00:11:47,149 --> 00:11:52,579
would get database blah blah blah

00:11:48,889 --> 00:11:56,300
directory and then get a two IPS for

00:11:52,579 --> 00:11:59,750
your two containers for this okay so

00:11:56,300 --> 00:12:01,790
this is what we offer else like basics

00:11:59,750 --> 00:12:04,130
at four networking's and services korean

00:12:01,790 --> 00:12:06,949
options and ecr's but you can bring your

00:12:04,130 --> 00:12:08,750
own technologies like our console as a

00:12:06,949 --> 00:12:11,329
great services career technology and

00:12:08,750 --> 00:12:15,220
this used by many companies and really

00:12:11,329 --> 00:12:18,079
popular so there's a installation we can

00:12:15,220 --> 00:12:21,380
install console in your dcs cluster and

00:12:18,079 --> 00:12:23,209
use this for service discovery and if

00:12:21,380 --> 00:12:25,910
you don't like Napster you could also

00:12:23,209 --> 00:12:28,970
use we've we for works container

00:12:25,910 --> 00:12:32,120
networking solutions so this is really

00:12:28,970 --> 00:12:34,100
open to like to go for your own networks

00:12:32,120 --> 00:12:38,060
and known services cover etudes if you

00:12:34,100 --> 00:12:39,860
want to okay and I think when we're

00:12:38,060 --> 00:12:42,740
talking about health checks and

00:12:39,860 --> 00:12:45,170
networking I think the next August are I

00:12:42,740 --> 00:12:46,820
think you can just I want to hand over

00:12:45,170 --> 00:12:49,069
to you now while we're talking about

00:12:46,820 --> 00:12:50,930
networks and production re you probably

00:12:49,069 --> 00:12:52,699
want to configure how strikes three

00:12:50,930 --> 00:12:55,310
applications and Gaston will tell you

00:12:52,699 --> 00:12:57,170
about yeah so before I start talking

00:12:55,310 --> 00:13:00,140
about health checks I would want you

00:12:57,170 --> 00:13:01,550
know a bit more about you so can you

00:13:00,140 --> 00:13:05,060
please raise your hands if you have ever

00:13:01,550 --> 00:13:06,740
used mesos or marathon before okay and

00:13:05,060 --> 00:13:10,639
how many of you have deployed anything

00:13:06,740 --> 00:13:12,260
in production production okay it's about

00:13:10,639 --> 00:13:14,410
one third of the room something like

00:13:12,260 --> 00:13:14,410
that

00:13:15,640 --> 00:13:23,410
so whenever you deploy an application

00:13:21,060 --> 00:13:25,660
it's pretty easy for medicine for you to

00:13:23,410 --> 00:13:30,579
find out if the obligation is dead and

00:13:25,660 --> 00:13:32,170
if it crashes because then Martin will

00:13:30,579 --> 00:13:34,149
simply restarted or what else killer

00:13:32,170 --> 00:13:38,620
you're using will be studied and but

00:13:34,149 --> 00:13:40,839
what happens if your application gets

00:13:38,620 --> 00:13:41,860
stuck in a loop or cannot reach a

00:13:40,839 --> 00:13:43,959
database or something that and it

00:13:41,860 --> 00:13:45,850
doesn't exit it will still be up and

00:13:43,959 --> 00:13:49,959
running so messes my thing it's okay and

00:13:45,850 --> 00:13:52,269
it would not be restarted and it will

00:13:49,959 --> 00:13:56,230
still serve what you want so you have to

00:13:52,269 --> 00:13:58,839
somehow detect a condition and react and

00:13:56,230 --> 00:14:00,670
probably restart the tasks so how can

00:13:58,839 --> 00:14:02,260
you do it how can you detect that well

00:14:00,670 --> 00:14:07,209
that's where health checks come into

00:14:02,260 --> 00:14:09,279
play and if you use health checks then

00:14:07,209 --> 00:14:10,180
either marathon or meses depending on

00:14:09,279 --> 00:14:13,269
the kind of health checks that you're

00:14:10,180 --> 00:14:16,480
using will probe your application

00:14:13,269 --> 00:14:20,829
periodically your tasks and could be

00:14:16,480 --> 00:14:22,149
your via HTTP or TCP or running commands

00:14:20,829 --> 00:14:27,519
and the same namespaces as your

00:14:22,149 --> 00:14:28,720
application as your tasks and then meses

00:14:27,519 --> 00:14:33,640
or martin will keep track of how many

00:14:28,720 --> 00:14:36,790
failures aware and if your task fails

00:14:33,640 --> 00:14:38,709
the probes a certain amount of times

00:14:36,790 --> 00:14:42,100
that you can define then it will be

00:14:38,709 --> 00:14:43,480
healed and Martin will replace them or

00:14:42,100 --> 00:14:46,000
whatever our framework would replace

00:14:43,480 --> 00:14:47,529
them so if your application crashes by

00:14:46,000 --> 00:14:49,990
itself whenever there's a failure and

00:14:47,529 --> 00:14:52,510
you are 100% sure that that's going to

00:14:49,990 --> 00:14:54,300
happen then you need health checks but

00:14:52,510 --> 00:14:56,769
normally that's not the case normally

00:14:54,300 --> 00:14:58,209
you know you can hang or maybe there's

00:14:56,769 --> 00:15:00,339
there are some connectivity issues or

00:14:58,209 --> 00:15:04,990
something like that and then you will

00:15:00,339 --> 00:15:07,529
want to use health checks there's when

00:15:04,990 --> 00:15:10,570
you define health check you can define

00:15:07,529 --> 00:15:12,550
the protocol that would see an HTTP TCP

00:15:10,570 --> 00:15:14,140
or command and depending on the protocol

00:15:12,550 --> 00:15:17,050
that you're going to use then you can

00:15:14,140 --> 00:15:20,230
define the command to run or the port to

00:15:17,050 --> 00:15:23,949
connect you or the path to use when you

00:15:20,230 --> 00:15:25,630
in an HTTP request something important

00:15:23,949 --> 00:15:28,990
you said until that you have to this

00:15:25,630 --> 00:15:32,430
decide when you're writing your update

00:15:28,990 --> 00:15:36,280
is how often your application will be

00:15:32,430 --> 00:15:37,210
probed and you also define a time out

00:15:36,280 --> 00:15:39,490
because it might be that your

00:15:37,210 --> 00:15:41,410
application hangs forever answering a

00:15:39,490 --> 00:15:44,260
request so you only want to give some

00:15:41,410 --> 00:15:47,800
time and then if that passes you will

00:15:44,260 --> 00:15:50,100
consider that check failed and our

00:15:47,800 --> 00:15:54,750
important thing is the grace period

00:15:50,100 --> 00:15:59,170
marathon and meses will not consider

00:15:54,750 --> 00:16:01,060
failing checks as failures during that

00:15:59,170 --> 00:16:06,100
grace period that gives your application

00:16:01,060 --> 00:16:07,510
some time to initialize and listen on

00:16:06,100 --> 00:16:09,700
the part that has to listen to stuff

00:16:07,510 --> 00:16:12,000
like that so the first failures will be

00:16:09,700 --> 00:16:15,400
ignored until the application either

00:16:12,000 --> 00:16:18,310
succeeds check or the grace period

00:16:15,400 --> 00:16:19,570
passes and another very important one is

00:16:18,310 --> 00:16:21,640
the maximum consecutive failures that

00:16:19,570 --> 00:16:22,720
you want to allow that's Martha will

00:16:21,640 --> 00:16:25,120
keep track of how many times your

00:16:22,720 --> 00:16:26,770
application failed health checks and if

00:16:25,120 --> 00:16:30,990
it reaches that number then that task

00:16:26,770 --> 00:16:32,980
will be killed and now we'll replace it

00:16:30,990 --> 00:16:35,410
so if you configure this on your

00:16:32,980 --> 00:16:37,210
application then you're certain that it

00:16:35,410 --> 00:16:40,350
will not hang in a loop or something

00:16:37,210 --> 00:16:43,240
like that that it will be replaced and

00:16:40,350 --> 00:16:47,080
also the state of the hashtag is exposed

00:16:43,240 --> 00:16:49,510
through different api's and then your

00:16:47,080 --> 00:16:51,940
load balancing solution can look into

00:16:49,510 --> 00:16:55,510
that and decide where it's it wants to

00:16:51,940 --> 00:16:57,400
start serving traffic or not besides

00:16:55,510 --> 00:16:58,930
health checks there's something else

00:16:57,400 --> 00:17:02,680
that Mountain offers called readiness

00:16:58,930 --> 00:17:04,750
checks it's easy to confuse one do

00:17:02,680 --> 00:17:06,370
whatever and start between those two so

00:17:04,750 --> 00:17:11,860
I will explain again why we have that

00:17:06,370 --> 00:17:14,199
and how they work let's say that your

00:17:11,860 --> 00:17:15,699
task is up and running but at the

00:17:14,199 --> 00:17:18,070
beginning it needs to warm up it needs

00:17:15,699 --> 00:17:22,329
to load stuff populate the cache or

00:17:18,070 --> 00:17:25,510
something like that and your load

00:17:22,329 --> 00:17:27,370
balancer needs to know when it's ready

00:17:25,510 --> 00:17:29,110
our milestone also needs to know when

00:17:27,370 --> 00:17:31,360
it's ready let's say you're upgrading an

00:17:29,110 --> 00:17:32,770
application in its you know when that

00:17:31,360 --> 00:17:37,000
task is up and running so you can kill

00:17:32,770 --> 00:17:40,360
one of the old ones and how can you take

00:17:37,000 --> 00:17:42,530
that so you can use something very

00:17:40,360 --> 00:17:46,790
similar to what we did before so you

00:17:42,530 --> 00:17:48,260
and greatness checks again your

00:17:46,790 --> 00:17:49,730
application should expose through an

00:17:48,260 --> 00:17:52,430
endpoint or you can run a command or

00:17:49,730 --> 00:17:55,640
something like that and Marcin will

00:17:52,430 --> 00:17:57,230
probe that endpoint so in this case I'm

00:17:55,640 --> 00:18:00,590
only talking about marathon not Marcin

00:17:57,230 --> 00:18:03,410
and mesos because marathon right now

00:18:00,590 --> 00:18:07,490
doesn't support me missis checks that

00:18:03,410 --> 00:18:09,680
something was added recently so yeah for

00:18:07,490 --> 00:18:12,490
when you're using radius checks it's all

00:18:09,680 --> 00:18:15,200
done just by marathon and Martin will

00:18:12,490 --> 00:18:16,750
connect you try to connect to tasks and

00:18:15,200 --> 00:18:20,030
[Music]

00:18:16,750 --> 00:18:22,520
will evaluate the result but if those

00:18:20,030 --> 00:18:23,810
checks fail it won't kill the task

00:18:22,520 --> 00:18:25,790
that's the main difference between a

00:18:23,810 --> 00:18:28,580
health check and a readiness check if a

00:18:25,790 --> 00:18:30,260
hashtag fails and number of times then

00:18:28,580 --> 00:18:31,990
Martin will kill the task and replace it

00:18:30,260 --> 00:18:35,150
to something else

00:18:31,990 --> 00:18:36,830
but if redness checks are failing Martin

00:18:35,150 --> 00:18:38,300
will just not continue deploying your

00:18:36,830 --> 00:18:43,580
application it will wait for that task

00:18:38,300 --> 00:18:45,140
to be ready and then continue so if

00:18:43,580 --> 00:18:46,550
Redis checks are failing you will

00:18:45,140 --> 00:18:51,160
observe your deployment that it will get

00:18:46,550 --> 00:18:51,160
stuck but your task will not be killed

00:18:52,060 --> 00:18:56,450
so what you can configure here is

00:18:55,220 --> 00:19:00,860
similar to what you can do with health

00:18:56,450 --> 00:19:04,940
checks the same protocols are supported

00:19:00,860 --> 00:19:07,970
HTTP TCP and command you can also define

00:19:04,940 --> 00:19:09,680
how often and say the timeout the main

00:19:07,970 --> 00:19:13,580
difference that there's no crisis period

00:19:09,680 --> 00:19:15,490
because there's no like if it's failing

00:19:13,580 --> 00:19:18,650
at the beginning which has waited longer

00:19:15,490 --> 00:19:20,900
so there's no need to give it a grace

00:19:18,650 --> 00:19:22,640
period and again because it will not

00:19:20,900 --> 00:19:26,270
kill anything which has only wait for

00:19:22,640 --> 00:19:27,650
the first successful probe there's no

00:19:26,270 --> 00:19:30,470
need to set any maximum delay for

00:19:27,650 --> 00:19:32,090
failures and something I forgot to say

00:19:30,470 --> 00:19:34,790
before is that a marathon will stop

00:19:32,090 --> 00:19:38,620
doing rainiest checks as soon as the

00:19:34,790 --> 00:19:42,050
first one passes so it's just there to

00:19:38,620 --> 00:19:43,970
have marathon wait a bit longer and make

00:19:42,050 --> 00:19:45,590
sure it doesn't kill your existing tasks

00:19:43,970 --> 00:19:50,810
before the new ones that it just

00:19:45,590 --> 00:19:52,700
launched it already I mention that these

00:19:50,810 --> 00:19:56,060
things can be done either by my source

00:19:52,700 --> 00:19:56,720
or by marathon so I want you to go into

00:19:56,060 --> 00:19:58,220
some implementation

00:19:56,720 --> 00:20:02,360
leaders that might help you define

00:19:58,220 --> 00:20:03,950
what's better for you so yeah the dust

00:20:02,360 --> 00:20:08,809
can be brought by a scalar that would be

00:20:03,950 --> 00:20:11,750
marathon or Aurora or something else so

00:20:08,809 --> 00:20:13,940
that means remotely or the menses

00:20:11,750 --> 00:20:16,940
ability executors also support health

00:20:13,940 --> 00:20:19,760
checks and they can do the health checks

00:20:16,940 --> 00:20:24,140
locally and then send a frame on the

00:20:19,760 --> 00:20:26,000
results and when you create your

00:20:24,140 --> 00:20:28,789
application you can when you write your

00:20:26,000 --> 00:20:31,520
application you can choose which

00:20:28,789 --> 00:20:34,700
approach you want to use if you want to

00:20:31,520 --> 00:20:37,460
use for example HTTP checks health

00:20:34,700 --> 00:20:39,289
checks then you can use your HTTP as a

00:20:37,460 --> 00:20:41,480
protocol that's that means that Martin

00:20:39,289 --> 00:20:44,120
will do the HTTP checks or you can use

00:20:41,480 --> 00:20:45,890
Metis underscore HTTP they work in the

00:20:44,120 --> 00:20:48,350
same way but it's messes now doing the

00:20:45,890 --> 00:20:50,090
hash tags locally on the agent and that

00:20:48,350 --> 00:20:54,850
has implications that we will see in the

00:20:50,090 --> 00:20:58,010
next slides so there are some issues

00:20:54,850 --> 00:21:01,330
when with checks originating from the

00:20:58,010 --> 00:21:03,919
scalar from marathon one of them is that

00:21:01,330 --> 00:21:05,690
well the API Falacci experts from one

00:21:03,919 --> 00:21:07,940
firm Union next but that doesn't matter

00:21:05,690 --> 00:21:11,480
here because we're focusing mostly on

00:21:07,940 --> 00:21:15,890
marathons so we can ignore that part one

00:21:11,480 --> 00:21:18,140
of the main problems is that in Marathon

00:21:15,890 --> 00:21:21,110
health check Tara was performed by the

00:21:18,140 --> 00:21:22,789
leader so you always have one note who's

00:21:21,110 --> 00:21:25,130
trying to connect to all your tasks so

00:21:22,789 --> 00:21:26,659
if you have a big number of tasks it

00:21:25,130 --> 00:21:31,340
will shred a lot of extra network

00:21:26,659 --> 00:21:34,220
traffic and that can be a bottleneck and

00:21:31,340 --> 00:21:36,289
also written of failure and again

00:21:34,220 --> 00:21:38,600
because matthan will try to connect to

00:21:36,289 --> 00:21:41,289
all the tasks across your cluster if

00:21:38,600 --> 00:21:43,610
there's any network failure between

00:21:41,289 --> 00:21:45,710
separating marathon from your tasks and

00:21:43,610 --> 00:21:47,419
the hashtags will fail but that doesn't

00:21:45,710 --> 00:21:49,610
mean that your tasks are not healthy

00:21:47,419 --> 00:21:50,720
could be that there's a network

00:21:49,610 --> 00:21:52,730
partition or something that that's

00:21:50,720 --> 00:21:54,770
harmless for users and only affects the

00:21:52,730 --> 00:21:58,460
connectivity within marathon and the

00:21:54,770 --> 00:21:59,780
nodes and so you can give you false

00:21:58,460 --> 00:22:04,640
positives or what negatives like

00:21:59,780 --> 00:22:07,309
unhealthy things and we did some scale

00:22:04,640 --> 00:22:09,440
tests after we after I add support for

00:22:07,309 --> 00:22:11,230
men's health checks and

00:22:09,440 --> 00:22:13,640
[Music]

00:22:11,230 --> 00:22:15,590
they scaled much we're in the math of

00:22:13,640 --> 00:22:17,270
health checks because again you can

00:22:15,590 --> 00:22:20,630
distribute the load horizontally across

00:22:17,270 --> 00:22:26,510
your agents Asian checks its own task

00:22:20,630 --> 00:22:28,100
and that's gets better so yeah so just

00:22:26,510 --> 00:22:31,100
of those issues we introduced messes

00:22:28,100 --> 00:22:32,810
native health checks if you use them

00:22:31,100 --> 00:22:35,570
then the hash tagging is not centralized

00:22:32,810 --> 00:22:39,890
it's done my aeration and that runs your

00:22:35,570 --> 00:22:42,620
tasks it's guys whether you can use

00:22:39,890 --> 00:22:44,660
command checks command checks always ran

00:22:42,620 --> 00:22:46,700
on the agent anyways there's no way of

00:22:44,660 --> 00:22:48,320
making Martin run a command on the test

00:22:46,700 --> 00:22:52,670
so it's that that has always been

00:22:48,320 --> 00:22:56,030
delegated to the nodes but in studying

00:22:52,670 --> 00:22:59,750
with message 1.3 also HTTP and TCP has

00:22:56,030 --> 00:23:02,300
checked on radiations and all built-in

00:22:59,750 --> 00:23:06,500
executor support all of these checks the

00:23:02,300 --> 00:23:08,900
docker executor the regular message

00:23:06,500 --> 00:23:11,030
executor welcome annex headers is a name

00:23:08,900 --> 00:23:14,840
and also the new default executor is

00:23:11,030 --> 00:23:16,640
mostly used for ports but most

00:23:14,840 --> 00:23:20,990
frameworks and were closer my rating

00:23:16,640 --> 00:23:23,960
sheet there are some limitations with

00:23:20,990 --> 00:23:27,110
message X anyways and more on

00:23:23,960 --> 00:23:28,700
limitations like trade-offs since they

00:23:27,110 --> 00:23:31,490
run now on the agents they consume more

00:23:28,700 --> 00:23:34,670
resources and on the agents so you need

00:23:31,490 --> 00:23:36,440
to prepare for that and reserve extra

00:23:34,670 --> 00:23:37,940
resources for them normally they should

00:23:36,440 --> 00:23:39,350
be lightweight but in the case of

00:23:37,940 --> 00:23:40,940
command has six you can run any command

00:23:39,350 --> 00:23:43,280
so if you run something very expensive

00:23:40,940 --> 00:23:45,440
it will run in the same namespace as

00:23:43,280 --> 00:23:47,720
your task so it could potentially start

00:23:45,440 --> 00:23:49,790
it so you should be careful and either

00:23:47,720 --> 00:23:51,950
allocate more resources or use

00:23:49,790 --> 00:23:55,910
lightweight checks and I weight commands

00:23:51,950 --> 00:24:00,260
and they always connect your localhost

00:23:55,910 --> 00:24:02,720
in a case of HTTP and TCP checks so your

00:24:00,260 --> 00:24:08,500
application must always listen log at

00:24:02,720 --> 00:24:10,820
host yes mother she said they are not

00:24:08,500 --> 00:24:12,620
able to network glitches or stuff like

00:24:10,820 --> 00:24:16,630
that but you have to make sure your

00:24:12,620 --> 00:24:19,700
application listens on that IB and

00:24:16,630 --> 00:24:22,370
because message will fork processes to

00:24:19,700 --> 00:24:23,410
run those checks eating goes from our

00:24:22,370 --> 00:24:26,230
head

00:24:23,410 --> 00:24:27,760
so again you have to plan for that and

00:24:26,230 --> 00:24:34,480
make sure the orations have enough

00:24:27,760 --> 00:24:39,910
capacity for that well I want to make a

00:24:34,480 --> 00:24:41,890
short demo of how how they work I would

00:24:39,910 --> 00:24:43,510
just scream because I need both hands to

00:24:41,890 --> 00:24:55,420
type and they're going to use this

00:24:43,510 --> 00:24:57,010
microphone let me go to that slide so we

00:24:55,420 --> 00:24:59,740
have our class first a data you can use

00:24:57,010 --> 00:25:02,230
at the end to try these things out I'm

00:24:59,740 --> 00:25:03,580
going to use the same cluster to do this

00:25:02,230 --> 00:25:06,790
demo

00:25:03,580 --> 00:25:14,650
it's a DCOs cluster and once you connect

00:25:06,790 --> 00:25:15,160
to it first it's a live demo so things

00:25:14,650 --> 00:25:19,780
can go wrong

00:25:15,160 --> 00:25:25,080
hopefully not connectivity seems to be

00:25:19,780 --> 00:25:28,570
quite slow okay so I'm logged in as

00:25:25,080 --> 00:25:31,480
admin and what I'm going to do is I have

00:25:28,570 --> 00:25:33,040
a very demo very small demo application

00:25:31,480 --> 00:25:35,620
that you can deploy on any class or you

00:25:33,040 --> 00:25:38,020
can run it locally it has a webserver

00:25:35,620 --> 00:25:41,610
and exposes two endpoints one for hash

00:25:38,020 --> 00:25:43,480
tags one for NS checks and we can see

00:25:41,610 --> 00:25:45,940
details about the request to those

00:25:43,480 --> 00:25:47,920
things and we can also make them fail

00:25:45,940 --> 00:25:50,470
and then we can see how marathon and

00:25:47,920 --> 00:25:54,310
necessary actually those failures I'm

00:25:50,470 --> 00:25:59,830
going to first deploy and be basic

00:25:54,310 --> 00:26:03,400
version of that cheese with the JSON

00:25:59,830 --> 00:26:07,740
ready you will be able to use again this

00:26:03,400 --> 00:26:12,250
again at the end and play with it a bit

00:26:07,740 --> 00:26:14,910
so let me quickly deploy the basic

00:26:12,250 --> 00:26:14,910
application first

00:26:21,660 --> 00:26:25,800
so it's a docker container that you

00:26:23,910 --> 00:26:29,100
listen to now fixed port and then that's

00:26:25,800 --> 00:26:32,130
exposed in a dynamic port in Emerson

00:26:29,100 --> 00:26:33,990
that's what this host port 0 does and we

00:26:32,130 --> 00:26:37,770
have marcin lb running on this cluster

00:26:33,990 --> 00:26:38,900
and Marcin L we will let you get to this

00:26:37,770 --> 00:26:43,490
application

00:26:38,900 --> 00:26:46,170
through the public IP I have to set here

00:26:43,490 --> 00:26:48,150
the poor that they will listen on so we

00:26:46,170 --> 00:26:57,180
can see define it so we're saying edge

00:26:48,150 --> 00:27:03,180
should listen on port 80 and I'm missing

00:26:57,180 --> 00:27:09,390
a closing thank you so we're going to

00:27:03,180 --> 00:27:10,740
run this should be pretty quickly it

00:27:09,390 --> 00:27:12,000
doesn't say it's healthy because we

00:27:10,740 --> 00:27:13,920
didn't add any health checks there it

00:27:12,000 --> 00:27:15,810
only say that it's running so now I'm

00:27:13,920 --> 00:27:24,240
going to connect sure they let me get

00:27:15,810 --> 00:27:25,890
the IP of the publication okay so this

00:27:24,240 --> 00:27:32,190
is the application it's very simple you

00:27:25,890 --> 00:27:35,070
can go to hashtags and say that the next

00:27:32,190 --> 00:27:37,350
end should fail or not and if you go to

00:27:35,070 --> 00:27:39,090
healthy endpoint then you can see that

00:27:37,350 --> 00:27:43,430
so on the request which I be it came

00:27:39,090 --> 00:27:46,710
from and which result we returned and

00:27:43,430 --> 00:27:48,600
yeah so I'm going to use this and say

00:27:46,710 --> 00:27:51,120
that you fail in the first end checks

00:27:48,600 --> 00:27:55,740
and then we can see how that affects

00:27:51,120 --> 00:27:56,970
marathon let's do it quickly I'm going

00:27:55,740 --> 00:27:59,180
to use an Arab definition or they have

00:27:56,970 --> 00:27:59,180
ready

00:28:03,300 --> 00:28:08,450
obviously we can use the same one and

00:28:05,130 --> 00:28:08,450
just odd how this checks amazing

00:28:09,410 --> 00:28:12,739
[Music]

00:28:19,070 --> 00:28:25,370
so we're adding here some arguments that

00:28:23,400 --> 00:28:28,530
will make it fail the first three checks

00:28:25,370 --> 00:28:30,179
we're going to use messes and checks so

00:28:28,530 --> 00:28:34,110
we will see that each checks are coming

00:28:30,179 --> 00:28:36,450
from localhost from the agent and here

00:28:34,110 --> 00:28:39,780
we set the path it's healthy such

00:28:36,450 --> 00:28:42,179
healthy and we're not even going to give

00:28:39,780 --> 00:28:43,800
it in any grace period so if it fails

00:28:42,179 --> 00:28:44,210
from the beginning then it will be

00:28:43,800 --> 00:28:46,440
killed

00:28:44,210 --> 00:28:50,250
and we're going to make the checks every

00:28:46,440 --> 00:28:52,260
five seconds so I expect these two I

00:28:50,250 --> 00:28:53,760
expect respect margin to kill the

00:28:52,260 --> 00:28:55,320
application after 15 seconds because we

00:28:53,760 --> 00:28:57,030
are telling it to fail the first three

00:28:55,320 --> 00:29:00,480
checks and we're not giving it any great

00:28:57,030 --> 00:29:01,920
spirit let's see if it works and that's

00:29:00,480 --> 00:29:03,660
why it's something you don't want in

00:29:01,920 --> 00:29:05,520
production young one is to happen to you

00:29:03,660 --> 00:29:06,590
because this means that the implement

00:29:05,520 --> 00:29:08,970
will never succeed

00:29:06,590 --> 00:29:11,400
after 15 seconds your task will be

00:29:08,970 --> 00:29:13,800
killed so we can discuss or I can show

00:29:11,400 --> 00:29:18,929
you how to fix that so you can see that

00:29:13,800 --> 00:29:22,170
the task is running and now it's

00:29:18,929 --> 00:29:26,340
considered unhealthy and it will be

00:29:22,170 --> 00:29:28,760
killed soon and rebase way now I have to

00:29:26,340 --> 00:29:28,760
refresh it

00:29:37,740 --> 00:29:42,120
so yeah so this is not northern healthy

00:29:40,770 --> 00:29:44,159
Blissett there's a narrow obligation now

00:29:42,120 --> 00:29:45,899
if you see all of them one what skills

00:29:44,159 --> 00:29:48,149
that was the failing one I was replaced

00:29:45,899 --> 00:29:49,200
by another one and this will happen all

00:29:48,149 --> 00:29:51,450
the time because we're not giving your

00:29:49,200 --> 00:29:53,669
application enough time or yeah we're

00:29:51,450 --> 00:29:55,110
not waiting long enough for it to become

00:29:53,669 --> 00:29:57,270
healthy so if we want to fix this

00:29:55,110 --> 00:30:02,789
situation what we have to do is just

00:29:57,270 --> 00:30:05,520
increase the grace period so in this

00:30:02,789 --> 00:30:09,690
case we're making it fail only three

00:30:05,520 --> 00:30:10,980
times so if we wait and we're checking

00:30:09,690 --> 00:30:12,570
every five seconds so if we wait at

00:30:10,980 --> 00:30:13,590
least 15 seconds then you your

00:30:12,570 --> 00:30:15,410
application will be healthy and then

00:30:13,590 --> 00:30:16,640
your deployment should succeed

00:30:15,410 --> 00:30:19,080
[Music]

00:30:16,640 --> 00:30:21,690
so just to be sure I will change this to

00:30:19,080 --> 00:30:23,940
20 seconds not 15 seconds but 15 should

00:30:21,690 --> 00:30:26,250
be enough let's deploy this and then

00:30:23,940 --> 00:30:30,600
wait for the application to become

00:30:26,250 --> 00:30:31,590
healthy afterwards you can play with the

00:30:30,600 --> 00:30:33,360
class and we can also play with

00:30:31,590 --> 00:30:35,340
rightness checks and see how instead of

00:30:33,360 --> 00:30:38,520
killing the tasks marathon we'll just

00:30:35,340 --> 00:30:42,299
wait for it that's something you can do

00:30:38,520 --> 00:30:46,559
yourself you can play with that later so

00:30:42,299 --> 00:30:48,240
we can see it it felt checks but if you

00:30:46,559 --> 00:30:51,210
wait long enough they're with us so we

00:30:48,240 --> 00:30:56,850
can go to the application again you

00:30:51,210 --> 00:30:59,820
close it but first we have to wait on

00:30:56,850 --> 00:31:03,320
the league application is deployed and

00:30:59,820 --> 00:31:03,320
they containers fetch standard stuff

00:31:07,669 --> 00:31:12,389
and we can see this no passing checks

00:31:10,679 --> 00:31:14,399
so the first three failed but there's no

00:31:12,389 --> 00:31:16,440
passing checks and if we look now in the

00:31:14,399 --> 00:31:18,419
DCO ITI we can see it's healthy and then

00:31:16,440 --> 00:31:22,350
the deployment is succeeded the

00:31:18,419 --> 00:31:25,380
application is up and running i could

00:31:22,350 --> 00:31:26,880
mitigate well yeah we can wait 20

00:31:25,380 --> 00:31:29,880
seconds so I can make it fail the next

00:31:26,880 --> 00:31:31,110
five checks and then in 15 seconds these

00:31:29,880 --> 00:31:34,380
tasks shouldn't be going should be

00:31:31,110 --> 00:31:39,690
killed by by marathon we can see I did

00:31:34,380 --> 00:31:40,769
ooh and unhealthy in this this could

00:31:39,690 --> 00:31:41,850
mean that your application in production

00:31:40,769 --> 00:31:44,159
for example is not responding to

00:31:41,850 --> 00:31:45,330
requests so I will take a few seconds

00:31:44,159 --> 00:31:47,610
depending on your settings we take that

00:31:45,330 --> 00:31:49,379
but now you can see Martha NIDA get that

00:31:47,610 --> 00:31:51,019
and now it's running in our instance and

00:31:49,379 --> 00:31:57,110
waiting for it to be get healthy and

00:31:51,019 --> 00:32:02,990
that's pretty much yeah the demo oh

00:31:57,110 --> 00:32:05,759
let's let's tie to DCs you I may be too

00:32:02,990 --> 00:32:08,190
if if you're at that point you're really

00:32:05,759 --> 00:32:09,629
happy so you mrs. Zimmerman was able

00:32:08,190 --> 00:32:11,519
through start your applications but

00:32:09,629 --> 00:32:13,649
sometimes you have a problem that you

00:32:11,519 --> 00:32:15,600
have a starving deployment and your

00:32:13,649 --> 00:32:19,019
applications never get launched and

00:32:15,600 --> 00:32:20,970
another fun one for we introduced a nice

00:32:19,019 --> 00:32:22,889
feature to like make it a little bit

00:32:20,970 --> 00:32:27,740
more easy to you to debug these kind of

00:32:22,889 --> 00:32:27,740
situations when you start a service

00:32:29,929 --> 00:32:41,610
start a new one and say we call it

00:32:35,570 --> 00:32:44,039
service 1 and we say a simple the sleepy

00:32:41,610 --> 00:32:47,490
application and no kentucker container

00:32:44,039 --> 00:32:49,919
something involved but you accidentally

00:32:47,490 --> 00:32:52,740
said that you want to have 10 CPUs for

00:32:49,919 --> 00:32:55,259
this application so what happens

00:32:52,740 --> 00:32:57,929
marathon away time so the marathon will

00:32:55,259 --> 00:32:59,519
receive an offer containing 10 CPUs but

00:32:57,929 --> 00:33:03,149
it's really like unlikely in this

00:32:59,519 --> 00:33:04,889
cluster that this will ever happen so we

00:33:03,149 --> 00:33:07,470
see that we're waiting and we're waiting

00:33:04,889 --> 00:33:11,100
and we were like - Christmas for this

00:33:07,470 --> 00:33:13,169
and what we introduced in marathon we

00:33:11,100 --> 00:33:16,350
introduced a REST API where you can

00:33:13,169 --> 00:33:18,899
inspect the current deployments and we

00:33:16,350 --> 00:33:29,929
have a DTS cluster you can go to

00:33:18,899 --> 00:33:37,830
marathon and go to service service some

00:33:29,929 --> 00:33:38,759
v2q oh thank you very much it's missing

00:33:37,830 --> 00:33:43,889
an age okay

00:33:38,759 --> 00:33:46,679
so what you will see is that you have a

00:33:43,889 --> 00:33:48,570
pretty nice format adjacent file and you

00:33:46,679 --> 00:33:51,149
see the processed offers marathon has

00:33:48,570 --> 00:33:54,809
seen so you see a summary of the last

00:33:51,149 --> 00:33:57,840
offers so you see that you received six

00:33:54,809 --> 00:33:59,759
offers and in the last time and one was

00:33:57,840 --> 00:34:02,909
declined because of an unmatching role

00:33:59,759 --> 00:34:05,460
so in this cluster we do have seven

00:34:02,909 --> 00:34:08,429
agents six private ones and one public

00:34:05,460 --> 00:34:10,530
ones probably no 5 pride ones and one

00:34:08,429 --> 00:34:12,389
public so six in total and you are

00:34:10,530 --> 00:34:16,429
constantly declining one because it's

00:34:12,389 --> 00:34:18,960
the public agent the wrong role and no

00:34:16,429 --> 00:34:21,240
offer was declined because of an

00:34:18,960 --> 00:34:21,569
unfulfilled constrained and so on and so

00:34:21,240 --> 00:34:25,319
forth

00:34:21,569 --> 00:34:28,280
and you have constantly declined offers

00:34:25,319 --> 00:34:31,679
because the cpu count is not matching

00:34:28,280 --> 00:34:34,319
so this summary of the last or first

00:34:31,679 --> 00:34:37,260
cycle of your cluster and on the other

00:34:34,319 --> 00:34:39,300
side you get an aggregated view on your

00:34:37,260 --> 00:34:42,419
launch attempts on you on your offers

00:34:39,300 --> 00:34:44,819
you saw and like when I would refresh it

00:34:42,419 --> 00:34:47,339
now the number of processed offers would

00:34:44,819 --> 00:34:50,520
be increased and you see that you and

00:34:47,339 --> 00:34:52,980
hopefuls or 18 offers and you decline 3

00:34:50,520 --> 00:34:56,429
because of the role and so on and you

00:34:52,980 --> 00:35:00,329
saw 15 offers and decline all 15 of them

00:34:56,429 --> 00:35:02,700
because of the unmatching cpu configured

00:35:00,329 --> 00:35:05,220
CPUs so if you see this it's probably

00:35:02,700 --> 00:35:07,859
really easy for for us developer or an

00:35:05,220 --> 00:35:10,589
administrator to like give into and see

00:35:07,859 --> 00:35:12,960
oh I need to fix the CPU count to get

00:35:10,589 --> 00:35:15,510
this application running and the great

00:35:12,960 --> 00:35:18,540
thing is you don't need to if you're

00:35:15,510 --> 00:35:21,000
using DCs you don't need to go to the

00:35:18,540 --> 00:35:26,339
REST API to see this there's a proper UI

00:35:21,000 --> 00:35:28,310
for this and also a good seal I see a

00:35:26,339 --> 00:35:30,840
Thai interpretation of this so

00:35:28,310 --> 00:35:33,359
or you go to the UI you need to go to

00:35:30,840 --> 00:35:37,740
the debug tab and then you see these

00:35:33,359 --> 00:35:40,200
summary of a summary of offers you see

00:35:37,740 --> 00:35:42,540
the declination because of the roll and

00:35:40,200 --> 00:35:45,690
really present they took the Nate the

00:35:42,540 --> 00:35:48,810
decline because of the CPUs and like

00:35:45,690 --> 00:35:50,700
this table showing all your notes and

00:35:48,810 --> 00:35:52,770
you see that you have all the red

00:35:50,700 --> 00:35:54,540
crosses on the CPU column so it's really

00:35:52,770 --> 00:35:59,430
obvious that the fix the CPU on this

00:35:54,540 --> 00:36:04,609
side and we go back to the slides this

00:35:59,430 --> 00:36:10,500
is also in the slides that sharks

00:36:04,609 --> 00:36:14,070
rotation yeah this one and if you would

00:36:10,500 --> 00:36:16,830
use the Scylla I thank you very much you

00:36:14,070 --> 00:36:18,630
could do DC Marathon debug list and then

00:36:16,830 --> 00:36:21,390
you get an overview of all your

00:36:18,630 --> 00:36:24,540
applications marathon's crud currently

00:36:21,390 --> 00:36:26,220
trying to start in an example it's a CPU

00:36:24,540 --> 00:36:28,260
tasks we want to launch one instance and

00:36:26,220 --> 00:36:30,180
you're waiting and you process six

00:36:28,260 --> 00:36:33,150
offers and didn't use anything of that

00:36:30,180 --> 00:36:35,520
and if you go to details and then enter

00:36:33,150 --> 00:36:40,770
the app ID you get the same table and

00:36:35,520 --> 00:36:43,200
CLI as you would see in in the UI okay

00:36:40,770 --> 00:36:45,840
so this is like if you're having a stock

00:36:43,200 --> 00:36:48,500
deployment and don't know why oh we see

00:36:45,840 --> 00:36:48,500
we have a question

00:37:00,420 --> 00:37:06,329
okay so the question was if if we would

00:37:03,210 --> 00:37:07,799
add a CPU account to this table as the

00:37:06,329 --> 00:37:16,200
overview GPU yeah

00:37:07,799 --> 00:37:20,130
GPUs so there was an GPU feature in in

00:37:16,200 --> 00:37:23,039
the in the past so if you're using the

00:37:20,130 --> 00:37:25,829
the rest endpoint there GPUs listed as

00:37:23,039 --> 00:37:27,240
well as well as if an offer is declined

00:37:25,829 --> 00:37:29,700
because you don't have the needed

00:37:27,240 --> 00:37:32,190
resource or resource reservations for

00:37:29,700 --> 00:37:34,430
this if you're using like local per

00:37:32,190 --> 00:37:38,250
system William so stuff like this I

00:37:34,430 --> 00:37:42,480
think it's there soon j-rod a charity

00:37:38,250 --> 00:37:44,430
could open to our GPUs as well so GPUs

00:37:42,480 --> 00:37:46,289
is fully supported the calculation is

00:37:44,430 --> 00:37:50,460
everything in place I think it's just

00:37:46,289 --> 00:37:51,920
not displayed currently in the CLI okay

00:37:50,460 --> 00:37:58,019
thank you

00:37:51,920 --> 00:37:59,670
great logging in metrics all right if

00:37:58,019 --> 00:38:05,390
you have any questions just feel free to

00:37:59,670 --> 00:38:09,000
raise your hand and shoot okay thank you

00:38:05,390 --> 00:38:12,059
cool hey everybody and so yeah metrics

00:38:09,000 --> 00:38:14,099
and logging so the first kind of

00:38:12,059 --> 00:38:16,230
question is like what what are they for

00:38:14,099 --> 00:38:17,880
they good for and what's the difference

00:38:16,230 --> 00:38:20,900
because they're both kind of like

00:38:17,880 --> 00:38:23,220
outputs from your application and

00:38:20,900 --> 00:38:24,630
they're both timestamps and they both

00:38:23,220 --> 00:38:26,970
provide some insight into how your

00:38:24,630 --> 00:38:27,809
applications performing and so you

00:38:26,970 --> 00:38:29,279
wanted to call out a couple of

00:38:27,809 --> 00:38:31,890
differences before diving into how they

00:38:29,279 --> 00:38:35,579
how they might work and logs are

00:38:31,890 --> 00:38:38,789
specific and metrics a generic so a log

00:38:35,579 --> 00:38:41,069
might say something like a user logged

00:38:38,789 --> 00:38:44,009
in and this is their ID where as a

00:38:41,069 --> 00:38:45,869
metric might be for the same event might

00:38:44,009 --> 00:38:49,529
be this many users logged in in the last

00:38:45,869 --> 00:38:52,109
minute logs are generally text metrics

00:38:49,529 --> 00:38:53,849
are generally numeric so it's reasonable

00:38:52,109 --> 00:38:55,170
for me to go and read a log file but

00:38:53,849 --> 00:38:58,230
it's kind of unreasonable for me to go

00:38:55,170 --> 00:39:00,089
and read a whole heap of metrics because

00:38:58,230 --> 00:39:06,779
I'm gonna want them pre-process somehow

00:39:00,089 --> 00:39:08,130
first and there is one particular thing

00:39:06,779 --> 00:39:09,790
that they have in common that's that you

00:39:08,130 --> 00:39:12,250
can monitor

00:39:09,790 --> 00:39:17,500
them you can monitor your application by

00:39:12,250 --> 00:39:19,300
looking at their outputs so a metric

00:39:17,500 --> 00:39:22,960
might help you detect an outlier when

00:39:19,300 --> 00:39:24,370
your applications performing weirdly you

00:39:22,960 --> 00:39:26,230
kind of hope that your metrics always

00:39:24,370 --> 00:39:27,760
going to be pretty dull if you're doing

00:39:26,230 --> 00:39:30,040
a lot of batch processing you might see

00:39:27,760 --> 00:39:32,020
Peaks sustained Peaks and then back down

00:39:30,040 --> 00:39:33,430
again and if you're running a social

00:39:32,020 --> 00:39:35,640
network you're going to see like a spike

00:39:33,430 --> 00:39:38,590
at 11 a.m. and a spike at 4 p.m. because

00:39:35,640 --> 00:39:39,850
people don't like working and it's when

00:39:38,590 --> 00:39:40,870
you see a deviation from that pattern

00:39:39,850 --> 00:39:43,810
that you know that you've got an

00:39:40,870 --> 00:39:45,700
exception emerging and if you get smart

00:39:43,810 --> 00:39:48,520
with your error bounds you can you can

00:39:45,700 --> 00:39:51,340
have a situation where something

00:39:48,520 --> 00:39:52,780
develops and before anything catches

00:39:51,340 --> 00:39:54,400
fire at all you know that something's

00:39:52,780 --> 00:39:55,510
going to go wrong and you can get in

00:39:54,400 --> 00:39:58,540
there and be fixing it before that

00:39:55,510 --> 00:40:00,360
happens and so you can catch things like

00:39:58,540 --> 00:40:02,860
your disks are filling up with logs

00:40:00,360 --> 00:40:05,070
before your disks completely fill up and

00:40:02,860 --> 00:40:09,010
freeze everything up

00:40:05,070 --> 00:40:11,320
and logs help you debug exceptions so

00:40:09,010 --> 00:40:13,180
after something's occurred you can go in

00:40:11,320 --> 00:40:16,330
and look at a particular a particular

00:40:13,180 --> 00:40:19,360
event so in this case somebody from

00:40:16,330 --> 00:40:21,640
Chrome OS logged in and looked at groups

00:40:19,360 --> 00:40:23,260
in marathon and I know their IP address

00:40:21,640 --> 00:40:25,440
and I know exactly what they did to

00:40:23,260 --> 00:40:29,440
cause whatever whatever happened next

00:40:25,440 --> 00:40:31,630
but you have a problem and that's that

00:40:29,440 --> 00:40:35,050
there are a lot of logs this is the

00:40:31,630 --> 00:40:36,780
output this is regular output from a

00:40:35,050 --> 00:40:40,810
node that's not doing all that much

00:40:36,780 --> 00:40:42,640
which I took this gift this morning and

00:40:40,810 --> 00:40:43,810
so you're just looking at a lot of data

00:40:42,640 --> 00:40:47,110
when you're when you're looking for your

00:40:43,810 --> 00:40:52,360
logs and picking exceptions out can be

00:40:47,110 --> 00:40:55,060
pretty hard so we give you some tools to

00:40:52,360 --> 00:40:57,100
help a little bit in Mesa sauce and eCos

00:40:55,060 --> 00:40:59,110
on the Left we have the output from the

00:40:57,100 --> 00:41:00,610
CLI when you're calling out one

00:40:59,110 --> 00:41:03,160
particular instance saying ok what's

00:41:00,610 --> 00:41:06,130
happened with that and on the right we

00:41:03,160 --> 00:41:07,900
have the same logs spooling through the

00:41:06,130 --> 00:41:09,970
UI but so far what I've what I've told

00:41:07,900 --> 00:41:12,040
you isn't particularly new it's like

00:41:09,970 --> 00:41:13,630
probably familiar to everybody and if

00:41:12,040 --> 00:41:15,670
you're familiar with the Mesa sauce logs

00:41:13,630 --> 00:41:20,290
interfaces this is this is pretty much

00:41:15,670 --> 00:41:21,970
the same thing and so I wanted to speak

00:41:20,290 --> 00:41:23,279
to what what DCOs can do a bit more and

00:41:21,970 --> 00:41:25,769
may sauce has no

00:41:23,279 --> 00:41:27,989
built-in log DB it just rights to file

00:41:25,769 --> 00:41:31,079
and pay just page through those like a

00:41:27,989 --> 00:41:33,509
regular I can make your application and

00:41:31,079 --> 00:41:35,339
and DCFS also has no log DB

00:41:33,509 --> 00:41:37,890
out-of-the-box but what it does have is

00:41:35,339 --> 00:41:40,079
the ability to install services like

00:41:37,890 --> 00:41:42,689
elastic Cabana and log stash which is

00:41:40,079 --> 00:41:48,749
the alec stack and easy integration with

00:41:42,689 --> 00:41:50,519
things like Splunk and so that's kind of

00:41:48,749 --> 00:41:52,619
cool because once you've got your

00:41:50,519 --> 00:41:53,909
application piped out to ELQ or Splunk

00:41:52,619 --> 00:41:56,159
or whatever you can start to write

00:41:53,909 --> 00:41:57,659
queries like this and so what we've done

00:41:56,159 --> 00:42:00,599
here that the highlighted bit is a

00:41:57,659 --> 00:42:03,059
framework ID and we've searched to every

00:42:00,599 --> 00:42:04,650
log related to that framework now that's

00:42:03,059 --> 00:42:07,249
cool because and that framework might

00:42:04,650 --> 00:42:11,699
have processes running on multiple nodes

00:42:07,249 --> 00:42:14,429
and with based by searching the specific

00:42:11,699 --> 00:42:16,380
ID we can get those without having to

00:42:14,429 --> 00:42:21,199
pull a load of log files and do that

00:42:16,380 --> 00:42:23,459
nothing manually so much for logs

00:42:21,199 --> 00:42:24,630
metrics are important now the first

00:42:23,459 --> 00:42:26,939
thing you see when you look into DCOs

00:42:24,630 --> 00:42:29,549
the dashboard shows you these these

00:42:26,939 --> 00:42:31,049
graphs but those graphs are allocation

00:42:29,549 --> 00:42:32,869
that's what may sauce knows about what

00:42:31,049 --> 00:42:34,769
it's trying to do at this moment

00:42:32,869 --> 00:42:36,119
so if we're going to be responsible

00:42:34,769 --> 00:42:40,949
cluster operators we need to look a

00:42:36,119 --> 00:42:42,809
little bit deeper than that but there's

00:42:40,949 --> 00:42:45,199
loads of different types of metric this

00:42:42,809 --> 00:42:47,039
is an attempt to explain the

00:42:45,199 --> 00:42:48,599
architecture of their metrics project

00:42:47,039 --> 00:42:53,640
itself and calling out the three

00:42:48,599 --> 00:42:55,679
different types which metrics from the

00:42:53,640 --> 00:42:57,119
node metrics from the containers running

00:42:55,679 --> 00:43:04,759
in may sauce and metrics from the apps

00:42:57,119 --> 00:43:07,199
running inside those so this is

00:43:04,759 --> 00:43:09,119
Prometheus there's a what we just

00:43:07,199 --> 00:43:10,469
queried a node CPU it's it's right at

00:43:09,119 --> 00:43:13,559
the top of that of that slide you can

00:43:10,469 --> 00:43:15,809
see and this is a smallish cluster and

00:43:13,559 --> 00:43:19,859
these are the the CPU utilization per

00:43:15,809 --> 00:43:24,539
node at over the last this is a two-hour

00:43:19,859 --> 00:43:26,159
query and say again so far so normal and

00:43:24,539 --> 00:43:28,919
this is something that you can probably

00:43:26,159 --> 00:43:30,089
get out of an AWS or GC or whatever

00:43:28,919 --> 00:43:31,660
whatever you're using to deploy your

00:43:30,089 --> 00:43:33,350
machines

00:43:31,660 --> 00:43:36,860
but we have more than that we have

00:43:33,350 --> 00:43:38,330
container level metrics these are kind

00:43:36,860 --> 00:43:40,280
of similar they look at first glance

00:43:38,330 --> 00:43:42,770
kind of similar to node metrics because

00:43:40,280 --> 00:43:44,600
you're looking at the same kind of kind

00:43:42,770 --> 00:43:47,450
of fields like CPU like memory like this

00:43:44,600 --> 00:43:49,190
consumption Network but they are

00:43:47,450 --> 00:43:51,260
slightly different because they are

00:43:49,190 --> 00:43:53,030
operating within constraints so when

00:43:51,260 --> 00:43:55,540
when you look at your note metrics you

00:43:53,030 --> 00:43:58,520
might say okay if my CPU rises above 80%

00:43:55,540 --> 00:43:59,930
of available resource then I probably

00:43:58,520 --> 00:44:01,280
got a problem or if my disk gets

00:43:59,930 --> 00:44:02,570
completely full and I definitely got a

00:44:01,280 --> 00:44:03,770
problem so you're gonna set some

00:44:02,570 --> 00:44:05,300
overhead in your constraints

00:44:03,770 --> 00:44:08,060
whereas when you're working with a

00:44:05,300 --> 00:44:10,040
container and you've constrained it

00:44:08,060 --> 00:44:11,660
already so if it's using a hundred

00:44:10,040 --> 00:44:13,520
percent of CPU you're not necessarily

00:44:11,660 --> 00:44:14,920
locking anything up although you do

00:44:13,520 --> 00:44:17,480
still want to monitor this stuff because

00:44:14,920 --> 00:44:19,369
it's probably not great and you probably

00:44:17,480 --> 00:44:23,390
need to increase your allocation of

00:44:19,369 --> 00:44:25,100
resource there and the last one is apt

00:44:23,390 --> 00:44:28,640
level metrics now these are the metrics

00:44:25,100 --> 00:44:31,040
that your app emits from within its

00:44:28,640 --> 00:44:35,540
container and what you're looking at

00:44:31,040 --> 00:44:37,580
here is many processes running HTTP

00:44:35,540 --> 00:44:39,859
server and this is the latency on the

00:44:37,580 --> 00:44:41,300
requests to those servers so you can you

00:44:39,859 --> 00:44:43,700
can see all of them stacked up there and

00:44:41,300 --> 00:44:45,080
you can see a point at which latency

00:44:43,700 --> 00:44:51,050
suddenly rose and a point where it

00:44:45,080 --> 00:44:52,119
suddenly dropped so alerting is the last

00:44:51,050 --> 00:44:54,109
thing I wanted to talk to you because

00:44:52,119 --> 00:44:56,810
you're going to want to alert from your

00:44:54,109 --> 00:44:58,340
logs from your metrics and they're both

00:44:56,810 --> 00:45:00,380
important and you can't you can't really

00:44:58,340 --> 00:45:02,450
just pick one if you start seeing many

00:45:00,380 --> 00:45:05,270
exceptions in your logs then you're

00:45:02,450 --> 00:45:07,640
going to want to get paged and if you

00:45:05,270 --> 00:45:09,260
start seeing very high traffic from your

00:45:07,640 --> 00:45:10,640
monitor from your metrics you wanna get

00:45:09,260 --> 00:45:12,770
paged if you start seeing very low

00:45:10,640 --> 00:45:13,940
traffic you really really really want to

00:45:12,770 --> 00:45:17,300
get paged because something's gone down

00:45:13,940 --> 00:45:19,160
and so I just wanted to say briefly

00:45:17,300 --> 00:45:21,770
there's this concept of white box

00:45:19,160 --> 00:45:23,150
metrics and black box metrics a white

00:45:21,770 --> 00:45:25,940
box and that trick is when you have full

00:45:23,150 --> 00:45:28,300
visibility into your app and something

00:45:25,940 --> 00:45:30,680
like Cassandra for example will give you

00:45:28,300 --> 00:45:32,119
will start to admit metrics about the

00:45:30,680 --> 00:45:33,800
state of the JVM it's running on so

00:45:32,119 --> 00:45:34,970
that's kind of cool like I know exactly

00:45:33,800 --> 00:45:38,720
how much heap space I'm currently

00:45:34,970 --> 00:45:40,910
consuming and that sort of thing but I

00:45:38,720 --> 00:45:44,369
also want to know whether I can still

00:45:40,910 --> 00:45:46,559
write to it so a black box in that trick

00:45:44,369 --> 00:45:48,450
you know can I actually access the site

00:45:46,559 --> 00:45:50,400
it's from the users a metric from the

00:45:48,450 --> 00:45:51,479
users perspective and kind of similar to

00:45:50,400 --> 00:45:53,930
what goes on in your harness were just

00:45:51,479 --> 00:45:59,789
talking about with health checks though

00:45:53,930 --> 00:46:03,390
so yeah do you both and that's sort of

00:45:59,789 --> 00:46:04,950
that was the wrong screen that was

00:46:03,390 --> 00:46:06,329
that's pretty much what I've got a few

00:46:04,950 --> 00:46:08,999
that was a quick charge through and that

00:46:06,329 --> 00:46:10,920
was a very screenshot heavy a few slides

00:46:08,999 --> 00:46:14,249
but everything I've shown you there was

00:46:10,920 --> 00:46:17,549
deployed on a dcs cluster and that's all

00:46:14,249 --> 00:46:20,999
like very very achievable today so yeah

00:46:17,549 --> 00:46:27,900
I'll hand over to Kevin again oh sorry

00:46:20,999 --> 00:46:29,640
we have a question go ahead the question

00:46:27,900 --> 00:46:35,099
was is the Prometheus plug-in baked into

00:46:29,640 --> 00:46:40,319
110 and the no no it's not actually

00:46:35,099 --> 00:46:42,349
baiting you have to deploy it on top go

00:46:40,319 --> 00:46:42,349
ahead

00:46:55,100 --> 00:46:58,430
so the question was if you're using the

00:46:56,690 --> 00:47:00,800
doctor agent and you deploy the fluence

00:46:58,430 --> 00:47:01,520
d logger does that prevent log showing

00:47:00,800 --> 00:47:04,310
up in the UI

00:47:01,520 --> 00:47:06,170
it shouldn't no because as far as I know

00:47:04,310 --> 00:47:07,910
that does not divert them and they

00:47:06,170 --> 00:47:14,420
should still be and they should still be

00:47:07,910 --> 00:47:18,860
written out - yeah no they should still

00:47:14,420 --> 00:47:24,920
be available I believe anybody else can

00:47:18,860 --> 00:47:27,020
i okay over to Kevin thanks Phillip so

00:47:24,920 --> 00:47:28,910
what I'm gonna talk about is the attach

00:47:27,020 --> 00:47:31,640
and exec support that we added into D

00:47:28,910 --> 00:47:33,860
CRS so it's basically mimicking the same

00:47:31,640 --> 00:47:35,180
kind of exec attached and exact that you

00:47:33,860 --> 00:47:38,060
would get if you were trying to use the

00:47:35,180 --> 00:47:40,820
the docker CLI but now add it into for

00:47:38,060 --> 00:47:43,760
support with mesas so a quick overview

00:47:40,820 --> 00:47:45,290
of what we did to add the support like I

00:47:43,760 --> 00:47:46,970
said it's added functionality to mesas

00:47:45,290 --> 00:47:48,530
to enable building tools that mimic the

00:47:46,970 --> 00:47:50,000
functionality of docker attached and

00:47:48,530 --> 00:47:52,400
docker exec so you can jump inside your

00:47:50,000 --> 00:47:54,560
containers debug what's going wrong and

00:47:52,400 --> 00:47:56,420
then come back out everything runs

00:47:54,560 --> 00:47:58,250
locally on your client so with this

00:47:56,420 --> 00:47:59,840
support there's no need to SSH on to the

00:47:58,250 --> 00:48:01,700
node where your task is running you can

00:47:59,840 --> 00:48:04,480
just jump right into the container

00:48:01,700 --> 00:48:06,860
itself from your from your local laptop

00:48:04,480 --> 00:48:09,350
we have a reference implementation of

00:48:06,860 --> 00:48:11,720
you know wrapping the AP is that we had

00:48:09,350 --> 00:48:14,150
to add tomatoes to build the support in

00:48:11,720 --> 00:48:16,010
the DC US CLI so if you wanted to invoke

00:48:14,150 --> 00:48:17,690
this actually you know take advantage of

00:48:16,010 --> 00:48:20,180
this new support you would run something

00:48:17,690 --> 00:48:22,820
like DCOs task exec say whether you

00:48:20,180 --> 00:48:25,370
wanted it to be interactive attach a TTY

00:48:22,820 --> 00:48:27,200
or not past the task ID the command you

00:48:25,370 --> 00:48:29,690
want to run any of the arguments and so

00:48:27,200 --> 00:48:30,950
on so we really tried to mimic the exact

00:48:29,690 --> 00:48:33,680
same command line that you would have

00:48:30,950 --> 00:48:38,140
from a docker exec but wrapping it for

00:48:33,680 --> 00:48:40,940
for for DC US and then similarly for

00:48:38,140 --> 00:48:43,880
attach which is implemented on the API

00:48:40,940 --> 00:48:45,560
side inside mesos but we haven't pushed

00:48:43,880 --> 00:48:48,230
that through to have it be supported in

00:48:45,560 --> 00:48:51,020
the in the CLI yet but it is coming soon

00:48:48,230 --> 00:48:52,850
and also native meso CLI support for

00:48:51,020 --> 00:48:54,230
this is coming soon as well you know we

00:48:52,850 --> 00:48:57,170
built the support in tomatoes it's

00:48:54,230 --> 00:48:59,360
obviously usable by Mesa CLI we just

00:48:57,170 --> 00:49:01,700
haven't prioritized building that that

00:48:59,360 --> 00:49:04,520
piece of it yet as a from the company's

00:49:01,700 --> 00:49:05,570
perspective so what are the exact

00:49:04,520 --> 00:49:07,630
commands really do what are they allow

00:49:05,570 --> 00:49:08,860
you to do you know they basically

00:49:07,630 --> 00:49:10,030
under the hood if you look at the

00:49:08,860 --> 00:49:12,010
implementation details of this we

00:49:10,030 --> 00:49:13,930
leverage the nested container support

00:49:12,010 --> 00:49:15,640
that we added some ASOS nested

00:49:13,930 --> 00:49:17,080
containers is also the same technology

00:49:15,640 --> 00:49:19,210
that we built to allow us to support

00:49:17,080 --> 00:49:21,130
something like pods similar to what you

00:49:19,210 --> 00:49:23,020
get from kubernetes we call them task

00:49:21,130 --> 00:49:24,250
groups kubernetes calls and pods but

00:49:23,020 --> 00:49:26,920
under the hood and maysa they're all

00:49:24,250 --> 00:49:28,300
nested containers the one difference or

00:49:26,920 --> 00:49:30,310
the one piece that we had to add to her

00:49:28,300 --> 00:49:31,420
existing nested container support was

00:49:30,310 --> 00:49:33,220
that we had to be able to tie the

00:49:31,420 --> 00:49:35,710
lifecycle of the nested container to the

00:49:33,220 --> 00:49:37,120
lifecycle of a connection so whenever we

00:49:35,710 --> 00:49:38,830
do one of these tasks execs into the

00:49:37,120 --> 00:49:41,050
container we launch a nested container

00:49:38,830 --> 00:49:42,880
inside of it with a brand new process it

00:49:41,050 --> 00:49:44,380
runs until that connection is dropped

00:49:42,880 --> 00:49:47,340
and then that container gets killed and

00:49:44,380 --> 00:49:50,020
cleaned up and kind of disappears right

00:49:47,340 --> 00:49:51,730
when this process is launched inside

00:49:50,020 --> 00:49:53,110
there we you know we isolate the nested

00:49:51,730 --> 00:49:54,610
container inside the same set of C

00:49:53,110 --> 00:49:56,530
groups and namespaces as the parent

00:49:54,610 --> 00:49:57,820
container so you know logically all

00:49:56,530 --> 00:49:59,140
you're doing is launching any process

00:49:57,820 --> 00:50:01,060
inside the container but what you really

00:49:59,140 --> 00:50:02,980
need to do to make this robust and

00:50:01,060 --> 00:50:04,720
usable is launch a separate container

00:50:02,980 --> 00:50:06,280
that just happens to have all the same C

00:50:04,720 --> 00:50:08,980
groups and namespaces as the original

00:50:06,280 --> 00:50:10,360
container right and then once you have

00:50:08,980 --> 00:50:11,680
that we just stream the input and output

00:50:10,360 --> 00:50:14,830
of the command back to the local

00:50:11,680 --> 00:50:16,630
terminal and we use HTTP so it's all

00:50:14,830 --> 00:50:17,950
streaming HTTP there's not a TCP

00:50:16,630 --> 00:50:19,390
connection that we set up which makes it

00:50:17,950 --> 00:50:21,970
a lot more flexible to be able to use

00:50:19,390 --> 00:50:23,440
from a UI from your CLI from everything

00:50:21,970 --> 00:50:27,640
because there's nice tools you can do

00:50:23,440 --> 00:50:29,140
just around the HTTP so similarly for

00:50:27,640 --> 00:50:30,700
attached command so if you think of exec

00:50:29,140 --> 00:50:33,580
is launching a new process inside your

00:50:30,700 --> 00:50:35,380
container attached is and then you know

00:50:33,580 --> 00:50:37,240
exec is launching a new process and then

00:50:35,380 --> 00:50:38,950
attaching to the the standard in and

00:50:37,240 --> 00:50:40,480
standard out to stream that back attach

00:50:38,950 --> 00:50:42,790
is just getting to the standard in and

00:50:40,480 --> 00:50:44,170
standard out of your primary process

00:50:42,790 --> 00:50:45,850
inside your container right so once you

00:50:44,170 --> 00:50:47,950
launch that initial container you just

00:50:45,850 --> 00:50:49,660
want to be able to get at its output or

00:50:47,950 --> 00:50:51,160
feed something into its input right

00:50:49,660 --> 00:50:55,150
that's the purpose of something like

00:50:51,160 --> 00:50:57,880
attach and currently even with the

00:50:55,150 --> 00:50:59,740
support that we have from ASOS we and

00:50:57,880 --> 00:51:01,720
and going forward we're only gonna allow

00:50:59,740 --> 00:51:03,790
people to attach to containers that have

00:51:01,720 --> 00:51:05,500
been pre launched with a TTY so when you

00:51:03,790 --> 00:51:07,420
set up your container and you're about

00:51:05,500 --> 00:51:09,610
to launch it one of the fields in your

00:51:07,420 --> 00:51:11,050
container config can be oh I want a TTY

00:51:09,610 --> 00:51:13,120
attached to this and then under the hood

00:51:11,050 --> 00:51:15,610
the container Iser will allocate a TTY

00:51:13,120 --> 00:51:17,410
for it stream all of the input and

00:51:15,610 --> 00:51:19,340
output through that TTY so that you can

00:51:17,410 --> 00:51:20,930
come along later and attach to it

00:51:19,340 --> 00:51:23,300
and the main reason we did this is that

00:51:20,930 --> 00:51:25,150
we - when I get into the implementation

00:51:23,300 --> 00:51:27,440
details of it in a second you'll see

00:51:25,150 --> 00:51:28,760
kind of how this is architected but you

00:51:27,440 --> 00:51:31,400
know we really want these things to be

00:51:28,760 --> 00:51:33,200
able to - you know we want the

00:51:31,400 --> 00:51:34,490
containers themselves to survive agent

00:51:33,200 --> 00:51:36,110
crashes we want everything to be really

00:51:34,490 --> 00:51:37,670
robust so in order to implement the

00:51:36,110 --> 00:51:39,200
support for being able to attach to its

00:51:37,670 --> 00:51:41,540
input and output we have to run a

00:51:39,200 --> 00:51:44,030
separate process alongside any of our

00:51:41,540 --> 00:51:45,890
tasks that we launched and so we didn't

00:51:44,030 --> 00:51:47,570
want to make force every user that ever

00:51:45,890 --> 00:51:49,550
launched at a task to have this separate

00:51:47,570 --> 00:51:51,350
sidecar process associated with it so we

00:51:49,550 --> 00:51:53,060
decided to limit the scope of this -

00:51:51,350 --> 00:51:55,220
only if you want a TTY attached to it

00:51:53,060 --> 00:51:56,990
well you get this extra process kind of

00:51:55,220 --> 00:52:00,470
launched alongside it so you can come

00:51:56,990 --> 00:52:02,840
and attach to it later so this is kind

00:52:00,470 --> 00:52:05,930
of the architecture of what we did so

00:52:02,840 --> 00:52:08,180
the the the boxes in blue are pieces

00:52:05,930 --> 00:52:09,560
that existed prior to this work and the

00:52:08,180 --> 00:52:12,500
orange ones are the things that we added

00:52:09,560 --> 00:52:13,910
so if I start on the left here you can

00:52:12,500 --> 00:52:15,560
see that you know the main thing we did

00:52:13,910 --> 00:52:16,940
is we added some new agent API is and

00:52:15,560 --> 00:52:19,280
I'll get into what the details of the of

00:52:16,940 --> 00:52:20,660
what those api's are in a second but the

00:52:19,280 --> 00:52:22,190
idea is that they're just some API is

00:52:20,660 --> 00:52:24,350
that you can attach directly to an agent

00:52:22,190 --> 00:52:26,990
to stream and put an output out of a

00:52:24,350 --> 00:52:29,570
container right we had to have a add a

00:52:26,990 --> 00:52:31,850
component for the handlers for those for

00:52:29,570 --> 00:52:34,100
those API calls which now call into

00:52:31,850 --> 00:52:37,370
something that we that we created called

00:52:34,100 --> 00:52:39,050
the container IO switchboard which you

00:52:37,370 --> 00:52:41,300
know it has a component that lives

00:52:39,050 --> 00:52:43,880
inside the agent but it's entire job is

00:52:41,300 --> 00:52:46,430
basically to you know both launched a

00:52:43,880 --> 00:52:48,380
new masers IO switch word process and

00:52:46,430 --> 00:52:50,330
glue the standard in and standard out

00:52:48,380 --> 00:52:52,070
between the iOS which were process and

00:52:50,330 --> 00:52:53,690
the task that's actually being launched

00:52:52,070 --> 00:52:54,620
because the IO switchboard is the thing

00:52:53,690 --> 00:52:56,780
that you're actually going to connect to

00:52:54,620 --> 00:52:58,670
to get the to make that connection with

00:52:56,780 --> 00:52:59,900
the input and output and so there's a

00:52:58,670 --> 00:53:01,490
you know there's a basically just a

00:52:59,900 --> 00:53:03,230
bunch of file descriptor and pipes that

00:53:01,490 --> 00:53:05,870
are set up to make sure that you know

00:53:03,230 --> 00:53:08,210
the IO switchboard you know gets your

00:53:05,870 --> 00:53:09,830
input and output make sure that you can

00:53:08,210 --> 00:53:11,330
feed that from the connection and then

00:53:09,830 --> 00:53:15,230
also write that to the application

00:53:11,330 --> 00:53:17,750
properly that make sense and what the

00:53:15,230 --> 00:53:20,330
API calls actually look like are this so

00:53:17,750 --> 00:53:22,430
there's three calls one of them is

00:53:20,330 --> 00:53:24,620
called launch nested container session

00:53:22,430 --> 00:53:27,440
and the purpose of that is to you know

00:53:24,620 --> 00:53:28,940
you pass it your task ID and you tell it

00:53:27,440 --> 00:53:30,890
what new command you want to run inside

00:53:28,940 --> 00:53:32,359
that it didn't choose the new nested

00:53:30,890 --> 00:53:34,009
container inside your container on the

00:53:32,359 --> 00:53:35,239
agent and it sit there and it's waiting

00:53:34,009 --> 00:53:36,440
for the next command and so the next

00:53:35,239 --> 00:53:38,630
command that you want to be able to send

00:53:36,440 --> 00:53:40,039
it is can touch container input so you

00:53:38,630 --> 00:53:42,049
can start streaming your input into that

00:53:40,039 --> 00:53:45,079
container now the third one we have is

00:53:42,049 --> 00:53:47,319
called a touch container output nested

00:53:45,079 --> 00:53:48,470
launched as a container session actually

00:53:47,319 --> 00:53:49,670
overloads

00:53:48,470 --> 00:53:51,079
the attached container output so you

00:53:49,670 --> 00:53:53,690
only really need the attached container

00:53:51,079 --> 00:53:56,089
output call if you're doing an attach

00:53:53,690 --> 00:53:57,589
rather than an exec because what ends up

00:53:56,089 --> 00:53:59,390
happening for launch that's a container

00:53:57,589 --> 00:54:01,430
session is you know you open a new

00:53:59,390 --> 00:54:04,009
connection into the agent that

00:54:01,430 --> 00:54:06,710
connection stays alive forever and then

00:54:04,009 --> 00:54:08,660
the response that comes back is just a

00:54:06,710 --> 00:54:10,640
streaming output from the container that

00:54:08,660 --> 00:54:11,960
you've that you've now launched right

00:54:10,640 --> 00:54:13,670
the whatever command you've launched

00:54:11,960 --> 00:54:15,259
once that connection happens you start

00:54:13,670 --> 00:54:18,170
getting an infinite stream of the output

00:54:15,259 --> 00:54:19,700
coming back if you come along later and

00:54:18,170 --> 00:54:21,470
do an attached container input now you

00:54:19,700 --> 00:54:23,029
can you know you have a persistent

00:54:21,470 --> 00:54:25,130
connection on the requests where you're

00:54:23,029 --> 00:54:26,720
constantly streaming your input into the

00:54:25,130 --> 00:54:28,940
application and then similarly if the

00:54:26,720 --> 00:54:30,499
attached container output it's the exact

00:54:28,940 --> 00:54:32,450
same thing going on in terms of

00:54:30,499 --> 00:54:34,309
streaming output that you get from

00:54:32,450 --> 00:54:35,509
launching as a container session except

00:54:34,309 --> 00:54:37,450
you don't actually launch and that's the

00:54:35,509 --> 00:54:40,549
container right you just attach to the

00:54:37,450 --> 00:54:45,829
output of whatever the container primary

00:54:40,549 --> 00:54:47,359
process already was right so going along

00:54:45,829 --> 00:54:49,910
with all this we have full integration

00:54:47,359 --> 00:54:51,559
with make sauces built in authentic an

00:54:49,910 --> 00:54:52,880
set up your ecl's to make sure that you

00:54:51,559 --> 00:54:55,039
know only certain users are able to

00:54:52,880 --> 00:54:57,289
access this functionality it's also

00:54:55,039 --> 00:54:59,210
integrated with DC us's fine-grained ACL

00:54:57,289 --> 00:55:01,309
support as well so you know you can make

00:54:59,210 --> 00:55:03,920
sure that only limited users have the

00:55:01,309 --> 00:55:05,630
ability to exec into the processes that

00:55:03,920 --> 00:55:08,809
they own and the tasks that they launch

00:55:05,630 --> 00:55:10,999
themselves and yet so you you know your

00:55:08,809 --> 00:55:13,039
that authorization is all set up and and

00:55:10,999 --> 00:55:16,819
and and and available if you decide to

00:55:13,039 --> 00:55:19,309
use it so what I want to do now is sort

00:55:16,819 --> 00:55:20,720
of demo how you can use this and like

00:55:19,309 --> 00:55:22,069
like we mentioned before we have an

00:55:20,720 --> 00:55:23,930
interactive session coming at the end of

00:55:22,069 --> 00:55:26,420
this where you can you know you can kind

00:55:23,930 --> 00:55:28,069
of rerun through this these exact set of

00:55:26,420 --> 00:55:30,230
steps yourself to play around with some

00:55:28,069 --> 00:55:31,309
of the support but what I basically want

00:55:30,230 --> 00:55:33,259
to show is that you know how you can

00:55:31,309 --> 00:55:35,779
bring up a vanilla Engine X docker

00:55:33,259 --> 00:55:38,749
container run it through marathon make

00:55:35,779 --> 00:55:40,489
sure that nothing's serving on the port

00:55:38,749 --> 00:55:42,680
that's been allocated nginx so by

00:55:40,489 --> 00:55:44,660
default nginx is launched with port 80

00:55:42,680 --> 00:55:45,690
set up so I don't change anything about

00:55:44,660 --> 00:55:47,250
engine acts I just

00:55:45,690 --> 00:55:49,320
launched it it tries to connect on port

00:55:47,250 --> 00:55:51,750
80 but that's not the port that makes us

00:55:49,320 --> 00:55:53,010
as allocated to it right so what I

00:55:51,750 --> 00:55:54,810
didn't do is say okay well I want to try

00:55:53,010 --> 00:55:56,340
and figure out what's wrong I started an

00:55:54,810 --> 00:55:58,380
interactive session with engine X's

00:55:56,340 --> 00:56:00,180
container and once I'm once I'm inside

00:55:58,380 --> 00:56:02,550
that container I update the port that

00:56:00,180 --> 00:56:03,960
Engine X listens on I restart Engine X

00:56:02,550 --> 00:56:05,400
and then I should be able to go to the

00:56:03,960 --> 00:56:07,950
web page and see that it's launched

00:56:05,400 --> 00:56:10,110
right because the port is now actually

00:56:07,950 --> 00:56:13,190
or nginx is now running with the port

00:56:10,110 --> 00:56:15,360
that makes us as allocated to it I

00:56:13,190 --> 00:56:17,280
pre-recorded this demo it's actually I

00:56:15,360 --> 00:56:18,950
usually give a longer version of this

00:56:17,280 --> 00:56:21,900
talk this is a very stripped-down

00:56:18,950 --> 00:56:25,410
version of it and so this is kind of a

00:56:21,900 --> 00:56:27,060
clipped version of the video from a talk

00:56:25,410 --> 00:56:28,350
that I've given in the past so there

00:56:27,060 --> 00:56:29,760
might be a few commands at the beginning

00:56:28,350 --> 00:56:31,770
they look confusing because it's kind of

00:56:29,760 --> 00:56:33,360
going from the flow of you know the the

00:56:31,770 --> 00:56:36,480
other steps that I usually give when I

00:56:33,360 --> 00:56:39,210
when I go through this but yeah I'll go

00:56:36,480 --> 00:56:40,230
ahead and start this and the first thing

00:56:39,210 --> 00:56:42,180
that I'm gonna do is I'm gonna actually

00:56:40,230 --> 00:56:44,160
set up my cluster to point through a

00:56:42,180 --> 00:56:46,230
DCOs cluster and I'm gonna log into it

00:56:44,160 --> 00:56:48,600
so you know by default whenever you

00:56:46,230 --> 00:56:49,950
launch a TCOs cluster the the first user

00:56:48,600 --> 00:56:51,870
the default user you get is called

00:56:49,950 --> 00:56:53,640
bootstrap user you type in the password

00:56:51,870 --> 00:56:55,050
for that and now you're logged in to

00:56:53,640 --> 00:56:57,500
that cluster so you can start deploying

00:56:55,050 --> 00:57:01,890
things on it and playing around with it

00:56:57,500 --> 00:57:03,510
so once once I'm logged in next thing

00:57:01,890 --> 00:57:06,360
that I'm gonna do is I'm going to take

00:57:03,510 --> 00:57:07,890
an engine x container did I have well I

00:57:06,360 --> 00:57:09,180
guess first I'm gonna show that you know

00:57:07,890 --> 00:57:10,590
there's no services running on this

00:57:09,180 --> 00:57:12,060
thing right now right this is a vanilla

00:57:10,590 --> 00:57:15,360
cluster nothing's running on it

00:57:12,060 --> 00:57:16,560
there's no services at all and you know

00:57:15,360 --> 00:57:19,830
now I'm going to deploy this nginx

00:57:16,560 --> 00:57:23,420
container so I have this nginx JSON file

00:57:19,830 --> 00:57:27,390
that I've sort of pre created for this

00:57:23,420 --> 00:57:30,480
if you look through the the contents of

00:57:27,390 --> 00:57:33,840
it let's see here yeah so I'm opening it

00:57:30,480 --> 00:57:35,370
up you can see that I've got the the

00:57:33,840 --> 00:57:36,930
name of it is going to be called nginx

00:57:35,370 --> 00:57:38,340
demo so when we go to the UI and look at

00:57:36,930 --> 00:57:40,950
this you'll you'll see that reflected in

00:57:38,340 --> 00:57:42,390
there the container type is meso so I'm

00:57:40,950 --> 00:57:45,420
going to use the maysa or a unified

00:57:42,390 --> 00:57:46,620
container Iser to launch this it's just

00:57:45,420 --> 00:57:48,510
the vanilla nginx

00:57:46,620 --> 00:57:50,880
docker image that you can get from

00:57:48,510 --> 00:57:52,710
docker hub the command I'm gonna launch

00:57:50,880 --> 00:57:55,050
again just for demonstration purposes is

00:57:52,710 --> 00:57:56,700
some long sleep command so that this

00:57:55,050 --> 00:57:59,510
thing will come up and stay running

00:57:56,700 --> 00:58:01,820
forever and then I'm going to give it

00:57:59,510 --> 00:58:03,860
you know point to CPUs and I'm gonna run

00:58:01,820 --> 00:58:05,360
a single instance of this and then I

00:58:03,860 --> 00:58:08,120
want it to run on a public node so that

00:58:05,360 --> 00:58:11,240
I can you know actually access the IP

00:58:08,120 --> 00:58:12,710
address and get to the port from from

00:58:11,240 --> 00:58:13,910
that on the on the on a public notary

00:58:12,710 --> 00:58:16,280
other than something private inside the

00:58:13,910 --> 00:58:18,290
cluster I could have also set up a

00:58:16,280 --> 00:58:20,060
marathon I'll be the same way Gaston

00:58:18,290 --> 00:58:21,260
showed earlier but for the purposes of

00:58:20,060 --> 00:58:25,880
this demo that that wasn't really

00:58:21,260 --> 00:58:28,220
necessary so yeah so the next step is to

00:58:25,880 --> 00:58:30,290
basically take that JSON and deploy it

00:58:28,220 --> 00:58:32,900
on the cluster so I've got this command

00:58:30,290 --> 00:58:35,090
here that you can execute from the DC OS

00:58:32,900 --> 00:58:36,680
CLI to go ahead and deploy that so you

00:58:35,090 --> 00:58:40,250
see that the deployments been created

00:58:36,680 --> 00:58:41,960
and I can go back to the UI and I see

00:58:40,250 --> 00:58:44,300
that it's running now right so engine X

00:58:41,960 --> 00:58:46,190
demo you can click through it and you

00:58:44,300 --> 00:58:47,390
see that it's running and Gaston talked

00:58:46,190 --> 00:58:49,820
about the health checks you can see that

00:58:47,390 --> 00:58:51,710
it's already healthy now what I can do

00:58:49,820 --> 00:58:54,140
what I do now is I come in and I look at

00:58:51,710 --> 00:58:55,880
what the endpoint that it thinks it has

00:58:54,140 --> 00:58:56,990
for that cluster is right there sorry

00:58:55,880 --> 00:58:58,850
not for that cluster for this

00:58:56,990 --> 00:59:02,060
application this is the endpoints on a

00:58:58,850 --> 00:59:03,290
private IP address on the port that's

00:59:02,060 --> 00:59:04,460
been allocated to it so there's

00:59:03,290 --> 00:59:06,380
obviously two things wrong with this one

00:59:04,460 --> 00:59:08,990
it's a private IP there's no way for me

00:59:06,380 --> 00:59:11,480
to be able to get to that from from my

00:59:08,990 --> 00:59:13,100
local laptop then the second one is that

00:59:11,480 --> 00:59:14,300
that's not port 80 and genetics thinks

00:59:13,100 --> 00:59:15,440
that it needs to be running them or

00:59:14,300 --> 00:59:16,930
thinks that it's running on port 80 and

00:59:15,440 --> 00:59:20,540
that's not what's been allocated to it

00:59:16,930 --> 00:59:21,800
so so how can I deal with this so first

00:59:20,540 --> 00:59:26,300
thing I want to do is I want to get the

00:59:21,800 --> 00:59:28,730
public IP of this node so that I can you

00:59:26,300 --> 00:59:33,200
know actually navigate to it right so we

00:59:28,730 --> 00:59:35,210
have a because we're running on on AWS

00:59:33,200 --> 00:59:36,890
I'm gonna be able to hit the metadata

00:59:35,210 --> 00:59:39,200
server to figure out what my public IP

00:59:36,890 --> 00:59:45,410
is right this is a standard URL you can

00:59:39,200 --> 00:59:47,240
hit inside AWS on AWS machines and so

00:59:45,410 --> 00:59:48,650
you can get the public IP so what

00:59:47,240 --> 00:59:50,180
happened here actually was I tried to

00:59:48,650 --> 00:59:52,100
run this I tried to use this DCS task

00:59:50,180 --> 00:59:53,750
exec command that I've now added but

00:59:52,100 --> 00:59:55,940
what ended up happening was Curl wasn't

00:59:53,750 --> 00:59:57,230
installed on that box so so what do i do

00:59:55,940 --> 00:59:58,670
how do i how do i deal with this so

00:59:57,230 --> 01:00:00,950
instead of trying to run that command

00:59:58,670 --> 01:00:03,560
directly i'll now jump into a bash shell

01:00:00,950 --> 01:00:06,530
using task exec i'll run apt-get update

01:00:03,560 --> 01:00:08,480
and I'll install curl right so you can

01:00:06,530 --> 01:00:09,860
you know kind of you can just kind of

01:00:08,480 --> 01:00:11,300
show the power of being able to jump in

01:00:09,860 --> 01:00:12,730
to the container just same way you would

01:00:11,300 --> 01:00:15,580
have with something like docker

01:00:12,730 --> 01:00:16,810
exact so I install curl once curl gets

01:00:15,580 --> 01:00:18,520
installed I can now from this

01:00:16,810 --> 01:00:21,100
interactive session basically rerun the

01:00:18,520 --> 01:00:22,690
same command I tried to before to get

01:00:21,100 --> 01:00:24,400
the public IP address of this node now

01:00:22,690 --> 01:00:25,990
you know this is kind of contrived you

01:00:24,400 --> 01:00:29,170
probably know the public IP address of

01:00:25,990 --> 01:00:30,670
your node out-of-band of this but this

01:00:29,170 --> 01:00:32,560
is just kind of showing you know how you

01:00:30,670 --> 01:00:34,810
can use these tools to go and figure

01:00:32,560 --> 01:00:37,090
this out if you if you need to so I do

01:00:34,810 --> 01:00:37,600
that I get my public IP everything's

01:00:37,090 --> 01:00:39,430
great

01:00:37,600 --> 01:00:41,620
I try and go back to my UI to replace

01:00:39,430 --> 01:00:43,090
the private IP with the public one and

01:00:41,620 --> 01:00:45,730
obviously it still doesn't work because

01:00:43,090 --> 01:00:47,020
the port's wrong right so how do I do

01:00:45,730 --> 01:00:49,450
with that how do I go in and actually

01:00:47,020 --> 01:00:50,710
get this application up and running so

01:00:49,450 --> 01:00:56,110
that it's actually you know running on

01:00:50,710 --> 01:01:00,400
that port so so if I go back to to my

01:00:56,110 --> 01:01:04,210
terminal and I go back to the list of

01:01:00,400 --> 01:01:06,100
instructions you can see that so you

01:01:04,210 --> 01:01:07,960
know I I'm still inside the the bash

01:01:06,100 --> 01:01:09,010
session so I don't need to restart it

01:01:07,960 --> 01:01:10,480
but so basically I'm gonna do is I'm

01:01:09,010 --> 01:01:13,300
gonna try to open up engine acess config

01:01:10,480 --> 01:01:16,480
file again vim doesn't happen to be

01:01:13,300 --> 01:01:18,220
installed so I need to install vim so I

01:01:16,480 --> 01:01:20,110
can open the file once vim is installed

01:01:18,220 --> 01:01:25,060
I'm gonna open up engine X's default

01:01:20,110 --> 01:01:26,620
comm file that should be coming in a

01:01:25,060 --> 01:01:32,620
second here once the once the vim

01:01:26,620 --> 01:01:35,500
install is done so I open up default

01:01:32,620 --> 01:01:38,110
conf I see that it's listening on port

01:01:35,500 --> 01:01:40,060
80 as expected and I'm going to change

01:01:38,110 --> 01:01:41,440
that now to the IP you're sorry to the

01:01:40,060 --> 01:01:43,710
port that I've actually allocated to

01:01:41,440 --> 01:01:43,710
this application

01:01:49,320 --> 01:01:52,740
83 16

01:01:54,540 --> 01:02:04,620
there it is now I close that out I come

01:02:00,060 --> 01:02:06,570
back I restart engine X and I would say

01:02:04,620 --> 01:02:08,070
let's hope the demo gods let this work

01:02:06,570 --> 01:02:12,750
but because I recorded it I know it

01:02:08,070 --> 01:02:14,310
works and when you refresh this page we

01:02:12,750 --> 01:02:18,030
should see you know the welcome to

01:02:14,310 --> 01:02:20,520
engine X then come up yeah so there it

01:02:18,030 --> 01:02:24,570
is and that's it for the demo

01:02:20,520 --> 01:02:26,700
and yes so let's see what else I have in

01:02:24,570 --> 01:02:28,440
the slides here so yeah so real quick I

01:02:26,700 --> 01:02:30,180
just wanted to give special thanks to a

01:02:28,440 --> 01:02:32,010
lot of the people that helped us work on

01:02:30,180 --> 01:02:35,280
these various features that we've kind

01:02:32,010 --> 01:02:37,020
of demonstrated today you know there's

01:02:35,280 --> 01:02:38,340
pieces that are involved with security

01:02:37,020 --> 01:02:39,720
to make sure that all the authorization

01:02:38,340 --> 01:02:41,580
stuff happens properly there's people

01:02:39,720 --> 01:02:44,280
that put work in so getting the

01:02:41,580 --> 01:02:46,080
different HTTP API is for task exec put

01:02:44,280 --> 01:02:47,700
in there I'm sure on the marathon side

01:02:46,080 --> 01:02:49,290
there was a lot of people involved more

01:02:47,700 --> 01:02:51,570
than johannes for forgetting some of

01:02:49,290 --> 01:02:53,660
that stuff working and so on and also on

01:02:51,570 --> 01:02:56,010
the CLI bits that we had to put together

01:02:53,660 --> 01:02:57,270
so yeah with that does anyone have any

01:02:56,010 --> 01:02:58,740
questions about this or anything else

01:02:57,270 --> 01:03:01,860
and you know right after this we'll jump

01:02:58,740 --> 01:03:03,360
into the the live interactive session

01:03:01,860 --> 01:03:07,580
with you guys so you can play around

01:03:03,360 --> 01:03:07,580
with some of these things yep

01:03:12,560 --> 01:03:17,210
so so the question was can you prohibit

01:03:15,110 --> 01:03:19,220
someone from exacting into the container

01:03:17,210 --> 01:03:21,260
that they themselves have deployed you

01:03:19,220 --> 01:03:22,910
could if you want to yeah it's it's very

01:03:21,260 --> 01:03:25,700
fine-grained ACLU you have to give them

01:03:22,910 --> 01:03:28,220
explicit permissions for hitting those

01:03:25,700 --> 01:03:29,360
api's that we've added so if they don't

01:03:28,220 --> 01:03:31,010
have permission to hit the launch that's

01:03:29,360 --> 01:03:32,600
a container session API they won't be

01:03:31,010 --> 01:03:34,070
able to do it even though they have the

01:03:32,600 --> 01:03:36,310
create permissions for the container to

01:03:34,070 --> 01:03:41,690
begin with yep

01:03:36,310 --> 01:03:44,200
any other questions okay so we'll leave

01:03:41,690 --> 01:03:46,250
this slide up but basically we have a

01:03:44,200 --> 01:03:48,080
DCFS cluster up and running that you

01:03:46,250 --> 01:03:50,660
guys can navigate to and play around

01:03:48,080 --> 01:03:52,940
with the public agent that we have so we

01:03:50,660 --> 01:03:54,710
you know I think gaston mentioned the

01:03:52,940 --> 01:03:56,750
the layout of this cluster we've got

01:03:54,710 --> 01:03:58,130
five agents that are private and one

01:03:56,750 --> 01:04:00,050
public agent and we're running a

01:03:58,130 --> 01:04:01,520
marathon I'll be on that public agent if

01:04:00,050 --> 01:04:03,230
you wanted to deploy that engine X

01:04:01,520 --> 01:04:05,150
container that I showed you before you

01:04:03,230 --> 01:04:06,650
can deploy that on this public agent

01:04:05,150 --> 01:04:08,930
it'll happen automatically by putting

01:04:06,650 --> 01:04:10,220
the slave public constraint but that's

01:04:08,930 --> 01:04:13,730
where it'll land in case you need the IP

01:04:10,220 --> 01:04:15,200
address we have a set of user accounts

01:04:13,730 --> 01:04:17,960
that we've created underneath the

01:04:15,200 --> 01:04:21,260
students group the username for that is

01:04:17,960 --> 01:04:23,030
user 1 through 40 I don't know the best

01:04:21,260 --> 01:04:26,120
way to allocate that out to you guys but

01:04:23,030 --> 01:04:27,380
you can try and log in with one and see

01:04:26,120 --> 01:04:28,820
if you're unique on there and if you

01:04:27,380 --> 01:04:31,250
don't care about other people messing

01:04:28,820 --> 01:04:33,500
with your environment then then you can

01:04:31,250 --> 01:04:35,810
switch to another one and then we have a

01:04:33,500 --> 01:04:37,190
gist with all the scripts and JSON for

01:04:35,810 --> 01:04:38,510
all the different things that we that we

01:04:37,190 --> 01:04:40,400
showed during this presentation so you

01:04:38,510 --> 01:04:43,730
can you know deploy those yourself and

01:04:40,400 --> 01:04:47,030
see how all this works so yep that's it

01:04:43,730 --> 01:04:49,070
there's no other questions me Phillip

01:04:47,030 --> 01:04:51,080
Gaston and Johannes will be around for

01:04:49,070 --> 01:04:52,250
you know any questions that you have as

01:04:51,080 --> 01:04:55,290
you're going through this stuff yourself

01:04:52,250 --> 01:04:58,590
so thanks again

01:04:55,290 --> 01:04:58,590

YouTube URL: https://www.youtube.com/watch?v=aBPzBPq-lHU


