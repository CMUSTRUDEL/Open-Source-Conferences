Title: LF Networking: "Self-Healing with Analytics for K8s Workloads" - Slides
Publication date: 2020-10-23
Playlist: LFN + LF Edge Demo Pavilion ONES 2020
Description: 
	LF Networking: "Self-Healing with Analytics for K8s Workloads" - Slides
Captions: 
	00:00:02,720 --> 00:00:05,680
hi

00:00:03,439 --> 00:00:07,680
i'm avner bernak and today i'm going to

00:00:05,680 --> 00:00:09,840
show you how platform telemetry

00:00:07,680 --> 00:00:10,800
enabled with operators can be used to

00:00:09,840 --> 00:00:12,880
maximize

00:00:10,800 --> 00:00:14,920
service availability for latency

00:00:12,880 --> 00:00:17,920
sensitive workloads in a kubernetes

00:00:14,920 --> 00:00:17,920
environment

00:00:21,520 --> 00:00:25,760
thanks to the team to help make this

00:00:23,199 --> 00:00:25,760
work happen

00:00:27,199 --> 00:00:30,720
latency sensitive kubernetes

00:00:28,880 --> 00:00:32,559
applications depend on access to

00:00:30,720 --> 00:00:34,559
resilient hardware resources

00:00:32,559 --> 00:00:35,840
to allow fast low latency packet

00:00:34,559 --> 00:00:37,760
processing

00:00:35,840 --> 00:00:39,200
understanding and exposing the health of

00:00:37,760 --> 00:00:40,800
the underlying platform

00:00:39,200 --> 00:00:42,800
is key in maintaining application

00:00:40,800 --> 00:00:43,440
performance allowing the network

00:00:42,800 --> 00:00:46,879
operator

00:00:43,440 --> 00:00:48,640
to react to issues and maintain slas

00:00:46,879 --> 00:00:49,920
and while this monitoring and reaction

00:00:48,640 --> 00:00:51,680
can be done manually

00:00:49,920 --> 00:00:53,440
this demo showcases the necessary

00:00:51,680 --> 00:00:55,600
components for our zero touch

00:00:53,440 --> 00:00:56,960
automated infrastructure combining

00:00:55,600 --> 00:00:58,559
kubernetes enhancements

00:00:56,960 --> 00:01:01,199
the operator framework and host

00:00:58,559 --> 00:01:01,199
telemetry

00:01:03,920 --> 00:01:08,240
closed loop automation is the process of

00:01:06,080 --> 00:01:10,720
identifying a set of metrics

00:01:08,240 --> 00:01:13,200
measuring them detecting anomalies and

00:01:10,720 --> 00:01:15,200
correcting them via necessary action

00:01:13,200 --> 00:01:18,159
this process of closed loop automation

00:01:15,200 --> 00:01:20,960
helps reduce the operational expenditure

00:01:18,159 --> 00:01:22,000
within opnfv the closed-loop automation

00:01:20,960 --> 00:01:24,479
working group

00:01:22,000 --> 00:01:25,360
focuses on nfvi based closed-loop use

00:01:24,479 --> 00:01:27,920
cases

00:01:25,360 --> 00:01:28,479
and helps catalog capabilities identify

00:01:27,920 --> 00:01:31,920
gaps

00:01:28,479 --> 00:01:31,920
and provide reference solutions

00:01:34,720 --> 00:01:38,720
using components from the infowatch

00:01:36,560 --> 00:01:39,520
project collecti and prometheus

00:01:38,720 --> 00:01:41,360
operators

00:01:39,520 --> 00:01:43,360
host health telemetry is streamed from

00:01:41,360 --> 00:01:44,560
the platform and used to make informed

00:01:43,360 --> 00:01:46,159
scheduling decisions

00:01:44,560 --> 00:01:48,240
with telemetry aware scheduler in the

00:01:46,159 --> 00:01:50,159
kubernetes control plane

00:01:48,240 --> 00:01:51,840
by automatically reacting to issues in

00:01:50,159 --> 00:01:53,439
the network and intelligently

00:01:51,840 --> 00:01:55,840
orchestrating solutions

00:01:53,439 --> 00:01:56,560
this minimizes application downtime and

00:01:55,840 --> 00:01:59,680
as a result

00:01:56,560 --> 00:02:01,600
maximizes service availability for the

00:01:59,680 --> 00:02:04,320
components used in this demo

00:02:01,600 --> 00:02:06,399
intel pmu and rdt collecti plugins

00:02:04,320 --> 00:02:08,479
available in opnfv barometer

00:02:06,399 --> 00:02:10,319
are used to monitor cpu cache counters

00:02:08,479 --> 00:02:12,000
and memory bandwidth usage

00:02:10,319 --> 00:02:13,840
these provide an indicator as to the

00:02:12,000 --> 00:02:15,680
health of the platform and there's more

00:02:13,840 --> 00:02:16,879
info on this in the accompanying demo

00:02:15,680 --> 00:02:19,040
video

00:02:16,879 --> 00:02:20,800
operators from the infrared project are

00:02:19,040 --> 00:02:21,760
used to deploy and configure collect d

00:02:20,800 --> 00:02:23,680
and prometheus

00:02:21,760 --> 00:02:25,840
providing up-to-date metrics from each

00:02:23,680 --> 00:02:27,920
cluster and node

00:02:25,840 --> 00:02:30,000
the kubernetes telemetry rare scheduler

00:02:27,920 --> 00:02:31,760
is used to deploy pods based on platform

00:02:30,000 --> 00:02:33,200
telemetry collected and stored

00:02:31,760 --> 00:02:35,680
with the collecti and prometheus

00:02:33,200 --> 00:02:36,000
operators this forms the basis for this

00:02:35,680 --> 00:02:38,480
platform

00:02:36,000 --> 00:02:40,400
resiliency demo and more info on this in

00:02:38,480 --> 00:02:44,080
a few slides

00:02:40,400 --> 00:02:45,840
for this demo two intel xeon 6230n

00:02:44,080 --> 00:02:48,239
platforms are used in this kubernetes

00:02:45,840 --> 00:02:50,720
cluster however the platform telemetry

00:02:48,239 --> 00:02:52,239
is available across all intel servers

00:02:50,720 --> 00:02:54,640
for a list of available collect-e

00:02:52,239 --> 00:02:57,599
plug-ins please see the link to op-nfe

00:02:54,640 --> 00:02:57,599
barometer provided

00:02:59,040 --> 00:03:03,280
in kubernetes an administrator can

00:03:01,200 --> 00:03:04,959
define various objects using manifests

00:03:03,280 --> 00:03:07,200
written in yaml

00:03:04,959 --> 00:03:09,120
these manifest define objects such as

00:03:07,200 --> 00:03:11,120
deployments for how to start a pod

00:03:09,120 --> 00:03:13,680
made up of one or more containers

00:03:11,120 --> 00:03:16,000
services for exposing service ports

00:03:13,680 --> 00:03:16,959
and ingress in kubernetes are roots in

00:03:16,000 --> 00:03:18,720
openshift

00:03:16,959 --> 00:03:20,640
for accessing the application from an

00:03:18,720 --> 00:03:22,239
external network

00:03:20,640 --> 00:03:24,319
instead of defining all the manifests

00:03:22,239 --> 00:03:25,760
manually and then having the overhead of

00:03:24,319 --> 00:03:26,799
managing the setup of different

00:03:25,760 --> 00:03:28,640
instances

00:03:26,799 --> 00:03:30,560
an operator can manage these objects for

00:03:28,640 --> 00:03:31,360
you by defining a custom resource

00:03:30,560 --> 00:03:33,599
definition

00:03:31,360 --> 00:03:35,599
extending the kubernetes api and a

00:03:33,599 --> 00:03:37,200
custom resource object

00:03:35,599 --> 00:03:39,200
operators enabled the automated

00:03:37,200 --> 00:03:39,599
management of these objects resulting in

00:03:39,200 --> 00:03:42,400
better

00:03:39,599 --> 00:03:46,959
object life cycle management ease of use

00:03:42,400 --> 00:03:48,720
and deployment

00:03:46,959 --> 00:03:50,599
the collect the operator defines a new

00:03:48,720 --> 00:03:52,640
api interface called

00:03:50,599 --> 00:03:55,040
collectee.infra.watch

00:03:52,640 --> 00:03:56,319
once the collecti operator is loaded an

00:03:55,040 --> 00:03:58,159
instance of collect d

00:03:56,319 --> 00:03:59,599
can then be started on all nodes by

00:03:58,159 --> 00:04:02,799
creating a collecti

00:03:59,599 --> 00:04:04,720
object when defining the collecti object

00:04:02,799 --> 00:04:06,720
the operator manages loading collecti

00:04:04,720 --> 00:04:08,560
configuration with a list of plugins and

00:04:06,720 --> 00:04:10,000
their parameters

00:04:08,560 --> 00:04:11,599
the collectee object will then be

00:04:10,000 --> 00:04:13,040
watched by the operator

00:04:11,599 --> 00:04:14,879
and will manage the creation of all

00:04:13,040 --> 00:04:17,519
dependent objects results again

00:04:14,879 --> 00:04:19,280
collectively running on all nodes

00:04:17,519 --> 00:04:20,720
the operator framework allows easier

00:04:19,280 --> 00:04:22,079
management and configuration of

00:04:20,720 --> 00:04:24,000
collect-e and its plug-ins in a

00:04:22,079 --> 00:04:26,320
kubernetes environment

00:04:24,000 --> 00:04:27,919
within kubernetes there is no mechanism

00:04:26,320 --> 00:04:28,800
for using telemetry and scheduling

00:04:27,919 --> 00:04:30,880
decisions

00:04:28,800 --> 00:04:32,960
and as a result there is no workload

00:04:30,880 --> 00:04:34,479
migration in advance or during node

00:04:32,960 --> 00:04:36,320
health deterioration

00:04:34,479 --> 00:04:38,800
this can lead to service disruption and

00:04:36,320 --> 00:04:41,199
application downtime

00:04:38,800 --> 00:04:43,120
telemetry of our scheduling or tas is an

00:04:41,199 --> 00:04:45,120
extension to the kubernetes scheduler

00:04:43,120 --> 00:04:46,800
that uses telemetry data to make pod

00:04:45,120 --> 00:04:48,800
scheduling decisions

00:04:46,800 --> 00:04:50,000
through a rule-based and user-defined

00:04:48,800 --> 00:04:51,600
task policy

00:04:50,000 --> 00:04:53,600
scheduling decisions are made with

00:04:51,600 --> 00:04:56,479
up-to-date known metrics

00:04:53,600 --> 00:04:58,479
by making tas aware of no telemetry pods

00:04:56,479 --> 00:05:00,160
can be orchestrated more intelligently

00:04:58,479 --> 00:05:03,919
and as a result reduce service

00:05:00,160 --> 00:05:03,919
disruption and application downtime

00:05:06,240 --> 00:05:10,000
taking a look at an example scenario a

00:05:08,639 --> 00:05:13,120
workload will be scheduled

00:05:10,000 --> 00:05:14,560
based on the node's cpu usage firstly

00:05:13,120 --> 00:05:17,199
the policy cpu

00:05:14,560 --> 00:05:18,479
used is created specifying to schedule

00:05:17,199 --> 00:05:21,840
pods on nodes

00:05:18,479 --> 00:05:24,400
with cpu usage of less than 50

00:05:21,840 --> 00:05:26,240
then a pod is specified with the cpu

00:05:24,400 --> 00:05:28,000
used policy

00:05:26,240 --> 00:05:30,400
when the pot is to be scheduled the

00:05:28,000 --> 00:05:32,240
scheduler asks taz for input

00:05:30,400 --> 00:05:35,360
tazer views the policy and gets

00:05:32,240 --> 00:05:36,720
associated metrics from the metrics api

00:05:35,360 --> 00:05:38,880
all nodes are then ranked and

00:05:36,720 --> 00:05:41,039
prioritized by their cpu usage

00:05:38,880 --> 00:05:42,800
and the scheduler makes decision based

00:05:41,039 --> 00:05:45,360
on the priorities and schedules the pod

00:05:42,800 --> 00:05:45,360
to node b

00:05:48,240 --> 00:05:52,479
to view a work demo of the closed loops

00:05:50,240 --> 00:05:56,160
resiliency components working together

00:05:52,479 --> 00:05:56,160
please view the accompanying video

00:05:58,880 --> 00:06:03,360
this demo showcases how components from

00:06:00,880 --> 00:06:05,360
infowatch combined with host telemetry

00:06:03,360 --> 00:06:07,600
enable a zero touch automated and

00:06:05,360 --> 00:06:09,759
resilient network infrastructure

00:06:07,600 --> 00:06:11,919
collect d can also be deployed using the

00:06:09,759 --> 00:06:13,520
same tooling in openstack platform

00:06:11,919 --> 00:06:15,520
to enable host telemetry within the

00:06:13,520 --> 00:06:17,120
service telemetry framework

00:06:15,520 --> 00:06:19,120
combining these elements into a closed

00:06:17,120 --> 00:06:20,560
loop solution provides a starting point

00:06:19,120 --> 00:06:24,240
for tomorrow's next generation

00:06:20,560 --> 00:06:24,240
self-healing and self-optimizing

00:06:32,520 --> 00:06:35,520

YouTube URL: https://www.youtube.com/watch?v=1eaDYPhvU5I


