Title: Extending Deployment for Internet Financial Mission-Critical Scenarios - Mengyi Zhou & Ke Wu
Publication date: 2019-07-10
Playlist: Shanghai '19: KubeCon + CloudNativeCon + Open Source Summit
Description: 
	Extending Deployment for Internet Financial Mission-Critical Scenarios - Mengyi Zhou & Ke Wu, Ant Financial 

The default deployment provides a good solution to perform a general version upgrade. However, deploying highly available and reliable services of large-scale as Internet financial applications is a different thing, not to mention the compatibility problems this workload is faced with the existing operation and maintenance systems.    The new workload introduced by Ant Financial solves these problems. It extends the ability of Deployment with the commitment of a reliable and flexible distribution, risk controlled deployment strategy, and high-performance in-place update. It removes tech barriers, specifically from financial service industry, enables developers and operators to focus on what truly matters to their businesses. 

https://sched.co/Nrlr
Captions: 
	00:00:00,150 --> 00:00:04,830
[Music]

00:00:01,399 --> 00:00:07,770
before I start my presentation today I

00:00:04,830 --> 00:00:11,670
would like to do a little survey how

00:00:07,770 --> 00:00:14,849
many of you here today have used or

00:00:11,670 --> 00:00:18,320
still using deployment of for release

00:00:14,849 --> 00:00:18,320
please show me your hands

00:00:18,980 --> 00:00:24,019
so how many of you have Anna tried

00:00:21,980 --> 00:00:27,180
deployment anymore

00:00:24,019 --> 00:00:29,580
so we still have for people who haven't

00:00:27,180 --> 00:00:33,960
put up their hands in the two rounds of

00:00:29,580 --> 00:00:36,989
a survey and I will be very quickly in

00:00:33,960 --> 00:00:41,579
the first part and I will save more time

00:00:36,989 --> 00:00:44,820
for the demo the kubernetes deployment

00:00:41,579 --> 00:00:47,640
is the replication controller and there

00:00:44,820 --> 00:00:51,360
are two important tasks completed that

00:00:47,640 --> 00:00:54,510
is at the replication and the rolling

00:00:51,360 --> 00:00:58,500
updated as too important has since then

00:00:54,510 --> 00:01:02,760
kubernetes are solving the problem of

00:00:58,500 --> 00:01:05,670
availability and the consistency when we

00:01:02,760 --> 00:01:08,850
move to deployment the deployment and

00:01:05,670 --> 00:01:11,790
the repla replica set has a structure of

00:01:08,850 --> 00:01:14,790
the two layers so we have the capability

00:01:11,790 --> 00:01:17,700
of a version control so we can do the

00:01:14,790 --> 00:01:20,189
rolling back after the release we can

00:01:17,700 --> 00:01:21,900
return back to the historical vision so

00:01:20,189 --> 00:01:24,530
that is the current status of a

00:01:21,900 --> 00:01:24,530
deployment

00:01:25,210 --> 00:01:32,000
so when we evaluate the deployment for

00:01:29,750 --> 00:01:34,039
the cloud native release what are their

00:01:32,000 --> 00:01:38,329
challenges the first challenge is that

00:01:34,039 --> 00:01:41,780
the deployment strategy as the R&D team

00:01:38,329 --> 00:01:44,119
I think you have one thing in common we

00:01:41,780 --> 00:01:46,310
have the application it doesn't mean our

00:01:44,119 --> 00:01:49,700
application is available especially

00:01:46,310 --> 00:01:52,100
during the deployment process if we want

00:01:49,700 --> 00:01:53,270
to check whether it is available what we

00:01:52,100 --> 00:01:55,640
do first

00:01:53,270 --> 00:01:58,250
we will check whether there is an

00:01:55,640 --> 00:02:01,759
abnormality from a monitoring we will

00:01:58,250 --> 00:02:04,100
refer back to the main link whether the

00:02:01,759 --> 00:02:05,990
major features are normal and the

00:02:04,100 --> 00:02:09,860
thirdly whether the changes are

00:02:05,990 --> 00:02:12,440
effective after we do this confirmation

00:02:09,860 --> 00:02:15,110
or verification we can make sure the

00:02:12,440 --> 00:02:19,549
service is available when we use a

00:02:15,110 --> 00:02:22,910
deployment of a rolling update we didn't

00:02:19,549 --> 00:02:26,209
have the time for verification and there

00:02:22,910 --> 00:02:30,739
is a pass mechanism in deployment but

00:02:26,209 --> 00:02:34,609
when we really ran the pass it's highly

00:02:30,739 --> 00:02:37,519
possible the rolling update is continued

00:02:34,609 --> 00:02:39,859
so we cannot have accurate the process

00:02:37,519 --> 00:02:42,440
control during the deployment process

00:02:39,859 --> 00:02:45,620
and the second challenge is at the pod

00:02:42,440 --> 00:02:47,120
scheduling strategy we found that under

00:02:45,620 --> 00:02:50,000
one deployment that the positive

00:02:47,120 --> 00:02:52,760
scheduling strategy is

00:02:50,000 --> 00:02:55,280
the same what is the problem we can have

00:02:52,760 --> 00:02:58,220
look at this scenario imagine we have

00:02:55,280 --> 00:03:03,230
the aircraft the clusters are

00:02:58,220 --> 00:03:06,440
distributed in two data centers and I

00:03:03,230 --> 00:03:08,840
want to release four poles and the

00:03:06,440 --> 00:03:14,090
scheduling strategy is exactly the same

00:03:08,840 --> 00:03:17,800
so it's possible that my four poles will

00:03:14,090 --> 00:03:21,950
be scheduled to only one server and

00:03:17,800 --> 00:03:24,650
surely is okay and it will all be routed

00:03:21,950 --> 00:03:27,770
to the same server but in case of

00:03:24,650 --> 00:03:33,980
disaster for example there is a road

00:03:27,770 --> 00:03:36,470
construction outside of the server and I

00:03:33,980 --> 00:03:39,500
don't have a power supply then I don't

00:03:36,470 --> 00:03:42,200
have this ourselves are still under with

00:03:39,500 --> 00:03:44,989
the carbon copper Nettie's operation

00:03:42,200 --> 00:03:48,680
mechanism of a well be shifted to other

00:03:44,989 --> 00:03:50,860
server but the backup time is out of our

00:03:48,680 --> 00:03:50,860
control

00:03:51,310 --> 00:03:59,630
that is why we say if you want to have a

00:03:54,590 --> 00:04:03,799
high availability we need to have these

00:03:59,630 --> 00:04:06,680
backup in one city or in multiple cities

00:04:03,799 --> 00:04:10,580
and if there's a disaster happening to

00:04:06,680 --> 00:04:13,100
our machine room or the computer room we

00:04:10,580 --> 00:04:15,620
might not have enough resources so

00:04:13,100 --> 00:04:18,080
whether my application can be recovered

00:04:15,620 --> 00:04:18,950
in another Center that is a very big

00:04:18,080 --> 00:04:22,120
question mark

00:04:18,950 --> 00:04:26,350
so these consistent all the same

00:04:22,120 --> 00:04:29,680
Carolyn strategy to not allow us to have

00:04:26,350 --> 00:04:32,139
the tolerance of disasters or the

00:04:29,680 --> 00:04:33,970
catastrophe tolerance and the third

00:04:32,139 --> 00:04:36,550
challenge is that the IP address

00:04:33,970 --> 00:04:38,530
restrictions we have some users they

00:04:36,550 --> 00:04:40,990
have a very complete maintenance and

00:04:38,530 --> 00:04:45,250
operation system but it is a based on

00:04:40,990 --> 00:04:47,169
the VM reads deployment under VM what

00:04:45,250 --> 00:04:51,300
are the features of all the deployment

00:04:47,169 --> 00:04:55,360
the physical resources the easy as my IP

00:04:51,300 --> 00:04:59,320
will not change with the iteration of

00:04:55,360 --> 00:05:02,260
application the application process kill

00:04:59,320 --> 00:05:05,460
and start again decay my VM I keep

00:05:02,260 --> 00:05:10,930
maintained the same so in the classical

00:05:05,460 --> 00:05:14,320
scenario we take IP as at a standard and

00:05:10,930 --> 00:05:18,300
it is a widely adopted they monitoring

00:05:14,320 --> 00:05:21,490
the link analysis the Gateway and the

00:05:18,300 --> 00:05:26,380
security wireless the strategy all have

00:05:21,490 --> 00:05:29,080
a taken IP as the standard if it is

00:05:26,380 --> 00:05:34,840
related relocated from the empty

00:05:29,080 --> 00:05:38,050
container model the IP addresses are

00:05:34,840 --> 00:05:42,099
still maintained the same and in the

00:05:38,050 --> 00:05:45,460
process we found that we can not take a

00:05:42,099 --> 00:05:48,849
deployment to satisfy the needs of our

00:05:45,460 --> 00:05:54,729
customers so we have developed our own

00:05:48,849 --> 00:05:58,690
CR and recorded a cafe deployment our

00:05:54,729 --> 00:06:03,490
cafe deployment supported the in place

00:05:58,690 --> 00:06:06,820
upgrade emissivity release scenarios we

00:06:03,490 --> 00:06:10,120
can make sure the pod and the pot is a

00:06:06,820 --> 00:06:12,880
stable IP is the same and we have

00:06:10,120 --> 00:06:17,130
considered the high availability in the

00:06:12,880 --> 00:06:19,660
financial scenarios and a supposed

00:06:17,130 --> 00:06:24,570
active/active replication or multi

00:06:19,660 --> 00:06:27,639
active replication and our poll has the

00:06:24,570 --> 00:06:28,660
graceful shutdown feature the official

00:06:27,639 --> 00:06:31,000
kubernetes

00:06:28,660 --> 00:06:33,219
also has a niqab ability of a graceful

00:06:31,000 --> 00:06:35,580
shutdown but if the concurrency

00:06:33,219 --> 00:06:38,560
accumulated to a certain amount the

00:06:35,580 --> 00:06:40,930
Porter cannot achieve a fully graceful

00:06:38,560 --> 00:06:43,150
shutdown and the cafe deployment

00:06:40,930 --> 00:06:46,710
supports a safe and a flexible

00:06:43,150 --> 00:06:50,860
deployment strategy and there's a beta

00:06:46,710 --> 00:06:53,229
or the rolling update in patches so that

00:06:50,860 --> 00:06:55,210
they can have accurate control over the

00:06:53,229 --> 00:06:57,460
deployment process and they have enough

00:06:55,210 --> 00:06:59,530
time of for verification so these are

00:06:57,460 --> 00:07:01,870
the three features of the cafe

00:06:59,530 --> 00:07:06,190
deployment max is that the structure of

00:07:01,870 --> 00:07:08,740
a cafe deployment and it is the same as

00:07:06,190 --> 00:07:11,370
a kubernetes deployment in Hydra three

00:07:08,740 --> 00:07:16,690
layers the middle air is in place that

00:07:11,370 --> 00:07:19,120
in place that is something which it is a

00:07:16,690 --> 00:07:23,289
replica set which support in-place

00:07:19,120 --> 00:07:26,950
upgrade and a maintenance and the number

00:07:23,289 --> 00:07:29,800
of reporters and cafe deployment will

00:07:26,950 --> 00:07:33,279
not have a direct interaction with the

00:07:29,800 --> 00:07:36,340
port and we have in place that it is

00:07:33,279 --> 00:07:37,660
amazing for the in-place upgrade either

00:07:36,340 --> 00:07:40,570
listed

00:07:37,660 --> 00:07:43,810
in place upgrade configuration we can

00:07:40,570 --> 00:07:46,510
support if the changes that happen in

00:07:43,810 --> 00:07:53,710
these areas we can maintain stable port

00:07:46,510 --> 00:07:56,500
and IP and the cafe deployment naturally

00:07:53,710 --> 00:08:00,430
support active active replication or

00:07:56,500 --> 00:08:05,260
multi active replication and in the spec

00:08:00,430 --> 00:08:08,710
we have a clear definition of the

00:08:05,260 --> 00:08:12,610
topology of the application on the left

00:08:08,710 --> 00:08:16,150
side you can see the output and the

00:08:12,610 --> 00:08:17,320
cluster has covered two data centers in

00:08:16,150 --> 00:08:21,040
each data center

00:08:17,320 --> 00:08:25,000
I have denote when the node is a created

00:08:21,040 --> 00:08:29,160
a wall labeled with the data center

00:08:25,000 --> 00:08:34,930
under this label helps during the

00:08:29,160 --> 00:08:38,039
calling of the note that it can satisfy

00:08:34,930 --> 00:08:42,070
the set of requirements and industry

00:08:38,039 --> 00:08:46,180
according to the topology in tasks in

00:08:42,070 --> 00:08:49,570
the spec it was created the unique in

00:08:46,180 --> 00:08:52,540
place at and in the in place that is a

00:08:49,570 --> 00:08:54,760
unique to the data center and in place

00:08:52,540 --> 00:08:58,330
that is responsible for the maintenance

00:08:54,760 --> 00:09:00,820
of the life cycle of the pas-de-calais

00:08:58,330 --> 00:09:04,750
deployment it takes that the code view

00:09:00,820 --> 00:09:08,380
and a decides in each data center how

00:09:04,750 --> 00:09:12,130
how many replicas I need under the cafe

00:09:08,380 --> 00:09:15,250
deployment as the in-place upgrade and

00:09:12,130 --> 00:09:16,990
the photo tolerance or disaster

00:09:15,250 --> 00:09:20,080
tolerance the feature next I want to

00:09:16,990 --> 00:09:24,190
introduce how tend to share with you

00:09:20,080 --> 00:09:26,370
that deployment strategy of a cafe thank

00:09:24,190 --> 00:09:26,370
you

00:09:27,190 --> 00:09:33,649
next I would like to present the

00:09:30,139 --> 00:09:37,190
features of a cafe deployment number one

00:09:33,649 --> 00:09:39,500
is the deployment strategy I will give

00:09:37,190 --> 00:09:41,589
you a simple example there is a cafe

00:09:39,500 --> 00:09:44,870
deployment and they're about two

00:09:41,589 --> 00:09:47,480
datacenters connected and we have in

00:09:44,870 --> 00:09:52,850
place at a and the employees at B and

00:09:47,480 --> 00:09:56,089
the left bottom is part of the specs of

00:09:52,850 --> 00:10:00,319
the cafe deployment and we have a ten

00:09:56,089 --> 00:10:04,279
replicas by default it they are divided

00:10:00,319 --> 00:10:07,279
them equally to the two data centers and

00:10:04,279 --> 00:10:10,100
the employees at a has a fight pose and

00:10:07,279 --> 00:10:14,029
increase FP also have a fight pause and

00:10:10,100 --> 00:10:17,089
we also have the beta of upgraded type

00:10:14,029 --> 00:10:19,639
and that the tempest will be divided

00:10:17,089 --> 00:10:22,670
into three groups the first is a beta

00:10:19,639 --> 00:10:26,750
group and the other two groups are the

00:10:22,670 --> 00:10:30,560
standard patches and the beta group let

00:10:26,750 --> 00:10:35,120
me explain we take one random part from

00:10:30,560 --> 00:10:37,819
each data center for a new version if

00:10:35,120 --> 00:10:40,310
the customers say no problem that we can

00:10:37,819 --> 00:10:44,420
move to the standard every our

00:10:40,310 --> 00:10:47,350
deployment and client and need to click

00:10:44,420 --> 00:10:47,350
and to confirm

00:10:49,860 --> 00:10:57,959
when we first have a cafe deployment the

00:10:53,940 --> 00:11:05,130
pod growth of our material if the

00:10:57,959 --> 00:11:07,950
deployment has a 100 replicas and if we

00:11:05,130 --> 00:11:11,040
create directly it is a 100 pass and the

00:11:07,950 --> 00:11:13,019
one part has a problem with the image

00:11:11,040 --> 00:11:15,360
and then it will cause waste of

00:11:13,019 --> 00:11:19,589
resources because your deployment

00:11:15,360 --> 00:11:24,120
affairs and the creation of the pause is

00:11:19,589 --> 00:11:27,810
a bad batch we will first create the

00:11:24,120 --> 00:11:30,870
pass in the beta group and one in each

00:11:27,810 --> 00:11:33,240
data center and if the customer confirms

00:11:30,870 --> 00:11:36,120
and no problem we will have the

00:11:33,240 --> 00:11:40,980
following patches and each batch we have

00:11:36,120 --> 00:11:43,230
a 4-4-2 in each data center and one in

00:11:40,980 --> 00:11:46,410
place that will have a two poles and

00:11:43,230 --> 00:11:50,279
then customer confirm and then the last

00:11:46,410 --> 00:11:53,760
batch of the four poles will be created

00:11:50,279 --> 00:11:56,820
in total temples during the upgrade of a

00:11:53,760 --> 00:11:59,010
part we will follow the same deployment

00:11:56,820 --> 00:12:01,560
strategy facilitator group with the

00:11:59,010 --> 00:12:06,300
customer confirms and no problem and we

00:12:01,560 --> 00:12:09,120
move through the batch upgrade and every

00:12:06,300 --> 00:12:12,290
batch is a four-poster and confirmed

00:12:09,120 --> 00:12:15,529
again and then we have the second batch

00:12:12,290 --> 00:12:18,140
during the deployment process cafe

00:12:15,529 --> 00:12:22,820
diploma controller

00:12:18,140 --> 00:12:26,079
well not directly apply on the part the

00:12:22,820 --> 00:12:29,630
part of creation and the upgrade is of

00:12:26,079 --> 00:12:32,329
vampire in place that controller and the

00:12:29,630 --> 00:12:35,959
cafe deployment controller is only to

00:12:32,329 --> 00:12:38,839
abstract the strategy during the

00:12:35,959 --> 00:12:43,640
deployment process if the part version

00:12:38,839 --> 00:12:49,399
has a problem and the customer can abort

00:12:43,640 --> 00:12:54,350
and undo the upgrade during the

00:12:49,399 --> 00:12:56,630
deployment process we might find that in

00:12:54,350 --> 00:13:00,500
place that a do not have enough

00:12:56,630 --> 00:13:05,510
resources and then maybe one port cannot

00:13:00,500 --> 00:13:08,470
be created so we have a rescheduling

00:13:05,510 --> 00:13:14,140
mechanism and it will be automatically

00:13:08,470 --> 00:13:17,870
rescheduled to the other in place at

00:13:14,140 --> 00:13:20,120
some customers can close the circle

00:13:17,870 --> 00:13:22,910
figuration if they don't need it if the

00:13:20,120 --> 00:13:26,000
customer want to have for for posts in

00:13:22,910 --> 00:13:29,180
one data center and all the others will

00:13:26,000 --> 00:13:32,329
be deployed in other in place that then

00:13:29,180 --> 00:13:36,680
you can have the specified for example

00:13:32,329 --> 00:13:38,920
DC age for pose and the DC be 60% of the

00:13:36,680 --> 00:13:38,920
past

00:13:42,190 --> 00:13:49,730
and the cafe deployment it's not

00:13:46,520 --> 00:13:52,880
directly working on the past there is an

00:13:49,730 --> 00:13:56,480
interface port self control interface

00:13:52,880 --> 00:14:04,480
and they use it this interface to work

00:13:56,480 --> 00:14:08,830
out the bottom workload like the replica

00:14:04,480 --> 00:14:08,830
etc in place that

00:14:10,810 --> 00:14:19,190
has realized the grouping and the high

00:14:16,190 --> 00:14:24,170
availability scheduling across different

00:14:19,190 --> 00:14:29,500
data centers some customers already use

00:14:24,170 --> 00:14:32,270
or plan to use like a regular set

00:14:29,500 --> 00:14:39,590
replica set or state for static

00:14:32,270 --> 00:14:41,510
searchers and the scheduling or the

00:14:39,590 --> 00:14:43,730
schedule between different groups or

00:14:41,510 --> 00:14:47,210
different centers can not be realized

00:14:43,730 --> 00:14:50,000
and they can also use the interface and

00:14:47,210 --> 00:14:54,620
the native work loader can be reinforced

00:14:50,000 --> 00:14:57,760
and we didn't change the code of the

00:14:54,620 --> 00:15:00,830
controllers so the roughly said all

00:14:57,760 --> 00:15:06,040
stateful set can be maintained at the

00:15:00,830 --> 00:15:09,680
previous data so the customers can

00:15:06,040 --> 00:15:12,200
maintain the previous and we will not

00:15:09,680 --> 00:15:15,560
have any attractor any innovation to the

00:15:12,200 --> 00:15:18,200
original code and we have a connected

00:15:15,560 --> 00:15:21,470
the replica set and we will also connect

00:15:18,200 --> 00:15:25,520
to stiva set and this is after

00:15:21,470 --> 00:15:27,860
connecting to the replica set and it is

00:15:25,520 --> 00:15:31,160
the same as in place that in different

00:15:27,860 --> 00:15:33,890
center we will have the replica set it

00:15:31,160 --> 00:15:37,910
is to make sure the version of the pod

00:15:33,890 --> 00:15:41,660
will do the deployment we will create

00:15:37,910 --> 00:15:44,579
new replica set and the new replica set

00:15:41,660 --> 00:15:47,399
will maintain the part of the

00:15:44,579 --> 00:15:50,339
version and the cafe deployment strategy

00:15:47,399 --> 00:15:54,839
or logic can also be used as a the

00:15:50,339 --> 00:15:58,379
replica set and we will create this part

00:15:54,839 --> 00:16:01,709
and customer confirms and then we will

00:15:58,379 --> 00:16:04,259
move to the second batch for Porter and

00:16:01,709 --> 00:16:09,569
the customer confirm again and the last

00:16:04,259 --> 00:16:12,480
batch of opposed will be created so when

00:16:09,569 --> 00:16:15,360
we use in place at afford appointment we

00:16:12,480 --> 00:16:18,089
might encounter the challenges for

00:16:15,360 --> 00:16:20,910
example the shutdown is not graceful and

00:16:18,089 --> 00:16:27,559
there might be feather of a request

00:16:20,910 --> 00:16:32,879
because IP shutdown is the reverse

00:16:27,559 --> 00:16:37,819
inverse incident trigger and that part

00:16:32,879 --> 00:16:42,269
is not ready and then a was inform and

00:16:37,819 --> 00:16:45,059
it will shut down the IP from the list

00:16:42,269 --> 00:16:49,889
and Ava have impact on the IP label and

00:16:45,059 --> 00:16:53,509
then it was shut down their routing if

00:16:49,889 --> 00:16:53,509
the customers

00:16:55,290 --> 00:17:02,320
can have the right to kill to the

00:16:59,350 --> 00:17:04,240
signals it can realize that the graceful

00:17:02,320 --> 00:17:06,220
shutdown but it is not guaranteed that

00:17:04,240 --> 00:17:10,020
you have the degrees for shelter every

00:17:06,220 --> 00:17:12,400
time so the implicit controller is

00:17:10,020 --> 00:17:16,299
selecting the folder graceful shutdown

00:17:12,400 --> 00:17:19,209
we have a routing principle shut down

00:17:16,299 --> 00:17:24,449
first and then we reach to the pardon

00:17:19,209 --> 00:17:32,890
for the crystal shadow and we use the

00:17:24,449 --> 00:17:36,780
replicas gala red nice gauge we can

00:17:32,890 --> 00:17:39,850
change that data for retinas gate to

00:17:36,780 --> 00:17:44,559
indicate whether the party is already or

00:17:39,850 --> 00:17:47,679
not when we upgrade the part year the

00:17:44,559 --> 00:17:51,490
bands we will start readiness gate as a

00:17:47,679 --> 00:17:54,179
force so the gate is not ready in events

00:17:51,490 --> 00:17:59,200
and the pod controller will realize that

00:17:54,179 --> 00:18:05,200
thesis data and it will remove the IP to

00:17:59,200 --> 00:18:08,380
the not ready list and the other routers

00:18:05,200 --> 00:18:11,169
like a cobra see will watch that it's

00:18:08,380 --> 00:18:13,809
not ready and it will also move it

00:18:11,169 --> 00:18:18,779
remove it from the list if we use a

00:18:13,809 --> 00:18:21,739
cluster a p2 exposure these parties

00:18:18,779 --> 00:18:24,779
in place that controller can notice

00:18:21,739 --> 00:18:29,519
realize whether these traffic or this

00:18:24,779 --> 00:18:32,849
router is offline and we here have wait

00:18:29,519 --> 00:18:37,320
30 seconds of logic we wait for 3

00:18:32,849 --> 00:18:39,989
seconds and we assumed that it was

00:18:37,320 --> 00:18:42,359
processed they request all the traffic

00:18:39,989 --> 00:18:44,519
in 3 seconds and then we were started

00:18:42,359 --> 00:18:46,950
the upgrade if the upgrade is a

00:18:44,519 --> 00:18:50,580
successful we will start at the

00:18:46,950 --> 00:18:54,149
readiness Gaeta - - and then the router

00:18:50,580 --> 00:18:56,669
information will be generated of course

00:18:54,149 --> 00:18:59,219
three seconds cannot be a full guarantee

00:18:56,669 --> 00:19:01,799
for all the traffic from the oldest

00:18:59,219 --> 00:19:04,409
process so a true quartz watch at them

00:19:01,799 --> 00:19:07,769
you need the involvement of a load

00:19:04,409 --> 00:19:09,929
balancer we have a sop controller who

00:19:07,769 --> 00:19:12,570
will work together with a Magneto and

00:19:09,929 --> 00:19:16,139
when support will be on the load

00:19:12,570 --> 00:19:19,259
balancer SLP they will have a finaliter

00:19:16,139 --> 00:19:21,809
on the pod used to guarantee support

00:19:19,259 --> 00:19:24,749
will not be revised or changed easily

00:19:21,809 --> 00:19:27,599
but we're truly upwards approach whoever

00:19:24,749 --> 00:19:31,559
takes the retinas gate as first and then

00:19:27,599 --> 00:19:34,200
the routing room will be recognized and

00:19:31,559 --> 00:19:36,509
there will be a true shutdown cause you

00:19:34,200 --> 00:19:39,690
have that and there is no traffic they

00:19:36,509 --> 00:19:40,600
will go back to remove the finalizer and

00:19:39,690 --> 00:19:44,980
then

00:19:40,600 --> 00:19:47,350
no it is a true remove of the traffic so

00:19:44,980 --> 00:19:50,259
there will now be 20 of three seconds

00:19:47,350 --> 00:19:52,899
and that is how you can guarantee it

00:19:50,259 --> 00:20:05,740
will be truly synchronized for the

00:19:52,899 --> 00:20:09,070
upgrading I will man they covered by

00:20:05,740 --> 00:20:12,850
control by creating a control opponent

00:20:09,070 --> 00:20:15,789
by a implicit workload and let me show

00:20:12,850 --> 00:20:19,450
you how did we create the pub in batches

00:20:15,789 --> 00:20:22,149
upgrade and robach and if there's a more

00:20:19,450 --> 00:20:25,269
time I'd like to show you how can we

00:20:22,149 --> 00:20:28,379
change it into silver tea set as the

00:20:25,269 --> 00:20:28,379
bottom or clothes

00:21:16,009 --> 00:21:27,869
okay it seems that it's not so easy to

00:21:22,200 --> 00:21:32,669
operate because my laptop does not show

00:21:27,869 --> 00:21:35,999
me the full screen of this code so allow

00:21:32,669 --> 00:21:44,159
me to frequently came back and now I'm

00:21:35,999 --> 00:21:51,200
going to create an environment that is

00:21:44,159 --> 00:21:51,200
clean with no cafe deployment created

00:22:11,040 --> 00:22:14,040
and

00:22:19,680 --> 00:22:25,020
the current cap a deployment

00:22:21,900 --> 00:22:31,380
configuration we will give you readiness

00:22:25,020 --> 00:22:33,540
and the stress feedback sent for that

00:22:31,380 --> 00:22:39,600
will be each faction was before caught

00:22:33,540 --> 00:22:42,030
up great the tide waits better after you

00:22:39,600 --> 00:22:47,640
do that we will have a time to wait and

00:22:42,030 --> 00:22:50,310
confirm and over here is a button right

00:22:47,640 --> 00:22:52,940
I will be watch how they thought was

00:22:50,310 --> 00:22:52,940
created

00:23:15,160 --> 00:23:21,740
okay you can see I just had a new pot

00:23:18,400 --> 00:23:25,240
and then on this part you can see

00:23:21,740 --> 00:23:32,590
already two part has been provisioned

00:23:25,240 --> 00:23:32,590
and for this window I will be

00:23:32,720 --> 00:23:38,710
keep watching the state of cafe

00:23:35,710 --> 00:23:38,710
deployment

00:23:47,360 --> 00:23:57,500
and over here you can see progressive is

00:23:53,730 --> 00:24:02,850
that it's waiting for confirmation that

00:23:57,500 --> 00:24:04,830
means the Catholic deployment is Odin

00:24:02,850 --> 00:24:07,340
ready waiting for confirmation there are

00:24:04,830 --> 00:24:07,340
two pause

00:24:14,809 --> 00:24:21,299
and I'm here confirmed for the current

00:24:20,700 --> 00:24:24,390
Abeyta

00:24:21,299 --> 00:24:27,500
inverter version of veteran leaves of

00:24:24,390 --> 00:24:27,500
the cafe deployment

00:24:32,650 --> 00:24:40,700
now we provide such a plan that he and

00:24:36,170 --> 00:24:43,370
Phyllis will had a mark if you think

00:24:40,700 --> 00:24:46,250
it's false that means it is not

00:24:43,370 --> 00:24:51,650
confirmed if these are found it is okay

00:24:46,250 --> 00:24:55,000
I think to to better it's okay and I can

00:24:51,650 --> 00:24:55,000
change that to true

00:25:00,409 --> 00:25:07,799
I'm surviving today you can see we

00:25:05,279 --> 00:25:11,549
started to relieve the first patch which

00:25:07,799 --> 00:25:13,799
is four parts the upper right you can

00:25:11,549 --> 00:25:16,379
see the current per crusher it is again

00:25:13,799 --> 00:25:24,889
waiting for confirmation because we have

00:25:16,379 --> 00:25:24,889
finished the first Randolph release I

00:25:25,789 --> 00:25:30,679
can confirm again

00:25:38,260 --> 00:25:47,590
and you can see this is the last one

00:25:41,330 --> 00:25:50,480
when you called number seven to ten

00:25:47,590 --> 00:25:54,140
until I provide you can say the purpose

00:25:50,480 --> 00:25:56,630
is exceeding and now it is completed we

00:25:54,140 --> 00:26:01,820
have all the tempos and are all ready

00:25:56,630 --> 00:26:07,180
and the progress is calculated and the

00:26:01,820 --> 00:26:07,180
left hand side you can see also pot

00:26:15,350 --> 00:26:22,200
how the heart we have a lot vision to

00:26:18,899 --> 00:26:25,799
guarantee there will be deployed in two

00:26:22,200 --> 00:26:28,980
different rooms accountant now the we

00:26:25,799 --> 00:26:32,330
will give him level and see no this

00:26:28,980 --> 00:26:32,330
matters foundation

00:26:43,270 --> 00:26:46,299
[Music]

00:26:50,669 --> 00:26:54,779
and here you can see

00:26:55,670 --> 00:27:02,420
we have had all the nobility of the note

00:26:59,270 --> 00:27:12,200
5 of the husband scientists are able to

00:27:02,420 --> 00:27:15,580
say the answers are for Ruby like to

00:27:12,200 --> 00:27:15,580
show you how we upgrade

00:27:24,740 --> 00:27:31,920
and deport actually has about both

00:27:29,730 --> 00:27:34,530
environments I can change this very well

00:27:31,920 --> 00:27:38,520
friends is fun to be choose let me show

00:27:34,530 --> 00:27:41,490
you how we operate and over here it's

00:27:38,520 --> 00:27:44,240
also culture you can see they have two

00:27:41,490 --> 00:27:44,240
fixed IP

00:27:54,710 --> 00:28:04,950
she tends to support R&D into progress

00:27:57,960 --> 00:28:07,789
of upgrading the currently face is still

00:28:04,950 --> 00:28:07,789
waiting for confirmation

00:28:14,580 --> 00:28:20,370
we can show all the boroughs of all

00:28:17,920 --> 00:28:20,370
parts

00:28:33,670 --> 00:28:40,309
and over here you can see because there

00:28:37,370 --> 00:28:43,910
is a beta release first batch will be

00:28:40,309 --> 00:28:46,400
cheap hot and you can see are they meet

00:28:43,910 --> 00:28:50,900
you I'm still waiting for confirmation

00:28:46,400 --> 00:28:53,320
we can confirm this grouping for beta

00:28:50,900 --> 00:28:53,320
release

00:29:11,920 --> 00:29:23,300
and you can see their first full pot you

00:29:20,690 --> 00:29:25,700
can see it support iti always affects

00:29:23,300 --> 00:29:30,310
because the bottom layer is interested

00:29:25,700 --> 00:29:30,310
so it is what we call in place upgrading

00:29:33,370 --> 00:29:42,910
and over here you can see are they six

00:29:36,830 --> 00:29:42,910
pot as I change this Vegas to be choose

00:29:45,700 --> 00:29:52,840
I want to go back I want to give up

00:29:49,550 --> 00:29:52,840
aunty current release

00:30:02,970 --> 00:30:08,610
yeah you can see you can change the same

00:30:06,130 --> 00:30:08,610
thing

00:30:12,769 --> 00:30:18,539
changed sanitation I gave a about

00:30:16,159 --> 00:30:19,769
ourselves at each other at night now

00:30:18,539 --> 00:30:27,859
that I want to abort

00:30:19,769 --> 00:30:32,570
I love I said you can see it is in the

00:30:27,859 --> 00:30:35,190
aborted status and the parties are

00:30:32,570 --> 00:30:48,509
controlling back to turn and that's

00:30:35,190 --> 00:30:52,440
finished and you can see all the six

00:30:48,509 --> 00:30:56,419
ports we just upgraded to reach ahead

00:30:52,440 --> 00:30:56,419
row back to the 1v1

00:30:56,800 --> 00:31:02,590
[Music]

00:30:59,830 --> 00:31:05,680
because of time in the mid

00:31:02,590 --> 00:31:05,680
[Music]

00:31:05,910 --> 00:31:10,720
for the phone

00:31:08,260 --> 00:31:13,309
changing to replace it were clothes I

00:31:10,720 --> 00:31:16,669
don't think I got time for that

00:31:13,309 --> 00:31:26,149
and what I would like to welcome your

00:31:16,669 --> 00:31:27,980
questions any questions it seems that

00:31:26,149 --> 00:31:31,309
the function is a little bit similar to

00:31:27,980 --> 00:31:33,470
custard formulation with the deployment

00:31:31,309 --> 00:31:37,070
about the clusters other than

00:31:33,470 --> 00:31:40,100
controlling its upgrades versions have

00:31:37,070 --> 00:31:42,470
you considered you know I don't know

00:31:40,100 --> 00:31:45,860
when you are deploying for different KPI

00:31:42,470 --> 00:31:49,519
clusters or you have different rooms in

00:31:45,860 --> 00:31:52,100
the big KPI clustered well right now we

00:31:49,519 --> 00:31:55,519
have one cluster that will be now across

00:31:52,100 --> 00:31:57,080
different rows of ourselves so in this

00:31:55,519 --> 00:31:59,679
curse rating they will have the

00:31:57,080 --> 00:31:59,679
interactions

00:32:04,410 --> 00:32:12,780
sorry I've got a question about the

00:32:07,500 --> 00:32:15,110
confirmations I can understand you

00:32:12,780 --> 00:32:20,220
confirm your change from false to true

00:32:15,110 --> 00:32:24,150
but after you finish this batch how can

00:32:20,220 --> 00:32:26,340
we change from true to false again a bit

00:32:24,150 --> 00:32:33,350
then Patrick effective on the controls

00:32:26,340 --> 00:32:33,350
or you need in place set to report

00:32:37,310 --> 00:32:44,430
annotation using metaphors that it's the

00:32:41,040 --> 00:32:46,500
logic control of the release so it is

00:32:44,430 --> 00:32:48,360
actually in traffic deployment and

00:32:46,500 --> 00:32:51,840
controlled by the committee for me

00:32:48,360 --> 00:32:55,590
controller in place that cafe deployment

00:32:51,840 --> 00:32:59,850
would watch this status and once it

00:32:55,590 --> 00:33:02,280
changed to waiting and then it will be

00:32:59,850 --> 00:33:04,740
much less that's to false right no no

00:33:02,280 --> 00:33:08,130
it's not important now you're kinda

00:33:04,740 --> 00:33:08,970
priorities working they would use the

00:33:08,130 --> 00:33:12,000
interface

00:33:08,970 --> 00:33:15,930
they do not care in the entry set of

00:33:12,000 --> 00:33:17,970
replicator structures they would love to

00:33:15,930 --> 00:33:19,410
see it's partition the president's a

00:33:17,970 --> 00:33:21,990
little bit strange

00:33:19,410 --> 00:33:25,020
it's a need for beta release there will

00:33:21,990 --> 00:33:27,210
be too hot and when these two poles are

00:33:25,020 --> 00:33:29,550
ready and they were coaches it'll fit

00:33:27,210 --> 00:33:33,410
bling controller I also like to say

00:33:29,550 --> 00:33:37,140
implicit itself would not judge whether

00:33:33,410 --> 00:33:40,500
my cell is in whatever vision they were

00:33:37,140 --> 00:33:41,790
just reported to face employ it how many

00:33:40,500 --> 00:33:44,160
of the pods are ready

00:33:41,790 --> 00:33:47,040
how many are not don't have a deployment

00:33:44,160 --> 00:33:49,890
based on to report of in place that they

00:33:47,040 --> 00:33:53,520
will make a decision that is it up from

00:33:49,890 --> 00:33:59,670
waiting or executing or whatever another

00:33:53,520 --> 00:34:04,100
question if it's a player deployment

00:33:59,670 --> 00:34:07,980
will that configuration also change from

00:34:04,100 --> 00:34:10,350
bottom to implicit it's just like a

00:34:07,980 --> 00:34:13,860
quality formant you have the Apple

00:34:10,350 --> 00:34:16,950
deployment and they will upgrade to

00:34:13,860 --> 00:34:21,120
replace that right yes yes

00:34:16,950 --> 00:34:25,110
okay these are the time we cannot have

00:34:21,120 --> 00:34:27,090
QA but if you are interested you're

00:34:25,110 --> 00:34:29,660
welcome to come to us offline I will

00:34:27,090 --> 00:34:29,660
remain here

00:34:30,190 --> 00:34:38,950
one last thing is it okay for me to take

00:34:34,000 --> 00:34:43,139
a picture with you so it's a gift for us

00:34:38,950 --> 00:34:43,139
it's a selfie for all of us

00:34:51,240 --> 00:34:54,780
one two three

00:34:56,230 --> 00:34:59,500

YouTube URL: https://www.youtube.com/watch?v=KUyP1zMluT8


