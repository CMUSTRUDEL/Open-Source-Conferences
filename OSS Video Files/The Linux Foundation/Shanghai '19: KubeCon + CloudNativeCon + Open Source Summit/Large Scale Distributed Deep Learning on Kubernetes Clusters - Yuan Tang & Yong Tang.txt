Title: Large Scale Distributed Deep Learning on Kubernetes Clusters - Yuan Tang & Yong Tang
Publication date: 2019-07-10
Playlist: Shanghai '19: KubeCon + CloudNativeCon + Open Source Summit
Description: 
	Large Scale Distributed Deep Learning on Kubernetes Clusters - Yuan Tang, Ant Financial & Yong Tang, MobileIron 

The focus of this talk is the deployments of large scale distributed deep learning with Kubernetes. The usage of operators to manage and automate training processes for machine learning are discussed. We share our experiences and compare two open source Kubernetes operators, tf-operator and mpi-operator in this talk. Both operators manage training jobs for TensorFlow but they have different distribution strategies, which lead to different performance results with respect to the utilization ratio among CPU, GPU, and network.    Deep learning tasks are both network and GPU intensive such that a proper optimization for orchestration is very important. There could easily be an imbalance leads to idle compute capacity which is too expensive for GPU nodes (compared with CPUs). We will share our experiences with the hope to provide helpful insight for better economics with machine learning tasks. 

https://sched.co/Nrnn
Captions: 
	00:00:00,620 --> 00:00:08,150
that's God's body good up no thanks for

00:00:05,370 --> 00:00:11,460
coming to this part

00:00:08,150 --> 00:00:16,109
my name is yang Tao I'm a maintainer and

00:00:11,460 --> 00:00:19,770
the sigil lead for technical project my

00:00:16,109 --> 00:00:23,640
co-speaker ian is a member of cash flow

00:00:19,770 --> 00:00:26,730
and approver and reviewer for three

00:00:23,640 --> 00:00:28,859
fellows the MPI operator is also

00:00:26,730 --> 00:00:32,070
mentioned on several open source project

00:00:28,859 --> 00:00:34,890
and still unfortunately is not able to

00:00:32,070 --> 00:00:37,110
attend this this talk because of the

00:00:34,890 --> 00:00:42,180
schedule conflict so and go to cover his

00:00:37,110 --> 00:00:45,450
part as well in today's talk I'm going

00:00:42,180 --> 00:00:48,780
to discuss about large-scale - sugar

00:00:45,450 --> 00:00:50,850
deep learning chromatic clusters I'm

00:00:48,780 --> 00:00:51,530
going to briefly discuss about the

00:00:50,850 --> 00:00:55,320
tensorflow

00:00:51,530 --> 00:00:57,660
discuss about kubernetes operators and

00:00:55,320 --> 00:01:01,379
also discuss about free flow which is

00:00:57,660 --> 00:01:04,290
the main focus of this talk because the

00:01:01,379 --> 00:01:07,770
the operator will talk about the TfL

00:01:04,290 --> 00:01:13,320
greater height watch operator and MPI

00:01:07,770 --> 00:01:16,920
rates are all belong to so let's get

00:01:13,320 --> 00:01:19,950
started so the before we jump into the

00:01:16,920 --> 00:01:22,619
deep learning transition a lot of

00:01:19,950 --> 00:01:23,520
briefly discussed about the background

00:01:22,619 --> 00:01:27,479
of tensorflow

00:01:23,520 --> 00:01:29,549
as many of you already know since the

00:01:27,479 --> 00:01:31,439
flow is one of the most popular open

00:01:29,549 --> 00:01:35,610
source frameworks for machine learning

00:01:31,439 --> 00:01:37,920
it was originally developed by Google it

00:01:35,610 --> 00:01:41,790
has been open source since 2015

00:01:37,920 --> 00:01:44,430
since then tensorflow has seen quite

00:01:41,790 --> 00:01:47,670
explosive growth force in open source

00:01:44,430 --> 00:01:49,860
and the machine learning community at

00:01:47,670 --> 00:01:53,280
the moment the most stable version of

00:01:49,860 --> 00:01:56,640
pencil flow is 1.14 which was released

00:01:53,280 --> 00:01:59,520
apple day several days ago but the

00:01:56,640 --> 00:02:01,770
tentacle o 2.0 is actually up and coming

00:01:59,520 --> 00:02:05,250
and the will be released so I want to

00:02:01,770 --> 00:02:06,810
discuss about the some of the change the

00:02:05,250 --> 00:02:09,720
intensive load without all because

00:02:06,810 --> 00:02:11,790
they're louder for lot of things that's

00:02:09,720 --> 00:02:12,970
change and that's actually are quite an

00:02:11,790 --> 00:02:16,890
impact

00:02:12,970 --> 00:02:19,060
to many areas will discuss in this 12

00:02:16,890 --> 00:02:23,700
intensive low to Davao the biggest

00:02:19,060 --> 00:02:29,940
change is obviously is eager execution

00:02:23,700 --> 00:02:33,879
back in pencil a 1.8 the static go up

00:02:29,940 --> 00:02:37,299
was executed which means if we are going

00:02:33,879 --> 00:02:38,980
to run a program instead of running it

00:02:37,299 --> 00:02:40,930
in an imperative way you have to

00:02:38,980 --> 00:02:45,310
construct it'll go up and wait for the

00:02:40,930 --> 00:02:47,799
grow up to be deployed and once it's

00:02:45,310 --> 00:02:50,950
finished you get two reads out this is

00:02:47,799 --> 00:02:52,450
obviously very good for production

00:02:50,950 --> 00:02:55,810
deployment but it also has a

00:02:52,450 --> 00:02:58,540
disadvantage in that if you try to debug

00:02:55,810 --> 00:03:01,840
it's very hard to debug because you will

00:02:58,540 --> 00:03:03,220
not get resolved immediately so in terms

00:03:01,840 --> 00:03:07,269
of load to that though the biggest

00:03:03,220 --> 00:03:09,970
change was with the execution model is

00:03:07,269 --> 00:03:14,799
that eager execution has been enabled

00:03:09,970 --> 00:03:17,579
you can't just run tensorflow like just

00:03:14,799 --> 00:03:20,169
like a Python program very lately play

00:03:17,579 --> 00:03:24,720
another big change in center flow to

00:03:20,169 --> 00:03:28,650
that though is that curse essentially is

00:03:24,720 --> 00:03:32,519
recommended high level API and our other

00:03:28,650 --> 00:03:36,660
model building API is pretty much drop

00:03:32,519 --> 00:03:39,790
intention for one dot X people have seen

00:03:36,660 --> 00:03:42,340
criticized for 1 dot X in that for high

00:03:39,790 --> 00:03:45,400
level model building you'll have several

00:03:42,340 --> 00:03:48,459
different ways you can use PF estimator

00:03:45,400 --> 00:03:50,650
you can use TF Kerris you can use the

00:03:48,459 --> 00:03:52,510
chip layer so you also have the TF sleep

00:03:50,650 --> 00:03:57,549
there are four different ways to build

00:03:52,510 --> 00:04:01,299
models in 2000 2000 what's at TF carrots

00:03:57,549 --> 00:04:04,150
the OBE is a recommended API and moving

00:04:01,299 --> 00:04:07,419
forward how other method will be

00:04:04,150 --> 00:04:11,230
deprecated pretty soon if you're looking

00:04:07,419 --> 00:04:13,989
to the principle of $2 a workflow you

00:04:11,230 --> 00:04:16,870
can notice that everything started with

00:04:13,989 --> 00:04:20,859
data input and pre-processing

00:04:16,870 --> 00:04:24,130
which is handled by TF data TF data is a

00:04:20,859 --> 00:04:26,710
pipeline to take the data from other

00:04:24,130 --> 00:04:29,020
sources and import that in

00:04:26,710 --> 00:04:31,210
- tentacles the grub they say it's a

00:04:29,020 --> 00:04:33,400
wild the most important step and this

00:04:31,210 --> 00:04:37,599
will be one of the focus of this talk

00:04:33,400 --> 00:04:39,849
because the data processing is very much

00:04:37,599 --> 00:04:43,840
tied to the ultra tradition of the deep

00:04:39,849 --> 00:04:46,599
learning the second step after the data

00:04:43,840 --> 00:04:49,180
has been imported into tens of floats

00:04:46,599 --> 00:04:51,849
graph it's a model building of the essay

00:04:49,180 --> 00:04:54,250
we talked about here Icarus and Pierre

00:04:51,849 --> 00:04:58,090
transmiter adulty Icarus is a

00:04:54,250 --> 00:05:02,620
recommended way for training now you

00:04:58,090 --> 00:05:05,770
have the eager execution also you could

00:05:02,620 --> 00:05:10,380
use the TF function to define two

00:05:05,770 --> 00:05:13,900
predefined small graph so that the

00:05:10,380 --> 00:05:17,919
execution can be speed up and finally

00:05:13,900 --> 00:05:19,900
for saving in training and the inference

00:05:17,919 --> 00:05:22,630
you can you'll save the model to save

00:05:19,900 --> 00:05:25,419
them Singam audio build and reload the

00:05:22,630 --> 00:05:27,819
back so that's tensorflow - that'll work

00:05:25,419 --> 00:05:31,330
follow if your look into that you'll

00:05:27,819 --> 00:05:36,789
notice that data input of pre process

00:05:31,330 --> 00:05:38,919
and model building probably are the

00:05:36,789 --> 00:05:42,400
steps that can be needed before you do a

00:05:38,919 --> 00:05:46,419
trimming so people always ask questions

00:05:42,400 --> 00:05:49,150
say ok why what's so special about data

00:05:46,419 --> 00:05:51,219
input and while they're building and how

00:05:49,150 --> 00:05:55,659
to apply that when you have a big

00:05:51,219 --> 00:05:57,759
cluster so let's come up with topical

00:05:55,659 --> 00:06:02,229
for all tradition for deep learning when

00:05:57,759 --> 00:06:04,389
we talk about our tradition we actually

00:06:02,229 --> 00:06:07,360
talk in deep learning

00:06:04,389 --> 00:06:10,539
it has a very special meaning most

00:06:07,360 --> 00:06:13,210
importantly observations for deep

00:06:10,539 --> 00:06:16,630
learning is always tied to the gpo we

00:06:13,210 --> 00:06:18,940
are knowing if you want to run deep

00:06:16,630 --> 00:06:19,599
learning tasks you are going to deal

00:06:18,940 --> 00:06:22,539
with zile

00:06:19,599 --> 00:06:25,719
which could be input/output most likely

00:06:22,539 --> 00:06:29,740
it's gonna be naturally imported cuz the

00:06:25,719 --> 00:06:32,770
data will be loaded from disk to memory

00:06:29,740 --> 00:06:36,370
pretty easily then the second piece is a

00:06:32,770 --> 00:06:39,430
CP o CP o it's pretty much as still

00:06:36,370 --> 00:06:40,510
needed for deep learning because not all

00:06:39,430 --> 00:06:42,940
the operations

00:06:40,510 --> 00:06:45,790
a feasible for GPO there are some

00:06:42,940 --> 00:06:46,810
operation just naturally now the good

00:06:45,790 --> 00:06:49,420
field of a GPO

00:06:46,810 --> 00:06:52,840
so you're still needs a CTO for certain

00:06:49,420 --> 00:06:56,260
operations and also GPO is typically the

00:06:52,840 --> 00:06:58,450
way to interact with SIA and finally you

00:06:56,260 --> 00:07:01,420
have a GPO that's heavily fitting piece

00:06:58,450 --> 00:07:03,550
to be done but if you're looking into

00:07:01,420 --> 00:07:05,830
that the biggest problem is that GPO is

00:07:03,550 --> 00:07:09,790
the most expensive one and the

00:07:05,830 --> 00:07:13,300
appropriately if we are going to run

00:07:09,790 --> 00:07:16,780
your ultra sation you'll notice that if

00:07:13,300 --> 00:07:18,820
the deep learning is not that scheduler

00:07:16,780 --> 00:07:23,320
very nicely you are going to waste a lot

00:07:18,820 --> 00:07:27,610
of GPO and that's a big waste for your

00:07:23,320 --> 00:07:30,760
investment if you look into the diagram

00:07:27,610 --> 00:07:35,170
showing in a slide on the top you can

00:07:30,760 --> 00:07:39,190
see that you do a batch processing you

00:07:35,170 --> 00:07:41,800
take the data from the input at the

00:07:39,190 --> 00:07:44,620
pass-through CP o for pre processing the

00:07:41,800 --> 00:07:48,250
next step is a passive GPO for training

00:07:44,620 --> 00:07:51,520
and the inference as you can see until

00:07:48,250 --> 00:07:53,470
data has reached to a GPO your GPO it's

00:07:51,520 --> 00:07:56,970
gonna be idle and you waste a lot of

00:07:53,470 --> 00:08:00,130
resource on the bottom that's the

00:07:56,970 --> 00:08:02,740
optimization of your observation and the

00:08:00,130 --> 00:08:06,070
scheduling in this case you'll try to

00:08:02,740 --> 00:08:09,280
build the i/o pipeline such that the

00:08:06,070 --> 00:08:10,030
data will be delivered to GPO as soon as

00:08:09,280 --> 00:08:12,490
possible

00:08:10,030 --> 00:08:15,580
and once it's in GPO most of the time

00:08:12,490 --> 00:08:19,360
you can do the training and to the i/o

00:08:15,580 --> 00:08:22,090
in parallel such that your photo utilize

00:08:19,360 --> 00:08:24,100
your chip EO and that's that's

00:08:22,090 --> 00:08:28,150
efficiency we talked about for how

00:08:24,100 --> 00:08:32,590
tradition there are several different

00:08:28,150 --> 00:08:35,260
ways for ultra deep learning oxidation

00:08:32,590 --> 00:08:38,349
the most common way is a so called a

00:08:35,260 --> 00:08:42,700
parameter silver model and tensorflow

00:08:38,349 --> 00:08:45,250
that's that's probably the that has been

00:08:42,700 --> 00:08:48,160
in place for things since almost the

00:08:45,250 --> 00:08:50,560
beginning the parameter server model is

00:08:48,160 --> 00:08:53,460
essentially defined parameter server

00:08:50,560 --> 00:08:53,460
which could be

00:08:53,550 --> 00:08:58,800
a subset of service that's dedicated for

00:08:57,019 --> 00:09:01,889
saving parameters

00:08:58,800 --> 00:09:05,009
you also have worker node which is GPU

00:09:01,889 --> 00:09:08,370
heavy the the workflow for parameter

00:09:05,009 --> 00:09:11,009
cell model is that each worker node it's

00:09:08,370 --> 00:09:12,930
going to collect a subset of data and is

00:09:11,009 --> 00:09:16,620
going to calculate it's going to

00:09:12,930 --> 00:09:18,720
calculate the gradient the partial

00:09:16,620 --> 00:09:21,420
grading will be passed to the parameter

00:09:18,720 --> 00:09:23,699
server when parameter server collect all

00:09:21,420 --> 00:09:26,910
the gradient it will do the calculation

00:09:23,699 --> 00:09:30,240
gets a weight and the weight will be

00:09:26,910 --> 00:09:32,879
further fitted back into the worker node

00:09:30,240 --> 00:09:37,290
to do the update this process will

00:09:32,879 --> 00:09:39,449
continue a parameter server like

00:09:37,290 --> 00:09:42,029
animation has been pretty much in place

00:09:39,449 --> 00:09:44,389
for quite several years and people

00:09:42,029 --> 00:09:47,459
invest a lot of money that's a lot of

00:09:44,389 --> 00:09:49,800
resource to improve that but there are

00:09:47,459 --> 00:09:52,670
some debating about whether parameter

00:09:49,800 --> 00:09:55,920
server is the best way to handle

00:09:52,670 --> 00:09:59,189
deplaning observation the biggest

00:09:55,920 --> 00:10:02,029
concern is that there are some people

00:09:59,189 --> 00:10:04,949
there are some discussion bother

00:10:02,029 --> 00:10:07,410
parameter server probably best certainly

00:10:04,949 --> 00:10:11,339
a server model probably best fitted for

00:10:07,410 --> 00:10:15,000
scenarios where you have your computer

00:10:11,339 --> 00:10:17,220
power our cities on the same machine so

00:10:15,000 --> 00:10:20,819
that your memory are shared by different

00:10:17,220 --> 00:10:23,930
GPUs but in case of a distributed deep

00:10:20,819 --> 00:10:26,610
many your GPS actually scattered around

00:10:23,930 --> 00:10:28,829
there could be a different host and they

00:10:26,610 --> 00:10:30,509
had to talk to a parameter server when

00:10:28,829 --> 00:10:33,920
they talk to a parameter server they had

00:10:30,509 --> 00:10:36,660
to go through network transfer and the

00:10:33,920 --> 00:10:38,579
huge amount of data are transferred back

00:10:36,660 --> 00:10:41,370
and forth between parameter selves and

00:10:38,579 --> 00:10:44,720
the worker node that could be a really

00:10:41,370 --> 00:10:48,839
big button to to your training

00:10:44,720 --> 00:10:49,589
but anyways for in the old days in

00:10:48,839 --> 00:10:52,069
tensorflow

00:10:49,589 --> 00:10:54,930
premier server it's pretty much the only

00:10:52,069 --> 00:10:57,620
only potential model you can drive for

00:10:54,930 --> 00:10:57,620
distributed learning

00:10:58,160 --> 00:11:03,680
recently people have been talking about

00:11:00,380 --> 00:11:06,139
that if we can use some other ways to do

00:11:03,680 --> 00:11:08,480
that this should be a deep learning most

00:11:06,139 --> 00:11:13,720
people actually comes to most people

00:11:08,480 --> 00:11:19,940
actually resort to the MPI model the MPI

00:11:13,720 --> 00:11:22,130
distributed distributor parallel

00:11:19,940 --> 00:11:24,709
processing has been resolved in a way

00:11:22,130 --> 00:11:27,949
for a long time and there are several

00:11:24,709 --> 00:11:33,290
ways to do the there are several ways in

00:11:27,949 --> 00:11:36,079
MPI to do the calculation MPI there are

00:11:33,290 --> 00:11:42,670
several models for reduce operation why

00:11:36,079 --> 00:11:47,300
the reduce another is how reduce so to

00:11:42,670 --> 00:11:50,690
show example of one of the reduce so

00:11:47,300 --> 00:11:52,670
what is that reduce it gives if you're

00:11:50,690 --> 00:11:57,589
looking to a graph that's going to show

00:11:52,670 --> 00:11:59,839
you exactly how the reduce essentially

00:11:57,589 --> 00:12:03,889
just for each node you are going to do

00:11:59,839 --> 00:12:05,750
the calculation and that you if you look

00:12:03,889 --> 00:12:08,269
into this graph you try to do a

00:12:05,750 --> 00:12:11,750
summation when you do the summation you

00:12:08,269 --> 00:12:14,540
start with a bottom node you do the

00:12:11,750 --> 00:12:18,529
calculation to the summation and at each

00:12:14,540 --> 00:12:21,620
node you're going to sum up the feature

00:12:18,529 --> 00:12:25,399
step you toward reduced ones and you

00:12:21,620 --> 00:12:29,000
reach the root node so if you show you

00:12:25,399 --> 00:12:33,319
that interactively it can be like that

00:12:29,000 --> 00:12:36,350
so the final step is that you reach to

00:12:33,319 --> 00:12:43,339
the root node which is the summation of

00:12:36,350 --> 00:12:48,170
all the node that's 28 its reduce MPI

00:12:43,339 --> 00:12:52,670
also have other another up of Tsukuba

00:12:48,170 --> 00:12:55,790
I'll reduce so what is our reduce now

00:12:52,670 --> 00:12:59,449
videos it's it's essentially saying that

00:12:55,790 --> 00:13:01,189
the returns except when the summation is

00:12:59,449 --> 00:13:03,860
linked how could it reach to the root

00:13:01,189 --> 00:13:07,309
node instead of just stop here you're

00:13:03,860 --> 00:13:09,260
going back and broadcast back to every

00:13:07,309 --> 00:13:11,270
node so every every node knows your

00:13:09,260 --> 00:13:13,310
result

00:13:11,270 --> 00:13:16,010
so if you're looking this way you try to

00:13:13,310 --> 00:13:19,670
lose summation you start with the bottom

00:13:16,010 --> 00:13:24,050
note 1 and the 2 that's 3 and the plus 5

00:13:19,670 --> 00:13:28,370
that's 8 you do another summation that's

00:13:24,050 --> 00:13:30,020
another note update and another

00:13:28,370 --> 00:13:33,920
summation you'll reach the loop note

00:13:30,020 --> 00:13:36,680
once you reach the room note the good

00:13:33,920 --> 00:13:40,430
reads out 28 will be broadcasted to

00:13:36,680 --> 00:13:41,360
every node so it's gonna be like set so

00:13:40,430 --> 00:13:44,960
that's a so-called

00:13:41,360 --> 00:13:48,100
I'll reduce so to sum are the our radio

00:13:44,960 --> 00:13:51,250
seats are reduced Plus broadcast and

00:13:48,100 --> 00:13:54,800
ferment for many people familiar with

00:13:51,250 --> 00:13:59,080
MPI they believe that it's a more

00:13:54,800 --> 00:13:59,080
efficient way of doing to she Oded

00:13:59,170 --> 00:14:06,460
calculations so let's go back to revisit

00:14:03,740 --> 00:14:09,350
parameter server because like I said in

00:14:06,460 --> 00:14:12,950
intensive low for a long time parameter

00:14:09,350 --> 00:14:16,850
server a permissible model is a standard

00:14:12,950 --> 00:14:20,510
way of doing distributive lending that

00:14:16,850 --> 00:14:22,160
debating is about paralyzed our machine

00:14:20,510 --> 00:14:24,950
and the paralyzed in a cluster may be

00:14:22,160 --> 00:14:29,000
different things many people believe

00:14:24,950 --> 00:14:31,760
that parameter server model is only

00:14:29,000 --> 00:14:36,160
suitable for permit paralyzed our

00:14:31,760 --> 00:14:39,080
machine it's not simple to paralyze

00:14:36,160 --> 00:14:44,810
tasking a cluster and it's kind of

00:14:39,080 --> 00:14:46,760
controversy the the biggest burden for

00:14:44,810 --> 00:14:49,420
parameter survey is that the cross

00:14:46,760 --> 00:14:52,460
device communication cost is very high

00:14:49,420 --> 00:14:55,910
but unfortunately a huge effort is being

00:14:52,460 --> 00:14:59,000
invested over the years so you probably

00:14:55,910 --> 00:15:01,820
can't really take said I spend a lot of

00:14:59,000 --> 00:15:05,270
time discuss about a permit server

00:15:01,820 --> 00:15:07,490
mostly because if you talk about all the

00:15:05,270 --> 00:15:10,100
existing machine learning or tradition

00:15:07,490 --> 00:15:12,650
frameworks they actually have the

00:15:10,100 --> 00:15:14,840
assumption that the yield parameter

00:15:12,650 --> 00:15:17,090
server and that's not necessarily the

00:15:14,840 --> 00:15:20,960
best approach so that's kind of like

00:15:17,090 --> 00:15:23,600
making the decision was shooting the

00:15:20,960 --> 00:15:25,190
best so-called deep learning for

00:15:23,600 --> 00:15:27,350
depletion observation

00:15:25,190 --> 00:15:29,240
frameworks kind of difficult because

00:15:27,350 --> 00:15:32,990
whatever frame while you try to choose

00:15:29,240 --> 00:15:37,310
it may not exactly fit the scenario you

00:15:32,990 --> 00:15:38,870
want to achieve to because in this field

00:15:37,310 --> 00:15:42,800
a lot of things are changing very

00:15:38,870 --> 00:15:46,040
frequently so many of the frameworks you

00:15:42,800 --> 00:15:47,920
feel for example you feels some Bob

00:15:46,040 --> 00:15:50,600
framework you probably notice that they

00:15:47,920 --> 00:15:53,060
cannot even run within your workshop

00:15:50,600 --> 00:16:00,009
tensorflow so that's going to be a big

00:15:53,060 --> 00:16:02,329
issue anyways if we if we look into the

00:16:00,009 --> 00:16:04,910
tradition for deep learning we notice

00:16:02,329 --> 00:16:09,410
there are several things if we summarize

00:16:04,910 --> 00:16:12,350
the the past diagram we mentioned about

00:16:09,410 --> 00:16:15,529
the parameter server model about videos

00:16:12,350 --> 00:16:18,560
and algorithms model they all require a

00:16:15,529 --> 00:16:21,980
state of Omega data and they also

00:16:18,560 --> 00:16:25,579
require lifecycle management which come

00:16:21,980 --> 00:16:28,160
up to the situation where we feel like

00:16:25,579 --> 00:16:31,579
we probably can use committees for our

00:16:28,160 --> 00:16:34,069
tradition and also when we talk about

00:16:31,579 --> 00:16:36,680
our communities we also single it's

00:16:34,069 --> 00:16:39,079
probably makes sense to have covenantal

00:16:36,680 --> 00:16:41,509
operators for machine learning as well

00:16:39,079 --> 00:16:45,579
because it's just a natural feed to menu

00:16:41,509 --> 00:16:50,180
stateful data and to manage a lifecycle

00:16:45,579 --> 00:16:52,819
so let's go to commercial operators in

00:16:50,180 --> 00:16:55,850
this talk and come through just terrific

00:16:52,819 --> 00:16:56,740
how several operators and needs could be

00:16:55,850 --> 00:17:00,380
flow

00:16:56,740 --> 00:17:05,470
organization the tr4 operator pipe watch

00:17:00,380 --> 00:17:08,539
operator and an FBI operative PI torches

00:17:05,470 --> 00:17:10,669
it's probably had some focus on paid

00:17:08,539 --> 00:17:12,289
watch which may have some computation

00:17:10,669 --> 00:17:17,900
which tends to flow but I want to cover

00:17:12,289 --> 00:17:23,120
this part as well but if operator that's

00:17:17,900 --> 00:17:24,020
obviously is it hide to tensorflow it's

00:17:23,120 --> 00:17:27,169
support tensorflow

00:17:24,020 --> 00:17:31,570
it also support tends to follow TF

00:17:27,169 --> 00:17:36,250
distributes strategy so this one has a

00:17:31,570 --> 00:17:38,540
slightly better in that TF distributor

00:17:36,250 --> 00:17:41,630
it's a relatively new

00:17:38,540 --> 00:17:45,770
and TF distributor it's actually

00:17:41,630 --> 00:17:48,950
supported different stretchers and the

00:17:45,770 --> 00:17:54,560
background like NPI and CCO permanent

00:17:48,950 --> 00:17:58,340
servant EPO model the TPL is the chief

00:17:54,560 --> 00:18:02,360
said the chip designed by Google that

00:17:58,340 --> 00:18:06,710
tells processor unit i torture operator

00:18:02,360 --> 00:18:09,650
has a focus port district torture

00:18:06,710 --> 00:18:14,440
history of module it also has support

00:18:09,650 --> 00:18:18,860
for NPR and HCC O Hara wall that's

00:18:14,440 --> 00:18:23,660
developed by by kuba Harlan would

00:18:18,860 --> 00:18:27,200
actually have has a widest support for

00:18:23,660 --> 00:18:31,760
different frameworks but I want to to

00:18:27,200 --> 00:18:33,890
point out that some of the the some of

00:18:31,760 --> 00:18:38,030
the distribution strategies that

00:18:33,890 --> 00:18:40,880
probably not exactly not exactly fitted

00:18:38,030 --> 00:18:51,440
for new versions of flow but anyway they

00:18:40,880 --> 00:18:54,200
just show we just showed those TF

00:18:51,440 --> 00:18:56,960
operators and MP operators to be

00:18:54,200 --> 00:18:58,670
deployed down cookie flow so as you

00:18:56,960 --> 00:19:02,510
could see they are pretty much very

00:18:58,670 --> 00:19:04,730
similar the difference is that the first

00:19:02,510 --> 00:19:10,490
of all you have a different kind of for

00:19:04,730 --> 00:19:13,910
jobs as a thr when the MPI job so the

00:19:10,490 --> 00:19:16,850
spec is either tier for rapid replicas

00:19:13,910 --> 00:19:19,310
back or ampere graphic respect the

00:19:16,850 --> 00:19:21,950
command line is slightly different but

00:19:19,310 --> 00:19:25,280
other than that it's very much a saying

00:19:21,950 --> 00:19:32,900
so the the container image are others

00:19:25,280 --> 00:19:34,720
are the same so let's let's look at if

00:19:32,900 --> 00:19:38,330
you have potential flow let's start with

00:19:34,720 --> 00:19:41,540
tencel program so how do we draw

00:19:38,330 --> 00:19:44,420
attention flow program your installer

00:19:41,540 --> 00:19:46,790
tensor flow and you can also install a

00:19:44,420 --> 00:19:51,230
tensile coil which give you the TF

00:19:46,790 --> 00:19:52,730
dataset support so the first line of

00:19:51,230 --> 00:19:57,800
course is

00:19:52,730 --> 00:20:00,530
to know the data into into data center

00:19:57,800 --> 00:20:04,580
that's as a first line of what they said

00:20:00,530 --> 00:20:07,210
equals M needs a date stand the data set

00:20:04,580 --> 00:20:09,770
that will do some transformation which

00:20:07,210 --> 00:20:12,530
which is necessary because you need to

00:20:09,770 --> 00:20:17,300
do some pre-processing so that your your

00:20:12,530 --> 00:20:20,540
dataset is in flux a tool full of 32

00:20:17,300 --> 00:20:21,830
data types you also do a batch of 1,000

00:20:20,540 --> 00:20:25,670
because batch

00:20:21,830 --> 00:20:28,610
actually the way to to help you to for

00:20:25,670 --> 00:20:31,730
your data to spit it into a GPO in one

00:20:28,610 --> 00:20:35,780
process it will pass the data one by one

00:20:31,730 --> 00:20:39,050
it may not be very efficient because the

00:20:35,780 --> 00:20:44,000
data transfer from CP over from memory

00:20:39,050 --> 00:20:48,260
to GPO it's also very expensive so next

00:20:44,000 --> 00:20:51,500
is the model building as we mentioned

00:20:48,260 --> 00:20:56,200
now intensive flow to that oh everything

00:20:51,500 --> 00:20:59,360
has been consolidated into into curves

00:20:56,200 --> 00:21:03,620
so you try to build a model that's going

00:20:59,360 --> 00:21:07,130
to be sequential model the first the

00:21:03,620 --> 00:21:09,470
first layer is a flattened and then you

00:21:07,130 --> 00:21:12,680
come up with a dense layer densely a

00:21:09,470 --> 00:21:17,390
standard layer the next layer is a

00:21:12,680 --> 00:21:22,220
tripod tripod is it's a technique to

00:21:17,390 --> 00:21:25,150
prevent overfitting essentially if you

00:21:22,220 --> 00:21:27,860
define a dropout with in the in this

00:21:25,150 --> 00:21:33,070
model you have a dropout of wizard read

00:21:27,860 --> 00:21:36,860
alpha 20% that is essentially means

00:21:33,070 --> 00:21:39,230
during each phase of the training for

00:21:36,860 --> 00:21:42,320
every node the the child is now the will

00:21:39,230 --> 00:21:46,880
be dropped and this this chance is 20%

00:21:42,320 --> 00:21:48,680
this will help prevent we help prevent

00:21:46,880 --> 00:21:53,030
overfitting problem you'll normally

00:21:48,680 --> 00:21:55,910
encounter when you build will build a

00:21:53,030 --> 00:21:59,300
new network the last layer is dense

00:21:55,910 --> 00:22:03,470
layers field so as we could see in 10

00:21:59,300 --> 00:22:04,240
2011 simplified so if you build a model

00:22:03,470 --> 00:22:07,110
the next

00:22:04,240 --> 00:22:09,910
I will compile the model that's one line

00:22:07,110 --> 00:22:13,570
the apps that it's modeled outfit

00:22:09,910 --> 00:22:16,870
essentially give you the training the

00:22:13,570 --> 00:22:20,020
data set has been passed in the last

00:22:16,870 --> 00:22:23,860
line the ad you can see that's evaluated

00:22:20,020 --> 00:22:27,880
that's actually the inference phase you

00:22:23,860 --> 00:22:29,980
can do that or you can optionally skip

00:22:27,880 --> 00:22:36,600
this step if you only focus on training

00:22:29,980 --> 00:22:40,090
so the this program we are talking about

00:22:36,600 --> 00:22:43,030
just a builder builder model and the

00:22:40,090 --> 00:22:44,980
write-down tensorflow now we talk about

00:22:43,030 --> 00:22:49,380
the distribution of course that's a

00:22:44,980 --> 00:22:52,780
focus out for this talk so how do we do

00:22:49,380 --> 00:22:54,610
distributed training so there are

00:22:52,780 --> 00:22:57,510
several strategies one strategy

00:22:54,610 --> 00:23:00,580
intensive flow now has mirrored strategy

00:22:57,510 --> 00:23:04,780
it's actually consolidated a lot with

00:23:00,580 --> 00:23:07,990
sameera strategy are going to do is just

00:23:04,780 --> 00:23:12,070
defined scope of four with mirror the

00:23:07,990 --> 00:23:14,740
strategy dot scope the the model

00:23:12,070 --> 00:23:20,050
compound can be done automatically which

00:23:14,740 --> 00:23:22,030
seems to be nice right we talked about a

00:23:20,050 --> 00:23:26,110
mere stretch it also want to mention

00:23:22,030 --> 00:23:29,110
that in the old days you have when your

00:23:26,110 --> 00:23:31,330
duty sugar training you have other tools

00:23:29,110 --> 00:23:34,120
available that can help you for example

00:23:31,330 --> 00:23:37,210
like a however would if you combined

00:23:34,120 --> 00:23:40,809
tensorflow with our world it's pretty

00:23:37,210 --> 00:23:43,900
much similar except that you have to

00:23:40,809 --> 00:23:48,750
pass different options and use callbacks

00:23:43,900 --> 00:23:51,970
to to achieve that to achieve the result

00:23:48,750 --> 00:23:55,470
once you point out is in this example

00:23:51,970 --> 00:23:57,700
you have th in but the action has been

00:23:55,470 --> 00:24:01,500
deprecated in touch below to the other

00:23:57,700 --> 00:24:03,910
words real so it's kind of unfortunately

00:24:01,500 --> 00:24:07,090
like I said there are a lot of things in

00:24:03,910 --> 00:24:09,010
digital 2.0 so which actually makes

00:24:07,090 --> 00:24:11,110
things a little messy but that's also

00:24:09,010 --> 00:24:13,440
that's also a good thing because it is

00:24:11,110 --> 00:24:17,620
actually point it's actually give you a

00:24:13,440 --> 00:24:20,200
brief idea how fast this area is moving

00:24:17,620 --> 00:24:23,050
every day like if you talk about a one

00:24:20,200 --> 00:24:24,820
daughter just like a how long how long I

00:24:23,050 --> 00:24:27,490
can go like one or two years ago and

00:24:24,820 --> 00:24:29,950
I'll suddenly champion toe-to-toe lot of

00:24:27,490 --> 00:24:33,370
same change I could hear like a

00:24:29,950 --> 00:24:36,040
parameter server model like tier Petrine

00:24:33,370 --> 00:24:39,930
has been replaced at applicator even TF

00:24:36,040 --> 00:24:43,059
estimator which used to be the father

00:24:39,930 --> 00:24:45,309
the father we are building a model for

00:24:43,059 --> 00:24:51,280
many programs now our has been gong

00:24:45,309 --> 00:24:53,110
tadada let's do our comparison but I

00:24:51,280 --> 00:24:55,420
know let's do a comparison so as you

00:24:53,110 --> 00:24:56,580
could see in tensorflow $2 was as a

00:24:55,420 --> 00:24:59,110
whole

00:24:56,580 --> 00:25:01,780
they're pretty much the same you build a

00:24:59,110 --> 00:25:04,000
similar model you know when you do the

00:25:01,780 --> 00:25:06,880
distribution because it's in Python so a

00:25:04,000 --> 00:25:10,179
lot of details has been abstracted and

00:25:06,880 --> 00:25:16,300
hiding so you can see it trying to make

00:25:10,179 --> 00:25:18,580
your life easier we also try to cover

00:25:16,300 --> 00:25:21,160
the pit up a torch plus a whole world

00:25:18,580 --> 00:25:24,610
because our real one thing with how load

00:25:21,160 --> 00:25:27,160
is that even if it's not exactly a fit

00:25:24,610 --> 00:25:31,240
for 10 photo dot oh it still has some

00:25:27,160 --> 00:25:33,850
users with MPI and with tight watch so

00:25:31,240 --> 00:25:36,010
you can see they behave slightly

00:25:33,850 --> 00:25:40,210
differently but they are pretty much

00:25:36,010 --> 00:25:43,150
similar bill so that's one of the

00:25:40,210 --> 00:25:46,530
positive side out for different of

00:25:43,150 --> 00:25:51,160
frameworks this kind support different

00:25:46,530 --> 00:25:56,620
open-source Alphaeus let's just go back

00:25:51,160 --> 00:26:01,480
to TF operator versus NP operator they

00:25:56,620 --> 00:26:06,370
are pretty much very similar as real as

00:26:01,480 --> 00:26:08,770
you could see they they they abstract

00:26:06,370 --> 00:26:12,040
out the details so now you can see the

00:26:08,770 --> 00:26:15,010
even llamo specification for your

00:26:12,040 --> 00:26:17,350
kubernetes for run your communities

00:26:15,010 --> 00:26:19,480
operators very much similar except for

00:26:17,350 --> 00:26:24,700
the last command line that's slightly

00:26:19,480 --> 00:26:28,929
different so as we discuss this field

00:26:24,700 --> 00:26:30,650
it's the 2 mm moving forward every day

00:26:28,929 --> 00:26:33,290
lot of things change

00:26:30,650 --> 00:26:35,780
I mean if actually you feel if you try

00:26:33,290 --> 00:26:37,340
to install for example tensorflow

00:26:35,780 --> 00:26:39,050
and they will try to use style nightly

00:26:37,340 --> 00:26:42,200
and you probably noticed so that

00:26:39,050 --> 00:26:44,840
whatever you're trying to to run may not

00:26:42,200 --> 00:26:47,390
work out like a month and within several

00:26:44,840 --> 00:26:49,460
miles test flow to that all will be

00:26:47,390 --> 00:26:53,390
released and there will be further

00:26:49,460 --> 00:26:56,450
change so it's it's really hard to to

00:26:53,390 --> 00:27:01,700
conclude what's a bad solution for you

00:26:56,450 --> 00:27:07,760
to select the moment because whatever

00:27:01,700 --> 00:27:10,880
that is the you have to make sure your

00:27:07,760 --> 00:27:13,340
authorization frameworks can support the

00:27:10,880 --> 00:27:17,870
best software which the latest version

00:27:13,340 --> 00:27:21,320
that's a stable right so so so what's a

00:27:17,870 --> 00:27:24,410
what's a conclusion coming with the to

00:27:21,320 --> 00:27:28,900
this take the conclusion that if we want

00:27:24,410 --> 00:27:31,400
to focus young people any oxidation

00:27:28,900 --> 00:27:34,910
first of all you are the focusing on

00:27:31,400 --> 00:27:38,480
trying to photograph gpo because of the

00:27:34,910 --> 00:27:41,570
essential piece otherwise de plan the

00:27:38,480 --> 00:27:44,450
oxidation it's probably probably not was

00:27:41,570 --> 00:27:49,270
a lot of effort a tool to whisper out

00:27:44,450 --> 00:27:52,490
that another piece is if you try to

00:27:49,270 --> 00:27:54,890
people tend to this debating about if

00:27:52,490 --> 00:27:57,140
you want to find the framework that's

00:27:54,890 --> 00:27:59,860
going to support our platform or our

00:27:57,140 --> 00:28:02,090
frameworks or you want to find a

00:27:59,860 --> 00:28:05,570
framework that's going to dedicate for

00:28:02,090 --> 00:28:09,580
something it's a really hard to say like

00:28:05,570 --> 00:28:12,950
I said initially tensile only have

00:28:09,580 --> 00:28:13,700
parameter server part is sugar

00:28:12,950 --> 00:28:18,020
tensorflow

00:28:13,700 --> 00:28:21,320
and later we have the mirror stretching

00:28:18,020 --> 00:28:24,350
we also have like our reduced strategy

00:28:21,320 --> 00:28:27,230
introduced in principle TF distribute

00:28:24,350 --> 00:28:32,720
that actually simplify a lot of things I

00:28:27,230 --> 00:28:35,930
think I think that's something you want

00:28:32,720 --> 00:28:38,630
to take take into consideration when you

00:28:35,930 --> 00:28:43,630
try to come up with a solution that's

00:28:38,630 --> 00:28:48,820
going to build for so called a I hope so

00:28:43,630 --> 00:28:50,770
so called machine learning kubernetes

00:28:48,820 --> 00:28:54,220
because whatever you're trying to build

00:28:50,770 --> 00:28:57,730
it may not be feasible after maybe just

00:28:54,220 --> 00:28:59,910
several months I see a lot of people

00:28:57,730 --> 00:29:04,300
just smiling to think okay what exactly

00:28:59,910 --> 00:29:07,300
deliver unfortunately that's I think

00:29:04,300 --> 00:29:09,850
that's a static call for for the kind of

00:29:07,300 --> 00:29:14,920
data for distributed distributed machine

00:29:09,850 --> 00:29:18,460
learning field it's too tied to the

00:29:14,920 --> 00:29:20,890
sugar framework and it's probably hard

00:29:18,460 --> 00:29:23,170
to come up with stable solution at the

00:29:20,890 --> 00:29:26,800
moment if you try to come up with stage

00:29:23,170 --> 00:29:28,300
surgeon I'm sorry probably in several

00:29:26,800 --> 00:29:33,610
miles you probably realize it's kind of

00:29:28,300 --> 00:29:35,620
like some of the latest version in but

00:29:33,610 --> 00:29:37,240
one thing is once you have lectures

00:29:35,620 --> 00:29:40,000
recommend it's a lot of people mentioned

00:29:37,240 --> 00:29:41,860
so what exactly are you going to do I

00:29:40,000 --> 00:29:45,190
think a cookie follow could be a

00:29:41,860 --> 00:29:46,900
framework that you can you can try there

00:29:45,190 --> 00:29:50,110
are several reasons first of all creeper

00:29:46,900 --> 00:29:53,760
is a very actively developer which means

00:29:50,110 --> 00:29:56,080
the final framework

00:29:53,760 --> 00:29:58,830
mechanician recruit for will try to

00:29:56,080 --> 00:30:01,720
catch up as soon as possible

00:29:58,830 --> 00:30:04,060
secondary fellow is has a very big

00:30:01,720 --> 00:30:05,770
community as far as I know has

00:30:04,060 --> 00:30:07,900
allowed out for developers actually

00:30:05,770 --> 00:30:10,300
countering through that they try to make

00:30:07,900 --> 00:30:13,780
adjustment as soon as possible so you

00:30:10,300 --> 00:30:16,450
for the upstream frame or like

00:30:13,780 --> 00:30:22,500
tensorflow make a change before tend to

00:30:16,450 --> 00:30:26,710
catch up where so yeah that's see that's

00:30:22,500 --> 00:30:29,110
that's pretty much for today's talk the

00:30:26,710 --> 00:30:33,310
another thing I want to mention is the

00:30:29,110 --> 00:30:36,880
crew flow also has several shared API

00:30:33,310 --> 00:30:41,410
and passive practices they have a common

00:30:36,880 --> 00:30:44,310
standardized API spec they have base job

00:30:41,410 --> 00:30:49,270
controller interface and as a

00:30:44,310 --> 00:30:52,060
consolidate the how that one in white

00:30:49,270 --> 00:30:57,640
repo but they have different

00:30:52,060 --> 00:31:00,910
different tissue okay they they

00:30:57,640 --> 00:31:04,150
consolidated those two common common API

00:31:00,910 --> 00:31:05,860
spec a common utility functions into

00:31:04,150 --> 00:31:08,710
white people but they has a different

00:31:05,860 --> 00:31:12,070
way so I'll deploy different framework

00:31:08,710 --> 00:31:15,480
like we have a key evaporator pipe water

00:31:12,070 --> 00:31:20,140
filter and it also has a MPI operators

00:31:15,480 --> 00:31:37,000
that's it's pretty much for today's for

00:31:20,140 --> 00:31:40,150
any any question regarding sir model

00:31:37,000 --> 00:31:46,090
serving what's your recommended

00:31:40,150 --> 00:31:48,700
technology thousand or others okay

00:31:46,090 --> 00:31:51,580
that's a that's another interesting

00:31:48,700 --> 00:31:54,700
topic ax yo if they talk about like

00:31:51,580 --> 00:32:00,960
you're talking about the serving models

00:31:54,700 --> 00:32:00,960
like a real quality you have TF serving

00:32:01,590 --> 00:32:10,030
that tip serving is one that's coming

00:32:06,400 --> 00:32:13,830
out of Google as real but I think they

00:32:10,030 --> 00:32:18,730
also falling behind with two Dada

00:32:13,830 --> 00:32:22,150
yes it's it's not compatible with 2.0

00:32:18,730 --> 00:32:22,690
models into another lot of single

00:32:22,150 --> 00:32:25,510
exchange

00:32:22,690 --> 00:32:28,360
remember intentional 2004 some well you

00:32:25,510 --> 00:32:33,100
have either execution behavior by D

00:32:28,360 --> 00:32:36,040
father secondly intrude so when people

00:32:33,100 --> 00:32:40,150
you would be in the passive and wondered

00:32:36,040 --> 00:32:43,900
aloud 1 dot X a lot of the frameworks

00:32:40,150 --> 00:32:46,570
use TF estimator as a default promote

00:32:43,900 --> 00:32:50,290
now this framework is being phased out

00:32:46,570 --> 00:32:52,690
of course tensorflow is still supported

00:32:50,290 --> 00:32:54,910
TF estimator but that doesn't mean it's

00:32:52,690 --> 00:32:58,390
a first-class suit anymore to be honest

00:32:54,910 --> 00:33:01,750
so I can only say a lot of things will

00:32:58,390 --> 00:33:04,570
change the produce on 4pf serving as

00:33:01,750 --> 00:33:05,080
well and also in tensorflow

00:33:04,570 --> 00:33:08,410
too

00:33:05,080 --> 00:33:11,470
though another concept is a so called TF

00:33:08,410 --> 00:33:13,930
function TF function essentially is a

00:33:11,470 --> 00:33:18,730
small sub graph that help you to

00:33:13,930 --> 00:33:21,190
optimize remembering in food or study

00:33:18,730 --> 00:33:24,120
graph as soon replaced by you got

00:33:21,190 --> 00:33:27,970
execution you need each and every step

00:33:24,120 --> 00:33:31,510
it can dry in particularly this actually

00:33:27,970 --> 00:33:34,210
slows down the speed of what training

00:33:31,510 --> 00:33:36,910
dramatically the one way to improve that

00:33:34,210 --> 00:33:40,210
is you can define a small function a

00:33:36,910 --> 00:33:41,560
small function is essentially sub graph

00:33:40,210 --> 00:33:45,610
or a small one

00:33:41,560 --> 00:33:49,090
it's go up you can apply so that give

00:33:45,610 --> 00:33:52,750
you that advantage of coop optimization

00:33:49,090 --> 00:33:55,810
at the same time is still give you the

00:33:52,750 --> 00:33:58,570
eager execution capability to allow you

00:33:55,810 --> 00:34:03,190
to do a knee back so waste here function

00:33:58,570 --> 00:34:06,130
if you try to write if you try to let's

00:34:03,190 --> 00:34:09,130
say you try to write a write a model you

00:34:06,130 --> 00:34:11,080
can come up with you can write it in a

00:34:09,130 --> 00:34:13,390
legal mode and then for your stabilized

00:34:11,080 --> 00:34:15,370
and the decided say your model it's

00:34:13,390 --> 00:34:18,640
pretty much free you can convert that

00:34:15,370 --> 00:34:21,300
into TF function to speed up so that's

00:34:18,640 --> 00:34:23,620
another thing that's going to help you

00:34:21,300 --> 00:34:26,290
like I said a lot of things that

00:34:23,620 --> 00:34:28,330
interchange in to that oh but I think

00:34:26,290 --> 00:34:31,810
that's how could change because like I

00:34:28,330 --> 00:34:35,140
mentioned for model building you have

00:34:31,810 --> 00:34:36,910
occurs consolidated you don't you don't

00:34:35,140 --> 00:34:41,170
need to think about which is the best

00:34:36,910 --> 00:34:43,870
way to build a model among the four

00:34:41,170 --> 00:34:46,330
different ways of building that we also

00:34:43,870 --> 00:34:52,750
with the execution that really helps

00:34:46,330 --> 00:34:55,710
allow that for debugging purposes any

00:34:52,750 --> 00:34:55,710
any other questions

00:34:57,190 --> 00:35:04,660
okay okay Sox thanks everyone for coming

00:35:00,829 --> 00:35:04,660

YouTube URL: https://www.youtube.com/watch?v=pmiC_FJ3xe4


