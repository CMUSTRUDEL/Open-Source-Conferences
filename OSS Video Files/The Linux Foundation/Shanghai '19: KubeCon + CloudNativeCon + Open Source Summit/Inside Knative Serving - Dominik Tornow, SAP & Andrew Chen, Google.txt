Title: Inside Knative Serving - Dominik Tornow, SAP & Andrew Chen, Google
Publication date: 2019-07-10
Playlist: Shanghai '19: KubeCon + CloudNativeCon + Open Source Summit
Description: 
	Inside Knative Serving - Dominik Tornow, SAP & Andrew Chen, Google 

Knative is a Kubernetes Extension that brings Functions as a Service to your Kubernetes Cluster. Knative provides components to build, serve, and auto-scale your apps.    Do you want to know how Knative Serving turns your Docker image into a function as a service? Do you want to know how Knative Serving scales your functions to meet demand? How Knative Serving scales to zero without ever dropping a single request? How does it work?    Using a systems modeling approach, this talk will explore Knative Serving, connect the dots between workload management and traffic management. You will leave with a concise and accurate understanding how Knative turns your source code into scalable, reliable applications.    Along the way, this talk highlights how Kubernetes Extensions enable brand new use cases, adding custom functionality and custom processes as first class citizens to your cluster. 

https://sched.co/NrlT
Captions: 
	00:00:00,030 --> 00:00:04,380
alright let's get started thank you

00:00:02,520 --> 00:00:07,259
everyone for coming to our talk inside

00:00:04,380 --> 00:00:09,000
Katy native serving I am Andrew Chen I'm

00:00:07,259 --> 00:00:11,570
a program manager and a technical writer

00:00:09,000 --> 00:00:15,420
and with me is Dominick tour now a

00:00:11,570 --> 00:00:17,100
principal engineer so for the past year

00:00:15,420 --> 00:00:19,050
we've been working together to try to

00:00:17,100 --> 00:00:21,900
figure out why kubernetes is so hard to

00:00:19,050 --> 00:00:24,180
understand when I started as a Technical

00:00:21,900 --> 00:00:26,340
Writer over two years ago I was

00:00:24,180 --> 00:00:28,769
bombarded with terms such as control

00:00:26,340 --> 00:00:32,009
plane data plane kubernetes object

00:00:28,769 --> 00:00:34,649
controller ingress you know and the list

00:00:32,009 --> 00:00:36,180
goes on and on all right I had at my

00:00:34,649 --> 00:00:38,760
disposal all of the committee's

00:00:36,180 --> 00:00:40,800
documentation yet I still lacked a big

00:00:38,760 --> 00:00:44,070
picture understanding of how everything

00:00:40,800 --> 00:00:46,200
worked together was it because I didn't

00:00:44,070 --> 00:00:48,690
read all the source code who even has

00:00:46,200 --> 00:00:50,879
the time to do that and do you even

00:00:48,690 --> 00:00:52,890
believe that it's possible to understand

00:00:50,879 --> 00:00:53,579
a software system without examining its

00:00:52,890 --> 00:00:55,770
source code

00:00:53,579 --> 00:00:58,489
well Dominic showed me that there is

00:00:55,770 --> 00:01:01,020
another way it's called systems modeling

00:00:58,489 --> 00:01:03,359
so for the past year we've been writing

00:01:01,020 --> 00:01:05,519
blog posts using formal and conceptual

00:01:03,359 --> 00:01:07,710
models to help people better understand

00:01:05,519 --> 00:01:09,570
how kubernetes works so that they may

00:01:07,710 --> 00:01:12,240
reason about its behavior with

00:01:09,570 --> 00:01:16,049
confidence so we're applying this

00:01:12,240 --> 00:01:17,880
methodology now to K native serving so

00:01:16,049 --> 00:01:20,369
that you may understand K native better

00:01:17,880 --> 00:01:21,750
and what makes this presentation a

00:01:20,369 --> 00:01:23,369
little different than others you may see

00:01:21,750 --> 00:01:25,049
is that we're not going to show you

00:01:23,369 --> 00:01:27,390
source code we're not going to show you

00:01:25,049 --> 00:01:28,829
a console we're not going to do a demo

00:01:27,390 --> 00:01:32,220
we're just going to be showing you

00:01:28,829 --> 00:01:34,200
systems models and yet you should still

00:01:32,220 --> 00:01:36,479
be able to walk away today with the

00:01:34,200 --> 00:01:39,479
crystal clear understanding of how cane

00:01:36,479 --> 00:01:41,880
ativ serving works and without further

00:01:39,479 --> 00:01:46,850
delay here is dominic turnout to talk

00:01:41,880 --> 00:01:46,850
about cane and is serving thanks Angela

00:01:49,070 --> 00:01:56,130
thank you if you heard of K native

00:01:52,560 --> 00:01:58,590
before you probably heard that K native

00:01:56,130 --> 00:02:01,799
is a kubernetes extension designed to

00:01:58,590 --> 00:02:03,979
manage service applications this

00:02:01,799 --> 00:02:06,509
statement about K native is correct

00:02:03,979 --> 00:02:09,660
however this statement does not describe

00:02:06,509 --> 00:02:12,810
K native well k native is a kubernetes

00:02:09,660 --> 00:02:15,360
extension a kubernetes extension is a

00:02:12,810 --> 00:02:18,000
collection of custom controllers and

00:02:15,360 --> 00:02:21,360
custom resource definitions that an

00:02:18,000 --> 00:02:24,600
ember that enable new use cases on top

00:02:21,360 --> 00:02:26,910
of kubernetes if you hear members of the

00:02:24,600 --> 00:02:29,850
kubernetes community safe kubernetes is

00:02:26,910 --> 00:02:33,780
a platform to build a platform they're

00:02:29,850 --> 00:02:36,330
talking about kubernetes extensions Kay

00:02:33,780 --> 00:02:39,510
native is in fact a collection of three

00:02:36,330 --> 00:02:43,140
kubernetes extensions candidate build k

00:02:39,510 --> 00:02:44,310
native serving and k native eventing in

00:02:43,140 --> 00:02:46,680
combination

00:02:44,310 --> 00:02:49,769
k native is not just a server list

00:02:46,680 --> 00:02:52,170
extension but it is a zero operations

00:02:49,769 --> 00:02:56,340
extension for reactive micro service

00:02:52,170 --> 00:03:00,630
applications hosted on kubernetes zero

00:02:56,340 --> 00:03:04,380
ops also called no ops or more formally

00:03:00,630 --> 00:03:07,680
operation automation refers to the fact

00:03:04,380 --> 00:03:10,170
that most or all tasks which are

00:03:07,680 --> 00:03:13,260
required to operate an application are

00:03:10,170 --> 00:03:17,160
performed by the system not performed by

00:03:13,260 --> 00:03:19,799
the developer today we will discuss K

00:03:17,160 --> 00:03:23,370
native serving in particular we will

00:03:19,799 --> 00:03:25,530
discuss two aspects canada serving as a

00:03:23,370 --> 00:03:27,989
zero operations extension for the

00:03:25,530 --> 00:03:30,720
lifecycle management of reactive micro

00:03:27,989 --> 00:03:34,799
services and k native serving as a

00:03:30,720 --> 00:03:37,470
server list extension to understand the

00:03:34,799 --> 00:03:39,720
benefits of canada serving we need to

00:03:37,470 --> 00:03:41,790
shift our point of view from an

00:03:39,720 --> 00:03:45,120
architectural perspective to an

00:03:41,790 --> 00:03:47,670
operational perspective from an

00:03:45,120 --> 00:03:50,850
architectural perspective a reactive

00:03:47,670 --> 00:03:53,910
micro service is an individual stateless

00:03:50,850 --> 00:03:57,900
component the processes individual

00:03:53,910 --> 00:03:59,500
requests micro services are located

00:03:57,900 --> 00:04:01,810
behind the Gateway

00:03:59,500 --> 00:04:04,570
the Gateway is responsible for traffic

00:04:01,810 --> 00:04:07,300
management the Gateway acts as a reverse

00:04:04,570 --> 00:04:12,030
proxy routing a request from a service

00:04:07,300 --> 00:04:12,030
consumer to the correct service provider

00:04:15,000 --> 00:04:20,920
from an operational perspective for each

00:04:18,400 --> 00:04:26,800
micro service there exists at least one

00:04:20,920 --> 00:04:28,810
version also called revision in order to

00:04:26,800 --> 00:04:31,260
release the initial revision of a

00:04:28,810 --> 00:04:38,080
service you have to perform two steps

00:04:31,260 --> 00:04:40,450
first you have to deploy the revision to

00:04:38,080 --> 00:04:43,200
deploy a revision you have to create a

00:04:40,450 --> 00:04:46,290
workload specification a workload

00:04:43,200 --> 00:04:49,990
specification is a set of resources that

00:04:46,290 --> 00:04:52,420
specify how to process requests and by

00:04:49,990 --> 00:04:54,730
the way if you are thinking workload

00:04:52,420 --> 00:04:58,170
specification that sounds like pod

00:04:54,730 --> 00:05:03,420
specification in essence you are correct

00:04:58,170 --> 00:05:03,420
second you have to roll out the revision

00:05:06,439 --> 00:05:11,449
to roll out a revision you have to

00:05:08,989 --> 00:05:14,119
create a traffic split specification a

00:05:11,449 --> 00:05:16,759
traffic split specification is a set of

00:05:14,119 --> 00:05:19,639
resources that specify how to route

00:05:16,759 --> 00:05:22,009
requests and now if you are thinking

00:05:19,639 --> 00:05:24,529
traffic split specification that sounds

00:05:22,009 --> 00:05:28,009
like ingress specification again in

00:05:24,529 --> 00:05:31,179
essence you are correct here all traffic

00:05:28,009 --> 00:05:34,369
splits are probabilistic traffic splits

00:05:31,179 --> 00:05:37,269
in order to release the next revision of

00:05:34,369 --> 00:05:40,069
a service you have to repeat these steps

00:05:37,269 --> 00:05:44,989
first you have to deploy the new

00:05:40,069 --> 00:05:47,839
revision second you have to roll out the

00:05:44,989 --> 00:05:50,149
revision with a probabilistic traffic

00:05:47,839 --> 00:05:50,659
split you have two options to do a

00:05:50,149 --> 00:05:53,569
rollout

00:05:50,659 --> 00:05:57,819
you may choose an immediate rollout or

00:05:53,569 --> 00:06:00,739
you may choose a gradual rollout to

00:05:57,819 --> 00:06:03,079
perform an immediate rollout you have to

00:06:00,739 --> 00:06:06,619
update the traffic split specification

00:06:03,079 --> 00:06:08,599
only once shifting all requests from the

00:06:06,619 --> 00:06:11,110
current to the next revision in an

00:06:08,599 --> 00:06:11,110
instant

00:06:18,400 --> 00:06:23,169
to perform a gradual rollout you have to

00:06:21,100 --> 00:06:25,630
update the traffic split specification

00:06:23,169 --> 00:06:27,190
multiple times shifting all requests

00:06:25,630 --> 00:06:30,870
from the current to the next revision

00:06:27,190 --> 00:06:30,870
over a period of time

00:06:39,260 --> 00:06:45,350
as an operator of a micro service you

00:06:42,320 --> 00:06:48,350
are locked in an endless tedious cycle

00:06:45,350 --> 00:06:50,120
of deployments and rollouts and if this

00:06:48,350 --> 00:06:52,970
dire situation does not ask for

00:06:50,120 --> 00:06:55,240
automation nothing else does but let's

00:06:52,970 --> 00:06:58,280
not get ahead of ourselves

00:06:55,240 --> 00:07:01,180
so far we talked about micro services

00:06:58,280 --> 00:07:04,340
but we did not talk about kubernetes and

00:07:01,180 --> 00:07:06,610
we did not talk about Canada's yet well

00:07:04,340 --> 00:07:09,620
let's talk about kubernetes first

00:07:06,610 --> 00:07:13,070
kubernetes is a prominent platform for

00:07:09,620 --> 00:07:15,980
hosting micro services kubernetes

00:07:13,070 --> 00:07:18,950
provides an extensive set of very

00:07:15,980 --> 00:07:21,950
rounded abstractions to compose a micro

00:07:18,950 --> 00:07:24,620
service however kubernetes does not

00:07:21,950 --> 00:07:27,950
provide a dedicated abstraction for a

00:07:24,620 --> 00:07:30,470
micro service simply put kubernetes

00:07:27,950 --> 00:07:33,530
gives us everything we need but we have

00:07:30,470 --> 00:07:36,350
to piece everything together ourselves

00:07:33,530 --> 00:07:40,670
and there are many options to choose

00:07:36,350 --> 00:07:44,360
from turns out the most prominent is

00:07:40,670 --> 00:07:46,730
also the most basic pattern here one

00:07:44,360 --> 00:07:50,140
micro service is represented by

00:07:46,730 --> 00:07:53,090
composing four kinds of objects a

00:07:50,140 --> 00:07:54,710
kubernetes deployment a kubernetes

00:07:53,090 --> 00:07:57,830
horizontal pod autoscaler

00:07:54,710 --> 00:08:03,110
a kubernetes service and o kubernetes

00:07:57,830 --> 00:08:05,600
ingress deployment and HP a represent

00:08:03,110 --> 00:08:07,850
the workload specification service and

00:08:05,600 --> 00:08:12,500
ingress represent the traffic split

00:08:07,850 --> 00:08:14,690
specification in order to release the

00:08:12,500 --> 00:08:17,630
initial revision the developer has to

00:08:14,690 --> 00:08:19,820
create a deployment object a horizontal

00:08:17,630 --> 00:08:24,440
port or a scalar object a service object

00:08:19,820 --> 00:08:26,510
and an ingress object in order to

00:08:24,440 --> 00:08:28,730
release a subsequent revision the

00:08:26,510 --> 00:08:33,920
developer has to update the deployment

00:08:28,730 --> 00:08:38,599
object again the basic pattern from an

00:08:33,920 --> 00:08:40,490
actions point of view in order to

00:08:38,599 --> 00:08:42,320
release the initial revision of a micro

00:08:40,490 --> 00:08:44,330
service the developer has to create a

00:08:42,320 --> 00:08:46,370
deployment object a horizontal port

00:08:44,330 --> 00:08:50,690
order scalar object an ingress object

00:08:46,370 --> 00:08:51,540
and a service object in order to release

00:08:50,690 --> 00:08:53,670
a subsequent

00:08:51,540 --> 00:08:56,880
vision the developer has to update the

00:08:53,670 --> 00:08:58,759
deployment object this pattern is simple

00:08:56,880 --> 00:09:01,680
to implement and simple to operate

00:08:58,759 --> 00:09:04,680
however the developer has limited

00:09:01,680 --> 00:09:06,240
control over the rollout that is limited

00:09:04,680 --> 00:09:10,079
control over the traffic split

00:09:06,240 --> 00:09:12,000
specification this diagram illustrates

00:09:10,079 --> 00:09:14,880
in mechanics of the traffic split

00:09:12,000 --> 00:09:16,800
specification of the basic pattern here

00:09:14,880 --> 00:09:20,670
we have one microservice that is

00:09:16,800 --> 00:09:22,440
currently in rollout that is traffic is

00:09:20,670 --> 00:09:24,810
shifted from deployment number one

00:09:22,440 --> 00:09:27,199
replica set number one to deployment

00:09:24,810 --> 00:09:30,360
number one replica set number two

00:09:27,199 --> 00:09:32,970
kubernetes ingress directs all requests

00:09:30,360 --> 00:09:36,360
that are bound to one micro service to

00:09:32,970 --> 00:09:39,389
one kubernetes service in turn the

00:09:36,360 --> 00:09:42,839
kubernetes service directs request to

00:09:39,389 --> 00:09:46,170
matching parts with equal probability in

00:09:42,839 --> 00:09:49,279
summary traffic split is implicit in

00:09:46,170 --> 00:09:52,079
effect determined solely by the

00:09:49,279 --> 00:09:55,050
kubernetes service and dependent on the

00:09:52,079 --> 00:09:58,889
number of ports per replica set for

00:09:55,050 --> 00:10:00,810
example here if there are three parts in

00:09:58,889 --> 00:10:03,060
replica set number one and three parts

00:10:00,810 --> 00:10:09,029
in replica set number two the resulting

00:10:03,060 --> 00:10:11,279
traffic split is 50/50 to grant the

00:10:09,029 --> 00:10:14,310
developer full control over the traffic

00:10:11,279 --> 00:10:17,579
split specification a more advanced

00:10:14,310 --> 00:10:20,339
pattern emerged here again one micro

00:10:17,579 --> 00:10:22,380
service is represented by composing four

00:10:20,339 --> 00:10:24,810
different kinds of objects a kubernetes

00:10:22,380 --> 00:10:25,470
deployment a kubernetes horizontal pod

00:10:24,810 --> 00:10:29,430
autoscaler

00:10:25,470 --> 00:10:33,600
a kubernetes service and an sto virtual

00:10:29,430 --> 00:10:36,540
service deployment and HP a represent

00:10:33,600 --> 00:10:38,970
the workload specification service and

00:10:36,540 --> 00:10:42,959
virtual service represent the traffic

00:10:38,970 --> 00:10:46,410
split specification in order to release

00:10:42,959 --> 00:10:48,089
the initial revision the developer has

00:10:46,410 --> 00:10:50,279
to create a deployment object a

00:10:48,089 --> 00:10:52,949
horizontal port autoscaler object a

00:10:50,279 --> 00:10:57,180
service object and an easier virtual

00:10:52,949 --> 00:10:59,430
service object in order to release a

00:10:57,180 --> 00:11:01,470
subsequent revision the developer has to

00:10:59,430 --> 00:11:03,629
create a new deployment object

00:11:01,470 --> 00:11:06,240
a new horizontal pod or a scalar object

00:11:03,629 --> 00:11:09,540
and a new service object and has to

00:11:06,240 --> 00:11:12,899
update the existing sto virtual service

00:11:09,540 --> 00:11:16,259
object again the advance pattern from an

00:11:12,899 --> 00:11:18,180
actions point of view in order to

00:11:16,259 --> 00:11:19,920
release the initial revision of a micro

00:11:18,180 --> 00:11:22,050
service the developer has to create a

00:11:19,920 --> 00:11:24,750
deployment object a horizontal port or a

00:11:22,050 --> 00:11:28,829
scalar object a service object and an

00:11:24,750 --> 00:11:31,560
sto virtual service object in order to

00:11:28,829 --> 00:11:33,779
release a subsequent revision the

00:11:31,560 --> 00:11:36,149
developer has to create a new deployment

00:11:33,779 --> 00:11:38,279
object a new horizontal port or the

00:11:36,149 --> 00:11:40,649
scalar object a new service object and

00:11:38,279 --> 00:11:44,189
has to update the existing is your

00:11:40,649 --> 00:11:46,290
virtual service object in case of an

00:11:44,189 --> 00:11:48,420
immediate rollout the developer has to

00:11:46,290 --> 00:11:51,420
update the ISTE of virtual service only

00:11:48,420 --> 00:11:53,550
once in case of a gradual rollout the

00:11:51,420 --> 00:11:56,250
developer has to update the ischial

00:11:53,550 --> 00:11:59,790
virtual service multiple times this

00:11:56,250 --> 00:12:04,230
pattern is still simple to implement but

00:11:59,790 --> 00:12:06,779
way more involved to operate however the

00:12:04,230 --> 00:12:09,269
developer has full control over the

00:12:06,779 --> 00:12:13,019
rollout that is full control over the

00:12:09,269 --> 00:12:14,939
traffic split specification this diagram

00:12:13,019 --> 00:12:16,800
illustrates the mechanics of the traffic

00:12:14,939 --> 00:12:19,949
split specification of the advanced

00:12:16,800 --> 00:12:21,750
pattern here we have one micro service

00:12:19,949 --> 00:12:25,079
that is currently in rollout that is

00:12:21,750 --> 00:12:27,019
traffic is split from I'm sorry traffic

00:12:25,079 --> 00:12:29,250
is shifted from deployment number one

00:12:27,019 --> 00:12:33,029
replica set number one to deployment

00:12:29,250 --> 00:12:35,880
number two replica set number one is to

00:12:33,029 --> 00:12:38,279
virtual service directs all requests

00:12:35,880 --> 00:12:40,860
that are bound to one micro service to a

00:12:38,279 --> 00:12:43,680
configurable set of kubernetes services

00:12:40,860 --> 00:12:45,660
in turn the kubernetes service directs

00:12:43,680 --> 00:12:46,500
request to matching parts with equal

00:12:45,660 --> 00:12:50,790
probability

00:12:46,500 --> 00:12:53,459
in summary traffic split is explicit in

00:12:50,790 --> 00:12:57,689
fact determined solely by the ISTE of

00:12:53,459 --> 00:13:00,079
virtual service so far we had good news

00:12:57,689 --> 00:13:02,610
and we have bad news the good news

00:13:00,079 --> 00:13:05,100
kubernetes is a convenient choice to

00:13:02,610 --> 00:13:07,620
implement micro service applications the

00:13:05,100 --> 00:13:09,959
bad news kubernetes is not a convenient

00:13:07,620 --> 00:13:12,509
choice to operate micro service

00:13:09,959 --> 00:13:15,089
applications you have to perform an

00:13:12,509 --> 00:13:18,029
involved with edit of sequence of

00:13:15,089 --> 00:13:22,649
steps for the initial release in every

00:13:18,029 --> 00:13:25,379
subsequent release well meet K native

00:13:22,649 --> 00:13:28,350
serving finally K native serving

00:13:25,379 --> 00:13:30,120
automates the sequence of steps for the

00:13:28,350 --> 00:13:34,079
initial release and every subsequent

00:13:30,120 --> 00:13:36,420
release of your micro-service K native

00:13:34,079 --> 00:13:39,029
provides a custom resource definition

00:13:36,420 --> 00:13:41,550
the K native service object that

00:13:39,029 --> 00:13:45,990
implements the advanced pattern and

00:13:41,550 --> 00:13:48,949
automates its operations when you create

00:13:45,990 --> 00:13:51,059
a K native service object K native

00:13:48,949 --> 00:13:53,730
automatically creates an initial

00:13:51,059 --> 00:13:56,790
deployment the initial autoscaler the

00:13:53,730 --> 00:14:00,959
initial service and the sto virtual

00:13:56,790 --> 00:14:02,850
service for you please note that K

00:14:00,959 --> 00:14:05,100
native replaces the horizontal part

00:14:02,850 --> 00:14:08,009
autoscaler with a K native pod

00:14:05,100 --> 00:14:10,920
autoscaler the kubernetes horizontal pod

00:14:08,009 --> 00:14:13,589
autoscaler scales instances based on

00:14:10,920 --> 00:14:16,889
metrics like CPU utilization or memory

00:14:13,589 --> 00:14:19,079
utilization the K native pod autoscaler

00:14:16,889 --> 00:14:23,189
scales instances based on in-flight

00:14:19,079 --> 00:14:26,279
request count in addition the k p8 can

00:14:23,189 --> 00:14:28,199
scale from n to 0 we will go into more

00:14:26,279 --> 00:14:32,790
detail in the next section of the

00:14:28,199 --> 00:14:35,610
presentation when you update a K native

00:14:32,790 --> 00:14:38,370
service object K native automatically

00:14:35,610 --> 00:14:40,589
creates the next deployment the next

00:14:38,370 --> 00:14:45,629
autoscaler and the next service and

00:14:40,589 --> 00:14:48,089
updates is to virtual service for you in

00:14:45,629 --> 00:14:50,970
conclusion from an actions point of view

00:14:48,089 --> 00:14:54,089
K native reduces the operational burden

00:14:50,970 --> 00:14:56,490
on the developer in order to release the

00:14:54,089 --> 00:14:59,189
initial revision of a micro service the

00:14:56,490 --> 00:15:03,410
developer simply has to create a K

00:14:59,189 --> 00:15:05,730
native serving object in turn can ativ

00:15:03,410 --> 00:15:08,179
automatically creates a required set of

00:15:05,730 --> 00:15:08,179
objects

00:15:10,440 --> 00:15:14,610
in order to release this subsequent

00:15:12,569 --> 00:15:16,889
revision the developer simply has to

00:15:14,610 --> 00:15:20,519
update the tentative serving object and

00:15:16,889 --> 00:15:24,000
in turn Canada automatically creates and

00:15:20,519 --> 00:15:26,399
updates a required set of objects still

00:15:24,000 --> 00:15:28,620
in case traffic is shifted immediately

00:15:26,399 --> 00:15:31,470
the developer updates the key native

00:15:28,620 --> 00:15:33,629
service once in case traffic is shifted

00:15:31,470 --> 00:15:38,279
gradually the developer updates the key

00:15:33,629 --> 00:15:40,199
native service multiple times let's take

00:15:38,279 --> 00:15:43,879
a closer look at the mechanics of

00:15:40,199 --> 00:15:46,620
Canada's AK a native service object

00:15:43,879 --> 00:15:49,490
combines the workload and traffic split

00:15:46,620 --> 00:15:52,230
specification of your micro service

00:15:49,490 --> 00:15:54,629
ultimately the workload specification is

00:15:52,230 --> 00:15:58,529
your part specification specifying the

00:15:54,629 --> 00:16:00,959
image of your service the traffic split

00:15:58,529 --> 00:16:04,019
specification is the ISTE of virtual

00:16:00,959 --> 00:16:06,329
service specification specifying one

00:16:04,019 --> 00:16:11,220
revision or two revisions with the

00:16:06,329 --> 00:16:13,050
traffic split as your traffic target for

00:16:11,220 --> 00:16:15,810
each K native service there exists

00:16:13,050 --> 00:16:18,259
exactly one K native configuration when

00:16:15,810 --> 00:16:20,819
the service object is created a

00:16:18,259 --> 00:16:22,680
configuration object will be created

00:16:20,819 --> 00:16:25,310
with the services initial workload

00:16:22,680 --> 00:16:27,449
specification when the workload

00:16:25,310 --> 00:16:29,430
specification of the service object is

00:16:27,449 --> 00:16:32,029
updated the configuration will be

00:16:29,430 --> 00:16:36,149
updated with the services new workload

00:16:32,029 --> 00:16:38,759
specification for each K native

00:16:36,149 --> 00:16:41,819
configuration there exists at least one

00:16:38,759 --> 00:16:44,579
K native revision when the configuration

00:16:41,819 --> 00:16:47,579
object is created a revision object is

00:16:44,579 --> 00:16:49,889
created with the configurations initial

00:16:47,579 --> 00:16:53,069
workload specification when the

00:16:49,889 --> 00:16:54,990
configuration object is updated a new

00:16:53,069 --> 00:16:56,430
revision object will be created with the

00:16:54,990 --> 00:17:00,149
configurations new workload

00:16:56,430 --> 00:17:02,279
specification each revision results in a

00:17:00,149 --> 00:17:08,189
deployment a K native port autoscaler

00:17:02,279 --> 00:17:10,079
and a service in addition for each K

00:17:08,189 --> 00:17:12,390
native service there exists a K native

00:17:10,079 --> 00:17:14,909
route when the service object is created

00:17:12,390 --> 00:17:16,909
a route object will be created with the

00:17:14,909 --> 00:17:19,679
services traffic split specification

00:17:16,909 --> 00:17:22,110
when the traffic split specification of

00:17:19,679 --> 00:17:24,100
the service object is updated the route

00:17:22,110 --> 00:17:26,500
object will be updated with

00:17:24,100 --> 00:17:30,610
services new traffic split specification

00:17:26,500 --> 00:17:34,299
and ultimately a route results in an sto

00:17:30,610 --> 00:17:36,070
virtual service let's take a closer look

00:17:34,299 --> 00:17:41,559
at the workload specification of your

00:17:36,070 --> 00:17:44,309
service as stated the workload

00:17:41,559 --> 00:17:47,860
specification is a part specification

00:17:44,309 --> 00:17:50,410
you have to specify one image called the

00:17:47,860 --> 00:17:52,770
user container image that contains your

00:17:50,410 --> 00:17:55,350
application or more specifically that

00:17:52,770 --> 00:17:58,179
contains a revision of your application

00:17:55,350 --> 00:18:02,940
your application must be an HTTP

00:17:58,179 --> 00:18:02,940
application processing HTTP requests

00:18:05,910 --> 00:18:11,280
kay native injects another image into

00:18:08,820 --> 00:18:13,650
the pot specification on creation of the

00:18:11,280 --> 00:18:16,290
deployment called the cue container

00:18:13,650 --> 00:18:18,870
image the cue container is a reverse

00:18:16,290 --> 00:18:21,930
proxy to the user container the cue

00:18:18,870 --> 00:18:24,180
container intercepts all requests to the

00:18:21,930 --> 00:18:26,150
user container and is responsible for

00:18:24,180 --> 00:18:29,100
collecting and reporting statistics

00:18:26,150 --> 00:18:31,800
namely the in-flight request count to

00:18:29,100 --> 00:18:33,630
the autoscaler again we will go into

00:18:31,800 --> 00:18:38,160
more detail in the next section of the

00:18:33,630 --> 00:18:39,930
presentation so to release the initial

00:18:38,160 --> 00:18:41,700
revision of a micro service the

00:18:39,930 --> 00:18:44,220
developer creates a que native service

00:18:41,700 --> 00:18:47,490
object with an initial workload and an

00:18:44,220 --> 00:18:49,620
initial traffic split specification in

00:18:47,490 --> 00:18:51,420
turn can aid of creates a Canadian

00:18:49,620 --> 00:18:56,550
configuration object with the workload

00:18:51,420 --> 00:18:58,260
specification in turn can ativ creates a

00:18:56,550 --> 00:19:02,730
candidate revision object with the

00:18:58,260 --> 00:19:04,710
workload specification in turn que

00:19:02,730 --> 00:19:06,930
native creates a deployment object with

00:19:04,710 --> 00:19:09,500
the workload specification a service and

00:19:06,930 --> 00:19:12,810
make a native autoscaler

00:19:09,500 --> 00:19:14,580
additionally que native creates a Canada

00:19:12,810 --> 00:19:19,350
Froude object with the initial traffic

00:19:14,580 --> 00:19:21,240
split specification in turn can aid of

00:19:19,350 --> 00:19:24,090
creates an SEO virtual service with the

00:19:21,240 --> 00:19:26,670
traffic split specification at this

00:19:24,090 --> 00:19:30,560
point the initial revision is released

00:19:26,670 --> 00:19:33,000
and receiving 100% of requests to

00:19:30,560 --> 00:19:35,160
release the next revision of a micro

00:19:33,000 --> 00:19:37,290
service the developer updates the

00:19:35,160 --> 00:19:39,540
kinetic service object here with an

00:19:37,290 --> 00:19:42,990
updated workload and an updated traffic

00:19:39,540 --> 00:19:45,270
split specification Canada updates a

00:19:42,990 --> 00:19:48,330
configuration object and in turn creates

00:19:45,270 --> 00:19:50,430
a new revision object additionally que

00:19:48,330 --> 00:19:53,430
native updates a route object and in

00:19:50,430 --> 00:19:55,640
turn updates a virtual service at this

00:19:53,430 --> 00:20:00,810
point the next revision is deployed and

00:19:55,640 --> 00:20:03,330
receiving 100% of requests so we could

00:20:00,810 --> 00:20:06,690
stop here and enjoy the sweet benefits

00:20:03,330 --> 00:20:09,540
of operation automation however Canada

00:20:06,690 --> 00:20:11,820
has one more trick up its sleeve que

00:20:09,540 --> 00:20:15,690
native serving is able to scale a

00:20:11,820 --> 00:20:17,870
revision from n to zero an approach that

00:20:15,690 --> 00:20:22,490
is frequently called serve

00:20:17,870 --> 00:20:26,620
as computing in a traditional

00:20:22,490 --> 00:20:30,320
environment resources must be acquired

00:20:26,620 --> 00:20:33,980
before a request can be received simply

00:20:30,320 --> 00:20:36,710
put in kubernetes terms if your pod is

00:20:33,980 --> 00:20:43,190
not up and running yet your application

00:20:36,710 --> 00:20:45,559
cannot receive requests in a service

00:20:43,190 --> 00:20:48,460
environment resources may be acquired

00:20:45,559 --> 00:20:52,400
after a request has been received in

00:20:48,460 --> 00:20:55,040
kubernetes terms even if your pod is not

00:20:52,400 --> 00:21:00,800
up and running yet your application may

00:20:55,040 --> 00:21:02,900
already receive requests typical

00:21:00,800 --> 00:21:05,540
implementations of a service environment

00:21:02,900 --> 00:21:09,080
do not release resources after

00:21:05,540 --> 00:21:12,700
processing a request immediately instead

00:21:09,080 --> 00:21:15,500
once acquired resources are held in

00:21:12,700 --> 00:21:20,990
anticipation of additional requests for

00:21:15,500 --> 00:21:23,870
some period of time code path refers to

00:21:20,990 --> 00:21:27,050
the situation where receiving a request

00:21:23,870 --> 00:21:32,540
and processing a request are separated

00:21:27,050 --> 00:21:34,550
by acquiring resources hot path refers

00:21:32,540 --> 00:21:38,050
to the situation where receiving a

00:21:34,550 --> 00:21:42,890
request and processing a request are not

00:21:38,050 --> 00:21:47,960
separated by acquiring resources but how

00:21:42,890 --> 00:21:50,690
does key native scale from n20 and if

00:21:47,960 --> 00:21:56,330
there is no part who is listening to

00:21:50,690 --> 00:22:00,080
your requests meet the key native

00:21:56,330 --> 00:22:03,290
serving activator for this walkthrough I

00:22:00,080 --> 00:22:06,380
assume one key native service service

00:22:03,290 --> 00:22:08,960
number one to revisions revision number

00:22:06,380 --> 00:22:11,630
one in revision number two and that the

00:22:08,960 --> 00:22:14,470
service is currently in roll out that is

00:22:11,630 --> 00:22:17,410
traffic is split between revisions

00:22:14,470 --> 00:22:22,059
currently both revision number one and

00:22:17,410 --> 00:22:24,310
revision number two are scale it to zero

00:22:22,059 --> 00:22:27,580
now this is a fun part

00:22:24,310 --> 00:22:30,940
when a request enters the system the

00:22:27,580 --> 00:22:33,670
Gateway inspects a request determines

00:22:30,940 --> 00:22:38,020
the service and selects a revision to

00:22:33,670 --> 00:22:39,930
process the request here I assume the

00:22:38,020 --> 00:22:43,240
Gateway selects revision number one

00:22:39,930 --> 00:22:46,120
since no instance of revision number one

00:22:43,240 --> 00:22:49,510
is running requests to revision number

00:22:46,120 --> 00:22:52,030
one are on a code path the Gateway is

00:22:49,510 --> 00:22:56,860
configured to forward the request to the

00:22:52,030 --> 00:22:59,620
activator the activator buffers the

00:22:56,860 --> 00:23:00,700
original request and sends a request to

00:22:59,620 --> 00:23:05,740
the autoscaler

00:23:00,700 --> 00:23:08,470
to scale revision number one the

00:23:05,740 --> 00:23:10,960
autoscaler sends a request to kubernetes

00:23:08,470 --> 00:23:13,060
to increase the replicas count of the

00:23:10,960 --> 00:23:20,650
deployment object corresponding to

00:23:13,060 --> 00:23:23,290
revision number one kubernetes creates a

00:23:20,650 --> 00:23:24,250
pot object and executes the queue and

00:23:23,290 --> 00:23:29,380
user container

00:23:24,250 --> 00:23:32,470
therefore scaling from zero to one the

00:23:29,380 --> 00:23:34,390
Gateway is configured to forward future

00:23:32,470 --> 00:23:37,690
requests for revision number one

00:23:34,390 --> 00:23:41,410
directly to a part of that revision by

00:23:37,690 --> 00:23:43,990
passing the activator the activator

00:23:41,410 --> 00:23:47,740
forwards a buffered original request of

00:23:43,990 --> 00:23:49,840
the queue container the queue container

00:23:47,740 --> 00:23:55,150
forwards the original request to the

00:23:49,840 --> 00:23:57,400
user container for processing in

00:23:55,150 --> 00:23:59,650
addition the queue container sends a

00:23:57,400 --> 00:24:02,950
request to the autoscaler to increase

00:23:59,650 --> 00:24:05,350
the in-flight request count omitted in

00:24:02,950 --> 00:24:07,600
this animation when the response of the

00:24:05,350 --> 00:24:09,850
user container is returned to the caller

00:24:07,600 --> 00:24:11,350
the queue container sends another

00:24:09,850 --> 00:24:15,390
request to the autoscaler

00:24:11,350 --> 00:24:15,390
to decrease the in-flight request count

00:24:16,470 --> 00:24:22,120
now to do this all over again when a new

00:24:19,750 --> 00:24:24,670
request enters the system the Gateway

00:24:22,120 --> 00:24:26,890
inspects a request determines the

00:24:24,670 --> 00:24:30,640
service and selects a revision to

00:24:26,890 --> 00:24:32,650
process the request here I assume the

00:24:30,640 --> 00:24:37,180
gateway selects revision number one

00:24:32,650 --> 00:24:37,690
again since an instance of revision

00:24:37,180 --> 00:24:40,480
number

00:24:37,690 --> 00:24:43,720
is running requests to revision number

00:24:40,480 --> 00:24:45,970
one are on a hot path the Gateway is

00:24:43,720 --> 00:24:48,640
configured to forward requests for

00:24:45,970 --> 00:24:52,950
revision number one directly to a part

00:24:48,640 --> 00:24:55,720
of that revision bypassing the activator

00:24:52,950 --> 00:24:57,640
again the queue container forwards to

00:24:55,720 --> 00:25:01,390
originally request to the user container

00:24:57,640 --> 00:25:03,430
for processing again the queue container

00:25:01,390 --> 00:25:08,230
sends a request to the autoscaler to

00:25:03,430 --> 00:25:11,320
increase in flight request count if the

00:25:08,230 --> 00:25:14,020
number of in-flight requests passes a

00:25:11,320 --> 00:25:16,390
configurable threshold the autoscaler

00:25:14,020 --> 00:25:18,040
sends a request to kubernetes to

00:25:16,390 --> 00:25:19,870
increase the replica count of the

00:25:18,040 --> 00:25:23,920
deployment object corresponding to

00:25:19,870 --> 00:25:26,380
revision number one kubernetes creates

00:25:23,920 --> 00:25:28,990
additional pods and execute the queue

00:25:26,380 --> 00:25:33,370
and you can use a container here scaling

00:25:28,990 --> 00:25:36,190
from one to three ultimately the same

00:25:33,370 --> 00:25:43,800
process takes place for revision number

00:25:36,190 --> 00:25:46,630
two in summary que native is a zero

00:25:43,800 --> 00:25:49,680
operations extension for reactive micro

00:25:46,630 --> 00:25:52,780
service applications on kubernetes

00:25:49,680 --> 00:25:55,150
within canada 'the que native serving is

00:25:52,780 --> 00:25:57,970
a zero operations extension for the

00:25:55,150 --> 00:26:01,270
lifecycle management of reactive micro

00:25:57,970 --> 00:26:03,370
service applications on kubernetes k

00:26:01,270 --> 00:26:05,410
native serving provides a dedicated

00:26:03,370 --> 00:26:08,320
abstraction for a micro service and

00:26:05,410 --> 00:26:12,780
automates its operation that is it

00:26:08,320 --> 00:26:15,730
automates deployment and rollout

00:26:12,780 --> 00:26:20,560
additionally k native serving is a

00:26:15,730 --> 00:26:23,650
service extension k native serving is

00:26:20,560 --> 00:26:27,790
able to scale a micro service from n to

00:26:23,650 --> 00:26:31,780
zero instances in response to service

00:26:27,790 --> 00:26:34,800
requests thank you very much with that I

00:26:31,780 --> 00:26:34,800
hand back to Andrew

00:26:35,830 --> 00:26:42,070
thank you thanks Dominic that was great

00:26:38,740 --> 00:26:44,380
so if you like how Dominic explained K

00:26:42,070 --> 00:26:46,210
native serving not just the content

00:26:44,380 --> 00:26:47,650
please come and talk to us after the

00:26:46,210 --> 00:26:49,600
presentation we'd love to hear your

00:26:47,650 --> 00:26:57,670
feedback and I think we have some time

00:26:49,600 --> 00:27:05,260
for some questions just one second for

00:26:57,670 --> 00:27:08,770
the microphone to arrive I won't ask if

00:27:05,260 --> 00:27:12,520
I run application in cognitive service

00:27:08,770 --> 00:27:15,930
and which component to ensures that my

00:27:12,520 --> 00:27:19,960
applications is a high available because

00:27:15,930 --> 00:27:23,680
my application perhaps you know has some

00:27:19,960 --> 00:27:27,930
trouble and it can be broke down in the

00:27:23,680 --> 00:27:32,290
Canadian Canadian service container so I

00:27:27,930 --> 00:27:37,120
should I develop another application to

00:27:32,290 --> 00:27:39,970
fix these to fix this problem or the

00:27:37,120 --> 00:27:43,270
Canadian who has a internal mechanism to

00:27:39,970 --> 00:27:45,730
have me to do that Canada actually does

00:27:43,270 --> 00:27:49,330
not address this problem can it it falls

00:27:45,730 --> 00:27:51,520
back on to kubernetes because a que

00:27:49,330 --> 00:27:53,430
native service or more specifically a

00:27:51,520 --> 00:27:56,310
revision of ack a native service

00:27:53,430 --> 00:27:59,620
ultimately translates into a deployment

00:27:56,310 --> 00:28:03,460
so from there on it inherits all the

00:27:59,620 --> 00:28:05,770
properties that the deployment exposes

00:28:03,460 --> 00:28:08,740
and since kubernetes deployment

00:28:05,770 --> 00:28:11,380
controller makes sure or at least tries

00:28:08,740 --> 00:28:13,540
as much as you can to bring up as many

00:28:11,380 --> 00:28:16,000
pots as specified in the replicas count

00:28:13,540 --> 00:28:18,490
k native falls back on to the

00:28:16,000 --> 00:28:21,970
availability guarantees of kubernetes

00:28:18,490 --> 00:28:25,090
but the problem the the point stands and

00:28:21,970 --> 00:28:27,430
is correct kubernetes deployment does

00:28:25,090 --> 00:28:29,950
not give you a guarantee that it can

00:28:27,430 --> 00:28:32,800
scale yes to this point it just gives

00:28:29,950 --> 00:28:34,480
you the guarantee it will keep trying k

00:28:32,800 --> 00:28:38,610
native does the same thing

00:28:34,480 --> 00:28:42,730
yep because you know our applications

00:28:38,610 --> 00:28:46,149
always was deployed deployed in our

00:28:42,730 --> 00:28:49,190
production environment through

00:28:46,149 --> 00:28:52,789
deployment workload and the stiff side

00:28:49,190 --> 00:28:56,470
workload but our application is a

00:28:52,789 --> 00:29:00,169
database system so there are so many

00:28:56,470 --> 00:29:03,500
unstable problems will be happened in

00:29:00,169 --> 00:29:08,500
the container so the Canadian stories is

00:29:03,500 --> 00:29:11,360
very convenient - for us if we want to

00:29:08,500 --> 00:29:12,380
provide as a database service to our

00:29:11,360 --> 00:29:15,710
customers yeah

00:29:12,380 --> 00:29:17,750
actually in that case so Canada Canada

00:29:15,710 --> 00:29:19,580
serving is specifically designed for

00:29:17,750 --> 00:29:22,100
reactive micro services that is

00:29:19,580 --> 00:29:24,590
stateless components that process HTTP

00:29:22,100 --> 00:29:27,380
requests if a database is running in

00:29:24,590 --> 00:29:29,750
your container then ke native serving is

00:29:27,380 --> 00:29:34,090
not the choice for you it's not a

00:29:29,750 --> 00:29:34,090
solution okay thank you you're welcome

00:29:38,080 --> 00:29:43,600
well thank you I'm a little bit confused

00:29:40,780 --> 00:29:48,220
with the concept with the cold path and

00:29:43,600 --> 00:29:50,560
how paths also so here confused about

00:29:48,220 --> 00:29:53,650
like say it seems that a native can

00:29:50,560 --> 00:29:57,190
sense first sense that there whether

00:29:53,650 --> 00:29:59,680
there is a rollout going on and then to

00:29:57,190 --> 00:30:02,350
put it and to trigger that with a cue

00:29:59,680 --> 00:30:04,390
container and then is that means that

00:30:02,350 --> 00:30:07,480
the to wait to finish that rollout and

00:30:04,390 --> 00:30:09,580
intuitive scaling so I mean like so if I

00:30:07,480 --> 00:30:13,390
see I mean the rollout and scaling will

00:30:09,580 --> 00:30:17,620
they happens simultaneously or a little

00:30:13,390 --> 00:30:22,630
bit confused about that thank you I see

00:30:17,620 --> 00:30:26,050
so actually the cue container is not

00:30:22,630 --> 00:30:28,600
involved in cold path or hot path the

00:30:26,050 --> 00:30:32,590
only component that is involved in cold

00:30:28,600 --> 00:30:36,610
path or hot path is the gateway and the

00:30:32,590 --> 00:30:40,090
activator the ingress gateway is if

00:30:36,610 --> 00:30:42,700
there is no no pod running the ingress

00:30:40,090 --> 00:30:45,160
gateway is configured to forward a

00:30:42,700 --> 00:30:47,320
request to the activator and then the

00:30:45,160 --> 00:30:54,220
activator will talk to the autoscaler to

00:30:47,320 --> 00:30:56,670
scale up in the future after just one

00:30:54,220 --> 00:30:56,670
second

00:31:02,760 --> 00:31:09,809
in the future after a pot has been

00:31:06,150 --> 00:31:12,690
acquired the gateway is reconfigured we

00:31:09,809 --> 00:31:17,040
are the ISTE of virtual service to then

00:31:12,690 --> 00:31:19,530
direct requests directly to the pot so

00:31:17,040 --> 00:31:21,690
this situation now or actually the blue

00:31:19,530 --> 00:31:24,960
line you can think of the blue line as

00:31:21,690 --> 00:31:28,710
the hot path and the queue container

00:31:24,960 --> 00:31:33,870
within the pot its sole responsibility

00:31:28,710 --> 00:31:36,270
is to count statistics so it every time

00:31:33,870 --> 00:31:38,070
it receives the requests it increases in

00:31:36,270 --> 00:31:40,740
flight requests count and every time it

00:31:38,070 --> 00:31:45,860
returns a response it decreases in

00:31:40,740 --> 00:31:45,860
flight requests come in super helpful

00:31:47,570 --> 00:31:54,380
please me have one question over there

00:31:51,260 --> 00:31:54,380
thank you

00:31:55,039 --> 00:31:58,460
there's some in the back

00:32:02,120 --> 00:32:07,909
hi the Gateway is configured to route

00:32:05,779 --> 00:32:09,919
certain requests to service one revision

00:32:07,909 --> 00:32:13,520
one and certain to say revision 2 while

00:32:09,919 --> 00:32:16,400
the rollout is happening right so now in

00:32:13,520 --> 00:32:18,440
the cold path we had the first request

00:32:16,400 --> 00:32:21,289
come to revision one and your user

00:32:18,440 --> 00:32:23,900
container came up and now it's directly

00:32:21,289 --> 00:32:27,409
going to the queue container off so this

00:32:23,900 --> 00:32:29,059
one revision one now say a request comes

00:32:27,409 --> 00:32:31,070
where the Gateway has to route it to

00:32:29,059 --> 00:32:33,770
revision 2 but the cold path is still

00:32:31,070 --> 00:32:37,880
enable there so to reduce the response

00:32:33,770 --> 00:32:41,750
time does it route it to so revision 1

00:32:37,880 --> 00:32:42,289
meanwhile no no no now not to my

00:32:41,750 --> 00:32:44,870
knowledge

00:32:42,289 --> 00:32:47,600
I mean to do please do not quote me on

00:32:44,870 --> 00:32:52,100
that but I am 99% sure no it does not

00:32:47,600 --> 00:32:54,460
right so if we wish for us to have SLA

00:32:52,100 --> 00:32:57,169
of 99 like triple nine or something and

00:32:54,460 --> 00:32:59,750
have lower response times can we

00:32:57,169 --> 00:33:03,559
configure it to do that though I do not

00:32:59,750 --> 00:33:06,770
think that this is a possibility no it's

00:33:03,559 --> 00:33:08,570
a it's a strictly probabilistic traffic

00:33:06,770 --> 00:33:10,450
split that does not take any other

00:33:08,570 --> 00:33:16,990
environmental conditions into account

00:33:10,450 --> 00:33:16,990
understand alright thank you very much

00:33:19,030 --> 00:33:23,200
all right I got I got the final warning

00:33:21,400 --> 00:33:25,420
so I'm not sure I can take more

00:33:23,200 --> 00:33:27,460
questions up here on stage but please do

00:33:25,420 --> 00:33:29,590
not hesitate come find us we're gonna

00:33:27,460 --> 00:33:31,330
hang out right in front of this room and

00:33:29,590 --> 00:33:34,800
are happy to answer any more questions

00:33:31,330 --> 00:33:38,150
thank you very much thanks for coming

00:33:34,800 --> 00:33:38,150

YouTube URL: https://www.youtube.com/watch?v=rHoGYlkPTpg


