Title: Optimising Mesos Utilization at Opentable - Jay Chin, Opentable
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Optimising Mesos Utilization at Opentable - Jay Chin, Opentable

I will provide an overview of how Mesos and Singularity is used at Opentable and the operational challenges supporting the various microservices that rely on Mesos. I will describe how we gather useful statistics on Mesos/Singularity and the tools we use (Grafana, Graphite, Elasticsearch, etc) to provide meaningful dashboards and data to users in order to optimise their usage and performance. I will also provide a brief description of the open source tools we have written for gathering statistics out of Mesos and building dashboards.

About Jay Chin
Jay is part of the Opentable Techops team that supports and engineer scalable solutions to support the Opentable platform. He currently supports the internal Mesos infrastructure that underpins the globally distributed microservices stack within the organization. Prior to opentable, Jay was involved in Grid computing at various financial services institutions where he managed compute clusters upwards of 100,000 cores.
Captions: 
	00:00:00,030 --> 00:00:05,160
hello everyone my name is Jason I'm part

00:00:03,540 --> 00:00:09,179
of the infrastructure engineering team

00:00:05,160 --> 00:00:11,190
at OpenTable where I lead the SRE team

00:00:09,179 --> 00:00:14,549
yeah

00:00:11,190 --> 00:00:16,560
so just bit of a background about me I

00:00:14,549 --> 00:00:19,710
used to come from Financial Services

00:00:16,560 --> 00:00:24,300
anyone from financial services yeah you

00:00:19,710 --> 00:00:27,900
know want to baby yeah and when I came

00:00:24,300 --> 00:00:31,529
to OpenTable it was sort of a culture

00:00:27,900 --> 00:00:33,059
shock for me back in the banks that I

00:00:31,529 --> 00:00:36,149
used to work with so I worked with

00:00:33,059 --> 00:00:38,460
compute grids lots of course running

00:00:36,149 --> 00:00:41,760
risk calculations overnight a shared

00:00:38,460 --> 00:00:44,399
grid that's used by different teams in a

00:00:41,760 --> 00:00:45,660
bank and whenever we introduced some

00:00:44,399 --> 00:00:50,010
sort of middleware there was always

00:00:45,660 --> 00:00:52,079
resistance development teams were very

00:00:50,010 --> 00:00:53,219
highly resistant to change

00:00:52,079 --> 00:00:56,309
I don't want to move to this new

00:00:53,219 --> 00:01:00,180
middleware but it was the exact opposite

00:00:56,309 --> 00:01:02,460
at opentable when I joined opentable I

00:01:00,180 --> 00:01:04,379
mean I think it was in the second week

00:01:02,460 --> 00:01:07,560
when team serum came to me and say I

00:01:04,379 --> 00:01:09,540
want to move to mesas when can you help

00:01:07,560 --> 00:01:11,310
us do that I mean we want to make those

00:01:09,540 --> 00:01:15,420
code changes we want to move to bases

00:01:11,310 --> 00:01:18,210
immediately and I was shocked and when I

00:01:15,420 --> 00:01:19,890
dug into it it was not just the

00:01:18,210 --> 00:01:21,990
technology of course missiles is great

00:01:19,890 --> 00:01:24,979
but it was the things that we built

00:01:21,990 --> 00:01:26,909
around mesas that sort of make the

00:01:24,979 --> 00:01:29,040
transition so much smoother for the

00:01:26,909 --> 00:01:32,130
development teams and everyone wanted to

00:01:29,040 --> 00:01:34,110
German to mesas so I'm here today to

00:01:32,130 --> 00:01:35,729
share with you guys on what we did to

00:01:34,110 --> 00:01:39,750
make mesas great and open table and

00:01:35,729 --> 00:01:47,040
while developers and engineers love Mesa

00:01:39,750 --> 00:01:50,040
so much okay so some numbers I guess how

00:01:47,040 --> 00:01:51,750
many of you actually use open table one

00:01:50,040 --> 00:01:53,759
two three

00:01:51,750 --> 00:01:55,049
it's it's not that big in Europe it's

00:01:53,759 --> 00:01:58,020
quite back in North America I mean

00:01:55,049 --> 00:02:01,020
that's that's our biggest market we're

00:01:58,020 --> 00:02:03,060
going to be 20 years a next year so it's

00:02:01,020 --> 00:02:07,140
a it's not a start-up it's quite an old

00:02:03,060 --> 00:02:10,920
company we've done about 1.4 billion

00:02:07,140 --> 00:02:13,710
online reservations we do about 2.3

00:02:10,920 --> 00:02:16,290
million dinars per month we had

00:02:13,710 --> 00:02:19,020
58 million verified reviews on

00:02:16,290 --> 00:02:21,240
restaurants so it's a if you're not

00:02:19,020 --> 00:02:23,340
looking through open table it's also a

00:02:21,240 --> 00:02:26,070
great place to go to look for restaurant

00:02:23,340 --> 00:02:30,180
reviews we have four to three thousand

00:02:26,070 --> 00:02:31,920
restaurants globally and then I think

00:02:30,180 --> 00:02:33,390
the is that's from 2017 showed that

00:02:31,920 --> 00:02:34,890
fifty five percent of all our

00:02:33,390 --> 00:02:40,080
reservations that we take come from

00:02:34,890 --> 00:02:42,360
mobile devices our busiest day is can

00:02:40,080 --> 00:02:46,740
anyone guess what's the busiest day you

00:02:42,360 --> 00:02:50,250
know Valentine's Day right I mean we get

00:02:46,740 --> 00:02:54,720
like 500 to 600 searches per second some

00:02:50,250 --> 00:02:57,030
stats on Valentine's Day 43% of all the

00:02:54,720 --> 00:03:00,060
reservations on Valentine's Day was the

00:02:57,030 --> 00:03:01,380
week leading up to the holiday so you

00:03:00,060 --> 00:03:03,840
know most people book you know

00:03:01,380 --> 00:03:07,020
Valentine's Days next week most people

00:03:03,840 --> 00:03:10,020
book this week last year we had

00:03:07,020 --> 00:03:13,080
something really unique the earliest

00:03:10,020 --> 00:03:16,230
reservation for 2016 Valentine's Day

00:03:13,080 --> 00:03:18,300
came on the 2015 Valentine's Day you

00:03:16,230 --> 00:03:21,090
know that's dedication right

00:03:18,300 --> 00:03:27,620
maybe that guy had bad experience

00:03:21,090 --> 00:03:30,720
booking a restaurant but yeah anyway so

00:03:27,620 --> 00:03:33,660
what does our text act look like well

00:03:30,720 --> 00:03:36,360
Bing Bing an old company we used to run

00:03:33,660 --> 00:03:40,740
everything before 2013 as a single

00:03:36,360 --> 00:03:42,630
single dotnet monolithic application it

00:03:40,740 --> 00:03:45,600
was a shared codebase all the

00:03:42,630 --> 00:03:48,450
applications contributed to their shared

00:03:45,600 --> 00:03:52,380
code base it was single tax tag across

00:03:48,450 --> 00:03:54,570
all our data centers globally every two

00:03:52,380 --> 00:03:58,440
three months or so when we do a release

00:03:54,570 --> 00:04:01,290
it sort of looked like the NASA command

00:03:58,440 --> 00:04:03,690
center all hands on deck right all the

00:04:01,290 --> 00:04:05,160
operations team development teams

00:04:03,690 --> 00:04:07,170
everyone was there because this was

00:04:05,160 --> 00:04:08,460
going to be a big release right we've

00:04:07,170 --> 00:04:10,860
worked two or three months putting those

00:04:08,460 --> 00:04:12,720
features let's do a big bang diploma now

00:04:10,860 --> 00:04:14,130
you know if things go wrong maybe we can

00:04:12,720 --> 00:04:16,489
fix the ear bugs you know make the

00:04:14,130 --> 00:04:18,780
release go well so everyone was there

00:04:16,489 --> 00:04:23,100
looking at metrics everything while we

00:04:18,780 --> 00:04:27,590
did the deployment sometimes things go

00:04:23,100 --> 00:04:30,740
well sometimes

00:04:27,590 --> 00:04:32,389
didn't go that well right so developer

00:04:30,740 --> 00:04:34,010
could be working on a feature for two or

00:04:32,389 --> 00:04:35,690
three months and then if something went

00:04:34,010 --> 00:04:38,000
wrong which was no fault of his own

00:04:35,690 --> 00:04:39,980
right someone introduced a bug his

00:04:38,000 --> 00:04:46,250
features had to be robach and it's

00:04:39,980 --> 00:04:48,020
probably him there all right so what we

00:04:46,250 --> 00:04:53,510
did to solve this problem was around

00:04:48,020 --> 00:04:55,160
2013 we moved to SOA all right so we

00:04:53,510 --> 00:04:58,370
split up the big monolith into

00:04:55,160 --> 00:05:00,590
individual services so as you can see we

00:04:58,370 --> 00:05:02,930
have this big website although it looks

00:05:00,590 --> 00:05:04,430
like a single website there's there's

00:05:02,930 --> 00:05:06,919
different features of the site's there

00:05:04,430 --> 00:05:10,030
for example search reveals emails all

00:05:06,919 --> 00:05:13,880
this were split into different services

00:05:10,030 --> 00:05:15,880
and it was great because we told the

00:05:13,880 --> 00:05:19,340
developers now split yourself into teams

00:05:15,880 --> 00:05:20,960
use whatever technology you like do

00:05:19,340 --> 00:05:24,320
whatever you want

00:05:20,960 --> 00:05:27,560
feel free go WOW right and developers

00:05:24,320 --> 00:05:29,450
just went out there you know sort of

00:05:27,560 --> 00:05:33,590
looked at bleeding edge technology so we

00:05:29,450 --> 00:05:36,560
have done that we have no cheers we even

00:05:33,590 --> 00:05:38,660
had closure we had everything that

00:05:36,560 --> 00:05:39,080
because they were micro services it was

00:05:38,660 --> 00:05:41,840
fine

00:05:39,080 --> 00:05:45,260
right they sort of didn't depend on a

00:05:41,840 --> 00:05:47,270
single shared code base and then with

00:05:45,260 --> 00:05:50,510
this came independent release of

00:05:47,270 --> 00:05:53,090
features and services from a two-man

00:05:50,510 --> 00:05:56,020
release cycle now we had thousands of

00:05:53,090 --> 00:05:58,340
deployments to production a week so

00:05:56,020 --> 00:05:59,500
product features could come out really

00:05:58,340 --> 00:06:04,150
really fast

00:05:59,500 --> 00:06:07,010
there was good iteration there and the

00:06:04,150 --> 00:06:09,650
product teams were really happy with

00:06:07,010 --> 00:06:12,260
what we did because now you know they

00:06:09,650 --> 00:06:15,560
take it instead of a two-man release

00:06:12,260 --> 00:06:18,140
cycle for product features such shutting

00:06:15,560 --> 00:06:23,060
it down to one or two weeks so which was

00:06:18,140 --> 00:06:25,940
great and how did we host those services

00:06:23,060 --> 00:06:29,450
was true virtualization so you can think

00:06:25,940 --> 00:06:33,050
of each of those services running on its

00:06:29,450 --> 00:06:35,240
own house as a single virtual machine so

00:06:33,050 --> 00:06:38,000
you can see their datacenter you know

00:06:35,240 --> 00:06:39,390
all those VMs running in those data

00:06:38,000 --> 00:06:43,380
centers

00:06:39,390 --> 00:06:45,060
and it was okay the developers had to

00:06:43,380 --> 00:06:50,760
write a bit of puppet code to get those

00:06:45,060 --> 00:06:53,040
virtual machines up and running and then

00:06:50,760 --> 00:06:56,520
when it came to scaling all we needed to

00:06:53,040 --> 00:06:59,400
do was just to clone the VMS simple all

00:06:56,520 --> 00:07:02,250
right but by the end of 2013 I think we

00:06:59,400 --> 00:07:05,040
had around 1200 VMs running around on

00:07:02,250 --> 00:07:10,020
all our data centers so loads of virtual

00:07:05,040 --> 00:07:11,760
machines the infrastructure teams which

00:07:10,020 --> 00:07:14,640
is same that I'm with infrastructure

00:07:11,760 --> 00:07:18,090
engineering though had a different

00:07:14,640 --> 00:07:20,760
viewpoint on what was happening so that

00:07:18,090 --> 00:07:22,770
that says stay in a life of us this is

00:07:20,760 --> 00:07:27,150
what we were doing herding cats all day

00:07:22,770 --> 00:07:28,530
it's because everyone was doing things

00:07:27,150 --> 00:07:30,450
on their own and there was no

00:07:28,530 --> 00:07:31,920
standardization everyone had their own

00:07:30,450 --> 00:07:32,520
logging some people had their own

00:07:31,920 --> 00:07:34,380
metrics

00:07:32,520 --> 00:07:36,270
some people even wrote puppet code to

00:07:34,380 --> 00:07:37,950
create their own logging cluster for

00:07:36,270 --> 00:07:40,950
example the own metrics collection

00:07:37,950 --> 00:07:44,190
cluster we spend most of our day

00:07:40,950 --> 00:07:46,290
actually just reviewing puppet

00:07:44,190 --> 00:07:50,870
infrastructure code how many of you have

00:07:46,290 --> 00:07:53,100
done puppet yeah do you like it

00:07:50,870 --> 00:07:56,220
raise your hands there anyone who likes

00:07:53,100 --> 00:07:59,250
writing proper code right so it's even

00:07:56,220 --> 00:08:01,220
worse for developers right because I

00:07:59,250 --> 00:08:03,540
think switching from actually developing

00:08:01,220 --> 00:08:05,220
in your app to actually doing puppet

00:08:03,540 --> 00:08:09,060
code is sort of like contact switching

00:08:05,220 --> 00:08:10,920
and we had puppet code to do everything

00:08:09,060 --> 00:08:14,010
so not just bringing up virtual machines

00:08:10,920 --> 00:08:15,710
but also monitoring metrics everything

00:08:14,010 --> 00:08:20,540
everything was done through puppet

00:08:15,710 --> 00:08:22,500
infrastructure scope all right so

00:08:20,540 --> 00:08:24,300
infrastructure engineering fell you know

00:08:22,500 --> 00:08:29,030
we were sort of to blame for you know

00:08:24,300 --> 00:08:32,370
introducing this this is what the whole

00:08:29,030 --> 00:08:33,720
development lifecycle looked like from a

00:08:32,370 --> 00:08:37,110
developer's point of view so there were

00:08:33,720 --> 00:08:39,120
four stages their local bill provision

00:08:37,110 --> 00:08:42,510
metrics monitoring and year local

00:08:39,120 --> 00:08:44,780
developers road code then they had to

00:08:42,510 --> 00:08:47,100
write infrastructure code to actually

00:08:44,780 --> 00:08:48,900
test it in you know their local

00:08:47,100 --> 00:08:51,540
environment so every all the year

00:08:48,900 --> 00:08:53,280
developers actually pull out the central

00:08:51,540 --> 00:08:55,020
puppet repository

00:08:53,280 --> 00:08:57,630
and then wrote infrastructure puppet

00:08:55,020 --> 00:08:59,880
code to actually bring up a virtual

00:08:57,630 --> 00:09:02,940
machine instance that would run deco so

00:08:59,880 --> 00:09:05,400
things like dependencies and you know

00:09:02,940 --> 00:09:07,620
extra service like running cron jobs for

00:09:05,400 --> 00:09:11,580
example everything had to be written by

00:09:07,620 --> 00:09:14,190
the developers due to get this done once

00:09:11,580 --> 00:09:17,250
they have tested this on a local vagrant

00:09:14,190 --> 00:09:19,620
bill so we used vagrant they would

00:09:17,250 --> 00:09:21,000
actually raise a review to the

00:09:19,620 --> 00:09:22,890
infrastructure teams they actually push

00:09:21,000 --> 00:09:25,140
push this out into production so

00:09:22,890 --> 00:09:28,520
infrastructure teams they daily

00:09:25,140 --> 00:09:32,640
reviewing all those infrastructure code

00:09:28,520 --> 00:09:34,440
once once it goes into production it's

00:09:32,640 --> 00:09:37,470
only ten that you can actually provision

00:09:34,440 --> 00:09:40,800
those VMs out into our data centers

00:09:37,470 --> 00:09:42,990
I think our puppet code base grew up to

00:09:40,800 --> 00:09:45,360
about one gigabyte of source code that's

00:09:42,990 --> 00:09:49,440
how big it became right because everyone

00:09:45,360 --> 00:09:52,050
was doing something different and same

00:09:49,440 --> 00:09:55,410
with the metrics part right

00:09:52,050 --> 00:09:58,530
it was also puppet code everything was

00:09:55,410 --> 00:10:01,010
controlled through puppet monitoring was

00:09:58,530 --> 00:10:03,390
the same but the good thing is

00:10:01,010 --> 00:10:06,000
everything's viewable as code right

00:10:03,390 --> 00:10:10,620
infrastructure code everything's there

00:10:06,000 --> 00:10:15,300
so you developers this this is how they

00:10:10,620 --> 00:10:20,190
felt about puppet something had to

00:10:15,300 --> 00:10:24,900
change around 2014 we started to look at

00:10:20,190 --> 00:10:27,030
mazes and we explored the possibility of

00:10:24,900 --> 00:10:31,350
instead of using VMs we could actually

00:10:27,030 --> 00:10:34,710
run them as meso services well that

00:10:31,350 --> 00:10:38,580
solve a few problems so instead of

00:10:34,710 --> 00:10:40,140
writing puppet code to deploy virtual

00:10:38,580 --> 00:10:43,640
machines on your own environment and to

00:10:40,140 --> 00:10:46,050
our data centers now this is how the

00:10:43,640 --> 00:10:49,260
development lifecycle look like after we

00:10:46,050 --> 00:10:52,740
had masers so look at the purple boxes

00:10:49,260 --> 00:10:54,210
it's blue here purple boxes up there now

00:10:52,740 --> 00:10:57,510
all you need to do is after the right

00:10:54,210 --> 00:10:59,970
code local doctor testing push it to a

00:10:57,510 --> 00:11:03,570
docker repo and then all they needed to

00:10:59,970 --> 00:11:06,209
do was push the stalker images into the

00:11:03,570 --> 00:11:06,760
various mason's clusters so we have a

00:11:06,209 --> 00:11:09,190
few

00:11:06,760 --> 00:11:12,190
versus some running in the cloud some

00:11:09,190 --> 00:11:15,010
running on-premise their globally so it

00:11:12,190 --> 00:11:19,000
does we have data centers in London we

00:11:15,010 --> 00:11:22,150
have data centers in United States so

00:11:19,000 --> 00:11:26,950
each service had to deploy to one of

00:11:22,150 --> 00:11:30,310
those data centers the metrics part of

00:11:26,950 --> 00:11:32,100
things will still remain the same they

00:11:30,310 --> 00:11:36,070
still had to write puppet code

00:11:32,100 --> 00:11:38,100
monitoring was the same now looking at

00:11:36,070 --> 00:11:43,080
this the infrastructure team thought

00:11:38,100 --> 00:11:46,630
okay let's see what we can do with

00:11:43,080 --> 00:11:48,460
metrics perhaps we could standardize the

00:11:46,630 --> 00:11:51,910
way metrics are collected since everyone

00:11:48,460 --> 00:11:53,740
is in on maysa now all the metrics we

00:11:51,910 --> 00:11:56,560
could get most of the metrics do the

00:11:53,740 --> 00:11:59,470
api's for example maysa api is the

00:11:56,560 --> 00:12:03,640
singularity api so we use singularity as

00:11:59,470 --> 00:12:07,270
the scheduler and we design a metrics

00:12:03,640 --> 00:12:09,640
pipeline so we wrote something called

00:12:07,270 --> 00:12:13,540
meso stats it's open source your else

00:12:09,640 --> 00:12:16,620
here that collected stats from may sews

00:12:13,540 --> 00:12:19,990
it goes into singularity goes into mesas

00:12:16,620 --> 00:12:23,530
collects those metrics pumps them into a

00:12:19,990 --> 00:12:27,010
Kafka queue and then it goes into a

00:12:23,530 --> 00:12:29,980
graphite cluster anyone use graphite out

00:12:27,010 --> 00:12:34,060
there yep lots of graphite users I love

00:12:29,980 --> 00:12:36,460
graphite the amount of metrics we pumped

00:12:34,060 --> 00:12:39,190
in was so much that we had to change the

00:12:36,460 --> 00:12:41,560
default cubben relay as you can see that

00:12:39,190 --> 00:12:44,110
it's a cup and c relay do you guys see

00:12:41,560 --> 00:12:46,720
this govern C relay yeah it's great if

00:12:44,110 --> 00:12:49,300
you run into performance issues you know

00:12:46,720 --> 00:12:51,340
it's a to replace it we've come and see

00:12:49,300 --> 00:12:52,960
relay and then we had Griffin as the

00:12:51,340 --> 00:12:57,670
front end where you know all the

00:12:52,960 --> 00:12:59,590
dashboards for me one significant thing

00:12:57,670 --> 00:13:02,920
that we did with Griffin Griffin ax is

00:12:59,590 --> 00:13:05,650
to use Griffin olive what we did was we

00:13:02,920 --> 00:13:08,710
created templates for their dashboards

00:13:05,650 --> 00:13:11,620
so any application team that started

00:13:08,710 --> 00:13:15,070
using maysa would get a dashboard

00:13:11,620 --> 00:13:16,900
created automatically for them so there

00:13:15,070 --> 00:13:19,000
would be a nice so to start running a

00:13:16,900 --> 00:13:20,320
maze O's you get a dashboard that shows

00:13:19,000 --> 00:13:23,079
your CPU you say

00:13:20,320 --> 00:13:24,850
memory and various other stats all for

00:13:23,079 --> 00:13:26,170
free you know you don't even have to

00:13:24,850 --> 00:13:28,180
touch your thing

00:13:26,170 --> 00:13:30,399
the other nice thing that we did with

00:13:28,180 --> 00:13:33,459
the graph on our dashboards is not just

00:13:30,399 --> 00:13:35,980
put graphs but to actually put text so

00:13:33,459 --> 00:13:38,769
you can't see the attacks there but it

00:13:35,980 --> 00:13:40,750
does explain what those graphs me so if

00:13:38,769 --> 00:13:43,720
something is created automatically for

00:13:40,750 --> 00:13:46,000
you most of the time you don't realize

00:13:43,720 --> 00:13:47,829
or don't even know what it is

00:13:46,000 --> 00:13:52,540
so we had to have tax in there because

00:13:47,829 --> 00:13:54,519
all this was template ice and developers

00:13:52,540 --> 00:13:57,550
were really really happy because you

00:13:54,519 --> 00:13:59,410
know deploy a service in May so I get

00:13:57,550 --> 00:14:02,310
this dashboard I will get help text that

00:13:59,410 --> 00:14:06,220
tells me what those dashboards ah

00:14:02,310 --> 00:14:09,339
so everything was automated um the other

00:14:06,220 --> 00:14:13,930
thing that helped with those dashboards

00:14:09,339 --> 00:14:17,079
was resource usage so this is an example

00:14:13,930 --> 00:14:20,709
singularity toss which which requests

00:14:17,079 --> 00:14:23,470
256 megabytes of memory on those

00:14:20,709 --> 00:14:26,019
dashboards we actually show the requests

00:14:23,470 --> 00:14:29,740
as well as the actual use so the request

00:14:26,019 --> 00:14:32,560
is in red the actual use is in yellow

00:14:29,740 --> 00:14:35,620
down there so you can see memory has

00:14:32,560 --> 00:14:39,040
been over provision for this box all

00:14:35,620 --> 00:14:41,050
right and the orange graph there is a

00:14:39,040 --> 00:14:42,550
recommendation from the infrastructure

00:14:41,050 --> 00:14:46,839
teams this is probably what you should

00:14:42,550 --> 00:14:48,850
change it to right so so a lot of new

00:14:46,839 --> 00:14:50,980
developers especially new ones would

00:14:48,850 --> 00:14:53,680
just put random numbers 2 5 6 megabyte

00:14:50,980 --> 00:14:56,949
of memory 0.1 CPU Oh sometimes you know

00:14:53,680 --> 00:15:00,790
I need more CPU one sip 1.0 C for you

00:14:56,949 --> 00:15:02,350
but those rent randoms those numbers are

00:15:00,790 --> 00:15:03,970
actually quite random you know I'll just

00:15:02,350 --> 00:15:07,360
come up with some number and I'll just

00:15:03,970 --> 00:15:11,920
use them but they actually involve

00:15:07,360 --> 00:15:15,130
wastage on our resources so having this

00:15:11,920 --> 00:15:17,439
sort of graphs actually does a bit of

00:15:15,130 --> 00:15:22,990
governance on you know the actual

00:15:17,439 --> 00:15:25,480
resource that are being used and our

00:15:22,990 --> 00:15:30,160
finance team really liked it because it

00:15:25,480 --> 00:15:33,399
save got a lot of money so everyone all

00:15:30,160 --> 00:15:34,180
the resource usage was optimized the

00:15:33,399 --> 00:15:37,510
other thing that

00:15:34,180 --> 00:15:39,820
I could get from those metrics and usage

00:15:37,510 --> 00:15:42,430
graphs was right sizing in our cloud

00:15:39,820 --> 00:15:45,490
instances so for example changing from

00:15:42,430 --> 00:15:47,770
our tree instances to m4 instances

00:15:45,490 --> 00:15:51,190
actually am four to our tree instances

00:15:47,770 --> 00:15:53,560
save as 20% of our cloud usage costs but

00:15:51,190 --> 00:15:56,529
it's all about having those metrics

00:15:53,560 --> 00:15:58,240
common metrics and being able to make

00:15:56,529 --> 00:16:03,910
sense of those metrics that you can make

00:15:58,240 --> 00:16:06,640
those informed decisions now after those

00:16:03,910 --> 00:16:09,510
after the automated dashboards this was

00:16:06,640 --> 00:16:10,959
what the development lifecycle look like

00:16:09,510 --> 00:16:14,350
so

00:16:10,959 --> 00:16:16,480
we've automated the atop Bart deer the

00:16:14,350 --> 00:16:19,120
only optional metrics that needed to be

00:16:16,480 --> 00:16:20,950
done was if some application had some

00:16:19,120 --> 00:16:24,010
custom metric that they want to be sent

00:16:20,950 --> 00:16:26,410
for example say JVM heap size fake

00:16:24,010 --> 00:16:28,810
somewhat so that they probably need to

00:16:26,410 --> 00:16:30,700
write extra code to do but all the abase

00:16:28,810 --> 00:16:35,830
health and welfare like CPU memory

00:16:30,700 --> 00:16:37,450
everything was done automatically so we

00:16:35,830 --> 00:16:39,640
start so the infrastructure teams then

00:16:37,450 --> 00:16:42,040
started to look a bit more into this and

00:16:39,640 --> 00:16:45,430
we had the final piece of the puzzle

00:16:42,040 --> 00:16:49,180
that self in terms of monitoring and we

00:16:45,430 --> 00:16:52,390
also look at the above the way our

00:16:49,180 --> 00:16:56,220
developers were doing deployments and we

00:16:52,390 --> 00:17:00,690
wanted to make things a bit better as

00:16:56,220 --> 00:17:05,530
usual so we develop something called sue

00:17:00,690 --> 00:17:09,910
it's open source to what what it is is a

00:17:05,530 --> 00:17:16,689
global deployment tool that we use

00:17:09,910 --> 00:17:19,209
internally and what it does is it

00:17:16,689 --> 00:17:22,059
abstracts out all the trust information

00:17:19,209 --> 00:17:26,410
so all the developer needs to do now is

00:17:22,059 --> 00:17:30,179
to write code DUSU deploy that's an

00:17:26,410 --> 00:17:34,390
internal command it actually builds the

00:17:30,179 --> 00:17:36,580
docker images wish with some extra meta

00:17:34,390 --> 00:17:39,460
information it goes into your docker

00:17:36,580 --> 00:17:42,429
repository and a deployment to any of

00:17:39,460 --> 00:17:45,580
our main source clusters or environments

00:17:42,429 --> 00:17:48,490
would involve just a manifest change so

00:17:45,580 --> 00:17:51,670
they would have a foul I'll show you

00:17:48,490 --> 00:17:54,550
tens of this far later on say change so

00:17:51,670 --> 00:17:58,150
for example if I want version 0.1 to run

00:17:54,550 --> 00:18:00,160
in the North America cluster and the

00:17:58,150 --> 00:18:03,460
London cluster all I needed to do was to

00:18:00,160 --> 00:18:06,760
change the contents of this file and the

00:18:03,460 --> 00:18:12,630
asou service would do the rest right

00:18:06,760 --> 00:18:15,100
the drm motivation for having a central

00:18:12,630 --> 00:18:21,150
deployment system like this was so that

00:18:15,100 --> 00:18:24,070
we could have governance in terms of

00:18:21,150 --> 00:18:26,290
knowing what was being deployed what the

00:18:24,070 --> 00:18:28,090
aversions are so the security teams

00:18:26,290 --> 00:18:30,130
would probably have hooks in there that

00:18:28,090 --> 00:18:33,280
could look at you know versions of

00:18:30,130 --> 00:18:35,740
libraries being packaged and if they

00:18:33,280 --> 00:18:37,900
were some of them were out of date

00:18:35,740 --> 00:18:40,030
we could also get statistics based on

00:18:37,900 --> 00:18:42,700
deployments we if something broke on our

00:18:40,030 --> 00:18:44,650
website having a global deployment

00:18:42,700 --> 00:18:48,030
manifest would show okay what were the

00:18:44,650 --> 00:18:53,320
three large changes that went into the

00:18:48,030 --> 00:18:58,170
open table site so this made middle of a

00:18:53,320 --> 00:19:01,510
lot of sense for us now this is what a

00:18:58,170 --> 00:19:03,520
global deployment file looks like so

00:19:01,510 --> 00:19:05,740
there you can see in the top line it's

00:19:03,520 --> 00:19:09,960
just the service name emails of people

00:19:05,740 --> 00:19:15,429
the services similar to the vamp

00:19:09,960 --> 00:19:21,300
deployment Wow and there we would have

00:19:15,429 --> 00:19:23,920
different clusters instance types memory

00:19:21,300 --> 00:19:25,480
trash shows that we need to have but the

00:19:23,920 --> 00:19:28,000
other thing that we added in there is

00:19:25,480 --> 00:19:30,280
the ability to add monitoring as well so

00:19:28,000 --> 00:19:32,860
with this single file you get to define

00:19:30,280 --> 00:19:36,730
versions of your code that you want in

00:19:32,860 --> 00:19:39,130
the environment you you specify number

00:19:36,730 --> 00:19:40,690
of instances that you want you specify

00:19:39,130 --> 00:19:44,620
the types of monitoring that you want

00:19:40,690 --> 00:19:48,250
everything was automated then and then

00:19:44,620 --> 00:19:51,520
we get to this so with soo-wee folders

00:19:48,250 --> 00:19:56,500
automated monitoring metrics collection

00:19:51,520 --> 00:19:58,750
this is where we got to so with all this

00:19:56,500 --> 00:20:01,570
effort that we put in to make everything

00:19:58,750 --> 00:20:04,840
easy for developers

00:20:01,570 --> 00:20:06,729
the pickup of using mesas was really

00:20:04,840 --> 00:20:09,249
high everyone wanted to move to mesas

00:20:06,729 --> 00:20:10,840
because of this because previously they

00:20:09,249 --> 00:20:12,850
were writing lots of puppet code to do

00:20:10,840 --> 00:20:15,609
all the year plumbing but by moving

00:20:12,850 --> 00:20:18,279
Tomatoes all they needed to do was have

00:20:15,609 --> 00:20:23,169
code and a central single deployment

00:20:18,279 --> 00:20:28,239
file so that was one of them main high

00:20:23,169 --> 00:20:31,919
points of our mesas deployment the last

00:20:28,239 --> 00:20:38,200
thing that I want to talk about is

00:20:31,919 --> 00:20:40,479
logging and how we do logging so one

00:20:38,200 --> 00:20:42,580
problem we had with micro services was

00:20:40,479 --> 00:20:44,859
everyone was logging on their own

00:20:42,580 --> 00:20:48,389
different teams had different standards

00:20:44,859 --> 00:20:51,580
there was no consistency for example

00:20:48,389 --> 00:20:53,619
restaurant underscore ID is actually

00:20:51,580 --> 00:20:56,320
logged by one team and then in the next

00:20:53,619 --> 00:20:59,470
team it's called our ID another team is

00:20:56,320 --> 00:21:01,739
called res ID so it can be you know it's

00:20:59,470 --> 00:21:04,599
the same feel can be named differently

00:21:01,739 --> 00:21:06,970
the other example as well is feel

00:21:04,599 --> 00:21:08,679
fulfilled types is if you're logging

00:21:06,970 --> 00:21:12,249
duration for example how long and

00:21:08,679 --> 00:21:14,229
requested the name of the field could be

00:21:12,249 --> 00:21:16,059
the same it could be duration but one

00:21:14,229 --> 00:21:18,220
team could be using milliseconds the

00:21:16,059 --> 00:21:20,529
other team could be using seconds so

00:21:18,220 --> 00:21:22,179
there needed to be a way to standardize

00:21:20,529 --> 00:21:24,399
all of this if we were going to make

00:21:22,179 --> 00:21:27,639
full use of the logging data and make

00:21:24,399 --> 00:21:31,539
sense of it so what we did was create a

00:21:27,639 --> 00:21:33,279
global unified data model for logging we

00:21:31,539 --> 00:21:38,049
also built a central logging system

00:21:33,279 --> 00:21:40,450
based on Doc's Cabana kafka anyone who

00:21:38,049 --> 00:21:43,029
wanted to use the central logging system

00:21:40,450 --> 00:21:45,429
would need first to define a logging

00:21:43,029 --> 00:21:49,269
schema and this was how and that logging

00:21:45,429 --> 00:21:53,019
schema needed to be reviewed before it

00:21:49,269 --> 00:21:55,690
got accepted but what that does it is it

00:21:53,019 --> 00:21:58,359
allows us to ensure the uniqueness of

00:21:55,690 --> 00:22:02,340
the fields it allows us to ensure that

00:21:58,359 --> 00:22:05,710
all the effuse match one another and

00:22:02,340 --> 00:22:08,889
from there we get to build really cool

00:22:05,710 --> 00:22:12,399
stuff because then we had feels that

00:22:08,889 --> 00:22:14,289
match we could we could see a request ID

00:22:12,399 --> 00:22:15,370
from one service going on to the next

00:22:14,289 --> 00:22:22,900
service or

00:22:15,370 --> 00:22:27,880
this match okay so every request that

00:22:22,900 --> 00:22:30,370
goes into a open table has a specific

00:22:27,880 --> 00:22:35,580
request ID it's actually a UUID

00:22:30,370 --> 00:22:38,830
and we use this request ID to track the

00:22:35,580 --> 00:22:41,890
response on to every microservices that

00:22:38,830 --> 00:22:44,559
we have so we built this tool called

00:22:41,890 --> 00:22:46,120
timeline it actually shows the requests

00:22:44,559 --> 00:22:48,220
going on to different micro-services

00:22:46,120 --> 00:22:50,080
again it's open source you know feel

00:22:48,220 --> 00:22:53,140
free to go out and have a look if you

00:22:50,080 --> 00:22:56,140
want and a request that comes in to the

00:22:53,140 --> 00:22:58,000
Open Table website we can look at it in

00:22:56,140 --> 00:22:59,500
terms of timeline so here you can see

00:22:58,000 --> 00:23:02,950
those green bars are when you know

00:22:59,500 --> 00:23:06,130
things actually hit the service on the

00:23:02,950 --> 00:23:10,540
left so I'll do a quick demo of this of

00:23:06,130 --> 00:23:13,179
you know after the stock but what that

00:23:10,540 --> 00:23:14,920
allows us is to identify bottlenecks in

00:23:13,179 --> 00:23:16,450
services for example if you look at the

00:23:14,920 --> 00:23:19,690
graph here you can see a bit of white

00:23:16,450 --> 00:23:21,880
space on the left you know why why does

00:23:19,690 --> 00:23:24,010
that service start only after 70

00:23:21,880 --> 00:23:26,230
milliseconds and then you can see those

00:23:24,010 --> 00:23:28,750
two long bars there you know those are

00:23:26,230 --> 00:23:32,380
probably candidates for optimization so

00:23:28,750 --> 00:23:35,140
having tools like this allows teams to

00:23:32,380 --> 00:23:37,120
actually go in and look at dependencies

00:23:35,140 --> 00:23:41,710
between services as well as how to

00:23:37,120 --> 00:23:43,420
optimize services all right Oh actually

00:23:41,710 --> 00:23:46,470
it's the it's demo time

00:23:43,420 --> 00:23:46,470
so how quickly

00:23:50,490 --> 00:23:53,360
let's go

00:24:31,429 --> 00:24:38,869
so this is the open table website okay

00:24:37,219 --> 00:24:41,149
I'm going to share something with you

00:24:38,869 --> 00:24:43,909
you can actually tell everyone that it

00:24:41,149 --> 00:24:48,200
runs of mesas because if you scroll down

00:24:43,909 --> 00:24:49,089
to our website what to Mouse it's a

00:24:48,200 --> 00:24:53,029
trick

00:24:49,089 --> 00:24:55,580
you highlight the invisible down there

00:24:53,029 --> 00:24:59,389
you can actually see mesas there it runs

00:24:55,580 --> 00:25:02,409
on Mesa slave 32 sled - prod alright and

00:24:59,389 --> 00:25:05,690
in this request down here we have the

00:25:02,409 --> 00:25:07,159
version names the bills but the

00:25:05,690 --> 00:25:10,729
important thing I want to show you guys

00:25:07,159 --> 00:25:13,549
is the request ID so anything that comes

00:25:10,729 --> 00:25:15,769
into our Open Table website we have a

00:25:13,549 --> 00:25:20,690
unique ID down there so I'm gonna take

00:25:15,769 --> 00:25:24,820
the request ID from down here and I'm

00:25:20,690 --> 00:25:24,820
gonna paste this in our timeline to

00:25:47,800 --> 00:25:53,570
oops sorry it's

00:25:50,580 --> 00:25:56,960
the resolution okay

00:25:53,570 --> 00:26:02,350
there you go it's real time there you

00:25:56,960 --> 00:26:06,830
can see the services are on the left and

00:26:02,350 --> 00:26:08,690
just a hit on a single website involves

00:26:06,830 --> 00:26:11,119
quite a few micro services so you can

00:26:08,690 --> 00:26:13,580
see different restaurant api's reviews

00:26:11,119 --> 00:26:15,739
API and clicking on one of this would

00:26:13,580 --> 00:26:19,070
actually show you know the alocs ring

00:26:15,739 --> 00:26:21,979
that was used to generate this so using

00:26:19,070 --> 00:26:25,549
this two teams can actually see what the

00:26:21,979 --> 00:26:28,190
bottleneck is if the website is slow it

00:26:25,549 --> 00:26:35,539
even if you scroll down to this it even

00:26:28,190 --> 00:26:38,690
has lot locks you can find locks by

00:26:35,539 --> 00:26:40,549
clicking on this so if I give up here

00:26:38,690 --> 00:26:43,340
all right based on this we know where

00:26:40,549 --> 00:26:46,039
the log files are we know which maces

00:26:43,340 --> 00:26:49,159
hosts it ran on various things like they

00:26:46,039 --> 00:26:54,099
so so having a global schema for logging

00:26:49,159 --> 00:26:54,099
allows us to build fancy tools like this

00:27:02,640 --> 00:27:06,320
resolutions very small

00:27:08,380 --> 00:27:12,850
sorry about the resolution I mean this

00:27:10,600 --> 00:27:15,730
site actually shows open table

00:27:12,850 --> 00:27:19,330
reservations on happening in real time

00:27:15,730 --> 00:27:21,940
as you can see resolution is a bit small

00:27:19,330 --> 00:27:23,830
but it's it's a global map that shows

00:27:21,940 --> 00:27:26,620
all the reservations even now you can

00:27:23,830 --> 00:27:30,420
see multiple reservations coming in from

00:27:26,620 --> 00:27:30,420
North America and different countries

00:27:32,310 --> 00:27:35,490
all right

00:27:43,139 --> 00:27:51,549
so key takeaways when deploying missiles

00:27:48,249 --> 00:27:54,129
in the environment is map out the

00:27:51,549 --> 00:27:55,509
developer workflows constantly look for

00:27:54,129 --> 00:27:59,710
opportunities on outstanding

00:27:55,509 --> 00:28:02,019
standardized automate and enhance make

00:27:59,710 --> 00:28:03,730
metrics and monitoring part and parcel

00:28:02,019 --> 00:28:06,759
of the Amazo service that's how you get

00:28:03,730 --> 00:28:10,269
you know quick adoption of mesas at

00:28:06,759 --> 00:28:11,559
least first that works quite well the

00:28:10,269 --> 00:28:14,379
engineers don't always make the best

00:28:11,559 --> 00:28:17,590
choice out of resource usage help them

00:28:14,379 --> 00:28:19,059
make an informed choice with metrics and

00:28:17,590 --> 00:28:22,330
monitoring and the tools that you can

00:28:19,059 --> 00:28:26,169
build and having a common deployment

00:28:22,330 --> 00:28:28,239
pipeline allows us to build various

00:28:26,169 --> 00:28:29,889
tools that could hook in and sanitize

00:28:28,239 --> 00:28:34,239
those micro services in terms of

00:28:29,889 --> 00:28:38,559
security standardization also finally a

00:28:34,239 --> 00:28:42,940
global data model for logging allows us

00:28:38,559 --> 00:28:46,149
to make consistent analysis because the

00:28:42,940 --> 00:28:48,009
fields are consistent and allows us to

00:28:46,149 --> 00:28:50,710
build tools on top of it that you know

00:28:48,009 --> 00:28:55,210
makes analysis and troubleshooting much

00:28:50,710 --> 00:28:59,100
much easier so that's the end of my talk

00:28:55,210 --> 00:29:02,249
can leave a bit more time for Q&A

00:28:59,100 --> 00:29:02,249
there's one

00:29:12,770 --> 00:29:19,650
hi Tim hi J I always ask this question

00:29:17,790 --> 00:29:24,330
now how did you solve your database

00:29:19,650 --> 00:29:27,210
problem database problem yes how does

00:29:24,330 --> 00:29:30,920
that work in your whole self

00:29:27,210 --> 00:29:35,580
provisioning teams run their own things

00:29:30,920 --> 00:29:37,800
mazes world okay so for database we

00:29:35,580 --> 00:29:41,420
haven't looked at persistent storage yet

00:29:37,800 --> 00:29:45,660
so data out database is still runs off

00:29:41,420 --> 00:29:48,120
the great puppet codebase but we're

00:29:45,660 --> 00:29:50,850
looking into persistent storage in the

00:29:48,120 --> 00:29:55,110
coming future for example one candidate

00:29:50,850 --> 00:29:58,320
is our Redis instances that we have so

00:29:55,110 --> 00:30:01,770
we're looking into that first and they

00:29:58,320 --> 00:30:03,450
would run on your cluster yes that's

00:30:01,770 --> 00:30:08,870
what we're looking at right now but they

00:30:03,450 --> 00:30:08,870
currently don't clear answer Thanks

00:30:13,710 --> 00:30:20,590
hey thanks for the talk how have you

00:30:18,190 --> 00:30:24,580
found singularity can you maybe tell us

00:30:20,590 --> 00:30:30,060
a bit about how you've used it like any

00:30:24,580 --> 00:30:33,130
war stories or yeah yeah sure

00:30:30,060 --> 00:30:35,200
I mean you the use of singularity goes

00:30:33,130 --> 00:30:37,930
back a long way I mean it's back in 2014

00:30:35,200 --> 00:30:42,750
so at that time we evaluated quite a few

00:30:37,930 --> 00:30:46,030
frameworks and singularity singularity

00:30:42,750 --> 00:30:49,210
bit seemed the easiest of all the

00:30:46,030 --> 00:30:52,390
frameworks plus the developers at

00:30:49,210 --> 00:30:57,670
HubSpot who developed hilarity were very

00:30:52,390 --> 00:30:59,320
close to our core platform team there

00:30:57,670 --> 00:31:03,640
were the guys that brought in

00:30:59,320 --> 00:31:06,220
singularity in terms of use of it we

00:31:03,640 --> 00:31:08,040
found it easy I mean the interfaces

00:31:06,220 --> 00:31:12,400
break easy

00:31:08,040 --> 00:31:13,510
the API is are great and it's working

00:31:12,400 --> 00:31:18,820
out for us

00:31:13,510 --> 00:31:21,250
so we have no we're course we evaluating

00:31:18,820 --> 00:31:24,910
marathon and things like that but as of

00:31:21,250 --> 00:31:27,240
today we haven't found a need to change

00:31:24,910 --> 00:31:30,870
from singularity to something else

00:31:27,240 --> 00:31:33,010
and just a like very brief follow-up

00:31:30,870 --> 00:31:35,080
could you give me like some picture of

00:31:33,010 --> 00:31:37,050
the scale of the number of apps that

00:31:35,080 --> 00:31:38,470
you're running under each like

00:31:37,050 --> 00:31:40,150
singularity do you have like a

00:31:38,470 --> 00:31:44,020
singularity instance per measures

00:31:40,150 --> 00:31:46,300
cluster and how many apps or tasks are

00:31:44,020 --> 00:31:48,850
you roughly managing right if you can

00:31:46,300 --> 00:31:52,360
share it yeah yeah I think on the

00:31:48,850 --> 00:31:54,580
smaller clusters so all all our clusters

00:31:52,360 --> 00:31:57,820
run a single instance of singularity

00:31:54,580 --> 00:31:59,860
actually sing run single instance but

00:31:57,820 --> 00:32:08,320
three three notes of singularity in

00:31:59,860 --> 00:32:09,850
there the size of I think these so it

00:32:08,320 --> 00:32:11,290
scales quite a bit because as you know

00:32:09,850 --> 00:32:16,420
Valentine's Day we go quite big but

00:32:11,290 --> 00:32:20,200
we've gone up to a hundred so it would

00:32:16,420 --> 00:32:22,660
be probably about 800 slaves probably

00:32:20,200 --> 00:32:27,610
during Valentine's Day go up to 800 900

00:32:22,660 --> 00:32:32,039
days per cluster the amount of services

00:32:27,610 --> 00:32:34,960
would probably be in the range of 120

00:32:32,039 --> 00:32:38,169
wish and it would be about two to three

00:32:34,960 --> 00:32:40,090
thousand tasks running yeah there's a

00:32:38,169 --> 00:32:42,250
rough numbers because you know depending

00:32:40,090 --> 00:32:44,830
on the time you know if we do scale

00:32:42,250 --> 00:32:47,130
quite a bit up and down right thanks

00:32:44,830 --> 00:32:47,130
very much

00:32:53,169 --> 00:33:00,260
all right thank you thank you

00:32:56,260 --> 00:33:00,260

YouTube URL: https://www.youtube.com/watch?v=8WlaNIss20c


