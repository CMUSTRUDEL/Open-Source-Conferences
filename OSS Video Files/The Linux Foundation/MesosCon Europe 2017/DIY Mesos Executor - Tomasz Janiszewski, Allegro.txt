Title: DIY Mesos Executor - Tomasz Janiszewski, Allegro
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	DIY Mesos Executor - Tomasz Janiszewski, Allegro

The Mesos executor is a part of Mesos that could be replaced with custom implementation. The executor controls tasks lifecycle. In this talk I will present the benefits that comes from writing a custom executor.

About Tomasz Janiszewski
Tomasz is a software engineer passionate about distributed systems. He believes in free and open source philosophy and occasionally contributes to projects on GitHub. At Allegro he works as a Software Engineer working with Mesos and Marathon cluster.
Captions: 
	00:00:00,050 --> 00:00:06,960
so let's start i'm Tomek and I'm working

00:00:04,650 --> 00:00:08,280
at Allegra which is one of the biggest

00:00:06,960 --> 00:00:10,679
e-commerce solution in the Central

00:00:08,280 --> 00:00:14,070
Europe we have been using some messes

00:00:10,679 --> 00:00:17,640
and marathon since 2015 on a production

00:00:14,070 --> 00:00:21,539
and today I want to share our insights

00:00:17,640 --> 00:00:24,420
of why you should consider writing your

00:00:21,539 --> 00:00:28,949
custom methods executor to obtain cloud

00:00:24,420 --> 00:00:31,500
native application ecosystem so if you

00:00:28,949 --> 00:00:33,739
are not familiar with a message executor

00:00:31,500 --> 00:00:36,780
I strongly encourage you to see the

00:00:33,739 --> 00:00:40,170
message con Asia talk by V naught and

00:00:36,780 --> 00:00:42,719
Greg about deep dive for the message

00:00:40,170 --> 00:00:45,059
executors talking about more technical

00:00:42,719 --> 00:00:47,309
details of the message executors and how

00:00:45,059 --> 00:00:47,750
they works and authentication and stuff

00:00:47,309 --> 00:00:49,980
like this

00:00:47,750 --> 00:00:54,360
today I will focus only on a

00:00:49,980 --> 00:00:57,300
functionality without code just just

00:00:54,360 --> 00:00:58,949
will focus only what masters executors

00:00:57,300 --> 00:01:02,420
could give us in terms of building

00:00:58,949 --> 00:01:07,590
better ecosystem for our applications so

00:01:02,420 --> 00:01:10,710
just a quick recap to have to share the

00:01:07,590 --> 00:01:12,630
same strength of might executor is a

00:01:10,710 --> 00:01:15,479
process that is launched on agent in

00:01:12,630 --> 00:01:17,970
order to launch tasks it works for

00:01:15,479 --> 00:01:20,759
frameworks it just get a task info or

00:01:17,970 --> 00:01:24,750
executors info in fact from protobuf and

00:01:20,759 --> 00:01:28,549
start the task that is defined there so

00:01:24,750 --> 00:01:31,200
I hope you are familiar with this image

00:01:28,549 --> 00:01:35,930
and this is the architecture of messes

00:01:31,200 --> 00:01:40,079
and today we will be talking and about

00:01:35,930 --> 00:01:41,820
this these guys they are the custom

00:01:40,079 --> 00:01:45,329
executors for different frameworks like

00:01:41,820 --> 00:01:49,399
Hadoop at VI and what they are working

00:01:45,329 --> 00:01:52,290
they are getting information from

00:01:49,399 --> 00:01:55,170
framework that are passed by master and

00:01:52,290 --> 00:01:58,530
works on slay a agents and stars the

00:01:55,170 --> 00:02:04,920
tasks and executor is responsible for

00:01:58,530 --> 00:02:07,020
maintaining the whole task lifecycle so

00:02:04,920 --> 00:02:09,959
there are four types of executors

00:02:07,020 --> 00:02:13,140
there's a command executor that is

00:02:09,959 --> 00:02:17,010
probably the most known and used

00:02:13,140 --> 00:02:23,610
executor in the market it's using the

00:02:17,010 --> 00:02:29,010
v-0 API of methods it's fine to lead

00:02:23,610 --> 00:02:31,530
nicest native library it's it's has a

00:02:29,010 --> 00:02:33,900
capability of running only one task and

00:02:31,530 --> 00:02:35,640
there is a docker executor that is

00:02:33,900 --> 00:02:39,120
really similar to a comet executor but

00:02:35,640 --> 00:02:41,790
is running docker there's the default

00:02:39,120 --> 00:02:47,459
executor that was introduced in meses

00:02:41,790 --> 00:02:51,269
1.0 it's using v1 HTTP api so there is

00:02:47,459 --> 00:02:55,800
no native binding it has capability of

00:02:51,269 --> 00:02:57,540
running pods and multiple tasks and what

00:02:55,800 --> 00:03:01,080
we are talking about today is a custom

00:02:57,540 --> 00:03:03,630
executor so it can be written whatever

00:03:01,080 --> 00:03:05,580
you like you can use deprecated API if

00:03:03,630 --> 00:03:09,269
you like or I strongly encourage you to

00:03:05,580 --> 00:03:14,040
use an HTTP v18 API for writing your

00:03:09,269 --> 00:03:16,590
custom executor to build a cloud native

00:03:14,040 --> 00:03:20,100
applications we need some guides at

00:03:16,590 --> 00:03:22,890
Allegro we follow 12 factor ups if you

00:03:20,100 --> 00:03:26,160
are not familiar it's something like a

00:03:22,890 --> 00:03:28,829
manifest of how to write a cloud native

00:03:26,160 --> 00:03:33,480
application designed by Heroku it

00:03:28,829 --> 00:03:37,079
consists of the 12 factors that need to

00:03:33,480 --> 00:03:38,880
be fulfilled in order to say that

00:03:37,079 --> 00:03:40,250
application is cognitive and what does

00:03:38,880 --> 00:03:46,079
it mean to be a cloud native in a

00:03:40,250 --> 00:03:47,760
mesocycle system it means it it's not

00:03:46,079 --> 00:03:50,570
only bind to a method when the

00:03:47,760 --> 00:03:52,950
application is cognitive we it has a

00:03:50,570 --> 00:03:54,329
really small contract that need to be

00:03:52,950 --> 00:03:56,640
fulfilled in order to run that

00:03:54,329 --> 00:03:58,890
application in different schedulers so

00:03:56,640 --> 00:04:01,380
if you have a cloud native application

00:03:58,890 --> 00:04:03,870
there is no no worry to switch between

00:04:01,380 --> 00:04:06,000
marathon and masculinities whatever

00:04:03,870 --> 00:04:08,700
scheduler you like and there are no

00:04:06,000 --> 00:04:11,700
dependency in app so there is no problem

00:04:08,700 --> 00:04:13,560
with starting it up or upgrading your

00:04:11,700 --> 00:04:15,239
ecosystem because application has a

00:04:13,560 --> 00:04:17,590
small contract that need to be fulfilled

00:04:15,239 --> 00:04:20,579
in a result application to run

00:04:17,590 --> 00:04:24,160
so let's walk through these twelve

00:04:20,579 --> 00:04:27,570
factors the code base application should

00:04:24,160 --> 00:04:30,630
be built in from a one single repository

00:04:27,570 --> 00:04:31,840
so executors will not help us here

00:04:30,630 --> 00:04:33,910
dependencies

00:04:31,840 --> 00:04:36,010
this means that applications should have

00:04:33,910 --> 00:04:40,060
all its dependencies in the application

00:04:36,010 --> 00:04:41,650
so no it should not rely on a system so

00:04:40,060 --> 00:04:43,750
if your application has the dependency

00:04:41,650 --> 00:04:46,210
to a Java it probably should have

00:04:43,750 --> 00:04:48,760
embedded Java inside the application

00:04:46,210 --> 00:04:51,360
package if it needs a Tomcat to run the

00:04:48,760 --> 00:04:54,100
Tomcat should be there or any other HTTP

00:04:51,360 --> 00:04:56,260
server runtime like nginx or stuff like

00:04:54,100 --> 00:04:59,500
this everything should be in a single

00:04:56,260 --> 00:05:02,440
package that could be run just wave a

00:04:59,500 --> 00:05:06,300
single command or in a container if you

00:05:02,440 --> 00:05:08,740
can't statically link your application

00:05:06,300 --> 00:05:10,840
configuration configuration should be

00:05:08,740 --> 00:05:12,400
stored in an environment variables and

00:05:10,840 --> 00:05:15,639
this is the place

00:05:12,400 --> 00:05:16,690
when executors can help us I will talk

00:05:15,639 --> 00:05:21,700
about it later

00:05:16,690 --> 00:05:23,860
backing services this means that if you

00:05:21,700 --> 00:05:26,350
need to some other services for example

00:05:23,860 --> 00:05:28,570
application requires Tomcat to be run in

00:05:26,350 --> 00:05:32,140
it should be embedded within an

00:05:28,570 --> 00:05:35,260
application built released and run it's

00:05:32,140 --> 00:05:37,479
pretty simple so that this steps should

00:05:35,260 --> 00:05:41,169
be separated executor will not help us

00:05:37,479 --> 00:05:43,450
here processes application should use

00:05:41,169 --> 00:05:47,710
one or more processes and it depends on

00:05:43,450 --> 00:05:49,930
your workflow there are some application

00:05:47,710 --> 00:05:54,520
that are single threaded and inter

00:05:49,930 --> 00:05:56,440
executor could help us scale them but

00:05:54,520 --> 00:05:58,780
Byronic and other processes but

00:05:56,440 --> 00:06:02,110
currently marathon does analysis does

00:05:58,780 --> 00:06:05,590
not support changing the resources on

00:06:02,110 --> 00:06:10,330
the fly port minding pretty easy

00:06:05,590 --> 00:06:13,600
applications should expose services some

00:06:10,330 --> 00:06:15,690
services binded to a port so it is easy

00:06:13,600 --> 00:06:18,070
to achieve in a massive ecosystem

00:06:15,690 --> 00:06:21,520
concurrency were scaling up the process

00:06:18,070 --> 00:06:23,440
by adding more applications more

00:06:21,520 --> 00:06:25,570
instances of the same application this

00:06:23,440 --> 00:06:28,960
possibility this is interesting our

00:06:25,570 --> 00:06:31,039
application should be available to start

00:06:28,960 --> 00:06:33,020
and kill at any time

00:06:31,039 --> 00:06:35,270
but and we should provide a graceful way

00:06:33,020 --> 00:06:40,249
of starting the application up and a

00:06:35,270 --> 00:06:43,490
graceful shutdown left broad priority so

00:06:40,249 --> 00:06:47,050
your ops or DevOps team should have the

00:06:43,490 --> 00:06:50,529
same environment or as close as possible

00:06:47,050 --> 00:06:53,990
in terms of configuration and used tools

00:06:50,529 --> 00:06:58,159
locks application should stream the logs

00:06:53,990 --> 00:07:01,099
to standard output and it should not

00:06:58,159 --> 00:07:05,149
have dependency on some Cabana on air

00:07:01,099 --> 00:07:08,210
cast stock to get to know where to send

00:07:05,149 --> 00:07:10,550
the logs just standard output or

00:07:08,210 --> 00:07:13,509
standard error and echo system should

00:07:10,550 --> 00:07:17,509
take care of where the logs are putted

00:07:13,509 --> 00:07:19,969
admin processes even management tasks is

00:07:17,509 --> 00:07:23,089
a run of tasks so executor is not

00:07:19,969 --> 00:07:25,699
helpful there so to sum up there are

00:07:23,089 --> 00:07:27,979
three factors that we can fix with a

00:07:25,699 --> 00:07:30,979
custom executor can fix this possibility

00:07:27,979 --> 00:07:34,309
and logs let's talk about conflicts so

00:07:30,979 --> 00:07:37,399
the profit source configuration in

00:07:34,309 --> 00:07:40,909
environment variables what does it mean

00:07:37,399 --> 00:07:44,349
there shouldn't be any config files that

00:07:40,909 --> 00:07:46,819
sort in a repository there shouldn't be

00:07:44,349 --> 00:07:50,330
some magical endpoint to download

00:07:46,819 --> 00:07:52,309
configuration from application should

00:07:50,330 --> 00:07:56,479
start up with the environment filled

00:07:52,309 --> 00:08:00,199
with the proper configuration how it's

00:07:56,479 --> 00:08:04,330
working at Allegro we have a certificate

00:08:00,199 --> 00:08:07,819
authority which is signing a certificate

00:08:04,330 --> 00:08:11,089
obtained by NASA's agent we have written

00:08:07,819 --> 00:08:13,370
a message hook that is generating a

00:08:11,089 --> 00:08:18,830
certificate for an application before it

00:08:13,370 --> 00:08:22,129
starts then master's agent starts an

00:08:18,830 --> 00:08:25,399
executor executor gets the certificate

00:08:22,129 --> 00:08:28,180
in the environment and call the

00:08:25,399 --> 00:08:32,779
configuration store to get some

00:08:28,180 --> 00:08:35,539
configuration configuration store keeps

00:08:32,779 --> 00:08:39,229
the encrypted version of the

00:08:35,539 --> 00:08:41,569
configuration and it could be it need to

00:08:39,229 --> 00:08:44,420
be authenticated and the crypt with

00:08:41,569 --> 00:08:47,700
certificate obtained by

00:08:44,420 --> 00:08:49,860
certificate authority so the missus

00:08:47,700 --> 00:08:54,240
agent is a single point of trust because

00:08:49,860 --> 00:08:57,030
it it has some token to talk with the

00:08:54,240 --> 00:08:58,950
certificate authority and then executors

00:08:57,030 --> 00:09:01,080
take this configuration from the

00:08:58,950 --> 00:09:03,300
conflict star and fill up the

00:09:01,080 --> 00:09:05,910
environment variables for the task and

00:09:03,300 --> 00:09:10,080
task runs and it just read the

00:09:05,910 --> 00:09:12,500
environment and setup accordingly to the

00:09:10,080 --> 00:09:12,500
configuration

00:09:12,980 --> 00:09:18,900
yesterday was a talk about secrets and

00:09:16,130 --> 00:09:21,540
you may wonder why we haven't used a

00:09:18,900 --> 00:09:24,180
secret because when we started about

00:09:21,540 --> 00:09:27,780
more than two years ago there was no bad

00:09:24,180 --> 00:09:30,660
feature in meses it was secrets are

00:09:27,780 --> 00:09:34,140
available in a I think one point four so

00:09:30,660 --> 00:09:37,650
it is pretty new feature that's why we

00:09:34,140 --> 00:09:41,600
build something like this and why we are

00:09:37,650 --> 00:09:45,600
not filling up the configuration in a

00:09:41,600 --> 00:09:47,820
hook like a master sook because the

00:09:45,600 --> 00:09:51,780
certificate obtained from a certificate

00:09:47,820 --> 00:09:53,880
authority is used also to communicate

00:09:51,780 --> 00:09:55,560
between the applications so if the

00:09:53,880 --> 00:09:59,910
application needs some stronger security

00:09:55,560 --> 00:10:04,590
due to our compliance they authenticate

00:09:59,910 --> 00:10:11,810
using this certificate okay second thing

00:10:04,590 --> 00:10:15,990
is possibility so the application should

00:10:11,810 --> 00:10:16,860
can be started at any time what does it

00:10:15,990 --> 00:10:20,270
mean

00:10:16,860 --> 00:10:23,340
it should try to minimize startup time

00:10:20,270 --> 00:10:25,890
just in order to be able to scale fast

00:10:23,340 --> 00:10:29,690
or in if there's an outage too quickly

00:10:25,890 --> 00:10:34,940
to minimize the outage consequences and

00:10:29,690 --> 00:10:37,200
it should have should support shutdown

00:10:34,940 --> 00:10:39,420
when there is a sick term applications

00:10:37,200 --> 00:10:40,850
should graceful shutdown graceful

00:10:39,420 --> 00:10:43,530
shutdown means that the application will

00:10:40,850 --> 00:10:46,530
try to finish all the transactions and

00:10:43,530 --> 00:10:50,300
operation that were started before but

00:10:46,530 --> 00:10:50,300
do not accept an incoming traffic

00:10:51,100 --> 00:10:55,870
so take a look and Application Lifecycle

00:10:53,670 --> 00:10:58,540
when the application is starts it is

00:10:55,870 --> 00:11:02,850
it's fetching its dependencies like

00:10:58,540 --> 00:11:05,470
packages it's done by message feature

00:11:02,850 --> 00:11:08,680
then the health checks start checking

00:11:05,470 --> 00:11:11,950
application and application is starting

00:11:08,680 --> 00:11:14,110
booting up like link it dependencies and

00:11:11,950 --> 00:11:17,740
stuff like this then it have some time

00:11:14,110 --> 00:11:19,600
to warm up caches and then it answers to

00:11:17,740 --> 00:11:26,020
a health check that okay I'm healthy I

00:11:19,600 --> 00:11:28,440
can get incoming traffic so what happens

00:11:26,020 --> 00:11:31,990
here well application is healthy

00:11:28,440 --> 00:11:34,840
we should plug it into our discovery

00:11:31,990 --> 00:11:39,400
service solution into load balancers

00:11:34,840 --> 00:11:42,850
cache external caches like varnish into

00:11:39,400 --> 00:11:45,250
monitoring and stuff like this so this

00:11:42,850 --> 00:11:51,190
is hard to obtain with events sent from

00:11:45,250 --> 00:11:54,820
a framework at the end when the

00:11:51,190 --> 00:11:58,600
application is should be killed it

00:11:54,820 --> 00:12:01,660
should get a sixth term finish all its

00:11:58,600 --> 00:12:08,560
jobs like finish a database transaction

00:12:01,660 --> 00:12:10,510
and pending processes so in that so when

00:12:08,560 --> 00:12:13,630
the application gets a sick term it

00:12:10,510 --> 00:12:17,440
should be removed from discovery service

00:12:13,630 --> 00:12:20,470
and every every place that was it was

00:12:17,440 --> 00:12:23,350
blocked during the startup and then it

00:12:20,470 --> 00:12:26,260
moved to a killing State and when

00:12:23,350 --> 00:12:28,960
everything goes down and the grace will

00:12:26,260 --> 00:12:32,380
timeout a couple of seconds or minutes

00:12:28,960 --> 00:12:38,500
depends on application and it's kill it

00:12:32,380 --> 00:12:43,810
with kill - 9 so sick you if it doesn't

00:12:38,500 --> 00:12:47,170
stop properly so why we can't rely on

00:12:43,810 --> 00:12:51,780
events delivered by frameworks if you

00:12:47,170 --> 00:12:53,770
remember the message architecture the

00:12:51,780 --> 00:12:56,850
communicate with the communication

00:12:53,770 --> 00:13:00,550
between task and framework comes by the

00:12:56,850 --> 00:13:03,220
number of hot groups so there is when

00:13:00,550 --> 00:13:04,910
the instance is started and it's healthy

00:13:03,220 --> 00:13:07,790
versus executor is

00:13:04,910 --> 00:13:09,530
in the event a task is running and it's

00:13:07,790 --> 00:13:12,500
healthy to a massive agent masseuse

00:13:09,530 --> 00:13:13,700
agents and information to masters master

00:13:12,500 --> 00:13:14,780
and masters master in front of the

00:13:13,700 --> 00:13:18,050
framework Hey

00:13:14,780 --> 00:13:20,810
that instance is healthy so there is a

00:13:18,050 --> 00:13:23,720
huge delay between when the application

00:13:20,810 --> 00:13:24,470
is healthy and when the framework has

00:13:23,720 --> 00:13:27,140
the information

00:13:24,470 --> 00:13:29,090
what's more the framework will send an

00:13:27,140 --> 00:13:35,110
event when it gets that information and

00:13:29,090 --> 00:13:38,600
a huge table or even the delay so the

00:13:35,110 --> 00:13:41,210
time difference between when frameworks

00:13:38,600 --> 00:13:45,080
send an events or created an event and

00:13:41,210 --> 00:13:46,430
when the our config service discovery

00:13:45,080 --> 00:13:48,740
solution got it

00:13:46,430 --> 00:13:52,550
so as you can see in a piec it's about

00:13:48,740 --> 00:13:56,390
30 milliseconds and in our scale that

00:13:52,550 --> 00:14:01,130
means that some people will get a blank

00:13:56,390 --> 00:14:05,180
page when it comes to Allegro and we we

00:14:01,130 --> 00:14:08,150
can it's not acceptable for us to have

00:14:05,180 --> 00:14:11,660
errors just by design so that's why we

00:14:08,150 --> 00:14:20,180
move to to monitor application lifecycle

00:14:11,660 --> 00:14:24,860
in meses executors so here is a diagram

00:14:20,180 --> 00:14:27,350
of a task lifecycle msys it's simplified

00:14:24,860 --> 00:14:29,750
there are not all connection that should

00:14:27,350 --> 00:14:35,840
be here and there are a couple of tasks

00:14:29,750 --> 00:14:39,680
of statuses missing so I will describe

00:14:35,840 --> 00:14:43,460
what is here the gray circles are

00:14:39,680 --> 00:14:48,200
managed by messes so you have no control

00:14:43,460 --> 00:14:52,850
in an executor over that statuses the

00:14:48,200 --> 00:14:55,160
double circle or the terminal States the

00:14:52,850 --> 00:14:58,070
the task should could not be recover

00:14:55,160 --> 00:15:04,270
from that state and probably probably

00:14:58,070 --> 00:15:08,690
died and the - it circle are optional

00:15:04,270 --> 00:15:13,140
for example default execution comment

00:15:08,690 --> 00:15:18,490
executor does not send starting

00:15:13,140 --> 00:15:22,990
information theory 21.5 I believe and so

00:15:18,490 --> 00:15:25,300
let's take a look how the how the

00:15:22,990 --> 00:15:29,530
lifecycle looks so the first task is

00:15:25,300 --> 00:15:32,680
staging at that moment meses is trying

00:15:29,530 --> 00:15:35,020
to find a place to set up the task so

00:15:32,680 --> 00:15:36,460
there's no instance running in fact

00:15:35,020 --> 00:15:38,770
nothing happened

00:15:36,460 --> 00:15:41,080
it could finish in a terminal state if

00:15:38,770 --> 00:15:43,300
the task list has wrong configuration

00:15:41,080 --> 00:15:45,910
you could finished with error could be

00:15:43,300 --> 00:15:48,580
dropped and then there is a starting

00:15:45,910 --> 00:15:51,100
phase as you can see in the starting

00:15:48,580 --> 00:15:53,980
phase message executors restarted it

00:15:51,100 --> 00:15:56,940
registered two masses agent and then it

00:15:53,980 --> 00:15:56,940
should start the task

00:15:57,270 --> 00:16:04,240
so when task list in a starting phase

00:16:00,610 --> 00:16:08,860
it's like downloading on it's all

00:16:04,240 --> 00:16:10,900
dependencies in our case is it's just

00:16:08,860 --> 00:16:13,090
starting and then we start health

00:16:10,900 --> 00:16:17,130
checking that service so when the

00:16:13,090 --> 00:16:21,000
service is started and the health checks

00:16:17,130 --> 00:16:25,590
are running it moves to running state

00:16:21,000 --> 00:16:30,250
the running state could have said a

00:16:25,590 --> 00:16:31,920
healthy state that's why this task the

00:16:30,250 --> 00:16:34,090
task could be could have multiple

00:16:31,920 --> 00:16:36,400
running statuses one after another

00:16:34,090 --> 00:16:39,970
because that it could be healthy

00:16:36,400 --> 00:16:42,520
unhealthy and works in a loop and from

00:16:39,970 --> 00:16:44,530
the running state when task is in the

00:16:42,520 --> 00:16:48,150
running state and for the first time is

00:16:44,530 --> 00:16:50,260
healthy we register the task in

00:16:48,150 --> 00:16:53,320
discovery service in our case it's

00:16:50,260 --> 00:16:58,350
consul and also with some load balancer

00:16:53,320 --> 00:17:01,450
if it's exposed to the rest of the world

00:16:58,350 --> 00:17:04,209
then when task is going to kill it

00:17:01,450 --> 00:17:06,790
should be moved to a killing State so

00:17:04,209 --> 00:17:11,170
before sending as sick theorem to an

00:17:06,790 --> 00:17:14,220
application we remove the application

00:17:11,170 --> 00:17:19,329
from remove the instance from config

00:17:14,220 --> 00:17:21,820
service discovery and from the from

00:17:19,329 --> 00:17:25,010
roadblasters so in this state

00:17:21,820 --> 00:17:28,990
application has some time to finish

00:17:25,010 --> 00:17:33,110
it's job and then if could move to be

00:17:28,990 --> 00:17:36,590
killed like it happens when it's like it

00:17:33,110 --> 00:17:42,110
regular upgrade felt if it was killed by

00:17:36,590 --> 00:17:44,299
a messes my Master's health check or

00:17:42,110 --> 00:17:50,419
finished if it's just a simple finished

00:17:44,299 --> 00:17:53,630
so so why do we need versus executors

00:17:50,419 --> 00:17:57,620
right here because you can say okay but

00:17:53,630 --> 00:18:00,350
default executors also has a property

00:17:57,620 --> 00:18:03,890
called grateful and start up a shutdown

00:18:00,350 --> 00:18:06,650
and grace period and in fact it's not

00:18:03,890 --> 00:18:10,460
working with the comment executors why

00:18:06,650 --> 00:18:13,990
because the comment executors started a

00:18:10,460 --> 00:18:20,150
comment with a shout so it started with

00:18:13,990 --> 00:18:23,570
SH - C and then some common trap so when

00:18:20,150 --> 00:18:26,419
you want to kill a task masters

00:18:23,570 --> 00:18:30,380
executors will just kill that have a

00:18:26,419 --> 00:18:33,470
handle to the shell and when it sent a

00:18:30,380 --> 00:18:37,400
signal to a shell shall immediately goes

00:18:33,470 --> 00:18:40,549
down then the Masters executors captured

00:18:37,400 --> 00:18:42,440
the event the sick child signal and see

00:18:40,549 --> 00:18:45,470
all that application is no longer

00:18:42,440 --> 00:18:47,900
running but in fact the application is

00:18:45,470 --> 00:18:52,910
running underneath but the initial goes

00:18:47,900 --> 00:18:57,559
down so there is no real grace period in

00:18:52,910 --> 00:19:00,049
a comment executors second thing that we

00:18:57,559 --> 00:19:03,380
we find when working with message health

00:19:00,049 --> 00:19:06,950
checks the message in a message CY

00:19:03,380 --> 00:19:09,770
Maseo startup time was wrong and we

00:19:06,950 --> 00:19:15,100
accidentally fix it by introducing the

00:19:09,770 --> 00:19:16,730
starting phase default execute command

00:19:15,100 --> 00:19:22,750
comment executors

00:19:16,730 --> 00:19:25,940
does not use starting task status so

00:19:22,750 --> 00:19:28,560
there is only a task running and because

00:19:25,940 --> 00:19:31,290
of how my sauce is handling the

00:19:28,560 --> 00:19:34,500
the same task statuses sometimes

00:19:31,290 --> 00:19:38,880
application was keeps the last health

00:19:34,500 --> 00:19:43,410
check timestamp as a as a task started

00:19:38,880 --> 00:19:45,120
it was not the big issue but it could be

00:19:43,410 --> 00:19:48,600
problematic when you go to UI and see

00:19:45,120 --> 00:19:50,280
that your tasks are started like a

00:19:48,600 --> 00:19:55,830
couple of minutes ago

00:19:50,280 --> 00:20:04,400
while they are working for a longer for

00:19:55,830 --> 00:20:08,010
a week or something like this okay so

00:20:04,400 --> 00:20:11,250
here is an example of graceful shutdown

00:20:08,010 --> 00:20:12,900
of one of our biggest application that

00:20:11,250 --> 00:20:18,900
is handling the traffic from our

00:20:12,900 --> 00:20:22,050
customers what you can see here at 3:50

00:20:18,900 --> 00:20:24,660
we enable executor so this is the

00:20:22,050 --> 00:20:27,060
regular deployment there was no graceful

00:20:24,660 --> 00:20:29,280
startup no graceful shutdown at that

00:20:27,060 --> 00:20:34,710
time as you can see we have over

00:20:29,280 --> 00:20:36,930
thousands of hours then we do a couple

00:20:34,710 --> 00:20:41,130
of restarts and you can see that with a

00:20:36,930 --> 00:20:45,570
graceful shutdown the errors has reduced

00:20:41,130 --> 00:20:53,430
to less than less than 500 and in fact

00:20:45,570 --> 00:20:55,500
about 200 and in the last at 350 we

00:20:53,430 --> 00:20:57,690
disabled executors this means we

00:20:55,500 --> 00:21:00,210
disabled the graceful shutdown of the

00:20:57,690 --> 00:21:02,700
application and do again a couple of

00:21:00,210 --> 00:21:06,660
fresh start and when you sum up that

00:21:02,700 --> 00:21:08,460
bars its return to the previous state

00:21:06,660 --> 00:21:12,540
when there are thousands of errors

00:21:08,460 --> 00:21:15,960
during the deployment yeah so that's

00:21:12,540 --> 00:21:20,550
that's how it helped us why there are

00:21:15,960 --> 00:21:24,060
errors even when the application is has

00:21:20,550 --> 00:21:26,160
aggregates we start a shutdown we

00:21:24,060 --> 00:21:28,200
suspect that the application needs some

00:21:26,160 --> 00:21:31,320
time to warm up and that was not

00:21:28,200 --> 00:21:35,580
implemented at the time so I haven't got

00:21:31,320 --> 00:21:38,130
a fresh fresh metrics from for that

00:21:35,580 --> 00:21:41,159
applications because it's pretty

00:21:38,130 --> 00:21:45,720
expensive to perform that type of death

00:21:41,159 --> 00:21:48,269
we start applications and some users may

00:21:45,720 --> 00:21:53,149
be complaining that they've got 500 on

00:21:48,269 --> 00:21:59,159
Telegraph yeah okay

00:21:53,149 --> 00:22:02,099
locks application should write its locks

00:21:59,159 --> 00:22:08,029
to an standard output so there should

00:22:02,099 --> 00:22:08,029
mean no log files and no need to manage

00:22:08,450 --> 00:22:14,599
log rotating and stuff like this when we

00:22:12,599 --> 00:22:17,700
started most of our application has a

00:22:14,599 --> 00:22:20,340
dependency to Cabana and all of our

00:22:17,700 --> 00:22:23,279
applications are sending locked directly

00:22:20,340 --> 00:22:27,690
to a Cabana this means when we want to

00:22:23,279 --> 00:22:30,210
change the URL or anything related to a

00:22:27,690 --> 00:22:32,159
log every application in our ecosystem

00:22:30,210 --> 00:22:34,950
needs to be updated for example when we

00:22:32,159 --> 00:22:40,639
were when we want to add some new field

00:22:34,950 --> 00:22:43,799
in metadata of each application in our

00:22:40,639 --> 00:22:46,259
ecosystem needs to be updated and this

00:22:43,799 --> 00:22:49,470
is this is hard to obtain when you've

00:22:46,259 --> 00:22:52,979
got more than 100 services because you

00:22:49,470 --> 00:22:56,279
need to ask the person people that are

00:22:52,979 --> 00:22:58,229
responsible for giving services hi could

00:22:56,279 --> 00:23:01,279
you update that dependency they probably

00:22:58,229 --> 00:23:03,330
said oh now we have some business to do

00:23:01,279 --> 00:23:07,470
we don't have time to do it

00:23:03,330 --> 00:23:10,039
so remove any dependency that is not

00:23:07,470 --> 00:23:14,039
needed from the application perspective

00:23:10,039 --> 00:23:16,590
so how it's working executors larger

00:23:14,039 --> 00:23:20,669
tasks and because it arches the task

00:23:16,590 --> 00:23:24,590
it has control over the STD STD out

00:23:20,669 --> 00:23:29,399
intested year of that task so it can

00:23:24,590 --> 00:23:32,039
manage to send the lock logs to some

00:23:29,399 --> 00:23:35,330
central lock store and also to write

00:23:32,039 --> 00:23:39,210
them to a file executor has all metadata

00:23:35,330 --> 00:23:41,519
about the task is running so the whole

00:23:39,210 --> 00:23:45,419
executor info that contains commenting

00:23:41,519 --> 00:23:48,019
for container info about the tasks so

00:23:45,419 --> 00:23:51,570
there there is no problem to other

00:23:48,019 --> 00:23:52,200
information for services like Cabana -

00:23:51,570 --> 00:23:57,869
so the

00:23:52,200 --> 00:24:01,559
but I can know in which bucket put put

00:23:57,869 --> 00:24:04,259
the lock you may wonder why we haven't

00:24:01,559 --> 00:24:06,739
used a container Locker for this because

00:24:04,259 --> 00:24:09,840
in fact the container logger is a

00:24:06,739 --> 00:24:13,109
interface that has the same capability

00:24:09,840 --> 00:24:15,749
you just implement one abstract class

00:24:13,109 --> 00:24:18,059
from the masses and it gets the same

00:24:15,749 --> 00:24:21,059
information as executor so there is

00:24:18,059 --> 00:24:24,169
executable info and you can write your

00:24:21,059 --> 00:24:26,309
logic what should happen with dialogues

00:24:24,169 --> 00:24:29,070
and the problem with the container

00:24:26,309 --> 00:24:31,919
logger is that you need to write it in a

00:24:29,070 --> 00:24:33,509
C++ and it's strongly related to a

00:24:31,919 --> 00:24:36,029
message so whenever you need you want to

00:24:33,509 --> 00:24:38,190
update the message you need to recompile

00:24:36,029 --> 00:24:40,259
your container logger and see and check

00:24:38,190 --> 00:24:43,679
if your dependencies has the same

00:24:40,259 --> 00:24:46,440
version as the as a message and if

00:24:43,679 --> 00:24:48,960
everything was compiled and linked

00:24:46,440 --> 00:24:51,629
properly and everything is working so

00:24:48,960 --> 00:24:54,929
that's why we prefer to use an executor

00:24:51,629 --> 00:24:58,799
because it has an HTTP API and there is

00:24:54,929 --> 00:25:04,639
a guarantee that whenever we update the

00:24:58,799 --> 00:25:07,679
mesos the executor will surely work so

00:25:04,639 --> 00:25:11,309
finally who is using this approach

00:25:07,679 --> 00:25:13,950
because I've seen that the marathon

00:25:11,309 --> 00:25:17,129
community is not using custom executors

00:25:13,950 --> 00:25:19,980
there are some code related to an custom

00:25:17,129 --> 00:25:25,129
executor in marathon but it looks like

00:25:19,980 --> 00:25:27,749
it's not it's not maintained from the

00:25:25,129 --> 00:25:31,470
couple of years right code isn't

00:25:27,749 --> 00:25:33,600
untouched from two or more years and we

00:25:31,470 --> 00:25:36,989
have some troubles when started using

00:25:33,600 --> 00:25:39,029
custom executors starting from the UI

00:25:36,989 --> 00:25:42,539
that was completely unaware of the

00:25:39,029 --> 00:25:47,309
executor thing and some configuration

00:25:42,539 --> 00:25:49,080
options in marathon so Aurora is the

00:25:47,309 --> 00:25:51,809
best example of a custom executor I

00:25:49,080 --> 00:25:56,340
think it's used it from the beginning

00:25:51,809 --> 00:26:00,210
but I'm not sure in our executor is

00:25:56,340 --> 00:26:03,700
called thermos and what is the similar

00:26:00,210 --> 00:26:07,000
thing that we are doing and Aurora does

00:26:03,700 --> 00:26:09,010
registering in zookeeper

00:26:07,000 --> 00:26:12,250
in a variety zookeeper in our case it's

00:26:09,010 --> 00:26:13,630
a console so executor takes care of

00:26:12,250 --> 00:26:16,360
registering in a service discovery

00:26:13,630 --> 00:26:20,380
perform self checking the same as our

00:26:16,360 --> 00:26:23,650
executor and we do not have some custom

00:26:20,380 --> 00:26:26,170
BSL like Aurora so the obviously does

00:26:23,650 --> 00:26:28,420
not have this feature and the second

00:26:26,170 --> 00:26:31,000
popular executor is a singularity

00:26:28,420 --> 00:26:34,480
executor it support custom Fator

00:26:31,000 --> 00:26:37,830
I don't know why did it need it but it

00:26:34,480 --> 00:26:39,790
uses some fancy as freedom loader

00:26:37,830 --> 00:26:44,140
probably something works better or

00:26:39,790 --> 00:26:47,559
faster it supports lock rotation so it

00:26:44,140 --> 00:26:50,500
helps you manage the logs and stuff like

00:26:47,559 --> 00:26:53,770
this and we support the graceful task

00:26:50,500 --> 00:26:56,530
beginning so two of the three factors

00:26:53,770 --> 00:26:59,590
from 12 factors up are covered by a

00:26:56,530 --> 00:27:03,330
singularity executor and the third I

00:26:59,590 --> 00:27:05,679
have heard about this executor yesterday

00:27:03,330 --> 00:27:09,309
so there are other companies that are

00:27:05,679 --> 00:27:14,080
investigating in fact I think this is

00:27:09,309 --> 00:27:17,760
OVH that are considering using custom

00:27:14,080 --> 00:27:21,040
executor and this Gomez's executor is

00:27:17,760 --> 00:27:23,740
similarly to our other executor built on

00:27:21,040 --> 00:27:27,820
ingo built on top of the Masters Goa

00:27:23,740 --> 00:27:31,270
library internally we are not using

00:27:27,820 --> 00:27:36,250
Dockers so much but this has support to

00:27:31,270 --> 00:27:40,600
a docker and has similar approach so I

00:27:36,250 --> 00:27:45,220
think you should start using executors

00:27:40,600 --> 00:27:47,530
just and gives a quick recap why we need

00:27:45,220 --> 00:27:50,020
it control over application and life

00:27:47,530 --> 00:27:52,780
cycle and environment without need to

00:27:50,020 --> 00:27:53,980
query scheduler muscles waiting for

00:27:52,780 --> 00:27:56,410
events and stuff like this

00:27:53,980 --> 00:27:59,860
you are master of your application and

00:27:56,410 --> 00:28:01,420
you can control how it behaves and when

00:27:59,860 --> 00:28:05,760
when it's healthy or not at well

00:28:01,420 --> 00:28:08,430
starting killing or anything

00:28:05,760 --> 00:28:10,710
so it's a replay nurse for framework

00:28:08,430 --> 00:28:13,320
event so this means your framework can

00:28:10,710 --> 00:28:16,380
do less because it will not have to send

00:28:13,320 --> 00:28:19,980
any vents and there is some space for

00:28:16,380 --> 00:28:21,200
workarounds and hugs as I will show you

00:28:19,980 --> 00:28:24,210
before that

00:28:21,200 --> 00:28:27,540
Nessus could have some box like this

00:28:24,210 --> 00:28:29,400
graceful shutdown or or not necessary

00:28:27,540 --> 00:28:31,610
box but could not works in a way that

00:28:29,400 --> 00:28:37,620
you expect and you can fix it with a

00:28:31,610 --> 00:28:40,790
message executor so that's all and I

00:28:37,620 --> 00:28:40,790
would be happy to take some questions

00:28:45,780 --> 00:28:51,349
okay no question so thank you

00:28:47,650 --> 00:28:51,349

YouTube URL: https://www.youtube.com/watch?v=Zm5RgED_1gM


