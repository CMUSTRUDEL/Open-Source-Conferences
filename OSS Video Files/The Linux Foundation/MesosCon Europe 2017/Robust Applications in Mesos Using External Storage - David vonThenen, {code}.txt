Title: Robust Applications in Mesos Using External Storage - David vonThenen, {code}
Publication date: 2017-10-27
Playlist: MesosCon Europe 2017
Description: 
	Robust Applications in Mesos Using External Storage - David vonThenen, {code}

Containers are starting to reach the masses and people are using them in ways other than what was originally intended. We now find persistent applications like SQL and NoSQL databases being run in container schedulers like Mesos, but how do we guarantee data availability for production applications in the wake of compute node failures? There are options for using direct attached or external storage, but the devil is in the details, as choices in storage types have significant repercussions.

We will discuss the benefits and challenges of using direct attached or external storage and how that impacts applications running in production environments. The trade-offs of each decision have interesting consequences starting from initial deployment to "day 2" operations and even how these applications tolerate system failures.

About David vonThenen
David vonThenen is an Open Source Engineer at {code}. The {code} team lives and breathes Open Source by making contributions to the community in a wide variety of projects ranging from Apache Mesos to storage orchestration platforms. Prior to joining {code}, David was a technical architect and development lead for a Backup/Recovery solution with a heavy focus in the virtualization space, VMware in particular.
Captions: 
	00:00:00,030 --> 00:00:05,339
good morning so for this first towards

00:00:03,419 --> 00:00:07,460
morning we're gonna have David from code

00:00:05,339 --> 00:00:09,719
I'm gonna let you introduce yourself

00:00:07,460 --> 00:00:11,790
just a quick reminder at the end of a

00:00:09,719 --> 00:00:13,710
session if you have question please let

00:00:11,790 --> 00:00:15,420
us know just the time to give you the

00:00:13,710 --> 00:00:17,670
mic because the session is recorded so

00:00:15,420 --> 00:00:20,310
it's gonna be good to have the other

00:00:17,670 --> 00:00:20,640
question or so recorded thank you go

00:00:20,310 --> 00:00:24,000
ahead

00:00:20,640 --> 00:00:25,560
so can guys hear me perfect yes all

00:00:24,000 --> 00:00:27,599
right cool awesome so robust

00:00:25,560 --> 00:00:30,210
applications and mazes using external

00:00:27,599 --> 00:00:32,669
storage so my name is David Vaughn tenon

00:00:30,210 --> 00:00:34,890
I work for the code team so the code

00:00:32,669 --> 00:00:36,690
team is a division of Dell technologies

00:00:34,890 --> 00:00:38,340
and it's basically the open source

00:00:36,690 --> 00:00:40,290
initiative within Dell so we do

00:00:38,340 --> 00:00:42,300
everything Dell it's a very large

00:00:40,290 --> 00:00:44,309
company they do a lot of things that are

00:00:42,300 --> 00:00:46,469
open source our group what we tend to

00:00:44,309 --> 00:00:48,500
focus on is container orchestrators and

00:00:46,469 --> 00:00:51,629
storage so our group focuses on

00:00:48,500 --> 00:00:54,320
kubernetes dr swarm and meso send DCOs

00:00:51,629 --> 00:00:58,020
which is why i'm here speaking today

00:00:54,320 --> 00:01:00,449
before working with meso sand DCOs

00:00:58,020 --> 00:01:03,000
in the past life I was working in the

00:01:00,449 --> 00:01:05,630
virtualization space so I was working in

00:01:03,000 --> 00:01:07,799
the backup recovery solution space and

00:01:05,630 --> 00:01:13,860
specifically working on VMware backup

00:01:07,799 --> 00:01:16,259
and recovery solutions so here's the

00:01:13,860 --> 00:01:18,420
agenda today so we're going to talk

00:01:16,259 --> 00:01:20,400
about a maze Oh storage options and

00:01:18,420 --> 00:01:22,799
what's available kind of past present

00:01:20,400 --> 00:01:25,229
and then even the future and then we're

00:01:22,799 --> 00:01:28,229
going to take a look at traditional

00:01:25,229 --> 00:01:30,750
databases and how the different storage

00:01:28,229 --> 00:01:32,549
options can affect your initial

00:01:30,750 --> 00:01:35,340
deployments of these applications and

00:01:32,549 --> 00:01:37,140
then also how they affect your day to

00:01:35,340 --> 00:01:39,780
operations and like disaster recovery

00:01:37,140 --> 00:01:41,820
and stuff like that and then we're also

00:01:39,780 --> 00:01:45,210
going to take a look at no sequel and

00:01:41,820 --> 00:01:46,350
key value store storage effectively

00:01:45,210 --> 00:01:48,659
doing the same thing we're gonna walk

00:01:46,350 --> 00:01:51,060
and see how different deployment

00:01:48,659 --> 00:01:53,070
strategies with storage effect and

00:01:51,060 --> 00:01:55,680
initial deployment and day 2 type stuff

00:01:53,070 --> 00:01:57,840
and then we'll also do just a brief

00:01:55,680 --> 00:01:59,880
wrap-up I kind of want to just highlight

00:01:57,840 --> 00:02:01,409
the important points about the

00:01:59,880 --> 00:02:02,790
differences of the storage options that

00:02:01,409 --> 00:02:04,890
are available and what kind of

00:02:02,790 --> 00:02:08,280
deployment strategies will kind of

00:02:04,890 --> 00:02:10,879
affect you know the lifecycle of your

00:02:08,280 --> 00:02:10,879
application

00:02:12,730 --> 00:02:20,030
so just jump right into it maze those

00:02:16,760 --> 00:02:22,280
storage options so I'm gonna structure

00:02:20,030 --> 00:02:24,799
this session a little differently so a

00:02:22,280 --> 00:02:27,980
lot of presentations you'll see will

00:02:24,799 --> 00:02:30,170
kind of like define the problem and then

00:02:27,980 --> 00:02:32,360
kind of build up to like what

00:02:30,170 --> 00:02:34,549
difficulties you know users and

00:02:32,360 --> 00:02:36,620
administrators have and then they kind

00:02:34,549 --> 00:02:37,970
of give you the solution at the end I'm

00:02:36,620 --> 00:02:40,610
kind of gonna switch it and do it in

00:02:37,970 --> 00:02:42,319
Reverse I'm going to give you the

00:02:40,610 --> 00:02:45,200
solutions for storage that are available

00:02:42,319 --> 00:02:46,160
there today and we're going to take and

00:02:45,200 --> 00:02:47,870
the reason why I'm going to do that is

00:02:46,160 --> 00:02:50,450
because we're gonna take a look at what

00:02:47,870 --> 00:02:53,239
decisions that we make in our storage

00:02:50,450 --> 00:02:54,739
options how they affect you know the

00:02:53,239 --> 00:02:56,840
life cycle of your application whether

00:02:54,739 --> 00:02:58,430
it's regards to maintenance disaster

00:02:56,840 --> 00:03:06,319
recovery and like all the day two

00:02:58,430 --> 00:03:08,750
related stuff for your app so if we look

00:03:06,319 --> 00:03:10,450
at the container ecosystem today there's

00:03:08,750 --> 00:03:12,409
a good mix of applications that are

00:03:10,450 --> 00:03:16,459
transient and then there's some that are

00:03:12,409 --> 00:03:17,900
long-running whether these applications

00:03:16,459 --> 00:03:20,180
are transient or long-running

00:03:17,900 --> 00:03:22,160
they generate data sometimes that data

00:03:20,180 --> 00:03:25,010
is useful and you want to persist that

00:03:22,160 --> 00:03:26,329
data and other times you know that data

00:03:25,010 --> 00:03:28,519
you can just throw it away and you don't

00:03:26,329 --> 00:03:30,620
really care and that that data can take

00:03:28,519 --> 00:03:33,079
in in shape many different shapes and

00:03:30,620 --> 00:03:34,790
then forms so there's user data so like

00:03:33,079 --> 00:03:36,829
if you're running a database you're

00:03:34,790 --> 00:03:37,700
obviously collecting data for your

00:03:36,829 --> 00:03:39,049
Postgres data

00:03:37,700 --> 00:03:41,510
there's also simples things like

00:03:39,049 --> 00:03:43,819
configuration right so maybe you have a

00:03:41,510 --> 00:03:45,709
particular application that I'm running

00:03:43,819 --> 00:03:48,349
in multiple places and but that

00:03:45,709 --> 00:03:50,750
configuration varies between instance to

00:03:48,349 --> 00:03:52,340
instance and but if you take a look at

00:03:50,750 --> 00:03:54,049
kind of containers today I have a look a

00:03:52,340 --> 00:03:56,780
little snapshot right here of docker hub

00:03:54,049 --> 00:03:59,840
and if you look at the top 20 apps that

00:03:56,780 --> 00:04:01,400
are in the docker store 10 of those are

00:03:59,840 --> 00:04:03,560
persistent applications so they are

00:04:01,400 --> 00:04:05,269
actually applications that you actually

00:04:03,560 --> 00:04:07,819
want to save that state or that data

00:04:05,269 --> 00:04:09,290
that's associated with that app so

00:04:07,819 --> 00:04:12,230
that's just as some examples right here

00:04:09,290 --> 00:04:16,519
you see Redis just looking my sequel

00:04:12,230 --> 00:04:19,849
 Postgres docker registry itself so

00:04:16,519 --> 00:04:21,919
there's a lot of yeah and there's a lot

00:04:19,849 --> 00:04:24,349
of applications that are out there that

00:04:21,919 --> 00:04:24,830
require persistence and it's becoming

00:04:24,349 --> 00:04:26,419
more

00:04:24,830 --> 00:04:28,819
prevalent in the container space today

00:04:26,419 --> 00:04:30,349
and you kind of ask yourself like why is

00:04:28,819 --> 00:04:33,199
that important why do we care that we

00:04:30,349 --> 00:04:37,750
can deploy stateful applications inside

00:04:33,199 --> 00:04:40,699
containers and the reason why is because

00:04:37,750 --> 00:04:43,370
containers have a lot of advantages for

00:04:40,699 --> 00:04:45,250
these stateful applications so if you

00:04:43,370 --> 00:04:48,740
take a look at what containers provide

00:04:45,250 --> 00:04:51,050
they provide a consistent mechanism so

00:04:48,740 --> 00:04:52,789
you can deploy that application with the

00:04:51,050 --> 00:04:54,800
same environment and same consistency

00:04:52,789 --> 00:04:58,520
everywhere no matter what form of

00:04:54,800 --> 00:05:02,090
compute that particular container lands

00:04:58,520 --> 00:05:03,440
on and also there's dependency

00:05:02,090 --> 00:05:06,110
management right so if you're going to

00:05:03,440 --> 00:05:08,120
use a container if you're gonna define a

00:05:06,110 --> 00:05:11,770
container and if it happens to be like a

00:05:08,120 --> 00:05:14,180
docker container that docker file

00:05:11,770 --> 00:05:16,009
outlines all the dependencies that that

00:05:14,180 --> 00:05:18,800
container that container that

00:05:16,009 --> 00:05:20,479
application needs so you're effectively

00:05:18,800 --> 00:05:22,789
defining all the packaging that's

00:05:20,479 --> 00:05:24,409
associated with your application and all

00:05:22,789 --> 00:05:28,340
that dependency gets rolled up within

00:05:24,409 --> 00:05:29,990
that container image and because you

00:05:28,340 --> 00:05:31,729
have all the dependencies wrapped up in

00:05:29,990 --> 00:05:33,979
a container and because you have that

00:05:31,729 --> 00:05:35,150
consistent environment everywhere if

00:05:33,979 --> 00:05:37,340
you're going to run whether it's

00:05:35,150 --> 00:05:38,750
stateful or stateless by having

00:05:37,340 --> 00:05:41,060
everything run within your container

00:05:38,750 --> 00:05:43,580
orchestrators like in maysa or DCOs

00:05:41,060 --> 00:05:45,560
the thing you're avoiding is you're

00:05:43,580 --> 00:05:48,319
avoiding snowflakes right so if you have

00:05:45,560 --> 00:05:50,419
all your stateless applications in May

00:05:48,319 --> 00:05:53,089
sauce or DCOs but you have your stateful

00:05:50,419 --> 00:05:55,069
like database or your no sequel running

00:05:53,089 --> 00:05:57,889
off to the side you're treating this set

00:05:55,069 --> 00:06:00,139
of hardware in this application like

00:05:57,889 --> 00:06:01,580
uniq right you're having the baby it

00:06:00,139 --> 00:06:03,169
you're having to maintain in a different

00:06:01,580 --> 00:06:05,120
way then you're treating your container

00:06:03,169 --> 00:06:07,460
environment but by having everything run

00:06:05,120 --> 00:06:08,870
together you're effectively treating all

00:06:07,460 --> 00:06:11,979
your applications whether the state

00:06:08,870 --> 00:06:14,000
lists are stateful in the same fashion

00:06:11,979 --> 00:06:16,069
so that's kind of like the Container

00:06:14,000 --> 00:06:18,409
attributes and how stateful applications

00:06:16,069 --> 00:06:20,389
can benefit from that now if you take a

00:06:18,409 --> 00:06:22,610
look at what container orchestrators can

00:06:20,389 --> 00:06:24,169
provide they provide things like health

00:06:22,610 --> 00:06:25,940
monitoring right so if you have your

00:06:24,169 --> 00:06:28,819
application whether it's like a simple

00:06:25,940 --> 00:06:30,589
rest service the container Orchestrator

00:06:28,819 --> 00:06:32,479
can provide health checks and they could

00:06:30,589 --> 00:06:34,729
be something as simple as doing an

00:06:32,479 --> 00:06:36,080
occasional ping of the REST API to make

00:06:34,729 --> 00:06:37,009
sure that the API is still up and

00:06:36,080 --> 00:06:39,580
running

00:06:37,009 --> 00:06:42,439
they also provide things like doing

00:06:39,580 --> 00:06:44,629
rollouts right if you want to roll out a

00:06:42,439 --> 00:06:46,189
new version of your application or if

00:06:44,629 --> 00:06:47,869
the rollout of your application didn't

00:06:46,189 --> 00:06:51,469
go so well and you need to rollback they

00:06:47,869 --> 00:06:52,580
also provide that functionality and then

00:06:51,469 --> 00:06:55,759
there's also a declarative configuration

00:06:52,580 --> 00:06:58,550
right so if you want to deploy a

00:06:55,759 --> 00:07:00,050
Postgres database that has external

00:06:58,550 --> 00:07:02,569
storage attached to it

00:07:00,050 --> 00:07:04,879
it's declarative in the sense that you

00:07:02,569 --> 00:07:06,770
just want to deploy it and what all the

00:07:04,879 --> 00:07:09,020
underlying details about the environment

00:07:06,770 --> 00:07:10,759
and the libraries that are associated

00:07:09,020 --> 00:07:12,949
with that application you really kind of

00:07:10,759 --> 00:07:14,479
don't care about that you trust that the

00:07:12,949 --> 00:07:16,909
container itself has all those

00:07:14,479 --> 00:07:19,129
dependencies baked into it and so when

00:07:16,909 --> 00:07:20,839
you want to deploy that that database or

00:07:19,129 --> 00:07:23,300
that no sequel the Cassandra instant

00:07:20,839 --> 00:07:25,550
Cassandra cluster you just really say I

00:07:23,300 --> 00:07:27,229
want to deploy Cassandra and you didn't

00:07:25,550 --> 00:07:30,050
the details are already taken care of

00:07:27,229 --> 00:07:32,479
you inside that container and that kind

00:07:30,050 --> 00:07:34,159
of lends to the next point here is like

00:07:32,479 --> 00:07:36,409
if you look at like DC OS right they

00:07:34,159 --> 00:07:38,809
have their curated app store it's

00:07:36,409 --> 00:07:40,580
effectively what it is you have a nice

00:07:38,809 --> 00:07:42,259
push button interface you go in there

00:07:40,580 --> 00:07:43,669
you click the application you want to

00:07:42,259 --> 00:07:45,949
deploy whether it's Postgres or

00:07:43,669 --> 00:07:47,779
Cassandra elasticsearch and it's really

00:07:45,949 --> 00:07:49,039
easy to roll out those applications and

00:07:47,779 --> 00:07:51,199
it's because of all of these things

00:07:49,039 --> 00:07:52,759
combined that you have that kind of

00:07:51,199 --> 00:07:59,719
experience in your container

00:07:52,759 --> 00:08:01,219
Orchestrator like in DC OS so we kind of

00:07:59,719 --> 00:08:02,539
are talking about containers and

00:08:01,219 --> 00:08:05,149
container orchestrators and all that

00:08:02,539 --> 00:08:06,319
stuff so we really have to take a look

00:08:05,149 --> 00:08:08,089
at the fundamental problems about

00:08:06,319 --> 00:08:10,129
containers right so and that's that the

00:08:08,089 --> 00:08:11,899
fact that they're ephemeral in nature so

00:08:10,129 --> 00:08:14,209
container comes up any data that

00:08:11,899 --> 00:08:16,189
collects gets collected and then when

00:08:14,209 --> 00:08:18,860
the container comes down the container

00:08:16,189 --> 00:08:20,419
and the data itself is gone but anybody

00:08:18,860 --> 00:08:22,039
that knows that if you're trying to run

00:08:20,419 --> 00:08:25,939
anything and put in like in a production

00:08:22,039 --> 00:08:29,419
environment if you're going to run a

00:08:25,939 --> 00:08:31,370
stateful application you're you need to

00:08:29,419 --> 00:08:32,930
have that state be available at all

00:08:31,370 --> 00:08:35,719
times and not only that but when you

00:08:32,930 --> 00:08:36,889
have failure on a particular node you

00:08:35,719 --> 00:08:40,519
want to make sure that when that

00:08:36,889 --> 00:08:42,139
container gets gets off basically moved

00:08:40,519 --> 00:08:44,600
around within your cluster that that

00:08:42,139 --> 00:08:46,939
data also follows that container so that

00:08:44,600 --> 00:08:49,610
kind of leads to the two options that

00:08:46,939 --> 00:08:50,810
are kind of available for data

00:08:49,610 --> 00:08:52,850
persistence so

00:08:50,810 --> 00:08:54,800
first option is you can use a local

00:08:52,850 --> 00:08:56,840
attached disc so basically direct

00:08:54,800 --> 00:08:58,850
attached storage to the hard disk that's

00:08:56,840 --> 00:09:01,010
connected directly to your computer and

00:08:58,850 --> 00:09:01,580
then the other option is external

00:09:01,010 --> 00:09:03,620
storage

00:09:01,580 --> 00:09:05,480
so that's storage that lives outside the

00:09:03,620 --> 00:09:09,430
compute that can be attached to that

00:09:05,480 --> 00:09:09,430
host and be mounted within that volume

00:09:11,440 --> 00:09:16,010
so I kind of want to just briefly touch

00:09:13,760 --> 00:09:17,750
upon the history a little bit and then

00:09:16,010 --> 00:09:19,550
we'll kind of start looking at what's

00:09:17,750 --> 00:09:23,150
available now and then what's available

00:09:19,550 --> 00:09:26,660
potentially in the future so the ability

00:09:23,150 --> 00:09:29,270
to carve out X amount of gigabytes on

00:09:26,660 --> 00:09:32,710
local disk on a given compute node was

00:09:29,270 --> 00:09:35,420
introduced in mazes zero to three and

00:09:32,710 --> 00:09:37,070
that just yeah basically if you wanted

00:09:35,420 --> 00:09:38,779
to persist some data for your

00:09:37,070 --> 00:09:41,420
application you could say I want you

00:09:38,779 --> 00:09:43,640
know 40 gigabytes of this 120 gigabyte

00:09:41,420 --> 00:09:46,070
disk and you'll basically carve that

00:09:43,640 --> 00:09:48,010
space out for it and then in September

00:09:46,070 --> 00:09:50,210
of 2015

00:09:48,010 --> 00:09:52,730
actually the code team had introduced

00:09:50,210 --> 00:09:55,550
the the maze Oh small DVD I which was a

00:09:52,730 --> 00:09:57,589
it's a third card third party component

00:09:55,550 --> 00:10:00,140
that you would manage outside of mezzos

00:09:57,589 --> 00:10:02,480
you'd install it alongside maze O's but

00:10:00,140 --> 00:10:04,360
this mazes module DVD I was it's

00:10:02,480 --> 00:10:07,310
basically a file system isolator

00:10:04,360 --> 00:10:10,820
implementation that called into existing

00:10:07,310 --> 00:10:13,250
docker volume driver interfaces and that

00:10:10,820 --> 00:10:15,320
it would effectively be able to give

00:10:13,250 --> 00:10:17,480
mezzos the ability to provision storage

00:10:15,320 --> 00:10:21,170
for anything running on the maze host

00:10:17,480 --> 00:10:22,940
Universal container Iser so and anything

00:10:21,170 --> 00:10:24,800
that was running on docker type workload

00:10:22,940 --> 00:10:26,630
a docker container Iser it would just

00:10:24,800 --> 00:10:30,320
effectively plug into the docker volume

00:10:26,630 --> 00:10:35,089
driver interface and then in mazes one

00:10:30,320 --> 00:10:37,640
oh that mazes module DVD I that actual

00:10:35,089 --> 00:10:39,650
that file system idles isolator as of

00:10:37,640 --> 00:10:42,500
one oh and higher was rican tributed

00:10:39,650 --> 00:10:44,780
back upstream into may sews and is now a

00:10:42,500 --> 00:10:47,360
part of mazes natively so that so that

00:10:44,780 --> 00:10:50,260
external volume support was is natively

00:10:47,360 --> 00:10:50,260
available as of one Oh

00:10:53,630 --> 00:10:58,130
so we already talked about this mate

00:10:55,940 --> 00:11:01,029
this maze OHS module DVD I which is a

00:10:58,130 --> 00:11:03,920
file system isolator that calls into

00:11:01,029 --> 00:11:05,899
docker volume driver implementations and

00:11:03,920 --> 00:11:08,149
so one of the things that our team the

00:11:05,899 --> 00:11:09,740
code team does is we have a docker

00:11:08,149 --> 00:11:12,050
volume driver implementation culture

00:11:09,740 --> 00:11:14,209
x-ray and what it is it's an agnostic

00:11:12,050 --> 00:11:17,839
storage or agnostic storage

00:11:14,209 --> 00:11:20,389
orchestration engine that abstracts

00:11:17,839 --> 00:11:22,100
back-end storage platforms and allows

00:11:20,389 --> 00:11:24,019
you to provision storage from those

00:11:22,100 --> 00:11:28,009
existing platforms so we support stuff

00:11:24,019 --> 00:11:29,509
like AWS GCE and that's for stuff like

00:11:28,009 --> 00:11:31,430
all basically all the local all the

00:11:29,509 --> 00:11:33,050
public cloud providers and then if

00:11:31,430 --> 00:11:35,899
you're looking to do stuff like on-prem

00:11:33,050 --> 00:11:38,029
we support SEF cinder and then there's a

00:11:35,899 --> 00:11:40,430
deli MC product called scale i/o which

00:11:38,029 --> 00:11:44,120
is a scale out completely software based

00:11:40,430 --> 00:11:45,470
storage solution in the second launches

00:11:44,120 --> 00:11:47,930
I've already talked about is mazes

00:11:45,470 --> 00:11:49,519
module DVD I write it allows you to so

00:11:47,930 --> 00:11:53,000
the first one rex ray provisions

00:11:49,519 --> 00:11:55,190
natively volumes and storage for docker

00:11:53,000 --> 00:11:57,769
workloads and then mais au s-- module

00:11:55,190 --> 00:11:59,569
DVD i does provision storage which calls

00:11:57,769 --> 00:12:03,019
into effectively calls in directs ray

00:11:59,569 --> 00:12:04,699
and provisions storage for the maizlish

00:12:03,019 --> 00:12:06,889
universal container right so the mazes

00:12:04,699 --> 00:12:08,930
container Iser and like i said it both

00:12:06,889 --> 00:12:10,370
of these are open source the second

00:12:08,930 --> 00:12:12,199
one's not really maintained as much

00:12:10,370 --> 00:12:14,000
anymore because it's already now in maze

00:12:12,199 --> 00:12:16,610
O's so the maintenance is happening

00:12:14,000 --> 00:12:18,230
there but both of our open source you

00:12:16,610 --> 00:12:20,240
can have they have the github links

00:12:18,230 --> 00:12:21,920
right there and yeah you can take a look

00:12:20,240 --> 00:12:29,600
at the repos and and all the fun stuff

00:12:21,920 --> 00:12:31,699
that that goes along with it so we've

00:12:29,600 --> 00:12:33,259
already talked mesas so I want to talk

00:12:31,699 --> 00:12:35,329
about a little bit about DC OS and the

00:12:33,259 --> 00:12:38,180
storage options that are out there so

00:12:35,329 --> 00:12:40,639
who here's running DC OS or has used it

00:12:38,180 --> 00:12:42,500
or anything like that ok cool and how

00:12:40,639 --> 00:12:45,019
many people of those people who have

00:12:42,500 --> 00:12:48,230
used it have used like deployed an

00:12:45,019 --> 00:12:51,860
application using external storage few

00:12:48,230 --> 00:12:54,019
people ok cool so the cool thing about

00:12:51,860 --> 00:12:56,360
DC OS is like I've already kind of said

00:12:54,019 --> 00:12:57,889
they they have this curated repository

00:12:56,360 --> 00:13:00,529
that allows you to easily provision

00:12:57,889 --> 00:13:02,300
applications and not only that but if

00:13:00,529 --> 00:13:04,610
those applications require some form of

00:13:02,300 --> 00:13:06,319
external storage there's a literally a

00:13:04,610 --> 00:13:07,250
simple little checkbox that you need to

00:13:06,319 --> 00:13:08,420
do

00:13:07,250 --> 00:13:09,920
how much storage you actually want to

00:13:08,420 --> 00:13:11,330
allocate and then where you want to

00:13:09,920 --> 00:13:13,160
mount that storage within the container

00:13:11,330 --> 00:13:15,430
and it's pretty easy to provision

00:13:13,160 --> 00:13:18,650
external external storage for your app

00:13:15,430 --> 00:13:20,540
now the cool thing is is that if you've

00:13:18,650 --> 00:13:23,090
used that feature that functionality

00:13:20,540 --> 00:13:24,770
before under the covers for DCOs what's

00:13:23,090 --> 00:13:26,420
actually running so there's actually for

00:13:24,770 --> 00:13:27,800
every DC OS node that's out there it's

00:13:26,420 --> 00:13:30,050
actually recs raised it actually

00:13:27,800 --> 00:13:32,090
installed on every node so without even

00:13:30,050 --> 00:13:33,890
realizing it you actually guys who have

00:13:32,090 --> 00:13:39,260
provisioned external storage for DCOs

00:13:33,890 --> 00:13:40,880
are actually recs great users so we've

00:13:39,260 --> 00:13:44,360
talked about May so's we've talked about

00:13:40,880 --> 00:13:47,390
DCOs storage options and now looking to

00:13:44,360 --> 00:13:48,950
the future so there's a initiative

00:13:47,390 --> 00:13:51,650
that's happening it's called the

00:13:48,950 --> 00:13:56,750
container storage interface or CSI and

00:13:51,650 --> 00:13:58,550
it's modeled after OCI and CNI so OCI is

00:13:56,750 --> 00:14:00,910
right the open container initiative and

00:13:58,550 --> 00:14:03,950
CNI is the container network interface

00:14:00,910 --> 00:14:05,450
the goal of CSI wall or at least one of

00:14:03,950 --> 00:14:07,160
the goals of the container storage

00:14:05,450 --> 00:14:09,680
interface is to standardize storage

00:14:07,160 --> 00:14:12,620
plug-ins across container orchestrators

00:14:09,680 --> 00:14:15,440
and so the idea is is that if you have a

00:14:12,620 --> 00:14:17,930
storage plug-in implementation that

00:14:15,440 --> 00:14:20,450
implements the CSI interface it's a spec

00:14:17,930 --> 00:14:24,170
right now and when that spec finally

00:14:20,450 --> 00:14:25,940
becomes hits the one point or 0.1

00:14:24,170 --> 00:14:27,680
release which it hopefully is sometime

00:14:25,940 --> 00:14:29,930
this month if it hasn't happened this

00:14:27,680 --> 00:14:32,240
week while we're here what if some

00:14:29,930 --> 00:14:34,670
plug-in implements that interface the

00:14:32,240 --> 00:14:37,400
idea is you could have mais au s-- and

00:14:34,670 --> 00:14:39,470
kubernetes implement the client portion

00:14:37,400 --> 00:14:42,950
of that call into these plugins and

00:14:39,470 --> 00:14:45,170
provision storage for your containers

00:14:42,950 --> 00:14:47,030
and the idea is that it's standardized

00:14:45,170 --> 00:14:48,500
and that it's basically you can swap

00:14:47,030 --> 00:14:50,780
them in and out with many different

00:14:48,500 --> 00:14:52,910
implementations if your guys are

00:14:50,780 --> 00:14:54,740
interested in that in that topic I only

00:14:52,910 --> 00:14:57,140
hinted at it but if you want a like a

00:14:54,740 --> 00:14:59,060
really good introduction and I think

00:14:57,140 --> 00:15:01,730
even deep dive of what the container

00:14:59,060 --> 00:15:04,520
storage interface is or the container

00:15:01,730 --> 00:15:07,640
storage interface is there's a session

00:15:04,520 --> 00:15:11,750
later on today at 4:30 up next door it's

00:15:07,640 --> 00:15:13,400
actually being done Co Co presented by

00:15:11,750 --> 00:15:15,740
one of my colleagues here up the front

00:15:13,400 --> 00:15:17,180
it's in Congress all - it's called the

00:15:15,740 --> 00:15:18,530
container storage initiative what's this

00:15:17,180 --> 00:15:22,000
project about and where are we going I

00:15:18,530 --> 00:15:22,000
highly recommend you check it out

00:15:26,520 --> 00:15:30,250
so now that we've kind of talked about

00:15:28,960 --> 00:15:32,890
the storage options that are available

00:15:30,250 --> 00:15:34,029
meso sandy cos I kind of want to take a

00:15:32,890 --> 00:15:36,610
look at the different deployment

00:15:34,029 --> 00:15:38,350
strategies for traditional databases and

00:15:36,610 --> 00:15:40,480
so that covers that what the initial

00:15:38,350 --> 00:15:42,190
deployment looks like and then also for

00:15:40,480 --> 00:15:49,660
day two's type stuff like disaster

00:15:42,190 --> 00:15:52,480
recovery and maintenance so traditional

00:15:49,660 --> 00:15:54,450
databases so when it comes to like

00:15:52,480 --> 00:15:56,470
traditional databases they tend to be

00:15:54,450 --> 00:15:58,360
simple and kind of straightforward and

00:15:56,470 --> 00:16:01,620
at least the majority of deployments for

00:15:58,360 --> 00:16:04,029
like Postgres maria DB and my sequel and

00:16:01,620 --> 00:16:05,560
because they're and because they're

00:16:04,029 --> 00:16:07,210
simple and straightforward they tend to

00:16:05,560 --> 00:16:10,060
be monolithic right it's a single

00:16:07,210 --> 00:16:11,740
instance with a single collection of

00:16:10,060 --> 00:16:13,029
databases but they're all tied to one

00:16:11,740 --> 00:16:15,370
instance that's living within a

00:16:13,029 --> 00:16:16,960
container now there are other types of

00:16:15,370 --> 00:16:19,060
deployments that are more complex so you

00:16:16,960 --> 00:16:20,560
can take like a my sequel database and

00:16:19,060 --> 00:16:23,320
you can do sharding on that or you can

00:16:20,560 --> 00:16:25,750
even cluster it I'm only gonna focus

00:16:23,320 --> 00:16:28,240
right now on the simple deployment it's

00:16:25,750 --> 00:16:30,339
just kind of basically to get a point

00:16:28,240 --> 00:16:31,690
across kind of a thing but if anybody

00:16:30,339 --> 00:16:32,830
who's interested in like sharding a

00:16:31,690 --> 00:16:35,770
clustering and more some of the more

00:16:32,830 --> 00:16:37,360
complex deployment cases i'm definitely

00:16:35,770 --> 00:16:44,920
hang around afterwards and we can talk

00:16:37,360 --> 00:16:46,720
more about that later so when you do an

00:16:44,920 --> 00:16:49,000
initial deployment of like Maria DB

00:16:46,720 --> 00:16:52,360
using local disk like I said it's simple

00:16:49,000 --> 00:16:53,620
and straightforward you basically were

00:16:52,360 --> 00:16:56,140
if you're going to deployed whether it's

00:16:53,620 --> 00:16:57,790
in DC OS or whether it's an ASIS if it's

00:16:56,140 --> 00:16:59,710
in D cos it's even easier you can just

00:16:57,790 --> 00:17:02,170
click on the App Store click add the

00:16:59,710 --> 00:17:06,220
external storage and then off it goes

00:17:02,170 --> 00:17:07,569
right you have a Maria DB instance now

00:17:06,220 --> 00:17:10,870
the interesting thing is if you want you

00:17:07,569 --> 00:17:12,670
deploy on local disk the performance is

00:17:10,870 --> 00:17:15,069
obviously based on the the compute nodes

00:17:12,670 --> 00:17:16,750
storage capabilities right so if you

00:17:15,069 --> 00:17:19,449
have a cluster that's all the same and

00:17:16,750 --> 00:17:21,670
if you only have like slow rotating disk

00:17:19,449 --> 00:17:22,959
on every node in your cluster you're

00:17:21,670 --> 00:17:24,520
obviously your performance of your

00:17:22,959 --> 00:17:26,890
database is going to be directly

00:17:24,520 --> 00:17:28,329
proportional to the slow rotating disk

00:17:26,890 --> 00:17:31,960
on that node right because if they're

00:17:28,329 --> 00:17:33,730
all the same now if you have like a

00:17:31,960 --> 00:17:34,360
heterogeneous environment where all your

00:17:33,730 --> 00:17:35,530
computing

00:17:34,360 --> 00:17:37,660
they're somewhat different maybe you

00:17:35,530 --> 00:17:39,820
have a section of compute that's only

00:17:37,660 --> 00:17:42,880
running slow rotating discs and another

00:17:39,820 --> 00:17:44,440
section that's running SSDs you have if

00:17:42,880 --> 00:17:46,059
you're going to do a deploy your initial

00:17:44,440 --> 00:17:47,890
deploy of your database you'll obviously

00:17:46,059 --> 00:17:49,450
have to be aware of what type of

00:17:47,890 --> 00:17:51,549
performance characteristic you want for

00:17:49,450 --> 00:17:53,020
your application right so if you don't

00:17:51,549 --> 00:17:55,690
really care about the performance maybe

00:17:53,020 --> 00:17:57,280
the slow rotating disk is fine but if

00:17:55,690 --> 00:17:58,450
you do care and you want something a

00:17:57,280 --> 00:18:01,240
little bit faster and you want to land

00:17:58,450 --> 00:18:03,940
that that container on SSD you have to

00:18:01,240 --> 00:18:06,429
do a targeted deploy of your container

00:18:03,940 --> 00:18:08,830
so you'd say I want maybe like the gold

00:18:06,429 --> 00:18:12,130
tier of storage versus like the silver

00:18:08,830 --> 00:18:13,900
or bronze tier what that means is now

00:18:12,130 --> 00:18:16,120
you have to be aware of like where those

00:18:13,900 --> 00:18:18,190
resources exist within your within your

00:18:16,120 --> 00:18:20,380
mezzos cluster and there are ways to

00:18:18,190 --> 00:18:23,470
handle that so you can basically tag

00:18:20,380 --> 00:18:25,360
like nodes saying this provides gold

00:18:23,470 --> 00:18:27,160
tier and this provides silver tier and

00:18:25,360 --> 00:18:28,809
this provides bronze tier and then when

00:18:27,160 --> 00:18:30,580
you do your deployment right you would

00:18:28,809 --> 00:18:32,290
deploy based on that teeth that tag

00:18:30,580 --> 00:18:39,309
right the tier of storage the class of

00:18:32,290 --> 00:18:41,500
storage you want so what does deploying

00:18:39,309 --> 00:18:43,960
a traditional type database like my

00:18:41,500 --> 00:18:45,760
sequel look like on external storage so

00:18:43,960 --> 00:18:49,299
obviously you need to have an external

00:18:45,760 --> 00:18:51,010
storage platform right and because you

00:18:49,299 --> 00:18:53,410
have an external storage platform there

00:18:51,010 --> 00:18:57,160
may be some specialized set up or

00:18:53,410 --> 00:18:59,679
configuration for that given platform so

00:18:57,160 --> 00:19:02,559
as an example right if you happen to be

00:18:59,679 --> 00:19:04,840
running an AWS most likely your storage

00:19:02,559 --> 00:19:07,090
platform is going to be EBS and in that

00:19:04,840 --> 00:19:08,770
case setup is trivial because it's given

00:19:07,090 --> 00:19:10,480
to you and because you've probably

00:19:08,770 --> 00:19:12,760
already been playing around with ec2

00:19:10,480 --> 00:19:14,290
instances there it's you already

00:19:12,760 --> 00:19:16,600
familiar with EBS and how it

00:19:14,290 --> 00:19:18,190
operationally works so the knowledge

00:19:16,600 --> 00:19:20,140
that's required to run that storage

00:19:18,190 --> 00:19:23,410
platform is minimized right it's almost

00:19:20,140 --> 00:19:26,610
trivial now on the flip side is if you

00:19:23,410 --> 00:19:29,830
have a external storage platform that's

00:19:26,610 --> 00:19:32,049
you know maybe you're gonna run a cinder

00:19:29,830 --> 00:19:33,490
implementation you have to know about

00:19:32,049 --> 00:19:35,049
cinder you have to know how to configure

00:19:33,490 --> 00:19:37,000
it you have to know how that cinder

00:19:35,049 --> 00:19:40,299
implementation interacts with the

00:19:37,000 --> 00:19:43,929
back-end storage right and you also have

00:19:40,299 --> 00:19:46,300
to think about the like the day to type

00:19:43,929 --> 00:19:47,470
stuff for that storage platform so what

00:19:46,300 --> 00:19:48,160
I'm talking about is the maintenance of

00:19:47,470 --> 00:19:50,710
that storage

00:19:48,160 --> 00:19:52,540
form in the case of EBS I guess you just

00:19:50,710 --> 00:19:53,470
trust that Amazon's gonna do the right

00:19:52,540 --> 00:19:56,560
thing and they're gonna take care of

00:19:53,470 --> 00:19:59,230
your storage and in the on-prem case

00:19:56,560 --> 00:20:00,850
like if you're running SEF and you have

00:19:59,230 --> 00:20:02,530
a store back-end storage platform that's

00:20:00,850 --> 00:20:04,030
associated with it and you have to be

00:20:02,530 --> 00:20:05,770
aware of what kind of maintenance

00:20:04,030 --> 00:20:07,390
operations you need to have on that

00:20:05,770 --> 00:20:08,800
whether it's a storage array or whether

00:20:07,390 --> 00:20:10,690
it's a completely software based

00:20:08,800 --> 00:20:12,670
solution you have to be aware about the

00:20:10,690 --> 00:20:15,010
the maintenance that's required to make

00:20:12,670 --> 00:20:16,960
that storage healthy at all in any given

00:20:15,010 --> 00:20:18,700
time so all that stuff is what I'm

00:20:16,960 --> 00:20:20,140
saying this is managed outside of May so

00:20:18,700 --> 00:20:21,940
that's something that you have to take

00:20:20,140 --> 00:20:25,210
care of based on the storage platform

00:20:21,940 --> 00:20:26,590
and just like rotating discs or local

00:20:25,210 --> 00:20:28,810
attached discs whether it's rotating

00:20:26,590 --> 00:20:30,640
discs or SSDs the performance is based

00:20:28,810 --> 00:20:34,150
on that platform and the same is true

00:20:30,640 --> 00:20:37,390
for external storage if you're like an

00:20:34,150 --> 00:20:38,890
AWS I believe well traditionally if

00:20:37,390 --> 00:20:40,750
you're an AWS and you're using EBS

00:20:38,890 --> 00:20:42,730
there's a like a one gig limit

00:20:40,750 --> 00:20:44,920
throughput on EBS volumes and you're

00:20:42,730 --> 00:20:47,920
you're basically stuck with that kind of

00:20:44,920 --> 00:20:49,630
performance characteristic and then the

00:20:47,920 --> 00:20:51,240
other thing too is if you're going to

00:20:49,630 --> 00:20:55,140
use an external storage platform

00:20:51,240 --> 00:20:57,640
minimally you have to be sure that a

00:20:55,140 --> 00:21:00,070
subset of your cluster nodes so your

00:20:57,640 --> 00:21:01,750
compute nodes have access to that

00:21:00,070 --> 00:21:04,150
storage platform that's what's the

00:21:01,750 --> 00:21:05,860
minimal case now if you want to make

00:21:04,150 --> 00:21:08,560
sure that your container can leave

00:21:05,860 --> 00:21:09,850
freely float between any node in the

00:21:08,560 --> 00:21:11,350
cluster and you compute node in the

00:21:09,850 --> 00:21:14,040
cluster you'd have to guarantee that

00:21:11,350 --> 00:21:16,090
that storage is accessible everywhere

00:21:14,040 --> 00:21:17,650
otherwise you'd incur the like a

00:21:16,090 --> 00:21:19,480
potential you know I'm gonna try to

00:21:17,650 --> 00:21:20,980
provision storage I don't have access

00:21:19,480 --> 00:21:27,790
I'm gonna be bounced to somewhere else

00:21:20,980 --> 00:21:32,050
right onto another piece of compute so

00:21:27,790 --> 00:21:33,700
that's the deploy right but and most a

00:21:32,050 --> 00:21:35,410
lot of presentations and sessions kind

00:21:33,700 --> 00:21:36,850
of stop there but I kind of want to look

00:21:35,410 --> 00:21:39,580
at the day to stuff and I think the day

00:21:36,850 --> 00:21:41,320
to stuff is more interesting and it's

00:21:39,580 --> 00:21:42,670
because things can go wrong right you

00:21:41,320 --> 00:21:44,710
have hardware failures you have

00:21:42,670 --> 00:21:46,480
maintenance that you need to do and so

00:21:44,710 --> 00:21:48,490
I'm calling like this the oh moment

00:21:46,480 --> 00:21:52,120
like when you realize that you have a

00:21:48,490 --> 00:21:53,590
problem and you need to fix it and the

00:21:52,120 --> 00:21:56,740
type of storage that you're going to

00:21:53,590 --> 00:21:58,450
pick can heavily influence like what

00:21:56,740 --> 00:22:01,770
difficulty you're going to have in your

00:21:58,450 --> 00:22:01,770
day to type operations

00:22:03,810 --> 00:22:11,860
so day two operations using local disk

00:22:08,190 --> 00:22:14,190
so if you're gonna use local disks like

00:22:11,860 --> 00:22:16,660
direct-attached storage on your compute

00:22:14,190 --> 00:22:20,170
the biggest problem is is that you have

00:22:16,660 --> 00:22:22,080
data locality right all your data for

00:22:20,170 --> 00:22:25,690
your given container your application

00:22:22,080 --> 00:22:27,400
exists on direct-attached storage so

00:22:25,690 --> 00:22:30,220
you're susceptible to things like disk

00:22:27,400 --> 00:22:32,020
failure host failure and those are the

00:22:30,220 --> 00:22:34,060
obvious things right now what happens if

00:22:32,020 --> 00:22:37,600
you have to perform maintenance on that

00:22:34,060 --> 00:22:40,150
particular host so that's not to say

00:22:37,600 --> 00:22:41,680
that your data is completely gone but if

00:22:40,150 --> 00:22:44,260
you have to take that host down for

00:22:41,680 --> 00:22:47,860
maintenance and you have to add more

00:22:44,260 --> 00:22:50,050
memory or upgrade the NIC on the thing

00:22:47,860 --> 00:22:52,990
to go from like single port to dual port

00:22:50,050 --> 00:22:54,550
your application for us however long it

00:22:52,990 --> 00:22:56,560
takes to perform that maintenance is

00:22:54,550 --> 00:22:59,410
going to be down for that given amount

00:22:56,560 --> 00:23:01,240
of time so you just need to be aware

00:22:59,410 --> 00:23:03,310
that local disk is going to tie you to

00:23:01,240 --> 00:23:06,460
that node and not only that but anytime

00:23:03,310 --> 00:23:09,090
you need to any time you have that

00:23:06,460 --> 00:23:11,080
application or that node go down you are

00:23:09,090 --> 00:23:13,780
essentially fixed to that node until

00:23:11,080 --> 00:23:15,310
that node comes up so in the case of

00:23:13,780 --> 00:23:17,020
hardware failure you have to have

00:23:15,310 --> 00:23:18,730
standby Hardware or whatever to bring

00:23:17,020 --> 00:23:20,920
that node back up as quickly as you can

00:23:18,730 --> 00:23:22,660
in order to preserve preserve that data

00:23:20,920 --> 00:23:24,820
now this is where right now we're

00:23:22,660 --> 00:23:26,170
focusing on traditional databases that

00:23:24,820 --> 00:23:28,330
are kind of monolithic it's gonna be a

00:23:26,170 --> 00:23:29,530
little different in the case for like no

00:23:28,330 --> 00:23:31,810
sequel and we'll touch upon that a

00:23:29,530 --> 00:23:33,100
little bit later and then the other

00:23:31,810 --> 00:23:36,730
thing that you kind of have to consider

00:23:33,100 --> 00:23:39,010
to write is that that host that node

00:23:36,730 --> 00:23:41,530
only has a fixed amount of disk space

00:23:39,010 --> 00:23:43,030
that's there right so it only has a

00:23:41,530 --> 00:23:45,930
limited number of disks that are in

00:23:43,030 --> 00:23:47,920
there and when you deploy your

00:23:45,930 --> 00:23:49,840
application like whether it's Maria DB

00:23:47,920 --> 00:23:52,960
Postgres you have to provision all of

00:23:49,840 --> 00:23:55,210
that storage up front and so the more

00:23:52,960 --> 00:23:57,100
storage you take if you have other

00:23:55,210 --> 00:23:59,770
applications that require some amount of

00:23:57,100 --> 00:24:02,860
storage you potentially might run into a

00:23:59,770 --> 00:24:05,950
thing a situation where you might not

00:24:02,860 --> 00:24:07,540
have enough storage capacity to run

00:24:05,950 --> 00:24:09,370
other containers on that node because

00:24:07,540 --> 00:24:12,070
you have to reserve all that capacity

00:24:09,370 --> 00:24:14,350
upfront and then the other thing is what

00:24:12,070 --> 00:24:14,650
happens when you start eating up like

00:24:14,350 --> 00:24:18,100
say

00:24:14,650 --> 00:24:20,200
you do deploy Postgres and you say I

00:24:18,100 --> 00:24:21,970
want all of the storage that's available

00:24:20,200 --> 00:24:23,530
on this host and just give it to me and

00:24:21,970 --> 00:24:25,660
it's effectively almost unless you don't

00:24:23,530 --> 00:24:27,370
care about persistent applications on

00:24:25,660 --> 00:24:29,590
that node you're only gonna be running

00:24:27,370 --> 00:24:32,830
transient like you know ephemeral

00:24:29,590 --> 00:24:34,600
containers on that node but and the only

00:24:32,830 --> 00:24:37,090
stateful app is going to be your

00:24:34,600 --> 00:24:39,340
Postgres instance but what happens even

00:24:37,090 --> 00:24:41,050
taking all of that storage what happens

00:24:39,340 --> 00:24:44,500
when you need more capacity beyond

00:24:41,050 --> 00:24:46,540
what's already there so you it's there

00:24:44,500 --> 00:24:48,100
are ways to get around that I mean you

00:24:46,540 --> 00:24:49,870
could have hot-swap more disk in there

00:24:48,100 --> 00:24:52,510
and stuff like that but now you it

00:24:49,870 --> 00:24:54,700
becomes a manual process a user has to

00:24:52,510 --> 00:24:58,690
get involved like a system administrator

00:24:54,700 --> 00:24:59,320
or whatnot and it is not trivial it's

00:24:58,690 --> 00:25:01,420
not easy

00:24:59,320 --> 00:25:03,040
there are ways around it but it's now it

00:25:01,420 --> 00:25:04,570
becomes more of a difficult problem that

00:25:03,040 --> 00:25:06,130
you may have to perform maintenance may

00:25:04,570 --> 00:25:10,300
have downtime for your application and

00:25:06,130 --> 00:25:12,910
and and whatnot and the kind of the idea

00:25:10,300 --> 00:25:14,230
is is by have if you do have a user that

00:25:12,910 --> 00:25:16,420
has to get involved like an

00:25:14,230 --> 00:25:18,430
administrator or whatnot now you're

00:25:16,420 --> 00:25:20,950
treating this particular node like a

00:25:18,430 --> 00:25:23,140
snowflake it's a special case thing

00:25:20,950 --> 00:25:25,360
right that I have my Postgres instance

00:25:23,140 --> 00:25:27,190
there I have my data sitting on these

00:25:25,360 --> 00:25:29,440
hard disks and I have to treat this

00:25:27,190 --> 00:25:33,120
service special just because we're

00:25:29,440 --> 00:25:33,120
running out of storage or disk space so

00:25:34,410 --> 00:25:40,990
now day 2 operations for external

00:25:37,270 --> 00:25:43,330
storage for traditional databases so the

00:25:40,990 --> 00:25:46,510
cool thing about external storage is

00:25:43,330 --> 00:25:48,790
that the volume will move with the

00:25:46,510 --> 00:25:51,760
container and you have to make sure that

00:25:48,790 --> 00:25:54,010
you have a storage Orchestrator

00:25:51,760 --> 00:25:56,410
something like R x-ray that can provide

00:25:54,010 --> 00:25:59,520
that functionality but wait if you ever

00:25:56,410 --> 00:26:02,470
have a hardware failure whether like the

00:25:59,520 --> 00:26:05,410
memory goes bad or this the motherboard

00:26:02,470 --> 00:26:07,060
on the server goes bad or that you know

00:26:05,410 --> 00:26:09,850
the disk itself well not the disk but

00:26:07,060 --> 00:26:11,650
the like the host itself whether it's

00:26:09,850 --> 00:26:13,960
like a power supply issue or whatever if

00:26:11,650 --> 00:26:15,850
you have those types of issues and that

00:26:13,960 --> 00:26:18,460
container needs to get rescheduled to

00:26:15,850 --> 00:26:20,200
another node like something like R x-ray

00:26:18,460 --> 00:26:22,930
like a storage orchestration engine can

00:26:20,200 --> 00:26:25,570
detach forcibly detach that volume and

00:26:22,930 --> 00:26:27,400
make it travel along with a container to

00:26:25,570 --> 00:26:28,330
whatever new piece of compute that it

00:26:27,400 --> 00:26:29,919
lands on

00:26:28,330 --> 00:26:31,720
and the same is true for maintenance if

00:26:29,919 --> 00:26:33,010
you need to take that host down that

00:26:31,720 --> 00:26:35,980
host becomes no longer becomes

00:26:33,010 --> 00:26:37,690
responsive and that like in the mazes

00:26:35,980 --> 00:26:39,820
case and the Maysles world and DCOs

00:26:37,690 --> 00:26:42,549
world your mazes master will realize

00:26:39,820 --> 00:26:44,080
that that agent is now offline and it

00:26:42,549 --> 00:26:45,279
will reschedule the container to move

00:26:44,080 --> 00:26:46,720
somewhere else and that volume will

00:26:45,279 --> 00:26:48,730
travel along with it so when that

00:26:46,720 --> 00:26:50,260
container comes back up that storage

00:26:48,730 --> 00:26:51,610
gets reattached to your Postgres

00:26:50,260 --> 00:26:54,760
instance and all your data is available

00:26:51,610 --> 00:26:56,230
and so basically what I'm describing

00:26:54,760 --> 00:26:59,110
here is I'm talking about high

00:26:56,230 --> 00:27:02,289
availability for containers right being

00:26:59,110 --> 00:27:04,600
able to have your application die come

00:27:02,289 --> 00:27:05,889
up somewhere else and then and also have

00:27:04,600 --> 00:27:08,200
all the storage and data that's

00:27:05,889 --> 00:27:10,299
available for your application and then

00:27:08,200 --> 00:27:12,340
the other kind of cool thing is because

00:27:10,299 --> 00:27:14,139
you're using external storage and

00:27:12,340 --> 00:27:17,350
depending on the the storage platform

00:27:14,139 --> 00:27:19,059
itself if if you are using external

00:27:17,350 --> 00:27:21,730
storage and the storage platform

00:27:19,059 --> 00:27:23,950
supports it you have the ability to grow

00:27:21,730 --> 00:27:25,809
beyond what the current capacity of your

00:27:23,950 --> 00:27:27,429
your storage platform is so if you're

00:27:25,809 --> 00:27:30,970
using like a traditional you know

00:27:27,429 --> 00:27:32,740
storage array of a big box you have the

00:27:30,970 --> 00:27:35,860
ability to add shelves with more disks

00:27:32,740 --> 00:27:38,740
expand that one or whatever and then do

00:27:35,860 --> 00:27:40,330
all that fun stuff right I used the old

00:27:38,740 --> 00:27:41,590
case but there are other platforms that

00:27:40,330 --> 00:27:42,880
are out there that are more suitable I

00:27:41,590 --> 00:27:44,500
already kind of alluded to one it's

00:27:42,880 --> 00:27:46,539
called scale i/o so that one's a

00:27:44,500 --> 00:27:48,580
completely software software based

00:27:46,539 --> 00:27:50,110
storage solution so it's basically you

00:27:48,580 --> 00:27:52,539
just install software on your computer

00:27:50,110 --> 00:27:54,880
and then if you want to expand the

00:27:52,539 --> 00:27:57,309
capacity you can just add more disks to

00:27:54,880 --> 00:27:59,049
that to that node to your scale aisle

00:27:57,309 --> 00:28:01,090
cluster and that will automatically

00:27:59,049 --> 00:28:03,760
expand the capacity for your storage

00:28:01,090 --> 00:28:07,080
array and you can expand your the

00:28:03,760 --> 00:28:07,080
storage for your given container

00:28:10,800 --> 00:28:16,060
so we talked about the the simple it's

00:28:14,470 --> 00:28:17,620
the simple use case right traditional

00:28:16,060 --> 00:28:20,800
databases that are kind of monolithic

00:28:17,620 --> 00:28:22,450
and that are standalone so now we're

00:28:20,800 --> 00:28:26,110
gonna take a look at no sequel and key

00:28:22,450 --> 00:28:28,510
value stores and see how your deployment

00:28:26,110 --> 00:28:30,490
strategies using storage affect how

00:28:28,510 --> 00:28:33,220
those day to operations get affected

00:28:30,490 --> 00:28:35,290
based on an initial deployment get based

00:28:33,220 --> 00:28:37,450
affected based on the the storage of

00:28:35,290 --> 00:28:45,850
weather using direct attached or

00:28:37,450 --> 00:28:48,040
external storage so it turns out for no

00:28:45,850 --> 00:28:49,450
sequel and like key value stores those

00:28:48,040 --> 00:28:52,510
types of applications whether it's like

00:28:49,450 --> 00:28:54,550
elastic search Redis MongoDB or

00:28:52,510 --> 00:28:58,090
Cassandra the initial deployments

00:28:54,550 --> 00:29:00,070
actually quite similar to a traditional

00:28:58,090 --> 00:29:02,230
database so if you're gonna use local

00:29:00,070 --> 00:29:03,700
disk you're going to just go ahead and

00:29:02,230 --> 00:29:05,230
you're gonna say if it's like happens to

00:29:03,700 --> 00:29:07,420
be Cassandra you're gonna deploy three

00:29:05,230 --> 00:29:09,220
nodes you're going to say I'm going to

00:29:07,420 --> 00:29:12,610
use local attach disk I'm gonna use this

00:29:09,220 --> 00:29:13,930
much space and your your like three

00:29:12,610 --> 00:29:15,610
Cassandra instances are gonna get

00:29:13,930 --> 00:29:17,740
deployed in your cluster and your mezzos

00:29:15,610 --> 00:29:20,650
cluster and off you're off and running

00:29:17,740 --> 00:29:23,500
and it happens to look exactly the same

00:29:20,650 --> 00:29:26,710
for external storage so in the external

00:29:23,500 --> 00:29:28,540
storage case you're going to create

00:29:26,710 --> 00:29:30,670
three instances of Cassandra you're

00:29:28,540 --> 00:29:31,870
gonna have three external volumes that

00:29:30,670 --> 00:29:34,780
are associated with each of the three

00:29:31,870 --> 00:29:37,030
instances of the Cassandra instances

00:29:34,780 --> 00:29:40,540
you're gonna launch your Cassandra

00:29:37,030 --> 00:29:41,980
cluster something like R x-ray will make

00:29:40,540 --> 00:29:44,290
sure that the storage gets attached to

00:29:41,980 --> 00:29:47,260
your container and then you're off and

00:29:44,290 --> 00:29:49,450
running but just and so because they're

00:29:47,260 --> 00:29:52,530
the same if you're using local disk

00:29:49,450 --> 00:29:54,730
you're still like constrained by the

00:29:52,530 --> 00:29:55,960
just like in the traditional database

00:29:54,730 --> 00:29:57,430
case you're still constrained by the

00:29:55,960 --> 00:29:59,860
performance that's available on the node

00:29:57,430 --> 00:30:01,270
you're still constrained by the size the

00:29:59,860 --> 00:30:03,340
amount of disk space that's available on

00:30:01,270 --> 00:30:05,620
the node and then the external storage

00:30:03,340 --> 00:30:07,510
case they're still constrained by having

00:30:05,620 --> 00:30:09,400
to have an external storage platform

00:30:07,510 --> 00:30:11,320
having to know the the details and the

00:30:09,400 --> 00:30:13,240
maintenance operations for that external

00:30:11,320 --> 00:30:15,310
storage platform and then you still have

00:30:13,240 --> 00:30:17,290
to worry about access among the various

00:30:15,310 --> 00:30:19,960
nodes of compute in in your mezzos

00:30:17,290 --> 00:30:22,179
cluster so all that basically stays the

00:30:19,960 --> 00:30:24,720
same there's some little differences but

00:30:22,179 --> 00:30:27,100
generally it's the same

00:30:24,720 --> 00:30:29,020
now the interesting stuff is what

00:30:27,100 --> 00:30:30,490
happens on day two like when you have

00:30:29,020 --> 00:30:33,490
maintenance when you have hardware

00:30:30,490 --> 00:30:35,710
failure when you have you know some or

00:30:33,490 --> 00:30:37,260
unforeseen like maintenance event or

00:30:35,710 --> 00:30:39,490
whatever that you have to take care of

00:30:37,260 --> 00:30:41,500
it gets interesting because it's the

00:30:39,490 --> 00:30:43,570
behavioral characteristics of these no

00:30:41,500 --> 00:30:45,520
sequel and these key values no sequel

00:30:43,570 --> 00:30:48,250
databases and these key value stores and

00:30:45,520 --> 00:30:50,770
it's basically because of the eventual

00:30:48,250 --> 00:30:52,510
consistent behavior that these no sequel

00:30:50,770 --> 00:30:53,770
and key value stores have right because

00:30:52,510 --> 00:30:56,289
they're all distributed and they're all

00:30:53,770 --> 00:30:58,450
multi node they're all making sure that

00:30:56,289 --> 00:30:59,919
the that within the Cassandra coaster

00:30:58,450 --> 00:31:01,270
they have data availability which means

00:30:59,919 --> 00:31:03,309
there's copies that are being sent to

00:31:01,270 --> 00:31:06,100
the different various nodes that are out

00:31:03,309 --> 00:31:08,679
there and so that's kind of what kind of

00:31:06,100 --> 00:31:10,750
makes sets these no sequel databases and

00:31:08,679 --> 00:31:19,539
key value stores different sets them

00:31:10,750 --> 00:31:21,610
apart from traditional databases so

00:31:19,539 --> 00:31:23,740
because those app those types of

00:31:21,610 --> 00:31:28,299
applications are somewhat more difficult

00:31:23,740 --> 00:31:30,070
to manage meso sand and DCOs in the form

00:31:28,299 --> 00:31:32,799
of their app store they have a really

00:31:30,070 --> 00:31:35,470
caught awesome concept that's really

00:31:32,799 --> 00:31:37,179
special to me so sand DCOs and that's

00:31:35,470 --> 00:31:39,700
the whole idea of frameworks right this

00:31:37,179 --> 00:31:44,710
two layer scheduling mechanism that they

00:31:39,700 --> 00:31:46,539
provide in order to make likes basically

00:31:44,710 --> 00:31:48,039
effectively specialization for your

00:31:46,539 --> 00:31:49,690
particular application so they can do

00:31:48,039 --> 00:31:51,610
things instead of like launching a

00:31:49,690 --> 00:31:54,070
generic application frameworks can

00:31:51,610 --> 00:31:55,570
actually tailor operational and

00:31:54,070 --> 00:31:57,429
behavioral characteristics for your

00:31:55,570 --> 00:31:58,809
applications so as an example like I

00:31:57,429 --> 00:32:00,340
actually have pictures of the

00:31:58,809 --> 00:32:01,900
elasticsearch framework this is what the

00:32:00,340 --> 00:32:03,909
UI for the elasticsearch framework looks

00:32:01,900 --> 00:32:06,460
like and when you deploy this framework

00:32:03,909 --> 00:32:08,320
what ends up happening is elastics this

00:32:06,460 --> 00:32:10,419
framework will actually go ahead and the

00:32:08,320 --> 00:32:12,010
minimum for right for eventual

00:32:10,419 --> 00:32:14,049
consistent databases is to have three

00:32:12,010 --> 00:32:17,530
nodes and the when you deploy the

00:32:14,049 --> 00:32:21,070
framework it'll find out what compute

00:32:17,530 --> 00:32:22,690
nodes best suit this particular instance

00:32:21,070 --> 00:32:25,030
of elasticsearch and it will

00:32:22,690 --> 00:32:30,100
automatically deploy elasticsearch three

00:32:25,030 --> 00:32:32,140
instances to those nodes and if you also

00:32:30,100 --> 00:32:33,340
take a look here like towards the the

00:32:32,140 --> 00:32:35,890
second screen that's a little behind

00:32:33,340 --> 00:32:38,110
there if you want to scale out

00:32:35,890 --> 00:32:40,240
elasticsearch so you want to go beyond

00:32:38,110 --> 00:32:42,700
three three instances that are there the

00:32:40,240 --> 00:32:45,040
framework has support for saying instead

00:32:42,700 --> 00:32:48,250
of three I want five right I want five

00:32:45,040 --> 00:32:51,970
instances so that I can tolerate failure

00:32:48,250 --> 00:32:54,190
better and so these frameworks go much

00:32:51,970 --> 00:32:55,809
beyond the initial deployment so a lot

00:32:54,190 --> 00:32:57,850
of the frameworks have the ability to do

00:32:55,809 --> 00:33:00,100
like I said scale and scale out and

00:32:57,850 --> 00:33:02,860
monitoring so like monitoring in the

00:33:00,100 --> 00:33:04,510
form of I recognize that this that this

00:33:02,860 --> 00:33:05,770
particular and it's up then maybe the

00:33:04,510 --> 00:33:07,600
container is actually healthy it's

00:33:05,770 --> 00:33:09,130
actually up and running but maybe the

00:33:07,600 --> 00:33:11,140
instant the application itself

00:33:09,130 --> 00:33:13,510
elasticsearch has kind of gone sideways

00:33:11,140 --> 00:33:16,419
and gone south and it's not behaving

00:33:13,510 --> 00:33:17,890
correctly the framework can recognize

00:33:16,419 --> 00:33:19,720
that right because it's application

00:33:17,890 --> 00:33:22,960
specific monitoring and it could bounce

00:33:19,720 --> 00:33:24,880
that node and move that that

00:33:22,960 --> 00:33:26,710
elasticsearch instance to a new node

00:33:24,880 --> 00:33:29,140
effectively starting up and resurrecting

00:33:26,710 --> 00:33:31,870
it from scratch and because it's going

00:33:29,140 --> 00:33:33,730
to have functionality to move like an

00:33:31,870 --> 00:33:35,470
elasticsearch instance from one node to

00:33:33,730 --> 00:33:37,630
the next it has the ability to do

00:33:35,470 --> 00:33:39,520
automated recovery and effectively

00:33:37,630 --> 00:33:41,799
bootstrapping and rebuilding that node

00:33:39,520 --> 00:33:46,380
to rebalance the data along your

00:33:41,799 --> 00:33:48,340
elasticsearch cluster right so

00:33:46,380 --> 00:33:50,620
frameworks are all great they're

00:33:48,340 --> 00:33:54,250
wonderful I love them I've implemented

00:33:50,620 --> 00:33:56,110
one myself now the interesting thing

00:33:54,250 --> 00:33:59,140
kind of like the elephant in the room is

00:33:56,110 --> 00:34:01,870
that all frameworks are great at

00:33:59,140 --> 00:34:03,429
deploying things like if you can't

00:34:01,870 --> 00:34:04,540
deploy your application then your

00:34:03,429 --> 00:34:07,179
framework kind of like doesn't work

00:34:04,540 --> 00:34:09,159
right so all good all frameworks are

00:34:07,179 --> 00:34:11,320
good at deploying applications some are

00:34:09,159 --> 00:34:13,470
even good at monitoring and a small

00:34:11,320 --> 00:34:15,940
fraction of those frameworks are

00:34:13,470 --> 00:34:18,340
actually able to handle like disaster

00:34:15,940 --> 00:34:19,629
recovery so when a node goes down I want

00:34:18,340 --> 00:34:21,629
to make sure I can recover from that

00:34:19,629 --> 00:34:26,230
situation so if we've gone going smaller

00:34:21,629 --> 00:34:28,230
initial deploy good at monitoring and

00:34:26,230 --> 00:34:31,149
then you've like a smaller subset is

00:34:28,230 --> 00:34:33,550
disaster recovery now the problem with

00:34:31,149 --> 00:34:34,810
disaster recovery is that you're what

00:34:33,550 --> 00:34:38,350
you really want to do is you want to do

00:34:34,810 --> 00:34:41,080
efficient disaster recovery right so if

00:34:38,350 --> 00:34:43,300
a node fails I want to make sure I want

00:34:41,080 --> 00:34:47,200
to handle that failure in that failure

00:34:43,300 --> 00:34:51,929
in the most efficient way as possible so

00:34:47,200 --> 00:34:54,420
I I'm a Star Trek geek sorry

00:34:51,929 --> 00:34:56,110
this is one of my favorite scenes from

00:34:54,420 --> 00:34:58,810
Star Trek

00:34:56,110 --> 00:35:01,750
yeah first oh sorry Star Trek

00:34:58,810 --> 00:35:03,790
generations and we're gonna take a look

00:35:01,750 --> 00:35:06,640
at what happens when we have a no sequel

00:35:03,790 --> 00:35:08,230
database or a key-value store like when

00:35:06,640 --> 00:35:09,550
we have our oh moment that's what

00:35:08,230 --> 00:35:11,380
he's actually saying he does say that in

00:35:09,550 --> 00:35:12,970
the movie like what happens in day 2

00:35:11,380 --> 00:35:14,440
operations and I have to have when I

00:35:12,970 --> 00:35:16,720
have a hardware failure or when I have

00:35:14,440 --> 00:35:18,370
to do maintenance on on my application

00:35:16,720 --> 00:35:20,830
or I have hardware failure on my note

00:35:18,370 --> 00:35:25,840
like what how do we handle those types

00:35:20,830 --> 00:35:28,990
of situations so if we take a look at

00:35:25,840 --> 00:35:31,000
what no sequel like no sequel databases

00:35:28,990 --> 00:35:32,620
and what key value stores what they look

00:35:31,000 --> 00:35:34,690
like when you're deploying on local disk

00:35:32,620 --> 00:35:36,580
and I'm gonna use Cassandra here as a

00:35:34,690 --> 00:35:40,540
specific example because it's just to

00:35:36,580 --> 00:35:41,380
give us a concrete example if you need

00:35:40,540 --> 00:35:43,450
to failover

00:35:41,380 --> 00:35:45,760
one of your three nodes of Cassandra

00:35:43,450 --> 00:35:49,660
using local disks to another piece of

00:35:45,760 --> 00:35:51,580
compute and you need to do a rebuild on

00:35:49,660 --> 00:35:55,510
Cassandra too basically restrike your

00:35:51,580 --> 00:35:57,550
data across Cassandra obviously the less

00:35:55,510 --> 00:36:00,760
data you have the faster that's going to

00:35:57,550 --> 00:36:02,980
be but like any other application if

00:36:00,760 --> 00:36:05,110
your application consumes and throws

00:36:02,980 --> 00:36:06,940
data into Cassandra the longer it's been

00:36:05,110 --> 00:36:09,430
running the more data you're going to

00:36:06,940 --> 00:36:11,500
accumulate and the more data you have if

00:36:09,430 --> 00:36:14,230
you have like a Cassandra in cluster

00:36:11,500 --> 00:36:16,330
that's very dense in data if you move

00:36:14,230 --> 00:36:19,240
that Cassandra instance from one node to

00:36:16,330 --> 00:36:22,810
the next that rebuild process can take a

00:36:19,240 --> 00:36:25,450
lot of time and it can it's so much time

00:36:22,810 --> 00:36:27,160
in fact it can take hours and it's so

00:36:25,450 --> 00:36:29,140
much time depending on how dense your

00:36:27,160 --> 00:36:31,900
Cassandra instance is that it can

00:36:29,140 --> 00:36:34,420
actually even take days and I'm not

00:36:31,900 --> 00:36:36,490
gonna go into all the operational stuff

00:36:34,420 --> 00:36:38,110
in the deep dive in Cassandra and and

00:36:36,490 --> 00:36:41,290
any of that stuff I'm just giving like a

00:36:38,110 --> 00:36:43,290
generalized overview of most sequel but

00:36:41,290 --> 00:36:45,760
if you're interested in more information

00:36:43,290 --> 00:36:49,270
I'm gonna butcher his name I can get the

00:36:45,760 --> 00:36:52,330
first part Alexandre de Jan offski so he

00:36:49,270 --> 00:36:54,400
he presented at Cassandra summit in 2016

00:36:52,330 --> 00:36:56,080
and he the session was entitled how to

00:36:54,400 --> 00:36:58,480
bootstrap and rebuild Cassandra

00:36:56,080 --> 00:37:00,340
obviously through the youtube link in

00:36:58,480 --> 00:37:02,590
here it's an hour-long presentation it's

00:37:00,340 --> 00:37:04,960
actually fantastic it kind of gives you

00:37:02,590 --> 00:37:06,940
a great look into

00:37:04,960 --> 00:37:08,800
how these no sequel databases like how

00:37:06,940 --> 00:37:11,740
they operationally work how you do

00:37:08,800 --> 00:37:13,660
disaster recovery how you do

00:37:11,740 --> 00:37:14,950
bootstrapping and rebuilding and you

00:37:13,660 --> 00:37:17,380
know he even references in his

00:37:14,950 --> 00:37:19,240
presentation that he jokingly referenced

00:37:17,380 --> 00:37:21,130
that it could take as long as 15 days

00:37:19,240 --> 00:37:23,680
but he actually had an example where

00:37:21,130 --> 00:37:25,900
that actually did happen so it could get

00:37:23,680 --> 00:37:28,800
quite frightening if your your

00:37:25,900 --> 00:37:31,030
application is crippled for 15 days and

00:37:28,800 --> 00:37:33,130
if you want the link you don't have to

00:37:31,030 --> 00:37:34,930
write down the little thing at the

00:37:33,130 --> 00:37:37,599
bottom you can just download the PDF on

00:37:34,930 --> 00:37:38,680
the mesas con agenda thing and download

00:37:37,599 --> 00:37:42,700
the presentation the link should be

00:37:38,680 --> 00:37:46,089
there so when you're actually doing this

00:37:42,700 --> 00:37:48,670
bootstrapping rebuild process the

00:37:46,089 --> 00:37:51,070
latency for Cassandra is increasing and

00:37:48,670 --> 00:37:54,849
the reason why is the repair process is

00:37:51,070 --> 00:37:56,710
expensive so it's expensive the fact

00:37:54,849 --> 00:37:58,060
that when you corn you trying to

00:37:56,710 --> 00:38:00,220
rebalance all the data within your

00:37:58,060 --> 00:38:02,320
Cassandra instance right you're moving

00:38:00,220 --> 00:38:04,300
data from one one node to the next so

00:38:02,320 --> 00:38:06,310
you're eating a lot of CPU you're eating

00:38:04,300 --> 00:38:07,510
a lot of i/o and the reason why that's

00:38:06,310 --> 00:38:09,280
happened is because your rebored

00:38:07,510 --> 00:38:10,810
rebuilding the Merkle trees that are

00:38:09,280 --> 00:38:12,820
within the cassandra on this new node

00:38:10,810 --> 00:38:15,760
right so you're basically restriping the

00:38:12,820 --> 00:38:18,960
data and building these Merkle trees in

00:38:15,760 --> 00:38:21,310
cassandra and what that translates to is

00:38:18,960 --> 00:38:24,280
because cassandra itself is kind of

00:38:21,310 --> 00:38:26,320
crippled your application which depends

00:38:24,280 --> 00:38:28,660
on Cassandra is going to start to slow

00:38:26,320 --> 00:38:35,710
down and it's gonna potentially even

00:38:28,660 --> 00:38:38,380
grind to a halt so it's very expensive

00:38:35,710 --> 00:38:41,950
to have a failure in these no sequel

00:38:38,380 --> 00:38:43,660
databases now granted that how you

00:38:41,950 --> 00:38:46,000
deploy your Cassandra cluster is

00:38:43,660 --> 00:38:49,240
important what your replication factor

00:38:46,000 --> 00:38:51,490
is is important but in any case you your

00:38:49,240 --> 00:38:53,140
your the actual overall performance of

00:38:51,490 --> 00:38:54,820
your Cassandra cluster can be view

00:38:53,140 --> 00:38:56,890
graded depending on how its configured

00:38:54,820 --> 00:38:59,349
and if you're not careful it can

00:38:56,890 --> 00:39:01,060
actually even bring your Cassandra

00:38:59,349 --> 00:39:02,410
cluster down so there's actually this in

00:39:01,060 --> 00:39:04,680
if you go take a look at the video

00:39:02,410 --> 00:39:06,940
there's this actually really cool

00:39:04,680 --> 00:39:09,339
section of the that his presentation

00:39:06,940 --> 00:39:11,140
where he talks about the actual part of

00:39:09,339 --> 00:39:13,270
doing the repair process and Constanta

00:39:11,140 --> 00:39:17,260
is so IO intensive that it actually

00:39:13,270 --> 00:39:18,820
brings itself down so yeah it's very so

00:39:17,260 --> 00:39:20,380
what I'm saying is losing local

00:39:18,820 --> 00:39:22,330
disk you have degraded performance and

00:39:20,380 --> 00:39:24,160
if you're not careful you can actually

00:39:22,330 --> 00:39:28,630
have your entire application be

00:39:24,160 --> 00:39:30,160
unresponsive and the other thing with

00:39:28,630 --> 00:39:32,650
local disk is you have a window of

00:39:30,160 --> 00:39:35,020
vulnerability right so if you have three

00:39:32,650 --> 00:39:37,870
a three node instance of Cassandra and

00:39:35,020 --> 00:39:39,730
one of your nodes goes out you have this

00:39:37,870 --> 00:39:41,700
window of vulnerability where however

00:39:39,730 --> 00:39:44,290
long it takes the bootstrap and rebuild

00:39:41,700 --> 00:39:46,300
that window if there's another failure

00:39:44,290 --> 00:39:48,460
on one of your other two nodes you could

00:39:46,300 --> 00:39:50,680
potentially be losing a lot of data and

00:39:48,460 --> 00:39:52,030
it like I said it depends on how what

00:39:50,680 --> 00:39:55,270
your deployment strategy looks like

00:39:52,030 --> 00:39:57,730
Cassandra what kind of replication

00:39:55,270 --> 00:40:01,800
factor you have but I kind of liken it

00:39:57,730 --> 00:40:04,060
to like you want to minimize risk and

00:40:01,800 --> 00:40:05,950
the last thing you would ever want to do

00:40:04,060 --> 00:40:07,840
is you don't want to run Windows you

00:40:05,950 --> 00:40:09,190
don't want to run Internet Explorer you

00:40:07,840 --> 00:40:11,860
don't if you don't have an anti-virus

00:40:09,190 --> 00:40:13,510
program and you don't have spyware your

00:40:11,860 --> 00:40:16,210
that's a pretty risky behavior right you

00:40:13,510 --> 00:40:18,430
don't want to do that so the idea is

00:40:16,210 --> 00:40:21,910
minimize risk that just make sure that

00:40:18,430 --> 00:40:24,610
you're covering your you know cya right

00:40:21,910 --> 00:40:26,530
so limit risk make sure that your data

00:40:24,610 --> 00:40:28,780
is always available and local disk

00:40:26,530 --> 00:40:31,270
because you're all your data is tied to

00:40:28,780 --> 00:40:32,500
a given node migrating over from one

00:40:31,270 --> 00:40:37,480
node to the next you're you're getting

00:40:32,500 --> 00:40:39,700
kurung this expensive rebuild time so we

00:40:37,480 --> 00:40:42,490
talked about local disks at local disks

00:40:39,700 --> 00:40:46,180
and how they have no sequel databases

00:40:42,490 --> 00:40:49,090
and key value stores are affected by

00:40:46,180 --> 00:40:51,820
like these day two type operations so

00:40:49,090 --> 00:40:53,940
this is how external storage can help so

00:40:51,820 --> 00:40:56,770
if you have a bad disk

00:40:53,940 --> 00:40:58,420
hire compute node fails even if you have

00:40:56,770 --> 00:41:01,270
something simple as a network partition

00:40:58,420 --> 00:41:03,040
event right the Cassandra instance no

00:41:01,270 --> 00:41:04,510
longer the Cocina instance on a given

00:41:03,040 --> 00:41:07,810
node no longer becomes responsive

00:41:04,510 --> 00:41:09,790
because something's going on the agent

00:41:07,810 --> 00:41:12,100
can't talk to the maizes master node

00:41:09,790 --> 00:41:14,320
that cassandra instance will get

00:41:12,100 --> 00:41:16,330
reschedule not somewhere else because

00:41:14,320 --> 00:41:18,520
we're talking external volumes that

00:41:16,330 --> 00:41:20,500
volume will travel along with the

00:41:18,520 --> 00:41:22,540
container so that cassandra instance

00:41:20,500 --> 00:41:24,310
when that Cassandra instance comes back

00:41:22,540 --> 00:41:27,750
up that in volume will get reattached to

00:41:24,310 --> 00:41:31,180
the node and effectively it'll basically

00:41:27,750 --> 00:41:34,729
continue on where it left off

00:41:31,180 --> 00:41:36,680
and yeah like I said if you have

00:41:34,729 --> 00:41:38,089
something like Rex Rey which can do that

00:41:36,680 --> 00:41:45,109
storage orchestration you'll have that

00:41:38,089 --> 00:41:48,049
functionality already built-in so if we

00:41:45,109 --> 00:41:49,519
do move one of the Cassandra instances

00:41:48,049 --> 00:41:52,759
from one note to the next and we are

00:41:49,519 --> 00:41:55,969
using external storage that operation

00:41:52,759 --> 00:41:57,410
itself to move that volume right take

00:41:55,969 --> 00:41:59,359
the container down bring the container

00:41:57,410 --> 00:42:02,059
up move the storage over attach the

00:41:59,359 --> 00:42:03,019
storage to the Cassandra instance that

00:42:02,059 --> 00:42:06,019
takes time

00:42:03,019 --> 00:42:08,119
right and but that small Delta of time

00:42:06,019 --> 00:42:11,690
that it took whether it's a minute or

00:42:08,119 --> 00:42:13,910
two minutes that in that - one or two

00:42:11,690 --> 00:42:15,259
minute time frame you're losing the the

00:42:13,910 --> 00:42:18,619
amount of date new data that's being

00:42:15,259 --> 00:42:21,380
funneled into Cassandra and the cool

00:42:18,619 --> 00:42:23,630
thing is because all that existing data

00:42:21,380 --> 00:42:26,089
is being moved over to a new compute

00:42:23,630 --> 00:42:27,950
with a new Cassandra instance when you

00:42:26,089 --> 00:42:30,469
do a rebuild and repair you're only

00:42:27,950 --> 00:42:33,769
talking about moving two minutes of data

00:42:30,469 --> 00:42:37,069
over from from the other healthy

00:42:33,769 --> 00:42:39,049
Cassandra nodes so when you do the

00:42:37,069 --> 00:42:41,329
repair got the healthy node on one side

00:42:39,049 --> 00:42:43,160
the middle kind of column represents the

00:42:41,329 --> 00:42:46,519
the Delta of data that you need to copy

00:42:43,160 --> 00:42:48,380
over to the the node that had failed and

00:42:46,519 --> 00:42:50,779
once that copy operations complete which

00:42:48,380 --> 00:42:53,839
is significantly less time especially in

00:42:50,779 --> 00:42:56,630
a data dense a Cassandra cluster will be

00:42:53,839 --> 00:42:59,509
insignificant compared to a full rebuild

00:42:56,630 --> 00:43:01,339
and if you think about something as

00:42:59,509 --> 00:43:02,900
simple as maybe a network partitioning

00:43:01,339 --> 00:43:04,339
event the last thing you would want to

00:43:02,900 --> 00:43:05,749
do in a network partition event which

00:43:04,339 --> 00:43:08,289
could be transient right maybe you have

00:43:05,749 --> 00:43:11,239
network connectivity problems that last

00:43:08,289 --> 00:43:13,190
10 to minutes or maybe 15 or even 30

00:43:11,239 --> 00:43:16,279
minutes the last thing you'd want to do

00:43:13,190 --> 00:43:18,200
is do a full rebuild of a Cassandra node

00:43:16,279 --> 00:43:20,299
start copying data all over the place

00:43:18,200 --> 00:43:23,029
only for that node to come back online

00:43:20,299 --> 00:43:25,160
like 15 minutes later especially when a

00:43:23,029 --> 00:43:32,539
repair operate a bootstrap and repair

00:43:25,160 --> 00:43:33,739
operation can take 15 days right so kind

00:43:32,539 --> 00:43:36,019
of brings me to the end of my

00:43:33,739 --> 00:43:37,279
presentation here I'm just gonna wrap

00:43:36,019 --> 00:43:40,430
this up really quickly so we can take

00:43:37,279 --> 00:43:42,680
questions so yeah using local storage

00:43:40,430 --> 00:43:44,320
you have an availability risk right you

00:43:42,680 --> 00:43:48,880
deploying application all your

00:43:44,320 --> 00:43:51,280
as effectively pins to that storage on

00:43:48,880 --> 00:43:52,780
your compute node your host goes down

00:43:51,280 --> 00:43:55,330
your data goes down along with it a

00:43:52,780 --> 00:43:57,460
scale limitation right you need more

00:43:55,330 --> 00:43:59,920
storage than the host has well I'm sorry

00:43:57,460 --> 00:44:02,440
you know it's you there are ways to get

00:43:59,920 --> 00:44:04,540
around that but as soon as you go beyond

00:44:02,440 --> 00:44:07,150
the the actual shelf capacity on that

00:44:04,540 --> 00:44:08,770
compute node then you're yeah you're

00:44:07,150 --> 00:44:10,090
gonna run into some problems right you

00:44:08,770 --> 00:44:12,370
have to figure out a way to migrate they

00:44:10,090 --> 00:44:14,620
migrate that data over to a new computed

00:44:12,370 --> 00:44:16,240
with a new instance and try to you know

00:44:14,620 --> 00:44:18,370
bootstrap your way to getting more

00:44:16,240 --> 00:44:20,200
storage effectively and then there's

00:44:18,370 --> 00:44:21,640
also performance characteristics I need

00:44:20,200 --> 00:44:25,030
to be need to be aware about so if you

00:44:21,640 --> 00:44:26,950
want fast you want higher performance

00:44:25,030 --> 00:44:31,300
and run on SSD if you don't really care

00:44:26,950 --> 00:44:33,100
you can run on a rotating disk but it is

00:44:31,300 --> 00:44:35,470
simple and it's relatively low cost

00:44:33,100 --> 00:44:38,080
right and it's very easy to get going on

00:44:35,470 --> 00:44:40,480
day one and if as long as you have

00:44:38,080 --> 00:44:42,280
enough capacity for what you need for

00:44:40,480 --> 00:44:43,030
your stateful application you know maybe

00:44:42,280 --> 00:44:47,110
that's good enough

00:44:43,030 --> 00:44:50,050
and then for external storage right if

00:44:47,110 --> 00:44:52,210
you have a stateful application that you

00:44:50,050 --> 00:44:54,040
need to have migrated from one node to

00:44:52,210 --> 00:44:56,860
the next because of hardware failure

00:44:54,040 --> 00:44:58,990
maintenance or you know a network

00:44:56,860 --> 00:45:00,910
partitioning event external storage can

00:44:58,990 --> 00:45:03,100
enable that for you like using something

00:45:00,910 --> 00:45:04,630
like recs right so container moves from

00:45:03,100 --> 00:45:07,270
one host to the next volume moves along

00:45:04,630 --> 00:45:09,250
with it and then you can do things take

00:45:07,270 --> 00:45:11,830
advantage of things like do thin

00:45:09,250 --> 00:45:13,330
provisioning you can add more shelves to

00:45:11,830 --> 00:45:15,190
your storage platform if you have like

00:45:13,330 --> 00:45:16,630
on-premise stuff if you're in the cloud

00:45:15,190 --> 00:45:19,120
maybe that's as simple as adding another

00:45:16,630 --> 00:45:20,560
EBS volume to whatever you need and then

00:45:19,120 --> 00:45:22,270
just be aware that the performance

00:45:20,560 --> 00:45:24,250
characteristics just like local attach

00:45:22,270 --> 00:45:30,000
disk is based on the storage platform

00:45:24,250 --> 00:45:34,530
itself and that is it thank you guys

00:45:30,000 --> 00:45:36,040
[Applause]

00:45:34,530 --> 00:45:45,280
questions

00:45:36,040 --> 00:45:48,220
I need a softball just like pigeon hey

00:45:45,280 --> 00:45:50,200
thanks a lot a great talk that external

00:45:48,220 --> 00:45:51,460
store it should that the data travels

00:45:50,200 --> 00:45:55,690
through the network right so there is a

00:45:51,460 --> 00:45:58,150
performance hit in terms of latency and

00:45:55,690 --> 00:45:59,950
bandwidth as well you have to do a

00:45:58,150 --> 00:46:01,510
capacity planning for the network to

00:45:59,950 --> 00:46:02,109
make sure that you can accommodate all

00:46:01,510 --> 00:46:05,710
this traffic

00:46:02,109 --> 00:46:07,089
but then if run Cassandra on external

00:46:05,710 --> 00:46:09,609
storage and we want to scale up the

00:46:07,089 --> 00:46:12,369
cluster the rebuild is will have to be

00:46:09,609 --> 00:46:14,470
done right so it will be even worse on

00:46:12,369 --> 00:46:16,630
the external storage because of all this

00:46:14,470 --> 00:46:19,450
performance impact comparing to the

00:46:16,630 --> 00:46:22,599
local disk well so if your so in the

00:46:19,450 --> 00:46:23,980
scale out case so there's some storage

00:46:22,599 --> 00:46:26,050
platforms that are out there that are

00:46:23,980 --> 00:46:27,520
actually very high performance Orage i

00:46:26,050 --> 00:46:29,859
would encourage you to take a look at

00:46:27,520 --> 00:46:32,230
scale i/o and that one's a completely

00:46:29,859 --> 00:46:34,329
software based storage platform and what

00:46:32,230 --> 00:46:35,710
happens is in like in scale i/o in it

00:46:34,329 --> 00:46:37,599
like I said it all depends on the

00:46:35,710 --> 00:46:39,520
storage platform itself right so if you

00:46:37,599 --> 00:46:42,250
have a very poor performance storage

00:46:39,520 --> 00:46:43,630
platform you're absolutely right but if

00:46:42,250 --> 00:46:47,260
you have something that's like scale i/o

00:46:43,630 --> 00:46:49,839
where it's a it's an elastic scale out

00:46:47,260 --> 00:46:51,040
software platform and the more nodes

00:46:49,839 --> 00:46:53,380
that you have that are available that

00:46:51,040 --> 00:46:56,050
are offering up storage when you

00:46:53,380 --> 00:46:57,670
actually pull data as a client from your

00:46:56,050 --> 00:46:59,890
storage platform it actually stripes

00:46:57,670 --> 00:47:01,390
data all at once from multiple nodes and

00:46:59,890 --> 00:47:03,190
so you actually gain your throughput

00:47:01,390 --> 00:47:05,380
because you have so many nodes that are

00:47:03,190 --> 00:47:07,569
scaled out but you're absolutely correct

00:47:05,380 --> 00:47:09,910
if depending on the storage platform you

00:47:07,569 --> 00:47:12,910
have if the performance if you have a

00:47:09,910 --> 00:47:15,040
bottleneck then scale out or are you

00:47:12,910 --> 00:47:18,099
actually adding like maybe fourth or

00:47:15,040 --> 00:47:19,540
fifth node can be very expensive but if

00:47:18,099 --> 00:47:21,280
you have something like scale IO that's

00:47:19,540 --> 00:47:23,650
providing the storage underneath you'll

00:47:21,280 --> 00:47:25,180
find that it's the performance is it's

00:47:23,650 --> 00:47:29,740
pretty pretty well matched

00:47:25,180 --> 00:47:32,710
okay and the follow-up can can someone

00:47:29,740 --> 00:47:35,380
run something like a scaly or SEF on

00:47:32,710 --> 00:47:38,369
meses itself using the local disk like

00:47:35,380 --> 00:47:41,109
coexist on the same workers you can

00:47:38,369 --> 00:47:43,420
actually there's actually if you want to

00:47:41,109 --> 00:47:44,560
take a look it's on our github page for

00:47:43,420 --> 00:47:47,020
my team I

00:47:44,560 --> 00:47:50,170
he wrote a scale i/o framework that I'll

00:47:47,020 --> 00:47:52,990
actually deploy scale i/o on an existing

00:47:50,170 --> 00:47:54,790
maize O's cluster so what it'll do is

00:47:52,990 --> 00:47:59,260
it'll bootstrap like the metadata nodes

00:47:54,790 --> 00:48:03,730
for doing the you know the data yeah and

00:47:59,260 --> 00:48:05,500
it'll also throw up the all the bits

00:48:03,730 --> 00:48:07,810
basically they have a scale i/o server

00:48:05,500 --> 00:48:09,070
which consumes local attached disk and

00:48:07,810 --> 00:48:10,930
then they have another component which

00:48:09,070 --> 00:48:13,720
is a client that you know allows you to

00:48:10,930 --> 00:48:15,970
connect and attached storage from scale

00:48:13,720 --> 00:48:17,650
i/o and if you want to have like a fully

00:48:15,970 --> 00:48:19,440
converged like the easiest examples if

00:48:17,650 --> 00:48:22,000
you want to have like a fully converged

00:48:19,440 --> 00:48:24,550
storage infrastructure like if you would

00:48:22,000 --> 00:48:27,400
have every node potentially every node

00:48:24,550 --> 00:48:29,200
of your every compute node have disk

00:48:27,400 --> 00:48:32,950
local tatts disk they would contribute

00:48:29,200 --> 00:48:34,480
those disks to scale i/o and what we do

00:48:32,950 --> 00:48:35,860
is take those individual discs make it

00:48:34,480 --> 00:48:38,170
look like one ubiquitous piece of

00:48:35,860 --> 00:48:39,730
storage and when you have a client that

00:48:38,170 --> 00:48:41,290
connects into it when it's actually

00:48:39,730 --> 00:48:42,970
doing reads from it it's actually

00:48:41,290 --> 00:48:44,500
reading from multiple places at once

00:48:42,970 --> 00:48:47,410
because that's striping it across all of

00:48:44,500 --> 00:48:50,710
your nodes that's pretty cool of the

00:48:47,410 --> 00:48:54,070
last one I'm just not aware of the CSI

00:48:50,710 --> 00:48:57,760
how this work goes is in that case if we

00:48:54,070 --> 00:49:00,430
run the storage platform on the same

00:48:57,760 --> 00:49:03,280
working outs is there a way to guarantee

00:49:00,430 --> 00:49:06,850
the data locality to the to place the

00:49:03,280 --> 00:49:09,220
container close maybe clothes on the

00:49:06,850 --> 00:49:12,400
rack may be closing the node to the

00:49:09,220 --> 00:49:14,710
actual disk space to avoid the network

00:49:12,400 --> 00:49:16,690
yeah so right yeah exactly so if you

00:49:14,710 --> 00:49:19,090
like I said depending on your storage

00:49:16,690 --> 00:49:21,850
platform if you if your storage platform

00:49:19,090 --> 00:49:23,620
is more performance by having you land

00:49:21,850 --> 00:49:25,270
closer to where you need to be to the

00:49:23,620 --> 00:49:27,520
source then you'd have to like do

00:49:25,270 --> 00:49:29,890
something like tag those hosts and say I

00:49:27,520 --> 00:49:31,600
want to land like on one of these given

00:49:29,890 --> 00:49:32,980
nodes because I know the performance of

00:49:31,600 --> 00:49:35,080
that storage platform is going to be

00:49:32,980 --> 00:49:38,650
better than you know then if I land over

00:49:35,080 --> 00:49:40,270
here and in the scale i/o case as an

00:49:38,650 --> 00:49:42,220
example right because you're striping

00:49:40,270 --> 00:49:44,140
data from all those you know all the

00:49:42,220 --> 00:49:46,210
effectively that's one ubiquitous pool

00:49:44,140 --> 00:49:48,460
all the disks that are on the computer

00:49:46,210 --> 00:49:49,960
being contributed to scale io the cool

00:49:48,460 --> 00:49:50,380
thing about it is no matter where you

00:49:49,960 --> 00:49:51,940
land

00:49:50,380 --> 00:49:54,070
since you're striping data from

00:49:51,940 --> 00:49:55,390
everywhere it's you don't really care

00:49:54,070 --> 00:49:57,690
where you land because it's all the same

00:49:55,390 --> 00:49:57,690
right

00:49:58,250 --> 00:50:03,750
hello yeah I said good Brenda my

00:50:01,260 --> 00:50:06,240
question is a in continuation to what

00:50:03,750 --> 00:50:09,840
you are answering if we have a Cassandra

00:50:06,240 --> 00:50:11,940
cluster on external storage is there any

00:50:09,840 --> 00:50:14,640
specific configuration in terms of read

00:50:11,940 --> 00:50:17,370
and write for the Cassandra setup which

00:50:14,640 --> 00:50:20,040
is required to make it successful yeah

00:50:17,370 --> 00:50:22,290
so that's what I was saying depending on

00:50:20,040 --> 00:50:23,550
how many instances Cassandra instances

00:50:22,290 --> 00:50:24,690
you have how many nodes or Cassander you

00:50:23,550 --> 00:50:27,300
have and depending on what your app

00:50:24,690 --> 00:50:28,890
replication factor is you can help

00:50:27,300 --> 00:50:31,710
mitigate some of these challenges that

00:50:28,890 --> 00:50:33,900
are associated with that but like I said

00:50:31,710 --> 00:50:36,480
it just all depends on what kind I'm not

00:50:33,900 --> 00:50:38,100
saying external storage or local attach

00:50:36,480 --> 00:50:40,710
disk or anything's better than the other

00:50:38,100 --> 00:50:42,480
kind of the focus of the talk is is when

00:50:40,710 --> 00:50:44,490
you pick whatever you're going to pick

00:50:42,480 --> 00:50:46,230
just make sure that you architect your

00:50:44,490 --> 00:50:47,880
deployment not only for initial

00:50:46,230 --> 00:50:50,430
deployment but also for DES to type

00:50:47,880 --> 00:50:52,770
stuff such that if you want to run local

00:50:50,430 --> 00:50:55,830
using local disks just make sure that

00:50:52,770 --> 00:50:57,120
you have n number of nodes to tolerate

00:50:55,830 --> 00:50:58,950
failure and make sure whatever

00:50:57,120 --> 00:51:02,190
replication factor you're gonna set like

00:50:58,950 --> 00:51:06,390
compliments that you know that your

00:51:02,190 --> 00:51:08,340
deployment right okay so specifically if

00:51:06,390 --> 00:51:12,240
I want to look for Cassandra as a

00:51:08,340 --> 00:51:14,220
persistence solution this solution will

00:51:12,240 --> 00:51:16,500
work fine external story with few of the

00:51:14,220 --> 00:51:18,300
good practices you just highlighted in

00:51:16,500 --> 00:51:20,040
continuation to that direction yeah and

00:51:18,300 --> 00:51:21,180
if you want we can like we can talk a

00:51:20,040 --> 00:51:24,180
little bit after this because I think

00:51:21,180 --> 00:51:25,830
we're over now I believe but we can keep

00:51:24,180 --> 00:51:27,090
going to come back up here we can talk

00:51:25,830 --> 00:51:28,140
about it and talk about how different

00:51:27,090 --> 00:51:30,510
storage platforms

00:51:28,140 --> 00:51:31,980
you know lend itself to like running

00:51:30,510 --> 00:51:35,700
workloads like Cassandra and stuff like

00:51:31,980 --> 00:51:38,520
that so alright I think we're out of

00:51:35,700 --> 00:51:40,100
time it's cool thank you very much thank

00:51:38,520 --> 00:51:42,460
you guys

00:51:40,100 --> 00:51:42,460

YouTube URL: https://www.youtube.com/watch?v=HbgBQ_TsPqs


