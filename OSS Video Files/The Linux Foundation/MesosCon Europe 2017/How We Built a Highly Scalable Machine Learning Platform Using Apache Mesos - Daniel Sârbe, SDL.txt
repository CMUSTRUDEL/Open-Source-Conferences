Title: How We Built a Highly Scalable Machine Learning Platform Using Apache Mesos - Daniel Sârbe, SDL
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	How We Built a Highly Scalable Machine Learning Platform Using Apache Mesos - Daniel Sârbe, SDL

Is there a way to combine new architectural patterns such as micro-services with Big Data technologies and run everything in Mesos?

In this talk I will present a novel, highly scalable Machine Learning platform for our Machine Translation use-cases.

I will explain how, in order to reach this goal, we have combined a wide variety of Big Data technologies(like Kafka, HBase, Hadoop), and I will discuss the challenges that we have faced along the way. I will also present how we adopted a containerized micro-services architecture(based on Mesos, Docker, Zookeeper) in order deploy our highly scalable Machine Learning platform.

About Daniel Sârbe
Daniel is leading the Big Data and Cloud Machine Translation group at SDL and in the last two years he was involved in building a highly scalable Machine Learning platform using some technologies from the BigData ecosystem like Kafka, HBase, Hadoop HDFS, ELK, Mesos in combination with new architectural patterns such as micro-services. Previously he worked on a Hadoop-based data pipeline that processes Petabytes of unstructured data; the resulting clean data is used to increase the quality of SDL’s Statistical Machine Translation engines. Daniel is also involved in the local Big Data Cluj Romania community as a meet-up organizer/speaker in the effort to raise the awareness of the Big Data, Hadoop, Spark, Machine Learning fields.
Captions: 
	00:00:00,299 --> 00:00:07,080
okay welcome everybody I'm quite excited

00:00:04,680 --> 00:00:10,530
to be here I hope you enjoyed the the

00:00:07,080 --> 00:00:14,519
conference till now I would like to

00:00:10,530 --> 00:00:17,279
start with a couple of questions so how

00:00:14,519 --> 00:00:18,000
many of you use mesos for more than two

00:00:17,279 --> 00:00:22,410
three years

00:00:18,000 --> 00:00:25,220
please raise your hand okay and how many

00:00:22,410 --> 00:00:30,960
of you use messes in production

00:00:25,220 --> 00:00:32,850
okay quite a lot that's great okay so

00:00:30,960 --> 00:00:36,149
the agenda for first today's

00:00:32,850 --> 00:00:39,770
presentation I will start with a little

00:00:36,149 --> 00:00:42,600
bit of machine translation context I

00:00:39,770 --> 00:00:45,450
will also answer to the question why we

00:00:42,600 --> 00:00:47,309
needed to build a new platform I will

00:00:45,450 --> 00:00:51,750
present our high tech chure overview of

00:00:47,309 --> 00:00:55,230
our current SAS solution and then I will

00:00:51,750 --> 00:00:57,239
enter into more details about the new

00:00:55,230 --> 00:00:59,850
micro services platform that we built

00:00:57,239 --> 00:01:02,370
and an important part of this

00:00:59,850 --> 00:01:05,159
presentation will be about the lesson

00:01:02,370 --> 00:01:09,330
learned while building this new platform

00:01:05,159 --> 00:01:11,430
I I also have a demo in which I will

00:01:09,330 --> 00:01:13,710
show how we scale our micro services in

00:01:11,430 --> 00:01:17,580
order to handle better more more

00:01:13,710 --> 00:01:20,970
translational requests okay just a few

00:01:17,580 --> 00:01:25,049
words about me I have a soft development

00:01:20,970 --> 00:01:27,360
background I'm passionate about people

00:01:25,049 --> 00:01:29,700
and technology I'm interested in

00:01:27,360 --> 00:01:32,640
anything that is related to scalability

00:01:29,700 --> 00:01:34,530
big data and machine learning and I'm

00:01:32,640 --> 00:01:37,500
currently leading the big data and

00:01:34,530 --> 00:01:40,829
machine translation group in fourth or

00:01:37,500 --> 00:01:42,990
SDL in Cluj Romania and I'm also the

00:01:40,829 --> 00:01:46,070
co-founder of the big data data science

00:01:42,990 --> 00:01:46,070
meetup closure

00:01:46,320 --> 00:01:51,329
SDL is activating in the translation

00:01:48,899 --> 00:01:54,030
industry being a large-scale machine

00:01:51,329 --> 00:01:55,979
translation provider but by large-scale

00:01:54,030 --> 00:01:59,070
machine translation provider I mean that

00:01:55,979 --> 00:02:03,299
we translate over 15 billion words every

00:01:59,070 --> 00:02:05,399
month probably you are familiar with

00:02:03,299 --> 00:02:09,580
with Google Translate we do the same

00:02:05,399 --> 00:02:11,380
thing but for enterprise company

00:02:09,580 --> 00:02:14,400
the large-scale machine translation

00:02:11,380 --> 00:02:16,870
providers over the course of the last

00:02:14,400 --> 00:02:20,890
years improved the quality of their

00:02:16,870 --> 00:02:23,250
services they did this boat by by

00:02:20,890 --> 00:02:26,590
improving them their machine learning

00:02:23,250 --> 00:02:28,960
algorithm I taking advantage of the

00:02:26,590 --> 00:02:30,580
technological progress being able to

00:02:28,960 --> 00:02:35,770
process more data store more data

00:02:30,580 --> 00:02:38,740
compute more in width with less money

00:02:35,770 --> 00:02:41,200
what made on the other hand what made

00:02:38,740 --> 00:02:43,240
really useful

00:02:41,200 --> 00:02:46,360
what made machine translation really

00:02:43,240 --> 00:02:48,730
useful in practice our customization by

00:02:46,360 --> 00:02:52,060
customization I mean the practice of

00:02:48,730 --> 00:02:55,630
training the statistical engines with

00:02:52,060 --> 00:02:59,790
data that is really close to the clients

00:02:55,630 --> 00:03:03,670
data or very similar to the clients data

00:02:59,790 --> 00:03:06,450
and this customized danger handle in

00:03:03,670 --> 00:03:10,540
average translation

00:03:06,450 --> 00:03:16,450
handle terminology with 24% better

00:03:10,540 --> 00:03:18,370
better quality on the other hand this

00:03:16,450 --> 00:03:20,830
that the training process is quite an

00:03:18,370 --> 00:03:24,870
expensive process it's it's an expensive

00:03:20,830 --> 00:03:28,000
process because of the time I mean if

00:03:24,870 --> 00:03:30,580
training an engine from scratch five

00:03:28,000 --> 00:03:34,450
years ago was taking days or even weeks

00:03:30,580 --> 00:03:37,720
now it takes six or eight hours which is

00:03:34,450 --> 00:03:39,850
quite a lot from hardware resources

00:03:37,720 --> 00:03:41,950
perspective you need a lot of CPU RAM

00:03:39,850 --> 00:03:45,760
and disk in order to train these engines

00:03:41,950 --> 00:03:48,250
and you also need from the data

00:03:45,760 --> 00:03:50,740
perspective you also need a lot of clean

00:03:48,250 --> 00:03:53,140
data both monolingual data and both

00:03:50,740 --> 00:03:55,540
bilingual data so we develop a new

00:03:53,140 --> 00:03:57,160
technology in order to address this

00:03:55,540 --> 00:04:00,640
problem the technology is called

00:03:57,160 --> 00:04:03,070
adaptive machine translation in the

00:04:00,640 --> 00:04:05,739
normal machine translation flow the

00:04:03,070 --> 00:04:09,100
engines are built by a statistical

00:04:05,739 --> 00:04:11,380
training process and after that if you

00:04:09,100 --> 00:04:13,300
translate the same same sentence 100

00:04:11,380 --> 00:04:15,580
times you will get 100 times the same

00:04:13,300 --> 00:04:18,790
answers the same answer on the other

00:04:15,580 --> 00:04:22,090
hand adaptive machine translation

00:04:18,790 --> 00:04:23,230
engines are able to to learn also after

00:04:22,090 --> 00:04:24,820
they

00:04:23,230 --> 00:04:27,610
they were trained so they learn in the

00:04:24,820 --> 00:04:29,440
same way in a similar way during the the

00:04:27,610 --> 00:04:32,290
training process but they are able to

00:04:29,440 --> 00:04:34,990
incorporate also feedback from the users

00:04:32,290 --> 00:04:37,000
after they were trained and they are

00:04:34,990 --> 00:04:40,360
getting better and better over time

00:04:37,000 --> 00:04:42,490
and let's take a concrete example so if

00:04:40,360 --> 00:04:45,340
we have a persona like a translator that

00:04:42,490 --> 00:04:47,890
sends a translation request - - to a

00:04:45,340 --> 00:04:52,990
machine translation engine he will get

00:04:47,890 --> 00:04:56,500
back an empty output he looks at this

00:04:52,990 --> 00:04:59,140
output and he decided that he has not he

00:04:56,500 --> 00:05:01,780
has a better way of translating that

00:04:59,140 --> 00:05:03,670
sentence so he actually modifies that

00:05:01,780 --> 00:05:06,070
translation that translation goes back

00:05:03,670 --> 00:05:08,830
into the adaptive machine translation

00:05:06,070 --> 00:05:11,260
engine that engine extracts the some

00:05:08,830 --> 00:05:13,710
rules that he will actually apply when

00:05:11,260 --> 00:05:17,500
the same users come with a similar

00:05:13,710 --> 00:05:19,810
translation input so it will get better

00:05:17,500 --> 00:05:25,990
and better over time for that specific

00:05:19,810 --> 00:05:28,510
user if we look also even to a sentence

00:05:25,990 --> 00:05:30,760
for for example an English to French

00:05:28,510 --> 00:05:33,010
sentence if we want to translate no

00:05:30,760 --> 00:05:35,830
further requirements are needed the

00:05:33,010 --> 00:05:39,250
machine translation will output poetry

00:05:35,830 --> 00:05:40,990
exigence own requisite and here the

00:05:39,250 --> 00:05:43,690
translator decide that he has a better

00:05:40,990 --> 00:05:46,420
translation oqx is on supplemental

00:05:43,690 --> 00:05:48,460
nécessaire so let's see what the

00:05:46,420 --> 00:05:50,770
adaptive engine actually learned from

00:05:48,460 --> 00:05:53,020
this so he learned some new rules like

00:05:50,770 --> 00:05:55,090
the fact that need it is probably in

00:05:53,020 --> 00:05:58,150
this context is bad that is better -

00:05:55,090 --> 00:06:00,640
translated as necessary further and

00:05:58,150 --> 00:06:03,310
supplemented and no further requirements

00:06:00,640 --> 00:06:05,110
are with oaken oxygen supplemental now

00:06:03,310 --> 00:06:08,380
he could also learn some bad

00:06:05,110 --> 00:06:10,840
translations but he actually compares

00:06:08,380 --> 00:06:12,850
these rules that he extracted from the

00:06:10,840 --> 00:06:15,160
text from the sentences that we gave

00:06:12,850 --> 00:06:17,410
them with the existing translation model

00:06:15,160 --> 00:06:19,750
that he has and he decides that it's

00:06:17,410 --> 00:06:21,790
better to not learn them because it will

00:06:19,750 --> 00:06:23,590
actually lower the quality of the

00:06:21,790 --> 00:06:27,960
translation so he he does not learn

00:06:23,590 --> 00:06:31,000
those one and before we actually

00:06:27,960 --> 00:06:35,710
implemented these several experiments

00:06:31,000 --> 00:06:38,170
were were done and we we with human with

00:06:35,710 --> 00:06:40,600
human help and we ended up with the

00:06:38,170 --> 00:06:42,970
conclusion that adaptive empty lowers

00:06:40,600 --> 00:06:47,440
the time of most editing with at least

00:06:42,970 --> 00:06:49,540
seven percent in average okay so this

00:06:47,440 --> 00:06:51,490
this was the first use adaptive machine

00:06:49,540 --> 00:06:53,740
translation and we had we have also

00:06:51,490 --> 00:06:56,050
another use case which is the neural

00:06:53,740 --> 00:06:58,270
machine translation so if we look back

00:06:56,050 --> 00:07:00,430
into into the history of machine

00:06:58,270 --> 00:07:02,230
translation we practically distinguish

00:07:00,430 --> 00:07:05,350
three main types of doing machine

00:07:02,230 --> 00:07:07,060
translation it's the rule base we in

00:07:05,350 --> 00:07:09,490
which you define and build the model by

00:07:07,060 --> 00:07:11,110
hand is the traditional statistical

00:07:09,490 --> 00:07:13,630
machine translation in which you define

00:07:11,110 --> 00:07:16,090
the model by hand and you statistically

00:07:13,630 --> 00:07:18,820
learn it from from the data and the

00:07:16,090 --> 00:07:22,330
neural machine translation I'm really

00:07:18,820 --> 00:07:24,490
happy that before my presentation was a

00:07:22,330 --> 00:07:28,260
presentation about deep learning neural

00:07:24,490 --> 00:07:31,240
networks so the neural networks wave

00:07:28,260 --> 00:07:34,990
reach also the statistical machine

00:07:31,240 --> 00:07:37,390
translation field so if for some time

00:07:34,990 --> 00:07:39,580
the the neural networks were to

00:07:37,390 --> 00:07:41,290
computational costly and resource

00:07:39,580 --> 00:07:44,140
demanding in order to compete with the

00:07:41,290 --> 00:07:46,750
state-of-the-art statistical machine

00:07:44,140 --> 00:07:49,330
translation the situation change in 2015

00:07:46,750 --> 00:07:52,030
so we now see engines trained with

00:07:49,330 --> 00:07:55,380
neural that are able to do that we are

00:07:52,030 --> 00:07:58,930
able to put them in production so

00:07:55,380 --> 00:08:02,050
compared to the two previous translation

00:07:58,930 --> 00:08:04,600
machine translation cases the neural

00:08:02,050 --> 00:08:06,400
empty in the neural machine translation

00:08:04,600 --> 00:08:09,550
case you define an architecture and the

00:08:06,400 --> 00:08:12,310
architecture takes care of discovering

00:08:09,550 --> 00:08:18,340
defining and learning the model from

00:08:12,310 --> 00:08:20,320
from the data actually the machine the

00:08:18,340 --> 00:08:22,840
neural machine translation uses deep

00:08:20,320 --> 00:08:24,730
learning architecture that is capable to

00:08:22,840 --> 00:08:26,650
learn the meaning of a text as a

00:08:24,730 --> 00:08:29,460
consequence the translation output is

00:08:26,650 --> 00:08:33,570
much more fluent and naturally sounding

00:08:29,460 --> 00:08:37,390
the neural MT also shows significant

00:08:33,570 --> 00:08:41,530
quality improvements over the the past

00:08:37,390 --> 00:08:43,720
engines because they are able to capture

00:08:41,530 --> 00:08:44,890
both local and global dependencies and

00:08:43,720 --> 00:08:48,190
they are able to handle

00:08:44,890 --> 00:08:49,600
Longridge word reordering

00:08:48,190 --> 00:08:53,050
in for example in

00:08:49,600 --> 00:08:54,970
our case we observed an impressive 30%

00:08:53,050 --> 00:08:57,579
improvement in quality for the English

00:08:54,970 --> 00:09:00,880
to German engine which for us and in

00:08:57,579 --> 00:09:04,269
general it's quite a challenge language

00:09:00,880 --> 00:09:07,329
paired to train and the previous let's

00:09:04,269 --> 00:09:09,459
say the previous our previous in our

00:09:07,329 --> 00:09:12,160
previous tries with with the statistical

00:09:09,459 --> 00:09:14,889
one we managed to improve it with 5% and

00:09:12,160 --> 00:09:20,470
or 10% after investing a lot of time in

00:09:14,889 --> 00:09:22,930
it so 30% is quite quite impressive we

00:09:20,470 --> 00:09:24,519
we already have some some never on

00:09:22,930 --> 00:09:27,480
machine translation actually more than

00:09:24,519 --> 00:09:30,579
10 that we are offering into our

00:09:27,480 --> 00:09:33,430
on-premise offer but we want to put

00:09:30,579 --> 00:09:36,610
these engines also into the cloud in

00:09:33,430 --> 00:09:40,240
order to put them into the cloud we need

00:09:36,610 --> 00:09:44,380
to be able to accommodate new hardware

00:09:40,240 --> 00:09:46,660
especially GPUs we need to have a

00:09:44,380 --> 00:09:50,920
flexible infrastructure that is able to

00:09:46,660 --> 00:09:54,160
handle both new engines and old engines

00:09:50,920 --> 00:09:55,899
both CPU and GPU engines and we need it

00:09:54,160 --> 00:09:58,899
we need to break the old implementation

00:09:55,899 --> 00:10:01,089
so we extract the common part and we

00:09:58,899 --> 00:10:04,149
keep this common part as a separate so

00:10:01,089 --> 00:10:06,370
we we deploy it as a separate and we

00:10:04,149 --> 00:10:08,949
scale it as a separating and for sure

00:10:06,370 --> 00:10:12,970
new and modern API would help us to -

00:10:08,949 --> 00:10:18,519
onward clients much more easy and much

00:10:12,970 --> 00:10:21,699
more fast ok so this these were the two

00:10:18,519 --> 00:10:24,100
new use case and we then we thought ok

00:10:21,699 --> 00:10:27,250
how can how can we do this how can we

00:10:24,100 --> 00:10:30,850
actually accommodate this use case in

00:10:27,250 --> 00:10:34,259
into into into our production so we

00:10:30,850 --> 00:10:36,310
actually started to look into our SAS

00:10:34,259 --> 00:10:39,730
solution that we currently have in

00:10:36,310 --> 00:10:43,540
production form for more than 10 years I

00:10:39,730 --> 00:10:45,490
would say the current solution it's a

00:10:43,540 --> 00:10:47,649
service-oriented architecture but all

00:10:45,490 --> 00:10:50,290
the services are deployed into a single

00:10:47,649 --> 00:10:52,509
application server so they are somehow

00:10:50,290 --> 00:10:55,000
packed together and deployed together so

00:10:52,509 --> 00:10:57,939
we cannot practically scale them quite

00:10:55,000 --> 00:11:00,730
easily but we look more closely into the

00:10:57,939 --> 00:11:02,620
SAS solution that we actually built with

00:11:00,730 --> 00:11:03,430
the same team that we are now building

00:11:02,620 --> 00:11:07,270
the new flat

00:11:03,430 --> 00:11:11,770
and it's quite a mature platform because

00:11:07,270 --> 00:11:14,650
we iterate on it over the course of five

00:11:11,770 --> 00:11:17,470
five years five seven years so we

00:11:14,650 --> 00:11:20,140
reached to a quite stable platform or

00:11:17,470 --> 00:11:22,600
solution as I mentioned in the beginning

00:11:20,140 --> 00:11:27,279
we translate 15 billion words with its

00:11:22,600 --> 00:11:31,120
with the solution currently we have a

00:11:27,279 --> 00:11:34,240
high availability and we don't have p1p2

00:11:31,120 --> 00:11:36,550
bugs reported ever it even flow from our

00:11:34,240 --> 00:11:39,940
external clients or from our technical

00:11:36,550 --> 00:11:43,990
support team so we practically are able

00:11:39,940 --> 00:11:46,630
to to discover all the bugs into the

00:11:43,990 --> 00:11:50,050
development or into the QA environments

00:11:46,630 --> 00:11:51,820
and it's the only large-scale commercial

00:11:50,050 --> 00:11:54,130
grain machine translation solution

00:11:51,820 --> 00:11:57,910
except the one from from Google and

00:11:54,130 --> 00:12:00,580
Microsoft so this were the pros of this

00:11:57,910 --> 00:12:03,460
solution and let's now look a little bit

00:12:00,580 --> 00:12:06,370
on the cause of this one so it has some

00:12:03,460 --> 00:12:09,700
flows that were built with requirements

00:12:06,370 --> 00:12:11,560
outdated requirements oh five seven

00:12:09,700 --> 00:12:14,260
years old requirements that at that time

00:12:11,560 --> 00:12:17,770
were great but in the meantime we with

00:12:14,260 --> 00:12:19,959
the situation changed our translation

00:12:17,770 --> 00:12:22,529
engine are not modular at all so we

00:12:19,959 --> 00:12:27,209
cannot easily add new things in there or

00:12:22,529 --> 00:12:30,040
extract things from there scaling down

00:12:27,209 --> 00:12:32,650
this solution requires quite a lot of

00:12:30,040 --> 00:12:36,220
human intervention so somebody needs to

00:12:32,650 --> 00:12:38,860
clone some VMs then run some scripts

00:12:36,220 --> 00:12:41,470
then add something into the database via

00:12:38,860 --> 00:12:43,720
some scripts and then the the engine is

00:12:41,470 --> 00:12:46,510
available to the customer so it takes

00:12:43,720 --> 00:12:51,910
probably let's say an hour in order to

00:12:46,510 --> 00:12:54,459
to to spin up a new engine and so we

00:12:51,910 --> 00:12:56,589
concluded that overall we are having a

00:12:54,459 --> 00:12:59,709
monolid solution that it's quite hard to

00:12:56,589 --> 00:13:01,300
adopt especially for the to use case

00:12:59,709 --> 00:13:05,380
that I mentioned in the beginning but in

00:13:01,300 --> 00:13:07,450
general for for any new use cases so the

00:13:05,380 --> 00:13:09,760
idea of a new platform came into the

00:13:07,450 --> 00:13:13,270
into the picture and we started to think

00:13:09,760 --> 00:13:15,339
more and more about this idea that

00:13:13,270 --> 00:13:17,040
probably we will need to build something

00:13:15,339 --> 00:13:20,050
from scratch

00:13:17,040 --> 00:13:22,510
okay and we we identify some key

00:13:20,050 --> 00:13:24,280
concepts that we would like to have into

00:13:22,510 --> 00:13:27,250
the new platform and we decided that

00:13:24,280 --> 00:13:29,230
scalability is very important for us we

00:13:27,250 --> 00:13:33,720
need to be able to accommodate new

00:13:29,230 --> 00:13:36,250
clients and especially large scale

00:13:33,720 --> 00:13:39,610
especially clients with with a lot of

00:13:36,250 --> 00:13:41,980
translations the latency and the speed

00:13:39,610 --> 00:13:44,140
is also important part for us because we

00:13:41,980 --> 00:13:47,860
have also some clients that wait in

00:13:44,140 --> 00:13:50,500
front of a monitor to see the

00:13:47,860 --> 00:13:54,280
translation coming we want to have

00:13:50,500 --> 00:13:56,680
independent services in this phase we

00:13:54,280 --> 00:13:58,450
did not necessarily decide that it will

00:13:56,680 --> 00:14:00,160
be micro services but we want to have

00:13:58,450 --> 00:14:02,020
independent services so we can scale

00:14:00,160 --> 00:14:05,140
them based on usage not to scale

00:14:02,020 --> 00:14:08,710
everything as a as a monolith we want to

00:14:05,140 --> 00:14:11,170
be elastic so we want to auto scale both

00:14:08,710 --> 00:14:14,500
up and also when when the clients are

00:14:11,170 --> 00:14:18,520
not there for for some hours we we want

00:14:14,500 --> 00:14:21,700
to to lower the deployment so we we

00:14:18,520 --> 00:14:23,650
don't pay so much we want to build also

00:14:21,700 --> 00:14:29,380
a solution that is responds well to

00:14:23,650 --> 00:14:33,400
failures we want to do as as less as

00:14:29,380 --> 00:14:36,790
possible manual steps I mean we want to

00:14:33,400 --> 00:14:38,560
do everything we have via scripts and we

00:14:36,790 --> 00:14:40,300
want to have reliable monitoring and

00:14:38,560 --> 00:14:42,910
alerts because this is quite important

00:14:40,300 --> 00:14:45,030
for us we don't want to be called over

00:14:42,910 --> 00:14:48,280
the weekend or or during the night

00:14:45,030 --> 00:14:52,990
except if manual intervention is is

00:14:48,280 --> 00:14:55,780
really needed so on we also start to

00:14:52,990 --> 00:14:57,550
look of what other company with similar

00:14:55,780 --> 00:15:00,910
scaling problem are doing and we

00:14:57,550 --> 00:15:03,040
actually observed quite a pattern so we

00:15:00,910 --> 00:15:06,910
see that a lot of them have started from

00:15:03,040 --> 00:15:09,030
a Manali pearl rails C++ then they

00:15:06,910 --> 00:15:11,589
continue with the Java Scala

00:15:09,030 --> 00:15:14,110
implementation and then they ended up

00:15:11,589 --> 00:15:18,010
using micro services we actually have a

00:15:14,110 --> 00:15:20,830
similar pad I couldn't I could say that

00:15:18,010 --> 00:15:23,410
X for example in 2010 we started with

00:15:20,830 --> 00:15:25,540
the same team to build the the Java

00:15:23,410 --> 00:15:27,430
implementation having a Ruby on Rails

00:15:25,540 --> 00:15:27,910
implementation that we could not scale

00:15:27,430 --> 00:15:30,590
anymore

00:15:27,910 --> 00:15:32,480
even with doubling the number of

00:15:30,590 --> 00:15:37,250
machines involvement we could not

00:15:32,480 --> 00:15:39,830
actually get more numbers out of that so

00:15:37,250 --> 00:15:43,360
we decided that we will follow the same

00:15:39,830 --> 00:15:46,850
path that to use the micro-services

00:15:43,360 --> 00:15:49,670
after some proof of concepts and a lot

00:15:46,850 --> 00:15:52,700
of experimentation we actually ended up

00:15:49,670 --> 00:15:57,290
with this technology stack in order to

00:15:52,700 --> 00:15:59,930
solve our scaling problem we decided to

00:15:57,290 --> 00:16:03,860
use mesos as our as our cluster manager

00:15:59,930 --> 00:16:07,550
and for sure we are using also marathon

00:16:03,860 --> 00:16:10,910
and Kronos on top of mesos we decided to

00:16:07,550 --> 00:16:12,320
use HBase as our null sequel database in

00:16:10,910 --> 00:16:14,810
which we store all the rules and

00:16:12,320 --> 00:16:18,640
everything that that needs to be stored

00:16:14,810 --> 00:16:22,520
into the platform we are using the HDFS

00:16:18,640 --> 00:16:25,340
layer from Hadoop in order as a storage

00:16:22,520 --> 00:16:27,320
both for for the HBase but also for

00:16:25,340 --> 00:16:30,050
anything that we need to store into the

00:16:27,320 --> 00:16:32,180
platform I mean all the content that we

00:16:30,050 --> 00:16:36,650
are we are actually using into the

00:16:32,180 --> 00:16:39,650
platform in order to Stu's to solve the

00:16:36,650 --> 00:16:42,230
latency and fault-tolerance part we

00:16:39,650 --> 00:16:45,700
decided to use Kafka as our messaging

00:16:42,230 --> 00:16:48,560
system all our micro services are

00:16:45,700 --> 00:16:53,330
stateless so we communicate between

00:16:48,560 --> 00:16:56,600
services using using messages and those

00:16:53,330 --> 00:17:00,410
messages are are persisted in Kafka we

00:16:56,600 --> 00:17:02,540
used sue keeper both as something that

00:17:00,410 --> 00:17:04,910
is tied to the message but we also

00:17:02,540 --> 00:17:07,480
implemented some logic in order to do

00:17:04,910 --> 00:17:10,520
discovery of the services around there

00:17:07,480 --> 00:17:12,890
we use protocol buffers in order to

00:17:10,520 --> 00:17:16,550
serialize all the messages that we are

00:17:12,890 --> 00:17:19,550
exchanging in between micro services on

00:17:16,550 --> 00:17:22,310
the infrastructure side we are using

00:17:19,550 --> 00:17:24,709
ansible in order to automate everything

00:17:22,310 --> 00:17:26,840
that previously we were doing manual or

00:17:24,709 --> 00:17:30,740
all the new things that we need to do

00:17:26,840 --> 00:17:34,430
into the new platform and our production

00:17:30,740 --> 00:17:36,470
environment is is in AWS we we actually

00:17:34,430 --> 00:17:38,930
started developing the solution in our

00:17:36,470 --> 00:17:41,900
private data center then we decided that

00:17:38,930 --> 00:17:44,060
is better to host it into AWS because

00:17:41,900 --> 00:17:46,610
the old solution

00:17:44,060 --> 00:17:48,710
taking out all of our resources in in

00:17:46,610 --> 00:17:51,320
the production facility so we did not

00:17:48,710 --> 00:17:54,200
have a place for for for new things to

00:17:51,320 --> 00:17:58,340
put in there so we decided to use a SS

00:17:54,200 --> 00:18:00,500
and it was quite a good decision for

00:17:58,340 --> 00:18:03,260
monitoring and others we are using the

00:18:00,500 --> 00:18:07,220
elastic elastic search locks touch tank

00:18:03,260 --> 00:18:09,590
even a stack the graph on apart we are

00:18:07,220 --> 00:18:11,780
using four four dashboards and metrics

00:18:09,590 --> 00:18:15,290
and I will actually show this part into

00:18:11,780 --> 00:18:18,740
into the demo and we are collecting

00:18:15,290 --> 00:18:21,320
application specific metrics into open

00:18:18,740 --> 00:18:23,660
PSDB and we show them voting Ravana or

00:18:21,320 --> 00:18:26,390
we actually use them to analyze how

00:18:23,660 --> 00:18:29,000
health is the is our deployment and we

00:18:26,390 --> 00:18:31,360
we monitor everything that is in to our

00:18:29,000 --> 00:18:35,630
platform using the desi bik's

00:18:31,360 --> 00:18:39,560
on the micro services side we use docker

00:18:35,630 --> 00:18:41,600
as a as our containerization platform we

00:18:39,560 --> 00:18:44,720
started with drop wizard than we

00:18:41,600 --> 00:18:50,630
understood that is more easy to use the

00:18:44,720 --> 00:18:55,600
spring boot part as a rest application

00:18:50,630 --> 00:18:59,270
framework and we started with Java 8

00:18:55,600 --> 00:19:01,220
three years ago when it was still at the

00:18:59,270 --> 00:19:03,290
beginning but we we had a good

00:19:01,220 --> 00:19:06,710
experience on it also this one was

00:19:03,290 --> 00:19:08,840
actually a good decision okay so this

00:19:06,710 --> 00:19:11,900
was the technology stack that we ended

00:19:08,840 --> 00:19:14,540
up using so now let's see the lesson

00:19:11,900 --> 00:19:16,580
that we learn in this last two three

00:19:14,540 --> 00:19:20,420
years while trying to build this

00:19:16,580 --> 00:19:23,090
scalable platform so the first one is

00:19:20,420 --> 00:19:25,160
related to the cost from the beginning

00:19:23,090 --> 00:19:30,020
we knew that we want to be to build

00:19:25,160 --> 00:19:32,960
something that is cost efficient and we

00:19:30,020 --> 00:19:35,870
were quite passionate about looking on

00:19:32,960 --> 00:19:37,880
how we can we can optimize our cost as I

00:19:35,870 --> 00:19:40,850
mentioned we started the development

00:19:37,880 --> 00:19:44,690
into our private data center and we

00:19:40,850 --> 00:19:47,870
postponed a little bit using add AWS and

00:19:44,690 --> 00:19:49,910
this part was not necessarily a good

00:19:47,870 --> 00:19:52,310
decision because when we started to put

00:19:49,910 --> 00:19:54,200
things into AWS we see a lot of things

00:19:52,310 --> 00:19:56,240
that were different compared to our

00:19:54,200 --> 00:19:57,930
private data center our private data

00:19:56,240 --> 00:20:01,650
center is located in

00:19:57,930 --> 00:20:05,610
for Colorado and we actually deployed

00:20:01,650 --> 00:20:08,250
into AWS region into Oregon when we put

00:20:05,610 --> 00:20:11,190
things into AWS we see some difference

00:20:08,250 --> 00:20:13,200
especially our engines are quite eioped

00:20:11,190 --> 00:20:15,990
intensive so they do a lot of input

00:20:13,200 --> 00:20:17,760
operate and output operation and we

00:20:15,990 --> 00:20:19,560
could not obtain the same performance

00:20:17,760 --> 00:20:22,800
that we obtain in our private data

00:20:19,560 --> 00:20:26,520
center in AWS this is mainly related to

00:20:22,800 --> 00:20:29,100
the fact that AWS gives you so the

00:20:26,520 --> 00:20:33,090
performance of your storage is more or

00:20:29,100 --> 00:20:36,270
less tied to how much how big the the

00:20:33,090 --> 00:20:39,030
partitions are that you are using but we

00:20:36,270 --> 00:20:41,730
in the end we actually had to change our

00:20:39,030 --> 00:20:44,160
implementation in order to to make the

00:20:41,730 --> 00:20:47,370
engines less I ops intensive in order to

00:20:44,160 --> 00:20:49,170
be able to to have the same numbers from

00:20:47,370 --> 00:20:52,290
our private data center and from our

00:20:49,170 --> 00:20:57,390
from our a SS deployment we also had a

00:20:52,290 --> 00:20:59,550
lot of configuration differences and we

00:20:57,390 --> 00:21:02,790
actually decided that is it's a good

00:20:59,550 --> 00:21:06,330
idea to do a production clone into AWS

00:21:02,790 --> 00:21:09,300
so we test all all these things before

00:21:06,330 --> 00:21:10,950
actually putting our our releasing

00:21:09,300 --> 00:21:13,050
production we test all the things in the

00:21:10,950 --> 00:21:14,820
production clone to see exactly how if

00:21:13,050 --> 00:21:17,970
we still have differences or not

00:21:14,820 --> 00:21:21,870
especially on the config part the

00:21:17,970 --> 00:21:25,830
production clone is quite similar with

00:21:21,870 --> 00:21:28,590
the production environment I mean it has

00:21:25,830 --> 00:21:30,750
the same IPS we are not using the same

00:21:28,590 --> 00:21:33,510
size of a message cluster but we are

00:21:30,750 --> 00:21:36,720
trying to especially for cost reasons

00:21:33,510 --> 00:21:39,330
but we are trying to to to choose the

00:21:36,720 --> 00:21:42,180
engines that are more relevant and and

00:21:39,330 --> 00:21:46,020
to those ones to do the tests and their

00:21:42,180 --> 00:21:48,980
validation still currently 40% of our

00:21:46,020 --> 00:21:52,620
cost it's a non production cost from the

00:21:48,980 --> 00:21:55,110
AWS bill I mean that only 60 percent of

00:21:52,620 --> 00:21:58,470
our cost its tick production we have

00:21:55,110 --> 00:22:00,480
some small dev clusters QA clusters and

00:21:58,470 --> 00:22:04,080
and production clones as I was

00:22:00,480 --> 00:22:06,890
mentioning in there to keep down the AWS

00:22:04,080 --> 00:22:09,780
cost we do periodical cleanups of

00:22:06,890 --> 00:22:11,730
everything practically I mean snapshot

00:22:09,780 --> 00:22:14,700
EBS volumes easy to

00:22:11,730 --> 00:22:17,610
instances and so on we have some alerts

00:22:14,700 --> 00:22:21,450
in place so when the number of instances

00:22:17,610 --> 00:22:23,340
that are running go above a threshold we

00:22:21,450 --> 00:22:26,010
are receiving a notification and we

00:22:23,340 --> 00:22:31,380
start to look on why the number of

00:22:26,010 --> 00:22:34,470
instances is so high and we also try to

00:22:31,380 --> 00:22:38,790
use the latest AWS type of instances so

00:22:34,470 --> 00:22:42,270
for example we are using a ec2 of a type

00:22:38,790 --> 00:22:46,800
r3 Forex large with which they have 16

00:22:42,270 --> 00:22:48,750
cores and 128 years AWS release a new

00:22:46,800 --> 00:22:51,120
version of this is easy two instances

00:22:48,750 --> 00:22:54,440
which are called air for Forex large

00:22:51,120 --> 00:22:56,640
with which have exactly the same

00:22:54,440 --> 00:23:00,720
specification in hardware I mean the

00:22:56,640 --> 00:23:07,230
same 16 core 128 gigs with approximately

00:23:00,720 --> 00:23:08,640
25% lower in cost the Meli dolly I mean

00:23:07,230 --> 00:23:10,890
the only difference that is that they

00:23:08,640 --> 00:23:14,610
have an ephemeral the the old ones they

00:23:10,890 --> 00:23:17,970
have ephemeral SSDs but we we did not

00:23:14,610 --> 00:23:20,880
use it so for us it was it was okay we

00:23:17,970 --> 00:23:23,160
also had a lot of discussion in the team

00:23:20,880 --> 00:23:26,520
about using the elastic block storage

00:23:23,160 --> 00:23:28,770
from from a SS or to using the elastic

00:23:26,520 --> 00:23:31,800
file storage and we actually decided to

00:23:28,770 --> 00:23:34,170
use the EFS for anything that is shared

00:23:31,800 --> 00:23:37,530
across across all the mass of slaves and

00:23:34,170 --> 00:23:41,250
to use the EBS for other things that are

00:23:37,530 --> 00:23:43,950
more local stuff and for sure we are

00:23:41,250 --> 00:23:46,350
reserving our instances especially for

00:23:43,950 --> 00:23:48,780
our production cluster in order to to

00:23:46,350 --> 00:23:54,000
bring down the cost with approximately

00:23:48,780 --> 00:23:57,060
30% the second lesson learned it's about

00:23:54,000 --> 00:24:00,020
the security so we we had in into this

00:23:57,060 --> 00:24:03,600
new platform we had to to look at this

00:24:00,020 --> 00:24:08,030
security part from a different angle

00:24:03,600 --> 00:24:10,740
we started by so the access to our

00:24:08,030 --> 00:24:14,550
production cluster in a double assets we

00:24:10,740 --> 00:24:16,710
are only one SSH Bastion host that host

00:24:14,550 --> 00:24:19,680
contains also some filters in terms of

00:24:16,710 --> 00:24:24,270
IP that they can that can access this

00:24:19,680 --> 00:24:25,559
host we also use GPG encryption in order

00:24:24,270 --> 00:24:28,960
to

00:24:25,559 --> 00:24:32,620
do not stores our passwords in clear

00:24:28,960 --> 00:24:35,260
indeed and to also restrict the access

00:24:32,620 --> 00:24:37,390
to specific environment so we had

00:24:35,260 --> 00:24:39,190
situation when some especially at the

00:24:37,390 --> 00:24:41,110
beginning when somebody was trying to

00:24:39,190 --> 00:24:43,180
run some ansible comments into the

00:24:41,110 --> 00:24:46,000
production clone but he ended up

00:24:43,180 --> 00:24:50,950
actually redeploying some services into

00:24:46,000 --> 00:24:54,070
the production cluster after that we we

00:24:50,950 --> 00:24:56,110
decided who will actually do mainly our

00:24:54,070 --> 00:24:58,900
deployments and we restricted the list

00:24:56,110 --> 00:25:01,420
of people who have access to to run

00:24:58,900 --> 00:25:04,330
ansible comments for the production in

00:25:01,420 --> 00:25:05,770
order to prevent this type of of of

00:25:04,330 --> 00:25:08,559
errors

00:25:05,770 --> 00:25:10,990
except for this in all all our clusters

00:25:08,559 --> 00:25:13,090
everybody has the same access and can do

00:25:10,990 --> 00:25:15,040
whatever they want as I mentioned the

00:25:13,090 --> 00:25:17,770
only restriction are are into the

00:25:15,040 --> 00:25:20,380
production environments and we enable a

00:25:17,770 --> 00:25:23,890
SS termination protection it's just a

00:25:20,380 --> 00:25:26,980
click in there but we ended up enabling

00:25:23,890 --> 00:25:29,020
as only after somebody from the team not

00:25:26,980 --> 00:25:31,679
the same person as before actually

00:25:29,020 --> 00:25:35,140
terminated a master slave by mistake

00:25:31,679 --> 00:25:37,660
which which was not a big issue but if

00:25:35,140 --> 00:25:39,940
he could if he would terminate the whole

00:25:37,660 --> 00:25:43,780
cluster then then that would have been

00:25:39,940 --> 00:25:45,760
an issue on the high availability side

00:25:43,780 --> 00:25:48,940
so we usually on the infrastructure side

00:25:45,760 --> 00:25:52,330
we usually usually allocate one node

00:25:48,940 --> 00:25:55,300
actually 10% more than the capacity that

00:25:52,330 --> 00:25:58,510
is quite needed in order to to be highly

00:25:55,300 --> 00:26:00,490
available even if some human error are

00:25:58,510 --> 00:26:03,400
happening as I was mentioning before but

00:26:00,490 --> 00:26:08,110
we also had cases man our ec2 instances

00:26:03,400 --> 00:26:10,990
were were were decommissioned and we had

00:26:08,110 --> 00:26:15,100
to rebuild them it did not happen

00:26:10,990 --> 00:26:18,640
frequently but we had some situation of

00:26:15,100 --> 00:26:21,910
this type and on the micro services side

00:26:18,640 --> 00:26:24,130
we are allocate at least two instances

00:26:21,910 --> 00:26:26,770
per type of micro service we are doing

00:26:24,130 --> 00:26:29,020
this for high availability purpose even

00:26:26,770 --> 00:26:32,260
if in some cases would not have been

00:26:29,020 --> 00:26:34,510
have been justified by the by the

00:26:32,260 --> 00:26:36,700
traffic on that micro service and we

00:26:34,510 --> 00:26:40,210
also put some constraints to not deploy

00:26:36,700 --> 00:26:44,289
these two instances on the same

00:26:40,210 --> 00:26:48,279
agent in order not if that agent goes

00:26:44,289 --> 00:26:51,909
down to not to not lose the the the that

00:26:48,279 --> 00:26:56,200
micro that micro service we also test

00:26:51,909 --> 00:26:58,210
our we are doing tests in our QA

00:26:56,200 --> 00:27:01,109
environment a performance environments

00:26:58,210 --> 00:27:03,609
with 5x or 10x more traffic in order

00:27:01,109 --> 00:27:06,759
compared to what we expect or what we

00:27:03,609 --> 00:27:09,339
have in production in order to reach the

00:27:06,759 --> 00:27:11,080
limits of our platform and to know if if

00:27:09,339 --> 00:27:12,879
things will break what would be the

00:27:11,080 --> 00:27:16,450
first components that that will actually

00:27:12,879 --> 00:27:18,159
break and we we including from the

00:27:16,450 --> 00:27:19,690
previous solution that we build we

00:27:18,159 --> 00:27:22,419
understood that monitoring is really

00:27:19,690 --> 00:27:25,029
important so we invested quite a lot of

00:27:22,419 --> 00:27:28,149
time in monitoring from all different

00:27:25,029 --> 00:27:30,609
angles these micro services and this

00:27:28,149 --> 00:27:32,229
platform so we are using zabbix as I

00:27:30,609 --> 00:27:34,779
want mentioning for the infrastructure

00:27:32,229 --> 00:27:37,769
we are collecting application specific

00:27:34,779 --> 00:27:40,950
metrics that we show we have a screen

00:27:37,769 --> 00:27:44,979
into our office that we show a lot of

00:27:40,950 --> 00:27:47,289
healthy - I mean that's more related to

00:27:44,979 --> 00:27:50,830
the health of the deployment we are

00:27:47,289 --> 00:27:53,289
collecting application usage statistic

00:27:50,830 --> 00:27:55,149
of everything that happens it to the

00:27:53,289 --> 00:27:56,979
platform all the activities that the

00:27:55,149 --> 00:27:59,559
user performing to a platform as I

00:27:56,979 --> 00:28:01,960
mentioned using the elasticsearch locks

00:27:59,559 --> 00:28:05,499
- cabana and we are also doing external

00:28:01,960 --> 00:28:08,109
monitoring using Pingdom and pager duty

00:28:05,499 --> 00:28:14,339
to call us in case something is really

00:28:08,109 --> 00:28:17,859
wrong on the resource allocation side so

00:28:14,339 --> 00:28:20,679
for example in the in the on the memory

00:28:17,859 --> 00:28:23,259
limitation part we initially started

00:28:20,679 --> 00:28:26,559
quite at the beginning by by setting

00:28:23,259 --> 00:28:30,580
only marathon restrictions on that on

00:28:26,559 --> 00:28:32,889
the container side I mean we had one

00:28:30,580 --> 00:28:37,389
container we see that we say that it has

00:28:32,889 --> 00:28:39,639
it needs 1 gig of memory what after some

00:28:37,389 --> 00:28:42,460
time we were seeing some containers

00:28:39,639 --> 00:28:45,129
being killed when we investigated the

00:28:42,460 --> 00:28:47,200
issue we see that we saw that actually

00:28:45,129 --> 00:28:52,810
those containers were seeing the memory

00:28:47,200 --> 00:28:55,930
of the whole mesos agent so 138 gigs

00:28:52,810 --> 00:29:00,850
compared to one geek I know that it's a

00:28:55,930 --> 00:29:03,850
quite dumb thing but we managed to fix

00:29:00,850 --> 00:29:06,610
it only after actually seeing it not in

00:29:03,850 --> 00:29:09,100
production in our QA environments so we

00:29:06,610 --> 00:29:13,270
said we set some limits both at marathon

00:29:09,100 --> 00:29:16,450
side and at the donker side of the same

00:29:13,270 --> 00:29:18,190
one gig let's say but occasionally we

00:29:16,450 --> 00:29:20,380
were still seeing some microservices

00:29:18,190 --> 00:29:22,330
dying and we when we investigated we

00:29:20,380 --> 00:29:24,640
were saying that actually the JVM was

00:29:22,330 --> 00:29:27,670
trying to take all the one gig of memory

00:29:24,640 --> 00:29:29,800
and he could not garbage collect so fast

00:29:27,670 --> 00:29:31,930
the things when when they were reaching

00:29:29,800 --> 00:29:35,860
to the higher limit so marathon just

00:29:31,930 --> 00:29:39,100
just killed it was just killing the dose

00:29:35,860 --> 00:29:42,940
container so we actually set constraints

00:29:39,100 --> 00:29:46,090
both at the JVM side this time it was

00:29:42,940 --> 00:29:50,860
let's say with 10% lower the limit like

00:29:46,090 --> 00:29:53,800
900 max on the on the JVM side one gig

00:29:50,860 --> 00:29:56,140
at the docker side and one gig at the

00:29:53,800 --> 00:29:59,170
marathon side and this actually solved

00:29:56,140 --> 00:30:01,660
our our problem while investigating

00:29:59,170 --> 00:30:03,640
these we understood that we are doing

00:30:01,660 --> 00:30:06,400
crash dumps but we don't actually save

00:30:03,640 --> 00:30:07,000
them outside so we are actually losing

00:30:06,400 --> 00:30:10,150
them

00:30:07,000 --> 00:30:13,060
so we mounted a partition on each of the

00:30:10,150 --> 00:30:15,430
container that is actually available

00:30:13,060 --> 00:30:18,340
even after the container died so we are

00:30:15,430 --> 00:30:19,900
able to investigate and take the core

00:30:18,340 --> 00:30:23,860
dump and see exactly what happened in

00:30:19,900 --> 00:30:27,580
there related to the CP allocation so as

00:30:23,860 --> 00:30:30,670
you probably know the CPU are actually

00:30:27,580 --> 00:30:33,370
CPU weights so initially we started if

00:30:30,670 --> 00:30:36,970
we needed one core we allocated one as a

00:30:33,370 --> 00:30:39,520
CPU weight we concluded that is not the

00:30:36,970 --> 00:30:44,470
best idea because in this case we could

00:30:39,520 --> 00:30:47,920
not use our clusters very efficient and

00:30:44,470 --> 00:30:50,290
we could not do over provision so we we

00:30:47,920 --> 00:30:53,500
actually lower all the all the CPU

00:30:50,290 --> 00:30:58,470
weights from 1 to 0 - and now we are

00:30:53,500 --> 00:31:02,350
able to use our cluster even even better

00:30:58,470 --> 00:31:05,650
okay so the the second one is related to

00:31:02,350 --> 00:31:06,640
the releases so when we started this

00:31:05,650 --> 00:31:08,560
platform we were doing

00:31:06,640 --> 00:31:10,990
okay we are building something scalable

00:31:08,560 --> 00:31:13,030
except for maybe the load balancer we

00:31:10,990 --> 00:31:16,750
don't have any single point of failure

00:31:13,030 --> 00:31:19,840
into this so releases with no time time

00:31:16,750 --> 00:31:22,900
will be quite quite fast to do it was

00:31:19,840 --> 00:31:26,250
not actually the case because they were

00:31:22,900 --> 00:31:29,260
uh there are so many components into

00:31:26,250 --> 00:31:33,700
involving to a deployment of this as I

00:31:29,260 --> 00:31:37,180
mentioned we have quite dirty I mean we

00:31:33,700 --> 00:31:41,050
currently have more than 35 type of

00:31:37,180 --> 00:31:44,470
micro services and a lot of things could

00:31:41,050 --> 00:31:47,050
go wrong so we actually invested quite

00:31:44,470 --> 00:31:48,850
some time in order to do the claw the

00:31:47,050 --> 00:31:50,890
production clone in order to simulate

00:31:48,850 --> 00:31:53,650
our deployments two or three times

00:31:50,890 --> 00:31:55,750
before actually releasing them into the

00:31:53,650 --> 00:31:58,230
into the production in order to catch

00:31:55,750 --> 00:32:00,790
all the differences that that are

00:31:58,230 --> 00:32:03,130
between our QA environment and the

00:32:00,790 --> 00:32:05,260
production environment we also did some

00:32:03,130 --> 00:32:07,710
scripts in order to monitor the downtime

00:32:05,260 --> 00:32:09,730
so we actually know if you want to have

00:32:07,710 --> 00:32:11,500
releases mean no downtime

00:32:09,730 --> 00:32:14,440
first we need to know how to measure

00:32:11,500 --> 00:32:18,610
them that they are without downtime and

00:32:14,440 --> 00:32:20,770
we also invested some time in in making

00:32:18,610 --> 00:32:22,950
these messages that are circulating

00:32:20,770 --> 00:32:26,080
between

00:32:22,950 --> 00:32:28,060
micro-services compatible from different

00:32:26,080 --> 00:32:30,340
version on this side the prod you see

00:32:28,060 --> 00:32:35,890
using the protocol protocol buffer

00:32:30,340 --> 00:32:38,530
helped us a lot we invested also some

00:32:35,890 --> 00:32:40,270
time in and symbolizing all the manual

00:32:38,530 --> 00:32:43,020
steps that were involved in to the

00:32:40,270 --> 00:32:46,360
deployment so now the deployment it's

00:32:43,020 --> 00:32:51,520
probably one or two ansible comments and

00:32:46,360 --> 00:32:53,850
everything happens as in there so we

00:32:51,520 --> 00:32:56,830
have we have ansible commands that are

00:32:53,850 --> 00:32:58,900
for almost all the activity that we are

00:32:56,830 --> 00:33:03,160
doing in order to keep this cluster up

00:32:58,900 --> 00:33:05,590
to do upgrades to do deployments and we

00:33:03,160 --> 00:33:08,200
are also able to bring up a cluster from

00:33:05,590 --> 00:33:12,400
zero I mean from zero not having nothing

00:33:08,200 --> 00:33:14,290
to bring up ec2 instances deploy all the

00:33:12,400 --> 00:33:16,600
components deploy all the micro services

00:33:14,290 --> 00:33:19,360
validate that the this cluster is

00:33:16,600 --> 00:33:21,940
healthy and we are doing

00:33:19,360 --> 00:33:26,650
it takes around 20 to 30 minutes to do

00:33:21,940 --> 00:33:29,530
all the steps all the investigations

00:33:26,650 --> 00:33:32,679
into this distributed platform became

00:33:29,530 --> 00:33:35,920
quite a big issue I mean it it became

00:33:32,679 --> 00:33:40,390
quite a complex issue so in the previous

00:33:35,920 --> 00:33:43,210
solution we had logs that were into I

00:33:40,390 --> 00:33:46,570
mean having all the services deployed

00:33:43,210 --> 00:33:49,660
into a single application server it's

00:33:46,570 --> 00:33:51,970
it's a big advantage because you can at

00:33:49,660 --> 00:33:54,400
least track when the client with a

00:33:51,970 --> 00:33:56,950
request hit your your first ten point

00:33:54,400 --> 00:33:59,169
and all the steps that he made along the

00:33:56,950 --> 00:34:01,210
way because he did it in the same

00:33:59,169 --> 00:34:04,929
application server if even if you have

00:34:01,210 --> 00:34:10,090
multiple deployments in this case when a

00:34:04,929 --> 00:34:12,100
flow maybe hits 10 to 12 or 15 micro

00:34:10,090 --> 00:34:15,570
services it's really hard to understand

00:34:12,100 --> 00:34:20,200
all the operation that the user made

00:34:15,570 --> 00:34:23,409
before from the entering point to the to

00:34:20,200 --> 00:34:26,859
the exit point so we aggregated our log

00:34:23,409 --> 00:34:29,619
into elasticsearch log slash Cabana we

00:34:26,859 --> 00:34:32,200
had to attach a request ID to each of

00:34:29,619 --> 00:34:34,030
the messages that were circulating and

00:34:32,200 --> 00:34:36,310
each of the logs that were circulating

00:34:34,030 --> 00:34:42,419
into the into the platform to be able to

00:34:36,310 --> 00:34:45,310
to to investigate per request later

00:34:42,419 --> 00:34:47,679
initially we were both shipping the logs

00:34:45,310 --> 00:34:51,280
into the elasticsearch but we were also

00:34:47,679 --> 00:34:54,520
using the STD out because it was much

00:34:51,280 --> 00:34:56,770
more easy to debug things in the message

00:34:54,520 --> 00:35:01,570
level quite quickly

00:34:56,770 --> 00:35:04,300
our mesos agents remain without disk so

00:35:01,570 --> 00:35:06,640
we actually had to disable the STD out

00:35:04,300 --> 00:35:08,740
and we only shipped them into

00:35:06,640 --> 00:35:11,830
elasticsearch after that when we started

00:35:08,740 --> 00:35:13,710
to to do some performance and to do some

00:35:11,830 --> 00:35:17,740
load on the platform we understood that

00:35:13,710 --> 00:35:19,720
the get the pander that we were using

00:35:17,740 --> 00:35:21,520
that were she was shipping things to

00:35:19,720 --> 00:35:23,320
elasticsearch could not keep up with the

00:35:21,520 --> 00:35:26,140
volume of the law that was coming from

00:35:23,320 --> 00:35:28,720
our clients so we had tried to to do a

00:35:26,140 --> 00:35:32,320
trick to send it first to Kafka and

00:35:28,720 --> 00:35:33,070
Kafka will send it to 12s nixar stone

00:35:32,320 --> 00:35:34,870
with

00:35:33,070 --> 00:35:37,350
we'll see a little bit of delay between

00:35:34,870 --> 00:35:40,360
when we see the request entering and and

00:35:37,350 --> 00:35:42,400
and when it appears in tower graph but

00:35:40,360 --> 00:35:45,430
it's it's quite an acceptable delay like

00:35:42,400 --> 00:35:48,790
of I don't know it was under a second

00:35:45,430 --> 00:35:52,240
let's say we are collecting applications

00:35:48,790 --> 00:35:55,330
as specific metrics into open the sdb

00:35:52,240 --> 00:35:57,340
and usually when something's go wrong we

00:35:55,330 --> 00:35:59,770
had to correlate all these sources and

00:35:57,340 --> 00:36:05,470
understand where were the problems what

00:35:59,770 --> 00:36:08,740
the problem is as I mentioned at the

00:36:05,470 --> 00:36:10,780
beginning when we we and identified the

00:36:08,740 --> 00:36:13,020
key concepts we said that we want to

00:36:10,780 --> 00:36:16,600
have independence independent

00:36:13,020 --> 00:36:19,240
micro-services it's it's something that

00:36:16,600 --> 00:36:22,120
all the people in the team agree with it

00:36:19,240 --> 00:36:25,960
was not so easy to it was not an easy

00:36:22,120 --> 00:36:29,170
goal to achieve as I mentioned we have

00:36:25,960 --> 00:36:31,150
more than 30 micro services now it was a

00:36:29,170 --> 00:36:33,850
challenge especially for the legacy code

00:36:31,150 --> 00:36:36,250
so we took some code especially related

00:36:33,850 --> 00:36:39,280
to our engines from the legacy solution

00:36:36,250 --> 00:36:41,950
and we we pack it at the micro service

00:36:39,280 --> 00:36:44,860
II as a micro service and we extract

00:36:41,950 --> 00:36:49,840
also parts that could be extracted as

00:36:44,860 --> 00:36:51,880
independent services what surprised us a

00:36:49,840 --> 00:36:53,980
little bit is that even on the new

00:36:51,880 --> 00:36:56,320
services that we define from scratch

00:36:53,980 --> 00:36:59,710
after two three months we realized that

00:36:56,320 --> 00:37:02,410
those services could be broken into

00:36:59,710 --> 00:37:07,350
multiple micro services all over again

00:37:02,410 --> 00:37:09,610
and so it's quite a continuous

00:37:07,350 --> 00:37:12,580
refactoring process in order to identify

00:37:09,610 --> 00:37:15,300
things that we can actually break into

00:37:12,580 --> 00:37:19,600
even more micro services probably the 30

00:37:15,300 --> 00:37:22,030
micro service number does not sound like

00:37:19,600 --> 00:37:26,110
a big number compared to the Netflix who

00:37:22,030 --> 00:37:29,950
were having I think 350 or something

00:37:26,110 --> 00:37:32,500
like that but for us from Manali to 30

00:37:29,950 --> 00:37:36,850
plus micro services it's it's quite an

00:37:32,500 --> 00:37:39,540
achievement and the last lesson the last

00:37:36,850 --> 00:37:42,550
lesson learned it's about the fact that

00:37:39,540 --> 00:37:44,830
we have had to reevaluate our

00:37:42,550 --> 00:37:46,650
assumptions periodically so when we

00:37:44,830 --> 00:37:49,060
started the platform

00:37:46,650 --> 00:37:50,830
considering also the previous experience

00:37:49,060 --> 00:37:53,110
that we had in the different solution

00:37:50,830 --> 00:37:55,090
and it on the serving different machine

00:37:53,110 --> 00:37:57,310
translation use case we were thinking

00:37:55,090 --> 00:38:00,820
that we can imagine chi quite well what

00:37:57,310 --> 00:38:04,000
what the users will do and then we

00:38:00,820 --> 00:38:06,370
released an alpha version and then a

00:38:04,000 --> 00:38:08,590
beta with for a similar limited set of

00:38:06,370 --> 00:38:12,550
users and then we understood that

00:38:08,590 --> 00:38:15,010
actually they don't use our api's in the

00:38:12,550 --> 00:38:17,500
way that we actually recommend it or in

00:38:15,010 --> 00:38:20,860
the way that we actually expected so we

00:38:17,500 --> 00:38:23,020
had to change some of our api's in some

00:38:20,860 --> 00:38:25,720
cases two or three versions in order to

00:38:23,020 --> 00:38:30,280
accommodate better the the way that the

00:38:25,720 --> 00:38:31,840
our clients are are using it in for the

00:38:30,280 --> 00:38:34,090
adaptive machine translation flow we

00:38:31,840 --> 00:38:37,390
have now I think more than two three

00:38:34,090 --> 00:38:39,160
thousand users which is not necessarily

00:38:37,390 --> 00:38:42,610
an impressive number compared to the

00:38:39,160 --> 00:38:46,240
previous solution but it's it's still a

00:38:42,610 --> 00:38:49,960
big number we understood that also the

00:38:46,240 --> 00:38:53,250
speed of our of our requested quite

00:38:49,960 --> 00:38:55,840
important especially for the sync

00:38:53,250 --> 00:38:58,480
requests because some people are waiting

00:38:55,840 --> 00:39:00,430
for for your translation to appear so it

00:38:58,480 --> 00:39:05,350
would be nice to to appear in in a

00:39:00,430 --> 00:39:07,720
decent time and since in to our previous

00:39:05,350 --> 00:39:10,510
solution the English to Spanish language

00:39:07,720 --> 00:39:12,360
pair was the most highly used one we

00:39:10,510 --> 00:39:15,250
were thinking that also for this

00:39:12,360 --> 00:39:17,710
adaptive machine translation it will be

00:39:15,250 --> 00:39:19,240
it was not the case we had more users on

00:39:17,710 --> 00:39:22,540
the English to French English to Dutch

00:39:19,240 --> 00:39:25,180
that then on English to two to Spanish

00:39:22,540 --> 00:39:28,060
so we had to adopt we had to adjust our

00:39:25,180 --> 00:39:30,420
our allocation of resources by lowering

00:39:28,060 --> 00:39:32,920
the English to Spanish resources and and

00:39:30,420 --> 00:39:39,370
giving more to the English French and

00:39:32,920 --> 00:39:42,430
English Dutch okay so even if we build

00:39:39,370 --> 00:39:44,800
we are building this platform for over

00:39:42,430 --> 00:39:46,930
two years now and we learn a lot of

00:39:44,800 --> 00:39:50,050
things we managed to fix a lot of things

00:39:46,930 --> 00:39:52,390
we are still I mean we realize that it's

00:39:50,050 --> 00:39:57,820
it's we still have a lot of things to do

00:39:52,390 --> 00:39:59,680
and for example the periodical upgrades

00:39:57,820 --> 00:40:00,369
of a stack you see that there are many

00:39:59,680 --> 00:40:03,220
things in the

00:40:00,369 --> 00:40:05,769
that it's quite a demanding task I mean

00:40:03,220 --> 00:40:08,559
it takes quite a lot in order to to keep

00:40:05,769 --> 00:40:11,170
up with the latest latest technology

00:40:08,559 --> 00:40:15,279
stack and we are periodically investing

00:40:11,170 --> 00:40:17,019
this time and try to minimize the the

00:40:15,279 --> 00:40:20,680
features that we are creating into the

00:40:17,019 --> 00:40:23,980
platform we still need to do

00:40:20,680 --> 00:40:26,230
improvements on the monitoring side even

00:40:23,980 --> 00:40:28,779
if we are at a decent level right now we

00:40:26,230 --> 00:40:32,140
we understand that we can do better than

00:40:28,779 --> 00:40:34,480
that from the beginning we realized that

00:40:32,140 --> 00:40:38,380
auto scaling would be quite a nice thing

00:40:34,480 --> 00:40:40,720
to do so right now as I mentioned in the

00:40:38,380 --> 00:40:43,059
previous framework in the previous

00:40:40,720 --> 00:40:46,630
solution sorry we had to manually add

00:40:43,059 --> 00:40:50,259
VMs ransom scripts had some DB scripts

00:40:46,630 --> 00:40:52,809
in order to accommodate an increase in

00:40:50,259 --> 00:40:55,059
in that deployment even if some of these

00:40:52,809 --> 00:40:57,220
things we were doing with scripts right

00:40:55,059 --> 00:40:59,559
now with a single click into into

00:40:57,220 --> 00:41:02,230
marathon you can scale your your micro

00:40:59,559 --> 00:41:04,390
services so it's quite easy but it still

00:41:02,230 --> 00:41:06,249
involves that somebody realized that we

00:41:04,390 --> 00:41:08,890
need to scale that micro service because

00:41:06,249 --> 00:41:12,369
the load on that specific engine it's

00:41:08,890 --> 00:41:14,829
now bigger so somebody needs to give

00:41:12,369 --> 00:41:17,950
that click we see also yesterday

00:41:14,829 --> 00:41:21,489
internet flicks presentation and a nice

00:41:17,950 --> 00:41:23,799
slide about how they do auto scaling we

00:41:21,489 --> 00:41:26,619
also have clients into the old solution

00:41:23,799 --> 00:41:30,039
that are coming like they translate for

00:41:26,619 --> 00:41:33,160
six months for six sorry so for six

00:41:30,039 --> 00:41:35,200
hours but a huge traffic after that for

00:41:33,160 --> 00:41:37,690
the whole day they don't come they don't

00:41:35,200 --> 00:41:41,440
come back they come only the next day so

00:41:37,690 --> 00:41:46,569
in this type of use cases doubt of

00:41:41,440 --> 00:41:49,299
scaling it it's really worth investing

00:41:46,569 --> 00:41:54,309
time in and because it can lower the

00:41:49,299 --> 00:41:57,339
cost quite significantly so also the

00:41:54,309 --> 00:42:00,309
maybe would be nice to say so the size

00:41:57,339 --> 00:42:04,539
of our mesos cluster it's between 10 and

00:42:00,309 --> 00:42:07,839
20 nodes which is not very huge for

00:42:04,539 --> 00:42:11,319
example into the old solution we have

00:42:07,839 --> 00:42:13,210
more than 3000 VMs that we accommodate

00:42:11,319 --> 00:42:13,990
into the production environment but

00:42:13,210 --> 00:42:17,560
these

00:42:13,990 --> 00:42:21,970
this so the the message part is mainly

00:42:17,560 --> 00:42:25,060
because we accommodated new features as

00:42:21,970 --> 00:42:27,220
as I mentioned adaptive NT which takes

00:42:25,060 --> 00:42:31,270
quite some time until people actually

00:42:27,220 --> 00:42:33,070
understood that idea are trying are

00:42:31,270 --> 00:42:35,110
trying it are seeing that they are

00:42:33,070 --> 00:42:38,530
benefits and they are adopting it so

00:42:35,110 --> 00:42:41,530
it's quite a process that it will take

00:42:38,530 --> 00:42:45,160
some time on the other hand we over the

00:42:41,530 --> 00:42:46,869
future we want to migrate even more

00:42:45,160 --> 00:42:47,920
clients from the whole solution to the

00:42:46,869 --> 00:42:50,860
to the new ones

00:42:47,920 --> 00:42:54,040
and then we will actually see a lot more

00:42:50,860 --> 00:42:57,720
traffic we still have a lot of

00:42:54,040 --> 00:42:59,710
components that are not in mesos

00:42:57,720 --> 00:43:04,180
especially I am referring here to the

00:42:59,710 --> 00:43:05,950
HBase HDFS Kafka and zookeeper currently

00:43:04,180 --> 00:43:09,460
all of them are actually managed by

00:43:05,950 --> 00:43:13,720
embody but we would like to actually

00:43:09,460 --> 00:43:17,860
migrate them into intimacies maybe into

00:43:13,720 --> 00:43:19,690
DCOs actually in order to be able to use

00:43:17,860 --> 00:43:22,060
the cluster is even more efficient so

00:43:19,690 --> 00:43:25,210
for example HBase is quite a CPU

00:43:22,060 --> 00:43:28,930
intensive for us and the machines that

00:43:25,210 --> 00:43:32,310
we allocated are are using the CPU

00:43:28,930 --> 00:43:36,220
eighty ninety percent and in the meses

00:43:32,310 --> 00:43:39,130
cluster we have some CPUs available so

00:43:36,220 --> 00:43:42,940
they would actually be a good fit to

00:43:39,130 --> 00:43:45,100
actually use the same resources but this

00:43:42,940 --> 00:43:48,880
migration requires quite quite some time

00:43:45,100 --> 00:43:51,490
so we for some time we we postpone it a

00:43:48,880 --> 00:43:54,010
little bit and there is also the elastic

00:43:51,490 --> 00:43:58,990
search but we which is still kept

00:43:54,010 --> 00:44:04,450
outside of our messes ok so let's see

00:43:58,990 --> 00:44:12,700
now a demo I actually recorded this part

00:44:04,450 --> 00:44:16,240
in order not to have surprises ok I will

00:44:12,700 --> 00:44:18,670
use it without without voice and I will

00:44:16,240 --> 00:44:21,280
try to explain so we will start our our

00:44:18,670 --> 00:44:23,680
demo from the marathon why we have here

00:44:21,280 --> 00:44:27,160
an English to Dutch engine that

00:44:23,680 --> 00:44:27,940
currently has one instance so one micro

00:44:27,160 --> 00:44:33,970
services

00:44:27,940 --> 00:44:37,270
that is healthy then we see here in that

00:44:33,970 --> 00:44:41,260
actually this is a our QA cluster of of

00:44:37,270 --> 00:44:44,110
messes we have nine nine slaves and a

00:44:41,260 --> 00:44:46,440
lot of micro service is deployed and we

00:44:44,110 --> 00:44:51,310
will use graph on ax in order to see

00:44:46,440 --> 00:44:52,840
some numbers on actually how the things

00:44:51,310 --> 00:44:54,730
are going into the platform

00:44:52,840 --> 00:44:57,010
especially the number of translation

00:44:54,730 --> 00:44:59,500
requests that are handled the number of

00:44:57,010 --> 00:45:03,040
translation words and the CPU cluster

00:44:59,500 --> 00:45:05,140
utilization I will use I will use J

00:45:03,040 --> 00:45:08,110
meter in order to create some load on

00:45:05,140 --> 00:45:09,970
the on the platform and we in this I

00:45:08,110 --> 00:45:13,330
already have a predefined script which

00:45:09,970 --> 00:45:16,120
has Ana synchronization flow defined

00:45:13,330 --> 00:45:18,130
which does one translation it waits for

00:45:16,120 --> 00:45:20,350
the translation to finish and it makes

00:45:18,130 --> 00:45:23,290
it to retrieve the actual the actual

00:45:20,350 --> 00:45:25,920
translated content content I will start

00:45:23,290 --> 00:45:28,090
the script and we see now that

00:45:25,920 --> 00:45:31,300
translation are star are already

00:45:28,090 --> 00:45:35,890
happening we are going back into the

00:45:31,300 --> 00:45:37,540
graph an IUI we see here that the number

00:45:35,890 --> 00:45:39,370
of translation request increased

00:45:37,540 --> 00:45:41,470
actually the first thing that we see is

00:45:39,370 --> 00:45:44,350
that the CPU cluster utilization

00:45:41,470 --> 00:45:50,890
increased from zero actually one to two

00:45:44,350 --> 00:45:52,510
to eleven ten eleven let's say we see

00:45:50,890 --> 00:45:56,490
that also the number of translation

00:45:52,510 --> 00:45:58,600
requests stabilized around 120

00:45:56,490 --> 00:46:01,060
translation requests per second so I

00:45:58,600 --> 00:46:02,890
came back after five minutes in order to

00:46:01,060 --> 00:46:04,990
leave the number to actually stabilize

00:46:02,890 --> 00:46:08,170
and we see here that the cluster

00:46:04,990 --> 00:46:10,720
utilization is around ten percent the

00:46:08,170 --> 00:46:12,760
number of requests are around 120

00:46:10,720 --> 00:46:14,950
requests per second we go back into the

00:46:12,760 --> 00:46:19,690
marathon and we scale the the

00:46:14,950 --> 00:46:21,700
microservices to two instances and we

00:46:19,690 --> 00:46:23,710
see that it enters in deployment quite

00:46:21,700 --> 00:46:27,790
quickly and in a few seconds we will

00:46:23,710 --> 00:46:31,480
also see that the GPU cluster

00:46:27,790 --> 00:46:35,500
utilization increases actually increase

00:46:31,480 --> 00:46:37,210
to to approximately 20 percent and

00:46:35,500 --> 00:46:39,760
gradually we will see that also the

00:46:37,210 --> 00:46:41,470
numbers the the translation request the

00:46:39,760 --> 00:46:44,080
translated the translate

00:46:41,470 --> 00:46:46,390
were also increase it we will come back

00:46:44,080 --> 00:46:48,130
also in this case after three minister

00:46:46,390 --> 00:46:51,070
so that numbers are are actually

00:46:48,130 --> 00:46:56,770
reaching a stable point we will see that

00:46:51,070 --> 00:47:01,840
we increase for 120 to approximately 240

00:46:56,770 --> 00:47:06,730
per translation that are happening in a

00:47:01,840 --> 00:47:10,060
second and we will now go back into into

00:47:06,730 --> 00:47:15,520
marathon and we will scale the

00:47:10,060 --> 00:47:17,619
microservices to ten instances we see

00:47:15,520 --> 00:47:22,720
that they will enter in a deployment

00:47:17,619 --> 00:47:25,210
state so gradually they will will come

00:47:22,720 --> 00:47:28,320
healthy so in when they are becoming

00:47:25,210 --> 00:47:30,970
healthy they are and they are actually

00:47:28,320 --> 00:47:32,890
added to the to the load balancer so

00:47:30,970 --> 00:47:41,200
they actually start to receive some

00:47:32,890 --> 00:47:43,750
traffic and we will then go into into

00:47:41,200 --> 00:47:47,109
the graph owner and see that the cluster

00:47:43,750 --> 00:47:50,380
utilization starts to increase that is

00:47:47,109 --> 00:47:53,020
tied to to the number of of instances

00:47:50,380 --> 00:47:54,640
that are are brought up we will come

00:47:53,020 --> 00:47:57,390
back also in this case after five

00:47:54,640 --> 00:48:01,780
minutes to live the numbers to actually

00:47:57,390 --> 00:48:04,290
reach stable point and we see first the

00:48:01,780 --> 00:48:07,000
the cluster utilization it's around

00:48:04,290 --> 00:48:10,900
ninety four percent nineteen ninety four

00:48:07,000 --> 00:48:14,050
ninety one percent and we reach 1000

00:48:10,900 --> 00:48:16,510
translation requests per second with

00:48:14,050 --> 00:48:19,980
these ten instances of this micro

00:48:16,510 --> 00:48:23,470
service and it's quite important to

00:48:19,980 --> 00:48:27,490
notice also the number of translated

00:48:23,470 --> 00:48:29,589
word increased it we will look now into

00:48:27,490 --> 00:48:32,290
perspective of what happened over the

00:48:29,589 --> 00:48:38,020
course of the last thirty thirty minutes

00:48:32,290 --> 00:48:41,380
and now we see that so we started from

00:48:38,020 --> 00:48:45,070
zero not having any translation in the

00:48:41,380 --> 00:48:47,880
system then we we reach 120 with one

00:48:45,070 --> 00:48:50,440
instance 240 with two instances and

00:48:47,880 --> 00:48:53,440
approximately one thousand with ten

00:48:50,440 --> 00:48:55,119
instances we also translate from zero

00:48:53,440 --> 00:48:58,420
words per second to

00:48:55,119 --> 00:49:00,849
thirty thousand words per second then to

00:48:58,420 --> 00:49:05,890
sixty thousand words per second and then

00:49:00,849 --> 00:49:09,390
to 240 words per second and we also saw

00:49:05,890 --> 00:49:14,019
that the cluster GPU cluster utilization

00:49:09,390 --> 00:49:16,719
stabilized around 91% if we go back into

00:49:14,019 --> 00:49:20,109
the meter we see that we have we did

00:49:16,719 --> 00:49:23,589
during the course of 25 minutes more

00:49:20,109 --> 00:49:26,529
than 800,000 translation requests with

00:49:23,589 --> 00:49:30,309
with no actually with very few failures

00:49:26,529 --> 00:49:33,369
one failure resulted either so this this

00:49:30,309 --> 00:49:37,299
was actually the demo that I I wanted to

00:49:33,369 --> 00:49:44,349
show you so now if you have if you have

00:49:37,299 --> 00:49:54,039
any questions please ask I will try to

00:49:44,349 --> 00:49:56,619
not move sorry as you're scaling it up I

00:49:54,039 --> 00:49:59,559
noticed that you didn't have any labels

00:49:56,619 --> 00:50:04,119
or in marathons so what you're using to

00:49:59,559 --> 00:50:07,299
your load balancing okay so we built a

00:50:04,119 --> 00:50:09,969
script that actually a bridge between AJ

00:50:07,299 --> 00:50:14,349
proxy and a marathon so we are exposing

00:50:09,969 --> 00:50:16,599
our our let's say our front-end via our

00:50:14,349 --> 00:50:20,019
endpoints are actually getting the

00:50:16,599 --> 00:50:23,049
request via the edge a proxy layer and

00:50:20,019 --> 00:50:24,910
practically when it's a custom script

00:50:23,049 --> 00:50:27,160
when we started two three years ago the

00:50:24,910 --> 00:50:29,559
marathon elbe and all the stuff that are

00:50:27,160 --> 00:50:32,819
not available were quite not available

00:50:29,559 --> 00:50:35,890
so we had to do this part quite by hand

00:50:32,819 --> 00:50:38,650
and it also contains a little bit of

00:50:35,890 --> 00:50:41,529
logic and in which we decide how to

00:50:38,650 --> 00:50:44,109
actually route our request but more or

00:50:41,529 --> 00:50:46,869
less is not something really fancy it

00:50:44,109 --> 00:50:49,119
enters into AJ proxy that decide where

00:50:46,869 --> 00:50:52,569
to send the request approximately we

00:50:49,119 --> 00:50:56,789
have an algorithm draw beam it's a cross

00:50:52,569 --> 00:50:56,789
custom implementation of round robin

00:50:59,510 --> 00:51:11,610
any other questions on your future slide

00:51:07,980 --> 00:51:20,250
you showed that you also want to bring

00:51:11,610 --> 00:51:21,090
HBase etc into a sauce it's a really

00:51:20,250 --> 00:51:24,840
good question

00:51:21,090 --> 00:51:26,640
I actually while being these two days of

00:51:24,840 --> 00:51:28,590
the presentation I actually try to ask

00:51:26,640 --> 00:51:31,800
many people how they are actually doing

00:51:28,590 --> 00:51:35,970
it I don't have actually an answer for

00:51:31,800 --> 00:51:39,120
that I see that the HDFS is already in

00:51:35,970 --> 00:51:41,280
into this us I actually see that some

00:51:39,120 --> 00:51:43,590
people on github it they already did

00:51:41,280 --> 00:51:46,140
some some implementation and also the

00:51:43,590 --> 00:51:47,940
guy from port works I think they they

00:51:46,140 --> 00:51:53,550
they told me that they have something to

00:51:47,940 --> 00:51:56,520
look into on how they are porting HBase

00:51:53,550 --> 00:51:58,830
into a meses cluster so I don't have I

00:51:56,520 --> 00:52:00,930
have a lot of tracks that I want to

00:51:58,830 --> 00:52:12,750
follow I don't have unfortunately a

00:52:00,930 --> 00:52:17,130
simple answer ok any other questions if

00:52:12,750 --> 00:52:21,810
not you can find me around here and yeah

00:52:17,130 --> 00:52:24,720
I also if you want I you can send me

00:52:21,810 --> 00:52:27,420
messages on the Twitter part or you can

00:52:24,720 --> 00:52:29,970
contact me in any way and I will try to

00:52:27,420 --> 00:52:31,010
answer your questions ok thank you very

00:52:29,970 --> 00:52:34,249
much

00:52:31,010 --> 00:52:34,249

YouTube URL: https://www.youtube.com/watch?v=k9kA4LJWGb8


