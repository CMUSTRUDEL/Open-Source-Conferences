Title: Building FAST Data Solutions with DC OS on Azure - Rob Bagby, Microsoft
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Building FAST Data Solutions with DC/OS on Azure - Rob Bagby, Microsoft

In this session, we will illustrate how to develop, deploy and manage FAST data solutions at scale on DC/OS and Azure. We will discuss the challenges of stateful containers in the cloud and provide guidance on how to implement both Cassandra and Kafka in DC/OS. We will further discuss how to manage your deployed solution with a partner solution. This session will be demo heavy, illustrating how to develop Cassandra and Kafka applications locally, run them at scale in DC/OS and manage them with a 3rd party workflow solution.
Captions: 
	00:00:00,030 --> 00:00:04,259
my name is Rob I'm an engineer at

00:00:02,070 --> 00:00:05,970
Microsoft my role is I work with

00:00:04,259 --> 00:00:08,429
partners in the container space

00:00:05,970 --> 00:00:10,170
specifically I work with them to enable

00:00:08,429 --> 00:00:11,969
them on the platform to make them run

00:00:10,170 --> 00:00:13,380
more efficiently

00:00:11,969 --> 00:00:16,220
today I want to talk to you about

00:00:13,380 --> 00:00:18,390
building fast data solutions with gos

00:00:16,220 --> 00:00:20,220
I'm gonna show you on Azure I'm not a

00:00:18,390 --> 00:00:21,810
sales guy I don't hold any quotas so I'm

00:00:20,220 --> 00:00:23,220
not gonna be trying to sell Asher but if

00:00:21,810 --> 00:00:25,050
you do choose to run on Azure there are

00:00:23,220 --> 00:00:27,230
a lot of good reasons to do so I'll give

00:00:25,050 --> 00:00:29,070
you some tips on how to do that

00:00:27,230 --> 00:00:31,769
hopefully you're familiar with what fast

00:00:29,070 --> 00:00:33,800
data is in the smack stack if you're not

00:00:31,769 --> 00:00:36,630
I'll give you a very high level overview

00:00:33,800 --> 00:00:38,340
fast data solutions are the typical IOT

00:00:36,630 --> 00:00:41,219
solutions but there's a time crew she

00:00:38,340 --> 00:00:43,200
ality involved and I'll give you a good

00:00:41,219 --> 00:00:45,719
example I have a customer that I worked

00:00:43,200 --> 00:00:47,910
with that they have those gates that as

00:00:45,719 --> 00:00:50,910
you exit stores if you haven't paid for

00:00:47,910 --> 00:00:52,710
an item they alarm and the way those

00:00:50,910 --> 00:00:55,170
things work is their little RF chips

00:00:52,710 --> 00:00:57,300
that they put in the tags on the

00:00:55,170 --> 00:00:59,789
whatever product you're looking to

00:00:57,300 --> 00:01:01,350
purchase and as you exit as you hit that

00:00:59,789 --> 00:01:03,660
sensor the sensor determines the

00:01:01,350 --> 00:01:04,890
direction you're moving in if it detects

00:01:03,660 --> 00:01:07,380
that you're moving towards the exit

00:01:04,890 --> 00:01:10,080
towards you're leaving it sends an event

00:01:07,380 --> 00:01:11,520
up to the cloud and then some

00:01:10,080 --> 00:01:13,530
computations are done to see if you

00:01:11,520 --> 00:01:15,360
purchased that product or not and then

00:01:13,530 --> 00:01:17,939
if you haven't an event gets back

00:01:15,360 --> 00:01:20,220
sitting back down to that sensor and the

00:01:17,939 --> 00:01:23,100
sensor alarms now you can imagine if

00:01:20,220 --> 00:01:25,409
that if that round trip took longer than

00:01:23,100 --> 00:01:28,200
say a couple hundred milliseconds if it

00:01:25,409 --> 00:01:30,210
took say 20 seconds that it would be

00:01:28,200 --> 00:01:32,520
valueless the person would have already

00:01:30,210 --> 00:01:35,850
been in their car driving away with

00:01:32,520 --> 00:01:38,130
whatever they've pinched right so that's

00:01:35,850 --> 00:01:40,470
that time sensitive nature now there's

00:01:38,130 --> 00:01:42,270
not just one of those one of those gates

00:01:40,470 --> 00:01:44,610
in the store there's probably five or

00:01:42,270 --> 00:01:45,960
six per store there may be a thousand

00:01:44,610 --> 00:01:48,090
stores and there are a couple you know

00:01:45,960 --> 00:01:49,740
maybe doesn't people walking through any

00:01:48,090 --> 00:01:51,780
gate at any point time so that's

00:01:49,740 --> 00:01:53,250
hundreds of thousands of events that are

00:01:51,780 --> 00:01:56,579
going on per second so that's kind of

00:01:53,250 --> 00:01:59,219
the IOT nature of fast data okay this

00:01:56,579 --> 00:02:03,509
Mac stack you're likely very familiar

00:01:59,219 --> 00:02:05,850
with but spark czaka Cassandra and cough

00:02:03,509 --> 00:02:08,869
a spark is distributed processing engine

00:02:05,850 --> 00:02:11,280
where you can do ML data processing

00:02:08,869 --> 00:02:13,220
mezzos is the reason why we're all here

00:02:11,280 --> 00:02:15,530
it's basically can act as a car

00:02:13,220 --> 00:02:18,880
for your data center or cluster of

00:02:15,530 --> 00:02:20,870
servers akka is is an actor based

00:02:18,880 --> 00:02:22,760
framework for building distributed

00:02:20,870 --> 00:02:25,790
applications cassandra is a linearly

00:02:22,760 --> 00:02:28,940
scalable no sequel database and Kafka is

00:02:25,790 --> 00:02:30,850
a linearly scalable event buffer so in

00:02:28,940 --> 00:02:34,070
this talk I'm going to be talking about

00:02:30,850 --> 00:02:35,360
mezzos Kafka and Cassandra specifically

00:02:34,070 --> 00:02:39,260
I'm going to be talking about DCOs

00:02:35,360 --> 00:02:41,150
around mezzos sessions sessions demo

00:02:39,260 --> 00:02:44,420
driven very dumb it's about half demo

00:02:41,150 --> 00:02:47,000
half slides live demos could fail all

00:02:44,420 --> 00:02:49,520
the you know hail to the demo gods blah

00:02:47,000 --> 00:02:50,300
blah blah so because it's demo driven I

00:02:49,520 --> 00:02:52,430
have to talk to you about the

00:02:50,300 --> 00:02:54,200
application I wrote for the demo okay

00:02:52,430 --> 00:02:56,540
imagine you have a bunch of sensors that

00:02:54,200 --> 00:02:58,070
are writing into Kafka now I didn't want

00:02:56,540 --> 00:03:00,890
to create a bunch of instances of those

00:02:58,070 --> 00:03:03,200
sensors so I created a something called

00:03:00,890 --> 00:03:05,660
a producer which acts like it's a bunch

00:03:03,200 --> 00:03:08,209
of sensors and it's writing to Kafka

00:03:05,660 --> 00:03:08,780
specifically to a topic called sensor

00:03:08,209 --> 00:03:14,330
temp

00:03:08,780 --> 00:03:17,420
okay now Kafka Kafka is is partitioned

00:03:14,330 --> 00:03:19,459
okay and so depending upon the key for

00:03:17,420 --> 00:03:21,200
my event it's gonna get written to a

00:03:19,459 --> 00:03:23,300
specific partition okay and the

00:03:21,200 --> 00:03:26,450
partitioning is why Kafka is linearly

00:03:23,300 --> 00:03:29,540
scalable okay so I've got to contain a

00:03:26,450 --> 00:03:31,430
consumer component that I wrote and it

00:03:29,540 --> 00:03:34,010
subscribes to the topic as a whole

00:03:31,430 --> 00:03:37,070
okay now that consumer can be assigned

00:03:34,010 --> 00:03:39,410
to one or more of those partitions okay

00:03:37,070 --> 00:03:41,090
when an event comes in on a partition

00:03:39,410 --> 00:03:43,280
that it's assigned to I'm going to

00:03:41,090 --> 00:03:46,310
capture that event and I'm gonna write

00:03:43,280 --> 00:03:48,799
some data into Cassandra makes sense so

00:03:46,310 --> 00:03:50,450
far gets a little bit more complex but

00:03:48,799 --> 00:03:52,760
not much more now you remember I

00:03:50,450 --> 00:03:54,640
mentioned that a consumer can be

00:03:52,760 --> 00:03:58,820
assigned to one or more partitions in

00:03:54,640 --> 00:03:59,870
fact when I have one consumer it's going

00:03:58,820 --> 00:04:02,269
to be assigned to all the partitions

00:03:59,870 --> 00:04:04,549
okay but it's not going to read off all

00:04:02,269 --> 00:04:05,630
the partitions at once so what it's

00:04:04,549 --> 00:04:07,670
going to do is it's going to be reading

00:04:05,630 --> 00:04:10,220
off of a partition and it's gonna get

00:04:07,670 --> 00:04:12,350
back a block of data maybe 50 events and

00:04:10,220 --> 00:04:14,600
number of events at a time okay

00:04:12,350 --> 00:04:15,920
the interesting thing about Kafka one of

00:04:14,600 --> 00:04:17,690
the challenges you'll find working with

00:04:15,920 --> 00:04:20,570
kapha is it doesn't really have this

00:04:17,690 --> 00:04:22,490
notion of how far behind on I on the

00:04:20,570 --> 00:04:24,320
queue as a whole or on that topic as a

00:04:22,490 --> 00:04:25,430
whole so you've seen things like you

00:04:24,320 --> 00:04:27,840
depth

00:04:25,430 --> 00:04:29,820
there's that that concept doesn't really

00:04:27,840 --> 00:04:32,360
exist within Kafka the way it works in

00:04:29,820 --> 00:04:35,370
Kafka is when I go to pull from a

00:04:32,360 --> 00:04:37,440
partition one of those n partitions I

00:04:35,370 --> 00:04:39,630
get back some metadata along with those

00:04:37,440 --> 00:04:41,340
events and that metadata tells me how

00:04:39,630 --> 00:04:45,000
far behind I am on that particular

00:04:41,340 --> 00:04:46,890
partition okay so I may be just pulling

00:04:45,000 --> 00:04:50,010
from the first partition and it says I'm

00:04:46,890 --> 00:04:52,440
50 or 500 behind in that partition but I

00:04:50,010 --> 00:04:56,160
have no notion of partitions 2 through X

00:04:52,440 --> 00:04:57,990
how far behind I am ok makes sense so

00:04:56,160 --> 00:04:59,220
far so what I wanted to do is I wanted

00:04:57,990 --> 00:05:02,130
to be able to handle that because I want

00:04:59,220 --> 00:05:04,650
to know how to scale I want to know how

00:05:02,130 --> 00:05:06,480
far behind I am on these in my partition

00:05:04,650 --> 00:05:08,340
so I can scale my consumers so what I

00:05:06,480 --> 00:05:11,100
decided to do is when I have a 2nd

00:05:08,340 --> 00:05:13,020
consumer and that rebalance occurs or n

00:05:11,100 --> 00:05:15,240
number of consumers every time a

00:05:13,020 --> 00:05:16,680
consumer gets back that metadata is

00:05:15,240 --> 00:05:19,710
saying how far it is behind on its

00:05:16,680 --> 00:05:22,860
partition I basically write to another

00:05:19,710 --> 00:05:24,540
topic saying hey I'm partition 1 I'm 500

00:05:22,860 --> 00:05:27,750
behind another one maybe writing on

00:05:24,540 --> 00:05:29,220
partition 3 I'm 50 behind and then I've

00:05:27,750 --> 00:05:31,290
got another component that's listening

00:05:29,220 --> 00:05:34,350
off of that that calculates the

00:05:31,290 --> 00:05:38,370
aggregate how far am i behind as a whole

00:05:34,350 --> 00:05:39,960
makes sense now it's not altogether 100%

00:05:38,370 --> 00:05:41,490
accurate because as I mentioned if

00:05:39,960 --> 00:05:42,840
you're not getting back data from

00:05:41,490 --> 00:05:45,300
partitions if they're partitions that

00:05:42,840 --> 00:05:47,190
are unread you're behind on them but you

00:05:45,300 --> 00:05:48,930
don't know that but we'll see how we can

00:05:47,190 --> 00:05:50,880
handle that a little bit later once we

00:05:48,930 --> 00:05:54,960
have that aggregate lag once I know how

00:05:50,880 --> 00:05:56,880
far back behind I am on my on my topic

00:05:54,960 --> 00:05:58,770
as a whole I'm gonna write that to a

00:05:56,880 --> 00:06:00,150
workflow engine and we're gonna see what

00:05:58,770 --> 00:06:01,800
that looks like later and we're gonna

00:06:00,150 --> 00:06:04,890
use that workflow engine to auto scale

00:06:01,800 --> 00:06:07,620
out my consumers make sense that's the

00:06:04,890 --> 00:06:09,600
scenario so what I'm going to illustrate

00:06:07,620 --> 00:06:12,660
I tend to think of container based

00:06:09,600 --> 00:06:13,920
solutions and in layers starting with

00:06:12,660 --> 00:06:16,380
just the containers themselves

00:06:13,920 --> 00:06:18,240
what do containers buy for me what do I

00:06:16,380 --> 00:06:20,670
get out of containers and how do they

00:06:18,240 --> 00:06:23,010
help me in this case and the in the

00:06:20,670 --> 00:06:24,930
context of this talk build fast data

00:06:23,010 --> 00:06:26,640
solutions so I'll start with that we

00:06:24,930 --> 00:06:27,810
won't spend a ton of time on that then

00:06:26,640 --> 00:06:30,420
we're gonna talk about running at scale

00:06:27,810 --> 00:06:33,330
what's the what do the orchestrator

00:06:30,420 --> 00:06:36,120
specifically DCOs give me what I'm

00:06:33,330 --> 00:06:37,740
building fast data solutions and because

00:06:36,120 --> 00:06:38,910
these are persistent or stateful

00:06:37,740 --> 00:06:40,440
workloads how

00:06:38,910 --> 00:06:42,390
I handle data persistence and I'm going

00:06:40,440 --> 00:06:44,010
to talk about using a product called

00:06:42,390 --> 00:06:46,080
port works to solve a lot of the

00:06:44,010 --> 00:06:47,700
challenges that you have when working

00:06:46,080 --> 00:06:50,160
with persistent workloads and I'll

00:06:47,700 --> 00:06:51,480
outline what those challenges are I'll

00:06:50,160 --> 00:06:52,950
talk about how port works is going to

00:06:51,480 --> 00:06:54,960
resolve those and then I'm going to

00:06:52,950 --> 00:06:56,070
touch on someday to operations how many

00:06:54,960 --> 00:07:00,120
of you guys have seen any of the talks

00:06:56,070 --> 00:07:02,460
that have dealt with vamp several have

00:07:00,120 --> 00:07:04,380
you have they were talking about canary

00:07:02,460 --> 00:07:07,320
releasing in vamp I'm going to be using

00:07:04,380 --> 00:07:09,180
vamp to do micro scaling so I'm going to

00:07:07,320 --> 00:07:11,010
use vamp to build a workflow that's

00:07:09,180 --> 00:07:13,530
going to do scaling that's going to

00:07:11,010 --> 00:07:17,480
scale out given the lag that I'm showing

00:07:13,530 --> 00:07:20,700
from from Kafka and that's about it

00:07:17,480 --> 00:07:23,580
so let's jump into the local development

00:07:20,700 --> 00:07:24,990
aspect just what containers do and when

00:07:23,580 --> 00:07:28,700
we're talking about from the perspective

00:07:24,990 --> 00:07:30,930
of fast data solutions ir-2 containers

00:07:28,700 --> 00:07:32,670
how would you how would you go about

00:07:30,930 --> 00:07:34,320
developing against Cassandra or Kafka

00:07:32,670 --> 00:07:37,010
you have to either install it yourself

00:07:34,320 --> 00:07:40,020
or you'd have a shared solution right

00:07:37,010 --> 00:07:42,960
and you know you'd be sharing with other

00:07:40,020 --> 00:07:45,450
people a pooled solution neither which

00:07:42,960 --> 00:07:47,490
are very good solutions installing it

00:07:45,450 --> 00:07:49,680
wasted calories very difficult to do

00:07:47,490 --> 00:07:51,240
challenging and then using a pooled

00:07:49,680 --> 00:07:52,290
solution other people can step on what

00:07:51,240 --> 00:07:55,680
you've got they can delete your data

00:07:52,290 --> 00:07:57,840
they can and it's not portable you just

00:07:55,680 --> 00:07:59,160
can't run on your laptop if it's if it's

00:07:57,840 --> 00:08:00,840
shared if you're running on a plane how

00:07:59,160 --> 00:08:02,610
does it work today it's as easy as

00:08:00,840 --> 00:08:05,070
running a container up you spin up the

00:08:02,610 --> 00:08:07,730
container develop against it and I'll

00:08:05,070 --> 00:08:10,470
show you how to do that in just a second

00:08:07,730 --> 00:08:14,460
containers give you beyond just the

00:08:10,470 --> 00:08:18,330
ability to to run things like Kafka or

00:08:14,460 --> 00:08:20,400
Cassandra and to me the the attribute of

00:08:18,330 --> 00:08:22,050
a container that really gives you

00:08:20,400 --> 00:08:24,510
everything is the fact that it

00:08:22,050 --> 00:08:27,540
encapsulates its dependencies that that

00:08:24,510 --> 00:08:29,250
nature that fact makes sure that you

00:08:27,540 --> 00:08:30,900
know that something runs on your machine

00:08:29,250 --> 00:08:34,169
it's going to run the same as it does in

00:08:30,900 --> 00:08:36,810
production on dev on staging and that

00:08:34,169 --> 00:08:39,810
feature also allows you to get density

00:08:36,810 --> 00:08:42,240
or dense workloads in your and your in

00:08:39,810 --> 00:08:43,560
your data center or in the cloud so if

00:08:42,240 --> 00:08:45,390
you don't know what I mean by that I've

00:08:43,560 --> 00:08:46,560
built a slide to kind of illustrate that

00:08:45,390 --> 00:08:48,450
the left-hand side of this slide

00:08:46,560 --> 00:08:49,980
illustrates kind of the old world we've

00:08:48,450 --> 00:08:51,690
got a hypervisor and a couple of yemm's

00:08:49,980 --> 00:08:52,770
running on it and the VMS are

00:08:51,690 --> 00:08:54,750
color-coded

00:08:52,770 --> 00:08:57,029
for indicating what dependencies they

00:08:54,750 --> 00:08:58,890
have on them okay and in the center here

00:08:57,029 --> 00:09:00,839
you've got these applications that are

00:08:58,890 --> 00:09:02,790
color-coded to determine what applicate

00:09:00,839 --> 00:09:05,100
what dependencies they require so you

00:09:02,790 --> 00:09:08,399
can see the dark green apps can run

00:09:05,100 --> 00:09:10,230
nicely on the VM that has the dark green

00:09:08,399 --> 00:09:12,839
dependencies installed on it and the

00:09:10,230 --> 00:09:14,430
slightly slight lighter green can run on

00:09:12,839 --> 00:09:15,899
those but I have all these apps sitting

00:09:14,430 --> 00:09:18,209
in the center they can't run anywhere

00:09:15,899 --> 00:09:19,920
and maybe these two servers these two

00:09:18,209 --> 00:09:23,490
VMs up here maybe they're only running

00:09:19,920 --> 00:09:25,980
at 10% CPU 10% memory but yet I've got

00:09:23,490 --> 00:09:29,040
all these unused workloads here so if

00:09:25,980 --> 00:09:31,260
you look really closely there you're

00:09:29,040 --> 00:09:34,770
gonna have to look really closely I put

00:09:31,260 --> 00:09:36,120
a very thin line around those boxes to

00:09:34,770 --> 00:09:39,330
indicate that they're now containerized

00:09:36,120 --> 00:09:41,820
okay and what that means is that the

00:09:39,330 --> 00:09:44,250
dependencies are now encapsulated inside

00:09:41,820 --> 00:09:46,560
of those containers and so now they can

00:09:44,250 --> 00:09:48,300
all run on any server that has a

00:09:46,560 --> 00:09:51,240
container engine running on it so now

00:09:48,300 --> 00:09:53,910
maybe this guy's running at 60 70 %

00:09:51,240 --> 00:09:55,980
capacity and that's the attribute of

00:09:53,910 --> 00:09:57,120
dependency encapsulation of the

00:09:55,980 --> 00:10:01,079
containers they enable that

00:09:57,120 --> 00:10:04,700
functionality so let's take a quick demo

00:10:01,079 --> 00:10:09,120
on developing locally to developing

00:10:04,700 --> 00:10:10,500
these fast data solutions locally so to

00:10:09,120 --> 00:10:12,240
take a look in here you can see I've run

00:10:10,500 --> 00:10:14,399
a docker PS I've got Kafka up and

00:10:12,240 --> 00:10:16,290
running okay and you can also see that

00:10:14,399 --> 00:10:18,959
I've just done a list of my of my

00:10:16,290 --> 00:10:21,420
networks okay now the easiest way for me

00:10:18,959 --> 00:10:23,850
to run up Cassandra is for me to just

00:10:21,420 --> 00:10:26,490
run a docker container so what I've done

00:10:23,850 --> 00:10:28,170
is if you if you want to run any of this

00:10:26,490 --> 00:10:30,480
code that you're gonna see later the

00:10:28,170 --> 00:10:32,850
producer the lag reader the consumer and

00:10:30,480 --> 00:10:37,320
Eve that's all this code is out on my

00:10:32,850 --> 00:10:39,570
github it's out here it it's out here at

00:10:37,320 --> 00:10:43,020
github Compaq Rob bagg BDCs Kafka

00:10:39,570 --> 00:10:46,020
Cassandra okay what I've got here under

00:10:43,020 --> 00:10:48,980
docs is also some nice guidance on how

00:10:46,020 --> 00:10:51,240
to run both Cassandra and Kafka

00:10:48,980 --> 00:10:52,800
containerized because there's ton of

00:10:51,240 --> 00:10:53,910
articles out there and they all kind of

00:10:52,800 --> 00:10:55,980
drive you in different directions it's

00:10:53,910 --> 00:10:57,899
actually kind of a bit painful to do but

00:10:55,980 --> 00:11:00,540
it talks about the why we need to set up

00:10:57,899 --> 00:11:02,640
a network and then gives me the command

00:11:00,540 --> 00:11:04,230
to run either docker for Windows or on

00:11:02,640 --> 00:11:06,890
Linux so I'm just gonna go ahead and

00:11:04,230 --> 00:11:06,890
just copy this command

00:11:07,399 --> 00:11:14,600
I'm gonna run it see now I'm running

00:11:12,360 --> 00:11:14,600
kiss

00:11:27,700 --> 00:11:33,720
yeah I guess we guess we'll have to

00:11:29,560 --> 00:11:33,720
sketch skip the local demo Oh

00:11:41,399 --> 00:11:47,290
or not we can just not panic okay so now

00:11:44,950 --> 00:11:49,630
cassandra is up and running so you can

00:11:47,290 --> 00:11:51,279
either trust me or as my father told me

00:11:49,630 --> 00:11:53,139
when I left for college trust no one

00:11:51,279 --> 00:11:54,760
so if some of you are like my father and

00:11:53,139 --> 00:11:56,050
you're not going to trust me I've got

00:11:54,760 --> 00:11:59,709
I've written a little utility to

00:11:56,050 --> 00:12:01,720
illustrate how you can how you can read

00:11:59,709 --> 00:12:03,160
and write to Cassandra so if you go back

00:12:01,720 --> 00:12:05,320
out to that little help page that I've

00:12:03,160 --> 00:12:06,880
got you can see I've got a test

00:12:05,320 --> 00:12:07,930
container it's good little Cassandra

00:12:06,880 --> 00:12:14,980
tester and I'll show you the code in a

00:12:07,930 --> 00:12:18,060
second but let's just run it up I think

00:12:14,980 --> 00:12:18,060
you Sandra tester is up and running

00:12:18,519 --> 00:12:22,489
okay if we take a look we've got our

00:12:20,720 --> 00:12:25,509
little Casandra tester and if I do it

00:12:22,489 --> 00:12:25,509
doctor logs

00:12:29,850 --> 00:12:34,270
you can see that every three seconds

00:12:32,350 --> 00:12:36,070
it's gonna be writing a message and then

00:12:34,270 --> 00:12:38,410
it's gonna be reading a message back at

00:12:36,070 --> 00:12:40,660
reading the last ten events back from

00:12:38,410 --> 00:12:42,790
Cassandra right so I've got Cassandra up

00:12:40,660 --> 00:12:45,850
and running now let's go ahead and stop

00:12:42,790 --> 00:12:47,350
this now when I'm working with Cassandra

00:12:45,850 --> 00:12:49,210
I want to work locally so I want to use

00:12:47,350 --> 00:12:52,900
all the tools that I'm normally using if

00:12:49,210 --> 00:12:56,290
you took a look at the Cassandra the

00:12:52,900 --> 00:13:01,720
docker run you can see that I mapped

00:12:56,290 --> 00:13:06,610
port 90 42 let's take a look here you

00:13:01,720 --> 00:13:08,770
can see right here that I mapped port 90

00:13:06,610 --> 00:13:10,900
42 on my host down to 90 42 in the

00:13:08,770 --> 00:13:13,750
container benefit of that is I can use

00:13:10,900 --> 00:13:15,760
anything like did a sex dead center to

00:13:13,750 --> 00:13:18,550
just directly connect to my local

00:13:15,760 --> 00:13:21,070
machine open the connection and then I

00:13:18,550 --> 00:13:25,060
can you start running and developing

00:13:21,070 --> 00:13:26,440
against that cluster make sense okay so

00:13:25,060 --> 00:13:28,270
now I've got Cassandra running I've got

00:13:26,440 --> 00:13:29,740
coffee up and running locally and I want

00:13:28,270 --> 00:13:31,540
to start building my fast data solution

00:13:29,740 --> 00:13:33,490
remember I've got my producer that's

00:13:31,540 --> 00:13:35,400
writing events to kafka consumer that's

00:13:33,490 --> 00:13:38,380
reading and it's also writing out to

00:13:35,400 --> 00:13:41,410
another lag and then I've got a lag

00:13:38,380 --> 00:13:43,930
reader so all that code again is out of

00:13:41,410 --> 00:13:46,030
that github I've got it locally here you

00:13:43,930 --> 00:13:47,770
can see the consumer to produce her in

00:13:46,030 --> 00:13:49,660
the lag reader and I've got a docker

00:13:47,770 --> 00:13:51,790
compose file so I'm just gonna go ahead

00:13:49,660 --> 00:13:54,070
and show you that docker compose files

00:13:51,790 --> 00:13:55,540
got the producer consumer the lag reader

00:13:54,070 --> 00:13:58,180
you can run docker build against the

00:13:55,540 --> 00:14:03,550
docker compose build build it run it so

00:13:58,180 --> 00:14:08,530
let's just go and run this thing docker

00:14:03,550 --> 00:14:15,000
compose up - F excuse me - do you run it

00:14:08,530 --> 00:14:15,000
as a daemon and we're going to spin up

00:14:15,630 --> 00:14:26,770
my reader my producer so we can do a

00:14:18,730 --> 00:14:33,070
doctor I can do a doctor logs and I can

00:14:26,770 --> 00:14:34,900
do my code producer and you can see that

00:14:33,070 --> 00:14:37,290
I'm publishing those messages locally -

00:14:34,900 --> 00:14:37,290
Kafka

00:14:39,240 --> 00:14:46,810
docker logs and I can do my consumer you

00:14:42,850 --> 00:14:49,120
can see that I'm reading those and I'm

00:14:46,810 --> 00:14:54,180
writing out two Kafka to that separate

00:14:49,120 --> 00:14:54,180
to that separate topic and then I can do

00:14:56,280 --> 00:15:05,470
over to my lag reader and you can see

00:15:04,450 --> 00:15:08,230
the lags are going to start growing

00:15:05,470 --> 00:15:10,060
they're growing to 3434 etc they're

00:15:08,230 --> 00:15:13,080
going to continually grow and so I can

00:15:10,060 --> 00:15:22,600
actually go ahead and use docker compose

00:15:13,080 --> 00:15:24,820
and I can do a scale and I can scale my

00:15:22,600 --> 00:15:27,220
consumer up and I can push that out and

00:15:24,820 --> 00:15:30,040
I can show while developing locally how

00:15:27,220 --> 00:15:31,780
I'm able to handle these scaling and

00:15:30,040 --> 00:15:32,830
make sure that all my code is working my

00:15:31,780 --> 00:15:35,380
lag reader is working appropriately

00:15:32,830 --> 00:15:37,300
right all that and if we watched if we

00:15:35,380 --> 00:15:39,250
watched the logs on the on the lag

00:15:37,300 --> 00:15:41,590
reader you'd start to see those slowly

00:15:39,250 --> 00:15:43,360
come down so as you can see from a

00:15:41,590 --> 00:15:46,120
container perspective I'm able to

00:15:43,360 --> 00:15:48,010
develop locally but I'm not gonna want

00:15:46,120 --> 00:15:50,200
to scale across my single node using

00:15:48,010 --> 00:15:51,940
docker compose am i that's the role of

00:15:50,200 --> 00:15:57,100
the orchestrator so let's jump back to

00:15:51,940 --> 00:15:58,300
the slides and take a look and see how

00:15:57,100 --> 00:16:01,420
we can take advantage of the

00:15:58,300 --> 00:16:03,250
orchestrator to solve to solve really

00:16:01,420 --> 00:16:07,090
running at scale not just moving past

00:16:03,250 --> 00:16:09,670
the development side okay so when you're

00:16:07,090 --> 00:16:11,860
running at scale that's the role of the

00:16:09,670 --> 00:16:14,320
orchestrator is to and if you look at

00:16:11,860 --> 00:16:16,450
the orchestrators they all tend to

00:16:14,320 --> 00:16:19,060
generally solve the same set of problems

00:16:16,450 --> 00:16:21,700
if you look at mezzo some DCOs

00:16:19,060 --> 00:16:24,220
docker swarm you look at kubernetes the

00:16:21,700 --> 00:16:27,820
all tend to act as kind of a kernel for

00:16:24,220 --> 00:16:29,680
for a cluster of machines you don't have

00:16:27,820 --> 00:16:32,680
to worry about where to schedule a

00:16:29,680 --> 00:16:34,420
specific service you basically declare

00:16:32,680 --> 00:16:36,160
this service has these requirements and

00:16:34,420 --> 00:16:38,050
the orchestrator will find somewhere to

00:16:36,160 --> 00:16:40,720
run it it can do health checks both at

00:16:38,050 --> 00:16:42,580
the process both both at the process as

00:16:40,720 --> 00:16:44,050
well as into the application and when

00:16:42,580 --> 00:16:45,880
that application is not running healthy

00:16:44,050 --> 00:16:48,400
it can reschedule that and restart it

00:16:45,880 --> 00:16:50,889
right it allows you to scale not only

00:16:48,400 --> 00:16:53,199
across one machine but off across

00:16:50,889 --> 00:16:55,660
cluster allows you to gain resiliency

00:16:53,199 --> 00:16:58,029
and high availability by scaling across

00:16:55,660 --> 00:17:00,879
fault domains all of the orchestrators

00:16:58,029 --> 00:17:04,480
tend to do those same things so what is

00:17:00,879 --> 00:17:06,279
it that's special about DCOs it's really

00:17:04,480 --> 00:17:08,740
specifically when we start talking about

00:17:06,279 --> 00:17:10,449
building these fast data solutions the

00:17:08,740 --> 00:17:12,880
answer if you sat through the keynote

00:17:10,449 --> 00:17:15,159
this morning he reacts really really hit

00:17:12,880 --> 00:17:17,829
home on that it's these vetted stateful

00:17:15,159 --> 00:17:19,809
frameworks what are they what are they

00:17:17,829 --> 00:17:21,699
well they're really distributed

00:17:19,809 --> 00:17:23,740
applications they've got the notion of a

00:17:21,699 --> 00:17:25,659
controller and workers they're called a

00:17:23,740 --> 00:17:27,730
scheduler and an executor and the

00:17:25,659 --> 00:17:30,070
benefit is the scheduler is both

00:17:27,730 --> 00:17:32,830
application and cluster aware so it can

00:17:30,070 --> 00:17:35,679
make application based decisions on how

00:17:32,830 --> 00:17:36,610
to run that application given what's

00:17:35,679 --> 00:17:38,940
going on in the cluster

00:17:36,610 --> 00:17:41,529
if you've ever installed something like

00:17:38,940 --> 00:17:43,720
Cassandra copter yourself you'll know

00:17:41,529 --> 00:17:47,230
that it's non-trivial to do that is all

00:17:43,720 --> 00:17:49,559
encapsulated inside of those frameworks

00:17:47,230 --> 00:17:51,610
so not only the installation

00:17:49,559 --> 00:17:53,679
understanding what the dependency tree

00:17:51,610 --> 00:17:56,500
is I need to make sure X is installed

00:17:53,679 --> 00:17:58,779
before Y this is all up and running not

00:17:56,500 --> 00:18:00,880
only not only beyond running or

00:17:58,779 --> 00:18:03,100
understanding getting an H a

00:18:00,880 --> 00:18:07,269
implementation of these frameworks

00:18:03,100 --> 00:18:09,279
running they go beyond that so if you

00:18:07,269 --> 00:18:11,139
want to do things like add a node into

00:18:09,279 --> 00:18:13,600
Cassandra that's again a non-trivial

00:18:11,139 --> 00:18:15,159
operation without these frameworks all

00:18:13,600 --> 00:18:16,750
of that and more I've gotten a little

00:18:15,159 --> 00:18:19,179
bit of an eye chart up here to

00:18:16,750 --> 00:18:21,940
illustrate to you the benefit of some of

00:18:19,179 --> 00:18:23,590
the benefits that are in the Cassandra

00:18:21,940 --> 00:18:25,149
framework you can look at you can look

00:18:23,590 --> 00:18:27,490
at each of these but they really should

00:18:25,149 --> 00:18:29,230
hopefully illustrate to you really the

00:18:27,490 --> 00:18:31,360
power of running these they're not

00:18:29,230 --> 00:18:32,649
templates it's not like a helmet art

00:18:31,360 --> 00:18:35,470
that you're running that's going to

00:18:32,649 --> 00:18:37,149
install on kubernetes because of the

00:18:35,470 --> 00:18:39,789
fact that it's it's an application

00:18:37,149 --> 00:18:41,919
that's aware of its surroundings as well

00:18:39,789 --> 00:18:43,750
as the app itself so hopefully we've

00:18:41,919 --> 00:18:47,529
beaten that down now if you want to run

00:18:43,750 --> 00:18:48,940
these frameworks on DCOs and you want to

00:18:47,529 --> 00:18:51,399
do it now sure there are a couple ways

00:18:48,940 --> 00:18:52,570
to install it one you can just go into

00:18:51,399 --> 00:18:54,820
our portal go into the marketplace

00:18:52,570 --> 00:18:55,269
search on mesosphere show you that in a

00:18:54,820 --> 00:18:56,500
second

00:18:55,269 --> 00:19:00,250
another way to do is sit through

00:18:56,500 --> 00:19:02,200
something called ACS engine in in Azure

00:19:00,250 --> 00:19:04,720
the way we deploy is something called an

00:19:02,200 --> 00:19:07,090
arm template as your resource manager

00:19:04,720 --> 00:19:09,190
it's a big giant thing at Jason big Bala

00:19:07,090 --> 00:19:12,190
Jason that defines the topology of what

00:19:09,190 --> 00:19:13,750
you want okay you can use a CSS engine

00:19:12,190 --> 00:19:15,520
which is an open-source project I'll

00:19:13,750 --> 00:19:17,230
show that to you in a second it's a

00:19:15,520 --> 00:19:19,450
little go app where you create this

00:19:17,230 --> 00:19:20,860
little tiny template and you run it

00:19:19,450 --> 00:19:22,930
against ACS engine and it gives you that

00:19:20,860 --> 00:19:25,240
arm template you just deploy it okay and

00:19:22,930 --> 00:19:26,830
that allows you to define custom

00:19:25,240 --> 00:19:28,840
topologies for DCOs

00:19:26,830 --> 00:19:31,120
so you could say maybe I want to private

00:19:28,840 --> 00:19:32,710
pools or maybe I want to deploy into a

00:19:31,120 --> 00:19:34,450
cut and do it into an existing virtual

00:19:32,710 --> 00:19:36,190
network because I want to build a hybrid

00:19:34,450 --> 00:19:39,670
solution or maybe I want to have

00:19:36,190 --> 00:19:41,110
attached disks for storage like me so

00:19:39,670 --> 00:19:42,340
you'd use ACS engine to do that and I'll

00:19:41,110 --> 00:19:43,210
give you a high level on how to do that

00:19:42,340 --> 00:19:47,950
a little bit

00:19:43,210 --> 00:19:50,140
that's the azure pitch right okay so we

00:19:47,950 --> 00:19:51,550
talked about the orchestrators we talked

00:19:50,140 --> 00:19:54,010
about specifically DCOs

00:19:51,550 --> 00:19:56,880
in the PAL DCOs now I want to talk to

00:19:54,010 --> 00:19:58,420
you about data persistence okay and

00:19:56,880 --> 00:19:59,740
specifically I'm going to talk to you

00:19:58,420 --> 00:20:01,060
about how port works is going to solve

00:19:59,740 --> 00:20:04,180
some of the challenges that we have

00:20:01,060 --> 00:20:05,530
within data persistence so let's talk

00:20:04,180 --> 00:20:06,640
about the options for container

00:20:05,530 --> 00:20:09,520
persistence I'm going to talk to you

00:20:06,640 --> 00:20:11,620
about a sharone's in specific but most

00:20:09,520 --> 00:20:14,590
of these can be generalized to other

00:20:11,620 --> 00:20:16,090
clouds or even on-prem this first one is

00:20:14,590 --> 00:20:19,420
as your specific it's as your files

00:20:16,090 --> 00:20:21,100
which is a file sharing service you

00:20:19,420 --> 00:20:24,070
might think as your files is your answer

00:20:21,100 --> 00:20:25,930
for if you're gonna run Cassandra Kafka

00:20:24,070 --> 00:20:27,580
for your persistence the challenge is

00:20:25,930 --> 00:20:29,860
that it wasn't designed for that it's a

00:20:27,580 --> 00:20:32,080
file share design for a specific files

00:20:29,860 --> 00:20:33,400
not for high ops it's highly throttled

00:20:32,080 --> 00:20:35,770
and if you're running something like

00:20:33,400 --> 00:20:37,210
Cassandra you know you're gonna get

00:20:35,770 --> 00:20:38,530
throttled and it's not gonna work for

00:20:37,210 --> 00:20:40,750
you so you don't want to use as your

00:20:38,530 --> 00:20:44,170
files next solution you could use the

00:20:40,750 --> 00:20:45,880
ephemeral disks on the VM and there are

00:20:44,170 --> 00:20:47,200
people that do that we've got people

00:20:45,880 --> 00:20:49,180
that want to eke out that last bit of

00:20:47,200 --> 00:20:50,680
performance in Cassandra and they run

00:20:49,180 --> 00:20:52,780
the ephemeral disks knowing that if the

00:20:50,680 --> 00:20:56,350
node dies they lose the they lose

00:20:52,780 --> 00:20:57,970
everything on that node right but they

00:20:56,350 --> 00:20:59,170
take that into account with their backup

00:20:57,970 --> 00:21:00,970
strategy and they're snapshotting

00:20:59,170 --> 00:21:03,040
strategy you can do that but that's that

00:21:00,970 --> 00:21:04,300
is for a highly advanced scenario where

00:21:03,040 --> 00:21:07,260
you need to eke out that last bit of

00:21:04,300 --> 00:21:09,760
performance it's not a generalized use

00:21:07,260 --> 00:21:11,710
so then you've got attached disks and

00:21:09,760 --> 00:21:13,270
that's probably the most common use that

00:21:11,710 --> 00:21:16,300
you have and I'll talk to you about

00:21:13,270 --> 00:21:17,170
attach this in more detail in just one

00:21:16,300 --> 00:21:19,240
second

00:21:17,170 --> 00:21:21,280
then you got pooled storage solutions

00:21:19,240 --> 00:21:23,080
and what pooled storage does typically

00:21:21,280 --> 00:21:25,450
is you take a bunch of maybe attach

00:21:23,080 --> 00:21:27,640
disks and then you aggregate all those

00:21:25,450 --> 00:21:30,070
together and then you serve out virtual

00:21:27,640 --> 00:21:32,590
volumes and I'll talk to you about how

00:21:30,070 --> 00:21:34,090
how these pooled storage solutions

00:21:32,590 --> 00:21:36,220
some examples are port works of

00:21:34,090 --> 00:21:38,320
Gloucester ifs how those pooled software

00:21:36,220 --> 00:21:39,580
solutions are pooled storage solutions

00:21:38,320 --> 00:21:41,740
solves some of the problems that you're

00:21:39,580 --> 00:21:42,760
going to see what the touch disks so

00:21:41,740 --> 00:21:45,730
with that let's talk about the

00:21:42,760 --> 00:21:47,980
challenges of attached disks the first

00:21:45,730 --> 00:21:49,900
challenge I think of when talking about

00:21:47,980 --> 00:21:51,250
attached disks has to do with container

00:21:49,900 --> 00:21:53,560
rescheduling if you're looking down at

00:21:51,250 --> 00:21:56,920
your PC you're missing about 30 minutes

00:21:53,560 --> 00:21:59,050
of work on an animation on this slide so

00:21:56,920 --> 00:22:02,560
everybody look up let's respect the

00:21:59,050 --> 00:22:04,030
animation all right so one note gets

00:22:02,560 --> 00:22:06,190
scheduled the node one needs to get

00:22:04,030 --> 00:22:09,040
rescheduled to node two what happens

00:22:06,190 --> 00:22:11,860
with that disk the disk is gonna have to

00:22:09,040 --> 00:22:13,270
move from node 1 to node 2 or you're

00:22:11,860 --> 00:22:14,920
gonna have to you can look down at your

00:22:13,270 --> 00:22:16,480
PC's again the cool animation is done

00:22:14,920 --> 00:22:18,400
for now I'll let you know when the next

00:22:16,480 --> 00:22:19,990
one's coming or you're gonna have to

00:22:18,400 --> 00:22:22,990
make sure that that work gets

00:22:19,990 --> 00:22:24,610
rescheduled on the same node now if the

00:22:22,990 --> 00:22:26,590
node was the problem that's not gonna

00:22:24,610 --> 00:22:28,270
work so you've got this disk movement

00:22:26,590 --> 00:22:30,820
problem what's the challenge when the

00:22:28,270 --> 00:22:33,130
disk moves even if the orchestrator can

00:22:30,820 --> 00:22:34,980
tell the disk detached from here and

00:22:33,130 --> 00:22:39,640
attach from over here you've got latency

00:22:34,980 --> 00:22:41,860
that's a moving part right the second

00:22:39,640 --> 00:22:43,720
challenge is this relationship I call it

00:22:41,860 --> 00:22:45,610
the container disk challenge it's this

00:22:43,720 --> 00:22:47,470
relationship you have between the

00:22:45,610 --> 00:22:49,120
containers and the disks if two

00:22:47,470 --> 00:22:51,430
containers are writing to the same disk

00:22:49,120 --> 00:22:52,930
naturally if you have to reschedule it

00:22:51,430 --> 00:22:54,070
you would have to reschedule both of

00:22:52,930 --> 00:22:57,550
those because you've got to move the

00:22:54,070 --> 00:23:00,820
disk right or you've got to enforce a

00:22:57,550 --> 00:23:02,460
one-to-one relationship what's the

00:23:00,820 --> 00:23:04,840
problem with the one-to-one relationship

00:23:02,460 --> 00:23:06,190
well the 1:1 relationship causes a

00:23:04,840 --> 00:23:09,850
couple problems one you may have a max

00:23:06,190 --> 00:23:12,300
number of this you can attach per VM but

00:23:09,850 --> 00:23:15,100
secondly it's not a very granular unit

00:23:12,300 --> 00:23:19,540
you might say that oh I want a five gig

00:23:15,100 --> 00:23:21,220
disk for container one right what if you

00:23:19,540 --> 00:23:23,470
need to go to ten dig ten gigs

00:23:21,220 --> 00:23:24,460
well over-provision well maybe now

00:23:23,470 --> 00:23:27,160
you're paying for something you don't

00:23:24,460 --> 00:23:29,170
need it's not it's not the best solution

00:23:27,160 --> 00:23:30,759
there are challenges there and so some

00:23:29,170 --> 00:23:32,109
of those challenges are

00:23:30,759 --> 00:23:33,789
dressed by these pooled software

00:23:32,109 --> 00:23:35,889
solutions so in this slide what you see

00:23:33,789 --> 00:23:37,389
is I've got two nodes and there's nodes

00:23:35,889 --> 00:23:40,539
have four disks attached to each of them

00:23:37,389 --> 00:23:42,279
each of those disks is on 128 gig the

00:23:40,539 --> 00:23:45,219
storage is going to take those and

00:23:42,279 --> 00:23:48,249
aggregate that into a terabyte and so

00:23:45,219 --> 00:23:51,159
when node one wants to spin up container

00:23:48,249 --> 00:23:53,589
one container one needs a 10 gig volume

00:23:51,159 --> 00:23:57,549
that pool of solution is going to give

00:23:53,589 --> 00:24:02,019
you a virtual volume of 10 gigs if it

00:23:57,549 --> 00:24:03,699
needs to be 15 voila it's 15 if it needs

00:24:02,019 --> 00:24:05,499
to get rescheduled the container needs

00:24:03,699 --> 00:24:07,719
to get rescheduled to node 2 no

00:24:05,499 --> 00:24:11,079
animations here by the way if it needs

00:24:07,719 --> 00:24:15,399
to get rescheduled to node 2 then voila

00:24:11,079 --> 00:24:17,440
no disk movement that's the benefit so

00:24:15,399 --> 00:24:20,229
what am i using as this pulled storage

00:24:17,440 --> 00:24:23,829
solution using port works ok

00:24:20,229 --> 00:24:25,929
poor works is a great solution not only

00:24:23,829 --> 00:24:28,539
does it kind of solve those problems I

00:24:25,929 --> 00:24:30,609
just mentioned for for pooling and

00:24:28,539 --> 00:24:32,919
giving out virtual volumes but it was

00:24:30,609 --> 00:24:34,629
built and designed for containers so it

00:24:32,919 --> 00:24:38,409
has a lot of per container functionality

00:24:34,629 --> 00:24:40,569
things like the ability to encrypt per

00:24:38,409 --> 00:24:42,099
volume so you get basically container by

00:24:40,569 --> 00:24:45,459
container concret by bringing your own

00:24:42,099 --> 00:24:47,319
keys you get a dr. volume driver and so

00:24:45,459 --> 00:24:50,289
the orchestrator as it's basically

00:24:47,319 --> 00:24:52,329
saying hey I need this I need this bit

00:24:50,289 --> 00:24:54,190
of compute this container scheduled over

00:24:52,329 --> 00:24:56,440
here can also call into the volume

00:24:54,190 --> 00:24:58,659
driver to get a volume and so you're

00:24:56,440 --> 00:24:59,949
treating storage the exact same way that

00:24:58,659 --> 00:25:01,719
you're treating your containers

00:24:59,949 --> 00:25:04,149
you also get enterprise features such as

00:25:01,719 --> 00:25:07,059
backups and snapshots so I don't want to

00:25:04,149 --> 00:25:08,949
be I don't wanna beat the drum for port

00:25:07,059 --> 00:25:09,969
works too hard Jeff's here from port

00:25:08,949 --> 00:25:11,339
works he'll be out in the hall if you

00:25:09,969 --> 00:25:14,649
want to talk to Jeff or the other folks

00:25:11,339 --> 00:25:16,569
please do they'll talk to you about it

00:25:14,649 --> 00:25:18,519
let's let's do a demo now about running

00:25:16,569 --> 00:25:20,229
at scale so basically what I what I want

00:25:18,519 --> 00:25:22,929
to do in this demo is I just want to

00:25:20,229 --> 00:25:25,149
show you how in a DC OS cluster I can

00:25:22,929 --> 00:25:26,799
get Cassandra up and running using port

00:25:25,149 --> 00:25:28,329
works on the backend and then in the

00:25:26,799 --> 00:25:32,429
last demo I'll show you running the

00:25:28,329 --> 00:25:32,429
entire app and then scaling it good

00:25:33,990 --> 00:25:39,190
okay so I basically created a little

00:25:37,270 --> 00:25:39,610
test cluster here okay it's a three node

00:25:39,190 --> 00:25:41,530
cluster

00:25:39,610 --> 00:25:43,300
it's got port works running across three

00:25:41,530 --> 00:25:45,550
nodes okay and I've got something called

00:25:43,300 --> 00:25:47,350
reap Roxy the port works UI is in

00:25:45,550 --> 00:25:49,780
sitting in the private agent pool so I

00:25:47,350 --> 00:25:51,910
need reap Roxy in order to serve serve

00:25:49,780 --> 00:25:53,680
that out okay so I've got that running

00:25:51,910 --> 00:25:55,660
you can see lighthouse out here because

00:25:53,680 --> 00:25:57,160
Cassandra takes a little while to deploy

00:25:55,660 --> 00:25:58,720
I'm gonna go and deploy that now and

00:25:57,160 --> 00:26:00,100
then I'll kind of backtrack and talk to

00:25:58,720 --> 00:26:02,260
you a little bit about it so I'm gonna

00:26:00,100 --> 00:26:04,260
just search for Cassandra and I'm going

00:26:02,260 --> 00:26:06,190
to install the port works Cassandra

00:26:04,260 --> 00:26:07,120
framework and the reason why I'm

00:26:06,190 --> 00:26:10,240
choosing the port works

00:26:07,120 --> 00:26:11,830
Cassandra framework and not the the

00:26:10,240 --> 00:26:13,600
other Cassandra framework is that this

00:26:11,830 --> 00:26:15,760
has the volume driver integrated in it

00:26:13,600 --> 00:26:19,450
so I'm just gonna put a two nodes out

00:26:15,760 --> 00:26:23,110
here and while this thing's installing

00:26:19,450 --> 00:26:24,730
one thing you'll notice is as this thing

00:26:23,110 --> 00:26:26,410
starts to deploy you're going to start

00:26:24,730 --> 00:26:28,570
seeing that kind of dependency tree

00:26:26,410 --> 00:26:29,860
Illustrated out here you're gonna start

00:26:28,570 --> 00:26:31,540
seeing things are gonna get deployed in

00:26:29,860 --> 00:26:33,010
order and that's illustrating what I was

00:26:31,540 --> 00:26:34,780
talking about the value of the scheduler

00:26:33,010 --> 00:26:36,610
but as this thing as this thing gets

00:26:34,780 --> 00:26:38,740
deployed let's take a look at lighthouse

00:26:36,610 --> 00:26:41,080
so this is the UI for port works you can

00:26:38,740 --> 00:26:43,480
see I had a three node cluster with 128

00:26:41,080 --> 00:26:45,370
gig attached disk on each node but you

00:26:43,480 --> 00:26:47,800
can see two port works its aggregated at

00:26:45,370 --> 00:26:50,590
that 384 gig you can see I've got three

00:26:47,800 --> 00:26:52,690
nodes they're all running but I've got

00:26:50,590 --> 00:26:54,940
no storage no volumes have been served

00:26:52,690 --> 00:26:58,060
out yet but what you'll if we watch this

00:26:54,940 --> 00:27:00,820
for a second as that scheduler starts to

00:26:58,060 --> 00:27:02,220
kick out my Cassandra nodes and don't

00:27:00,820 --> 00:27:05,350
worry about the big red line I didn't

00:27:02,220 --> 00:27:08,230
configure an email server so whatever

00:27:05,350 --> 00:27:09,700
but up there you can see it now so the

00:27:08,230 --> 00:27:10,960
first node for Cassandra's up on the

00:27:09,700 --> 00:27:12,940
second node comes up you're gonna see

00:27:10,960 --> 00:27:14,800
another tenth gig volume is going to be

00:27:12,940 --> 00:27:16,870
it's gonna be handed out I didn't do

00:27:14,800 --> 00:27:18,880
anything the orchestrator is controlling

00:27:16,870 --> 00:27:21,760
all this and that shows you the value of

00:27:18,880 --> 00:27:23,320
of that of that volume driver so we're

00:27:21,760 --> 00:27:25,570
gonna let Cassandra run for a little bit

00:27:23,320 --> 00:27:27,760
the install run because there's still a

00:27:25,570 --> 00:27:30,190
second node that needs to get going I'm

00:27:27,760 --> 00:27:31,630
gonna take a quick break over to ACS

00:27:30,190 --> 00:27:33,610
engine and show you how you might

00:27:31,630 --> 00:27:36,120
install a cluster I'm just gonna give

00:27:33,610 --> 00:27:37,900
you the high level here and in but

00:27:36,120 --> 00:27:40,060
hopefully it'll give you enough to go on

00:27:37,900 --> 00:27:40,720
if you want to in Azure so you got to

00:27:40,060 --> 00:27:43,390
github.com

00:27:40,720 --> 00:27:45,840
black Azure whack ACS engine and if you

00:27:43,390 --> 00:27:49,340
go into Docs

00:27:45,840 --> 00:27:51,720
and inside a docks under ACS Engine MD

00:27:49,340 --> 00:27:54,090
you can basically see it'll give you the

00:27:51,720 --> 00:27:57,330
install instructions of installing ACS

00:27:54,090 --> 00:27:59,809
engine what in general what you do the

00:27:57,330 --> 00:28:02,299
easiest way to install it is either to

00:27:59,809 --> 00:28:04,860
either to run it inside of docker

00:28:02,299 --> 00:28:06,750
easiest way to do it or you can just

00:28:04,860 --> 00:28:10,440
grab the Canaries under the releases of

00:28:06,750 --> 00:28:12,150
those binaries and what you do is if you

00:28:10,440 --> 00:28:15,150
go into ACS engine you can see these

00:28:12,150 --> 00:28:19,110
examples so under the examples these are

00:28:15,150 --> 00:28:20,370
these little mini templates that these

00:28:19,110 --> 00:28:22,169
little templates that you have to fill

00:28:20,370 --> 00:28:24,000
out that you run against ACS engine so

00:28:22,169 --> 00:28:25,770
you call ACS engine generate and you

00:28:24,000 --> 00:28:29,279
pass it one of these templates in this

00:28:25,770 --> 00:28:29,909
case I use this template so I've got

00:28:29,279 --> 00:28:33,090
DCOs

00:28:29,909 --> 00:28:37,669
I've chosen I want one master I entered

00:28:33,090 --> 00:28:41,429
in my DNS prefix and then I I chose one

00:28:37,669 --> 00:28:44,250
one disk 128 gig and I entered in my

00:28:41,429 --> 00:28:45,750
public key okay I ran that against it I

00:28:44,250 --> 00:28:47,100
got norm template and then I deployed

00:28:45,750 --> 00:28:49,590
that armed template okay

00:28:47,100 --> 00:28:53,929
that's essentially as simple as it is is

00:28:49,590 --> 00:28:56,730
to pull in deploying an ACS ACS cluster

00:28:53,929 --> 00:28:58,289
so now you can see Sanders I've just got

00:28:56,730 --> 00:29:00,120
served up my second volume so I can see

00:28:58,289 --> 00:29:01,770
the Sanders up and running so I've got

00:29:00,120 --> 00:29:03,960
two nodes in Cassandra up and running

00:29:01,770 --> 00:29:05,940
well this one's staging so wait till

00:29:03,960 --> 00:29:08,730
that finishes up and then what I can do

00:29:05,940 --> 00:29:10,169
is I can get that Cassandra tester we

00:29:08,730 --> 00:29:12,450
can run it up here and just prove that

00:29:10,169 --> 00:29:14,610
that runs then we'll jump back to the

00:29:12,450 --> 00:29:17,070
slides very briefly and then I'll show

00:29:14,610 --> 00:29:19,020
you how we can how we can auto scale

00:29:17,070 --> 00:29:21,179
this thing so everything's up and

00:29:19,020 --> 00:29:24,690
running let's grab that auto tester I

00:29:21,179 --> 00:29:30,720
just happen to have the marathon config

00:29:24,690 --> 00:29:34,890
for my auto tester so take that we're

00:29:30,720 --> 00:29:39,350
going to DC LS services add my little

00:29:34,890 --> 00:29:39,350
auto tester Cassandra tester

00:29:45,430 --> 00:29:53,150
there's Cassandra Chester it's gonna

00:29:47,600 --> 00:29:57,590
deploy quick that little thing starts to

00:29:53,150 --> 00:30:02,810
buzz when I lie did you notice that or

00:29:57,590 --> 00:30:04,790
when I'm about to lie let's click on it

00:30:02,810 --> 00:30:07,150
it's still staging let's give it a

00:30:04,790 --> 00:30:07,150
second

00:30:07,420 --> 00:30:11,930
it only takes this long if you're on

00:30:09,740 --> 00:30:13,820
stage and it takes longer if there are

00:30:11,930 --> 00:30:16,570
more people in the audience with for

00:30:13,820 --> 00:30:19,670
every 20 people it's another 10 seconds

00:30:16,570 --> 00:30:24,590
ok it's up and running we can click on

00:30:19,670 --> 00:30:33,290
and I lost my mouse so we can click on

00:30:24,590 --> 00:30:34,880
here take a look at the logs now Sanders

00:30:33,290 --> 00:30:36,380
taking a little bit longer I'll kind of

00:30:34,880 --> 00:30:39,290
jump back into this a little bit later

00:30:36,380 --> 00:30:41,450
if we have time but Sanders not properly

00:30:39,290 --> 00:30:43,010
up completely yet so it's it's taking a

00:30:41,450 --> 00:30:44,510
minute so I'll jump back to this in a

00:30:43,010 --> 00:30:46,400
little bit but let me just let me pop

00:30:44,510 --> 00:30:47,900
into the slides and keep going because

00:30:46,400 --> 00:30:51,050
we want to get to the money part of this

00:30:47,900 --> 00:30:53,000
demo which is the vamp the vamp solution

00:30:51,050 --> 00:30:56,980
so what we want to do next is talk about

00:30:53,000 --> 00:30:59,150
kind of the next day so hopefully I've

00:30:56,980 --> 00:31:01,970
the ease of which you can install

00:30:59,150 --> 00:31:05,810
Cassandra the ease of which you can take

00:31:01,970 --> 00:31:07,340
advantage of pooling solutions such as

00:31:05,810 --> 00:31:09,650
port works to handle those persistence

00:31:07,340 --> 00:31:12,050
problems so now what we want to do is

00:31:09,650 --> 00:31:15,650
talk about day two again if you missed

00:31:12,050 --> 00:31:17,960
the talks by Tim and by Julian on vamp

00:31:15,650 --> 00:31:19,820
earlier one of them is DC one of them is

00:31:17,960 --> 00:31:22,220
running DevOps the other one was just on

00:31:19,820 --> 00:31:24,320
vamp highly urge you to watch those

00:31:22,220 --> 00:31:26,510
recordings unbelievable presentations

00:31:24,320 --> 00:31:28,340
they talk about canary deployments and

00:31:26,510 --> 00:31:30,320
all the power of them from a canary

00:31:28,340 --> 00:31:32,630
deployment perspective I'm going to be

00:31:30,320 --> 00:31:34,610
talking to you about vamp from the

00:31:32,630 --> 00:31:36,860
perspective using it to do micro scaling

00:31:34,610 --> 00:31:37,490
so what is vamp if you didn't see those

00:31:36,860 --> 00:31:39,110
solutions

00:31:37,490 --> 00:31:41,630
it's a canary releasing an auto scaling

00:31:39,110 --> 00:31:42,410
for microservices systems basically what

00:31:41,630 --> 00:31:44,000
they do is they take a bunch of

00:31:42,410 --> 00:31:46,010
telemetry out of the orchestrator and

00:31:44,000 --> 00:31:49,220
they make that telemetry available to

00:31:46,010 --> 00:31:51,110
you inside of workflows ok that's the

00:31:49,220 --> 00:31:52,790
the simplest way to think about it it's

00:31:51,110 --> 00:31:55,310
they've got these constructs that

00:31:52,790 --> 00:31:58,460
represent the artifacts that you want so

00:31:55,310 --> 00:32:00,170
I've got containers there's our and

00:31:58,460 --> 00:32:02,480
workflows and all of that and they're

00:32:00,170 --> 00:32:04,040
encapsulated in these constructs but the

00:32:02,480 --> 00:32:06,620
real money is you've got all of these

00:32:04,040 --> 00:32:09,830
you've got this set of rich set of data

00:32:06,620 --> 00:32:12,110
which is includes data that's coming

00:32:09,830 --> 00:32:13,670
from the orchestrators but also data

00:32:12,110 --> 00:32:14,750
that you can push in yourself from your

00:32:13,670 --> 00:32:16,340
application

00:32:14,750 --> 00:32:18,289
so you can push this data on to what you

00:32:16,340 --> 00:32:20,230
can think of as a bus and then you can

00:32:18,289 --> 00:32:23,210
write work does it operate against that

00:32:20,230 --> 00:32:25,130
which is exactly what I'm gonna do I'm

00:32:23,210 --> 00:32:26,900
gonna take that like that aggregate lag

00:32:25,130 --> 00:32:29,090
that tells me how far behind I am and

00:32:26,900 --> 00:32:30,980
Kafka I'm gonna push that into vamp and

00:32:29,090 --> 00:32:33,559
then I'm gonna write a workflow that

00:32:30,980 --> 00:32:36,320
says hey if my a gregarious greater than

00:32:33,559 --> 00:32:38,480
500 and I've got less than 10 consumers

00:32:36,320 --> 00:32:41,929
running spin up another consumer I might

00:32:38,480 --> 00:32:43,580
do that every 15 seconds but if my

00:32:41,929 --> 00:32:45,679
aggregate lag is less than 100 and I've

00:32:43,580 --> 00:32:47,960
got more than one instance spin it back

00:32:45,679 --> 00:32:50,900
down so eventually I should level set

00:32:47,960 --> 00:32:53,179
given my rudimentary algorithm makes

00:32:50,900 --> 00:32:55,100
sense I've got to explain to you a

00:32:53,179 --> 00:32:56,330
couple artifacts damp so you understand

00:32:55,100 --> 00:32:58,700
it from a high level but don't have to

00:32:56,330 --> 00:33:00,950
go into too much detail breeds they're

00:32:58,700 --> 00:33:02,539
basically described entities so you can

00:33:00,950 --> 00:33:04,669
think of them as like a marathon jacent

00:33:02,539 --> 00:33:07,580
file or a llam file inside of

00:33:04,669 --> 00:33:09,950
kubernetes just defines the entity a

00:33:07,580 --> 00:33:12,770
blueprint is a topology it really

00:33:09,950 --> 00:33:15,590
typically has several entities so if my

00:33:12,770 --> 00:33:17,630
blueprint is gonna have my lag reader my

00:33:15,590 --> 00:33:19,580
consumer and my producer make sense

00:33:17,630 --> 00:33:22,610
don't you hate it when a guy says make

00:33:19,580 --> 00:33:24,320
sense I can't stop saying it now and

00:33:22,610 --> 00:33:26,140
then you've got deployments a deployment

00:33:24,320 --> 00:33:29,090
it's just a blueprint that's running

00:33:26,140 --> 00:33:30,679
okay and then you got workflows and a

00:33:29,090 --> 00:33:32,840
workflow it's just a little nodejs

00:33:30,679 --> 00:33:34,850
application deployed is a container that

00:33:32,840 --> 00:33:38,059
runs against that telemetry either

00:33:34,850 --> 00:33:39,860
Orchestrator telemetry or you can run it

00:33:38,059 --> 00:33:42,500
against your own custom flama tree which

00:33:39,860 --> 00:33:44,570
I'm going to so why don't we just take a

00:33:42,500 --> 00:33:48,020
look at a demo let's see if my life if

00:33:44,570 --> 00:33:49,280
my Cassander testers working it there

00:33:48,020 --> 00:33:50,450
you go my Cassandra tester is up and

00:33:49,280 --> 00:33:52,039
running so it could Sanders up and

00:33:50,450 --> 00:33:53,630
running I actually didn't write such a

00:33:52,039 --> 00:33:55,700
crap component that I normally write it

00:33:53,630 --> 00:33:59,480
retried so I should get a little clap

00:33:55,700 --> 00:34:02,179
for that I'll get myself yes no crap

00:33:59,480 --> 00:34:03,830
code here there's a lot of crap code

00:34:02,179 --> 00:34:08,030
here by the way all right so I'm gonna

00:34:03,830 --> 00:34:09,889
close this out and I'm going to close

00:34:08,030 --> 00:34:13,179
out this session and I'm going to go

00:34:09,889 --> 00:34:13,179
ahead and connect to another cluster

00:34:17,300 --> 00:34:21,800
you guys ever see the movie Dumb and

00:34:18,530 --> 00:34:23,929
Dumber where he says just when I think

00:34:21,800 --> 00:34:26,840
you couldn't write do something any

00:34:23,929 --> 00:34:27,650
dumber you do that totally reading

00:34:26,840 --> 00:34:32,780
yourself

00:34:27,650 --> 00:34:34,520
hopefully I totally redeem myself all

00:34:32,780 --> 00:34:45,350
right so I'm connected to another

00:34:34,520 --> 00:34:46,670
cluster let's take a look at it I'm

00:34:45,350 --> 00:34:50,030
doing port forwarding by the way that's

00:34:46,670 --> 00:34:51,830
why I'm on localhost so you can see this

00:34:50,030 --> 00:34:54,380
cluster got a bunch of stuff running

00:34:51,830 --> 00:34:57,110
I got port works and reap Roxy again for

00:34:54,380 --> 00:34:59,090
my back-end I've got Kafka and Cassandra

00:34:57,110 --> 00:35:02,750
both running both of those are the port

00:34:59,090 --> 00:35:05,000
works the port works versions that have

00:35:02,750 --> 00:35:07,190
the port works volume driver I've also

00:35:05,000 --> 00:35:09,410
got vamp running and elasticsearch so

00:35:07,190 --> 00:35:10,250
the amp is using elastic search for its

00:35:09,410 --> 00:35:13,220
time series data

00:35:10,250 --> 00:35:15,140
okay so vamp if we take a look here

00:35:13,220 --> 00:35:16,880
click into it there's a little group

00:35:15,140 --> 00:35:18,860
here and the most important guy here is

00:35:16,880 --> 00:35:21,020
the UI actually the most port guys the

00:35:18,860 --> 00:35:23,750
API but for the purpose this time of

00:35:21,020 --> 00:35:25,520
them most important guys the API the UI

00:35:23,750 --> 00:35:27,590
so if we pop into here and we take a

00:35:25,520 --> 00:35:30,170
look at the breeds you can see I've got

00:35:27,590 --> 00:35:35,180
a breed for my producer and if we take a

00:35:30,170 --> 00:35:37,070
look at that you can see that really all

00:35:35,180 --> 00:35:39,680
this thing is is it it's basically

00:35:37,070 --> 00:35:42,080
defining my entity and the real money

00:35:39,680 --> 00:35:44,780
there is that it's pointing to inside a

00:35:42,080 --> 00:35:46,820
docker hub is a container our Bagby wack

00:35:44,780 --> 00:35:48,020
demo producer right and it's got a bunch

00:35:46,820 --> 00:35:50,030
of environment variables and those

00:35:48,020 --> 00:35:53,090
variables can either get overwritten by

00:35:50,030 --> 00:35:56,390
the can either get overridden by the

00:35:53,090 --> 00:35:58,400
blueprint or if you're if you're calling

00:35:56,390 --> 00:36:00,230
into the API you can override it okay so

00:35:58,400 --> 00:36:03,170
I've got my producer my lag reader and

00:36:00,230 --> 00:36:05,900
my consumer and then I got a blueprint

00:36:03,170 --> 00:36:08,060
that's my ready demo here right and set

00:36:05,900 --> 00:36:10,220
my ready demo you can see it's got the

00:36:08,060 --> 00:36:12,380
breed for my producer and I can override

00:36:10,220 --> 00:36:14,030
those environment variables there can

00:36:12,380 --> 00:36:17,420
you guys see this well enough without me

00:36:14,030 --> 00:36:19,070
zooming in yeah the consumer the details

00:36:17,420 --> 00:36:21,020
don't really matter too much and my

00:36:19,070 --> 00:36:23,090
producer so I got this blueprint I can

00:36:21,020 --> 00:36:27,030
run this thing up okay so let's go ahead

00:36:23,090 --> 00:36:29,520
and run this I'm going to deploy it

00:36:27,030 --> 00:36:30,840
now remember I told you that we're gonna

00:36:29,520 --> 00:36:32,820
get all these events when this thing

00:36:30,840 --> 00:36:34,620
deploys so you can see we're starting to

00:36:32,820 --> 00:36:36,060
deploy here and this thing's gonna go

00:36:34,620 --> 00:36:37,650
ahead and try and deploy those three

00:36:36,060 --> 00:36:39,570
containers into DCOs

00:36:37,650 --> 00:36:41,100
and when the lag reader gets up and

00:36:39,570 --> 00:36:43,050
running and it starts calculating the

00:36:41,100 --> 00:36:45,480
aggregate lag I'm actually writing that

00:36:43,050 --> 00:36:48,690
back to the vamp API so I should be able

00:36:45,480 --> 00:36:50,550
to see in these events my aggregate lag

00:36:48,690 --> 00:36:54,600
I'm gonna get rid of the health and the

00:36:50,550 --> 00:37:00,750
metrics and the allocation events and we

00:36:54,600 --> 00:37:02,790
should we should start seeing some lag

00:37:00,750 --> 00:37:03,570
events popping up if I go over here back

00:37:02,790 --> 00:37:05,280
to DCOs

00:37:03,570 --> 00:37:07,470
and take a look in services you can see

00:37:05,280 --> 00:37:09,270
my services are running so I've got my

00:37:07,470 --> 00:37:13,590
producer up and running if we take a

00:37:09,270 --> 00:37:16,860
look at the logs you can see my producer

00:37:13,590 --> 00:37:19,440
is pushing 50 messages every second if I

00:37:16,860 --> 00:37:21,030
go back to the services you can see that

00:37:19,440 --> 00:37:22,860
all my services are up and running and

00:37:21,030 --> 00:37:25,080
the lag readers out there pushing those

00:37:22,860 --> 00:37:27,270
lag events and let's see where it's

00:37:25,080 --> 00:37:30,750
pushing them to you can see it's out

00:37:27,270 --> 00:37:33,060
here pushing one was 294 33 if we go

00:37:30,750 --> 00:37:34,710
back to vamp you could see I've got all

00:37:33,060 --> 00:37:38,340
those lag events are getting pushed into

00:37:34,710 --> 00:37:40,320
van now I've written a workflow that

00:37:38,340 --> 00:37:42,840
just like I told you let's take a look

00:37:40,320 --> 00:37:44,850
at it so let's go into the breeds let's

00:37:42,840 --> 00:37:46,890
hide these events for a second look at

00:37:44,850 --> 00:37:49,620
the breeds and look at my auto scale and

00:37:46,890 --> 00:37:52,110
here's the workflow and I'll just show

00:37:49,620 --> 00:37:55,020
you the money bid here oops

00:37:52,110 --> 00:37:58,200
okay so here we go what you can see here

00:37:55,020 --> 00:38:00,930
is I've basically got the events an HTTP

00:37:58,200 --> 00:38:04,050
GET to my URI to vamp running in the

00:38:00,930 --> 00:38:06,180
cluster to the events where the tag is

00:38:04,050 --> 00:38:08,160
lag so basically just getting my lag

00:38:06,180 --> 00:38:10,740
events back I'm then getting those back

00:38:08,160 --> 00:38:12,540
past me as a response and then I'm just

00:38:10,740 --> 00:38:14,460
running this logic if the lag is greater

00:38:12,540 --> 00:38:16,830
than 500 and the scale of my in my

00:38:14,460 --> 00:38:19,110
instance count is less than 10 scale the

00:38:16,830 --> 00:38:22,470
instances otherwise if it's less than

00:38:19,110 --> 00:38:23,700
100 scaling down makes sense so let's

00:38:22,470 --> 00:38:25,080
run this thing

00:38:23,700 --> 00:38:27,430
[Music]

00:38:25,080 --> 00:38:29,380
let's take a look at my events and see

00:38:27,430 --> 00:38:31,300
where my legs are my legs are over 500

00:38:29,380 --> 00:38:33,220
now so when I kick this thing off it

00:38:31,300 --> 00:38:35,710
should immediately start pushing these

00:38:33,220 --> 00:38:40,030
things out so there's my auto-scale

00:38:35,710 --> 00:38:42,220
ready workflow can let's start it this

00:38:40,030 --> 00:38:45,119
thing's gonna run for every 15 seconds

00:38:42,220 --> 00:38:48,100
it's gonna run so let's just watch it go

00:38:45,119 --> 00:38:49,359
while this syncs kicking you can see it

00:38:48,100 --> 00:38:52,300
just ran it didn't do anything because

00:38:49,359 --> 00:38:54,280
my lag is 458 remember that lag can kind

00:38:52,300 --> 00:38:57,930
of go up and down artificially because

00:38:54,280 --> 00:39:00,310
it only there are certain nodes or

00:38:57,930 --> 00:39:02,260
partitions that it's not reading off but

00:39:00,310 --> 00:39:03,880
eventually we'll hit over 500 and this

00:39:02,260 --> 00:39:05,800
thing will kick off and when it does

00:39:03,880 --> 00:39:07,480
we're gonna see my instance can't go up

00:39:05,800 --> 00:39:09,430
and we're gonna instantly see here we go

00:39:07,480 --> 00:39:12,850
we're now up to two instances can you

00:39:09,430 --> 00:39:15,220
all see that two instances popping up if

00:39:12,850 --> 00:39:18,820
we go back to DCOs and look at it you

00:39:15,220 --> 00:39:20,410
can basically see the services you can

00:39:18,820 --> 00:39:22,960
see I'm not deploying my second instance

00:39:20,410 --> 00:39:26,290
out here let's take a look in DCOs

00:39:22,960 --> 00:39:27,730
just that kind of the the density that

00:39:26,290 --> 00:39:30,550
we're running at and you should see it

00:39:27,730 --> 00:39:32,380
start move up higher than this as we

00:39:30,550 --> 00:39:34,359
start deploying more and more services

00:39:32,380 --> 00:39:36,040
and as vamps start seeing these things

00:39:34,359 --> 00:39:38,109
going up you can see my leg is starting

00:39:36,040 --> 00:39:40,660
to is starting to increase rather

00:39:38,109 --> 00:39:42,840
drastically because as I spin up new

00:39:40,660 --> 00:39:44,830
consumers they're hitting different

00:39:42,840 --> 00:39:46,869
partitions that were never being run off

00:39:44,830 --> 00:39:50,350
before so all the sudden it says

00:39:46,869 --> 00:39:52,720
partition X now has 50 or a hundred or

00:39:50,350 --> 00:39:54,430
500 and eventually the single start

00:39:52,720 --> 00:39:57,369
moving up and you'll start seeing all of

00:39:54,430 --> 00:39:59,530
the once we have ten consumers out there

00:39:57,369 --> 00:40:01,810
you're gonna have basically a clarity on

00:39:59,530 --> 00:40:05,290
what your entire lag was and eventually

00:40:01,810 --> 00:40:08,109
we'll move that down to back down to

00:40:05,290 --> 00:40:10,210
below 100 because my consumers will now

00:40:08,109 --> 00:40:11,740
catch up and will start moving down so

00:40:10,210 --> 00:40:13,480
to eventually smooth itself down to

00:40:11,740 --> 00:40:17,640
somewhere around between four and five

00:40:13,480 --> 00:40:20,380
four and five containers makes sense

00:40:17,640 --> 00:40:22,750
there does that make sense again I can't

00:40:20,380 --> 00:40:25,720
stop saying it so let's take one more

00:40:22,750 --> 00:40:27,460
look over here at eCos we can see that

00:40:25,720 --> 00:40:29,200
we've got now six running we're trying

00:40:27,460 --> 00:40:31,540
to deploy the seventh and if we go into

00:40:29,200 --> 00:40:32,800
the nodes you can start seeing that

00:40:31,540 --> 00:40:35,080
we're actually getting higher and higher

00:40:32,800 --> 00:40:36,520
density so it's the container the fact

00:40:35,080 --> 00:40:37,839
that encapsulates its dependencies

00:40:36,520 --> 00:40:39,940
allows it to run anyway

00:40:37,839 --> 00:40:42,309
but it's the orchestrator that allows

00:40:39,940 --> 00:40:44,440
you to schedule it anywhere but now it's

00:40:42,309 --> 00:40:46,539
this workflow component that gives you

00:40:44,440 --> 00:40:49,269
the ease of use of spinning them up and

00:40:46,539 --> 00:40:51,640
spinning them down upon need or upon use

00:40:49,269 --> 00:40:56,349
the benefit is that you have workflows

00:40:51,640 --> 00:40:58,089
or if you have work items that vary

00:40:56,349 --> 00:40:59,769
throughout the day you can spin up

00:40:58,089 --> 00:41:02,109
containers for the ones that need more

00:40:59,769 --> 00:41:04,269
spin down for others that don't and you

00:41:02,109 --> 00:41:06,160
can really kind of not have to worry

00:41:04,269 --> 00:41:09,190
about what's what I call macro scaling

00:41:06,160 --> 00:41:11,650
scaling up nodes as much as you do just

00:41:09,190 --> 00:41:15,910
spinning up and down workloads inside of

00:41:11,650 --> 00:41:17,440
those existing clusters so let's just go

00:41:15,910 --> 00:41:21,549
ahead and back to them for a second here

00:41:17,440 --> 00:41:23,380
and we'll just keep it on here as the as

00:41:21,549 --> 00:41:26,259
the lag starts to go down we're sitting

00:41:23,380 --> 00:41:27,729
here I think we've got nine or ten the

00:41:26,259 --> 00:41:30,009
lag will eventually start going down as

00:41:27,729 --> 00:41:32,469
it goes down below 100 this thing will

00:41:30,009 --> 00:41:34,329
uh the they'll start to spin back down

00:41:32,469 --> 00:41:36,609
but for now why don't I take any

00:41:34,329 --> 00:41:39,819
questions if we have any and then we've

00:41:36,609 --> 00:41:41,829
got a microphone here and if their port

00:41:39,819 --> 00:41:43,779
works related again the port works guys

00:41:41,829 --> 00:41:45,400
are out there we've also got somebody

00:41:43,779 --> 00:41:48,299
from vampyre that can answer higher

00:41:45,400 --> 00:41:48,299
level vamp questions

00:41:56,490 --> 00:42:03,670
first of all thing for the torque maybe

00:42:00,220 --> 00:42:07,840
it's a port works question but I said we

00:42:03,670 --> 00:42:10,290
are talking about fast application how

00:42:07,840 --> 00:42:14,110
do you measure the delay of rating in a

00:42:10,290 --> 00:42:16,660
in a scalable data store you know and

00:42:14,110 --> 00:42:18,280
distributed in a pullet thought that you

00:42:16,660 --> 00:42:22,300
I'm sorry can you repeat the question

00:42:18,280 --> 00:42:24,700
yes when I work with these kind of

00:42:22,300 --> 00:42:27,220
technologies one of the most important

00:42:24,700 --> 00:42:30,100
parts is the data locality so if you

00:42:27,220 --> 00:42:33,970
will write or read from a boolean data

00:42:30,100 --> 00:42:37,090
store how do you measure these the delay

00:42:33,970 --> 00:42:39,520
of the writing of the under reading from

00:42:37,090 --> 00:42:42,130
from each of the components how do you

00:42:39,520 --> 00:42:44,260
measure the delay is really because we

00:42:42,130 --> 00:42:49,110
are talking about files application so I

00:42:44,260 --> 00:42:54,370
mean I assume that all of this is in a

00:42:49,110 --> 00:42:57,700
microsecond scale do you measure this

00:42:54,370 --> 00:43:00,160
delay or this is this in this app I'm

00:42:57,700 --> 00:43:01,870
not I'm not doing that but we can we can

00:43:00,160 --> 00:43:03,280
take that offline if you want and I can

00:43:01,870 --> 00:43:04,390
kind of direct you to some people that

00:43:03,280 --> 00:43:12,220
can probably help you from that

00:43:04,390 --> 00:43:13,900
perspective well that's it if you've got

00:43:12,220 --> 00:43:15,160
patience for just another couple of

00:43:13,900 --> 00:43:16,960
seconds we've just hit below the 100

00:43:15,160 --> 00:43:21,550
threshold and we should see this thing

00:43:16,960 --> 00:43:25,010
start to scale back that's it and thanks

00:43:21,550 --> 00:43:28,419
for attending and we scaled down to nine

00:43:25,010 --> 00:43:28,419

YouTube URL: https://www.youtube.com/watch?v=UYSchHnp1JE


