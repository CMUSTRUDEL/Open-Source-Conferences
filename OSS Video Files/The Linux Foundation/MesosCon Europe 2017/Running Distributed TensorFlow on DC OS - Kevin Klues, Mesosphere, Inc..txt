Title: Running Distributed TensorFlow on DC OS - Kevin Klues, Mesosphere, Inc.
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Running Distributed TensorFlow on DC/OS - Kevin Klues, Mesosphere, Inc.

Running distributed TensorFlow is challenging, especially if you want to train large models on your own infrastructure. In this talk, Kevin Klues and Sam Pringle will present an open source TensorFlow framework for distributed training on DC/OS. This framework addresses several challenges associated with distributed TensorFlow, and they hope it will make life much easier for anyone doing machine learning with large models/datasets. Kevin will introduce TensorFlow on Mesos and DC/OS, and Sam will give a live demo of the framework.

About Kevin Klues
Kevin Klues is a Tech Lead Manager at Mesosphere running the DC/OS ClusterOps team. Since joining Mesosphere, Kevin has been involved in the design and implementation of a number of Mesosâ€™s core subsystems, including GPU isolation, Pods, the Mesos CLI and Attach/Exec support. He now leads a team of 10 Engineers working on everything from the DC/OS CLI to the installation, logging, backup/restore, and gathering / reporting of metrics and diagnostics of a running DC/OS cluster. When not working, you can usually find Kevin on a snowboard or up in the mountains in some capacity or another.
Captions: 
	00:00:00,060 --> 00:00:05,970
all right I think we're gonna go ahead

00:00:02,550 --> 00:00:08,610
and get started here math did you want

00:00:05,970 --> 00:00:11,990
to say anything to lunch this off or

00:00:08,610 --> 00:00:15,000
should I just jump right in okay great

00:00:11,990 --> 00:00:19,109
so this is the session on running

00:00:15,000 --> 00:00:20,520
distributed tensor flow on D see us if

00:00:19,109 --> 00:00:23,279
you were at the keynote this morning you

00:00:20,520 --> 00:00:26,820
heard Erie X talk about how we just

00:00:23,279 --> 00:00:29,010
yesterday released a package into the

00:00:26,820 --> 00:00:30,840
into the DC OS catalog for a running

00:00:29,010 --> 00:00:32,040
distributed tensor flow and what I'm

00:00:30,840 --> 00:00:34,860
gonna do today is kind of go into the

00:00:32,040 --> 00:00:37,410
details of you know what what problem

00:00:34,860 --> 00:00:39,390
this package solves in the community of

00:00:37,410 --> 00:00:42,329
people trying to run and deploy tensor

00:00:39,390 --> 00:00:44,070
flow in in production and how we

00:00:42,329 --> 00:00:46,530
actually built this package to solve

00:00:44,070 --> 00:00:49,320
those problems my name is Kevin Clues I

00:00:46,530 --> 00:00:51,059
work at meso sphere and I lead a team

00:00:49,320 --> 00:00:52,860
there called the DC OS cluster

00:00:51,059 --> 00:00:54,600
Operations team in terms of

00:00:52,860 --> 00:00:56,129
contributions to Mesa so in the past

00:00:54,600 --> 00:00:58,609
when I when I first joined my so sphere

00:00:56,129 --> 00:01:00,840
I added the initial GPU support tomatoes

00:00:58,609 --> 00:01:02,719
from there I worked on some of the pod

00:01:00,840 --> 00:01:05,129
support or what we call task groups

00:01:02,719 --> 00:01:06,659
which I worked on the containerization

00:01:05,129 --> 00:01:08,340
pieces of that in order to enable that

00:01:06,659 --> 00:01:10,350
support inside my sis and then I moved

00:01:08,340 --> 00:01:13,860
on to work on some of the attached exec

00:01:10,350 --> 00:01:15,299
supports if you've ever used docker exec

00:01:13,860 --> 00:01:17,520
to kind of jump inside of a container

00:01:15,299 --> 00:01:19,110
and run a process there I added similar

00:01:17,520 --> 00:01:21,180
support to meso sand kind of push that

00:01:19,110 --> 00:01:23,970
up the rest of the stack and so you know

00:01:21,180 --> 00:01:26,009
that's kind of my my link into maysa and

00:01:23,970 --> 00:01:27,450
then the GPU support is what sort of

00:01:26,009 --> 00:01:28,680
launched me into working on tensor flow

00:01:27,450 --> 00:01:30,750
and getting some of these other things

00:01:28,680 --> 00:01:32,520
around how do we use the GPUs now that

00:01:30,750 --> 00:01:36,090
we have that amazes to do you know more

00:01:32,520 --> 00:01:37,439
useful stuff on top so what I want to

00:01:36,090 --> 00:01:39,570
start off first with is just giving a

00:01:37,439 --> 00:01:41,670
really quick brief intro into tensor

00:01:39,570 --> 00:01:43,560
flow for those of you don't know this is

00:01:41,670 --> 00:01:45,899
the same slide that I react showed this

00:01:43,560 --> 00:01:48,420
morning with this quote from tensor flow

00:01:45,899 --> 00:01:50,189
org that says an open-source software

00:01:48,420 --> 00:01:53,700
library for machine intelligence that's

00:01:50,189 --> 00:01:54,840
what tensor flow purports to be so so

00:01:53,700 --> 00:01:56,969
what exactly is machine intelligence

00:01:54,840 --> 00:01:58,829
well you know machine intelligence is a

00:01:56,969 --> 00:02:00,899
broad term used to describe techniques

00:01:58,829 --> 00:02:02,759
that allow computers to learn by

00:02:00,899 --> 00:02:05,850
analyzing very large data sets using

00:02:02,759 --> 00:02:07,439
artificial neural networks okay so you

00:02:05,850 --> 00:02:10,020
know if that's what machine intelligence

00:02:07,439 --> 00:02:11,160
is this is a representation of one of

00:02:10,020 --> 00:02:13,860
these neural networks that you might

00:02:11,160 --> 00:02:15,660
might might build in order to

00:02:13,860 --> 00:02:17,280
- to do this kind of analysis and so

00:02:15,660 --> 00:02:18,690
what you see here the the typical flow

00:02:17,280 --> 00:02:20,550
for one of these kind of networks is you

00:02:18,690 --> 00:02:22,350
have some large data set that you want

00:02:20,550 --> 00:02:24,360
to flow through your network to produce

00:02:22,350 --> 00:02:25,470
some output at the end and so you know

00:02:24,360 --> 00:02:26,520
one of the things that makes deep

00:02:25,470 --> 00:02:27,660
learning a little different than

00:02:26,520 --> 00:02:29,100
traditional machine learning is you have

00:02:27,660 --> 00:02:31,860
lots and lots of layers here that you're

00:02:29,100 --> 00:02:33,960
passing your your images through and so

00:02:31,860 --> 00:02:35,460
in this example here you might feed a

00:02:33,960 --> 00:02:37,530
whole bunch of faces into this network

00:02:35,460 --> 00:02:39,330
you have some layers detecting if you've

00:02:37,530 --> 00:02:41,160
got some patterns of local contrast in

00:02:39,330 --> 00:02:42,390
those faces you want to find pass it on

00:02:41,160 --> 00:02:43,920
to the next layer you see if there's

00:02:42,390 --> 00:02:45,840
some facial features that pop out from

00:02:43,920 --> 00:02:47,940
that and the final layer you're actually

00:02:45,840 --> 00:02:49,560
able to say oh here's some faces now

00:02:47,940 --> 00:02:51,000
whenever new images come in I can

00:02:49,560 --> 00:02:54,990
recognize that that's a face or not with

00:02:51,000 --> 00:02:56,400
some level of certainty right okay so

00:02:54,990 --> 00:02:58,050
you know the second part of this

00:02:56,400 --> 00:02:59,640
definition is that tenter photos are

00:02:58,050 --> 00:03:01,080
software library so you know if that's

00:02:59,640 --> 00:03:02,550
what machine intelligence is what does

00:03:01,080 --> 00:03:04,560
tensorflow really give you it gives you

00:03:02,550 --> 00:03:06,480
this library that makes it easy for

00:03:04,560 --> 00:03:08,850
developers to construct these artificial

00:03:06,480 --> 00:03:10,950
neural networks to analyze their data of

00:03:08,850 --> 00:03:12,630
data of interest so if you take a look

00:03:10,950 --> 00:03:14,130
at this picture here it kind of shows

00:03:12,630 --> 00:03:16,170
the picture of okay I've got some Python

00:03:14,130 --> 00:03:17,370
application I want to run I can import

00:03:16,170 --> 00:03:18,420
this tensor for the library and it's

00:03:17,370 --> 00:03:21,290
going to take care of all these

00:03:18,420 --> 00:03:24,540
low-level details of how do I actually

00:03:21,290 --> 00:03:26,130
execute some data flow graph what

00:03:24,540 --> 00:03:27,510
compute kernels do I want to have how do

00:03:26,130 --> 00:03:29,700
I handle networking so that I can

00:03:27,510 --> 00:03:31,200
actually you know ship distributed jobs

00:03:29,700 --> 00:03:34,200
between different machines I have in my

00:03:31,200 --> 00:03:35,519
cluster and if GPUs or CPUs or even TP

00:03:34,200 --> 00:03:37,050
user available how do I make sure I

00:03:35,519 --> 00:03:40,680
schedule jobs on the right piece of

00:03:37,050 --> 00:03:43,280
hardware automatically to take advantage

00:03:40,680 --> 00:03:45,120
of that hardware that exists underneath

00:03:43,280 --> 00:03:46,800
okay and then the last piece of this

00:03:45,120 --> 00:03:47,790
that it's open source which is great for

00:03:46,800 --> 00:03:49,260
us because we're able to take it

00:03:47,790 --> 00:03:50,820
leverage it change it do whatever we

00:03:49,260 --> 00:03:56,070
want and it was open source by Google

00:03:50,820 --> 00:03:57,030
back in November 2015 ok so second parts

00:03:56,070 --> 00:03:58,830
of this you know we're trying to take

00:03:57,030 --> 00:04:00,330
tensorflow run it on top of DCC us I

00:03:58,830 --> 00:04:02,340
think most of you by this point probably

00:04:00,330 --> 00:04:04,070
know what DC OS is but for those of you

00:04:02,340 --> 00:04:07,290
don't it's an open source distributed

00:04:04,070 --> 00:04:09,570
operating system and its main goal is to

00:04:07,290 --> 00:04:11,519
take masers and build upon it with some

00:04:09,570 --> 00:04:12,810
additional services and functionality so

00:04:11,519 --> 00:04:15,090
the way I usually like to think about it

00:04:12,810 --> 00:04:16,620
is you know what it gives you on top of

00:04:15,090 --> 00:04:19,019
maysa is some built-in support for

00:04:16,620 --> 00:04:22,140
service discovery load balancing

00:04:19,019 --> 00:04:24,600
security ease of installation some extra

00:04:22,140 --> 00:04:26,970
tooling including a comprehensive CLI

00:04:24,600 --> 00:04:28,800
GUI on top of that

00:04:26,970 --> 00:04:30,750
built-in frameworks for launching

00:04:28,800 --> 00:04:32,099
long-running services the the the

00:04:30,750 --> 00:04:34,199
canonical scheduler we used for this is

00:04:32,099 --> 00:04:36,330
marathon we now have kubernetes support

00:04:34,199 --> 00:04:40,379
and then we also have batch job support

00:04:36,330 --> 00:04:41,340
in the form of metronome or chromos in

00:04:40,379 --> 00:04:44,069
addition to that we also have a

00:04:41,340 --> 00:04:45,569
repository or app store for installing

00:04:44,069 --> 00:04:47,129
other common packages and framework this

00:04:45,569 --> 00:04:49,650
is what Ruby reax was referring to

00:04:47,129 --> 00:04:51,090
earlier is the catalog and you know for

00:04:49,650 --> 00:04:52,440
a while now we've had spark Kafka

00:04:51,090 --> 00:04:54,419
Cassandra some of these other frameworks

00:04:52,440 --> 00:04:57,960
and now we're adding tensorflow in there

00:04:54,419 --> 00:04:59,870
as well so a very high-level picture

00:04:57,960 --> 00:05:02,430
this imagine you know you've got some

00:04:59,870 --> 00:05:04,139
GPU Hardware GPU enabled Hardware

00:05:02,430 --> 00:05:05,129
underneath you've got that running on

00:05:04,139 --> 00:05:07,259
the cloud you've got it running on

00:05:05,129 --> 00:05:09,599
premise you've got Mesa sitting on top

00:05:07,259 --> 00:05:10,740
of that do get your batch scheduler your

00:05:09,599 --> 00:05:12,690
marathon schedules or any other

00:05:10,740 --> 00:05:14,190
frameworks that you want this all

00:05:12,690 --> 00:05:15,389
packaged together is you know what we

00:05:14,190 --> 00:05:17,069
what we call DCOs

00:05:15,389 --> 00:05:18,979
so that you can leverage all those

00:05:17,069 --> 00:05:21,360
things together in a nice coherent way

00:05:18,979 --> 00:05:23,729
and what we're adding now is support for

00:05:21,360 --> 00:05:25,560
you know running integrated support for

00:05:23,729 --> 00:05:28,560
running sensor flow on top of this

00:05:25,560 --> 00:05:31,050
infrastructure okay so a quick overview

00:05:28,560 --> 00:05:32,400
of the talk first thing I'm going to do

00:05:31,050 --> 00:05:34,889
is I'm gonna go through a really quick

00:05:32,400 --> 00:05:36,509
deep learning overview primer just to

00:05:34,889 --> 00:05:37,710
kind of give those of you that don't

00:05:36,509 --> 00:05:39,180
really have a background on deep

00:05:37,710 --> 00:05:41,009
learning and what it is just a quick

00:05:39,180 --> 00:05:42,330
intro to know what types of problems you

00:05:41,009 --> 00:05:43,650
can actually solve a sense with

00:05:42,330 --> 00:05:45,389
tensorflow why would you use this

00:05:43,650 --> 00:05:47,789
package and then I'm going to really

00:05:45,389 --> 00:05:49,680
quickly set up my demo because it takes

00:05:47,789 --> 00:05:51,860
a while to train your models and so I

00:05:49,680 --> 00:05:54,360
want to set up the demo get some of my

00:05:51,860 --> 00:05:55,770
models training and then we'll jump back

00:05:54,360 --> 00:05:57,449
into the rest of the presentation and

00:05:55,770 --> 00:05:59,580
then you know look back at the demo and

00:05:57,449 --> 00:06:00,419
see how they're doing later on and then

00:05:59,580 --> 00:06:02,190
we're gonna go through a typical

00:06:00,419 --> 00:06:02,849
developer workflow for tensorflow how

00:06:02,190 --> 00:06:04,949
this looks today

00:06:02,849 --> 00:06:06,690
both in a setting where you're gonna run

00:06:04,949 --> 00:06:08,069
on a single node environment or in a

00:06:06,690 --> 00:06:09,840
distributed setting which is the problem

00:06:08,069 --> 00:06:11,129
we're trying to solve for here I'll then

00:06:09,840 --> 00:06:12,539
talk through what the existing

00:06:11,129 --> 00:06:14,490
challenges and running distributed

00:06:12,539 --> 00:06:16,529
tensorflow are today and then how

00:06:14,490 --> 00:06:18,690
running tensorflow on DCOs helps to

00:06:16,529 --> 00:06:19,919
solve a lot of those problems and as I

00:06:18,690 --> 00:06:22,800
said I'll go I'll jump back to the demo

00:06:19,919 --> 00:06:23,789
we can analyze how it's been performing

00:06:22,800 --> 00:06:25,830
and then I'll talk about some of the

00:06:23,789 --> 00:06:27,509
next steps to where we're going next

00:06:25,830 --> 00:06:31,620
with this tensorflow package that we've

00:06:27,509 --> 00:06:33,990
built okay so real quick deep learning

00:06:31,620 --> 00:06:36,300
overview primer for those of you

00:06:33,990 --> 00:06:38,159
familiar with traditional machine

00:06:36,300 --> 00:06:39,330
learning the the process that you

00:06:38,159 --> 00:06:40,409
usually have to go through if you want

00:06:39,330 --> 00:06:42,629
to train some model is you

00:06:40,409 --> 00:06:44,309
got some input let's say we have some

00:06:42,629 --> 00:06:46,769
pictures of dogs that we want to analyze

00:06:44,309 --> 00:06:48,119
the first step traditionally used to be

00:06:46,769 --> 00:06:50,129
you'd have some human that would look at

00:06:48,119 --> 00:06:51,659
that dog and say okay what are all the

00:06:50,129 --> 00:06:53,339
different features that I have right

00:06:51,659 --> 00:06:54,839
there's there's ears there's a nose

00:06:53,339 --> 00:06:56,789
there's some eyes there's some paws

00:06:54,839 --> 00:06:58,769
there's a tail I would have to mark all

00:06:56,789 --> 00:07:00,839
these things and make them available to

00:06:58,769 --> 00:07:02,610
my model so that it could take that look

00:07:00,839 --> 00:07:04,619
at pictures of dogs see if it could find

00:07:02,610 --> 00:07:06,449
all those different features in it then

00:07:04,619 --> 00:07:08,279
use the neural network to classify oh

00:07:06,449 --> 00:07:10,169
can I actually recognize a dog given all

00:07:08,279 --> 00:07:12,330
these features and then output is this a

00:07:10,169 --> 00:07:13,949
dog or is it not right what deep

00:07:12,330 --> 00:07:15,989
learning what you do is sort of take the

00:07:13,949 --> 00:07:17,339
human out of the equation and now all

00:07:15,989 --> 00:07:18,899
you have to do instead of marking all

00:07:17,339 --> 00:07:20,099
these different features of a dog all

00:07:18,899 --> 00:07:21,899
you have to do is say hey here's a

00:07:20,099 --> 00:07:24,029
picture and here's a little box around

00:07:21,899 --> 00:07:25,649
what a dog is and the neural network

00:07:24,029 --> 00:07:27,689
will take that figure out what all the

00:07:25,649 --> 00:07:28,949
features are that make up a dog and then

00:07:27,689 --> 00:07:30,869
in addition to that to start to

00:07:28,949 --> 00:07:33,360
recognize what a dog is and pop out is

00:07:30,869 --> 00:07:34,709
this a dog or is it not once you get to

00:07:33,360 --> 00:07:37,009
the the phase of actually trying to

00:07:34,709 --> 00:07:41,539
infer what what pictures look like right

00:07:37,009 --> 00:07:44,819
so diving a little bit deeper into this

00:07:41,539 --> 00:07:46,619
if you if you think about deep learning

00:07:44,819 --> 00:07:47,519
and the phase that your different models

00:07:46,619 --> 00:07:49,499
that you're running through go through

00:07:47,519 --> 00:07:50,789
there's always a training phase and an

00:07:49,499 --> 00:07:52,619
inference phrase and the training phase

00:07:50,789 --> 00:07:54,959
is the one that takes hours or weeks or

00:07:52,619 --> 00:07:57,179
days where you you have this model you

00:07:54,959 --> 00:07:58,949
want to make it recognize dogs in this

00:07:57,179 --> 00:08:00,539
case so you feed it a whole bunch of

00:07:58,949 --> 00:08:02,159
pictures of dogs you label those dogs

00:08:00,539 --> 00:08:04,019
saying this thing this object and this

00:08:02,159 --> 00:08:06,209
picture is a dog the neural network

00:08:04,019 --> 00:08:08,249
eventually with after it has all of

00:08:06,209 --> 00:08:09,509
these you know lots and lots of samples

00:08:08,249 --> 00:08:10,949
of what a dog looks like it eventually

00:08:09,509 --> 00:08:13,199
trains itself to figure out what that

00:08:10,949 --> 00:08:14,759
looks like out pops a trained model and

00:08:13,199 --> 00:08:17,699
then from there you can use this model

00:08:14,759 --> 00:08:19,259
to in the future infer whether this

00:08:17,699 --> 00:08:20,909
picture is actually a dog or not and

00:08:19,259 --> 00:08:23,579
that process becomes instantaneous and

00:08:20,909 --> 00:08:25,619
so by instantaneous meaning you hit some

00:08:23,579 --> 00:08:27,329
end point it walks through the network

00:08:25,619 --> 00:08:29,219
really quickly and tells you with some

00:08:27,329 --> 00:08:32,430
amount of certainty this picture is you

00:08:29,219 --> 00:08:35,219
know 97% a dog or it might be a panda in

00:08:32,430 --> 00:08:37,709
some in some very small small cases

00:08:35,219 --> 00:08:39,569
right the advantage of this is that you

00:08:37,709 --> 00:08:41,789
know you can train your model offline

00:08:39,569 --> 00:08:44,250
for a very long time and then hand this

00:08:41,789 --> 00:08:45,509
to someone else and say ok this now is a

00:08:44,250 --> 00:08:46,980
train a neural network that is able to

00:08:45,509 --> 00:08:48,870
recognize dogs and they can put that in

00:08:46,980 --> 00:08:53,699
their software and it will run you know

00:08:48,870 --> 00:08:54,400
very quickly ok so just to recap again

00:08:53,699 --> 00:08:56,110
real quick on

00:08:54,400 --> 00:08:57,430
what these different layers do so you

00:08:56,110 --> 00:08:59,740
know instead of the human have to having

00:08:57,430 --> 00:09:01,060
to extract these features the the deeper

00:08:59,740 --> 00:09:02,980
neural network is actually able to you

00:09:01,060 --> 00:09:05,500
know at one layer it'll automatically

00:09:02,980 --> 00:09:07,090
detect some sort of you know pieces of

00:09:05,500 --> 00:09:09,220
what might end up becoming a dog snout

00:09:07,090 --> 00:09:11,260
and at the next layer that sort of forms

00:09:09,220 --> 00:09:12,940
itself into I recognize this is a common

00:09:11,260 --> 00:09:15,130
feature that makes up parts of a dog's

00:09:12,940 --> 00:09:17,170
face and then at the final layer it's

00:09:15,130 --> 00:09:18,790
able to say okay great this is actually

00:09:17,170 --> 00:09:19,990
what a dog looks like and you know

00:09:18,790 --> 00:09:22,150
because of lots of different samples

00:09:19,990 --> 00:09:23,470
it's able to over time extrapolate you

00:09:22,150 --> 00:09:25,420
know kind of the idea of what the

00:09:23,470 --> 00:09:28,030
abstract idea internally in the computer

00:09:25,420 --> 00:09:29,110
who knows exactly what it's thinking but

00:09:28,030 --> 00:09:31,270
it's able to figure out what a dog

00:09:29,110 --> 00:09:34,780
actually looks like in terms of its own

00:09:31,270 --> 00:09:36,370
neural networks okay so for the demo

00:09:34,780 --> 00:09:39,460
itself what I'm going to do is I'm going

00:09:36,370 --> 00:09:41,950
to train the inception v3 image

00:09:39,460 --> 00:09:43,300
classification model on the sea far ten

00:09:41,950 --> 00:09:46,300
dataset so for those of you not familiar

00:09:43,300 --> 00:09:47,950
with what those are the inception v3

00:09:46,300 --> 00:09:49,420
image classification model is an open

00:09:47,950 --> 00:09:50,740
source image recognition model it's

00:09:49,420 --> 00:09:53,020
something I just took off the internet

00:09:50,740 --> 00:09:54,250
it's able to you know treat you if you

00:09:53,020 --> 00:09:55,630
train it with a bunch of data it will

00:09:54,250 --> 00:09:57,670
eventually be able to recognize a bunch

00:09:55,630 --> 00:10:00,370
of images this image that I show here

00:09:57,670 --> 00:10:03,430
down below that's actually a snapshot of

00:10:00,370 --> 00:10:06,370
the representation of the real Inception

00:10:03,430 --> 00:10:08,260
v3 model it's not just a mock-up and

00:10:06,370 --> 00:10:10,540
then the sea far ten dataset is a

00:10:08,260 --> 00:10:12,520
well-known dataset with 60,000 low

00:10:10,540 --> 00:10:13,870
resolution images with ten different

00:10:12,520 --> 00:10:16,780
classes of objects so there's trucks

00:10:13,870 --> 00:10:18,550
planes ships birds cats etc so we're

00:10:16,780 --> 00:10:20,170
going to train this model to recognize

00:10:18,550 --> 00:10:23,860
all these different types of objects

00:10:20,170 --> 00:10:25,480
right and what I'm gonna do for this

00:10:23,860 --> 00:10:26,680
demo is I'm basically going to set it up

00:10:25,480 --> 00:10:28,990
in two different ways I'm gonna run two

00:10:26,680 --> 00:10:30,400
different tensor flow jobs on this one

00:10:28,990 --> 00:10:32,380
of them is going to be a non distributed

00:10:30,400 --> 00:10:33,850
job running on a single worker so I'm

00:10:32,380 --> 00:10:35,650
going to feed the input into that it's

00:10:33,850 --> 00:10:37,300
gonna run on that one worker try and

00:10:35,650 --> 00:10:39,430
output a train model and then I'm going

00:10:37,300 --> 00:10:40,900
to do the same thing in a distributed

00:10:39,430 --> 00:10:42,580
job where I have several workers some of

00:10:40,900 --> 00:10:47,110
which will be running on CPUs and some

00:10:42,580 --> 00:10:50,500
of which will be running on GPUs in this

00:10:47,110 --> 00:10:52,210
setup I basically have a a one master

00:10:50,500 --> 00:10:55,300
eight agent cluster where each of these

00:10:52,210 --> 00:10:57,970
agents has four Tesla kad GPUs on it

00:10:55,300 --> 00:10:59,970
eight CPUs and 32 gigabytes of memory

00:10:57,970 --> 00:11:02,350
and I'm running this all on top of

00:10:59,970 --> 00:11:04,780
Google compute engine to do my

00:11:02,350 --> 00:11:06,250
calculations

00:11:04,780 --> 00:11:07,960
once the done was actually done so we

00:11:06,250 --> 00:11:09,310
won't look at this part right now but

00:11:07,960 --> 00:11:12,250
once it's all done we'll be able to

00:11:09,310 --> 00:11:14,410
connect a visualization tool called

00:11:12,250 --> 00:11:15,760
tensor board to the data that's output

00:11:14,410 --> 00:11:17,410
from this train model and we can kind of

00:11:15,760 --> 00:11:18,970
see you know it won't be done because

00:11:17,410 --> 00:11:20,830
this is a series model that takes weeks

00:11:18,970 --> 00:11:22,540
to run but we'll be able to see the

00:11:20,830 --> 00:11:25,390
progress of what's going on in real time

00:11:22,540 --> 00:11:26,920
as to how the two models are compared

00:11:25,390 --> 00:11:29,470
you know progressing alongside each

00:11:26,920 --> 00:11:31,120
other so the the single node version and

00:11:29,470 --> 00:11:33,370
the distributed version walking

00:11:31,120 --> 00:11:34,270
side-by-side and just to reiterate what

00:11:33,370 --> 00:11:35,800
I just said you know this is a serious

00:11:34,270 --> 00:11:37,570
model it's gonna take a you know

00:11:35,800 --> 00:11:39,160
potentially over a week to fully train

00:11:37,570 --> 00:11:40,900
even on a cluster of expensive machines

00:11:39,160 --> 00:11:42,670
and the goal here is simply just to

00:11:40,900 --> 00:11:45,370
demonstrate how easy it is to deploy and

00:11:42,670 --> 00:11:49,060
monitor these large tensorflow jobs on

00:11:45,370 --> 00:11:50,830
DC US right okay so with that I will

00:11:49,060 --> 00:11:52,750
jump to the demo and I usually like to

00:11:50,830 --> 00:11:55,540
record my demos and I didn't this time

00:11:52,750 --> 00:11:58,780
so I'm really hoping everything works as

00:11:55,540 --> 00:12:00,550
expected but I have a script here

00:11:58,780 --> 00:12:03,070
basically just to kind of give you a

00:12:00,550 --> 00:12:04,180
quick rundown of everything that I'm

00:12:03,070 --> 00:12:06,280
gonna do so the first thing I'm going to

00:12:04,180 --> 00:12:07,510
do is I'm going to show you that I

00:12:06,280 --> 00:12:09,700
actually have this cluster up and

00:12:07,510 --> 00:12:11,710
running so I have a terminal already

00:12:09,700 --> 00:12:14,110
open here I have a command line tool

00:12:11,710 --> 00:12:16,030
they wrote called DCOs GCP which it's

00:12:14,110 --> 00:12:17,980
not something that that that's that's

00:12:16,030 --> 00:12:19,240
available for for for everyone to use

00:12:17,980 --> 00:12:20,980
but it's you know it's something that I

00:12:19,240 --> 00:12:22,870
use myself just to kind of get clustered

00:12:20,980 --> 00:12:24,280
up and running on GCE hardware and so

00:12:22,870 --> 00:12:27,130
you can see that in this one I'm running

00:12:24,280 --> 00:12:29,530
a stable version of DC OS which is DC

00:12:27,130 --> 00:12:31,870
u.s. 1.10 I've got a single master I've

00:12:29,530 --> 00:12:33,670
got a single agents I've sorry I've got

00:12:31,870 --> 00:12:35,170
eight agents the status of the cluster

00:12:33,670 --> 00:12:38,080
is currently healthy and you know this

00:12:35,170 --> 00:12:40,600
is my leading master IP from there I

00:12:38,080 --> 00:12:43,060
want to show you know that we now have

00:12:40,600 --> 00:12:48,940
this tensorflow package inside the DC OS

00:12:43,060 --> 00:12:51,640
catalog so if I come back to here and I

00:12:48,940 --> 00:12:53,320
go to the catalog if you go ahead and

00:12:51,640 --> 00:12:54,910
start typing tensorflow you'll see that

00:12:53,320 --> 00:12:57,190
there is a beta tensorflow package

00:12:54,910 --> 00:12:58,960
available this went live yesterday so if

00:12:57,190 --> 00:13:01,090
any of you ever spin up a DC rs110

00:12:58,960 --> 00:13:04,450
cluster now this should be available for

00:13:01,090 --> 00:13:06,100
you to use from there what I what I

00:13:04,450 --> 00:13:08,110
would do is I would clone this repo

00:13:06,100 --> 00:13:09,880
called the DC OS tensorflow tools repo

00:13:08,110 --> 00:13:12,340
it's got a bunch of examples including

00:13:09,880 --> 00:13:14,350
this CFR 10 example that I'm going to

00:13:12,340 --> 00:13:16,990
run through I've obviously already pre

00:13:14,350 --> 00:13:17,660
cloned this and I'm going to CD into it

00:13:16,990 --> 00:13:19,910
and then I'm going to show

00:13:17,660 --> 00:13:22,400
you these these two examples I'm gonna

00:13:19,910 --> 00:13:24,680
run the CFR single and the CFR multiple

00:13:22,400 --> 00:13:29,450
example so if I go ahead and open these

00:13:24,680 --> 00:13:30,920
two guys up we can see what it what an

00:13:29,450 --> 00:13:32,680
actual package definition of something

00:13:30,920 --> 00:13:34,850
you might want to launch on DC u.s.

00:13:32,680 --> 00:13:36,290
senator flow is so you know one model

00:13:34,850 --> 00:13:37,850
would be if I wanted to launch this

00:13:36,290 --> 00:13:40,430
package I could click on here

00:13:37,850 --> 00:13:42,050
I could go to configure I could go here

00:13:40,430 --> 00:13:44,390
and manually specify a whole bunch of

00:13:42,050 --> 00:13:47,300
parameters here but typically the the

00:13:44,390 --> 00:13:50,060
the simpler deployment strategy is to

00:13:47,300 --> 00:13:51,260
have a a JSON file to find that fills in

00:13:50,060 --> 00:13:52,970
all those parameters then you can use

00:13:51,260 --> 00:13:54,770
the command line to deploy right and so

00:13:52,970 --> 00:13:56,450
that's what I'm gonna do here and so we

00:13:54,770 --> 00:13:58,280
can see that you know we have a URL to a

00:13:56,450 --> 00:14:02,240
zip file that contains all of the

00:13:58,280 --> 00:14:04,130
artifacts that we need in order to you

00:14:02,240 --> 00:14:06,200
know run this application on in any

00:14:04,130 --> 00:14:08,180
individual node in the cluster there's a

00:14:06,200 --> 00:14:10,550
path to the actual binary that I want to

00:14:08,180 --> 00:14:12,950
execute once that zip file has been

00:14:10,550 --> 00:14:14,630
downloaded on an individual machine I've

00:14:12,950 --> 00:14:16,910
got some name for the job that I want to

00:14:14,630 --> 00:14:20,620
run there's some job context which is

00:14:16,910 --> 00:14:23,390
some variables set up inside of the

00:14:20,620 --> 00:14:24,290
inside of the job that you did or you

00:14:23,390 --> 00:14:25,670
know basically they're gonna be

00:14:24,290 --> 00:14:26,480
parameters that I can see in tensor

00:14:25,670 --> 00:14:29,210
board and then I could potentially

00:14:26,480 --> 00:14:31,310
change on the fly to optimize how this

00:14:29,210 --> 00:14:33,680
model actually runs over time and then a

00:14:31,310 --> 00:14:35,270
bunch of things pointing it at a shared

00:14:33,680 --> 00:14:37,790
file system where I want my output to

00:14:35,270 --> 00:14:40,220
eventually be put that includes the data

00:14:37,790 --> 00:14:41,570
for the model being trained and so on

00:14:40,220 --> 00:14:42,950
and where the real meter this comes is

00:14:41,570 --> 00:14:45,410
that you can see that there's sections

00:14:42,950 --> 00:14:47,660
in here for specifying how many GPU

00:14:45,410 --> 00:14:49,700
workers I want how many workers I want

00:14:47,660 --> 00:14:51,350
which are just normal CPU workers and

00:14:49,700 --> 00:14:53,720
how many parameter servers I want right

00:14:51,350 --> 00:14:55,670
so I can quickly just specify exactly

00:14:53,720 --> 00:14:58,130
what I want the configuration of this

00:14:55,670 --> 00:14:59,840
cluster to be in terms of how many

00:14:58,130 --> 00:15:02,150
parameter servers and workers I have and

00:14:59,840 --> 00:15:03,440
then you know DC US will take care of

00:15:02,150 --> 00:15:04,610
stitching that all together and getting

00:15:03,440 --> 00:15:06,110
it running for it and so you know

00:15:04,610 --> 00:15:08,300
obviously in this single example I've

00:15:06,110 --> 00:15:10,340
got a single GPU worker and a single

00:15:08,300 --> 00:15:13,280
parameter server and if I go ahead and

00:15:10,340 --> 00:15:16,010
just quickly show you the the sefa or

00:15:13,280 --> 00:15:17,900
multiple one the only real difference in

00:15:16,010 --> 00:15:19,670
here is that the name has changed and

00:15:17,900 --> 00:15:22,610
now I'm gonna run it on three GPU

00:15:19,670 --> 00:15:28,200
workers and two parameter servers make

00:15:22,610 --> 00:15:31,050
sense all right so with that

00:15:28,200 --> 00:15:32,010
I think the next step is oh the next

00:15:31,050 --> 00:15:33,180
thing I want to do is I just want to

00:15:32,010 --> 00:15:34,890
show you real quick you know I mentioned

00:15:33,180 --> 00:15:37,170
the storage bucket where all of this

00:15:34,890 --> 00:15:38,730
outputs going to be dropped I wanted to

00:15:37,170 --> 00:15:39,900
just go there real quickly and I ran

00:15:38,730 --> 00:15:42,240
this a little bit earlier today so

00:15:39,900 --> 00:15:43,890
there's this folder in here that's

00:15:42,240 --> 00:15:45,900
produced called mesas con I'm gonna

00:15:43,890 --> 00:15:47,910
actually go ahead and delete that just

00:15:45,900 --> 00:15:49,530
to show you know once this actually runs

00:15:47,910 --> 00:15:51,300
that will get created again and we'll

00:15:49,530 --> 00:15:54,590
have brand new data populated inside

00:15:51,300 --> 00:15:57,780
there I also want to just quickly show

00:15:54,590 --> 00:16:00,930
that you know at the moment there is

00:15:57,780 --> 00:16:02,370
absolutely no CPUs or GPUs allocated to

00:16:00,930 --> 00:16:03,780
anything in the cluster and once I

00:16:02,370 --> 00:16:04,890
launched these guys we'll see that you

00:16:03,780 --> 00:16:06,270
know they get allocated to these

00:16:04,890 --> 00:16:08,070
different jobs and then they start

00:16:06,270 --> 00:16:12,990
executing on there so right now I've got

00:16:08,070 --> 00:16:16,530
64 cpus free 30g Jupe 32 GPUs free and

00:16:12,990 --> 00:16:19,130
so on so if I now take this DC RS

00:16:16,530 --> 00:16:23,930
package install beta its beta tensorflow

00:16:19,130 --> 00:16:25,980
commands and run it with that CFR single

00:16:23,930 --> 00:16:27,870
JSON configuration that I showed you

00:16:25,980 --> 00:16:30,300
guys before go ahead and run that from

00:16:27,870 --> 00:16:33,690
the command line say yes I accept that

00:16:30,300 --> 00:16:35,640
it's a beta version it's going to go

00:16:33,690 --> 00:16:42,350
through and execute it and if I go ahead

00:16:35,640 --> 00:16:42,350
and start up the second one there we go

00:16:42,470 --> 00:16:51,420
agree to that and then jump back to the

00:16:47,420 --> 00:16:53,640
to the dashboard and go to services I

00:16:51,420 --> 00:16:54,870
should see both of these guys coming up

00:16:53,640 --> 00:16:56,820
and running soon so they're both in the

00:16:54,870 --> 00:16:58,230
deployment phase so you know once these

00:16:56,820 --> 00:17:00,540
guys actually start running they'll have

00:16:58,230 --> 00:17:02,250
a whole bunch of jobs underneath them

00:17:00,540 --> 00:17:04,320
because this is these tensor flow

00:17:02,250 --> 00:17:05,820
packages built with our SDK so it's not

00:17:04,320 --> 00:17:08,699
just a single job that's launched it's a

00:17:05,820 --> 00:17:10,079
collection of jobs where the first job

00:17:08,699 --> 00:17:11,850
that comes up with the scheduler and

00:17:10,079 --> 00:17:13,740
then eventually a bunch of other jobs

00:17:11,850 --> 00:17:15,000
will pop up so I'm just going to leave

00:17:13,740 --> 00:17:16,949
this running for now and we'll come back

00:17:15,000 --> 00:17:18,600
to this at the end of the talk and see

00:17:16,949 --> 00:17:20,790
what the progress is that these guys are

00:17:18,600 --> 00:17:22,800
making actually the Seafarer single one

00:17:20,790 --> 00:17:26,249
we can see is already started and is

00:17:22,800 --> 00:17:32,470
healthy okay

00:17:26,249 --> 00:17:38,350
and so is that one right jumping back to

00:17:32,470 --> 00:17:39,669
the talk so what I want to go through

00:17:38,350 --> 00:17:41,320
next as I mentioned I wanted to talk

00:17:39,669 --> 00:17:43,299
about really quickly what the typical

00:17:41,320 --> 00:17:45,100
developer workflow for someone working

00:17:43,299 --> 00:17:46,809
with tensorflow looks like first in a

00:17:45,100 --> 00:17:48,639
single node environments and then in a

00:17:46,809 --> 00:17:49,869
distributed environment and sort of show

00:17:48,639 --> 00:17:51,220
you what some of these challenges are

00:17:49,869 --> 00:17:53,619
with actually getting something to run

00:17:51,220 --> 00:17:54,669
in a distributed environment right so

00:17:53,619 --> 00:17:55,840
the first thing someone would do if they

00:17:54,669 --> 00:17:57,460
wanted to work with sensor flow is they

00:17:55,840 --> 00:17:59,080
probably download and install the Python

00:17:57,460 --> 00:18:00,850
tensorflow library you need to do that

00:17:59,080 --> 00:18:02,649
is the very first step so you can get

00:18:00,850 --> 00:18:04,299
anything going right then you want to

00:18:02,649 --> 00:18:05,919
design your model in terms of tensorflow

00:18:04,299 --> 00:18:07,419
basic machine learning primitives make

00:18:05,919 --> 00:18:09,700
sure that you can use those primitives

00:18:07,419 --> 00:18:11,470
to actually do the job that you have at

00:18:09,700 --> 00:18:13,029
hand then you write your code you

00:18:11,470 --> 00:18:14,499
optimize it for single node performance

00:18:13,029 --> 00:18:17,320
and then you train your data on that

00:18:14,499 --> 00:18:19,239
single node and I'll pop some train

00:18:17,320 --> 00:18:21,989
model in the end right pretty

00:18:19,239 --> 00:18:23,859
straightforward in in in relative terms

00:18:21,989 --> 00:18:25,269
now if you want to move this to a

00:18:23,859 --> 00:18:26,559
distributed setting you know the first

00:18:25,269 --> 00:18:28,779
three steps are very similar you

00:18:26,559 --> 00:18:30,399
download the the central library you

00:18:28,779 --> 00:18:32,289
design your model in terms of those you

00:18:30,399 --> 00:18:34,230
write your code now optimized for

00:18:32,289 --> 00:18:37,239
distributed computation rather than

00:18:34,230 --> 00:18:38,379
single node computation but where the

00:18:37,239 --> 00:18:39,909
real challenge come is that now you've

00:18:38,379 --> 00:18:40,659
got all these other steps to get this up

00:18:39,909 --> 00:18:42,070
and running right

00:18:40,659 --> 00:18:43,779
you have to provision some set of

00:18:42,070 --> 00:18:45,789
machines to run your computation you

00:18:43,779 --> 00:18:47,559
have to install tensorflow on them you

00:18:45,789 --> 00:18:49,059
have to write the code to map any of

00:18:47,559 --> 00:18:50,830
your distributed computations to the

00:18:49,059 --> 00:18:51,940
exact IP address of the machine where

00:18:50,830 --> 00:18:53,769
those computations are going to be

00:18:51,940 --> 00:18:55,690
performed you have to deploy your code

00:18:53,769 --> 00:18:57,879
on every machine and only once you've

00:18:55,690 --> 00:18:59,859
done all of that then you can start

00:18:57,879 --> 00:19:01,809
training your data on the cluster and

00:18:59,859 --> 00:19:03,639
eventually output some train model right

00:19:01,809 --> 00:19:04,989
and this whole process can be tedious

00:19:03,639 --> 00:19:06,609
and error-prone especially if you want

00:19:04,989 --> 00:19:07,720
to iterate on different optimizations

00:19:06,609 --> 00:19:09,909
that you're trying to make in the code

00:19:07,720 --> 00:19:11,739
over time so what DCOs does is it

00:19:09,909 --> 00:19:13,179
automates all of this process for you it

00:19:11,739 --> 00:19:15,070
basically gives you a very similar

00:19:13,179 --> 00:19:17,950
workflow for what you would have with

00:19:15,070 --> 00:19:22,149
your single node setup but now you can

00:19:17,950 --> 00:19:23,320
do it in a distributed way so you know

00:19:22,149 --> 00:19:24,909
what are some of these challenges in a

00:19:23,320 --> 00:19:26,379
little bit more detail so the first one

00:19:24,909 --> 00:19:29,169
which I which I hinted at at the a

00:19:26,379 --> 00:19:30,940
minute ago is that there is a cluster

00:19:29,169 --> 00:19:33,580
spec that you have to specify at the top

00:19:30,940 --> 00:19:34,840
of any distributed tensorflow

00:19:33,580 --> 00:19:36,429
application that you're writing and it's

00:19:34,840 --> 00:19:38,109
incredibly tedious to actually try and

00:19:36,429 --> 00:19:39,820
keep in sync with whatever machines

00:19:38,109 --> 00:19:41,260
you've created and deployed so

00:19:39,820 --> 00:19:42,850
you know users basically need to rewrite

00:19:41,260 --> 00:19:44,170
this code for every job they want to run

00:19:42,850 --> 00:19:45,940
in a distributed setting you go

00:19:44,170 --> 00:19:48,040
provision your machines you can come

00:19:45,940 --> 00:19:49,810
back to this cluster spec you specify

00:19:48,040 --> 00:19:51,580
the IP imports of all the workers that

00:19:49,810 --> 00:19:53,560
you've provisioned you do the same thing

00:19:51,580 --> 00:19:54,850
for all your parameter servers and then

00:19:53,560 --> 00:19:56,290
you even have to do this for any code

00:19:54,850 --> 00:19:57,880
that you inherit from your standard

00:19:56,290 --> 00:19:59,350
models right so if you take some code

00:19:57,880 --> 00:20:00,580
off the internet and you want to run

00:19:59,350 --> 00:20:02,590
that in some distribute setting you have

00:20:00,580 --> 00:20:05,080
to go into the code create this cluster

00:20:02,590 --> 00:20:06,460
spec and then you know stitch all of

00:20:05,080 --> 00:20:08,050
this stuff together before you can run

00:20:06,460 --> 00:20:10,360
it

00:20:08,050 --> 00:20:12,730
next thing is that dealing dealing with

00:20:10,360 --> 00:20:14,140
failures is is not typically graceful in

00:20:12,730 --> 00:20:16,570
a traditional tensorflow

00:20:14,140 --> 00:20:18,700
setup unless you do some a lot of work

00:20:16,570 --> 00:20:20,470
yourself so you know users need to stop

00:20:18,700 --> 00:20:22,840
training change their hard-coded cluster

00:20:20,470 --> 00:20:24,460
spec and then manually restart any jobs

00:20:22,840 --> 00:20:26,140
if something happens to go wrong there's

00:20:24,460 --> 00:20:28,750
no easy way to just tear it all down and

00:20:26,140 --> 00:20:32,200
deploy something new if you found a bug

00:20:28,750 --> 00:20:33,310
in what you were working on the next

00:20:32,200 --> 00:20:34,780
thing is that you know manually

00:20:33,310 --> 00:20:36,190
configuring each of the nodes in the

00:20:34,780 --> 00:20:38,740
cluster that you want to run your tents

00:20:36,190 --> 00:20:40,030
for the job on can take a long time and

00:20:38,740 --> 00:20:42,220
is you know typically very error-prone

00:20:40,030 --> 00:20:44,170
you know one example of this is setting

00:20:42,220 --> 00:20:45,520
up access to a shared file system so you

00:20:44,170 --> 00:20:47,560
can actually checkpoint some of these

00:20:45,520 --> 00:20:49,030
summaries and the output data that you

00:20:47,560 --> 00:20:50,380
want to have come back from your train

00:20:49,030 --> 00:20:51,910
model you know that requires

00:20:50,380 --> 00:20:53,020
authenticating onto every node somehow

00:20:51,910 --> 00:20:55,150
passing some credentials there

00:20:53,020 --> 00:20:56,470
potentially and so on and then in

00:20:55,150 --> 00:20:58,420
addition to this if you ever want to

00:20:56,470 --> 00:21:00,550
tweak any of your parameters to make

00:20:58,420 --> 00:21:02,920
your your model run in a more optimized

00:21:00,550 --> 00:21:04,690
fashion that requires reuploading your

00:21:02,920 --> 00:21:08,770
code to every node and then redeploying

00:21:04,690 --> 00:21:11,350
everything right so you know how does

00:21:08,770 --> 00:21:12,880
DCOs fix this well instead of having you

00:21:11,350 --> 00:21:14,020
define one of these cluster specs for

00:21:12,880 --> 00:21:16,420
everything that you do we have this

00:21:14,020 --> 00:21:18,040
high-level service definition which I

00:21:16,420 --> 00:21:19,720
showed you guys earlier for the the two

00:21:18,040 --> 00:21:21,160
examples that were running now where

00:21:19,720 --> 00:21:23,290
instead of saying okay here's the exact

00:21:21,160 --> 00:21:25,000
machines and here's what type of you

00:21:23,290 --> 00:21:26,560
know worker or parameter server or

00:21:25,000 --> 00:21:28,300
component they they associate with

00:21:26,560 --> 00:21:30,840
instead you can just say well this is

00:21:28,300 --> 00:21:34,150
how many workers I want here's how many

00:21:30,840 --> 00:21:35,860
parameter servers I want and then DCOs

00:21:34,150 --> 00:21:38,140
and the SDK will take care of

00:21:35,860 --> 00:21:39,970
automatically creating this cluster spec

00:21:38,140 --> 00:21:40,930
for you and then passing it down to the

00:21:39,970 --> 00:21:43,650
code that you're actually going to

00:21:40,930 --> 00:21:45,940
launch right so that's kind of what this

00:21:43,650 --> 00:21:46,900
next picture shows and the thing I want

00:21:45,940 --> 00:21:48,070
to highlight from this one though is

00:21:46,900 --> 00:21:50,170
that what we're really doing here is

00:21:48,070 --> 00:21:51,760
we're sort of separating the deployer

00:21:50,170 --> 00:21:52,400
responsibilities from the developer

00:21:51,760 --> 00:21:53,960
response

00:21:52,400 --> 00:21:55,970
right the developer can say I'm gonna

00:21:53,960 --> 00:21:57,800
focus on how do I build my model the

00:21:55,970 --> 00:21:58,820
best way I possibly can and then I'm

00:21:57,800 --> 00:22:01,160
going to leave it up to the person that

00:21:58,820 --> 00:22:02,510
actually goes to deploy this to specify

00:22:01,160 --> 00:22:05,900
okay well this is how many parameter

00:22:02,510 --> 00:22:07,460
servers I want this is how many workers

00:22:05,900 --> 00:22:09,920
I want to actually execute this model

00:22:07,460 --> 00:22:14,120
for this instance of running it right

00:22:09,920 --> 00:22:17,120
and the SDK and and and and ecos will

00:22:14,120 --> 00:22:19,280
take care of this mapping for you the

00:22:17,120 --> 00:22:21,710
other thing that we do is because we're

00:22:19,280 --> 00:22:23,420
running with the DC OS Commons SDK we

00:22:21,710 --> 00:22:25,640
have the ability to clean read cleanly

00:22:23,420 --> 00:22:27,380
restart any failed tasks and reconnect

00:22:25,640 --> 00:22:29,450
them to the cluster if something goes

00:22:27,380 --> 00:22:31,130
wrong right as I mentioned before if

00:22:29,450 --> 00:22:33,080
this happens in a traditional setting

00:22:31,130 --> 00:22:34,700
you have to you know first of all notice

00:22:33,080 --> 00:22:36,890
that it happened and then go in and

00:22:34,700 --> 00:22:39,970
restart the jobs yourself manually here

00:22:36,890 --> 00:22:42,320
will restart them for you using the SDK

00:22:39,970 --> 00:22:43,760
second one is that any credentials that

00:22:42,320 --> 00:22:45,710
you want to pass you don't have to get

00:22:43,760 --> 00:22:47,720
authentication onto the into the rest of

00:22:45,710 --> 00:22:51,200
the system you can use our integrated

00:22:47,720 --> 00:22:53,480
DCs secrets service to you know store

00:22:51,200 --> 00:22:55,070
secrets beforehand and then every

00:22:53,480 --> 00:22:57,190
service has that available to them so I

00:22:55,070 --> 00:23:00,190
actually use this in this example to

00:22:57,190 --> 00:23:03,020
give my tensorflow application

00:23:00,190 --> 00:23:05,060
credentials to write all of its output

00:23:03,020 --> 00:23:07,070
to the Google storage bucket that I have

00:23:05,060 --> 00:23:08,960
running you know external to the cluster

00:23:07,070 --> 00:23:10,490
so I didn't show you this but one thing

00:23:08,960 --> 00:23:12,710
I did before when I first set up the

00:23:10,490 --> 00:23:14,300
cluster was I went in and I did some

00:23:12,710 --> 00:23:16,700
service account credentials from Google

00:23:14,300 --> 00:23:20,270
put that in a secret and then I pointed

00:23:16,700 --> 00:23:21,890
my jobs at that secret so it would know

00:23:20,270 --> 00:23:25,490
how to how to pull it down and use it to

00:23:21,890 --> 00:23:26,780
write its output right the last thing is

00:23:25,490 --> 00:23:28,820
that we actually use a runtime

00:23:26,780 --> 00:23:30,170
configuration dictionary to quick to

00:23:28,820 --> 00:23:32,420
quickly tweak any of these hyper

00:23:30,170 --> 00:23:33,860
parameters that you might have between

00:23:32,420 --> 00:23:36,290
different runs of the same models so if

00:23:33,860 --> 00:23:37,760
you know you want to you know you you

00:23:36,290 --> 00:23:39,440
run something you realize oh this isn't

00:23:37,760 --> 00:23:40,670
quite running as quickly as I'd want to

00:23:39,440 --> 00:23:42,860
I want to be able to optimize something

00:23:40,670 --> 00:23:45,050
I can quickly just jump in and change

00:23:42,860 --> 00:23:46,970
these parameters without actually

00:23:45,050 --> 00:23:48,500
tearing down and redeploying the cluster

00:23:46,970 --> 00:23:50,480
from scratch you can just tweak these on

00:23:48,500 --> 00:23:52,940
the fly and then the for the ones that

00:23:50,480 --> 00:23:55,760
matter the SDK will automatically make

00:23:52,940 --> 00:23:57,410
sure to redeploy just those instances of

00:23:55,760 --> 00:24:02,060
the of the workers and parameter servers

00:23:57,410 --> 00:24:04,610
and so on make sense okay so that's it

00:24:02,060 --> 00:24:06,140
for the the meat of the talk hopefully

00:24:04,610 --> 00:24:08,060
I've given these jobs

00:24:06,140 --> 00:24:10,070
that I launched on and eCos enough time

00:24:08,060 --> 00:24:11,930
to actually produce some output so we

00:24:10,070 --> 00:24:15,500
can look at it through tensor board so

00:24:11,930 --> 00:24:17,810
if I go back to my UI I should be able

00:24:15,500 --> 00:24:20,660
to quick through see far multiple and I

00:24:17,810 --> 00:24:24,320
see that there's still some node staging

00:24:20,660 --> 00:24:26,120
here let's see it looks like I didn't

00:24:24,320 --> 00:24:28,370
quite give these guys enough time to

00:24:26,120 --> 00:24:29,950
actually finish and start running

00:24:28,370 --> 00:24:40,430
either that or they're actually failing

00:24:29,950 --> 00:24:42,530
which is not good let's see hmm there's

00:24:40,430 --> 00:24:44,210
nothing I can really do to combat this

00:24:42,530 --> 00:24:46,100
at the moment fortunately I'm not gonna

00:24:44,210 --> 00:24:47,870
be able to show you data from the live

00:24:46,100 --> 00:24:50,480
demo this is why I normally record this

00:24:47,870 --> 00:24:52,850
stuff right but what I do have is I have

00:24:50,480 --> 00:24:58,760
a backup of this so instead of pointing

00:24:52,850 --> 00:25:01,040
my my job at the the the data that I

00:24:58,760 --> 00:25:04,100
would have had in this bucket I do have

00:25:01,040 --> 00:25:07,310
a from a run of this from earlier today

00:25:04,100 --> 00:25:10,730
or last night actually where if I go

00:25:07,310 --> 00:25:12,530
ahead and I grab the censor board

00:25:10,730 --> 00:25:13,960
command which would be pointing at all

00:25:12,530 --> 00:25:16,970
of the data that popped out from these

00:25:13,960 --> 00:25:21,200
if I point that at a clue squid sensor

00:25:16,970 --> 00:25:22,760
flow backup repo I should be able to

00:25:21,200 --> 00:25:24,410
pull this data that I that I ran through

00:25:22,760 --> 00:25:27,470
earlier to gather the data from these

00:25:24,410 --> 00:25:29,840
guys okay so if I do that I now have a

00:25:27,470 --> 00:25:32,240
server running on my local host so I'll

00:25:29,840 --> 00:25:37,850
go ahead and grab that come back to the

00:25:32,240 --> 00:25:39,350
browser and execute it what this is

00:25:37,850 --> 00:25:42,710
going to do is it's going to load all of

00:25:39,350 --> 00:25:45,140
the data from those and we should be

00:25:42,710 --> 00:25:49,070
able to browse through all of the data

00:25:45,140 --> 00:25:52,850
that results from its see actually takes

00:25:49,070 --> 00:25:54,740
quite a while to load so we may not see

00:25:52,850 --> 00:25:56,720
it come up for a while so why don't I go

00:25:54,740 --> 00:25:58,730
ahead and jump back to the slides and

00:25:56,720 --> 00:26:00,440
just finish up what I have from here and

00:25:58,730 --> 00:26:03,320
hopefully we can come back and see

00:26:00,440 --> 00:26:05,270
what's going on there so one thing I

00:26:03,320 --> 00:26:06,530
wanted to do is you know just reiterate

00:26:05,270 --> 00:26:09,020
what the demo set up was going to be

00:26:06,530 --> 00:26:11,240
it's training this inception v3 image

00:26:09,020 --> 00:26:14,510
classification model running it on the

00:26:11,240 --> 00:26:15,920
two different settings and then you know

00:26:14,510 --> 00:26:17,810
looking and visualizing that through

00:26:15,920 --> 00:26:19,549
tensor board so one thing I wanted to

00:26:17,810 --> 00:26:21,080
just touch on here then is you

00:26:19,549 --> 00:26:24,019
kind of what our next steps for all this

00:26:21,080 --> 00:26:26,840
stuff are so what we have today is this

00:26:24,019 --> 00:26:28,340
single framework so what that means is

00:26:26,840 --> 00:26:31,190
that you know you you take this

00:26:28,340 --> 00:26:32,929
framework using our SDK you install it

00:26:31,190 --> 00:26:34,190
by a standard eCos package management

00:26:32,929 --> 00:26:37,610
tools so the command-line that you saw

00:26:34,190 --> 00:26:40,249
me run was a DCOs package install beta

00:26:37,610 --> 00:26:41,840
tensorflow and so on right but because

00:26:40,249 --> 00:26:44,239
of the the way that that's currently run

00:26:41,840 --> 00:26:46,039
you actually have to manually start and

00:26:44,239 --> 00:26:48,049
stop and remove the framework from the

00:26:46,039 --> 00:26:49,730
cluster whenever it completes right and

00:26:48,049 --> 00:26:50,989
so this is a little bit tedious because

00:26:49,730 --> 00:26:52,940
you have to monitor it make sure you

00:26:50,989 --> 00:26:54,619
know when it's done tear it down

00:26:52,940 --> 00:26:55,700
you know remove it from the cluster and

00:26:54,619 --> 00:26:56,989
then if you ever have a second one you

00:26:55,700 --> 00:26:59,269
want around you have to go and do the

00:26:56,989 --> 00:27:01,340
same thing right the world we want to

00:26:59,269 --> 00:27:03,109
move to is where we can have this sort

00:27:01,340 --> 00:27:05,299
of meta framework that you launch once

00:27:03,109 --> 00:27:07,669
it sits up there in the cluster you can

00:27:05,299 --> 00:27:09,139
interact with it through a a CLI and

00:27:07,669 --> 00:27:11,929
then what it will do is it will take

00:27:09,139 --> 00:27:13,489
these instances of these frameworks that

00:27:11,929 --> 00:27:15,980
we're currently able to launch today and

00:27:13,489 --> 00:27:19,519
it will be able to launch multiple of

00:27:15,980 --> 00:27:20,989
them side by side right so that you know

00:27:19,519 --> 00:27:23,059
this mater framework you can interact

00:27:20,989 --> 00:27:25,869
with it to track the progress of any of

00:27:23,059 --> 00:27:28,639
the frameworks that it's launched and

00:27:25,869 --> 00:27:30,950
you know notice when they've completed

00:27:28,639 --> 00:27:32,149
analyze their data and then even once

00:27:30,950 --> 00:27:34,190
they've come once they have completed

00:27:32,149 --> 00:27:37,220
the meta framework will store some of

00:27:34,190 --> 00:27:40,369
that information about how they ran and

00:27:37,220 --> 00:27:42,259
and where their data has been their

00:27:40,369 --> 00:27:44,929
output data has been shipped to and so

00:27:42,259 --> 00:27:46,519
on and then you know in addition that it

00:27:44,929 --> 00:27:48,409
will artem automatically start and stop

00:27:46,519 --> 00:27:50,869
any of these frameworks once they've

00:27:48,409 --> 00:27:52,399
completed and remove them so kind of the

00:27:50,869 --> 00:27:54,139
the flow way that we want to see for

00:27:52,399 --> 00:27:57,440
this is the ability to be able to say

00:27:54,139 --> 00:27:59,179
you know DCOs tensorflow run this job I

00:27:57,440 --> 00:28:00,739
want this many workers I want this many

00:27:59,179 --> 00:28:02,149
parameter servers and then you

00:28:00,739 --> 00:28:04,580
communicate with that meta server and it

00:28:02,149 --> 00:28:05,989
just goes off and launches this right so

00:28:04,580 --> 00:28:08,929
again we don't have this today but this

00:28:05,989 --> 00:28:13,279
is the world we want to move to in the

00:28:08,929 --> 00:28:15,739
next few months or or year okay so

00:28:13,279 --> 00:28:17,899
special thanks to all collaborators on

00:28:15,739 --> 00:28:19,909
this stuff so Sam Pringle he was the one

00:28:17,899 --> 00:28:21,499
that really sort of took control this

00:28:19,909 --> 00:28:23,929
and built a lot of the functionality

00:28:21,499 --> 00:28:25,820
that did we see in this he was an intern

00:28:23,929 --> 00:28:28,159
last summer and he's continued on

00:28:25,820 --> 00:28:31,249
throughout the year working kind of ten

00:28:28,159 --> 00:28:32,330
hours a week on improving this and then

00:28:31,249 --> 00:28:33,230
a bunch of other people are involved

00:28:32,330 --> 00:28:35,150
especially people

00:28:33,230 --> 00:28:37,850
the SDK team being very helpful with any

00:28:35,150 --> 00:28:40,610
questions we had to get things up and

00:28:37,850 --> 00:28:42,380
running in the way that makes sense not

00:28:40,610 --> 00:28:43,970
just for oh let's get it kind of working

00:28:42,380 --> 00:28:46,400
but eventually moving towards a

00:28:43,970 --> 00:28:49,940
certified version of this package over

00:28:46,400 --> 00:28:53,030
time right okay

00:28:49,940 --> 00:28:54,350
so questions and links you can follow in

00:28:53,030 --> 00:28:58,010
these links all of these slides are

00:28:54,350 --> 00:28:59,960
available on the on the schedule

00:28:58,010 --> 00:29:02,660
schedule comm or whatever the the

00:28:59,960 --> 00:29:04,190
website is for form a siscon you can

00:29:02,660 --> 00:29:05,870
navigate to this talk you can download

00:29:04,190 --> 00:29:09,080
these slides and all these links will be

00:29:05,870 --> 00:29:10,220
available in there the the the third one

00:29:09,080 --> 00:29:11,450
is probably the most important if you

00:29:10,220 --> 00:29:14,210
actually want to learn you know just

00:29:11,450 --> 00:29:15,890
quickly download a cluster or so I spin

00:29:14,210 --> 00:29:17,679
up a cluster download the tensorflow

00:29:15,890 --> 00:29:21,169
package and then start running something

00:29:17,679 --> 00:29:23,210
today so yep I think that's it I

00:29:21,169 --> 00:29:25,010
apologize that the demo didn't quite go

00:29:23,210 --> 00:29:28,520
as planned but these things happen

00:29:25,010 --> 00:29:33,010
sometimes and that's a record your demos

00:29:28,520 --> 00:29:37,610
beforehand alright thank you

00:29:33,010 --> 00:29:39,710
oh yeah sorry good point so yeah I

00:29:37,610 --> 00:29:41,960
mentioned that the that sometimes tensor

00:29:39,710 --> 00:29:44,809
board takes a while to load up so we can

00:29:41,960 --> 00:29:46,940
actually it's still it's still loading

00:29:44,809 --> 00:29:48,140
now but maybe after the talk if people

00:29:46,940 --> 00:29:49,640
want to look at it and see what's going

00:29:48,140 --> 00:29:51,590
on you can come up here and we can talk

00:29:49,640 --> 00:29:53,150
so I think we have about 20 more minutes

00:29:51,590 --> 00:29:55,130
in this session I ended early because of

00:29:53,150 --> 00:29:57,590
the some of the the problems with the

00:29:55,130 --> 00:30:03,370
demo there so cool all right thanks

00:29:57,590 --> 00:30:03,370
again any questions yep

00:30:06,400 --> 00:30:11,510
how different is it to the TF measles

00:30:09,080 --> 00:30:13,910
package which is available on github

00:30:11,510 --> 00:30:16,010
yeah so we originally looked at the TF

00:30:13,910 --> 00:30:18,200
mesas package and we thought about using

00:30:16,010 --> 00:30:20,210
it or leveraging it and building our our

00:30:18,200 --> 00:30:21,740
stuff around it but what we one thing

00:30:20,210 --> 00:30:23,570
that TF masers doesn't give you that

00:30:21,740 --> 00:30:26,030
this does is it doesn't give you quite

00:30:23,570 --> 00:30:27,890
the same integrated experience with DC

00:30:26,030 --> 00:30:29,420
OS that we wanted we really are pushing

00:30:27,890 --> 00:30:31,190
to have most of our services running

00:30:29,420 --> 00:30:33,020
with the SDK framework because if you

00:30:31,190 --> 00:30:34,640
remember something that Erie X mentioned

00:30:33,020 --> 00:30:36,350
in his keynote earlier today is that if

00:30:34,640 --> 00:30:38,720
you build the frameworks using the SDK

00:30:36,350 --> 00:30:40,910
there's a very common pattern for how

00:30:38,720 --> 00:30:42,530
you manage them and upgrade them and

00:30:40,910 --> 00:30:43,910
these sorts of things so we really

00:30:42,530 --> 00:30:47,510
wanted to make sure we were building it

00:30:43,910 --> 00:30:50,540
in that same ecosystem and alongside

00:30:47,510 --> 00:30:51,950
that I actually was working with the TF

00:30:50,540 --> 00:30:53,750
mesas people for quite a while to figure

00:30:51,950 --> 00:30:56,120
out how to get it running on DCOs

00:30:53,750 --> 00:30:58,160
it runs very well on standalone mesas

00:30:56,120 --> 00:31:00,170
but I couldn't quite get it running on D

00:30:58,160 --> 00:31:02,300
cos the way that I thought I wanted to

00:31:00,170 --> 00:31:03,560
and it wasn't that I just gave up but it

00:31:02,300 --> 00:31:04,940
was well we had these other reasons we

00:31:03,560 --> 00:31:14,750
wanted to get it with the SDK and so I

00:31:04,940 --> 00:31:17,810
just kind of moved to that instead on

00:31:14,750 --> 00:31:19,550
this removing engineering from between

00:31:17,810 --> 00:31:22,490
data science the scientists and the

00:31:19,550 --> 00:31:24,970
cluster topic so GPUs are quite scarce

00:31:22,490 --> 00:31:27,830
usually right so can we define quotas

00:31:24,970 --> 00:31:31,940
for the particular data scientist so

00:31:27,830 --> 00:31:34,160
GPUs like how many hours of say k80 so

00:31:31,940 --> 00:31:37,190
big hundreds they can consume yeah you

00:31:34,160 --> 00:31:38,540
definitely can in may sews I don't think

00:31:37,190 --> 00:31:40,550
that supports been pushed up through

00:31:38,540 --> 00:31:42,800
DCOs yet but it's definitely something

00:31:40,550 --> 00:31:44,420
we're looking at because it's you know

00:31:42,800 --> 00:31:46,820
that is the model we definitely want to

00:31:44,420 --> 00:31:49,820
move to in the future right the we have

00:31:46,820 --> 00:31:51,350
some things in place to try and combat

00:31:49,820 --> 00:31:54,740
this scarce resource problem

00:31:51,350 --> 00:31:56,330
specifically with these with GPUs so you

00:31:54,740 --> 00:31:58,520
know one of these is that you have to be

00:31:56,330 --> 00:32:00,890
a framework aware of GPUs so that you

00:31:58,520 --> 00:32:03,560
can do GPU we're scheduling for any jobs

00:32:00,890 --> 00:32:05,180
that come in marathon happens to be one

00:32:03,560 --> 00:32:08,600
of these GPU aware schedulers which is

00:32:05,180 --> 00:32:10,280
why it can launch jobs on GPUs and we

00:32:08,600 --> 00:32:12,560
also built this tensor flow framework to

00:32:10,280 --> 00:32:16,070
be a GPU aware scheduler so that it can

00:32:12,560 --> 00:32:17,960
do that but any other random bearing

00:32:16,070 --> 00:32:19,520
work that you bring up probably can't

00:32:17,960 --> 00:32:21,260
schedule things on your GPU and

00:32:19,520 --> 00:32:23,690
so he it's not perfect but it at least

00:32:21,260 --> 00:32:26,180
isolates some jobs from randomly landing

00:32:23,690 --> 00:32:28,790
on those GPU machines and using up all

00:32:26,180 --> 00:32:30,440
the other resources and making those

00:32:28,790 --> 00:32:32,210
GPUs basically just idle because you

00:32:30,440 --> 00:32:36,460
can't get any other work to to land

00:32:32,210 --> 00:32:39,230
there right okay any other questions

00:32:36,460 --> 00:32:42,410
Kevin I had a quick question um there

00:32:39,230 --> 00:32:44,690
was a couple of terms that you

00:32:42,410 --> 00:32:47,750
introduced early on in the in the slides

00:32:44,690 --> 00:32:50,270
that maybe for the for the folks like me

00:32:47,750 --> 00:32:52,070
you're not too familiar with the machine

00:32:50,270 --> 00:32:54,350
learning world and maybe you could

00:32:52,070 --> 00:32:57,050
quickly explain what those are and the

00:32:54,350 --> 00:33:00,530
first was compute kernels and the second

00:32:57,050 --> 00:33:04,340
one was TP use mm-hmm so compute kernels

00:33:00,530 --> 00:33:05,750
are I'll probably not do a great job of

00:33:04,340 --> 00:33:07,280
this because I'm a systems person more

00:33:05,750 --> 00:33:10,130
than a more than a machine learning

00:33:07,280 --> 00:33:12,140
person but compute kernels as far as I

00:33:10,130 --> 00:33:15,590
understand our standard ways of

00:33:12,140 --> 00:33:18,710
analyzing typically matrices and doing

00:33:15,590 --> 00:33:23,720
you know computations on matrices in

00:33:18,710 --> 00:33:25,100
order to you know produce some results

00:33:23,720 --> 00:33:26,510
from that and so you know there's

00:33:25,100 --> 00:33:28,640
there's a lots of different types of

00:33:26,510 --> 00:33:30,320
compute kernels that are very standard

00:33:28,640 --> 00:33:32,360
and that you can integrate into your

00:33:30,320 --> 00:33:34,880
machine learning algorithm or any other

00:33:32,360 --> 00:33:36,110
thing that you're working on and yeah

00:33:34,880 --> 00:33:38,450
that's all I was I was trying to get I

00:33:36,110 --> 00:33:39,500
was at tensorflow has implementations of

00:33:38,450 --> 00:33:41,570
all these things so that you can

00:33:39,500 --> 00:33:45,920
leverage them from your from your code

00:33:41,570 --> 00:33:47,900
yep yeah and TPU so CPU is central

00:33:45,920 --> 00:33:49,790
processing unit GPU is graphical

00:33:47,900 --> 00:33:51,620
processing unit and I don't know what

00:33:49,790 --> 00:33:53,930
the T is but it says something

00:33:51,620 --> 00:33:56,000
processing unit and it's a piece of

00:33:53,930 --> 00:33:57,680
hardware that only exists on Google

00:33:56,000 --> 00:34:01,250
infrastructure so they built this up and

00:33:57,680 --> 00:34:03,470
it's kind of a more heavy duty GPU I

00:34:01,250 --> 00:34:05,330
guess is when we designed specifically

00:34:03,470 --> 00:34:07,490
for running tensorflow

00:34:05,330 --> 00:34:09,649
type applications yeah maybe it stands

00:34:07,490 --> 00:34:14,929
for tensor processing unit does it okay

00:34:09,649 --> 00:34:16,909
yeah okay so the comment was that it's a

00:34:14,929 --> 00:34:17,419
collection of GPUs basically awesome

00:34:16,909 --> 00:34:23,780
thank you

00:34:17,419 --> 00:34:26,450
yeah any other questions so these jobs

00:34:23,780 --> 00:34:29,780
that you showed us seems to be that it's

00:34:26,450 --> 00:34:30,500
it's mainly batch oriented correct

00:34:29,780 --> 00:34:31,840
mm-hmm

00:34:30,500 --> 00:34:36,130
so

00:34:31,840 --> 00:34:38,260
um maybe this is not time to take up

00:34:36,130 --> 00:34:39,910
this specific issue but I have a very

00:34:38,260 --> 00:34:42,520
specific issue where I have a lot of

00:34:39,910 --> 00:34:44,920
vectors that I need to compare with

00:34:42,520 --> 00:34:49,330
other vectors and I would like to do

00:34:44,920 --> 00:34:51,970
this as part of my a web front-end would

00:34:49,330 --> 00:34:54,490
it be possible to set up and use tensor

00:34:51,970 --> 00:34:57,160
flow as a back-end for requests in real

00:34:54,490 --> 00:34:58,390
time yeah I'm not probably I'm probably

00:34:57,160 --> 00:34:59,770
not the right person to answer that

00:34:58,390 --> 00:35:01,300
question because I'm actually not a

00:34:59,770 --> 00:35:02,500
tensor flow expert I know how the the

00:35:01,300 --> 00:35:03,880
backend works to get these things

00:35:02,500 --> 00:35:06,040
running but I don't know the details

00:35:03,880 --> 00:35:10,470
about what the best usage of tensor flow

00:35:06,040 --> 00:35:10,470
itself would be unfortunately all right

00:35:10,890 --> 00:35:18,210
any other questions one in the back

00:35:24,349 --> 00:35:29,479
while the links you mentioned that it's

00:35:26,779 --> 00:35:32,059
currently closed source so is there a

00:35:29,479 --> 00:35:34,460
date or something that will be open

00:35:32,059 --> 00:35:37,519
available there's no date yet

00:35:34,460 --> 00:35:39,739
but there we are in talks about making

00:35:37,519 --> 00:35:41,869
it open source there hasn't been a final

00:35:39,739 --> 00:35:43,700
decision about when or even if it will

00:35:41,869 --> 00:35:50,269
be but the hope is that it will be

00:35:43,700 --> 00:35:58,369
fairly soon yep okay all right any other

00:35:50,269 --> 00:36:02,430
questions all right thank you very much

00:35:58,369 --> 00:36:05,550
and I hope you enjoyed the talk Thanks

00:36:02,430 --> 00:36:05,550

YouTube URL: https://www.youtube.com/watch?v=vsyU9R3y7VA


