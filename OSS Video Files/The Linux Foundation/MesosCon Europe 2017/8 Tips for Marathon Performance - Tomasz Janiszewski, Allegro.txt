Title: 8 Tips for Marathon Performance - Tomasz Janiszewski, Allegro
Publication date: 2017-10-27
Playlist: MesosCon Europe 2017
Description: 
	8 Tips for Marathon Performance - Tomasz Janiszewski, Allegro

Mesosphere Marathon is a Mesos scheduler that can handle huge production installation. In this presentation, I will present 8 tips that improves Marathon performance and prevents outages. Each tip will have summary with information when and how it can decrease performance and how to avoid the danger of the outage.

About Tomasz Janiszewski
Tomasz is a software engineer passionate about distributed systems. He believes in free and open source philosophy and occasionally contributes to projects on GitHub. At Allegro he works as a Software Engineer working with Mesos and Marathon cluster.
Captions: 
	00:00:00,000 --> 00:00:07,230
okay cool so we'll be starting and hi my

00:00:05,609 --> 00:00:10,950
name is stomach and today I'll talk you

00:00:07,230 --> 00:00:12,929
about tips for Martin performance so I'm

00:00:10,950 --> 00:00:15,150
working catalog which is one of the

00:00:12,929 --> 00:00:17,180
biggest e-commerce solution in Central

00:00:15,150 --> 00:00:21,470
Europe and we are located in Poland and

00:00:17,180 --> 00:00:26,010
we started using marathon in fall 2013

00:00:21,470 --> 00:00:30,210
14 and this actually is our my first

00:00:26,010 --> 00:00:32,040
pull request to marathon when we started

00:00:30,210 --> 00:00:34,860
using it on a production with the

00:00:32,040 --> 00:00:36,750
production load so with the couple of

00:00:34,860 --> 00:00:38,940
years we've got some insights on a

00:00:36,750 --> 00:00:41,170
performance issue and I want to share it

00:00:38,940 --> 00:00:42,559
with you

00:00:41,170 --> 00:00:46,350
[Music]

00:00:42,559 --> 00:00:48,750
Allegro is built on a top of weak

00:00:46,350 --> 00:00:50,460
monolid and also we've got switch it to

00:00:48,750 --> 00:00:53,399
micro services currently on our

00:00:50,460 --> 00:00:55,710
production we have about 500 micro

00:00:53,399 --> 00:00:58,859
services and depending on the

00:00:55,710 --> 00:01:02,670
environment we've got on to 3,000 tasks

00:00:58,859 --> 00:01:04,680
running so first thing when we talk

00:01:02,670 --> 00:01:07,380
about performance we should talk about

00:01:04,680 --> 00:01:09,290
matrix without matrix you are blind and

00:01:07,380 --> 00:01:13,140
we don't know what happened in our

00:01:09,290 --> 00:01:15,840
cluster or in our product so first thing

00:01:13,140 --> 00:01:17,670
is to enable matrix in Martin they're

00:01:15,840 --> 00:01:19,890
not enabled by default because we

00:01:17,670 --> 00:01:23,130
marathon but know where you want to

00:01:19,890 --> 00:01:26,189
store that matrix matrix appears in

00:01:23,130 --> 00:01:28,799
marathon in every version of point 14

00:01:26,189 --> 00:01:32,040
and from the beginning it started to

00:01:28,799 --> 00:01:34,710
support all major players in a market

00:01:32,040 --> 00:01:37,079
this means they were supporting in

00:01:34,710 --> 00:01:40,409
graphite Delta doc study and there's

00:01:37,079 --> 00:01:44,159
also poor to support Pro materials there

00:01:40,409 --> 00:01:46,979
is a github repo with a primitives

00:01:44,159 --> 00:01:49,740
adapter cost as an exposed matrix in a

00:01:46,979 --> 00:01:55,890
JSON way and primitives needs some

00:01:49,740 --> 00:01:58,740
different schema to read the matrix so

00:01:55,890 --> 00:02:01,219
to enable matrix we need to add line

00:01:58,740 --> 00:02:05,280
something like this to our configuration

00:02:01,219 --> 00:02:08,789
by default matrix are gathered by every

00:02:05,280 --> 00:02:11,459
ten seconds what does it mean that on a

00:02:08,789 --> 00:02:13,530
big installation matrix gathering could

00:02:11,459 --> 00:02:16,459
take about 20%

00:02:13,530 --> 00:02:22,650
of your cpu time this is a flame graph

00:02:16,459 --> 00:02:24,840
that was gathered with profiling tor for

00:02:22,650 --> 00:02:27,810
Java and if you are not familiar with

00:02:24,840 --> 00:02:30,060
flame graphs basically here on the

00:02:27,810 --> 00:02:33,270
bottom there is 100 percent of your cpu

00:02:30,060 --> 00:02:35,760
time that application in music and the

00:02:33,270 --> 00:02:38,550
stack on the flame graph is like a

00:02:35,760 --> 00:02:41,100
function call and have each call is

00:02:38,550 --> 00:02:44,040
stuck on the another so here's a whole

00:02:41,100 --> 00:02:46,830
stack trace of matrix gathering because

00:02:44,040 --> 00:02:50,100
marathon is written in Scala the flame

00:02:46,830 --> 00:02:52,500
graph does not looks like a flame it's

00:02:50,100 --> 00:02:56,580
more like a firewall with many small

00:02:52,500 --> 00:03:00,540
firebrick so how we can fix this and

00:02:56,580 --> 00:03:05,310
this problem with the CPU that is taking

00:03:00,540 --> 00:03:07,890
to that we need to decrease the or

00:03:05,310 --> 00:03:11,430
increase and metrics gathering interval

00:03:07,890 --> 00:03:14,060
so after increasing heat to 55 seconds

00:03:11,430 --> 00:03:14,060
which is

00:03:22,230 --> 00:03:31,500
okay I hope that will be better so after

00:03:29,459 --> 00:03:33,450
switching heat to 55 seconds which is

00:03:31,500 --> 00:03:35,700
more sensible in our infrastructure

00:03:33,450 --> 00:03:37,080
because our metrics resolution in

00:03:35,700 --> 00:03:39,630
metrics store that we are using

00:03:37,080 --> 00:03:42,569
currently it's graphite is about one

00:03:39,630 --> 00:03:46,110
minute we reduced the metrics gathering

00:03:42,569 --> 00:03:49,319
time to two percent of the CPU so it is

00:03:46,110 --> 00:03:52,440
a big deal to have a sensible

00:03:49,319 --> 00:03:56,040
configuration and do not rely on default

00:03:52,440 --> 00:03:58,230
because they might not work for you if

00:03:56,040 --> 00:04:00,630
you are interested this part this small

00:03:58,230 --> 00:04:03,540
block is a message native library you

00:04:00,630 --> 00:04:05,099
should also monitor that because when

00:04:03,540 --> 00:04:07,019
there are some problems with lead my

00:04:05,099 --> 00:04:09,720
sauce which is a night binding for my

00:04:07,019 --> 00:04:12,750
sauce used by marathon you could see

00:04:09,720 --> 00:04:17,090
here the increased CPU time or some

00:04:12,750 --> 00:04:20,370
memory leaks so you should monitor that

00:04:17,090 --> 00:04:24,210
marathon at the beginning use use the

00:04:20,370 --> 00:04:26,400
code hi a cut rope wizard matrix but in

00:04:24,210 --> 00:04:29,789
one point for the switch to come on

00:04:26,400 --> 00:04:31,740
which is another matrix solution for

00:04:29,789 --> 00:04:34,260
Scala that is designed for applications

00:04:31,740 --> 00:04:38,639
that are written with actors and more

00:04:34,260 --> 00:04:39,960
reactive way why it's important because

00:04:38,639 --> 00:04:41,910
the rule of thumb is that you should

00:04:39,960 --> 00:04:45,720
know dependencies of your system

00:04:41,910 --> 00:04:47,280
so in our case once they we changed the

00:04:45,720 --> 00:04:51,750
configuration of load balancer behind

00:04:47,280 --> 00:04:54,210
before our graphite solution and it

00:04:51,750 --> 00:04:58,830
turns out that we lost every second

00:04:54,210 --> 00:05:01,560
matrix and that was B because our the

00:04:58,830 --> 00:05:03,840
drop Wizard version used in marathon has

00:05:01,560 --> 00:05:06,150
a back and we need to that was fixed in

00:05:03,840 --> 00:05:07,610
the next release even minor ease and we

00:05:06,150 --> 00:05:14,070
need to update it and deploy and

00:05:07,610 --> 00:05:19,789
everything works I'm sorry my company

00:05:14,070 --> 00:05:22,919
wants to me so to be virus free okay

00:05:19,789 --> 00:05:25,919
next thing as I said marathon is written

00:05:22,919 --> 00:05:28,200
in Java so you should in the rule of

00:05:25,919 --> 00:05:30,659
thumb paste to monitor your dependencies

00:05:28,200 --> 00:05:31,289
and the Java is a big deal in terms of

00:05:30,659 --> 00:05:32,610
performance

00:05:31,289 --> 00:05:35,500
there are many talks about Java

00:05:32,610 --> 00:05:38,770
performance and I will only show you the

00:05:35,500 --> 00:05:42,580
tip like a slide steep how to fix the

00:05:38,770 --> 00:05:44,410
jvm issues first mink in JVM when you

00:05:42,580 --> 00:05:48,400
heard about Java you should think about

00:05:44,410 --> 00:05:50,860
garbage collection and to monitor

00:05:48,400 --> 00:05:56,080
garbage collection in marathon you need

00:05:50,860 --> 00:06:00,040
to add this piece of code and that will

00:05:56,080 --> 00:06:02,170
create the GC log file that contains

00:06:00,040 --> 00:06:05,200
information about what happened in a

00:06:02,170 --> 00:06:07,650
garbage in garbage collection when it

00:06:05,200 --> 00:06:11,080
happened what it what the garbage

00:06:07,650 --> 00:06:14,260
collector does and how much memory was

00:06:11,080 --> 00:06:19,060
moved freed and everything that happened

00:06:14,260 --> 00:06:23,230
during GC time once you got the GC lock

00:06:19,060 --> 00:06:25,300
you should parse it or if you have some

00:06:23,230 --> 00:06:28,150
experience maybe you can see what

00:06:25,300 --> 00:06:30,310
happened just looking and a text version

00:06:28,150 --> 00:06:34,620
of it by I recommend using some tools

00:06:30,310 --> 00:06:37,390
like Sansom and sans long will print you

00:06:34,620 --> 00:06:40,570
pretty nice information what happened

00:06:37,390 --> 00:06:43,870
and what should be done in order to get

00:06:40,570 --> 00:06:47,080
better performance so like here this is

00:06:43,870 --> 00:06:51,940
an example from our marathon instance

00:06:47,080 --> 00:06:53,830
that I take in a summer and we got an

00:06:51,940 --> 00:06:56,140
information that there is problem with

00:06:53,830 --> 00:06:58,240
too small heap and premature promotion

00:06:56,140 --> 00:06:59,350
so the objects were created on a heap

00:06:58,240 --> 00:07:03,790
once again

00:06:59,350 --> 00:07:06,100
yep question 10 zoom yeah

00:07:03,790 --> 00:07:12,880
I will provide the slice and there is

00:07:06,100 --> 00:07:15,160
clickable link in here so so there is

00:07:12,880 --> 00:07:17,470
hip to small indicator so that mean you

00:07:15,160 --> 00:07:21,330
may increase the heap or change some

00:07:17,470 --> 00:07:26,050
configuration about promoting the

00:07:21,330 --> 00:07:27,940
objects in on a heap so like GC is big

00:07:26,050 --> 00:07:31,890
topic there are dedicated conferences

00:07:27,940 --> 00:07:34,060
for it so I will leave it here and

00:07:31,890 --> 00:07:36,130
everything depends when we are talking

00:07:34,060 --> 00:07:37,890
in a heap it depends on your usage and

00:07:36,130 --> 00:07:42,010
in depends on the hardware that you can

00:07:37,890 --> 00:07:46,140
provide here are the visual illustration

00:07:42,010 --> 00:07:47,289
of the garbage collection times from our

00:07:46,140 --> 00:07:48,790
servers

00:07:47,289 --> 00:07:52,600
so as you

00:07:48,790 --> 00:07:55,300
see this is pretty big time wasted on a

00:07:52,600 --> 00:07:57,100
garbage collection and it is yang to see

00:07:55,300 --> 00:08:00,400
this means this could be stopped the

00:07:57,100 --> 00:08:04,200
world situation and yank garbage

00:08:00,400 --> 00:08:09,220
collection means that the objects from a

00:08:04,200 --> 00:08:12,880
yang space in a heap are moved to old

00:08:09,220 --> 00:08:14,830
space and then it's generally not good

00:08:12,880 --> 00:08:16,870
thing if you move object from young

00:08:14,830 --> 00:08:23,160
space to old space and then clean up

00:08:16,870 --> 00:08:25,780
them in a full GC from from old space

00:08:23,160 --> 00:08:31,300
next week when we are talking about Java

00:08:25,780 --> 00:08:33,460
is akka akka is a actor model for scala

00:08:31,300 --> 00:08:36,250
and i think it could be used with java

00:08:33,460 --> 00:08:40,060
too and that is strongly influenced by

00:08:36,250 --> 00:08:43,000
our lunk the the inspiration is so great

00:08:40,060 --> 00:08:46,810
so when you read the akka documentation

00:08:43,000 --> 00:08:49,780
you often get note about that oh this is

00:08:46,810 --> 00:08:56,070
copied from airlock so when we talk

00:08:49,780 --> 00:08:58,450
about akka and there is a couple of

00:08:56,070 --> 00:09:02,110
config change a conflict that could be

00:08:58,450 --> 00:09:05,200
changed to get some different behavior

00:09:02,110 --> 00:09:08,290
for example there is a true pod that is

00:09:05,200 --> 00:09:11,140
a number of messages that could be read

00:09:08,290 --> 00:09:14,800
by an actor and process in one batch by

00:09:11,140 --> 00:09:17,590
default it's four four five and if we

00:09:14,800 --> 00:09:22,060
set it to 1 this means that each actor

00:09:17,590 --> 00:09:25,600
will be a spawn take one message process

00:09:22,060 --> 00:09:30,490
it and will be released and next actor

00:09:25,600 --> 00:09:33,400
could proceed so to change this value

00:09:30,490 --> 00:09:36,700
you need to add this configuration line

00:09:33,400 --> 00:09:40,270
to your Java code in our production we

00:09:36,700 --> 00:09:45,510
switch it to increase it four times to

00:09:40,270 --> 00:09:48,460
get twenty and it was just manual

00:09:45,510 --> 00:09:52,390
detection that we see oh it looks like

00:09:48,460 --> 00:09:54,390
it performed better but it really

00:09:52,390 --> 00:09:58,660
depends on your use case we are heavily

00:09:54,390 --> 00:10:00,970
using the events and that might be our

00:09:58,660 --> 00:10:01,840
issue was because the events was not

00:10:00,970 --> 00:10:03,220
dispatched

00:10:01,840 --> 00:10:06,340
in a proper time because the budgets

00:10:03,220 --> 00:10:08,890
were worse too small so you can do it

00:10:06,340 --> 00:10:14,040
but check with the metrics if that

00:10:08,890 --> 00:10:17,220
really helped first tip is the zookeeper

00:10:14,040 --> 00:10:21,010
zookeeper is a state store for marathon

00:10:17,220 --> 00:10:27,190
and this is strong consistency key value

00:10:21,010 --> 00:10:29,860
database so zookeeper is great store but

00:10:27,190 --> 00:10:34,770
it has a problem when we try it when you

00:10:29,860 --> 00:10:37,000
try to store too big element so

00:10:34,770 --> 00:10:41,200
sometimes in marathon you could get

00:10:37,000 --> 00:10:44,110
error like this this means that when you

00:10:41,200 --> 00:10:46,240
want to deploy a really important app to

00:10:44,110 --> 00:10:49,060
a production there is a problem with

00:10:46,240 --> 00:10:52,630
storing an object because in Marathon

00:10:49,060 --> 00:10:54,610
Puri 21.4 marathon used to store the

00:10:52,630 --> 00:10:58,000
whole state of the an application in one

00:10:54,610 --> 00:11:02,160
element this means if you have hundreds

00:10:58,000 --> 00:11:05,580
of applications the note start in a

00:11:02,160 --> 00:11:09,130
zookeeper will have the whole state and

00:11:05,580 --> 00:11:12,400
when you create a deploy deploy element

00:11:09,130 --> 00:11:16,000
was generated by having previous and

00:11:12,400 --> 00:11:19,240
then you configuration so this means if

00:11:16,000 --> 00:11:21,580
your whole state takes half of the

00:11:19,240 --> 00:11:24,730
megabytes the deploy element will take

00:11:21,580 --> 00:11:28,960
about one megabyte so this is pretty big

00:11:24,730 --> 00:11:31,210
it was fixed in a 1.4 1.4 change the

00:11:28,960 --> 00:11:33,760
completely changed the layout of the

00:11:31,210 --> 00:11:37,840
zookeeper data so this should not happen

00:11:33,760 --> 00:11:42,340
but if you get that error in releases

00:11:37,840 --> 00:11:43,720
priority 1.4 you need to do what is

00:11:42,340 --> 00:11:47,830
advice here

00:11:43,720 --> 00:11:52,300
so increased maximum load size in our

00:11:47,830 --> 00:11:56,610
production we have double it and what's

00:11:52,300 --> 00:12:00,160
the problem because the changing the

00:11:56,610 --> 00:12:03,340
martin configuration is not enough you

00:12:00,160 --> 00:12:05,920
should also change the configuration of

00:12:03,340 --> 00:12:09,460
the zookeeper because of it if you take

00:12:05,920 --> 00:12:12,430
a look at a zookeeper documentation and

00:12:09,460 --> 00:12:13,830
the max buffer which is a buffer for

00:12:12,430 --> 00:12:17,310
incoming

00:12:13,830 --> 00:12:21,180
data must be set on all servers and

00:12:17,310 --> 00:12:23,010
client and second thing is that you

00:12:21,180 --> 00:12:25,320
should keep in mind that the zookeeper

00:12:23,010 --> 00:12:27,390
is designed to store data on the order

00:12:25,320 --> 00:12:29,700
of kilobytes in size so if you are

00:12:27,390 --> 00:12:31,440
changing this element you are probably

00:12:29,700 --> 00:12:35,910
have something wrong in your

00:12:31,440 --> 00:12:37,650
infrastructure we did this and what

00:12:35,910 --> 00:12:40,320
happens to us

00:12:37,650 --> 00:12:42,090
was that the zookeeper rights take more

00:12:40,320 --> 00:12:44,760
and more time because when the zookeeper

00:12:42,090 --> 00:12:47,280
neaten negotiate the state the bigger

00:12:44,760 --> 00:12:49,290
elements takes longer to generate the

00:12:47,280 --> 00:12:52,140
checksum and stuff like this and also

00:12:49,290 --> 00:12:55,680
they need to be passed it through the

00:12:52,140 --> 00:13:00,990
network so those smaller elements are in

00:12:55,680 --> 00:13:04,080
generally better yeah as I said Martin

00:13:00,990 --> 00:13:07,470
starts a group only with references so

00:13:04,080 --> 00:13:09,450
this looks like the better solution but

00:13:07,470 --> 00:13:11,850
the problem with storing on the air

00:13:09,450 --> 00:13:17,460
references in a key value store is that

00:13:11,850 --> 00:13:20,400
you are creating transactions in no

00:13:17,460 --> 00:13:23,340
sequel database what does it mean when

00:13:20,400 --> 00:13:25,650
there is some problem with the zookeeper

00:13:23,340 --> 00:13:27,930
for example one node could not be

00:13:25,650 --> 00:13:30,960
written to the zookeeper you've got an

00:13:27,930 --> 00:13:33,080
outage of the whole Moroccan plaster but

00:13:30,960 --> 00:13:39,180
that warrior this also happen in a

00:13:33,080 --> 00:13:40,710
1-point feet 12 but less often first

00:13:39,180 --> 00:13:43,890
thing in when we are talking about

00:13:40,710 --> 00:13:46,710
zookeeper is latency so this is pretty

00:13:43,890 --> 00:13:49,560
easy if the nodes are closer to each

00:13:46,710 --> 00:13:53,010
other and the network latency is smaller

00:13:49,560 --> 00:13:56,700
they perform faster and the data and the

00:13:53,010 --> 00:13:59,310
transaction happens quicker so you

00:13:56,700 --> 00:14:01,950
should keep the zookeeper nodes as close

00:13:59,310 --> 00:14:03,630
to each other as possible remembering

00:14:01,950 --> 00:14:06,450
that they should be probably in a

00:14:03,630 --> 00:14:10,320
different availability zones just in

00:14:06,450 --> 00:14:12,330
case one zone goes off and they could be

00:14:10,320 --> 00:14:15,000
they should be stored on a different

00:14:12,330 --> 00:14:18,060
machines than the marathon and messes

00:14:15,000 --> 00:14:21,030
because in otherwise there will be small

00:14:18,060 --> 00:14:24,660
latency between the marathon master and

00:14:21,030 --> 00:14:26,100
the zookeeper master that when they are

00:14:24,660 --> 00:14:26,690
on the same machines but they will

00:14:26,100 --> 00:14:31,580
compete

00:14:26,690 --> 00:14:38,030
resources that because there were on one

00:14:31,580 --> 00:14:41,960
machine for tip is to update to at least

00:14:38,030 --> 00:14:44,980
one point 3.14 and here's a little star

00:14:41,960 --> 00:14:50,510
I will describe what I mean later

00:14:44,980 --> 00:14:52,490
first thing threads merit encode has to

00:14:50,510 --> 00:14:54,110
place when they are configuring the

00:14:52,490 --> 00:14:58,030
thread pools that are using by marathon

00:14:54,110 --> 00:15:01,490
his first that configure Scala

00:14:58,030 --> 00:15:04,400
concurrent context so the actors that

00:15:01,490 --> 00:15:08,120
are used by akka so here you can see

00:15:04,400 --> 00:15:12,380
they're only maximum value is 64 threads

00:15:08,120 --> 00:15:13,700
for the whole actor pool to use the

00:15:12,380 --> 00:15:20,540
second thing is the number of threads

00:15:13,700 --> 00:15:26,800
for for i/o operation and it's set to

00:15:20,540 --> 00:15:29,390
100 so a little bit of math 64 plus 100

00:15:26,800 --> 00:15:31,490
means that marathon could spawn up to

00:15:29,390 --> 00:15:37,430
2,000 threads

00:15:31,490 --> 00:15:40,370
why does it happen in fact is a big ol

00:15:37,430 --> 00:15:42,680
notation from the number of tasks why

00:15:40,370 --> 00:15:45,290
does it happen because when you have an

00:15:42,680 --> 00:15:48,230
actor and you call a blocking quickest

00:15:45,290 --> 00:15:50,840
in that actor acha will spawned a new

00:15:48,230 --> 00:15:53,360
trait for you and it cannot be

00:15:50,840 --> 00:15:55,970
configured to behave differently because

00:15:53,360 --> 00:16:01,190
that's how the actor model is designed

00:15:55,970 --> 00:16:06,200
to not block at any time I advise you to

00:16:01,190 --> 00:16:10,900
update to one problem 3.13 with a star

00:16:06,200 --> 00:16:13,640
because there is a patch and that is

00:16:10,900 --> 00:16:15,680
it's not that much I think the one point

00:16:13,640 --> 00:16:18,380
three point seven was patched so when

00:16:15,680 --> 00:16:21,860
you deploy it you will get this is the

00:16:18,380 --> 00:16:25,070
before we deploy one point three point

00:16:21,860 --> 00:16:27,560
seven and he is after so we still get

00:16:25,070 --> 00:16:32,540
them more threads then configured

00:16:27,560 --> 00:16:34,610
because in my point is nearly 200 tries

00:16:32,540 --> 00:16:38,140
but we are not killing cover machines

00:16:34,610 --> 00:16:38,140
with thousands of threads

00:16:39,380 --> 00:16:46,140
next thing there is something called no

00:16:43,530 --> 00:16:48,440
knob optimization so the fastest coat

00:16:46,140 --> 00:16:52,230
you can possibly write is no code at all

00:16:48,440 --> 00:16:53,970
and we can rephrase it so the nope

00:16:52,230 --> 00:16:55,080
optimization is the fastest operation

00:16:53,970 --> 00:16:57,810
you can possibly perform it's no

00:16:55,080 --> 00:17:00,060
oppression at all so what does it mean

00:16:57,810 --> 00:17:01,680
there are hashtags health checks in

00:17:00,060 --> 00:17:04,530
Marathon were introduced from every

00:17:01,680 --> 00:17:06,240
beginning and in the beginning we were

00:17:04,530 --> 00:17:09,060
performed by a single marathon instance

00:17:06,240 --> 00:17:11,640
and it was working for us until sometime

00:17:09,060 --> 00:17:14,550
why because the single Martinus toss was

00:17:11,640 --> 00:17:17,640
querying for every task it has to detect

00:17:14,550 --> 00:17:20,069
if it's healthy like this or unhealthy

00:17:17,640 --> 00:17:23,520
and store this value and present them to

00:17:20,069 --> 00:17:27,510
a user in a nice UI but what happens

00:17:23,520 --> 00:17:28,950
when we scale there is no way to perform

00:17:27,510 --> 00:17:34,200
health checks from a single instance

00:17:28,950 --> 00:17:38,720
without taking timeouts or getting some

00:17:34,200 --> 00:17:42,000
other issues when I said that we should

00:17:38,720 --> 00:17:44,310
not perform operation at all this means

00:17:42,000 --> 00:17:49,530
that we can move the operation we want

00:17:44,310 --> 00:17:51,780
to do to some other so in this case we

00:17:49,530 --> 00:17:54,630
could ask masses to perform health

00:17:51,780 --> 00:17:58,590
checks for marathon the tasks are not

00:17:54,630 --> 00:18:01,800
running in a void they are grouped by a

00:17:58,590 --> 00:18:04,290
message agent and every task is running

00:18:01,800 --> 00:18:07,230
by an executor that could perform health

00:18:04,290 --> 00:18:10,410
checks for us so well executors perform

00:18:07,230 --> 00:18:12,510
a health checks it detective the task is

00:18:10,410 --> 00:18:15,720
healthy and informed marathon about this

00:18:12,510 --> 00:18:18,140
so the marathon get notification only

00:18:15,720 --> 00:18:21,450
when the health check state change and

00:18:18,140 --> 00:18:24,020
task is killed not by Martin but by the

00:18:21,450 --> 00:18:26,400
half-tracks there will be props in a

00:18:24,020 --> 00:18:30,090
problem I think the message one point

00:18:26,400 --> 00:18:31,740
for there's currently not support by

00:18:30,090 --> 00:18:39,780
marathon but I think they would be

00:18:31,740 --> 00:18:44,310
supported in 1.5 1.6 so if you read the

00:18:39,780 --> 00:18:47,490
Gaston Kleeman blog post he did a test

00:18:44,310 --> 00:18:50,360
so it turns out that single marathon

00:18:47,490 --> 00:18:53,660
instance he didn't say

00:18:50,360 --> 00:18:57,170
instance his pound could handle up to

00:18:53,660 --> 00:19:02,180
nearly 4,000 tasks with TCP health

00:18:57,170 --> 00:19:05,480
checks and up to 2,000 tasks in HTTP

00:19:02,180 --> 00:19:08,720
objects and we were hidden by this we've

00:19:05,480 --> 00:19:11,690
got over 2,000 tasks on our production

00:19:08,720 --> 00:19:14,860
cluster and tasks we get randomly killed

00:19:11,690 --> 00:19:22,640
by Martin because the holodecks time out

00:19:14,860 --> 00:19:25,850
so what we did we just ported marathon

00:19:22,640 --> 00:19:28,760
health checks from 1.4 to 1.3 which we

00:19:25,850 --> 00:19:32,690
are using and switched every task 2

00:19:28,760 --> 00:19:35,780
meses health checks and this solved the

00:19:32,690 --> 00:19:39,350
problem with the tasks killed even if

00:19:35,780 --> 00:19:44,870
they are healthy and also helped us with

00:19:39,350 --> 00:19:49,250
the marathon issues here's a comparison

00:19:44,870 --> 00:19:51,260
because we've got a Martin 1.3 point 10

00:19:49,250 --> 00:19:55,040
and we've got a problem with health

00:19:51,260 --> 00:19:56,480
checks so we see that 1.4 introduces

00:19:55,040 --> 00:19:58,790
messes health checks

00:19:56,480 --> 00:20:02,840
versus native health checks but after

00:19:58,790 --> 00:20:06,530
upgrading it was not working for us so

00:20:02,840 --> 00:20:08,840
the there was constantly timeouts people

00:20:06,530 --> 00:20:12,350
cannot deploy anything and then we

00:20:08,840 --> 00:20:13,790
decide to port message health checks

00:20:12,350 --> 00:20:17,480
from one point four to one point three

00:20:13,790 --> 00:20:20,570
point ten and everything get more stable

00:20:17,480 --> 00:20:26,420
even than before and the health check

00:20:20,570 --> 00:20:30,260
starts passing again what you see here

00:20:26,420 --> 00:20:32,930
is a problem that was in that started in

00:20:30,260 --> 00:20:35,000
a marathon one point four there was an

00:20:32,930 --> 00:20:39,280
issue for this and finally it gets

00:20:35,000 --> 00:20:42,410
resolved in one point four point seven

00:20:39,280 --> 00:20:44,690
actually we haven't still updated yet

00:20:42,410 --> 00:20:51,950
because we searched it on a different

00:20:44,690 --> 00:20:55,130
topics I will cover later five tip is to

00:20:51,950 --> 00:20:57,470
do not use an even bus for marathon

00:20:55,130 --> 00:21:01,690
Martin provides two ways of getting

00:20:57,470 --> 00:21:03,490
notifications from Martin

00:21:01,690 --> 00:21:05,650
so there you can subscribe for a

00:21:03,490 --> 00:21:10,660
callback so Martin will send you a post

00:21:05,650 --> 00:21:13,450
with an event in JSON and this is the

00:21:10,660 --> 00:21:16,920
deprecated format but the problem with

00:21:13,450 --> 00:21:20,470
this was that it sends every event and

00:21:16,920 --> 00:21:23,530
even that some events are pretty big

00:21:20,470 --> 00:21:25,390
like over 10 megabytes it is deprecated

00:21:23,530 --> 00:21:28,090
and there is some weird policy of

00:21:25,390 --> 00:21:32,380
retrying and it was totally resource

00:21:28,090 --> 00:21:35,070
consuming so I don't you should not use

00:21:32,380 --> 00:21:39,730
the call box instead you should use

00:21:35,070 --> 00:21:42,580
server sent events which is working in a

00:21:39,730 --> 00:21:45,100
quite opposite way so martin is no

00:21:42,580 --> 00:21:47,290
longer sending and events to a client

00:21:45,100 --> 00:21:49,300
but you are connecting to a marathon

00:21:47,290 --> 00:21:51,490
with a persistent connection and martin

00:21:49,300 --> 00:21:53,040
is sending a data on one connection to

00:21:51,490 --> 00:21:56,320
one client

00:21:53,040 --> 00:21:58,480
what's more martin support even

00:21:56,320 --> 00:22:00,880
filtering from one point three point

00:21:58,480 --> 00:22:03,010
seven and this means that you can filter

00:22:00,880 --> 00:22:07,860
out the big events of your application

00:22:03,010 --> 00:22:07,860
do not to filter them on on their side

00:22:08,230 --> 00:22:15,430
these are the metrics from our solution

00:22:12,280 --> 00:22:17,560
to register tasks in from marathon into

00:22:15,430 --> 00:22:21,180
a console called marathon console it's

00:22:17,560 --> 00:22:25,120
fork of a cisco cloud martin console and

00:22:21,180 --> 00:22:29,650
we used to have a configuration that

00:22:25,120 --> 00:22:33,250
relies on callbacks and with callbacks

00:22:29,650 --> 00:22:36,310
we've got the seven minutes of delay

00:22:33,250 --> 00:22:39,790
between a time stamp that was source in

00:22:36,310 --> 00:22:41,650
the event and the time stamp and the

00:22:39,790 --> 00:22:44,560
time on a machine when the event was

00:22:41,650 --> 00:22:46,690
received so in our case this means like

00:22:44,560 --> 00:22:49,450
a huge outage because the application

00:22:46,690 --> 00:22:53,470
will respond in seven minutes and killed

00:22:49,450 --> 00:22:57,460
and the for the seven minutes

00:22:53,470 --> 00:23:00,490
our stating our config configure service

00:22:57,460 --> 00:23:00,910
discovery solution console was totally

00:23:00,490 --> 00:23:04,270
invalid

00:23:00,910 --> 00:23:10,420
so after switch to service an event we

00:23:04,270 --> 00:23:13,499
filtering we reduced our delay to less

00:23:10,420 --> 00:23:17,070
than forty seconds

00:23:13,499 --> 00:23:19,619
about milliseconds but still this means

00:23:17,070 --> 00:23:21,840
that sometimes we have invalid state in

00:23:19,619 --> 00:23:23,940
a configuration service and this

00:23:21,840 --> 00:23:28,730
probably is not the best way of

00:23:23,940 --> 00:23:31,379
registering in a distributed world

00:23:28,730 --> 00:23:34,739
another thing I mentioned before that

00:23:31,379 --> 00:23:37,470
some events has are pretty big like over

00:23:34,739 --> 00:23:40,049
ten megabytes usually there are the

00:23:37,470 --> 00:23:42,509
deployments event and there is a pull

00:23:40,049 --> 00:23:45,869
request I think it's yeah it's from

00:23:42,509 --> 00:23:49,230
today so it's still open that will that

00:23:45,869 --> 00:23:52,200
will use introduced lightweight SSA

00:23:49,230 --> 00:23:55,679
flags so the events will be smaller

00:23:52,200 --> 00:23:58,200
without unnecessary information about

00:23:55,679 --> 00:24:03,359
what have been deployed or stuff like

00:23:58,200 --> 00:24:06,440
this six tip is using a custom executor

00:24:03,359 --> 00:24:09,330
ah I will talk about it more tomorrow

00:24:06,440 --> 00:24:12,029
where I think it's important because

00:24:09,330 --> 00:24:15,869
when you see that we've got a five

00:24:12,029 --> 00:24:18,859
milliseconds delay between event that

00:24:15,869 --> 00:24:21,690
the application is created and actually

00:24:18,859 --> 00:24:23,909
registering it in a console this means

00:24:21,690 --> 00:24:29,220
that the application is created or

00:24:23,909 --> 00:24:33,090
killed this means and with executor we

00:24:29,220 --> 00:24:35,460
can register our application in a

00:24:33,090 --> 00:24:38,879
configuration service coverage solution

00:24:35,460 --> 00:24:41,249
one it start be healthy for example

00:24:38,879 --> 00:24:45,869
because executor is an actual thing that

00:24:41,249 --> 00:24:48,480
is taking care of a whole application or

00:24:45,869 --> 00:24:53,879
instance life cycle and this is the way

00:24:48,480 --> 00:24:56,399
how the Aurora works for example seven

00:24:53,879 --> 00:25:00,299
tip is to prefer budget deployments

00:24:56,399 --> 00:25:06,480
instead of menacing was when you create

00:25:00,299 --> 00:25:10,799
a budget deployment this means that you

00:25:06,480 --> 00:25:12,330
each of your users create a request to a

00:25:10,799 --> 00:25:14,820
marathon and deploy one application

00:25:12,330 --> 00:25:17,730
Martin don't like many deployments at

00:25:14,820 --> 00:25:19,259
once because it means every deployment

00:25:17,730 --> 00:25:23,129
needs to be stored in the zookeeper and

00:25:19,259 --> 00:25:25,289
this increases the zookeeper usage so we

00:25:23,129 --> 00:25:26,070
can use a facade that will gather the

00:25:25,289 --> 00:25:28,740
deployments into

00:25:26,070 --> 00:25:31,080
grrrrrr ones and deploy them in a batch

00:25:28,740 --> 00:25:34,040
this is something that we haven't

00:25:31,080 --> 00:25:36,390
introduced but we were considering

00:25:34,040 --> 00:25:38,760
instead of this we start starting our

00:25:36,390 --> 00:25:42,060
modern installation that's why the

00:25:38,760 --> 00:25:45,090
everything I showed you does not

00:25:42,060 --> 00:25:46,880
I don't do not test the latest martin

00:25:45,090 --> 00:25:49,800
version because right now we have

00:25:46,880 --> 00:25:53,570
shorted marathon so the load on each

00:25:49,800 --> 00:25:57,510
instance is lower than we have before

00:25:53,570 --> 00:26:02,340
and how it looks so we just got a

00:25:57,510 --> 00:26:04,040
phosphate before marathon and in that

00:26:02,340 --> 00:26:06,630
Fassett we are taking care of

00:26:04,040 --> 00:26:10,440
authentication and authorization users

00:26:06,630 --> 00:26:12,090
to deploy something to marathon so in

00:26:10,440 --> 00:26:14,550
fact this is the better solutions and

00:26:12,090 --> 00:26:17,790
plugins because plugins work in a single

00:26:14,550 --> 00:26:18,990
trial and need to check for every for

00:26:17,790 --> 00:26:21,210
example a new user want to query

00:26:18,990 --> 00:26:23,880
marathon to see all instances the

00:26:21,210 --> 00:26:29,250
plug-in need to ask if if that

00:26:23,880 --> 00:26:31,800
particular user Heaven is authorized to

00:26:29,250 --> 00:26:33,510
see that application so with thousands

00:26:31,800 --> 00:26:37,770
of applications this start to be a

00:26:33,510 --> 00:26:40,890
problem and when there is a sharding we

00:26:37,770 --> 00:26:42,180
just keep the simple facade and without

00:26:40,890 --> 00:26:44,610
user noticing it

00:26:42,180 --> 00:26:46,590
we are moving its instances between

00:26:44,610 --> 00:26:48,870
different marathon installation using

00:26:46,590 --> 00:26:54,590
consistent hashing cover application

00:26:48,870 --> 00:26:57,180
name so that will be all here's a

00:26:54,590 --> 00:26:59,670
summary of what we should do so just

00:26:57,180 --> 00:27:02,490
enable metrics to see what happened to

00:26:59,670 --> 00:27:05,340
JVM and remember to tune JVM also for a

00:27:02,490 --> 00:27:07,770
zookeeper using the same tools like you

00:27:05,340 --> 00:27:10,370
use for marathon remember that the

00:27:07,770 --> 00:27:12,240
zookeeper list has pretty different

00:27:10,370 --> 00:27:15,830
responsibilities and works in a

00:27:12,240 --> 00:27:21,780
different way so they need a special car

00:27:15,830 --> 00:27:24,420
of the update marathon to 1.14 and use

00:27:21,780 --> 00:27:26,640
message health checks if you can so the

00:27:24,420 --> 00:27:30,030
best you can do right now is to use

00:27:26,640 --> 00:27:33,510
martin 1.4.7

00:27:30,030 --> 00:27:36,780
or 1.5 but i haven't used it and can't

00:27:33,510 --> 00:27:39,330
really recommend it do not use the even

00:27:36,780 --> 00:27:43,860
pass instead of user exactly

00:27:39,330 --> 00:27:45,450
that will take care of application

00:27:43,860 --> 00:27:47,880
lifecycle mmm

00:27:45,450 --> 00:27:50,940
prefer batching when talking to marathon

00:27:47,880 --> 00:27:53,850
and and chart it if you need it if you

00:27:50,940 --> 00:27:56,940
take a hit the wall hit the roof and

00:27:53,850 --> 00:28:00,810
Martin is no longer working for you so

00:27:56,940 --> 00:28:03,060
think about charging so that's all and

00:28:00,810 --> 00:28:05,990
if you have any question we have to do

00:28:03,060 --> 00:28:05,990
also

00:28:23,950 --> 00:28:31,090
yep so we are not using no longer using

00:28:27,980 --> 00:28:33,350
marathon UI we are using it for the back

00:28:31,090 --> 00:28:35,240
proposal Martin us and no longer

00:28:33,350 --> 00:28:39,290
supported by mesosphere and there are no

00:28:35,240 --> 00:28:41,510
comets on their site and there was some

00:28:39,290 --> 00:28:44,480
movement to update the Mart anywhere but

00:28:41,510 --> 00:28:47,720
the initiative died so we have our

00:28:44,480 --> 00:28:52,070
custom solution here where it's called

00:28:47,720 --> 00:28:55,070
App Engine and when the user can see is

00:28:52,070 --> 00:29:00,190
his all application they state databases

00:28:55,070 --> 00:29:04,550
that he's using configured load balancer

00:29:00,190 --> 00:29:06,650
caches and stuff like this so yep the

00:29:04,550 --> 00:29:10,760
problem is that in our case this

00:29:06,650 --> 00:29:12,650
phosphate also take paper and API so the

00:29:10,760 --> 00:29:15,850
UI is pretty simple we just ask the

00:29:12,650 --> 00:29:18,850
faucet and all magic and routing is done

00:29:15,850 --> 00:29:18,850
here

00:29:24,840 --> 00:29:29,370
okay so I think there are no more

00:29:26,759 --> 00:29:31,620
questions so thank you if you have any

00:29:29,370 --> 00:29:34,830
more code if you get any question I will

00:29:31,620 --> 00:29:38,410
be here today and tomorrow and even on

00:29:34,830 --> 00:29:43,749
the town hall so thank you

00:29:38,410 --> 00:29:43,749

YouTube URL: https://www.youtube.com/watch?v=GUFNvYe76yk


