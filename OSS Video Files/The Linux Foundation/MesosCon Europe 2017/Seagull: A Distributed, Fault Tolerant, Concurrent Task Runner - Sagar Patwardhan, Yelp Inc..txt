Title: Seagull: A Distributed, Fault Tolerant, Concurrent Task Runner - Sagar Patwardhan, Yelp Inc.
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Seagull: A Distributed, Fault Tolerant, Concurrent Task Runner - Sagar Patwardhan, Yelp Inc.

At Yelp, we all strive to increase developer productivity by decreasing the time to test, deploy, and monitor changes. To enable developers to push code safely, we run more than 20 million tests every day. Yelp has a monolith web application which has 100,000 tests; running them sequentially takes approximately 2 days to finish. We built an in-house distributed system called Seagull, which splits these 100,000 tests into smaller chunks(bundles) using our bespoke algorithm and runs these chunks in parallel on the compute cluster to finish all the tests in less than 10 mins. Seagull uses Apache Mesos to schedule these run test bundles on AWS spotfleet. We have written a custom autoscaler for AWS spotfleet which dynamically adjusts the cluster capacity based on different utilization metrics for optimal use of resources. Seagull runs more than 2 million ephemeral docker containers every day.

About Sagar Patwardhan
My name is Sagar Patwardhan and I am a software engineer at Yelp Inc. I am part of the distributed systems(Infrastructure) team. I have been with Yelp for a little over two years; I am currently working on building mesos infrastructure and next-gen batch processing infrastructure at Yelp. My previous projects include an asynchronous processing system called kew, which is used for processing all of Yelp's emails, push notifications and online transactions in real time. I also worked on our distributed, fault tolerant, concurrent task executor called seagull, which is used for running millions of tests every day in a cost effective manner. I got my M.S. in computer science(specialized in distributed systems and networks) from USC. I was part of the networked system lab where I did research in TCP. I got my B.E. in computers from University of Pune, India.
Captions: 
	00:00:00,410 --> 00:00:05,250
everyone welcome to this talk my name is

00:00:03,360 --> 00:00:07,950
Sagar Patwardhan I'm a software engineer

00:00:05,250 --> 00:00:10,080
at Yale I'm part of the distributed

00:00:07,950 --> 00:00:12,900
systems team and I've been working at

00:00:10,080 --> 00:00:14,570
Yale for last two and a half years so

00:00:12,900 --> 00:00:16,650
today I'm going to talk about Siegel

00:00:14,570 --> 00:00:20,279
distributed fault tolerant concurrent

00:00:16,650 --> 00:00:23,939
task runner that we built to mainly run

00:00:20,279 --> 00:00:25,980
our tests in parallel so let's talk a

00:00:23,939 --> 00:00:28,320
little bit about Yelp so Yelp is a

00:00:25,980 --> 00:00:30,660
mobile app and a website that connects

00:00:28,320 --> 00:00:32,460
people with great local businesses so

00:00:30,660 --> 00:00:34,710
it's basically a search engine where you

00:00:32,460 --> 00:00:38,340
can find whatever you're looking for a

00:00:34,710 --> 00:00:40,649
restaurant to dentist a doctor maybe to

00:00:38,340 --> 00:00:43,920
give you a sense of how big we are we

00:00:40,649 --> 00:00:46,289
have 28 million mobile web users these

00:00:43,920 --> 00:00:49,110
are the monthly active unique visitors

00:00:46,289 --> 00:00:52,649
that we get we have 74 million mobile

00:00:49,110 --> 00:00:57,180
web users and 83 million desktop web

00:00:52,649 --> 00:00:58,739
users so we're pretty big so in today's

00:00:57,180 --> 00:01:01,730
talk what I mean gonna what I'm going to

00:00:58,739 --> 00:01:03,840
cover is what is Siegel why we built it

00:01:01,730 --> 00:01:06,240
I'm also going to give a comprehensive

00:01:03,840 --> 00:01:08,460
overview of signal how it works so deep

00:01:06,240 --> 00:01:10,170
dive into Siegel I'm going to talk a

00:01:08,460 --> 00:01:13,320
little bit about the cluster order

00:01:10,170 --> 00:01:15,229
scaler that we built at Yelp next thing

00:01:13,320 --> 00:01:17,939
will be challenges and lessons learned

00:01:15,229 --> 00:01:21,509
and last thing will be a future of

00:01:17,939 --> 00:01:24,509
Siegel so let's get started

00:01:21,509 --> 00:01:28,409
to give you a little overview of testing

00:01:24,509 --> 00:01:30,630
at Yelp we have a hundred thousand tests

00:01:28,409 --> 00:01:34,439
that we need to run in order for our

00:01:30,630 --> 00:01:37,170
developers to push go to prod we have a

00:01:34,439 --> 00:01:38,810
monolithic application which has a lot

00:01:37,170 --> 00:01:42,180
of tests and we also have

00:01:38,810 --> 00:01:46,350
service-oriented architecture and over

00:01:42,180 --> 00:01:48,390
150 services so if you execute these

00:01:46,350 --> 00:01:52,380
tests serially it will take you two days

00:01:48,390 --> 00:01:55,200
to complete all these tests so we can't

00:01:52,380 --> 00:01:59,000
really waste that much time on testing

00:01:55,200 --> 00:02:01,469
we have north of 500 developers and

00:01:59,000 --> 00:02:03,750
testing kind of has a direct impact on

00:02:01,469 --> 00:02:05,280
developer productivity the more time you

00:02:03,750 --> 00:02:09,979
spend in testing the less productive our

00:02:05,280 --> 00:02:12,120
developers are so Siegel to the rescue

00:02:09,979 --> 00:02:13,990
so as I mentioned before it's a

00:02:12,120 --> 00:02:17,230
distributed fault-tolerant system

00:02:13,990 --> 00:02:19,800
veterans tasks in parallel so to give

00:02:17,230 --> 00:02:22,660
you a sense of how big Segal is we

00:02:19,800 --> 00:02:24,640
usually have 350 Segal runs every day an

00:02:22,660 --> 00:02:26,470
average run time for each Segal run

00:02:24,640 --> 00:02:28,270
varies between 10 to 15 minutes

00:02:26,470 --> 00:02:31,000
depending on what kind of tests we are

00:02:28,270 --> 00:02:32,680
running we spin out 2.5 million

00:02:31,000 --> 00:02:34,840
ephemeral docker containers just for the

00:02:32,680 --> 00:02:37,110
testing use case these containers are

00:02:34,840 --> 00:02:39,220
basically different services at Yelp

00:02:37,110 --> 00:02:42,700
developers want to test their service

00:02:39,220 --> 00:02:45,280
against different services so we spin up

00:02:42,700 --> 00:02:47,020
a lot of containers we use cluster

00:02:45,280 --> 00:02:50,440
autoscaler so our cluster is always

00:02:47,020 --> 00:02:52,060
varying in size so at night when its

00:02:50,440 --> 00:02:55,600
load on our cluster is minimum we have

00:02:52,060 --> 00:02:57,880
70 instances and a scale our cluster

00:02:55,600 --> 00:03:00,390
goes up to 450 instances during peak

00:02:57,880 --> 00:03:04,380
time which is in the afternoon us time

00:03:00,390 --> 00:03:06,489
we have started using spot instances so

00:03:04,380 --> 00:03:10,030
initially we started off with our eyes

00:03:06,489 --> 00:03:12,880
and spot instances now we are just using

00:03:10,030 --> 00:03:16,600
spot instances every day a seagull

00:03:12,880 --> 00:03:19,239
executes 25 million tests it's basically

00:03:16,600 --> 00:03:23,560
to ensure that a developer's pushing

00:03:19,239 --> 00:03:25,390
code that is okay to push the product so

00:03:23,560 --> 00:03:27,310
let's talk about applications of seagull

00:03:25,390 --> 00:03:28,900
before diving into seagull itself so

00:03:27,310 --> 00:03:31,450
mainly it was written for the testing

00:03:28,900 --> 00:03:33,489
use case to run tests in parallel but we

00:03:31,450 --> 00:03:36,760
have extended it to our different use

00:03:33,489 --> 00:03:38,680
cases so we have a load testing

00:03:36,760 --> 00:03:41,890
framework called locust this framework

00:03:38,680 --> 00:03:44,049
is mainly used for testing different end

00:03:41,890 --> 00:03:46,030
points of services so let's say you are

00:03:44,049 --> 00:03:47,890
a service owner and you want to say you

00:03:46,030 --> 00:03:49,269
want to see how many requests per second

00:03:47,890 --> 00:03:51,400
you can sustain for a certain end point

00:03:49,269 --> 00:03:53,709
of your service you would use this

00:03:51,400 --> 00:03:56,049
framework and make a parallel calls to

00:03:53,709 --> 00:03:59,799
your service and see if it can sustain

00:03:56,049 --> 00:04:02,079
the sustain to load next use cases photo

00:03:59,799 --> 00:04:05,290
classification so Yelp has tens with

00:04:02,079 --> 00:04:07,390
tens of millions of high-quality photos

00:04:05,290 --> 00:04:09,040
and we use deep learning to figure out

00:04:07,390 --> 00:04:11,140
whether a photo was taken inside a

00:04:09,040 --> 00:04:12,700
restaurant that it was taken outside a

00:04:11,140 --> 00:04:15,430
restaurant whether it contains food

00:04:12,700 --> 00:04:19,040
whether it contains drink etc so we use

00:04:15,430 --> 00:04:21,620
we train the model and then we

00:04:19,040 --> 00:04:24,410
you basically use that model to classify

00:04:21,620 --> 00:04:26,780
photos and Segel so initially we used to

00:04:24,410 --> 00:04:28,970
run this one spark but now we have

00:04:26,780 --> 00:04:33,320
market migrated it to run on Segel and

00:04:28,970 --> 00:04:36,980
we run classifiers on close to ninety

00:04:33,320 --> 00:04:39,800
million photos in less than a day so

00:04:36,980 --> 00:04:41,780
let's talk about Siegel so I'm gonna

00:04:39,800 --> 00:04:43,490
walk you through our testing workflow

00:04:41,780 --> 00:04:46,670
and then I'll go into specifics about

00:04:43,490 --> 00:04:50,060
each each section so let's say you are a

00:04:46,670 --> 00:04:51,770
developer at Yelp and you have created

00:04:50,060 --> 00:04:54,110
your branch you have committed your code

00:04:51,770 --> 00:04:55,670
and you wanna run tests the first thing

00:04:54,110 --> 00:04:57,440
you will do is you will use a command

00:04:55,670 --> 00:04:59,090
line tool called run Segal

00:04:57,440 --> 00:05:01,040
you specify which branch you want to

00:04:59,090 --> 00:05:04,910
test and which test Suites you want to

00:05:01,040 --> 00:05:07,220
run after starting a single run we start

00:05:04,910 --> 00:05:08,900
a build on Jenkins we do that mainly

00:05:07,220 --> 00:05:10,370
because there are certain things that

00:05:08,900 --> 00:05:13,310
need to happen before we can start

00:05:10,370 --> 00:05:15,470
executing tests on your branch so one of

00:05:13,310 --> 00:05:18,560
the first things is we create an

00:05:15,470 --> 00:05:21,380
artifact so we pull your build pull

00:05:18,560 --> 00:05:23,810
developers get branch and then basically

00:05:21,380 --> 00:05:26,240
create a tarball do a bunch of things

00:05:23,810 --> 00:05:30,020
and upload it to s3 this is later used

00:05:26,240 --> 00:05:31,430
by executives to run tests the next

00:05:30,020 --> 00:05:33,350
thing we do is we need to figure out how

00:05:31,430 --> 00:05:35,840
many tests are there in your branch so

00:05:33,350 --> 00:05:38,260
we run a job called test discovery this

00:05:35,840 --> 00:05:40,340
job goes ahead and basically calculate

00:05:38,260 --> 00:05:45,050
basically figures out how many tests are

00:05:40,340 --> 00:05:47,480
there in your branch so next thing that

00:05:45,050 --> 00:05:49,160
we do is as I mentioned before we run

00:05:47,480 --> 00:05:50,900
tests in parallel so we need to split

00:05:49,160 --> 00:05:53,600
these tests into smaller chunks or

00:05:50,900 --> 00:05:56,260
bundles as we call them so we run a

00:05:53,600 --> 00:05:58,790
program called a bundle creator which

00:05:56,260 --> 00:06:01,910
divides these tests into a multiple

00:05:58,790 --> 00:06:05,090
bundles after doing that we start The

00:06:01,910 --> 00:06:07,670
Seagull meso scheduler which talks to

00:06:05,090 --> 00:06:09,170
maces master and it basically gets

00:06:07,670 --> 00:06:12,730
offers from different agents in our

00:06:09,170 --> 00:06:18,550
cluster after getting all the offers we

00:06:12,730 --> 00:06:21,680
start sending executors to the agents

00:06:18,550 --> 00:06:23,450
first thing agents do is they download

00:06:21,680 --> 00:06:27,860
the artifact that we uploaded in

00:06:23,450 --> 00:06:31,250
previous stage and they enter the entire

00:06:27,860 --> 00:06:32,690
file they compile pyc files and start

00:06:31,250 --> 00:06:34,910
executing tests

00:06:32,690 --> 00:06:36,860
after all the tests are done the results

00:06:34,910 --> 00:06:39,260
are reported to elasticsearch and Cabana

00:06:36,860 --> 00:06:40,940
which is our main data store for storing

00:06:39,260 --> 00:06:43,820
test results

00:06:40,940 --> 00:06:46,580
one thing that single executor does

00:06:43,820 --> 00:06:48,200
apart from reporting results is upload

00:06:46,580 --> 00:06:51,260
standard error standard out of each

00:06:48,200 --> 00:06:54,140
executor so this is mainly used for

00:06:51,260 --> 00:06:56,120
debugging so we direct sanded out

00:06:54,140 --> 00:06:58,730
standard error to a specific file called

00:06:56,120 --> 00:07:02,990
siegel log and we upload it to s3 at the

00:06:58,730 --> 00:07:05,660
end of each single run we have a we also

00:07:02,990 --> 00:07:08,320
report a real-time metrics to signal

00:07:05,660 --> 00:07:11,330
effects and DynamoDB from our scheduler

00:07:08,320 --> 00:07:13,640
we have a UI service called test results

00:07:11,330 --> 00:07:16,880
this is mainly used by developers to see

00:07:13,640 --> 00:07:19,720
results of their tests whether all the

00:07:16,880 --> 00:07:22,570
tests in their brands are passing or not

00:07:19,720 --> 00:07:25,910
and yeah that's like a complete overview

00:07:22,570 --> 00:07:28,670
of Siebel so this is specifically for

00:07:25,910 --> 00:07:30,950
the testing use case depending on the

00:07:28,670 --> 00:07:35,470
use case workflow will vary so now I

00:07:30,950 --> 00:07:35,470
will talk about specific sex-specific

00:07:35,530 --> 00:07:40,940
specifics of Segel so first a seagull

00:07:38,600 --> 00:07:42,740
meso scheduler so the scheduler itself

00:07:40,940 --> 00:07:45,290
is written in Python we are currently

00:07:42,740 --> 00:07:49,220
using Lib missiles to talk to mrs.

00:07:45,290 --> 00:07:52,790
you've not migrated to HTTP API yet we

00:07:49,220 --> 00:07:54,050
have one scheduler per test suite as I

00:07:52,790 --> 00:07:57,050
mentioned before there are different

00:07:54,050 --> 00:07:59,930
test Suites at Yelp for example unit

00:07:57,050 --> 00:08:05,530
tests acceptance tests integration tests

00:07:59,930 --> 00:08:08,000
selenium tests etc so each test suite is

00:08:05,530 --> 00:08:10,790
basically corresponds to a masive

00:08:08,000 --> 00:08:13,540
scheduler so at peak time we have more

00:08:10,790 --> 00:08:16,700
than 50 schedulers running so we rely on

00:08:13,540 --> 00:08:19,190
missus SDRF algorithm to distribute

00:08:16,700 --> 00:08:20,000
offers to us and so far it has worked

00:08:19,190 --> 00:08:21,800
really well

00:08:20,000 --> 00:08:23,750
we have never run into an issue where a

00:08:21,800 --> 00:08:26,800
scheduler is starving for offers or a

00:08:23,750 --> 00:08:29,419
schedule that is getting a lot of offers

00:08:26,800 --> 00:08:31,850
Siegel also has customizable concurrency

00:08:29,419 --> 00:08:35,360
so because it is supposed to run patches

00:08:31,850 --> 00:08:37,130
you can either run your patches really

00:08:35,360 --> 00:08:39,260
quickly by setting the concurrency to a

00:08:37,130 --> 00:08:41,510
very high number or you can reduce the

00:08:39,260 --> 00:08:42,740
concurrency and let your branch let your

00:08:41,510 --> 00:08:45,380
batch run for

00:08:42,740 --> 00:08:47,090
really long time signal scheduler is

00:08:45,380 --> 00:08:48,590
also fault-tolerant so it can deal with

00:08:47,090 --> 00:08:50,450
days and failures it can deal with

00:08:48,590 --> 00:08:53,330
bundle failures

00:08:50,450 --> 00:08:56,410
it basically retries bundles and I will

00:08:53,330 --> 00:08:59,780
talk more about the retraining strategy

00:08:56,410 --> 00:09:01,790
let's talk about placement strategies so

00:08:59,780 --> 00:09:04,730
basically the aim here is to optimize

00:09:01,790 --> 00:09:06,410
for Segal bundle setup time as I

00:09:04,730 --> 00:09:09,440
mentioned before each executor has to

00:09:06,410 --> 00:09:13,250
download tar ball from s3 the tar ball

00:09:09,440 --> 00:09:15,920
is typically a few jeebies and we also

00:09:13,250 --> 00:09:17,870
have to do some things before running

00:09:15,920 --> 00:09:22,280
tests like compiled Python files

00:09:17,870 --> 00:09:24,680
etcetera so to optimize for sequel

00:09:22,280 --> 00:09:28,280
bundled setup time we use two strategies

00:09:24,680 --> 00:09:31,250
one is affinity for slaves so we if you

00:09:28,280 --> 00:09:33,050
have used any particular maysa agent

00:09:31,250 --> 00:09:35,210
before we will try to use the agent

00:09:33,050 --> 00:09:36,560
again so that the executor tarball is

00:09:35,210 --> 00:09:38,540
already available and ready to use for

00:09:36,560 --> 00:09:41,450
the new executor that is getting

00:09:38,540 --> 00:09:44,720
scheduled and the other thing is we use

00:09:41,450 --> 00:09:47,000
as many offers as many resources in an

00:09:44,720 --> 00:09:50,570
offer as possible so if you get an offer

00:09:47,000 --> 00:09:53,210
with they say 30 CPUs 200 gigs of memory

00:09:50,570 --> 00:09:55,550
we try to fit as many single executors

00:09:53,210 --> 00:09:58,160
into that offer as possible Siegel is

00:09:55,550 --> 00:10:01,460
supposed to be a batch runner so it's

00:09:58,160 --> 00:10:03,110
really time flexible there's no upper

00:10:01,460 --> 00:10:05,990
bound on when a single run should finish

00:10:03,110 --> 00:10:09,920
it's okay for single run gets delayed by

00:10:05,990 --> 00:10:11,810
a minute if agent goes down and we lose

00:10:09,920 --> 00:10:14,630
all the executors on that agent we'll

00:10:11,810 --> 00:10:17,900
reschedule them and they'll finish in a

00:10:14,630 --> 00:10:20,510
few minutes this also simplifies scaled

00:10:17,900 --> 00:10:22,160
down because we are using up all the

00:10:20,510 --> 00:10:24,290
resources that are offered to us we are

00:10:22,160 --> 00:10:25,670
always left with a few idle agents in

00:10:24,290 --> 00:10:28,370
our cluster which can be terminated

00:10:25,670 --> 00:10:34,070
without causing any addressed disruption

00:10:28,370 --> 00:10:36,800
in our cluster so this UI here is what

00:10:34,070 --> 00:10:39,920
we call bundle visualizer so y-axis is

00:10:36,800 --> 00:10:42,620
the time scale and x-axis was basically

00:10:39,920 --> 00:10:44,630
bundles so the green bundles are the

00:10:42,620 --> 00:10:46,130
ones that have finished successfully the

00:10:44,630 --> 00:10:48,800
red ones are the ones that have failed

00:10:46,130 --> 00:10:51,740
so let's say a bundle fails what we do

00:10:48,800 --> 00:10:53,840
is we split the bundle into two bundles

00:10:51,740 --> 00:10:55,740
of equal size so let's say four bundles

00:10:53,840 --> 00:10:57,120
of 10 minutes worth of tests

00:10:55,740 --> 00:10:59,340
then we'll split it into two bundles

00:10:57,120 --> 00:11:00,930
each of five minutes this was to

00:10:59,340 --> 00:11:02,970
basically make sure that we are

00:11:00,930 --> 00:11:06,270
finishing within a reasonably reasonable

00:11:02,970 --> 00:11:07,830
time we don't wanna schedule the bundle

00:11:06,270 --> 00:11:14,670
again and spend ten minutes on that

00:11:07,830 --> 00:11:17,700
bundle Seigle executor so see we have

00:11:14,670 --> 00:11:19,470
written a custom executor in Python this

00:11:17,700 --> 00:11:20,910
executor is currently using missus

00:11:19,470 --> 00:11:24,030
container Iser and different c group

00:11:20,910 --> 00:11:25,740
Isolators the main job of this executor

00:11:24,030 --> 00:11:27,540
is to do setup and teardown

00:11:25,740 --> 00:11:29,540
so setup as I mentioned before is

00:11:27,540 --> 00:11:32,900
basically getting the tarball and

00:11:29,540 --> 00:11:35,370
teardown is reporting a bunch of stats

00:11:32,900 --> 00:11:38,730
apart from setup teardown it also

00:11:35,370 --> 00:11:40,680
reports utilization of our process so we

00:11:38,730 --> 00:11:43,080
run a thread in that executor which is

00:11:40,680 --> 00:11:47,040
constantly getting CPU metrics memory

00:11:43,080 --> 00:11:50,310
metrics Ned how many network connections

00:11:47,040 --> 00:11:55,040
were using disk usage etc we use that to

00:11:50,310 --> 00:11:58,410
allocate resources to our missus bundle

00:11:55,040 --> 00:12:02,940
it also uploads log log files to s3 and

00:11:58,410 --> 00:12:05,400
reports test results we also have

00:12:02,940 --> 00:12:07,590
special constraints to run these tests

00:12:05,400 --> 00:12:09,840
we have some resources that we call them

00:12:07,590 --> 00:12:12,530
cluster wide resources these resources

00:12:09,840 --> 00:12:14,820
are not tied to a particular agent so

00:12:12,530 --> 00:12:16,620
the typical example of these resources

00:12:14,820 --> 00:12:19,790
is selenium connections and database

00:12:16,620 --> 00:12:24,090
connections if you want to allocate them

00:12:19,790 --> 00:12:26,490
without without tying them to specific

00:12:24,090 --> 00:12:29,730
agents so to achieve this we use a

00:12:26,490 --> 00:12:32,640
zookeeper ephemeral cenotes so there are

00:12:29,730 --> 00:12:34,260
two z nodes in our zookeeper cluster one

00:12:32,640 --> 00:12:36,180
keeps track of how many connections you

00:12:34,260 --> 00:12:38,750
can use and the other acts as a parent

00:12:36,180 --> 00:12:42,300
node for ephemeral child Z nodes

00:12:38,750 --> 00:12:43,620
children Z nodes so basically let's say

00:12:42,300 --> 00:12:45,570
you are an executor and you want to use

00:12:43,620 --> 00:12:46,950
a selenium connection you'll first check

00:12:45,570 --> 00:12:49,410
what is the limit on number of

00:12:46,950 --> 00:12:52,050
connections let's say for selenium then

00:12:49,410 --> 00:12:54,780
you will query get children on the

00:12:52,050 --> 00:12:57,180
parents you node and see how many FML Z

00:12:54,780 --> 00:12:59,310
notes already exist if it is less than

00:12:57,180 --> 00:13:01,910
the limit then you will create a Z in

00:12:59,310 --> 00:13:04,950
order and use the selenium connection

00:13:01,910 --> 00:13:06,840
same goes for database connections we

00:13:04,950 --> 00:13:08,940
use a zookeeper locks to make sure that

00:13:06,840 --> 00:13:11,280
you know our access is atomic

00:13:08,940 --> 00:13:15,660
we are not running into any race

00:13:11,280 --> 00:13:16,680
conditions as you may know once the

00:13:15,660 --> 00:13:19,200
executor goes away

00:13:16,680 --> 00:13:21,270
these ephemeral zookeeper connections

00:13:19,200 --> 00:13:27,420
will be removed and will basically

00:13:21,270 --> 00:13:29,970
reclaim the resource at that point let's

00:13:27,420 --> 00:13:31,290
talk about monitoring and alerting so we

00:13:29,970 --> 00:13:34,050
have real-time monitoring and alerting

00:13:31,290 --> 00:13:35,910
using signal effects so we send time

00:13:34,050 --> 00:13:39,060
series data to signal effects and they

00:13:35,910 --> 00:13:41,010
visualize the data for us and they also

00:13:39,060 --> 00:13:43,920
provide alerting so the graph that you

00:13:41,010 --> 00:13:46,170
see here is representing the number of

00:13:43,920 --> 00:13:49,650
bundles that have failed in last one day

00:13:46,170 --> 00:13:52,650
so green bars are corresponding to

00:13:49,650 --> 00:13:55,440
finished bundles a red bars are the

00:13:52,650 --> 00:13:57,780
bundles that have failed blue bars are

00:13:55,440 --> 00:14:01,440
the bundles killed and yellow bars are

00:13:57,780 --> 00:14:04,320
the money lost for each bundle in Segal

00:14:01,440 --> 00:14:07,100
we specify run time so for example for a

00:14:04,320 --> 00:14:09,750
test use case we specify the run time

00:14:07,100 --> 00:14:11,370
equal to 30 minutes so if a bundle goes

00:14:09,750 --> 00:14:12,870
over 30 minutes we go ahead and kill the

00:14:11,370 --> 00:14:17,640
bundle assuming there's something wrong

00:14:12,870 --> 00:14:19,320
with the bundle we also do log

00:14:17,640 --> 00:14:21,270
aggregation in Splunk as I mentioned

00:14:19,320 --> 00:14:24,120
before we upload s3i standard error and

00:14:21,270 --> 00:14:26,250
standard out of each executors to s3 we

00:14:24,120 --> 00:14:29,850
then ingest all this data into Splunk

00:14:26,250 --> 00:14:32,250
and we basically query Splunk to get

00:14:29,850 --> 00:14:35,190
statistics across multiple Seigle runs

00:14:32,250 --> 00:14:36,660
and across the cluster so in this

00:14:35,190 --> 00:14:40,320
example I'm searching for Yelp

00:14:36,660 --> 00:14:42,540
serializable object validation error so

00:14:40,320 --> 00:14:45,390
it's basically showing us when it

00:14:42,540 --> 00:14:48,840
happened in last four hours this has

00:14:45,390 --> 00:14:50,640
been really helpful in debugging cluster

00:14:48,840 --> 00:14:54,110
wide issues so we have better

00:14:50,640 --> 00:14:54,110
instrumentation of what's going on

00:14:56,950 --> 00:15:04,270
so as I mentioned before we split tasks

00:15:00,430 --> 00:15:06,910
our tests into multiple chunks so to do

00:15:04,270 --> 00:15:08,709
so we use two different algorithms first

00:15:06,910 --> 00:15:11,980
is a greedy algorithm which is basically

00:15:08,709 --> 00:15:13,839
a bin packing algorithm we keep track of

00:15:11,980 --> 00:15:16,060
test timings so whenever a test is

00:15:13,839 --> 00:15:17,890
executed we store how long it took to

00:15:16,060 --> 00:15:20,560
execute and we upload it to

00:15:17,890 --> 00:15:23,620
elasticsearch we then run a nightly cron

00:15:20,560 --> 00:15:26,290
job which calculates p94 last one week

00:15:23,620 --> 00:15:29,500
for each individual test and it uploads

00:15:26,290 --> 00:15:31,660
it to DynamoDB and this data is

00:15:29,500 --> 00:15:34,480
basically used for bundling so when we

00:15:31,660 --> 00:15:38,020
are creating chunks we sort the test

00:15:34,480 --> 00:15:41,430
list test list and we basically split

00:15:38,020 --> 00:15:43,750
the tests into bundles of ten minutes

00:15:41,430 --> 00:15:47,070
this algorithm has worked really well

00:15:43,750 --> 00:15:49,630
for us we have also tried different

00:15:47,070 --> 00:15:51,190
linear programming algorithms but the

00:15:49,630 --> 00:15:56,740
gain that we get with those algorithms

00:15:51,190 --> 00:15:58,149
is not significant so in our selenium

00:15:56,740 --> 00:16:01,029
test suite there are a bunch of

00:15:58,149 --> 00:16:03,070
constraints for executing tests so some

00:16:01,029 --> 00:16:05,560
tests have to be executed together while

00:16:03,070 --> 00:16:08,709
some tests can not be executed together

00:16:05,560 --> 00:16:11,260
so to achieve that we have written a

00:16:08,709 --> 00:16:13,329
pulp LP solver so this is a really

00:16:11,260 --> 00:16:16,329
simple solver which has three main goals

00:16:13,329 --> 00:16:18,450
one to make sure that a single test is

00:16:16,329 --> 00:16:21,310
present in a single bundle bundle never

00:16:18,450 --> 00:16:23,709
goes about 10 minutes and number of

00:16:21,310 --> 00:16:29,020
bundles created is as minimum as

00:16:23,709 --> 00:16:31,899
possible so as I mentioned before we use

00:16:29,020 --> 00:16:34,660
an auto scaler for that cluster you must

00:16:31,899 --> 00:16:38,290
be wondering why it's a bad system why

00:16:34,660 --> 00:16:41,110
do you care so this is the weekly usage

00:16:38,290 --> 00:16:43,270
trend of our cluster so during weekends

00:16:41,110 --> 00:16:46,029
the cluster is at minimum scale on

00:16:43,270 --> 00:16:48,339
Mondays it scales up Tuesday and it's a

00:16:46,029 --> 00:16:50,800
Thursday and I it again the resource

00:16:48,339 --> 00:16:52,540
utilization goes down so all the dollars

00:16:50,800 --> 00:16:57,370
that you see here is the money that we

00:16:52,540 --> 00:17:00,160
save same goes for daily usage trend so

00:16:57,370 --> 00:17:02,709
we have developers in Europe which they

00:17:00,160 --> 00:17:03,150
basically pushed code in the morning us

00:17:02,709 --> 00:17:06,939
time

00:17:03,150 --> 00:17:09,459
3m etcetera and during us office hours

00:17:06,939 --> 00:17:12,010
times the cluster utilisation goes up

00:17:09,459 --> 00:17:14,350
you can also see a spike during lunch

00:17:12,010 --> 00:17:19,120
hour so the utilization goes down during

00:17:14,350 --> 00:17:21,610
that time so this is the overall

00:17:19,120 --> 00:17:24,040
architecture of our autoscaler fleet my

00:17:21,610 --> 00:17:27,880
sir so there are two components of fleet

00:17:24,040 --> 00:17:31,750
miser one basically gathers data from

00:17:27,880 --> 00:17:33,430
ASOS and other other sources and it

00:17:31,750 --> 00:17:37,660
stores the data into elasticsearch and

00:17:33,430 --> 00:17:39,040
DynamoDB we use a signal affection

00:17:37,660 --> 00:17:42,820
sensor for monitoring the autoscaler

00:17:39,040 --> 00:17:45,520
itself so it it emits useful data

00:17:42,820 --> 00:17:48,300
regarding when it took a particular

00:17:45,520 --> 00:17:50,620
decision what were the what were the

00:17:48,300 --> 00:17:53,470
what did the cluster look like and why

00:17:50,620 --> 00:17:57,640
it took a particular decision and it

00:17:53,470 --> 00:18:00,940
basically queries Amazon API to scale up

00:17:57,640 --> 00:18:03,460
or scale down the cluster so the auto

00:18:00,940 --> 00:18:04,900
scaling strategies that we use are we

00:18:03,460 --> 00:18:07,870
use two different strategies CPU

00:18:04,900 --> 00:18:10,870
utilization and signal runs in flight so

00:18:07,870 --> 00:18:13,300
CPU utilization our workload cpu-bound

00:18:10,870 --> 00:18:17,710
so we always run out of CPUs before

00:18:13,300 --> 00:18:21,720
memory so we track CPU utilization and

00:18:17,710 --> 00:18:25,000
if the CPU ttle ization goes above a 65%

00:18:21,720 --> 00:18:28,300
for last 15 minutes then we scale up the

00:18:25,000 --> 00:18:30,340
cluster and while scaling down we check

00:18:28,300 --> 00:18:33,790
if the cluster utilization is below 35%

00:18:30,340 --> 00:18:38,710
for 30 minutes if it is then we scale

00:18:33,790 --> 00:18:42,309
down the cluster this signal was added

00:18:38,710 --> 00:18:44,950
last year so we basically whenever a new

00:18:42,309 --> 00:18:48,700
seagull run is triggered or the scaler

00:18:44,950 --> 00:18:51,220
gets notified that a run is in flight so

00:18:48,700 --> 00:18:53,320
this has helped us a lot to give you an

00:18:51,220 --> 00:18:55,600
example let's say we are about to scale

00:18:53,320 --> 00:18:58,210
down our cluster has been idle for last

00:18:55,600 --> 00:19:01,450
30 minutes but all of a sudden

00:18:58,210 --> 00:19:02,440
developers in EU come along in send a

00:19:01,450 --> 00:19:04,929
bunch of seagull runs

00:19:02,440 --> 00:19:06,490
so our autoscaler anticipates how many

00:19:04,929 --> 00:19:08,980
single runs are in flight and how many

00:19:06,490 --> 00:19:11,340
resources they'll need and depending on

00:19:08,980 --> 00:19:16,740
that it either prevents the scale down

00:19:11,340 --> 00:19:16,740
or adds more resources if needed

00:19:18,810 --> 00:19:24,300
scaling down is difficult so we have

00:19:21,870 --> 00:19:30,300
implemented placement strategy is to

00:19:24,300 --> 00:19:32,460
allow smooth scaling down we also use aw

00:19:30,300 --> 00:19:34,770
spot fleet and one disadvantage of that

00:19:32,460 --> 00:19:35,970
is that we cannot specify which

00:19:34,770 --> 00:19:38,760
instances terminate

00:19:35,970 --> 00:19:40,830
so our autoscaler queries each

00:19:38,760 --> 00:19:43,740
individual missus agent and figures out

00:19:40,830 --> 00:19:45,210
how many tasks it is running and if a

00:19:43,740 --> 00:19:47,280
particular agent is not running any

00:19:45,210 --> 00:19:49,290
tasks then it will select that instance

00:19:47,280 --> 00:19:51,210
for termination and it will go ahead

00:19:49,290 --> 00:19:53,370
terminate the instance and then readjust

00:19:51,210 --> 00:19:54,900
the spot rate capacity this is to

00:19:53,370 --> 00:19:57,330
basically a trick spot rate into

00:19:54,900 --> 00:20:03,000
thinking that you know we have

00:19:57,330 --> 00:20:05,610
terminated the instances that we want as

00:20:03,000 --> 00:20:08,430
I mentioned before view spot instances

00:20:05,610 --> 00:20:10,620
so back in May of 2015 we were using a

00:20:08,430 --> 00:20:13,650
combination of our eyes reserved

00:20:10,620 --> 00:20:15,300
instances and on-demand instances we

00:20:13,650 --> 00:20:17,670
slowly started ramping up on in spot

00:20:15,300 --> 00:20:23,130
instances and you can see in July 2015

00:20:17,670 --> 00:20:26,070
we saw a 55% reduction in cost and we

00:20:23,130 --> 00:20:30,240
started using all spot instances around

00:20:26,070 --> 00:20:32,610
December of 2015 and utilization the

00:20:30,240 --> 00:20:34,830
cost of the cluster has gone down eighty

00:20:32,610 --> 00:20:40,610
percent so that's a huge amount of

00:20:34,830 --> 00:20:43,260
saving now I'll be talking about some

00:20:40,610 --> 00:20:45,000
challenges that we faced while building

00:20:43,260 --> 00:20:48,000
the system and the solutions that we

00:20:45,000 --> 00:20:52,820
came up with so first issue we ran into

00:20:48,000 --> 00:20:56,220
was bandwidth while talking to s3 so are

00:20:52,820 --> 00:20:59,430
we are using NAT boxes to talk to s 3

00:20:56,220 --> 00:21:03,750
and there's limited bandwidth that night

00:20:59,430 --> 00:21:09,000
boxes have so we also use docker images

00:21:03,750 --> 00:21:11,910
which are backed by s3 buckets so we had

00:21:09,000 --> 00:21:14,130
to come up with and with a way to kind

00:21:11,910 --> 00:21:19,650
of avoid we're talking via night night

00:21:14,130 --> 00:21:21,930
boxes so basically the solution here was

00:21:19,650 --> 00:21:24,710
to use VP cs3 endpoints

00:21:21,930 --> 00:21:28,680
they provide fast and secure access to

00:21:24,710 --> 00:21:30,429
AWS s3 without any bandwidth limitation

00:21:28,680 --> 00:21:32,350
all the s3

00:21:30,429 --> 00:21:35,019
eliminations are enforced but in terms

00:21:32,350 --> 00:21:39,129
of bandwidth there are they don't impose

00:21:35,019 --> 00:21:41,590
any restrictions and one good thing here

00:21:39,129 --> 00:21:42,879
is that your traffic never leaves Amazon

00:21:41,590 --> 00:21:45,730
Network your traffic never goes to

00:21:42,879 --> 00:21:49,629
public Internet it just stays on Amazon

00:21:45,730 --> 00:21:52,149
network but one caveat here is to hear

00:21:49,629 --> 00:21:54,610
here is that your data and your

00:21:52,149 --> 00:21:56,320
application has to be in same AWS region

00:21:54,610 --> 00:21:58,629
so if your bucket is present in either

00:21:56,320 --> 00:22:01,090
place region then you must go locate

00:21:58,629 --> 00:22:04,919
your application in that same region to

00:22:01,090 --> 00:22:07,860
take advantage of PPC endpoints after

00:22:04,919 --> 00:22:11,110
moving to VPC endpoints we have seen

00:22:07,860 --> 00:22:13,480
lots of lots of free bandwidth on our

00:22:11,110 --> 00:22:17,039
NAT boxes and other applications in the

00:22:13,480 --> 00:22:17,039
V PC are happy now

00:22:17,499 --> 00:22:21,669
central docker registries as I mentioned

00:22:19,450 --> 00:22:24,759
before we run 2.5 million docker

00:22:21,669 --> 00:22:26,889
containers and lots of different images

00:22:24,759 --> 00:22:29,559
we download from docker registries

00:22:26,889 --> 00:22:32,379
docker registries are backed by s3 so

00:22:29,559 --> 00:22:35,289
they simply send s3 redirects after

00:22:32,379 --> 00:22:37,570
getting a request but so initial setup

00:22:35,289 --> 00:22:39,279
we had at Yelp was basically we had

00:22:37,570 --> 00:22:41,289
multiple Locker registries on a single

00:22:39,279 --> 00:22:43,029
host backed by an engine extensions

00:22:41,289 --> 00:22:47,320
nginx was supposed to load balanced

00:22:43,029 --> 00:22:49,450
across these registries but the scale at

00:22:47,320 --> 00:22:51,789
which we are operating basically failed

00:22:49,450 --> 00:22:55,289
to cope up with the number of requests

00:22:51,789 --> 00:22:57,429
we were making we also tried to put

00:22:55,289 --> 00:23:00,009
registries on different in different

00:22:57,429 --> 00:23:02,440
hosts and you know put it put them

00:23:00,009 --> 00:23:05,950
behind lbs but we ran into issues which

00:23:02,440 --> 00:23:08,320
strictly cookies etc and we couldn't

00:23:05,950 --> 00:23:10,509
really do it so the solution here is

00:23:08,320 --> 00:23:13,149
that we run multiple doctor registries

00:23:10,509 --> 00:23:15,730
on each agent so docker registry is

00:23:13,149 --> 00:23:18,249
basically a docker container which sends

00:23:15,730 --> 00:23:21,249
a redirects to s3 and it doesn't matter

00:23:18,249 --> 00:23:22,960
where it is running so on each Seigle

00:23:21,249 --> 00:23:26,070
agent we have two different registries

00:23:22,960 --> 00:23:28,809
running and we use Etsy hosts to

00:23:26,070 --> 00:23:31,960
basically resolve the resolve to local

00:23:28,809 --> 00:23:33,789
host so this has been a game changer

00:23:31,960 --> 00:23:36,159
we have never run into any issues with a

00:23:33,789 --> 00:23:38,139
bandwidth or docket registry is not

00:23:36,159 --> 00:23:40,389
being able to cope up with with the

00:23:38,139 --> 00:23:42,650
number of requests we are making I

00:23:40,389 --> 00:23:44,420
highly recommend this if you have

00:23:42,650 --> 00:23:45,620
have run into this issue you should

00:23:44,420 --> 00:23:49,100
definitely try it out try out this

00:23:45,620 --> 00:23:51,200
solution the second main problem is spot

00:23:49,100 --> 00:23:53,780
instances spot instances can be

00:23:51,200 --> 00:23:56,000
reclaimed at any point of time by Amazon

00:23:53,780 --> 00:23:58,040
so you have to make sure that your

00:23:56,000 --> 00:24:00,230
application is fault tolerant it can

00:23:58,040 --> 00:24:04,580
deal with executors going away it can

00:24:00,230 --> 00:24:07,040
recover from this from this failure so

00:24:04,580 --> 00:24:09,110
what we do is we have a cron job which

00:24:07,040 --> 00:24:11,090
is constantly checking for termination

00:24:09,110 --> 00:24:13,910
notice after getting the termination

00:24:11,090 --> 00:24:16,690
notice we terminate all the executors

00:24:13,910 --> 00:24:19,670
that are running on the on the agent our

00:24:16,690 --> 00:24:21,530
scheduler gets tasks lost status update

00:24:19,670 --> 00:24:23,060
for each executor and it basically

00:24:21,530 --> 00:24:25,730
reschedules them elsewhere

00:24:23,060 --> 00:24:29,210
we also terminate missus agent process

00:24:25,730 --> 00:24:34,120
in order to prevent it from getting more

00:24:29,210 --> 00:24:36,530
tasks and basically losing those tasks

00:24:34,120 --> 00:24:36,830
so this has also worked really well for

00:24:36,530 --> 00:24:39,800
us

00:24:36,830 --> 00:24:43,640
we've not run into any issues with spot

00:24:39,800 --> 00:24:47,240
instances going away as you may know

00:24:43,640 --> 00:24:49,310
spot markets pretty volatile bed prices

00:24:47,240 --> 00:24:52,430
for in spot instance prices can go up

00:24:49,310 --> 00:24:54,320
down they can fluctuate and can have an

00:24:52,430 --> 00:24:57,880
adverse impact on your application

00:24:54,320 --> 00:25:00,740
so getting the bid price is really hard

00:24:57,880 --> 00:25:03,920
how much money you build on a particular

00:25:00,740 --> 00:25:05,150
instance it's hard to determine and the

00:25:03,920 --> 00:25:07,010
trade-off that you have here is

00:25:05,150 --> 00:25:09,290
basically availability and cost savings

00:25:07,010 --> 00:25:12,140
if you are willing to pay more cost then

00:25:09,290 --> 00:25:15,470
you can get more availability and vice

00:25:12,140 --> 00:25:16,900
versa so the solution here is to make

00:25:15,470 --> 00:25:19,850
your application fault tolerant and

00:25:16,900 --> 00:25:22,700
diversify into more spot markets so for

00:25:19,850 --> 00:25:24,740
Siegel we use ten different instance

00:25:22,700 --> 00:25:27,590
types and three different daisies so we

00:25:24,740 --> 00:25:31,100
have 30 different spot markets we use a

00:25:27,590 --> 00:25:36,320
wide range of instances c4i twos am

00:25:31,100 --> 00:25:39,890
force and basically this has allowed us

00:25:36,320 --> 00:25:43,100
to you know have compute cluster

00:25:39,890 --> 00:25:47,600
available all the time we also use spot

00:25:43,100 --> 00:25:49,580
fleet that makes sure that we are never

00:25:47,600 --> 00:25:52,370
paying more than on-demand price of an

00:25:49,580 --> 00:25:54,560
instance if price of an instance goes

00:25:52,370 --> 00:25:56,510
above on-demand price then we basically

00:25:54,560 --> 00:25:58,790
terminate those instances

00:25:56,510 --> 00:26:03,669
and we get capacity from other markets

00:25:58,790 --> 00:26:03,669
so this allows us to keep the cost down

00:26:03,730 --> 00:26:08,660
issues with docker daemon we have run

00:26:06,169 --> 00:26:10,700
into a lot of issues with docker mainly

00:26:08,660 --> 00:26:13,130
because of dhaka Damon

00:26:10,700 --> 00:26:16,450
sometimes docker basically gets locked

00:26:13,130 --> 00:26:19,100
up and stops responding to our requests

00:26:16,450 --> 00:26:21,890
we've also seen deadlocks and docker

00:26:19,100 --> 00:26:25,150
Damon every time we upgrade docker we

00:26:21,890 --> 00:26:26,870
always run into somewhat the other issue

00:26:25,150 --> 00:26:29,900
docker Damon

00:26:26,870 --> 00:26:31,549
sometimes fails to resolve DNS while

00:26:29,900 --> 00:26:33,530
other tools like take are working fine

00:26:31,549 --> 00:26:36,610
we have tried different solutions like

00:26:33,530 --> 00:26:40,130
Sego dns etc but it has not really

00:26:36,610 --> 00:26:42,290
worked for us we use au officer as a

00:26:40,130 --> 00:26:46,130
reunion file system and we are running

00:26:42,290 --> 00:26:48,980
kernel for 4.2 and we've often run into

00:26:46,130 --> 00:26:53,570
issues where it basically causes kernel

00:26:48,980 --> 00:26:55,880
panic and our CPU is going too soft CPU

00:26:53,570 --> 00:26:56,900
lockup state where we you cannot run any

00:26:55,880 --> 00:26:59,900
task on the agent

00:26:56,900 --> 00:27:02,990
so we basically run a cron job which

00:26:59,900 --> 00:27:07,130
periodically it SS is just to each box

00:27:02,990 --> 00:27:08,750
and it checks D message and sees if the

00:27:07,130 --> 00:27:10,340
instance has gone bad

00:27:08,750 --> 00:27:13,850
if it has then it goes ahead and

00:27:10,340 --> 00:27:17,210
terminates the instance but the IE v FS

00:27:13,850 --> 00:27:17,809
is probably one of the better Union file

00:27:17,210 --> 00:27:21,530
systems

00:27:17,809 --> 00:27:25,429
based on our experience orphan docker

00:27:21,530 --> 00:27:28,730
containers so after running 2.5 million

00:27:25,429 --> 00:27:32,150
docker containers we have had a lot of

00:27:28,730 --> 00:27:34,280
orphaned containers so these what

00:27:32,150 --> 00:27:37,160
happens is that application tries to

00:27:34,280 --> 00:27:38,840
remove containers but because Dhaka

00:27:37,160 --> 00:27:41,450
Damon is not responding it gets an

00:27:38,840 --> 00:27:43,520
exception and just exits and talker

00:27:41,450 --> 00:27:45,410
containers are left behind these

00:27:43,520 --> 00:27:46,700
containers are not account the resources

00:27:45,410 --> 00:27:49,549
that these containers use are not

00:27:46,700 --> 00:27:50,750
accounted for in misses so misses things

00:27:49,549 --> 00:27:53,210
that you know there's more room to

00:27:50,750 --> 00:27:55,010
schedule tasks on the host and

00:27:53,210 --> 00:27:59,030
eventually our boxes around out of

00:27:55,010 --> 00:28:02,120
memory so this used to cause a lot of

00:27:59,030 --> 00:28:04,790
issues for us and then we came up with a

00:28:02,120 --> 00:28:07,340
solution so we wrote a tool called of a

00:28:04,790 --> 00:28:09,830
rocker Reaper so it's a proxy for dhaka

00:28:07,340 --> 00:28:12,850
Damon it's written in Co it's a

00:28:09,830 --> 00:28:16,309
was to be transparent so if you send a

00:28:12,850 --> 00:28:19,460
particular signal to it will forward to

00:28:16,309 --> 00:28:21,740
its children and it basically cleans up

00:28:19,460 --> 00:28:24,919
docker containers after the child

00:28:21,740 --> 00:28:28,220
process goes away so the way it works is

00:28:24,919 --> 00:28:31,130
we have makes us executors it creates a

00:28:28,220 --> 00:28:33,200
docker Reaper it basically creates the

00:28:31,130 --> 00:28:35,330
UNIX socket and sets the docker host

00:28:33,200 --> 00:28:38,630
environment variable which is used by

00:28:35,330 --> 00:28:41,210
docker clients toc-toc a daemon at

00:28:38,630 --> 00:28:43,220
forque execs is a child process a child

00:28:41,210 --> 00:28:46,190
process then goes and creates containers

00:28:43,220 --> 00:28:49,399
so create container API call is

00:28:46,190 --> 00:28:52,399
intercepted by our proxy it forwards the

00:28:49,399 --> 00:28:55,130
call to talk a daemon when it gets a

00:28:52,399 --> 00:28:56,960
response back it gets the container ID

00:28:55,130 --> 00:28:59,269
it records the container ID in memory

00:28:56,960 --> 00:29:01,669
and forwards it forwards it to the child

00:28:59,269 --> 00:29:03,889
process when the child process goes away

00:29:01,669 --> 00:29:07,000
it goes ahead and removes the container

00:29:03,889 --> 00:29:09,789
so this has basically allowed us to

00:29:07,000 --> 00:29:12,019
reduce the number of orphaned containers

00:29:09,789 --> 00:29:14,269
but because of various issues of the

00:29:12,019 --> 00:29:16,639
akka diamond we still are left with a

00:29:14,269 --> 00:29:18,860
few orphan containers so we run

00:29:16,639 --> 00:29:21,019
different cron jobs on the on the agents

00:29:18,860 --> 00:29:22,460
that check which instance which docker

00:29:21,019 --> 00:29:24,889
containers are running for more than 30

00:29:22,460 --> 00:29:29,750
minutes and they go ahead and remove the

00:29:24,889 --> 00:29:31,490
containers makes us maintenance mode so

00:29:29,750 --> 00:29:34,570
this is a great feature that makes us

00:29:31,490 --> 00:29:36,860
provides but unfortunately it is

00:29:34,570 --> 00:29:39,169
designed to be used by a single operator

00:29:36,860 --> 00:29:40,820
so anytime you make a post request to

00:29:39,169 --> 00:29:43,850
this API it will overwrite the existing

00:29:40,820 --> 00:29:45,980
maintenance schedule so if you want to

00:29:43,850 --> 00:29:49,039
use this feature in your agents you need

00:29:45,980 --> 00:29:52,220
to make sure that only one agent is able

00:29:49,039 --> 00:29:54,830
to talk to mrs. and make post requests

00:29:52,220 --> 00:29:56,690
to a particular endpoint you need

00:29:54,830 --> 00:29:59,990
basically external locking to ensure

00:29:56,690 --> 00:30:01,940
that this happens so you can use a

00:29:59,990 --> 00:30:03,740
zookeeper based mechanism to make sure

00:30:01,940 --> 00:30:07,669
only one agent is talking to the

00:30:03,740 --> 00:30:09,769
zookeeper to missus say yeah we are

00:30:07,669 --> 00:30:12,080
talking to me so spokes and we're trying

00:30:09,769 --> 00:30:16,460
to figure out if we can improve the

00:30:12,080 --> 00:30:19,419
maintenance mode and make it usable by

00:30:16,460 --> 00:30:19,419
multiple operators

00:30:20,000 --> 00:30:25,160
yeah future of seagulls

00:30:22,520 --> 00:30:27,910
where would we like to go in next year

00:30:25,160 --> 00:30:30,830
or so we'd love to use our subscription

00:30:27,910 --> 00:30:33,050
so our subscription will basically allow

00:30:30,830 --> 00:30:35,030
us to use residual capacity in the

00:30:33,050 --> 00:30:36,770
cluster and run low priority patch

00:30:35,030 --> 00:30:39,350
workload for example for the

00:30:36,770 --> 00:30:42,170
classification if there's no stringent

00:30:39,350 --> 00:30:44,270
upper bound on when classification

00:30:42,170 --> 00:30:47,530
should finish we can just run it on the

00:30:44,270 --> 00:30:50,660
on the residual capacity in the cluster

00:30:47,530 --> 00:30:52,730
you're also replacing the core component

00:30:50,660 --> 00:30:55,970
of Seigle scheduler single scheduler was

00:30:52,730 --> 00:30:56,330
written a long time back and it needs an

00:30:55,970 --> 00:30:58,430
update

00:30:56,330 --> 00:31:01,190
so we have basically written a library

00:30:58,430 --> 00:31:03,320
called task processing and we are

00:31:01,190 --> 00:31:04,910
replacing the scheduler with this

00:31:03,320 --> 00:31:07,490
library

00:31:04,910 --> 00:31:09,740
we'd also love to use CSI plugin once it

00:31:07,490 --> 00:31:11,900
is available and replaced the cluster

00:31:09,740 --> 00:31:14,690
wise resources with this solution which

00:31:11,900 --> 00:31:16,640
is more robust and we delegate the

00:31:14,690 --> 00:31:19,210
responsibility of allocating cluster

00:31:16,640 --> 00:31:22,520
wise resources to mesos which is great

00:31:19,210 --> 00:31:25,130
we also want to make siegel easy of

00:31:22,520 --> 00:31:27,950
easier to use some more more and more

00:31:25,130 --> 00:31:31,000
people at Yelp can take advantage of

00:31:27,950 --> 00:31:33,500
parallelization that siegel provides

00:31:31,000 --> 00:31:35,360
executor improvements would love to

00:31:33,500 --> 00:31:37,460
containerize everything we love on

00:31:35,360 --> 00:31:41,570
docker containers most of our products

00:31:37,460 --> 00:31:45,430
and workload runs on docker and we would

00:31:41,570 --> 00:31:48,080
love to containerize everything but also

00:31:45,430 --> 00:31:49,910
like to use mesas container Iser and

00:31:48,080 --> 00:31:53,230
docker runtime and basically eliminate

00:31:49,910 --> 00:31:57,820
the need to talk to dr. Damon this will

00:31:53,230 --> 00:32:00,680
save us a lot of effort and money

00:31:57,820 --> 00:32:03,550
basically we don't have to talk to dr.

00:32:00,680 --> 00:32:06,020
Damon after using mises container I said

00:32:03,550 --> 00:32:08,660
so makes us recently added this feature

00:32:06,020 --> 00:32:11,960
called nested containers and pots is a

00:32:08,660 --> 00:32:16,760
great feature especially for Seigle we

00:32:11,960 --> 00:32:18,800
can use nested container Iser and spin

00:32:16,760 --> 00:32:21,950
up docker containers within that

00:32:18,800 --> 00:32:23,600
container and once once our application

00:32:21,950 --> 00:32:25,250
finishes the container will go away and

00:32:23,600 --> 00:32:27,350
you will reap all the containers that

00:32:25,250 --> 00:32:30,050
were started within the nested container

00:32:27,350 --> 00:32:33,050
so we don't have to deal with often

00:32:30,050 --> 00:32:35,540
docker containers

00:32:33,050 --> 00:32:37,880
autoscaler improvements while our

00:32:35,540 --> 00:32:40,550
autoscaler works reasonably well right

00:32:37,880 --> 00:32:42,350
now there are things we can do there are

00:32:40,550 --> 00:32:44,990
better algorithms that we can use and

00:32:42,350 --> 00:32:48,410
you know drive up the utilization of our

00:32:44,990 --> 00:32:50,900
cluster right now we are using a single

00:32:48,410 --> 00:32:53,360
spot fleet request and this basically

00:32:50,900 --> 00:32:56,210
prevents us from using more spot markets

00:32:53,360 --> 00:32:58,550
so with a single sport fleet request you

00:32:56,210 --> 00:33:00,620
can only use 30 spot markets we would

00:32:58,550 --> 00:33:03,110
like to use multiple spot spot fleets

00:33:00,620 --> 00:33:09,290
and expand into more markets and see if

00:33:03,110 --> 00:33:11,870
we can get more savings and we will also

00:33:09,290 --> 00:33:16,250
would also love to use more instance

00:33:11,870 --> 00:33:21,590
types than our single cluster so that

00:33:16,250 --> 00:33:24,290
that may drive down the cost cost so

00:33:21,590 --> 00:33:26,480
yeah we are hiring in Europe and in

00:33:24,290 --> 00:33:28,880
London and Hamburg office we're looking

00:33:26,480 --> 00:33:31,480
for distributed systems engineers and

00:33:28,880 --> 00:33:34,190
managers if you are interested in

00:33:31,480 --> 00:33:37,250
working at Yelp please reach out to me I

00:33:34,190 --> 00:33:43,910
can get you in touch with the people the

00:33:37,250 --> 00:33:49,870
right people so yeah that's all I have

00:33:43,910 --> 00:33:49,870
for this talk anybody has any questions

00:33:59,809 --> 00:34:02,440
okay

00:34:02,600 --> 00:34:05,690

YouTube URL: https://www.youtube.com/watch?v=LhQa_QqQm44


