Title: Using External Persistent Volumes to Reduce Recovery Times and Achieve High Availability on DC OS
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Using External Persistent Volumes to Reduce Recovery Times and Achieve High Availability on DC/OS - Dinesh Israni, Portworx Inc

Most modern distributed applications like Cassandra and HDFS provide replication of data across nodes and failure zones to be able to deal with failures. But the time taken to recover to a pre-failure level of redundancy in cases of permanent node failures can be large, since a lot of data needs to be copied over to the new node. Also, some of these applications cannot accept new writes on the nodes being bootstrapped, further increasing the recovery time. 

Dinesh Israni will talk about how you can use dcos-commons frameworks for Cassandra, Elasticsearch, HDFS, Kafka and Spark along with External Persistent volumes to reduce recovery times for your distributed applications and achieve high availability for applications that donâ€™t provide replication.

About Dinesh Israni
Dinesh Israni is a Senior Software Engineer at Portworx with over 7 years of experience building Distributed Storage solutions. Prior to Portworx, Dinesh was at Microsoft, through their acquisition of StorSimple, working on their Hybrid Cloud Storage solution. Recently, he has been involved in the DC/OS commons frameworks project.
Captions: 
	00:00:00,420 --> 00:00:06,870
welcome everyone my name is Denise Rani

00:00:02,760 --> 00:00:09,030
and I'm suffering today I'm going to be

00:00:06,870 --> 00:00:11,040
talking about how you can use external

00:00:09,030 --> 00:00:13,860
persistent volumes to achieve higher

00:00:11,040 --> 00:00:17,960
mobility and reduce recovery times when

00:00:13,860 --> 00:00:22,020
using stateful services on dis us so

00:00:17,960 --> 00:00:25,109
let's jump right in here are topics that

00:00:22,020 --> 00:00:26,789
I'm to be going to covered I'm going to

00:00:25,109 --> 00:00:28,800
cover today we'll talk about the

00:00:26,789 --> 00:00:31,170
different kinds of stateful services

00:00:28,800 --> 00:00:34,230
then I'll go through the advantages of

00:00:31,170 --> 00:00:35,910
using external persistent volumes I'll

00:00:34,230 --> 00:00:37,770
also give you an introduction about port

00:00:35,910 --> 00:00:40,610
works and how you can deploy services on

00:00:37,770 --> 00:00:42,809
DCOs to take advantage of Botox volumes

00:00:40,610 --> 00:00:45,780
then I'll look them off showing how you

00:00:42,809 --> 00:00:48,000
can install footworks and Cassandra to

00:00:45,780 --> 00:00:49,739
use Botox volumes and basically

00:00:48,000 --> 00:00:56,520
demonstrate failover and some other

00:00:49,739 --> 00:00:59,760
useful scenarios so let's talk about

00:00:56,520 --> 00:01:01,739
stateful services so there are basically

00:00:59,760 --> 00:01:05,040
two types of stateful services when it

00:01:01,739 --> 00:01:07,409
comes to persisting data the first are

00:01:05,040 --> 00:01:09,780
simple applications which don't do their

00:01:07,409 --> 00:01:11,760
own replication they basically rely on

00:01:09,780 --> 00:01:14,189
the underneath storage layer to be

00:01:11,760 --> 00:01:16,049
always available so in cases of failures

00:01:14,189 --> 00:01:18,270
you would the storage layer would

00:01:16,049 --> 00:01:20,549
basically make the same valuable volume

00:01:18,270 --> 00:01:22,320
available on another node so that the

00:01:20,549 --> 00:01:26,189
application can come up with the same

00:01:22,320 --> 00:01:28,080
state the second type is where

00:01:26,189 --> 00:01:31,470
applications do their own replication

00:01:28,080 --> 00:01:33,659
across nodes so in case of node dies or

00:01:31,470 --> 00:01:36,869
fails there's always another copy of the

00:01:33,659 --> 00:01:39,000
data on the cluster so if the nita that

00:01:36,869 --> 00:01:41,549
if the note that had crash does come

00:01:39,000 --> 00:01:43,979
online then the replication takes care

00:01:41,549 --> 00:01:45,979
of basically repairing data back onto

00:01:43,979 --> 00:01:49,350
that node that it had missed

00:01:45,979 --> 00:01:51,210
now this replication or a reapplication

00:01:49,350 --> 00:01:53,460
or repair can either be manually

00:01:51,210 --> 00:01:56,280
triggered or it can be automatic

00:01:53,460 --> 00:01:57,600
depending on the application so some of

00:01:56,280 --> 00:02:00,030
the examples for the first type of

00:01:57,600 --> 00:02:02,640
application are basically WordPress or

00:02:00,030 --> 00:02:04,860
MySQL which run in simple mode and don't

00:02:02,640 --> 00:02:06,990
do their own replication whereas for the

00:02:04,860 --> 00:02:09,479
second type it would be Cassandra or

00:02:06,990 --> 00:02:11,879
HDFS which basically do replication

00:02:09,479 --> 00:02:14,540
across nodes and then they do repairs in

00:02:11,879 --> 00:02:14,540
case of failures

00:02:15,959 --> 00:02:22,230
so now you might you might be asking why

00:02:19,569 --> 00:02:25,030
is this replication strategy important

00:02:22,230 --> 00:02:26,950
well it's because bad things happen all

00:02:25,030 --> 00:02:29,170
the time right your notes could crash

00:02:26,950 --> 00:02:30,970
your network could have issues for

00:02:29,170 --> 00:02:33,040
example your network could get

00:02:30,970 --> 00:02:36,250
partitions so none of your nodes are in

00:02:33,040 --> 00:02:38,470
quorum your disk could gonna go down or

00:02:36,250 --> 00:02:40,209
your nodes could go down you bring your

00:02:38,470 --> 00:02:44,500
entire rack could go down bringing down

00:02:40,209 --> 00:02:46,180
multiple nodes for applications that do

00:02:44,500 --> 00:02:48,220
their own replication there is always

00:02:46,180 --> 00:02:50,769
another copy on one of the other nodes

00:02:48,220 --> 00:02:52,450
in the cluster so you can still continue

00:02:50,769 --> 00:02:56,230
to serve iOS and your application will

00:02:52,450 --> 00:02:59,349
not be affected by the downtime by one

00:02:56,230 --> 00:03:01,329
node being down and if you have to

00:02:59,349 --> 00:03:05,189
replace a node you can just bootstrap it

00:03:01,329 --> 00:03:05,189
and repair all the necessary data to it

00:03:05,310 --> 00:03:09,459
this does end up taking a lot of time

00:03:07,810 --> 00:03:11,319
sometimes or depending on how much data

00:03:09,459 --> 00:03:13,389
you had on the node that did go down and

00:03:11,319 --> 00:03:16,269
in that in that time that you are

00:03:13,389 --> 00:03:19,209
repairing data to that node you would

00:03:16,269 --> 00:03:21,400
your throughput for IO from that for

00:03:19,209 --> 00:03:23,139
that service would basically drop for

00:03:21,400 --> 00:03:25,780
instance if you had a Cassandra cluster

00:03:23,139 --> 00:03:27,250
that was doing replication a three-way

00:03:25,780 --> 00:03:29,169
replication and one of your nodes goes

00:03:27,250 --> 00:03:31,150
down you would then only be able to

00:03:29,169 --> 00:03:32,919
serve IR reads from two of the nodes

00:03:31,150 --> 00:03:35,500
instead of being able to serve from

00:03:32,919 --> 00:03:38,230
three of the nodes and while the repair

00:03:35,500 --> 00:03:39,909
is going down you would also your

00:03:38,230 --> 00:03:41,470
network would also take a hit because

00:03:39,909 --> 00:03:43,419
all the data would need to be

00:03:41,470 --> 00:03:47,129
transferred from the two nodes back to

00:03:43,419 --> 00:03:49,810
the one node that is being brought up

00:03:47,129 --> 00:03:52,359
for non-clustered applications though if

00:03:49,810 --> 00:03:54,459
you had no backup and were using local

00:03:52,359 --> 00:03:56,500
storage your application would be doomed

00:03:54,459 --> 00:03:58,659
because you would not be able to bring

00:03:56,500 --> 00:04:01,419
it back up until you can either move

00:03:58,659 --> 00:04:04,540
your data from the nodes data this form

00:04:01,419 --> 00:04:06,699
the nodes that went down and in case

00:04:04,540 --> 00:04:09,549
your data nodes actually had corruption

00:04:06,699 --> 00:04:11,349
had physical corruption then then your

00:04:09,549 --> 00:04:13,590
you will basically end up losing your

00:04:11,349 --> 00:04:13,590
data

00:04:16,959 --> 00:04:24,100
so how can external persistent storage

00:04:19,989 --> 00:04:26,410
help in all of this well for the case of

00:04:24,100 --> 00:04:28,630
applications like MySQL and WordPress

00:04:26,410 --> 00:04:29,800
which don't do their own replication it

00:04:28,630 --> 00:04:32,889
can help you provide higher availability

00:04:29,800 --> 00:04:34,660
for your sources and it basically makes

00:04:32,889 --> 00:04:38,530
sure that you're dying downtime is

00:04:34,660 --> 00:04:41,110
eliminated and for services that do do

00:04:38,530 --> 00:04:43,449
their own replication it can help reduce

00:04:41,110 --> 00:04:45,160
recovery times by a large amount because

00:04:43,449 --> 00:04:47,440
what will end up happening is you will

00:04:45,160 --> 00:04:49,360
not have to have to bootstrap data back

00:04:47,440 --> 00:04:52,570
to your new node all you would have to

00:04:49,360 --> 00:04:54,970
do is basically repair data for the time

00:04:52,570 --> 00:04:57,220
that you that for the time that the node

00:04:54,970 --> 00:05:00,729
was down before it came back on another

00:04:57,220 --> 00:05:05,229
node I'll talk a little bit more about

00:05:00,729 --> 00:05:07,210
this in the next slides another

00:05:05,229 --> 00:05:10,120
advantage of using external storage

00:05:07,210 --> 00:05:11,949
providers like port works is that it

00:05:10,120 --> 00:05:14,320
helps you watch utilize your storage so

00:05:11,949 --> 00:05:16,800
you can grow your you can basically grow

00:05:14,320 --> 00:05:24,550
your compute and power and storage needs

00:05:16,800 --> 00:05:26,440
independently of each other ok so before

00:05:24,550 --> 00:05:27,699
we talk about the scenarios I just

00:05:26,440 --> 00:05:29,560
wanted to give a brief introduction

00:05:27,699 --> 00:05:31,900
about port works because a lot of the

00:05:29,560 --> 00:05:33,940
scenarios that we talk about depend on a

00:05:31,900 --> 00:05:37,020
software storage solution like port

00:05:33,940 --> 00:05:40,349
works already installed on your cluster

00:05:37,020 --> 00:05:42,130
so port works is the first

00:05:40,349 --> 00:05:44,260
production-ready software-defined

00:05:42,130 --> 00:05:48,070
storage designed for the from the ground

00:05:44,260 --> 00:05:50,050
up for micro services in mind so using

00:05:48,070 --> 00:05:53,169
port works you can provision and manage

00:05:50,050 --> 00:05:55,389
container granular virtual devices and

00:05:53,169 --> 00:05:57,159
our title integration with schedulers

00:05:55,389 --> 00:05:58,960
and container orchestrators basically

00:05:57,159 --> 00:06:03,370
help run your workload local to where

00:05:58,960 --> 00:06:05,110
the storage is is provisioned apart from

00:06:03,370 --> 00:06:07,289
this we also have a couple of other

00:06:05,110 --> 00:06:10,900
useful features that help manage your

00:06:07,289 --> 00:06:12,550
storage daily daily needs for storage so

00:06:10,900 --> 00:06:15,039
you can basically take copy-on-write

00:06:12,550 --> 00:06:17,139
snapshots which can be restored from in

00:06:15,039 --> 00:06:19,030
case you have any outage or any issue

00:06:17,139 --> 00:06:22,389
with the services that are using your

00:06:19,030 --> 00:06:24,009
volumes you can also take you can also

00:06:22,389 --> 00:06:25,570
take something called cloud snaps which

00:06:24,009 --> 00:06:28,630
will basically back up your entire

00:06:25,570 --> 00:06:31,210
volumes into any object store outside

00:06:28,630 --> 00:06:32,740
your cluster so basically that object

00:06:31,210 --> 00:06:34,870
stole can be any as three compliant

00:06:32,740 --> 00:06:39,460
objects store your Azure blob storage or

00:06:34,870 --> 00:06:42,580
Google Google Cloud storage we also

00:06:39,460 --> 00:06:44,800
allow encryption of volumes on either

00:06:42,580 --> 00:06:47,199
cluster level so you can you can have

00:06:44,800 --> 00:06:49,720
one cluster wide encryption key or you

00:06:47,199 --> 00:06:51,940
can have you can have encryption keys on

00:06:49,720 --> 00:06:54,039
a per volume basis so this is very

00:06:51,940 --> 00:06:55,900
helpful in cases where you have multi

00:06:54,039 --> 00:06:57,310
tenants and you don't want you don't

00:06:55,900 --> 00:06:59,680
want to share our encryption keys

00:06:57,310 --> 00:07:05,680
between different volumes and different

00:06:59,680 --> 00:07:08,710
services another feature of port works

00:07:05,680 --> 00:07:11,530
is that everything is basically API

00:07:08,710 --> 00:07:13,599
driven so everything can basically be

00:07:11,530 --> 00:07:15,460
automated you don't have to manually go

00:07:13,599 --> 00:07:18,160
in and either provision volumes or

00:07:15,460 --> 00:07:20,919
manage your data you can do it you can

00:07:18,160 --> 00:07:23,199
basically this is written for our devops

00:07:20,919 --> 00:07:24,789
from the ground up so everything is

00:07:23,199 --> 00:07:26,229
basically automatable and you would

00:07:24,789 --> 00:07:28,840
never have to actually go in and

00:07:26,229 --> 00:07:33,009
manually do any kind of repair or

00:07:28,840 --> 00:07:35,169
recovery and we actually run as a

00:07:33,009 --> 00:07:39,490
container ourselves so it makes it very

00:07:35,169 --> 00:07:41,320
easy to deploy and maintain and you must

00:07:39,490 --> 00:07:43,330
have heard about CSI in the past couple

00:07:41,320 --> 00:07:46,539
of days and as soon as the schedule

00:07:43,330 --> 00:07:52,440
orchestrators have support for CSI port

00:07:46,539 --> 00:07:55,270
works we'll also have so have support so

00:07:52,440 --> 00:07:58,210
this is how basically a poor folks would

00:07:55,270 --> 00:07:59,860
look once it's deployed so port works

00:07:58,210 --> 00:08:01,750
would basically scan all the block

00:07:59,860 --> 00:08:04,330
devices whether it is your direct

00:08:01,750 --> 00:08:06,520
attached storage storage devices or an

00:08:04,330 --> 00:08:10,000
EBS volume or assured managed disks

00:08:06,520 --> 00:08:13,330
disks and cast them out into one big

00:08:10,000 --> 00:08:15,219
clustered storage pool and when your

00:08:13,330 --> 00:08:17,490
pods or your containers are spinning up

00:08:15,219 --> 00:08:19,990
they would basically go ahead and

00:08:17,490 --> 00:08:22,750
request for thinly provisioned volumes

00:08:19,990 --> 00:08:25,599
from this cluster storage pool and we

00:08:22,750 --> 00:08:28,630
would basically replicate data for these

00:08:25,599 --> 00:08:29,889
volumes across nodes so that in case one

00:08:28,630 --> 00:08:31,360
of your nodes goes down you're still

00:08:29,889 --> 00:08:35,380
able to bring up the container on

00:08:31,360 --> 00:08:38,050
another node with the same state so here

00:08:35,380 --> 00:08:39,909
basically the orange part is what port

00:08:38,050 --> 00:08:41,709
works would comprise off and you would

00:08:39,909 --> 00:08:44,440
basically run different apps which would

00:08:41,709 --> 00:08:46,709
which would request for volumes from

00:08:44,440 --> 00:08:46,709
port

00:08:48,810 --> 00:08:51,640
so I talked about block-level

00:08:50,800 --> 00:08:53,769
replication

00:08:51,640 --> 00:08:56,500
this is how basically it would work so

00:08:53,769 --> 00:08:58,510
all all all rights would basically be

00:08:56,500 --> 00:09:01,810
synchronously replicated across the

00:08:58,510 --> 00:09:03,820
replicas across multiple nodes and these

00:09:01,810 --> 00:09:05,380
replicas are actually accessible not

00:09:03,820 --> 00:09:07,329
just from the nodes where the data lies

00:09:05,380 --> 00:09:10,510
but from any node where Botox is

00:09:07,329 --> 00:09:13,120
installed so in case you you want to

00:09:10,510 --> 00:09:15,700
scale your cluster where you have a few

00:09:13,120 --> 00:09:19,060
nodes which have more compute and memory

00:09:15,700 --> 00:09:21,820
and have separate nodes which have more

00:09:19,060 --> 00:09:23,589
storage you can always have you can

00:09:21,820 --> 00:09:26,380
always start up port works as storage

00:09:23,589 --> 00:09:27,700
less nodes to consume storage from from

00:09:26,380 --> 00:09:31,300
the nodes which have more storage

00:09:27,700 --> 00:09:35,079
available and in this case what would

00:09:31,300 --> 00:09:37,149
happen is basically all all all all

00:09:35,079 --> 00:09:39,160
writes would basically be synchronously

00:09:37,149 --> 00:09:41,740
replicated on to other nodes and if a

00:09:39,160 --> 00:09:43,029
node goes down basically you would be

00:09:41,740 --> 00:09:46,470
able you would still be able to use

00:09:43,029 --> 00:09:48,820
another replica from another node and

00:09:46,470 --> 00:09:50,829
when the node that had gone gone down

00:09:48,820 --> 00:09:53,110
basically comes back up port works in

00:09:50,829 --> 00:09:56,680
the background would repair the data to

00:09:53,110 --> 00:09:59,829
bring that replica back up to state with

00:09:56,680 --> 00:10:02,079
the current state of the volume and if

00:09:59,829 --> 00:10:03,940
that node goes down permanently what

00:10:02,079 --> 00:10:06,370
port works would do is basically we

00:10:03,940 --> 00:10:08,890
would read replicate that data onto an

00:10:06,370 --> 00:10:11,320
other node so that you always have the

00:10:08,890 --> 00:10:22,029
minimum replication that you had

00:10:11,320 --> 00:10:23,829
specified for your volume so now let's

00:10:22,029 --> 00:10:27,240
talk about the recovery times with local

00:10:23,829 --> 00:10:29,649
volumes and external persistent volumes

00:10:27,240 --> 00:10:32,970
this is this mostly talking about the

00:10:29,649 --> 00:10:36,810
second type of applications where where

00:10:32,970 --> 00:10:40,089
applications do their own replication

00:10:36,810 --> 00:10:41,529
now when tasks use they use local

00:10:40,089 --> 00:10:45,279
storage they are basically pinned to

00:10:41,529 --> 00:10:48,250
that node so with local volumes if a

00:10:45,279 --> 00:10:50,050
node crashes the fastest that the node

00:10:48,250 --> 00:10:52,060
comes back up online is better because

00:10:50,050 --> 00:10:55,510
then you have less amount of data to

00:10:52,060 --> 00:10:57,939
repair but but in reality that is not

00:10:55,510 --> 00:10:59,679
always the case notes sometimes good

00:10:57,939 --> 00:11:01,509
you you could take down nodes for

00:10:59,679 --> 00:11:03,489
maintenance cycles and in that case your

00:11:01,509 --> 00:11:05,439
your your time period that the node is

00:11:03,489 --> 00:11:07,599
down could be large so you will end up

00:11:05,439 --> 00:11:11,979
having to repair a large amount of data

00:11:07,599 --> 00:11:13,869
and and and nodes do take up some time

00:11:11,979 --> 00:11:17,079
to come back up in case they had crashed

00:11:13,869 --> 00:11:18,999
so that just extends the cycle extends

00:11:17,079 --> 00:11:22,089
the time that that you would have to

00:11:18,999 --> 00:11:26,049
repair data for but in case that the

00:11:22,089 --> 00:11:28,419
node fails permanently this this repair

00:11:26,049 --> 00:11:29,949
time can be even longer because you

00:11:28,419 --> 00:11:33,609
would basically have to rewrap legate

00:11:29,949 --> 00:11:36,039
all your data from from one of the other

00:11:33,609 --> 00:11:38,679
replicas in the cluster back to that

00:11:36,039 --> 00:11:40,509
node and this could end up taking a lot

00:11:38,679 --> 00:11:43,509
of time it could range from anywhere

00:11:40,509 --> 00:11:45,249
between hours to days and while this is

00:11:43,509 --> 00:11:48,069
happening like I had mentioned before

00:11:45,249 --> 00:11:50,859
the throughput of your service will be

00:11:48,069 --> 00:11:53,049
affected and also you would you would

00:11:50,859 --> 00:11:55,239
end up spending a lot of you would end

00:11:53,049 --> 00:11:57,459
up spending here you would end up using

00:11:55,239 --> 00:12:00,249
a lot of network throughput just to the

00:11:57,459 --> 00:12:03,009
repair or data on back onto that node

00:12:00,249 --> 00:12:06,099
and this can actually bring down your

00:12:03,009 --> 00:12:07,599
entire service if you basically end up

00:12:06,099 --> 00:12:14,199
using a lot of throughput just to do the

00:12:07,599 --> 00:12:16,779
repairs now let's take a let's take a

00:12:14,199 --> 00:12:20,099
look at the recovery times with with

00:12:16,779 --> 00:12:22,629
external storage so with external

00:12:20,099 --> 00:12:24,989
volumes basically all your data is

00:12:22,629 --> 00:12:27,999
accessible from any node in the cluster

00:12:24,989 --> 00:12:29,409
so if a node does go down you don't wait

00:12:27,999 --> 00:12:34,179
you don't have to wait for it to come

00:12:29,409 --> 00:12:36,879
back up to basically to to use your to

00:12:34,179 --> 00:12:38,979
bring your service back up Allwood all

00:12:36,879 --> 00:12:41,979
that would need to happen is your shilla

00:12:38,979 --> 00:12:43,989
would need to schedule that service on

00:12:41,979 --> 00:12:45,789
to another node and ask for it to use

00:12:43,989 --> 00:12:48,609
the same volume and it would come back

00:12:45,789 --> 00:12:50,499
to its same state and all you would have

00:12:48,609 --> 00:12:52,569
to do is basically repair data for the

00:12:50,499 --> 00:12:55,779
time that it took for the scheduler to

00:12:52,569 --> 00:12:59,799
reschedule your no rishaad you look your

00:12:55,779 --> 00:13:01,689
pod on to another node and this is

00:12:59,799 --> 00:13:04,299
actually similar in case a note dies

00:13:01,689 --> 00:13:06,759
permanently too because since there is

00:13:04,299 --> 00:13:09,429
always another replica available on on

00:13:06,759 --> 00:13:11,290
the cluster you would just need to wait

00:13:09,429 --> 00:13:13,240
for the scheduler to shut you your

00:13:11,290 --> 00:13:15,040
continue onto another node and it would

00:13:13,240 --> 00:13:16,750
again you would just need to repair the

00:13:15,040 --> 00:13:23,529
leader back onto that node for the time

00:13:16,750 --> 00:13:24,820
that the node was done okay so so so

00:13:23,529 --> 00:13:27,370
here are some of the advantage of using

00:13:24,820 --> 00:13:31,149
port works on a nice software-defined

00:13:27,370 --> 00:13:35,350
storage as like port works for your fire

00:13:31,149 --> 00:13:37,959
solution basically using a sign or an

00:13:35,350 --> 00:13:41,800
ass is an anti-pattern in micro sources

00:13:37,959 --> 00:13:43,600
because a by using a sign on an ass you

00:13:41,800 --> 00:13:46,569
end up losing the flexibility that you

00:13:43,600 --> 00:13:48,519
have for from micro-services because

00:13:46,569 --> 00:13:51,490
you're basically siloing your storage

00:13:48,519 --> 00:13:55,870
from out into into a storage array

00:13:51,490 --> 00:13:59,019
that's outside your current cluster this

00:13:55,870 --> 00:14:01,089
also introduces Layton sees because for

00:13:59,019 --> 00:14:02,560
e for all the data that is to be written

00:14:01,089 --> 00:14:05,290
to your storage array it needs to

00:14:02,560 --> 00:14:07,529
basically Travis Travis from your

00:14:05,290 --> 00:14:10,480
computer cluster to your storage cluster

00:14:07,529 --> 00:14:13,089
and it is also actually a one point of

00:14:10,480 --> 00:14:14,529
failure so in case you lose that link

00:14:13,089 --> 00:14:16,300
between your compute cluster and your

00:14:14,529 --> 00:14:20,260
storage cluster all your stateful

00:14:16,300 --> 00:14:23,079
services will go down at that point and

00:14:20,260 --> 00:14:25,660
and actually having having an external

00:14:23,079 --> 00:14:28,750
storage provide for having an external

00:14:25,660 --> 00:14:30,760
storage array increases increases

00:14:28,750 --> 00:14:33,459
complexities and failures when you are

00:14:30,760 --> 00:14:38,260
dealing with dealing with node crashes

00:14:33,459 --> 00:14:42,100
and other other scenarios that might

00:14:38,260 --> 00:14:44,110
happen in your cluster so like I didn't

00:14:42,100 --> 00:14:46,630
mention pootabucks is built for micro

00:14:44,110 --> 00:14:48,010
services from the ground up and one of

00:14:46,630 --> 00:14:50,110
the things that we do is that we have

00:14:48,010 --> 00:14:53,199
Titan integration with chillers so we

00:14:50,110 --> 00:14:55,839
can actually influence schedulers to

00:14:53,199 --> 00:14:56,560
co-locate your tasks with where the data

00:14:55,839 --> 00:14:59,829
is located

00:14:56,560 --> 00:15:02,410
instead of basically having it located

00:14:59,829 --> 00:15:04,600
anywhere and so we don't shard data

00:15:02,410 --> 00:15:06,790
across multiple across all the nodes we

00:15:04,600 --> 00:15:08,709
make it so that containers can take

00:15:06,790 --> 00:15:11,220
advantage of having data local to your

00:15:08,709 --> 00:15:14,649
to the node where they are scheduled

00:15:11,220 --> 00:15:17,470
another advantage is that there is a

00:15:14,649 --> 00:15:20,829
common solution for hybrid deployments

00:15:17,470 --> 00:15:22,129
so there are a lot of scenarios nowadays

00:15:20,829 --> 00:15:25,100
where

00:15:22,129 --> 00:15:27,470
have have hybrid deployments where they

00:15:25,100 --> 00:15:30,800
have an on-prem cluster but they want to

00:15:27,470 --> 00:15:31,970
basically burst into the cloud now if

00:15:30,800 --> 00:15:33,889
you were not using a software-defined

00:15:31,970 --> 00:15:35,480
storage solution like port works you

00:15:33,889 --> 00:15:37,489
would basically need to have automation

00:15:35,480 --> 00:15:40,879
and tools that were different for both

00:15:37,489 --> 00:15:43,220
your on-prem and cloud solutions by

00:15:40,879 --> 00:15:45,259
having one unified layer of my off

00:15:43,220 --> 00:15:47,929
managing your storage you can eliminate

00:15:45,259 --> 00:15:50,540
that and build your apps faster rather

00:15:47,929 --> 00:15:58,189
than having to worry about managing your

00:15:50,540 --> 00:16:01,309
stories on different deployments okay so

00:15:58,189 --> 00:16:04,730
now you might ask why not just use EBS

00:16:01,309 --> 00:16:08,269
statically so like a pointer out first

00:16:04,730 --> 00:16:11,569
of all EBS will work only on only on AWS

00:16:08,269 --> 00:16:14,959
same thing with other like Azure manage

00:16:11,569 --> 00:16:16,069
this or good cloud storage so you would

00:16:14,959 --> 00:16:18,920
basically need to have different

00:16:16,069 --> 00:16:22,579
automation and tools for your for your

00:16:18,920 --> 00:16:25,910
multi for your hybrid deployments also

00:16:22,579 --> 00:16:28,610
solution also EBS has a limit of 16

00:16:25,910 --> 00:16:31,369
volumes that you can attach to an EC ec2

00:16:28,610 --> 00:16:33,589
instance so in case you have ec2 and

00:16:31,369 --> 00:16:36,439
beefy ec2 instances and you want to work

00:16:33,589 --> 00:16:37,999
a lot of containers on that you will end

00:16:36,439 --> 00:16:40,160
up hitting a limit so you will not be

00:16:37,999 --> 00:16:45,470
able to tightly pack your services on to

00:16:40,160 --> 00:16:48,470
a node in that case one of the most

00:16:45,470 --> 00:16:50,569
common scenarios that you would run into

00:16:48,470 --> 00:16:53,119
with EBS is actually when you're testing

00:16:50,569 --> 00:16:55,339
failover scenarios you will see a lot of

00:16:53,119 --> 00:16:58,100
the time EBS volumes would get stuck in

00:16:55,339 --> 00:17:01,160
attaching or detaching state this is

00:16:58,100 --> 00:17:02,540
this again this problematic if you want

00:17:01,160 --> 00:17:04,130
to basically automate your entire

00:17:02,540 --> 00:17:06,589
environment because somebody will have

00:17:04,130 --> 00:17:08,899
to manually go in and then detach your

00:17:06,589 --> 00:17:10,850
volume or attach it to the new node and

00:17:08,899 --> 00:17:13,760
in fact I was just talking to somebody

00:17:10,850 --> 00:17:16,220
earlier earlier this week and they

00:17:13,760 --> 00:17:18,110
mentioned that one of the volumes got

00:17:16,220 --> 00:17:20,360
stuck in a detaching state because they

00:17:18,110 --> 00:17:21,799
were using a volume plugin and there was

00:17:20,360 --> 00:17:23,929
no way out of it the only thing that

00:17:21,799 --> 00:17:26,419
they could basically do was delete the

00:17:23,929 --> 00:17:28,250
EPS volumes and that is that is a data

00:17:26,419 --> 00:17:32,240
loss situation that you don't want to

00:17:28,250 --> 00:17:34,669
get into in production another thing

00:17:32,240 --> 00:17:35,720
with EBS is the performance is not

00:17:34,669 --> 00:17:37,549
always a

00:17:35,720 --> 00:17:40,159
to mark so you can always pay for

00:17:37,549 --> 00:17:41,750
provision dye ops but at that point you

00:17:40,159 --> 00:17:46,820
will end up spending a lot of lot more

00:17:41,750 --> 00:17:49,460
money and the last thing is filler words

00:17:46,820 --> 00:17:52,070
slow so EBS wasn't designed for the

00:17:49,460 --> 00:17:54,289
micro services in mind so it wasn't

00:17:52,070 --> 00:17:55,520
designed for scenarios where you would

00:17:54,289 --> 00:17:59,360
basically be failing over your

00:17:55,520 --> 00:18:01,490
containers or services very often so it

00:17:59,360 --> 00:18:04,250
ends up taking a lot of time to fail

00:18:01,490 --> 00:18:08,210
over your EBS volumes form from one node

00:18:04,250 --> 00:18:11,900
to another so so there's nothing wrong

00:18:08,210 --> 00:18:13,760
with using EBS as such I mean the only

00:18:11,900 --> 00:18:15,470
thing is you don't want to be using EBS

00:18:13,760 --> 00:18:17,780
volumes directly with your containers

00:18:15,470 --> 00:18:20,299
you want to have a layer on top of EBS

00:18:17,780 --> 00:18:22,490
which does the management of your

00:18:20,299 --> 00:18:23,900
container granular volumes so that you

00:18:22,490 --> 00:18:28,070
can easily failover your containers

00:18:23,900 --> 00:18:36,080
instead of having to depend on EBS doing

00:18:28,070 --> 00:18:37,640
the the move for your volumes so how can

00:18:36,080 --> 00:18:40,789
use port works with your stateful

00:18:37,640 --> 00:18:43,220
services so there are three ways that

00:18:40,789 --> 00:18:45,650
you can do this right now so the first

00:18:43,220 --> 00:18:49,520
way is you can deploy simple services in

00:18:45,650 --> 00:18:52,490
marathon using using port works as the

00:18:49,520 --> 00:18:54,200
volume driver and any applications in

00:18:52,490 --> 00:18:56,960
the DC US universe which allow you to

00:18:54,200 --> 00:18:59,419
configure an external volume provider

00:18:56,960 --> 00:19:03,799
you can basically specify port works as

00:18:59,419 --> 00:19:05,840
as driver the second way is you can

00:19:03,799 --> 00:19:08,720
basically deploy services that we've

00:19:05,840 --> 00:19:11,000
developed based on DCs comments which

00:19:08,720 --> 00:19:14,360
are also available in the universe I'll

00:19:11,000 --> 00:19:15,919
go through these in detail later and we

00:19:14,360 --> 00:19:17,600
also have the DC u.s. Commons framework

00:19:15,919 --> 00:19:19,789
which we've modified to be able to you

00:19:17,600 --> 00:19:21,740
know be able to use port works volumes

00:19:19,789 --> 00:19:27,500
so you can always use that to develop

00:19:21,740 --> 00:19:29,419
your own services too alright so this is

00:19:27,500 --> 00:19:32,840
an example of how you can use marathon

00:19:29,419 --> 00:19:35,600
to use port works volumes this is a

00:19:32,840 --> 00:19:37,970
simple example of my SQL container that

00:19:35,600 --> 00:19:41,330
you would spin up all you would have to

00:19:37,970 --> 00:19:42,770
do is in the parameters for your in the

00:19:41,330 --> 00:19:44,390
parameters for your dock for your

00:19:42,770 --> 00:19:47,419
container you would basically need to

00:19:44,390 --> 00:19:50,149
specify the volume dry or as port works

00:19:47,419 --> 00:19:51,950
in the in the volume parameter you would

00:19:50,149 --> 00:19:53,839
you would be able to specify the size

00:19:51,950 --> 00:19:56,179
for the volume the replication factor

00:19:53,839 --> 00:19:57,559
the name for the volume as well as any

00:19:56,179 --> 00:20:01,070
other parameters that you want to use

00:19:57,559 --> 00:20:02,749
while creating the volume and you don't

00:20:01,070 --> 00:20:04,579
need to pre provision volumes at all in

00:20:02,749 --> 00:20:06,529
this case all of this would be

00:20:04,579 --> 00:20:09,409
dynamically done so as soon as you

00:20:06,529 --> 00:20:10,879
launch this this your message would

00:20:09,409 --> 00:20:13,609
basically try to spin up a docker

00:20:10,879 --> 00:20:16,219
container we would get a request to

00:20:13,609 --> 00:20:18,099
basically mount this volume we would see

00:20:16,219 --> 00:20:20,629
that this volume has not been created

00:20:18,099 --> 00:20:22,429
these options would then get passed to

00:20:20,629 --> 00:20:23,839
us and we would dynamically create these

00:20:22,429 --> 00:20:27,440
volumes and mounted inside your

00:20:23,839 --> 00:20:31,820
container and you can basically use a

00:20:27,440 --> 00:20:35,259
similar type of similar spec to to use

00:20:31,820 --> 00:20:35,259
with docker as well as UCR

00:20:40,290 --> 00:20:46,140
so about the DCOs comments based

00:20:44,280 --> 00:20:49,590
services that I was talking about

00:20:46,140 --> 00:20:51,720
so we basically modified enhanced the

00:20:49,590 --> 00:20:54,990
DCOs comments framework to work with

00:20:51,720 --> 00:20:57,030
port Vox volumes and there are basically

00:20:54,990 --> 00:20:58,380
four services that are available through

00:20:57,030 --> 00:21:00,540
DCOs comments and you can obviously

00:20:58,380 --> 00:21:03,210
write more but the four one four

00:21:00,540 --> 00:21:06,120
services that are available Cassandra

00:21:03,210 --> 00:21:07,530
Hadoop elasticsearch and Kafka and we've

00:21:06,120 --> 00:21:09,360
actually added support for port works

00:21:07,530 --> 00:21:12,210
volumes and submitted these to the

00:21:09,360 --> 00:21:13,740
universe so all you have to do is go to

00:21:12,210 --> 00:21:15,420
the universe and search for port works

00:21:13,740 --> 00:21:18,270
and you will be able to install these

00:21:15,420 --> 00:21:23,400
four services with port works volumes

00:21:18,270 --> 00:21:24,990
backing the state so we've actually made

00:21:23,400 --> 00:21:27,810
a couple of more enhancements where

00:21:24,990 --> 00:21:31,260
we've we've allowed for the task to

00:21:27,810 --> 00:21:32,580
failover between nodes because there is

00:21:31,260 --> 00:21:34,650
there is nothing that's pinning the

00:21:32,580 --> 00:21:38,310
tasks to a particular node now since

00:21:34,650 --> 00:21:39,180
there is port works or backing backing

00:21:38,310 --> 00:21:42,990
these services

00:21:39,180 --> 00:21:45,150
so this basically means that your

00:21:42,990 --> 00:21:47,130
services will have a higher uptime as

00:21:45,150 --> 00:21:51,270
well as your your recovery times will be

00:21:47,130 --> 00:21:53,040
reduced by a large margin we've also

00:21:51,270 --> 00:21:54,900
made changes to the framework so that

00:21:53,040 --> 00:21:57,690
your volumes are co-located with your

00:21:54,900 --> 00:22:02,960
tasks so this reduces latency zalim as

00:21:57,690 --> 00:22:02,960
well as network network usage

00:22:06,930 --> 00:22:12,240
so this is an example of a hello world

00:22:09,600 --> 00:22:15,330
program this basically available in this

00:22:12,240 --> 00:22:17,100
us comments and we've the parts that I

00:22:15,330 --> 00:22:19,590
have highlighted is basically what all

00:22:17,100 --> 00:22:22,140
you will need to change to use Botox

00:22:19,590 --> 00:22:24,360
volumes instead of the default route and

00:22:22,140 --> 00:22:25,890
Mount volumes so Decius Commons is

00:22:24,360 --> 00:22:28,230
actually a great way to write stateful

00:22:25,890 --> 00:22:30,870
services the only thing is right now

00:22:28,230 --> 00:22:33,330
before that support for si si the only

00:22:30,870 --> 00:22:35,580
support route and mount disks disks

00:22:33,330 --> 00:22:38,190
which basically means that your services

00:22:35,580 --> 00:22:40,590
up into a particular node so in case

00:22:38,190 --> 00:22:42,570
that node goes down that task is not

00:22:40,590 --> 00:22:44,520
gonna spin up on another node because

00:22:42,570 --> 00:22:45,470
that data is available only on the node

00:22:44,520 --> 00:22:47,940
that went down

00:22:45,470 --> 00:22:49,920
so we've like I mentioned we've

00:22:47,940 --> 00:22:51,930
basically modified it to support port

00:22:49,920 --> 00:22:54,330
Vox volumes and all you have to do is

00:22:51,930 --> 00:22:57,390
basically change the type of the volume

00:22:54,330 --> 00:23:00,960
from root amount to docker then specify

00:22:57,390 --> 00:23:03,270
the docker volume driver as PXD the

00:23:00,960 --> 00:23:06,090
docker volume name which is the one name

00:23:03,270 --> 00:23:08,610
for the volume so over there you can

00:23:06,090 --> 00:23:10,980
also specify the size of not the size

00:23:08,610 --> 00:23:13,170
the size is a different parameter but

00:23:10,980 --> 00:23:14,460
you can specify the replication factor

00:23:13,170 --> 00:23:16,290
you want to use as well as if you want

00:23:14,460 --> 00:23:17,460
to encrypt volumes and any other

00:23:16,290 --> 00:23:20,010
parameters that you would be able to

00:23:17,460 --> 00:23:22,590
pass to port works so basically you can

00:23:20,010 --> 00:23:25,770
specify any service as a spec like this

00:23:22,590 --> 00:23:28,320
and and specify docker volume drivers

00:23:25,770 --> 00:23:32,310
and port works to to take advantage of

00:23:28,320 --> 00:23:34,080
port works and all of this again volumes

00:23:32,310 --> 00:23:35,610
are all dynamically provisioned so you

00:23:34,080 --> 00:23:37,440
don't have to go out of band or talk to

00:23:35,610 --> 00:23:39,060
your storage provider to provision

00:23:37,440 --> 00:23:40,470
volumes all of this will be taken

00:23:39,060 --> 00:23:43,650
automatically when the service first

00:23:40,470 --> 00:23:46,560
comes up so the source code for this is

00:23:43,650 --> 00:23:49,080
available at github.com slash port works

00:23:46,560 --> 00:23:51,410
slash dcs Commons in case you want to

00:23:49,080 --> 00:23:51,410
take a look

00:23:55,110 --> 00:24:02,760
all right demo time so what I'm going to

00:23:58,679 --> 00:24:05,130
show you is basically I'm going to

00:24:02,760 --> 00:24:06,660
install port works on D cos I'm going to

00:24:05,130 --> 00:24:12,059
show you how easy it is to install this

00:24:06,660 --> 00:24:15,179
us and then I'm gonna install Cassandra

00:24:12,059 --> 00:24:21,179
and go through a few scenarios to

00:24:15,179 --> 00:24:23,429
basically show go through a couple of

00:24:21,179 --> 00:24:26,360
scenarios that you could encounter in in

00:24:23,429 --> 00:24:26,360
your production

00:24:44,900 --> 00:24:48,040
that's what's happening

00:24:49,560 --> 00:24:53,360
let me just open it up again sorry

00:25:10,350 --> 00:25:15,090
yeah so basically port works as well as

00:25:13,350 --> 00:25:16,649
all the other services I specified are

00:25:15,090 --> 00:25:18,120
available in the universe so all you

00:25:16,649 --> 00:25:20,850
have to do is go and search for port

00:25:18,120 --> 00:25:23,309
works we're gonna select port works the

00:25:20,850 --> 00:25:25,200
service and so if you have a six node

00:25:23,309 --> 00:25:27,269
cluster which is basically five private

00:25:25,200 --> 00:25:29,039
nodes and one public agent and we're

00:25:27,269 --> 00:25:31,880
going to install port works on the five

00:25:29,039 --> 00:25:34,080
private agents we're gonna specify the

00:25:31,880 --> 00:25:36,389
management data interface server we want

00:25:34,080 --> 00:25:38,429
to use and just click review and deploy

00:25:36,389 --> 00:25:40,620
and what this is going to end up doing

00:25:38,429 --> 00:25:43,259
is it's gonna spin up an HDD cluster and

00:25:40,620 --> 00:25:45,600
then it's gonna spin up so we use

00:25:43,259 --> 00:25:48,629
actually to store our cluster control

00:25:45,600 --> 00:25:50,370
plane state so it's it's all going to

00:25:48,629 --> 00:25:52,769
spin that up it's gonna spin up in flux

00:25:50,370 --> 00:25:54,179
DB which we use to store statistics it's

00:25:52,769 --> 00:25:57,720
all the burners also going to spin up

00:25:54,179 --> 00:26:00,000
lighthouse which is our UI and finally

00:25:57,720 --> 00:26:04,049
it's going to install port works on all

00:26:00,000 --> 00:26:05,580
the five private agents so I sped up the

00:26:04,049 --> 00:26:08,940
video a little because it ends up taking

00:26:05,580 --> 00:26:11,220
around five six minutes to install but

00:26:08,940 --> 00:26:13,320
as you can see the achieve cluster

00:26:11,220 --> 00:26:15,000
Gardens got installed on the proxy core

00:26:13,320 --> 00:26:17,309
install in flux DB got installed and

00:26:15,000 --> 00:26:19,200
lighthouse gonna install so basically

00:26:17,309 --> 00:26:21,000
what's happening now is the port works

00:26:19,200 --> 00:26:24,029
port works is basically getting

00:26:21,000 --> 00:26:26,340
installed on all the nodes and if you go

00:26:24,029 --> 00:26:28,759
to completed you will see the five tasks

00:26:26,340 --> 00:26:32,549
for the port works install that finished

00:26:28,759 --> 00:26:34,679
so basically what I'm doing now is I'm I

00:26:32,549 --> 00:26:37,980
have assessed into one of the private

00:26:34,679 --> 00:26:40,320
agents and I'm just gonna watch for for

00:26:37,980 --> 00:26:43,350
the status fall from pixie cuddle which

00:26:40,320 --> 00:26:45,950
is our CLI and wait for all the nodes to

00:26:43,350 --> 00:26:45,950
come up

00:26:52,330 --> 00:26:58,960
so as you can see three nodes I have

00:26:54,730 --> 00:27:01,570
come up and I have I have purposely not

00:26:58,960 --> 00:27:02,950
attached at this to one of the nodes so

00:27:01,570 --> 00:27:05,470
you can see that all of these

00:27:02,950 --> 00:27:07,480
automatically get provisioned get added

00:27:05,470 --> 00:27:09,520
to a cluster as either a storage node or

00:27:07,480 --> 00:27:11,320
a storage less note depending on if

00:27:09,520 --> 00:27:13,900
there were block devices attached to

00:27:11,320 --> 00:27:15,430
that node or not so we're just gonna

00:27:13,900 --> 00:27:17,830
pause it here for a second and then

00:27:15,430 --> 00:27:19,180
basically the cluster is up now and as

00:27:17,830 --> 00:27:22,210
you can see we have five nodes in the

00:27:19,180 --> 00:27:24,670
cluster for four of these nodes have 400

00:27:22,210 --> 00:27:27,600
GB or dis attached to them whereas one

00:27:24,670 --> 00:27:30,700
of them is basically a storage less node

00:27:27,600 --> 00:27:32,530
so now that the port works cluster is up

00:27:30,700 --> 00:27:35,110
what we're going to do is we are going

00:27:32,530 --> 00:27:40,330
to go ahead and spin up a three node

00:27:35,110 --> 00:27:42,640
Cassandra cluster on top of this and so

00:27:40,330 --> 00:27:44,320
again the Cassandra port works service

00:27:42,640 --> 00:27:46,390
is available in the universe I'm just

00:27:44,320 --> 00:27:48,640
gonna show you that there are no volumes

00:27:46,390 --> 00:27:51,070
created initially so you just need to

00:27:48,640 --> 00:27:55,150
run pixie Colonel volume list and you

00:27:51,070 --> 00:27:56,650
can see there are no volumes so now

00:27:55,150 --> 00:28:00,210
you're going to go back into the catalog

00:27:56,650 --> 00:28:04,630
and search for the port works Cassandra

00:28:00,210 --> 00:28:06,580
so is and we don't really have to change

00:28:04,630 --> 00:28:08,290
anything but by default the options are

00:28:06,580 --> 00:28:10,930
not specified because it depends on what

00:28:08,290 --> 00:28:12,430
you want to provision it as but we're

00:28:10,930 --> 00:28:14,620
going to go ahead and specify that we

00:28:12,430 --> 00:28:17,110
want a poor box cluster with a

00:28:14,620 --> 00:28:20,650
replication factor of three and this is

00:28:17,110 --> 00:28:22,780
basically going to create three 10gb

00:28:20,650 --> 00:28:25,650
volumes and attach them to each one of

00:28:22,780 --> 00:28:25,650
the Cassandra nodes

00:28:29,400 --> 00:28:35,640
all right so we just have to clear if

00:28:31,500 --> 00:28:36,510
you deploy and then deploy and if you

00:28:35,640 --> 00:28:38,309
couldn't look at the state of the

00:28:36,510 --> 00:28:40,080
service again I've sped it up a little

00:28:38,309 --> 00:28:42,930
because it takes time for Cassandra's to

00:28:40,080 --> 00:28:44,430
spin up to pull the artifacts but as you

00:28:42,930 --> 00:28:46,140
saw one of the nodes that spun up and in

00:28:44,430 --> 00:28:47,820
automatically provisioned a 10 GB volume

00:28:46,140 --> 00:28:51,809
with the replication factor of 3 and

00:28:47,820 --> 00:28:55,590
attached it to a node whose IP ends with

00:28:51,809 --> 00:28:57,450
1 v 1 which will see from the DCOs UI is

00:28:55,590 --> 00:28:59,850
where the DCR

00:28:57,450 --> 00:29:02,910
the node had now cassandra node had spun

00:28:59,850 --> 00:29:04,650
up so the second node comes up and then

00:29:02,910 --> 00:29:06,750
the third node comes up so at this point

00:29:04,650 --> 00:29:09,809
all three node all three volumes will be

00:29:06,750 --> 00:29:11,280
provisioned we'll just take a look just

00:29:09,809 --> 00:29:13,800
do a volume list again and we see that

00:29:11,280 --> 00:29:15,840
all three in all three volumes have been

00:29:13,800 --> 00:29:18,270
created with our application factor of

00:29:15,840 --> 00:29:20,100
three so all we had needed to do was

00:29:18,270 --> 00:29:21,690
specify the base name for the volume and

00:29:20,100 --> 00:29:23,580
then for node 0 it basically created

00:29:21,690 --> 00:29:27,780
Cassandra 0 cassandra one and then

00:29:23,580 --> 00:29:29,580
Cassandra two so now that the cluster is

00:29:27,780 --> 00:29:31,230
up one of the scenarios that I was

00:29:29,580 --> 00:29:34,110
talking about was what happens if your

00:29:31,230 --> 00:29:36,650
node fails so if you didn't have

00:29:34,110 --> 00:29:38,910
something like port works running

00:29:36,650 --> 00:29:41,309
providing storage to your Cassandra

00:29:38,910 --> 00:29:43,679
cluster the framework would basically

00:29:41,309 --> 00:29:46,350
keep retrying waiting for the node to

00:29:43,679 --> 00:29:48,390
come back up and it will not start it on

00:29:46,350 --> 00:29:49,800
another node unless you manually went in

00:29:48,390 --> 00:29:51,780
and said that you want to replace a node

00:29:49,800 --> 00:29:53,460
now if you replace the node you would

00:29:51,780 --> 00:29:54,990
basically need to run a bootstrap panel

00:29:53,460 --> 00:29:58,140
repair command again which could be

00:29:54,990 --> 00:30:00,270
expensive what's going to happen here is

00:29:58,140 --> 00:30:04,230
since we have replicated the data across

00:30:00,270 --> 00:30:05,730
nodes as soon as we kill the node so I'm

00:30:04,230 --> 00:30:07,890
just going to go ahead and kill one of

00:30:05,730 --> 00:30:10,100
the nodes where where the task is

00:30:07,890 --> 00:30:13,260
running I'm just going to power it off

00:30:10,100 --> 00:30:14,400
what we going to see is that the

00:30:13,260 --> 00:30:16,860
framework is going to realize that

00:30:14,400 --> 00:30:20,190
there's nothing for the Stars that is

00:30:16,860 --> 00:30:22,170
pinning it to that node and as you can

00:30:20,190 --> 00:30:24,840
see DCOs already figures out that the

00:30:22,170 --> 00:30:26,429
node is offline and the framework is

00:30:24,840 --> 00:30:27,210
also going to realize that the node node

00:30:26,429 --> 00:30:31,080
has gone offline

00:30:27,210 --> 00:30:35,250
there's nothing all it requires from a

00:30:31,080 --> 00:30:36,809
node is our CPU memory and this and data

00:30:35,250 --> 00:30:38,310
that is not really pinned to a node so

00:30:36,809 --> 00:30:40,920
it's going to basically go ahead and

00:30:38,310 --> 00:30:43,110
spin up that same Cassandra node onto

00:30:40,920 --> 00:30:49,169
another node which was node 8

00:30:43,110 --> 00:30:52,970
ending with 1 1 1 3 1 which is a 3 so

00:30:49,169 --> 00:30:55,499
I'm just gonna go back into a 3 and

00:30:52,970 --> 00:30:57,419
we'll just run quick see Carol volume

00:30:55,499 --> 00:31:01,679
list which will show us that the volume

00:30:57,419 --> 00:31:02,340
has now been attached on to this node so

00:31:01,679 --> 00:31:03,899
yeah

00:31:02,340 --> 00:31:09,690
if you check status that's the node that

00:31:03,899 --> 00:31:11,340
we actually took down and if you do a

00:31:09,690 --> 00:31:13,350
volume list you'll see that Cassandra -

00:31:11,340 --> 00:31:16,440
that's the that's the task that we had

00:31:13,350 --> 00:31:17,850
killed basically comes up on on is

00:31:16,440 --> 00:31:21,960
basically attached to the new node right

00:31:17,850 --> 00:31:23,970
now now one more thing you must notice

00:31:21,960 --> 00:31:27,330
is that I actually just allocated 10 GB

00:31:23,970 --> 00:31:28,470
of data to each one of these nodes now

00:31:27,330 --> 00:31:30,359
if you're running in a production

00:31:28,470 --> 00:31:34,139
cluster you spun up this is one of this

00:31:30,359 --> 00:31:36,059
cluster and give it to your give it to -

00:31:34,139 --> 00:31:38,129
give it to the production guys to use

00:31:36,059 --> 00:31:40,799
you would soon realize that this is not

00:31:38,129 --> 00:31:42,509
enough now if you were using if you are

00:31:40,799 --> 00:31:43,950
not using something like port works

00:31:42,509 --> 00:31:45,779
under need what would need to happen is

00:31:43,950 --> 00:31:50,129
you would need to spin up an entire new

00:31:45,779 --> 00:31:52,950
cluster and allocated more space but

00:31:50,129 --> 00:31:54,929
with port works all you really need to

00:31:52,950 --> 00:31:57,299
do is so I'm just showing the output of

00:31:54,929 --> 00:32:01,080
DF over here and you you can see that

00:31:57,299 --> 00:32:03,929
the volume which is mounted under value

00:32:01,080 --> 00:32:08,159
as the amounts cassandra 0 has a size of

00:32:03,929 --> 00:32:09,509
9.8 CB roughly 10 GB so what we're going

00:32:08,159 --> 00:32:12,720
to do here is we can actually

00:32:09,509 --> 00:32:14,159
dynamically resize this volume so you so

00:32:12,720 --> 00:32:16,710
that you don't really need to take your

00:32:14,159 --> 00:32:18,809
application or service offline all you

00:32:16,710 --> 00:32:20,700
need to do is run once a simple command

00:32:18,809 --> 00:32:23,129
and it will automatically resize your

00:32:20,700 --> 00:32:26,879
volume so all these commands that I'm

00:32:23,129 --> 00:32:28,919
showing I'm using the CLI is just to

00:32:26,879 --> 00:32:32,460
make it clear but all of these are

00:32:28,919 --> 00:32:34,950
actually using the same rest int of rest

00:32:32,460 --> 00:32:37,259
API interface that you can automate

00:32:34,950 --> 00:32:39,450
against so basically you can check that

00:32:37,259 --> 00:32:41,039
as you can dynamically you you can list

00:32:39,450 --> 00:32:43,859
your volumes you can even provision your

00:32:41,039 --> 00:32:45,480
volumes you can you even if you want to

00:32:43,859 --> 00:32:48,499
update the size of your volumes all of

00:32:45,480 --> 00:32:52,740
that is can be driven through api's so

00:32:48,499 --> 00:32:55,510
at this point all I all an admin or a

00:32:52,740 --> 00:32:58,510
DevOps person would need to do is

00:32:55,510 --> 00:33:02,080
Ronna command all use the REST API to

00:32:58,510 --> 00:33:04,530
basically update the volumes size so I'm

00:33:02,080 --> 00:33:07,200
just gonna basically update the size two

00:33:04,530 --> 00:33:12,490
to 100gb

00:33:07,200 --> 00:33:14,230
and this just takes a few seconds and if

00:33:12,490 --> 00:33:16,990
I check the FI finesse you will see that

00:33:14,230 --> 00:33:19,420
this volume is now a hundred gb volume

00:33:16,990 --> 00:33:21,220
and this will basically be available to

00:33:19,420 --> 00:33:22,870
your to your application right away

00:33:21,220 --> 00:33:26,310
so you don't have to take it down or do

00:33:22,870 --> 00:33:26,310
any kind of maintenance for this

00:33:46,650 --> 00:33:55,860
I think I lost my slides yes

00:33:53,600 --> 00:33:56,880
all right so that's the end of the

00:33:55,860 --> 00:33:59,630
presentation

00:33:56,880 --> 00:33:59,630
any questions

00:34:07,950 --> 00:34:11,820
so we actually will awaiting import

00:34:10,169 --> 00:34:15,000
works to be production already inside

00:34:11,820 --> 00:34:15,720
our company and we hit like a chicken

00:34:15,000 --> 00:34:17,970
egg problem

00:34:15,720 --> 00:34:21,960
he's a sapling so we're using de cos

00:34:17,970 --> 00:34:23,490
Martin pigs D driver right and you rot

00:34:21,960 --> 00:34:25,530
some disaster recovery test so basically

00:34:23,490 --> 00:34:28,410
killings note and checking the if not

00:34:25,530 --> 00:34:31,890
successfully during the cluster what

00:34:28,410 --> 00:34:33,570
actually happens is that dr. D after

00:34:31,890 --> 00:34:37,350
reboot he's getting stuck for fifteen

00:34:33,570 --> 00:34:39,990
twelve minutes I actively logging that

00:34:37,350 --> 00:34:42,090
pigs D driver isn't accessible yes after

00:34:39,990 --> 00:34:43,530
some internal timeout it precedes and

00:34:42,090 --> 00:34:45,750
then picks it up and the reason for that

00:34:43,530 --> 00:34:48,360
that the port works itself is inside

00:34:45,750 --> 00:34:50,580
container right so Ricci Connect yes so

00:34:48,360 --> 00:34:53,070
what would you suggest to work around

00:34:50,580 --> 00:34:54,929
the problem yes so we have encountered

00:34:53,070 --> 00:34:57,240
that problem that is actually a

00:34:54,929 --> 00:34:59,610
limitation with darker the width the

00:34:57,240 --> 00:35:02,010
darker works is every time it starts up

00:34:59,610 --> 00:35:04,470
it queries for all the volumes I didn't

00:35:02,010 --> 00:35:06,300
you and it basically tries to talk to

00:35:04,470 --> 00:35:11,130
the plugin to figure out what the state

00:35:06,300 --> 00:35:12,500
of the volume is so the way we've we

00:35:11,130 --> 00:35:15,420
worked around this is we basically

00:35:12,500 --> 00:35:18,120
applying to roll-out port works where

00:35:15,420 --> 00:35:19,710
you would install it as a run c

00:35:18,120 --> 00:35:22,230
container instead of deploying it as a

00:35:19,710 --> 00:35:24,810
put as a docker container so there would

00:35:22,230 --> 00:35:26,670
be no dependency between docker and port

00:35:24,810 --> 00:35:28,770
works in that case so what would happen

00:35:26,670 --> 00:35:30,600
is you would kill docker but port works

00:35:28,770 --> 00:35:32,880
would still be up and running and docker

00:35:30,600 --> 00:35:35,100
would basically go ahead and query port

00:35:32,880 --> 00:35:37,520
works for all the the volumes and it

00:35:35,100 --> 00:35:40,520
would it would not block at that point

00:35:37,520 --> 00:35:40,520
yeah

00:35:42,600 --> 00:35:45,620
any other questions

00:35:53,810 --> 00:35:59,280
so with the block level replication what

00:35:56,760 --> 00:36:01,170
kind of speed hit would we be seeing

00:35:59,280 --> 00:36:03,270
this thought works because it's having

00:36:01,170 --> 00:36:05,310
to send it out but what kind of

00:36:03,270 --> 00:36:06,840
performance we'll be looking at to

00:36:05,310 --> 00:36:10,830
replicate a block layer like three

00:36:06,840 --> 00:36:12,600
replicas so anything greater than one

00:36:10,830 --> 00:36:14,490
replica there will be a slight

00:36:12,600 --> 00:36:18,330
performance it because obviously it's

00:36:14,490 --> 00:36:20,550
going it's you're sending packets across

00:36:18,330 --> 00:36:23,400
the network and what have you seen is

00:36:20,550 --> 00:36:27,180
the the heat is anywhere between three

00:36:23,400 --> 00:36:28,980
and five percent so there is a small hit

00:36:27,180 --> 00:36:31,560
but at that cause you are basically

00:36:28,980 --> 00:36:33,120
getting an added advantage of getting

00:36:31,560 --> 00:36:35,220
high availability and all the other

00:36:33,120 --> 00:36:50,850
features that the storage layer provides

00:36:35,220 --> 00:36:53,190
in this case any other questions hi

00:36:50,850 --> 00:36:55,110
thank you for the presentation I just

00:36:53,190 --> 00:36:57,420
I'm just trying to get familiar with

00:36:55,110 --> 00:36:59,220
some of the concepts you introduced I

00:36:57,420 --> 00:37:02,400
for example wasn't aware of some of the

00:36:59,220 --> 00:37:04,830
components I saw in the least etcd and I

00:37:02,400 --> 00:37:07,520
started wondering about that the

00:37:04,830 --> 00:37:11,520
persistence and what is actually needed

00:37:07,520 --> 00:37:13,320
by port works to operate and will happen

00:37:11,520 --> 00:37:18,240
if we lost for example

00:37:13,320 --> 00:37:20,310
PTC D good question so the way it is in

00:37:18,240 --> 00:37:22,860
the universe we made it very simple for

00:37:20,310 --> 00:37:24,990
somebody to just install and try out

00:37:22,860 --> 00:37:28,140
port works for production though we

00:37:24,990 --> 00:37:31,230
suggest that you install HCD independent

00:37:28,140 --> 00:37:33,270
of a source or D cos because what kind

00:37:31,230 --> 00:37:35,460
of what will happen in that case is in

00:37:33,270 --> 00:37:37,560
case your D cos or in case you have to

00:37:35,460 --> 00:37:39,120
reinstall mesos or T cos your poor folks

00:37:37,560 --> 00:37:42,090
installation will not be affected at all

00:37:39,120 --> 00:37:44,430
so in this case if d 0 if you had to

00:37:42,090 --> 00:37:46,500
reinstall my sauce at da cos since that

00:37:44,430 --> 00:37:47,940
city is using local volumes you will not

00:37:46,500 --> 00:37:49,800
get but at the same offers and you'll

00:37:47,940 --> 00:37:52,530
not be able to bring back at CD with the

00:37:49,800 --> 00:37:54,720
same state so in production we actually

00:37:52,530 --> 00:37:57,360
suggest you have an external HDD apart

00:37:54,720 --> 00:37:59,400
from this but the the point over here is

00:37:57,360 --> 00:38:01,080
to demonstrate how easy it is and in

00:37:59,400 --> 00:38:05,010
case somebody just wants to try it out

00:38:01,080 --> 00:38:07,650
really quickly and HDD is just used to

00:38:05,010 --> 00:38:09,569
store actually just used to store the

00:38:07,650 --> 00:38:10,950
metadata for a control plane there is no

00:38:09,569 --> 00:38:14,369
actual data stored over there

00:38:10,950 --> 00:38:16,259
so it basically stores information on

00:38:14,369 --> 00:38:18,180
what volumes have been provisioned and

00:38:16,259 --> 00:38:19,829
all that such and we're actually working

00:38:18,180 --> 00:38:21,359
on a way that in case you do lose your

00:38:19,829 --> 00:38:23,369
ICD you will still be able to

00:38:21,359 --> 00:38:25,529
reconstruct that CD by looking at data

00:38:23,369 --> 00:38:26,789
across you by looking at all the block

00:38:25,529 --> 00:38:29,099
devices that you have in your cluster

00:38:26,789 --> 00:38:32,180
and basically reconstruct that so we are

00:38:29,099 --> 00:38:32,180
working towards having that yeah

00:38:43,630 --> 00:38:49,150
I show you that you have to launch

00:38:47,259 --> 00:38:51,970
another framework different from the

00:38:49,150 --> 00:38:54,999
missiles framework in order to to launch

00:38:51,970 --> 00:39:00,789
Cassandra I assume with poor box

00:38:54,999 --> 00:39:02,710
I assume it's the same with HDFS yeah so

00:39:00,789 --> 00:39:06,039
so all these frameworks are based on the

00:39:02,710 --> 00:39:11,559
US comments which has so basically this

00:39:06,039 --> 00:39:12,970
US Commons has is an SDK to launch and

00:39:11,559 --> 00:39:15,369
manage your stateful services

00:39:12,970 --> 00:39:17,380
the only thing over there is that they

00:39:15,369 --> 00:39:20,410
had support for local root and mount

00:39:17,380 --> 00:39:22,779
discs so we've we've only modified that

00:39:20,410 --> 00:39:26,589
framework to be able to support Botox

00:39:22,779 --> 00:39:28,749
volumes so and we've just rebuilt the

00:39:26,589 --> 00:39:30,700
we've we change the parameters that you

00:39:28,749 --> 00:39:33,700
pass in to the yamen spec so that they

00:39:30,700 --> 00:39:35,710
can use the port works volumes so it's

00:39:33,700 --> 00:39:40,119
basically one SDK and we've built these

00:39:35,710 --> 00:39:41,799
for services on top of that SDK you

00:39:40,119 --> 00:39:44,349
answer my question but all the

00:39:41,799 --> 00:39:47,440
functionality that is to the framework

00:39:44,349 --> 00:39:52,420
is still in your in your framework isn't

00:39:47,440 --> 00:39:54,069
it like Kerberos for HDFS or yes all

00:39:52,420 --> 00:40:03,099
that support that's then the base DCOs

00:39:54,069 --> 00:40:06,150
commons is also in this yes all right

00:40:03,099 --> 00:40:06,150
any other questions

00:40:08,210 --> 00:40:12,950
all right so you can always visit us at

00:40:11,329 --> 00:40:14,270
at our booth if you have any more

00:40:12,950 --> 00:40:17,359
questions you can always go to our

00:40:14,270 --> 00:40:19,880
website to like I mentioned all our

00:40:17,359 --> 00:40:21,349
services are in the catalog so you can

00:40:19,880 --> 00:40:24,020
always just search for port works and

00:40:21,349 --> 00:40:25,609
download and install them you can also

00:40:24,020 --> 00:40:28,609
visit our Doc's website if you want to

00:40:25,609 --> 00:40:31,190
get started we have a free px developer

00:40:28,609 --> 00:40:33,290
version we also have a px Enterprise

00:40:31,190 --> 00:40:35,480
version that's feed which is which has

00:40:33,290 --> 00:40:38,750
all the features enabled and is free to

00:40:35,480 --> 00:40:40,700
try for three days you can contact us at

00:40:38,750 --> 00:40:44,900
info at port works.com for more

00:40:40,700 --> 00:40:48,349
information all right thank you

00:40:44,900 --> 00:40:48,349

YouTube URL: https://www.youtube.com/watch?v=FwKf2Xem-kM


