Title: Disaster Recovery with a Distributed Database - Max Neunhöffer & Jörg Schad, Mesosphere, Inc.
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Disaster Recovery with a Distributed Database - Max Neunhöffer & Jörg Schad, Mesosphere, Inc.

Enterprises large and small want to have a disaster recovery plan at the ready when they are running distributed databases. The simplest such setup uses two database clusters in two different data centers with asynchronous replication of all updates. However, already this seemingly simple approach causes considerable head scratching for architects and developers of a distributed data store, considering fault tolerance, network failures, automatic failover and write load spikes.

This talk explains how ArangoDB implements asynchronous DC to DC replication between clusters using the Kafka message broker on both sides together with an incremental replication protocol and automatic write ahead log tailing. Apache Mesos plays a vital role in managing the deployment, managing resources, improve utilization, ensure scalability and ensuring liveness of the whole system.

About Max Neunhöffer
Max Neunhöffer is a mathematician turned database developer. In his academic career he has worked for 16 years on the development and implementation of new algorithms in computer algebra. During this time he has juggled a lot with mathematical big data like group orbits containing trillions of points. Recently he has returned from St Andrews to Germany, has shifted his focus to NoSQL databases, and now helps to develop ArangoDB. He has spoken at international conferences including Strata London.

About Jörg Schad
Jörg is a software engineer at Mesosphere in Hamburg. In his previous life he implemented distributed and in memory databases and conducted research in the Hadoop and Cloud area. His speaking experience includes various Meetups, international conferences, and lecture halls.
Captions: 
	00:00:00,030 --> 00:00:04,680
thank you so much first of all it really

00:00:02,340 --> 00:00:06,540
feels great like this morning I was

00:00:04,680 --> 00:00:08,639
first up on stage and tonight I get to

00:00:06,540 --> 00:00:10,440
be last on the stage that's really cool

00:00:08,639 --> 00:00:12,480
Thanks that you're all still here for

00:00:10,440 --> 00:00:16,440
the last session no not the last session

00:00:12,480 --> 00:00:18,330
but my last session so what I want to

00:00:16,440 --> 00:00:20,760
talk about little is about yea

00:00:18,330 --> 00:00:22,769
distributed data so we talked a lot

00:00:20,760 --> 00:00:25,500
about persistence we talked a lot about

00:00:22,769 --> 00:00:27,449
how to store different data sets so for

00:00:25,500 --> 00:00:29,429
example when we had C smack workshop

00:00:27,449 --> 00:00:32,009
today we had a lot of discussions about

00:00:29,429 --> 00:00:35,360
what it likes the best data models to

00:00:32,009 --> 00:00:38,100
fix and where to store my data and

00:00:35,360 --> 00:00:40,530
usually this talk I would have mainly

00:00:38,100 --> 00:00:42,390
been given by max from a Rango dB

00:00:40,530 --> 00:00:44,910
because we gonna be talking about a

00:00:42,390 --> 00:00:47,250
Rango DB he unfortunately couldn't make

00:00:44,910 --> 00:00:49,379
it and he asked me to do it and a Rango

00:00:47,250 --> 00:00:50,879
DB is one of those exceptions where I'm

00:00:49,379 --> 00:00:52,530
actually more than willing to do it

00:00:50,879 --> 00:00:55,469
because there have been really great

00:00:52,530 --> 00:00:57,180
partners around mesas for a long time so

00:00:55,469 --> 00:00:59,730
they really helped us implements he

00:00:57,180 --> 00:01:02,399
persisted volumes they really helped us

00:00:59,730 --> 00:01:04,949
to always test new mesas features as

00:01:02,399 --> 00:01:07,920
kind of first guinea pigs with their

00:01:04,949 --> 00:01:12,420
framework so this is why I'm standing

00:01:07,920 --> 00:01:14,939
here today and not max so max he is the

00:01:12,420 --> 00:01:18,180
senior Software Architect at a Rango DB

00:01:14,939 --> 00:01:20,130
he mostly wrote the data center - data

00:01:18,180 --> 00:01:23,640
center replication we will look at here

00:01:20,130 --> 00:01:25,700
right now and is I was introduced I'm a

00:01:23,640 --> 00:01:30,390
distributed systems engineer at

00:01:25,700 --> 00:01:32,340
mesosphere Y distributed data so as we

00:01:30,390 --> 00:01:33,930
are distributing our infrastructure and

00:01:32,340 --> 00:01:36,299
this is kind of see promise of Apache

00:01:33,930 --> 00:01:39,689
Miso's we also have to distribute our

00:01:36,299 --> 00:01:41,490
data so as we're scaling and this is

00:01:39,689 --> 00:01:44,430
kind of one of the reasons why we need

00:01:41,490 --> 00:01:46,770
so many so many large clusters is that

00:01:44,430 --> 00:01:49,799
we have a lot of web cloud applications

00:01:46,770 --> 00:01:53,070
look at Twitter look at Netflix which

00:01:49,799 --> 00:01:55,219
are actually generating data and one of

00:01:53,070 --> 00:01:58,259
the realizations was said so it's exact

00:01:55,219 --> 00:02:01,829
existing monolithic systems may it be a

00:01:58,259 --> 00:02:04,950
large sa P instance may it be a large

00:02:01,829 --> 00:02:07,350
Oracle instance do not really scope Val

00:02:04,950 --> 00:02:09,929
and do not scale well choose this kind

00:02:07,350 --> 00:02:12,599
of workloads and this is why we actually

00:02:09,929 --> 00:02:14,849
seen that many of the companies large

00:02:12,599 --> 00:02:17,310
companies like face

00:02:14,849 --> 00:02:18,959
have been developing their own solutions

00:02:17,310 --> 00:02:23,489
for storing data in a distributed

00:02:18,959 --> 00:02:26,610
fashion and yes this is basically the

00:02:23,489 --> 00:02:28,890
realization that those new tools which

00:02:26,610 --> 00:02:31,680
those company have been developing

00:02:28,890 --> 00:02:33,920
they're actually not on a single box so

00:02:31,680 --> 00:02:37,230
what they try to do is basically

00:02:33,920 --> 00:02:38,910
actually have different data models

00:02:37,230 --> 00:02:41,700
because if I have a traditional

00:02:38,910 --> 00:02:43,860
relational database whenever I scale

00:02:41,700 --> 00:02:47,010
that I'll probably hit certain limits

00:02:43,860 --> 00:02:48,750
and usually the guarantees which a

00:02:47,010 --> 00:02:50,250
relational data store gives me they're

00:02:48,750 --> 00:02:52,470
really great but they're also

00:02:50,250 --> 00:02:57,660
restricting the model in some kind of

00:02:52,470 --> 00:03:00,480
way so those big relational databases

00:02:57,660 --> 00:03:03,000
have been made for a different era don't

00:03:00,480 --> 00:03:05,400
understand me wrong we still need

00:03:03,000 --> 00:03:08,160
relational databases but we might not

00:03:05,400 --> 00:03:10,590
only need relational databases we might

00:03:08,160 --> 00:03:14,250
want to talk about different data stores

00:03:10,590 --> 00:03:18,690
as well and the Clipper doesn't quite

00:03:14,250 --> 00:03:21,359
work so this is why we actually nowadays

00:03:18,690 --> 00:03:23,609
talk about so-called multi model

00:03:21,359 --> 00:03:26,220
databases and what does it actually mean

00:03:23,609 --> 00:03:29,340
I can't have different models for my

00:03:26,220 --> 00:03:31,470
data I can store it for example in a

00:03:29,340 --> 00:03:35,040
relational database I can store it in a

00:03:31,470 --> 00:03:39,299
graph store and whatever is popping up

00:03:35,040 --> 00:03:41,160
here we'll see that in a second I can

00:03:39,299 --> 00:03:44,099
store it as a graph I can store it in a

00:03:41,160 --> 00:03:46,349
document store and all those data models

00:03:44,099 --> 00:03:48,269
they have several adduce cases if I'm

00:03:46,349 --> 00:03:50,730
restricted to my only relational

00:03:48,269 --> 00:03:52,950
database what I have to do is I have to

00:03:50,730 --> 00:03:55,799
map all of those different data models

00:03:52,950 --> 00:04:00,540
to a relational model meaning I have to

00:03:55,799 --> 00:04:02,280
break up my Jason I have to break up my

00:04:00,540 --> 00:04:05,069
key values and generate like a very

00:04:02,280 --> 00:04:06,690
simple relation through that and also my

00:04:05,069 --> 00:04:08,280
graph there are multiple ways of

00:04:06,690 --> 00:04:10,440
representing graphs and relational

00:04:08,280 --> 00:04:13,829
databases but usually they're rather

00:04:10,440 --> 00:04:16,680
inefficient so it doesn't actually scale

00:04:13,829 --> 00:04:20,370
out in the way we would need it for this

00:04:16,680 --> 00:04:23,099
new era of web applications and also

00:04:20,370 --> 00:04:24,840
those traditional databases as I were

00:04:23,099 --> 00:04:26,400
single node instances usually they

00:04:24,840 --> 00:04:27,720
haven't been designed with too much

00:04:26,400 --> 00:04:30,750
resilience in

00:04:27,720 --> 00:04:34,260
or automatic failover that they can

00:04:30,750 --> 00:04:40,110
actually deal with failing tasks and I'm

00:04:34,260 --> 00:04:47,150
just gonna see why my okay why this

00:04:40,110 --> 00:04:50,720
keeps on popping up and this is exactly

00:04:47,150 --> 00:04:53,790
if I'm a database vendor and I have to

00:04:50,720 --> 00:04:56,670
write my distributed database I usually

00:04:53,790 --> 00:04:58,980
I want to focus on something as oh yeah

00:04:56,670 --> 00:05:01,230
data storing they being a database and

00:04:58,980 --> 00:05:03,120
all those new requirements that I

00:05:01,230 --> 00:05:05,250
actually be able to spread out across a

00:05:03,120 --> 00:05:07,590
large cluster that I have to be able to

00:05:05,250 --> 00:05:09,840
detect failures and I have to restart

00:05:07,590 --> 00:05:11,880
and so on and so on this is really more

00:05:09,840 --> 00:05:14,130
an annoyance because it's not my core

00:05:11,880 --> 00:05:16,860
business I want to focus on and this is

00:05:14,130 --> 00:05:18,960
why I wrangle DB was one of the first to

00:05:16,860 --> 00:05:21,290
actually write their own really

00:05:18,960 --> 00:05:23,669
impressive missus framework and

00:05:21,290 --> 00:05:26,960
following this approach of integrating

00:05:23,669 --> 00:05:29,490
in kind of orchestration tools which

00:05:26,960 --> 00:05:32,400
don't require anymore is that I

00:05:29,490 --> 00:05:34,710
implement everything by myself so as

00:05:32,400 --> 00:05:36,900
mentioned Arango DB it's a native multi

00:05:34,710 --> 00:05:39,480
model database meaning I can store

00:05:36,900 --> 00:05:41,700
adjacent data in a native representation

00:05:39,480 --> 00:05:44,760
and it can store graph data in a native

00:05:41,700 --> 00:05:47,910
representation and also key value data

00:05:44,760 --> 00:05:49,919
all of these I can actually store and I

00:05:47,910 --> 00:05:52,320
can interlink so for example I could

00:05:49,919 --> 00:05:55,200
have a graph where each node is actually

00:05:52,320 --> 00:05:57,210
adjacent document and that makes

00:05:55,200 --> 00:06:00,080
actually then also crewing all those

00:05:57,210 --> 00:06:04,560
this distributed structure rather nicely

00:06:00,080 --> 00:06:07,380
they have their own language to enable

00:06:04,560 --> 00:06:10,169
that to create across all those

00:06:07,380 --> 00:06:12,660
different models and they're actually

00:06:10,169 --> 00:06:14,820
quite scalable so they can all of this

00:06:12,660 --> 00:06:16,380
can run in a distributed fashion and I

00:06:14,820 --> 00:06:21,960
can actually also query across

00:06:16,380 --> 00:06:23,940
distributed graphs if we look at a model

00:06:21,960 --> 00:06:25,890
and maybe this helps us to understand a

00:06:23,940 --> 00:06:29,030
little better why we actually need those

00:06:25,890 --> 00:06:33,750
different models in a relational world

00:06:29,030 --> 00:06:37,140
my tables look here as on the as on the

00:06:33,750 --> 00:06:39,030
latter left here we're half like columns

00:06:37,140 --> 00:06:41,550
of data and each column has like the

00:06:39,030 --> 00:06:43,440
same data type I also can have something

00:06:41,550 --> 00:06:46,530
like graphs we have connections between

00:06:43,440 --> 00:06:48,540
certain entities and maybe the last

00:06:46,530 --> 00:06:52,860
thing if we talk about different values

00:06:48,540 --> 00:06:54,870
is an here talking about a document

00:06:52,860 --> 00:06:57,810
structure so a document structure it

00:06:54,870 --> 00:06:59,970
might look similar to the relational

00:06:57,810 --> 00:07:02,280
structure on the left but it's different

00:06:59,970 --> 00:07:04,680
in that way that actually the schema of

00:07:02,280 --> 00:07:07,080
each document can be quite different so

00:07:04,680 --> 00:07:09,990
whereas in a relational model I really

00:07:07,080 --> 00:07:12,360
have the same schema for each row in a

00:07:09,990 --> 00:07:15,090
document I have a quite different schema

00:07:12,360 --> 00:07:19,170
for each of the documents which could be

00:07:15,090 --> 00:07:21,450
rows in that document and so this is

00:07:19,170 --> 00:07:23,340
actually why different models make sense

00:07:21,450 --> 00:07:25,800
and we could talk about the other ones

00:07:23,340 --> 00:07:27,540
like time series which is really good if

00:07:25,800 --> 00:07:30,540
I have a lot of sensor data which looks

00:07:27,540 --> 00:07:33,900
very similar columnar data stores if I

00:07:30,540 --> 00:07:35,760
really have repeating same data value

00:07:33,900 --> 00:07:38,970
and I want to have aggregate crews

00:07:35,760 --> 00:07:40,350
across a certain column or key value

00:07:38,970 --> 00:07:43,170
stories if I have a very simple

00:07:40,350 --> 00:07:47,090
representation but all of them's I have

00:07:43,170 --> 00:07:50,250
say use cases and they're helping me to

00:07:47,090 --> 00:07:52,560
natively match my data to my data store

00:07:50,250 --> 00:07:54,600
because otherwise if I all have to match

00:07:52,560 --> 00:07:56,520
it into one datastore I always have to

00:07:54,600 --> 00:07:59,370
rewrite data which first of all adds

00:07:56,520 --> 00:08:02,580
logic and secondly it adds a lot of

00:07:59,370 --> 00:08:06,960
complexity to my datastore and usually

00:08:02,580 --> 00:08:10,440
duplicates data so the native approach

00:08:06,960 --> 00:08:13,980
is for a rango DB to take both documents

00:08:10,440 --> 00:08:15,660
or documents graphs and key value stores

00:08:13,980 --> 00:08:18,450
and all put it in like one big database

00:08:15,660 --> 00:08:20,820
make it available why a one query

00:08:18,450 --> 00:08:22,920
language and one deployment artifact so

00:08:20,820 --> 00:08:25,380
I don't have to install a document

00:08:22,920 --> 00:08:27,750
database plus a graph database plus a

00:08:25,380 --> 00:08:30,510
key value store and then have curious in

00:08:27,750 --> 00:08:36,660
between them I can create a similar

00:08:30,510 --> 00:08:38,490
cohesive fashion how does it look if I

00:08:36,660 --> 00:08:42,450
deploys it and we'll see how that looks

00:08:38,490 --> 00:08:45,420
on DCOs in just a second so I have DB

00:08:42,450 --> 00:08:47,400
servers which are really responsible for

00:08:45,420 --> 00:08:49,800
storing the data and then have

00:08:47,400 --> 00:08:52,740
coordinators which are responsible for

00:08:49,800 --> 00:08:54,840
coordinating those queries across all

00:08:52,740 --> 00:08:57,420
notes so I can

00:08:54,840 --> 00:09:00,660
have multiple coordinators and a number

00:08:57,420 --> 00:09:02,730
of DB servers and this is the way we can

00:09:00,660 --> 00:09:05,190
scale a coordinator has certain overhead

00:09:02,730 --> 00:09:06,810
for running a query so if I see that my

00:09:05,190 --> 00:09:10,650
career workload is going down

00:09:06,810 --> 00:09:12,750
I can deploy more coordinators if my

00:09:10,650 --> 00:09:14,850
storage space is running out of my

00:09:12,750 --> 00:09:19,230
compute capacity I usually would add

00:09:14,850 --> 00:09:22,320
more DB servers there is an agency with

00:09:19,230 --> 00:09:24,630
agency you can also replace it in your

00:09:22,320 --> 00:09:27,180
head with zookeeper @cd but it's your

00:09:24,630 --> 00:09:29,450
own implementation which helps to

00:09:27,180 --> 00:09:32,010
coordinate all this so you still need

00:09:29,450 --> 00:09:36,270
distributed system you still need some

00:09:32,010 --> 00:09:38,190
kind of majority decisions and overall

00:09:36,270 --> 00:09:40,110
way to persistently store certain

00:09:38,190 --> 00:09:42,450
decisions so for example where data is

00:09:40,110 --> 00:09:47,550
stored or who is responsible right now

00:09:42,450 --> 00:09:49,530
who is owning that data so as we've seen

00:09:47,550 --> 00:09:52,380
on this slide this is a distributed set

00:09:49,530 --> 00:09:54,030
up even deploying that on missiles

00:09:52,380 --> 00:09:56,070
whereas missus makes it already much

00:09:54,030 --> 00:10:00,150
much simpler of writing something like

00:09:56,070 --> 00:10:02,790
that it makes it much easier it's still

00:10:00,150 --> 00:10:05,010
very challenging so especially getting

00:10:02,790 --> 00:10:06,930
the scheduling logic right so where do

00:10:05,010 --> 00:10:08,730
you want to deploy which task and what

00:10:06,930 --> 00:10:11,100
do you do in certain failure scenarios

00:10:08,730 --> 00:10:12,810
failure scenarios they are rather easy

00:10:11,100 --> 00:10:14,910
if we're talking about stateless

00:10:12,810 --> 00:10:17,400
services because if my nginx container

00:10:14,910 --> 00:10:20,370
fails I'll usually just restart it great

00:10:17,400 --> 00:10:22,530
but if my Arango DB database server

00:10:20,370 --> 00:10:26,030
fails I actually need to recover data

00:10:22,530 --> 00:10:30,020
and I need to make sure that my replica

00:10:26,030 --> 00:10:33,120
number of replicas is consistent again

00:10:30,020 --> 00:10:35,160
and so the first iteration of this

00:10:33,120 --> 00:10:39,090
Arango DB framework consisted of over

00:10:35,160 --> 00:10:40,980
5,000 lines of C++ code and with that

00:10:39,090 --> 00:10:42,900
number just keep in mind that was the

00:10:40,980 --> 00:10:45,860
first implementation of a framework

00:10:42,900 --> 00:10:49,410
really using persistent volumes and

00:10:45,860 --> 00:10:52,740
reservations and Mase's so it also took

00:10:49,410 --> 00:10:55,710
us some time to really experience or get

00:10:52,740 --> 00:10:57,480
a feeling for how it should be used so

00:10:55,710 --> 00:10:59,430
probably if we would rewrite it from

00:10:57,480 --> 00:11:01,770
scratch it would be a little smaller by

00:10:59,430 --> 00:11:04,950
now but still in the orders of thousand

00:11:01,770 --> 00:11:07,260
lines of code so what this framework

00:11:04,950 --> 00:11:08,700
does it handles deployments it deals

00:11:07,260 --> 00:11:12,510
with persistent volumes

00:11:08,700 --> 00:11:13,230
and reservations and does take care

00:11:12,510 --> 00:11:16,530
failover

00:11:13,230 --> 00:11:22,020
and it also enables up and scaling down

00:11:16,530 --> 00:11:25,440
which I personally like a lot then just

00:11:22,020 --> 00:11:28,080
to give you feeling for how complex this

00:11:25,440 --> 00:11:32,250
framework is so this is the state

00:11:28,080 --> 00:11:34,140
diagram for creating reservations so if

00:11:32,250 --> 00:11:37,610
you want to create a new persistent

00:11:34,140 --> 00:11:39,840
volume you first you try to reserve and

00:11:37,610 --> 00:11:42,510
once you have reserved you try to

00:11:39,840 --> 00:11:45,630
persist and so on and so on and it each

00:11:42,510 --> 00:11:49,410
layer it might actually fail so only the

00:11:45,630 --> 00:11:51,570
way to get a real reservation to get a

00:11:49,410 --> 00:11:54,720
persistent volume which can be used by

00:11:51,570 --> 00:11:58,410
the framework actually requires rather

00:11:54,720 --> 00:12:00,240
complex code here and this is as

00:11:58,410 --> 00:12:03,180
mentioned this morning this is exactly

00:12:00,240 --> 00:12:05,340
the idea for the DC US SDK because this

00:12:03,180 --> 00:12:07,800
pattern it's actually it's the same for

00:12:05,340 --> 00:12:10,020
almost any framework which wants to use

00:12:07,800 --> 00:12:12,120
persistent volumes they all have to go

00:12:10,020 --> 00:12:15,290
through the same pattern so the idea for

00:12:12,120 --> 00:12:18,660
the DC OS SDK is to kind of pull out

00:12:15,290 --> 00:12:22,950
this logic and make it available as a

00:12:18,660 --> 00:12:25,860
library or as a generator and if you're

00:12:22,950 --> 00:12:28,110
using the defaults so if you if that's

00:12:25,860 --> 00:12:31,050
exactly what you want you actually you

00:12:28,110 --> 00:12:33,120
only have to write your llamo similar to

00:12:31,050 --> 00:12:36,270
writing a docker compose file you don't

00:12:33,120 --> 00:12:38,100
have to write any real Java code and you

00:12:36,270 --> 00:12:40,770
also don't have to be an app expert you

00:12:38,100 --> 00:12:44,040
simply say I want to have n number of

00:12:40,770 --> 00:12:47,370
database servers up and running if you

00:12:44,040 --> 00:12:49,830
need custom failover logic or in case of

00:12:47,370 --> 00:12:53,880
a regular DB custom logic to scale up

00:12:49,830 --> 00:12:56,190
and scale down you might need to write a

00:12:53,880 --> 00:12:58,890
little code so you can still use your

00:12:56,190 --> 00:13:00,870
overall llamo to generate your scheduler

00:12:58,890 --> 00:13:03,150
but you might override certain parts

00:13:00,870 --> 00:13:05,610
with your own custom logic so for

00:13:03,150 --> 00:13:08,310
example for scaling down and this isn't

00:13:05,610 --> 00:13:10,950
yet implemented in the SDK is this is

00:13:08,310 --> 00:13:13,560
one for them one of the biggest missing

00:13:10,950 --> 00:13:16,470
steps which they can't get working on

00:13:13,560 --> 00:13:19,940
the SDK yet is the process of scaling

00:13:16,470 --> 00:13:22,050
down if I'm scaling down is first

00:13:19,940 --> 00:13:23,970
stateful service I

00:13:22,050 --> 00:13:26,430
we have to make sure that I may migrate

00:13:23,970 --> 00:13:28,050
the data first so I first have to clear

00:13:26,430 --> 00:13:30,870
out the database server I have to move

00:13:28,050 --> 00:13:32,880
the data somewhere else and once the

00:13:30,870 --> 00:13:35,640
servers empty there are no connections

00:13:32,880 --> 00:13:37,860
no data anymore or no data which needs

00:13:35,640 --> 00:13:41,310
to be replicated I can then shut it down

00:13:37,860 --> 00:13:45,269
a wrangle Yui has that implemented for

00:13:41,310 --> 00:13:50,370
their own framework but this is not yet

00:13:45,269 --> 00:13:51,720
easy with the SDK so we felt we have to

00:13:50,370 --> 00:13:53,490
ride on would have to write a lot of

00:13:51,720 --> 00:13:58,230
code if we wanted to implement that in

00:13:53,490 --> 00:14:00,000
the SDK and so for now Arango DB is

00:13:58,230 --> 00:14:01,890
actually still down here using their own

00:14:00,000 --> 00:14:04,320
scheduler but they're really trying to

00:14:01,890 --> 00:14:07,529
move up the stack because it simplifies

00:14:04,320 --> 00:14:10,079
things so as said for now and also they

00:14:07,529 --> 00:14:13,529
started before the SDK was around the

00:14:10,079 --> 00:14:16,470
helps your own scheduler as soon as we

00:14:13,529 --> 00:14:18,300
feel it has enough we have production

00:14:16,470 --> 00:14:21,120
quality similar to the existing

00:14:18,300 --> 00:14:27,450
framework they will probably move up the

00:14:21,120 --> 00:14:30,329
stack here replication so even though we

00:14:27,450 --> 00:14:33,180
support failover by having persistent

00:14:30,329 --> 00:14:35,610
volumes by being able to shut down an

00:14:33,180 --> 00:14:38,279
individual database server here this

00:14:35,610 --> 00:14:42,360
still will fail if an entire datacenter

00:14:38,279 --> 00:14:44,670
is failing so a common problem is how

00:14:42,360 --> 00:14:48,300
can we actually replicate data across

00:14:44,670 --> 00:14:51,329
two data centers and so we have

00:14:48,300 --> 00:14:55,470
different options for that the first one

00:14:51,329 --> 00:14:57,540
or the yeah the first level is simply to

00:14:55,470 --> 00:15:01,800
use a replication as an off-site backup

00:14:57,540 --> 00:15:04,649
so I simply I create a database dump

00:15:01,800 --> 00:15:07,320
every every hour and I write it to an

00:15:04,649 --> 00:15:10,470
external data center this is one option

00:15:07,320 --> 00:15:13,199
and one goal basically have an off-site

00:15:10,470 --> 00:15:16,560
backup of my data the second one could

00:15:13,199 --> 00:15:18,779
be for disaster recovery which actually

00:15:16,560 --> 00:15:21,019
would mean that my second secondary

00:15:18,779 --> 00:15:26,640
datacenter can take over that workload

00:15:21,019 --> 00:15:29,190
if it needs to and the last level and

00:15:26,640 --> 00:15:31,709
the most complicated one is actually to

00:15:29,190 --> 00:15:34,220
have two active data centers which can

00:15:31,709 --> 00:15:35,760
then also be used for geolocation

00:15:34,220 --> 00:15:37,800
meaning

00:15:35,760 --> 00:15:41,670
I can actually get requests to both

00:15:37,800 --> 00:15:44,160
datacenters so if we look down here the

00:15:41,670 --> 00:15:46,290
first approach basically means dump your

00:15:44,160 --> 00:15:46,920
data to the remote data center somewhere

00:15:46,290 --> 00:15:49,260
else

00:15:46,920 --> 00:15:51,270
the second approach means you have

00:15:49,260 --> 00:15:54,180
exactly this picture where you have two

00:15:51,270 --> 00:15:56,070
requests coming to one datacenter but if

00:15:54,180 --> 00:15:58,740
that datacenter fails you can actually

00:15:56,070 --> 00:16:01,380
buy a load balancer by some kind of

00:15:58,740 --> 00:16:03,300
switch redirect the request to the

00:16:01,380 --> 00:16:05,820
second cluster and there might be a

00:16:03,300 --> 00:16:08,850
small lag some minimal data might have

00:16:05,820 --> 00:16:12,930
been lost but the applications can keep

00:16:08,850 --> 00:16:16,290
on running as before in the first

00:16:12,930 --> 00:16:18,930
implementation for rango DB what they

00:16:16,290 --> 00:16:21,900
focused on is the first two parts as

00:16:18,930 --> 00:16:24,240
mentioned offering geolocation services

00:16:21,900 --> 00:16:25,680
this actually requires a two-way

00:16:24,240 --> 00:16:27,630
communication and two-way

00:16:25,680 --> 00:16:29,990
synchronization with the clusters

00:16:27,630 --> 00:16:32,910
because all the sudden you have two

00:16:29,990 --> 00:16:36,750
masters up and running which can both

00:16:32,910 --> 00:16:39,510
accept requests so for now they simply

00:16:36,750 --> 00:16:44,340
decided is to have a secondary cluster

00:16:39,510 --> 00:16:46,800
as a fallback and but only you can only

00:16:44,340 --> 00:16:53,160
send requests and write to one of the

00:16:46,800 --> 00:16:54,810
clusters at at the same time so for is

00:16:53,160 --> 00:16:57,870
this first iteration the goal is

00:16:54,810 --> 00:17:00,780
actually to run database clusters in

00:16:57,870 --> 00:17:02,820
both datacenters or in more if you want

00:17:00,780 --> 00:17:05,310
to but the current customers they are

00:17:02,820 --> 00:17:09,480
doing it in two and then replicate data

00:17:05,310 --> 00:17:11,520
automatically between them and one one

00:17:09,480 --> 00:17:15,480
of the data centers fails they can

00:17:11,520 --> 00:17:19,980
switch over so this is a goal of what we

00:17:15,480 --> 00:17:24,620
are presenting here today the v1

00:17:19,980 --> 00:17:28,140
application is to basically take the

00:17:24,620 --> 00:17:30,330
default Arango DB clusters including all

00:17:28,140 --> 00:17:32,640
user settings and replicate them to the

00:17:30,330 --> 00:17:35,670
other datacenter and they actually

00:17:32,640 --> 00:17:39,180
already have an existing replication API

00:17:35,670 --> 00:17:42,960
called Arango sync which used to be to

00:17:39,180 --> 00:17:44,250
some clusters if you've restarted or you

00:17:42,960 --> 00:17:47,310
wanted to switch between different

00:17:44,250 --> 00:17:49,620
versions for example that tool was used

00:17:47,310 --> 00:17:51,780
and what's that actually does

00:17:49,620 --> 00:17:54,570
in this new scenario it uses Kafka on

00:17:51,780 --> 00:17:58,140
both sides and Arango sink on the one

00:17:54,570 --> 00:18:01,770
side will write all changes into the

00:17:58,140 --> 00:18:03,750
Kafka queue and on the other side the

00:18:01,770 --> 00:18:08,059
other wrangle simp will read it out of

00:18:03,750 --> 00:18:10,710
it and apply it to the other cluster and

00:18:08,059 --> 00:18:12,960
using Kafka there has multiple

00:18:10,710 --> 00:18:15,630
advantages first of all we can also

00:18:12,960 --> 00:18:17,820
handle spike workloads in datacenter a

00:18:15,630 --> 00:18:20,730
so dangerous enter a being the active

00:18:17,820 --> 00:18:23,220
one so and if some point the workload

00:18:20,730 --> 00:18:26,429
gets too much Arango soon can still dump

00:18:23,220 --> 00:18:28,140
it but it's just too quick to apply it

00:18:26,429 --> 00:18:30,450
in real time on the other cluster

00:18:28,140 --> 00:18:34,500
actually Kafka can serve as kind of a

00:18:30,450 --> 00:18:38,040
buffer and help us reduce that risk of

00:18:34,500 --> 00:18:41,460
losing data and also Kafka already comes

00:18:38,040 --> 00:18:44,520
with between datacenter replication so

00:18:41,460 --> 00:18:46,309
it's helpful because it's already been

00:18:44,520 --> 00:18:50,550
tested between different data centers

00:18:46,309 --> 00:18:52,950
and similarly as we see a spike

00:18:50,550 --> 00:18:54,929
workloads in general it helps to

00:18:52,950 --> 00:18:57,240
distribute the pressure because it can

00:18:54,929 --> 00:19:02,400
also give me back pressure to the other

00:18:57,240 --> 00:19:03,770
side and important for advanced setup is

00:19:02,400 --> 00:19:06,420
I actually have an encrypted

00:19:03,770 --> 00:19:08,880
communication between the clusters so

00:19:06,420 --> 00:19:11,070
I'm not sending any unencrypted set

00:19:08,880 --> 00:19:15,360
potentially sensitive information across

00:19:11,070 --> 00:19:18,210
the internet so this is basically than

00:19:15,360 --> 00:19:22,830
how it looks like so I have here Arango

00:19:18,210 --> 00:19:25,559
sync which is aware of each other and it

00:19:22,830 --> 00:19:28,410
basically uses Kafka underneath to

00:19:25,559 --> 00:19:30,960
synchronize that data so in datacenter a

00:19:28,410 --> 00:19:36,330
they just Center a usually being C

00:19:30,960 --> 00:19:38,820
active one it writes the data into it

00:19:36,330 --> 00:19:41,309
dumps the data into wire Arango sink

00:19:38,820 --> 00:19:43,950
into the Kafka Q and the Kafka queue on

00:19:41,309 --> 00:19:45,660
the other side will read it Arango sync

00:19:43,950 --> 00:19:49,110
will then apply it to the database

00:19:45,660 --> 00:19:54,270
servers and the second cluster is then

00:19:49,110 --> 00:19:59,340
up-to-date with the first one in the

00:19:54,270 --> 00:20:03,480
first v1 implementation this is

00:19:59,340 --> 00:20:06,180
asynchronous so if data is written he

00:20:03,480 --> 00:20:08,700
it's not immediately applied here and if

00:20:06,180 --> 00:20:11,070
I have a transaction for example a user

00:20:08,700 --> 00:20:13,530
writing data the user will be told your

00:20:11,070 --> 00:20:16,160
data is written before it's necessarily

00:20:13,530 --> 00:20:20,250
synced to the second data center and

00:20:16,160 --> 00:20:23,070
this mostly has to do with the overhead

00:20:20,250 --> 00:20:25,950
because Kafka adds a certain latency and

00:20:23,070 --> 00:20:28,950
it would really reduce the performance

00:20:25,950 --> 00:20:31,350
of a Wrangler DB if they would be

00:20:28,950 --> 00:20:34,590
waiting for each each of those events to

00:20:31,350 --> 00:20:37,770
be soon and secondary in many

00:20:34,590 --> 00:20:39,660
applications a little tiny bit of data

00:20:37,770 --> 00:20:43,800
loss doesn't matter too much so if I

00:20:39,660 --> 00:20:46,050
lose like a few events in many of their

00:20:43,800 --> 00:20:48,750
customers scenario it doesn't matter too

00:20:46,050 --> 00:20:53,520
much just I have to be able to keep on

00:20:48,750 --> 00:20:55,350
running so see most the most important

00:20:53,520 --> 00:20:57,450
goal here is actually to keep the data

00:20:55,350 --> 00:21:00,480
center up and running and not so much to

00:20:57,450 --> 00:21:07,800
prevent the latest mini tiny bit of data

00:21:00,480 --> 00:21:10,830
loss and the other disadvantages said

00:21:07,800 --> 00:21:14,040
basically I have a complete duplicated

00:21:10,830 --> 00:21:16,590
data center which usually might include

00:21:14,040 --> 00:21:19,260
up to somewhere around three to five

00:21:16,590 --> 00:21:23,040
nodes but I'm not using those notes for

00:21:19,260 --> 00:21:25,920
any performance against for my users if

00:21:23,040 --> 00:21:28,320
I had a hot hot migration and double

00:21:25,920 --> 00:21:30,570
master set up so if I could actually

00:21:28,320 --> 00:21:32,670
create both of them at the same time I

00:21:30,570 --> 00:21:36,440
could split up my user workload for

00:21:32,670 --> 00:21:36,440
example between those two data centers

00:21:37,190 --> 00:21:42,600
as we're talking about

00:21:39,690 --> 00:21:45,060
Arango DB on missiles and a wrangler DB

00:21:42,600 --> 00:21:47,940
on DCOs let's briefly talk about the

00:21:45,060 --> 00:21:51,480
replication between two data centers or

00:21:47,940 --> 00:21:54,930
the possibility of running TCO s slash

00:21:51,480 --> 00:21:58,380
meters across two data centers so what

00:21:54,930 --> 00:22:00,990
actually works rather well is to

00:21:58,380 --> 00:22:04,290
distribute agents across regions across

00:22:00,990 --> 00:22:06,870
availability zones what is a lot harder

00:22:04,290 --> 00:22:09,780
so what I can easily set up is I have

00:22:06,870 --> 00:22:12,750
all my masters in one availability zone

00:22:09,780 --> 00:22:15,030
and then I have additional agents and

00:22:12,750 --> 00:22:17,280
different data centers what's kind of a

00:22:15,030 --> 00:22:19,620
little more challenging to set up is to

00:22:17,280 --> 00:22:26,070
distributes the masters and this is

00:22:19,620 --> 00:22:30,210
mainly due to to the fact that zookeeper

00:22:26,070 --> 00:22:34,320
doesn't really handle high latency links

00:22:30,210 --> 00:22:37,410
very well so what some people are doing

00:22:34,320 --> 00:22:39,480
and it works rather well is to

00:22:37,410 --> 00:22:44,190
distribute them inside an availability

00:22:39,480 --> 00:22:46,980
zone in Amazon terms so kind of near

00:22:44,190 --> 00:22:50,640
local not too high latency links but

00:22:46,980 --> 00:22:52,680
kind of to fault domains but what we

00:22:50,640 --> 00:23:00,050
recommend not to do is to distribute the

00:22:52,680 --> 00:23:00,050
masters across different regions so and

00:23:00,110 --> 00:23:05,310
you can also have a circle a synchronous

00:23:03,720 --> 00:23:06,840
so this would be the synchronous part

00:23:05,310 --> 00:23:09,240
because you basically just split up your

00:23:06,840 --> 00:23:13,620
cluster the other part is actually to

00:23:09,240 --> 00:23:15,870
spin up to two clusters and it makes

00:23:13,620 --> 00:23:18,870
them aware which there's ongoing work

00:23:15,870 --> 00:23:20,520
but the current awareness only consists

00:23:18,870 --> 00:23:22,470
of that the two clusters know of each

00:23:20,520 --> 00:23:24,390
other and you can easily switch besides

00:23:22,470 --> 00:23:26,670
them they actually they wouldn't

00:23:24,390 --> 00:23:29,460
replicate any data so for replicating

00:23:26,670 --> 00:23:32,550
data in today's wals you have those two

00:23:29,460 --> 00:23:34,260
options you either you distribute your

00:23:32,550 --> 00:23:39,450
notes so you distribute one cluster

00:23:34,260 --> 00:23:41,900
across to availability zones or you add

00:23:39,450 --> 00:23:44,250
a proxy upfront which would actually

00:23:41,900 --> 00:23:46,350
duplicate all requests so I've seen some

00:23:44,250 --> 00:23:48,810
people doing that for example for

00:23:46,350 --> 00:23:50,370
marathon where they have a proxy in

00:23:48,810 --> 00:23:52,370
front of marathon which is then

00:23:50,370 --> 00:23:58,110
distributing all Korea's two marathons

00:23:52,370 --> 00:24:01,010
to two data centers alright and this

00:23:58,110 --> 00:24:05,880
actually gives us time for a demo and

00:24:01,010 --> 00:24:13,140
the first thing I would like to show my

00:24:05,880 --> 00:24:14,760
cluster isn't here is simply a rango DB

00:24:13,140 --> 00:24:17,730
up and running and I spun up a new

00:24:14,760 --> 00:24:19,910
cluster and this is an EE cluster it

00:24:17,730 --> 00:24:22,910
will tell us that it doesn't know the

00:24:19,910 --> 00:24:22,910
certificate

00:24:27,780 --> 00:24:37,410
and here we have an empty cluster as a

00:24:31,020 --> 00:24:45,570
Rango DB is in the universe can simply

00:24:37,410 --> 00:24:50,180
go here install it so right now the

00:24:45,570 --> 00:24:53,610
scheduler is coming up and maybe one

00:24:50,180 --> 00:24:57,690
neat feature which isn't a naval by

00:24:53,610 --> 00:24:59,280
default but we now since two days ago

00:24:57,690 --> 00:25:02,880
that's when they changed it we can also

00:24:59,280 --> 00:25:04,350
deploy it in a UCR configuration so and

00:25:02,880 --> 00:25:07,740
what we installed right now it's

00:25:04,350 --> 00:25:11,160
actually it's using the docker container

00:25:07,740 --> 00:25:14,460
Iser but we can also switch it to use

00:25:11,160 --> 00:25:21,710
the Maysles container riser which is

00:25:14,460 --> 00:25:21,710
hard to see here okay

00:25:25,690 --> 00:25:31,630
our main schedulers up and running and

00:25:28,870 --> 00:25:35,260
right now the database and coordinator

00:25:31,630 --> 00:25:37,930
services are coming up and we'll have a

00:25:35,260 --> 00:25:41,310
look what that means in just a second

00:25:37,930 --> 00:25:41,310
that looks good

00:25:41,340 --> 00:25:46,410
see where zou I not yet available

00:25:59,929 --> 00:26:09,270
it's slow but it's yeah it's not healthy

00:26:02,820 --> 00:26:11,159
yet so while it's still deploying let me

00:26:09,270 --> 00:26:12,530
jump to the second part of the demo

00:26:11,159 --> 00:26:17,610
enzyme we're just going to switch back

00:26:12,530 --> 00:26:20,159
in terms of time so what I've done here

00:26:17,610 --> 00:26:22,559
before or what they have done for me is

00:26:20,159 --> 00:26:26,039
I set up two clusters so this first one

00:26:22,559 --> 00:26:28,830
is being based in Paris as we had two

00:26:26,039 --> 00:26:32,010
French keynote speakers today and the

00:26:28,830 --> 00:26:34,440
second one is based in Vilnius so

00:26:32,010 --> 00:26:37,230
there's the same cluster set up saves

00:26:34,440 --> 00:26:40,260
the same user settings and they are

00:26:37,230 --> 00:26:42,510
linked to each other so this this is

00:26:40,260 --> 00:26:46,559
data center one let me actually create a

00:26:42,510 --> 00:26:49,679
collection a collection is this term

00:26:46,559 --> 00:26:52,080
where I can then store documents so in a

00:26:49,679 --> 00:26:59,570
traditional database imagine it being

00:26:52,080 --> 00:26:59,570
like a space with multiple tables and

00:27:00,740 --> 00:27:06,140
mrs. Cohen it should be of type document

00:27:08,360 --> 00:27:13,130
give it to shorts

00:27:13,510 --> 00:27:21,970
so here we are generating our should be

00:27:19,000 --> 00:27:24,940
up yes it's here now let's have a look

00:27:21,970 --> 00:27:29,559
when it shows up here it's already here

00:27:24,940 --> 00:27:32,380
so we were too slow to switch over but

00:27:29,559 --> 00:27:36,880
as you'll notice if you put in more and

00:27:32,380 --> 00:27:45,250
more data and Ken just creates a random

00:27:36,880 --> 00:27:48,600
document here key test so I really

00:27:45,250 --> 00:27:48,600
create an empty document

00:27:56,850 --> 00:27:59,509
test

00:28:04,000 --> 00:28:10,059
it's saved see how quickly it shows up

00:28:07,210 --> 00:28:12,309
over here already here so this is

00:28:10,059 --> 00:28:15,039
basically sinking see data over from one

00:28:12,309 --> 00:28:17,169
data center to the other and I believe

00:28:15,039 --> 00:28:18,850
and this is why I find this important

00:28:17,169 --> 00:28:20,350
that also people are trying this out

00:28:18,850 --> 00:28:22,269
this is going to be one of the

00:28:20,350 --> 00:28:26,139
challenges if we really really want to

00:28:22,269 --> 00:28:29,169
support fault tolerance across data

00:28:26,139 --> 00:28:31,750
centers so we've seen that one of the

00:28:29,169 --> 00:28:34,929
first data stores to provide that

00:28:31,750 --> 00:28:37,210
probably was Cassandra but over time

00:28:34,929 --> 00:28:38,980
more and more services will have to

00:28:37,210 --> 00:28:41,500
provide that and I believe this is

00:28:38,980 --> 00:28:43,990
something really relevant for us working

00:28:41,500 --> 00:28:46,029
with mesas so if we care about it early

00:28:43,990 --> 00:28:49,679
and this is why I like to see people

00:28:46,029 --> 00:28:50,889
experimenting with these kind of options

00:28:49,679 --> 00:28:53,350
okay

00:28:50,889 --> 00:28:58,049
I hope my yes my Arango DB is green now

00:28:53,350 --> 00:28:58,049
and now I also get the UI

00:29:04,330 --> 00:29:10,269
and it's a similar UI as we just seen

00:29:07,179 --> 00:29:14,469
the only difference is that this is a

00:29:10,269 --> 00:29:17,259
community edition but still works for

00:29:14,469 --> 00:29:19,719
everything we want to show the one

00:29:17,259 --> 00:29:22,509
feature I won't create another

00:29:19,719 --> 00:29:24,639
collection right now but what I really

00:29:22,509 --> 00:29:27,489
like about the integration into mythos

00:29:24,639 --> 00:29:31,659
it's that I can actually scale up and

00:29:27,489 --> 00:29:33,609
scale down from the UI so this is a

00:29:31,659 --> 00:29:36,249
special missus UI and if we now look

00:29:33,609 --> 00:29:39,879
here it's a resource utilization and we

00:29:36,249 --> 00:29:41,349
scale up so the easy part to scale up as

00:29:39,879 --> 00:29:44,979
a coordinators because they are

00:29:41,349 --> 00:29:47,619
stateless so there is a certain one

00:29:44,979 --> 00:29:49,869
coming up and let's also scale up a

00:29:47,619 --> 00:29:53,249
database server which probably will take

00:29:49,869 --> 00:29:55,299
a little longer after this is deployed

00:29:53,249 --> 00:30:01,209
let's have a look at the resource

00:29:55,299 --> 00:30:03,659
utilization should go hopefully go up in

00:30:01,209 --> 00:30:03,659
a second

00:30:10,130 --> 00:30:14,780
yeah we see the resource utilization

00:30:11,809 --> 00:30:17,830
going up and if we look at the different

00:30:14,780 --> 00:30:20,539
tasks here yeah it's already running tur

00:30:17,830 --> 00:30:22,700
good now we have stre up and running

00:30:20,539 --> 00:30:26,809
let's do something similar with the

00:30:22,700 --> 00:30:29,500
database servers we're bringing up a new

00:30:26,809 --> 00:30:29,500
database server

00:30:35,470 --> 00:30:38,549
that looks good

00:30:39,500 --> 00:30:45,169
so right now it's recharging the data so

00:30:42,409 --> 00:30:47,120
what will happen is as we're adding one

00:30:45,169 --> 00:30:49,039
more server and this should actually be

00:30:47,120 --> 00:30:51,260
production ready I mean we don't have

00:30:49,039 --> 00:30:53,210
any data on there so it doesn't

00:30:51,260 --> 00:30:54,980
shouldn't take too long

00:30:53,210 --> 00:30:56,990
yeah and it's up there but it'll

00:30:54,980 --> 00:30:59,120
actually recharge the data when we bring

00:30:56,990 --> 00:31:01,039
it up so it will take the existing data

00:30:59,120 --> 00:31:04,460
which is on those two database servers

00:31:01,039 --> 00:31:08,000
and will then distribute it across all

00:31:04,460 --> 00:31:09,590
servers now and this is the fun part I

00:31:08,000 --> 00:31:13,220
wanted to show is actually we're also

00:31:09,590 --> 00:31:15,320
able to scale down so just briefly

00:31:13,220 --> 00:31:16,970
switching back here we see there was

00:31:15,320 --> 00:31:18,650
another increase in grease or soot

00:31:16,970 --> 00:31:22,070
realization as we'd launched a new task

00:31:18,650 --> 00:31:29,929
and now we can actually also scale down

00:31:22,070 --> 00:31:32,809
and this will most likely even take

00:31:29,929 --> 00:31:35,210
longer but I believe it's a really nice

00:31:32,809 --> 00:31:39,100
feature because it allows us to scale up

00:31:35,210 --> 00:31:42,559
and scale down in both directions and

00:31:39,100 --> 00:31:44,630
actually we can try that as well I won't

00:31:42,559 --> 00:31:47,390
be able to scale down to less than two

00:31:44,630 --> 00:31:50,000
servers because we have defined that we

00:31:47,390 --> 00:31:53,720
need to have two charts for this one

00:31:50,000 --> 00:31:57,289
collection we earlier created now we've

00:31:53,720 --> 00:31:59,360
scaled down and we also see here the

00:31:57,289 --> 00:32:03,669
resources are free to again and can

00:31:59,360 --> 00:32:03,669
actually be used by different tasks

00:32:08,809 --> 00:32:16,909
all right this was actually already the

00:32:14,570 --> 00:32:19,460
demo and just lets me finish way too

00:32:16,909 --> 00:32:24,669
early but at least it gives us some time

00:32:19,460 --> 00:32:27,320
for question sorry it's not my talk so I

00:32:24,669 --> 00:32:30,399
probably a max could have spent some

00:32:27,320 --> 00:32:30,399
more time on certain slides

00:32:34,980 --> 00:32:44,490
yeah I can repeat the question so what's

00:32:41,610 --> 00:32:47,519
the consistency model it uses the

00:32:44,490 --> 00:32:51,269
consistency model the consistency model

00:32:47,519 --> 00:32:54,059
the consistency model is a half a notion

00:32:51,269 --> 00:32:56,639
of distributed transaction but by

00:32:54,059 --> 00:32:59,010
default is just going to be a persistent

00:32:56,639 --> 00:33:04,500
prayer agent so you have a notion of

00:32:59,010 --> 00:33:10,190
atomicity per single key space which is

00:33:04,500 --> 00:33:10,190
stored on a single server okay thank you

00:33:16,340 --> 00:33:28,310
okay behind you behind you behind you oh

00:33:19,300 --> 00:33:30,380
I have a few weird questions so you're

00:33:28,310 --> 00:33:32,270
talking about people building schedulers

00:33:30,380 --> 00:33:35,350
and stuff like that and that you want to

00:33:32,270 --> 00:33:42,140
see more people develop stuff like this

00:33:35,350 --> 00:33:45,440
so yeah okay so my question is how

00:33:42,140 --> 00:33:47,780
so basically container con was there and

00:33:45,440 --> 00:33:52,160
we saw that I mean we can all kind of

00:33:47,780 --> 00:33:55,940
see that Cuba Nettie's has a kind of

00:33:52,160 --> 00:34:01,430
huge push from the community so what is

00:33:55,940 --> 00:34:04,070
the mesosphere if you will or what is

00:34:01,430 --> 00:34:06,920
the mezzos community doing in that

00:34:04,070 --> 00:34:08,900
department regarding trying to kind of

00:34:06,920 --> 00:34:12,350
you know get the hold of people to

00:34:08,900 --> 00:34:14,420
actually build schedulers or there were

00:34:12,350 --> 00:34:17,030
some steps with the sdk you know and

00:34:14,420 --> 00:34:18,410
okay now it's a little bit easier but

00:34:17,030 --> 00:34:22,880
it's still not dead simple

00:34:18,410 --> 00:34:25,520
if you yeah so what I believe over time

00:34:22,880 --> 00:34:30,080
and this is also why I believe it's

00:34:25,520 --> 00:34:32,870
still hard we see SDK and also talking

00:34:30,080 --> 00:34:36,170
to the outside it's probably hard for

00:34:32,870 --> 00:34:38,920
people external or not collaborating

00:34:36,170 --> 00:34:41,630
with mesosphere to use the SDK what I

00:34:38,920 --> 00:34:42,590
really enjoy seeing in the last month's

00:34:41,630 --> 00:34:44,930
that we adding more and more

00:34:42,590 --> 00:34:47,120
documentation and actually there a lot

00:34:44,930 --> 00:34:49,910
of community frameworks based on the SDK

00:34:47,120 --> 00:34:55,400
are coming up and so I believe it will

00:34:49,910 --> 00:34:57,800
get easier over time what I like about

00:34:55,400 --> 00:34:59,630
me sauce is it that simply gives me the

00:34:57,800 --> 00:35:02,270
choice right if you say there's a lot of

00:34:59,630 --> 00:35:05,630
push around kubernetes I can run

00:35:02,270 --> 00:35:07,460
kubernetes and on top because it's

00:35:05,630 --> 00:35:11,900
actually it's the application scheduler

00:35:07,460 --> 00:35:15,710
and if I want to run multiple kubernetes

00:35:11,900 --> 00:35:18,860
clusters Mace's enables me to do that so

00:35:15,710 --> 00:35:21,860
yeah no I completely agree with that I'm

00:35:18,860 --> 00:35:25,700
not this is like not a tech yeah yeah

00:35:21,860 --> 00:35:28,040
we're all community I'll come to the

00:35:25,700 --> 00:35:30,230
community question as this is probably

00:35:28,040 --> 00:35:32,240
relevant to me especially

00:35:30,230 --> 00:35:37,400
because I'm part of this community team

00:35:32,240 --> 00:35:39,980
so I believe we we are working on doing

00:35:37,400 --> 00:35:43,490
a better job there but overall I think

00:35:39,980 --> 00:35:45,020
it's a lot is due to documentation and

00:35:43,490 --> 00:35:48,079
this is actually where the community can

00:35:45,020 --> 00:35:52,150
help us well contributing documentation

00:35:48,079 --> 00:35:55,099
as you mentioned it's hard things but I

00:35:52,150 --> 00:35:58,250
believe that in certain things

00:35:55,099 --> 00:36:02,690
Mace's is also taking the professional

00:35:58,250 --> 00:36:04,760
aspect to certain things so for it for

00:36:02,690 --> 00:36:08,200
example some some default values or some

00:36:04,760 --> 00:36:10,220
very conservative values where

00:36:08,200 --> 00:36:12,380
kubernetes is more developer-friendly

00:36:10,220 --> 00:36:15,589
and sorry for taking up that example

00:36:12,380 --> 00:36:16,970
again but DC us basically chooses a way

00:36:15,589 --> 00:36:20,180
which will work in all distributed

00:36:16,970 --> 00:36:22,820
scenarios whereas kubernetes is often or

00:36:20,180 --> 00:36:24,950
sometimes giving you choices which make

00:36:22,820 --> 00:36:26,450
it easy to develop but then if you

00:36:24,950 --> 00:36:29,810
really want to deploy it into production

00:36:26,450 --> 00:36:32,570
it still makes it kind of harder so what

00:36:29,810 --> 00:36:34,910
I believe is happening in the future we

00:36:32,570 --> 00:36:39,560
are working hard on making things easier

00:36:34,910 --> 00:36:41,150
like we see SDK metrics API and in my

00:36:39,560 --> 00:36:43,310
opinion most importantly adding

00:36:41,150 --> 00:36:48,589
documentation around things how to do

00:36:43,310 --> 00:36:50,540
stuff whereas kubernetes I seen for

00:36:48,589 --> 00:36:51,950
example I was just at spark summit

00:36:50,540 --> 00:36:54,230
yesterday and there were two talked

00:36:51,950 --> 00:36:58,310
about running a spark and kubernetes and

00:36:54,230 --> 00:37:00,710
that was really really hacky and in my

00:36:58,310 --> 00:37:02,119
opinion I can it could count at least

00:37:00,710 --> 00:37:04,400
like three cases where that would

00:37:02,119 --> 00:37:06,560
probably fail in production if those

00:37:04,400 --> 00:37:08,780
failure scenarios happened and this is

00:37:06,560 --> 00:37:10,490
something I like about me so that in the

00:37:08,780 --> 00:37:12,740
mesas well if we take this conservative

00:37:10,490 --> 00:37:15,200
approach which is not that developer

00:37:12,740 --> 00:37:18,319
friendly but will work in the engine

00:37:15,200 --> 00:37:23,180
production and I believe mesas will work

00:37:18,319 --> 00:37:26,420
move towards developers more or DCOs for

00:37:23,180 --> 00:37:28,369
that as well and coronaries will in

00:37:26,420 --> 00:37:30,410
certain aspects also become more complex

00:37:28,369 --> 00:37:34,010
whenever you start a greenfield project

00:37:30,410 --> 00:37:36,560
it's usually easier did that kind of go

00:37:34,010 --> 00:37:39,430
in the direction you want it okay I

00:37:36,560 --> 00:37:39,430
tried

00:37:42,730 --> 00:37:49,070
and I'm happy to discuss that afterwards

00:37:45,440 --> 00:37:51,920
for longer then thank you very much and

00:37:49,070 --> 00:37:54,100
enjoy the last sessions from mrs. Kahn

00:37:51,920 --> 00:37:54,100

YouTube URL: https://www.youtube.com/watch?v=CseOows3870


