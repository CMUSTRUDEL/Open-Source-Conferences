Title: Apache Kafka + Apache Mesos = Highly Scalable Streaming Microservices - Kai Waehner, Confluent
Publication date: 2017-10-27
Playlist: MesosCon Europe 2017
Description: 
	Apache Kafka + Apache Mesos = Highly Scalable Streaming Microservices - Kai Waehner, Confluent

This session discusses how to build a highly scalable, performant, mission-critical microservice infrastructure with Apache Kafka and Apache Mesos. Apache Kafka brokers are used as powerful, scalable, distributed message backbone. Kafka Streams’ API allows to embed stream processing directly into any microservice or business application; without the need for a dedicated streaming cluster. Apache Mesos is used as scalable infrastructure 
under the hood of Apache Kafka and Kafka Streams applications to 
leverage the benefits of a cloud native platforms like service discovery, health checks, or fail-over management.

A live demo shows how to develop real time applications for your core business with Kafka messaging brokers and Kafka Streams API and how to deploy / manage / scale them on a Mesos cluster using different deployment options like Marathon, Docker, Kubernetes.

About Kai Waehner
Kai Wähner works as Technology Evangelist at Confluent. Kai’s main area of expertise lies within the fields of Big Data Analytics, Machine Learning, Integration, Microservices, Internet of Things, Stream Processing and Blockchain. He is regular speaker at international conferences such as JavaOne, O’Reilly Software Architecture or ApacheCon, writes articles for professional journals, and shares his experiences with new technologies on his blog (www.kai-waehner.de/blog). Twitter: @KaiWaehner
Captions: 
	00:00:00,000 --> 00:00:05,220
hello everybody welcome great to be here

00:00:02,220 --> 00:00:06,960
at mrs. Kahn Europe in Prague today I

00:00:05,220 --> 00:00:09,059
want to talk about a Pesci Kafka and

00:00:06,960 --> 00:00:11,280
measles a combination we see a lot of

00:00:09,059 --> 00:00:13,590
different customers so I work for

00:00:11,280 --> 00:00:15,900
confluent we work a lot together with

00:00:13,590 --> 00:00:18,029
the mesosphere guys and I want to show

00:00:15,900 --> 00:00:20,400
you today especially how to build highly

00:00:18,029 --> 00:00:22,619
scalable streaming micro services I know

00:00:20,400 --> 00:00:24,810
it's a lot of password in there but it's

00:00:22,619 --> 00:00:25,260
also really working well and that's what

00:00:24,810 --> 00:00:28,619
I want

00:00:25,260 --> 00:00:30,630
today in this presentation so here you

00:00:28,619 --> 00:00:33,690
see the agenda for the next 30 or 35

00:00:30,630 --> 00:00:35,550
minutes first a short explanation about

00:00:33,690 --> 00:00:37,050
what I mean with scalable micro services

00:00:35,550 --> 00:00:39,719
and what submitted motivation for this

00:00:37,050 --> 00:00:41,640
talk I will also talk shortly up about

00:00:39,719 --> 00:00:42,930
Apache Kafka and the confluent platform

00:00:41,640 --> 00:00:45,180
so that we all have the same knowledge

00:00:42,930 --> 00:00:46,980
about that and then I will go into more

00:00:45,180 --> 00:00:49,110
details about Kafka streams which is

00:00:46,980 --> 00:00:51,750
part of the Apache Kafka sauce project

00:00:49,110 --> 00:00:53,730
to build streaming micro services or in

00:00:51,750 --> 00:00:55,680
the end to build stream processing and I

00:00:53,730 --> 00:00:57,600
will explain a little bit how it differs

00:00:55,680 --> 00:00:59,760
from other stream processing engines

00:00:57,600 --> 00:01:02,550
like spark streaming or fleeing or storm

00:00:59,760 --> 00:01:04,619
and all these other zona market and then

00:01:02,550 --> 00:01:06,750
I will talk about why we see many

00:01:04,619 --> 00:01:08,369
customers using Apache kafka above the

00:01:06,750 --> 00:01:10,740
broker side and also kafka streams

00:01:08,369 --> 00:01:12,930
together of missiles and DCOs and what's

00:01:10,740 --> 00:01:15,030
the benefits of that and then also I

00:01:12,930 --> 00:01:17,490
have build a use case very short life

00:01:15,030 --> 00:01:19,590
demo I'm how to deploy cough kind also

00:01:17,490 --> 00:01:21,869
kafka streams micro-services tomatoes

00:01:19,590 --> 00:01:24,420
and scale it up and down that's the

00:01:21,869 --> 00:01:26,700
agenda let's start with the motivation

00:01:24,420 --> 00:01:29,610
for his talk scalable micro services and

00:01:26,700 --> 00:01:31,680
I think I just have one picture here

00:01:29,610 --> 00:01:33,900
about micro services we all know this

00:01:31,680 --> 00:01:36,180
term already probably I think the main

00:01:33,900 --> 00:01:37,650
goal is that we do not build a big huge

00:01:36,180 --> 00:01:40,590
complex monolith anymore

00:01:37,650 --> 00:01:42,540
but smaller independent services so here

00:01:40,590 --> 00:01:43,140
you already see this is not a monolith

00:01:42,540 --> 00:01:45,149
anymore

00:01:43,140 --> 00:01:48,119
this is different independent services

00:01:45,149 --> 00:01:50,610
with specific functionality I do not

00:01:48,119 --> 00:01:52,409
think here that each one is really micro

00:01:50,610 --> 00:01:54,329
so I think the term is misleading but

00:01:52,409 --> 00:01:56,850
it's its own function its own domain of

00:01:54,329 --> 00:01:58,469
one of these services so often I simply

00:01:56,850 --> 00:02:01,170
say service instead of micro service

00:01:58,469 --> 00:02:02,939
also and why do we want to build this

00:02:01,170 --> 00:02:04,829
kind of micro service I hate extras

00:02:02,939 --> 00:02:07,590
instead of the monoliths I think there

00:02:04,829 --> 00:02:09,929
is two key ideas behind that why this

00:02:07,590 --> 00:02:12,510
gives us a lot of value the first is

00:02:09,929 --> 00:02:13,800
they are independently deployable so

00:02:12,510 --> 00:02:16,440
that means we do not have to Amman

00:02:13,800 --> 00:02:19,650
which we develop and test and version

00:02:16,440 --> 00:02:23,130
and deploy so and we independently build

00:02:19,650 --> 00:02:25,890
autonomous micro services and this helps

00:02:23,130 --> 00:02:28,890
us a lot to scale and prove scale I mean

00:02:25,890 --> 00:02:31,140
two things I mean it's one about people

00:02:28,890 --> 00:02:32,670
so that's very important thing so that

00:02:31,140 --> 00:02:35,280
we don't have all to work on the same

00:02:32,670 --> 00:02:37,350
project or in the same software package

00:02:35,280 --> 00:02:40,200
itself but we have our independent

00:02:37,350 --> 00:02:43,710
projects which we can test the back

00:02:40,200 --> 00:02:45,780
develop deploy reversion and deploy new

00:02:43,710 --> 00:02:48,900
features on that independently from the

00:02:45,780 --> 00:02:51,180
other teams there's the first very big

00:02:48,900 --> 00:02:53,790
benefit of micro services that's more

00:02:51,180 --> 00:02:55,340
about the our organization and then of

00:02:53,790 --> 00:02:58,620
course about the technical perspective

00:02:55,340 --> 00:03:00,900
and so we can scale in infrastructure

00:02:58,620 --> 00:03:02,850
terms that's also very important so we

00:03:00,900 --> 00:03:05,100
don't have to scale up or down our mana

00:03:02,850 --> 00:03:06,600
load which is much harder to do but in

00:03:05,100 --> 00:03:08,880
that instead of that we can

00:03:06,600 --> 00:03:11,400
independently scale up and down our

00:03:08,880 --> 00:03:14,220
business functions in the end so that's

00:03:11,400 --> 00:03:17,030
a huge benefit of micro services and so

00:03:14,220 --> 00:03:19,290
both technical advantages and also

00:03:17,030 --> 00:03:21,110
organizational ones of course all of the

00:03:19,290 --> 00:03:23,430
trade-offs and complexities behind that

00:03:21,110 --> 00:03:25,590
but the question is if we want to use

00:03:23,430 --> 00:03:28,020
micro service for only project how can

00:03:25,590 --> 00:03:30,270
we build that and there we have seen at

00:03:28,020 --> 00:03:32,010
many of our customers and to get there

00:03:30,270 --> 00:03:34,500
you need a few characteristics in your

00:03:32,010 --> 00:03:36,360
architecture one is loose coupling which

00:03:34,500 --> 00:03:38,250
is very important so that really these

00:03:36,360 --> 00:03:40,200
micro services are independent of each

00:03:38,250 --> 00:03:42,120
other if they are still connected and

00:03:40,200 --> 00:03:44,220
you can't upgrade one micro service

00:03:42,120 --> 00:03:45,930
without relating to other ones then it

00:03:44,220 --> 00:03:49,260
does not work that way they really have

00:03:45,930 --> 00:03:50,970
to be independently deployable and also

00:03:49,260 --> 00:03:52,950
um the other key point is that it's even

00:03:50,970 --> 00:03:55,590
driven so that this can happen whenever

00:03:52,950 --> 00:03:57,180
it works well for you this means when we

00:03:55,590 --> 00:03:59,430
have Micro Service which produced data

00:03:57,180 --> 00:04:01,560
that's absolutely fine but also this has

00:03:59,430 --> 00:04:03,810
to be done independently of consumers of

00:04:01,560 --> 00:04:05,760
the data so some might consume this data

00:04:03,810 --> 00:04:07,980
in batch mode for further analytics

00:04:05,760 --> 00:04:09,570
another one needs a real-time update

00:04:07,980 --> 00:04:11,580
because of alerting or something like

00:04:09,570 --> 00:04:13,470
this and so we really need this loose

00:04:11,580 --> 00:04:16,200
coupling and even driven architecture

00:04:13,470 --> 00:04:18,330
where we enable things like operational

00:04:16,200 --> 00:04:20,400
transparency behind all of that that's

00:04:18,330 --> 00:04:22,669
more or less how we get to micro service

00:04:20,400 --> 00:04:25,350
in a scalable way which we can manage

00:04:22,669 --> 00:04:27,180
that's the motivation for this talk in

00:04:25,350 --> 00:04:29,880
the end and what I want to show you here

00:04:27,180 --> 00:04:31,740
first of all I want to introduce shortly

00:04:29,880 --> 00:04:33,720
a Pecha Kafka in the confluent platform

00:04:31,740 --> 00:04:35,430
I assume most of you already noticed but

00:04:33,720 --> 00:04:37,680
just a to three slides that we have the

00:04:35,430 --> 00:04:40,410
same knowledge that so

00:04:37,680 --> 00:04:42,660
I betcha Kafka I'm was billed as a

00:04:40,410 --> 00:04:44,820
distributed fault tolerant commit clock

00:04:42,660 --> 00:04:46,139
so it's not like traditional messaging

00:04:44,820 --> 00:04:48,120
where you have a queue in the middle and

00:04:46,139 --> 00:04:49,919
you send messages and then read them and

00:04:48,120 --> 00:04:52,110
then they are gone from the middle from

00:04:49,919 --> 00:04:53,940
the queue but here you sent them to a

00:04:52,110 --> 00:04:56,729
commit clock this is distributed and

00:04:53,940 --> 00:04:59,039
scalable and many different consumers

00:04:56,729 --> 00:05:00,389
can read the logs when they want again

00:04:59,039 --> 00:05:02,190
one can do it in batch processing

00:05:00,389 --> 00:05:04,289
another wanted in stream processing in

00:05:02,190 --> 00:05:05,729
real time like the one could be

00:05:04,289 --> 00:05:07,889
elasticsearch the other one could be a

00:05:05,729 --> 00:05:09,210
Hadoop cluster that doesn't matter

00:05:07,889 --> 00:05:11,639
because Kafka in the middle

00:05:09,210 --> 00:05:13,110
also stores the messages it's not just

00:05:11,639 --> 00:05:15,960
messaging it's also storing of the

00:05:13,110 --> 00:05:17,250
messages and that's what was the date of

00:05:15,960 --> 00:05:19,889
appaji Kafka that was the beginning

00:05:17,250 --> 00:05:21,930
seven years or so ago when some guys had

00:05:19,889 --> 00:05:23,729
linked and created a Pecha Kafka and

00:05:21,930 --> 00:05:25,800
this is also the guys which created

00:05:23,729 --> 00:05:29,460
confluence three years ago where I work

00:05:25,800 --> 00:05:31,560
now and focus on Kafka ecosystem so if

00:05:29,460 --> 00:05:33,150
that um Kafka was created as messaging

00:05:31,560 --> 00:05:36,180
layer that's what everybody knows it

00:05:33,150 --> 00:05:38,909
about what many people do not about know

00:05:36,180 --> 00:05:40,590
about that it's already much more so

00:05:38,909 --> 00:05:42,599
even if you just take a look at a Pecha

00:05:40,590 --> 00:05:44,970
Kafka open source project which you use

00:05:42,599 --> 00:05:47,370
for download from the apache website it

00:05:44,970 --> 00:05:50,010
already includes Kafka connect and Kafka

00:05:47,370 --> 00:05:51,060
streams so in addition to the messaging

00:05:50,010 --> 00:05:53,789
which you see here in the middle as the

00:05:51,060 --> 00:05:56,340
Kafka brokers you also have connect to

00:05:53,789 --> 00:05:58,139
integrate with different systems and it

00:05:56,340 --> 00:05:59,880
also leverages all the advantages under

00:05:58,139 --> 00:06:02,280
the hood of Kafka like fault tolerant

00:05:59,880 --> 00:06:04,770
scalability and you like other

00:06:02,280 --> 00:06:07,139
integration components or products you

00:06:04,770 --> 00:06:10,020
have layers and connectors to things

00:06:07,139 --> 00:06:12,930
like elasticsearch HDFS s3

00:06:10,020 --> 00:06:14,909
relational databases and so on so it's

00:06:12,930 --> 00:06:17,220
pretty easy to configure the integration

00:06:14,909 --> 00:06:18,990
with other systems as option right you

00:06:17,220 --> 00:06:21,659
can still write your Kafka producers and

00:06:18,990 --> 00:06:23,729
consumers with api's like java.net Go

00:06:21,659 --> 00:06:25,620
Python whatever but you also can

00:06:23,729 --> 00:06:27,539
leverage Kafka connect if you want and

00:06:25,620 --> 00:06:29,669
on the other side we also have Kafka

00:06:27,539 --> 00:06:31,800
streams which is also part of the apache

00:06:29,669 --> 00:06:33,270
open-source project if you download and

00:06:31,800 --> 00:06:35,009
use the messaging layer you also have

00:06:33,270 --> 00:06:37,500
Kafka streams already with that and

00:06:35,009 --> 00:06:39,479
that's built um to do a stream

00:06:37,500 --> 00:06:41,190
processing on top of Apache Kafka and

00:06:39,479 --> 00:06:43,350
that's what I will also explain and

00:06:41,190 --> 00:06:44,820
much more later but that's the

00:06:43,350 --> 00:06:47,790
foundation that's Apache kafka

00:06:44,820 --> 00:06:49,560
open-source project on top of that

00:06:47,790 --> 00:06:51,360
there's also confluent open source

00:06:49,560 --> 00:06:53,610
projects which also many people use

00:06:51,360 --> 00:06:56,040
already they are also under Apache 2

00:06:53,610 --> 00:06:58,470
license so that syncs for example like

00:06:56,040 --> 00:07:01,050
the rest proxy if you do not want to use

00:06:58,470 --> 00:07:09,780
Java or.net or Python to produce and

00:07:01,050 --> 00:07:11,520
consume messages HTTP okay now it's

00:07:09,780 --> 00:07:13,800
working again so another component is

00:07:11,520 --> 00:07:15,840
the schema registry the schema registry

00:07:13,800 --> 00:07:18,000
is there to define structures in afro

00:07:15,840 --> 00:07:19,800
and then to and validated incoming and

00:07:18,000 --> 00:07:21,720
outgoing messages on the right way and

00:07:19,800 --> 00:07:24,150
it leverages all the features of Afro

00:07:21,720 --> 00:07:26,790
under a hood like a schema evolution so

00:07:24,150 --> 00:07:28,890
that you don't have to deploy updates on

00:07:26,790 --> 00:07:30,990
the producer and consumer side at the

00:07:28,890 --> 00:07:32,820
same time and that's very important

00:07:30,990 --> 00:07:34,380
component for micro services where the

00:07:32,820 --> 00:07:36,510
components are really independent of

00:07:34,380 --> 00:07:38,370
each other so the producer can upgrade

00:07:36,510 --> 00:07:41,220
to version 2 while the consumers still

00:07:38,370 --> 00:07:42,870
version 1 and so on so this is the

00:07:41,220 --> 00:07:45,000
confluent open source components and

00:07:42,870 --> 00:07:47,370
then of course as a company we also have

00:07:45,000 --> 00:08:04,590
enterprise tools like the control center

00:07:47,370 --> 00:08:07,230
for - end - let's take the other one so

00:08:04,590 --> 00:08:08,910
this might be better so and then we have

00:08:07,230 --> 00:08:11,100
things like the control center for into

00:08:08,910 --> 00:08:13,290
end stream monitoring from the producer

00:08:11,100 --> 00:08:16,530
very broker to the consumer to see find

00:08:13,290 --> 00:08:19,470
things like duplicate messages latency

00:08:16,530 --> 00:08:21,300
issues lost messages and all of that is

00:08:19,470 --> 00:08:23,910
a lot of powerful things or for example

00:08:21,300 --> 00:08:25,650
they a confluent replicator to do data

00:08:23,910 --> 00:08:27,180
center replication which is a much more

00:08:25,650 --> 00:08:29,640
powerful tool than mirror maker for

00:08:27,180 --> 00:08:32,219
example so enough of the confluent

00:08:29,640 --> 00:08:34,380
Enterprise part in the end the summary

00:08:32,219 --> 00:08:36,660
is that Kafka evolved evolved from a

00:08:34,380 --> 00:08:39,060
messaging layer to much more to a

00:08:36,660 --> 00:08:40,979
streaming platform even if just Kafka

00:08:39,060 --> 00:08:43,140
you have included epic Kafka connect and

00:08:40,979 --> 00:08:44,490
Kafka streams and then a lot of open

00:08:43,140 --> 00:08:47,070
source and commercial tooling around

00:08:44,490 --> 00:08:49,320
that that's an ENT summary of an

00:08:47,070 --> 00:08:51,360
overview of Apache Kafka ecosystem and

00:08:49,320 --> 00:08:52,320
of dead if you go back to the micro

00:08:51,360 --> 00:08:54,780
service and

00:08:52,320 --> 00:08:56,400
thinking it helps us a lot because as we

00:08:54,780 --> 00:08:58,710
see here we have a lot of different

00:08:56,400 --> 00:09:00,450
micro services and in the middle we have

00:08:58,710 --> 00:09:02,520
to commit clock which where you can send

00:09:00,450 --> 00:09:04,950
your messages to and where you can read

00:09:02,520 --> 00:09:07,080
messages from either we are Kafka

00:09:04,950 --> 00:09:09,150
producers or consumers with an API or

00:09:07,080 --> 00:09:11,550
with Kafka Connect or if Kafka streams

00:09:09,150 --> 00:09:13,020
and in this way we can decouple the

00:09:11,550 --> 00:09:14,610
different micro services from each other

00:09:13,020 --> 00:09:16,410
and can upgrade them and they can be

00:09:14,610 --> 00:09:18,840
down independently of each other so

00:09:16,410 --> 00:09:20,490
that's the huge benefit of use Kafka in

00:09:18,840 --> 00:09:21,870
the middle here and that's what we see

00:09:20,490 --> 00:09:23,580
it many people when they build their

00:09:21,870 --> 00:09:25,320
micro service architecture with many

00:09:23,580 --> 00:09:27,420
different technologies that they use

00:09:25,320 --> 00:09:30,930
Apache Kafka here in the middle is

00:09:27,420 --> 00:09:32,700
central nervous system for that with

00:09:30,930 --> 00:09:34,620
that we come now to Kafka screams which

00:09:32,700 --> 00:09:36,810
I will then use also in a live demo

00:09:34,620 --> 00:09:38,910
later to build some real scalable micro

00:09:36,810 --> 00:09:41,850
services with Kafka and Kafka streams

00:09:38,910 --> 00:09:43,410
let's shortly talk about what Kafka

00:09:41,850 --> 00:09:46,170
streams is and and what we mean about

00:09:43,410 --> 00:09:48,150
stream processing here the first key

00:09:46,170 --> 00:09:49,860
point is about stream processing I mean

00:09:48,150 --> 00:09:52,050
probably most of you are aware of that

00:09:49,860 --> 00:09:53,760
so we don't talk about request response

00:09:52,050 --> 00:09:56,580
where the data has addressed this means

00:09:53,760 --> 00:09:58,110
typically views HTTP or soap or other

00:09:56,580 --> 00:10:00,300
technologies you store something in a

00:09:58,110 --> 00:10:02,220
database or on disk and then you request

00:10:00,300 --> 00:10:04,860
it and read it again to get some updates

00:10:02,220 --> 00:10:06,540
and Merrill's so in contrary to that the

00:10:04,860 --> 00:10:09,330
stream processing we process the data

00:10:06,540 --> 00:10:11,580
while it is in motion so we continuously

00:10:09,330 --> 00:10:13,470
process data in an even driven

00:10:11,580 --> 00:10:15,420
architecture that's in the end also what

00:10:13,470 --> 00:10:17,640
Kafka is doing even if you can implement

00:10:15,420 --> 00:10:20,100
batch processing and request response we

00:10:17,640 --> 00:10:22,230
arrest proxy the main core is even

00:10:20,100 --> 00:10:24,900
driven here so that's the main

00:10:22,230 --> 00:10:26,580
definition of stream processing and the

00:10:24,900 --> 00:10:28,560
point here really is that many people

00:10:26,580 --> 00:10:30,750
still think about stream processing well

00:10:28,560 --> 00:10:32,970
that's faster or MapReduce so that's how

00:10:30,750 --> 00:10:34,620
it started some years ago where we had

00:10:32,970 --> 00:10:36,630
things like instead of using Hadoop with

00:10:34,620 --> 00:10:39,360
MapReduce we then used a spark streaming

00:10:36,630 --> 00:10:41,460
for example or Apache storm and deployed

00:10:39,360 --> 00:10:43,410
that onto our big data cluster to two

00:10:41,460 --> 00:10:45,690
stream processing which was faster

00:10:43,410 --> 00:10:48,600
continuous processing but still on the

00:10:45,690 --> 00:10:50,070
big data cluster so um and here is where

00:10:48,600 --> 00:10:52,530
Kafka streams differs a little bit from

00:10:50,070 --> 00:10:54,840
that no matter which one you take a look

00:10:52,530 --> 00:10:57,180
at spark streaming fleeing storm Kafka

00:10:54,840 --> 00:10:59,010
streams the the concepts are the same

00:10:57,180 --> 00:11:01,260
the idea so you have a stream processing

00:10:59,010 --> 00:11:02,730
pipeline so from the left you see you

00:11:01,260 --> 00:11:04,860
have all the input information from

00:11:02,730 --> 00:11:05,910
sensors from social networks from blogs

00:11:04,860 --> 00:11:07,890
whatever

00:11:05,910 --> 00:11:09,110
and then you process that you process

00:11:07,890 --> 00:11:11,490
that you do things like filtering

00:11:09,110 --> 00:11:12,900
transformations and Richmond's and you

00:11:11,490 --> 00:11:15,120
can do more powerful things like

00:11:12,900 --> 00:11:17,190
aggregations like applying contextual

00:11:15,120 --> 00:11:19,530
rules or even applying machine learning

00:11:17,190 --> 00:11:20,820
in such a stream process like important

00:11:19,530 --> 00:11:22,800
thing is you don't do that we are

00:11:20,820 --> 00:11:25,050
request/response but really continuously

00:11:22,800 --> 00:11:27,060
while the data is in motion that's the

00:11:25,050 --> 00:11:28,920
basic concepts behind stream processing

00:11:27,060 --> 00:11:32,100
no matter which technology you use and

00:11:28,920 --> 00:11:34,050
so now let's take a look I'm when to use

00:11:32,100 --> 00:11:35,970
kafka streams for stream processing and

00:11:34,050 --> 00:11:37,890
the idea behind that framework is not

00:11:35,970 --> 00:11:40,110
really drive your own big data cluster

00:11:37,890 --> 00:11:42,240
for stream processing but to really do

00:11:40,110 --> 00:11:44,190
it where it is needed so it can be small

00:11:42,240 --> 00:11:46,470
stream processing it need not always be

00:11:44,190 --> 00:11:48,420
powerful analytic stuff it can be maybe

00:11:46,470 --> 00:11:50,220
just a transformation from an input and

00:11:48,420 --> 00:11:52,290
output layer or something like this and

00:11:50,220 --> 00:11:54,560
therefore if you take a deeper look at

00:11:52,290 --> 00:11:57,270
kafka streams the goal was to build

00:11:54,560 --> 00:11:58,830
powerful but still simple tool to use

00:11:57,270 --> 00:12:01,080
and that's what I want to show you to

00:11:58,830 --> 00:12:02,820
know how that works and why then also

00:12:01,080 --> 00:12:05,100
measles is a great combination for that

00:12:02,820 --> 00:12:08,580
to scale it up and down without using

00:12:05,100 --> 00:12:11,160
your big data cluster for that so um

00:12:08,580 --> 00:12:13,080
Kafka streams is just a char library so

00:12:11,160 --> 00:12:15,540
that means you can embed that into any

00:12:13,080 --> 00:12:17,310
kind of java application so here you see

00:12:15,540 --> 00:12:19,530
a few different examples where you can

00:12:17,310 --> 00:12:21,870
run that you can use the very cool stuff

00:12:19,530 --> 00:12:23,400
like kubernetes or measles or docker and

00:12:21,870 --> 00:12:26,130
so on where you run Kafka streams

00:12:23,400 --> 00:12:28,020
processes you cannot still use the

00:12:26,130 --> 00:12:30,420
uncool things like a war file for a web

00:12:28,020 --> 00:12:32,160
application or anything like that so we

00:12:30,420 --> 00:12:33,570
still have existing application mono

00:12:32,160 --> 00:12:35,550
lots and even there you can add that

00:12:33,570 --> 00:12:37,980
that works pretty well because it's just

00:12:35,550 --> 00:12:39,570
a char library I will go into more

00:12:37,980 --> 00:12:42,420
detail here later about how Kafka

00:12:39,570 --> 00:12:44,640
streams works and the main motivation

00:12:42,420 --> 00:12:46,440
for that really is that we have not just

00:12:44,640 --> 00:12:48,360
one big data cluster where different

00:12:46,440 --> 00:12:50,190
teams have to think about how can i

00:12:48,360 --> 00:12:52,710
deploy my services there and upgrade

00:12:50,190 --> 00:12:55,380
them and version them and change them or

00:12:52,710 --> 00:12:57,660
fix something because we can focus on

00:12:55,380 --> 00:12:59,310
our application like here you see some

00:12:57,660 --> 00:13:01,230
examples like a fraud I have a monitor

00:12:59,310 --> 00:13:04,440
app a recommender up and a payment app

00:13:01,230 --> 00:13:06,330
so um you can use your technology so one

00:13:04,440 --> 00:13:08,820
can deploy it locally maybe in a web

00:13:06,330 --> 00:13:10,620
application another one can deploy it on

00:13:08,820 --> 00:13:12,630
missiles the next one on kubernetes

00:13:10,620 --> 00:13:14,100
maybe kubernetes on missiles I don't

00:13:12,630 --> 00:13:15,900
know and but they are all working

00:13:14,100 --> 00:13:18,500
independently they can use their own

00:13:15,900 --> 00:13:20,060
versions and upgrade that when they want

00:13:18,500 --> 00:13:21,860
and that means both that means the

00:13:20,060 --> 00:13:23,420
technology under the hood with Kafka

00:13:21,860 --> 00:13:25,400
streams but also on top of that the

00:13:23,420 --> 00:13:26,960
business application so you do not

00:13:25,400 --> 00:13:29,120
depend on the big cluster which you have

00:13:26,960 --> 00:13:30,430
to manage and care about your resources

00:13:29,120 --> 00:13:32,240
there because you have your own

00:13:30,430 --> 00:13:34,340
infrastructure where you use Kafka

00:13:32,240 --> 00:13:36,110
streams that has all that trade-off so

00:13:34,340 --> 00:13:37,880
I'm not saying do everything with Kafka

00:13:36,110 --> 00:13:39,080
streams now so for some situations it

00:13:37,880 --> 00:13:41,120
might be better to have your big data

00:13:39,080 --> 00:13:42,410
cluster right everything runs then it's

00:13:41,120 --> 00:13:44,540
still the right thing for sprog

00:13:42,410 --> 00:13:46,010
streaming of link or so on but this is

00:13:44,540 --> 00:13:47,840
simply another option if you want to

00:13:46,010 --> 00:13:51,040
have simple lightweight but still

00:13:47,840 --> 00:13:53,630
scalable um stream processing services

00:13:51,040 --> 00:13:55,910
so let's take a closer look to that

00:13:53,630 --> 00:13:57,260
Kafka streams as I said it's I'm just a

00:13:55,910 --> 00:13:59,900
library which you embed into your

00:13:57,260 --> 00:14:01,340
project and I'm from the features

00:13:59,900 --> 00:14:03,140
perspective it's pretty similar to all

00:14:01,340 --> 00:14:05,500
the others you have two functions you

00:14:03,140 --> 00:14:08,150
need to do aggregations and windowing

00:14:05,500 --> 00:14:10,760
transformations and so on the huge

00:14:08,150 --> 00:14:12,170
benefit is that as it leverages Kafka

00:14:10,760 --> 00:14:14,120
under the hood like all the other

00:14:12,170 --> 00:14:16,220
frameworks like Connect um it also

00:14:14,120 --> 00:14:18,080
leverages all the scalability the high

00:14:16,220 --> 00:14:19,610
volume processing you can do

00:14:18,080 --> 00:14:21,530
reprocessing if you want to start from

00:14:19,610 --> 00:14:23,420
the beginning again all these features

00:14:21,530 --> 00:14:25,040
which you like Kafka for you can do the

00:14:23,420 --> 00:14:28,310
same for Kafka streams under the hood

00:14:25,040 --> 00:14:29,840
and also it has local state so while it

00:14:28,310 --> 00:14:32,240
means that you have different Kafka

00:14:29,840 --> 00:14:33,860
streams instances let's say ten Java

00:14:32,240 --> 00:14:35,840
applications which run anywhere in a

00:14:33,860 --> 00:14:37,880
docker container or in kubernetes or in

00:14:35,840 --> 00:14:40,430
Miso's and then if one of these fails

00:14:37,880 --> 00:14:42,380
and Kafka streams automatically manages

00:14:40,430 --> 00:14:44,450
to failover so it only uses the other

00:14:42,380 --> 00:14:46,640
nine of them but without losing any data

00:14:44,450 --> 00:14:49,190
because under the hood it has local

00:14:46,640 --> 00:14:50,810
store but also it uses Kafka is backup

00:14:49,190 --> 00:14:52,760
in the end so that it sends information

00:14:50,810 --> 00:14:55,000
to the topics so that you can be sure to

00:14:52,760 --> 00:14:57,290
not lose any information with that and

00:14:55,000 --> 00:14:58,880
for that for that reason you can also

00:14:57,290 --> 00:15:00,680
use Kafka streams which is lightweight

00:14:58,880 --> 00:15:04,310
but still powerful because you can scale

00:15:00,680 --> 00:15:05,900
it up and down without any data loss the

00:15:04,310 --> 00:15:07,790
key operations look pretty similar you

00:15:05,900 --> 00:15:10,400
have things like map filter aggregate

00:15:07,790 --> 00:15:12,050
and join that's the same like if you

00:15:10,400 --> 00:15:14,270
have a streams library or Scala

00:15:12,050 --> 00:15:15,980
collections or reactive things with

00:15:14,270 --> 00:15:17,390
these frameworks and so the key

00:15:15,980 --> 00:15:19,040
difference is really this is already

00:15:17,390 --> 00:15:20,930
stateful and fault tolerant and

00:15:19,040 --> 00:15:22,370
distributed and you don't have to care

00:15:20,930 --> 00:15:26,150
about that because you leverage the

00:15:22,370 --> 00:15:28,100
Kafka cluster and also one more feature

00:15:26,150 --> 00:15:29,210
I want to talk about what's a little bit

00:15:28,100 --> 00:15:31,420
different from many others

00:15:29,210 --> 00:15:33,970
Kafka streams uses tables

00:15:31,420 --> 00:15:36,130
stream so both concepts are built into

00:15:33,970 --> 00:15:38,550
kafka streams so streams are you have

00:15:36,130 --> 00:15:41,140
continuous information which you process

00:15:38,550 --> 00:15:43,180
tables means like a database from the

00:15:41,140 --> 00:15:45,550
relational world where you store it and

00:15:43,180 --> 00:15:47,800
update it right so you only have the

00:15:45,550 --> 00:15:49,900
newest version in your new HQ on your

00:15:47,800 --> 00:15:51,970
topic this is in the end under the hood

00:15:49,900 --> 00:15:54,400
uses compacted Kafka topics if you know

00:15:51,970 --> 00:15:56,020
the details about Kafka and so if that

00:15:54,400 --> 00:15:58,420
you can easily build applications or

00:15:56,020 --> 00:16:01,030
micro-services which also star state

00:15:58,420 --> 00:16:05,500
easily with both streams and also with

00:16:01,030 --> 00:16:07,150
tables in one API in one instance so

00:16:05,500 --> 00:16:09,400
here you see one example of such an

00:16:07,150 --> 00:16:11,020
application you have an input topic so

00:16:09,400 --> 00:16:13,420
Kafka streams always gets information

00:16:11,020 --> 00:16:15,640
from Kafka topic and sends it to Kafka

00:16:13,420 --> 00:16:17,560
topic if you want to store it in for

00:16:15,640 --> 00:16:19,420
example also in a database or no sequel

00:16:17,560 --> 00:16:21,640
or Hadoop you would typically use a

00:16:19,420 --> 00:16:23,800
Kafka producer after the output stream

00:16:21,640 --> 00:16:26,170
or you would use Kafka connect for that

00:16:23,800 --> 00:16:28,330
you could also write directly from the

00:16:26,170 --> 00:16:29,740
stream process to some database but

00:16:28,330 --> 00:16:31,030
that's not really a continuous flow

00:16:29,740 --> 00:16:32,500
because then you have to handle this

00:16:31,030 --> 00:16:34,270
exceptions and so on there and that's

00:16:32,500 --> 00:16:36,460
not a recommended way but you could do

00:16:34,270 --> 00:16:39,700
it as it's just a Java library which you

00:16:36,460 --> 00:16:41,320
embed anyhow and then you run in this

00:16:39,700 --> 00:16:43,120
way so the topics of course were in the

00:16:41,320 --> 00:16:44,830
Kafka cluster and Kafka streams is

00:16:43,120 --> 00:16:48,370
deployed anywhere as I have discussed

00:16:44,830 --> 00:16:49,960
before so here's one example for a

00:16:48,370 --> 00:16:51,970
source code I don't want to go into a

00:16:49,960 --> 00:16:53,500
detail here and also in the life team I

00:16:51,970 --> 00:16:55,360
will focus more on how to run that with

00:16:53,500 --> 00:16:57,310
measles because I think that's the focus

00:16:55,360 --> 00:16:59,320
of the talk but here you see one example

00:16:57,310 --> 00:17:00,850
of a word count so this is like

00:16:59,320 --> 00:17:02,440
MapReduce verb count but it's not a

00:17:00,850 --> 00:17:04,570
batch mode but this is continuous word

00:17:02,440 --> 00:17:06,310
count so for every new Kafka event which

00:17:04,570 --> 00:17:08,380
is coming in it continuously counts the

00:17:06,310 --> 00:17:09,100
words and so on and the goal is here

00:17:08,380 --> 00:17:10,870
just to show you

00:17:09,100 --> 00:17:13,120
um how simple it is so it's the app

00:17:10,870 --> 00:17:14,830
configuration of your alliance it's the

00:17:13,120 --> 00:17:17,020
processing which is the key part of the

00:17:14,830 --> 00:17:19,150
class may define all the aggregations

00:17:17,020 --> 00:17:21,160
the filtering the transformation and so

00:17:19,150 --> 00:17:23,470
on and then you start processing and

00:17:21,160 --> 00:17:25,240
this is embedded into one class which

00:17:23,470 --> 00:17:26,680
you then packaged into one char file or

00:17:25,240 --> 00:17:31,570
run on one docker container or in

00:17:26,680 --> 00:17:33,280
measles like I will do later so I will

00:17:31,570 --> 00:17:35,320
talk mostly about Kafka streams now I

00:17:33,280 --> 00:17:37,600
also showed in the demo we also

00:17:35,320 --> 00:17:40,300
announced case equal two months ago case

00:17:37,600 --> 00:17:42,560
equal is a sequel like streaming engine

00:17:40,300 --> 00:17:44,900
on top of Kafka streams so I'm at

00:17:42,560 --> 00:17:46,730
Kasich will is a streams application

00:17:44,900 --> 00:17:48,950
under the hood but you don't care you

00:17:46,730 --> 00:17:51,440
just write cql queries to do continuous

00:17:48,950 --> 00:17:51,830
processes and continuous queries and so

00:17:51,440 --> 00:17:53,360
on

00:17:51,830 --> 00:17:55,280
so it simply makes it much easier

00:17:53,360 --> 00:17:57,140
especially for non Java developers to

00:17:55,280 --> 00:17:59,930
write your own streaming queries on top

00:17:57,140 --> 00:18:01,820
of page Kafka just as an option but I

00:17:59,930 --> 00:18:04,100
will now focus more on Kafka streams and

00:18:01,820 --> 00:18:05,870
how they deployed that on my sauce but

00:18:04,100 --> 00:18:07,850
in the same way we could use a sequel if

00:18:05,870 --> 00:18:10,190
you just want to write scalable queries

00:18:07,850 --> 00:18:14,240
with a query language instead of writing

00:18:10,190 --> 00:18:16,640
java code with Kafka streams so this was

00:18:14,240 --> 00:18:18,440
the intro to Kafka and Kafka streams and

00:18:16,640 --> 00:18:20,420
now let's take a look how that works

00:18:18,440 --> 00:18:22,220
together with me sauce and TCOs so I

00:18:20,420 --> 00:18:24,440
typically use both words for the same so

00:18:22,220 --> 00:18:26,990
I only use DC or s of course here for a

00:18:24,440 --> 00:18:29,600
demos also and how to build scalable

00:18:26,990 --> 00:18:31,310
micro-services for that here we see the

00:18:29,600 --> 00:18:32,990
Maysles architecture I think it a miso

00:18:31,310 --> 00:18:34,970
scone I don't have to talk much about

00:18:32,990 --> 00:18:37,130
that here but in the middle you see the

00:18:34,970 --> 00:18:41,060
Missis master quorum with one leader and

00:18:37,130 --> 00:18:43,730
to standby masses here it uses zookeeper

00:18:41,060 --> 00:18:45,890
of missiles here so its own instances

00:18:43,730 --> 00:18:49,220
and then on the left side you see the

00:18:45,890 --> 00:18:51,260
the frameworks like marathon and on the

00:18:49,220 --> 00:18:53,870
right side you see how these are used

00:18:51,260 --> 00:18:55,910
for execution of tasks where in the end

00:18:53,870 --> 00:18:58,100
you do the the execution and later

00:18:55,910 --> 00:19:00,710
that's the architecture and how does

00:18:58,100 --> 00:19:03,110
that relate to Kafka so Kafka is

00:19:00,710 --> 00:19:04,640
different components and first of all in

00:19:03,110 --> 00:19:06,200
the middle we see the Kafka broker

00:19:04,640 --> 00:19:08,270
that's the key part for the messaging

00:19:06,200 --> 00:19:10,820
where we send our messages to and where

00:19:08,270 --> 00:19:12,530
we read messages from this guy's up and

00:19:10,820 --> 00:19:16,040
down you can have more and more brokers

00:19:12,530 --> 00:19:17,720
so this can scale up horizontally and on

00:19:16,040 --> 00:19:20,510
the other side also needs to keep her

00:19:17,720 --> 00:19:21,380
the same way like me sauce and as you

00:19:20,510 --> 00:19:23,510
have seen in the keynote this morning

00:19:21,380 --> 00:19:26,030
I'm for Kafka it's also the idea to

00:19:23,510 --> 00:19:27,410
remove zookeeper the dependency if we

00:19:26,030 --> 00:19:30,230
will ever see that I don't know I'm

00:19:27,410 --> 00:19:32,090
permisos inform for for Kafka because

00:19:30,230 --> 00:19:33,560
it's not easy to remove it so right now

00:19:32,090 --> 00:19:35,720
it's the same status as for me sauce

00:19:33,560 --> 00:19:38,600
you'd need some keeper its own instances

00:19:35,720 --> 00:19:41,150
by default it uses the instances of DC

00:19:38,600 --> 00:19:42,950
OSS we Celia Tom but for big deployments

00:19:41,150 --> 00:19:45,710
we would recommend own zookeeper

00:19:42,950 --> 00:19:47,180
instances for Kafka and on top you see

00:19:45,710 --> 00:19:49,580
the Kafka streams applications

00:19:47,180 --> 00:19:52,010
that's Kafka draw Java consumers

00:19:49,580 --> 00:19:53,929
producers Ora Kafka streams applications

00:19:52,010 --> 00:19:55,820
which run independently so that's the

00:19:53,929 --> 00:19:58,159
micro services which we build here

00:19:55,820 --> 00:19:59,899
and on the right side we also see some

00:19:58,159 --> 00:20:01,820
other components like arrest proxy or

00:19:59,899 --> 00:20:04,129
schema registry which you can run in the

00:20:01,820 --> 00:20:05,690
same way also on TCOs if they are also

00:20:04,129 --> 00:20:08,840
available in a catalog as we will see

00:20:05,690 --> 00:20:10,399
and so um if we want to combine that

00:20:08,840 --> 00:20:11,840
here we see on the left side is in this

00:20:10,399 --> 00:20:14,419
case we can choose frameworks like

00:20:11,840 --> 00:20:16,340
maratona kubernetes and then on the

00:20:14,419 --> 00:20:18,169
right side we see how we run and the

00:20:16,340 --> 00:20:21,049
execute test error so on the top the

00:20:18,169 --> 00:20:23,090
kafka broker and on the bottom it's the

00:20:21,049 --> 00:20:26,119
Kafka streams instances and they can

00:20:23,090 --> 00:20:28,820
independently scale up and down that's

00:20:26,119 --> 00:20:30,169
the architecture and so before I go to a

00:20:28,820 --> 00:20:32,330
live demo let's talk a little bit about

00:20:30,169 --> 00:20:35,179
many of our customers combined cough

00:20:32,330 --> 00:20:38,080
candy cos in the end there are three key

00:20:35,179 --> 00:20:41,229
benefits summarized so the first one is

00:20:38,080 --> 00:20:43,609
d cos su here at mrs. Khanna know I'm

00:20:41,229 --> 00:20:46,190
allows automated provisioning and

00:20:43,609 --> 00:20:47,929
upgrading of Khafre components this is

00:20:46,190 --> 00:20:50,570
I'm pretty straightforward if you use

00:20:47,929 --> 00:20:52,309
missiles and if you if you know T cos no

00:20:50,570 --> 00:20:54,049
matter if your script or use the UI and

00:20:52,309 --> 00:20:56,029
you can do it for all the Kafka

00:20:54,049 --> 00:20:58,039
components and here again it's important

00:20:56,029 --> 00:21:00,559
to understand that many people do not

00:20:58,039 --> 00:21:02,919
just use the kafka brokers but also the

00:21:00,559 --> 00:21:05,539
Kafka consumers the Kafka producers and

00:21:02,919 --> 00:21:07,460
Kafka streams Kafka Connect and all

00:21:05,539 --> 00:21:09,229
commercial enterprise components from

00:21:07,460 --> 00:21:11,690
confluent they can all run on these

00:21:09,229 --> 00:21:14,019
euros s if you want or so we have can

00:21:11,690 --> 00:21:16,489
have some outside of course if you want

00:21:14,019 --> 00:21:19,779
this tecnique big benefit is the unified

00:21:16,489 --> 00:21:21,859
managed mended monitoring so um I

00:21:19,779 --> 00:21:24,529
restarted also for the demo it's pretty

00:21:21,859 --> 00:21:26,450
easy to do with the UI and also the

00:21:24,529 --> 00:21:28,759
benefit is you can manage multiple Kafka

00:21:26,450 --> 00:21:30,649
clusters on one infrastructure including

00:21:28,759 --> 00:21:32,570
multi talents II that's one of the huge

00:21:30,649 --> 00:21:35,059
benefits of course if you use TCO is

00:21:32,570 --> 00:21:38,389
under the hood instead of just AWS for

00:21:35,059 --> 00:21:39,979
example so that's the second huge

00:21:38,389 --> 00:21:42,229
benefit and also here of course you can

00:21:39,979 --> 00:21:44,359
combine with other big data components

00:21:42,229 --> 00:21:46,999
so as we have seen the keynote so many

00:21:44,359 --> 00:21:49,519
of T cos or mesosphere customers use

00:21:46,999 --> 00:21:51,769
many technologies like Kafka like spark

00:21:49,519 --> 00:21:54,289
like Cassandra and deploy all of them on

00:21:51,769 --> 00:21:57,139
DC OS and that makes it so easy and also

00:21:54,289 --> 00:21:58,309
to give the resources to which framework

00:21:57,139 --> 00:22:00,889
needs it for the moment

00:21:58,309 --> 00:22:02,599
and the third prepare first and the

00:22:00,889 --> 00:22:04,429
elastic scaling and fault tolerance I

00:22:02,599 --> 00:22:06,349
mean these benefits are typically not

00:22:04,429 --> 00:22:07,820
much different from the other components

00:22:06,349 --> 00:22:08,640
like Cassandra sparks or the huge

00:22:07,820 --> 00:22:10,920
benefit

00:22:08,640 --> 00:22:12,929
the same but there are some special

00:22:10,920 --> 00:22:14,970
things too for example or I liked a lot

00:22:12,929 --> 00:22:16,620
when I get started here is the Kafka VIP

00:22:14,970 --> 00:22:19,799
connection so that you will really have

00:22:16,620 --> 00:22:21,360
one static bootstrap server URL so this

00:22:19,799 --> 00:22:23,130
doesn't mean even if you start new Kafka

00:22:21,360 --> 00:22:25,080
brokers or a completely new cluster or

00:22:23,130 --> 00:22:27,090
so for example for my demo I started one

00:22:25,080 --> 00:22:29,370
on AWS and then one week later I

00:22:27,090 --> 00:22:30,990
restarted again and I never had to care

00:22:29,370 --> 00:22:33,570
about things like IP addresses because I

00:22:30,990 --> 00:22:35,850
always use the same we IP connection so

00:22:33,570 --> 00:22:38,040
that I have just one URL which I use

00:22:35,850 --> 00:22:40,070
also from my micro services which

00:22:38,040 --> 00:22:42,929
communicate with the Kafka brokers and

00:22:40,070 --> 00:22:47,309
therefore this bread will really very

00:22:42,929 --> 00:22:49,200
well together with LaCava ecosystem so

00:22:47,309 --> 00:22:51,929
with that um let's now take a look at a

00:22:49,200 --> 00:22:54,120
use case in this case am i build a

00:22:51,929 --> 00:22:56,580
scalable fly prediction micro service I

00:22:54,120 --> 00:22:57,870
simply use this one because I use it in

00:22:56,580 --> 00:22:59,309
a lot of other demos where I talk more

00:22:57,870 --> 00:23:02,010
about machine learning with Kafka

00:22:59,309 --> 00:23:04,260
streams so I have simply reused the same

00:23:02,010 --> 00:23:07,230
micro service which I built before or

00:23:04,260 --> 00:23:09,510
implemented before in Java and here you

00:23:07,230 --> 00:23:12,000
see the technologies I use DCOs and then

00:23:09,510 --> 00:23:13,740
Kafka brokers and Kafka streams and of

00:23:12,000 --> 00:23:16,140
course in Kafka streams I can use any

00:23:13,740 --> 00:23:18,120
other kind of library because Kafka

00:23:16,140 --> 00:23:20,100
streams is just a Java library you

00:23:18,120 --> 00:23:22,230
combine it with others like you want in

00:23:20,100 --> 00:23:24,570
this case I built an analytic model to

00:23:22,230 --> 00:23:26,669
also show some more powerful example and

00:23:24,570 --> 00:23:29,040
I embedded the analytic model which was

00:23:26,669 --> 00:23:30,870
built with h2o framework and embedded

00:23:29,040 --> 00:23:33,330
that into my Kafka streams application

00:23:30,870 --> 00:23:34,860
the use case is an airline flight delay

00:23:33,330 --> 00:23:36,660
prediction so that we can predict the

00:23:34,860 --> 00:23:38,760
future flights are probably delayed or

00:23:36,660 --> 00:23:40,530
not I use decision trees here but that's

00:23:38,760 --> 00:23:41,790
again just one use case no matter which

00:23:40,530 --> 00:23:45,600
kind of stream process you want to

00:23:41,790 --> 00:23:48,540
implement it would work the same way so

00:23:45,600 --> 00:23:50,040
if we go back to our architecture here

00:23:48,540 --> 00:23:52,500
we see now what we do and we have

00:23:50,040 --> 00:23:54,690
Marathon in this case and on maratona I

00:23:52,500 --> 00:23:57,059
have two different kind of executors I

00:23:54,690 --> 00:23:58,410
use the Kafka broker so I have three

00:23:57,059 --> 00:24:01,020
different brokers running on my meat

00:23:58,410 --> 00:24:02,910
sauce cluster and I have Kafka streams

00:24:01,020 --> 00:24:04,650
instances and we will see in a minute

00:24:02,910 --> 00:24:07,770
how we can scale it up and down and

00:24:04,650 --> 00:24:12,900
automatically of course TCOs managers if

00:24:07,770 --> 00:24:14,549
applicable itself manages to scale up

00:24:12,900 --> 00:24:16,740
and down so if one of them is stopped or

00:24:14,549 --> 00:24:20,850
gets crashed the processing is done with

00:24:16,740 --> 00:24:22,269
the other four instances so here we are

00:24:20,850 --> 00:24:23,559
that's in the end

00:24:22,269 --> 00:24:25,719
process it's a pretty simple stream

00:24:23,559 --> 00:24:27,519
process we have incoming data we do some

00:24:25,719 --> 00:24:30,519
filtering and then we apply the analytic

00:24:27,519 --> 00:24:32,379
model and here just we see some examples

00:24:30,519 --> 00:24:34,299
of the source code but this is again to

00:24:32,379 --> 00:24:35,830
show you this is Java code which is in

00:24:34,299 --> 00:24:38,889
the end and one char file which we want

00:24:35,830 --> 00:24:43,209
to run and so um let's take a look at

00:24:38,889 --> 00:24:45,249
that no SD COS does not work perfectly

00:24:43,209 --> 00:24:47,649
on my laptop um I tried it before with

00:24:45,249 --> 00:24:49,479
background locally but the cooler got

00:24:47,649 --> 00:24:51,339
too loud and so I decided to do it all

00:24:49,479 --> 00:24:53,080
on up AWS that was a little bit easier

00:24:51,339 --> 00:24:55,749
and therefore I only have this as

00:24:53,080 --> 00:24:56,889
recording because otherwise you never

00:24:55,749 --> 00:24:59,469
know what conference is how the

00:24:56,889 --> 00:25:01,299
connection to the Wi-Fi is but anyway um

00:24:59,469 --> 00:25:03,789
you see the same thing so here you see

00:25:01,299 --> 00:25:06,129
the dashboard where I have all my stuff

00:25:03,789 --> 00:25:07,509
running this is running on AWS with

00:25:06,129 --> 00:25:09,609
cloud formation and that was really

00:25:07,509 --> 00:25:11,320
great I could set it up in 10 minutes or

00:25:09,609 --> 00:25:13,539
so without any knowledge about missiles

00:25:11,320 --> 00:25:17,409
before and here you see the service I

00:25:13,539 --> 00:25:24,820
have already oh sorry that's good to

00:25:17,409 --> 00:25:30,849
know I have to get out of my

00:25:24,820 --> 00:25:35,079
presentation so here this looks better

00:25:30,849 --> 00:25:38,109
let's get back a second here we are

00:25:35,079 --> 00:25:39,789
know this looks better so um here you

00:25:38,109 --> 00:25:42,940
see it again notice the dashboard of ADA

00:25:39,789 --> 00:25:45,099
of TCOs which I've running on AWS with

00:25:42,940 --> 00:25:47,289
cloud formation I started that in five

00:25:45,099 --> 00:25:49,389
minutes or so and here now we have this

00:25:47,289 --> 00:25:51,489
service is running so I have now here a

00:25:49,389 --> 00:25:53,320
confluent kafka cluster which you can

00:25:51,489 --> 00:25:55,029
select from the catalog you can see

00:25:53,320 --> 00:25:56,979
there in get a catalog of missiles all

00:25:55,029 --> 00:25:59,200
the different components and confluent

00:25:56,979 --> 00:26:01,329
components and here we now see already I

00:25:59,200 --> 00:26:02,709
have the scheduler running and for the

00:26:01,329 --> 00:26:05,409
Kafka broca's and I have three different

00:26:02,709 --> 00:26:08,139
brokers running I also switched to UM

00:26:05,409 --> 00:26:09,999
the zookeeper managed by exhibitor here

00:26:08,139 --> 00:26:11,829
you see this is the missiles I'm default

00:26:09,999 --> 00:26:14,109
zookeeper and you'll see that it adds

00:26:11,829 --> 00:26:16,649
some information for a Kafka like the

00:26:14,109 --> 00:26:19,089
broker information the topic information

00:26:16,649 --> 00:26:21,909
here of course it's important again this

00:26:19,089 --> 00:26:23,829
works for small deployments for huge big

00:26:21,909 --> 00:26:26,589
employments we typically recommend to

00:26:23,829 --> 00:26:28,719
use an own zookeeper infrastructure on

00:26:26,589 --> 00:26:30,070
top of missiles for the Kafka ports but

00:26:28,719 --> 00:26:33,429
that's then for the bigger deployments

00:26:30,070 --> 00:26:35,620
and for more discussion later so this is

00:26:33,429 --> 00:26:39,500
the services I have running here already

00:26:35,620 --> 00:26:42,080
and what I want to do next then is I was

00:26:39,500 --> 00:26:43,610
born to own also start my micro services

00:26:42,080 --> 00:26:45,680
so we have the basic infrastructure

00:26:43,610 --> 00:26:47,720
running the calf care cluster and now we

00:26:45,680 --> 00:26:49,790
can deploy a micro service in this case

00:26:47,720 --> 00:26:52,280
I give it a survey tidy and I start with

00:26:49,790 --> 00:26:54,110
one instance and scale it up later I say

00:26:52,280 --> 00:26:57,440
where my docker container image is that

00:26:54,110 --> 00:26:59,720
is docker hub in this case and after I

00:26:57,440 --> 00:27:01,730
give it a CPU and memory I'm I simply

00:26:59,720 --> 00:27:03,950
selected some some random numbers here

00:27:01,730 --> 00:27:06,110
more or less and I select docker and

00:27:03,950 --> 00:27:07,580
chin to deploy it here the configuration

00:27:06,110 --> 00:27:10,610
as you see here is done also in in a

00:27:07,580 --> 00:27:12,590
minute or so and now I can start it it's

00:27:10,610 --> 00:27:14,440
just a small java application it starts

00:27:12,590 --> 00:27:16,490
in and I don't know three seconds or so

00:27:14,440 --> 00:27:18,710
only the Refresh here in the web UI

00:27:16,490 --> 00:27:21,260
isn't refreshed every three seconds but

00:27:18,710 --> 00:27:23,720
it's already running now and so um this

00:27:21,260 --> 00:27:25,610
is the Kafka streams applications which

00:27:23,720 --> 00:27:27,620
is waiting for input data to do bit the

00:27:25,610 --> 00:27:30,710
predictions and then send output data to

00:27:27,620 --> 00:27:33,860
another topic so here now we see one

00:27:30,710 --> 00:27:35,750
instance running I had another few

00:27:33,860 --> 00:27:37,610
running before so I'm this is one active

00:27:35,750 --> 00:27:39,650
phone that's important part right now I

00:27:37,610 --> 00:27:42,980
only have one micro service running and

00:27:39,650 --> 00:27:44,660
so now we have the command line um where

00:27:42,980 --> 00:27:46,820
it's important to see so what I did

00:27:44,660 --> 00:27:49,610
after I created a cluster on missiles I

00:27:46,820 --> 00:27:52,460
also use the DCOs command line with the

00:27:49,610 --> 00:27:54,650
kafka commands to create the topics so

00:27:52,460 --> 00:27:56,780
we have an inline an input topic in an

00:27:54,650 --> 00:27:58,790
output topic the input topic is where we

00:27:56,780 --> 00:28:00,350
get messages into the streams micro

00:27:58,790 --> 00:28:02,450
service and the output topic is where

00:28:00,350 --> 00:28:05,150
the streams micro service and state at

00:28:02,450 --> 00:28:07,310
home here you see that URL that's the

00:28:05,150 --> 00:28:09,320
important part about this VIP connection

00:28:07,310 --> 00:28:11,810
so that I always have to use the same

00:28:09,320 --> 00:28:13,580
connection URL no matter if I restart

00:28:11,810 --> 00:28:15,860
brokest or if new instances are created

00:28:13,580 --> 00:28:17,630
or edit and that's also a huge benefit I

00:28:15,860 --> 00:28:23,120
think for operations and not just for

00:28:17,630 --> 00:28:26,420
development on so as next part here now

00:28:23,120 --> 00:28:29,420
we can create a consumer in the producer

00:28:26,420 --> 00:28:30,740
so I'm just waiting until it happens so

00:28:29,420 --> 00:28:32,150
on the left side we have I'm the

00:28:30,740 --> 00:28:34,310
consumer running which is waiting for

00:28:32,150 --> 00:28:36,080
new predictions and on the right side I

00:28:34,310 --> 00:28:40,250
have a script which creates new input

00:28:36,080 --> 00:28:42,500
data so every one line is one new input

00:28:40,250 --> 00:28:44,360
information sent to a Kafka topic and

00:28:42,500 --> 00:28:46,670
this is done continuously every second

00:28:44,360 --> 00:28:48,290
to simply demonstrate or simulate some

00:28:46,670 --> 00:28:50,420
some input information

00:28:48,290 --> 00:28:52,940
and as soon as I click return it will

00:28:50,420 --> 00:28:54,920
shown the left side under predictions so

00:28:52,940 --> 00:28:56,420
in this case I admit I have to improve

00:28:54,920 --> 00:28:58,040
it because I always send the same line

00:28:56,420 --> 00:28:59,930
so it's always a prediction of yes and

00:28:58,040 --> 00:29:03,080
with normal information but you see the

00:28:59,930 --> 00:29:05,900
kind of idea behind that that I started

00:29:03,080 --> 00:29:07,970
producing messages continuously and the

00:29:05,900 --> 00:29:10,190
Kafka streams micro-service processes

00:29:07,970 --> 00:29:12,380
the messages in real time and sends the

00:29:10,190 --> 00:29:14,420
information the output the prediction to

00:29:12,380 --> 00:29:17,870
the output topic which is here on the

00:29:14,420 --> 00:29:20,240
left side so that's what I did mainly

00:29:17,870 --> 00:29:22,430
with one micro service and now with DCOs

00:29:20,240 --> 00:29:24,860
and we can easily scale it up or down in

00:29:22,430 --> 00:29:27,170
different ways in this case I use the

00:29:24,860 --> 00:29:28,850
web UI and of course we can do it via

00:29:27,170 --> 00:29:31,460
command line or via some kind of auto

00:29:28,850 --> 00:29:33,680
scaling features of D CRS and what I

00:29:31,460 --> 00:29:36,560
will now do I am start five of these

00:29:33,680 --> 00:29:39,140
services this is five independent Kafka

00:29:36,560 --> 00:29:41,840
streams micro-services in the end five

00:29:39,140 --> 00:29:43,580
Java applications and as soon as they

00:29:41,840 --> 00:29:45,440
get started which takes probably two

00:29:43,580 --> 00:29:48,590
seconds for every one or so they also

00:29:45,440 --> 00:29:49,820
start processing and that is what we

00:29:48,590 --> 00:29:51,830
will also see in a second

00:29:49,820 --> 00:29:54,320
so I'm each of these Kafka streams

00:29:51,830 --> 00:29:56,840
applications some registers to the same

00:29:54,320 --> 00:29:59,090
Kafka broker and the same topics because

00:29:56,840 --> 00:30:01,550
it's the same application and so um

00:29:59,090 --> 00:30:03,710
Kafka automatically sends the total

00:30:01,550 --> 00:30:05,810
information from the input topic to the

00:30:03,710 --> 00:30:07,160
different streams applications so here

00:30:05,810 --> 00:30:09,050
you see just um the old one is still

00:30:07,160 --> 00:30:11,870
running so it's still producing messages

00:30:09,050 --> 00:30:14,330
but now and we will also see that it's

00:30:11,870 --> 00:30:16,880
processed by all the five microservices

00:30:14,330 --> 00:30:21,110
so every message is only processed once

00:30:16,880 --> 00:30:22,700
and it processes it but by each for each

00:30:21,110 --> 00:30:24,950
of the five instances processes are

00:30:22,700 --> 00:30:27,860
messages so here we take a look at one

00:30:24,950 --> 00:30:29,510
of the log files to see that this is one

00:30:27,860 --> 00:30:31,610
of the ones which I started later this

00:30:29,510 --> 00:30:33,650
was not the first one but still I see

00:30:31,610 --> 00:30:35,630
here I'm some logging so I implemented

00:30:33,650 --> 00:30:37,670
the micro service so that it does a log

00:30:35,630 --> 00:30:39,680
for every prediction at us and you see

00:30:37,670 --> 00:30:41,660
here that it already has received some

00:30:39,680 --> 00:30:44,000
ones and now we also see the one which I

00:30:41,660 --> 00:30:46,730
start at first not after one minute ago

00:30:44,000 --> 00:30:49,520
about four minutes ago and here we will

00:30:46,730 --> 00:30:50,990
see that it also still processes data so

00:30:49,520 --> 00:30:53,420
how that's happening is that I'm by

00:30:50,990 --> 00:30:55,940
default as my Kafka messages do not have

00:30:53,420 --> 00:30:58,040
a key it uses round robin to process the

00:30:55,940 --> 00:31:00,260
messages so one two three five four one

00:30:58,040 --> 00:31:02,179
two three five four and you can

00:31:00,260 --> 00:31:03,980
configure anything here so for example

00:31:02,179 --> 00:31:06,409
your messages have a key then it

00:31:03,980 --> 00:31:08,149
automatically I'm orders it by key and

00:31:06,409 --> 00:31:10,429
sends all with key one to a specific

00:31:08,149 --> 00:31:12,710
streams application and did by two by

00:31:10,429 --> 00:31:14,299
two another one or you can kind of write

00:31:12,710 --> 00:31:16,549
any custom partition knows that's the

00:31:14,299 --> 00:31:19,490
same concept as Kafka for Kafka streams

00:31:16,549 --> 00:31:21,440
here so um that's mainly what I wanted

00:31:19,490 --> 00:31:24,559
to show you here how you can scale it up

00:31:21,440 --> 00:31:27,490
and down and how that works on TCOs so

00:31:24,559 --> 00:31:31,070
that you can scale up and down the

00:31:27,490 --> 00:31:44,869
micro-services so let's go back to the

00:31:31,070 --> 00:31:47,389
presentation go back here okay this was

00:31:44,869 --> 00:31:49,340
the demo a few more notes on that as I

00:31:47,389 --> 00:31:50,509
said I'm on AWS it was pretty easy for

00:31:49,340 --> 00:31:52,399
me to get started with that

00:31:50,509 --> 00:31:54,320
I might just used a cloud formation I'm

00:31:52,399 --> 00:31:55,909
scripts from mesosphere that worked

00:31:54,320 --> 00:31:57,830
pretty well because I'm also not that

00:31:55,909 --> 00:32:01,580
TCOs expert which would like to setup

00:31:57,830 --> 00:32:03,590
this by your own and then I configured

00:32:01,580 --> 00:32:05,720
the Kafka broker so here you see some of

00:32:03,590 --> 00:32:07,399
the web UI the interesting one is also

00:32:05,720 --> 00:32:09,169
on the left top side where you see there

00:32:07,399 --> 00:32:11,240
are many different components so I just

00:32:09,169 --> 00:32:13,369
use the Kafka brokers here but all the

00:32:11,240 --> 00:32:14,749
other Kafka components from the

00:32:13,369 --> 00:32:16,879
ecosystem are available like the rest

00:32:14,749 --> 00:32:18,409
proxy the schema registry the control

00:32:16,879 --> 00:32:20,179
center and so on you can all start and

00:32:18,409 --> 00:32:23,960
scale them in the same way we at eCos

00:32:20,179 --> 00:32:25,610
and only next slide this is the only

00:32:23,960 --> 00:32:28,279
thing where I had really problems when I

00:32:25,610 --> 00:32:30,889
built this demo here for DCOs together

00:32:28,279 --> 00:32:32,960
with Kafka streams because on the top

00:32:30,889 --> 00:32:35,600
left you'll see the Kafka client which

00:32:32,960 --> 00:32:38,570
mesosphere uses and all their tutorials

00:32:35,600 --> 00:32:40,129
and demos I went to use Kafka the

00:32:38,570 --> 00:32:41,929
problem with that is it still works but

00:32:40,129 --> 00:32:44,570
it's a pretty old version it's used as

00:32:41,929 --> 00:32:46,820
Kafka I'm 0.9 and the current version is

00:32:44,570 --> 00:32:49,490
your del 11 which is much much more

00:32:46,820 --> 00:32:51,230
sophisticated and for just Kafka's

00:32:49,490 --> 00:32:53,149
messaging this still works in all the

00:32:51,230 --> 00:32:54,619
demos and tutorials but for Kafka

00:32:53,149 --> 00:32:56,539
streams it does not work anymore because

00:32:54,619 --> 00:32:58,909
Kafka streams messages require

00:32:56,539 --> 00:33:01,730
timestamps and time stamps were added in

00:32:58,909 --> 00:33:03,470
I think 0 to 10 and so um Kafka streams

00:33:01,730 --> 00:33:05,869
applications on T cos throw exceptions

00:33:03,470 --> 00:33:07,610
and so I had to build my own missiles

00:33:05,869 --> 00:33:09,830
Kafka client so this is just in the end

00:33:07,610 --> 00:33:12,139
the command line which I used to produce

00:33:09,830 --> 00:33:13,460
and consume messages so I'm simply just

00:33:12,139 --> 00:33:15,680
to give you a hint if you want to build

00:33:13,460 --> 00:33:17,360
your own if you want to follow tutorials

00:33:15,680 --> 00:33:19,040
mesosphere and with this is the only

00:33:17,360 --> 00:33:20,570
problem which does not work you have to

00:33:19,040 --> 00:33:23,660
upgrade that or simply use my docker

00:33:20,570 --> 00:33:26,450
container which I uploaded to my doctor

00:33:23,660 --> 00:33:28,340
hub so here is in the end the Kafka

00:33:26,450 --> 00:33:29,870
streams micro-service um it's also on

00:33:28,340 --> 00:33:32,450
github with the links you see the slides

00:33:29,870 --> 00:33:34,670
later it's pretty simple docker files

00:33:32,450 --> 00:33:36,800
and it's really just a Chara file

00:33:34,670 --> 00:33:38,690
Kafka streams is a small application

00:33:36,800 --> 00:33:41,060
this is in the end a main operation and

00:33:38,690 --> 00:33:42,700
main method in a Java class and just I

00:33:41,060 --> 00:33:46,160
don't know 20 lines of code very used to

00:33:42,700 --> 00:33:50,810
kafka streams API and I use h2o for the

00:33:46,160 --> 00:33:52,400
model as I said here so here you see the

00:33:50,810 --> 00:33:54,260
Kafka streams that's what we also saw in

00:33:52,400 --> 00:33:56,390
a video it's more or less for reference

00:33:54,260 --> 00:33:58,310
and also after wall video also with a

00:33:56,390 --> 00:33:59,930
little bit more details on YouTube so if

00:33:58,310 --> 00:34:04,280
you want to watch that again or try out

00:33:59,930 --> 00:34:06,290
and one last note here so again what we

00:34:04,280 --> 00:34:08,690
did before I'm now is after streams

00:34:06,290 --> 00:34:11,180
since two months you can also use case

00:34:08,690 --> 00:34:12,710
sequel its build on Kafka streams but

00:34:11,180 --> 00:34:14,180
you don't see the streams part but it

00:34:12,710 --> 00:34:16,100
uses the same concepts under the hood

00:34:14,180 --> 00:34:18,410
and with that you can write sequel

00:34:16,100 --> 00:34:20,900
queries to do continuous processing

00:34:18,410 --> 00:34:23,120
continuous stream processing so you can

00:34:20,900 --> 00:34:26,060
do things with that like anomaly

00:34:23,120 --> 00:34:27,440
detection real-time ETL building

00:34:26,060 --> 00:34:29,840
real-time dashboards that's much easier

00:34:27,440 --> 00:34:32,090
if Kasich well and so we already see a

00:34:29,840 --> 00:34:33,290
lot of adoption of that and the same

00:34:32,090 --> 00:34:35,060
could be used of course in the same with

00:34:33,290 --> 00:34:36,860
TCOs so you can use it in the same way

00:34:35,060 --> 00:34:38,960
in a docker container for example and

00:34:36,860 --> 00:34:41,240
run it um we are marathon or via

00:34:38,960 --> 00:34:44,480
kubernetes on Miso's works the same very

00:34:41,240 --> 00:34:46,220
as you do it with Kafka streams so with

00:34:44,480 --> 00:34:46,850
that let's go to the key takeaways of

00:34:46,220 --> 00:34:50,270
this session

00:34:46,850 --> 00:34:51,800
Apache Kefka echo system on TC is for

00:34:50,270 --> 00:34:53,840
highly scalable falter and micro

00:34:51,800 --> 00:34:55,610
services I think that's really important

00:34:53,840 --> 00:34:57,590
part that I'm of course Kafka is used by

00:34:55,610 --> 00:34:59,600
many people for on TCO is already like

00:34:57,590 --> 00:35:01,100
Cassandra and so on with this Mac stack

00:34:59,600 --> 00:35:03,170
right we had heard it in the keynote

00:35:01,100 --> 00:35:04,820
also but also the other parts like

00:35:03,170 --> 00:35:07,280
building the micro services around that

00:35:04,820 --> 00:35:08,750
no matter if it's just Kafka producers

00:35:07,280 --> 00:35:11,330
and consumers or if you really want to

00:35:08,750 --> 00:35:12,770
build Kafka streams of micro services it

00:35:11,330 --> 00:35:16,280
works in the same way in very well with

00:35:12,770 --> 00:35:17,030
Kafka and with TC RS and the great thing

00:35:16,280 --> 00:35:19,010
is you have a lot of features

00:35:17,030 --> 00:35:21,020
out-of-the-box most of them are also

00:35:19,010 --> 00:35:23,300
true for all the other applications of

00:35:21,020 --> 00:35:24,980
this Mac stack and other components but

00:35:23,300 --> 00:35:27,170
also some specific ones like the Kafka

00:35:24,980 --> 00:35:28,550
VIP connection and that helps a lot to

00:35:27,170 --> 00:35:31,250
develop and operate

00:35:28,550 --> 00:35:33,920
kafka applications on missiles or DCOs

00:35:31,250 --> 00:35:35,660
and therefore also true about the kafka

00:35:33,920 --> 00:35:38,120
streams micro-services that's what I

00:35:35,660 --> 00:35:39,680
Indian wanted to show you and as we see

00:35:38,120 --> 00:35:41,690
many customers also using kubernetes

00:35:39,680 --> 00:35:43,280
more and more I would not be surprised

00:35:41,690 --> 00:35:45,440
if for example and the brokers are

00:35:43,280 --> 00:35:48,080
running out of the box on marathon is

00:35:45,440 --> 00:35:49,640
right now and many will use kubernetes

00:35:48,080 --> 00:35:52,070
for the orchestration of the micro

00:35:49,640 --> 00:35:54,200
services so this tag will then be TCOs

00:35:52,070 --> 00:35:57,680
kubernetes and Kafka streams micro

00:35:54,200 --> 00:35:59,570
service on top of that so that was my

00:35:57,680 --> 00:36:01,700
presentation I hope I go beyond good

00:35:59,570 --> 00:36:04,040
overview about Kafka Kafka streams on

00:36:01,700 --> 00:36:06,340
TCOs and now we also have time for

00:36:04,040 --> 00:36:06,340
questions

00:36:17,229 --> 00:36:21,939
if Kafka the question was how long does

00:36:20,079 --> 00:36:24,489
core routers kafka story messages is it

00:36:21,939 --> 00:36:26,079
forever and the basic thing is that its

00:36:24,489 --> 00:36:27,910
configuration so you can do whatever you

00:36:26,079 --> 00:36:30,339
want there is a concept called retention

00:36:27,910 --> 00:36:32,199
time in Kafka and then that can be

00:36:30,339 --> 00:36:33,999
either configured we our time so for

00:36:32,199 --> 00:36:35,829
example for the last four weeks or it

00:36:33,999 --> 00:36:37,989
can be defined by storage so let's only

00:36:35,829 --> 00:36:40,749
store 100 gigabyte and that can be done

00:36:37,989 --> 00:36:42,039
by topic and very specifically and that

00:36:40,749 --> 00:36:46,929
way you can manage exactly what is

00:36:42,039 --> 00:36:49,119
stored and how long it is stored so it's

00:36:46,929 --> 00:36:50,559
it's its start just storing on disk so

00:36:49,119 --> 00:36:52,869
it's not really an obstacle check store

00:36:50,559 --> 00:36:54,219
and there's still some kind of query on

00:36:52,869 --> 00:36:56,259
top of that so you can also query

00:36:54,219 --> 00:36:57,669
information out of that but the

00:36:56,259 --> 00:37:00,189
important thing so it's not really a

00:36:57,669 --> 00:37:01,989
mass equal like layer where you have

00:37:00,189 --> 00:37:03,009
indexing and so these things so that's

00:37:01,989 --> 00:37:05,199
not what it's used for so it's not

00:37:03,009 --> 00:37:06,880
simply also a search engine so typically

00:37:05,199 --> 00:37:17,380
there for use things like elastic search

00:37:06,880 --> 00:37:19,359
or so yes so the question was is it

00:37:17,380 --> 00:37:21,609
possible to do joints with kafka streams

00:37:19,359 --> 00:37:23,049
and k sequel and the answer is yes there

00:37:21,609 --> 00:37:24,519
are several different joints I'm

00:37:23,049 --> 00:37:26,589
available and with all different

00:37:24,519 --> 00:37:28,749
application windows but the key here is

00:37:26,589 --> 00:37:30,640
the joints are right now done on the

00:37:28,749 --> 00:37:32,829
keys right so the kafka message have

00:37:30,640 --> 00:37:34,839
keys and values and its own on the keys

00:37:32,829 --> 00:37:36,640
so you cannot do any kind of joint which

00:37:34,839 --> 00:37:39,009
you can do with some other query or

00:37:36,640 --> 00:37:47,349
search engines but some kind of joints

00:37:39,009 --> 00:37:49,059
are possible yes the question was is

00:37:47,349 --> 00:37:51,249
there an upper limit to the size of the

00:37:49,059 --> 00:37:53,229
which can be put into the queue so as

00:37:51,249 --> 00:37:55,269
I'm not sure if there is if you can put

00:37:53,229 --> 00:37:57,849
it a one terabyte file or I don't know

00:37:55,269 --> 00:37:58,959
but Sorum for the real world example

00:37:57,849 --> 00:38:00,519
there's not really any limit so there's

00:37:58,959 --> 00:38:01,749
you can also use that for storing and

00:38:00,519 --> 00:38:03,609
for example pictures or something like

00:38:01,749 --> 00:38:06,839
this so it's pretty I'm pretty big

00:38:03,609 --> 00:38:06,839
limits if there are limits

00:38:18,860 --> 00:38:22,230
so your question is if you want to

00:38:21,030 --> 00:38:30,800
aggregate information from different

00:38:22,230 --> 00:38:30,800
cuff topics you mean right mm-hmm

00:38:31,310 --> 00:38:34,590
okay so the question was how to

00:38:33,120 --> 00:38:36,450
aggregate information from different

00:38:34,590 --> 00:38:39,240
cuff care clusters which all run on TCOs

00:38:36,450 --> 00:38:41,370
so in the end there it's the same answer

00:38:39,240 --> 00:38:42,960
as we would give about TCOs so if you

00:38:41,370 --> 00:38:44,220
have different kafka clusters running

00:38:42,960 --> 00:38:46,850
and we have these situations for example

00:38:44,220 --> 00:38:49,230
if your real world example uber uber has

00:38:46,850 --> 00:38:50,640
as kafka running for example in New York

00:38:49,230 --> 00:38:52,530
in San Francisco and they have it run it

00:38:50,640 --> 00:38:54,600
locally to improve the processes of the

00:38:52,530 --> 00:38:56,400
taxis and so on and then they also want

00:38:54,600 --> 00:38:58,170
to match that together for aggregations

00:38:56,400 --> 00:39:00,300
and so on and there you typically do

00:38:58,170 --> 00:39:02,340
that via replication so you'd replicate

00:39:00,300 --> 00:39:03,840
between different clusters and this can

00:39:02,340 --> 00:39:05,070
be either done in active passive mode

00:39:03,840 --> 00:39:06,840
that it means that you have active

00:39:05,070 --> 00:39:08,760
clusters for the current processing and

00:39:06,840 --> 00:39:09,960
you have a big back and cluster let's

00:39:08,760 --> 00:39:12,180
say where you store it from all the

00:39:09,960 --> 00:39:13,560
other clusters to do analyzes to do for

00:39:12,180 --> 00:39:15,510
example machine learning and find out

00:39:13,560 --> 00:39:17,520
insights or you can also do active

00:39:15,510 --> 00:39:18,990
active where you have three systems

00:39:17,520 --> 00:39:21,240
which are all three important and you

00:39:18,990 --> 00:39:22,650
have to do if you updated at one you

00:39:21,240 --> 00:39:24,570
also send it to the others for

00:39:22,650 --> 00:39:26,490
aggregations or so that's also possible

00:39:24,570 --> 00:39:28,110
there are two options for doing this

00:39:26,490 --> 00:39:29,910
kind of replication with Kafka there is

00:39:28,110 --> 00:39:31,440
the open source mirror maker and there

00:39:29,910 --> 00:39:33,930
is a commercial tool confluent

00:39:31,440 --> 00:39:35,430
replicator which has a few um advantages

00:39:33,930 --> 00:39:37,290
and and is let's say more Enterprise

00:39:35,430 --> 00:39:39,120
ready a comparison about that is also on

00:39:37,290 --> 00:39:42,080
the web but um the replication is the

00:39:39,120 --> 00:39:42,080
way to do that typically

00:39:53,630 --> 00:40:00,620
but what was the question can you also

00:39:55,370 --> 00:40:01,820
run that on marathon or yes I I think

00:40:00,620 --> 00:40:03,140
actually that's what happenings overlap

00:40:01,820 --> 00:40:05,960
de Kafka brokers are running via

00:40:03,140 --> 00:40:07,340
marathon right now and I selected for

00:40:05,960 --> 00:40:09,020
the Kafka streams application at docker

00:40:07,340 --> 00:40:10,250
containers but you could also do the

00:40:09,020 --> 00:40:28,100
same with marathon so that works the

00:40:10,250 --> 00:40:31,370
same way yes yes yes so the question was

00:40:28,100 --> 00:40:33,200
how can you expose the Kafka interfaces

00:40:31,370 --> 00:40:34,580
to the outside world outside of TCOs is

00:40:33,200 --> 00:40:37,070
it possible trust be arrests or some

00:40:34,580 --> 00:40:38,240
other ways so um that simply depends

00:40:37,070 --> 00:40:40,370
more or less on the security confer

00:40:38,240 --> 00:40:42,320
configurations and so you have so what

00:40:40,370 --> 00:40:43,820
many actually really do is they use rest

00:40:42,320 --> 00:40:45,740
for that and they're confident rest

00:40:43,820 --> 00:40:47,990
proxy which is open source because this

00:40:45,740 --> 00:40:49,400
is the easiest way on the other way side

00:40:47,990 --> 00:40:52,220
you have all the trade-offs of rest and

00:40:49,400 --> 00:40:54,560
HTTP because it's lower through port and

00:40:52,220 --> 00:40:56,060
lower latency so um if you can open the

00:40:54,560 --> 00:40:57,710
right parts and configure the securities

00:40:56,060 --> 00:40:59,360
in the right way and you can also do

00:40:57,710 --> 00:41:01,340
that so I've got support security

00:40:59,360 --> 00:41:02,900
standards like a Kerberos and so on so

00:41:01,340 --> 00:41:04,310
that more or less depends on your

00:41:02,900 --> 00:41:05,930
configuration in your setup but you can

00:41:04,310 --> 00:41:07,640
also expose it to the outside world and

00:41:05,930 --> 00:41:10,340
I think that's typically done at most

00:41:07,640 --> 00:41:11,930
customers because many of the producers

00:41:10,340 --> 00:41:17,450
and consumers are outside the TCOs

00:41:11,930 --> 00:41:19,010
cluster ok then thanks a lot for coming

00:41:17,450 --> 00:41:20,660
also connect to me on LinkedIn if you

00:41:19,010 --> 00:41:22,310
wanted more information and so on and

00:41:20,660 --> 00:41:26,489
thanks a lot

00:41:22,310 --> 00:41:26,489

YouTube URL: https://www.youtube.com/watch?v=Ur68XbLUZmQ


