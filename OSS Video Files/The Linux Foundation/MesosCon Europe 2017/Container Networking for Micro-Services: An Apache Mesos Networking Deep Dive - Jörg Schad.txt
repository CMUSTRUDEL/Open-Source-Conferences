Title: Container Networking for Micro-Services: An Apache Mesos Networking Deep Dive - Jörg Schad
Publication date: 2017-10-27
Playlist: MesosCon Europe 2017
Description: 
	Container Networking for Micro-Services: An Apache Mesos Networking Deep Dive - Jörg Schad & Deepak Goel, Mesosphere, Inc.

Apache Mesos and DC/OS allows users to deploy distributed applications and in particular micro-services across a large cluster. Therefore, networking becomes an important aspect especially when trying to provide highly-available applications on top of an unreliable infrastructure.
In this talk, we will first present the various challenges around networking for distributed micro-service architectures, including
* Connectivity
* Service Discovery
* Load-balancing
* Isolation

As for most of the above challenges there is not a one-size-fits-all solution we have an in-depth look at the trade-offs between different solutions. 
Afterwards, we will deep dive into the actual implementation of the different components in order to understand how we can achieve a scalable networking

About Deepak Goel
Software Engineer, Mesosphere Inc.

About Jörg Schad
Jörg is a software engineer at Mesosphere in Hamburg. In his previous life he implemented distributed and in memory databases and conducted research in the Hadoop and Cloud area. His speaking experience includes various Meetups, international conferences, and lecture halls.
Captions: 
	00:00:00,030 --> 00:00:05,400
okay let's start I hope you had a good

00:00:02,580 --> 00:00:07,319
lunch now ready to to have a little bit

00:00:05,400 --> 00:00:10,349
into engineering I'm happy to introduce

00:00:07,319 --> 00:00:12,690
my colleague Deepak who is the

00:00:10,349 --> 00:00:14,969
maintainer of open-source networking

00:00:12,690 --> 00:00:17,930
components that we built upon on top of

00:00:14,969 --> 00:00:21,029
mezzos maybe you've heard Spartan

00:00:17,930 --> 00:00:23,400
Minutemen Navstar and he is going to

00:00:21,029 --> 00:00:26,310
talk about the networking stack this

00:00:23,400 --> 00:00:36,149
u.s. networking stack Deepak thank you

00:00:26,310 --> 00:00:38,430
thank you Alex welcome everybody so to

00:00:36,149 --> 00:00:41,570
begin with alex has given some

00:00:38,430 --> 00:00:44,070
introduction to who am I'm Deepak who's

00:00:41,570 --> 00:00:46,680
basically a technical lead at mesosphere

00:00:44,070 --> 00:00:48,530
maintaining the open source project that

00:00:46,680 --> 00:00:51,629
Alex just mentioned which includes

00:00:48,530 --> 00:00:54,379
service discovery components and load

00:00:51,629 --> 00:00:57,360
balancing components and today's talk is

00:00:54,379 --> 00:01:01,170
primarily focusing on these components

00:00:57,360 --> 00:01:04,680
and how they work together to build

00:01:01,170 --> 00:01:07,950
container networking in DC US so

00:01:04,680 --> 00:01:11,180
micro-services has really emerged as a

00:01:07,950 --> 00:01:13,590
modern day architecture for applications

00:01:11,180 --> 00:01:15,720
the reason being they are flexible

00:01:13,590 --> 00:01:17,970
because you can divide your monolithic

00:01:15,720 --> 00:01:20,820
application into simpler components and

00:01:17,970 --> 00:01:23,220
you can design develop and deploy these

00:01:20,820 --> 00:01:26,070
components independently also they are

00:01:23,220 --> 00:01:29,460
scalable because you can independently

00:01:26,070 --> 00:01:32,400
decide which component you want to run

00:01:29,460 --> 00:01:36,470
multiple instances but the scalability

00:01:32,400 --> 00:01:40,310
and flexibility comes at a cost of

00:01:36,470 --> 00:01:40,310
deployment complexity

00:01:46,479 --> 00:01:52,150
imagine that you have these components

00:01:49,420 --> 00:01:54,369
and their dependencies and if these

00:01:52,150 --> 00:01:56,530
components happen to run on same host

00:01:54,369 --> 00:01:59,799
then the dependency might conflict with

00:01:56,530 --> 00:02:02,259
each other well the answer is run

00:01:59,799 --> 00:02:05,470
everything in containers continuous is

00:02:02,259 --> 00:02:09,009
this nice concept where you pack your

00:02:05,470 --> 00:02:11,680
dependency as well as component and run

00:02:09,009 --> 00:02:14,650
them together in an isolation such that

00:02:11,680 --> 00:02:17,349
even if two containers happens to run on

00:02:14,650 --> 00:02:19,420
the same host the dependency of the

00:02:17,349 --> 00:02:21,790
component won't conflict with each other

00:02:19,420 --> 00:02:24,220
but I'm sure those who have played

00:02:21,790 --> 00:02:26,519
enough with containers would understand

00:02:24,220 --> 00:02:30,069
that container themselves are not enough

00:02:26,519 --> 00:02:30,580
one these containers are transient they

00:02:30,069 --> 00:02:33,790
are mortal

00:02:30,580 --> 00:02:35,620
so they die or they could die and then

00:02:33,790 --> 00:02:37,600
you need a system which could

00:02:35,620 --> 00:02:40,989
continuously monitor these containers

00:02:37,600 --> 00:02:43,750
and can launch them in your cluster each

00:02:40,989 --> 00:02:45,760
container consumes certain amount of

00:02:43,750 --> 00:02:48,100
system resources like CPU memory and

00:02:45,760 --> 00:02:50,829
disk you need some sort of resource

00:02:48,100 --> 00:02:53,799
management for your containers in the

00:02:50,829 --> 00:02:55,269
cluster and finally the services that

00:02:53,799 --> 00:02:57,670
are running within these cluster

00:02:55,269 --> 00:03:01,030
containers needs to be able to talk and

00:02:57,670 --> 00:03:03,100
discover each other and so you need some

00:03:01,030 --> 00:03:05,500
sort of a mechanism that can do service

00:03:03,100 --> 00:03:09,700
management for this container in your

00:03:05,500 --> 00:03:12,010
cluster and this is where DCOs concern

00:03:09,700 --> 00:03:14,109
Decius is this container orchestration

00:03:12,010 --> 00:03:17,640
platform which has missiles at its heart

00:03:14,109 --> 00:03:19,390
and it along with its framework supports

00:03:17,640 --> 00:03:22,049
container scheduling and resource

00:03:19,390 --> 00:03:24,910
management it also do service discovery

00:03:22,049 --> 00:03:27,220
through through service discovery

00:03:24,910 --> 00:03:30,400
mechanism and load balancing and we will

00:03:27,220 --> 00:03:35,380
see in today's talk how this is done and

00:03:30,400 --> 00:03:38,980
TCOs but before that before we actually

00:03:35,380 --> 00:03:41,049
look into specific DC or stack I wanted

00:03:38,980 --> 00:03:44,370
you to understand what are the

00:03:41,049 --> 00:03:46,510
complexities or challenges in providing

00:03:44,370 --> 00:03:49,299
container networking so let's say you

00:03:46,510 --> 00:03:53,440
have this Decius container orchestration

00:03:49,299 --> 00:03:55,299
platform running bunch of containers the

00:03:53,440 --> 00:03:57,510
very first challenge is to provide IP

00:03:55,299 --> 00:03:59,799
connectivity among these containers and

00:03:57,510 --> 00:04:00,220
the reason this is a challenge is

00:03:59,799 --> 00:04:02,200
because

00:04:00,220 --> 00:04:03,970
these containers have different modes of

00:04:02,200 --> 00:04:07,210
operations they could be running on the

00:04:03,970 --> 00:04:09,520
host mode just like a host or any

00:04:07,210 --> 00:04:12,220
application on the host or they could be

00:04:09,520 --> 00:04:13,750
sitting deep inside of VM which again

00:04:12,220 --> 00:04:19,720
could be running on a horse so it's like

00:04:13,750 --> 00:04:22,810
multi-layer deployment right once you

00:04:19,720 --> 00:04:24,610
resolve the IP connectivity the next

00:04:22,810 --> 00:04:26,500
challenge is to provide service

00:04:24,610 --> 00:04:28,390
discovery mechanism like I was saying

00:04:26,500 --> 00:04:30,520
earlier the services that are running in

00:04:28,390 --> 00:04:34,390
these containers needs to be able to

00:04:30,520 --> 00:04:36,670
talk to each other and as these

00:04:34,390 --> 00:04:39,070
containers are transient they can be

00:04:36,670 --> 00:04:41,110
continuously be dying and getting these

00:04:39,070 --> 00:04:43,090
scheduled on a different host so your

00:04:41,110 --> 00:04:47,260
service discovery mechanism should be

00:04:43,090 --> 00:04:51,610
efficient enough to update the service

00:04:47,260 --> 00:04:53,830
records in a timely manner and finally

00:04:51,610 --> 00:04:56,050
you would want multiple instances of

00:04:53,830 --> 00:04:58,330
these containers to be running behind a

00:04:56,050 --> 00:05:00,220
load balancer and load balancer also

00:04:58,330 --> 00:05:02,860
pretty much share the same challenge as

00:05:00,220 --> 00:05:04,480
service discovery it needs to reflect

00:05:02,860 --> 00:05:08,620
the changes that are happening in the

00:05:04,480 --> 00:05:12,030
cluster in a timely manner this brings

00:05:08,620 --> 00:05:15,460
us to our today's talk will be touching

00:05:12,030 --> 00:05:17,350
or I'll be giving a detailed overview of

00:05:15,460 --> 00:05:19,750
what is CNI container networking

00:05:17,350 --> 00:05:21,520
interface then we'll go and see how

00:05:19,750 --> 00:05:23,950
service discovery is done in DC u.s.

00:05:21,520 --> 00:05:30,880
networking stack and finally the load

00:05:23,950 --> 00:05:33,130
balancing but before we dive deep into

00:05:30,880 --> 00:05:35,320
each of these components let's see how

00:05:33,130 --> 00:05:37,750
the overall picture looks like and how

00:05:35,320 --> 00:05:39,760
these components fits and together to

00:05:37,750 --> 00:05:41,320
complete the container networking in DC

00:05:39,760 --> 00:05:45,520
worse so let's say that you have a

00:05:41,320 --> 00:05:49,090
master with bunch of agent nodes you can

00:05:45,520 --> 00:05:52,000
either use docker run time to launch

00:05:49,090 --> 00:05:53,500
docker containers or you can use

00:05:52,000 --> 00:05:57,310
something called Universal container

00:05:53,500 --> 00:06:01,350
runtime missiles run time to launch both

00:05:57,310 --> 00:06:01,350
mesos containers or docker containers

00:06:01,890 --> 00:06:08,530
now

00:06:04,620 --> 00:06:10,150
UCR has a native support for CNI which

00:06:08,530 --> 00:06:14,380
is container networking interface and

00:06:10,150 --> 00:06:17,470
we'll be seeing what it is in follow up

00:06:14,380 --> 00:06:19,930
slides but you see our Sienna is the

00:06:17,470 --> 00:06:22,420
specification mix-match is really easy

00:06:19,930 --> 00:06:24,220
for any third-party network provider to

00:06:22,420 --> 00:06:26,260
write a plug-in against that

00:06:24,220 --> 00:06:29,440
specification and then it it

00:06:26,260 --> 00:06:31,210
automatically works with my sauce so

00:06:29,440 --> 00:06:33,130
that takes and that's why you see that

00:06:31,210 --> 00:06:36,880
there are so many networking third-party

00:06:33,130 --> 00:06:38,710
network providers that provides the IP

00:06:36,880 --> 00:06:40,840
connectivity so they take care of

00:06:38,710 --> 00:06:43,120
connecting the containers and providing

00:06:40,840 --> 00:06:44,620
a flight network docker on the other

00:06:43,120 --> 00:06:47,710
hand use something called container

00:06:44,620 --> 00:06:50,200
network model which is kind of similar

00:06:47,710 --> 00:06:52,120
to CNI but it's not like a standard it's

00:06:50,200 --> 00:06:56,260
their own homegrown

00:06:52,120 --> 00:06:58,030
thing but it works similarly now the

00:06:56,260 --> 00:07:00,910
service discovery part is done by a

00:06:58,030 --> 00:07:04,480
component called Spartan and massage DNS

00:07:00,910 --> 00:07:06,220
Spartan is is a component that turns on

00:07:04,480 --> 00:07:08,830
all the nodes including masters in a

00:07:06,220 --> 00:07:10,510
distributed fashion and we'll see what

00:07:08,830 --> 00:07:14,320
benefit it gives us being distributed

00:07:10,510 --> 00:07:18,190
and then they gossip around to get the

00:07:14,320 --> 00:07:20,320
cluster information similarly load

00:07:18,190 --> 00:07:22,720
balancing is provided by a component

00:07:20,320 --> 00:07:24,790
called management it is also like

00:07:22,720 --> 00:07:27,130
Spartan fully distributed and runs on

00:07:24,790 --> 00:07:30,040
each node and then they gossip around to

00:07:27,130 --> 00:07:31,630
get the cluster global via so just keep

00:07:30,040 --> 00:07:33,730
this picture in mind while we are

00:07:31,630 --> 00:07:35,890
discussing each of these components

00:07:33,730 --> 00:07:37,930
separately to just give you a context as

00:07:35,890 --> 00:07:40,020
how these component fits in the entire

00:07:37,930 --> 00:07:42,280
picture

00:07:40,020 --> 00:07:44,350
starting with container networking

00:07:42,280 --> 00:07:49,150
interface so this is something proposed

00:07:44,350 --> 00:07:52,540
by core OS and now it's adopted by CN CF

00:07:49,150 --> 00:07:55,180
body so as I was saying earlier UCR has

00:07:52,540 --> 00:08:00,130
a native support for this so the way it

00:07:55,180 --> 00:08:03,490
works is there is a isolator in in mesos

00:08:00,130 --> 00:08:06,070
which is which is a network CNI isolator

00:08:03,490 --> 00:08:07,720
it's responsible for creating the

00:08:06,070 --> 00:08:09,940
network namespace for a particular

00:08:07,720 --> 00:08:13,270
container and then this namespace it

00:08:09,940 --> 00:08:15,550
invokes a CNI plugin which is setting on

00:08:13,270 --> 00:08:17,230
each agent at a predefined location and

00:08:15,550 --> 00:08:19,600
it gives this container

00:08:17,230 --> 00:08:22,150
a namespace or the network namespace to

00:08:19,600 --> 00:08:24,460
that plugin the plug-in does the actual

00:08:22,150 --> 00:08:26,200
work of connecting the network namespace

00:08:24,460 --> 00:08:29,320
of the container to the host and

00:08:26,200 --> 00:08:31,860
provides the connectivity that way to

00:08:29,320 --> 00:08:36,280
give you a specific example

00:08:31,860 --> 00:08:39,700
so each CNI or each virtual network in

00:08:36,280 --> 00:08:41,860
Decius comes with a configuration file

00:08:39,700 --> 00:08:43,980
and this configuration files pretty much

00:08:41,860 --> 00:08:46,270
define the name of the virtual network

00:08:43,980 --> 00:08:47,740
which is an important field and the

00:08:46,270 --> 00:08:49,540
second important field is the type of

00:08:47,740 --> 00:08:51,340
plug-in so there are different plugins

00:08:49,540 --> 00:08:53,290
depending on the functionality they

00:08:51,340 --> 00:08:55,420
provide like you have a bridge plug-in

00:08:53,290 --> 00:08:57,610
you have an eye Pam plug-in which is

00:08:55,420 --> 00:09:00,340
responsible for IP addresses you have IP

00:08:57,610 --> 00:09:03,220
VLAN plug-in mac ville and plugins and

00:09:00,340 --> 00:09:05,260
many more so the type defines what type

00:09:03,220 --> 00:09:07,300
of plug-in you would be using for this

00:09:05,260 --> 00:09:10,390
configuration and the name defines a

00:09:07,300 --> 00:09:12,670
virtual network and we will see why this

00:09:10,390 --> 00:09:15,460
name is so important so this

00:09:12,670 --> 00:09:19,090
configuration sets on each agent at a

00:09:15,460 --> 00:09:21,910
predefined location now let's say there

00:09:19,090 --> 00:09:25,630
is a task that is being launched on this

00:09:21,910 --> 00:09:27,640
agent the way this particular virtual

00:09:25,630 --> 00:09:29,800
network is used or the way this

00:09:27,640 --> 00:09:31,720
particular task get assigned to this

00:09:29,800 --> 00:09:35,410
network is through the name so if you

00:09:31,720 --> 00:09:37,660
see the name field is missus net which

00:09:35,410 --> 00:09:40,420
is which matching which is matching to

00:09:37,660 --> 00:09:42,310
the cni configuration now let's say this

00:09:40,420 --> 00:09:45,580
task is getting launched or this agent

00:09:42,310 --> 00:09:47,620
the agent the cni isolator running in

00:09:45,580 --> 00:09:50,410
the agent will create the network

00:09:47,620 --> 00:09:53,590
namespace for the container and then it

00:09:50,410 --> 00:09:55,510
will handover this namespace to to

00:09:53,590 --> 00:09:57,070
bridge plugin the reason it has bridge

00:09:55,510 --> 00:09:58,420
plug-in is because the type in the

00:09:57,070 --> 00:10:00,520
configuration is for this

00:09:58,420 --> 00:10:02,500
particular example and so bridge plugin

00:10:00,520 --> 00:10:05,020
will take this network namespace and

00:10:02,500 --> 00:10:07,660
connect the container to to the host

00:10:05,020 --> 00:10:11,830
networking or whatever logic it has

00:10:07,660 --> 00:10:15,640
built this is how the the container

00:10:11,830 --> 00:10:19,390
networking interface in general work now

00:10:15,640 --> 00:10:21,790
one specific implementation of this cni

00:10:19,390 --> 00:10:24,820
is present in DCOs in the form of

00:10:21,790 --> 00:10:26,710
overlay networks why we need overlay

00:10:24,820 --> 00:10:29,350
network is there is this requirement

00:10:26,710 --> 00:10:30,529
that sometimes you want to give IP to a

00:10:29,350 --> 00:10:32,629
container

00:10:30,529 --> 00:10:36,739
usually the IP addresses that you give

00:10:32,629 --> 00:10:37,969
to a container are non-routable like if

00:10:36,739 --> 00:10:40,729
you are launching a container in a

00:10:37,969 --> 00:10:43,009
bridge mode then that IP is not routable

00:10:40,729 --> 00:10:45,739
on the host network but you could create

00:10:43,009 --> 00:10:47,239
something called overlay on the host the

00:10:45,739 --> 00:10:48,889
reason why it is called overlays because

00:10:47,239 --> 00:10:50,719
the host network is considered as

00:10:48,889 --> 00:10:53,959
underlay and you are kind of doing an

00:10:50,719 --> 00:10:56,599
encapsulation on top of that so say you

00:10:53,959 --> 00:10:58,909
have two agents and running two

00:10:56,599 --> 00:11:01,369
containers the container has its own IP

00:10:58,909 --> 00:11:04,069
address which is in a subnet which is

00:11:01,369 --> 00:11:06,199
different from the hosts IP address so

00:11:04,069 --> 00:11:07,939
then if container one wants to talk to

00:11:06,199 --> 00:11:10,729
container to the way it happens is

00:11:07,939 --> 00:11:12,529
container one will will form a packet

00:11:10,729 --> 00:11:14,869
with the destination of container twos

00:11:12,529 --> 00:11:16,309
IP address it will send that to

00:11:14,869 --> 00:11:18,649
something called a we tap interface

00:11:16,309 --> 00:11:20,749
which is running in the host we tab

00:11:18,649 --> 00:11:23,949
interface will create this encapsulation

00:11:20,749 --> 00:11:25,939
and send it to a neighboring we tap and

00:11:23,949 --> 00:11:27,769
neighboring VT trevita

00:11:25,939 --> 00:11:30,259
will D capsulate this packet and send it

00:11:27,769 --> 00:11:32,809
to the container two so that's why this

00:11:30,259 --> 00:11:40,189
IP per container is achieved through and

00:11:32,809 --> 00:11:42,109
overlay now the the reason why I said

00:11:40,189 --> 00:11:45,169
that this is a particular CNI

00:11:42,109 --> 00:11:48,679
configuration because it uses a bridge

00:11:45,169 --> 00:11:49,879
CNI plugin for connectivity you if you

00:11:48,679 --> 00:11:51,949
see the previous picture there is a

00:11:49,879 --> 00:11:53,959
bridge involved so this bridge is coming

00:11:51,949 --> 00:11:55,609
from the bridge C and I plug in and then

00:11:53,959 --> 00:11:58,309
it uses something called V excellent

00:11:55,609 --> 00:12:01,669
which is out of shelf encapsulation

00:11:58,309 --> 00:12:03,829
algorithm in Linux kernels the way you

00:12:01,669 --> 00:12:06,589
could use an overlay network is through

00:12:03,829 --> 00:12:08,419
a config dot am I'm sure those who have

00:12:06,589 --> 00:12:10,249
played enough with DC us would have

00:12:08,419 --> 00:12:12,559
encountered this conflict or tml it is

00:12:10,249 --> 00:12:14,749
basically a J's y ml file with the

00:12:12,559 --> 00:12:18,379
initial configuration of your cluster so

00:12:14,749 --> 00:12:20,959
you can define your overlay for your

00:12:18,379 --> 00:12:22,369
cluster so this is the the configuration

00:12:20,959 --> 00:12:24,379
that you see on the screen is the

00:12:22,369 --> 00:12:26,569
default configuration it comes out of

00:12:24,379 --> 00:12:28,849
box but you could or you could change

00:12:26,569 --> 00:12:31,309
this configuration either the the subnet

00:12:28,849 --> 00:12:32,629
or you could add new overlays each

00:12:31,309 --> 00:12:36,199
overlay is defined by a particular

00:12:32,629 --> 00:12:38,539
subnet in this case the subnet is 9/8

00:12:36,199 --> 00:12:42,199
subnet you could have your own overlay

00:12:38,539 --> 00:12:44,300
with a different subnet to give you a

00:12:42,199 --> 00:12:46,400
high-level view of how the

00:12:44,300 --> 00:12:49,690
request or how this overlay set up in

00:12:46,400 --> 00:12:52,190
Indy cos it's let's say that there is a

00:12:49,690 --> 00:12:54,730
there is a agent module that is running

00:12:52,190 --> 00:12:57,650
on each agent as well as on the master

00:12:54,730 --> 00:12:59,960
when agent comes when this overlay

00:12:57,650 --> 00:13:02,810
module comes for the first time it

00:12:59,960 --> 00:13:05,030
registers to the master at the time of

00:13:02,810 --> 00:13:08,120
registration master will take the the

00:13:05,030 --> 00:13:10,520
whole subnet and slice it down into

00:13:08,120 --> 00:13:11,270
equal chance and give this one chunks to

00:13:10,520 --> 00:13:13,370
the agent

00:13:11,270 --> 00:13:15,590
that's how agent gets the IP address so

00:13:13,370 --> 00:13:18,440
the partition of the subnet for overlay

00:13:15,590 --> 00:13:20,960
is very static right then there is a

00:13:18,440 --> 00:13:24,140
utility called EPSA that is running on

00:13:20,960 --> 00:13:26,660
each agent that pulls the local module

00:13:24,140 --> 00:13:29,000
the holy module so when the model on

00:13:26,660 --> 00:13:32,720
local model gets the subnet from the

00:13:29,000 --> 00:13:35,870
master this utility picks up that subnet

00:13:32,720 --> 00:13:38,150
and configures routes appropriately into

00:13:35,870 --> 00:13:40,570
the into the host kernel it also

00:13:38,150 --> 00:13:43,160
gossiped this information to rest of the

00:13:40,570 --> 00:13:44,750
nets that are running on other agents

00:13:43,160 --> 00:13:48,890
and that's how they built a complete

00:13:44,750 --> 00:13:50,600
mesh of network so that each agent knows

00:13:48,890 --> 00:13:53,320
what are the subnets the neighboring

00:13:50,600 --> 00:13:56,210
agent is holding and how to reach there

00:13:53,320 --> 00:13:58,790
so when a task is launched by a

00:13:56,210 --> 00:14:00,680
framework say marathon and the task now

00:13:58,790 --> 00:14:02,420
has their routable IP address because

00:14:00,680 --> 00:14:07,880
the routes are already configured by

00:14:02,420 --> 00:14:11,330
Napster so that was about the about the

00:14:07,880 --> 00:14:13,550
IP connectivity now coming to to service

00:14:11,330 --> 00:14:16,430
discovery service discovering DCOs is

00:14:13,550 --> 00:14:18,770
done through two components one is

00:14:16,430 --> 00:14:24,470
Spartan and one is Miss another is

00:14:18,770 --> 00:14:25,640
missiles DNS for each task or service

00:14:24,470 --> 00:14:28,910
that is launched on

00:14:25,640 --> 00:14:31,340
Decius cluster there is a there is a

00:14:28,910 --> 00:14:34,490
certain amount certain DNS records that

00:14:31,340 --> 00:14:36,790
are created there are a records that are

00:14:34,490 --> 00:14:40,850
created by both Spartan and mesos and

00:14:36,790 --> 00:14:42,650
also the survey records not to give you

00:14:40,850 --> 00:14:44,120
a high level view how how they interact

00:14:42,650 --> 00:14:46,700
with the system or how they create these

00:14:44,120 --> 00:14:49,160
are so called so both massage DNS and

00:14:46,700 --> 00:14:52,070
Spartan that are running on the master

00:14:49,160 --> 00:14:56,020
poles the state dot Jason that is the

00:14:52,070 --> 00:14:56,020
state that is exposed by Messrs

00:14:57,080 --> 00:15:03,210
and then Spartan gets this information

00:15:00,840 --> 00:15:05,520
creates the record necessary records

00:15:03,210 --> 00:15:07,290
that delay NSR we record and gossip this

00:15:05,520 --> 00:15:09,120
information to the rest of the cluster

00:15:07,290 --> 00:15:10,770
so there's a Spartan as I was saying

00:15:09,120 --> 00:15:13,890
earlier there is a Spartan running on

00:15:10,770 --> 00:15:16,470
each agent and they gossip to get the

00:15:13,890 --> 00:15:18,930
complete view of the cluster now when

00:15:16,470 --> 00:15:21,360
when the actual tasks get launched on a

00:15:18,930 --> 00:15:23,670
particular agent and it queries for an

00:15:21,360 --> 00:15:25,920
Indian s record that query is

00:15:23,670 --> 00:15:29,280
intercepted locally by that local

00:15:25,920 --> 00:15:32,760
Spartan and thus responds and respond to

00:15:29,280 --> 00:15:36,510
that container or the service so

00:15:32,760 --> 00:15:38,550
literally the dienes if it is internal

00:15:36,510 --> 00:15:41,310
to the cluster it never leaves the agent

00:15:38,550 --> 00:15:43,140
and that's one of the benefit of having

00:15:41,310 --> 00:15:47,160
Spartan distributed and running on each

00:15:43,140 --> 00:15:49,800
node so Spartan is this venous proxy

00:15:47,160 --> 00:15:53,220
that intercept all the Tina's queries

00:15:49,800 --> 00:15:55,400
that are coming from the services that

00:15:53,220 --> 00:15:58,140
are running on that particular host

00:15:55,400 --> 00:16:00,060
along with being distributed it also has

00:15:58,140 --> 00:16:04,170
another functionality called dual

00:16:00,060 --> 00:16:06,750
dispatch usually the way DNS works is if

00:16:04,170 --> 00:16:08,670
you have couple of name servers then the

00:16:06,750 --> 00:16:11,550
DNS query will go to the first name

00:16:08,670 --> 00:16:13,320
server and for some reason if it is not

00:16:11,550 --> 00:16:15,930
responding it will wait for the timeout

00:16:13,320 --> 00:16:18,470
to happen and then the second DNS it

00:16:15,930 --> 00:16:21,450
will leads to the second DNS name server

00:16:18,470 --> 00:16:23,730
what Spartan does it simultaneously send

00:16:21,450 --> 00:16:26,370
queries to both the name server so that

00:16:23,730 --> 00:16:27,990
we don't waste time in waiting for one

00:16:26,370 --> 00:16:31,050
name server to respond and who's our

00:16:27,990 --> 00:16:33,450
respond first Spartan takes that query

00:16:31,050 --> 00:16:36,000
response and send it to the client so

00:16:33,450 --> 00:16:39,089
that gives a little bit of speed-up in

00:16:36,000 --> 00:16:41,730
DNS resolution beside that Spartan has

00:16:39,089 --> 00:16:44,040
this ability to have authoritive you can

00:16:41,730 --> 00:16:47,070
configure our domain and you can

00:16:44,040 --> 00:16:48,630
configure an upstream for a domain like

00:16:47,070 --> 00:16:52,410
for example if you have an upstream that

00:16:48,630 --> 00:16:54,690
can only handle dot-com as the TLD then

00:16:52,410 --> 00:16:58,980
you could configure multiple up streams

00:16:54,690 --> 00:17:02,820
for different TLDs the way dual dispatch

00:16:58,980 --> 00:17:05,250
works is when there is a query from any

00:17:02,820 --> 00:17:09,520
agent then Spartan intercepts could that

00:17:05,250 --> 00:17:12,370
query but it sends that query to

00:17:09,520 --> 00:17:14,560
to obscene nameserver simultaneously and

00:17:12,370 --> 00:17:17,230
who's ever respond first it takes that

00:17:14,560 --> 00:17:20,589
response send it to the agent and then

00:17:17,230 --> 00:17:22,990
it also stores the metric for future so

00:17:20,589 --> 00:17:25,270
it it remembers which name server

00:17:22,990 --> 00:17:27,970
responded first so that in the next

00:17:25,270 --> 00:17:33,070
query it can choose that name server has

00:17:27,970 --> 00:17:35,790
compared to the slower one so if there

00:17:33,070 --> 00:17:39,220
is a task that is running on an agent

00:17:35,790 --> 00:17:41,320
most of the time the the DNS is are

00:17:39,220 --> 00:17:43,500
local to the cluster so the DNS

00:17:41,320 --> 00:17:46,570
resolution happens local to that agent

00:17:43,500 --> 00:17:48,760
but if it is something like external

00:17:46,570 --> 00:17:49,930
which is like kind of don't calm then it

00:17:48,760 --> 00:17:51,700
goes to the upstream which has

00:17:49,930 --> 00:17:54,130
configured for his partner or if it is

00:17:51,700 --> 00:17:57,610
dot missus then it goes to the missus

00:17:54,130 --> 00:17:59,200
Dean is running on the master so that

00:17:57,610 --> 00:18:02,140
was about service discovery through the

00:17:59,200 --> 00:18:04,120
anus coming to load balancing part so

00:18:02,140 --> 00:18:05,830
load balancing is done at two different

00:18:04,120 --> 00:18:09,220
layer and two by two different

00:18:05,830 --> 00:18:10,810
components so at layer 4 it's also the

00:18:09,220 --> 00:18:14,650
east-west load balancing within the

00:18:10,810 --> 00:18:17,350
cluster then it is done by minutemen the

00:18:14,650 --> 00:18:19,330
layer 7 load balancing is done through a

00:18:17,350 --> 00:18:21,760
comprehend or atonal B which is a

00:18:19,330 --> 00:18:24,120
wrapper around H a proxy so we'll see

00:18:21,760 --> 00:18:27,370
both of them today

00:18:24,120 --> 00:18:30,550
Minutemen Minutemen uses something

00:18:27,370 --> 00:18:33,970
called IP vs load balancer which is in

00:18:30,550 --> 00:18:36,400
Linux kernel so it programs the IP vs

00:18:33,970 --> 00:18:38,980
entry into the Linux kernel and there

00:18:36,400 --> 00:18:40,720
also the control plane is done by the

00:18:38,980 --> 00:18:42,970
minutemen but the data plane is entirely

00:18:40,720 --> 00:18:44,740
in kernel the full load balancing is

00:18:42,970 --> 00:18:46,560
happening in in the kernel itself and

00:18:44,740 --> 00:18:49,150
the algorithm that we use today is

00:18:46,560 --> 00:18:51,160
weighted least connection but the weight

00:18:49,150 --> 00:18:54,640
is one for all the connection so it's

00:18:51,160 --> 00:18:58,300
pretty much the least connection the way

00:18:54,640 --> 00:19:00,700
you use web in DC OS is is through app

00:18:58,300 --> 00:19:02,860
definition when you launch an app

00:19:00,700 --> 00:19:05,370
through marathon you need to provide

00:19:02,860 --> 00:19:07,990
this JSON file which contains your

00:19:05,370 --> 00:19:10,360
configuration for the app and in that

00:19:07,990 --> 00:19:13,300
you could specify a label saying web

00:19:10,360 --> 00:19:16,270
which with the web name the actual web

00:19:13,300 --> 00:19:18,730
that the whole the fuel fully-qualified

00:19:16,270 --> 00:19:20,080
deenis that is generated is is the

00:19:18,730 --> 00:19:23,090
bottom of the screen which is web

00:19:20,080 --> 00:19:26,000
service at marathon dot l4 lb test

00:19:23,090 --> 00:19:28,850
us directory the reason the name is so

00:19:26,000 --> 00:19:31,640
big because we wanted to have an ability

00:19:28,850 --> 00:19:32,810
for multiple frameworks to have same app

00:19:31,640 --> 00:19:34,430
with the same name

00:19:32,810 --> 00:19:37,040
for example web server could be with

00:19:34,430 --> 00:19:39,620
some other framework so the the way the

00:19:37,040 --> 00:19:42,740
DNS is expanded is there is a service

00:19:39,620 --> 00:19:48,730
name dot framework name then dot L 4lb

00:19:42,740 --> 00:19:51,020
dis dcs and directory at a high level

00:19:48,730 --> 00:19:53,020
let's say there is a task that is

00:19:51,020 --> 00:19:56,210
launched by marathon on a particular

00:19:53,020 --> 00:19:59,570
agent so it will it will convey that

00:19:56,210 --> 00:20:01,580
information to the master master will

00:19:59,570 --> 00:20:06,290
select one of the agent based on the

00:20:01,580 --> 00:20:07,760
resources and so the agent is so let's

00:20:06,290 --> 00:20:09,650
say the agent selected by master is

00:20:07,760 --> 00:20:14,500
agent one and the label that you are

00:20:09,650 --> 00:20:18,560
seeing foo colon 5000 is the actual web

00:20:14,500 --> 00:20:20,240
but the task so the whip whip port is

00:20:18,560 --> 00:20:22,250
different from the actual port on which

00:20:20,240 --> 00:20:23,810
the task is running in this particular

00:20:22,250 --> 00:20:27,200
case it is let's say it's six seven

00:20:23,810 --> 00:20:31,430
eight nine minutemen which is running on

00:20:27,200 --> 00:20:33,830
agent one locally pulls the state of

00:20:31,430 --> 00:20:36,170
messes on that particular agent and

00:20:33,830 --> 00:20:39,680
that's how it learns that there is a

00:20:36,170 --> 00:20:42,380
task launched by a whip what it does it

00:20:39,680 --> 00:20:44,960
gossip this information to all the

00:20:42,380 --> 00:20:47,630
minutemen so that they can also create a

00:20:44,960 --> 00:20:50,840
record the and they can also program

00:20:47,630 --> 00:20:52,760
their IP vs entries so each minute meant

00:20:50,840 --> 00:20:55,580
when it gets this information that there

00:20:52,760 --> 00:20:58,520
is a task launched by a launch whip and

00:20:55,580 --> 00:21:01,520
it needs a load balancing it takes that

00:20:58,520 --> 00:21:06,080
and assign an IP address which is local

00:21:01,520 --> 00:21:09,160
to that agent and it also programs the

00:21:06,080 --> 00:21:15,710
kernel with with that information so the

00:21:09,160 --> 00:21:18,410
so the web IP is 1 2 3 4 : 500 : 5,000

00:21:15,710 --> 00:21:23,630
and then the actual backend is the task

00:21:18,410 --> 00:21:26,120
X with the actual photo now when tasks

00:21:23,630 --> 00:21:29,540
to say randomly it wants to connect to

00:21:26,120 --> 00:21:34,640
task 1 through a whip it it first

00:21:29,540 --> 00:21:36,170
queries the DNS for that web the the DNS

00:21:34,640 --> 00:21:36,830
is intercepted the DNS query is

00:21:36,170 --> 00:21:39,740
intercepted

00:21:36,830 --> 00:21:42,019
Spartan and responded back to this agent

00:21:39,740 --> 00:21:48,519
with the actual IP address that is the

00:21:42,019 --> 00:21:48,519
VAP IP address : 5,000 and then when it

00:21:50,200 --> 00:21:55,190
when it tries to connect to that web the

00:21:53,269 --> 00:21:57,529
IP vs entry that was created by

00:21:55,190 --> 00:22:00,110
Minutemen in the kernel intercept that

00:21:57,529 --> 00:22:03,529
connection request and forwards it to

00:22:00,110 --> 00:22:04,640
the actual task that is X : 6 7 8 9

00:22:03,529 --> 00:22:06,620
that's all

00:22:04,640 --> 00:22:10,880
the load balancing is working and in

00:22:06,620 --> 00:22:13,059
Decius now coming to layer 7 load

00:22:10,880 --> 00:22:15,620
balancing which is done by marathon lb

00:22:13,059 --> 00:22:19,220
as I said earlier it's a wrapper around

00:22:15,620 --> 00:22:21,169
H a proxy so it takes the configuration

00:22:19,220 --> 00:22:23,510
basically it hooks itself to the

00:22:21,169 --> 00:22:24,919
marathon event bus so as soon as there

00:22:23,510 --> 00:22:28,669
is a application that is launched by

00:22:24,919 --> 00:22:33,830
marathon it gets the event and then it

00:22:28,669 --> 00:22:36,620
programs the H a proxy accordingly so as

00:22:33,830 --> 00:22:38,870
so if a client connects to marathon LB

00:22:36,620 --> 00:22:40,970
marathon I'll be already watching for

00:22:38,870 --> 00:22:43,940
for the tasks that are launched by

00:22:40,970 --> 00:22:47,210
marathon it configures the H a proxy for

00:22:43,940 --> 00:22:48,919
those tasks and then when the client

00:22:47,210 --> 00:22:53,659
actually connects it load-balanced them

00:22:48,919 --> 00:22:55,460
for these tasks the way you use marathon

00:22:53,659 --> 00:22:58,519
al view of the way you instruct marathon

00:22:55,460 --> 00:23:01,070
lb for any particular task is again

00:22:58,519 --> 00:23:03,590
through the app definition in marathon

00:23:01,070 --> 00:23:06,169
and the and you have to specify labels

00:23:03,590 --> 00:23:08,299
those they are like bunch of labels for

00:23:06,169 --> 00:23:11,179
different configurations so everything

00:23:08,299 --> 00:23:13,220
that NH a proxy can be configured with

00:23:11,179 --> 00:23:15,710
is there is a corresponding label for

00:23:13,220 --> 00:23:18,230
that so in this case there are two

00:23:15,710 --> 00:23:20,269
labels that I'm showing one is the

00:23:18,230 --> 00:23:21,889
external which says that the

00:23:20,269 --> 00:23:23,990
configuration that you're creating is

00:23:21,889 --> 00:23:31,130
for the external client and the second

00:23:23,990 --> 00:23:32,630
is second is the V host this is the DNS

00:23:31,130 --> 00:23:39,230
at which the external client would be

00:23:32,630 --> 00:23:41,600
connecting right and this is something

00:23:39,230 --> 00:23:45,620
that we are working on currently and

00:23:41,600 --> 00:23:47,289
should have support in future so we are

00:23:45,620 --> 00:23:50,660
working on something called ipv6 support

00:23:47,289 --> 00:23:53,240
so far our dcs network stack is

00:23:50,660 --> 00:23:56,330
only ipv4 but going forward it will

00:23:53,240 --> 00:23:58,160
support both I feel for an ipv6 then we

00:23:56,330 --> 00:24:02,690
are working on something called CNI

00:23:58,160 --> 00:24:06,830
speck versions 0.3 which introduced a

00:24:02,690 --> 00:24:12,190
nice concept called service chaining so

00:24:06,830 --> 00:24:15,320
imagine that you want different services

00:24:12,190 --> 00:24:17,870
to use from different CNI provider today

00:24:15,320 --> 00:24:20,330
you cannot really mix and match those

00:24:17,870 --> 00:24:22,250
CNI providers into a single virtual

00:24:20,330 --> 00:24:25,100
network but with service chaining you

00:24:22,250 --> 00:24:29,530
would be able to do that and then a

00:24:25,100 --> 00:24:32,840
support of multi-tenancy today any

00:24:29,530 --> 00:24:35,750
operator can use any virtual network to

00:24:32,840 --> 00:24:39,770
launch their container we want to make

00:24:35,750 --> 00:24:41,660
it more secure in such a way that an

00:24:39,770 --> 00:24:45,200
operator should be able to define rules

00:24:41,660 --> 00:24:47,060
and only certain users or operator can

00:24:45,200 --> 00:24:49,010
launch containers or certain networks

00:24:47,060 --> 00:24:52,160
and should be forbidden to launch

00:24:49,010 --> 00:24:56,870
container on others so this is something

00:24:52,160 --> 00:25:01,970
we are working on building and finally

00:24:56,870 --> 00:25:04,460
the the stack that we started with so

00:25:01,970 --> 00:25:07,510
this concludes my talk today and I open

00:25:04,460 --> 00:25:07,510
the floor for Question and Answer

00:25:07,570 --> 00:25:18,830
thanks Deepak for questions we have a

00:25:11,060 --> 00:25:21,520
microphone here do we have questions one

00:25:18,830 --> 00:25:21,520
question there

00:25:26,720 --> 00:25:32,810
I thank you so I have a questions

00:25:29,770 --> 00:25:35,510
specifically to this us barton component

00:25:32,810 --> 00:25:39,020
you are saying that you can configure it

00:25:35,510 --> 00:25:41,210
to use the tool dispatch mode so in that

00:25:39,020 --> 00:25:43,250
particular case I'm interested if you

00:25:41,210 --> 00:25:45,340
are limited to the number of upstream

00:25:43,250 --> 00:25:49,070
DNS resolvers that you can configure

00:25:45,340 --> 00:25:51,170
because as far as I know it KEK the dual

00:25:49,070 --> 00:25:54,430
dispatch is made to two separate master

00:25:51,170 --> 00:25:57,050
nodes right so what happens if I

00:25:54,430 --> 00:25:59,870
accidentally or if I configure three

00:25:57,050 --> 00:26:02,960
upstream resolvers and it selects just

00:25:59,870 --> 00:26:04,700
the two ones that don't resolve that

00:26:02,960 --> 00:26:05,120
particularly records that I'm looking

00:26:04,700 --> 00:26:08,260
for

00:26:05,120 --> 00:26:12,620
so am i limited to just two in this case

00:26:08,260 --> 00:26:14,330
so that's a good question so let's say

00:26:12,620 --> 00:26:16,100
you have multiple upstream that you have

00:26:14,330 --> 00:26:18,710
configured yeah it will randomly select

00:26:16,100 --> 00:26:22,790
two each time when it does the selection

00:26:18,710 --> 00:26:25,580
it does it randomly so let's say if two

00:26:22,790 --> 00:26:27,680
it's selected unfortunately both of them

00:26:25,580 --> 00:26:29,900
are not responding then it will remember

00:26:27,680 --> 00:26:32,120
the next time when it selects then it

00:26:29,900 --> 00:26:34,370
will give lower weight age to these two

00:26:32,120 --> 00:26:36,740
and it will select the third one so over

00:26:34,370 --> 00:26:39,020
time it will learn that there is an

00:26:36,740 --> 00:26:40,790
upstream that always respond and it will

00:26:39,020 --> 00:26:41,510
always include that upstream in the dual

00:26:40,790 --> 00:26:44,300
dispatch

00:26:41,510 --> 00:26:46,310
but yes initially if your upstreams are

00:26:44,300 --> 00:26:49,160
not responding then at least one two

00:26:46,310 --> 00:26:50,780
trial will it will take two to get that

00:26:49,160 --> 00:26:57,890
information that your upstreams are not

00:26:50,780 --> 00:27:03,280
responding back okay thank you other

00:26:57,890 --> 00:27:07,370
questions I wondering if you are

00:27:03,280 --> 00:27:10,310
considering switching from ipfs to EBP F

00:27:07,370 --> 00:27:16,540
in Minutemen are considering this

00:27:10,310 --> 00:27:20,480
solution IP vs - IP V s to EBP F yes so

00:27:16,540 --> 00:27:23,210
we want to the only challenge there is

00:27:20,480 --> 00:27:25,970
that ebf require certain minimum amount

00:27:23,210 --> 00:27:30,140
of minimum version of kernel like Linux

00:27:25,970 --> 00:27:31,850
kernel I think it is 4.4.1 thing but

00:27:30,140 --> 00:27:34,340
many of our customers are running really

00:27:31,850 --> 00:27:37,070
old colonel there we do not have a

00:27:34,340 --> 00:27:39,620
support for a b PF but we are moving

00:27:37,070 --> 00:27:42,320
like we want to provide this support

00:27:39,620 --> 00:27:44,509
for the Colonel's that are already there

00:27:42,320 --> 00:27:45,830
so customers should be free like in

00:27:44,509 --> 00:27:48,470
future they will be free to choose

00:27:45,830 --> 00:27:50,090
whether they want to use IP vs or it be

00:27:48,470 --> 00:27:59,169
PF depending on the support in the

00:27:50,090 --> 00:27:59,169
corner okay thank you are the questions

00:28:08,480 --> 00:28:14,310
comes to marathon I'll be uh

00:28:11,430 --> 00:28:17,490
when you have a syntactic error in the

00:28:14,310 --> 00:28:19,830
labels or the H a proxy won't restart

00:28:17,490 --> 00:28:23,970
and so the cluster isn't reachable

00:28:19,830 --> 00:28:26,820
anymore are there any plans to work on

00:28:23,970 --> 00:28:30,390
this sorry I didn't get the question

00:28:26,820 --> 00:28:32,490
there when essentially when you deploy a

00:28:30,390 --> 00:28:36,000
container with a label that has a

00:28:32,490 --> 00:28:38,460
syntactical arrow okay and so H a proxy

00:28:36,000 --> 00:28:40,110
won't restart right oh yeah your whole

00:28:38,460 --> 00:28:44,780
cluster isn't reachable anymore are

00:28:40,110 --> 00:28:44,780
there any plans to work around this I

00:28:46,130 --> 00:28:56,070
guess not like you need to fix that

00:28:50,040 --> 00:28:58,560
problem right but you see the impact

00:28:56,070 --> 00:29:01,230
your one developer sets a label and a

00:28:58,560 --> 00:29:04,530
whole cluster for so you need you're

00:29:01,230 --> 00:29:06,330
saying is there a way the validation can

00:29:04,530 --> 00:29:09,600
happen before everything goes right

00:29:06,330 --> 00:29:11,250
right yeah just like a dummy restart or

00:29:09,600 --> 00:29:13,170
something like that yeah

00:29:11,250 --> 00:29:16,320
I mean that's a good point today we

00:29:13,170 --> 00:29:18,690
don't do any validation at least on the

00:29:16,320 --> 00:29:23,460
AB definition level we could we could

00:29:18,690 --> 00:29:25,320
think about adding that or that MLB is

00:29:23,460 --> 00:29:27,020
like an open source project if you have

00:29:25,320 --> 00:29:30,540
something in mind you could also

00:29:27,020 --> 00:29:32,490
contribute to that I think that's a pull

00:29:30,540 --> 00:29:38,190
request for that issue pending since a

00:29:32,490 --> 00:29:40,950
few months okay I think we can also

00:29:38,190 --> 00:29:42,290
write a grammarly plugin every second

00:29:40,950 --> 00:29:44,670
time I watch a YouTube video it

00:29:42,290 --> 00:29:49,920
recommends me to use grammarly maybe it

00:29:44,670 --> 00:29:56,420
will help you in this case other

00:29:49,920 --> 00:29:56,420
questions one in front

00:29:59,250 --> 00:30:06,480
um hi I have a purely almost theoretical

00:30:02,730 --> 00:30:12,539
question so what if I do not want to use

00:30:06,480 --> 00:30:16,110
IV per container what if I can I use

00:30:12,539 --> 00:30:20,929
Network isolated without that and let's

00:30:16,110 --> 00:30:23,820
say that I'm in high-performance trading

00:30:20,929 --> 00:30:27,570
where the distance between the rotor and

00:30:23,820 --> 00:30:30,960
the rack actually matters and I want to

00:30:27,570 --> 00:30:35,010
bind the container to than to a network

00:30:30,960 --> 00:30:38,870
card how does C&I help me there and also

00:30:35,010 --> 00:30:42,440
what is the performance hit when I use

00:30:38,870 --> 00:30:46,250
one of the well for example the proposed

00:30:42,440 --> 00:30:50,150
bridge overlay network right okay

00:30:46,250 --> 00:30:54,419
so there are a couple of questions that

00:30:50,150 --> 00:30:56,100
the first question is where the cni

00:30:54,419 --> 00:30:58,950
network will help you to connect to a

00:30:56,100 --> 00:31:02,220
particular network card right yes the

00:30:58,950 --> 00:31:05,070
answer is yes actually the way C&I works

00:31:02,220 --> 00:31:08,070
is it the logic of how the network has

00:31:05,070 --> 00:31:09,929
to be done is in the plug-in itself so

00:31:08,070 --> 00:31:12,090
as long as plugin has a particular law

00:31:09,929 --> 00:31:15,150
deck as how it want to layout the

00:31:12,090 --> 00:31:19,020
network it will work with misses and DC

00:31:15,150 --> 00:31:21,179
was because as I was saying the isolator

00:31:19,020 --> 00:31:23,250
the network isolator is simply creating

00:31:21,179 --> 00:31:25,799
the network namespace and handing over

00:31:23,250 --> 00:31:27,780
this namespace to CN I plug in now it's

00:31:25,799 --> 00:31:29,730
up to the plug-in how it wants to

00:31:27,780 --> 00:31:31,679
connect so it can pick up a particular

00:31:29,730 --> 00:31:34,470
network card and connects your container

00:31:31,679 --> 00:31:36,450
to that now coming to second question

00:31:34,470 --> 00:31:40,740
what is the performance hit of using

00:31:36,450 --> 00:31:43,980
bridge mode or overlay mode so so both

00:31:40,740 --> 00:31:45,570
bridge so from the technical point of

00:31:43,980 --> 00:31:48,179
view we need to understand where the

00:31:45,570 --> 00:31:49,830
performance hit might come the proform

00:31:48,179 --> 00:31:51,600
in in in networking world the

00:31:49,830 --> 00:31:54,240
performance it always happens if you

00:31:51,600 --> 00:31:56,940
have to copy the packet the actual

00:31:54,240 --> 00:31:59,340
packet usually why these switches and

00:31:56,940 --> 00:32:01,380
routers work blazingly fast is because

00:31:59,340 --> 00:32:04,350
they always work on headers they never

00:32:01,380 --> 00:32:06,120
pull the entire packet in memory but if

00:32:04,350 --> 00:32:07,860
you have to pull the entire packet in

00:32:06,120 --> 00:32:10,169
memory because you are doing some sort

00:32:07,860 --> 00:32:11,920
of an ad or you are doing some sort of

00:32:10,169 --> 00:32:13,750
encapsulation then you

00:32:11,920 --> 00:32:16,170
we'll have a performance set and usually

00:32:13,750 --> 00:32:19,210
the performance hit is like 40 percent

00:32:16,170 --> 00:32:21,610
we just mailed mainly because of the

00:32:19,210 --> 00:32:23,500
fact that you you are taking the entire

00:32:21,610 --> 00:32:26,790
packet and encapsulating into another

00:32:23,500 --> 00:32:29,350
packet that's with overlay with bridge

00:32:26,790 --> 00:32:32,020
the Linux bridge implementation has

00:32:29,350 --> 00:32:35,080
improved quite a bit but it still has a

00:32:32,020 --> 00:32:37,780
hit again because of the copying of the

00:32:35,080 --> 00:32:42,280
packet now there are implementation like

00:32:37,780 --> 00:32:46,120
there is there is this fast bridge in

00:32:42,280 --> 00:32:49,240
Obion which which has in removed some of

00:32:46,120 --> 00:32:52,990
these performance hit so you could use

00:32:49,240 --> 00:32:54,810
that for the performance improvement but

00:32:52,990 --> 00:32:57,460
as long as you are using Linux bridge

00:32:54,810 --> 00:33:01,710
native Linux implementation or the

00:32:57,460 --> 00:33:01,710
overlay you'll have a performance it

00:33:01,950 --> 00:33:09,870
does this answer all the questions do

00:33:06,910 --> 00:33:09,870
you have other questions

00:33:13,330 --> 00:33:22,230
then I'm alright I think we can thank

00:33:17,200 --> 00:33:22,230

YouTube URL: https://www.youtube.com/watch?v=QG0zSr5nyRE


