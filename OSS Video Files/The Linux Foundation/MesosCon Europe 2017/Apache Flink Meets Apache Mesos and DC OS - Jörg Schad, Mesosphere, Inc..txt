Title: Apache Flink Meets Apache Mesos and DC OS - Jörg Schad, Mesosphere, Inc.
Publication date: 2017-10-31
Playlist: MesosCon Europe 2017
Description: 
	Apache Flink Meets Apache Mesos and DC/OS - Jörg Schad, Mesosphere, Inc.

Apache Mesos allows operators to run distributed applications across an entire datacenter and is attracting ever increasing interest. As much as distributed applications see increased use enabled by Mesos, Mesos also sees increasing use due to a growing ecosystem of well integrated applications. One of the latest additions to the Mesos family is Apache Flink. 

Flink is one of the most popular open source systems for real-time high scale data processing and allows users to deal with low-latency streaming analytical workloads on Mesos. 

In this talk we explain the challenges solved while integrating Flink with Mesos, including how Flink’s distributed architecture can be modeled as a Mesos framework, and how Flink was integrated with Fenzo. Next, we describe how Flink was packaged to easily run on DC/OS.

About Jörg Schad
Jörg is a software engineer at Mesosphere in Hamburg. In his previous life he implemented distributed and in memory databases and conducted research in the Hadoop and Cloud area. His speaking experience includes various Meetups, international conferences, and lecture halls.
Captions: 
	00:00:00,410 --> 00:00:03,270
[Music]

00:00:00,810 --> 00:00:05,790
yeah welcome to the first session this

00:00:03,270 --> 00:00:09,750
morning and what I would like to talk

00:00:05,790 --> 00:00:12,420
about is Apache flink on Apache missiles

00:00:09,750 --> 00:00:15,690
so maybe just a quick raise of hands how

00:00:12,420 --> 00:00:18,289
many of you have used flink before how

00:00:15,690 --> 00:00:21,619
many of you have used flink on missiles

00:00:18,289 --> 00:00:24,390
one okay let's maybe change that a bit

00:00:21,619 --> 00:00:26,570
so why I'm actually really excited about

00:00:24,390 --> 00:00:30,330
this project is because it's a really

00:00:26,570 --> 00:00:32,270
it's one of those showcases of community

00:00:30,330 --> 00:00:35,219
we're coming together so this Apache

00:00:32,270 --> 00:00:37,610
Fling scheduler which is being written

00:00:35,219 --> 00:00:43,320
that's a collaboration of people from

00:00:37,610 --> 00:00:45,480
light band EMC data artisans and us

00:00:43,320 --> 00:00:48,239
mesosphere and they are actually also

00:00:45,480 --> 00:00:49,980
always other like individual community

00:00:48,239 --> 00:00:52,410
contributions so this is why I

00:00:49,980 --> 00:00:53,879
especially like this project quite a lot

00:00:52,410 --> 00:00:56,750
because it's actually a lot of people

00:00:53,879 --> 00:00:59,670
coming together developing really cool

00:00:56,750 --> 00:01:04,350
software really cool integrations of two

00:00:59,670 --> 00:01:06,570
open-source projects this talk till he's

00:01:04,350 --> 00:01:08,460
working with data artisans and he is

00:01:06,570 --> 00:01:10,500
actually right now they're having a

00:01:08,460 --> 00:01:12,540
release which we'll see in the end head

00:01:10,500 --> 00:01:15,330
which has some pretty cool new features

00:01:12,540 --> 00:01:17,549
even giving us more power when running

00:01:15,330 --> 00:01:19,229
on mesas giving us more elasticity and

00:01:17,549 --> 00:01:22,770
this is why he unfortunately couldn't

00:01:19,229 --> 00:01:25,890
make it here to mrs. Khan but he still

00:01:22,770 --> 00:01:29,729
he's like part of this presentation in

00:01:25,890 --> 00:01:32,070
heart if we look back at computing if we

00:01:29,729 --> 00:01:35,490
look back like 10 maybe even like 15

00:01:32,070 --> 00:01:37,619
years time flies it was pretty simple we

00:01:35,490 --> 00:01:39,119
had like one option if we were open

00:01:37,619 --> 00:01:41,579
source if we weren't Google or Facebook

00:01:39,119 --> 00:01:44,399
and this was like built a huge Hadoop

00:01:41,579 --> 00:01:47,430
MapReduce cluster and crunch your data

00:01:44,399 --> 00:01:49,229
so that kind of worked we didn't have

00:01:47,430 --> 00:01:52,049
much other choices and that was

00:01:49,229 --> 00:01:53,670
basically our one big cluster nowadays

00:01:52,049 --> 00:01:55,979
it's unfortunately it's a little more

00:01:53,670 --> 00:01:58,469
complex and we need to turn faster we

00:01:55,979 --> 00:02:00,960
have realized that MapReduce isn't

00:01:58,469 --> 00:02:04,140
really efficient isn't really fast in

00:02:00,960 --> 00:02:06,240
latency when it comes down to latency so

00:02:04,140 --> 00:02:09,209
we actually we need something faster

00:02:06,240 --> 00:02:13,260
here what do we actually mean by faster

00:02:09,209 --> 00:02:13,920
and so what we often see is that people

00:02:13,260 --> 00:02:15,690
implement

00:02:13,920 --> 00:02:19,050
something which is called to smack stack

00:02:15,690 --> 00:02:22,050
so this Mack stack this is basically

00:02:19,050 --> 00:02:23,730
this typical iteration if I want to

00:02:22,050 --> 00:02:26,489
implement something which needs to be

00:02:23,730 --> 00:02:29,130
faster like on the left side I now have

00:02:26,489 --> 00:02:29,580
events where if we look back to Map

00:02:29,130 --> 00:02:31,920
Reduce

00:02:29,580 --> 00:02:34,470
it often were like large chunks of batch

00:02:31,920 --> 00:02:37,230
data we were collecting over either the

00:02:34,470 --> 00:02:38,700
entire month over the week but now when

00:02:37,230 --> 00:02:40,560
we're talking about fascinated we are

00:02:38,700 --> 00:02:43,800
more talking about events and events

00:02:40,560 --> 00:02:46,440
that could be for example credit card

00:02:43,800 --> 00:02:48,780
transactions that can be a plane that

00:02:46,440 --> 00:02:50,730
can be any kind of infrastructure which

00:02:48,780 --> 00:02:52,170
has sensors attached self-driving cars

00:02:50,730 --> 00:02:54,720
probably come to mind

00:02:52,170 --> 00:02:56,220
uber and all those technologies but

00:02:54,720 --> 00:02:59,580
you're actually collecting a lot of data

00:02:56,220 --> 00:03:01,140
in real time and with such kind of

00:02:59,580 --> 00:03:03,420
systems the infrastructure becomes a

00:03:01,140 --> 00:03:05,819
little more complex so all the sudden I

00:03:03,420 --> 00:03:08,730
can't simply write it all to a big file

00:03:05,819 --> 00:03:10,680
and then digest it at one point because

00:03:08,730 --> 00:03:12,930
I want to do that in real time so what

00:03:10,680 --> 00:03:15,750
I'll usually do instead is I'll write it

00:03:12,930 --> 00:03:18,720
into an ingestion queue which could be

00:03:15,750 --> 00:03:20,940
for example Apache Kafka and then I

00:03:18,720 --> 00:03:23,850
actually have my analytics layer which

00:03:20,940 --> 00:03:26,700
could be spark which could be fling or

00:03:23,850 --> 00:03:29,760
any tool I actually want here and the

00:03:26,700 --> 00:03:33,329
results isin usually stores somewhere in

00:03:29,760 --> 00:03:34,829
some distributed data store and storing

00:03:33,329 --> 00:03:37,290
of course it's not sufficient by itself

00:03:34,829 --> 00:03:39,650
I actually have to act upon this as well

00:03:37,290 --> 00:03:42,030
so usually I also in such kind of

00:03:39,650 --> 00:03:45,870
infrastructure for processing fast agent

00:03:42,030 --> 00:03:47,670
I also have like an actor which often is

00:03:45,870 --> 00:03:49,500
implemented occur because it's really

00:03:47,670 --> 00:03:53,370
nice but there are also many different

00:03:49,500 --> 00:03:56,130
other implementations and with this

00:03:53,370 --> 00:03:58,260
stack we actually we're fulfilling like

00:03:56,130 --> 00:04:00,959
one part of this picture which would be

00:03:58,260 --> 00:04:03,840
the event processing part but if we look

00:04:00,959 --> 00:04:06,810
at the overall picture of our overall

00:04:03,840 --> 00:04:09,150
data analytics needs it's not just this

00:04:06,810 --> 00:04:09,690
mac stack it's our it's not just this

00:04:09,150 --> 00:04:12,150
fast

00:04:09,690 --> 00:04:14,370
event processing data it's actually it's

00:04:12,150 --> 00:04:17,820
more we still have used cases for batch

00:04:14,370 --> 00:04:20,160
data for example where cases where don't

00:04:17,820 --> 00:04:22,140
really care about latency where I can't

00:04:20,160 --> 00:04:24,630
collect all my data and then I run it at

00:04:22,140 --> 00:04:26,550
the end of C months which like best

00:04:24,630 --> 00:04:27,750
effort on my infrastructure basically

00:04:26,550 --> 00:04:29,790
just maxing out

00:04:27,750 --> 00:04:32,250
whatever compute resources I have left

00:04:29,790 --> 00:04:35,190
from my other services then for many

00:04:32,250 --> 00:04:37,920
other services so for example if I'm

00:04:35,190 --> 00:04:39,750
Amazon I want to show you new product

00:04:37,920 --> 00:04:42,690
recommendations I want to update my

00:04:39,750 --> 00:04:44,820
real-time pricing depending on what

00:04:42,690 --> 00:04:47,850
users are willing to pay or how many

00:04:44,820 --> 00:04:49,470
users are actually I want something fast

00:04:47,850 --> 00:04:51,690
to be cruises then I cannot wait for

00:04:49,470 --> 00:04:54,540
days or hours to actually update that

00:04:51,690 --> 00:04:57,630
but minutes and maybe like tens of

00:04:54,540 --> 00:05:00,050
seconds is actually okay so that's an

00:04:57,630 --> 00:05:03,540
what we usually talk about micro batches

00:05:00,050 --> 00:05:06,120
and then this last use case we're

00:05:03,540 --> 00:05:08,580
actually need response times immediately

00:05:06,120 --> 00:05:10,590
so if I'm using my credit card here in

00:05:08,580 --> 00:05:12,930
product I don't want to wait for ten or

00:05:10,590 --> 00:05:14,460
twenty seconds until the bank also rises

00:05:12,930 --> 00:05:16,680
as some says yeah this is an okay

00:05:14,460 --> 00:05:20,790
transaction I want an answer as quickly

00:05:16,680 --> 00:05:23,550
as possible so as we've seen their use

00:05:20,790 --> 00:05:25,770
cases for actually many of those in our

00:05:23,550 --> 00:05:28,170
infrastructure so it's not just reducing

00:05:25,770 --> 00:05:31,170
to one of them we actually we need all

00:05:28,170 --> 00:05:36,510
of this for some kind of scenarios or in

00:05:31,170 --> 00:05:40,620
most scenarios and for me this Mac stack

00:05:36,510 --> 00:05:42,960
so coming back to this PAC picture smack

00:05:40,620 --> 00:05:46,470
stack actually as you might have guessed

00:05:42,960 --> 00:05:50,490
is comes from this individual name so

00:05:46,470 --> 00:05:52,620
the S actually stands for SPARC the M

00:05:50,490 --> 00:05:55,320
actually stands for Apache missiles as

00:05:52,620 --> 00:05:58,380
we here at mrs. Khan CA stands for akka

00:05:55,320 --> 00:06:00,440
C stands for Cassandra and Kay is

00:05:58,380 --> 00:06:03,180
representing Kafka in our case and

00:06:00,440 --> 00:06:05,070
through me if I'm talking about this mac

00:06:03,180 --> 00:06:08,010
stack it's not so much about those

00:06:05,070 --> 00:06:10,080
individual technologies making up the

00:06:08,010 --> 00:06:12,360
name because actually in each of those

00:06:10,080 --> 00:06:14,340
layers we have a different number of

00:06:12,360 --> 00:06:17,790
options to implement them differently

00:06:14,340 --> 00:06:20,250
and here actually would like to take a

00:06:17,790 --> 00:06:23,490
look at the analytics layer what kind of

00:06:20,250 --> 00:06:25,919
options we have here so if we look at

00:06:23,490 --> 00:06:28,080
stream processing even though these

00:06:25,919 --> 00:06:29,910
Mac's tag is named after the first

00:06:28,080 --> 00:06:33,300
letter comes from spark there are

00:06:29,910 --> 00:06:35,130
actually many other options so first one

00:06:33,300 --> 00:06:38,100
probably around there being used for its

00:06:35,130 --> 00:06:41,169
Apache storm then spark is very common

00:06:38,100 --> 00:06:44,080
still probably the most favorite tools

00:06:41,169 --> 00:06:47,770
to be used here but we also have other

00:06:44,080 --> 00:06:50,169
tools as samsa fling or apex becoming

00:06:47,770 --> 00:06:53,110
really popular right now I was really

00:06:50,169 --> 00:06:55,120
happy to see the APEC stickers at the

00:06:53,110 --> 00:06:57,789
booth over at the apache boost for

00:06:55,120 --> 00:07:00,069
example and then if we also take a look

00:06:57,789 --> 00:07:02,409
at those different cloud providers we

00:07:00,069 --> 00:07:04,240
are having most of them are actually

00:07:02,409 --> 00:07:07,629
offering their own solutions like

00:07:04,240 --> 00:07:10,240
kinases data flow so all of a sudden I

00:07:07,629 --> 00:07:13,569
actually have a lot of choice to choose

00:07:10,240 --> 00:07:15,370
from so if we take a look at spark and I

00:07:13,569 --> 00:07:17,889
said spark is probably still the most

00:07:15,370 --> 00:07:20,379
commonly used one and that's probably

00:07:17,889 --> 00:07:22,719
for this reason because it's not just

00:07:20,379 --> 00:07:25,360
for stream processing a lot of people

00:07:22,719 --> 00:07:27,159
have already implemented jobs in spark

00:07:25,360 --> 00:07:28,719
for example for their batch processing

00:07:27,159 --> 00:07:30,580
they already have a set up

00:07:28,719 --> 00:07:32,589
infrastructure which they might use for

00:07:30,580 --> 00:07:35,409
any of those use cases up there so

00:07:32,589 --> 00:07:37,749
machine learning graph processing even

00:07:35,409 --> 00:07:40,180
like spark sequel so there's actually an

00:07:37,749 --> 00:07:42,400
entire ecosystem around spark and spark

00:07:40,180 --> 00:07:47,560
streaming is just one part which nicely

00:07:42,400 --> 00:07:50,110
fits into this picture if we look at

00:07:47,560 --> 00:07:52,930
sparks dreaming or actually as I was

00:07:50,110 --> 00:07:55,060
just at spark summit we are mostly

00:07:52,930 --> 00:07:58,509
talking about Spock streaming 2.0

00:07:55,060 --> 00:08:00,669
nowadays so what's kind of the

00:07:58,509 --> 00:08:02,469
implementation detail which is also

00:08:00,669 --> 00:08:04,689
important if you consider whether it's

00:08:02,469 --> 00:08:06,460
suitable for you or not it's actually

00:08:04,689 --> 00:08:09,009
the way in which SPARC is dealing with

00:08:06,460 --> 00:08:11,259
that spark has been originally been

00:08:09,009 --> 00:08:14,020
written as this batch processor and they

00:08:11,259 --> 00:08:17,080
actually utilized the same code pass and

00:08:14,020 --> 00:08:19,599
the same structure for their streaming

00:08:17,080 --> 00:08:22,960
jobs and how they do that is they

00:08:19,599 --> 00:08:24,729
actually collect individual tuples so

00:08:22,960 --> 00:08:27,039
they might for example collect five

00:08:24,729 --> 00:08:29,830
tuples and then they will actually go

00:08:27,039 --> 00:08:32,079
and process this micro batch similar as

00:08:29,830 --> 00:08:34,930
they would with a large batch of data so

00:08:32,079 --> 00:08:37,719
they actually can reuse the same jobs

00:08:34,930 --> 00:08:39,940
and the same code to do so that is very

00:08:37,719 --> 00:08:42,940
nice because first of all it's efficient

00:08:39,940 --> 00:08:44,589
secondly yeah it's really cool for me

00:08:42,940 --> 00:08:47,440
because I don't necessarily have to

00:08:44,589 --> 00:08:49,540
write new code but on the other hand it

00:08:47,440 --> 00:08:51,370
adds some latency because all of a

00:08:49,540 --> 00:08:54,920
sudden I have to wait for this micro

00:08:51,370 --> 00:08:57,560
batch to fill up so if

00:08:54,920 --> 00:08:58,660
I actually have to choose between spark

00:08:57,560 --> 00:09:01,310
and maybe some of those other

00:08:58,660 --> 00:09:03,230
implementations some things I should

00:09:01,310 --> 00:09:05,810
consider is for example the execution

00:09:03,230 --> 00:09:07,700
model whether I have to meet the latency

00:09:05,810 --> 00:09:10,220
need for something native streaming

00:09:07,700 --> 00:09:12,410
which would process each tuple

00:09:10,220 --> 00:09:16,220
individually or whether I'm actually

00:09:12,410 --> 00:09:17,540
okay with micro batches I should

00:09:16,220 --> 00:09:20,029
consider which fault-tolerance

00:09:17,540 --> 00:09:22,370
guarantees I need how quickly I should

00:09:20,029 --> 00:09:24,800
recover from failures so for example if

00:09:22,370 --> 00:09:28,370
I'm running on a lot of spot instances

00:09:24,800 --> 00:09:31,910
and I really expect many of them to fail

00:09:28,370 --> 00:09:32,360
/ be shut down during the runtime of a

00:09:31,910 --> 00:09:34,190
job

00:09:32,360 --> 00:09:36,050
maybe I should also consider like how

00:09:34,190 --> 00:09:40,430
long does it take to recover from a

00:09:36,050 --> 00:09:42,079
failed job and if we look at this

00:09:40,430 --> 00:09:44,120
execution model and this is basically

00:09:42,079 --> 00:09:46,910
what we've been talking about this main

00:09:44,120 --> 00:09:49,220
distinction is that spark is collecting

00:09:46,910 --> 00:09:50,810
those micro batches of data and then on

00:09:49,220 --> 00:09:52,730
the other side most of the other

00:09:50,810 --> 00:09:55,399
frameworks they're actually processing

00:09:52,730 --> 00:09:58,190
each tuple individually this is not

00:09:55,399 --> 00:10:00,560
necessarily just purely better this is

00:09:58,190 --> 00:10:03,920
just different because it also adds some

00:10:00,560 --> 00:10:06,110
overheads such as accounting such as

00:10:03,920 --> 00:10:08,180
processing if I can just process one

00:10:06,110 --> 00:10:10,160
batch at a time this of course is being

00:10:08,180 --> 00:10:12,560
more efficient than if I have to have

00:10:10,160 --> 00:10:14,930
the yeah accounting overhead for each

00:10:12,560 --> 00:10:17,180
individual tupple and this is actually

00:10:14,930 --> 00:10:19,730
this this is true also for the fault

00:10:17,180 --> 00:10:23,110
tolerance guarantees and as we can see

00:10:19,730 --> 00:10:25,940
flank is actually kind of cheating or

00:10:23,110 --> 00:10:30,050
trying to be more efficient there as

00:10:25,940 --> 00:10:32,089
well as actually keeping track of each

00:10:30,050 --> 00:10:34,300
tuple for fault tolerance so basically

00:10:32,089 --> 00:10:37,550
saying checkpoint this type has been

00:10:34,300 --> 00:10:42,319
process and if there's a failure I don't

00:10:37,550 --> 00:10:44,120
have to reprocess it it actually doesn't

00:10:42,319 --> 00:10:44,750
want to do that per each individual

00:10:44,120 --> 00:10:47,589
tuples

00:10:44,750 --> 00:10:52,670
so what fling does is the acknowledgment

00:10:47,589 --> 00:10:55,760
/ prove batch and I think I this should

00:10:52,670 --> 00:10:59,180
be in the other order sorry for that

00:10:55,760 --> 00:11:01,339
so what storm does storm actually does

00:10:59,180 --> 00:11:03,920
actually acknowledge each individual

00:11:01,339 --> 00:11:06,860
record and what flink is doing is a

00:11:03,920 --> 00:11:08,600
checkpoint per batch so they go in they

00:11:06,860 --> 00:11:10,190
process each tuple

00:11:08,600 --> 00:11:12,709
but then the checkpoint that you're

00:11:10,190 --> 00:11:15,470
making is actually per batch of data or

00:11:12,709 --> 00:11:17,569
per micro batch of data and obviously a

00:11:15,470 --> 00:11:19,190
spark as a processing micro batches

00:11:17,569 --> 00:11:23,779
anyhow is doing the same for fault

00:11:19,190 --> 00:11:26,899
tolerance delivery guarantees and

00:11:23,779 --> 00:11:30,290
somehow on this Beamer I sink my head

00:11:26,899 --> 00:11:33,319
and headings get messed up so what storm

00:11:30,290 --> 00:11:35,509
is providing or claims to provide us for

00:11:33,319 --> 00:11:38,060
example exactly one semantics so they're

00:11:35,509 --> 00:11:43,940
saying I've really just gonna process

00:11:38,060 --> 00:11:46,639
each of them at least once what many

00:11:43,940 --> 00:11:49,190
other frameworks write on on the outside

00:11:46,639 --> 00:11:51,019
is it's a supports is exactly once

00:11:49,190 --> 00:11:52,940
guarantees but you should be careful

00:11:51,019 --> 00:11:54,949
what's being meant by that because we're

00:11:52,940 --> 00:11:58,630
talking about distributed systems and

00:11:54,949 --> 00:12:01,100
actually in distributed systems it's

00:11:58,630 --> 00:12:03,470
never you can never guaranteed your

00:12:01,100 --> 00:12:05,300
purses each double just once because

00:12:03,470 --> 00:12:07,670
there's always like one point of failure

00:12:05,300 --> 00:12:09,440
one failure mode whereas this might fail

00:12:07,670 --> 00:12:11,240
because it fails at exactly the

00:12:09,440 --> 00:12:14,120
instruction where you would check point

00:12:11,240 --> 00:12:16,579
something so you should be careful when

00:12:14,120 --> 00:12:19,089
writing your applications whether you

00:12:16,579 --> 00:12:22,339
actually going to receive or process

00:12:19,089 --> 00:12:25,519
tuples multiple times or whether you

00:12:22,339 --> 00:12:27,800
expect them to be just processed once so

00:12:25,519 --> 00:12:29,750
for example a typical pattern is to

00:12:27,800 --> 00:12:31,910
include include some key in your data

00:12:29,750 --> 00:12:35,089
and once you've processed that key you

00:12:31,910 --> 00:12:38,149
actually wouldn't process it again what

00:12:35,089 --> 00:12:39,920
does it mean for our data center it

00:12:38,149 --> 00:12:41,990
means for our data centers those times

00:12:39,920 --> 00:12:43,880
where we where Hadoop would own the

00:12:41,990 --> 00:12:47,300
entire cluster they are basically over

00:12:43,880 --> 00:12:49,490
and so nowadays we actually talk a lot

00:12:47,300 --> 00:12:51,050
about different sub partitions in our

00:12:49,490 --> 00:12:53,870
cluster we might have a fling sub

00:12:51,050 --> 00:12:56,000
partition we might have a Kafka sub

00:12:53,870 --> 00:12:58,880
partition we might have a micro service

00:12:56,000 --> 00:13:01,579
sub partition like 10 20 nodes for our

00:12:58,880 --> 00:13:03,589
micro services and this is usually very

00:13:01,579 --> 00:13:06,550
annoying because first of all it adds

00:13:03,589 --> 00:13:09,439
operator overhead and secondly the

00:13:06,550 --> 00:13:11,509
utilization is really going down because

00:13:09,439 --> 00:13:14,990
I'm wasting resources in each of those

00:13:11,509 --> 00:13:17,209
sub clusters and as weird mrs. Khan this

00:13:14,990 --> 00:13:19,850
is exactly the vision of missus to

00:13:17,209 --> 00:13:22,310
basically unify those all those

00:13:19,850 --> 00:13:24,350
resources into like one big pool

00:13:22,310 --> 00:13:26,329
and I'm actually treating them as like

00:13:24,350 --> 00:13:28,519
one big resource and hence

00:13:26,329 --> 00:13:31,129
I don't care on which note for example

00:13:28,519 --> 00:13:33,980
flink is being scheduled I simply say I

00:13:31,129 --> 00:13:38,600
expect fling to half I don't know 20

00:13:33,980 --> 00:13:40,309
CPUs worth of computing time still if

00:13:38,600 --> 00:13:44,319
I'm just looking at pure misses are

00:13:40,309 --> 00:13:48,499
still challenges so if you've seen re X

00:13:44,319 --> 00:13:50,720
DC us talk actually missus is just a

00:13:48,499 --> 00:13:52,850
kernel and we have need for stuff around

00:13:50,720 --> 00:13:57,139
so for example scheduling monitoring

00:13:52,850 --> 00:14:00,139
security CLI companies ask radio as

00:13:57,139 --> 00:14:02,269
Apple as Netflix they all have large

00:14:00,139 --> 00:14:05,779
teams around it which can actually built

00:14:02,269 --> 00:14:07,459
it themselves but in general we don't

00:14:05,779 --> 00:14:09,079
want to build all this themselves we

00:14:07,459 --> 00:14:12,769
just want to install our operating

00:14:09,079 --> 00:14:16,309
system and be able to roll and this is

00:14:12,769 --> 00:14:19,279
the vision of the open source TCS where

00:14:16,309 --> 00:14:21,139
I basically can install all of that out

00:14:19,279 --> 00:14:23,509
of the box I don't want to go in more

00:14:21,139 --> 00:14:26,329
detail on DCs because I think we heard a

00:14:23,509 --> 00:14:29,149
lot about it this morning still

00:14:26,329 --> 00:14:32,779
developing all those services developing

00:14:29,149 --> 00:14:35,089
a spark service on DC us developing a

00:14:32,779 --> 00:14:37,850
kafka service developing a Cassandra

00:14:35,089 --> 00:14:39,800
service is really hard so for example

00:14:37,850 --> 00:14:41,569
here this is a state diagram for is a

00:14:39,800 --> 00:14:43,819
persistence and one of those frameworks

00:14:41,569 --> 00:14:45,920
so you can see you don't necessarily

00:14:43,819 --> 00:14:48,980
have to understand it in detail but you

00:14:45,920 --> 00:14:51,350
can see it has some complexity of really

00:14:48,980 --> 00:14:53,209
ensuring that you have just reserved

00:14:51,350 --> 00:14:56,870
persistent volumes in a meaningful

00:14:53,209 --> 00:14:59,000
fashion and there are other challenges

00:14:56,870 --> 00:15:00,800
such as how do I support multiple

00:14:59,000 --> 00:15:03,350
frameworks how do I support upgrades

00:15:00,800 --> 00:15:06,379
between frameworks and so on and this is

00:15:03,350 --> 00:15:08,990
exactly where we need to operate those

00:15:06,379 --> 00:15:11,689
distributed services or develop them and

00:15:08,990 --> 00:15:13,809
this is where the SDK which was also

00:15:11,689 --> 00:15:16,939
already mentioned this morning comes in

00:15:13,809 --> 00:15:19,069
where I actually can simply write a Yama

00:15:16,939 --> 00:15:22,309
file and potentially extend it with

00:15:19,069 --> 00:15:24,439
custom strategies so I don't actually

00:15:22,309 --> 00:15:27,079
have to write the scheduler from scratch

00:15:24,439 --> 00:15:29,420
with all the complexities such as

00:15:27,079 --> 00:15:31,850
reserving persistent volumes but I can

00:15:29,420 --> 00:15:35,269
actually just install that out of the

00:15:31,850 --> 00:15:36,050
box and this is actually what we are

00:15:35,269 --> 00:15:39,080
currently working

00:15:36,050 --> 00:15:41,300
for flink as well flink currently is one

00:15:39,080 --> 00:15:44,089
of those schedulers which is down here

00:15:41,300 --> 00:15:46,910
built your own scheduler but we actually

00:15:44,089 --> 00:15:49,279
are progressing rather quickly in both

00:15:46,910 --> 00:15:53,540
bringing that scheduler forward and also

00:15:49,279 --> 00:15:56,089
moving towards the SDK so right now our

00:15:53,540 --> 00:15:59,240
focus is actually to implement new flink

00:15:56,089 --> 00:16:01,579
features so we'll talk about this really

00:15:59,240 --> 00:16:03,980
cool JIRA which is called flip six in

00:16:01,579 --> 00:16:06,320
just a second which actually adds a lot

00:16:03,980 --> 00:16:08,930
of elasticity so that will currently

00:16:06,320 --> 00:16:10,880
still be implemented in the original

00:16:08,930 --> 00:16:13,100
scheduler which lives in the fling code

00:16:10,880 --> 00:16:18,890
but in the future we actually are

00:16:13,100 --> 00:16:21,500
planning to move to the SDK yeah so why

00:16:18,890 --> 00:16:23,450
is that actually a good match so what do

00:16:21,500 --> 00:16:25,070
we need if we are flink so we actually

00:16:23,450 --> 00:16:27,800
we need to be able to run multiple

00:16:25,070 --> 00:16:29,870
applications multiple jobs we need to

00:16:27,800 --> 00:16:32,630
have dynamic resource allocation and a

00:16:29,870 --> 00:16:34,579
large cluster and we most important we

00:16:32,630 --> 00:16:36,589
actually we need to be available if our

00:16:34,579 --> 00:16:38,959
stream streaming jobs are running and

00:16:36,589 --> 00:16:40,790
there's a node failing I still want that

00:16:38,959 --> 00:16:42,649
the job is up and running because it

00:16:40,790 --> 00:16:46,579
should actually process all the events

00:16:42,649 --> 00:16:49,010
coming in why does apache maysa help me

00:16:46,579 --> 00:16:51,740
there because actually misses is exactly

00:16:49,010 --> 00:16:53,690
written for this goal to implement fault

00:16:51,740 --> 00:16:55,730
tolerant and elastic distributed

00:16:53,690 --> 00:16:58,279
applications and it's an interesting

00:16:55,730 --> 00:17:01,760
fact together with the Flint community

00:16:58,279 --> 00:17:04,459
we had a survey of how people are using

00:17:01,760 --> 00:17:06,350
fling and Shirley percent of see survey

00:17:04,459 --> 00:17:09,079
respondents sets they were running flink

00:17:06,350 --> 00:17:11,809
on misses even before we introduced the

00:17:09,079 --> 00:17:13,640
official support into before we

00:17:11,809 --> 00:17:15,290
introduced this official missles

00:17:13,640 --> 00:17:17,209
framework so they were running it in

00:17:15,290 --> 00:17:20,510
marathon or some kind of other set up

00:17:17,209 --> 00:17:23,150
simply to have sus fault tolerance and

00:17:20,510 --> 00:17:25,329
so that was kind of secure for us to

00:17:23,150 --> 00:17:28,400
actually write a proper missles

00:17:25,329 --> 00:17:30,830
integration of in in the Flint codebase

00:17:28,400 --> 00:17:33,320
and it's kind of it's a traditional

00:17:30,830 --> 00:17:35,390
scheduler if you want so to say so

00:17:33,320 --> 00:17:37,610
see scheduler parts this is there on the

00:17:35,390 --> 00:17:40,130
left of where we have the resource

00:17:37,610 --> 00:17:42,590
manager which is talking to the missus

00:17:40,130 --> 00:17:45,110
master and then the job manager which is

00:17:42,590 --> 00:17:48,380
basically responsible for spawning up

00:17:45,110 --> 00:17:49,970
all the tasks so kind of sees two

00:17:48,380 --> 00:17:52,100
responsibilities of a fair

00:17:49,970 --> 00:17:55,220
first of all resource management and

00:17:52,100 --> 00:17:59,179
secondly those task management has been

00:17:55,220 --> 00:18:01,990
divided into those two components and if

00:17:59,179 --> 00:18:05,600
we look a little deeper into the

00:18:01,990 --> 00:18:07,820
resource manager it actually has four

00:18:05,600 --> 00:18:09,710
components first of all is simply the

00:18:07,820 --> 00:18:11,929
connection manager which is checking hey

00:18:09,710 --> 00:18:13,549
can I still talk to the Mises master or

00:18:11,929 --> 00:18:15,140
do you actually need is there a new

00:18:13,549 --> 00:18:19,280
leader election has there been a failure

00:18:15,140 --> 00:18:21,620
in the Mises master then there is the

00:18:19,280 --> 00:18:24,140
task manager which is monitoring the

00:18:21,620 --> 00:18:25,730
tasks and as the launch coordinator

00:18:24,140 --> 00:18:28,159
which is actually responsible for

00:18:25,730 --> 00:18:30,320
launching them and the interesting part

00:18:28,159 --> 00:18:32,360
for the launch cannot coordinator is

00:18:30,320 --> 00:18:35,140
that we choose to write it in phen so

00:18:32,360 --> 00:18:38,780
how many of you actually know fence oh

00:18:35,140 --> 00:18:42,350
yeah many actually so it's it's a

00:18:38,780 --> 00:18:44,299
library which allows me which allows me

00:18:42,350 --> 00:18:47,659
to easily write schedulers and in

00:18:44,299 --> 00:18:50,390
particular it allows me to easily write

00:18:47,659 --> 00:18:52,669
off of matching logic so often if I'm

00:18:50,390 --> 00:18:55,789
implementing my own scheduler I have to

00:18:52,669 --> 00:18:57,559
write things like I have or actually

00:18:55,789 --> 00:18:59,720
constantly I have to decide whether I

00:18:57,559 --> 00:19:01,610
want to accept an offer or not and this

00:18:59,720 --> 00:19:04,100
should be based on certain criteria do I

00:19:01,610 --> 00:19:07,760
have enough resources is this co-located

00:19:04,100 --> 00:19:10,159
with other stuff and so basically what

00:19:07,760 --> 00:19:12,590
most scheduler implementations do is

00:19:10,159 --> 00:19:14,330
I'll collect some offers and then

00:19:12,590 --> 00:19:16,100
they'll decide should I keep an or

00:19:14,330 --> 00:19:18,320
reject them how many of them should I

00:19:16,100 --> 00:19:20,690
actually accept and fans're is a really

00:19:18,320 --> 00:19:22,580
easy tool to makes it very simple to

00:19:20,690 --> 00:19:25,400
implement so you don't have to implement

00:19:22,580 --> 00:19:27,650
all that logic from scratch the

00:19:25,400 --> 00:19:30,440
reconciliation coordinator is

00:19:27,650 --> 00:19:32,659
responsible for if there has been a

00:19:30,440 --> 00:19:35,240
failure if there's been a master switch

00:19:32,659 --> 00:19:37,580
if C framework is restarted somewhere

00:19:35,240 --> 00:19:40,309
else because the scheduler failed for

00:19:37,580 --> 00:19:41,990
some reason it actually we always need

00:19:40,309 --> 00:19:44,870
to reconcile the state between the

00:19:41,990 --> 00:19:47,780
master and the scheduler so if you want

00:19:44,870 --> 00:19:49,820
that both have the same view on the

00:19:47,780 --> 00:19:54,559
cluster again and this is what the

00:19:49,820 --> 00:19:57,140
reconciliation coordinators for so the

00:19:54,559 --> 00:19:58,669
entry place basically here's those the

00:19:57,140 --> 00:20:01,520
missus master is going to send out

00:19:58,669 --> 00:20:03,630
offers and the launch coordinator will

00:20:01,520 --> 00:20:06,900
then decide whether it should start

00:20:03,630 --> 00:20:11,820
saying launch coordinator will then

00:20:06,900 --> 00:20:17,130
receive star double tasks from either

00:20:11,820 --> 00:20:19,740
the reconciliation coordinator and then

00:20:17,130 --> 00:20:21,420
it will basically launch them once it

00:20:19,740 --> 00:20:23,580
has launched them the task monitor is

00:20:21,420 --> 00:20:25,770
responsible for actually monitoring Sam

00:20:23,580 --> 00:20:28,620
so that's the component which will

00:20:25,770 --> 00:20:31,350
purchase the stat task status updates

00:20:28,620 --> 00:20:33,750
it's gonna receive from the master and

00:20:31,350 --> 00:20:36,090
if something is failing the

00:20:33,750 --> 00:20:39,270
reconciliation coordinator will

00:20:36,090 --> 00:20:42,840
coordinate with the master - yeah

00:20:39,270 --> 00:20:45,960
restart all those new tasks to recover

00:20:42,840 --> 00:20:48,570
those tasks as mentioned this fenzel

00:20:45,960 --> 00:20:51,150
library if you ever go to implement your

00:20:48,570 --> 00:20:53,700
own scheduler without C SDK I can just

00:20:51,150 --> 00:20:57,990
highly recommend taking a look here so

00:20:53,700 --> 00:20:59,790
it's kind of really helpful because it

00:20:57,990 --> 00:21:02,310
has this pluggable Fitness evaluator

00:20:59,790 --> 00:21:03,630
which as mentioned helps you to decide

00:21:02,310 --> 00:21:05,850
whether you want to accept an offer

00:21:03,630 --> 00:21:09,150
always you don't want to accept an offer

00:21:05,850 --> 00:21:11,880
and this is just integrated into the

00:21:09,150 --> 00:21:13,860
launch coordinator here so the launch

00:21:11,880 --> 00:21:15,750
coordinators basically receives the

00:21:13,860 --> 00:21:18,620
tasks it wants to launch with their

00:21:15,750 --> 00:21:22,890
tasks description and then fan so can

00:21:18,620 --> 00:21:25,860
automatically match them to research in

00:21:22,890 --> 00:21:28,880
resource offers so and then kind of

00:21:25,860 --> 00:21:31,530
return which one should be started and

00:21:28,880 --> 00:21:33,630
now this is actually my problem my

00:21:31,530 --> 00:21:35,220
favorite slide as this is a new

00:21:33,630 --> 00:21:37,170
architecture which will be in the next

00:21:35,220 --> 00:21:39,540
flink version and this is what I

00:21:37,170 --> 00:21:42,360
previously described as flip six and

00:21:39,540 --> 00:21:44,880
we're actually restructuring a lot of C

00:21:42,360 --> 00:21:47,790
code around so first of all we're gonna

00:21:44,880 --> 00:21:49,650
have a proper Emmaus dispatcher and this

00:21:47,790 --> 00:21:51,780
actually helps if we want to spin up

00:21:49,650 --> 00:21:54,000
multiple of those C way you would

00:21:51,780 --> 00:21:56,220
currently start Fling it's basically you

00:21:54,000 --> 00:21:58,020
spin up here is this component here but

00:21:56,220 --> 00:22:00,900
now you actually have this long-running

00:21:58,020 --> 00:22:03,090
dispatcher and for any job you would you

00:22:00,900 --> 00:22:07,140
can start you would spin up one of those

00:22:03,090 --> 00:22:09,870
new resource manager job manager process

00:22:07,140 --> 00:22:13,140
bundles so it makes it much easier to

00:22:09,870 --> 00:22:16,080
actually run multiple jobs on flink the

00:22:13,140 --> 00:22:17,400
other big advantage we are having which

00:22:16,080 --> 00:22:19,590
maybe it's not

00:22:17,400 --> 00:22:22,230
quite apparent on this slide but as we

00:22:19,590 --> 00:22:25,620
restructured this code we actually made

00:22:22,230 --> 00:22:27,420
it flexible the resources you can

00:22:25,620 --> 00:22:30,390
allocate you running job

00:22:27,420 --> 00:22:32,430
imagine you have a long-running flink

00:22:30,390 --> 00:22:34,830
job at the beginning you might actually

00:22:32,430 --> 00:22:38,400
might want to use some more resources

00:22:34,830 --> 00:22:40,170
but then at the end or maybe at night

00:22:38,400 --> 00:22:42,000
you actually don't have that many users

00:22:40,170 --> 00:22:44,460
you don't have that many events coming

00:22:42,000 --> 00:22:47,100
in so now you can actually scale it down

00:22:44,460 --> 00:22:49,650
and then scale it up again while the job

00:22:47,100 --> 00:22:52,590
is running so you can basically spin up

00:22:49,650 --> 00:22:54,660
those task managers more dynamically

00:22:52,590 --> 00:22:57,120
during the runtime of a job which

00:22:54,660 --> 00:23:00,060
previously was only possible when you

00:22:57,120 --> 00:23:02,940
started up your system so this really

00:23:00,060 --> 00:23:05,300
gives us a lot of flexibility and better

00:23:02,940 --> 00:23:09,470
resource utilization when running large

00:23:05,300 --> 00:23:12,000
long-running flink jobs on top of misses

00:23:09,470 --> 00:23:14,940
all right

00:23:12,000 --> 00:23:17,510
let's flip over here because how much

00:23:14,940 --> 00:23:17,510
time do I have left

00:23:18,290 --> 00:23:25,920
really okay awesome so this gives us

00:23:24,270 --> 00:23:29,940
enough time to actually go through the

00:23:25,920 --> 00:23:32,730
demo and the demo is something where I

00:23:29,940 --> 00:23:35,670
simply want to show one of those smack

00:23:32,730 --> 00:23:38,580
like pipelines together with flink here

00:23:35,670 --> 00:23:40,140
so on the Left we have our data

00:23:38,580 --> 00:23:42,720
generator which is basically just

00:23:40,140 --> 00:23:44,940
putting out financial transactions so

00:23:42,720 --> 00:23:48,090
our goal with this demo is that we

00:23:44,940 --> 00:23:51,600
actually want to detect fraud fraud in

00:23:48,090 --> 00:23:53,250
this meaning of money laundering so a

00:23:51,600 --> 00:23:54,990
transaction is basically I'm

00:23:53,250 --> 00:23:57,900
transferring money from account a to

00:23:54,990 --> 00:24:00,000
account B and whenever this over

00:23:57,900 --> 00:24:02,400
multiple transactions sums up to more

00:24:00,000 --> 00:24:04,410
than ten thousand US dollars there's

00:24:02,400 --> 00:24:07,160
potential money-laundering and then

00:24:04,410 --> 00:24:10,350
something someone should be alerted and

00:24:07,160 --> 00:24:11,820
so how's this demo is set up and it's

00:24:10,350 --> 00:24:15,450
mentioned this is kind of kind of C

00:24:11,820 --> 00:24:19,260
typical smack stack setup so this is on

00:24:15,450 --> 00:24:21,750
the Left our data generator this will

00:24:19,260 --> 00:24:24,390
write all those transaction data a to

00:24:21,750 --> 00:24:27,480
bees this amount it will write it into

00:24:24,390 --> 00:24:29,670
Kafka and then I can have I have

00:24:27,480 --> 00:24:31,350
multiple options I can either run spark

00:24:29,670 --> 00:24:33,299
I can run flink

00:24:31,350 --> 00:24:36,570
and those would be consuming the data

00:24:33,299 --> 00:24:39,240
out of Kafka and trying to aggregate it

00:24:36,570 --> 00:24:40,770
over certain time windows and trying to

00:24:39,240 --> 00:24:42,570
detect whether the amount of

00:24:40,770 --> 00:24:45,059
transactions in this time window is

00:24:42,570 --> 00:24:47,460
greater than ten thousand US dollars

00:24:45,059 --> 00:24:50,280
will see that in code in just a second

00:24:47,460 --> 00:24:51,960
but what I really like about it and this

00:24:50,280 --> 00:24:54,419
is kind of one of the advantages of

00:24:51,960 --> 00:24:56,970
using Kafka and such kind of smack stack

00:24:54,419 --> 00:24:59,340
architecture is that actually here in

00:24:56,970 --> 00:25:03,270
the middle I have a lot of flexibility

00:24:59,340 --> 00:25:06,390
and what I want to do I can actually run

00:25:03,270 --> 00:25:09,419
spark and flink simultaneously

00:25:06,390 --> 00:25:12,150
processing the same data from the Kafka

00:25:09,419 --> 00:25:16,770
queue and this is because Kafka actually

00:25:12,150 --> 00:25:18,780
is also persisting data and so I can

00:25:16,770 --> 00:25:22,980
have multiple consumers consuming the

00:25:18,780 --> 00:25:25,350
same data and this feature of Kafka that

00:25:22,980 --> 00:25:27,179
is actually also persisting data you

00:25:25,350 --> 00:25:30,450
make use of that in the fourth step as

00:25:27,179 --> 00:25:33,659
well we actually used Kafka as the data

00:25:30,450 --> 00:25:35,490
store and so the results out of this

00:25:33,659 --> 00:25:39,030
fling shop they're actually written back

00:25:35,490 --> 00:25:41,280
into Kafka as kind of the persistence

00:25:39,030 --> 00:25:42,990
layer in our smack stack and then we

00:25:41,280 --> 00:25:48,809
have a short little display which will

00:25:42,990 --> 00:25:55,049
actually show that in the end so let me

00:25:48,809 --> 00:25:59,960
Oh and people are already clapping over

00:25:55,049 --> 00:25:59,960
there do I really have half an hour left

00:26:01,370 --> 00:26:12,090
ok I'll try to type quickly so if you

00:26:09,360 --> 00:26:15,990
haven't seen DCOs it kind of comes with

00:26:12,090 --> 00:26:17,880
this with this app store this app store

00:26:15,990 --> 00:26:20,760
makes it really easy to install things

00:26:17,880 --> 00:26:24,990
and first of all actually I need to

00:26:20,760 --> 00:26:29,429
install Cassandra because no I don't I'm

00:26:24,990 --> 00:26:32,159
talking wrong wrong wrong demo first

00:26:29,429 --> 00:26:37,049
thing I should install this flink so let

00:26:32,159 --> 00:26:39,659
me install flink here great and the

00:26:37,049 --> 00:26:42,270
second thing we actually need recalling

00:26:39,659 --> 00:26:44,820
my slide and recalling the right demo is

00:26:42,270 --> 00:26:48,480
I need Kafka so that

00:26:44,820 --> 00:26:53,669
also installed Kafka and I want a normal

00:26:48,480 --> 00:26:55,740
Kafka Nazi confluent version great so if

00:26:53,669 --> 00:27:00,990
we look at our services we see that both

00:26:55,740 --> 00:27:03,299
flink and Kafka are deploying and while

00:27:00,990 --> 00:27:06,330
they are doing that we can actually

00:27:03,299 --> 00:27:13,139
already go in and distribute our data

00:27:06,330 --> 00:27:15,330
generator so here I'm and actually this

00:27:13,139 --> 00:27:19,049
demo is online so anyone who wants to

00:27:15,330 --> 00:27:21,539
run that feel free to do so and just to

00:27:19,049 --> 00:27:24,870
have a look at it the generator it's a

00:27:21,539 --> 00:27:29,460
really easy easy JSON file and all it

00:27:24,870 --> 00:27:33,629
does is basically curl this image curl Z

00:27:29,460 --> 00:27:36,269
binary and then run it and in my opinion

00:27:33,629 --> 00:27:38,700
this is something really nice about DCs

00:27:36,269 --> 00:27:41,460
and missiles therefore that I don't have

00:27:38,700 --> 00:27:43,409
to construct a full container image so

00:27:41,460 --> 00:27:45,419
usually I would expect that I have

00:27:43,409 --> 00:27:47,490
packaged that into a docker container I

00:27:45,419 --> 00:27:49,500
pull that docker container around the

00:27:47,490 --> 00:27:53,279
docker container here I can actually

00:27:49,500 --> 00:27:56,669
just run that s is and kind of construct

00:27:53,279 --> 00:28:05,850
the container on the fly so let's deploy

00:27:56,669 --> 00:28:09,149
that Decius Merson and generated a json

00:28:05,850 --> 00:28:10,710
and this is deploying so we hopefully

00:28:09,149 --> 00:28:13,230
see that here in a second

00:28:10,710 --> 00:28:15,570
yes it's coming up and it's already

00:28:13,230 --> 00:28:17,879
running because actually I don't have to

00:28:15,570 --> 00:28:19,740
pull like large images I simply need

00:28:17,879 --> 00:28:22,769
some pool like this I think it's like 2

00:28:19,740 --> 00:28:25,200
megabyte binary and it's actually up and

00:28:22,769 --> 00:28:27,840
running so if we take a look here into

00:28:25,200 --> 00:28:32,600
the locks hopefully already producing

00:28:27,840 --> 00:28:35,700
yes it's already producing transactions

00:28:32,600 --> 00:28:37,980
Kafka is also already up and running so

00:28:35,700 --> 00:28:40,950
the next thing I need to do is I need to

00:28:37,980 --> 00:28:44,549
create those pipes in Kafka so I can do

00:28:40,950 --> 00:28:49,730
that from the CLI as well TCAs Kafka top

00:28:44,549 --> 00:28:52,379
hey and guess it's just topic create

00:28:49,730 --> 00:28:55,159
brought let me create the first one

00:28:52,379 --> 00:28:55,159
which would be the output

00:28:56,179 --> 00:29:02,510
Decius as I installed Kafka from the UI

00:28:59,940 --> 00:29:05,880
I don't have to CLI extension installed

00:29:02,510 --> 00:29:15,570
so let me quickly do that package

00:29:05,880 --> 00:29:23,280
install Kafka yes I want to see li

00:29:15,570 --> 00:29:28,650
extension and now I hopefully can create

00:29:23,280 --> 00:29:31,650
my topic that's looking good because

00:29:28,650 --> 00:29:38,900
it's taking long great and now let me

00:29:31,650 --> 00:29:38,900
also create a trance actions topic

00:29:45,880 --> 00:29:52,809
transaction ah it already exists because

00:29:50,950 --> 00:29:54,850
my generator is already up and running

00:29:52,809 --> 00:29:56,950
as the generator is actually writing

00:29:54,850 --> 00:29:59,350
into transactions the generator was

00:29:56,950 --> 00:30:00,929
faster and bringing it up so once we

00:29:59,350 --> 00:30:02,950
automatically write data into

00:30:00,929 --> 00:30:05,620
non-existent topic it's automatically

00:30:02,950 --> 00:30:11,950
created so good we have both our topics

00:30:05,620 --> 00:30:15,669
here great so uh let's look at flink

00:30:11,950 --> 00:30:18,460
here and fling comes with a usual web UI

00:30:15,669 --> 00:30:20,590
you might be used to by the way I can't

00:30:18,460 --> 00:30:23,950
use the same from the CLI so similar as

00:30:20,590 --> 00:30:27,610
Kafka flink also has a CLI but I usually

00:30:23,950 --> 00:30:38,220
I like the squirrel just so much so I

00:30:27,610 --> 00:30:38,220
usually go why I hear too many folders

00:30:41,810 --> 00:30:56,360
it's in my NGO directory because there's

00:30:45,530 --> 00:30:59,000
some NGO code in there almost there ok

00:30:56,360 --> 00:31:04,700
flink job and now i simply gonna upload

00:30:59,000 --> 00:31:10,490
to jar upload and if we care about the

00:31:04,700 --> 00:31:13,130
code I don't think we have time so or

00:31:10,490 --> 00:31:15,500
now we'll we'll skip the code the code

00:31:13,130 --> 00:31:17,240
is online so what I like about the flink

00:31:15,500 --> 00:31:20,000
code and this is actually where I choose

00:31:17,240 --> 00:31:22,880
flink to do this demo is simply because

00:31:20,000 --> 00:31:26,030
it has a very nice way of dealing with

00:31:22,880 --> 00:31:27,860
event time so if we are dealing if we

00:31:26,030 --> 00:31:30,320
doing stream processing with multiple

00:31:27,860 --> 00:31:32,720
options to deal with time I can either

00:31:30,320 --> 00:31:34,940
so if I'm doing a window for example

00:31:32,720 --> 00:31:37,760
over a day so if I'm saying within a day

00:31:34,940 --> 00:31:40,070
I don't want to see any transactions

00:31:37,760 --> 00:31:42,080
summing up to more than $10,000 and

00:31:40,070 --> 00:31:43,730
there the question is what notion of

00:31:42,080 --> 00:31:46,370
time we were talking about this is the

00:31:43,730 --> 00:31:48,890
notion at which the event arrives at sea

00:31:46,370 --> 00:31:50,930
stream processor is it or is it actually

00:31:48,890 --> 00:31:53,690
the time at which the event was created

00:31:50,930 --> 00:31:56,090
and flink has very nice support for this

00:31:53,690 --> 00:31:58,700
event time so actually the data

00:31:56,090 --> 00:32:01,090
generator is adding the timestamp and

00:31:58,700 --> 00:32:04,330
which time the event is created and

00:32:01,090 --> 00:32:07,700
flink makes it really easy to utilize

00:32:04,330 --> 00:32:10,100
that event time and sum up over event

00:32:07,700 --> 00:32:18,200
time or have the window size over event

00:32:10,100 --> 00:32:20,510
on let me just starts a job cool we see

00:32:18,200 --> 00:32:22,790
it's up and running and now this is

00:32:20,510 --> 00:32:24,740
basically this streaming job which could

00:32:22,790 --> 00:32:26,780
run forever which could run forever in

00:32:24,740 --> 00:32:29,390
the background so for example also when

00:32:26,780 --> 00:32:31,700
you would upgrade your flink job your

00:32:29,390 --> 00:32:34,220
flink version you can keep the job up

00:32:31,700 --> 00:32:36,050
and running which is kind of necessary

00:32:34,220 --> 00:32:37,880
if you actually want like high

00:32:36,050 --> 00:32:39,650
availability and you don't want to

00:32:37,880 --> 00:32:42,830
impact your users simply because you're

00:32:39,650 --> 00:32:45,110
upgrading your system cool and we see

00:32:42,830 --> 00:32:47,240
that everything is running now that's

00:32:45,110 --> 00:32:52,100
great and the last thing we actually

00:32:47,240 --> 00:32:54,050
need is our our monitoring tool and this

00:32:52,100 --> 00:32:55,610
is actually also here in the repo it

00:32:54,050 --> 00:33:02,049
said the link is also

00:32:55,610 --> 00:33:02,049
slides so this is our actor and

00:33:08,650 --> 00:33:14,200
simply going to deploy that again this

00:33:11,500 --> 00:33:15,130
is a simple go binary so it should be up

00:33:14,200 --> 00:33:17,590
rather quickly

00:33:15,130 --> 00:33:20,220
it's deploying pulling see binary and

00:33:17,590 --> 00:33:20,220
now it's up and running

00:33:22,830 --> 00:33:30,400
and now all we can do here we already

00:33:27,760 --> 00:33:34,390
have the first detected fraud so what

00:33:30,400 --> 00:33:36,430
this system will do it will show us from

00:33:34,390 --> 00:33:38,740
which time term stamp to which time

00:33:36,430 --> 00:33:42,220
stamp we've detected transactions coming

00:33:38,740 --> 00:33:46,470
up to more than $10,000 and this is like

00:33:42,220 --> 00:33:49,630
one transaction over $3,300 and one over

00:33:46,470 --> 00:33:51,430
$8,000 so it sums up to more and if we

00:33:49,630 --> 00:33:54,550
keep that running it's probably going to

00:33:51,430 --> 00:33:57,160
detect more over time so as it goes on

00:33:54,550 --> 00:33:59,470
it detects more and more fraud over time

00:33:57,160 --> 00:34:02,650
so this is as easy it is to setup such

00:33:59,470 --> 00:34:04,960
kind of pipeline in an orchestrated

00:34:02,650 --> 00:34:07,300
fashion we see that our cluster

00:34:04,960 --> 00:34:09,040
utilization it's going up and this is

00:34:07,300 --> 00:34:12,100
probably one of those metrics I should

00:34:09,040 --> 00:34:13,990
monitor how good my cluster utilization

00:34:12,100 --> 00:34:19,780
is which actually brings me to my next

00:34:13,990 --> 00:34:21,610
slide which would be about how to how to

00:34:19,780 --> 00:34:24,159
keep this actually up and running so if

00:34:21,610 --> 00:34:26,409
I go here usually when giving a demo I

00:34:24,159 --> 00:34:28,090
really like this demo effect it's up and

00:34:26,409 --> 00:34:30,850
running in the end but if you're in

00:34:28,090 --> 00:34:32,500
operator even in developer developing

00:34:30,850 --> 00:34:34,149
such kind of pipeline you should

00:34:32,500 --> 00:34:36,610
remember that the hard part actually

00:34:34,149 --> 00:34:39,010
comes afterwards how do I keep that up

00:34:36,610 --> 00:34:40,960
and running how can I update my Kafka

00:34:39,010 --> 00:34:42,880
how can I update my spark how can I

00:34:40,960 --> 00:34:44,710
update my fling while keeping this

00:34:42,880 --> 00:34:46,960
pipeline up and running and being

00:34:44,710 --> 00:34:48,909
available for the users because if I'm

00:34:46,960 --> 00:34:51,100
using my credit card and that wouldn't

00:34:48,909 --> 00:34:52,899
detect like money laundering fraud but

00:34:51,100 --> 00:34:54,850
it would see whether my credit card

00:34:52,899 --> 00:34:58,270
transaction is okay then I actually

00:34:54,850 --> 00:35:00,340
don't want to wait for the system to be

00:34:58,270 --> 00:35:02,610
upgraded until I can use my credit cards

00:35:00,340 --> 00:35:08,500
it should be up and running all the time

00:35:02,610 --> 00:35:11,470
so a conclusion fling is has really nice

00:35:08,500 --> 00:35:13,540
integration with missiles and it's as

00:35:11,470 --> 00:35:15,280
said it's going to be even nicer in the

00:35:13,540 --> 00:35:17,980
next flink release which should be out

00:35:15,280 --> 00:35:20,200
next months because we actually support

00:35:17,980 --> 00:35:22,630
dynamic resource allocation during a

00:35:20,200 --> 00:35:25,300
running job and this actually

00:35:22,630 --> 00:35:27,730
together with DCs and the other packages

00:35:25,300 --> 00:35:30,550
which are available in DCs it's a really

00:35:27,730 --> 00:35:32,470
easy way of building

00:35:30,550 --> 00:35:34,900
first of all of running fling and

00:35:32,470 --> 00:35:37,900
secondly of building an entire pipeline

00:35:34,900 --> 00:35:39,340
which usually is needed to run flink in

00:35:37,900 --> 00:35:42,370
an efficient way

00:35:39,340 --> 00:35:46,840
in a streaming architecture thank you

00:35:42,370 --> 00:35:49,510
very much for listening as set till he

00:35:46,840 --> 00:35:52,570
was also quite involved in here but he's

00:35:49,510 --> 00:35:55,450
finishing the next flink release so we

00:35:52,570 --> 00:36:10,720
have even cooler sinks to showcase next

00:35:55,450 --> 00:36:19,030
time around any questions I I believe

00:36:10,720 --> 00:36:21,600
you were first or behind you Thanks

00:36:19,030 --> 00:36:23,800
a question about the upgrades or

00:36:21,600 --> 00:36:26,440
deployments and new flink versions of

00:36:23,800 --> 00:36:28,600
new jobs versions if we fix something in

00:36:26,440 --> 00:36:31,570
the job there's a lose the save points

00:36:28,600 --> 00:36:34,510
what how does it work to minimize that

00:36:31,570 --> 00:36:36,520
downtime to minimize that downtown you

00:36:34,510 --> 00:36:39,580
should configure if linked together with

00:36:36,520 --> 00:36:42,070
a stable store usually HDFS and then

00:36:39,580 --> 00:36:44,950
it's checkpointing those yeah

00:36:42,070 --> 00:36:47,140
checkpoints on a stable storage and when

00:36:44,950 --> 00:36:50,580
upgrading as this will also be easier

00:36:47,140 --> 00:36:54,820
with the next version you basically

00:36:50,580 --> 00:36:57,910
using you you kill one worker

00:36:54,820 --> 00:36:59,800
it's pinups a new version worker that

00:36:57,910 --> 00:37:02,890
picks up a checkpoint and so on and so

00:36:59,800 --> 00:37:06,100
on and you just cycle through can I spin

00:37:02,890 --> 00:37:10,780
up the new version while the old version

00:37:06,100 --> 00:37:12,520
running and then you can do the same so

00:37:10,780 --> 00:37:14,320
this what I describe first would kind of

00:37:12,520 --> 00:37:16,210
be a rolling upgrade the second thing

00:37:14,320 --> 00:37:20,800
would kind of be a Bluegreen deployment

00:37:16,210 --> 00:37:22,330
style of upgrade okay thanks and oh by

00:37:20,800 --> 00:37:24,250
the way if you're interested in or

00:37:22,330 --> 00:37:27,700
exactly have such kind of questions

00:37:24,250 --> 00:37:31,210
there's in the DC OS slack channel which

00:37:27,700 --> 00:37:33,040
is TCOs with chateau d cos io there's a

00:37:31,210 --> 00:37:35,810
flink channel where exactly those things

00:37:33,040 --> 00:37:38,870
are being discussed thank you

00:37:35,810 --> 00:37:42,500
thank you for the talk I have one

00:37:38,870 --> 00:37:46,130
question if I may be for my lack of

00:37:42,500 --> 00:37:48,580
knowledge self link but do you know do

00:37:46,130 --> 00:37:52,040
you talk about dynamic resource

00:37:48,580 --> 00:37:56,840
allocation but how come flinkman it's in

00:37:52,040 --> 00:37:59,510
a state a stateful situation like it

00:37:56,840 --> 00:38:02,810
always think like a spark when spark use

00:37:59,510 --> 00:38:05,720
of desde Waikiki and you have to have

00:38:02,810 --> 00:38:08,330
these very location you have to raise

00:38:05,720 --> 00:38:11,240
your your programming external services

00:38:08,330 --> 00:38:14,090
is the same for flink or as another

00:38:11,240 --> 00:38:15,680
architecture the architecture is

00:38:14,090 --> 00:38:16,940
slightly different I yesterday had a

00:38:15,680 --> 00:38:19,040
really interesting discussion about

00:38:16,940 --> 00:38:26,330
exactly this architectural difference at

00:38:19,040 --> 00:38:28,130
sparks summit key TL TL TL DR is that if

00:38:26,330 --> 00:38:30,620
link is keeping track of the state

00:38:28,130 --> 00:38:32,990
differently so if you spin up a new

00:38:30,620 --> 00:38:34,850
worker in the new version right I'm

00:38:32,990 --> 00:38:37,460
talking about the unreleased version

00:38:34,850 --> 00:38:40,730
right now it will basically spin up a

00:38:37,460 --> 00:38:44,170
new worker and then using similar to

00:38:40,730 --> 00:38:46,880
dynamic hashing techniques we basically

00:38:44,170 --> 00:38:49,940
redistribute the new income increase and

00:38:46,880 --> 00:38:52,220
it can automatically gather as set see

00:38:49,940 --> 00:38:54,410
check bones are written to HDFS or some

00:38:52,220 --> 00:38:56,120
kind of stable storage in the cluster so

00:38:54,410 --> 00:39:00,170
the new worker can also retrieve that

00:38:56,120 --> 00:39:02,930
stable storage that is a tldr I'm happy

00:39:00,170 --> 00:39:06,610
to share the design Docs for that if

00:39:02,930 --> 00:39:06,610
you're caring about more detail on that

00:39:12,900 --> 00:39:18,940
if I understand well yeah I think that

00:39:16,510 --> 00:39:22,630
you say that the framework is moving to

00:39:18,940 --> 00:39:25,270
the Commons in the future yeah not for

00:39:22,630 --> 00:39:29,800
the next release but there plans to move

00:39:25,270 --> 00:39:31,780
it it kind of depends but we are

00:39:29,800 --> 00:39:35,110
actively discussing it let's just put it

00:39:31,780 --> 00:39:37,540
this way there are meetings as said if

00:39:35,110 --> 00:39:39,220
you're interested and also in putting

00:39:37,540 --> 00:39:42,160
your opinions there so it's a lot of

00:39:39,220 --> 00:39:46,680
discussions of pure missile support

00:39:42,160 --> 00:39:50,470
whereas there DCOs support so kind of

00:39:46,680 --> 00:39:52,780
balancing it's just the discussion point

00:39:50,470 --> 00:39:55,210
we are seeing is that some people

00:39:52,780 --> 00:39:59,140
running it on dcs that would benefit by

00:39:55,210 --> 00:40:00,790
having better security integration so

00:39:59,140 --> 00:40:03,430
right now you can set it up but it's

00:40:00,790 --> 00:40:05,500
pretty difficult you have to set a lot

00:40:03,430 --> 00:40:08,770
of parameters and if it's basically

00:40:05,500 --> 00:40:11,560
generated by the dcs SDK you kind of

00:40:08,770 --> 00:40:14,050
gets it all very simple in a very simple

00:40:11,560 --> 00:40:15,370
set up way what most likely will happen

00:40:14,050 --> 00:40:19,450
is that at the beginning we're going to

00:40:15,370 --> 00:40:24,130
maintain to kind of versions until first

00:40:19,450 --> 00:40:27,310
of all the DC US SDK is either it's

00:40:24,130 --> 00:40:27,850
runnable on mesos so it kind of has an

00:40:27,310 --> 00:40:32,460
output

00:40:27,850 --> 00:40:32,460
let's see where does that happens or

00:40:32,670 --> 00:40:37,780
kind of we have all features completed

00:40:35,950 --> 00:40:39,970
so we'll always let's just say we'll

00:40:37,780 --> 00:40:42,130
always support the missus users as well

00:40:39,970 --> 00:40:43,330
which you're not running on T cos this

00:40:42,130 --> 00:40:48,700
is like an important part of our

00:40:43,330 --> 00:40:51,610
discussion the Darlington is more relate

00:40:48,700 --> 00:40:56,980
to to sort of frame worse than

00:40:51,610 --> 00:40:57,520
professing one I suppose you mean like

00:40:56,980 --> 00:40:59,560
this

00:40:57,520 --> 00:41:02,560
for example to dispatch our architecture

00:40:59,560 --> 00:41:04,030
yeah yeah right right now this is also

00:41:02,560 --> 00:41:09,400
why this is technically zijn why we're

00:41:04,030 --> 00:41:11,260
still waiting is the SDK support but

00:41:09,400 --> 00:41:14,530
this is in general we also found some

00:41:11,260 --> 00:41:18,790
ways to hack around set for for the

00:41:14,530 --> 00:41:20,950
tensorflow framework and this is the

00:41:18,790 --> 00:41:23,800
same problem there so we could find out

00:41:20,950 --> 00:41:24,849
figure out a way how to do that it's

00:41:23,800 --> 00:41:27,400
just once the

00:41:24,849 --> 00:41:29,140
SDK probably supports that this kind of

00:41:27,400 --> 00:41:31,150
dispatcher architecture which you also

00:41:29,140 --> 00:41:33,309
have in spark which you have and many

00:41:31,150 --> 00:41:36,880
other frameworks then it's gonna be much

00:41:33,309 --> 00:41:39,640
nicer and then the last question is how

00:41:36,880 --> 00:41:41,979
often so how do you decide to include

00:41:39,640 --> 00:41:44,950
fen so at the core of day of this

00:41:41,979 --> 00:41:47,859
scheduling versus I support implementing

00:41:44,950 --> 00:41:51,759
that line of logic a crystal are you for

00:41:47,859 --> 00:41:54,900
doing that we we first of all we looked

00:41:51,759 --> 00:41:57,640
at the logic in spark then we looked at

00:41:54,900 --> 00:42:00,430
so the history of how we got to

00:41:57,640 --> 00:42:03,339
scheduler being written was the initial

00:42:00,430 --> 00:42:05,529
support and flake only yarn and then we

00:42:03,339 --> 00:42:08,410
moved over so we kind of refactored the

00:42:05,529 --> 00:42:10,420
Flint code and moved over moved it to a

00:42:08,410 --> 00:42:15,880
more general architecture so we can

00:42:10,420 --> 00:42:17,680
support both yarn and Miso's and in that

00:42:15,880 --> 00:42:20,319
we figured out that we actually we have

00:42:17,680 --> 00:42:22,420
to do a lot of code of matching which

00:42:20,319 --> 00:42:26,019
you know that yarn is kind of a

00:42:22,420 --> 00:42:27,759
different model where a request um when

00:42:26,019 --> 00:42:29,890
writing that it was kind of holding us

00:42:27,759 --> 00:42:32,349
up and they're actually fencer was

00:42:29,890 --> 00:42:34,029
really helpful in writing it because it

00:42:32,349 --> 00:42:35,729
took away all this logic we would

00:42:34,029 --> 00:42:42,999
otherwise have to write and maintain

00:42:35,729 --> 00:42:45,190
into a very simple component thank you

00:42:42,999 --> 00:42:48,889
so much

00:42:45,190 --> 00:42:48,889

YouTube URL: https://www.youtube.com/watch?v=xu_hmc_lYS0


