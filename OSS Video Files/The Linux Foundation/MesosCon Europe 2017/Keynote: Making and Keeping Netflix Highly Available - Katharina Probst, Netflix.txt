Title: Keynote: Making and Keeping Netflix Highly Available - Katharina Probst, Netflix
Publication date: 2017-10-27
Playlist: MesosCon Europe 2017
Description: 
	Keynote: Making and Keeping Netflix Highly Available: The Role of Mesos in Operational Insights - Katharina Probst, Engineering Director, Netflix

About Katharina Probst
Katharina Probst is an Engineering Director at Netflix, where she is responsible for the availability and reliability of the streaming service used by more than 100 million people around the world. She previously led the API team at Netflix. 

Prior to joining Netflix, she was in the cloud computing team at Google, where she experienced cloud computing from the provider side. Her interests include scalable and reliable, distributed systems, cloud computing, and building effective and successful teams. 

She also holds a PhD in Computer Science from Carnegie Mellon University.
Captions: 
	00:00:00,030 --> 00:00:06,480
thank you for having me here today

00:00:03,200 --> 00:00:10,980
everyday Netflix offers highly curated

00:00:06,480 --> 00:00:13,670
and highly produced content to over 100

00:00:10,980 --> 00:00:16,320
million customers around the world

00:00:13,670 --> 00:00:17,820
stranger things is one of them I haven't

00:00:16,320 --> 00:00:21,930
seen the second season but I highly

00:00:17,820 --> 00:00:27,449
recommended I've heard very good things

00:00:21,930 --> 00:00:30,929
about it so my job at Netflix is to make

00:00:27,449 --> 00:00:33,899
sure as you said that when customers

00:00:30,929 --> 00:00:36,780
come to our service they just get what

00:00:33,899 --> 00:00:38,850
they want all they want to do is search

00:00:36,780 --> 00:00:42,899
or find the content that they want to

00:00:38,850 --> 00:00:46,890
see and watch in so our goal is to have

00:00:42,899 --> 00:00:49,379
over 100 million happy customers but the

00:00:46,890 --> 00:00:52,590
truth is that we run a relatively

00:00:49,379 --> 00:00:56,309
complex distributed system and sometimes

00:00:52,590 --> 00:00:58,140
something goes wrong and then our

00:00:56,309 --> 00:01:02,520
customers are not as happy as you may

00:00:58,140 --> 00:01:04,979
have hoped when you have over a hundred

00:01:02,520 --> 00:01:08,430
million customers and when something

00:01:04,979 --> 00:01:11,580
goes wrong in your service usually not

00:01:08,430 --> 00:01:17,490
only one customer is affected or even

00:01:11,580 --> 00:01:19,080
twenty five or even 100 usually many

00:01:17,490 --> 00:01:20,850
many more are affected and this is

00:01:19,080 --> 00:01:25,200
something that we think about a lot and

00:01:20,850 --> 00:01:27,689
one of the things we think about is if

00:01:25,200 --> 00:01:31,460
you think about what happens when when

00:01:27,689 --> 00:01:33,960
we have an incident very very quickly

00:01:31,460 --> 00:01:37,350
the customers that affect it become

00:01:33,960 --> 00:01:40,229
faceless there are only 900 pictures on

00:01:37,350 --> 00:01:42,390
this image and on this slide but

00:01:40,229 --> 00:01:44,640
oftentimes when we have an incident

00:01:42,390 --> 00:01:46,740
it can affect it can it doesn't always

00:01:44,640 --> 00:01:51,000
but it can affect even millions of

00:01:46,740 --> 00:01:56,610
people so let me give you a little bit

00:01:51,000 --> 00:01:59,640
more context on my work as we said we

00:01:56,610 --> 00:02:02,939
have more than 100 million customers

00:01:59,640 --> 00:02:04,680
around the world together they watch

00:02:02,939 --> 00:02:09,720
more than a hundred and twenty five

00:02:04,680 --> 00:02:11,760
million hours of content every day so if

00:02:09,720 --> 00:02:13,090
you put those two numbers together you

00:02:11,760 --> 00:02:16,750
will see

00:02:13,090 --> 00:02:18,550
that if one of my services fails you can

00:02:16,750 --> 00:02:21,430
affect a lot of people relatively

00:02:18,550 --> 00:02:23,050
quickly so time is of the essence and as

00:02:21,430 --> 00:02:25,480
we'll go through this presentation this

00:02:23,050 --> 00:02:30,400
will become clear how we think about

00:02:25,480 --> 00:02:32,049
this today we have about 380 micro

00:02:30,400 --> 00:02:34,540
services in production I say about

00:02:32,049 --> 00:02:36,790
because that changes sometimes it is

00:02:34,540 --> 00:02:39,190
also the case that some of the services

00:02:36,790 --> 00:02:40,959
that we run really don't deserve a name

00:02:39,190 --> 00:02:43,420
that has Micro in it because they're

00:02:40,959 --> 00:02:44,860
really gigantic but we have that's

00:02:43,420 --> 00:02:48,430
that's about the scale that we're

00:02:44,860 --> 00:02:52,680
looking at and Netflix also runs on over

00:02:48,430 --> 00:02:55,510
1,000 device types so that ranges from

00:02:52,680 --> 00:02:59,739
you know your mobile phones even older

00:02:55,510 --> 00:03:01,930
kinds of devices to set-top boxes and to

00:02:59,739 --> 00:03:04,780
your TV at home to your Smart TV

00:03:01,930 --> 00:03:08,049
all of these we support so over a

00:03:04,780 --> 00:03:13,720
thousand device types and we do all of

00:03:08,049 --> 00:03:15,549
this with less than 10 Korres sarees so

00:03:13,720 --> 00:03:21,190
you may think that we're a little bit

00:03:15,549 --> 00:03:22,630
crazy and that may well be true but let

00:03:21,190 --> 00:03:25,389
me tell you a little bit about how we

00:03:22,630 --> 00:03:28,000
accomplish this one of the ways that we

00:03:25,389 --> 00:03:30,519
accomplish this is by having a very

00:03:28,000 --> 00:03:32,920
strong DevOps culture which is not the

00:03:30,519 --> 00:03:36,790
topic of this talk but I'm happy to tell

00:03:32,920 --> 00:03:39,220
you all about off line essentially every

00:03:36,790 --> 00:03:41,109
team is responsible for the own services

00:03:39,220 --> 00:03:44,069
that they run and then the central core

00:03:41,109 --> 00:03:49,450
SRE team is responsible for the overall

00:03:44,069 --> 00:03:53,470
reliability of the service this picture

00:03:49,450 --> 00:03:56,410
gives you a more better visualization of

00:03:53,470 --> 00:03:59,139
our distributed service now these are

00:03:56,410 --> 00:04:01,510
the 380 roughly services that I

00:03:59,139 --> 00:04:04,150
mentioned earlier now note that these

00:04:01,510 --> 00:04:06,430
are only the services that are what we

00:04:04,150 --> 00:04:09,090
call on the streaming path and that

00:04:06,430 --> 00:04:11,319
means when you come to Netflix

00:04:09,090 --> 00:04:13,120
everything that you need in order to

00:04:11,319 --> 00:04:15,160
actually be able to watch a movie

00:04:13,120 --> 00:04:17,799
normally you don't need all of them in

00:04:15,160 --> 00:04:20,590
order to be able to watch your movie but

00:04:17,799 --> 00:04:22,560
those every circle here represents a

00:04:20,590 --> 00:04:25,300
service that is responsible for

00:04:22,560 --> 00:04:26,320
something that you may do on the service

00:04:25,300 --> 00:04:31,160
sir

00:04:26,320 --> 00:04:32,470
recommendations localized descriptions

00:04:31,160 --> 00:04:34,940
and so forth

00:04:32,470 --> 00:04:36,889
so what you see over here is you see

00:04:34,940 --> 00:04:39,080
traffic coming in from the internet from

00:04:36,889 --> 00:04:41,750
the left and going through our various

00:04:39,080 --> 00:04:44,840
gateways so that we have a gateway that

00:04:41,750 --> 00:04:47,600
routes traffic and the gateway routes

00:04:44,840 --> 00:04:49,580
traffic to our various front ends the

00:04:47,600 --> 00:04:51,979
biggest of which you can think of is the

00:04:49,580 --> 00:04:56,360
the dot here in the middle which is our

00:04:51,979 --> 00:05:00,320
API what we call our API and the API to

00:04:56,360 --> 00:05:03,410
give you a sense talks directly to about

00:05:00,320 --> 00:05:05,840
70 services downstream from it and then

00:05:03,410 --> 00:05:08,870
of course indirectly to several hundred

00:05:05,840 --> 00:05:12,139
as we've as we talked so if you think

00:05:08,870 --> 00:05:16,940
about the system and our central team

00:05:12,139 --> 00:05:20,450
being responsible for this what I see is

00:05:16,940 --> 00:05:23,210
that insights are everything you need to

00:05:20,450 --> 00:05:25,400
automate the heck out of this thing you

00:05:23,210 --> 00:05:28,130
need to make sure that whatever you can

00:05:25,400 --> 00:05:30,020
get automatically you get it is not

00:05:28,130 --> 00:05:32,030
feasible that one person understands

00:05:30,020 --> 00:05:37,880
everything that goes on inside of the

00:05:32,030 --> 00:05:40,490
system so how do we get insights we have

00:05:37,880 --> 00:05:45,590
various tools in our toolbox but one of

00:05:40,490 --> 00:05:48,349
our most critical ones is mantis mantis

00:05:45,590 --> 00:05:52,400
is a cloud native stream processing

00:05:48,349 --> 00:05:55,580
service built on top of mezzos the way

00:05:52,400 --> 00:05:57,680
mantis works is that we instrument a lot

00:05:55,580 --> 00:06:00,110
of the micro services that you saw in

00:05:57,680 --> 00:06:03,970
the previous slide not all of them but

00:06:00,110 --> 00:06:08,510
many of the very critical ones and we

00:06:03,970 --> 00:06:10,220
give them the ability to emit metrics to

00:06:08,510 --> 00:06:12,800
mantis

00:06:10,220 --> 00:06:15,080
we also gather we also have the ability

00:06:12,800 --> 00:06:18,620
to get gather metrics from various other

00:06:15,080 --> 00:06:23,270
data sources such as Kafka and Amazon s3

00:06:18,620 --> 00:06:25,970
and various others and so then the data

00:06:23,270 --> 00:06:29,690
flows through mantis and the way this

00:06:25,970 --> 00:06:33,470
works is that internal users so Netflix

00:06:29,690 --> 00:06:35,900
developers can write mantis jobs mantis

00:06:33,470 --> 00:06:37,340
like I said is built on top of mazes

00:06:35,900 --> 00:06:39,409
it's a mazes framework that we've

00:06:37,340 --> 00:06:43,309
developed and then our our net

00:06:39,409 --> 00:06:46,189
developers crate made mantas jobs that

00:06:43,309 --> 00:06:48,529
can aggregate data that can filter data

00:06:46,189 --> 00:06:50,869
that can perform operations on that data

00:06:48,529 --> 00:06:53,589
and do various other things that

00:06:50,869 --> 00:06:57,099
eventually lead to such things as alerts

00:06:53,589 --> 00:07:00,050
operational dashboards anomaly detection

00:06:57,099 --> 00:07:02,749
so the way to think about this is our

00:07:00,050 --> 00:07:06,169
micro services and these other data

00:07:02,749 --> 00:07:08,389
sources emit operational metrics that

00:07:06,169 --> 00:07:11,469
give you insights about service health

00:07:08,389 --> 00:07:14,569
and those metrics are processed

00:07:11,469 --> 00:07:16,759
automatically with mantis jobs and then

00:07:14,569 --> 00:07:19,039
lead to alerts operational dashboards

00:07:16,759 --> 00:07:21,860
and so forth and they do all of that in

00:07:19,039 --> 00:07:23,809
real time and again that is very

00:07:21,860 --> 00:07:25,999
critical because we have we can have a

00:07:23,809 --> 00:07:31,039
lot of users affected very very quickly

00:07:25,999 --> 00:07:34,819
when something goes wrong let's take a

00:07:31,039 --> 00:07:37,819
little bit of a look under the hood as

00:07:34,819 --> 00:07:41,509
I've mentioned mantis is is a mazes

00:07:37,819 --> 00:07:44,899
framework and in order for it to satisfy

00:07:41,509 --> 00:07:46,519
our own needs our own Netflix needs we

00:07:44,899 --> 00:07:52,179
developed our own scheduler called

00:07:46,519 --> 00:07:54,860
fenzel which is open source penso is

00:07:52,179 --> 00:07:58,419
optimized for cloud it is very important

00:07:54,860 --> 00:08:02,269
for us because all of our our entire

00:07:58,419 --> 00:08:04,309
streaming infrastructure and all of our

00:08:02,269 --> 00:08:08,409
Mantis jobs and everything else that we

00:08:04,309 --> 00:08:10,819
run runs on top of AWS in this case ec2

00:08:08,409 --> 00:08:13,339
so what does it mean for fenzel to be

00:08:10,819 --> 00:08:16,129
optimized for cloud one of the things

00:08:13,339 --> 00:08:20,689
that allows us to do is it allows us to

00:08:16,129 --> 00:08:22,669
scale the underlying agent cluster auto

00:08:20,689 --> 00:08:25,009
scaling is very important to us and

00:08:22,669 --> 00:08:27,499
we'll see that a little bit later on but

00:08:25,009 --> 00:08:30,499
the amount of data that gets pushed

00:08:27,499 --> 00:08:32,539
through mantas jobs there is greatly

00:08:30,499 --> 00:08:34,819
over the course of the day and the

00:08:32,539 --> 00:08:37,490
course of the week so as our resource

00:08:34,819 --> 00:08:42,409
needs change we also need to scale the

00:08:37,490 --> 00:08:45,230
underlying clusters fens was also

00:08:42,409 --> 00:08:47,600
designed to allow us to have to satisfy

00:08:45,230 --> 00:08:49,390
various other constraints that we have

00:08:47,600 --> 00:08:51,670
many of which are you know

00:08:49,390 --> 00:08:54,520
pertain to the cloud some of which that

00:08:51,670 --> 00:08:58,210
don't so for instance we can do ping bin

00:08:54,520 --> 00:09:00,310
packing task affinity and one of the

00:08:58,210 --> 00:09:02,350
other things that has to do with you

00:09:00,310 --> 00:09:06,850
know the way we have our cloud set up is

00:09:02,350 --> 00:09:09,400
spreading tasks across ec2 availability

00:09:06,850 --> 00:09:11,410
zones so that he can accomplish high

00:09:09,400 --> 00:09:17,860
availability which obviously is quite

00:09:11,410 --> 00:09:20,110
important for us let's talk a little bit

00:09:17,860 --> 00:09:24,730
about how we use mantis and what we use

00:09:20,110 --> 00:09:27,550
mantis for internally at Netflix one of

00:09:24,730 --> 00:09:30,580
the things that we do with mantis is

00:09:27,550 --> 00:09:33,610
something called real-time SPS SPS

00:09:30,580 --> 00:09:35,290
stands for stream starts per second so

00:09:33,610 --> 00:09:37,540
essentially this is a metric of how

00:09:35,290 --> 00:09:40,180
often people are able to come to Netflix

00:09:37,540 --> 00:09:44,950
click Play and actually are successfully

00:09:40,180 --> 00:09:47,680
able to watch so it works this is our

00:09:44,950 --> 00:09:50,110
one of our most important top level

00:09:47,680 --> 00:09:54,790
metrics that we use to determine system

00:09:50,110 --> 00:09:58,450
health so if SPS hits a wall and drops

00:09:54,790 --> 00:10:00,970
we know something is going wrong so

00:09:58,450 --> 00:10:05,160
real-time SPS which is powered by mantis

00:10:00,970 --> 00:10:08,380
allows us to get insights in real-time

00:10:05,160 --> 00:10:10,840
into whether SPS is in line with our

00:10:08,380 --> 00:10:14,230
expectations or not so what you see here

00:10:10,840 --> 00:10:16,570
generally are two lines and the black

00:10:14,230 --> 00:10:19,960
line is essentially the expectation that

00:10:16,570 --> 00:10:21,970
we have of what will happen which is

00:10:19,960 --> 00:10:24,070
based on historical data and various

00:10:21,970 --> 00:10:27,310
algorithms that we have and the blue

00:10:24,070 --> 00:10:29,590
line is the actual observed SPS so

00:10:27,310 --> 00:10:31,630
sometimes they're a little bit off I

00:10:29,590 --> 00:10:33,760
didn't show the numbers here because I

00:10:31,630 --> 00:10:35,890
cannot but the scale obviously matters

00:10:33,760 --> 00:10:40,260
here but it gives you an idea of how

00:10:35,890 --> 00:10:40,260
closely aligned we are with expectations

00:10:41,490 --> 00:10:47,050
real-time SPS lets us know that

00:10:44,890 --> 00:10:49,090
something is wrong if something is wrong

00:10:47,050 --> 00:10:52,150
that something is wrong in seconds not

00:10:49,090 --> 00:10:53,680
minutes we have various other ways in

00:10:52,150 --> 00:10:56,680
which we can determine whether something

00:10:53,680 --> 00:10:59,710
is wrong with SPS or our overall system

00:10:56,680 --> 00:11:01,660
health but real-time SPS really lets us

00:10:59,710 --> 00:11:04,060
know within seconds sometimes

00:11:01,660 --> 00:11:06,070
and that can make the difference between

00:11:04,060 --> 00:11:07,810
you know nine hundred and nine million

00:11:06,070 --> 00:11:11,250
users being affected so this is

00:11:07,810 --> 00:11:11,250
something that's pretty critical to us

00:11:12,390 --> 00:11:17,560
real-time SPS in our case and for many

00:11:15,550 --> 00:11:19,630
of our metrics this is the case we're

00:11:17,560 --> 00:11:21,820
able to break it down by a region this

00:11:19,630 --> 00:11:24,100
is important because many of the

00:11:21,820 --> 00:11:27,910
incidents that we have these days are

00:11:24,100 --> 00:11:31,840
isolated to one region we run in

00:11:27,910 --> 00:11:34,380
multiple regions on top of AWS and the

00:11:31,840 --> 00:11:37,210
way we generally roll out our software

00:11:34,380 --> 00:11:40,090
ensures that when incidents do happen

00:11:37,210 --> 00:11:41,920
they happen generally only in one region

00:11:40,090 --> 00:11:44,260
which is great because then we have

00:11:41,920 --> 00:11:46,330
several mitigation techniques that we

00:11:44,260 --> 00:11:48,100
can apply for instance shifting traffic

00:11:46,330 --> 00:11:51,100
from run region to another which we do

00:11:48,100 --> 00:11:54,430
frequently we just did last night and

00:11:51,100 --> 00:11:56,440
hopefully nobody noticed so break down

00:11:54,430 --> 00:11:58,330
by region is important and then break

00:11:56,440 --> 00:12:01,480
down by device type is also important so

00:11:58,330 --> 00:12:03,090
SPS real-time SPS and our other metrics

00:12:01,480 --> 00:12:05,860
are generally broken down by device type

00:12:03,090 --> 00:12:08,770
which allows us to narrow in very

00:12:05,860 --> 00:12:12,160
quickly on which device or devices are

00:12:08,770 --> 00:12:14,320
affected that is important because as I

00:12:12,160 --> 00:12:16,750
mentioned we have a very strong DevOps

00:12:14,320 --> 00:12:20,320
culture we have subject matter matter

00:12:16,750 --> 00:12:22,960
experts that are you know in the know

00:12:20,320 --> 00:12:25,060
and responsible for specific parts of

00:12:22,960 --> 00:12:27,520
our system and that also applies to

00:12:25,060 --> 00:12:31,210
specific device types so if we have an

00:12:27,520 --> 00:12:37,260
issue for instance that only effects say

00:12:31,210 --> 00:12:37,260
TVs then we know whom to call for help

00:12:37,890 --> 00:12:44,560
we don't only use mantas for SPS as I've

00:12:42,160 --> 00:12:45,640
alluded to we use it for a variety of

00:12:44,560 --> 00:12:48,850
different metrics

00:12:45,640 --> 00:12:53,440
again the idea is the same teams can set

00:12:48,850 --> 00:12:56,860
up individual dashboards and alerts and

00:12:53,440 --> 00:13:00,910
other kinds of analyses for all kinds of

00:12:56,860 --> 00:13:03,400
different metrics that we gather so for

00:13:00,910 --> 00:13:05,710
instance here what you see is I had to

00:13:03,400 --> 00:13:11,020
block out the specific paths but what

00:13:05,710 --> 00:13:14,640
you can do here is the API team set up a

00:13:11,020 --> 00:13:18,150
dashboard that shows elevated 500

00:13:14,640 --> 00:13:20,310
specific paths and the specific paths or

00:13:18,150 --> 00:13:23,100
endpoints are specific to specific

00:13:20,310 --> 00:13:27,030
devices so again that lets you if you

00:13:23,100 --> 00:13:29,040
have now elevated five hundreds error

00:13:27,030 --> 00:13:32,250
codes coming from a specific endpoint

00:13:29,040 --> 00:13:33,810
then you know that gives you clues as to

00:13:32,250 --> 00:13:40,620
what might have happened if something

00:13:33,810 --> 00:13:42,630
does go wrong as I mentioned before auto

00:13:40,620 --> 00:13:43,950
scaling is really important to us and

00:13:42,630 --> 00:13:47,190
that's one of the reasons that we

00:13:43,950 --> 00:13:49,710
developed fins oh we run on top of ec2

00:13:47,190 --> 00:13:52,710
and we scale the cluster day in and day

00:13:49,710 --> 00:13:54,780
out all the time that goes for our

00:13:52,710 --> 00:13:57,540
services that are on the streaming path

00:13:54,780 --> 00:14:00,870
as well as for the Mantis jobs that we

00:13:57,540 --> 00:14:03,240
run for analysis and insights as you can

00:14:00,870 --> 00:14:06,030
see here roughly eighteen million

00:14:03,240 --> 00:14:08,820
messages flow through mantis drops at

00:14:06,030 --> 00:14:12,480
peak whereas only about 6 million flow

00:14:08,820 --> 00:14:14,640
through mantis during trough and if you

00:14:12,480 --> 00:14:16,890
think about it why is that this is

00:14:14,640 --> 00:14:18,870
because you know usage of our actual

00:14:16,890 --> 00:14:20,640
service varies greatly over the course

00:14:18,870 --> 00:14:22,440
of the day you know people don't want

00:14:20,640 --> 00:14:24,090
tend to watch a lot of Netflix while

00:14:22,440 --> 00:14:26,640
they're sleeping but they do tend to

00:14:24,090 --> 00:14:28,290
watch Netflix while they're at home in

00:14:26,640 --> 00:14:30,210
the evening and not so much while

00:14:28,290 --> 00:14:34,590
they're at work so you see this you know

00:14:30,210 --> 00:14:37,380
the cyclical nature of our of our the

00:14:34,590 --> 00:14:39,090
usage of our service and the usage or

00:14:37,380 --> 00:14:41,970
the number of messages that flow through

00:14:39,090 --> 00:14:44,550
mantas excuse me mirrors that over the

00:14:41,970 --> 00:14:47,670
course of the day so as we as we go we

00:14:44,550 --> 00:14:52,050
have to scale the cluster so we can make

00:14:47,670 --> 00:14:54,990
better use of our resources but we don't

00:14:52,050 --> 00:14:57,390
stop there we do something that I call

00:14:54,990 --> 00:15:01,170
streaming on-demand and the way this

00:14:57,390 --> 00:15:04,230
works is that we allow users again

00:15:01,170 --> 00:15:07,890
that's internal Netflix developers to

00:15:04,230 --> 00:15:10,140
set up ad hoc queries that allow them to

00:15:07,890 --> 00:15:13,770
stream data only when it's needed for

00:15:10,140 --> 00:15:17,100
operational insights the way this works

00:15:13,770 --> 00:15:20,580
is that you as the developer you can set

00:15:17,100 --> 00:15:25,020
up a query say that pertains to a

00:15:20,580 --> 00:15:28,480
specific status code or path and only

00:15:25,020 --> 00:15:30,339
then that data is streamed from our say

00:15:28,480 --> 00:15:31,870
p.i service or one of the other micro

00:15:30,339 --> 00:15:34,750
services that we run on the streaming

00:15:31,870 --> 00:15:37,420
path that data is then streamed and

00:15:34,750 --> 00:15:39,209
analyzed by mantas and you can get

00:15:37,420 --> 00:15:44,500
real-time insights and real-time

00:15:39,209 --> 00:15:48,190
information on on that service only when

00:15:44,500 --> 00:15:50,500
you need it and again that is important

00:15:48,190 --> 00:15:52,630
because we have so much data that we

00:15:50,500 --> 00:15:54,579
would otherwise have to push through the

00:15:52,630 --> 00:15:57,660
system that it's actually just not

00:15:54,579 --> 00:15:59,949
really feasible so with this we actually

00:15:57,660 --> 00:16:02,290
greatly reduce the amount of data that

00:15:59,949 --> 00:16:05,680
does need to get streamed through our

00:16:02,290 --> 00:16:07,510
system while at the same time allowing

00:16:05,680 --> 00:16:09,519
for the same kind of flexibility and the

00:16:07,510 --> 00:16:11,949
ability to have those insights so when

00:16:09,519 --> 00:16:14,769
you define one of these ad-hoc queries a

00:16:11,949 --> 00:16:19,240
mantas job gets created under the hood

00:16:14,769 --> 00:16:22,269
and started on one of our agents it

00:16:19,240 --> 00:16:24,430
hosts again auto-scaling comes into the

00:16:22,269 --> 00:16:28,480
picture here so this doesn't follow

00:16:24,430 --> 00:16:30,519
exactly the same cyclic nature as we saw

00:16:28,480 --> 00:16:32,829
for the number of messages but what you

00:16:30,519 --> 00:16:35,589
see here is essentially how many jobs

00:16:32,829 --> 00:16:39,790
get set up on top of mantas over the

00:16:35,589 --> 00:16:41,709
course of a day so essentially when

00:16:39,790 --> 00:16:44,110
people are at work and they're debugging

00:16:41,709 --> 00:16:46,389
and they're trying different things or

00:16:44,110 --> 00:16:48,069
different smaller things cannot come up

00:16:46,389 --> 00:16:52,120
that they want to chase down they set up

00:16:48,069 --> 00:16:56,190
mantas jobs to to dig into before

00:16:52,120 --> 00:16:58,870
performance and behavior of our service

00:16:56,190 --> 00:17:01,149
what does that all mean let's bring it

00:16:58,870 --> 00:17:02,860
back I think it's important that we

00:17:01,149 --> 00:17:05,559
sometimes take a step back and think

00:17:02,860 --> 00:17:07,329
about the infrastructure work that we do

00:17:05,559 --> 00:17:09,640
or the work that feels like it's very

00:17:07,329 --> 00:17:10,839
deep down in the stack and take a step

00:17:09,640 --> 00:17:15,819
back and think about how it actually

00:17:10,839 --> 00:17:19,959
effects real users in my case what

00:17:15,819 --> 00:17:22,120
mantas means for us is faster detection

00:17:19,959 --> 00:17:24,220
of issues so we know of issues in

00:17:22,120 --> 00:17:26,350
seconds rather than minutes which

00:17:24,220 --> 00:17:28,000
somebody sitting in I don't know India

00:17:26,350 --> 00:17:32,910
in their living room trying to watch

00:17:28,000 --> 00:17:32,910
watch Netflix it may really affect them

00:17:33,270 --> 00:17:38,620
we have faster insights into the causes

00:17:36,190 --> 00:17:41,430
of those incidents so not only do we

00:17:38,620 --> 00:17:44,310
understand something is wrong

00:17:41,430 --> 00:17:46,890
because of mantas we can actually get

00:17:44,310 --> 00:17:51,720
faster insights into specifically what

00:17:46,890 --> 00:17:54,900
happened and what went wrong and again

00:17:51,720 --> 00:17:57,120
we have a faster path to mitigation so

00:17:54,900 --> 00:17:58,860
we can if we know what's going on or if

00:17:57,120 --> 00:18:01,530
we know for instance which region is

00:17:58,860 --> 00:18:03,960
affected we can very quickly take steps

00:18:01,530 --> 00:18:05,580
to mitigate the issue so if only one

00:18:03,960 --> 00:18:07,620
region is affected again we can just

00:18:05,580 --> 00:18:10,020
shift traffic away from that region and

00:18:07,620 --> 00:18:12,270
nobody no users will ever know that

00:18:10,020 --> 00:18:17,460
anything bad when happened in the first

00:18:12,270 --> 00:18:18,840
place hopefully and this is what we

00:18:17,460 --> 00:18:22,080
really care about at the end of the day

00:18:18,840 --> 00:18:24,660
all of us right and we care about making

00:18:22,080 --> 00:18:26,520
sure that our customers are happy and

00:18:24,660 --> 00:18:31,230
that they get the service that they

00:18:26,520 --> 00:18:32,670
actually signed up for if you want to

00:18:31,230 --> 00:18:35,580
contact me here's my contact information

00:18:32,670 --> 00:18:37,400
I'll be around for questions if any of

00:18:35,580 --> 00:18:41,619
you have any thank you

00:18:37,400 --> 00:18:41,619

YouTube URL: https://www.youtube.com/watch?v=vzJFDHuYE8E


