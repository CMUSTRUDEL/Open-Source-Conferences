Title: Webinar: Resource Management in Kubernetes: Navigating the Waters to Cloud Efficiency
Publication date: 2020-09-16
Playlist: LF Live Webinars
Description: 
	Sponsored by Carbon Relay 
Correctly provisioning Kubernetes resources is challenging. It is difficult to manage configuration options at scale, and understanding the impact of Kubernetes changes on underlying infrastructure can be borderline humanly impossible. How can you run containerized applications while dealing with the complexities of managing multiple resources needed for environments?
Captions: 
	00:00:00,000 --> 00:00:04,080
my screen real quick so thank you

00:00:02,879 --> 00:00:07,279
everyone for joining

00:00:04,080 --> 00:00:08,960
um kristen said my name is ofer i'm the

00:00:07,279 --> 00:00:11,120
cto of carbon relay

00:00:08,960 --> 00:00:12,639
um i'll be talking to you today about

00:00:11,120 --> 00:00:15,360
resource management and kubernetes

00:00:12,639 --> 00:00:17,359
and uh what it means and tell you a bit

00:00:15,360 --> 00:00:20,480
about what we do to

00:00:17,359 --> 00:00:21,199
all of that so hopefully you all can see

00:00:20,480 --> 00:00:24,400
my screen

00:00:21,199 --> 00:00:25,840
um start with a quick agenda

00:00:24,400 --> 00:00:27,840
so some of the topics will be about

00:00:25,840 --> 00:00:28,560
today obviously resource management is a

00:00:27,840 --> 00:00:31,519
big thing

00:00:28,560 --> 00:00:32,559
which is a lot of different topics i'm

00:00:31,519 --> 00:00:35,040
going to focus

00:00:32,559 --> 00:00:36,160
starting with the basics requests limits

00:00:35,040 --> 00:00:37,680
quality of service

00:00:36,160 --> 00:00:39,360
uh and if these things don't matter to

00:00:37,680 --> 00:00:41,840
you well hopefully they will soon

00:00:39,360 --> 00:00:43,600
we'll talk about common pitfalls my

00:00:41,840 --> 00:00:45,200
experience the company's experience and

00:00:43,600 --> 00:00:47,120
i'm sure at least some of you will have

00:00:45,200 --> 00:00:48,640
experienced that before um

00:00:47,120 --> 00:00:50,879
moving to best practices and what you

00:00:48,640 --> 00:00:53,039
should be doing when you're looking into

00:00:50,879 --> 00:00:54,480
resource allocation kubernetes and then

00:00:53,039 --> 00:00:55,920
we'll shift into

00:00:54,480 --> 00:00:57,760
what i consider a slightly more advanced

00:00:55,920 --> 00:00:59,840
topic in terms of performance

00:00:57,760 --> 00:01:01,840
optimization resource utilization and

00:00:59,840 --> 00:01:03,520
tuning evolving application and

00:01:01,840 --> 00:01:05,040
um will be called the continuous

00:01:03,520 --> 00:01:07,520
optimization process on

00:01:05,040 --> 00:01:08,320
kubernetes obviously at the end i'll

00:01:07,520 --> 00:01:10,320
leave time for

00:01:08,320 --> 00:01:12,159
for your questions hopefully i can

00:01:10,320 --> 00:01:15,200
answer all

00:01:12,159 --> 00:01:16,640
so to start off why are we even talking

00:01:15,200 --> 00:01:19,360
about resource management

00:01:16,640 --> 00:01:20,880
um but hopefully for those of you here

00:01:19,360 --> 00:01:23,840
you're not actually asking that question

00:01:20,880 --> 00:01:24,960
um but as background you know for those

00:01:23,840 --> 00:01:27,040
of you who've been

00:01:24,960 --> 00:01:28,960
in the development world for a while you

00:01:27,040 --> 00:01:30,640
know that in the old days we had

00:01:28,960 --> 00:01:32,640
applications that were running standard

00:01:30,640 --> 00:01:35,360
mode right i would write my application

00:01:32,640 --> 00:01:37,360
this big monolith of code and my

00:01:35,360 --> 00:01:40,960
expectation would be to deploy it

00:01:37,360 --> 00:01:43,520
to a single machine and that box

00:01:40,960 --> 00:01:45,280
is all my application had to play with

00:01:43,520 --> 00:01:47,600
and it would grab as many resources

00:01:45,280 --> 00:01:48,799
as it could because it was down there

00:01:47,600 --> 00:01:50,560
was no need for

00:01:48,799 --> 00:01:52,320
any kind of fencing or dating there was

00:01:50,560 --> 00:01:53,040
no no neighbors which i'll talk about

00:01:52,320 --> 00:01:55,840
later

00:01:53,040 --> 00:01:57,280
um life life was nice and simple um

00:01:55,840 --> 00:01:58,560
obviously that's not

00:01:57,280 --> 00:02:00,240
the future not where we're going and

00:01:58,560 --> 00:02:02,000
it's not even the situation today for

00:02:00,240 --> 00:02:04,079
most of us

00:02:02,000 --> 00:02:05,680
um most of us if not all of us are

00:02:04,079 --> 00:02:08,800
moving towards distributed systems

00:02:05,680 --> 00:02:10,640
for a variety of reasons um when we

00:02:08,800 --> 00:02:12,319
have applications that are distributed

00:02:10,640 --> 00:02:14,000
both in terms of the workload and in

00:02:12,319 --> 00:02:16,239
terms of the resources

00:02:14,000 --> 00:02:17,440
we start talking about orchestration we

00:02:16,239 --> 00:02:18,959
need to fence

00:02:17,440 --> 00:02:21,520
different resource allocations we need

00:02:18,959 --> 00:02:22,640
to figure out how different components

00:02:21,520 --> 00:02:24,480
work together

00:02:22,640 --> 00:02:26,319
um in the case of old applications these

00:02:24,480 --> 00:02:29,120
applications are usually not

00:02:26,319 --> 00:02:30,480
meant to be containerized um again like

00:02:29,120 --> 00:02:31,519
i said they were meant to one standalone

00:02:30,480 --> 00:02:32,879
whereas today

00:02:31,519 --> 00:02:34,800
you know you have plenty of containers

00:02:32,879 --> 00:02:37,920
pods um different services

00:02:34,800 --> 00:02:40,319
all running together all sharing uh

00:02:37,920 --> 00:02:41,200
a pool of resources that is your cluster

00:02:40,319 --> 00:02:42,879
or

00:02:41,200 --> 00:02:44,239
your data center or whatever that is

00:02:42,879 --> 00:02:47,200
what this means to you is

00:02:44,239 --> 00:02:47,840
you know doing the things we used to do

00:02:47,200 --> 00:02:49,120
in the past

00:02:47,840 --> 00:02:50,560
when it comes to resource allocation

00:02:49,120 --> 00:02:52,160
it's not going to work when you're

00:02:50,560 --> 00:02:54,239
moving to the detriment systems

00:02:52,160 --> 00:02:55,840
that's true if you're migrating

00:02:54,239 --> 00:02:57,200
obviously application

00:02:55,840 --> 00:02:59,440
that's true if you're developing a new

00:02:57,200 --> 00:03:01,760
application to be cloud native and it's

00:02:59,440 --> 00:03:04,159
even true as i'll show you if you're

00:03:01,760 --> 00:03:05,680
just growing some example online and

00:03:04,159 --> 00:03:09,120
trying to run it

00:03:05,680 --> 00:03:09,120
on a cluster in a distributed fashion

00:03:09,440 --> 00:03:13,280
so getting up and running with

00:03:10,879 --> 00:03:15,360
kubernetes i don't know how many of you

00:03:13,280 --> 00:03:16,400
are actually running applications on

00:03:15,360 --> 00:03:17,920
kubernetes now

00:03:16,400 --> 00:03:19,519
for those of you who have already gone

00:03:17,920 --> 00:03:21,120
down math uh or

00:03:19,519 --> 00:03:22,720
journey of kubernetes you know how easy

00:03:21,120 --> 00:03:24,239
it is right super simple you just get a

00:03:22,720 --> 00:03:25,519
cluster you get your audition you deploy

00:03:24,239 --> 00:03:28,480
everything's nice and

00:03:25,519 --> 00:03:30,319
life is great um obviously i like that

00:03:28,480 --> 00:03:32,000
facetiously kubernetes is

00:03:30,319 --> 00:03:33,920
amazing for many different ways

00:03:32,000 --> 00:03:36,560
simplicity is not one of them

00:03:33,920 --> 00:03:37,519
um you know when we when we first want

00:03:36,560 --> 00:03:39,519
to

00:03:37,519 --> 00:03:40,560
get our distributed application up and

00:03:39,519 --> 00:03:42,959
running and

00:03:40,560 --> 00:03:44,000
focusing on kubernetes first step is

00:03:42,959 --> 00:03:47,680
grab a cluster

00:03:44,000 --> 00:03:49,200
uh honestly not the the not the painful

00:03:47,680 --> 00:03:52,400
process that it used to be in the past

00:03:49,200 --> 00:03:54,879
you can go on gta or eks today

00:03:52,400 --> 00:03:56,799
click a few buttons uh give them a card

00:03:54,879 --> 00:03:59,680
and get a cluster up and running

00:03:56,799 --> 00:04:00,640
and then the second step is an

00:03:59,680 --> 00:04:03,200
application that i

00:04:00,640 --> 00:04:04,560
run i want to design and deploy it right

00:04:03,200 --> 00:04:06,080
i can write it separately and then

00:04:04,560 --> 00:04:08,080
deploy it to the cluster

00:04:06,080 --> 00:04:09,840
that is not as simple a process as

00:04:08,080 --> 00:04:11,840
getting a cluster up and running

00:04:09,840 --> 00:04:13,280
i need to figure out well what are the

00:04:11,840 --> 00:04:15,040
different components that need to be

00:04:13,280 --> 00:04:16,479
allocated different resources

00:04:15,040 --> 00:04:17,919
how do i think and how do i make sure

00:04:16,479 --> 00:04:18,320
they ship the right amount of resources

00:04:17,919 --> 00:04:20,639
and they

00:04:18,320 --> 00:04:22,560
all sort of behave not together how do i

00:04:20,639 --> 00:04:25,199
make sure they scale the right

00:04:22,560 --> 00:04:26,800
so that when my applications are load

00:04:25,199 --> 00:04:28,160
they actually behave the way i want them

00:04:26,800 --> 00:04:31,360
to be

00:04:28,160 --> 00:04:32,320
and this this really this is the crux of

00:04:31,360 --> 00:04:32,880
what we're going to be talking about

00:04:32,320 --> 00:04:34,560
today

00:04:32,880 --> 00:04:36,479
how do you take an application and you

00:04:34,560 --> 00:04:38,080
make sure you set the right parameters

00:04:36,479 --> 00:04:40,800
for in order for it to behave where you

00:04:38,080 --> 00:04:40,800
want it to behave

00:04:41,040 --> 00:04:44,720
so starting with the very basics we're

00:04:43,199 --> 00:04:45,840
talking about resource automation we

00:04:44,720 --> 00:04:47,600
have to talk about the resources

00:04:45,840 --> 00:04:48,479
themselves and what we have available to

00:04:47,600 --> 00:04:50,720
us

00:04:48,479 --> 00:04:52,240
so in binaries there are a few what i

00:04:50,720 --> 00:04:52,880
would call native types things that are

00:04:52,240 --> 00:04:55,840
managed

00:04:52,880 --> 00:04:56,720
uh by coop for you the memory the cpu

00:04:55,840 --> 00:04:58,880
i'm not going to touch

00:04:56,720 --> 00:05:00,240
page sizes as much but it's good to know

00:04:58,880 --> 00:05:01,840
that it's out there

00:05:00,240 --> 00:05:03,039
those are things that natively we all

00:05:01,840 --> 00:05:04,080
think about when we start thinking about

00:05:03,039 --> 00:05:05,360
resource allocation

00:05:04,080 --> 00:05:07,039
right whenever someone says well how

00:05:05,360 --> 00:05:08,080
many resources does your application

00:05:07,039 --> 00:05:09,919
need

00:05:08,080 --> 00:05:11,759
the immediate thought is well how much

00:05:09,919 --> 00:05:13,199
memory and cpu does each pod or does

00:05:11,759 --> 00:05:15,039
each container need

00:05:13,199 --> 00:05:17,919
right how much is this going to consume

00:05:15,039 --> 00:05:19,520
how much memory does this database need

00:05:17,919 --> 00:05:21,360
in order to not crash or flow over how

00:05:19,520 --> 00:05:22,880
much cpu do i need to give to my workers

00:05:21,360 --> 00:05:24,479
so that they can perform

00:05:22,880 --> 00:05:26,560
and give me the performance that i

00:05:24,479 --> 00:05:28,800
expect or the the users will have the

00:05:26,560 --> 00:05:31,440
performance that i expect

00:05:28,800 --> 00:05:33,440
underlying those let's say you know as a

00:05:31,440 --> 00:05:36,639
second as a second order

00:05:33,440 --> 00:05:39,680
there is a slew of parameters um

00:05:36,639 --> 00:05:40,560
or slew of resources that we still need

00:05:39,680 --> 00:05:41,680
to think of

00:05:40,560 --> 00:05:44,800
right if you're running a java

00:05:41,680 --> 00:05:47,199
application um for for the java crowd

00:05:44,800 --> 00:05:49,280
um if you've ever tried to run a java

00:05:47,199 --> 00:05:51,280
application in a distributed fashion

00:05:49,280 --> 00:05:53,360
and didn't have to tinker with the gmvm

00:05:51,280 --> 00:05:56,400
um you're probably some sort of magician

00:05:53,360 --> 00:05:57,680
uh that's amazing but jvm heaps is super

00:05:56,400 --> 00:05:59,440
critical to think about when you're when

00:05:57,680 --> 00:06:01,600
you're splitting up your resources

00:05:59,440 --> 00:06:03,280
the same goes for resources obviously

00:06:01,600 --> 00:06:04,800
there's tons under it

00:06:03,280 --> 00:06:06,319
um and there's a lot of things that we

00:06:04,800 --> 00:06:08,080
have developers need to touch

00:06:06,319 --> 00:06:09,199
these days in order to make sure the

00:06:08,080 --> 00:06:10,960
applications are running in a stable

00:06:09,199 --> 00:06:12,000
fashion

00:06:10,960 --> 00:06:13,759
the first thing to think about when

00:06:12,000 --> 00:06:14,400
you're going to kubernetes is requests

00:06:13,759 --> 00:06:16,880
and limits

00:06:14,400 --> 00:06:18,319
um i'm going to have to make a slight

00:06:16,880 --> 00:06:20,160
note on the word request

00:06:18,319 --> 00:06:21,840
this is obviously within the kubernetes

00:06:20,160 --> 00:06:23,120
world so for those of you who aren't as

00:06:21,840 --> 00:06:24,560
familiar with kubernetes

00:06:23,120 --> 00:06:26,960
the requests i'm talking about here are

00:06:24,560 --> 00:06:29,680
not to be conflated with http requests

00:06:26,960 --> 00:06:30,800
these are not the the same requests this

00:06:29,680 --> 00:06:33,520
is a particular

00:06:30,800 --> 00:06:34,960
sort of reserve word in kubernetes when

00:06:33,520 --> 00:06:36,319
you start an application

00:06:34,960 --> 00:06:38,000
you define an application when you write

00:06:36,319 --> 00:06:40,000
the manifests for an application the

00:06:38,000 --> 00:06:42,560
definitions of what the application is

00:06:40,000 --> 00:06:44,880
you can um specify two things at the

00:06:42,560 --> 00:06:48,319
container level requests and limits

00:06:44,880 --> 00:06:50,880
requests for any given resource

00:06:48,319 --> 00:06:51,599
is as it sounds me as the container i'm

00:06:50,880 --> 00:06:53,680
requesting

00:06:51,599 --> 00:06:55,120
a gig of memory in order to operate

00:06:53,680 --> 00:06:56,240
right the minimum amount of resource

00:06:55,120 --> 00:06:58,240
that i need

00:06:56,240 --> 00:07:00,400
in order to be able to carry on my my

00:06:58,240 --> 00:07:02,080
operations the actual resources designed

00:07:00,400 --> 00:07:03,360
to me can be bigger than the requests

00:07:02,080 --> 00:07:06,479
but they can't be smaller

00:07:03,360 --> 00:07:09,919
if i don't have in my cluster

00:07:06,479 --> 00:07:11,199
enough resources to give to each to

00:07:09,919 --> 00:07:14,080
the container requesting it the

00:07:11,199 --> 00:07:17,120
container just won't start

00:07:14,080 --> 00:07:17,919
the complemented that is limit so if we

00:07:17,120 --> 00:07:20,800
said request

00:07:17,919 --> 00:07:22,160
is the absolute minimal number uh that i

00:07:20,800 --> 00:07:24,400
need in order to operate

00:07:22,160 --> 00:07:25,759
limit is the absolute limit here's where

00:07:24,400 --> 00:07:27,599
i would say you know

00:07:25,759 --> 00:07:28,960
i can have a container i would define

00:07:27,599 --> 00:07:29,759
the request for the memory for the

00:07:28,960 --> 00:07:31,759
container as

00:07:29,759 --> 00:07:33,599
one gigabyte memory i can define the

00:07:31,759 --> 00:07:35,680
limit as two gigabytes

00:07:33,599 --> 00:07:37,199
give me the bare minimum one gig i need

00:07:35,680 --> 00:07:38,160
that to operate but don't ever give me

00:07:37,199 --> 00:07:40,080
more than two

00:07:38,160 --> 00:07:41,440
because i know i don't need it maybe i

00:07:40,080 --> 00:07:43,520
run some tests on it before

00:07:41,440 --> 00:07:44,800
maybe i just don't have it limits is

00:07:43,520 --> 00:07:46,240
where you set

00:07:44,800 --> 00:07:48,080
the right guardrails to make sure your

00:07:46,240 --> 00:07:49,599
applications don't don't uh and

00:07:48,080 --> 00:07:51,520
your containers and your pads don't get

00:07:49,599 --> 00:07:54,800
ahead of themselves and

00:07:51,520 --> 00:07:54,800
you know overrun your cluster

00:07:55,360 --> 00:07:59,039
so what does it actually look like when

00:07:57,280 --> 00:08:00,319
we go to the point application so

00:07:59,039 --> 00:08:02,000
on the right hand side you can see a

00:08:00,319 --> 00:08:03,599
snippet of a manifest if you've never

00:08:02,000 --> 00:08:06,240
seen a manifest before

00:08:03,599 --> 00:08:07,759
um great starting point to kubernetes

00:08:06,240 --> 00:08:09,759
it's all based on yaml files

00:08:07,759 --> 00:08:12,319
and and we write things as verbosely as

00:08:09,759 --> 00:08:14,319
possible so what you see here is a pod

00:08:12,319 --> 00:08:16,160
that has two containers and the two

00:08:14,319 --> 00:08:18,319
containers have explicit

00:08:16,160 --> 00:08:19,680
limits and requests for both cpu and

00:08:18,319 --> 00:08:22,800
memory

00:08:19,680 --> 00:08:24,800
when i go to deploy it i may have a node

00:08:22,800 --> 00:08:27,039
and sort of see a simple picture on the

00:08:24,800 --> 00:08:28,479
left i have a node that has a certain

00:08:27,039 --> 00:08:31,199
amount of cpu numbering

00:08:28,479 --> 00:08:31,759
and kubernetes will ask well okay can i

00:08:31,199 --> 00:08:34,399
deploy

00:08:31,759 --> 00:08:35,279
this pod to this node and what it checks

00:08:34,399 --> 00:08:38,320
for

00:08:35,279 --> 00:08:39,760
is the total number of requests given by

00:08:38,320 --> 00:08:40,800
all the containers on the pod both on

00:08:39,760 --> 00:08:42,399
memory and cpu

00:08:40,800 --> 00:08:44,320
so if i look at this i have a total

00:08:42,399 --> 00:08:47,360
request of

00:08:44,320 --> 00:08:47,920
1.5 gigs of memory and i have a total

00:08:47,360 --> 00:08:51,040
request

00:08:47,920 --> 00:08:54,640
of 0.6 cores on the cpu

00:08:51,040 --> 00:08:56,000
when i deploy this pod um

00:08:54,640 --> 00:08:58,080
kube will basically go and look for a

00:08:56,000 --> 00:08:59,680
node that has enough of those

00:08:58,080 --> 00:09:01,760
right if it doesn't this pod cannot

00:08:59,680 --> 00:09:02,560
cannot be deployed if it does it'll slap

00:09:01,760 --> 00:09:04,160
it on a node

00:09:02,560 --> 00:09:06,800
and um i'm gonna be talking about

00:09:04,160 --> 00:09:07,279
scheduling in bits and pieces i won't go

00:09:06,800 --> 00:09:09,760
into it

00:09:07,279 --> 00:09:10,959
too deeply but the process of putting a

00:09:09,760 --> 00:09:13,519
pod on a node

00:09:10,959 --> 00:09:14,320
is called scheduling inside kubernetes

00:09:13,519 --> 00:09:16,480
so

00:09:14,320 --> 00:09:18,720
that's on the requests right if i have

00:09:16,480 --> 00:09:21,200
enough i'll be put on a certain node

00:09:18,720 --> 00:09:22,320
the limits as i said before is once the

00:09:21,200 --> 00:09:24,480
pod is on the node

00:09:22,320 --> 00:09:26,480
dynamically it can be allocated more or

00:09:24,480 --> 00:09:28,959
less resources

00:09:26,480 --> 00:09:30,160
this manifest on the on the right

00:09:28,959 --> 00:09:32,160
guarantees

00:09:30,160 --> 00:09:33,600
that this pod will not get more than

00:09:32,160 --> 00:09:37,279
three gigs of memory overall

00:09:33,600 --> 00:09:39,839
uh and the same for 1.5 course so

00:09:37,279 --> 00:09:41,519
again those are the the very very basic

00:09:39,839 --> 00:09:44,560
uh of requests and limits

00:09:41,519 --> 00:09:47,120
at the pod and container level

00:09:44,560 --> 00:09:48,640
so when we go to write these manifest

00:09:47,120 --> 00:09:50,640
right the whole goal here at the end of

00:09:48,640 --> 00:09:53,040
the day is to assign values

00:09:50,640 --> 00:09:54,560
to the memory and cpu at the pod and

00:09:53,040 --> 00:09:56,080
container level and like i said not just

00:09:54,560 --> 00:09:56,720
memory and cpu right we're going to get

00:09:56,080 --> 00:09:58,720
to

00:09:56,720 --> 00:10:00,959
places where it's it's way broader than

00:09:58,720 --> 00:10:02,720
that how do we think about these numbers

00:10:00,959 --> 00:10:05,440
and how do we think about

00:10:02,720 --> 00:10:07,040
what should i put there well obviously

00:10:05,440 --> 00:10:08,959
there's there's going to be some sort of

00:10:07,040 --> 00:10:10,399
goldilocks that that may or may not even

00:10:08,959 --> 00:10:13,200
be static

00:10:10,399 --> 00:10:14,240
too low if i set my requests and limits

00:10:13,200 --> 00:10:15,760
to be too low

00:10:14,240 --> 00:10:17,279
what i'm going to face is performance

00:10:15,760 --> 00:10:18,720
degradation right

00:10:17,279 --> 00:10:20,320
don't get don't give enough memory to

00:10:18,720 --> 00:10:22,800
your database it might crash

00:10:20,320 --> 00:10:24,240
don't give enough cpu to a worker your

00:10:22,800 --> 00:10:26,000
users are going to be experiencing high

00:10:24,240 --> 00:10:29,360
latency

00:10:26,000 --> 00:10:30,880
too high which i will admit is that the

00:10:29,360 --> 00:10:32,399
fault that i see today and and the

00:10:30,880 --> 00:10:34,079
default we all sort of

00:10:32,399 --> 00:10:36,959
converge to if we don't know what we're

00:10:34,079 --> 00:10:38,399
doing if i say you know what i have no

00:10:36,959 --> 00:10:41,440
idea what my database needs to be

00:10:38,399 --> 00:10:43,680
just give it five gigs but it'll be fine

00:10:41,440 --> 00:10:44,959
um i'm running into two issues there the

00:10:43,680 --> 00:10:47,760
first one is obviously i'm just

00:10:44,959 --> 00:10:49,200
wasting resources right i if i don't

00:10:47,760 --> 00:10:50,640
truly need it

00:10:49,200 --> 00:10:52,880
then my database is going to sit there

00:10:50,640 --> 00:10:54,320
idle uh eating away both my credit card

00:10:52,880 --> 00:10:56,880
bill and my ability to deploy other

00:10:54,320 --> 00:10:58,880
applications to my cluster

00:10:56,880 --> 00:11:00,000
more than that if i did constrain my

00:10:58,880 --> 00:11:02,959
cluster to to a certain

00:11:00,000 --> 00:11:04,480
size and now i'm requesting large chunks

00:11:02,959 --> 00:11:04,959
of memory and cpu what i'm going to end

00:11:04,480 --> 00:11:07,120
up with

00:11:04,959 --> 00:11:08,480
is unscheduled pods and kubernetes is

00:11:07,120 --> 00:11:09,920
going to come back in and say sorry i

00:11:08,480 --> 00:11:11,200
just don't have room for this

00:11:09,920 --> 00:11:12,560
and then i'm going to have to go back

00:11:11,200 --> 00:11:14,240
basically go back to the drawing board

00:11:12,560 --> 00:11:14,720
figure out well why am i requesting five

00:11:14,240 --> 00:11:16,480
gigs and

00:11:14,720 --> 00:11:17,920
what do i actually have left i'll have

00:11:16,480 --> 00:11:18,320
to go through deployment and monitoring

00:11:17,920 --> 00:11:19,760
and

00:11:18,320 --> 00:11:21,440
do this manual process that i'll

00:11:19,760 --> 00:11:24,320
describe later to figure out

00:11:21,440 --> 00:11:25,839
what where what is just right um and at

00:11:24,320 --> 00:11:29,120
the end we'll get to a really nice

00:11:25,839 --> 00:11:31,200
automated process to get the just right

00:11:29,120 --> 00:11:32,320
so that's on requests and limits as a

00:11:31,200 --> 00:11:34,480
baseline

00:11:32,320 --> 00:11:35,519
obviously with everything uh communities

00:11:34,480 --> 00:11:37,600
there's always a

00:11:35,519 --> 00:11:38,560
more complex layer and in this case it's

00:11:37,600 --> 00:11:40,160
the quality of service

00:11:38,560 --> 00:11:41,600
so quality of service is something that

00:11:40,160 --> 00:11:42,000
i will limit i was not aware of when i

00:11:41,600 --> 00:11:43,680
first

00:11:42,000 --> 00:11:45,440
started with kubernetes i heard requests

00:11:43,680 --> 00:11:46,640
and limits numbers make perfect sense to

00:11:45,440 --> 00:11:48,399
me let's just go

00:11:46,640 --> 00:11:49,680
but actually there's a second order of

00:11:48,399 --> 00:11:51,680
complexity here

00:11:49,680 --> 00:11:53,040
um that is tied to the relationship

00:11:51,680 --> 00:11:56,399
between the requests on the limits

00:11:53,040 --> 00:11:59,680
at the pot level so quality of service

00:11:56,399 --> 00:12:01,279
is a a definition at the pod level

00:11:59,680 --> 00:12:02,959
and there are basically three classes

00:12:01,279 --> 00:12:03,839
for quality of service guaranteed

00:12:02,959 --> 00:12:06,560
burstable

00:12:03,839 --> 00:12:07,760
and best effort and they are just as

00:12:06,560 --> 00:12:10,560
they sound

00:12:07,760 --> 00:12:11,680
guaranteed in order for a pod to be

00:12:10,560 --> 00:12:13,839
classified as

00:12:11,680 --> 00:12:15,360
guaranteed quality of service every

00:12:13,839 --> 00:12:18,160
container inside the pod

00:12:15,360 --> 00:12:19,120
has to have a definition of both limits

00:12:18,160 --> 00:12:21,440
and requests

00:12:19,120 --> 00:12:22,399
and those have to be equal to each other

00:12:21,440 --> 00:12:24,720
so what does that mean

00:12:22,399 --> 00:12:26,800
it means that for every container i say

00:12:24,720 --> 00:12:29,440
i'm requesting one cpu of memory

00:12:26,800 --> 00:12:31,200
and my limit is one cpu of memory that

00:12:29,440 --> 00:12:34,000
means it is the safest

00:12:31,200 --> 00:12:34,959
bet i can take hey always always always

00:12:34,000 --> 00:12:37,519
give this

00:12:34,959 --> 00:12:39,040
container one cpu of memory always

00:12:37,519 --> 00:12:40,079
always always give this container two

00:12:39,040 --> 00:12:42,079
gigs

00:12:40,079 --> 00:12:44,240
sorry once if you one course of you and

00:12:42,079 --> 00:12:46,480
two gigs of memory or whatever it is

00:12:44,240 --> 00:12:48,399
there's no fluctuation um you know

00:12:46,480 --> 00:12:50,079
there's no questions on whether or not

00:12:48,399 --> 00:12:51,440
i'm i'm gonna have i'm gonna eat more

00:12:50,079 --> 00:12:52,800
resources and have noisy neighbors or

00:12:51,440 --> 00:12:54,639
anything like that

00:12:52,800 --> 00:12:57,120
what it does incur is rigidity and

00:12:54,639 --> 00:12:58,959
scheduling so like we said before

00:12:57,120 --> 00:13:00,800
in order to schedule a pot in order to

00:12:58,959 --> 00:13:02,720
place a pod on a node

00:13:00,800 --> 00:13:04,720
that node has to have enough resources

00:13:02,720 --> 00:13:07,120
to accommodate the pod

00:13:04,720 --> 00:13:08,959
because i'm being rigid with my request

00:13:07,120 --> 00:13:10,160
on limits i'm likely going to go over

00:13:08,959 --> 00:13:12,800
what my baseline is

00:13:10,160 --> 00:13:14,240
which means i'll have a harder time

00:13:12,800 --> 00:13:15,360
scheduling i'm going to have a very

00:13:14,240 --> 00:13:17,519
rigid scheduling

00:13:15,360 --> 00:13:19,279
um of the pods and i'm probably going to

00:13:17,519 --> 00:13:20,480
have to bump up my cluster a little bit

00:13:19,279 --> 00:13:22,079
or at least understand

00:13:20,480 --> 00:13:24,399
the exact resource utilization in my

00:13:22,079 --> 00:13:24,399
cluster

00:13:24,480 --> 00:13:27,920
the next class of quality of service is

00:13:26,480 --> 00:13:31,120
burstable so again

00:13:27,920 --> 00:13:32,880
just as the name suggests these are pods

00:13:31,120 --> 00:13:35,600
that are meant to be first of all

00:13:32,880 --> 00:13:38,959
they're meant to be able to

00:13:35,600 --> 00:13:40,320
respond to a workload by upping their

00:13:38,959 --> 00:13:42,880
resources

00:13:40,320 --> 00:13:43,440
but hopefully stay at a lower baseline

00:13:42,880 --> 00:13:45,600
so

00:13:43,440 --> 00:13:47,680
where does this come in you can think of

00:13:45,600 --> 00:13:50,160
maybe an e-commerce application

00:13:47,680 --> 00:13:52,000
that during the day sees a very low

00:13:50,160 --> 00:13:53,839
threshold of activity

00:13:52,000 --> 00:13:55,199
and then 5 pm when everyone logs off

00:13:53,839 --> 00:13:57,600
slack they go in and they start buying

00:13:55,199 --> 00:14:01,040
stuff and i get this sort of a spike

00:13:57,600 --> 00:14:02,160
well how do i want to to get my requests

00:14:01,040 --> 00:14:04,079
and limits in

00:14:02,160 --> 00:14:05,920
i don't want to set them at the baseline

00:14:04,079 --> 00:14:08,160
level because if i do when i see that

00:14:05,920 --> 00:14:09,279
spike things are going to start crashing

00:14:08,160 --> 00:14:10,720
i also don't want to set them at the

00:14:09,279 --> 00:14:11,839
spike level because that means that 80

00:14:10,720 --> 00:14:12,959
percent of my day i'll just be wasting

00:14:11,839 --> 00:14:15,920
resources

00:14:12,959 --> 00:14:16,880
this is where burstable uh burstable

00:14:15,920 --> 00:14:18,560
pods are an option

00:14:16,880 --> 00:14:21,040
first of all pods are defined as those

00:14:18,560 --> 00:14:22,720
that the limits are actually larger than

00:14:21,040 --> 00:14:24,000
the requests the strictly larger they're

00:14:22,720 --> 00:14:25,199
not equal meaning they're not in the

00:14:24,000 --> 00:14:26,800
guarantee

00:14:25,199 --> 00:14:28,240
what this will do is it will allow me to

00:14:26,800 --> 00:14:32,000
schedule the pods

00:14:28,240 --> 00:14:34,320
um more easily than i would have

00:14:32,000 --> 00:14:36,880
in a guaranteed quality of service but

00:14:34,320 --> 00:14:39,920
it will also allow the pods to extend

00:14:36,880 --> 00:14:41,440
um their resources if something comes in

00:14:39,920 --> 00:14:42,160
and there's a bigger workload by setting

00:14:41,440 --> 00:14:44,160
the limits

00:14:42,160 --> 00:14:45,839
larger i basically tell the i tell

00:14:44,160 --> 00:14:46,959
kubernetes and then the pod tells women

00:14:45,839 --> 00:14:48,480
is hey i can stretch

00:14:46,959 --> 00:14:50,480
up to three gigs or i can stretch up

00:14:48,480 --> 00:14:53,279
three-quarters or whatever it is

00:14:50,480 --> 00:14:54,959
the downside here is uh a term i

00:14:53,279 --> 00:14:56,959
mentioned before and i'll mention again

00:14:54,959 --> 00:14:58,720
several times noising evers so now you

00:14:56,959 --> 00:15:00,560
can imagine if i have multiple burstable

00:14:58,720 --> 00:15:03,760
pods on a single node

00:15:00,560 --> 00:15:05,040
and they request more resources at the

00:15:03,760 --> 00:15:08,000
same time right because

00:15:05,040 --> 00:15:09,839
one thing we don't control is when when

00:15:08,000 --> 00:15:11,360
do these resources get allocated there

00:15:09,839 --> 00:15:12,959
are ways of controlling it those those

00:15:11,360 --> 00:15:16,480
are much more advanced than sort of uh

00:15:12,959 --> 00:15:18,560
in real time you can see a place where

00:15:16,480 --> 00:15:19,920
one service requested more resources at

00:15:18,560 --> 00:15:22,399
a given moment

00:15:19,920 --> 00:15:24,000
basically throttled the node now i can't

00:15:22,399 --> 00:15:26,399
schedule or i can't even

00:15:24,000 --> 00:15:28,639
um get more juice out of the other other

00:15:26,399 --> 00:15:31,920
parts of the note

00:15:28,639 --> 00:15:34,079
my third class best effort um again

00:15:31,920 --> 00:15:34,959
it is what it sounds no requests or

00:15:34,079 --> 00:15:38,079
limits set

00:15:34,959 --> 00:15:38,560
uh at all this is the most dangerous

00:15:38,079 --> 00:15:40,639
option

00:15:38,560 --> 00:15:42,240
and i'll show you an example later it's

00:15:40,639 --> 00:15:43,279
actually one of those things that if you

00:15:42,240 --> 00:15:45,680
were not aware

00:15:43,279 --> 00:15:47,360
of the best effort quality of service uh

00:15:45,680 --> 00:15:50,000
you may have fallen into it because you

00:15:47,360 --> 00:15:51,040
just didn't set anything very closely

00:15:50,000 --> 00:15:53,279
two things that are going to happen with

00:15:51,040 --> 00:15:54,000
best effort so one again i keep talking

00:15:53,279 --> 00:15:55,600
about scheduling

00:15:54,000 --> 00:15:57,360
there's there's something in scheduling

00:15:55,600 --> 00:15:58,320
uh that determines the priority of each

00:15:57,360 --> 00:16:00,160
pod

00:15:58,320 --> 00:16:01,600
best effort pods are going to be evicted

00:16:00,160 --> 00:16:02,079
first meaning if you didn't send

00:16:01,600 --> 00:16:03,839
anything

00:16:02,079 --> 00:16:05,680
and some other pod needs more resources

00:16:03,839 --> 00:16:08,160
now kube might just

00:16:05,680 --> 00:16:10,560
kick your pot out altogether and it'll

00:16:08,160 --> 00:16:12,959
get evicted from from the node

00:16:10,560 --> 00:16:14,320
the the other side to that is by not

00:16:12,959 --> 00:16:15,920
setting a limit

00:16:14,320 --> 00:16:17,279
what is likely to happen is you'll just

00:16:15,920 --> 00:16:19,040
grab as much as you can we're now going

00:16:17,279 --> 00:16:20,720
back to sort of the old days where

00:16:19,040 --> 00:16:22,800
parts of the application can just grab

00:16:20,720 --> 00:16:25,120
as many resources as they can thereby

00:16:22,800 --> 00:16:27,279
throttling anyone else that's on the

00:16:25,120 --> 00:16:30,000
node

00:16:27,279 --> 00:16:30,560
so what usually happens what do we

00:16:30,000 --> 00:16:34,480
usually do

00:16:30,560 --> 00:16:36,000
and this is called these little sins are

00:16:34,480 --> 00:16:38,240
something that every single one of us

00:16:36,000 --> 00:16:39,440
have done again if someone

00:16:38,240 --> 00:16:41,519
contradicts me this i'm going to call

00:16:39,440 --> 00:16:42,800
you a magician but someone everyone has

00:16:41,519 --> 00:16:45,360
done this to some extent

00:16:42,800 --> 00:16:47,040
right going with defaults or just

00:16:45,360 --> 00:16:48,560
ignoring it all together well i have my

00:16:47,040 --> 00:16:50,240
application right i used to run this

00:16:48,560 --> 00:16:50,880
we've we've ran this application for 20

00:16:50,240 --> 00:16:51,920
years

00:16:50,880 --> 00:16:53,680
i don't need to change anything that

00:16:51,920 --> 00:16:55,040
runs just fine well we've talked about

00:16:53,680 --> 00:16:56,480
this cloud native architecture is not

00:16:55,040 --> 00:16:58,560
the same

00:16:56,480 --> 00:16:59,519
as what you're used to running on

00:16:58,560 --> 00:17:01,519
monoliths

00:16:59,519 --> 00:17:03,360
and it will require resourcing updates

00:17:01,519 --> 00:17:04,959
at the bare minimum requires sort of

00:17:03,360 --> 00:17:06,480
fencing even if the total number of

00:17:04,959 --> 00:17:08,559
resources is set

00:17:06,480 --> 00:17:10,000
understanding the different components

00:17:08,559 --> 00:17:11,439
of your of your application and the

00:17:10,000 --> 00:17:13,120
resources required for those different

00:17:11,439 --> 00:17:15,120
components is critical

00:17:13,120 --> 00:17:16,880
um something similar hopping from a blog

00:17:15,120 --> 00:17:18,880
post i'll show you example of just that

00:17:16,880 --> 00:17:21,360
if i if i have time

00:17:18,880 --> 00:17:22,079
configurations that work for someone uh

00:17:21,360 --> 00:17:24,640
is is

00:17:22,079 --> 00:17:26,000
the equivalent of it runs on my machine

00:17:24,640 --> 00:17:28,319
um

00:17:26,000 --> 00:17:29,520
the configurations and the actual

00:17:28,319 --> 00:17:31,200
parameters that you set in your

00:17:29,520 --> 00:17:33,520
application are specific to

00:17:31,200 --> 00:17:35,360
your application your cluster and most

00:17:33,520 --> 00:17:36,960
importantly your workload

00:17:35,360 --> 00:17:38,240
even if you have the exact same cluster

00:17:36,960 --> 00:17:38,880
but you're seeing different traffic

00:17:38,240 --> 00:17:40,240
patterns

00:17:38,880 --> 00:17:41,760
you may be seeing completely different

00:17:40,240 --> 00:17:43,600
behaviors for the exact same

00:17:41,760 --> 00:17:45,520
configuration you have to tune your

00:17:43,600 --> 00:17:47,840
configuration to the workload

00:17:45,520 --> 00:17:49,120
i know the third thing is not testing

00:17:47,840 --> 00:17:49,600
this not going through a rigorous

00:17:49,120 --> 00:17:51,200
process

00:17:49,600 --> 00:17:53,120
this isn't something that's historically

00:17:51,200 --> 00:17:54,799
been part of our

00:17:53,120 --> 00:17:56,559
toolkit as developers we never had to

00:17:54,799 --> 00:18:00,559
think about this we just deployed

00:17:56,559 --> 00:18:02,000
um deploying blind is super dangerous um

00:18:00,559 --> 00:18:03,840
you know the first step could be things

00:18:02,000 --> 00:18:06,640
don't deploy honestly

00:18:03,840 --> 00:18:08,240
that's even the safer option um the more

00:18:06,640 --> 00:18:09,200
risky option is it does deploy when you

00:18:08,240 --> 00:18:11,679
deploy it

00:18:09,200 --> 00:18:13,039
and then you know at two in the morning

00:18:11,679 --> 00:18:13,440
when when someone on the other side of

00:18:13,039 --> 00:18:15,120
the

00:18:13,440 --> 00:18:16,960
world wakes up and starts doing

00:18:15,120 --> 00:18:20,000
transactions and crashes

00:18:16,960 --> 00:18:22,880
so we have to make sure

00:18:20,000 --> 00:18:23,520
we tune our application to our specific

00:18:22,880 --> 00:18:26,840
needs

00:18:23,520 --> 00:18:28,960
application cluster and the workload

00:18:26,840 --> 00:18:31,760
itself

00:18:28,960 --> 00:18:32,559
converse to that what should we do well

00:18:31,760 --> 00:18:34,640
first of all

00:18:32,559 --> 00:18:35,760
uh i've mentioned quality of service you

00:18:34,640 --> 00:18:38,160
have to understand

00:18:35,760 --> 00:18:39,760
the quality of service of your pods um

00:18:38,160 --> 00:18:40,720
like i said before when i started

00:18:39,760 --> 00:18:41,840
i don't even know what quality of

00:18:40,720 --> 00:18:43,280
service was i didn't know that it

00:18:41,840 --> 00:18:45,760
existed and

00:18:43,280 --> 00:18:46,559
the class itself is just as important as

00:18:45,760 --> 00:18:48,480
the numbers

00:18:46,559 --> 00:18:50,080
um that you assign to the requested

00:18:48,480 --> 00:18:51,600
limits you have to think of the

00:18:50,080 --> 00:18:52,720
different pods and how critical they are

00:18:51,600 --> 00:18:54,720
to the operation

00:18:52,720 --> 00:18:56,000
of the application some may have higher

00:18:54,720 --> 00:18:57,280
priorities than others

00:18:56,000 --> 00:19:00,559
and you have to set the quality of

00:18:57,280 --> 00:19:02,960
service in that way your critical paths

00:19:00,559 --> 00:19:04,640
i'm going to say have to be guaranteed

00:19:02,960 --> 00:19:06,000
they don't have to but they have to take

00:19:04,640 --> 00:19:07,039
high priority when it comes to the

00:19:06,000 --> 00:19:08,559
actual

00:19:07,039 --> 00:19:10,400
resources you allocate to them and the

00:19:08,559 --> 00:19:13,679
confidence that you have in

00:19:10,400 --> 00:19:15,280
those pods getting those resources noisy

00:19:13,679 --> 00:19:16,320
neighbors already talked about this

00:19:15,280 --> 00:19:18,240
depending on the number of your nodes

00:19:16,320 --> 00:19:19,760
and number of your services you have to

00:19:18,240 --> 00:19:21,760
start planning for

00:19:19,760 --> 00:19:22,960
what happens if these two pods fall in

00:19:21,760 --> 00:19:25,760
the same node

00:19:22,960 --> 00:19:27,200
um and in the the last point here is

00:19:25,760 --> 00:19:28,400
obviously dynamic loads i've talked

00:19:27,200 --> 00:19:30,160
about the load itself

00:19:28,400 --> 00:19:31,919
if you have a very static load that is

00:19:30,160 --> 00:19:33,360
constant throughout the day

00:19:31,919 --> 00:19:35,120
there's not a lot of reason to go with

00:19:33,360 --> 00:19:35,600
burstable pods all they're going to get

00:19:35,120 --> 00:19:38,000
you

00:19:35,600 --> 00:19:39,600
is just more noise in the system if if

00:19:38,000 --> 00:19:41,360
you have a way of tailoring your

00:19:39,600 --> 00:19:44,080
requirements

00:19:41,360 --> 00:19:45,760
of your resources to a good baseline

00:19:44,080 --> 00:19:49,280
you'll save money

00:19:45,760 --> 00:19:51,280
and and you won't have stability issues

00:19:49,280 --> 00:19:52,640
specifying resources explicitly someone

00:19:51,280 --> 00:19:55,280
tied to quality of service

00:19:52,640 --> 00:19:55,840
right don't ever ever ever use best

00:19:55,280 --> 00:19:58,080
effort

00:19:55,840 --> 00:19:59,280
um some people may yell at me for saying

00:19:58,080 --> 00:20:01,520
that but i'm i

00:19:59,280 --> 00:20:02,880
i'm gonna say that as a source of proof

00:20:01,520 --> 00:20:05,760
don't use best effort

00:20:02,880 --> 00:20:06,000
it's it's not stable enough and more

00:20:05,760 --> 00:20:08,080
than

00:20:06,000 --> 00:20:09,919
that your fellow developers if i come

00:20:08,080 --> 00:20:10,960
into a manifest and i don't see anything

00:20:09,919 --> 00:20:12,559
specified i'm

00:20:10,960 --> 00:20:14,159
probably just going to gloss over it if

00:20:12,559 --> 00:20:15,679
i don't know what i'm looking for

00:20:14,159 --> 00:20:18,960
increased visibility into what the

00:20:15,679 --> 00:20:20,400
resources are is always good for you

00:20:18,960 --> 00:20:22,240
the last thing it provides and i'll show

00:20:20,400 --> 00:20:24,559
you in a second if you do have resources

00:20:22,240 --> 00:20:27,200
specified explicitly in your manifest

00:20:24,559 --> 00:20:28,880
then at least you can tell um later on

00:20:27,200 --> 00:20:31,280
when you when you go and you iterate on

00:20:28,880 --> 00:20:33,280
the configurations that you have

00:20:31,280 --> 00:20:34,880
um either history if you're going with

00:20:33,280 --> 00:20:38,640
get ops or at least a way to see

00:20:34,880 --> 00:20:40,080
and tune things individually um

00:20:38,640 --> 00:20:41,760
last thing which i haven't talked about

00:20:40,080 --> 00:20:42,559
a lot and again as a whole sort of

00:20:41,760 --> 00:20:44,960
separate topic

00:20:42,559 --> 00:20:46,320
is is quotas so if you really want to

00:20:44,960 --> 00:20:46,799
manage your cluster well if you really

00:20:46,320 --> 00:20:49,760
want to

00:20:46,799 --> 00:20:50,960
keep a tight ship set quotas at the name

00:20:49,760 --> 00:20:52,320
space level

00:20:50,960 --> 00:20:54,159
separate out your applications to name

00:20:52,320 --> 00:20:56,480
spaces and understand

00:20:54,159 --> 00:20:58,080
what each application needs to get if

00:20:56,480 --> 00:20:59,440
you set that correctly if you know that

00:20:58,080 --> 00:21:01,280
this application as a whole

00:20:59,440 --> 00:21:02,640
again needs two gigs of memory and two

00:21:01,280 --> 00:21:04,159
course fully

00:21:02,640 --> 00:21:05,360
set that quota and then you can

00:21:04,159 --> 00:21:06,880
backtrack from that to the different

00:21:05,360 --> 00:21:10,480
containers and pods

00:21:06,880 --> 00:21:10,480
and understand how to separate those out

00:21:10,960 --> 00:21:14,799
so now let's talk about actual tuning

00:21:13,600 --> 00:21:18,320
and the actual

00:21:14,799 --> 00:21:20,080
uh process of getting to the right

00:21:18,320 --> 00:21:21,520
numbers i mentioned before you know if

00:21:20,080 --> 00:21:22,480
we have all these what we call

00:21:21,520 --> 00:21:23,440
parameters and i'll show you in the

00:21:22,480 --> 00:21:24,799
second line

00:21:23,440 --> 00:21:27,600
we have all these parameters we have all

00:21:24,799 --> 00:21:29,760
these resources that we want to tune

00:21:27,600 --> 00:21:31,600
the issue is the number of these

00:21:29,760 --> 00:21:33,760
parameters grows exponentially with the

00:21:31,600 --> 00:21:36,960
complexity of your application

00:21:33,760 --> 00:21:38,720
right if i look at a simple simple

00:21:36,960 --> 00:21:40,400
simple application five services i'll

00:21:38,720 --> 00:21:43,520
show you in a second and i want to tune

00:21:40,400 --> 00:21:44,240
resource limits requests for those five

00:21:43,520 --> 00:21:46,400
services

00:21:44,240 --> 00:21:48,240
so imagine maybe i make it easier on

00:21:46,400 --> 00:21:50,799
myself i have five pods

00:21:48,240 --> 00:21:52,559
each pod has one container for each

00:21:50,799 --> 00:21:55,280
container i want to tune

00:21:52,559 --> 00:21:56,159
requests limits of memory and cpu i'm

00:21:55,280 --> 00:21:58,000
looking at 20

00:21:56,159 --> 00:21:59,360
20 numbers that i need to set and i

00:21:58,000 --> 00:22:00,080
haven't touched replicas i haven't

00:21:59,360 --> 00:22:01,520
touched

00:22:00,080 --> 00:22:03,200
jvm if i have java in there i haven't

00:22:01,520 --> 00:22:07,840
touched anything internally

00:22:03,200 --> 00:22:09,679
um i will say me personally as a human

00:22:07,840 --> 00:22:11,360
being able to see 20 numbers at the same

00:22:09,679 --> 00:22:12,640
time all of which are correlated to some

00:22:11,360 --> 00:22:13,679
extent obviously because when the

00:22:12,640 --> 00:22:14,880
application runs

00:22:13,679 --> 00:22:17,360
there's a lot of things happening in the

00:22:14,880 --> 00:22:19,840
background incredibly hard

00:22:17,360 --> 00:22:21,039
i i don't have a good mental tally of 20

00:22:19,840 --> 00:22:21,520
different numbers and i can say well if

00:22:21,039 --> 00:22:22,960
i just

00:22:21,520 --> 00:22:24,720
tweak this one a little bit higher a

00:22:22,960 --> 00:22:27,840
little bit lower

00:22:24,720 --> 00:22:29,919
i could get to the right answer um again

00:22:27,840 --> 00:22:31,280
we talked about distributed systems

00:22:29,919 --> 00:22:32,480
when you run these things a lot of

00:22:31,280 --> 00:22:34,000
things happen in the background it's

00:22:32,480 --> 00:22:36,559
completely dynamic

00:22:34,000 --> 00:22:37,840
there's no way to just immediately know

00:22:36,559 --> 00:22:39,280
oh i understand completely how the

00:22:37,840 --> 00:22:40,400
application works

00:22:39,280 --> 00:22:42,400
you have things that are out of your

00:22:40,400 --> 00:22:42,960
control unless you really really lock

00:22:42,400 --> 00:22:44,640
down

00:22:42,960 --> 00:22:46,159
how your scheduling works how your

00:22:44,640 --> 00:22:47,360
resource allocation works everything's

00:22:46,159 --> 00:22:49,760
going to be dynamic because that's part

00:22:47,360 --> 00:22:51,840
of the power of kubernetes

00:22:49,760 --> 00:22:54,000
because the allocation is is dynamic and

00:22:51,840 --> 00:22:55,440
it's out of one shared pool

00:22:54,000 --> 00:22:56,400
this tuning is a moving target that's

00:22:55,440 --> 00:22:58,000
what we talked about earlier with

00:22:56,400 --> 00:22:59,120
scaling and i'll talk about that again

00:22:58,000 --> 00:23:01,760
in a second

00:22:59,120 --> 00:23:04,000
not all tunings meet the same kind of

00:23:01,760 --> 00:23:06,640
criteria when it comes to a workload

00:23:04,000 --> 00:23:08,240
all that is to say actually tuning if

00:23:06,640 --> 00:23:10,080
you're trying to do this by hand

00:23:08,240 --> 00:23:11,760
it's an incredibly inefficient and

00:23:10,080 --> 00:23:14,000
tedious process um

00:23:11,760 --> 00:23:16,320
not sort of dare anyone to come and tell

00:23:14,000 --> 00:23:18,240
me that they actually like it

00:23:16,320 --> 00:23:19,440
so where do we see this process come in

00:23:18,240 --> 00:23:21,120
and where is it important to actually

00:23:19,440 --> 00:23:23,360
look into it i will admit that a lot of

00:23:21,120 --> 00:23:26,400
us have not even thought of doing this

00:23:23,360 --> 00:23:27,840
right um a couple of years ago we

00:23:26,400 --> 00:23:30,159
strongly believe that

00:23:27,840 --> 00:23:32,080
as part of your cicd process as part of

00:23:30,159 --> 00:23:34,080
the cicd pipeline there has to come a

00:23:32,080 --> 00:23:35,600
step in which you tune

00:23:34,080 --> 00:23:37,840
your parameter values inside your

00:23:35,600 --> 00:23:40,960
application and

00:23:37,840 --> 00:23:41,840
where that part comes is between your

00:23:40,960 --> 00:23:43,440
unit testing

00:23:41,840 --> 00:23:45,440
so when i build a new component

00:23:43,440 --> 00:23:47,919
obviously i do some unit testing

00:23:45,440 --> 00:23:50,400
uh i push it maybe to a staging area and

00:23:47,919 --> 00:23:53,120
i want to start doing integration tests

00:23:50,400 --> 00:23:55,200
the integration tests historically tell

00:23:53,120 --> 00:23:56,880
me well all these things work together

00:23:55,200 --> 00:23:58,400
you know data is falling from one to

00:23:56,880 --> 00:24:00,480
another my requests are coming in

00:23:58,400 --> 00:24:02,159
nothing's not formed everything's okay

00:24:00,480 --> 00:24:03,679
but as we've talked about before that

00:24:02,159 --> 00:24:04,000
has nothing to do with your workload it

00:24:03,679 --> 00:24:05,360
doesn't

00:24:04,000 --> 00:24:07,039
say anything about your application

00:24:05,360 --> 00:24:08,320
being stable or being able to meet

00:24:07,039 --> 00:24:10,720
multiple loads

00:24:08,320 --> 00:24:11,679
this is where tuning the actual resource

00:24:10,720 --> 00:24:13,200
requirements and the different

00:24:11,679 --> 00:24:14,559
parameters inside your application

00:24:13,200 --> 00:24:16,799
becomes critical

00:24:14,559 --> 00:24:18,159
as as you do integration tests and as

00:24:16,799 --> 00:24:20,320
you sort of build the application

00:24:18,159 --> 00:24:21,919
back up you have to put it on the road

00:24:20,320 --> 00:24:24,400
when you have to think of

00:24:21,919 --> 00:24:25,039
which resources am i am i allocating

00:24:24,400 --> 00:24:27,200
where

00:24:25,039 --> 00:24:29,440
and will this meet the demands of the

00:24:27,200 --> 00:24:31,039
business at the end of the day

00:24:29,440 --> 00:24:32,960
all that has to come before deployment

00:24:31,039 --> 00:24:34,880
and all that has to be done continuously

00:24:32,960 --> 00:24:36,240
this isn't a fire forget well i did this

00:24:34,880 --> 00:24:37,919
once now i'm done

00:24:36,240 --> 00:24:39,279
the application involves the workload

00:24:37,919 --> 00:24:41,360
evolves the clusters evolve

00:24:39,279 --> 00:24:43,039
and the other applications around your

00:24:41,360 --> 00:24:44,880
application evolves right

00:24:43,039 --> 00:24:46,799
if you have you know 10 or 50 other

00:24:44,880 --> 00:24:47,840
developers putting stuff on the cluster

00:24:46,799 --> 00:24:48,960
you're going to run through these noisy

00:24:47,840 --> 00:24:50,559
neighbors all the time you're going to

00:24:48,960 --> 00:24:51,200
have to keep tuning your application as

00:24:50,559 --> 00:24:52,720
you go

00:24:51,200 --> 00:24:55,600
so all this process has to be as

00:24:52,720 --> 00:24:55,600
automated as possible

00:24:55,760 --> 00:24:59,120
so some options that we have as

00:24:58,480 --> 00:25:00,640
developers

00:24:59,120 --> 00:25:03,279
the first one i already talked about

00:25:00,640 --> 00:25:05,279
trial and error i'll just put something

00:25:03,279 --> 00:25:07,039
hopefully i won't ignore it at the very

00:25:05,279 --> 00:25:09,279
least i'll put some number

00:25:07,039 --> 00:25:10,400
to my parameters and you know i'll

00:25:09,279 --> 00:25:12,080
deploy it and

00:25:10,400 --> 00:25:14,240
cross my fingers hope nothing breaks in

00:25:12,080 --> 00:25:17,840
production

00:25:14,240 --> 00:25:21,039
easiest way to get started absolute

00:25:17,840 --> 00:25:21,840
worst way to to make your time efficient

00:25:21,039 --> 00:25:23,919
and valuable

00:25:21,840 --> 00:25:25,279
and again i personally have done this

00:25:23,919 --> 00:25:28,000
manually

00:25:25,279 --> 00:25:29,840
i hated it because it's confusing it's

00:25:28,000 --> 00:25:31,440
not intuitive and there's nothing

00:25:29,840 --> 00:25:33,360
at least to me incredibly rewarding

00:25:31,440 --> 00:25:34,559
about doing sort of manual tuning and

00:25:33,360 --> 00:25:36,159
figuring out

00:25:34,559 --> 00:25:38,799
what is the memory and cpu that i need

00:25:36,159 --> 00:25:40,960
to have in this this particular piece

00:25:38,799 --> 00:25:42,559
um an option that i particularly like as

00:25:40,960 --> 00:25:45,760
a scientist is do what we call

00:25:42,559 --> 00:25:46,720
a design of experiment so treat

00:25:45,760 --> 00:25:48,640
optimization as

00:25:46,720 --> 00:25:50,320
a scientific term say well i have some

00:25:48,640 --> 00:25:51,919
numbers that i need to figure out

00:25:50,320 --> 00:25:53,360
i need to do this methodically and i

00:25:51,919 --> 00:25:54,720
need to think it through

00:25:53,360 --> 00:25:56,240
maybe i have some things that i already

00:25:54,720 --> 00:25:56,960
know maybe i deployed this application

00:25:56,240 --> 00:25:58,720
before

00:25:56,960 --> 00:26:00,559
and my devops team came back and told me

00:25:58,720 --> 00:26:03,279
hey we monitored this and this pod is

00:26:00,559 --> 00:26:04,480
really getting throttled on cpu or hey

00:26:03,279 --> 00:26:05,440
we you know we're monitoring the

00:26:04,480 --> 00:26:07,039
resources and

00:26:05,440 --> 00:26:08,799
you know you ask for five gigs but

00:26:07,039 --> 00:26:10,080
really you're utilizing one so i have a

00:26:08,799 --> 00:26:12,720
couple of

00:26:10,080 --> 00:26:13,600
ideas i can take two or three parameters

00:26:12,720 --> 00:26:16,480
at most

00:26:13,600 --> 00:26:17,520
and try to come up with a grid of

00:26:16,480 --> 00:26:18,640
potential values

00:26:17,520 --> 00:26:20,960
and just go ahead and run through all of

00:26:18,640 --> 00:26:22,400
them run tests and see what works the

00:26:20,960 --> 00:26:24,000
best

00:26:22,400 --> 00:26:25,600
this works significantly better than

00:26:24,000 --> 00:26:26,960
doing trial and error it's

00:26:25,600 --> 00:26:28,559
like i said it's scientific it's

00:26:26,960 --> 00:26:30,080
methodical at least you're thinking

00:26:28,559 --> 00:26:32,400
through the problem you're getting some

00:26:30,080 --> 00:26:33,919
some kind of information out um but as i

00:26:32,400 --> 00:26:35,840
mentioned before like

00:26:33,919 --> 00:26:38,080
the higher your complexity the harder

00:26:35,840 --> 00:26:40,720
it's going to be to to manage

00:26:38,080 --> 00:26:42,480
multiple parameters at the same time two

00:26:40,720 --> 00:26:44,320
or three is

00:26:42,480 --> 00:26:45,760
is a good number if you get to five

00:26:44,320 --> 00:26:48,400
you're already

00:26:45,760 --> 00:26:49,760
starting to be thoroughly confused the

00:26:48,400 --> 00:26:50,320
third option and one that i'll show you

00:26:49,760 --> 00:26:51,679
soon

00:26:50,320 --> 00:26:53,600
is using machine learning and i'll

00:26:51,679 --> 00:26:54,240
explain why machine learning is a

00:26:53,600 --> 00:26:57,279
perfect

00:26:54,240 --> 00:26:58,320
uh use case for this the goal that we

00:26:57,279 --> 00:27:00,400
are

00:26:58,320 --> 00:27:02,240
pushing towards is running this thing

00:27:00,400 --> 00:27:03,440
automatically no developer should sit

00:27:02,240 --> 00:27:06,080
there and try to think

00:27:03,440 --> 00:27:07,120
exactly if they need one or two cores or

00:27:06,080 --> 00:27:09,120
whatever they need to understand the

00:27:07,120 --> 00:27:11,039
overall architecture of the system

00:27:09,120 --> 00:27:12,720
they need to define sort of high level

00:27:11,039 --> 00:27:14,320
definitions

00:27:12,720 --> 00:27:16,400
of this test or what we call an

00:27:14,320 --> 00:27:17,360
experiment i need to go ahead and run

00:27:16,400 --> 00:27:18,880
this automatically

00:27:17,360 --> 00:27:21,520
intelligently automatically completely

00:27:18,880 --> 00:27:24,000
hands off all you have to do is say hey

00:27:21,520 --> 00:27:25,600
i want to tune these 10 parameters and i

00:27:24,000 --> 00:27:28,080
think they should vary between

00:27:25,600 --> 00:27:30,640
x and y whatever it is tell me what the

00:27:28,080 --> 00:27:32,399
best configuration is

00:27:30,640 --> 00:27:34,000
and as i mentioned before this this

00:27:32,399 --> 00:27:37,039
continuous optimization process should

00:27:34,000 --> 00:27:38,399
be closely tied to your csd pipeline

00:27:37,039 --> 00:27:39,679
so that whenever you have a new release

00:27:38,399 --> 00:27:40,559
or you have a new tweak or whatever it

00:27:39,679 --> 00:27:42,159
is that you do

00:27:40,559 --> 00:27:44,000
you run through this if you think that

00:27:42,159 --> 00:27:45,440
it's going to have a meaningful impact

00:27:44,000 --> 00:27:46,399
on your cluster or the other

00:27:45,440 --> 00:27:50,480
applications

00:27:46,399 --> 00:27:52,320
applications that are running so

00:27:50,480 --> 00:27:53,840
quickly on on the platform that we have

00:27:52,320 --> 00:27:55,120
redscaps and how do we do it

00:27:53,840 --> 00:27:58,159
i mentioned the words experiments and

00:27:55,120 --> 00:28:01,200
parameters what we provide

00:27:58,159 --> 00:28:02,960
is an ability to do this design of

00:28:01,200 --> 00:28:05,200
experiment but at a much larger scale

00:28:02,960 --> 00:28:07,039
much much faster and and much more

00:28:05,200 --> 00:28:09,200
scientific than what i can do by

00:28:07,039 --> 00:28:11,600
you know guessing on a grid you as a

00:28:09,200 --> 00:28:14,720
developer like i said you define

00:28:11,600 --> 00:28:15,679
a few things parameters those are the

00:28:14,720 --> 00:28:17,919
things you want to tune

00:28:15,679 --> 00:28:19,200
so memory cpu for the following pods

00:28:17,919 --> 00:28:20,240
could be two could be five could be

00:28:19,200 --> 00:28:21,520
whatever you want

00:28:20,240 --> 00:28:23,679
you want to tune the jvm you can tune

00:28:21,520 --> 00:28:25,919
the jv at the same time um

00:28:23,679 --> 00:28:28,240
replicas disk size whatever it is

00:28:25,919 --> 00:28:30,080
anything that you can expose

00:28:28,240 --> 00:28:31,760
the platform can tune and the nice thing

00:28:30,080 --> 00:28:33,440
about kubernetes is the whole point is

00:28:31,760 --> 00:28:34,240
to be as flexible as possible we can

00:28:33,440 --> 00:28:36,880
expose almost

00:28:34,240 --> 00:28:37,919
anything we want in there once we have

00:28:36,880 --> 00:28:40,720
that definition

00:28:37,919 --> 00:28:41,279
um the experiment itself is completely

00:28:40,720 --> 00:28:42,960
automated

00:28:41,279 --> 00:28:44,960
so the machine learning model explores

00:28:42,960 --> 00:28:47,600
the parameter space

00:28:44,960 --> 00:28:48,640
and it comes back with uh configurations

00:28:47,600 --> 00:28:50,240
to try

00:28:48,640 --> 00:28:51,279
and we have our own kubernetes

00:28:50,240 --> 00:28:52,799
controller which i'll show you in a

00:28:51,279 --> 00:28:53,679
second which actually goes ahead and

00:28:52,799 --> 00:28:55,039
tries

00:28:53,679 --> 00:28:56,880
the different configuration each one of

00:28:55,039 --> 00:28:58,640
those we call a trial

00:28:56,880 --> 00:29:00,159
as it tries those the machine learning

00:28:58,640 --> 00:29:01,919
model learns from the performance of the

00:29:00,159 --> 00:29:05,039
application and it learns

00:29:01,919 --> 00:29:07,039
what the parameter space uh looks like

00:29:05,039 --> 00:29:08,240
now one thing that that's really really

00:29:07,039 --> 00:29:09,760
important to mention here like we're

00:29:08,240 --> 00:29:11,120
optimizing towards something right i

00:29:09,760 --> 00:29:12,640
keep saying well we want an optimized

00:29:11,120 --> 00:29:13,919
application but what does it mean to be

00:29:12,640 --> 00:29:17,440
optimized

00:29:13,919 --> 00:29:20,159
and the reality of situation is optimal

00:29:17,440 --> 00:29:21,520
application is completely up to you

00:29:20,159 --> 00:29:23,840
right and this is why

00:29:21,520 --> 00:29:25,200
before i said defaults if you take

00:29:23,840 --> 00:29:26,559
someone else's defaults you're also

00:29:25,200 --> 00:29:27,840
taking their assumption of what does it

00:29:26,559 --> 00:29:28,640
mean for their application to be

00:29:27,840 --> 00:29:30,159
performed

00:29:28,640 --> 00:29:31,840
it could be low latency it could be high

00:29:30,159 --> 00:29:33,600
throughput maybe you just want to

00:29:31,840 --> 00:29:34,720
minimize resource utilization overall

00:29:33,600 --> 00:29:37,120
while the application

00:29:34,720 --> 00:29:37,840
sort of doesn't crash all those use

00:29:37,120 --> 00:29:40,720
cases

00:29:37,840 --> 00:29:42,080
are valid and they're driven by either

00:29:40,720 --> 00:29:42,799
the developer needs or the business

00:29:42,080 --> 00:29:44,960
needs

00:29:42,799 --> 00:29:46,240
and so we allow you to define the

00:29:44,960 --> 00:29:47,360
different metrics that you want to tune

00:29:46,240 --> 00:29:48,880
for so you can say

00:29:47,360 --> 00:29:50,559
again hey i have these 10 parameters

00:29:48,880 --> 00:29:53,440
that i want to tune

00:29:50,559 --> 00:29:54,080
i want to focus on two metrics one is

00:29:53,440 --> 00:29:56,640
throughput

00:29:54,080 --> 00:29:57,360
i want to get as many users uh in as

00:29:56,640 --> 00:29:59,279
possible

00:29:57,360 --> 00:30:01,120
and i want to minimize my overall

00:29:59,279 --> 00:30:01,760
resource utilization so maximize

00:30:01,120 --> 00:30:03,919
throughput

00:30:01,760 --> 00:30:05,440
and minimize utilization and the machine

00:30:03,919 --> 00:30:07,360
learning model will learn

00:30:05,440 --> 00:30:08,640
the optimal result or in this case

00:30:07,360 --> 00:30:10,080
results if you have

00:30:08,640 --> 00:30:13,039
competing metrics for that and give you

00:30:10,080 --> 00:30:13,039
the optimal configuration

00:30:13,120 --> 00:30:16,720
so let me show you a quick example i'll

00:30:14,720 --> 00:30:19,360
show you an example of the pitfalls and

00:30:16,720 --> 00:30:20,480
and how we actually end up solving it

00:30:19,360 --> 00:30:23,760
some of you may know

00:30:20,480 --> 00:30:26,080
the docker dogs versus cats

00:30:23,760 --> 00:30:27,039
voting app somewhat simple app like i

00:30:26,080 --> 00:30:29,120
said simple

00:30:27,039 --> 00:30:30,559
five services right i have my back end

00:30:29,120 --> 00:30:31,440
the calculates the results i have a

00:30:30,559 --> 00:30:33,120
redis key

00:30:31,440 --> 00:30:37,120
i have a worker a db and the actual

00:30:33,120 --> 00:30:39,360
front end that i serve to the user and

00:30:37,120 --> 00:30:41,440
it's it's sort of up there it's

00:30:39,360 --> 00:30:44,000
available i can always go

00:30:41,440 --> 00:30:44,720
and and pull this example online right

00:30:44,000 --> 00:30:46,480
so this is where

00:30:44,720 --> 00:30:47,760
sort of these blog posts come in hey i

00:30:46,480 --> 00:30:48,720
saw this cool thing i'm starting with

00:30:47,760 --> 00:30:50,640
kubernetes

00:30:48,720 --> 00:30:51,919
i would love to get this thing deployed

00:30:50,640 --> 00:30:53,760
i go to github

00:30:51,919 --> 00:30:55,520
i close the repo you know what even

00:30:53,760 --> 00:30:56,880
better they have a whole folder on

00:30:55,520 --> 00:30:58,799
kubernetes specifications

00:30:56,880 --> 00:30:59,919
fantastic here are the five services db

00:30:58,799 --> 00:31:02,559
redis result

00:30:59,919 --> 00:31:03,440
voting worker i go into the result

00:31:02,559 --> 00:31:06,960
service

00:31:03,440 --> 00:31:08,480
okay looks good hopefully you'll

00:31:06,960 --> 00:31:10,640
immediately notice that

00:31:08,480 --> 00:31:11,840
the word requests limits memory cpu

00:31:10,640 --> 00:31:15,039
replicas

00:31:11,840 --> 00:31:16,240
nothing's there all these manifests in

00:31:15,039 --> 00:31:18,320
in docker's

00:31:16,240 --> 00:31:20,320
repo they're all going to be defined as

00:31:18,320 --> 00:31:22,080
best effort because

00:31:20,320 --> 00:31:24,159
none of the services have any kind of

00:31:22,080 --> 00:31:26,240
requests or limits in them

00:31:24,159 --> 00:31:27,360
so what's going to happen if i actually

00:31:26,240 --> 00:31:30,640
try to deploy it

00:31:27,360 --> 00:31:31,039
so i have a little cluster here i have

00:31:30,640 --> 00:31:33,279
the

00:31:31,039 --> 00:31:34,640
uh the application itself i will admit i

00:31:33,279 --> 00:31:36,320
made tiny tweaks

00:31:34,640 --> 00:31:38,080
so that i can uh expose the actual

00:31:36,320 --> 00:31:40,080
voting service and show it to you but

00:31:38,080 --> 00:31:41,360
really all i'm doing is i just go ahead

00:31:40,080 --> 00:31:43,919
and group ctl apply

00:31:41,360 --> 00:31:45,600
everything that's in there um and and

00:31:43,919 --> 00:31:46,320
pray a little bit that it's gonna deploy

00:31:45,600 --> 00:31:48,000
right so

00:31:46,320 --> 00:31:50,000
okay everything's created i'm gonna go

00:31:48,000 --> 00:31:52,720
to the cluster and see

00:31:50,000 --> 00:31:53,519
everything looks good so far right see

00:31:52,720 --> 00:31:56,559
the last few

00:31:53,519 --> 00:31:57,760
uh stragglers you know what i'm done i

00:31:56,559 --> 00:31:59,440
deployed

00:31:57,760 --> 00:32:00,799
i'm actually wrong there's nothing wrong

00:31:59,440 --> 00:32:03,440
here everything works

00:32:00,799 --> 00:32:04,240
fantastically well um well like i said

00:32:03,440 --> 00:32:06,480
this this is

00:32:04,240 --> 00:32:07,840
part of the old way of doing things is

00:32:06,480 --> 00:32:09,600
assuming that if the application

00:32:07,840 --> 00:32:11,919
deployed you're actually running well

00:32:09,600 --> 00:32:14,320
so i'm actually going to go ahead and do

00:32:11,919 --> 00:32:17,360
a little port forwarding so you guys can

00:32:14,320 --> 00:32:17,360
see the application

00:32:17,760 --> 00:32:21,519
so all i'm doing now is just uh

00:32:19,760 --> 00:32:24,960
forwarding both the result and

00:32:21,519 --> 00:32:25,840
the voting service so i have these two

00:32:24,960 --> 00:32:27,200
guys up here

00:32:25,840 --> 00:32:29,279
so really what the application looks

00:32:27,200 --> 00:32:30,799
like is this

00:32:29,279 --> 00:32:32,720
uh it's waiting for incoming votes

00:32:30,799 --> 00:32:33,600
there's no votes yet and all it's going

00:32:32,720 --> 00:32:34,880
to tell me is

00:32:33,600 --> 00:32:37,200
what is the percentage of people who

00:32:34,880 --> 00:32:39,360
voted for dogs versus cats

00:32:37,200 --> 00:32:41,120
like i said application's running i can

00:32:39,360 --> 00:32:42,559
see it i mean more than that i can just

00:32:41,120 --> 00:32:45,519
go ahead and vote and

00:32:42,559 --> 00:32:46,880
obviously i'm going to vote for dogs um

00:32:45,519 --> 00:32:49,840
boom 100

00:32:46,880 --> 00:32:51,360
applications working fantastically well

00:32:49,840 --> 00:32:53,840
obviously in this day and age you

00:32:51,360 --> 00:32:55,200
have to to do some sort of performance

00:32:53,840 --> 00:32:57,679
test on your application

00:32:55,200 --> 00:32:59,200
so for that i have um locust up and

00:32:57,679 --> 00:33:00,320
running let's imagine i wanted to expose

00:32:59,200 --> 00:33:01,600
this you know i want to

00:33:00,320 --> 00:33:03,039
put those up on twitter and obviously

00:33:01,600 --> 00:33:04,720
have a huge following and have everyone

00:33:03,039 --> 00:33:08,000
vote cats versus dogs

00:33:04,720 --> 00:33:09,360
um i'll do a thousand users not even

00:33:08,000 --> 00:33:12,640
that much

00:33:09,360 --> 00:33:15,760
spawn a couple of users a second um

00:33:12,640 --> 00:33:17,039
and start swarming okay so far so good

00:33:15,760 --> 00:33:19,120
this thing is up and running you'll see

00:33:17,039 --> 00:33:22,799
the votes come in at the bottom

00:33:19,120 --> 00:33:25,840
oop 433 votes and looks like i'm choking

00:33:22,799 --> 00:33:26,799
um yeah too many files open my incoming

00:33:25,840 --> 00:33:28,399
connections are already

00:33:26,799 --> 00:33:30,000
starting to drop my performance is

00:33:28,399 --> 00:33:31,519
already degrading um

00:33:30,000 --> 00:33:33,600
by doing this out of the box i've

00:33:31,519 --> 00:33:35,760
managed to get 433 votes in

00:33:33,600 --> 00:33:38,640
before my application crashed definitely

00:33:35,760 --> 00:33:40,640
not not something i i want to do so

00:33:38,640 --> 00:33:42,640
what can i do now well i can go into the

00:33:40,640 --> 00:33:44,720
logs i can start looking at

00:33:42,640 --> 00:33:45,840
certain things see if i can maybe get

00:33:44,720 --> 00:33:48,159
some more cpu is it

00:33:45,840 --> 00:33:49,840
is the workers is it the cue things are

00:33:48,159 --> 00:33:52,080
coming back up again okay great

00:33:49,840 --> 00:33:53,919
some some you know something clear

00:33:52,080 --> 00:33:55,279
though should i go maybe to the database

00:33:53,919 --> 00:33:57,760
maybe it's the python back end

00:33:55,279 --> 00:33:58,799
i have i have no idea i have no idea

00:33:57,760 --> 00:34:00,399
what um

00:33:58,799 --> 00:34:02,720
what's actually causing the issues and

00:34:00,399 --> 00:34:05,519
there are plenty of ways to debugging it

00:34:02,720 --> 00:34:06,000
my point is i don't want to this this

00:34:05,519 --> 00:34:07,760
part

00:34:06,000 --> 00:34:09,200
the part of trying to figure out where

00:34:07,760 --> 00:34:10,960
should i put in requests and limits for

00:34:09,200 --> 00:34:14,480
my application is not something that i

00:34:10,960 --> 00:34:17,520
i find particularly um

00:34:14,480 --> 00:34:18,399
you know intriguing so i'll show you

00:34:17,520 --> 00:34:22,079
very quickly

00:34:18,399 --> 00:34:24,800
um how to do this in a sort of

00:34:22,079 --> 00:34:25,679
scientific and automatic fashion so with

00:34:24,800 --> 00:34:26,879
redsky apps

00:34:25,679 --> 00:34:28,000
like i mentioned we have our own

00:34:26,879 --> 00:34:29,359
controller which i'll show you you can

00:34:28,000 --> 00:34:30,639
grab from from github

00:34:29,359 --> 00:34:33,280
you install the controller in the

00:34:30,639 --> 00:34:35,200
cluster um i think i have mine installed

00:34:33,280 --> 00:34:36,960
as well but you can just see

00:34:35,200 --> 00:34:39,679
we have our own tool called the red sky

00:34:36,960 --> 00:34:41,119
ctl you run red sky ct line at

00:34:39,679 --> 00:34:42,399
10 20 seconds and and you have a

00:34:41,119 --> 00:34:42,879
controller you'll see it unchanged

00:34:42,399 --> 00:34:44,639
because

00:34:42,879 --> 00:34:47,760
um i think i already had it installed

00:34:44,639 --> 00:34:49,760
but it takes the same amount of time

00:34:47,760 --> 00:34:51,679
once i have red sky ctl and i've logged

00:34:49,760 --> 00:34:52,639
in we have a we have a completely free

00:34:51,679 --> 00:34:54,720
tier you can use

00:34:52,639 --> 00:34:56,639
to connect to a machine learning model i

00:34:54,720 --> 00:34:59,680
can go ahead and define an experiment

00:34:56,639 --> 00:35:01,359
so i have my voting app and

00:34:59,680 --> 00:35:03,359
like i said i haven't defined requests

00:35:01,359 --> 00:35:05,839
on limits for any of my services

00:35:03,359 --> 00:35:07,440
so at a bare minimum i want to start

00:35:05,839 --> 00:35:08,800
defining my parameters and tuning my

00:35:07,440 --> 00:35:10,640
parameters

00:35:08,800 --> 00:35:12,640
um for for the different services and

00:35:10,640 --> 00:35:14,640
i'll show you here we have 10.

00:35:12,640 --> 00:35:16,720
so we're tuning replicas cpu and memory

00:35:14,640 --> 00:35:18,480
for for most of the most of the services

00:35:16,720 --> 00:35:20,079
you'll see redis on the database and

00:35:18,480 --> 00:35:21,520
um the backend and the worker the only

00:35:20,079 --> 00:35:22,960
thing i don't actually tune is the front

00:35:21,520 --> 00:35:25,200
end in this case

00:35:22,960 --> 00:35:26,560
and as i mentioned you know the question

00:35:25,200 --> 00:35:30,000
is what are you tuning for

00:35:26,560 --> 00:35:32,079
so in this experiment file

00:35:30,000 --> 00:35:33,760
which is sort of our preparatory yaml to

00:35:32,079 --> 00:35:36,079
define these experiments

00:35:33,760 --> 00:35:37,520
i want to i'm going to do two things i

00:35:36,079 --> 00:35:40,320
want to maximize my throughput meaning i

00:35:37,520 --> 00:35:42,800
want to get as many votes in as possible

00:35:40,320 --> 00:35:44,240
and i want to minimize you can think of

00:35:42,800 --> 00:35:45,680
costs here as basically cluster

00:35:44,240 --> 00:35:47,040
resources i don't want to pay a lot for

00:35:45,680 --> 00:35:49,440
this it's just a hobby app

00:35:47,040 --> 00:35:50,880
right so get me as low a cost as

00:35:49,440 --> 00:35:53,920
possible

00:35:50,880 --> 00:35:54,560
once i've defined this um everything

00:35:53,920 --> 00:35:57,359
that we do is

00:35:54,560 --> 00:35:58,560
sort of um group native so i can just go

00:35:57,359 --> 00:35:59,920
ahead and apply it

00:35:58,560 --> 00:36:01,599
and as soon as these things come up

00:35:59,920 --> 00:36:05,119
you'll see pods

00:36:01,599 --> 00:36:07,200
uh come live and

00:36:05,119 --> 00:36:09,040
what i'm gonna do now is basically wait

00:36:07,200 --> 00:36:11,119
but i'm gonna wait

00:36:09,040 --> 00:36:12,720
for the machine learning model to come

00:36:11,119 --> 00:36:14,960
back with a few suggestions

00:36:12,720 --> 00:36:14,960
so

00:36:22,880 --> 00:36:26,960
so this is the redscan apps ui you can

00:36:25,359 --> 00:36:28,560
see i already have an example

00:36:26,960 --> 00:36:30,240
that i started this is the example that

00:36:28,560 --> 00:36:31,119
just started running it's going to take

00:36:30,240 --> 00:36:32,320
it a while to run

00:36:31,119 --> 00:36:34,720
basically what's happening now in the

00:36:32,320 --> 00:36:36,160
background is

00:36:34,720 --> 00:36:37,760
the controller set the machine learning

00:36:36,160 --> 00:36:39,040
model of the experiment said hey here

00:36:37,760 --> 00:36:40,000
are my definitions go run this

00:36:39,040 --> 00:36:41,599
experiment tell me what's going

00:36:40,000 --> 00:36:43,359
on the machinery model started

00:36:41,599 --> 00:36:46,640
suggesting what we call trials

00:36:43,359 --> 00:36:48,480
which is uh to say configurations and

00:36:46,640 --> 00:36:50,160
the the controller tries them so as you

00:36:48,480 --> 00:36:51,200
saw those pods come up basically the

00:36:50,160 --> 00:36:53,200
controller is spinning up

00:36:51,200 --> 00:36:54,640
the application and loading it so we

00:36:53,200 --> 00:36:57,280
actually have locust

00:36:54,640 --> 00:36:59,520
served inside the cluster to to perform

00:36:57,280 --> 00:37:01,119
a performance test on the application

00:36:59,520 --> 00:37:03,200
so what you see here is a completed

00:37:01,119 --> 00:37:03,760
experiment it can take anything from

00:37:03,200 --> 00:37:05,760
like

00:37:03,760 --> 00:37:07,200
30 minutes to maybe an hour or two

00:37:05,760 --> 00:37:09,440
depending on the complexity of

00:37:07,200 --> 00:37:12,240
of your application and each one of

00:37:09,440 --> 00:37:13,920
these guys is what we call a trial

00:37:12,240 --> 00:37:15,440
and you can see here at the bottom the

00:37:13,920 --> 00:37:16,720
different configurations

00:37:15,440 --> 00:37:18,000
of each one of the trials like i said

00:37:16,720 --> 00:37:19,599
we're turning 10 parameters

00:37:18,000 --> 00:37:20,720
automatically

00:37:19,599 --> 00:37:22,960
and at the end of the day what you're

00:37:20,720 --> 00:37:26,880
left with is

00:37:22,960 --> 00:37:29,680
these what we call best so these points

00:37:26,880 --> 00:37:31,119
um otherwise known as the burrito front

00:37:29,680 --> 00:37:32,240
are the ones that can't be beat on both

00:37:31,119 --> 00:37:33,520
fruit but in cost

00:37:32,240 --> 00:37:34,800
you can't get something which is super

00:37:33,520 --> 00:37:36,880
cheap and super performant at the same

00:37:34,800 --> 00:37:38,960
time but you do get the trade-offs

00:37:36,880 --> 00:37:40,400
so instead of me going in and tinkering

00:37:38,960 --> 00:37:41,280
with 10 parameters and trying to figure

00:37:40,400 --> 00:37:43,040
out you know

00:37:41,280 --> 00:37:44,480
am i here am i here am i somewhere in

00:37:43,040 --> 00:37:45,200
the middle i have no visibility in any

00:37:44,480 --> 00:37:47,680
of this

00:37:45,200 --> 00:37:48,240
i can just get all this automated and

00:37:47,680 --> 00:37:50,560
then

00:37:48,240 --> 00:37:51,839
find the configuration that i want go

00:37:50,560 --> 00:37:54,800
ahead and export the manifest and the

00:37:51,839 --> 00:37:54,800
platform and be downloaded

00:37:54,880 --> 00:37:58,640
so a couple last notes before i open it

00:37:57,760 --> 00:38:00,400
up for questions

00:37:58,640 --> 00:38:02,240
um i just want to talk about the machine

00:38:00,400 --> 00:38:04,560
learning model really briefly

00:38:02,240 --> 00:38:05,440
why why machine learning so obviously

00:38:04,560 --> 00:38:08,480
i'm heavily biased

00:38:05,440 --> 00:38:09,119
uh towards machine learning being uh you

00:38:08,480 --> 00:38:11,920
know

00:38:09,119 --> 00:38:13,760
having a background machining myself but

00:38:11,920 --> 00:38:14,720
really this is not just to say we're

00:38:13,760 --> 00:38:17,280
doing machine learning

00:38:14,720 --> 00:38:19,520
uh for the sake of machine learning the

00:38:17,280 --> 00:38:20,960
core of the issue here is the complexity

00:38:19,520 --> 00:38:22,160
and and the number of parameters and

00:38:20,960 --> 00:38:24,160
really what we would call the

00:38:22,160 --> 00:38:25,680
dimensionality of the parameter space

00:38:24,160 --> 00:38:27,359
i don't know a human that can fit in

00:38:25,680 --> 00:38:31,200
their brain uh

00:38:27,359 --> 00:38:32,720
10 parameters let alone 20. and so

00:38:31,200 --> 00:38:34,480
we have to use a machine in the mall

00:38:32,720 --> 00:38:36,000
that can handle that kind of complexity

00:38:34,480 --> 00:38:38,000
that level of complexity

00:38:36,000 --> 00:38:39,760
it can explore the parameter space way

00:38:38,000 --> 00:38:41,119
more efficiently than we can

00:38:39,760 --> 00:38:44,079
if you look at the number of trials i

00:38:41,119 --> 00:38:46,480
had up there i had about 200 trials

00:38:44,079 --> 00:38:48,400
and again i challenge every one of you

00:38:46,480 --> 00:38:49,920
to to go through 200 trials and give me

00:38:48,400 --> 00:38:51,359
the same kind of results for for

00:38:49,920 --> 00:38:52,320
throughput cost

00:38:51,359 --> 00:38:53,920
and the nice thing about our

00:38:52,320 --> 00:38:55,040
optimization process it's completely

00:38:53,920 --> 00:38:57,599
like problem agnostic

00:38:55,040 --> 00:38:59,440
i don't need like a whiz data scientist

00:38:57,599 --> 00:39:01,119
i don't need a lot of data up front

00:38:59,440 --> 00:39:02,960
all i need to know is what's your

00:39:01,119 --> 00:39:05,359
application your actual

00:39:02,960 --> 00:39:06,560
uh manifest and the parameters that you

00:39:05,359 --> 00:39:08,160
want to tune right you define the

00:39:06,560 --> 00:39:08,960
parameters you define the metrics and

00:39:08,160 --> 00:39:12,320
you define the

00:39:08,960 --> 00:39:16,000
um the load test and and you just

00:39:12,320 --> 00:39:17,520
run it so with that uh i'll open up the

00:39:16,000 --> 00:39:19,920
questions thank you all for coming

00:39:17,520 --> 00:39:20,800
um again i i highly encourage you if

00:39:19,920 --> 00:39:23,520
you're only taking

00:39:20,800 --> 00:39:25,200
uh a few things out of this is think

00:39:23,520 --> 00:39:28,480
about your resource allocation

00:39:25,200 --> 00:39:29,280
really think about how you tune it ahead

00:39:28,480 --> 00:39:30,720
of time

00:39:29,280 --> 00:39:32,000
not as an afterthought but something you

00:39:30,720 --> 00:39:33,520
do pre-deployment make sure you

00:39:32,000 --> 00:39:34,160
understand what your quality of service

00:39:33,520 --> 00:39:36,960
is

00:39:34,160 --> 00:39:38,240
um and and really think about having

00:39:36,960 --> 00:39:40,160
this as part of your process

00:39:38,240 --> 00:39:41,520
this tuning and optimization process is

00:39:40,160 --> 00:39:45,920
a critical

00:39:41,520 --> 00:39:45,920
piece of your cicd pipeline in general

00:39:46,720 --> 00:39:53,599
cool and uh with that i'm going to go to

00:39:50,880 --> 00:39:53,599
q a panel

00:39:54,079 --> 00:39:58,160
so first of all what is the role of of

00:39:56,000 --> 00:40:00,480
limits on the container level

00:39:58,160 --> 00:40:01,520
um i'm not sure what you mean by by

00:40:00,480 --> 00:40:05,119
container level

00:40:01,520 --> 00:40:08,400
um but i would say like limits as

00:40:05,119 --> 00:40:11,280
i'll i'll go back to um

00:40:08,400 --> 00:40:12,160
well i'll go back to the original slide

00:40:11,280 --> 00:40:14,400
limits is

00:40:12,160 --> 00:40:15,440
as it sounds the container if you set

00:40:14,400 --> 00:40:18,319
the limits to be

00:40:15,440 --> 00:40:19,920
one cpu all this container will ever be

00:40:18,319 --> 00:40:20,640
able to get us one cpu we'll never be

00:40:19,920 --> 00:40:22,640
able to get

00:40:20,640 --> 00:40:24,480
more than one core essentially same goes

00:40:22,640 --> 00:40:25,920
for memory

00:40:24,480 --> 00:40:28,240
um someone asked if we have two

00:40:25,920 --> 00:40:30,880
containers in one pod each has a limit

00:40:28,240 --> 00:40:32,800
of one cpu request of 0.5 cpu

00:40:30,880 --> 00:40:33,920
it's great i feel like i'm in math class

00:40:32,800 --> 00:40:35,119
uh request

00:40:33,920 --> 00:40:37,200
point five c how much more will be

00:40:35,119 --> 00:40:40,400
reserved so the

00:40:37,200 --> 00:40:42,079
the word reserved is interesting so

00:40:40,400 --> 00:40:44,079
really what i have is i have a total

00:40:42,079 --> 00:40:46,960
request of 0.5 cp

00:40:44,079 --> 00:40:47,599
sorry i have a total request of one cpu

00:40:46,960 --> 00:40:48,880
right five

00:40:47,599 --> 00:40:50,800
two containers each one of them have

00:40:48,880 --> 00:40:54,319
half if i can still do arithmetic then

00:40:50,800 --> 00:40:57,359
i'm going to get to one core uh

00:40:54,319 --> 00:40:58,880
the word reserved is uh you know i

00:40:57,359 --> 00:41:00,400
i'm going to say the other one so that

00:40:58,880 --> 00:41:03,440
it's reserved

00:41:00,400 --> 00:41:04,240
who will look for a node that has one

00:41:03,440 --> 00:41:07,040
free core

00:41:04,240 --> 00:41:08,800
of cpu in order to schedule this pod

00:41:07,040 --> 00:41:11,920
once it's been scheduled it can

00:41:08,800 --> 00:41:13,920
burst up to what two cores right

00:41:11,920 --> 00:41:16,160
um but at a bare minimum it has to be

00:41:13,920 --> 00:41:19,520
scheduled on a node that has one

00:41:16,160 --> 00:41:21,119
free course of view

00:41:19,520 --> 00:41:23,680
someone else is redsky apps available

00:41:21,119 --> 00:41:25,040
for any kubernetes distribution be on

00:41:23,680 --> 00:41:28,480
cloud on premises

00:41:25,040 --> 00:41:30,160
absolutely um you know we support

00:41:28,480 --> 00:41:31,760
we have back support i forget which

00:41:30,160 --> 00:41:35,200
version but maybe we go

00:41:31,760 --> 00:41:36,720
back uh quite a lot when it comes to

00:41:35,200 --> 00:41:38,800
on-premises if you have

00:41:36,720 --> 00:41:40,640
if you're air-gapped uh then then come

00:41:38,800 --> 00:41:42,400
talk to us right now the the free tier

00:41:40,640 --> 00:41:43,440
offering is obviously hosted by us

00:41:42,400 --> 00:41:45,920
so you need to have some sort of

00:41:43,440 --> 00:41:46,880
internet connectivity um so i suggest

00:41:45,920 --> 00:41:48,240
trying it on

00:41:46,880 --> 00:41:50,480
one of the cloud providers but if you

00:41:48,240 --> 00:41:52,240
have a need for something more than that

00:41:50,480 --> 00:41:54,160
we have complete capability to deploy

00:41:52,240 --> 00:41:56,079
everything on chrome

00:41:54,160 --> 00:41:57,760
uh do you do auto scaling real time or

00:41:56,079 --> 00:42:00,839
only using experiments

00:41:57,760 --> 00:42:04,240
uh oh this

00:42:00,839 --> 00:42:04,880
this i i'm i'm not sure exactly what

00:42:04,240 --> 00:42:06,240
you're asking so

00:42:04,880 --> 00:42:08,560
i'll talk about a few things at the same

00:42:06,240 --> 00:42:12,560
time so first of all auto scaling

00:42:08,560 --> 00:42:14,560
um i'm i'm gonna talk about the actual

00:42:12,560 --> 00:42:16,800
cube native auto scalers in a second but

00:42:14,560 --> 00:42:18,319
before that i would say for scaling in

00:42:16,800 --> 00:42:20,079
general if we're thinking about

00:42:18,319 --> 00:42:23,839
what we talked about with with varying

00:42:20,079 --> 00:42:26,160
loads uh varying workloads

00:42:23,839 --> 00:42:27,599
there are ways for you to run multiple

00:42:26,160 --> 00:42:30,000
multiple experiments

00:42:27,599 --> 00:42:31,440
that tailor multiple loads or run an

00:42:30,000 --> 00:42:32,400
experiment that has a variable load in

00:42:31,440 --> 00:42:33,680
it

00:42:32,400 --> 00:42:35,040
and then what you can do is you either

00:42:33,680 --> 00:42:36,560
run an experiment with a variable load

00:42:35,040 --> 00:42:38,560
and you export the manifest for that

00:42:36,560 --> 00:42:40,240
or something that i particularly like is

00:42:38,560 --> 00:42:42,319
doing what i call scenarios

00:42:40,240 --> 00:42:44,160
i basically i can run a scenario for low

00:42:42,319 --> 00:42:46,800
low low medium high or whatever that is

00:42:44,160 --> 00:42:47,359
export all three and then save them we

00:42:46,800 --> 00:42:50,720
currently

00:42:47,359 --> 00:42:51,760
don't have um a way for you to switch

00:42:50,720 --> 00:42:53,200
between them in production because we

00:42:51,760 --> 00:42:54,480
don't work in production

00:42:53,200 --> 00:42:56,000
but we're working towards sort of

00:42:54,480 --> 00:42:56,720
extending our pipeline completely right

00:42:56,000 --> 00:42:59,119
now you

00:42:56,720 --> 00:43:00,640
basically have to um switch between

00:42:59,119 --> 00:43:02,319
scenarios yourself basically

00:43:00,640 --> 00:43:04,079
uh group ctl apply growth in whatever

00:43:02,319 --> 00:43:06,800
process you go to to apply the different

00:43:04,079 --> 00:43:09,040
scenarios um on auto scanning in general

00:43:06,800 --> 00:43:11,280
i will say we're working towards

00:43:09,040 --> 00:43:12,160
optimizing your auto scalers themselves

00:43:11,280 --> 00:43:14,480
so

00:43:12,160 --> 00:43:16,000
as part of your experiment you can tune

00:43:14,480 --> 00:43:17,599
the hpa for instance like if you know

00:43:16,000 --> 00:43:19,520
you're deploying with the hpa

00:43:17,599 --> 00:43:21,599
you can actually expose the hpa's

00:43:19,520 --> 00:43:23,680
parameters in the experiment

00:43:21,599 --> 00:43:25,440
and then what you get is an intelligent

00:43:23,680 --> 00:43:26,079
auto scaler inside your experience so

00:43:25,440 --> 00:43:27,920
basically

00:43:26,079 --> 00:43:29,200
you're making your whole system the

00:43:27,920 --> 00:43:30,960
entire kubernetes ecosystem more

00:43:29,200 --> 00:43:33,280
intelligent

00:43:30,960 --> 00:43:34,800
um with this work with cloud native k

00:43:33,280 --> 00:43:38,079
it's as a service eks

00:43:34,800 --> 00:43:40,720
ranger yes to all of it so um

00:43:38,079 --> 00:43:42,560
we work ourselves like our stuff is

00:43:40,720 --> 00:43:44,640
deployed on both gk and eks we're

00:43:42,560 --> 00:43:46,560
perfectly fine with that uh ranchers as

00:43:44,640 --> 00:43:50,480
a good partner of ours

00:43:46,560 --> 00:43:51,760
um i say yes across the board um

00:43:50,480 --> 00:43:53,200
in the case of replicas we're kind of

00:43:51,760 --> 00:43:54,079
load balancing you consider how low

00:43:53,200 --> 00:43:57,280
balancing can affect

00:43:54,079 --> 00:43:57,920
uh verticals getting it's a good

00:43:57,280 --> 00:44:00,000
question i mean

00:43:57,920 --> 00:44:00,960
right now if you go to our recipes repo

00:44:00,000 --> 00:44:03,200
so you can see here

00:44:00,960 --> 00:44:05,760
red snaps recipes we have multiple

00:44:03,200 --> 00:44:09,280
recipes including the one i showed you

00:44:05,760 --> 00:44:13,119
that does tune the replicas

00:44:09,280 --> 00:44:15,839
um i think we basically use

00:44:13,119 --> 00:44:18,000
one kind of load balancer i don't know i

00:44:15,839 --> 00:44:19,599
honestly haven't looked too much into

00:44:18,000 --> 00:44:21,839
how that would affect vertical scaling

00:44:19,599 --> 00:44:23,119
but i'm wondering if that's something

00:44:21,839 --> 00:44:24,480
that you can expose

00:44:23,119 --> 00:44:26,560
i can tell you that we're working on

00:44:24,480 --> 00:44:27,920
adding you may have already added

00:44:26,560 --> 00:44:28,960
categorical parameters so you don't have

00:44:27,920 --> 00:44:30,240
to put in numbers

00:44:28,960 --> 00:44:31,760
you know you can switch between

00:44:30,240 --> 00:44:32,880
different types of load balancing if you

00:44:31,760 --> 00:44:34,839
have something

00:44:32,880 --> 00:44:36,560
fancy and even incorporate that in your

00:44:34,839 --> 00:44:39,839
experiment

00:44:36,560 --> 00:44:42,720
how much does it cost free to just go up

00:44:39,839 --> 00:44:43,920
sign up and you can start today you

00:44:42,720 --> 00:44:46,960
don't have to

00:44:43,920 --> 00:44:48,319
pay us anything um show me the ammo for

00:44:46,960 --> 00:44:50,400
setting parameters how you can set the

00:44:48,319 --> 00:44:52,720
limit request values in the container

00:44:50,400 --> 00:44:54,480
um so all this is online i can go back

00:44:52,720 --> 00:44:54,880
to that soon if we still have time i

00:44:54,480 --> 00:44:58,400
think

00:44:54,880 --> 00:45:01,440
we're bumping up on the r soon um

00:44:58,400 --> 00:45:02,160
the way you set the limits actually i'll

00:45:01,440 --> 00:45:03,520
just do it now

00:45:02,160 --> 00:45:05,839
so the way you set the limits in the

00:45:03,520 --> 00:45:06,960
container so here's the experiment file

00:45:05,839 --> 00:45:08,160
right this is the experiment that i

00:45:06,960 --> 00:45:09,280
started running and i said here are the

00:45:08,160 --> 00:45:11,680
parameters

00:45:09,280 --> 00:45:13,040
what really happens is as part of the

00:45:11,680 --> 00:45:15,520
experiment we patch

00:45:13,040 --> 00:45:16,800
your manifests so in the back here in

00:45:15,520 --> 00:45:18,480
the application folder

00:45:16,800 --> 00:45:20,640
i have a bunch of manifest and we just

00:45:18,480 --> 00:45:22,480
go ahead and patch that

00:45:20,640 --> 00:45:23,680
based on the values that we get from the

00:45:22,480 --> 00:45:28,720
machine learning model

00:45:23,680 --> 00:45:28,720
i hope that answers the question um

00:45:30,240 --> 00:45:33,359
so how do you keep the system pods

00:45:31,839 --> 00:45:34,800
stable like the autoscaler power which

00:45:33,359 --> 00:45:35,839
is in the coop system if the autoscaler

00:45:34,800 --> 00:45:37,119
pod goes down

00:45:35,839 --> 00:45:39,680
uh and then the scaling advantage is

00:45:37,119 --> 00:45:41,599
dropping so this is this is

00:45:39,680 --> 00:45:43,119
just a general question that we get

00:45:41,599 --> 00:45:45,200
along i really like

00:45:43,119 --> 00:45:46,480
um we find more and more people are

00:45:45,200 --> 00:45:48,000
interested in tuning

00:45:46,480 --> 00:45:49,920
more than just their application by

00:45:48,000 --> 00:45:52,319
actually tuning kube resources

00:45:49,920 --> 00:45:53,760
you totally can um i don't have a great

00:45:52,319 --> 00:45:55,680
example like like i said we

00:45:53,760 --> 00:45:57,040
we tune we have great examples of tuning

00:45:55,680 --> 00:45:59,359
the hp itself

00:45:57,040 --> 00:46:00,640
so if that's part of your experiment if

00:45:59,359 --> 00:46:04,400
if the hp

00:46:00,640 --> 00:46:06,000
pod sort of goes down or doesn't deploy

00:46:04,400 --> 00:46:07,200
our experiment will show that as a

00:46:06,000 --> 00:46:08,240
failed trial and the machine learning

00:46:07,200 --> 00:46:08,560
model will learn from that but if you

00:46:08,240 --> 00:46:10,160
know

00:46:08,560 --> 00:46:11,839
one thing i haven't mentioned is it

00:46:10,160 --> 00:46:13,359
actually learns from failures so let's

00:46:11,839 --> 00:46:15,760
just keep trying things if it

00:46:13,359 --> 00:46:17,200
notices space or areas in the parameter

00:46:15,760 --> 00:46:18,000
space where the application fails to

00:46:17,200 --> 00:46:20,240
deploy

00:46:18,000 --> 00:46:21,760
it doesn't touch it anymore but yeah you

00:46:20,240 --> 00:46:23,200
can add more and more google resources

00:46:21,760 --> 00:46:26,319
to your experiment and

00:46:23,200 --> 00:46:26,319
just keep going with that

00:46:27,520 --> 00:46:30,319
doesn't work with openshift or just

00:46:28,640 --> 00:46:32,000
regular case i think we actually just

00:46:30,319 --> 00:46:33,280
had the first experiment on openshift

00:46:32,000 --> 00:46:35,520
a week or two so it does work on

00:46:33,280 --> 00:46:37,280
openshift um and you know if you have

00:46:35,520 --> 00:46:38,079
specific questions on versions and stuff

00:46:37,280 --> 00:46:41,599
like that you can come

00:46:38,079 --> 00:46:45,920
talk to us um

00:46:41,599 --> 00:46:45,920
previous question plus at the cicd uh

00:46:46,319 --> 00:46:49,440
not not sure what you mean so right now

00:46:48,560 --> 00:46:51,440
we

00:46:49,440 --> 00:46:53,440
what i showed you sort of a standalone

00:46:51,440 --> 00:46:54,640
piece of what i consider to be a cfcd

00:46:53,440 --> 00:46:56,560
pipeline

00:46:54,640 --> 00:46:58,000
we're working on building integrations

00:46:56,560 --> 00:47:00,319
into common um

00:46:58,000 --> 00:47:01,359
ci cd tools like jenkins or circle or

00:47:00,319 --> 00:47:03,440
whatever that is but

00:47:01,359 --> 00:47:04,960
i i don't have that quite yet so we're

00:47:03,440 --> 00:47:08,000
defined throttling through

00:47:04,960 --> 00:47:08,560
graph using cabana so i'm not sure if

00:47:08,000 --> 00:47:10,160
you mean

00:47:08,560 --> 00:47:11,680
in the experiment or just in general i

00:47:10,160 --> 00:47:14,400
mean obviously if you have

00:47:11,680 --> 00:47:14,400
prometheus

00:47:16,880 --> 00:47:20,079
honestly like that that totally depends

00:47:19,040 --> 00:47:23,040
on your deployment

00:47:20,079 --> 00:47:24,400
um it's just yes you can fight

00:47:23,040 --> 00:47:26,079
broadlying i mean if

00:47:24,400 --> 00:47:28,079
if you're looking to monitor your system

00:47:26,079 --> 00:47:29,440
and look into specific things like

00:47:28,079 --> 00:47:31,040
funneling i i would recommend

00:47:29,440 --> 00:47:32,559
uh prometheus and grafana and sort of

00:47:31,040 --> 00:47:35,010
online monitoring but that's a that's a

00:47:32,559 --> 00:47:36,960
separate thing altogether

00:47:35,010 --> 00:47:38,319
[Music]

00:47:36,960 --> 00:47:40,000
how to manage two parts of the burst at

00:47:38,319 --> 00:47:41,440
the same time should we use namespace to

00:47:40,000 --> 00:47:42,079
ensure these pods don't run on the scene

00:47:41,440 --> 00:47:44,640
whatever

00:47:42,079 --> 00:47:46,000
so um that's a good question uh you

00:47:44,640 --> 00:47:47,520
don't have to use namespaces you can use

00:47:46,000 --> 00:47:49,440
taints and tolerations to to make sure

00:47:47,520 --> 00:47:53,680
pods don't land at the same place

00:47:49,440 --> 00:47:56,880
again complete uh other uh other topic

00:47:53,680 --> 00:47:59,040
um you know my my

00:47:56,880 --> 00:48:00,319
my first step would actually to be to

00:47:59,040 --> 00:48:01,680
run an experiment with these two

00:48:00,319 --> 00:48:04,160
burstable pods

00:48:01,680 --> 00:48:05,599
and see how it works out because uh if

00:48:04,160 --> 00:48:07,040
if you do have issues

00:48:05,599 --> 00:48:09,359
those will be surfaced throughout the

00:48:07,040 --> 00:48:09,920
experiment where if we don't control

00:48:09,359 --> 00:48:11,680
scheduling

00:48:09,920 --> 00:48:14,079
if we use the native kubernetes

00:48:11,680 --> 00:48:15,520
scheduler as you go through trials

00:48:14,079 --> 00:48:17,119
uh you know some of them will have more

00:48:15,520 --> 00:48:18,000
scheduling issues than others and you

00:48:17,119 --> 00:48:20,319
sort of call us

00:48:18,000 --> 00:48:22,319
to the right answer but um if you want

00:48:20,319 --> 00:48:23,839
to make if you know you have those you

00:48:22,319 --> 00:48:24,960
should just obtain some of them ahead of

00:48:23,839 --> 00:48:27,520
time

00:48:24,960 --> 00:48:29,040
um how does it affect the hp and vpa i i

00:48:27,520 --> 00:48:30,480
just talked about this like basically

00:48:29,040 --> 00:48:32,400
we believe that you should be tuning

00:48:30,480 --> 00:48:34,400
those um as well as you run through

00:48:32,400 --> 00:48:36,319
these experiments

00:48:34,400 --> 00:48:38,079
how much resources the redskate uh

00:48:36,319 --> 00:48:39,599
controller in in kubernetes needs

00:48:38,079 --> 00:48:41,280
very little um so yeah you saw the

00:48:39,599 --> 00:48:42,559
little red sky um

00:48:41,280 --> 00:48:44,559
system name space in the right sky

00:48:42,559 --> 00:48:45,720
controller pod you can go to our docs

00:48:44,559 --> 00:48:48,640
that's

00:48:45,720 --> 00:48:49,440
redskype.dev um it'll list everything

00:48:48,640 --> 00:48:53,040
it's it's

00:48:49,440 --> 00:48:55,599
incredibly minimal um when the product

00:48:53,040 --> 00:48:59,119
will put to a node

00:48:55,599 --> 00:49:00,480
uh okay when the partners put your node

00:48:59,119 --> 00:49:01,599
where the limit is higher than what is

00:49:00,480 --> 00:49:02,720
available on the node it causes the

00:49:01,599 --> 00:49:03,839
abstract

00:49:02,720 --> 00:49:05,920
i don't know where to evict the thought

00:49:03,839 --> 00:49:06,880
when it exceeds uh it will be how do you

00:49:05,920 --> 00:49:10,160
handle this so

00:49:06,880 --> 00:49:11,760
this is where it's it's i believe what

00:49:10,160 --> 00:49:15,200
you're talking about is actually having

00:49:11,760 --> 00:49:16,880
a um uh well

00:49:15,200 --> 00:49:17,920
it doesn't have to be a burstable well

00:49:16,880 --> 00:49:18,800
it has to be a versatile quality of

00:49:17,920 --> 00:49:20,400
service because if

00:49:18,800 --> 00:49:22,240
if there's no if there's no memory

00:49:20,400 --> 00:49:25,040
available then it won't be scheduled

00:49:22,240 --> 00:49:26,400
this work doing experiments with uh

00:49:25,040 --> 00:49:27,359
burst hole quality of service is

00:49:26,400 --> 00:49:29,440
interesting

00:49:27,359 --> 00:49:30,480
again the experiment you know you can

00:49:29,440 --> 00:49:33,520
just run

00:49:30,480 --> 00:49:35,119
um we recommend running 20x trials to

00:49:33,520 --> 00:49:36,640
the number of parameters it gives you

00:49:35,119 --> 00:49:38,240
really really good coverage and the

00:49:36,640 --> 00:49:40,160
experiment itself will see it

00:49:38,240 --> 00:49:42,000
right so if the pod gets evicted or the

00:49:40,160 --> 00:49:44,319
app crashes like i said before the

00:49:42,000 --> 00:49:47,440
machine in the model will learn from it

00:49:44,319 --> 00:49:49,440
and so it'll keep trying and if

00:49:47,440 --> 00:49:50,720
if there's no other place to schedule it

00:49:49,440 --> 00:49:52,160
then you'll see your experience is just

00:49:50,720 --> 00:49:53,760
not succeeding then you'll have to go

00:49:52,160 --> 00:49:54,960
back and

00:49:53,760 --> 00:49:56,559
make sure maybe you don't have enough

00:49:54,960 --> 00:49:57,119
nodes you're gonna you've got to figure

00:49:56,559 --> 00:50:00,160
out like

00:49:57,119 --> 00:50:02,880
you gotta limit the the um

00:50:00,160 --> 00:50:04,480
the numbers even more how can you tie

00:50:02,880 --> 00:50:06,000
the patches into customize

00:50:04,480 --> 00:50:07,760
we already use customize i mean if you

00:50:06,000 --> 00:50:08,720
saw me i run the experiment using

00:50:07,760 --> 00:50:12,000
customize and

00:50:08,720 --> 00:50:13,920
um basically everything that we do

00:50:12,000 --> 00:50:15,200
you can then go ahead and export using

00:50:13,920 --> 00:50:18,720
customized patch

00:50:15,200 --> 00:50:18,720
i hope that answers the question

00:50:18,960 --> 00:50:21,359
um

00:50:23,599 --> 00:50:26,800
that's all these questions i think are

00:50:25,200 --> 00:50:28,960
coming in different times i think

00:50:26,800 --> 00:50:30,319
a couple of more uh is it required to

00:50:28,960 --> 00:50:31,839
have an account to redsky

00:50:30,319 --> 00:50:33,760
to use the application can we train the

00:50:31,839 --> 00:50:36,000
ml on premise

00:50:33,760 --> 00:50:37,280
so um yes to the second question i

00:50:36,000 --> 00:50:38,640
talked about that briefly we can deploy

00:50:37,280 --> 00:50:41,200
the machine learning

00:50:38,640 --> 00:50:41,839
model on premise um come talk to us for

00:50:41,200 --> 00:50:43,119
that

00:50:41,839 --> 00:50:44,720
uh do you have to have an account to use

00:50:43,119 --> 00:50:46,000
this application so in order to use the

00:50:44,720 --> 00:50:47,040
machine learning model you just sign up

00:50:46,000 --> 00:50:49,599
for an account online

00:50:47,040 --> 00:50:50,400
super easy just email um if you don't

00:50:49,599 --> 00:50:52,319
want to

00:50:50,400 --> 00:50:53,599
give us your email the controller itself

00:50:52,319 --> 00:50:54,960
is completely open source you can go

00:50:53,599 --> 00:50:56,800
ahead and get the controller

00:50:54,960 --> 00:50:58,079
and you can still run experiments so

00:50:56,800 --> 00:50:58,960
you'll see in the docs the suggest

00:50:58,079 --> 00:51:00,720
command

00:50:58,960 --> 00:51:02,800
um what it means is you won't have the

00:51:00,720 --> 00:51:04,240
intelligence behind the experiment

00:51:02,800 --> 00:51:05,760
because you won't be connected to the

00:51:04,240 --> 00:51:07,119
machinery model you can still run

00:51:05,760 --> 00:51:10,000
you know so the second option that i

00:51:07,119 --> 00:51:13,200
mentioned of design of experiment

00:51:10,000 --> 00:51:16,240
uh and i think

00:51:13,200 --> 00:51:18,559
that is it unless someone has any other

00:51:16,240 --> 00:51:18,559
question

00:51:20,720 --> 00:51:24,240
cool well thank you all for coming thank

00:51:23,520 --> 00:51:28,000
you very much

00:51:24,240 --> 00:51:35,599
um and i hope to see you again soon

00:51:28,000 --> 00:51:35,599

YouTube URL: https://www.youtube.com/watch?v=p36zWqcyhvo


