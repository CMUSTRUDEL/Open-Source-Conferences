Title: Webinar: Unlocking the Power of Flash with Open Source Software-Enabled Flash™ Technology
Publication date: 2020-08-27
Playlist: LF Live Webinars
Description: 
	Sponsored by Kioxia - It’s time to align how flash storage works with applications and the needs of hyperscale environments. This webinar will discuss the problems facing hyperscalers, real-world hyperscale requirements, and the solution that brings control of flash to the storage developer.
Captions: 
	00:00:00,080 --> 00:00:05,440
techs for software enabled flash a newly

00:00:02,879 --> 00:00:07,040
announced technology from keoksha

00:00:05,440 --> 00:00:09,840
today i will be covering what software

00:00:07,040 --> 00:00:12,480
enabled flash is why we created it

00:00:09,840 --> 00:00:14,799
the concept and technologies it contains

00:00:12,480 --> 00:00:16,960
and i will also show some demonstrations

00:00:14,799 --> 00:00:20,160
of the software-enabled flash technology

00:00:16,960 --> 00:00:22,480
running on our prototype fpga

00:00:20,160 --> 00:00:23,680
i'll finish by adding a little section

00:00:22,480 --> 00:00:25,199
on coding examples

00:00:23,680 --> 00:00:28,320
and then directing you to where you can

00:00:25,199 --> 00:00:28,320
obtain more information

00:00:28,720 --> 00:00:35,280
so what is software enabled flash

00:00:33,120 --> 00:00:37,360
well software enable flash is a media

00:00:35,280 --> 00:00:38,800
based host managed hardware approach

00:00:37,360 --> 00:00:40,960
we'll talk more about this in a few

00:00:38,800 --> 00:00:43,040
slides we're trying to redefine

00:00:40,960 --> 00:00:44,960
the way the host interacts with flash to

00:00:43,040 --> 00:00:47,039
allow applications to maximize

00:00:44,960 --> 00:00:48,399
performance and provide functionality

00:00:47,039 --> 00:00:50,239
that would be difficult if not

00:00:48,399 --> 00:00:51,600
impossible to achieve with existing

00:00:50,239 --> 00:00:53,120
interfaces

00:00:51,600 --> 00:00:55,520
giving the host control over media

00:00:53,120 --> 00:00:57,600
enables the host to define the behavior

00:00:55,520 --> 00:00:59,760
of the media and the performance

00:00:57,600 --> 00:01:01,600
characteristics for flash

00:00:59,760 --> 00:01:03,039
this can enable new storage

00:01:01,600 --> 00:01:04,640
functionality

00:01:03,039 --> 00:01:07,200
that maximizes performance and

00:01:04,640 --> 00:01:09,280
efficiencies or stated simply

00:01:07,200 --> 00:01:13,840
the ability to extract the maximum value

00:01:09,280 --> 00:01:13,840
from your flash resources

00:01:14,720 --> 00:01:19,520
coupled with this media based hardware

00:01:16,320 --> 00:01:21,439
approach is a software enabling api

00:01:19,520 --> 00:01:23,920
this api is designed to make managing

00:01:21,439 --> 00:01:25,680
flash media easy while exposing all the

00:01:23,920 --> 00:01:28,080
capabilities of flash

00:01:25,680 --> 00:01:30,079
so software enabled flash technology

00:01:28,080 --> 00:01:32,560
isn't just software but it is software

00:01:30,079 --> 00:01:32,560
enabled

00:01:38,960 --> 00:01:42,799
the highlights of this apr first it is

00:01:41,840 --> 00:01:46,000
an open source

00:01:42,799 --> 00:01:47,759
flash native api next is designed to

00:01:46,000 --> 00:01:49,600
make it easy to create storage solutions

00:01:47,759 --> 00:01:51,840
with powerful capabilities

00:01:49,600 --> 00:01:54,240
it abstracts low level details that are

00:01:51,840 --> 00:01:55,920
vendor and flash generation specific

00:01:54,240 --> 00:01:57,680
so that your code can take advantage of

00:01:55,920 --> 00:02:00,960
the latest flash technologies

00:01:57,680 --> 00:02:01,600
without modification being an open

00:02:00,960 --> 00:02:03,119
project

00:02:01,600 --> 00:02:05,040
any flash vendor can build

00:02:03,119 --> 00:02:07,200
software-enabled flash devices that are

00:02:05,040 --> 00:02:09,679
optimized for their own media

00:02:07,200 --> 00:02:11,520
and finally even though just the api and

00:02:09,679 --> 00:02:13,120
documentation are published today

00:02:11,520 --> 00:02:14,800
kyokushio will be releasing an open

00:02:13,120 --> 00:02:16,959
source software development kit

00:02:14,800 --> 00:02:19,520
with reference source code utilities and

00:02:16,959 --> 00:02:19,520
examples

00:02:21,440 --> 00:02:25,599
so before we go into more details i'd

00:02:23,680 --> 00:02:27,360
just like to reinforce the message that

00:02:25,599 --> 00:02:29,440
software enabled flash was created to

00:02:27,360 --> 00:02:30,319
enable the development of custom storage

00:02:29,440 --> 00:02:33,440
systems that provide

00:02:30,319 --> 00:02:35,280
innovative solutions to storage problems

00:02:33,440 --> 00:02:37,920
software enabled flash is free from

00:02:35,280 --> 00:02:40,640
legacy disk interfaces and paradigms

00:02:37,920 --> 00:02:41,120
it offers total control over the flash

00:02:40,640 --> 00:02:42,800
media

00:02:41,120 --> 00:02:45,040
and can extract the full performance and

00:02:42,800 --> 00:02:46,319
efficiencies of flash by providing

00:02:45,040 --> 00:02:49,040
solutions that are flash

00:02:46,319 --> 00:02:51,040
native software enabled flash is

00:02:49,040 --> 00:02:52,080
extremely easy to adapt to new storage

00:02:51,040 --> 00:02:54,959
applications

00:02:52,080 --> 00:02:57,040
to foster innovation and storage and

00:02:54,959 --> 00:02:58,319
also to enable rapid deployment of new

00:02:57,040 --> 00:03:04,879
storage solutions

00:02:58,319 --> 00:03:07,760
and i'll show some examples of this too

00:03:04,879 --> 00:03:08,400
so in summary software enable flash is

00:03:07,760 --> 00:03:12,080
shedding the

00:03:08,400 --> 00:03:15,040
legacies of hard disks allowing

00:03:12,080 --> 00:03:16,720
applications to have much more control

00:03:15,040 --> 00:03:20,319
to have predictable performance

00:03:16,720 --> 00:03:23,120
within the flash device and

00:03:20,319 --> 00:03:24,730
enabling flexibility and scalability for

00:03:23,120 --> 00:03:27,850
software developers

00:03:24,730 --> 00:03:27,850
[Music]

00:03:28,400 --> 00:03:31,280
as previously introduced

00:03:29,840 --> 00:03:32,879
software-enabled flash consists of

00:03:31,280 --> 00:03:34,319
hardware and software components working

00:03:32,879 --> 00:03:36,480
together

00:03:34,319 --> 00:03:38,159
over the past several years i've had the

00:03:36,480 --> 00:03:39,360
fantastic opportunity to meet with the

00:03:38,159 --> 00:03:42,560
storage developers

00:03:39,360 --> 00:03:44,080
of most of the world's hyperscalers

00:03:42,560 --> 00:03:46,319
taken with the input of other engineers

00:03:44,080 --> 00:03:48,000
at kyocha it has allowed us to distill

00:03:46,319 --> 00:03:50,319
a list of basic requirements and

00:03:48,000 --> 00:03:51,440
features for hyperscale customers

00:03:50,319 --> 00:03:53,599
i should mention that although the

00:03:51,440 --> 00:03:55,200
hyperscalers face similar problems

00:03:53,599 --> 00:03:58,400
their individual priorities and

00:03:55,200 --> 00:04:01,120
approaches vary significantly

00:03:58,400 --> 00:04:03,120
on the requirements side of the slide

00:04:01,120 --> 00:04:05,840
flash abstraction is not just about

00:04:03,120 --> 00:04:07,200
code reuse there could be major economic

00:04:05,840 --> 00:04:08,400
and performance advantages to

00:04:07,200 --> 00:04:12,159
transitioning quickly

00:04:08,400 --> 00:04:13,680
to newer generations of flash at scale

00:04:12,159 --> 00:04:16,560
scheduling is an item that's going to be

00:04:13,680 --> 00:04:19,600
covered in depth later

00:04:16,560 --> 00:04:23,280
access to all the flash media

00:04:19,600 --> 00:04:26,400
in hyperscale environments they refer to

00:04:23,280 --> 00:04:27,360
raid within a device as a raid tax their

00:04:26,400 --> 00:04:30,400
systems are

00:04:27,360 --> 00:04:31,280
designed with redundancy at the system

00:04:30,400 --> 00:04:34,400
level

00:04:31,280 --> 00:04:37,040
and so they are trying to access all the

00:04:34,400 --> 00:04:38,560
capacity that's native to the device

00:04:37,040 --> 00:04:40,160
since they're going to be ensuring

00:04:38,560 --> 00:04:43,680
resiliency at higher levels

00:04:40,160 --> 00:04:46,160
in their environment also

00:04:43,680 --> 00:04:47,919
for these hyperscale environments cpu

00:04:46,160 --> 00:04:50,240
offload is important because they viewed

00:04:47,919 --> 00:04:54,240
the cpu as a sellable resource

00:04:50,240 --> 00:04:57,759
particularly in hosting environments

00:04:54,240 --> 00:05:00,160
and finally device dram configurations

00:04:57,759 --> 00:05:02,240
need to be flexible

00:05:00,160 --> 00:05:04,240
device dram has to handle worst case

00:05:02,240 --> 00:05:07,520
scenarios but often that leads to

00:05:04,240 --> 00:05:10,720
stranded resources in their environment

00:05:07,520 --> 00:05:12,639
on the functionality side data placement

00:05:10,720 --> 00:05:13,840
is important to try and minimize right

00:05:12,639 --> 00:05:17,039
amplification

00:05:13,840 --> 00:05:19,039
for flash media isolation

00:05:17,039 --> 00:05:20,960
is both for security and noisy neighbor

00:05:19,039 --> 00:05:22,800
problems

00:05:20,960 --> 00:05:24,240
latency control is a topic that we're

00:05:22,800 --> 00:05:26,240
going to cover in depth later in the

00:05:24,240 --> 00:05:28,080
presentation too

00:05:26,240 --> 00:05:29,360
buffer management well that's tied back

00:05:28,080 --> 00:05:32,639
to the flexible dram

00:05:29,360 --> 00:05:35,280
configurations and we'll show more

00:05:32,639 --> 00:05:37,759
around that later and finally

00:05:35,280 --> 00:05:39,440
adaptability the hyperscale environment

00:05:37,759 --> 00:05:41,520
is a very dynamic environment and

00:05:39,440 --> 00:05:42,720
changes quickly behind the scenes

00:05:41,520 --> 00:05:44,560
the preferences for standard

00:05:42,720 --> 00:05:46,479
configurations that can be provisioned

00:05:44,560 --> 00:05:52,960
and deployed in real time to meet

00:05:46,479 --> 00:05:55,120
changing workloads

00:05:52,960 --> 00:05:56,240
in response to those requirements and

00:05:55,120 --> 00:05:58,479
required features

00:05:56,240 --> 00:06:00,400
the software-enabled flash api covers

00:05:58,479 --> 00:06:02,960
these three or these major areas

00:06:00,400 --> 00:06:04,880
and a few more that aren't listed group

00:06:02,960 --> 00:06:05,680
broadly the areas of the api are

00:06:04,880 --> 00:06:08,479
concerned with

00:06:05,680 --> 00:06:09,840
the hardware structure buffer management

00:06:08,479 --> 00:06:12,319
programming of the media

00:06:09,840 --> 00:06:13,840
and error management not listed

00:06:12,319 --> 00:06:14,319
separately but touching many of these

00:06:13,840 --> 00:06:17,600
areas

00:06:14,319 --> 00:06:18,560
is latency control in order to maximize

00:06:17,600 --> 00:06:20,560
the performance

00:06:18,560 --> 00:06:22,240
of flash storage it is necessary to be

00:06:20,560 --> 00:06:23,600
aware of the geometry of the flash

00:06:22,240 --> 00:06:25,680
resources

00:06:23,600 --> 00:06:27,280
the software enabled flash api allows

00:06:25,680 --> 00:06:29,600
storage applications to query this

00:06:27,280 --> 00:06:31,280
geometry in a standardized manner

00:06:29,600 --> 00:06:33,199
some of the characteristics suppose are

00:06:31,280 --> 00:06:35,520
listed here

00:06:33,199 --> 00:06:37,560
the api also allows control over how

00:06:35,520 --> 00:06:39,759
many flash blocks may be open

00:06:37,560 --> 00:06:41,199
simultaneously and can control the

00:06:39,759 --> 00:06:43,120
management of the associated write

00:06:41,199 --> 00:06:45,440
buffers

00:06:43,120 --> 00:06:46,720
when discussing the programming of flash

00:06:45,440 --> 00:06:48,800
it's important to note

00:06:46,720 --> 00:06:50,560
that the optimal programming algorithms

00:06:48,800 --> 00:06:52,400
vary from vendor to vendor and often

00:06:50,560 --> 00:06:53,599
between generations of flash from a

00:06:52,400 --> 00:06:55,520
single vendor

00:06:53,599 --> 00:06:57,280
software enabled flash handles all the

00:06:55,520 --> 00:07:00,319
details of programming

00:06:57,280 --> 00:07:01,039
and lets the flash vendor optimize for

00:07:00,319 --> 00:07:04,160
their own

00:07:01,039 --> 00:07:05,599
particular media the api was created

00:07:04,160 --> 00:07:06,960
with consideration for other flash

00:07:05,599 --> 00:07:09,280
vendors as well as

00:07:06,960 --> 00:07:12,400
all the foreseeable needs of kyokshi's

00:07:09,280 --> 00:07:14,720
own future flash generations

00:07:12,400 --> 00:07:16,560
finally with respect to error management

00:07:14,720 --> 00:07:18,400
software enabled flash allows vendors to

00:07:16,560 --> 00:07:19,360
optimize error reduction techniques for

00:07:18,400 --> 00:07:22,080
their specific

00:07:19,360 --> 00:07:23,840
flash media characteristics and it also

00:07:22,080 --> 00:07:26,720
controls the time budget for error

00:07:23,840 --> 00:07:26,720
recovery attempts

00:07:29,680 --> 00:07:35,520
this is a high-level block diagram

00:07:32,800 --> 00:07:37,199
of our software-enabled flash controller

00:07:35,520 --> 00:07:38,720
it is one possible design and other

00:07:37,199 --> 00:07:40,400
vendors are free to implement different

00:07:38,720 --> 00:07:41,840
architectures as long as they comply at

00:07:40,400 --> 00:07:43,599
the api level

00:07:41,840 --> 00:07:44,879
for example although we currently use

00:07:43,599 --> 00:07:46,800
the toggle interface

00:07:44,879 --> 00:07:48,319
at the bottom of this block diagram to

00:07:46,800 --> 00:07:50,240
connect to our flash chips

00:07:48,319 --> 00:07:51,360
other vendors may implement on fee

00:07:50,240 --> 00:07:54,000
interconnects or

00:07:51,360 --> 00:07:55,680
a different style of interconnect note

00:07:54,000 --> 00:07:57,120
that our design uses standardized

00:07:55,680 --> 00:07:59,280
interfaces wherever possible

00:07:57,120 --> 00:08:02,240
including the pcie interface at the top

00:07:59,280 --> 00:08:02,240
of the block diagram

00:08:02,560 --> 00:08:06,080
the controller itself is focused on

00:08:04,400 --> 00:08:08,000
management of the flash media

00:08:06,080 --> 00:08:09,599
programming lifetime management and

00:08:08,000 --> 00:08:11,199
defect management

00:08:09,599 --> 00:08:13,759
the controller has advanced scheduling

00:08:11,199 --> 00:08:15,759
capabilities on a per die basis

00:08:13,759 --> 00:08:17,919
and hardware acceleration for garbage

00:08:15,759 --> 00:08:20,560
collection operations if needed

00:08:17,919 --> 00:08:22,879
as well as wear leveling tasks and other

00:08:20,560 --> 00:08:24,840
administrative tasks

00:08:22,879 --> 00:08:26,240
another important call out here is the

00:08:24,840 --> 00:08:29,360
use

00:08:26,240 --> 00:08:31,919
of on device dram is optional

00:08:29,360 --> 00:08:33,839
in the software enabled flash technology

00:08:31,919 --> 00:08:36,240
host memory resources may be used

00:08:33,839 --> 00:08:36,240
instead

00:08:39,440 --> 00:08:44,240
so as we just mentioned the use of dram

00:08:42,399 --> 00:08:45,200
on our software enabled flash devices is

00:08:44,240 --> 00:08:48,800
optional

00:08:45,200 --> 00:08:50,480
why is that important well the answer

00:08:48,800 --> 00:08:51,519
lies in the fact that hyperscale

00:08:50,480 --> 00:08:53,360
customers

00:08:51,519 --> 00:08:55,920
often require thousands of

00:08:53,360 --> 00:08:57,440
simultaneously open flash blocks on each

00:08:55,920 --> 00:08:59,839
flash device

00:08:57,440 --> 00:09:00,560
the actual numbers vary from customer to

00:08:59,839 --> 00:09:03,920
customer

00:09:00,560 --> 00:09:04,720
but we've heard requirements between 4

00:09:03,920 --> 00:09:08,080
00:09:04,720 --> 00:09:11,120
to up to 16 000 open blocks

00:09:08,080 --> 00:09:13,120
on each individual device since each

00:09:11,120 --> 00:09:15,360
open block requires a right buffer this

00:09:13,120 --> 00:09:17,600
can create a demand for a lot of memory

00:09:15,360 --> 00:09:18,800
within the device potentially tens of

00:09:17,600 --> 00:09:22,240
gigabytes

00:09:18,800 --> 00:09:23,680
of dram dedicated to use as a right

00:09:22,240 --> 00:09:25,440
buffer

00:09:23,680 --> 00:09:27,920
it's often the case that the actual

00:09:25,440 --> 00:09:28,800
number of open flash blocks can vary

00:09:27,920 --> 00:09:31,519
over time

00:09:28,800 --> 00:09:32,640
as a function of the system load so

00:09:31,519 --> 00:09:34,560
sizing the device

00:09:32,640 --> 00:09:36,240
the ram configuration for a worst case

00:09:34,560 --> 00:09:38,000
scenario creates what we call

00:09:36,240 --> 00:09:39,360
stranded dram resources within the

00:09:38,000 --> 00:09:42,480
device

00:09:39,360 --> 00:09:45,519
they must be present to handle peak load

00:09:42,480 --> 00:09:46,080
but in normal operation that's dram that

00:09:45,519 --> 00:09:48,839
is

00:09:46,080 --> 00:09:50,560
essentially idled and not used for other

00:09:48,839 --> 00:09:53,200
purposes

00:09:50,560 --> 00:09:56,080
software enabled flash supports device

00:09:53,200 --> 00:09:58,080
dram configurations

00:09:56,080 --> 00:09:59,440
it also supports host only dram

00:09:58,080 --> 00:10:01,360
configurations

00:09:59,440 --> 00:10:03,360
and most interestingly hyper

00:10:01,360 --> 00:10:05,920
configurations that allow the dram to be

00:10:03,360 --> 00:10:08,800
sized within the device for normal usage

00:10:05,920 --> 00:10:09,519
and for the device to drop on host dram

00:10:08,800 --> 00:10:12,640
resources

00:10:09,519 --> 00:10:16,320
during periods exceeding the limits

00:10:12,640 --> 00:10:16,320
of the drive's internal resources

00:10:16,480 --> 00:10:21,440
we should note that in systems that use

00:10:18,320 --> 00:10:24,160
host dram resources

00:10:21,440 --> 00:10:25,600
there are system requirements to protect

00:10:24,160 --> 00:10:27,680
against data loss in the event of

00:10:25,600 --> 00:10:29,440
unexpected power loss

00:10:27,680 --> 00:10:30,959
many hyperscale environments already

00:10:29,440 --> 00:10:32,160
have this in place with non-volatile

00:10:30,959 --> 00:10:34,000
memory resources

00:10:32,160 --> 00:10:35,440
system level mirroring and system level

00:10:34,000 --> 00:10:39,600
erasure coding

00:10:35,440 --> 00:10:43,120
so in summary we support both

00:10:39,600 --> 00:10:47,600
dram within the device dram

00:10:43,120 --> 00:10:50,240
only in the host or hybrid solutions

00:10:47,600 --> 00:10:52,160
that contain dram both in the device and

00:10:50,240 --> 00:10:56,079
in the host

00:10:52,160 --> 00:10:58,079
there are benefits to each approach

00:10:56,079 --> 00:10:59,360
with the unified right buffers that can

00:10:58,079 --> 00:11:01,519
draw upon

00:10:59,360 --> 00:11:02,959
host side resources we can dynamically

00:11:01,519 --> 00:11:05,440
adapt to changing

00:11:02,959 --> 00:11:06,079
requirements and workloads and we can

00:11:05,440 --> 00:11:08,160
optimize

00:11:06,079 --> 00:11:11,600
dram at the system level without

00:11:08,160 --> 00:11:13,440
strength stranding any dram resources

00:11:11,600 --> 00:11:14,800
the conventional architectures are more

00:11:13,440 --> 00:11:17,200
appropriate for systems

00:11:14,800 --> 00:11:18,880
that cannot meet the resiliency

00:11:17,200 --> 00:11:21,760
requirements at the system level

00:11:18,880 --> 00:11:22,560
and rely upon the drive for data loss

00:11:21,760 --> 00:11:28,240
protection

00:11:22,560 --> 00:11:29,839
in the event of sudden power loss

00:11:28,240 --> 00:11:32,399
now i'd like to introduce the software

00:11:29,839 --> 00:11:34,480
components of software enabled flash

00:11:32,399 --> 00:11:37,120
kyochea has created and will soon

00:11:34,480 --> 00:11:39,200
release a software development kit

00:11:37,120 --> 00:11:41,120
this will provide open source reference

00:11:39,200 --> 00:11:43,040
block drivers open source reference

00:11:41,120 --> 00:11:45,360
flash translation layers

00:11:43,040 --> 00:11:47,040
and open source device management

00:11:45,360 --> 00:11:48,560
utilities

00:11:47,040 --> 00:11:50,320
bundled with the software development

00:11:48,560 --> 00:11:53,839
kit will be an api library

00:11:50,320 --> 00:11:56,240
once again in open source and also we'll

00:11:53,839 --> 00:11:58,639
have a device driver too

00:11:56,240 --> 00:11:59,360
this block diagram shows how the pieces

00:11:58,639 --> 00:12:01,519
of buffer

00:11:59,360 --> 00:12:03,760
development kit interface with each

00:12:01,519 --> 00:12:03,760
other

00:12:05,120 --> 00:12:08,320
two notes on the software layering first

00:12:07,600 --> 00:12:11,760
as you can see

00:12:08,320 --> 00:12:14,480
it's possible for a user application

00:12:11,760 --> 00:12:15,040
that is software enable flash native to

00:12:14,480 --> 00:12:17,839
interact

00:12:15,040 --> 00:12:18,720
directly with the device bypassing file

00:12:17,839 --> 00:12:21,680
systems

00:12:18,720 --> 00:12:22,240
and traditional flash translation layers

00:12:21,680 --> 00:12:24,639
and block

00:12:22,240 --> 00:12:26,560
drivers we have a couple of proof of

00:12:24,639 --> 00:12:28,480
concept applications running today on

00:12:26,560 --> 00:12:30,399
our own fpga prototypes

00:12:28,480 --> 00:12:31,920
these include a software enabled flash

00:12:30,399 --> 00:12:34,160
engine for fio

00:12:31,920 --> 00:12:36,399
as well as a version of roxdb and a

00:12:34,160 --> 00:12:39,680
version of firecracker that are software

00:12:36,399 --> 00:12:41,040
software enabled flash native although

00:12:39,680 --> 00:12:43,279
these applications are currently

00:12:41,040 --> 00:12:44,240
just proof of concepts in the future we

00:12:43,279 --> 00:12:46,399
plan to include

00:12:44,240 --> 00:12:48,320
open source native applications as part

00:12:46,399 --> 00:12:51,600
of the sdk

00:12:48,320 --> 00:12:54,880
the second note is that the

00:12:51,600 --> 00:12:56,720
block labeled sds stack is a software

00:12:54,880 --> 00:12:58,399
defined storage stack

00:12:56,720 --> 00:13:00,079
hyperscalers are already running their

00:12:58,399 --> 00:13:01,440
own software-defined storage stacks in

00:13:00,079 --> 00:13:02,959
their environments

00:13:01,440 --> 00:13:04,720
these software-defined storage stacks

00:13:02,959 --> 00:13:06,720
can be modified to interface

00:13:04,720 --> 00:13:08,399
with the software-enabled flash library

00:13:06,720 --> 00:13:09,360
and are not dependent on the use of the

00:13:08,399 --> 00:13:11,680
sdk

00:13:09,360 --> 00:13:14,399
in such cases the sdk serves as the

00:13:11,680 --> 00:13:18,079
detailed example of the best practices

00:13:14,399 --> 00:13:18,079
for using software enabled flash

00:13:18,240 --> 00:13:21,600
the apl library is once again open

00:13:20,800 --> 00:13:24,880
source

00:13:21,600 --> 00:13:26,560
runs on linux it has both c and c plus

00:13:24,880 --> 00:13:29,040
plus bindings

00:13:26,560 --> 00:13:31,680
it supports use internal space as well

00:13:29,040 --> 00:13:34,320
as user space

00:13:31,680 --> 00:13:34,959
and provides easy access to all the

00:13:34,320 --> 00:13:38,160
features

00:13:34,959 --> 00:13:38,160
software enabled flash

00:13:39,120 --> 00:13:42,880
and now for a system level view of one

00:13:41,519 --> 00:13:46,000
possible deployment

00:13:42,880 --> 00:13:48,079
of software enabled flash note that the

00:13:46,000 --> 00:13:49,920
items outlined with the red dashed lines

00:13:48,079 --> 00:13:52,160
or items that would likely be customized

00:13:49,920 --> 00:13:55,360
for different environments

00:13:52,160 --> 00:13:57,519
let's start at the top and work down so

00:13:55,360 --> 00:14:00,399
at the very top of this

00:13:57,519 --> 00:14:01,680
diagram you see unmodified applications

00:14:00,399 --> 00:14:04,480
running within

00:14:01,680 --> 00:14:06,639
a virtualized guest those applications

00:14:04,480 --> 00:14:08,000
typically use the posix api to talk to a

00:14:06,639 --> 00:14:10,320
file system

00:14:08,000 --> 00:14:11,519
and the file systems use block device i

00:14:10,320 --> 00:14:13,360
o

00:14:11,519 --> 00:14:14,560
to talk to devices in this particular

00:14:13,360 --> 00:14:16,880
case since these are

00:14:14,560 --> 00:14:17,920
virtualized guest environments they're

00:14:16,880 --> 00:14:22,639
actually talking

00:14:17,920 --> 00:14:25,279
to a hypervisor within the kernel

00:14:22,639 --> 00:14:26,160
in this case we're calling out that we

00:14:25,279 --> 00:14:28,959
have built

00:14:26,160 --> 00:14:30,959
a software enabled flash qemu block

00:14:28,959 --> 00:14:33,199
device driver

00:14:30,959 --> 00:14:34,399
this interfaces with our reference flash

00:14:33,199 --> 00:14:38,720
translation layer

00:14:34,399 --> 00:14:40,959
and the software enabled flash library

00:14:38,720 --> 00:14:42,320
to bring block level features and

00:14:40,959 --> 00:14:45,839
functionality

00:14:42,320 --> 00:14:45,839
to the virtualized guests

00:14:46,320 --> 00:14:51,279
the software enabled flash reference ftl

00:14:49,760 --> 00:14:53,519
communicates to the software enabled

00:14:51,279 --> 00:14:55,199
flash library using the software enabled

00:14:53,519 --> 00:14:58,480
flash api

00:14:55,199 --> 00:15:01,680
the library in turn uses iou ring

00:14:58,480 --> 00:15:02,560
to talk to the software enabled flash

00:15:01,680 --> 00:15:05,760
driver

00:15:02,560 --> 00:15:08,240
in kernel space this allows us to

00:15:05,760 --> 00:15:09,920
bypass system call overhead for greater

00:15:08,240 --> 00:15:13,839
performance

00:15:09,920 --> 00:15:13,839
the final thing to note is that

00:15:13,920 --> 00:15:20,720
on the far left side of the diagram

00:15:17,760 --> 00:15:22,320
outlined in red there are ceph native

00:15:20,720 --> 00:15:23,519
applications software-enabled flash

00:15:22,320 --> 00:15:25,519
native applications

00:15:23,519 --> 00:15:27,839
that can interface directly with our

00:15:25,519 --> 00:15:27,839
library

00:15:28,000 --> 00:15:34,399
utilizing the iou ring mechanism

00:15:31,040 --> 00:15:36,639
to allow applications to interact with

00:15:34,399 --> 00:15:39,440
software-enabled flash devices

00:15:36,639 --> 00:15:41,839
without system call overhead for high

00:15:39,440 --> 00:15:41,839
performance

00:15:43,360 --> 00:15:47,360
finally the software enabled flash qmu

00:15:46,320 --> 00:15:49,839
block driver

00:15:47,360 --> 00:15:50,480
as i mentioned it's part of this sdk is

00:15:49,839 --> 00:15:51,920
useful

00:15:50,480 --> 00:15:53,839
because it allows unmodified

00:15:51,920 --> 00:15:55,199
applications to take advantage of many

00:15:53,839 --> 00:15:56,000
of the features of software enabled

00:15:55,199 --> 00:15:58,880
flash

00:15:56,000 --> 00:16:00,320
such as isolation latency control and

00:15:58,880 --> 00:16:04,240
die time allocation

00:16:00,320 --> 00:16:07,279
even without modification so

00:16:04,240 --> 00:16:10,720
i encourage people to

00:16:07,279 --> 00:16:13,120
look into the possible uses of the

00:16:10,720 --> 00:16:15,440
software enabled flash qmu block device

00:16:13,120 --> 00:16:15,440
driver

00:16:16,320 --> 00:16:20,560
all right at this point we're gonna

00:16:18,560 --> 00:16:23,440
introduce some concepts and features

00:16:20,560 --> 00:16:24,880
of the software enabled flash technology

00:16:23,440 --> 00:16:28,639
that are going to be necessary

00:16:24,880 --> 00:16:28,639
to understand later examples

00:16:31,440 --> 00:16:38,000
this diagram is a representation of a

00:16:34,959 --> 00:16:39,759
possible software enabled flash device

00:16:38,000 --> 00:16:41,920
at the top of the diagram we have the

00:16:39,759 --> 00:16:45,680
actual control logic

00:16:41,920 --> 00:16:49,360
and then this particular device has

00:16:45,680 --> 00:16:51,920
32 die that are arranged on

00:16:49,360 --> 00:16:54,320
eight channels with four banks per

00:16:51,920 --> 00:16:54,320
channel

00:16:55,440 --> 00:17:00,480
the first concept that i'd like to

00:16:57,600 --> 00:17:03,440
introduce is that of a virtual device

00:17:00,480 --> 00:17:04,640
a virtual device is a set of one or more

00:17:03,440 --> 00:17:07,679
flash die

00:17:04,640 --> 00:17:09,360
that provides hardware isolation so when

00:17:07,679 --> 00:17:12,000
you define a virtual device

00:17:09,360 --> 00:17:12,559
you get to define how many channels the

00:17:12,000 --> 00:17:15,199
device

00:17:12,559 --> 00:17:17,600
is spread across as well as how many

00:17:15,199 --> 00:17:17,600
banks

00:17:17,839 --> 00:17:24,079
individual flash dye are mapped

00:17:20,880 --> 00:17:24,400
one-to-one to a virtual device a flash

00:17:24,079 --> 00:17:28,880
die

00:17:24,400 --> 00:17:28,880
is never shared between virtual devices

00:17:30,880 --> 00:17:35,840
the next concept is that of a quality of

00:17:33,280 --> 00:17:37,520
service domain or a qos domain

00:17:35,840 --> 00:17:39,200
this is a mechanism that allows us to

00:17:37,520 --> 00:17:41,200
impose capacity quotas

00:17:39,200 --> 00:17:44,880
and scheduling policy as well as to

00:17:41,200 --> 00:17:47,919
provide software-based isolation

00:17:44,880 --> 00:17:51,039
note that unlike the virtual devices

00:17:47,919 --> 00:17:54,320
you can have qos domains that

00:17:51,039 --> 00:17:54,799
share a single virtual device so in this

00:17:54,320 --> 00:17:57,919
example

00:17:54,799 --> 00:18:01,120
qos domain 0 and qos main 2

00:17:57,919 --> 00:18:03,760
are sharing virtual device zero likewise

00:18:01,120 --> 00:18:07,919
qs domain 4 and qs domain 5

00:18:03,760 --> 00:18:07,919
are also sharing a virtual device

00:18:10,160 --> 00:18:14,080
the final concept is that of a placement

00:18:12,559 --> 00:18:16,000
id and this is a mechanism

00:18:14,080 --> 00:18:17,440
that we'll talk more about later that

00:18:16,000 --> 00:18:19,760
allows us to group

00:18:17,440 --> 00:18:22,799
data at the superblock level within a

00:18:19,760 --> 00:18:22,799
qos domain

00:18:26,880 --> 00:18:31,039
this slide describes how the concepts

00:18:28,960 --> 00:18:33,200
introduced on the previous slide

00:18:31,039 --> 00:18:35,919
provide control over data isolation and

00:18:33,200 --> 00:18:35,919
data placement

00:18:36,080 --> 00:18:44,400
superblocks within a virtual device

00:18:40,320 --> 00:18:46,320
start out in a free pool as qos domains

00:18:44,400 --> 00:18:49,039
allocate storage

00:18:46,320 --> 00:18:51,200
a block is drawn from the free pool and

00:18:49,039 --> 00:18:53,200
assigned to a quest domain

00:18:51,200 --> 00:18:55,360
the device can choose any free block

00:18:53,200 --> 00:18:56,240
from the pool so can track block wear

00:18:55,360 --> 00:18:59,039
and block health

00:18:56,240 --> 00:19:01,520
and assign the optimal block to maximize

00:18:59,039 --> 00:19:03,200
device endurance

00:19:01,520 --> 00:19:05,280
super blocks are never shared between

00:19:03,200 --> 00:19:07,200
domains there is no mixing of data at

00:19:05,280 --> 00:19:09,120
the block level

00:19:07,200 --> 00:19:10,400
once a superblock is released it's

00:19:09,120 --> 00:19:12,240
returned to the free poll

00:19:10,400 --> 00:19:13,919
so over the lifetime of the device

00:19:12,240 --> 00:19:14,799
ownership of a super block may change

00:19:13,919 --> 00:19:16,880
several times

00:19:14,799 --> 00:19:19,760
if there are multiple qos domains within

00:19:16,880 --> 00:19:19,760
a virtual device

00:19:21,520 --> 00:19:24,960
so the virtual device provides physical

00:19:24,160 --> 00:19:27,440
isolation

00:19:24,960 --> 00:19:28,400
at the dye level and can be used to

00:19:27,440 --> 00:19:32,080
shape both

00:19:28,400 --> 00:19:35,200
the capacity and i o throughput

00:19:32,080 --> 00:19:35,200
for an application

00:19:37,280 --> 00:19:43,039
within the virtual device blocks

00:19:40,880 --> 00:19:44,160
are combined to form super blocks that

00:19:43,039 --> 00:19:47,200
are striped across

00:19:44,160 --> 00:19:48,799
each die in the virtual device this

00:19:47,200 --> 00:19:49,600
controls the amount of parallelism and

00:19:48,799 --> 00:19:52,240
the amount of

00:19:49,600 --> 00:19:54,720
potential throughput for the virtual

00:19:52,240 --> 00:19:54,720
device

00:19:59,039 --> 00:20:02,880
to summarize data isolation the two main

00:20:01,520 --> 00:20:04,799
mechanisms are listed here with their

00:20:02,880 --> 00:20:06,960
benefits and restrictions so

00:20:04,799 --> 00:20:08,480
we have dye level isolation in the form

00:20:06,960 --> 00:20:11,440
of virtual devices

00:20:08,480 --> 00:20:14,640
and block level isolation in the form of

00:20:11,440 --> 00:20:16,960
qs domains

00:20:14,640 --> 00:20:18,080
dye level isolation is the most

00:20:16,960 --> 00:20:21,120
effective

00:20:18,080 --> 00:20:24,159
isolation it provides

00:20:21,120 --> 00:20:28,000
full hardware isolation uh between

00:20:24,159 --> 00:20:30,960
applications however typical devices

00:20:28,000 --> 00:20:31,919
only have a small number of flash die

00:20:30,960 --> 00:20:35,120
per device

00:20:31,919 --> 00:20:38,400
this can be in the range of

00:20:35,120 --> 00:20:41,520
16 to at the high end

00:20:38,400 --> 00:20:41,520
low hundreds so

00:20:41,679 --> 00:20:47,120
there's a small number of potential

00:20:45,120 --> 00:20:49,919
virtual devices and it's limited by the

00:20:47,120 --> 00:20:54,880
actual number of physical dye

00:20:49,919 --> 00:20:54,880
on the device however

00:20:56,000 --> 00:21:02,000
block level isolation using qs domains

00:20:59,200 --> 00:21:03,440
is far more scalable it can scale into

00:21:02,000 --> 00:21:06,559
the thousands

00:21:03,440 --> 00:21:09,679
of tenants and while the isolation is

00:21:06,559 --> 00:21:12,240
not total hardware isolation

00:21:09,679 --> 00:21:14,320
we can guarantee predictable performance

00:21:12,240 --> 00:21:17,840
and extend lifetime

00:21:14,320 --> 00:21:19,760
and keep data separated

00:21:17,840 --> 00:21:21,840
at the block level between different

00:21:19,760 --> 00:21:24,159
applications

00:21:21,840 --> 00:21:26,559
for either security or performance

00:21:24,159 --> 00:21:26,559
reasons

00:21:29,360 --> 00:21:34,000
closely related to data isolation is

00:21:32,000 --> 00:21:36,080
data placement

00:21:34,000 --> 00:21:38,080
here we're introducing the concept of a

00:21:36,080 --> 00:21:39,600
nameless write mechanism to control data

00:21:38,080 --> 00:21:41,760
placement

00:21:39,600 --> 00:21:43,280
why is a new write mechanism desirable

00:21:41,760 --> 00:21:45,679
why did we do this

00:21:43,280 --> 00:21:47,919
well although the system benefits can be

00:21:45,679 --> 00:21:50,559
realized

00:21:47,919 --> 00:21:52,159
with control over data placement if

00:21:50,559 --> 00:21:52,880
physical addressing is allowed for

00:21:52,159 --> 00:21:54,960
rights

00:21:52,880 --> 00:21:56,720
the host becomes responsible for device

00:21:54,960 --> 00:21:59,280
wear

00:21:56,720 --> 00:22:00,559
flash memory is a consumable it wears

00:21:59,280 --> 00:22:02,559
out with use

00:22:00,559 --> 00:22:04,480
and so poor choices for physical data

00:22:02,559 --> 00:22:06,320
placement can wear out flash devices

00:22:04,480 --> 00:22:09,440
quickly

00:22:06,320 --> 00:22:11,039
so even though the host would like to be

00:22:09,440 --> 00:22:13,200
able to control data placement

00:22:11,039 --> 00:22:16,000
to minimize right amplification and

00:22:13,200 --> 00:22:19,440
group data from applications or tenants

00:22:16,000 --> 00:22:21,120
that have potentially similar lifespans

00:22:19,440 --> 00:22:23,760
the host really doesn't want to be

00:22:21,120 --> 00:22:26,240
responsible for the lifetime management

00:22:23,760 --> 00:22:28,000
of the media itself

00:22:26,240 --> 00:22:30,000
how can the host have control over data

00:22:28,000 --> 00:22:31,600
placement without this responsibility of

00:22:30,000 --> 00:22:34,720
ensuring the device health

00:22:31,600 --> 00:22:37,280
well the answer is nameless right

00:22:34,720 --> 00:22:38,159
when a new super block is required for a

00:22:37,280 --> 00:22:39,919
placement id

00:22:38,159 --> 00:22:42,159
or if a new super block is manually

00:22:39,919 --> 00:22:44,880
allocated the device chooses

00:22:42,159 --> 00:22:46,320
the optimal super block to use this is

00:22:44,880 --> 00:22:47,600
the framework for nameless write

00:22:46,320 --> 00:22:50,720
mechanism

00:22:47,600 --> 00:22:55,039
now let's show how the actual nameless

00:22:50,720 --> 00:22:57,039
write works

00:22:55,039 --> 00:22:58,960
nameless rights allow the device to

00:22:57,039 --> 00:23:00,559
choose where the to physically write the

00:22:58,960 --> 00:23:02,320
data

00:23:00,559 --> 00:23:04,080
but allows the host to bound the

00:23:02,320 --> 00:23:05,360
possible choices for where the device

00:23:04,080 --> 00:23:07,679
will put it

00:23:05,360 --> 00:23:10,320
as mentioned earlier qos domains are

00:23:07,679 --> 00:23:11,919
mapped to device nodes in the system

00:23:10,320 --> 00:23:14,400
and so the nameless write operations

00:23:11,919 --> 00:23:16,480
must supply a qs domain handle

00:23:14,400 --> 00:23:17,679
as well as either a placement id for

00:23:16,480 --> 00:23:19,760
auto allocation

00:23:17,679 --> 00:23:21,600
or a super block flash address returned

00:23:19,760 --> 00:23:26,080
by a previous manual super superblock

00:23:21,600 --> 00:23:27,200
allocate command the qos domain maps to

00:23:26,080 --> 00:23:29,760
a virtual device

00:23:27,200 --> 00:23:31,840
which in turn specifies which die can be

00:23:29,760 --> 00:23:35,280
used for the right

00:23:31,840 --> 00:23:37,760
so by virtue of the qos domain you've

00:23:35,280 --> 00:23:40,799
specified a subset of dye

00:23:37,760 --> 00:23:43,200
where this write can be performed

00:23:40,799 --> 00:23:45,440
the placement id or flash address

00:23:43,200 --> 00:23:46,799
specifies the super block which is owned

00:23:45,440 --> 00:23:49,120
by the qos domain

00:23:46,799 --> 00:23:50,720
that should be used for the right if a

00:23:49,120 --> 00:23:52,640
placement id is specified

00:23:50,720 --> 00:23:54,880
a nameless write command can span

00:23:52,640 --> 00:23:56,400
superblocks and additional superblocks

00:23:54,880 --> 00:23:57,360
will be allocated to the domain as

00:23:56,400 --> 00:23:59,200
needed

00:23:57,360 --> 00:24:02,480
in manual allocation mode nameless

00:23:59,200 --> 00:24:02,480
rights cannot span

00:24:02,840 --> 00:24:07,600
superblocks

00:24:04,640 --> 00:24:09,919
as illustrated the device is free to

00:24:07,600 --> 00:24:12,080
write the data to any empty space within

00:24:09,919 --> 00:24:13,679
the bounds specified by the host

00:24:12,080 --> 00:24:16,480
and when the write is complete the

00:24:13,679 --> 00:24:18,960
actual physical address is returned

00:24:16,480 --> 00:24:22,000
this enables direct physical read path

00:24:18,960 --> 00:24:23,840
with no address for translation required

00:24:22,000 --> 00:24:26,480
direct physical read optimizes

00:24:23,840 --> 00:24:27,840
performance and minimizes latency

00:24:26,480 --> 00:24:29,760
the nameless write operation

00:24:27,840 --> 00:24:34,400
automatically handles

00:24:29,760 --> 00:24:37,840
all media defects so that

00:24:34,400 --> 00:24:39,039
you always are given the actual physical

00:24:37,840 --> 00:24:46,000
address at which

00:24:39,039 --> 00:24:48,640
the data resides

00:24:46,000 --> 00:24:49,279
similar to the nameless write operation

00:24:48,640 --> 00:24:51,440
operable

00:24:49,279 --> 00:24:53,039
flash has a nameless copy operation that

00:24:51,440 --> 00:24:54,240
can be used to move data within the

00:24:53,039 --> 00:24:56,880
device without post

00:24:54,240 --> 00:24:56,880
processing

00:24:58,880 --> 00:25:02,080
this is useful for implementing garbage

00:25:00,640 --> 00:25:04,240
collection wear leveling

00:25:02,080 --> 00:25:05,520
and other administrative tasks the

00:25:04,240 --> 00:25:08,320
nameless copy function

00:25:05,520 --> 00:25:09,600
takes as input a source superblock a

00:25:08,320 --> 00:25:11,919
destination superblock

00:25:09,600 --> 00:25:14,799
and copy instructions these copy

00:25:11,919 --> 00:25:17,200
instructions are powerful primitives

00:25:14,799 --> 00:25:18,080
supporting valid data bitmaps lists of

00:25:17,200 --> 00:25:20,480
logical block

00:25:18,080 --> 00:25:22,320
addresses or even filters to select

00:25:20,480 --> 00:25:24,880
copied data based on logical block

00:25:22,320 --> 00:25:24,880
addresses

00:25:25,039 --> 00:25:29,520
a nameless copy command can operate on

00:25:27,039 --> 00:25:31,200
entire superblocks with a single command

00:25:29,520 --> 00:25:33,120
this animation illustrates the

00:25:31,200 --> 00:25:34,880
difference in impact for the host to

00:25:33,120 --> 00:25:36,640
implement a garbage collect

00:25:34,880 --> 00:25:38,720
using standard read and write commands

00:25:36,640 --> 00:25:42,159
versus the nameless copy

00:25:38,720 --> 00:25:43,360
as you can see for the manual copy

00:25:42,159 --> 00:25:46,720
operation

00:25:43,360 --> 00:25:48,480
the host had to issue 20 commands 16 of

00:25:46,720 --> 00:25:51,760
which were reads and fours were right

00:25:48,480 --> 00:25:54,799
to harvest the

00:25:51,760 --> 00:25:56,880
data for garbage collection the nameless

00:25:54,799 --> 00:26:00,559
copy operation issued a single command

00:25:56,880 --> 00:26:00,559
to perform the exact same work

00:26:03,840 --> 00:26:12,960
now i'm going to show a video that was

00:26:08,400 --> 00:26:12,960
captured on our fpga prototype system

00:26:17,120 --> 00:26:23,840
so on the left hand side

00:26:21,360 --> 00:26:24,559
manual copy is being used for garbage

00:26:23,840 --> 00:26:26,559
collection

00:26:24,559 --> 00:26:28,080
on the right hand side nameless copy is

00:26:26,559 --> 00:26:31,440
being used

00:26:28,080 --> 00:26:32,000
the two workloads are identical the blue

00:26:31,440 --> 00:26:34,320
rising

00:26:32,000 --> 00:26:36,720
graph are the actual garbage collect

00:26:34,320 --> 00:26:38,480
operations

00:26:36,720 --> 00:26:42,080
over the course of this workload there's

00:26:38,480 --> 00:26:44,320
a dramatic difference

00:26:42,080 --> 00:26:45,760
in the impact to the system as you can

00:26:44,320 --> 00:26:48,000
see from the center graph

00:26:45,760 --> 00:26:49,360
cpu utilization was much lower for

00:26:48,000 --> 00:26:52,799
nameless copy

00:26:49,360 --> 00:26:54,640
why was it lower well to perform the

00:26:52,799 --> 00:26:58,960
exact same workload

00:26:54,640 --> 00:27:01,840
nameless copy only needed 24 commands

00:26:58,960 --> 00:27:02,799
to do garbage collection versus the

00:27:01,840 --> 00:27:06,159
manual copy

00:27:02,799 --> 00:27:10,000
needed over 800 000 commands

00:27:06,159 --> 00:27:12,400
likewise nameless copy only transferred

00:27:10,000 --> 00:27:14,880
120 kilobytes of data

00:27:12,400 --> 00:27:16,799
these were the copy instructions versus

00:27:14,880 --> 00:27:18,720
over three gigabytes of data that flowed

00:27:16,799 --> 00:27:21,360
through the host for a manual

00:27:18,720 --> 00:27:24,320
garbage collect operation pretty

00:27:21,360 --> 00:27:24,320
dramatic results

00:27:27,679 --> 00:27:31,200
another key feature of software event

00:27:30,159 --> 00:27:32,799
enabled flash

00:27:31,200 --> 00:27:34,799
is the advanced queuing and scheduling

00:27:32,799 --> 00:27:37,440
features we'll spend a bit of time here

00:27:34,799 --> 00:27:39,600
with several slides

00:27:37,440 --> 00:27:40,960
scheduling and queuing controls how die

00:27:39,600 --> 00:27:43,200
time is spent

00:27:40,960 --> 00:27:45,360
this means how much time is spent for

00:27:43,200 --> 00:27:47,200
read operations versus write operations

00:27:45,360 --> 00:27:48,880
versus copy operations

00:27:47,200 --> 00:27:50,880
as well as the prioritization of

00:27:48,880 --> 00:27:53,039
multiple workloads

00:27:50,880 --> 00:27:54,960
consider a multi-tenant environment

00:27:53,039 --> 00:27:56,880
there may be business reasons to enforce

00:27:54,960 --> 00:27:59,039
fairness or give certain tenants

00:27:56,880 --> 00:28:00,799
priority and these business needs may

00:27:59,039 --> 00:28:02,480
change over time

00:28:00,799 --> 00:28:04,320
these tenants can share a device and

00:28:02,480 --> 00:28:06,240
wait if their queuing can support the

00:28:04,320 --> 00:28:08,159
performance goals the host is allowed to

00:28:06,240 --> 00:28:11,039
prioritize and manage die time

00:28:08,159 --> 00:28:14,840
through the software-enabled flash api

00:28:11,039 --> 00:28:17,840
and the device enforces scheduling

00:28:14,840 --> 00:28:17,840
policy

00:28:19,120 --> 00:28:24,159
this is the basic architecture of our

00:28:21,679 --> 00:28:25,600
software enabled flash scheduler

00:28:24,159 --> 00:28:27,520
first i'm going to go down the feature

00:28:25,600 --> 00:28:30,960
list each

00:28:27,520 --> 00:28:31,840
virtual device has eight fifo command

00:28:30,960 --> 00:28:33,440
cues

00:28:31,840 --> 00:28:37,200
these are eight input cues to the

00:28:33,440 --> 00:28:40,720
scheduler and since a virtual device

00:28:37,200 --> 00:28:43,360
can be as small as a single die

00:28:40,720 --> 00:28:44,240
it can get down to a one-to-one ratio of

00:28:43,360 --> 00:28:47,679
a scheduler

00:28:44,240 --> 00:28:48,320
gateway scheduler per die the device

00:28:47,679 --> 00:28:52,159
scheduler

00:28:48,320 --> 00:28:54,080
handles all suspend resume

00:28:52,159 --> 00:28:58,320
operations for programming erase

00:28:54,080 --> 00:29:01,919
commands giving read operations priority

00:28:58,320 --> 00:29:05,600
the host can specify which of these hq's

00:29:01,919 --> 00:29:07,360
input cues are used for each type of

00:29:05,600 --> 00:29:10,399
flash access command

00:29:07,360 --> 00:29:13,120
so for example you could configure

00:29:10,399 --> 00:29:14,159
all read operations to be submitted to

00:29:13,120 --> 00:29:16,720
q0

00:29:14,159 --> 00:29:17,440
all write off or program operations

00:29:16,720 --> 00:29:20,640
going to

00:29:17,440 --> 00:29:24,240
q1 the assignment of

00:29:20,640 --> 00:29:24,960
flash operations to scheduler input

00:29:24,240 --> 00:29:28,880
queues

00:29:24,960 --> 00:29:32,159
is done on a q a per qos domain basis

00:29:28,880 --> 00:29:33,600
so one qos domain can be submitting its

00:29:32,159 --> 00:29:35,840
reads to q0

00:29:33,600 --> 00:29:38,559
a second qs domain could be submitting

00:29:35,840 --> 00:29:41,840
its reads to q2

00:29:38,559 --> 00:29:41,840
anything's possible there

00:29:41,919 --> 00:29:48,559
every q can specify die time waiting

00:29:45,600 --> 00:29:50,080
for both read operations program

00:29:48,559 --> 00:29:53,120
operations

00:29:50,080 --> 00:29:55,679
and erase commands it's a credit based

00:29:53,120 --> 00:29:58,880
system that allows us to allocate

00:29:55,679 --> 00:30:02,000
die time based on very

00:29:58,880 --> 00:30:02,799
complex and flexible needs and finally

00:30:02,000 --> 00:30:04,960
the host can

00:30:02,799 --> 00:30:06,000
override the defaults for both cue

00:30:04,960 --> 00:30:08,720
assignment

00:30:06,000 --> 00:30:10,720
and operation weight for each individual

00:30:08,720 --> 00:30:12,080
command

00:30:10,720 --> 00:30:14,159
now let's talk a little bit about the

00:30:12,080 --> 00:30:17,679
functionality we mentioned

00:30:14,159 --> 00:30:19,840
that each one of the cues can have

00:30:17,679 --> 00:30:20,720
separate weights for each type of

00:30:19,840 --> 00:30:24,320
operation

00:30:20,720 --> 00:30:27,440
an erase a program or a read if you set

00:30:24,320 --> 00:30:30,000
all the weights to zero this works

00:30:27,440 --> 00:30:31,440
as an eight level priority scheduler

00:30:30,000 --> 00:30:34,000
with

00:30:31,440 --> 00:30:37,840
input q zero being the highest priority

00:30:34,000 --> 00:30:39,600
in input q7 being the lowest priority

00:30:37,840 --> 00:30:42,320
when all the weights are the same it

00:30:39,600 --> 00:30:44,640
works as a round robin scheduler

00:30:42,320 --> 00:30:45,520
and when all the weights are unique it

00:30:44,640 --> 00:30:51,840
works as a die

00:30:45,520 --> 00:30:51,840
time weighted fair queuing scheduler

00:30:52,880 --> 00:30:56,880
and now another demo of weighted fare

00:30:56,080 --> 00:31:01,279
queuing

00:30:56,880 --> 00:31:01,279
running on our fpga prototype

00:31:02,399 --> 00:31:06,559
so some things to note we started off

00:31:05,360 --> 00:31:09,919
with qos domain

00:31:06,559 --> 00:31:13,200
1 in red having a weight of 198

00:31:09,919 --> 00:31:14,320
and qos domain 2 in blue at a weight of

00:31:13,200 --> 00:31:15,840
202.

00:31:14,320 --> 00:31:17,679
we did that so the lines wouldn't

00:31:15,840 --> 00:31:20,159
overlap each other

00:31:17,679 --> 00:31:21,919
we're now going to alter the weight for

00:31:20,159 --> 00:31:24,320
read operations

00:31:21,919 --> 00:31:26,000
giving a much lower weight for qos

00:31:24,320 --> 00:31:29,519
domain 2.

00:31:26,000 --> 00:31:32,559
this graph is a graph of read response

00:31:29,519 --> 00:31:36,640
latency and so as we lowered the weight

00:31:32,559 --> 00:31:38,320
the latency was reduced in real time

00:31:36,640 --> 00:31:40,960
now we've gone ahead and shifted the

00:31:38,320 --> 00:31:42,559
weight over to favor qs domain one and

00:31:40,960 --> 00:31:45,360
you can see

00:31:42,559 --> 00:31:46,640
the latency response curves for the two

00:31:45,360 --> 00:31:49,120
qos domains

00:31:46,640 --> 00:32:01,840
have now inverted qs domain 1 has

00:31:49,120 --> 00:32:01,840
priority over qs domain 2.

00:32:03,840 --> 00:32:09,039
i promised we'd talk about latency

00:32:05,519 --> 00:32:11,919
control and it's a really complex issue

00:32:09,039 --> 00:32:13,039
these two graphs have the same average

00:32:11,919 --> 00:32:15,600
latency

00:32:13,039 --> 00:32:18,640
but the top graph has a better minimum

00:32:15,600 --> 00:32:20,960
and a much worse maximum latency

00:32:18,640 --> 00:32:21,840
so which is the better response curve

00:32:20,960 --> 00:32:24,559
well

00:32:21,840 --> 00:32:27,120
the answer is it's a trick question it

00:32:24,559 --> 00:32:28,880
really depends on the application

00:32:27,120 --> 00:32:30,399
some applications prefer consistent

00:32:28,880 --> 00:32:31,919
latency and

00:32:30,399 --> 00:32:33,519
others need the best possible

00:32:31,919 --> 00:32:38,640
performance and can tolerate

00:32:33,519 --> 00:32:40,240
long tail latencies with this in mind

00:32:38,640 --> 00:32:42,559
and with the theme of host control

00:32:40,240 --> 00:32:45,360
software enabled flash allows control

00:32:42,559 --> 00:32:46,640
over the latency response curve we

00:32:45,360 --> 00:32:49,679
demonstrated a bit of that

00:32:46,640 --> 00:32:52,559
in the last slide we do this using many

00:32:49,679 --> 00:32:56,000
features that we've already introduced

00:32:52,559 --> 00:32:59,519
so we can isolate workloads to make

00:32:56,000 --> 00:33:02,320
sure that they do not interfere or

00:32:59,519 --> 00:33:04,640
impact the latency of other workloads

00:33:02,320 --> 00:33:07,760
through the concepts of virtual devices

00:33:04,640 --> 00:33:11,200
or qos domain scheduling

00:33:07,760 --> 00:33:14,320
we can map commands to different die

00:33:11,200 --> 00:33:17,679
cues per qs domain to once again

00:33:14,320 --> 00:33:19,919
prioritize and shape latency response

00:33:17,679 --> 00:33:22,320
we can weight the commands per key per

00:33:19,919 --> 00:33:25,440
die queue and per qos domain

00:33:22,320 --> 00:33:27,919
to prioritize the different commands

00:33:25,440 --> 00:33:28,480
for each individual command we can

00:33:27,919 --> 00:33:30,080
provide

00:33:28,480 --> 00:33:32,080
overrides so in the previous

00:33:30,080 --> 00:33:33,360
demonstration

00:33:32,080 --> 00:33:35,279
that's what we were doing we were

00:33:33,360 --> 00:33:38,320
altering

00:33:35,279 --> 00:33:40,080
in real time the weights assigned to

00:33:38,320 --> 00:33:43,360
read operations

00:33:40,080 --> 00:33:45,760
by using this override mechanism

00:33:43,360 --> 00:33:46,880
the system also provides automatic

00:33:45,760 --> 00:33:50,559
program and erase

00:33:46,880 --> 00:33:50,559
suspend and resume functionality

00:33:51,200 --> 00:33:55,279
software enabled flash also gives host

00:33:53,279 --> 00:33:56,080
control over garbage collection with the

00:33:55,279 --> 00:33:59,440
offloaded

00:33:56,080 --> 00:34:01,440
nameless copy accelerator you can

00:33:59,440 --> 00:34:03,760
prioritize

00:34:01,440 --> 00:34:05,120
and assign die times nameless copy

00:34:03,760 --> 00:34:07,519
operations separate

00:34:05,120 --> 00:34:08,560
from user read and user program

00:34:07,519 --> 00:34:10,240
operations

00:34:08,560 --> 00:34:12,399
and you can adjust the weighting to

00:34:10,240 --> 00:34:14,639
account for right amplification

00:34:12,399 --> 00:34:16,960
to give consistent latency response

00:34:14,639 --> 00:34:19,599
curves

00:34:16,960 --> 00:34:20,320
we also provide post control over wear

00:34:19,599 --> 00:34:22,960
leveling

00:34:20,320 --> 00:34:24,159
and patrol operations once again to

00:34:22,960 --> 00:34:28,800
allow

00:34:24,159 --> 00:34:32,159
the best possible response curve

00:34:28,800 --> 00:34:32,159
to the needs of the individual

00:34:32,839 --> 00:34:35,839
applications

00:34:36,320 --> 00:34:40,639
this is the final demonstration of our

00:34:38,320 --> 00:34:43,839
fpga prototype

00:34:40,639 --> 00:34:45,760
this and is just a single uh still frame

00:34:43,839 --> 00:34:47,760
taken from a live demo that we gave a

00:34:45,760 --> 00:34:49,599
while back

00:34:47,760 --> 00:34:51,280
in this demonstration we defined three

00:34:49,599 --> 00:34:53,679
separate virtual devices

00:34:51,280 --> 00:34:54,960
and they were running three different

00:34:53,679 --> 00:34:58,560
workloads

00:34:54,960 --> 00:35:00,720
with three different storage protocols

00:34:58,560 --> 00:35:01,599
in this example we were running the zns

00:35:00,720 --> 00:35:03,440
protocol

00:35:01,599 --> 00:35:05,680
a standard block protocol and a

00:35:03,440 --> 00:35:08,720
hyperscale ftl protocol

00:35:05,680 --> 00:35:12,079
in the different virtual devices

00:35:08,720 --> 00:35:16,079
the graph is a heap map of dye activity

00:35:12,079 --> 00:35:17,920
the blue bars represent read operations

00:35:16,079 --> 00:35:19,200
and the red bars represent write

00:35:17,920 --> 00:35:21,760
operations on a per

00:35:19,200 --> 00:35:23,520
die basis none of the workloads are

00:35:21,760 --> 00:35:25,040
impacting each other's

00:35:23,520 --> 00:35:27,200
and the same device is supporting the

00:35:25,040 --> 00:35:29,760
three different protocols simultaneously

00:35:27,200 --> 00:35:30,880
in this first domain which is circled

00:35:29,760 --> 00:35:34,079
the workload

00:35:30,880 --> 00:35:37,119
was a mix of reads and writes a fairly

00:35:34,079 --> 00:35:39,119
balanced workload

00:35:37,119 --> 00:35:41,599
the second domain was doing a read

00:35:39,119 --> 00:35:44,400
dominated workload

00:35:41,599 --> 00:35:47,520
and the third domain was doing a right

00:35:44,400 --> 00:35:47,520
dominated workload

00:35:49,200 --> 00:35:55,200
we really don't think that running

00:35:52,320 --> 00:35:56,400
different storage protocols

00:35:55,200 --> 00:35:59,359
simultaneously

00:35:56,400 --> 00:36:01,119
on a single device is a common use case

00:35:59,359 --> 00:36:02,160
but the value is that a single device

00:36:01,119 --> 00:36:03,839
can be deployed

00:36:02,160 --> 00:36:06,320
at scale and then provisioned and

00:36:03,839 --> 00:36:07,200
configured in real time to match dynamic

00:36:06,320 --> 00:36:09,839
needs

00:36:07,200 --> 00:36:10,640
as new storage protocols evolve software

00:36:09,839 --> 00:36:13,839
will flash

00:36:10,640 --> 00:36:13,839
and adapt quickly

00:36:18,400 --> 00:36:21,520
and now we're going to have some actual

00:36:20,240 --> 00:36:27,839
examples that are taken from the

00:36:21,520 --> 00:36:27,839
software developer kit

00:36:28,000 --> 00:36:35,119
scf cli is used to configure

00:36:31,839 --> 00:36:36,560
a software-enabled flash device it is a

00:36:35,119 --> 00:36:39,520
command line tool

00:36:36,560 --> 00:36:41,200
it's open source and included in the sdk

00:36:39,520 --> 00:36:43,200
and any of its functionality can be

00:36:41,200 --> 00:36:44,720
incorporated into an application if

00:36:43,200 --> 00:36:46,960
needed

00:36:44,720 --> 00:36:48,400
there's extensive built-in help and this

00:36:46,960 --> 00:36:51,200
is the top level of the health

00:36:48,400 --> 00:36:52,880
output in addition to allowing

00:36:51,200 --> 00:36:54,800
configuration of the device

00:36:52,880 --> 00:36:57,440
it also supports all of the api

00:36:54,800 --> 00:37:00,880
primitives you can read and write data

00:36:57,440 --> 00:37:04,000
with this command line tool if you want

00:37:00,880 --> 00:37:07,760
but probably the neatest feature

00:37:04,000 --> 00:37:10,720
of this is the built-in python shell for

00:37:07,760 --> 00:37:13,839
interactive programming of the device

00:37:10,720 --> 00:37:17,200
so in the target

00:37:13,839 --> 00:37:20,240
section of the help you'll see that

00:37:17,200 --> 00:37:22,480
the different functions supported by

00:37:20,240 --> 00:37:27,680
this cli tool

00:37:22,480 --> 00:37:30,079
are grouped by functionality so we have

00:37:27,680 --> 00:37:32,079
functions that deal with virtual devices

00:37:30,079 --> 00:37:34,160
that could be creating a virtual device

00:37:32,079 --> 00:37:36,000
listing virtual devices deleting virtual

00:37:34,160 --> 00:37:40,079
devices or changing virtual

00:37:36,000 --> 00:37:42,000
devices we have functions that apply to

00:37:40,079 --> 00:37:44,240
superblocks

00:37:42,000 --> 00:37:46,400
the shell which is the interactive

00:37:44,240 --> 00:37:50,240
python shell

00:37:46,400 --> 00:37:53,520
unit level controls

00:37:50,240 --> 00:37:54,720
the block layer controls root pointers

00:37:53,520 --> 00:37:57,200
many things that we aren't going to be

00:37:54,720 --> 00:38:00,160
able to cover in this introductory

00:37:57,200 --> 00:38:00,160
uh webinar

00:38:00,480 --> 00:38:07,520
but getting back to that python shell

00:38:03,599 --> 00:38:10,000
it's extremely useful tool for examining

00:38:07,520 --> 00:38:11,280
the device for diagnostic purposes while

00:38:10,000 --> 00:38:12,400
you're debugging your software

00:38:11,280 --> 00:38:14,960
development

00:38:12,400 --> 00:38:15,440
you can even supply python scripts to

00:38:14,960 --> 00:38:18,480
the

00:38:15,440 --> 00:38:21,280
scfcli program and have it execute those

00:38:18,480 --> 00:38:23,119
scripts for you

00:38:21,280 --> 00:38:24,800
this is really helpful once again for

00:38:23,119 --> 00:38:28,880
common debugging tasks

00:38:24,800 --> 00:38:28,880
we use this all the time in-house

00:38:29,359 --> 00:38:34,800
so now that i've introduced the sef cli

00:38:32,800 --> 00:38:35,920
program let's go over an example of its

00:38:34,800 --> 00:38:37,839
use

00:38:35,920 --> 00:38:39,520
note that in all of the coming examples

00:38:37,839 --> 00:38:39,839
there are many possible settings that

00:38:39,520 --> 00:38:43,040
aren't

00:38:39,839 --> 00:38:44,720
illustrated in the pursuit of clarity

00:38:43,040 --> 00:38:47,760
and simplicity

00:38:44,720 --> 00:38:48,640
when you don't supply an argument or

00:38:47,760 --> 00:38:50,800
possible

00:38:48,640 --> 00:38:51,760
setting there are default values

00:38:50,800 --> 00:38:55,200
specified

00:38:51,760 --> 00:38:56,400
for each parameter so let's go over how

00:38:55,200 --> 00:39:02,079
you would create

00:38:56,400 --> 00:39:05,520
a virtual device with the scf cli tool

00:39:02,079 --> 00:39:07,920
as you can see um you can specify

00:39:05,520 --> 00:39:08,720
the starting channel and the number of

00:39:07,920 --> 00:39:12,240
channels

00:39:08,720 --> 00:39:15,200
this defines the channel level

00:39:12,240 --> 00:39:16,640
parallelism of the virtual device

00:39:15,200 --> 00:39:20,480
likewise the next argument

00:39:16,640 --> 00:39:24,079
start bank and number of banks

00:39:20,480 --> 00:39:25,920
determine the number of die per channel

00:39:24,079 --> 00:39:27,680
that you've specified that will be

00:39:25,920 --> 00:39:29,599
assigned the combination

00:39:27,680 --> 00:39:31,040
of channels number of channels and

00:39:29,599 --> 00:39:33,839
number of banks

00:39:31,040 --> 00:39:36,720
defines the total number of die in the

00:39:33,839 --> 00:39:39,280
virtual device which in turn

00:39:36,720 --> 00:39:39,839
defines the maximum parallelism of the

00:39:39,280 --> 00:39:41,599
device

00:39:39,839 --> 00:39:43,359
and the size of a super block for the

00:39:41,599 --> 00:39:45,680
device

00:39:43,359 --> 00:39:47,680
when we create this virtual device we

00:39:45,680 --> 00:39:49,520
assign it a virtual device id

00:39:47,680 --> 00:39:51,920
in this example it's being set to

00:39:49,520 --> 00:39:55,119
virtual id 0

00:39:51,920 --> 00:39:58,000
and we can also set a parameter

00:39:55,119 --> 00:39:58,640
to limit the number of qos domains that

00:39:58,000 --> 00:40:00,960
can be

00:39:58,640 --> 00:40:01,760
created within this virtual device in

00:40:00,960 --> 00:40:04,560
this example

00:40:01,760 --> 00:40:06,000
we've set that to one once you've

00:40:04,560 --> 00:40:09,040
created a virtual device

00:40:06,000 --> 00:40:11,839
you can also use the sef cli

00:40:09,040 --> 00:40:13,280
command to list the virtual devices and

00:40:11,839 --> 00:40:17,359
that's illustrated

00:40:13,280 --> 00:40:17,359
at the bottom of the slide example

00:40:22,240 --> 00:40:26,640
next we're going to create a quality of

00:40:24,319 --> 00:40:29,200
service domain or a qs domain

00:40:26,640 --> 00:40:29,839
with a qs domain although it's not shown

00:40:29,200 --> 00:40:31,520
here

00:40:29,839 --> 00:40:34,240
you can specify the queue assignments

00:40:31,520 --> 00:40:38,319
for each of the flash operations

00:40:34,240 --> 00:40:42,400
so here we have told scf cli

00:40:38,319 --> 00:40:45,839
we want to create a qos domain

00:40:42,400 --> 00:40:46,800
we are creating this domain minus s0 is

00:40:45,839 --> 00:40:51,280
specifying

00:40:46,800 --> 00:40:54,560
which software enabled flash device

00:40:51,280 --> 00:40:57,599
we want to create this domain on

00:40:54,560 --> 00:41:01,280
we have to say which virtual device

00:40:57,599 --> 00:41:03,760
on that software enabled flash device

00:41:01,280 --> 00:41:06,079
we want to create this qs main in so

00:41:03,760 --> 00:41:08,319
we're going to create this qs domain in

00:41:06,079 --> 00:41:10,240
virtual device 0 the one we just created

00:41:08,319 --> 00:41:12,800
the previous example

00:41:10,240 --> 00:41:14,560
we're going to give this a qs domain an

00:41:12,800 --> 00:41:18,400
id of 2.

00:41:14,560 --> 00:41:22,480
we're going to specify the capacity

00:41:18,400 --> 00:41:22,480
of this qos domain

00:41:23,200 --> 00:41:28,800
this is always expressed in terms of

00:41:26,240 --> 00:41:30,880
atomic data units or adus and we're

00:41:28,800 --> 00:41:32,000
going to specify the size of the atomic

00:41:30,880 --> 00:41:34,960
data unit

00:41:32,000 --> 00:41:38,240
in this case we are specifying a 4k

00:41:34,960 --> 00:41:38,240
atomic data unit side

00:41:39,200 --> 00:41:43,440
the last two parameters are fairly

00:41:41,920 --> 00:41:46,880
interesting parameters

00:41:43,440 --> 00:41:46,880
number of root pointers

00:41:47,280 --> 00:41:53,440
the number of root pointers is

00:41:50,400 --> 00:41:56,480
a argument that allows you

00:41:53,440 --> 00:41:57,280
to specify how much metadata how many

00:41:56,480 --> 00:41:59,280
metadata

00:41:57,280 --> 00:42:00,319
root pointers you want to store within

00:41:59,280 --> 00:42:03,920
this device so

00:42:00,319 --> 00:42:04,560
if you were implementing a key value

00:42:03,920 --> 00:42:07,760
store

00:42:04,560 --> 00:42:10,800
with software enabled flash you may

00:42:07,760 --> 00:42:13,119
have a set of

00:42:10,800 --> 00:42:14,960
key value mappings that you need to

00:42:13,119 --> 00:42:18,400
store or persist

00:42:14,960 --> 00:42:21,359
or if you were implementing a

00:42:18,400 --> 00:42:22,640
standard ssd with software enabled flash

00:42:21,359 --> 00:42:24,079
technology

00:42:22,640 --> 00:42:26,079
you would have a lookup table for the

00:42:24,079 --> 00:42:27,920
lbas

00:42:26,079 --> 00:42:29,599
that metadata you would want to be able

00:42:27,920 --> 00:42:31,359
to store and retrieve

00:42:29,599 --> 00:42:33,280
the problem is is where do you store

00:42:31,359 --> 00:42:34,800
your metadata well our answer is you

00:42:33,280 --> 00:42:38,240
store the metadata

00:42:34,800 --> 00:42:40,560
within the qos domain itself because

00:42:38,240 --> 00:42:41,520
qos domain can be running potentially a

00:42:40,560 --> 00:42:44,240
different

00:42:41,520 --> 00:42:45,440
protocol and have a different type of

00:42:44,240 --> 00:42:48,640
metadata

00:42:45,440 --> 00:42:50,560
unique metadata in fact and that

00:42:48,640 --> 00:42:52,720
poses the problem if you store the

00:42:50,560 --> 00:42:55,359
metadata within the

00:42:52,720 --> 00:42:57,119
device itself how can you retrieve it

00:42:55,359 --> 00:42:59,040
well you need an anchor point and that's

00:42:57,119 --> 00:43:02,640
what a root pointer is

00:42:59,040 --> 00:43:06,319
it is a piece of metadata

00:43:02,640 --> 00:43:09,440
that we guarantee that you can retrieve

00:43:06,319 --> 00:43:10,240
on system initialization that can point

00:43:09,440 --> 00:43:12,720
to

00:43:10,240 --> 00:43:14,160
all of your other metadata that you need

00:43:12,720 --> 00:43:17,200
to initialize

00:43:14,160 --> 00:43:18,880
your storage application and then the

00:43:17,200 --> 00:43:20,480
number of placement ids

00:43:18,880 --> 00:43:22,079
as mentioned earlier in the nameless

00:43:20,480 --> 00:43:25,520
write discussion

00:43:22,079 --> 00:43:28,560
uh one form of nameless right is to

00:43:25,520 --> 00:43:30,480
specify that this data needs to be

00:43:28,560 --> 00:43:34,240
grouped with other data

00:43:30,480 --> 00:43:37,280
and all data with the same

00:43:34,240 --> 00:43:40,319
placement id will be put in a set

00:43:37,280 --> 00:43:42,400
of super superblocks that are actually

00:43:40,319 --> 00:43:43,040
allocated for that placement id and we

00:43:42,400 --> 00:43:46,319
will not

00:43:43,040 --> 00:43:48,000
mix data with this with different

00:43:46,319 --> 00:43:51,359
placement ids within the same super

00:43:48,000 --> 00:43:54,480
block in this case in this qs domain

00:43:51,359 --> 00:43:57,760
we're specifying that we can have four

00:43:54,480 --> 00:44:02,240
different uh placement ids active

00:43:57,760 --> 00:44:05,920
within this uh qos domain simultaneously

00:44:02,240 --> 00:44:08,800
and so that is what it takes to

00:44:05,920 --> 00:44:10,079
define a qos domain and once you've

00:44:08,800 --> 00:44:13,520
defined the qs domain

00:44:10,079 --> 00:44:15,200
you can list out all the qs domains

00:44:13,520 --> 00:44:16,720
that are present on the device with the

00:44:15,200 --> 00:44:23,119
list qos

00:44:16,720 --> 00:44:26,480
command illustrated below

00:44:23,119 --> 00:44:30,160
this next example is a nameless write

00:44:26,480 --> 00:44:34,240
and this is the minimal

00:44:30,160 --> 00:44:38,720
program necessary to form a rite of data

00:44:34,240 --> 00:44:42,480
into a previously existing qs domain

00:44:38,720 --> 00:44:45,040
so if you would use the um

00:44:42,480 --> 00:44:46,240
sef cli command to define a virtual

00:44:45,040 --> 00:44:48,480
device and then

00:44:46,240 --> 00:44:50,000
later to define a qs domain within that

00:44:48,480 --> 00:44:52,319
virtual device

00:44:50,000 --> 00:44:54,079
you could write this program simple c

00:44:52,319 --> 00:44:56,800
program

00:44:54,079 --> 00:44:56,800
the first

00:44:57,280 --> 00:45:03,920
call is to get handle

00:45:00,319 --> 00:45:04,880
on a cef unit so this says i would like

00:45:03,920 --> 00:45:08,319
to interact

00:45:04,880 --> 00:45:11,359
with this particular

00:45:08,319 --> 00:45:12,960
software-enabled flash device they are

00:45:11,359 --> 00:45:16,880
enumerated

00:45:12,960 --> 00:45:19,359
by the system automatically

00:45:16,880 --> 00:45:20,880
and of course you would need appropriate

00:45:19,359 --> 00:45:22,560
permissions to be able to open the

00:45:20,880 --> 00:45:24,400
device but once you've opened the device

00:45:22,560 --> 00:45:26,560
you get a handle to it

00:45:24,400 --> 00:45:28,079
once you have a handle to the device you

00:45:26,560 --> 00:45:31,760
can then call

00:45:28,079 --> 00:45:32,800
sef open qos domain you supply the

00:45:31,760 --> 00:45:35,599
handle to this

00:45:32,800 --> 00:45:36,960
device and the id of the qs domain you

00:45:35,599 --> 00:45:40,640
would like

00:45:36,960 --> 00:45:43,200
to open you can also optionally

00:45:40,640 --> 00:45:44,720
uh supply a notification function to get

00:45:43,200 --> 00:45:48,000
asynchronous notifications

00:45:44,720 --> 00:45:51,920
for that qos domain as well as

00:45:48,000 --> 00:45:54,079
a context which is a piece of data

00:45:51,920 --> 00:45:56,079
that we will preserve through the

00:45:54,079 --> 00:46:00,000
operations

00:45:56,079 --> 00:46:02,079
that we don't use internally at all its

00:46:00,000 --> 00:46:04,319
user context

00:46:02,079 --> 00:46:05,839
very helpful for maintaining uh

00:46:04,319 --> 00:46:09,440
asynchronous

00:46:05,839 --> 00:46:11,359
state finally key was the encryption key

00:46:09,440 --> 00:46:15,680
for the qos domain

00:46:11,359 --> 00:46:19,200
and the last argument is a

00:46:15,680 --> 00:46:21,280
address of a qos handle

00:46:19,200 --> 00:46:22,319
that will be returned by the open

00:46:21,280 --> 00:46:25,520
command

00:46:22,319 --> 00:46:29,200
so first you get a handle to

00:46:25,520 --> 00:46:31,599
the software enabled flash unit next you

00:46:29,200 --> 00:46:32,960
open a qs domain and get a handle to the

00:46:31,599 --> 00:46:35,520
qos domain

00:46:32,960 --> 00:46:36,800
and with those two pieces of information

00:46:35,520 --> 00:46:39,680
you can issue

00:46:36,800 --> 00:46:40,640
as many rights as you would like and so

00:46:39,680 --> 00:46:44,560
here is

00:46:40,640 --> 00:46:45,760
a call to sef right without physical

00:46:44,560 --> 00:46:48,079
address one

00:46:45,760 --> 00:46:49,280
it takes the qos handle that you just

00:46:48,079 --> 00:46:52,480
opened

00:46:49,280 --> 00:46:55,119
in this case we're doing auto allocate

00:46:52,480 --> 00:46:56,880
when you're doing auto allocate mode you

00:46:55,119 --> 00:46:59,359
need to supply a placement id of

00:46:56,880 --> 00:47:00,720
what placement id you want to group this

00:46:59,359 --> 00:47:04,640
data

00:47:00,720 --> 00:47:07,119
with you can supply a

00:47:04,640 --> 00:47:09,680
user address which is a piece of

00:47:07,119 --> 00:47:12,560
metadata that will be recorded along

00:47:09,680 --> 00:47:12,560
with the

00:47:14,319 --> 00:47:18,720
user data you specify the number of

00:47:17,599 --> 00:47:21,440
atomic data units

00:47:18,720 --> 00:47:22,720
you're trying to write an io vector

00:47:21,440 --> 00:47:26,240
mapping out the

00:47:22,720 --> 00:47:28,400
right buffers for the right in this case

00:47:26,240 --> 00:47:30,800
the one is a count of how many entries

00:47:28,400 --> 00:47:34,559
are in the i o vector

00:47:30,800 --> 00:47:37,440
and we will return

00:47:34,559 --> 00:47:39,200
the permanent address once again you set

00:47:37,440 --> 00:47:40,000
the bounds for where we can perform the

00:47:39,200 --> 00:47:42,800
right

00:47:40,000 --> 00:47:44,640
the device actually chooses the actual

00:47:42,800 --> 00:47:46,559
physical address and returns that to you

00:47:44,640 --> 00:47:49,839
in the permanent address field

00:47:46,559 --> 00:47:51,440
and then we also return the distance to

00:47:49,839 --> 00:47:52,000
the end of the super block in case

00:47:51,440 --> 00:47:54,400
you're doing

00:47:52,000 --> 00:47:55,440
manual allocation mode it's helpful to

00:47:54,400 --> 00:47:57,200
know when you're approaching the end of

00:47:55,440 --> 00:47:59,760
a super block and need to consider

00:47:57,200 --> 00:48:01,839
allocating another super block and the

00:47:59,760 --> 00:48:03,680
final parameter to nameless write

00:48:01,839 --> 00:48:05,119
is an override structure which allows

00:48:03,680 --> 00:48:09,119
you to override either

00:48:05,119 --> 00:48:09,119
cue placement or cue waiting

00:48:11,359 --> 00:48:17,440
i mentioned when you opened a

00:48:14,400 --> 00:48:19,200
qs domain that you could supply a

00:48:17,440 --> 00:48:20,960
notification function to handle

00:48:19,200 --> 00:48:24,160
asynchronous events

00:48:20,960 --> 00:48:26,800
there are several uh asynchronous

00:48:24,160 --> 00:48:27,599
events that a software enabled flash

00:48:26,800 --> 00:48:31,119
device

00:48:27,599 --> 00:48:32,559
can generate once again in this example

00:48:31,119 --> 00:48:36,400
we're showing that you

00:48:32,559 --> 00:48:40,559
open aqs domain and you

00:48:36,400 --> 00:48:43,119
supply a handle to the

00:48:40,559 --> 00:48:44,400
asynchronous event notification function

00:48:43,119 --> 00:48:45,760
and then beneath that

00:48:44,400 --> 00:48:48,000
we have the actual asynchronous

00:48:45,760 --> 00:48:50,559
notification function this is just a

00:48:48,000 --> 00:48:51,760
small stub of a very large switch

00:48:50,559 --> 00:48:54,319
statement

00:48:51,760 --> 00:48:56,160
uh showing some of the different types

00:48:54,319 --> 00:48:59,280
of asynchronous events

00:48:56,160 --> 00:49:02,480
there is an address update event which

00:48:59,280 --> 00:49:04,800
is used when

00:49:02,480 --> 00:49:06,079
data is moved data can be moved by the

00:49:04,800 --> 00:49:09,200
device

00:49:06,079 --> 00:49:10,880
under a number of circumstances but one

00:49:09,200 --> 00:49:14,559
of the most common

00:49:10,880 --> 00:49:17,520
would be if in fact

00:49:14,559 --> 00:49:19,520
there was a right error when performing

00:49:17,520 --> 00:49:21,599
the right to the device we could have

00:49:19,520 --> 00:49:23,520
uh decided we could have told you that

00:49:21,599 --> 00:49:25,280
we intend to place the data at one

00:49:23,520 --> 00:49:26,640
location and end up that we

00:49:25,280 --> 00:49:29,520
have to write it to an alternate

00:49:26,640 --> 00:49:32,400
location due to a media defect

00:49:29,520 --> 00:49:34,319
and the next example there is a blocked

00:49:32,400 --> 00:49:35,839
state change notification so you can

00:49:34,319 --> 00:49:39,440
tell when a block

00:49:35,839 --> 00:49:39,440
has been filled and closed

00:49:39,520 --> 00:49:43,680
there are also some capacity related

00:49:41,760 --> 00:49:47,040
events

00:49:43,680 --> 00:49:50,960
when we defined the qos domain

00:49:47,040 --> 00:49:53,119
we gave that domain a capacity quota

00:49:50,960 --> 00:49:54,640
it is possible to do thin provisioning

00:49:53,119 --> 00:49:58,720
and in those cases

00:49:54,640 --> 00:50:00,480
you can hit events or running out of

00:49:58,720 --> 00:50:01,760
space on the device if you've over

00:50:00,480 --> 00:50:07,520
committed

00:50:01,760 --> 00:50:07,520
data and needs to take action

00:50:08,640 --> 00:50:14,800
this next example is an example

00:50:12,079 --> 00:50:17,040
taken from our reference flash

00:50:14,800 --> 00:50:20,160
translation layer

00:50:17,040 --> 00:50:22,640
and it shows uh how we

00:50:20,160 --> 00:50:22,980
update the lookup tables the translation

00:50:22,640 --> 00:50:24,480
between

00:50:22,980 --> 00:50:28,160
[Music]

00:50:24,480 --> 00:50:30,640
a logical block address and a physical

00:50:28,160 --> 00:50:33,359
flash address

00:50:30,640 --> 00:50:34,800
for a block mode implementation or

00:50:33,359 --> 00:50:38,319
traditional

00:50:34,800 --> 00:50:41,760
ssd implementation or block mode driver

00:50:38,319 --> 00:50:44,960
and the important thing to note here

00:50:41,760 --> 00:50:48,160
is that uh you see once again

00:50:44,960 --> 00:50:50,960
a context which is useful for managing

00:50:48,160 --> 00:50:52,480
multiple independent flash translation

00:50:50,960 --> 00:50:54,400
layers this tells you which

00:50:52,480 --> 00:50:56,319
glass translation layer you're operating

00:50:54,400 --> 00:50:59,280
on

00:50:56,319 --> 00:51:00,000
the user logical block address the

00:50:59,280 --> 00:51:03,920
actual

00:51:00,000 --> 00:51:06,800
physical flash address and

00:51:03,920 --> 00:51:06,800
what you can see

00:51:07,200 --> 00:51:14,000
in this example is that we are using

00:51:10,559 --> 00:51:17,599
atomic exchange operations

00:51:14,000 --> 00:51:22,559
and we are updating in this example both

00:51:17,599 --> 00:51:22,559
the valid bitmap showing that this is

00:51:25,440 --> 00:51:28,880
valid data within the lookup table and

00:51:28,319 --> 00:51:34,400
then

00:51:28,880 --> 00:51:34,400
we're performing the atomic exchange

00:51:35,440 --> 00:51:38,559
on the actual lookup table itself so

00:51:37,599 --> 00:51:44,240
that we're entirely

00:51:38,559 --> 00:51:44,240
lockless upon our table insertion

00:51:47,440 --> 00:51:55,280
the next example is an address update

00:51:51,520 --> 00:51:58,880
this could happen once again if in fact

00:51:55,280 --> 00:52:01,200
we had to move data internally

00:51:58,880 --> 00:52:03,200
as a result of some internal event in

00:52:01,200 --> 00:52:05,119
the device

00:52:03,200 --> 00:52:06,400
lots of comments here but the key

00:52:05,119 --> 00:52:09,839
takeaways are

00:52:06,400 --> 00:52:12,960
that the address table

00:52:09,839 --> 00:52:16,079
update is also lockless

00:52:12,960 --> 00:52:19,520
we supply

00:52:16,079 --> 00:52:22,319
the expected flash address

00:52:19,520 --> 00:52:24,000
as well as the new flash address and

00:52:22,319 --> 00:52:27,280
this allows us to handle

00:52:24,000 --> 00:52:31,119
race conditions between data overwrites

00:52:27,280 --> 00:52:34,160
and data movement without doing

00:52:31,119 --> 00:52:35,920
any locks whatsoever in the ftl

00:52:34,160 --> 00:52:37,200
so once again the reference ftl is a

00:52:35,920 --> 00:52:40,640
very high quality

00:52:37,200 --> 00:52:40,640
high performing ftl

00:52:43,280 --> 00:52:47,839
this example is an example of a direct

00:52:46,319 --> 00:52:50,319
access read

00:52:47,839 --> 00:52:51,280
once again the housekeeping of opening

00:52:50,319 --> 00:52:54,400
up a unit

00:52:51,280 --> 00:52:56,319
opening up a qs domain and then

00:52:54,400 --> 00:52:58,480
reading the physical address by

00:52:56,319 --> 00:53:02,800
specifying the qs handle

00:52:58,480 --> 00:53:05,200
and the flash address you want to read

00:53:02,800 --> 00:53:07,520
and i think it should be fairly obvious

00:53:05,200 --> 00:53:08,000
um once you've opened up the qos domain

00:53:07,520 --> 00:53:09,760
you can

00:53:08,000 --> 00:53:12,480
issue as many read and write commands as

00:53:09,760 --> 00:53:16,240
you want you only have to open it once

00:53:12,480 --> 00:53:17,920
but those uh operations are listed for

00:53:16,240 --> 00:53:20,960
completeness here

00:53:17,920 --> 00:53:22,559
and there's also a little bit of a note

00:53:20,960 --> 00:53:23,440
here that the error recovery mode for a

00:53:22,559 --> 00:53:25,359
quest domain

00:53:23,440 --> 00:53:27,359
is set at the time of creation of the qs

00:53:25,359 --> 00:53:29,920
domain and this determines

00:53:27,359 --> 00:53:30,559
if if the software enabled flash unit

00:53:29,920 --> 00:53:32,559
will perform

00:53:30,559 --> 00:53:34,480
automatic error recovery on the qs

00:53:32,559 --> 00:53:37,680
domain and so

00:53:34,480 --> 00:53:41,280
uh you can

00:53:37,680 --> 00:53:45,200
actually set a read

00:53:41,280 --> 00:53:46,720
deadline that allows us to limit

00:53:45,200 --> 00:53:48,559
recovery attempts and this is very

00:53:46,720 --> 00:53:51,119
useful in

00:53:48,559 --> 00:53:52,160
environments where you have mirroring at

00:53:51,119 --> 00:53:55,839
the system level

00:53:52,160 --> 00:53:58,640
it's oftentimes quicker to

00:53:55,839 --> 00:54:01,040
retrieve data from a mirror upon a read

00:53:58,640 --> 00:54:06,400
error than it is to actually do the

00:54:01,040 --> 00:54:06,400
heroic recovery attempts on read retry

00:54:06,480 --> 00:54:10,960
sorry to jump in here rory but we do

00:54:08,880 --> 00:54:12,079
just have four minutes left in the

00:54:10,960 --> 00:54:15,920
presentation

00:54:12,079 --> 00:54:16,640
so um yeah feel free to continue on here

00:54:15,920 --> 00:54:18,720
thanks

00:54:16,640 --> 00:54:21,440
thanks i knew it was going to be close

00:54:18,720 --> 00:54:22,400
so this is actually the last of the api

00:54:21,440 --> 00:54:24,160
commands

00:54:22,400 --> 00:54:26,000
and this is just an illustration that

00:54:24,160 --> 00:54:27,359
all of the previous command examples i

00:54:26,000 --> 00:54:28,800
showed were synchronous

00:54:27,359 --> 00:54:31,040
command examples they all have

00:54:28,800 --> 00:54:32,480
asynchronous counterparts that take i o

00:54:31,040 --> 00:54:36,839
control blocks

00:54:32,480 --> 00:54:39,760
and uh operate asynchronously

00:54:36,839 --> 00:54:42,319
so let's get on to the summary and how

00:54:39,760 --> 00:54:45,359
to learn more

00:54:42,319 --> 00:54:46,799
so i hope you've seen that uh software

00:54:45,359 --> 00:54:50,079
enabled flash technologies

00:54:46,799 --> 00:54:53,359
is a pretty uh interesting new approach

00:54:50,079 --> 00:54:55,119
to handling flash media giving control

00:54:53,359 --> 00:54:58,480
to the host

00:54:55,119 --> 00:55:01,839
so it is purpose-built hardware

00:54:58,480 --> 00:55:03,839
it is an open source flash native api

00:55:01,839 --> 00:55:05,520
it's leveraging the industry standards

00:55:03,839 --> 00:55:08,079
wherever possible

00:55:05,520 --> 00:55:10,079
and it can be used as a building block

00:55:08,079 --> 00:55:12,240
for multiple drive protocols so

00:55:10,079 --> 00:55:14,559
we've actually demonstrated traditional

00:55:12,240 --> 00:55:16,240
block ssds built on top of software

00:55:14,559 --> 00:55:19,040
enabled flash technology

00:55:16,240 --> 00:55:20,000
zns drives built on top of it custom

00:55:19,040 --> 00:55:23,599
hyperscaler

00:55:20,000 --> 00:55:26,480
ftls uh implemented on top of it

00:55:23,599 --> 00:55:26,960
and as well we've gone flash native with

00:55:26,480 --> 00:55:30,160
several

00:55:26,960 --> 00:55:31,920
uh different applications

00:55:30,160 --> 00:55:34,000
and we're trying to combine full host

00:55:31,920 --> 00:55:37,280
control with ease of use it's a very

00:55:34,000 --> 00:55:37,280
delicate balance to strike

00:55:38,079 --> 00:55:43,520
so for more information the first thing

00:55:40,960 --> 00:55:47,680
would be to direct you to the microsite

00:55:43,520 --> 00:55:50,799
that's www.softwareenabledflash.com

00:55:47,680 --> 00:55:51,359
there you can find the white paper as

00:55:50,799 --> 00:55:54,000
well as

00:55:51,359 --> 00:55:56,640
pointers to the actual repository for

00:55:54,000 --> 00:55:59,920
the header files and api documentation

00:55:56,640 --> 00:55:59,920
that have already been published

00:56:00,160 --> 00:56:03,280
and with that i'm done with the

00:56:02,319 --> 00:56:07,760
presentation

00:56:03,280 --> 00:56:08,079
thank you awesome thank you so much rory

00:56:07,760 --> 00:56:10,319
for

00:56:08,079 --> 00:56:11,280
joining us and thank you to all of our

00:56:10,319 --> 00:56:13,200
attendees

00:56:11,280 --> 00:56:14,799
hope you enjoyed the presentation and we

00:56:13,200 --> 00:56:15,599
look forward to having you join us next

00:56:14,799 --> 00:56:22,799
time

00:56:15,599 --> 00:56:22,799

YouTube URL: https://www.youtube.com/watch?v=X5g7sO9-PqU


