Title: LF Live Webinar: High Performance Tracing in Production
Publication date: 2021-05-12
Playlist: LF Live Webinars
Description: 
	LF Live Webinar: High Performance Tracing in Production, sponsored by Capsule8

The underused suite of Linux dynamic tracing tools can be used to analyze and understand system bottlenecks regardless of what technologies the systems were developed in. This is critical in modern environments that often include an overwhelming variety of workloads, where services are written in various stacks and composed together. Join this session to investigate and examine common performance problems.
Captions: 
	00:00:02,879 --> 00:00:06,160
hi

00:00:03,600 --> 00:00:07,279
thanks for the kind intro this is high

00:00:06,160 --> 00:00:11,280
performance tracing

00:00:07,279 --> 00:00:11,280
in production let's get started

00:00:13,440 --> 00:00:17,119
uh what will we cover today i'm going to

00:00:15,839 --> 00:00:19,439
start by covering

00:00:17,119 --> 00:00:21,359
what is tracing and why might you do it

00:00:19,439 --> 00:00:23,199
on a production system

00:00:21,359 --> 00:00:24,560
some example performance problems and

00:00:23,199 --> 00:00:27,920
how you might analyze it

00:00:24,560 --> 00:00:29,760
using tracing tools more

00:00:27,920 --> 00:00:31,119
common performance problems one might

00:00:29,760 --> 00:00:33,200
encounter

00:00:31,119 --> 00:00:35,520
i will map performance problems to the

00:00:33,200 --> 00:00:37,520
appropriate observability tools

00:00:35,520 --> 00:00:39,520
and then take a quick tour of the

00:00:37,520 --> 00:00:41,200
subsystem that power these amazing

00:00:39,520 --> 00:00:44,239
pieces of software

00:00:41,200 --> 00:00:46,719
let's dig in so to start off

00:00:44,239 --> 00:00:48,719
what is tracing i like to think of it as

00:00:46,719 --> 00:00:51,039
the observation and reporting of

00:00:48,719 --> 00:00:52,480
information about a system's behavior

00:00:51,039 --> 00:00:53,920
and execution

00:00:52,480 --> 00:00:56,320
and this is really critical to

00:00:53,920 --> 00:00:58,559
understand the behavior of a system

00:00:56,320 --> 00:00:59,840
one can make an assumption about how a

00:00:58,559 --> 00:01:02,160
system might behave

00:00:59,840 --> 00:01:04,080
by testing it in prod or simulating it

00:01:02,160 --> 00:01:05,920
or even through its design

00:01:04,080 --> 00:01:09,600
but really the the best way to get to

00:01:05,920 --> 00:01:09,600
know a system is to observe it

00:01:10,000 --> 00:01:13,360
so can we safely trace and profile in

00:01:12,320 --> 00:01:15,840
production

00:01:13,360 --> 00:01:16,479
um that does sound like a dangerous

00:01:15,840 --> 00:01:19,759
premise

00:01:16,479 --> 00:01:20,560
it turns out yes and in fact you often

00:01:19,759 --> 00:01:22,720
have to

00:01:20,560 --> 00:01:24,000
uh even the most robustly constructed

00:01:22,720 --> 00:01:26,479
test environments

00:01:24,000 --> 00:01:28,479
are gonna differ uh quite quite

00:01:26,479 --> 00:01:30,720
distinctly from real workloads

00:01:28,479 --> 00:01:31,680
uh not only that real workloads change

00:01:30,720 --> 00:01:33,600
you can have

00:01:31,680 --> 00:01:35,680
traffic spikes you can have new forms of

00:01:33,600 --> 00:01:36,560
data that that weren't previously

00:01:35,680 --> 00:01:39,360
experienced

00:01:36,560 --> 00:01:42,720
and applications can misbehave in ways

00:01:39,360 --> 00:01:42,720
that are unforeseen

00:01:44,720 --> 00:01:48,479
environments often include a diverse set

00:01:46,560 --> 00:01:49,840
of software

00:01:48,479 --> 00:01:51,600
even in organizations that have

00:01:49,840 --> 00:01:54,079
standardized on common stack

00:01:51,600 --> 00:01:56,000
right enterprises are getting bigger

00:01:54,079 --> 00:01:58,880
software is powering

00:01:56,000 --> 00:02:01,119
more parts of the world and more and

00:01:58,880 --> 00:02:02,719
more businesses are defining themselves

00:02:01,119 --> 00:02:04,159
on their their software and service

00:02:02,719 --> 00:02:05,680
strategy

00:02:04,159 --> 00:02:07,520
for those of you who are familiar with

00:02:05,680 --> 00:02:08,959
the cloud native computing foundation

00:02:07,520 --> 00:02:10,640
you might notice their

00:02:08,959 --> 00:02:12,000
interactive landscape map here in the

00:02:10,640 --> 00:02:13,840
background um

00:02:12,000 --> 00:02:16,080
and this is just cloud native uh

00:02:13,840 --> 00:02:16,720
software it's it's very easy to get lost

00:02:16,080 --> 00:02:20,160
in this map

00:02:16,720 --> 00:02:22,319
and a lot of enterprise environments

00:02:20,160 --> 00:02:23,680
end up having a similar sort of like

00:02:22,319 --> 00:02:27,040
architecture with

00:02:23,680 --> 00:02:28,800
thousands of services that um no one

00:02:27,040 --> 00:02:30,879
even knows the name of all of them

00:02:28,800 --> 00:02:32,480
so it's it's important to be able to

00:02:30,879 --> 00:02:34,720
understand a system

00:02:32,480 --> 00:02:38,800
even uh you know if you aren't

00:02:34,720 --> 00:02:41,760
responsible for the details of it

00:02:38,800 --> 00:02:43,040
uh so tracing tools right uh languages

00:02:41,760 --> 00:02:46,879
have their own tracing tools

00:02:43,040 --> 00:02:48,959
right each ecosystem has their own tools

00:02:46,879 --> 00:02:50,080
that's great when you're uh sort of an

00:02:48,959 --> 00:02:52,400
expert on that

00:02:50,080 --> 00:02:53,840
but they're often inconsistent sometimes

00:02:52,400 --> 00:02:56,560
incomplete and uh

00:02:53,840 --> 00:02:58,640
may uh be unsafe to run on systems

00:02:56,560 --> 00:03:00,480
serving live traffic

00:02:58,640 --> 00:03:02,800
so those you might be familiar with

00:03:00,480 --> 00:03:05,840
would be visual vm for java

00:03:02,800 --> 00:03:08,480
g prof for c plus p prop for go

00:03:05,840 --> 00:03:10,319
there's a long list of these um and

00:03:08,480 --> 00:03:11,440
they're often pretty fantastic and they

00:03:10,319 --> 00:03:14,480
definitely have

00:03:11,440 --> 00:03:15,040
the best integration with each language

00:03:14,480 --> 00:03:17,760
family

00:03:15,040 --> 00:03:19,040
right the java tools in particular are

00:03:17,760 --> 00:03:22,319
world renowned

00:03:19,040 --> 00:03:23,840
um so they're great if you you have a

00:03:22,319 --> 00:03:24,879
deep intimate knowledge of the

00:03:23,840 --> 00:03:27,599
application

00:03:24,879 --> 00:03:28,080
um but you know if you're responsible

00:03:27,599 --> 00:03:30,400
for

00:03:28,080 --> 00:03:32,640
maintaining a lot of software many of

00:03:30,400 --> 00:03:34,000
which uses sort of a plurality of

00:03:32,640 --> 00:03:36,640
different tools

00:03:34,000 --> 00:03:38,080
or languages or ecosystems it's kind of

00:03:36,640 --> 00:03:41,760
hard to learn them all

00:03:38,080 --> 00:03:44,000
and it's often inappropriate right if

00:03:41,760 --> 00:03:45,040
if you don't know that the tracing tool

00:03:44,000 --> 00:03:46,959
well

00:03:45,040 --> 00:03:49,360
running it in production is like the

00:03:46,959 --> 00:03:51,120
last thing you you want to do right that

00:03:49,360 --> 00:03:52,799
just adds risk when you're already in a

00:03:51,120 --> 00:03:55,040
stressful situation

00:03:52,799 --> 00:03:56,400
um of course if the service is down

00:03:55,040 --> 00:03:59,280
anyway maybe there's

00:03:56,400 --> 00:04:00,560
not really that much danger to it um but

00:03:59,280 --> 00:04:04,000
there actually is a better path

00:04:00,560 --> 00:04:07,040
and that's um to use the standard set of

00:04:04,000 --> 00:04:10,560
uh perf based tracing tools that um

00:04:07,040 --> 00:04:13,040
support every workload on linux so um

00:04:10,560 --> 00:04:14,080
key points here are these tools for

00:04:13,040 --> 00:04:16,079
tracing all aspects

00:04:14,080 --> 00:04:18,000
of the system not just the application

00:04:16,079 --> 00:04:20,479
layer so if something is misbehaving

00:04:18,000 --> 00:04:21,040
on disk or in kernel or on the network

00:04:20,479 --> 00:04:24,240
layer

00:04:21,040 --> 00:04:26,639
uh there's probably a tool for it um

00:04:24,240 --> 00:04:28,000
and often you get better visibility into

00:04:26,639 --> 00:04:31,040
the application layer than

00:04:28,000 --> 00:04:33,440
language ecosystem zone tools these

00:04:31,040 --> 00:04:36,639
tools are focused on

00:04:33,440 --> 00:04:38,639
providing visibility and that's that's

00:04:36,639 --> 00:04:40,320
something they do well

00:04:38,639 --> 00:04:42,880
most of them are safe to run on live

00:04:40,320 --> 00:04:45,759
systems use only a little cpu in memory

00:04:42,880 --> 00:04:47,520
um this is not the case for many

00:04:45,759 --> 00:04:49,199
language specific tools which

00:04:47,520 --> 00:04:50,800
you know have deep hooks into the

00:04:49,199 --> 00:04:53,120
language runtimes that

00:04:50,800 --> 00:04:54,400
uh you know maybe slow it down by a

00:04:53,120 --> 00:04:55,840
factor of two or four

00:04:54,400 --> 00:04:58,080
and that's just something you can't

00:04:55,840 --> 00:05:00,639
really afford to to run in

00:04:58,080 --> 00:05:01,919
production and importantly and this is

00:05:00,639 --> 00:05:04,800
this is from an operational

00:05:01,919 --> 00:05:05,280
learning perspective um it's one tool

00:05:04,800 --> 00:05:07,520
set

00:05:05,280 --> 00:05:11,039
that you can use on all the services

00:05:07,520 --> 00:05:12,720
that an organization might run

00:05:11,039 --> 00:05:14,240
so i like to think of perf events as

00:05:12,720 --> 00:05:18,560
like the one

00:05:14,240 --> 00:05:18,560
ring buffer to to rule them all

00:05:20,000 --> 00:05:23,199
so let's dive into a sample performance

00:05:22,560 --> 00:05:26,320
problem

00:05:23,199 --> 00:05:29,440
uh we've got here and i'm going to demo

00:05:26,320 --> 00:05:30,639
a simple web application that accepts

00:05:29,440 --> 00:05:33,680
requests

00:05:30,639 --> 00:05:35,600
it logs the request details or it logs

00:05:33,680 --> 00:05:36,880
the request details and then returns a

00:05:35,600 --> 00:05:39,919
response

00:05:36,880 --> 00:05:42,240
um and this application is misbehaving

00:05:39,919 --> 00:05:43,840
right it's not behaving as it could um

00:05:42,240 --> 00:05:45,680
it has hit a limit on the number of

00:05:43,840 --> 00:05:47,520
requests it processes

00:05:45,680 --> 00:05:49,039
but it's not using all the resources

00:05:47,520 --> 00:05:50,000
assigned to it it's not using all the

00:05:49,039 --> 00:05:53,360
cpu

00:05:50,000 --> 00:05:53,680
network or disk resources um and this is

00:05:53,360 --> 00:05:56,479
a

00:05:53,680 --> 00:05:57,919
a typical performance problem for which

00:05:56,479 --> 00:06:01,120
there's no like

00:05:57,919 --> 00:06:01,919
obvious cause right it's pretty obvious

00:06:01,120 --> 00:06:04,160
like if

00:06:01,919 --> 00:06:05,440
traffic grows and you end up saturating

00:06:04,160 --> 00:06:07,680
your network link

00:06:05,440 --> 00:06:09,360
you might need another replica of the

00:06:07,680 --> 00:06:13,280
service but in this case there's no

00:06:09,360 --> 00:06:15,360
obvious like problem with it it's just

00:06:13,280 --> 00:06:17,680
not using all its resources and it's hit

00:06:15,360 --> 00:06:17,680
a wall

00:06:19,120 --> 00:06:22,240
so what i'm going to do is i'm going to

00:06:20,639 --> 00:06:23,919
profile a

00:06:22,240 --> 00:06:25,600
a running application so i'm going to

00:06:23,919 --> 00:06:28,800
start the application

00:06:25,600 --> 00:06:31,360
i'm going to run apaches apache bench

00:06:28,800 --> 00:06:33,600
command to simulate incoming traffic

00:06:31,360 --> 00:06:35,360
i'm doing this because i actually don't

00:06:33,600 --> 00:06:36,400
have this this environment running in a

00:06:35,360 --> 00:06:38,319
production environment

00:06:36,400 --> 00:06:40,639
and it would be sort of like a little

00:06:38,319 --> 00:06:41,840
reckless to demo a real production

00:06:40,639 --> 00:06:43,840
environment on

00:06:41,840 --> 00:06:45,520
on a webinar i'm going to run the

00:06:43,840 --> 00:06:47,600
profile profiler against it

00:06:45,520 --> 00:06:49,280
uh to see what the application is

00:06:47,600 --> 00:06:52,000
actively working on

00:06:49,280 --> 00:06:53,680
uh i will run the the wake up time

00:06:52,000 --> 00:06:55,919
profiler against it to see what the

00:06:53,680 --> 00:06:57,840
application is is waiting on

00:06:55,919 --> 00:06:59,919
um i will convert the outputs to flame

00:06:57,840 --> 00:07:02,400
graphs using a handy little

00:06:59,919 --> 00:07:03,520
little frame flame graph script uh and

00:07:02,400 --> 00:07:05,440
examine the contents

00:07:03,520 --> 00:07:07,440
and then we'll discover that actually

00:07:05,440 --> 00:07:10,000
it's the logging system that's

00:07:07,440 --> 00:07:10,720
syncing each right to disk and that

00:07:10,000 --> 00:07:14,080
requires

00:07:10,720 --> 00:07:16,720
the disk controller to to respond with

00:07:14,080 --> 00:07:19,280
yes i have definitely let it hit the the

00:07:16,720 --> 00:07:22,800
platters or the nvme flash

00:07:19,280 --> 00:07:24,160
as uh neat maybe um so importantly this

00:07:22,800 --> 00:07:26,800
is not a real environment

00:07:24,160 --> 00:07:27,360
um but it's adequate and enough to

00:07:26,800 --> 00:07:29,280
simulate

00:07:27,360 --> 00:07:31,840
and and show and explore the use of the

00:07:29,280 --> 00:07:34,960
tools

00:07:31,840 --> 00:07:38,639
so i have pre-recorded the demo

00:07:34,960 --> 00:07:41,520
just so that the demo gods will

00:07:38,639 --> 00:07:44,319
look favorably on me and we will get

00:07:41,520 --> 00:07:46,000
started with the video here

00:07:44,319 --> 00:07:47,919
so you can see here we're going to start

00:07:46,000 --> 00:07:49,120
the application and it's now listening

00:07:47,919 --> 00:07:52,960
on port

00:07:49,120 --> 00:07:55,440
8090 and we're going to run just

00:07:52,960 --> 00:07:56,319
apache bench command against it and it's

00:07:55,440 --> 00:07:58,639
now running

00:07:56,319 --> 00:08:00,560
and you'll note here that it is not

00:07:58,639 --> 00:08:02,000
consuming all the cpu

00:08:00,560 --> 00:08:03,759
right this is all running locally so

00:08:02,000 --> 00:08:04,879
there shouldn't be a real like network

00:08:03,759 --> 00:08:07,280
bottleneck

00:08:04,879 --> 00:08:09,039
um you know but we've got we've got

00:08:07,280 --> 00:08:11,599
plenty of spare cpu and it's

00:08:09,039 --> 00:08:13,280
it's not using it and that's that's not

00:08:11,599 --> 00:08:14,639
great

00:08:13,280 --> 00:08:16,960
so we're going to run the profiler

00:08:14,639 --> 00:08:18,240
against it and this is the the profile

00:08:16,960 --> 00:08:22,160
script out of the

00:08:18,240 --> 00:08:23,680
bcc collection of tools from i o visor

00:08:22,160 --> 00:08:25,199
and it has gone and captured and

00:08:23,680 --> 00:08:29,120
generated this file from

00:08:25,199 --> 00:08:31,919
10 seconds of activity um so you'll note

00:08:29,120 --> 00:08:34,080
entire time the app is running uh you

00:08:31,919 --> 00:08:36,719
know like cpu usage barely

00:08:34,080 --> 00:08:38,159
budged for that that profiler tool and

00:08:36,719 --> 00:08:39,599
we're going to run it through our

00:08:38,159 --> 00:08:41,360
our little generation script that

00:08:39,599 --> 00:08:42,399
produces a flame graph so this is great

00:08:41,360 --> 00:08:44,240
we can run this on

00:08:42,399 --> 00:08:46,080
any software and get an assessment of

00:08:44,240 --> 00:08:48,880
what it's actively working on

00:08:46,080 --> 00:08:50,000
including the behavior inside the kernel

00:08:48,880 --> 00:08:52,959
and that's the

00:08:50,000 --> 00:08:54,080
the bits above the the dashes so

00:08:52,959 --> 00:08:57,600
anything above

00:08:54,080 --> 00:08:59,120
the the gray dash boxes that is actually

00:08:57,600 --> 00:09:00,720
code running in the kernel so you can

00:08:59,120 --> 00:09:05,440
trace all of the

00:09:00,720 --> 00:09:07,519
usage of cpu right through to the kernel

00:09:05,440 --> 00:09:08,880
next up we're going to run the off wait

00:09:07,519 --> 00:09:11,360
time uh

00:09:08,880 --> 00:09:12,080
off wake time profiler and what this is

00:09:11,360 --> 00:09:14,320
gonna do

00:09:12,080 --> 00:09:15,440
is determine for each of these these

00:09:14,320 --> 00:09:17,360
things that the

00:09:15,440 --> 00:09:19,120
the program is waiting on why are they

00:09:17,360 --> 00:09:20,800
waiting and what makes them up

00:09:19,120 --> 00:09:23,279
um so we're gonna do the same thing run

00:09:20,800 --> 00:09:25,519
it for 10 seconds

00:09:23,279 --> 00:09:27,120
as it's processing traffic and

00:09:25,519 --> 00:09:29,839
processing our benchmark

00:09:27,120 --> 00:09:32,000
um you can see here um it's actually

00:09:29,839 --> 00:09:34,000
lost a few stack traces and that's fine

00:09:32,000 --> 00:09:36,080
and that's that's part of the lossy

00:09:34,000 --> 00:09:38,160
nature of data collection that lets

00:09:36,080 --> 00:09:39,760
these subsystems run safely in

00:09:38,160 --> 00:09:42,240
production if

00:09:39,760 --> 00:09:43,360
you know they ever run out of resources

00:09:42,240 --> 00:09:45,120
you know we'd rather

00:09:43,360 --> 00:09:46,399
the application go forward and we can

00:09:45,120 --> 00:09:47,839
see here

00:09:46,399 --> 00:09:49,279
we actually have a better view we can

00:09:47,839 --> 00:09:50,480
see that most of the time the

00:09:49,279 --> 00:09:52,640
application is waiting and we're going

00:09:50,480 --> 00:09:53,839
to zoom in on our main function that

00:09:52,640 --> 00:09:55,920
handles data

00:09:53,839 --> 00:09:57,120
it's actually waiting on this sync call

00:09:55,920 --> 00:09:59,360
right this is

00:09:57,120 --> 00:10:00,480
this is not good you don't want your

00:09:59,360 --> 00:10:02,880
your application on

00:10:00,480 --> 00:10:04,320
spending all of its time waiting for

00:10:02,880 --> 00:10:07,040
disk to complete

00:10:04,320 --> 00:10:08,480
um and we can actually see even what has

00:10:07,040 --> 00:10:11,360
woken it up which is the

00:10:08,480 --> 00:10:14,240
the completeness notification interrupt

00:10:11,360 --> 00:10:16,079
from the nvme disk

00:10:14,240 --> 00:10:18,079
so that's great we've now determined

00:10:16,079 --> 00:10:20,880
what the source of this problem is

00:10:18,079 --> 00:10:22,120
it's an unnecessary sync call it's only

00:10:20,880 --> 00:10:24,959
processing

00:10:22,120 --> 00:10:26,160
1337 requests per second which is not

00:10:24,959 --> 00:10:28,320
very leaked

00:10:26,160 --> 00:10:30,480
um so we're going to go in here and and

00:10:28,320 --> 00:10:34,000
fix it we're going to find that sinkhole

00:10:30,480 --> 00:10:34,000
and we're going to get rid of the sync

00:10:36,079 --> 00:10:39,200
let's go rebuild the software start the

00:10:38,079 --> 00:10:42,240
service again

00:10:39,200 --> 00:10:44,560
um and we will run the benchmark again

00:10:42,240 --> 00:10:46,240
and let's see how much resources is this

00:10:44,560 --> 00:10:48,480
application using

00:10:46,240 --> 00:10:49,440
excellent that's much better it's now

00:10:48,480 --> 00:10:52,720
using

00:10:49,440 --> 00:10:56,480
uh close to 100 cpu with

00:10:52,720 --> 00:10:59,040
uh some cpu for uh the the benchmark

00:10:56,480 --> 00:11:00,000
right um so that's great that's much

00:10:59,040 --> 00:11:03,200
better

00:11:00,000 --> 00:11:03,680
much better cpu usage we're gonna now

00:11:03,200 --> 00:11:15,839
run

00:11:03,680 --> 00:11:15,839
our our profile again

00:11:16,240 --> 00:11:20,800
and we're going to load it up and see

00:11:17,760 --> 00:11:23,680
actually now the application is spending

00:11:20,800 --> 00:11:24,480
uh most of its time actually responding

00:11:23,680 --> 00:11:26,240
to requests

00:11:24,480 --> 00:11:27,760
this is sort of the healthy behavior you

00:11:26,240 --> 00:11:29,360
want to see from

00:11:27,760 --> 00:11:31,519
at least this application a little bit

00:11:29,360 --> 00:11:34,480
of time spent writing to the file

00:11:31,519 --> 00:11:34,959
a lot of time spent uh responding to the

00:11:34,480 --> 00:11:38,399
socket

00:11:34,959 --> 00:11:40,720
and processing the the http request

00:11:38,399 --> 00:11:44,000
and we can go back to the uh off wake

00:11:40,720 --> 00:11:57,839
time profiler as well

00:11:44,000 --> 00:11:57,839
and and see what it ends up waiting on

00:11:59,440 --> 00:12:04,000
and we're gonna wait for this this

00:12:00,720 --> 00:12:04,000
benchmark to finish first

00:12:07,600 --> 00:12:11,680
and that benchmark number is much better

00:12:09,839 --> 00:12:13,760
it's it's

00:12:11,680 --> 00:12:16,000
roughly 10 times as many requests per

00:12:13,760 --> 00:12:19,839
second which is fantastic

00:12:16,000 --> 00:12:19,839
so let's capture that off wake time

00:12:22,839 --> 00:12:25,839
profile

00:12:31,279 --> 00:12:38,240
and we can open it up here generate our

00:12:33,279 --> 00:12:40,320
flame graph

00:12:38,240 --> 00:12:41,600
and excellent we can see now there's

00:12:40,320 --> 00:12:44,079
hardly any waiting

00:12:41,600 --> 00:12:44,880
it's only internal coordination by adobe

00:12:44,079 --> 00:12:48,480
runtime

00:12:44,880 --> 00:12:50,480
it happens to be a go program um

00:12:48,480 --> 00:12:52,800
but that is a much healthier profiler

00:12:50,480 --> 00:12:55,519
and you can see we're using making much

00:12:52,800 --> 00:12:57,680
better use of resources on this machine

00:12:55,519 --> 00:12:58,959
great now let's dive in and actually

00:12:57,680 --> 00:13:01,920
discover during this

00:12:58,959 --> 00:13:02,800
what the developer meant to do um is

00:13:01,920 --> 00:13:05,680
lock

00:13:02,800 --> 00:13:07,279
mutex instead of syncing disk because we

00:13:05,680 --> 00:13:09,200
want log messages to

00:13:07,279 --> 00:13:12,720
not be interleaved with each other so

00:13:09,200 --> 00:13:12,720
we're gonna go fix the bug properly

00:13:13,040 --> 00:13:16,079
run the service again

00:13:16,800 --> 00:13:23,120
double check and make sure that it

00:13:19,279 --> 00:13:23,120
appropriately uses resources

00:13:23,279 --> 00:13:26,320
and see that it does

00:13:29,600 --> 00:13:34,639
and this is a quick example of how one

00:13:32,320 --> 00:13:37,839
might use some of the profiling tools

00:13:34,639 --> 00:13:49,600
to trace through the activity of

00:13:37,839 --> 00:13:52,560
real live running software

00:13:49,600 --> 00:13:55,120
excellent i'm going to jump back now to

00:13:52,560 --> 00:13:55,120
the slides

00:14:01,199 --> 00:14:06,320
so a quick recap of what we did during

00:14:02,959 --> 00:14:06,320
the demo um

00:14:07,120 --> 00:14:10,320
profilers surface relevant information

00:14:09,680 --> 00:14:12,560
on

00:14:10,320 --> 00:14:13,920
the application the system and the

00:14:12,560 --> 00:14:15,199
kernel behavior

00:14:13,920 --> 00:14:17,040
and this is really important because it

00:14:15,199 --> 00:14:18,959
gives a holistic view

00:14:17,040 --> 00:14:20,880
of what was going on in the system and

00:14:18,959 --> 00:14:22,959
that is what we saw in the demo

00:14:20,880 --> 00:14:25,120
the application didn't need to be run in

00:14:22,959 --> 00:14:27,199
a special mode or be restarted

00:14:25,120 --> 00:14:28,639
um we could just jump onto a live system

00:14:27,199 --> 00:14:30,399
as it's misbehaving

00:14:28,639 --> 00:14:33,600
run the profiling tools and get

00:14:30,399 --> 00:14:36,240
information on what is going on

00:14:33,600 --> 00:14:38,560
performance overhead was very low if the

00:14:36,240 --> 00:14:40,079
tools are already installed

00:14:38,560 --> 00:14:42,000
you can run this on a heavily loaded

00:14:40,079 --> 00:14:43,040
system sometimes even you know if the

00:14:42,000 --> 00:14:44,720
system is so

00:14:43,040 --> 00:14:46,720
overloaded that it can't even write to

00:14:44,720 --> 00:14:48,720
disk um

00:14:46,720 --> 00:14:49,760
that can be a problem so i highly

00:14:48,720 --> 00:14:51,839
recommend

00:14:49,760 --> 00:14:54,000
installing the tools by default as part

00:14:51,839 --> 00:14:56,560
of a production

00:14:54,000 --> 00:14:58,639
deploy right you should know what type

00:14:56,560 --> 00:14:59,760
of observability what type of visibility

00:14:58,639 --> 00:15:02,639
you're going to want

00:14:59,760 --> 00:15:04,639
to debug a system before you've deployed

00:15:02,639 --> 00:15:06,160
it

00:15:04,639 --> 00:15:07,680
and one important thing to note is the

00:15:06,160 --> 00:15:09,120
data is lossy

00:15:07,680 --> 00:15:10,800
and that is to avoid introducing

00:15:09,120 --> 00:15:12,560
bottlenecks on the running system

00:15:10,800 --> 00:15:13,839
we did lose a little bit of data

00:15:12,560 --> 00:15:16,079
collection

00:15:13,839 --> 00:15:17,600
but that didn't impede us from tracing

00:15:16,079 --> 00:15:18,639
and understanding the behavior of the

00:15:17,600 --> 00:15:21,040
system

00:15:18,639 --> 00:15:22,800
and often incomplete information is

00:15:21,040 --> 00:15:26,160
enough to make really good decisions

00:15:22,800 --> 00:15:29,360
anyways and lastly

00:15:26,160 --> 00:15:32,480
uh flame graphs they're really powerful

00:15:29,360 --> 00:15:33,839
um they're a summarization of uh in the

00:15:32,480 --> 00:15:36,560
example we looked at

00:15:33,839 --> 00:15:37,759
uh hundreds of thousands of data points

00:15:36,560 --> 00:15:40,399
organized into a

00:15:37,759 --> 00:15:41,040
graph that is very easy for humans to

00:15:40,399 --> 00:15:42,959
understand

00:15:41,040 --> 00:15:44,880
and it's it's a really good sort of like

00:15:42,959 --> 00:15:48,399
interchange format between

00:15:44,880 --> 00:15:50,240
the machine and the human and

00:15:48,399 --> 00:15:52,000
i found that organizing data into this

00:15:50,240 --> 00:15:53,920
format is very useful

00:15:52,000 --> 00:15:56,959
at least when it comes to tracing and

00:15:53,920 --> 00:15:56,959
performance analysis

00:15:59,040 --> 00:16:02,240
so some other common performance

00:16:00,399 --> 00:16:04,240
problems

00:16:02,240 --> 00:16:06,079
many of you may have seen these before

00:16:04,240 --> 00:16:08,480
high application cpu usage

00:16:06,079 --> 00:16:09,680
kernel cpu usage saturated network

00:16:08,480 --> 00:16:12,480
bandwidth

00:16:09,680 --> 00:16:13,040
high memory usage or swapping um that's

00:16:12,480 --> 00:16:16,560
often

00:16:13,040 --> 00:16:18,399
the the end of your production up time

00:16:16,560 --> 00:16:21,759
is when your application swaps

00:16:18,399 --> 00:16:24,320
poor disk io or file cache behavior

00:16:21,759 --> 00:16:25,600
long tail response latency this one is

00:16:24,320 --> 00:16:27,519
often

00:16:25,600 --> 00:16:29,120
very difficult to trace down and then of

00:16:27,519 --> 00:16:32,480
course good old system

00:16:29,120 --> 00:16:32,480
or memory contention

00:16:33,279 --> 00:16:36,720
and if we were to take a look at sort of

00:16:34,720 --> 00:16:40,399
like the high level view of

00:16:36,720 --> 00:16:42,240
of a system the application is actually

00:16:40,399 --> 00:16:45,040
just one small part of it

00:16:42,240 --> 00:16:45,680
right and there's the hardware the

00:16:45,040 --> 00:16:47,279
software

00:16:45,680 --> 00:16:48,959
all the different layers in between the

00:16:47,279 --> 00:16:53,680
kernel actually plays a huge

00:16:48,959 --> 00:16:53,680
part in the operation of software

00:16:53,839 --> 00:16:59,199
and linux has tracing tools for pretty

00:16:56,079 --> 00:17:00,720
much all parts of the system

00:16:59,199 --> 00:17:02,240
any part of the system there's probably

00:17:00,720 --> 00:17:04,959
some basic tool

00:17:02,240 --> 00:17:05,839
to analyze data collect data uh give

00:17:04,959 --> 00:17:08,559
visibility

00:17:05,839 --> 00:17:10,000
into what is going on and maybe even

00:17:08,559 --> 00:17:13,039
summarize it in a way that

00:17:10,000 --> 00:17:15,439
is accessible to humans um

00:17:13,039 --> 00:17:16,319
even things like like fans and power

00:17:15,439 --> 00:17:18,319
supplies

00:17:16,319 --> 00:17:20,559
you know have probes in them and can

00:17:18,319 --> 00:17:21,520
access that data via linux tracing tools

00:17:20,559 --> 00:17:23,199
which

00:17:21,520 --> 00:17:25,679
when i first saw this chart i was i was

00:17:23,199 --> 00:17:28,400
really amazed

00:17:25,679 --> 00:17:29,360
there's tools basically for everything

00:17:28,400 --> 00:17:32,559
and

00:17:29,360 --> 00:17:34,080
should get to know them so let's take

00:17:32,559 --> 00:17:36,799
some mapping of those

00:17:34,080 --> 00:17:38,480
performance problems we had before uh to

00:17:36,799 --> 00:17:41,360
some of the tools we have here

00:17:38,480 --> 00:17:42,080
so the cpu usage we demoed this during

00:17:41,360 --> 00:17:45,600
the

00:17:42,080 --> 00:17:47,440
uh during the demo uh we've got the

00:17:45,600 --> 00:17:48,640
the main profile too and we've got dash

00:17:47,440 --> 00:17:51,520
u for user lan and

00:17:48,640 --> 00:17:53,440
k for kernel for network bandwidth we

00:17:51,520 --> 00:17:55,280
can look at tcp top

00:17:53,440 --> 00:17:56,559
for high memory usage and swapping we've

00:17:55,280 --> 00:18:00,480
got mem leak and

00:17:56,559 --> 00:18:02,240
swap in for disk i o or file cache

00:18:00,480 --> 00:18:04,000
behavior we've got vfs stat

00:18:02,240 --> 00:18:06,640
that looks at the virtual file system

00:18:04,000 --> 00:18:08,320
layer in linux cache stat

00:18:06,640 --> 00:18:10,559
which looks at the caching behavior and

00:18:08,320 --> 00:18:13,760
of course file top

00:18:10,559 --> 00:18:15,520
we've got long long tail response

00:18:13,760 --> 00:18:18,640
latency we've got tcp live

00:18:15,520 --> 00:18:20,000
tcp accept and latency top

00:18:18,640 --> 00:18:21,840
there's a good number of other tools

00:18:20,000 --> 00:18:23,360
here because the network system is very

00:18:21,840 --> 00:18:25,520
complicated

00:18:23,360 --> 00:18:26,720
so definitely i recommend more

00:18:25,520 --> 00:18:28,880
exploration there

00:18:26,720 --> 00:18:29,840
and then system or memory contention

00:18:28,880 --> 00:18:32,400
we've got

00:18:29,840 --> 00:18:33,520
off wake time and cpu dist and this is

00:18:32,400 --> 00:18:36,640
just a

00:18:33,520 --> 00:18:40,080
little snack preview of

00:18:36,640 --> 00:18:43,360
all of the tracing and analysis tools

00:18:40,080 --> 00:18:46,720
that are available on linux

00:18:43,360 --> 00:18:49,039
so how do they work um the the

00:18:46,720 --> 00:18:50,559
there's a few different approaches to to

00:18:49,039 --> 00:18:53,919
to how they gather data and

00:18:50,559 --> 00:18:55,760
how that all fits together um and the

00:18:53,919 --> 00:18:59,120
mechanisms they use are sample based

00:18:55,760 --> 00:19:01,760
profiling dynamic instrumentation

00:18:59,120 --> 00:19:02,960
in kernel virtual machines which many of

00:19:01,760 --> 00:19:05,760
you may know as

00:19:02,960 --> 00:19:10,000
evpf and of course asynchronous data

00:19:05,760 --> 00:19:12,720
communication with userland

00:19:10,000 --> 00:19:14,400
so sample based profiling is when you

00:19:12,720 --> 00:19:16,720
interrupt the system on an interval

00:19:14,400 --> 00:19:18,720
and just take a snapshot of what it's

00:19:16,720 --> 00:19:21,120
doing um

00:19:18,720 --> 00:19:22,160
and this has pretty low overhead um it

00:19:21,120 --> 00:19:24,000
turns out that

00:19:22,160 --> 00:19:25,760
the system has to be interrupted anyways

00:19:24,000 --> 00:19:27,919
to do scheduling so this is this is what

00:19:25,760 --> 00:19:29,280
the linux scheduler does it time slices

00:19:27,919 --> 00:19:32,240
of cpu

00:19:29,280 --> 00:19:33,760
and distributes that cpu to all the

00:19:32,240 --> 00:19:34,480
different processes that are running on

00:19:33,760 --> 00:19:36,880
the system

00:19:34,480 --> 00:19:38,320
so during that interruption you can

00:19:36,880 --> 00:19:39,520
actually take a snapshot of what the

00:19:38,320 --> 00:19:43,520
system is doing and

00:19:39,520 --> 00:19:47,039
count samples and perform behavior there

00:19:43,520 --> 00:19:48,720
importantly this is great because um

00:19:47,039 --> 00:19:50,240
it's even right it doesn't skew the

00:19:48,720 --> 00:19:52,480
measurement results

00:19:50,240 --> 00:19:54,160
but it is fundamentally a sample based

00:19:52,480 --> 00:19:56,000
mechanism so if there is like

00:19:54,160 --> 00:19:57,760
long tail events that you want to look

00:19:56,000 --> 00:19:58,240
at or rare events or if you want to

00:19:57,760 --> 00:20:01,200
capture

00:19:58,240 --> 00:20:03,039
every activity um it's just not possible

00:20:01,200 --> 00:20:05,360
to do that because you're only sampling

00:20:03,039 --> 00:20:08,480
the signal

00:20:05,360 --> 00:20:09,679
and on perf um this is done via the the

00:20:08,480 --> 00:20:13,440
kernel scheduler

00:20:09,679 --> 00:20:14,720
um and it wakes up does the scheduling

00:20:13,440 --> 00:20:17,200
behavior

00:20:14,720 --> 00:20:18,640
annotates what was running um and drops

00:20:17,200 --> 00:20:21,120
the samples into

00:20:18,640 --> 00:20:22,559
uh what is known as a a perf buffer

00:20:21,120 --> 00:20:23,440
right so this is the buffer that's

00:20:22,559 --> 00:20:25,679
shared between

00:20:23,440 --> 00:20:27,840
the tracing tool and the kernel and the

00:20:25,679 --> 00:20:31,280
kernel will record information

00:20:27,840 --> 00:20:31,280
that the tracing tool can pick up

00:20:32,320 --> 00:20:36,159
dynamic instrumentation is the other

00:20:34,400 --> 00:20:39,039
approach and this is to

00:20:36,159 --> 00:20:39,919
hot patch the code um with calls into a

00:20:39,039 --> 00:20:42,880
handler

00:20:39,919 --> 00:20:45,600
that record when an operation occurs um

00:20:42,880 --> 00:20:47,919
so this introduces a small overhead

00:20:45,600 --> 00:20:49,840
but only on the parts of the system

00:20:47,919 --> 00:20:51,760
actually being instrumented

00:20:49,840 --> 00:20:53,440
and so this is very useful to be able to

00:20:51,760 --> 00:20:55,760
go and get

00:20:53,440 --> 00:20:57,760
you know either long tail access or to

00:20:55,760 --> 00:21:00,080
record every activity and get

00:20:57,760 --> 00:21:00,880
a full view of the system this does

00:21:00,080 --> 00:21:02,559
create

00:21:00,880 --> 00:21:05,039
some bias in the data since the

00:21:02,559 --> 00:21:07,760
measurement is unevenly distributed

00:21:05,039 --> 00:21:08,320
but um often that bias can be corrected

00:21:07,760 --> 00:21:10,320
for

00:21:08,320 --> 00:21:12,480
or it doesn't even matter for the the

00:21:10,320 --> 00:21:15,360
type of data insight that

00:21:12,480 --> 00:21:16,000
you're trying to get an assessment of so

00:21:15,360 --> 00:21:18,720
the

00:21:16,000 --> 00:21:20,720
linux kernel and perf has uh

00:21:18,720 --> 00:21:23,520
standardized safe facilities

00:21:20,720 --> 00:21:24,640
um that all tracing tools use and are

00:21:23,520 --> 00:21:26,559
also used in other

00:21:24,640 --> 00:21:27,840
subsystems of the kernel as well and so

00:21:26,559 --> 00:21:30,159
that is trace points

00:21:27,840 --> 00:21:32,159
uh trace points are dynamic

00:21:30,159 --> 00:21:34,720
instrumentation points that are

00:21:32,159 --> 00:21:36,400
uh placed in predefined operations in

00:21:34,720 --> 00:21:39,679
the kernel so things like

00:21:36,400 --> 00:21:41,600
forking or executing or opening a file

00:21:39,679 --> 00:21:42,880
might have a trace point annotated in

00:21:41,600 --> 00:21:46,400
the kernel that

00:21:42,880 --> 00:21:49,760
probes can be attached to k-probes

00:21:46,400 --> 00:21:52,640
are the safe dynamic patching of

00:21:49,760 --> 00:21:54,559
arbitrary kernel functions so almost

00:21:52,640 --> 00:21:56,880
every function in the kernel can have a

00:21:54,559 --> 00:22:00,240
k-probe attached to it

00:21:56,880 --> 00:22:01,039
that can then trap and um record data

00:22:00,240 --> 00:22:03,280
that can be

00:22:01,039 --> 00:22:05,600
uh picked up by the tracing tool and

00:22:03,280 --> 00:22:07,360
then you probes which is the same sort

00:22:05,600 --> 00:22:09,600
of dynamic patching behavior

00:22:07,360 --> 00:22:10,559
uh but that occurs in user land so you

00:22:09,600 --> 00:22:14,000
can trace

00:22:10,559 --> 00:22:17,520
the inner workings of userland programs

00:22:14,000 --> 00:22:19,360
as they are executed and just like the

00:22:17,520 --> 00:22:21,039
sample-based profiling

00:22:19,360 --> 00:22:23,039
data on these operations are written

00:22:21,039 --> 00:22:24,320
into these perf buffers that are shared

00:22:23,039 --> 00:22:27,840
with and read by the

00:22:24,320 --> 00:22:27,840
the tracing tool

00:22:29,360 --> 00:22:33,919
layered on top of this and this actually

00:22:31,200 --> 00:22:35,520
makes uh you know the difference between

00:22:33,919 --> 00:22:37,120
uh some of the traditional tools and

00:22:35,520 --> 00:22:39,600
some of the more modern tools

00:22:37,120 --> 00:22:40,720
is this this idea of an internal virtual

00:22:39,600 --> 00:22:42,720
machine

00:22:40,720 --> 00:22:45,280
and on linux this is realized through

00:22:42,720 --> 00:22:48,080
the use of ebpf

00:22:45,280 --> 00:22:49,919
and this technology allows instead of

00:22:48,080 --> 00:22:52,960
writing directly into the buffer

00:22:49,919 --> 00:22:55,280
a tiny little program can be run

00:22:52,960 --> 00:22:56,720
inside the kernel as the data is about

00:22:55,280 --> 00:22:58,400
to be collected

00:22:56,720 --> 00:22:59,760
that makes decisions on what to do with

00:22:58,400 --> 00:23:02,159
it

00:22:59,760 --> 00:23:03,280
so it replaces that that copy into the

00:23:02,159 --> 00:23:05,280
buffer

00:23:03,280 --> 00:23:06,559
with a run of this program and this can

00:23:05,280 --> 00:23:09,280
do

00:23:06,559 --> 00:23:11,840
really anything um it has to have you

00:23:09,280 --> 00:23:13,840
know a fixed amount of execution because

00:23:11,840 --> 00:23:15,120
it's very important that the normal

00:23:13,840 --> 00:23:17,919
operation of the curl not

00:23:15,120 --> 00:23:19,679
interrupted but you can do a lot in a

00:23:17,919 --> 00:23:22,799
fixed amount of time including

00:23:19,679 --> 00:23:24,480
custom summarization or filtering or you

00:23:22,799 --> 00:23:27,280
know like histograms

00:23:24,480 --> 00:23:27,679
um and a lot of these tools just just go

00:23:27,280 --> 00:23:30,000
and

00:23:27,679 --> 00:23:31,200
and perform all of their analysis

00:23:30,000 --> 00:23:33,679
incrementally

00:23:31,200 --> 00:23:35,200
in kernel as the data is collected and

00:23:33,679 --> 00:23:38,480
have a tiny little

00:23:35,200 --> 00:23:40,400
drop of data coming out the end

00:23:38,480 --> 00:23:41,520
for those of you writing ebpf programs

00:23:40,400 --> 00:23:44,480
they must terminate

00:23:41,520 --> 00:23:47,360
in a provably bounded number of cycles

00:23:44,480 --> 00:23:47,360
and cannot block

00:23:47,679 --> 00:23:51,760
and then lastly is this this

00:23:49,919 --> 00:23:54,400
asynchronous data communication

00:23:51,760 --> 00:23:56,720
um and this is how these tiny little

00:23:54,400 --> 00:24:00,320
programs or this copying of ring buff

00:23:56,720 --> 00:24:01,919
data in the kernel actually gets back to

00:24:00,320 --> 00:24:04,720
your hands as an operator

00:24:01,919 --> 00:24:05,760
right um so all of this thing all of

00:24:04,720 --> 00:24:07,679
this behavior of

00:24:05,760 --> 00:24:10,320
tracing this activity happens in the

00:24:07,679 --> 00:24:13,600
kernel it gets copied into

00:24:10,320 --> 00:24:15,360
either ring buffer or ebbpf maps

00:24:13,600 --> 00:24:17,279
um and then the tracing program will

00:24:15,360 --> 00:24:19,360
either periodically or at the end of its

00:24:17,279 --> 00:24:21,679
activity

00:24:19,360 --> 00:24:22,799
read these ring buffers or maps and then

00:24:21,679 --> 00:24:25,360
convert them to

00:24:22,799 --> 00:24:26,240
sort of a user visible form so something

00:24:25,360 --> 00:24:28,159
that might be

00:24:26,240 --> 00:24:29,279
useful for a human so either a text

00:24:28,159 --> 00:24:32,640
output or

00:24:29,279 --> 00:24:35,120
a graph or something like that um

00:24:32,640 --> 00:24:36,159
and and this is really critical because

00:24:35,120 --> 00:24:38,320
um this sort of like

00:24:36,159 --> 00:24:39,520
asynchronous communication model means

00:24:38,320 --> 00:24:42,799
that um

00:24:39,520 --> 00:24:43,919
you have fixed amount of uh activity

00:24:42,799 --> 00:24:47,520
that happens in the kernel

00:24:43,919 --> 00:24:48,080
incrementally and then the the tracing

00:24:47,520 --> 00:24:50,000
tool

00:24:48,080 --> 00:24:52,400
never stops the workload from running so

00:24:50,000 --> 00:24:55,279
it never interrupts and that means

00:24:52,400 --> 00:24:57,039
um it has a little bit of overhead but

00:24:55,279 --> 00:25:01,840
you can rely on these tools

00:24:57,039 --> 00:25:01,840
uh never to disrupt a production system

00:25:02,240 --> 00:25:08,080
and so a recap use the tools

00:25:05,520 --> 00:25:09,200
they're fantastic they work great

00:25:08,080 --> 00:25:10,960
they're low overhead

00:25:09,200 --> 00:25:13,039
you can use them on pretty much any

00:25:10,960 --> 00:25:15,120
software i didn't have to do anything

00:25:13,039 --> 00:25:18,080
special to get my demo running i just

00:25:15,120 --> 00:25:18,720
used the standard tool chain built to

00:25:18,080 --> 00:25:21,600
you know

00:25:18,720 --> 00:25:23,279
misbehaving demo app ran it and the

00:25:21,600 --> 00:25:25,919
tools just worked

00:25:23,279 --> 00:25:27,120
and they pointed me directly at the

00:25:25,919 --> 00:25:30,720
problem

00:25:27,120 --> 00:25:30,720
that the application was experienced

00:25:30,799 --> 00:25:34,480
they're fast they're safe and they're

00:25:33,679 --> 00:25:37,440
consistent

00:25:34,480 --> 00:25:38,480
between uh you know all the software you

00:25:37,440 --> 00:25:41,760
might run

00:25:38,480 --> 00:25:43,919
if you're running hundreds of different

00:25:41,760 --> 00:25:45,279
applications and services in production

00:25:43,919 --> 00:25:47,520
um learning

00:25:45,279 --> 00:25:48,320
a hundred different set of tools to

00:25:47,520 --> 00:25:51,760
maintain them

00:25:48,320 --> 00:25:53,919
is just it's not possible um

00:25:51,760 --> 00:25:55,279
so get to know a consistent set of tools

00:25:53,919 --> 00:25:57,840
and uh use them on

00:25:55,279 --> 00:25:57,840
everything

00:25:58,400 --> 00:26:01,440
and you can rely on them to provide that

00:26:00,640 --> 00:26:03,440
visibility

00:26:01,440 --> 00:26:05,200
into all the software you might run on a

00:26:03,440 --> 00:26:08,320
linux system

00:26:05,200 --> 00:26:11,120
there they're there get them deployed

00:26:08,320 --> 00:26:12,400
in your pre-prod and dev environments

00:26:11,120 --> 00:26:14,400
get to know them

00:26:12,400 --> 00:26:15,840
run them against your software and

00:26:14,400 --> 00:26:17,679
discover

00:26:15,840 --> 00:26:19,600
how it behaves discover the inner

00:26:17,679 --> 00:26:22,799
workings discover what your

00:26:19,600 --> 00:26:25,440
your teams are actually doing

00:26:22,799 --> 00:26:25,840
uh and i'd like to point out big thanks

00:26:25,440 --> 00:26:28,000
to

00:26:25,840 --> 00:26:29,440
all the contributors uh to the linux

00:26:28,000 --> 00:26:32,159
performance subsystem

00:26:29,440 --> 00:26:33,840
um this like seeing the growth of it

00:26:32,159 --> 00:26:36,960
over the last decade or so

00:26:33,840 --> 00:26:38,000
um it's really a phenomenal system that

00:26:36,960 --> 00:26:39,440
exposes a lot of data

00:26:38,000 --> 00:26:41,840
and makes working with production

00:26:39,440 --> 00:26:43,919
services a lot easier

00:26:41,840 --> 00:26:45,520
even when they misbehave and then of

00:26:43,919 --> 00:26:47,520
course the iovisor

00:26:45,520 --> 00:26:49,120
project and bren greg for building some

00:26:47,520 --> 00:26:52,559
of these tools

00:26:49,120 --> 00:26:53,520
they're really great bcc is just a joy

00:26:52,559 --> 00:26:57,200
to work with

00:26:53,520 --> 00:26:57,200
and put them to good use

00:26:59,760 --> 00:27:03,279
and that is uh the content i've got to

00:27:02,400 --> 00:27:06,080
cover today

00:27:03,279 --> 00:27:07,840
um thanks for attending i'm ryan i work

00:27:06,080 --> 00:27:09,440
at capsulate where we use

00:27:07,840 --> 00:27:11,600
some of these systems to build security

00:27:09,440 --> 00:27:21,840
tools um

00:27:11,600 --> 00:27:21,840
and i will now take questions

00:27:32,159 --> 00:27:38,720
so we have a question here from uh

00:27:35,360 --> 00:27:40,240
ryan perry who says since the overhead

00:27:38,720 --> 00:27:40,640
is so low what are your thoughts on

00:27:40,240 --> 00:27:42,799
doing

00:27:40,640 --> 00:27:43,919
continuous profiling in production and

00:27:42,799 --> 00:27:46,720
being able to go

00:27:43,919 --> 00:27:48,240
back and look at particular time periods

00:27:46,720 --> 00:27:50,240
to revisit profiles

00:27:48,240 --> 00:27:51,520
yes this is actually a really

00:27:50,240 --> 00:27:54,320
interesting approach

00:27:51,520 --> 00:27:54,640
um i've seen a lot of the modern sort of

00:27:54,320 --> 00:27:57,440
like

00:27:54,640 --> 00:27:58,240
observability tools um do this right

00:27:57,440 --> 00:28:00,240
they just

00:27:58,240 --> 00:28:02,240
plug into the tracing subsystems and

00:28:00,240 --> 00:28:04,799
they observe the state of the machine

00:28:02,240 --> 00:28:06,640
and they're very safe to use some of

00:28:04,799 --> 00:28:06,960
them do have a little overhead like it's

00:28:06,640 --> 00:28:09,440
not

00:28:06,960 --> 00:28:10,000
zero but it's very manageable and the

00:28:09,440 --> 00:28:13,600
risk is

00:28:10,000 --> 00:28:16,399
is really uh really low um

00:28:13,600 --> 00:28:17,520
so yeah i highly recommend this and we

00:28:16,399 --> 00:28:20,320
see

00:28:17,520 --> 00:28:21,760
vendors from sort of taking and doing

00:28:20,320 --> 00:28:23,120
this from multiple angles we're

00:28:21,760 --> 00:28:23,760
certainly doing it from the security

00:28:23,120 --> 00:28:26,880
angle

00:28:23,760 --> 00:28:28,399
um but definitely the the ops folks are

00:28:26,880 --> 00:28:30,080
are ahead there they want to understand

00:28:28,399 --> 00:28:31,840
how systems behave they want to

00:28:30,080 --> 00:28:33,600
continuously know if the system is

00:28:31,840 --> 00:28:35,120
healthy and what's going on

00:28:33,600 --> 00:28:37,279
and they've been doing it for a while

00:28:35,120 --> 00:28:37,279
now

00:28:37,440 --> 00:28:43,840
we've got a question from soham

00:28:41,360 --> 00:28:45,919
does running the profiler itself add any

00:28:43,840 --> 00:28:47,679
performance overhead to the application

00:28:45,919 --> 00:28:48,799
i mean does capturing the stack traces

00:28:47,679 --> 00:28:49,679
reduce the performance of the

00:28:48,799 --> 00:28:52,080
application

00:28:49,679 --> 00:28:53,279
uh there is a little bit of measurement

00:28:52,080 --> 00:28:55,760
cost to

00:28:53,279 --> 00:28:56,720
uh to doing this but it is distributed

00:28:55,760 --> 00:28:59,039
through the

00:28:56,720 --> 00:29:00,640
scheduling uh that the linux kernel does

00:28:59,039 --> 00:29:02,640
so it makes the scheduling a little bit

00:29:00,640 --> 00:29:03,919
more expensive since it has to capture

00:29:02,640 --> 00:29:07,520
this data

00:29:03,919 --> 00:29:09,279
um uh generally uh like if you're right

00:29:07,520 --> 00:29:13,120
at the like

00:29:09,279 --> 00:29:14,960
99 you're like just about to tip over

00:29:13,120 --> 00:29:16,240
um or just about to overflow your

00:29:14,960 --> 00:29:18,640
performance budget

00:29:16,240 --> 00:29:20,320
maybe that's something to worry about

00:29:18,640 --> 00:29:23,440
but if you're there anyways you're

00:29:20,320 --> 00:29:25,200
probably already in trouble um so

00:29:23,440 --> 00:29:28,399
understanding how the system is behaving

00:29:25,200 --> 00:29:30,960
so maybe you can fix it might be better

00:29:28,399 --> 00:29:31,440
u probes in particular do have a little

00:29:30,960 --> 00:29:33,440
bit of

00:29:31,440 --> 00:29:35,200
cost because they have to switch over to

00:29:33,440 --> 00:29:36,960
the kernel whenever they get hit

00:29:35,200 --> 00:29:39,200
so that is something to be aware of as

00:29:36,960 --> 00:29:39,200
well

00:29:39,279 --> 00:29:42,559
uh but generally the cost is pretty low

00:29:41,760 --> 00:29:45,039
um and

00:29:42,559 --> 00:29:46,480
i would say get to know them in dev and

00:29:45,039 --> 00:29:48,960
staging before you like

00:29:46,480 --> 00:29:51,200
actually leap into prod with them right

00:29:48,960 --> 00:29:54,399
know your tools before you

00:29:51,200 --> 00:29:54,880
start uh using them on on problems for

00:29:54,399 --> 00:29:58,240
which

00:29:54,880 --> 00:30:02,320
there is pressure and risk

00:29:58,240 --> 00:30:03,520
uh uh carl asks uh where can we get the

00:30:02,320 --> 00:30:05,440
chart of tools

00:30:03,520 --> 00:30:07,600
uh this was taken from brendan greg's

00:30:05,440 --> 00:30:11,520
website so that's

00:30:07,600 --> 00:30:13,520
his last name has uh 2gs uh

00:30:11,520 --> 00:30:14,640
really fantastic chart there is just so

00:30:13,520 --> 00:30:17,760
much in there

00:30:14,640 --> 00:30:19,440
um and like the cloud native computing

00:30:17,760 --> 00:30:22,080
foundations chart it's just like

00:30:19,440 --> 00:30:22,640
getting more complete there's so much to

00:30:22,080 --> 00:30:24,880
it

00:30:22,640 --> 00:30:26,320
um uh which is which is good because

00:30:24,880 --> 00:30:27,440
there's so many great tools you can just

00:30:26,320 --> 00:30:29,440
use on everything

00:30:27,440 --> 00:30:31,120
um and and that's kind of what we like

00:30:29,440 --> 00:30:34,799
to see that's part of the

00:30:31,120 --> 00:30:36,559
the benefit of the linux community is um

00:30:34,799 --> 00:30:38,080
you know like the the software is so

00:30:36,559 --> 00:30:40,559
composable and you can

00:30:38,080 --> 00:30:41,679
have components interact and be used

00:30:40,559 --> 00:30:45,600
against each other

00:30:41,679 --> 00:30:45,600
um so that's great

00:30:46,320 --> 00:30:51,840
uh uh

00:30:50,000 --> 00:30:53,200
may i know what are the performance

00:30:51,840 --> 00:30:54,080
indicators you recommend that differs

00:30:53,200 --> 00:30:55,520
from the government

00:30:54,080 --> 00:30:57,519
uh to live environments such as

00:30:55,520 --> 00:31:00,640
lightweight latency dependency

00:30:57,519 --> 00:31:03,279
uh this is jayakar i'm not quite

00:31:00,640 --> 00:31:04,080
sure how to answer this i think each

00:31:03,279 --> 00:31:07,200
application

00:31:04,080 --> 00:31:08,399
has uh different metrics that they want

00:31:07,200 --> 00:31:10,559
to be measured by

00:31:08,399 --> 00:31:11,519
like some applications latency is really

00:31:10,559 --> 00:31:14,799
important some

00:31:11,519 --> 00:31:17,840
throughput is important um

00:31:14,799 --> 00:31:19,440
i do know that if uh sort of like it's

00:31:17,840 --> 00:31:21,200
not a batch system

00:31:19,440 --> 00:31:23,200
you kind of want to make sure that you

00:31:21,200 --> 00:31:25,840
have some

00:31:23,200 --> 00:31:27,919
additional headroom on each of the

00:31:25,840 --> 00:31:30,799
different resource types you have

00:31:27,919 --> 00:31:32,240
or have the ability to auto scale them

00:31:30,799 --> 00:31:35,039
in time um

00:31:32,240 --> 00:31:36,799
which is a common modern approach um so

00:31:35,039 --> 00:31:40,080
i think the indicators are actually

00:31:36,799 --> 00:31:43,200
uh uh sort of like

00:31:40,080 --> 00:31:44,799
service dependent right um and a lot of

00:31:43,200 --> 00:31:46,240
people just run their applications with

00:31:44,799 --> 00:31:48,320
like hey let's give it

00:31:46,240 --> 00:31:49,360
so much headroom that there's no chance

00:31:48,320 --> 00:31:51,039
because

00:31:49,360 --> 00:31:53,120
human peace of mind on keeping the

00:31:51,039 --> 00:31:56,159
service up is better

00:31:53,120 --> 00:31:56,799
i will say that often with performance

00:31:56,159 --> 00:31:59,440
tracing

00:31:56,799 --> 00:32:00,000
it's it's finding the bottleneck right

00:31:59,440 --> 00:32:03,519
like

00:32:00,000 --> 00:32:05,760
some some resource is uh

00:32:03,519 --> 00:32:07,840
you know hit its limit and that you have

00:32:05,760 --> 00:32:11,360
to determine what it is and why

00:32:07,840 --> 00:32:14,159
um and sometimes the solution

00:32:11,360 --> 00:32:16,000
is give it more of that resource and

00:32:14,159 --> 00:32:17,919
sometimes the solution is

00:32:16,000 --> 00:32:19,279
figure out why it's using so much of

00:32:17,919 --> 00:32:20,559
that resource because it really

00:32:19,279 --> 00:32:22,480
shouldn't be doing that and

00:32:20,559 --> 00:32:24,480
that was the case here right we were we

00:32:22,480 --> 00:32:27,519
were sinking to disk way too often

00:32:24,480 --> 00:32:27,519
and just shouldn't have been

00:32:27,600 --> 00:32:32,880
uh chris muldrack asks

00:32:30,880 --> 00:32:34,159
uh when was this functionality added to

00:32:32,880 --> 00:32:36,720
the kernel

00:32:34,159 --> 00:32:37,440
trace points k probes u probes which

00:32:36,720 --> 00:32:40,960
version

00:32:37,440 --> 00:32:43,440
ah that is a really good question um

00:32:40,960 --> 00:32:44,640
each of those were added uh trades

00:32:43,440 --> 00:32:47,120
points k probes

00:32:44,640 --> 00:32:49,360
have been around for a good amount of

00:32:47,120 --> 00:32:52,240
time you probes are relatively new

00:32:49,360 --> 00:32:53,360
um but they were introduced for other

00:32:52,240 --> 00:32:56,480
parts of the system

00:32:53,360 --> 00:32:57,120
first and attached to the perf tracing

00:32:56,480 --> 00:33:00,240
systems

00:32:57,120 --> 00:33:04,159
later i suspect around

00:33:00,240 --> 00:33:04,960
the 4 8 series is where you start to see

00:33:04,159 --> 00:33:08,640
them get

00:33:04,960 --> 00:33:11,279
like usable in production

00:33:08,640 --> 00:33:12,320
the 414s series is where they're like

00:33:11,279 --> 00:33:14,480
really solid and

00:33:12,320 --> 00:33:16,480
feature complete um and there's been

00:33:14,480 --> 00:33:16,960
some really cool new features added to

00:33:16,480 --> 00:33:20,000
them in

00:33:16,960 --> 00:33:22,320
in the 5x series um so as long as you're

00:33:20,000 --> 00:33:25,840
using a modern kernel you have

00:33:22,320 --> 00:33:27,600
a pretty good support for a lot of these

00:33:25,840 --> 00:33:30,399
features

00:33:27,600 --> 00:33:32,559
red hat has also back ported them to the

00:33:30,399 --> 00:33:34,960
310 series

00:33:32,559 --> 00:33:36,799
and so you can use some some tracing

00:33:34,960 --> 00:33:39,760
tools on that

00:33:36,799 --> 00:33:40,559
and uh but the support isn't there's

00:33:39,760 --> 00:33:42,080
there's some

00:33:40,559 --> 00:33:43,840
dimensions of it i'm not sure exactly

00:33:42,080 --> 00:33:46,480
which version they've backboarded but

00:33:43,840 --> 00:33:47,200
um they're a good vendor and uh they you

00:33:46,480 --> 00:33:49,120
know they

00:33:47,200 --> 00:33:52,399
they make sure that everything is

00:33:49,120 --> 00:33:55,440
enterprise ready

00:33:52,399 --> 00:33:56,960
um we have from tim sander what is the

00:33:55,440 --> 00:33:59,919
best way to trace

00:33:56,960 --> 00:34:01,600
rt workloads i think that might mean

00:33:59,919 --> 00:34:05,440
real-time workloads

00:34:01,600 --> 00:34:06,559
um i i guess it depends on whether your

00:34:05,440 --> 00:34:09,760
workload is like

00:34:06,559 --> 00:34:10,480
hard real time or soft real time uh soft

00:34:09,760 --> 00:34:13,760
real time

00:34:10,480 --> 00:34:16,560
this is definitely appropriate um and it

00:34:13,760 --> 00:34:18,159
makes sense to to like know where you

00:34:16,560 --> 00:34:20,800
can and can't probe

00:34:18,159 --> 00:34:22,320
but you definitely uh could make use of

00:34:20,800 --> 00:34:23,520
some of these tracing tools if you're

00:34:22,320 --> 00:34:26,240
like

00:34:23,520 --> 00:34:27,679
fintech and you're pinning like an

00:34:26,240 --> 00:34:31,040
application to a core

00:34:27,679 --> 00:34:33,599
and talking to the network you know

00:34:31,040 --> 00:34:34,079
interface directly and have hard

00:34:33,599 --> 00:34:37,119
real-time

00:34:34,079 --> 00:34:39,440
requirements probably not you're

00:34:37,119 --> 00:34:42,800
probably in a completely different world

00:34:39,440 --> 00:34:43,679
of performance but uh you probably

00:34:42,800 --> 00:34:46,800
already know that

00:34:43,679 --> 00:34:47,280
and and have have have tools to deal

00:34:46,800 --> 00:34:50,079
with it

00:34:47,280 --> 00:34:51,839
um but most everyone else probably it's

00:34:50,079 --> 00:34:54,079
it's appropriate to use the tools

00:34:51,839 --> 00:34:55,919
um obviously test and prod and know what

00:34:54,079 --> 00:34:59,520
they're or test in pre-prod

00:34:55,919 --> 00:35:02,560
and dev and know what uh the impact is

00:34:59,520 --> 00:35:05,599
but um uh

00:35:02,560 --> 00:35:08,640
you know you probably appropriate

00:35:05,599 --> 00:35:12,000
uh we have uh pablo asking uh

00:35:08,640 --> 00:35:13,760
how to create uh flame graph files um

00:35:12,000 --> 00:35:15,839
so that is actually uh just a little

00:35:13,760 --> 00:35:19,520
script

00:35:15,839 --> 00:35:22,240
accessible on a flame graph repository

00:35:19,520 --> 00:35:25,680
which is linked from brendan greg's site

00:35:22,240 --> 00:35:28,720
you basically pipe in a list of

00:35:25,680 --> 00:35:32,000
stack traces with counts

00:35:28,720 --> 00:35:35,040
and it will summarize them in that flame

00:35:32,000 --> 00:35:37,839
formation for you and give you a an

00:35:35,040 --> 00:35:38,640
svg that you can interact with and zoom

00:35:37,839 --> 00:35:41,200
in on different

00:35:38,640 --> 00:35:43,280
parts and i didn't realize svgs can be

00:35:41,200 --> 00:35:45,200
interactive but it turns out they can

00:35:43,280 --> 00:35:46,560
and uh so you can click through and see

00:35:45,200 --> 00:35:49,520
the the details and

00:35:46,560 --> 00:35:51,200
zoom in um so i highly recommend that

00:35:49,520 --> 00:35:53,440
that pearl script that

00:35:51,200 --> 00:35:54,480
takes the textual information which is

00:35:53,440 --> 00:35:57,359
quite verbose

00:35:54,480 --> 00:36:00,160
and just gives a visual uh

00:35:57,359 --> 00:36:00,160
representation

00:36:00,480 --> 00:36:03,599
uh ricardo asks is there a similar tool

00:36:02,720 --> 00:36:06,640
such as a b

00:36:03,599 --> 00:36:09,280
for nginx uh that i have used or

00:36:06,640 --> 00:36:09,680
recommended um actually the apache bench

00:36:09,280 --> 00:36:13,520
tool

00:36:09,680 --> 00:36:14,640
is pretty great um it is limited and

00:36:13,520 --> 00:36:17,200
then it only does

00:36:14,640 --> 00:36:19,359
one kind of request you tell it what

00:36:17,200 --> 00:36:22,560
request to do and then it

00:36:19,359 --> 00:36:24,320
like does that

00:36:22,560 --> 00:36:25,760
repeatedly so it's not great if you want

00:36:24,320 --> 00:36:28,800
to test like a whole

00:36:25,760 --> 00:36:31,839
distributed you know like how does

00:36:28,800 --> 00:36:33,280
the pattern of user activity replaying

00:36:31,839 --> 00:36:34,800
production traffic it's not good for

00:36:33,280 --> 00:36:35,760
that but just for a simple demo like

00:36:34,800 --> 00:36:39,440
this

00:36:35,760 --> 00:36:41,520
you can run apache bench against any uh

00:36:39,440 --> 00:36:43,680
any web service you know server that

00:36:41,520 --> 00:36:45,520
that runs um and indeed i didn't run it

00:36:43,680 --> 00:36:46,000
against apache i ran it against my

00:36:45,520 --> 00:36:49,680
little

00:36:46,000 --> 00:36:49,680
my little toy web application

00:36:50,720 --> 00:36:54,000
uh chris asks are there any books that

00:36:52,960 --> 00:36:57,839
you can recommend on

00:36:54,000 --> 00:36:59,839
this topic there are i don't want to

00:36:57,839 --> 00:37:02,400
give a specific endorsement

00:36:59,839 --> 00:37:03,839
here because i'm not sure if it's

00:37:02,400 --> 00:37:05,920
appropriate to do that

00:37:03,839 --> 00:37:07,040
there are plenty of books on this topic

00:37:05,920 --> 00:37:09,760
and

00:37:07,040 --> 00:37:10,320
if you send me a message on twitter or

00:37:09,760 --> 00:37:12,960
ryan

00:37:10,320 --> 00:37:15,359
capsule8.com i'd be happy to follow up

00:37:12,960 --> 00:37:15,359
with that

00:37:16,079 --> 00:37:19,440
uh promise asks just a follow-up

00:37:18,800 --> 00:37:21,920
question with

00:37:19,440 --> 00:37:23,119
monitoring tools like new relic stackify

00:37:21,920 --> 00:37:24,160
having their agents installed on

00:37:23,119 --> 00:37:25,440
production servers

00:37:24,160 --> 00:37:26,880
do we need to install these linux

00:37:25,440 --> 00:37:27,440
tracing tools on the production servers

00:37:26,880 --> 00:37:30,720
as well

00:37:27,440 --> 00:37:32,160
um in many cases like uh

00:37:30,720 --> 00:37:33,760
vendors are coming into the space

00:37:32,160 --> 00:37:36,400
they're offering commercial tools

00:37:33,760 --> 00:37:38,079
that you like sort of do this from a

00:37:36,400 --> 00:37:41,280
like agent in the sky

00:37:38,079 --> 00:37:41,839
um if those tools work for you great use

00:37:41,280 --> 00:37:44,079
them

00:37:41,839 --> 00:37:46,160
um i don't think they might offer the

00:37:44,079 --> 00:37:49,440
same level of visibility as

00:37:46,160 --> 00:37:51,200
the open source stack does um but it's

00:37:49,440 --> 00:37:52,480
really great to have a vendor that you

00:37:51,200 --> 00:37:54,880
can talk to that like

00:37:52,480 --> 00:37:55,920
can help you through it um and in many

00:37:54,880 --> 00:37:57,839
cases like

00:37:55,920 --> 00:37:59,520
vendors are advertising hey we have the

00:37:57,839 --> 00:38:01,119
ebpf and we use perf

00:37:59,520 --> 00:38:03,680
um which is really great because then

00:38:01,119 --> 00:38:05,599
you mean you you know they're to be safe

00:38:03,680 --> 00:38:08,000
right um because they have the same

00:38:05,599 --> 00:38:10,560
properties as these open source tools so

00:38:08,000 --> 00:38:11,359
um the the same thing applies to those

00:38:10,560 --> 00:38:16,640
vendor tools

00:38:11,359 --> 00:38:16,640
as does uh these these perf event tools

00:38:16,880 --> 00:38:23,200
uh my class uh to add to ryan's question

00:38:21,200 --> 00:38:24,800
would any data cleansing required to

00:38:23,200 --> 00:38:27,119
build a useful dashboard

00:38:24,800 --> 00:38:28,160
i think that depends on how sensitive

00:38:27,119 --> 00:38:31,040
your data is

00:38:28,160 --> 00:38:33,200
uh everything we've seen here that i

00:38:31,040 --> 00:38:34,160
looked at doesn't have sensitive date

00:38:33,200 --> 00:38:37,599
there's no

00:38:34,160 --> 00:38:39,520
like stack traces i'm not sure how they

00:38:37,599 --> 00:38:41,520
could be considered sensitive maybe

00:38:39,520 --> 00:38:43,599
someone could make an argument uh for

00:38:41,520 --> 00:38:44,079
them to be um there is definitely the

00:38:43,599 --> 00:38:46,320
case

00:38:44,079 --> 00:38:48,079
where you can get sensitive data right

00:38:46,320 --> 00:38:50,480
um so things like

00:38:48,079 --> 00:38:52,320
exec snoop or io snoop or that sort of

00:38:50,480 --> 00:38:52,960
thing you can get file names or you can

00:38:52,320 --> 00:38:55,760
get process

00:38:52,960 --> 00:38:58,720
names um and the tools the probes are

00:38:55,760 --> 00:39:00,800
very sensitive you can put

00:38:58,720 --> 00:39:02,480
you can sniff network traffic you can do

00:39:00,800 --> 00:39:04,400
anything like that so definitely you can

00:39:02,480 --> 00:39:06,480
get access to sensitive data

00:39:04,400 --> 00:39:08,560
all the tools i demoed here you could

00:39:06,480 --> 00:39:10,880
put them in a dashboard and not worry

00:39:08,560 --> 00:39:10,880
about it

00:39:12,839 --> 00:39:18,960
uh uh

00:39:16,000 --> 00:39:20,800
nemeca asks uh please i would like to

00:39:18,960 --> 00:39:21,359
know how i go about using these tracing

00:39:20,800 --> 00:39:23,440
tool

00:39:21,359 --> 00:39:24,640
cross server fleets i think there are

00:39:23,440 --> 00:39:27,040
some vendors that

00:39:24,640 --> 00:39:28,160
um offer this sort of thing i think a

00:39:27,040 --> 00:39:29,520
few of them were mentioned in the

00:39:28,160 --> 00:39:32,960
earlier questions

00:39:29,520 --> 00:39:33,599
um uh some of those are great um i think

00:39:32,960 --> 00:39:35,440
there's some

00:39:33,599 --> 00:39:36,720
some like homegrown automation open

00:39:35,440 --> 00:39:39,359
source projects as well

00:39:36,720 --> 00:39:40,800
that will do this um yeah there's

00:39:39,359 --> 00:39:42,480
there's a lot of different options here

00:39:40,800 --> 00:39:46,640
i think some of the space is

00:39:42,480 --> 00:39:49,200
emerging um i definitely like to see

00:39:46,640 --> 00:39:50,400
some more tools to be able to not just

00:39:49,200 --> 00:39:52,720
do this when something

00:39:50,400 --> 00:39:54,720
goes bad but like to be able to run

00:39:52,720 --> 00:39:57,599
these and collect the data historically

00:39:54,720 --> 00:39:58,800
and then ah like something is going bad

00:39:57,599 --> 00:40:02,240
let's compare

00:39:58,800 --> 00:40:05,280
today's view to yesterday's view when

00:40:02,240 --> 00:40:05,280
the system was healthy

00:40:06,079 --> 00:40:10,960
uh alexander asks is it still in kernel

00:40:08,800 --> 00:40:11,920
eppf mechanics when we use u-probe for

00:40:10,960 --> 00:40:14,800
user space trace

00:40:11,920 --> 00:40:15,839
yes so the way the u-probes work is they

00:40:14,800 --> 00:40:18,960
trap to the kernel

00:40:15,839 --> 00:40:20,240
which incurs a user space to kernel

00:40:18,960 --> 00:40:22,960
transition

00:40:20,240 --> 00:40:24,079
the kernel bpf program runs and then it

00:40:22,960 --> 00:40:26,319
transitions back so

00:40:24,079 --> 00:40:27,359
there actually is more of a cost to you

00:40:26,319 --> 00:40:31,040
probes than to

00:40:27,359 --> 00:40:34,079
to k probes um so that may or may not be

00:40:31,040 --> 00:40:35,040
relevant for the application um most of

00:40:34,079 --> 00:40:36,880
the tools on here

00:40:35,040 --> 00:40:38,480
aren't aren't you probe based but that

00:40:36,880 --> 00:40:41,359
is uh

00:40:38,480 --> 00:40:41,359
something to know

00:40:43,359 --> 00:40:49,520
excellent and that is it for q

00:40:46,800 --> 00:40:51,040
a today um thank you all for coming i

00:40:49,520 --> 00:40:53,200
will hand it back over to the linux

00:40:51,040 --> 00:40:55,040
foundation

00:40:53,200 --> 00:40:56,560
all right thank you so much to ryan for

00:40:55,040 --> 00:40:58,400
his time today and thank you to all the

00:40:56,560 --> 00:41:00,480
participants who joined us

00:40:58,400 --> 00:41:03,280
as a reminder this recording will be on

00:41:00,480 --> 00:41:04,720
the linux foundation youtube page soon

00:41:03,280 --> 00:41:12,319
we hope you're able to join us for

00:41:04,720 --> 00:41:12,319

YouTube URL: https://www.youtube.com/watch?v=sAg2Hbnfui8


