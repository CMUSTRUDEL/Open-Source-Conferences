Title: LF Live Webinar: Serverless Event Driven Scale for Any Container with KEDA
Publication date: 2021-01-14
Playlist: LF Live Webinars
Description: 
	Sponsored by Microsoft Azure

Event driven workloads are growing in popularity given trends around serverless computing and FaaS. However, event driven benefits donâ€™t have to be limited to a FaaS service like AWS Lambda or Azure Functions. With KEDA you can bring event driven capabilities to *any* workload running in Kubernetes, to provide on demand compute and elastic scale. This webinar will discuss the benefits of event driven compute, how KEDA provides event driven scale, and discuss best practices and real world use cases for bringing event driven to every cluster.

This webinar is sponsored by Microsoft Azure and hosted by The Linux Foundation.
Captions: 
	00:00:00,000 --> 00:00:03,040
much uh it's thrilled to be here thank

00:00:01,920 --> 00:00:05,920
you all as well

00:00:03,040 --> 00:00:07,200
for dialing in uh whether it's morning

00:00:05,920 --> 00:00:09,200
afternoon or evening

00:00:07,200 --> 00:00:10,880
or watching this recording as was

00:00:09,200 --> 00:00:12,719
mentioned in the introduction i'm going

00:00:10,880 --> 00:00:15,280
to be talking today about cada

00:00:12,719 --> 00:00:16,400
kubernetes event driven auto scaling i'm

00:00:15,280 --> 00:00:18,560
excited to do that

00:00:16,400 --> 00:00:21,439
before we jump into it though let me

00:00:18,560 --> 00:00:23,519
give a quick introduction about myself

00:00:21,439 --> 00:00:25,359
fantastic to meet you all i am jeff

00:00:23,519 --> 00:00:28,080
holland i've been at microsoft for

00:00:25,359 --> 00:00:30,080
a little over seven years now i'm

00:00:28,080 --> 00:00:31,920
currently the lead product manager for

00:00:30,080 --> 00:00:34,399
azure serverless so that includes the

00:00:31,920 --> 00:00:36,880
azure functions serverless service

00:00:34,399 --> 00:00:38,879
that includes azure static web apps to

00:00:36,880 --> 00:00:40,879
do some serverless web hosting

00:00:38,879 --> 00:00:42,079
uh it's been great i thought i would add

00:00:40,879 --> 00:00:45,039
something a little bit more

00:00:42,079 --> 00:00:46,320
fun into the mix as well so i decided to

00:00:45,039 --> 00:00:49,039
call out that i have

00:00:46,320 --> 00:00:50,399
uh actually been a guest on the ellen

00:00:49,039 --> 00:00:52,000
degeneres show

00:00:50,399 --> 00:00:53,760
my wife and i got called up to play a

00:00:52,000 --> 00:00:56,480
game during one of her shows

00:00:53,760 --> 00:00:58,480
i was not a guest to to talk about keda

00:00:56,480 --> 00:00:59,760
or serverless or anything fun like that

00:00:58,480 --> 00:01:02,239
but it is just something

00:00:59,760 --> 00:01:03,520
uh more light you can find me on twitter

00:01:02,239 --> 00:01:05,119
at jeff paulin

00:01:03,520 --> 00:01:07,040
if you ever want to reach out have

00:01:05,119 --> 00:01:09,680
questions uh or

00:01:07,040 --> 00:01:10,960
whatever it is just want to say hi uh

00:01:09,680 --> 00:01:14,240
and and speaking of which

00:01:10,960 --> 00:01:16,159
i do have the questions here live so as

00:01:14,240 --> 00:01:19,360
you are going through this presentation

00:01:16,159 --> 00:01:21,200
if you want to go and drop me a question

00:01:19,360 --> 00:01:23,520
uh about cada about what we're talking

00:01:21,200 --> 00:01:25,439
about i'll probably pause here

00:01:23,520 --> 00:01:27,280
in 20 minutes or so and grab a few of

00:01:25,439 --> 00:01:29,360
them depending on what's there

00:01:27,280 --> 00:01:31,119
and make some time at the end as well so

00:01:29,360 --> 00:01:33,119
feel free to keep those coming in and

00:01:31,119 --> 00:01:35,920
i'll make some time for them

00:01:33,119 --> 00:01:37,680
as for our session today there's a few

00:01:35,920 --> 00:01:38,560
topics i want to spend some time

00:01:37,680 --> 00:01:40,560
covering

00:01:38,560 --> 00:01:41,680
the first is a little bit of an overview

00:01:40,560 --> 00:01:43,680
of what we mean by

00:01:41,680 --> 00:01:45,600
serverless and event driven these are

00:01:43,680 --> 00:01:48,560
both incredibly

00:01:45,600 --> 00:01:50,479
powerful and emerging popular trends a

00:01:48,560 --> 00:01:51,759
lot of buzzwords involved but there's

00:01:50,479 --> 00:01:53,360
some meat behind them there's some

00:01:51,759 --> 00:01:55,119
benefits behind them there's some use

00:01:53,360 --> 00:01:56,719
cases that i want to go over

00:01:55,119 --> 00:01:59,040
we're going to weave that into how that

00:01:56,719 --> 00:02:01,280
works with cada and what's cada's role

00:01:59,040 --> 00:02:02,799
in this event-driven world some

00:02:01,280 --> 00:02:04,560
different concepts of cada including

00:02:02,799 --> 00:02:05,280
scaled objects and skill jobs that we'll

00:02:04,560 --> 00:02:06,960
get to

00:02:05,280 --> 00:02:10,160
we're going to talk about how serverless

00:02:06,960 --> 00:02:13,280
runtimes and cada can work together and

00:02:10,160 --> 00:02:16,239
makes some time for q and a

00:02:13,280 --> 00:02:16,959
all right so let's start with those fun

00:02:16,239 --> 00:02:20,000
words

00:02:16,959 --> 00:02:22,480
uh let's do event driven first

00:02:20,000 --> 00:02:23,280
so event driven architectures is not new

00:02:22,480 --> 00:02:25,760
this has been around

00:02:23,280 --> 00:02:27,599
for a very long time but the idea is

00:02:25,760 --> 00:02:28,959
that you want to run your workload in

00:02:27,599 --> 00:02:31,920
reaction to

00:02:28,959 --> 00:02:32,720
events this is very natural in some ways

00:02:31,920 --> 00:02:35,200
right you don't

00:02:32,720 --> 00:02:36,720
answer your door unless somebody knocks

00:02:35,200 --> 00:02:38,400
on it there is some event

00:02:36,720 --> 00:02:39,920
that lets you know i need to get up and

00:02:38,400 --> 00:02:42,319
answer the door uh

00:02:39,920 --> 00:02:43,840
you don't do things usually until

00:02:42,319 --> 00:02:44,560
something happens so event driven is

00:02:43,840 --> 00:02:46,879
saying

00:02:44,560 --> 00:02:48,959
can you build your application in a way

00:02:46,879 --> 00:02:50,879
so that it is responding to the business

00:02:48,959 --> 00:02:52,080
events to the technology events to the

00:02:50,879 --> 00:02:55,519
plants

00:02:52,080 --> 00:02:57,519
that actually have work that needs to be

00:02:55,519 --> 00:02:58,560
means is you'll have something like a

00:02:57,519 --> 00:03:00,640
new employee

00:02:58,560 --> 00:03:02,480
is hired that's an event that gets

00:03:00,640 --> 00:03:05,440
emitted into your system

00:03:02,480 --> 00:03:07,280
well maybe an employee is hired to issue

00:03:05,440 --> 00:03:09,200
a company email address

00:03:07,280 --> 00:03:10,319
and so one of your event-driven pieces

00:03:09,200 --> 00:03:12,560
of code might say

00:03:10,319 --> 00:03:14,159
oh i see the employee hired event i'm

00:03:12,560 --> 00:03:16,159
going to go issue them an email address

00:03:14,159 --> 00:03:17,440
maybe somebody else is is you know i

00:03:16,159 --> 00:03:18,000
need to go make sure that they're

00:03:17,440 --> 00:03:21,120
registered

00:03:18,000 --> 00:03:22,959
in our hr systems you get the idea when

00:03:21,120 --> 00:03:24,319
you're building off of these events

00:03:22,959 --> 00:03:26,400
some of the benefits is that your

00:03:24,319 --> 00:03:27,360
architecture becomes very loosely

00:03:26,400 --> 00:03:29,760
coupled

00:03:27,360 --> 00:03:30,400
and very highly composable right i can

00:03:29,760 --> 00:03:33,280
add

00:03:30,400 --> 00:03:34,080
additional for when unemployed

00:03:33,280 --> 00:03:36,480
necessarily

00:03:34,080 --> 00:03:38,239
go change the issuing the email or the

00:03:36,480 --> 00:03:40,799
registering hr system i just write

00:03:38,239 --> 00:03:42,720
another microservice another component

00:03:40,799 --> 00:03:44,560
that says oh i actually care about the

00:03:42,720 --> 00:03:46,159
event an employee gets hired

00:03:44,560 --> 00:03:49,040
so you can move these around you can

00:03:46,159 --> 00:03:51,519
reuse them it has some benefits

00:03:49,040 --> 00:03:52,879
now as part of event driven this is

00:03:51,519 --> 00:03:55,280
taken off a bit more

00:03:52,879 --> 00:03:56,560
in the cloud recently because of

00:03:55,280 --> 00:03:58,480
serverless

00:03:56,560 --> 00:04:00,080
often you'll hear about things like aws

00:03:58,480 --> 00:04:02,319
lambda azure functions google

00:04:00,080 --> 00:04:04,319
cloud functions openfast a myriad of

00:04:02,319 --> 00:04:06,319
other serverless options

00:04:04,319 --> 00:04:08,000
in the intent of serverless while the

00:04:06,319 --> 00:04:10,560
word itself is a misnomer

00:04:08,000 --> 00:04:12,239
yes there are servers in serverless uh

00:04:10,560 --> 00:04:13,200
it's more or less a meaningless term in

00:04:12,239 --> 00:04:15,599
many ways

00:04:13,200 --> 00:04:16,479
but the gist of it what we often try to

00:04:15,599 --> 00:04:18,320
get across

00:04:16,479 --> 00:04:20,079
when we talk about serverless is

00:04:18,320 --> 00:04:22,880
enabling developers

00:04:20,079 --> 00:04:23,919
enabling users to focus as much as

00:04:22,880 --> 00:04:26,880
possible on writing

00:04:23,919 --> 00:04:28,000
their code in the other aspects of

00:04:26,880 --> 00:04:31,280
shipping and deploying

00:04:28,000 --> 00:04:34,160
software are taken care of by your

00:04:31,280 --> 00:04:34,720
form by your cloud vendor by something

00:04:34,160 --> 00:04:36,240
else

00:04:34,720 --> 00:04:37,840
so you're focusing on the layers that

00:04:36,240 --> 00:04:41,280
matter your individual

00:04:37,840 --> 00:04:43,120
ip your differentiated tech in the other

00:04:41,280 --> 00:04:43,600
elements like patching the operating

00:04:43,120 --> 00:04:46,160
system

00:04:43,600 --> 00:04:47,280
scaling managing capacity managing

00:04:46,160 --> 00:04:49,520
resources

00:04:47,280 --> 00:04:50,960
those aren't in the full of the mind of

00:04:49,520 --> 00:04:53,520
the developer

00:04:50,960 --> 00:04:55,440
now part of that in something your

00:04:53,520 --> 00:04:56,639
function services if i talk about you

00:04:55,440 --> 00:04:58,080
know where i mentioned i have a lot of

00:04:56,639 --> 00:04:59,680
experience

00:04:58,080 --> 00:05:01,840
that means that people are deploying

00:04:59,680 --> 00:05:02,880
these functions to the azure cloud and

00:05:01,840 --> 00:05:04,400
they're saying hey when something

00:05:02,880 --> 00:05:05,360
happens i want you to run my little bit

00:05:04,400 --> 00:05:07,440
of code

00:05:05,360 --> 00:05:08,479
well in order to run their bit of code

00:05:07,440 --> 00:05:10,400
when something happens

00:05:08,479 --> 00:05:12,320
it needs to be event driven they need to

00:05:10,400 --> 00:05:14,080
associate it with some event hey when a

00:05:12,320 --> 00:05:16,160
message gets dropped in a queue

00:05:14,080 --> 00:05:18,160
when an image shows up in the storage

00:05:16,160 --> 00:05:18,880
account when a record is changed in this

00:05:18,160 --> 00:05:22,080
database

00:05:18,880 --> 00:05:23,360
when an http request is made when it's

00:05:22,080 --> 00:05:25,520
friday at 5 00 pm

00:05:23,360 --> 00:05:27,759
an event could be anything but that's

00:05:25,520 --> 00:05:30,160
one of the core elements of serverless

00:05:27,759 --> 00:05:32,080
you also have this aspect of on-demand

00:05:30,160 --> 00:05:33,360
compute because it's event driven you

00:05:32,080 --> 00:05:35,199
can build your applications and

00:05:33,360 --> 00:05:37,360
serverless is that way where

00:05:35,199 --> 00:05:38,400
you only pay for a serverless function

00:05:37,360 --> 00:05:41,759
when it's actually

00:05:38,400 --> 00:05:43,919
running when happens if no employees are

00:05:41,759 --> 00:05:46,479
hired in the month of january

00:05:43,919 --> 00:05:47,440
your new employee serverless functions

00:05:46,479 --> 00:05:49,039
never run

00:05:47,440 --> 00:05:51,520
and you're never charged they never

00:05:49,039 --> 00:05:53,199
consume any cpu or memory

00:05:51,520 --> 00:05:54,720
and so part of that too is the billing

00:05:53,199 --> 00:05:56,240
aspect now i want to introduce these

00:05:54,720 --> 00:05:56,960
concepts now because it's going to weave

00:05:56,240 --> 00:05:59,600
into

00:05:56,960 --> 00:06:01,280
what cada is and what how it brings

00:05:59,600 --> 00:06:02,639
these capabilities

00:06:01,280 --> 00:06:04,720
pretty seamlessly and in a very

00:06:02,639 --> 00:06:07,199
user-friendly way to any kubernetes

00:06:04,720 --> 00:06:09,039
cluster running anywhere on premises in

00:06:07,199 --> 00:06:12,319
the cloud with red hat

00:06:09,039 --> 00:06:14,400
azure doesn't matter now just an example

00:06:12,319 --> 00:06:15,759
of of where we see these event driven

00:06:14,400 --> 00:06:18,400
and serverless patterns used

00:06:15,759 --> 00:06:20,319
these are four big ones i see and that

00:06:18,400 --> 00:06:23,600
you might have seen yourself

00:06:20,319 --> 00:06:24,080
to accomplish automation tasks hey when

00:06:23,600 --> 00:06:26,000
a

00:06:24,080 --> 00:06:27,440
pull request is submitted i need to run

00:06:26,000 --> 00:06:29,680
some checks when

00:06:27,440 --> 00:06:30,800
uh you know friday at 5 pm i need to

00:06:29,680 --> 00:06:32,960
re-index my

00:06:30,800 --> 00:06:34,319
anything that maybe manually i'm having

00:06:32,960 --> 00:06:35,039
to do now that you would love to

00:06:34,319 --> 00:06:37,280
automate

00:06:35,039 --> 00:06:38,720
it's often a great ask for serverless or

00:06:37,280 --> 00:06:40,400
event driven

00:06:38,720 --> 00:06:42,080
often you'll hear about serverless being

00:06:40,400 --> 00:06:45,600
used as the glue

00:06:42,080 --> 00:06:48,720
between different services i have my

00:06:45,600 --> 00:06:51,120
postgres database here and i have my

00:06:48,720 --> 00:06:53,199
file store over there and i need them to

00:06:51,120 --> 00:06:55,520
be able to talk to each other somehow

00:06:53,199 --> 00:06:57,039
and if there's not a direct bridge i can

00:06:55,520 --> 00:06:58,720
stick in a serverless function

00:06:57,039 --> 00:07:00,880
and say like hey whenever data gets

00:06:58,720 --> 00:07:02,240
changed here transform it and write a

00:07:00,880 --> 00:07:05,120
record over there

00:07:02,240 --> 00:07:06,639
building very rapid apis especially apis

00:07:05,120 --> 00:07:08,479
that you want to be able to scale

00:07:06,639 --> 00:07:10,160
in that event driven way and then

00:07:08,479 --> 00:07:12,639
finally event streams

00:07:10,160 --> 00:07:14,400
whether it's a kafka stream an event hub

00:07:12,639 --> 00:07:16,720
stream a kinesis stream

00:07:14,400 --> 00:07:17,759
having streams of data from something

00:07:16,720 --> 00:07:20,479
like an iot

00:07:17,759 --> 00:07:22,319
device or from system telemetry another

00:07:20,479 --> 00:07:23,919
great candidate for event driven and

00:07:22,319 --> 00:07:26,720
serverless because you have these

00:07:23,919 --> 00:07:28,240
iot events the temperature is this uh

00:07:26,720 --> 00:07:31,039
someone has entered the room

00:07:28,240 --> 00:07:33,199
pop run your event driven code in a real

00:07:31,039 --> 00:07:35,520
world use case that we've seen of this

00:07:33,199 --> 00:07:37,199
is something like in retail and so i

00:07:35,520 --> 00:07:38,479
pulled a slide from a prediction of

00:07:37,199 --> 00:07:40,240
one of our retail partners that they

00:07:38,479 --> 00:07:42,000
gave it the java one conference

00:07:40,240 --> 00:07:43,360
where they showed their architecture of

00:07:42,000 --> 00:07:44,400
how they're using event driven in

00:07:43,360 --> 00:07:45,919
serverless

00:07:44,400 --> 00:07:47,199
and this will kind of give you the idea

00:07:45,919 --> 00:07:48,080
of the types of scenarios folks are

00:07:47,199 --> 00:07:50,080
building so

00:07:48,080 --> 00:07:51,759
this is a massive retailer they need to

00:07:50,080 --> 00:07:52,720
be able to scale the billions of

00:07:51,759 --> 00:07:54,639
transactions

00:07:52,720 --> 00:07:56,720
on an event like black friday or the

00:07:54,639 --> 00:07:59,199
holiday season that just happened

00:07:56,720 --> 00:07:59,840
and so they have these individual event

00:07:59,199 --> 00:08:01,759
driven

00:07:59,840 --> 00:08:03,120
functions in their case to say like hey

00:08:01,759 --> 00:08:05,039
someone places an order

00:08:03,120 --> 00:08:06,960
i need to run this process function i

00:08:05,039 --> 00:08:09,039
need to then run this notify function i

00:08:06,960 --> 00:08:11,280
need to run this persist function

00:08:09,039 --> 00:08:12,960
a few capabilities that were important

00:08:11,280 --> 00:08:14,000
for their serverless platform

00:08:12,960 --> 00:08:16,479
is they need to make sure it was

00:08:14,000 --> 00:08:19,360
resilient if something like an error

00:08:16,479 --> 00:08:21,360
occurs that kafka stream needs to be

00:08:19,360 --> 00:08:24,000
able to replay those messages

00:08:21,360 --> 00:08:25,199
losing a transaction not getting the

00:08:24,000 --> 00:08:28,000
item that was ordered

00:08:25,199 --> 00:08:29,280
actually shipped is a really big deal so

00:08:28,000 --> 00:08:30,240
you need to have some resilience here

00:08:29,280 --> 00:08:32,880
you need to be able to use

00:08:30,240 --> 00:08:35,200
like a check pointing if you're familiar

00:08:32,880 --> 00:08:36,479
with event streaming terminology

00:08:35,200 --> 00:08:38,080
they really cared about cost

00:08:36,479 --> 00:08:39,599
optimizations you can see here they're

00:08:38,080 --> 00:08:41,360
using things like kubernetes and

00:08:39,599 --> 00:08:42,479
openshift in addition to some azure

00:08:41,360 --> 00:08:44,880
cloud functions

00:08:42,479 --> 00:08:46,720
they want to make sure that in fact this

00:08:44,880 --> 00:08:48,160
partner they're tracking things like the

00:08:46,720 --> 00:08:50,399
cost per order

00:08:48,160 --> 00:08:51,200
you don't want to be spending cpu cycles

00:08:50,399 --> 00:08:53,839
or running

00:08:51,200 --> 00:08:55,040
events or scaling when you don't need it

00:08:53,839 --> 00:08:57,279
right you don't want to be running at

00:08:55,040 --> 00:08:59,200
black friday capacity in a black friday

00:08:57,279 --> 00:09:00,080
load all of the time you want to be very

00:08:59,200 --> 00:09:02,160
smart

00:09:00,080 --> 00:09:04,240
about when and how you're using those

00:09:02,160 --> 00:09:05,440
precious compute resources

00:09:04,240 --> 00:09:07,440
and then finally they had some more

00:09:05,440 --> 00:09:08,959
advanced patterns too we'll get to this

00:09:07,440 --> 00:09:09,920
maybe a little bit further into the

00:09:08,959 --> 00:09:12,080
presentation

00:09:09,920 --> 00:09:13,680
but things like the order that those

00:09:12,080 --> 00:09:16,720
events get processed in

00:09:13,680 --> 00:09:19,200
so order made order

00:09:16,720 --> 00:09:21,279
sold it's very important that the

00:09:19,200 --> 00:09:22,880
functions that the serverless pieces of

00:09:21,279 --> 00:09:24,640
code that were running this

00:09:22,880 --> 00:09:26,880
grab those in the right order you can't

00:09:24,640 --> 00:09:29,120
just throw them out and hope

00:09:26,880 --> 00:09:30,800
that the order canceled is processed

00:09:29,120 --> 00:09:34,000
after the order was made

00:09:30,800 --> 00:09:35,360
uh event so a few patterns that we see

00:09:34,000 --> 00:09:38,000
from time to time

00:09:35,360 --> 00:09:39,760
um in a few kind of an example use case

00:09:38,000 --> 00:09:40,959
in this case in retail

00:09:39,760 --> 00:09:42,720
though you could see similar

00:09:40,959 --> 00:09:43,760
architectures whether it's in finance

00:09:42,720 --> 00:09:47,279
healthcare

00:09:43,760 --> 00:09:49,120
uh you name it i t so

00:09:47,279 --> 00:09:50,720
the one thing out here and it's kind of

00:09:49,120 --> 00:09:52,560
the history of cada

00:09:50,720 --> 00:09:54,000
is now we're kind of we've had an intro

00:09:52,560 --> 00:09:55,600
into event driven in serverless we've

00:09:54,000 --> 00:09:58,720
seen an example architecture

00:09:55,600 --> 00:10:00,399
what what about cada well one of the

00:09:58,720 --> 00:10:02,640
ways that this came about is

00:10:00,399 --> 00:10:03,839
from our learnings in azure of running

00:10:02,640 --> 00:10:05,279
serverless services like the

00:10:03,839 --> 00:10:06,720
architecture i've shown

00:10:05,279 --> 00:10:08,399
more and more we saw that people are

00:10:06,720 --> 00:10:09,120
interested in running these event driven

00:10:08,399 --> 00:10:10,800
workloads

00:10:09,120 --> 00:10:12,480
not necessarily using the azure

00:10:10,800 --> 00:10:13,040
serverless service or any serverless

00:10:12,480 --> 00:10:14,560
service

00:10:13,040 --> 00:10:15,920
maybe they wanted to run on premises

00:10:14,560 --> 00:10:16,560
maybe they wanted a little bit more

00:10:15,920 --> 00:10:18,079
control

00:10:16,560 --> 00:10:20,959
maybe they're just unifying their

00:10:18,079 --> 00:10:23,680
strategy around things like kubernetes

00:10:20,959 --> 00:10:25,440
and what we saw was that the default

00:10:23,680 --> 00:10:28,079
kubernetes scaling

00:10:25,440 --> 00:10:29,120
was not optimized for event driven

00:10:28,079 --> 00:10:31,040
applications

00:10:29,120 --> 00:10:32,320
so how it usually works in kubernetes

00:10:31,040 --> 00:10:34,320
for those of you who are familiar

00:10:32,320 --> 00:10:36,320
is i'll have my application let's say i

00:10:34,320 --> 00:10:38,320
order a processing application

00:10:36,320 --> 00:10:40,640
and i'll deploy it to kubernetes what

00:10:38,320 --> 00:10:43,120
kubernetes by default is going to watch

00:10:40,640 --> 00:10:45,279
are what's called resource metrics

00:10:43,120 --> 00:10:47,200
that's things like how much cpu

00:10:45,279 --> 00:10:48,399
is this consuming and how much memory is

00:10:47,200 --> 00:10:50,240
this consuming

00:10:48,399 --> 00:10:52,000
and as it watches those it will be like

00:10:50,240 --> 00:10:54,240
well the app you publish

00:10:52,000 --> 00:10:55,440
seems to be using a lot of its cpu it's

00:10:54,240 --> 00:10:57,760
at like 80

00:10:55,440 --> 00:10:59,120
i don't know why it's using cpu but it

00:10:57,760 --> 00:11:00,880
seems to be using a lot

00:10:59,120 --> 00:11:02,480
so i'm going to scale it and then i'm

00:11:00,880 --> 00:11:04,959
going to wait and i'm going to walk

00:11:02,480 --> 00:11:06,640
see if it's still using a lot of its cpu

00:11:04,959 --> 00:11:09,040
it's very reactive it's looking at the

00:11:06,640 --> 00:11:11,120
symptoms of what's happening it's ccp

00:11:09,040 --> 00:11:13,120
rising it doesn't know how it doesn't

00:11:11,120 --> 00:11:14,720
know why it doesn't know how long the

00:11:13,120 --> 00:11:16,959
cpu is pricing for

00:11:14,720 --> 00:11:18,720
and it's making the best decision it can

00:11:16,959 --> 00:11:22,160
based on that information

00:11:18,720 --> 00:11:24,000
now that's fine but it's not optimal

00:11:22,160 --> 00:11:25,839
in running a service like azure

00:11:24,000 --> 00:11:28,399
functions a serverless service

00:11:25,839 --> 00:11:30,160
we use what i call proactive scaling

00:11:28,399 --> 00:11:32,880
event driven scaling

00:11:30,160 --> 00:11:33,920
where rather than looking at the cpu for

00:11:32,880 --> 00:11:36,160
a function

00:11:33,920 --> 00:11:37,920
we look at the event source we look at

00:11:36,160 --> 00:11:38,640
the thing that is triggering your

00:11:37,920 --> 00:11:40,480
function

00:11:38,640 --> 00:11:43,040
so if this is an order processing

00:11:40,480 --> 00:11:45,440
function and you have a kafka stream

00:11:43,040 --> 00:11:46,399
we're going to look at that kafka stream

00:11:45,440 --> 00:11:49,360
we're going to say

00:11:46,399 --> 00:11:51,120
hey there are a million messages here

00:11:49,360 --> 00:11:52,079
someone just dropped a million messages

00:11:51,120 --> 00:11:54,639
in kafka

00:11:52,079 --> 00:11:56,000
we need to scale right now like it

00:11:54,639 --> 00:11:57,360
doesn't even necessarily matter what the

00:11:56,000 --> 00:11:58,959
cpu and the memory are

00:11:57,360 --> 00:12:00,880
there are a lot of events that are

00:11:58,959 --> 00:12:02,720
coming in so let's scale you out to

00:12:00,880 --> 00:12:04,480
process those events that's that event

00:12:02,720 --> 00:12:06,959
driven scaling that we're talking about

00:12:04,480 --> 00:12:08,399
it's much more rapid uh you're scaling

00:12:06,959 --> 00:12:10,000
on the cause you're scaling on the

00:12:08,399 --> 00:12:12,079
actual events that are happening

00:12:10,000 --> 00:12:13,760
and not just on the symptom it also

00:12:12,079 --> 00:12:15,440
enables you to do things like scaling

00:12:13,760 --> 00:12:18,079
all the way to zero

00:12:15,440 --> 00:12:19,519
because if there's no messages on that

00:12:18,079 --> 00:12:20,880
stream of data

00:12:19,519 --> 00:12:22,639
you don't need to be running that

00:12:20,880 --> 00:12:23,200
application at all there's no work to be

00:12:22,639 --> 00:12:24,800
done

00:12:23,200 --> 00:12:26,639
so event driven skill also lets you

00:12:24,800 --> 00:12:29,200
scale all the way down to zero

00:12:26,639 --> 00:12:30,240
so let's show you a little bit of an

00:12:29,200 --> 00:12:33,200
overview of cada

00:12:30,240 --> 00:12:34,480
so keda was born this was a partnership

00:12:33,200 --> 00:12:37,200
initially between

00:12:34,480 --> 00:12:38,480
microsoft azure and red hat we kind of

00:12:37,200 --> 00:12:40,079
huddled together talked about some of

00:12:38,480 --> 00:12:42,079
these problems and and worked together

00:12:40,079 --> 00:12:43,600
to to build the initial release of cada

00:12:42,079 --> 00:12:46,079
that was released a little over a year

00:12:43,600 --> 00:12:48,079
and a half ago it's currently a cncf

00:12:46,079 --> 00:12:49,279
sandbox project so this is full open

00:12:48,079 --> 00:12:50,800
governance i mentioned i work at

00:12:49,279 --> 00:12:53,040
microsoft but cada

00:12:50,800 --> 00:12:54,399
is not a microsoft project it's a cncf

00:12:53,040 --> 00:12:56,480
sandbox project

00:12:54,399 --> 00:12:58,800
it's going to monitor the rates of

00:12:56,480 --> 00:13:01,120
events happening in your system

00:12:58,800 --> 00:13:03,040
enable you to proactively scale any of

00:13:01,120 --> 00:13:05,200
your containers any of your apps

00:13:03,040 --> 00:13:07,519
you can say cada go scale this thing

00:13:05,200 --> 00:13:10,639
using event driven auto scaling

00:13:07,519 --> 00:13:11,920
it feeds the data into kubernetes apis

00:13:10,639 --> 00:13:13,600
you'll see in a second when we walk

00:13:11,920 --> 00:13:14,880
through how cata works but it's very

00:13:13,600 --> 00:13:17,600
non-intrusive

00:13:14,880 --> 00:13:18,560
it's a single purpose component it only

00:13:17,600 --> 00:13:20,560
does one thing

00:13:18,560 --> 00:13:21,600
event driven auto scaling but it does it

00:13:20,560 --> 00:13:23,200
really well

00:13:21,600 --> 00:13:25,040
uh and it integrates with what

00:13:23,200 --> 00:13:26,639
kubernetes already provides we didn't

00:13:25,040 --> 00:13:29,120
want to reinvent

00:13:26,639 --> 00:13:30,639
kubernetes we just want you to say use

00:13:29,120 --> 00:13:31,920
this with kubernetes you're going to

00:13:30,639 --> 00:13:34,880
find a lot of value from

00:13:31,920 --> 00:13:36,800
it lets you scale to zero and back out

00:13:34,880 --> 00:13:39,600
of zero so you can really efficiently

00:13:36,800 --> 00:13:40,639
use your cpus in your cores you can add

00:13:39,600 --> 00:13:42,480
it to any cluster

00:13:40,639 --> 00:13:43,839
a new cluster you could have an existing

00:13:42,480 --> 00:13:44,399
cluster that you've already been running

00:13:43,839 --> 00:13:46,639
for

00:13:44,399 --> 00:13:48,320
years or months you can pop data in

00:13:46,639 --> 00:13:49,839
there it's not going to get in the way

00:13:48,320 --> 00:13:51,839
it won't do anything until you tell it

00:13:49,839 --> 00:13:54,160
to and one of the great things

00:13:51,839 --> 00:13:56,079
about cada is that it knows about all of

00:13:54,160 --> 00:13:58,160
these different event sources

00:13:56,079 --> 00:14:00,639
so keda knows how to monitor events from

00:13:58,160 --> 00:14:02,720
kafka from rabbitmq from promisius

00:14:00,639 --> 00:14:05,040
from a number of cloud services like

00:14:02,720 --> 00:14:07,120
azure queues aws gcp

00:14:05,040 --> 00:14:08,079
i think at this point there's like 33

00:14:07,120 --> 00:14:10,480
event sources

00:14:08,079 --> 00:14:11,839
it's extensible as well so it can really

00:14:10,480 --> 00:14:14,000
monitor anything

00:14:11,839 --> 00:14:15,199
uh but it's super easy and efficient to

00:14:14,000 --> 00:14:17,519
use there

00:14:15,199 --> 00:14:18,880
and just a quick answer to one question

00:14:17,519 --> 00:14:21,120
uh that uh

00:14:18,880 --> 00:14:22,639
that i'll do cncf and thank you it's a

00:14:21,120 --> 00:14:24,399
good one this is the cloud native

00:14:22,639 --> 00:14:26,160
compute foundation

00:14:24,399 --> 00:14:28,320
uh so this is a open governance

00:14:26,160 --> 00:14:30,000
foundation uh sponsored by a lot of

00:14:28,320 --> 00:14:32,959
large companies like microsoft aws

00:14:30,000 --> 00:14:35,279
red hat you name it um that that is a

00:14:32,959 --> 00:14:37,040
safe spot for open government projects

00:14:35,279 --> 00:14:40,240
it's actually where kubernetes itself

00:14:37,040 --> 00:14:41,600
is hosted is out of this ncf uh so

00:14:40,240 --> 00:14:44,000
a great foundation to be a part of we

00:14:41,600 --> 00:14:46,800
were thrilled when we became a sandbox

00:14:44,000 --> 00:14:48,399
project uh okay so there's a few

00:14:46,800 --> 00:14:49,279
questions around how keto works i figure

00:14:48,399 --> 00:14:51,680
at this point

00:14:49,279 --> 00:14:53,120
i'm gonna show a quick demo and then

00:14:51,680 --> 00:14:54,160
we'll walk through what happens behind

00:14:53,120 --> 00:14:55,360
the scenes i think this is actually

00:14:54,160 --> 00:14:57,519
gonna answer

00:14:55,360 --> 00:14:58,959
a lot of the questions uh that i see

00:14:57,519 --> 00:15:01,519
coming in so

00:14:58,959 --> 00:15:02,160
i have here a quick scenario that i want

00:15:01,519 --> 00:15:04,560
to show you all

00:15:02,160 --> 00:15:06,000
and i see this little bar up here i

00:15:04,560 --> 00:15:07,920
don't think you can see it but i'm going

00:15:06,000 --> 00:15:10,240
to move it out of the way just

00:15:07,920 --> 00:15:11,040
so i'm showing here on my desktop

00:15:10,240 --> 00:15:14,399
console

00:15:11,040 --> 00:15:15,600
uh i have a kubernetes cluster that is

00:15:14,399 --> 00:15:16,959
up and running

00:15:15,600 --> 00:15:18,320
if you're not familiar with kubernetes

00:15:16,959 --> 00:15:18,880
that's okay i'm gonna walk you through

00:15:18,320 --> 00:15:20,160
this there's

00:15:18,880 --> 00:15:22,079
you don't have to know about kubernetes

00:15:20,160 --> 00:15:23,920
to follow along here in the story

00:15:22,079 --> 00:15:26,120
what i have here in this kubernetes

00:15:23,920 --> 00:15:29,360
cluster is a

00:15:26,120 --> 00:15:30,959
rabbitmq so this is just a message queue

00:15:29,360 --> 00:15:32,800
and it's totally empty right now there

00:15:30,959 --> 00:15:34,959
are no messages on that queue

00:15:32,800 --> 00:15:35,920
and what i want to publish is an

00:15:34,959 --> 00:15:38,399
application

00:15:35,920 --> 00:15:40,560
that will hold messages from that queue

00:15:38,399 --> 00:15:42,320
so maybe this is my order process

00:15:40,560 --> 00:15:44,079
function whenever something drops to

00:15:42,320 --> 00:15:46,240
this queue i want to process a message

00:15:44,079 --> 00:15:47,600
okay and i i don't think i actually have

00:15:46,240 --> 00:15:48,800
that application there now i think i

00:15:47,600 --> 00:15:49,279
need to deploy but i just want to make

00:15:48,800 --> 00:15:51,360
sure

00:15:49,279 --> 00:15:53,279
okay yeah so nothing there's nothing

00:15:51,360 --> 00:15:56,480
here i haven't deployed any apps

00:15:53,279 --> 00:15:59,440
okay so what i have over

00:15:56,480 --> 00:16:00,720
in this window uh i think it's right

00:15:59,440 --> 00:16:03,839
here

00:16:00,720 --> 00:16:05,839
i have a simple uh

00:16:03,839 --> 00:16:07,680
piece of go code that i want to run and

00:16:05,839 --> 00:16:08,079
this is just a simple container that's

00:16:07,680 --> 00:16:11,120
going to

00:16:08,079 --> 00:16:12,240
talk to rabbitmq and pull the messages

00:16:11,120 --> 00:16:13,759
doesn't matter you don't need to worry

00:16:12,240 --> 00:16:15,360
about knowing what this code does it's

00:16:13,759 --> 00:16:17,040
it's just the hello world for pulling a

00:16:15,360 --> 00:16:19,440
message from a queue

00:16:17,040 --> 00:16:20,160
and i'm going to now deploy that to my

00:16:19,440 --> 00:16:22,160
cluster

00:16:20,160 --> 00:16:24,320
okay so i'm going to do a kubernetes

00:16:22,160 --> 00:16:25,440
deployment this is how i defined it i'm

00:16:24,320 --> 00:16:27,519
saying hey i have this thing that's

00:16:25,440 --> 00:16:29,519
going to pull rabbitmq messages nothing

00:16:27,519 --> 00:16:31,759
here is different yes i do have my

00:16:29,519 --> 00:16:33,600
username and password in plain text

00:16:31,759 --> 00:16:34,880
but that's okay it's a local one you

00:16:33,600 --> 00:16:37,120
can't do much with it

00:16:34,880 --> 00:16:39,040
we'll get to this in a little bit um but

00:16:37,120 --> 00:16:41,920
as part of this deployment

00:16:39,040 --> 00:16:43,199
i'm giving it some metadata to tell kada

00:16:41,920 --> 00:16:45,519
hey by the way

00:16:43,199 --> 00:16:47,519
you know this thing i'm about to deploy

00:16:45,519 --> 00:16:48,639
it cares about messages coming from

00:16:47,519 --> 00:16:50,959
rabbitmq

00:16:48,639 --> 00:16:52,079
and i want you to this using event

00:16:50,959 --> 00:16:54,000
driven scaling

00:16:52,079 --> 00:16:55,680
based on rabbitmq so i'm telling it the

00:16:54,000 --> 00:16:56,160
deployment like hey i'm going to deploy

00:16:55,680 --> 00:17:00,079
this

00:16:56,160 --> 00:17:01,040
grab an mq and i want you to monitor and

00:17:00,079 --> 00:17:02,720
scale it

00:17:01,040 --> 00:17:04,799
there's even this other metadata i can

00:17:02,720 --> 00:17:05,360
provide where i can more securely handle

00:17:04,799 --> 00:17:07,120
secrets

00:17:05,360 --> 00:17:09,120
now i'm only half using it as you can

00:17:07,120 --> 00:17:12,720
see here but you get the idea

00:17:09,120 --> 00:17:14,799
uh so short answer is i'm gonna deploy

00:17:12,720 --> 00:17:16,240
something that triggers and listens to

00:17:14,799 --> 00:17:19,199
rabbitmq messages

00:17:16,240 --> 00:17:19,839
and i'm telling kubernetes let keda do

00:17:19,199 --> 00:17:22,319
it

00:17:19,839 --> 00:17:23,760
okay so let's run that in there now so

00:17:22,319 --> 00:17:26,799
i'm going to add that

00:17:23,760 --> 00:17:29,120
to my cluster

00:17:26,799 --> 00:17:30,160
right here so now i'm telling kubernetes

00:17:29,120 --> 00:17:32,640
hey uh

00:17:30,160 --> 00:17:33,840
go go about my container that pulls

00:17:32,640 --> 00:17:37,039
rabbitmq

00:17:33,840 --> 00:17:38,320
this pier's been cool if i look at

00:17:37,039 --> 00:17:40,960
kubernetes now

00:17:38,320 --> 00:17:42,960
and i say tell me about your deployments

00:17:40,960 --> 00:17:44,559
i have my consumer running

00:17:42,960 --> 00:17:46,240
but what i mean when i say skill to zero

00:17:44,559 --> 00:17:48,880
it's not actually running

00:17:46,240 --> 00:17:51,200
it knows about my container but it's not

00:17:48,880 --> 00:17:52,720
running it's not consuming any cpu it's

00:17:51,200 --> 00:17:55,120
not consuming any memory it's not

00:17:52,720 --> 00:17:57,200
consuming any container spots because

00:17:55,120 --> 00:17:59,200
kata knows that there's nothing on that

00:17:57,200 --> 00:18:01,039
queue so why should i run this container

00:17:59,200 --> 00:18:03,120
right now well let's change that

00:18:01,039 --> 00:18:04,320
let's actually go ahead in this other

00:18:03,120 --> 00:18:05,760
tab i'm going to

00:18:04,320 --> 00:18:07,840
watch we're going to watch this happen

00:18:05,760 --> 00:18:08,720
in real time let's watch the containers

00:18:07,840 --> 00:18:10,559
that are running

00:18:08,720 --> 00:18:12,400
so i only have one container in my whole

00:18:10,559 --> 00:18:15,840
cluster right now and that's the actual

00:18:12,400 --> 00:18:17,760
queue itself so i'm going to deploy

00:18:15,840 --> 00:18:19,520
something that's going to drop i think a

00:18:17,760 --> 00:18:22,640
thousand messages

00:18:19,520 --> 00:18:24,080
to this so i'm just going to publish a

00:18:22,640 --> 00:18:25,200
thousand messages to the queue and let's

00:18:24,080 --> 00:18:27,120
see what happens

00:18:25,200 --> 00:18:28,640
so i'm going to run that if i come here

00:18:27,120 --> 00:18:30,000
we should see the container that's going

00:18:28,640 --> 00:18:30,559
to spin up and publish all these

00:18:30,000 --> 00:18:32,400
messages

00:18:30,559 --> 00:18:34,640
but you'll notice actually look at all

00:18:32,400 --> 00:18:37,520
these consumers that are spinning up

00:18:34,640 --> 00:18:38,160
as soon as i start publishing those

00:18:37,520 --> 00:18:42,080
thousand

00:18:38,160 --> 00:18:44,160
q messages heda realizes oh there's

00:18:42,080 --> 00:18:45,520
actually work to be done now and very

00:18:44,160 --> 00:18:47,120
quickly hopefully you saw it if you

00:18:45,520 --> 00:18:50,080
blinked you might have missed it

00:18:47,120 --> 00:18:50,880
i went from zero instances to four

00:18:50,080 --> 00:18:53,200
instances

00:18:50,880 --> 00:18:54,320
now i'm spinning up to eight instances

00:18:53,200 --> 00:18:57,280
this is much

00:18:54,320 --> 00:18:59,120
faster and more reactive than something

00:18:57,280 --> 00:19:01,600
like kubernetes default scaling

00:18:59,120 --> 00:19:03,919
cpu hadn't even been hit before i

00:19:01,600 --> 00:19:05,200
realized oh there are a lot of messages

00:19:03,919 --> 00:19:06,960
to be processed here i might have

00:19:05,200 --> 00:19:08,480
cranked this up to ten thousand messages

00:19:06,960 --> 00:19:10,320
to make the demo more apparent

00:19:08,480 --> 00:19:12,960
but you can see what keta is doing here

00:19:10,320 --> 00:19:14,640
it's scaling out very rapidly

00:19:12,960 --> 00:19:16,480
i've configured it to scale out rapidly

00:19:14,640 --> 00:19:18,559
you can actually kind of scale it down

00:19:16,480 --> 00:19:20,559
and tell it not to go too far too fast

00:19:18,559 --> 00:19:22,799
but i wanted to blow your minds here so

00:19:20,559 --> 00:19:24,320
i've told it scale to the moon

00:19:22,799 --> 00:19:26,320
but this is scaling a lot like a

00:19:24,320 --> 00:19:28,960
serverless function does in the cloud

00:19:26,320 --> 00:19:30,640
in this case in my kubernetes cluster if

00:19:28,960 --> 00:19:32,160
i waited here for a little bit longer we

00:19:30,640 --> 00:19:34,480
actually might wait long enough

00:19:32,160 --> 00:19:35,280
uh what will end up happening is once

00:19:34,480 --> 00:19:37,520
the queue

00:19:35,280 --> 00:19:39,120
is drained once i've emptied that queue

00:19:37,520 --> 00:19:40,960
i processed all the messages

00:19:39,120 --> 00:19:42,480
kate is going to scale it back it's

00:19:40,960 --> 00:19:43,200
going to say okay we did all the work we

00:19:42,480 --> 00:19:45,360
needed to do

00:19:43,200 --> 00:19:46,960
oh wow it's happening as if it's

00:19:45,360 --> 00:19:49,280
listening to me in real time

00:19:46,960 --> 00:19:50,320
the queue is empty cada realizes there's

00:19:49,280 --> 00:19:52,160
no more work to be done

00:19:50,320 --> 00:19:54,160
scaling me back down to zero i don't

00:19:52,160 --> 00:19:56,400
need those cpus anymore

00:19:54,160 --> 00:19:58,400
uh so i've i've had this very serverless

00:19:56,400 --> 00:20:00,640
scale-like experience

00:19:58,400 --> 00:20:01,679
in this case on kubernetes with rabbit

00:20:00,640 --> 00:20:03,760
iq

00:20:01,679 --> 00:20:04,960
okay so let's walk through what happened

00:20:03,760 --> 00:20:07,919
behind the scenes and then i'll pause

00:20:04,960 --> 00:20:11,440
there and answer a few questions okay

00:20:07,919 --> 00:20:13,679
all right uh so what what happened

00:20:11,440 --> 00:20:14,960
here let's watch this what was happening

00:20:13,679 --> 00:20:17,919
behind the scenes

00:20:14,960 --> 00:20:19,120
well a few things the first before i

00:20:17,919 --> 00:20:21,440
ever did the demo

00:20:19,120 --> 00:20:23,520
i had deployed what's called the cade

00:20:21,440 --> 00:20:26,240
operator this is a one-time

00:20:23,520 --> 00:20:27,200
install step uh i'll actually show you

00:20:26,240 --> 00:20:30,159
what it looks like so

00:20:27,200 --> 00:20:31,679
hey get me the pods i have a special

00:20:30,159 --> 00:20:34,559
name space for it

00:20:31,679 --> 00:20:36,320
i've deployed here with this is a really

00:20:34,559 --> 00:20:39,760
smooth bit i susp

00:20:36,320 --> 00:20:40,320
in go entirely and go very tiny little

00:20:39,760 --> 00:20:43,039
operator

00:20:40,320 --> 00:20:44,640
that just knows how to orchestrate what

00:20:43,039 --> 00:20:47,679
you just saw happen okay

00:20:44,640 --> 00:20:48,159
so i have the keta operator it has all

00:20:47,679 --> 00:20:51,280
these

00:20:48,159 --> 00:20:52,960
what we call scalars scalars are all of

00:20:51,280 --> 00:20:54,480
the different things that keda knows how

00:20:52,960 --> 00:20:57,120
to monitor and scale for

00:20:54,480 --> 00:20:58,080
i'm i just brought over uh the cada

00:20:57,120 --> 00:21:00,080
homepage

00:20:58,080 --> 00:21:02,400
you can just the list as i scroll

00:21:00,080 --> 00:21:04,640
through it these are all scalars

00:21:02,400 --> 00:21:06,640
that keda knows about out of the box so

00:21:04,640 --> 00:21:09,520
these are all the things that it can

00:21:06,640 --> 00:21:11,039
drive and run okay so that's sitting

00:21:09,520 --> 00:21:13,600
there in my cluster

00:21:11,039 --> 00:21:14,400
now these darker blue boxes or gray

00:21:13,600 --> 00:21:16,080
boxes

00:21:14,400 --> 00:21:17,840
these are actually just pieces of

00:21:16,080 --> 00:21:19,280
kubernetes itself this isn't part of the

00:21:17,840 --> 00:21:20,240
cada project this is a part of

00:21:19,280 --> 00:21:22,080
kubernetes

00:21:20,240 --> 00:21:23,440
kubernetes has this thing called the

00:21:22,080 --> 00:21:26,960
metrics api

00:21:23,440 --> 00:21:29,600
which enables you to emit and dis

00:21:26,960 --> 00:21:31,760
and publish specific metrics about your

00:21:29,600 --> 00:21:34,320
application or about your system

00:21:31,760 --> 00:21:37,120
and it has this other thing i annotated

00:21:34,320 --> 00:21:38,960
it is the hpa this is the horizontal pod

00:21:37,120 --> 00:21:40,640
autoscaler this is the default

00:21:38,960 --> 00:21:41,360
kubernetes autoscaler the one that

00:21:40,640 --> 00:21:43,919
scales

00:21:41,360 --> 00:21:45,600
based on cpu and memory it's not a bad

00:21:43,919 --> 00:21:47,600
thing it just doesn't know a whole lot

00:21:45,600 --> 00:21:50,640
about what your code is doing

00:21:47,600 --> 00:21:51,760
and what keta does here is if i oh let

00:21:50,640 --> 00:21:53,840
me switch

00:21:51,760 --> 00:21:55,760
to the to the next part of the animation

00:21:53,840 --> 00:21:57,600
i've got my rabbitmq

00:21:55,760 --> 00:21:59,120
my event source in these slides i'm

00:21:57,600 --> 00:22:01,120
going to switch this to kafka

00:21:59,120 --> 00:22:03,280
they all work the same like the dance

00:22:01,120 --> 00:22:07,120
that's going to happen here is the same

00:22:03,280 --> 00:22:09,600
and i deploy my app along with

00:22:07,120 --> 00:22:11,200
the scaled object you remember i saw

00:22:09,600 --> 00:22:12,159
that briefly this is the little bit of

00:22:11,200 --> 00:22:15,280
metadata

00:22:12,159 --> 00:22:15,919
that tells keda hey the thing i just

00:22:15,280 --> 00:22:18,799
deployed

00:22:15,919 --> 00:22:21,120
might cares about kafka it cares about

00:22:18,799 --> 00:22:23,600
rabbitmq it cares about azure cues

00:22:21,120 --> 00:22:25,760
you need to go listen and escape i have

00:22:23,600 --> 00:22:26,080
a value here called the lag threshold

00:22:25,760 --> 00:22:28,480
this

00:22:26,080 --> 00:22:29,120
is how aggressively it scales so 50

00:22:28,480 --> 00:22:31,120
means

00:22:29,120 --> 00:22:32,400
if there's 50 messages that haven't been

00:22:31,120 --> 00:22:35,120
processed i

00:22:32,400 --> 00:22:36,159
only need one instance of my container

00:22:35,120 --> 00:22:38,159
if i set this to

00:22:36,159 --> 00:22:40,880
one the live threshold the one that

00:22:38,159 --> 00:22:43,919
would mean if i have 50 messages i need

00:22:40,880 --> 00:22:44,320
instances of my container one message

00:22:43,919 --> 00:22:46,159
per

00:22:44,320 --> 00:22:48,159
container is my target it's just a

00:22:46,159 --> 00:22:48,799
target it won't always be honored but it

00:22:48,159 --> 00:22:50,559
kind of

00:22:48,799 --> 00:22:52,240
if i make the number higher i'll scale

00:22:50,559 --> 00:22:54,240
slower if i make the number lower i'll

00:22:52,240 --> 00:22:57,120
scale more aggressively

00:22:54,240 --> 00:22:58,880
i go and pop that deployment my

00:22:57,120 --> 00:22:59,600
kubernetes deployment and the scaled

00:22:58,880 --> 00:23:02,960
object

00:22:59,600 --> 00:23:03,919
description into my cluster cada now

00:23:02,960 --> 00:23:06,080
knows okay

00:23:03,919 --> 00:23:07,280
here's your deployment you care about

00:23:06,080 --> 00:23:11,039
kafka

00:23:07,280 --> 00:23:12,720
now keda starts asking your event source

00:23:11,039 --> 00:23:14,720
how many events are being generated hey

00:23:12,720 --> 00:23:15,200
kafka how many messages here aren't

00:23:14,720 --> 00:23:17,440
processed

00:23:15,200 --> 00:23:18,400
the deployment cares about now when i

00:23:17,440 --> 00:23:20,480
started the demo

00:23:18,400 --> 00:23:21,840
the event source was totally empty so

00:23:20,480 --> 00:23:24,400
keda told the metrics

00:23:21,840 --> 00:23:26,080
api which told the hpa actually in this

00:23:24,400 --> 00:23:28,640
case cada did it all itself

00:23:26,080 --> 00:23:30,720
heda scaled that deployment down to zero

00:23:28,640 --> 00:23:33,200
it said we don't need to run it at all

00:23:30,720 --> 00:23:35,280
now as soon as a message comes in the

00:23:33,200 --> 00:23:38,480
message hits your event source

00:23:35,280 --> 00:23:40,480
cada asks this question how many events

00:23:38,480 --> 00:23:44,159
are being generated it sees that there

00:23:40,480 --> 00:23:47,039
are messages to be processed cada tells

00:23:44,159 --> 00:23:48,960
the hpa hey this deployment has a

00:23:47,039 --> 00:23:51,039
hostages that need to be processed

00:23:48,960 --> 00:23:52,240
kubernetes now goes and does its cool

00:23:51,039 --> 00:23:53,919
scaling thing

00:23:52,240 --> 00:23:55,279
because cada has just made it a lot

00:23:53,919 --> 00:23:58,159
smarter it's told it

00:23:55,279 --> 00:23:59,360
about how many being generated now your

00:23:58,159 --> 00:24:02,320
deployment up

00:23:59,360 --> 00:24:03,200
and it pulls the messages so one note i

00:24:02,320 --> 00:24:05,600
want to call here

00:24:03,200 --> 00:24:06,400
i know there's a lot of info cada

00:24:05,600 --> 00:24:08,960
doesn't

00:24:06,400 --> 00:24:10,080
pull the messages and send the messages

00:24:08,960 --> 00:24:11,840
to your deployment

00:24:10,080 --> 00:24:13,840
kata just make sure that your deployment

00:24:11,840 --> 00:24:15,279
gets scaled and it gets woken up

00:24:13,840 --> 00:24:17,600
it's up to your deployment it's up to

00:24:15,279 --> 00:24:19,120
your app itself to actually go and fetch

00:24:17,600 --> 00:24:19,760
those messages and there's a reason for

00:24:19,120 --> 00:24:21,279
that

00:24:19,760 --> 00:24:22,960
because it means that you can now do

00:24:21,279 --> 00:24:24,640
things like checkpointing and order

00:24:22,960 --> 00:24:27,200
delivery and

00:24:24,640 --> 00:24:28,640
all of the things that whatever event

00:24:27,200 --> 00:24:31,200
source you're using is going to have its

00:24:28,640 --> 00:24:33,840
own semantics to do that

00:24:31,200 --> 00:24:34,320
now this goes on and on until uh as you

00:24:33,840 --> 00:24:35,919
saw

00:24:34,320 --> 00:24:38,080
if i'm actually getting a lot of

00:24:35,919 --> 00:24:38,640
messages if maybe it's not one message

00:24:38,080 --> 00:24:41,440
it's

00:24:38,640 --> 00:24:42,720
thousands cada's telling the coupe's

00:24:41,440 --> 00:24:44,480
auto scaler hey

00:24:42,720 --> 00:24:46,159
i need i need you to scale it a little

00:24:44,480 --> 00:24:47,919
bit more there's a lot of stuff coming

00:24:46,159 --> 00:24:50,559
and you'll see what happened in that

00:24:47,919 --> 00:24:52,159
demo uh okay let me go over these core

00:24:50,559 --> 00:24:52,640
principles and i'll do a quick pass

00:24:52,159 --> 00:24:55,760
through the

00:24:52,640 --> 00:24:57,520
the questions so when we build cada

00:24:55,760 --> 00:24:58,799
and as a maintainer now we have some

00:24:57,520 --> 00:25:00,320
core principles one is

00:24:58,799 --> 00:25:01,840
we didn't want to rebuild anything that

00:25:00,320 --> 00:25:03,440
kubernetes offered itself

00:25:01,840 --> 00:25:05,919
we didn't want to provide our own auto

00:25:03,440 --> 00:25:07,360
scaler kubernetes has an auto scaler we

00:25:05,919 --> 00:25:08,159
just want to figure out how can we

00:25:07,360 --> 00:25:09,600
extend

00:25:08,159 --> 00:25:12,320
that auto scaler how can we make it a

00:25:09,600 --> 00:25:14,159
little bit smarter a little bit faster

00:25:12,320 --> 00:25:15,919
so we only built the pieces that made it

00:25:14,159 --> 00:25:16,720
smarter and faster we didn't rebuild the

00:25:15,919 --> 00:25:19,440
whole thing

00:25:16,720 --> 00:25:20,559
it's very stingle purpose it's not doing

00:25:19,440 --> 00:25:22,240
a lot

00:25:20,559 --> 00:25:23,760
and we want it that way if you want to

00:25:22,240 --> 00:25:25,279
do more than pull in other stuff that

00:25:23,760 --> 00:25:25,919
does other things it's not a service

00:25:25,279 --> 00:25:28,799
mesh

00:25:25,919 --> 00:25:30,080
it's not an event broker it just scales

00:25:28,799 --> 00:25:30,720
based on the number of events that are

00:25:30,080 --> 00:25:33,039
happening

00:25:30,720 --> 00:25:33,840
cato works with any container with any

00:25:33,039 --> 00:25:36,080
workload

00:25:33,840 --> 00:25:36,880
it can scale stateful sets it can scale

00:25:36,080 --> 00:25:39,120
deployments

00:25:36,880 --> 00:25:40,640
it can scale go code it can scale java

00:25:39,120 --> 00:25:43,440
code it can scale

00:25:40,640 --> 00:25:45,520
azure function code it can scale a

00:25:43,440 --> 00:25:47,760
python flash code

00:25:45,520 --> 00:25:49,600
anything works with it uh we want to

00:25:47,760 --> 00:25:50,480
preserve what makes messaging brokers

00:25:49,600 --> 00:25:52,720
powerful

00:25:50,480 --> 00:25:54,559
so there are things like kafka and

00:25:52,720 --> 00:25:56,400
rabbitmq that give you these rich

00:25:54,559 --> 00:25:58,480
semantics for processing messages

00:25:56,400 --> 00:25:59,919
we didn't want to bypass any of that and

00:25:58,480 --> 00:26:03,120
then finally uh

00:25:59,919 --> 00:26:04,320
we want to make kato and built by and

00:26:03,120 --> 00:26:06,000
with the community

00:26:04,320 --> 00:26:08,000
we have bi-weekly stand-ups every

00:26:06,000 --> 00:26:08,880
tuesday that you're all invited to join

00:26:08,000 --> 00:26:11,919
anytime

00:26:08,880 --> 00:26:13,600
kubernetes slack channel on keda we

00:26:11,919 --> 00:26:16,320
wanted to make this general purpose

00:26:13,600 --> 00:26:17,120
not just a microsoft tool or a red hat

00:26:16,320 --> 00:26:20,159
tool

00:26:17,120 --> 00:26:22,080
uh or a code it tool or you name it

00:26:20,159 --> 00:26:24,400
a few folks who are using cada today

00:26:22,080 --> 00:26:26,240
these are just a few of uh our logos

00:26:24,400 --> 00:26:28,799
alibaba cloud has it integrated with

00:26:26,240 --> 00:26:30,559
some of their offerings apache airflow

00:26:28,799 --> 00:26:32,880
we're using it at microsoft for a few

00:26:30,559 --> 00:26:34,480
projects uh a number of folks who have

00:26:32,880 --> 00:26:36,960
been using cada in production

00:26:34,480 --> 00:26:38,159
as well all right so let me take a pause

00:26:36,960 --> 00:26:40,320
here i'm just going to look

00:26:38,159 --> 00:26:41,279
at a few of these questions and see if

00:26:40,320 --> 00:26:43,760
there's anything

00:26:41,279 --> 00:26:45,279
here uh there's one question about

00:26:43,760 --> 00:26:46,559
incoming traffic i'm actually going to

00:26:45,279 --> 00:26:47,360
pause that one and answer it a little

00:26:46,559 --> 00:26:51,279
bit later

00:26:47,360 --> 00:26:53,440
um for time um

00:26:51,279 --> 00:26:55,039
yeah and i think there's one question on

00:26:53,440 --> 00:26:58,159
how we can figure

00:26:55,039 --> 00:26:59,600
the burstiness of the metric so i i

00:26:58,159 --> 00:27:00,880
briefly went over this but let me just

00:26:59,600 --> 00:27:04,799
show you this

00:27:00,880 --> 00:27:08,000
very quickly here if i come over here to

00:27:04,799 --> 00:27:09,760
the kata site again cater. if questions

00:27:08,000 --> 00:27:12,080
i'd recommend come poke around here in

00:27:09,760 --> 00:27:13,279
the docs too after the webinar hang out

00:27:12,080 --> 00:27:14,960
with me for a bit more i've got some

00:27:13,279 --> 00:27:18,399
more stuff i want to share with you

00:27:14,960 --> 00:27:20,000
uh but when i deploy that scaled object

00:27:18,399 --> 00:27:21,760
to tell it to scale deployment

00:27:20,000 --> 00:27:24,000
i've got a few knobs here that i didn't

00:27:21,760 --> 00:27:26,799
go over i can configure the polling

00:27:24,000 --> 00:27:30,080
interview how frequently does kata ask

00:27:26,799 --> 00:27:31,200
kafka if there's messages for my demos i

00:27:30,080 --> 00:27:33,520
made this really low

00:27:31,200 --> 00:27:34,799
because i wanted it to scale quickly but

00:27:33,520 --> 00:27:36,799
that means that i'm

00:27:34,799 --> 00:27:38,559
adding a bit more traffic there cool

00:27:36,799 --> 00:27:39,760
down period is how quickly it can scale

00:27:38,559 --> 00:27:41,840
back down to zero

00:27:39,760 --> 00:27:43,520
i can set minimum maybe i never want to

00:27:41,840 --> 00:27:44,640
scale to zero maybe i always want to

00:27:43,520 --> 00:27:47,200
keep one available

00:27:44,640 --> 00:27:49,120
so i don't need additional latency i can

00:27:47,200 --> 00:27:52,559
control whatever minimum i want 1

00:27:49,120 --> 00:27:54,080
10 50. i can set a map hey that's cool

00:27:52,559 --> 00:27:56,880
if you want to scale to the moon but

00:27:54,080 --> 00:27:58,159
only scale up to 20 because at past 20

00:27:56,880 --> 00:27:59,279
you're gonna start hogging too many

00:27:58,159 --> 00:28:02,399
resources

00:27:59,279 --> 00:28:03,840
i can control these fancy hpa

00:28:02,399 --> 00:28:06,240
metrics i won't go into them now which

00:28:03,840 --> 00:28:08,799
is like how many it adds and how quickly

00:28:06,240 --> 00:28:10,960
it adds so there's a few knobs here

00:28:08,799 --> 00:28:12,080
uh and more i kind of mentioned like the

00:28:10,960 --> 00:28:14,240
lag threshold

00:28:12,080 --> 00:28:16,240
that give me a bit more control over the

00:28:14,240 --> 00:28:17,440
behavior of cada so it's not just

00:28:16,240 --> 00:28:19,120
running wild

00:28:17,440 --> 00:28:20,559
i'm trying to move this window out of

00:28:19,120 --> 00:28:25,279
the way but there

00:28:20,559 --> 00:28:25,279
okay um let's look here

00:28:25,679 --> 00:28:29,760
uh i'm gonna count this one as answered

00:28:27,919 --> 00:28:31,200
hopefully i described it simple enough i

00:28:29,760 --> 00:28:32,080
know a few of you here are are more

00:28:31,200 --> 00:28:33,840
beginner

00:28:32,080 --> 00:28:35,679
but at the very least i want you to at

00:28:33,840 --> 00:28:37,520
least understand kata is helping your

00:28:35,679 --> 00:28:39,200
container scale

00:28:37,520 --> 00:28:40,559
kind of like you imagine a serverless

00:28:39,200 --> 00:28:42,799
function scale

00:28:40,559 --> 00:28:44,000
we did mention cata is written and go we

00:28:42,799 --> 00:28:46,640
talked about what cncf

00:28:44,000 --> 00:28:47,919
stood for uh i talked a little bit more

00:28:46,640 --> 00:28:50,480
about what scaling took

00:28:47,919 --> 00:28:52,720
uh another one of comparing cada to some

00:28:50,480 --> 00:28:57,200
other open source projects like k-native

00:28:52,720 --> 00:29:00,240
uh i planned out better along with hd

00:28:57,200 --> 00:29:01,840
um and this is i'll answer this one now

00:29:00,240 --> 00:29:03,279
actually i'll pause let's move on let's

00:29:01,840 --> 00:29:04,720
move on so and you there

00:29:03,279 --> 00:29:06,480
keep them coming we'll take another

00:29:04,720 --> 00:29:07,679
break and i it's a few i didn't answer

00:29:06,480 --> 00:29:10,320
i'll get to them

00:29:07,679 --> 00:29:11,600
so there's one more arcade pattern i

00:29:10,320 --> 00:29:13,120
want to go over

00:29:11,600 --> 00:29:15,360
before we dig into some of those other

00:29:13,120 --> 00:29:17,600
questions and that's this quest we

00:29:15,360 --> 00:29:18,960
get sometimes which is what about long

00:29:17,600 --> 00:29:22,080
running executions

00:29:18,960 --> 00:29:25,200
you know what by that demo i showed

00:29:22,080 --> 00:29:27,440
how quickly i scaled out to

00:29:25,200 --> 00:29:28,960
you know 50 instances and how quickly i

00:29:27,440 --> 00:29:30,960
scaled back down

00:29:28,960 --> 00:29:32,000
well what if each of those messages

00:29:30,960 --> 00:29:34,240
wasn't something

00:29:32,000 --> 00:29:35,360
small doing a simplification what if it

00:29:34,240 --> 00:29:38,559
was actually like

00:29:35,360 --> 00:29:41,600
the link to a video a three hour video

00:29:38,559 --> 00:29:42,159
that i need to transcode or encode well

00:29:41,600 --> 00:29:43,919
that's

00:29:42,159 --> 00:29:46,559
a lot of work that might take a few

00:29:43,919 --> 00:29:49,039
hours to do how do i make sure that

00:29:46,559 --> 00:29:50,960
cada scales me with scale but it doesn't

00:29:49,039 --> 00:29:52,000
disrupt a long running process and let

00:29:50,960 --> 00:29:55,039
me show you why that

00:29:52,000 --> 00:29:58,320
because cadet or i mean kubernetes

00:29:55,039 --> 00:30:00,399
kubernetes has this entry let's imagine

00:29:58,320 --> 00:30:03,919
i'm scaled out to four

00:30:00,399 --> 00:30:05,760
inspired containers of my app running

00:30:03,919 --> 00:30:08,080
and each of them i've got animated like

00:30:05,760 --> 00:30:10,799
these little progress bars imagine i'm

00:30:08,080 --> 00:30:11,120
encoding a video and like instance one

00:30:10,799 --> 00:30:13,600
is

00:30:11,120 --> 00:30:15,200
barely started instance two is almost

00:30:13,600 --> 00:30:17,200
done maybe it's gonna finish in the next

00:30:15,200 --> 00:30:19,840
minute it's three

00:30:17,200 --> 00:30:21,360
and instance first kind of far somewhere

00:30:19,840 --> 00:30:25,520
in the middle

00:30:21,360 --> 00:30:27,840
naked brunetti looking at

00:30:25,520 --> 00:30:29,440
uh and sorry i'm getting unnoticed i

00:30:27,840 --> 00:30:32,480
might have a bit of a hiccup

00:30:29,440 --> 00:30:32,799
in my uh in my uh video feed so i'm

00:30:32,480 --> 00:30:34,240
gonna

00:30:32,799 --> 00:30:36,480
i'm gonna turn off my camera i'm gonna

00:30:34,240 --> 00:30:40,080
keep hopefully save some bandwidth

00:30:36,480 --> 00:30:41,919
let's imagine that keda tells kubernetes

00:30:40,080 --> 00:30:43,600
hey we actually don't want a message in

00:30:41,919 --> 00:30:45,200
the queue we can probably start to scale

00:30:43,600 --> 00:30:47,200
you down a little bit we might not need

00:30:45,200 --> 00:30:50,240
four instances anymore

00:30:47,200 --> 00:30:51,520
well when twos tries to something down i

00:30:50,240 --> 00:30:52,640
often talk about it's kind of like

00:30:51,520 --> 00:30:57,600
thanos

00:30:52,640 --> 00:30:57,600
where it snaps its fingers uh oh

00:30:58,559 --> 00:31:03,120
i said thanos and siri thought i was

00:31:00,240 --> 00:31:06,640
talking uh it snaps its fingers

00:31:03,120 --> 00:31:08,799
and kubernetes is going to sound

00:31:06,640 --> 00:31:10,960
but it you decide what it's down it's

00:31:08,799 --> 00:31:12,880
going to randomly decide what it scales

00:31:10,960 --> 00:31:14,559
down so you can see here in this case

00:31:12,880 --> 00:31:16,399
it's going to scale down one of those

00:31:14,559 --> 00:31:18,880
things that was almost done

00:31:16,399 --> 00:31:20,399
and you're like no now that's gonna get

00:31:18,880 --> 00:31:22,240
have to restart

00:31:20,399 --> 00:31:23,600
on one of these other instances like i

00:31:22,240 --> 00:31:25,600
have something long running please don't

00:31:23,600 --> 00:31:28,240
scale me down while i'm in the process

00:31:25,600 --> 00:31:30,000
so if it's short-lived that's fine if

00:31:28,240 --> 00:31:33,120
it's long running

00:31:30,000 --> 00:31:35,519
you're gonna have options one is there

00:31:33,120 --> 00:31:37,600
are some apis in kubernetes to tell

00:31:35,519 --> 00:31:38,159
kubernetes like hey hold on give me a

00:31:37,600 --> 00:31:39,600
second

00:31:38,159 --> 00:31:41,679
i know you're trying to scale me down

00:31:39,600 --> 00:31:43,919
i'm stunned in a second

00:31:41,679 --> 00:31:45,760
it's probably intended to really give

00:31:43,919 --> 00:31:46,240
you like a few seconds to clean things

00:31:45,760 --> 00:31:48,000
up

00:31:46,240 --> 00:31:49,840
but we have tried this out and you can

00:31:48,000 --> 00:31:50,320
kind of like do this for hours where

00:31:49,840 --> 00:31:51,919
you're like

00:31:50,320 --> 00:31:53,440
hey kubernetes give me a second and it

00:31:51,919 --> 00:31:54,880
comes back a second later like just give

00:31:53,440 --> 00:31:56,159
me a second you do that for hours it's

00:31:54,880 --> 00:31:57,679
fine

00:31:56,159 --> 00:32:01,039
the other option though is use this

00:31:57,679 --> 00:32:03,840
capability of cada called scaled jobs

00:32:01,039 --> 00:32:04,960
now what scale jobs does is instead of

00:32:03,840 --> 00:32:06,720
scheduling

00:32:04,960 --> 00:32:09,039
containers that are going to be

00:32:06,720 --> 00:32:11,039
constantly pulling those messages

00:32:09,039 --> 00:32:12,320
you create what's called the kubernetes

00:32:11,039 --> 00:32:14,159
job for

00:32:12,320 --> 00:32:15,919
every single message and jobs are

00:32:14,159 --> 00:32:18,799
different than deployments

00:32:15,919 --> 00:32:19,120
because jobs the intent is that wakes up

00:32:18,799 --> 00:32:21,440
it

00:32:19,120 --> 00:32:22,240
runs to completion and then it

00:32:21,440 --> 00:32:24,320
terminates

00:32:22,240 --> 00:32:26,320
right i don't want to terminate my web

00:32:24,320 --> 00:32:26,960
server my web server should keep serving

00:32:26,320 --> 00:32:29,039
traffic

00:32:26,960 --> 00:32:30,720
constantly but a job is something like

00:32:29,039 --> 00:32:32,000
wake up process this data and then

00:32:30,720 --> 00:32:34,480
terminate yourself

00:32:32,000 --> 00:32:35,760
so you can tell kubernetes hey for every

00:32:34,480 --> 00:32:36,960
queue message

00:32:35,760 --> 00:32:39,760
i actually want you to create a

00:32:36,960 --> 00:32:41,919
kubernetes job the job wakes up

00:32:39,760 --> 00:32:43,440
pulls one queue message encodes the

00:32:41,919 --> 00:32:45,440
video terminates

00:32:43,440 --> 00:32:46,960
and then it's done caden is going to

00:32:45,440 --> 00:32:48,640
orchestrate this process for you it's

00:32:46,960 --> 00:32:50,240
going to periodically remove

00:32:48,640 --> 00:32:51,760
completed and failed jobs so it will

00:32:50,240 --> 00:32:53,840
keep them clean

00:32:51,760 --> 00:32:54,880
you can control how many jobs you want

00:32:53,840 --> 00:32:57,279
to have in parallel

00:32:54,880 --> 00:32:58,000
like hey i'm cool transcoding 10 videos

00:32:57,279 --> 00:32:59,600
at a time

00:32:58,000 --> 00:33:01,279
so you can have 10 parallel jobs

00:32:59,600 --> 00:33:03,039
happening at once you can control how

00:33:01,279 --> 00:33:04,480
aggressively it creates those jobs

00:33:03,039 --> 00:33:06,799
there's a lot there so i just want to

00:33:04,480 --> 00:33:08,159
know if the thing if the event driven

00:33:06,799 --> 00:33:09,519
thing you want to do is a little bit

00:33:08,159 --> 00:33:11,360
more long running

00:33:09,519 --> 00:33:12,720
you might be interested in this scaled

00:33:11,360 --> 00:33:15,360
jobs pattern

00:33:12,720 --> 00:33:16,559
other than that skilled deployment one

00:33:15,360 --> 00:33:18,080
that we kind of talked about where it

00:33:16,559 --> 00:33:19,919
just wakes up and keeps pulling and

00:33:18,080 --> 00:33:20,320
pulling and pulling as many messages as

00:33:19,919 --> 00:33:21,840
it can

00:33:20,320 --> 00:33:24,240
so very useful pattern here that i want

00:33:21,840 --> 00:33:24,880
to share another thing i want to cover

00:33:24,240 --> 00:33:27,919
quickly

00:33:24,880 --> 00:33:30,720
uh keda currently is 2.0

00:33:27,919 --> 00:33:31,760
we actually rolled out the 2.0 version

00:33:30,720 --> 00:33:34,720
uh in november

00:33:31,760 --> 00:33:36,159
at kubecon uh so a few things i wanted

00:33:34,720 --> 00:33:37,519
to highlight for those of you who might

00:33:36,159 --> 00:33:40,799
have looked at cada before

00:33:37,519 --> 00:33:41,919
things that are new uh we uh we broke

00:33:40,799 --> 00:33:44,240
out scale job

00:33:41,919 --> 00:33:45,519
as its own custom resource so we always

00:33:44,240 --> 00:33:46,880
supported scaling

00:33:45,519 --> 00:33:48,480
jobs but it's to kind of be like a

00:33:46,880 --> 00:33:49,679
subset of functionality now it's its own

00:33:48,480 --> 00:33:51,279
first class thing

00:33:49,679 --> 00:33:53,039
we let you scale lots of stuff you can

00:33:51,279 --> 00:33:54,960
scale any type of

00:33:53,039 --> 00:33:57,440
resource you could scale a stateful set

00:33:54,960 --> 00:33:58,799
you can scale custom resources like argo

00:33:57,440 --> 00:34:00,000
rollouts if you're using other

00:33:58,799 --> 00:34:02,399
frameworks

00:34:00,000 --> 00:34:04,320
we allow you to define multiple triggers

00:34:02,399 --> 00:34:07,200
on a scaled object and we will scale

00:34:04,320 --> 00:34:10,240
based on the noisiest of the triggers

00:34:07,200 --> 00:34:12,720
we provided a bunch of new scalers we

00:34:10,240 --> 00:34:13,440
enabled better extensibility so that you

00:34:12,720 --> 00:34:16,399
can

00:34:13,440 --> 00:34:17,919
do external scalars maybe you have like

00:34:16,399 --> 00:34:20,000
at microsoft one of the ways we're using

00:34:17,919 --> 00:34:22,320
cada is we have some like internal tools

00:34:20,000 --> 00:34:24,000
called geneva that we use to monitor our

00:34:22,320 --> 00:34:26,079
services you can build a

00:34:24,000 --> 00:34:27,839
geneva scaler that knows how to scale

00:34:26,079 --> 00:34:30,560
based on our own

00:34:27,839 --> 00:34:32,240
proprietary thing uh and just attach it

00:34:30,560 --> 00:34:34,159
to cada just by writing a little bit of

00:34:32,240 --> 00:34:36,240
extensibility code

00:34:34,159 --> 00:34:38,560
we support liveness and readiness probes

00:34:36,240 --> 00:34:40,879
so that cada stays reliable

00:34:38,560 --> 00:34:42,399
we expose some prometheus metrics for

00:34:40,879 --> 00:34:43,839
every scalar so you can have better

00:34:42,399 --> 00:34:45,679
first class monitoring

00:34:43,839 --> 00:34:46,960
more around that 2.0 releases in this

00:34:45,679 --> 00:34:48,079
blog post though if you're curious and

00:34:46,960 --> 00:34:50,720
checking that out

00:34:48,079 --> 00:34:51,919
okay uh let's get him here this is this

00:34:50,720 --> 00:34:52,960
will actually start to answer one of the

00:34:51,919 --> 00:34:56,480
questions here

00:34:52,960 --> 00:34:58,640
um and i'm seeing a few other ones of

00:34:56,480 --> 00:35:00,079
how cada compares to some of these other

00:34:58,640 --> 00:35:02,720
serverless runtimes

00:35:00,079 --> 00:35:04,640
uh including things like k-native is a

00:35:02,720 --> 00:35:06,240
very popular one and a very useful tool

00:35:04,640 --> 00:35:08,160
that we get a lot of questions on

00:35:06,240 --> 00:35:09,920
and the way i want to kind of dive into

00:35:08,160 --> 00:35:13,839
this a little bit is talking some about

00:35:09,920 --> 00:35:16,000
a serverless runtime in addition to cada

00:35:13,839 --> 00:35:16,880
so here's how i want to explain this

00:35:16,000 --> 00:35:20,160
what i'm showing

00:35:16,880 --> 00:35:22,480
here on my screen right now is

00:35:20,160 --> 00:35:23,599
the code required you actually saw this

00:35:22,480 --> 00:35:27,599
a minute ago

00:35:23,599 --> 00:35:30,720
to pull a message from a rabbitmq

00:35:27,599 --> 00:35:31,839
uh so this is the code this is like the

00:35:30,720 --> 00:35:33,760
hello world thing

00:35:31,839 --> 00:35:36,079
i've gotta create a connection i've

00:35:33,760 --> 00:35:37,839
gotta connect to an exchange i need to

00:35:36,079 --> 00:35:39,359
pull in messages from rabbitmq

00:35:37,839 --> 00:35:41,119
it's not a whole lot like this isn't

00:35:39,359 --> 00:35:42,079
terrible uh this will only take you a

00:35:41,119 --> 00:35:44,000
few minutes to write

00:35:42,079 --> 00:35:46,480
but there's code here that i need to

00:35:44,000 --> 00:35:48,160
write to actually connect to that queue

00:35:46,480 --> 00:35:50,079
to pull in the message to make sure that

00:35:48,160 --> 00:35:51,599
i'm retrying it and keeping it resilient

00:35:50,079 --> 00:35:53,839
and doing all those cool things that i

00:35:51,599 --> 00:35:55,440
want to do when i'm using a queue

00:35:53,839 --> 00:35:56,960
now the challenge with this though is

00:35:55,440 --> 00:35:58,800
that that is a bit of code

00:35:56,960 --> 00:36:00,880
and and oftentimes when you think about

00:35:58,800 --> 00:36:02,400
serverless you don't want to have to

00:36:00,880 --> 00:36:05,280
write all of this code like if i'm

00:36:02,400 --> 00:36:06,880
writing an azure function or aws lambda

00:36:05,280 --> 00:36:08,320
when i'm pulling messages from a queue i

00:36:06,880 --> 00:36:10,480
don't have to write all that code in

00:36:08,320 --> 00:36:13,280
fact what a serverless runtime

00:36:10,480 --> 00:36:14,320
will provide is code that looks more

00:36:13,280 --> 00:36:16,480
like this

00:36:14,320 --> 00:36:18,079
this is the azure functions runtime very

00:36:16,480 --> 00:36:19,440
similar code sample could be pulled from

00:36:18,079 --> 00:36:21,760
aws lambda

00:36:19,440 --> 00:36:22,960
google cloud functions you name it it's

00:36:21,760 --> 00:36:24,240
doing the same thing

00:36:22,960 --> 00:36:25,839
the left and the right side of the

00:36:24,240 --> 00:36:27,520
screen are doing the same thing but what

00:36:25,839 --> 00:36:30,640
a serverless runtime

00:36:27,520 --> 00:36:33,839
is going to provide is all of this code

00:36:30,640 --> 00:36:34,960
on the left as part of the runtime like

00:36:33,839 --> 00:36:36,800
you don't have to write it as a

00:36:34,960 --> 00:36:39,200
developer you just write like hey

00:36:36,800 --> 00:36:41,599
i want a cue message when i get that q

00:36:39,200 --> 00:36:44,160
message this is what i want you to do

00:36:41,599 --> 00:36:45,920
so pairing a serverless runtime with

00:36:44,160 --> 00:36:47,839
cada can be very useful

00:36:45,920 --> 00:36:49,680
now there's a few serverless runtimes

00:36:47,839 --> 00:36:50,960
that are open source azure functions is

00:36:49,680 --> 00:36:52,800
one of them for sure

00:36:50,960 --> 00:36:55,040
so you can create an azure functions

00:36:52,800 --> 00:36:56,800
project and you can just write

00:36:55,040 --> 00:36:58,160
you know hey i have this azure function

00:36:56,800 --> 00:36:59,119
i want you to package it up in a

00:36:58,160 --> 00:37:01,440
container

00:36:59,119 --> 00:37:03,200
and i'll go deploy it to kubernetes so

00:37:01,440 --> 00:37:06,160
instead of running this in a cloud

00:37:03,200 --> 00:37:08,000
serverless provider using cada you can

00:37:06,160 --> 00:37:09,839
package that run time with the

00:37:08,000 --> 00:37:11,040
code using some of these commands and go

00:37:09,839 --> 00:37:12,560
run it in kubernetes

00:37:11,040 --> 00:37:14,720
so hopefully you get the idea that you

00:37:12,560 --> 00:37:15,920
can still have that serverless runtime

00:37:14,720 --> 00:37:19,040
experience

00:37:15,920 --> 00:37:20,000
in kubernetes you use cada to do the

00:37:19,040 --> 00:37:22,560
scaling

00:37:20,000 --> 00:37:24,320
you use the runtime to minimize the

00:37:22,560 --> 00:37:26,800
amount of code you have to run

00:37:24,320 --> 00:37:28,079
so there's another demo that i could do

00:37:26,800 --> 00:37:29,599
here to kind of

00:37:28,079 --> 00:37:31,200
make that solid maybe i'll run through

00:37:29,599 --> 00:37:32,240
it very quickly because i don't want to

00:37:31,200 --> 00:37:33,680
spend too much time

00:37:32,240 --> 00:37:35,760
just to kind of get you the sense of

00:37:33,680 --> 00:37:37,280
like what does using a serverless

00:37:35,760 --> 00:37:39,680
runtime with cada

00:37:37,280 --> 00:37:41,359
look like um let's create a quick

00:37:39,680 --> 00:37:43,599
project here

00:37:41,359 --> 00:37:45,760
um i'm going to just create a folder

00:37:43,599 --> 00:37:46,880
called live demo

00:37:45,760 --> 00:37:49,040
it looks like i've used something like

00:37:46,880 --> 00:37:49,760
that before so if i'm using a serverless

00:37:49,040 --> 00:37:53,359
runtime

00:37:49,760 --> 00:37:55,359
let's say i want to use python uh here

00:37:53,359 --> 00:37:57,119
i can choose a bunch of different event

00:37:55,359 --> 00:37:58,720
sources that i actually care about

00:37:57,119 --> 00:38:01,280
uh i think there's even more here that

00:37:58,720 --> 00:38:04,560
i'm not showing like kafka and rabbitmq

00:38:01,280 --> 00:38:08,560
but the idea here is that i can

00:38:04,560 --> 00:38:11,119
use a serverless runtime

00:38:08,560 --> 00:38:12,960
in this case inside a visual studio code

00:38:11,119 --> 00:38:16,480
to pull messages from my queue

00:38:12,960 --> 00:38:19,280
which i have lovingly named q i could

00:38:16,480 --> 00:38:20,720
build this debug it run it the same way

00:38:19,280 --> 00:38:22,880
i would a serverless app that's going to

00:38:20,720 --> 00:38:26,320
be published to the public cloud

00:38:22,880 --> 00:38:26,800
to azure in this case but the last step

00:38:26,320 --> 00:38:29,200
of this

00:38:26,800 --> 00:38:31,200
and i'll just show this part quickly

00:38:29,200 --> 00:38:33,440
here's all the code that i have to have

00:38:31,200 --> 00:38:35,040
right this is pulling from a queue all

00:38:33,440 --> 00:38:37,040
of the additional code is handled in the

00:38:35,040 --> 00:38:38,560
run time i want to pull from a queue

00:38:37,040 --> 00:38:40,079
i could come in here and do whatever

00:38:38,560 --> 00:38:41,839
work i wanted to do

00:38:40,079 --> 00:38:43,200
but once i'm ready once i've debugged

00:38:41,839 --> 00:38:44,000
this thing and it's doing what i want it

00:38:43,200 --> 00:38:46,000
to do

00:38:44,000 --> 00:38:48,560
the piece that i wanted to highlight is

00:38:46,000 --> 00:38:51,119
that i can just simply run this command

00:38:48,560 --> 00:38:52,480
in fact i have it uh saved here because

00:38:51,119 --> 00:38:53,920
i know i was going to need to use it

00:38:52,480 --> 00:38:56,320
maybe i don't

00:38:53,920 --> 00:39:00,079
um i can run this command func

00:38:56,320 --> 00:39:02,800
kubernetes deploy

00:39:00,079 --> 00:39:04,720
uh we'll name this python function and

00:39:02,800 --> 00:39:07,200
i'll give it a registry

00:39:04,720 --> 00:39:08,720
to stick the container into uh but what

00:39:07,200 --> 00:39:09,680
this is now going to do is it's going to

00:39:08,720 --> 00:39:11,280
build

00:39:09,680 --> 00:39:12,960
oh yeah i'm not going to worry about

00:39:11,280 --> 00:39:16,640
this right now oh

00:39:12,960 --> 00:39:19,920
it's going to build a docker container

00:39:16,640 --> 00:39:20,880
uh for my function uh this docker

00:39:19,920 --> 00:39:22,480
container in fact

00:39:20,880 --> 00:39:23,920
it's going to build the docker container

00:39:22,480 --> 00:39:25,359
for the serverless runtime

00:39:23,920 --> 00:39:28,160
and it's going to stick it up in my

00:39:25,359 --> 00:39:29,760
cluster so that cada can now scale it

00:39:28,160 --> 00:39:30,560
so apologies i know i ran throughout

00:39:29,760 --> 00:39:32,079
there really quickly it's because i

00:39:30,560 --> 00:39:34,160
won't get to the other stuff but

00:39:32,079 --> 00:39:35,440
just give you an idea the the thing i

00:39:34,160 --> 00:39:38,480
want to emphasize is

00:39:35,440 --> 00:39:40,400
you're now using a serverless runtime to

00:39:38,480 --> 00:39:42,079
pull in those events to help broker

00:39:40,400 --> 00:39:43,040
those events to simplify the code you

00:39:42,079 --> 00:39:45,200
have to run

00:39:43,040 --> 00:39:46,800
keda in this case is just going to be

00:39:45,200 --> 00:39:48,240
doing the scaling so once this is

00:39:46,800 --> 00:39:50,320
finished and i'm not going to wait for

00:39:48,240 --> 00:39:51,440
it i would be able to come into my

00:39:50,320 --> 00:39:54,480
cluster here

00:39:51,440 --> 00:39:56,560
and you would see that my

00:39:54,480 --> 00:39:58,160
serverless function is parting and

00:39:56,560 --> 00:39:59,119
actually it's there wow that happened

00:39:58,160 --> 00:40:02,079
quickly

00:39:59,119 --> 00:40:03,280
there's my function scaled to zero if i

00:40:02,079 --> 00:40:06,240
drop something in the queue

00:40:03,280 --> 00:40:06,720
you would see this code spin up and run

00:40:06,240 --> 00:40:08,160
so

00:40:06,720 --> 00:40:09,920
again hopefully that makes sense that

00:40:08,160 --> 00:40:10,800
the main gist here is serverless

00:40:09,920 --> 00:40:14,160
runtimes

00:40:10,800 --> 00:40:16,000
can simplify the code all right uh

00:40:14,160 --> 00:40:18,240
yep brings an event-driven programming

00:40:16,000 --> 00:40:19,760
model paired with cada for scale

00:40:18,240 --> 00:40:21,680
it gives you that same serverless

00:40:19,760 --> 00:40:22,400
developer experience package it in a

00:40:21,680 --> 00:40:25,119
container

00:40:22,400 --> 00:40:26,720
scale it with cada there's more docs in

00:40:25,119 --> 00:40:29,040
that if you're interested as well

00:40:26,720 --> 00:40:29,920
and samples too in fact every demo i've

00:40:29,040 --> 00:40:32,880
showed here

00:40:29,920 --> 00:40:35,119
there is a sample on cata.sh where you

00:40:32,880 --> 00:40:37,920
could run through the rabbitmq demo

00:40:35,119 --> 00:40:38,240
or using a serverless runtime both those

00:40:37,920 --> 00:40:39,359
are

00:40:38,240 --> 00:40:41,280
are ready for you to walk through if

00:40:39,359 --> 00:40:42,800
you're curious to learn more so what's

00:40:41,280 --> 00:40:44,480
some of the reasons that folks might

00:40:42,800 --> 00:40:46,000
want to use a serverless runtime and

00:40:44,480 --> 00:40:47,680
host it on kubernetes with

00:40:46,000 --> 00:40:50,000
cada instead of hosting it in a

00:40:47,680 --> 00:40:52,240
serverless provider whether that's azure

00:40:50,000 --> 00:40:53,839
functions aws lambda you name it well

00:40:52,240 --> 00:40:54,640
you might want to use your serverless

00:40:53,839 --> 00:40:56,000
on-premises

00:40:54,640 --> 00:40:57,520
you don't want to be running in a data

00:40:56,000 --> 00:40:58,480
center all of the time you might be more

00:40:57,520 --> 00:41:00,160
hybrid

00:40:58,480 --> 00:41:02,480
maybe you have existing kubernetes

00:41:00,160 --> 00:41:04,160
investments you have more custom compute

00:41:02,480 --> 00:41:05,440
security or networking requirements

00:41:04,160 --> 00:41:07,280
that's when i face a lot when i'm

00:41:05,440 --> 00:41:09,040
working with like financial companies

00:41:07,280 --> 00:41:11,119
they're like hey i love the serverless

00:41:09,040 --> 00:41:12,720
service you have but i have more

00:41:11,119 --> 00:41:14,560
constraints around what i can

00:41:12,720 --> 00:41:16,480
and can't pass through in networking or

00:41:14,560 --> 00:41:18,400
maybe i need special nodes

00:41:16,480 --> 00:41:20,000
that have watchdogs running on them or

00:41:18,400 --> 00:41:23,040
whatever it is

00:41:20,000 --> 00:41:25,200
and then finally it's vendor lock-in if

00:41:23,040 --> 00:41:26,240
i'm running in kubernetes with my own

00:41:25,200 --> 00:41:28,880
containers

00:41:26,240 --> 00:41:31,280
uh i can run that anywhere i could run

00:41:28,880 --> 00:41:33,280
that same like i didn't even specify for

00:41:31,280 --> 00:41:34,880
this demo that i've been running

00:41:33,280 --> 00:41:37,520
where is this running i could do this

00:41:34,880 --> 00:41:40,560
exact demo on the azure cloud

00:41:37,520 --> 00:41:40,960
on digitalocean on azure i said azure

00:41:40,560 --> 00:41:44,000
twice

00:41:40,960 --> 00:41:46,560
on aws on google uh i can run this

00:41:44,000 --> 00:41:48,800
wherever it makes the most sense for me

00:41:46,560 --> 00:41:49,839
uh okay so there was one question now

00:41:48,800 --> 00:41:50,880
let's start to answer some of these

00:41:49,839 --> 00:41:54,640
other ones

00:41:50,880 --> 00:41:56,000
uh how can you do cada with incoming

00:41:54,640 --> 00:41:57,839
traffic

00:41:56,000 --> 00:41:59,119
so often when i think about incoming

00:41:57,839 --> 00:42:01,040
traffic i think about

00:41:59,119 --> 00:42:02,560
maybe a non-traditional event source

00:42:01,040 --> 00:42:04,000
well that's a bad way to say it not a

00:42:02,560 --> 00:42:07,040
message broker

00:42:04,000 --> 00:42:08,240
but something like http traffic so how

00:42:07,040 --> 00:42:11,280
can i use cada

00:42:08,240 --> 00:42:12,560
with http traffic uh and the answer is

00:42:11,280 --> 00:42:14,720
you can

00:42:12,560 --> 00:42:16,560
it just requires an additional piece

00:42:14,720 --> 00:42:18,800
here into the mix

00:42:16,560 --> 00:42:20,319
so i think actually in there there's a i

00:42:18,800 --> 00:42:21,599
had a slide here because i knew this was

00:42:20,319 --> 00:42:24,720
going to come up but in the

00:42:21,599 --> 00:42:26,880
faq of cada.sh

00:42:24,720 --> 00:42:28,319
we have a question here which is hey can

00:42:26,880 --> 00:42:31,520
i use cada

00:42:28,319 --> 00:42:33,280
with http and the answer is yes

00:42:31,520 --> 00:42:34,560
uh you just need to add in one

00:42:33,280 --> 00:42:36,079
additional piece in fact there's a

00:42:34,560 --> 00:42:39,680
diagram right here

00:42:36,079 --> 00:42:41,119
so keda needs to ask something how many

00:42:39,680 --> 00:42:43,040
messages are coming in

00:42:41,119 --> 00:42:44,720
right that's how kata works so i need to

00:42:43,040 --> 00:42:48,160
keda needs to talk to something

00:42:44,720 --> 00:42:48,960
it can't talk to the ingress controller

00:42:48,160 --> 00:42:50,800
directly

00:42:48,960 --> 00:42:52,560
today um it's something we've actually

00:42:50,800 --> 00:42:54,240
considered doing so the pattern we

00:42:52,560 --> 00:42:55,839
recommend doing here and i'd recommend

00:42:54,240 --> 00:42:56,319
you saw how i found this doc so whoever

00:42:55,839 --> 00:42:57,440
asked that

00:42:56,319 --> 00:42:59,680
question i would recommend you come to

00:42:57,440 --> 00:43:02,000
this doc you would usually set up

00:42:59,680 --> 00:43:04,800
something like a prometheus server

00:43:02,000 --> 00:43:06,079
have prometheus expose how many http

00:43:04,800 --> 00:43:08,800
messages are coming in

00:43:06,079 --> 00:43:09,920
and then cada can scale the thing uh the

00:43:08,800 --> 00:43:11,359
other thing i'll mention quickly here

00:43:09,920 --> 00:43:13,440
since we're kind of in the q a section

00:43:11,359 --> 00:43:14,800
now we're actually spinning up an

00:43:13,440 --> 00:43:18,400
additional project

00:43:14,800 --> 00:43:21,599
as part of the k to org called cada

00:43:18,400 --> 00:43:23,359
http let me see if i can find it if i

00:43:21,599 --> 00:43:25,359
can't find it i won't worry about it

00:43:23,359 --> 00:43:26,880
but we actually have some folks http

00:43:25,359 --> 00:43:28,240
add-in we actually have some folks who

00:43:26,880 --> 00:43:30,079
are looking at building

00:43:28,240 --> 00:43:32,160
capabilities yet still in the early

00:43:30,079 --> 00:43:33,200
stages that let you do http without

00:43:32,160 --> 00:43:35,760
having to manually

00:43:33,200 --> 00:43:36,480
set up what i've just described so it is

00:43:35,760 --> 00:43:38,000
possible

00:43:36,480 --> 00:43:39,760
you have to use prometheus today or at

00:43:38,000 --> 00:43:41,920
least that's what i recommend but there

00:43:39,760 --> 00:43:43,680
is some work happening in that space

00:43:41,920 --> 00:43:45,440
all right so good question the other

00:43:43,680 --> 00:43:46,560
question here was around how does this

00:43:45,440 --> 00:43:48,480
compare to k

00:43:46,560 --> 00:43:49,520
native and i i knew this question was

00:43:48,480 --> 00:43:51,280
going to come up so i prepared some

00:43:49,520 --> 00:43:54,720
slides

00:43:51,280 --> 00:43:58,400
keda i mentioned if i think about my

00:43:54,720 --> 00:44:01,920
container the actual user code

00:43:58,400 --> 00:44:03,839
my container is responsible to pull

00:44:01,920 --> 00:44:05,839
messages from the event source it is up

00:44:03,839 --> 00:44:08,720
to my container maybe a serverless

00:44:05,839 --> 00:44:09,920
runtime in that container but something

00:44:08,720 --> 00:44:13,119
needs to connect to

00:44:09,920 --> 00:44:16,319
rabbitmq or kafka or whatever it is

00:44:13,119 --> 00:44:18,319
and pull the messages in what usually

00:44:16,319 --> 00:44:19,839
happens with solutions like k-native and

00:44:18,319 --> 00:44:21,760
open faz

00:44:19,839 --> 00:44:23,599
is that what happens is you write a

00:44:21,760 --> 00:44:24,319
container that knows how to respond to

00:44:23,599 --> 00:44:25,599
http

00:44:24,319 --> 00:44:28,160
traffic so you would write something

00:44:25,599 --> 00:44:30,480
that's like receive http requests

00:44:28,160 --> 00:44:31,839
and process the messages if you want

00:44:30,480 --> 00:44:34,560
that container to process

00:44:31,839 --> 00:44:36,400
kafka messages you would deploy what's

00:44:34,560 --> 00:44:38,160
called a kafka adapter

00:44:36,400 --> 00:44:39,920
and there would be some adapter that's

00:44:38,160 --> 00:44:42,240
provided by the project openfast and

00:44:39,920 --> 00:44:44,720
canada both have a kafka adapter

00:44:42,240 --> 00:44:46,160
and you have this adapter that pulls the

00:44:44,720 --> 00:44:48,720
messages from kafka

00:44:46,160 --> 00:44:50,640
and then sends them over http to your

00:44:48,720 --> 00:44:52,319
container okay so there's a little bit

00:44:50,640 --> 00:44:53,839
of a difference here and how the events

00:44:52,319 --> 00:44:55,920
are distributed

00:44:53,839 --> 00:44:57,280
now there's pros and cons to both of

00:44:55,920 --> 00:44:59,599
these approaches

00:44:57,280 --> 00:45:01,680
so the pros of kind of the alternate the

00:44:59,599 --> 00:45:04,240
non-cada way of doing stuff

00:45:01,680 --> 00:45:05,920
is that the developer just has to know

00:45:04,240 --> 00:45:07,359
how to talk about http

00:45:05,920 --> 00:45:09,119
they don't have to know how to talk to

00:45:07,359 --> 00:45:10,880
kafka

00:45:09,119 --> 00:45:12,880
everything becomes http which is great

00:45:10,880 --> 00:45:14,400
because pretty much everyone can ride an

00:45:12,880 --> 00:45:16,880
http server

00:45:14,400 --> 00:45:18,000
now some of the downsides though is that

00:45:16,880 --> 00:45:21,040
when i have

00:45:18,000 --> 00:45:23,680
some more specific message systems

00:45:21,040 --> 00:45:24,240
like if i'm using kafka kafka provides

00:45:23,680 --> 00:45:26,800
way

00:45:24,240 --> 00:45:27,440
for this sdk to preserve ordering for it

00:45:26,800 --> 00:45:30,319
to do check

00:45:27,440 --> 00:45:30,720
pointing this becomes a lot harder when

00:45:30,319 --> 00:45:33,359
you're

00:45:30,720 --> 00:45:34,800
translating everything to http uh in

00:45:33,359 --> 00:45:36,560
fact none of the providers that i'm

00:45:34,800 --> 00:45:39,359
aware of enable you to do something like

00:45:36,560 --> 00:45:40,319
checkpoint or do ordered messaging at

00:45:39,359 --> 00:45:42,240
scale

00:45:40,319 --> 00:45:43,760
uh using this pattern so you get some

00:45:42,240 --> 00:45:45,520
pros but there are some cons you you

00:45:43,760 --> 00:45:46,720
can't talk directly to that event source

00:45:45,520 --> 00:45:48,640
anymore

00:45:46,720 --> 00:45:50,480
so things like stream processing where

00:45:48,640 --> 00:45:51,200
you're windowing and pulling in windows

00:45:50,480 --> 00:45:52,640
of data

00:45:51,200 --> 00:45:54,000
just become really difficult when you're

00:45:52,640 --> 00:45:55,760
trying to do all that windowing with

00:45:54,000 --> 00:45:57,280
just http messaging

00:45:55,760 --> 00:45:59,359
and then the last one here is like this

00:45:57,280 --> 00:46:00,720
event adapter is now responsible for

00:45:59,359 --> 00:46:02,560
talking to the messages

00:46:00,720 --> 00:46:05,280
your developer can no longer control

00:46:02,560 --> 00:46:07,359
that code for better or for worse

00:46:05,280 --> 00:46:09,599
now again the uh the alternate approach

00:46:07,359 --> 00:46:11,680
the catapros is not perfect either

00:46:09,599 --> 00:46:13,200
it has pros and cons too it looks like i

00:46:11,680 --> 00:46:15,040
i messed this one up

00:46:13,200 --> 00:46:16,240
i think it's almost the opposite the

00:46:15,040 --> 00:46:17,200
downside here now is though the

00:46:16,240 --> 00:46:19,119
developer

00:46:17,200 --> 00:46:21,119
has to have this sdk code in there

00:46:19,119 --> 00:46:21,760
somewhere i mentioned a serverless

00:46:21,119 --> 00:46:23,680
runtime

00:46:21,760 --> 00:46:25,760
is a useful library that can abstract

00:46:23,680 --> 00:46:27,440
that but that has to exist

00:46:25,760 --> 00:46:29,440
and then the pros though being like you

00:46:27,440 --> 00:46:31,200
can leverage things like dead lettering

00:46:29,440 --> 00:46:32,079
sessions partitions checkpointing

00:46:31,200 --> 00:46:33,839
windowing

00:46:32,079 --> 00:46:35,359
those all become possible because this

00:46:33,839 --> 00:46:37,680
could be over http

00:46:35,359 --> 00:46:38,960
this could be over grpc this could be

00:46:37,680 --> 00:46:40,560
over amqp

00:46:38,960 --> 00:46:42,319
this could be over any protocol you're

00:46:40,560 --> 00:46:44,720
talking directly to the event source

00:46:42,319 --> 00:46:46,480
cada is just scaling it up uh the only

00:46:44,720 --> 00:46:48,319
other thing i'll mention in terms of

00:46:46,480 --> 00:46:50,480
i'll leave this up as we answer the last

00:46:48,319 --> 00:46:51,920
few questions uh the only other thing

00:46:50,480 --> 00:46:53,200
i'll mention in terms of differences

00:46:51,920 --> 00:46:54,960
between tayden k native

00:46:53,200 --> 00:46:56,720
i mentioned cada's very single purpose

00:46:54,960 --> 00:47:00,319
it just does the scaling thing

00:46:56,720 --> 00:47:02,640
k-native provides actually a lot more um

00:47:00,319 --> 00:47:03,599
so k-native provides like ingress it

00:47:02,640 --> 00:47:05,680
provides things called

00:47:03,599 --> 00:47:07,680
replicas where you can do versions of

00:47:05,680 --> 00:47:08,800
apps it provides routing so that you can

00:47:07,680 --> 00:47:10,319
route between the apps

00:47:08,800 --> 00:47:12,160
you can optionally provide in like this

00:47:10,319 --> 00:47:12,960
eventing component it does a lot of

00:47:12,160 --> 00:47:15,920
things like

00:47:12,960 --> 00:47:16,400
30 40 things all pretty useful things

00:47:15,920 --> 00:47:18,560
but you

00:47:16,400 --> 00:47:19,680
grab a lot of them when you use k native

00:47:18,560 --> 00:47:21,440
cada just does

00:47:19,680 --> 00:47:23,760
event driven auto scaling it's very

00:47:21,440 --> 00:47:25,520
unobtrusive it just sits there

00:47:23,760 --> 00:47:27,760
you apply it to scale what you want it

00:47:25,520 --> 00:47:28,800
to uh that's not saying one is right and

00:47:27,760 --> 00:47:31,359
one is wrong

00:47:28,800 --> 00:47:33,599
but it's worth noting that if you if you

00:47:31,359 --> 00:47:35,200
use k-native you also will be adopting a

00:47:33,599 --> 00:47:35,599
lot of other features for better or

00:47:35,200 --> 00:47:39,119
worse

00:47:35,599 --> 00:47:40,000
as well um to it so hopefully that helps

00:47:39,119 --> 00:47:42,559
some i

00:47:40,000 --> 00:47:44,400
i i am a fan of k native i follow that

00:47:42,559 --> 00:47:45,760
progress of that project a lot

00:47:44,400 --> 00:47:48,319
i think it's doing some incredible

00:47:45,760 --> 00:47:49,839
things uh and k native and kate have

00:47:48,319 --> 00:47:51,280
actually been working together for a bit

00:47:49,839 --> 00:47:52,559
now to figure out how we can bridge some

00:47:51,280 --> 00:47:54,640
of these worlds together

00:47:52,559 --> 00:47:55,599
to make k-native uh do other stuff as

00:47:54,640 --> 00:47:58,400
well

00:47:55,599 --> 00:47:59,760
take a look take a look at both um okay

00:47:58,400 --> 00:48:02,160
let's answer a few

00:47:59,760 --> 00:48:04,400
more of these as we wrap it up in the

00:48:02,160 --> 00:48:07,599
last five minutes here

00:48:04,400 --> 00:48:10,079
uh limit scaling we talked about already

00:48:07,599 --> 00:48:11,760
you can define it uh i'm gonna ignore

00:48:10,079 --> 00:48:13,680
this one around service meshes it's a

00:48:11,760 --> 00:48:15,040
great question but not this webinar i

00:48:13,680 --> 00:48:16,720
think there's other webinars on service

00:48:15,040 --> 00:48:18,960
messages definitely check that out

00:48:16,720 --> 00:48:20,160
uh canadian eventing that event pattern

00:48:18,960 --> 00:48:21,839
is where i talked about k-native

00:48:20,160 --> 00:48:23,119
eventing is that the kind of thing in

00:48:21,839 --> 00:48:24,079
the middle that turns everything into

00:48:23,119 --> 00:48:26,800
http

00:48:24,079 --> 00:48:27,119
so hopefully that answers that question

00:48:26,800 --> 00:48:29,839
uh

00:48:27,119 --> 00:48:32,079
let's let me read this one live uh can

00:48:29,839 --> 00:48:34,000
we combine cata with the usual hpa for

00:48:32,079 --> 00:48:35,760
example if the app is processing images

00:48:34,000 --> 00:48:37,920
of different sizes it may happen that

00:48:35,760 --> 00:48:40,880
just scaling and function of the images

00:48:37,920 --> 00:48:41,440
may not be enough yes um so the answer

00:48:40,880 --> 00:48:45,440
is yes

00:48:41,440 --> 00:48:47,920
you can you can combine hpa scaling

00:48:45,440 --> 00:48:50,160
with k to scaling and you kind of leave

00:48:47,920 --> 00:48:52,960
it up to the hpa to use its algorithm

00:48:50,160 --> 00:48:54,160
to use both bits of data uh but great

00:48:52,960 --> 00:48:55,760
question sergio

00:48:54,160 --> 00:48:58,319
and the answer is yes you can't combine

00:48:55,760 --> 00:49:00,319
them all um

00:48:58,319 --> 00:49:02,240
extensibility good question kevin if

00:49:00,319 --> 00:49:03,359
you're extending cada like you have your

00:49:02,240 --> 00:49:05,920
own event source

00:49:03,359 --> 00:49:07,359
does your extensibility need to be

00:49:05,920 --> 00:49:09,599
written in go

00:49:07,359 --> 00:49:11,440
and the answer is no we actually provide

00:49:09,599 --> 00:49:14,319
a grpc

00:49:11,440 --> 00:49:16,400
uh protocol for you to follow one of our

00:49:14,319 --> 00:49:17,200
most popular extensibility pieces for

00:49:16,400 --> 00:49:19,200
cater right now

00:49:17,200 --> 00:49:21,680
is actually for this tool called durable

00:49:19,200 --> 00:49:23,359
functions the extensibility that lets

00:49:21,680 --> 00:49:25,680
kata know how to scale a durable

00:49:23,359 --> 00:49:27,839
function is written in dot-net

00:49:25,680 --> 00:49:28,960
but it can talk to cada which happens to

00:49:27,839 --> 00:49:31,280
be reninco

00:49:28,960 --> 00:49:32,640
so no you can extend it in any language

00:49:31,280 --> 00:49:34,880
great question

00:49:32,640 --> 00:49:35,920
uh yep ada this is similar to the other

00:49:34,880 --> 00:49:37,760
one i see someone adds

00:49:35,920 --> 00:49:39,760
cada can be combined with traditional

00:49:37,760 --> 00:49:42,000
metrics so you can use cada

00:49:39,760 --> 00:49:43,680
but also include cpu and memory you just

00:49:42,000 --> 00:49:45,200
make it smarter

00:49:43,680 --> 00:49:47,520
you don't have to say it's only event

00:49:45,200 --> 00:49:51,280
driven or it's only cpu

00:49:47,520 --> 00:49:52,400
um let's see what else is here we talked

00:49:51,280 --> 00:49:53,839
some about keda and k

00:49:52,400 --> 00:49:55,359
native someone asked i just like this

00:49:53,839 --> 00:49:57,200
question we got about three more minutes

00:49:55,359 --> 00:49:59,920
left

00:49:57,200 --> 00:50:00,720
incorporate cada with quantum computing

00:49:59,920 --> 00:50:03,119
sure

00:50:00,720 --> 00:50:04,559
as long as quantum computing works with

00:50:03,119 --> 00:50:06,559
k kubernetes

00:50:04,559 --> 00:50:08,240
uh i would assume that there is a world

00:50:06,559 --> 00:50:09,440
or a project somewhere that has

00:50:08,240 --> 00:50:13,040
kubernetes

00:50:09,440 --> 00:50:15,040
using computing i have no idea

00:50:13,040 --> 00:50:17,520
i'm not a i'm not a quantum expert but

00:50:15,040 --> 00:50:20,640
nothing would stop you from that in fact

00:50:17,520 --> 00:50:22,400
i guess an interesting note uh because

00:50:20,640 --> 00:50:24,400
how keta integrates just natively with

00:50:22,400 --> 00:50:25,599
kubernetes apis usually if kubernetes

00:50:24,400 --> 00:50:27,760
can do something

00:50:25,599 --> 00:50:30,000
cada can do it as well in fact almost

00:50:27,760 --> 00:50:31,839
always so like for example

00:50:30,000 --> 00:50:33,200
a few cloud providers have this feature

00:50:31,839 --> 00:50:34,880
called like virtual nodes

00:50:33,200 --> 00:50:36,400
there's another cncf project called

00:50:34,880 --> 00:50:38,559
virtual cubelets where

00:50:36,400 --> 00:50:40,160
let's say i want to scale out really far

00:50:38,559 --> 00:50:42,880
really fast uh

00:50:40,160 --> 00:50:44,480
similar to how i did it before like

00:50:42,880 --> 00:50:45,599
imagine if your cluster didn't have the

00:50:44,480 --> 00:50:48,480
capacity

00:50:45,599 --> 00:50:48,960
to scale out as far as maybe your best

00:50:48,480 --> 00:50:51,040
uh

00:50:48,960 --> 00:50:52,960
your your black friday sale is causing

00:50:51,040 --> 00:50:55,440
you to scale out

00:50:52,960 --> 00:50:56,400
you can turn on this virtual cubelet and

00:50:55,440 --> 00:50:58,160
scale out to

00:50:56,400 --> 00:50:59,760
serverless containers it's this cool

00:50:58,160 --> 00:51:00,559
feature check it out virtual cubelet's

00:50:59,760 --> 00:51:03,440
another project

00:51:00,559 --> 00:51:05,599
anyway cato works great with that you

00:51:03,440 --> 00:51:06,960
can use cada to scale into serverless

00:51:05,599 --> 00:51:07,599
containers that aren't even in your

00:51:06,960 --> 00:51:09,520
cluster

00:51:07,599 --> 00:51:11,520
because cada just tells kubernetes i

00:51:09,520 --> 00:51:13,119
need to scale kubernetes is actually in

00:51:11,520 --> 00:51:14,720
charge of taking care of it whether that

00:51:13,119 --> 00:51:16,720
invokes cluster scaling

00:51:14,720 --> 00:51:18,160
serverless cube container scaling you

00:51:16,720 --> 00:51:20,800
name it

00:51:18,160 --> 00:51:22,000
uh all right doing a lastpass here

00:51:20,800 --> 00:51:23,920
before we run out of

00:51:22,000 --> 00:51:25,760
time great questions i appreciate all of

00:51:23,920 --> 00:51:29,359
you asking all of these as well

00:51:25,760 --> 00:51:31,280
incoming traffic um

00:51:29,359 --> 00:51:32,960
yeah i'll just wrap up with this one mo

00:51:31,280 --> 00:51:34,160
i know this one was a little bit earlier

00:51:32,960 --> 00:51:36,800
i could have grabbed this one earlier i

00:51:34,160 --> 00:51:39,920
think i just missed it any benefits

00:51:36,800 --> 00:51:41,359
to event driven scale over uh

00:51:39,920 --> 00:51:44,160
traditional scaling

00:51:41,359 --> 00:51:46,000
and it depends on the solution i think

00:51:44,160 --> 00:51:47,520
the big benefit of event driven if it's

00:51:46,000 --> 00:51:49,920
much more proactive

00:51:47,520 --> 00:51:50,559
you're scaling based on the cause of the

00:51:49,920 --> 00:51:53,680
load

00:51:50,559 --> 00:51:55,280
so you actually are scaling far and fast

00:51:53,680 --> 00:51:57,200
uh you're able to scale all the way to

00:51:55,280 --> 00:51:58,720
zero which isn't really possible with

00:51:57,200 --> 00:52:00,240
resource scaling because

00:51:58,720 --> 00:52:01,760
like the cpu is never going to be at

00:52:00,240 --> 00:52:02,960
zero percent so when did you scale it

00:52:01,760 --> 00:52:05,359
down to zero

00:52:02,960 --> 00:52:06,960
that said resource scaling isn't all bad

00:52:05,359 --> 00:52:08,240
uh and in fact i would encourage you and

00:52:06,960 --> 00:52:09,440
we kind of talked about okay to let you

00:52:08,240 --> 00:52:11,280
combine them both

00:52:09,440 --> 00:52:13,520
see where kato might fit in see where it

00:52:11,280 --> 00:52:15,599
doesn't it's super unobtrusive as i

00:52:13,520 --> 00:52:17,920
mentioned you can pop it into a cluster

00:52:15,599 --> 00:52:19,040
you can only have one of your containers

00:52:17,920 --> 00:52:21,839
using cada to

00:52:19,040 --> 00:52:23,839
help its scaling the other 99 containers

00:52:21,839 --> 00:52:25,680
might just be using vanilla kubernetes

00:52:23,839 --> 00:52:28,160
that's totally cool maybe you want to

00:52:25,680 --> 00:52:28,960
mix and match some event driven metrics

00:52:28,160 --> 00:52:31,440
some

00:52:28,960 --> 00:52:33,359
resource metrics that's totally cool too

00:52:31,440 --> 00:52:35,040
but it's a very valuable piece

00:52:33,359 --> 00:52:36,400
in your tool kit definitely worth

00:52:35,040 --> 00:52:37,760
considering and so thank you all again

00:52:36,400 --> 00:52:38,559
for joining thanks for watching this

00:52:37,760 --> 00:52:40,480
recording

00:52:38,559 --> 00:52:42,160
this slide was up hopefully you read it

00:52:40,480 --> 00:52:43,839
check out the site to learn more

00:52:42,160 --> 00:52:45,680
reach out if you have questions on our

00:52:43,839 --> 00:52:46,960
kubernetes slack channel i would love to

00:52:45,680 --> 00:52:47,599
see some of you in our community

00:52:46,960 --> 00:52:49,920
standups

00:52:47,599 --> 00:52:50,640
whether you want to contribute with code

00:52:49,920 --> 00:52:54,319
docs

00:52:50,640 --> 00:52:56,160
issues samples designs just saying hey

00:52:54,319 --> 00:52:57,520
thumbs up i noticed

00:52:56,160 --> 00:52:59,359
all of it is very welcome we want to

00:52:57,520 --> 00:53:00,800
bring everyone in so thank you all very

00:52:59,359 --> 00:53:01,200
much and with that i'm going to wrap it

00:53:00,800 --> 00:53:03,119
up

00:53:01,200 --> 00:53:04,720
and pass it back over so thanks all for

00:53:03,119 --> 00:53:07,280
joining

00:53:04,720 --> 00:53:07,920
thank you jeff that was awesome really

00:53:07,280 --> 00:53:09,359
appreciate

00:53:07,920 --> 00:53:11,119
all your knowledge there and thanks

00:53:09,359 --> 00:53:15,839
everyone for joining us today

00:53:11,119 --> 00:53:15,839

YouTube URL: https://www.youtube.com/watch?v=8THSf98Fhu8


