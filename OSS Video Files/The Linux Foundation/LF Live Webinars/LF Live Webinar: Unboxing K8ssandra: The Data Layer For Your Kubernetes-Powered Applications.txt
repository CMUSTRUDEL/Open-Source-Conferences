Title: LF Live Webinar: Unboxing K8ssandra: The Data Layer For Your Kubernetes-Powered Applications
Publication date: 2021-04-06
Playlist: LF Live Webinars
Description: 
	LF Live: Unboxing K8ssandra: The Data Layer For Your Kubernetes-Powered Applications, Linux Foundation Webinar, sponsored by DataStax
Captions: 
	00:00:01,360 --> 00:00:06,160
hey uh thank you very much

00:00:03,199 --> 00:00:07,600
christina it's great to be here today um

00:00:06,160 --> 00:00:09,679
and i'm really excited to

00:00:07,600 --> 00:00:10,639
be uh talking with you along with chris

00:00:09,679 --> 00:00:13,679
bradford

00:00:10,639 --> 00:00:18,000
and uh our goal for today

00:00:13,679 --> 00:00:18,800
is um light on slides heavy on demos if

00:00:18,000 --> 00:00:20,960
i'm right so

00:00:18,800 --> 00:00:22,000
uh we'll see how we do with that and

00:00:20,960 --> 00:00:24,640
we're going to be talking about

00:00:22,000 --> 00:00:27,599
unboxing kate sandra the data layer for

00:00:24,640 --> 00:00:29,519
your kubernetes powered applications

00:00:27,599 --> 00:00:30,880
um so first of all we want to start off

00:00:29,519 --> 00:00:34,160
from the very beginning

00:00:30,880 --> 00:00:34,480
what is kate sandra and before i before

00:00:34,160 --> 00:00:36,800
i

00:00:34,480 --> 00:00:38,320
uh kick chris off on that i'm going to

00:00:36,800 --> 00:00:38,960
have you take a look at the logo and see

00:00:38,320 --> 00:00:41,280
if you can

00:00:38,960 --> 00:00:42,960
uh kind of guess here you see a little

00:00:41,280 --> 00:00:45,520
bit of the nautical element with the

00:00:42,960 --> 00:00:47,760
sextant right and then there is this uh

00:00:45,520 --> 00:00:49,280
star here um and there there's a little

00:00:47,760 --> 00:00:51,120
bit of a clue there so

00:00:49,280 --> 00:00:52,640
but why don't you unpack it a little bit

00:00:51,120 --> 00:00:55,920
for us chris

00:00:52,640 --> 00:00:57,760
yeah thanks jeff so yeah so kate sandra

00:00:55,920 --> 00:01:00,000
is a bit of a mashup here we have

00:00:57,760 --> 00:01:01,359
kubernetes right as well as cassandra or

00:01:00,000 --> 00:01:03,039
apache cassandra

00:01:01,359 --> 00:01:05,280
and we're going to talk about running

00:01:03,039 --> 00:01:06,080
cassandra on kubernetes but also some of

00:01:05,280 --> 00:01:08,880
the pieces

00:01:06,080 --> 00:01:11,280
that enable you to run a production-like

00:01:08,880 --> 00:01:11,280
system

00:01:11,520 --> 00:01:15,840
on on a so specifically not just the

00:01:14,479 --> 00:01:17,759
database i think

00:01:15,840 --> 00:01:19,200
uh it's pretty easy to just spin up a

00:01:17,759 --> 00:01:21,439
pod or a container

00:01:19,200 --> 00:01:22,400
and in the case of cassandra a few of

00:01:21,439 --> 00:01:24,080
those

00:01:22,400 --> 00:01:26,240
but also the supporting technologies

00:01:24,080 --> 00:01:29,280
that surround it

00:01:26,240 --> 00:01:30,159
cool yeah well let's so i mean we do got

00:01:29,280 --> 00:01:32,000
to start with

00:01:30,159 --> 00:01:33,759
the foundation of this and we've already

00:01:32,000 --> 00:01:36,560
tipped our hat that it's uh

00:01:33,759 --> 00:01:37,119
cassandra on kubernetes is the basis of

00:01:36,560 --> 00:01:39,680
this

00:01:37,119 --> 00:01:41,520
so uh you know we're going to assume

00:01:39,680 --> 00:01:42,399
some familiarity with kubernetes here

00:01:41,520 --> 00:01:45,040
but we want to

00:01:42,399 --> 00:01:46,479
we want to kind of mention why like what

00:01:45,040 --> 00:01:49,840
is what is the problem that

00:01:46,479 --> 00:01:51,759
kubernetes really solves and and why did

00:01:49,840 --> 00:01:54,079
it win it was competing with other

00:01:51,759 --> 00:01:55,280
uh orchestration systems for containers

00:01:54,079 --> 00:01:57,200
right so

00:01:55,280 --> 00:01:58,880
um there's some aspects of this that i

00:01:57,200 --> 00:02:00,159
think that uh

00:01:58,880 --> 00:02:01,680
will make it a natural to go with

00:02:00,159 --> 00:02:02,159
cassandra but let's unpack that for a

00:02:01,680 --> 00:02:05,040
second

00:02:02,159 --> 00:02:06,320
what do you think chris yeah definitely

00:02:05,040 --> 00:02:07,280
and i think one of the things that we

00:02:06,320 --> 00:02:10,160
want to look at

00:02:07,280 --> 00:02:10,160
specifically is

00:02:10,319 --> 00:02:13,920
what is what is what is kubernetes

00:02:12,319 --> 00:02:17,120
solving right

00:02:13,920 --> 00:02:18,879
it's really a platform for deploying

00:02:17,120 --> 00:02:21,200
a number of containers across a fleet of

00:02:18,879 --> 00:02:22,800
servers i've heard it described as the

00:02:21,200 --> 00:02:24,000
operating system of the data center and

00:02:22,800 --> 00:02:26,560
i think that's a pretty apt

00:02:24,000 --> 00:02:26,560
description

00:02:28,080 --> 00:02:31,200
when we talk about a system like

00:02:29,840 --> 00:02:32,480
cassandra which we'll do here in a

00:02:31,200 --> 00:02:34,560
second

00:02:32,480 --> 00:02:36,000
it runs across multiple servers already

00:02:34,560 --> 00:02:38,560
and so it's interesting to see

00:02:36,000 --> 00:02:40,000
how multiple distributed systems kind of

00:02:38,560 --> 00:02:44,800
come together to less

00:02:40,000 --> 00:02:44,800
into a scalable self-healing

00:02:44,959 --> 00:02:49,760
if things are configured correctly

00:02:47,599 --> 00:02:51,280
platform for deploying your applications

00:02:49,760 --> 00:02:52,720
i like to think of it as removing a lot

00:02:51,280 --> 00:02:54,319
of the tdm

00:02:52,720 --> 00:02:58,400
from the work that i used to do in the

00:02:54,319 --> 00:02:58,400
knock or on individual machines

00:03:01,519 --> 00:03:07,599
awesome all right and that brings us to

00:03:04,720 --> 00:03:07,599
cassandra

00:03:08,000 --> 00:03:13,040
so apache cassandra

00:03:11,040 --> 00:03:14,720
is has a lot of similarities to

00:03:13,040 --> 00:03:18,560
kubernetes and in that it's a

00:03:14,720 --> 00:03:20,239
distributed system right um and

00:03:18,560 --> 00:03:22,239
so at the surface at the high level we

00:03:20,239 --> 00:03:24,400
have these common terminology

00:03:22,239 --> 00:03:26,239
uh that are common to both as

00:03:24,400 --> 00:03:28,640
distributed systems of

00:03:26,239 --> 00:03:30,560
nodes and nodes that are formed into

00:03:28,640 --> 00:03:32,239
clusters

00:03:30,560 --> 00:03:34,400
but then there is a point at which the

00:03:32,239 --> 00:03:35,360
similarities stop and there are a few

00:03:34,400 --> 00:03:37,599
differences

00:03:35,360 --> 00:03:38,400
so the mapping maybe appears really easy

00:03:37,599 --> 00:03:40,080
at a high level

00:03:38,400 --> 00:03:41,599
but then at the lower level there are

00:03:40,080 --> 00:03:43,120
some details

00:03:41,599 --> 00:03:44,560
uh that we need to work out and we will

00:03:43,120 --> 00:03:46,400
kind of talk through that later on in

00:03:44,560 --> 00:03:48,239
the presentation but let's go

00:03:46,400 --> 00:03:50,560
let's let's step back and say like why

00:03:48,239 --> 00:03:52,480
cassandra like what makes cassandra

00:03:50,560 --> 00:03:55,280
a great solution for cloud native

00:03:52,480 --> 00:03:55,280
architectures

00:03:55,519 --> 00:03:59,280
sure so one of the things that i think

00:03:57,519 --> 00:04:02,640
cassandra's done really well is it's

00:03:59,280 --> 00:04:04,720
it's a peer-to-peer database there's no

00:04:02,640 --> 00:04:06,959
leader and follower there's no concept

00:04:04,720 --> 00:04:08,640
of uh like a write replica and then

00:04:06,959 --> 00:04:10,720
redrops because

00:04:08,640 --> 00:04:12,959
any node can answer any query and it

00:04:10,720 --> 00:04:16,079
will route that to the appropriate

00:04:12,959 --> 00:04:18,799
actual instance that hosts the data um

00:04:16,079 --> 00:04:20,799
so it's it's a really interesting fit

00:04:18,799 --> 00:04:22,400
here when we look at a system like

00:04:20,799 --> 00:04:24,400
kubernetes where

00:04:22,400 --> 00:04:26,160
traditionally it was really solid for

00:04:24,400 --> 00:04:26,800
stateful workloads or sorry stateless

00:04:26,160 --> 00:04:28,240
workloads

00:04:26,800 --> 00:04:29,360
those are pretty easy to scale you just

00:04:28,240 --> 00:04:30,400
spin up more of them and make sure

00:04:29,360 --> 00:04:32,240
they're added to your load balancer and

00:04:30,400 --> 00:04:34,880
away you go right

00:04:32,240 --> 00:04:37,520
but when we look at stateful services

00:04:34,880 --> 00:04:38,639
like cassandra or really any database

00:04:37,520 --> 00:04:40,560
things get a little bit trickier you

00:04:38,639 --> 00:04:42,080
can't just add a new node

00:04:40,560 --> 00:04:43,919
and expect it to be fully functional

00:04:42,080 --> 00:04:45,440
right away what if a read request goes

00:04:43,919 --> 00:04:46,560
that well it doesn't have any data when

00:04:45,440 --> 00:04:49,280
it first starts

00:04:46,560 --> 00:04:51,120
so there's this process that you have to

00:04:49,280 --> 00:04:53,680
go through to make sure okay this is

00:04:51,120 --> 00:04:55,040
ready to start accepting traffic or if

00:04:53,680 --> 00:04:56,880
node goes down it comes back up

00:04:55,040 --> 00:04:58,560
is there data there to operate on or

00:04:56,880 --> 00:05:00,479
does it need to bootstrap off of other

00:04:58,560 --> 00:05:02,560
nodes in the cluster

00:05:00,479 --> 00:05:04,639
so it's this android's a really

00:05:02,560 --> 00:05:06,320
interesting fit because it does some of

00:05:04,639 --> 00:05:08,720
those things out of the box

00:05:06,320 --> 00:05:11,280
and what i like about it at least in the

00:05:08,720 --> 00:05:12,160
realm of kubernetes and data on

00:05:11,280 --> 00:05:15,120
kubernetes

00:05:12,160 --> 00:05:16,880
is that it kind of aligns with the

00:05:15,120 --> 00:05:19,759
expectations of kubernetes

00:05:16,880 --> 00:05:20,639
like kubernetes has of its containers um

00:05:19,759 --> 00:05:22,960
it brings a lot of

00:05:20,639 --> 00:05:24,880
the behavior of the the stateless where

00:05:22,960 --> 00:05:26,960
you can scale out

00:05:24,880 --> 00:05:28,800
pretty easily right and you have high

00:05:26,960 --> 00:05:30,240
availability your self-healing of no

00:05:28,800 --> 00:05:31,840
goes down it can come back up

00:05:30,240 --> 00:05:33,199
uh that you would get with your

00:05:31,840 --> 00:05:34,479
stateless workloads you get that with

00:05:33,199 --> 00:05:38,000
your your staple workload

00:05:34,479 --> 00:05:39,440
with cassandra but do you want to

00:05:38,000 --> 00:05:41,680
jeff do you want to go into a little bit

00:05:39,440 --> 00:05:43,280
about cassandra you have a

00:05:41,680 --> 00:05:45,759
experience here you've written a book on

00:05:43,280 --> 00:05:49,759
the topic

00:05:45,759 --> 00:05:51,680
well that is true i mean i have

00:05:49,759 --> 00:05:53,520
written a book on cassandra one of the

00:05:51,680 --> 00:05:55,039
things that

00:05:53,520 --> 00:05:57,440
i found interesting over the past few

00:05:55,039 --> 00:06:01,520
years i've actually done two editions

00:05:57,440 --> 00:06:03,440
of the cassandra book for o'reilly and

00:06:01,520 --> 00:06:05,199
this i i did the second edition and the

00:06:03,440 --> 00:06:06,960
third edition and when about

00:06:05,199 --> 00:06:08,479
uh five years ago when i was writing the

00:06:06,960 --> 00:06:11,039
second edition

00:06:08,479 --> 00:06:12,720
uh it was very it was clear that i

00:06:11,039 --> 00:06:16,080
needed to add a section on running

00:06:12,720 --> 00:06:18,240
um cassandra in containers in docker and

00:06:16,080 --> 00:06:19,919
i i ended up putting a um

00:06:18,240 --> 00:06:21,919
a note in that was basically like yeah

00:06:19,919 --> 00:06:23,520
you should totally try this out and use

00:06:21,919 --> 00:06:25,440
it in your dev environment but it

00:06:23,520 --> 00:06:27,280
i wouldn't recommend it in prod so that

00:06:25,440 --> 00:06:30,240
was kind of like the

00:06:27,280 --> 00:06:32,080
my word of record for three years until

00:06:30,240 --> 00:06:33,680
i did the third edition of the book and

00:06:32,080 --> 00:06:34,560
by then the landscape had completely

00:06:33,680 --> 00:06:37,199
changed

00:06:34,560 --> 00:06:38,720
and you know i was like yes you could

00:06:37,199 --> 00:06:39,919
play cassandra in containers and it's

00:06:38,720 --> 00:06:42,160
going to work fine

00:06:39,919 --> 00:06:44,160
and then it was kind of like and this

00:06:42,160 --> 00:06:47,199
area of deploying in kubernetes

00:06:44,160 --> 00:06:49,440
and managing with kubernetes is emerging

00:06:47,199 --> 00:06:51,120
so and again it's probably almost a year

00:06:49,440 --> 00:06:53,759
and a half since i wrote that

00:06:51,120 --> 00:06:54,880
um so now we're in a very different

00:06:53,759 --> 00:06:57,440
landscape here

00:06:54,880 --> 00:06:58,960
um and i think you've had kind of a a

00:06:57,440 --> 00:07:00,319
what do i call a conversion experience

00:06:58,960 --> 00:07:01,680
of your own i mean that we had a blog

00:07:00,319 --> 00:07:02,880
that just went live yesterday where you

00:07:01,680 --> 00:07:05,680
kind of shared your story but

00:07:02,880 --> 00:07:06,479
why don't you nutshell that for us yeah

00:07:05,680 --> 00:07:09,039
it's actually really

00:07:06,479 --> 00:07:10,160
fascinating uh just at a high level if

00:07:09,039 --> 00:07:12,400
you asked me

00:07:10,160 --> 00:07:14,639
just a number of years ago and probably

00:07:12,400 --> 00:07:16,240
up to whether you should be running

00:07:14,639 --> 00:07:19,360
databases on kubernetes so we've been

00:07:16,240 --> 00:07:21,599
like uh no no that's a horrible idea

00:07:19,360 --> 00:07:23,599
and it's been interesting to see like

00:07:21,599 --> 00:07:26,800
how kubernetes has grown

00:07:23,599 --> 00:07:28,479
to accommodate stable workloads from the

00:07:26,800 --> 00:07:30,560
primitives with persistent volumes

00:07:28,479 --> 00:07:33,520
versus volume claims stateful sets

00:07:30,560 --> 00:07:35,120
uh and and and then starting to leverage

00:07:33,520 --> 00:07:35,520
those from the database side and say

00:07:35,120 --> 00:07:36,639
okay

00:07:35,520 --> 00:07:38,639
this is this is a little bit more

00:07:36,639 --> 00:07:41,840
reasonable uh so

00:07:38,639 --> 00:07:42,639
we'll we'll get that that link out in

00:07:41,840 --> 00:07:44,160
the chat here

00:07:42,639 --> 00:07:46,720
in a moment so you can take a look at

00:07:44,160 --> 00:07:49,599
that i don't want to go too deep into

00:07:46,720 --> 00:07:51,759
right there but i was a former skeptic

00:07:49,599 --> 00:07:54,240
and now wholeheartedly embrace

00:07:51,759 --> 00:07:55,759
the data on kubernetes approach

00:07:54,240 --> 00:07:57,520
absolutely

00:07:55,759 --> 00:08:00,720
good and i think that that um to the

00:07:57,520 --> 00:08:03,440
point we have a question from john doe

00:08:00,720 --> 00:08:04,639
and i don't know john doe john smith uh

00:08:03,440 --> 00:08:07,280
anonymous friend

00:08:04,639 --> 00:08:09,199
that uh is asking if we're if we have

00:08:07,280 --> 00:08:10,800
anybody using keith sander and prod yet

00:08:09,199 --> 00:08:12,240
and you want to talk to that a little

00:08:10,800 --> 00:08:14,319
bit like i think we have different

00:08:12,240 --> 00:08:15,360
piece pieces of the kate sander

00:08:14,319 --> 00:08:18,000
ecosystem that are

00:08:15,360 --> 00:08:19,039
in various stages of adoption as well as

00:08:18,000 --> 00:08:20,879
kind of a whole

00:08:19,039 --> 00:08:22,879
assemblage that we're about to unbox for

00:08:20,879 --> 00:08:24,479
you so

00:08:22,879 --> 00:08:26,000
yeah i think it's it's important to note

00:08:24,479 --> 00:08:28,639
so we started with

00:08:26,000 --> 00:08:30,479
with our journey into cassandra and

00:08:28,639 --> 00:08:31,599
kubernetes we started with an operator

00:08:30,479 --> 00:08:33,200
and we'll talk about all the other

00:08:31,599 --> 00:08:35,200
operators here in a second but we

00:08:33,200 --> 00:08:37,839
started with just an operator

00:08:35,200 --> 00:08:39,120
and that's grown to containerizing and

00:08:37,839 --> 00:08:42,080
productionizing

00:08:39,120 --> 00:08:43,599
multiple components that make up a

00:08:42,080 --> 00:08:45,120
cohesive stacker platform that's what

00:08:43,599 --> 00:08:46,880
case sander is it's the collection of

00:08:45,120 --> 00:08:49,200
all these technologies

00:08:46,880 --> 00:08:50,560
to run a production ready workbooks

00:08:49,200 --> 00:08:52,560
there are people that are running

00:08:50,560 --> 00:08:53,600
cassandra on kubernetes today in

00:08:52,560 --> 00:08:56,640
production

00:08:53,600 --> 00:08:57,440
uh with cads operator uh and and some of

00:08:56,640 --> 00:08:59,760
the other

00:08:57,440 --> 00:09:01,200
technologies as far as the packaging of

00:08:59,760 --> 00:09:02,959
it all under one name

00:09:01,200 --> 00:09:05,120
uh that's something that's that's

00:09:02,959 --> 00:09:06,560
currently in flight a great question

00:09:05,120 --> 00:09:08,480
yeah that's i mean that's a relatively

00:09:06,560 --> 00:09:10,959
new we had a 1.0

00:09:08,480 --> 00:09:12,160
release a couple months back and we're

00:09:10,959 --> 00:09:16,000
dropping a one

00:09:12,160 --> 00:09:17,279
one o release this week with some uh

00:09:16,000 --> 00:09:19,600
enhancements that maybe we can talk

00:09:17,279 --> 00:09:20,959
about in line here as we get to it in

00:09:19,600 --> 00:09:23,360
the presentation but

00:09:20,959 --> 00:09:25,040
uh you know one of the things as we talk

00:09:23,360 --> 00:09:26,160
about unboxing right and looking at

00:09:25,040 --> 00:09:29,360
what's inside

00:09:26,160 --> 00:09:31,279
this kate stander distribution um one of

00:09:29,360 --> 00:09:34,640
the architectural goals that we had

00:09:31,279 --> 00:09:36,640
was composability so we are saying when

00:09:34,640 --> 00:09:38,760
when you get a release of kate sandra

00:09:36,640 --> 00:09:40,399
that we're saying this is a blessed

00:09:38,760 --> 00:09:42,240
configuration

00:09:40,399 --> 00:09:44,399
that we know and we're confident saying

00:09:42,240 --> 00:09:46,480
this all works together and it's patched

00:09:44,399 --> 00:09:47,600
packaged together well and yes you can

00:09:46,480 --> 00:09:50,640
customize it

00:09:47,600 --> 00:09:51,920
and swap other pieces in and out so uh

00:09:50,640 --> 00:09:53,440
i feel like we've been teasing the

00:09:51,920 --> 00:09:54,080
unboxing part for too long now though

00:09:53,440 --> 00:09:56,720
yeah

00:09:54,080 --> 00:09:57,120
let's push it on let's dig into it so

00:09:56,720 --> 00:09:59,760
the

00:09:57,120 --> 00:10:00,160
the first component to really look into

00:09:59,760 --> 00:10:02,160
is

00:10:00,160 --> 00:10:03,519
is monitoring any production system well

00:10:02,160 --> 00:10:04,000
any system that i want to ship to

00:10:03,519 --> 00:10:05,760
production

00:10:04,000 --> 00:10:07,680
maybe it doesn't always happen but i try

00:10:05,760 --> 00:10:11,600
to make it happen should have

00:10:07,680 --> 00:10:13,920
observability for monitoring uh and so

00:10:11,600 --> 00:10:15,120
kate sandra uh has taken the q

00:10:13,920 --> 00:10:18,079
prometheus stack

00:10:15,120 --> 00:10:18,720
uh helm chart and and brought it in but

00:10:18,079 --> 00:10:22,320
we we

00:10:18,720 --> 00:10:24,640
actually handle all the wiring of

00:10:22,320 --> 00:10:25,680
prometheus and the the running cassandra

00:10:24,640 --> 00:10:27,920
cluster so

00:10:25,680 --> 00:10:29,200
uh the kate sander distribution make

00:10:27,920 --> 00:10:31,120
sure that we enables

00:10:29,200 --> 00:10:32,720
the metric collector for apache

00:10:31,120 --> 00:10:34,000
cassandra inside of each of the

00:10:32,720 --> 00:10:35,680
containers

00:10:34,000 --> 00:10:37,200
uh and then it spins up the service

00:10:35,680 --> 00:10:38,399
monitors in the dashboard so actually

00:10:37,200 --> 00:10:40,240
wire this in

00:10:38,399 --> 00:10:41,440
to prometheus and grafana running inside

00:10:40,240 --> 00:10:44,959
of your

00:10:41,440 --> 00:10:46,640
your kubernetes infrastructure now

00:10:44,959 --> 00:10:49,200
you might already have that in your

00:10:46,640 --> 00:10:51,839
infrastructure we're seeing a number of

00:10:49,200 --> 00:10:52,800
uh vendor-backed kubernetes distros

00:10:51,839 --> 00:10:54,399
actually

00:10:52,800 --> 00:10:56,320
already shipped yes pre-installed it's

00:10:54,399 --> 00:10:59,279
centralized centralized monitoring is

00:10:56,320 --> 00:11:01,519
something that you would see and under

00:10:59,279 --> 00:11:03,120
the the banner of composability

00:11:01,519 --> 00:11:05,040
you can just turn this off like you can

00:11:03,120 --> 00:11:06,880
say i already have my own prometheus i

00:11:05,040 --> 00:11:09,920
already have my own graphoni just

00:11:06,880 --> 00:11:11,760
don't bother with that but we have the

00:11:09,920 --> 00:11:12,720
hooks in place that if you do want kit

00:11:11,760 --> 00:11:15,120
sander to still

00:11:12,720 --> 00:11:16,240
push out the service monitors which uh

00:11:15,120 --> 00:11:18,959
provide the

00:11:16,240 --> 00:11:19,839
the the plumbing between uh prometheus's

00:11:18,959 --> 00:11:21,680
scrape targets

00:11:19,839 --> 00:11:23,360
and the actual nodes we can still

00:11:21,680 --> 00:11:25,120
provision this for you without setting

00:11:23,360 --> 00:11:27,120
up an entire stack

00:11:25,120 --> 00:11:29,040
right and that's a that's a really nice

00:11:27,120 --> 00:11:31,200
and flexible configuration

00:11:29,040 --> 00:11:32,079
one of the things that i love about

00:11:31,200 --> 00:11:34,720
grafana

00:11:32,079 --> 00:11:35,360
you know as a as a platform for uh

00:11:34,720 --> 00:11:37,839
graphical

00:11:35,360 --> 00:11:38,480
and prometheus as well is you know the

00:11:37,839 --> 00:11:41,040
fact that

00:11:38,480 --> 00:11:42,320
um i can build a complete observability

00:11:41,040 --> 00:11:44,160
solution for

00:11:42,320 --> 00:11:46,399
my application all the way down the

00:11:44,160 --> 00:11:50,320
stack right so i can have

00:11:46,399 --> 00:11:53,120
uh graphs that are showing the workload

00:11:50,320 --> 00:11:54,639
that's coming in on my microservices and

00:11:53,120 --> 00:11:55,600
my applications here but also put that

00:11:54,639 --> 00:11:56,880
alongside

00:11:55,600 --> 00:11:58,560
graphs that are showing what's happening

00:11:56,880 --> 00:12:00,320
with the underlying standard database

00:11:58,560 --> 00:12:02,480
and i can you know compose my own

00:12:00,320 --> 00:12:03,920
dashboard that really kind of gives me a

00:12:02,480 --> 00:12:05,600
top to bottom picture which i think is

00:12:03,920 --> 00:12:07,440
just amazing

00:12:05,600 --> 00:12:09,040
yeah i love the the composition of

00:12:07,440 --> 00:12:10,720
looking at your application metrics and

00:12:09,040 --> 00:12:12,720
look at looking at your database metrics

00:12:10,720 --> 00:12:14,480
okay where exactly is this going

00:12:12,720 --> 00:12:17,760
sideways

00:12:14,480 --> 00:12:20,800
it can really give you the whole picture

00:12:17,760 --> 00:12:23,120
um i think it's right some people might

00:12:20,800 --> 00:12:25,040
say well what if i don't use

00:12:23,120 --> 00:12:27,920
uh prometheus what do you think about

00:12:25,040 --> 00:12:30,639
that uh jeff

00:12:27,920 --> 00:12:31,360
uh well as we've been talking about the

00:12:30,639 --> 00:12:33,519
uh

00:12:31,360 --> 00:12:34,880
you know that that format of the

00:12:33,519 --> 00:12:36,800
prometheus

00:12:34,880 --> 00:12:38,720
the way that it represents metrics is

00:12:36,800 --> 00:12:40,160
becoming some of somewhat of a de facto

00:12:38,720 --> 00:12:42,160
standard and so

00:12:40,160 --> 00:12:44,079
we're seeing a lot of other monitoring

00:12:42,160 --> 00:12:45,200
stacks provide compatibility with that

00:12:44,079 --> 00:12:47,360
so

00:12:45,200 --> 00:12:48,399
i think that's our short-term answer for

00:12:47,360 --> 00:12:50,560
that right

00:12:48,399 --> 00:12:51,600
yeah even if you don't use prometheus a

00:12:50,560 --> 00:12:53,360
number of like the

00:12:51,600 --> 00:12:55,440
monitoring solutions out there can still

00:12:53,360 --> 00:12:58,639
scrape those those metrics and and

00:12:55,440 --> 00:13:00,399
ingest them into their systems

00:12:58,639 --> 00:13:02,160
all right so monitoring knowing what's

00:13:00,399 --> 00:13:03,839
going on is one thing

00:13:02,160 --> 00:13:05,519
but there's some definitely some

00:13:03,839 --> 00:13:07,440
operational tasks

00:13:05,519 --> 00:13:08,880
that kubernetes itself isn't going to

00:13:07,440 --> 00:13:10,320
magically just do for you

00:13:08,880 --> 00:13:12,160
so i feel like we should talk about some

00:13:10,320 --> 00:13:14,480
of those

00:13:12,160 --> 00:13:16,320
yeah and i think one of the key things

00:13:14,480 --> 00:13:18,480
that comes to my mind is well

00:13:16,320 --> 00:13:19,760
what happens if so monitoring is great

00:13:18,480 --> 00:13:20,639
but all of a sudden i just don't have

00:13:19,760 --> 00:13:23,760
notes

00:13:20,639 --> 00:13:26,639
when we do that there

00:13:23,760 --> 00:13:28,880
there's a stray command point in the

00:13:26,639 --> 00:13:30,480
wrong name space and now i don't have my

00:13:28,880 --> 00:13:32,240
cluster what can i do

00:13:30,480 --> 00:13:33,920
it's not just human error too right i

00:13:32,240 --> 00:13:35,360
mean we have worker nodes that can fail

00:13:33,920 --> 00:13:37,200
in a kubernetes cluster and it's

00:13:35,360 --> 00:13:38,959
designed around that

00:13:37,200 --> 00:13:41,120
but sure and now what if those nodes

00:13:38,959 --> 00:13:42,320
have data

00:13:41,120 --> 00:13:44,079
yeah and that's that's where the

00:13:42,320 --> 00:13:45,440
stateful conversation gets interesting

00:13:44,079 --> 00:13:50,160
right and so

00:13:45,440 --> 00:13:52,320
we've we've taken the the medusa project

00:13:50,160 --> 00:13:53,680
out of the last pickle and i think it

00:13:52,320 --> 00:13:54,160
i'm trying to remember if that came out

00:13:53,680 --> 00:13:56,720
of

00:13:54,160 --> 00:13:57,440
spotify or if that was the repair system

00:13:56,720 --> 00:14:02,079
but

00:13:57,440 --> 00:14:05,120
in any case it it powers backups

00:14:02,079 --> 00:14:08,240
via kubernetes custom resources

00:14:05,120 --> 00:14:09,120
to a object store so you can instead of

00:14:08,240 --> 00:14:10,720
having to

00:14:09,120 --> 00:14:13,040
kind of orchestrate this by hand you

00:14:10,720 --> 00:14:14,800
just say hey i want a cassandra backup

00:14:13,040 --> 00:14:16,560
and it says all right it will reach out

00:14:14,800 --> 00:14:17,279
to all the nodes trigger the backup

00:14:16,560 --> 00:14:20,000
operation

00:14:17,279 --> 00:14:22,480
snapshots the data files then ships

00:14:20,000 --> 00:14:26,399
those off to an s3 compatible

00:14:22,480 --> 00:14:28,240
object store so and that's part of the

00:14:26,399 --> 00:14:29,440
the next release that jeff into that

00:14:28,240 --> 00:14:32,720
earlier is

00:14:29,440 --> 00:14:34,959
compatibility with midio and uh

00:14:32,720 --> 00:14:36,639
as as an example an example s3

00:14:34,959 --> 00:14:37,920
compatible store

00:14:36,639 --> 00:14:40,000
yeah but what do you think some of the

00:14:37,920 --> 00:14:42,560
other use cases are for this kind of

00:14:40,000 --> 00:14:43,199
uh backup and restore functionality jeff

00:14:42,560 --> 00:14:44,399
um

00:14:43,199 --> 00:14:46,800
yeah we're starting to see a lot of

00:14:44,399 --> 00:14:49,279
interest in uh incorporating

00:14:46,800 --> 00:14:51,680
uh backup and restore as a tool for

00:14:49,279 --> 00:14:53,440
building ci cd pipelines so in other

00:14:51,680 --> 00:14:55,680
words i want to spin up

00:14:53,440 --> 00:14:56,480
uh you know an instance of my

00:14:55,680 --> 00:14:59,040
application

00:14:56,480 --> 00:15:00,800
stack in order to run integration tests

00:14:59,040 --> 00:15:02,240
against it let's say and we want to

00:15:00,800 --> 00:15:04,480
as part of that to have the database

00:15:02,240 --> 00:15:05,600
there as well but maybe i want to have

00:15:04,480 --> 00:15:07,279
an initial state

00:15:05,600 --> 00:15:09,279
that's represented in the database so i

00:15:07,279 --> 00:15:10,399
can actually use it's pretty handy to be

00:15:09,279 --> 00:15:12,720
able to

00:15:10,399 --> 00:15:14,079
restore a data set and then uh rather

00:15:12,720 --> 00:15:15,120
than having to do like a bunch of data

00:15:14,079 --> 00:15:17,839
loading tasks

00:15:15,120 --> 00:15:18,880
or run kubernetes jobs to do that you

00:15:17,839 --> 00:15:22,000
can actually just

00:15:18,880 --> 00:15:24,720
uh do a restore and you again because

00:15:22,000 --> 00:15:26,639
there is an api for this in medusa

00:15:24,720 --> 00:15:27,920
you can easily you know just hit that

00:15:26,639 --> 00:15:29,519
api do the restore

00:15:27,920 --> 00:15:30,800
and script that as as part of your

00:15:29,519 --> 00:15:31,680
pipeline so that's i think that's pretty

00:15:30,800 --> 00:15:33,519
cool

00:15:31,680 --> 00:15:35,440
yeah and it's all done with cube ctl too

00:15:33,519 --> 00:15:38,000
so there's no extra

00:15:35,440 --> 00:15:38,560
cli or any any interfaces that you have

00:15:38,000 --> 00:15:40,720
to access

00:15:38,560 --> 00:15:41,600
it's a custom resource inside of your

00:15:40,720 --> 00:15:44,880
environment

00:15:41,600 --> 00:15:46,720
that's a great yeah you're using uh

00:15:44,880 --> 00:15:49,199
cassandra friendly terminology and

00:15:46,720 --> 00:15:52,959
abstractions that are built on top of

00:15:49,199 --> 00:15:54,160
kubernetes at that point so

00:15:52,959 --> 00:15:56,240
one of the interesting things about a

00:15:54,160 --> 00:15:59,519
distributed system though is

00:15:56,240 --> 00:16:00,320
not is that data can sometimes get out

00:15:59,519 --> 00:16:01,600
of sync and we

00:16:00,320 --> 00:16:03,199
we do everything we can to make sure

00:16:01,600 --> 00:16:04,160
that doesn't happen cassandra's been

00:16:03,199 --> 00:16:06,560
around for

00:16:04,160 --> 00:16:08,000
i think it's north of 10 years now and

00:16:06,560 --> 00:16:10,720
so they're

00:16:08,000 --> 00:16:12,000
is we call it repair i like to refer to

00:16:10,720 --> 00:16:13,759
it as anti-entropy things are going to

00:16:12,000 --> 00:16:17,680
get out of sync though whether that's

00:16:13,759 --> 00:16:19,680
uh maybe a gc ran too long and mr right

00:16:17,680 --> 00:16:20,959
or the node was just down when a right

00:16:19,680 --> 00:16:22,399
came in

00:16:20,959 --> 00:16:24,480
and like i said there's a layered

00:16:22,399 --> 00:16:26,320
approach to resolving these issues

00:16:24,480 --> 00:16:27,920
um but i think it's worth noting that

00:16:26,320 --> 00:16:30,880
there's one that's called

00:16:27,920 --> 00:16:32,639
the out-of-band repair process in this

00:16:30,880 --> 00:16:36,639
case we're using

00:16:32,639 --> 00:16:36,639
a process called reaper which

00:16:37,279 --> 00:16:41,920
handles calling the api endpoints to

00:16:40,240 --> 00:16:43,759
trigger these repair processes

00:16:41,920 --> 00:16:45,360
we have a best practice where you want

00:16:43,759 --> 00:16:48,320
to do this every 10 days across your

00:16:45,360 --> 00:16:51,120
entire cluster so do a full repair

00:16:48,320 --> 00:16:52,320
and again ideally it's it's very minor

00:16:51,120 --> 00:16:54,560
repairs that need to happen

00:16:52,320 --> 00:16:55,839
um and there are other systems in place

00:16:54,560 --> 00:16:58,240
to assist with this

00:16:55,839 --> 00:16:59,839
uh but what this tool does is instead of

00:16:58,240 --> 00:17:01,040
you as an operator having to log into

00:16:59,839 --> 00:17:03,120
each server and say all right

00:17:01,040 --> 00:17:04,160
server a it's your turn today running

00:17:03,120 --> 00:17:05,760
the repair command

00:17:04,160 --> 00:17:07,679
this actually breaks it up into smaller

00:17:05,760 --> 00:17:11,839
chunks and runs it over

00:17:07,679 --> 00:17:11,839
over that entire uh 10-day period

00:17:13,280 --> 00:17:16,400
right yeah this is one of those uh

00:17:15,679 --> 00:17:18,000
things that's

00:17:16,400 --> 00:17:19,600
you know one of those fine print kinds

00:17:18,000 --> 00:17:20,160
of things that a lot of people forget to

00:17:19,600 --> 00:17:22,559
do

00:17:20,160 --> 00:17:24,240
uh you know in order to maintain the

00:17:22,559 --> 00:17:25,679
health of their system until

00:17:24,240 --> 00:17:27,520
um all of a sudden they realize that

00:17:25,679 --> 00:17:29,280
it's a problem um so it's not that big

00:17:27,520 --> 00:17:29,679
of a deal if you start doing it from day

00:17:29,280 --> 00:17:32,080
one

00:17:29,679 --> 00:17:34,160
but then if you you know if you don't

00:17:32,080 --> 00:17:35,760
follow the proper operational procedures

00:17:34,160 --> 00:17:37,760
and ignore it then that you're not in a

00:17:35,760 --> 00:17:40,320
good situation either

00:17:37,760 --> 00:17:41,919
yeah and and for what it's worth i like

00:17:40,320 --> 00:17:43,360
the phrase always be repairing

00:17:41,919 --> 00:17:44,320
and even when you're doing your capacity

00:17:43,360 --> 00:17:46,240
planning you should make sure you have

00:17:44,320 --> 00:17:48,320
repairs turned on and running

00:17:46,240 --> 00:17:49,760
so you can account for that operational

00:17:48,320 --> 00:17:50,799
overhead when you're trying to figure

00:17:49,760 --> 00:17:52,799
out how many nodes you need for your

00:17:50,799 --> 00:17:55,840
cluster

00:17:52,799 --> 00:17:59,039
yes yes of course so

00:17:55,840 --> 00:18:00,880
we mentioned here a second ago

00:17:59,039 --> 00:18:01,919
taking this away from the the human

00:18:00,880 --> 00:18:02,720
operator operator's kind of an

00:18:01,919 --> 00:18:05,039
overloaded term

00:18:02,720 --> 00:18:06,960
in kubernetes land uh taking away from

00:18:05,039 --> 00:18:07,600
the computer operators with the dbas

00:18:06,960 --> 00:18:10,559
right

00:18:07,600 --> 00:18:12,320
and instead of having them run this we

00:18:10,559 --> 00:18:15,600
have a helpful tool here but

00:18:12,320 --> 00:18:18,880
when we talk about running

00:18:15,600 --> 00:18:20,480
a distributed system it can be tricky

00:18:18,880 --> 00:18:22,240
and automation helps with that

00:18:20,480 --> 00:18:24,240
significantly i mean i've

00:18:22,240 --> 00:18:25,280
i've seen clusters that had hundreds of

00:18:24,240 --> 00:18:28,400
nodes

00:18:25,280 --> 00:18:30,000
uh that were all configured by hand

00:18:28,400 --> 00:18:31,679
we very quickly moved over to an

00:18:30,000 --> 00:18:33,039
automation tool i think that particular

00:18:31,679 --> 00:18:34,559
case was ansible

00:18:33,039 --> 00:18:37,039
and it was helpful right you can still

00:18:34,559 --> 00:18:39,120
log in and say okay i want to deploy

00:18:37,039 --> 00:18:40,640
cassandra to all these servers and it

00:18:39,120 --> 00:18:41,360
would go through to each one install the

00:18:40,640 --> 00:18:43,039
packages

00:18:41,360 --> 00:18:44,559
and start it up and rinse and repeat

00:18:43,039 --> 00:18:45,840
throughout the rest of the cluster but i

00:18:44,559 --> 00:18:47,520
tell you what when you get into hundreds

00:18:45,840 --> 00:18:50,720
of nodes that can take

00:18:47,520 --> 00:18:51,919
hours it can take a lot of time uh and

00:18:50,720 --> 00:18:54,080
so one of the things that

00:18:51,919 --> 00:18:56,240
that we've created uh and we mentioned

00:18:54,080 --> 00:18:57,679
it earlier was this cass operator it's a

00:18:56,240 --> 00:18:59,280
cassandra operator

00:18:57,679 --> 00:19:01,840
and it does a really good job of

00:18:59,280 --> 00:19:04,559
translating these like logical concepts

00:19:01,840 --> 00:19:05,760
of a cassandra data center and cassandra

00:19:04,559 --> 00:19:07,440
racks and

00:19:05,760 --> 00:19:10,000
the size of a cluster in the topology

00:19:07,440 --> 00:19:13,280
and all this into

00:19:10,000 --> 00:19:15,039
kubernetes resources so it'll say okay i

00:19:13,280 --> 00:19:16,559
know that a logical rack inside of

00:19:15,039 --> 00:19:18,720
cassandra is equal to a very

00:19:16,559 --> 00:19:19,679
stable set yeah if you describe that

00:19:18,720 --> 00:19:22,000
rack to me

00:19:19,679 --> 00:19:23,440
i will i being as operator we'll go

00:19:22,000 --> 00:19:25,600
ahead and say hey

00:19:23,440 --> 00:19:27,039
kubernetes api can you make a staple set

00:19:25,600 --> 00:19:28,799
for me

00:19:27,039 --> 00:19:31,120
and just like we mentioned with with

00:19:28,799 --> 00:19:32,720
repairs and with uh

00:19:31,120 --> 00:19:34,960
with backups um there are other

00:19:32,720 --> 00:19:36,480
operators as well so the meduse operator

00:19:34,960 --> 00:19:38,559
the reaper operator handled that

00:19:36,480 --> 00:19:40,000
translation for you uh but they're also

00:19:38,559 --> 00:19:41,280
kubernetes controllers they do

00:19:40,000 --> 00:19:44,320
reconciliation so

00:19:41,280 --> 00:19:46,400
in the case of a pod going down uh

00:19:44,320 --> 00:19:47,360
for for cassandra kubernetes will

00:19:46,400 --> 00:19:49,440
restart the pod

00:19:47,360 --> 00:19:50,880
right and try to bring it back online

00:19:49,440 --> 00:19:53,120
but uh

00:19:50,880 --> 00:19:54,720
in the complexity of running a

00:19:53,120 --> 00:19:56,799
distributed system maybe we want to

00:19:54,720 --> 00:19:58,400
restart pods in a certain order

00:19:56,799 --> 00:20:00,240
uh one of the common operations is a

00:19:58,400 --> 00:20:02,400
rolling restart of a cassandra cluster

00:20:00,240 --> 00:20:04,080
and so you can just in your custom

00:20:02,400 --> 00:20:06,320
resource say i want you to restart

00:20:04,080 --> 00:20:08,320
and the operator will handle terminating

00:20:06,320 --> 00:20:09,440
pods letting them come back up

00:20:08,320 --> 00:20:11,760
in a rolling fashion throughout the

00:20:09,440 --> 00:20:12,720
cluster or performing upgrades that kind

00:20:11,760 --> 00:20:14,799
of thing

00:20:12,720 --> 00:20:16,159
yeah that's the upgrade case i think is

00:20:14,799 --> 00:20:17,840
super interesting i mean

00:20:16,159 --> 00:20:21,120
basically it's a it's an instance of a

00:20:17,840 --> 00:20:22,000
rolling restart with a little twist

00:20:21,120 --> 00:20:24,159
yeah we're just going to change the

00:20:22,000 --> 00:20:24,960
binaries on you uh i feel like we've

00:20:24,159 --> 00:20:26,960
been

00:20:24,960 --> 00:20:28,320
given team we've given a lot of love to

00:20:26,960 --> 00:20:30,640
the operations

00:20:28,320 --> 00:20:32,240
side here chris i don't know you should

00:20:30,640 --> 00:20:34,720
finish your point but i want to talk

00:20:32,240 --> 00:20:36,159
about devs too oh yeah sure

00:20:34,720 --> 00:20:37,919
i think one of the things that that is

00:20:36,159 --> 00:20:39,679
worth calling out though is

00:20:37,919 --> 00:20:41,039
and when you go to do something like a

00:20:39,679 --> 00:20:41,440
like an upgrade you want to make sure

00:20:41,039 --> 00:20:42,640
that

00:20:41,440 --> 00:20:44,720
everything isn't just going to go

00:20:42,640 --> 00:20:46,159
sideways i would i would hate to say

00:20:44,720 --> 00:20:48,000
okay switch out all the binaries and for

00:20:46,159 --> 00:20:49,360
the cluster to not come back up

00:20:48,000 --> 00:20:51,120
so one of the key features that we

00:20:49,360 --> 00:20:52,799
really don't talk about too much is that

00:20:51,120 --> 00:20:54,159
we allow for canary upgrades so we can

00:20:52,799 --> 00:20:56,799
target a single node

00:20:54,159 --> 00:20:58,640
or a selection of nodes to upgrade

00:20:56,799 --> 00:21:00,240
before moving on to

00:20:58,640 --> 00:21:01,600
the rest of the cluster as a whole but

00:21:00,240 --> 00:21:02,799
to your point jeff i do think we need to

00:21:01,600 --> 00:21:03,840
switch gears and start talking about how

00:21:02,799 --> 00:21:06,000
the heck do we actually talk to the

00:21:03,840 --> 00:21:07,280
system instead of just running it

00:21:06,000 --> 00:21:09,200
well that's right i mean if we have a

00:21:07,280 --> 00:21:10,960
perfectly running database that uh no

00:21:09,200 --> 00:21:12,720
one's putting data in or getting data

00:21:10,960 --> 00:21:15,760
out i don't know that's much use

00:21:12,720 --> 00:21:17,760
so now traditionally in

00:21:15,760 --> 00:21:19,360
a cassandra database we have the center

00:21:17,760 --> 00:21:21,360
query language

00:21:19,360 --> 00:21:23,679
that's very similar to you know classic

00:21:21,360 --> 00:21:27,760
sql that that we're using

00:21:23,679 --> 00:21:30,720
one of the things that we've found is uh

00:21:27,760 --> 00:21:32,320
that a lot of companies have begun

00:21:30,720 --> 00:21:35,200
building out their own

00:21:32,320 --> 00:21:36,880
api layers on top of cassandra and not

00:21:35,200 --> 00:21:40,240
necessarily letting every

00:21:36,880 --> 00:21:42,720
application developer just write raw cql

00:21:40,240 --> 00:21:42,720
queries

00:21:42,840 --> 00:21:49,039
true yeah definitely

00:21:46,559 --> 00:21:50,720
and uh so you know the the purpose of

00:21:49,039 --> 00:21:52,240
this stargate project that we've started

00:21:50,720 --> 00:21:52,880
this is another open source project

00:21:52,240 --> 00:21:55,679
that's kind of a

00:21:52,880 --> 00:21:56,960
you know a companion or or fits nicely

00:21:55,679 --> 00:21:59,840
with kate sandra

00:21:56,960 --> 00:22:00,320
and we've incorporated stargate indicate

00:21:59,840 --> 00:22:03,120
sandra

00:22:00,320 --> 00:22:04,960
so it is cql compatible access to

00:22:03,120 --> 00:22:08,159
cassandra but also provides

00:22:04,960 --> 00:22:10,960
rest graphql and document

00:22:08,159 --> 00:22:12,720
apis so the idea is you know no matter

00:22:10,960 --> 00:22:14,960
what flavor of api

00:22:12,720 --> 00:22:16,159
you're most comfortable with interacting

00:22:14,960 --> 00:22:17,600
with as a developer

00:22:16,159 --> 00:22:19,520
that's that's something that's available

00:22:17,600 --> 00:22:21,440
to you

00:22:19,520 --> 00:22:23,520
yeah and you might have an existing

00:22:21,440 --> 00:22:24,880
application that already speaks cql

00:22:23,520 --> 00:22:26,159
but one of the things that can be tricky

00:22:24,880 --> 00:22:27,919
at least it was for me when i first

00:22:26,159 --> 00:22:30,640
started writing cassandra

00:22:27,919 --> 00:22:32,320
back in 2013 i went to cassandra summit

00:22:30,640 --> 00:22:35,039
because the buddy had an extra ticket

00:22:32,320 --> 00:22:36,559
and i spent the the second day just

00:22:35,039 --> 00:22:38,400
trying to use cassandra and being

00:22:36,559 --> 00:22:39,600
so angry because i was like this looks

00:22:38,400 --> 00:22:41,280
just like sql

00:22:39,600 --> 00:22:42,640
and it's not there are certain things

00:22:41,280 --> 00:22:44,080
you're not allowed to do because it's a

00:22:42,640 --> 00:22:45,919
distributed system

00:22:44,080 --> 00:22:47,760
which is fascinating but i tell you what

00:22:45,919 --> 00:22:49,280
if i had a rest interface or

00:22:47,760 --> 00:22:52,400
a document interface i would have been

00:22:49,280 --> 00:22:53,679
able to go a lot further that day

00:22:52,400 --> 00:22:55,919
so let's take a look at what that looks

00:22:53,679 --> 00:22:56,640
like uh jeff you want to dig in a little

00:22:55,919 --> 00:23:00,559
bit into

00:22:56,640 --> 00:23:03,760
kind of the the interfaces that are

00:23:00,559 --> 00:23:04,559
sure i mean this this slide can you know

00:23:03,760 --> 00:23:06,720
appear pretty

00:23:04,559 --> 00:23:08,000
busy um i i'd encourage you to look at

00:23:06,720 --> 00:23:10,159
it in two halves the top

00:23:08,000 --> 00:23:11,840
half on the bottom half so the idea is

00:23:10,159 --> 00:23:12,880
that stargate is an extensible

00:23:11,840 --> 00:23:16,080
architecture

00:23:12,880 --> 00:23:18,159
you can add in new apis so

00:23:16,080 --> 00:23:19,520
existing right now are rest graphql and

00:23:18,159 --> 00:23:22,240
document api

00:23:19,520 --> 00:23:22,960
and of course the center query language

00:23:22,240 --> 00:23:25,840
there's a

00:23:22,960 --> 00:23:27,440
kafka integration in progress uh pulsar

00:23:25,840 --> 00:23:29,520
integration on the roadmap

00:23:27,440 --> 00:23:31,440
and then on the back end it's also

00:23:29,520 --> 00:23:34,320
designed so that you could plug in

00:23:31,440 --> 00:23:35,840
a compatible data store so right now the

00:23:34,320 --> 00:23:37,120
the ones that we have to date are

00:23:35,840 --> 00:23:41,120
cassandra compatible

00:23:37,120 --> 00:23:44,400
databases um so open source casino 3.1

00:23:41,120 --> 00:23:46,320
3.11 um the 4.0 release which cassandra

00:23:44,400 --> 00:23:48,960
4.0 is about

00:23:46,320 --> 00:23:50,480
to go live here imminently there's a

00:23:48,960 --> 00:23:53,520
release candidate out there right

00:23:50,480 --> 00:23:54,400
and uh so we'll we'll be upgrading case

00:23:53,520 --> 00:23:57,679
sandra as

00:23:54,400 --> 00:23:59,840
just as soon as that uh goes live

00:23:57,679 --> 00:24:01,760
um so that kate sander would ship with

00:23:59,840 --> 00:24:03,919
the official cassandra 4.0

00:24:01,760 --> 00:24:05,600
oh and then of course the sacs the

00:24:03,919 --> 00:24:07,600
enterprise distribution as well

00:24:05,600 --> 00:24:09,600
uh fits uh but the idea is that you

00:24:07,600 --> 00:24:11,279
could plug this in

00:24:09,600 --> 00:24:12,960
you could do the work and want to if you

00:24:11,279 --> 00:24:14,240
wanted to plug in a different database

00:24:12,960 --> 00:24:15,520
based engine on the back end

00:24:14,240 --> 00:24:17,039
so very composable that's the

00:24:15,520 --> 00:24:18,240
architectural principle that's at work

00:24:17,039 --> 00:24:19,600
here

00:24:18,240 --> 00:24:21,679
one of the fascinating things i think is

00:24:19,600 --> 00:24:24,080
kind of interesting here we've done some

00:24:21,679 --> 00:24:26,960
testing this space is that you can use

00:24:24,080 --> 00:24:27,760
stargate almost as a if you think about

00:24:26,960 --> 00:24:29,279
the traditional

00:24:27,760 --> 00:24:30,400
separation of compute and storage that

00:24:29,279 --> 00:24:30,960
people talk about inside their data

00:24:30,400 --> 00:24:32,720
centers

00:24:30,960 --> 00:24:34,080
you can do something similar here with

00:24:32,720 --> 00:24:37,760
with stargate it's

00:24:34,080 --> 00:24:39,279
it is a stateless layer in front of your

00:24:37,760 --> 00:24:41,279
your your backend database so if you

00:24:39,279 --> 00:24:42,799
have a large number of clients

00:24:41,279 --> 00:24:44,080
rather than having the actual nodes that

00:24:42,799 --> 00:24:45,520
hold the data handle like switching

00:24:44,080 --> 00:24:47,679
contacts and things like that

00:24:45,520 --> 00:24:48,559
um you can scale that layer

00:24:47,679 --> 00:24:50,400
independently

00:24:48,559 --> 00:24:52,000
of your data layer we've seen some

00:24:50,400 --> 00:24:53,440
reductions in latency

00:24:52,000 --> 00:24:54,240
while reducing the total number of nodes

00:24:53,440 --> 00:24:56,000
in the cluster which is kind of

00:24:54,240 --> 00:24:57,440
fascinating it's especially great for

00:24:56,000 --> 00:25:01,120
read heavy workloads

00:24:57,440 --> 00:25:01,840
so so let's look at what it takes to

00:25:01,120 --> 00:25:05,200
actually

00:25:01,840 --> 00:25:06,799
install uh kate sandra and actually

00:25:05,200 --> 00:25:08,400
fire it up inside of your your

00:25:06,799 --> 00:25:09,360
kubernetes environment so it's pretty

00:25:08,400 --> 00:25:12,320
straightforward

00:25:09,360 --> 00:25:13,919
you add the helm repo for case sandra

00:25:12,320 --> 00:25:15,360
you update your repo to make sure you

00:25:13,919 --> 00:25:18,480
have the latest version of the chart

00:25:15,360 --> 00:25:20,000
and then you just install it um and

00:25:18,480 --> 00:25:21,279
given that we have a helm install

00:25:20,000 --> 00:25:22,240
command here i think it's probably a

00:25:21,279 --> 00:25:23,360
good time to

00:25:22,240 --> 00:25:24,720
transition actually showing you what

00:25:23,360 --> 00:25:25,679
this looks like why don't you just show

00:25:24,720 --> 00:25:28,960
us yeah

00:25:25,679 --> 00:25:31,120
yeah so i i've actually run that same

00:25:28,960 --> 00:25:33,679
command here inside of my terminal

00:25:31,120 --> 00:25:34,720
um we're just doing a helm install here

00:25:33,679 --> 00:25:36,480
i'm overriding

00:25:34,720 --> 00:25:38,880
a couple of values that are unique just

00:25:36,480 --> 00:25:40,799
to my machine it's nothing crazy i just

00:25:38,880 --> 00:25:42,240
told it to use a specific storage class

00:25:40,799 --> 00:25:43,440
inside of my kubernetes cluster called

00:25:42,240 --> 00:25:45,760
local path

00:25:43,440 --> 00:25:48,559
and the data volume i want to use is is

00:25:45,760 --> 00:25:50,799
one gigabyte in size

00:25:48,559 --> 00:25:52,000
when this this takes about two minutes

00:25:50,799 --> 00:25:53,840
per node to

00:25:52,000 --> 00:25:55,200
to spin up but rather than waste your

00:25:53,840 --> 00:25:56,159
time here i've already run it so let's

00:25:55,200 --> 00:25:59,120
go over here to lens

00:25:56,159 --> 00:26:00,159
though and try and make this uh i cannot

00:25:59,120 --> 00:26:01,440
make that a little bit thicker that's

00:26:00,159 --> 00:26:03,279
okay

00:26:01,440 --> 00:26:05,039
and let's take a look at the components

00:26:03,279 --> 00:26:05,360
here that have been deployed so if i go

00:26:05,039 --> 00:26:08,640
back

00:26:05,360 --> 00:26:11,279
and i look at my my custom resources

00:26:08,640 --> 00:26:12,480
we can see there is a cassandra data

00:26:11,279 --> 00:26:13,520
center that's been provisioned called

00:26:12,480 --> 00:26:16,000
dc1

00:26:13,520 --> 00:26:17,679
um and if we go and actually look at it

00:26:16,000 --> 00:26:20,080
there's there's a bit more here so this

00:26:17,679 --> 00:26:22,240
is all templated out with helm

00:26:20,080 --> 00:26:23,120
managed fields tends to be a bit verbose

00:26:22,240 --> 00:26:26,080
but you can see

00:26:23,120 --> 00:26:26,640
we've configured password authentication

00:26:26,080 --> 00:26:30,480
um

00:26:26,640 --> 00:26:31,600
we've set the number of replicas this is

00:26:30,480 --> 00:26:33,039
a single node installation because i'm

00:26:31,600 --> 00:26:35,600
running here on my laptop

00:26:33,039 --> 00:26:37,840
but if you're in a larger environment

00:26:35,600 --> 00:26:39,679
let's say you have a three node cluster

00:26:37,840 --> 00:26:41,679
we could set the replication factor to

00:26:39,679 --> 00:26:45,440
three

00:26:41,679 --> 00:26:47,679
we describe uh in this case we have ways

00:26:45,440 --> 00:26:49,039
to override certain

00:26:47,679 --> 00:26:50,960
pieces inside of the cassandra data

00:26:49,039 --> 00:26:51,520
center so in this case we're configuring

00:26:50,960 --> 00:26:53,600
reaper

00:26:51,520 --> 00:26:56,000
we mentioned that part of cate sandra is

00:26:53,600 --> 00:26:58,000
we wanted to have a cohesive platform

00:26:56,000 --> 00:27:00,159
and so we we handle setting up things

00:26:58,000 --> 00:27:02,400
like init containers for you

00:27:00,159 --> 00:27:04,240
instead of having to roll that yourself

00:27:02,400 --> 00:27:07,919
but let's say you have

00:27:04,240 --> 00:27:09,520
a a special system for handling secrets

00:27:07,919 --> 00:27:10,799
or for ssl certificates

00:27:09,520 --> 00:27:12,799
you can actually add your own init

00:27:10,799 --> 00:27:14,159
container here to request those

00:27:12,799 --> 00:27:16,720
certificates and add them to

00:27:14,159 --> 00:27:18,799
configuration so this is

00:27:16,720 --> 00:27:20,480
secure by default i think that's an

00:27:18,799 --> 00:27:21,039
important thing to note is that when we

00:27:20,480 --> 00:27:24,720
deploy

00:27:21,039 --> 00:27:27,200
cassandra and kubernetes here i mean

00:27:24,720 --> 00:27:28,559
it's uh it's not just wide open access

00:27:27,200 --> 00:27:29,919
right

00:27:28,559 --> 00:27:31,120
yeah and for what it's worth i don't

00:27:29,919 --> 00:27:35,039
know the credentials yet we're going to

00:27:31,120 --> 00:27:35,039
find those out here in a second for how

00:27:36,159 --> 00:27:40,080
but we can see we have a helpful status

00:27:38,320 --> 00:27:41,440
as well i believe we just added to the

00:27:40,080 --> 00:27:42,799
docs it's really nice one liner where

00:27:41,440 --> 00:27:43,760
you can just say hey i want you to wait

00:27:42,799 --> 00:27:45,200
until the kubernetes

00:27:43,760 --> 00:27:47,600
until the cassandra cluster is up and

00:27:45,200 --> 00:27:48,399
running but let's go back to our pods

00:27:47,600 --> 00:27:50,080
view

00:27:48,399 --> 00:27:52,480
and take a look at what's going on here

00:27:50,080 --> 00:27:54,080
so here we can see

00:27:52,480 --> 00:27:55,440
a number of components first we have the

00:27:54,080 --> 00:27:56,320
actual cast operator that we mentioned

00:27:55,440 --> 00:27:57,760
before

00:27:56,320 --> 00:28:00,559
there's the operator for prometheus

00:27:57,760 --> 00:28:01,760
reaper and medusa is not enabled on this

00:28:00,559 --> 00:28:03,919
particular instance

00:28:01,760 --> 00:28:05,919
but if we had enabled medusa we'd see

00:28:03,919 --> 00:28:07,440
them produce operator running as well

00:28:05,919 --> 00:28:09,039
there's a job that ran to configure the

00:28:07,440 --> 00:28:10,559
schema for reaper which just

00:28:09,039 --> 00:28:12,240
is for tracking the progress of the

00:28:10,559 --> 00:28:14,159
repairs

00:28:12,240 --> 00:28:16,159
we can see our grafana and then these

00:28:14,159 --> 00:28:17,919
two are the really important ones

00:28:16,159 --> 00:28:19,919
in my opinion we have our stargate pod

00:28:17,919 --> 00:28:23,279
so a single stargate instance

00:28:19,919 --> 00:28:25,760
as well as as the cassandra pod

00:28:23,279 --> 00:28:27,120
and what's fascinating here is we

00:28:25,760 --> 00:28:29,760
mentioned a little bit earlier that

00:28:27,120 --> 00:28:31,120
cassandra racks are analogous to

00:28:29,760 --> 00:28:33,440
stateful sets

00:28:31,120 --> 00:28:34,320
and so when we when we talk about like

00:28:33,440 --> 00:28:36,399
the topology

00:28:34,320 --> 00:28:38,080
of a cassandra cluster what what's the

00:28:36,399 --> 00:28:40,240
purpose of a rack here jeff can

00:28:38,080 --> 00:28:41,919
you go into that a little bit sure i

00:28:40,240 --> 00:28:44,799
mean in cassandra we use

00:28:41,919 --> 00:28:46,159
racks because we want to achieve high

00:28:44,799 --> 00:28:49,600
availability of our data

00:28:46,159 --> 00:28:51,760
by using replication so the idea is

00:28:49,600 --> 00:28:53,279
that a rack is a representation of kind

00:28:51,760 --> 00:28:56,240
of like a physical server

00:28:53,279 --> 00:28:57,679
or a failure domain such that you know

00:28:56,240 --> 00:28:59,360
if we have a

00:28:57,679 --> 00:29:00,799
piece of physical equipment that's

00:28:59,360 --> 00:29:02,320
running our uh our

00:29:00,799 --> 00:29:03,919
node in our cassandra cluster that goes

00:29:02,320 --> 00:29:06,240
down we don't want all of our

00:29:03,919 --> 00:29:07,679
standard nodes to be you know on that

00:29:06,240 --> 00:29:09,440
piece of hardware that goes down we want

00:29:07,679 --> 00:29:10,880
to have some distribution of cassandra

00:29:09,440 --> 00:29:11,520
nodes that are on running on different

00:29:10,880 --> 00:29:14,080
hardware

00:29:11,520 --> 00:29:16,080
so the these concepts that are built

00:29:14,080 --> 00:29:19,200
into cassandra's terminology like

00:29:16,080 --> 00:29:21,600
data center and rack are used

00:29:19,200 --> 00:29:22,320
uh to manage the topology of the cluster

00:29:21,600 --> 00:29:24,240
such that

00:29:22,320 --> 00:29:26,320
the different copies of each piece of

00:29:24,240 --> 00:29:26,960
data that are stored are actually spread

00:29:26,320 --> 00:29:30,240
out

00:29:26,960 --> 00:29:31,760
across multiple failure domains so if i

00:29:30,240 --> 00:29:33,840
was looking at a cloud environment would

00:29:31,760 --> 00:29:35,760
it be safe to say that a rack is like a

00:29:33,840 --> 00:29:38,720
an az and a data center is more

00:29:35,760 --> 00:29:40,559
analogous to like a a region

00:29:38,720 --> 00:29:42,159
that's the i would say that that's the

00:29:40,559 --> 00:29:42,640
kind of the default assumption i think

00:29:42,159 --> 00:29:44,480
you know

00:29:42,640 --> 00:29:46,080
if you want to have additional logical

00:29:44,480 --> 00:29:48,559
data centers for different reasons

00:29:46,080 --> 00:29:50,240
like you want to offload some uh read

00:29:48,559 --> 00:29:50,799
heavy workloads or something people do

00:29:50,240 --> 00:29:52,799
do that

00:29:50,799 --> 00:29:55,440
but i think the the layout that you

00:29:52,799 --> 00:29:58,720
described as kind of a good default

00:29:55,440 --> 00:30:00,720
okay so looking a bit more at the

00:29:58,720 --> 00:30:03,279
at an instance of a cassandra pod or

00:30:00,720 --> 00:30:05,039
which is the same as a cassandra node

00:30:03,279 --> 00:30:06,480
uh we see a number of containers first

00:30:05,039 --> 00:30:08,720
there's this init container called

00:30:06,480 --> 00:30:09,600
server configurant and what this does is

00:30:08,720 --> 00:30:12,799
actually takes

00:30:09,600 --> 00:30:16,480
a json structure of the configuration

00:30:12,799 --> 00:30:17,840
and this matches the the config

00:30:16,480 --> 00:30:19,840
block that we see here in the cassandra

00:30:17,840 --> 00:30:22,159
dc

00:30:19,840 --> 00:30:23,360
custom resource and it actually takes

00:30:22,159 --> 00:30:27,360
that and it renders that

00:30:23,360 --> 00:30:29,440
that into a suite of configuration files

00:30:27,360 --> 00:30:31,279
uh that you would see traditionally

00:30:29,440 --> 00:30:34,559
under like etsy cassandra

00:30:31,279 --> 00:30:37,919
on on your virtual machine installations

00:30:34,559 --> 00:30:39,360
so this applies a number of defaults but

00:30:37,919 --> 00:30:40,559
for the most part this is this is what's

00:30:39,360 --> 00:30:42,799
used to actually build the configuration

00:30:40,559 --> 00:30:45,440
of the individual pod

00:30:42,799 --> 00:30:46,080
jmx credentials just sets up uh the

00:30:45,440 --> 00:30:49,200
reaper

00:30:46,080 --> 00:30:50,799
and super user jmx credentials

00:30:49,200 --> 00:30:52,000
now we get to the interesting containers

00:30:50,799 --> 00:30:53,039
so those are knit containers the actual

00:30:52,000 --> 00:30:54,159
running containers the first is

00:30:53,039 --> 00:30:55,679
cassandra so this is the actual

00:30:54,159 --> 00:30:58,080
cassandra process

00:30:55,679 --> 00:31:00,480
uh we can see has a number of ports

00:30:58,080 --> 00:31:03,679
exposed including those for prometheus

00:31:00,480 --> 00:31:06,720
uh the management api which is used by

00:31:03,679 --> 00:31:07,760
the cassandra operator for certain tasks

00:31:06,720 --> 00:31:09,679
one of the things that's interesting

00:31:07,760 --> 00:31:10,799
when we think about multiple distributed

00:31:09,679 --> 00:31:11,760
systems growing up kind of

00:31:10,799 --> 00:31:14,080
in the same time we've mentioned that

00:31:11,760 --> 00:31:16,159
cassandra's going on 10 years old

00:31:14,080 --> 00:31:18,480
or might be 10 years old oh yeah it's

00:31:16,159 --> 00:31:20,880
over man it's like 12 or 13.

00:31:18,480 --> 00:31:21,600
oh my goodness i'm so out of date here

00:31:20,880 --> 00:31:24,320
and and

00:31:21,600 --> 00:31:25,600
kubernetes uh which is what five going

00:31:24,320 --> 00:31:28,640
on six years old

00:31:25,600 --> 00:31:30,480
uh they've kind of you see a bunch of

00:31:28,640 --> 00:31:32,320
similarities between the components

00:31:30,480 --> 00:31:33,679
but where some other items have diverged

00:31:32,320 --> 00:31:35,039
so one of the things that's that's

00:31:33,679 --> 00:31:37,039
interesting about cassandra is

00:31:35,039 --> 00:31:38,240
when you go to start notes the cluster

00:31:37,039 --> 00:31:39,519
you want to start them in a particular

00:31:38,240 --> 00:31:42,240
order

00:31:39,519 --> 00:31:44,000
and with with kubernetes you can sort of

00:31:42,240 --> 00:31:47,200
do that but what we do instead is we

00:31:44,000 --> 00:31:48,720
schedule all the pods all the pods start

00:31:47,200 --> 00:31:50,159
but they're just running this management

00:31:48,720 --> 00:31:51,760
api and then once we see that all the

00:31:50,159 --> 00:31:52,480
clouds are running and they're ready to

00:31:51,760 --> 00:31:54,000
start

00:31:52,480 --> 00:31:56,720
then the cassandra operator actually

00:31:54,000 --> 00:31:58,080
reaches out over this secure interface

00:31:56,720 --> 00:32:00,000
and says hey go ahead and start your

00:31:58,080 --> 00:32:01,279
cassandra process it's just a way to

00:32:00,000 --> 00:32:04,240
orchestrate

00:32:01,279 --> 00:32:04,960
uh some of the differences between how

00:32:04,240 --> 00:32:07,519
how these

00:32:04,960 --> 00:32:08,559
two technologies have evolved and like i

00:32:07,519 --> 00:32:09,919
said they're similar but sometimes

00:32:08,559 --> 00:32:11,519
they're a little different

00:32:09,919 --> 00:32:13,679
there's that little bit of scaffolding

00:32:11,519 --> 00:32:15,600
you need because you know in

00:32:13,679 --> 00:32:16,799
in kubernetes it wants to treat every

00:32:15,600 --> 00:32:20,000
pod the same

00:32:16,799 --> 00:32:21,120
you know that's of a a given image uh in

00:32:20,000 --> 00:32:24,720
cassandra

00:32:21,120 --> 00:32:26,320
we know that you know it is a um

00:32:24,720 --> 00:32:28,000
it is a peer-to-peer architecture where

00:32:26,320 --> 00:32:28,720
there is no one node that's calling all

00:32:28,000 --> 00:32:31,200
the shots

00:32:28,720 --> 00:32:32,880
but it that doesn't mean that there

00:32:31,200 --> 00:32:34,080
isn't some sort of coordination

00:32:32,880 --> 00:32:36,399
you know there is there is that

00:32:34,080 --> 00:32:38,159
communication that nodes kind of need to

00:32:36,399 --> 00:32:38,399
negotiate with each other as they come

00:32:38,159 --> 00:32:40,720
up

00:32:38,399 --> 00:32:42,399
so that you know that's a little bit of

00:32:40,720 --> 00:32:44,080
that behind-the-scenes scaffolding that

00:32:42,399 --> 00:32:44,880
it really takes to operate cassandra

00:32:44,080 --> 00:32:47,679
effectively

00:32:44,880 --> 00:32:49,200
in kubernetes but for what it's worth

00:32:47,679 --> 00:32:51,120
based on like what i mentioned before

00:32:49,200 --> 00:32:52,399
starting up hundreds of nodes could take

00:32:51,120 --> 00:32:53,840
hours right uh

00:32:52,399 --> 00:32:55,600
yeah doing a rolling restart to take

00:32:53,840 --> 00:32:56,559
hours the nice thing about the operator

00:32:55,600 --> 00:32:58,320
here is you say hey

00:32:56,559 --> 00:32:59,679
i want you to start the cluster you can

00:32:58,320 --> 00:33:01,200
go get a coffee right you don't have to

00:32:59,679 --> 00:33:03,279
check to see if like certain hosts were

00:33:01,200 --> 00:33:04,799
skipped because something bad happened

00:33:03,279 --> 00:33:07,120
yeah that's right you can just come back

00:33:04,799 --> 00:33:08,799
and and your clusters either running or

00:33:07,120 --> 00:33:10,240
it's reconciling and working to make

00:33:08,799 --> 00:33:12,080
your cluster run

00:33:10,240 --> 00:33:14,799
and like i said if you have a local

00:33:12,080 --> 00:33:16,399
registry the start time per pod is a

00:33:14,799 --> 00:33:19,360
couple of minutes

00:33:16,399 --> 00:33:21,120
and it's it's really refreshing to have

00:33:19,360 --> 00:33:21,679
something else handle all that work for

00:33:21,120 --> 00:33:23,760
you

00:33:21,679 --> 00:33:25,519
instead of instead of having to babysit

00:33:23,760 --> 00:33:28,080
an ssh terminal

00:33:25,519 --> 00:33:29,600
that's right so looking back here though

00:33:28,080 --> 00:33:31,279
we have another container here called

00:33:29,600 --> 00:33:33,120
server system logger

00:33:31,279 --> 00:33:34,320
and this is just uh we mentioned that

00:33:33,120 --> 00:33:35,519
the management api we wanted to make

00:33:34,320 --> 00:33:37,440
sure that there were two log streams so

00:33:35,519 --> 00:33:39,440
you could still use cube ctl logs

00:33:37,440 --> 00:33:42,080
the server system logger is actually the

00:33:39,440 --> 00:33:44,240
output of the main cassandra process

00:33:42,080 --> 00:33:46,480
and the output of the cassandra

00:33:44,240 --> 00:33:47,519
container is the management api process

00:33:46,480 --> 00:33:49,600
so you can

00:33:47,519 --> 00:33:51,519
look at the separate log streams

00:33:49,600 --> 00:33:53,600
individually good number of volumes we

00:33:51,519 --> 00:33:56,159
really don't need to go into that but

00:33:53,600 --> 00:33:57,440
we can see we have a running node we

00:33:56,159 --> 00:33:58,880
have a running stargate everything's

00:33:57,440 --> 00:34:00,799
green across the board so let's take a

00:33:58,880 --> 00:34:02,799
look at how we actually talk to this

00:34:00,799 --> 00:34:04,080
i think this is pretty interesting so if

00:34:02,799 --> 00:34:05,840
we go over here to the network

00:34:04,080 --> 00:34:07,200
and we look at services well there's a

00:34:05,840 --> 00:34:08,399
whole suite of services whether you're

00:34:07,200 --> 00:34:09,839
looking at the

00:34:08,399 --> 00:34:12,240
the monitoring interface which we will

00:34:09,839 --> 00:34:15,520
take a look at here in a second or uh

00:34:12,240 --> 00:34:17,119
the the uh reaper ui

00:34:15,520 --> 00:34:18,639
i do want to call out that there are a

00:34:17,119 --> 00:34:20,960
number of of

00:34:18,639 --> 00:34:22,560
headless services that represent your

00:34:20,960 --> 00:34:24,480
cassandra cluster so

00:34:22,560 --> 00:34:25,919
your application would point at maybe

00:34:24,480 --> 00:34:28,159
the dc1 service

00:34:25,919 --> 00:34:29,200
where your monitoring might point at the

00:34:28,159 --> 00:34:30,560
all pod service

00:34:29,200 --> 00:34:33,040
because that one shows pods that aren't

00:34:30,560 --> 00:34:34,560
up yet but there are a number of

00:34:33,040 --> 00:34:35,520
endpoints there's also the stargate

00:34:34,560 --> 00:34:38,399
service which is where

00:34:35,520 --> 00:34:39,520
i would point my applications and now

00:34:38,399 --> 00:34:42,159
finally

00:34:39,520 --> 00:34:43,599
we want to get into the actual um secret

00:34:42,159 --> 00:34:47,760
so this is how we actually

00:34:43,599 --> 00:34:47,760
we communicate with the

00:34:47,839 --> 00:34:54,399
the cluster so let's take a look at the

00:34:51,040 --> 00:34:57,680
super user so kate sander has created

00:34:54,399 --> 00:34:58,720
our our cluster it's also created our

00:34:57,680 --> 00:35:00,560
user account

00:34:58,720 --> 00:35:02,640
so here the name username is kate sander

00:35:00,560 --> 00:35:03,280
dash superuser and this is just a random

00:35:02,640 --> 00:35:05,440
string

00:35:03,280 --> 00:35:07,599
you can create the secret yourself and

00:35:05,440 --> 00:35:09,280
use predefined credentials

00:35:07,599 --> 00:35:10,480
um but just for the sake of the demo

00:35:09,280 --> 00:35:12,560
here we went with the default which is

00:35:10,480 --> 00:35:15,280
just they make it for me

00:35:12,560 --> 00:35:16,000
so let's go and kind of explore the

00:35:15,280 --> 00:35:19,520
interfaces a little

00:35:16,000 --> 00:35:22,880
bit uh i'm going to go back over here so

00:35:19,520 --> 00:35:23,839
i want to connect first to our

00:35:22,880 --> 00:35:27,119
monitoring system

00:35:23,839 --> 00:35:28,400
so we go to services and if we go to kit

00:35:27,119 --> 00:35:31,359
center grafana

00:35:28,400 --> 00:35:32,640
i should just be able to click this and

00:35:31,359 --> 00:35:35,760
there we go

00:35:32,640 --> 00:35:37,839
and i'm now taken to our did you port

00:35:35,760 --> 00:35:39,440
forward that to expose how did you get

00:35:37,839 --> 00:35:41,920
that access so quickly

00:35:39,440 --> 00:35:43,520
oh lens is awesome so the kubernetes

00:35:41,920 --> 00:35:44,000
lens tool if you click on the port will

00:35:43,520 --> 00:35:46,800
set up the

00:35:44,000 --> 00:35:49,040
recording for you awesome if i'm not

00:35:46,800 --> 00:35:52,320
mistaken it's admin secret

00:35:49,040 --> 00:35:55,359
um no no go away all these plugins are

00:35:52,320 --> 00:35:56,880
now like we want to save your password

00:35:55,359 --> 00:35:58,880
so you're going to see this says that

00:35:56,880 --> 00:36:00,720
there's nothing here it's misleading

00:35:58,880 --> 00:36:03,040
um the dashboards are already installed

00:36:00,720 --> 00:36:05,200
here we go and we can see we have

00:36:03,040 --> 00:36:06,640
four pre-installed for you if we go to

00:36:05,200 --> 00:36:08,480
cassandra overview

00:36:06,640 --> 00:36:10,560
um our cluster is up and running we can

00:36:08,480 --> 00:36:12,079
see the status of our node

00:36:10,560 --> 00:36:13,920
we'll run some queries against it here

00:36:12,079 --> 00:36:15,760
in a second but we

00:36:13,920 --> 00:36:17,280
again you didn't have i didn't have to

00:36:15,760 --> 00:36:19,280
do anything here to set this

00:36:17,280 --> 00:36:20,480
up uh all the automation spun this up

00:36:19,280 --> 00:36:21,920
for me the

00:36:20,480 --> 00:36:23,839
really interesting thing is if i change

00:36:21,920 --> 00:36:24,400
the number of nodes say from one node to

00:36:23,839 --> 00:36:26,000
three

00:36:24,400 --> 00:36:27,440
they'll automatically get added to the

00:36:26,000 --> 00:36:28,720
monitoring system for us and

00:36:27,440 --> 00:36:29,680
that's not that's like another item in

00:36:28,720 --> 00:36:31,280
the checklist that it didn't

00:36:29,680 --> 00:36:34,000
accidentally do again

00:36:31,280 --> 00:36:35,040
uh right this is just handled for you uh

00:36:34,000 --> 00:36:38,480
which is

00:36:35,040 --> 00:36:40,160
uh refreshing and so

00:36:38,480 --> 00:36:41,680
besides just cassandra we also have a

00:36:40,160 --> 00:36:44,960
stargate a separate stargate

00:36:41,680 --> 00:36:47,520
um dashboard a condensed dashboard

00:36:44,960 --> 00:36:48,160
and if your application is using uh

00:36:47,520 --> 00:36:50,000
metrics

00:36:48,160 --> 00:36:51,760
or if you're using centralized logging

00:36:50,000 --> 00:36:55,200
you can pull this into your own

00:36:51,760 --> 00:36:55,839
uh structure so that's the monitoring

00:36:55,200 --> 00:36:58,880
interface

00:36:55,839 --> 00:37:00,720
i do want to just mention that uh

00:36:58,880 --> 00:37:01,839
prometheus one of the one of the health

00:37:00,720 --> 00:37:03,200
checks that i like to do to make sure

00:37:01,839 --> 00:37:04,640
that my cluster's healthy

00:37:03,200 --> 00:37:06,839
is i'll go and i'll look at the targets

00:37:04,640 --> 00:37:08,400
and make sure that a that they exist

00:37:06,839 --> 00:37:09,920
here and b

00:37:08,400 --> 00:37:11,599
that we're seeing all the labels come in

00:37:09,920 --> 00:37:13,520
see again you can see

00:37:11,599 --> 00:37:14,880
uh the prometheus is scraping the

00:37:13,520 --> 00:37:18,320
metrics uh

00:37:14,880 --> 00:37:23,839
from both my stargate node and my

00:37:18,320 --> 00:37:25,440
uh my cassandra net that's a great tip

00:37:23,839 --> 00:37:26,880
yeah if something isn't showing up in

00:37:25,440 --> 00:37:27,440
here i always go look at prometheus

00:37:26,880 --> 00:37:30,400
first

00:37:27,440 --> 00:37:31,440
uh sure i've gotten into plenty of edge

00:37:30,400 --> 00:37:34,000
cases where things just have

00:37:31,440 --> 00:37:36,640
kind of gone sideways but let's talk

00:37:34,000 --> 00:37:38,560
about let's see where are we on time

00:37:36,640 --> 00:37:39,920
i want to make sure i don't well we got

00:37:38,560 --> 00:37:42,560
about 10 minutes

00:37:39,920 --> 00:37:43,920
left for the main the main piece um i

00:37:42,560 --> 00:37:46,240
wanted to interject a quick

00:37:43,920 --> 00:37:48,160
question about where you're running this

00:37:46,240 --> 00:37:49,839
as you kind of transition here

00:37:48,160 --> 00:37:52,640
is this on your desktop are you running

00:37:49,839 --> 00:37:55,680
in a cloud does it matter

00:37:52,640 --> 00:37:58,079
uh it doesn't matter we test and run

00:37:55,680 --> 00:38:01,119
on a number of kubernetes versions uh in

00:37:58,079 --> 00:38:03,599
my case i'm running on k3d

00:38:01,119 --> 00:38:05,680
on my local laptop so i'm running a

00:38:03,599 --> 00:38:08,400
single

00:38:05,680 --> 00:38:09,680
a single node with with a load balancer

00:38:08,400 --> 00:38:11,760
as part of k3d

00:38:09,680 --> 00:38:13,920
we test and kind with our integration

00:38:11,760 --> 00:38:15,359
tests and as a part of

00:38:13,920 --> 00:38:16,800
the current build out we're building out

00:38:15,359 --> 00:38:17,520
a whole suite of integration tests that

00:38:16,800 --> 00:38:21,040
target

00:38:17,520 --> 00:38:22,800
the major cloud kubernetes vendors

00:38:21,040 --> 00:38:25,280
um but you if we go to the project you

00:38:22,800 --> 00:38:26,720
can see uh yeah i supported a number of

00:38:25,280 --> 00:38:28,400
kubernetes versions and tests against

00:38:26,720 --> 00:38:30,320
those this is where i think the

00:38:28,400 --> 00:38:33,839
composability of this is really

00:38:30,320 --> 00:38:37,280
helpful because i i also mostly run

00:38:33,839 --> 00:38:38,240
kate sandra on k3d on my desktop and so

00:38:37,280 --> 00:38:40,240
that's kind of like

00:38:38,240 --> 00:38:42,320
why you have that configuration where

00:38:40,240 --> 00:38:43,920
the medusa is turned off because

00:38:42,320 --> 00:38:45,839
maybe if you're just doing dev work

00:38:43,920 --> 00:38:48,160
locally you don't need backup and

00:38:45,839 --> 00:38:51,520
restore capability okay so just don't

00:38:48,160 --> 00:38:53,040
you know turn that part off yeah and i

00:38:51,520 --> 00:38:54,400
was i was doing some work just the other

00:38:53,040 --> 00:38:56,000
day where i didn't i didn't

00:38:54,400 --> 00:38:58,400
really need repairs locally either so i

00:38:56,000 --> 00:39:00,079
turned that piece off just to slim down

00:38:58,400 --> 00:39:01,839
the amount of resources that were needed

00:39:00,079 --> 00:39:04,160
just for my development right

00:39:01,839 --> 00:39:05,520
and alex st janovsky has a great post on

00:39:04,160 --> 00:39:08,960
the kate sandra i o

00:39:05,520 --> 00:39:10,960
site about like what is that uh minimum

00:39:08,960 --> 00:39:12,560
development configuration because we

00:39:10,960 --> 00:39:14,960
know a lot of people like to run

00:39:12,560 --> 00:39:15,920
you know just the essentials locally so

00:39:14,960 --> 00:39:18,640
definitely look for that

00:39:15,920 --> 00:39:21,839
that post on the on the blog on kate

00:39:18,640 --> 00:39:23,359
sander.io for that minimum config

00:39:21,839 --> 00:39:25,200
there's this fascinating conversation

00:39:23,359 --> 00:39:27,359
around do we go with

00:39:25,200 --> 00:39:28,560
a single single node or do i run

00:39:27,359 --> 00:39:29,760
multiple nodes in my development

00:39:28,560 --> 00:39:31,839
environment and that way you can

00:39:29,760 --> 00:39:33,520
simulate what happens when

00:39:31,839 --> 00:39:35,359
two pods go down and maybe you have a

00:39:33,520 --> 00:39:38,320
reduced consistency level

00:39:35,359 --> 00:39:39,839
yeah what's your recommendation

00:39:38,320 --> 00:39:40,640
personally i like to just keep things

00:39:39,839 --> 00:39:44,800
really

00:39:40,640 --> 00:39:46,880
uh minimal on my local machine so as i

00:39:44,800 --> 00:39:48,880
wrap up features or as i do my testing

00:39:46,880 --> 00:39:49,520
i'll run against a cluster with multiple

00:39:48,880 --> 00:39:51,440
nodes

00:39:49,520 --> 00:39:52,720
because i do like to test those failure

00:39:51,440 --> 00:39:56,000
scenarios

00:39:52,720 --> 00:39:57,839
but i also am cognizant of depending on

00:39:56,000 --> 00:40:00,800
the size of the data maybe a single node

00:39:57,839 --> 00:40:01,440
is creation um this this is kind of

00:40:00,800 --> 00:40:02,640
related to

00:40:01,440 --> 00:40:05,040
i don't want to derail you too much

00:40:02,640 --> 00:40:06,640
sorry chris but uh there's a there is a

00:40:05,040 --> 00:40:08,079
pertinent question from eric rodriguez

00:40:06,640 --> 00:40:10,319
here what are the hardware

00:40:08,079 --> 00:40:12,000
and storage type considerations if any

00:40:10,319 --> 00:40:13,520
for running cate sandra

00:40:12,000 --> 00:40:15,200
in other words like what instance type

00:40:13,520 --> 00:40:16,400
and backing storage do you want to use

00:40:15,200 --> 00:40:17,680
in your various public clouds i'm

00:40:16,400 --> 00:40:20,400
paraphrasing the end of the question

00:40:17,680 --> 00:40:23,760
there to broaden it a little bit

00:40:20,400 --> 00:40:26,319
sure yeah so it's it's interesting

00:40:23,760 --> 00:40:28,079
we just use a storage class uh to to

00:40:26,319 --> 00:40:29,359
provision our storage so here i'm using

00:40:28,079 --> 00:40:30,720
the local path because well that's what

00:40:29,359 --> 00:40:32,880
comes with with rancher

00:40:30,720 --> 00:40:34,000
or with k3d it's from rancher and it's

00:40:32,880 --> 00:40:37,280
pretty easy to use

00:40:34,000 --> 00:40:39,119
but if i was going to deploy into gke i

00:40:37,280 --> 00:40:41,920
may use something like a pd ssd

00:40:39,119 --> 00:40:43,760
if i'm if i'm looking at eks i'd be

00:40:41,920 --> 00:40:45,119
looking at like elastic block storage

00:40:43,760 --> 00:40:46,480
um one of the nice things about

00:40:45,119 --> 00:40:49,040
cassandra is it does a lot of the

00:40:46,480 --> 00:40:50,880
shuffling of the data for you so you can

00:40:49,040 --> 00:40:53,200
for instance leverage those ephemeral

00:40:50,880 --> 00:40:56,319
volumes which are super fast

00:40:53,200 --> 00:40:57,680
uh but the downside is but yeah they're

00:40:56,319 --> 00:41:00,000
pinned to the server right

00:40:57,680 --> 00:41:01,599
and so if if a kubernetes worker goes

00:41:00,000 --> 00:41:02,960
down what happens to that data well

00:41:01,599 --> 00:41:05,839
you have no guarantee that it's coming

00:41:02,960 --> 00:41:08,240
back uh the nice thing about

00:41:05,839 --> 00:41:09,599
the the operators in case sandra is well

00:41:08,240 --> 00:41:12,720
if

00:41:09,599 --> 00:41:14,800
you're moved to a new kubernetes worker

00:41:12,720 --> 00:41:15,920
uh we actually are aware that you're

00:41:14,800 --> 00:41:18,000
using uh

00:41:15,920 --> 00:41:19,440
a volume that can move or or not if it

00:41:18,000 --> 00:41:21,440
comes up and it says oh this volume's

00:41:19,440 --> 00:41:21,680
empty well then we'll reboot strap with

00:41:21,440 --> 00:41:23,680
from

00:41:21,680 --> 00:41:24,880
off the other nodes in the cluster now

00:41:23,680 --> 00:41:26,480
let's say you're using something like

00:41:24,880 --> 00:41:28,960
ebs where the data can

00:41:26,480 --> 00:41:30,240
follow that pod to another worker well

00:41:28,960 --> 00:41:31,839
that will happen and then we won't have

00:41:30,240 --> 00:41:33,359
to do that bootstrap but that

00:41:31,839 --> 00:41:35,359
full bootstrap process will just boost

00:41:33,359 --> 00:41:37,200
up the data that we already have

00:41:35,359 --> 00:41:38,400
and rejoin the ring so it's a great

00:41:37,200 --> 00:41:39,359
question i think it's highly dependent

00:41:38,400 --> 00:41:41,200
on your use case

00:41:39,359 --> 00:41:43,200
you're super latency sensitive i'd be

00:41:41,200 --> 00:41:44,240
looking at a thermal volumes

00:41:43,200 --> 00:41:46,640
if you have a little bit more

00:41:44,240 --> 00:41:49,359
flexibility and want the

00:41:46,640 --> 00:41:50,640
i would call a more available story it's

00:41:49,359 --> 00:41:52,240
misleading we're not

00:41:50,640 --> 00:41:54,000
it's not that we're not highly available

00:41:52,240 --> 00:41:55,920
using local volumes it's just

00:41:54,000 --> 00:41:58,079
the rescheduling and spinning back up

00:41:55,920 --> 00:42:00,240
where we move a volume is a lot faster

00:41:58,079 --> 00:42:01,280
than rehydrating like an entire terabyte

00:42:00,240 --> 00:42:04,400
of data

00:42:01,280 --> 00:42:06,480
off of existing replicas and uh to peel

00:42:04,400 --> 00:42:07,119
back the covers just a little bit we are

00:42:06,480 --> 00:42:09,680
working

00:42:07,119 --> 00:42:11,440
on more extensive documentation and

00:42:09,680 --> 00:42:16,240
recommended deployment options

00:42:11,440 --> 00:42:16,240
per cloud aws gke

00:42:16,720 --> 00:42:20,480
right so there will be more coming in

00:42:18,720 --> 00:42:22,319
that area with even more specific

00:42:20,480 --> 00:42:24,400
guidance

00:42:22,319 --> 00:42:25,440
definitely definitely and and i expect

00:42:24,400 --> 00:42:27,119
to see a number of blog posts in that

00:42:25,440 --> 00:42:28,560
area too so like like we saw before with

00:42:27,119 --> 00:42:29,920
the cassandra data center

00:42:28,560 --> 00:42:32,079
um from a storage perspective

00:42:29,920 --> 00:42:33,760
perspective you just specify

00:42:32,079 --> 00:42:35,200
the storage class and then the size of

00:42:33,760 --> 00:42:37,359
the volume uh

00:42:35,200 --> 00:42:39,280
it's right here so we're asking for read

00:42:37,359 --> 00:42:40,880
write wants the local path provision

00:42:39,280 --> 00:42:42,560
so if you're on a cloud provider they

00:42:40,880 --> 00:42:44,319
may already have a source class name or

00:42:42,560 --> 00:42:46,960
you can create a storage class

00:42:44,319 --> 00:42:48,000
with the name of their provisioner again

00:42:46,960 --> 00:42:50,800
it's highly dependent on the cloud that

00:42:48,000 --> 00:42:50,800
you're using or if you're on

00:42:51,040 --> 00:42:54,800
okay so i'm going to take a look real

00:42:52,800 --> 00:42:57,200
quick i'd like to show

00:42:54,800 --> 00:42:59,440
we've talked a little bit about stargate

00:42:57,200 --> 00:43:00,800
it would i think we should

00:42:59,440 --> 00:43:02,079
see if we have a second to just show

00:43:00,800 --> 00:43:03,839
what that looks like so yeah it's going

00:43:02,079 --> 00:43:07,119
to report forward stargate

00:43:03,839 --> 00:43:10,640
um using cube ctl go over here

00:43:07,119 --> 00:43:12,240
all the api is exposed yeah yeah well

00:43:10,640 --> 00:43:14,640
just a single command would be nice

00:43:12,240 --> 00:43:17,599
right and so we're going to copy

00:43:14,640 --> 00:43:18,319
our password for the case standard super

00:43:17,599 --> 00:43:21,280
user

00:43:18,319 --> 00:43:23,200
so we're just going to look at that copy

00:43:21,280 --> 00:43:24,480
if i go back here to postman

00:43:23,200 --> 00:43:26,319
um the first thing that we do is we

00:43:24,480 --> 00:43:29,920
authenticate with a

00:43:26,319 --> 00:43:32,319
uh to retrieve a token

00:43:29,920 --> 00:43:34,160
make this just a little bit bigger there

00:43:32,319 --> 00:43:35,520
we go so we send this

00:43:34,160 --> 00:43:37,920
and it comes back it gave us an off

00:43:35,520 --> 00:43:39,200
token i've taken the liberty of stashing

00:43:37,920 --> 00:43:41,839
it here inside of an environment

00:43:39,200 --> 00:43:43,520
variable so it's easy to use

00:43:41,839 --> 00:43:45,200
but from here so i'm using the document

00:43:43,520 --> 00:43:46,400
api so the document api has a flexible

00:43:45,200 --> 00:43:48,640
schema you can just kind of send it

00:43:46,400 --> 00:43:50,240
whatever json you want and it'll work

00:43:48,640 --> 00:43:52,880
so here we can see these are all the

00:43:50,240 --> 00:43:54,400
name spaces that exist

00:43:52,880 --> 00:43:56,400
but i don't have one for my application

00:43:54,400 --> 00:43:58,079
yet so we'll go ahead and create one

00:43:56,400 --> 00:44:00,880
um so here we're going to say create a

00:43:58,079 --> 00:44:02,800
new namespace named to my app

00:44:00,880 --> 00:44:04,800
it created the namespace and then if i

00:44:02,800 --> 00:44:05,200
go back and i read that name space we'll

00:44:04,800 --> 00:44:07,920
see

00:44:05,200 --> 00:44:08,720
it exists the next thing that i'd like

00:44:07,920 --> 00:44:11,200
to do is

00:44:08,720 --> 00:44:13,280
is create in this example i'm talking

00:44:11,200 --> 00:44:15,599
about surveys let's create a survey

00:44:13,280 --> 00:44:17,920
this will say oh there's no survey table

00:44:15,599 --> 00:44:18,880
you can see the url it's namespaces my

00:44:17,920 --> 00:44:21,440
namespace name

00:44:18,880 --> 00:44:22,079
my collection here i'm going to create a

00:44:21,440 --> 00:44:23,599
survey

00:44:22,079 --> 00:44:26,480
and in this case it's just a title and

00:44:23,599 --> 00:44:28,480
an array of questions that's empty

00:44:26,480 --> 00:44:29,520
and this comes back with the document id

00:44:28,480 --> 00:44:30,480
this one takes a little bit longer

00:44:29,520 --> 00:44:31,760
because it's actually creating the

00:44:30,480 --> 00:44:33,760
tables in the background it said it

00:44:31,760 --> 00:44:35,680
didn't have tables before now it does

00:44:33,760 --> 00:44:36,880
and so here we can see the document id

00:44:35,680 --> 00:44:39,520
if we go back and list

00:44:36,880 --> 00:44:40,400
out we can see this is the first

00:44:39,520 --> 00:44:43,119
document

00:44:40,400 --> 00:44:45,119
i've submitted same thing happens if i

00:44:43,119 --> 00:44:48,880
want to create a new

00:44:45,119 --> 00:44:51,200
a new collection in this case questions

00:44:48,880 --> 00:44:52,480
i'm submitting uh what is your favorite

00:44:51,200 --> 00:44:53,839
color and i've i just

00:44:52,480 --> 00:44:56,000
i'm using environment variables here to

00:44:53,839 --> 00:44:58,560
keep the ids in place so now i have

00:44:56,000 --> 00:45:00,640
a new document id i can actually update

00:44:58,560 --> 00:45:04,160
my existing

00:45:00,640 --> 00:45:05,680
uh document with that new id you might

00:45:04,160 --> 00:45:07,200
be saying chris this doesn't

00:45:05,680 --> 00:45:08,720
why why don't you just have like nested

00:45:07,200 --> 00:45:09,520
documents inside of here that's a fair

00:45:08,720 --> 00:45:11,200
question

00:45:09,520 --> 00:45:12,800
uh one of the reasons i'm i'm talking

00:45:11,200 --> 00:45:14,480
about it in this kind of context is that

00:45:12,800 --> 00:45:15,119
i like to play around with my data model

00:45:14,480 --> 00:45:17,040
here

00:45:15,119 --> 00:45:19,359
with the document api and then start to

00:45:17,040 --> 00:45:22,640
turn that into more concrete

00:45:19,359 --> 00:45:24,720
uh tables that i would use with the

00:45:22,640 --> 00:45:26,319
regular rest api or even with cql

00:45:24,720 --> 00:45:28,240
and you know for those that might be

00:45:26,319 --> 00:45:30,880
less familiar with cassandra

00:45:28,240 --> 00:45:31,680
this is or even those that are right not

00:45:30,880 --> 00:45:33,520
what

00:45:31,680 --> 00:45:34,800
cassandra interactions have looked like

00:45:33,520 --> 00:45:37,920
historically

00:45:34,800 --> 00:45:39,440
um it's not a

00:45:37,920 --> 00:45:41,599
i'm just going to experiment and play

00:45:39,440 --> 00:45:43,040
around with a data model and try some

00:45:41,599 --> 00:45:44,720
different document formats

00:45:43,040 --> 00:45:47,599
it's not how things have worked historic

00:45:44,720 --> 00:45:49,359
so this uh this document api that that

00:45:47,599 --> 00:45:50,400
stargate provides is a bit of a game

00:45:49,359 --> 00:45:52,720
changer that really

00:45:50,400 --> 00:45:54,400
kind of changes the developer experience

00:45:52,720 --> 00:45:58,319
for working with cassandra

00:45:54,400 --> 00:46:00,640
so like what you just saw is

00:45:58,319 --> 00:46:01,599
something that five years ago would have

00:46:00,640 --> 00:46:05,920
been

00:46:01,599 --> 00:46:05,920
like you did not just what

00:46:06,079 --> 00:46:08,800
yeah i like that i can just start adding

00:46:07,440 --> 00:46:10,800
fields willy nilly and i having to like

00:46:08,800 --> 00:46:12,160
go into cql and say alter table and add

00:46:10,800 --> 00:46:13,920
my columns and think well do i need to

00:46:12,160 --> 00:46:14,640
add this as part of the key and all that

00:46:13,920 --> 00:46:17,359
stuff right

00:46:14,640 --> 00:46:18,720
um so i think this is this is a great

00:46:17,359 --> 00:46:19,839
point i'd love to go into a bit more but

00:46:18,720 --> 00:46:21,280
i think this is a really good time to

00:46:19,839 --> 00:46:24,480
transition into talking about

00:46:21,280 --> 00:46:25,359
questions that have come in oh yeah you

00:46:24,480 --> 00:46:27,280
should

00:46:25,359 --> 00:46:28,720
um let me just tackle this because

00:46:27,280 --> 00:46:31,040
thinking about it i

00:46:28,720 --> 00:46:32,240
started writing an answer to it uh and

00:46:31,040 --> 00:46:36,400
then i was like

00:46:32,240 --> 00:46:39,200
wow this is too long to type so um

00:46:36,400 --> 00:46:41,280
sharabesh is asking so our enterprise is

00:46:39,200 --> 00:46:43,520
using oracle

00:46:41,280 --> 00:46:45,440
and we're ready to move most of our apps

00:46:43,520 --> 00:46:47,440
to kubernetes clusters

00:46:45,440 --> 00:46:49,200
but how do we present cassandra is a

00:46:47,440 --> 00:46:52,240
good option for cloud native apps

00:46:49,200 --> 00:46:54,000
so i this there's multiple levels on

00:46:52,240 --> 00:46:57,200
which to answer this question

00:46:54,000 --> 00:47:00,960
um so let's think about it in terms of

00:46:57,200 --> 00:47:02,480
developer experience so one is what is

00:47:00,960 --> 00:47:03,520
the skill sets that your application

00:47:02,480 --> 00:47:07,440
developers have

00:47:03,520 --> 00:47:10,720
versus what do you want them to have um

00:47:07,440 --> 00:47:12,880
if they are prepared to learn cql

00:47:10,720 --> 00:47:13,920
i'm great in a lot of cases that might

00:47:12,880 --> 00:47:15,280
not be

00:47:13,920 --> 00:47:17,599
what they want to spend their time doing

00:47:15,280 --> 00:47:19,200
it all depends on the needs right so

00:47:17,599 --> 00:47:21,839
um having something where you have

00:47:19,200 --> 00:47:22,640
flexible apis like provided by stargate

00:47:21,839 --> 00:47:26,000
that's a good

00:47:22,640 --> 00:47:27,599
developer experience story another

00:47:26,000 --> 00:47:29,200
story you might want to look at is the

00:47:27,599 --> 00:47:32,559
cost story

00:47:29,200 --> 00:47:34,960
so the cost of operating this system

00:47:32,559 --> 00:47:36,160
are you able to tune and provision the

00:47:34,960 --> 00:47:38,400
resources to

00:47:36,160 --> 00:47:40,559
exactly what you need not only for peak

00:47:38,400 --> 00:47:41,359
but can you also have an elastic system

00:47:40,559 --> 00:47:45,680
that

00:47:41,359 --> 00:47:46,720
scales back down and

00:47:45,680 --> 00:47:48,800
you know so that there's those

00:47:46,720 --> 00:47:51,119
considerations as well there's also

00:47:48,800 --> 00:47:54,480
operational cost of how much labor

00:47:51,119 --> 00:47:55,839
do you have to invest there's a lot of

00:47:54,480 --> 00:47:57,760
maturation in

00:47:55,839 --> 00:47:59,359
in this kubernetes world for a lot of

00:47:57,760 --> 00:48:01,520
different databases there are a lot of

00:47:59,359 --> 00:48:03,200
different operators out there

00:48:01,520 --> 00:48:05,119
especially on the relational side it's

00:48:03,200 --> 00:48:07,280
almost like a wealth of choices and

00:48:05,119 --> 00:48:08,880
you have the interesting problem of

00:48:07,280 --> 00:48:10,800
having to evaluate multiple different

00:48:08,880 --> 00:48:12,880
operators and pick which one

00:48:10,800 --> 00:48:13,920
you're going to use in the cassandra

00:48:12,880 --> 00:48:16,400
world right now

00:48:13,920 --> 00:48:16,960
there's a smaller set of those but we've

00:48:16,400 --> 00:48:18,640
been

00:48:16,960 --> 00:48:20,559
working with this cast operator project

00:48:18,640 --> 00:48:22,880
to kind of take the best of all

00:48:20,559 --> 00:48:24,160
of the the existing cassandra operators

00:48:22,880 --> 00:48:27,119
and kind of bake that into

00:48:24,160 --> 00:48:28,319
kind of one operator that the community

00:48:27,119 --> 00:48:30,480
is really rallying

00:48:28,319 --> 00:48:31,839
around so those are the two main ways i

00:48:30,480 --> 00:48:33,359
would look at it is from the

00:48:31,839 --> 00:48:35,040
the developer skill side and then the

00:48:33,359 --> 00:48:36,000
operational cost side i hope that's

00:48:35,040 --> 00:48:37,520
helpful

00:48:36,000 --> 00:48:39,520
uh and then also i would just say you

00:48:37,520 --> 00:48:40,480
need to you know experiment and try this

00:48:39,520 --> 00:48:43,839
out for yourself

00:48:40,480 --> 00:48:45,839
and give us the feedback uh you probably

00:48:43,839 --> 00:48:48,559
heard that we

00:48:45,839 --> 00:48:50,640
don't have everything answered yet this

00:48:48,559 --> 00:48:52,480
is very much like an emerging area that

00:48:50,640 --> 00:48:54,240
we're putting a lot of work into

00:48:52,480 --> 00:48:56,319
and a lot of collaboration and a lot of

00:48:54,240 --> 00:48:58,400
input is required so we

00:48:56,319 --> 00:48:59,839
uh we love getting the feedback about

00:48:58,400 --> 00:49:01,520
what works and what doesn't

00:48:59,839 --> 00:49:02,559
and the questions that you have the

00:49:01,520 --> 00:49:04,720
questions that you're asking are

00:49:02,559 --> 00:49:07,760
honestly gold

00:49:04,720 --> 00:49:08,880
you want to tackle one chris yeah so uh

00:49:07,760 --> 00:49:10,400
edward has a question

00:49:08,880 --> 00:49:12,559
all the standard features are free to

00:49:10,400 --> 00:49:14,480
use or the full version is paid

00:49:12,559 --> 00:49:16,000
this is an open source project you can

00:49:14,480 --> 00:49:18,079
download it you can run it there are no

00:49:16,000 --> 00:49:19,680
licenses involved but there's the apache

00:49:18,079 --> 00:49:21,520
software license but that's

00:49:19,680 --> 00:49:23,119
that's what the code is distributed on

00:49:21,520 --> 00:49:25,440
it uh so

00:49:23,119 --> 00:49:27,040
there's there's no issues there if

00:49:25,440 --> 00:49:27,520
you're looking for support there are

00:49:27,040 --> 00:49:28,960
some

00:49:27,520 --> 00:49:31,359
there's self-service support through the

00:49:28,960 --> 00:49:34,720
repos there's also paid support through

00:49:31,359 --> 00:49:37,599
data taxes luna offering um but uh

00:49:34,720 --> 00:49:39,440
there's no uh there's nothing stopping

00:49:37,599 --> 00:49:42,000
you from running this today

00:49:39,440 --> 00:49:42,960
yeah do you want to take this next one

00:49:42,000 --> 00:49:45,040
to you about if

00:49:42,960 --> 00:49:47,040
you already have a cluster running with

00:49:45,040 --> 00:49:49,200
cast operator

00:49:47,040 --> 00:49:50,400
how do you just integrate the rest of

00:49:49,200 --> 00:49:52,079
the key standard elements

00:49:50,400 --> 00:49:54,079
kind of on top of that in place you

00:49:52,079 --> 00:49:55,599
could do that right

00:49:54,079 --> 00:49:58,880
so we're putting together the

00:49:55,599 --> 00:50:00,960
documentation on this it's

00:49:58,880 --> 00:50:02,160
there are some assumptions that are made

00:50:00,960 --> 00:50:05,839
with that kate sander

00:50:02,160 --> 00:50:08,960
it plugs into the the uh existing

00:50:05,839 --> 00:50:10,240
cast dc or into it the cast dc i want to

00:50:08,960 --> 00:50:11,599
make sure i understand the full

00:50:10,240 --> 00:50:12,720
implications of what that would do to

00:50:11,599 --> 00:50:15,680
existing cast dc

00:50:12,720 --> 00:50:17,359
the seamless way that i would pursue is

00:50:15,680 --> 00:50:20,400
i'd set up a new

00:50:17,359 --> 00:50:22,240
data center with case sandra and have it

00:50:20,400 --> 00:50:23,760
connect to your existing data center

00:50:22,240 --> 00:50:25,920
that's being managed purely by cast

00:50:23,760 --> 00:50:27,200
operator then you can have it migrate

00:50:25,920 --> 00:50:28,960
the data into it and spin down the old

00:50:27,200 --> 00:50:32,000
dc after you perform the migration

00:50:28,960 --> 00:50:34,160
um it's it's it's zero downtime

00:50:32,000 --> 00:50:35,599
uh it's it's pretty painless we have

00:50:34,160 --> 00:50:36,319
some documentation coming out about this

00:50:35,599 --> 00:50:39,359
here

00:50:36,319 --> 00:50:40,800
uh in the coming weeks uh but dress is

00:50:39,359 --> 00:50:44,240
true we have been looking into that

00:50:40,800 --> 00:50:46,559
uh we would love the specific details of

00:50:44,240 --> 00:50:49,760
this use case so please reach out

00:50:46,559 --> 00:50:52,640
to us uh the community page

00:50:49,760 --> 00:50:53,119
uh on the ktm on the case android io

00:50:52,640 --> 00:50:54,880
site

00:50:53,119 --> 00:50:56,720
lists out nice ways that you can get in

00:50:54,880 --> 00:51:00,480
contact with us probably the best way

00:50:56,720 --> 00:51:03,520
right now is our mailing list but

00:51:00,480 --> 00:51:05,680
of course filing it uh on the on the

00:51:03,520 --> 00:51:07,280
github repo is also fine

00:51:05,680 --> 00:51:09,040
maybe not if you just have a question

00:51:07,280 --> 00:51:11,200
like if you have a specific issue

00:51:09,040 --> 00:51:12,079
on an issue and then we are working

00:51:11,200 --> 00:51:15,200
toward launching

00:51:12,079 --> 00:51:18,559
forums so we can have a more responsive

00:51:15,200 --> 00:51:22,640
uh turn around on your quest to see the

00:51:18,559 --> 00:51:24,480
answers to that's coming live very soon

00:51:22,640 --> 00:51:25,760
yeah so there's another question of how

00:51:24,480 --> 00:51:27,119
can i add these dashboards to an

00:51:25,760 --> 00:51:29,359
existing grafana

00:51:27,119 --> 00:51:30,720
so with kate sandra we have a flag that

00:51:29,359 --> 00:51:32,160
if you don't want to deploy the full

00:51:30,720 --> 00:51:33,839
cube prometheus stack you just want us

00:51:32,160 --> 00:51:35,520
to provision the dashboards

00:51:33,839 --> 00:51:36,720
we'll create those in a config map and

00:51:35,520 --> 00:51:39,040
then you can wire those into your

00:51:36,720 --> 00:51:41,680
existing griffon installation

00:51:39,040 --> 00:51:43,359
should you have a type of installation

00:51:41,680 --> 00:51:45,760
that doesn't support that kind of

00:51:43,359 --> 00:51:46,720
deployment of dashboards we have links

00:51:45,760 --> 00:51:48,400
to those

00:51:46,720 --> 00:51:49,599
in the project and they're coming from

00:51:48,400 --> 00:51:50,079
the metrics collector for apache

00:51:49,599 --> 00:51:53,200
cassandra

00:51:50,079 --> 00:51:53,680
project that uh so that that's also

00:51:53,200 --> 00:51:56,800
available

00:51:53,680 --> 00:51:58,400
so if if you can't use the config maps

00:51:56,800 --> 00:51:59,839
we also just have the json files and

00:51:58,400 --> 00:52:02,800
urls that you can put in because

00:51:59,839 --> 00:52:04,720
i'm fairly certain the grafana pod

00:52:02,800 --> 00:52:05,359
supports downloading dashboards from url

00:52:04,720 --> 00:52:06,640
as well

00:52:05,359 --> 00:52:08,800
so there's there's a couple ways you can

00:52:06,640 --> 00:52:11,280
bring these into your existing grafana

00:52:08,800 --> 00:52:11,280
environment

00:52:12,160 --> 00:52:16,160
there's a question here jeff uh about

00:52:14,880 --> 00:52:17,520
the performance

00:52:16,160 --> 00:52:19,599
if we're putting in additional layer of

00:52:17,520 --> 00:52:22,640
abstractions i think the the

00:52:19,599 --> 00:52:25,520
uh the person robert is referring to

00:52:22,640 --> 00:52:26,720
uh stargate between your clients and the

00:52:25,520 --> 00:52:29,040
data nodes and

00:52:26,720 --> 00:52:30,480
i know traditionally we would say hey

00:52:29,040 --> 00:52:32,240
just point your clients directly because

00:52:30,480 --> 00:52:33,520
xander cluster maybe it'll skip an extra

00:52:32,240 --> 00:52:34,880
hop or it knows directly about which

00:52:33,520 --> 00:52:38,000
notes to go to

00:52:34,880 --> 00:52:39,920
um do you have any

00:52:38,000 --> 00:52:42,240
any feedback around that the performance

00:52:39,920 --> 00:52:44,720
question

00:52:42,240 --> 00:52:46,480
um well i think it's it it really is

00:52:44,720 --> 00:52:48,079
going to depend on the deployment

00:52:46,480 --> 00:52:49,920
that you choose i mean one of the things

00:52:48,079 --> 00:52:52,240
that we're we're doing here is the

00:52:49,920 --> 00:52:53,119
starting nodes are deployed very close

00:52:52,240 --> 00:52:54,960
to

00:52:53,119 --> 00:52:57,359
uh the cassandra nodes so that is going

00:52:54,960 --> 00:53:00,800
to cut down kind of on your network hop

00:52:57,359 --> 00:53:02,720
latency i think you have to consider the

00:53:00,800 --> 00:53:04,319
there's definitely some trade-offs here

00:53:02,720 --> 00:53:07,920
so you know one of the things that

00:53:04,319 --> 00:53:10,079
stargate nodes are really doing is uh

00:53:07,920 --> 00:53:12,079
providing a layer that that can help you

00:53:10,079 --> 00:53:14,400
scale out reads a little bit better

00:53:12,079 --> 00:53:15,920
and and uh take some of that traffic

00:53:14,400 --> 00:53:18,800
that's off of

00:53:15,920 --> 00:53:20,240
the core cassandra nodes that maybe can

00:53:18,800 --> 00:53:21,040
focus a little bit more on handling the

00:53:20,240 --> 00:53:24,079
right load

00:53:21,040 --> 00:53:24,800
so um it's it's it's sort of like a

00:53:24,079 --> 00:53:26,480
matter of

00:53:24,800 --> 00:53:28,720
how do you configure this and what's

00:53:26,480 --> 00:53:32,559
your ratio of starting notes to

00:53:28,720 --> 00:53:34,160
uh old-school cassandra nodes um we

00:53:32,559 --> 00:53:36,640
one of the things that we're working on

00:53:34,160 --> 00:53:37,599
is a blog about how you kind of run your

00:53:36,640 --> 00:53:40,000
own performance

00:53:37,599 --> 00:53:42,160
tests using something called nosql bench

00:53:40,000 --> 00:53:44,400
and another open source

00:53:42,160 --> 00:53:46,400
um tool that that we've been working

00:53:44,400 --> 00:53:47,599
with um so look for more content

00:53:46,400 --> 00:53:49,920
on how you do that here in the near

00:53:47,599 --> 00:53:49,920
future

00:53:50,000 --> 00:53:53,920
there is a question here about uh in the

00:53:52,400 --> 00:53:54,800
architecture we're showing both graphql

00:53:53,920 --> 00:53:57,440
and sql

00:53:54,800 --> 00:53:59,520
do we need both of them no it that's

00:53:57,440 --> 00:54:00,720
that's there so you can choose

00:53:59,520 --> 00:54:01,839
the language that you want to use

00:54:00,720 --> 00:54:03,040
whether you want to communicate with

00:54:01,839 --> 00:54:05,119
graphql

00:54:03,040 --> 00:54:06,800
or just simple rest calls or the

00:54:05,119 --> 00:54:08,559
document database like we

00:54:06,800 --> 00:54:10,000
we showed there's also the native

00:54:08,559 --> 00:54:12,640
protocol it's really

00:54:10,000 --> 00:54:14,240
about flexibility so um if you have no

00:54:12,640 --> 00:54:16,800
need for the graphql endpoint well

00:54:14,240 --> 00:54:18,319
you you just don't call it uh and and

00:54:16,800 --> 00:54:20,960
it's it's not an issue

00:54:18,319 --> 00:54:22,400
but there are some architectures that

00:54:20,960 --> 00:54:23,920
like the graphql interface and when you

00:54:22,400 --> 00:54:25,200
start looking at things like federation

00:54:23,920 --> 00:54:27,839
for graphql

00:54:25,200 --> 00:54:29,200
uh it gets pretty interesting as you you

00:54:27,839 --> 00:54:30,880
can then query across multiple

00:54:29,200 --> 00:54:32,160
datastores that's a whole another

00:54:30,880 --> 00:54:34,400
webinar

00:54:32,160 --> 00:54:35,920
it is we're running out of time so i

00:54:34,400 --> 00:54:37,520
wonder if we might just kind of put up

00:54:35,920 --> 00:54:40,400
some of our

00:54:37,520 --> 00:54:41,680
last little promos and contact info here

00:54:40,400 --> 00:54:43,520
and we'll try to get to the

00:54:41,680 --> 00:54:45,200
questions that we didn't get get to and

00:54:43,520 --> 00:54:46,720
work on on figuring out how to follow up

00:54:45,200 --> 00:54:49,119
with that

00:54:46,720 --> 00:54:49,920
uh we are i'm doing a workshop at

00:54:49,119 --> 00:54:52,319
kubecon

00:54:49,920 --> 00:54:53,200
europe i mean that's less than a month

00:54:52,319 --> 00:54:56,640
away now

00:54:53,200 --> 00:55:00,240
man that's coming up fast myself and

00:54:56,640 --> 00:55:03,359
alex wachnev he's a

00:55:00,240 --> 00:55:05,839
fantastic developer advocate and

00:55:03,359 --> 00:55:07,280
leads a great workshop so i'm excited to

00:55:05,839 --> 00:55:11,440
do that with him

00:55:07,280 --> 00:55:11,440
free workshop and that's on may 4th

00:55:12,480 --> 00:55:15,760
i think audrey just shared out the the

00:55:14,240 --> 00:55:18,559
url if you if you're interested in

00:55:15,760 --> 00:55:18,559
registering for that

00:55:19,359 --> 00:55:23,839
awesome well i appreciate it jeff

00:55:21,520 --> 00:55:26,000
christina you want to take us out

00:55:23,839 --> 00:55:27,760
yeah sure thank you so much to chris and

00:55:26,000 --> 00:55:29,920
jeff for their time today and thank you

00:55:27,760 --> 00:55:32,640
to all the participants who joined us

00:55:29,920 --> 00:55:32,960
um as a reminder this recording will be

00:55:32,640 --> 00:55:34,880
on

00:55:32,960 --> 00:55:37,200
the linux foundation youtube page later

00:55:34,880 --> 00:55:40,319
today we hope you're able to join us for

00:55:37,200 --> 00:55:45,280
future webinars have a wonderful day

00:55:40,319 --> 00:55:45,280

YouTube URL: https://www.youtube.com/watch?v=oFbyYlmDMRw


