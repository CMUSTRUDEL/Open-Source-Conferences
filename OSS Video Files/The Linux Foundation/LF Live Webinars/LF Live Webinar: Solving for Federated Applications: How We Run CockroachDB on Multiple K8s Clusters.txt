Title: LF Live Webinar: Solving for Federated Applications: How We Run CockroachDB on Multiple K8s Clusters
Publication date: 2021-04-08
Playlist: LF Live Webinars
Description: 
	sponsored by Cockroach Labs

In this hands-on session, we will walk through the deployment of CockroachDB on Kubernetes across three clusters. Running anything multi-cluster in Kubernetes presents a range of challenges and in this session, we will talk through a few of the major issues with security and networking.
Captions: 
	00:00:00,240 --> 00:00:03,199
alrighty um thank you for that wonderful

00:00:02,480 --> 00:00:05,120
introduction

00:00:03,199 --> 00:00:07,040
um i hope it's all clear to everybody uh

00:00:05,120 --> 00:00:08,240
keith and i have done webinars a few

00:00:07,040 --> 00:00:10,719
times together

00:00:08,240 --> 00:00:11,360
and so as as noted we are happy to get

00:00:10,719 --> 00:00:13,120
questions

00:00:11,360 --> 00:00:14,639
throughout uh the course of this i know

00:00:13,120 --> 00:00:15,759
sometimes you wait until the end and

00:00:14,639 --> 00:00:17,279
answer things but

00:00:15,759 --> 00:00:18,960
um we are happy to get questions along

00:00:17,279 --> 00:00:20,320
the way um we're gonna give a little bit

00:00:18,960 --> 00:00:21,600
of a presentation up front but we're

00:00:20,320 --> 00:00:22,720
gonna get deep into

00:00:21,600 --> 00:00:25,039
you know we're gonna have some fingers

00:00:22,720 --> 00:00:27,359
on keyboard and show you uh

00:00:25,039 --> 00:00:29,760
a demo here as well um on this on this

00:00:27,359 --> 00:00:32,399
topic now this is something that is

00:00:29,760 --> 00:00:33,440
very near and dear to my heart i'm uh i

00:00:32,399 --> 00:00:35,280
have been kind of

00:00:33,440 --> 00:00:37,920
in the kubernetes community for a while

00:00:35,280 --> 00:00:38,960
and and i'm a real advocate of all

00:00:37,920 --> 00:00:40,879
things that are happening in our

00:00:38,960 --> 00:00:44,000
community across lots of different ways

00:00:40,879 --> 00:00:46,239
um but in particular you know running

00:00:44,000 --> 00:00:47,200
workloads in kubernetes that that use

00:00:46,239 --> 00:00:50,000
data

00:00:47,200 --> 00:00:51,280
and that use state and and these sort of

00:00:50,000 --> 00:00:53,440
things but more importantly

00:00:51,280 --> 00:00:55,600
in this context how do you do that uh

00:00:53,440 --> 00:00:57,039
globally dispersed you know how do you

00:00:55,600 --> 00:00:58,399
you know are you running multiple

00:00:57,039 --> 00:01:00,160
clusters and then you're doing

00:00:58,399 --> 00:01:01,840
asynchronous data replication between

00:01:00,160 --> 00:01:02,960
like there's a lot of challenges i think

00:01:01,840 --> 00:01:04,799
that we have

00:01:02,960 --> 00:01:06,240
um and it really comes down to you know

00:01:04,799 --> 00:01:07,360
i i think of that as federated

00:01:06,240 --> 00:01:09,280
applications

00:01:07,360 --> 00:01:11,040
um you know the sig fed group in in the

00:01:09,280 --> 00:01:12,320
kubernetes community has had some fits

00:01:11,040 --> 00:01:13,600
and starts but it's always because

00:01:12,320 --> 00:01:16,240
there's like some really

00:01:13,600 --> 00:01:16,640
difficult things to solve and i and i

00:01:16,240 --> 00:01:18,640
hope

00:01:16,640 --> 00:01:20,159
what we present here today is is

00:01:18,640 --> 00:01:22,000
valuable to everybody

00:01:20,159 --> 00:01:23,840
and can help give some context into some

00:01:22,000 --> 00:01:24,560
of the challenges that that that are

00:01:23,840 --> 00:01:26,159
faced when we

00:01:24,560 --> 00:01:27,680
when we do get geographically

00:01:26,159 --> 00:01:29,520
distributed um

00:01:27,680 --> 00:01:31,759
in our in our kubernetes deployments and

00:01:29,520 --> 00:01:33,520
and how do you run our applications so

00:01:31,759 --> 00:01:35,280
we are going to use cockroachdb as an

00:01:33,520 --> 00:01:36,960
example of of how we're solving for this

00:01:35,280 --> 00:01:38,240
problem well that's what we do we are a

00:01:36,960 --> 00:01:38,960
database that was kind of designed for

00:01:38,240 --> 00:01:40,640
this world but

00:01:38,960 --> 00:01:41,759
um but and that's what we're going to

00:01:40,640 --> 00:01:43,280
talk about and sorry for not

00:01:41,759 --> 00:01:44,240
capitalizing kubernetes here you guys

00:01:43,280 --> 00:01:47,680
i'm so sorry

00:01:44,240 --> 00:01:49,759
i i apologize to the entire community um

00:01:47,680 --> 00:01:51,119
but by by means of introduction keith

00:01:49,759 --> 00:01:52,799
you want to give a quick introduction of

00:01:51,119 --> 00:01:54,640
your experience in the in the community

00:01:52,799 --> 00:01:56,880
and kind of where you come

00:01:54,640 --> 00:01:57,840
from yeah absolutely but before i do

00:01:56,880 --> 00:01:59,840
that i'm going to give you

00:01:57,840 --> 00:02:01,119
i'm going to give you a little bit of uh

00:01:59,840 --> 00:02:02,479
of crud here

00:02:01,119 --> 00:02:04,399
because you didn't tell me i had to be

00:02:02,479 --> 00:02:05,600
polite on this call

00:02:04,399 --> 00:02:07,280
at the top of the call they're like you

00:02:05,600 --> 00:02:09,200
got to be really polite and nice to

00:02:07,280 --> 00:02:09,840
people you know that's not what i do jim

00:02:09,200 --> 00:02:11,680
why don't you

00:02:09,840 --> 00:02:13,760
do it all the time invite me to that i

00:02:11,680 --> 00:02:16,800
love you so um keith mcclellan

00:02:13,760 --> 00:02:19,440
um so i uh have been

00:02:16,800 --> 00:02:21,040
in the the greater uh cloud native space

00:02:19,440 --> 00:02:24,319
for

00:02:21,040 --> 00:02:27,680
what seven years now i started my

00:02:24,319 --> 00:02:29,360
my career in containers at mesosphere so

00:02:27,680 --> 00:02:31,440
you know kind of a precursor to

00:02:29,360 --> 00:02:33,680
kubernetes um

00:02:31,440 --> 00:02:35,360
i have uh i have been kind of leading

00:02:33,680 --> 00:02:36,480
our kubernetes efforts here at cockroach

00:02:35,360 --> 00:02:39,519
labs now for

00:02:36,480 --> 00:02:41,440
a little over two years um

00:02:39,519 --> 00:02:43,200
what the thing that kind of brought me

00:02:41,440 --> 00:02:43,920
to cockroach labs was one of the the

00:02:43,200 --> 00:02:47,120
things that's

00:02:43,920 --> 00:02:47,920
hardest to do in a cloud native

00:02:47,120 --> 00:02:50,720
ecosystem

00:02:47,920 --> 00:02:51,519
is manage state right so that's kind of

00:02:50,720 --> 00:02:54,160
why

00:02:51,519 --> 00:02:54,720
why i joined the company right it's this

00:02:54,160 --> 00:02:57,519
is a

00:02:54,720 --> 00:02:59,200
really hard thing to do and cockroachdb

00:02:57,519 --> 00:03:00,720
quite frankly is the easiest database

00:02:59,200 --> 00:03:01,519
i've found to run regardless where

00:03:00,720 --> 00:03:03,360
you're running it

00:03:01,519 --> 00:03:04,879
which makes it perfect for running in

00:03:03,360 --> 00:03:07,280
kubernetes yeah

00:03:04,879 --> 00:03:07,920
in pods on kubernetes and yeah and and

00:03:07,280 --> 00:03:10,400
likewise

00:03:07,920 --> 00:03:11,760
and and keith you i go to keith all the

00:03:10,400 --> 00:03:13,120
time with some really deeper technical

00:03:11,760 --> 00:03:14,560
questions so y'all feel free to ask

00:03:13,120 --> 00:03:17,200
whatever you want of keith uh

00:03:14,560 --> 00:03:18,000
we can go to lots of different levels um

00:03:17,200 --> 00:03:19,840
i i originally

00:03:18,000 --> 00:03:21,280
got introduced to kubernetes i was at a

00:03:19,840 --> 00:03:24,560
company called core os

00:03:21,280 --> 00:03:25,360
and i i i too saw the problem with some

00:03:24,560 --> 00:03:28,319
of the

00:03:25,360 --> 00:03:29,840
state like i was i'm an ex developer the

00:03:28,319 --> 00:03:31,760
application needs a database

00:03:29,840 --> 00:03:34,080
how does that work right and so that's

00:03:31,760 --> 00:03:35,920
how i got introduced to cockroach and um

00:03:34,080 --> 00:03:37,519
i'm happy to be here now it's it's an

00:03:35,920 --> 00:03:38,879
interesting thing so let me give a

00:03:37,519 --> 00:03:39,519
little quick introduction five ten

00:03:38,879 --> 00:03:41,120
minutes on

00:03:39,519 --> 00:03:43,680
some slides and we'll get right into

00:03:41,120 --> 00:03:45,280
demo um you know cockroach and and i

00:03:43,680 --> 00:03:46,640
think what keith and i both saw was this

00:03:45,280 --> 00:03:47,519
is something that was architected for

00:03:46,640 --> 00:03:49,040
the cloud

00:03:47,519 --> 00:03:50,560
but ultimately i believe it was

00:03:49,040 --> 00:03:52,239
architected for kubernetes

00:03:50,560 --> 00:03:54,319
you know cockroach is direct descendant

00:03:52,239 --> 00:03:55,840
of google cloud spanner um it was

00:03:54,319 --> 00:03:57,360
actually the google cloud spanner white

00:03:55,840 --> 00:03:59,920
paper which was the

00:03:57,360 --> 00:04:00,640
um kind of the the the how we got

00:03:59,920 --> 00:04:03,519
started here

00:04:00,640 --> 00:04:04,400
um three google engineers uh spencer

00:04:03,519 --> 00:04:05,840
peter and ben

00:04:04,400 --> 00:04:07,519
our three founders actually started this

00:04:05,840 --> 00:04:10,400
company to try to build um

00:04:07,519 --> 00:04:11,920
spanner but not tied to to google

00:04:10,400 --> 00:04:13,040
infrastructure uh how do you build an

00:04:11,920 --> 00:04:14,239
open source version of that and so

00:04:13,040 --> 00:04:16,000
that's really the

00:04:14,239 --> 00:04:17,600
the the foundation of where we started

00:04:16,000 --> 00:04:18,720
but this is a relational database and

00:04:17,600 --> 00:04:20,079
all the beauty that you get with the

00:04:18,720 --> 00:04:22,160
relation database

00:04:20,079 --> 00:04:24,479
with familiar sql with consistent you

00:04:22,160 --> 00:04:25,440
know isolation of strict serializable

00:04:24,479 --> 00:04:26,800
isolation

00:04:25,440 --> 00:04:28,720
um but more importantly this isn't a

00:04:26,800 --> 00:04:29,600
document document model underneath where

00:04:28,720 --> 00:04:31,440
you're kind of

00:04:29,600 --> 00:04:33,040
uh accelerating complexity when you

00:04:31,440 --> 00:04:33,360
start to change things or the more model

00:04:33,040 --> 00:04:35,199
them

00:04:33,360 --> 00:04:37,360
the more documents they have the more

00:04:35,199 --> 00:04:39,840
difficult let's use the

00:04:37,360 --> 00:04:41,360
the simplicity and the elegance of of a

00:04:39,840 --> 00:04:42,880
relational database

00:04:41,360 --> 00:04:44,880
and but make that distributed and that's

00:04:42,880 --> 00:04:46,479
really what cockroach delivers but

00:04:44,880 --> 00:04:48,000
we implement we are implemented as a

00:04:46,479 --> 00:04:50,479
series of nodes you can imagine

00:04:48,000 --> 00:04:52,560
each one of these running in a pod spin

00:04:50,479 --> 00:04:54,560
up a new node pointed at the cluster and

00:04:52,560 --> 00:04:56,160
it just takes care of rebalancing and

00:04:54,560 --> 00:04:57,759
dealing with kind of where data lives in

00:04:56,160 --> 00:04:59,280
that cluster we'll show you that

00:04:57,759 --> 00:05:00,880
we could scale further we could scale

00:04:59,280 --> 00:05:01,840
across regions and when you start to

00:05:00,880 --> 00:05:04,000
scale

00:05:01,840 --> 00:05:05,440
across regions this is where kubernetes

00:05:04,000 --> 00:05:06,479
starts to have some problems right are

00:05:05,440 --> 00:05:08,400
you going to have

00:05:06,479 --> 00:05:10,400
can can you run one kubernetes cluster

00:05:08,400 --> 00:05:13,840
across multiple regions and clouds

00:05:10,400 --> 00:05:15,680
that's difficult to do and federating a

00:05:13,840 --> 00:05:17,039
single control plane across all that

00:05:15,680 --> 00:05:18,240
that's some tough stuff

00:05:17,039 --> 00:05:19,600
lots of companies out there trying to

00:05:18,240 --> 00:05:20,960
figure that out right now and i think

00:05:19,600 --> 00:05:23,360
it's interesting

00:05:20,960 --> 00:05:25,280
but for us we just federate at the data

00:05:23,360 --> 00:05:26,720
layer we just create one single logical

00:05:25,280 --> 00:05:27,840
database across you know all the

00:05:26,720 --> 00:05:29,120
different regions and we're going to

00:05:27,840 --> 00:05:31,440
show you that

00:05:29,120 --> 00:05:32,960
um in cockroach but again being able to

00:05:31,440 --> 00:05:34,320
do that it's not simple you actually

00:05:32,960 --> 00:05:35,600
have to deal with the speed of light it

00:05:34,320 --> 00:05:37,520
becomes a big issue

00:05:35,600 --> 00:05:39,440
now we are going to show you today not

00:05:37,520 --> 00:05:41,039
only can we do that across regions one

00:05:39,440 --> 00:05:42,720
single logical database

00:05:41,039 --> 00:05:44,560
well we could actually deploy a single

00:05:42,720 --> 00:05:45,919
logical database ask any node for the

00:05:44,560 --> 00:05:47,280
data that's going to access the data

00:05:45,919 --> 00:05:48,800
wherever it lives

00:05:47,280 --> 00:05:50,240
across three different cloud providers

00:05:48,800 --> 00:05:51,520
and i and and you're going to see a

00:05:50,240 --> 00:05:52,080
diagram of what we're going to do but we

00:05:51,520 --> 00:05:54,800
can do that

00:05:52,080 --> 00:05:56,240
as well which there's a lot of questions

00:05:54,800 --> 00:05:57,280
there's just such thing as a multi-cloud

00:05:56,240 --> 00:05:59,600
application

00:05:57,280 --> 00:06:01,199
we have a multi-cloud database like one

00:05:59,600 --> 00:06:03,199
single logical database

00:06:01,199 --> 00:06:04,240
any endpoint in cockroach can service

00:06:03,199 --> 00:06:05,759
reads and rights

00:06:04,240 --> 00:06:07,360
but cockroach in the name really comes

00:06:05,759 --> 00:06:09,520
from our resilient nature

00:06:07,360 --> 00:06:11,120
um you know depending on what you want

00:06:09,520 --> 00:06:12,880
to survive from an h a point of view

00:06:11,120 --> 00:06:15,360
whatever your failure domain is

00:06:12,880 --> 00:06:16,400
is it a node is it a rack is it a server

00:06:15,360 --> 00:06:18,880
is it a rack

00:06:16,400 --> 00:06:20,160
is it a data center is a region is it a

00:06:18,880 --> 00:06:22,160
cluster

00:06:20,160 --> 00:06:24,160
um and so there's a lot of stuff that we

00:06:22,160 --> 00:06:25,039
do within cockroach that allows you to

00:06:24,160 --> 00:06:28,400
survive the

00:06:25,039 --> 00:06:29,360
failure of a node without service

00:06:28,400 --> 00:06:30,639
disruption

00:06:29,360 --> 00:06:32,479
what we're doing is we're ultimately

00:06:30,639 --> 00:06:33,759
writing data in triplicate across

00:06:32,479 --> 00:06:35,759
multiple different nodes

00:06:33,759 --> 00:06:37,759
and then we use raft just simply it's

00:06:35,759 --> 00:06:38,000
very similar to the way that xcd works

00:06:37,759 --> 00:06:40,479
in

00:06:38,000 --> 00:06:41,039
in kubernetes in fact we contribute to

00:06:40,479 --> 00:06:43,039
upstream

00:06:41,039 --> 00:06:44,800
uh to upstream fcd and a lot of the

00:06:43,039 --> 00:06:46,720
stuff that we've done is actually

00:06:44,800 --> 00:06:48,479
uh under the cover is actually helping

00:06:46,720 --> 00:06:49,599
kubernetes a lot of the things that

00:06:48,479 --> 00:06:51,919
we've found

00:06:49,599 --> 00:06:52,639
um uh to fix an scds so if you look at

00:06:51,919 --> 00:06:55,199
raft

00:06:52,639 --> 00:06:56,720
or lcd slash raft there's lots of

00:06:55,199 --> 00:06:58,160
innovation that we've done there and

00:06:56,720 --> 00:06:59,840
that's a lot of the area we we

00:06:58,160 --> 00:07:02,240
contribute this is a

00:06:59,840 --> 00:07:04,240
our code base i like to think of as a a

00:07:02,240 --> 00:07:05,680
phd in distributed systems there are

00:07:04,240 --> 00:07:07,120
some really interesting things that we

00:07:05,680 --> 00:07:08,319
solve for as we've kind of battled the

00:07:07,120 --> 00:07:09,919
speed of light

00:07:08,319 --> 00:07:11,360
what's really interesting i think when

00:07:09,919 --> 00:07:12,639
you start to think about multiple

00:07:11,360 --> 00:07:14,560
different clusters

00:07:12,639 --> 00:07:16,160
um and the way that cockroach works

00:07:14,560 --> 00:07:16,880
because we are a series of different

00:07:16,160 --> 00:07:18,960
nodes

00:07:16,880 --> 00:07:20,560
uh you can simply set up a cluster and

00:07:18,960 --> 00:07:25,039
it's one single logical database

00:07:20,560 --> 00:07:26,800
any node can service a read or a write

00:07:25,039 --> 00:07:29,680
and that's really important because each

00:07:26,800 --> 00:07:31,199
node is is the same binary

00:07:29,680 --> 00:07:33,280
in fact if you wanted to bring up our

00:07:31,199 --> 00:07:34,800
management ui or db console which

00:07:33,280 --> 00:07:36,560
keith is going to show you you could

00:07:34,800 --> 00:07:38,319
point that at any cluster and god

00:07:36,560 --> 00:07:39,759
man keith i hope you don't kill the node

00:07:38,319 --> 00:07:40,639
that you're actually doing it on later

00:07:39,759 --> 00:07:42,639
but that's a yeah

00:07:40,639 --> 00:07:44,240
you're going to get into that dude we

00:07:42,639 --> 00:07:45,840
had some fun building this demo i think

00:07:44,240 --> 00:07:48,080
you're gonna love this demo

00:07:45,840 --> 00:07:49,440
um but our ui is actually service from

00:07:48,080 --> 00:07:51,520
any one of these you can

00:07:49,440 --> 00:07:53,120
literally so like it's it's it's kind of

00:07:51,520 --> 00:07:53,759
a core distributed principle of what we

00:07:53,120 --> 00:07:55,520
do

00:07:53,759 --> 00:07:57,360
we can ask for data but find it anywhere

00:07:55,520 --> 00:07:59,360
within that cluster so i can ask a node

00:07:57,360 --> 00:08:00,400
that's in us west but if that data lives

00:07:59,360 --> 00:08:02,160
over in

00:08:00,400 --> 00:08:03,759
europe or u.s east or whatever it could

00:08:02,160 --> 00:08:06,080
find that right

00:08:03,759 --> 00:08:07,120
but we locate data we allow

00:08:06,080 --> 00:08:09,840
organizations and

00:08:07,120 --> 00:08:11,280
and customers to pinpoint or pin data to

00:08:09,840 --> 00:08:13,120
a particular location so that we can

00:08:11,280 --> 00:08:14,879
beat the speed of light

00:08:13,120 --> 00:08:16,639
so that we can actually locate data

00:08:14,879 --> 00:08:18,080
closest to users now people love this

00:08:16,639 --> 00:08:19,759
for other reasons like

00:08:18,080 --> 00:08:21,840
can i comply with data privacy

00:08:19,759 --> 00:08:23,759
regulations to say all german data lives

00:08:21,840 --> 00:08:24,240
on german servers that sort of thing and

00:08:23,759 --> 00:08:25,759
so

00:08:24,240 --> 00:08:28,560
there's some really unique capabilities

00:08:25,759 --> 00:08:30,080
in cockroach but it's very well aligned

00:08:28,560 --> 00:08:31,599
with the distributed nature of

00:08:30,080 --> 00:08:33,599
kubernetes and in fact

00:08:31,599 --> 00:08:35,360
built within the same core principles

00:08:33,599 --> 00:08:37,120
the same way that pods work

00:08:35,360 --> 00:08:40,000
the way that we survive the way that we

00:08:37,120 --> 00:08:42,479
can scale uh very very similar in fact

00:08:40,000 --> 00:08:44,080
the same kind of you know etcd and raf

00:08:42,479 --> 00:08:46,959
type stuff right and so

00:08:44,080 --> 00:08:48,000
um some core principles at the base of

00:08:46,959 --> 00:08:50,959
what we're doing

00:08:48,000 --> 00:08:51,600
very very similar to to uh to kubernetes

00:08:50,959 --> 00:08:53,920
so

00:08:51,600 --> 00:08:55,120
that's a quick intro it's actually an

00:08:53,920 --> 00:08:56,160
important intro because it's actually

00:08:55,120 --> 00:08:57,200
going to allow us to explain some

00:08:56,160 --> 00:08:58,480
principles that are going on in

00:08:57,200 --> 00:08:59,760
kubernetes underneath

00:08:58,480 --> 00:09:01,600
when we get to kind of the networking

00:08:59,760 --> 00:09:02,480
stuff and everything else so keith you

00:09:01,600 --> 00:09:05,519
want to walk through

00:09:02,480 --> 00:09:06,640
um you know disaster and and set up the

00:09:05,519 --> 00:09:10,320
demo a little bit

00:09:06,640 --> 00:09:11,600
yeah absolutely so when we start moving

00:09:10,320 --> 00:09:14,480
to

00:09:11,600 --> 00:09:16,080
the cloud native actually leave let's

00:09:14,480 --> 00:09:17,839
stay on that previous slide for for a

00:09:16,080 --> 00:09:20,160
couple oh sorry buddy i i was going to

00:09:17,839 --> 00:09:22,320
the qa and i i got lost sorry no that's

00:09:20,160 --> 00:09:25,440
okay so um

00:09:22,320 --> 00:09:28,880
so when we start moving to to

00:09:25,440 --> 00:09:31,120
cloud native disaster recovery the

00:09:28,880 --> 00:09:32,880
we need to treat a different class of

00:09:31,120 --> 00:09:33,760
events as if they're an everyday

00:09:32,880 --> 00:09:36,320
experience

00:09:33,760 --> 00:09:37,279
so when you're managing your own data

00:09:36,320 --> 00:09:39,920
centers

00:09:37,279 --> 00:09:41,760
um the the most common type of failure

00:09:39,920 --> 00:09:44,640
domain that you're managing for

00:09:41,760 --> 00:09:45,839
is rack failure where you have you know

00:09:44,640 --> 00:09:46,480
you'll have a switch at the top of your

00:09:45,839 --> 00:09:48,800
rack

00:09:46,480 --> 00:09:50,240
that switch might fail that'll take out

00:09:48,800 --> 00:09:52,160
all of the

00:09:50,240 --> 00:09:53,440
the communications to any of the servers

00:09:52,160 --> 00:09:55,360
that are in that rack

00:09:53,440 --> 00:09:57,360
so generally speaking you want to make

00:09:55,360 --> 00:09:59,360
sure that you spread your applications

00:09:57,360 --> 00:10:00,800
across multiple racks so that you're not

00:09:59,360 --> 00:10:03,279
so that a single rack failure doesn't

00:10:00,800 --> 00:10:05,600
bring down all your infrastructure right

00:10:03,279 --> 00:10:06,480
as we move to the cloud we've had to

00:10:05,600 --> 00:10:09,600
start treating

00:10:06,480 --> 00:10:11,120
either availability zones or regions as

00:10:09,600 --> 00:10:13,839
kind of the failure domain depending on

00:10:11,120 --> 00:10:15,519
what we need to be able to survive

00:10:13,839 --> 00:10:17,760
and that means that we need to

00:10:15,519 --> 00:10:18,320
automatically heal from that type of a

00:10:17,760 --> 00:10:20,640
failure

00:10:18,320 --> 00:10:21,839
because as soon as a human has to be

00:10:20,640 --> 00:10:24,800
involved

00:10:21,839 --> 00:10:26,560
in a desi in an event then that's what

00:10:24,800 --> 00:10:29,760
basically what makes it a disaster

00:10:26,560 --> 00:10:32,959
right because humans are disasters

00:10:29,760 --> 00:10:33,680
and computers are highly available right

00:10:32,959 --> 00:10:36,000
so

00:10:33,680 --> 00:10:38,720
so when we when we start talking about

00:10:36,000 --> 00:10:42,240
this we want to make sure that

00:10:38,720 --> 00:10:45,360
detecting a failure is autonomous

00:10:42,240 --> 00:10:49,200
that um that the healing process is

00:10:45,360 --> 00:10:51,440
is automatic um that we don't lose any

00:10:49,200 --> 00:10:53,360
data

00:10:51,440 --> 00:10:54,640
that if we do have a service disruption

00:10:53,360 --> 00:10:57,440
that's rto

00:10:54,640 --> 00:10:58,399
recovery time objective that it's

00:10:57,440 --> 00:11:01,839
measured

00:10:58,399 --> 00:11:05,279
in seconds not in minutes or hours

00:11:01,839 --> 00:11:07,040
right um we want to and we want to make

00:11:05,279 --> 00:11:08,399
sure that the people that are tasked

00:11:07,040 --> 00:11:12,000
with fixing the problem

00:11:08,399 --> 00:11:15,200
are the people that are also tasked with

00:11:12,000 --> 00:11:17,360
um building the solution of the problem

00:11:15,200 --> 00:11:19,600
so in a lot of cases in traditional dr

00:11:17,360 --> 00:11:21,120
where you have two sites um the

00:11:19,600 --> 00:11:23,440
application team

00:11:21,120 --> 00:11:24,560
effectively created the problem but the

00:11:23,440 --> 00:11:26,320
solution

00:11:24,560 --> 00:11:28,079
is really on the storage team because

00:11:26,320 --> 00:11:29,360
the storage team is then restoring back

00:11:28,079 --> 00:11:32,240
from backups and

00:11:29,360 --> 00:11:32,560
tape drives or you know off-site storage

00:11:32,240 --> 00:11:36,399
or

00:11:32,560 --> 00:11:38,399
or whatnot um so instead what we want to

00:11:36,399 --> 00:11:42,399
do is we want to be able to detect

00:11:38,399 --> 00:11:44,800
a failure and continue to operate

00:11:42,399 --> 00:11:46,640
and we're going to demonstrate how a one

00:11:44,800 --> 00:11:47,920
of the various ways we could do that

00:11:46,640 --> 00:11:49,519
here today

00:11:47,920 --> 00:11:51,279
i i think what's interesting here keith

00:11:49,519 --> 00:11:52,079
too by the way i love the the process

00:11:51,279 --> 00:11:54,079
owner

00:11:52,079 --> 00:11:55,440
where do we think about these things and

00:11:54,079 --> 00:11:57,519
it's actually extending

00:11:55,440 --> 00:11:59,200
up the stack from storage all the way up

00:11:57,519 --> 00:12:01,120
into the application layer

00:11:59,200 --> 00:12:02,959
and i think we architect for disaster

00:12:01,120 --> 00:12:04,480
and we are we architect for dr now

00:12:02,959 --> 00:12:05,040
pretty much at every layer right like i

00:12:04,480 --> 00:12:06,240
think

00:12:05,040 --> 00:12:07,360
there was another version of this we

00:12:06,240 --> 00:12:08,800
were talking about it from a security

00:12:07,360 --> 00:12:10,000
point of view and certificates i mean

00:12:08,800 --> 00:12:12,000
there's lots of different ways

00:12:10,000 --> 00:12:14,160
or points where you want to think about

00:12:12,000 --> 00:12:17,120
dr in a cloud native world right

00:12:14,160 --> 00:12:18,560
yeah and so um what you really need to

00:12:17,120 --> 00:12:20,720
do is you need to change

00:12:18,560 --> 00:12:21,680
your perspective on how you think about

00:12:20,720 --> 00:12:25,519
vr

00:12:21,680 --> 00:12:29,120
so um so legacy thinking

00:12:25,519 --> 00:12:29,120
on this is um

00:12:29,519 --> 00:12:34,560
is designing for failure right right

00:12:32,639 --> 00:12:37,360
this thing has failed i need to get it

00:12:34,560 --> 00:12:39,360
back up and running how do i do that

00:12:37,360 --> 00:12:40,800
what we're doing is we're designing for

00:12:39,360 --> 00:12:42,880
survival

00:12:40,800 --> 00:12:44,959
we're saying we know this thing is going

00:12:42,880 --> 00:12:45,920
to fail how do we make sure we continue

00:12:44,959 --> 00:12:49,680
to operate

00:12:45,920 --> 00:12:52,000
even when that happens and that

00:12:49,680 --> 00:12:54,399
slight change in perspective informs an

00:12:52,000 --> 00:12:56,880
enormous number of design decisions

00:12:54,399 --> 00:12:58,880
around how you build your system and

00:12:56,880 --> 00:13:01,519
where you invest your time and energy

00:12:58,880 --> 00:13:03,519
and what the the side effect is that it

00:13:01,519 --> 00:13:05,120
ends up actually being less expensive to

00:13:03,519 --> 00:13:06,880
design to survive

00:13:05,120 --> 00:13:08,560
than it does to design to fail because

00:13:06,880 --> 00:13:11,600
for when you design to fail

00:13:08,560 --> 00:13:15,279
i've got to have a complete cold standby

00:13:11,600 --> 00:13:16,800
of everything so i have to buy two times

00:13:15,279 --> 00:13:18,800
my infrastructure

00:13:16,800 --> 00:13:20,160
right um just to be able to recover

00:13:18,800 --> 00:13:21,760
because i don't know you know there

00:13:20,160 --> 00:13:23,760
could be a fire in my data center

00:13:21,760 --> 00:13:25,279
right that's the type of disaster i

00:13:23,760 --> 00:13:27,200
might need to survive

00:13:25,279 --> 00:13:29,680
or recover from if i'm designing to

00:13:27,200 --> 00:13:31,360
survive what i'm actually doing is i'm

00:13:29,680 --> 00:13:32,720
in our case we're splitting across three

00:13:31,360 --> 00:13:34,399
data centers we're going to jump to this

00:13:32,720 --> 00:13:36,720
here in a second

00:13:34,399 --> 00:13:38,639
if i need if i lose a data center i just

00:13:36,720 --> 00:13:42,399
need to be able to continue to service

00:13:38,639 --> 00:13:44,160
all of my requests with two of the

00:13:42,399 --> 00:13:46,000
with my two remaining data centers so

00:13:44,160 --> 00:13:48,959
instead of having two times my

00:13:46,000 --> 00:13:50,480
infrastructure i really only have 1.5 x

00:13:48,959 --> 00:13:51,519
my infrastructure to be able to survive

00:13:50,480 --> 00:13:54,560
a data center failure

00:13:51,519 --> 00:13:57,040
so i can actually bring my my costs

00:13:54,560 --> 00:13:58,959
down by being more distributed and using

00:13:57,040 --> 00:13:59,519
a database like cockroachdb to enable

00:13:58,959 --> 00:14:02,160
that

00:13:59,519 --> 00:14:03,760
and then and then avoid all the the

00:14:02,160 --> 00:14:06,079
manual labor costs of

00:14:03,760 --> 00:14:08,399
manual remediation of an event i mean

00:14:06,079 --> 00:14:09,680
that's not even active

00:14:08,399 --> 00:14:11,440
it's not even the labor costs it's the

00:14:09,680 --> 00:14:13,040
mistakes yeah exactly

00:14:11,440 --> 00:14:15,680
i mean so when i was setting this thing

00:14:13,040 --> 00:14:17,279
up this morning because this demos

00:14:15,680 --> 00:14:18,800
got a lot of moving parts which is why

00:14:17,279 --> 00:14:20,720
we're we're gonna show the

00:14:18,800 --> 00:14:21,839
we're gonna show the um the julia

00:14:20,720 --> 00:14:25,040
child's uh

00:14:21,839 --> 00:14:26,639
uh finally final baked piece and not the

00:14:25,040 --> 00:14:28,000
no i think i'd like to think about any

00:14:26,639 --> 00:14:29,519
one of those fishing shows where they

00:14:28,000 --> 00:14:30,720
have a scuba diver down there underneath

00:14:29,519 --> 00:14:31,120
the water and they put like a fish on

00:14:30,720 --> 00:14:33,680
the hook

00:14:31,120 --> 00:14:34,320
yeah yeah yeah so much it's very much

00:14:33,680 --> 00:14:37,279
like that

00:14:34,320 --> 00:14:39,519
um you know when you're designing when

00:14:37,279 --> 00:14:42,399
you're designing to survive these things

00:14:39,519 --> 00:14:44,000
um there there's just a lot of stuff

00:14:42,399 --> 00:14:45,680
that can go wrong so when i was setting

00:14:44,000 --> 00:14:47,440
up this cluster this morning i

00:14:45,680 --> 00:14:48,800
put in the commands in the wrong order

00:14:47,440 --> 00:14:50,079
and i had to blow everything away and

00:14:48,800 --> 00:14:51,920
get started again

00:14:50,079 --> 00:14:53,120
and that's the kind of thing that

00:14:51,920 --> 00:14:56,560
happens when you

00:14:53,120 --> 00:15:00,160
have traditional dr a human

00:14:56,560 --> 00:15:03,360
runs the wrong restore command or

00:15:00,160 --> 00:15:04,639
a human does like most disasters are

00:15:03,360 --> 00:15:05,920
human created they aren't actually

00:15:04,639 --> 00:15:07,600
infrastructure created

00:15:05,920 --> 00:15:09,199
like i think i don't remember what the

00:15:07,600 --> 00:15:11,920
exact statistics are but it's like

00:15:09,199 --> 00:15:12,720
senate seven out of ten disasters were

00:15:11,920 --> 00:15:14,800
caused by a

00:15:12,720 --> 00:15:15,760
person doing something they should have

00:15:14,800 --> 00:15:19,120
either

00:15:15,760 --> 00:15:20,399
not been able to do or um even if they

00:15:19,120 --> 00:15:21,760
should been able to do that should have

00:15:20,399 --> 00:15:22,560
been double checked before they were

00:15:21,760 --> 00:15:24,160
able to do it

00:15:22,560 --> 00:15:25,680
right those are the types of things that

00:15:24,160 --> 00:15:27,120
cause disasters generally they're like

00:15:25,680 --> 00:15:27,920
me they've fat fingered it you know what

00:15:27,120 --> 00:15:29,759
i mean like

00:15:27,920 --> 00:15:31,519
it's hard everything anything go around

00:15:29,759 --> 00:15:33,440
and always does right so

00:15:31,519 --> 00:15:34,800
so designing to survive when someone

00:15:33,440 --> 00:15:37,279
does something stupid

00:15:34,800 --> 00:15:37,920
um like we all do on a pretty regular

00:15:37,279 --> 00:15:40,720
basis

00:15:37,920 --> 00:15:41,600
is is really the most important thing um

00:15:40,720 --> 00:15:43,839
so

00:15:41,600 --> 00:15:45,040
um if you want to jump to the next slide

00:15:43,839 --> 00:15:46,560
yeah um

00:15:45,040 --> 00:15:48,240
it just it's just a really point it's

00:15:46,560 --> 00:15:50,079
funny keith i often

00:15:48,240 --> 00:15:51,920
what's the trade-off of you know and how

00:15:50,079 --> 00:15:53,279
different is availability from disaster

00:15:51,920 --> 00:15:53,920
recovery i think in the new world the

00:15:53,279 --> 00:15:56,240
modern world

00:15:53,920 --> 00:15:57,680
we design for things to be available and

00:15:56,240 --> 00:15:59,360
optimize for that and

00:15:57,680 --> 00:16:00,880
disaster becomes something you don't you

00:15:59,360 --> 00:16:02,720
don't you don't react to it's actually

00:16:00,880 --> 00:16:03,600
you design it into the system almost

00:16:02,720 --> 00:16:06,160
right and so

00:16:03,600 --> 00:16:08,240
it's it's interesting so yeah so so

00:16:06,160 --> 00:16:10,800
here's a picture of what we didn't do

00:16:08,240 --> 00:16:11,519
i didn't set up three cockroach db

00:16:10,800 --> 00:16:14,800
clusters

00:16:11,519 --> 00:16:17,279
in three different sites

00:16:14,800 --> 00:16:19,920
and then kind of try to set up some

00:16:17,279 --> 00:16:22,399
asynchronous replication across them

00:16:19,920 --> 00:16:24,320
because i might lose data there wouldn't

00:16:22,399 --> 00:16:26,800
be globally consistent

00:16:24,320 --> 00:16:28,560
um this is this is actually really hard

00:16:26,800 --> 00:16:29,920
architecture to manage

00:16:28,560 --> 00:16:31,680
because and this is this is typically

00:16:29,920 --> 00:16:33,839
what people do though right i mean and

00:16:31,680 --> 00:16:36,320
you know not with cockroaches

00:16:33,839 --> 00:16:38,480
with uh instance of postgres or you know

00:16:36,320 --> 00:16:41,360
like what you know insert database right

00:16:38,480 --> 00:16:42,560
if you were to do the try to span

00:16:41,360 --> 00:16:44,720
multiple sites

00:16:42,560 --> 00:16:45,839
with a traditional database what you

00:16:44,720 --> 00:16:48,560
would do is you'd have

00:16:45,839 --> 00:16:49,920
instances of the database in multiple

00:16:48,560 --> 00:16:51,519
locations you'd have some sort of

00:16:49,920 --> 00:16:53,440
synchronization process

00:16:51,519 --> 00:16:54,959
you'd also have some sort of a sharding

00:16:53,440 --> 00:16:58,959
process um

00:16:54,959 --> 00:17:01,759
to be able to kind of make this stuff

00:16:58,959 --> 00:17:03,279
um distributed at least partially um

00:17:01,759 --> 00:17:04,720
it'd be really painful it'd probably

00:17:03,279 --> 00:17:06,880
break a lot unless you had some really

00:17:04,720 --> 00:17:10,160
smart engineers who basically built

00:17:06,880 --> 00:17:11,520
a distributed database for you um but if

00:17:10,160 --> 00:17:13,039
we move to the next slide i'm going to

00:17:11,520 --> 00:17:14,319
start showing even if you're doing this

00:17:13,039 --> 00:17:15,839
and you're distributed in a single

00:17:14,319 --> 00:17:17,120
environment you're still not solving the

00:17:15,839 --> 00:17:20,079
problem of

00:17:17,120 --> 00:17:21,679
latencies at scale across multiple

00:17:20,079 --> 00:17:23,360
clusters right and i think that's and

00:17:21,679 --> 00:17:25,039
you're still managing three clusters but

00:17:23,360 --> 00:17:26,480
you're managing three databases as well

00:17:25,039 --> 00:17:29,039
which

00:17:26,480 --> 00:17:31,120
operational nightmare so right and well

00:17:29,039 --> 00:17:32,559
and the the reality is is your failure

00:17:31,120 --> 00:17:35,679
domain there is a pod or

00:17:32,559 --> 00:17:38,880
exactly a node right and so

00:17:35,679 --> 00:17:41,600
um so what we did

00:17:38,880 --> 00:17:42,559
was um we created a single logical

00:17:41,600 --> 00:17:46,480
cluster across

00:17:42,559 --> 00:17:47,360
three different cloud providers um we're

00:17:46,480 --> 00:17:49,120
using the

00:17:47,360 --> 00:17:51,520
different kubernetes distributions in

00:17:49,120 --> 00:17:54,640
each of them actually so we've got eks

00:17:51,520 --> 00:17:58,080
in amazon gke in google

00:17:54,640 --> 00:17:59,600
and aks in azure mainly because those

00:17:58,080 --> 00:18:00,480
were the ones that were convenient for

00:17:59,600 --> 00:18:02,640
me to spin up

00:18:00,480 --> 00:18:04,080
um i think you know jim i'm a big open

00:18:02,640 --> 00:18:06,960
shift fan i would have

00:18:04,080 --> 00:18:08,799
loved to have made one of these uh an

00:18:06,960 --> 00:18:12,000
open shifter region and now that

00:18:08,799 --> 00:18:13,919
rosa is uh is publicly available i think

00:18:12,000 --> 00:18:15,440
i'm gonna add a fourth region to this

00:18:13,919 --> 00:18:16,320
demo but i didn't get a chance to do it

00:18:15,440 --> 00:18:19,200
this week

00:18:16,320 --> 00:18:20,720
um under the covers we had a couple of

00:18:19,200 --> 00:18:21,520
network peering options that we could

00:18:20,720 --> 00:18:23,919
have selected

00:18:21,520 --> 00:18:25,520
i think folks if if anyone that's on the

00:18:23,919 --> 00:18:26,880
call has has been on one of our previous

00:18:25,520 --> 00:18:29,520
webinars you've seen us

00:18:26,880 --> 00:18:31,200
do this with something called submariner

00:18:29,520 --> 00:18:34,080
which does an ipsec tunnel

00:18:31,200 --> 00:18:35,440
across the sites um it's probably what i

00:18:34,080 --> 00:18:37,200
recommend for

00:18:35,440 --> 00:18:38,960
for most production deployments today

00:18:37,200 --> 00:18:43,039
it's a pretty performance solution

00:18:38,960 --> 00:18:45,679
um but to to kind of highlight

00:18:43,039 --> 00:18:47,120
a different way of doing this i chose to

00:18:45,679 --> 00:18:49,840
use something called scupper

00:18:47,120 --> 00:18:50,320
so scupper is another uh open source

00:18:49,840 --> 00:18:52,240
project

00:18:50,320 --> 00:18:54,240
it creates something called a van or a

00:18:52,240 --> 00:18:57,600
virtual application network

00:18:54,240 --> 00:19:01,600
effectively we're doing tp tcp

00:18:57,600 --> 00:19:02,720
over http so um so scupper creates these

00:19:01,600 --> 00:19:06,720
kind of

00:19:02,720 --> 00:19:09,919
tls tunnels between each of the sites

00:19:06,720 --> 00:19:11,360
and um and then uses that to encapsulate

00:19:09,919 --> 00:19:14,559
the tcp

00:19:11,360 --> 00:19:16,640
um packets that uh would normally be

00:19:14,559 --> 00:19:20,880
transported between our nodes

00:19:16,640 --> 00:19:24,240
so our nodes talk over over grpc

00:19:20,880 --> 00:19:26,480
right so so all the grpc traffic is then

00:19:24,240 --> 00:19:28,400
going over these scupper tunnels to

00:19:26,480 --> 00:19:30,320
so that we can get uh communication

00:19:28,400 --> 00:19:32,400
between all of the nodes

00:19:30,320 --> 00:19:34,799
so keith this is like solving i i think

00:19:32,400 --> 00:19:36,480
when we think about federated clusters

00:19:34,799 --> 00:19:37,280
one of the biggest issues is networking

00:19:36,480 --> 00:19:38,559
right because you have different

00:19:37,280 --> 00:19:39,520
networks and that that's what you're

00:19:38,559 --> 00:19:41,280
solving here

00:19:39,520 --> 00:19:43,360
the other is security right and so i

00:19:41,280 --> 00:19:44,960
know we didn't do this in this demo

00:19:43,360 --> 00:19:46,559
but you have solved for that as well in

00:19:44,960 --> 00:19:49,679
the past as well right

00:19:46,559 --> 00:19:50,799
yeah so in this case we're um i am

00:19:49,679 --> 00:19:54,000
pre-sharing

00:19:50,799 --> 00:19:56,240
um tls certificates sure um one of the

00:19:54,000 --> 00:19:58,720
great things about another open source

00:19:56,240 --> 00:20:02,159
technology called vault from hashicorp

00:19:58,720 --> 00:20:04,240
is it also uses etcd um and

00:20:02,159 --> 00:20:06,799
it uses the same kind of underlying

00:20:04,240 --> 00:20:09,039
protocol as ncd uses raft

00:20:06,799 --> 00:20:11,760
so you could we could have just as

00:20:09,039 --> 00:20:14,159
easily set up scupper and set up vault

00:20:11,760 --> 00:20:14,960
and had a single um certificate

00:20:14,159 --> 00:20:17,520
authority

00:20:14,960 --> 00:20:19,200
right sign our certificates um right

00:20:17,520 --> 00:20:20,320
that that's something that will probably

00:20:19,200 --> 00:20:23,360
add to this demo

00:20:20,320 --> 00:20:25,200
over time um you know we have shown it

00:20:23,360 --> 00:20:27,520
at to your point before you with

00:20:25,200 --> 00:20:28,559
submariner um submariner is a little bit

00:20:27,520 --> 00:20:30,880
more

00:20:28,559 --> 00:20:32,640
sensitive to having that as a

00:20:30,880 --> 00:20:33,200
requirement than scupper is scupper lets

00:20:32,640 --> 00:20:36,000
you

00:20:33,200 --> 00:20:37,760
pretty easily inject like a sign ca

00:20:36,000 --> 00:20:41,440
which is what i did for this demo

00:20:37,760 --> 00:20:44,720
um whereas um submariner

00:20:41,440 --> 00:20:45,200
is is going to do its own um it's going

00:20:44,720 --> 00:20:46,799
to

00:20:45,200 --> 00:20:48,799
reach out to cert manager and actually

00:20:46,799 --> 00:20:50,400
request certificates which is the right

00:20:48,799 --> 00:20:51,760
way to do things skipper hasn't got that

00:20:50,400 --> 00:20:52,960
feature yet yeah the only the only

00:20:51,760 --> 00:20:55,360
reason i wanted to highlight

00:20:52,960 --> 00:20:56,240
we talk about you know designing for

00:20:55,360 --> 00:20:59,360
disaster

00:20:56,240 --> 00:21:00,640
um and actually thinking through and and

00:20:59,360 --> 00:21:02,240
it's it's part of the overall

00:21:00,640 --> 00:21:03,679
architecture there's so many different

00:21:02,240 --> 00:21:05,440
layers to think about

00:21:03,679 --> 00:21:06,400
federating of clusters is not simple

00:21:05,440 --> 00:21:07,440
you're still going to have to manage

00:21:06,400 --> 00:21:08,799
three clusters here

00:21:07,440 --> 00:21:10,480
but what are the things that you can do

00:21:08,799 --> 00:21:12,480
in a distributed system to do it across

00:21:10,480 --> 00:21:14,000
multiple and i think that's a

00:21:12,480 --> 00:21:15,440
you know and i think the networking

00:21:14,000 --> 00:21:16,960
thing we're getting closer to solving

00:21:15,440 --> 00:21:18,159
this at scale with with lots of

00:21:16,960 --> 00:21:19,120
different solutions here but we're

00:21:18,159 --> 00:21:21,440
talking about networking in

00:21:19,120 --> 00:21:22,880
three different cloud providers here so

00:21:21,440 --> 00:21:24,559
i i don't want to underestimate the

00:21:22,880 --> 00:21:26,880
magic of what's going on in this

00:21:24,559 --> 00:21:28,159
this diagram like that's pretty

00:21:26,880 --> 00:21:30,559
phenomenal dude

00:21:28,159 --> 00:21:32,320
yeah and and we're peering all three of

00:21:30,559 --> 00:21:33,440
these networks across three different

00:21:32,320 --> 00:21:35,120
cloud providers

00:21:33,440 --> 00:21:37,120
where the infrastructure for the cloud

00:21:35,120 --> 00:21:38,000
providers doesn't provide any capability

00:21:37,120 --> 00:21:39,200
that's right

00:21:38,000 --> 00:21:40,960
they don't want to by the way they

00:21:39,200 --> 00:21:42,000
really don't want to yeah yeah yeah and

00:21:40,960 --> 00:21:44,880
so like yeah like

00:21:42,000 --> 00:21:45,520
you talk about multi-cloud uh or even

00:21:44,880 --> 00:21:47,280
hybrid

00:21:45,520 --> 00:21:49,039
if you want to run openshift on-prem and

00:21:47,280 --> 00:21:50,880
you do it that's just a multi-cloud

00:21:49,039 --> 00:21:52,159
problem in my opinion multi-hybrid it's

00:21:50,880 --> 00:21:53,360
it's multiple

00:21:52,159 --> 00:21:55,200
multiple regions you're going to have

00:21:53,360 --> 00:21:58,159
this problem right so yeah

00:21:55,200 --> 00:21:58,720
um yeah and i think it's it's uh this

00:21:58,159 --> 00:22:02,400
can this

00:21:58,720 --> 00:22:05,679
is happening is it easy to do this keith

00:22:02,400 --> 00:22:06,400
i mean um i set this up this demo up

00:22:05,679 --> 00:22:10,840
this morning

00:22:06,400 --> 00:22:13,440
so it's not that hard um

00:22:10,840 --> 00:22:16,559
the it's i wouldn't say it's

00:22:13,440 --> 00:22:19,760
easy i think scupper is probably

00:22:16,559 --> 00:22:23,360
currently the easiest way to do this

00:22:19,760 --> 00:22:25,039
um i think submariner is a bit more

00:22:23,360 --> 00:22:26,320
robust but it's a little bit more

00:22:25,039 --> 00:22:27,679
difficult although there's some big

00:22:26,320 --> 00:22:29,120
changes that have come to that project

00:22:27,679 --> 00:22:30,640
in the last month that make it a lot

00:22:29,120 --> 00:22:32,640
easier to install and

00:22:30,640 --> 00:22:33,840
working with one of the guys on the team

00:22:32,640 --> 00:22:36,400
over there to

00:22:33,840 --> 00:22:38,000
to update our submariner demo to use the

00:22:36,400 --> 00:22:40,880
new install pattern which i think

00:22:38,000 --> 00:22:41,679
might get it closer to this um what i

00:22:40,880 --> 00:22:44,960
really like

00:22:41,679 --> 00:22:48,880
about scupper is that

00:22:44,960 --> 00:22:52,400
because it's over tls it's over http

00:22:48,880 --> 00:22:54,799
i don't have to do any kind of um

00:22:52,400 --> 00:22:55,679
i don't have to get the network team

00:22:54,799 --> 00:22:59,039
involved

00:22:55,679 --> 00:23:00,000
and get uh firewall rules set and

00:22:59,039 --> 00:23:03,039
whatnot

00:23:00,000 --> 00:23:04,400
um the downside is that it's um

00:23:03,039 --> 00:23:06,320
it's got more overhead because we're

00:23:04,400 --> 00:23:09,760
wrapping all of our tcp packets

00:23:06,320 --> 00:23:12,880
in http right so um

00:23:09,760 --> 00:23:14,720
so it it's better for

00:23:12,880 --> 00:23:16,240
environments like this where we want to

00:23:14,720 --> 00:23:16,720
kind of try something out and see how it

00:23:16,240 --> 00:23:18,960
goes

00:23:16,720 --> 00:23:19,760
um whereas submariner is probably what i

00:23:18,960 --> 00:23:22,000
would use

00:23:19,760 --> 00:23:24,799
today if i needed to do this for a

00:23:22,000 --> 00:23:27,600
production workload

00:23:24,799 --> 00:23:28,320
what what's really cool though is both

00:23:27,600 --> 00:23:32,159
projects

00:23:28,320 --> 00:23:35,600
are making huge leaps forward

00:23:32,159 --> 00:23:36,880
in a very in a real rapid way um

00:23:35,600 --> 00:23:38,400
i i've already been i've been

00:23:36,880 --> 00:23:40,320
interacting with the scupper team over

00:23:38,400 --> 00:23:42,320
the last few weeks when i was trying

00:23:40,320 --> 00:23:44,080
like well we started talking about this

00:23:42,320 --> 00:23:46,240
demo what three weeks ago jim

00:23:44,080 --> 00:23:48,080
i already put two bug fixes into scupper

00:23:46,240 --> 00:23:49,279
for me to make sure that this demo was

00:23:48,080 --> 00:23:51,760
was working for us they've been

00:23:49,279 --> 00:23:53,039
phenomenal i have every confidence that

00:23:51,760 --> 00:23:54,960
this is going to be every bit the

00:23:53,039 --> 00:23:57,279
production grade solution

00:23:54,960 --> 00:23:59,520
that um submariner is going to be and

00:23:57,279 --> 00:24:02,960
it's going to have the advantage of

00:23:59,520 --> 00:24:05,440
um being more being more flexible in its

00:24:02,960 --> 00:24:07,840
deployment pattern than submariner

00:24:05,440 --> 00:24:08,960
um but i don't i don't i also think

00:24:07,840 --> 00:24:10,640
submariner is probably gonna end up

00:24:08,960 --> 00:24:11,279
being the most more performant solution

00:24:10,640 --> 00:24:12,480
well and i think

00:24:11,279 --> 00:24:14,159
you know look at this tradeoffs for

00:24:12,480 --> 00:24:14,799
whatever and simplicity or speed or

00:24:14,159 --> 00:24:16,720
whatever you know

00:24:14,799 --> 00:24:18,799
overhead like you know there's different

00:24:16,720 --> 00:24:20,000
this is the beauty of this community

00:24:18,799 --> 00:24:21,760
i think there's different approaches

00:24:20,000 --> 00:24:23,360
that are emerging and i think the right

00:24:21,760 --> 00:24:24,240
approach for each implementation is

00:24:23,360 --> 00:24:26,799
going to

00:24:24,240 --> 00:24:27,760
win for that implementation and so i

00:24:26,799 --> 00:24:29,440
just i'm just

00:24:27,760 --> 00:24:31,679
excited that you were like keith i i

00:24:29,440 --> 00:24:33,679
know how hard you work and

00:24:31,679 --> 00:24:34,880
this this the the concepts are here to

00:24:33,679 --> 00:24:36,240
actually do this

00:24:34,880 --> 00:24:37,919
you just need to go out and find them

00:24:36,240 --> 00:24:39,039
and do it and that's the funny thing and

00:24:37,919 --> 00:24:40,960
that's what's interesting about this

00:24:39,039 --> 00:24:42,320
community but let's move on

00:24:40,960 --> 00:24:43,919
let's get out of slides dude so let's

00:24:42,320 --> 00:24:44,720
talk about what we're going to do here

00:24:43,919 --> 00:24:46,240
in the demo

00:24:44,720 --> 00:24:47,760
yeah so if you move to the next slide

00:24:46,240 --> 00:24:50,080
for me um

00:24:47,760 --> 00:24:51,279
so we're running something called tpcc

00:24:50,080 --> 00:24:54,240
which is a workload

00:24:51,279 --> 00:24:55,919
um it's a it's a standards-based

00:24:54,240 --> 00:24:57,200
benchmark tpc.org

00:24:55,919 --> 00:24:59,120
feel free to go read about it it

00:24:57,200 --> 00:25:01,440
simulates a warehouse

00:24:59,120 --> 00:25:02,240
workload we're going to be running not

00:25:01,440 --> 00:25:04,320
that big of a

00:25:02,240 --> 00:25:05,760
a load because i'm going to be doing

00:25:04,320 --> 00:25:08,320
some kind of

00:25:05,760 --> 00:25:09,760
uh just 100 what you're like just 150

00:25:08,320 --> 00:25:10,960
warehouses right yeah we're running 50

00:25:09,760 --> 00:25:14,080
warehouses per

00:25:10,960 --> 00:25:17,120
site um and that that

00:25:14,080 --> 00:25:18,880
uh workload client is just kind of

00:25:17,120 --> 00:25:20,559
you know generating some load against

00:25:18,880 --> 00:25:21,840
the cluster yeah and then we're going to

00:25:20,559 --> 00:25:23,600
use something kind of neat

00:25:21,840 --> 00:25:25,600
called one second before you do that by

00:25:23,600 --> 00:25:26,960
the way the tpcc workload we actually

00:25:25,600 --> 00:25:28,640
package that in the binary with

00:25:26,960 --> 00:25:30,799
cockroach a little commercial

00:25:28,640 --> 00:25:32,559
uh we package a couple different

00:25:30,799 --> 00:25:33,120
workloads within the binary that if you

00:25:32,559 --> 00:25:36,080
wanted to

00:25:33,120 --> 00:25:37,600
start using cockroach cockroach demo uh

00:25:36,080 --> 00:25:39,679
you can actually run a tpc

00:25:37,600 --> 00:25:40,640
workload against our our database today

00:25:39,679 --> 00:25:43,600
but yeah and we

00:25:40,640 --> 00:25:44,640
um test cockroachdb up to 140 000

00:25:43,600 --> 00:25:48,960
workloads so

00:25:44,640 --> 00:25:52,080
that's what three roughly three orders

00:25:48,960 --> 00:25:54,240
four orders of magnitude um larger than

00:25:52,080 --> 00:25:56,320
what i'm doing today i was

00:25:54,240 --> 00:25:57,760
um this is just kind of a this is more

00:25:56,320 --> 00:25:59,520
of a functional demo than

00:25:57,760 --> 00:26:01,679
than it was meant to be a performance

00:25:59,520 --> 00:26:02,080
demo um and then we're doing something

00:26:01,679 --> 00:26:03,600
called

00:26:02,080 --> 00:26:05,600
using something called kube doom which

00:26:03,600 --> 00:26:09,600
is literally what it sounds like

00:26:05,600 --> 00:26:10,559
it's doom like from in software 1997's

00:26:09,600 --> 00:26:13,279
doom

00:26:10,559 --> 00:26:14,640
that's um they released it as a as a gpl

00:26:13,279 --> 00:26:18,000
project

00:26:14,640 --> 00:26:21,440
um and someone um hacked it

00:26:18,000 --> 00:26:24,640
to do chaos testing on kubernetes pods

00:26:21,440 --> 00:26:26,559
so we're going to be um killing some

00:26:24,640 --> 00:26:29,919
roach demons

00:26:26,559 --> 00:26:33,840
um in um

00:26:29,919 --> 00:26:37,039
in kubdum and it's going to kill pods

00:26:33,840 --> 00:26:40,240
in our aks region

00:26:37,039 --> 00:26:40,559
um so a couple of and which is kind of

00:26:40,240 --> 00:26:42,080
fun

00:26:40,559 --> 00:26:44,320
and we're gonna show the database keeps

00:26:42,080 --> 00:26:46,400
running um it may end up killing the

00:26:44,320 --> 00:26:49,440
workload generator in the east

00:26:46,400 --> 00:26:51,279
because um uh it is uh

00:26:49,440 --> 00:26:52,480
monsters don't appear differently based

00:26:51,279 --> 00:26:54,320
on what the pod is

00:26:52,480 --> 00:26:56,559
that's right so i might i might actually

00:26:54,320 --> 00:26:59,840
pod's a pod my monster is a monster

00:26:56,559 --> 00:27:00,880
i might shoot the tpcc demon um it

00:26:59,840 --> 00:27:03,200
actually is not

00:27:00,880 --> 00:27:04,559
uh thread safe either so it might i

00:27:03,200 --> 00:27:07,760
might kill

00:27:04,559 --> 00:27:11,200
um kub doom itself which could be fun

00:27:07,760 --> 00:27:15,039
so that's a little bit uh meta um using

00:27:11,200 --> 00:27:16,880
uh doom to kill its own process

00:27:15,039 --> 00:27:18,640
but keith i mean the trick here is like

00:27:16,880 --> 00:27:20,320
even if you were to kill that kubernetes

00:27:18,640 --> 00:27:21,600
is going to survive that pod the pod's

00:27:20,320 --> 00:27:22,720
going to come back right i mean

00:27:21,600 --> 00:27:24,960
yeah i just have to i'll have to

00:27:22,720 --> 00:27:26,880
reconnect exactly but and the trick here

00:27:24,960 --> 00:27:29,520
is and you may actually kill the

00:27:26,880 --> 00:27:30,559
the the admin ui that we were hitting or

00:27:29,520 --> 00:27:31,440
well maybe you're doing it from another

00:27:30,559 --> 00:27:34,480
region

00:27:31,440 --> 00:27:36,880
but the point is is like once a pod dies

00:27:34,480 --> 00:27:38,799
you're killing it we're killing the pod

00:27:36,880 --> 00:27:39,919
not the instance of cockroach we're

00:27:38,799 --> 00:27:41,440
killing the pod

00:27:39,919 --> 00:27:43,279
kubernetes automatically is going to

00:27:41,440 --> 00:27:44,399
restart that pod and cockroach is going

00:27:43,279 --> 00:27:46,080
to start in that

00:27:44,399 --> 00:27:47,679
and the database is not going to have

00:27:46,080 --> 00:27:49,440
any sort of

00:27:47,679 --> 00:27:51,120
well the database will continue the

00:27:49,440 --> 00:27:52,640
database will continue to run

00:27:51,120 --> 00:27:54,159
regardless of how long it takes for

00:27:52,640 --> 00:27:56,480
those pods to recover exactly

00:27:54,159 --> 00:27:57,200
um the ui i think i'm actually reading

00:27:56,480 --> 00:27:59,360
the ui

00:27:57,200 --> 00:28:00,640
out of the google um region oh good so

00:27:59,360 --> 00:28:02,640
you won't kill it this time because we

00:28:00,640 --> 00:28:05,279
had a couple of fun yeah so

00:28:02,640 --> 00:28:06,559
we shouldn't although i i mean i am

00:28:05,279 --> 00:28:08,799
using the load balancer

00:28:06,559 --> 00:28:09,600
um so one of the other things to keep in

00:28:08,799 --> 00:28:12,799
mind this is

00:28:09,600 --> 00:28:13,360
this is a specific discover so a van

00:28:12,799 --> 00:28:16,399
uses

00:28:13,360 --> 00:28:20,240
forward proxies um

00:28:16,399 --> 00:28:22,960
so when when i pull up the screen

00:28:20,240 --> 00:28:24,320
each region can see all the pods for all

00:28:22,960 --> 00:28:27,760
three of these regions

00:28:24,320 --> 00:28:30,320
and so in the east

00:28:27,760 --> 00:28:31,279
region it's going to see the central and

00:28:30,320 --> 00:28:33,120
west pods

00:28:31,279 --> 00:28:34,880
those are actually proxy containers

00:28:33,120 --> 00:28:37,679
they're just all they're doing is

00:28:34,880 --> 00:28:38,559
um acting as kind of a transport layer

00:28:37,679 --> 00:28:39,919
for the

00:28:38,559 --> 00:28:42,240
for the community the nodes to

00:28:39,919 --> 00:28:44,159
communicate with each other um

00:28:42,240 --> 00:28:46,559
we could end up killing a bunch of those

00:28:44,159 --> 00:28:48,240
too so um why don't we go ahead and if

00:28:46,559 --> 00:28:51,840
you could stop sharing your screen

00:28:48,240 --> 00:28:56,159
i will start sharing mine okay

00:28:51,840 --> 00:28:58,720
we'll go kill some stuff i love this

00:28:56,159 --> 00:29:00,960
so i also already used some cheat codes

00:28:58,720 --> 00:29:04,320
so that i can actually kill some demons

00:29:00,960 --> 00:29:05,440
there's something like 40 pods running

00:29:04,320 --> 00:29:10,480
in this environment

00:29:05,440 --> 00:29:12,000
so um so there's a there are a number of

00:29:10,480 --> 00:29:14,000
hey keith real quick there was a quick

00:29:12,000 --> 00:29:15,440
question there i mean one of these

00:29:14,000 --> 00:29:16,799
regions could have been on bare metal

00:29:15,440 --> 00:29:17,440
right i mean it's there's no i mean

00:29:16,799 --> 00:29:19,679
whatever

00:29:17,440 --> 00:29:20,720
like yeah regions or regions the clouds

00:29:19,679 --> 00:29:22,720
of cloud

00:29:20,720 --> 00:29:25,360
uh hybrid environments it could have

00:29:22,720 --> 00:29:26,720
been it could have been equinix metal

00:29:25,360 --> 00:29:28,320
we just chose these three because that's

00:29:26,720 --> 00:29:30,720
what we had it's good fun to watch we

00:29:28,320 --> 00:29:32,320
chose these three because i had accounts

00:29:30,720 --> 00:29:34,240
yeah that's right spin up infrastructure

00:29:32,320 --> 00:29:35,679
for these three i could have these we

00:29:34,240 --> 00:29:39,039
could have just as easily done this

00:29:35,679 --> 00:29:39,520
in digitalocean or done this on bare

00:29:39,039 --> 00:29:42,000
metal

00:29:39,520 --> 00:29:43,039
if i under under your under your desk

00:29:42,000 --> 00:29:45,600
yeah my i think

00:29:43,039 --> 00:29:46,880
we probably would not have wanted to run

00:29:45,600 --> 00:29:49,760
it on my laptop

00:29:46,880 --> 00:29:52,080
but we could have technically i mean i i

00:29:49,760 --> 00:29:55,200
tested kubedum on my laptop it did work

00:29:52,080 --> 00:29:58,159
um so um left-hand side of my screen

00:29:55,200 --> 00:29:58,799
the first three boxes the red um green

00:29:58,159 --> 00:30:00,320
and blue

00:29:58,799 --> 00:30:02,480
boxes those are just the workload

00:30:00,320 --> 00:30:04,320
generator running in the background um

00:30:02,480 --> 00:30:06,480
so we're processing transactions i

00:30:04,320 --> 00:30:07,919
kicked this off about 45 minutes ago so

00:30:06,480 --> 00:30:10,159
we've been processing transactions for a

00:30:07,919 --> 00:30:12,640
little while um the black

00:30:10,159 --> 00:30:13,440
area of the console screen is showing

00:30:12,640 --> 00:30:16,559
you

00:30:13,440 --> 00:30:20,000
the the current status of the pods in

00:30:16,559 --> 00:30:22,640
east so this is in azure so you can see

00:30:20,000 --> 00:30:23,200
the the cockroach db dash east-zero

00:30:22,640 --> 00:30:25,760
through eight

00:30:23,200 --> 00:30:27,360
pods are those are the actual local

00:30:25,760 --> 00:30:28,080
nodes that i'm gonna potentially be

00:30:27,360 --> 00:30:30,399
killing

00:30:28,080 --> 00:30:32,799
there's also uh the internal proxies

00:30:30,399 --> 00:30:34,960
these are the outbound proxies for

00:30:32,799 --> 00:30:37,279
uh for the cockroachdb pods that are

00:30:34,960 --> 00:30:39,120
local and then we have the inbound

00:30:37,279 --> 00:30:42,559
proxies which are marked as

00:30:39,120 --> 00:30:44,000
central and west okay

00:30:42,559 --> 00:30:48,000
there are also a couple of other things

00:30:44,000 --> 00:30:52,159
going on here we have a client container

00:30:48,000 --> 00:30:54,640
which is um

00:30:52,159 --> 00:30:56,000
the what's actually run in tpcc we have

00:30:54,640 --> 00:30:59,039
the service router

00:30:56,000 --> 00:30:59,760
um and we have the service controller um

00:30:59,039 --> 00:31:01,600
the only thing

00:30:59,760 --> 00:31:03,600
podge you're not seeing because this is

00:31:01,600 --> 00:31:05,760
showing the copper's db namespace

00:31:03,600 --> 00:31:07,600
is the kub doom pop um which is running

00:31:05,760 --> 00:31:11,360
in the kubdube namespace

00:31:07,600 --> 00:31:15,039
um so i didn't switch to that so um

00:31:11,360 --> 00:31:15,679
you can see we're running um 500 queries

00:31:15,039 --> 00:31:17,760
per second

00:31:15,679 --> 00:31:18,960
i didn't optimize the performance of the

00:31:17,760 --> 00:31:22,080
queries here at all

00:31:18,960 --> 00:31:23,760
so ignore the p99 latencies um

00:31:22,080 --> 00:31:25,279
if you've watched any of the demos that

00:31:23,760 --> 00:31:27,120
jim and i have done in the past

00:31:25,279 --> 00:31:29,120
you know i can get these latencies for

00:31:27,120 --> 00:31:32,080
tpcc down the

00:31:29,120 --> 00:31:33,600
you know 100-ish milliseconds for our

00:31:32,080 --> 00:31:36,559
p99 um

00:31:33,600 --> 00:31:38,159
have just didn't choose to do that uh

00:31:36,559 --> 00:31:41,039
for this particular demo

00:31:38,159 --> 00:31:41,840
um i did make sure that we could dig

00:31:41,039 --> 00:31:44,240
into

00:31:41,840 --> 00:31:45,760
um each of the regions and take a look

00:31:44,240 --> 00:31:47,120
at each of the nodes so you can see all

00:31:45,760 --> 00:31:49,600
of the nodes that i'm going to

00:31:47,120 --> 00:31:52,399
potentially be killing

00:31:49,600 --> 00:31:54,720
are all running some queries okay so

00:31:52,399 --> 00:31:58,080
i've run queries running across

00:31:54,720 --> 00:32:01,120
all of the nodes in the cluster

00:31:58,080 --> 00:32:04,480
um all right so

00:32:01,120 --> 00:32:07,279
now for the fun stuff um i'm gonna go

00:32:04,480 --> 00:32:07,279
over to doom

00:32:07,600 --> 00:32:11,120
i'm gonna make sure that i i can still

00:32:09,600 --> 00:32:14,399
fire walls

00:32:11,120 --> 00:32:16,240
i've got no clipping set setup so you

00:32:14,399 --> 00:32:19,039
can see the names of all the cockroach

00:32:16,240 --> 00:32:19,039
tv pods

00:32:21,840 --> 00:32:26,080
i'm going to go and kill some stuff i

00:32:23,679 --> 00:32:29,279
should still be kind of immune

00:32:26,080 --> 00:32:33,840
for a little bit

00:32:29,279 --> 00:32:36,320
oh wow keith really bloody pod killing

00:32:33,840 --> 00:32:36,880
yep bloody pod killing you're gonna

00:32:36,320 --> 00:32:38,880
start to

00:32:36,880 --> 00:32:41,039
see some of the pods see on the left

00:32:38,880 --> 00:32:42,640
hand side one of the east proxies died

00:32:41,039 --> 00:32:46,559
and restarted

00:32:42,640 --> 00:32:46,559
we got all kinds of stuff is dying

00:32:47,440 --> 00:32:51,760
oh i ran and then kubernetes and then

00:32:50,159 --> 00:32:52,480
you're terminating kubernetes just

00:32:51,760 --> 00:32:54,159
bringing them back

00:32:52,480 --> 00:32:56,240
meanwhile the database is not having any

00:32:54,159 --> 00:32:57,840
sort of impact

00:32:56,240 --> 00:33:00,960
now i'm shooting them with shotguns so

00:32:57,840 --> 00:33:02,880
they're dying faster

00:33:00,960 --> 00:33:04,080
because before i was using the gatling

00:33:02,880 --> 00:33:07,760
thing

00:33:04,080 --> 00:33:10,799
i can switch to the the one that

00:33:07,760 --> 00:33:14,320
like will probably kill me too the

00:33:10,799 --> 00:33:14,320
the um bfg

00:33:16,320 --> 00:33:23,279
up finally killed the cubed in pod

00:33:19,600 --> 00:33:24,720
well so we just saw a bunch of stuff die

00:33:23,279 --> 00:33:26,799
and you can see and we're still creating

00:33:24,720 --> 00:33:29,760
containers

00:33:26,799 --> 00:33:30,640
we killed the tpcc process that was

00:33:29,760 --> 00:33:33,840
local

00:33:30,640 --> 00:33:35,440
but my queries are still running on the

00:33:33,840 --> 00:33:37,919
west and in the central

00:33:35,440 --> 00:33:39,760
you see three nodes actually still

00:33:37,919 --> 00:33:43,440
haven't recovered yet they're recovering

00:33:39,760 --> 00:33:45,120
for us um we don't have any unavailable

00:33:43,440 --> 00:33:46,320
ranges which means we're continuing to

00:33:45,120 --> 00:33:50,159
process data

00:33:46,320 --> 00:33:50,799
as we go um and now that this suspect

00:33:50,159 --> 00:33:53,200
node

00:33:50,799 --> 00:33:55,200
uh list is down to zero our under

00:33:53,200 --> 00:33:56,320
replicated ranges will go back to zero

00:33:55,200 --> 00:33:57,039
and the cluster will be completely

00:33:56,320 --> 00:33:58,320
healthy

00:33:57,039 --> 00:33:59,840
so keith one of the things that we

00:33:58,320 --> 00:34:00,640
didn't kind of describe to the audience

00:33:59,840 --> 00:34:02,240
i i

00:34:00,640 --> 00:34:03,679
in cockroach when we write data to a

00:34:02,240 --> 00:34:04,960
cluster we're actually writing data in

00:34:03,679 --> 00:34:06,559
triplicate so

00:34:04,960 --> 00:34:07,679
you know the replica set is you know

00:34:06,559 --> 00:34:09,119
these three things that are running so

00:34:07,679 --> 00:34:10,560
why you're doing that keith l

00:34:09,119 --> 00:34:12,159
um so we're writing data in triplicate

00:34:10,560 --> 00:34:13,760
we're writing across multiple different

00:34:12,159 --> 00:34:15,200
nodes physical servers and that so that

00:34:13,760 --> 00:34:17,839
we can survive that so

00:34:15,200 --> 00:34:20,079
an underreplicated range just means that

00:34:17,839 --> 00:34:23,119
one of the replicas of that raft

00:34:20,079 --> 00:34:24,480
group is missing and so the database

00:34:23,119 --> 00:34:26,000
just knows that and it's just telling us

00:34:24,480 --> 00:34:27,200
oh wait you're missing you're missing

00:34:26,000 --> 00:34:29,679
some data here

00:34:27,200 --> 00:34:30,560
um what's interesting is that at no

00:34:29,679 --> 00:34:32,720
moment in time

00:34:30,560 --> 00:34:34,320
was there an unavailable range or range

00:34:32,720 --> 00:34:35,760
you think it as like a tablet or a shard

00:34:34,320 --> 00:34:36,560
we automate all that underneath the

00:34:35,760 --> 00:34:38,320
covers

00:34:36,560 --> 00:34:39,919
so the data no matter where it was being

00:34:38,320 --> 00:34:43,520
accessed in west

00:34:39,919 --> 00:34:45,679
east or in central was accessible

00:34:43,520 --> 00:34:48,399
um and that's the critical thing here we

00:34:45,679 --> 00:34:50,480
didn't have any impact to uh

00:34:48,399 --> 00:34:52,079
the availability of that data right and

00:34:50,480 --> 00:34:53,679
but but we did have

00:34:52,079 --> 00:34:56,079
because we killed some paws that stuff

00:34:53,679 --> 00:34:57,839
went away now when the pods came back

00:34:56,079 --> 00:34:59,680
we're just using stateful sets

00:34:57,839 --> 00:35:00,720
um to actually bring that pod back to

00:34:59,680 --> 00:35:02,800
its current state right reconnect

00:35:00,720 --> 00:35:04,079
reconnected to the storage it was using

00:35:02,800 --> 00:35:05,680
somebody was asking you know what's the

00:35:04,079 --> 00:35:07,200
best practice for you know deploying

00:35:05,680 --> 00:35:08,560
cockroach is it you know

00:35:07,200 --> 00:35:10,640
well how does that storage work well

00:35:08,560 --> 00:35:12,480
each node is actually has its own

00:35:10,640 --> 00:35:14,800
storage connected to it its own pv

00:35:12,480 --> 00:35:16,880
claim right and so ultimately stateful

00:35:14,800 --> 00:35:19,119
sets is managing that in kubernetes and

00:35:16,880 --> 00:35:21,119
as you kill the pod it just comes back

00:35:19,119 --> 00:35:22,880
uh and then the range is back that that

00:35:21,119 --> 00:35:26,720
little piece of data right keith

00:35:22,880 --> 00:35:28,800
yeah that's correct yep so

00:35:26,720 --> 00:35:31,520
so did you want to show uh the impact on

00:35:28,800 --> 00:35:35,599
the some of the metrics keith or no

00:35:31,520 --> 00:35:38,400
yeah um so i am waiting for the cluster

00:35:35,599 --> 00:35:41,040
to decide that it can get its metrics

00:35:38,400 --> 00:35:41,680
all right and i will reshare this is you

00:35:41,040 --> 00:35:43,760
know

00:35:41,680 --> 00:35:44,720
what'd you kill what did you you killed

00:35:43,760 --> 00:35:47,119
cube doom

00:35:44,720 --> 00:35:47,839
it killed coog doom i killed the tpcc

00:35:47,119 --> 00:35:50,160
container

00:35:47,839 --> 00:35:51,520
i think i killed all of the forward

00:35:50,160 --> 00:35:56,000
proxies in east

00:35:51,520 --> 00:35:56,880
as well as um most of the database pods

00:35:56,000 --> 00:35:59,359
at least once

00:35:56,880 --> 00:36:00,000
um right it looks like i don't have

00:35:59,359 --> 00:36:03,680
anything

00:36:00,000 --> 00:36:07,680
left i have look so for east

00:36:03,680 --> 00:36:09,680
i killed half of the database pods

00:36:07,680 --> 00:36:11,359
so anything that's here that says 4h

00:36:09,680 --> 00:36:12,560
those those are no longer sharing by the

00:36:11,359 --> 00:36:13,870
way keith did you know or

00:36:12,560 --> 00:36:16,160
oh sorry yes um

00:36:13,870 --> 00:36:18,720
[Music]

00:36:16,160 --> 00:36:20,079
there we go sorry so um for anything

00:36:18,720 --> 00:36:25,040
here that says 4-h

00:36:20,079 --> 00:36:25,040
um is um didn't die

00:36:25,359 --> 00:36:28,880
yeah because it's running for four hours

00:36:27,280 --> 00:36:31,040
right so um but the ones that

00:36:28,880 --> 00:36:32,960
said there so we killed uh a bunch of

00:36:31,040 --> 00:36:36,400
the forward proxies

00:36:32,960 --> 00:36:38,960
um both inbound and outbound

00:36:36,400 --> 00:36:40,880
we killed it looks like three or four of

00:36:38,960 --> 00:36:44,240
the cockroach tv pods

00:36:40,880 --> 00:36:46,320
um and um i managed to not

00:36:44,240 --> 00:36:47,760
kill the scupper router or the scupper

00:36:46,320 --> 00:36:48,800
service controller which is good

00:36:47,760 --> 00:36:51,119
which is good because that would have

00:36:48,800 --> 00:36:52,320
killed all of the proxies um that

00:36:51,119 --> 00:36:53,599
definitely would have taken

00:36:52,320 --> 00:36:55,760
that's effectively like taking the

00:36:53,599 --> 00:36:57,839
entire region out of service um

00:36:55,760 --> 00:36:59,200
the database will survive that too but

00:36:57,839 --> 00:37:00,960
let me ask you about that keith

00:36:59,200 --> 00:37:02,560
if you had done that i mean it would it

00:37:00,960 --> 00:37:04,560
would have basically i think we did this

00:37:02,560 --> 00:37:06,800
last time we ran this right like it

00:37:04,560 --> 00:37:08,480
it would resolve over time correct i

00:37:06,800 --> 00:37:09,040
mean it would just basically automate

00:37:08,480 --> 00:37:12,560
and

00:37:09,040 --> 00:37:14,000
heal correct that is correct yeah now

00:37:12,560 --> 00:37:16,560
the challenge that we're having right

00:37:14,000 --> 00:37:18,160
now is i i think i was wrong i think i

00:37:16,560 --> 00:37:20,960
actually killed the pod that was

00:37:18,160 --> 00:37:22,160
hosting the admin oh so you got to go to

00:37:20,960 --> 00:37:25,839
a different pod

00:37:22,160 --> 00:37:25,839
yeah so you have a different ip

00:37:27,839 --> 00:37:31,920
so again what keith can do is just point

00:37:29,839 --> 00:37:34,079
our point the browser at a different

00:37:31,920 --> 00:37:35,839
node that's running because i didn't set

00:37:34,079 --> 00:37:36,400
up a load global load balance yeah

00:37:35,839 --> 00:37:40,000
that's right

00:37:36,400 --> 00:37:40,000
yeah otherwise yeah it would have been

00:37:42,720 --> 00:37:49,520
all right there we go

00:37:46,000 --> 00:37:52,720
oh there we go yeah you're back

00:37:49,520 --> 00:37:55,440
all right let's see yep

00:37:52,720 --> 00:37:57,280
there we go so you can see we definitely

00:37:55,440 --> 00:37:58,720
had an impact to our queries per second

00:37:57,280 --> 00:38:00,560
while this was going on

00:37:58,720 --> 00:38:02,480
which is not surprising we did just take

00:38:00,560 --> 00:38:04,720
out a third of the cluster basically

00:38:02,480 --> 00:38:06,560
um but the database stayed available it

00:38:04,720 --> 00:38:09,119
was always processing queries

00:38:06,560 --> 00:38:09,920
and it's um it's starting to recover

00:38:09,119 --> 00:38:11,440
right now

00:38:09,920 --> 00:38:13,760
it won't because one of the load

00:38:11,440 --> 00:38:17,119
generators um died

00:38:13,760 --> 00:38:18,160
um it won't get back to full unless i

00:38:17,119 --> 00:38:21,119
restart that

00:38:18,160 --> 00:38:22,640
load generator but you aren't going to

00:38:21,119 --> 00:38:24,000
kill the load generator in real life

00:38:22,640 --> 00:38:24,720
because that's a bunch of people hitting

00:38:24,000 --> 00:38:27,760
your

00:38:24,720 --> 00:38:29,440
app which does yeah normally that would

00:38:27,760 --> 00:38:31,280
have a load balancer in front of it so

00:38:29,440 --> 00:38:35,040
it would have

00:38:31,280 --> 00:38:38,720
it would have continued um anyway um

00:38:35,040 --> 00:38:40,240
you know our latency numbers um

00:38:38,720 --> 00:38:41,920
kind of jumped around a little bit which

00:38:40,240 --> 00:38:42,880
is not surprising but the point is is

00:38:41,920 --> 00:38:46,160
that the cluster

00:38:42,880 --> 00:38:46,560
survived right and healed itself right

00:38:46,160 --> 00:38:47,920
and

00:38:46,560 --> 00:38:49,680
and i really didn't have to do much

00:38:47,920 --> 00:38:50,480
anything other than talk to you for a

00:38:49,680 --> 00:38:51,839
couple of minutes

00:38:50,480 --> 00:38:53,839
and this is one of those things where

00:38:51,839 --> 00:38:55,520
it's like you know we've we've talked to

00:38:53,839 --> 00:38:57,200
customers and they're like well we

00:38:55,520 --> 00:38:58,480
didn't even know an event happened until

00:38:57,200 --> 00:39:02,000
we went back in time

00:38:58,480 --> 00:39:04,000
and because stuff just basically

00:39:02,000 --> 00:39:05,359
survives right i think that's the that's

00:39:04,000 --> 00:39:06,880
the trick here it's like you know you

00:39:05,359 --> 00:39:09,760
gotta actually go find when

00:39:06,880 --> 00:39:10,160
you have issues our biggest challenge is

00:39:09,760 --> 00:39:12,400
um

00:39:10,160 --> 00:39:13,680
when customers want us to show failure

00:39:12,400 --> 00:39:14,800
in kubernetes

00:39:13,680 --> 00:39:16,560
it's kind of the reason why i want to

00:39:14,800 --> 00:39:18,320
build the demo it's actually hard to do

00:39:16,560 --> 00:39:19,200
without like straight up deleting the

00:39:18,320 --> 00:39:21,040
stateful sets

00:39:19,200 --> 00:39:22,320
because kubernetes will regenerate the

00:39:21,040 --> 00:39:23,680
pods so fast

00:39:22,320 --> 00:39:27,280
that we don't even have like a

00:39:23,680 --> 00:39:27,280
substantial service outage yeah

00:39:30,240 --> 00:39:34,320
all right well keith i love you i'm

00:39:32,480 --> 00:39:37,200
gonna i'm gonna stop sharing

00:39:34,320 --> 00:39:38,079
yeah um i do i do like doing chaos

00:39:37,200 --> 00:39:41,200
testing with

00:39:38,079 --> 00:39:42,160
with doom it brings me back to my video

00:39:41,200 --> 00:39:44,400
gaming heyday

00:39:42,160 --> 00:39:45,920
so it's always fun keith there were a

00:39:44,400 --> 00:39:47,440
couple questions in chat and if anybody

00:39:45,920 --> 00:39:48,560
has any questions of anything we went

00:39:47,440 --> 00:39:50,320
through i'll go through a couple of them

00:39:48,560 --> 00:39:51,440
i think jim answered a lot thank you jim

00:39:50,320 --> 00:39:53,440
for answering these

00:39:51,440 --> 00:39:54,800
um but somebody was actually asking

00:39:53,440 --> 00:39:56,640
about um

00:39:54,800 --> 00:39:58,800
a project i think they were asking about

00:39:56,640 --> 00:40:00,720
submariner it's just submarine or io

00:39:58,800 --> 00:40:03,680
right isn't that the submariner.io

00:40:00,720 --> 00:40:04,680
yeah and then scupper is scupper.io uh

00:40:03,680 --> 00:40:06,800
yeah

00:40:04,680 --> 00:40:07,520
s-k-u-p-p-e-r that's right now that's

00:40:06,800 --> 00:40:09,359
right

00:40:07,520 --> 00:40:12,880
yeah so if you're looking for that um i

00:40:09,359 --> 00:40:12,880
think those are the two projects um

00:40:14,160 --> 00:40:18,960
let's see here jim there was one

00:40:17,599 --> 00:40:20,480
question i didn't do a very good job

00:40:18,960 --> 00:40:23,440
answering about

00:40:20,480 --> 00:40:24,079
you limits in docker and kubernetes and

00:40:23,440 --> 00:40:26,800
kind of how we

00:40:24,079 --> 00:40:28,480
how we deal with that keith are you

00:40:26,800 --> 00:40:32,240
familiar with u-limits and whatnot

00:40:28,480 --> 00:40:33,520
yeah so um we don't run into a lot of

00:40:32,240 --> 00:40:36,720
problems with you limits

00:40:33,520 --> 00:40:37,359
um generally um that's this is

00:40:36,720 --> 00:40:40,880
definitely

00:40:37,359 --> 00:40:43,680
like a low-level kubernetes thing um

00:40:40,880 --> 00:40:45,920
when using the managed service providers

00:40:43,680 --> 00:40:47,520
kubernetes we don't

00:40:45,920 --> 00:40:51,119
have a whole lot of problems with this

00:40:47,520 --> 00:40:54,240
stuff sometimes we run into it with um

00:40:51,119 --> 00:40:55,920
um with self-managed um generally

00:40:54,240 --> 00:40:59,680
speaking you fix them outside

00:40:55,920 --> 00:41:02,160
of kubernetes um and then restart the

00:40:59,680 --> 00:41:02,880
the kubelet on the on the pod host and

00:41:02,160 --> 00:41:06,160
that

00:41:02,880 --> 00:41:07,760
and then that gets inherited um

00:41:06,160 --> 00:41:09,680
but that's yeah that's not something

00:41:07,760 --> 00:41:11,359
that we normally need to even really

00:41:09,680 --> 00:41:13,280
worry about we do have some instructions

00:41:11,359 --> 00:41:15,520
on what to set those uni limits to

00:41:13,280 --> 00:41:17,040
and our in our docs to support the

00:41:15,520 --> 00:41:18,839
database because we do have minimum

00:41:17,040 --> 00:41:21,520
requirements there right

00:41:18,839 --> 00:41:22,640
right so um

00:41:21,520 --> 00:41:24,079
another question that came through and

00:41:22,640 --> 00:41:24,720
this is a real basic kind of cockroach

00:41:24,079 --> 00:41:26,960
question keith

00:41:24,720 --> 00:41:28,720
how are paws distributed across nodes

00:41:26,960 --> 00:41:29,280
since those stateful sets are used does

00:41:28,720 --> 00:41:31,680
this mean

00:41:29,280 --> 00:41:33,520
it's restricted to one pod one node what

00:41:31,680 --> 00:41:36,000
happens if we want to score

00:41:33,520 --> 00:41:37,599
horizontally scale kind of a softball of

00:41:36,000 --> 00:41:39,359
a question

00:41:37,599 --> 00:41:40,960
yeah so by default we do use

00:41:39,359 --> 00:41:43,200
anti-affinities to stop

00:41:40,960 --> 00:41:45,599
multiple cockroach db pods from

00:41:43,200 --> 00:41:48,079
launching on the same physical server

00:41:45,599 --> 00:41:48,720
largely because we need to treat the

00:41:48,079 --> 00:41:52,000
server

00:41:48,720 --> 00:41:54,400
as a fault domain as well and so um

00:41:52,000 --> 00:41:55,760
so now in a multi-region deployment like

00:41:54,400 --> 00:41:58,319
this that's less important

00:41:55,760 --> 00:41:59,119
because we're treating our top level

00:41:58,319 --> 00:42:01,520
fault domain

00:41:59,119 --> 00:42:03,040
so from a replication perspective the

00:42:01,520 --> 00:42:06,480
top-level fault domain is the

00:42:03,040 --> 00:42:08,240
the region or the cloud so we're already

00:42:06,480 --> 00:42:11,200
not going to have multiple replicas

00:42:08,240 --> 00:42:12,319
on the same physical server um by

00:42:11,200 --> 00:42:16,240
default anyway

00:42:12,319 --> 00:42:19,040
um but you can enable

00:42:16,240 --> 00:42:20,000
um running multiple cockroach tv pods uh

00:42:19,040 --> 00:42:21,200
generally speaking

00:42:20,000 --> 00:42:23,680
you're you're going to get better

00:42:21,200 --> 00:42:26,800
performance in that scenario if you run

00:42:23,680 --> 00:42:30,880
larger individual cockroachdb pods

00:42:26,800 --> 00:42:32,400
and then um scale out by having more pot

00:42:30,880 --> 00:42:34,640
hosts

00:42:32,400 --> 00:42:35,520
um yeah that that's that's generally the

00:42:34,640 --> 00:42:37,359
best practice

00:42:35,520 --> 00:42:39,040
it can be done you can disable the

00:42:37,359 --> 00:42:42,400
affinities but then you have to be

00:42:39,040 --> 00:42:45,839
very careful about how you

00:42:42,400 --> 00:42:49,839
set the locality flag so that we don't

00:42:45,839 --> 00:42:51,359
um we don't uh put multiple replicas of

00:42:49,839 --> 00:42:54,000
the same data on the same

00:42:51,359 --> 00:42:55,920
physical node and and keith i mean

00:42:54,000 --> 00:42:57,599
that's a pattern to think of

00:42:55,920 --> 00:42:59,680
regardless of cockroach like you know

00:42:57,599 --> 00:43:01,280
like i think that's just something that

00:42:59,680 --> 00:43:02,480
as you're building your own applications

00:43:01,280 --> 00:43:03,760
and services and you're running on

00:43:02,480 --> 00:43:05,200
kubernetes like that is a

00:43:03,760 --> 00:43:06,319
that's really good advice because i

00:43:05,200 --> 00:43:07,359
think it's one of those things that you

00:43:06,319 --> 00:43:09,440
don't know until

00:43:07,359 --> 00:43:10,560
it hurts you yeah for a stateless

00:43:09,440 --> 00:43:12,960
application running

00:43:10,560 --> 00:43:14,560
five copies of the same no problem on a

00:43:12,960 --> 00:43:16,079
node is not a big deal at all but for

00:43:14,560 --> 00:43:18,720
stateful applications

00:43:16,079 --> 00:43:19,839
um generally that's not a best practice

00:43:18,720 --> 00:43:20,960
that's not to say that there aren't

00:43:19,839 --> 00:43:22,720
scenarios where we might

00:43:20,960 --> 00:43:24,400
suggest it for that's right for certain

00:43:22,720 --> 00:43:28,319
requirements but um

00:43:24,400 --> 00:43:30,480
in that in those scenarios we have to

00:43:28,319 --> 00:43:32,079
we have to so we didn't cover a whole

00:43:30,480 --> 00:43:32,640
lot of cockroach db architecture stuff

00:43:32,079 --> 00:43:34,800
today but

00:43:32,640 --> 00:43:37,200
when an a node or a pod joins the

00:43:34,800 --> 00:43:37,760
cluster it announces its location and

00:43:37,200 --> 00:43:40,800
it's

00:43:37,760 --> 00:43:42,960
in the topology right right so um

00:43:40,800 --> 00:43:45,200
in a scenario where i'm going to delete

00:43:42,960 --> 00:43:48,400
all of the anti-affinities

00:43:45,200 --> 00:43:51,520
from our stateful set config i need to

00:43:48,400 --> 00:43:54,640
add the node

00:43:51,520 --> 00:43:55,520
um like the the host name of the pod

00:43:54,640 --> 00:43:58,960
host

00:43:55,520 --> 00:44:01,119
as um as kind of a

00:43:58,960 --> 00:44:02,880
part of the hierarchy for the locality

00:44:01,119 --> 00:44:07,520
so that i could so that the database

00:44:02,880 --> 00:44:09,359
knows not to put multiple replicas

00:44:07,520 --> 00:44:10,560
in on the same physical hardware that

00:44:09,359 --> 00:44:11,599
would be the only thing that would have

00:44:10,560 --> 00:44:13,680
to be done there

00:44:11,599 --> 00:44:15,119
um but performance is generally going to

00:44:13,680 --> 00:44:16,960
be better if you just use

00:44:15,119 --> 00:44:20,240
larger cockroach tv pods in that

00:44:16,960 --> 00:44:22,960
scenario okay

00:44:20,240 --> 00:44:24,800
well cool i think we actually hit all of

00:44:22,960 --> 00:44:25,359
the questions i mean jim i'm sorry jim

00:44:24,800 --> 00:44:28,800
too

00:44:25,359 --> 00:44:30,079
i got to call you jim too live buddy um

00:44:28,800 --> 00:44:31,680
you know thank you for jumping in there

00:44:30,079 --> 00:44:32,720
i'd love it i think we should call him

00:44:31,680 --> 00:44:34,800
the hatch

00:44:32,720 --> 00:44:36,240
the voice of the voice of jim came in

00:44:34,800 --> 00:44:37,520
there for a second it was great jim did

00:44:36,240 --> 00:44:38,560
we hit every i think we hit everything

00:44:37,520 --> 00:44:40,720
yeah

00:44:38,560 --> 00:44:41,920
um the few questions left but um i think

00:44:40,720 --> 00:44:45,200
we think we

00:44:41,920 --> 00:44:46,640
addressed most of them yeah yeah

00:44:45,200 --> 00:44:48,079
all right well is it there are any other

00:44:46,640 --> 00:44:50,400
questions out there we're happy to talk

00:44:48,079 --> 00:44:52,640
through um i think this this

00:44:50,400 --> 00:44:53,520
your code the repo it's all available

00:44:52,640 --> 00:44:55,680
right

00:44:53,520 --> 00:44:57,599
keith you've published all this to to

00:44:55,680 --> 00:45:00,640
github

00:44:57,599 --> 00:45:02,160
um there yes um although the not quite

00:45:00,640 --> 00:45:03,599
yet because my notes are still

00:45:02,160 --> 00:45:05,119
it's in github but it's not public yet

00:45:03,599 --> 00:45:05,760
but we'll get it out there in the next

00:45:05,119 --> 00:45:07,280
week or two

00:45:05,760 --> 00:45:09,440
oh there was a question there's a

00:45:07,280 --> 00:45:10,480
question about scaling down cockroachdb

00:45:09,440 --> 00:45:11,920
that i do would like to

00:45:10,480 --> 00:45:13,760
i missed it i'm sorry buddy yeah it's

00:45:11,920 --> 00:45:16,000
brand new um so

00:45:13,760 --> 00:45:17,599
someone was asking about scaling down

00:45:16,000 --> 00:45:19,040
and seeing it as failing nodes and is

00:45:17,599 --> 00:45:21,119
there a way to do it gracefully yes we

00:45:19,040 --> 00:45:24,880
have a node decommissioning process

00:45:21,119 --> 00:45:27,680
right so um it is um

00:45:24,880 --> 00:45:29,760
um this is a pattern that is a little

00:45:27,680 --> 00:45:33,119
bit more challenging to do

00:45:29,760 --> 00:45:36,480
in um using our current config we are

00:45:33,119 --> 00:45:38,800
working on um on an operator

00:45:36,480 --> 00:45:39,760
um because effectively that's a that's a

00:45:38,800 --> 00:45:42,160
like a day two

00:45:39,760 --> 00:45:43,839
operation not a day one operation so

00:45:42,160 --> 00:45:45,040
stateful second fake's great at the day

00:45:43,839 --> 00:45:48,720
one ops

00:45:45,040 --> 00:45:50,079
um that will that will handle gracefully

00:45:48,720 --> 00:45:51,599
scaling down your cluster

00:45:50,079 --> 00:45:53,200
uh in a way that's not going to create

00:45:51,599 --> 00:45:56,160
data unavailability

00:45:53,200 --> 00:45:57,680
um the the other option is to just do

00:45:56,160 --> 00:45:59,920
one node at a time and make sure that

00:45:57,680 --> 00:46:02,960
that under replicated range

00:45:59,920 --> 00:46:05,920
count is at zero before continuing

00:46:02,960 --> 00:46:06,960
to shrink your cluster um that's a bit

00:46:05,920 --> 00:46:09,200
more of kind of a

00:46:06,960 --> 00:46:10,319
hard muscle way of doing it but but

00:46:09,200 --> 00:46:12,000
we'll survive that as well

00:46:10,319 --> 00:46:14,160
you got to be careful you can't just

00:46:12,000 --> 00:46:14,640
take out this is true with any stateful

00:46:14,160 --> 00:46:17,839
service

00:46:14,640 --> 00:46:18,160
any any any i mean again this is a truth

00:46:17,839 --> 00:46:20,160
for

00:46:18,160 --> 00:46:21,680
all things deployed in kubernetes you go

00:46:20,160 --> 00:46:23,200
kill half the thing you're going to have

00:46:21,680 --> 00:46:26,160
some problems

00:46:23,200 --> 00:46:27,280
yeah so so we need to have a quorum of

00:46:26,160 --> 00:46:29,200
um

00:46:27,280 --> 00:46:30,800
of the replicas for each of the ranges

00:46:29,200 --> 00:46:32,480
available for us to be able to answer

00:46:30,800 --> 00:46:35,599
queries against those ranges right

00:46:32,480 --> 00:46:36,880
so so if we have a quorum if we're if we

00:46:35,599 --> 00:46:40,160
have a

00:46:36,880 --> 00:46:41,680
um a raft group of three replicas um we

00:46:40,160 --> 00:46:43,040
need to have two of those replicas

00:46:41,680 --> 00:46:44,000
available to be able to respond to

00:46:43,040 --> 00:46:47,280
queries so

00:46:44,000 --> 00:46:48,560
um that's why we would only recommend

00:46:47,280 --> 00:46:50,480
scaling down one at a time

00:46:48,560 --> 00:46:51,920
if you if all of your tables were a

00:46:50,480 --> 00:46:53,280
replication factor of five you could

00:46:51,920 --> 00:46:54,800
scale down two at a time

00:46:53,280 --> 00:46:56,319
and at seven you could scale down three

00:46:54,800 --> 00:46:58,319
at a time um

00:46:56,319 --> 00:46:59,440
it's just that's just kind of the nature

00:46:58,319 --> 00:47:02,880
of

00:46:59,440 --> 00:47:04,560
of um consensus-based replication

00:47:02,880 --> 00:47:06,079
um that'd be true for any stateful

00:47:04,560 --> 00:47:07,680
application running kubernetes not just

00:47:06,079 --> 00:47:09,359
cockroach tv so keith

00:47:07,680 --> 00:47:10,319
let's shift the topic a little bit and

00:47:09,359 --> 00:47:11,440
there's a couple of questions that are

00:47:10,319 --> 00:47:12,960
coming in kind of

00:47:11,440 --> 00:47:14,160
related to something and this is a day

00:47:12,960 --> 00:47:15,520
two operations thing is what we're

00:47:14,160 --> 00:47:17,200
talking about here how do we scale down

00:47:15,520 --> 00:47:19,839
and these sort of things and

00:47:17,200 --> 00:47:20,559
you know um we get we get asked about

00:47:19,839 --> 00:47:23,520
operators

00:47:20,559 --> 00:47:24,000
fairly often yeah um and i know you do a

00:47:23,520 --> 00:47:26,079
lot of work

00:47:24,000 --> 00:47:27,440
on that what's the current state of our

00:47:26,079 --> 00:47:28,960
operator for cockroach i mean because

00:47:27,440 --> 00:47:30,400
like honestly cockroach didn't need an

00:47:28,960 --> 00:47:32,240
operator to do any of this stuff that we

00:47:30,400 --> 00:47:34,640
were showing today it's like

00:47:32,240 --> 00:47:35,920
literally kind of aligned perfectly with

00:47:34,640 --> 00:47:37,599
this whole

00:47:35,920 --> 00:47:39,359
like the kubernetes architecture but we

00:47:37,599 --> 00:47:40,960
do have an operator so what is our

00:47:39,359 --> 00:47:43,760
operator kind of delivering that

00:47:40,960 --> 00:47:44,640
yeah so our operator is making it um

00:47:43,760 --> 00:47:48,160
easy

00:47:44,640 --> 00:47:51,040
for users to um

00:47:48,160 --> 00:47:52,400
to do common administrative tasks

00:47:51,040 --> 00:47:56,160
against the cluster

00:47:52,400 --> 00:47:58,640
um in a way that's safe

00:47:56,160 --> 00:47:59,599
right so all of all of the things that

00:47:58,640 --> 00:48:02,000
the operator's doing

00:47:59,599 --> 00:48:02,960
you could already do with the existing

00:48:02,000 --> 00:48:06,000
staple set

00:48:02,960 --> 00:48:08,319
right but it might involve you

00:48:06,000 --> 00:48:09,920
updating ammo files and mounting file

00:48:08,319 --> 00:48:12,000
systems and

00:48:09,920 --> 00:48:13,200
you know like so a rolling upgrade for

00:48:12,000 --> 00:48:14,559
example right

00:48:13,200 --> 00:48:16,559
we have a manual process for doing a

00:48:14,559 --> 00:48:19,440
rolling upgrade in kubernetes today it's

00:48:16,559 --> 00:48:20,319
pretty straightforward um but the

00:48:19,440 --> 00:48:23,920
operator

00:48:20,319 --> 00:48:26,079
is going to manage it in a way where um

00:48:23,920 --> 00:48:27,200
where we're going to do things like

00:48:26,079 --> 00:48:29,359
health check

00:48:27,200 --> 00:48:31,440
fully health check the cluster not just

00:48:29,359 --> 00:48:33,680
the pods before rolling

00:48:31,440 --> 00:48:34,559
um the next pod we're also going to do

00:48:33,680 --> 00:48:37,119
things like

00:48:34,559 --> 00:48:38,880
make sure that when you upgrade you're

00:48:37,119 --> 00:48:40,160
upgrading to a valid version from the

00:48:38,880 --> 00:48:41,839
upgrade that you're going to and you

00:48:40,160 --> 00:48:44,079
didn't skip a feature release

00:48:41,839 --> 00:48:45,599
it's it's not um it's not that you

00:48:44,079 --> 00:48:46,960
couldn't already do all this stuff

00:48:45,599 --> 00:48:48,079
because we couldn't already do all this

00:48:46,960 --> 00:48:49,119
stuff we wouldn't be able to build an

00:48:48,079 --> 00:48:54,160
operator to do it

00:48:49,119 --> 00:48:54,160
it's a matter of making it so that um

00:48:54,400 --> 00:48:57,680
you can't accidentally fat finger a

00:48:56,960 --> 00:49:00,400
config

00:48:57,680 --> 00:49:02,319
and then break your cluster right so

00:49:00,400 --> 00:49:03,200
that's those types of operations that

00:49:02,319 --> 00:49:04,800
we're working on

00:49:03,200 --> 00:49:06,240
i like to think of you as making things

00:49:04,800 --> 00:49:08,480
more graceful

00:49:06,240 --> 00:49:10,480
well i i i see the operator you know the

00:49:08,480 --> 00:49:13,599
the club like the the operations

00:49:10,480 --> 00:49:16,000
i i see the operator as a way for

00:49:13,599 --> 00:49:18,079
getting how to operate cockroach cb on

00:49:16,000 --> 00:49:20,720
kubernetes out of my head and

00:49:18,079 --> 00:49:21,359
into everyone's hands exactly that's the

00:49:20,720 --> 00:49:22,559
that's the

00:49:21,359 --> 00:49:24,400
the reason why i've been spending so

00:49:22,559 --> 00:49:26,160
much time and by the way that was that

00:49:24,400 --> 00:49:27,200
that was the original intent with the

00:49:26,160 --> 00:49:29,280
operator pattern

00:49:27,200 --> 00:49:31,040
um way back you know years ago when it

00:49:29,280 --> 00:49:32,480
started was how do you take that

00:49:31,040 --> 00:49:34,319
the knowledge and expertise out of

00:49:32,480 --> 00:49:35,839
somebody who's an sr are you doing these

00:49:34,319 --> 00:49:37,680
things and create some sort of framework

00:49:35,839 --> 00:49:39,040
within kubernetes it's exactly what it

00:49:37,680 --> 00:49:40,640
is so

00:49:39,040 --> 00:49:42,319
um there was a question on if we're

00:49:40,640 --> 00:49:44,720
going to support my sequel

00:49:42,319 --> 00:49:46,000
alongside like we're wire compatible

00:49:44,720 --> 00:49:47,440
with the postgresql we've gone a long

00:49:46,000 --> 00:49:49,200
way to do that there's no plan

00:49:47,440 --> 00:49:50,960
immediately to do anything that is like

00:49:49,200 --> 00:49:52,400
my sequel um

00:49:50,960 --> 00:49:54,079
you know we have to make a choice it's a

00:49:52,400 --> 00:49:55,280
lot of work by the way so well so the

00:49:54,079 --> 00:49:56,800
other thing is is that we're

00:49:55,280 --> 00:49:57,920
serializably isolated we didn't really

00:49:56,800 --> 00:49:59,200
talk about database at all we're

00:49:57,920 --> 00:50:02,079
serializably isolated

00:49:59,200 --> 00:50:03,359
um which is um postgres can run in

00:50:02,079 --> 00:50:07,040
serializably isolated

00:50:03,359 --> 00:50:09,280
exactly my sequel can't and so um

00:50:07,040 --> 00:50:10,079
it would be much harder for us to

00:50:09,280 --> 00:50:13,920
provide

00:50:10,079 --> 00:50:14,559
a mysql compliant endpoint because

00:50:13,920 --> 00:50:16,720
there's no

00:50:14,559 --> 00:50:18,480
way for us to signal to the client that

00:50:16,720 --> 00:50:19,119
we're running in serializably isolated

00:50:18,480 --> 00:50:21,280
mode

00:50:19,119 --> 00:50:22,720
whereas with postgres we can do that

00:50:21,280 --> 00:50:24,400
yeah learns i learned something new

00:50:22,720 --> 00:50:25,839
thank you keith

00:50:24,400 --> 00:50:27,839
i love when i learn things new

00:50:25,839 --> 00:50:31,440
especially from you

00:50:27,839 --> 00:50:33,599
which is all the time or never or never

00:50:31,440 --> 00:50:35,359
i know you know 100 of the time i'm

00:50:33,599 --> 00:50:38,000
making it up 90 of the time

00:50:35,359 --> 00:50:40,400
right i i don't even know the quote i

00:50:38,000 --> 00:50:41,920
can't even start so well that's i mean

00:50:40,400 --> 00:50:43,920
you know i could let me go back to the

00:50:41,920 --> 00:50:44,880
slides we'll do we'll stay on screen

00:50:43,920 --> 00:50:49,040
here keith or

00:50:44,880 --> 00:50:53,040
you know we can go back to this is it so

00:50:49,040 --> 00:50:55,040
where was i so i i just realized 50

00:50:53,040 --> 00:50:56,720
52 minutes into the webinar that because

00:50:55,040 --> 00:50:58,559
i'm using the fancy camera

00:50:56,720 --> 00:51:00,240
um people can see that my headphones

00:50:58,559 --> 00:51:01,520
have messed up my hair for the day which

00:51:00,240 --> 00:51:03,520
i think is awesome

00:51:01,520 --> 00:51:05,119
yeah you know the problem with a fancy

00:51:03,520 --> 00:51:08,400
camera is like i have to shave

00:51:05,119 --> 00:51:09,680
like and and you know anyway so we are

00:51:08,400 --> 00:51:11,520
a database that was designed with

00:51:09,680 --> 00:51:12,720
kubernetes in mind y'all i hope we

00:51:11,520 --> 00:51:14,160
showed that to you today

00:51:12,720 --> 00:51:15,920
uh but it's you know i think there's a

00:51:14,160 --> 00:51:17,040
lot of principles that we've done in in

00:51:15,920 --> 00:51:18,400
in our code base that might be

00:51:17,040 --> 00:51:19,760
applicable to what you're doing in in

00:51:18,400 --> 00:51:20,800
your code base or what you're actually

00:51:19,760 --> 00:51:22,400
trying to implement

00:51:20,800 --> 00:51:25,119
i like to think of cockroach in our code

00:51:22,400 --> 00:51:26,800
base as a a little bit of a course in

00:51:25,119 --> 00:51:28,160
distributed systems and some of the cool

00:51:26,800 --> 00:51:30,240
things that we have done

00:51:28,160 --> 00:51:31,599
um keith talked about you know deploying

00:51:30,240 --> 00:51:32,880
and and making things a little bit

00:51:31,599 --> 00:51:34,880
easier to do

00:51:32,880 --> 00:51:36,559
we do have a kubernetes operator that's

00:51:34,880 --> 00:51:39,200
absolutely available to all

00:51:36,559 --> 00:51:40,480
um lots of companies using us today uh

00:51:39,200 --> 00:51:42,000
to do these sort of things

00:51:40,480 --> 00:51:44,079
and keith you know what's the percentage

00:51:42,000 --> 00:51:45,040
of of deployments that are on kubernetes

00:51:44,079 --> 00:51:47,680
that that use him

00:51:45,040 --> 00:51:48,800
you work with a lot of customers yeah so

00:51:47,680 --> 00:51:50,720
we know

00:51:48,800 --> 00:51:53,359
of the of the clusters that have

00:51:50,720 --> 00:51:55,520
telemetry enabled we know that roughly

00:51:53,359 --> 00:51:57,280
40 of them are running on kubernetes and

00:51:55,520 --> 00:52:00,559
another 30 of them

00:51:57,280 --> 00:52:02,079
are running in containers but we don't

00:52:00,559 --> 00:52:05,359
know for sure if they're running

00:52:02,079 --> 00:52:08,559
kubernetes so anywhere between 50

00:52:05,359 --> 00:52:10,319
and 70 of cockroach db

00:52:08,559 --> 00:52:12,480
environments are running kubernetes

00:52:10,319 --> 00:52:14,000
today these are long-running clusters so

00:52:12,480 --> 00:52:15,920
we define a long-running cluster as one

00:52:14,000 --> 00:52:16,559
that's been up for more than seven days

00:52:15,920 --> 00:52:18,559
so

00:52:16,559 --> 00:52:19,760
not not something you run on your laptop

00:52:18,559 --> 00:52:20,480
and shutting down at the end of the

00:52:19,760 --> 00:52:22,400
night

00:52:20,480 --> 00:52:23,599
um also a hundred percent of our

00:52:22,400 --> 00:52:26,480
cockroach cloud

00:52:23,599 --> 00:52:28,800
right busters are running on kubernetes

00:52:26,480 --> 00:52:29,440
so there's a lot of internal dog fooding

00:52:28,800 --> 00:52:32,800
around

00:52:29,440 --> 00:52:34,960
our um our use of kubernetes yeah

00:52:32,800 --> 00:52:36,960
and so yeah so you could spin up start

00:52:34,960 --> 00:52:37,760
running cockroach cloud today um off our

00:52:36,960 --> 00:52:40,400
website

00:52:37,760 --> 00:52:41,760
um it's all available any of this stuff

00:52:40,400 --> 00:52:43,359
that we went through is available on our

00:52:41,760 --> 00:52:45,119
website but i think most importantly and

00:52:43,359 --> 00:52:46,160
keith oh my god how do we go an entire

00:52:45,119 --> 00:52:49,040
session with an out

00:52:46,160 --> 00:52:50,880
saying nice things about docs um i can't

00:52:49,040 --> 00:52:52,240
believe we made it this long

00:52:50,880 --> 00:52:54,079
because there weren't any there weren't

00:52:52,240 --> 00:52:55,200
any docs on how to get cockroaches to be

00:52:54,079 --> 00:52:57,359
running out of scuppers

00:52:55,200 --> 00:52:59,119
well that's true too there's this now

00:52:57,359 --> 00:52:59,599
but our docs does do a really good job

00:52:59,119 --> 00:53:01,280
of

00:52:59,599 --> 00:53:02,880
if you want to like dive into how we do

00:53:01,280 --> 00:53:05,520
our transactional layer you know there's

00:53:02,880 --> 00:53:06,960
questions around like how we do

00:53:05,520 --> 00:53:08,240
typically in our docs it's there if it's

00:53:06,960 --> 00:53:09,920
not there you know we do have a

00:53:08,240 --> 00:53:10,480
community slack channel where people are

00:53:09,920 --> 00:53:13,440
always

00:53:10,480 --> 00:53:15,280
on and always answering so um if there

00:53:13,440 --> 00:53:16,480
are any follow-up questions or or other

00:53:15,280 --> 00:53:18,079
stuff i mean we're pretty

00:53:16,480 --> 00:53:19,440
open and transparent about everything

00:53:18,079 --> 00:53:21,520
that's going on here at cockroach and

00:53:19,440 --> 00:53:23,359
we're happy to engage and

00:53:21,520 --> 00:53:25,200
um we would love for anybody to be part

00:53:23,359 --> 00:53:27,920
of this community um and to

00:53:25,200 --> 00:53:29,200
and to you know tell us what they're

00:53:27,920 --> 00:53:30,800
doing or if they have any questions

00:53:29,200 --> 00:53:33,920
about how this stuff works

00:53:30,800 --> 00:53:35,680
kubernetes and database or without and

00:53:33,920 --> 00:53:37,839
we're we're happy to engage and you know

00:53:35,680 --> 00:53:41,040
proud member of this community so

00:53:37,839 --> 00:53:44,079
um keith thank you as well thank you

00:53:41,040 --> 00:53:46,319
yeah um that was a it's a wonderful

00:53:44,079 --> 00:53:47,280
demo and i i just i really really like

00:53:46,319 --> 00:53:49,119
it so

00:53:47,280 --> 00:53:50,640
um i hope this was valuable to everybody

00:53:49,119 --> 00:53:51,680
today um you know

00:53:50,640 --> 00:53:53,359
we tried to make it a little

00:53:51,680 --> 00:53:54,079
entertaining i love the cube doom thing

00:53:53,359 --> 00:53:55,280
so

00:53:54,079 --> 00:53:56,720
but this is something i've never seen

00:53:55,280 --> 00:53:58,400
before i've never seen an application

00:53:56,720 --> 00:54:00,000
run across multiple kubernetes clusters

00:53:58,400 --> 00:54:02,240
until i saw this demo

00:54:00,000 --> 00:54:04,319
and uh kudos buddy i'm it's super

00:54:02,240 --> 00:54:06,319
impressive thank you thanks

00:54:04,319 --> 00:54:09,920
all right so marisa and linus foundation

00:54:06,319 --> 00:54:11,599
we are done thank you very much as well

00:54:09,920 --> 00:54:13,040
awesome thank you guys so much for that

00:54:11,599 --> 00:54:16,480
wonderful presentation

00:54:13,040 --> 00:54:19,040
keith jim and jim as well that's jim

00:54:16,480 --> 00:54:19,760
and thank you yeah jim too and everybody

00:54:19,040 --> 00:54:21,599
of course for

00:54:19,760 --> 00:54:23,599
uh participating and joining us here

00:54:21,599 --> 00:54:24,079
today as a quick reminder the recording

00:54:23,599 --> 00:54:26,000
will be

00:54:24,079 --> 00:54:27,839
posted to the linux foundation's youtube

00:54:26,000 --> 00:54:28,800
page should be up by the end of the day

00:54:27,839 --> 00:54:30,400
today

00:54:28,800 --> 00:54:32,559
okay uh we hope to see you guys back

00:54:30,400 --> 00:54:35,520
here again for some future webinars

00:54:32,559 --> 00:54:37,839
okay have a good one thank you bye thank

00:54:35,520 --> 00:54:37,839

YouTube URL: https://www.youtube.com/watch?v=zaE1VXRREVg


