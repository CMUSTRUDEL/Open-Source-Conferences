Title: Webinar: Deploying a Database on (not next to) Kubernetes, sponsored by Cockroach Labs
Publication date: 2020-12-09
Playlist: LF Live Webinars
Description: 
	Kubernetes is a paradigm shift. It requires a wholly new way of thinking about our applications as services. But what about the database? In this webinar, we will take a deep dive into the architecture of a distributed database, discuss some of the challenges, and more.

Speakers:
Jim Walker, VP, Product Marketing, and Keith McClellan, Director of Partner Solutions Engineering, Cockroach Labs
Captions: 
	00:00:00,080 --> 00:00:05,839
is of cockroach do this so you know you

00:00:02,399 --> 00:00:07,520
have one single database and you simply

00:00:05,839 --> 00:00:09,519
keep adding nodes to it and you're

00:00:07,520 --> 00:00:10,719
expanding it not just from a volume

00:00:09,519 --> 00:00:12,799
point of view from

00:00:10,719 --> 00:00:14,639
the size of the database itself but from

00:00:12,799 --> 00:00:16,240
transactional volume as well

00:00:14,639 --> 00:00:17,680
you can hit any one of these nodes and

00:00:16,240 --> 00:00:19,520
it's going to act like a single logical

00:00:17,680 --> 00:00:20,800
database right

00:00:19,520 --> 00:00:22,080
interesting enough we can actually

00:00:20,800 --> 00:00:23,680
deploy these things in multiple

00:00:22,080 --> 00:00:24,480
different regions as well or across

00:00:23,680 --> 00:00:26,160
multiple different

00:00:24,480 --> 00:00:28,400
clusters which is actually really cool

00:00:26,160 --> 00:00:29,279
so what looks like one single logical

00:00:28,400 --> 00:00:30,720
database actually

00:00:29,279 --> 00:00:32,079
underneath the covers is implemented

00:00:30,720 --> 00:00:33,280
across multiple different regions so i

00:00:32,079 --> 00:00:36,079
can survive things

00:00:33,280 --> 00:00:37,840
and whatnot right so number one um a

00:00:36,079 --> 00:00:39,280
couple of cool things about

00:00:37,840 --> 00:00:41,280
cockroach i'm going to go five quick

00:00:39,280 --> 00:00:41,760
things and then keith is going to go and

00:00:41,280 --> 00:00:44,800
do

00:00:41,760 --> 00:00:46,719
operator and demo stuff number one we uh

00:00:44,800 --> 00:00:48,239
this is standard sql we aren't requiring

00:00:46,719 --> 00:00:50,640
you to learn anything else that we are

00:00:48,239 --> 00:00:52,640
wire compatible with postgres so

00:00:50,640 --> 00:00:54,239
if you know sql you can interact with

00:00:52,640 --> 00:00:56,719
cockroach tv

00:00:54,239 --> 00:00:57,360
number two you could ask any node in a

00:00:56,719 --> 00:00:59,760
cluster

00:00:57,360 --> 00:01:00,879
for data and it will find that that data

00:00:59,760 --> 00:01:02,480
throughout the cluster

00:01:00,879 --> 00:01:04,000
um every node is a single consistent

00:01:02,480 --> 00:01:04,960
gateway to the entirety of the database

00:01:04,000 --> 00:01:06,960
this is what allows us to be

00:01:04,960 --> 00:01:08,479
kind of one single logical database i

00:01:06,960 --> 00:01:09,840
could ask any any any node i'm going to

00:01:08,479 --> 00:01:12,400
find that data

00:01:09,840 --> 00:01:14,400
number three you know in this in this

00:01:12,400 --> 00:01:16,640
first question i well i was asking

00:01:14,400 --> 00:01:18,080
uh for this record and that was in

00:01:16,640 --> 00:01:19,360
region one it's got to go all the way

00:01:18,080 --> 00:01:21,759
over to europe and back

00:01:19,360 --> 00:01:24,080
that's not efficient how do we geo

00:01:21,759 --> 00:01:26,240
locate data near users to reduce

00:01:24,080 --> 00:01:27,200
read write latencies and how can the

00:01:26,240 --> 00:01:29,439
database do that

00:01:27,200 --> 00:01:31,439
cockroach has a very unique capability

00:01:29,439 --> 00:01:32,720
called geopartitioning that allows us to

00:01:31,439 --> 00:01:35,439
actually do that so

00:01:32,720 --> 00:01:36,479
now when this user spencer campbell asks

00:01:35,439 --> 00:01:38,720
for his data

00:01:36,479 --> 00:01:40,320
well it's actually located in europe say

00:01:38,720 --> 00:01:42,000
he's in he's in europe right so we can

00:01:40,320 --> 00:01:43,759
actually geolocate data and

00:01:42,000 --> 00:01:45,600
geopartition data it's actually pretty

00:01:43,759 --> 00:01:48,000
unique and allows us to deal with these

00:01:45,600 --> 00:01:50,479
latency issues at global scale

00:01:48,000 --> 00:01:52,079
finally i talked about scale simply spin

00:01:50,479 --> 00:01:53,520
up a node point it at the cluster and

00:01:52,079 --> 00:01:56,399
what happens is cockroach

00:01:53,520 --> 00:01:56,880
will consume that new node and balance

00:01:56,399 --> 00:01:58,399
all of

00:01:56,880 --> 00:02:00,399
the data throughout the cluster so that

00:01:58,399 --> 00:02:01,439
we can balance for volume of

00:02:00,399 --> 00:02:03,600
transactions or

00:02:01,439 --> 00:02:05,280
our volume of data whatever it is there

00:02:03,600 --> 00:02:07,600
is no manual sharding

00:02:05,280 --> 00:02:09,759
involved and then finally when a node

00:02:07,600 --> 00:02:11,360
fails or even a whole region

00:02:09,759 --> 00:02:13,280
um what we've done is we've actually

00:02:11,360 --> 00:02:15,280
created replicas of data so

00:02:13,280 --> 00:02:16,400
that the data is still existed you still

00:02:15,280 --> 00:02:18,720
have access to this

00:02:16,400 --> 00:02:20,560
the the cluster itself would actually

00:02:18,720 --> 00:02:22,000
remedy this situation actually deal with

00:02:20,560 --> 00:02:23,599
these sort of things in terms of

00:02:22,000 --> 00:02:25,599
create you know a third copy of that

00:02:23,599 --> 00:02:26,720
data somewhere else but we can actually

00:02:25,599 --> 00:02:29,120
you know survive

00:02:26,720 --> 00:02:30,879
failure of a node or complete region

00:02:29,120 --> 00:02:32,080
right so

00:02:30,879 --> 00:02:34,640
that's the high level overview of

00:02:32,080 --> 00:02:36,480
cockroachdb in just a couple slides and

00:02:34,640 --> 00:02:38,239
oh gosh i did it man we're 10 minutes in

00:02:36,480 --> 00:02:39,040
keith i did it in about eight or nine

00:02:38,239 --> 00:02:42,160
minutes so

00:02:39,040 --> 00:02:43,920
um but i wanted to get into a demo of

00:02:42,160 --> 00:02:45,120
cockroachdb and getting started so keith

00:02:43,920 --> 00:02:46,160
do you want to take over here yeah

00:02:45,120 --> 00:02:48,160
absolutely so

00:02:46,160 --> 00:02:49,280
so we're gonna demo can you hey keith

00:02:48,160 --> 00:02:50,959
really quickly just let me

00:02:49,280 --> 00:02:53,200
just let me remind everybody please do

00:02:50,959 --> 00:02:55,040
ask questions in the qa i will be

00:02:53,200 --> 00:02:56,640
monitoring the qa for questions along

00:02:55,040 --> 00:02:57,440
the way and dump those into keith i just

00:02:56,640 --> 00:02:59,760
wanted to remind you

00:02:57,440 --> 00:03:01,360
sorry keep going no problem so i'm going

00:02:59,760 --> 00:03:02,640
to demonstrate two things today i'm

00:03:01,360 --> 00:03:05,519
going to demonstrate using

00:03:02,640 --> 00:03:06,159
our operator to set up a cockroachdb

00:03:05,519 --> 00:03:09,519
cluster

00:03:06,159 --> 00:03:11,280
in a single region in this case it's

00:03:09,519 --> 00:03:12,959
going to be a

00:03:11,280 --> 00:03:15,280
an open shift environment that's running

00:03:12,959 --> 00:03:16,879
on my laptop and then later on we're

00:03:15,280 --> 00:03:19,519
going to

00:03:16,879 --> 00:03:20,959
be working on a distributed environment

00:03:19,519 --> 00:03:24,000
that i set up previously

00:03:20,959 --> 00:03:24,480
that's across three regions in in google

00:03:24,000 --> 00:03:28,239
cloud

00:03:24,480 --> 00:03:30,879
so i think i'm running that on gke

00:03:28,239 --> 00:03:34,080
so um i'm going to go ahead and kind of

00:03:30,879 --> 00:03:37,440
share the the first bit

00:03:34,080 --> 00:03:40,560
so hopefully you can uh see my my screen

00:03:37,440 --> 00:03:41,840
jim so what what we have here is

00:03:40,560 --> 00:03:43,200
openshift you're not familiar with

00:03:41,840 --> 00:03:46,799
openshift it's

00:03:43,200 --> 00:03:49,120
red hat's kubernetes distribution right

00:03:46,799 --> 00:03:51,040
we have published the cockroachdb

00:03:49,120 --> 00:03:53,439
operator in their operator hub which

00:03:51,040 --> 00:03:56,879
makes it super easy to get started

00:03:53,439 --> 00:03:57,840
with cockroachdb so um all i need to do

00:03:56,879 --> 00:03:59,439
is

00:03:57,840 --> 00:04:01,840
search for the operator we got the

00:03:59,439 --> 00:04:03,920
cockroach operator right here

00:04:01,840 --> 00:04:06,000
um i'm going to click on the install

00:04:03,920 --> 00:04:07,840
button i'm going to install it into my

00:04:06,000 --> 00:04:10,319
cockroachdb namespace that i created

00:04:07,840 --> 00:04:11,519
a couple of minutes ago and and that's

00:04:10,319 --> 00:04:14,640
going to go ahead and actually

00:04:11,519 --> 00:04:16,239
install the operator on the cluster now

00:04:14,640 --> 00:04:17,600
for while i'm waiting for this to

00:04:16,239 --> 00:04:18,799
install i'm going to talk a little bit

00:04:17,600 --> 00:04:22,960
about what an operator

00:04:18,799 --> 00:04:25,520
is so an operator is a

00:04:22,960 --> 00:04:27,040
cust is a manager for a custom resource

00:04:25,520 --> 00:04:30,080
in kubernetes

00:04:27,040 --> 00:04:33,199
so we define a custom resource

00:04:30,080 --> 00:04:36,479
called a crdb cluster and then

00:04:33,199 --> 00:04:39,840
the operator is is a pod actually

00:04:36,479 --> 00:04:43,600
that that we assign state to

00:04:39,840 --> 00:04:46,160
we we publish custom resources too

00:04:43,600 --> 00:04:47,199
and it configures and maintains the

00:04:46,160 --> 00:04:50,800
database for us

00:04:47,199 --> 00:04:52,080
so it does things like um easy rolling

00:04:50,800 --> 00:04:54,720
upgrades

00:04:52,080 --> 00:04:55,759
um it's going to make it easier in the

00:04:54,720 --> 00:04:58,960
future to

00:04:55,759 --> 00:05:00,479
to set up backups and restores and

00:04:58,960 --> 00:05:02,240
do auto scaling and all that kind of

00:05:00,479 --> 00:05:06,479
stuff

00:05:02,240 --> 00:05:09,520
right now it's it's doing a full install

00:05:06,479 --> 00:05:13,440
and and upgrades for us

00:05:09,520 --> 00:05:14,400
which is um not super hard in kubernetes

00:05:13,440 --> 00:05:16,320
we got

00:05:14,400 --> 00:05:17,759
we got away with not having an operator

00:05:16,320 --> 00:05:19,759
for a long time

00:05:17,759 --> 00:05:20,960
but what it's really designed to do is

00:05:19,759 --> 00:05:22,080
make some of those kind of day two

00:05:20,960 --> 00:05:24,639
operations like

00:05:22,080 --> 00:05:25,840
post install operations easy to manage

00:05:24,639 --> 00:05:28,080
in production

00:05:25,840 --> 00:05:30,800
so so we have this operator um it

00:05:28,080 --> 00:05:33,120
provides a cockroachdb cluster api

00:05:30,800 --> 00:05:33,840
i can kind of can click on the link to

00:05:33,120 --> 00:05:38,080
create

00:05:33,840 --> 00:05:40,320
a um create a cluster

00:05:38,080 --> 00:05:42,320
and the back end it's creating a cr

00:05:40,320 --> 00:05:45,680
we're going to be

00:05:42,320 --> 00:05:49,360
we're going to name the cr crdb tls

00:05:45,680 --> 00:05:51,600
example it's going to have tls enabled

00:05:49,360 --> 00:05:53,840
so it's going to be an encrypted

00:05:51,600 --> 00:05:56,000
it's gonna be set up with encryption um

00:05:53,840 --> 00:05:58,400
it's gonna spin up three nodes

00:05:56,000 --> 00:05:59,600
in this open shift environment and each

00:05:58,400 --> 00:06:01,759
of them are gonna have

00:05:59,600 --> 00:06:03,919
ten gigs of storage so we're gonna go

00:06:01,759 --> 00:06:07,520
ahead and create this real quick

00:06:03,919 --> 00:06:10,880
um and over the next 90 seconds or so

00:06:07,520 --> 00:06:13,759
um it's going to

00:06:10,880 --> 00:06:15,039
um it's going to install the database

00:06:13,759 --> 00:06:18,160
for us

00:06:15,039 --> 00:06:20,319
so that's it that's all you need to do

00:06:18,160 --> 00:06:22,720
to get cockroachdb up and running if

00:06:20,319 --> 00:06:24,800
you're running not in openshift

00:06:22,720 --> 00:06:26,639
um we have published instructions on our

00:06:24,800 --> 00:06:28,479
website on how to use the operator

00:06:26,639 --> 00:06:29,840
we also have a published helm chart that

00:06:28,479 --> 00:06:31,440
you can use for

00:06:29,840 --> 00:06:32,880
you know that's more for test and dev

00:06:31,440 --> 00:06:36,080
type environments

00:06:32,880 --> 00:06:37,680
also super easy we also for custom

00:06:36,080 --> 00:06:38,880
deployments which is

00:06:37,680 --> 00:06:42,319
something we're going to talk about a

00:06:38,880 --> 00:06:44,960
little bit later we do also publish

00:06:42,319 --> 00:06:46,160
stateful set configurations for

00:06:44,960 --> 00:06:48,720
publishing

00:06:46,160 --> 00:06:49,520
for installing cockroachdb manually

00:06:48,720 --> 00:06:52,479
against

00:06:49,520 --> 00:06:53,440
kubernetes um right now because the

00:06:52,479 --> 00:06:56,560
operator

00:06:53,440 --> 00:06:59,840
is currently um single region

00:06:56,560 --> 00:07:00,479
is a single region operator um you would

00:06:59,840 --> 00:07:02,319
still use

00:07:00,479 --> 00:07:04,319
the stateful sets for deploying across

00:07:02,319 --> 00:07:05,919
multiple

00:07:04,319 --> 00:07:08,800
data centers which i which i've done

00:07:05,919 --> 00:07:11,680
here for the later demo

00:07:08,800 --> 00:07:13,360
let's see if it's created my resources

00:07:11,680 --> 00:07:17,039
yet it has

00:07:13,360 --> 00:07:20,160
so i have um it's starting to create my

00:07:17,039 --> 00:07:22,400
my database pods so um

00:07:20,160 --> 00:07:23,599
we'll go ahead and let it finish

00:07:22,400 --> 00:07:24,880
starting up we're gonna

00:07:23,599 --> 00:07:27,280
we have three more that are gonna need

00:07:24,880 --> 00:07:27,280
to start

00:07:27,680 --> 00:07:31,199
you can see in the logs that it's

00:07:29,120 --> 00:07:33,840
already kind of coming up and

00:07:31,199 --> 00:07:33,840
and going

00:07:39,919 --> 00:07:46,879
um let's go ahead and see

00:07:42,960 --> 00:07:49,599
if we have

00:07:46,879 --> 00:07:51,599
where almost the database is almost live

00:07:49,599 --> 00:07:54,319
and healthy we got just one more pod

00:07:51,599 --> 00:07:54,319
that needs to come up

00:07:54,800 --> 00:07:58,319
hey keith really quickly while we're

00:07:56,160 --> 00:08:00,720
waiting for that to come up um you know

00:07:58,319 --> 00:08:02,879
we're showing the operator via you know

00:08:00,720 --> 00:08:04,240
you know open shift in the marketplace

00:08:02,879 --> 00:08:06,319
um it's also

00:08:04,240 --> 00:08:08,639
available just raw in our get rid of in

00:08:06,319 --> 00:08:11,520
our git repo as well right

00:08:08,639 --> 00:08:13,520
yes absolutely so we have a this is open

00:08:11,520 --> 00:08:16,240
source so this is published

00:08:13,520 --> 00:08:17,599
um hold on a second i'm going to log in

00:08:16,240 --> 00:08:18,879
into the database and create a quick

00:08:17,599 --> 00:08:20,319
user and then we're going to log into

00:08:18,879 --> 00:08:23,680
the admin giveaway to prove i didn't

00:08:20,319 --> 00:08:23,680
just like kind of fake it so

00:08:24,840 --> 00:08:27,840
okay

00:08:34,080 --> 00:08:39,200
i love watching your type buddy this is

00:08:36,560 --> 00:08:44,000
the only part i couldn't scratch

00:08:39,200 --> 00:08:44,000
i know here we go sql shell

00:08:51,600 --> 00:08:57,440
all right so um

00:08:55,040 --> 00:08:58,080
it's pretty super standard sql so i'm

00:08:57,440 --> 00:09:01,440
gonna go ahead

00:08:58,080 --> 00:09:03,920
and do um localhost

00:09:01,440 --> 00:09:05,519
because i am i did a quick port forward

00:09:03,920 --> 00:09:08,720
it logs me in

00:09:05,519 --> 00:09:10,160
um i have three nodes that are running

00:09:08,720 --> 00:09:12,399
um you can see that all these are

00:09:10,160 --> 00:09:13,519
running uh kind of locally on my laptop

00:09:12,399 --> 00:09:16,320
right now that's right

00:09:13,519 --> 00:09:17,360
um and keith you did localhost into one

00:09:16,320 --> 00:09:19,920
of the pods right

00:09:17,360 --> 00:09:21,120
one of the instances of cockroach right

00:09:19,920 --> 00:09:22,480
it could have been any one of those

00:09:21,120 --> 00:09:23,839
instances correct and then you would get

00:09:22,480 --> 00:09:27,040
the db console

00:09:23,839 --> 00:09:28,480
that that is absolutely correct yeah

00:09:27,040 --> 00:09:30,399
yeah we don't implement like a separate

00:09:28,480 --> 00:09:31,760
node to do the console and then database

00:09:30,399 --> 00:09:34,480
it's everything is

00:09:31,760 --> 00:09:35,839
like a distributed system single binary

00:09:34,480 --> 00:09:37,120
every single node implements the same

00:09:35,839 --> 00:09:40,399
binary right so

00:09:37,120 --> 00:09:43,120
right everything is a single binary

00:09:40,399 --> 00:09:44,800
um which makes it really easy to kind of

00:09:43,120 --> 00:09:48,160
work with this stuff which is awesome

00:09:44,800 --> 00:09:49,600
yeah and easy to scale and aligned with

00:09:48,160 --> 00:09:51,360
the core principles of kind of

00:09:49,600 --> 00:09:53,440
distributed systems and how these things

00:09:51,360 --> 00:09:55,360
work right so

00:09:53,440 --> 00:09:56,959
um real quickly there was a question

00:09:55,360 --> 00:09:58,080
about rancher as well you know could we

00:09:56,959 --> 00:09:59,839
use this with rancher i don't think

00:09:58,080 --> 00:10:00,720
we're in the rancher marketplace yet

00:09:59,839 --> 00:10:02,160
right keith but

00:10:00,720 --> 00:10:04,399
i mean the operator would work with the

00:10:02,160 --> 00:10:05,920
instance i mean yeah so um

00:10:04,399 --> 00:10:08,480
so we do work we work with any

00:10:05,920 --> 00:10:10,880
kubernetes distribution um

00:10:08,480 --> 00:10:13,760
you know we we're not necessarily listed

00:10:10,880 --> 00:10:16,000
in every single marketplace but our

00:10:13,760 --> 00:10:17,600
generic kubernetes instructions um i

00:10:16,000 --> 00:10:20,320
have yet to find a

00:10:17,600 --> 00:10:21,600
kubernetes distribution that um that

00:10:20,320 --> 00:10:25,120
didn't work

00:10:21,600 --> 00:10:27,920
um right so so really it's a matter of

00:10:25,120 --> 00:10:30,640
of kind of just following the

00:10:27,920 --> 00:10:33,760
instructions in our documentation right

00:10:30,640 --> 00:10:34,399
yeah exactly so um but there's another

00:10:33,760 --> 00:10:37,040
question too

00:10:34,399 --> 00:10:38,560
we you know i think we're in this

00:10:37,040 --> 00:10:39,440
install we're definitely using stateful

00:10:38,560 --> 00:10:41,760
sets

00:10:39,440 --> 00:10:43,600
um are we i mean is there an option to

00:10:41,760 --> 00:10:45,360
do this a different way keith or is that

00:10:43,600 --> 00:10:46,160
just kind of like the standard way of

00:10:45,360 --> 00:10:49,200
doing things

00:10:46,160 --> 00:10:51,200
um so stateful sets

00:10:49,200 --> 00:10:53,279
are the the standard way of doing things

00:10:51,200 --> 00:10:56,640
we could theoretically

00:10:53,279 --> 00:10:59,680
um use something like

00:10:56,640 --> 00:11:00,000
um demon sets and there's certain use

00:10:59,680 --> 00:11:03,600
cases

00:11:00,000 --> 00:11:05,279
for demon sets um but generally speaking

00:11:03,600 --> 00:11:07,120
demon sets are more appropriate for

00:11:05,279 --> 00:11:08,079
things like uh log stash agent or

00:11:07,120 --> 00:11:10,399
whatnot where

00:11:08,079 --> 00:11:11,200
you want it to run in every single node

00:11:10,399 --> 00:11:13,839
whereas for

00:11:11,200 --> 00:11:14,880
us we would have to use taints in the

00:11:13,839 --> 00:11:16,560
background to

00:11:14,880 --> 00:11:18,959
make sure that the database only landed

00:11:16,560 --> 00:11:21,440
on on specific nodes

00:11:18,959 --> 00:11:23,760
that's right yeah and i mean ultimately

00:11:21,440 --> 00:11:25,680
staple sets is helping a lot especially

00:11:23,760 --> 00:11:26,640
with the concept of a database right i

00:11:25,680 --> 00:11:28,480
mean you

00:11:26,640 --> 00:11:30,399
there is a fair amount that actually

00:11:28,480 --> 00:11:31,920
goes into what we're using staple sets

00:11:30,399 --> 00:11:33,680
for right so

00:11:31,920 --> 00:11:35,680
it's it's actually a critical piece of

00:11:33,680 --> 00:11:38,959
the whole thing absolutely

00:11:35,680 --> 00:11:40,800
yeah um and then uh did

00:11:38,959 --> 00:11:43,200
oh there was also a question about open

00:11:40,800 --> 00:11:44,640
id and do we support openid connect

00:11:43,200 --> 00:11:47,839
protocol

00:11:44,640 --> 00:11:50,959
so um so open id

00:11:47,839 --> 00:11:52,160
is would maybe be appropriate for the

00:11:50,959 --> 00:11:55,279
admin ui

00:11:52,160 --> 00:11:57,760
um i am not

00:11:55,279 --> 00:11:58,880
aware of us specifically having

00:11:57,760 --> 00:12:01,200
supported that

00:11:58,880 --> 00:12:03,040
single sign-on mechanism right right now

00:12:01,200 --> 00:12:06,079
our off

00:12:03,040 --> 00:12:09,600
authorization mechanisms are password

00:12:06,079 --> 00:12:13,200
user and password um um

00:12:09,600 --> 00:12:17,279
tls certificate authentication and

00:12:13,200 --> 00:12:20,399
um and then gss api which would be

00:12:17,279 --> 00:12:21,360
um backed up by like a like a kerberos

00:12:20,399 --> 00:12:24,240
based

00:12:21,360 --> 00:12:24,800
infrastructure like active directory um

00:12:24,240 --> 00:12:27,120
we

00:12:24,800 --> 00:12:28,399
we have discussed and we've done this

00:12:27,120 --> 00:12:31,839
for cockroach cloud

00:12:28,399 --> 00:12:35,600
which is our kind of managed service and

00:12:31,839 --> 00:12:38,720
um for um database as a service

00:12:35,600 --> 00:12:41,519
um where you do have

00:12:38,720 --> 00:12:43,519
single sign-on so theoretically we could

00:12:41,519 --> 00:12:46,399
make that available in self-hosted

00:12:43,519 --> 00:12:47,440
as well right um i don't think we

00:12:46,399 --> 00:12:49,360
current publish

00:12:47,440 --> 00:12:51,040
currently publish the the tooling that

00:12:49,360 --> 00:12:52,240
would be required to do that but it

00:12:51,040 --> 00:12:55,680
would be feasible

00:12:52,240 --> 00:12:57,519
um i hope that answers the question yeah

00:12:55,680 --> 00:12:58,880
i think so keith and then

00:12:57,519 --> 00:13:00,720
um one more you know is this the

00:12:58,880 --> 00:13:03,120
preferred method of installing

00:13:00,720 --> 00:13:04,480
and running cockroach um you know using

00:13:03,120 --> 00:13:06,720
the operator i mean i know we've had a

00:13:04,480 --> 00:13:08,240
helm chart as well like and so

00:13:06,720 --> 00:13:10,240
i think i see you just using the

00:13:08,240 --> 00:13:11,839
operator now um over helm

00:13:10,240 --> 00:13:13,519
like what is your preferred approach

00:13:11,839 --> 00:13:16,800
because we use either right

00:13:13,519 --> 00:13:19,440
yeah we can use either um so for

00:13:16,800 --> 00:13:20,000
production deployments i still recommend

00:13:19,440 --> 00:13:23,600
the the

00:13:20,000 --> 00:13:25,600
static configs um so

00:13:23,600 --> 00:13:27,519
largely because there's always something

00:13:25,600 --> 00:13:29,040
custom you need to do and

00:13:27,519 --> 00:13:31,120
the operator is pretty new i mean i

00:13:29,040 --> 00:13:34,959
think you're aware we we announced it

00:13:31,120 --> 00:13:38,079
um in the odd in the fall right so it's

00:13:34,959 --> 00:13:41,360
we're still um you know we're still

00:13:38,079 --> 00:13:42,480
kind of um working to to make sure that

00:13:41,360 --> 00:13:44,480
it has full

00:13:42,480 --> 00:13:45,920
full feature coverage that we need there

00:13:44,480 --> 00:13:49,680
the helm chart is great

00:13:45,920 --> 00:13:51,519
for like super simple

00:13:49,680 --> 00:13:53,360
test and dev environments you certainly

00:13:51,519 --> 00:13:54,880
could use that in production but

00:13:53,360 --> 00:13:56,880
certain administrative tasks are a

00:13:54,880 --> 00:13:59,760
little bit more complicated using home

00:13:56,880 --> 00:14:01,279
right um i don't think there's a wrong i

00:13:59,760 --> 00:14:02,399
don't think there's a bad way to install

00:14:01,279 --> 00:14:05,519
cockroachdb

00:14:02,399 --> 00:14:08,720
on on uh kubernetes

00:14:05,519 --> 00:14:11,199
the i think that generally speaking

00:14:08,720 --> 00:14:13,199
it's going to be really hard to to

00:14:11,199 --> 00:14:16,560
really mess things up for yourself

00:14:13,199 --> 00:14:20,079
um right the the the helm chart is

00:14:16,560 --> 00:14:23,519
is very flexible for for a basic install

00:14:20,079 --> 00:14:24,320
um yeah the and then the stateful sets

00:14:23,519 --> 00:14:27,360
are

00:14:24,320 --> 00:14:29,760
super dynamic for when you have

00:14:27,360 --> 00:14:31,199
custom networking and all the the kinds

00:14:29,760 --> 00:14:32,800
of things that go into that

00:14:31,199 --> 00:14:34,399
yeah you know somebody was asking about

00:14:32,800 --> 00:14:36,639
you know how do you persist data

00:14:34,399 --> 00:14:38,240
you know is it just pv and what are you

00:14:36,639 --> 00:14:40,240
doing and i think that's all kind of

00:14:38,240 --> 00:14:41,440
covered i think you you kind of spoke to

00:14:40,240 --> 00:14:43,519
that keith but

00:14:41,440 --> 00:14:44,959
staple sets is really that key thing so

00:14:43,519 --> 00:14:48,079
that when things

00:14:44,959 --> 00:14:51,120
do fail the persistent storage and

00:14:48,079 --> 00:14:52,880
remounting that to whatever pod that is

00:14:51,120 --> 00:14:54,560
um it's just a key piece of the overall

00:14:52,880 --> 00:14:56,160
equation it's part of what you know

00:14:54,560 --> 00:14:58,160
because we can actually survive the loss

00:14:56,160 --> 00:14:59,680
of a piece of data within the database

00:14:58,160 --> 00:15:01,279
we can lose a pod pretty easily

00:14:59,680 --> 00:15:02,720
and and survive that pretty well you're

00:15:01,279 --> 00:15:03,360
going to show that in the next demo

00:15:02,720 --> 00:15:05,600
right

00:15:03,360 --> 00:15:07,279
yeah in the in the distributed demo

00:15:05,600 --> 00:15:08,800
across multiple data centers i'm going

00:15:07,279 --> 00:15:11,440
to show that so

00:15:08,800 --> 00:15:13,199
um it's a little easier here because of

00:15:11,440 --> 00:15:13,839
the gui to show off some of the stuff

00:15:13,199 --> 00:15:16,000
that's in

00:15:13,839 --> 00:15:17,120
that we're actually doing so we're

00:15:16,000 --> 00:15:20,320
mounting

00:15:17,120 --> 00:15:23,760
um secrets as a volume right

00:15:20,320 --> 00:15:25,519
um and we're also mounting a persistent

00:15:23,760 --> 00:15:28,880
volume

00:15:25,519 --> 00:15:31,519
claim a pv into um

00:15:28,880 --> 00:15:32,880
into the cluster right um and each node

00:15:31,519 --> 00:15:36,000
gets its own

00:15:32,880 --> 00:15:38,160
volume right so it's

00:15:36,000 --> 00:15:40,399
um and that way if we lose a node or we

00:15:38,160 --> 00:15:41,519
lose a pod we can recover from that very

00:15:40,399 --> 00:15:43,199
easily

00:15:41,519 --> 00:15:44,480
and so one last thing before we move on

00:15:43,199 --> 00:15:46,480
keith and i just wanted to comment there

00:15:44,480 --> 00:15:47,920
was a i'm trying to hit the questions

00:15:46,480 --> 00:15:49,279
live y'all like uh

00:15:47,920 --> 00:15:51,279
there's there's a lot of questions here

00:15:49,279 --> 00:15:52,720
which is really fantastic so forgive me

00:15:51,279 --> 00:15:54,399
if i'm not repeating the question i am

00:15:52,720 --> 00:15:55,519
paraphrasing and kind of lobbying them

00:15:54,399 --> 00:15:57,759
into keith here

00:15:55,519 --> 00:15:59,040
um i just wanted to talk to a couple

00:15:57,759 --> 00:16:00,560
things our operator

00:15:59,040 --> 00:16:02,240
is somebody's asking if it's open source

00:16:00,560 --> 00:16:03,759
or not our operator is definitely open

00:16:02,240 --> 00:16:04,800
source the git repo is out there i'll

00:16:03,759 --> 00:16:07,120
get it out to everybody

00:16:04,800 --> 00:16:09,440
um as keith is doing this i'll go look

00:16:07,120 --> 00:16:12,800
it up and get the link to that um

00:16:09,440 --> 00:16:14,240
it is written in go um which is

00:16:12,800 --> 00:16:16,399
which is kind of unique to and i mean

00:16:14,240 --> 00:16:18,000
well i mean unique to us actually

00:16:16,399 --> 00:16:20,240
our entire database is written and go so

00:16:18,000 --> 00:16:21,519
it's pretty well aligned um kind of with

00:16:20,240 --> 00:16:22,399
kind of how these things come together

00:16:21,519 --> 00:16:24,480
so

00:16:22,399 --> 00:16:26,240
um dude there are a lot of really really

00:16:24,480 --> 00:16:28,079
deep questions here keith i'm gonna

00:16:26,240 --> 00:16:29,279
let i'm gonna i'm gonna do the second

00:16:28,079 --> 00:16:30,160
half of the presentation just to talk

00:16:29,279 --> 00:16:32,079
about how we scale

00:16:30,160 --> 00:16:33,440
and how we survive things and then we'll

00:16:32,079 --> 00:16:35,680
get you back into the

00:16:33,440 --> 00:16:37,279
the demo um while keith kind of sets

00:16:35,680 --> 00:16:40,560
things up i'm going to go back into

00:16:37,279 --> 00:16:43,600
the uh screen sharing here y'all okay

00:16:40,560 --> 00:16:43,600
let me see where's this guy

00:16:45,279 --> 00:16:52,240
okay uh keith can you see my

00:16:48,880 --> 00:16:55,600
the the share yes

00:16:52,240 --> 00:16:56,639
okay great awesome so um we showed you

00:16:55,600 --> 00:16:58,880
how to kind of

00:16:56,639 --> 00:16:59,839
deal with installation and the operator

00:16:58,880 --> 00:17:01,600
what not um

00:16:59,839 --> 00:17:03,519
you know underneath the covers what what

00:17:01,600 --> 00:17:04,000
cockroach implements and what we use and

00:17:03,519 --> 00:17:06,480
we rely

00:17:04,000 --> 00:17:08,799
on pretty heavily is a a distributed

00:17:06,480 --> 00:17:10,559
consensus protocol called raft

00:17:08,799 --> 00:17:12,000
now if you're not familiar with raft uh

00:17:10,559 --> 00:17:13,520
go check it out i mean if you're

00:17:12,000 --> 00:17:14,720
interested in distributed systems we're

00:17:13,520 --> 00:17:17,120
after paxos

00:17:14,720 --> 00:17:18,000
um really interesting cool stuff um and

00:17:17,120 --> 00:17:21,439
we use raft

00:17:18,000 --> 00:17:23,120
uh to to a major extent i think

00:17:21,439 --> 00:17:24,880
i think i heard and it's it's not easy

00:17:23,120 --> 00:17:27,199
to actually get done and do well

00:17:24,880 --> 00:17:27,919
um within within a distributed system i

00:17:27,199 --> 00:17:29,679
think

00:17:27,919 --> 00:17:30,880
i think i heard ben darnell who's one of

00:17:29,679 --> 00:17:32,400
our founders probably one of the best

00:17:30,880 --> 00:17:33,039
software engineers i've ever met in my

00:17:32,400 --> 00:17:34,880
life

00:17:33,039 --> 00:17:36,400
say hey i could have probably coded raft

00:17:34,880 --> 00:17:38,400
in a couple days when i was in college

00:17:36,400 --> 00:17:41,200
but to do it for a distributed

00:17:38,400 --> 00:17:41,679
like production grade database it's

00:17:41,200 --> 00:17:43,679
taken

00:17:41,679 --> 00:17:44,880
years to get this right because we're

00:17:43,679 --> 00:17:46,960
dealing with some really

00:17:44,880 --> 00:17:47,919
interesting challenges when it comes to

00:17:46,960 --> 00:17:49,600
a database and

00:17:47,919 --> 00:17:51,039
and when you start dealing with raft and

00:17:49,600 --> 00:17:54,240
distributed

00:17:51,039 --> 00:17:55,520
consensus at major scale across you know

00:17:54,240 --> 00:17:57,280
different parts of

00:17:55,520 --> 00:17:58,480
the planet there's lots of different

00:17:57,280 --> 00:17:59,760
things that go into that and there's

00:17:58,480 --> 00:18:01,360
been a lot of really interesting

00:17:59,760 --> 00:18:02,960
software engineering from

00:18:01,360 --> 00:18:05,039
you know the cockroach labs engineering

00:18:02,960 --> 00:18:07,120
team to actually fix some

00:18:05,039 --> 00:18:08,799
really interesting things uh that that

00:18:07,120 --> 00:18:10,080
we found in raf that

00:18:08,799 --> 00:18:12,400
you know we actually contribute to

00:18:10,080 --> 00:18:14,080
upstream fcd and there was a question

00:18:12,400 --> 00:18:15,520
here about split brain and

00:18:14,080 --> 00:18:17,360
some of these things actually we've

00:18:15,520 --> 00:18:20,559
architected out some of those those

00:18:17,360 --> 00:18:21,280
really really difficult problems to deal

00:18:20,559 --> 00:18:22,640
with uh

00:18:21,280 --> 00:18:24,559
you know around you know atomic

00:18:22,640 --> 00:18:26,400
replication and whatnot and so

00:18:24,559 --> 00:18:27,760
um there's lots of stuff on our blog i'm

00:18:26,400 --> 00:18:29,679
not gonna get too deep into the the

00:18:27,760 --> 00:18:31,520
particulars of what we've done here but

00:18:29,679 --> 00:18:33,760
the same kind of core concepts that are

00:18:31,520 --> 00:18:36,559
driving kubernetes and you know

00:18:33,760 --> 00:18:38,240
wrath and fcd are the same concepts that

00:18:36,559 --> 00:18:39,520
are here in cockroach and we share a lot

00:18:38,240 --> 00:18:40,880
of that same lineage and we actually

00:18:39,520 --> 00:18:42,480
contribute upstream to these

00:18:40,880 --> 00:18:44,320
things so i know there was a question

00:18:42,480 --> 00:18:45,440
about split brain man that's a really

00:18:44,320 --> 00:18:47,200
really deep question there's

00:18:45,440 --> 00:18:48,720
definitely a great blog post about how

00:18:47,200 --> 00:18:51,440
we've actually dealt with that um

00:18:48,720 --> 00:18:53,039
in cockroach um i think you know there's

00:18:51,440 --> 00:18:55,039
been some recent issues about

00:18:53,039 --> 00:18:57,039
split brain and distributed consensus

00:18:55,039 --> 00:18:59,200
and it's a whole other world but

00:18:57,039 --> 00:19:01,120
but we use raft there's a raft group in

00:18:59,200 --> 00:19:02,720
cockroach raft group basically is you

00:19:01,120 --> 00:19:04,799
know replicas of data

00:19:02,720 --> 00:19:07,600
and a leaseholder or raft leader where

00:19:04,799 --> 00:19:11,200
basically most transactions will concern

00:19:07,600 --> 00:19:13,039
um we'll we'll we'll commit um

00:19:11,200 --> 00:19:14,320
when we actually place data within a

00:19:13,039 --> 00:19:14,640
cluster somebody was actually asking

00:19:14,320 --> 00:19:16,960
this

00:19:14,640 --> 00:19:18,799
in the uh in the chat you know how do

00:19:16,960 --> 00:19:20,640
you how do you distribute data

00:19:18,799 --> 00:19:23,120
i have four nodes and i actually want to

00:19:20,640 --> 00:19:25,440
take a table and distribute replicas

00:19:23,120 --> 00:19:26,960
of the ranges within this table

00:19:25,440 --> 00:19:29,120
ultimately underneath the cover is in

00:19:26,960 --> 00:19:32,320
cockroach every table is broken down

00:19:29,120 --> 00:19:34,640
into 256 megabit chunks of data

00:19:32,320 --> 00:19:36,160
um and so what that allows us to do is

00:19:34,640 --> 00:19:37,520
move data around and create these

00:19:36,160 --> 00:19:39,760
replica sets

00:19:37,520 --> 00:19:41,360
um and so you can imagine this table

00:19:39,760 --> 00:19:43,200
which is dog names here

00:19:41,360 --> 00:19:44,640
um you know we're writing you know

00:19:43,200 --> 00:19:46,640
ranges of this data you can think of

00:19:44,640 --> 00:19:47,760
these as just charts but we automate

00:19:46,640 --> 00:19:50,000
all of that chart and we'll show you

00:19:47,760 --> 00:19:52,160
that right and so um but when we write

00:19:50,000 --> 00:19:54,880
data to a cluster we can we can optimize

00:19:52,160 --> 00:19:56,480
this for different failure domains or

00:19:54,880 --> 00:19:58,320
basically for latency

00:19:56,480 --> 00:19:59,520
uh but we can actually survive lots of

00:19:58,320 --> 00:20:01,520
different things when we write the first

00:19:59,520 --> 00:20:03,200
range we write three nodes we write

00:20:01,520 --> 00:20:04,240
the second range the right third range

00:20:03,200 --> 00:20:06,240
and and what we're doing is we're

00:20:04,240 --> 00:20:07,440
distributing data evenly across these

00:20:06,240 --> 00:20:09,760
various different nodes

00:20:07,440 --> 00:20:12,000
we can also do things we use heuristics

00:20:09,760 --> 00:20:15,120
in the database itself so that when

00:20:12,000 --> 00:20:15,679
load is heavy on a particular range we

00:20:15,120 --> 00:20:18,000
can actually

00:20:15,679 --> 00:20:19,760
segment that range out to its own node

00:20:18,000 --> 00:20:21,280
or its own pod if you will

00:20:19,760 --> 00:20:23,280
so that it can actually you know deal

00:20:21,280 --> 00:20:24,480
with that transactional volume over what

00:20:23,280 --> 00:20:26,400
that is right so

00:20:24,480 --> 00:20:28,799
um another way in which we can actually

00:20:26,400 --> 00:20:29,120
deal with the the placement of data is

00:20:28,799 --> 00:20:31,360
to

00:20:29,120 --> 00:20:32,400
optimize the overall performance of the

00:20:31,360 --> 00:20:34,080
cluster

00:20:32,400 --> 00:20:35,600
now we can also do something really cool

00:20:34,080 --> 00:20:37,360
and if you

00:20:35,600 --> 00:20:39,520
have a deep deeper layer there is you

00:20:37,360 --> 00:20:41,039
know the architectural deep

00:20:39,520 --> 00:20:42,720
dive of cockroach if you're really

00:20:41,039 --> 00:20:44,640
interested in how this stuff works

00:20:42,720 --> 00:20:46,159
uh we go into a great explanation of how

00:20:44,640 --> 00:20:47,840
this works if there's it's in our

00:20:46,159 --> 00:20:48,480
youtube channel if you really want after

00:20:47,840 --> 00:20:50,640
it but

00:20:48,480 --> 00:20:52,240
we can actually overload the primary key

00:20:50,640 --> 00:20:55,120
uh for each of the tables

00:20:52,240 --> 00:20:55,840
and actually integrate in uh say a

00:20:55,120 --> 00:20:58,240
location

00:20:55,840 --> 00:21:00,000
so that we can actually control where

00:20:58,240 --> 00:21:00,960
data is going to be placed in a cluster

00:21:00,000 --> 00:21:02,480
as well

00:21:00,960 --> 00:21:03,520
um and so there's lots of different

00:21:02,480 --> 00:21:05,120
things we can do here this is a unique

00:21:03,520 --> 00:21:07,200
capability to cockroach

00:21:05,120 --> 00:21:08,320
um it's called geo partitioning i

00:21:07,200 --> 00:21:09,600
believe keith you are going to show a

00:21:08,320 --> 00:21:10,480
little bit of geo petitioning in the

00:21:09,600 --> 00:21:11,520
demo right

00:21:10,480 --> 00:21:13,679
yeah we're going to use gene

00:21:11,520 --> 00:21:14,400
partitioning across three data centers

00:21:13,679 --> 00:21:17,520
in the

00:21:14,400 --> 00:21:19,039
in the as a part of the demo and so you

00:21:17,520 --> 00:21:20,640
know ultimately a cockroach look at

00:21:19,039 --> 00:21:21,840
there there are two primitives of which

00:21:20,640 --> 00:21:23,520
we've defined for

00:21:21,840 --> 00:21:25,200
and which we designed and implemented

00:21:23,520 --> 00:21:28,000
for number one is

00:21:25,200 --> 00:21:29,520
consistency of data so you know we've

00:21:28,000 --> 00:21:30,960
implemented a database that is true

00:21:29,520 --> 00:21:32,880
system of records so

00:21:30,960 --> 00:21:34,640
serializable isolation so data is

00:21:32,880 --> 00:21:36,480
guaranteed correct in our database this

00:21:34,640 --> 00:21:37,840
isn't eventually consistent you're what

00:21:36,480 --> 00:21:38,880
you read is what you're going to get

00:21:37,840 --> 00:21:40,559
right and that's

00:21:38,880 --> 00:21:41,760
and when we've optimized for that

00:21:40,559 --> 00:21:42,720
there's a lot of things we had to do in

00:21:41,760 --> 00:21:44,799
distributed systems

00:21:42,720 --> 00:21:46,400
actually deal with that right so there's

00:21:44,799 --> 00:21:47,600
these challenges to the cap theorem i

00:21:46,400 --> 00:21:48,799
think there's been a couple questions

00:21:47,600 --> 00:21:49,600
about them we could talk about that in a

00:21:48,799 --> 00:21:51,600
little bit

00:21:49,600 --> 00:21:52,960
um but we've also optimized for the

00:21:51,600 --> 00:21:55,280
speed of light that's our biggest

00:21:52,960 --> 00:21:57,919
competitor how do we actually guarantee

00:21:55,280 --> 00:21:59,520
a transaction and then give low latency

00:21:57,919 --> 00:22:00,799
access to database no matter where

00:21:59,520 --> 00:22:02,400
people are on the planet

00:22:00,799 --> 00:22:04,000
which is also really really difficult to

00:22:02,400 --> 00:22:04,480
do for it and so we've done a lot of

00:22:04,000 --> 00:22:07,840
those

00:22:04,480 --> 00:22:09,919
those those things when we add replicas

00:22:07,840 --> 00:22:11,679
or we add a node to a cluster what it's

00:22:09,919 --> 00:22:12,000
doing is just simply moving replicas

00:22:11,679 --> 00:22:14,240
around

00:22:12,000 --> 00:22:15,200
and rebalancing the cluster um we can

00:22:14,240 --> 00:22:17,520
actually survive

00:22:15,200 --> 00:22:19,200
failure um say node three goes down

00:22:17,520 --> 00:22:20,799
those two replicas are gone

00:22:19,200 --> 00:22:22,080
the database is smart enough to heal

00:22:20,799 --> 00:22:23,440
that it's gonna create you know the

00:22:22,080 --> 00:22:24,720
other replicas and it's always gonna be

00:22:23,440 --> 00:22:26,799
available so

00:22:24,720 --> 00:22:28,159
um real quick kind of overview i just

00:22:26,799 --> 00:22:29,600
wanted to give a little breather so

00:22:28,159 --> 00:22:32,960
keith can go into the second

00:22:29,600 --> 00:22:35,760
half of the demo

00:22:32,960 --> 00:22:35,760
keith that's all you guys

00:22:39,280 --> 00:22:43,600
so can you see my screen now jim

00:22:43,679 --> 00:22:48,159
uh yes i can keith i'm by the way y'all

00:22:46,799 --> 00:22:48,799
i'm trying to answer questions in the

00:22:48,159 --> 00:22:51,280
background

00:22:48,799 --> 00:22:52,880
um as much as i can keep them coming um

00:22:51,280 --> 00:22:54,400
we'll get all of them afterwards if we

00:22:52,880 --> 00:22:55,840
don't get to everything but i'll try to

00:22:54,400 --> 00:22:57,200
lob things into keith along the way as

00:22:55,840 --> 00:22:57,840
well so thank you very much for all the

00:22:57,200 --> 00:23:00,880
questions

00:22:57,840 --> 00:23:03,919
so this this is a cluster that i set up

00:23:00,880 --> 00:23:07,520
earlier today um it's nine

00:23:03,919 --> 00:23:09,600
cockroachdb nodes plus an additional

00:23:07,520 --> 00:23:11,120
worker node in each region

00:23:09,600 --> 00:23:12,559
um which you're not going to see in here

00:23:11,120 --> 00:23:14,480
because they didn't i didn't have them

00:23:12,559 --> 00:23:16,240
join the database cluster

00:23:14,480 --> 00:23:18,080
um on that worker node i'm running a

00:23:16,240 --> 00:23:20,799
load balancer and

00:23:18,080 --> 00:23:22,000
a load generator so we're generating a

00:23:20,799 --> 00:23:24,400
load in each of these

00:23:22,000 --> 00:23:26,960
regions the app that we're running is a

00:23:24,400 --> 00:23:29,120
simulated ride sharing app that we call

00:23:26,960 --> 00:23:30,799
mover a lot of our tutorials on the

00:23:29,120 --> 00:23:33,679
website use this app

00:23:30,799 --> 00:23:34,080
so this is just a scripted version of a

00:23:33,679 --> 00:23:37,240
of

00:23:34,080 --> 00:23:40,159
a tutorial uh the geopartition

00:23:37,240 --> 00:23:40,559
replicas tutorial in our on our website

00:23:40,159 --> 00:23:41,840
if you

00:23:40,559 --> 00:23:43,760
wanted to go through and do this

00:23:41,840 --> 00:23:46,480
yourself so um

00:23:43,760 --> 00:23:48,240
right now we're running performance

00:23:46,480 --> 00:23:49,679
isn't great it's running like 500

00:23:48,240 --> 00:23:51,440
milliseconds per

00:23:49,679 --> 00:23:52,799
for query i haven't done anything to the

00:23:51,440 --> 00:23:55,039
database to to make it

00:23:52,799 --> 00:23:56,480
fast uh we're running about 650

00:23:55,039 --> 00:24:00,320
transactions per second

00:23:56,480 --> 00:24:04,240
across the the nine nodes okay

00:24:00,320 --> 00:24:06,000
so um so what i'm going to do first

00:24:04,240 --> 00:24:08,159
is i'm going to demonstrate that the

00:24:06,000 --> 00:24:10,720
database continues to operate

00:24:08,159 --> 00:24:12,080
um when i kill a node so i'm about i

00:24:10,720 --> 00:24:15,679
just killed one of the nodes

00:24:12,080 --> 00:24:18,400
in the u.s east region um

00:24:15,679 --> 00:24:19,679
so i think we'll be able to see it here

00:24:18,400 --> 00:24:23,039
in a second or two

00:24:19,679 --> 00:24:26,720
um we have a suspect node so us east

00:24:23,039 --> 00:24:27,120
1c just went down um i'm going to show

00:24:26,720 --> 00:24:29,600
that

00:24:27,120 --> 00:24:32,320
how the database um we're going to have

00:24:29,600 --> 00:24:34,320
a bit of a performance blip

00:24:32,320 --> 00:24:35,520
of course but then the the remaining

00:24:34,320 --> 00:24:37,200
nodes are going to kind of take over

00:24:35,520 --> 00:24:38,240
those those transactions as they go

00:24:37,200 --> 00:24:41,120
forward

00:24:38,240 --> 00:24:42,000
um there was a question about cap

00:24:41,120 --> 00:24:44,880
theorem right

00:24:42,000 --> 00:24:45,679
so you can't guarantee consistency

00:24:44,880 --> 00:24:48,880
guarantee

00:24:45,679 --> 00:24:50,559
availability and be partition tolerant

00:24:48,880 --> 00:24:53,039
all at the same time and and that's

00:24:50,559 --> 00:24:56,080
still mathematically true

00:24:53,039 --> 00:24:59,360
what what we do is

00:24:56,080 --> 00:25:00,960
because we distribute and use the data

00:24:59,360 --> 00:25:04,159
and use consensus

00:25:00,960 --> 00:25:05,679
we can design the database to survive

00:25:04,159 --> 00:25:07,120
certain failure scenarios so in this

00:25:05,679 --> 00:25:09,919
case

00:25:07,120 --> 00:25:11,120
i um the the database i just

00:25:09,919 --> 00:25:13,360
demonstrated

00:25:11,120 --> 00:25:15,120
surviving a node failure i could have

00:25:13,360 --> 00:25:17,840
taken out an entire region

00:25:15,120 --> 00:25:19,440
as well and the other two remaining

00:25:17,840 --> 00:25:21,919
regions were would be

00:25:19,440 --> 00:25:22,880
able to continue to process data for the

00:25:21,919 --> 00:25:24,880
database

00:25:22,880 --> 00:25:27,200
um if we were to lose two of the three

00:25:24,880 --> 00:25:28,080
regions though and i lost a quorum for

00:25:27,200 --> 00:25:29,840
my ranges

00:25:28,080 --> 00:25:31,919
then there would be data on availability

00:25:29,840 --> 00:25:35,919
so so it's not like we

00:25:31,919 --> 00:25:38,080
um completely disproved cap theorem

00:25:35,919 --> 00:25:39,520
um instead we're working around those

00:25:38,080 --> 00:25:42,640
limitations to try

00:25:39,520 --> 00:25:45,760
to kind of make the the

00:25:42,640 --> 00:25:47,039
the database be able to survive the

00:25:45,760 --> 00:25:47,840
types of failures we need to be able to

00:25:47,039 --> 00:25:49,360
survive

00:25:47,840 --> 00:25:51,679
now i'm going to go ahead and i'm going

00:25:49,360 --> 00:25:55,200
to let that that node that i failed

00:25:51,679 --> 00:25:57,360
come back up um it's going to take

00:25:55,200 --> 00:25:59,440
a little bit of time once it does you'll

00:25:57,360 --> 00:26:00,000
see that this under replicated ranges

00:25:59,440 --> 00:26:01,200
count

00:26:00,000 --> 00:26:03,200
that you're seeing on my screen right

00:26:01,200 --> 00:26:06,240
now um we'll go back to zero

00:26:03,200 --> 00:26:08,880
so the help the node has restarted and

00:26:06,240 --> 00:26:10,000
um now we have caught the under

00:26:08,880 --> 00:26:12,720
replicated ranges

00:26:10,000 --> 00:26:13,360
up and so the database is fully healthy

00:26:12,720 --> 00:26:15,840
um

00:26:13,360 --> 00:26:17,360
what's great about all of this is with

00:26:15,840 --> 00:26:20,559
the exception of that

00:26:17,360 --> 00:26:24,240
uh a dip where we were re-electing

00:26:20,559 --> 00:26:25,520
the the leaders for certain um sections

00:26:24,240 --> 00:26:28,000
of the data which

00:26:25,520 --> 00:26:30,000
meant that we were holding some queries

00:26:28,000 --> 00:26:31,919
the database was completely self-healing

00:26:30,000 --> 00:26:34,080
and it actually self-healed to the

00:26:31,919 --> 00:26:38,000
original performance metrics

00:26:34,080 --> 00:26:41,279
um with um before i even restart

00:26:38,000 --> 00:26:42,960
the new node okay so but we haven't done

00:26:41,279 --> 00:26:46,159
any performance optimization

00:26:42,960 --> 00:26:46,799
and quite frankly you know a p99 latency

00:26:46,159 --> 00:26:48,799
of

00:26:46,799 --> 00:26:51,279
you know in the half a second range is

00:26:48,799 --> 00:26:53,200
not great for most user-facing apps

00:26:51,279 --> 00:26:54,400
yeah can i just before you get into the

00:26:53,200 --> 00:26:55,600
performance thing can i just ask

00:26:54,400 --> 00:26:57,679
there was actually one question that was

00:26:55,600 --> 00:26:59,360
kind of interesting this whole thing um

00:26:57,679 --> 00:27:00,799
and it's it's applicable to where you're

00:26:59,360 --> 00:27:03,120
at right now and so

00:27:00,799 --> 00:27:04,880
look it we just so showed survival that

00:27:03,120 --> 00:27:05,600
one node went down there were still two

00:27:04,880 --> 00:27:06,960
nodes

00:27:05,600 --> 00:27:08,320
there was no real impact to the

00:27:06,960 --> 00:27:09,679
application i mean there was a little

00:27:08,320 --> 00:27:11,840
bit of a dip

00:27:09,679 --> 00:27:12,960
but queries were still serviced right

00:27:11,840 --> 00:27:16,320
what happens if two

00:27:12,960 --> 00:27:19,600
or three nodes go down however so then

00:27:16,320 --> 00:27:21,679
then we would um two of three nodes

00:27:19,600 --> 00:27:24,640
or so because it's a nine node cluster

00:27:21,679 --> 00:27:27,919
remember i would have to have lost

00:27:24,640 --> 00:27:28,640
um i could have lost all of my nodes in

00:27:27,919 --> 00:27:30,399
one region

00:27:28,640 --> 00:27:31,679
and the database would have continued to

00:27:30,399 --> 00:27:34,000
operate if

00:27:31,679 --> 00:27:35,600
i lost because i'm only doing a

00:27:34,000 --> 00:27:38,480
replication factor of three here

00:27:35,600 --> 00:27:40,240
if i were to start losing nodes in

00:27:38,480 --> 00:27:42,720
across multiple

00:27:40,240 --> 00:27:44,080
regions there's the chance that some

00:27:42,720 --> 00:27:44,880
sections of the data would not be

00:27:44,080 --> 00:27:47,039
available

00:27:44,880 --> 00:27:48,399
um i could manage for that by increasing

00:27:47,039 --> 00:27:49,679
my replication factor

00:27:48,399 --> 00:27:52,240
right so if i were to increase my

00:27:49,679 --> 00:27:54,480
replication factor in this case to five

00:27:52,240 --> 00:27:55,679
i could survive either a full data

00:27:54,480 --> 00:27:58,320
center loss

00:27:55,679 --> 00:27:59,039
or any two arbitrary nodes in the

00:27:58,320 --> 00:28:03,039
cluster

00:27:59,039 --> 00:28:06,480
right right right now i can lose um

00:28:03,039 --> 00:28:08,640
oh i can only lose a node or a fault

00:28:06,480 --> 00:28:11,679
domain in this case a data center

00:28:08,640 --> 00:28:12,720
right um if in the scenario you're

00:28:11,679 --> 00:28:14,559
describing if i

00:28:12,720 --> 00:28:16,000
like upped my replication factor to five

00:28:14,559 --> 00:28:18,880
then i could lose two arbitrary

00:28:16,000 --> 00:28:19,760
nodes right or or an entire fault domain

00:28:18,880 --> 00:28:22,480
right or a

00:28:19,760 --> 00:28:23,279
fault domain plus an additional node i

00:28:22,480 --> 00:28:25,600
mean

00:28:23,279 --> 00:28:26,720
i guess yeah i mean long story short

00:28:25,600 --> 00:28:28,159
keith it's basically like

00:28:26,720 --> 00:28:30,720
well what do you want to survive and how

00:28:28,159 --> 00:28:31,360
do you architect the database to survive

00:28:30,720 --> 00:28:32,559
that

00:28:31,360 --> 00:28:34,640
we're going to give you a couple

00:28:32,559 --> 00:28:36,880
different knobs and dials to turn

00:28:34,640 --> 00:28:38,640
to optimize for what you want to do but

00:28:36,880 --> 00:28:39,760
i think you have to think about that

00:28:38,640 --> 00:28:42,080
when you're doing when you're

00:28:39,760 --> 00:28:44,399
architecting you know the deployment the

00:28:42,080 --> 00:28:47,200
topology of this thing right yes

00:28:44,399 --> 00:28:48,559
so um so we have a great tool that's

00:28:47,200 --> 00:28:51,039
linked from our website

00:28:48,559 --> 00:28:52,080
about kind of what your minimum topology

00:28:51,039 --> 00:28:53,679
needs to look like

00:28:52,080 --> 00:28:55,760
and your minimum replication factor

00:28:53,679 --> 00:28:59,039
needs to look like to

00:28:55,760 --> 00:29:02,320
um to design your database to survive

00:28:59,039 --> 00:29:04,880
certain failure scenarios um right there

00:29:02,320 --> 00:29:06,480
um you know there's no magic here i mean

00:29:04,880 --> 00:29:09,520
i it feels magical

00:29:06,480 --> 00:29:12,240
because distributed sql is hard but

00:29:09,520 --> 00:29:14,080
the reality is that we're using the same

00:29:12,240 --> 00:29:14,799
fundamental underpinnings that fcd is

00:29:14,080 --> 00:29:16,399
using

00:29:14,799 --> 00:29:17,919
to survive the loss of a masternode in

00:29:16,399 --> 00:29:20,960
kubernetes and that's

00:29:17,919 --> 00:29:21,840
that's the reason why we can do this and

00:29:20,960 --> 00:29:24,880
we can

00:29:21,840 --> 00:29:27,039
run in kubernetes and have a pod be

00:29:24,880 --> 00:29:29,279
maybe get restarted on out from under us

00:29:27,039 --> 00:29:30,720
and still have the database operate the

00:29:29,279 --> 00:29:32,640
way we expect

00:29:30,720 --> 00:29:34,159
so so what i'm going to do in the

00:29:32,640 --> 00:29:37,840
background now is i'm going to start

00:29:34,159 --> 00:29:39,760
optimizing the environment um for

00:29:37,840 --> 00:29:41,279
being geo-replicated across three sites

00:29:39,760 --> 00:29:43,360
so the first thing i'm going to do

00:29:41,279 --> 00:29:44,480
is i'm going to partition my tables and

00:29:43,360 --> 00:29:47,120
indexes

00:29:44,480 --> 00:29:48,000
so right now we're using just kind of

00:29:47,120 --> 00:29:51,360
the default

00:29:48,000 --> 00:29:53,360
which is okay right but

00:29:51,360 --> 00:29:54,720
you know as you said earlier a customer

00:29:53,360 --> 00:29:56,080
interaction really needs to be 100

00:29:54,720 --> 00:29:58,399
milliseconds or less

00:29:56,080 --> 00:29:59,919
which means the database probably needs

00:29:58,399 --> 00:30:02,480
to be

00:29:59,919 --> 00:30:03,679
you know three to five times faster than

00:30:02,480 --> 00:30:08,159
that at worst case

00:30:03,679 --> 00:30:10,399
to have consistent kind of real time

00:30:08,159 --> 00:30:11,840
performance or seemingly real-time

00:30:10,399 --> 00:30:13,120
performance for a user-facing

00:30:11,840 --> 00:30:16,159
application which is

00:30:13,120 --> 00:30:18,880
largely what we're focused on so what

00:30:16,159 --> 00:30:21,760
i'm doing here is i'm altering my tables

00:30:18,880 --> 00:30:22,799
to make some partitioning decisions

00:30:21,760 --> 00:30:26,159
basically to

00:30:22,799 --> 00:30:29,039
have the leaseholder which is uh

00:30:26,159 --> 00:30:30,159
the raft leader so technically the raft

00:30:29,039 --> 00:30:33,279
leader manages

00:30:30,159 --> 00:30:35,919
right consensus the leaseholder is

00:30:33,279 --> 00:30:37,120
the replica the leading replica that's

00:30:35,919 --> 00:30:39,200
able to

00:30:37,120 --> 00:30:41,600
respond to reads without doing a quorum

00:30:39,200 --> 00:30:43,520
check okay

00:30:41,600 --> 00:30:45,360
so we're configuring the database on the

00:30:43,520 --> 00:30:48,480
back end right now

00:30:45,360 --> 00:30:50,960
making some changes to the tables to

00:30:48,480 --> 00:30:52,320
optimize the placement of those things

00:30:50,960 --> 00:30:54,720
so we're running

00:30:52,320 --> 00:30:55,760
a um this ride-sharing app in three

00:30:54,720 --> 00:30:58,880
cities

00:30:55,760 --> 00:31:02,000
new york chicago and los angeles

00:30:58,880 --> 00:31:03,519
and so i'm setting up primary rules for

00:31:02,000 --> 00:31:06,720
each of those cities

00:31:03,519 --> 00:31:08,159
to to have their primary replicas live

00:31:06,720 --> 00:31:09,679
in the data center that's closest to

00:31:08,159 --> 00:31:11,039
those cities and so what you're going to

00:31:09,679 --> 00:31:13,200
see

00:31:11,039 --> 00:31:14,559
is that our query per second count is

00:31:13,200 --> 00:31:15,840
going to go up because the database is

00:31:14,559 --> 00:31:18,640
becoming more efficient

00:31:15,840 --> 00:31:19,519
and our p99 latency is going to start

00:31:18,640 --> 00:31:22,720
creeping down

00:31:19,519 --> 00:31:25,200
here over the next minute or two

00:31:22,720 --> 00:31:26,399
um and the net effect of that is in in

00:31:25,200 --> 00:31:28,480
mover i'm gonna go ahead and i'll

00:31:26,399 --> 00:31:32,000
actually show you

00:31:28,480 --> 00:31:34,159
um what we've done here

00:31:32,000 --> 00:31:36,320
so i'm gonna go ahead and kind of click

00:31:34,159 --> 00:31:40,000
into to one of these

00:31:36,320 --> 00:31:40,880
um these tables it's gonna take a second

00:31:40,000 --> 00:31:47,840
to

00:31:40,880 --> 00:31:47,840
refresh hold on a sec

00:31:59,679 --> 00:32:03,279
i am having some internet connectivity

00:32:01,600 --> 00:32:05,279
problems

00:32:03,279 --> 00:32:06,720
um so i apologize for that so i had a

00:32:05,279 --> 00:32:08,480
couple of internet connectivity problems

00:32:06,720 --> 00:32:11,679
there let me go ahead and

00:32:08,480 --> 00:32:12,720
get this going here we go so um you see

00:32:11,679 --> 00:32:16,000
we have

00:32:12,720 --> 00:32:18,320
the the ddl for this table now has a

00:32:16,000 --> 00:32:21,039
bunch of partitions in it

00:32:18,320 --> 00:32:23,039
and we assign those partitions to

00:32:21,039 --> 00:32:26,799
different regions

00:32:23,039 --> 00:32:28,960
okay and what that allows us to do is

00:32:26,799 --> 00:32:30,640
basically this is the dba setting

00:32:28,960 --> 00:32:31,760
default rules for how the database

00:32:30,640 --> 00:32:35,360
should act

00:32:31,760 --> 00:32:38,000
um in any kind of given situation

00:32:35,360 --> 00:32:39,039
so what we see now is our p99 latency is

00:32:38,000 --> 00:32:42,720
much better

00:32:39,039 --> 00:32:44,640
right um 65 milliseconds as opposed to

00:32:42,720 --> 00:32:46,880
almost 600 milliseconds

00:32:44,640 --> 00:32:49,279
our transactions per second went up from

00:32:46,880 --> 00:32:50,399
roughly 650 transactions per second

00:32:49,279 --> 00:32:53,440
across this cluster

00:32:50,399 --> 00:32:55,039
it's almost eleven hundred um my my

00:32:53,440 --> 00:32:57,200
query latency is going to

00:32:55,039 --> 00:32:59,120
continue to improve here over the next i

00:32:57,200 --> 00:33:02,559
don't know a couple of minutes

00:32:59,120 --> 00:33:04,559
um but um 35 milliseconds is great

00:33:02,559 --> 00:33:05,919
but really i want to be sub 10

00:33:04,559 --> 00:33:07,600
milliseconds because i want to make sure

00:33:05,919 --> 00:33:09,919
i have tons of headroom

00:33:07,600 --> 00:33:10,960
so now i'm going to do a further

00:33:09,919 --> 00:33:13,919
enhancement

00:33:10,960 --> 00:33:14,559
um we have a table in here called promo

00:33:13,919 --> 00:33:16,559
codes

00:33:14,559 --> 00:33:19,120
which is really a global table right we

00:33:16,559 --> 00:33:20,880
don't have new york only promo codes

00:33:19,120 --> 00:33:22,559
right so what we're going to do is we're

00:33:20,880 --> 00:33:24,799
going to create some

00:33:22,559 --> 00:33:26,480
a global index which is going to allow

00:33:24,799 --> 00:33:28,080
us to do fast reads

00:33:26,480 --> 00:33:30,240
from all of the regions against that

00:33:28,080 --> 00:33:33,200
table uh at the cost of a

00:33:30,240 --> 00:33:34,880
write performance hit so if i were to

00:33:33,200 --> 00:33:37,840
add a record to this table

00:33:34,880 --> 00:33:38,559
it's um we're going to be writing to

00:33:37,840 --> 00:33:40,799
some

00:33:38,559 --> 00:33:41,600
additional indexes to make sure that we

00:33:40,799 --> 00:33:43,039
do fast reads

00:33:41,600 --> 00:33:44,640
so our right performance will suffer but

00:33:43,039 --> 00:33:46,159
our read performance will get

00:33:44,640 --> 00:33:48,000
dramatically better so i'm going to go

00:33:46,159 --> 00:33:50,720
ahead and um

00:33:48,000 --> 00:33:52,320
and build those indexes um you can see

00:33:50,720 --> 00:33:53,600
they're already kind of starting to take

00:33:52,320 --> 00:33:57,919
effect

00:33:53,600 --> 00:33:59,840
um and the net effect of this

00:33:57,919 --> 00:34:01,919
is we're gonna we're gonna have

00:33:59,840 --> 00:34:05,200
consistent performance

00:34:01,919 --> 00:34:08,240
in the database of sub 10 milliseconds

00:34:05,200 --> 00:34:09,679
for 99 out of 100 of our queries

00:34:08,240 --> 00:34:12,720
that means that we could theoretically

00:34:09,679 --> 00:34:16,720
do 10 database transactions

00:34:12,720 --> 00:34:20,399
um within a kind of a real-time app

00:34:16,720 --> 00:34:22,639
um like user um

00:34:20,399 --> 00:34:23,919
interaction if we absolutely had to that

00:34:22,639 --> 00:34:26,480
would be

00:34:23,919 --> 00:34:28,720
a little unusual right usually it's one

00:34:26,480 --> 00:34:32,079
transaction or maybe two or three

00:34:28,720 --> 00:34:35,520
um this is going to allow us to

00:34:32,079 --> 00:34:37,520
um to potentially have you know 10 or

00:34:35,520 --> 00:34:40,240
more if we really needed to

00:34:37,520 --> 00:34:40,879
um now what you're also going to notice

00:34:40,240 --> 00:34:42,800
is our

00:34:40,879 --> 00:34:44,560
our queries per second isn't going to

00:34:42,800 --> 00:34:47,040
change dramatically with these

00:34:44,560 --> 00:34:48,639
with this change because this is only

00:34:47,040 --> 00:34:50,560
impacting like two percent of the

00:34:48,639 --> 00:34:54,240
queries in this application

00:34:50,560 --> 00:34:56,000
um but our p99 latency is going to get

00:34:54,240 --> 00:34:58,560
is going to continue to get better until

00:34:56,000 --> 00:35:00,240
it lands at like

00:34:58,560 --> 00:35:02,079
i think i think it lands somewhere

00:35:00,240 --> 00:35:05,599
between four and five milliseconds

00:35:02,079 --> 00:35:08,160
if you let it um um once

00:35:05,599 --> 00:35:09,440
once the database itself continues to

00:35:08,160 --> 00:35:10,079
because the database as you mentioned

00:35:09,440 --> 00:35:12,000
earlier

00:35:10,079 --> 00:35:13,200
self optimizes under the covers as well

00:35:12,000 --> 00:35:16,400
so once that

00:35:13,200 --> 00:35:19,920
optimization is done um

00:35:16,400 --> 00:35:22,480
then you'll be um

00:35:19,920 --> 00:35:23,839
we're going to be kind of stick pretty

00:35:22,480 --> 00:35:25,280
solidly in that three to four

00:35:23,839 --> 00:35:27,280
millisecond range you see we're

00:35:25,280 --> 00:35:29,520
we're already kind of getting there

00:35:27,280 --> 00:35:32,480
right now

00:35:29,520 --> 00:35:33,200
um so that that was what i had to show

00:35:32,480 --> 00:35:36,400
today

00:35:33,200 --> 00:35:37,520
jim um yeah and so keith basically you

00:35:36,400 --> 00:35:39,680
could then

00:35:37,520 --> 00:35:41,440
alter the yeah i mean the the

00:35:39,680 --> 00:35:43,119
partitioning as well and the database is

00:35:41,440 --> 00:35:44,960
gonna start moving data

00:35:43,119 --> 00:35:46,800
around correct like i mean this was all

00:35:44,960 --> 00:35:50,880
done in production correct

00:35:46,800 --> 00:35:52,240
yeah so um there are times when we move

00:35:50,880 --> 00:35:54,720
data

00:35:52,240 --> 00:35:56,400
um they're more frequently what we're

00:35:54,720 --> 00:35:58,240
actually doing is moving the authority

00:35:56,400 --> 00:36:00,240
to act on that data

00:35:58,240 --> 00:36:01,599
okay so the leaseholder and the ref

00:36:00,240 --> 00:36:04,640
leader those are the

00:36:01,599 --> 00:36:07,680
the that's the authority to um

00:36:04,640 --> 00:36:10,240
read and write that range respectively

00:36:07,680 --> 00:36:10,800
so what how the way cockroachdb works is

00:36:10,240 --> 00:36:13,920
that

00:36:10,800 --> 00:36:16,000
every node is responsible for some

00:36:13,920 --> 00:36:19,200
portion of the data in the database

00:36:16,000 --> 00:36:22,079
right um so

00:36:19,200 --> 00:36:23,440
and in a normally configured environment

00:36:22,079 --> 00:36:24,800
you could theoretically break this if

00:36:23,440 --> 00:36:26,160
you wanted to but i don't know why you

00:36:24,800 --> 00:36:29,520
would yeah um

00:36:26,160 --> 00:36:33,040
every node is going to be the leader

00:36:29,520 --> 00:36:34,880
for some portion of the data follower

00:36:33,040 --> 00:36:37,200
a follower for other portions of the

00:36:34,880 --> 00:36:37,760
data and completely uninvolved in

00:36:37,200 --> 00:36:41,040
certain

00:36:37,760 --> 00:36:42,160
sections of the data right so so most of

00:36:41,040 --> 00:36:45,920
what i did here

00:36:42,160 --> 00:36:49,680
was actually moving the authority to act

00:36:45,920 --> 00:36:51,760
right if if we set a rule where

00:36:49,680 --> 00:36:53,760
the authority to act so with those

00:36:51,760 --> 00:36:54,960
partitions schemas let's say i had four

00:36:53,760 --> 00:36:57,760
data centers here

00:36:54,960 --> 00:36:58,240
and i decided for any particular you

00:36:57,760 --> 00:37:00,880
know

00:36:58,240 --> 00:37:02,079
uh partition i wanted to move to the

00:37:00,880 --> 00:37:05,599
authority to act

00:37:02,079 --> 00:37:07,359
to a location that it we didn't have a

00:37:05,599 --> 00:37:09,680
replica

00:37:07,359 --> 00:37:11,040
right then the database is going to move

00:37:09,680 --> 00:37:12,960
the data for you yes

00:37:11,040 --> 00:37:14,320
so so you don't have to make you don't

00:37:12,960 --> 00:37:16,000
have to manually load the data in the

00:37:14,320 --> 00:37:17,920
background and do all this kind of stuff

00:37:16,000 --> 00:37:19,920
right we will handle that for you but

00:37:17,920 --> 00:37:20,960
it's much more likely that we're going

00:37:19,920 --> 00:37:24,160
to

00:37:20,960 --> 00:37:25,359
move the authority to to act on the data

00:37:24,160 --> 00:37:26,079
than it is that we're going to move the

00:37:25,359 --> 00:37:27,760
data

00:37:26,079 --> 00:37:29,200
yeah the exceptions to be that would be

00:37:27,760 --> 00:37:31,440
like hot ranges

00:37:29,200 --> 00:37:32,960
right if we take let's say we're having

00:37:31,440 --> 00:37:35,119
a lot of transactions across

00:37:32,960 --> 00:37:36,800
a very small number of ranges then we

00:37:35,119 --> 00:37:37,280
might have some things that we would do

00:37:36,800 --> 00:37:39,280
there

00:37:37,280 --> 00:37:40,800
hey dude we only have four minutes and

00:37:39,280 --> 00:37:42,240
then we gotta cut this thing down

00:37:40,800 --> 00:37:43,839
off so i just want to make sure i get to

00:37:42,240 --> 00:37:45,119
like there were like two other questions

00:37:43,839 --> 00:37:46,480
and i'm just gonna share my screen to

00:37:45,119 --> 00:37:48,000
have that wrap up slides so while we're

00:37:46,480 --> 00:37:48,800
taking qa people have that if you can

00:37:48,000 --> 00:37:50,960
just turn

00:37:48,800 --> 00:37:52,400
off screen that's great um so one of the

00:37:50,960 --> 00:37:53,839
questions and it came in a couple

00:37:52,400 --> 00:37:56,320
different ways um

00:37:53,839 --> 00:37:57,680
how about backup restore uh and how does

00:37:56,320 --> 00:37:59,920
that work in cockroach db

00:37:57,680 --> 00:38:01,200
because it's a unique problem right

00:37:59,920 --> 00:38:02,800
because we're

00:38:01,200 --> 00:38:04,000
guaranteeing data lives in locations

00:38:02,800 --> 00:38:05,040
right and so how does that work in

00:38:04,000 --> 00:38:08,320
cockroach

00:38:05,040 --> 00:38:11,359
yeah so um so the core database that's

00:38:08,320 --> 00:38:14,240
the free to use open source database

00:38:11,359 --> 00:38:15,920
includes uh backup and restore to a

00:38:14,240 --> 00:38:17,920
single location so

00:38:15,920 --> 00:38:19,040
you know let's say you add an s3 bucket

00:38:17,920 --> 00:38:22,000
in aws

00:38:19,040 --> 00:38:22,800
or you know or whatnot you could do you

00:38:22,000 --> 00:38:25,760
know

00:38:22,800 --> 00:38:27,119
backup to s3 and all of the nodes would

00:38:25,760 --> 00:38:28,640
in your database would just need to be

00:38:27,119 --> 00:38:32,720
able to write to that

00:38:28,640 --> 00:38:33,599
s3 bucket um the enterprise um so we're

00:38:32,720 --> 00:38:36,720
kind of

00:38:33,599 --> 00:38:37,760
we we're the entire data space is source

00:38:36,720 --> 00:38:39,760
available certain

00:38:37,760 --> 00:38:41,280
features are licensed as enterprise

00:38:39,760 --> 00:38:42,320
features that you have to pay cockroach

00:38:41,280 --> 00:38:44,720
labs for

00:38:42,320 --> 00:38:46,880
um if you have an enterprise license we

00:38:44,720 --> 00:38:48,560
also support distributed backup that

00:38:46,880 --> 00:38:52,000
allows you to have

00:38:48,560 --> 00:38:54,079
an arbitrary number of nodes back

00:38:52,000 --> 00:38:56,320
up to an arbitrary number of locations

00:38:54,079 --> 00:38:58,480
so let's say for my

00:38:56,320 --> 00:39:00,079
you know simulated use case here where

00:38:58,480 --> 00:39:03,200
i'm in

00:39:00,079 --> 00:39:04,960
u.s east u.s central and us west i could

00:39:03,200 --> 00:39:06,160
theoretically have the nodes in each of

00:39:04,960 --> 00:39:09,359
those locations

00:39:06,160 --> 00:39:09,920
back up to a local object store and then

00:39:09,359 --> 00:39:11,440
i could

00:39:09,920 --> 00:39:13,280
you know move those replicas around in

00:39:11,440 --> 00:39:14,960
the background that's going to be faster

00:39:13,280 --> 00:39:17,680
of course in a distributed

00:39:14,960 --> 00:39:19,520
scenario where i'm backing up um to

00:39:17,680 --> 00:39:21,599
something that's closer

00:39:19,520 --> 00:39:23,040
right but all of our no all of our

00:39:21,599 --> 00:39:25,760
backups are distributed it's

00:39:23,040 --> 00:39:27,280
a question of whether or not they're

00:39:25,760 --> 00:39:29,359
it's a

00:39:27,280 --> 00:39:31,200
one distribution or a many-to-many

00:39:29,359 --> 00:39:31,520
distribution and the latter one is what

00:39:31,200 --> 00:39:33,119
we

00:39:31,520 --> 00:39:34,720
we charge customers yeah and the latter

00:39:33,119 --> 00:39:36,320
one basically i mean it's used in

00:39:34,720 --> 00:39:37,599
you know typically we see customers who

00:39:36,320 --> 00:39:39,760
are trying to deal with like you know

00:39:37,599 --> 00:39:40,240
compliancy regulations with our database

00:39:39,760 --> 00:39:41,760
and

00:39:40,240 --> 00:39:43,760
you know like where the with a backup

00:39:41,760 --> 00:39:46,240
itself needs to be

00:39:43,760 --> 00:39:47,599
controlled from jurisdictional issues

00:39:46,240 --> 00:39:49,920
right and i think it's you know it's one

00:39:47,599 --> 00:39:53,440
of those more advanced features so

00:39:49,920 --> 00:39:54,480
um another question keith was um oh gosh

00:39:53,440 --> 00:39:56,000
oh there was a

00:39:54,480 --> 00:39:57,440
there was there were you guys there were

00:39:56,000 --> 00:39:58,880
a lot of really great questions i think

00:39:57,440 --> 00:40:00,000
one that's kind of easy to pick off and

00:39:58,880 --> 00:40:01,839
to go through

00:40:00,000 --> 00:40:03,200
what about when you run like a query

00:40:01,839 --> 00:40:04,000
that's hitting all the records you know

00:40:03,200 --> 00:40:06,400
like a sum

00:40:04,000 --> 00:40:08,079
or a report you know because we don't

00:40:06,400 --> 00:40:09,839
want to range scan the entire

00:40:08,079 --> 00:40:11,440
database right we're all over the place

00:40:09,839 --> 00:40:12,800
like how does cockroach deal with those

00:40:11,440 --> 00:40:15,599
sort of things keith

00:40:12,800 --> 00:40:17,520
yeah so there are a couple of different

00:40:15,599 --> 00:40:18,960
strategies for me remember we only have

00:40:17,520 --> 00:40:19,760
about one minute so just so you know

00:40:18,960 --> 00:40:22,880
yeah so

00:40:19,760 --> 00:40:24,560
um that would be a sub-optimal query

00:40:22,880 --> 00:40:27,200
pattern for cockroachdb the

00:40:24,560 --> 00:40:29,119
the easiest thing to do would be to run

00:40:27,200 --> 00:40:29,599
what we call an as of system time or a

00:40:29,119 --> 00:40:32,480
time

00:40:29,599 --> 00:40:33,040
travel query um that way that that long

00:40:32,480 --> 00:40:36,240
running

00:40:33,040 --> 00:40:37,680
kind of aggregate query isn't blocking

00:40:36,240 --> 00:40:40,960
other transactions

00:40:37,680 --> 00:40:42,720
all right we're serializably isolated so

00:40:40,960 --> 00:40:45,280
if you weren't to do that then then you

00:40:42,720 --> 00:40:47,280
potentially have some performance issues

00:40:45,280 --> 00:40:49,119
um there are some other mechanisms you

00:40:47,280 --> 00:40:52,160
can use um

00:40:49,119 --> 00:40:55,040
with uh with ctes and

00:40:52,160 --> 00:40:56,480
um and kind of pre-calculated aggregates

00:40:55,040 --> 00:40:57,760
like we could theoretically create an

00:40:56,480 --> 00:40:59,680
index that included

00:40:57,760 --> 00:41:02,400
the some of those pre-calculated index

00:40:59,680 --> 00:41:04,319
um but we also i mean we also implement

00:41:02,400 --> 00:41:06,400
vectorized queries so we can actually do

00:41:04,319 --> 00:41:08,480
sum of columns pretty easily as well

00:41:06,400 --> 00:41:10,880
right so yeah so vectorized

00:41:08,480 --> 00:41:11,839
um you know the vectorized execution

00:41:10,880 --> 00:41:13,680
engine would

00:41:11,839 --> 00:41:16,000
help the performance there but we're

00:41:13,680 --> 00:41:17,760
still going to have to hit all of the

00:41:16,000 --> 00:41:19,440
leaders for all the ranges that host

00:41:17,760 --> 00:41:22,240
that data

00:41:19,440 --> 00:41:23,280
that'll just eliminate some of the like

00:41:22,240 --> 00:41:25,839
overhead of

00:41:23,280 --> 00:41:26,560
of the transactional engine there that's

00:41:25,839 --> 00:41:28,319
right

00:41:26,560 --> 00:41:29,680
that's right i mean i mean we've

00:41:28,319 --> 00:41:31,040
optimized a lot of things there's also a

00:41:29,680 --> 00:41:33,200
cost based optimizer

00:41:31,040 --> 00:41:35,119
in cockroachdb that deals with like the

00:41:33,200 --> 00:41:36,560
distributed nature of the data there's

00:41:35,119 --> 00:41:38,480
there's lots of things i mean you could

00:41:36,560 --> 00:41:39,680
actually go and inspect queries and see

00:41:38,480 --> 00:41:41,280
what's holding things up

00:41:39,680 --> 00:41:43,280
we have just barely scratched the

00:41:41,280 --> 00:41:46,480
surface in this webinar today

00:41:43,280 --> 00:41:47,520
um you know cockroachdb is available off

00:41:46,480 --> 00:41:49,280
our website i mean

00:41:47,520 --> 00:41:51,040
the core version the free downloadable

00:41:49,280 --> 00:41:51,440
version is is available offer so i just

00:41:51,040 --> 00:41:53,760
get

00:41:51,440 --> 00:41:56,160
get copper hdb we have a world of

00:41:53,760 --> 00:41:58,000
resources around

00:41:56,160 --> 00:42:00,160
kubernetes in terms of you know how well

00:41:58,000 --> 00:42:02,400
we work with this we do feel that we are

00:42:00,160 --> 00:42:04,800
probably the the best database that is

00:42:02,400 --> 00:42:06,319
really designed to run on kubernetes in

00:42:04,800 --> 00:42:07,839
fact cockroach cloud

00:42:06,319 --> 00:42:10,000
um which is our managed service of

00:42:07,839 --> 00:42:10,560
cockroachdb runs on kubernetes we have

00:42:10,000 --> 00:42:13,520
deployed

00:42:10,560 --> 00:42:14,560
thousands of clusters on cockroach on

00:42:13,520 --> 00:42:16,000
kubernetes

00:42:14,560 --> 00:42:17,839
uh in cockroach cloud a lot of what

00:42:16,000 --> 00:42:18,319
we've packaged into the operator is that

00:42:17,839 --> 00:42:19,760
that

00:42:18,319 --> 00:42:21,760
that knowledge that we've actually

00:42:19,760 --> 00:42:23,359
captured but um that is really the

00:42:21,760 --> 00:42:24,720
easiest way to get up and running and

00:42:23,359 --> 00:42:26,560
try this today we actually give away

00:42:24,720 --> 00:42:28,560
free clusters for about 30 days you know

00:42:26,560 --> 00:42:30,880
stay tuned for you know

00:42:28,560 --> 00:42:32,400
unlimited free clusters uh in in our

00:42:30,880 --> 00:42:33,359
future and so there's some really cool

00:42:32,400 --> 00:42:35,760
stuff coming out of us

00:42:33,359 --> 00:42:37,839
so um keith thank you for that i want to

00:42:35,760 --> 00:42:39,680
be respectful of you know the linux

00:42:37,839 --> 00:42:40,960
foundation and our time requirements you

00:42:39,680 --> 00:42:42,240
and i can sit here and talk about this

00:42:40,960 --> 00:42:44,880
stuff for days

00:42:42,240 --> 00:42:46,880
um like i said i think we just really

00:42:44,880 --> 00:42:49,359
just scratched the surface today

00:42:46,880 --> 00:42:50,319
um and and also there were a lot of

00:42:49,359 --> 00:42:53,680
questions that

00:42:50,319 --> 00:42:54,240
came into the the qa um i will get these

00:42:53,680 --> 00:42:56,400
from

00:42:54,240 --> 00:42:58,560
um from the lf and and get answers out

00:42:56,400 --> 00:43:00,720
to everybody as much as we can so

00:42:58,560 --> 00:43:02,560
um keith thank you thank you appreciate

00:43:00,720 --> 00:43:06,000
it yes thank you both

00:43:02,560 --> 00:43:08,000
so much thanks jim thanks keith

00:43:06,000 --> 00:43:09,680
and as we mentioned earlier this

00:43:08,000 --> 00:43:11,440
recording will be available up on the

00:43:09,680 --> 00:43:13,520
youtube

00:43:11,440 --> 00:43:14,480
later today so thank you all for

00:43:13,520 --> 00:43:18,200
participating

00:43:14,480 --> 00:43:21,200
and we hope you have a great day thanks

00:43:18,200 --> 00:43:21,200

YouTube URL: https://www.youtube.com/watch?v=tAXze1wQ40A


