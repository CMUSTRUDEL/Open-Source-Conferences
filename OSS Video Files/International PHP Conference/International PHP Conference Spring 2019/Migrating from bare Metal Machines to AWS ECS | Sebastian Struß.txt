Title: Migrating from bare Metal Machines to AWS ECS | Sebastian StruÃŸ
Publication date: 2019-08-28
Playlist: International PHP Conference Spring 2019
Description: 
	Speaker: Sebastian StruÃŸ (AppLike) | https://phpconference.com/speaker/sebastian-strus/

I will talk about how AppLike moved from bare metal machines to AWS ECS (and vagrant to Docker in the development environment).
Part of the talk will be:
 - changes needed within the application and its configuration (like DotEnv component for symfony)
 - pitfalls and issues when starting to use AWS ECS
 - changes in the deployment strategy

ðŸ¤— Come, join us at the next International PHP Conference | https://phpconference.com/
ðŸ‘ Like us on Facebook | https://www.facebook.com/ipc.germany/
ðŸ‘‰ Follow us on Twitter | https://twitter.com/phpconference
Captions: 
	00:00:00,250 --> 00:00:06,990
[Music]

00:00:04,370 --> 00:00:08,880
thank you for attending my talk we are

00:00:06,990 --> 00:00:11,700
going to talk about migrating from bare

00:00:08,880 --> 00:00:18,449
metal machines to AWS dcs a real-world

00:00:11,700 --> 00:00:29,699
scenario as soon as this works

00:00:18,449 --> 00:00:32,820
come on yeah yeah first few words about

00:00:29,699 --> 00:00:35,489
myself my name is Austin schmooze I've

00:00:32,820 --> 00:00:38,090
been working with : for about a year and

00:00:35,489 --> 00:00:41,879
with PHP for about 20 years

00:00:38,090 --> 00:00:44,520
wait for about 10 years sorry and I'm a

00:00:41,879 --> 00:00:50,910
technology enthusiast since about 20

00:00:44,520 --> 00:00:54,000
years and yeah this is my first talk so

00:00:50,910 --> 00:00:58,859
let's first see well what we are going

00:00:54,000 --> 00:01:01,680
to talk about first we are going to talk

00:00:58,859 --> 00:01:05,220
about the motivation why we needed to

00:01:01,680 --> 00:01:07,650
change to AWS ECS instead of just using

00:01:05,220 --> 00:01:10,520
bare metal machines then we're going to

00:01:07,650 --> 00:01:12,990
talk about how the stack looked before

00:01:10,520 --> 00:01:14,780
how we changed it in our development

00:01:12,990 --> 00:01:18,479
environment from break current to docker

00:01:14,780 --> 00:01:23,400
then the containerization from our bare

00:01:18,479 --> 00:01:25,229
metal machines to AWS CCS then we're

00:01:23,400 --> 00:01:27,869
going to talk about the changes and the

00:01:25,229 --> 00:01:30,329
application in its configuration the

00:01:27,869 --> 00:01:32,759
changes in our deployment strategy and

00:01:30,329 --> 00:01:41,090
the pitfalls when starting to use AWS

00:01:32,759 --> 00:01:44,129
CCS and of course such an opening

00:01:41,090 --> 00:01:44,129
[Music]

00:01:49,009 --> 00:01:52,790
and in the end we are going to talk

00:01:50,600 --> 00:01:55,100
about what's what's our next steps and

00:01:52,790 --> 00:01:58,689
after all we are going to have a QA

00:01:55,100 --> 00:02:03,350
where you can ask any questions you like

00:01:58,689 --> 00:02:06,399
so motivation why did we need it first

00:02:03,350 --> 00:02:09,110
of all we want to get more flexible and

00:02:06,399 --> 00:02:11,930
have a better performance when scaling

00:02:09,110 --> 00:02:15,170
of course in the first place we had when

00:02:11,930 --> 00:02:17,269
we had ec2 instances on bare metal we

00:02:15,170 --> 00:02:19,850
had the issue we needed to spin up in

00:02:17,269 --> 00:02:23,350
your machine then deploy some web server

00:02:19,850 --> 00:02:26,510
honor to get the configuration copy that

00:02:23,350 --> 00:02:30,319
deploy the application itself and get

00:02:26,510 --> 00:02:32,450
everything set up so that's where AWS

00:02:30,319 --> 00:02:35,480
ECS is a bit better because we can just

00:02:32,450 --> 00:02:39,410
on containers and be good or spawn

00:02:35,480 --> 00:02:41,930
instances then spawn containers and yeah

00:02:39,410 --> 00:02:44,209
next thing is making the ecosystem

00:02:41,930 --> 00:02:48,080
easier deployable that's actually what I

00:02:44,209 --> 00:02:51,079
already mentioned so why we needed it is

00:02:48,080 --> 00:02:53,090
our requirements change over time

00:02:51,079 --> 00:02:55,430
everyone knows you develop a feature

00:02:53,090 --> 00:02:59,030
today works great and then in the end

00:02:55,430 --> 00:03:01,609
with user increases it was increasing

00:02:59,030 --> 00:03:03,470
user rate is he okay

00:03:01,609 --> 00:03:08,630
this doesn't perform anymore so we need

00:03:03,470 --> 00:03:12,829
to scale then next point is load

00:03:08,630 --> 00:03:15,079
increase yeah our performance need to be

00:03:12,829 --> 00:03:21,079
improved if our features don't perform

00:03:15,079 --> 00:03:24,200
well and yeah expected q2 account

00:03:21,079 --> 00:03:29,840
increase so yeah also need more

00:03:24,200 --> 00:03:31,639
performance need more scalability this

00:03:29,840 --> 00:03:33,950
is how our stack looked before the

00:03:31,639 --> 00:03:36,170
migration we have a document storage on

00:03:33,950 --> 00:03:38,030
multiple bare metal machines and we have

00:03:36,170 --> 00:03:41,180
a message broker also on multiple

00:03:38,030 --> 00:03:43,850
machines with a lock transformer we have

00:03:41,180 --> 00:03:46,609
bare metal machines for our API that

00:03:43,850 --> 00:03:49,760
contain a web server of the application

00:03:46,609 --> 00:03:52,970
itself a key value storage and also we

00:03:49,760 --> 00:03:55,620
use some of our AWS hosted services not

00:03:52,970 --> 00:03:59,110
all elicitor

00:03:55,620 --> 00:04:01,300
this is how our development environment

00:03:59,110 --> 00:04:04,660
looked like with everything in one big

00:04:01,300 --> 00:04:06,730
vagrant box not very optimal if you want

00:04:04,660 --> 00:04:09,459
to reflect something how it looks in

00:04:06,730 --> 00:04:12,400
production so we changed that to occur

00:04:09,459 --> 00:04:16,120
with compose and have every component in

00:04:12,400 --> 00:04:19,600
its own container so it looks much more

00:04:16,120 --> 00:04:26,440
like an AWS DCs and in combination with

00:04:19,600 --> 00:04:30,070
the AWS services this is an example of

00:04:26,440 --> 00:04:32,440
our docker compose file looks like so we

00:04:30,070 --> 00:04:35,860
have an H a proxy which is kind of like

00:04:32,440 --> 00:04:38,110
load balancer and 800 CCS because you're

00:04:35,860 --> 00:04:41,710
not directly talking to the API and

00:04:38,110 --> 00:04:44,980
production as well then we have our

00:04:41,710 --> 00:04:47,740
nginx we have our PHP app like backend

00:04:44,980 --> 00:04:50,440
we have the document storage a block

00:04:47,740 --> 00:04:52,960
transformer and message broker and local

00:04:50,440 --> 00:04:58,240
stack to emulate our AWS services that

00:04:52,960 --> 00:05:01,450
we use in the cloud as you can see we

00:04:58,240 --> 00:05:03,550
are using the built property there to

00:05:01,450 --> 00:05:06,729
build the context because we want to put

00:05:03,550 --> 00:05:09,340
our custom nginx configuration in that

00:05:06,729 --> 00:05:12,760
container and also we want to fix some

00:05:09,340 --> 00:05:19,690
permission issues we had by setting a

00:05:12,760 --> 00:05:22,240
UID and GID with fixed your ID yeah so

00:05:19,690 --> 00:05:28,270
this is how it and the end looked like

00:05:22,240 --> 00:05:31,090
after going the ECS way we have an ICS

00:05:28,270 --> 00:05:34,510
cluster which host or API and the worker

00:05:31,090 --> 00:05:38,289
nodes and all of them are coupled and

00:05:34,510 --> 00:05:41,620
tasks which are for API and worker and

00:05:38,289 --> 00:05:43,479
scheduler of course which contained a

00:05:41,620 --> 00:05:46,440
webserver at least for the API the

00:05:43,479 --> 00:05:49,960
application and the key value storage

00:05:46,440 --> 00:05:52,840
all of the other services we left

00:05:49,960 --> 00:05:54,970
untouched for now because our first goal

00:05:52,840 --> 00:05:57,539
was to have the application itself be

00:05:54,970 --> 00:05:57,539
scalable

00:05:59,800 --> 00:06:04,460
changes to the application where

00:06:01,850 --> 00:06:07,550
basically minimal we just added the end

00:06:04,460 --> 00:06:10,190
component for Symphony so we are able to

00:06:07,550 --> 00:06:13,220
read environment parameters as

00:06:10,190 --> 00:06:16,280
configuration instead of having

00:06:13,220 --> 00:06:18,500
everything in the parameter C and so we

00:06:16,280 --> 00:06:21,199
are actually more modular with that and

00:06:18,500 --> 00:06:25,660
can deploy the same image in every

00:06:21,199 --> 00:06:28,010
environment mm-hmm and to not store any

00:06:25,660 --> 00:06:30,710
configuration or secrets in our

00:06:28,010 --> 00:06:34,639
parameters you know by just reading it

00:06:30,710 --> 00:06:38,090
from the environment another positive

00:06:34,639 --> 00:06:40,340
thing there is we can tell it how to how

00:06:38,090 --> 00:06:42,680
to parse a value so we can say ok the

00:06:40,340 --> 00:06:45,110
debug toolbar that's a bool either true

00:06:42,680 --> 00:06:47,389
or false and we can also say ok

00:06:45,110 --> 00:06:49,190
interpret this the input as jason and

00:06:47,389 --> 00:06:54,100
then we can have our array or object

00:06:49,190 --> 00:06:54,100
there too we use it

00:06:55,300 --> 00:07:02,330
next are the changes to the deployment

00:06:58,160 --> 00:07:04,340
strategy where we do not use when we do

00:07:02,330 --> 00:07:08,300
not need to use automation tools anymore

00:07:04,340 --> 00:07:11,090
over 24 when we actually always deployed

00:07:08,300 --> 00:07:13,310
our engineers its configuration which be

00:07:11,090 --> 00:07:21,260
ended configuration and the application

00:07:13,310 --> 00:07:23,210
itself instead of that we use our people

00:07:21,260 --> 00:07:26,479
containers that we can use for every

00:07:23,210 --> 00:07:30,229
environment and another and another

00:07:26,479 --> 00:07:33,770
change is the use of AWS and that

00:07:30,229 --> 00:07:36,680
enables us to use the AWS SSM parameter

00:07:33,770 --> 00:07:38,840
store it's where you can store every

00:07:36,680 --> 00:07:41,270
configuration and every secret that you

00:07:38,840 --> 00:07:43,910
have for your application and just tell

00:07:41,270 --> 00:07:46,370
it ok use the space path and then it

00:07:43,910 --> 00:07:48,710
will download it and put it into your

00:07:46,370 --> 00:07:52,699
environment so the application can use

00:07:48,710 --> 00:07:56,030
it in combination with dot n next change

00:07:52,699 --> 00:07:58,669
was to use ECS deploy another handy tool

00:07:56,030 --> 00:08:03,860
both of them by the way are available in

00:07:58,669 --> 00:08:07,580
github ICS deploy is good for well

00:08:03,860 --> 00:08:11,060
deploying obviously it changes your

00:08:07,580 --> 00:08:15,620
service and your task definition too

00:08:11,060 --> 00:08:17,480
the new world container and you can use

00:08:15,620 --> 00:08:20,000
it also for rolling back if something

00:08:17,480 --> 00:08:22,430
went wrong and you can specify the time

00:08:20,000 --> 00:08:25,240
art to say okay if it takes longer than

00:08:22,430 --> 00:08:28,310
this it doesn't work

00:08:25,240 --> 00:08:32,660
next thing is rolling upgrades and

00:08:28,310 --> 00:08:34,760
downgrades maybe some of you or many of

00:08:32,660 --> 00:08:38,750
you have already experienced that we

00:08:34,760 --> 00:08:41,690
didn't do it before but using ICS deploy

00:08:38,750 --> 00:08:50,230
we are then able to do that or using it

00:08:41,690 --> 00:08:54,370
obviously CS next thing is pitfalls so

00:08:50,230 --> 00:08:57,380
with the container placement strategy

00:08:54,370 --> 00:08:59,600
you always want to ensure that your API

00:08:57,380 --> 00:09:02,600
is not hosted on the same machine as

00:08:59,600 --> 00:09:08,390
like a worker or so because they can be

00:09:02,600 --> 00:09:11,540
IO they can have have heavy io usage or

00:09:08,390 --> 00:09:15,560
heavy CPU usage or memory usage all of

00:09:11,540 --> 00:09:17,600
them you don't want to have as a limit

00:09:15,560 --> 00:09:23,150
on your API it's because they respond

00:09:17,600 --> 00:09:26,060
slower then yeah next is ec2 instances

00:09:23,150 --> 00:09:28,640
and resource limits as already mentioned

00:09:26,060 --> 00:09:30,860
you don't want to use your resources to

00:09:28,640 --> 00:09:34,570
the next because if you do and you're

00:09:30,860 --> 00:09:37,030
going to have trouble with their API and

00:09:34,570 --> 00:09:39,740
minimum and maximum healthy percentage

00:09:37,030 --> 00:09:41,420
there we had a funny issue in the

00:09:39,740 --> 00:09:44,210
beginning when we when we brought our

00:09:41,420 --> 00:09:46,490
sandbox up we had issues deploying our

00:09:44,210 --> 00:09:49,250
stuff because we had only one API as a

00:09:46,490 --> 00:09:53,090
desired count and our minimum and

00:09:49,250 --> 00:09:58,550
maximum health at 100% so yeah not

00:09:53,090 --> 00:10:01,670
deployable but in the end AWS actually

00:09:58,550 --> 00:10:02,840
fixed that and now we say hey by the way

00:10:01,670 --> 00:10:07,220
if you do that

00:10:02,840 --> 00:10:09,170
you won't be able to deploy so when you

00:10:07,220 --> 00:10:11,270
only have desired count of one you also

00:10:09,170 --> 00:10:13,310
have to take into consideration other

00:10:11,270 --> 00:10:17,120
setting the minimum to zero or the

00:10:13,310 --> 00:10:18,390
maximum greater than 100 or greater than

00:10:17,120 --> 00:10:22,270
00:10:18,390 --> 00:10:25,420
next is PHP applications with frameworks

00:10:22,270 --> 00:10:27,580
they can be slow warming up the caches

00:10:25,420 --> 00:10:30,790
can take quite a bit of time depending

00:10:27,580 --> 00:10:32,950
on the size of the application we did

00:10:30,790 --> 00:10:34,930
that while deploying and still for some

00:10:32,950 --> 00:10:39,010
reason the first requests on the API

00:10:34,930 --> 00:10:42,160
were but slow so they have checked did

00:10:39,010 --> 00:10:44,500
also take quite a bit of time and yeah

00:10:42,160 --> 00:10:48,340
if that takes long then also your

00:10:44,500 --> 00:10:50,610
service doesn't go in service fast so

00:10:48,340 --> 00:10:55,840
you have to take that into consideration

00:10:50,610 --> 00:10:58,690
another thing is easy s can be slow when

00:10:55,840 --> 00:11:01,660
using docker maybe many of you know

00:10:58,690 --> 00:11:04,960
already you should include like large

00:11:01,660 --> 00:11:07,380
images we have huge dependencies inside

00:11:04,960 --> 00:11:09,880
so you have to take into consideration

00:11:07,380 --> 00:11:11,950
whenever you deploy something it needs

00:11:09,880 --> 00:11:15,400
to get download from ECR which is the

00:11:11,950 --> 00:11:22,690
repository the docker repository for PCs

00:11:15,400 --> 00:11:26,590
and yeah next thing our chrome task and

00:11:22,690 --> 00:11:29,020
the runtimes as already said with the

00:11:26,590 --> 00:11:31,900
PHP applications it can be slow and

00:11:29,020 --> 00:11:35,200
there are also some strange issues we

00:11:31,900 --> 00:11:38,860
encountered when using AWS UCF regarding

00:11:35,200 --> 00:11:41,320
front us they usually run for us five

00:11:38,860 --> 00:11:45,120
minutes on bare metal and on ACS we have

00:11:41,320 --> 00:11:48,520
strange issues with like 7 minutes or so

00:11:45,120 --> 00:11:51,580
in the end we fixed it with locking here

00:11:48,520 --> 00:11:57,340
and there and somehow it didn't work but

00:11:51,580 --> 00:12:02,580
yeah next thing is our lifecycle

00:11:57,340 --> 00:12:05,920
policies when putting images to ECR you

00:12:02,580 --> 00:12:08,430
quite fast get into the account limit

00:12:05,920 --> 00:12:11,860
which is I think four thousand images

00:12:08,430 --> 00:12:14,260
four thousand texts per image so say you

00:12:11,860 --> 00:12:14,710
have an application image for ten times

00:12:14,260 --> 00:12:17,620
a day

00:12:14,710 --> 00:12:19,180
if you develop us then you reach a limit

00:12:17,620 --> 00:12:23,110
quite fast so you need a lifecycle

00:12:19,180 --> 00:12:25,960
policy to delete your images when unused

00:12:23,110 --> 00:12:29,710
like say we want to delete images that

00:12:25,960 --> 00:12:30,430
are intact after a day or so and let's

00:12:29,710 --> 00:12:34,420
say we

00:12:30,430 --> 00:12:39,310
to delete sandbox images that are older

00:12:34,420 --> 00:12:47,800
than 10 days or like less than more than

00:12:39,310 --> 00:12:52,149
10 days old young next thing is our next

00:12:47,800 --> 00:12:53,920
steps so we want to change our app to

00:12:52,149 --> 00:12:56,529
use a microservice pattern because

00:12:53,920 --> 00:13:01,149
before we had a big monolith application

00:12:56,529 --> 00:13:05,260
that we tried to separate we are trying

00:13:01,149 --> 00:13:07,690
to do this by using golang as a kind of

00:13:05,260 --> 00:13:13,529
replacement for PHP parts that were slow

00:13:07,690 --> 00:13:13,529
and try to gain some performance there

00:13:13,709 --> 00:13:19,660
another thing is we want to use more

00:13:16,060 --> 00:13:22,420
cloud services because we want to be

00:13:19,660 --> 00:13:24,670
able to adopt new features faster and

00:13:22,420 --> 00:13:27,190
have less administration overhead

00:13:24,670 --> 00:13:30,220
because you don't want to manage that

00:13:27,190 --> 00:13:35,110
elasticsearch cluster for example when

00:13:30,220 --> 00:13:38,560
there is a no failing or so yeah and we

00:13:35,110 --> 00:13:41,050
are hiring we are hiring smart people if

00:13:38,560 --> 00:13:44,260
anyone is interested please come to me

00:13:41,050 --> 00:13:46,900
after the talk or just apply a Java app

00:13:44,260 --> 00:13:49,410
like info and refer and refer to this

00:13:46,900 --> 00:13:49,410
conference

00:13:51,840 --> 00:14:03,300
last slide it was already quite fast are

00:13:58,450 --> 00:14:03,300
there any questions yes please

00:14:10,480 --> 00:14:16,420
we do that by using awsm and we just

00:14:14,110 --> 00:14:19,480
call it in our container startup script

00:14:16,420 --> 00:14:21,880
with a specific path that we also preset

00:14:19,480 --> 00:14:23,770
in the task definition so we say in the

00:14:21,880 --> 00:14:26,050
task definition hey pass this as an

00:14:23,770 --> 00:14:29,530
environment parameter at this AWS and

00:14:26,050 --> 00:14:32,230
path where you say slash company slash

00:14:29,530 --> 00:14:34,270
something slash application or so and

00:14:32,230 --> 00:14:36,580
then it downloads everything from that

00:14:34,270 --> 00:14:49,120
path and it also says that if you

00:14:36,580 --> 00:14:53,850
evaluate the output of the command yeah

00:14:49,120 --> 00:14:53,850
that's yeah it

00:15:07,959 --> 00:15:10,959
yourself

00:15:31,410 --> 00:15:43,550
I'm not 100% sure how our current

00:15:34,110 --> 00:15:43,550
configuration looks like but otherwise I

00:15:43,760 --> 00:16:28,530
would have to look that up we didn't

00:16:26,550 --> 00:16:32,250
have too many problems there

00:16:28,530 --> 00:16:35,060
we have several parts of our application

00:16:32,250 --> 00:16:41,100
that were already kind of separated and

00:16:35,060 --> 00:16:43,770
they both share a common common core but

00:16:41,100 --> 00:16:46,770
in the end we didn't have much issues

00:16:43,770 --> 00:16:50,240
because we already used message queues

00:16:46,770 --> 00:16:53,340
and stuff to have them kind of separated

00:16:50,240 --> 00:16:57,980
so we already followed a program pattern

00:16:53,340 --> 00:16:57,980
to be able to do that

00:16:58,540 --> 00:17:01,600
[Music]

00:17:04,850 --> 00:17:14,540
no not not really not really yes please

00:17:24,339 --> 00:17:32,850
[Applause]

00:17:26,839 --> 00:17:35,490
that's right what do you mean they run

00:17:32,850 --> 00:17:37,880
on the same network and the engineer has

00:17:35,490 --> 00:17:42,500
an upstream configuration for the fpm

00:17:37,880 --> 00:17:42,500
maybe I maybe I got your question wrong

00:17:55,130 --> 00:18:01,410
yeah it's exactly like you described it

00:17:59,010 --> 00:18:03,960
where hpfp I'm container and an engine X

00:18:01,410 --> 00:18:07,860
container and the same container X

00:18:03,960 --> 00:18:11,360
exposes the 9000 port for F p.m. and

00:18:07,860 --> 00:18:16,040
then the nginx can forward to that port

00:18:11,360 --> 00:18:16,040
for the for what the request to port

00:18:18,230 --> 00:18:24,840
they are in both containers so there

00:18:23,570 --> 00:18:38,460
yeah

00:18:24,840 --> 00:18:40,860
it's basically copied yes we are not yet

00:18:38,460 --> 00:18:44,870
really done on that it's still in

00:18:40,860 --> 00:18:51,180
progress we are working on that for

00:18:44,870 --> 00:18:53,490
longer longer than a year I think but we

00:18:51,180 --> 00:18:56,610
were not like non-stop working on that

00:18:53,490 --> 00:18:59,280
but rather to it step by step and have

00:18:56,610 --> 00:19:08,880
new features implemented meanwhile and

00:18:59,280 --> 00:19:11,100
we have currently we currently we are

00:19:08,880 --> 00:19:14,970
send at a base but we already have

00:19:11,100 --> 00:19:16,830
approaches for for separation where we

00:19:14,970 --> 00:19:20,460
have several databases which then

00:19:16,830 --> 00:19:22,650
communicate or where changes you know

00:19:20,460 --> 00:19:36,330
where what changes are communicated via

00:19:22,650 --> 00:19:38,540
AWS SNS and SQS I'm not sure how to us

00:19:36,330 --> 00:19:38,540
on that

00:19:43,430 --> 00:19:46,880
no not really

00:19:46,940 --> 00:19:52,010
what's done by the leads

00:19:53,170 --> 00:19:57,000
any other questions

00:20:05,300 --> 00:20:22,549

YouTube URL: https://www.youtube.com/watch?v=ET99vmlKC34


