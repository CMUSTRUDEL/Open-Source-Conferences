Title: Build your own Neural Network – with PHP! | Vitor Brandao
Publication date: 2019-11-13
Playlist: International PHP Conference Spring 2019
Description: 
	Speaker: Vitor Brandao (Noiselabs Consulting Ltd.) | https://phpconference.com/speaker/vitor-brandao/

Curious about all the hype around machine learning and artificial intelligence? Heard of "neural networks" and "deep learning", but confused about what it really means? In this talk, you’ll see what artificial neural networks (ANN) look like and how they can learn. And along the way, you’ll discover how you can build your own ANN – with PHP of course!

🤗 Come, join us at the next International PHP Conference | https://phpconference.com/
👍 Like us on Facebook | https://www.facebook.com/ipc.germany/
👉 Follow us on Twitter | https://twitter.com/phpconference
Captions: 
	00:00:00,250 --> 00:00:04,840
[Music]

00:00:05,359 --> 00:00:10,889
and thank you all for coming and thank

00:00:09,990 --> 00:00:13,950
you too

00:00:10,889 --> 00:00:15,660
ie PC organizers for having me it's a

00:00:13,950 --> 00:00:18,119
pleasure it's my first time in this

00:00:15,660 --> 00:00:21,800
conference for Simon Berlin as well I'm

00:00:18,119 --> 00:00:26,840
enjoying a lot and this talk is called

00:00:21,800 --> 00:00:31,910
build your own neural network with PHP

00:00:26,840 --> 00:00:34,790
so today we are going to build a an

00:00:31,910 --> 00:00:39,090
artificial neural network from scratch

00:00:34,790 --> 00:00:42,390
using good old PHP and yes that's

00:00:39,090 --> 00:00:47,820
possible so first just a few things

00:00:42,390 --> 00:00:50,059
about myself I'm Vito I'm Portuguese as

00:00:47,820 --> 00:00:54,989
you can tell by my accent

00:00:50,059 --> 00:00:58,559
but I'm working as a software consultant

00:00:54,989 --> 00:01:00,300
now in UK so if you do follow my twitter

00:00:58,559 --> 00:01:04,019
at noise labs

00:01:00,300 --> 00:01:07,710
heads up that's 90% praxic tweets I

00:01:04,019 --> 00:01:10,850
don't recommend it but if it's instead

00:01:07,710 --> 00:01:14,729
you go to my blog noise labs at i/o I

00:01:10,850 --> 00:01:20,909
can guarantee that one is 100% brexit

00:01:14,729 --> 00:01:22,920
free and now a few disclaimers because I

00:01:20,909 --> 00:01:24,960
have to start with those so first one

00:01:22,920 --> 00:01:29,970
I'm I'm not an expert

00:01:24,960 --> 00:01:32,780
I can see someone laughing I'm not an

00:01:29,970 --> 00:01:35,369
expert in this field I don't have

00:01:32,780 --> 00:01:37,380
experience with AI or machine learning

00:01:35,369 --> 00:01:40,439
production and as such I wouldn't be

00:01:37,380 --> 00:01:43,399
able to give you industry advice just

00:01:40,439 --> 00:01:47,460
consider myself to be an A student

00:01:43,399 --> 00:01:51,140
second there will be math throughout the

00:01:47,460 --> 00:01:55,500
talk sorry about that

00:01:51,140 --> 00:01:58,710
or to be more precise oversimplified

00:01:55,500 --> 00:02:02,369
math and I hope it's not too scary and

00:01:58,710 --> 00:02:05,189
you just stayed throughout a talk and do

00:02:02,369 --> 00:02:07,170
not leave hopefully and the last one is

00:02:05,189 --> 00:02:09,179
that there is a lot to cover in this

00:02:07,170 --> 00:02:12,120
topic there are a lot of slides so I'm

00:02:09,179 --> 00:02:13,680
I'll be going fairly quickly but if you

00:02:12,120 --> 00:02:15,569
have any doubt

00:02:13,680 --> 00:02:18,329
on some slides please just raise your

00:02:15,569 --> 00:02:22,230
hand and I'll stop and I can clarify it

00:02:18,329 --> 00:02:24,569
right there okay yeah okay so the reason

00:02:22,230 --> 00:02:27,030
why I'm giving this talk is because AI

00:02:24,569 --> 00:02:29,340
and machine learning is all the hype

00:02:27,030 --> 00:02:32,069
these days I mean you can you can barely

00:02:29,340 --> 00:02:35,069
go one day without hearing about machine

00:02:32,069 --> 00:02:41,459
learning the news and the trend is

00:02:35,069 --> 00:02:45,720
growing and AI is everywhere and it's

00:02:41,459 --> 00:02:49,650
kind of magical I guess the only thing

00:02:45,720 --> 00:02:51,540
more hype than AI is Bach chain but

00:02:49,650 --> 00:02:54,090
about that I know pretty much close to

00:02:51,540 --> 00:02:57,299
nothing so you have to find another talk

00:02:54,090 --> 00:02:58,859
and another speaker for this now the

00:02:57,299 --> 00:03:01,950
second reason why I'm giving this talk

00:02:58,859 --> 00:03:05,609
is because it's kind of rare to see PHP

00:03:01,950 --> 00:03:07,349
and machine learning together and I

00:03:05,609 --> 00:03:10,319
think we deserve our fair share of AI

00:03:07,349 --> 00:03:12,150
and also if you're trying to learn

00:03:10,319 --> 00:03:14,549
something complex such as machine

00:03:12,150 --> 00:03:16,560
learning can be then I think it makes

00:03:14,549 --> 00:03:18,629
sense to pick a language you're already

00:03:16,560 --> 00:03:21,299
comfortable with such as PHP that's your

00:03:18,629 --> 00:03:23,549
primary language and learn then learn

00:03:21,299 --> 00:03:25,199
the other complex thing which would be

00:03:23,549 --> 00:03:31,769
machine learning I think that makes

00:03:25,199 --> 00:03:33,690
sense so AI is used pretty much

00:03:31,769 --> 00:03:35,760
everywhere ie you you can see it on

00:03:33,690 --> 00:03:38,159
speech recognition hand written

00:03:35,760 --> 00:03:41,129
recognition automated trading systems

00:03:38,159 --> 00:03:46,169
and the recommender systems that you see

00:03:41,129 --> 00:03:49,199
on Amazon or Netflix even though

00:03:46,169 --> 00:03:54,419
sometimes you wonder what kind of AI is

00:03:49,199 --> 00:03:58,260
there based on those recommendations so

00:03:54,419 --> 00:04:01,769
AI will be driving as soon everywhere

00:03:58,260 --> 00:04:06,090
it will be our doctors it will be our

00:04:01,769 --> 00:04:08,370
lawyers it will even replace us as

00:04:06,090 --> 00:04:10,709
programmers by the way this is a pretty

00:04:08,370 --> 00:04:12,810
cool article there is a link at the

00:04:10,709 --> 00:04:16,289
bottom check it out the slides already

00:04:12,810 --> 00:04:19,700
online it's really cool maybe you can

00:04:16,289 --> 00:04:23,090
use it and still charge your client

00:04:19,700 --> 00:04:25,460
and if you have enough free time why not

00:04:23,090 --> 00:04:30,200
use machine learning to put Nicolas Cage

00:04:25,460 --> 00:04:34,420
face on films such as Indiana Jones what

00:04:30,200 --> 00:04:37,430
the Lord of the Rings instant classics

00:04:34,420 --> 00:04:39,290
now at the foundation of many of these

00:04:37,430 --> 00:04:43,630
are AI applications that we have just

00:04:39,290 --> 00:04:47,600
seen we have artificial neural networks

00:04:43,630 --> 00:04:50,890
so today we're going to see why are they

00:04:47,600 --> 00:04:53,360
called new networks how do they learn

00:04:50,890 --> 00:04:57,260
how can we write one using our favorite

00:04:53,360 --> 00:05:03,260
language for some and what the hell is

00:04:57,260 --> 00:05:07,970
deep learning so without further delay

00:05:03,260 --> 00:05:10,820
let's start with the light topic the

00:05:07,970 --> 00:05:16,160
human brain so this is a picture of a

00:05:10,820 --> 00:05:18,950
brain on half of it and in a full brain

00:05:16,160 --> 00:05:22,490
you can see you you have 86 billion

00:05:18,950 --> 00:05:24,860
neurons and the neuron which looks like

00:05:22,490 --> 00:05:27,080
this is a cell that transmits

00:05:24,860 --> 00:05:31,940
information through chemical and

00:05:27,080 --> 00:05:34,070
electrical signals so you have the

00:05:31,940 --> 00:05:36,140
dendrite that connects to the previous

00:05:34,070 --> 00:05:38,660
neurons where the camera with the

00:05:36,140 --> 00:05:41,210
signals arrive then they are processed

00:05:38,660 --> 00:05:43,520
in the cell body and the nucleus and

00:05:41,210 --> 00:05:47,030
they flow through the next set of

00:05:43,520 --> 00:05:49,370
neurons through the accent so another

00:05:47,030 --> 00:05:51,650
way to look at this it to say that our

00:05:49,370 --> 00:05:55,010
dendrite is performing some sort of

00:05:51,650 --> 00:05:59,240
input our nucleus is an activation

00:05:55,010 --> 00:06:02,870
function and the axon can be we can say

00:05:59,240 --> 00:06:07,880
it's the upper four for this neuron so

00:06:02,870 --> 00:06:09,950
in 1943 McCulloch and Pitt's inspired by

00:06:07,880 --> 00:06:12,260
this biological model they developed a

00:06:09,950 --> 00:06:17,330
mathematical model which became the

00:06:12,260 --> 00:06:20,540
first artificial neuron and it looks

00:06:17,330 --> 00:06:22,340
something like this so this is another

00:06:20,540 --> 00:06:24,560
representation of the neuron so let's

00:06:22,340 --> 00:06:26,480
say we have two inputs so it's

00:06:24,560 --> 00:06:29,400
connecting to two previous neurons let's

00:06:26,480 --> 00:06:31,860
say it's X 1 and X 2

00:06:29,400 --> 00:06:37,080
and each collection he's affected by a

00:06:31,860 --> 00:06:41,340
weight w1 and w2 and then within the

00:06:37,080 --> 00:06:44,009
within the cell we perform this sum so

00:06:41,340 --> 00:06:49,110
we multiply weight times X which is our

00:06:44,009 --> 00:06:51,690
input plus the bias and the weight and

00:06:49,110 --> 00:06:55,860
the bias is what we call the learn about

00:06:51,690 --> 00:06:58,020
parameters so the weights they are the

00:06:55,860 --> 00:07:03,289
influence of one neuron on the next one

00:06:58,020 --> 00:07:08,430
so the connection can be thin or thick

00:07:03,289 --> 00:07:11,250
the bias is an addition term that allow

00:07:08,430 --> 00:07:13,710
us to shift the signal output signal

00:07:11,250 --> 00:07:16,830
left or right so that it matches our

00:07:13,710 --> 00:07:23,280
desired output so these are our learn

00:07:16,830 --> 00:07:26,400
about parameters so that sum plus that

00:07:23,280 --> 00:07:31,500
sum will then be puff through a function

00:07:26,400 --> 00:07:34,680
an activation function and we get the

00:07:31,500 --> 00:07:37,260
output for that neuron so we have input

00:07:34,680 --> 00:07:39,150
and a will be our output this is what

00:07:37,260 --> 00:07:45,210
happening in a muhfucka mathematical

00:07:39,150 --> 00:07:47,550
model of a neuron so in summary for two

00:07:45,210 --> 00:07:51,360
inputs this is what's happening W 1 and

00:07:47,550 --> 00:07:52,860
W 2 plus B equals Z with passes L

00:07:51,360 --> 00:07:57,539
through an activation function we get

00:07:52,860 --> 00:08:01,320
near an output activation function

00:07:57,539 --> 00:08:03,000
models the the firing rate is used to

00:08:01,320 --> 00:08:05,250
model the firing rate and most

00:08:03,000 --> 00:08:07,199
importantly it adds nonlinear

00:08:05,250 --> 00:08:09,330
capabilities to the network and we're

00:08:07,199 --> 00:08:11,250
going to see why that matters what is

00:08:09,330 --> 00:08:14,039
why that's really important later in a

00:08:11,250 --> 00:08:16,169
talk so a very common activation

00:08:14,039 --> 00:08:18,780
function is the sigmoid function which

00:08:16,169 --> 00:08:22,080
is this s-shaped function so for var for

00:08:18,780 --> 00:08:24,570
large negative numbers the output will

00:08:22,080 --> 00:08:27,570
be 0 and for large positive numbers the

00:08:24,570 --> 00:08:31,560
output will be 1 so the output will be

00:08:27,570 --> 00:08:34,740
always between 0 and 1 this path shape

00:08:31,560 --> 00:08:37,950
function the sigmoid function so if you

00:08:34,740 --> 00:08:42,180
have another look at our our neuron and

00:08:37,950 --> 00:08:42,780
if we zoom out then there you have it

00:08:42,180 --> 00:08:45,900
our

00:08:42,780 --> 00:08:48,150
very first neural network very simple

00:08:45,900 --> 00:08:51,000
neural network we just one neuron one

00:08:48,150 --> 00:08:53,790
upper layer because funnily enough input

00:08:51,000 --> 00:08:58,550
layer doesn't count as a layer say one

00:08:53,790 --> 00:08:58,550
output layer one neuron and two inputs

00:08:59,390 --> 00:09:06,570
now it does that funny sum and you go

00:09:05,160 --> 00:09:10,950
through an activation function and

00:09:06,570 --> 00:09:12,930
wherever but how does it learn I mean

00:09:10,950 --> 00:09:16,650
how does it do something interesting and

00:09:12,930 --> 00:09:21,810
the answer is by using machine learning

00:09:16,650 --> 00:09:25,740
and in this case specifically supervised

00:09:21,810 --> 00:09:26,070
machine learning so what is machine

00:09:25,740 --> 00:09:28,980
learning

00:09:26,070 --> 00:09:32,910
so in machine learning we have a set of

00:09:28,980 --> 00:09:35,820
known inputs so there are known inputs X

00:09:32,910 --> 00:09:38,010
and we have a set of known outputs Y and

00:09:35,820 --> 00:09:40,560
what we want is to learn a function that

00:09:38,010 --> 00:09:44,130
map's X to Y we don't know the function

00:09:40,560 --> 00:09:46,350
yet we need to find it out so that once

00:09:44,130 --> 00:09:50,100
we have found out that function we can

00:09:46,350 --> 00:09:53,280
throw it new inputs from the same data

00:09:50,100 --> 00:09:56,070
set and we can see some very good

00:09:53,280 --> 00:09:58,950
predictions that we can trust so that's

00:09:56,070 --> 00:10:01,020
the basis for supervised machine

00:09:58,950 --> 00:10:02,370
learning and this is the departure from

00:10:01,020 --> 00:10:04,140
traditional programming so in our

00:10:02,370 --> 00:10:07,140
programs and our traditional programs

00:10:04,140 --> 00:10:09,710
what we do is we write the rules we

00:10:07,140 --> 00:10:12,630
write know the routine if statements and

00:10:09,710 --> 00:10:14,880
we give it some data and we get some

00:10:12,630 --> 00:10:16,250
answers back that's how programs work

00:10:14,880 --> 00:10:19,770
basically

00:10:16,250 --> 00:10:22,230
but with machine learning what we do is

00:10:19,770 --> 00:10:25,140
we throw it all the data we give it the

00:10:22,230 --> 00:10:28,230
answers as well and then the system will

00:10:25,140 --> 00:10:33,300
figure out the rules which I think it's

00:10:28,230 --> 00:10:38,250
pretty neat so if we have another look

00:10:33,300 --> 00:10:40,350
at our nearer Network we can see that

00:10:38,250 --> 00:10:42,450
this one with just one single neuron is

00:10:40,350 --> 00:10:44,910
able to solve linear problems such as

00:10:42,450 --> 00:10:47,190
this one so we can find out this

00:10:44,910 --> 00:10:49,890
function and if we ask you to predict

00:10:47,190 --> 00:10:52,220
the new value it will say it's over

00:10:49,890 --> 00:10:52,220
there

00:10:53,490 --> 00:11:01,210
but most interesting problems are

00:10:56,470 --> 00:11:05,320
nonlinear so what we have to do we have

00:11:01,210 --> 00:11:08,560
to upgrade our network and we do that by

00:11:05,320 --> 00:11:14,050
adding a hidden layer so our network now

00:11:08,560 --> 00:11:16,330
becomes this and now I have two layers

00:11:14,050 --> 00:11:19,980
because again input layer doesn't count

00:11:16,330 --> 00:11:23,920
I don't know why that's how it's it is

00:11:19,980 --> 00:11:27,610
so there we have it a new a network that

00:11:23,920 --> 00:11:30,250
is able to solve nonlinear problems and

00:11:27,610 --> 00:11:32,920
that's why because when the activation

00:11:30,250 --> 00:11:36,940
function is nonlinear such as the Sigma

00:11:32,920 --> 00:11:38,920
days then a two layer or bigger can be

00:11:36,940 --> 00:11:40,330
proven to be an Universal function

00:11:38,920 --> 00:11:42,640
approximator and that's why it makes

00:11:40,330 --> 00:11:46,180
them so powerful because they can in

00:11:42,640 --> 00:11:47,860
theory represent any function so let's

00:11:46,180 --> 00:11:49,930
have a look at what the training

00:11:47,860 --> 00:11:51,730
approach the learning process is going

00:11:49,930 --> 00:11:53,470
to be for this artificial neural network

00:11:51,730 --> 00:11:56,230
so we're going to start with random

00:11:53,470 --> 00:11:58,150
weights then we're going to make a

00:11:56,230 --> 00:12:00,580
prediction which I say it's y hat

00:11:58,150 --> 00:12:02,710
based on my input data it's going to be

00:12:00,580 --> 00:12:05,200
a random prediction and we're going to

00:12:02,710 --> 00:12:07,540
compare that prediction with my known

00:12:05,200 --> 00:12:08,230
input my answer the thing I know it's

00:12:07,540 --> 00:12:12,010
right

00:12:08,230 --> 00:12:13,780
y and then based on the difference I'm

00:12:12,010 --> 00:12:16,420
going to adjust my parameters my learn

00:12:13,780 --> 00:12:18,670
about parameters W and B and then I'm

00:12:16,420 --> 00:12:21,100
going to repeat that until they we are

00:12:18,670 --> 00:12:23,320
really close and I say I have a good

00:12:21,100 --> 00:12:26,350
function that map's my inputs to my

00:12:23,320 --> 00:12:29,350
known output so this is what we're going

00:12:26,350 --> 00:12:34,360
to do now let's say you're going to

00:12:29,350 --> 00:12:37,780
build you're good to build a and up to

00:12:34,360 --> 00:12:41,080
classify cats a cat falsifier y know you

00:12:37,780 --> 00:12:43,330
could make some money on that to do this

00:12:41,080 --> 00:12:45,460
you would have to collect a bunch of

00:12:43,330 --> 00:12:49,210
pictures of cats that's your data set

00:12:45,460 --> 00:12:51,130
and you would label that with a 1/4 it

00:12:49,210 --> 00:12:54,870
is a cat and then you would get a bunch

00:12:51,130 --> 00:12:57,430
of other pictures or the animals even

00:12:54,870 --> 00:13:01,300
objects and then you would label that

00:12:57,430 --> 00:13:04,210
with a 0 for it's not the cat you would

00:13:01,300 --> 00:13:05,020
feed those images all your data set with

00:13:04,210 --> 00:13:08,200
your labels

00:13:05,020 --> 00:13:11,890
into your network by the way that's just

00:13:08,200 --> 00:13:16,120
decomposed in the RGB image into a array

00:13:11,890 --> 00:13:18,339
a vector and we'd fit that into your new

00:13:16,120 --> 00:13:20,830
network you keep it training and you get

00:13:18,339 --> 00:13:23,800
a good function and then if you send it

00:13:20,830 --> 00:13:27,459
a new picture it hasn't seen before and

00:13:23,800 --> 00:13:30,370
you'd get an answer of say not when 73

00:13:27,459 --> 00:13:38,320
you could say yeah it's pretty close to

00:13:30,370 --> 00:13:40,839
one okay I think it's a cat now as much

00:13:38,320 --> 00:13:42,850
I would like and there's much I would

00:13:40,839 --> 00:13:45,399
love to teach you how to build a car

00:13:42,850 --> 00:13:48,490
classifier that's not possible in just

00:13:45,399 --> 00:13:51,040
45 minutes so I'm sorry to disappoint

00:13:48,490 --> 00:13:57,370
you but we're going to see an X or

00:13:51,040 --> 00:14:00,850
instead and just a quick reminder on the

00:13:57,370 --> 00:14:03,520
X or for X or looks like this so for

00:14:00,850 --> 00:14:07,029
equal inputs you get an upper of 0 for

00:14:03,520 --> 00:14:10,930
different inputs you get an output of 1

00:14:07,029 --> 00:14:14,020
so that's my input and output that's my

00:14:10,930 --> 00:14:16,360
data set it's really small but it's it's

00:14:14,020 --> 00:14:19,839
enough for this talk and so if I put

00:14:16,360 --> 00:14:23,410
this on a chart you can see that there

00:14:19,839 --> 00:14:28,570
is no straight line where I can separate

00:14:23,410 --> 00:14:30,190
my zeros for my ones so my very simple

00:14:28,570 --> 00:14:32,079
internetwork which can only solve new

00:14:30,190 --> 00:14:36,040
linear problems isn't it snowing good

00:14:32,079 --> 00:14:41,079
enough for an axle it sucks so for that

00:14:36,040 --> 00:14:43,089
I need this one so with a hidden layer

00:14:41,079 --> 00:14:45,910
it now I'm able to solve nonlinear

00:14:43,089 --> 00:14:48,070
problems and my network which is the one

00:14:45,910 --> 00:14:51,880
that we're going to code looks like this

00:14:48,070 --> 00:14:55,140
ok so as you can see x1 x2 mine on

00:14:51,880 --> 00:14:58,540
output my buyers over there and then

00:14:55,140 --> 00:15:02,500
each connection each neuron is connected

00:14:58,540 --> 00:15:06,010
and for each one I have an Associated

00:15:02,500 --> 00:15:07,870
weight so it's fully connected and this

00:15:06,010 --> 00:15:10,209
is an example for a feed-forward neural

00:15:07,870 --> 00:15:12,760
network there are a lot more apologies

00:15:10,209 --> 00:15:16,420
this one is a feed for a new network and

00:15:12,760 --> 00:15:20,930
it's good for an EXO

00:15:16,420 --> 00:15:28,220
so with this information we're now ready

00:15:20,930 --> 00:15:30,050
to start coding by the way all the codes

00:15:28,220 --> 00:15:33,530
I'm going to show you the next slides is

00:15:30,050 --> 00:15:37,400
available on github it's it it is a

00:15:33,530 --> 00:15:39,320
fully working example so don't worry if

00:15:37,400 --> 00:15:41,240
you're going if you miss some details on

00:15:39,320 --> 00:15:43,760
the next slides you can just check it

00:15:41,240 --> 00:15:47,120
out run it locally I think it even runs

00:15:43,760 --> 00:15:50,870
on docker and just don't worry you'll be

00:15:47,120 --> 00:15:52,420
able to play with it after the talk okay

00:15:50,870 --> 00:15:55,130
so let's start with something

00:15:52,420 --> 00:15:57,440
predictable let's say let's write a

00:15:55,130 --> 00:15:58,970
class called neural network class and in

00:15:57,440 --> 00:16:01,550
this class I'm going to have two methods

00:15:58,970 --> 00:16:05,570
the Train method for the learning phase

00:16:01,550 --> 00:16:09,650
any predict method for one I want to

00:16:05,570 --> 00:16:13,490
make predictions or new unseen data okay

00:16:09,650 --> 00:16:15,950
and so for the XOR example I'm going to

00:16:13,490 --> 00:16:18,980
have my data set with four training

00:16:15,950 --> 00:16:23,300
examples and for each one I have my

00:16:18,980 --> 00:16:26,050
answer below there just much and I can

00:16:23,300 --> 00:16:30,710
now instantiate my new network class

00:16:26,050 --> 00:16:33,680
which does pretty much nothing so let's

00:16:30,710 --> 00:16:36,830
go to step number two let's define our

00:16:33,680 --> 00:16:38,690
topology so you already seen this we

00:16:36,830 --> 00:16:42,020
have two inputs we're going to have to

00:16:38,690 --> 00:16:45,680
heat and neurons and because it's just

00:16:42,020 --> 00:16:48,350
one hidden layer two layers hidden an

00:16:45,680 --> 00:16:51,350
output and one output so this is my

00:16:48,350 --> 00:16:53,330
topology so I need to store my

00:16:51,350 --> 00:16:56,750
parameters as well so in my class

00:16:53,330 --> 00:17:01,520
parameters I will store weight and

00:16:56,750 --> 00:17:05,810
buyers z na what which you already seen

00:17:01,520 --> 00:17:07,970
on that neuron diagram and I saw that in

00:17:05,810 --> 00:17:11,240
my variable P I know we shouldn't use

00:17:07,970 --> 00:17:14,569
shopping for variables but I think you

00:17:11,240 --> 00:17:19,010
can excuse me on this one now I could be

00:17:14,569 --> 00:17:20,689
using to store to store my wbd and I

00:17:19,010 --> 00:17:23,060
could be using nested arrays and

00:17:20,689 --> 00:17:27,510
associative arrays to solve that I

00:17:23,060 --> 00:17:31,200
actually done that and it turns out it's

00:17:27,510 --> 00:17:34,860
pretty much a nightmare so there is a

00:17:31,200 --> 00:17:37,280
much better data structure can you guess

00:17:34,860 --> 00:17:37,280
what it is

00:17:39,200 --> 00:17:47,100
webinar yes it's much better much

00:17:45,150 --> 00:17:51,150
cleaner makes total sound and that's

00:17:47,100 --> 00:17:54,530
what we have it's actually used in data

00:17:51,150 --> 00:17:57,210
science machine learning so with a

00:17:54,530 --> 00:17:59,490
matrix what I'm going to have is

00:17:57,210 --> 00:18:01,890
probably one and B one store like this

00:17:59,490 --> 00:18:03,570
so as you can see connections coming

00:18:01,890 --> 00:18:05,429
into the first neuron on the first row

00:18:03,570 --> 00:18:09,360
connections coming to the second neuron

00:18:05,429 --> 00:18:11,730
on the second row same for B one and the

00:18:09,360 --> 00:18:14,790
same goes for the upper layer because

00:18:11,730 --> 00:18:21,480
it's just one neuron on the output it's

00:18:14,790 --> 00:18:24,179
just one right so with this and to you

00:18:21,480 --> 00:18:25,770
to implement my matrix I'm not going to

00:18:24,179 --> 00:18:28,650
write any code there is a pretty pretty

00:18:25,770 --> 00:18:32,040
good library called math PHP it's very

00:18:28,650 --> 00:18:34,230
very complete actually I recommend it

00:18:32,040 --> 00:18:36,090
so with this library I can do I can

00:18:34,230 --> 00:18:39,660
declare my matrix as an array of arrays

00:18:36,090 --> 00:18:42,540
and then I can create the matrix using

00:18:39,660 --> 00:18:45,660
the helper and then I can add matrices

00:18:42,540 --> 00:18:46,679
multiply it to a harder math product

00:18:45,660 --> 00:18:48,570
which is just an element-wise

00:18:46,679 --> 00:18:51,360
multiplication and I can go to each

00:18:48,570 --> 00:18:56,100
element in the matrix and just apply a

00:18:51,360 --> 00:18:57,330
function any function so quick note on

00:18:56,100 --> 00:19:00,690
vectorization

00:18:57,330 --> 00:19:05,760
so other languages and and libraries

00:19:00,690 --> 00:19:08,010
such as Python and numpy they do offer

00:19:05,760 --> 00:19:12,090
vectorization which is a very efficient

00:19:08,010 --> 00:19:14,490
way of multiplying matrices to the best

00:19:12,090 --> 00:19:17,820
of my knowledge PHP doesn't have this

00:19:14,490 --> 00:19:20,669
and that's why it makes it slower when

00:19:17,820 --> 00:19:23,790
compared it is other libraries but I

00:19:20,669 --> 00:19:27,270
found this PHP extension which is PHP SC

00:19:23,790 --> 00:19:31,980
I i think it's an extension yes

00:19:27,270 --> 00:19:33,179
eh BSC I we'd stay promise by the time I

00:19:31,980 --> 00:19:35,340
took this screenshot they said two

00:19:33,179 --> 00:19:38,190
thousand times last time I saw it's

00:19:35,340 --> 00:19:40,030
eight hundred maybe because just in

00:19:38,190 --> 00:19:43,240
vanilla PHP go faster

00:19:40,030 --> 00:19:44,470
and they promise to that array operation

00:19:43,240 --> 00:19:46,560
size multiplication is going to be much

00:19:44,470 --> 00:19:49,750
quicker I haven't tried it yet but

00:19:46,560 --> 00:19:54,100
please do I think there is some hope for

00:19:49,750 --> 00:19:56,290
pitch be so next step the activation

00:19:54,100 --> 00:19:59,620
function so if you remember the sigmoid

00:19:56,290 --> 00:20:03,250
function which is defined by this

00:19:59,620 --> 00:20:06,040
equation and by weight you can you can

00:20:03,250 --> 00:20:07,360
have different activations functions the

00:20:06,040 --> 00:20:11,310
sigmoid it just happens to be a good

00:20:07,360 --> 00:20:13,030
example for the axon so in my namespace

00:20:11,310 --> 00:20:15,100
activation I'm going to write a class

00:20:13,030 --> 00:20:17,310
sigmoid and then I'm going to just

00:20:15,100 --> 00:20:21,660
implement the equation pretty easy to do

00:20:17,310 --> 00:20:24,910
and then I'm going to write my compute

00:20:21,660 --> 00:20:26,680
function which applies in a matrix and

00:20:24,910 --> 00:20:29,650
then for each element in the matrix I

00:20:26,680 --> 00:20:32,860
just call the method below and there we

00:20:29,650 --> 00:20:34,240
have it a class that calculates the

00:20:32,860 --> 00:20:38,590
sigmoid honor matrix

00:20:34,240 --> 00:20:44,290
I just set it in my constructor that's

00:20:38,590 --> 00:20:49,200
my first parameter and now for the

00:20:44,290 --> 00:20:57,100
training phase and this is the algorithm

00:20:49,200 --> 00:20:58,660
that I'm going to use so I'm going to

00:20:57,100 --> 00:21:01,990
start by initializing weights and bias

00:20:58,660 --> 00:21:04,270
and then in the loop what I'm going to

00:21:01,990 --> 00:21:06,910
do is I'm going to set a maximum number

00:21:04,270 --> 00:21:09,840
of iterations because I don't want this

00:21:06,910 --> 00:21:13,830
stuff running forever if it goes bad or

00:21:09,840 --> 00:21:18,970
until my error the network error is

00:21:13,830 --> 00:21:21,430
above my desired maximum error so if the

00:21:18,970 --> 00:21:23,170
network keeps running for a long time it

00:21:21,430 --> 00:21:25,660
will be stopped by iterations if it

00:21:23,170 --> 00:21:28,450
works it will stop because the error has

00:21:25,660 --> 00:21:31,660
fallen below and then for each training

00:21:28,450 --> 00:21:34,330
example so in the X or we have just for

00:21:31,660 --> 00:21:38,110
training examples in the cat example you

00:21:34,330 --> 00:21:40,240
had a load more training examples for

00:21:38,110 --> 00:21:42,250
each training example what I'm going to

00:21:40,240 --> 00:21:44,500
do I'm going to call this four steps I'm

00:21:42,250 --> 00:21:47,710
going to do a forward pass that I'm

00:21:44,500 --> 00:21:50,470
going to compute the cost then run a

00:21:47,710 --> 00:21:52,000
back propagation step and I'm going to

00:21:50,470 --> 00:21:53,410
adjust my weight and bias and try again

00:21:52,000 --> 00:21:57,010
keep this in a loop

00:21:53,410 --> 00:21:59,200
and this is the learning process for my

00:21:57,010 --> 00:22:02,770
neural network so step number one

00:21:59,200 --> 00:22:05,380
initializing the parameters so I'm good

00:22:02,770 --> 00:22:07,240
right just a function and what I'm going

00:22:05,380 --> 00:22:10,690
to do is for the hidden layer I'm going

00:22:07,240 --> 00:22:13,240
to initialize my bias to zero that's

00:22:10,690 --> 00:22:15,190
what I'm going to do and for the weight

00:22:13,240 --> 00:22:19,930
I'm going to initialize it with the

00:22:15,190 --> 00:22:22,720
value between zero and one random value

00:22:19,930 --> 00:22:25,360
and for the output layer I'm going to do

00:22:22,720 --> 00:22:28,060
exactly the same by a zero weights

00:22:25,360 --> 00:22:29,560
random value zero between zero and one

00:22:28,060 --> 00:22:31,330
why is here in one because it just

00:22:29,560 --> 00:22:34,450
happens because of the sigmoid function

00:22:31,330 --> 00:22:37,420
time use that's a really good strategy

00:22:34,450 --> 00:22:40,540
to keep values between 0 1 0 and 1 and

00:22:37,420 --> 00:22:43,750
randomly initialization is also a very

00:22:40,540 --> 00:22:45,940
good practice when initializing a new

00:22:43,750 --> 00:22:47,680
network so I'm going to call this in my

00:22:45,940 --> 00:22:52,120
constructor and it's called only once

00:22:47,680 --> 00:22:56,260
only one creating the network so this is

00:22:52,120 --> 00:22:58,780
the initial state of my new network so

00:22:56,260 --> 00:23:01,510
as you can see random values on all the

00:22:58,780 --> 00:23:03,430
weights and the buyers are 0 so this

00:23:01,510 --> 00:23:05,080
what is what the new network looks like

00:23:03,430 --> 00:23:09,580
when it starts this is just the initial

00:23:05,080 --> 00:23:12,280
state and now for the training method

00:23:09,580 --> 00:23:14,350
first step is I just convert my inputs

00:23:12,280 --> 00:23:18,160
and targets into a matrix because that's

00:23:14,350 --> 00:23:20,860
what I use inside and then I'm going to

00:23:18,160 --> 00:23:24,910
set some constants so I'm going to say

00:23:20,860 --> 00:23:30,430
that my maximum error is going to be at

00:23:24,910 --> 00:23:32,650
a point naught naught 1 and I will lie

00:23:30,430 --> 00:23:35,200
to my network train until it reaches 20

00:23:32,650 --> 00:23:37,990
thousand iterations if we got to this

00:23:35,200 --> 00:23:40,510
number and there is still above I just

00:23:37,990 --> 00:23:42,850
assume that you failed to learn maybe

00:23:40,510 --> 00:23:45,430
there is still going up again and not

00:23:42,850 --> 00:23:48,790
decreasing like I expected so this is

00:23:45,430 --> 00:23:50,170
just a safe switch if it gets that point

00:23:48,790 --> 00:23:52,570
things have gone bad

00:23:50,170 --> 00:23:56,590
let's stop it not burn more CPU cycles

00:23:52,570 --> 00:23:59,720
and the original error is going to be

00:23:56,590 --> 00:24:01,860
infinite that's my starting point

00:23:59,720 --> 00:24:03,420
obviously you can play with its numbers

00:24:01,860 --> 00:24:09,870
and other numbers make sense for all the

00:24:03,420 --> 00:24:13,200
examples so this is the skeleton for my

00:24:09,870 --> 00:24:17,550
training step so the outer loop and then

00:24:13,200 --> 00:24:19,920
the loop for a for the examples and then

00:24:17,550 --> 00:24:32,190
below I just keep track of the error for

00:24:19,920 --> 00:24:55,410
that I think it's gone oh maybe the

00:24:32,190 --> 00:25:08,430
projector still showing on my screen I'm

00:24:55,410 --> 00:25:13,500
just going to plug it in again yeah okay

00:25:08,430 --> 00:25:16,890
we're back it seems like I have what 15

00:25:13,500 --> 00:25:21,540
more minutes we go to time zones okay so

00:25:16,890 --> 00:25:22,890
next step forward propagation I'm going

00:25:21,540 --> 00:25:25,290
to use the same equations from the

00:25:22,890 --> 00:25:26,790
original example just now it's going to

00:25:25,290 --> 00:25:29,310
be in matrix format

00:25:26,790 --> 00:25:34,140
hence the Capitals so for the hidden

00:25:29,310 --> 00:25:36,180
layer W times X and then pass through my

00:25:34,140 --> 00:25:38,730
function with these sigmoid it's this

00:25:36,180 --> 00:25:40,740
symbol right here okay so that's my

00:25:38,730 --> 00:25:42,630
activation function and below the only

00:25:40,740 --> 00:25:45,990
difference is that I'm going to use

00:25:42,630 --> 00:25:48,810
activations from the previous layer so I

00:25:45,990 --> 00:25:50,550
have the input and the input for the

00:25:48,810 --> 00:25:54,090
acts layer is the output from a previous

00:25:50,550 --> 00:25:58,980
layer makes sense I could have more

00:25:54,090 --> 00:26:01,860
layers it just works like that okay so

00:25:58,980 --> 00:26:03,929
that's my generic formula for that

00:26:01,860 --> 00:26:07,080
for the hidden layer and for the upper

00:26:03,929 --> 00:26:10,669
layer so if I do it for the forward

00:26:07,080 --> 00:26:13,260
propagation step what I'm going to do is

00:26:10,669 --> 00:26:15,919
I'm going through in the loop I'm going

00:26:13,260 --> 00:26:18,630
to calculate my Z using the formula and

00:26:15,919 --> 00:26:20,640
the previous formula and using the

00:26:18,630 --> 00:26:24,779
helpers for that matrix library such as

00:26:20,640 --> 00:26:27,720
multiply and add if you'd have to do

00:26:24,779 --> 00:26:30,390
this with arrays it would be like two

00:26:27,720 --> 00:26:33,720
hundred five hundred lines just to do

00:26:30,390 --> 00:26:35,850
that and you mess it up and then I keep

00:26:33,720 --> 00:26:37,230
activation functions and L is just to

00:26:35,850 --> 00:26:41,370
keep track of my layer it's just an

00:26:37,230 --> 00:26:42,480
Indian's index and last one is I

00:26:41,370 --> 00:26:44,190
returned the vault

00:26:42,480 --> 00:26:46,740
I'll return the output for my network

00:26:44,190 --> 00:26:48,389
and that's actually my prediction so

00:26:46,740 --> 00:26:51,269
what comes out from the network is my

00:26:48,389 --> 00:26:53,309
prediction and so this is what's

00:26:51,269 --> 00:26:56,159
happening in a forward path for low pass

00:26:53,309 --> 00:26:58,769
I get my inputs which in XR is zero zero

00:26:56,159 --> 00:27:02,039
i flow through the network and i got a

00:26:58,769 --> 00:27:05,399
value back and that's my first step in

00:27:02,039 --> 00:27:08,100
this loop right here so if you have a

00:27:05,399 --> 00:27:09,960
look after so as you remember I've

00:27:08,100 --> 00:27:11,820
initialized my network with random

00:27:09,960 --> 00:27:14,669
parameters and I gave it the first

00:27:11,820 --> 00:27:18,360
example which is a zero zero and my

00:27:14,669 --> 00:27:24,090
network told me this is 0.6 T 1 is it is

00:27:18,360 --> 00:27:26,669
it a good answer or you think no well

00:27:24,090 --> 00:27:30,750
let's find out using some math of course

00:27:26,669 --> 00:27:33,899
so I need a complete cost and I need to

00:27:30,750 --> 00:27:35,940
use a cost function I'm going to pick a

00:27:33,899 --> 00:27:38,370
pretty common one min square error and

00:27:35,940 --> 00:27:40,889
which is actually quite simple it's just

00:27:38,370 --> 00:27:44,130
output minus a prediction squared that's

00:27:40,889 --> 00:27:46,980
it so if I go and write a function my

00:27:44,130 --> 00:27:49,139
class mean square error in my cost

00:27:46,980 --> 00:27:50,789
function activation because you can use

00:27:49,139 --> 00:27:52,919
different classes for this if you want

00:27:50,789 --> 00:27:55,019
I'm going to compute and it's so simple

00:27:52,919 --> 00:27:59,070
just target - prediction squared that's

00:27:55,019 --> 00:28:01,710
it that's my second step actually I'm

00:27:59,070 --> 00:28:04,440
just initializing it here and that's me

00:28:01,710 --> 00:28:07,649
calling the cost function in my compute

00:28:04,440 --> 00:28:10,020
cost which I'm going to add so that's my

00:28:07,649 --> 00:28:13,020
second step there and I keep track of it

00:28:10,020 --> 00:28:13,900
it might cost array okay so that's the

00:28:13,020 --> 00:28:19,030
second step

00:28:13,900 --> 00:28:21,880
and so if I have a look so my desires my

00:28:19,030 --> 00:28:25,809
known output is here actually gave me

00:28:21,880 --> 00:28:26,620
point 61 which is crop squared equals

00:28:25,809 --> 00:28:29,200
point 38

00:28:26,620 --> 00:28:33,670
that's my network error that's how good

00:28:29,200 --> 00:28:35,680
or bad the network is behaving so what I

00:28:33,670 --> 00:28:37,240
want is that what I hope is that it

00:28:35,680 --> 00:28:39,280
starts with a really high error

00:28:37,240 --> 00:28:41,260
why because the initial weights are just

00:28:39,280 --> 00:28:43,000
random and over time and it starts

00:28:41,260 --> 00:28:45,820
adjusting the weighting bias it will

00:28:43,000 --> 00:28:48,970
slowly go down goes below my error stops

00:28:45,820 --> 00:28:51,670
training and I'm happy that's hopefully

00:28:48,970 --> 00:28:53,200
what's going to happen and for that to

00:28:51,670 --> 00:28:56,380
happen for the weights and buyers to be

00:28:53,200 --> 00:28:58,179
adjusted I need this magical step which

00:28:56,380 --> 00:29:02,820
is called back propagation it's the most

00:28:58,179 --> 00:29:02,820
scary one any questions so far

00:29:02,850 --> 00:29:09,160
everything's pretty clear then okay so

00:29:06,370 --> 00:29:11,080
back propagation is a supervised

00:29:09,160 --> 00:29:15,910
learning method for this kind of feed

00:29:11,080 --> 00:29:19,000
forward networks and it uses gradient

00:29:15,910 --> 00:29:22,330
descent so what he's going to do is

00:29:19,000 --> 00:29:23,830
going to look at the gradient of the

00:29:22,330 --> 00:29:27,370
error function which is actually the

00:29:23,830 --> 00:29:29,080
loss function with respect to each per

00:29:27,370 --> 00:29:37,750
learnable parameter sort of weights and

00:29:29,080 --> 00:29:41,610
bias so here's a scary slide for you who

00:29:37,750 --> 00:29:41,610
has done remember partial derivatives

00:29:42,270 --> 00:29:48,070
every one then okay so basically this is

00:29:46,510 --> 00:29:50,110
what's what's happening what's going

00:29:48,070 --> 00:29:51,970
what were you in to do and in back

00:29:50,110 --> 00:29:54,970
propagation is just look at each

00:29:51,970 --> 00:29:57,670
individual weight and bias in the

00:29:54,970 --> 00:29:59,710
network okay and we see I'm going to

00:29:57,670 --> 00:30:02,860
look at how much it contributes to the

00:29:59,710 --> 00:30:04,360
overall error each individual weight and

00:30:02,860 --> 00:30:06,400
bias I'm going to see if I adjust it

00:30:04,360 --> 00:30:08,650
slightly what happens my error does it

00:30:06,400 --> 00:30:10,059
go better does he go worse that's what's

00:30:08,650 --> 00:30:13,000
happening that's the partial derivative

00:30:10,059 --> 00:30:14,350
that's what's happening so if I put this

00:30:13,000 --> 00:30:17,740
in equation I know it's going to be a

00:30:14,350 --> 00:30:20,050
bit scary but the pitch view part is on

00:30:17,740 --> 00:30:22,179
the hard actually so for the output

00:30:20,050 --> 00:30:24,100
layer it's in a special case my starting

00:30:22,179 --> 00:30:26,350
point I'm going to use da is the

00:30:24,100 --> 00:30:27,670
derivative of the loss function and then

00:30:26,350 --> 00:30:30,700
I'm going to calculate DZ

00:30:27,670 --> 00:30:32,470
which is a previous step element-wise

00:30:30,700 --> 00:30:38,020
multiplication with a derivative of the

00:30:32,470 --> 00:30:39,970
sigmoid function d w DZ times a DB just

00:30:38,020 --> 00:30:42,490
easy okay

00:30:39,970 --> 00:30:45,810
so that's that's the output layer for

00:30:42,490 --> 00:30:48,280
the hidden layer they are it the

00:30:45,810 --> 00:30:50,920
starting point is slightly different so

00:30:48,280 --> 00:30:52,980
i'm still i'm going to use DZ from the

00:30:50,920 --> 00:30:56,410
previous step this one right here and

00:30:52,980 --> 00:30:58,210
i'm going to use the weight matrix from

00:30:56,410 --> 00:31:00,340
the previous layer that's my starting

00:30:58,210 --> 00:31:03,520
point for da the rest is exactly the

00:31:00,340 --> 00:31:05,560
same so this is just a special case for

00:31:03,520 --> 00:31:07,720
L per layer the rest is exactly the same

00:31:05,560 --> 00:31:09,420
you could have ten layers it'll be the

00:31:07,720 --> 00:31:13,450
same just repeat put that in the loop

00:31:09,420 --> 00:31:14,800
easily done okay let's see that in code

00:31:13,450 --> 00:31:17,380
so I'm just going to add two more

00:31:14,800 --> 00:31:20,470
parameters DW and DB because I'm going

00:31:17,380 --> 00:31:24,340
to need that and let's look at the

00:31:20,470 --> 00:31:26,620
derivative for the loss function which

00:31:24,340 --> 00:31:29,650
is basically two times friction

00:31:26,620 --> 00:31:32,140
- output - production pretty easy to

00:31:29,650 --> 00:31:34,930
implement so I'm going to add a second

00:31:32,140 --> 00:31:37,120
method or my mean square error class so

00:31:34,930 --> 00:31:38,830
it was a role already computing doing

00:31:37,120 --> 00:31:40,270
compute for the forward pass and now it

00:31:38,830 --> 00:31:42,700
will do different shade for back

00:31:40,270 --> 00:31:46,540
propagation it's just this times two and

00:31:42,700 --> 00:31:48,490
that's it and we get the matrix back for

00:31:46,540 --> 00:31:50,800
the sigmoid is actually quite easy as

00:31:48,490 --> 00:31:54,390
well it's just a sigmoid itself times

00:31:50,800 --> 00:31:57,250
one minus a sigma not the heart

00:31:54,390 --> 00:32:00,400
differentiate my third I'm just going to

00:31:57,250 --> 00:32:03,750
call the sigmoid again times 1 minus the

00:32:00,400 --> 00:32:06,540
sigmoid and that's it so compute method

00:32:03,750 --> 00:32:10,930
differentiate method in my sigmoid

00:32:06,540 --> 00:32:14,770
activation function so once I have those

00:32:10,930 --> 00:32:18,790
I'm just going to start with upper layer

00:32:14,770 --> 00:32:21,640
so as you can see calculate da just

00:32:18,790 --> 00:32:25,870
going differentiate on my loss function

00:32:21,640 --> 00:32:27,370
and then I'm going to do DZ Adam at

00:32:25,870 --> 00:32:31,090
profit because that's element wise

00:32:27,370 --> 00:32:32,620
multiplication dwn DB and then if I go

00:32:31,090 --> 00:32:34,240
for the hidden layer exactly the same

00:32:32,620 --> 00:32:38,920
thing it's just the a is slightly

00:32:34,240 --> 00:32:40,150
different but then I calculate TC and DW

00:32:38,920 --> 00:32:42,130
and DB

00:32:40,150 --> 00:32:46,660
it can have a look later on on these

00:32:42,130 --> 00:32:49,660
details so here we have my do back

00:32:46,660 --> 00:32:52,090
propagation step and the last step is a

00:32:49,660 --> 00:32:53,550
date parameters so for update parameters

00:32:52,090 --> 00:32:56,080
I'm going to use gradient descent and

00:32:53,550 --> 00:32:59,380
for that this is how I'm going to

00:32:56,080 --> 00:33:03,330
applied my W so the new W the new weight

00:32:59,380 --> 00:33:06,700
is going to be the current weight minus

00:33:03,330 --> 00:33:10,650
TW which I'm I've stored in a PHP

00:33:06,700 --> 00:33:14,050
variable times alpha and alpha is the

00:33:10,650 --> 00:33:15,910
learn herbal parameter and for B is

00:33:14,050 --> 00:33:16,650
exactly the same but I'm using DB

00:33:15,910 --> 00:33:19,240
instead

00:33:16,650 --> 00:33:22,300
so the learn herbal parameter alpha

00:33:19,240 --> 00:33:25,710
allows me to control the jump between

00:33:22,300 --> 00:33:28,960
each iteration so if if office two big

00:33:25,710 --> 00:33:31,120
jumps are too too big as well and I

00:33:28,960 --> 00:33:33,340
might miss this is the error

00:33:31,120 --> 00:33:35,200
I might miss the optimum value for the

00:33:33,340 --> 00:33:37,390
error if the learning parameter is too

00:33:35,200 --> 00:33:40,840
big but if the learner will prompt is

00:33:37,390 --> 00:33:43,120
too small it will take a long time to

00:33:40,840 --> 00:33:45,580
converge it will take a long time and it

00:33:43,120 --> 00:33:47,320
may even miss the optimum value as well

00:33:45,580 --> 00:33:50,590
so there is a sweet spot for the long

00:33:47,320 --> 00:33:53,800
above parameter so that's why I'm going

00:33:50,590 --> 00:33:56,800
to do here I'm going to say my learning

00:33:53,800 --> 00:33:59,140
rate is going to be 0.1 we can play with

00:33:56,800 --> 00:34:00,520
this it's just a fixed value that I'm

00:33:59,140 --> 00:34:02,760
going to put there so to update

00:34:00,520 --> 00:34:04,840
parameters I'm just going to every layer

00:34:02,760 --> 00:34:07,570
calculate the new W based on the

00:34:04,840 --> 00:34:11,620
previous W times my learning rate and

00:34:07,570 --> 00:34:14,920
same for P and that's my last step and

00:34:11,620 --> 00:34:16,660
finally I just keep track of the error

00:34:14,920 --> 00:34:19,450
because I need to know when to stop

00:34:16,660 --> 00:34:22,750
above okay

00:34:19,450 --> 00:34:26,260
so this is the full code for the

00:34:22,750 --> 00:34:28,900
learning for learning in a neural

00:34:26,260 --> 00:34:35,290
network written in PHP that's a full

00:34:28,900 --> 00:34:39,340
code for that and for this training

00:34:35,290 --> 00:34:44,050
complaints the worst part is over by the

00:34:39,340 --> 00:34:45,679
way no one left yet so I guess you can

00:34:44,050 --> 00:34:48,559
say to the end

00:34:45,679 --> 00:34:50,929
okay so just recap so algorithm for

00:34:48,559 --> 00:34:53,000
training I started with initializing

00:34:50,929 --> 00:34:55,220
weights and buyers randomly and also to

00:34:53,000 --> 00:34:59,569
zero I've set some conditions for when

00:34:55,220 --> 00:35:02,540
to stop because I don't want this

00:34:59,569 --> 00:35:04,369
running forever if it's it has no point

00:35:02,540 --> 00:35:08,089
and then for each training example I

00:35:04,369 --> 00:35:10,130
just don't do those four steps I do for

00:35:08,089 --> 00:35:12,349
a pass from the beginning goes through

00:35:10,130 --> 00:35:15,079
all the end to the network I check the

00:35:12,349 --> 00:35:17,180
error on compute costs I applied some

00:35:15,079 --> 00:35:20,480
magic using back propagation and

00:35:17,180 --> 00:35:22,160
derivatives back to the beginning I

00:35:20,480 --> 00:35:24,290
adjust my weights and buyers keep

00:35:22,160 --> 00:35:27,559
looping until weighted buyers get to an

00:35:24,290 --> 00:35:31,339
optimum value so this is an example

00:35:27,559 --> 00:35:33,260
taken from the tensorflow website which

00:35:31,339 --> 00:35:36,109
is a Python framework for machine

00:35:33,260 --> 00:35:38,000
learning and hopefully this is what

00:35:36,109 --> 00:35:40,369
we're going to have what's what's going

00:35:38,000 --> 00:35:42,500
to happen during the learning phase so

00:35:40,369 --> 00:35:47,690
as you can see some connections will

00:35:42,500 --> 00:35:50,720
grow stronger so a bit thinner and some

00:35:47,690 --> 00:35:54,109
neurons will specialize on identifying

00:35:50,720 --> 00:35:57,230
the blue ones some other will specialize

00:35:54,109 --> 00:36:00,410
on identifying orange ones so over time

00:35:57,230 --> 00:36:03,799
as you can see it will slowly be able to

00:36:00,410 --> 00:36:07,069
find some boundaries between the blue

00:36:03,799 --> 00:36:08,240
and the orange circles so hopefully

00:36:07,069 --> 00:36:10,700
that's what's going to happen and you

00:36:08,240 --> 00:36:12,589
can see the error going dropping right

00:36:10,700 --> 00:36:17,869
there so hopefully that's what's going

00:36:12,589 --> 00:36:21,950
to happen during the learning phase so

00:36:17,869 --> 00:36:25,549
let's put this to the training so i'm

00:36:21,950 --> 00:36:28,460
going to call my script and epoch is

00:36:25,549 --> 00:36:32,119
just a number iteration so after 1000 X

00:36:28,460 --> 00:36:36,170
I get an error of not point 22 almost no

00:36:32,119 --> 00:36:39,440
point 23 but 400 iterations later

00:36:36,170 --> 00:36:42,190
it has dropped 100 times actually forget

00:36:39,440 --> 00:36:47,420
it's going the right direction and

00:36:42,190 --> 00:36:50,299
around 10000 epochs it has fallen or

00:36:47,420 --> 00:36:52,520
it's going to fall below my maximum

00:36:50,299 --> 00:36:57,000
error which was if you remember naught

00:36:52,520 --> 00:37:00,970
point naught naught 1 so it stops

00:36:57,000 --> 00:37:03,340
and now if we put this to a test and I'm

00:37:00,970 --> 00:37:05,740
going to cheat here because I only have

00:37:03,340 --> 00:37:08,590
for example so I'm going to use the same

00:37:05,740 --> 00:37:13,990
examples in real world you would feed it

00:37:08,590 --> 00:37:17,170
on something new on scene say if I to

00:37:13,990 --> 00:37:19,510
implement predict it's simply called do

00:37:17,170 --> 00:37:21,760
forward propagation remember now the

00:37:19,510 --> 00:37:24,580
network has to add stop learning so the

00:37:21,760 --> 00:37:28,750
weight and buyers they are frozen so

00:37:24,580 --> 00:37:30,370
keep it like gold those parameters are

00:37:28,750 --> 00:37:31,960
frozen so I just called do forward

00:37:30,370 --> 00:37:33,160
propagation and the value that I get at

00:37:31,960 --> 00:37:36,400
the end of the network that's my

00:37:33,160 --> 00:37:39,550
prediction so if I call it with zero

00:37:36,400 --> 00:37:41,500
zero I get a prediction pack of point

00:37:39,550 --> 00:37:46,630
zero three which is pretty close to zero

00:37:41,500 --> 00:37:48,940
same here and if I call read my inputs

00:37:46,630 --> 00:37:52,380
which are 0 1 and 1 0 I get something

00:37:48,940 --> 00:38:01,060
that's really close to 1 so I'll say

00:37:52,380 --> 00:38:04,840
this is a success ok you don't have to

00:38:01,060 --> 00:38:07,540
use the XR I'll challenge you to pick a

00:38:04,840 --> 00:38:10,780
more creative example you have more time

00:38:07,540 --> 00:38:12,400
than just 45 minutes so go and play with

00:38:10,780 --> 00:38:15,730
this we need something more creative or

00:38:12,400 --> 00:38:19,210
more useful play with different

00:38:15,730 --> 00:38:20,770
activation and cost functions as you can

00:38:19,210 --> 00:38:23,200
see you have just implemented sigmoid

00:38:20,770 --> 00:38:26,500
but there are many other activation

00:38:23,200 --> 00:38:28,600
functions the relative function tan age

00:38:26,500 --> 00:38:31,150
there's a lot more that you can play

00:38:28,600 --> 00:38:33,310
with it get better results you can play

00:38:31,150 --> 00:38:35,920
with different different cost functions

00:38:33,310 --> 00:38:37,840
as well and the learning rate you can

00:38:35,920 --> 00:38:40,690
play with that let's make it train

00:38:37,840 --> 00:38:43,480
faster let's see if it still works or I

00:38:40,690 --> 00:38:45,910
know that sweet point 1 it works but or

00:38:43,480 --> 00:38:48,700
if we use just one maybe just converts

00:38:45,910 --> 00:38:50,710
off for 1,000 trations and if you do

00:38:48,700 --> 00:38:54,760
that you can open a PR on this Reaper

00:38:50,710 --> 00:38:56,770
that I've shared before and why not try

00:38:54,760 --> 00:39:00,220
with different topology I've used one

00:38:56,770 --> 00:39:01,900
hidden layer why not use two five hidden

00:39:00,220 --> 00:39:04,120
layers how does he going to work maybe

00:39:01,900 --> 00:39:07,090
odd on that one hidden layer mat you can

00:39:04,120 --> 00:39:08,900
put five neurons five hidden neurons how

00:39:07,090 --> 00:39:11,960
does he go into work

00:39:08,900 --> 00:39:14,120
and you can even compete a kegger and

00:39:11,960 --> 00:39:15,260
you can win some prizes it's a data

00:39:14,120 --> 00:39:18,280
science machine lying competition

00:39:15,260 --> 00:39:21,050
website check it out

00:39:18,280 --> 00:39:25,370
so quick recap on what we have seen

00:39:21,050 --> 00:39:28,460
today near networks are inspired by the

00:39:25,370 --> 00:39:31,280
human brain but actually today is more

00:39:28,460 --> 00:39:32,510
of a mathematical challenge and

00:39:31,280 --> 00:39:37,430
engineering challenge

00:39:32,510 --> 00:39:39,530
it's divergent a bit from biology and we

00:39:37,430 --> 00:39:41,990
saw a feed-forward network but there are

00:39:39,530 --> 00:39:44,900
others so if you if you'd go and let's

00:39:41,990 --> 00:39:46,970
say use something for video processing

00:39:44,900 --> 00:39:49,010
that's just an autonomous driving then

00:39:46,970 --> 00:39:51,320
you'd use convolutional networks which

00:39:49,010 --> 00:39:54,950
are a lot more complex and wouldn't fit

00:39:51,320 --> 00:39:57,410
in the screen and or for speech you

00:39:54,950 --> 00:40:01,400
could use recurrent neural networks

00:39:57,410 --> 00:40:07,430
because they look at the previous setups

00:40:01,400 --> 00:40:09,500
and words of what you said we today we

00:40:07,430 --> 00:40:12,860
use supervised machine learning which is

00:40:09,500 --> 00:40:15,350
just learning from examples but there is

00:40:12,860 --> 00:40:19,280
also unsupervised machine learning in

00:40:15,350 --> 00:40:21,500
which you don't label data you just ask

00:40:19,280 --> 00:40:24,260
it to find patterns in data to find

00:40:21,500 --> 00:40:27,140
similarities so that's that's that's

00:40:24,260 --> 00:40:30,170
really useful as well because labeling

00:40:27,140 --> 00:40:32,150
data is a lot of work there are actually

00:40:30,170 --> 00:40:35,630
some companies making money just by

00:40:32,150 --> 00:40:38,330
building the data sets because machine

00:40:35,630 --> 00:40:40,910
learning needs large data sets of

00:40:38,330 --> 00:40:42,890
labeled data so some companies even make

00:40:40,910 --> 00:40:44,300
money just taking pictures and putting a

00:40:42,890 --> 00:40:46,210
label in it and then selling it to

00:40:44,300 --> 00:40:50,960
companies that actually use machine like

00:40:46,210 --> 00:40:54,140
if you want to do that and in this

00:40:50,960 --> 00:40:57,320
example we have used back propagation

00:40:54,140 --> 00:41:00,980
and grading descent which I think is the

00:40:57,320 --> 00:41:04,850
magical bit in a neural network and can

00:41:00,980 --> 00:41:06,650
be done in using PHP and in theory as

00:41:04,850 --> 00:41:09,770
you can see you can approximate any

00:41:06,650 --> 00:41:11,630
function if we are using a non-linear

00:41:09,770 --> 00:41:15,280
activation function just as a sigmoid

00:41:11,630 --> 00:41:18,680
and that's what makes them so powerful

00:41:15,280 --> 00:41:20,740
okay so today you have seen a network

00:41:18,680 --> 00:41:27,410
like this

00:41:20,740 --> 00:41:30,320
but if you added another hidden layer or

00:41:27,410 --> 00:41:36,530
maybe even more hidden layers and if you

00:41:30,320 --> 00:41:40,040
kept adding more hidden layers then you

00:41:36,530 --> 00:41:42,050
enter the realms of deep learning so if

00:41:40,040 --> 00:41:47,000
you see deep learning in an article

00:41:42,050 --> 00:41:50,840
under knees is basically new networks

00:41:47,000 --> 00:41:52,430
with many many many layers then you then

00:41:50,840 --> 00:41:55,790
we can call it deep learning

00:41:52,430 --> 00:41:58,490
that's why deploying is all about so

00:41:55,790 --> 00:42:01,730
before I finish just a few resources I

00:41:58,490 --> 00:42:06,530
would like to share so if you're still

00:42:01,730 --> 00:42:08,450
interested in this topic the Wikipedia

00:42:06,530 --> 00:42:10,760
article is quite good on artificial in

00:42:08,450 --> 00:42:13,940
your networks there are some really good

00:42:10,760 --> 00:42:19,040
courses I think this one was a the

00:42:13,940 --> 00:42:22,190
original course on Coursera this one is

00:42:19,040 --> 00:42:26,200
really good and it covers modern uses

00:42:22,190 --> 00:42:29,840
modern tools and but I think it's paid

00:42:26,200 --> 00:42:31,490
they covers things such as convolutional

00:42:29,840 --> 00:42:35,210
networks with autonomous driving and

00:42:31,490 --> 00:42:40,760
speech recognition and it uses Python

00:42:35,210 --> 00:42:46,280
mostly I think tensorflow actually the

00:42:40,760 --> 00:42:51,200
first one is more of theory it's really

00:42:46,280 --> 00:42:53,720
cool this is one of the heroes of AI

00:42:51,200 --> 00:42:57,320
Geoffrey Hinton so please check out

00:42:53,720 --> 00:42:59,120
these slides I would say start with this

00:42:57,320 --> 00:43:03,860
ones because it's a little hard to

00:42:59,120 --> 00:43:05,360
digest and there are so many resources

00:43:03,860 --> 00:43:07,670
nowadays of our machine learning and

00:43:05,360 --> 00:43:09,290
they actually free google has a really

00:43:07,670 --> 00:43:12,620
good one as a crash course on machine

00:43:09,290 --> 00:43:15,980
learning and it's very hands-on as well

00:43:12,620 --> 00:43:17,990
and last one which I would like to

00:43:15,980 --> 00:43:19,850
highlight that one which is really good

00:43:17,990 --> 00:43:21,370
if we're into the backpropagation step

00:43:19,850 --> 00:43:26,480
one to understand a little bit better

00:43:21,370 --> 00:43:33,560
more partial derivatives maybe and so

00:43:26,480 --> 00:43:36,020
this book as well it's not HP its users

00:43:33,560 --> 00:43:37,310
- but it's hands-on so you see some

00:43:36,020 --> 00:43:42,140
examples you can play with that it's

00:43:37,310 --> 00:43:45,710
really good and if you're in a rush this

00:43:42,140 --> 00:43:47,750
is a less than five minutes video that

00:43:45,710 --> 00:43:50,590
explains just a neuro-networks

00:43:47,750 --> 00:43:53,060
it's really good it's also from the deep

00:43:50,590 --> 00:43:54,830
specialization course I put on the

00:43:53,060 --> 00:43:58,970
previous slide and it's available on

00:43:54,830 --> 00:44:02,470
YouTube and with that I reach the end of

00:43:58,970 --> 00:44:07,490
my talk thanks so much for your patience

00:44:02,470 --> 00:44:09,540
and I'm ready for some questions if you

00:44:07,490 --> 00:44:14,389
have it thank you

00:44:09,540 --> 00:44:14,389
[Applause]

00:44:16,300 --> 00:44:31,469
[Music]

00:44:28,070 --> 00:44:31,469

YouTube URL: https://www.youtube.com/watch?v=iC13u7tspyg


