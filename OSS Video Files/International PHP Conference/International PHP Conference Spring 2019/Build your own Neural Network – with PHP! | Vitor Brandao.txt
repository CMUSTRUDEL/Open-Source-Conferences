Title: Build your own Neural Network â€“ with PHP! | Vitor Brandao
Publication date: 2019-11-13
Playlist: International PHP Conference Spring 2019
Description: 
	Speaker: Vitor Brandao (Noiselabs Consulting Ltd.) | https://phpconference.com/speaker/vitor-brandao/

Curious about all the hype around machine learning and artificial intelligence? Heard of "neural networks" and "deep learning", but confused about what it really means? In this talk, youâ€™ll see what artificial neural networks (ANN) look like and how they can learn. And along the way, youâ€™ll discover how you can build your own ANN â€“ with PHP of course!

ðŸ¤— Come, join us at the next International PHP Conference | https://phpconference.com/
ðŸ‘ Like us on Facebook | https://www.facebook.com/ipc.germany/
ðŸ‘‰ Follow us on Twitter | https://twitter.com/phpconference
Captions: 
	00:00:00,250 --> 00:00:04,840
[Music]

00:00:05,359 --> 00:00:10,889
and thank you all for coming and thank

00:00:09,990 --> 00:00:13,950
you too

00:00:10,889 --> 00:00:15,660
ie PC organizers for having me it's a

00:00:13,950 --> 00:00:18,119
pleasure it's my first time in this

00:00:15,660 --> 00:00:21,800
conference for Simon Berlin as well I'm

00:00:18,119 --> 00:00:26,840
enjoying a lot and this talk is called

00:00:21,800 --> 00:00:31,910
build your own neural network with PHP

00:00:26,840 --> 00:00:34,790
so today we are going to build a an

00:00:31,910 --> 00:00:39,090
artificial neural network from scratch

00:00:34,790 --> 00:00:42,390
using good old PHP and yes that's

00:00:39,090 --> 00:00:47,820
possible so first just a few things

00:00:42,390 --> 00:00:50,059
about myself I'm Vito I'm Portuguese as

00:00:47,820 --> 00:00:54,989
you can tell by my accent

00:00:50,059 --> 00:00:58,559
but I'm working as a software consultant

00:00:54,989 --> 00:01:00,300
now in UK so if you do follow my twitter

00:00:58,559 --> 00:01:04,019
at noise labs

00:01:00,300 --> 00:01:07,710
heads up that's 90% praxic tweets I

00:01:04,019 --> 00:01:10,850
don't recommend it but if it's instead

00:01:07,710 --> 00:01:14,729
you go to my blog noise labs at i/o I

00:01:10,850 --> 00:01:20,909
can guarantee that one is 100% brexit

00:01:14,729 --> 00:01:22,920
free and now a few disclaimers because I

00:01:20,909 --> 00:01:24,960
have to start with those so first one

00:01:22,920 --> 00:01:29,970
I'm I'm not an expert

00:01:24,960 --> 00:01:32,780
I can see someone laughing I'm not an

00:01:29,970 --> 00:01:35,369
expert in this field I don't have

00:01:32,780 --> 00:01:37,380
experience with AI or machine learning

00:01:35,369 --> 00:01:40,439
production and as such I wouldn't be

00:01:37,380 --> 00:01:43,399
able to give you industry advice just

00:01:40,439 --> 00:01:47,460
consider myself to be an A student

00:01:43,399 --> 00:01:51,140
second there will be math throughout the

00:01:47,460 --> 00:01:55,500
talk sorry about that

00:01:51,140 --> 00:01:58,710
or to be more precise oversimplified

00:01:55,500 --> 00:02:02,369
math and I hope it's not too scary and

00:01:58,710 --> 00:02:05,189
you just stayed throughout a talk and do

00:02:02,369 --> 00:02:07,170
not leave hopefully and the last one is

00:02:05,189 --> 00:02:09,179
that there is a lot to cover in this

00:02:07,170 --> 00:02:12,120
topic there are a lot of slides so I'm

00:02:09,179 --> 00:02:13,680
I'll be going fairly quickly but if you

00:02:12,120 --> 00:02:15,569
have any doubt

00:02:13,680 --> 00:02:18,329
on some slides please just raise your

00:02:15,569 --> 00:02:22,230
hand and I'll stop and I can clarify it

00:02:18,329 --> 00:02:24,569
right there okay yeah okay so the reason

00:02:22,230 --> 00:02:27,030
why I'm giving this talk is because AI

00:02:24,569 --> 00:02:29,340
and machine learning is all the hype

00:02:27,030 --> 00:02:32,069
these days I mean you can you can barely

00:02:29,340 --> 00:02:35,069
go one day without hearing about machine

00:02:32,069 --> 00:02:41,459
learning the news and the trend is

00:02:35,069 --> 00:02:45,720
growing and AI is everywhere and it's

00:02:41,459 --> 00:02:49,650
kind of magical I guess the only thing

00:02:45,720 --> 00:02:51,540
more hype than AI is Bach chain but

00:02:49,650 --> 00:02:54,090
about that I know pretty much close to

00:02:51,540 --> 00:02:57,299
nothing so you have to find another talk

00:02:54,090 --> 00:02:58,859
and another speaker for this now the

00:02:57,299 --> 00:03:01,950
second reason why I'm giving this talk

00:02:58,859 --> 00:03:05,609
is because it's kind of rare to see PHP

00:03:01,950 --> 00:03:07,349
and machine learning together and I

00:03:05,609 --> 00:03:10,319
think we deserve our fair share of AI

00:03:07,349 --> 00:03:12,150
and also if you're trying to learn

00:03:10,319 --> 00:03:14,549
something complex such as machine

00:03:12,150 --> 00:03:16,560
learning can be then I think it makes

00:03:14,549 --> 00:03:18,629
sense to pick a language you're already

00:03:16,560 --> 00:03:21,299
comfortable with such as PHP that's your

00:03:18,629 --> 00:03:23,549
primary language and learn then learn

00:03:21,299 --> 00:03:25,199
the other complex thing which would be

00:03:23,549 --> 00:03:31,769
machine learning I think that makes

00:03:25,199 --> 00:03:33,690
sense so AI is used pretty much

00:03:31,769 --> 00:03:35,760
everywhere ie you you can see it on

00:03:33,690 --> 00:03:38,159
speech recognition hand written

00:03:35,760 --> 00:03:41,129
recognition automated trading systems

00:03:38,159 --> 00:03:46,169
and the recommender systems that you see

00:03:41,129 --> 00:03:49,199
on Amazon or Netflix even though

00:03:46,169 --> 00:03:54,419
sometimes you wonder what kind of AI is

00:03:49,199 --> 00:03:58,260
there based on those recommendations so

00:03:54,419 --> 00:04:01,769
AI will be driving as soon everywhere

00:03:58,260 --> 00:04:06,090
it will be our doctors it will be our

00:04:01,769 --> 00:04:08,370
lawyers it will even replace us as

00:04:06,090 --> 00:04:10,709
programmers by the way this is a pretty

00:04:08,370 --> 00:04:12,810
cool article there is a link at the

00:04:10,709 --> 00:04:16,289
bottom check it out the slides already

00:04:12,810 --> 00:04:19,700
online it's really cool maybe you can

00:04:16,289 --> 00:04:23,090
use it and still charge your client

00:04:19,700 --> 00:04:25,460
and if you have enough free time why not

00:04:23,090 --> 00:04:30,200
use machine learning to put Nicolas Cage

00:04:25,460 --> 00:04:34,420
face on films such as Indiana Jones what

00:04:30,200 --> 00:04:37,430
the Lord of the Rings instant classics

00:04:34,420 --> 00:04:39,290
now at the foundation of many of these

00:04:37,430 --> 00:04:43,630
are AI applications that we have just

00:04:39,290 --> 00:04:47,600
seen we have artificial neural networks

00:04:43,630 --> 00:04:50,890
so today we're going to see why are they

00:04:47,600 --> 00:04:53,360
called new networks how do they learn

00:04:50,890 --> 00:04:57,260
how can we write one using our favorite

00:04:53,360 --> 00:05:03,260
language for some and what the hell is

00:04:57,260 --> 00:05:07,970
deep learning so without further delay

00:05:03,260 --> 00:05:10,820
let's start with the light topic the

00:05:07,970 --> 00:05:16,160
human brain so this is a picture of a

00:05:10,820 --> 00:05:18,950
brain on half of it and in a full brain

00:05:16,160 --> 00:05:22,490
you can see you you have 86 billion

00:05:18,950 --> 00:05:24,860
neurons and the neuron which looks like

00:05:22,490 --> 00:05:27,080
this is a cell that transmits

00:05:24,860 --> 00:05:31,940
information through chemical and

00:05:27,080 --> 00:05:34,070
electrical signals so you have the

00:05:31,940 --> 00:05:36,140
dendrite that connects to the previous

00:05:34,070 --> 00:05:38,660
neurons where the camera with the

00:05:36,140 --> 00:05:41,210
signals arrive then they are processed

00:05:38,660 --> 00:05:43,520
in the cell body and the nucleus and

00:05:41,210 --> 00:05:47,030
they flow through the next set of

00:05:43,520 --> 00:05:49,370
neurons through the accent so another

00:05:47,030 --> 00:05:51,650
way to look at this it to say that our

00:05:49,370 --> 00:05:55,010
dendrite is performing some sort of

00:05:51,650 --> 00:05:59,240
input our nucleus is an activation

00:05:55,010 --> 00:06:02,870
function and the axon can be we can say

00:05:59,240 --> 00:06:07,880
it's the upper four for this neuron so

00:06:02,870 --> 00:06:09,950
in 1943 McCulloch and Pitt's inspired by

00:06:07,880 --> 00:06:12,260
this biological model they developed a

00:06:09,950 --> 00:06:17,330
mathematical model which became the

00:06:12,260 --> 00:06:20,540
first artificial neuron and it looks

00:06:17,330 --> 00:06:22,340
something like this so this is another

00:06:20,540 --> 00:06:24,560
representation of the neuron so let's

00:06:22,340 --> 00:06:26,480
say we have two inputs so it's

00:06:24,560 --> 00:06:29,400
connecting to two previous neurons let's

00:06:26,480 --> 00:06:31,860
say it's X 1 and X 2

00:06:29,400 --> 00:06:37,080
and each collection he's affected by a

00:06:31,860 --> 00:06:41,340
weight w1 and w2 and then within the

00:06:37,080 --> 00:06:44,009
within the cell we perform this sum so

00:06:41,340 --> 00:06:49,110
we multiply weight times X which is our

00:06:44,009 --> 00:06:51,690
input plus the bias and the weight and

00:06:49,110 --> 00:06:55,860
the bias is what we call the learn about

00:06:51,690 --> 00:06:58,020
parameters so the weights they are the

00:06:55,860 --> 00:07:03,289
influence of one neuron on the next one

00:06:58,020 --> 00:07:08,430
so the connection can be thin or thick

00:07:03,289 --> 00:07:11,250
the bias is an addition term that allow

00:07:08,430 --> 00:07:13,710
us to shift the signal output signal

00:07:11,250 --> 00:07:16,830
left or right so that it matches our

00:07:13,710 --> 00:07:23,280
desired output so these are our learn

00:07:16,830 --> 00:07:26,400
about parameters so that sum plus that

00:07:23,280 --> 00:07:31,500
sum will then be puff through a function

00:07:26,400 --> 00:07:34,680
an activation function and we get the

00:07:31,500 --> 00:07:37,260
output for that neuron so we have input

00:07:34,680 --> 00:07:39,150
and a will be our output this is what

00:07:37,260 --> 00:07:45,210
happening in a muhfucka mathematical

00:07:39,150 --> 00:07:47,550
model of a neuron so in summary for two

00:07:45,210 --> 00:07:51,360
inputs this is what's happening W 1 and

00:07:47,550 --> 00:07:52,860
W 2 plus B equals Z with passes L

00:07:51,360 --> 00:07:57,539
through an activation function we get

00:07:52,860 --> 00:08:01,320
near an output activation function

00:07:57,539 --> 00:08:03,000
models the the firing rate is used to

00:08:01,320 --> 00:08:05,250
model the firing rate and most

00:08:03,000 --> 00:08:07,199
importantly it adds nonlinear

00:08:05,250 --> 00:08:09,330
capabilities to the network and we're

00:08:07,199 --> 00:08:11,250
going to see why that matters what is

00:08:09,330 --> 00:08:14,039
why that's really important later in a

00:08:11,250 --> 00:08:16,169
talk so a very common activation

00:08:14,039 --> 00:08:18,780
function is the sigmoid function which

00:08:16,169 --> 00:08:22,080
is this s-shaped function so for var for

00:08:18,780 --> 00:08:24,570
large negative numbers the output will

00:08:22,080 --> 00:08:27,570
be 0 and for large positive numbers the

00:08:24,570 --> 00:08:31,560
output will be 1 so the output will be

00:08:27,570 --> 00:08:34,740
always between 0 and 1 this path shape

00:08:31,560 --> 00:08:37,950
function the sigmoid function so if you

00:08:34,740 --> 00:08:42,180
have another look at our our neuron and

00:08:37,950 --> 00:08:42,780
if we zoom out then there you have it

00:08:42,180 --> 00:08:45,900
our

00:08:42,780 --> 00:08:48,150
very first neural network very simple

00:08:45,900 --> 00:08:51,000
neural network we just one neuron one

00:08:48,150 --> 00:08:53,790
upper layer because funnily enough input

00:08:51,000 --> 00:08:58,550
layer doesn't count as a layer say one

00:08:53,790 --> 00:08:58,550
output layer one neuron and two inputs

00:08:59,390 --> 00:09:06,570
now it does that funny sum and you go

00:09:05,160 --> 00:09:10,950
through an activation function and

00:09:06,570 --> 00:09:12,930
wherever but how does it learn I mean

00:09:10,950 --> 00:09:16,650
how does it do something interesting and

00:09:12,930 --> 00:09:21,810
the answer is by using machine learning

00:09:16,650 --> 00:09:25,740
and in this case specifically supervised

00:09:21,810 --> 00:09:26,070
machine learning so what is machine

00:09:25,740 --> 00:09:28,980
learning

00:09:26,070 --> 00:09:32,910
so in machine learning we have a set of

00:09:28,980 --> 00:09:35,820
known inputs so there are known inputs X

00:09:32,910 --> 00:09:38,010
and we have a set of known outputs Y and

00:09:35,820 --> 00:09:40,560
what we want is to learn a function that

00:09:38,010 --> 00:09:44,130
map's X to Y we don't know the function

00:09:40,560 --> 00:09:46,350
yet we need to find it out so that once

00:09:44,130 --> 00:09:50,100
we have found out that function we can

00:09:46,350 --> 00:09:53,280
throw it new inputs from the same data

00:09:50,100 --> 00:09:56,070
set and we can see some very good

00:09:53,280 --> 00:09:58,950
predictions that we can trust so that's

00:09:56,070 --> 00:10:01,020
the basis for supervised machine

00:09:58,950 --> 00:10:02,370
learning and this is the departure from

00:10:01,020 --> 00:10:04,140
traditional programming so in our

00:10:02,370 --> 00:10:07,140
programs and our traditional programs

00:10:04,140 --> 00:10:09,710
what we do is we write the rules we

00:10:07,140 --> 00:10:12,630
write know the routine if statements and

00:10:09,710 --> 00:10:14,880
we give it some data and we get some

00:10:12,630 --> 00:10:16,250
answers back that's how programs work

00:10:14,880 --> 00:10:19,770
basically

00:10:16,250 --> 00:10:22,230
but with machine learning what we do is

00:10:19,770 --> 00:10:25,140
we throw it all the data we give it the

00:10:22,230 --> 00:10:28,230
answers as well and then the system will

00:10:25,140 --> 00:10:33,300
figure out the rules which I think it's

00:10:28,230 --> 00:10:38,250
pretty neat so if we have another look

00:10:33,300 --> 00:10:40,350
at our nearer Network we can see that

00:10:38,250 --> 00:10:42,450
this one with just one single neuron is

00:10:40,350 --> 00:10:44,910
able to solve linear problems such as

00:10:42,450 --> 00:10:47,190
this one so we can find out this

00:10:44,910 --> 00:10:49,890
function and if we ask you to predict

00:10:47,190 --> 00:10:52,220
the new value it will say it's over

00:10:49,890 --> 00:10:52,220
there

00:10:53,490 --> 00:11:01,210
but most interesting problems are

00:10:56,470 --> 00:11:05,320
nonlinear so what we have to do we have

00:11:01,210 --> 00:11:08,560
to upgrade our network and we do that by

00:11:05,320 --> 00:11:14,050
adding a hidden layer so our network now

00:11:08,560 --> 00:11:16,330
becomes this and now I have two layers

00:11:14,050 --> 00:11:19,980
because again input layer doesn't count

00:11:16,330 --> 00:11:23,920
I don't know why that's how it's it is

00:11:19,980 --> 00:11:27,610
so there we have it a new a network that

00:11:23,920 --> 00:11:30,250
is able to solve nonlinear problems and

00:11:27,610 --> 00:11:32,920
that's why because when the activation

00:11:30,250 --> 00:11:36,940
function is nonlinear such as the Sigma

00:11:32,920 --> 00:11:38,920
days then a two layer or bigger can be

00:11:36,940 --> 00:11:40,330
proven to be an Universal function

00:11:38,920 --> 00:11:42,640
approximator and that's why it makes

00:11:40,330 --> 00:11:46,180
them so powerful because they can in

00:11:42,640 --> 00:11:47,860
theory represent any function so let's

00:11:46,180 --> 00:11:49,930
have a look at what the training

00:11:47,860 --> 00:11:51,730
approach the learning process is going

00:11:49,930 --> 00:11:53,470
to be for this artificial neural network

00:11:51,730 --> 00:11:56,230
so we're going to start with random

00:11:53,470 --> 00:11:58,150
weights then we're going to make a

00:11:56,230 --> 00:12:00,580
prediction which I say it's y hat

00:11:58,150 --> 00:12:02,710
based on my input data it's going to be

00:12:00,580 --> 00:12:05,200
a random prediction and we're going to

00:12:02,710 --> 00:12:07,540
compare that prediction with my known

00:12:05,200 --> 00:12:08,230
input my answer the thing I know it's

00:12:07,540 --> 00:12:12,010
right

00:12:08,230 --> 00:12:13,780
y and then based on the difference I'm

00:12:12,010 --> 00:12:16,420
going to adjust my parameters my learn

00:12:13,780 --> 00:12:18,670
about parameters W and B and then I'm

00:12:16,420 --> 00:12:21,100
going to repeat that until they we are

00:12:18,670 --> 00:12:23,320
really close and I say I have a good

00:12:21,100 --> 00:12:26,350
function that map's my inputs to my

00:12:23,320 --> 00:12:29,350
known output so this is what we're going

00:12:26,350 --> 00:12:34,360
to do now let's say you're going to

00:12:29,350 --> 00:12:37,780
build you're good to build a and up to

00:12:34,360 --> 00:12:41,080
classify cats a cat falsifier y know you

00:12:37,780 --> 00:12:43,330
could make some money on that to do this

00:12:41,080 --> 00:12:45,460
you would have to collect a bunch of

00:12:43,330 --> 00:12:49,210
pictures of cats that's your data set

00:12:45,460 --> 00:12:51,130
and you would label that with a 1/4 it

00:12:49,210 --> 00:12:54,870
is a cat and then you would get a bunch

00:12:51,130 --> 00:12:57,430
of other pictures or the animals even

00:12:54,870 --> 00:13:01,300
objects and then you would label that

00:12:57,430 --> 00:13:04,210
with a 0 for it's not the cat you would

00:13:01,300 --> 00:13:05,020
feed those images all your data set with

00:13:04,210 --> 00:13:08,200
your labels

00:13:05,020 --> 00:13:11,890
into your network by the way that's just

00:13:08,200 --> 00:13:16,120
decomposed in the RGB image into a array

00:13:11,890 --> 00:13:18,339
a vector and we'd fit that into your new

00:13:16,120 --> 00:13:20,830
network you keep it training and you get

00:13:18,339 --> 00:13:23,800
a good function and then if you send it

00:13:20,830 --> 00:13:27,459
a new picture it hasn't seen before and

00:13:23,800 --> 00:13:30,370
you'd get an answer of say not when 73

00:13:27,459 --> 00:13:38,320
you could say yeah it's pretty close to

00:13:30,370 --> 00:13:40,839
one okay I think it's a cat now as much

00:13:38,320 --> 00:13:42,850
I would like and there's much I would

00:13:40,839 --> 00:13:45,399
love to teach you how to build a car

00:13:42,850 --> 00:13:48,490
classifier that's not possible in just

00:13:45,399 --> 00:13:51,040
45 minutes so I'm sorry to disappoint

00:13:48,490 --> 00:13:57,370
you but we're going to see an X or

00:13:51,040 --> 00:14:00,850
instead and just a quick reminder on the

00:13:57,370 --> 00:14:03,520
X or for X or looks like this so for

00:14:00,850 --> 00:14:07,029
equal inputs you get an upper of 0 for

00:14:03,520 --> 00:14:10,930
different inputs you get an output of 1

00:14:07,029 --> 00:14:14,020
so that's my input and output that's my

00:14:10,930 --> 00:14:16,360
data set it's really small but it's it's

00:14:14,020 --> 00:14:19,839
enough for this talk and so if I put

00:14:16,360 --> 00:14:23,410
this on a chart you can see that there

00:14:19,839 --> 00:14:28,570
is no straight line where I can separate

00:14:23,410 --> 00:14:30,190
my zeros for my ones so my very simple

00:14:28,570 --> 00:14:32,079
internetwork which can only solve new

00:14:30,190 --> 00:14:36,040
linear problems isn't it snowing good

00:14:32,079 --> 00:14:41,079
enough for an axle it sucks so for that

00:14:36,040 --> 00:14:43,089
I need this one so with a hidden layer

00:14:41,079 --> 00:14:45,910
it now I'm able to solve nonlinear

00:14:43,089 --> 00:14:48,070
problems and my network which is the one

00:14:45,910 --> 00:14:51,880
that we're going to code looks like this

00:14:48,070 --> 00:14:55,140
ok so as you can see x1 x2 mine on

00:14:51,880 --> 00:14:58,540
output my buyers over there and then

00:14:55,140 --> 00:15:02,500
each connection each neuron is connected

00:14:58,540 --> 00:15:06,010
and for each one I have an Associated

00:15:02,500 --> 00:15:07,870
weight so it's fully connected and this

00:15:06,010 --> 00:15:10,209
is an example for a feed-forward neural

00:15:07,870 --> 00:15:12,760
network there are a lot more apologies

00:15:10,209 --> 00:15:16,420
this one is a feed for a new network and

00:15:12,760 --> 00:15:20,930
it's good for an EXO

00:15:16,420 --> 00:15:28,220
so with this information we're now ready

00:15:20,930 --> 00:15:30,050
to start coding by the way all the codes

00:15:28,220 --> 00:15:33,530
I'm going to show you the next slides is

00:15:30,050 --> 00:15:37,400
available on github it's it it is a

00:15:33,530 --> 00:15:39,320
fully working example so don't worry if

00:15:37,400 --> 00:15:41,240
you're going if you miss some details on

00:15:39,320 --> 00:15:43,760
the next slides you can just check it

00:15:41,240 --> 00:15:47,120
out run it locally I think it even runs

00:15:43,760 --> 00:15:50,870
on docker and just don't worry you'll be

00:15:47,120 --> 00:15:52,420
able to play with it after the talk okay

00:15:50,870 --> 00:15:55,130
so let's start with something

00:15:52,420 --> 00:15:57,440
predictable let's say let's write a

00:15:55,130 --> 00:15:58,970
class called neural network class and in

00:15:57,440 --> 00:16:01,550
this class I'm going to have two methods

00:15:58,970 --> 00:16:05,570
the Train method for the learning phase

00:16:01,550 --> 00:16:09,650
any predict method for one I want to

00:16:05,570 --> 00:16:13,490
make predictions or new unseen data okay

00:16:09,650 --> 00:16:15,950
and so for the XOR example I'm going to

00:16:13,490 --> 00:16:18,980
have my data set with four training

00:16:15,950 --> 00:16:23,300
examples and for each one I have my

00:16:18,980 --> 00:16:26,050
answer below there just much and I can

00:16:23,300 --> 00:16:30,710
now instantiate my new network class

00:16:26,050 --> 00:16:33,680
which does pretty much nothing so let's

00:16:30,710 --> 00:16:36,830
go to step number two let's define our

00:16:33,680 --> 00:16:38,690
topology so you already seen this we

00:16:36,830 --> 00:16:42,020
have two inputs we're going to have to

00:16:38,690 --> 00:16:45,680
heat and neurons and because it's just

00:16:42,020 --> 00:16:48,350
one hidden layer two layers hidden an

00:16:45,680 --> 00:16:51,350
output and one output so this is my

00:16:48,350 --> 00:16:53,330
topology so I need to store my

00:16:51,350 --> 00:16:56,750
parameters as well so in my class

00:16:53,330 --> 00:17:01,520
parameters I will store weight and

00:16:56,750 --> 00:17:05,810
buyers z na what which you already seen

00:17:01,520 --> 00:17:07,970
on that neuron diagram and I saw that in

00:17:05,810 --> 00:17:11,240
my variable P I know we shouldn't use

00:17:07,970 --> 00:17:14,569
shopping for variables but I think you

00:17:11,240 --> 00:17:19,010
can excuse me on this one now I could be

00:17:14,569 --> 00:17:20,689
using to store to store my wbd and I

00:17:19,010 --> 00:17:23,060
could be using nested arrays and

00:17:20,689 --> 00:17:27,510
associative arrays to solve that I

00:17:23,060 --> 00:17:31,200
actually done that and it turns out it's

00:17:27,510 --> 00:17:34,860
pretty much a nightmare so there is a

00:17:31,200 --> 00:17:37,280
much better data structure can you guess

00:17:34,860 --> 00:17:37,280
what it is

00:17:39,200 --> 00:17:47,100
webinar yes it's much better much

00:17:45,150 --> 00:17:51,150
cleaner makes total sound and that's

00:17:47,100 --> 00:17:54,530
what we have it's actually used in data

00:17:51,150 --> 00:17:57,210
science machine learning so with a

00:17:54,530 --> 00:17:59,490
matrix what I'm going to have is

00:17:57,210 --> 00:18:01,890
probably one and B one store like this

00:17:59,490 --> 00:18:03,570
so as you can see connections coming

00:18:01,890 --> 00:18:05,429
into the first neuron on the first row

00:18:03,570 --> 00:18:09,360
connections coming to the second neuron

00:18:05,429 --> 00:18:11,730
on the second row same for B one and the

00:18:09,360 --> 00:18:14,790
same goes for the upper layer because

00:18:11,730 --> 00:18:21,480
it's just one neuron on the output it's

00:18:14,790 --> 00:18:24,179
just one right so with this and to you

00:18:21,480 --> 00:18:25,770
to implement my matrix I'm not going to

00:18:24,179 --> 00:18:28,650
write any code there is a pretty pretty

00:18:25,770 --> 00:18:32,040
good library called math PHP it's very

00:18:28,650 --> 00:18:34,230
very complete actually I recommend it

00:18:32,040 --> 00:18:36,090
so with this library I can do I can

00:18:34,230 --> 00:18:39,660
declare my matrix as an array of arrays

00:18:36,090 --> 00:18:42,540
and then I can create the matrix using

00:18:39,660 --> 00:18:45,660
the helper and then I can add matrices

00:18:42,540 --> 00:18:46,679
multiply it to a harder math product

00:18:45,660 --> 00:18:48,570
which is just an element-wise

00:18:46,679 --> 00:18:51,360
multiplication and I can go to each

00:18:48,570 --> 00:18:56,100
element in the matrix and just apply a

00:18:51,360 --> 00:18:57,330
function any function so quick note on

00:18:56,100 --> 00:19:00,690
vectorization

00:18:57,330 --> 00:19:05,760
so other languages and and libraries

00:19:00,690 --> 00:19:08,010
such as Python and numpy they do offer

00:19:05,760 --> 00:19:12,090
vectorization which is a very efficient

00:19:08,010 --> 00:19:14,490
way of multiplying matrices to the best

00:19:12,090 --> 00:19:17,820
of my knowledge PHP doesn't have this

00:19:14,490 --> 00:19:20,669
and that's why it makes it slower when

00:19:17,820 --> 00:19:23,790
compared it is other libraries but I

00:19:20,669 --> 00:19:27,270
found this PHP extension which is PHP SC

00:19:23,790 --> 00:19:31,980
I i think it's an extension yes

00:19:27,270 --> 00:19:33,179
eh BSC I we'd stay promise by the time I

00:19:31,980 --> 00:19:35,340
took this screenshot they said two

00:19:33,179 --> 00:19:38,190
thousand times last time I saw it's

00:19:35,340 --> 00:19:40,030
eight hundred maybe because just in

00:19:38,190 --> 00:19:43,240
vanilla PHP go faster

00:19:40,030 --> 00:19:44,470
and they promise to that array operation

00:19:43,240 --> 00:19:46,560
size multiplication is going to be much

00:19:44,470 --> 00:19:49,750
quicker I haven't tried it yet but

00:19:46,560 --> 00:19:54,100
please do I think there is some hope for

00:19:49,750 --> 00:19:56,290
pitch be so next step the activation

00:19:54,100 --> 00:19:59,620
function so if you remember the sigmoid

00:19:56,290 --> 00:20:03,250
function which is defined by this

00:19:59,620 --> 00:20:06,040
equation and by weight you can you can

00:20:03,250 --> 00:20:07,360
have different activations functions the

00:20:06,040 --> 00:20:11,310
sigmoid it just happens to be a good

00:20:07,360 --> 00:20:13,030
example for the axon so in my namespace

00:20:11,310 --> 00:20:15,100
activation I'm going to write a class

00:20:13,030 --> 00:20:17,310
sigmoid and then I'm going to just

00:20:15,100 --> 00:20:21,660
implement the equation pretty easy to do

00:20:17,310 --> 00:20:24,910
and then I'm going to write my compute

00:20:21,660 --> 00:20:26,680
function which applies in a matrix and

00:20:24,910 --> 00:20:29,650
then for each element in the matrix I

00:20:26,680 --> 00:20:32,860
just call the method below and there we

00:20:29,650 --> 00:20:34,240
have it a class that calculates the

00:20:32,860 --> 00:20:38,590
sigmoid honor matrix

00:20:34,240 --> 00:20:44,290
I just set it in my constructor that's

00:20:38,590 --> 00:20:49,200
my first parameter and now for the

00:20:44,290 --> 00:20:57,100
training phase and this is the algorithm

00:20:49,200 --> 00:20:58,660
that I'm going to use so I'm going to

00:20:57,100 --> 00:21:01,990
start by initializing weights and bias

00:20:58,660 --> 00:21:04,270
and then in the loop what I'm going to

00:21:01,990 --> 00:21:06,910
do is I'm going to set a maximum number

00:21:04,270 --> 00:21:09,840
of iterations because I don't want this

00:21:06,910 --> 00:21:13,830
stuff running forever if it goes bad or

00:21:09,840 --> 00:21:18,970
until my error the network error is

00:21:13,830 --> 00:21:21,430
above my desired maximum error so if the

00:21:18,970 --> 00:21:23,170
network keeps running for a long time it

00:21:21,430 --> 00:21:25,660
will be stopped by iterations if it

00:21:23,170 --> 00:21:28,450
works it will stop because the error has

00:21:25,660 --> 00:21:31,660
fallen below and then for each training

00:21:28,450 --> 00:21:34,330
example so in the X or we have just for

00:21:31,660 --> 00:21:38,110
training examples in the cat example you

00:21:34,330 --> 00:21:40,240
had a load more training examples for

00:21:38,110 --> 00:21:42,250
each training example what I'm going to

00:21:40,240 --> 00:21:44,500
do I'm going to call this four steps I'm

00:21:42,250 --> 00:21:47,710
going to do a forward pass that I'm

00:21:44,500 --> 00:21:50,470
going to compute the cost then run a

00:21:47,710 --> 00:21:52,000
back propagation step and I'm going to

00:21:50,470 --> 00:21:53,410
adjust my weight and bias and try again

00:21:52,000 --> 00:21:57,010
keep this in a loop

00:21:53,410 --> 00:21:59,200
and this is the learning process for my

00:21:57,010 --> 00:22:02,770
neural network so step number one

00:21:59,200 --> 00:22:05,380
initializing the parameters so I'm good

00:22:02,770 --> 00:22:07,240
right just a function and what I'm going

00:22:05,380 --> 00:22:10,690
to do is for the hidden layer I'm going

00:22:07,240 --> 00:22:13,240
to initialize my bias to zero that's

00:22:10,690 --> 00:22:15,190
what I'm going to do and for the weight

00:22:13,240 --> 00:22:19,930
I'm going to initialize it with the

00:22:15,190 --> 00:22:22,720
value between zero and one random value

00:22:19,930 --> 00:22:25,360
and for the output layer I'm going to do

00:22:22,720 --> 00:22:28,060
exactly the same by a zero weights

00:22:25,360 --> 00:22:29,560
random value zero between zero and one

00:22:28,060 --> 00:22:31,330
why is here in one because it just

00:22:29,560 --> 00:22:34,450
happens because of the sigmoid function

00:22:31,330 --> 00:22:37,420
time use that's a really good strategy

00:22:34,450 --> 00:22:40,540
to keep values between 0 1 0 and 1 and

00:22:37,420 --> 00:22:43,750
randomly initialization is also a very

00:22:40,540 --> 00:22:45,940
good practice when initializing a new

00:22:43,750 --> 00:22:47,680
network so I'm going to call this in my

00:22:45,940 --> 00:22:52,120
constructor and it's called only once

00:22:47,680 --> 00:22:56,260
only one creating the network so this is

00:22:52,120 --> 00:22:58,780
the initial state of my new network so

00:22:56,260 --> 00:23:01,510
as you can see random values on all the

00:22:58,780 --> 00:23:03,430
weights and the buyers are 0 so this

00:23:01,510 --> 00:23:05,080
what is what the new network looks like

00:23:03,430 --> 00:23:09,580
when it starts this is just the initial

00:23:05,080 --> 00:23:12,280
state and now for the training method

00:23:09,580 --> 00:23:14,350
first step is I just convert my inputs

00:23:12,280 --> 00:23:18,160
and targets into a matrix because that's

00:23:14,350 --> 00:23:20,860
what I use inside and then I'm going to

00:23:18,160 --> 00:23:24,910
set some constants so I'm going to say

00:23:20,860 --> 00:23:30,430
that my maximum error is going to be at

00:23:24,910 --> 00:23:32,650
a point naught naught 1 and I will lie

00:23:30,430 --> 00:23:35,200
to my network train until it reaches 20

00:23:32,650 --> 00:23:37,990
thousand iterations if we got to this

00:23:35,200 --> 00:23:40,510
number and there is still above I just

00:23:37,990 --> 00:23:42,850
assume that you failed to learn maybe

00:23:40,510 --> 00:23:45,430
there is still going up again and not

00:23:42,850 --> 00:23:48,790
decreasing like I expected so this is

00:23:45,430 --> 00:23:50,170
just a safe switch if it gets that point

00:23:48,790 --> 00:23:52,570
things have gone bad

00:23:50,170 --> 00:23:56,590
let's stop it not burn more CPU cycles

00:23:52,570 --> 00:23:59,720
and the original error is going to be

00:23:56,590 --> 00:24:01,860
infinite that's my starting point

00:23:59,720 --> 00:24:03,420
obviously you can play with its numbers

00:24:01,860 --> 00:24:09,870
and other numbers make sense for all the

00:24:03,420 --> 00:24:13,200
examples so this is the skeleton for my

00:24:09,870 --> 00:24:17,550
training step so the outer loop and then

00:24:13,200 --> 00:24:19,920
the loop for a for the examples and then

00:24:17,550 --> 00:24:32,190
below I just keep track of the error for

00:24:19,920 --> 00:24:55,410
that I think it's gone oh maybe the

00:24:32,190 --> 00:25:08,430
projector still showing on my screen I'm

00:24:55,410 --> 00:25:13,500
just going to plug it in again yeah okay

00:25:08,430 --> 00:25:16,890
we're back it seems like I have what 15

00:25:13,500 --> 00:25:21,540
more minutes we go to time zones okay so

00:25:16,890 --> 00:25:22,890
next step forward propagation I'm going

00:25:21,540 --> 00:25:25,290
to use the same equations from the

00:25:22,890 --> 00:25:26,790
original example just now it's going to

00:25:25,290 --> 00:25:29,310
be in matrix format

00:25:26,790 --> 00:25:34,140
hence the Capitals so for the hidden

00:25:29,310 --> 00:25:36,180
layer W times X and then pass through my

00:25:34,140 --> 00:25:38,730
function with these sigmoid it's this

00:25:36,180 --> 00:25:40,740
symbol right here okay so that's my

00:25:38,730 --> 00:25:42,630
activation function and below the only

00:25:40,740 --> 00:25:45,990
difference is that I'm going to use

00:25:42,630 --> 00:25:48,810
activations from the previous layer so I

00:25:45,990 --> 00:25:50,550
have the input and the input for the

00:25:48,810 --> 00:25:54,090
acts layer is the output from a previous

00:25:50,550 --> 00:25:58,980
layer makes sense I could have more

00:25:54,090 --> 00:26:01,860
layers it just works like that okay so

00:25:58,980 --> 00:26:03,929
that's my generic formula for that

00:26:01,860 --> 00:26:07,080
for the hidden layer and for the upper

00:26:03,929 --> 00:26:10,669
layer so if I do it for the forward

00:26:07,080 --> 00:26:13,260
propagation step what I'm going to do is

00:26:10,669 --> 00:26:15,919
I'm going through in the loop I'm going

00:26:13,260 --> 00:26:18,630
to calculate my Z using the formula and

00:26:15,919 --> 00:26:20,640
the previous formula and using the

00:26:18,630 --> 00:26:24,779
helpers for that matrix library such as

00:26:20,640 --> 00:26:27,720
multiply and add if you'd have to do

00:26:24,779 --> 00:26:30,390
this with arrays it would be like two

00:26:27,720 --> 00:26:33,720
hundred five hundred lines just to do

00:26:30,390 --> 00:26:35,850
that and you mess it up and then I keep

00:26:33,720 --> 00:26:37,230
activation functions and L is just to

00:26:35,850 --> 00:26:41,370
keep track of my layer it's just an

00:26:37,230 --> 00:26:42,480
Indian's index and last one is I

00:26:41,370 --> 00:26:44,190
returned the vault

00:26:42,480 --> 00:26:46,740
I'll return the output for my network

00:26:44,190 --> 00:26:48,389
and that's actually my prediction so

00:26:46,740 --> 00:26:51,269
what comes out from the network is my

00:26:48,389 --> 00:26:53,309
prediction and so this is what's

00:26:51,269 --> 00:26:56,159
happening in a forward path for low pass

00:26:53,309 --> 00:26:58,769
I get my inputs which in XR is zero zero

00:26:56,159 --> 00:27:02,039
i flow through the network and i got a

00:26:58,769 --> 00:27:05,399
value back and that's my first step in

00:27:02,039 --> 00:27:08,100
this loop right here so if you have a

00:27:05,399 --> 00:27:09,960
look after so as you remember I've

00:27:08,100 --> 00:27:11,820
initialized my network with random

00:27:09,960 --> 00:27:14,669
parameters and I gave it the first

00:27:11,820 --> 00:27:18,360
example which is a zero zero and my

00:27:14,669 --> 00:27:24,090
network told me this is 0.6 T 1 is it is

00:27:18,360 --> 00:27:26,669
it a good answer or you think no well

00:27:24,090 --> 00:27:30,750
let's find out using some math of course

00:27:26,669 --> 00:27:33,899
so I need a complete cost and I need to

00:27:30,750 --> 00:27:35,940
use a cost function I'm going to pick a

00:27:33,899 --> 00:27:38,370
pretty common one min square error and

00:27:35,940 --> 00:27:40,889
which is actually quite simple it's just

00:27:38,370 --> 00:27:44,130
output minus a prediction squared that's

00:27:40,889 --> 00:27:46,980
it so if I go and write a function my

00:27:44,130 --> 00:27:49,139
class mean square error in my cost

00:27:46,980 --> 00:27:50,789
function activation because you can use

00:27:49,139 --> 00:27:52,919
different classes for this if you want

00:27:50,789 --> 00:27:55,019
I'm going to compute and it's so simple

00:27:52,919 --> 00:27:59,070
just target - prediction squared that's

00:27:55,019 --> 00:28:01,710
it that's my second step actually I'm

00:27:59,070 --> 00:28:04,440
just initializing it here and that's me

00:28:01,710 --> 00:28:07,649
calling the cost function in my compute

00:28:04,440 --> 00:28:10,020
cost which I'm going to add so that's my

00:28:07,649 --> 00:28:13,020
second step there and I keep track of it

00:28:10,020 --> 00:28:13,900
it might cost array okay so that's the

00:28:13,020 --> 00:28:19,030
second step

00:28:13,900 --> 00:28:21,880
and so if I have a look so my desires my

00:28:19,030 --> 00:28:25,809
known output is here actually gave me

00:28:21,880 --> 00:28:26,620
point 61 which is crop squared equals

00:28:25,809 --> 00:28:29,200
point 38

00:28:26,620 --> 00:28:33,670
that's my network error that's how good

00:28:29,200 --> 00:28:35,680
or bad the network is behaving so what I

00:28:33,670 --> 00:28:37,240
want is that what I hope is that it

00:28:35,680 --> 00:28:39,280
starts with a really high error

00:28:37,240 --> 00:28:41,260
why because the initial weights are just

00:28:39,280 --> 00:28:43,000
random and over time and it starts

00:28:41,260 --> 00:28:45,820
adjusting the weighting bias it will

00:28:43,000 --> 00:28:48,970
slowly go down goes below my error stops

00:28:45,820 --> 00:28:51,670
training and I'm happy that's hopefully

00:28:48,970 --> 00:28:53,200
what's going to happen and for that to

00:28:51,670 --> 00:28:56,380
happen for the weights and buyers to be

00:28:53,200 --> 00:28:58,179
adjusted I need this magical step which

00:28:56,380 --> 00:29:02,820
is called back propagation it's the most

00:28:58,179 --> 00:29:02,820
scary one any questions so far

00:29:02,850 --> 00:29:09,160
everything's pretty clear then okay so

00:29:06,370 --> 00:29:11,080
back propagation is a supervised

00:29:09,160 --> 00:29:15,910
learning method for this kind of feed

00:29:11,080 --> 00:29:19,000
forward networks and it uses gradient

00:29:15,910 --> 00:29:22,330
descent so what he's going to do is

00:29:19,000 --> 00:29:23,830
going to look at the gradient of the

00:29:22,330 --> 00:29:27,370
error function which is actually the

00:29:23,830 --> 00:29:29,080
loss function with respect to each per

00:29:27,370 --> 00:29:37,750
learnable parameter sort of weights and

00:29:29,080 --> 00:29:41,610
bias so here's a scary slide for you who

00:29:37,750 --> 00:29:41,610
has done remember partial derivatives

00:29:42,270 --> 00:29:48,070
every one then okay so basically this is

00:29:46,510 --> 00:29:50,110
what's what's happening what's going

00:29:48,070 --> 00:29:51,970
what were you in to do and in back

00:29:50,110 --> 00:29:54,970
propagation is just look at each

00:29:51,970 --> 00:29:57,670
individual weight and bias in the

00:29:54,970 --> 00:29:59,710
network okay and we see I'm going to

00:29:57,670 --> 00:30:02,860
look at how much it contributes to the

00:29:59,710 --> 00:30:04,360
overall error each individual weight and

00:30:02,860 --> 00:30:06,400
bias I'm going to see if I adjust it

00:30:04,360 --> 00:30:08,650
slightly what happens my error does it

00:30:06,400 --> 00:30:10,059
go better does he go worse that's what's

00:30:08,650 --> 00:30:13,000
happening that's the partial derivative

00:30:10,059 --> 00:30:14,350
that's what's happening so if I put this

00:30:13,000 --> 00:30:17,740
in equation I know it's going to be a

00:30:14,350 --> 00:30:20,050
bit scary but the pitch view part is on

00:30:17,740 --> 00:30:22,179
the hard actually so for the output

00:30:20,050 --> 00:30:24,100
layer it's in a special case my starting

00:30:22,179 --> 00:30:26,350
point I'm going to use da is the

00:30:24,100 --> 00:30:27,670
derivative of the loss function and then

00:30:26,350 --> 00:30:30,700
I'm going to calculate DZ

00:30:27,670 --> 00:30:32,470
which is a previous step element-wise

00:30:30,700 --> 00:30:38,020
multiplication with a derivative of the

00:30:32,470 --> 00:30:39,970
sigmoid function d w DZ times a DB just

00:30:38,020 --> 00:30:42,490
easy okay

00:30:39,970 --> 00:30:45,810
so that's that's the output layer for

00:30:42,490 --> 00:30:48,280
the hidden layer they are it the

00:30:45,810 --> 00:30:50,920
starting point is slightly different so

00:30:48,280 --> 00:30:52,980
i'm still i'm going to use DZ from the

00:30:50,920 --> 00:30:56,410
previous step this one right here and

00:30:52,980 --> 00:30:58,210
i'm going to use the weight matrix from

00:30:56,410 --> 00:31:00,340
the previous layer that's my starting

00:30:58,210 --> 00:31:03,520
point for da the rest is exactly the

00:31:00,340 --> 00:31:05,560
same so this is just a special case for

00:31:03,520 --> 00:31:07,720
L per layer the rest is exactly the same

00:31:05,560 --> 00:31:09,420
you could have ten layers it'll be the

00:31:07,720 --> 00:31:13,450
same just repeat put that in the loop

00:31:09,420 --> 00:31:14,800
easily done okay let's see that in code

00:31:13,450 --> 00:31:17,380
so I'm just going to add two more

00:31:14,800 --> 00:31:20,470
parameters DW and DB because I'm going

00:31:17,380 --> 00:31:24,340
to need that and let's look at the

00:31:20,470 --> 00:31:26,620
derivative for the loss function which

00:31:24,340 --> 00:31:29,650
is basically two times friction

00:31:26,620 --> 00:31:32,140
- output - production pretty easy to

00:31:29,650 --> 00:31:34,930
implement so I'm going to add a second

00:31:32,140 --> 00:31:37,120
method or my mean square error class so

00:31:34,930 --> 00:31:38,830
it was a role already computing doing

00:31:37,120 --> 00:31:40,270
compute for the forward pass and now it

00:31:38,830 --> 00:31:42,700
will do different shade for back

00:31:40,270 --> 00:31:46,540
propagation it's just this times two and

00:31:42,700 --> 00:31:48,490
that's it and we get the matrix back for

00:31:46,540 --> 00:31:50,800
the sigmoid is actually quite easy as

00:31:48,490 --> 00:31:54,390
well it's just a sigmoid itself times

00:31:50,800 --> 00:31:57,250
one minus a sigma not the heart

00:31:54,390 --> 00:32:00,400
differentiate my third I'm just going to

00:31:57,250 --> 00:32:03,750
call the sigmoid again times 1 minus the

00:32:00,400 --> 00:32:06,540
sigmoid and that's it so compute method

00:32:03,750 --> 00:32:10,930
differentiate method in my sigmoid

00:32:06,540 --> 00:32:14,770
activation function so once I have those

00:32:10,930 --> 00:32:18,790
I'm just going to start with upper layer

00:32:14,770 --> 00:32:21,640
so as you can see calculate da just

00:32:18,790 --> 00:32:25,870
going differentiate on my loss function

00:32:21,640 --> 00:32:27,370
and then I'm going to do DZ Adam at

00:32:25,870 --> 00:32:31,090
profit because that's element wise

00:32:27,370 --> 00:32:32,620
multiplication dwn DB and then if I go

00:32:31,090 --> 00:32:34,240
for the hidden layer exactly the same

00:32:32,620 --> 00:32:38,920
thing it's just the a is slightly

00:32:34,240 --> 00:32:40,150
different but then I calculate TC and DW

00:32:38,920 --> 00:32:42,130
and DB

00:32:40,150 --> 00:32:46,660
it can have a look later on on these

00:32:42,130 --> 00:32:49,660
details so here we have my do back

00:32:46,660 --> 00:32:52,090
propagation step and the last step is a

00:32:49,660 --> 00:32:53,550
date parameters so for update parameters

00:32:52,090 --> 00:32:56,080
I'm going to use gradient descent and

00:32:53,550 --> 00:32:59,380
for that this is how I'm going to

00:32:56,080 --> 00:33:03,330
applied my W so the new W the new weight

00:32:59,380 --> 00:33:06,700
is going to be the current weight minus

00:33:03,330 --> 00:33:10,650
TW which I'm I've stored in a PHP

00:33:06,700 --> 00:33:14,050
variable times alpha and alpha is the

00:33:10,650 --> 00:33:15,910
learn herbal parameter and for B is

00:33:14,050 --> 00:33:16,650
exactly the same but I'm using DB

00:33:15,910 --> 00:33:19,240
instead

00:33:16,650 --> 00:33:22,300
so the learn herbal parameter alpha

00:33:19,240 --> 00:33:25,710
allows me to control the jump between

00:33:22,300 --> 00:33:28,960
each iteration so if if office two big

00:33:25,710 --> 00:33:31,120
jumps are too too big as well and I

00:33:28,960 --> 00:33:33,340
might miss this is the error

00:33:31,120 --> 00:33:35,200
I might miss the optimum value for the

00:33:33,340 --> 00:33:37,390
error if the learning parameter is too

00:33:35,200 --> 00:33:40,840
big but if the learner will prompt is

00:33:37,390 --> 00:33:43,120
too small it will take a long time to

00:33:40,840 --> 00:33:45,580
converge it will take a long time and it

00:33:43,120 --> 00:33:47,320
may even miss the optimum value as well

00:33:45,580 --> 00:33:50,590
so there is a sweet spot for the long

00:33:47,320 --> 00:33:53,800
above parameter so that's why I'm going

00:33:50,590 --> 00:33:56,800
to do here I'm going to say my learning

00:33:53,800 --> 00:33:59,140
rate is going to be 0.1 we can play with

00:33:56,800 --> 00:34:00,520
this it's just a fixed value that I'm

00:33:59,140 --> 00:34:02,760
going to put there so to update

00:34:00,520 --> 00:34:04,840
parameters I'm just going to every layer

00:34:02,760 --> 00:34:07,570
calculate the new W based on the

00:34:04,840 --> 00:34:11,620
previous W times my learning rate and

00:34:07,570 --> 00:34:14,920
same for P and that's my last step and

00:34:11,620 --> 00:34:16,660
finally I just keep track of the error

00:34:14,920 --> 00:34:19,450
because I need to know when to stop

00:34:16,660 --> 00:34:22,750
above okay

00:34:19,450 --> 00:34:26,260
so this is the full code for the

00:34:22,750 --> 00:34:28,900
learning for learning in a neural

00:34:26,260 --> 00:34:35,290
network written in PHP that's a full

00:34:28,900 --> 00:34:39,340
code for that and for this training

00:34:35,290 --> 00:34:44,050
complaints the worst part is over by the

00:34:39,340 --> 00:34:45,679
way no one left yet so I guess you can

00:34:44,050 --> 00:34:48,559
say to the end

00:34:45,679 --> 00:34:50,929
okay so just recap so algorithm for

00:34:48,559 --> 00:34:53,000
training I started with initializing

00:34:50,929 --> 00:34:55,220
weights and buyers randomly and also to

00:34:53,000 --> 00:34:59,569
zero I've set some conditions for when

00:34:55,220 --> 00:35:02,540
to stop because I don't want this

00:34:59,569 --> 00:35:04,369
running forever if it's it has no point

00:35:02,540 --> 00:35:08,089
and then for each training example I

00:35:04,369 --> 00:35:10,130
just don't do those four steps I do for

00:35:08,089 --> 00:35:12,349
a pass from the beginning goes through

00:35:10,130 --> 00:35:15,079
all the end to the network I check the

00:35:12,349 --> 00:35:17,180
error on compute costs I applied some

00:35:15,079 --> 00:35:20,480
magic using back propagation and

00:35:17,180 --> 00:35:22,160
derivatives back to the beginning I

00:35:20,480 --> 00:35:24,290
adjust my weights and buyers keep

00:35:22,160 --> 00:35:27,559
looping until weighted buyers get to an

00:35:24,290 --> 00:35:31,339
optimum value so this is an example

00:35:27,559 --> 00:35:33,260
taken from the tensorflow website which

00:35:31,339 --> 00:35:36,109
is a Python framework for machine

00:35:33,260 --> 00:35:38,000
learning and hopefully this is what

00:35:36,109 --> 00:35:40,369
we're going to have what's what's going

00:35:38,000 --> 00:35:42,500
to happen during the learning phase so

00:35:40,369 --> 00:35:47,690
as you can see some connections will

00:35:42,500 --> 00:35:50,720
grow stronger so a bit thinner and some

00:35:47,690 --> 00:35:54,109
neurons will specialize on identifying

00:35:50,720 --> 00:35:57,230
the blue ones some other will specialize

00:35:54,109 --> 00:36:00,410
on identifying orange ones so over time

00:35:57,230 --> 00:36:03,799
as you can see it will slowly be able to

00:36:00,410 --> 00:36:07,069
find some boundaries between the blue

00:36:03,799 --> 00:36:08,240
and the orange circles so hopefully

00:36:07,069 --> 00:36:10,700
that's what's going to happen and you

00:36:08,240 --> 00:36:12,589
can see the error going dropping right

00:36:10,700 --> 00:36:17,869
there so hopefully that's what's going

00:36:12,589 --> 00:36:21,950
to happen during the learning phase so

00:36:17,869 --> 00:36:25,549
let's put this to the training so i'm

00:36:21,950 --> 00:36:28,460
going to call my script and epoch is

00:36:25,549 --> 00:36:32,119
just a number iteration so after 1000 X

00:36:28,460 --> 00:36:36,170
I get an error of not point 22 almost no

00:36:32,119 --> 00:36:39,440
point 23 but 400 iterations later

00:36:36,170 --> 00:36:42,190
it has dropped 100 times actually forget

00:36:39,440 --> 00:36:47,420
it's going the right direction and

00:36:42,190 --> 00:36:50,299
around 10000 epochs it has fallen or

00:36:47,420 --> 00:36:52,520
it's going to fall below my maximum

00:36:50,299 --> 00:36:57,000
error which was if you remember naught

00:36:52,520 --> 00:37:00,970
point naught naught 1 so it stops

00:36:57,000 --> 00:37:03,340
and now if we put this to a test and I'm

00:37:00,970 --> 00:37:05,740
going to cheat here because I only have

00:37:03,340 --> 00:37:08,590
for example so I'm going to use the same

00:37:05,740 --> 00:37:13,990
examples in real world you would feed it

00:37:08,590 --> 00:37:17,170
on something new on scene say if I to

00:37:13,990 --> 00:37:19,510
implement predict it's simply called do

00:37:17,170 --> 00:37:21,760
forward propagation remember now the

00:37:19,510 --> 00:37:24,580
network has to add stop learning so the

00:37:21,760 --> 00:37:28,750
weight and buyers they are frozen so

00:37:24,580 --> 00:37:30,370
keep it like gold those parameters are

00:37:28,750 --> 00:37:31,960
frozen so I just called do forward

00:37:30,370 --> 00:37:33,160
propagation and the value that I get at

00:37:31,960 --> 00:37:36,400
the end of the network that's my

00:37:33,160 --> 00:37:39,550
prediction so if I call it with zero

00:37:36,400 --> 00:37:41,500
zero I get a prediction pack of point

00:37:39,550 --> 00:37:46,630
zero three which is pretty close to zero

00:37:41,500 --> 00:37:48,940
same here and if I call read my inputs

00:37:46,630 --> 00:37:52,380
which are 0 1 and 1 0 I get something

00:37:48,940 --> 00:38:01,060
that's really close to 1 so I'll say

00:37:52,380 --> 00:38:04,840
this is a success ok you don't have to

00:38:01,060 --> 00:38:07,540
use the XR I'll challenge you to pick a

00:38:04,840 --> 00:38:10,780
more creative example you have more time

00:38:07,540 --> 00:38:12,400
than just 45 minutes so go and play with

00:38:10,780 --> 00:38:15,730
this we need something more creative or

00:38:12,400 --> 00:38:19,210
more useful play with different

00:38:15,730 --> 00:38:20,770
activation and cost functions as you can

00:38:19,210 --> 00:38:23,200
see you have just implemented sigmoid

00:38:20,770 --> 00:38:26,500
but there are many other activation

00:38:23,200 --> 00:38:28,600
functions the relative function tan age

00:38:26,500 --> 00:38:31,150
there's a lot more that you can play

00:38:28,600 --> 00:38:33,310
with it get better results you can play

00:38:31,150 --> 00:38:35,920
with different different cost functions

00:38:33,310 --> 00:38:37,840
as well and the learning rate you can

00:38:35,920 --> 00:38:40,690
play with that let's make it train

00:38:37,840 --> 00:38:43,480
faster let's see if it still works or I

00:38:40,690 --> 00:38:45,910
know that sweet point 1 it works but or

00:38:43,480 --> 00:38:48,700
if we use just one maybe just converts

00:38:45,910 --> 00:38:50,710
off for 1,000 trations and if you do

00:38:48,700 --> 00:38:54,760
that you can open a PR on this Reaper

00:38:50,710 --> 00:38:56,770
that I've shared before and why not try

00:38:54,760 --> 00:39:00,220
with different topology I've used one

00:38:56,770 --> 00:39:01,900
hidden layer why not use two five hidden

00:39:00,220 --> 00:39:04,120
layers how does he going to work maybe

00:39:01,900 --> 00:39:07,090
odd on that one hidden layer mat you can

00:39:04,120 --> 00:39:08,900
put five neurons five hidden neurons how

00:39:07,090 --> 00:39:11,960
does he go into work

00:39:08,900 --> 00:39:14,120
and you can even compete a kegger and

00:39:11,960 --> 00:39:15,260
you can win some prizes it's a data

00:39:14,120 --> 00:39:18,280
science machine lying competition

00:39:15,260 --> 00:39:21,050
website check it out

00:39:18,280 --> 00:39:25,370
so quick recap on what we have seen

00:39:21,050 --> 00:39:28,460
today near networks are inspired by the

00:39:25,370 --> 00:39:31,280
human brain but actually today is more

00:39:28,460 --> 00:39:32,510
of a mathematical challenge and

00:39:31,280 --> 00:39:37,430
engineering challenge

00:39:32,510 --> 00:39:39,530
it's divergent a bit from biology and we

00:39:37,430 --> 00:39:41,990
saw a feed-forward network but there are

00:39:39,530 --> 00:39:44,900
others so if you if you'd go and let's

00:39:41,990 --> 00:39:46,970
say use something for video processing

00:39:44,900 --> 00:39:49,010
that's just an autonomous driving then

00:39:46,970 --> 00:39:51,320
you'd use convolutional networks which

00:39:49,010 --> 00:39:54,950
are a lot more complex and wouldn't fit

00:39:51,320 --> 00:39:57,410
in the screen and or for speech you

00:39:54,950 --> 00:40:01,400
could use recurrent neural networks

00:39:57,410 --> 00:40:07,430
because they look at the previous setups

00:40:01,400 --> 00:40:09,500
and words of what you said we today we

00:40:07,430 --> 00:40:12,860
use supervised machine learning which is

00:40:09,500 --> 00:40:15,350
just learning from examples but there is

00:40:12,860 --> 00:40:19,280
also unsupervised machine learning in

00:40:15,350 --> 00:40:21,500
which you don't label data you just ask

00:40:19,280 --> 00:40:24,260
it to find patterns in data to find

00:40:21,500 --> 00:40:27,140
similarities so that's that's that's

00:40:24,260 --> 00:40:30,170
really useful as well because labeling

00:40:27,140 --> 00:40:32,150
data is a lot of work there are actually

00:40:30,170 --> 00:40:35,630
some companies making money just by

00:40:32,150 --> 00:40:38,330
building the data sets because machine

00:40:35,630 --> 00:40:40,910
learning needs large data sets of

00:40:38,330 --> 00:40:42,890
labeled data so some companies even make

00:40:40,910 --> 00:40:44,300
money just taking pictures and putting a

00:40:42,890 --> 00:40:46,210
label in it and then selling it to

00:40:44,300 --> 00:40:50,960
companies that actually use machine like

00:40:46,210 --> 00:40:54,140
if you want to do that and in this

00:40:50,960 --> 00:40:57,320
example we have used back propagation

00:40:54,140 --> 00:41:00,980
and grading descent which I think is the

00:40:57,320 --> 00:41:04,850
magical bit in a neural network and can

00:41:00,980 --> 00:41:06,650
be done in using PHP and in theory as

00:41:04,850 --> 00:41:09,770
you can see you can approximate any

00:41:06,650 --> 00:41:11,630
function if we are using a non-linear

00:41:09,770 --> 00:41:15,280
activation function just as a sigmoid

00:41:11,630 --> 00:41:18,680
and that's what makes them so powerful

00:41:15,280 --> 00:41:20,740
okay so today you have seen a network

00:41:18,680 --> 00:41:27,410
like this

00:41:20,740 --> 00:41:30,320
but if you added another hidden layer or

00:41:27,410 --> 00:41:36,530
maybe even more hidden layers and if you

00:41:30,320 --> 00:41:40,040
kept adding more hidden layers then you

00:41:36,530 --> 00:41:42,050
enter the realms of deep learning so if

00:41:40,040 --> 00:41:47,000
you see deep learning in an article

00:41:42,050 --> 00:41:50,840
under knees is basically new networks

00:41:47,000 --> 00:41:52,430
with many many many layers then you then

00:41:50,840 --> 00:41:55,790
we can call it deep learning

00:41:52,430 --> 00:41:58,490
that's why deploying is all about so

00:41:55,790 --> 00:42:01,730
before I finish just a few resources I

00:41:58,490 --> 00:42:06,530
would like to share so if you're still

00:42:01,730 --> 00:42:08,450
interested in this topic the Wikipedia

00:42:06,530 --> 00:42:10,760
article is quite good on artificial in

00:42:08,450 --> 00:42:13,940
your networks there are some really good

00:42:10,760 --> 00:42:19,040
courses I think this one was a the

00:42:13,940 --> 00:42:22,190
original course on Coursera this one is

00:42:19,040 --> 00:42:26,200
really good and it covers modern uses

00:42:22,190 --> 00:42:29,840
modern tools and but I think it's paid

00:42:26,200 --> 00:42:31,490
they covers things such as convolutional

00:42:29,840 --> 00:42:35,210
networks with autonomous driving and

00:42:31,490 --> 00:42:40,760
speech recognition and it uses Python

00:42:35,210 --> 00:42:46,280
mostly I think tensorflow actually the

00:42:40,760 --> 00:42:51,200
first one is more of theory it's really

00:42:46,280 --> 00:42:53,720
cool this is one of the heroes of AI

00:42:51,200 --> 00:42:57,320
Geoffrey Hinton so please check out

00:42:53,720 --> 00:42:59,120
these slides I would say start with this

00:42:57,320 --> 00:43:03,860
ones because it's a little hard to

00:42:59,120 --> 00:43:05,360
digest and there are so many resources

00:43:03,860 --> 00:43:07,670
nowadays of our machine learning and

00:43:05,360 --> 00:43:09,290
they actually free google has a really

00:43:07,670 --> 00:43:12,620
good one as a crash course on machine

00:43:09,290 --> 00:43:15,980
learning and it's very hands-on as well

00:43:12,620 --> 00:43:17,990
and last one which I would like to

00:43:15,980 --> 00:43:19,850
highlight that one which is really good

00:43:17,990 --> 00:43:21,370
if we're into the backpropagation step

00:43:19,850 --> 00:43:26,480
one to understand a little bit better

00:43:21,370 --> 00:43:33,560
more partial derivatives maybe and so

00:43:26,480 --> 00:43:36,020
this book as well it's not HP its users

00:43:33,560 --> 00:43:37,310
- but it's hands-on so you see some

00:43:36,020 --> 00:43:42,140
examples you can play with that it's

00:43:37,310 --> 00:43:45,710
really good and if you're in a rush this

00:43:42,140 --> 00:43:47,750
is a less than five minutes video that

00:43:45,710 --> 00:43:50,590
explains just a neuro-networks

00:43:47,750 --> 00:43:53,060
it's really good it's also from the deep

00:43:50,590 --> 00:43:54,830
specialization course I put on the

00:43:53,060 --> 00:43:58,970
previous slide and it's available on

00:43:54,830 --> 00:44:02,470
YouTube and with that I reach the end of

00:43:58,970 --> 00:44:07,490
my talk thanks so much for your patience

00:44:02,470 --> 00:44:09,540
and I'm ready for some questions if you

00:44:07,490 --> 00:44:14,389
have it thank you

00:44:09,540 --> 00:44:14,389
[Applause]

00:44:16,300 --> 00:44:31,469
[Music]

00:44:28,070 --> 00:44:31,469

YouTube URL: https://www.youtube.com/watch?v=iC13u7tspyg


