Title: Azure Friday | Apache Spark Connector for Azure Cosmos DB
Publication date: 2018-01-10
Playlist: Microsoft Open Source Summit
Description: 
	Emily Lawton stops by to chat with Scott Hanselman about the Azure Cosmos DB Spark Connector, which enables Azure Cosmos DB to act as an input source or output sink for Apache Spark jobs.

For more information, see:

Azure Cosmos DB Spark Wiki - https://github.com/azure/azure-cosmosdb-spark/wiki
Captions: 
	00:00:00,000 --> 00:00:01,900
>> Hey, friends. I'm Scott Hansen,

00:00:01,900 --> 00:00:02,740
and it is Azure Friday.

00:00:02,740 --> 00:00:03,845
I'm here with Emily Lawton,

00:00:03,845 --> 00:00:05,920
and we're talking about Cosmos DB again,

00:00:05,920 --> 00:00:08,592
except this time, the Spark Connector.

00:00:08,592 --> 00:00:11,085
Two things I haven't plugged together in my brain yet,

00:00:11,085 --> 00:00:14,040
you got Cosmos DB and Apache Spark.

00:00:14,040 --> 00:00:15,460
>> So, as we all know,

00:00:15,460 --> 00:00:18,510
real time data is incredibly prevalent around us today,

00:00:18,510 --> 00:00:23,170
and people love to use Cosmos DB as the way to query and

00:00:23,170 --> 00:00:25,510
persist that real time data for

00:00:25,510 --> 00:00:28,360
Web, Mobile, IoT, Applications.

00:00:28,360 --> 00:00:32,575
And Cosmos also has support for a number

00:00:32,575 --> 00:00:34,810
of aggregation functions such as sum,

00:00:34,810 --> 00:00:37,515
min, average, count.

00:00:37,515 --> 00:00:39,790
But if you're a data scientist,

00:00:39,790 --> 00:00:41,455
some of these functions aren't

00:00:41,455 --> 00:00:43,210
sufficient for the types of

00:00:43,210 --> 00:00:45,460
complex querying and analytics

00:00:45,460 --> 00:00:47,005
that you want to do over your data.

00:00:47,005 --> 00:00:50,200
So, I'm excited to be here today to talk about

00:00:50,200 --> 00:00:52,420
this new connector we have to bring

00:00:52,420 --> 00:00:55,405
together Spark and Cosmos DB.

00:00:55,405 --> 00:00:59,275
And essentially, what this connector allows is Cosmos DB

00:00:59,275 --> 00:01:00,940
can act as an input source or

00:01:00,940 --> 00:01:03,370
an output sync for Apache Spark jobs.

00:01:03,370 --> 00:01:05,930
>> Okay. So, if I've got a huge amount of data in

00:01:05,930 --> 00:01:07,210
Cosmos or I want to

00:01:07,210 --> 00:01:08,935
use Cosmos to a store huge amount of data,

00:01:08,935 --> 00:01:10,940
I'm already doing Spark, it just plugs right in?

00:01:10,940 --> 00:01:11,250
>> Yes.

00:01:11,250 --> 00:01:12,130
>> It just works?

00:01:12,130 --> 00:01:12,790
>> Yes.

00:01:12,790 --> 00:01:15,550
>> That's pretty cool. Is it an open source connector?

00:01:15,550 --> 00:01:18,980
>> This connector is open source.

00:01:18,980 --> 00:01:21,695
You can go to our Github and see, well,

00:01:21,695 --> 00:01:23,230
the connector itself is not in source

00:01:23,230 --> 00:01:26,065
but if you go to our Github,

00:01:26,065 --> 00:01:28,180
you can see all the samples and how

00:01:28,180 --> 00:01:30,895
to get started and ramped up on it.

00:01:30,895 --> 00:01:32,380
>> And do you have some actual live

00:01:32,380 --> 00:01:33,713
demos where you plug this right now?

00:01:33,713 --> 00:01:36,490
>> We do and I'll go through one shortly.

00:01:36,490 --> 00:01:39,490
But I also want to acknowledge the kind of

00:01:39,490 --> 00:01:41,710
the two advantages of

00:01:41,710 --> 00:01:43,930
bringing these two services together now.

00:01:43,930 --> 00:01:45,355
With Spark, you can do

00:01:45,355 --> 00:01:50,815
very complex distributed aggregations over your data,

00:01:50,815 --> 00:01:53,095
and Cosmos, what that brings the table

00:01:53,095 --> 00:01:55,810
is your access to the real time data itself.

00:01:55,810 --> 00:01:57,905
And everything that comes along with

00:01:57,905 --> 00:02:02,260
a fully managed database service like we have,

00:02:02,260 --> 00:02:05,125
automatic indexing, turnkey global distribution,

00:02:05,125 --> 00:02:08,400
all those great features you get as well.

00:02:08,400 --> 00:02:09,700
>> Do you think that Cosmos is like

00:02:09,700 --> 00:02:11,030
the best place to put Spark data.

00:02:11,030 --> 00:02:12,070
Like there's lots of places you could

00:02:12,070 --> 00:02:13,300
put data that comes from Spark, right?

00:02:13,300 --> 00:02:15,940
>> Yeah. Well, the fact that Cosmos DB is just fully

00:02:15,940 --> 00:02:18,585
managed and with such great SLAs,

00:02:18,585 --> 00:02:21,280
it is a really great place to keep your data.

00:02:21,280 --> 00:02:23,541
>> And you use that term aggregations.

00:02:23,541 --> 00:02:24,975
I'm not super familiar with that,

00:02:24,975 --> 00:02:28,630
like the sum function is a kind of aggregate.

00:02:28,630 --> 00:02:30,250
>> So, what Spark actually

00:02:30,250 --> 00:02:32,500
does is it distributes out this kind of

00:02:32,500 --> 00:02:34,045
functions to a bunch of worker nodes

00:02:34,045 --> 00:02:35,260
and then do all the work

00:02:35,260 --> 00:02:36,610
on smaller amounts of

00:02:36,610 --> 00:02:39,055
the data and then it's aggregated back together.

00:02:39,055 --> 00:02:41,950
That's where the power of Spark really lies.

00:02:41,950 --> 00:02:43,295
>> Okay. So, it takes

00:02:43,295 --> 00:02:45,325
account or sum or min or match, whatever,

00:02:45,325 --> 00:02:47,912
and it says, do it across all of these.

00:02:47,912 --> 00:02:49,090
Spark is smart.

00:02:49,090 --> 00:02:52,000
Does all the math, the mathy stuff.

00:02:52,000 --> 00:02:53,320
And then, of course,

00:02:53,320 --> 00:02:55,270
combine with another smart global database

00:02:55,270 --> 00:02:57,235
which is the Cosmos DB,

00:02:57,235 --> 00:02:58,290
you get the best of all worlds.

00:02:58,290 --> 00:02:59,000
>> Exactly.

00:02:59,000 --> 00:03:00,170
>> That's cool.

00:03:00,170 --> 00:03:01,225
>> So, let's jump over to

00:03:01,225 --> 00:03:04,095
this Python Notebook here that we have.

00:03:04,095 --> 00:03:06,850
Python notebooks are what a lot of people,

00:03:06,850 --> 00:03:08,320
data scientists who use Spark,

00:03:08,320 --> 00:03:09,837
are most familiar with.

00:03:09,837 --> 00:03:12,565
So, before we actually jump into a demo,

00:03:12,565 --> 00:03:14,290
let's look at this diagram of

00:03:14,290 --> 00:03:17,504
the architecture of how this connector is actually built.

00:03:17,504 --> 00:03:19,380
So, when a user makes

00:03:19,380 --> 00:03:23,125
a connection between Spark and Cosmos,

00:03:23,125 --> 00:03:25,120
what's actually happening under the hood is that

00:03:25,120 --> 00:03:27,230
a Spark master node is connected to

00:03:27,230 --> 00:03:30,225
Cosmos DB gateway node

00:03:30,225 --> 00:03:33,420
to get the partition information from Cosmos.

00:03:33,420 --> 00:03:37,540
That information is translated back to Spark and is then

00:03:37,540 --> 00:03:39,760
distributed among the worker nodes

00:03:39,760 --> 00:03:42,490
which can then interact when a query comes in,

00:03:42,490 --> 00:03:44,620
they know which partition in Cosmos to

00:03:44,620 --> 00:03:46,915
access and can bring the data back to Spark,

00:03:46,915 --> 00:03:51,170
the worker nodes, to kind of do their work on it.

00:03:51,170 --> 00:03:54,397
>> And is this largely transparent?

00:03:54,397 --> 00:03:55,870
>> The user is not aware

00:03:55,870 --> 00:03:57,550
of what's actually happening here.

00:03:57,550 --> 00:03:59,170
>> Cool.

00:03:59,170 --> 00:04:00,390
>> So, these first cells,

00:04:00,390 --> 00:04:01,980
I already pre-baked them.

00:04:01,980 --> 00:04:03,570
I'm going to skip over them because

00:04:03,570 --> 00:04:06,055
they are just specifying the jars,

00:04:06,055 --> 00:04:08,430
the Spark jars, and defining

00:04:08,430 --> 00:04:11,985
how the connector should actually talk to Cosmos.

00:04:11,985 --> 00:04:16,305
And this next cell creates the flights,

00:04:16,305 --> 00:04:18,105
so I should give some context on this demo first.

00:04:18,105 --> 00:04:21,870
This is three months of on time flight

00:04:21,870 --> 00:04:24,120
performance data from the United States

00:04:24,120 --> 00:04:26,050
Department of Transportation Statistics.

00:04:26,050 --> 00:04:27,900
>> So, hundreds of thousands, millions of flights?

00:04:27,900 --> 00:04:29,695
>> There's a lot of flights here.

00:04:29,695 --> 00:04:32,740
I don't think millions but hundreds of thousands.

00:04:32,740 --> 00:04:35,070
And so, we create

00:04:35,070 --> 00:04:37,680
a Spark data frame here which is essentially

00:04:37,680 --> 00:04:40,425
just a distributed collection

00:04:40,425 --> 00:04:45,120
of the flight data that's organized into named columns.

00:04:45,120 --> 00:04:47,515
It's almost like a relational table, in fact.

00:04:47,515 --> 00:04:49,710
Okay , so, the first query

00:04:49,710 --> 00:04:51,030
we're actually going to do here is

00:04:51,030 --> 00:04:53,295
calculate the top 10 delayed destinations

00:04:53,295 --> 00:04:55,188
originating from Seattle.

00:04:55,188 --> 00:04:57,436
So, if we run this here.

00:04:57,436 --> 00:05:02,140
>> This would be the places not to go.

00:05:02,140 --> 00:05:03,600
>> Yes. And if we look at the bar graph,

00:05:03,600 --> 00:05:06,600
you can see that the flights to San Francisco from

00:05:06,600 --> 00:05:10,905
Seattle are most often delayed or are most delayed.

00:05:10,905 --> 00:05:13,260
And then something that another query we can

00:05:13,260 --> 00:05:15,930
do that has a function that is

00:05:15,930 --> 00:05:18,750
definitely not built-in in Cosmos is we can calculate

00:05:18,750 --> 00:05:19,800
the median delays by

00:05:19,800 --> 00:05:22,755
destination cities departing from Seattle.

00:05:22,755 --> 00:05:24,690
And this uses Spark's percentile

00:05:24,690 --> 00:05:27,420
approximation function which, again,

00:05:27,420 --> 00:05:29,010
if you're interested in math or algorithms,

00:05:29,010 --> 00:05:30,360
I think it's doing some sort

00:05:30,360 --> 00:05:32,566
of algorithm similar to a HyperLogLog.

00:05:32,566 --> 00:05:34,020
So, it's in an estimation

00:05:34,020 --> 00:05:35,420
function but it's still very accurate.

00:05:35,420 --> 00:05:37,100
>> So, it's quite a sophisticated algorithm

00:05:37,100 --> 00:05:37,630
across a huge amount of data.

00:05:37,630 --> 00:05:39,990
>> Exactly. And here,

00:05:39,990 --> 00:05:42,825
we see that the city,

00:05:42,825 --> 00:05:46,530
the flight from Seattle to Cleveland actually has

00:05:46,530 --> 00:05:50,834
the greatest median delay of 12 minutes.

00:05:50,834 --> 00:05:54,805
>> That's not too bad.

00:05:54,805 --> 00:05:56,350
>> It is not terrible.

00:05:56,350 --> 00:05:58,165
Other interesting things that you can do with

00:05:58,165 --> 00:06:01,215
the connector or actually do graph queries as well.

00:06:01,215 --> 00:06:04,495
And so, what these cells actually do is create

00:06:04,495 --> 00:06:06,235
a graph frame which is composed

00:06:06,235 --> 00:06:08,466
of Vertex and Edge data frames.

00:06:08,466 --> 00:06:13,300
A Vertex data frame contains a special column named ID,

00:06:13,300 --> 00:06:16,270
which contains the ID of the Vertex.

00:06:16,270 --> 00:06:19,265
And the Edge data frames contain

00:06:19,265 --> 00:06:22,210
a source and input column with

00:06:22,210 --> 00:06:29,970
the source Vertex and destination Vertex ID.

00:06:29,970 --> 00:06:32,290
And so, we can actually figure

00:06:32,290 --> 00:06:34,465
out the most important airport

00:06:34,465 --> 00:06:36,400
in terms of like the number of

00:06:36,400 --> 00:06:38,845
connections going through that airport.

00:06:38,845 --> 00:06:40,780
>> Oh, I see. So, we walk the whole graph.

00:06:40,780 --> 00:06:41,970
This is, again, a quite

00:06:41,970 --> 00:06:44,170
sophisticated aggregate that has a large amount of data.

00:06:44,170 --> 00:06:46,740
>> So, this would take like a lot of grouping

00:06:46,740 --> 00:06:49,060
and it would take some time to

00:06:49,060 --> 00:06:52,710
compose a SQL query that could do this.

00:06:52,710 --> 00:06:54,199
But here, we see with Spark,

00:06:54,199 --> 00:06:55,360
we can just use

00:06:55,360 --> 00:07:00,150
the graph degree method and quickly find that,

00:07:00,150 --> 00:07:03,840
as expected, you know a lot about flights.

00:07:03,840 --> 00:07:05,520
Atlanta has the greatest number of

00:07:05,520 --> 00:07:07,140
connections going through it followed by

00:07:07,140 --> 00:07:07,870
Dallas and Chicago.

00:07:07,870 --> 00:07:08,190
>> And it's so fast.

00:07:08,190 --> 00:07:11,195
>> It's very fast.

00:07:11,195 --> 00:07:13,080
That's largely the power of

00:07:13,080 --> 00:07:18,660
Spark combined with Cosmos. But they work great together.

00:07:18,660 --> 00:07:20,880
Another interesting query that we can do is see if there

00:07:20,880 --> 00:07:23,229
are any direct flights between two cities.

00:07:23,229 --> 00:07:26,351
So, I pitched San Jose and Buffalo here.

00:07:26,351 --> 00:07:30,825
So, this is using Sparks Breadth-first search function,

00:07:30,825 --> 00:07:33,780
and if we set the max path length to one,

00:07:33,780 --> 00:07:37,250
that means there is no stop in between those two points.

00:07:37,250 --> 00:07:40,095
And this query returns no results,

00:07:40,095 --> 00:07:42,315
meaning that there are no paths

00:07:42,315 --> 00:07:44,430
of length one between these two cities.

00:07:44,430 --> 00:07:46,080
>> So, let's try to actually increase

00:07:46,080 --> 00:07:49,197
the max length to two.

00:07:49,197 --> 00:07:50,950
>> So, this is seeing if there

00:07:50,950 --> 00:07:52,850
are flights between Seattle and Buffalo.

00:07:52,850 --> 00:07:53,856
>> With one stop.

00:07:53,856 --> 00:07:55,370
>> One stop to a location. Okay.

00:07:55,370 --> 00:07:59,320
>> And this one has taken a little longer.

00:07:59,320 --> 00:08:00,140
>> There's a ton.

00:08:00,140 --> 00:08:01,630
>> There are a lot. And so,

00:08:01,630 --> 00:08:04,600
we can see that of the top 20 that are shown here.

00:08:04,600 --> 00:08:07,170
They all stop in Boston in between.

00:08:07,170 --> 00:08:08,867
So, there are two legs of a flight,

00:08:08,867 --> 00:08:11,610
that's why the path length is two.

00:08:11,610 --> 00:08:14,130
And you can see that this took

00:08:14,130 --> 00:08:16,830
only very simple query to do.

00:08:16,830 --> 00:08:18,285
>> This might be a dumb question

00:08:18,285 --> 00:08:19,320
but if I were going to try

00:08:19,320 --> 00:08:22,575
to do these kinds of queries with Cosmos directly,

00:08:22,575 --> 00:08:24,240
would I have trouble because I don't have

00:08:24,240 --> 00:08:26,340
the aggregates available that I want to have?

00:08:26,340 --> 00:08:30,115
>> We don't have this BFS function.

00:08:30,115 --> 00:08:33,690
In SQL syntax, this would be very difficult to do.

00:08:33,690 --> 00:08:35,438
But if you're not already aware,

00:08:35,438 --> 00:08:37,920
we actually do have an API for Gremlin which

00:08:37,920 --> 00:08:41,070
is also an Apache graph,

00:08:41,070 --> 00:08:44,285
sort of graph way of query and documents.

00:08:44,285 --> 00:08:47,295
You could also do something similar using that.

00:08:47,295 --> 00:08:50,730
But I think using Spark will be much faster.

00:08:50,730 --> 00:08:52,140
>> So, when I make a decision about

00:08:52,140 --> 00:08:53,595
to include Apache Spark

00:08:53,595 --> 00:08:56,625
in my Cosmos ecosystem of data,

00:08:56,625 --> 00:08:58,710
do I need to be thinking about the kinds

00:08:58,710 --> 00:09:00,860
of data that I store and the shape of the data?

00:09:00,860 --> 00:09:01,399
>> Yeah.

00:09:01,399 --> 00:09:02,610
>> Like if I just had a table the books,

00:09:02,610 --> 00:09:04,140
I probably don't need this.

00:09:04,140 --> 00:09:06,660
>> I think that's very important.

00:09:06,660 --> 00:09:10,080
And the cool goal of a roadmap for Cosmos is

00:09:10,080 --> 00:09:13,890
being able to have multiple API's so users can,

00:09:13,890 --> 00:09:15,435
depending on their dataset, query

00:09:15,435 --> 00:09:17,550
with different API's and

00:09:17,550 --> 00:09:19,770
use the API that will give them

00:09:19,770 --> 00:09:22,160
the best performance. Makes most sense.

00:09:22,160 --> 00:09:23,010
>> And it's one of the cool things about

00:09:23,010 --> 00:09:24,370
Cosmos is that it supports those multi-way.

00:09:24,370 --> 00:09:25,300
>> Yeah.

00:09:25,300 --> 00:09:26,678
>> Okay, cool.

00:09:26,678 --> 00:09:31,785
So, I can talk to this with Jupyter. What else can I do?

00:09:31,785 --> 00:09:34,140
>> So, this is the main way.

00:09:34,140 --> 00:09:36,750
We also have support for Scala.

00:09:36,750 --> 00:09:38,000
If you like programming and Scala,

00:09:38,000 --> 00:09:40,720
we also have some tools for that.

00:09:40,720 --> 00:09:42,780
I think Python is the main language

00:09:42,780 --> 00:09:45,070
that people who are using Spark

00:09:45,070 --> 00:09:47,280
use so this is generally what

00:09:47,280 --> 00:09:49,520
we direct people to use and we have most support for it.

00:09:49,520 --> 00:09:53,910
If you go to our Github, it's github.com/azure,

00:09:53,910 --> 00:09:59,490
I can show you in a moment, /AzureCosmosDBSpark,

00:09:59,490 --> 00:10:02,115
and we have a bunch of performance tips,

00:10:02,115 --> 00:10:04,400
tools, and how to just get ramped up,

00:10:04,400 --> 00:10:05,775
a number of samples,

00:10:05,775 --> 00:10:08,895
other Python Notebooks of other sorts of

00:10:08,895 --> 00:10:13,230
scenarios that you can go through yourself and to unload.

00:10:13,230 --> 00:10:15,810
>> Can I do ML algorithms with Spark?

00:10:15,810 --> 00:10:17,925
>> Yeah, I was actually just going to,

00:10:17,925 --> 00:10:19,500
I don't think we have time to go

00:10:19,500 --> 00:10:21,000
through this whole demo here.

00:10:21,000 --> 00:10:22,725
But you can also,

00:10:22,725 --> 00:10:25,795
using like a data set that you have in Cosmos,

00:10:25,795 --> 00:10:26,820
you can split it in two with

00:10:26,820 --> 00:10:29,550
Spark training set in a test set.

00:10:29,550 --> 00:10:31,435
You can train a model in your training data.

00:10:31,435 --> 00:10:32,910
You can make predictions.

00:10:32,910 --> 00:10:36,090
So, what this demo actually

00:10:36,090 --> 00:10:40,000
does is predicts whether a flight will be delayed or not.

00:10:40,000 --> 00:10:45,550
And so, you can create your ML pipeline.

00:10:45,550 --> 00:10:46,720
And in this example,

00:10:46,720 --> 00:10:49,810
we ran a logistic regression model.

00:10:49,810 --> 00:10:52,750
And I actually did pre-bake this and

00:10:52,750 --> 00:10:55,720
this is a confusion matrix that shows the results.

00:10:55,720 --> 00:10:56,990
>> What is a confusion matrix?

00:10:56,990 --> 00:11:00,990
>> Basically, it shows like the number it splits up.

00:11:00,990 --> 00:11:03,050
If you're testing,

00:11:03,050 --> 00:11:06,175
evaluating whether an aisle model is good or not,

00:11:06,175 --> 00:11:10,060
you can see what the percentages

00:11:10,060 --> 00:11:14,565
of times it guessed whether a flight would be delayed,

00:11:14,565 --> 00:11:16,535
or if the flight is delayed,

00:11:16,535 --> 00:11:18,610
how many times it actually predict that that

00:11:18,610 --> 00:11:21,460
flight would be delayed.

00:11:21,460 --> 00:11:21,890
>> How often it was delayed.

00:11:21,890 --> 00:11:24,010
>> Exactly. So, if we sum

00:11:24,010 --> 00:11:26,470
up the true negatives and the true positives here,

00:11:26,470 --> 00:11:28,270
meaning, when the algorithm was correct,

00:11:28,270 --> 00:11:30,475
we see that we can predict with

00:11:30,475 --> 00:11:32,620
over 70 percent accuracy whether a flight

00:11:32,620 --> 00:11:35,157
was delayed or not, which is not bad.

00:11:35,157 --> 00:11:37,610
And this whole model,

00:11:37,610 --> 00:11:39,760
you can train and run in about five minutes.

00:11:39,760 --> 00:11:40,859
We don't have time now.

00:11:40,859 --> 00:11:43,570
>> That's amazing.

00:11:43,570 --> 00:11:46,630
Can people use the Spark Connector for Cosmos today?

00:11:46,630 --> 00:11:48,720
>> They can. If you go to our Wiki,

00:11:48,720 --> 00:11:51,255
it says, all in public preview.

00:11:51,255 --> 00:11:54,335
You can download the tools, you can get started,

00:11:54,335 --> 00:11:56,090
run these samples on your own,

00:11:56,090 --> 00:11:59,830
and experiment, and do whatever your heart desires.

00:11:59,830 --> 00:12:00,910
>> Very cool. Well, thank you so

00:12:00,910 --> 00:12:02,290
much for getting me up to speed.

00:12:02,290 --> 00:12:03,190
>> Yes, no problem.

00:12:03,190 --> 00:12:05,770
>> All right. I learned all about the Spark Connector for

00:12:05,770 --> 00:12:09,500

YouTube URL: https://www.youtube.com/watch?v=ufxQ25BOZNk


