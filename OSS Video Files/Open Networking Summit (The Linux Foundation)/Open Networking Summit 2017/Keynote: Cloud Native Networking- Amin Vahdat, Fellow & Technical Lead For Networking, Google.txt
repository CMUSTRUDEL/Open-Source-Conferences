Title: Keynote: Cloud Native Networking- Amin Vahdat, Fellow & Technical Lead For Networking, Google
Publication date: 2017-04-10
Playlist: Open Networking Summit 2017
Description: 
	Keynote: Cloud Native Networking- Amin Vahdat, Fellow & Technical Lead For Networking, Google

About Amin Vahdat
Google Fellow & Technical Lead for Networking
Amin Vahdat is a Google Fellow and Technical Lead for networking at Google. He has contributed to Google's data center, wide area, edge/CDN, and cloud networking infrastructure, with a particular focus on driving vertical integration across large-scale compute, networking, and storage. Vahdat has published more than 150 papers in computer systems, with fundamental contributions to cloud computing, data consistency, energy-efficient computing, data center architecture, and optical networking. In the past, he served as the SAIC Professor of Computer Science and Engineering at UC San Diego and the Director of UCSD's Center for Networked Systems. Vahdat received his PhD from UC Berkeley in Computer Science, is an ACM Fellow and a past recipient of the NSF CAREER award, the Alfred P. Sloan Fellowship, and the Duke University David and Janet Vaughn Teaching Award."
Captions: 
	00:00:00,030 --> 00:00:05,549
good morning I'm very excited to be here

00:00:02,610 --> 00:00:07,259
today I am presenting the work of an

00:00:05,549 --> 00:00:09,139
incredibly talented team spread across

00:00:07,259 --> 00:00:12,000
Google technical infrastructure and

00:00:09,139 --> 00:00:13,290
Google cloud platform I'm privileged to

00:00:12,000 --> 00:00:14,700
be part of this team and I'm also

00:00:13,290 --> 00:00:17,820
privileged to be presenting to you on

00:00:14,700 --> 00:00:20,250
their behalf today so I'm going to split

00:00:17,820 --> 00:00:22,890
this talk into two parts the first part

00:00:20,250 --> 00:00:24,869
of the talk I'm going to snapshot and

00:00:22,890 --> 00:00:27,090
reflect on where we are with networking

00:00:24,869 --> 00:00:28,170
at Google a little bit of a

00:00:27,090 --> 00:00:30,000
retrospective on software-defined

00:00:28,170 --> 00:00:32,759
networking and I'm going to use that as

00:00:30,000 --> 00:00:34,110
the launching point to talk about some

00:00:32,759 --> 00:00:36,360
of the challenges that I see for

00:00:34,110 --> 00:00:38,579
networking moving forward to me the

00:00:36,360 --> 00:00:40,620
question of whether software-defined

00:00:38,579 --> 00:00:42,600
networking is a good idea or not is

00:00:40,620 --> 00:00:46,879
closed software-defined networking is

00:00:42,600 --> 00:00:46,879
how we do networking so now what's next

00:00:47,510 --> 00:00:53,460
okay first a little bit of background on

00:00:50,219 --> 00:00:56,129
Google Network it is certainly much more

00:00:53,460 --> 00:00:59,640
than a collection of data centers so

00:00:56,129 --> 00:01:02,699
this map depicts our presence across the

00:00:59,640 --> 00:01:04,860
internet it's worth noting that 25% of

00:01:02,699 --> 00:01:07,110
Internet traffic originates from Google

00:01:04,860 --> 00:01:08,939
today across all of our services that's

00:01:07,110 --> 00:01:12,299
one out of four bytes delivered to

00:01:08,939 --> 00:01:14,820
enterprise delivered to end-users we do

00:01:12,299 --> 00:01:16,799
so not only from our data centers and

00:01:14,820 --> 00:01:18,390
actually principally from about 100

00:01:16,799 --> 00:01:20,280
points of presence but across the world

00:01:18,390 --> 00:01:22,890
right this is where we appear with and

00:01:20,280 --> 00:01:25,439
partner with Internet service providers

00:01:22,890 --> 00:01:27,930
it's also as you see in yellow in this

00:01:25,439 --> 00:01:30,060
picture delivered from Google's global

00:01:27,930 --> 00:01:32,610
cash edge nodes that are actually

00:01:30,060 --> 00:01:34,409
deployed within ISPs for the best

00:01:32,610 --> 00:01:36,930
experience to our users across the world

00:01:34,409 --> 00:01:39,150
and of course it consists of a large

00:01:36,930 --> 00:01:41,220
fiber network depicted in blue inter

00:01:39,150 --> 00:01:45,720
connecting our network together across

00:01:41,220 --> 00:01:49,290
the planet over the last few years we

00:01:45,720 --> 00:01:51,689
have gone sort of headfirst fully into

00:01:49,290 --> 00:01:54,240
cloud to the Google cloud platform

00:01:51,689 --> 00:01:56,670
and this has expanded our network in new

00:01:54,240 --> 00:01:58,740
and exciting ways so what we see layered

00:01:56,670 --> 00:02:01,469
on top of all our deployments is

00:01:58,740 --> 00:02:03,869
actually our sites for new cloud and

00:02:01,469 --> 00:02:06,299
it's new and existing cloud regions so

00:02:03,869 --> 00:02:08,729
what you see in green are the current

00:02:06,299 --> 00:02:11,039
battery regions spread across the world

00:02:08,729 --> 00:02:12,060
and new ones that have recent

00:02:11,039 --> 00:02:16,430
been announced on that are going to be

00:02:12,060 --> 00:02:16,430
coming up over the coming months the

00:02:16,549 --> 00:02:20,519
cloud has been a really exciting

00:02:18,569 --> 00:02:24,030
opportunity for us in networking so

00:02:20,519 --> 00:02:26,159
we've had to build one of the largest

00:02:24,030 --> 00:02:28,079
networks in the world for more than a

00:02:26,159 --> 00:02:30,569
decade to support Google internal

00:02:28,079 --> 00:02:34,650
services whether that's web search Gmail

00:02:30,569 --> 00:02:36,540
YouTube or whatnot but with the move to

00:02:34,650 --> 00:02:38,370
cloud we're now hosting of course the

00:02:36,540 --> 00:02:41,909
world's external services as well and

00:02:38,370 --> 00:02:44,310
this is really pushing us into a step

00:02:41,909 --> 00:02:47,549
function of functionality and capability

00:02:44,310 --> 00:02:49,290
so our architecture is really built

00:02:47,549 --> 00:02:51,629
around disaggregation and this pushes

00:02:49,290 --> 00:02:53,250
our requirements for networking within

00:02:51,629 --> 00:02:56,790
the data center what I mean by this

00:02:53,250 --> 00:02:59,150
aggregation is that storage and compute

00:02:56,790 --> 00:03:02,639
is spread across the entire building and

00:02:59,150 --> 00:03:05,010
we're not thinking about which server

00:03:02,639 --> 00:03:06,659
holds a particular piece of data we're

00:03:05,010 --> 00:03:08,459
replicating the data across the entire

00:03:06,659 --> 00:03:09,989
data center and what that means is that

00:03:08,459 --> 00:03:12,239
the bandwidth requirements and the

00:03:09,989 --> 00:03:14,400
latency requirements for accessing

00:03:12,239 --> 00:03:17,010
anything anywhere have just jumped

00:03:14,400 --> 00:03:19,889
substantially we're pushed within the

00:03:17,010 --> 00:03:21,989
campus we have many buildings in one

00:03:19,889 --> 00:03:24,720
campus each hosting multiple clusters

00:03:21,989 --> 00:03:27,000
the campus is the unit of replication

00:03:24,720 --> 00:03:30,209
for our services no matter how hard

00:03:27,000 --> 00:03:31,769
anyone tries you can only get so much

00:03:30,209 --> 00:03:33,959
availability out of a single cluster

00:03:31,769 --> 00:03:35,970
it's a fault zone in the end we have

00:03:33,959 --> 00:03:36,840
highly available clusters but if you

00:03:35,970 --> 00:03:39,269
want to get to the next level

00:03:36,840 --> 00:03:42,269
availability for your global users you

00:03:39,269 --> 00:03:45,299
must replicate across buildings that's

00:03:42,269 --> 00:03:46,530
just sort of an inherent law what that

00:03:45,299 --> 00:03:49,349
means is that you have to replicate

00:03:46,530 --> 00:03:51,419
content between buildings in real time

00:03:49,349 --> 00:03:53,790
if an update is applied to your service

00:03:51,419 --> 00:03:56,069
in one place it must be reflected in

00:03:53,790 --> 00:03:58,709
real time fully durable fully

00:03:56,069 --> 00:04:01,290
transactional to another building this

00:03:58,709 --> 00:04:03,620
pushes the campus bandwidth needs again

00:04:01,290 --> 00:04:06,120
what we're seeing is a 10x step function

00:04:03,620 --> 00:04:08,009
similarly between our campuses across

00:04:06,120 --> 00:04:10,949
the wide area network the availability

00:04:08,009 --> 00:04:12,689
of turnkey video distribution the rise

00:04:10,949 --> 00:04:14,579
of the Internet of Things if you will

00:04:12,689 --> 00:04:16,470
but really what this also means is that

00:04:14,579 --> 00:04:18,870
just many sensors connected to the

00:04:16,470 --> 00:04:21,510
network cameras real-time video streams

00:04:18,870 --> 00:04:24,410
coming in analysis going out is pushing

00:04:21,510 --> 00:04:27,590
our land bandwidth needs by 10x

00:04:24,410 --> 00:04:30,440
what we realized more than a decade ago

00:04:27,590 --> 00:04:32,750
is that we couldn't buy the network that

00:04:30,440 --> 00:04:35,000
we needed to fulfill our needs and that

00:04:32,750 --> 00:04:39,100
is even more true today we do need

00:04:35,000 --> 00:04:42,230
disruptions really in bandwidth latency

00:04:39,100 --> 00:04:43,400
availability and predictability I want

00:04:42,230 --> 00:04:46,730
to emphasize this last point of

00:04:43,400 --> 00:04:50,210
predictability for humans a predictable

00:04:46,730 --> 00:04:52,310
network is really important but for good

00:04:50,210 --> 00:04:53,990
or bad humans are much more tolerant to

00:04:52,310 --> 00:04:55,490
variability in our performance if your

00:04:53,990 --> 00:04:57,230
webpage takes a little bit longer to

00:04:55,490 --> 00:04:58,820
load every once in a while you're

00:04:57,230 --> 00:05:00,560
probably going to be willing to live

00:04:58,820 --> 00:05:02,840
with it computers are much less

00:05:00,560 --> 00:05:04,580
forgiving they need predictability and

00:05:02,840 --> 00:05:06,860
they're going to actually be built for

00:05:04,580 --> 00:05:08,840
the worst case rather than the common

00:05:06,860 --> 00:05:14,990
case so making the network much more

00:05:08,840 --> 00:05:18,740
predictable is critical so the history

00:05:14,990 --> 00:05:21,620
of modern network architecture I think

00:05:18,740 --> 00:05:24,290
is going to begin at ons in other words

00:05:21,620 --> 00:05:27,710
this community has been responsible for

00:05:24,290 --> 00:05:29,330
defining what networking looks like in

00:05:27,710 --> 00:05:32,360
the modern age and it's really different

00:05:29,330 --> 00:05:33,830
from what it has been at Google we've

00:05:32,360 --> 00:05:36,350
been excited to be able to tell part of

00:05:33,830 --> 00:05:38,090
that story so let me again snapshot on

00:05:36,350 --> 00:05:40,820
some of the things that we've been doing

00:05:38,090 --> 00:05:42,380
and well that's a broadly networking but

00:05:40,820 --> 00:05:44,480
softer Defined Networking the pillars of

00:05:42,380 --> 00:05:45,980
Sdn and Google and actually all of these

00:05:44,480 --> 00:05:49,130
things have been described to you Donna

00:05:45,980 --> 00:05:51,380
ons over the years in 2013 we presented

00:05:49,130 --> 00:05:53,290
before our wide area network

00:05:51,380 --> 00:05:56,180
interconnect for our data centers in

00:05:53,290 --> 00:05:57,890
2014 we described Andromeda this is our

00:05:56,180 --> 00:06:00,560
network function virtualization stack

00:05:57,890 --> 00:06:02,950
our network virtualization stack that

00:06:00,560 --> 00:06:05,990
forms the basis of Google Cloud and

00:06:02,950 --> 00:06:08,150
finally in 2015 we describe Jupiter our

00:06:05,990 --> 00:06:09,890
data center network let me spend just a

00:06:08,150 --> 00:06:11,360
few seconds on each one of these I'm not

00:06:09,890 --> 00:06:12,710
going to go into any of them in any

00:06:11,360 --> 00:06:15,620
detail just because I want to get to

00:06:12,710 --> 00:06:17,990
some of the newer material so before is

00:06:15,620 --> 00:06:20,330
Google software to find web and this

00:06:17,990 --> 00:06:21,440
picture depicts our data centers spread

00:06:20,330 --> 00:06:23,390
across the world and the

00:06:21,440 --> 00:06:26,630
interconnectivity of those data centers

00:06:23,390 --> 00:06:29,180
through before this is built entirely on

00:06:26,630 --> 00:06:33,020
sort of white white boxes with our

00:06:29,180 --> 00:06:34,450
software controlling it at the time we

00:06:33,020 --> 00:06:37,729
believe that was the world's largest

00:06:34,450 --> 00:06:40,009
private network likely is one of

00:06:37,729 --> 00:06:43,789
the biggest today and it continues to

00:06:40,009 --> 00:06:46,610
grow at an unprecedented rate so when we

00:06:43,789 --> 00:06:49,370
revealed it we and built it actually our

00:06:46,610 --> 00:06:50,960
goal was to build a copy Network what we

00:06:49,370 --> 00:06:54,770
refer to as a copy network internally

00:06:50,960 --> 00:06:57,319
what I mean was sort of lots of free ish

00:06:54,770 --> 00:06:58,909
cheap at least bandwidth that might not

00:06:57,319 --> 00:07:01,309
be so reliable so you could use it

00:06:58,909 --> 00:07:02,449
opportunistically as it's grown it's

00:07:01,309 --> 00:07:04,189
really become mission-critical

00:07:02,449 --> 00:07:06,199
right it's really transformed to being

00:07:04,189 --> 00:07:07,550
something that everyone counts on for

00:07:06,199 --> 00:07:09,559
the highest levels of availability and

00:07:07,550 --> 00:07:12,080
the highest levels of bandwidth so this

00:07:09,559 --> 00:07:15,080
graph here shows the growth of traffic

00:07:12,080 --> 00:07:18,770
across before overtime until about the

00:07:15,080 --> 00:07:20,870
end of 2016 and a very interesting thing

00:07:18,770 --> 00:07:22,999
for us is that before actually continues

00:07:20,870 --> 00:07:24,979
to grow faster and carry more traffic

00:07:22,999 --> 00:07:26,300
than our public network and again our

00:07:24,979 --> 00:07:27,860
public network of care is about 25

00:07:26,300 --> 00:07:30,080
percent of Internet traffic so this

00:07:27,860 --> 00:07:31,909
should give you a sense for the demands

00:07:30,080 --> 00:07:33,800
for computer-to-computer communication

00:07:31,909 --> 00:07:37,879
across the internet through our data

00:07:33,800 --> 00:07:39,399
centers and dromeda is our network

00:07:37,879 --> 00:07:41,959
function virtualization stack and

00:07:39,399 --> 00:07:43,969
basically the way to look at this is how

00:07:41,959 --> 00:07:46,009
do we support multi-tenancy within our

00:07:43,969 --> 00:07:48,680
internal cloud such that external

00:07:46,009 --> 00:07:51,080
customers can run on this platform how

00:07:48,680 --> 00:07:53,360
do you give the view of one dedicated

00:07:51,080 --> 00:07:55,849
private network for all of our customers

00:07:53,360 --> 00:07:58,430
allowing them to configure it expand it

00:07:55,849 --> 00:08:00,430
shrink it replicated across multiple

00:07:58,430 --> 00:08:07,159
buildings or even multiple regions

00:08:00,430 --> 00:08:09,559
interactively and finally Jupiter is our

00:08:07,159 --> 00:08:11,899
software-defined approach to datacenter

00:08:09,559 --> 00:08:14,449
networking it reflects the combination

00:08:11,899 --> 00:08:16,939
of about 10 years of work in multiple

00:08:14,449 --> 00:08:19,159
generations of different internal

00:08:16,939 --> 00:08:23,209
solutions that we built to support our

00:08:19,159 --> 00:08:25,279
data centers as of 2013 our Jupiter

00:08:23,209 --> 00:08:27,889
Network supports more than a pet a bit

00:08:25,279 --> 00:08:29,330
per second of bandwidth sort of

00:08:27,889 --> 00:08:31,189
bisection bandwidth among all the

00:08:29,330 --> 00:08:33,079
servers about a hundred thousand servers

00:08:31,189 --> 00:08:34,940
up to a hundred thousand servers within

00:08:33,079 --> 00:08:39,380
the data center and it's continued to

00:08:34,940 --> 00:08:41,750
grow so these are the three pillars of

00:08:39,380 --> 00:08:43,669
Sdn at Google and of course we were very

00:08:41,750 --> 00:08:47,779
proud of the work but one of the pieces

00:08:43,669 --> 00:08:50,930
of feedback that I always got from folks

00:08:47,779 --> 00:08:53,180
outside of Google was this is great

00:08:50,930 --> 00:08:55,430
it only works for internal applications

00:08:53,180 --> 00:08:56,600
you have your walled gardens with your

00:08:55,430 --> 00:09:00,170
data center solution and it's your

00:08:56,600 --> 00:09:01,910
private LAN your cloud solutions but the

00:09:00,170 --> 00:09:03,440
real challenge and I do agree with this

00:09:01,910 --> 00:09:05,270
the real challenge is how do you bring

00:09:03,440 --> 00:09:09,470
Software Defined Networking to the

00:09:05,270 --> 00:09:11,420
public internet so we realize this

00:09:09,470 --> 00:09:14,510
challenge and we realize that our

00:09:11,420 --> 00:09:16,700
network would only be as strong as its

00:09:14,510 --> 00:09:19,310
weakest link in other words if we are to

00:09:16,700 --> 00:09:21,440
deliver a highly available predictable

00:09:19,310 --> 00:09:23,690
high performance experience for our

00:09:21,440 --> 00:09:26,089
customers end-to-end we need to be able

00:09:23,690 --> 00:09:27,709
to extend the power the capability of

00:09:26,089 --> 00:09:29,839
our network not just from our data

00:09:27,709 --> 00:09:32,930
centers between our data centers but all

00:09:29,839 --> 00:09:34,520
the way to the public internet so today

00:09:32,930 --> 00:09:36,830
I'm going to be telling you just a

00:09:34,520 --> 00:09:40,160
little bit about espresso which is our

00:09:36,830 --> 00:09:41,990
Sdn approach for the public Internet the

00:09:40,160 --> 00:09:43,580
special has been in production at Google

00:09:41,990 --> 00:09:45,080
for two years it's carrying a

00:09:43,580 --> 00:09:47,209
substantial fraction of our traffic

00:09:45,080 --> 00:09:49,279
transparent so in other words if you're

00:09:47,209 --> 00:09:51,589
using Google there's a pretty good

00:09:49,279 --> 00:09:55,730
chance that espresso is responsible for

00:09:51,589 --> 00:09:57,650
carrying some of that traffic today let

00:09:55,730 --> 00:09:59,930
me put a special in context for you

00:09:57,650 --> 00:10:02,510
so I described our Jupiter data centers

00:09:59,930 --> 00:10:05,540
that are interconnected within Google by

00:10:02,510 --> 00:10:08,839
rb4 Network we have a second network

00:10:05,540 --> 00:10:12,440
called b2 alright this network connects

00:10:08,839 --> 00:10:14,570
our data centers to our peirong metros

00:10:12,440 --> 00:10:16,520
recall I told you about the hundred

00:10:14,570 --> 00:10:18,560
points of presence 100 peering locations

00:10:16,520 --> 00:10:21,380
we have across the world where we appear

00:10:18,560 --> 00:10:22,940
with our partners this is our b2 network

00:10:21,380 --> 00:10:24,740
of public network and of course we have

00:10:22,940 --> 00:10:26,959
these peering metros that contain both

00:10:24,740 --> 00:10:29,600
routers but also fair number of servers

00:10:26,959 --> 00:10:31,700
a fair amount of storage we do run some

00:10:29,600 --> 00:10:33,980
computation there we serve cached

00:10:31,700 --> 00:10:37,579
content for YouTube and other large

00:10:33,980 --> 00:10:42,650
object distribution sites at the edge

00:10:37,579 --> 00:10:44,570
and of course around this is the

00:10:42,650 --> 00:10:47,959
internet where our users interact with

00:10:44,570 --> 00:10:50,240
us through the pairing metros what we've

00:10:47,959 --> 00:10:52,190
now done is we've taken our approach to

00:10:50,240 --> 00:10:57,640
software-defined networking all the way

00:10:52,190 --> 00:11:01,540
to the peering edge with espresso so why

00:10:57,640 --> 00:11:04,640
the before and after previous to

00:11:01,540 --> 00:11:06,320
espresso we did networking just lie

00:11:04,640 --> 00:11:10,730
essentially everybody else did we ran

00:11:06,320 --> 00:11:13,220
protocols on high-end routers and peered

00:11:10,730 --> 00:11:15,680
with our partners but these routing

00:11:13,220 --> 00:11:18,560
protocols running inside individual

00:11:15,680 --> 00:11:20,060
routers had a very local view they had

00:11:18,560 --> 00:11:21,740
to have a local view right this router

00:11:20,060 --> 00:11:23,210
is going to appear with that router and

00:11:21,740 --> 00:11:25,360
it's going to build up its view of

00:11:23,210 --> 00:11:27,590
connectivity for the network and

00:11:25,360 --> 00:11:30,080
internet protocols in the end are

00:11:27,590 --> 00:11:32,120
optimized for connectivity first in

00:11:30,080 --> 00:11:34,010
other words if you go back to the

00:11:32,120 --> 00:11:36,830
history and the genesis of internet

00:11:34,010 --> 00:11:39,380
protocols the goal of routing protocols

00:11:36,830 --> 00:11:42,560
was to find a path between source and

00:11:39,380 --> 00:11:46,190
destination once you find a path you're

00:11:42,560 --> 00:11:47,990
done right success the goal of internet

00:11:46,190 --> 00:11:49,910
routing protocols is not to find the

00:11:47,990 --> 00:11:51,800
best path and it's certainly not to

00:11:49,910 --> 00:11:53,750
dynamically shift the pattern in

00:11:51,800 --> 00:11:56,330
real-time depending on what's happening

00:11:53,750 --> 00:11:58,370
in the network that doesn't scale at

00:11:56,330 --> 00:12:02,390
least the way that the protocols are

00:11:58,370 --> 00:12:04,760
typically built furthermore internet

00:12:02,390 --> 00:12:06,560
protocols have relatively coarse fault

00:12:04,760 --> 00:12:08,540
recovery if there is a failure in the

00:12:06,560 --> 00:12:10,850
network you're going to require multiple

00:12:08,540 --> 00:12:12,590
rounds of exchange pairwise exchange

00:12:10,850 --> 00:12:15,860
among different routers but across the

00:12:12,590 --> 00:12:17,510
internet before and easily minutes can

00:12:15,860 --> 00:12:19,610
pass before the information spreads

00:12:17,510 --> 00:12:21,530
across the network and and these routers

00:12:19,610 --> 00:12:23,780
can come to convergence come to

00:12:21,530 --> 00:12:25,460
agreement as to some alternate path that

00:12:23,780 --> 00:12:29,300
they might follow in getting to a

00:12:25,460 --> 00:12:31,730
destination with espresso SDM peering

00:12:29,300 --> 00:12:34,580
what we were able to do is basically

00:12:31,730 --> 00:12:36,320
break out of the box we can have a view

00:12:34,580 --> 00:12:39,200
of what's happening in an entire Metro

00:12:36,320 --> 00:12:41,630
entire peering location across many

00:12:39,200 --> 00:12:43,700
wrappers across many servers we can

00:12:41,630 --> 00:12:46,340
further take a global view across all of

00:12:43,700 --> 00:12:48,560
our metros we can use application

00:12:46,340 --> 00:12:50,240
signals with respect to how well our

00:12:48,560 --> 00:12:52,160
applications are actually delivering

00:12:50,240 --> 00:12:54,980
content to end-users spread across the

00:12:52,160 --> 00:12:57,320
world to update how we perform routing

00:12:54,980 --> 00:13:01,490
real time and of course we can then push

00:12:57,320 --> 00:13:03,080
this information in real time this is

00:13:01,490 --> 00:13:04,460
going to be too fast so I apologize for

00:13:03,080 --> 00:13:07,700
that in advance but let me just give you

00:13:04,460 --> 00:13:09,110
a snapshot of how we built this so at

00:13:07,700 --> 00:13:11,090
the bottom we have our external peers

00:13:09,110 --> 00:13:14,270
these external peers are running BGP

00:13:11,090 --> 00:13:16,600
just as they always had but what we've

00:13:14,270 --> 00:13:18,380
done is we've replaced the high end

00:13:16,600 --> 00:13:19,940
complex

00:13:18,380 --> 00:13:21,230
routers that we typically have at the

00:13:19,940 --> 00:13:23,330
edge of our network with a label

00:13:21,230 --> 00:13:24,590
switched fabric you'll see a little bit

00:13:23,330 --> 00:13:27,080
more about what that label switch that

00:13:24,590 --> 00:13:29,450
it means in a moment furthermore we've

00:13:27,080 --> 00:13:31,820
broken BGP out of the box we've

00:13:29,450 --> 00:13:34,190
implemented our own fully compliant in

00:13:31,820 --> 00:13:36,970
an Internet standard BGP stack and we

00:13:34,190 --> 00:13:40,040
now run this BGP stack on servers

00:13:36,970 --> 00:13:43,010
co-located with this label switch fabric

00:13:40,040 --> 00:13:45,740
at the edge of our network and this is

00:13:43,010 --> 00:13:49,070
all within the Metro we have servers

00:13:45,740 --> 00:13:51,350
within the Metro they are terminating

00:13:49,070 --> 00:13:54,470
TCP connections they are serving up

00:13:51,350 --> 00:13:57,470
video content they're doing compute we

00:13:54,470 --> 00:13:59,300
have a packet processor running on every

00:13:57,470 --> 00:14:03,140
one of these hosts we've been able to

00:13:59,300 --> 00:14:04,610
leverage the same basis at least that we

00:14:03,140 --> 00:14:06,620
use for our Andromeda network

00:14:04,610 --> 00:14:09,260
virtualization stack to also do

00:14:06,620 --> 00:14:11,870
high-speed line rate packet processing

00:14:09,260 --> 00:14:13,670
at the edge of our network basically

00:14:11,870 --> 00:14:15,800
what these packet processors do is they

00:14:13,670 --> 00:14:18,980
insert a label onto every packet and

00:14:15,800 --> 00:14:21,470
this label tells the label switched

00:14:18,980 --> 00:14:24,140
fabric which port to use to egress a

00:14:21,470 --> 00:14:26,000
packet so now what have we done we've

00:14:24,140 --> 00:14:28,490
removed the need for an internet scale

00:14:26,000 --> 00:14:30,410
fit I basically think of it as a really

00:14:28,490 --> 00:14:34,190
really big forwarding table that doesn't

00:14:30,410 --> 00:14:36,530
fit on most commodity chips the router

00:14:34,190 --> 00:14:39,350
is no longer need to know how to forward

00:14:36,530 --> 00:14:41,780
data they need to read a label that's

00:14:39,350 --> 00:14:44,150
inserted by the hosts host of lots of

00:14:41,780 --> 00:14:46,100
memory cheap memory erm is much much

00:14:44,150 --> 00:14:49,340
cheaper than what you get on your

00:14:46,100 --> 00:14:51,860
ladders ok and now this course is all

00:14:49,340 --> 00:14:55,250
hooked into a control plane we have a

00:14:51,860 --> 00:14:57,590
local controller within every Metro this

00:14:55,250 --> 00:15:00,170
local controller is simply programming

00:14:57,590 --> 00:15:03,200
the fabric the label switched fabric

00:15:00,170 --> 00:15:07,540
saying for this label egress from this

00:15:03,200 --> 00:15:11,210
port ok the server's are sending

00:15:07,540 --> 00:15:14,750
summaries of how the flows are behaving

00:15:11,210 --> 00:15:16,820
real-time to central control basically

00:15:14,750 --> 00:15:18,770
across all of our connectivity to users

00:15:16,820 --> 00:15:21,080
across the world how our application is

00:15:18,770 --> 00:15:24,530
behaving when I use this particular

00:15:21,080 --> 00:15:26,360
egress path ok so the local controller

00:15:24,530 --> 00:15:28,340
can be making updates in real time if

00:15:26,360 --> 00:15:29,390
there's a failure it can shift things it

00:15:28,340 --> 00:15:31,270
might have backup paths already

00:15:29,390 --> 00:15:33,279
available in some other router

00:15:31,270 --> 00:15:35,500
similarly the global controller can be

00:15:33,279 --> 00:15:37,180
integrating across all Metro views

00:15:35,500 --> 00:15:39,279
so remember 100 metros spread across the

00:15:37,180 --> 00:15:40,930
planet all fitting information in

00:15:39,279 --> 00:15:44,230
real-time to this global controller

00:15:40,930 --> 00:15:46,660
which can react if path a happens to be

00:15:44,230 --> 00:15:48,040
better than path B in real time imagine

00:15:46,660 --> 00:15:51,040
that there's a traffic jam there's

00:15:48,040 --> 00:15:52,510
congestion we can adjust so in real-time

00:15:51,040 --> 00:15:54,520
what we're trying to do is determine

00:15:52,510 --> 00:15:57,820
what's the best way to deliver the best

00:15:54,520 --> 00:16:00,220
quality of experience to end-users based

00:15:57,820 --> 00:16:02,649
on application specific signals and this

00:16:00,220 --> 00:16:04,800
is critical right a router can't know

00:16:02,649 --> 00:16:07,209
how well an application is behaving as

00:16:04,800 --> 00:16:09,459
packets fly by in real time the

00:16:07,209 --> 00:16:11,740
application knows it has those signals

00:16:09,459 --> 00:16:13,540
it provides them to the controller which

00:16:11,740 --> 00:16:14,709
will then make decisions and then feed

00:16:13,540 --> 00:16:19,000
it back down to the routers for

00:16:14,709 --> 00:16:21,160
adjustment and so this has been really

00:16:19,000 --> 00:16:22,690
transformative for us and it has been

00:16:21,160 --> 00:16:24,430
challenging because now what we're doing

00:16:22,690 --> 00:16:26,440
is we're saying we're taking it out of

00:16:24,430 --> 00:16:29,350
fully our control and we're actually

00:16:26,440 --> 00:16:32,880
putting the traffic that is a real-time

00:16:29,350 --> 00:16:34,000
to users across the world on this path

00:16:32,880 --> 00:16:35,589
ok

00:16:34,000 --> 00:16:38,980
so this gives you hopefully a snapshot

00:16:35,589 --> 00:16:40,870
of where we've been what we've been

00:16:38,980 --> 00:16:45,250
doing and as I said for me I consider

00:16:40,870 --> 00:16:48,579
the book more or less closed on SDM from

00:16:45,250 --> 00:16:50,320
starting from our data centers to the

00:16:48,579 --> 00:16:52,329
LAN to network virtualization and now

00:16:50,320 --> 00:16:55,899
all the way to the peering edge of the

00:16:52,329 --> 00:16:56,740
network so what what's next and of

00:16:55,899 --> 00:16:58,180
course that's the end is going to be

00:16:56,740 --> 00:16:59,200
playing a substantial role in it but

00:16:58,180 --> 00:17:01,480
what are going to be the drivers moving

00:16:59,200 --> 00:17:04,030
forward I won't have time to describe

00:17:01,480 --> 00:17:06,790
most of this so I'm going to be focusing

00:17:04,030 --> 00:17:08,910
on just some of the cloud aspects and

00:17:06,790 --> 00:17:14,530
the implications I'll start with

00:17:08,910 --> 00:17:17,110
serverless compute in cloud 3.0 so where

00:17:14,530 --> 00:17:19,540
is cloud going and what role is

00:17:17,110 --> 00:17:21,670
networking play in this context again a

00:17:19,540 --> 00:17:23,709
bit of history so with what I'm going to

00:17:21,670 --> 00:17:26,770
refer to here as cloud 1.0 this was

00:17:23,709 --> 00:17:29,200
think of it circa 2000 maybe 15 years

00:17:26,770 --> 00:17:31,750
ago or so what we had is that

00:17:29,200 --> 00:17:33,730
virtualization allowed enterprises to

00:17:31,750 --> 00:17:35,740
consolidate the servers they could run

00:17:33,730 --> 00:17:37,480
multiple workloads on one machine you

00:17:35,740 --> 00:17:39,370
could have eight different operating

00:17:37,480 --> 00:17:42,040
systems configured just the way you

00:17:39,370 --> 00:17:43,929
wanted on one physical server rather

00:17:42,040 --> 00:17:45,070
than have a physical servers configured

00:17:43,929 --> 00:17:46,930
just the way you need it for

00:17:45,070 --> 00:17:48,490
different applications and this was a

00:17:46,930 --> 00:17:49,990
big deal it really changed how

00:17:48,490 --> 00:17:53,920
enterprises could leverage the data

00:17:49,990 --> 00:17:55,990
centers what were in the middle of right

00:17:53,920 --> 00:17:58,390
now is what I'm referring to as cloud

00:17:55,990 --> 00:18:00,090
2.0 we now have the capability of

00:17:58,390 --> 00:18:02,260
delivering harder on-demand

00:18:00,090 --> 00:18:04,080
configured just the way that you want

00:18:02,260 --> 00:18:09,040
but now you can scale up and down

00:18:04,080 --> 00:18:11,410
dynamically so this public cloud this

00:18:09,040 --> 00:18:13,270
move to the public cloud has really

00:18:11,410 --> 00:18:15,070
freed enterprises from building private

00:18:13,270 --> 00:18:16,540
hardware infrastructure and building

00:18:15,070 --> 00:18:18,340
scalable efficient hardware

00:18:16,540 --> 00:18:20,680
infrastructure is actually really hard

00:18:18,340 --> 00:18:23,200
doing it efficiently is a challenge and

00:18:20,680 --> 00:18:25,210
one that we're now able to leverage by

00:18:23,200 --> 00:18:27,970
basically scaling up and down our

00:18:25,210 --> 00:18:31,330
hardware needs however we're still

00:18:27,970 --> 00:18:32,740
thinking in terms of hardware boxes even

00:18:31,330 --> 00:18:33,780
if there's pseudo hardware boxes that

00:18:32,740 --> 00:18:35,890
happen to be running in the cloud

00:18:33,780 --> 00:18:37,690
running our network running our

00:18:35,890 --> 00:18:39,940
infrastructure hasn't gotten any easier

00:18:37,690 --> 00:18:41,200
we still have to manage everything it

00:18:39,940 --> 00:18:42,880
just happens to be in somebody else's

00:18:41,200 --> 00:18:47,700
data center so we're still thinking in

00:18:42,880 --> 00:18:50,320
terms of scheduling load balancing etc

00:18:47,700 --> 00:18:52,480
well we need to be doing and what we

00:18:50,320 --> 00:18:55,150
will be doing moving forward is moving

00:18:52,480 --> 00:18:58,780
to cloud 3.0 and here the emphasis is on

00:18:55,150 --> 00:19:00,940
compute not on servers and this is what

00:18:58,780 --> 00:19:02,290
I mean by server let's compute what we

00:19:00,940 --> 00:19:04,510
really should be enabling people to do

00:19:02,290 --> 00:19:06,490
is focus on their core competency which

00:19:04,510 --> 00:19:08,710
is to specify their business logic and

00:19:06,490 --> 00:19:10,690
to specify their data how their data

00:19:08,710 --> 00:19:14,620
gets updated how the data gets leveraged

00:19:10,690 --> 00:19:16,630
so we want to enable a move to real-time

00:19:14,620 --> 00:19:19,630
intelligence machine learning and

00:19:16,630 --> 00:19:21,610
service compute not on where your data's

00:19:19,630 --> 00:19:23,350
place how you load balance among the

00:19:21,610 --> 00:19:24,730
different components in your system how

00:19:23,350 --> 00:19:26,230
you're going to configure the operating

00:19:24,730 --> 00:19:30,630
systems on your virtual machines how

00:19:26,230 --> 00:19:32,680
you're going to attach them etc and that

00:19:30,630 --> 00:19:34,000
what I would argue certainly as we're

00:19:32,680 --> 00:19:35,680
looking to the next decade and hopefully

00:19:34,000 --> 00:19:39,100
sooner is that we need to be aiming for

00:19:35,680 --> 00:19:41,170
cloud 3.0 in other words the rush to

00:19:39,100 --> 00:19:43,600
Cloud 2.0 has been take everything that

00:19:41,170 --> 00:19:45,670
I understand in my own private

00:19:43,600 --> 00:19:47,650
enterprise and move it to the cloud and

00:19:45,670 --> 00:19:49,570
this is important and a lot of great

00:19:47,650 --> 00:19:52,780
work has gone into this but I would say

00:19:49,570 --> 00:19:56,220
that this is a waypoint I'm not the

00:19:52,780 --> 00:19:58,450
final destination for virtualization

00:19:56,220 --> 00:20:01,029
now working is going to play a huge role

00:19:58,450 --> 00:20:03,249
in cloud 3.0 so let me highlight some of

00:20:01,029 --> 00:20:05,139
the bits that I see here storage this

00:20:03,249 --> 00:20:08,649
aggregation is going to be critical and

00:20:05,139 --> 00:20:10,149
as I alluded to earlier the data center

00:20:08,649 --> 00:20:12,669
as a whole is going to be your storage

00:20:10,149 --> 00:20:14,409
appliance furthermore multiple data

00:20:12,669 --> 00:20:18,159
centers are going to be the basis for

00:20:14,409 --> 00:20:22,110
your high availability design seamlessly

00:20:18,159 --> 00:20:24,850
you're going to be able to replicate and

00:20:22,110 --> 00:20:26,529
transact on your data with the highest

00:20:24,850 --> 00:20:28,990
levels of availability and of course the

00:20:26,529 --> 00:20:31,330
highest levels of durability think about

00:20:28,990 --> 00:20:34,389
the networking requirements to really

00:20:31,330 --> 00:20:37,179
make any disk and maybe much much

00:20:34,389 --> 00:20:39,190
tougher any flash device in multiple

00:20:37,179 --> 00:20:40,840
data centers appear as if it were local

00:20:39,190 --> 00:20:44,970
how do you deliver that kind of

00:20:40,840 --> 00:20:48,190
predictability on one network at scale

00:20:44,970 --> 00:20:51,220
seamless telemetry this is basically the

00:20:48,190 --> 00:20:53,200
generalization of load balancing scale

00:20:51,220 --> 00:20:55,149
up and scale down again how do you

00:20:53,200 --> 00:20:56,769
specify what your performance

00:20:55,149 --> 00:20:58,840
requirements are what your availability

00:20:56,769 --> 00:21:00,340
requirements are and have your compute

00:20:58,840 --> 00:21:03,220
scale up and down transparently

00:21:00,340 --> 00:21:05,950
underneath the hoods the application

00:21:03,220 --> 00:21:08,110
signals and the spin up spin down of

00:21:05,950 --> 00:21:11,320
compute infrastructure is going to have

00:21:08,110 --> 00:21:13,600
to be sub-second it can't be operating

00:21:11,320 --> 00:21:16,649
at the granularity of even multiple

00:21:13,600 --> 00:21:19,119
seconds certainly not minutes and hours

00:21:16,649 --> 00:21:21,009
we found that transplant live migration

00:21:19,119 --> 00:21:23,379
is key people don't want the server's to

00:21:21,009 --> 00:21:25,179
go down that was true before cloud for

00:21:23,379 --> 00:21:26,740
Google and it's even more true now you

00:21:25,179 --> 00:21:28,980
can't say I'm going to be taking this

00:21:26,740 --> 00:21:32,860
over down once a week for maintenance

00:21:28,980 --> 00:21:35,350
what we've been able to do is move

00:21:32,860 --> 00:21:38,529
virtual machines transparently hitless

00:21:35,350 --> 00:21:40,269
ly from one place to another even

00:21:38,529 --> 00:21:42,610
orchestrating moves from one data center

00:21:40,269 --> 00:21:44,470
to another data center without customers

00:21:42,610 --> 00:21:45,879
being able to notice once again the

00:21:44,470 --> 00:21:46,960
networking requirements were doing so

00:21:45,879 --> 00:21:48,639
just think about all the states

00:21:46,960 --> 00:21:51,220
associated with all these virtual

00:21:48,639 --> 00:21:54,249
machines and then moving them in a

00:21:51,220 --> 00:21:56,590
manner that no one notices remains at

00:21:54,249 --> 00:21:58,509
least partially an open question finally

00:21:56,590 --> 00:22:00,100
we need to be able to support an open

00:21:58,509 --> 00:22:01,720
marketplace if you're going to specify

00:22:00,100 --> 00:22:03,879
compute you're going to have third

00:22:01,720 --> 00:22:06,539
parties providing supporting services to

00:22:03,879 --> 00:22:09,200
you how do you plug that in securely

00:22:06,539 --> 00:22:13,779
transparently into your network

00:22:09,200 --> 00:22:13,779
while obeying your higher-level policies

00:22:14,860 --> 00:22:19,940
so some of the main things I want to

00:22:17,179 --> 00:22:22,490
emphasize here are we need to move to

00:22:19,940 --> 00:22:24,590
policy not middleboxes right

00:22:22,490 --> 00:22:26,330
today we're still specifying how packets

00:22:24,590 --> 00:22:29,149
flow through individual middleboxes

00:22:26,330 --> 00:22:31,010
to implement NFC functionality what you

00:22:29,149 --> 00:22:34,399
really want to be able to say is here's

00:22:31,010 --> 00:22:37,309
the NFC functionality firewalls load

00:22:34,399 --> 00:22:38,960
balancing whatever that I need and then

00:22:37,309 --> 00:22:41,539
have the network configure itself to

00:22:38,960 --> 00:22:44,809
deliver that for you similarly we want

00:22:41,539 --> 00:22:46,820
to move toward SL O's I'm service level

00:22:44,809 --> 00:22:48,320
objectives you mean here is the response

00:22:46,820 --> 00:22:50,720
time that I would like my application to

00:22:48,320 --> 00:22:52,519
have here's the level availability that

00:22:50,720 --> 00:22:54,889
I need to deliver for my application and

00:22:52,519 --> 00:22:57,019
then underneath the hoods the system can

00:22:54,889 --> 00:22:58,460
figure out how much replication we

00:22:57,019 --> 00:23:00,860
should have both within a data center

00:22:58,460 --> 00:23:02,539
and across data centers potentially

00:23:00,860 --> 00:23:04,940
across the planet to meet your latency

00:23:02,539 --> 00:23:07,460
needs for global user population to meet

00:23:04,940 --> 00:23:09,320
your availability needs and to meet of

00:23:07,460 --> 00:23:11,539
course your compute demand depending on

00:23:09,320 --> 00:23:18,049
real-time signals for popularity of your

00:23:11,539 --> 00:23:20,570
services so over the next decade

00:23:18,049 --> 00:23:22,700
again the network is going to be central

00:23:20,570 --> 00:23:24,289
for defining what compute means it's

00:23:22,700 --> 00:23:26,269
going to enable next-generation compute

00:23:24,289 --> 00:23:28,159
infrastructure it must in that we're

00:23:26,269 --> 00:23:30,100
going to certainly within individual

00:23:28,159 --> 00:23:32,899
racks but I would say that even larger

00:23:30,100 --> 00:23:34,970
granularity be moving to new computing

00:23:32,899 --> 00:23:37,010
models right so in other words the

00:23:34,970 --> 00:23:39,950
computer is going to break out of the

00:23:37,010 --> 00:23:41,090
confines of an individual server the

00:23:39,950 --> 00:23:43,580
network will define next-generation

00:23:41,090 --> 00:23:44,690
storage infrastructure so once again how

00:23:43,580 --> 00:23:47,750
do we move to a world where we don't

00:23:44,690 --> 00:23:50,029
think about local disk local flash and

00:23:47,750 --> 00:23:52,100
we think about storage that is available

00:23:50,029 --> 00:23:55,669
to us as if they were locals but across

00:23:52,100 --> 00:23:57,919
the data center and hopefully what I've

00:23:55,669 --> 00:23:59,360
been emphasizing here is that the right

00:23:57,919 --> 00:24:02,059
network infrastructure can deliver

00:23:59,360 --> 00:24:03,110
fundamental new capability so to

00:24:02,059 --> 00:24:05,330
emphasize some of the points made

00:24:03,110 --> 00:24:08,000
earlier it's not only about cost

00:24:05,330 --> 00:24:10,220
it's not about bandwidth it's not about

00:24:08,000 --> 00:24:12,049
latency it's about new capability right

00:24:10,220 --> 00:24:15,440
building applications that you wouldn't

00:24:12,049 --> 00:24:18,860
imagine building otherwise ok I only

00:24:15,440 --> 00:24:22,310
have a little bit of time so I'll flash

00:24:18,860 --> 00:24:25,640
up two things one is how we internally

00:24:22,310 --> 00:24:28,610
prioritize our infrastructure work and

00:24:25,640 --> 00:24:30,830
what I want to say is that everything

00:24:28,610 --> 00:24:32,510
starts with of availability if you don't

00:24:30,830 --> 00:24:34,250
have an available infrastructure you

00:24:32,510 --> 00:24:37,270
really don't have any purchase to do

00:24:34,250 --> 00:24:41,450
anything else besides that

00:24:37,270 --> 00:24:42,680
then comes manageability once you have a

00:24:41,450 --> 00:24:45,760
manageable infrastructure you can look

00:24:42,680 --> 00:24:48,230
at velocity I'll talk about that shortly

00:24:45,760 --> 00:24:50,360
with velocity you can look at reducing

00:24:48,230 --> 00:24:52,790
your stranding and finally when you have

00:24:50,360 --> 00:24:54,440
all that you can get the performance and

00:24:52,790 --> 00:24:57,500
I know that there's a lot of strong

00:24:54,440 --> 00:25:00,500
academic people in the audience I also

00:24:57,500 --> 00:25:02,410
emphasize this pyramid in terms of how

00:25:00,500 --> 00:25:06,080
to prioritize some of the research work

00:25:02,410 --> 00:25:08,560
I'm going to skip to one last bit which

00:25:06,080 --> 00:25:12,260
is velocity which i think is a

00:25:08,560 --> 00:25:14,960
super-important I define velocity to be

00:25:12,260 --> 00:25:16,730
a speed of iteration for us to get to

00:25:14,960 --> 00:25:19,550
where we need to go we have to be able

00:25:16,730 --> 00:25:21,470
to move fast and what I mean here is not

00:25:19,550 --> 00:25:23,360
have a fast Network but have a network

00:25:21,470 --> 00:25:27,110
that can be evolved on a weekly basis

00:25:23,360 --> 00:25:29,150
what we strive for and what we demand at

00:25:27,110 --> 00:25:30,790
Google is we can upgrade our network

00:25:29,150 --> 00:25:33,650
with new functionality new features

00:25:30,790 --> 00:25:35,060
again without anyone noticing so the

00:25:33,650 --> 00:25:37,910
availability remembers the base of the

00:25:35,060 --> 00:25:40,040
pyramid every week if you can only

00:25:37,910 --> 00:25:42,410
iterate every month every three months

00:25:40,040 --> 00:25:44,030
every six months we heard about five to

00:25:42,410 --> 00:25:45,770
seven year refresh rates from Martine

00:25:44,030 --> 00:25:47,360
earlier you're never going to get to

00:25:45,770 --> 00:25:49,210
where you need to go and the network

00:25:47,360 --> 00:25:53,510
will always be the bottleneck in

00:25:49,210 --> 00:25:54,920
enabling innovation so how do you get to

00:25:53,510 --> 00:25:57,950
a place where you architect the system

00:25:54,920 --> 00:25:59,870
from the ground up from the hardware to

00:25:57,950 --> 00:26:01,850
the network infrastructure to the

00:25:59,870 --> 00:26:04,880
applications running on top of it such

00:26:01,850 --> 00:26:07,630
that you build for velocity right every

00:26:04,880 --> 00:26:09,950
week you get a new software release and

00:26:07,630 --> 00:26:11,510
no one notices there is no downtime

00:26:09,950 --> 00:26:14,210
there is no maintenance window you don't

00:26:11,510 --> 00:26:15,440
tell people at 11:43 p.m. I'm going to

00:26:14,210 --> 00:26:17,180
be upgrading the network please stop

00:26:15,440 --> 00:26:20,570
running your applications it gets

00:26:17,180 --> 00:26:22,870
upgraded and people then benefit from

00:26:20,570 --> 00:26:26,660
the new functionality that you deliver

00:26:22,870 --> 00:26:28,670
so what we really follow here is launch

00:26:26,660 --> 00:26:30,400
and iterate how do you get your service

00:26:28,670 --> 00:26:32,360
out there with Minimum Viable

00:26:30,400 --> 00:26:33,070
functionality still hopefully

00:26:32,360 --> 00:26:37,900
powerful

00:26:33,070 --> 00:26:40,600
and then every week make it better okay

00:26:37,900 --> 00:26:44,750
so I'm going to skip the rest of the

00:26:40,600 --> 00:26:53,640
presentation and stop for any questions

00:26:44,750 --> 00:26:56,560
[Applause]

00:26:53,640 --> 00:27:00,430
fantastic I mean Michael Howard from IHS

00:26:56,560 --> 00:27:05,500
market you must have considered the

00:27:00,430 --> 00:27:08,980
stateless segment routing and but you've

00:27:05,500 --> 00:27:14,100
chosen to put state nearby and available

00:27:08,980 --> 00:27:17,040
on servers and somehow build your header

00:27:14,100 --> 00:27:19,230
from that what were your considerations

00:27:17,040 --> 00:27:23,590
great question and we are a big fan of

00:27:19,230 --> 00:27:25,060
segment routing and certainly we will

00:27:23,590 --> 00:27:28,360
leverage it I think the main

00:27:25,060 --> 00:27:32,410
consideration for us is again iteration

00:27:28,360 --> 00:27:36,300
speed and velocity so segment routing to

00:27:32,410 --> 00:27:38,680
first order isn't available so how do we

00:27:36,300 --> 00:27:42,070
how do we launch our functionality

00:27:38,680 --> 00:27:43,840
without Nessus so again to make sure I'm

00:27:42,070 --> 00:27:46,120
clear I'm a big fan of segment routing

00:27:43,840 --> 00:27:49,120
but I don't want to be dependent on a

00:27:46,120 --> 00:27:51,040
piece of software and harder becoming

00:27:49,120 --> 00:27:52,840
available commercially before I can move

00:27:51,040 --> 00:27:54,610
forward with my functionality so the

00:27:52,840 --> 00:27:59,500
main consideration was how do we get

00:27:54,610 --> 00:28:01,150
those soon as possible I'm going to ask

00:27:59,500 --> 00:28:05,350
a question here's maybe a little bit

00:28:01,150 --> 00:28:07,690
away from from the focus but concerning

00:28:05,350 --> 00:28:10,060
your advances in clouds and information

00:28:07,690 --> 00:28:12,100
caching and information kind of like

00:28:10,060 --> 00:28:14,050
hosting all over the world and you have

00:28:12,100 --> 00:28:16,300
the biggest content provider have you

00:28:14,050 --> 00:28:18,850
guys thought about what you're going to

00:28:16,300 --> 00:28:21,130
do next for information centric networks

00:28:18,850 --> 00:28:23,200
ICN and content based networks you have

00:28:21,130 --> 00:28:24,730
any projects that are like promoting

00:28:23,200 --> 00:28:27,580
this kind of things especially if you

00:28:24,730 --> 00:28:28,870
have open source projects that people

00:28:27,580 --> 00:28:32,170
can collaborate with you already can

00:28:28,870 --> 00:28:33,940
open offer some kind of like toolkits

00:28:32,170 --> 00:28:35,200
for for folks because I think that's

00:28:33,940 --> 00:28:39,400
going to be also one of the next

00:28:35,200 --> 00:28:41,470
challenges for for clouds networking ok

00:28:39,400 --> 00:28:44,080
yeah it's a good question I think that

00:28:41,470 --> 00:28:45,270
broadly speaking information centric

00:28:44,080 --> 00:28:47,280
networking as

00:28:45,270 --> 00:28:48,810
and that were engaged in especially if

00:28:47,280 --> 00:28:51,600
you think about the implications of

00:28:48,810 --> 00:28:53,430
cloud 3.0 so in other words really here

00:28:51,600 --> 00:28:55,130
we're talking about name data that's

00:28:53,430 --> 00:28:57,060
transparently available to you

00:28:55,130 --> 00:28:59,250
regardless of where you happen to be

00:28:57,060 --> 00:29:00,690
running your computes so certainly happy

00:28:59,250 --> 00:29:04,730
to talk more about that I do agree that

00:29:00,690 --> 00:29:04,730
this compute model is an interesting one

00:29:05,270 --> 00:29:12,920
okay weird about 5g which is coming

00:29:08,970 --> 00:29:17,070
pretty soon millions billion of device

00:29:12,920 --> 00:29:20,730
5g the pure ipv6 environment when ipv6

00:29:17,070 --> 00:29:22,790
is coming from Google yes yes so

00:29:20,730 --> 00:29:25,140
certainly we have support for ipv6

00:29:22,790 --> 00:29:27,480
endpoints and have had it for a long

00:29:25,140 --> 00:29:30,720
time we've had a number of internal

00:29:27,480 --> 00:29:32,400
projects that are studying ipv6 across

00:29:30,720 --> 00:29:34,380
our network so depending on the

00:29:32,400 --> 00:29:37,350
particular context that you're asking a

00:29:34,380 --> 00:29:40,260
question either ipv6 is already at

00:29:37,350 --> 00:29:42,360
Google or is coming certainly for some

00:29:40,260 --> 00:29:48,620
cloud settings shortly you can have ipv6

00:29:42,360 --> 00:29:49,770
endpoints today without any issue okay

00:29:48,620 --> 00:29:51,840
great

00:29:49,770 --> 00:29:55,520
I think we're wrapping it up yep thank

00:29:51,840 --> 00:29:55,520

YouTube URL: https://www.youtube.com/watch?v=1xBZ5DGZZmQ


