Title: Managing Kubernetes Edge Fleets Prone to Network Fault Tolerance - Mark Abrams, Rancher Labs
Publication date: 2020-10-28
Playlist: Open Networking & Edge Summit 2020
Description: 
	Managing Kubernetes Edge Fleets Prone to Network Fault Tolerance - Mark Abrams, Rancher Labs
Captions: 
	00:00:02,320 --> 00:00:06,240
hello

00:00:03,120 --> 00:00:07,919
and welcome to the managing kubernetes

00:00:06,240 --> 00:00:08,960
edge fleets prone to network fault

00:00:07,919 --> 00:00:17,440
tolerance talk

00:00:08,960 --> 00:00:20,240
for the open network and edge summit

00:00:17,440 --> 00:00:21,439
i am mark abrams i'm a field engineer

00:00:20,240 --> 00:00:25,119
edge specialist with

00:00:21,439 --> 00:00:28,560
rancher labs and um what i do

00:00:25,119 --> 00:00:32,079
is i work with pre and post sales teams

00:00:28,560 --> 00:00:33,520
as a technical resource for rancher labs

00:00:32,079 --> 00:00:36,000
and to help

00:00:33,520 --> 00:00:38,399
teams learn how to build things out on

00:00:36,000 --> 00:00:40,640
the edge using kubernetes

00:00:38,399 --> 00:00:41,920
and to learn about how they're doing it

00:00:40,640 --> 00:00:44,879
prior to to

00:00:41,920 --> 00:00:46,320
us getting involved and sort of help the

00:00:44,879 --> 00:00:47,920
the rancher team develop the right

00:00:46,320 --> 00:00:50,079
product by interfacing with these other

00:00:47,920 --> 00:00:54,079
teams

00:00:50,079 --> 00:00:57,199
i have an agenda here for us today

00:00:54,079 --> 00:00:59,680
and what i'd like to do i want to do

00:00:57,199 --> 00:01:02,160
things a little bit different today

00:00:59,680 --> 00:01:03,280
normally i save my demos for the end of

00:01:02,160 --> 00:01:05,119
a presentation but

00:01:03,280 --> 00:01:08,159
because of the way we're presenting

00:01:05,119 --> 00:01:12,080
today i want to make sure i get the

00:01:08,159 --> 00:01:13,280
demo in and so we'll demo first you can

00:01:12,080 --> 00:01:14,720
always come back to it

00:01:13,280 --> 00:01:15,920
if you if you're thinking about it

00:01:14,720 --> 00:01:17,920
throughout the presentation of course

00:01:15,920 --> 00:01:19,600
it's pre-recorded so you can

00:01:17,920 --> 00:01:21,040
roll back and take a look and see what i

00:01:19,600 --> 00:01:23,119
was doing um

00:01:21,040 --> 00:01:24,560
and then after i present we'll talk

00:01:23,119 --> 00:01:26,560
about it a little bit

00:01:24,560 --> 00:01:27,759
after that i want to talk about some of

00:01:26,560 --> 00:01:31,040
the foundations

00:01:27,759 --> 00:01:33,759
um for this presentation

00:01:31,040 --> 00:01:35,520
and then i'll talk about containers and

00:01:33,759 --> 00:01:37,680
container orchestration

00:01:35,520 --> 00:01:39,600
now many of you already know about

00:01:37,680 --> 00:01:40,960
containers and container orchestration

00:01:39,600 --> 00:01:42,640
some of you don't though so i want to

00:01:40,960 --> 00:01:44,479
really make sure i i cover it for

00:01:42,640 --> 00:01:45,759
everybody and that i'm really addressing

00:01:44,479 --> 00:01:48,880
everybody who's

00:01:45,759 --> 00:01:50,240
interested in in this information so

00:01:48,880 --> 00:01:51,439
i'll do it briefly it's going to be a

00:01:50,240 --> 00:01:53,520
summary if you don't know about

00:01:51,439 --> 00:01:55,759
containers and container orchestration

00:01:53,520 --> 00:01:57,360
this is not the only talk you should be

00:01:55,759 --> 00:01:59,360
watching there's a lot more to it and

00:01:57,360 --> 00:02:00,479
you should go check it out

00:01:59,360 --> 00:02:02,399
of course i'm going to talk about

00:02:00,479 --> 00:02:04,799
network fault tolerance and

00:02:02,399 --> 00:02:06,719
um sort of the what is what it is and

00:02:04,799 --> 00:02:10,000
and give some examples of

00:02:06,719 --> 00:02:13,040
where we see it um on the edge and

00:02:10,000 --> 00:02:15,280
in our customers and and the people that

00:02:13,040 --> 00:02:17,120
i talk to data day in and day out

00:02:15,280 --> 00:02:18,640
and then i'll talk about uh k3s in

00:02:17,120 --> 00:02:20,400
practice and and the

00:02:18,640 --> 00:02:22,080
lightweight kubernetes and how it can be

00:02:20,400 --> 00:02:25,280
used with

00:02:22,080 --> 00:02:27,360
the edge and network fault tolerance the

00:02:25,280 --> 00:02:28,959
fleet is part of the title fleet

00:02:27,360 --> 00:02:31,920
references

00:02:28,959 --> 00:02:33,760
large numbers many large numbers of

00:02:31,920 --> 00:02:36,319
kubernetes clusters

00:02:33,760 --> 00:02:38,640
well fleet references larging man number

00:02:36,319 --> 00:02:41,680
managing large numbers of anything

00:02:38,640 --> 00:02:44,400
in the um

00:02:41,680 --> 00:02:46,160
basically in the in this presentation

00:02:44,400 --> 00:02:49,519
i'm really talking about edge fleets

00:02:46,160 --> 00:02:51,360
of kubernetes clusters so let's get on

00:02:49,519 --> 00:02:52,879
with it so let me

00:02:51,360 --> 00:02:54,480
move right along we'll move on to the

00:02:52,879 --> 00:02:57,200
demo and for that

00:02:54,480 --> 00:03:00,239
i'm going to go to what i have as a

00:02:57,200 --> 00:03:03,360
pre-recorded terminal session

00:03:00,239 --> 00:03:05,680
so if you look at up at the top

00:03:03,360 --> 00:03:06,560
left here you'll see i've already loaded

00:03:05,680 --> 00:03:08,560
a command watch

00:03:06,560 --> 00:03:09,840
kube ctl get pods and that's the first

00:03:08,560 --> 00:03:11,040
command i'm going to run

00:03:09,840 --> 00:03:13,120
i'm going to go ahead and play this and

00:03:11,040 --> 00:03:13,760
i'll talk through it as it goes so i run

00:03:13,120 --> 00:03:16,319
that command

00:03:13,760 --> 00:03:18,239
and there's nothing there i get an error

00:03:16,319 --> 00:03:19,760
it actually doesn't find any resources

00:03:18,239 --> 00:03:21,519
because there are none

00:03:19,760 --> 00:03:23,120
so i move down to the bottom part of the

00:03:21,519 --> 00:03:26,080
terminal and i

00:03:23,120 --> 00:03:27,440
apply this speed limit yaml which is the

00:03:26,080 --> 00:03:29,599
configuration file

00:03:27,440 --> 00:03:31,120
for some resources that i want to put

00:03:29,599 --> 00:03:34,319
into kubernetes

00:03:31,120 --> 00:03:36,720
those resources include the

00:03:34,319 --> 00:03:37,920
fault tolerance namespace and a daemon

00:03:36,720 --> 00:03:40,560
set

00:03:37,920 --> 00:03:42,799
with this speed limit application in it

00:03:40,560 --> 00:03:45,840
and you can see now i'm up in the top

00:03:42,799 --> 00:03:47,920
right hand side and i'm actually running

00:03:45,840 --> 00:03:50,000
i try and look at the logs for this

00:03:47,920 --> 00:03:52,480
speed limit container that popped up

00:03:50,000 --> 00:03:53,439
on the top left but nothing came up

00:03:52,480 --> 00:03:55,360
there there was

00:03:53,439 --> 00:03:57,040
there were no logs at first and that's

00:03:55,360 --> 00:03:57,920
because there's an initialization

00:03:57,040 --> 00:04:01,599
container

00:03:57,920 --> 00:04:03,680
that in it container actually is

00:04:01,599 --> 00:04:05,360
holding back the the main container from

00:04:03,680 --> 00:04:05,760
starting because it's checking to make

00:04:05,360 --> 00:04:07,840
sure

00:04:05,760 --> 00:04:08,959
that the network tolerance the network

00:04:07,840 --> 00:04:12,239
speeds

00:04:08,959 --> 00:04:14,080
are acceptable so it checks it figures

00:04:12,239 --> 00:04:15,840
out yep the network speeds are good

00:04:14,080 --> 00:04:17,600
my new container pops up and it starts

00:04:15,840 --> 00:04:18,959
downloading content and i'm actually

00:04:17,600 --> 00:04:22,000
just looping through

00:04:18,959 --> 00:04:22,880
and pulling down cncf.io again and again

00:04:22,000 --> 00:04:26,320
and again

00:04:22,880 --> 00:04:28,400
until i go over to my network control

00:04:26,320 --> 00:04:29,680
and i actually start throttling the

00:04:28,400 --> 00:04:32,320
network

00:04:29,680 --> 00:04:34,160
for this device um so it's something i

00:04:32,320 --> 00:04:35,919
can do to demonstrate

00:04:34,160 --> 00:04:37,199
what it looks like when my bandwidth

00:04:35,919 --> 00:04:40,080
goes goes sour

00:04:37,199 --> 00:04:41,040
right so bad bandwidth rather than

00:04:40,080 --> 00:04:42,880
running

00:04:41,040 --> 00:04:44,639
stopping the speed limit container you

00:04:42,880 --> 00:04:45,440
can see i didn't the container is still

00:04:44,639 --> 00:04:47,600
running

00:04:45,440 --> 00:04:48,960
but actually i stopped downloading so i

00:04:47,600 --> 00:04:52,160
we're in another part of the

00:04:48,960 --> 00:04:53,199
the um app where it just stops pulling

00:04:52,160 --> 00:04:55,199
down the cncf

00:04:53,199 --> 00:04:57,040
stuff then i go back and i throttle it

00:04:55,199 --> 00:04:59,440
back up and you'll see it starts

00:04:57,040 --> 00:05:02,720
downloading content again

00:04:59,440 --> 00:05:05,440
so this is just one representation of

00:05:02,720 --> 00:05:07,600
what you can do using kubernetes to

00:05:05,440 --> 00:05:09,120
manage network faults

00:05:07,600 --> 00:05:10,800
what i have here and i'll just pause the

00:05:09,120 --> 00:05:11,440
video right here because this is a

00:05:10,800 --> 00:05:14,800
static

00:05:11,440 --> 00:05:16,400
information but this is a view of what's

00:05:14,800 --> 00:05:19,600
actually deployed

00:05:16,400 --> 00:05:21,199
into this kubernetes this k3s cluster

00:05:19,600 --> 00:05:23,120
and you can see that the first thing

00:05:21,199 --> 00:05:26,639
here is an init container

00:05:23,120 --> 00:05:27,759
and that in it container is really just

00:05:26,639 --> 00:05:29,199
running speed test

00:05:27,759 --> 00:05:32,000
this is something you've probably run in

00:05:29,199 --> 00:05:35,520
your browser before to try and test the

00:05:32,000 --> 00:05:36,800
uh the network latency of your own home

00:05:35,520 --> 00:05:40,160
network

00:05:36,800 --> 00:05:42,080
or you're in a coffee shop or going well

00:05:40,160 --> 00:05:44,479
so you run speed test right

00:05:42,080 --> 00:05:46,400
um and that that's run as a command line

00:05:44,479 --> 00:05:47,360
tool um so i'm able to run that in the

00:05:46,400 --> 00:05:49,120
container

00:05:47,360 --> 00:05:50,960
get the results and figure out what do i

00:05:49,120 --> 00:05:52,479
want to do with my workload

00:05:50,960 --> 00:05:54,080
but the first one the init container

00:05:52,479 --> 00:05:55,600
actually prevents the workload from

00:05:54,080 --> 00:05:57,440
starting

00:05:55,600 --> 00:05:59,600
but once it's started i have this

00:05:57,440 --> 00:06:02,880
liveness check and i run that

00:05:59,600 --> 00:06:05,120
i run the liveness check periodically

00:06:02,880 --> 00:06:06,960
you can see it delays for 20 seconds and

00:06:05,120 --> 00:06:08,720
it timeouts for 30 sec

00:06:06,960 --> 00:06:12,000
it allows a timeout of 30 seconds so

00:06:08,720 --> 00:06:15,039
it'll allow 30 seconds for it to run

00:06:12,000 --> 00:06:15,759
um and it then it'll do this every five

00:06:15,039 --> 00:06:18,880
seconds

00:06:15,759 --> 00:06:22,319
um the the liveness check is

00:06:18,880 --> 00:06:25,199
almost identical to the init container

00:06:22,319 --> 00:06:25,520
but it's actually running um as part of

00:06:25,199 --> 00:06:27,360
just

00:06:25,520 --> 00:06:29,680
part of the functionality that i get

00:06:27,360 --> 00:06:33,759
from container orchestration

00:06:29,680 --> 00:06:35,840
so um my i'm managing my

00:06:33,759 --> 00:06:38,080
application i'm managing how much it

00:06:35,840 --> 00:06:40,319
accesses the network

00:06:38,080 --> 00:06:41,360
using some kubernetes facilities to do

00:06:40,319 --> 00:06:43,840
so

00:06:41,360 --> 00:06:44,960
so cool that's the demo let's talk about

00:06:43,840 --> 00:06:48,000
how i got there

00:06:44,960 --> 00:06:49,919
and what the significance of these

00:06:48,000 --> 00:06:51,039
um you know the other parts of network

00:06:49,919 --> 00:06:53,199
fault tolerance

00:06:51,039 --> 00:06:54,319
why use kubernetes on the edge and how

00:06:53,199 --> 00:06:57,759
it can help us with

00:06:54,319 --> 00:07:00,400
problems like this so here's some

00:06:57,759 --> 00:07:02,639
of the foundations to the presentation

00:07:00,400 --> 00:07:05,360
some of the constraints that

00:07:02,639 --> 00:07:07,120
i saw when looking at network talk fault

00:07:05,360 --> 00:07:08,720
tolerance with our customers and

00:07:07,120 --> 00:07:10,960
prospects

00:07:08,720 --> 00:07:11,840
and with the you know lots and lots of

00:07:10,960 --> 00:07:14,880
different

00:07:11,840 --> 00:07:18,080
edge scenarios

00:07:14,880 --> 00:07:19,120
it turns out one of the most common

00:07:18,080 --> 00:07:23,599
problems

00:07:19,120 --> 00:07:26,880
for edge devices is uh

00:07:23,599 --> 00:07:28,800
network fault tolerance it's extremely

00:07:26,880 --> 00:07:31,599
common

00:07:28,800 --> 00:07:33,280
low bandwidth limit connectivity spotty

00:07:31,599 --> 00:07:34,800
connectivity

00:07:33,280 --> 00:07:36,800
every time i have a conversation not

00:07:34,800 --> 00:07:39,680
every time but very often

00:07:36,800 --> 00:07:42,800
when when we have conversations with

00:07:39,680 --> 00:07:44,160
customers who are working on the edge

00:07:42,800 --> 00:07:46,400
those are issues that they have

00:07:44,160 --> 00:07:49,520
sometimes you know they it's

00:07:46,400 --> 00:07:51,680
um even like an air gap network so they

00:07:49,520 --> 00:07:53,360
built a network that just doesn't allow

00:07:51,680 --> 00:07:54,000
any interaction with the outside at all

00:07:53,360 --> 00:07:56,639
so

00:07:54,000 --> 00:07:58,560
it's not really a network fault but

00:07:56,639 --> 00:08:01,840
often the edge is disconnected

00:07:58,560 --> 00:08:04,960
at long periods of time um the other

00:08:01,840 --> 00:08:06,080
sort of common theme for um the edge

00:08:04,960 --> 00:08:09,520
stack that i

00:08:06,080 --> 00:08:12,639
see is hundreds or

00:08:09,520 --> 00:08:14,800
tens of thousands of devices um

00:08:12,639 --> 00:08:16,800
you know just just massive numbers of

00:08:14,800 --> 00:08:18,879
devices at the edge

00:08:16,800 --> 00:08:21,440
um and that's what i call fleet right

00:08:18,879 --> 00:08:23,680
it's it's a large number of devices

00:08:21,440 --> 00:08:25,120
um this obviously i did this demo on a

00:08:23,680 --> 00:08:28,319
single device so it applies

00:08:25,120 --> 00:08:32,240
single device but we need to be able to

00:08:28,319 --> 00:08:34,719
handle this across many devices

00:08:32,240 --> 00:08:36,320
i'm not going to talk a lot about fleet

00:08:34,719 --> 00:08:37,919
today there wasn't really enough time

00:08:36,320 --> 00:08:41,120
for me to really

00:08:37,919 --> 00:08:43,200
talk about how to manage fleets

00:08:41,120 --> 00:08:45,519
but the idea of network fault tolerance

00:08:43,200 --> 00:08:49,279
in fleets is really

00:08:45,519 --> 00:08:51,200
common so and there will be other talks

00:08:49,279 --> 00:08:52,720
i have some other upcoming talks that

00:08:51,200 --> 00:08:54,080
about fleet as well so

00:08:52,720 --> 00:08:55,839
hopefully that'll get to a linux

00:08:54,080 --> 00:08:59,040
foundation event coming to you

00:08:55,839 --> 00:09:01,120
soon i also have

00:08:59,040 --> 00:09:02,560
seen non-homogeneous resources at the

00:09:01,120 --> 00:09:04,160
edge i mean this is pretty common the

00:09:02,560 --> 00:09:06,240
data center

00:09:04,160 --> 00:09:07,360
we just lay out these servers and the

00:09:06,240 --> 00:09:09,360
idea is that

00:09:07,360 --> 00:09:11,120
everything's got cpu and ram and that's

00:09:09,360 --> 00:09:13,760
what our resource that's what our

00:09:11,120 --> 00:09:14,959
applications need at the edge that's not

00:09:13,760 --> 00:09:18,000
the case

00:09:14,959 --> 00:09:21,920
at the edge what we find is that um

00:09:18,000 --> 00:09:22,959
there's more than just cpu and ram

00:09:21,920 --> 00:09:24,640
oftentimes

00:09:22,959 --> 00:09:26,240
uh well in the in the data center

00:09:24,640 --> 00:09:29,279
sometimes it's gpus

00:09:26,240 --> 00:09:30,160
at the edge it's gpus but it's also as

00:09:29,279 --> 00:09:33,920
well

00:09:30,160 --> 00:09:35,440
um everything from um you know actuators

00:09:33,920 --> 00:09:37,680
and sensors

00:09:35,440 --> 00:09:39,040
to devices the the hardware right on the

00:09:37,680 --> 00:09:42,240
device for example

00:09:39,040 --> 00:09:43,200
um in one of my demos which i won't show

00:09:42,240 --> 00:09:45,920
you today but i

00:09:43,200 --> 00:09:47,360
actually control the nic right from

00:09:45,920 --> 00:09:50,800
kubernetes so

00:09:47,360 --> 00:09:52,560
um you know we may have other types of

00:09:50,800 --> 00:09:54,080
devices bluetooth zigbee

00:09:52,560 --> 00:09:56,320
stuff like that we'll talk about that

00:09:54,080 --> 00:09:58,320
later so

00:09:56,320 --> 00:10:00,480
why container orchestration why should i

00:09:58,320 --> 00:10:02,800
use container orchestration at the edge

00:10:00,480 --> 00:10:04,560
um i want systemic consistency across

00:10:02,800 --> 00:10:06,399
the enterprise i want

00:10:04,560 --> 00:10:08,880
my development teams who are developing

00:10:06,399 --> 00:10:10,240
apps to have consistency even as we go

00:10:08,880 --> 00:10:13,440
out to the edge

00:10:10,240 --> 00:10:14,399
traditionally the edge has been embedded

00:10:13,440 --> 00:10:16,880
systems

00:10:14,399 --> 00:10:18,399
and my app is tightly coupled with my

00:10:16,880 --> 00:10:22,160
operating system

00:10:18,399 --> 00:10:24,959
and to update that package i need to do

00:10:22,160 --> 00:10:26,079
a firmware upgrade and so it's a very

00:10:24,959 --> 00:10:28,399
you know onerous

00:10:26,079 --> 00:10:29,360
um problematic task it doesn't happen

00:10:28,399 --> 00:10:32,560
often

00:10:29,360 --> 00:10:35,360
the whereas the lifecycle for app ups

00:10:32,560 --> 00:10:36,560
um we're approaching you know we're

00:10:35,360 --> 00:10:40,000
never going to get to

00:10:36,560 --> 00:10:43,360
zero um minutes but we're approaching

00:10:40,000 --> 00:10:45,040
momentary updates in the data center

00:10:43,360 --> 00:10:46,720
right and not everybody's doing that

00:10:45,040 --> 00:10:47,440
today but that that was the goal that's

00:10:46,720 --> 00:10:50,399
how we

00:10:47,440 --> 00:10:52,079
developed distributed computing in part

00:10:50,399 --> 00:10:55,200
you know that was the

00:10:52,079 --> 00:10:57,200
impetus behind it so container

00:10:55,200 --> 00:10:58,880
orchestration brings me

00:10:57,200 --> 00:11:02,000
availability and scalability and a

00:10:58,880 --> 00:11:04,000
minimum times reliability

00:11:02,000 --> 00:11:05,760
it allows me to separate my concerns at

00:11:04,000 --> 00:11:06,320
the edge so i can take that embedded

00:11:05,760 --> 00:11:09,279
system

00:11:06,320 --> 00:11:10,800
as my app built in separate the hardware

00:11:09,279 --> 00:11:11,839
from the operating system from the

00:11:10,800 --> 00:11:14,640
application

00:11:11,839 --> 00:11:15,040
up update those independently keep my

00:11:14,640 --> 00:11:17,680
app

00:11:15,040 --> 00:11:20,079
on a more consistent update lifecycle

00:11:17,680 --> 00:11:22,640
lead into an application life cycle

00:11:20,079 --> 00:11:24,079
more similar to what we see in the cloud

00:11:22,640 --> 00:11:26,320
these are all things that we see

00:11:24,079 --> 00:11:28,240
our edge customers desiring to get and

00:11:26,320 --> 00:11:31,040
that they can achieve through kubernetes

00:11:28,240 --> 00:11:31,920
and container orchestration um in

00:11:31,040 --> 00:11:33,839
addition

00:11:31,920 --> 00:11:35,200
i get then the remote management right

00:11:33,839 --> 00:11:38,640
so now i can i can

00:11:35,200 --> 00:11:41,519
update my um actually through kubernetes

00:11:38,640 --> 00:11:42,880
i can even update my hardware as well i

00:11:41,519 --> 00:11:46,079
can update

00:11:42,880 --> 00:11:46,880
my my os and my other parts of the

00:11:46,079 --> 00:11:50,240
hardware

00:11:46,880 --> 00:11:52,720
depending on how it's operating but

00:11:50,240 --> 00:11:53,760
it is possible to do even using

00:11:52,720 --> 00:11:55,360
kubernetes

00:11:53,760 --> 00:11:56,800
with things like a system upgrade

00:11:55,360 --> 00:11:59,120
control

00:11:56,800 --> 00:11:59,839
controller and then of course i want to

00:11:59,120 --> 00:12:02,959
manage

00:11:59,839 --> 00:12:06,079
tens to hundreds of thousands of devices

00:12:02,959 --> 00:12:08,079
so um a typical

00:12:06,079 --> 00:12:10,240
scenario on the edge this is a typical

00:12:08,079 --> 00:12:11,519
network layout like just really

00:12:10,240 --> 00:12:14,079
superficial right

00:12:11,519 --> 00:12:15,760
i've got device one through device n and

00:12:14,079 --> 00:12:16,880
they just talk back to the data center

00:12:15,760 --> 00:12:19,360
or the cloud

00:12:16,880 --> 00:12:21,040
what have you um and then there's these

00:12:19,360 --> 00:12:24,320
other scenarios where i have these

00:12:21,040 --> 00:12:27,440
devices but i insert a gauge

00:12:24,320 --> 00:12:29,200
device map um so my devices don't talk

00:12:27,440 --> 00:12:30,639
directly to the cloud they talk through

00:12:29,200 --> 00:12:34,240
a gateway and then they

00:12:30,639 --> 00:12:37,760
say so pretty much these two

00:12:34,240 --> 00:12:39,839
scenarios um they can get

00:12:37,760 --> 00:12:41,600
these networks can get much more complex

00:12:39,839 --> 00:12:43,040
behind the data center or downstream

00:12:41,600 --> 00:12:45,519
from the data center

00:12:43,040 --> 00:12:46,959
um but this is the general gist of it

00:12:45,519 --> 00:12:49,040
and the issue is

00:12:46,959 --> 00:12:50,639
the issue for network fault tolerance

00:12:49,040 --> 00:12:52,480
anyways is when

00:12:50,639 --> 00:12:54,720
we lose connectivity at any one of these

00:12:52,480 --> 00:12:57,920
points so if any of these things

00:12:54,720 --> 00:13:01,839
lose connectivity to their partner

00:12:57,920 --> 00:13:04,240
that has an impact but it shouldn't

00:13:01,839 --> 00:13:06,399
cause total failure of the system and

00:13:04,240 --> 00:13:08,000
again kubernetes can help with that

00:13:06,399 --> 00:13:09,519
so i'm going to leave you with that as

00:13:08,000 --> 00:13:12,399
sort of the foundations for this

00:13:09,519 --> 00:13:13,680
this talk um we did the demo already i'm

00:13:12,399 --> 00:13:15,200
going to touch on containers and

00:13:13,680 --> 00:13:17,360
container orchestration i'm going to

00:13:15,200 --> 00:13:19,200
zip over it so for those of you that

00:13:17,360 --> 00:13:20,399
know it please bear with me for those of

00:13:19,200 --> 00:13:22,399
you that don't

00:13:20,399 --> 00:13:23,920
this is very superficial but i want to

00:13:22,399 --> 00:13:26,399
just make sure you're with me for the

00:13:23,920 --> 00:13:29,040
rest of the talk

00:13:26,399 --> 00:13:30,800
so for containers and container

00:13:29,040 --> 00:13:32,320
orchestration

00:13:30,800 --> 00:13:34,560
on the right hand side what you see is

00:13:32,320 --> 00:13:35,519
sort of the traditional processes right

00:13:34,560 --> 00:13:38,000
you'll see

00:13:35,519 --> 00:13:39,760
that i've got my device and then i've

00:13:38,000 --> 00:13:43,199
got my operating system

00:13:39,760 --> 00:13:46,959
i've got some process and my process

00:13:43,199 --> 00:13:49,920
has dependencies on the left hand side

00:13:46,959 --> 00:13:50,560
i've containerized those processes so

00:13:49,920 --> 00:13:53,199
they're

00:13:50,560 --> 00:13:54,079
they're sitting on top of this docker

00:13:53,199 --> 00:13:56,160
runtime you've

00:13:54,079 --> 00:13:57,680
i'm sure you've heard of docker it's a

00:13:56,160 --> 00:13:59,440
container runtime

00:13:57,680 --> 00:14:02,320
and it just allows me to run these

00:13:59,440 --> 00:14:05,360
containers with their processes

00:14:02,320 --> 00:14:10,000
so that we can fully contain the

00:14:05,360 --> 00:14:11,360
tests and its dependencies in most cases

00:14:10,000 --> 00:14:13,680
in the data center resources are

00:14:11,360 --> 00:14:15,120
homogeneous so i can i can pretty much

00:14:13,680 --> 00:14:16,800
fully contain them and just

00:14:15,120 --> 00:14:18,720
depend on that every machine is going to

00:14:16,800 --> 00:14:21,120
have the resources i need

00:14:18,720 --> 00:14:23,120
or i can use things like taints and

00:14:21,120 --> 00:14:26,639
tolerations and kubernetes

00:14:23,120 --> 00:14:29,680
to target specific

00:14:26,639 --> 00:14:32,959
hardware however at the edge i

00:14:29,680 --> 00:14:35,040
the resources at the edge are yes um but

00:14:32,959 --> 00:14:36,240
they are finite and known for any given

00:14:35,040 --> 00:14:38,399
scenario

00:14:36,240 --> 00:14:40,639
which means at some point i must decide

00:14:38,399 --> 00:14:42,160
what hardware operating system sensor

00:14:40,639 --> 00:14:44,399
actuator whatever it is

00:14:42,160 --> 00:14:46,000
that my edge device will have and i can

00:14:44,399 --> 00:14:48,399
start targeting processes

00:14:46,000 --> 00:14:49,760
to the resources that are available

00:14:48,399 --> 00:14:53,120
there

00:14:49,760 --> 00:14:54,000
so um with that let's look when we

00:14:53,120 --> 00:14:56,560
insert so

00:14:54,000 --> 00:14:58,240
you just have containers right but what

00:14:56,560 --> 00:14:59,760
i want is container orchestration

00:14:58,240 --> 00:15:02,959
containers aren't enough

00:14:59,760 --> 00:15:04,560
um they can contain my app sort of um

00:15:02,959 --> 00:15:06,240
but some of the dependencies are

00:15:04,560 --> 00:15:07,760
in the hardware right they're not in the

00:15:06,240 --> 00:15:10,320
container themselves

00:15:07,760 --> 00:15:11,920
um with container orchestration i'm

00:15:10,320 --> 00:15:13,680
adding this other layer you can see i

00:15:11,920 --> 00:15:16,720
have k3s here

00:15:13,680 --> 00:15:18,639
which is kubernetes and it has cri so

00:15:16,720 --> 00:15:20,959
that's the container runtime

00:15:18,639 --> 00:15:22,000
k3s happens to use container d not

00:15:20,959 --> 00:15:23,839
docker

00:15:22,000 --> 00:15:25,760
but it's got my pods with my container

00:15:23,839 --> 00:15:26,880
so it looks very similar to what we had

00:15:25,760 --> 00:15:29,199
it's just my

00:15:26,880 --> 00:15:32,000
container runtime is inside of of the

00:15:29,199 --> 00:15:34,320
container orchestration

00:15:32,000 --> 00:15:36,240
application that's running and this

00:15:34,320 --> 00:15:37,920
allows me to do things like networking

00:15:36,240 --> 00:15:39,680
scheduling

00:15:37,920 --> 00:15:41,360
and when i talk about scheduling i'm not

00:15:39,680 --> 00:15:41,920
talking about putting things on your

00:15:41,360 --> 00:15:44,399
calendar

00:15:41,920 --> 00:15:46,880
i'm talking about scheduling resources

00:15:44,399 --> 00:15:47,920
what what workloads need what resources

00:15:46,880 --> 00:15:50,639
where are they

00:15:47,920 --> 00:15:51,120
kubernetes can help me with that i can

00:15:50,639 --> 00:15:54,240
manage

00:15:51,120 --> 00:15:56,560
app and service life cycles i can

00:15:54,240 --> 00:15:57,920
get scalability and reliability of the

00:15:56,560 --> 00:16:00,399
services

00:15:57,920 --> 00:16:02,320
and if i have multiple nodes clustered

00:16:00,399 --> 00:16:04,800
together i can get availability

00:16:02,320 --> 00:16:07,279
i can have things that will function

00:16:04,800 --> 00:16:09,680
even when one of the nodes goes out

00:16:07,279 --> 00:16:10,399
that looks sort of like this where i

00:16:09,680 --> 00:16:12,320
have a

00:16:10,399 --> 00:16:14,000
resource pool and this is really what we

00:16:12,320 --> 00:16:16,639
see in the cloud right where we pool our

00:16:14,000 --> 00:16:20,240
resources of cpu and ram

00:16:16,639 --> 00:16:22,959
and then those four pods that i had

00:16:20,240 --> 00:16:23,759
in the previous in this slide those four

00:16:22,959 --> 00:16:26,800
pods

00:16:23,759 --> 00:16:28,639
can be spread across sources now in that

00:16:26,800 --> 00:16:30,959
example i showed you earlier

00:16:28,639 --> 00:16:33,040
we actually had a demon set so that

00:16:30,959 --> 00:16:35,759
would just be one pod a demon set means

00:16:33,040 --> 00:16:39,040
i'm going to run it on every node

00:16:35,759 --> 00:16:39,759
and that was the in this situation that

00:16:39,040 --> 00:16:42,079
i'm

00:16:39,759 --> 00:16:42,800
simulating every node is networked in

00:16:42,079 --> 00:16:45,360
some way

00:16:42,800 --> 00:16:47,360
and there so that every node needs to be

00:16:45,360 --> 00:16:51,839
able to have access to understanding

00:16:47,360 --> 00:16:51,839
information so we use a daemon set

00:16:52,399 --> 00:16:56,480
so um container orchestration on the

00:16:54,639 --> 00:16:57,040
edge let's just talk about that a little

00:16:56,480 --> 00:17:00,240
bit

00:16:57,040 --> 00:17:02,959
um so some of the scenarios that

00:17:00,240 --> 00:17:04,720
that we see it's actually very common to

00:17:02,959 --> 00:17:07,520
see single node clusters so

00:17:04,720 --> 00:17:08,079
a single device where people just want

00:17:07,520 --> 00:17:11,199
to take

00:17:08,079 --> 00:17:13,120
a advantage of some of the

00:17:11,199 --> 00:17:14,400
availability and scalability

00:17:13,120 --> 00:17:18,000
capabilities

00:17:14,400 --> 00:17:21,039
or the they are using like the container

00:17:18,000 --> 00:17:24,480
orchestration to as a

00:17:21,039 --> 00:17:26,160
um a step in for advisor and i'll talk

00:17:24,480 --> 00:17:27,919
about that a little bit more

00:17:26,160 --> 00:17:29,200
in a little further on in the

00:17:27,919 --> 00:17:31,840
presentation

00:17:29,200 --> 00:17:33,600
um in addition we've got of course a

00:17:31,840 --> 00:17:35,679
container orchestration layer

00:17:33,600 --> 00:17:36,799
um and that's what the container runtime

00:17:35,679 --> 00:17:40,720
is and then

00:17:36,799 --> 00:17:42,400
um with k3s we have and with edge we

00:17:40,720 --> 00:17:46,240
have device size limitations

00:17:42,400 --> 00:17:48,880
k3s will run well in about one cpu

00:17:46,240 --> 00:17:49,600
with a gig of ram we often get requests

00:17:48,880 --> 00:17:53,600
to run it

00:17:49,600 --> 00:17:55,200
under a quarter of that size about 256

00:17:53,600 --> 00:17:58,640
megabytes of ram

00:17:55,200 --> 00:18:00,400
but that is not something that we can

00:17:58,640 --> 00:18:03,039
really do well today i've run it in

00:18:00,400 --> 00:18:06,480
about 512 megabytes of ram

00:18:03,039 --> 00:18:08,080
um but we do recommend one gig

00:18:06,480 --> 00:18:10,960
all right so let's get on to the good

00:18:08,080 --> 00:18:14,320
stuff network fault tolerance

00:18:10,960 --> 00:18:16,480
so some types of network faults i

00:18:14,320 --> 00:18:17,440
i showed you a network faults um that

00:18:16,480 --> 00:18:21,200
had to do with

00:18:17,440 --> 00:18:22,000
uh low bandwidth right a low limit was

00:18:21,200 --> 00:18:24,880
hit

00:18:22,000 --> 00:18:26,720
but um in my diagrams i was showing

00:18:24,880 --> 00:18:28,160
total loss of communication like that

00:18:26,720 --> 00:18:31,280
you know the gateway just

00:18:28,160 --> 00:18:34,080
blows out can't communicate um and then

00:18:31,280 --> 00:18:35,919
if we have single nodes um what happens

00:18:34,080 --> 00:18:37,919
if they lose connectivity

00:18:35,919 --> 00:18:39,520
um if it's a single node cluster of

00:18:37,919 --> 00:18:41,200
course that's it it's out just like

00:18:39,520 --> 00:18:43,360
total loss

00:18:41,200 --> 00:18:45,360
a multi-node h8 cluster with kubernetes

00:18:43,360 --> 00:18:47,840
or container orchestration

00:18:45,360 --> 00:18:49,280
um if the cluster itself is ha i might

00:18:47,840 --> 00:18:51,200
be able to move a workload

00:18:49,280 --> 00:18:52,880
over to another node so there are some

00:18:51,200 --> 00:18:54,640
advantages there with multi-node

00:18:52,880 --> 00:18:57,360
clusters

00:18:54,640 --> 00:18:58,080
and we do see that in in some scenarios

00:18:57,360 --> 00:18:59,919
where there

00:18:58,080 --> 00:19:02,720
it is possible to have multiple nodes

00:18:59,919 --> 00:19:06,000
and and sort of that h a capability

00:19:02,720 --> 00:19:07,840
uh as well so it varies on your your um

00:19:06,000 --> 00:19:09,120
use case of course and then of course

00:19:07,840 --> 00:19:11,200
multi-node non-ha

00:19:09,120 --> 00:19:12,640
we need to do something it's it's it's a

00:19:11,200 --> 00:19:14,240
problem i don't know what the problem is

00:19:12,640 --> 00:19:16,480
until you tell me more about your use

00:19:14,240 --> 00:19:16,480
case

00:19:16,559 --> 00:19:20,240
so let's look at from here what i want

00:19:19,039 --> 00:19:23,440
to do is look at

00:19:20,240 --> 00:19:25,120
examples specific examples from

00:19:23,440 --> 00:19:28,160
customers i've talked to

00:19:25,120 --> 00:19:29,280
obviously i can't share customer names

00:19:28,160 --> 00:19:31,360
but

00:19:29,280 --> 00:19:32,880
here's an example we've talked to a

00:19:31,360 --> 00:19:34,640
number of

00:19:32,880 --> 00:19:38,240
customers and prospects that have this

00:19:34,640 --> 00:19:40,000
issue they have these service vehicles

00:19:38,240 --> 00:19:41,679
this is pretty common in the energy

00:19:40,000 --> 00:19:44,559
industry um

00:19:41,679 --> 00:19:45,440
these vehicles you know think of like a

00:19:44,559 --> 00:19:48,000
large

00:19:45,440 --> 00:19:49,280
truck like a a dump truck or garbage

00:19:48,000 --> 00:19:51,440
truck but instead of

00:19:49,280 --> 00:19:53,120
having the garbage mechanism in the back

00:19:51,440 --> 00:19:55,120
it's basically got a data center in the

00:19:53,120 --> 00:19:56,240
back they have onboard high performance

00:19:55,120 --> 00:19:58,160
computing

00:19:56,240 --> 00:20:00,400
they manage fleets of hundreds of these

00:19:58,160 --> 00:20:03,679
vehicles and of course with that

00:20:00,400 --> 00:20:04,799
managing multiple devices on each of

00:20:03,679 --> 00:20:07,600
these computers but

00:20:04,799 --> 00:20:08,240
the the people on board are not i.t

00:20:07,600 --> 00:20:10,400
they're not

00:20:08,240 --> 00:20:12,799
you know trained in networking they're

00:20:10,400 --> 00:20:14,480
not trained in data center

00:20:12,799 --> 00:20:17,840
practices they're trained in running

00:20:14,480 --> 00:20:19,120
that and operating that truck

00:20:17,840 --> 00:20:21,120
each of these trucks because their

00:20:19,120 --> 00:20:23,440
mobile vehicles they have the potential

00:20:21,120 --> 00:20:25,120
to exit the network area

00:20:23,440 --> 00:20:27,200
they often are connected only by

00:20:25,120 --> 00:20:27,679
cellular or satellite so they already

00:20:27,200 --> 00:20:31,039
have

00:20:27,679 --> 00:20:33,280
often low bandwidth scenarios

00:20:31,039 --> 00:20:35,120
and then during a network faults they

00:20:33,280 --> 00:20:35,520
need to continue to operate like if

00:20:35,120 --> 00:20:36,960
they're

00:20:35,520 --> 00:20:38,799
if they're doing some sort of drilling

00:20:36,960 --> 00:20:41,679
or or um

00:20:38,799 --> 00:20:42,080
some sort of geo detection you know

00:20:41,679 --> 00:20:45,679
they're

00:20:42,080 --> 00:20:47,520
mapping the earth um they they need to

00:20:45,679 --> 00:20:49,919
continue to operate in that capacity

00:20:47,520 --> 00:20:53,919
even without the connectivity

00:20:49,919 --> 00:20:55,440
up uplink so um you know they recognize

00:20:53,919 --> 00:20:56,640
they can't receive updates they can't

00:20:55,440 --> 00:20:58,480
send data out

00:20:56,640 --> 00:20:59,919
but they they can handle this this

00:20:58,480 --> 00:21:00,720
outage they're aware that it's going to

00:20:59,919 --> 00:21:02,080
happen

00:21:00,720 --> 00:21:05,360
so they take action they stop

00:21:02,080 --> 00:21:08,240
communicating upstream

00:21:05,360 --> 00:21:10,159
locally they when the network comes back

00:21:08,240 --> 00:21:14,960
they flush the data

00:21:10,159 --> 00:21:17,200
that's one example another example is

00:21:14,960 --> 00:21:18,880
these could be retail stores they could

00:21:17,200 --> 00:21:22,000
be um

00:21:18,880 --> 00:21:24,480
uh food chains um you know the

00:21:22,000 --> 00:21:25,919
conglomerates often own multiple retail

00:21:24,480 --> 00:21:28,400
stores so this these

00:21:25,919 --> 00:21:29,440
uh scenarios get to a situation where

00:21:28,400 --> 00:21:32,240
they have a lot

00:21:29,440 --> 00:21:33,840
of devices they tend to be small one to

00:21:32,240 --> 00:21:35,280
three node clusters

00:21:33,840 --> 00:21:37,440
maybe the type of device i would

00:21:35,280 --> 00:21:40,480
actually put under my

00:21:37,440 --> 00:21:42,240
under my desk right a little home

00:21:40,480 --> 00:21:43,600
device sometimes it's enough sometimes

00:21:42,240 --> 00:21:46,720
these are raspberry pi

00:21:43,600 --> 00:21:49,039
sized the store uses business grade or

00:21:46,720 --> 00:21:50,720
consumer grade network

00:21:49,039 --> 00:21:53,200
from the same providers you and i get

00:21:50,720 --> 00:21:55,679
our network home network from

00:21:53,200 --> 00:21:56,799
network outages are generally isolated

00:21:55,679 --> 00:21:58,640
to stores

00:21:56,799 --> 00:22:00,240
we do have a lot of clusters as i was

00:21:58,640 --> 00:22:02,080
saying so again

00:22:00,240 --> 00:22:04,159
during a network fault the store has to

00:22:02,080 --> 00:22:05,840
continue to operate in every capacity

00:22:04,159 --> 00:22:06,720
that it can people still need to be able

00:22:05,840 --> 00:22:09,039
to pay

00:22:06,720 --> 00:22:10,640
things of course have changed during the

00:22:09,039 --> 00:22:13,280
times of pandemic

00:22:10,640 --> 00:22:14,080
in fact we're seeing that these these

00:22:13,280 --> 00:22:16,080
types of

00:22:14,080 --> 00:22:17,520
problems they're increasing the amount

00:22:16,080 --> 00:22:19,600
of technology that they're putting into

00:22:17,520 --> 00:22:23,039
the stores not decreasing it

00:22:19,600 --> 00:22:24,960
so um customers they're trying to give

00:22:23,039 --> 00:22:26,640
customers the best experience possible

00:22:24,960 --> 00:22:28,640
you know similar

00:22:26,640 --> 00:22:30,400
actions that they take stop

00:22:28,640 --> 00:22:33,600
communicating upstream

00:22:30,400 --> 00:22:35,520
store data locally transmit network

00:22:33,600 --> 00:22:38,559
comes back up

00:22:35,520 --> 00:22:41,840
and then the last one is in a factory

00:22:38,559 --> 00:22:42,159
uh so an assembly um in this scenario

00:22:41,840 --> 00:22:44,320
they

00:22:42,159 --> 00:22:46,240
they often have gateways so everything

00:22:44,320 --> 00:22:48,320
behind and downstream of the gateway

00:22:46,240 --> 00:22:49,760
doesn't even talk outside it only talks

00:22:48,320 --> 00:22:51,760
to the gateway

00:22:49,760 --> 00:22:53,360
often they're less concerned about loss

00:22:51,760 --> 00:22:55,200
of connectivity between the smaller

00:22:53,360 --> 00:22:57,840
devices or the robot

00:22:55,200 --> 00:23:01,200
necessarily smaller and the gateway

00:22:57,840 --> 00:23:05,360
these gateways often have ai capability

00:23:01,200 --> 00:23:07,280
or are often highly clusters

00:23:05,360 --> 00:23:09,840
and then the edge devices will flow

00:23:07,280 --> 00:23:11,440
through the

00:23:09,840 --> 00:23:13,360
but the gateway has the potential to

00:23:11,440 --> 00:23:15,840
disconnect um so

00:23:13,360 --> 00:23:17,840
again during a network fault their

00:23:15,840 --> 00:23:21,120
assembly lines can't go down

00:23:17,840 --> 00:23:22,960
um but the the fleet management it

00:23:21,120 --> 00:23:26,240
cannot

00:23:22,960 --> 00:23:28,880
see the the clusters anymore so it can't

00:23:26,240 --> 00:23:30,640
even know about the downstream stuff

00:23:28,880 --> 00:23:31,919
doesn't know about the gateways but

00:23:30,640 --> 00:23:34,240
that's okay

00:23:31,919 --> 00:23:35,120
um as long as you know they take the

00:23:34,240 --> 00:23:38,240
right actions

00:23:35,120 --> 00:23:40,880
similarly stop trying to communicate

00:23:38,240 --> 00:23:44,159
continue operating the the assembly line

00:23:40,880 --> 00:23:45,919
flush data on return

00:23:44,159 --> 00:23:47,840
sometimes maybe what you want to do is

00:23:45,919 --> 00:23:49,679
tag the gateway as unavailable so in

00:23:47,840 --> 00:23:51,679
kubernetes you can create labels and say

00:23:49,679 --> 00:23:53,360
hey this thing's not available

00:23:51,679 --> 00:23:55,120
it's possible with these sometimes

00:23:53,360 --> 00:23:57,520
there's multiple buildings

00:23:55,120 --> 00:23:58,559
on a site each building has its own

00:23:57,520 --> 00:24:00,159
gateway

00:23:58,559 --> 00:24:01,840
could connect through another gateway in

00:24:00,159 --> 00:24:02,880
another building if the networking in

00:24:01,840 --> 00:24:05,520
the and the

00:24:02,880 --> 00:24:06,720
site is appropriate so you can actually

00:24:05,520 --> 00:24:09,600
still

00:24:06,720 --> 00:24:11,360
operate you know have functionality

00:24:09,600 --> 00:24:13,360
depending on the scenario

00:24:11,360 --> 00:24:15,360
um and then often there's socket

00:24:13,360 --> 00:24:16,880
communication just between the robotics

00:24:15,360 --> 00:24:18,159
the systems that are there not going

00:24:16,880 --> 00:24:21,919
through the gateway

00:24:18,159 --> 00:24:25,200
so um it can continue operating

00:24:21,919 --> 00:24:26,480
um we have about five minutes left

00:24:25,200 --> 00:24:29,039
a little bit i would like to try and

00:24:26,480 --> 00:24:31,039
save time for questions um

00:24:29,039 --> 00:24:32,960
so general considerations right stop

00:24:31,039 --> 00:24:36,799
trying to store your data

00:24:32,960 --> 00:24:38,480
flush tag advice um we saw that through

00:24:36,799 --> 00:24:41,360
all those examples

00:24:38,480 --> 00:24:42,159
um i kind of went through and i laid out

00:24:41,360 --> 00:24:46,559
the

00:24:42,159 --> 00:24:48,799
the um details on going to

00:24:46,559 --> 00:24:49,840
do each of these very quickly because

00:24:48,799 --> 00:24:51,279
you can

00:24:49,840 --> 00:24:55,039
look through if you want to stop on any

00:24:51,279 --> 00:24:57,360
of these pages you can do that

00:24:55,039 --> 00:24:58,320
so the next one was storing data right

00:24:57,360 --> 00:25:01,679
so

00:24:58,320 --> 00:25:03,200
we buffer and flush um flush when the

00:25:01,679 --> 00:25:05,840
network comes back up

00:25:03,200 --> 00:25:06,880
so this will vary depending on a bunch

00:25:05,840 --> 00:25:08,799
of things

00:25:06,880 --> 00:25:10,400
um there are different strategies for

00:25:08,799 --> 00:25:13,520
how you might do this

00:25:10,400 --> 00:25:14,000
and then tag the device if it's a device

00:25:13,520 --> 00:25:17,039
that can get

00:25:14,000 --> 00:25:19,279
used um that you know maybe

00:25:17,039 --> 00:25:20,720
these devices are redundant in some way

00:25:19,279 --> 00:25:21,600
maybe there's another one something else

00:25:20,720 --> 00:25:25,760
you can use

00:25:21,600 --> 00:25:27,440
do it network faults

00:25:25,760 --> 00:25:29,360
container orchestration can help by

00:25:27,440 --> 00:25:30,480
recognizing network faults managing the

00:25:29,360 --> 00:25:33,679
device hardware

00:25:30,480 --> 00:25:35,200
bringing up an alternative network and

00:25:33,679 --> 00:25:37,919
allowing us to do things like change

00:25:35,200 --> 00:25:37,919
default routes

00:25:38,559 --> 00:25:42,000
the k3s allows us to container

00:25:41,440 --> 00:25:45,679
orchestration

00:25:42,000 --> 00:25:47,279
allows us to network i can

00:25:45,679 --> 00:25:48,880
drain and cord my nodes so that's where

00:25:47,279 --> 00:25:51,039
i can take the workloads from one

00:25:48,880 --> 00:25:52,480
move them over to another so if one

00:25:51,039 --> 00:25:54,400
device in the in a

00:25:52,480 --> 00:25:55,600
group of gateway devices lost

00:25:54,400 --> 00:25:58,480
connectivity

00:25:55,600 --> 00:25:59,520
board and drain it and and the device is

00:25:58,480 --> 00:26:01,760
operating

00:25:59,520 --> 00:26:03,279
and that's great everything's running

00:26:01,760 --> 00:26:05,200
let's talk about k3s

00:26:03,279 --> 00:26:07,360
uh practically you know the practical

00:26:05,200 --> 00:26:10,400
implications of k3s

00:26:07,360 --> 00:26:12,240
one of the things you can do uh is own

00:26:10,400 --> 00:26:14,240
your hardware and in the case of network

00:26:12,240 --> 00:26:17,360
faults that means possibly owning the

00:26:14,240 --> 00:26:19,279
network interface controller

00:26:17,360 --> 00:26:20,720
i have an example and i will share the

00:26:19,279 --> 00:26:22,240
url with you

00:26:20,720 --> 00:26:24,159
obviously we won't have time to to

00:26:22,240 --> 00:26:26,720
demonstrate it but um

00:26:24,159 --> 00:26:28,559
and you can use one network device to

00:26:26,720 --> 00:26:30,240
bootstrap the kubernetes system

00:26:28,559 --> 00:26:32,640
and then use a different network device

00:26:30,240 --> 00:26:34,159
to actually for communication and

00:26:32,640 --> 00:26:35,840
and so there are things you can do with

00:26:34,159 --> 00:26:38,320
kubernetes to actually

00:26:35,840 --> 00:26:40,080
run uh lower level parts of the

00:26:38,320 --> 00:26:42,320
operating system

00:26:40,080 --> 00:26:45,120
and devices that are connected there are

00:26:42,320 --> 00:26:46,960
other devices possible as well

00:26:45,120 --> 00:26:48,400
in my first attempt when i first started

00:26:46,960 --> 00:26:50,000
doing this stuff when i was first trying

00:26:48,400 --> 00:26:51,919
to solve some of these problems

00:26:50,000 --> 00:26:53,760
i thought oh you know i just i want to

00:26:51,919 --> 00:26:55,120
run system control right so let me let

00:26:53,760 --> 00:26:56,720
me just take over

00:26:55,120 --> 00:26:58,880
i want to just run the supervisors of

00:26:56,720 --> 00:27:01,760
the way the designs

00:26:58,880 --> 00:27:03,600
um and what i found was that to do that

00:27:01,760 --> 00:27:04,400
i had to give privileged access i had to

00:27:03,600 --> 00:27:06,880
give like

00:27:04,400 --> 00:27:07,919
serious access to this container that

00:27:06,880 --> 00:27:11,360
was trying to

00:27:07,919 --> 00:27:13,039
control the nick and honestly it was

00:27:11,360 --> 00:27:14,320
terrible right it's just it's too much

00:27:13,039 --> 00:27:15,120
control i didn't like the way i was

00:27:14,320 --> 00:27:17,679
doing it

00:27:15,120 --> 00:27:19,679
and i realized well you know what if i

00:27:17,679 --> 00:27:20,159
just take ownership from the supervisor

00:27:19,679 --> 00:27:22,640
i mean

00:27:20,159 --> 00:27:24,240
okay the supervisor is useful but

00:27:22,640 --> 00:27:26,159
actually i have this

00:27:24,240 --> 00:27:27,520
kubernetes like control plane that can

00:27:26,159 --> 00:27:29,600
do all sorts of stuff pretty much

00:27:27,520 --> 00:27:33,840
everything the supervisor can do

00:27:29,600 --> 00:27:34,640
so i can get scalability and reliability

00:27:33,840 --> 00:27:38,000
um

00:27:34,640 --> 00:27:39,760
of my process whatever that is and i can

00:27:38,000 --> 00:27:41,760
do dependency management which is one of

00:27:39,760 --> 00:27:43,760
the things your supervisor does

00:27:41,760 --> 00:27:45,360
so in things like the demon set that i

00:27:43,760 --> 00:27:46,960
showed right i can have something that

00:27:45,360 --> 00:27:48,000
runs everywhere and make sure it's

00:27:46,960 --> 00:27:50,640
always running

00:27:48,000 --> 00:27:51,919
i can use kubernetes jobs to run

00:27:50,640 --> 00:27:53,840
something once

00:27:51,919 --> 00:27:55,039
so i can prepare for something else that

00:27:53,840 --> 00:27:56,720
needs to run

00:27:55,039 --> 00:27:58,240
and use any containers i showed you a

00:27:56,720 --> 00:28:00,320
demonstration of

00:27:58,240 --> 00:28:03,600
use liveness and readiness probes to see

00:28:00,320 --> 00:28:06,880
if things are working as expected

00:28:03,600 --> 00:28:07,200
so container orchestration it needs to

00:28:06,880 --> 00:28:09,279
run

00:28:07,200 --> 00:28:12,640
it itself needs to run as a service so

00:28:09,279 --> 00:28:14,720
the supervisor is running my k3s

00:28:12,640 --> 00:28:17,279
my container orchestration and that

00:28:14,720 --> 00:28:18,480
means the k3s is always on right so if

00:28:17,279 --> 00:28:20,000
that goes down

00:28:18,480 --> 00:28:22,080
the supervisor will make sure my

00:28:20,000 --> 00:28:22,880
kubernetes is running and then i can use

00:28:22,080 --> 00:28:25,440
my coup

00:28:22,880 --> 00:28:28,720
to do things like manage my devices for

00:28:25,440 --> 00:28:30,960
network interface management

00:28:28,720 --> 00:28:33,039
the container orchestration of course so

00:28:30,960 --> 00:28:33,600
i said it runs as a consent supervisor

00:28:33,039 --> 00:28:36,080
yep

00:28:33,600 --> 00:28:37,600
oh the capabilities i can access all

00:28:36,080 --> 00:28:39,679
sorts of stuff in linux using

00:28:37,600 --> 00:28:41,120
uh kubernetes capabilities the process

00:28:39,679 --> 00:28:43,760
id um

00:28:41,120 --> 00:28:44,720
the the host network um get to the host

00:28:43,760 --> 00:28:47,440
processes

00:28:44,720 --> 00:28:48,720
there are a ton more do man capabilities

00:28:47,440 --> 00:28:50,960
in linux or

00:28:48,720 --> 00:28:52,480
look at kubernetes capabilities to find

00:28:50,960 --> 00:28:54,799
more out about that

00:28:52,480 --> 00:28:56,080
and with that i'm going to share my

00:28:54,799 --> 00:28:57,840
related projects

00:28:56,080 --> 00:28:59,919
the first project is the one we didn't

00:28:57,840 --> 00:29:02,240
demo today the turnkey project

00:28:59,919 --> 00:29:03,279
example of owning the network interface

00:29:02,240 --> 00:29:06,480
and the second one

00:29:03,279 --> 00:29:08,480
is um to watch for throttle bandwidth in

00:29:06,480 --> 00:29:12,159
that example we did see today

00:29:08,480 --> 00:29:13,919
um what's next up to you um

00:29:12,159 --> 00:29:15,679
define a controller we can do all of

00:29:13,919 --> 00:29:18,880
this through a control

00:29:15,679 --> 00:29:20,799
uh one-off demon set type thing um

00:29:18,880 --> 00:29:22,320
and take a look at linux capabilities

00:29:20,799 --> 00:29:25,919
thanks for joining me today

00:29:22,320 --> 00:29:26,799
and i think my time is up and um i'll

00:29:25,919 --> 00:29:41,840
hope to

00:29:26,799 --> 00:29:41,840
present to you again sometime soon

00:29:47,440 --> 00:29:52,720
hi um there's just uh there's just one

00:29:52,840 --> 00:29:56,240
question um

00:29:56,559 --> 00:30:00,640
great so um there there's one question

00:29:59,679 --> 00:30:02,399
in the q a

00:30:00,640 --> 00:30:03,600
um if you have any others um go ahead

00:30:02,399 --> 00:30:05,120
and put them in and we'll probably have

00:30:03,600 --> 00:30:08,640
to switch over to slack

00:30:05,120 --> 00:30:11,360
um the the question came up um

00:30:08,640 --> 00:30:11,840
is fleet some offering from rancher um

00:30:11,360 --> 00:30:13,760
fleet

00:30:11,840 --> 00:30:15,279
both refers to sort of a fleet of

00:30:13,760 --> 00:30:16,960
devices um

00:30:15,279 --> 00:30:18,960
first time i heard the fleet term was

00:30:16,960 --> 00:30:20,480
probably 20 years ago in terms of a

00:30:18,960 --> 00:30:23,360
fleet of trucks

00:30:20,480 --> 00:30:23,679
um where you know uh using credit cards

00:30:23,360 --> 00:30:25,919
they

00:30:23,679 --> 00:30:27,120
there they would manage the the fleet of

00:30:25,919 --> 00:30:29,200
trucks that could

00:30:27,120 --> 00:30:31,039
go to the gas station and what what they

00:30:29,200 --> 00:30:33,279
could access from the credit card

00:30:31,039 --> 00:30:34,799
in this case it's a fleet of devices it

00:30:33,279 --> 00:30:36,559
could be a fleet of

00:30:34,799 --> 00:30:38,080
the the devices may be a part of a

00:30:36,559 --> 00:30:41,039
cluster so there may be uh

00:30:38,080 --> 00:30:41,840
a single device maybe a cluster in its

00:30:41,039 --> 00:30:44,000
own right

00:30:41,840 --> 00:30:45,360
but that device may also be a part of a

00:30:44,000 --> 00:30:47,520
larger cluster

00:30:45,360 --> 00:30:48,960
uh it can't be both it either has to be

00:30:47,520 --> 00:30:50,240
its own cluster or part of a larger

00:30:48,960 --> 00:30:52,880
cluster

00:30:50,240 --> 00:30:54,080
and then there is a project from rancher

00:30:52,880 --> 00:30:57,200
which will be

00:30:54,080 --> 00:30:59,919
released uh in early october which

00:30:57,200 --> 00:31:01,919
um and actually the project is out in

00:30:59,919 --> 00:31:05,440
open source on github now

00:31:01,919 --> 00:31:06,000
and that is a fleet manager so i would

00:31:05,440 --> 00:31:10,320
manage

00:31:06,000 --> 00:31:12,480
managing multiple clusters

00:31:10,320 --> 00:31:14,640
and it's designed to manage tens of

00:31:12,480 --> 00:31:18,080
thousands of clusters

00:31:14,640 --> 00:31:19,360
in a git ops manner and i will be doing

00:31:18,080 --> 00:31:21,919
some demos on that in

00:31:19,360 --> 00:31:23,919
other talks at other sessions not at

00:31:21,919 --> 00:31:28,399
linux foundation this year but

00:31:23,919 --> 00:31:28,399
um in upcoming uh summits

00:31:31,039 --> 00:31:36,880
so um to keep the conversation going

00:31:34,399 --> 00:31:39,760
um please visit the number two cloud

00:31:36,880 --> 00:31:47,840
native networking on our slack workspace

00:31:39,760 --> 00:31:47,840
after the session ends

00:31:50,000 --> 00:32:01,840
thank you

00:32:15,120 --> 00:32:17,200

YouTube URL: https://www.youtube.com/watch?v=G8B7bwUwAL0


