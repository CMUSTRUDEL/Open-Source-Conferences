Title: LF Networking: "Self-Healing with Analytics for K8s Workloads"
Publication date: 2020-09-30
Playlist: Open Networking & Edge Summit 2020
Description: 
	LF Networking: "Self-Healing with Analytics for K8s Workloads"
Captions: 
	00:00:01,040 --> 00:00:05,120
in the accompanying video each of the

00:00:03,280 --> 00:00:07,279
closed loop resiliency components and

00:00:05,120 --> 00:00:08,960
their roles in this demo were discussed

00:00:07,279 --> 00:00:11,280
to view that video please follow the

00:00:08,960 --> 00:00:13,200
link provided in this video

00:00:11,280 --> 00:00:17,920
we will combine those components to

00:00:13,200 --> 00:00:17,920
realize a platform resiliency use case

00:00:19,039 --> 00:00:22,880
the last level cache load misses a

00:00:21,039 --> 00:00:25,199
memory bandwidth usage of each node is

00:00:22,880 --> 00:00:26,400
monitored using the intel pmu and intel

00:00:25,199 --> 00:00:28,880
rdt plugins

00:00:26,400 --> 00:00:30,720
enabled in the collectd operator these

00:00:28,880 --> 00:00:32,800
metrics are then stored in prometheus

00:00:30,720 --> 00:00:34,800
and made available to tas via the custom

00:00:32,800 --> 00:00:37,360
metrics api

00:00:34,800 --> 00:00:38,640
for the demo scenario a sample nginx

00:00:37,360 --> 00:00:40,399
deployment is created

00:00:38,640 --> 00:00:41,760
and linked to tas with the resiliency

00:00:40,399 --> 00:00:43,440
task policy

00:00:41,760 --> 00:00:45,440
tas will only schedule pods from this

00:00:43,440 --> 00:00:47,520
deployment

00:00:45,440 --> 00:00:49,680
memory bandwidth usage and llc load

00:00:47,520 --> 00:00:51,520
misses are used to indicate node health

00:00:49,680 --> 00:00:53,680
as deploying a latency sense of

00:00:51,520 --> 00:00:55,199
application on a node with high levels

00:00:53,680 --> 00:00:57,760
of llc misses

00:00:55,199 --> 00:00:58,879
and memory bandwidth usage can result in

00:00:57,760 --> 00:01:02,000
significant jitter

00:00:58,879 --> 00:01:02,000
and latency increases

00:01:02,320 --> 00:01:05,760
the resiliency task policy defines three

00:01:04,400 --> 00:01:07,200
states of note health

00:01:05,760 --> 00:01:09,040
and associated scheduling and

00:01:07,200 --> 00:01:10,880
de-scheduling rules

00:01:09,040 --> 00:01:13,119
the healthy state where the node is

00:01:10,880 --> 00:01:14,799
healthy and pods can be scheduled

00:01:13,119 --> 00:01:16,640
the warning state where the node is

00:01:14,799 --> 00:01:17,280
unhealthy and no more parts will be

00:01:16,640 --> 00:01:19,119
scheduled

00:01:17,280 --> 00:01:21,360
but the existing parts will remain on

00:01:19,119 --> 00:01:23,360
the node and the critical state

00:01:21,360 --> 00:01:25,200
where the node health is critical and

00:01:23,360 --> 00:01:26,080
existing pods will be rescheduled on a

00:01:25,200 --> 00:01:27,680
healthy node

00:01:26,080 --> 00:01:30,159
and no more pods will be scheduled on

00:01:27,680 --> 00:01:30,159
this node

00:01:30,400 --> 00:01:34,640
firstly with both nodes in a healthy

00:01:32,320 --> 00:01:37,759
state one engine x pod is created and

00:01:34,640 --> 00:01:37,759
deployed to node two

00:01:39,759 --> 00:01:43,200
stress ng is then deployed to node 2 to

00:01:41,920 --> 00:01:45,680
stress the cpu

00:01:43,200 --> 00:01:47,600
this increases the llc load misses and

00:01:45,680 --> 00:01:49,920
memory bandwidth usage

00:01:47,600 --> 00:01:51,680
with stress and destressing node 2 the

00:01:49,920 --> 00:01:53,600
llc load misses and memory bandwidth

00:01:51,680 --> 00:01:54,000
usage increased to put the node into a

00:01:53,600 --> 00:01:56,399
warning

00:01:54,000 --> 00:01:56,399
state

00:01:59,680 --> 00:02:03,680
then when the deployment is increased

00:02:01,280 --> 00:02:04,799
three pods tas recognizes the warning

00:02:03,680 --> 00:02:06,240
state of node two

00:02:04,799 --> 00:02:08,239
and schedules the new pods on the

00:02:06,240 --> 00:02:10,640
healthy node one

00:02:08,239 --> 00:02:14,000
the first part remains on node 2 as per

00:02:10,640 --> 00:02:14,000
the resiliency task policy

00:02:15,760 --> 00:02:23,440
stress ngs then increase the trigger

00:02:17,599 --> 00:02:26,000
critical state in node 2.

00:02:23,440 --> 00:02:27,680
as per the resiliency task policy pods

00:02:26,000 --> 00:02:28,480
on a node in a critical state will be

00:02:27,680 --> 00:02:30,959
descheduled

00:02:28,480 --> 00:02:31,599
and redeployed on a healthy node in this

00:02:30,959 --> 00:02:34,000
case

00:02:31,599 --> 00:02:36,400
the first pod will be descheduled and

00:02:34,000 --> 00:02:38,800
redeployed on a healthy node one

00:02:36,400 --> 00:02:40,080
and now to see the demo in action on the

00:02:38,800 --> 00:02:41,920
left hand side of the screen

00:02:40,080 --> 00:02:44,560
there is the memory bandwidth usage and

00:02:41,920 --> 00:02:46,239
llc load misses for node 1 and node 2

00:02:44,560 --> 00:02:48,560
with the warning threshold in yellow and

00:02:46,239 --> 00:02:50,959
the critical threshold in red

00:02:48,560 --> 00:02:52,480
on the top right the engine x pods and

00:02:50,959 --> 00:02:53,440
what node they are deployed to are

00:02:52,480 --> 00:02:55,440
listed

00:02:53,440 --> 00:02:56,959
and on the bottom right the tiles logs

00:02:55,440 --> 00:03:00,239
are shown detailing the current

00:02:56,959 --> 00:03:02,400
scheduling decisions being made

00:03:00,239 --> 00:03:19,680
firstly one engine xpod is deployed to

00:03:02,400 --> 00:03:22,239
node 2 scheduled by taz

00:03:19,680 --> 00:03:24,080
stress ng is started on node 2. this

00:03:22,239 --> 00:03:26,080
increases the llc load misses

00:03:24,080 --> 00:03:33,840
and memory bandwidth usage to the

00:03:26,080 --> 00:03:33,840
warning state

00:03:36,640 --> 00:03:40,799
two more pods are then created and

00:03:39,360 --> 00:03:43,040
scheduled to node one

00:03:40,799 --> 00:03:47,840
as node two is in a warning state the

00:03:43,040 --> 00:03:47,840
first part remains on node two

00:04:02,159 --> 00:04:15,840
stress ng is increased to trigger a

00:04:04,239 --> 00:04:15,840
critical state in node two

00:04:22,320 --> 00:04:25,440
taz then recognizes this critical state

00:04:24,400 --> 00:04:28,000
in node two

00:04:25,440 --> 00:04:29,919
d schedules the engine x part on node 2

00:04:28,000 --> 00:04:43,840
and a new part it is deployed to the

00:04:29,919 --> 00:04:43,840
healthy node 1.

00:04:46,160 --> 00:04:50,560
this demo showcases how components from

00:04:48,240 --> 00:04:52,400
infowatch combined with host telemetry

00:04:50,560 --> 00:04:54,720
enable a zero touch automated and

00:04:52,400 --> 00:04:56,639
resilient network infrastructure

00:04:54,720 --> 00:04:58,160
collecti can also be deployed using the

00:04:56,639 --> 00:04:59,759
same tooling in osp

00:04:58,160 --> 00:05:01,919
to enable host telemetry within the

00:04:59,759 --> 00:05:03,680
service telemetry framework

00:05:01,919 --> 00:05:06,080
combining these elements into a closed

00:05:03,680 --> 00:05:07,680
loop solution provides a starting point

00:05:06,080 --> 00:05:08,479
for tomorrow's next generation

00:05:07,680 --> 00:05:22,880
self-healing

00:05:08,479 --> 00:05:22,880

YouTube URL: https://www.youtube.com/watch?v=u01hV1ZXcW0


