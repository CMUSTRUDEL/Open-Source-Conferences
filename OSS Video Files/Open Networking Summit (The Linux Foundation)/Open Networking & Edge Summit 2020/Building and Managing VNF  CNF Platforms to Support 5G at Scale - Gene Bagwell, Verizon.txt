Title: Building and Managing VNF  CNF Platforms to Support 5G at Scale - Gene Bagwell, Verizon
Publication date: 2020-10-28
Playlist: Open Networking & Edge Summit 2020
Description: 
	Building and Managing VNF/ CNF Platforms to Support 5G at Scale - Gene Bagwell, Verizon
Captions: 
	00:00:01,839 --> 00:00:05,600
hello my name is gene baden and i'm an

00:00:04,160 --> 00:00:07,200
associate fellow with verizon's

00:00:05,600 --> 00:00:08,639
technology architecture planning and

00:00:07,200 --> 00:00:10,240
strategy group

00:00:08,639 --> 00:00:12,480
i'm here today to talk about building

00:00:10,240 --> 00:00:17,279
and managing enfs and cms to support

00:00:12,480 --> 00:00:19,279
5g at scale before we get started

00:00:17,279 --> 00:00:21,760
just a bit of background the verizon

00:00:19,279 --> 00:00:23,279
cloud platform vcp is a telco cloud

00:00:21,760 --> 00:00:24,880
designed to support network function

00:00:23,279 --> 00:00:26,320
virtualization across all of our

00:00:24,880 --> 00:00:28,400
business units

00:00:26,320 --> 00:00:29,359
as such we do not host standard i.t

00:00:28,400 --> 00:00:31,119
workloads

00:00:29,359 --> 00:00:33,920
we have a separate cloud system for

00:00:31,119 --> 00:00:33,920
those workloads

00:00:34,000 --> 00:00:38,160
pcp is a global platform deployed in 94

00:00:36,399 --> 00:00:39,280
locations around the world

00:00:38,160 --> 00:00:41,200
the majority of sites are in the

00:00:39,280 --> 00:00:44,719
continental us supporting our 4g

00:00:41,200 --> 00:00:46,559
and 5g services

00:00:44,719 --> 00:00:48,079
bcp is divided into three service

00:00:46,559 --> 00:00:50,559
offerings core

00:00:48,079 --> 00:00:52,480
edge and far edge core handles our

00:00:50,559 --> 00:00:53,120
common network functions such as ins and

00:00:52,480 --> 00:00:56,079
bolting

00:00:53,120 --> 00:00:58,000
edge handles 4g and 5g epc elements in

00:00:56,079 --> 00:00:59,840
common grand functions

00:00:58,000 --> 00:01:01,680
far edge is the virtualization of the

00:00:59,840 --> 00:01:02,640
rand infrastructure of the cell towers

00:01:01,680 --> 00:01:04,400
and this is what we're going to be

00:01:02,640 --> 00:01:06,320
talking about today

00:01:04,400 --> 00:01:08,240
we are predominantly a red hat shop and

00:01:06,320 --> 00:01:10,479
as such use red hat openstack and

00:01:08,240 --> 00:01:12,159
openshift to support our vnf and cnf

00:01:10,479 --> 00:01:13,439
environments in their core and edge

00:01:12,159 --> 00:01:15,040
locations

00:01:13,439 --> 00:01:16,560
for far edge we need to meet very

00:01:15,040 --> 00:01:18,240
stringent latency requirements for

00:01:16,560 --> 00:01:20,320
transport and data processing

00:01:18,240 --> 00:01:22,000
which i'll touch on more in a moment but

00:01:20,320 --> 00:01:23,439
this led to the selection of a second

00:01:22,000 --> 00:01:26,560
vendor for our far edge

00:01:23,439 --> 00:01:28,080
paths and cavs

00:01:26,560 --> 00:01:29,759
the core of the network is located in

00:01:28,080 --> 00:01:30,240
the network equipment centers or main

00:01:29,759 --> 00:01:32,320
seats

00:01:30,240 --> 00:01:33,280
this is where our vcp core platforms are

00:01:32,320 --> 00:01:36,799
deployed

00:01:33,280 --> 00:01:39,680
vcp core hosts our 4g ims sdn and volte

00:01:36,799 --> 00:01:41,600
vnfs and the new 5g core ns

00:01:39,680 --> 00:01:42,960
moving out from the nec are the service

00:01:41,600 --> 00:01:44,880
access points or saps

00:01:42,960 --> 00:01:46,079
this is where the vcp edge platform is

00:01:44,880 --> 00:01:49,200
deployed

00:01:46,079 --> 00:01:50,320
vcp edge hosts vnms for 4g epc functions

00:01:49,200 --> 00:01:54,000
such as p gateway

00:01:50,320 --> 00:01:56,560
s gateway mme spc and mrf

00:01:54,000 --> 00:01:57,600
for 5g the sat sites will host the upf

00:01:56,560 --> 00:01:59,759
and smf

00:01:57,600 --> 00:02:02,159
nfs as well as the 5g home fixed

00:01:59,759 --> 00:02:03,680
wireless access platform

00:02:02,159 --> 00:02:05,920
the next hop towards the edge of the

00:02:03,680 --> 00:02:07,360
network are the telco aggregation points

00:02:05,920 --> 00:02:08,879
or tabs

00:02:07,360 --> 00:02:10,640
these are the aggregation points for

00:02:08,879 --> 00:02:13,120
transport between the cell sites at the

00:02:10,640 --> 00:02:15,599
saps which is also known as backhaul

00:02:13,120 --> 00:02:16,879
if latency is a concern we can deploy 4g

00:02:15,599 --> 00:02:18,480
and 5g epc

00:02:16,879 --> 00:02:21,520
elements running on the bcp edge

00:02:18,480 --> 00:02:23,599
infrastructure in these locations

00:02:21,520 --> 00:02:25,280
finally the c-ran and d-ran sites

00:02:23,599 --> 00:02:26,959
equipment in these sites controls the

00:02:25,280 --> 00:02:29,520
antennas and the conversion of radio

00:02:26,959 --> 00:02:31,519
access protocol to tcp

00:02:29,520 --> 00:02:33,360
d-rand sites serve urban or rural

00:02:31,519 --> 00:02:35,040
locations and are typically deployed at

00:02:33,360 --> 00:02:37,360
the base of the cell tower

00:02:35,040 --> 00:02:38,160
c-ran sites are found in metropolitan

00:02:37,360 --> 00:02:41,200
areas that control

00:02:38,160 --> 00:02:42,800
multiple towers cran at dram sites

00:02:41,200 --> 00:02:45,200
houses three components

00:02:42,800 --> 00:02:47,200
the radio unit or radio heads sometimes

00:02:45,200 --> 00:02:49,840
called the ru or geno b

00:02:47,200 --> 00:02:50,959
the baseband unit or bbu this is the

00:02:49,840 --> 00:02:53,680
control unit

00:02:50,959 --> 00:02:55,760
which has two parts the data unit or du

00:02:53,680 --> 00:02:58,800
and the control unit or cu

00:02:55,760 --> 00:03:00,560
and finally the cell cell router or csr

00:02:58,800 --> 00:03:02,879
which provides ethernet switching within

00:03:00,560 --> 00:03:06,800
the c-ram and d-ran sites and also layer

00:03:02,879 --> 00:03:06,800
3 for back calling to the tap sites

00:03:10,000 --> 00:03:14,159
our 5g infrastructure is fully

00:03:11,840 --> 00:03:16,560
virtualized this includes virtualization

00:03:14,159 --> 00:03:18,560
of the ram baseband unit or bdu

00:03:16,560 --> 00:03:20,879
as previously stated the dbu has two

00:03:18,560 --> 00:03:22,720
components the data unit or du

00:03:20,879 --> 00:03:24,000
which handles the user point functions

00:03:22,720 --> 00:03:26,400
of the bbu

00:03:24,000 --> 00:03:29,120
and the control unit or cu which handles

00:03:26,400 --> 00:03:31,840
the control point functions of the dbu

00:03:29,120 --> 00:03:33,360
cues are deployed as dnfs on vcp edge

00:03:31,840 --> 00:03:36,159
and sap and tap sites

00:03:33,360 --> 00:03:40,400
while du's are deployed as cnfs on vcp

00:03:36,159 --> 00:03:40,400
far edge and cran and drain sites

00:03:40,879 --> 00:03:44,239
in a traditional ram architecture there

00:03:42,720 --> 00:03:45,680
is a one to one relationship between the

00:03:44,239 --> 00:03:47,120
cu and the du

00:03:45,680 --> 00:03:48,720
this one-to-one relationship is

00:03:47,120 --> 00:03:50,239
inefficient with ig

00:03:48,720 --> 00:03:52,239
where the cell size do the higher

00:03:50,239 --> 00:03:54,159
frequencies used is smaller

00:03:52,239 --> 00:03:56,400
smaller cell size the more cells that

00:03:54,159 --> 00:03:58,080
are needed to cover a different area

00:03:56,400 --> 00:03:59,760
the increased number of cell sites

00:03:58,080 --> 00:04:01,760
results in more handoffs between the

00:03:59,760 --> 00:04:03,840
dbus which is less efficient and can

00:04:01,760 --> 00:04:06,159
impact customer experience

00:04:03,840 --> 00:04:06,959
separating the du and cu functions into

00:04:06,159 --> 00:04:09,439
virtual

00:04:06,959 --> 00:04:12,000
data units or vdu and virtual control

00:04:09,439 --> 00:04:13,840
units or vcus allows us to create

00:04:12,000 --> 00:04:15,920
a one-to-many relationship between an

00:04:13,840 --> 00:04:18,000
ecu and many vdus

00:04:15,920 --> 00:04:19,120
this is known as a c-u-d-u split

00:04:18,000 --> 00:04:20,959
architecture

00:04:19,120 --> 00:04:23,040
the split architecture allows for many

00:04:20,959 --> 00:04:24,800
cell-to-cell handoffs to be handled by

00:04:23,040 --> 00:04:26,560
intra-genome v-mobility

00:04:24,800 --> 00:04:29,840
improving network efficiency and the

00:04:26,560 --> 00:04:29,840
overall user experience

00:04:34,160 --> 00:04:37,360
this is a typical drain site this is a

00:04:36,639 --> 00:04:39,360
class 2

00:04:37,360 --> 00:04:40,800
telco environment that means all

00:04:39,360 --> 00:04:42,320
equipment installed must be hardened to

00:04:40,800 --> 00:04:43,759
withstand significant temperature and

00:04:42,320 --> 00:04:45,360
humidity studies

00:04:43,759 --> 00:04:48,240
it also means that all the hardware in

00:04:45,360 --> 00:04:49,759
the site is powered by 48 volt dc

00:04:48,240 --> 00:04:52,240
the tan color cabinets in the middle

00:04:49,759 --> 00:04:54,240
house the bpu and the csr

00:04:52,240 --> 00:04:56,240
cabinets contain dust filters and fans

00:04:54,240 --> 00:05:00,000
for air movement that in most cases do

00:04:56,240 --> 00:05:00,000
not provide air conditioning or heating

00:05:00,240 --> 00:05:04,720
this is a typical c-ran site this is a

00:05:02,560 --> 00:05:07,440
class 1 telco environment

00:05:04,720 --> 00:05:08,639
class 1 sites can be ac or dc powered

00:05:07,440 --> 00:05:12,400
and they provide heating

00:05:08,639 --> 00:05:13,919
air conditioning and humidity control

00:05:12,400 --> 00:05:15,440
let's take a look at the hardware needed

00:05:13,919 --> 00:05:17,919
to support 5g and a c

00:05:15,440 --> 00:05:20,320
random unit at the bottom of the page

00:05:17,919 --> 00:05:22,000
are the remote radio units or rrnus

00:05:20,320 --> 00:05:23,680
which are sometimes referred to as radio

00:05:22,000 --> 00:05:25,919
heads or radio units

00:05:23,680 --> 00:05:28,080
for 4g the argues connect directly to

00:05:25,919 --> 00:05:30,240
the bpu's at the top of the page

00:05:28,080 --> 00:05:32,160
in 5g the connections to the ru's are

00:05:30,240 --> 00:05:33,120
relocated to the front wall switch unit

00:05:32,160 --> 00:05:36,080
or fsu

00:05:33,120 --> 00:05:38,160
we'll see why in a moment fsu's are also

00:05:36,080 --> 00:05:39,840
called front hall gateways

00:05:38,160 --> 00:05:41,759
connections from the rnu's to either the

00:05:39,840 --> 00:05:44,080
bbu or the fsu

00:05:41,759 --> 00:05:46,000
are referred to as front hall front hall

00:05:44,080 --> 00:05:48,320
connections are shown in yellow

00:05:46,000 --> 00:05:51,120
next is the vcp far edge worker node

00:05:48,320 --> 00:05:53,360
which connects to the fsu via mid hall

00:05:51,120 --> 00:05:54,880
mid-hall connections are shown in blue

00:05:53,360 --> 00:05:58,319
and finally the csr

00:05:54,880 --> 00:06:00,400
the csr provides l2 l3 connectivity

00:05:58,319 --> 00:06:02,240
in the cell site and l3 connectivity

00:06:00,400 --> 00:06:04,479
back to the tap it's at

00:06:02,240 --> 00:06:06,720
connections from the pvu or vdu back to

00:06:04,479 --> 00:06:10,560
the tap and sat for known as backhaul

00:06:06,720 --> 00:06:10,560
backhaul connections are shown in green

00:06:10,880 --> 00:06:15,120
these are three of the radio heads used

00:06:12,639 --> 00:06:16,720
in the network r use connect via fiber

00:06:15,120 --> 00:06:19,039
to the bbu

00:06:16,720 --> 00:06:20,880
in 4g the common public radio interface

00:06:19,039 --> 00:06:22,479
recipe is used for signaling between the

00:06:20,880 --> 00:06:24,240
ru and the bdu

00:06:22,479 --> 00:06:26,319
even though simply is a standard space

00:06:24,240 --> 00:06:27,520
protocol each vendor's implementation is

00:06:26,319 --> 00:06:29,759
for graduate

00:06:27,520 --> 00:06:32,160
in 5g front hall signaling is handled by

00:06:29,759 --> 00:06:34,479
an ant simpry or esibri

00:06:32,160 --> 00:06:36,319
the e-super specification enables radio

00:06:34,479 --> 00:06:38,080
data transmission over packet-based

00:06:36,319 --> 00:06:39,199
transport networks such as ip over

00:06:38,080 --> 00:06:40,960
ethernet

00:06:39,199 --> 00:06:42,400
this is the first step in adopting an

00:06:40,960 --> 00:06:44,160
open communication standard for

00:06:42,400 --> 00:06:46,960
front-wheel signaling as defined by the

00:06:44,160 --> 00:06:50,479
oran project

00:06:46,960 --> 00:06:52,560
in 4g 700 megahertz 850 megahertz and

00:06:50,479 --> 00:06:55,919
the aws plus pcs radio

00:06:52,560 --> 00:06:58,080
ads are cable directly to the pvu for 5g

00:06:55,919 --> 00:07:00,479
we need to split out the 850 megahertz

00:06:58,080 --> 00:07:03,680
spectrum and send it to the vdu

00:07:00,479 --> 00:07:05,520
this is the role of the fsu during the

00:07:03,680 --> 00:07:07,360
installation of the fsu the connections

00:07:05,520 --> 00:07:09,680
from the radio heads are relocated from

00:07:07,360 --> 00:07:12,160
the dpu to the fsu

00:07:09,680 --> 00:07:13,440
new connections from the fsu to the bpu

00:07:12,160 --> 00:07:16,880
are installed

00:07:13,440 --> 00:07:18,720
the fsu then acts as a splitter the 850

00:07:16,880 --> 00:07:20,800
megahertz spectrum from the radio heads

00:07:18,720 --> 00:07:22,000
is split out and sent to the ddu for

00:07:20,800 --> 00:07:25,039
processing

00:07:22,000 --> 00:07:26,880
the 700 megahertz and aws plus pcs

00:07:25,039 --> 00:07:30,960
spectrum pass through the fsu and are

00:07:26,880 --> 00:07:32,639
processed by the legacy dvu

00:07:30,960 --> 00:07:34,000
the requirements for vran timing and

00:07:32,639 --> 00:07:35,759
sync are dependent on the radio

00:07:34,000 --> 00:07:36,720
technologies deployed in the spectrum

00:07:35,759 --> 00:07:38,880
used

00:07:36,720 --> 00:07:40,400
in frequency divided spectrum such as 7

00:07:38,880 --> 00:07:42,560
and 850 megahertz

00:07:40,400 --> 00:07:45,120
a relatively low synchronization of 3 to

00:07:42,560 --> 00:07:46,879
5 microseconds is required

00:07:45,120 --> 00:07:49,039
time division duplex spectrum such as

00:07:46,879 --> 00:07:51,199
cprs and millimeter wave require tiger

00:07:49,039 --> 00:07:52,639
time and phase synchronization

00:07:51,199 --> 00:07:54,400
the higher precision is needed to

00:07:52,639 --> 00:07:55,680
prevent interference between a blink and

00:07:54,400 --> 00:07:57,440
downward signaling

00:07:55,680 --> 00:07:59,599
loss of service during a cell to settle

00:07:57,440 --> 00:08:02,160
handoff and for correct scheduling of

00:07:59,599 --> 00:08:04,080
software tasks of the bdu

00:08:02,160 --> 00:08:05,280
with timing requirements of 1.5

00:08:04,080 --> 00:08:09,919
microseconds or less

00:08:05,280 --> 00:08:13,280
ieee 1588 b2 ptp timing is required

00:08:09,919 --> 00:08:14,800
the fsu bbq and csr all receive timing

00:08:13,280 --> 00:08:17,039
signals from the global navigation

00:08:14,800 --> 00:08:20,080
satellite system or gps

00:08:17,039 --> 00:08:21,919
the fsu acting as a ptp grand master

00:08:20,080 --> 00:08:23,759
provides time sync to all components in

00:08:21,919 --> 00:08:25,520
the cran or dram site

00:08:23,759 --> 00:08:27,360
where they're locked to gps or in

00:08:25,520 --> 00:08:28,479
holdover mode when satellite signal is

00:08:27,360 --> 00:08:30,560
lost

00:08:28,479 --> 00:08:32,000
during holdover the fsu's built-in

00:08:30,560 --> 00:08:36,000
oscillator can provide

00:08:32,000 --> 00:08:36,000
stable clock signals for up to 10 hours

00:08:37,039 --> 00:08:40,640
the next component is the far edge

00:08:38,560 --> 00:08:42,159
compute node or worker node

00:08:40,640 --> 00:08:43,839
we were not able to locate cots

00:08:42,159 --> 00:08:45,519
platforms that could function in a class

00:08:43,839 --> 00:08:47,040
2 environment and still comply with our

00:08:45,519 --> 00:08:48,399
net's requirements

00:08:47,040 --> 00:08:49,839
to meet these requirements we had to

00:08:48,399 --> 00:08:52,160
work with our hardware suppliers to

00:08:49,839 --> 00:08:53,920
develop new server platforms

00:08:52,160 --> 00:08:55,600
what we ultimately wound up with is a

00:08:53,920 --> 00:08:58,399
2ru chassis that supports

00:08:55,600 --> 00:08:59,279
up to 2 blades or sleds each blade is

00:08:58,399 --> 00:09:01,600
self-contained

00:08:59,279 --> 00:09:03,279
including a single socket cpu memory

00:09:01,600 --> 00:09:04,880
storage and networking

00:09:03,279 --> 00:09:06,800
the chassis provides redundant power

00:09:04,880 --> 00:09:09,440
supplies and both slits can be run off a

00:09:06,800 --> 00:09:11,440
single power supply if one should fail

00:09:09,440 --> 00:09:12,880
for the initial rollout a chassis and a

00:09:11,440 --> 00:09:15,600
single slide are being deployed to

00:09:12,880 --> 00:09:17,200
support the 850 megahertz spectrum

00:09:15,600 --> 00:09:19,279
additional blades and chassis will be

00:09:17,200 --> 00:09:23,120
deployed later to support cprs

00:09:19,279 --> 00:09:25,120
and millimeter wave spectrum

00:09:23,120 --> 00:09:27,200
in vram package from the rmu must be

00:09:25,120 --> 00:09:28,800
processed by the bdu based on a very

00:09:27,200 --> 00:09:30,720
tight latency model

00:09:28,800 --> 00:09:32,080
consistent packet processing within a

00:09:30,720 --> 00:09:33,440
fixed latency window

00:09:32,080 --> 00:09:36,640
requires the use of a real-time

00:09:33,440 --> 00:09:37,839
operating system or rtls by separating

00:09:36,640 --> 00:09:39,519
application functions into

00:09:37,839 --> 00:09:41,519
self-contained tasks and implementing

00:09:39,519 --> 00:09:43,920
on-demand scheduling of those tasks

00:09:41,519 --> 00:09:45,040
the rtos can guarantee task execution

00:09:43,920 --> 00:09:46,720
time

00:09:45,040 --> 00:09:48,560
initially we were unable to locate a

00:09:46,720 --> 00:09:50,000
supplier supporting an rtos and a

00:09:48,560 --> 00:09:52,080
container-based virtualization

00:09:50,000 --> 00:09:54,000
platform this meant partnering with a

00:09:52,080 --> 00:09:56,800
supplier willing to develop support and

00:09:54,000 --> 00:09:58,720
maintain a pass based on an rtls

00:09:56,800 --> 00:10:00,800
for us that partner was wind river and

00:09:58,720 --> 00:10:02,880
the wind river cloud platform

00:10:00,800 --> 00:10:04,320
however the general purpose intel xeon

00:10:02,880 --> 00:10:06,399
processor processors used in the worker

00:10:04,320 --> 00:10:08,959
notes were unable to keep pace with the

00:10:06,399 --> 00:10:10,640
packet rate this meant that some form of

00:10:08,959 --> 00:10:13,120
hardware acceleration was going to be

00:10:10,640 --> 00:10:13,120
required

00:10:13,440 --> 00:10:18,079
vran l1 to l3 packet processing requires

00:10:16,320 --> 00:10:19,760
many floating point operations to

00:10:18,079 --> 00:10:21,200
implement features such as excessive

00:10:19,760 --> 00:10:24,480
interface cancellation

00:10:21,200 --> 00:10:26,959
in my load or forward error correction

00:10:24,480 --> 00:10:28,640
the general purpose intel xeon sp cpus

00:10:26,959 --> 00:10:30,720
in the far end worker notes

00:10:28,640 --> 00:10:32,000
are by themselves not able to keep up

00:10:30,720 --> 00:10:34,880
with the number of floating point

00:10:32,000 --> 00:10:37,200
operations required to support v-ray an

00:10:34,880 --> 00:10:39,040
intel fpga in every worker node provides

00:10:37,200 --> 00:10:42,160
the additional compute

00:10:39,040 --> 00:10:44,240
power needed to support vram however it

00:10:42,160 --> 00:10:46,240
presents another problem

00:10:44,240 --> 00:10:48,480
each vdu vendor implements their own

00:10:46,240 --> 00:10:51,440
methodology for offloading signal data

00:10:48,480 --> 00:10:53,440
processing operations to the fpga

00:10:51,440 --> 00:10:55,760
the vendor code for the fpga is called

00:10:53,440 --> 00:10:59,120
an acceleration function

00:10:55,760 --> 00:11:00,000
unit or afu right now the installation

00:10:59,120 --> 00:11:03,279
of the aoe

00:11:00,000 --> 00:11:06,079
requires flashing the fpga a task that

00:11:03,279 --> 00:11:08,240
can take up to 40 minutes complete

00:11:06,079 --> 00:11:10,720
even with automation this severely

00:11:08,240 --> 00:11:12,800
impacts deployment velocity and edu

00:11:10,720 --> 00:11:15,200
lifecycle management

00:11:12,800 --> 00:11:16,720
future intel cpus will include new

00:11:15,200 --> 00:11:20,000
instruction sets

00:11:16,720 --> 00:11:21,279
such as adx512 better caching and micro

00:11:20,000 --> 00:11:23,120
architectures

00:11:21,279 --> 00:11:24,560
that will deliver more instructions per

00:11:23,120 --> 00:11:26,240
cycle

00:11:24,560 --> 00:11:28,000
the improvement may allow for the

00:11:26,240 --> 00:11:30,480
removal of the fpga

00:11:28,000 --> 00:11:33,200
or the replacement within with an easier

00:11:30,480 --> 00:11:36,000
to manage gpu by the time we deploy

00:11:33,200 --> 00:11:36,000
other sphenes

00:11:38,959 --> 00:11:42,880
vcp far edge which hosts the vram

00:11:40,959 --> 00:11:44,800
applications is a distributed system

00:11:42,880 --> 00:11:46,240
with centralized management

00:11:44,800 --> 00:11:48,560
the centralized management system

00:11:46,240 --> 00:11:50,399
supports inventory pass provisioning on

00:11:48,560 --> 00:11:52,399
local and remote worker nodes

00:11:50,399 --> 00:11:53,760
installation of the cads and deployment

00:11:52,399 --> 00:11:55,040
of workloads

00:11:53,760 --> 00:11:56,880
controller nodes are deployed as

00:11:55,040 --> 00:11:58,399
clusters in this app sites the current

00:11:56,880 --> 00:12:00,240
version of the centralized controller

00:11:58,399 --> 00:12:02,000
platform can support approximately 200

00:12:00,240 --> 00:12:03,920
worker nodes per cluster

00:12:02,000 --> 00:12:06,000
depending on the number of cell sites in

00:12:03,920 --> 00:12:08,959
the geographic area asap may host

00:12:06,000 --> 00:12:10,959
multiple pairs of control clusters we

00:12:08,959 --> 00:12:12,720
estimate that between 200 and 300

00:12:10,959 --> 00:12:14,000
control clusters will be needed to cover

00:12:12,720 --> 00:12:15,920
the network

00:12:14,000 --> 00:12:18,240
an automated provisioning system is used

00:12:15,920 --> 00:12:20,079
to deploy and configure the controllers

00:12:18,240 --> 00:12:21,440
ip addresses of the bmc's are

00:12:20,079 --> 00:12:23,279
pre-configured during physical

00:12:21,440 --> 00:12:25,120
installation of the hardware

00:12:23,279 --> 00:12:26,959
and an automated process periodically

00:12:25,120 --> 00:12:28,720
pulls the ip addresses assigned to the

00:12:26,959 --> 00:12:31,279
controller vmcs

00:12:28,720 --> 00:12:33,839
when the bnc responds to a poll the

00:12:31,279 --> 00:12:35,680
controller is queued for installation

00:12:33,839 --> 00:12:37,839
during the first phase of installation

00:12:35,680 --> 00:12:39,680
redfish is used to validate the bnc and

00:12:37,839 --> 00:12:41,200
bios firmware versions

00:12:39,680 --> 00:12:43,920
if an upgrade is required that is

00:12:41,200 --> 00:12:45,440
handled by redfish

00:12:43,920 --> 00:12:47,120
once the firmware versions are up to

00:12:45,440 --> 00:12:48,959
date redfish is used to configure the

00:12:47,120 --> 00:12:50,399
bmc and the bios

00:12:48,959 --> 00:12:52,320
pass installation begins with the

00:12:50,399 --> 00:12:54,000
building of a custom iso image

00:12:52,320 --> 00:12:55,519
this image contains site-specific

00:12:54,000 --> 00:12:57,040
information for the controller such as

00:12:55,519 --> 00:12:58,480
the host name and interface ip

00:12:57,040 --> 00:13:00,240
addressing

00:12:58,480 --> 00:13:02,320
redfish is used to mount the controller

00:13:00,240 --> 00:13:04,880
specific iso via bmc

00:13:02,320 --> 00:13:07,040
virtual media and kick off the install

00:13:04,880 --> 00:13:08,399
once the os is installed

00:13:07,040 --> 00:13:10,320
ansible is used to complete the

00:13:08,399 --> 00:13:12,079
configuration of the controller

00:13:10,320 --> 00:13:14,160
same process is used for the second

00:13:12,079 --> 00:13:16,560
troller controller in the cluster sites

00:13:14,160 --> 00:13:18,480
and the worker notes

00:13:16,560 --> 00:13:19,680
to cover 200 million points of presence

00:13:18,480 --> 00:13:21,519
across the u.s

00:13:19,680 --> 00:13:23,600
vcp firearms will need to support tens

00:13:21,519 --> 00:13:25,680
of thousands of worker nodes

00:13:23,600 --> 00:13:27,440
the scale of far edge necessitates a

00:13:25,680 --> 00:13:29,040
zero touch provisioning process for the

00:13:27,440 --> 00:13:30,880
hardware

00:13:29,040 --> 00:13:32,880
we developed a custom uefa firmware

00:13:30,880 --> 00:13:35,279
image for the bmc which is installed by

00:13:32,880 --> 00:13:37,600
the vendor during server manufacturing

00:13:35,279 --> 00:13:39,279
this custom image includes an 802.1x

00:13:37,600 --> 00:13:41,519
certificate that allows the compute node

00:13:39,279 --> 00:13:43,600
to authenticate itself to the csr

00:13:41,519 --> 00:13:45,199
during an initialization of the bmc

00:13:43,600 --> 00:13:47,120
network interface

00:13:45,199 --> 00:13:49,279
the authentication is to prevent someone

00:13:47,120 --> 00:13:51,760
from attaching a wrong device to the csr

00:13:49,279 --> 00:13:53,760
and gaining access to the network the

00:13:51,760 --> 00:13:56,079
csrs are already configured to generate

00:13:53,760 --> 00:13:57,839
ipv6 routing advertisements

00:13:56,079 --> 00:14:00,560
so once the compute node authenticates

00:13:57,839 --> 00:14:02,720
with the csr the bmc is able to form an

00:14:00,560 --> 00:14:03,839
ipv6 address using stateless address

00:14:02,720 --> 00:14:07,440
auto configuration

00:14:03,839 --> 00:14:09,120
or slack within a valid ipv6 address the

00:14:07,440 --> 00:14:10,320
bnc will connect to a centralized

00:14:09,120 --> 00:14:12,959
database server

00:14:10,320 --> 00:14:15,120
and register its slack address one

00:14:12,959 --> 00:14:16,079
segment in the ipv6 address contains an

00:14:15,120 --> 00:14:19,040
encoded location

00:14:16,079 --> 00:14:20,560
id location id is used as a key to

00:14:19,040 --> 00:14:22,320
locate and assign

00:14:20,560 --> 00:14:24,800
a permanent global routable address for

00:14:22,320 --> 00:14:28,240
the bmc and to find the site-specific

00:14:24,800 --> 00:14:28,959
paths iso file for the node a web book

00:14:28,240 --> 00:14:31,120
then kicks off

00:14:28,959 --> 00:14:32,720
jenkins job that uses redfish to

00:14:31,120 --> 00:14:34,800
validate or upgrade firmware

00:14:32,720 --> 00:14:38,399
provision the bmc in the bios and to

00:14:34,800 --> 00:14:40,560
install the bmc permanent ipv6 address

00:14:38,399 --> 00:14:41,839
jenkins then uses an api call to a

00:14:40,560 --> 00:14:44,079
central controller

00:14:41,839 --> 00:14:46,079
where the worker node will be registered

00:14:44,079 --> 00:14:48,560
the controller uses redfish to mount the

00:14:46,079 --> 00:14:49,760
no specific iso file on the bmc virtual

00:14:48,560 --> 00:14:52,079
media interface

00:14:49,760 --> 00:14:54,160
and begins the install of the pass

00:14:52,079 --> 00:14:56,000
following a post-install reboot the node

00:14:54,160 --> 00:14:57,440
will register with the controller

00:14:56,000 --> 00:14:58,800
successful registration with the

00:14:57,440 --> 00:15:00,720
controller will trigger final

00:14:58,800 --> 00:15:02,560
configuration via ansible

00:15:00,720 --> 00:15:04,880
the worker node is now ready to receive

00:15:02,560 --> 00:15:07,279
workloads

00:15:04,880 --> 00:15:08,959
onboarding an application for vcp farad

00:15:07,279 --> 00:15:10,639
starts with uploading a csr file

00:15:08,959 --> 00:15:13,120
containing the docker images

00:15:10,639 --> 00:15:14,240
helm charts yaml files and any required

00:15:13,120 --> 00:15:16,160
scripts

00:15:14,240 --> 00:15:17,760
marketplace then breaks out the contents

00:15:16,160 --> 00:15:20,160
of the cstar file

00:15:17,760 --> 00:15:22,160
docker images go to artifactory helm

00:15:20,160 --> 00:15:23,680
charts are registered with nfvo which is

00:15:22,160 --> 00:15:25,360
our orchestration platform

00:15:23,680 --> 00:15:27,600
and yaml files and scripts are sent to

00:15:25,360 --> 00:15:28,880
gitlab once all the components are

00:15:27,600 --> 00:15:29,680
registered with their respective

00:15:28,880 --> 00:15:31,839
platforms

00:15:29,680 --> 00:15:33,040
a deployment can be triggered by an api

00:15:31,839 --> 00:15:36,800
call to nfo

00:15:33,040 --> 00:15:38,639
or manually initiated via the nfvo ui

00:15:36,800 --> 00:15:40,959
once container images are successfully

00:15:38,639 --> 00:15:43,440
instantiated an install complete message

00:15:40,959 --> 00:15:45,199
is placed on the kafka message bus

00:15:43,440 --> 00:15:47,519
an install complete message triggers

00:15:45,199 --> 00:15:55,040
other tasks such as adding the cnf

00:15:47,519 --> 00:15:56,800
to alarming logging and monitoring

00:15:55,040 --> 00:15:58,639
once the partner site is installed in

00:15:56,800 --> 00:16:00,959
the vdu workload deployed

00:15:58,639 --> 00:16:02,959
both need to be monitored in line with

00:16:00,959 --> 00:16:04,320
the introduction of the vram project we

00:16:02,959 --> 00:16:06,240
are deploying new monitoring and

00:16:04,320 --> 00:16:07,120
alarming tools to replace our legacy

00:16:06,240 --> 00:16:09,839
openview and

00:16:07,120 --> 00:16:11,440
expert platforms data collection is

00:16:09,839 --> 00:16:14,800
handled by telegraph collect d

00:16:11,440 --> 00:16:16,639
s p and redfish events information from

00:16:14,800 --> 00:16:17,279
each of these tools is placed on a kafka

00:16:16,639 --> 00:16:20,000
bus

00:16:17,279 --> 00:16:21,440
and sent to a data lake logstash

00:16:20,000 --> 00:16:23,759
alaskasearch cabana

00:16:21,440 --> 00:16:25,440
influx db and graphana mine data from

00:16:23,759 --> 00:16:26,880
the data lake to build dashboards to

00:16:25,440 --> 00:16:30,000
display metrics

00:16:26,880 --> 00:16:31,279
view log files or display trends right

00:16:30,000 --> 00:16:33,279
now we have one instance of the

00:16:31,279 --> 00:16:35,680
monitoring stack for each of the vcp

00:16:33,279 --> 00:16:36,800
platforms the stacks were designed with

00:16:35,680 --> 00:16:38,800
scaling in mind

00:16:36,800 --> 00:16:40,000
as the number of vcp platform nodes

00:16:38,800 --> 00:16:41,680
increases additional

00:16:40,000 --> 00:16:44,240
instances of the monitoring stack can be

00:16:41,680 --> 00:16:45,920
added the implementation scaling of

00:16:44,240 --> 00:16:47,040
these new tools are still very much a

00:16:45,920 --> 00:16:50,800
work in progress

00:16:47,040 --> 00:16:52,480
but early results are promising

00:16:50,800 --> 00:16:54,320
development of the hardware and software

00:16:52,480 --> 00:16:56,480
needed to build and deploy a virtual ram

00:16:54,320 --> 00:16:57,839
infrastructure for 5g services has been

00:16:56,480 --> 00:16:59,440
a two-year journey

00:16:57,839 --> 00:17:01,120
this culminated a month ago with the

00:16:59,440 --> 00:17:02,160
successful completion of the first

00:17:01,120 --> 00:17:04,319
end-to-end

00:17:02,160 --> 00:17:05,760
fully virtualized 5g data session in a

00:17:04,319 --> 00:17:07,520
live network

00:17:05,760 --> 00:17:09,120
all of the nfs involved from the core of

00:17:07,520 --> 00:17:09,679
the network out to the far edge of the

00:17:09,120 --> 00:17:12,400
network

00:17:09,679 --> 00:17:13,280
were deployed as vnfs or cnfs running on

00:17:12,400 --> 00:17:16,319
vcp core

00:17:13,280 --> 00:17:18,079
vcp edge and vcp far edge the link in

00:17:16,319 --> 00:17:20,000
the slide contains more detail about

00:17:18,079 --> 00:17:23,520
this milestone event

00:17:20,000 --> 00:17:27,280
thank you for your time and attention

00:17:23,520 --> 00:17:27,280
at this time i'd be happy to take any

00:17:30,840 --> 00:17:33,840
questions

00:17:36,080 --> 00:17:40,320
our speaker is now available live to

00:17:38,799 --> 00:17:42,880
answer any questions

00:17:40,320 --> 00:17:43,760
please submit your questions via the q a

00:17:42,880 --> 00:17:47,840
chat panel

00:17:43,760 --> 00:17:47,840
and he will be happy to answer them

00:18:11,840 --> 00:18:16,320
i'm hoping you can hear me so if we have

00:18:14,480 --> 00:18:19,280
problems with redfish

00:18:16,320 --> 00:18:20,880
the bmc will reboot and go back to its

00:18:19,280 --> 00:18:22,720
initial state so that we can start

00:18:20,880 --> 00:18:26,000
pushing the

00:18:22,720 --> 00:18:26,000
the path image again

00:18:26,960 --> 00:18:30,559
uh the next question was what percentage

00:18:28,799 --> 00:18:31,440
of the ran and the core are virtualized

00:18:30,559 --> 00:18:33,679
today

00:18:31,440 --> 00:18:36,160
uh the the rand we're just starting uh

00:18:33,679 --> 00:18:37,120
we have approximately 700 sites deployed

00:18:36,160 --> 00:18:39,760
today

00:18:37,120 --> 00:18:41,200
with our core infrastructure we're

00:18:39,760 --> 00:18:43,840
approximately 60

00:18:41,200 --> 00:18:43,840
deployed

00:18:47,280 --> 00:18:50,960
so are you looking at fpgas only on vdu

00:18:50,480 --> 00:18:54,400
or

00:18:50,960 --> 00:18:56,799
also gpa gpu and ea

00:18:54,400 --> 00:18:58,160
fpga was used because it was the

00:18:56,799 --> 00:19:01,120
quickest to get

00:18:58,160 --> 00:19:03,360
into production um and a lot of the vdu

00:19:01,120 --> 00:19:06,080
vendors had already started development

00:19:03,360 --> 00:19:07,039
of their specific code for the fpga we

00:19:06,080 --> 00:19:09,679
are looking to

00:19:07,039 --> 00:19:11,440
move away from the fpga because of the

00:19:09,679 --> 00:19:12,880
maintenance requirements and the time it

00:19:11,440 --> 00:19:15,919
takes to provision

00:19:12,880 --> 00:19:17,440
uh gpus are probably the next place that

00:19:15,919 --> 00:19:19,440
we'll go

00:19:17,440 --> 00:19:22,000
we've looked at a couple of each but

00:19:19,440 --> 00:19:25,840
they haven't turned out to be

00:19:22,000 --> 00:19:25,840
as promising as gpus are

00:19:30,880 --> 00:19:34,559
the next question was about did we build

00:19:32,720 --> 00:19:35,360
everything in-house yes everything

00:19:34,559 --> 00:19:37,120
that's

00:19:35,360 --> 00:19:39,520
shown in the presentation was built

00:19:37,120 --> 00:19:42,000
in-house

00:19:39,520 --> 00:19:43,840
we had a lot of false starts with trying

00:19:42,000 --> 00:19:45,280
to do our own development verizon or at

00:19:43,840 --> 00:19:47,600
least the group that i work with in

00:19:45,280 --> 00:19:51,039
verizon we're not really a development

00:19:47,600 --> 00:19:53,280
house so we had to learn how to develop

00:19:51,039 --> 00:19:54,799
and how to develop ci cd pipelines at

00:19:53,280 --> 00:19:56,480
the same time as we were trying to push

00:19:54,799 --> 00:19:59,039
this platform into production

00:19:56,480 --> 00:20:00,720
fortunately it's worked out well and

00:19:59,039 --> 00:20:06,400
we're now looking to

00:20:00,720 --> 00:20:08,080
deploy nationwide

00:20:06,400 --> 00:20:10,960
the question about what's your approach

00:20:08,080 --> 00:20:14,559
to guaranteeing cnf and vnf security

00:20:10,960 --> 00:20:16,559
um so for the vdu

00:20:14,559 --> 00:20:18,480
working in the far edge implementation

00:20:16,559 --> 00:20:20,640
there's only a single

00:20:18,480 --> 00:20:22,400
worker node there and only a single

00:20:20,640 --> 00:20:24,400
container that's there today

00:20:22,400 --> 00:20:26,559
but we have an internal strategy for

00:20:24,400 --> 00:20:31,360
securing all of our

00:20:26,559 --> 00:20:34,880
vns or cnfs as part of the marketplace

00:20:31,360 --> 00:20:37,039
aspect of deployment in that process the

00:20:34,880 --> 00:20:38,159
applications go through a very extensive

00:20:37,039 --> 00:20:40,799
screening process

00:20:38,159 --> 00:20:42,240
scanning process and then we perform pin

00:20:40,799 --> 00:20:45,600
testing on each of the

00:20:42,240 --> 00:20:47,919
applications once they're in production

00:20:45,600 --> 00:20:49,919
we have a security group that maintains

00:20:47,919 --> 00:20:50,559
and looks at vulnerabilities and can

00:20:49,919 --> 00:20:53,840
trigger an

00:20:50,559 --> 00:20:55,440
update to correct a vulnerability that's

00:20:53,840 --> 00:20:57,440
found

00:20:55,440 --> 00:21:03,840
from all from a central location out to

00:20:57,440 --> 00:21:03,840
all of the vran sites

00:21:08,480 --> 00:21:12,480
so we do everything we in the question

00:21:11,200 --> 00:21:14,000
one of the questions is what cloud

00:21:12,480 --> 00:21:15,440
provider do we use

00:21:14,000 --> 00:21:17,280
actually everything is on a private

00:21:15,440 --> 00:21:18,960
cloud it's a private telco cloud that

00:21:17,280 --> 00:21:21,919
was built internally

00:21:18,960 --> 00:21:22,559
by verizon it's only used for hosting

00:21:21,919 --> 00:21:26,159
our

00:21:22,559 --> 00:21:29,679
5g 4g vnf vnf and cns

00:21:26,159 --> 00:21:30,880
infrastructure we're federally regulated

00:21:29,679 --> 00:21:33,919
we can't use

00:21:30,880 --> 00:21:38,720
aws or azure azure

00:21:33,919 --> 00:21:45,840
sorry for our for our

00:21:38,720 --> 00:21:45,840
presentation i mean for our systems

00:21:48,480 --> 00:21:52,640
so we are not currently using uh machine

00:21:51,760 --> 00:21:54,320
learning or

00:21:52,640 --> 00:21:56,400
rand intelligent controllers that will

00:21:54,320 --> 00:22:02,000
be part of the next initiative

00:21:56,400 --> 00:22:02,000
as we move into c-band and cbrs

00:22:04,559 --> 00:22:11,440
and yes we do anticipate going 100 cnf

00:22:07,760 --> 00:22:11,440
and migrating away from dns

00:22:11,679 --> 00:22:15,120
and that's the last one in the queue

00:22:22,080 --> 00:22:26,000
hey gene it looks like we have a couple

00:22:24,400 --> 00:22:28,720
questions in the chat as

00:22:26,000 --> 00:22:29,039
well were you able to address those i

00:22:28,720 --> 00:22:31,520
just

00:22:29,039 --> 00:22:31,520
just saw

00:22:34,080 --> 00:22:39,360
um i'm assuming by with a question about

00:22:37,760 --> 00:22:41,840
you know how do you compare your edge

00:22:39,360 --> 00:22:45,840
and differentiate from atp

00:22:41,840 --> 00:22:47,600
um not really sure that i can say

00:22:45,840 --> 00:22:49,440
i can actually answer that one right now

00:22:47,600 --> 00:22:51,840
because of some

00:22:49,440 --> 00:22:55,840
internal issues with what we can say and

00:22:51,840 --> 00:22:55,840
what we can't say about our competitors

00:22:56,640 --> 00:23:00,960
znf enf security already answered that

00:22:58,720 --> 00:23:03,600
one

00:23:00,960 --> 00:23:05,520
so we actually part of our the question

00:23:03,600 --> 00:23:08,640
about using onap

00:23:05,520 --> 00:23:10,559
hp nfvd is our primary orchestration

00:23:08,640 --> 00:23:12,480
platform we built a bunch of components

00:23:10,559 --> 00:23:15,919
around that marketplace

00:23:12,480 --> 00:23:19,440
and atlas but the hp vnfp

00:23:15,919 --> 00:23:20,840
or hp nfvd is an onap compliant

00:23:19,440 --> 00:23:23,840
provisioning

00:23:20,840 --> 00:23:23,840
system

00:23:25,919 --> 00:23:29,440
do you anticipate etsy compliance for

00:23:28,000 --> 00:23:31,679
core and edge

00:23:29,440 --> 00:23:33,200
we've been trying to follow etsy but in

00:23:31,679 --> 00:23:36,400
many cases

00:23:33,200 --> 00:23:38,320
etsy gets in the way where we can be

00:23:36,400 --> 00:23:38,640
compliant with etsy we absolutely try to

00:23:38,320 --> 00:23:51,840
be

00:23:38,640 --> 00:23:51,840
but in some cases we have to work around

00:23:59,440 --> 00:24:07,440
and that's the last question oh one more

00:24:04,799 --> 00:24:09,279
uh yes we will eventually use it to use

00:24:07,440 --> 00:24:11,120
the vran infrastructure to roll out

00:24:09,279 --> 00:24:12,880
millimeter wave and 5g

00:24:11,120 --> 00:24:14,640
uh we'll be using the same a similar

00:24:12,880 --> 00:24:18,720
infrastructure for both 5g

00:24:14,640 --> 00:24:22,480
cbrs and c-band

00:24:18,720 --> 00:24:24,159
so in three to five years our entire

00:24:22,480 --> 00:24:25,279
rand infrastructure will be totally

00:24:24,159 --> 00:24:27,279
virtualized

00:24:25,279 --> 00:24:28,640
and we'll be getting going away from the

00:24:27,279 --> 00:24:33,840
legacy bbu

00:24:28,640 --> 00:24:33,840
infrastructure completely

00:24:46,080 --> 00:25:01,840
do we have any other questions from our

00:24:48,400 --> 00:25:01,840
viewers today

00:25:05,520 --> 00:25:09,679
any plans for 400g optical

00:25:07,760 --> 00:25:11,120
infrastructure yes we are currently

00:25:09,679 --> 00:25:14,640
looking at that

00:25:11,120 --> 00:25:16,880
um how we get that out to

00:25:14,640 --> 00:25:18,640
sea ran deer insights is still a work in

00:25:16,880 --> 00:25:21,039
progress but we are

00:25:18,640 --> 00:25:21,679
actively looking at that not only for

00:25:21,039 --> 00:25:24,960
backhaul

00:25:21,679 --> 00:25:25,760
from the cran d-ran sites to sap and tap

00:25:24,960 --> 00:25:28,960
but also

00:25:25,760 --> 00:25:32,720
400 gig infrastructure within the sap

00:25:28,960 --> 00:25:34,159
and the nec locations to support

00:25:32,720 --> 00:25:43,840
the aggregation of all the traffic

00:25:34,159 --> 00:25:43,840
coming from the edge of the network

00:25:53,120 --> 00:25:56,799
all right well i think that concludes

00:25:54,799 --> 00:25:59,120
our webcast for today thank you

00:25:56,799 --> 00:26:00,400
everybody for joining

00:25:59,120 --> 00:26:03,600
we look forward to seeing you on the

00:26:00,400 --> 00:26:11,840
next session have a great day

00:26:03,600 --> 00:26:11,840
thank you

00:26:22,840 --> 00:26:25,840
you

00:26:55,919 --> 00:26:58,000

YouTube URL: https://www.youtube.com/watch?v=AbtL6j6XL8U


