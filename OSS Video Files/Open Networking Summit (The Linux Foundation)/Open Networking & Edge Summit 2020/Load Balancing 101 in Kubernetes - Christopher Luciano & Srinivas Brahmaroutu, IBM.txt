Title: Load Balancing 101 in Kubernetes - Christopher Luciano & Srinivas Brahmaroutu, IBM
Publication date: 2020-10-28
Playlist: Open Networking & Edge Summit 2020
Description: 
	Load Balancing 101 in Kubernetes - Christopher Luciano & Srinivas Brahmaroutu, IBM
Captions: 
	00:00:02,639 --> 00:00:06,480
hello and thank you for joining me for a

00:00:04,080 --> 00:00:08,400
load balancing 101 in kubernetes

00:00:06,480 --> 00:00:11,040
my name is christopher i'm luciano when

00:00:08,400 --> 00:00:12,480
i work for ibm

00:00:11,040 --> 00:00:14,719
today we're going to be going over how

00:00:12,480 --> 00:00:16,400
networking works inside of kubernetes

00:00:14,719 --> 00:00:16,960
we're going to pay particular attention

00:00:16,400 --> 00:00:19,359
to

00:00:16,960 --> 00:00:20,080
how these service types align with each

00:00:19,359 --> 00:00:22,480
other

00:00:20,080 --> 00:00:25,039
as well as how networking works to

00:00:22,480 --> 00:00:27,039
travel from node to node by way of ip

00:00:25,039 --> 00:00:28,880
tables

00:00:27,039 --> 00:00:30,720
a colleague shrini will join us

00:00:28,880 --> 00:00:32,960
afterwards to

00:00:30,720 --> 00:00:34,880
talk about some of the key features

00:00:32,960 --> 00:00:37,840
missing inside of layer 4

00:00:34,880 --> 00:00:38,800
layer 7 load balancing as well as

00:00:37,840 --> 00:00:40,879
demonstrating

00:00:38,800 --> 00:00:42,320
a shared load balancer project that

00:00:40,879 --> 00:00:45,440
we've been working on

00:00:42,320 --> 00:00:47,039
to solve some of these problems so in

00:00:45,440 --> 00:00:48,559
our typical kubernetes cluster we're

00:00:47,039 --> 00:00:50,320
working with a few nodes

00:00:48,559 --> 00:00:52,239
and each of these nodes contain a

00:00:50,320 --> 00:00:55,920
collection of containers

00:00:52,239 --> 00:00:56,480
known as a pod one of the missing pieces

00:00:55,920 --> 00:00:59,120
that we're

00:00:56,480 --> 00:01:00,800
wondering about is how does traffic get

00:00:59,120 --> 00:01:03,280
from node one to node two

00:01:00,800 --> 00:01:05,040
what is the missing piece for pod one on

00:01:03,280 --> 00:01:07,920
node one to get to pod two

00:01:05,040 --> 00:01:07,920
on node two

00:01:08,320 --> 00:01:11,360
this is where the container networking

00:01:09,760 --> 00:01:14,000
interface comes up

00:01:11,360 --> 00:01:15,360
and the container networking interface

00:01:14,000 --> 00:01:18,799
provides with

00:01:15,360 --> 00:01:21,439
a spec and libraries for

00:01:18,799 --> 00:01:23,600
tearing down and bringing up

00:01:21,439 --> 00:01:26,640
connectivity between containers

00:01:23,600 --> 00:01:29,200
on the node it takes care of ip address

00:01:26,640 --> 00:01:31,840
management known as ipam

00:01:29,200 --> 00:01:32,479
and gives all of those ips out from a

00:01:31,840 --> 00:01:36,079
range

00:01:32,479 --> 00:01:38,400
specified at startup time on to your api

00:01:36,079 --> 00:01:41,360
server

00:01:38,400 --> 00:01:44,000
and these can be either ipv4 or ipv6

00:01:41,360 --> 00:01:44,000
addresses

00:01:45,759 --> 00:01:49,840
you'll see a variety of different

00:01:47,920 --> 00:01:51,520
plugins inside of cni

00:01:49,840 --> 00:01:53,920
i think plugins are normally associated

00:01:51,520 --> 00:01:55,600
with a sort of brand name like calico or

00:01:53,920 --> 00:01:57,439
psyllium

00:01:55,600 --> 00:01:59,680
and a lot of these incorporate thin

00:01:57,439 --> 00:02:01,360
plugins which are found in the cni

00:01:59,680 --> 00:02:02,880
plugins repository

00:02:01,360 --> 00:02:04,799
and these are things like setting up a

00:02:02,880 --> 00:02:09,840
linux bridge

00:02:04,799 --> 00:02:09,840
dhcp or port mapping

00:02:10,319 --> 00:02:14,720
also within the types are underlay

00:02:12,319 --> 00:02:17,680
plugins versus overlay plugins

00:02:14,720 --> 00:02:19,520
underlay plugins run in conjunction with

00:02:17,680 --> 00:02:22,160
your existing network much the same way

00:02:19,520 --> 00:02:23,840
that switches and routers do these are

00:02:22,160 --> 00:02:25,520
normally considered a bit simpler

00:02:23,840 --> 00:02:27,200
than their counter parts of an overlay

00:02:25,520 --> 00:02:30,239
network

00:02:27,200 --> 00:02:33,040
and performance increases can be seen by

00:02:30,239 --> 00:02:37,040
using an underlay plug-in as well

00:02:33,040 --> 00:02:39,280
popular protocols include bgp and ospf

00:02:37,040 --> 00:02:40,160
on the overlay side we see a separate

00:02:39,280 --> 00:02:43,360
network

00:02:40,160 --> 00:02:45,840
created atop your underlay thus

00:02:43,360 --> 00:02:47,920
overlay and this is because it creates

00:02:45,840 --> 00:02:51,040
its own virtual network

00:02:47,920 --> 00:02:51,680
thus segmenting your network from your

00:02:51,040 --> 00:02:54,800
underlying

00:02:51,680 --> 00:02:56,000
underlay popular protocols for this

00:02:54,800 --> 00:02:59,760
include vxlan

00:02:56,000 --> 00:03:03,120
and gre so in our typical vm

00:02:59,760 --> 00:03:06,720
setup we have one app per node

00:03:03,120 --> 00:03:09,920
and this app communicates with other

00:03:06,720 --> 00:03:12,640
apps by going through the eth0 interface

00:03:09,920 --> 00:03:12,640
on the machine

00:03:13,519 --> 00:03:17,760
now in a kubernetes sense we have

00:03:15,680 --> 00:03:21,360
multiple pods running on the nodes

00:03:17,760 --> 00:03:24,480
so how can these pods

00:03:21,360 --> 00:03:26,720
work through the same node and hit pods

00:03:24,480 --> 00:03:28,159
on other nodes well this is where cni

00:03:26,720 --> 00:03:31,840
comes in

00:03:28,159 --> 00:03:34,080
cn not with cni each pod has its own ip

00:03:31,840 --> 00:03:37,440
and each container within the pod

00:03:34,080 --> 00:03:39,840
gets assigned a unique port

00:03:37,440 --> 00:03:40,959
and then we have a collection known as

00:03:39,840 --> 00:03:43,920
an endpoint

00:03:40,959 --> 00:03:47,360
which has all of these individual ips

00:03:43,920 --> 00:03:49,200
representing the different pods

00:03:47,360 --> 00:03:50,959
and we're able to do this by using linux

00:03:49,200 --> 00:03:54,239
technologies

00:03:50,959 --> 00:03:57,360
known as namespaces to simulate

00:03:54,239 --> 00:03:58,879
the one app per vm setup as we have seen

00:03:57,360 --> 00:04:01,760
before

00:03:58,879 --> 00:04:02,239
we use pod network name spaces each pod

00:04:01,760 --> 00:04:05,599
gets

00:04:02,239 --> 00:04:07,920
own network name space with its own eth0

00:04:05,599 --> 00:04:10,239
and then it communicates to the root

00:04:07,920 --> 00:04:12,879
network namespace

00:04:10,239 --> 00:04:14,400
by way of a virtual ethernet setup for

00:04:12,879 --> 00:04:16,479
each individual

00:04:14,400 --> 00:04:17,600
networking name space that are bridged

00:04:16,479 --> 00:04:21,440
together

00:04:17,600 --> 00:04:21,440
to exit through the eth0

00:04:22,479 --> 00:04:27,199
so in this setup we see a pod network

00:04:24,720 --> 00:04:30,720
namespace one would communicate with the

00:04:27,199 --> 00:04:33,440
youth zero and that would hit the bridge

00:04:30,720 --> 00:04:36,479
and then flow out through eth0 interface

00:04:33,440 --> 00:04:36,479
onto the node two

00:04:37,280 --> 00:04:41,199
so we collect together all of these

00:04:39,680 --> 00:04:44,800
individual pods into an

00:04:41,199 --> 00:04:46,960
endpoints object as we can see here for

00:04:44,800 --> 00:04:50,400
the kube dns

00:04:46,960 --> 00:04:53,199
pods we have two different nodes

00:04:50,400 --> 00:04:56,880
any on each of those nodes has a unique

00:04:53,199 --> 00:04:58,800
ip address representing that pod

00:04:56,880 --> 00:05:00,400
and we have two different lists inside

00:04:58,800 --> 00:05:02,560
of the endpoints objects

00:05:00,400 --> 00:05:04,160
for ready endpoints and one for not

00:05:02,560 --> 00:05:07,039
ready endpoints that have not passed

00:05:04,160 --> 00:05:10,080
their health check yet

00:05:07,039 --> 00:05:13,759
and we can refer to these using

00:05:10,080 --> 00:05:17,520
one vip by way of a

00:05:13,759 --> 00:05:18,320
service similarly to the endpoints we're

00:05:17,520 --> 00:05:21,600
selecting

00:05:18,320 --> 00:05:23,919
over labels on pods

00:05:21,600 --> 00:05:25,520
so for the front end service we look for

00:05:23,919 --> 00:05:28,560
all of the

00:05:25,520 --> 00:05:33,120
pods matching the front end label

00:05:28,560 --> 00:05:33,120
and we're targeting individually the

00:05:33,199 --> 00:05:38,400
port 9376

00:05:36,320 --> 00:05:40,240
which will be tacked onto our cluster ip

00:05:38,400 --> 00:05:42,080
and will point to port 80

00:05:40,240 --> 00:05:43,759
inside of a pod because we can have

00:05:42,080 --> 00:05:45,520
multiple containers inside of the pot so

00:05:43,759 --> 00:05:48,160
we want to make sure that we hit

00:05:45,520 --> 00:05:53,680
the actual web service application and

00:05:48,160 --> 00:05:56,000
not something like a logging agent

00:05:53,680 --> 00:05:56,880
and this is by way of the cluster ip

00:05:56,000 --> 00:06:01,039
which gets

00:05:56,880 --> 00:06:04,000
assigned from a range passed at runtime

00:06:01,039 --> 00:06:06,000
to your api server

00:06:04,000 --> 00:06:07,360
these are reachable only within side of

00:06:06,000 --> 00:06:11,039
your cluster

00:06:07,360 --> 00:06:13,360
so it thus cannot be hit

00:06:11,039 --> 00:06:16,319
outside of your cluster unless you've

00:06:13,360 --> 00:06:16,319
bridged your way in

00:06:18,639 --> 00:06:22,880
another example is the node port service

00:06:21,280 --> 00:06:25,600
and this opens up

00:06:22,880 --> 00:06:26,960
a port specified by whatever you put in

00:06:25,600 --> 00:06:29,680
for node ports

00:06:26,960 --> 00:06:32,080
on each of your individual nodes such

00:06:29,680 --> 00:06:35,600
that i can hit the public ip

00:06:32,080 --> 00:06:39,360
of that given node

00:06:35,600 --> 00:06:42,639
with this node port and i'll be directed

00:06:39,360 --> 00:06:45,600
to the given cluster ip

00:06:42,639 --> 00:06:46,960
resembling the pod that i want to hit at

00:06:45,600 --> 00:06:50,319
its given port

00:06:46,960 --> 00:06:50,319
these things wrapped together

00:06:50,880 --> 00:06:53,919
and then the final way to do this is

00:06:53,280 --> 00:06:56,319
with the

00:06:53,919 --> 00:06:58,000
load balancer service these are normally

00:06:56,319 --> 00:07:00,080
fairly cloud specific

00:06:58,000 --> 00:07:02,400
but in the end normally you get a tcp

00:07:00,080 --> 00:07:04,720
load balancer from your cloud

00:07:02,400 --> 00:07:05,919
and it is assigned a publicly

00:07:04,720 --> 00:07:08,479
addressable

00:07:05,919 --> 00:07:11,039
ip as we can see in this example starts

00:07:08,479 --> 00:07:14,000
with 969 at the bottom

00:07:11,039 --> 00:07:15,599
and we still if we look into the example

00:07:14,000 --> 00:07:17,440
here we still see node port

00:07:15,599 --> 00:07:19,360
and target port in 80 because all of

00:07:17,440 --> 00:07:21,919
these different service types

00:07:19,360 --> 00:07:22,479
get wrapped up in the end so when i hit

00:07:21,919 --> 00:07:26,000
this

00:07:22,479 --> 00:07:28,960
public addressable and ip address

00:07:26,000 --> 00:07:29,680
that resembles the load balancer i get

00:07:28,960 --> 00:07:33,120
bounced to

00:07:29,680 --> 00:07:35,199
a node port backing the given

00:07:33,120 --> 00:07:38,080
service that i want to hit and then to

00:07:35,199 --> 00:07:40,000
the given cluster ip that i want to hit

00:07:38,080 --> 00:07:41,520
there are certain cloud providers that

00:07:40,000 --> 00:07:42,720
allow you to go directly from load

00:07:41,520 --> 00:07:46,080
balancer

00:07:42,720 --> 00:07:48,160
to the given pod and also they also have

00:07:46,080 --> 00:07:50,000
features like the pod ready plus plus

00:07:48,160 --> 00:07:53,440
application to make sure that

00:07:50,000 --> 00:07:56,080
i've tested does the load balancer

00:07:53,440 --> 00:07:59,840
successfully reach the given pod if not

00:07:56,080 --> 00:07:59,840
don't add it to the load balancing pool

00:08:00,080 --> 00:08:03,440
the one thing to note here that we'll

00:08:01,680 --> 00:08:04,720
discuss a little later is when i'm

00:08:03,440 --> 00:08:05,440
creating each of these individual

00:08:04,720 --> 00:08:07,599
services

00:08:05,440 --> 00:08:10,879
i'm getting a load balancer every single

00:08:07,599 --> 00:08:10,879
time for each of these things

00:08:11,680 --> 00:08:16,400
now how does all of this traffic get

00:08:13,680 --> 00:08:19,039
there once it hits that given node port

00:08:16,400 --> 00:08:21,039
what if it's not running on there this

00:08:19,039 --> 00:08:23,280
is where the crew proxy comes in

00:08:21,039 --> 00:08:25,599
and it's a component that runs on each

00:08:23,280 --> 00:08:27,840
of your nodes

00:08:25,599 --> 00:08:28,879
and all it does is sit there and it

00:08:27,840 --> 00:08:32,399
waits for

00:08:28,879 --> 00:08:34,399
service creation requests that come into

00:08:32,399 --> 00:08:36,959
the api server

00:08:34,399 --> 00:08:39,120
and then in the default mode it will

00:08:36,959 --> 00:08:42,240
create iptables rules to direct this

00:08:39,120 --> 00:08:44,399
traffic in and out of nodes

00:08:42,240 --> 00:08:47,440
based on the cluster ip and the node

00:08:44,399 --> 00:08:48,640
port and all of that

00:08:47,440 --> 00:08:50,560
there's a couple of different modes

00:08:48,640 --> 00:08:50,959
though aside from myp tables as we

00:08:50,560 --> 00:08:53,600
mentioned

00:08:50,959 --> 00:08:54,399
iptables is the default and all this

00:08:53,600 --> 00:08:56,560
really does

00:08:54,399 --> 00:09:00,399
is creates a series of rules in the nat

00:08:56,560 --> 00:09:00,399
pre-routing hook of iptables

00:09:00,880 --> 00:09:05,279
and this is a little bit simple it's

00:09:03,519 --> 00:09:08,000
pretty simple to debug

00:09:05,279 --> 00:09:09,680
but i put a trademark symbol on there

00:09:08,000 --> 00:09:11,120
because this is really dependent on how

00:09:09,680 --> 00:09:14,240
often you've seen

00:09:11,120 --> 00:09:16,160
the iptable's output and in the ip

00:09:14,240 --> 00:09:16,640
tables mode the output could get kind of

00:09:16,160 --> 00:09:18,080
messy

00:09:16,640 --> 00:09:19,680
we'll go through an example of this a

00:09:18,080 --> 00:09:23,200
little later

00:09:19,680 --> 00:09:25,920
because iptables creates a rule for each

00:09:23,200 --> 00:09:28,000
of these individual backends

00:09:25,920 --> 00:09:30,080
representing the pods

00:09:28,000 --> 00:09:31,920
and you could have multiple rules

00:09:30,080 --> 00:09:33,600
depending on how many nodes this given

00:09:31,920 --> 00:09:35,920
pod is running on

00:09:33,600 --> 00:09:36,640
the algorithm is really more of an o of

00:09:35,920 --> 00:09:39,600
n

00:09:36,640 --> 00:09:39,600
type of problem

00:09:39,680 --> 00:09:44,720
to offset this a bit the ipvs kernel

00:09:42,800 --> 00:09:47,920
module is used in the ipvs

00:09:44,720 --> 00:09:50,399
mode and this is specifically

00:09:47,920 --> 00:09:53,200
suited for load balancing it's got a

00:09:50,399 --> 00:09:56,399
more constant cluster lookup size

00:09:53,200 --> 00:09:58,640
and the api also offers several

00:09:56,399 --> 00:09:59,600
specific load balancing algorithms that

00:09:58,640 --> 00:10:02,079
can assist you

00:09:59,600 --> 00:10:02,880
with load bouncing or traffic like round

00:10:02,079 --> 00:10:06,720
robin

00:10:02,880 --> 00:10:08,720
least requested or shortest distance

00:10:06,720 --> 00:10:10,079
be sure that you're looking into it does

00:10:08,720 --> 00:10:12,240
your cni plug-in

00:10:10,079 --> 00:10:13,120
support ipvs because while it has

00:10:12,240 --> 00:10:16,320
existed in

00:10:13,120 --> 00:10:17,519
the kubernetes releases for a while some

00:10:16,320 --> 00:10:20,320
cni plug-ins

00:10:17,519 --> 00:10:22,240
are still not available to leverage it

00:10:20,320 --> 00:10:25,600
they're relying on the ip

00:10:22,240 --> 00:10:26,640
tables mode general advice on which one

00:10:25,600 --> 00:10:29,040
to choose

00:10:26,640 --> 00:10:31,600
ipvs has been shown to scale a little

00:10:29,040 --> 00:10:33,519
bit better in terms of cpu time

00:10:31,600 --> 00:10:36,000
as well as round trip time when you're

00:10:33,519 --> 00:10:39,120
getting above a thousand services

00:10:36,000 --> 00:10:42,959
however you only see a modest increase

00:10:39,120 --> 00:10:44,240
above ip ip tables if in the ip tables

00:10:42,959 --> 00:10:46,640
mode you just

00:10:44,240 --> 00:10:49,839
make sufficient usage of application

00:10:46,640 --> 00:10:52,240
keep alive connections

00:10:49,839 --> 00:10:55,120
let's delve into the iptables mode

00:10:52,240 --> 00:10:56,560
though because it is the default

00:10:55,120 --> 00:10:58,320
because this mode essentially just

00:10:56,560 --> 00:10:59,680
writes out iptables rules we could

00:10:58,320 --> 00:11:00,480
determine how we're going to reach our

00:10:59,680 --> 00:11:03,440
services

00:11:00,480 --> 00:11:04,560
by inspecting all of the iptable's rules

00:11:03,440 --> 00:11:08,000
so

00:11:04,560 --> 00:11:09,440
iptable save spits out a ton of

00:11:08,000 --> 00:11:12,720
information

00:11:09,440 --> 00:11:14,079
most of it pretty scary but it it in the

00:11:12,720 --> 00:11:16,959
end just gives us a linear

00:11:14,079 --> 00:11:18,399
list that gets followed until matches

00:11:16,959 --> 00:11:22,320
for our individual services

00:11:18,399 --> 00:11:25,680
are found so let's start with

00:11:22,320 --> 00:11:26,240
looking at the coupe ctl get on a given

00:11:25,680 --> 00:11:29,519
service

00:11:26,240 --> 00:11:31,519
of kubernetes dashboard

00:11:29,519 --> 00:11:34,399
in this example when we look through our

00:11:31,519 --> 00:11:37,760
ip table save dash l output

00:11:34,399 --> 00:11:41,279
we get this rule here for kubescer

00:11:37,760 --> 00:11:43,200
that specifies uh do we want to be

00:11:41,279 --> 00:11:46,640
hitting the coupe services

00:11:43,200 --> 00:11:50,160
service range there with the 172 address

00:11:46,640 --> 00:11:53,440
we're matching on the tcp

00:11:50,160 --> 00:11:54,880
option and we even have a successful

00:11:53,440 --> 00:11:55,519
comment for what this is supposed to be

00:11:54,880 --> 00:11:57,040
used for

00:11:55,519 --> 00:11:59,600
we're saying this is the kubernetes

00:11:57,040 --> 00:12:03,360
dashboard cluster ip

00:11:59,600 --> 00:12:04,720
and then we we want to match all the

00:12:03,360 --> 00:12:10,399
tcps

00:12:04,720 --> 00:12:13,760
traffic destined for this given service

00:12:10,399 --> 00:12:16,240
and jump to kube service xg blah blah

00:12:13,760 --> 00:12:16,240
blah blah

00:12:16,959 --> 00:12:20,399
so let's uh take a look for that crew

00:12:19,200 --> 00:12:23,839
service blah blah blah

00:12:20,399 --> 00:12:25,920
in the output that next jump leads us to

00:12:23,839 --> 00:12:28,959
yet another separator

00:12:25,920 --> 00:12:33,600
and this line in the end directs us

00:12:28,959 --> 00:12:35,440
over to the final destination of 17230

00:12:33,600 --> 00:12:37,600
the cluster ip of the service

00:12:35,440 --> 00:12:39,440
which we can tell in the end matches if

00:12:37,600 --> 00:12:42,160
we do a coupe ctl

00:12:39,440 --> 00:12:42,160
pod describe

00:12:43,279 --> 00:12:47,360
and the dns you're seeing in here is

00:12:45,279 --> 00:12:49,200
actually the destination network address

00:12:47,360 --> 00:12:51,120
translation that happens

00:12:49,200 --> 00:12:53,839
when we're jumping around between these

00:12:51,120 --> 00:12:53,839
given pods

00:12:56,480 --> 00:12:59,120
now you might be wondering why do we

00:12:57,760 --> 00:13:00,399
need to jump through all these hops

00:12:59,120 --> 00:13:02,399
instead of just pointing to

00:13:00,399 --> 00:13:04,560
the first line in the ip table's rules

00:13:02,399 --> 00:13:06,079
that directs us to the given endpoints

00:13:04,560 --> 00:13:07,600
the answer is a lot clearer when you

00:13:06,079 --> 00:13:10,720
have multiple pods

00:13:07,600 --> 00:13:13,519
so let's take a look at a service

00:13:10,720 --> 00:13:14,079
node local dns that we know as multiple

00:13:13,519 --> 00:13:16,959
pods

00:13:14,079 --> 00:13:18,639
spread out across multiple nodes we

00:13:16,959 --> 00:13:20,560
first look at the cluster ip

00:13:18,639 --> 00:13:22,160
for the node local dns pod and find

00:13:20,560 --> 00:13:25,279
multiple rules

00:13:22,160 --> 00:13:28,079
one for each protocol we have tcp and

00:13:25,279 --> 00:13:31,360
udp if we look at the dash m flag let's

00:13:28,079 --> 00:13:33,600
trace the udp protocol jump

00:13:31,360 --> 00:13:34,720
so once again our grep shows two

00:13:33,600 --> 00:13:36,399
different rule sets

00:13:34,720 --> 00:13:38,320
and we see more options beyond the

00:13:36,399 --> 00:13:40,560
simple jump that we saw on kubernetes

00:13:38,320 --> 00:13:43,519
dashboard

00:13:40,560 --> 00:13:45,440
so let's just trace uh wanna keep

00:13:43,519 --> 00:13:47,600
tracing that udp one

00:13:45,440 --> 00:13:50,000
and this option we have a random

00:13:47,600 --> 00:13:53,040
probability mode of iptables

00:13:50,000 --> 00:13:56,240
and this uses a random number generator

00:13:53,040 --> 00:13:57,440
to cause 33 of the traffic to hit one

00:13:56,240 --> 00:14:01,120
endpoint

00:13:57,440 --> 00:14:02,320
and the second rule uh 50 percent of the

00:14:01,120 --> 00:14:05,279
time will hit a different

00:14:02,320 --> 00:14:06,240
endpoint now on subsequent request

00:14:05,279 --> 00:14:07,839
contract

00:14:06,240 --> 00:14:09,440
will be able to remember and for the

00:14:07,839 --> 00:14:11,120
request over the same connection

00:14:09,440 --> 00:14:12,560
so you're not going to get multiple

00:14:11,120 --> 00:14:16,800
things hitting multiple different

00:14:12,560 --> 00:14:18,639
backing pots

00:14:16,800 --> 00:14:20,959
and if we continue to follow that 33

00:14:18,639 --> 00:14:23,199
percent rule we landed on our final rule

00:14:20,959 --> 00:14:25,040
which just de-nets the traffic over to

00:14:23,199 --> 00:14:28,959
our destination

00:14:25,040 --> 00:14:30,480
endpoint of 172. so in summary each

00:14:28,959 --> 00:14:33,120
service will have a kube

00:14:30,480 --> 00:14:34,720
service rule for each different port and

00:14:33,120 --> 00:14:37,040
we'll also see a number of

00:14:34,720 --> 00:14:40,160
cube service hash entries with various

00:14:37,040 --> 00:14:42,079
endpoint weights for each port

00:14:40,160 --> 00:14:44,160
each port endpoint will see a small

00:14:42,079 --> 00:14:46,720
number of kube separator hashes with the

00:14:44,160 --> 00:14:48,399
dean added pod endpoint

00:14:46,720 --> 00:14:50,079
depending on how many different nodes

00:14:48,399 --> 00:14:51,839
this is running on

00:14:50,079 --> 00:14:53,440
and that exact number can also be

00:14:51,839 --> 00:14:55,920
influenced by

00:14:53,440 --> 00:14:57,600
the total number of endpoints and

00:14:55,920 --> 00:14:59,040
whether you have a bunch of node ports

00:14:57,600 --> 00:15:01,760
or load bouncers in the way

00:14:59,040 --> 00:15:04,320
so we can see a huge chunk of iptables

00:15:01,760 --> 00:15:08,240
was dedicated to this

00:15:04,320 --> 00:15:09,519
we can also use dns to refer to these

00:15:08,240 --> 00:15:11,199
things we don't have to remember these

00:15:09,519 --> 00:15:13,199
cluster ips

00:15:11,199 --> 00:15:14,800
and kubernetes provides this with core

00:15:13,199 --> 00:15:18,320
dns

00:15:14,800 --> 00:15:20,720
the pod service ips is stable

00:15:18,320 --> 00:15:21,600
for corbett dns so you don't need to

00:15:20,720 --> 00:15:24,240
constantly

00:15:21,600 --> 00:15:25,199
be killing your cache in order to hit

00:15:24,240 --> 00:15:28,560
the

00:15:25,199 --> 00:15:28,560
possible ip that you want

00:15:29,519 --> 00:15:34,320
and the final method that we can use for

00:15:32,720 --> 00:15:35,759
reaching things inside of a cluster is

00:15:34,320 --> 00:15:39,360
ingress and

00:15:35,759 --> 00:15:41,920
ingress operates at the layer 7

00:15:39,360 --> 00:15:42,399
level popular controllers that provide

00:15:41,920 --> 00:15:46,160
this

00:15:42,399 --> 00:15:48,959
h a proxy and ingress engine x

00:15:46,160 --> 00:15:49,600
the current api is uh a little limited

00:15:48,959 --> 00:15:52,320
in scope

00:15:49,600 --> 00:15:54,000
in order to have maximum portability at

00:15:52,320 --> 00:15:56,000
http level

00:15:54,000 --> 00:15:57,040
but there has been enhancements to

00:15:56,000 --> 00:15:58,399
specify

00:15:57,040 --> 00:16:00,079
this a little and make it a little

00:15:58,399 --> 00:16:03,040
clearer with the version one

00:16:00,079 --> 00:16:05,040
offered in kubernetes 1.19 but here's a

00:16:03,040 --> 00:16:05,920
basic example which you'll be familiar

00:16:05,040 --> 00:16:07,680
with

00:16:05,920 --> 00:16:10,399
if you've seen nginx or apache

00:16:07,680 --> 00:16:12,959
configuration

00:16:10,399 --> 00:16:13,600
and in this example we're focusing on

00:16:12,959 --> 00:16:17,040
the my

00:16:13,600 --> 00:16:19,360
app service which is backed by

00:16:17,040 --> 00:16:20,079
something listed on port 80 and we want

00:16:19,360 --> 00:16:22,320
to

00:16:20,079 --> 00:16:24,560
once we hit this ingress controller

00:16:22,320 --> 00:16:27,920
specified by an ip address

00:16:24,560 --> 00:16:31,199
at the special path directory there we

00:16:27,920 --> 00:16:33,440
want to forward on to the my app

00:16:31,199 --> 00:16:35,440
now let's move into the second part of

00:16:33,440 --> 00:16:36,880
our presentation with srini to talk

00:16:35,440 --> 00:16:38,480
about

00:16:36,880 --> 00:16:40,560
ways that we can get around some of

00:16:38,480 --> 00:16:43,199
these limitations where we have

00:16:40,560 --> 00:16:45,680
multiple entries for these things per

00:16:43,199 --> 00:16:49,040
service

00:16:45,680 --> 00:16:50,000
hello everyone uh thanks chris chris has

00:16:49,040 --> 00:16:53,199
talked about

00:16:50,000 --> 00:16:54,880
networking kubernetes um and

00:16:53,199 --> 00:16:56,639
he also talked about the services

00:16:54,880 --> 00:16:59,040
different types of services like

00:16:56,639 --> 00:17:00,160
northport or load balancer as you can

00:16:59,040 --> 00:17:02,880
see here

00:17:00,160 --> 00:17:03,519
whether services are used to connect to

00:17:02,880 --> 00:17:06,160
a back-end

00:17:03,519 --> 00:17:07,520
and for workload rl7 workload they are

00:17:06,160 --> 00:17:10,079
dedicated

00:17:07,520 --> 00:17:11,120
but with l7 we have english controllers

00:17:10,079 --> 00:17:14,319
which will allow you

00:17:11,120 --> 00:17:16,559
to have incoming traffic connecting

00:17:14,319 --> 00:17:18,000
multiple workloads in the backend

00:17:16,559 --> 00:17:20,319
that means we can share this in the

00:17:18,000 --> 00:17:22,559
connection um there are many

00:17:20,319 --> 00:17:23,520
popular ingress controllers like nginx

00:17:22,559 --> 00:17:26,880
on y

00:17:23,520 --> 00:17:30,000
traffic etc available but if you look at

00:17:26,880 --> 00:17:32,400
uh l4 something is missing here

00:17:30,000 --> 00:17:34,480
i would like to demo a solution to the

00:17:32,400 --> 00:17:36,960
problem we have here

00:17:34,480 --> 00:17:37,760
how can i expose my l4 internal

00:17:36,960 --> 00:17:42,640
workloads

00:17:37,760 --> 00:17:42,640
in a shared way using an l4 in this

00:17:42,720 --> 00:17:49,440
um the problem as you can see a user

00:17:46,640 --> 00:17:51,679
has to know how to connect to load

00:17:49,440 --> 00:17:54,080
balances for service a or service b

00:17:51,679 --> 00:17:56,960
to connect to the backend workloads uh

00:17:54,080 --> 00:17:59,200
on poly one or part v1

00:17:56,960 --> 00:18:00,640
this is not a hypothetical requirement

00:17:59,200 --> 00:18:04,720
we are trying to solve

00:18:00,640 --> 00:18:07,200
our internal team required to have

00:18:04,720 --> 00:18:08,240
such a shared connection for their l4

00:18:07,200 --> 00:18:10,160
services

00:18:08,240 --> 00:18:12,880
to minimize the cost connections and

00:18:10,160 --> 00:18:15,039
wanted application to be portable

00:18:12,880 --> 00:18:17,679
primary motivation is cost of course we

00:18:15,039 --> 00:18:20,000
wanted our solution to be user friendly

00:18:17,679 --> 00:18:22,320
so that i do not have to remember many

00:18:20,000 --> 00:18:24,320
ip addresses of all the load balancers i

00:18:22,320 --> 00:18:27,039
am creating

00:18:24,320 --> 00:18:29,039
and also have a uniform va to manage the

00:18:27,039 --> 00:18:30,080
infrastructure the main problem can be

00:18:29,039 --> 00:18:33,200
broken into three

00:18:30,080 --> 00:18:35,679
simple problems how do i open

00:18:33,200 --> 00:18:36,880
additional ports on a load balancer to

00:18:35,679 --> 00:18:39,679
make a chair

00:18:36,880 --> 00:18:40,000
how do i associate the ports to the back

00:18:39,679 --> 00:18:43,120
00:18:40,000 --> 00:18:43,760
parts and how do i give this accessing

00:18:43,120 --> 00:18:46,000
information

00:18:43,760 --> 00:18:48,000
back to the end user for example the

00:18:46,000 --> 00:18:49,520
user should be able to query a simple

00:18:48,000 --> 00:18:51,760
kubernetes object

00:18:49,520 --> 00:18:53,200
to get the ip address and report of a

00:18:51,760 --> 00:18:57,520
well-known

00:18:53,200 --> 00:19:00,799
service that he wants to connect to

00:18:57,520 --> 00:19:03,440
so if you look here uh i have a custom

00:19:00,799 --> 00:19:06,559
resource object called sharedmp

00:19:03,440 --> 00:19:08,480
slp in chart uh the expected goal is to

00:19:06,559 --> 00:19:09,039
use information through cubecontrol and

00:19:08,480 --> 00:19:10,880
you

00:19:09,039 --> 00:19:13,120
and be able to connect to your

00:19:10,880 --> 00:19:14,240
application and load balancer being

00:19:13,120 --> 00:19:16,559
transparent

00:19:14,240 --> 00:19:17,919
you see the custom object here called

00:19:16,559 --> 00:19:19,919
shared bit that is providing

00:19:17,919 --> 00:19:22,799
connectivity information we need and

00:19:19,919 --> 00:19:24,640
also refer to the cloud infrastructures

00:19:22,799 --> 00:19:27,840
load balancer just in case

00:19:24,640 --> 00:19:30,400
um like uh shared lb

00:19:27,840 --> 00:19:32,960
has four instances here four instances

00:19:30,400 --> 00:19:35,760
of using the same external ip

00:19:32,960 --> 00:19:36,960
which is the ipl the load balancer uh

00:19:35,760 --> 00:19:39,600
with different ports

00:19:36,960 --> 00:19:41,840
4001 connects to a backend application

00:19:39,600 --> 00:19:44,080
4002 connects to a different backend

00:19:41,840 --> 00:19:47,280
application

00:19:44,080 --> 00:19:48,559
to simplify the view now we have instead

00:19:47,280 --> 00:19:50,400
of two

00:19:48,559 --> 00:19:52,000
load balances we have one shared load

00:19:50,400 --> 00:19:53,760
balancer with two ports port here and

00:19:52,000 --> 00:19:57,200
port be connected to the

00:19:53,760 --> 00:19:59,039
different backend applications

00:19:57,200 --> 00:20:02,320
now my application is sharing the load

00:19:59,039 --> 00:20:05,039
balancer this is more cost effective

00:20:02,320 --> 00:20:06,159
user friendly minimum operation efforts

00:20:05,039 --> 00:20:08,400
reusing existing

00:20:06,159 --> 00:20:09,679
communities as assets without

00:20:08,400 --> 00:20:12,320
reinventing the wheel

00:20:09,679 --> 00:20:14,159
consistent with kubernetes programming

00:20:12,320 --> 00:20:17,120
model

00:20:14,159 --> 00:20:18,799
let me explain this in detail a load

00:20:17,120 --> 00:20:21,039
balancer's incoming port

00:20:18,799 --> 00:20:22,400
is connected to a north port of an

00:20:21,039 --> 00:20:25,440
internal service

00:20:22,400 --> 00:20:28,080
which we create to a target port of the

00:20:25,440 --> 00:20:30,799
pod which is running the workload

00:20:28,080 --> 00:20:31,280
three things are happening here we

00:20:30,799 --> 00:20:33,440
derive

00:20:31,280 --> 00:20:35,200
information from the custom resource

00:20:33,440 --> 00:20:37,200
object that the user created

00:20:35,200 --> 00:20:38,880
and we create a service community

00:20:37,200 --> 00:20:40,960
service in the back end

00:20:38,880 --> 00:20:42,480
and we create or use an existing load

00:20:40,960 --> 00:20:45,520
balancer

00:20:42,480 --> 00:20:48,559
by just opening a port or

00:20:45,520 --> 00:20:49,919
and then associate the service with the

00:20:48,559 --> 00:20:51,919
load balancer

00:20:49,919 --> 00:20:53,520
so the entire connectivity happens for

00:20:51,919 --> 00:20:55,600
us

00:20:53,520 --> 00:20:57,440
so as you can see there are five steps

00:20:55,600 --> 00:20:58,720
here in the diagram step one user

00:20:57,440 --> 00:21:02,000
creates a shared

00:20:58,720 --> 00:21:05,280
lpa with the information we need using

00:21:02,000 --> 00:21:08,159
that we either find an existing lp

00:21:05,280 --> 00:21:09,280
load balancer or we create a new load

00:21:08,159 --> 00:21:12,000
balancer

00:21:09,280 --> 00:21:12,960
and we create a port on that and we also

00:21:12,000 --> 00:21:16,480
in step 3

00:21:12,960 --> 00:21:18,480
create a normal kubernetes surface

00:21:16,480 --> 00:21:20,559
uh with source ports and target boards

00:21:18,480 --> 00:21:23,280
and proper label selectors

00:21:20,559 --> 00:21:24,080
uh derived from the spec of the cr

00:21:23,280 --> 00:21:27,840
object we

00:21:24,080 --> 00:21:28,960
created and then this is connected to

00:21:27,840 --> 00:21:33,200
the back-end

00:21:28,960 --> 00:21:36,320
workload using kubernetes mechanisms

00:21:33,200 --> 00:21:38,240
and once we do that in step four we get

00:21:36,320 --> 00:21:40,400
the information about the ip and the

00:21:38,240 --> 00:21:42,960
port that we have created

00:21:40,400 --> 00:21:44,080
like put here and put that information

00:21:42,960 --> 00:21:47,520
back into the customers

00:21:44,080 --> 00:21:49,440
subject and step 5 user runs queue

00:21:47,520 --> 00:21:50,080
control command to get that information

00:21:49,440 --> 00:21:54,159
from the

00:21:50,080 --> 00:21:56,559
shared ld we use

00:21:54,159 --> 00:21:57,679
cloud providers sdk to do things like

00:21:56,559 --> 00:21:59,919
open a port

00:21:57,679 --> 00:22:01,600
configure security groups incoming rule

00:21:59,919 --> 00:22:04,559
to make firewall happy

00:22:01,600 --> 00:22:06,960
to pass through traffic to this port

00:22:04,559 --> 00:22:09,200
create internal service

00:22:06,960 --> 00:22:11,039
that uses kubernetes network to talk to

00:22:09,200 --> 00:22:12,799
the workload

00:22:11,039 --> 00:22:14,640
making sure that traffic hits the

00:22:12,799 --> 00:22:18,799
internal service using

00:22:14,640 --> 00:22:20,080
our lb port rules um we are using crds

00:22:18,799 --> 00:22:22,960
as a facade

00:22:20,080 --> 00:22:26,159
for the end user and namespace crds are

00:22:22,960 --> 00:22:28,559
used so there are no security concerns

00:22:26,159 --> 00:22:30,880
create real load balancer on-demand

00:22:28,559 --> 00:22:34,000
basis and manage the life cycle of

00:22:30,880 --> 00:22:37,200
it we make a

00:22:34,000 --> 00:22:39,120
and configurable which is the number of

00:22:37,200 --> 00:22:39,600
the capacity of the road balances so you

00:22:39,120 --> 00:22:41,760
can

00:22:39,600 --> 00:22:43,760
by default it is fine you can have five

00:22:41,760 --> 00:22:44,880
connections on the load balancer or you

00:22:43,760 --> 00:22:46,960
can tune it to

00:22:44,880 --> 00:22:48,960
whatever number you want depending on

00:22:46,960 --> 00:22:50,559
the criteria on the throughput of your

00:22:48,960 --> 00:22:54,320
workloads or you know

00:22:50,559 --> 00:22:58,480
um the latency on the workflows we adopt

00:22:54,320 --> 00:23:01,039
all the best practices um for the

00:22:58,480 --> 00:23:01,600
for the controller uh like controller

00:23:01,039 --> 00:23:03,840
ref

00:23:01,600 --> 00:23:04,720
uh finalizers etc to clean up the

00:23:03,840 --> 00:23:08,080
objects

00:23:04,720 --> 00:23:10,240
let us do a demo i mean our solution

00:23:08,080 --> 00:23:12,799
works on all the cloud providers

00:23:10,240 --> 00:23:14,080
but right now i want to show you on

00:23:12,799 --> 00:23:19,120
google cloud

00:23:14,080 --> 00:23:22,799
um we have a three node cluster on gke

00:23:19,120 --> 00:23:25,039
and we have uh let's create

00:23:22,799 --> 00:23:26,880
four deployments for workloads running

00:23:25,039 --> 00:23:28,640
in gke

00:23:26,880 --> 00:23:31,360
we have a definition of the shared load

00:23:28,640 --> 00:23:34,480
balancer on gke

00:23:31,360 --> 00:23:36,880
and now um if you look at the

00:23:34,480 --> 00:23:38,480
shared shared will be the customers to

00:23:36,880 --> 00:23:41,039
subjects there are none

00:23:38,480 --> 00:23:42,000
so we need to create four for the four

00:23:41,039 --> 00:23:44,480
workloads

00:23:42,000 --> 00:23:45,279
so we created all the four at this point

00:23:44,480 --> 00:23:47,679
in time our

00:23:45,279 --> 00:23:48,559
controller reacts and creates the

00:23:47,679 --> 00:23:50,400
balancer

00:23:48,559 --> 00:23:53,120
how many load balancers do you think it

00:23:50,400 --> 00:23:56,159
created it creates only one

00:23:53,120 --> 00:23:57,120
even though we you pushed four shared

00:23:56,159 --> 00:23:59,520
lbase

00:23:57,120 --> 00:24:01,440
to be created concurrently the

00:23:59,520 --> 00:24:03,039
controller is smart enough to create one

00:24:01,440 --> 00:24:06,960
load balancer for you

00:24:03,039 --> 00:24:10,159
if you go to the gke console and look at

00:24:06,960 --> 00:24:12,799
here for the load balancers

00:24:10,159 --> 00:24:14,880
there is one load balancer created and

00:24:12,799 --> 00:24:16,400
of course we create this load balancer

00:24:14,880 --> 00:24:19,600
with one port

00:24:16,400 --> 00:24:20,640
default dummy port um for creation you

00:24:19,600 --> 00:24:23,679
need that so

00:24:20,640 --> 00:24:25,360
3 3 3 3 3 is used as that report you can

00:24:23,679 --> 00:24:27,279
ignore that

00:24:25,360 --> 00:24:29,120
but now we have we started processing

00:24:27,279 --> 00:24:30,640
this course shared and based so that

00:24:29,120 --> 00:24:33,200
means we are going to open

00:24:30,640 --> 00:24:35,360
four ports and as you can see two of

00:24:33,200 --> 00:24:36,640
those shared lds are already processed

00:24:35,360 --> 00:24:39,919
and you got the

00:24:36,640 --> 00:24:42,559
uh ip address association report numbers

00:24:39,919 --> 00:24:44,320
and associated services will show you

00:24:42,559 --> 00:24:44,960
the port numbers we created on the load

00:24:44,320 --> 00:24:52,159
balances

00:24:44,960 --> 00:24:53,840
31 725 30205 31 878 31 174 all these

00:24:52,159 --> 00:24:56,799
ports are created on the load balancer

00:24:53,840 --> 00:24:59,840
we go back to the gte console limits

00:24:56,799 --> 00:25:00,480
refresh you see all those four ports are

00:24:59,840 --> 00:25:03,919
open

00:25:00,480 --> 00:25:07,679
on the load balancer not only that we

00:25:03,919 --> 00:25:11,600
also created a firewall rule on this

00:25:07,679 --> 00:25:14,080
network so we open

00:25:11,600 --> 00:25:16,080
all the ports from thirty thousand to

00:25:14,080 --> 00:25:18,320
thirty two thousand seven sixty seven

00:25:16,080 --> 00:25:21,760
for both tcp and udp

00:25:18,320 --> 00:25:23,840
uh we we can only do uh one five volt

00:25:21,760 --> 00:25:25,520
rule for each of the ports we open but

00:25:23,840 --> 00:25:27,919
this way you know it's simpler we'll

00:25:25,520 --> 00:25:29,440
have one row and

00:25:27,919 --> 00:25:32,320
as you can see there is only one

00:25:29,440 --> 00:25:34,880
external ip that we are paying for

00:25:32,320 --> 00:25:36,799
uh with four forwarding rules for four

00:25:34,880 --> 00:25:40,559
workloads that we are running inside

00:25:36,799 --> 00:25:42,720
our kubernetes cluster so

00:25:40,559 --> 00:25:43,840
each of those ports correspond to one

00:25:42,720 --> 00:25:46,400
workflow

00:25:43,840 --> 00:25:47,520
and in the selfies now they're all

00:25:46,400 --> 00:25:49,520
populated

00:25:47,520 --> 00:25:52,400
with the external ip and the port so you

00:25:49,520 --> 00:25:54,880
can cat any of these ips and ports and

00:25:52,400 --> 00:25:58,400
you should be able to use the backend

00:25:54,880 --> 00:26:00,559
application now going back

00:25:58,400 --> 00:26:00,559
to

00:26:02,080 --> 00:26:10,799
our presentation as you can see

00:26:06,480 --> 00:26:13,760
now we have um presentations about

00:26:10,799 --> 00:26:14,480
similar demos available for azure um

00:26:13,760 --> 00:26:17,679
amazon

00:26:14,480 --> 00:26:19,679
and ibm cloud but in the interest of

00:26:17,679 --> 00:26:21,360
time i would like to conclude and our

00:26:19,679 --> 00:26:22,840
information is provided here if you are

00:26:21,360 --> 00:26:26,320
interested in this

00:26:22,840 --> 00:26:29,200
uh solution you can

00:26:26,320 --> 00:26:30,960
you can reach out to us with that i'll

00:26:29,200 --> 00:26:33,200
open up

00:26:30,960 --> 00:26:34,080
for questions thanks for listening to us

00:26:33,200 --> 00:26:37,120
i'm

00:26:34,080 --> 00:26:37,120
grateful to be here

00:26:37,200 --> 00:26:53,840
and ready for questions thank you

00:27:13,840 --> 00:27:18,240
hey everyone uh so we're still sitting

00:27:16,880 --> 00:27:19,760
around for the next few minutes just to

00:27:18,240 --> 00:27:22,159
answer any questions

00:27:19,760 --> 00:27:24,799
you guys might have about the

00:27:22,159 --> 00:27:26,880
presentation we have one question

00:27:24,799 --> 00:27:28,159
here in the chat about what the

00:27:26,880 --> 00:27:31,039
potential

00:27:28,159 --> 00:27:31,360
cost savings of this approach would be

00:27:31,039 --> 00:27:32,880
and

00:27:31,360 --> 00:27:34,399
it's it's really dependent on which

00:27:32,880 --> 00:27:37,840
cloud you're using

00:27:34,399 --> 00:27:39,200
a lot of this is based on prices that

00:27:37,840 --> 00:27:41,840
get charged

00:27:39,200 --> 00:27:42,880
as referred to the number of load

00:27:41,840 --> 00:27:44,960
balances

00:27:42,880 --> 00:27:47,600
load balancers created or if you have a

00:27:44,960 --> 00:27:49,679
significant amount of services

00:27:47,600 --> 00:27:51,120
so it's really going to be dependent on

00:27:49,679 --> 00:27:54,159
your cloud provider

00:27:51,120 --> 00:27:55,120
so it's a little off to try to figure

00:27:54,159 --> 00:27:57,679
out

00:27:55,120 --> 00:27:58,880
exactly how much cloud savings or

00:27:57,679 --> 00:28:02,320
exactly how much

00:27:58,880 --> 00:28:05,360
dollar amount it would be

00:28:02,320 --> 00:28:07,679
on the second question we see

00:28:05,360 --> 00:28:10,320
can the sherlock balance or crd be made

00:28:07,679 --> 00:28:15,039
to work with a bare metal infrastructure

00:28:10,320 --> 00:28:15,039
yes it could assuming that you're using

00:28:15,120 --> 00:28:19,360
some sort of bare metal infrastructure

00:28:17,760 --> 00:28:21,279
that provides

00:28:19,360 --> 00:28:23,679
apis for gathering this sort of

00:28:21,279 --> 00:28:26,559
information

00:28:23,679 --> 00:28:28,000
such as in the openstack world there's

00:28:26,559 --> 00:28:30,240
different types of

00:28:28,000 --> 00:28:31,360
apis available that some people have

00:28:30,240 --> 00:28:33,520
ported over

00:28:31,360 --> 00:28:35,120
to work on bare metal as well so if

00:28:33,520 --> 00:28:38,159
there's an api one could

00:28:35,120 --> 00:28:39,760
be able to do it another question what's

00:28:38,159 --> 00:28:41,440
the performance of the integrated load

00:28:39,760 --> 00:28:42,399
balancer assume is running in the kernel

00:28:41,440 --> 00:28:44,159
space

00:28:42,399 --> 00:28:45,600
how to control guarantee some level

00:28:44,159 --> 00:28:47,039
performance

00:28:45,600 --> 00:28:48,880
so this is also going to be dependent on

00:28:47,039 --> 00:28:50,640
the the load bouncer

00:28:48,880 --> 00:28:52,799
of your choice depending on the cloud

00:28:50,640 --> 00:28:55,840
provider since all we're really doing

00:28:52,799 --> 00:28:56,480
with the crd is talking to the load

00:28:55,840 --> 00:28:59,440
balancer

00:28:56,480 --> 00:29:00,480
that you already have it's going to be

00:28:59,440 --> 00:29:03,520
the same level of

00:29:00,480 --> 00:29:04,320
performance as what you're already

00:29:03,520 --> 00:29:06,240
getting

00:29:04,320 --> 00:29:13,840
from that cloud provider we're just

00:29:06,240 --> 00:29:13,840
slotting in more information

00:29:16,960 --> 00:29:24,960
uh let's see fourth question on

00:29:20,640 --> 00:29:27,200
running a private infrastructure there's

00:29:24,960 --> 00:29:28,559
uh four million requests per second

00:29:27,200 --> 00:29:31,600
private cloud

00:29:28,559 --> 00:29:33,120
uh and they're worried about

00:29:31,600 --> 00:29:35,360
you know what what to do since they

00:29:33,120 --> 00:29:38,559
don't have anything capacity

00:29:35,360 --> 00:29:40,640
on the load balancers what technology

00:29:38,559 --> 00:29:41,360
can station multiple load balancers like

00:29:40,640 --> 00:29:43,200
nginx

00:29:41,360 --> 00:29:44,720
to stream the inbound requests over all

00:29:43,200 --> 00:29:47,120
the load balancers

00:29:44,720 --> 00:29:48,320
uh this is kind of a general kubernetes

00:29:47,120 --> 00:29:51,679
question

00:29:48,320 --> 00:29:53,200
uh you can often have uh like an overall

00:29:51,679 --> 00:29:56,320
load balancer

00:29:53,200 --> 00:29:56,960
atop some of these things to distribute

00:29:56,320 --> 00:30:00,399
the

00:29:56,960 --> 00:30:03,919
request there's nothing in

00:30:00,399 --> 00:30:05,600
kubernetes uh natively that does this

00:30:03,919 --> 00:30:07,919
it's almost more of like a layering

00:30:05,600 --> 00:30:11,120
approach such that

00:30:07,919 --> 00:30:13,200
you can distribute it out uh with

00:30:11,120 --> 00:30:14,240
like a master global load balancer or

00:30:13,200 --> 00:30:16,799
you can use a project

00:30:14,240 --> 00:30:17,600
similar to istio or roll your own with

00:30:16,799 --> 00:30:19,039
envoy

00:30:17,600 --> 00:30:21,039
so that you can have more of a global

00:30:19,039 --> 00:30:23,120
view and space it out

00:30:21,039 --> 00:30:24,320
based on requests or whatever circuit

00:30:23,120 --> 00:30:27,440
breaker policies

00:30:24,320 --> 00:30:27,440
that you have set up for that

00:30:30,480 --> 00:30:36,880
i think that's looking for questions

00:30:34,159 --> 00:30:38,080
if you have follow um replies or

00:30:36,880 --> 00:30:40,799
questions

00:30:38,080 --> 00:30:44,080
um please visit us in the number two

00:30:40,799 --> 00:30:45,600
spotlight of the networking

00:30:44,080 --> 00:30:51,840
slack channel and we can answer more

00:30:45,600 --> 00:30:51,840
questions in there

00:31:03,840 --> 00:31:05,919

YouTube URL: https://www.youtube.com/watch?v=7R9yMIyyPfI


