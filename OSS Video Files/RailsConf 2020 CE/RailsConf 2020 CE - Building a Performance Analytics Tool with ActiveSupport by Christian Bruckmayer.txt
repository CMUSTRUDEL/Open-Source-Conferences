Title: RailsConf 2020 CE - Building a Performance Analytics Tool with ActiveSupport by Christian Bruckmayer
Publication date: 2020-04-24
Playlist: RailsConf 2020 CE
Description: 
	Building a Performance Analytics Tool with ActiveSupport by Christian Bruckmayer

"Setting up a performance analytics tool like NewRelic or Skylight is one of the first things many developers do in a new project. However, have you ever wondered how these tools work under the hood?

In this talk, we will build a basic performance analytics tool from scratch. We will deep dive into ActiveSupport instrumentations to collect, group and normalise metrics from your app. To persist these metrics, we will use a time series database and visualise the data on a dashboard. By the end of the talk, you will know how your favourite performance analytics tool works."

__________

"Christian Bruckmayer originally from Nuremberg, Germany, but now calls Bristol, UK, his home. In his day job, he makes everyday cooking fun at Cookpad, the best place to find and share home cooked recipes. Since 2014 he is an avid open source contributor hacking for instance on JRuby, openSUSE Linux or various gems.

If he's not hacking Ruby, he's out doing what 'young' people do: traveling the world, skiing in the alps or going to concerts of his favourite band."
Captions: 
	00:00:09,120 --> 00:00:14,350
hello everyone and welcome to race cones

00:00:12,520 --> 00:00:16,779
this year is a little bit different and

00:00:14,350 --> 00:00:18,730
we cannot meet in Portland I hope you

00:00:16,779 --> 00:00:20,770
still enjoy all the talks we recorded

00:00:18,730 --> 00:00:23,320
for you and thank you for watching my

00:00:20,770 --> 00:00:25,539
talk I would like to start my talk today

00:00:23,320 --> 00:00:27,850
with a quote from Richard Feynman who

00:00:25,539 --> 00:00:31,449
was a famous physicist and even got a

00:00:27,850 --> 00:00:31,960
Nobel Prize in 1965 at the time of his

00:00:31,449 --> 00:00:34,090
death

00:00:31,960 --> 00:00:36,400
they found this quote what I cannot

00:00:34,090 --> 00:00:39,220
create I do not understand

00:00:36,400 --> 00:00:42,700
on his blackboard but what did he mean

00:00:39,220 --> 00:00:45,370
with it by building something from

00:00:42,700 --> 00:00:47,969
scratch step by step we will develop a

00:00:45,370 --> 00:00:50,469
firm understanding of the problem

00:00:47,969 --> 00:00:52,390
sometimes we have to admit that we do

00:00:50,469 --> 00:00:55,090
not understand something we thought we

00:00:52,390 --> 00:00:57,850
already to understand and if we

00:00:55,090 --> 00:01:00,100
eventually succeed it is the ultimate

00:00:57,850 --> 00:01:02,920
proof that we understood the problem at

00:01:00,100 --> 00:01:05,170
hand today we will build a performance

00:01:02,920 --> 00:01:07,360
monitoring tool from ground up which

00:01:05,170 --> 00:01:09,880
will help us to use these tools better

00:01:07,360 --> 00:01:11,950
in the future my name is Krisha

00:01:09,880 --> 00:01:14,350
brookmire you can find me on Twitter web

00:01:11,950 --> 00:01:17,380
at brookmire or on my blog brookmire

00:01:14,350 --> 00:01:19,570
dotnet I live in the southwest of

00:01:17,380 --> 00:01:22,420
England in the beautiful city of Bristol

00:01:19,570 --> 00:01:26,619
and I work for cook pad an online recipe

00:01:22,420 --> 00:01:28,180
sharing application let's talk a little

00:01:26,619 --> 00:01:31,090
bit about performance monitoring

00:01:28,180 --> 00:01:33,580
performance monitoring helps us to spot

00:01:31,090 --> 00:01:36,340
and identify performance bottlenecks in

00:01:33,580 --> 00:01:39,250
our applications many of you probably

00:01:36,340 --> 00:01:42,460
use software as a service platform like

00:01:39,250 --> 00:01:44,320
New Relic or skylight today we will

00:01:42,460 --> 00:01:46,780
build a tool called active monitoring

00:01:44,320 --> 00:01:48,280
together which will implement a subset

00:01:46,780 --> 00:01:51,070
of the features you know from a new

00:01:48,280 --> 00:01:53,439
relic or skylight because we only have

00:01:51,070 --> 00:01:56,200
30 minutes time our implementation will

00:01:53,439 --> 00:02:00,340
only cover response time and database

00:01:56,200 --> 00:02:03,220
operation metrics the tool will have

00:02:00,340 --> 00:02:05,890
three components a Ruby on Rails plugin

00:02:03,220 --> 00:02:09,579
a data storage and a dashboard for data

00:02:05,890 --> 00:02:12,370
visualizations everything what I present

00:02:09,579 --> 00:02:14,140
today is also published on my blog so if

00:02:12,370 --> 00:02:17,370
something is a little bit too fast you

00:02:14,140 --> 00:02:19,959
can look it up on my blog article two

00:02:17,370 --> 00:02:21,890
most of the knowledge I present today is

00:02:19,959 --> 00:02:24,350
extracted from the influx TV

00:02:21,890 --> 00:02:27,550
hm which is an open source performance

00:02:24,350 --> 00:02:30,170
monitoring tool I maintain with a friend

00:02:27,550 --> 00:02:32,150
because we only have 30 minutes time

00:02:30,170 --> 00:02:34,550
today we will not discuss how to

00:02:32,150 --> 00:02:36,860
actually spot and fix performance issues

00:02:34,550 --> 00:02:38,959
if you're interested in this topic I

00:02:36,860 --> 00:02:42,260
recommend to watch the talk from meteor

00:02:38,959 --> 00:02:44,660
Quebec from last year's race conf this

00:02:42,260 --> 00:02:46,250
table is divided into four chapters in

00:02:44,660 --> 00:02:48,290
our first chapter we will start

00:02:46,250 --> 00:02:51,260
collecting data with the help of active

00:02:48,290 --> 00:02:54,050
support after that we will continue with

00:02:51,260 --> 00:02:58,040
storing processing and visualizing of

00:02:54,050 --> 00:03:00,170
our collected data as mentioned in the

00:02:58,040 --> 00:03:02,030
beginning we will use active support to

00:03:00,170 --> 00:03:04,250
build our performance monitoring tool

00:03:02,030 --> 00:03:08,239
and active support is a utility

00:03:04,250 --> 00:03:11,090
framework of Ruby on Rails the more

00:03:08,239 --> 00:03:13,610
popular features of active support the

00:03:11,090 --> 00:03:16,489
Ruby core extensions some examples are

00:03:13,610 --> 00:03:21,200
the date and area extensions like from

00:03:16,489 --> 00:03:22,760
now to sentence or many but active

00:03:21,200 --> 00:03:24,980
supporters also shipped with an

00:03:22,760 --> 00:03:26,959
instrumentation framework you can

00:03:24,980 --> 00:03:29,390
imagine this a little bit like an event

00:03:26,959 --> 00:03:31,250
stream so at the top we instrument an

00:03:29,390 --> 00:03:34,790
event with a name add which has a

00:03:31,250 --> 00:03:38,570
payload containing a term and we will

00:03:34,790 --> 00:03:40,850
execute 1+1 in the function block and at

00:03:38,570 --> 00:03:43,220
the bottom we subscribe to this event at

00:03:40,850 --> 00:03:45,709
and every time the code at the top gets

00:03:43,220 --> 00:03:48,829
executed the SUBSCRIBE block gets

00:03:45,709 --> 00:03:51,290
executed as well and in it we just look

00:03:48,829 --> 00:03:54,739
out the start and the payload of the

00:03:51,290 --> 00:03:56,570
term luckily Ruby on Rails already ships

00:03:54,739 --> 00:03:58,579
with more than 50 hooks into the

00:03:56,570 --> 00:04:02,180
framework which helps us to extract

00:03:58,579 --> 00:04:04,670
metrics from our apps a few events we

00:04:02,180 --> 00:04:06,980
will use today are the start processing

00:04:04,670 --> 00:04:10,489
event which gets fired before controller

00:04:06,980 --> 00:04:12,709
action gets executed the process action

00:04:10,489 --> 00:04:15,440
event which gets fired after controller

00:04:12,709 --> 00:04:17,780
action was executed an active record

00:04:15,440 --> 00:04:22,130
even provides an event for each SQL

00:04:17,780 --> 00:04:23,960
query which gets executed it's a first

00:04:22,130 --> 00:04:26,300
step I would like to re-implement this

00:04:23,960 --> 00:04:29,960
framework so we can see how it actually

00:04:26,300 --> 00:04:30,760
works let's start with a subscribe

00:04:29,960 --> 00:04:34,510
method

00:04:30,760 --> 00:04:37,660
we need a class attribute notifier which

00:04:34,510 --> 00:04:40,030
is an empty hash and the SUBSCRIBE

00:04:37,660 --> 00:04:43,570
method accepts a name and a block which

00:04:40,030 --> 00:04:45,850
we will store in this hash the

00:04:43,570 --> 00:04:49,270
instrument method also accepts a name

00:04:45,850 --> 00:04:52,000
and the payload we store that time

00:04:49,270 --> 00:04:53,800
before we execute the block we execute

00:04:52,000 --> 00:04:57,520
the block and then we store the time

00:04:53,800 --> 00:04:59,680
after we execute the proc as well and in

00:04:57,520 --> 00:05:01,960
the second step we just iterate about

00:04:59,680 --> 00:05:05,110
over all the callbacks we stored in the

00:05:01,960 --> 00:05:07,240
SUBSCRIBE method and we call them with a

00:05:05,110 --> 00:05:12,610
payload and merge entry at the start in

00:05:07,240 --> 00:05:14,590
the finish time as already mentioned

00:05:12,610 --> 00:05:17,410
Ruby and raised already ships with more

00:05:14,590 --> 00:05:19,510
than 50 hooks into the framework but

00:05:17,410 --> 00:05:21,790
sometimes this is not enough and we want

00:05:19,510 --> 00:05:25,240
to also instrument methods from external

00:05:21,790 --> 00:05:27,250
libraries like Redis or memcache in this

00:05:25,240 --> 00:05:29,830
case we need to install a monkey patch

00:05:27,250 --> 00:05:31,780
and wrap the function insert into an

00:05:29,830 --> 00:05:34,510
instrument block which we can later

00:05:31,780 --> 00:05:36,880
subscribe to in this example we just

00:05:34,510 --> 00:05:39,970
monkey patch the call method of the

00:05:36,880 --> 00:05:41,830
Redis library in this chapter we had a

00:05:39,970 --> 00:05:44,200
look at active support and it's

00:05:41,830 --> 00:05:46,210
instrumentation framework we now know

00:05:44,200 --> 00:05:48,460
which hooks Ruby and rays already

00:05:46,210 --> 00:05:51,310
provides and we can use to extract data

00:05:48,460 --> 00:05:53,470
from our apps in this chapter we will

00:05:51,310 --> 00:05:56,170
implement the storage of the collected

00:05:53,470 --> 00:05:58,180
metrics as a first step we need to

00:05:56,170 --> 00:06:00,340
install the process action subscriber

00:05:58,180 --> 00:06:03,250
which we do in a Ruby on Rails plugin

00:06:00,340 --> 00:06:06,070
plugins our way to extend race in hook

00:06:03,250 --> 00:06:07,930
into its internals the process action

00:06:06,070 --> 00:06:09,970
event payload provides us with s

00:06:07,930 --> 00:06:12,640
information initially we will only

00:06:09,970 --> 00:06:15,580
extract the control and H name and the

00:06:12,640 --> 00:06:17,920
execution runtime later we could also

00:06:15,580 --> 00:06:19,930
extend this to also store the response

00:06:17,920 --> 00:06:22,780
former the method and the response

00:06:19,930 --> 00:06:24,940
status weather information from the

00:06:22,780 --> 00:06:28,120
payload we could build a table like this

00:06:24,940 --> 00:06:30,100
we have an ID at times them and we can

00:06:28,120 --> 00:06:32,500
calculate the duration from the start

00:06:30,100 --> 00:06:34,270
and finish times of the event we have

00:06:32,500 --> 00:06:36,670
the hook name which is the name of the

00:06:34,270 --> 00:06:38,160
event and the location is a control and

00:06:36,670 --> 00:06:41,169
each name

00:06:38,160 --> 00:06:45,580
so this looks a lot like relational

00:06:41,169 --> 00:06:48,220
database and most relational databases

00:06:45,580 --> 00:06:51,760
implement a create read update and

00:06:48,220 --> 00:06:54,580
delete operation and these operations

00:06:51,760 --> 00:06:58,450
very good to our controller actions

00:06:54,580 --> 00:07:01,030
which often have create show update and

00:06:58,450 --> 00:07:02,950
destroy H but for the performance

00:07:01,030 --> 00:07:04,960
monitoring tool we are planning to

00:07:02,950 --> 00:07:07,540
implement we don't really need all these

00:07:04,960 --> 00:07:10,540
operations of course we need a create

00:07:07,540 --> 00:07:13,389
operation because we want to write our

00:07:10,540 --> 00:07:16,450
metrics to the database but for each

00:07:13,389 --> 00:07:20,620
request we will write many many metrics

00:07:16,450 --> 00:07:22,540
to the database we plan to write one

00:07:20,620 --> 00:07:24,610
metric for the response time and

00:07:22,540 --> 00:07:26,710
additionally metrics for each SQL

00:07:24,610 --> 00:07:29,560
queries in the future we might even

00:07:26,710 --> 00:07:33,850
extend this to include view render times

00:07:29,560 --> 00:07:36,610
or background chops we will of course

00:07:33,850 --> 00:07:39,400
read the data but only timely related

00:07:36,610 --> 00:07:42,280
together so we will read the metrics of

00:07:39,400 --> 00:07:43,020
the last three hours or the last 24

00:07:42,280 --> 00:07:46,090
hours

00:07:43,020 --> 00:07:48,510
we will almost never update any of the

00:07:46,090 --> 00:07:51,340
metrics if a metric is persisted after

00:07:48,510 --> 00:07:53,770
execution of a control action we will

00:07:51,340 --> 00:07:56,350
never go back and change it because the

00:07:53,770 --> 00:08:00,310
response time won't change after it's

00:07:56,350 --> 00:08:01,870
written sometimes we will delete metrics

00:08:00,310 --> 00:08:04,240
but this is more like a garbage

00:08:01,870 --> 00:08:06,550
collection if you're only interested in

00:08:04,240 --> 00:08:08,620
the metrics for the last month once a

00:08:06,550 --> 00:08:14,889
month we can run a background job to

00:08:08,620 --> 00:08:16,450
delete everything which is older if you

00:08:14,889 --> 00:08:18,820
have a look how most relational

00:08:16,450 --> 00:08:21,010
databases are implemented we will see

00:08:18,820 --> 00:08:24,789
that they use a data structure like a B

00:08:21,010 --> 00:08:29,080
tree or B+ tree and the B tree contains

00:08:24,789 --> 00:08:31,840
several pages with a data and each page

00:08:29,080 --> 00:08:34,450
has point us to the next page if one of

00:08:31,840 --> 00:08:37,810
these pages is getting too big it's

00:08:34,450 --> 00:08:41,610
getting split it in different ways which

00:08:37,810 --> 00:08:44,260
also contain pointers to the new pages

00:08:41,610 --> 00:08:46,750
because we will write a lot of metrics

00:08:44,260 --> 00:08:48,940
we would constantly need to create new

00:08:46,750 --> 00:08:51,450
pages and write to the disk this

00:08:48,940 --> 00:08:54,790
wouldn't be very efficient

00:08:51,450 --> 00:08:58,149
and in the last few years a different

00:08:54,790 --> 00:09:02,649
type of database but very popular which

00:08:58,149 --> 00:09:05,110
is called a time series database so time

00:09:02,649 --> 00:09:07,899
series databases are nothing new they

00:09:05,110 --> 00:09:10,630
already exist since 30 or 40 years but

00:09:07,899 --> 00:09:14,560
mostly in the finance industry one use

00:09:10,630 --> 00:09:17,230
case is if we want to track the price of

00:09:14,560 --> 00:09:21,370
a stock over time we could store this is

00:09:17,230 --> 00:09:22,779
in a time series database and the reason

00:09:21,370 --> 00:09:26,560
why they got very popular in recent

00:09:22,779 --> 00:09:29,260
years is Internet of Things we now have

00:09:26,560 --> 00:09:32,920
a lot of devices with sensors and we

00:09:29,260 --> 00:09:34,839
constantly run want to write the results

00:09:32,920 --> 00:09:39,820
of these sensors for instance like a

00:09:34,839 --> 00:09:41,769
temperature or humidity a few popular

00:09:39,820 --> 00:09:44,890
time series databases you might have

00:09:41,769 --> 00:09:47,800
heard about in flux DB Prometheus or

00:09:44,890 --> 00:09:52,450
graphite for the rest of the talk I will

00:09:47,800 --> 00:09:55,180
use now in flux TB if you have a look at

00:09:52,450 --> 00:09:57,459
the data schema in in flux DB we will

00:09:55,180 --> 00:10:01,870
notice that this is very similar to the

00:09:57,459 --> 00:10:04,209
schema in a relational database instead

00:10:01,870 --> 00:10:08,199
of tables however we have measurements

00:10:04,209 --> 00:10:11,290
and each measurements has several

00:10:08,199 --> 00:10:15,430
metrics and each metrics has x them

00:10:11,290 --> 00:10:18,399
which is the primary key additionally we

00:10:15,430 --> 00:10:21,970
have fields in this case it's a duration

00:10:18,399 --> 00:10:24,390
and text in this case it's hook name and

00:10:21,970 --> 00:10:27,430
location and the main difference between

00:10:24,390 --> 00:10:30,579
field and attack is the text getting

00:10:27,430 --> 00:10:34,839
indexed and we can use them in filtering

00:10:30,579 --> 00:10:36,820
our metrics why a time series database

00:10:34,839 --> 00:10:39,579
is now better for high volume right

00:10:36,820 --> 00:10:41,350
applications instead of a b-tree they

00:10:39,579 --> 00:10:43,510
use a data structure called a log

00:10:41,350 --> 00:10:45,760
structured merge tree under the hood a

00:10:43,510 --> 00:10:48,220
log structured merge lis consists

00:10:45,760 --> 00:10:50,410
basically of two elements the first

00:10:48,220 --> 00:10:52,899
element is a sorted tree in memory which

00:10:50,410 --> 00:10:54,940
accepts new writes and whenever this

00:10:52,899 --> 00:10:57,970
tree is getting too big we will dump it

00:10:54,940 --> 00:10:59,890
into a log file on disk the log files on

00:10:57,970 --> 00:11:02,440
disk are also sorted by time and

00:10:59,890 --> 00:11:03,890
containing index so we can search them

00:11:02,440 --> 00:11:06,260
very efficient

00:11:03,890 --> 00:11:08,870
if you search a metric in our database

00:11:06,260 --> 00:11:11,420
we will first look into the memory tree

00:11:08,870 --> 00:11:14,410
and if it doesn't exist we would iterate

00:11:11,420 --> 00:11:17,060
over all the log files on disk

00:11:14,410 --> 00:11:18,829
the big advantage she is now that most

00:11:17,060 --> 00:11:21,230
time series databases already

00:11:18,829 --> 00:11:23,510
implemented retention policy policy

00:11:21,230 --> 00:11:27,019
which does a garbage collection for us

00:11:23,510 --> 00:11:29,149
empty needs or log files additionally it

00:11:27,019 --> 00:11:31,490
also takes care of compressing and down

00:11:29,149 --> 00:11:33,350
sampling the log files which makes it

00:11:31,490 --> 00:11:37,279
more efficient than using a relational

00:11:33,350 --> 00:11:39,649
database with this knowledge we can now

00:11:37,279 --> 00:11:43,070
write the request metric TOI influx

00:11:39,649 --> 00:11:45,649
database we take the timestamp we

00:11:43,070 --> 00:11:49,279
calculate the text which is a hook name

00:11:45,649 --> 00:11:51,860
and the location and we calculate the

00:11:49,279 --> 00:11:55,760
value which is a finished time - the

00:11:51,860 --> 00:11:57,829
start time we also want to monitor their

00:11:55,760 --> 00:12:00,290
escrow egg first but before we can do

00:11:57,829 --> 00:12:02,930
that we first need to clean normalize

00:12:00,290 --> 00:12:05,870
and group the payload we will receive we

00:12:02,930 --> 00:12:07,370
will do this in the next chapter in this

00:12:05,870 --> 00:12:09,800
chapter we had a look at the data

00:12:07,370 --> 00:12:12,709
structure of our matrix and which data

00:12:09,800 --> 00:12:14,959
storage would be the most efficient we

00:12:12,709 --> 00:12:18,290
decided to use a time series database

00:12:14,959 --> 00:12:20,209
like in flux to be in the end in the

00:12:18,290 --> 00:12:24,440
third chapter we will process the data

00:12:20,209 --> 00:12:26,449
received in the SQL event before we can

00:12:24,440 --> 00:12:30,110
store the data in the database we will

00:12:26,449 --> 00:12:32,680
need to clean normalize and groupid we

00:12:30,110 --> 00:12:35,300
will start with a normalization process

00:12:32,680 --> 00:12:38,810
the payload will receive from rays

00:12:35,300 --> 00:12:41,930
contains the SQL query and the name if

00:12:38,810 --> 00:12:45,829
we have a look at the SQL query with a

00:12:41,930 --> 00:12:48,589
notice that it does contain values for

00:12:45,829 --> 00:12:50,899
analyzing the data we need to remove

00:12:48,589 --> 00:12:53,600
these values and normalize the queries

00:12:50,899 --> 00:12:57,829
this is necessary so we can group them

00:12:53,600 --> 00:13:00,290
together later one approach we can do is

00:12:57,829 --> 00:13:02,870
using a regular expression to find and

00:13:00,290 --> 00:13:05,120
replace all values yes there are several

00:13:02,870 --> 00:13:07,670
different positions in an SQL query

00:13:05,120 --> 00:13:10,519
where values can occur we will end up

00:13:07,670 --> 00:13:12,410
with several regular expressions - it's

00:13:10,519 --> 00:13:15,020
also worth noting that there are

00:13:12,410 --> 00:13:16,120
different SQL dialects and we might need

00:13:15,020 --> 00:13:18,610
to adapt our

00:13:16,120 --> 00:13:21,820
expressions depending on the database we

00:13:18,610 --> 00:13:24,130
use the next step is cleaning to remove

00:13:21,820 --> 00:13:27,640
unnecessary data which does not provide

00:13:24,130 --> 00:13:29,560
any value without the cleaning step we

00:13:27,640 --> 00:13:32,230
will notice that we will also write SQL

00:13:29,560 --> 00:13:34,420
furs which are not executed in our

00:13:32,230 --> 00:13:36,970
eraser but by active record in the

00:13:34,420 --> 00:13:38,920
background most queries from active

00:13:36,970 --> 00:13:41,380
record like getting the version of the

00:13:38,920 --> 00:13:43,839
database or the schema version we want

00:13:41,380 --> 00:13:47,200
to filter out because usually they won't

00:13:43,839 --> 00:13:49,720
cause any performance issues they are

00:13:47,200 --> 00:13:52,270
also creating object commands which are

00:13:49,720 --> 00:13:55,360
usually executed in migrations or tests

00:13:52,270 --> 00:13:59,190
we also want to filter these out we can

00:13:55,360 --> 00:14:01,900
build a query object which only tracks

00:13:59,190 --> 00:14:05,050
SQL commands we're interested in like

00:14:01,900 --> 00:14:10,180
select insert update and delete and also

00:14:05,050 --> 00:14:12,279
does not track schema migrations the

00:14:10,180 --> 00:14:16,150
last step of processing our data is

00:14:12,279 --> 00:14:18,460
grouping we would like to see which

00:14:16,150 --> 00:14:21,029
queries get executed by which controller

00:14:18,460 --> 00:14:23,920
action to identify potential problems

00:14:21,029 --> 00:14:25,990
the payload of the SQL query event

00:14:23,920 --> 00:14:28,660
however does not contain a request ID

00:14:25,990 --> 00:14:31,120
the reason is that active record is a

00:14:28,660 --> 00:14:33,070
standalone framework and we can use it

00:14:31,120 --> 00:14:36,070
outside the request and response cycle

00:14:33,070 --> 00:14:38,970
of course we can use it my migrations

00:14:36,070 --> 00:14:42,279
background shops or even without rails

00:14:38,970 --> 00:14:44,740
in the beginning I showed that there's

00:14:42,279 --> 00:14:47,520
also a start processing event triggered

00:14:44,740 --> 00:14:50,560
at the start of each controller action

00:14:47,520 --> 00:14:52,839
we can subscribe to the start processing

00:14:50,560 --> 00:14:54,760
event to fill a cache which we later

00:14:52,839 --> 00:14:57,580
read when storing the active record

00:14:54,760 --> 00:15:00,040
matrix you see a current attributes

00:14:57,580 --> 00:15:03,880
which has a location in a request ID

00:15:00,040 --> 00:15:06,279
attribute current attributes is also

00:15:03,880 --> 00:15:08,770
implemented in active support but we can

00:15:06,279 --> 00:15:11,020
easily implement it by ourselves by

00:15:08,770 --> 00:15:13,630
storing a hash in the current thread and

00:15:11,020 --> 00:15:15,910
then implement getter and setter methods

00:15:13,630 --> 00:15:20,589
to store the key and value pairs in this

00:15:15,910 --> 00:15:22,630
hash if we put everything together we

00:15:20,589 --> 00:15:25,780
first check if we want to track this

00:15:22,630 --> 00:15:28,120
query we then fetch their current

00:15:25,780 --> 00:15:29,640
location in request ID from our current

00:15:28,120 --> 00:15:32,010
attributes

00:15:29,640 --> 00:15:34,350
we fetched their normalized query and

00:15:32,010 --> 00:15:38,190
then store everything in our influx to

00:15:34,350 --> 00:15:40,380
be in this chapter we had a look at

00:15:38,190 --> 00:15:44,100
cleaning normalizing and grouping our

00:15:40,380 --> 00:15:46,170
data the processing step is necessary so

00:15:44,100 --> 00:15:49,620
our metrics provide more value and

00:15:46,170 --> 00:15:51,360
insights later on the downside of this

00:15:49,620 --> 00:15:54,630
approach is that we will add some

00:15:51,360 --> 00:15:56,760
additional time to our response time we

00:15:54,630 --> 00:15:59,010
could implement the normalization step

00:15:56,760 --> 00:16:00,690
in a sieve library or even move it out

00:15:59,010 --> 00:16:03,960
to a background shop if this is a

00:16:00,690 --> 00:16:05,790
concern in the last chapter of this talk

00:16:03,960 --> 00:16:08,340
we will look into visualizing the

00:16:05,790 --> 00:16:10,530
collected metrics but why is visualizing

00:16:08,340 --> 00:16:13,140
even necessary if you have only a few

00:16:10,530 --> 00:16:15,120
metrics like here we can easily spot

00:16:13,140 --> 00:16:17,280
that there's a problem with our third

00:16:15,120 --> 00:16:20,280
metric because it has a higher response

00:16:17,280 --> 00:16:22,740
time but in a real world application we

00:16:20,280 --> 00:16:24,750
would have thousands or even millions of

00:16:22,740 --> 00:16:27,060
data points and it would be impossible

00:16:24,750 --> 00:16:29,010
for us to spot any problems if we

00:16:27,060 --> 00:16:31,680
visualize our metrics in a dashboard

00:16:29,010 --> 00:16:34,470
like here we can see that most of our

00:16:31,680 --> 00:16:37,020
users have a response time between 16

00:16:34,470 --> 00:16:39,360
and 60 milliseconds but some of our

00:16:37,020 --> 00:16:41,790
users would even experience a response

00:16:39,360 --> 00:16:42,570
time of up to one second which would be

00:16:41,790 --> 00:16:45,690
a big problem

00:16:42,570 --> 00:16:50,430
so visualization of the metrics helps us

00:16:45,690 --> 00:16:52,350
to spot issues in our application some

00:16:50,430 --> 00:16:54,420
metrics we can calculate are for

00:16:52,350 --> 00:16:56,520
instance the average which is the sum of

00:16:54,420 --> 00:16:59,690
all values divided by the number of

00:16:56,520 --> 00:17:03,180
values in this case it would be 105

00:16:59,690 --> 00:17:05,190
milliseconds another useful metric is a

00:17:03,180 --> 00:17:07,709
median which we can calculate by

00:17:05,190 --> 00:17:10,079
ordering all values and pick the one in

00:17:07,709 --> 00:17:13,770
the middle in this case it would be 56

00:17:10,079 --> 00:17:16,079
million milliseconds and with the help

00:17:13,770 --> 00:17:18,270
of influx to B we can easily calculate

00:17:16,079 --> 00:17:22,709
these metrics because it already

00:17:18,270 --> 00:17:24,630
implements mathematical functions if we

00:17:22,709 --> 00:17:26,940
group our values in buckets and

00:17:24,630 --> 00:17:29,220
visualize it on a histogram we will see

00:17:26,940 --> 00:17:32,370
that in a normal distribution the median

00:17:29,220 --> 00:17:34,680
and average are close together the

00:17:32,370 --> 00:17:37,650
response time of most web applications

00:17:34,680 --> 00:17:39,870
is not a normal distribution though most

00:17:37,650 --> 00:17:42,240
web applications have a lot of fast

00:17:39,870 --> 00:17:43,140
response times but then also some really

00:17:42,240 --> 00:17:45,780
slow ones

00:17:43,140 --> 00:17:48,330
instance for users with a lot of data in

00:17:45,780 --> 00:17:50,880
this distribution we see that the

00:17:48,330 --> 00:17:52,410
average a median are not similar and the

00:17:50,880 --> 00:17:54,840
average would give us the false

00:17:52,410 --> 00:17:57,480
impression of a response time of around

00:17:54,840 --> 00:17:59,520
a hundred milliseconds the median would

00:17:57,480 --> 00:18:01,770
be more accurate here which tells us

00:17:59,520 --> 00:18:05,730
that most of our users have a respawn

00:18:01,770 --> 00:18:09,690
response time of faster than 130

00:18:05,730 --> 00:18:12,750
milliseconds in this histogram we can

00:18:09,690 --> 00:18:15,800
also see that 10% of our users even have

00:18:12,750 --> 00:18:18,480
a response time slower than 10 seconds

00:18:15,800 --> 00:18:20,250
Richard Lehman published a really good

00:18:18,480 --> 00:18:22,050
blog article about this topic so if

00:18:20,250 --> 00:18:24,830
you're interested in this I definitely

00:18:22,050 --> 00:18:27,930
recommend to check this out

00:18:24,830 --> 00:18:30,810
visualization also helps us to set the

00:18:27,930 --> 00:18:32,820
right priorities we can calculate in

00:18:30,810 --> 00:18:35,700
which controller actions we spent the

00:18:32,820 --> 00:18:38,310
most time in this table we can see that

00:18:35,700 --> 00:18:40,560
we spend almost 50 percent of our time

00:18:38,310 --> 00:18:42,110
in the first action there would be a

00:18:40,560 --> 00:18:46,980
good candidate for any performance

00:18:42,110 --> 00:18:49,860
improvements the 80/20 rule tells us

00:18:46,980 --> 00:18:53,370
that often 80% of our time is spent in

00:18:49,860 --> 00:18:54,960
20% of our controller actions if we want

00:18:53,370 --> 00:18:58,230
to improve the performance of our

00:18:54,960 --> 00:19:00,540
application we should focus on these 20%

00:18:58,230 --> 00:19:05,550
first otherwise our effort could be

00:19:00,540 --> 00:19:07,770
wasted visualization not only helps us

00:19:05,550 --> 00:19:10,350
to spot performance problems but also

00:19:07,770 --> 00:19:13,830
gives us an indication about what is

00:19:10,350 --> 00:19:16,170
causing these problems we could for

00:19:13,830 --> 00:19:18,570
instance calculate the maximum and the

00:19:16,170 --> 00:19:20,910
median times of SQL queries which could

00:19:18,570 --> 00:19:25,290
indicate that there is an index missing

00:19:20,910 --> 00:19:28,200
and we can also count these SQL queries

00:19:25,290 --> 00:19:30,120
which gives us an indication about n

00:19:28,200 --> 00:19:34,290
plus 1 queries or even missing

00:19:30,120 --> 00:19:36,630
memorization in this chapter we talked

00:19:34,290 --> 00:19:39,030
about visualization of our matrix and

00:19:36,630 --> 00:19:41,490
how it can help to support issues set

00:19:39,030 --> 00:19:43,890
priority for refactorings and even give

00:19:41,490 --> 00:19:47,550
us an indication about the course of any

00:19:43,890 --> 00:19:49,620
performance issues today we build the

00:19:47,550 --> 00:19:52,050
performance monitoring tools step by

00:19:49,620 --> 00:19:54,000
step together we started by collecting

00:19:52,050 --> 00:19:56,500
metrics from Ruby on Rails and stored

00:19:54,000 --> 00:19:58,570
them into a time series database

00:19:56,500 --> 00:20:00,850
some of the metrics we needed to clean

00:19:58,570 --> 00:20:03,429
normalize in group first which we

00:20:00,850 --> 00:20:06,010
discussed in the third chapter last but

00:20:03,429 --> 00:20:09,190
not least we also visualized the data to

00:20:06,010 --> 00:20:11,380
spot potential performance issues to

00:20:09,190 --> 00:20:13,900
build this tool we heavily used open

00:20:11,380 --> 00:20:17,140
source software we used a Ruby on Rails

00:20:13,900 --> 00:20:19,390
plugin stored the metrics in in flux TP

00:20:17,140 --> 00:20:22,870
and visualized the data on a dashboard

00:20:19,390 --> 00:20:25,360
with a graph owner the big advantages

00:20:22,870 --> 00:20:26,950
are that we can customize the solution

00:20:25,360 --> 00:20:28,720
and bend it to our needs we can

00:20:26,950 --> 00:20:31,480
collaborate with a Ruby on Rails

00:20:28,720 --> 00:20:34,630
community and share best practices and

00:20:31,480 --> 00:20:36,970
dashboards to come back to the quote

00:20:34,630 --> 00:20:39,850
from the beginning what I cannot create

00:20:36,970 --> 00:20:41,830
I do not understand by building a

00:20:39,850 --> 00:20:44,500
performance monitoring tools step by

00:20:41,830 --> 00:20:47,080
step we now understand how its internals

00:20:44,500 --> 00:20:48,580
work which also helps us to use these

00:20:47,080 --> 00:20:52,000
tools better in the future

00:20:48,580 --> 00:20:54,070
as mentioned in the beginning most of

00:20:52,000 --> 00:20:56,799
the knowledge in this talk is extracted

00:20:54,070 --> 00:20:59,260
from the influx TV race gem we always

00:20:56,799 --> 00:21:00,880
look for new contributors and help so if

00:20:59,260 --> 00:21:03,400
this talk made you curious about

00:21:00,880 --> 00:21:06,909
performance monitoring you can find us

00:21:03,400 --> 00:21:09,429
on github this year I can't take your

00:21:06,909 --> 00:21:12,640
questions in person but I'm very happy

00:21:09,429 --> 00:21:15,159
to take any questions so feel free to

00:21:12,640 --> 00:21:17,830
send me a message on Twitter or an email

00:21:15,159 --> 00:21:20,559
the EDD brookmire or christian ed broke

00:21:17,830 --> 00:21:22,980
my net thank you again for watching my

00:21:20,559 --> 00:21:22,980

YouTube URL: https://www.youtube.com/watch?v=2yc_kWJGLW8


