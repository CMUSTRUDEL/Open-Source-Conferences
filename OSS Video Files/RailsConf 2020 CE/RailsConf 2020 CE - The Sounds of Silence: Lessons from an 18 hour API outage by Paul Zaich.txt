Title: RailsConf 2020 CE - The Sounds of Silence: Lessons from an 18 hour API outage by Paul Zaich
Publication date: 2020-04-24
Playlist: RailsConf 2020 CE
Description: 
	The Sounds of Silence: Lessons from an 18 hour API outage by Paul Zaich

Sometimes applications are behaving “normally” along strict definitions of HTTP statuses but under the surface, something is terribly wrong. In 2017, Checkr’s most important API endpoint went down for 12 hours without detection. In this talk I’ll talk about this incident, how we responded (what went well and what could have gone better) and explore how we’ve hardened our systems today with simple monitoring patterns.

___________

Paul hails from Denver, CO where he works as an Engineering Manager at Checkr. He’s passionate about building technology for the new world of work. In a former life, Paul was a competitive swimmer. He now spends most of his free time on dry land with his wife and three children.
Captions: 
	00:00:08,990 --> 00:00:14,759
today I'll be sharing the Sounds of

00:00:11,309 --> 00:00:19,919
Silence lessons from an 18-hour API

00:00:14,759 --> 00:00:23,189
outage I'd like to start by talking

00:00:19,919 --> 00:00:26,340
about bugs bugs are a constant companion

00:00:23,189 --> 00:00:28,770
of an engineer any time a change is

00:00:26,340 --> 00:00:30,599
introduced into a complex system no

00:00:28,770 --> 00:00:34,110
matter how strict the quality controls

00:00:30,599 --> 00:00:36,539
and tooling there's a chance that human

00:00:34,110 --> 00:00:38,910
error will introduce a bug let's face it

00:00:36,539 --> 00:00:40,310
bugs are a reality of software

00:00:38,910 --> 00:00:45,090
development

00:00:40,310 --> 00:00:50,670
so if bugs are inevitable the question

00:00:45,090 --> 00:00:53,670
is how do we reduce their impact my name

00:00:50,670 --> 00:00:56,700
is Paul I'm an engineering manager at

00:00:53,670 --> 00:00:59,640
checker I live in Denver with my wife

00:00:56,700 --> 00:01:05,579
Erin and our three kids Bennett Delphine

00:00:59,640 --> 00:01:07,790
and Hugo what is checker checker is a

00:01:05,579 --> 00:01:10,439
people trust platform delivering

00:01:07,790 --> 00:01:13,439
background checks to make hiring safer

00:01:10,439 --> 00:01:15,600
more efficient and inclusive background

00:01:13,439 --> 00:01:20,939
checks are often a crucial final step in

00:01:15,600 --> 00:01:23,249
the hiring process for our customers in

00:01:20,939 --> 00:01:26,009
2017 on October 6

00:01:23,249 --> 00:01:28,350
we had an 18-hour API outage where

00:01:26,009 --> 00:01:34,619
customers were unable to create reports

00:01:28,350 --> 00:01:38,340
using our API checker a group has grown

00:01:34,619 --> 00:01:41,219
to over 150 engineers as of 2020 but in

00:01:38,340 --> 00:01:44,299
2017 the team consisted of 30 engineers

00:01:41,219 --> 00:01:46,289
spread across approximately six teams

00:01:44,299 --> 00:01:48,990
request fallin had grown significantly

00:01:46,289 --> 00:01:51,119
over just a few years and much of the

00:01:48,990 --> 00:01:53,520
team's focus at the time was on stable

00:01:51,119 --> 00:01:56,130
stabilizing the system to handle the

00:01:53,520 --> 00:01:58,049
increased load the team had doubled in

00:01:56,130 --> 00:01:59,609
the previous year and much of the

00:01:58,049 --> 00:02:06,029
knowledge of the system was held

00:01:59,609 --> 00:02:09,179
informally by members of the team so you

00:02:06,029 --> 00:02:10,830
may be wondering what is a report why

00:02:09,179 --> 00:02:13,069
we're custom what does it matter if

00:02:10,830 --> 00:02:16,260
customers were unable to create a report

00:02:13,069 --> 00:02:17,940
in the checker system a report is the

00:02:16,260 --> 00:02:21,129
model that describes a checkered

00:02:17,940 --> 00:02:26,120
checkered background check

00:02:21,129 --> 00:02:27,170
and customers would submit a request to

00:02:26,120 --> 00:02:33,319
create a background check

00:02:27,170 --> 00:02:37,790
using our REST API when a customer DUT

00:02:33,319 --> 00:02:40,069
does so they initiate a mechanism bike

00:02:37,790 --> 00:02:42,650
to create a background check for a given

00:02:40,069 --> 00:02:44,959
candidate in our system screenings like

00:02:42,650 --> 00:02:48,650
a motor vehicle report or criminal check

00:02:44,959 --> 00:02:50,359
are associated with that report in all

00:02:48,650 --> 00:02:52,760
screenings and other orchestration

00:02:50,359 --> 00:02:58,970
occurs as a result of that first request

00:02:52,760 --> 00:03:02,810
from the customer the incident began at

00:02:58,970 --> 00:03:05,569
4:30 p.m. on a Friday afternoon a script

00:03:02,810 --> 00:03:09,049
was run to migrate some old screening

00:03:05,569 --> 00:03:15,670
records from an integer foreign key to

00:03:09,049 --> 00:03:18,709
use an au ID instead within an hour an

00:03:15,670 --> 00:03:21,560
on-call engineer on an unrelated team

00:03:18,709 --> 00:03:27,709
was paged because of a spike of errors

00:03:21,560 --> 00:03:29,959
on one of our front end applications the

00:03:27,709 --> 00:03:33,319
application submitted a small percentage

00:03:29,959 --> 00:03:36,230
of our total volume of reports did not

00:03:33,319 --> 00:03:40,340
use the main API that our customers was

00:03:36,230 --> 00:03:42,560
using a potting initial investigation

00:03:40,340 --> 00:03:45,949
the team decided that the air was likely

00:03:42,560 --> 00:03:54,949
the result of user air they snooze the

00:03:45,949 --> 00:03:58,010
page early on Saturday morning the same

00:03:54,949 --> 00:04:00,709
air triggered another page the on-call

00:03:58,010 --> 00:04:03,650
engineer began investigating the air and

00:04:00,709 --> 00:04:06,650
eventually noticed a strange status code

00:04:03,650 --> 00:04:12,709
being returned by a reports create

00:04:06,650 --> 00:04:16,060
endpoint both our public and private

00:04:12,709 --> 00:04:19,190
report endpoints we're returning 404

00:04:16,060 --> 00:04:21,769
for many but not all of the requests

00:04:19,190 --> 00:04:25,220
that were submitted at this point around

00:04:21,769 --> 00:04:26,870
9:30 a.m. on Saturday it was finally

00:04:25,220 --> 00:04:30,860
clear that there was a major problem

00:04:26,870 --> 00:04:34,190
that impacted our api's and not just

00:04:30,860 --> 00:04:40,760
that client application that was was

00:04:34,190 --> 00:04:42,740
if you sing the air the initial

00:04:40,760 --> 00:04:46,580
responder escalated the issue to get all

00:04:42,740 --> 00:04:48,530
on call engineers involved with a

00:04:46,580 --> 00:04:53,270
smaller engineering team at the time

00:04:48,530 --> 00:04:57,320
escalation often occurred via slack and

00:04:53,270 --> 00:04:59,120
we used a channel called image fire to

00:04:57,320 --> 00:05:02,270
alert the rest of the team that was on

00:04:59,120 --> 00:05:05,870
call the rest of the team started to

00:05:02,270 --> 00:05:09,050
login to our VPN I remember pulling my

00:05:05,870 --> 00:05:15,680
laptop out in the parking lot of Crissy

00:05:09,050 --> 00:05:18,050
Field in San Francisco unfortunately at

00:05:15,680 --> 00:05:21,650
the same time there's a certain as there

00:05:18,050 --> 00:05:26,120
was a surge in our VPN traffic the VPN

00:05:21,650 --> 00:05:28,430
went down we all needed to go into the

00:05:26,120 --> 00:05:31,610
office to access it they internet and

00:05:28,430 --> 00:05:37,040
see production logs on our servers as a

00:05:31,610 --> 00:05:40,400
result finally we made it into office

00:05:37,040 --> 00:05:43,460
around 10:00 a.m. now with full access

00:05:40,400 --> 00:05:46,100
to production logs and all the requests

00:05:43,460 --> 00:05:48,850
that were made we were able to resolve

00:05:46,100 --> 00:05:53,120
or identify the issue in about an hour

00:05:48,850 --> 00:05:58,430
and shortly thereafter we reproduce the

00:05:53,120 --> 00:06:00,950
issue within about 15 minutes we had a

00:05:58,430 --> 00:06:07,750
temporary fix to solve the immediate

00:06:00,950 --> 00:06:11,900
issue so what went wrong

00:06:07,750 --> 00:06:14,870
at checker we strive to have a blameless

00:06:11,900 --> 00:06:17,000
culture where we learn from our mistakes

00:06:14,870 --> 00:06:19,940
and make improvements as a result of

00:06:17,000 --> 00:06:22,580
outages an important part of that

00:06:19,940 --> 00:06:27,410
process for us is creating a post-mortem

00:06:22,580 --> 00:06:30,200
document here's a screenshot of the

00:06:27,410 --> 00:06:32,090
template that the team uses today the

00:06:30,200 --> 00:06:34,520
post mortem dark document and process

00:06:32,090 --> 00:06:37,370
has evolved over time as the team has

00:06:34,520 --> 00:06:39,050
grown but the goal remains the same to

00:06:37,370 --> 00:06:42,440
learn from the incident so that the same

00:06:39,050 --> 00:06:44,660
mistake does not happen again the post

00:06:42,440 --> 00:06:48,369
mortem document captures the root cause

00:06:44,660 --> 00:06:50,629
timeline and follow-up action

00:06:48,369 --> 00:06:53,809
identifying the root cause an important

00:06:50,629 --> 00:06:57,110
component of the post-mortem but we

00:06:53,809 --> 00:07:02,929
value the action items as equally

00:06:57,110 --> 00:07:07,279
important here's what we learned problem

00:07:02,929 --> 00:07:09,379
1 the root cause was exposed due to a

00:07:07,279 --> 00:07:11,770
backfill run at the start of the

00:07:09,379 --> 00:07:15,259
incident

00:07:11,770 --> 00:07:19,309
The Associated records and database

00:07:15,259 --> 00:07:21,080
level constraints were not enforced this

00:07:19,309 --> 00:07:23,719
was in part due to the use of two

00:07:21,080 --> 00:07:30,589
different databases for storing reports

00:07:23,719 --> 00:07:33,919
and screenings the root cause the air

00:07:30,589 --> 00:07:36,139
turned out to be a combination of common

00:07:33,919 --> 00:07:39,019
bugs in programming the backfill

00:07:36,139 --> 00:07:41,779
corrupted records in our database in our

00:07:39,019 --> 00:07:44,539
relations assume that foreign keys would

00:07:41,779 --> 00:07:46,969
always be present some screenings ended

00:07:44,539 --> 00:07:49,429
up with a nullified reference to their

00:07:46,969 --> 00:07:53,899
parent report and had no value for

00:07:49,429 --> 00:07:56,469
report ID when a new report was

00:07:53,899 --> 00:07:59,479
validated by active record validations

00:07:56,469 --> 00:08:01,669
the report ensures that it's associated

00:07:59,479 --> 00:08:03,919
screenings are also valid given the

00:08:01,669 --> 00:08:06,619
configuration of the report and the

00:08:03,919 --> 00:08:09,709
information provided about the candidate

00:08:06,619 --> 00:08:14,569
a screening object is found in the

00:08:09,709 --> 00:08:18,110
database or built in memory since the

00:08:14,569 --> 00:08:20,990
report ID of the new report was null the

00:08:18,110 --> 00:08:25,089
nullified screenings due to the backfill

00:08:20,990 --> 00:08:28,819
were found instead of creating a new

00:08:25,089 --> 00:08:31,309
object in memory when the validation was

00:08:28,819 --> 00:08:33,919
called on the screening the screenings

00:08:31,309 --> 00:08:37,610
foreign key was used to look up the

00:08:33,919 --> 00:08:40,909
report in this case the report could not

00:08:37,610 --> 00:08:44,269
be found in an active record not found

00:08:40,909 --> 00:08:46,009
exception was raised our API routes

00:08:44,269 --> 00:08:48,860
handle active record not found

00:08:46,009 --> 00:08:52,819
automatically in return a 404 response

00:08:48,860 --> 00:08:58,519
which is why we were seeing so many 404

00:08:52,819 --> 00:09:00,860
responses the backfill only impacted two

00:08:58,519 --> 00:09:03,470
of our screening tables

00:09:00,860 --> 00:09:06,080
as a result of that some types of

00:09:03,470 --> 00:09:09,140
reports continued to create successfully

00:09:06,080 --> 00:09:11,590
while others didn't this turned out to

00:09:09,140 --> 00:09:15,560
be a contributing factor in our overall

00:09:11,590 --> 00:09:25,520
Incident Response because some reports

00:09:15,560 --> 00:09:28,580
were still completing problem - the

00:09:25,520 --> 00:09:32,210
second problem we identified was the 14

00:09:28,580 --> 00:09:36,460
hour lag time we had in identifying that

00:09:32,210 --> 00:09:36,460
report creation was severely impacted

00:09:36,670 --> 00:09:43,190
let's go back to the incident timeline

00:09:39,170 --> 00:09:46,250
for a moment this timeline can be broken

00:09:43,190 --> 00:09:49,070
into two buckets the time to resolution

00:09:46,250 --> 00:09:51,770
and the time to response in this

00:09:49,070 --> 00:09:54,710
incident the time to response severely

00:09:51,770 --> 00:09:59,960
impacted the overall duration of the

00:09:54,710 --> 00:10:02,570
degraded report API 75% of the incidents

00:09:59,960 --> 00:10:05,540
duration occurred before an active

00:10:02,570 --> 00:10:08,870
response was initiated every hour of

00:10:05,540 --> 00:10:12,710
additional downtime was a multiplier in

00:10:08,870 --> 00:10:14,570
terms of impacted API requests we asked

00:10:12,710 --> 00:10:22,640
ourselves what would have alerted us to

00:10:14,570 --> 00:10:24,710
the issue sooner remember we did see an

00:10:22,640 --> 00:10:27,560
alert within an hour of the start of the

00:10:24,710 --> 00:10:29,840
incident but the Lert was far removed

00:10:27,560 --> 00:10:32,360
from the impacted component of the

00:10:29,840 --> 00:10:35,540
application it was an entirely different

00:10:32,360 --> 00:10:37,730
repo the alert pointed to a front-end

00:10:35,540 --> 00:10:40,490
service and the service utilized an

00:10:37,730 --> 00:10:43,400
internal API that received only a small

00:10:40,490 --> 00:10:47,630
percentage of our overall report traffic

00:10:43,400 --> 00:10:50,240
and the alert from sentry was was better

00:10:47,630 --> 00:10:53,120
than nothing but it didn't contain clear

00:10:50,240 --> 00:10:55,910
actionable information that a responder

00:10:53,120 --> 00:11:00,340
could use to quickly make an informed

00:10:55,910 --> 00:11:00,340
decision an assessment of the problem

00:11:04,589 --> 00:11:10,029
here's the most painful part of the

00:11:06,820 --> 00:11:12,339
incident from a response perspective we

00:11:10,029 --> 00:11:15,850
actually had a monitor specifically set

00:11:12,339 --> 00:11:19,210
up for the express purpose of detecting

00:11:15,850 --> 00:11:22,480
an outage in report creation we captured

00:11:19,210 --> 00:11:25,660
a stats D event every time a new report

00:11:22,480 --> 00:11:27,600
was created in the system this was

00:11:25,660 --> 00:11:30,339
exactly what we needed

00:11:27,600 --> 00:11:33,610
problem was that our alerting rules were

00:11:30,339 --> 00:11:36,610
far too simplistic to detect more subtle

00:11:33,610 --> 00:11:38,770
failures like we saw when the monitor

00:11:36,610 --> 00:11:41,290
was set up we made the assumption that

00:11:38,770 --> 00:11:46,180
we'd see close to 100 percent of reports

00:11:41,290 --> 00:11:49,060
failing to create as a result we set up

00:11:46,180 --> 00:11:52,510
a constant threshold floor of about a

00:11:49,060 --> 00:11:55,150
hundred reports created in 30 minutes if

00:11:52,510 --> 00:11:57,330
the rate dropped below that threshold an

00:11:55,150 --> 00:12:00,190
alarm would sound

00:11:57,330 --> 00:12:03,670
unfortunately the outage did not impact

00:12:00,190 --> 00:12:06,070
all reports as I mentioned earlier only

00:12:03,670 --> 00:12:09,130
certain report configurations room were

00:12:06,070 --> 00:12:11,650
impacted as a result of we saw the

00:12:09,130 --> 00:12:14,950
report creation metric drop abnormally

00:12:11,650 --> 00:12:20,050
low but never dipped below the floor

00:12:14,950 --> 00:12:22,690
value in our monitor the monitor was not

00:12:20,050 --> 00:12:29,709
in summary was not sensitive enough to

00:12:22,690 --> 00:12:31,209
detect a shoe coming out of the incident

00:12:29,709 --> 00:12:34,870
we identified that we needed to improve

00:12:31,209 --> 00:12:38,350
our overall observability into our

00:12:34,870 --> 00:12:40,720
systems by definition observability is a

00:12:38,350 --> 00:12:43,110
measure of how well internal system

00:12:40,720 --> 00:12:46,260
states of a system can be inferred from

00:12:43,110 --> 00:12:49,480
knowledge of its external outputs in

00:12:46,260 --> 00:12:52,800
this next section I will explore tools

00:12:49,480 --> 00:12:55,900
for improving overall observant

00:12:52,800 --> 00:12:59,350
observability of your application how

00:12:55,900 --> 00:13:02,230
these tools work together and how to use

00:12:59,350 --> 00:13:08,350
those tools to craft meaningful alerts

00:13:02,230 --> 00:13:10,810
when something is wrong there are three

00:13:08,350 --> 00:13:15,100
basic components to building a

00:13:10,810 --> 00:13:18,190
durability stack first you need to you

00:13:15,100 --> 00:13:20,680
need mechanisms for gathering metrics

00:13:18,190 --> 00:13:22,610
measurable events that indicate

00:13:20,680 --> 00:13:26,330
something about the state of your

00:13:22,610 --> 00:13:29,510
application second you need to define

00:13:26,330 --> 00:13:33,130
about monitoring rules that define when

00:13:29,510 --> 00:13:36,230
a particulate or metric is green or red

00:13:33,130 --> 00:13:39,040
third you're monitoring rules should be

00:13:36,230 --> 00:13:42,020
connected to an incident management

00:13:39,040 --> 00:13:45,650
platform that governs rules around

00:13:42,020 --> 00:13:48,790
on-call responders and escalation do not

00:13:45,650 --> 00:13:54,740
use email or slack as your primary

00:13:48,790 --> 00:13:57,470
notification method let's talk a little

00:13:54,740 --> 00:14:00,290
bit about metric collection there are

00:13:57,470 --> 00:14:03,310
three broad categories of metric

00:14:00,290 --> 00:14:06,320
collection common in web applications

00:14:03,310 --> 00:14:10,120
there are many Saxon solutions that can

00:14:06,320 --> 00:14:13,070
address one or more of these categories

00:14:10,120 --> 00:14:16,070
one of the most common types of metric

00:14:13,070 --> 00:14:18,590
collection are exception trackers these

00:14:16,070 --> 00:14:20,960
are services that provide libraries to

00:14:18,590 --> 00:14:23,740
capture events when exceptions occur in

00:14:20,960 --> 00:14:28,160
your application examples include

00:14:23,740 --> 00:14:31,150
Century roll bar and air break another

00:14:28,160 --> 00:14:33,680
common type of metric collection is

00:14:31,150 --> 00:14:37,730
application performance monitoring or

00:14:33,680 --> 00:14:40,190
APM APM gives you access to injury

00:14:37,730 --> 00:14:44,810
industry standard metrics across common

00:14:40,190 --> 00:14:46,870
protocols and stacks the benefit of APM

00:14:44,810 --> 00:14:50,450
is that you get a wealth of generic

00:14:46,870 --> 00:14:53,170
metrics basically for free you can

00:14:50,450 --> 00:14:57,080
easily drill down to see request volume

00:14:53,170 --> 00:14:59,300
latency or investigate a full trace from

00:14:57,080 --> 00:15:03,110
the application from the application to

00:14:59,300 --> 00:15:07,490
the database and back the final category

00:15:03,110 --> 00:15:10,760
is real time custom application metric

00:15:07,490 --> 00:15:13,910
collection these are metrics implemented

00:15:10,760 --> 00:15:16,310
by the engineer to describe specific

00:15:13,910 --> 00:15:19,010
business processes performed by your

00:15:16,310 --> 00:15:22,820
application these metrics are the

00:15:19,010 --> 00:15:25,370
hardest to define and maintain but give

00:15:22,820 --> 00:15:28,060
you direct visibility into the state of

00:15:25,370 --> 00:15:28,060
your application

00:15:29,030 --> 00:15:33,800
to visualize the kind of visibility that

00:15:31,070 --> 00:15:36,350
each of these metrics gives to you let's

00:15:33,800 --> 00:15:41,840
imagine that your application is a black

00:15:36,350 --> 00:15:45,860
box exception tracking will give you

00:15:41,840 --> 00:15:50,810
targeted insights into hotspots in your

00:15:45,860 --> 00:15:53,900
application it will give you a sense of

00:15:50,810 --> 00:15:56,270
the impact of the air through the number

00:15:53,900 --> 00:15:58,190
of events collected where the exception

00:15:56,270 --> 00:16:00,620
is being triggered via the stack trace

00:15:58,190 --> 00:16:04,430
and depending on the size of the

00:16:00,620 --> 00:16:06,890
application and the amount of context

00:16:04,430 --> 00:16:09,920
that an engineer has it may or may not

00:16:06,890 --> 00:16:12,920
be clear what the overall impact of that

00:16:09,920 --> 00:16:18,580
particular exception is as we saw

00:16:12,920 --> 00:16:21,440
earlier in the incident exception events

00:16:18,580 --> 00:16:24,130
eventually triggered a response to the

00:16:21,440 --> 00:16:27,110
incident but it wasn't immediately clear

00:16:24,130 --> 00:16:33,620
what if anything was wrong in that

00:16:27,110 --> 00:16:36,470
particularly particularly think of a

00:16:33,620 --> 00:16:39,650
p.m. as a heat map overlaid over your

00:16:36,470 --> 00:16:42,170
application it's possible to zoom in on

00:16:39,650 --> 00:16:44,600
a specific part of the application but

00:16:42,170 --> 00:16:48,940
it also gives you a high-level view of

00:16:44,600 --> 00:16:51,650
the overall health of your application

00:16:48,940 --> 00:16:53,600
it's very useful for identifying

00:16:51,650 --> 00:16:59,120
system-wide outliers that might indicate

00:16:53,600 --> 00:17:01,460
a problem a p.m. often comes in handy

00:16:59,120 --> 00:17:04,430
when you haven't anticipated a specific

00:17:01,460 --> 00:17:06,980
mode of failure a recent example of that

00:17:04,430 --> 00:17:09,560
occurred for us back in February the

00:17:06,980 --> 00:17:12,440
configuration change was made that

00:17:09,560 --> 00:17:15,560
impacted the ability of several services

00:17:12,440 --> 00:17:21,950
in our system from authenticating with

00:17:15,560 --> 00:17:25,100
each other as we saw some more targeted

00:17:21,950 --> 00:17:29,600
metrics begin to trend in the wrong

00:17:25,100 --> 00:17:31,820
direction the spike in forw ones that we

00:17:29,600 --> 00:17:33,560
saw an APM made it clear that something

00:17:31,820 --> 00:17:40,280
goes wrong specifically with with

00:17:33,560 --> 00:17:42,590
authentication custom metrics give you

00:17:40,280 --> 00:17:44,539
the engineer a way of describe

00:17:42,590 --> 00:17:47,539
in the health of specific features or

00:17:44,539 --> 00:17:49,130
components in your application you can

00:17:47,539 --> 00:17:51,799
view each of these components as a

00:17:49,130 --> 00:17:59,150
distinct unit using the telemetry

00:17:51,799 --> 00:18:01,669
generated by the metric once you have

00:17:59,150 --> 00:18:05,539
the metrics you can identify when

00:18:01,669 --> 00:18:09,559
intervention is needed so what marks a

00:18:05,539 --> 00:18:13,130
good monitor first it needs to be a high

00:18:09,559 --> 00:18:15,590
fidelity indicator of system health you

00:18:13,130 --> 00:18:17,960
don't want to miss true positives that

00:18:15,590 --> 00:18:20,840
indicate a problem but you also don't

00:18:17,960 --> 00:18:23,559
want an overly sensitive monitor that

00:18:20,840 --> 00:18:26,690
constantly alerts on false positives

00:18:23,559 --> 00:18:31,210
false positives tend to degrade

00:18:26,690 --> 00:18:34,299
confidence in the response and overall

00:18:31,210 --> 00:18:38,059
there's a risk that you won't respond

00:18:34,299 --> 00:18:40,820
when there is a true positive secondly

00:18:38,059 --> 00:18:42,890
in most cases it's best to define

00:18:40,820 --> 00:18:46,669
monitors that measure the health of a

00:18:42,890 --> 00:18:49,250
discreet feature or component a more

00:18:46,669 --> 00:18:52,039
narrowly defined monitor will give the

00:18:49,250 --> 00:18:53,750
responder actionable information that

00:18:52,039 --> 00:18:58,220
will make a response quicker and more

00:18:53,750 --> 00:19:00,140
effective finally ideally the indicators

00:18:58,220 --> 00:19:02,840
should be a leading indicator rather

00:19:00,140 --> 00:19:06,020
than a lagging and a lagging one for

00:19:02,840 --> 00:19:10,700
example at checker we also measure

00:19:06,020 --> 00:19:13,940
report completion to ensure that reports

00:19:10,700 --> 00:19:16,549
are successfully completed this metric

00:19:13,940 --> 00:19:18,340
would not be a good indicator of the

00:19:16,549 --> 00:19:20,929
health of report creation however

00:19:18,340 --> 00:19:26,270
because reports can take several hours

00:19:20,929 --> 00:19:29,059
two days to complete let's talk about a

00:19:26,270 --> 00:19:31,669
few types of monitor patterns that could

00:19:29,059 --> 00:19:38,570
have helped us be alerted much sooner

00:19:31,669 --> 00:19:40,250
during our incident recall that our

00:19:38,570 --> 00:19:45,080
monitor failed due to lack of

00:19:40,250 --> 00:19:48,320
sensitivity first let's talk about

00:19:45,080 --> 00:19:48,980
composite monitors the concept is pretty

00:19:48,320 --> 00:19:52,070
straightforward

00:19:48,980 --> 00:19:56,150
tie multiple metrics together as a

00:19:52,070 --> 00:19:58,430
combined signal we ultimately added a

00:19:56,150 --> 00:20:00,650
posit monitor as part of our fall of

00:19:58,430 --> 00:20:03,500
action items as a response to this

00:20:00,650 --> 00:20:06,770
incident we combine measurements that

00:20:03,500 --> 00:20:09,920
looked at both report and individual

00:20:06,770 --> 00:20:13,250
screening creation to spot issues where

00:20:09,920 --> 00:20:17,750
a specific screening was causing report

00:20:13,250 --> 00:20:19,850
failures in retrospect there are a few

00:20:17,750 --> 00:20:23,180
other patterns that could also have been

00:20:19,850 --> 00:20:33,920
used to make monitor more sensitive to

00:20:23,180 --> 00:20:36,830
failures one way is to make our

00:20:33,920 --> 00:20:39,950
measurements relative rather than using

00:20:36,830 --> 00:20:43,070
absolute metrics measuring the report

00:20:39,950 --> 00:20:46,310
created count against the total number

00:20:43,070 --> 00:20:49,190
of report creation attempts gives you a

00:20:46,310 --> 00:20:56,630
metric that easily adapts to spikes and

00:20:49,190 --> 00:20:59,240
dips and overall requests volume the

00:20:56,630 --> 00:21:02,950
second option is to use a product like

00:20:59,240 --> 00:21:05,960
data dogs anomaly detection monitor

00:21:02,950 --> 00:21:10,280
anomaly detection gives you the ability

00:21:05,960 --> 00:21:12,440
to detect anomalies in your metric this

00:21:10,280 --> 00:21:14,870
method allows you to apply statistical

00:21:12,440 --> 00:21:17,180
methods like standard deviation to

00:21:14,870 --> 00:21:20,270
account for a variability and define a

00:21:17,180 --> 00:21:23,470
constantly refitted bounds rather than

00:21:20,270 --> 00:21:26,150
defining rules against absolute values

00:21:23,470 --> 00:21:29,000
this type of monitor has a benefit of

00:21:26,150 --> 00:21:32,000
adjusting over time as your metric

00:21:29,000 --> 00:21:35,800
patterns change accounting for things

00:21:32,000 --> 00:21:38,780
like seasonality and long term trends in

00:21:35,800 --> 00:21:41,780
practice however so far at least we

00:21:38,780 --> 00:21:44,030
haven't found these monitors to be super

00:21:41,780 --> 00:21:48,710
effective for our use case they tend to

00:21:44,030 --> 00:21:51,770
be a little bit over sensitive even when

00:21:48,710 --> 00:21:54,490
we've tried to make them their bounding

00:21:51,770 --> 00:21:54,490
more flexible

00:21:59,020 --> 00:22:08,870
so how do you build observability into

00:22:02,300 --> 00:22:12,080
your culture start small when building

00:22:08,870 --> 00:22:16,160
observer ability don't try to measure

00:22:12,080 --> 00:22:19,160
everything let's talk a little bit about

00:22:16,160 --> 00:22:22,070
how these metrics could work together in

00:22:19,160 --> 00:22:24,800
a growing business let's pretend that

00:22:22,070 --> 00:22:28,370
we've started our own e-commerce store

00:22:24,800 --> 00:22:32,450
to support a brand new subscription box

00:22:28,370 --> 00:22:34,550
business when you're when you're first

00:22:32,450 --> 00:22:36,730
starting out your traffic is very low

00:22:34,550 --> 00:22:40,310
and you have a small team that

00:22:36,730 --> 00:22:42,560
understands the full system it's likely

00:22:40,310 --> 00:22:46,250
that when an exception occurs you will

00:22:42,560 --> 00:22:49,970
know pretty quickly what in why in era's

00:22:46,250 --> 00:22:52,130
occurred traffic is inconsistent at this

00:22:49,970 --> 00:22:54,380
point so it's hard to predict how many

00:22:52,130 --> 00:22:56,120
boxes will be purchased and how many

00:22:54,380 --> 00:23:00,590
shoppers are even interacting with your

00:22:56,120 --> 00:23:02,810
store as your store starts to grow it

00:23:00,590 --> 00:23:04,090
becomes harder to understand Everett how

00:23:02,810 --> 00:23:06,500
everything works together

00:23:04,090 --> 00:23:09,680
maybe you've added new features to your

00:23:06,500 --> 00:23:11,960
product page and recently added coupon

00:23:09,680 --> 00:23:15,950
codes as a feature in your checkout

00:23:11,960 --> 00:23:20,150
process how do you know that your core

00:23:15,950 --> 00:23:23,630
store features are still working that's

00:23:20,150 --> 00:23:25,700
where custom metrics start to shine you

00:23:23,630 --> 00:23:29,180
can start to instrument higher volume

00:23:25,700 --> 00:23:31,910
events like Add to Cart first and as

00:23:29,180 --> 00:23:35,150
traffic continues to grow you can add on

00:23:31,910 --> 00:23:39,490
other critical features as discreet

00:23:35,150 --> 00:23:39,490
monitored events as well later on

00:23:44,270 --> 00:23:49,230
when it comes to implementing custom

00:23:47,220 --> 00:23:53,669
metrics ask yourselves

00:23:49,230 --> 00:23:57,980
two questions what way would I want to

00:23:53,669 --> 00:23:57,980
know before my customer Note notices

00:23:58,460 --> 00:24:03,570
target monitoring in places where you

00:24:01,830 --> 00:24:07,200
you're delivering the most value to your

00:24:03,570 --> 00:24:15,410
customers and secondly what systems are

00:24:07,200 --> 00:24:20,090
most brittle and at risk of breaking as

00:24:15,410 --> 00:24:24,290
your system expand expands in scope

00:24:20,090 --> 00:24:27,120
introduce more scope more structure

00:24:24,290 --> 00:24:29,460
consider defining the importance of

00:24:27,120 --> 00:24:32,250
specific services or components more

00:24:29,460 --> 00:24:36,360
explicitly and mapping those

00:24:32,250 --> 00:24:39,720
requirements specifically to those tiers

00:24:36,360 --> 00:24:42,740
of importance you can map specific

00:24:39,720 --> 00:24:46,970
requirements like monitoring and on-call

00:24:42,740 --> 00:24:51,390
to those tiers so that it's very clear

00:24:46,970 --> 00:24:54,090
what types of reliability and

00:24:51,390 --> 00:24:56,070
observability need to be in place given

00:24:54,090 --> 00:24:59,040
the importance of that system and then

00:24:56,070 --> 00:25:05,820
constantly reevaluate the importance of

00:24:59,040 --> 00:25:08,790
those particular systems to recap bugs

00:25:05,820 --> 00:25:12,630
are inevitable and our fixture of

00:25:08,790 --> 00:25:15,150
software development but our

00:25:12,630 --> 00:25:21,540
responsibility is not over when we press

00:25:15,150 --> 00:25:25,320
deploy observability is the foundation

00:25:21,540 --> 00:25:28,710
of building a reliable system start

00:25:25,320 --> 00:25:33,419
small turn key tools like exception

00:25:28,710 --> 00:25:35,340
trackers and APM turnkey tools like

00:25:33,419 --> 00:25:38,809
exception trackers an APM provide a huge

00:25:35,340 --> 00:25:41,549
amount of value with limited investment

00:25:38,809 --> 00:25:43,410
measure things that are critical to your

00:25:41,549 --> 00:25:46,980
application don't try to measure

00:25:43,410 --> 00:25:50,400
everything and finally iterate on your

00:25:46,980 --> 00:25:53,460
monitoring rules tune them to make

00:25:50,400 --> 00:25:57,260
alerts of higher fidelity and more

00:25:53,460 --> 00:25:57,260
actionable for your responders

00:25:57,370 --> 00:26:00,210

YouTube URL: https://www.youtube.com/watch?v=bSPvuaFtN6M


