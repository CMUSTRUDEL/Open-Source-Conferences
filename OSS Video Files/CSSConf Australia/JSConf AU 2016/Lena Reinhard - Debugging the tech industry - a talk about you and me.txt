Title: Lena Reinhard - Debugging the tech industry - a talk about you and me
Publication date: 2017-05-19
Playlist: JSConf AU 2016
Description: 
	Thank you to Mozilla for making video recording possible.

Going into the year 2017, what does it mean to work on software today? - No matter if we're software developers, designers, product managers, leaders, or work on software in any other way: our work has impact. But how is this impact constituted, what does it look like in practice - and what does all of this mean for us?

This talk shows why we need to understand and care about the social and ethical implications of our work, and highlights ways for approaching these topics - by ourselves, and in conversations with the people we work with. This talk wants to encourage you to think about your role, your power, and your responsibilities. You'll learn what you can do to live up to them - and how you can help debug the tech industry.
Captions: 
	00:00:07,520 --> 00:00:14,100
hello Def Con how are you doing

00:00:10,590 --> 00:00:18,000
did you have a good day did you have a

00:00:14,100 --> 00:00:19,800
really good day I would like to ask you

00:00:18,000 --> 00:00:21,330
to give a big hand to the organizers for

00:00:19,800 --> 00:00:30,359
putting together this fantastic event

00:00:21,330 --> 00:00:32,850
thanks very much and thanks for the

00:00:30,359 --> 00:00:35,790
beautiful intro Karina so my name is

00:00:32,850 --> 00:00:37,680
Lena why not I'm at LR and arty on

00:00:35,790 --> 00:00:40,230
Twitter it's very hard to remember it so

00:00:37,680 --> 00:00:42,210
I got it on the slides and I'm leading

00:00:40,230 --> 00:00:43,920
the design and platform engineering team

00:00:42,210 --> 00:00:46,170
at travesty is so if you want to try the

00:00:43,920 --> 00:00:49,260
speaker later come find me at the party

00:00:46,170 --> 00:00:52,829
I'm also a writer a photographer and

00:00:49,260 --> 00:00:57,480
well I am a speaker which also brings me

00:00:52,829 --> 00:01:00,390
to the reason why I'm here today this is

00:00:57,480 --> 00:01:03,809
a talk about debugging about the tech

00:01:00,390 --> 00:01:09,000
industry and most of all this is a talk

00:01:03,809 --> 00:01:10,200
about you and me let's start by talking

00:01:09,000 --> 00:01:14,190
about software bugs

00:01:10,200 --> 00:01:16,080
what are bugs bugs our errors flaws or

00:01:14,190 --> 00:01:18,570
failures in software that cause it to

00:01:16,080 --> 00:01:21,720
produce incorrect undesired or

00:01:18,570 --> 00:01:23,630
unexpected output a bug can be many

00:01:21,720 --> 00:01:25,740
things like a performance problem

00:01:23,630 --> 00:01:29,370
configuration issue an issue with

00:01:25,740 --> 00:01:31,650
functionality or a system error at all

00:01:29,370 --> 00:01:33,570
points in our development cycle we can

00:01:31,650 --> 00:01:39,690
end up with bugs in our software or

00:01:33,570 --> 00:01:43,070
systems so we need to debug it we need

00:01:39,690 --> 00:01:45,630
to find and resolve these defects and

00:01:43,070 --> 00:01:48,360
with increasing complexity of our

00:01:45,630 --> 00:01:50,370
software and systems debugging becomes

00:01:48,360 --> 00:01:54,390
more and more difficult like with these

00:01:50,370 --> 00:01:56,370
players here the system that I want to

00:01:54,390 --> 00:01:59,310
talk to you about today in is more than

00:01:56,370 --> 00:02:02,130
code it's a complex system that's grown

00:01:59,310 --> 00:02:05,060
over many years a system that were all

00:02:02,130 --> 00:02:10,289
part of and that we all contribute to

00:02:05,060 --> 00:02:12,480
the system is the tech industry in order

00:02:10,289 --> 00:02:15,359
to debug a system we need to understand

00:02:12,480 --> 00:02:17,349
it and to understand the system there

00:02:15,359 --> 00:02:20,090
are a few things that we can do

00:02:17,349 --> 00:02:22,340
we can look at each individual component

00:02:20,090 --> 00:02:24,980
of the system we can look at our overall

00:02:22,340 --> 00:02:28,250
components interact and we can look at

00:02:24,980 --> 00:02:30,950
the system output so let's take a look

00:02:28,250 --> 00:02:34,569
at the tech industry as a system its

00:02:30,950 --> 00:02:37,250
components their interactions and output

00:02:34,569 --> 00:02:40,700
I'll show all of that to you in a

00:02:37,250 --> 00:02:43,849
simplified Venn diagram the tech in the

00:02:40,700 --> 00:02:45,980
screen has a few major components we

00:02:43,849 --> 00:02:47,750
have communities like free labor and

00:02:45,980 --> 00:02:50,860
open source software communities and

00:02:47,750 --> 00:02:53,780
here you can see one sample community

00:02:50,860 --> 00:02:55,580
there are also companies from a few

00:02:53,780 --> 00:02:59,569
people who just launched the startup to

00:02:55,580 --> 00:03:01,310
giant corporations and like any system

00:02:59,569 --> 00:03:04,099
the tech industry doesn't exist in

00:03:01,310 --> 00:03:06,440
isolation with all those components this

00:03:04,099 --> 00:03:10,910
industries or for part of our societies

00:03:06,440 --> 00:03:15,080
and then there's one more component one

00:03:10,910 --> 00:03:17,480
human like you and me you and me each of

00:03:15,080 --> 00:03:21,109
us are of one individual component in

00:03:17,480 --> 00:03:24,680
this system we are the smallest entity

00:03:21,109 --> 00:03:26,600
but our role is important all of us are

00:03:24,680 --> 00:03:29,090
part of communities companies this

00:03:26,600 --> 00:03:32,870
industry and all of us influence them

00:03:29,090 --> 00:03:34,700
and we are influenced by them some of us

00:03:32,870 --> 00:03:37,609
contribute to open source projects and

00:03:34,700 --> 00:03:40,609
software licenses determine how our work

00:03:37,609 --> 00:03:41,930
can be reused and distributed will also

00:03:40,609 --> 00:03:45,170
work on products and projects and

00:03:41,930 --> 00:03:47,180
companies and these companies determine

00:03:45,170 --> 00:03:50,359
how we work what we work on and how we

00:03:47,180 --> 00:03:52,250
compensated we also decide to how

00:03:50,359 --> 00:03:55,069
systems are designed and which code is

00:03:52,250 --> 00:03:57,920
written and our own social norms

00:03:55,069 --> 00:04:02,780
determine the way we think and the way

00:03:57,920 --> 00:04:04,730
we make decisions and so forth but a

00:04:02,780 --> 00:04:07,040
system is more than just the sum of its

00:04:04,730 --> 00:04:08,510
parts all these components are

00:04:07,040 --> 00:04:10,579
intertwined and they accumulate

00:04:08,510 --> 00:04:13,760
characteristics of other components and

00:04:10,579 --> 00:04:17,570
together all of them influence the tech

00:04:13,760 --> 00:04:20,539
industry as a system and they produce

00:04:17,570 --> 00:04:22,789
output this output is the software we

00:04:20,539 --> 00:04:25,580
build and each and every one of us

00:04:22,789 --> 00:04:29,070
influences this output no matter if we

00:04:25,580 --> 00:04:32,110
are aware of it or not

00:04:29,070 --> 00:04:34,060
who of you all is a software developer

00:04:32,110 --> 00:04:35,440
designer project manager or works on

00:04:34,060 --> 00:04:38,970
software in any other way

00:04:35,440 --> 00:04:42,640
hands up that's almost the whole room

00:04:38,970 --> 00:04:46,830
who of you has ever harmed a user of

00:04:42,640 --> 00:04:50,500
your software with something you did

00:04:46,830 --> 00:04:52,480
it's probably 20 people or so and love

00:04:50,500 --> 00:04:56,650
you is not sure if you've ever harmed a

00:04:52,480 --> 00:05:00,160
user with something you did we can never

00:04:56,650 --> 00:05:02,310
know for sure can we all of us need to

00:05:00,160 --> 00:05:04,960
care about the impact of our work as

00:05:02,310 --> 00:05:09,460
individuals and as an industry and

00:05:04,960 --> 00:05:11,980
here's why as a still relatively young

00:05:09,460 --> 00:05:13,660
industry we have accumulated a massive

00:05:11,980 --> 00:05:16,840
amount of influence within a very short

00:05:13,660 --> 00:05:19,870
time our work has impacts and put

00:05:16,840 --> 00:05:21,340
impacts comes responsibility these

00:05:19,870 --> 00:05:24,330
impact and responsibilities are

00:05:21,340 --> 00:05:27,030
constituted through four major factors

00:05:24,330 --> 00:05:29,650
first of all there's your big routine

00:05:27,030 --> 00:05:30,880
software systems are ubiquitous in all

00:05:29,650 --> 00:05:33,610
aspects of public and private

00:05:30,880 --> 00:05:35,620
institutions software is running on

00:05:33,610 --> 00:05:38,700
everything from computers to juicers

00:05:35,620 --> 00:05:41,140
smoke detectors and washing machines

00:05:38,700 --> 00:05:44,020
software systems also lie at the heart

00:05:41,140 --> 00:05:46,450
of modern decision making algorithms

00:05:44,020 --> 00:05:49,090
decide how much our insurance will cost

00:05:46,450 --> 00:05:52,570
us whether we get credit or not and

00:05:49,090 --> 00:05:54,790
whether we're potential criminals but do

00:05:52,570 --> 00:06:00,220
we really know where our software is run

00:05:54,790 --> 00:06:03,580
and what is useful our impact also comes

00:06:00,220 --> 00:06:05,140
with our role and with scale for most

00:06:03,580 --> 00:06:07,150
tech companies the word of our

00:06:05,140 --> 00:06:09,220
addressable market and there are only

00:06:07,150 --> 00:06:12,310
few Geographic constraints for where we

00:06:09,220 --> 00:06:15,790
can do business we can build things for

00:06:12,310 --> 00:06:17,890
a lot of very different people but do we

00:06:15,790 --> 00:06:18,460
really know what all users of our

00:06:17,890 --> 00:06:24,040
software

00:06:18,460 --> 00:06:25,930
I like impact and responsibility also

00:06:24,040 --> 00:06:28,600
really comes with our own software

00:06:25,930 --> 00:06:31,750
development is an act of representation

00:06:28,600 --> 00:06:34,470
it means we need to provide solutions to

00:06:31,750 --> 00:06:37,210
actual problems that people are facing

00:06:34,470 --> 00:06:40,730
but do we know what people really need

00:06:37,210 --> 00:06:44,250
and do we even want to know

00:06:40,730 --> 00:06:47,100
and our impact lastly come through

00:06:44,250 --> 00:06:49,800
dependency people rely on and Trust

00:06:47,100 --> 00:06:51,660
software not because they choose to

00:06:49,800 --> 00:06:54,300
trust it but because they usually don't

00:06:51,660 --> 00:06:56,760
have a choice because they depend on it

00:06:54,300 --> 00:06:59,040
to achieve what they need to achieve if

00:06:56,760 --> 00:07:00,750
people don't use social networks or

00:06:59,040 --> 00:07:02,910
messenger apps access to friends or

00:07:00,750 --> 00:07:05,250
family becomes more difficult if

00:07:02,910 --> 00:07:06,690
websites aren't accessible disabled

00:07:05,250 --> 00:07:10,170
people can't get access to information

00:07:06,690 --> 00:07:14,210
that they need but how do we all handle

00:07:10,170 --> 00:07:14,210
the effects that people depend on us

00:07:14,540 --> 00:07:19,950
ubiquitous panel representation and

00:07:16,980 --> 00:07:22,850
dependency all these factors lead to the

00:07:19,950 --> 00:07:26,210
giant impact of our work and our

00:07:22,850 --> 00:07:26,210
responsibility for it

00:07:26,540 --> 00:07:31,920
so what does our software do to people

00:07:29,340 --> 00:07:36,180
right now what does our system output as

00:07:31,920 --> 00:07:38,760
an industry look like software can have

00:07:36,180 --> 00:07:40,590
positive impact on people's life it can

00:07:38,760 --> 00:07:42,840
help people connect with others get

00:07:40,590 --> 00:07:44,670
access to information or support help

00:07:42,840 --> 00:07:45,350
them get where they need to go and so

00:07:44,670 --> 00:07:48,150
much more

00:07:45,350 --> 00:07:52,110
software can empower people and that's a

00:07:48,150 --> 00:07:54,870
great thing but then there's the other

00:07:52,110 --> 00:07:59,760
side of software the other side that we

00:07:54,870 --> 00:08:02,060
rarely think about software enables it

00:07:59,760 --> 00:08:04,500
enables harassment and abuse any

00:08:02,060 --> 00:08:07,200
software with human interactions is a

00:08:04,500 --> 00:08:08,850
greenhouse for harassment this includes

00:08:07,200 --> 00:08:11,160
important networks like Facebook or

00:08:08,850 --> 00:08:15,330
Twitter and any other software was built

00:08:11,160 --> 00:08:17,130
in human interactions 40% of adults on

00:08:15,330 --> 00:08:20,400
the internet had experienced online

00:08:17,130 --> 00:08:22,200
harassment widespread harassment and

00:08:20,400 --> 00:08:24,680
abuse have made these important

00:08:22,200 --> 00:08:27,000
technologies unusable for many people

00:08:24,680 --> 00:08:29,670
numerous people like people of color

00:08:27,000 --> 00:08:31,470
queer trans people activists and many

00:08:29,670 --> 00:08:33,599
more had to leave these platforms

00:08:31,470 --> 00:08:40,080
because there has meant literally made

00:08:33,599 --> 00:08:43,469
them sick software needs users needs but

00:08:40,080 --> 00:08:45,270
only some users need when Apple

00:08:43,469 --> 00:08:48,350
introduced itself kit app in September

00:08:45,270 --> 00:08:51,740
2014 you could track everything their

00:08:48,350 --> 00:08:54,550
measurements fitness nutrition sleep

00:08:51,740 --> 00:08:56,830
vitals but not your period

00:08:54,550 --> 00:08:58,690
it took F of one be here until they

00:08:56,830 --> 00:09:02,080
finally introduced period trekking for

00:08:58,690 --> 00:09:03,910
health kids and as a study from earlier

00:09:02,080 --> 00:09:04,600
this year showed vibes Virtual

00:09:03,910 --> 00:09:07,210
Assistants

00:09:04,600 --> 00:09:10,270
like Apple Siri Google now and Microsoft

00:09:07,210 --> 00:09:12,640
Cortana redirected users to help when

00:09:10,270 --> 00:09:14,350
they said they had a heart attack but

00:09:12,640 --> 00:09:16,270
none of these assistants were able to

00:09:14,350 --> 00:09:18,940
recognize what it meant when a user said

00:09:16,270 --> 00:09:20,860
that they're being abused and one in

00:09:18,940 --> 00:09:25,180
three women worldwide is abused at some

00:09:20,860 --> 00:09:29,040
point in her life software also

00:09:25,180 --> 00:09:31,990
recognizes people as long as their white

00:09:29,040 --> 00:09:34,840
face detection software and cameras can

00:09:31,990 --> 00:09:36,670
warn people when they're blinking but it

00:09:34,840 --> 00:09:38,770
also asked Asian people if they were

00:09:36,670 --> 00:09:42,850
blinking no matter if they were actually

00:09:38,770 --> 00:09:44,880
blinking in portrait photos or not other

00:09:42,850 --> 00:09:47,440
face detection software and webcams

00:09:44,880 --> 00:09:50,890
completely failed to even recognize the

00:09:47,440 --> 00:09:53,470
faces of black people Flickr photo

00:09:50,890 --> 00:09:55,420
tagging algorithm or detect the photo of

00:09:53,470 --> 00:09:59,610
a black man with the words animal and

00:09:55,420 --> 00:10:03,450
ape and recently six thousand people

00:09:59,610 --> 00:10:06,040
participated in an online beauty contest

00:10:03,450 --> 00:10:08,710
these six thousand people were from 100

00:10:06,040 --> 00:10:11,740
countries worldwide and all photos were

00:10:08,710 --> 00:10:14,290
judged by an algorithm out of the 44

00:10:11,740 --> 00:10:16,840
winners of this contest almost all were

00:10:14,290 --> 00:10:19,840
whites a handful were Asian and only one

00:10:16,840 --> 00:10:24,090
person had dark skin although many

00:10:19,840 --> 00:10:26,770
people of color had submitted photos and

00:10:24,090 --> 00:10:29,590
when our software is learning what does

00:10:26,770 --> 00:10:31,570
it what does it actually learn when

00:10:29,590 --> 00:10:34,350
Microsoft lone launched its artificial

00:10:31,570 --> 00:10:37,090
intelligence chat forte six months ago

00:10:34,350 --> 00:10:40,780
the but was supposed to learn humans

00:10:37,090 --> 00:10:42,900
each pattern within only a few hours the

00:10:40,780 --> 00:10:46,600
bots started posting racist fascist

00:10:42,900 --> 00:10:50,880
sexist homophobic and other tweets and

00:10:46,600 --> 00:10:53,200
it started harassing twitter users and

00:10:50,880 --> 00:10:55,690
when I was soft very helps people get in

00:10:53,200 --> 00:10:59,110
from access to information what kind of

00:10:55,690 --> 00:11:01,570
information is that recent report found

00:10:59,110 --> 00:11:03,690
that false or misleading content which

00:11:01,570 --> 00:11:06,070
reinforces people's existing beliefs

00:11:03,690 --> 00:11:07,050
receive stronger engagement rates on

00:11:06,070 --> 00:11:10,830
Facebook

00:11:07,050 --> 00:11:12,570
then accurate sexual content algorithms

00:11:10,830 --> 00:11:19,170
became and forceful for confirmation

00:11:12,570 --> 00:11:21,120
bias and last cleans foster empowers it

00:11:19,170 --> 00:11:23,070
enables privacy violations and

00:11:21,120 --> 00:11:26,279
distributed denial-of-service attacks or

00:11:23,070 --> 00:11:29,760
DDoS attacks the so-called Internet of

00:11:26,279 --> 00:11:32,220
Things is a privacy nightmare routers

00:11:29,760 --> 00:11:35,250
connected baby monitors readings window

00:11:32,220 --> 00:11:37,260
blinds webcam TVs toasters fridges

00:11:35,250 --> 00:11:40,500
vacuum cleaners and many other devices

00:11:37,260 --> 00:11:43,230
are in people's homes estimates say that

00:11:40,500 --> 00:11:46,890
by 2020 there will be more than 20

00:11:43,230 --> 00:11:49,920
billion connected devices worldwide many

00:11:46,890 --> 00:11:52,709
of these devices are unprotected buggies

00:11:49,920 --> 00:11:54,630
have built-in backdoors there are open

00:11:52,709 --> 00:12:00,450
doors to hackers and they're used for

00:11:54,630 --> 00:12:02,459
DDoS attacks and all of these this is a

00:12:00,450 --> 00:12:04,980
summary of the examples I just mentioned

00:12:02,459 --> 00:12:07,950
all of these are just very few examples

00:12:04,980 --> 00:12:09,450
of what software does yes there is

00:12:07,950 --> 00:12:12,440
software that does great things for

00:12:09,450 --> 00:12:15,270
people that empowers and enables them

00:12:12,440 --> 00:12:16,829
and many of these problematic examples I

00:12:15,270 --> 00:12:20,010
mentioned earlier have been fixed in the

00:12:16,829 --> 00:12:21,060
mean time but none of these problems

00:12:20,010 --> 00:12:24,209
should have been there in the first

00:12:21,060 --> 00:12:27,480
place and such issues like these keep

00:12:24,209 --> 00:12:29,730
happening there are far too many far too

00:12:27,480 --> 00:12:33,420
bad examples for the harm that we do as

00:12:29,730 --> 00:12:36,980
an industry this industry is a broken

00:12:33,420 --> 00:12:40,230
system and it produces broken output

00:12:36,980 --> 00:12:43,529
output that has severe consequences on

00:12:40,230 --> 00:12:46,950
people's life and all of us need to stop

00:12:43,529 --> 00:12:49,589
ignoring that we need to understand

00:12:46,950 --> 00:12:52,680
every piece of software is the result of

00:12:49,589 --> 00:12:55,380
a judgement call everybody every word

00:12:52,680 --> 00:12:59,579
every pop-up if we add every line of

00:12:55,380 --> 00:13:01,709
code right now through our software all

00:12:59,579 --> 00:13:06,269
of us are forcing our own prejudices

00:13:01,709 --> 00:13:08,010
norms assumptions our racism sexism and

00:13:06,269 --> 00:13:12,050
our narrow views of the world on the

00:13:08,010 --> 00:13:14,459
people who use it the software we build

00:13:12,050 --> 00:13:17,820
systematically ignores people and their

00:13:14,459 --> 00:13:19,800
needs with the software we build we are

00:13:17,820 --> 00:13:23,130
normalizing racism

00:13:19,800 --> 00:13:25,769
sexism homophobia harassment

00:13:23,130 --> 00:13:29,100
misinformation and surveillance on our

00:13:25,769 --> 00:13:31,110
platforms we are creating new realities

00:13:29,100 --> 00:13:35,040
in which all of these things are the new

00:13:31,110 --> 00:13:38,399
status clothes and that is a serious

00:13:35,040 --> 00:13:40,589
problem we are the industry that's

00:13:38,399 --> 00:13:42,839
building self-driving cars and planning

00:13:40,589 --> 00:13:44,490
mass missions and we haven't even

00:13:42,839 --> 00:13:48,570
figured out how to make sure people's

00:13:44,490 --> 00:13:50,670
vacuum cleaners don't join botnets as an

00:13:48,570 --> 00:13:53,339
industry we love talking about the next

00:13:50,670 --> 00:13:55,709
big thing that our technology could do

00:13:53,339 --> 00:13:57,779
we love thinking and talking about

00:13:55,709 --> 00:14:00,209
artificial intelligence self-driving

00:13:57,779 --> 00:14:03,290
cars and math missions and all of these

00:14:00,209 --> 00:14:06,120
things are great and they're important

00:14:03,290 --> 00:14:09,060
but by building software we create

00:14:06,120 --> 00:14:11,790
environment and all of us we are the

00:14:09,060 --> 00:14:14,160
ones who are making the rules for these

00:14:11,790 --> 00:14:18,360
environments for how they work and for

00:14:14,160 --> 00:14:20,670
whom they work and at the same time we

00:14:18,360 --> 00:14:23,360
like answers to fundamental questions of

00:14:20,670 --> 00:14:26,850
the ethical implications of our work and

00:14:23,360 --> 00:14:28,500
most of the time we even refuse to ask

00:14:26,850 --> 00:14:32,820
ourself questions about these

00:14:28,500 --> 00:14:34,350
implications building software is so

00:14:32,820 --> 00:14:38,399
much more than typing code into an

00:14:34,350 --> 00:14:41,339
editor we are enables enablers for

00:14:38,399 --> 00:14:45,240
empowerment of people and enables for

00:14:41,339 --> 00:14:46,980
harassment and abuse this means that we

00:14:45,240 --> 00:14:50,700
need to understand social and ethical

00:14:46,980 --> 00:14:53,279
issues no matter if we care about them

00:14:50,700 --> 00:14:56,250
or not ethics are built into everything

00:14:53,279 --> 00:14:58,610
we do so we need to be deliberate about

00:14:56,250 --> 00:14:58,610
them

00:15:05,850 --> 00:15:13,029
ethics are moral principles that govern

00:15:09,250 --> 00:15:15,070
our actions there are limitations for

00:15:13,029 --> 00:15:18,490
how ethical we can be in an industry

00:15:15,070 --> 00:15:20,470
that's part of capitalism but that also

00:15:18,490 --> 00:15:23,700
means that we need to be even more aware

00:15:20,470 --> 00:15:26,050
of what we do and how we do it

00:15:23,700 --> 00:15:28,440
ethics and software development means

00:15:26,050 --> 00:15:31,270
thinking past current trends and

00:15:28,440 --> 00:15:34,630
anticipating every future utilization of

00:15:31,270 --> 00:15:36,100
what we built and as an industry we

00:15:34,630 --> 00:15:41,080
don't care about ethics enough and

00:15:36,100 --> 00:15:43,630
here's why first of all ethics are

00:15:41,080 --> 00:15:46,360
complex they're pretty obvious in some

00:15:43,630 --> 00:15:49,089
cases like software theft or deliberate

00:15:46,360 --> 00:15:53,529
invasions of privacy but in those cases

00:15:49,089 --> 00:15:54,880
ethics are far more subtle ethics also

00:15:53,529 --> 00:15:56,800
aren't part of computer science

00:15:54,880 --> 00:15:58,390
curriculum and not built into our

00:15:56,800 --> 00:16:01,420
processes in our companies and

00:15:58,390 --> 00:16:04,209
communities ethics aren't a checklist

00:16:01,420 --> 00:16:08,170
and we don't make room for thinking

00:16:04,209 --> 00:16:10,149
about them we also don't care about

00:16:08,170 --> 00:16:12,820
ethics because ethics aren't automated

00:16:10,149 --> 00:16:14,920
we have automation plays for showing us

00:16:12,820 --> 00:16:16,660
the technical limits of our work for

00:16:14,920 --> 00:16:19,839
when our tests are failing and for when

00:16:16,660 --> 00:16:21,399
servers are reaching their limits but we

00:16:19,839 --> 00:16:24,570
have no built-in boundaries for

00:16:21,399 --> 00:16:28,680
violations of our users rights and

00:16:24,570 --> 00:16:32,410
lastly we refuse to understand our role

00:16:28,680 --> 00:16:34,779
we all know the stereotypes the

00:16:32,410 --> 00:16:37,089
stereotypes the picture of as a bunch of

00:16:34,779 --> 00:16:39,430
unathletic and creative loners that wear

00:16:37,089 --> 00:16:41,260
glasses work all night all day from a

00:16:39,430 --> 00:16:44,320
basement probably their parents basement

00:16:41,260 --> 00:16:46,120
with no human interaction society is

00:16:44,320 --> 00:16:49,480
based on energy drinks and cold pizza

00:16:46,120 --> 00:16:51,550
and who have no social life this is

00:16:49,480 --> 00:16:54,579
actually a stock photo that I got out of

00:16:51,550 --> 00:16:58,839
that description and all of us know that

00:16:54,579 --> 00:17:01,329
these stereotypes are wrong but we act

00:16:58,839 --> 00:17:02,950
like they're still holding true we are

00:17:01,329 --> 00:17:04,920
still behaving like we're just a bunch

00:17:02,950 --> 00:17:07,929
of people hacking on some cool stuff

00:17:04,920 --> 00:17:10,750
when in fact all of us have become so

00:17:07,929 --> 00:17:12,850
privileged so powerful when our

00:17:10,750 --> 00:17:14,940
companies are among the most influential

00:17:12,850 --> 00:17:17,160
and most valuable in the world

00:17:14,940 --> 00:17:21,980
and when our work has such fundamental

00:17:17,160 --> 00:17:24,270
consequences in people's life our

00:17:21,980 --> 00:17:27,630
scientists have done things which nobody

00:17:24,270 --> 00:17:29,190
has ever done before yeah yeah but your

00:17:27,630 --> 00:17:31,860
scientists were so preoccupied with

00:17:29,190 --> 00:17:34,920
whether or not they could that they

00:17:31,860 --> 00:17:36,240
didn't stop to think if they should it's

00:17:34,920 --> 00:17:39,750
probably one of the most famous quotes

00:17:36,240 --> 00:17:42,840
from Jurassic Park as people in this

00:17:39,750 --> 00:17:44,850
industry we have power and with great

00:17:42,840 --> 00:17:47,880
power comes great responsibility and

00:17:44,850 --> 00:17:50,450
that's a platitude but we even fail at

00:17:47,880 --> 00:17:53,700
acknowledging that very simple truth

00:17:50,450 --> 00:17:56,700
right now all of us are so preoccupied

00:17:53,700 --> 00:18:01,800
with whether or not we can that we don't

00:17:56,700 --> 00:18:04,290
think whether we should every one of us

00:18:01,800 --> 00:18:07,050
has impact on our company's communities

00:18:04,290 --> 00:18:10,470
and this industry and on the software we

00:18:07,050 --> 00:18:12,720
build no matter who you are in tech at

00:18:10,470 --> 00:18:15,480
what point you are in your career or

00:18:12,720 --> 00:18:18,360
what you're working on you can make

00:18:15,480 --> 00:18:20,760
change happen and we need you to make a

00:18:18,360 --> 00:18:24,120
difference the people using technology

00:18:20,760 --> 00:18:26,940
you need you to make a difference use

00:18:24,120 --> 00:18:29,850
the power you have for good use your

00:18:26,940 --> 00:18:34,050
power to debug this industry and to make

00:18:29,850 --> 00:18:36,390
sure it generates meaningful output over

00:18:34,050 --> 00:18:38,940
the next minute I'll show you five

00:18:36,390 --> 00:18:49,650
things you can do to debug the tech

00:18:38,940 --> 00:18:53,250
industry step one to debugging the tech

00:18:49,650 --> 00:18:55,860
industry is understanding ourselves if

00:18:53,250 --> 00:18:58,530
people in tech most of us are extremely

00:18:55,860 --> 00:19:00,750
privileged having privilege means that

00:18:58,530 --> 00:19:02,970
we can exist in our society and this

00:19:00,750 --> 00:19:05,820
industry with ease and gain influence

00:19:02,970 --> 00:19:07,830
without scrutiny or suspicion which

00:19:05,820 --> 00:19:10,980
gives us advantages relative to other

00:19:07,830 --> 00:19:13,700
people our privilege comes from factors

00:19:10,980 --> 00:19:16,890
like our gender ethnicity class

00:19:13,700 --> 00:19:21,360
education upbringing physical and mental

00:19:16,890 --> 00:19:23,670
health and many more privilege is the

00:19:21,360 --> 00:19:26,390
human version of but it works on my

00:19:23,670 --> 00:19:28,210
machine or it works in my life

00:19:26,390 --> 00:19:30,940
privilege puts us in a

00:19:28,210 --> 00:19:33,130
vision where we don't see problems that

00:19:30,940 --> 00:19:35,140
are workhorses and don't see problems in

00:19:33,130 --> 00:19:38,800
this industry because we're not

00:19:35,140 --> 00:19:41,290
personally affected by them we need to

00:19:38,800 --> 00:19:43,570
work through our privileges we need to

00:19:41,290 --> 00:19:46,120
understand that not everyone has what we

00:19:43,570 --> 00:19:48,600
have but not everyone has the money

00:19:46,120 --> 00:19:51,280
electricity high-speed Internet

00:19:48,600 --> 00:19:54,460
education and so many more things that

00:19:51,280 --> 00:19:56,680
all of us are just used to and we need

00:19:54,460 --> 00:19:58,810
to understand that not everyone has the

00:19:56,680 --> 00:20:01,870
same privileged position in society that

00:19:58,810 --> 00:20:04,810
many others have we need to understand

00:20:01,870 --> 00:20:08,640
that what's safe and easy for us is not

00:20:04,810 --> 00:20:11,020
safe and easy for many other people and

00:20:08,640 --> 00:20:13,510
it has to become visible in our work

00:20:11,020 --> 00:20:15,910
that we understand that we need to

00:20:13,510 --> 00:20:18,610
question our beliefs our reasoning and

00:20:15,910 --> 00:20:23,800
our assumptions and we need to stop

00:20:18,610 --> 00:20:25,750
assuming that everyone is like us in our

00:20:23,800 --> 00:20:29,080
work and interacting with people we also

00:20:25,750 --> 00:20:31,060
need to address our biases biases shape

00:20:29,080 --> 00:20:33,730
our decisions and they're based on our

00:20:31,060 --> 00:20:36,970
stereotypes assumptions knowledge and

00:20:33,730 --> 00:20:39,460
cultural norms we need to be very

00:20:36,970 --> 00:20:42,280
conscious and deliberate about how and

00:20:39,460 --> 00:20:46,090
why we make decisions and about how we

00:20:42,280 --> 00:20:48,220
interact with people our privileges and

00:20:46,090 --> 00:20:51,430
biases are baked into everything we do

00:20:48,220 --> 00:20:55,660
and they contribute to the horrifying

00:20:51,430 --> 00:20:58,050
output of the tech industry same goes

00:20:55,660 --> 00:21:01,000
for our collective lack of empathy

00:20:58,050 --> 00:21:03,160
having empathy means being able to

00:21:01,000 --> 00:21:06,430
relate to another person as though they

00:21:03,160 --> 00:21:10,270
were us and as an industry we are

00:21:06,430 --> 00:21:13,810
lacking empathy but empathy is the main

00:21:10,270 --> 00:21:16,570
determinant for how are usable how safe

00:21:13,810 --> 00:21:20,770
and how meaningful our technology is for

00:21:16,570 --> 00:21:23,790
other people as people in tech empathy

00:21:20,770 --> 00:21:23,790
is our responsibility

00:21:25,290 --> 00:21:31,270
step 2 - debugging the tech industry is

00:21:28,410 --> 00:21:35,710
we need to care about our work and its

00:21:31,270 --> 00:21:38,530
impact we need to understand that we are

00:21:35,710 --> 00:21:40,580
not neutral that our code is not neutral

00:21:38,530 --> 00:21:44,149
our algorithms are not you

00:21:40,580 --> 00:21:48,200
our technology is not neutral our work

00:21:44,149 --> 00:21:50,840
is political we need to learn about

00:21:48,200 --> 00:21:54,350
ethics and the ethical implications that

00:21:50,840 --> 00:21:56,240
our work has on people and we need to

00:21:54,350 --> 00:22:01,760
make room for thinking about ethics and

00:21:56,240 --> 00:22:03,740
our companies and communities and we

00:22:01,760 --> 00:22:06,039
need to ask questions that challenge the

00:22:03,740 --> 00:22:08,600
status quo

00:22:06,039 --> 00:22:10,220
yesterday morning on my way here I got

00:22:08,600 --> 00:22:14,419
coffee from a place around the corner

00:22:10,220 --> 00:22:16,100
and this is what it said on the cup if

00:22:14,419 --> 00:22:18,289
you have any questions we will be able

00:22:16,100 --> 00:22:21,649
be happy to provide full and Beach oh

00:22:18,289 --> 00:22:23,840
sorry that was completely wrong so I'm

00:22:21,649 --> 00:22:25,250
doing that again if you have any answers

00:22:23,840 --> 00:22:27,169
that's where I'm going

00:22:25,250 --> 00:22:29,330
we will be happy to provide full and

00:22:27,169 --> 00:22:30,700
detailed questions else the rest of this

00:22:29,330 --> 00:22:34,490
talk is not going to make any more sense

00:22:30,700 --> 00:22:36,260
only so I saw that and took a photo that

00:22:34,490 --> 00:22:40,039
you can see you see SSKIN sign up there

00:22:36,260 --> 00:22:42,080
actually and like the people at this

00:22:40,039 --> 00:22:44,740
coffee place and probably also this cup

00:22:42,080 --> 00:22:47,240
of coffee I don't have answers for you

00:22:44,740 --> 00:22:49,929
but over the next minutes I'll pose a

00:22:47,240 --> 00:22:52,429
number of full and detailed questions

00:22:49,929 --> 00:22:56,059
questions that challenge the status quo

00:22:52,429 --> 00:22:58,370
I want to encourage you to think about

00:22:56,059 --> 00:23:00,889
these questions and discuss them with

00:22:58,370 --> 00:23:04,730
the people you work with and here are a

00:23:00,889 --> 00:23:06,110
few of them when we're working at

00:23:04,730 --> 00:23:08,779
companies or when were part of

00:23:06,110 --> 00:23:10,850
communities who's working there are

00:23:08,779 --> 00:23:16,190
they're people who are different from us

00:23:10,850 --> 00:23:18,320
are our organizations inclusive other

00:23:16,190 --> 00:23:21,500
people working at our company as diverse

00:23:18,320 --> 00:23:25,460
as the user base of our software if not

00:23:21,500 --> 00:23:27,679
why is that to our organizations of

00:23:25,460 --> 00:23:31,789
ethical guidelines for how they work and

00:23:27,679 --> 00:23:33,409
who they work with when we develop

00:23:31,789 --> 00:23:36,019
software do we move fast and break

00:23:33,409 --> 00:23:38,389
things even though that can mean we

00:23:36,019 --> 00:23:43,220
expose our users to data breaches bugs

00:23:38,389 --> 00:23:45,919
and security issues does our software

00:23:43,220 --> 00:23:48,380
actually do what it claims or do we

00:23:45,919 --> 00:23:51,580
trick users into doing things that they

00:23:48,380 --> 00:23:54,200
wouldn't want if they were aware of it

00:23:51,580 --> 00:23:55,850
do we really think through whether

00:23:54,200 --> 00:24:00,680
and how our software could be misused

00:23:55,850 --> 00:24:03,440
and when you fix bug 3:6 bugs of high

00:24:00,680 --> 00:24:05,960
paying customers first or do we treat

00:24:03,440 --> 00:24:08,030
them all equally and how do we decide

00:24:05,960 --> 00:24:12,710
whether a bug is serious enough to be

00:24:08,030 --> 00:24:15,410
fixed it's all software accessible if

00:24:12,710 --> 00:24:18,980
not why and who do we exclude by not

00:24:15,410 --> 00:24:21,320
making it accessible when you build

00:24:18,980 --> 00:24:23,360
interfaces and algorithms which

00:24:21,320 --> 00:24:28,370
information do we display to people is

00:24:23,360 --> 00:24:29,660
that information factually true what do

00:24:28,370 --> 00:24:33,040
we allow users to see in an offline

00:24:29,660 --> 00:24:35,570
state how much data do our s consume and

00:24:33,040 --> 00:24:37,700
what does that mean who for who can

00:24:35,570 --> 00:24:43,190
actually financially afford to use our

00:24:37,700 --> 00:24:47,300
software when we collect user data do we

00:24:43,190 --> 00:24:49,850
only collect what we really need when we

00:24:47,300 --> 00:24:52,400
store data could this data be used to

00:24:49,850 --> 00:24:55,280
track or profile people like protesters

00:24:52,400 --> 00:24:57,530
or activists what could happen to our

00:24:55,280 --> 00:25:00,590
users if our data is obtained by someone

00:24:57,530 --> 00:25:04,820
with bad intentions which risks are we

00:25:00,590 --> 00:25:07,490
exposing our users to and when we keep

00:25:04,820 --> 00:25:11,870
log files are they adequately protected

00:25:07,490 --> 00:25:15,970
who has access to them and when we say

00:25:11,870 --> 00:25:18,860
we delete data do we actually destroy it

00:25:15,970 --> 00:25:21,080
what do we do when we are asked to hand

00:25:18,860 --> 00:25:23,960
over user data to authorities what

00:25:21,080 --> 00:25:25,370
government regulations that are

00:25:23,960 --> 00:25:28,040
currently in place to prevent

00:25:25,370 --> 00:25:30,320
unreasonable demands can change anytime

00:25:28,040 --> 00:25:34,040
and it's a safe bet to say that they

00:25:30,320 --> 00:25:39,290
will what will we do when we receive

00:25:34,040 --> 00:25:41,810
demands to hand over data and lastly do

00:25:39,290 --> 00:25:44,360
we consider that paradigm around the

00:25:41,810 --> 00:25:48,200
software we build the data restore can

00:25:44,360 --> 00:25:50,770
change anytime in many countries across

00:25:48,200 --> 00:25:57,050
the globe fascism is on the rise again

00:25:50,770 --> 00:26:00,320
knowing that what will we do all of

00:25:57,050 --> 00:26:01,400
these are the questions I just asked and

00:26:00,320 --> 00:26:03,410
these are just a few of the questions

00:26:01,400 --> 00:26:06,990
that I want to encourage you to ask

00:26:03,410 --> 00:26:08,399
yourself these questions are important

00:26:06,990 --> 00:26:10,919
we need to discuss them with our

00:26:08,399 --> 00:26:13,799
co-workers our managers the people in

00:26:10,919 --> 00:26:17,429
our communities and we all need to work

00:26:13,799 --> 00:26:19,020
on finding answers to them this is what

00:26:17,429 --> 00:26:21,570
the second step to debugging the tech

00:26:19,020 --> 00:26:24,000
industry means we need to care about the

00:26:21,570 --> 00:26:26,539
impact of our work and we need to ask

00:26:24,000 --> 00:26:28,409
questions that challenge the status quo

00:26:26,539 --> 00:26:34,770
this is urgent

00:26:28,409 --> 00:26:35,370
because people's lives are at stake step

00:26:34,770 --> 00:26:38,429
00:26:35,370 --> 00:26:42,029
- debugging the tech industry is we need

00:26:38,429 --> 00:26:44,399
to rethink the way we work the output of

00:26:42,029 --> 00:26:46,860
our work is not just about an F we work

00:26:44,399 --> 00:26:50,250
on it's about what it enables people to

00:26:46,860 --> 00:26:53,159
do what it empowers them to do and what

00:26:50,250 --> 00:26:55,350
it exposes them to we need to approach

00:26:53,159 --> 00:26:57,450
our work holistically we need to

00:26:55,350 --> 00:27:00,270
understand which environment or software

00:26:57,450 --> 00:27:04,409
creates and how this environment impacts

00:27:00,270 --> 00:27:06,299
its users therefore we also need to

00:27:04,409 --> 00:27:08,700
actually understand the people who use

00:27:06,299 --> 00:27:13,950
our software and we need to get better

00:27:08,700 --> 00:27:16,649
at communicating with them step 4 in

00:27:13,950 --> 00:27:18,720
debugging the tech industry is every one

00:27:16,649 --> 00:27:23,279
of us need to foster change in our

00:27:18,720 --> 00:27:25,350
companies and communities this industry

00:27:23,279 --> 00:27:29,340
has an enormous problem with diversity

00:27:25,350 --> 00:27:31,200
and inclusion in Europe and Australia 70

00:27:29,340 --> 00:27:34,190
percent of people working in tech come

00:27:31,200 --> 00:27:37,679
in the numbers of women people of color

00:27:34,190 --> 00:27:39,690
lesbian gay bisexual intersexual trans

00:27:37,679 --> 00:27:44,190
and queer people or non-binary people in

00:27:39,690 --> 00:27:47,010
tech are incredibly low diversity could

00:27:44,190 --> 00:27:48,960
be our greatest strength software

00:27:47,010 --> 00:27:51,929
development is an act of representation

00:27:48,960 --> 00:27:55,289
and to adequately represent the users of

00:27:51,929 --> 00:27:58,950
our software deciphers user base we need

00:27:55,289 --> 00:28:01,140
diversity in our organizations but all

00:27:58,950 --> 00:28:04,950
of us are failing at this representation

00:28:01,140 --> 00:28:07,380
role right now we need diversity in our

00:28:04,950 --> 00:28:10,770
industry to show that we respect all our

00:28:07,380 --> 00:28:15,360
users their needs their values their

00:28:10,770 --> 00:28:17,100
safety and their humanity this is why

00:28:15,360 --> 00:28:19,080
every one of us need to support

00:28:17,100 --> 00:28:21,299
diversity in our companies

00:28:19,080 --> 00:28:23,070
we need to make sure that our companies

00:28:21,299 --> 00:28:24,830
hire people who are different from us

00:28:23,070 --> 00:28:28,110
different in genders ethnicities

00:28:24,830 --> 00:28:31,259
backgrounds skills upbringing and so

00:28:28,110 --> 00:28:34,070
many other factors we need to get

00:28:31,259 --> 00:28:37,830
diverse groups of people into tech and

00:28:34,070 --> 00:28:39,450
we need to make sure to include them we

00:28:37,830 --> 00:28:42,059
need to make sure that our workplaces

00:28:39,450 --> 00:28:45,200
are inclusive for diverse groups of

00:28:42,059 --> 00:28:48,590
people and that our communities are safe

00:28:45,200 --> 00:28:50,730
respecting spaces for everyone

00:28:48,590 --> 00:28:53,009
diversity and inclusion are our

00:28:50,730 --> 00:29:00,179
responsibilities towards everyone in

00:28:53,009 --> 00:29:02,070
tech and towards our users step wise and

00:29:00,179 --> 00:29:05,100
the last step for debugging the tech

00:29:02,070 --> 00:29:08,220
industry we need to support others who

00:29:05,100 --> 00:29:10,200
work on debugging this industry support

00:29:08,220 --> 00:29:12,139
activists and organizations that work on

00:29:10,200 --> 00:29:14,220
diversity and inclusion in tech

00:29:12,139 --> 00:29:15,779
organizations that care about mental

00:29:14,220 --> 00:29:18,239
health and social justice in this

00:29:15,779 --> 00:29:21,869
industry organizations that work on

00:29:18,239 --> 00:29:24,690
privacy tools and user advocacy we need

00:29:21,869 --> 00:29:28,609
to give our time knowledge technical

00:29:24,690 --> 00:29:31,889
skills and our money to support them and

00:29:28,609 --> 00:29:33,869
we need to become allies we need to

00:29:31,889 --> 00:29:36,230
stand with people and organizations

00:29:33,869 --> 00:29:38,970
fighting for change in this industry

00:29:36,230 --> 00:29:41,389
geyser alley chef comm is a fantastic

00:29:38,970 --> 00:29:44,549
open source resource by Emmeline a month

00:29:41,389 --> 00:29:46,379
about how to be an ally and I want to

00:29:44,549 --> 00:29:51,889
strongly encourage you all to read it

00:29:46,379 --> 00:29:55,379
and to become allies to summarize in

00:29:51,889 --> 00:29:57,109
order to debug the tech industry we need

00:29:55,379 --> 00:30:00,539
to understand ourselves in our role

00:29:57,109 --> 00:30:03,629
develop empathy care about how we work

00:30:00,539 --> 00:30:05,519
and the impact of this work we need to

00:30:03,629 --> 00:30:08,730
ask questions that challenge the status

00:30:05,519 --> 00:30:13,049
quo and we need to foster change in our

00:30:08,730 --> 00:30:15,299
companies and communities all of us need

00:30:13,049 --> 00:30:18,659
to do that and we need to do it right

00:30:15,299 --> 00:30:21,470
now because this is about the future of

00:30:18,659 --> 00:30:21,470
Technology

00:30:23,250 --> 00:30:29,560
this is a talk about debugging in the

00:30:26,560 --> 00:30:34,600
tech industry and debugging the tech

00:30:29,560 --> 00:30:36,580
industry is about you and me what we

00:30:34,600 --> 00:30:40,030
have now is a chance to build something

00:30:36,580 --> 00:30:41,950
bigger something that lasts what we have

00:30:40,030 --> 00:30:44,200
now is a chance to debug the tech

00:30:41,950 --> 00:30:48,340
industry and make it a better system

00:30:44,200 --> 00:30:49,750
that is about justice and equality we

00:30:48,340 --> 00:30:52,180
need to make this industry a better

00:30:49,750 --> 00:30:54,430
place for everyone so that we can build

00:30:52,180 --> 00:30:57,190
technology that is for everyone

00:30:54,430 --> 00:31:00,520
technologies that enables empowers and

00:30:57,190 --> 00:31:03,160
helps people technology that is

00:31:00,520 --> 00:31:05,020
meaningful for people and that solves

00:31:03,160 --> 00:31:08,470
many of the problems that humanity is

00:31:05,020 --> 00:31:10,240
facing right now we all need to stop

00:31:08,470 --> 00:31:13,890
being hiding behind our screens and

00:31:10,240 --> 00:31:16,930
start caring about what we do to people

00:31:13,890 --> 00:31:19,570
now is the time to debug the tech

00:31:16,930 --> 00:31:21,940
industry this is the biggest and most

00:31:19,570 --> 00:31:25,660
important task that all of us have as

00:31:21,940 --> 00:31:28,450
people in tech right now and the

00:31:25,660 --> 00:31:33,640
question is when we debug the tech

00:31:28,450 --> 00:31:39,459
industry will you join us thank you

00:31:33,640 --> 00:31:39,459

YouTube URL: https://www.youtube.com/watch?v=BHkY2zoJmPA


