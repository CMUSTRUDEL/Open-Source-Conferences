Title: Designing for User-Centered Privacy - by Venetia Tay - Mozilla Developer Roadshow - Singapore
Publication date: 2020-01-21
Playlist: Mozilla Developer Roadshow 2019 - Asia
Description: 
	Head of Audience Insights at Mozilla, Venetia Tay describes the research her team has done to arrive at their user-centric perspective on privacy in the browser.
Captions: 
	00:00:04,200 --> 00:00:05,200
Hi! everyone.

00:00:05,600 --> 00:00:06,700
Everyone still awake?

00:00:07,900 --> 00:00:09,700
Everyone got good pizza in your stomach?

00:00:10,750 --> 00:00:11,800
All good? Okay.

00:00:12,200 --> 00:00:12,400
Great.

00:00:12,400 --> 00:00:15,480
So, I'm really excited to be back in my hometown,

00:00:15,780 --> 00:00:22,800
Reunited with my teh c siu dai (milk tea less sugar), my chye tau kueh (carrot cake).

00:00:22,800 --> 00:00:25,200
And speaking at the Mozilla Developer Roadshow,

00:00:25,200 --> 00:00:28,000
I'm the last speaker, so I'm closing it out.

00:00:28,000 --> 00:00:30,300
So, get as much pizza as you can get

00:00:30,300 --> 00:00:31,800
even though I think they have all run out.

00:00:33,300 --> 00:00:36,560
So, I'm here to talk about designing for user centered privacy.

00:00:37,900 --> 00:00:40,200
I have been taught that

00:00:40,200 --> 00:00:43,200
it's easier to first understand human behavior

00:00:43,720 --> 00:00:47,000
and design something that's in line with their mental models,

00:00:47,350 --> 00:00:49,800
so that it's easier for users to adopt things.

00:00:50,250 --> 00:00:55,800
Trying to teach a new user to actually learn a new gesture, a new workflow

00:00:56,350 --> 00:00:59,500
and everyone tells you that you need to build things that's intuitive,

00:00:59,500 --> 00:01:01,000
that's easy for people.

00:01:01,500 --> 00:01:04,700
And then you try to convince users to change their viewpoint

00:01:04,700 --> 00:01:07,500
and change their fundamental human behavior

00:01:07,750 --> 00:01:09,900
honestly is a near impossible task.

00:01:10,400 --> 00:01:12,500
I've been grappling with this wicked problem

00:01:12,650 --> 00:01:15,000
and I'm going to try a different tactic today.

00:01:16,800 --> 00:01:18,800
So, hello, my name is Venetia Tay.

00:01:19,000 --> 00:01:21,500
I'm the head of audience insights at Mozilla.

00:01:21,500 --> 00:01:23,200
I'm based in San Francisco.

00:01:23,750 --> 00:01:27,350
I'm a mixed methods researcher and a strategist

00:01:27,350 --> 00:01:31,200
and my background is in business strategy and user research.

00:01:31,700 --> 00:01:35,880
My role at Mozilla requires me to understand our target audience

00:01:36,400 --> 00:01:39,380
and to actually translate their insights for teams

00:01:39,380 --> 00:01:41,000
to help them drive decisions

00:01:41,100 --> 00:01:42,700
from people within the marketing team

00:01:42,700 --> 00:01:44,600
all the way to developing products.

00:01:44,600 --> 00:01:48,120
I also sometimes work with the emerging technologies team as well,

00:01:48,320 --> 00:01:51,000
which is why I know some of them, not everyone

00:01:51,400 --> 00:01:54,360
and all of this is in service of Mozilla's mission

00:01:54,460 --> 00:01:57,800
which is to keep the web open and accessible to all.

00:01:58,600 --> 00:02:03,000
So, the wicked problem that I've been tackling for the last three years has been

00:02:03,300 --> 00:02:08,520
how do I get everyday people to care about their safety online

00:02:09,100 --> 00:02:12,200
and how do I get them to care about their own privacy

00:02:12,420 --> 00:02:14,120
without using fear tactics

00:02:14,120 --> 00:02:16,800
because that's actually really easy to scare the shit out of people.

00:02:18,200 --> 00:02:19,900
We're not in the business of doing that.

00:02:20,400 --> 00:02:24,120
So, today instead of talking to users and everyday people,

00:02:24,300 --> 00:02:25,820
I want to talk to all of you.

00:02:26,120 --> 00:02:28,220
You are makers of systems,

00:02:28,220 --> 00:02:31,600
designers of products and designers of interfaces,

00:02:31,850 --> 00:02:33,600
you are architects of algorithms.

00:02:34,100 --> 00:02:38,080
I want you all to be thinking about designing for user-centred privacy.

00:02:40,950 --> 00:02:44,100
So, the buzz word in the industry is user-centred design,

00:02:44,300 --> 00:02:46,100
customer centricity.

00:02:46,350 --> 00:02:49,840
If you believe in creating a customer -centred experience

00:02:49,840 --> 00:02:55,080
you have the responsibility of keeping your user's data safe.

00:02:57,380 --> 00:02:59,800
And why does data matter?

00:03:00,100 --> 00:03:04,400
Well, data is the new most invaluable resource out there right now.

00:03:04,500 --> 00:03:07,160
Even our Singapore government thinks that as well.

00:03:08,400 --> 00:03:10,680
There is a market for user data.

00:03:10,680 --> 00:03:12,780
A market research firm, Optimus

00:03:12,780 --> 00:03:16,920
estimated that we will reach about 250 million by 2020.

00:03:17,320 --> 00:03:18,220
Think about that.

00:03:18,220 --> 00:03:21,400
That's actually a rounding error for most of big tech firms out there.

00:03:21,600 --> 00:03:25,200
That's how much Google probably generates revenue in like 16 hours.

00:03:25,200 --> 00:03:26,000
It's not much.

00:03:26,550 --> 00:03:28,700
But if you think that data is the new currency

00:03:30,100 --> 00:03:33,500
how are you and how are your companies actually managing it right now?

00:03:35,850 --> 00:03:39,080
So, let's touch a little bit on this data privacy.

00:03:39,400 --> 00:03:41,080
Where do we stand right now?

00:03:42,880 --> 00:03:44,780
I do a lot of global research.

00:03:44,780 --> 00:03:47,280
I talk to users from all over the world.

00:03:47,980 --> 00:03:50,300
These are some of the common things that they say

00:03:50,300 --> 00:03:53,360
especially the people who are very unfamiliar with the privacy space.

00:03:55,760 --> 00:03:57,860
“I'm not a target. I'm so insignificant."

00:03:57,860 --> 00:03:59,860
"I'm so small, why would anyone care about me?”

00:04:02,100 --> 00:04:03,300
“I'm not a celebrity."

00:04:03,400 --> 00:04:05,500
"No. No one is interested in what I do.”

00:04:07,700 --> 00:04:09,200
“You know what? I've got nothing to hide."

00:04:09,200 --> 00:04:11,200
"So, even if everything I had leaked out,"

00:04:11,200 --> 00:04:12,800
"I really have nothing to hide at all.”

00:04:15,200 --> 00:04:17,200
And perhaps on the other end

00:04:17,450 --> 00:04:20,680
the people who are aware that companies are collecting their data,

00:04:21,000 --> 00:04:22,680
the haphazard use of data

00:04:22,680 --> 00:04:24,680
and the overall need to protect themselves.

00:04:25,150 --> 00:04:28,380
This is what I've heard their excuses that they say,

00:04:29,300 --> 00:04:31,100
“It's so much work,"

00:04:31,100 --> 00:04:34,600
"it's so convenient if everything connected, all my data connected.”

00:04:34,600 --> 00:04:36,500
“Privacy is just so much work,"

00:04:36,500 --> 00:04:38,500
"like why should I even bother doing that?”

00:04:38,900 --> 00:04:42,520
In fact, our Singapore government does actually hint at that as well.

00:04:44,000 --> 00:04:48,520
“If I worry about every little thing. I won't be able to live my life”

00:04:48,520 --> 00:04:50,620
like I got so many other things to worry about,

00:04:50,620 --> 00:04:52,220
climate change,

00:04:52,220 --> 00:04:54,100
rising taxes,

00:04:54,100 --> 00:04:56,300
like why would I care something about privacy?

00:04:56,300 --> 00:04:58,100
That's nothing to me.

00:04:59,100 --> 00:05:01,880
And the other thing that people say is “you know what?"

00:05:01,880 --> 00:05:04,680
"This is the way it works right now. We're already f*****."

00:05:04,880 --> 00:05:06,680
"What else can we do about it?”

00:05:06,680 --> 00:05:08,100
“Companies take our data all the time."

00:05:08,100 --> 00:05:11,100
"I can't say no because I need to use their stuff.”

00:05:13,400 --> 00:05:17,920
And in Asia in a very family-centred society

00:05:18,260 --> 00:05:21,680
that can make individual privacy very very difficult.

00:05:22,000 --> 00:05:23,980
We have a kampung mentality.

00:05:24,480 --> 00:05:27,040
It makes personal privacy very challenging.

00:05:27,440 --> 00:05:29,040
Like most people if you're not married,

00:05:29,040 --> 00:05:31,040
you're probably still living with your parents right now.

00:05:31,240 --> 00:05:33,040
That's the honest truth about Singapore

00:05:34,440 --> 00:05:37,850
and for example, the relationships between families and friends

00:05:38,100 --> 00:05:40,000
are very close in Indonesia

00:05:40,000 --> 00:05:42,680
which is why we do some of our research there.

00:05:42,880 --> 00:05:46,980
It's normal for people to share mobile phones between each other

00:05:46,980 --> 00:05:48,980
for a short period of time.

00:05:48,980 --> 00:05:50,450
So, again think about privacy

00:05:50,450 --> 00:05:52,200
where you think everything's private in your phone,

00:05:52,200 --> 00:05:53,550
and you share it with your best friend,

00:05:53,550 --> 00:05:56,600
your family, your mom, your dad, your sister, your brother.

00:05:57,850 --> 00:06:01,000
I remember that when I was growing up in Singapore

00:06:01,650 --> 00:06:03,960
we used to fill out lucky draw forms

00:06:04,150 --> 00:06:08,960
that we put down our address, our phone number, our IC number,

00:06:09,260 --> 00:06:12,280
we drop it into like a transparent box outside NTUC

00:06:12,580 --> 00:06:15,280
hoping to win maybe that cookery set,

00:06:15,380 --> 00:06:17,280
hoping to get a discount in something,

00:06:17,280 --> 00:06:21,040
we really give out all our information just for the promise of a discount

00:06:21,140 --> 00:06:22,540
or even a promise of a prize.

00:06:23,340 --> 00:06:26,600
In Singapore we know and we heard that

00:06:26,600 --> 00:06:30,450
just the chance of winning a lucky draw or discount

00:06:30,450 --> 00:06:33,450
is enough reason for many people to give up their data,

00:06:34,100 --> 00:06:35,950
give up all their personal information.

00:06:37,650 --> 00:06:39,600
So, now that we are in Singapore,

00:06:39,600 --> 00:06:41,200
so specifically Singapore,

00:06:43,200 --> 00:06:48,000
the government has outlined a SMART nation policy about five years ago.

00:06:48,400 --> 00:06:51,100
This includes nationwide digitization

00:06:51,100 --> 00:06:56,160
from ERP all the way to national electronic health records.

00:06:56,500 --> 00:07:01,000
The government wants to encourage the responsible sharing of personal data

00:07:01,000 --> 00:07:03,300
in order to generate value for our economy.

00:07:03,800 --> 00:07:07,700
Now, the government also talks about pulling all our personal data together

00:07:07,700 --> 00:07:09,650
so that multiple government agencies

00:07:09,650 --> 00:07:11,650
can provide a more hassle-free

00:07:11,650 --> 00:07:15,240
seamless online transaction experience for anyone.

00:07:15,440 --> 00:07:19,200
If all the agencies all shared their data together, easy for you,

00:07:19,400 --> 00:07:22,100
don't need to fill multiple forms up.

00:07:22,750 --> 00:07:27,320
In fact, our IMDA Chief Executive Officer Tan Kiat How

00:07:27,320 --> 00:07:28,900
has even detailed his vision

00:07:28,900 --> 00:07:32,320
about establishing the world's first global data exchange

00:07:32,720 --> 00:07:37,600
to then be able for more people to sell their data in a very regulated space.

00:07:40,100 --> 00:07:42,440
So, in recent years,

00:07:42,440 --> 00:07:43,940
actually the last two years

00:07:43,940 --> 00:07:46,940
you've seen a number of new laws and regulations

00:07:46,940 --> 00:07:48,940
that the Singapore government has come up with.

00:07:49,240 --> 00:07:52,250
I believe the new Personal Data Regulation Law,

00:07:52,250 --> 00:07:54,250
the one that you see in the graphic on the left,

00:07:54,650 --> 00:07:58,750
this will make the days where I fill out lucky draw forms

00:07:58,750 --> 00:08:01,650
and I put my NRIC number and I drop into a box like that

00:08:01,650 --> 00:08:03,050
no longer exists anymore.

00:08:03,150 --> 00:08:06,760
This law came into effect I believe September 2019, this year,

00:08:07,150 --> 00:08:09,760
the government started a campaign to educate users

00:08:09,760 --> 00:08:12,660
about safeguarding your NRIC number.

00:08:12,660 --> 00:08:15,000
So, you don't give your NRIC number away to anyone,

00:08:15,000 --> 00:08:16,500
you still keep it very close to you.

00:08:17,100 --> 00:08:20,240
The other interesting regulation that we have

00:08:20,240 --> 00:08:24,840
is also what sort of shorthand is the Anti-Fake News bill

00:08:24,840 --> 00:08:27,190
that actually bars dissemination online

00:08:27,190 --> 00:08:29,290
which deems to be against public interest.

00:08:29,290 --> 00:08:33,240
You could be fined up to $1 million or 10 years in jail

00:08:33,240 --> 00:08:36,560
for spreading misinformation even in your private networks,

00:08:36,560 --> 00:08:39,880
so like in WhatsApp or Signal - whatever you have been using.

00:08:40,480 --> 00:08:42,880
The key question is, if you ask yourself,

00:08:43,100 --> 00:08:45,600
how exactly would the government actually access

00:08:45,600 --> 00:08:49,200
all these private discussions on encrypted apps to police them?

00:08:49,200 --> 00:08:50,500
You've got to kind of think about that

00:08:50,800 --> 00:08:52,500
but given that we're in Singapore

00:08:53,500 --> 00:08:56,400
they'll find a way, they'll find a way.

00:08:58,200 --> 00:09:01,500
And we all know that Singapore is actually big on surveillance.

00:09:01,500 --> 00:09:03,750
I'm sure you all know there are multiple cameras everywhere,

00:09:03,750 --> 00:09:05,750
there are probably multiple cameras here right now

00:09:06,450 --> 00:09:07,600
but I didn't know

00:09:07,600 --> 00:09:09,600
I bet you didn't know that Singapore

00:09:09,600 --> 00:09:13,400
is actually the third most surveilled city outside of China.

00:09:13,750 --> 00:09:18,800
There's about 15.25 cameras for every 1000 people in Singapore.

00:09:21,000 --> 00:09:23,000
But I'm not here to kind of make a statement of

00:09:23,000 --> 00:09:25,500
where Singapore should be on the state of privacy,

00:09:25,500 --> 00:09:27,500
just want to kind of highlight that to you.

00:09:28,200 --> 00:09:30,400
So, another question I get from people a lot,

00:09:30,400 --> 00:09:32,400
especially in Singapore and especially in Asia,

00:09:33,500 --> 00:09:35,240
Why does privacy matter?

00:09:35,400 --> 00:09:36,840
In the case of surveillance,

00:09:37,240 --> 00:09:42,000
it means that we are a safer place because we keep tabs on terrorism.

00:09:42,200 --> 00:09:42,700
I know that.

00:09:42,700 --> 00:09:46,000
I grew up in a period of time where we were scared about the terrorist attacks

00:09:46,500 --> 00:09:50,100
and having people being surveilled kind of kept us safe,

00:09:52,100 --> 00:09:55,100
but I also could paint you a very dystopian future

00:09:55,100 --> 00:09:57,600
that could come straight out of Black Mirror episode

00:09:57,600 --> 00:09:59,900
which I'm sure you guys get here as well.

00:10:00,400 --> 00:10:06,240
Governments could actually use personal identifiable information

00:10:06,240 --> 00:10:08,940
to harm vulnerable communities

00:10:09,100 --> 00:10:10,940
and to keep tabs on the population.

00:10:11,340 --> 00:10:14,700
Filter bubbles, impact ideas and concepts

00:10:14,700 --> 00:10:16,160
that people actually are exposed to

00:10:16,160 --> 00:10:18,840
making people very very narrow minded,

00:10:18,840 --> 00:10:20,000
very insular.

00:10:20,250 --> 00:10:21,640
I mean look at the United States.

00:10:21,640 --> 00:10:24,800
That's a pretty obvious example of how filter bubbles can actually work

00:10:26,100 --> 00:10:32,640
and especially potentially very very close to our family-centred society, doxing.

00:10:32,900 --> 00:10:34,640
In some interviews that I've done,

00:10:34,940 --> 00:10:37,400
I heard about people losing their jobs,

00:10:37,800 --> 00:10:39,400
being threatened in real life

00:10:39,700 --> 00:10:43,000
because they were outed before they were ready to be outed.

00:10:43,400 --> 00:10:45,680
So, these are very real problems that could happen.

00:10:46,480 --> 00:10:49,960
But some of these concepts may seem foreign to us.

00:10:50,800 --> 00:10:52,760
And another way to look at it is that,

00:10:53,860 --> 00:10:57,440
the lack of privacy just means that I get more targeted ads,

00:10:57,640 --> 00:11:00,240
I search online, I was looking at a pair of shoes

00:11:00,440 --> 00:11:03,330
and for the next like, two to three weeks

00:11:03,530 --> 00:11:05,930
that same pair of shoes follows me around the Internet.

00:11:05,930 --> 00:11:07,030
I see it everywhere,

00:11:07,230 --> 00:11:09,960
sometimes even after I bought it as well.

00:11:10,260 --> 00:11:13,760
It's irritating but it's not dire.

00:11:13,760 --> 00:11:16,760
I won't die because of a pair of shoes following me around the Internet.

00:11:17,250 --> 00:11:18,560
So, let me paint a picture

00:11:18,560 --> 00:11:22,060
that I think is very very relevant to the Singapore audience right now.

00:11:23,400 --> 00:11:24,060
The first one.

00:11:24,060 --> 00:11:25,360
In our work in India,

00:11:26,100 --> 00:11:29,880
almost every participant we interviewed there had a VPN on their phone.

00:11:30,500 --> 00:11:35,640
They used VPN to access websites blocked by the government

00:11:35,940 --> 00:11:38,240
like Korean dramas.

00:11:38,640 --> 00:11:40,840
So, they used a VPN to watch the Korean dramas

00:11:41,240 --> 00:11:44,180
and they said that another reason why they used a VPN

00:11:44,180 --> 00:11:48,000
is to escape music suggestion from Bollywood only music.

00:11:48,300 --> 00:11:50,000
You are in India, you don’t have a VPN,

00:11:50,150 --> 00:11:53,760
all you get in your streaming services are Bollywood music

00:11:54,360 --> 00:11:55,460
and they felt that,

00:11:55,460 --> 00:11:59,120
our users in India felt that it's their freedom

00:11:59,120 --> 00:12:01,960
to actually receive any content that they want

00:12:02,260 --> 00:12:04,660
and they want to be able to explore beyond that.

00:12:06,250 --> 00:12:09,140
Another thing that I think will really hit the hearts of our Singaporeans

00:12:09,140 --> 00:12:11,800
is that air tickets, air flights.

00:12:12,400 --> 00:12:15,500
Have you noticed sometimes if you don't use incognito mode

00:12:15,500 --> 00:12:16,800
or you don't use Firefox,

00:12:17,400 --> 00:12:20,680
the more you check your prices for your airline tickets

00:12:20,680 --> 00:12:23,400
the more it kind of increases over time,

00:12:23,900 --> 00:12:25,900
the higher and higher it seems to get.

00:12:26,500 --> 00:12:29,800
So, there's also research papers out there that says that,

00:12:29,800 --> 00:12:34,680
if you're using a Mac you could potentially see higher ticket prices

00:12:35,150 --> 00:12:37,700
than if you're using Windows.

00:12:37,700 --> 00:12:40,800
The assumption is that because you have a Mac you have more money.

00:12:40,800 --> 00:12:43,000
So, if you have more money you can pay for higher prices.

00:12:43,950 --> 00:12:46,680
Another story that will strike fear in people's hearts is

00:12:47,150 --> 00:12:49,150
insurance premiums might go up.

00:12:49,750 --> 00:12:55,480
So, for example, if you were Googling what does this mole actually mean,

00:12:56,150 --> 00:12:59,700
if your insurance company actually had access to that,

00:12:59,700 --> 00:13:01,700
they could think that mole could be cancerous.

00:13:01,700 --> 00:13:04,800
I'm going to start raising that person's insurance premiums

00:13:06,000 --> 00:13:08,550
and they might charge you higher premiums as a result of that,

00:13:08,550 --> 00:13:10,150
just for something that you searched.

00:13:10,150 --> 00:13:12,350
So, anything weird that you might have searched

00:13:12,350 --> 00:13:14,350
they could use it against you.

00:13:16,500 --> 00:13:17,500
And lastly,

00:13:17,500 --> 00:13:19,900
I know I see out here mostly men,

00:13:19,900 --> 00:13:23,600
so you may not really engage with this example

00:13:23,600 --> 00:13:25,300
but period trackers,

00:13:25,300 --> 00:13:28,280
trackers that actually track women's menstrual apps,

00:13:28,580 --> 00:13:31,840
women actually often track when it happens,

00:13:32,100 --> 00:13:34,680
what they were doing, how they were feeling at that point in time.

00:13:35,380 --> 00:13:38,360
This is very sensitive health information data.

00:13:39,200 --> 00:13:42,300
They could be leaked, could even be sold to your employer.

00:13:42,300 --> 00:13:45,160
So, can you imagine if you're trying to get pregnant

00:13:45,600 --> 00:13:47,900
your company could actually fire you in advance

00:13:47,900 --> 00:13:49,900
because they don't want to pay for your maternity leave.

00:13:50,550 --> 00:13:52,700
And these are really real problems.

00:13:52,700 --> 00:13:54,350
All the examples I listed out here

00:13:54,350 --> 00:13:56,950
are things that actually do happen around the world

00:13:56,950 --> 00:13:58,250
and not only in Singapore,

00:13:58,450 --> 00:13:59,250
this is what happens.

00:14:01,250 --> 00:14:02,450
So, what could you do?

00:14:02,650 --> 00:14:04,450
I did mention at the start of my talk

00:14:04,450 --> 00:14:07,360
that I'm here to have you guys come on board.

00:14:08,560 --> 00:14:11,920
At Mozilla we recommend a lean data policy.

00:14:13,100 --> 00:14:14,000
What does that mean?

00:14:15,100 --> 00:14:17,700
We all know that data is the new currency.

00:14:18,000 --> 00:14:20,560
Most people know that companies are collecting their data.

00:14:21,000 --> 00:14:22,860
Most people don't know when,

00:14:22,860 --> 00:14:24,060
they don't know what,

00:14:24,560 --> 00:14:26,060
they don't know why,

00:14:26,060 --> 00:14:28,060
and they don't know what it's used for.

00:14:30,000 --> 00:14:33,850
So, the truth is some data collection is necessary

00:14:33,850 --> 00:14:37,000
to make a smooth seamless experience for your user,

00:14:37,700 --> 00:14:40,500
but make sure that every piece of data that you collect

00:14:40,500 --> 00:14:42,800
provides direct user benefit.

00:14:45,100 --> 00:14:46,700
As I mentioned, data is the new currency,

00:14:46,700 --> 00:14:48,700
so as a developer

00:14:48,700 --> 00:14:53,000
I want you to think about when, what, why,

00:14:53,000 --> 00:14:55,000
and what could it be used for.

00:14:55,300 --> 00:14:56,100
Think about it.

00:14:56,100 --> 00:14:58,100
Like can your ‘work’ still function

00:14:58,100 --> 00:15:02,240
if a user refrains from providing you with that one piece of information.

00:15:02,540 --> 00:15:04,640
If you can, you don't need it.

00:15:07,740 --> 00:15:10,300
And if you are going to collect data,

00:15:11,150 --> 00:15:13,880
we encourage you to collect data in the open,

00:15:13,880 --> 00:15:16,880
operate in good faith and be open by default.

00:15:17,380 --> 00:15:20,920
At Mozilla we talk about no surprises.

00:15:20,920 --> 00:15:22,920
Be transparent.

00:15:23,100 --> 00:15:25,100
Disclose the benefits to the user.

00:15:25,550 --> 00:15:27,100
We talk about user control.

00:15:27,100 --> 00:15:29,700
So, the user is in control of their data

00:15:29,900 --> 00:15:31,400
and their online experience.

00:15:32,100 --> 00:15:34,110
We talk about limited data collection.

00:15:34,110 --> 00:15:35,510
Only collect what you need.

00:15:35,910 --> 00:15:37,910
De-identify whenever you can.

00:15:38,210 --> 00:15:40,880
And after you're done with it, delete everything that you have.

00:15:44,300 --> 00:15:47,300
And we live this policy within Mozilla.

00:15:47,300 --> 00:15:49,250
So, you can actually go to our Wiki

00:15:49,250 --> 00:15:52,760
and check out our Firefox data collection policy

00:15:52,960 --> 00:15:54,760
it's something we really believe in as well.

00:15:57,460 --> 00:15:58,760
So, quick quiz.

00:15:58,760 --> 00:16:00,760
This is a popular Singapore website.

00:16:00,760 --> 00:16:03,500
I've kind of shaded out the name of the website

00:16:03,500 --> 00:16:06,700
just not to put them in a bad light or on the tough spot.

00:16:07,350 --> 00:16:10,920
So, can anyone tell me how this experience could be improved?

00:16:17,600 --> 00:16:18,600
It's hard to see it?

00:16:18,600 --> 00:16:20,600
Oh! Should make it bigger.

00:16:21,000 --> 00:16:22,230
So, I'll give you.

00:16:22,230 --> 00:16:25,000
So, in providing convenience for the customer,

00:16:25,700 --> 00:16:31,400
they allow the customer to either log in by Google or log in with Facebook.

00:16:31,700 --> 00:16:35,700
And because of that you actually end up asking more data than you really need

00:16:35,700 --> 00:16:38,000
because you're actually giving data to the third party

00:16:38,000 --> 00:16:39,800
like Google and Facebook as well,

00:16:40,200 --> 00:16:41,700
in order to improve the experience

00:16:41,700 --> 00:16:44,560
they should actually have an option for you to check out as a guest

00:16:45,200 --> 00:16:50,800
and you don't have to then kind of link all your shopping on this website

00:16:50,800 --> 00:16:53,600
to the backend of another third party website.

00:16:53,850 --> 00:16:57,200
So, this is one thing you could do to improve apart from just the UX.

00:16:58,900 --> 00:17:00,640
So, key takeaways.

00:17:02,900 --> 00:17:06,160
Think about framework for privacy standards in design

00:17:06,160 --> 00:17:07,850
and all of you here are designers

00:17:07,850 --> 00:17:10,500
even though you may not have the word ‘design’ in your title,

00:17:10,500 --> 00:17:12,500
developers are designers too.

00:17:13,100 --> 00:17:14,000
Number One

00:17:14,300 --> 00:17:16,300
Adopt a lean data policy.

00:17:16,800 --> 00:17:20,000
Map out what you need and why you need it.

00:17:20,800 --> 00:17:21,700
Number Two

00:17:21,950 --> 00:17:23,900
Communicate the effects and impacts.

00:17:24,400 --> 00:17:27,120
You know, demonstrate the benefits and the risks

00:17:27,120 --> 00:17:29,900
both internally to your internal stakeholders

00:17:29,900 --> 00:17:31,200
as well as to consumers.

00:17:32,700 --> 00:17:33,800
Number Three

00:17:34,100 --> 00:17:35,800
Build a trusted relationship.

00:17:36,000 --> 00:17:37,800
Ask for active consent,

00:17:38,100 --> 00:17:40,600
provide straightforward privacy settings

00:17:41,000 --> 00:17:42,450
and easy opt-outs.

00:17:42,450 --> 00:17:46,450
Don't hide the opt-out section somewhere in the middle or at the bottom.

00:17:46,650 --> 00:17:49,840
Make sure it's obvious for the users and give the users control.

00:17:50,100 --> 00:17:51,000
And lastly

00:17:51,500 --> 00:17:52,800
Respect the individual.

00:17:52,800 --> 00:17:59,080
When possible share out and report what you use and why you used it.

00:18:00,880 --> 00:18:04,280
So, before you request for a user’s data

00:18:04,480 --> 00:18:06,680
ask yourself these four things.

00:18:06,680 --> 00:18:07,680
Number One

00:18:07,980 --> 00:18:09,600
Would you ask for that piece of data

00:18:09,600 --> 00:18:11,900
if that person was sitting next to you in real life?

00:18:12,800 --> 00:18:13,700
Chances are I would say

00:18:13,700 --> 00:18:16,700
about 50% of people will actually scale back on that.

00:18:17,650 --> 00:18:18,500
Number Two

00:18:18,950 --> 00:18:22,240
Have I actively informed the user about what I'm collecting

00:18:22,240 --> 00:18:23,540
and why I'm collecting it?

00:18:25,940 --> 00:18:26,940
Number Three

00:18:27,340 --> 00:18:31,400
Can I map out what I'm using the data for

00:18:31,400 --> 00:18:34,480
and why I am collecting the piece of data?

00:18:35,100 --> 00:18:37,550
And lastly, this is the one that kind of trumps all.

00:18:37,550 --> 00:18:38,550
The last question.

00:18:39,350 --> 00:18:40,550
Is it creepy?

00:18:41,550 --> 00:18:43,800
If you feel it's creepy don't do it.

00:18:49,500 --> 00:18:54,000
So, in short, you have the responsibility to keep your data users safe.

00:18:54,000 --> 00:18:55,080
So, even if you don't want

00:18:55,080 --> 00:18:58,700
even if you yourself aren't interested in data privacy

00:18:58,700 --> 00:19:00,120
for your own personal benefit

00:19:00,120 --> 00:19:02,420
and your own personal individual benefit

00:19:02,700 --> 00:19:05,960
think about your users and help keep them safe.

00:19:07,100 --> 00:19:07,960
Thank you.

00:19:08,560 --> 00:19:11,260

YouTube URL: https://www.youtube.com/watch?v=F1ljByIHj3Q


