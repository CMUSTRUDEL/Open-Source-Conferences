Title: Corin Dwyer   Tomas Lin "Evolution of a Cloud Provider: Titus"
Publication date: 2018-11-03
Playlist: Spinnaker Summit 2018
Description: 
	
Captions: 
	00:00:01,190 --> 00:00:14,400
[Music]

00:00:16,640 --> 00:00:22,439
hey all thanks for coming and welcome to

00:00:19,230 --> 00:00:24,960
an evolution of a cloud provider Titus

00:00:22,439 --> 00:00:26,789
I'm Koren Dwyer I'm an engineer on the

00:00:24,960 --> 00:00:30,480
Titus team and I focused primarily on

00:00:26,789 --> 00:00:33,480
the control plane and the scheduler I'm

00:00:30,480 --> 00:00:36,329
Tomas I do everything he doesn't do I've

00:00:33,480 --> 00:00:38,250
been working on the the Tennis Club

00:00:36,329 --> 00:00:40,770
provider for now the last two three

00:00:38,250 --> 00:00:45,300
years of my life so it's this isn't this

00:00:40,770 --> 00:00:46,920
is what I do all the time now okay let's

00:00:45,300 --> 00:00:48,989
get started so today we're gonna walk

00:00:46,920 --> 00:00:50,760
you through first and foremost what is

00:00:48,989 --> 00:00:53,850
Titus and how we've gotten to where we

00:00:50,760 --> 00:00:56,010
are today Tomas is then gonna walk you

00:00:53,850 --> 00:00:57,750
through Hal spinnaker has integrated

00:00:56,010 --> 00:00:59,730
with Titus in order to make it more

00:00:57,750 --> 00:01:01,320
accessible to the end user and then

00:00:59,730 --> 00:01:06,750
we're gonna finish it up with just open

00:01:01,320 --> 00:01:10,560
Q&A okay so what is Titus other than

00:01:06,750 --> 00:01:11,909
this cool little red robot guy which by

00:01:10,560 --> 00:01:13,740
the way we have stickers if you want

00:01:11,909 --> 00:01:16,320
some stickers later feel free to come up

00:01:13,740 --> 00:01:18,810
and grab them so Titus is Netflix's

00:01:16,320 --> 00:01:21,330
container management platform with a

00:01:18,810 --> 00:01:24,299
unified scheduler for both service and

00:01:21,330 --> 00:01:26,549
batch jobs jobs meaning more of the

00:01:24,299 --> 00:01:29,670
description or the definition of a

00:01:26,549 --> 00:01:34,920
workload task meaning the container or

00:01:29,670 --> 00:01:38,280
the actual execution of that process so

00:01:34,920 --> 00:01:41,340
our containers today have to be able to

00:01:38,280 --> 00:01:43,740
support both AWS as well as the Netflix

00:01:41,340 --> 00:01:48,600
ecosystem in order for you to run your

00:01:43,740 --> 00:01:50,790
workload within Netflix so why is

00:01:48,600 --> 00:01:54,060
Netflix interested in containers just

00:01:50,790 --> 00:01:55,829
like the rest of the industry I think it

00:01:54,060 --> 00:01:57,719
really comes down to three things our

00:01:55,829 --> 00:02:00,180
users wanted simpler management of

00:01:57,719 --> 00:02:02,280
compute resources such that let's say

00:02:00,180 --> 00:02:04,649
I'm a work or I'm the workload owner and

00:02:02,280 --> 00:02:07,230
I want to launch a container with one

00:02:04,649 --> 00:02:09,900
gig of ram and

00:02:07,230 --> 00:02:11,790
no 20 CPUs you can't really do that very

00:02:09,900 --> 00:02:14,790
easily on most cloud providers so this

00:02:11,790 --> 00:02:18,020
really gives the user more control in

00:02:14,790 --> 00:02:20,790
what their application actually needs

00:02:18,020 --> 00:02:22,440
users also wanted a simpler deployment

00:02:20,790 --> 00:02:25,320
packaging model such that they could

00:02:22,440 --> 00:02:27,990
build an artifact run it locally run it

00:02:25,320 --> 00:02:31,680
in their CI system and eventually run it

00:02:27,990 --> 00:02:33,270
in production on the cloud and users

00:02:31,680 --> 00:02:35,460
wanted a consistent local development

00:02:33,270 --> 00:02:38,400
experience and what do I mean here is

00:02:35,460 --> 00:02:40,860
more on today let's say I'm a Java

00:02:38,400 --> 00:02:42,900
developer at Netflix and I'm building a

00:02:40,860 --> 00:02:46,440
typical web service that powers

00:02:42,900 --> 00:02:49,350
something I would most likely build my

00:02:46,440 --> 00:02:51,000
Java code within an IDE such as IntelliJ

00:02:49,350 --> 00:02:52,770
and just run it from the IDE and that's

00:02:51,000 --> 00:02:55,020
how I would do most of my development

00:02:52,770 --> 00:02:56,670
loop and then when it's time for me to

00:02:55,020 --> 00:02:59,430
run it in production I'd probably build

00:02:56,670 --> 00:03:03,150
this into a war bake it into an AMI and

00:02:59,430 --> 00:03:05,370
then put it in a in a folder such that

00:03:03,150 --> 00:03:07,710
Tomcat can read the war and run the

00:03:05,370 --> 00:03:09,180
application but on that ami there's

00:03:07,710 --> 00:03:11,010
actually tons of other things going on

00:03:09,180 --> 00:03:13,080
there might be an Apache reverse proxy

00:03:11,010 --> 00:03:15,450
server there might be some kind of

00:03:13,080 --> 00:03:17,459
process that's managing configurations

00:03:15,450 --> 00:03:18,930
and you wouldn't actually know whether

00:03:17,459 --> 00:03:20,370
or not your Apache or anything other

00:03:18,930 --> 00:03:22,890
things were configured until you were in

00:03:20,370 --> 00:03:24,480
the cloud so a container here gives you

00:03:22,890 --> 00:03:26,760
the ability to actually run all of these

00:03:24,480 --> 00:03:28,739
things bundles together locally and be

00:03:26,760 --> 00:03:30,390
able to guess iterate faster on those

00:03:28,739 --> 00:03:33,260
things that maybe are not your java

00:03:30,390 --> 00:03:33,260
application

00:03:34,590 --> 00:03:39,390
so we've been doing containers for a

00:03:37,110 --> 00:03:41,069
long time or maybe not that long but

00:03:39,390 --> 00:03:43,170
decent amount of time

00:03:41,069 --> 00:03:46,290
Titus started out as a batch scheduling

00:03:43,170 --> 00:03:51,420
basic batch scheduling system back in

00:03:46,290 --> 00:03:54,030
late 2015 and in 2016 we started adding

00:03:51,420 --> 00:03:58,519
basic support for services so nothing

00:03:54,030 --> 00:04:01,200
critical in late 2016 we started adding

00:03:58,519 --> 00:04:03,269
still not completely critical but more

00:04:01,200 --> 00:04:04,890
important services such as like stream

00:04:03,269 --> 00:04:07,350
processing jobs things that power

00:04:04,890 --> 00:04:09,090
analytics stuff like that to where it

00:04:07,350 --> 00:04:12,500
was important to the business but it

00:04:09,090 --> 00:04:15,870
wasn't necessarily impacting customers

00:04:12,500 --> 00:04:18,840
at the same time we started testing in

00:04:15,870 --> 00:04:21,900
user impacting services through the use

00:04:18,840 --> 00:04:25,860
of shadow traffic these were no js'

00:04:21,900 --> 00:04:28,919
processes and more or less at the at the

00:04:25,860 --> 00:04:30,840
edge we would proxy request to the

00:04:28,919 --> 00:04:34,260
correct API server as well as this new

00:04:30,840 --> 00:04:36,510
no js' process server and we did that

00:04:34,260 --> 00:04:39,660
for a couple of quarters and then and

00:04:36,510 --> 00:04:42,240
the second quarter of 2017 we actually

00:04:39,660 --> 00:04:44,220
turned it on live and today most of the

00:04:42,240 --> 00:04:49,860
device based requests actually go

00:04:44,220 --> 00:04:52,650
through a tightest container so just to

00:04:49,860 --> 00:04:55,800
give you a sense of our current scale we

00:04:52,650 --> 00:04:57,539
run about 240,000 jobs a day there's

00:04:55,800 --> 00:05:00,300
over a thousand different types of

00:04:57,539 --> 00:05:03,389
applications on Titus we have seven

00:05:00,300 --> 00:05:06,360
regionally isolated tightest deployments

00:05:03,389 --> 00:05:08,970
for services the largest single

00:05:06,360 --> 00:05:11,850
applications cluster size that I've seen

00:05:08,970 --> 00:05:14,039
has been about 5,000 containers and

00:05:11,850 --> 00:05:17,310
we've tested it up to about maybe 12,000

00:05:14,039 --> 00:05:19,830
in a synthetic benchmark um these

00:05:17,310 --> 00:05:23,490
containers are running on over 6,000

00:05:19,830 --> 00:05:26,340
managed VMs by the titus team each VM is

00:05:23,490 --> 00:05:28,880
a 64 CPU instance type so it's about

00:05:26,340 --> 00:05:32,039
three hundred eighty four thousand CPUs

00:05:28,880 --> 00:05:34,500
for batch we run about 450,000

00:05:32,039 --> 00:05:38,010
containers a day and that typically runs

00:05:34,500 --> 00:05:40,500
on anywhere between 600 and 1,200 VMs

00:05:38,010 --> 00:05:43,370
that auto scale throughout the day based

00:05:40,500 --> 00:05:43,370
on the demand

00:05:44,330 --> 00:05:50,699
so the tightest product strategy has

00:05:46,830 --> 00:05:52,680
really focused on three tenets the first

00:05:50,699 --> 00:05:55,650
one is the most important is developer

00:05:52,680 --> 00:05:57,150
velocity the second being reliability of

00:05:55,650 --> 00:05:59,490
the overall platform such that you can

00:05:57,150 --> 00:06:03,419
run a container at any point and lastly

00:05:59,490 --> 00:06:05,400
is cost efficiency we also wanted to

00:06:03,419 --> 00:06:07,470
make it very easy for our users to take

00:06:05,400 --> 00:06:09,479
an existing workload running in an ec2

00:06:07,470 --> 00:06:13,620
instance and be able to convert that

00:06:09,479 --> 00:06:15,599
into a container and lastly we just

00:06:13,620 --> 00:06:18,960
wanted to really focus on what Netflix

00:06:15,599 --> 00:06:24,060
needs and not necessarily the needs of

00:06:18,960 --> 00:06:26,430
everyone else so to give you a sense of

00:06:24,060 --> 00:06:28,560
how titus is architected or built I

00:06:26,430 --> 00:06:30,930
guess I'm not gonna go through all of

00:06:28,560 --> 00:06:33,539
this but I wanted to walk you through a

00:06:30,930 --> 00:06:35,520
typical job submission request so let's

00:06:33,539 --> 00:06:37,710
say I'm a user and I want to go through

00:06:35,520 --> 00:06:41,759
spinnaker and I want to launch my web

00:06:37,710 --> 00:06:43,560
service on Titus I probably go to

00:06:41,759 --> 00:06:46,650
spinnaker I would create a server group

00:06:43,560 --> 00:06:50,099
spinnaker who then issue a job submit

00:06:46,650 --> 00:06:52,229
API call to Titus in which case we would

00:06:50,099 --> 00:06:54,300
Titus would receive that job definition

00:06:52,229 --> 00:06:57,360
there would be a desired in that job

00:06:54,300 --> 00:07:00,389
definition Titus would then create tasks

00:06:57,360 --> 00:07:02,550
based on that desired it with in place

00:07:00,389 --> 00:07:05,849
or scheduled those tasks to hey Titus

00:07:02,550 --> 00:07:08,130
agents and store this information in

00:07:05,849 --> 00:07:10,620
Cassandra and then send these requests

00:07:08,130 --> 00:07:12,870
through maysa to the Titus agents and

00:07:10,620 --> 00:07:14,400
then the Titus executor on the agent

00:07:12,870 --> 00:07:17,629
would then be responsible for actually

00:07:14,400 --> 00:07:17,629
running the container

00:07:19,950 --> 00:07:24,300
like I mentioned earlier we really

00:07:22,380 --> 00:07:26,610
wanted to focus on making it easy for

00:07:24,300 --> 00:07:29,760
our users to be able to take an existing

00:07:26,610 --> 00:07:33,930
workload on ec2 and move it into a

00:07:29,760 --> 00:07:36,540
container on Titus and one of the big

00:07:33,930 --> 00:07:38,640
challenges there is there's a lot of

00:07:36,540 --> 00:07:40,590
Netflix sirs that are used to operating

00:07:38,640 --> 00:07:42,870
and running their services in a very

00:07:40,590 --> 00:07:44,160
particular way and we wanted to make

00:07:42,870 --> 00:07:47,490
sure that this was very seamless

00:07:44,160 --> 00:07:50,160
experience so we did a lot of work for

00:07:47,490 --> 00:07:52,770
making sure Alice metrics appeared very

00:07:50,160 --> 00:07:56,040
similar service discovery works the

00:07:52,770 --> 00:07:57,750
exact same way this information appears

00:07:56,040 --> 00:07:59,640
in Etta which is used by a lot of

00:07:57,750 --> 00:08:00,900
systems to I guess discover that the

00:07:59,640 --> 00:08:03,720
application is running in the first

00:08:00,900 --> 00:08:06,660
place so we wanted to make sure that

00:08:03,720 --> 00:08:08,910
again a user after moving it wouldn't

00:08:06,660 --> 00:08:10,770
have to relearn all their operation and

00:08:08,910 --> 00:08:15,180
their years of just being a service

00:08:10,770 --> 00:08:17,370
owner in general we also wanted to make

00:08:15,180 --> 00:08:20,430
sure that users didn't have to relearn

00:08:17,370 --> 00:08:23,580
the entire cloud concepts that they they

00:08:20,430 --> 00:08:26,220
knew from AWS so for every container

00:08:23,580 --> 00:08:30,060
launched in Titus that container gets

00:08:26,220 --> 00:08:33,240
its own IP from a VPC subnet in its own

00:08:30,060 --> 00:08:35,940
set of security groups it gets its own

00:08:33,240 --> 00:08:38,010
iam role and there is a embedded

00:08:35,940 --> 00:08:41,270
metadata proxy within the container such

00:08:38,010 --> 00:08:44,520
that when you launch your container and

00:08:41,270 --> 00:08:48,420
let's say you have a traditional AWS SDK

00:08:44,520 --> 00:08:51,480
it's going to make a call to 169 to 54

00:08:48,420 --> 00:08:52,890
169 to 54 it's going to get that I am

00:08:51,480 --> 00:08:54,270
role that you launched your container

00:08:52,890 --> 00:08:58,260
with and all of your existing

00:08:54,270 --> 00:09:00,540
integrations will work as is so and what

00:08:58,260 --> 00:09:03,330
this really means is if I had an ec2

00:09:00,540 --> 00:09:05,970
instance I could bring my exact I am

00:09:03,330 --> 00:09:07,650
roles and I can bring my exact security

00:09:05,970 --> 00:09:09,180
groups with me without changing anything

00:09:07,650 --> 00:09:11,100
else the only thing that you would be

00:09:09,180 --> 00:09:13,590
responsible for is figuring out how do I

00:09:11,100 --> 00:09:15,720
go from an ami to a docker container

00:09:13,590 --> 00:09:17,580
which for the most of the company that's

00:09:15,720 --> 00:09:22,560
it's pretty easy because the deployment

00:09:17,580 --> 00:09:24,540
artifact was at war a Java war so

00:09:22,560 --> 00:09:27,900
another thing that I guess you could

00:09:24,540 --> 00:09:30,270
just uplift would be the auto scaling

00:09:27,900 --> 00:09:32,820
policies so we have integrated with any

00:09:30,270 --> 00:09:33,600
scale the AWS products such that you can

00:09:32,820 --> 00:09:35,970
take your exist

00:09:33,600 --> 00:09:39,269
auto-scaling policy and just move it

00:09:35,970 --> 00:09:40,500
over and it will make the I guess ec2

00:09:39,269 --> 00:09:42,990
not used to ASG

00:09:40,500 --> 00:09:45,829
service we'll be making those calls into

00:09:42,990 --> 00:09:49,829
Titus to autoscale the application and

00:09:45,829 --> 00:09:52,170
finally we also support using the AWS

00:09:49,829 --> 00:09:54,149
alb or application load balancing

00:09:52,170 --> 00:09:59,339
product to do a server-side based load

00:09:54,149 --> 00:10:01,019
balancer Titus today supports two

00:09:59,339 --> 00:10:03,750
different types of jobs I mentioned them

00:10:01,019 --> 00:10:06,350
earlier the first one is a batch job or

00:10:03,750 --> 00:10:10,230
a run to completion workload

00:10:06,350 --> 00:10:12,480
my favorite example workload on titus

00:10:10,230 --> 00:10:14,670
today for this is the image ranking

00:10:12,480 --> 00:10:18,060
process so if you go to netflix.com

00:10:14,670 --> 00:10:19,649
there's actually tons of artwork all

00:10:18,060 --> 00:10:20,399
over the place I'm sure you guys have

00:10:19,649 --> 00:10:23,069
seen it before

00:10:20,399 --> 00:10:27,000
a lot of that artwork is not chosen by a

00:10:23,069 --> 00:10:28,740
human anymore it's actually what happens

00:10:27,000 --> 00:10:31,550
is you have a video source file and

00:10:28,740 --> 00:10:34,410
these still images are extracted and

00:10:31,550 --> 00:10:36,899
several computer vision processes are

00:10:34,410 --> 00:10:39,389
run over these still images and then

00:10:36,899 --> 00:10:41,759
this this process the image ranker more

00:10:39,389 --> 00:10:44,610
or less runs over this last pass and it

00:10:41,759 --> 00:10:46,019
determines what is the best image so as

00:10:44,610 --> 00:10:49,230
you can see here in the picture there's

00:10:46,019 --> 00:10:51,439
lots of faces and I think the most

00:10:49,230 --> 00:10:53,639
important one is typically blurriness

00:10:51,439 --> 00:10:58,500
obviously no one wants to look at a very

00:10:53,639 --> 00:11:02,459
blurry image so more or less is choosing

00:10:58,500 --> 00:11:05,069
the best one algorithmically the second

00:11:02,459 --> 00:11:08,819
type of job is the service job and this

00:11:05,069 --> 00:11:12,810
is a run determination workload more or

00:11:08,819 --> 00:11:15,540
less traditional web servers this job is

00:11:12,810 --> 00:11:17,519
very similar to the AWS auto scaling

00:11:15,540 --> 00:11:20,189
group concept where you have a min/max

00:11:17,519 --> 00:11:22,709
desired and the system is always trying

00:11:20,189 --> 00:11:25,079
to make sure the current number of

00:11:22,709 --> 00:11:28,380
containers in the system is equal to the

00:11:25,079 --> 00:11:31,439
desired an example of this type of

00:11:28,380 --> 00:11:35,490
workload is Apache flink our stream

00:11:31,439 --> 00:11:39,060
processing team runs Apache flink to

00:11:35,490 --> 00:11:41,699
where they have an integration say they

00:11:39,060 --> 00:11:43,470
pull from a source such as Kafka they

00:11:41,699 --> 00:11:46,000
transform the data and then they write

00:11:43,470 --> 00:11:50,529
it to a scene such as elasticsearch or

00:11:46,000 --> 00:11:52,420
s3 all right and with that Tomas is

00:11:50,529 --> 00:11:55,360
gonna talk about he'll spinnaker is

00:11:52,420 --> 00:11:57,459
really leveraging this concepts in order

00:11:55,360 --> 00:12:03,519
to make it easier for users to directly

00:11:57,459 --> 00:12:05,230
use Titus just as a show of hands how

00:12:03,519 --> 00:12:06,519
many of you here are actually building a

00:12:05,230 --> 00:12:08,649
cloud provider right now like how many

00:12:06,519 --> 00:12:10,720
of you're working on maintaining a cloud

00:12:08,649 --> 00:12:12,490
provider okay

00:12:10,720 --> 00:12:16,420
so you should be familiar with this

00:12:12,490 --> 00:12:17,980
process then so when you create a cloud

00:12:16,420 --> 00:12:19,990
provider there's a series of steps that

00:12:17,980 --> 00:12:21,459
you go through right if you're out how

00:12:19,990 --> 00:12:23,079
do I get the credentials if you're out

00:12:21,459 --> 00:12:25,389
what things do I need to be configurable

00:12:23,079 --> 00:12:27,129
you write a few caching agents that go

00:12:25,389 --> 00:12:29,019
through and index all the things that

00:12:27,129 --> 00:12:31,089
you want to remember

00:12:29,019 --> 00:12:34,209
you then have to implement all the

00:12:31,089 --> 00:12:36,750
operations so that everything that you

00:12:34,209 --> 00:12:38,980
tell your cloud provider to do happens

00:12:36,750 --> 00:12:41,350
you have to figure out what are the

00:12:38,980 --> 00:12:42,430
triggers that I'm gonna use so what are

00:12:41,350 --> 00:12:45,040
the things that are gonna kick up my

00:12:42,430 --> 00:12:48,579
pipelines whether they be Jenkins jobs

00:12:45,040 --> 00:12:50,110
in our cases the doctor registry but the

00:12:48,579 --> 00:12:52,509
interesting about titus is that we also

00:12:50,110 --> 00:12:53,889
added a Jenkins integration so that when

00:12:52,509 --> 00:12:56,139
our of Jenkins shall finish they write

00:12:53,889 --> 00:12:57,939
out for some properties file and then

00:12:56,139 --> 00:12:59,199
that property file contains the image

00:12:57,939 --> 00:13:02,019
that is actually being deployed for

00:12:59,199 --> 00:13:04,059
example you gotta write your health

00:13:02,019 --> 00:13:06,129
checks make sure that if you're

00:13:04,059 --> 00:13:08,079
integrating with discovery systems like

00:13:06,129 --> 00:13:09,399
Eureka those get taking into account

00:13:08,079 --> 00:13:12,970
when you're indexing your cloud provider

00:13:09,399 --> 00:13:15,550
you have to take the load balancing or

00:13:12,970 --> 00:13:17,620
any other sources of truth into account

00:13:15,550 --> 00:13:19,480
and then you integrate with all your

00:13:17,620 --> 00:13:21,370
external systems once you've done all

00:13:19,480 --> 00:13:22,809
that you're done right like you got a

00:13:21,370 --> 00:13:25,420
cloud provider it's beautiful is

00:13:22,809 --> 00:13:29,800
wonderful it works that's it right like

00:13:25,420 --> 00:13:33,459
it never changes as far this also you

00:13:29,800 --> 00:13:35,649
need to map or your terms so spinnaker

00:13:33,459 --> 00:13:40,420
has a very kind of applications like

00:13:35,649 --> 00:13:43,300
very Amazon ec2 specific way of naming

00:13:40,420 --> 00:13:44,829
things so you have to figure out in this

00:13:43,300 --> 00:13:47,350
example we have a server group and their

00:13:44,829 --> 00:13:49,059
maps to the concept of a job in Titus if

00:13:47,350 --> 00:13:51,100
an instance that maps the concept of a

00:13:49,059 --> 00:13:52,550
task in Titus you have load balancer

00:13:51,100 --> 00:13:55,520
that maps to an application of

00:13:52,550 --> 00:13:57,400
answer so once you don't know that so

00:13:55,520 --> 00:13:59,270
you can fear a trend shows your your

00:13:57,400 --> 00:14:04,520
configurations you've done all this work

00:13:59,270 --> 00:14:06,110
your cloud provider is done right is it

00:14:04,520 --> 00:14:09,830
that's it right like it never goes he

00:14:06,110 --> 00:14:11,480
never changes it's done so what we came

00:14:09,830 --> 00:14:14,060
to realize after working with status for

00:14:11,480 --> 00:14:16,760
maybe two or three years is that there

00:14:14,060 --> 00:14:21,410
are a few other things that then happen

00:14:16,760 --> 00:14:22,790
as your cloud provider passes the stage

00:14:21,410 --> 00:14:25,100
of done and is actually being used by

00:14:22,790 --> 00:14:27,350
people so in this part of the talk I'm

00:14:25,100 --> 00:14:29,210
going to really tell you about some of

00:14:27,350 --> 00:14:32,750
the examples that we ran into with Titus

00:14:29,210 --> 00:14:34,370
and maybe use it as an example for you

00:14:32,750 --> 00:14:38,720
to think about when you're building your

00:14:34,370 --> 00:14:41,960
own cloud providers so the first thing

00:14:38,720 --> 00:14:43,850
that we start realizing is that in terms

00:14:41,960 --> 00:14:45,350
of usability in terms of other user

00:14:43,850 --> 00:14:47,090
interface there are a lot of

00:14:45,350 --> 00:14:51,140
improvements that can happen when people

00:14:47,090 --> 00:14:53,720
start using the system as you should be

00:14:51,140 --> 00:14:54,920
aware the first thing that happens when

00:14:53,720 --> 00:14:56,570
you introduce more than one cloud

00:14:54,920 --> 00:14:58,370
provider is that you need to then start

00:14:56,570 --> 00:15:00,950
giving people the ability to choose

00:14:58,370 --> 00:15:03,680
which cloud provider to use a lot of

00:15:00,950 --> 00:15:05,150
that happens automatically in spinnaker

00:15:03,680 --> 00:15:06,680
when you have to issue more than one

00:15:05,150 --> 00:15:09,140
cloud provider but many stage types

00:15:06,680 --> 00:15:10,820
don't support having more than one cloud

00:15:09,140 --> 00:15:12,860
provider so you have to go in and out at

00:15:10,820 --> 00:15:14,330
all your stages now and in here for

00:15:12,860 --> 00:15:16,310
example we have Canary deployment stage

00:15:14,330 --> 00:15:18,260
and we needed to make sure that first of

00:15:16,310 --> 00:15:20,240
all the backend works with whatever this

00:15:18,260 --> 00:15:21,620
stage is doing and then modify the

00:15:20,240 --> 00:15:24,200
front-end so that now you have this

00:15:21,620 --> 00:15:26,350
option of choosing that separate cloud

00:15:24,200 --> 00:15:28,660
provider

00:15:26,350 --> 00:15:30,220
another thing that makes start happening

00:15:28,660 --> 00:15:32,410
is that you may realize that a lot of

00:15:30,220 --> 00:15:35,320
this concepts this values that you have

00:15:32,410 --> 00:15:37,510
for your stage you is don't really apply

00:15:35,320 --> 00:15:39,910
to Titus or are different for Titus so

00:15:37,510 --> 00:15:43,330
for example in here and Netflix when we

00:15:39,910 --> 00:15:44,950
configure a bake we pass it a bunch of

00:15:43,330 --> 00:15:47,980
this parameter that might be very

00:15:44,950 --> 00:15:49,570
specific to the ami image that's being

00:15:47,980 --> 00:15:52,540
produced so for example if you look at

00:15:49,570 --> 00:15:54,310
the store type here you know is EBS or

00:15:52,540 --> 00:15:55,450
s3 but when you're building for Titan

00:15:54,310 --> 00:15:56,740
you're building a container so that

00:15:55,450 --> 00:15:59,500
actually just goes to the docker

00:15:56,740 --> 00:16:03,160
registry so what we had to do also is

00:15:59,500 --> 00:16:05,250
reevaluate to use the questions that are

00:16:03,160 --> 00:16:09,430
being asked for each of the stages and

00:16:05,250 --> 00:16:12,130
actually build bake stages that make

00:16:09,430 --> 00:16:14,620
sense for titus so in this scenario for

00:16:12,130 --> 00:16:17,350
example we notice that a lot of people

00:16:14,620 --> 00:16:20,140
were triggering get commits and then

00:16:17,350 --> 00:16:22,720
just putting their docker file inside of

00:16:20,140 --> 00:16:25,240
their git repositories so that's

00:16:22,720 --> 00:16:28,750
substantially different than an AWS

00:16:25,240 --> 00:16:30,640
approach we also added the ability for

00:16:28,750 --> 00:16:32,710
example to add things like build time

00:16:30,640 --> 00:16:36,220
variables which get passed to the

00:16:32,710 --> 00:16:39,610
underlying real system as arguments into

00:16:36,220 --> 00:16:41,920
the dock or build process so then by

00:16:39,610 --> 00:16:44,260
kind of rethinking the functionality of

00:16:41,920 --> 00:16:45,910
some of the stages you quickly realize

00:16:44,260 --> 00:16:48,430
that you have to examine your entire

00:16:45,910 --> 00:16:50,920
end-to-end pipeline UI to then fit

00:16:48,430 --> 00:16:52,450
within you know the container world so

00:16:50,920 --> 00:16:54,280
that's another thing that you got to

00:16:52,450 --> 00:16:57,490
look at when you're looking at all your

00:16:54,280 --> 00:17:00,900
different stages and triggers when

00:16:57,490 --> 00:17:00,900
you're customizing that cloud provider

00:17:00,930 --> 00:17:08,980
the use cases of qaeda's were really

00:17:04,630 --> 00:17:11,350
interesting compared to the AWS BMS what

00:17:08,980 --> 00:17:15,189
we realize here for example was people

00:17:11,350 --> 00:17:18,939
would usually build maybe like 15 30

00:17:15,189 --> 00:17:20,530
clusters in AWS and that'd be it but for

00:17:18,939 --> 00:17:22,780
some cases that we found like flank

00:17:20,530 --> 00:17:25,480
people would like some of these

00:17:22,780 --> 00:17:27,010
applications ended up having 1,600

00:17:25,480 --> 00:17:29,410
clusters some of this application that

00:17:27,010 --> 00:17:33,549
are having a you know 3,500 instances

00:17:29,410 --> 00:17:35,500
and that quickly becomes very difficult

00:17:33,549 --> 00:17:37,210
to manage if you're just looking at all

00:17:35,500 --> 00:17:40,150
of them at the same time in that spirit

00:17:37,210 --> 00:17:42,640
if I in fact I believe that if at one

00:17:40,150 --> 00:17:44,020
point you could like go to that page and

00:17:42,640 --> 00:17:46,600
just watch your browser crash it was

00:17:44,020 --> 00:17:48,600
great so we had to come up with other

00:17:46,600 --> 00:17:51,970
interfaces that will allow us to scale

00:17:48,600 --> 00:17:54,070
the kind of navigation of this clusters

00:17:51,970 --> 00:17:56,620
so here for example is the expanded

00:17:54,070 --> 00:17:58,240
cluster UI so if you go to any

00:17:56,620 --> 00:18:02,650
application of spinnaker and you have

00:17:58,240 --> 00:18:04,570
above 500 clusters we won't show you all

00:18:02,650 --> 00:18:06,640
the clusters you have to go in and you

00:18:04,570 --> 00:18:08,770
have to select the list of clusters that

00:18:06,640 --> 00:18:11,650
you really care about that you want to

00:18:08,770 --> 00:18:14,350
come in and when you have 800 clusters

00:18:11,650 --> 00:18:17,799
we found that this view was a lot easier

00:18:14,350 --> 00:18:20,650
to use than having everything shown to

00:18:17,799 --> 00:18:22,360
you at the same time so that was

00:18:20,650 --> 00:18:25,990
something that we had to come up with

00:18:22,360 --> 00:18:28,299
for titus because the use cases of flink

00:18:25,990 --> 00:18:32,309
on titus were very very different than

00:18:28,299 --> 00:18:32,309
what we traditionally use for PM's

00:18:32,460 --> 00:18:38,500
we also link to external you eyes are

00:18:36,309 --> 00:18:41,980
provided by the titus team so on the

00:18:38,500 --> 00:18:43,659
sidebar for example there's a job ID and

00:18:41,980 --> 00:18:46,090
an instance ID and when you click on any

00:18:43,659 --> 00:18:49,110
of these they would point to the you

00:18:46,090 --> 00:18:53,409
guys are provided by the titus system so

00:18:49,110 --> 00:18:55,390
in AWS we try to hide as much of the AWS

00:18:53,409 --> 00:18:57,159
console details as possible because it

00:18:55,390 --> 00:18:59,530
obvious console was frankly very very

00:18:57,159 --> 00:19:02,830
difficult to use and once you go beyond

00:18:59,530 --> 00:19:04,630
three hundred four hundred large

00:19:02,830 --> 00:19:06,340
configurations but in titles they

00:19:04,630 --> 00:19:08,230
actually had a UI that was usable and

00:19:06,340 --> 00:19:10,760
that was very helpful when you're trying

00:19:08,230 --> 00:19:14,600
to debug a series of containers

00:19:10,760 --> 00:19:16,610
and so forth so we link to that the

00:19:14,600 --> 00:19:18,350
Tigers team also built our docker

00:19:16,610 --> 00:19:20,510
registry UI that would give you

00:19:18,350 --> 00:19:22,790
information about the history of the

00:19:20,510 --> 00:19:25,460
image where the images are available the

00:19:22,790 --> 00:19:28,309
tags available for this image and we

00:19:25,460 --> 00:19:31,130
then also decided to show them as part

00:19:28,309 --> 00:19:32,510
of the Titus UI so when you're looking

00:19:31,130 --> 00:19:37,070
at your clusters you can just click on

00:19:32,510 --> 00:19:40,970
the doctor registry UI and see how your

00:19:37,070 --> 00:19:43,400
image came about and how to use them we

00:19:40,970 --> 00:19:46,760
have to make sure that our performance

00:19:43,400 --> 00:19:50,210
and metrics links work with both AWS and

00:19:46,760 --> 00:19:52,490
Titus so here's a link to one of our

00:19:50,210 --> 00:19:54,110
internal tools and these are the same

00:19:52,490 --> 00:19:57,049
things that would be available to you if

00:19:54,110 --> 00:19:58,730
you're using AWS VM

00:19:57,049 --> 00:20:01,340
but what we're doing here is we're

00:19:58,730 --> 00:20:03,770
making sure that all the metrics and all

00:20:01,340 --> 00:20:08,360
the links then work with you know the

00:20:03,770 --> 00:20:11,990
Titus counterparts and then finally we

00:20:08,360 --> 00:20:13,640
added features like being able to SSH

00:20:11,990 --> 00:20:16,370
into an instance by just copying that

00:20:13,640 --> 00:20:18,470
task ID and this becomes very powerful

00:20:16,370 --> 00:20:20,630
when people need to come in and debug a

00:20:18,470 --> 00:20:21,919
container so what they do here is they

00:20:20,630 --> 00:20:23,809
copy that link they paste it on the

00:20:21,919 --> 00:20:25,309
terminal and then you can log into this

00:20:23,809 --> 00:20:27,919
instance in case you needed to do any

00:20:25,309 --> 00:20:30,919
sort of debugging so those are the kind

00:20:27,919 --> 00:20:32,840
of UI evolutions that came about once we

00:20:30,919 --> 00:20:35,780
finish the titus container people

00:20:32,840 --> 00:20:38,980
started using them and they started

00:20:35,780 --> 00:20:38,980
requesting for features like that

00:20:39,700 --> 00:20:44,990
another thing that was really

00:20:41,390 --> 00:20:48,559
interesting with titus and it probably

00:20:44,990 --> 00:20:50,450
comes about just because titus sits on

00:20:48,559 --> 00:20:53,179
top of it obvious so we we use a lot of

00:20:50,450 --> 00:20:55,760
the ADA obvious concepts spinnaker does

00:20:53,179 --> 00:21:01,370
not actually allow you to reuse an

00:20:55,760 --> 00:21:03,919
account name so we couldn't set the test

00:21:01,370 --> 00:21:05,480
account for example to map to both titus

00:21:03,919 --> 00:21:07,520
in $1 so we had to come out with a

00:21:05,480 --> 00:21:10,580
naming convention and in this case we

00:21:07,520 --> 00:21:12,770
just named it titus test and BPC is just

00:21:10,580 --> 00:21:16,700
before we had a different naming

00:21:12,770 --> 00:21:19,130
convention but let's ignore that but in

00:21:16,700 --> 00:21:21,590
order to demonstrate that that titus

00:21:19,130 --> 00:21:24,660
research that you're using was kind of

00:21:21,590 --> 00:21:27,060
using the Amazon information underneath

00:21:24,660 --> 00:21:29,670
we had to also modify the UI so it was

00:21:27,060 --> 00:21:31,650
clear that when you chose tightest SPCC

00:21:29,670 --> 00:21:33,690
you would be deploying to the test

00:21:31,650 --> 00:21:35,490
accountant for example so if you see

00:21:33,690 --> 00:21:38,580
there under a counter we have to made it

00:21:35,490 --> 00:21:39,930
visible in the UI that this was the

00:21:38,580 --> 00:21:41,460
account that you're dealing with and you

00:21:39,930 --> 00:21:43,290
would run into the same issue if you're

00:21:41,460 --> 00:21:45,480
building let's say a eazy-e's provider

00:21:43,290 --> 00:21:47,370
or if you're doing Amazon lambda because

00:21:45,480 --> 00:21:48,960
you can reuse that account name so you

00:21:47,370 --> 00:21:51,270
know you got to come out with something

00:21:48,960 --> 00:21:53,630
the UI that indicates that the

00:21:51,270 --> 00:21:56,220
underlying account that you're using is

00:21:53,630 --> 00:22:00,240
the Amazon account that you're mapping

00:21:56,220 --> 00:22:02,430
to and this screen shows it a little bit

00:22:00,240 --> 00:22:04,020
more obviously where if you're adding a

00:22:02,430 --> 00:22:05,250
load balancer in Titus you would tell

00:22:04,020 --> 00:22:07,740
you that that belongs to the test

00:22:05,250 --> 00:22:09,240
account write a security tag as it tells

00:22:07,740 --> 00:22:11,880
you that it belongs that has account and

00:22:09,240 --> 00:22:13,770
the moment that you switched accounts

00:22:11,880 --> 00:22:15,870
the underlying it address account

00:22:13,770 --> 00:22:17,160
becomes visible for you otherwise we're

00:22:15,870 --> 00:22:18,390
running into a lot of problems where

00:22:17,160 --> 00:22:20,940
people are just wondering well I have

00:22:18,390 --> 00:22:22,140
this tightest sbpc account and I don't

00:22:20,940 --> 00:22:24,900
know where it is I don't know what it is

00:22:22,140 --> 00:22:27,800
so in the UI we try to make this as

00:22:24,900 --> 00:22:27,800
peaceful as possible

00:22:31,770 --> 00:22:36,840
so those are all the UI modifications

00:22:34,080 --> 00:22:41,640
that we made we started we started

00:22:36,840 --> 00:22:43,590
reusing a lot of the so this is a

00:22:41,640 --> 00:22:44,520
feature called container migration do

00:22:43,590 --> 00:22:49,860
you want to explain what container

00:22:44,520 --> 00:22:53,610
migration does so basically titus runs

00:22:49,860 --> 00:22:55,260
on ec2 and we cannot do mutable upgrades

00:22:53,610 --> 00:22:57,210
for all of our components so we have to

00:22:55,260 --> 00:22:58,800
rebake an image and we have to figure

00:22:57,210 --> 00:23:02,220
out how to get all the existing

00:22:58,800 --> 00:23:04,140
workloads on to these new instances so

00:23:02,220 --> 00:23:09,929
the process in which we do that is

00:23:04,140 --> 00:23:10,950
called the container migration so what's

00:23:09,929 --> 00:23:14,820
interesting about the container

00:23:10,950 --> 00:23:16,800
migration is that when we implemented

00:23:14,820 --> 00:23:19,590
the container migration we use this

00:23:16,800 --> 00:23:22,280
spinner to redeploy natives so when an

00:23:19,590 --> 00:23:24,960
instant needed to be moved from one

00:23:22,280 --> 00:23:26,850
account to the other you can issue a

00:23:24,960 --> 00:23:28,410
rolling push for example and then with

00:23:26,850 --> 00:23:29,760
the rolling push is gonna do is if

00:23:28,410 --> 00:23:31,350
you're gonna take one instance is gonna

00:23:29,760 --> 00:23:32,730
move it to the new container you're

00:23:31,350 --> 00:23:34,710
gonna wait for that to finish to become

00:23:32,730 --> 00:23:37,170
healthy and terminate their instance and

00:23:34,710 --> 00:23:39,840
continue that until all the containers

00:23:37,170 --> 00:23:41,490
that need to be moved are then moved now

00:23:39,840 --> 00:23:43,380
in here what we're able to do is use the

00:23:41,490 --> 00:23:45,720
backend orchestration functionality

00:23:43,380 --> 00:23:47,150
within spinnaker to then enable this

00:23:45,720 --> 00:23:49,440
kind of migrations

00:23:47,150 --> 00:23:50,610
what's interesting here as well is that

00:23:49,440 --> 00:23:53,250
then you take advantage of the health

00:23:50,610 --> 00:23:55,950
check so if something registers itself

00:23:53,250 --> 00:23:58,920
is discovery the container will be

00:23:55,950 --> 00:24:00,330
healthy in in spinnaker so the platform

00:23:58,920 --> 00:24:01,950
doesn't really know that something is

00:24:00,330 --> 00:24:04,230
healthy Titus nodes and know that

00:24:01,950 --> 00:24:06,960
discovery status but sprinter does so we

00:24:04,230 --> 00:24:09,570
can take full advantage of that we can

00:24:06,960 --> 00:24:11,429
also allow people to specify that if

00:24:09,570 --> 00:24:12,480
whenever something needed to be migrated

00:24:11,429 --> 00:24:15,150
and they're managing their own

00:24:12,480 --> 00:24:16,770
deployments just run a pipeline and what

00:24:15,150 --> 00:24:18,630
this does is it just replaces all those

00:24:16,770 --> 00:24:20,820
instances in that cluster when that

00:24:18,630 --> 00:24:23,760
container migration happens so this is

00:24:20,820 --> 00:24:25,320
an example of kind of a more robust type

00:24:23,760 --> 00:24:28,470
of integration that you can do in your

00:24:25,320 --> 00:24:30,330
cloud provider to take advantage of all

00:24:28,470 --> 00:24:33,360
the orchestration mechanics in spinnaker

00:24:30,330 --> 00:24:35,010
and then leverage that in order to make

00:24:33,360 --> 00:24:35,879
your cloud provider migrations and other

00:24:35,010 --> 00:24:42,979
functionally like

00:24:35,879 --> 00:24:47,669
better so Titus does service shops and

00:24:42,979 --> 00:24:52,139
batch jobs and before for AWS all we had

00:24:47,669 --> 00:24:56,069
was service jobs so what does batch jobs

00:24:52,139 --> 00:24:58,589
unlock for us so when we implemented the

00:24:56,069 --> 00:25:01,199
Titus cloud provider oh the Sun we had

00:24:58,589 --> 00:25:04,139
the ability to use this thing called run

00:25:01,199 --> 00:25:05,879
job or like the ability to take batch

00:25:04,139 --> 00:25:09,479
jobs and make them as part of our

00:25:05,879 --> 00:25:14,369
pipelines and this became very powerful

00:25:09,479 --> 00:25:17,909
for us because Titus is inherently more

00:25:14,369 --> 00:25:20,309
secure than Jenkins and that was the big

00:25:17,909 --> 00:25:22,649
insight for us which Titus you can lock

00:25:20,309 --> 00:25:25,049
down I in profiles you can tell

00:25:22,649 --> 00:25:27,119
something here are the security groups

00:25:25,049 --> 00:25:30,239
that I care about you can lock down

00:25:27,119 --> 00:25:34,469
permissions you had better control over

00:25:30,239 --> 00:25:36,539
your the blast radius of your of your of

00:25:34,469 --> 00:25:38,399
your deployment right so you can really

00:25:36,539 --> 00:25:41,789
lock down whatever each one of these bad

00:25:38,399 --> 00:25:43,469
shops do there to set up a batch job all

00:25:41,789 --> 00:25:45,779
you need to do is you run the batch shop

00:25:43,469 --> 00:25:47,549
stage this is the minimal amount of

00:25:45,779 --> 00:25:51,359
information that you need in order to

00:25:47,549 --> 00:25:52,469
run something in Titus what's

00:25:51,359 --> 00:25:54,119
interesting is if you actually think

00:25:52,469 --> 00:25:56,039
that show advanced option you can

00:25:54,119 --> 00:25:58,440
actually specify things like I am Rose

00:25:56,039 --> 00:26:00,839
and security groups and so forth which

00:25:58,440 --> 00:26:04,199
allow you to then really control both at

00:26:00,839 --> 00:26:06,269
that and also kind of it allows you to

00:26:04,199 --> 00:26:10,579
specify how often I want to retry this

00:26:06,269 --> 00:26:10,579
batch job and so forth

00:26:10,829 --> 00:26:15,329
the other interesting thing that the bad

00:26:12,929 --> 00:26:18,479
shocks allowed for Titus was the ability

00:26:15,329 --> 00:26:20,489
to then reuse property files take the

00:26:18,479 --> 00:26:22,649
result of this batch job that I have run

00:26:20,489 --> 00:26:25,049
and then injected back into the pipeline

00:26:22,649 --> 00:26:27,389
so what happens with Titus is after

00:26:25,049 --> 00:26:30,539
Titus job finishes you upload everything

00:26:27,389 --> 00:26:32,099
that it has done into everything inside

00:26:30,539 --> 00:26:34,679
the locks directory gets updated

00:26:32,099 --> 00:26:36,359
uploaded into s3 so we can take

00:26:34,679 --> 00:26:38,609
advantage of this because we know that

00:26:36,359 --> 00:26:40,499
for that job everything that gets

00:26:38,609 --> 00:26:42,209
written to the log directory we can

00:26:40,499 --> 00:26:43,949
retrieve and use it later on in

00:26:42,209 --> 00:26:47,590
spinnaker so what happens here is a

00:26:43,949 --> 00:26:49,029
Titus Java runs it uploads into s3 and

00:26:47,590 --> 00:26:51,490
what Spirit er can do when you specify

00:26:49,029 --> 00:26:54,070
this property file fear feel down here

00:26:51,490 --> 00:26:56,020
this is gonna read that file from s3 and

00:26:54,070 --> 00:26:58,330
it's gonna inject that value so now

00:26:56,020 --> 00:27:00,970
you're able to use that value later on

00:26:58,330 --> 00:27:03,990
so if you're bad shop created something

00:27:00,970 --> 00:27:06,100
like I am gonna merge this pull request

00:27:03,990 --> 00:27:08,380
you can then write out whether that

00:27:06,100 --> 00:27:10,419
process was successful or not or whether

00:27:08,380 --> 00:27:12,789
any change was needed or a commit hash

00:27:10,419 --> 00:27:15,510
or whatever else is necessary there so

00:27:12,789 --> 00:27:19,539
then this allows you to really create

00:27:15,510 --> 00:27:24,010
containerize a lot of your logic into

00:27:19,539 --> 00:27:26,169
spinnaker so in spinnaker now I can

00:27:24,010 --> 00:27:28,450
extend the idea of batch shop and I can

00:27:26,169 --> 00:27:31,899
say I want to trigger something from a

00:27:28,450 --> 00:27:34,330
crown trigger I want to run a job with

00:27:31,899 --> 00:27:36,309
the round up step I can chain two of

00:27:34,330 --> 00:27:38,169
these jobs together I can run them in

00:27:36,309 --> 00:27:40,659
parallel and then I can even use the

00:27:38,169 --> 00:27:42,580
results from that first Titus job and

00:27:40,659 --> 00:27:44,230
move it to the second one and then maybe

00:27:42,580 --> 00:27:46,120
the third step there is I'm gonna send

00:27:44,230 --> 00:27:47,890
out an email but I can also do the

00:27:46,120 --> 00:27:49,299
search step there to be and then do a

00:27:47,890 --> 00:27:53,409
deployment based on this kind of

00:27:49,299 --> 00:27:57,279
information a more concrete example

00:27:53,409 --> 00:27:59,470
might be I can run a cron job I can run

00:27:57,279 --> 00:28:01,450
my tests when the test is successful I

00:27:59,470 --> 00:28:04,090
can merge the PR and I can email the

00:28:01,450 --> 00:28:06,070
user and then at the same time I also do

00:28:04,090 --> 00:28:09,490
an upload of an artifact very first

00:28:06,070 --> 00:28:11,950
simple so we have a team and Netflix Co

00:28:09,490 --> 00:28:13,720
edge productivity and I had a

00:28:11,950 --> 00:28:15,340
conversation with them and what they

00:28:13,720 --> 00:28:18,820
said that was really interesting to me

00:28:15,340 --> 00:28:22,020
was that this kind of workloads allow

00:28:18,820 --> 00:28:25,510
them to then really not rely on

00:28:22,020 --> 00:28:27,370
extending spinnaker natively they could

00:28:25,510 --> 00:28:29,020
just write this containers that had a

00:28:27,370 --> 00:28:32,620
little bit of functionality that had

00:28:29,020 --> 00:28:34,230
very well-known outputs and then test

00:28:32,620 --> 00:28:36,399
them out like really iterate through

00:28:34,230 --> 00:28:38,140
spinnaker functionality very quickly

00:28:36,399 --> 00:28:39,630
because oh they needed to do was push a

00:28:38,140 --> 00:28:43,630
container with functionality

00:28:39,630 --> 00:28:46,090
run it and then Titus ensure that things

00:28:43,630 --> 00:28:48,940
run reliably Titus ensure that things

00:28:46,090 --> 00:28:52,330
run securely and Titus allow them to

00:28:48,940 --> 00:28:53,710
have superfast velocities so any like

00:28:52,330 --> 00:28:55,990
the same process that they were use to

00:28:53,710 --> 00:28:56,730
write their services they can now use to

00:28:55,990 --> 00:28:59,130
write

00:28:56,730 --> 00:29:02,190
their tightest batch jobs and this

00:28:59,130 --> 00:29:04,320
became very very powerful to us this is

00:29:02,190 --> 00:29:07,110
also available in kubernetes and I

00:29:04,320 --> 00:29:11,549
suspect lambdas kind of fit within that

00:29:07,110 --> 00:29:13,350
pattern but it's definitely kind of one

00:29:11,549 --> 00:29:15,450
of the things that we found really

00:29:13,350 --> 00:29:23,059
unlocked the possibilities of spinnaker

00:29:15,450 --> 00:29:25,980
within Netflix titles also allowed us to

00:29:23,059 --> 00:29:28,620
ensure a lot of the security concerns

00:29:25,980 --> 00:29:34,530
that we had a Netflix so what does that

00:29:28,620 --> 00:29:38,220
mean because we had control over the

00:29:34,530 --> 00:29:41,280
entire tightest API control frame what

00:29:38,220 --> 00:29:43,140
we're able to do is when a job comes in

00:29:41,280 --> 00:29:45,929
gets submitted to spinnaker we can

00:29:43,140 --> 00:29:48,750
actually sign that job and say this job

00:29:45,929 --> 00:29:53,070
came from spinnaker and it's secured

00:29:48,750 --> 00:29:55,080
once that title job is launched that had

00:29:53,070 --> 00:29:57,710
a job then checks the signature that

00:29:55,080 --> 00:30:00,000
spinnaker provides and is able to then

00:29:57,710 --> 00:30:01,679
access a lot of the secrets that are

00:30:00,000 --> 00:30:04,200
done within Netflix so we have system

00:30:01,679 --> 00:30:06,510
call Metatron and what the titus

00:30:04,200 --> 00:30:08,910
containers are able to do is they're

00:30:06,510 --> 00:30:11,220
able to talk to the underlying platform

00:30:08,910 --> 00:30:13,320
get the secrets that they need and pass

00:30:11,220 --> 00:30:15,090
them on under the tightest job so this

00:30:13,320 --> 00:30:17,640
kind of tight integration allowed us to

00:30:15,090 --> 00:30:21,360
really make our container platform very

00:30:17,640 --> 00:30:23,010
secure and also allowed us to take

00:30:21,360 --> 00:30:25,049
advantage of a lot of the things that

00:30:23,010 --> 00:30:26,940
our security team wanted to make sure

00:30:25,049 --> 00:30:29,340
that everything that is launched the

00:30:26,940 --> 00:30:31,530
spinnaker is signed but if somebody came

00:30:29,340 --> 00:30:33,480
in through the tightest UI and then just

00:30:31,530 --> 00:30:36,419
decided to launch a container they

00:30:33,480 --> 00:30:39,179
wouldn't get the security permissions

00:30:36,419 --> 00:30:40,140
for that and that enabled us to ensure

00:30:39,179 --> 00:30:41,850
that everything that went to the

00:30:40,140 --> 00:30:46,230
spinnaker was properly audited and

00:30:41,850 --> 00:30:47,880
secured as well so by that was one of

00:30:46,230 --> 00:30:49,530
the things that we had to evolve as part

00:30:47,880 --> 00:30:51,960
of this there's this tightest thing

00:30:49,530 --> 00:30:53,460
where there were a lot of internal

00:30:51,960 --> 00:30:55,530
initiatives and then we had to build

00:30:53,460 --> 00:30:59,330
that into the platform in order to

00:30:55,530 --> 00:30:59,330
devolve this part of the cloud provider

00:31:01,490 --> 00:31:06,559
so when I think back to kind of all the

00:31:04,220 --> 00:31:08,450
changes that we have made to Titus it's

00:31:06,559 --> 00:31:11,630
normally within three categories right

00:31:08,450 --> 00:31:13,700
now we had to make changes for usability

00:31:11,630 --> 00:31:17,000
reasons like people wanted more clarity

00:31:13,700 --> 00:31:18,409
on different things we added features

00:31:17,000 --> 00:31:20,570
you know like somebody added the ability

00:31:18,409 --> 00:31:23,120
to talk to a ef-s drive so we need to

00:31:20,570 --> 00:31:24,740
add it to that cloud provider but one of

00:31:23,120 --> 00:31:28,669
the biggest things that we were able to

00:31:24,740 --> 00:31:31,070
do with Titus is really make changes for

00:31:28,669 --> 00:31:33,080
performance improvements as the

00:31:31,070 --> 00:31:35,299
underlying Titus platform evolved and it

00:31:33,080 --> 00:31:37,309
grew up and they started solving a lot

00:31:35,299 --> 00:31:38,630
of their own problems we could we start

00:31:37,309 --> 00:31:40,549
taking advantage of a lot of this

00:31:38,630 --> 00:31:42,559
constructs that they built in order to

00:31:40,549 --> 00:31:44,899
make the cloud provider itself more

00:31:42,559 --> 00:31:46,970
performance now this is something that

00:31:44,899 --> 00:31:48,679
might not be easy to do if you're

00:31:46,970 --> 00:31:51,020
working with something like AWS where

00:31:48,679 --> 00:31:53,299
you request give us a streaming API and

00:31:51,020 --> 00:31:55,460
it takes a long time for that to happen

00:31:53,299 --> 00:31:58,700
but because we have control of both

00:31:55,460 --> 00:32:01,100
Titus and spinnaker we're able to take

00:31:58,700 --> 00:32:04,220
advantage of those kind of improvements

00:32:01,100 --> 00:32:07,580
and this is one example this is the

00:32:04,220 --> 00:32:10,880
evolution of the way that spinner great

00:32:07,580 --> 00:32:13,070
indexes Titus so there's an evolution

00:32:10,880 --> 00:32:16,279
here we started with a REST API that

00:32:13,070 --> 00:32:19,880
gave you everything then we moved on to

00:32:16,279 --> 00:32:22,399
G RPC blocking based API that allows us

00:32:19,880 --> 00:32:25,640
to separate jobs and tasks within this

00:32:22,399 --> 00:32:27,710
provider and finally we move to a gr PC

00:32:25,640 --> 00:32:32,029
streaming API that allowed us to the

00:32:27,710 --> 00:32:34,850
real-time updates and it looks like this

00:32:32,029 --> 00:32:37,220
so the REST API pretty straightforward

00:32:34,850 --> 00:32:38,960
you ask Titus for something it gives you

00:32:37,220 --> 00:32:42,320
all the jobs and all the tasks and then

00:32:38,960 --> 00:32:44,779
you index them when the Titus team

00:32:42,320 --> 00:32:46,789
introduced the ability to do G RPC

00:32:44,779 --> 00:32:48,200
polling they also made changes to their

00:32:46,789 --> 00:32:50,630
interfaces so they would separate

00:32:48,200 --> 00:32:53,330
request for all the jobs and all the

00:32:50,630 --> 00:32:55,370
tasks or instances so that allowed us to

00:32:53,330 --> 00:32:57,500
really kind of separate those into

00:32:55,370 --> 00:33:00,890
separate caching agents we're able to

00:32:57,500 --> 00:33:03,169
use features within Titus such as Eureka

00:33:00,890 --> 00:33:05,809
and federated endpoints and also we

00:33:03,169 --> 00:33:09,110
converted a lot of the one of logic on

00:33:05,809 --> 00:33:10,490
res interfaces into G RPC interceptors

00:33:09,110 --> 00:33:13,159
so we had interceptors for logging

00:33:10,490 --> 00:33:15,409
interceptors for authentication retries

00:33:13,159 --> 00:33:18,539
and so forth

00:33:15,409 --> 00:33:21,809
then we moved on to a streaming based

00:33:18,539 --> 00:33:24,779
model of G of G RPC so we listen and

00:33:21,809 --> 00:33:28,190
pull for changes within Titus itself and

00:33:24,779 --> 00:33:31,260
the biggest benefit there was really the

00:33:28,190 --> 00:33:33,210
assurance that every update that happens

00:33:31,260 --> 00:33:35,490
would automatically update the cloud

00:33:33,210 --> 00:33:37,200
provider and then this allowed us to get

00:33:35,490 --> 00:33:39,419
rid of the forced cache refreshes that

00:33:37,200 --> 00:33:43,590
were happening in an actual deployment

00:33:39,419 --> 00:33:47,460
we saw on average about 20 seconds being

00:33:43,590 --> 00:33:49,649
dropped from you issuing a request to

00:33:47,460 --> 00:33:52,740
your server being fully updated just by

00:33:49,649 --> 00:33:56,580
eliminating the four skies refreshes how

00:33:52,740 --> 00:33:58,529
does this look like so in the Reston

00:33:56,580 --> 00:34:02,250
point so the black bits are jobs and the

00:33:58,529 --> 00:34:04,440
white bits are tasks right so in the

00:34:02,250 --> 00:34:07,710
rest api what would happen is you would

00:34:04,440 --> 00:34:09,780
come in and you to poll for the state of

00:34:07,710 --> 00:34:11,669
the world right you're cashing agent

00:34:09,780 --> 00:34:14,879
would then come in and get the state of

00:34:11,669 --> 00:34:17,639
the world wait 30 seconds you'll get the

00:34:14,879 --> 00:34:19,859
next state you'll wait 30 seconds and

00:34:17,639 --> 00:34:22,169
you'll get the next state so in the

00:34:19,859 --> 00:34:23,849
second state we see that another task

00:34:22,169 --> 00:34:25,740
was added to the end and then in the

00:34:23,849 --> 00:34:30,690
final state we see that two tasks were

00:34:25,740 --> 00:34:32,490
removed in the G RPC polling world we

00:34:30,690 --> 00:34:36,899
were able to separate tasks and jobs

00:34:32,490 --> 00:34:39,089
so on the first query you get all the

00:34:36,899 --> 00:34:41,490
jobs and then second you get all the

00:34:39,089 --> 00:34:42,929
tasks simultaneously that made the cash

00:34:41,490 --> 00:34:46,950
and provider happen that made the cash

00:34:42,929 --> 00:34:48,599
in happen a lot quicker so in the second

00:34:46,950 --> 00:34:50,609
stage you then come in and you get over

00:34:48,599 --> 00:34:52,260
the task and other jobs and you see that

00:34:50,609 --> 00:34:54,480
at this you note as being tacked at the

00:34:52,260 --> 00:34:57,270
end and then in the final polling phase

00:34:54,480 --> 00:35:01,790
you see here we have all the jobs and

00:34:57,270 --> 00:35:01,790
now we have the two tasks that come back

00:35:01,820 --> 00:35:08,090
in the streaming world what happens is

00:35:04,450 --> 00:35:11,840
you have that initial state the same

00:35:08,090 --> 00:35:14,330
thing as the rest API but then in the

00:35:11,840 --> 00:35:17,600
intermediate period when that task gets

00:35:14,330 --> 00:35:20,140
added the state with the instrument gets

00:35:17,600 --> 00:35:23,650
up there gets updated right away and

00:35:20,140 --> 00:35:27,080
that then becomes visible immediately

00:35:23,650 --> 00:35:28,100
when the next polling cycle happens the

00:35:27,080 --> 00:35:29,990
first thing we're gonna do is get the

00:35:28,100 --> 00:35:31,490
state of the world so if we ever miss

00:35:29,990 --> 00:35:35,540
that event we can still reconcile with

00:35:31,490 --> 00:35:37,790
that but then along the way we can see

00:35:35,540 --> 00:35:40,820
that one task was deleted and then

00:35:37,790 --> 00:35:44,990
another task was deleted and then at

00:35:40,820 --> 00:35:46,400
that point we're up to date we and then

00:35:44,990 --> 00:35:47,660
at the start of the next polling cycle

00:35:46,400 --> 00:35:50,300
we just get that state of the world

00:35:47,660 --> 00:35:52,940
again right so by using the streaming

00:35:50,300 --> 00:35:54,770
API we're able to then really improve

00:35:52,940 --> 00:35:56,810
the performance within Titus and

00:35:54,770 --> 00:35:58,730
spinnaker and that was kind of one of

00:35:56,810 --> 00:36:00,290
the biggest bottlenecks that we have

00:35:58,730 --> 00:36:02,840
struggled with which Titus ever since

00:36:00,290 --> 00:36:05,240
the beginning so by taking advantage of

00:36:02,840 --> 00:36:07,490
changes in the system and by coming out

00:36:05,240 --> 00:36:10,280
with Nova in an innovative ways of doing

00:36:07,490 --> 00:36:13,780
caching we're able to then improve the

00:36:10,280 --> 00:36:13,780
performance of titus tremendously

00:36:15,570 --> 00:36:18,630
maybe as a final note when you're

00:36:17,430 --> 00:36:22,440
running one of these things in

00:36:18,630 --> 00:36:24,210
production we found that here's the

00:36:22,440 --> 00:36:26,790
things are really important and that

00:36:24,210 --> 00:36:28,770
really help when you're debugging and

00:36:26,790 --> 00:36:29,790
building a cloud provider so the first

00:36:28,770 --> 00:36:33,390
thing is in sure that you have proper

00:36:29,790 --> 00:36:35,610
metrics and logging there's an instance

00:36:33,390 --> 00:36:37,860
for example where we were indexing

00:36:35,610 --> 00:36:39,720
security groups in AWS but we were

00:36:37,860 --> 00:36:41,520
getting the entire object for that

00:36:39,720 --> 00:36:43,680
security group and so people are

00:36:41,520 --> 00:36:45,570
wondering why does it take another 30

00:36:43,680 --> 00:36:47,490
seconds for us to start a job and titus

00:36:45,570 --> 00:36:48,990
through metrics and logging we were

00:36:47,490 --> 00:36:51,000
actually able to figure out that we were

00:36:48,990 --> 00:36:51,960
just getting too much information of the

00:36:51,000 --> 00:36:53,760
security group when we were just

00:36:51,960 --> 00:36:55,740
checking that they existed by

00:36:53,760 --> 00:36:59,850
eliminating that we're able to shed 30

00:36:55,740 --> 00:37:01,680
seconds of that entire process and a lot

00:36:59,850 --> 00:37:05,310
of it was basically due to just having

00:37:01,680 --> 00:37:07,320
proper metrics and logging charting of

00:37:05,310 --> 00:37:09,690
cloud providers when dealing with

00:37:07,320 --> 00:37:11,010
performance cloud driver has a lot of

00:37:09,690 --> 00:37:13,560
functionality around being able to

00:37:11,010 --> 00:37:16,410
isolate the traffic that you're sending

00:37:13,560 --> 00:37:18,540
to each shard and so as a performance

00:37:16,410 --> 00:37:20,550
characteristic sometimes what we were

00:37:18,540 --> 00:37:22,830
able to do is for example send a lot of

00:37:20,550 --> 00:37:25,080
the non-critical requests to another

00:37:22,830 --> 00:37:26,940
shot of cloud providers I know Cameron

00:37:25,080 --> 00:37:28,520
is going to talk a lot about more about

00:37:26,940 --> 00:37:30,930
like what you can do to optimize

00:37:28,520 --> 00:37:33,900
spinnaker and production but when you're

00:37:30,930 --> 00:37:35,700
building a cloud provider knowing that

00:37:33,900 --> 00:37:37,710
sharding exists and taking advantage of

00:37:35,700 --> 00:37:39,210
a lot of this functionality they're

00:37:37,710 --> 00:37:43,200
building to spinet or separate your

00:37:39,210 --> 00:37:44,820
traffic because we're important for

00:37:43,200 --> 00:37:46,710
Titus what we found is adding feature

00:37:44,820 --> 00:37:48,240
flags for each new functionality as

00:37:46,710 --> 00:37:50,370
you're evolving your cloud driver I

00:37:48,240 --> 00:37:53,460
serve on your cloud provider was really

00:37:50,370 --> 00:37:55,050
important if there were bugs in the new

00:37:53,460 --> 00:37:57,450
provider we can easily switch back to

00:37:55,050 --> 00:38:00,270
the old version of that provider if

00:37:57,450 --> 00:38:01,590
we're adding things that we weren't sure

00:38:00,270 --> 00:38:03,090
what's gonna have an impact of

00:38:01,590 --> 00:38:05,460
performance having features flags

00:38:03,090 --> 00:38:06,870
allowed us to kind of very easily just

00:38:05,460 --> 00:38:08,430
roll it up and maybe they'd like the

00:38:06,870 --> 00:38:10,200
test account and a test region first

00:38:08,430 --> 00:38:13,740
before moving it into our entire

00:38:10,200 --> 00:38:15,240
production stack and finally api's are

00:38:13,740 --> 00:38:18,330
going to evolve so having a strategy

00:38:15,240 --> 00:38:21,120
around evolving that API specifying

00:38:18,330 --> 00:38:23,220
version numbers and so forth you notice

00:38:21,120 --> 00:38:24,640
that there's a kubernetes b1 and a b2

00:38:23,220 --> 00:38:26,290
provider

00:38:24,640 --> 00:38:29,140
think about that when building your

00:38:26,290 --> 00:38:31,870
cloud provider what is going to be hard

00:38:29,140 --> 00:38:34,270
versions that we map to and what he's

00:38:31,870 --> 00:38:36,120
gonna build backwards compatibility and

00:38:34,270 --> 00:38:41,680
Netflix it was very difficult for us to

00:38:36,120 --> 00:38:44,440
shift away our API s because there was a

00:38:41,680 --> 00:38:46,240
lot of automation built around the

00:38:44,440 --> 00:38:50,650
spinnaker API so we had what we had to

00:38:46,240 --> 00:38:53,290
do is actually keep the api's at a lower

00:38:50,650 --> 00:38:55,710
level so if i had to redo this again i

00:38:53,290 --> 00:38:58,840
would have started with like a very

00:38:55,710 --> 00:39:01,150
robust versioning scheme for my cloud

00:38:58,840 --> 00:39:03,280
provider versus very earlier on so that

00:39:01,150 --> 00:39:05,470
later on you're not thinking about

00:39:03,280 --> 00:39:07,860
breaking everybody that is using a cup

00:39:05,470 --> 00:39:07,860

YouTube URL: https://www.youtube.com/watch?v=MxlrpelB2Ww


