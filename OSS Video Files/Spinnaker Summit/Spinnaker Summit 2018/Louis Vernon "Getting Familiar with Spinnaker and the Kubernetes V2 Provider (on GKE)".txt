Title: Louis Vernon "Getting Familiar with Spinnaker and the Kubernetes V2 Provider (on GKE)"
Publication date: 2018-10-24
Playlist: Spinnaker Summit 2018
Description: 
	
Captions: 
	00:00:01,270 --> 00:00:05,380
[Music]

00:00:09,820 --> 00:00:13,259
[Music]

00:00:16,949 --> 00:00:21,689
let's get started okay so my name is

00:00:19,830 --> 00:00:23,550
Louie Vernon I'm a site reliability

00:00:21,689 --> 00:00:27,510
engineer at a company called Descartes

00:00:23,550 --> 00:00:29,010
labs based in New Mexico I'm going to be

00:00:27,510 --> 00:00:31,080
talking to you today about getting

00:00:29,010 --> 00:00:33,600
familiar with spinnaker and specifically

00:00:31,080 --> 00:00:35,219
the kubernetes v2 provider we happen to

00:00:33,600 --> 00:00:36,750
be running on gke there's not much that

00:00:35,219 --> 00:00:41,250
she carries Pacific there are a couple

00:00:36,750 --> 00:00:42,809
of things I guess I'll put it out there

00:00:41,250 --> 00:00:46,620
that this is like a kind of a user group

00:00:42,809 --> 00:00:49,200
talk and sort of I just guess like some

00:00:46,620 --> 00:00:51,239
some of our pipelines I also wanted to

00:00:49,200 --> 00:00:53,250
mention this is this is my first-ever

00:00:51,239 --> 00:00:57,059
technical talk as a site reliability

00:00:53,250 --> 00:00:58,680
engineer previously I was a scientist a

00:00:57,059 --> 00:01:00,750
year ago as a scientist at Los Alamos

00:00:58,680 --> 00:01:03,690
National Lab and I worked on the

00:01:00,750 --> 00:01:06,060
performance optimization of scientific

00:01:03,690 --> 00:01:08,100
coats like monolithic applications

00:01:06,060 --> 00:01:11,189
running it to two million or more

00:01:08,100 --> 00:01:13,560
threads then I moved to tech cloud-based

00:01:11,189 --> 00:01:15,780
start-up started working on the problem

00:01:13,560 --> 00:01:17,670
of micro services at tens of thousands

00:01:15,780 --> 00:01:18,960
of threads that's definitely a harder

00:01:17,670 --> 00:01:22,860
problem I just wanted to put that out

00:01:18,960 --> 00:01:23,490
there okay so I mentioned I work for

00:01:22,860 --> 00:01:27,000
tech cart labs

00:01:23,490 --> 00:01:29,729
what is take art labs take our labs is

00:01:27,000 --> 00:01:32,220
is a tech startup in New Mexico there

00:01:29,729 --> 00:01:33,840
aren't many of us it's also out of Los

00:01:32,220 --> 00:01:35,430
Alamos National Lab it was founded by a

00:01:33,840 --> 00:01:37,860
bunch of scientists really great

00:01:35,430 --> 00:01:39,689
scientists who had some great ideas

00:01:37,860 --> 00:01:42,600
about computer vision and HUD forms good

00:01:39,689 --> 00:01:44,100
computer vision stuff it's currently

00:01:42,600 --> 00:01:47,460
based in Santa Fe it started off in Los

00:01:44,100 --> 00:01:49,860
Alamos and our goal is to provide a

00:01:47,460 --> 00:01:52,680
platform to derive insights from

00:01:49,860 --> 00:01:55,049
petabytes of geospatial information and

00:01:52,680 --> 00:01:56,490
yeah ideally the end users don't even

00:01:55,049 --> 00:01:58,799
know the scale of the underlying data

00:01:56,490 --> 00:02:00,960
they're processing currently we said

00:01:58,799 --> 00:02:04,320
about 80 employees I think we have over

00:02:00,960 --> 00:02:06,270
20 engineers and the company and since I

00:02:04,320 --> 00:02:06,570
joined in the last year is doubled in

00:02:06,270 --> 00:02:08,429
size

00:02:06,570 --> 00:02:11,569
what sort of it's kind of relevant to

00:02:08,429 --> 00:02:14,569
what I talk about in this presentation

00:02:11,569 --> 00:02:16,519
oh and that's that's Frannie is just

00:02:14,569 --> 00:02:18,640
like director of greetings or something

00:02:16,519 --> 00:02:21,920
like that okay

00:02:18,640 --> 00:02:24,799
what does our platform look like so

00:02:21,920 --> 00:02:26,780
Descartes labs platform it's primarily

00:02:24,799 --> 00:02:29,420
Python we're using the flask framework

00:02:26,780 --> 00:02:31,760
we we do have a number of languages we

00:02:29,420 --> 00:02:33,650
we have some rust and we have some go

00:02:31,760 --> 00:02:35,720
that we use in production rust is bolded

00:02:33,650 --> 00:02:37,760
because it's very a very small amount of

00:02:35,720 --> 00:02:39,650
rust but it does very very important

00:02:37,760 --> 00:02:40,280
jobs or very important job in our

00:02:39,650 --> 00:02:42,230
platform

00:02:40,280 --> 00:02:44,659
there's also C and C++ and various other

00:02:42,230 --> 00:02:46,220
languages out there but most of the

00:02:44,659 --> 00:02:48,980
deployment stuff is related to

00:02:46,220 --> 00:02:53,299
applications that are Python and flask -

00:02:48,980 --> 00:02:55,310
flask it's a platform is built almost

00:02:53,299 --> 00:02:56,750
entirely on Google cloud platform I'm

00:02:55,310 --> 00:02:59,840
like our core platform these are all in

00:02:56,750 --> 00:03:03,349
GCP we do have small pieces on other

00:02:59,840 --> 00:03:05,840
cloud providers like just a wet where it

00:03:03,349 --> 00:03:08,150
makes sense because obviously we suck in

00:03:05,840 --> 00:03:10,730
a lot of data and sometimes our data

00:03:08,150 --> 00:03:11,989
sources or in other cloud providers we

00:03:10,730 --> 00:03:13,819
have we don't have any managed to

00:03:11,989 --> 00:03:16,699
infrastructure like at the hardware

00:03:13,819 --> 00:03:18,349
level it they're probably like a

00:03:16,699 --> 00:03:20,359
particularly notable thing about our

00:03:18,349 --> 00:03:22,940
relationship with GCP is we have over 10

00:03:20,359 --> 00:03:26,989
petabytes of data and GCS and yeah we do

00:03:22,940 --> 00:03:29,150
sort of analytics over that data our

00:03:26,989 --> 00:03:30,889
core platform is built in google

00:03:29,150 --> 00:03:33,739
kubernetes engine it's all I've known

00:03:30,889 --> 00:03:35,750
since I joined the company I checked

00:03:33,739 --> 00:03:38,479
slack and my crazy manager has been

00:03:35,750 --> 00:03:41,180
using it in production GK since Cuba

00:03:38,479 --> 00:03:43,250
Nettie's version 0.1 9 which is back in

00:03:41,180 --> 00:03:46,280
2015 I mean like it was limited

00:03:43,250 --> 00:03:48,739
production use at that time but still we

00:03:46,280 --> 00:03:52,760
began like the the migration in earnest

00:03:48,739 --> 00:03:56,599
just before version 1.6 so that was that

00:03:52,760 --> 00:03:58,010
was last year sometime and our spinnaker

00:03:56,599 --> 00:04:02,269
pipelines are only concerned with the

00:03:58,010 --> 00:04:03,739
infrastructure that lives on TK we do

00:04:02,269 --> 00:04:05,900
have some legacy infrastructure on TC

00:04:03,739 --> 00:04:10,729
slowly we're moving stuff over as the

00:04:05,900 --> 00:04:11,900
platform becomes more more capable it's

00:04:10,729 --> 00:04:13,819
kind of important to know what kind of

00:04:11,900 --> 00:04:15,680
utilization there is a platform because

00:04:13,819 --> 00:04:17,120
we're doing continuous deployment and we

00:04:15,680 --> 00:04:20,239
want services to be healthy all the time

00:04:17,120 --> 00:04:22,250
and so the utilization our platform is

00:04:20,239 --> 00:04:23,960
we have some static load because we're

00:04:22,250 --> 00:04:25,040
constantly looking at real-time ingest

00:04:23,960 --> 00:04:26,390
of geospatial data

00:04:25,040 --> 00:04:28,520
there's corresponding analytics it's

00:04:26,390 --> 00:04:31,070
performed as its pulled in we pull in

00:04:28,520 --> 00:04:32,540
ten plus terabytes a day you know

00:04:31,070 --> 00:04:33,800
sometimes that's that's order of

00:04:32,540 --> 00:04:37,190
magnitude higher when we add a new

00:04:33,800 --> 00:04:38,690
partner or what but but the vast bulk of

00:04:37,190 --> 00:04:40,160
our load is dynamic load so that's

00:04:38,690 --> 00:04:42,320
people doing live exploration of the

00:04:40,160 --> 00:04:44,630
geospatial products post process data

00:04:42,320 --> 00:04:47,390
derived analytics that kind of thing or

00:04:44,630 --> 00:04:50,720
they're building and testing sort of ml

00:04:47,390 --> 00:04:52,400
TV models but the biggest load very very

00:04:50,720 --> 00:04:53,780
substantial load on our platform is when

00:04:52,400 --> 00:04:56,000
people actually are satisfied with a

00:04:53,780 --> 00:04:57,680
model and they want to run it over large

00:04:56,000 --> 00:04:59,930
scales and that's when we hit these tens

00:04:57,680 --> 00:05:07,280
of thousands of V CPUs I mentioned in

00:04:59,930 --> 00:05:09,050
the abstract this is a crazy drawing of

00:05:07,280 --> 00:05:10,100
our platform that a platform engineer

00:05:09,050 --> 00:05:12,350
just happened to put together for a

00:05:10,100 --> 00:05:14,660
different talk the other day all this

00:05:12,350 --> 00:05:16,310
serves to show is we have a number of

00:05:14,660 --> 00:05:21,320
micro services they have relatively

00:05:16,310 --> 00:05:24,050
complex inter dependencies and so we

00:05:21,320 --> 00:05:27,230
have sort of a complex enough system I

00:05:24,050 --> 00:05:30,530
think to sort of worry about continuous

00:05:27,230 --> 00:05:32,660
deployment serviceability the the stuff

00:05:30,530 --> 00:05:35,540
at the bottom of this chart is all like

00:05:32,660 --> 00:05:37,960
our persistent storage abstractions and

00:05:35,540 --> 00:05:40,970
there's pub/sub GCS bigquery datastore

00:05:37,960 --> 00:05:42,950
Redis post grows they are all DCP

00:05:40,970 --> 00:05:46,040
managed services well managed to some

00:05:42,950 --> 00:05:48,470
extent some more than others

00:05:46,040 --> 00:05:50,270
the the exception there is elastic

00:05:48,470 --> 00:05:53,270
search we have a very big wall I think

00:05:50,270 --> 00:05:55,310
it's big elastic search cluster in GK

00:05:53,270 --> 00:05:57,140
and and oh I should have mentioned for

00:05:55,310 --> 00:05:59,420
the SRE team which is just the three of

00:05:57,140 --> 00:06:01,010
us but that's that's probably the it

00:05:59,420 --> 00:06:07,460
it's recently at least it's been the

00:06:01,010 --> 00:06:09,860
biggest maintenance burden okay so let's

00:06:07,460 --> 00:06:11,600
talk about growing pains we flash back

00:06:09,860 --> 00:06:13,900
to the beginning of 2018 are still

00:06:11,600 --> 00:06:16,220
pretty fresh faced necessary at the time

00:06:13,900 --> 00:06:18,440
we had a really rapidly growing

00:06:16,220 --> 00:06:20,210
engineering team we had emerging sub

00:06:18,440 --> 00:06:22,610
teams for the first time we had like

00:06:20,210 --> 00:06:25,190
communication breaking down as teams

00:06:22,610 --> 00:06:27,950
were being partitioned we had a rapidly

00:06:25,190 --> 00:06:31,970
expanding catalogue of services being

00:06:27,950 --> 00:06:33,260
figured out we had we we really liked

00:06:31,970 --> 00:06:35,420
how Google does things we're trying to

00:06:33,260 --> 00:06:36,600
emulate them whenever possible so we

00:06:35,420 --> 00:06:39,160
were

00:06:36,600 --> 00:06:41,830
consolidating all of our we have like

00:06:39,160 --> 00:06:44,380
over 100 repos I'm consolidating

00:06:41,830 --> 00:06:45,880
everything into one mono repo using

00:06:44,380 --> 00:06:48,910
basil actually I think I've got three

00:06:45,880 --> 00:06:50,680
colleagues at basil con right now and we

00:06:48,910 --> 00:06:53,080
have several CI technologies so

00:06:50,680 --> 00:06:54,940
different teams that like put together

00:06:53,080 --> 00:06:56,590
at their own what made sense to them in

00:06:54,940 --> 00:06:59,020
terms of CI pipelines so we had drone to

00:06:56,590 --> 00:07:00,850
IO circle CI contain a builder and

00:06:59,020 --> 00:07:02,470
Jenkins actually I think Jake it's

00:07:00,850 --> 00:07:05,980
already been deprecated by the time I

00:07:02,470 --> 00:07:07,900
start but more importantly we didn't

00:07:05,980 --> 00:07:11,950
have any formal continuous deployment

00:07:07,900 --> 00:07:13,360
process one person on side one person

00:07:11,950 --> 00:07:15,790
our company he put together this to

00:07:13,360 --> 00:07:17,980
cuddle bot which basically was you could

00:07:15,790 --> 00:07:19,720
post your coop cuddle deploy command to

00:07:17,980 --> 00:07:21,430
a slack channel and it would apply to

00:07:19,720 --> 00:07:23,860
the bots and the only reason for that

00:07:21,430 --> 00:07:25,770
was just have a nice audit trail who did

00:07:23,860 --> 00:07:28,780
what right

00:07:25,770 --> 00:07:30,730
engineers had gke privileges we didn't

00:07:28,780 --> 00:07:32,950
have like a formal hisses best practice

00:07:30,730 --> 00:07:34,660
this is a good deployment strategy so we

00:07:32,950 --> 00:07:37,090
got all sorts of namespaces popping up

00:07:34,660 --> 00:07:41,680
all over the place hopefully people can

00:07:37,090 --> 00:07:43,480
relate to this at the same time the SRE

00:07:41,680 --> 00:07:46,240
team have had this big push around

00:07:43,480 --> 00:07:48,190
managing complex service - well actually

00:07:46,240 --> 00:07:50,980
we'd have a big push about capturing our

00:07:48,190 --> 00:07:53,350
infrastructure as code so again that was

00:07:50,980 --> 00:07:55,180
you know we interacted with some Google

00:07:53,350 --> 00:07:57,700
solutions engineers and we really liked

00:07:55,180 --> 00:07:59,710
the sound of that at the time there was

00:07:57,700 --> 00:08:02,410
like an embarrassing amount of G cloud

00:07:59,710 --> 00:08:05,290
CLI stuff but now you know stuff is

00:08:02,410 --> 00:08:06,760
being all captured in terraform so we

00:08:05,290 --> 00:08:08,410
have this like nice redeploy abou

00:08:06,760 --> 00:08:10,810
infrastructure through container builder

00:08:08,410 --> 00:08:13,660
which executed whenever we we made

00:08:10,810 --> 00:08:15,580
change to our repo but kubernetes

00:08:13,660 --> 00:08:17,620
manifests were like a totally different

00:08:15,580 --> 00:08:19,030
world like the Wild West there though if

00:08:17,620 --> 00:08:20,560
they were captured at all they were

00:08:19,030 --> 00:08:23,350
poorly captured and I went immediately

00:08:20,560 --> 00:08:24,640
stale people are editing the manifests

00:08:23,350 --> 00:08:26,890
like coop cuddle edit or they're going

00:08:24,640 --> 00:08:29,620
in the console and changing them so

00:08:26,890 --> 00:08:31,540
they're immediately out of date and so

00:08:29,620 --> 00:08:34,020
we wanted to way an automated way to

00:08:31,540 --> 00:08:36,039
deploy kubernetes manifest from code and

00:08:34,020 --> 00:08:38,560
we wanted it integrated into our

00:08:36,039 --> 00:08:40,570
software development workflows we wanted

00:08:38,560 --> 00:08:44,710
to be able to like do a PR trigger build

00:08:40,570 --> 00:08:46,480
have have a nice like revision dog

00:08:44,710 --> 00:08:49,360
football history we wanted to be

00:08:46,480 --> 00:08:51,700
reproducible with by version to execute

00:08:49,360 --> 00:08:53,620
executions we want it to be reversible

00:08:51,700 --> 00:08:55,269
so I either through a manual or

00:08:53,620 --> 00:08:57,360
automated procedure we'd be able to roll

00:08:55,269 --> 00:09:01,269
back a deployment and these three things

00:08:57,360 --> 00:09:03,730
kind of meant that by tacking it on to

00:09:01,269 --> 00:09:10,149
some CI process just didn't didn't make

00:09:03,730 --> 00:09:11,890
sense right so it's mid-february we have

00:09:10,149 --> 00:09:16,180
a conversation with the Google solutions

00:09:11,890 --> 00:09:17,800
engineer like I called Vic and Vic says

00:09:16,180 --> 00:09:20,230
well you guys already have all your

00:09:17,800 --> 00:09:22,540
infrastructure captured as code and if

00:09:20,230 --> 00:09:24,790
you like that you're gonna love the

00:09:22,540 --> 00:09:28,149
spinnaker v2 kubernetes provider which

00:09:24,790 --> 00:09:32,320
at the time was just launched in alpha 1

00:09:28,149 --> 00:09:34,240
dot 6.0 and was like despite the fact

00:09:32,320 --> 00:09:37,140
that we really appreciate it it was very

00:09:34,240 --> 00:09:39,430
very painful to use at the time

00:09:37,140 --> 00:09:41,170
initially we did like an explorative

00:09:39,430 --> 00:09:43,300
effort where we were trying to manage

00:09:41,170 --> 00:09:44,980
clusters using the v1 and v2 providers

00:09:43,300 --> 00:09:47,860
but we'd come from like a purely

00:09:44,980 --> 00:09:49,769
manifest kubernetes world and the v1

00:09:47,860 --> 00:09:53,500
provider like explicitly managing

00:09:49,769 --> 00:09:55,120
resources like having like defined you

00:09:53,500 --> 00:09:58,089
know load balancers that better that

00:09:55,120 --> 00:09:59,079
sort of that that was sort of orthogonal

00:09:58,089 --> 00:10:00,790
to the way we'd already been thinking

00:09:59,079 --> 00:10:04,390
about the problem so v2 and naturally

00:10:00,790 --> 00:10:05,769
appealed to us and and also when you try

00:10:04,390 --> 00:10:07,329
and use them both at the same time you

00:10:05,769 --> 00:10:09,040
end up with these confusing deployment

00:10:07,329 --> 00:10:11,019
models because while you can't right

00:10:09,040 --> 00:10:13,029
there's there's artifacts that get

00:10:11,019 --> 00:10:15,279
produced by one stage that don't make

00:10:13,029 --> 00:10:19,720
any sense to a different stage uses the

00:10:15,279 --> 00:10:21,339
v2 provider just to give you some

00:10:19,720 --> 00:10:23,560
context actually if you don't already

00:10:21,339 --> 00:10:26,079
know like what the differences are

00:10:23,560 --> 00:10:28,060
between the two providers so like

00:10:26,079 --> 00:10:30,220
typically I say many spirit providers

00:10:28,060 --> 00:10:32,350
buy these maybe all of them apart from

00:10:30,220 --> 00:10:34,180
the v2 provider spinnaker explicitly

00:10:32,350 --> 00:10:35,860
manages the underlying resources are

00:10:34,180 --> 00:10:38,320
involved in rollouts rollbacks

00:10:35,860 --> 00:10:40,480
deployment strategies scaling and so on

00:10:38,320 --> 00:10:44,110
or maybe not explicitly scaling all the

00:10:40,480 --> 00:10:46,510
time but it can scale right the v1

00:10:44,110 --> 00:10:48,790
provider it follows follows that same

00:10:46,510 --> 00:10:50,140
model so you kubernetes clusters have

00:10:48,790 --> 00:10:52,959
treated more or less like a generic

00:10:50,140 --> 00:10:54,490
cloud offering and like I said this was

00:10:52,959 --> 00:10:56,079
sort of orthogonal to communities

00:10:54,490 --> 00:10:59,279
declarative object management

00:10:56,079 --> 00:11:01,569
architecture and so the v2 provider

00:10:59,279 --> 00:11:02,150
leverages kubernetes native object

00:11:01,569 --> 00:11:04,330
management

00:11:02,150 --> 00:11:07,750
let's kubernetes take care of like a

00:11:04,330 --> 00:11:09,560
rolling upgrade of a server deployment

00:11:07,750 --> 00:11:10,730
anyway it's very nicely with your

00:11:09,560 --> 00:11:16,250
existing manifest

00:11:10,730 --> 00:11:17,990
oh yeah so before I continue I just

00:11:16,250 --> 00:11:20,390
wanted to know like how many in this

00:11:17,990 --> 00:11:21,980
audience here using kubernetes right now

00:11:20,390 --> 00:11:22,760
for the services ok so that's that's

00:11:21,980 --> 00:11:25,220
most of you

00:11:22,760 --> 00:11:26,839
what about using using spinnaker it may

00:11:25,220 --> 00:11:29,089
be seem like a really stupid question at

00:11:26,839 --> 00:11:29,440
this point but but I don't know ok all

00:11:29,089 --> 00:11:32,600
right

00:11:29,440 --> 00:11:35,060
not everybody and who's using the v2

00:11:32,600 --> 00:11:39,890
provider already for the our case so

00:11:35,060 --> 00:11:43,430
there's a good number of you before I

00:11:39,890 --> 00:11:45,050
even talk about the deployments inside

00:11:43,430 --> 00:11:46,430
of spinnaker it seems like a worthwhile

00:11:45,050 --> 00:11:48,410
conversation let's talk about how we

00:11:46,430 --> 00:11:51,140
deploy spinnaker I've already found

00:11:48,410 --> 00:11:53,029
quite a few people that like I've had

00:11:51,140 --> 00:11:55,180
issues configuring spinnaker correctly

00:11:53,029 --> 00:11:57,710
and we built some automation around that

00:11:55,180 --> 00:12:00,620
it's you know it's not sophisticated at

00:11:57,710 --> 00:12:03,050
all but it's nice and reproducible so we

00:12:00,620 --> 00:12:04,580
have a pipeline that creates a spinnaker

00:12:03,050 --> 00:12:06,589
gke cluster

00:12:04,580 --> 00:12:08,480
I put the credentials or encryption in

00:12:06,589 --> 00:12:10,970
the repo only is like a confession that

00:12:08,480 --> 00:12:14,390
I feel terrible about but that's the

00:12:10,970 --> 00:12:16,820
model we use so this launches are how

00:12:14,390 --> 00:12:18,350
your deployment we defined a kubernetes

00:12:16,820 --> 00:12:20,660
deployment that has Howard in it

00:12:18,350 --> 00:12:23,750
launches it in the cluster and there any

00:12:20,660 --> 00:12:26,600
explicitly you know a bash at a bachelor

00:12:23,750 --> 00:12:28,100
ball here the coop cuddle copies this

00:12:26,600 --> 00:12:30,980
how are your dynamic configuration

00:12:28,100 --> 00:12:33,670
script that we have and they execute

00:12:30,980 --> 00:12:36,110
that script on the pod right it's it's

00:12:33,670 --> 00:12:38,300
it doesn't sound pretty but it works

00:12:36,110 --> 00:12:40,160
very well and actually leads me to this

00:12:38,300 --> 00:12:42,500
conversation of Yama's versus dynamic

00:12:40,160 --> 00:12:44,870
comfort so you know you could in theory

00:12:42,500 --> 00:12:47,990
configure you how he had pod capture all

00:12:44,870 --> 00:12:50,300
the configuration state and just like

00:12:47,990 --> 00:12:53,150
copy that state in or you know build a

00:12:50,300 --> 00:12:57,410
pod that has that state in it and build

00:12:53,150 --> 00:13:01,250
an image and we actually went with that

00:12:57,410 --> 00:13:02,690
model first and in fact what we were

00:13:01,250 --> 00:13:04,640
doing we weren't baked it into the image

00:13:02,690 --> 00:13:06,650
but we were putting it in GCS and then

00:13:04,640 --> 00:13:10,339
we were using the GCS fuse plug-in to

00:13:06,650 --> 00:13:12,770
expose that that config and then long

00:13:10,339 --> 00:13:14,100
story short we've actually had more

00:13:12,770 --> 00:13:16,110
stability

00:13:14,100 --> 00:13:17,430
we interacted with how he had to

00:13:16,110 --> 00:13:19,170
generate the config which we could of

00:13:17,430 --> 00:13:22,880
course do offline have best of both

00:13:19,170 --> 00:13:24,800
worlds then trying to persist the state

00:13:22,880 --> 00:13:27,329
hopefully make sense for everybody

00:13:24,800 --> 00:13:28,980
there's some customize customization we

00:13:27,329 --> 00:13:32,130
have to do and I don't know how much of

00:13:28,980 --> 00:13:35,089
this is necessary today but we have to

00:13:32,130 --> 00:13:38,130
customize our gate local yml because

00:13:35,089 --> 00:13:40,259
something about you know the Google load

00:13:38,130 --> 00:13:41,610
balancer going through our gateway ends

00:13:40,259 --> 00:13:44,100
up like some headers get stripped out

00:13:41,610 --> 00:13:46,889
it's been a tries to redirect to an HTTP

00:13:44,100 --> 00:13:49,079
endpoint otherwise and the other problem

00:13:46,889 --> 00:13:51,149
was if you used if you do set up the v1

00:13:49,079 --> 00:13:53,310
provider spinnaker used to be very

00:13:51,149 --> 00:13:56,310
aggressive about polling the registry

00:13:53,310 --> 00:13:59,190
and Google would like start spitting out

00:13:56,310 --> 00:14:00,329
49z hidden hidden the GCR too hard and

00:13:59,190 --> 00:14:02,250
then you end up in a kind of fun

00:14:00,329 --> 00:14:03,509
situation where your deployments can't

00:14:02,250 --> 00:14:06,930
start because they're not allowed to

00:14:03,509 --> 00:14:08,790
pull the pull the image so you can

00:14:06,930 --> 00:14:11,009
actually check you can change the cache

00:14:08,790 --> 00:14:15,600
interval which makes it less aggressive

00:14:11,009 --> 00:14:18,480
and and solves that problem so all the

00:14:15,600 --> 00:14:20,459
clusters that we manage are configured

00:14:18,480 --> 00:14:22,470
using the v2 providers all the clusters

00:14:20,459 --> 00:14:24,000
that we actually deploy our production

00:14:22,470 --> 00:14:26,970
infrastructures managed using the v2

00:14:24,000 --> 00:14:29,009
provider all our pipelines use v2 one

00:14:26,970 --> 00:14:31,319
thing that is worth mentioning is we use

00:14:29,009 --> 00:14:34,170
a special spinnaker service account with

00:14:31,319 --> 00:14:36,209
custom our back rolls and that is not

00:14:34,170 --> 00:14:37,860
handled by this deployment because that

00:14:36,209 --> 00:14:39,839
varies from cluster to cluster what do

00:14:37,860 --> 00:14:41,490
we want spinnaker to be able to see what

00:14:39,839 --> 00:14:43,560
namespace is doing spinnaker to be able

00:14:41,490 --> 00:14:45,300
to deploy into you know like we might

00:14:43,560 --> 00:14:47,310
not want you know the cube system

00:14:45,300 --> 00:14:49,410
namespace to be exposed to spinnaker for

00:14:47,310 --> 00:14:51,360
example so that's managed separately all

00:14:49,410 --> 00:14:52,889
of our deployment infrastructure for the

00:14:51,360 --> 00:14:54,810
different different clusters that we

00:14:52,889 --> 00:14:57,120
have they have custom customized

00:14:54,810 --> 00:14:58,680
spinnaker our back rolls actually in

00:14:57,120 --> 00:15:00,620
general they're almost all the same role

00:14:58,680 --> 00:15:04,139
but in theory they could all be changed

00:15:00,620 --> 00:15:06,540
so how you deploy spinnaker and like

00:15:04,139 --> 00:15:07,949
another maybe noteworthy thing is we

00:15:06,540 --> 00:15:10,050
deploy spinnaker to a cluster that's

00:15:07,949 --> 00:15:12,779
configured using a v1 provider and the

00:15:10,050 --> 00:15:16,139
reason for that is when we originally

00:15:12,779 --> 00:15:17,550
tried to do this the the v2 provider

00:15:16,139 --> 00:15:20,939
when we deployed spinnaker to it would

00:15:17,550 --> 00:15:22,620
never stabilize we we like put a little

00:15:20,939 --> 00:15:24,630
bit of offense trying to debug it but it

00:15:22,620 --> 00:15:25,170
just worked when we defined it as the v1

00:15:24,630 --> 00:15:27,720
provider

00:15:25,170 --> 00:15:30,600
I saw a medium post maybe like a month

00:15:27,720 --> 00:15:32,160
go where somebody said that they have to

00:15:30,600 --> 00:15:33,870
change the health check on one of the

00:15:32,160 --> 00:15:36,269
end points for the v2 provider for it to

00:15:33,870 --> 00:15:37,709
be healthy so it could be as simple as

00:15:36,269 --> 00:15:39,269
that or it might even be fixed

00:15:37,709 --> 00:15:41,970
the other thing our deployment script

00:15:39,269 --> 00:15:44,730
does is it configures all the pub sub

00:15:41,970 --> 00:15:47,009
topics for GCR so a few guys maybe you

00:15:44,730 --> 00:15:49,319
were all happily using this already with

00:15:47,009 --> 00:15:51,420
the v2 provider but unlike the v1

00:15:49,319 --> 00:15:52,889
provider which like polls your docker

00:15:51,420 --> 00:15:53,579
registry to find out what's available

00:15:52,889 --> 00:15:56,040
there

00:15:53,579 --> 00:15:58,769
the v2 provider relies on being notified

00:15:56,040 --> 00:16:01,019
there's a new image available and so the

00:15:58,769 --> 00:16:03,300
scheme that we use for that if he's sort

00:16:01,019 --> 00:16:05,370
of recommended scheme is we set up pub

00:16:03,300 --> 00:16:07,680
subtopics and when there's a push to GC

00:16:05,370 --> 00:16:11,009
our triggers pops up topic and spinnaker

00:16:07,680 --> 00:16:12,930
gets notified there's I threw together a

00:16:11,009 --> 00:16:17,459
kind of stripped out version of this

00:16:12,930 --> 00:16:18,779
script this just it is not pretty but

00:16:17,459 --> 00:16:20,329
it'll show you all that how your

00:16:18,779 --> 00:16:24,389
commands we do pretty much all of them

00:16:20,329 --> 00:16:26,910
oh and the other thing that I mention is

00:16:24,389 --> 00:16:30,509
you can see the the spinnaker pole that

00:16:26,910 --> 00:16:33,750
we deploy to so it's pretty small it's

00:16:30,509 --> 00:16:35,670
just two notes hymen GCP is constantly

00:16:33,750 --> 00:16:37,639
telling us that it's underutilized and

00:16:35,670 --> 00:16:39,689
that's despite the fact that we have

00:16:37,639 --> 00:16:42,180
what I think is a very high number of

00:16:39,689 --> 00:16:44,610
pods in our cluster and like I said the

00:16:42,180 --> 00:16:48,059
cluster you know sometimes sits at tens

00:16:44,610 --> 00:16:50,490
of thousands of you CPUs the only caveat

00:16:48,059 --> 00:16:54,000
is originally we were using a standard

00:16:50,490 --> 00:16:55,470
node type and then just recently we were

00:16:54,000 --> 00:16:58,860
running into a cloud driver out of

00:16:55,470 --> 00:17:00,540
memory error and that doesn't get that

00:16:58,860 --> 00:17:04,409
doesn't trip the kubernetes health

00:17:00,540 --> 00:17:06,539
checks so your spinnaker cluster stops

00:17:04,409 --> 00:17:08,189
working but none of the pods get

00:17:06,539 --> 00:17:10,380
restarted so we switch to high mam and

00:17:08,189 --> 00:17:12,000
it fix that problem again like your

00:17:10,380 --> 00:17:16,470
mileage may vary on this is what I

00:17:12,000 --> 00:17:20,850
should say all right so let's talk about

00:17:16,470 --> 00:17:22,949
what gke clusters we manage like really

00:17:20,850 --> 00:17:24,750
Jim Burley we're talking about just two

00:17:22,949 --> 00:17:26,870
clusters and now getting its like what

00:17:24,750 --> 00:17:30,659
is our deployment strategy looked like

00:17:26,870 --> 00:17:33,990
so platform our core platform is a

00:17:30,659 --> 00:17:36,809
single big gke cluster and it hosts the

00:17:33,990 --> 00:17:38,280
core take our labs platform stack so

00:17:36,809 --> 00:17:40,980
like all of those twelve services

00:17:38,280 --> 00:17:45,299
sitting there all of live traffic

00:17:40,980 --> 00:17:48,179
to that end point we do expose prod and

00:17:45,299 --> 00:17:50,220
dev and points the problem Pradas prod

00:17:48,179 --> 00:17:52,740
only in the context of this slide

00:17:50,220 --> 00:17:56,040
unfortunately prod can be a couple of

00:17:52,740 --> 00:17:58,620
different namespaces but that's where

00:17:56,040 --> 00:18:00,299
all dynamic traffic goes I'll just sort

00:17:58,620 --> 00:18:02,250
of give you a spoiler here that we don't

00:18:00,299 --> 00:18:05,179
actually have any real customer traffic

00:18:02,250 --> 00:18:08,250
go to dev it's just purely development

00:18:05,179 --> 00:18:10,919
all of our services get hit by external

00:18:08,250 --> 00:18:12,390
blackbox tests we wrote we put quite a

00:18:10,919 --> 00:18:15,980
bit of effort into making these tests

00:18:12,390 --> 00:18:18,419
pretty realistic and and thorough and

00:18:15,980 --> 00:18:21,530
those tests actually inform a lot of

00:18:18,419 --> 00:18:24,450
what we do then we have platform lab

00:18:21,530 --> 00:18:26,669
which is almost a complete replica of

00:18:24,450 --> 00:18:28,830
our platform cluster we do have it in

00:18:26,669 --> 00:18:30,330
the same region a project that made our

00:18:28,830 --> 00:18:33,090
lives easier but you know it's probably

00:18:30,330 --> 00:18:36,780
not ideal for doing that it could could

00:18:33,090 --> 00:18:39,330
be completely isolated typically like I

00:18:36,780 --> 00:18:40,770
said no dynamic traffic that can vary

00:18:39,330 --> 00:18:42,630
but it's only for like special

00:18:40,770 --> 00:18:47,160
experiments that the SRA team doesn't

00:18:42,630 --> 00:18:49,140
like and their services inside that

00:18:47,160 --> 00:18:51,299
cluster are hit by an internal version

00:18:49,140 --> 00:18:55,710
of the same black box tester that hits a

00:18:51,299 --> 00:18:58,500
public cluster we do have like I put

00:18:55,710 --> 00:19:00,059
that asterisks because we have a service

00:18:58,500 --> 00:19:01,679
that talks to the kubernetes control

00:19:00,059 --> 00:19:04,710
pain plane and it has the ability to

00:19:01,679 --> 00:19:08,660
spawn user models as they execute in

00:19:04,710 --> 00:19:11,400
separate clusters but we don't have any

00:19:08,660 --> 00:19:15,150
infrastructure around deploying to those

00:19:11,400 --> 00:19:17,070
clusters mm-hmm okay so what are the

00:19:15,150 --> 00:19:19,950
triggers what is our deployment workflow

00:19:17,070 --> 00:19:22,910
look like a developer makes a or maybe

00:19:19,950 --> 00:19:25,740
let's say they get a PR merged in github

00:19:22,910 --> 00:19:27,210
github calls a couple of web hooks tells

00:19:25,740 --> 00:19:29,490
droned that there's something new to

00:19:27,210 --> 00:19:31,169
build tell spinnaker there was just

00:19:29,490 --> 00:19:36,540
maybe a change to a manifest at the same

00:19:31,169 --> 00:19:37,799
time that's a drone runs basil builds a

00:19:36,540 --> 00:19:39,570
bunch of images pushing them to

00:19:37,799 --> 00:19:41,700
container registry triggers the pub so

00:19:39,570 --> 00:19:43,080
that tells spinnaker oh there's also a

00:19:41,700 --> 00:19:44,700
new image so it knows has been changed

00:19:43,080 --> 00:19:46,620
to manifest maybe an attention image

00:19:44,700 --> 00:19:48,780
ideally they'd be decoupled it's just

00:19:46,620 --> 00:19:51,120
like that two possible paths right and

00:19:48,780 --> 00:19:52,030
then spinnaker like I said it deploys to

00:19:51,120 --> 00:19:57,580
platform

00:19:52,030 --> 00:19:59,080
or platform ok so now I'm gonna talk

00:19:57,580 --> 00:20:01,000
about some of the spinnaker pipelines we

00:19:59,080 --> 00:20:01,600
have maybe that's the reason you're all

00:20:01,000 --> 00:20:03,850
here

00:20:01,600 --> 00:20:06,340
hope it's not disappointing so these

00:20:03,850 --> 00:20:10,860
these are not like full pipelines

00:20:06,340 --> 00:20:13,270
they're most of the logic so we have

00:20:10,860 --> 00:20:15,160
ICBC platform lab and platform and

00:20:13,270 --> 00:20:16,900
there's this nice like annotation it

00:20:15,160 --> 00:20:19,240
might be specific to the v2 provider I'm

00:20:16,900 --> 00:20:22,840
not sure that shows you the specific

00:20:19,240 --> 00:20:24,700
kubernetes cluster you're targeting so

00:20:22,840 --> 00:20:27,220
we have like will deploy to platform lab

00:20:24,700 --> 00:20:28,990
will deploy to platform tasks here as a

00:20:27,220 --> 00:20:31,750
random service I picked for this fun

00:20:28,990 --> 00:20:34,720
experiment and then deploy the latest

00:20:31,750 --> 00:20:37,900
image to the dev environment and then we

00:20:34,720 --> 00:20:40,480
have a not often used for probably

00:20:37,900 --> 00:20:43,960
obvious reasons deploy tasks from

00:20:40,480 --> 00:20:47,680
feature branch to platform lab but it's

00:20:43,960 --> 00:20:50,710
there if you need it ok so what does the

00:20:47,680 --> 00:20:52,690
deployment looked like and this is this

00:20:50,710 --> 00:20:53,800
is you know probably most interesting if

00:20:52,690 --> 00:20:56,800
you're thinking about setting up these

00:20:53,800 --> 00:20:59,260
pipelines so this is the configuration

00:20:56,800 --> 00:21:01,750
stage here this is how we define

00:20:59,260 --> 00:21:04,420
basically what we want to trigger

00:21:01,750 --> 00:21:06,400
deployments off of from our github repo

00:21:04,420 --> 00:21:07,960
so you can here see here we've set up

00:21:06,400 --> 00:21:12,490
that puppet secret is obviously not real

00:21:07,960 --> 00:21:14,560
you can see here we set up you know like

00:21:12,490 --> 00:21:16,360
a configuration to watch I'm on a repo

00:21:14,560 --> 00:21:18,940
on the master branch and if there's a

00:21:16,360 --> 00:21:23,830
change to the deployment yeah mall or

00:21:18,940 --> 00:21:25,780
the config map for platform lab I should

00:21:23,830 --> 00:21:28,420
explain that in a minute it'll trigger a

00:21:25,780 --> 00:21:30,310
deployment and then over on the right

00:21:28,420 --> 00:21:32,590
hand side we configure expected

00:21:30,310 --> 00:21:34,690
artifacts for this this stage of the

00:21:32,590 --> 00:21:35,800
pipeline so basically we say if you

00:21:34,690 --> 00:21:37,660
change the config map that the

00:21:35,800 --> 00:21:39,640
deployment is not in the context then

00:21:37,660 --> 00:21:41,110
you need to go and get it and vice versa

00:21:39,640 --> 00:21:43,120
if you change the deployment config map

00:21:41,110 --> 00:21:44,770
style context don't get it change both

00:21:43,120 --> 00:21:50,980
from the same time that's fine my screen

00:21:44,770 --> 00:21:52,300
just met ok so oh yeah one thing I

00:21:50,980 --> 00:21:54,610
should mention is we try and make

00:21:52,300 --> 00:21:57,100
platform lab look exactly like platform

00:21:54,610 --> 00:21:59,580
in every respect and it almost does

00:21:57,100 --> 00:22:02,020
there are just a couple of exceptions

00:21:59,580 --> 00:22:04,720
those exceptions are why in this case

00:22:02,020 --> 00:22:05,530
for this service there is a custom

00:22:04,720 --> 00:22:07,870
config

00:22:05,530 --> 00:22:09,610
for this particular environment that's

00:22:07,870 --> 00:22:11,350
how we're differentiating like that end

00:22:09,610 --> 00:22:14,170
point that the service is hitting for a

00:22:11,350 --> 00:22:17,230
debugger or something like that ideally

00:22:14,170 --> 00:22:20,590
there would not be a cluster specific

00:22:17,230 --> 00:22:22,120
differentiator on the config map okay so

00:22:20,590 --> 00:22:24,280
that's configuration that defines how

00:22:22,120 --> 00:22:26,140
things get kicked off or you could

00:22:24,280 --> 00:22:28,030
always obviously manually execute and

00:22:26,140 --> 00:22:31,540
then there's no artifacts in the context

00:22:28,030 --> 00:22:33,550
you'll go and get both of them then we

00:22:31,540 --> 00:22:36,370
have the first stage the deploy config

00:22:33,550 --> 00:22:38,620
map to in platform lab stage and this is

00:22:36,370 --> 00:22:40,810
what I'm discussing because here

00:22:38,620 --> 00:22:43,060
spinnaker behaves a bit weird if you're

00:22:40,810 --> 00:22:44,080
like a kubernetes manifest person and

00:22:43,060 --> 00:22:46,060
you like how that works

00:22:44,080 --> 00:22:48,520
given a spinnaker does something that

00:22:46,060 --> 00:22:50,940
that makes total sense but you might not

00:22:48,520 --> 00:22:53,830
expect so when you deploy the config map

00:22:50,940 --> 00:22:55,720
spinnaker will it'll get the manifest

00:22:53,830 --> 00:22:57,850
it's in the context and it will evaluate

00:22:55,720 --> 00:23:00,340
the you know what the deployed config

00:22:57,850 --> 00:23:02,890
maps are already and it will explicitly

00:23:00,340 --> 00:23:05,290
version your config map that gets

00:23:02,890 --> 00:23:07,750
deployed and maybe you guys know all

00:23:05,290 --> 00:23:10,750
about this but it's very important to

00:23:07,750 --> 00:23:13,240
know and understand so it gets versioned

00:23:10,750 --> 00:23:15,430
it gets deployed as a specific version

00:23:13,240 --> 00:23:17,950
as you can see on the right hand side

00:23:15,430 --> 00:23:20,770
maybe you could see there's like task

00:23:17,950 --> 00:23:23,020
config - V zero zero five and if you've

00:23:20,770 --> 00:23:25,150
ever used the v1 provider they use that

00:23:23,020 --> 00:23:29,560
same kind of versioning schema for lots

00:23:25,150 --> 00:23:32,740
of kubernetes objects and then then what

00:23:29,560 --> 00:23:35,020
happens is the conflict map the the name

00:23:32,740 --> 00:23:36,790
of the config map gets bound to the

00:23:35,020 --> 00:23:39,610
deployment manifest in the next stage so

00:23:36,790 --> 00:23:42,190
it overrides the reference to the config

00:23:39,610 --> 00:23:46,360
map with the version reference that it

00:23:42,190 --> 00:23:48,100
just just created and this means that

00:23:46,360 --> 00:23:49,540
your manifests are no longer a source of

00:23:48,100 --> 00:23:50,920
truth which is one of the main reasons

00:23:49,540 --> 00:23:52,600
why I'm doing this in the first place

00:23:50,920 --> 00:23:56,110
like using this particular provider I

00:23:52,600 --> 00:23:59,140
very much like internally my repo

00:23:56,110 --> 00:24:00,760
reflecting exactly what's deployed but

00:23:59,140 --> 00:24:02,740
it means that you have this great

00:24:00,760 --> 00:24:04,810
consistency in your deployments you

00:24:02,740 --> 00:24:06,910
never end up in a situation where like

00:24:04,810 --> 00:24:08,500
your config map changes underneath your

00:24:06,910 --> 00:24:10,210
deployment and maybe a pod has been

00:24:08,500 --> 00:24:12,220
restarted it's picked up the new

00:24:10,210 --> 00:24:14,200
environment or maybe it hasn't you don't

00:24:12,220 --> 00:24:16,060
know you never end up in that situation

00:24:14,200 --> 00:24:18,429
it was like explicitly bound together

00:24:16,060 --> 00:24:19,179
and it also makes the roll forward and

00:24:18,429 --> 00:24:20,859
roll back

00:24:19,179 --> 00:24:23,529
strategies very well defined because

00:24:20,859 --> 00:24:28,929
they're very again explicitly bound to

00:24:23,529 --> 00:24:30,759
each other okay so when you actually

00:24:28,929 --> 00:24:33,580
make a deployment of your service

00:24:30,759 --> 00:24:36,460
it's just kubernetes a regular rolling

00:24:33,580 --> 00:24:38,169
update you can you can configure this

00:24:36,460 --> 00:24:39,879
you know the strategy like in your

00:24:38,169 --> 00:24:42,549
manifest or define a pod disruption

00:24:39,879 --> 00:24:44,019
budget it will deploy accordingly it's

00:24:42,549 --> 00:24:45,700
been a cool sit there and wait for the

00:24:44,019 --> 00:24:48,460
deployment to roll out all the way and

00:24:45,700 --> 00:24:49,869
stabilize and and if it doesn't it's

00:24:48,460 --> 00:24:51,789
very key tells you that the the

00:24:49,869 --> 00:24:54,789
deployment failed and it can execute

00:24:51,789 --> 00:24:56,109
other logic if desired but otherwise

00:24:54,789 --> 00:25:04,599
they will just sit here with a failed

00:24:56,109 --> 00:25:06,909
pipeline okay so yeah it's on the

00:25:04,599 --> 00:25:10,929
right-hand side in that box you can see

00:25:06,909 --> 00:25:13,089
the ammo and the details links the UI is

00:25:10,929 --> 00:25:15,909
a bit spotty for me sometimes the

00:25:13,089 --> 00:25:18,489
details button doesn't doesn't work

00:25:15,909 --> 00:25:21,070
correctly but when you click the ammo

00:25:18,489 --> 00:25:22,839
that's nice because you actually see the

00:25:21,070 --> 00:25:24,519
ammo with all the bound objects via

00:25:22,839 --> 00:25:26,019
config map or later and you've talked

00:25:24,519 --> 00:25:27,639
about images getting bounced oh you see

00:25:26,019 --> 00:25:29,889
the actual deployed configuration with

00:25:27,639 --> 00:25:32,200
all of the annotations that spinnaker

00:25:29,889 --> 00:25:36,309
injects if you click on the details tab

00:25:32,200 --> 00:25:37,809
it takes you to this view and this view

00:25:36,309 --> 00:25:41,349
again if you're like coming from

00:25:37,809 --> 00:25:42,999
kubernetes world it's a it's a little

00:25:41,349 --> 00:25:44,499
confusing because at the top of this

00:25:42,999 --> 00:25:45,789
they have like a coster's tab where

00:25:44,499 --> 00:25:50,440
clusters are not your kubernetes

00:25:45,789 --> 00:25:52,749
clusters clusters represent replicas

00:25:50,440 --> 00:25:55,570
sets and your server group represents

00:25:52,749 --> 00:25:56,940
your deployment it's a it's it takes a

00:25:55,570 --> 00:26:01,539
little while to get used to like the

00:25:56,940 --> 00:26:03,279
naming naming pattern

00:26:01,539 --> 00:26:04,839
unfortunately this side on the Left

00:26:03,279 --> 00:26:06,999
which is showing that deployment that

00:26:04,839 --> 00:26:09,099
just rolled out that the right hand side

00:26:06,999 --> 00:26:11,019
reflects that does not automatically

00:26:09,099 --> 00:26:12,669
filter that's something I would love if

00:26:11,019 --> 00:26:14,559
there any spinnaker developers in the

00:26:12,669 --> 00:26:17,139
room I'd really love to see like the

00:26:14,559 --> 00:26:18,279
filtered view of the deployment on the

00:26:17,139 --> 00:26:20,349
left hand side rather than having to

00:26:18,279 --> 00:26:23,109
find it myself but on the right hand

00:26:20,349 --> 00:26:24,700
side you can see see the deployment and

00:26:23,109 --> 00:26:27,639
what you could do with that server group

00:26:24,700 --> 00:26:29,440
manager is you can undo a rollout if you

00:26:27,639 --> 00:26:31,360
want you can you can just undo the

00:26:29,440 --> 00:26:34,480
rollout you just did

00:26:31,360 --> 00:26:38,680
and you can do other stuff like scale we

00:26:34,480 --> 00:26:40,240
never do that as a developer and I guess

00:26:38,680 --> 00:26:42,760
it's worth mentioning our developers are

00:26:40,240 --> 00:26:44,980
encouraged to modify these pipelines

00:26:42,760 --> 00:26:47,590
like we we add the three-man SRE team

00:26:44,980 --> 00:26:49,270
doesn't want to be responsible for for

00:26:47,590 --> 00:26:50,740
maintaining pipelines and that model

00:26:49,270 --> 00:26:53,200
like developers doing the hard work that

00:26:50,740 --> 00:26:54,670
doesn't work very well but in theory

00:26:53,200 --> 00:26:57,520
that that's something we would like to

00:26:54,670 --> 00:26:59,950
have the developers they come from this

00:26:57,520 --> 00:27:02,650
user interface they can click on any one

00:26:59,950 --> 00:27:04,150
of these green rectangles from there

00:27:02,650 --> 00:27:06,460
they populate on the right-hand side

00:27:04,150 --> 00:27:08,500
information about that specific pot and

00:27:06,460 --> 00:27:10,540
if they scroll down to the bottom they

00:27:08,500 --> 00:27:12,580
can view the logs for the pod all this

00:27:10,540 --> 00:27:14,700
means is they can try and debug the

00:27:12,580 --> 00:27:18,700
service without having any gke

00:27:14,700 --> 00:27:23,980
permissions right which is nice in

00:27:18,700 --> 00:27:27,310
theory so let's say we did a deployment

00:27:23,980 --> 00:27:29,380
Oh actually yeah so the point of this

00:27:27,310 --> 00:27:31,150
was the first part of our deployment

00:27:29,380 --> 00:27:33,010
stage deploys to this environment called

00:27:31,150 --> 00:27:34,510
platform lab and I already tell it told

00:27:33,010 --> 00:27:37,480
you we don't have any production traffic

00:27:34,510 --> 00:27:41,140
there there's no live traffic at this

00:27:37,480 --> 00:27:43,090
point what happens is the the service

00:27:41,140 --> 00:27:45,250
stabilizes and platform labs spinnaker

00:27:43,090 --> 00:27:47,740
blues on to the next stage which today

00:27:45,250 --> 00:27:49,480
is a manual validation stage but I think

00:27:47,740 --> 00:27:52,090
in a week it's going to be automated I'm

00:27:49,480 --> 00:27:54,820
so excited about that but right now it's

00:27:52,090 --> 00:27:56,980
a manual validation stage so here you're

00:27:54,820 --> 00:27:58,660
presented with a view monitor health of

00:27:56,980 --> 00:28:01,060
tasks deployment and platform Alvin if

00:27:58,660 --> 00:28:03,370
you click on that view it takes you to

00:28:01,060 --> 00:28:05,520
the graph on a chart that you see below

00:28:03,370 --> 00:28:07,900
the final chart here reflects

00:28:05,520 --> 00:28:10,690
information that's presented from the

00:28:07,900 --> 00:28:13,480
sto service mesh like default set of

00:28:10,690 --> 00:28:16,990
dashboards I should have asked who here

00:28:13,480 --> 00:28:18,700
knows about sto that that's great

00:28:16,990 --> 00:28:20,710
because I met plenty of people who have

00:28:18,700 --> 00:28:24,280
never heard of it at this conference and

00:28:20,710 --> 00:28:25,960
to me like everyone I thought everybody

00:28:24,280 --> 00:28:28,000
was excited about it I accidentally

00:28:25,960 --> 00:28:30,160
become this huge evangelist of ISTE oh I

00:28:28,000 --> 00:28:32,980
think I've convinced dozens of people to

00:28:30,160 --> 00:28:36,760
use it but anyway sto is this really

00:28:32,980 --> 00:28:38,380
lovely service mesh that you know it's

00:28:36,760 --> 00:28:41,410
it got a really nice integration with

00:28:38,380 --> 00:28:43,510
kubernetes you can just have it it takes

00:28:41,410 --> 00:28:44,920
a little work but basically you can have

00:28:43,510 --> 00:28:47,200
it set up so the

00:28:44,920 --> 00:28:49,030
sto sigh cars get deployed with every

00:28:47,200 --> 00:28:50,140
one of your services when they come up

00:28:49,030 --> 00:28:52,210
and all your service traffic goes

00:28:50,140 --> 00:28:53,890
through the side cars and all of our

00:28:52,210 --> 00:28:56,800
production services at Descartes labs

00:28:53,890 --> 00:28:58,060
you are behind sto there all of them all

00:28:56,800 --> 00:28:59,950
the traffic has to go through the

00:28:58,060 --> 00:29:03,580
service mesh to CRC our production

00:28:59,950 --> 00:29:04,900
services so they click on this link and

00:29:03,580 --> 00:29:07,390
here I'm a developer I just did a

00:29:04,900 --> 00:29:08,860
deployment at the platform lab I've been

00:29:07,390 --> 00:29:11,260
pretty quick about clicking on that link

00:29:08,860 --> 00:29:12,700
and I can see from this graph on a chart

00:29:11,260 --> 00:29:13,720
and this isn't a great way to do it like

00:29:12,700 --> 00:29:16,840
with two steps away from having

00:29:13,720 --> 00:29:19,360
automation here that I'll the number of

00:29:16,840 --> 00:29:20,980
non 5xx responses is just full it off

00:29:19,360 --> 00:29:23,800
really quickly I could see the request

00:29:20,980 --> 00:29:26,350
duration just as a minute ago has spiked

00:29:23,800 --> 00:29:28,930
I've done something pretty wrong with

00:29:26,350 --> 00:29:31,810
this deployment and so I can you know I

00:29:28,930 --> 00:29:33,780
can stop and execute other logic yeah so

00:29:31,810 --> 00:29:35,920
this is right now like the most

00:29:33,780 --> 00:29:37,570
empowerment that we provide for people

00:29:35,920 --> 00:29:38,770
to make decisions about whether or not

00:29:37,570 --> 00:29:43,150
to deploy stuff in our production

00:29:38,770 --> 00:29:45,910
environment the conditional expression

00:29:43,150 --> 00:29:48,730
there it's kind of just I threw it in

00:29:45,910 --> 00:29:51,010
there because in the pipeline I showed

00:29:48,730 --> 00:29:53,140
you it only triggers off of master but

00:29:51,010 --> 00:29:54,610
you could have a pipeline like this that

00:29:53,140 --> 00:29:57,580
triggers off of you know other things

00:29:54,610 --> 00:29:59,470
and here one thing you can do to just

00:29:57,580 --> 00:30:01,540
make sure that only the merges into

00:29:59,470 --> 00:30:03,820
master get deployed to production is

00:30:01,540 --> 00:30:06,670
with some very simple spell you can just

00:30:03,820 --> 00:30:08,680
validate what branch the commit came

00:30:06,670 --> 00:30:10,540
from so I could say was it was it from

00:30:08,680 --> 00:30:12,340
master verse from master let's keep

00:30:10,540 --> 00:30:13,600
going with this pipeline otherwise we're

00:30:12,340 --> 00:30:18,480
going to stop right here we don't go

00:30:13,600 --> 00:30:18,480
past platform lab okay that make sense

00:30:18,810 --> 00:30:24,760
okay all right so another deployment

00:30:22,350 --> 00:30:28,540
pipeline that I showed you here was the

00:30:24,760 --> 00:30:30,910
deploy latest image to death and that's

00:30:28,540 --> 00:30:32,560
a really simple pipeline but it's quite

00:30:30,910 --> 00:30:37,120
useful and it's kind of hard to get

00:30:32,560 --> 00:30:39,820
right well it's sort of awkward all this

00:30:37,120 --> 00:30:42,460
does is whenever there's a push to GCR

00:30:39,820 --> 00:30:45,340
for the service at all deploy in a they

00:30:42,460 --> 00:30:48,690
have namespace in our production cluster

00:30:45,340 --> 00:30:52,240
and devastate that through the gateway

00:30:48,690 --> 00:30:53,910
so the way you configure this is a

00:30:52,240 --> 00:30:56,460
little different too

00:30:53,910 --> 00:30:57,900
configuration but basically on the right

00:30:56,460 --> 00:31:00,870
hand side I've got the automated trigger

00:30:57,900 --> 00:31:04,350
I'm saying in this case and I'm using a

00:31:00,870 --> 00:31:06,390
Google pub/sub system the subscription

00:31:04,350 --> 00:31:08,750
name got dynamically generated as part

00:31:06,390 --> 00:31:13,260
of that script I linked to earlier and

00:31:08,750 --> 00:31:15,000
the the rest of it like payload

00:31:13,260 --> 00:31:16,800
constraints and attribute constraints we

00:31:15,000 --> 00:31:19,290
don't usually have to set for any reason

00:31:16,800 --> 00:31:21,150
and then just at the bottom so on the

00:31:19,290 --> 00:31:22,920
Left we define the expected artifact and

00:31:21,150 --> 00:31:25,320
that says what specific image we were

00:31:22,920 --> 00:31:27,200
waiting for and then on the bottom right

00:31:25,320 --> 00:31:29,820
we just say yep that's what we need

00:31:27,200 --> 00:31:31,290
every time there's a push of that image

00:31:29,820 --> 00:31:34,230
and triggers this pipeline and this

00:31:31,290 --> 00:31:35,490
pipeline this pipeline deploys to death

00:31:34,230 --> 00:31:37,560
okay I should do it there is something

00:31:35,490 --> 00:31:39,780
importantly okay so what this pipeline

00:31:37,560 --> 00:31:42,060
does is this pipeline has a patch

00:31:39,780 --> 00:31:44,340
manifest stage another thing that's kind

00:31:42,060 --> 00:31:46,710
of fiddly to get right so the patch

00:31:44,340 --> 00:31:49,020
manifest stages we're saying you've just

00:31:46,710 --> 00:31:50,340
got a new image reference it's in it's

00:31:49,020 --> 00:31:52,710
in the it's in the context of this

00:31:50,340 --> 00:31:54,180
pipeline executing we now want to find a

00:31:52,710 --> 00:31:56,220
resource that's deployed in our dev

00:31:54,180 --> 00:31:59,250
namespace and our production cluster and

00:31:56,220 --> 00:32:02,100
here just for like simplicity sake I put

00:31:59,250 --> 00:32:04,230
it as a text and text sort manifest

00:32:02,100 --> 00:32:06,540
source and this is basically just

00:32:04,230 --> 00:32:11,130
patching okay in fact what happens is

00:32:06,540 --> 00:32:13,440
that that that US GTR lui task is just

00:32:11,130 --> 00:32:16,500
like a handle and that gets overridden

00:32:13,440 --> 00:32:18,120
by the bound image that's always present

00:32:16,500 --> 00:32:19,620
in the context of this executing

00:32:18,120 --> 00:32:20,870
pipeline does that make sense to

00:32:19,620 --> 00:32:26,940
everybody

00:32:20,870 --> 00:32:28,850
okay so oh god if it doesn't work it is

00:32:26,940 --> 00:32:31,200
super hard to find out why you get

00:32:28,850 --> 00:32:33,120
errors like that and that's not even

00:32:31,200 --> 00:32:34,800
Spinnaker's felt that's kubernetes at

00:32:33,120 --> 00:32:37,590
coop kernel that's the quality of

00:32:34,800 --> 00:32:39,180
response you get what I will say is you

00:32:37,590 --> 00:32:41,370
know you can patch from the command line

00:32:39,180 --> 00:32:42,810
if you're trying to build out this patch

00:32:41,370 --> 00:32:44,910
logic and you want to make sure it does

00:32:42,810 --> 00:32:46,620
match against the deployed manifest do

00:32:44,910 --> 00:32:49,440
it offline tend to it in spirit good

00:32:46,620 --> 00:32:50,790
just do all this the CLI iterate a bunch

00:32:49,440 --> 00:32:52,080
and when you've when you build that

00:32:50,790 --> 00:32:53,310
figured out now you're ready to actually

00:32:52,080 --> 00:32:58,730
create the spinnaker pipeline because

00:32:53,310 --> 00:32:58,730
it's a it's no fun trying to debug

00:32:59,630 --> 00:33:04,250
once that deployment actually happens

00:33:02,690 --> 00:33:06,050
it's just like your regular deployment

00:33:04,250 --> 00:33:09,260
of your manifests you just did it does a

00:33:06,050 --> 00:33:11,180
rolling deployment you get you know ya

00:33:09,260 --> 00:33:13,070
know and details click on e llamo and it

00:33:11,180 --> 00:33:19,400
shows the right image that just got

00:33:13,070 --> 00:33:21,920
deployed yeah oh one thing one thing

00:33:19,400 --> 00:33:25,010
that I didn't notice until preparing for

00:33:21,920 --> 00:33:27,950
this talk is the status tab always

00:33:25,010 --> 00:33:30,530
reflects the current status they last

00:33:27,950 --> 00:33:33,440
mentioned that's being decided that it's

00:33:30,530 --> 00:33:35,000
not a good UI future so if you look at a

00:33:33,440 --> 00:33:36,950
historical pipeline you view the status

00:33:35,000 --> 00:33:38,270
it's not going to show the status of

00:33:36,950 --> 00:33:40,610
that deployment it's going to show the

00:33:38,270 --> 00:33:43,190
status of the latest deployment the poll

00:33:40,610 --> 00:33:47,570
pulls it as you click on that

00:33:43,190 --> 00:33:49,100
effectively okay so that basically is it

00:33:47,570 --> 00:33:50,780
in terms of deployment strategies I've

00:33:49,100 --> 00:33:53,030
left off all the stuff you probably

00:33:50,780 --> 00:33:55,700
wanted to hear about like you know how

00:33:53,030 --> 00:33:58,010
we do like how we could do or sort of

00:33:55,700 --> 00:33:59,570
automated rollback see deployment one

00:33:58,010 --> 00:34:01,160
thing that I was debating whether or not

00:33:59,570 --> 00:34:03,260
to include in this talk is because we

00:34:01,160 --> 00:34:07,100
use sto because we got virtual services

00:34:03,260 --> 00:34:10,060
service entries and whatnot we have been

00:34:07,100 --> 00:34:13,970
exploring at just an experimental stage

00:34:10,060 --> 00:34:16,550
the using virtual services for Canaries

00:34:13,970 --> 00:34:18,680
and I don't know if you guys saw it but

00:34:16,550 --> 00:34:21,890
there was a talk on that yesterday the

00:34:18,680 --> 00:34:24,590
sto spinnaker communities v2 cooling it

00:34:21,890 --> 00:34:26,270
together it's by the cubular guys but if

00:34:24,590 --> 00:34:27,560
you're interested in that they I think

00:34:26,270 --> 00:34:31,490
they have some stuff online and can

00:34:27,560 --> 00:34:33,710
point you to tips is really random here

00:34:31,490 --> 00:34:36,860
it's just stuff that was painful and I

00:34:33,710 --> 00:34:38,390
felt like sharing so yeah if you want to

00:34:36,860 --> 00:34:41,480
work on spell and you want to see like

00:34:38,390 --> 00:34:44,090
the automated population of fields that

00:34:41,480 --> 00:34:46,880
are accessible through spell you have to

00:34:44,090 --> 00:34:48,320
do it in the manual or I can't find any

00:34:46,880 --> 00:34:51,590
other way to do it than the manual

00:34:48,320 --> 00:34:53,720
judgment configuration stage right this

00:34:51,590 --> 00:34:55,820
is not useful for you know like usually

00:34:53,720 --> 00:34:58,490
I don't really need to evaluate stuff in

00:34:55,820 --> 00:35:01,340
this field but what I can do is I can

00:34:58,490 --> 00:35:02,870
bring this up explore like the

00:35:01,340 --> 00:35:04,100
population of values go okay that's

00:35:02,870 --> 00:35:06,200
actually the the field that I

00:35:04,100 --> 00:35:08,000
specifically wanted and then copy and

00:35:06,200 --> 00:35:11,980
paste the spell I generated into the

00:35:08,000 --> 00:35:11,980
stage that needs it for real

00:35:12,869 --> 00:35:19,200
one really useful thing we did by

00:35:16,170 --> 00:35:22,200
accident was made a stack driver alert

00:35:19,200 --> 00:35:25,260
the triggers when there's an update to

00:35:22,200 --> 00:35:27,540
kubernetes deployments and we did this

00:35:25,260 --> 00:35:28,859
just so we know before we actually had

00:35:27,540 --> 00:35:30,930
all of the spinnaker stuff fleshed out

00:35:28,859 --> 00:35:35,700
so we would know when developers were

00:35:30,930 --> 00:35:37,140
making updates to services and for

00:35:35,700 --> 00:35:39,720
whatever reason I think because

00:35:37,140 --> 00:35:42,839
kubernetes like patch manifest logic

00:35:39,720 --> 00:35:44,339
this particular filter is not triggered

00:35:42,839 --> 00:35:47,220
by spinnaker deployments but it is

00:35:44,339 --> 00:35:49,650
generally triggered by users cue

00:35:47,220 --> 00:35:51,150
cuddling or using a console or something

00:35:49,650 --> 00:35:54,210
so this is a great way to actually

00:35:51,150 --> 00:35:56,220
identify pain points for your developers

00:35:54,210 --> 00:35:58,380
that's what it accidentally became like

00:35:56,220 --> 00:36:00,990
we get alerts somebody has been

00:35:58,380 --> 00:36:02,640
deploying manually you can see it even

00:36:00,990 --> 00:36:04,079
tells you who did the offensive

00:36:02,640 --> 00:36:05,970
deployment there's my name right there

00:36:04,079 --> 00:36:08,130
and you can contact them and say hey

00:36:05,970 --> 00:36:09,750
what you know what's going on why why

00:36:08,130 --> 00:36:12,390
aren't you using spinnaker for this what

00:36:09,750 --> 00:36:16,200
where are we failing you and and it's

00:36:12,390 --> 00:36:18,390
been very helpful actually this is

00:36:16,200 --> 00:36:22,650
another random tip how many of you guys

00:36:18,390 --> 00:36:25,530
have seen this error all right that that

00:36:22,650 --> 00:36:28,589
error is the bane of my life that error

00:36:25,530 --> 00:36:30,540
means that you've configured actually it

00:36:28,589 --> 00:36:32,190
doesn't always mean that you could write

00:36:30,540 --> 00:36:35,130
nine out of ten times that means that

00:36:32,190 --> 00:36:37,380
your reference to like the the UI URI

00:36:35,130 --> 00:36:40,859
endpoint per default artifact in github

00:36:37,380 --> 00:36:43,880
is wrong and you can figure that out if

00:36:40,859 --> 00:36:46,770
you look at the stack driver logs

00:36:43,880 --> 00:36:48,990
spinnaker logs but otherwise it's just

00:36:46,770 --> 00:36:51,510
like everybody knows this they got labs

00:36:48,990 --> 00:36:53,130
right I've seen retrofit 500 that means

00:36:51,510 --> 00:36:54,630
I messed up but I've got like the path

00:36:53,130 --> 00:36:56,490
wrong or something like that so this is

00:36:54,630 --> 00:36:59,490
really valuable Intel I'm giving you

00:36:56,490 --> 00:37:02,310
guys here the other thing we just ran

00:36:59,490 --> 00:37:03,869
into this this issue so the cluster name

00:37:02,310 --> 00:37:07,410
at the beginning of the execution

00:37:03,869 --> 00:37:09,960
pipeline it wasn't there up until a very

00:37:07,410 --> 00:37:12,180
recent version of kubernetes and when it

00:37:09,960 --> 00:37:14,550
popped up we had all these like really

00:37:12,180 --> 00:37:17,490
long highly verbose handles to our

00:37:14,550 --> 00:37:19,920
clusters and and the year I can deal

00:37:17,490 --> 00:37:22,110
with it and and we lost the ability to

00:37:19,920 --> 00:37:24,600
manually execute pipelines

00:37:22,110 --> 00:37:26,520
and then then we had to go through we

00:37:24,600 --> 00:37:28,740
had to like generate new handles and and

00:37:26,520 --> 00:37:30,210
we had to go through and sort of

00:37:28,740 --> 00:37:32,700
manually change the references which

00:37:30,210 --> 00:37:34,170
actually kind of brings me to a point

00:37:32,700 --> 00:37:37,710
I'll talk about in just a second

00:37:34,170 --> 00:37:38,910
but first so in conclusion all of our

00:37:37,710 --> 00:37:41,280
descartes lab services have been

00:37:38,910 --> 00:37:42,690
migrated to spinnaker and it's great how

00:37:41,280 --> 00:37:44,310
excited the developers are there's no

00:37:42,690 --> 00:37:46,020
even though like I said it's good to

00:37:44,310 --> 00:37:47,370
know when they've done a note it's

00:37:46,020 --> 00:37:49,260
actually called our ugly deployment

00:37:47,370 --> 00:37:51,450
detector detector it's good to know when

00:37:49,260 --> 00:37:52,890
they've done an ugly deployment most of

00:37:51,450 --> 00:37:55,170
them are really happy to have this in

00:37:52,890 --> 00:37:57,720
place the pace of the v2 provided

00:37:55,170 --> 00:37:59,370
development has been amazing since we

00:37:57,720 --> 00:38:01,320
started in fact I should say when I

00:37:59,370 --> 00:38:03,990
submitted the abstract I have like a

00:38:01,320 --> 00:38:06,330
dozen things that were like workarounds

00:38:03,990 --> 00:38:09,440
for issues with that provider that have

00:38:06,330 --> 00:38:11,820
pretty much all been taken care of now

00:38:09,440 --> 00:38:14,100
this does mean though as they add new

00:38:11,820 --> 00:38:15,660
features and as they fix bugs deployment

00:38:14,100 --> 00:38:17,280
pipelines are constantly in flux every

00:38:15,660 --> 00:38:18,990
time we make a new pipeline there's a

00:38:17,280 --> 00:38:20,130
better way to do it and that's partly

00:38:18,990 --> 00:38:23,010
because we're learning but it's also

00:38:20,130 --> 00:38:25,470
because the underlying to provider is

00:38:23,010 --> 00:38:27,960
changing and that does mean because we

00:38:25,470 --> 00:38:30,390
have manual worth doing we're creating

00:38:27,960 --> 00:38:32,040
our pipelines manually today that if you

00:38:30,390 --> 00:38:33,750
look at a service that's like not really

00:38:32,040 --> 00:38:35,580
hot and it's not getting deployed very

00:38:33,750 --> 00:38:37,530
frequently it's gonna have a completely

00:38:35,580 --> 00:38:38,760
different pipeline logic not completely

00:38:37,530 --> 00:38:40,620
but it's gonna look different to the

00:38:38,760 --> 00:38:43,350
developers nobody wants that

00:38:40,620 --> 00:38:45,060
so one thing you know with our

00:38:43,350 --> 00:38:48,030
automation around pipeline creation our

00:38:45,060 --> 00:38:49,740
pipelines get stale and we had tried a

00:38:48,030 --> 00:38:52,350
couple we've had a few pushes on

00:38:49,740 --> 00:38:53,970
pipeline automated pipeline creation it

00:38:52,350 --> 00:38:57,500
wasn't until this conference I really

00:38:53,970 --> 00:39:01,200
learned some great ways to do that and

00:38:57,500 --> 00:39:02,490
the other thing very similar but you

00:39:01,200 --> 00:39:04,470
know right now we're relying on our

00:39:02,490 --> 00:39:05,910
developers clicking that link if they're

00:39:04,470 --> 00:39:07,530
lucky actually because it's not even on

00:39:05,910 --> 00:39:09,120
all pipelines but clicking the link

00:39:07,530 --> 00:39:10,530
looking at the graph on a chart and

00:39:09,120 --> 00:39:11,760
making an informed decision about

00:39:10,530 --> 00:39:13,920
whether or not mean you want to promote

00:39:11,760 --> 00:39:16,440
this such production it does a work

00:39:13,920 --> 00:39:17,760
great like I'll come in on a Monday and

00:39:16,440 --> 00:39:20,040
there's been a deployment that's just

00:39:17,760 --> 00:39:21,870
hang in there for you know two days

00:39:20,040 --> 00:39:24,120
because the developer you know never

00:39:21,870 --> 00:39:25,500
never evaluated it it's the only city in

00:39:24,120 --> 00:39:27,540
our platform that cluster it's not the

00:39:25,500 --> 00:39:31,320
end of the world but you know it's not

00:39:27,540 --> 00:39:33,060
like nice high velocity deployments the

00:39:31,320 --> 00:39:34,820
other thing is there is like a whole

00:39:33,060 --> 00:39:36,500
world of possible deployments

00:39:34,820 --> 00:39:37,880
strategies with spinnaker and sto and

00:39:36,500 --> 00:39:39,200
it's kind of overwhelming thinking about

00:39:37,880 --> 00:39:42,620
the different approaches we could take

00:39:39,200 --> 00:39:45,590
for that I know that Lars and and Ethan

00:39:42,620 --> 00:39:47,210
and Co really care you know they don't

00:39:45,590 --> 00:39:49,370
want us to solve this problem they want

00:39:47,210 --> 00:39:50,960
to be able to solve it through spinnaker

00:39:49,370 --> 00:39:53,900
features like they'll have like built-in

00:39:50,960 --> 00:39:58,160
strategies that leverage sto features

00:39:53,900 --> 00:40:00,560
and I'm excited about that I just don't

00:39:58,160 --> 00:40:02,900
know how to like best participate in

00:40:00,560 --> 00:40:04,220
that conversation hopefully maybe this

00:40:02,900 --> 00:40:08,210
afternoon we can talk about it a little

00:40:04,220 --> 00:40:09,560
bit and and yet I can't believe we're

00:40:08,210 --> 00:40:11,570
not doing this already we all have all

00:40:09,560 --> 00:40:13,100
these great sto metrics that are getting

00:40:11,570 --> 00:40:15,800
sunk in to Prometheus you just have a

00:40:13,100 --> 00:40:17,840
look at like one dashboard so we really

00:40:15,800 --> 00:40:20,780
need automated analysis coming in like

00:40:17,840 --> 00:40:23,090
seems like I answer is is absolutely

00:40:20,780 --> 00:40:24,350
no-brainer and we just I guess partly

00:40:23,090 --> 00:40:25,850
because we're we don't have the

00:40:24,350 --> 00:40:28,330
resources we haven't quite appreciated

00:40:25,850 --> 00:40:28,330
there yet

00:40:28,450 --> 00:40:30,510

YouTube URL: https://www.youtube.com/watch?v=aNg4LmvgfFg


