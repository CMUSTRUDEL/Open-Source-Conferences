Title: Cameron Fieber  "Spinnaker in Production: Lessons Learned from the Trenches at Netflix"
Publication date: 2018-10-24
Playlist: Spinnaker Summit 2018
Description: 
	
Captions: 
	00:00:01,270 --> 00:00:05,380
[Music]

00:00:09,820 --> 00:00:13,259
[Music]

00:00:18,060 --> 00:00:25,170
so today I want to do a technical deep

00:00:21,119 --> 00:00:27,769
dive into how we operate spinnaker in

00:00:25,170 --> 00:00:30,449
production at Netflix and specifically

00:00:27,769 --> 00:00:33,360
dig into the features that we've added

00:00:30,449 --> 00:00:36,600
to the system that we're leveraging that

00:00:33,360 --> 00:00:41,430
are enabling us to run at the scale that

00:00:36,600 --> 00:00:44,040
we're running at so we have basically

00:00:41,430 --> 00:00:48,600
constant API traffic against our

00:00:44,040 --> 00:00:52,289
spinnaker instance that is driven by

00:00:48,600 --> 00:00:55,620
external systems and automation kind of

00:00:52,289 --> 00:00:58,590
all hours of the day which Peaks up

00:00:55,620 --> 00:01:05,100
during during sort of business hours

00:00:58,590 --> 00:01:06,630
additionally we we have users that get

00:01:05,100 --> 00:01:11,189
on the system sort of during working

00:01:06,630 --> 00:01:13,290
hours and sort of all told we peak

00:01:11,189 --> 00:01:17,990
somewhere over a hundred requests per

00:01:13,290 --> 00:01:17,990
second during during the day

00:01:20,680 --> 00:01:25,990
our deployment foot for foot print for

00:01:23,380 --> 00:01:29,940
spinnaker at Netflix is we have a single

00:01:25,990 --> 00:01:33,420
shared spinnaker that all the users use

00:01:29,940 --> 00:01:35,800
each service is deployed independently

00:01:33,420 --> 00:01:38,320
largely because we're developing and

00:01:35,800 --> 00:01:40,180
iterating on them but we do have also

00:01:38,320 --> 00:01:43,230
dedicated clusters for each micro

00:01:40,180 --> 00:01:45,490
service this lets us scale them up

00:01:43,230 --> 00:01:48,400
changing instance types to give more

00:01:45,490 --> 00:01:53,040
compute resources as well as scaling

00:01:48,400 --> 00:01:57,010
them out to add more to add more nodes

00:01:53,040 --> 00:01:59,920
but eventually no matter how much

00:01:57,010 --> 00:02:06,790
computer or nodes we give something we

00:01:59,920 --> 00:02:08,500
tend to run into bottlenecks so we can

00:02:06,790 --> 00:02:11,050
scale up and scale out our services but

00:02:08,500 --> 00:02:12,520
over the course of our sort of history

00:02:11,050 --> 00:02:19,060
of our deployment of spinnaker we've run

00:02:12,520 --> 00:02:22,750
into a number of bottlenecks in in gate

00:02:19,060 --> 00:02:25,000
our API service we've we've had poorly

00:02:22,750 --> 00:02:27,100
tuned historic spools that we've managed

00:02:25,000 --> 00:02:30,700
to saturate kind of preventing any

00:02:27,100 --> 00:02:34,000
requests from getting downstream in Orca

00:02:30,700 --> 00:02:37,120
or orchestration engine for a long time

00:02:34,000 --> 00:02:38,590
we had stateful stateful deploys where

00:02:37,120 --> 00:02:40,870
each deploy was stuck on a particular

00:02:38,590 --> 00:02:43,690
instance and we could overwhelm an

00:02:40,870 --> 00:02:44,580
instance by landing too many deploys on

00:02:43,690 --> 00:02:47,500
it

00:02:44,580 --> 00:02:52,030
additionally the persistence that backed

00:02:47,500 --> 00:02:54,459
our Orca instance was based in Redis and

00:02:52,030 --> 00:02:57,640
not great for the access patterns of

00:02:54,459 --> 00:02:59,230
that service needed and then cloud

00:02:57,640 --> 00:03:03,910
driver which is our cloud integration

00:02:59,230 --> 00:03:05,739
layer does a lot of work aggregating and

00:03:03,910 --> 00:03:06,850
sort of indexing the state of the cloud

00:03:05,739 --> 00:03:10,570
for all the different cloud providers

00:03:06,850 --> 00:03:13,570
and accounts that are configured as well

00:03:10,570 --> 00:03:17,160
as serving serving that aggregated model

00:03:13,570 --> 00:03:17,160
for its API

00:03:19,630 --> 00:03:24,890
so as we've been operating spinnaker

00:03:22,010 --> 00:03:28,520
we've managed to unclog a number of

00:03:24,890 --> 00:03:29,090
these bottlenecks Gate was relatively

00:03:28,520 --> 00:03:31,010
isn't he

00:03:29,090 --> 00:03:33,710
tuning and partitioning hystrix a little

00:03:31,010 --> 00:03:36,560
bit better Orca we undertook some very

00:03:33,710 --> 00:03:39,620
significant efforts we are connecting it

00:03:36,560 --> 00:03:41,660
into a distributed work queue where an

00:03:39,620 --> 00:03:45,590
orchestration and progress essentially

00:03:41,660 --> 00:03:49,220
just naturally propagates around all the

00:03:45,590 --> 00:03:52,100
nodes that are available and is no

00:03:49,220 --> 00:03:53,900
longer stateful and we rewrote its

00:03:52,100 --> 00:03:57,650
persistence back-end and sequel recently

00:03:53,900 --> 00:04:01,550
I actually don't know if that's urged

00:03:57,650 --> 00:04:05,120
into open source or not but that's been

00:04:01,550 --> 00:04:07,880
a big win for us the not only from the

00:04:05,120 --> 00:04:10,700
access patterns that are better for that

00:04:07,880 --> 00:04:13,010
specific data set was sequel but also

00:04:10,700 --> 00:04:14,450
the ability to move on to a hosted

00:04:13,010 --> 00:04:18,170
sequel solution in our case we're using

00:04:14,450 --> 00:04:23,420
Aurora on AWS which gives us a good

00:04:18,170 --> 00:04:27,650
story around replication as well cloud

00:04:23,420 --> 00:04:31,400
driver not so much so that'll be a focus

00:04:27,650 --> 00:04:33,020
of the rest of this talk mostly so cloud

00:04:31,400 --> 00:04:36,410
drivers the service that goes out and

00:04:33,020 --> 00:04:40,040
crawls the cloud the cloud provider API

00:04:36,410 --> 00:04:43,160
is pulls all that data in aggregates it

00:04:40,040 --> 00:04:48,620
serves up API requests so it's both very

00:04:43,160 --> 00:04:52,190
read heavy and very write heavy when we

00:04:48,620 --> 00:04:55,390
initially started out we first

00:04:52,190 --> 00:04:57,980
implemented that in memory it was great

00:04:55,390 --> 00:05:01,970
but that doesn't scale past one node

00:04:57,980 --> 00:05:04,070
because we can't we can't build a sane

00:05:01,970 --> 00:05:07,100
system where each node has a different

00:05:04,070 --> 00:05:08,600
view of the world there was some guy

00:05:07,100 --> 00:05:11,930
that thought it would be a good idea to

00:05:08,600 --> 00:05:13,910
put that stuff in Redis because then we

00:05:11,930 --> 00:05:15,590
could scale the caching it would all

00:05:13,910 --> 00:05:17,180
index somewhere centrally we could scale

00:05:15,590 --> 00:05:21,950
the API reads they would all read from

00:05:17,180 --> 00:05:24,560
the central spot so my name is Cameron

00:05:21,950 --> 00:05:27,970
Bieber I'm a senior software engineer on

00:05:24,560 --> 00:05:30,740
the delivery engineering team at Netflix

00:05:27,970 --> 00:05:31,940
if you've run into scaling problems with

00:05:30,740 --> 00:05:34,430
cloud driver it's

00:05:31,940 --> 00:05:40,760
probably largely my fault so sorry about

00:05:34,430 --> 00:05:42,350
that but hopefully today we're going to

00:05:40,760 --> 00:05:44,870
talk about some of the features that

00:05:42,350 --> 00:05:46,820
exists and the tools that we built and

00:05:44,870 --> 00:05:50,270
how we're using them to kind of get past

00:05:46,820 --> 00:05:56,540
the scaling bottlenecks that exist in

00:05:50,270 --> 00:05:58,790
cloud driver and maybe while I'm in the

00:05:56,540 --> 00:06:00,680
apology mode here it's all so I guess

00:05:58,790 --> 00:06:01,880
worth pointing out that hopefully at

00:06:00,680 --> 00:06:03,680
some point in the future all of this

00:06:01,880 --> 00:06:06,350
talk becomes meaningless because we're

00:06:03,680 --> 00:06:08,720
very actively focused on addressing

00:06:06,350 --> 00:06:11,270
these performance problems in better

00:06:08,720 --> 00:06:14,690
ways than just figuring out how to throw

00:06:11,270 --> 00:06:16,640
more hardware at it and sort of get past

00:06:14,690 --> 00:06:19,640
the scaling bottlenecks so these are

00:06:16,640 --> 00:06:21,320
these are I would say are very tactical

00:06:19,640 --> 00:06:26,240
solutions that we've come up with to

00:06:21,320 --> 00:06:28,610
kind of keep the ship afloat to put sand

00:06:26,240 --> 00:06:35,210
on the tyre fire as we're actively

00:06:28,610 --> 00:06:38,810
working to make it better so we're going

00:06:35,210 --> 00:06:41,360
to talk about scaling cloud driver which

00:06:38,810 --> 00:06:44,210
will lead into some stuff around how we

00:06:41,360 --> 00:06:46,040
manage Redis then we're going to talk

00:06:44,210 --> 00:06:50,180
about scaling cloud drive or even more

00:06:46,040 --> 00:06:51,620
now that we can manage reticences we'll

00:06:50,180 --> 00:06:54,110
talk about some of the metrics and

00:06:51,620 --> 00:06:57,560
monitoring that we use to figure out

00:06:54,110 --> 00:06:58,970
that we're in a good place or not a

00:06:57,560 --> 00:07:01,940
little bit about how we do maintenance

00:06:58,970 --> 00:07:09,230
since we're now taking on managing

00:07:01,940 --> 00:07:10,970
stateful services and then Q&A time so

00:07:09,230 --> 00:07:12,860
we'll start with kind of the basic kind

00:07:10,970 --> 00:07:15,230
of first step you could take scaling out

00:07:12,860 --> 00:07:19,940
cloud driver so in the initial state of

00:07:15,230 --> 00:07:21,620
the system you've kind of got Dec your

00:07:19,940 --> 00:07:23,030
your UI where the users are coming in

00:07:21,620 --> 00:07:26,660
you've got API users they're all

00:07:23,030 --> 00:07:29,240
funneling through Kate and if Kate needs

00:07:26,660 --> 00:07:31,550
some information to serve just a read

00:07:29,240 --> 00:07:34,430
API request it'll ask cloud driver for

00:07:31,550 --> 00:07:37,850
that the user asks spinnaker to do

00:07:34,430 --> 00:07:40,220
something that flows over to orca orcas

00:07:37,850 --> 00:07:43,190
off that work as that works progressing

00:07:40,220 --> 00:07:45,140
it's doing a lot of checking the state

00:07:43,190 --> 00:07:45,500
of the cloud to see how I got where I'm

00:07:45,140 --> 00:07:48,110
trying

00:07:45,500 --> 00:07:49,700
- so the wait for up instances tasks

00:07:48,110 --> 00:07:52,180
which you may be familiar with if you've

00:07:49,700 --> 00:07:55,910
been sitting there trying to watch your

00:07:52,180 --> 00:07:58,940
watch your pot boil or your pipeline

00:07:55,910 --> 00:08:06,410
complete so all of that is is read

00:07:58,940 --> 00:08:07,850
traffic against cloud driver so the sort

00:08:06,410 --> 00:08:11,330
of the first pass we can take it

00:08:07,850 --> 00:08:12,950
sharding out the system we've kind of

00:08:11,330 --> 00:08:15,650
obviously identified we've got two

00:08:12,950 --> 00:08:17,450
different sort of usage patterns and

00:08:15,650 --> 00:08:21,580
we've got a system that naturally lends

00:08:17,450 --> 00:08:24,260
itself to two sharding to support those

00:08:21,580 --> 00:08:28,550
so we've got right heavy traffic from

00:08:24,260 --> 00:08:30,830
our cashing instances and we've got all

00:08:28,550 --> 00:08:33,050
the read requests that we need to serve

00:08:30,830 --> 00:08:35,200
this sort of the API requests in the

00:08:33,050 --> 00:08:38,419
state of orchestrations as they're

00:08:35,200 --> 00:08:43,700
proceeding so the first thing we can do

00:08:38,419 --> 00:08:46,910
is is introduce some replication so what

00:08:43,700 --> 00:08:49,130
we do is we add a read replica and we

00:08:46,910 --> 00:08:51,110
stand up another cloud driver cluster

00:08:49,130 --> 00:08:54,500
and we call that one or read-only

00:08:51,110 --> 00:08:56,630
cluster and because gate is never

00:08:54,500 --> 00:08:58,730
actually making changes to the state of

00:08:56,630 --> 00:09:03,710
the cloud it can always talk to the

00:08:58,730 --> 00:09:05,600
read-only cluster orca will kick off

00:09:03,710 --> 00:09:07,730
work that modifies the state of the

00:09:05,600 --> 00:09:10,100
cloud but then watch for completion of

00:09:07,730 --> 00:09:12,020
network so Orca can initiate a request

00:09:10,100 --> 00:09:14,630
to the main Cloud Drive or instance and

00:09:12,020 --> 00:09:17,380
then do all its read traffic against the

00:09:14,630 --> 00:09:17,380
read-only replicas

00:09:20,050 --> 00:09:26,110
so in order to make that partition we

00:09:23,529 --> 00:09:28,800
added a reed replica we stood up a

00:09:26,110 --> 00:09:31,600
dedicated cloud driver read-only cluster

00:09:28,800 --> 00:09:34,629
pointing at that reed replica and that

00:09:31,600 --> 00:09:37,509
cluster has its own ELB it has a

00:09:34,629 --> 00:09:39,759
dedicated hostname and on that cluster

00:09:37,509 --> 00:09:42,489
we disable caching so there's a feature

00:09:39,759 --> 00:09:45,220
flag in the cloud driver Yama file

00:09:42,489 --> 00:09:47,350
caching right enabled and we set that to

00:09:45,220 --> 00:09:51,040
false and that stops running caching

00:09:47,350 --> 00:09:53,410
agents against the replica gate points

00:09:51,040 --> 00:09:56,559
all its API traffic against our new

00:09:53,410 --> 00:09:58,779
hostname for read only and Orca has a

00:09:56,559 --> 00:10:01,089
feature where you can say here's a URL

00:09:58,779 --> 00:10:04,689
for a read-only cloud driver so we set

00:10:01,089 --> 00:10:07,869
that we set that configuration so a

00:10:04,689 --> 00:10:11,410
cloud driver read-only URL and Orca will

00:10:07,869 --> 00:10:16,389
start watching the replicas as it tries

00:10:11,410 --> 00:10:17,860
to achieve its its state of the world so

00:10:16,389 --> 00:10:22,779
that's a that's a good incremental step

00:10:17,860 --> 00:10:25,600
and kind of another step along the same

00:10:22,779 --> 00:10:29,860
lines is partitioning out the actual

00:10:25,600 --> 00:10:31,899
execution of caching the cat the way the

00:10:29,860 --> 00:10:33,600
caching works is it's crawling all the

00:10:31,899 --> 00:10:37,149
different collections in the in each

00:10:33,600 --> 00:10:40,029
underlying cloud provider for each

00:10:37,149 --> 00:10:41,410
account for each region and you start

00:10:40,029 --> 00:10:44,559
multiplying all those together and you

00:10:41,410 --> 00:10:46,589
have hundreds plus of caching agents

00:10:44,559 --> 00:10:49,779
that need to run and be scheduled as

00:10:46,589 --> 00:10:53,110
those pull data they can pull large data

00:10:49,779 --> 00:10:55,269
sets into memory process that data and

00:10:53,110 --> 00:10:58,449
that can become very sort of memory and

00:10:55,269 --> 00:11:00,519
compute-intensive so there's a couple

00:10:58,449 --> 00:11:02,740
region reasons to partition that off one

00:11:00,519 --> 00:11:05,799
is that you can have a little better

00:11:02,740 --> 00:11:07,660
control over instance sizing so that you

00:11:05,799 --> 00:11:10,749
don't say blow up memory as those

00:11:07,660 --> 00:11:15,429
caching agents are running as well as

00:11:10,749 --> 00:11:18,100
the sort of the cpu load that executing

00:11:15,429 --> 00:11:20,860
that caching can put on can actually

00:11:18,100 --> 00:11:22,959
introduce not-so-great latency is it

00:11:20,860 --> 00:11:26,230
just in in serving API requests off the

00:11:22,959 --> 00:11:27,730
same note so this one is fairly

00:11:26,230 --> 00:11:31,329
straightforward we added we added

00:11:27,730 --> 00:11:33,760
another cluster so now we have a Cloud

00:11:31,329 --> 00:11:35,590
Drive or API cluster this one's read

00:11:33,760 --> 00:11:38,200
this is where operations are going to

00:11:35,590 --> 00:11:41,080
kick off from we have a cloud driver

00:11:38,200 --> 00:11:42,970
cluster that just does caching and then

00:11:41,080 --> 00:11:44,590
we still have our read-only replicas

00:11:42,970 --> 00:11:50,860
that we had it because it was a good

00:11:44,590 --> 00:11:52,570
idea so now in this model or at least

00:11:50,860 --> 00:11:55,150
the way we've we've approached it is we

00:11:52,570 --> 00:11:56,830
added a cloud driver API and that one

00:11:55,150 --> 00:12:00,580
actually keeps the host name in the ELB

00:11:56,830 --> 00:12:02,410
from our original configuration in our

00:12:00,580 --> 00:12:04,240
cloud driver caching cluster nobody's

00:12:02,410 --> 00:12:06,250
actually making API calls to it it just

00:12:04,240 --> 00:12:10,810
sits there and runs these caching

00:12:06,250 --> 00:12:14,590
processes one thing to note is there is

00:12:10,810 --> 00:12:16,660
some configuration in cloud driver where

00:12:14,590 --> 00:12:19,690
you can actually tune the number of

00:12:16,660 --> 00:12:25,090
concurrent caching agents that will run

00:12:19,690 --> 00:12:26,920
on a node and this is really handy for a

00:12:25,090 --> 00:12:28,780
couple reasons one if you're you're just

00:12:26,920 --> 00:12:31,360
trying to do local development and you

00:12:28,780 --> 00:12:34,570
run cloud driver previously that would

00:12:31,360 --> 00:12:37,660
like spin up all the fans on your CPU it

00:12:34,570 --> 00:12:39,310
would try to - a Bitcoin but it would it

00:12:37,660 --> 00:12:41,530
wouldn't succeed but then it would try

00:12:39,310 --> 00:12:43,090
and run all the caching agents with this

00:12:41,530 --> 00:12:45,640
configuration that you can tune that way

00:12:43,090 --> 00:12:48,040
back and not actually melt your desk

00:12:45,640 --> 00:12:52,180
with your laptop but also for your

00:12:48,040 --> 00:12:55,840
caching cluster you can sort of control

00:12:52,180 --> 00:13:00,280
the number of concurrent executions and

00:12:55,840 --> 00:13:02,050
you know guard against loading too many

00:13:00,280 --> 00:13:04,450
gigantic data sets on one node and

00:13:02,050 --> 00:13:11,770
cropping it out blowing up memory or

00:13:04,450 --> 00:13:14,830
whatnot so one of the things we did in

00:13:11,770 --> 00:13:18,190
order to shard this as we added Redis

00:13:14,830 --> 00:13:22,170
replicas and it turns out for us that we

00:13:18,190 --> 00:13:24,490
ended up managing Redis ourself and

00:13:22,170 --> 00:13:28,120
maybe the first question is why not

00:13:24,490 --> 00:13:30,610
ElastiCache because maintaining stateful

00:13:28,120 --> 00:13:33,340
services is hard and it sucks and it's

00:13:30,610 --> 00:13:34,600
great if someone else can do it we

00:13:33,340 --> 00:13:38,140
started down the path of using

00:13:34,600 --> 00:13:40,930
ElastiCache in history initially we ran

00:13:38,140 --> 00:13:42,700
into some unpredictable behavior during

00:13:40,930 --> 00:13:46,790
the maintenance windows we ran into you

00:13:42,700 --> 00:13:49,040
and and this probably is also related to

00:13:46,790 --> 00:13:51,050
what was causing us to trigger failover

00:13:49,040 --> 00:13:52,790
this was kind of early in the days of us

00:13:51,050 --> 00:13:55,720
understanding the performance

00:13:52,790 --> 00:13:58,850
characteristics but we found that

00:13:55,720 --> 00:14:00,980
running an elastic ash cluster we would

00:13:58,850 --> 00:14:06,590
just end up in a state where it would

00:14:00,980 --> 00:14:07,790
vary surprisingly trigger fail overs and

00:14:06,590 --> 00:14:11,390
then part of that is we didn't have

00:14:07,790 --> 00:14:14,450
great insight into it because there was

00:14:11,390 --> 00:14:17,630
just that disconnect between how the

00:14:14,450 --> 00:14:25,550
ElastiCache publishes metrics and how we

00:14:17,630 --> 00:14:28,910
like to consume them so for our reticent

00:14:25,550 --> 00:14:31,550
4 structure we essentially have our own

00:14:28,910 --> 00:14:34,100
am i with Redis on it and we pre

00:14:31,550 --> 00:14:36,400
allocate a bunch of elastic network

00:14:34,100 --> 00:14:40,730
interfaces so each of those

00:14:36,400 --> 00:14:43,220
corresponding to a Redis instance we

00:14:40,730 --> 00:14:46,460
stick an ec2 tag on those that says this

00:14:43,220 --> 00:14:48,200
is an en I for this cluster and we put

00:14:46,460 --> 00:14:50,330
host names on the IP addresses

00:14:48,200 --> 00:14:52,700
associated with those en eyes so those

00:14:50,330 --> 00:14:55,250
are private IPS in our V PC space but

00:14:52,700 --> 00:14:57,020
they're fixed and we can use DNS naming

00:14:55,250 --> 00:15:00,380
to to address these things

00:14:57,020 --> 00:15:04,130
so in our convention we would have a

00:15:00,380 --> 00:15:06,550
main cluster as a master we would have a

00:15:04,130 --> 00:15:09,950
replica cluster for all the replicas and

00:15:06,550 --> 00:15:13,640
then our ami is aware of of that naming

00:15:09,950 --> 00:15:15,500
convention and essentially what we've

00:15:13,640 --> 00:15:17,390
done is we've we've got a team that

00:15:15,500 --> 00:15:19,670
manages a base image that kind of all

00:15:17,390 --> 00:15:24,110
Netflix software runs on we do have a

00:15:19,670 --> 00:15:26,060
vanilla Redis built on there but this

00:15:24,110 --> 00:15:28,820
this gives us the insight via metrics

00:15:26,060 --> 00:15:32,480
publishing we also also built a pretty

00:15:28,820 --> 00:15:34,970
simple sidecar app that just monitors

00:15:32,480 --> 00:15:37,490
Redis by issuing the info command and

00:15:34,970 --> 00:15:41,720
exports those metrics out through

00:15:37,490 --> 00:15:42,710
spectator so that they end up aggregated

00:15:41,720 --> 00:15:45,920
in the same way as all the other

00:15:42,710 --> 00:15:49,640
spinnaker components do we get an health

00:15:45,920 --> 00:15:50,990
indicator through Eureka which we use as

00:15:49,640 --> 00:15:53,660
long as the Redis instances is

00:15:50,990 --> 00:15:56,450
responding to pay so on startup this

00:15:53,660 --> 00:15:58,460
instance will look look out for an

00:15:56,450 --> 00:16:00,529
network interface with a tag that

00:15:58,460 --> 00:16:02,870
matches the cluster it's deployed in

00:16:00,529 --> 00:16:04,970
it'll grab it and attach it if it's

00:16:02,870 --> 00:16:06,410
cluster name ends with replica it'll

00:16:04,970 --> 00:16:07,939
issue the slave of command and connect

00:16:06,410 --> 00:16:10,249
to the master and it's good to go it's

00:16:07,939 --> 00:16:16,879
all kind of hands-off we just launched

00:16:10,249 --> 00:16:19,879
one of these so now that launching Redis

00:16:16,879 --> 00:16:21,790
is easy and we have a hammer everything

00:16:19,879 --> 00:16:24,350
looks like adding a replica problem

00:16:21,790 --> 00:16:29,629
we're gonna talk about how we do that

00:16:24,350 --> 00:16:32,420
even more so with our initial our

00:16:29,629 --> 00:16:35,899
initial partition of a read replica and

00:16:32,420 --> 00:16:37,309
a master we're in an all right spot but

00:16:35,899 --> 00:16:40,100
we're still in a spot where all of the

00:16:37,309 --> 00:16:45,379
read traffic is shared to one replicas

00:16:40,100 --> 00:16:48,740
and one sort of bad actor or really busy

00:16:45,379 --> 00:16:51,920
busy deployment or something can impact

00:16:48,740 --> 00:16:54,050
all of the users of the system so one of

00:16:51,920 --> 00:16:58,309
the first user groups that we wanted to

00:16:54,050 --> 00:17:00,290
address and not impact as much as

00:16:58,309 --> 00:17:03,800
possible is people that are actually in

00:17:00,290 --> 00:17:07,100
the UI trying to look at the state of

00:17:03,800 --> 00:17:10,010
their instances or take action so for us

00:17:07,100 --> 00:17:12,110
if there is an incident people get

00:17:10,010 --> 00:17:16,069
involved very often the first thing they

00:17:12,110 --> 00:17:18,620
want to do is go in and maybe scale up a

00:17:16,069 --> 00:17:21,110
scale up a server group maybe I want to

00:17:18,620 --> 00:17:23,630
do the automated rollback but

00:17:21,110 --> 00:17:25,339
essentially we don't want some other

00:17:23,630 --> 00:17:29,149
system that happens to be running a big

00:17:25,339 --> 00:17:32,390
deployment job to impact the UI in such

00:17:29,149 --> 00:17:36,909
a way where those people can take those

00:17:32,390 --> 00:17:39,799
actions so one of our heavy API users

00:17:36,909 --> 00:17:43,190
crushes our replicas we get slow API

00:17:39,799 --> 00:17:45,380
responses or even timeouts we get a

00:17:43,190 --> 00:17:47,630
slower unresponsive UI and then we get

00:17:45,380 --> 00:17:51,100
angry users in our spinnaker stock

00:17:47,630 --> 00:17:51,100
Channel and saying why's spinnaker down

00:17:53,840 --> 00:18:01,950
so we add another replica because that's

00:17:57,179 --> 00:18:04,710
what we do and we added a feature where

00:18:01,950 --> 00:18:10,020
Dec always adds a header that says X

00:18:04,710 --> 00:18:12,919
rate limit app is Dec and gate is able

00:18:10,020 --> 00:18:15,929
to use that header to do dynamic routing

00:18:12,919 --> 00:18:17,309
so we stand up a replica we stand up our

00:18:15,929 --> 00:18:23,309
claw driver cluster pointing at that

00:18:17,309 --> 00:18:26,400
replica excuse me and then we add some

00:18:23,309 --> 00:18:31,039
configuration in gate that tells it to

00:18:26,400 --> 00:18:31,039
route all this traffic to that replica

00:18:31,340 --> 00:18:36,750
so that's fairly straightforward in gate

00:18:34,650 --> 00:18:39,240
you can add some configuration for

00:18:36,750 --> 00:18:42,929
dynamic endpoints so services dot cloud

00:18:39,240 --> 00:18:45,570
driver config dynamic endpoints and so

00:18:42,929 --> 00:18:47,520
anything with that X rate limit app that

00:18:45,570 --> 00:18:50,669
matches deck will go to this claw driver

00:18:47,520 --> 00:18:53,600
read-only deck replica and now we've

00:18:50,669 --> 00:19:01,200
partitioned you I read traffic away from

00:18:53,600 --> 00:19:05,220
our main replica so the last sort of

00:19:01,200 --> 00:19:07,740
thing we're going to do is we want to

00:19:05,220 --> 00:19:11,760
isolate the the big users of the system

00:19:07,740 --> 00:19:13,679
we have a number of a number of systems

00:19:11,760 --> 00:19:16,440
that are constantly running automation

00:19:13,679 --> 00:19:21,020
that may kick off a very large number of

00:19:16,440 --> 00:19:22,950
concurrent orchestrations we've got some

00:19:21,020 --> 00:19:27,179
services that are deployed that have

00:19:22,950 --> 00:19:33,020
upwards of 1,500 clusters within their

00:19:27,179 --> 00:19:36,750
application so those systems or those

00:19:33,020 --> 00:19:40,880
tools when they want to kick off work

00:19:36,750 --> 00:19:43,500
can often put a large amount of load on

00:19:40,880 --> 00:19:44,789
spinnaker and particularly if they're

00:19:43,500 --> 00:19:49,320
kicking off a bunch of concurrent

00:19:44,789 --> 00:19:53,909
orchestration jobs can saturator Redis

00:19:49,320 --> 00:19:56,190
instance so we want to solve that

00:19:53,909 --> 00:19:58,289
problem but we also don't necessarily

00:19:56,190 --> 00:20:01,350
want to inflict the solution of that

00:19:58,289 --> 00:20:02,730
problem on our users so we kind of

00:20:01,350 --> 00:20:04,799
constrained ourselves to solve that

00:20:02,730 --> 00:20:06,809
behind our API layer we don't want to

00:20:04,799 --> 00:20:08,190
give out hey you get this endpoint you

00:20:06,809 --> 00:20:13,169
get this endpoint or you get this stuck

00:20:08,190 --> 00:20:15,539
and you get the stack we don't for

00:20:13,169 --> 00:20:18,389
better or worse we don't build and

00:20:15,539 --> 00:20:22,409
maintain a like smart client for our API

00:20:18,389 --> 00:20:23,789
so our API is just the REST API so we

00:20:22,409 --> 00:20:29,249
don't have the ability to sort of push

00:20:23,789 --> 00:20:30,899
out dynamic endpoint decisions to users

00:20:29,249 --> 00:20:33,899
of that API and even if we did we'd have

00:20:30,899 --> 00:20:36,720
to maintain it in a number of different

00:20:33,899 --> 00:20:38,580
languages so essentially we want

00:20:36,720 --> 00:20:42,629
everyone coming into our one you know

00:20:38,580 --> 00:20:49,049
API dot spinnaker hostname and we'll

00:20:42,629 --> 00:20:50,850
we'll solve it behind that so now what

00:20:49,049 --> 00:20:54,779
we can do is add more replicas because

00:20:50,850 --> 00:20:58,830
that's what we do so here we can add any

00:20:54,779 --> 00:21:06,990
number of replicas that are dedicated to

00:20:58,830 --> 00:21:08,789
your work a traffic and and each of

00:21:06,990 --> 00:21:11,279
those is set up the standard way they've

00:21:08,789 --> 00:21:14,249
got a cluster they've got a host host

00:21:11,279 --> 00:21:16,740
name in yell B and we use what's called

00:21:14,249 --> 00:21:19,529
a service selector in Orca in order to

00:21:16,740 --> 00:21:21,659
figure out which one of those clusters

00:21:19,529 --> 00:21:25,549
to land on so again we still have the

00:21:21,659 --> 00:21:28,200
the issue of we want consistent reads

00:21:25,549 --> 00:21:31,110
for our for a particular user

00:21:28,200 --> 00:21:33,240
so each as we add all these replicas

00:21:31,110 --> 00:21:36,240
they're all in sort of a different state

00:21:33,240 --> 00:21:39,539
of replication we don't want everyone

00:21:36,240 --> 00:21:42,029
some particular use case to bounce

00:21:39,539 --> 00:21:44,879
between different replicas because you

00:21:42,029 --> 00:21:47,909
might end up in a in a state where I

00:21:44,879 --> 00:21:50,429
read something and I see a server groups

00:21:47,909 --> 00:21:52,169
there and then I read and I land on a

00:21:50,429 --> 00:21:53,490
different replica and that one's not

00:21:52,169 --> 00:21:55,049
caught up to the point where that server

00:21:53,490 --> 00:21:56,730
group was created so I don't see it

00:21:55,049 --> 00:21:58,649
and then my automation gets confused

00:21:56,730 --> 00:22:00,450
because I saw it and I don't see it so

00:21:58,649 --> 00:22:03,320
now what do I do did it has it got

00:22:00,450 --> 00:22:07,230
created yet did it just get deleted

00:22:03,320 --> 00:22:09,779
confusion ensues so we want consistent

00:22:07,230 --> 00:22:13,259
routing to all these replicas and that's

00:22:09,779 --> 00:22:16,169
what the server selectors give us so

00:22:13,259 --> 00:22:17,070
essentially we get rule-based routing to

00:22:16,169 --> 00:22:18,520
all these different cloud driver

00:22:17,070 --> 00:22:22,510
endpoints that we're standing up

00:22:18,520 --> 00:22:25,210
orco and we have a list of these these

00:22:22,510 --> 00:22:28,990
cloud driver endpoints each one has a

00:22:25,210 --> 00:22:30,640
URL they're prioritized because it's

00:22:28,990 --> 00:22:33,430
very possible to configure these service

00:22:30,640 --> 00:22:34,690
selectors where it would match a number

00:22:33,430 --> 00:22:36,670
of service selectors might match a

00:22:34,690 --> 00:22:41,440
particular requests a priority lets you

00:22:36,670 --> 00:22:43,690
control which one wins we have a few

00:22:41,440 --> 00:22:46,780
options out-of-the-box for doing this

00:22:43,690 --> 00:22:49,150
selection so we can select on the

00:22:46,780 --> 00:22:51,610
execution type so is this a pipeline or

00:22:49,150 --> 00:22:53,080
an orchestration really the difference

00:22:51,610 --> 00:22:55,060
between the two is an orchestration is

00:22:53,080 --> 00:22:57,880
something that a user in the UI goes

00:22:55,060 --> 00:22:59,560
goes in and clicks like resize or

00:22:57,880 --> 00:23:01,930
rollback or whatever whereas the

00:22:59,560 --> 00:23:05,170
pipeline as an execution through the

00:23:01,930 --> 00:23:08,800
pipeline configuration we can select by

00:23:05,170 --> 00:23:10,870
the authenticated user so if if we know

00:23:08,800 --> 00:23:13,570
there's some automation like chaos

00:23:10,870 --> 00:23:16,720
monkey that comes in and constantly is

00:23:13,570 --> 00:23:20,640
is issuing termination commands we could

00:23:16,720 --> 00:23:23,830
route that all to a particular replica

00:23:20,640 --> 00:23:26,500
we have an or origin service selector

00:23:23,830 --> 00:23:29,020
and that's where we're leveraging that

00:23:26,500 --> 00:23:33,010
header from deck that flows through the

00:23:29,020 --> 00:23:34,960
system as an origin so we can shard by

00:23:33,010 --> 00:23:38,170
that origin and sure and it can ensure

00:23:34,960 --> 00:23:41,440
that not only are all the reads from the

00:23:38,170 --> 00:23:42,880
UI on some dedicated infrastructure but

00:23:41,440 --> 00:23:45,700
also all the actions that somebody's

00:23:42,880 --> 00:23:48,040
trying to take and again we've got one

00:23:45,700 --> 00:23:52,350
to shard just on the application that's

00:23:48,040 --> 00:23:56,170
being operated on so as a quick example

00:23:52,350 --> 00:23:57,580
we stood up three replicas here we've

00:23:56,170 --> 00:23:59,800
got one that's selecting on the

00:23:57,580 --> 00:24:02,730
applications we've got a big app we got

00:23:59,800 --> 00:24:05,020
one and we've got all of our tightest

00:24:02,730 --> 00:24:13,770
infrastructure and we're gonna share

00:24:05,020 --> 00:24:16,240
route that to this Orca one replicas and

00:24:13,770 --> 00:24:18,490
we've got another selector we want all

00:24:16,240 --> 00:24:21,430
our UI operations to route to a

00:24:18,490 --> 00:24:24,460
dedicated shard and then finally we have

00:24:21,430 --> 00:24:26,410
a just a default so with no

00:24:24,460 --> 00:24:30,340
configuration that becomes the

00:24:26,410 --> 00:24:32,620
fall back for everything else so that's

00:24:30,340 --> 00:24:36,340
kind of the extent of the the options

00:24:32,620 --> 00:24:38,590
that exist for partitioning the traffic

00:24:36,340 --> 00:24:42,880
and claw driver you'll notice it was a

00:24:38,590 --> 00:24:44,410
lot of partitioning the read traffic we

00:24:42,880 --> 00:24:46,960
don't currently have a good story for

00:24:44,410 --> 00:24:50,200
partitioning the right traffic so if we

00:24:46,960 --> 00:24:53,620
got to the state of overwhelming the

00:24:50,200 --> 00:24:56,260
Redis master the best we can kind of do

00:24:53,620 --> 00:24:59,560
is tune back the number of concurrent

00:24:56,260 --> 00:25:03,010
agent executions agent executions to

00:24:59,560 --> 00:25:07,600
make sure that we we don't overwhelm it

00:25:03,010 --> 00:25:10,890
and maybe deploy it on an x1 or

00:25:07,600 --> 00:25:10,890
something like that and hope that works

00:25:12,480 --> 00:25:19,480
so I want to talk a little bit about how

00:25:14,830 --> 00:25:23,500
we're monitoring to determine that the

00:25:19,480 --> 00:25:25,060
system is in a happy state and ideally

00:25:23,500 --> 00:25:27,490
it's not our users and our spinnaker

00:25:25,060 --> 00:25:31,230
channel in slack but sometimes often it

00:25:27,490 --> 00:25:31,230
is it's unfortunate

00:25:31,470 --> 00:25:38,260
so we've heavily instrumented all of

00:25:34,180 --> 00:25:40,780
spinnaker with metrics and they're all

00:25:38,260 --> 00:25:42,880
published out if you're using the

00:25:40,780 --> 00:25:45,520
spinnaker monitoring component all these

00:25:42,880 --> 00:25:48,220
metrics should be available but here's a

00:25:45,520 --> 00:25:50,700
few key ones that we use that give us a

00:25:48,220 --> 00:25:55,080
good indicator of sort of the health of

00:25:50,700 --> 00:25:57,970
this Redis infrastructure specifically

00:25:55,080 --> 00:26:00,340
there's a metric controller in vacations

00:25:57,970 --> 00:26:03,840
and that exists on all the services this

00:26:00,340 --> 00:26:09,750
gives you timings of every API request

00:26:03,840 --> 00:26:15,870
for us we can use this to alert on our

00:26:09,750 --> 00:26:19,750
read replicas api's if they get really

00:26:15,870 --> 00:26:22,660
congested then those timings will shoot

00:26:19,750 --> 00:26:25,540
up and we have sort of identified some

00:26:22,660 --> 00:26:30,130
good thresholds there that for us are

00:26:25,540 --> 00:26:32,910
worthy of alerting on we have a counter

00:26:30,130 --> 00:26:35,170
so all of the caching agents that run

00:26:32,910 --> 00:26:37,390
will just increment a counter and

00:26:35,170 --> 00:26:38,740
they'll tag that counter with a success

00:26:37,390 --> 00:26:43,779
or failure count

00:26:38,740 --> 00:26:47,200
so we can alert on cashing agents

00:26:43,779 --> 00:26:48,610
failing to complete successfully this

00:26:47,200 --> 00:26:51,159
can be an indicator that the Redis

00:26:48,610 --> 00:26:53,440
master is overloaded this can also be an

00:26:51,159 --> 00:26:57,149
indicator that the underlying cloud

00:26:53,440 --> 00:26:59,649
platform is serving up some errors

00:26:57,149 --> 00:27:02,380
either way it's usually a good thing to

00:26:59,649 --> 00:27:04,659
be aware of but probably one of the most

00:27:02,380 --> 00:27:05,950
interesting metrics and this one comes

00:27:04,659 --> 00:27:10,390
from our little sidecar but it's

00:27:05,950 --> 00:27:13,809
essentially the if you did a read a CLI

00:27:10,390 --> 00:27:15,820
info replication one of the items in

00:27:13,809 --> 00:27:18,100
there is the is called the slave Delta

00:27:15,820 --> 00:27:20,500
and that's the number of bytes that a

00:27:18,100 --> 00:27:23,940
particular slave is offset from the

00:27:20,500 --> 00:27:23,940
Redis master that it's connected to

00:27:24,149 --> 00:27:27,270
excuse me

00:27:27,809 --> 00:27:35,020
so in kind of the steady-state normal

00:27:30,789 --> 00:27:38,649
case these offsets are relatively low in

00:27:35,020 --> 00:27:42,789
in the sort of order of megabytes at

00:27:38,649 --> 00:27:45,159
most but what we'll see is a couple of

00:27:42,789 --> 00:27:49,029
different examples of when things go

00:27:45,159 --> 00:27:52,330
sort of sideways so one example is we'll

00:27:49,029 --> 00:27:54,580
see one replica and it'll just start

00:27:52,330 --> 00:27:57,820
getting lagged and it'll get into this

00:27:54,580 --> 00:28:00,010
situation where it's essentially

00:27:57,820 --> 00:28:01,929
unrecoverable I lagged because it's got

00:28:00,010 --> 00:28:03,970
so much data to ingest from the

00:28:01,929 --> 00:28:07,440
replication stream that's sort of any

00:28:03,970 --> 00:28:10,120
ongoing API request traffic against that

00:28:07,440 --> 00:28:13,809
puts too much load on it for it to ever

00:28:10,120 --> 00:28:15,789
get caught up the other thing that we'll

00:28:13,809 --> 00:28:17,350
see so that's like a single replica kind

00:28:15,789 --> 00:28:21,330
of going bad the other thing that we'll

00:28:17,350 --> 00:28:24,880
see is all of the replicas start to go

00:28:21,330 --> 00:28:29,140
go bad at the same time and that's

00:28:24,880 --> 00:28:31,299
generally an indicator that the there's

00:28:29,140 --> 00:28:33,399
too much churn from the master we

00:28:31,299 --> 00:28:38,049
haven't really seen that in a long time

00:28:33,399 --> 00:28:43,600
except for very recently our sort of

00:28:38,049 --> 00:28:46,210
proxy layer between the AWS API and our

00:28:43,600 --> 00:28:48,039
caching infrastructure got into a bad

00:28:46,210 --> 00:28:49,600
state where it was flopping between a

00:28:48,039 --> 00:28:51,850
really old view of the world and really

00:28:49,600 --> 00:28:52,880
current view of the world and we were

00:28:51,850 --> 00:28:54,770
constantly it

00:28:52,880 --> 00:28:58,310
everything out of the cache and REE

00:28:54,770 --> 00:29:06,500
caching it and tipped us over and ruined

00:28:58,310 --> 00:29:08,420
a weekend for some people so kind of the

00:29:06,500 --> 00:29:11,990
next thing I want to talk about is is

00:29:08,420 --> 00:29:13,370
maintaining this this infrastructure so

00:29:11,990 --> 00:29:17,450
we've added all these instances they're

00:29:13,370 --> 00:29:20,750
all stateful instances we need a way to

00:29:17,450 --> 00:29:22,730
be able to apply sup apply security

00:29:20,750 --> 00:29:26,120
patches we need a way to be able to

00:29:22,730 --> 00:29:28,160
update the version of Redis we don't

00:29:26,120 --> 00:29:29,750
really have a deployment window where we

00:29:28,160 --> 00:29:33,530
can take things offline to do this kind

00:29:29,750 --> 00:29:36,560
of stuff and we really don't want to do

00:29:33,530 --> 00:29:38,270
any sort of mutable infrastructure types

00:29:36,560 --> 00:29:39,740
things so we still want to stick with an

00:29:38,270 --> 00:29:47,330
immutable infrastructure model where

00:29:39,740 --> 00:29:49,490
we're pushing out these a.m. eyes so I

00:29:47,330 --> 00:29:52,220
know we'll add more reticence so

00:29:49,490 --> 00:29:54,500
essentially what we do is we have our

00:29:52,220 --> 00:29:57,830
current stack of all these replicas and

00:29:54,500 --> 00:29:59,450
all these all these cloud driver

00:29:57,830 --> 00:30:03,410
clusters that are reading from these

00:29:59,450 --> 00:30:06,740
replicas and we just basically rubber

00:30:03,410 --> 00:30:10,610
stamp that into another another another

00:30:06,740 --> 00:30:12,200
batch so what we we stick in our naming

00:30:10,610 --> 00:30:13,640
convention for our clusters is we call

00:30:12,200 --> 00:30:17,750
one of them a and we call one of them B

00:30:13,640 --> 00:30:21,530
and in our steady state we're operating

00:30:17,750 --> 00:30:23,630
against one of the two of those and if

00:30:21,530 --> 00:30:26,870
we need to make infrastructure changes

00:30:23,630 --> 00:30:30,380
to push out a new Redis version or or

00:30:26,870 --> 00:30:32,930
whatnot then we run a deployment where

00:30:30,380 --> 00:30:37,040
we essentially move everything onto a

00:30:32,930 --> 00:30:40,100
new a new set of instances and really

00:30:37,040 --> 00:30:43,660
the the one thing that we need to enable

00:30:40,100 --> 00:30:46,820
there to make that like seamless ish is

00:30:43,660 --> 00:30:48,650
as Clau drivers running tasks it's

00:30:46,820 --> 00:30:51,760
writing and update into the into the

00:30:48,650 --> 00:30:55,970
Redis database tracking the state of

00:30:51,760 --> 00:30:58,580
that task execution and so when we end

00:30:55,970 --> 00:31:02,270
up on a new reticence since we support

00:30:58,580 --> 00:31:03,770
reading and I probably should have put

00:31:02,270 --> 00:31:05,660
this configuration property in here I

00:31:03,770 --> 00:31:06,870
apologize we support reading from the

00:31:05,660 --> 00:31:09,180
previous Redis so there's

00:31:06,870 --> 00:31:13,350
an actually a configuration property

00:31:09,180 --> 00:31:17,400
Retta previous connection and so if we

00:31:13,350 --> 00:31:19,650
deploy to - if we're running on - a we

00:31:17,400 --> 00:31:23,370
deployed about - B it knows - connects

00:31:19,650 --> 00:31:25,980
to the - a Redis if it ever gets a cache

00:31:23,370 --> 00:31:28,410
miss looking up tasks information so

00:31:25,980 --> 00:31:31,740
this is what gives us the sort of

00:31:28,410 --> 00:31:34,080
ability to deploy a whole new set of

00:31:31,740 --> 00:31:36,570
Redis and this is actually what we

00:31:34,080 --> 00:31:39,450
leverage if we get to the point of

00:31:36,570 --> 00:31:41,520
replicas going sideways and never going

00:31:39,450 --> 00:31:44,490
to get caught up again is we just deploy

00:31:41,520 --> 00:31:47,460
onto a fresh Redis instance and I'll

00:31:44,490 --> 00:31:51,390
sort of show you what that looks like we

00:31:47,460 --> 00:31:52,679
call that the flippy floppy flush so

00:31:51,390 --> 00:31:55,230
essentially in our steady state we're

00:31:52,679 --> 00:31:59,070
just deploying claw driver updates by a

00:31:55,230 --> 00:32:00,870
simple red block deployment pipeline but

00:31:59,070 --> 00:32:03,390
in the Redis maintenance case or the

00:32:00,870 --> 00:32:07,620
emergency we flip onto the other reddish

00:32:03,390 --> 00:32:09,540
Redis stack and we flush or recreate

00:32:07,620 --> 00:32:12,900
that so it's essentially starts empty

00:32:09,540 --> 00:32:15,360
from a kind of happy happy path there's

00:32:12,900 --> 00:32:17,400
a little bit of a scripting call in

00:32:15,360 --> 00:32:19,410
there just that we use we make an API

00:32:17,400 --> 00:32:23,460
call to our spinnaker instance or

00:32:19,410 --> 00:32:26,640
spinnaker API that says what is the

00:32:23,460 --> 00:32:28,650
current the current stack is are we

00:32:26,640 --> 00:32:30,260
currently on a or we currently on B we

00:32:28,650 --> 00:32:32,790
just do that by looking at all of the

00:32:30,260 --> 00:32:35,280
clusters in cloud driver and if we see

00:32:32,790 --> 00:32:37,290
well there's there's a bunch of - A's

00:32:35,280 --> 00:32:39,540
and no - B's then we know we're

00:32:37,290 --> 00:32:42,809
currently running on a flipping on to B

00:32:39,540 --> 00:32:45,120
so we just export out of that scripting

00:32:42,809 --> 00:32:46,559
call some parameters and we feed that

00:32:45,120 --> 00:32:49,170
into the rest of this pipeline so we

00:32:46,559 --> 00:32:52,080
have one deployment pipeline for cloud

00:32:49,170 --> 00:32:54,900
driver and that can either do our happy

00:32:52,080 --> 00:33:00,690
path regular software push or this

00:32:54,900 --> 00:33:02,790
full-on flip so our happy case is fairly

00:33:00,690 --> 00:33:04,980
straightforward we look up an image we

00:33:02,790 --> 00:33:08,490
deploy we've kind of partitioned our

00:33:04,980 --> 00:33:15,360
deploy into cashing agents first and

00:33:08,490 --> 00:33:18,179
then the rest of our api clusters that's

00:33:15,360 --> 00:33:19,690
less important for this happy happy path

00:33:18,179 --> 00:33:25,179
deployment

00:33:19,690 --> 00:33:29,080
but it lets us for the the flippy-floppy

00:33:25,179 --> 00:33:31,690
flush it lets us push these caching

00:33:29,080 --> 00:33:31,990
agents and then we have a wait stage in

00:33:31,690 --> 00:33:34,299
there

00:33:31,990 --> 00:33:37,529
it's parameterised so in the case of

00:33:34,299 --> 00:33:39,610
this rebuilding the world mode we just

00:33:37,529 --> 00:33:41,500
dynamically stick a weight in there and

00:33:39,610 --> 00:33:43,539
say okay well we're going to run this

00:33:41,500 --> 00:33:46,779
for five minutes let all the caching

00:33:43,539 --> 00:33:48,159
agents run and populate data and then

00:33:46,779 --> 00:33:52,960
we'll proceed with the deployment to the

00:33:48,159 --> 00:33:55,450
API nodes so all our API nodes are for

00:33:52,960 --> 00:34:00,539
both our a and our B are behind the same

00:33:55,450 --> 00:34:03,759
load balancers so we kind of violate our

00:34:00,539 --> 00:34:06,190
don't give people inconsistent reads but

00:34:03,759 --> 00:34:07,899
we try and be in that state for as short

00:34:06,190 --> 00:34:09,569
a time as possible

00:34:07,899 --> 00:34:13,149
so as we fail over onto the other one

00:34:09,569 --> 00:34:14,560
then our pipeline proceeds it's it uses

00:34:13,149 --> 00:34:16,750
a sort of an expression or a

00:34:14,560 --> 00:34:19,000
conditionals check to figure out okay

00:34:16,750 --> 00:34:20,800
did I do this fail over deploy now I'm

00:34:19,000 --> 00:34:22,780
going to go and tear down all the old

00:34:20,800 --> 00:34:25,349
infrastructure that was associated with

00:34:22,780 --> 00:34:25,349
the other stack

00:34:26,720 --> 00:34:28,780

YouTube URL: https://www.youtube.com/watch?v=Cqk2QkYDlx8


