Title: Asher Feldman   Mark Gustafson  "OCA Firmware Delivery"
Publication date: 2018-10-24
Playlist: Spinnaker Summit 2018
Description: 
	
Captions: 
	00:00:01,200 --> 00:00:05,390
[Music]

00:00:09,780 --> 00:00:13,250
[Music]

00:00:16,039 --> 00:00:21,869
I'm mark gustafson I work on our open

00:00:19,080 --> 00:00:22,560
connect CDN and again I'll explain what

00:00:21,869 --> 00:00:24,539
all that is

00:00:22,560 --> 00:00:27,210
this is asher Feldman actually did most

00:00:24,539 --> 00:00:29,429
of the work and he'll be getting into

00:00:27,210 --> 00:00:31,109
the guts of what did we have to do to

00:00:29,429 --> 00:00:34,040
make spinnaker deal with something that

00:00:31,109 --> 00:00:37,200
it's definitely not the cloud provider

00:00:34,040 --> 00:00:39,930
so first we will run through I'll give

00:00:37,200 --> 00:00:42,149
those of you in the audience an

00:00:39,930 --> 00:00:44,040
introduction I won't introduce Netflix

00:00:42,149 --> 00:00:46,739
I'm hoping everyone understands what

00:00:44,040 --> 00:00:48,660
Netflix is what we do as a service if

00:00:46,739 --> 00:00:50,640
you don't know that then you talk to any

00:00:48,660 --> 00:00:52,949
after people here and we can explain

00:00:50,640 --> 00:00:54,780
what Netflix is but I don't expect

00:00:52,949 --> 00:00:57,180
people to necessarily know what open

00:00:54,780 --> 00:00:59,340
connect is so we'll run through that we

00:00:57,180 --> 00:01:02,160
will run through what we mean by our

00:00:59,340 --> 00:01:04,519
firmware deployment model and why we

00:01:02,160 --> 00:01:08,189
felt any need to change it whatsoever

00:01:04,519 --> 00:01:10,260
then do a little bit of history about

00:01:08,189 --> 00:01:12,799
where we came from and again why are you

00:01:10,260 --> 00:01:16,500
space is so very different from the most

00:01:12,799 --> 00:01:19,229
cloud instance continuous deployment

00:01:16,500 --> 00:01:20,970
models and then how we did end up

00:01:19,229 --> 00:01:23,370
changing it and how we are using

00:01:20,970 --> 00:01:24,840
spinnaker in production today

00:01:23,370 --> 00:01:27,150
then again I'll turn it over to Asher

00:01:24,840 --> 00:01:29,310
we'll get into some of the gory details

00:01:27,150 --> 00:01:33,690
for those of you who are really into

00:01:29,310 --> 00:01:36,180
spinnaker dev slash configuration so

00:01:33,690 --> 00:01:39,210
again just as an introduction so Netflix

00:01:36,180 --> 00:01:41,490
again we serve movies we delight our

00:01:39,210 --> 00:01:43,200
customers with movies and TV shows we

00:01:41,490 --> 00:01:45,690
stream video over the Internet

00:01:43,200 --> 00:01:47,760
up in the left you'll see that is your

00:01:45,690 --> 00:01:50,640
kind of Netflix device and then down

00:01:47,760 --> 00:01:52,890
here is where everyone generally at this

00:01:50,640 --> 00:01:54,960
conference is pretty familiar with the

00:01:52,890 --> 00:01:58,020
idea of Netflix and the idea of running

00:01:54,960 --> 00:02:00,960
Netflix within AWS and that is true for

00:01:58,020 --> 00:02:02,820
the vast majority a lot of interactions

00:02:00,960 --> 00:02:04,800
that the client has with our set of

00:02:02,820 --> 00:02:06,540
micro services they're all in what I'm

00:02:04,800 --> 00:02:08,580
controlling calling here at the control

00:02:06,540 --> 00:02:10,740
plane or what we call the control plane

00:02:08,580 --> 00:02:12,180
what people may or may not understand

00:02:10,740 --> 00:02:14,130
about our service though is we do

00:02:12,180 --> 00:02:16,170
differentiate when you have this upper

00:02:14,130 --> 00:02:18,150
layer the data plane which includes a

00:02:16,170 --> 00:02:20,370
content delivery network that we call

00:02:18,150 --> 00:02:22,680
the open connect so again just to make

00:02:20,370 --> 00:02:25,100
it even more obvious how this breaks

00:02:22,680 --> 00:02:27,989
down is everything up to the point

00:02:25,100 --> 00:02:29,760
before you start streaming when you're

00:02:27,989 --> 00:02:32,250
interacting with the app you're browsing

00:02:29,760 --> 00:02:34,739
for movies you've logged in etc all

00:02:32,250 --> 00:02:37,020
those interactions are with AWS but once

00:02:34,739 --> 00:02:39,000
you hit play and the client actually

00:02:37,020 --> 00:02:40,950
goes back to AWS and gets a little bit

00:02:39,000 --> 00:02:44,430
of steering information and it's going

00:02:40,950 --> 00:02:48,100
to start making requests directly to a

00:02:44,430 --> 00:02:52,000
data plane to open connect

00:02:48,100 --> 00:02:55,210
more open connect open connect is a

00:02:52,000 --> 00:02:57,880
Netflix original service that is the

00:02:55,210 --> 00:03:00,780
content delivery network purpose-built

00:02:57,880 --> 00:03:03,460
for delivering our content to users

00:03:00,780 --> 00:03:06,490
incredible efficiency accrediting and

00:03:03,460 --> 00:03:07,960
incredible scale what is a content

00:03:06,490 --> 00:03:08,730
delivery network you can just in case

00:03:07,960 --> 00:03:11,380
you don't know

00:03:08,730 --> 00:03:13,720
you know the idea is you take lots and

00:03:11,380 --> 00:03:15,820
lots of caching servers you place them

00:03:13,720 --> 00:03:17,620
all around the globe as many places as

00:03:15,820 --> 00:03:20,110
you're trying to get close to your end

00:03:17,620 --> 00:03:21,730
customers for Netflix our customers are

00:03:20,110 --> 00:03:24,040
basically everywhere in the world with

00:03:21,730 --> 00:03:26,070
some very very minor exceptions and so

00:03:24,040 --> 00:03:29,410
we are trying to get geographically

00:03:26,070 --> 00:03:31,510
distributed and inherent in the idea of

00:03:29,410 --> 00:03:33,730
a CDN is that you also have some amount

00:03:31,510 --> 00:03:37,470
of logic about how you actually direct

00:03:33,730 --> 00:03:40,180
client requests to the right cache and

00:03:37,470 --> 00:03:42,610
open connect again we are very

00:03:40,180 --> 00:03:44,410
purpose-built for our application where

00:03:42,610 --> 00:03:46,480
we know that customers are looking for a

00:03:44,410 --> 00:03:47,860
particular set of content so again we

00:03:46,480 --> 00:03:50,260
have the concept of steering and

00:03:47,860 --> 00:03:52,360
customers directly to the best cache

00:03:50,260 --> 00:03:55,080
that we know already has the content

00:03:52,360 --> 00:03:55,080
they're looking for

00:03:55,650 --> 00:04:00,840
a little bit of history again about how

00:03:58,349 --> 00:04:02,220
this kind of has evolved and again you

00:04:00,840 --> 00:04:03,900
know what the open connect

00:04:02,220 --> 00:04:06,480
infrastructure looks like relative to

00:04:03,900 --> 00:04:08,790
again other cloud services other micro

00:04:06,480 --> 00:04:11,069
services or even other simi enza's we

00:04:08,790 --> 00:04:14,400
started back in about 2007 doing

00:04:11,069 --> 00:04:15,750
streaming over the internet hub that

00:04:14,400 --> 00:04:17,579
honestly I can't tell you very much

00:04:15,750 --> 00:04:19,109
about because I wasn't there and it's

00:04:17,579 --> 00:04:20,759
almost not to many people I know

00:04:19,109 --> 00:04:24,750
actually actually worked on it that are

00:04:20,759 --> 00:04:26,880
still around and then we moved into a

00:04:24,750 --> 00:04:27,990
period of time several years where we

00:04:26,880 --> 00:04:30,150
did start to work with some of the

00:04:27,990 --> 00:04:32,660
largest content delivery networks in the

00:04:30,150 --> 00:04:36,090
world that are again multi-tenant server

00:04:32,660 --> 00:04:37,620
groups and we used multi CDN in order to

00:04:36,090 --> 00:04:39,180
make sure that we weren't you know we

00:04:37,620 --> 00:04:40,949
didn't even have a single point of

00:04:39,180 --> 00:04:43,500
failure on any of the even major

00:04:40,949 --> 00:04:45,960
services etc and the service was growing

00:04:43,500 --> 00:04:49,889
you know during these years and we could

00:04:45,960 --> 00:04:52,650
see pretty early on or as we went up

00:04:49,889 --> 00:04:55,830
that growth ramp that eventually our

00:04:52,650 --> 00:04:58,740
scale of doing video over the Internet

00:04:55,830 --> 00:05:00,630
was gonna be more than what's going to

00:04:58,740 --> 00:05:04,260
be comfortably provided even by the

00:05:00,630 --> 00:05:07,380
largest CDN or even all of the largest

00:05:04,260 --> 00:05:09,060
CDNs we're gonna have trouble handling

00:05:07,380 --> 00:05:11,940
the scale that we knew we could grow to

00:05:09,060 --> 00:05:13,800
and in fact that has been the case we've

00:05:11,940 --> 00:05:17,120
seen the growth since we brought open

00:05:13,800 --> 00:05:19,620
connect so we luckily did a great job of

00:05:17,120 --> 00:05:22,050
making the decision and designing a

00:05:19,620 --> 00:05:23,880
brand new purpose-built CDN from the

00:05:22,050 --> 00:05:28,800
ground up again starting in about 2011

00:05:23,880 --> 00:05:31,050
so that is what open connect is now part

00:05:28,800 --> 00:05:33,690
of again you know what goes into

00:05:31,050 --> 00:05:37,340
building your own CDN is in our case we

00:05:33,690 --> 00:05:40,830
also design our own hardware this is a

00:05:37,340 --> 00:05:41,970
purpose-built for our use cases where we

00:05:40,830 --> 00:05:45,000
kind of segregate

00:05:41,970 --> 00:05:47,520
two general classes of what we call low

00:05:45,000 --> 00:05:48,810
CAS or open connect appliances or I kind

00:05:47,520 --> 00:05:52,260
of skipped it over there but that is the

00:05:48,810 --> 00:05:54,330
OCA actual caching appliances this is an

00:05:52,260 --> 00:05:57,340
example of a storage appliance which

00:05:54,330 --> 00:05:59,680
again we have two basic models storage

00:05:57,340 --> 00:06:01,660
to the next one moment but basically

00:05:59,680 --> 00:06:04,150
this is for you know all of your content

00:06:01,660 --> 00:06:05,530
including all your tale content which

00:06:04,150 --> 00:06:08,260
again we shark the content across

00:06:05,530 --> 00:06:10,270
multiple boxes so it's not like every

00:06:08,260 --> 00:06:13,270
single box has the entire regional

00:06:10,270 --> 00:06:15,010
catalog but a set or stack of boxes in a

00:06:13,270 --> 00:06:17,440
given region will hold the entire

00:06:15,010 --> 00:06:19,150
catalog and so any given request will

00:06:17,440 --> 00:06:23,740
find someplace that you can serve it

00:06:19,150 --> 00:06:26,710
from within that stack then we also have

00:06:23,740 --> 00:06:28,180
flash or offload appliances Oh CAS and

00:06:26,710 --> 00:06:30,280
again the whole notion here as well

00:06:28,180 --> 00:06:33,000
these are all flash drives very high i/o

00:06:30,280 --> 00:06:36,610
we can get again we've published this

00:06:33,000 --> 00:06:39,880
publicly that we can get close to 100

00:06:36,610 --> 00:06:41,620
gigs off a single box it when again

00:06:39,880 --> 00:06:43,450
we've got lots of copies of the most

00:06:41,620 --> 00:06:45,790
popular content and our steering logic

00:06:43,450 --> 00:06:48,639
knows to basically put these boxes in

00:06:45,790 --> 00:06:51,039
front of the vast bulk of those requests

00:06:48,639 --> 00:06:53,260
that are coming in for really really

00:06:51,039 --> 00:06:54,970
popular content for those that again are

00:06:53,260 --> 00:06:57,280
not so popular it shouldn't even be

00:06:54,970 --> 00:06:59,110
found on a flash drive so that request

00:06:57,280 --> 00:07:00,789
would never even go there instead that

00:06:59,110 --> 00:07:05,500
would get shuttled back to a storage

00:07:00,789 --> 00:07:06,760
server so this combination of basically

00:07:05,500 --> 00:07:09,460
flash Oca

00:07:06,760 --> 00:07:11,289
and storage o CAS forhead content and

00:07:09,460 --> 00:07:14,169
the broad tale that basically makes up

00:07:11,289 --> 00:07:16,780
you know our our physical infrastructure

00:07:14,169 --> 00:07:19,060
for actually serving it and we'll talk

00:07:16,780 --> 00:07:20,889
it a little bit about how we you know

00:07:19,060 --> 00:07:24,190
kind of need to manage those or not but

00:07:20,889 --> 00:07:25,960
first I'll also talk about the design of

00:07:24,190 --> 00:07:28,360
the content delivery network itself

00:07:25,960 --> 00:07:30,070
besides designing our own Hardware the

00:07:28,360 --> 00:07:31,389
other thing that we really wanted to do

00:07:30,070 --> 00:07:32,919
that it's pretty fundamental to the

00:07:31,389 --> 00:07:35,050
design of open connect because we wanted

00:07:32,919 --> 00:07:38,020
it to be as distributed and well

00:07:35,050 --> 00:07:39,639
connected as possible so I will get into

00:07:38,020 --> 00:07:42,880
exactly what that does look like but

00:07:39,639 --> 00:07:45,700
that number even 95% of our traffic is

00:07:42,880 --> 00:07:48,370
directly connected that's a safe number

00:07:45,700 --> 00:07:49,950
meaning it's if anything it's probably

00:07:48,370 --> 00:07:52,150
even a couple of percentage points

00:07:49,950 --> 00:07:55,660
closer to hundred percent we're not

00:07:52,150 --> 00:07:58,610
quite 100% directly connected to all of

00:07:55,660 --> 00:08:01,639
our end-user customers networks

00:07:58,610 --> 00:08:03,530
we are very very close and that's very

00:08:01,639 --> 00:08:06,080
very deliberate and again we'll get into

00:08:03,530 --> 00:08:08,210
exactly why that's important and how

00:08:06,080 --> 00:08:10,069
that manifests itself in both a network

00:08:08,210 --> 00:08:11,449
design and also how we actually manage

00:08:10,069 --> 00:08:15,979
deployments because it does have

00:08:11,449 --> 00:08:17,960
important implications so again one way

00:08:15,979 --> 00:08:20,389
that we achieve that is we make it

00:08:17,960 --> 00:08:24,139
really really low friction for all of

00:08:20,389 --> 00:08:26,330
the end-user ISPs to basically get open

00:08:24,139 --> 00:08:28,639
connect appliances in their edge

00:08:26,330 --> 00:08:31,490
networks again basically directly

00:08:28,639 --> 00:08:33,740
connected to the users we can give them

00:08:31,490 --> 00:08:35,630
appliances we provide it it becomes

00:08:33,740 --> 00:08:39,140
their hardware they have to provide

00:08:35,630 --> 00:08:41,329
space and power to keep it online and we

00:08:39,140 --> 00:08:43,010
manage it for them we will directly

00:08:41,329 --> 00:08:45,470
connect to it and they can monitor and

00:08:43,010 --> 00:08:47,269
manage and handle firmware upgrades but

00:08:45,470 --> 00:08:51,980
basically it's their server in their

00:08:47,269 --> 00:08:54,890
location and in addition to that we also

00:08:51,980 --> 00:08:56,600
provide very very dense connectivity so

00:08:54,890 --> 00:08:59,180
that's kind of this what I'll call the

00:08:56,600 --> 00:09:01,279
sparse connectivity about the edge well

00:08:59,180 --> 00:09:03,110
provide very dense connectivity at

00:09:01,279 --> 00:09:05,120
internet exchange locations throughout

00:09:03,110 --> 00:09:06,829
the world and if you're not familiar

00:09:05,120 --> 00:09:09,440
with what an internet exchange location

00:09:06,829 --> 00:09:13,220
is here's an example so you probably

00:09:09,440 --> 00:09:14,959
can't unless you're really really into

00:09:13,220 --> 00:09:16,820
networking you probably put out what

00:09:14,959 --> 00:09:19,310
building that happens to be it's just a

00:09:16,820 --> 00:09:22,040
nondescript great building if you knew

00:09:19,310 --> 00:09:24,260
it it is the london internet exchange if

00:09:22,040 --> 00:09:26,060
you went inside you'd see lots of this

00:09:24,260 --> 00:09:27,920
stuff basically a lot like any other

00:09:26,060 --> 00:09:30,529
data center with rows and rows and rows

00:09:27,920 --> 00:09:32,240
of caged servers but what's unusual

00:09:30,529 --> 00:09:34,279
about an internet exchange point is

00:09:32,240 --> 00:09:36,829
they're specifically there to connect to

00:09:34,279 --> 00:09:40,339
each other it's not just Colo for a

00:09:36,829 --> 00:09:42,980
large AWS or or Google or Azure

00:09:40,339 --> 00:09:45,170
build-out it specifically places where

00:09:42,980 --> 00:09:47,750
networks can come together and basically

00:09:45,170 --> 00:09:50,420
have ports in close proximity and

00:09:47,750 --> 00:09:52,010
directly connect the networks and

00:09:50,420 --> 00:09:54,900
there's again a number of these around

00:09:52,010 --> 00:09:57,390
the world we'll see a little bit

00:09:54,900 --> 00:09:59,940
so here's an example of an

00:09:57,390 --> 00:10:01,110
implementation inside of an IX we're off

00:09:59,940 --> 00:10:03,779
to the left you actually see some

00:10:01,110 --> 00:10:05,760
networking equipment again just a pair

00:10:03,779 --> 00:10:08,010
they're mostly for redundancy and we see

00:10:05,760 --> 00:10:11,040
a couple of stats again stacks of flash

00:10:08,010 --> 00:10:13,290
servers and snaps of storage servers and

00:10:11,040 --> 00:10:16,860
this is an example of a fairly typical

00:10:13,290 --> 00:10:19,500
size and again IX locations where we are

00:10:16,860 --> 00:10:22,650
connecting directly to hundreds of other

00:10:19,500 --> 00:10:24,839
networks in a single location can very

00:10:22,650 --> 00:10:30,900
well have hundreds of servers hundreds

00:10:24,839 --> 00:10:33,810
of OSI A's in a single site again just

00:10:30,900 --> 00:10:36,029
to make it even more obvious so if we

00:10:33,810 --> 00:10:37,500
have our end users over here one of the

00:10:36,029 --> 00:10:39,870
ways that they can be most readily

00:10:37,500 --> 00:10:41,940
directly connected to us is their going

00:10:39,870 --> 00:10:44,670
upstream immediately to their ISP who

00:10:41,940 --> 00:10:46,230
has an OSI a on-premise in their data

00:10:44,670 --> 00:10:48,330
center basically as close as anything

00:10:46,230 --> 00:10:49,920
else on the Internet can be and they

00:10:48,330 --> 00:10:52,529
when they're streaming their content are

00:10:49,920 --> 00:10:56,220
getting it directly from there we do

00:10:52,529 --> 00:10:58,290
always for the sake of resiliency want

00:10:56,220 --> 00:11:00,720
to make sure that every client has more

00:10:58,290 --> 00:11:03,660
than one spot they can go to retrieve

00:11:00,720 --> 00:11:06,690
content so even if that OCA does go down

00:11:03,660 --> 00:11:09,360
and if there were only one dossier in a

00:11:06,690 --> 00:11:11,040
site which is possible and if the OSI

00:11:09,360 --> 00:11:13,620
needed to be rebooted we'll get to that

00:11:11,040 --> 00:11:16,230
a moment then they are there is upstream

00:11:13,620 --> 00:11:19,350
connectivity so a user over here again

00:11:16,230 --> 00:11:22,170
if we take a hypothetical example of a

00:11:19,350 --> 00:11:24,750
family of four who wants to start

00:11:22,170 --> 00:11:27,420
streaming a new hour-long movie every 15

00:11:24,750 --> 00:11:29,790
minutes on four different devices if I

00:11:27,420 --> 00:11:31,620
actually took that Oca down they might

00:11:29,790 --> 00:11:34,350
not even notice that anything happened

00:11:31,620 --> 00:11:36,060
right they'll simply fetched from

00:11:34,350 --> 00:11:39,600
someplace upstream and again as long as

00:11:36,060 --> 00:11:41,760
that upstream connectivity is decent

00:11:39,600 --> 00:11:43,850
enough family they won't notice that

00:11:41,760 --> 00:11:46,860
they're actually streaming from the is

00:11:43,850 --> 00:11:49,320
in this case again if that och went out

00:11:46,860 --> 00:11:50,760
of the picture or maybe it was never

00:11:49,320 --> 00:11:52,890
there in the first place again there are

00:11:50,760 --> 00:11:55,320
some ISPs that only directly connect to

00:11:52,890 --> 00:11:58,110
us in the i-x and don't necessarily have

00:11:55,320 --> 00:11:59,400
local au cas the user doesn't

00:11:58,110 --> 00:12:01,890
necessarily know

00:11:59,400 --> 00:12:04,050
now the ISP probably definitely does

00:12:01,890 --> 00:12:05,940
know and we'll talk about that where

00:12:04,050 --> 00:12:09,540
again especially in those cases where

00:12:05,940 --> 00:12:12,000
you have an OCA on-site at an ISP they

00:12:09,540 --> 00:12:14,310
care very deeply about maintaining that

00:12:12,000 --> 00:12:16,080
traffic locally that helps their fixed

00:12:14,310 --> 00:12:17,790
cost get amortized over a certain amount

00:12:16,080 --> 00:12:20,130
of traffic they don't have to pay

00:12:17,790 --> 00:12:22,230
marginal costs or deal with again a

00:12:20,130 --> 00:12:23,580
potentially scarce resource in terms of

00:12:22,230 --> 00:12:29,610
their upstream bandwidth all the way

00:12:23,580 --> 00:12:32,040
back to the i-x so here's a latest

00:12:29,610 --> 00:12:33,930
snapshot of again our global deployments

00:12:32,040 --> 00:12:35,970
around the world you can kind of see

00:12:33,930 --> 00:12:38,820
there's slightly larger orange dots

00:12:35,970 --> 00:12:43,800
those are IX locations all the green

00:12:38,820 --> 00:12:46,620
dots of various sizes are individual ISP

00:12:43,800 --> 00:12:50,190
locations don't try counting them I'll

00:12:46,620 --> 00:12:52,980
give you a hint there's thousands so

00:12:50,190 --> 00:12:54,660
thousands of individual sites and in

00:12:52,980 --> 00:12:56,400
every site there's at least one box

00:12:54,660 --> 00:12:59,610
sometimes only one box sometimes

00:12:56,400 --> 00:13:01,650
hundreds so we have many thousands of

00:12:59,610 --> 00:13:05,100
ossie a's around the around the world

00:13:01,650 --> 00:13:08,310
and we do our best to try to think of

00:13:05,100 --> 00:13:09,720
managing them as a fleet again if we go

00:13:08,310 --> 00:13:12,060
back to the paradigm about how you

00:13:09,720 --> 00:13:14,779
manage your servers you'll herds versus

00:13:12,060 --> 00:13:19,140
pets we'd like to think of them as herds

00:13:14,779 --> 00:13:23,520
but the downside is again when you're

00:13:19,140 --> 00:13:25,620
one ISP in a remote area of Brazil which

00:13:23,520 --> 00:13:28,230
there are a number and maybe you don't

00:13:25,620 --> 00:13:29,400
have enough traffic to really justify

00:13:28,230 --> 00:13:32,850
more than one Oca

00:13:29,400 --> 00:13:36,270
the map OCA is a pet and you care very

00:13:32,850 --> 00:13:37,870
deeply about every minute of uptime on

00:13:36,270 --> 00:13:41,680
that OCA

00:13:37,870 --> 00:13:44,110
so that now gets us into well we've been

00:13:41,680 --> 00:13:45,790
managing firmware updates on these OCS

00:13:44,110 --> 00:13:47,620
for as long as we've had the open

00:13:45,790 --> 00:13:49,660
connect service you know we periodically

00:13:47,620 --> 00:13:51,520
have to modify the software and buy

00:13:49,660 --> 00:13:53,110
firmware again it's a little bit

00:13:51,520 --> 00:13:54,610
different it doesn't just mean chip

00:13:53,110 --> 00:13:56,890
firmware although it can't go down to

00:13:54,610 --> 00:13:59,110
the level of possibly updating the image

00:13:56,890 --> 00:14:02,320
on a NIC controller on a hard disk

00:13:59,110 --> 00:14:04,570
controller right as well as the base OS

00:14:02,320 --> 00:14:06,700
which is FreeBSD in our case as well as

00:14:04,570 --> 00:14:09,730
any application stack which is basically

00:14:06,700 --> 00:14:12,810
nginx for the main web server and a

00:14:09,730 --> 00:14:15,460
variety of other again pretty fixed

00:14:12,810 --> 00:14:17,920
images of software that we run all of

00:14:15,460 --> 00:14:20,440
our proprietary tooling and everything

00:14:17,920 --> 00:14:24,340
on the box is what we call the firmware

00:14:20,440 --> 00:14:27,040
so this is a bare metal base OS and

00:14:24,340 --> 00:14:31,120
application image that's applied to the

00:14:27,040 --> 00:14:32,830
box when we need to change it so we've

00:14:31,120 --> 00:14:34,870
been doing this again for as long as

00:14:32,830 --> 00:14:36,970
we've had open connected we've had to be

00:14:34,870 --> 00:14:39,220
able to manage these boxes out in the

00:14:36,970 --> 00:14:41,560
field update them periodically and so

00:14:39,220 --> 00:14:44,310
we've built a number of tools over the

00:14:41,560 --> 00:14:47,530
years primarily using Python scripts

00:14:44,310 --> 00:14:49,480
primarily operating against again kind

00:14:47,530 --> 00:14:51,700
of a core toolset that here we

00:14:49,480 --> 00:14:53,530
internally we call it just CDN admin

00:14:51,700 --> 00:14:55,660
this is a set of api's and things that

00:14:53,530 --> 00:14:58,120
were built to manage the state and keep

00:14:55,660 --> 00:15:02,800
track of the state of every cache in our

00:14:58,120 --> 00:15:05,230
system now as we've grown over the years

00:15:02,800 --> 00:15:06,940
you know this has become more complex we

00:15:05,230 --> 00:15:09,820
will talk a little bit about some of the

00:15:06,940 --> 00:15:12,580
variants that we've had to account for

00:15:09,820 --> 00:15:15,270
and just other complexity that comes

00:15:12,580 --> 00:15:18,670
into play as we get just more and more

00:15:15,270 --> 00:15:21,190
OC a's out in the field and just other

00:15:18,670 --> 00:15:23,920
aspects of that backend data system that

00:15:21,190 --> 00:15:25,870
really helps see them add them together

00:15:23,920 --> 00:15:29,620
that we felt it really did need to

00:15:25,870 --> 00:15:31,260
become it needed to get refactored so as

00:15:29,620 --> 00:15:35,560
we were looking at making that change

00:15:31,260 --> 00:15:37,660
now basically just about a year ago we

00:15:35,560 --> 00:15:39,440
basically were looking at okay if we're

00:15:37,660 --> 00:15:42,530
gonna have to do this refactoring

00:15:39,440 --> 00:15:44,060
what other types of things should we

00:15:42,530 --> 00:15:45,320
even consider in terms of how we do it

00:15:44,060 --> 00:15:47,690
did we just rewrite everything from

00:15:45,320 --> 00:15:50,750
scratch on a new set of api's and

00:15:47,690 --> 00:15:52,670
changed nothing else or do we take the

00:15:50,750 --> 00:15:54,560
bigger step and actually move on to what

00:15:52,670 --> 00:15:56,210
we could consider to be a stronger

00:15:54,560 --> 00:15:57,680
foundation and that's exactly where

00:15:56,210 --> 00:15:59,690
spinnaker comes in and that's exactly

00:15:57,680 --> 00:16:01,520
how we end up at this conference is

00:15:59,690 --> 00:16:03,590
spinnaker you know within the last

00:16:01,520 --> 00:16:06,320
couple of years and especially within

00:16:03,590 --> 00:16:08,690
the last year as absolutely become kind

00:16:06,320 --> 00:16:11,540
of the paved path within Netflix a lot

00:16:08,690 --> 00:16:13,700
of support a lot of momentum and it just

00:16:11,540 --> 00:16:17,180
again gets used for all kinds of

00:16:13,700 --> 00:16:19,730
workloads now not for our workload and

00:16:17,180 --> 00:16:21,950
again our workload is very very

00:16:19,730 --> 00:16:23,900
different and again natural going to

00:16:21,950 --> 00:16:25,280
again some of those nitty gritty details

00:16:23,900 --> 00:16:28,370
but I think you can again kind of see

00:16:25,280 --> 00:16:32,450
how you're managing a number of hundreds

00:16:28,370 --> 00:16:35,720
of potentially distinct unique OCS out

00:16:32,450 --> 00:16:38,150
in these very sparsely populated edge

00:16:35,720 --> 00:16:39,650
locations it's a bit different than how

00:16:38,150 --> 00:16:43,220
you generally manage your cloud

00:16:39,650 --> 00:16:44,470
instances or instances about my career

00:16:43,220 --> 00:16:46,720
service

00:16:44,470 --> 00:16:49,630
so I'll go into a little bit about you

00:16:46,720 --> 00:16:52,360
know what how we how we did used to

00:16:49,630 --> 00:16:54,940
handle it well okay we again I already

00:16:52,360 --> 00:16:56,680
shared its Python scripts developed and

00:16:54,940 --> 00:16:58,660
maintained within our relatively small

00:16:56,680 --> 00:17:01,420
LCD and Augustine basically all about

00:16:58,660 --> 00:17:06,640
command line ribbon stuff within a bash

00:17:01,420 --> 00:17:08,709
shell it again was fairly manual in

00:17:06,640 --> 00:17:10,540
terms of again as an operator you run

00:17:08,709 --> 00:17:12,040
the scripts you'll see in a moment

00:17:10,540 --> 00:17:14,260
you'll get a certain amount of output

00:17:12,040 --> 00:17:16,540
effectively console output that you can

00:17:14,260 --> 00:17:17,949
look at it's also all being logged so if

00:17:16,540 --> 00:17:19,300
you do need to go back and look at

00:17:17,949 --> 00:17:23,740
something you can go check through the

00:17:19,300 --> 00:17:25,270
log grip it or tail it but that trying

00:17:23,740 --> 00:17:27,520
to actually look at well what's the

00:17:25,270 --> 00:17:29,080
overall status of my deployment cuz

00:17:27,520 --> 00:17:31,450
again if we now take a step back and

00:17:29,080 --> 00:17:33,430
think about deployments do take a number

00:17:31,450 --> 00:17:36,010
of days we do them in batches we do them

00:17:33,430 --> 00:17:38,260
in sets we start with small potentially

00:17:36,010 --> 00:17:40,240
pilot sets make sure that everything is

00:17:38,260 --> 00:17:42,160
looking good and then by the time we

00:17:40,240 --> 00:17:45,190
actually get out to those ISPs that have

00:17:42,160 --> 00:17:46,960
a single box we want to know again that

00:17:45,190 --> 00:17:48,760
it's already been proven in the i-x

00:17:46,960 --> 00:17:51,460
locations it is a little easier for us

00:17:48,760 --> 00:17:53,220
to recover in ix locations where we have

00:17:51,460 --> 00:17:55,810
a little bit more kind of hands on

00:17:53,220 --> 00:17:58,360
remote smart hands without having to go

00:17:55,810 --> 00:18:01,230
and actually involve the technicians

00:17:58,360 --> 00:18:05,549
from the ISPs things of that nature

00:18:01,230 --> 00:18:08,279
so it's again a very progressive rollout

00:18:05,549 --> 00:18:11,429
with a lot of quality checks as we go

00:18:08,279 --> 00:18:14,460
through it and really seeing that status

00:18:11,429 --> 00:18:16,590
knowing exactly how many have failed and

00:18:14,460 --> 00:18:18,149
then again across many thousands of

00:18:16,590 --> 00:18:19,980
servers every time that we do roll this

00:18:18,149 --> 00:18:22,649
out there'll be some failures whether

00:18:19,980 --> 00:18:24,929
it's just an issue with the disk disk on

00:18:22,649 --> 00:18:27,090
the box or something else went wrong or

00:18:24,929 --> 00:18:28,620
quite frankly maybe the ISP had a

00:18:27,090 --> 00:18:30,299
network carrier issue and they weren't

00:18:28,620 --> 00:18:31,889
able to down download the firmware

00:18:30,299 --> 00:18:34,230
properly the first time

00:18:31,889 --> 00:18:36,809
etc a number of things can go wrong and

00:18:34,230 --> 00:18:39,690
if you do something enough thousands of

00:18:36,809 --> 00:18:42,510
times it will go wrong it may or may not

00:18:39,690 --> 00:18:43,950
be recoverable again sometimes boxes

00:18:42,510 --> 00:18:46,019
actually go bad we actually have to

00:18:43,950 --> 00:18:48,000
submit them to get returned back to us

00:18:46,019 --> 00:18:49,440
there we again we have some boxes out

00:18:48,000 --> 00:18:54,630
there six years old they aren't gonna

00:18:49,440 --> 00:18:56,760
last forever just as another example of

00:18:54,630 --> 00:18:58,679
again kind of what our what we would be

00:18:56,760 --> 00:19:01,230
looking at and again don't try to read

00:18:58,679 --> 00:19:03,570
it it's just console bash to shell out

00:19:01,230 --> 00:19:05,669
but but the highlight is that basically

00:19:03,570 --> 00:19:07,080
we would run these custom scripts that

00:19:05,669 --> 00:19:09,600
would give us a couple of different

00:19:07,080 --> 00:19:11,610
panes of information one would be about

00:19:09,600 --> 00:19:13,080
the actual upgrade engine kind of

00:19:11,610 --> 00:19:14,760
background and then the lower part is

00:19:13,080 --> 00:19:16,590
basically all about the scheduling and

00:19:14,760 --> 00:19:18,659
again what has happened so depending on

00:19:16,590 --> 00:19:20,580
exactly how large of a batch you may

00:19:18,659 --> 00:19:22,320
have addressed within the last you know

00:19:20,580 --> 00:19:24,899
n number of minutes you might see the

00:19:22,320 --> 00:19:27,720
last 10 minutes maybe up to an hour of

00:19:24,899 --> 00:19:29,429
the most recent information to go any

00:19:27,720 --> 00:19:31,470
farther back from that back than that

00:19:29,429 --> 00:19:33,419
again you're looking in through log

00:19:31,470 --> 00:19:35,340
files and again we would have the

00:19:33,419 --> 00:19:37,169
ability to also set things up to

00:19:35,340 --> 00:19:39,840
schedule them so you could again set up

00:19:37,169 --> 00:19:41,669
roughly four days or even a week's worth

00:19:39,840 --> 00:19:43,440
of activities but then would actually

00:19:41,669 --> 00:19:45,450
continue to run so you could disconnect

00:19:43,440 --> 00:19:49,110
from this reconnect but basically a

00:19:45,450 --> 00:19:49,990
long-running script is basically what

00:19:49,110 --> 00:19:55,600
our

00:19:49,990 --> 00:19:58,210
past deployment method was and again in

00:19:55,600 --> 00:20:00,730
terms of you know how we had to interact

00:19:58,210 --> 00:20:02,320
with this as as operators we did have a

00:20:00,730 --> 00:20:04,419
number of again manual checks that we

00:20:02,320 --> 00:20:05,890
would go through before we start we do

00:20:04,419 --> 00:20:07,840
go through the process every time of

00:20:05,890 --> 00:20:10,539
charting the OCS because and we have

00:20:07,840 --> 00:20:12,850
this fairly significant resiliency rules

00:20:10,539 --> 00:20:14,380
so again if you have a small site you

00:20:12,850 --> 00:20:17,649
don't want to take all the boxes down at

00:20:14,380 --> 00:20:19,450
once I'm pretty obvious we have

00:20:17,649 --> 00:20:21,700
sometimes specialized roles again I

00:20:19,450 --> 00:20:23,590
won't really go into them yeah I refer

00:20:21,700 --> 00:20:25,840
to them and there's boxes at our fill

00:20:23,590 --> 00:20:27,340
site boxes with special versions of ours

00:20:25,840 --> 00:20:29,559
of our firm where they're called no

00:20:27,340 --> 00:20:30,970
delete again I won't dig into all those

00:20:29,559 --> 00:20:32,559
but just take my word for it that they

00:20:30,970 --> 00:20:35,169
are variants that have to be handled

00:20:32,559 --> 00:20:37,720
especially so we would learn how to do

00:20:35,169 --> 00:20:39,909
that or trigger them separately the

00:20:37,720 --> 00:20:41,830
other thing that's interesting is again

00:20:39,909 --> 00:20:45,039
notifications to ISP partners because

00:20:41,830 --> 00:20:45,730
again you know these ISPs care deeply

00:20:45,039 --> 00:20:47,620
about

00:20:45,730 --> 00:20:49,450
every single box in their infrastructure

00:20:47,620 --> 00:20:52,480
and how well they can keep that traffic

00:20:49,450 --> 00:20:54,309
local so they want a lot of notification

00:20:52,480 --> 00:20:56,559
also some of these ISPs are very large

00:20:54,309 --> 00:20:59,590
high speeds you know you're kind of

00:20:56,559 --> 00:21:02,440
income until nationalized telcos that

00:20:59,590 --> 00:21:03,880
are very very processed driven and so

00:21:02,440 --> 00:21:05,710
the idea that you're gonna have

00:21:03,880 --> 00:21:06,909
maintenance without telling them about

00:21:05,710 --> 00:21:08,289
it days in advance

00:21:06,909 --> 00:21:10,990
and having it line up with their

00:21:08,289 --> 00:21:13,120
maintenance window you know it's not

00:21:10,990 --> 00:21:15,100
gonna fly so you give them in advance

00:21:13,120 --> 00:21:16,360
you you tell them exactly when it's

00:21:15,100 --> 00:21:18,070
gonna happen you make sure it stays in

00:21:16,360 --> 00:21:20,230
advance they give you a ticket back and

00:21:18,070 --> 00:21:21,970
blah blah blah and the point is that all

00:21:20,230 --> 00:21:25,120
of our scripts would have already

00:21:21,970 --> 00:21:26,500
handled that and then again checking the

00:21:25,120 --> 00:21:28,750
status and again responding to issues

00:21:26,500 --> 00:21:29,860
because there are gonna be failures so

00:21:28,750 --> 00:21:31,840
we would have automatic ticket

00:21:29,860 --> 00:21:33,250
generation we would be able to track

00:21:31,840 --> 00:21:36,250
through and follow up with every single

00:21:33,250 --> 00:21:38,119
OCA that didn't manage to not come back

00:21:36,250 --> 00:21:39,979
successfully or not get upgraded

00:21:38,119 --> 00:21:43,909
Cecily and then we would have a post

00:21:39,979 --> 00:21:46,009
upgrade checklist so how's that

00:21:43,909 --> 00:21:47,959
different now that we can actually use

00:21:46,009 --> 00:21:50,299
spinnaker again Asheville go into all

00:21:47,959 --> 00:21:52,039
the details about how we translated

00:21:50,299 --> 00:21:55,449
those same sets of requirements over but

00:21:52,039 --> 00:21:58,039
basically we did map and so far we have

00:21:55,449 --> 00:22:00,319
mostly just replicated the same overall

00:21:58,039 --> 00:22:02,269
process but I'll also give you a preview

00:22:00,319 --> 00:22:03,709
that part of the reason why we did it is

00:22:02,269 --> 00:22:06,199
that we think that as we move forward in

00:22:03,709 --> 00:22:08,209
the future and we actually kind of think

00:22:06,199 --> 00:22:10,219
out of the box about how to manage these

00:22:08,209 --> 00:22:12,409
deployments it'll be much easier to

00:22:10,219 --> 00:22:14,569
iterate and much easier to change within

00:22:12,409 --> 00:22:17,989
spinnaker than having to go back and

00:22:14,569 --> 00:22:20,179
again just rewrite things in Python so

00:22:17,989 --> 00:22:22,339
in short we we still have a shardene

00:22:20,179 --> 00:22:23,929
step where we have a pipeline that goes

00:22:22,339 --> 00:22:26,209
through and again applies these very

00:22:23,929 --> 00:22:29,299
specific resiliency rules about exactly

00:22:26,209 --> 00:22:33,439
how many in which sets of OSI A's can be

00:22:29,299 --> 00:22:36,709
operated on at a given point in time and

00:22:33,439 --> 00:22:38,599
then once we're actually ready to deploy

00:22:36,709 --> 00:22:40,579
the firmware we have a deployment

00:22:38,599 --> 00:22:44,059
pipeline that again takes in and the

00:22:40,579 --> 00:22:45,889
main point here if these is not to look

00:22:44,059 --> 00:22:48,379
at when all the parameters are but this

00:22:45,889 --> 00:22:50,689
there's a few parameters that do specify

00:22:48,379 --> 00:22:53,329
we we name the batches we can specify

00:22:50,689 --> 00:22:55,729
start times so that again it's going to

00:22:53,329 --> 00:22:58,879
take place during only during trough

00:22:55,729 --> 00:23:01,009
hours in local time again as we'll talk

00:22:58,879 --> 00:23:02,179
about how that becomes interesting

00:23:01,009 --> 00:23:04,879
complexity because again we deal with

00:23:02,179 --> 00:23:06,919
every single time zone in the world we

00:23:04,879 --> 00:23:08,779
do specify or to get a firmware version

00:23:06,919 --> 00:23:12,589
we also attach tickets to it you can

00:23:08,779 --> 00:23:14,679
apply additional scoping this be your

00:23:12,589 --> 00:23:17,479
ready to push the button and run and

00:23:14,679 --> 00:23:19,339
then as as it goes through it basically

00:23:17,479 --> 00:23:20,449
turns in all the various stages and

00:23:19,339 --> 00:23:22,879
again if you guys are familiar with

00:23:20,449 --> 00:23:24,979
spinnaker I think most of you are it's a

00:23:22,879 --> 00:23:27,739
fairly standard UI basically telling you

00:23:24,979 --> 00:23:28,370
what it is operating on or what's coming

00:23:27,739 --> 00:23:33,200
up next

00:23:28,370 --> 00:23:34,220
in the prepared of great plan so that's

00:23:33,200 --> 00:23:36,620
the overview Oh

00:23:34,220 --> 00:23:38,360
most of the overviews ash will go into

00:23:36,620 --> 00:23:42,320
the deep deep details I do want to back

00:23:38,360 --> 00:23:43,610
up and note so what all you know again

00:23:42,320 --> 00:23:46,250
is involved in this effort

00:23:43,610 --> 00:23:48,309
how much dev time did it take well it

00:23:46,250 --> 00:23:51,409
was only about a year ago that basically

00:23:48,309 --> 00:23:55,309
Dayton and Andy Glover even talked about

00:23:51,409 --> 00:23:57,740
my boss's deed by the way they talked

00:23:55,309 --> 00:23:59,600
about us even wanting to do this we

00:23:57,740 --> 00:24:01,909
gathered requirements again in queue for

00:23:59,600 --> 00:24:04,100
last year basically it was q1 of this

00:24:01,909 --> 00:24:07,250
year for basically one dev and that's

00:24:04,100 --> 00:24:08,990
basically Asher after who did most of

00:24:07,250 --> 00:24:13,190
the work we have been doing a certain

00:24:08,990 --> 00:24:14,840
amount a slow rollout since then and we

00:24:13,190 --> 00:24:17,450
had some dependencies over the summer in

00:24:14,840 --> 00:24:18,740
terms of again new API endpoints that we

00:24:17,450 --> 00:24:21,049
actually needed as dependencies that

00:24:18,740 --> 00:24:22,789
weren't necessarily ready yet so that's

00:24:21,049 --> 00:24:24,799
the only reason why we're basically not

00:24:22,789 --> 00:24:26,210
you know ready to basically say we're a

00:24:24,799 --> 00:24:27,980
total feature parody and we're basically

00:24:26,210 --> 00:24:30,289
ready to go into maintenance mode or

00:24:27,980 --> 00:24:32,270
minor feature updates so we do still

00:24:30,289 --> 00:24:33,830
have something to where we're but we are

00:24:32,270 --> 00:24:35,630
now able to use it for all of our

00:24:33,830 --> 00:24:38,450
batches we've again been running it in

00:24:35,630 --> 00:24:41,649
production for the fleet for several

00:24:38,450 --> 00:24:43,909
months now and again it's been great and

00:24:41,649 --> 00:24:48,110
with that I'll actually hand it over to

00:24:43,909 --> 00:24:50,059
after talking about how we did it thank

00:24:48,110 --> 00:24:52,039
you Mark and yeah we got regarding that

00:24:50,059 --> 00:24:54,559
timeline everything about the cover took

00:24:52,039 --> 00:24:56,630
about eight weeks of development time

00:24:54,559 --> 00:24:59,539
spread out over quarter with plenty of

00:24:56,630 --> 00:25:01,820
interruptions with some perhaps four

00:24:59,539 --> 00:25:04,399
weeks or so follow-up work in the

00:25:01,820 --> 00:25:07,960
subsequent quarter I'm just adding some

00:25:04,399 --> 00:25:11,809
UI polish and additional features so

00:25:07,960 --> 00:25:14,240
initial concerns when when approaching

00:25:11,809 --> 00:25:16,309
this problem set for the first time as

00:25:14,240 --> 00:25:18,230
well if you if you look at the history

00:25:16,309 --> 00:25:20,830
spinnaker is really really developed

00:25:18,230 --> 00:25:24,730
around Netflix's cloud delivery needs

00:25:20,830 --> 00:25:28,330
and club delivery pattern so we've got

00:25:24,730 --> 00:25:31,039
deployment strategies like red black

00:25:28,330 --> 00:25:33,440
immutable infrastructure clusters of

00:25:31,039 --> 00:25:35,899
applications contain ephemeral server

00:25:33,440 --> 00:25:38,750
groups made up of even more ephemeral

00:25:35,899 --> 00:25:40,640
instances if something goes wrong with a

00:25:38,750 --> 00:25:42,620
software deploy

00:25:40,640 --> 00:25:44,960
it's not really a big deal you can you

00:25:42,620 --> 00:25:47,510
can always roll forward or quickly roll

00:25:44,960 --> 00:25:50,960
back if you deployed with a red-black if

00:25:47,510 --> 00:25:53,750
a bad application build goes out and you

00:25:50,960 --> 00:25:56,059
have maybe broken server groups left

00:25:53,750 --> 00:25:58,429
left over that doesn't really matter

00:25:56,059 --> 00:25:59,659
either you can roll forward and

00:25:58,429 --> 00:26:03,470
eventually we're just automatically

00:25:59,659 --> 00:26:05,809
going to get cleaned up unfortunately if

00:26:03,470 --> 00:26:08,510
OCA is where we're left out of service

00:26:05,809 --> 00:26:10,730
due to a failed failed deployment for

00:26:08,510 --> 00:26:13,549
any reason that could could impact

00:26:10,730 --> 00:26:16,549
streaming availability or perhaps more

00:26:13,549 --> 00:26:19,789
importantly given the large amount of

00:26:16,549 --> 00:26:22,279
redundancy and capacity and the open

00:26:19,789 --> 00:26:26,750
connectively it could could impact

00:26:22,279 --> 00:26:31,039
customers of individual ISPs so if an

00:26:26,750 --> 00:26:34,039
ISP is sir serving thirty five percent

00:26:31,039 --> 00:26:35,929
of its network traffic to Netflix

00:26:34,039 --> 00:26:37,820
streaming customers on their network and

00:26:35,929 --> 00:26:39,799
suddenly that bandwidth was going

00:26:37,820 --> 00:26:41,480
outside of their network that could

00:26:39,799 --> 00:26:46,090
impact all of their customers whether or

00:26:41,480 --> 00:26:48,440
not they happen to be streaming Netflix

00:26:46,090 --> 00:26:51,380
operations must be idempotent and

00:26:48,440 --> 00:26:55,039
resilient idempotent infrastructure

00:26:51,380 --> 00:26:59,029
operations are just far easier to handle

00:26:55,039 --> 00:27:02,029
failure cases for so I'm bringing this

00:26:59,029 --> 00:27:04,070
up specifically because at least the AWS

00:27:02,029 --> 00:27:05,539
cloud provider there are many things

00:27:04,070 --> 00:27:07,549
within spinnaker that are not currently

00:27:05,539 --> 00:27:08,630
item potent things like create server

00:27:07,549 --> 00:27:10,720
group there are cases where it would

00:27:08,630 --> 00:27:14,240
actually fail a deployment pipeline

00:27:10,720 --> 00:27:16,909
because something went wrong with with

00:27:14,240 --> 00:27:18,409
the cloud provider that we we don't

00:27:16,909 --> 00:27:21,649
necessarily know what the end state is

00:27:18,409 --> 00:27:25,480
going to be so we just give up we want

00:27:21,649 --> 00:27:27,679
to avoid that as much as possible here I

00:27:25,480 --> 00:27:31,159
think we'll get there with with the

00:27:27,679 --> 00:27:33,019
public cloud providers as well then one

00:27:31,159 --> 00:27:35,840
of the initial debates was should we

00:27:33,019 --> 00:27:38,710
treat the open and connect fleet as a

00:27:35,840 --> 00:27:42,710
cloud provider so it's this huge global

00:27:38,710 --> 00:27:44,809
footprint of powerful servers we have an

00:27:42,710 --> 00:27:46,970
in-house configuration management system

00:27:44,809 --> 00:27:51,019
for them that happens to provide a very

00:27:46,970 --> 00:27:54,019
nice full documented strongly-typed API

00:27:51,019 --> 00:27:57,379
we during the deployment process we do

00:27:54,019 --> 00:28:00,889
need to cache the state of individual

00:27:57,379 --> 00:28:03,139
OCA s just similar to things cloud

00:28:00,889 --> 00:28:06,590
driver doesn't caching cloud provider

00:28:03,139 --> 00:28:08,899
state however if you happen to attend

00:28:06,590 --> 00:28:11,179
Robb Fletcher's talk yesterday on an

00:28:08,899 --> 00:28:13,429
orca he touched a little bit about the

00:28:11,179 --> 00:28:16,549
the journey Orca went into becoming a

00:28:13,429 --> 00:28:18,799
truly resilient distributed system that

00:28:16,549 --> 00:28:21,049
heart can horizontally scale work and

00:28:18,799 --> 00:28:23,749
which has an adorable queue so that you

00:28:21,049 --> 00:28:26,419
know if a instance working on a task

00:28:23,749 --> 00:28:27,979
fails that task is still is gonna just a

00:28:26,419 --> 00:28:30,200
failover to another instance cloud

00:28:27,979 --> 00:28:33,049
drivers not quite there yet

00:28:30,200 --> 00:28:34,460
currently tasks are sent to East are

00:28:33,049 --> 00:28:37,009
bound to a specific college ever

00:28:34,460 --> 00:28:39,349
instance if that instance fails that

00:28:37,009 --> 00:28:42,669
unit of work is lost so I kind of broke

00:28:39,349 --> 00:28:44,840
with our own internal conventions and

00:28:42,669 --> 00:28:50,419
everything we're going to talk about was

00:28:44,840 --> 00:28:53,119
implemented purely in Orca so some

00:28:50,419 --> 00:28:55,249
logistical challenges in ways that open

00:28:53,119 --> 00:28:59,899
connect deployments really differ from

00:28:55,249 --> 00:29:02,509
our typical cloud deployments so OCA

00:28:59,899 --> 00:29:07,149
maintenance occurs at trough mostly

00:29:02,509 --> 00:29:10,369
around being kind to smaller ISPs where

00:29:07,149 --> 00:29:12,559
they may not actually have enough excess

00:29:10,369 --> 00:29:14,989
capacity to have all of their Netflix

00:29:12,559 --> 00:29:17,599
traffic suddenly transit outside of

00:29:14,989 --> 00:29:20,479
their network and as you saw from the

00:29:17,599 --> 00:29:22,909
the global map were in practically every

00:29:20,479 --> 00:29:26,330
time zone there's actually 38 time zones

00:29:22,909 --> 00:29:28,609
around the world today due to some

00:29:26,330 --> 00:29:34,429
countries having 30 and 45 minute

00:29:28,609 --> 00:29:36,979
offsets so we've got already have a

00:29:34,429 --> 00:29:39,769
deployment window feature within

00:29:36,979 --> 00:29:42,440
spinnaker which would commonly be set

00:29:39,769 --> 00:29:44,330
wants per pipeline in thinking about

00:29:42,440 --> 00:29:46,759
this we'd have to we have to think about

00:29:44,330 --> 00:29:48,470
okay how can we have up to 38 different

00:29:46,759 --> 00:29:52,899
deployment windows for what would

00:29:48,470 --> 00:29:52,899
essentially be a single deployment stage

00:29:53,480 --> 00:29:59,910
and as mentioned we need to honor

00:29:56,840 --> 00:30:02,250
site-specific redundancy models there

00:29:59,910 --> 00:30:04,170
are some large open connect sites or we

00:30:02,250 --> 00:30:06,110
could just randomly pick a quarter of

00:30:04,170 --> 00:30:08,910
the hosts to take down to upgrade and

00:30:06,110 --> 00:30:10,590
nothing could possibly go wrong

00:30:08,910 --> 00:30:12,240
there are other sites where if you

00:30:10,590 --> 00:30:14,910
randomly picked a quarter of the hosts

00:30:12,240 --> 00:30:17,250
you might take down the entire site in

00:30:14,910 --> 00:30:20,300
doing so so we can't leave that to

00:30:17,250 --> 00:30:23,370
chance so the the first stage that was

00:30:20,300 --> 00:30:27,810
implemented within Orca around this was

00:30:23,370 --> 00:30:30,060
a sharding stage basically gets all of

00:30:27,810 --> 00:30:32,730
the OCA metadata from the configuration

00:30:30,060 --> 00:30:36,680
management system and build shard keys

00:30:32,730 --> 00:30:39,810
that basically account for the key

00:30:36,680 --> 00:30:41,970
resiliency aspects in order of operation

00:30:39,810 --> 00:30:45,450
so the type of content they include

00:30:41,970 --> 00:30:47,640
whether or not they announced BGP routes

00:30:45,450 --> 00:30:50,910
in which set of BGP ever else they're

00:30:47,640 --> 00:30:54,630
announcing if so basically stuff that

00:30:50,910 --> 00:30:59,310
into a sorted red black tree as a map of

00:30:54,630 --> 00:31:01,050
key to cache IDs cyclically assign them

00:30:59,310 --> 00:31:04,410
to charge and since we've we've

00:31:01,050 --> 00:31:06,440
clustered together like caches with the

00:31:04,410 --> 00:31:11,760
same responsibilities that just

00:31:06,440 --> 00:31:13,530
guarantees that all of the OCA is at

00:31:11,760 --> 00:31:15,360
this amsterdam site announcing this

00:31:13,530 --> 00:31:19,610
particular set of routes are going to

00:31:15,360 --> 00:31:24,150
end up in different shards so the

00:31:19,610 --> 00:31:26,580
deployment process itself the first

00:31:24,150 --> 00:31:30,420
stage relevant there is really just a a

00:31:26,580 --> 00:31:32,280
specialized scheduling stage it takes

00:31:30,420 --> 00:31:36,000
the parameters that mark showed earlier

00:31:32,280 --> 00:31:41,220
and those parameters really define the

00:31:36,000 --> 00:31:44,670
desired end State I want OCA is in the

00:31:41,220 --> 00:31:48,330
shard or a subset of those OCA s to run

00:31:44,670 --> 00:31:54,120
a specific firmware version this stage

00:31:48,330 --> 00:31:57,390
basically just outputs a map of a start

00:31:54,120 --> 00:31:59,190
of of times to OCA list this didn't

00:31:57,390 --> 00:32:01,320
necessarily need to be built as a

00:31:59,190 --> 00:32:04,650
standalone stage but my first reasoning

00:32:01,320 --> 00:32:07,650
was paranoia

00:32:04,650 --> 00:32:10,670
this is an absolutely critical system to

00:32:07,650 --> 00:32:14,100
Netflix and I wanted to ensure that

00:32:10,670 --> 00:32:16,290
operators had the opportunity to review

00:32:14,100 --> 00:32:18,810
that upgrade plan before it was acted on

00:32:16,290 --> 00:32:22,350
I would not want a spinnaker bug to

00:32:18,810 --> 00:32:26,130
result in all West Coast OCA is going

00:32:22,350 --> 00:32:28,650
offline at the same time I think we're a

00:32:26,130 --> 00:32:30,720
little bit past the need for that but

00:32:28,650 --> 00:32:34,830
having this is a dedicated stage also

00:32:30,720 --> 00:32:38,340
allows us to generate future plans

00:32:34,830 --> 00:32:41,760
consumed by other tasks so in this case

00:32:38,340 --> 00:32:43,500
we have a need to inform ISPs of

00:32:41,760 --> 00:32:46,620
maintenance several days in advance

00:32:43,500 --> 00:32:48,360
and we can produce a plan that meets

00:32:46,620 --> 00:32:51,270
that need as well as the actual

00:32:48,360 --> 00:32:53,040
deployment theme so if you looked at the

00:32:51,270 --> 00:32:55,740
pipeline view for one of these four more

00:32:53,040 --> 00:32:57,240
pipelines it's it's really simple that

00:32:55,740 --> 00:32:59,880
prepare upgrade plan this is the

00:32:57,240 --> 00:33:01,830
scheduler optional manual judgment and

00:32:59,880 --> 00:33:04,620
there's two parallel stages scheduled

00:33:01,830 --> 00:33:06,390
upgrades and upgrading Jens depending on

00:33:04,620 --> 00:33:08,700
the scope of a deployment though these

00:33:06,390 --> 00:33:11,070
actually turn into between five and

00:33:08,700 --> 00:33:13,080
eighty three parallel stages so the

00:33:11,070 --> 00:33:15,390
stage graph looks something like this

00:33:13,080 --> 00:33:19,980
over larger the dotted lines represent

00:33:15,390 --> 00:33:21,750
the actual stages in the graph when

00:33:19,980 --> 00:33:25,260
building a pipeline the solid lines are

00:33:21,750 --> 00:33:27,510
synthetically injected stages so we end

00:33:25,260 --> 00:33:30,810
up with a start up grade stage per

00:33:27,510 --> 00:33:32,520
unique start time and this thing called

00:33:30,810 --> 00:33:36,030
the upgrade engine which was really just

00:33:32,520 --> 00:33:38,190
named that way to carry over terminology

00:33:36,030 --> 00:33:40,470
from the previous open connect

00:33:38,190 --> 00:33:43,410
deployment processes which spins up a

00:33:40,470 --> 00:33:46,740
bunch of purpose-built stages each one

00:33:43,410 --> 00:33:48,690
responsible for a individual part of the

00:33:46,740 --> 00:33:54,300
the state change along the deployment

00:33:48,690 --> 00:33:56,850
process so the start upgrade stages they

00:33:54,300 --> 00:34:00,090
operate on a predetermined set of OCA s

00:33:56,850 --> 00:34:02,550
at a predetermined set of times and

00:34:00,090 --> 00:34:05,010
their main responsibility is to put OCA

00:34:02,550 --> 00:34:07,790
s when they enter their local time zone

00:34:05,010 --> 00:34:11,550
trough into a state of traffic draining

00:34:07,790 --> 00:34:13,530
so basically the net flix control plane

00:34:11,550 --> 00:34:16,590
will stop steering new streams to those

00:34:13,530 --> 00:34:17,110
OCA s however they will continue serving

00:34:16,590 --> 00:34:20,170
any

00:34:17,110 --> 00:34:23,410
any existing streams the upgrade engine

00:34:20,170 --> 00:34:25,450
stages are completely orthogonal to that

00:34:23,410 --> 00:34:27,250
schedule they do not operate on a

00:34:25,450 --> 00:34:29,500
predetermined set of OSI A's or a

00:34:27,250 --> 00:34:32,890
schedule and this design is really

00:34:29,500 --> 00:34:35,680
important to making this process as

00:34:32,890 --> 00:34:37,690
reliable as possible so just a little

00:34:35,680 --> 00:34:40,480
code snippet this is an example of how

00:34:37,690 --> 00:34:44,680
we take a schedule which is a map of

00:34:40,480 --> 00:34:47,230
times two caches and turn that into a

00:34:44,680 --> 00:34:51,780
bunch of stages add it to the graph with

00:34:47,230 --> 00:34:51,780
a start time a list of caches involved

00:34:55,639 --> 00:35:02,760
so what did those upgraded and sages do

00:34:59,730 --> 00:35:07,160
of look at the monitor draining stage

00:35:02,760 --> 00:35:23,540
stage it's basically just continually

00:35:07,160 --> 00:35:23,540
yeah yeah we belong ggest strength yeah

00:35:29,660 --> 00:35:35,970
which is tunable as well so yeah we

00:35:34,260 --> 00:35:38,160
might have some some titles that are

00:35:35,970 --> 00:35:40,530
three hours three and a half but not not

00:35:38,160 --> 00:35:42,599
much beyond that so this is just

00:35:40,530 --> 00:35:47,490
continually searching for OCS that meet

00:35:42,599 --> 00:35:50,310
that original deployment scope that have

00:35:47,490 --> 00:35:52,020
started the draining process and keeps

00:35:50,310 --> 00:35:54,810
track of when they're first seen so that

00:35:52,020 --> 00:35:56,609
we can time so we can time out and fail

00:35:54,810 --> 00:35:58,800
and OCA for some reason it's still

00:35:56,609 --> 00:36:00,960
pushing too much traffic after after the

00:35:58,800 --> 00:36:04,800
timeout window which is typically four

00:36:00,960 --> 00:36:07,440
hours this stage integrates with our the

00:36:04,800 --> 00:36:09,390
Netflix telemetry system Atlas and it is

00:36:07,440 --> 00:36:12,030
continually querying Atlas for bandwidth

00:36:09,390 --> 00:36:15,300
metrics about all the OSI A's it is

00:36:12,030 --> 00:36:18,630
currently concerned turned with if an

00:36:15,300 --> 00:36:20,280
OCA fails to reach the the reach below

00:36:18,630 --> 00:36:22,349
the bandwidth threshold within the four

00:36:20,280 --> 00:36:24,569
hour timeout window it marks it as

00:36:22,349 --> 00:36:27,480
failed and which we'll handle later in a

00:36:24,569 --> 00:36:29,160
different stage once it if it drops

00:36:27,480 --> 00:36:32,550
below the bandwidth threshold within the

00:36:29,160 --> 00:36:35,339
allotted time it initially it's the

00:36:32,550 --> 00:36:38,190
actual firmware upgrade process which

00:36:35,339 --> 00:36:41,310
causes the OCA to pull down the desired

00:36:38,190 --> 00:36:42,990
firmware version apply it to a boot

00:36:41,310 --> 00:36:45,300
partition we said some other special

00:36:42,990 --> 00:36:47,940
flags and over reboot into the new new

00:36:45,300 --> 00:36:51,300
firmware release the monitor upgrade

00:36:47,940 --> 00:36:53,609
stage is similar to how monitor draining

00:36:51,300 --> 00:36:54,960
is continually just scanning for OCA s

00:36:53,609 --> 00:36:57,440
that mean it's condition is looking for

00:36:54,960 --> 00:37:00,150
OCA s in the deployment scope that have

00:36:57,440 --> 00:37:01,890
had there that are that are have been

00:37:00,150 --> 00:37:04,740
told to upgrade to the new firmware but

00:37:01,890 --> 00:37:05,540
have last reported in as on as running

00:37:04,740 --> 00:37:07,430
on a different for

00:37:05,540 --> 00:37:10,160
we release we also track the time I

00:37:07,430 --> 00:37:12,680
think we give it was two hours

00:37:10,160 --> 00:37:16,760
successfully boot into the into the new

00:37:12,680 --> 00:37:20,210
firmware version if that doesn't happen

00:37:16,760 --> 00:37:23,150
there again Marcus failed if it does

00:37:20,210 --> 00:37:25,310
happen once it sees an OCA that it was

00:37:23,150 --> 00:37:27,200
previously tracking that is now reported

00:37:25,310 --> 00:37:28,520
as up and running on the new firmware

00:37:27,200 --> 00:37:30,980
it's all going to be out of service at

00:37:28,520 --> 00:37:32,750
that at that time the monitor upgrading

00:37:30,980 --> 00:37:38,090
stage is responsible for transitioning

00:37:32,750 --> 00:37:40,100
it back to its prior state when Martin

00:37:38,090 --> 00:37:44,450
when caches have been marked as failed

00:37:40,100 --> 00:37:46,520
they have a dedicated stage to to handle

00:37:44,450 --> 00:37:52,940
that since there's some additional

00:37:46,520 --> 00:37:55,160
business logic in there we set them into

00:37:52,940 --> 00:37:58,400
a special state called wonky which

00:37:55,160 --> 00:38:02,000
allows open and connect operators to

00:37:58,400 --> 00:38:06,590
easily quickly identify any any OCS that

00:38:02,000 --> 00:38:08,540
fell into this vortex and any any OC in

00:38:06,590 --> 00:38:12,940
that state is out of service to our two

00:38:08,540 --> 00:38:16,310
RS during control plane will open a

00:38:12,940 --> 00:38:19,400
individual JIRA ticket for every fail

00:38:16,310 --> 00:38:21,830
dossier which gets applied assigned to

00:38:19,400 --> 00:38:28,880
whoever lucky persons started the

00:38:21,830 --> 00:38:31,160
pipeline if that OCA was is belongs to

00:38:28,880 --> 00:38:34,520
an ISP versus a Netflix internet

00:38:31,160 --> 00:38:36,530
exchange we will notify that ISP that

00:38:34,520 --> 00:38:38,600
their cache is that the particular cache

00:38:36,530 --> 00:38:40,730
is left out of service and a Netflix

00:38:38,600 --> 00:38:44,450
engineer will follow up with them during

00:38:40,730 --> 00:38:47,210
business hours I've also got a fairly

00:38:44,450 --> 00:38:48,800
important stage monitor deployment on

00:38:47,210 --> 00:38:50,870
the one hand it's collating metrics

00:38:48,800 --> 00:38:54,140
across all of these parallel stages to

00:38:50,870 --> 00:38:58,460
to feed the UI perhaps more importantly

00:38:54,140 --> 00:39:01,400
it implements a circuit breaker so if

00:38:58,460 --> 00:39:03,890
more than a tunable percentage or

00:39:01,400 --> 00:39:05,660
absolute value of OCA is fatal fall into

00:39:03,890 --> 00:39:08,420
the failed state there might be

00:39:05,660 --> 00:39:10,070
something wrong possibly with the

00:39:08,420 --> 00:39:12,140
firmware release possibly something else

00:39:10,070 --> 00:39:16,760
but we might want to shut things down

00:39:12,140 --> 00:39:18,680
early it actually sets a flag within the

00:39:16,760 --> 00:39:19,340
execution context that all of the other

00:39:18,680 --> 00:39:21,380
stages

00:39:19,340 --> 00:39:23,780
are continually checking so if they're

00:39:21,380 --> 00:39:28,310
still start upgrade stages out there

00:39:23,780 --> 00:39:30,650
that have yet to start oh ca'se down the

00:39:28,310 --> 00:39:31,670
path of draining traffic the stages

00:39:30,650 --> 00:39:35,950
become no ops

00:39:31,670 --> 00:39:38,360
if the monitor draining stage is

00:39:35,950 --> 00:39:40,430
watching a set of essays that are

00:39:38,360 --> 00:39:42,470
draining traffic it will actually just

00:39:40,430 --> 00:39:46,190
revert them back to their prior state so

00:39:42,470 --> 00:39:48,620
put them back into service once once OCS

00:39:46,190 --> 00:39:50,090
have started the process of rebooting

00:39:48,620 --> 00:39:51,860
into the new firmware those are kind of

00:39:50,090 --> 00:39:55,580
past the point of no return

00:39:51,860 --> 00:39:58,400
so the monitor upgrading stage doesn't

00:39:55,580 --> 00:40:00,140
do anything different but this stage

00:39:58,400 --> 00:40:03,290
once the circuit breaker has been set

00:40:00,140 --> 00:40:05,360
we'll wait and will discontinue watch

00:40:03,290 --> 00:40:09,710
for the state being reached where there

00:40:05,360 --> 00:40:13,430
are no OCA s that are actively draining

00:40:09,710 --> 00:40:14,570
or upgrading or where there are no OCS

00:40:13,430 --> 00:40:17,180
mark dis failed that have not been

00:40:14,570 --> 00:40:19,670
handled and once that condition is

00:40:17,180 --> 00:40:23,260
reached it will terminate the pipeline

00:40:19,670 --> 00:40:23,260
with a status of canceled

00:40:23,440 --> 00:40:31,610
so these upgrade engine stages they work

00:40:28,670 --> 00:40:35,780
asynchronously they don't share state

00:40:31,610 --> 00:40:37,610
across the stages if spinnaker was to go

00:40:35,780 --> 00:40:40,300
down for any reason which definitely

00:40:37,610 --> 00:40:40,300
never happens

00:40:42,160 --> 00:40:48,860
definitely might be left with hundreds

00:40:44,510 --> 00:40:51,620
of OCS out of service what do you do you

00:40:48,860 --> 00:40:54,800
really just have to rerun the pipeline

00:40:51,620 --> 00:40:56,960
with the same deployment scope those out

00:40:54,800 --> 00:40:59,210
of service OCS are not going to appear

00:40:56,960 --> 00:41:01,820
in the in the new deployment schedule

00:40:59,210 --> 00:41:03,980
but the upgraded engine stages will pick

00:41:01,820 --> 00:41:06,890
them up and we'll still get them to the

00:41:03,980 --> 00:41:10,490
desired state without really any any

00:41:06,890 --> 00:41:14,270
thinking about cleanup specifically so

00:41:10,490 --> 00:41:16,550
result reliable repeatable delivery to

00:41:14,270 --> 00:41:19,040
stateful mission-critical physical

00:41:16,550 --> 00:41:22,010
infrastructure just Mark's description

00:41:19,040 --> 00:41:24,680
for the talk so everything everything I

00:41:22,010 --> 00:41:25,820
just described is is closed source you

00:41:24,680 --> 00:41:28,580
know primarily because it's a deep

00:41:25,820 --> 00:41:30,860
integration and to existing closed

00:41:28,580 --> 00:41:33,170
source Netflix services and

00:41:30,860 --> 00:41:34,480
infrastructure but I think

00:41:33,170 --> 00:41:37,970
this is a great example of how

00:41:34,480 --> 00:41:40,790
extensible spinnaker is especially Orca

00:41:37,970 --> 00:41:43,700
so if your business has complex workflow

00:41:40,790 --> 00:41:45,260
automation needs that don't correlate to

00:41:43,700 --> 00:41:47,720
cloud deployment primitives or

00:41:45,260 --> 00:41:49,790
Spinnaker's existing functionality don't

00:41:47,720 --> 00:41:52,270
let you scare that off it can be

00:41:49,790 --> 00:41:56,630
expanded to handle just about anything

00:41:52,270 --> 00:41:59,660
and thanks to people across many teams

00:41:56,630 --> 00:42:01,760
who helped make that happen and I think

00:41:59,660 --> 00:42:03,900
we're we might be out of time but happy

00:42:01,760 --> 00:42:05,930
to answer any questions

00:42:03,900 --> 00:42:09,570
[Applause]

00:42:05,930 --> 00:42:09,570

YouTube URL: https://www.youtube.com/watch?v=ot4--JQ0vy8


