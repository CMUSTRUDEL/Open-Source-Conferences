Title: Amy Smidutz @ Spinnaker.Live: Infrastructure Experimentation Using Spinnaker
Publication date: 2020-06-18
Playlist: Spinnaker.Live 2020
Description: 
	At Netflix, we leverage AB testing and experimentation to gain confidence that our apps are behaving as designed. In this presentation, I give a high-level overview of how Spinnaker can be used to enable infrastructure experimentation. I will demonstrate how we test in production, run Chaos experiments, and how we leverage automated canary analysis. The goal is that the audience walks away inspired to start using Spinnaker to conduct, manage, and monitor controlled experiments within their infrastructure.
Captions: 
	00:00:06,160 --> 00:00:11,559
hey everyone welcome to spin of her life

00:00:08,710 --> 00:00:13,719
my name is Amy Smith I'm the manager for

00:00:11,559 --> 00:00:14,830
resilience engineering and Netflix and

00:00:13,719 --> 00:00:16,750
I'm here today to talk about

00:00:14,830 --> 00:00:20,890
infrastructure experimentation using

00:00:16,750 --> 00:00:24,519
spinnaker most of you are at least

00:00:20,890 --> 00:00:26,140
probably familiar with Netflix we have a

00:00:24,519 --> 00:00:28,060
hundred and eighty three million paid

00:00:26,140 --> 00:00:31,000
memberships and over a hundred and

00:00:28,060 --> 00:00:32,830
ninety countries and in order to fill

00:00:31,000 --> 00:00:35,019
over content to all those people in all

00:00:32,830 --> 00:00:38,050
those places we maintain a very complex

00:00:35,019 --> 00:00:40,980
distributed system this picture that I

00:00:38,050 --> 00:00:46,270
have up on the screen is a bit outdated

00:00:40,980 --> 00:00:50,530
but it still gives you an example of how

00:00:46,270 --> 00:00:53,530
complex the Netflix application is we

00:00:50,530 --> 00:00:57,039
have so many micro services and they're

00:00:53,530 --> 00:01:00,789
all all manner of patterns of traffic

00:00:57,039 --> 00:01:04,059
routing and in order for us to operate a

00:01:00,789 --> 00:01:05,770
system of this size and complexity it

00:01:04,059 --> 00:01:09,130
requires heavy investment and

00:01:05,770 --> 00:01:11,289
reliability and resilience practices so

00:01:09,130 --> 00:01:13,270
today I want to give an overview of one

00:01:11,289 --> 00:01:16,990
such practice which is infrastructure

00:01:13,270 --> 00:01:18,819
experimentation we use infrastructure

00:01:16,990 --> 00:01:20,349
experimentation to proactively avoid

00:01:18,819 --> 00:01:22,509
production problems by running

00:01:20,349 --> 00:01:26,770
production tests and learning about our

00:01:22,509 --> 00:01:29,470
systems and spinnaker plays a huge part

00:01:26,770 --> 00:01:32,709
in this my goal for the talk today is

00:01:29,470 --> 00:01:35,679
really to introduce you to how we're

00:01:32,709 --> 00:01:37,119
experimenting using spinnaker so that

00:01:35,679 --> 00:01:39,489
you could think about how you could

00:01:37,119 --> 00:01:43,569
start experimenting in your own

00:01:39,489 --> 00:01:45,099
environments um and then before I get

00:01:43,569 --> 00:01:47,319
started I just want to tell you a little

00:01:45,099 --> 00:01:50,560
bit more about what the resilience team

00:01:47,319 --> 00:01:52,599
does we want to help Netflix engineers

00:01:50,560 --> 00:01:54,700
that are working directly on the Netflix

00:01:52,599 --> 00:01:57,550
application avoid this we don't want our

00:01:54,700 --> 00:01:59,200
customers to see screens like this we

00:01:57,550 --> 00:02:01,810
know how we know that things are gonna

00:01:59,200 --> 00:02:04,149
go wrong so we're trying to be resilient

00:02:01,810 --> 00:02:05,950
when they do which is another way of

00:02:04,149 --> 00:02:07,989
saying we're trying to degrade

00:02:05,950 --> 00:02:10,209
gracefully when things go wrong when we

00:02:07,989 --> 00:02:13,030
have failures we're definitely not

00:02:10,209 --> 00:02:14,620
trying to drive out any failure where we

00:02:13,030 --> 00:02:15,670
know things we definitely know things

00:02:14,620 --> 00:02:18,010
are going to go wrong in the system

00:02:15,670 --> 00:02:19,990
we're just trying to drive resiliency

00:02:18,010 --> 00:02:23,980
practices so things keep working

00:02:19,990 --> 00:02:27,100
even when there are problems so one of

00:02:23,980 --> 00:02:29,380
the ways that the resilience team does

00:02:27,100 --> 00:02:31,330
this is to provide tools and insights

00:02:29,380 --> 00:02:32,980
that engineers can use to experiment

00:02:31,330 --> 00:02:34,510
with their application and production

00:02:32,980 --> 00:02:37,270
and this is what I mean by

00:02:34,510 --> 00:02:39,460
infrastructure experimentation it's

00:02:37,270 --> 00:02:41,500
really as simple as wanting engineers to

00:02:39,460 --> 00:02:42,730
build up resiliency to failure by

00:02:41,500 --> 00:02:48,430
learning and experiment with

00:02:42,730 --> 00:02:50,260
experimenting with their application so

00:02:48,430 --> 00:02:52,510
infrastructure experimentation starts

00:02:50,260 --> 00:02:56,650
with a B style testing using a canary

00:02:52,510 --> 00:02:58,960
strategy and canary in and of itself is

00:02:56,650 --> 00:03:00,970
an infrastructure experiment with the

00:02:58,960 --> 00:03:04,330
main purpose of safely testing in

00:03:00,970 --> 00:03:05,860
production canary is a loaded term that

00:03:04,330 --> 00:03:07,660
means different things at different

00:03:05,860 --> 00:03:09,790
companies so I'll give you a quick

00:03:07,660 --> 00:03:14,740
overview of what I mean by canary and

00:03:09,790 --> 00:03:16,510
how we use it at Netflix simply it's

00:03:14,740 --> 00:03:20,290
setting up two clusters in production

00:03:16,510 --> 00:03:22,240
alongside your normal cluster we have a

00:03:20,290 --> 00:03:24,550
normal in this diagram we have a normal

00:03:22,240 --> 00:03:26,500
production cluster and then we have a

00:03:24,550 --> 00:03:29,050
smaller cluster that's serving new

00:03:26,500 --> 00:03:31,030
software we call that the canary or the

00:03:29,050 --> 00:03:32,890
experiment cluster and then we have a

00:03:31,030 --> 00:03:35,170
baseline cluster which is also serving

00:03:32,890 --> 00:03:37,660
the same known working software as our

00:03:35,170 --> 00:03:40,300
production cluster and the point of this

00:03:37,660 --> 00:03:43,930
particular strategy is to make sure the

00:03:40,300 --> 00:03:45,190
software change is good so I'm just

00:03:43,930 --> 00:03:46,720
gonna throw some numbers over here so

00:03:45,190 --> 00:03:49,930
you can get a feel for what this looks

00:03:46,720 --> 00:03:52,030
like in practice we have 98 instances

00:03:49,930 --> 00:03:54,310
already in production and then we want

00:03:52,030 --> 00:03:56,830
to deploy some new software so we set up

00:03:54,310 --> 00:04:00,460
two new small clusters of one instance

00:03:56,830 --> 00:04:02,830
each the actual size of the clusters can

00:04:00,460 --> 00:04:05,320
vary it doesn't have to be a single

00:04:02,830 --> 00:04:08,740
instance but they're always very small

00:04:05,320 --> 00:04:12,550
and we we're using a very small cluster

00:04:08,740 --> 00:04:15,130
so that we can we can have just a small

00:04:12,550 --> 00:04:16,840
subset of traffic going to them and this

00:04:15,130 --> 00:04:19,359
is kind of what I mean by us all a small

00:04:16,840 --> 00:04:22,680
subset of traffic so using a canary

00:04:19,359 --> 00:04:26,650
pattern we have reduced the blast radius

00:04:22,680 --> 00:04:28,150
so by reducing the blast radius I mean

00:04:26,650 --> 00:04:30,970
that if something goes wrong with our

00:04:28,150 --> 00:04:32,890
experiment canary we will only affect a

00:04:30,970 --> 00:04:34,289
small number of customers for a short

00:04:32,890 --> 00:04:37,330
period of time

00:04:34,289 --> 00:04:39,189
so what we're testing our deploy in

00:04:37,330 --> 00:04:41,409
production because it is the most

00:04:39,189 --> 00:04:43,330
accurate way to test we're still only

00:04:41,409 --> 00:04:45,189
subjecting a small number of customers

00:04:43,330 --> 00:04:51,240
to the change so if something goes wrong

00:04:45,189 --> 00:04:54,580
the blast radius will be small we use

00:04:51,240 --> 00:04:57,280
spinnaker for orchestration and cluster

00:04:54,580 --> 00:04:59,050
management for canary so spinnaker takes

00:04:57,280 --> 00:05:02,009
care of the heavy lifting for us it sets

00:04:59,050 --> 00:05:04,389
up these clusters the all the routing

00:05:02,009 --> 00:05:07,740
and then it will tear everything down

00:05:04,389 --> 00:05:11,379
for us what after our experiment is over

00:05:07,740 --> 00:05:14,139
so now that we have two clusters that we

00:05:11,379 --> 00:05:16,419
can experiment with we are able to

00:05:14,139 --> 00:05:18,789
measure the difference between the two

00:05:16,419 --> 00:05:21,370
so we can learn if our software changes

00:05:18,789 --> 00:05:23,590
behaving as expected so by having to

00:05:21,370 --> 00:05:25,360
like sized clusters we're able to

00:05:23,590 --> 00:05:27,430
actually get metrics from each we can

00:05:25,360 --> 00:05:29,199
look at those graphs and then we can

00:05:27,430 --> 00:05:31,300
make it a comparison and say like hey

00:05:29,199 --> 00:05:34,539
they look pretty much the same it's

00:05:31,300 --> 00:05:37,210
probably fine let's go and then we would

00:05:34,539 --> 00:05:39,190
proceed with a deploy otherwise we'd say

00:05:37,210 --> 00:05:41,469
wow these these graphs are wildly

00:05:39,190 --> 00:05:43,029
different there's a problem let's not

00:05:41,469 --> 00:05:45,729
push to production let's investigate

00:05:43,029 --> 00:05:47,979
further so it's really about easily

00:05:45,729 --> 00:05:53,349
seeing deviations if something has gone

00:05:47,979 --> 00:05:55,449
wrong but analysis is hard over the

00:05:53,349 --> 00:05:57,969
years we started with that same manual

00:05:55,449 --> 00:05:59,919
process that I described which requires

00:05:57,969 --> 00:06:01,419
engineers to look over all the data or

00:05:59,919 --> 00:06:03,099
receiving and then use their own

00:06:01,419 --> 00:06:06,580
judgment to decide if it's safe to

00:06:03,099 --> 00:06:09,189
deploy to production and not only is

00:06:06,580 --> 00:06:09,729
that tedious and and causing a lot of

00:06:09,189 --> 00:06:11,199
toil

00:06:09,729 --> 00:06:13,719
but it's also fraught with errors

00:06:11,199 --> 00:06:15,250
because we're using humans to pour

00:06:13,719 --> 00:06:18,729
through a bunch of data and then and

00:06:15,250 --> 00:06:20,830
then make judgments so over time we knew

00:06:18,729 --> 00:06:25,060
we had to improve our ability to analyze

00:06:20,830 --> 00:06:27,129
metrics automatically so a few years ago

00:06:25,060 --> 00:06:30,460
in conjunction with engineers at Google

00:06:27,129 --> 00:06:33,909
we introduced the Cayenne to platform

00:06:30,460 --> 00:06:36,370
and basically what Clanton does it's an

00:06:33,909 --> 00:06:38,860
open source software that's available to

00:06:36,370 --> 00:06:40,629
you today and it enables automatic

00:06:38,860 --> 00:06:43,449
canary analysis by measuring

00:06:40,629 --> 00:06:46,149
pre-configured metrics and just to give

00:06:43,449 --> 00:06:46,910
you a few fun facts about canta canta is

00:06:46,149 --> 00:06:48,980
named

00:06:46,910 --> 00:06:51,350
cool mine in Arizona in the Black Mesa

00:06:48,980 --> 00:06:52,430
area which keeps up with the whole

00:06:51,350 --> 00:06:55,070
canary in the coalmine

00:06:52,430 --> 00:06:57,890
analogy this picture is actually from my

00:06:55,070 --> 00:06:59,450
honeymoon I when we were driving through

00:06:57,890 --> 00:07:01,490
Arizona so I could send it back to the

00:06:59,450 --> 00:07:02,510
team to just so they could get a kick

00:07:01,490 --> 00:07:04,460
out of it

00:07:02,510 --> 00:07:08,960
and it comes in handy ever since because

00:07:04,460 --> 00:07:11,690
we don't have a cool logo for diantha so

00:07:08,960 --> 00:07:13,700
getting back to our diagram spinnaker

00:07:11,690 --> 00:07:15,740
can call out to Kyoto to run an

00:07:13,700 --> 00:07:17,300
automatic canary analysis which will

00:07:15,740 --> 00:07:19,010
detect when the baseline and the

00:07:17,300 --> 00:07:20,720
experiment cluster D be enough to

00:07:19,010 --> 00:07:23,120
indicate that it's not safe to deploy

00:07:20,720 --> 00:07:25,930
this gives our service owners confidence

00:07:23,120 --> 00:07:28,730
in their that their deployment is safe

00:07:25,930 --> 00:07:31,130
so now that we know a little about

00:07:28,730 --> 00:07:32,750
canary let's talk about other kinds of

00:07:31,130 --> 00:07:37,780
infrastructure experiment teaching that

00:07:32,750 --> 00:07:37,780
we do building on this canary strategy

00:07:39,190 --> 00:07:44,960
so chaos engineering this is something

00:07:41,750 --> 00:07:48,980
that has been a part of the reasons team

00:07:44,960 --> 00:07:51,830
for a long time and Cass is exciting

00:07:48,980 --> 00:07:54,970
right just the word itself is actually

00:07:51,830 --> 00:07:58,130
exciting maybe even a little taboo um

00:07:54,970 --> 00:07:59,870
and maybe sometimes a little scary

00:07:58,130 --> 00:08:03,020
maybe we all have a little too much

00:07:59,870 --> 00:08:04,940
chaos in our life right now we can apply

00:08:03,020 --> 00:08:06,890
this same feeling to engineers that are

00:08:04,940 --> 00:08:09,410
already familiar with some chaos

00:08:06,890 --> 00:08:12,260
practices even if it is an exciting

00:08:09,410 --> 00:08:14,360
topic to them and they want to employ

00:08:12,260 --> 00:08:17,690
this practice and they see the value in

00:08:14,360 --> 00:08:19,370
this practice it still is a little bit

00:08:17,690 --> 00:08:22,130
scary from time to time especially if

00:08:19,370 --> 00:08:26,120
they already have a heavy operational

00:08:22,130 --> 00:08:28,820
load and the reason that it can be

00:08:26,120 --> 00:08:30,800
hectic is because chaos engineering is

00:08:28,820 --> 00:08:33,260
chaotic because we are creating

00:08:30,800 --> 00:08:35,390
real-world problems in production so

00:08:33,260 --> 00:08:38,419
that means when things go wrong with

00:08:35,390 --> 00:08:41,150
chaos alarms will go off monitors will

00:08:38,419 --> 00:08:45,290
turn red and of course subsequently your

00:08:41,150 --> 00:08:46,940
blood pressure will probably go up so to

00:08:45,290 --> 00:08:48,500
give a little bit of history on chaos

00:08:46,940 --> 00:08:50,870
engineering a lot of you are probably

00:08:48,500 --> 00:08:52,850
familiar with chaos monkey which is one

00:08:50,870 --> 00:08:56,380
of the first or possibly the first foray

00:08:52,850 --> 00:09:00,590
into chaos tensioning for Netflix and

00:08:56,380 --> 00:09:03,710
chaos monkey well it still is in exist

00:09:00,590 --> 00:09:07,130
since today and well used at Netflix it

00:09:03,710 --> 00:09:09,440
is a blunt tool it creates a common

00:09:07,130 --> 00:09:12,050
failure which is that instance types

00:09:09,440 --> 00:09:14,060
sometimes disappear and the point of

00:09:12,050 --> 00:09:15,950
that is to teach engineers to architect

00:09:14,060 --> 00:09:18,890
their systems to be resilient against

00:09:15,950 --> 00:09:21,470
disappearing instances this was a big

00:09:18,890 --> 00:09:24,500
problem for us when we were moving from

00:09:21,470 --> 00:09:27,440
our data centers into AWS and all of a

00:09:24,500 --> 00:09:30,650
sudden we we had to deal with ephemeral

00:09:27,440 --> 00:09:34,340
instances so that's where chaos monkey

00:09:30,650 --> 00:09:38,060
had its the its greatest value was to

00:09:34,340 --> 00:09:40,220
very quickly teach our engineers how to

00:09:38,060 --> 00:09:42,350
fight against this particular problem

00:09:40,220 --> 00:09:46,070
and build resiliency into their systems

00:09:42,350 --> 00:09:48,470
about that and we saw so much value in

00:09:46,070 --> 00:09:50,150
that that we continued in to invested

00:09:48,470 --> 00:09:52,250
these kinds of chaos tools which

00:09:50,150 --> 00:09:55,360
basically just caused random failures

00:09:52,250 --> 00:09:59,660
that are common in a distributed system

00:09:55,360 --> 00:10:02,390
we some of these tools ended up in what

00:09:59,660 --> 00:10:04,790
is known as some simian army which has

00:10:02,390 --> 00:10:06,470
been deprecated now and all all the

00:10:04,790 --> 00:10:11,960
pieces have been pushed into other

00:10:06,470 --> 00:10:13,520
systems but at that time after after

00:10:11,960 --> 00:10:15,170
engineers were getting used to the

00:10:13,520 --> 00:10:16,790
practices of chaos engineering and

00:10:15,170 --> 00:10:20,980
they'd seen a lot of value out of it

00:10:16,790 --> 00:10:23,720
they also were starting to opt out and

00:10:20,980 --> 00:10:26,120
when we when we were looking into that

00:10:23,720 --> 00:10:28,580
we found that the pain of experiencing

00:10:26,120 --> 00:10:31,310
these failures was beginning to outweigh

00:10:28,580 --> 00:10:33,380
the lessons that were learned so even

00:10:31,310 --> 00:10:35,540
though they realized that learning these

00:10:33,380 --> 00:10:38,360
patterns and building resiliency against

00:10:35,540 --> 00:10:41,930
them was a valuable thing they were

00:10:38,360 --> 00:10:44,030
overwhelmed and the the the idea that a

00:10:41,930 --> 00:10:46,520
chaos tool was going to come in and

00:10:44,030 --> 00:10:48,770
stomp on your service was just it was

00:10:46,520 --> 00:10:51,260
just too much and so they were starting

00:10:48,770 --> 00:10:53,060
to push back on on maybe not taking part

00:10:51,260 --> 00:10:58,490
in all the chaos tools that we're

00:10:53,060 --> 00:11:00,770
offering so that's when chaos

00:10:58,490 --> 00:11:03,490
experimentation started that ideas

00:11:00,770 --> 00:11:05,480
around that starting to form can say

00:11:03,490 --> 00:11:08,300
experimentation is a way we can perform

00:11:05,480 --> 00:11:11,090
chaos engineering with more safety and

00:11:08,300 --> 00:11:13,939
less fear and one of the ways we do this

00:11:11,090 --> 00:11:16,849
is by marrying chaos with a

00:11:13,939 --> 00:11:19,909
airing strategy so just to give you a

00:11:16,849 --> 00:11:22,209
little bit of an idea of what chaos

00:11:19,909 --> 00:11:25,519
engineering generally is and what chaos

00:11:22,209 --> 00:11:28,729
experimentation generally is so chaos

00:11:25,519 --> 00:11:30,619
engineering and chaos experimentation

00:11:28,729 --> 00:11:32,809
they both run in production that's a

00:11:30,619 --> 00:11:36,439
very important part of chaos if it's not

00:11:32,809 --> 00:11:38,929
in production then it's not chaos you

00:11:36,439 --> 00:11:40,819
you want your chaos engineering and your

00:11:38,929 --> 00:11:43,699
experimentation to be automated and

00:11:40,819 --> 00:11:46,339
continuous so this these these things

00:11:43,699 --> 00:11:48,829
should be happening to build up

00:11:46,339 --> 00:11:52,309
resiliency which means that like it

00:11:48,829 --> 00:11:55,609
needs to be continually happening and we

00:11:52,309 --> 00:11:57,589
always want our chaos whether it's

00:11:55,609 --> 00:12:00,859
engineering or experimentation to

00:11:57,589 --> 00:12:03,889
simulate real-world failures and this is

00:12:00,859 --> 00:12:05,949
just about prioritization so we we want

00:12:03,889 --> 00:12:08,479
to make sure that whatever we're testing

00:12:05,949 --> 00:12:11,269
is something that is likely to actually

00:12:08,479 --> 00:12:12,829
happen to us and oftentimes it's

00:12:11,269 --> 00:12:14,929
something that has actually happened to

00:12:12,829 --> 00:12:19,220
us and that's why it's something that we

00:12:14,929 --> 00:12:21,529
continually simulate what some of the

00:12:19,220 --> 00:12:23,119
differences are is that the chaos

00:12:21,529 --> 00:12:24,889
engineering that we were practicing

00:12:23,119 --> 00:12:28,549
using chaos monkey and we still are

00:12:24,889 --> 00:12:30,679
today and some of those other tools they

00:12:28,549 --> 00:12:36,249
don't they don't necessarily minimize

00:12:30,679 --> 00:12:39,739
the blast radius and so with chaos

00:12:36,249 --> 00:12:43,729
experimentation we've we put a first

00:12:39,739 --> 00:12:46,879
principle towards minimizing blast

00:12:43,729 --> 00:12:48,709
radius so that when we are experimenting

00:12:46,879 --> 00:12:50,779
we're actually affecting as few

00:12:48,709 --> 00:12:55,789
customers as we possibly can while still

00:12:50,779 --> 00:12:59,179
getting great results we also are using

00:12:55,789 --> 00:13:02,089
advanced monitoring so we're we're not

00:12:59,179 --> 00:13:03,829
only pulling in information from canary

00:13:02,089 --> 00:13:05,479
but we're using other tools to let us

00:13:03,829 --> 00:13:07,519
know how things are going while the

00:13:05,479 --> 00:13:09,679
experiment is taking place which allows

00:13:07,519 --> 00:13:12,999
us to actually implement automatic

00:13:09,679 --> 00:13:16,639
stopping so if you do accidentally

00:13:12,999 --> 00:13:18,889
create a problem in production we have

00:13:16,639 --> 00:13:21,319
automatic stopping in place so that we

00:13:18,889 --> 00:13:23,359
can we can short an experiment and and

00:13:21,319 --> 00:13:25,539
stop it before it continues to cause

00:13:23,359 --> 00:13:25,539
problems

00:13:27,250 --> 00:13:34,570
so in order to actually provide chaos

00:13:32,170 --> 00:13:36,970
experimentation we realized we needed

00:13:34,570 --> 00:13:39,270
more sophisticated tooling that allows

00:13:36,970 --> 00:13:43,360
engineers to safely learn from chaos and

00:13:39,270 --> 00:13:45,370
that's why we created chap chap was

00:13:43,360 --> 00:13:49,360
originally named the chaos automation

00:13:45,370 --> 00:13:51,670
platform but as our tooling has grown

00:13:49,360 --> 00:13:53,160
we've realized that it's capable of

00:13:51,670 --> 00:13:56,140
creating other kinds of information

00:13:53,160 --> 00:13:57,790
infrastructure experiments that go

00:13:56,140 --> 00:14:00,610
beyond chaos and I'll talk a little bit

00:13:57,790 --> 00:14:04,030
about that later in the talk

00:14:00,610 --> 00:14:06,340
so chap which we sort of refer to as

00:14:04,030 --> 00:14:08,470
change automation platform but we

00:14:06,340 --> 00:14:12,190
realize that that can be confusing as

00:14:08,470 --> 00:14:14,560
well so name is still up in the air but

00:14:12,190 --> 00:14:16,450
chap the platform enables us to run

00:14:14,560 --> 00:14:17,980
infrastructure experiments like chaos

00:14:16,450 --> 00:14:20,650
experiments by pulling together other

00:14:17,980 --> 00:14:24,700
platforms like spinnaker and Kayenta and

00:14:20,650 --> 00:14:27,700
actually a whole bunch more and again

00:14:24,700 --> 00:14:29,620
chaos experimentation is a different

00:14:27,700 --> 00:14:31,660
kind of chaos engineering because it's

00:14:29,620 --> 00:14:34,390
more of us studying the problem than it

00:14:31,660 --> 00:14:36,339
is simply reacting to it since chap

00:14:34,390 --> 00:14:38,680
enables engineers to use chaos

00:14:36,339 --> 00:14:40,839
experimentation in a safer way it makes

00:14:38,680 --> 00:14:42,310
chaos engineering less scary so it's

00:14:40,839 --> 00:14:45,160
really attacking that problem of

00:14:42,310 --> 00:14:48,220
engineers being afraid to engage in

00:14:45,160 --> 00:14:50,470
chaos engineering or chaos engineering

00:14:48,220 --> 00:14:54,010
is causing way too many problems for

00:14:50,470 --> 00:14:56,020
engineers to react to you and creates a

00:14:54,010 --> 00:14:57,790
situation where you can still get all

00:14:56,020 --> 00:15:05,760
the benefits from chaos engineering but

00:14:57,790 --> 00:15:11,290
in a much safer and less scary way and

00:15:05,760 --> 00:15:14,050
along with chaos experimentation as a

00:15:11,290 --> 00:15:17,050
change in mind frame we also are using

00:15:14,050 --> 00:15:20,710
chat to run more complex experiments so

00:15:17,050 --> 00:15:22,600
we're looking to effect and learn from

00:15:20,710 --> 00:15:26,020
resiliency to downstream service

00:15:22,600 --> 00:15:28,210
failures we we want to make sure fall

00:15:26,020 --> 00:15:33,790
backs are behaving as expected and we're

00:15:28,210 --> 00:15:35,260
looking for systemic weaknesses so I I

00:15:33,790 --> 00:15:37,390
really want to give you an example of

00:15:35,260 --> 00:15:39,390
what we mean by a chaos experiment so

00:15:37,390 --> 00:15:41,139
I'm gonna zoom into one of these things

00:15:39,390 --> 00:15:43,660
do my fallback

00:15:41,139 --> 00:15:45,489
work as expected this is a common

00:15:43,660 --> 00:15:47,529
experiment that non critical service

00:15:45,489 --> 00:15:50,049
owners use to verify their service

00:15:47,529 --> 00:15:53,259
really is non critical so let's walk

00:15:50,049 --> 00:15:56,529
through this test to get a quick view of

00:15:53,259 --> 00:15:58,149
some of the mechanics and I just go

00:15:56,529 --> 00:16:02,939
ahead and start by explaining what a

00:15:58,149 --> 00:16:05,290
fallback is in case you're unfamiliar so

00:16:02,939 --> 00:16:07,419
anyone that has ever watched Netflix

00:16:05,290 --> 00:16:11,109
before is probably familiar with this

00:16:07,419 --> 00:16:13,029
screen or something similar when Netflix

00:16:11,109 --> 00:16:16,029
is working normally you'll see a screen

00:16:13,029 --> 00:16:17,619
that looks just like this the play

00:16:16,029 --> 00:16:19,749
button is our critical functionality

00:16:17,619 --> 00:16:22,329
pushing this allows for playback to

00:16:19,749 --> 00:16:24,459
begin we want everything on this screen

00:16:22,329 --> 00:16:26,350
to be available to a user but if some of

00:16:24,459 --> 00:16:30,579
it isn't for a short period of time

00:16:26,350 --> 00:16:32,679
that's fine so when a customer pushes

00:16:30,579 --> 00:16:35,609
the button we want them to see this the

00:16:32,679 --> 00:16:40,299
big end shows up you hear the bomb and

00:16:35,609 --> 00:16:43,600
and then joy commences we do not want

00:16:40,299 --> 00:16:50,169
them to see this or any other kind of UI

00:16:43,600 --> 00:16:52,179
related error and and basically all all

00:16:50,169 --> 00:16:55,029
of our effort is to try to degrade

00:16:52,179 --> 00:16:58,089
gracefully so that when errors pop up

00:16:55,029 --> 00:17:00,639
that are avoidable instead of seeing

00:16:58,089 --> 00:17:02,559
this screen you would instead see

00:17:00,639 --> 00:17:06,429
degraded functionality on that first

00:17:02,559 --> 00:17:09,429
screen there are a ton of items on the

00:17:06,429 --> 00:17:11,230
screen and many of them are not critical

00:17:09,429 --> 00:17:13,839
to be able to actually play an episode

00:17:11,230 --> 00:17:16,630
so here's an example this is called a

00:17:13,839 --> 00:17:20,019
badge and just lets you know something

00:17:16,630 --> 00:17:21,750
about the show this is a badge that lets

00:17:20,019 --> 00:17:24,449
you know that it's a top ten show and

00:17:21,750 --> 00:17:27,490
these buttons are for our rating system

00:17:24,449 --> 00:17:30,039
so these and many other things on the

00:17:27,490 --> 00:17:32,429
page are all separate micro surfaces

00:17:30,039 --> 00:17:38,710
that make up the Netflix application UI

00:17:32,429 --> 00:17:40,210
and many of them are not critical so to

00:17:38,710 --> 00:17:41,559
talk a little bit about the experiment

00:17:40,210 --> 00:17:44,559
we're going to run I created a simple

00:17:41,559 --> 00:17:47,590
and largely inaccurate diagram as an

00:17:44,559 --> 00:17:49,179
example all the red boxes are critical

00:17:47,590 --> 00:17:50,889
which means they are required for

00:17:49,179 --> 00:17:54,950
playback to work and all the blue are

00:17:50,889 --> 00:17:58,640
non critical non critical services use

00:17:54,950 --> 00:18:00,290
strategies like fallbacks which means if

00:17:58,640 --> 00:18:03,320
a critical service reaches out to a

00:18:00,290 --> 00:18:05,180
non-critical critical service that is

00:18:03,320 --> 00:18:08,690
unable to communicate will send fallback

00:18:05,180 --> 00:18:10,460
data rather than error this allows some

00:18:08,690 --> 00:18:12,500
elements of the UI to be unavailable

00:18:10,460 --> 00:18:15,500
rather than causing the UI to crash or

00:18:12,500 --> 00:18:17,360
playback to fail but how can we test

00:18:15,500 --> 00:18:22,070
that the fallback actually works in

00:18:17,360 --> 00:18:23,390
production we could use an issue we

00:18:22,070 --> 00:18:25,070
could actually cause an issue in

00:18:23,390 --> 00:18:26,780
production we could we could take down

00:18:25,070 --> 00:18:28,160
that service and see what happens and

00:18:26,780 --> 00:18:31,370
just hope that our fallback actually

00:18:28,160 --> 00:18:33,290
works but if our fallback isn't working

00:18:31,370 --> 00:18:35,900
correctly we could cause an impact to

00:18:33,290 --> 00:18:38,060
the service this was a would cause a

00:18:35,900 --> 00:18:41,410
problem for all Netflix customers in the

00:18:38,060 --> 00:18:41,410
region that we perform the test

00:18:41,920 --> 00:18:48,790
so this is where minimum ID is

00:18:44,270 --> 00:18:51,320
minimizing the blast radius comes in and

00:18:48,790 --> 00:18:54,470
also where the canary strategy comes in

00:18:51,320 --> 00:18:56,360
so by creating those two small clusters

00:18:54,470 --> 00:18:58,280
that are exact replicas of the service

00:18:56,360 --> 00:19:00,650
we want to test

00:18:58,280 --> 00:19:03,110
just like the deployment Canaries there

00:19:00,650 --> 00:19:04,730
stood up in production and we can send a

00:19:03,110 --> 00:19:06,860
small amount of customers to each of

00:19:04,730 --> 00:19:07,460
these clusters and that will reduce the

00:19:06,860 --> 00:19:10,250
blast radius

00:19:07,460 --> 00:19:12,890
so as we experiment if there is a

00:19:10,250 --> 00:19:17,060
problem it will only affect a small

00:19:12,890 --> 00:19:19,250
amount of our users so in this scenario

00:19:17,060 --> 00:19:20,630
we cause communication failures between

00:19:19,250 --> 00:19:22,750
the experiment cluster and the service

00:19:20,630 --> 00:19:25,250
calling it and while the baseline and

00:19:22,750 --> 00:19:28,370
the baseline just continues to work as

00:19:25,250 --> 00:19:31,010
normal this will trigger our fallback

00:19:28,370 --> 00:19:34,700
which if working correctly will send

00:19:31,010 --> 00:19:36,560
dummy data back and if our experiment

00:19:34,700 --> 00:19:38,090
fails the customers that are in that

00:19:36,560 --> 00:19:40,520
experiment will not be able to start

00:19:38,090 --> 00:19:42,230
playback if the experiment passes

00:19:40,520 --> 00:19:44,930
playback will work as expected

00:19:42,230 --> 00:19:47,000
and regardless of the experiment outcome

00:19:44,930 --> 00:19:48,980
all customers that are not in the

00:19:47,000 --> 00:19:53,090
experiment will continue to have fully

00:19:48,980 --> 00:19:55,880
operational service so we are

00:19:53,090 --> 00:19:57,920
experimenting with real people and even

00:19:55,880 --> 00:20:00,140
though all the tests that we run we

00:19:57,920 --> 00:20:02,660
assume will work we don't ever run a

00:20:00,140 --> 00:20:05,240
test that we assume won't because again

00:20:02,660 --> 00:20:08,540
we're testing with real people even

00:20:05,240 --> 00:20:10,610
though our hypothesis is always positive

00:20:08,540 --> 00:20:14,390
Berman's do fail often that's why it's

00:20:10,610 --> 00:20:15,980
important to continuously test so even

00:20:14,390 --> 00:20:16,910
if it is a small number of people in our

00:20:15,980 --> 00:20:18,740
experiment

00:20:16,910 --> 00:20:21,020
we don't want to cause them problems for

00:20:18,740 --> 00:20:22,910
too long these are actual customers and

00:20:21,020 --> 00:20:25,010
we love them so we don't ever want to

00:20:22,910 --> 00:20:25,690
cause them pain if we if if we don't

00:20:25,010 --> 00:20:30,230
have to

00:20:25,690 --> 00:20:33,500
so chap measures real-time analytics

00:20:30,230 --> 00:20:35,300
using a platform called mantis mantis is

00:20:33,500 --> 00:20:38,300
from Netflix it's also an open source

00:20:35,300 --> 00:20:40,820
technology and it's a stream processing

00:20:38,300 --> 00:20:44,360
platform that allows us to get real-time

00:20:40,820 --> 00:20:46,070
operational insights we use this to

00:20:44,360 --> 00:20:50,750
measure metrics from the baseline in the

00:20:46,070 --> 00:20:52,520
experiment cluster which allows us to

00:20:50,750 --> 00:20:54,260
measure a lot of metrics while running

00:20:52,520 --> 00:20:57,140
an experiment but one of our key

00:20:54,260 --> 00:21:00,530
performance indicators is stream starts

00:20:57,140 --> 00:21:02,210
per second or SBS this metric most

00:21:00,530 --> 00:21:04,400
closely aligns with customer is not

00:21:02,210 --> 00:21:06,020
being able to start playback so this

00:21:04,400 --> 00:21:09,440
allows us to detect if there's an issue

00:21:06,020 --> 00:21:11,660
within five minutes and this allows us

00:21:09,440 --> 00:21:13,310
this is what allows us to be able to

00:21:11,660 --> 00:21:17,030
safely experiment with a small part of

00:21:13,310 --> 00:21:19,460
the population and production so to give

00:21:17,030 --> 00:21:21,410
you an example again we're using our

00:21:19,460 --> 00:21:24,230
trusty canary strategy to test if

00:21:21,410 --> 00:21:27,880
there's a deviation the blue line is the

00:21:24,230 --> 00:21:30,950
baseline and the red is the experiment

00:21:27,880 --> 00:21:33,170
we even boost this signal by using what

00:21:30,950 --> 00:21:35,060
we call a stick what we call a sticky

00:21:33,170 --> 00:21:36,920
canary which means we tagged the

00:21:35,060 --> 00:21:38,870
population of customers that are routed

00:21:36,920 --> 00:21:40,550
in the start of the experiment so they

00:21:38,870 --> 00:21:44,300
will always be routed back to our

00:21:40,550 --> 00:21:45,890
experiment until it ends if SVS for the

00:21:44,300 --> 00:21:48,230
experiment cluster deviates from the

00:21:45,890 --> 00:21:50,960
baseline cluster we can automatically

00:21:48,230 --> 00:21:55,060
detect the problem quickly and chap will

00:21:50,960 --> 00:21:57,560
automatically stop the experiment so

00:21:55,060 --> 00:21:59,150
this is important because this is our

00:21:57,560 --> 00:22:01,910
stopping mechanism and this is what I

00:21:59,150 --> 00:22:05,660
meant by advanced metrics because we're

00:22:01,910 --> 00:22:08,510
using advanced metrics and we have a key

00:22:05,660 --> 00:22:10,820
performance indicator we're able to

00:22:08,510 --> 00:22:13,130
actually put a stop to an experiment

00:22:10,820 --> 00:22:15,860
before it causes too much harm to our

00:22:13,130 --> 00:22:17,780
customers and since we're able to do

00:22:15,860 --> 00:22:20,690
this we can actually stop an experiment

00:22:17,780 --> 00:22:22,550
soon enough that it's unlikely that a

00:22:20,690 --> 00:22:24,290
customer is going to get so

00:22:22,550 --> 00:22:25,880
said that they they wander off to use a

00:22:24,290 --> 00:22:30,350
different service or an or leave our

00:22:25,880 --> 00:22:31,730
service altogether in addition to the

00:22:30,350 --> 00:22:35,570
real-time metrics that we're getting

00:22:31,730 --> 00:22:38,480
from mantis we also run a full analysis

00:22:35,570 --> 00:22:40,820
using a canary analysis platform ok at

00:22:38,480 --> 00:22:42,170
the end of an experiment so service

00:22:40,820 --> 00:22:45,140
owners can get additional information

00:22:42,170 --> 00:22:47,350
about their service and this is just an

00:22:45,140 --> 00:22:50,600
example canary report to give you a

00:22:47,350 --> 00:22:54,140
high-level idea of what you can see in

00:22:50,600 --> 00:22:56,660
this kind of report so kind of allows us

00:22:54,140 --> 00:22:58,310
to run standard metrics plus anything

00:22:56,660 --> 00:23:00,290
else the service owner wants to keep an

00:22:58,310 --> 00:23:04,970
eye on including metrics that are

00:23:00,290 --> 00:23:07,100
specific to their app and so far I've

00:23:04,970 --> 00:23:09,320
talked abouts vinegar helps us house

00:23:07,100 --> 00:23:10,910
vinegar helps us orchestrate experiments

00:23:09,320 --> 00:23:12,650
using canary and connecting those

00:23:10,910 --> 00:23:15,620
experiments with canary announced this

00:23:12,650 --> 00:23:17,570
using panda but I but we also use it to

00:23:15,620 --> 00:23:19,670
schedule continuous experimentation

00:23:17,570 --> 00:23:23,500
because again that's a really important

00:23:19,670 --> 00:23:28,430
part of infrastructure experimentation

00:23:23,500 --> 00:23:32,210
so here's an example of a custom stage

00:23:28,430 --> 00:23:34,310
that we created so we've added several

00:23:32,210 --> 00:23:36,650
custom stages to spinnaker so engineers

00:23:34,310 --> 00:23:39,260
can configure an experiment right from

00:23:36,650 --> 00:23:42,350
spinnaker and set up pipelines that

00:23:39,260 --> 00:23:45,620
include Kaos experiments so this means

00:23:42,350 --> 00:23:47,810
that if they want to have any particular

00:23:45,620 --> 00:23:49,460
experiment that we offer as part of

00:23:47,810 --> 00:23:51,200
their deployment pipeline that they can

00:23:49,460 --> 00:23:53,630
if they wanted as a separate pipeline

00:23:51,200 --> 00:23:56,750
they can but the important thing is is

00:23:53,630 --> 00:24:01,070
that they have tools to enable running

00:23:56,750 --> 00:24:03,200
these things on a regular basis so I

00:24:01,070 --> 00:24:05,870
mentioned in the beginning that chaos is

00:24:03,200 --> 00:24:09,170
not the only infrastructure experiment

00:24:05,870 --> 00:24:11,900
that we run using chat so using chat

00:24:09,170 --> 00:24:14,540
components were able to orchestrate a

00:24:11,900 --> 00:24:18,170
production load test and this experiment

00:24:14,540 --> 00:24:20,690
is called squeeze so this allows us to

00:24:18,170 --> 00:24:24,830
safely perform an accurate load test in

00:24:20,690 --> 00:24:26,660
production and engineers generally use

00:24:24,830 --> 00:24:30,140
this to get accurate measurements of

00:24:26,660 --> 00:24:31,940
their optimum requests per second they

00:24:30,140 --> 00:24:34,130
can also use this if they want to test

00:24:31,940 --> 00:24:38,050
changes to their service such as making

00:24:34,130 --> 00:24:38,050
a change to an instant site family

00:24:39,320 --> 00:24:44,150
spinnaker helps engines run squeeze

00:24:41,750 --> 00:24:46,760
experiments regularly which allows them

00:24:44,150 --> 00:24:49,160
to see performance regressions over time

00:24:46,760 --> 00:24:51,710
or improvements as is the case in this

00:24:49,160 --> 00:24:56,240
chart but the important thing is is that

00:24:51,710 --> 00:24:58,670
we can actually see the changes as as we

00:24:56,240 --> 00:25:00,380
run squeezes over time a lot of people

00:24:58,670 --> 00:25:02,840
are running these daily some of some of

00:25:00,380 --> 00:25:03,980
them less frequently some of them more

00:25:02,840 --> 00:25:06,380
frequently as they're trying to

00:25:03,980 --> 00:25:08,570
investigate problems but this is just

00:25:06,380 --> 00:25:13,670
another experiment type that we offer

00:25:08,570 --> 00:25:15,170
for our customers so again the takeaway

00:25:13,670 --> 00:25:17,180
here is that spinnaker canary and

00:25:15,170 --> 00:25:19,880
cayenne tie can provide powerful

00:25:17,180 --> 00:25:22,610
insights to your deployments and other

00:25:19,880 --> 00:25:25,310
infrastructure experiments

00:25:22,610 --> 00:25:27,530
so while chap is not open-source

00:25:25,310 --> 00:25:29,990
spinnaker and Canada are so you can get

00:25:27,530 --> 00:25:31,430
started with canary as part of your

00:25:29,990 --> 00:25:33,440
deployment pipe with pipe lines right

00:25:31,430 --> 00:25:37,490
now and then add to those tools over

00:25:33,440 --> 00:25:39,650
time so that's it thank you all for

00:25:37,490 --> 00:25:42,800
sticking with me I hope everyone is safe

00:25:39,650 --> 00:25:45,500
and sound and please connect with me

00:25:42,800 --> 00:25:47,420
this is community and I would love to

00:25:45,500 --> 00:25:49,430
get to know you I'd love to talk more

00:25:47,420 --> 00:25:50,570
about these things I'd love to hear what

00:25:49,430 --> 00:25:53,570
you're doing with your own

00:25:50,570 --> 00:25:55,730
infrastructure experimentation and also

00:25:53,570 --> 00:25:57,680
if you love testing and production if

00:25:55,730 --> 00:25:59,150
you love reliability or if you're just

00:25:57,680 --> 00:26:01,220
passionate about making your fellow

00:25:59,150 --> 00:26:03,860
engineer successful please reach out to

00:26:01,220 --> 00:26:05,900
me the resilience team is hiring and I

00:26:03,860 --> 00:26:07,730
definitely want to talk to you thank you

00:26:05,900 --> 00:26:10,570
all and I hope you enjoy the rest of

00:26:07,730 --> 00:26:10,570

YouTube URL: https://www.youtube.com/watch?v=TA_jBrFznEo


