Title: Miles Matthias "Monitoring Spinnaker with Prometheus Operator on GKE" - Spinnaker Summit 2019
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	The third annual Spinnaker Summit (Diamond Sponsors: Netflix, Google and Armory) was held at the Hard Rock Hotel in San Diego, CA from November 15-17, 2019 and welcomed over 500 members of the rapidly growing Spinnaker open source community.
Captions: 
	00:00:00,780 --> 00:00:04,259
like Daniel said I want to be talking

00:00:02,100 --> 00:00:06,540
about monitoring spinnaker has an

00:00:04,259 --> 00:00:08,790
application and using Prometheus

00:00:06,540 --> 00:00:10,620
operator to do it so I'm gonna talk

00:00:08,790 --> 00:00:11,969
about gke specifically but a lot of this

00:00:10,620 --> 00:00:16,680
is applicable to any kubernetes

00:00:11,969 --> 00:00:19,289
environment as well so a little quick

00:00:16,680 --> 00:00:21,270
introduction of me I am a cloud

00:00:19,289 --> 00:00:26,640
consultant a partner at container heroes

00:00:21,270 --> 00:00:28,650
comm and I consult companies on all

00:00:26,640 --> 00:00:32,340
sorts of things but I also partner with

00:00:28,650 --> 00:00:35,190
the Google PSO team and spent most of

00:00:32,340 --> 00:00:38,520
this year leading the CI CD efforts on a

00:00:35,190 --> 00:00:40,680
very large on-prem to GCP migration and

00:00:38,520 --> 00:00:42,540
that's where a lot of this information

00:00:40,680 --> 00:00:44,250
that you're about to see comes from and

00:00:42,540 --> 00:00:49,410
I contributed back to the open source as

00:00:44,250 --> 00:00:50,879
much as I could so the scenario we're

00:00:49,410 --> 00:00:52,620
talking about today is you're running

00:00:50,879 --> 00:00:55,230
spinnaker on kubernetes you're an

00:00:52,620 --> 00:00:56,940
operator of spinnaker and you're

00:00:55,230 --> 00:00:58,770
familiar with Prometheus cough on alert

00:00:56,940 --> 00:01:02,070
manager that's cool but you want to

00:00:58,770 --> 00:01:04,350
monitor spinnaker you want to understand

00:01:02,070 --> 00:01:05,760
what's going on in your application you

00:01:04,350 --> 00:01:08,610
want to see make sure that you're giving

00:01:05,760 --> 00:01:10,680
enough resources to spinnaker to be able

00:01:08,610 --> 00:01:15,409
to service all of you or users which are

00:01:10,680 --> 00:01:18,900
your internal developer teams so

00:01:15,409 --> 00:01:21,299
specifically what we want to do in kind

00:01:18,900 --> 00:01:25,500
of the end state of where I'm going to

00:01:21,299 --> 00:01:27,659
take you is kind of these things so you

00:01:25,500 --> 00:01:30,240
want to be able to pull up Prometheus

00:01:27,659 --> 00:01:33,720
and you want to see spinnaker as what

00:01:30,240 --> 00:01:36,000
Prometheus calls targets and so in this

00:01:33,720 --> 00:01:37,560
screen here it's probably pretty hard to

00:01:36,000 --> 00:01:40,229
see but all those blue things are

00:01:37,560 --> 00:01:42,210
communities pod labels and each one of

00:01:40,229 --> 00:01:46,020
those rows are the different micro

00:01:42,210 --> 00:01:48,810
services that compose spinnaker so cloud

00:01:46,020 --> 00:01:50,460
driver hork echo all those are in that

00:01:48,810 --> 00:01:52,950
list right there and they're all

00:01:50,460 --> 00:01:55,560
different different targets you can see

00:01:52,950 --> 00:01:58,649
at the last time that Prometheus scraped

00:01:55,560 --> 00:02:00,329
metrics from each of them then we want

00:01:58,649 --> 00:02:02,460
to be able to write some queries if

00:02:00,329 --> 00:02:03,719
you've got fat fingers like me and you

00:02:02,460 --> 00:02:05,219
want to be able to see what are all the

00:02:03,719 --> 00:02:05,939
metrics that spinnaker is just throwing

00:02:05,219 --> 00:02:08,039
up

00:02:05,939 --> 00:02:09,569
Prometheus is a lot of you know I

00:02:08,039 --> 00:02:10,890
already has some nice auto-completion

00:02:09,569 --> 00:02:12,660
and all these metrics and so you can

00:02:10,890 --> 00:02:13,680
just type in or go and be like oh sweet

00:02:12,660 --> 00:02:16,590
look at all these metrics

00:02:13,680 --> 00:02:19,500
Orca type in cloud driver and see a

00:02:16,590 --> 00:02:23,310
bunch of metrics here in your Prometheus

00:02:19,500 --> 00:02:24,989
query query writer and then we want to

00:02:23,310 --> 00:02:27,659
be able to run some queries so this is

00:02:24,989 --> 00:02:29,640
again it's it's a screen shot so it's oh

00:02:27,659 --> 00:02:31,590
and a little far away for everybody but

00:02:29,640 --> 00:02:34,500
basically here I'm just trying to see

00:02:31,590 --> 00:02:36,540
how many completed executions have been

00:02:34,500 --> 00:02:39,900
run on Orca here and getting some values

00:02:36,540 --> 00:02:42,450
back in the corner and only three for

00:02:39,900 --> 00:02:44,040
one of them because this is like a brand

00:02:42,450 --> 00:02:46,500
new cluster and install that I did the

00:02:44,040 --> 00:02:47,819
other day so that's why we have very low

00:02:46,500 --> 00:02:50,329
numbers but in production obviously

00:02:47,819 --> 00:02:52,470
you'd have more than three executions

00:02:50,329 --> 00:02:54,420
but you also want to see some sweet

00:02:52,470 --> 00:02:56,849
gravano graphs you want to see some

00:02:54,420 --> 00:03:00,120
graphs of like what's going on with my

00:02:56,849 --> 00:03:02,609
spinnaker install what's how much memory

00:03:00,120 --> 00:03:04,590
is being used how many people are using

00:03:02,609 --> 00:03:08,010
different stages what applications are

00:03:04,590 --> 00:03:11,459
running right now and so we're gonna get

00:03:08,010 --> 00:03:15,510
into all that so I want to do a little

00:03:11,459 --> 00:03:17,040
bit of backstory I I don't know how many

00:03:15,510 --> 00:03:20,489
of you are really super familiar with

00:03:17,040 --> 00:03:23,250
kubernetes the concept of operators I

00:03:20,489 --> 00:03:26,340
know for me when you start to look into

00:03:23,250 --> 00:03:27,870
a lot of these things a lot of people

00:03:26,340 --> 00:03:29,540
just skip over the backstory so I'm

00:03:27,870 --> 00:03:34,109
going to go over some context real quick

00:03:29,540 --> 00:03:36,959
what's an operator at all so a

00:03:34,109 --> 00:03:38,930
kubernetes operator is it's just a

00:03:36,959 --> 00:03:40,980
method of packaging a specific

00:03:38,930 --> 00:03:44,310
application and running it on kubernetes

00:03:40,980 --> 00:03:46,169
and then responding to different

00:03:44,310 --> 00:03:47,639
manifests that are applied to the

00:03:46,169 --> 00:03:49,739
cluster and then updating the

00:03:47,639 --> 00:03:53,540
application so first and foremost it

00:03:49,739 --> 00:03:57,659
installs an application in our instance

00:03:53,540 --> 00:03:59,699
we're installing Prometheus and then

00:03:57,659 --> 00:04:02,940
it's going to define a bunch of CRD so

00:03:59,699 --> 00:04:04,379
custom resource definitions where those

00:04:02,940 --> 00:04:06,930
are things that are unique to the

00:04:04,379 --> 00:04:09,810
application you can keep cut all apply a

00:04:06,930 --> 00:04:12,900
manifest the CR D and the operator will

00:04:09,810 --> 00:04:14,760
pick up on it and then understand how to

00:04:12,900 --> 00:04:18,120
make modifications to the running

00:04:14,760 --> 00:04:22,469
application in response to the Sierra tu

00:04:18,120 --> 00:04:23,550
applied so yeah operator hub die is a

00:04:22,469 --> 00:04:26,039
place where you can see a bunch of

00:04:23,550 --> 00:04:27,930
kubernetes operators there is one for

00:04:26,039 --> 00:04:30,780
spinnaker that actually

00:04:27,930 --> 00:04:33,750
armory has its own ups MX has another

00:04:30,780 --> 00:04:36,690
there are a lot of its in the works

00:04:33,750 --> 00:04:38,580
right now to kind of discuss if there

00:04:36,690 --> 00:04:40,650
should be kind of an official spinner

00:04:38,580 --> 00:04:45,060
community one so you can look out that

00:04:40,650 --> 00:04:47,520
for next year but Prometheus operator

00:04:45,060 --> 00:04:49,680
specifically is gonna do a lot of stuff

00:04:47,520 --> 00:04:52,590
and so it's going to install Prometheus

00:04:49,680 --> 00:04:55,140
core fauna and alert manager and cube

00:04:52,590 --> 00:04:57,690
stats and then it's going to define a

00:04:55,140 --> 00:05:00,960
bunch of CR DS and then react to those

00:04:57,690 --> 00:05:05,090
CR DS when we installed them so a bunch

00:05:00,960 --> 00:05:08,550
of stuff going on here but it's gonna

00:05:05,090 --> 00:05:10,680
it's going to make your life a lot

00:05:08,550 --> 00:05:12,270
easier by installing all these

00:05:10,680 --> 00:05:14,190
applications just for you and

00:05:12,270 --> 00:05:18,300
automatically hookup graph on ax to read

00:05:14,190 --> 00:05:20,370
from Prometheus right away so it's it's

00:05:18,300 --> 00:05:22,230
a good thing to use so here's what it

00:05:20,370 --> 00:05:25,680
looks like when you have spinnaker

00:05:22,230 --> 00:05:28,170
running on a GK a specifically but this

00:05:25,680 --> 00:05:30,000
could be any kubernetes cluster all of

00:05:28,170 --> 00:05:33,840
your micro services for spinnaker and

00:05:30,000 --> 00:05:36,480
then Prometheus operator you can see on

00:05:33,840 --> 00:05:38,810
the top six or so has different

00:05:36,480 --> 00:05:43,050
deployments for graph on ax operator

00:05:38,810 --> 00:05:48,150
itself all their manager and Prometheus

00:05:43,050 --> 00:05:52,620
itself so how do we get there what how

00:05:48,150 --> 00:05:54,540
do we set this up well first step is a

00:05:52,620 --> 00:05:57,540
pretty easy straightforward halyard

00:05:54,540 --> 00:05:59,850
command of just enabling a Prometheus

00:05:57,540 --> 00:06:04,050
metric store there are different types

00:05:59,850 --> 00:06:06,750
of metrics doors for spinnaker data dog

00:06:04,050 --> 00:06:09,090
and stackdriver are the other two data

00:06:06,750 --> 00:06:10,860
dog and stack driver obviously paid

00:06:09,090 --> 00:06:12,900
services stack driver is a pretty

00:06:10,860 --> 00:06:16,140
popular one because it's obviously comes

00:06:12,900 --> 00:06:18,090
with GCP but it can be very verbose

00:06:16,140 --> 00:06:20,180
spinnaker by default throws a lots of

00:06:18,090 --> 00:06:23,310
lots of metrics and so if you're not

00:06:20,180 --> 00:06:24,930
paying attention to that and you want to

00:06:23,310 --> 00:06:28,260
see how high your strapped driver bill

00:06:24,930 --> 00:06:31,590
can get you should enable it you should

00:06:28,260 --> 00:06:33,560
see how it goes there's a there's a PR I

00:06:31,590 --> 00:06:38,400
think they just got merged last month

00:06:33,560 --> 00:06:39,720
kind of putting some opinions and

00:06:38,400 --> 00:06:43,230
experience

00:06:39,720 --> 00:06:44,700
one of the customers ran into in the

00:06:43,230 --> 00:06:45,660
actual documentation now so when you

00:06:44,700 --> 00:06:47,280
actually go look at the stackdriver

00:06:45,660 --> 00:06:51,290
documentation now you'll see a little

00:06:47,280 --> 00:06:53,520
bit of a warning prometheus is great but

00:06:51,290 --> 00:06:56,400
after you enable one of these three

00:06:53,520 --> 00:06:59,550
stores you'll see a sidecar and all of

00:06:56,400 --> 00:07:00,960
the pods so all of your orca pods cloud

00:06:59,550 --> 00:07:02,820
driver pods all of the micro service

00:07:00,960 --> 00:07:04,050
pods if you dial into one of the pods

00:07:02,820 --> 00:07:06,120
and look at the actual containers

00:07:04,050 --> 00:07:07,590
running after you've enabled and

00:07:06,120 --> 00:07:10,020
deployed a metric store you'll see a

00:07:07,590 --> 00:07:13,860
monitoring daemon the monitoring daemon

00:07:10,020 --> 00:07:16,260
is just another repo in the spinnaker

00:07:13,860 --> 00:07:19,490
organization on github it's another

00:07:16,260 --> 00:07:22,980
spring application that knows how to

00:07:19,490 --> 00:07:25,800
talk to the micro service and work on

00:07:22,980 --> 00:07:29,730
this case and then expose a port for

00:07:25,800 --> 00:07:32,490
Prometheus to screen so in the

00:07:29,730 --> 00:07:34,500
Prometheus operator world one of the CR

00:07:32,490 --> 00:07:37,050
DS that it defines is called a service

00:07:34,500 --> 00:07:40,320
monitor and what that means is it's

00:07:37,050 --> 00:07:42,450
essentially a way to say hey Prometheus

00:07:40,320 --> 00:07:45,210
I've got these places on my cluster

00:07:42,450 --> 00:07:48,630
where you can scrape metrics so go and

00:07:45,210 --> 00:07:51,810
get them this is the service manager

00:07:48,630 --> 00:07:54,570
monitor that I used on just a fresh

00:07:51,810 --> 00:07:57,210
spinnaker install so it has all the

00:07:54,570 --> 00:07:58,800
default values when you install

00:07:57,210 --> 00:08:00,930
spinnaker these are the default values

00:07:58,800 --> 00:08:03,330
these the default ports and paths that

00:08:00,930 --> 00:08:05,700
you're going to find there's only one

00:08:03,330 --> 00:08:09,090
area on line 9 there I have a little

00:08:05,700 --> 00:08:11,760
comment that's the default value but if

00:08:09,090 --> 00:08:16,020
you change it for some reason and that's

00:08:11,760 --> 00:08:19,680
what you need to change so yeah that's

00:08:16,020 --> 00:08:22,440
just a way for Prometheus to select

00:08:19,680 --> 00:08:24,840
which pods it should query for metrics

00:08:22,440 --> 00:08:28,890
where what port and what path it should

00:08:24,840 --> 00:08:32,000
do that on and then the last thing is

00:08:28,890 --> 00:08:35,219
that the spinnaker monitoring repo

00:08:32,000 --> 00:08:37,320
underneath spinnaker on github has a

00:08:35,219 --> 00:08:38,820
bunch of awesome graph ahna dashboards

00:08:37,320 --> 00:08:40,800
like that you can use and that they're

00:08:38,820 --> 00:08:43,830
just right away here here's some great

00:08:40,800 --> 00:08:46,950
dashboards they're all JSON files which

00:08:43,830 --> 00:08:48,390
is cool if you're in a VM land you can

00:08:46,950 --> 00:08:50,339
upload them and even in the kubernetes

00:08:48,390 --> 00:08:52,950
world on a running graph on a pod I

00:08:50,339 --> 00:08:55,320
could do cube got old CP and then

00:08:52,950 --> 00:08:56,760
load all these JSON things but in the

00:08:55,320 --> 00:08:58,530
kubernetes world we don't ever really

00:08:56,760 --> 00:09:00,180
want to have like a pet that's like

00:08:58,530 --> 00:09:02,040
having a pet BM we don't want to have a

00:09:00,180 --> 00:09:03,690
pet pod because as you like delete it

00:09:02,040 --> 00:09:06,360
and then have a new pod and then and

00:09:03,690 --> 00:09:08,190
then all those are gone right and so in

00:09:06,360 --> 00:09:10,470
the kubernetes world we either want it

00:09:08,190 --> 00:09:12,360
to be a yamo manifest or some kind of

00:09:10,470 --> 00:09:15,840
other infrastructure as code where you

00:09:12,360 --> 00:09:18,810
can you can use those and so that's kind

00:09:15,840 --> 00:09:20,400
of step three to shoehorn these this big

00:09:18,810 --> 00:09:25,760
pile of JSON to get all these Griffon

00:09:20,400 --> 00:09:29,130
dashboards so isn't there an easier way

00:09:25,760 --> 00:09:32,760
well let's check this out the

00:09:29,130 --> 00:09:35,010
documentation is super small but the

00:09:32,760 --> 00:09:37,020
official vinegar documentation if you're

00:09:35,010 --> 00:09:38,520
running spinnaker and Prometheus on

00:09:37,020 --> 00:09:40,530
virtual machines oh look at this there's

00:09:38,520 --> 00:09:42,180
an apt apt get install spinnaker

00:09:40,530 --> 00:09:44,910
monitoring and he's just a what an easy

00:09:42,180 --> 00:09:47,850
setup script isn't that nice shouldn't

00:09:44,910 --> 00:09:49,290
we get one for kubernetes well if you

00:09:47,850 --> 00:09:51,390
are deploying spinnaker on kubernetes

00:09:49,290 --> 00:09:53,040
hang tight support is coming that's

00:09:51,390 --> 00:09:58,020
literally what the doc said earlier this

00:09:53,040 --> 00:10:00,600
year so I was at one of our clients with

00:09:58,020 --> 00:10:02,640
almost 100 kubernetes clusters deploying

00:10:00,600 --> 00:10:05,280
to spinnaker and running spinnaker and

00:10:02,640 --> 00:10:08,160
kubernetes and this is what I was what I

00:10:05,280 --> 00:10:11,760
was given so great well forget that

00:10:08,160 --> 00:10:13,230
let's let's make a script our own dang

00:10:11,760 --> 00:10:16,530
selves and open source it so that's what

00:10:13,230 --> 00:10:21,210
I did and so now we can look at the docs

00:10:16,530 --> 00:10:24,350
right before virtual machines because

00:10:21,210 --> 00:10:27,450
it's 2019 so let's talk kubernetes is

00:10:24,350 --> 00:10:29,580
set up scripts so let's read about

00:10:27,450 --> 00:10:31,800
prometheus operator support and there's

00:10:29,580 --> 00:10:33,950
a set up that Sh script that does all

00:10:31,800 --> 00:10:39,890
this stuff for you and you're good to go

00:10:33,950 --> 00:10:42,360
so there's even a prometheus operator

00:10:39,890 --> 00:10:44,070
directory so this is the spinnaker

00:10:42,360 --> 00:10:46,800
monitoring github repo this is super

00:10:44,070 --> 00:10:49,290
small only bump it up data dog

00:10:46,800 --> 00:10:53,370
Prometheus stack driver now there's one

00:10:49,290 --> 00:10:56,040
just for Prometheus operator yes here's

00:10:53,370 --> 00:10:58,020
a big readme in here to help you here's

00:10:56,040 --> 00:11:01,770
your service monitor already defined for

00:10:58,020 --> 00:11:04,980
you here's here's the setup script we'll

00:11:01,770 --> 00:11:07,680
tie let's look at it real quick so this

00:11:04,980 --> 00:11:10,230
is a bunch of bash

00:11:07,680 --> 00:11:12,440
but like most Bosch it looks more

00:11:10,230 --> 00:11:14,399
intimidating than it actually is

00:11:12,440 --> 00:11:18,240
basically what we're doing here is just

00:11:14,399 --> 00:11:20,190
applying your service monitor so again

00:11:18,240 --> 00:11:22,050
the service monitor that's in this repo

00:11:20,190 --> 00:11:25,230
that's the official spinnaker monitoring

00:11:22,050 --> 00:11:27,630
group O is the defaults that you will

00:11:25,230 --> 00:11:29,329
use when you install spinnaker if for

00:11:27,630 --> 00:11:31,980
some reason you change some of those

00:11:29,329 --> 00:11:33,990
that's fine the only place you would

00:11:31,980 --> 00:11:40,680
actually change it I can show you a

00:11:33,990 --> 00:11:44,279
quick is if you prometheus operator has

00:11:40,680 --> 00:11:47,279
a set of labels that it looks for on

00:11:44,279 --> 00:11:49,500
config on service monitors and that's

00:11:47,279 --> 00:11:52,140
what this service monitor selector

00:11:49,500 --> 00:11:54,570
attribute is it says here's the label

00:11:52,140 --> 00:11:56,579
that a service monitor should have in

00:11:54,570 --> 00:11:58,800
order for me to pick up on it and then

00:11:56,579 --> 00:12:01,649
apply it to a Prometheus and process it

00:11:58,800 --> 00:12:03,930
as a service monitor so by default when

00:12:01,649 --> 00:12:06,240
you install Prometheus monitor operator

00:12:03,930 --> 00:12:08,279
this is the value so if you change that

00:12:06,240 --> 00:12:10,529
then good for you go change it but it's

00:12:08,279 --> 00:12:15,209
right there the next thing in our setup

00:12:10,529 --> 00:12:18,180
script we this is basically looking at

00:12:15,209 --> 00:12:21,240
all of the JSON dashboards in the

00:12:18,180 --> 00:12:25,260
Prometheus directory all those VM people

00:12:21,240 --> 00:12:28,290
and process it into a llamÃ³ file and

00:12:25,260 --> 00:12:32,459
then apply all those generated

00:12:28,290 --> 00:12:37,950
dashboards we can look at that real

00:12:32,459 --> 00:12:40,770
quick but the template again this is a

00:12:37,950 --> 00:12:44,220
CR D this is the way Prometheus operator

00:12:40,770 --> 00:12:46,500
works again so if you don't want you

00:12:44,220 --> 00:12:48,630
know upload all these JSON files to your

00:12:46,500 --> 00:12:50,970
graph on up pod and then have the pod

00:12:48,630 --> 00:12:53,520
disappear and all your graphs disappear

00:12:50,970 --> 00:12:55,470
you want to put those as config Maps on

00:12:53,520 --> 00:12:59,010
your cluster under the previous operator

00:12:55,470 --> 00:13:00,959
can see that because we apply this label

00:12:59,010 --> 00:13:02,970
graph on a dashboard equals true and

00:13:00,959 --> 00:13:04,350
then the Prometheus operator picks up on

00:13:02,970 --> 00:13:06,329
and goes oh there's a new config map

00:13:04,350 --> 00:13:08,250
with this label on it let me apply it

00:13:06,329 --> 00:13:09,779
and add it to all microfauna pods to get

00:13:08,250 --> 00:13:12,480
you a nice dashboard so that's what's

00:13:09,779 --> 00:13:14,880
going on there so again that setup

00:13:12,480 --> 00:13:17,670
script is pretty simple it's doing very

00:13:14,880 --> 00:13:19,230
code just a couple of steps but it has

00:13:17,670 --> 00:13:21,430
to do all this kind of shoehorning of

00:13:19,230 --> 00:13:23,440
the JSON but as that JSON is up

00:13:21,430 --> 00:13:28,830
in the Prometheus world you get it for

00:13:23,440 --> 00:13:31,899
free here with this with the skirt so

00:13:28,830 --> 00:13:34,600
this is something I ran into what if you

00:13:31,899 --> 00:13:37,180
use tear form so the spinnaker

00:13:34,600 --> 00:13:38,620
organization is not really you know

00:13:37,180 --> 00:13:40,930
obviously in charge of any kind of

00:13:38,620 --> 00:13:44,010
infrastructure as code but as you as

00:13:40,930 --> 00:13:48,970
operators of spinnaker may run into this

00:13:44,010 --> 00:13:53,380
well I have a repo here where you can

00:13:48,970 --> 00:14:00,760
actually generate all those dashboards

00:13:53,380 --> 00:14:02,920
and use it in a Tara form template so

00:14:00,760 --> 00:14:06,130
here's the terraform file applying a

00:14:02,920 --> 00:14:09,190
kubernetes config map and again the

00:14:06,130 --> 00:14:11,950
setup script here downloads the latest

00:14:09,190 --> 00:14:14,740
from the spinnaker monitoring repo gets

00:14:11,950 --> 00:14:16,240
all those Prometheus Griffon dashboards

00:14:14,740 --> 00:14:17,410
that have that's kind of the central

00:14:16,240 --> 00:14:19,750
place for the monitoring in the

00:14:17,410 --> 00:14:21,870
community to store where we think those

00:14:19,750 --> 00:14:25,660
graphs should go download it and then

00:14:21,870 --> 00:14:28,930
put it into a terraform repo terraform

00:14:25,660 --> 00:14:30,790
format for you then to do a terraform

00:14:28,930 --> 00:14:33,480
apply on and have those all this config

00:14:30,790 --> 00:14:38,560
Maps check on all those graphs into your

00:14:33,480 --> 00:14:43,420
source code and you're good good to go

00:14:38,560 --> 00:14:44,950
so in that repo - I have a helm

00:14:43,420 --> 00:14:46,870
directory as well that just does the

00:14:44,950 --> 00:14:48,940
same thing but just for Gamal but if you

00:14:46,870 --> 00:14:51,310
only want to go later and look at this I

00:14:48,940 --> 00:14:53,230
have a nice demo script in here that

00:14:51,310 --> 00:14:55,660
actually spins up a new cluster on gke

00:14:53,230 --> 00:14:58,209
for you installs spinnaker installs

00:14:55,660 --> 00:15:01,270
Prometheus operator and then does the

00:14:58,209 --> 00:15:03,400
same same steps that the setup script

00:15:01,270 --> 00:15:05,650
does where we just apply your service

00:15:03,400 --> 00:15:07,540
monitor and then apply it generate a

00:15:05,650 --> 00:15:10,480
bunch of these dashboards and then apply

00:15:07,540 --> 00:15:12,459
them for you so if you want to go

00:15:10,480 --> 00:15:14,920
through that it's really nice I've got

00:15:12,459 --> 00:15:16,900
it running and we can look at it in a

00:15:14,920 --> 00:15:22,209
second here but but that's what that

00:15:16,900 --> 00:15:29,730
does okay so I guess real quick any

00:15:22,209 --> 00:15:29,730
questions and in any of that yeah

00:15:30,850 --> 00:15:42,620
yep that's the worst of all yep use that

00:15:34,100 --> 00:15:45,560
in 12 yeah that will just output the

00:15:42,620 --> 00:15:47,329
kubernetes config map resources us and

00:15:45,560 --> 00:15:49,040
so you can put that in any resource you

00:15:47,329 --> 00:15:51,410
want so ideally in your terraform

00:15:49,040 --> 00:15:53,600
workspaces each tariff own workspace

00:15:51,410 --> 00:15:54,920
would map to a github repo and what I'm

00:15:53,600 --> 00:15:56,120
saying here is you would kind of you

00:15:54,920 --> 00:15:57,649
would check in those terraform files

00:15:56,120 --> 00:16:02,870
into that repo and they'd be in that

00:15:57,649 --> 00:16:04,579
workspace yeah so let's kind of go on to

00:16:02,870 --> 00:16:07,630
then what does it mean to monitor

00:16:04,579 --> 00:16:10,100
spinnaker so we have spinnaker running

00:16:07,630 --> 00:16:11,899
so we want to make sure that we're

00:16:10,100 --> 00:16:16,130
servicing all of our users pretty well

00:16:11,899 --> 00:16:18,589
so real quick we'll introduce what

00:16:16,130 --> 00:16:21,860
metrics types of metrics spinnaker emits

00:16:18,589 --> 00:16:23,810
each of its microservices emits a bunch

00:16:21,860 --> 00:16:25,839
of metrics trying to report back what's

00:16:23,810 --> 00:16:28,820
going on edge the micro services and

00:16:25,839 --> 00:16:30,740
unless you're one of the developers who

00:16:28,820 --> 00:16:32,209
contributed to the micro service it will

00:16:30,740 --> 00:16:35,329
be really hard for you to understand

00:16:32,209 --> 00:16:38,000
which metrics it's popping off and what

00:16:35,329 --> 00:16:40,880
those meaning right the three types that

00:16:38,000 --> 00:16:42,560
spinnaker emits are counters which are

00:16:40,880 --> 00:16:44,890
just increasing numbers numbers that get

00:16:42,560 --> 00:16:48,470
bigger a gauge it's just a random

00:16:44,890 --> 00:16:51,890
instantaneous number and a timer always

00:16:48,470 --> 00:16:53,089
in nanoseconds so if you're again if

00:16:51,890 --> 00:16:56,240
you're getting too much data to process

00:16:53,089 --> 00:16:57,370
you'll likely run into this with like

00:16:56,240 --> 00:17:00,110
stackdriver

00:16:57,370 --> 00:17:03,290
you can actually limit which metrics are

00:17:00,110 --> 00:17:05,089
collected with reg X so again the

00:17:03,290 --> 00:17:09,079
monitoring daemon the monitoring side

00:17:05,089 --> 00:17:12,559
car is a spring boot application and you

00:17:09,079 --> 00:17:15,350
can in the spring profile here you can

00:17:12,559 --> 00:17:17,510
also put in some reg X so for instance I

00:17:15,350 --> 00:17:19,730
just want to have these two metrics this

00:17:17,510 --> 00:17:26,240
is where you would do it currently right

00:17:19,730 --> 00:17:28,520
now this will change in 2020 discussing

00:17:26,240 --> 00:17:30,590
with Rob and Netflix and some of these

00:17:28,520 --> 00:17:32,500
other people that are working on this on

00:17:30,590 --> 00:17:34,580
the monitoring side of things

00:17:32,500 --> 00:17:36,770
ideally what's going to happen is that

00:17:34,580 --> 00:17:39,830
each microservice will kind of accept a

00:17:36,770 --> 00:17:42,440
different flag level so kind of almost

00:17:39,830 --> 00:17:44,150
like a verbosity level really

00:17:42,440 --> 00:17:45,830
I'm just kind of getting to know these

00:17:44,150 --> 00:17:47,150
services so just help me give me the

00:17:45,830 --> 00:17:49,970
bare minimum amount of metrics that I

00:17:47,150 --> 00:17:52,220
care about or I have really detailed

00:17:49,970 --> 00:17:54,650
trying to debug figure out what's going

00:17:52,220 --> 00:17:57,080
on so the kind of configuration of some

00:17:54,650 --> 00:17:59,960
of those is going to change in 2020 so

00:17:57,080 --> 00:18:02,210
you can look out for that so what are

00:17:59,960 --> 00:18:06,070
some use cases for wanting to like even

00:18:02,210 --> 00:18:08,720
see metrics that spinnaker emits well

00:18:06,070 --> 00:18:11,450
you want to be able to support your

00:18:08,720 --> 00:18:13,670
users your developer teams when they're

00:18:11,450 --> 00:18:16,670
having releases and when you're updating

00:18:13,670 --> 00:18:19,300
spinnaker it was super helpful countless

00:18:16,670 --> 00:18:23,600
times working with big customers where

00:18:19,300 --> 00:18:25,430
we would be we could hop onto a graph on

00:18:23,600 --> 00:18:27,440
a graph and see which applications

00:18:25,430 --> 00:18:29,180
currently running which stages they're

00:18:27,440 --> 00:18:31,460
running on and how long they've been

00:18:29,180 --> 00:18:33,890
running and be able to say things to a

00:18:31,460 --> 00:18:35,570
customer like hey I noticed this the

00:18:33,890 --> 00:18:38,150
plane has been taking forever what's the

00:18:35,570 --> 00:18:40,010
backstory right your manual judgment has

00:18:38,150 --> 00:18:41,750
been running for 14 days are you guys

00:18:40,010 --> 00:18:45,080
gonna push yes or no or like you know

00:18:41,750 --> 00:18:46,640
what's up the other case where it came

00:18:45,080 --> 00:18:48,680
in handy was when you're making updates

00:18:46,640 --> 00:18:51,050
to spinnaker so rolling at new versions

00:18:48,680 --> 00:18:53,390
spinnaker is always going really fast as

00:18:51,050 --> 00:18:55,280
a community which is awesome to see and

00:18:53,390 --> 00:18:57,640
if you're updating it frequently you

00:18:55,280 --> 00:19:00,470
know it's really helpful to see like Oh

00:18:57,640 --> 00:19:02,330
Jane is doing a new deployment so hey

00:19:00,470 --> 00:19:03,740
Jane you might have to hit refresh on

00:19:02,330 --> 00:19:07,550
your screen in a second because I'm

00:19:03,740 --> 00:19:08,900
about to release a new version it's also

00:19:07,550 --> 00:19:11,870
really helpful when you're working with

00:19:08,900 --> 00:19:13,820
the people in the community around a

00:19:11,870 --> 00:19:15,800
micro service so I ran into this a bunch

00:19:13,820 --> 00:19:17,300
with cloud driver if any of you were

00:19:15,800 --> 00:19:19,940
here for the kubernetes talk that just

00:19:17,300 --> 00:19:22,100
happened in this room quite a few of

00:19:19,940 --> 00:19:25,310
those improvements that they were

00:19:22,100 --> 00:19:27,260
talking about came from some clients

00:19:25,310 --> 00:19:30,320
that I worked with and being able to

00:19:27,260 --> 00:19:32,150
have metrics to be able to actually show

00:19:30,320 --> 00:19:33,740
here's the performance that we're having

00:19:32,150 --> 00:19:35,600
in cloud driver and here's what metrics

00:19:33,740 --> 00:19:38,300
are going off was really helpful in

00:19:35,600 --> 00:19:39,800
actually getting support from the people

00:19:38,300 --> 00:19:44,030
that were most familiar with those micro

00:19:39,800 --> 00:19:45,680
service code you also want to use a lot

00:19:44,030 --> 00:19:48,080
of this monitoring when in order to

00:19:45,680 --> 00:19:50,060
provide a really stable application so

00:19:48,080 --> 00:19:52,040
when do we need to actually scale

00:19:50,060 --> 00:19:55,100
spinnaker resources when do we need to

00:19:52,040 --> 00:19:56,210
get more pods of Orca or cloud driver

00:19:55,100 --> 00:19:59,629
those are the main two

00:19:56,210 --> 00:20:01,850
that you're gonna be focused on yeah and

00:19:59,629 --> 00:20:04,309
so then you can also add alerts to alert

00:20:01,850 --> 00:20:05,450
manager to say hey you know we're

00:20:04,309 --> 00:20:07,190
getting bottlenecked here and some of

00:20:05,450 --> 00:20:08,119
these resources we need to put some more

00:20:07,190 --> 00:20:12,919
horsepower behind one of these

00:20:08,119 --> 00:20:16,039
micro-services all right so okay I've

00:20:12,919 --> 00:20:20,450
got some metrics I have some great use

00:20:16,039 --> 00:20:24,080
cases well okay what happens when these

00:20:20,450 --> 00:20:26,950
metrics go haywire well eventually the

00:20:24,080 --> 00:20:30,409
vision is to kind of make a run book

00:20:26,950 --> 00:20:32,210
that's kind of the 20/20 vision will so

00:20:30,409 --> 00:20:35,509
be on the lookout for that in 2020

00:20:32,210 --> 00:20:38,330
that's kind of a community effort to map

00:20:35,509 --> 00:20:40,190
allies metrics more towards options and

00:20:38,330 --> 00:20:43,119
things that you might actually want to

00:20:40,190 --> 00:20:47,119
change or might actually want to

00:20:43,119 --> 00:20:50,119
reconfigure in the meantime probably the

00:20:47,119 --> 00:20:52,340
best resource that I found to actually

00:20:50,119 --> 00:20:54,470
understand what you should do and what's

00:20:52,340 --> 00:20:56,960
amazing I tricked mean is an awesome

00:20:54,470 --> 00:21:00,830
blog post last year from Rob and Netflix

00:20:56,960 --> 00:21:02,690
who's over there hey Rob so it took Rob

00:21:00,830 --> 00:21:04,850
like a month to put this one blog post

00:21:02,690 --> 00:21:06,919
together it's like super detailed we can

00:21:04,850 --> 00:21:08,529
look at it real quick but I linked to it

00:21:06,919 --> 00:21:12,200
and a bunch of things in the code here

00:21:08,529 --> 00:21:14,149
but it's super long and he does an

00:21:12,200 --> 00:21:16,610
awesome job of like showing all these

00:21:14,149 --> 00:21:18,289
graphs of hairs how we're using it here

00:21:16,610 --> 00:21:19,519
the metrics and here's this metric and

00:21:18,289 --> 00:21:22,970
what does that mean and here's what you

00:21:19,519 --> 00:21:25,309
should do when the metric goes off it I

00:21:22,970 --> 00:21:26,720
probably spent like a month digesting

00:21:25,309 --> 00:21:28,940
all of this and trying to understand

00:21:26,720 --> 00:21:31,490
exactly what everything meant and and

00:21:28,940 --> 00:21:34,879
being able to you know then put it into

00:21:31,490 --> 00:21:37,399
action for instance one of his best

00:21:34,879 --> 00:21:38,659
recommendations is if you get users that

00:21:37,399 --> 00:21:41,360
come to you and just say spinnaker is

00:21:38,659 --> 00:21:43,970
slow there's one metric in Orca

00:21:41,360 --> 00:21:47,269
called ready messages that that should

00:21:43,970 --> 00:21:49,159
also always be zero and if it's more

00:21:47,269 --> 00:21:52,149
than zero then that means that there are

00:21:49,159 --> 00:21:55,450
tasks just sitting there waiting for

00:21:52,149 --> 00:21:58,190
Orca to pick it up and go process it and

00:21:55,450 --> 00:22:01,549
that's obviously really really bad so

00:21:58,190 --> 00:22:06,799
you want more more Orca pods than or

00:22:01,549 --> 00:22:09,799
more Orca memories to be you but oh if

00:22:06,799 --> 00:22:10,250
only you had a nice dashboard nice graph

00:22:09,799 --> 00:22:12,860
on a dead

00:22:10,250 --> 00:22:15,320
Ford from Rob's awesome post that helped

00:22:12,860 --> 00:22:17,360
you identify those metrics and then you

00:22:15,320 --> 00:22:18,980
know put Rob's explanation context

00:22:17,360 --> 00:22:21,950
around those metrics right in there

00:22:18,980 --> 00:22:24,740
along with some alerts well I made it

00:22:21,950 --> 00:22:28,370
and it's open source it's in one of my

00:22:24,740 --> 00:22:31,490
repos I'll link to it here called I

00:22:28,370 --> 00:22:34,040
called the RZ dashboard it's in it's in

00:22:31,490 --> 00:22:36,140
this dashboard here and so not only up

00:22:34,040 --> 00:22:39,200
here in this JSON by the way this is

00:22:36,140 --> 00:22:41,750
this is what one of these generated the

00:22:39,200 --> 00:22:44,180
mo files will look like after you run my

00:22:41,750 --> 00:22:47,000
script that takes some of the JSON out

00:22:44,180 --> 00:22:48,860
you get a config map here with a graph

00:22:47,000 --> 00:22:51,740
on a dashboard true label and then

00:22:48,860 --> 00:22:53,960
you'll have this data of a JSON and it's

00:22:51,740 --> 00:22:56,750
all just shoved in here so not only do

00:22:53,960 --> 00:22:59,240
we have some some the actual gravano

00:22:56,750 --> 00:23:00,650
dashboard then you can you can't read

00:22:59,240 --> 00:23:03,080
yes you can read some of this but this

00:23:00,650 --> 00:23:04,730
is actually English this is Rob's blog

00:23:03,080 --> 00:23:07,150
post like this is Rob's like hey

00:23:04,730 --> 00:23:09,740
literally copy and pasted similar

00:23:07,150 --> 00:23:11,240
objects plaining what this metric is so

00:23:09,740 --> 00:23:12,890
that you can go into grow fauna and just

00:23:11,240 --> 00:23:14,420
hover over it and go oh yeah that's what

00:23:12,890 --> 00:23:20,510
that is that's why I care about that

00:23:14,420 --> 00:23:21,920
right so that's pretty cool and the ones

00:23:20,510 --> 00:23:23,480
on the right side actually even have

00:23:21,920 --> 00:23:25,850
health monitors on them so you can

00:23:23,480 --> 00:23:27,410
actually already predefined for you to

00:23:25,850 --> 00:23:31,730
say oh this is good this is bad

00:23:27,410 --> 00:23:39,100
cool all right any questions before we

00:23:31,730 --> 00:23:39,100
move on okay yeah

00:23:39,480 --> 00:23:47,310
the Diigo fauna has monitor a alerting

00:23:43,050 --> 00:23:49,870
optional it's only phone so Cortana has

00:23:47,310 --> 00:23:52,120
kind of they call it health checks and

00:23:49,870 --> 00:23:54,130
so it can be you know green a healthy or

00:23:52,120 --> 00:23:56,200
unhealthy and then alert manager can

00:23:54,130 --> 00:23:57,850
interface with other services like slack

00:23:56,200 --> 00:24:00,730
or text message or email in order to

00:23:57,850 --> 00:24:02,380
actually send an alert it can actually

00:24:00,730 --> 00:24:04,540
it also has just a little dashboard to

00:24:02,380 --> 00:24:08,170
show you status some different alerts as

00:24:04,540 --> 00:24:11,220
well but yeah so it's kind of a

00:24:08,170 --> 00:24:14,290
combination of Griffin and alert manager

00:24:11,220 --> 00:24:15,760
Griffin a in the dashboard can tell you

00:24:14,290 --> 00:24:17,650
whether that metric is healthy or not

00:24:15,760 --> 00:24:20,410
and then it's up to you to configure

00:24:17,650 --> 00:24:22,240
alert manager to say I'm gonna write it

00:24:20,410 --> 00:24:25,570
out to slack or I'm gonna call somebody

00:24:22,240 --> 00:24:32,290
or whatever you want to get anything

00:24:25,570 --> 00:24:39,430
else cool let's do a little bit of

00:24:32,290 --> 00:24:41,580
demoing whoops this would be a little

00:24:39,430 --> 00:24:41,580
difficult

00:24:48,930 --> 00:24:53,130
switch computers here real quick

00:24:57,160 --> 00:25:09,850
so give this a second yeah cool that's

00:25:05,350 --> 00:25:16,300
super slow little but so I've got three

00:25:09,850 --> 00:25:21,580
clusters on G K here about 150 gigs of

00:25:16,300 --> 00:25:23,310
memory here and for TCP is so one of

00:25:21,580 --> 00:25:26,620
them is actually running spinnaker and

00:25:23,310 --> 00:25:29,290
another one is just a blank so I might

00:25:26,620 --> 00:25:31,930
actually we'll see if the demo gods like

00:25:29,290 --> 00:25:34,720
us or not maybe I'll maybe I'll actually

00:25:31,930 --> 00:25:36,520
write to do in the the codes but we can

00:25:34,720 --> 00:25:46,590
also just jump into the finished product

00:25:36,520 --> 00:25:57,340
here let's just look at that first so

00:25:46,590 --> 00:25:59,470
this is just the spinnaker prometheus

00:25:57,340 --> 00:26:02,110
repo that I have where I have that demo

00:25:59,470 --> 00:26:04,300
I also have some really handy connect

00:26:02,110 --> 00:26:05,650
and disconnect cuba at all port forward

00:26:04,300 --> 00:26:09,210
commands in there so that's what i was

00:26:05,650 --> 00:26:13,210
just running so we just connected to

00:26:09,210 --> 00:26:16,180
spinnaker on one of my clusters here's a

00:26:13,210 --> 00:26:19,540
hello world application that just does a

00:26:16,180 --> 00:26:24,490
straightforward kubernetes v2 provider

00:26:19,540 --> 00:26:32,760
deployment on our app cluster and if i

00:26:24,490 --> 00:26:32,760
just restart it we can get some graphs

00:26:33,360 --> 00:26:41,370
so here's one that is so i guess i

00:26:39,310 --> 00:26:45,040
should set back for a second this is

00:26:41,370 --> 00:26:47,560
besides the RZ dashboard these are all

00:26:45,040 --> 00:26:49,780
the dashboards that are in the spinnaker

00:26:47,560 --> 00:26:52,600
monitoring repo right now that you can

00:26:49,780 --> 00:26:55,660
go out and get there is one for each

00:26:52,600 --> 00:26:58,120
micro service you can see that's what

00:26:55,660 --> 00:27:00,070
the top eight are then there's one for

00:26:58,120 --> 00:27:03,430
specific spinnaker application details

00:27:00,070 --> 00:27:05,320
one for AWS one for GCP one for just

00:27:03,430 --> 00:27:07,870
kubernetes and then one called

00:27:05,320 --> 00:27:12,370
minimalist that someone decided was kind

00:27:07,870 --> 00:27:15,700
of just nice to look at so each of them

00:27:12,370 --> 00:27:17,890
provide different metrics they all after

00:27:15,700 --> 00:27:21,640
we have this Prometheus operator setup

00:27:17,890 --> 00:27:26,230
going already is configured to listen to

00:27:21,640 --> 00:27:28,870
Prometheus so here's the dashboard that

00:27:26,230 --> 00:27:31,270
breaks everything down by application so

00:27:28,870 --> 00:27:32,470
I can hover over this and this is

00:27:31,270 --> 00:27:34,960
probably really hard to see for most

00:27:32,470 --> 00:27:38,020
people in the back but this is saying

00:27:34,960 --> 00:27:42,940
that the hello world application used

00:27:38,020 --> 00:27:44,800
the deploy manifest stage and so yeah

00:27:42,940 --> 00:27:46,930
like that's good that's what I want to

00:27:44,800 --> 00:27:49,720
see the data coming in and have live

00:27:46,930 --> 00:27:52,450
graphs here to help me understand what's

00:27:49,720 --> 00:27:55,780
what's going on here

00:27:52,450 --> 00:27:58,030
here's that RZ spinnaker dashboard I was

00:27:55,780 --> 00:28:00,970
talking about that's pretty helpful so

00:27:58,030 --> 00:28:04,750
this is that orca queue depth that we

00:28:00,970 --> 00:28:08,230
were discussing before and yeah that's a

00:28:04,750 --> 00:28:09,490
long overlay but it's helpful to be able

00:28:08,230 --> 00:28:13,000
to read Rob Rob

00:28:09,490 --> 00:28:14,200
some wise words here to remember oh yeah

00:28:13,000 --> 00:28:19,530
what is this thing and why do I care

00:28:14,200 --> 00:28:21,880
about it and that's uh so that's green

00:28:19,530 --> 00:28:25,990
yeah this this is a really helpful graph

00:28:21,880 --> 00:28:28,450
to see basically like I said before Orca

00:28:25,990 --> 00:28:31,809
has a queues of tasks and if it's

00:28:28,450 --> 00:28:33,340
doesn't have enough resources to keep up

00:28:31,809 --> 00:28:37,030
with that queue and keep things moving

00:28:33,340 --> 00:28:40,290
things will just sit in the queue and so

00:28:37,030 --> 00:28:40,290
that's obviously really bad

00:28:40,510 --> 00:28:45,460
here's one for just Orca all of these

00:28:43,110 --> 00:28:46,960
micro-services also have memory usage

00:28:45,460 --> 00:28:51,520
graphs down here which are really nice

00:28:46,960 --> 00:28:52,840
to be able to be able to see so that's

00:28:51,520 --> 00:28:54,220
really cool cloud driver is the other

00:28:52,840 --> 00:28:57,940
one too that you're going to want to

00:28:54,220 --> 00:29:01,450
make sure scaling and operating well and

00:28:57,940 --> 00:29:04,000
so you can see again just a bunch of

00:29:01,450 --> 00:29:09,309
bunch of awesome data around any kind of

00:29:04,000 --> 00:29:15,850
failures memory usage latency yeah it's

00:29:09,309 --> 00:29:18,930
really helpful yeah so

00:29:15,850 --> 00:29:18,930
[Music]

00:29:19,350 --> 00:29:25,640
any other questions before I move on

00:29:21,440 --> 00:29:25,640
okay yeah

00:29:30,460 --> 00:29:36,940
what's the graph the teach off of the

00:29:34,029 --> 00:29:39,909
application states it shows the current

00:29:36,940 --> 00:29:44,019
this one the left one it's the actual

00:29:39,909 --> 00:29:45,639
applications that'll this is the hello

00:29:44,019 --> 00:29:47,379
world so if we have multiple

00:29:45,639 --> 00:29:52,830
applications on the same spinnaker so

00:29:47,379 --> 00:29:52,830
you'll see everything no we can actually

00:29:53,970 --> 00:30:02,049
so I wrote a script that I've got like

00:29:58,570 --> 00:30:04,869
four different applications here I'll go

00:30:02,049 --> 00:30:06,639
back here second I've got ones just that

00:30:04,869 --> 00:30:07,690
explain just some basic things so

00:30:06,639 --> 00:30:10,059
there's one that just kind of waits

00:30:07,690 --> 00:30:12,549
first or amount of time one that just is

00:30:10,059 --> 00:30:15,309
a hello world one that does a manual

00:30:12,549 --> 00:30:17,679
judgment here we can look at Canarian

00:30:15,309 --> 00:30:22,809
here in a second but i just popped off a

00:30:17,679 --> 00:30:30,179
whole ton of triggers so we can go back

00:30:22,809 --> 00:30:30,179
and look yeah so

00:30:31,680 --> 00:30:37,810
five minutes is the least we can do but

00:30:34,720 --> 00:30:47,340
so yeah you can see we just popped off a

00:30:37,810 --> 00:30:47,340
few more applications so yeah a lot more

00:30:49,290 --> 00:30:57,820
so I I can step through if it's helpful

00:30:53,110 --> 00:31:01,030
at some point the actual each line that

00:30:57,820 --> 00:31:02,950
this ateb script is doing but one thing

00:31:01,030 --> 00:31:07,480
I probably want to get to that will be

00:31:02,950 --> 00:31:10,450
fun is one thing I wanted to mention was

00:31:07,480 --> 00:31:14,080
canarian with prometheus

00:31:10,450 --> 00:31:16,120
so again carrying you can use different

00:31:14,080 --> 00:31:18,850
metric stores different ways to collect

00:31:16,120 --> 00:31:21,340
that one situation we ran into with a

00:31:18,850 --> 00:31:24,940
call with one of our clients was what if

00:31:21,340 --> 00:31:26,710
you have multiple clusters and you

00:31:24,940 --> 00:31:28,480
usually like I said you want to install

00:31:26,710 --> 00:31:30,880
Prometheus operator I'm probably all of

00:31:28,480 --> 00:31:33,460
your clusters in this case you know we

00:31:30,880 --> 00:31:35,740
you can use whatever you're using to

00:31:33,460 --> 00:31:37,270
provision clusters and then put you know

00:31:35,740 --> 00:31:39,910
kind of pre installed applications on

00:31:37,270 --> 00:31:43,450
each cluster in our case we're using

00:31:39,910 --> 00:31:45,760
terraform so we have all these clusters

00:31:43,450 --> 00:31:47,290
you have like 100 G K clusters and they

00:31:45,760 --> 00:31:48,730
all have fermius operator on them

00:31:47,290 --> 00:31:50,920
you're gonna deploy an application to

00:31:48,730 --> 00:31:53,530
one of those clusters it's going to then

00:31:50,920 --> 00:31:57,220
report metrics to that clusters instance

00:31:53,530 --> 00:31:59,290
of Prometheus well how does spinnaker

00:31:57,220 --> 00:32:01,290
you're running over on this cluster see

00:31:59,290 --> 00:32:04,480
all these other clusters Prometheus

00:32:01,290 --> 00:32:07,180
instances to be able to analyze metrics

00:32:04,480 --> 00:32:08,680
on an application that you just deployed

00:32:07,180 --> 00:32:10,000
on that cluster right and then you can

00:32:08,680 --> 00:32:14,320
make a canary analysis and decision

00:32:10,000 --> 00:32:16,450
right so again canary has different

00:32:14,320 --> 00:32:21,070
metrics stores options but one of the

00:32:16,450 --> 00:32:24,370
things we had to work with the awesome

00:32:21,070 --> 00:32:25,840
core spinnaker team Maggie did an

00:32:24,370 --> 00:32:27,430
awesome job I don't think she's here but

00:32:25,840 --> 00:32:29,590
she did an awesome job with the UI here

00:32:27,430 --> 00:32:32,970
we're actually now in your community

00:32:29,590 --> 00:32:38,110
analysis stage if you have multiple

00:32:32,970 --> 00:32:40,450
metrics accounts you can select them so

00:32:38,110 --> 00:32:42,520
this spinnaker cluster is the cluster

00:32:40,450 --> 00:32:44,770
where I'm running spinnaker this is my

00:32:42,520 --> 00:32:48,040
application cluster where

00:32:44,770 --> 00:32:53,680
deploying my application and so I can

00:32:48,040 --> 00:32:59,770
just go in there and and and tell it to

00:32:53,680 --> 00:33:04,710
hit the right Prometheus instance any

00:32:59,770 --> 00:33:04,710
other questions yeah

00:33:06,270 --> 00:33:13,480
are there any predefined alerts which we

00:33:09,909 --> 00:33:15,490
can use so I left the alert manager

00:33:13,480 --> 00:33:18,159
alerts part up to you and your

00:33:15,490 --> 00:33:19,510
installation because obviously when

00:33:18,159 --> 00:33:22,029
you're you know deploying

00:33:19,510 --> 00:33:24,039
setting up with your slack account and

00:33:22,029 --> 00:33:25,840
your Twilio credentials and stuff like

00:33:24,039 --> 00:33:28,870
that I left that to you guys

00:33:25,840 --> 00:33:31,659
the arsy dashboard has you know some

00:33:28,870 --> 00:33:33,880
health status on here so you could see

00:33:31,659 --> 00:33:36,100
that green heart there says like we're

00:33:33,880 --> 00:33:37,960
good and if that were to be in a bad

00:33:36,100 --> 00:33:39,640
state it would be red and start

00:33:37,960 --> 00:33:42,130
complaining and so you would Steve stuff

00:33:39,640 --> 00:33:44,350
but I left the kind of alerting to

00:33:42,130 --> 00:33:50,309
external services configuration up to up

00:33:44,350 --> 00:33:50,309
to you yeah cool any other questions

00:33:51,149 --> 00:33:56,370
cool awesome thanks for coming

00:33:56,480 --> 00:33:59,970

YouTube URL: https://www.youtube.com/watch?v=3XHsuUuN9zY


