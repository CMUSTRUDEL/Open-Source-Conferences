Title: Rob Zienert and Adam Jordens "Evolution of Spinnaker Internals & Operations" - Spinnaker Summit 2019
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	The third annual Spinnaker Summit (Diamond Sponsors: Netflix, Google and Armory) was held at the Hard Rock Hotel in San Diego, CA from November 15-17, 2019 and welcomed over 500 members of the rapidly growing Spinnaker open source community.
Captions: 
	00:00:05,580 --> 00:00:10,990
[Laughter]

00:00:14,650 --> 00:00:19,570
I'm Adam this is Rob and we're gonna

00:00:17,560 --> 00:00:22,540
spend the next 40 minutes or so talking

00:00:19,570 --> 00:00:24,790
about spinnaker internals we'll talk

00:00:22,540 --> 00:00:26,680
about how they've changed year over year

00:00:24,790 --> 00:00:29,320
and what some of the motivations have

00:00:26,680 --> 00:00:32,980
been well touch on what operations looks

00:00:29,320 --> 00:00:36,070
like at Netflix and in particular some

00:00:32,980 --> 00:00:38,080
of the challenges and winds that we've

00:00:36,070 --> 00:00:39,820
had over the last year many of those

00:00:38,080 --> 00:00:42,670
have made their way back into

00:00:39,820 --> 00:00:44,830
investments in core and spinnaker core

00:00:42,670 --> 00:00:48,070
that are available to everyone in the

00:00:44,830 --> 00:00:51,250
community Rob and I both work at Netflix

00:00:48,070 --> 00:00:53,559
and remember remembers of a platform

00:00:51,250 --> 00:00:56,710
team that ultimately has responsibility

00:00:53,559 --> 00:00:59,950
for spinnaker operations performance

00:00:56,710 --> 00:01:02,289
reliability and extensibility we're the

00:00:59,950 --> 00:01:03,910
team that's on call if there's a

00:01:02,289 --> 00:01:07,869
spinnaker incident and availability

00:01:03,910 --> 00:01:10,270
impacting incident we're also members of

00:01:07,869 --> 00:01:12,789
numerous spinnaker governance committees

00:01:10,270 --> 00:01:15,430
we're both on the TOC the technical

00:01:12,789 --> 00:01:18,909
Oversight Committee and Rob chairs that

00:01:15,430 --> 00:01:21,220
group at netflix were part of a larger

00:01:18,909 --> 00:01:24,040
organization called delivery engineering

00:01:21,220 --> 00:01:26,650
and that work has ultimate ownership

00:01:24,040 --> 00:01:30,040
around spinnaker but earlier this year

00:01:26,650 --> 00:01:31,810
the resilience engineering team came and

00:01:30,040 --> 00:01:34,119
joined us so not only do we have

00:01:31,810 --> 00:01:37,060
responsibility for the pulling software

00:01:34,119 --> 00:01:39,580
to production we also provide the tools

00:01:37,060 --> 00:01:42,729
the platforms and the experiences that

00:01:39,580 --> 00:01:44,710
engineers throughout Netflix use to help

00:01:42,729 --> 00:01:47,680
their systems behave better under

00:01:44,710 --> 00:01:49,930
failure chaos monkey is an early

00:01:47,680 --> 00:01:51,909
incarnation of that more recently

00:01:49,930 --> 00:01:54,490
they've built something called the chaos

00:01:51,909 --> 00:01:56,250
automation platform where engineers are

00:01:54,490 --> 00:01:58,990
able to go and set up self-service

00:01:56,250 --> 00:02:02,680
experiments to deploy their their

00:01:58,990 --> 00:02:05,320
systems integrated States and test the

00:02:02,680 --> 00:02:10,570
impact and a lot of that is delivered

00:02:05,320 --> 00:02:14,320
through spinnaker this talk was touch on

00:02:10,570 --> 00:02:16,620
three large themes certainly will cover

00:02:14,320 --> 00:02:20,520
the evolution how things have changed

00:02:16,620 --> 00:02:23,020
since we gave a similar talk last year

00:02:20,520 --> 00:02:25,300
operations have been getting much better

00:02:23,020 --> 00:02:28,660
we'll talk about the challenges that led

00:02:25,300 --> 00:02:31,600
to where we're at today and we aim to

00:02:28,660 --> 00:02:34,480
not do things in isolation we believe in

00:02:31,600 --> 00:02:37,600
the community and we strive to take what

00:02:34,480 --> 00:02:40,330
we build and implement of Netflix and an

00:02:37,600 --> 00:02:42,850
upstream in to the community we talk

00:02:40,330 --> 00:02:47,110
about things we blog about things and we

00:02:42,850 --> 00:02:48,400
write code that anybody can use we'll

00:02:47,110 --> 00:02:53,170
look at where we're making investments

00:02:48,400 --> 00:02:54,730
both strategically and tactically if we

00:02:53,170 --> 00:02:56,830
were to give a similar talk like this at

00:02:54,730 --> 00:02:58,330
spinnaker summit next year it'll be

00:02:56,830 --> 00:03:00,490
curious to think about will it be on the

00:02:58,330 --> 00:03:06,040
same subject when we'd be talking about

00:03:00,490 --> 00:03:07,720
the same challenges I hope not I don't

00:03:06,040 --> 00:03:10,420
know if we'll have a lot of time for Q&A

00:03:07,720 --> 00:03:12,250
at the end of this talk but there's Anna

00:03:10,420 --> 00:03:14,650
there's a open slack Channel right here

00:03:12,250 --> 00:03:16,270
we'll share the slides in there and Rob

00:03:14,650 --> 00:03:21,340
and I are happy to make time to answer

00:03:16,270 --> 00:03:23,830
any questions just reach out enough with

00:03:21,340 --> 00:03:24,520
the preamble so when I think of

00:03:23,830 --> 00:03:27,550
evolution

00:03:24,520 --> 00:03:31,060
I think of growth and when I think of

00:03:27,550 --> 00:03:33,040
growth I think of numbers so let's take

00:03:31,060 --> 00:03:35,440
a moment to reflect back on where

00:03:33,040 --> 00:03:38,260
Netflix has been and where we're at

00:03:35,440 --> 00:03:41,380
today so when I spoke at the first

00:03:38,260 --> 00:03:44,200
spinnaker summit back in 2017 spinnaker

00:03:41,380 --> 00:03:46,300
was well entrenched at Netflix it was

00:03:44,200 --> 00:03:48,820
used by most of the engineers at the

00:03:46,300 --> 00:03:51,430
time a much smaller set of Engineers

00:03:48,820 --> 00:03:53,980
than we have today but we also had a

00:03:51,430 --> 00:03:56,770
number of critical services that it

00:03:53,980 --> 00:04:00,010
integrated with spinnaker with a number

00:03:56,770 --> 00:04:04,150
of API customers that were dependent on

00:04:00,010 --> 00:04:07,060
the data that we were surfacing through

00:04:04,150 --> 00:04:10,180
to last year we continue to grow we did

00:04:07,060 --> 00:04:12,820
encounter some scaling concerns which

00:04:10,180 --> 00:04:15,790
led to last year's talk that was largely

00:04:12,820 --> 00:04:18,459
around overcoming the first set of those

00:04:15,790 --> 00:04:22,330
we talked about running more complicated

00:04:18,459 --> 00:04:25,030
topologies how we manage Redis replicas

00:04:22,330 --> 00:04:30,220
and chain Revit Redis replicas off of

00:04:25,030 --> 00:04:32,260
register applica to allow us to scale we

00:04:30,220 --> 00:04:35,270
did hit a bit of a wall

00:04:32,260 --> 00:04:38,660
interestingly enough just after last

00:04:35,270 --> 00:04:40,010
year spinnaker summit and we it took it

00:04:38,660 --> 00:04:42,800
took a couple of months to get through

00:04:40,010 --> 00:04:46,190
that wall it kind of lasted until early

00:04:42,800 --> 00:04:48,560
early q1 and Rob will touch on some of

00:04:46,190 --> 00:04:53,900
the some of the strategies that we used

00:04:48,560 --> 00:04:55,790
to overcome that suffice to say we can

00:04:53,900 --> 00:04:58,729
we are continuing to grow and these are

00:04:55,790 --> 00:05:01,190
PS numbers these are these are not large

00:04:58,729 --> 00:05:02,060
by any means but they are indicative of

00:05:01,190 --> 00:05:04,790
stickiness

00:05:02,060 --> 00:05:06,830
these are front door requests to gate

00:05:04,790 --> 00:05:10,250
which if anybody's familiar with the

00:05:06,830 --> 00:05:12,530
internal topology of of spinnaker

00:05:10,250 --> 00:05:16,370
they're they're blown out to all the

00:05:12,530 --> 00:05:20,120
internal micro-services and if you're

00:05:16,370 --> 00:05:23,120
currently emitting metrics to telemetry

00:05:20,120 --> 00:05:24,970
system these metrics are available here

00:05:23,120 --> 00:05:27,500
here's the query that we're using

00:05:24,970 --> 00:05:30,260
internally and anybody should be able to

00:05:27,500 --> 00:05:34,789
get this style of data out of their

00:05:30,260 --> 00:05:36,979
spinnaker usage in terms of performance

00:05:34,789 --> 00:05:40,100
this is where we're at right now this is

00:05:36,979 --> 00:05:43,190
just an arbitrary trailing seven days

00:05:40,100 --> 00:05:46,220
look at all calls to servers fairly

00:05:43,190 --> 00:05:49,940
representative of expensive calls that

00:05:46,220 --> 00:05:52,760
the UI is making and API customers are

00:05:49,940 --> 00:05:56,000
making and if we were to look back a

00:05:52,760 --> 00:06:00,770
year ago we would see averages on the

00:05:56,000 --> 00:06:05,090
order of 20 30 40 seconds under an

00:06:00,770 --> 00:06:09,620
healthy State when degraded we can we

00:06:05,090 --> 00:06:12,080
could spike to over 60 and if anybody's

00:06:09,620 --> 00:06:15,080
familiar with how the spinnaker services

00:06:12,080 --> 00:06:17,300
communicate with one another you might

00:06:15,080 --> 00:06:20,960
realize that hey there's a 60 second

00:06:17,300 --> 00:06:23,300
network time oh by default and when we

00:06:20,960 --> 00:06:25,910
were spiking over that chances are the

00:06:23,300 --> 00:06:28,220
originating side was retrying some of

00:06:25,910 --> 00:06:30,800
those requests just throwing more fuel

00:06:28,220 --> 00:06:33,530
on a fire and further overwhelming

00:06:30,800 --> 00:06:36,470
services that couldn't handle the base

00:06:33,530 --> 00:06:38,539
load fortunately we're well through this

00:06:36,470 --> 00:06:41,810
and we're orders of magnitude better

00:06:38,539 --> 00:06:44,720
across the board both for large services

00:06:41,810 --> 00:06:46,460
we have some services that peak at

00:06:44,720 --> 00:06:48,260
thousand instances in a single

00:06:46,460 --> 00:06:49,970
application and we also had some

00:06:48,260 --> 00:06:51,860
services that run thousands and

00:06:49,970 --> 00:06:53,180
thousands of clusters those

00:06:51,860 --> 00:06:57,380
traditionally have stretched us on

00:06:53,180 --> 00:06:59,930
different dimensions I mentioned

00:06:57,380 --> 00:07:01,580
stickiness I believe wholeheartedly that

00:06:59,930 --> 00:07:04,130
it's successful spinnaker in an

00:07:01,580 --> 00:07:07,580
organization is sticky it has

00:07:04,130 --> 00:07:10,640
integrations with other systems in your

00:07:07,580 --> 00:07:13,970
company that's very prevalent at Netflix

00:07:10,640 --> 00:07:16,280
here's a few examples we have a number

00:07:13,970 --> 00:07:18,110
of custom auto scalars that are both

00:07:16,280 --> 00:07:21,760
retrieving a lot of data from

00:07:18,110 --> 00:07:26,810
Spinnaker's but also making a lot of

00:07:21,760 --> 00:07:29,380
orchestrated changes if we look at the

00:07:26,810 --> 00:07:33,080
usage of spinnaker broken down into

00:07:29,380 --> 00:07:36,980
people so real humans engineers member

00:07:33,080 --> 00:07:40,640
members of the product community on any

00:07:36,980 --> 00:07:42,700
given day 750 people interact with

00:07:40,640 --> 00:07:46,310
spinnaker through the user interface

00:07:42,700 --> 00:07:49,130
this does drop on the weekends obviously

00:07:46,310 --> 00:07:51,500
we don't have a culture of going on

00:07:49,130 --> 00:07:54,530
weekends making a lot of changes off

00:07:51,500 --> 00:07:57,770
hours chances are this 125 people our

00:07:54,530 --> 00:07:59,750
operators maybe they've gotten page and

00:07:57,770 --> 00:08:03,919
they're directed to spinnaker to take a

00:07:59,750 --> 00:08:06,200
look at their service what's interesting

00:08:03,919 --> 00:08:09,290
is the API integrations we have much

00:08:06,200 --> 00:08:12,669
less of them but they are fairly fairly

00:08:09,290 --> 00:08:15,669
consistent day over day and growing

00:08:12,669 --> 00:08:15,669
quarter-over-quarter

00:08:16,630 --> 00:08:23,030
Netflix is still all in on AWS the

00:08:20,360 --> 00:08:26,540
majority of our deployments are two VMs

00:08:23,030 --> 00:08:28,700
running on ec2 but there has been there

00:08:26,540 --> 00:08:30,919
has been continuing growth on the

00:08:28,700 --> 00:08:34,030
container side and all of our containers

00:08:30,919 --> 00:08:37,039
are deployed to Titus Netflix built

00:08:34,030 --> 00:08:43,310
container orchestration platform and it

00:08:37,039 --> 00:08:46,400
runs itself runs on top of AWS we're

00:08:43,310 --> 00:08:51,050
deploying upwards of 10,000 times to ec2

00:08:46,400 --> 00:08:53,030
VMs and 6,000 times with containers now

00:08:51,050 --> 00:08:56,209
there's a bit of a caveat here

00:08:53,030 --> 00:08:56,850
deployment is a new server group created

00:08:56,209 --> 00:08:58,440
and be

00:08:56,850 --> 00:09:01,190
cuz we're deploying to multiple regions

00:08:58,440 --> 00:09:04,019
and multiple accounts there is some

00:09:01,190 --> 00:09:07,079
double counting here or triple counting

00:09:04,019 --> 00:09:13,019
we're certainly not pushing 16,000

00:09:07,079 --> 00:09:15,779
pieces of of new code there's been a lot

00:09:13,019 --> 00:09:20,310
of growth in accounts certainly on the

00:09:15,779 --> 00:09:22,740
AWS side we've gone from 20 in 2017 to

00:09:20,310 --> 00:09:25,079
upwards of 70 now and if anybody's

00:09:22,740 --> 00:09:27,089
familiar with cloud driver internals you

00:09:25,079 --> 00:09:29,430
might realize that this poses some

00:09:27,089 --> 00:09:32,569
scaling concerns with the way that we do

00:09:29,430 --> 00:09:36,389
indexing threads for every account

00:09:32,569 --> 00:09:38,300
region and cloud resource we're creating

00:09:36,389 --> 00:09:40,829
a new thread that needs to be scheduled

00:09:38,300 --> 00:09:44,519
so in this case we're probably trying to

00:09:40,829 --> 00:09:46,980
schedule 12 to 1500 indexers at any one

00:09:44,519 --> 00:09:49,440
time so it is a concern on how much

00:09:46,980 --> 00:09:51,600
farther we can scale this the current

00:09:49,440 --> 00:09:54,300
approach is to scale horizontally and

00:09:51,600 --> 00:09:56,699
use a dial which lets us adjust how much

00:09:54,300 --> 00:10:00,180
concurrency any single instance or

00:09:56,699 --> 00:10:02,370
handle this is an opportunity that we

00:10:00,180 --> 00:10:04,589
might invest in into the future but also

00:10:02,370 --> 00:10:09,449
an opportunity for the community to get

00:10:04,589 --> 00:10:11,970
involved and certainly spinnaker as an

00:10:09,449 --> 00:10:15,360
orchestration platform does a lot more

00:10:11,970 --> 00:10:18,209
than deploy to the cloud we push client

00:10:15,360 --> 00:10:20,519
JavaScript assets to CDNs across the

00:10:18,209 --> 00:10:22,709
globe we push firmware updates to the

00:10:20,519 --> 00:10:26,600
devices that house the content that is

00:10:22,709 --> 00:10:29,160
streamed to your house or your devices

00:10:26,600 --> 00:10:31,019
we run nightly smoke tests many teams

00:10:29,160 --> 00:10:34,380
from nightly smoke tests we do data

00:10:31,019 --> 00:10:38,250
processing jobs and I break the captive

00:10:34,380 --> 00:10:41,579
category of this work into two groups

00:10:38,250 --> 00:10:44,069
that is that repeatable so you can think

00:10:41,579 --> 00:10:46,649
of them as imperative stages that are

00:10:44,069 --> 00:10:48,709
triggered based on some event and those

00:10:46,649 --> 00:10:51,389
that are ad hoc so things that are

00:10:48,709 --> 00:10:56,279
reactionary based on some external

00:10:51,389 --> 00:11:00,300
stimulus we run more than 40,000 pipe

00:10:56,279 --> 00:11:03,180
lines a day and the vast majority of

00:11:00,300 --> 00:11:04,949
these more than 90% are internally

00:11:03,180 --> 00:11:08,040
triggered so the things that are running

00:11:04,949 --> 00:11:08,620
krons or kicked off on the completion of

00:11:08,040 --> 00:11:12,790
some other

00:11:08,620 --> 00:11:14,590
type 1 event ad hoc is work it's

00:11:12,790 --> 00:11:17,800
interesting there are a number of

00:11:14,590 --> 00:11:20,640
declarative systems at Netflix that are

00:11:17,800 --> 00:11:24,190
orchestrating orchestrating intents

00:11:20,640 --> 00:11:27,490
through spinnaker manage delivery is one

00:11:24,190 --> 00:11:29,140
such thing but we also have custom auto

00:11:27,490 --> 00:11:30,910
scalars that are maintaining their own

00:11:29,140 --> 00:11:33,550
model and when they detect a change

00:11:30,910 --> 00:11:36,250
they're triggering tasks in the

00:11:33,550 --> 00:11:40,210
spinnaker and we run more than a hundred

00:11:36,250 --> 00:11:44,140
thousand of those every day I think we

00:11:40,210 --> 00:11:47,530
spike in terms of concurrency upwards of

00:11:44,140 --> 00:11:52,680
five or six thousand concurrent tasks or

00:11:47,530 --> 00:11:55,180
pipelines at kind of peak during the day

00:11:52,680 --> 00:11:57,760
but last year it felt like we were

00:11:55,180 --> 00:12:00,910
walking a tightrope at any one time we

00:11:57,760 --> 00:12:04,810
could fail we could fall off we didn't

00:12:00,910 --> 00:12:08,740
have that much Headroom and in fact we

00:12:04,810 --> 00:12:11,710
did we failed fairly catastrophic Lee in

00:12:08,740 --> 00:12:17,320
terms of looking at the load on our team

00:12:11,710 --> 00:12:18,640
we were constantly firefighting and as

00:12:17,320 --> 00:12:21,550
the team that was responsible for

00:12:18,640 --> 00:12:25,090
performance and reliability it was very

00:12:21,550 --> 00:12:29,620
stressful and of course we had this fine

00:12:25,090 --> 00:12:33,370
fellow to keep us to keep us on the ball

00:12:29,620 --> 00:12:36,190
and awake during the day at least the

00:12:33,370 --> 00:12:39,400
pages typically do subside overnight as

00:12:36,190 --> 00:12:42,280
usage drops but looking at numbers over

00:12:39,400 --> 00:12:46,270
the last year we've been paged a little

00:12:42,280 --> 00:12:48,910
over 300 times our team the vast

00:12:46,270 --> 00:12:52,660
majority of these were during that a

00:12:48,910 --> 00:12:55,180
period of crises kind of December

00:12:52,660 --> 00:12:57,690
through December through January the

00:12:55,180 --> 00:12:59,500
last six months operations have improved

00:12:57,690 --> 00:13:01,840
Rob will touch on some of the

00:12:59,500 --> 00:13:04,030
investments we've made to drop it we've

00:13:01,840 --> 00:13:06,250
been paid less than 50 times over the

00:13:04,030 --> 00:13:09,640
last six months and we're now on track

00:13:06,250 --> 00:13:12,160
we get a page every week or two our team

00:13:09,640 --> 00:13:13,840
runs a weekly on call and a lot of the

00:13:12,160 --> 00:13:15,700
pages we're seeing now are not

00:13:13,840 --> 00:13:18,430
self-inflicted they're not performance

00:13:15,700 --> 00:13:22,359
related there's something else in the

00:13:18,430 --> 00:13:25,239
system degrading a downstream dependency

00:13:22,359 --> 00:13:27,369
which still manifests itself as

00:13:25,239 --> 00:13:30,069
something behaving abnormally and

00:13:27,369 --> 00:13:33,629
spinnaker so we get paged we triage it

00:13:30,069 --> 00:13:33,629
and we work with our partner teams

00:13:34,169 --> 00:13:38,589
spinnaker is a critical service at

00:13:36,579 --> 00:13:40,779
Netflix certainly one of the most

00:13:38,589 --> 00:13:43,029
critical services in the developer flow

00:13:40,779 --> 00:13:46,239
we couldn't just tell the company hey

00:13:43,029 --> 00:13:48,339
stop sorry we can't handle the load go

00:13:46,239 --> 00:13:51,659
find another service come back in a

00:13:48,339 --> 00:13:56,559
couple of quarters when you fix things

00:13:51,659 --> 00:13:58,449
jokingly we refer to the meantime to a

00:13:56,559 --> 00:14:00,909
user telling us about a problem and

00:13:58,449 --> 00:14:03,509
that's during the workday certainly

00:14:00,909 --> 00:14:05,889
measured in single digit minutes

00:14:03,509 --> 00:14:09,249
oftentimes we'll hear about a problem

00:14:05,889 --> 00:14:11,499
from a user before our alerts even fire

00:14:09,249 --> 00:14:15,189
we've got a very passionate user base

00:14:11,499 --> 00:14:18,009
who has very high expectations they

00:14:15,189 --> 00:14:19,569
expect spinnaker to be a product razzes

00:14:18,009 --> 00:14:21,819
the team responsible for spinnaker we're

00:14:19,569 --> 00:14:24,009
aware of all the pieces that make it

00:14:21,819 --> 00:14:28,329
that make it up and that's where we feel

00:14:24,009 --> 00:14:30,429
we can make improvements but our users

00:14:28,329 --> 00:14:34,739
they want something that's performant

00:14:30,429 --> 00:14:37,749
and reliable the operators in all of us

00:14:34,739 --> 00:14:40,539
we want spinner to be operable we don't

00:14:37,749 --> 00:14:43,119
want to have many different dials that

00:14:40,539 --> 00:14:44,829
we have to tweak in particular orders we

00:14:43,119 --> 00:14:47,319
don't want a very complicated production

00:14:44,829 --> 00:14:49,839
topology that we have to artisanally

00:14:47,319 --> 00:14:51,999
deploy we want something simple and

00:14:49,839 --> 00:14:54,549
that's been a major focus of our

00:14:51,999 --> 00:14:59,259
investments and spinnaker internals over

00:14:54,549 --> 00:15:01,629
the last year so at this point I'm gonna

00:14:59,259 --> 00:15:03,309
hand it over to Rob who will walk us

00:15:01,629 --> 00:15:06,759
through these improvements and what the

00:15:03,309 --> 00:15:10,029
impacts have been hi

00:15:06,759 --> 00:15:12,939
so yeah complexity hurts spinnaker is a

00:15:10,029 --> 00:15:15,009
very large system made up of from an

00:15:12,939 --> 00:15:16,959
open source perspective about 10 micro

00:15:15,009 --> 00:15:19,929
services but internally at Netflix it's

00:15:16,959 --> 00:15:21,819
more like 20 each service has its own

00:15:19,929 --> 00:15:25,599
data store and its own unique operator

00:15:21,819 --> 00:15:27,909
story to keep things to keep up with

00:15:25,599 --> 00:15:29,589
performance demands we tried stretching

00:15:27,909 --> 00:15:33,960
the architecture like Adam mentioned

00:15:29,589 --> 00:15:36,290
with federating out cloud driver into up

00:15:33,960 --> 00:15:41,820
words of 50 instances and I think we had

00:15:36,290 --> 00:15:43,500
12 Redis servers backing it

00:15:41,820 --> 00:15:45,060
and before long that's apology started

00:15:43,500 --> 00:15:48,960
to look a lot like a root started to

00:15:45,060 --> 00:15:51,840
look like a Rube Goldberg machine and

00:15:48,960 --> 00:15:54,420
when you are when you're operating a

00:15:51,840 --> 00:15:58,560
service you can only hold so many things

00:15:54,420 --> 00:16:01,710
and when it was last year right after

00:15:58,560 --> 00:16:04,050
spank or summit we we were trying to

00:16:01,710 --> 00:16:05,520
hold all these different limes and while

00:16:04,050 --> 00:16:07,320
we were trying to hold these limes one

00:16:05,520 --> 00:16:15,690
would drop and if we went to go pick one

00:16:07,320 --> 00:16:20,420
up we would drop more a few war stories

00:16:15,690 --> 00:16:24,150
in kind of our most common issues were

00:16:20,420 --> 00:16:26,160
source than Redis either Redis whoa

00:16:24,150 --> 00:16:27,920
Redis is a single threaded application

00:16:26,160 --> 00:16:32,910
so any operation that's happening is

00:16:27,920 --> 00:16:34,890
going to block so we have to we have to

00:16:32,910 --> 00:16:41,210
either spread that load across many

00:16:34,890 --> 00:16:43,410
services or many servers of Redis or

00:16:41,210 --> 00:16:46,800
yeah actually that's about it

00:16:43,410 --> 00:16:49,170
but Redis has this issue of not being a

00:16:46,800 --> 00:16:52,320
durable datastore so when you start

00:16:49,170 --> 00:16:54,090
expanding out its memory it might go out

00:16:52,320 --> 00:16:56,070
of memory and then it flushes all the

00:16:54,090 --> 00:16:58,680
data that's in it and that could result

00:16:56,070 --> 00:17:01,050
in Oracle losing all of its state and

00:16:58,680 --> 00:17:02,940
that means that pipelines that were

00:17:01,050 --> 00:17:05,730
running are now just magically not there

00:17:02,940 --> 00:17:07,770
and anything that was in the process of

00:17:05,730 --> 00:17:10,500
deploying would just be left in this

00:17:07,770 --> 00:17:12,089
half languishing state and operators

00:17:10,500 --> 00:17:15,620
would have to come in after the fact and

00:17:12,089 --> 00:17:18,270
clean up what spec are left on the floor

00:17:15,620 --> 00:17:21,620
we also had issues of greedy

00:17:18,270 --> 00:17:24,660
applications so Adam was talking about

00:17:21,620 --> 00:17:28,770
API users these auto scalars sometimes

00:17:24,660 --> 00:17:31,500
they would hit us with upwards of a

00:17:28,770 --> 00:17:35,490
thousand requests in a second to say

00:17:31,500 --> 00:17:38,040
please run these thousand pipelines all

00:17:35,490 --> 00:17:40,400
at the same time and that would triple

00:17:38,040 --> 00:17:44,760
the system and then hurt other users of

00:17:40,400 --> 00:17:46,310
spinnaker and often cases when these

00:17:44,760 --> 00:17:48,020
larger

00:17:46,310 --> 00:17:51,740
once we come through that would bog down

00:17:48,020 --> 00:17:56,120
the system and then make the UI slow and

00:17:51,740 --> 00:17:58,970
sometimes totally unresponsive it was it

00:17:56,120 --> 00:18:01,090
was time to tidy up spinnaker delivery

00:17:58,970 --> 00:18:03,650
engineering can't scale linearly with

00:18:01,090 --> 00:18:05,630
with the rest of the Netflix engineering

00:18:03,650 --> 00:18:08,930
organization if we're to be successful

00:18:05,630 --> 00:18:11,540
we have to scale sub linearly and to do

00:18:08,930 --> 00:18:17,120
that simplification is really the only

00:18:11,540 --> 00:18:21,230
way forward and what we decided is that

00:18:17,120 --> 00:18:23,420
the future is primarily sequel for

00:18:21,230 --> 00:18:28,610
anyone that's been operating spinnaker

00:18:23,420 --> 00:18:30,200
in really any capacity Redis is is hard

00:18:28,610 --> 00:18:33,470
it's it's very hard if you want to have

00:18:30,200 --> 00:18:36,650
a a service that is stable that is

00:18:33,470 --> 00:18:39,520
reliable having it all built on top of a

00:18:36,650 --> 00:18:42,850
datastore that is largely ephemeral

00:18:39,520 --> 00:18:47,600
presents some interesting operational

00:18:42,850 --> 00:18:49,580
complexities we decide to choose sequel

00:18:47,600 --> 00:18:51,680
rather than any other datastore like

00:18:49,580 --> 00:18:54,320
Cassandra even though that's a more

00:18:51,680 --> 00:18:56,180
paved path thing because it's it's more

00:18:54,320 --> 00:19:00,020
efficient it's more familiar for the

00:18:56,180 --> 00:19:04,340
entire community and in adopting

00:19:00,020 --> 00:19:06,500
spinnaker we were able to reduce the

00:19:04,340 --> 00:19:08,680
footprint of our individual services by

00:19:06,500 --> 00:19:11,990
quite a lot because we were able to make

00:19:08,680 --> 00:19:16,970
individual queries more efficient and so

00:19:11,990 --> 00:19:19,040
we'll be moving in 2020 towards getting

00:19:16,970 --> 00:19:22,490
all the different services to use sequel

00:19:19,040 --> 00:19:25,540
as its back-end store we've already done

00:19:22,490 --> 00:19:29,840
that for orca cloud driver front 50 and

00:19:25,540 --> 00:19:31,370
echo partially so we have a handful of

00:19:29,840 --> 00:19:33,740
other ones that we still need to to

00:19:31,370 --> 00:19:35,020
migrate over and the intent is to

00:19:33,740 --> 00:19:38,870
deprecate

00:19:35,020 --> 00:19:41,350
and eventually remove Redis and the blob

00:19:38,870 --> 00:19:45,700
store implementations as they provide an

00:19:41,350 --> 00:19:45,700
inferior operation experience

00:19:46,540 --> 00:19:51,890
so tidying up our services is one thing

00:19:49,160 --> 00:19:53,660
but as Adam mentioned we've seen

00:19:51,890 --> 00:19:55,940
tremendous growth in spinnaker and it

00:19:53,660 --> 00:19:58,150
isn't slowing down if anything the usage

00:19:55,940 --> 00:20:00,440
will continue to hockey stick and

00:19:58,150 --> 00:20:02,480
especially with managed delivery will

00:20:00,440 --> 00:20:05,600
enable Netflix to turn down as last

00:20:02,480 --> 00:20:07,580
major competing deployment tool meaning

00:20:05,600 --> 00:20:12,020
that we will hockey stick even more in

00:20:07,580 --> 00:20:13,910
usage with that we've needed to make

00:20:12,020 --> 00:20:19,240
some large strides forward in in

00:20:13,910 --> 00:20:23,330
performance and with that sequel again

00:20:19,240 --> 00:20:25,310
Redis is very fast query to command to

00:20:23,330 --> 00:20:29,030
command it's extremely fast you can't

00:20:25,310 --> 00:20:31,310
really beat it but it's not very

00:20:29,030 --> 00:20:33,980
efficient most of the data inside of

00:20:31,310 --> 00:20:36,800
spinnaker is relational and trying to

00:20:33,980 --> 00:20:41,120
model relational data inside of a key

00:20:36,800 --> 00:20:44,360
value system is difficult and is

00:20:41,120 --> 00:20:47,030
actually very inefficient there's been

00:20:44,360 --> 00:20:49,430
common since last year that the way that

00:20:47,030 --> 00:20:52,900
we implemented sequel inside of Orca was

00:20:49,430 --> 00:20:55,040
kind of weird or maybe wrong but we

00:20:52,900 --> 00:20:57,020
implemented in such a way that we knew

00:20:55,040 --> 00:20:58,940
would scale to the performance numbers

00:20:57,020 --> 00:21:02,320
that we need while still providing all

00:20:58,940 --> 00:21:06,770
the durability and correctness that

00:21:02,320 --> 00:21:10,760
sequel provides for everyone I have some

00:21:06,770 --> 00:21:11,750
numbers right here so Redis for example

00:21:10,760 --> 00:21:16,910
inside of cloud driver

00:21:11,750 --> 00:21:18,710
we saw a 3.3 queried key efficiency for

00:21:16,910 --> 00:21:21,680
retrieving cache data so let's say

00:21:18,710 --> 00:21:26,210
you're looking at a server group inside

00:21:21,680 --> 00:21:28,970
of the inside of the plow driver API we

00:21:26,210 --> 00:21:31,850
would query a whole bunch of information

00:21:28,970 --> 00:21:35,360
inside of Redis and then only use 3.3

00:21:31,850 --> 00:21:40,070
percent of those keys and then that

00:21:35,360 --> 00:21:41,420
actually converted to since it's a key

00:21:40,070 --> 00:21:43,610
value system we have to kind of scan

00:21:41,420 --> 00:21:46,250
over all these possible keys can that

00:21:43,610 --> 00:21:49,970
translate to about a 4% efficiency of

00:21:46,250 --> 00:21:51,710
key hit rate so that's a lot of effort

00:21:49,970 --> 00:21:52,820
that we're going through for not a whole

00:21:51,710 --> 00:21:57,650
lot of information that we're getting

00:21:52,820 --> 00:21:59,210
back while sequel is slower query for

00:21:57,650 --> 00:22:01,130
query it's far more efficient

00:21:59,210 --> 00:22:05,720
and then in aggregate is much more

00:22:01,130 --> 00:22:08,690
performant we also have done some more

00:22:05,720 --> 00:22:11,059
tactical work around force cache refresh

00:22:08,690 --> 00:22:13,789
if you're familiar with this it's this

00:22:11,059 --> 00:22:16,100
kind of nebulous task that's happening

00:22:13,789 --> 00:22:18,590
inside of random stages that's that

00:22:16,100 --> 00:22:21,890
takes like seven minutes or whatever and

00:22:18,590 --> 00:22:24,230
we had we had theorized that maybe some

00:22:21,890 --> 00:22:28,070
of these are not actually necessary so

00:22:24,230 --> 00:22:31,070
we have disabled force cache refresh

00:22:28,070 --> 00:22:34,120
successfully inside of Netflix for all

00:22:31,070 --> 00:22:36,620
of these different stages and

00:22:34,120 --> 00:22:40,940
subsequently have increased performance

00:22:36,620 --> 00:22:43,490
of these stages quite a lot we want to

00:22:40,940 --> 00:22:45,169
make these make this the default but

00:22:43,490 --> 00:22:48,169
it's kind of a scary change to put on

00:22:45,169 --> 00:22:48,850
users because if it isn't actually

00:22:48,169 --> 00:22:51,020
correct

00:22:48,850 --> 00:22:54,230
everyone's operations experience is

00:22:51,020 --> 00:22:56,390
going to plummet so right now they're

00:22:54,230 --> 00:22:59,960
they're done through configuration so if

00:22:56,390 --> 00:23:01,700
you want to experiment with this we'll

00:22:59,960 --> 00:23:05,120
be adding some documentation that you

00:23:01,700 --> 00:23:11,659
guys can play around and give us some

00:23:05,120 --> 00:23:15,130
feedback going fast as well and good but

00:23:11,659 --> 00:23:17,450
it's often not useful if things fail

00:23:15,130 --> 00:23:20,120
failure happens even when you don't plan

00:23:17,450 --> 00:23:22,429
for it or rather it failure happens even

00:23:20,120 --> 00:23:24,500
when you do plan for it distributed

00:23:22,429 --> 00:23:26,750
systems are phenomenally hard and it's

00:23:24,500 --> 00:23:28,100
even harder when you're orchestrating

00:23:26,750 --> 00:23:31,520
distributed systems that you don't even

00:23:28,100 --> 00:23:33,649
own at Netflix as we've continued to

00:23:31,520 --> 00:23:37,130
scale the tolerance for failures from

00:23:33,649 --> 00:23:41,000
our customers that is other Netflix

00:23:37,130 --> 00:23:43,010
engineers has dropped as it becomes more

00:23:41,000 --> 00:23:45,080
adopted people are less tolerant of

00:23:43,010 --> 00:23:50,090
issues coming from spinnaker because

00:23:45,080 --> 00:23:53,750
it's such a critical system so we're

00:23:50,090 --> 00:23:56,600
actively working on refactoring the

00:23:53,750 --> 00:23:58,460
architecture between Orca the

00:23:56,600 --> 00:24:00,890
orchestration Engine and cloud reiver

00:23:58,460 --> 00:24:02,330
which is the service responsible for

00:24:00,890 --> 00:24:04,909
interacting with all the cloud providers

00:24:02,330 --> 00:24:08,960
and to make that interaction more item

00:24:04,909 --> 00:24:10,820
potent we've integrated a new saga

00:24:08,960 --> 00:24:11,789
pattern which if you're familiar or

00:24:10,820 --> 00:24:14,600
rather if you're not

00:24:11,789 --> 00:24:17,309
Miller is a design pattern for doing

00:24:14,600 --> 00:24:21,359
transactionality across a distributed

00:24:17,309 --> 00:24:24,059
system and so we're able to start a

00:24:21,359 --> 00:24:26,100
stage let's say a deploy operation and

00:24:24,059 --> 00:24:27,899
regardless of transient errors

00:24:26,100 --> 00:24:31,409
underneath like let's say inside of

00:24:27,899 --> 00:24:34,769
Titus or inside of AWS cloud Ivor and

00:24:31,409 --> 00:24:37,619
Orca will no longer just fail because

00:24:34,769 --> 00:24:39,239
there is a throttle rate limit or

00:24:37,619 --> 00:24:42,090
anything like that if it's a transient

00:24:39,239 --> 00:24:45,389
error even a long-term failure of 30

00:24:42,090 --> 00:24:47,369
minutes claw driver can or spinnaker can

00:24:45,389 --> 00:24:48,899
recover automatically without affecting

00:24:47,369 --> 00:24:52,289
the end user

00:24:48,899 --> 00:24:55,109
right now this is Titus only and we'll

00:24:52,289 --> 00:24:58,259
be looking to add this functionality in

00:24:55,109 --> 00:25:04,440
the ec2 and other cloud providers next

00:24:58,259 --> 00:25:07,289
year this was just released in in q4

00:25:04,440 --> 00:25:12,899
here so it's it's still pretty new but

00:25:07,289 --> 00:25:16,679
we've already seen it successfully avoid

00:25:12,899 --> 00:25:19,739
issues in two or three hundred pipelines

00:25:16,679 --> 00:25:22,590
at this point which is quite a lot even

00:25:19,739 --> 00:25:24,629
if it's even if we're doing thousands of

00:25:22,590 --> 00:25:26,399
deployments a day that's two or three

00:25:24,629 --> 00:25:28,889
hundred times where a customer doesn't

00:25:26,399 --> 00:25:30,840
have to come to us and say hey this

00:25:28,889 --> 00:25:36,659
pipeline failed you left us in this

00:25:30,840 --> 00:25:38,879
weird States why and this feature is

00:25:36,659 --> 00:25:41,489
also only available for us the sequel

00:25:38,879 --> 00:25:45,059
persistence back-end so that will just

00:25:41,489 --> 00:25:47,249
be another encouragement for people to

00:25:45,059 --> 00:25:50,609
get on to sequel of course it could be

00:25:47,249 --> 00:25:53,119
implemented for Redis it's just it

00:25:50,609 --> 00:25:53,119
hasn't been

00:25:53,850 --> 00:26:00,720
Netflix is widely known for its mantra

00:25:59,070 --> 00:26:04,649
of freedom and responsibility and

00:26:00,720 --> 00:26:07,679
spinnaker as it exists today kind of

00:26:04,649 --> 00:26:09,239
reflects that before before Fiat was

00:26:07,679 --> 00:26:11,460
introduced the authorization service

00:26:09,239 --> 00:26:13,229
people could go in and do anything they

00:26:11,460 --> 00:26:15,929
wanted it doesn't matter if their

00:26:13,229 --> 00:26:17,999
application was owned by them some other

00:26:15,929 --> 00:26:22,049
some other engineer could come in and

00:26:17,999 --> 00:26:23,700
delete a cluster and this was true for

00:26:22,049 --> 00:26:25,559
Netflix with our thousands of

00:26:23,700 --> 00:26:28,379
applications and thousands of Engineers

00:26:25,559 --> 00:26:31,229
anyone can go into anywhere let's say

00:26:28,379 --> 00:26:33,210
the API cluster or the API application

00:26:31,229 --> 00:26:34,710
and delete everything that they have the

00:26:33,210 --> 00:26:39,840
freedom to do that it's not very

00:26:34,710 --> 00:26:42,570
responsible but they could do it and we

00:26:39,840 --> 00:26:45,419
really wanted to reduce our operational

00:26:42,570 --> 00:26:47,940
overhead we have one spinnaker to manage

00:26:45,419 --> 00:26:51,419
all infrastructure at Netflix but then

00:26:47,940 --> 00:26:53,159
this one small little spinnaker

00:26:51,419 --> 00:26:57,389
installation for managing our PCI

00:26:53,159 --> 00:26:58,879
compliant account we wanted to merge

00:26:57,389 --> 00:27:03,330
that so that there would be a single

00:26:58,879 --> 00:27:05,599
spinnaker to rule them all so we adopted

00:27:03,330 --> 00:27:09,419
Fiat in the beginning of the year and

00:27:05,599 --> 00:27:12,570
through this we have improved

00:27:09,419 --> 00:27:14,429
performance really resilience of Fiat

00:27:12,570 --> 00:27:17,809
itself as well as spinnaker in general

00:27:14,429 --> 00:27:23,789
if Fiat works you for some reason

00:27:17,809 --> 00:27:26,840
disappear or it's Redis or to die it

00:27:23,789 --> 00:27:29,129
won't kill all of spinnaker which

00:27:26,840 --> 00:27:31,229
initially when we were adopting it was

00:27:29,129 --> 00:27:34,379
something that we hadn't seen and

00:27:31,229 --> 00:27:35,899
through security audits that we've gone

00:27:34,379 --> 00:27:39,359
through we have now consolidated

00:27:35,899 --> 00:27:41,820
everything into one single spinnaker

00:27:39,359 --> 00:27:44,399
installation so even our PCI compliant

00:27:41,820 --> 00:27:48,389
accounts are all managed to a single

00:27:44,399 --> 00:27:50,009
spinnaker and people are able to secure

00:27:48,389 --> 00:27:51,989
off certain applications that are only

00:27:50,009 --> 00:27:53,399
allowed for people that have gone

00:27:51,989 --> 00:27:55,519
through background checks on that kind

00:27:53,399 --> 00:27:55,519
of thing

00:27:55,680 --> 00:28:03,000
and as Adam mentioned again we have a

00:27:58,980 --> 00:28:06,810
steady growth of machine traffic api's

00:28:03,000 --> 00:28:09,120
and spinnaker are polymorphic they're

00:28:06,810 --> 00:28:13,050
dynamically typed and as a results

00:28:09,120 --> 00:28:15,030
relatively difficult to discover for

00:28:13,050 --> 00:28:17,910
anyone that's tried to write an API

00:28:15,030 --> 00:28:21,690
client with spinnaker the usual workflow

00:28:17,910 --> 00:28:24,330
today is load up the UI run a pipeline

00:28:21,690 --> 00:28:25,830
with the network tab open and then

00:28:24,330 --> 00:28:28,950
inspect whatever was sent across the

00:28:25,830 --> 00:28:32,670
wire and then copy paste that into your

00:28:28,950 --> 00:28:37,320
code and rinse repeat until you get

00:28:32,670 --> 00:28:40,670
something that works we are looking to

00:28:37,320 --> 00:28:44,010
switch to G RPC for inter-service RPC

00:28:40,670 --> 00:28:45,930
and as a result that will mean that

00:28:44,010 --> 00:28:47,970
we'll have to have stronger typing for

00:28:45,930 --> 00:28:52,470
the actual individual services of our

00:28:47,970 --> 00:28:55,800
services and as as we continue to build

00:28:52,470 --> 00:28:58,380
out the inter-service api's internally

00:28:55,800 --> 00:29:02,100
that will bubble out to a public API

00:28:58,380 --> 00:29:04,710
that's strongly typed discoverable and

00:29:02,100 --> 00:29:09,440
potentially sprinkled with some event

00:29:04,710 --> 00:29:11,070
streaming which is a very common request

00:29:09,440 --> 00:29:15,450
let's see

00:29:11,070 --> 00:29:18,600
and then extension there was already a

00:29:15,450 --> 00:29:22,110
talk on spinnaker plugins earlier by the

00:29:18,600 --> 00:29:23,610
armory folks today in in Netflix we use

00:29:22,110 --> 00:29:26,190
open source spinnaker but we layer

00:29:23,610 --> 00:29:28,860
things on like a cake and like a cake if

00:29:26,190 --> 00:29:30,450
you want to add a new layer you have to

00:29:28,860 --> 00:29:32,580
do that at bake time you can't just add

00:29:30,450 --> 00:29:36,110
another layer on to a baked cake and

00:29:32,580 --> 00:29:36,110
expect it to work so

00:29:36,750 --> 00:29:43,600
unfortunately that kind of that kind of

00:29:39,340 --> 00:29:48,700
development process requires some

00:29:43,600 --> 00:29:51,100
aptitude of the JVM gradle and just

00:29:48,700 --> 00:29:54,879
building this entire service it's a very

00:29:51,100 --> 00:29:56,799
complex system so adding all having to

00:29:54,879 --> 00:29:59,289
know like spring boots and then all the

00:29:56,799 --> 00:30:01,870
internals of spinnaker that's that's

00:29:59,289 --> 00:30:04,750
quite the mounting to come back to climb

00:30:01,870 --> 00:30:06,340
if you're not working on spinnaker stain

00:30:04,750 --> 00:30:09,240
and day out even if you are working on

00:30:06,340 --> 00:30:13,269
spanker day in and day out that can be

00:30:09,240 --> 00:30:16,419
arduous so we're working on a plug-in

00:30:13,269 --> 00:30:19,929
architecture and that means that custom

00:30:16,419 --> 00:30:25,919
builds are not necessarily needed we are

00:30:19,929 --> 00:30:29,230
working on the capability of adding a

00:30:25,919 --> 00:30:31,360
plug-in jar that sits aside like next to

00:30:29,230 --> 00:30:34,269
the spinnaker service that gets loaded

00:30:31,360 --> 00:30:36,700
at boot time configured and then siloed

00:30:34,269 --> 00:30:39,039
off into its own class loader if you're

00:30:36,700 --> 00:30:43,929
unfamiliar with class loaders it's the

00:30:39,039 --> 00:30:46,330
way the JVM finds code and the value of

00:30:43,929 --> 00:30:48,600
having the plug-in code separate into a

00:30:46,330 --> 00:30:50,830
separate class odors that prevents

00:30:48,600 --> 00:30:52,990
dependency conflicts you can add your

00:30:50,830 --> 00:30:54,610
own dependencies without breaking the

00:30:52,990 --> 00:30:57,659
actual core service and that kind of

00:30:54,610 --> 00:31:01,179
thing plugins will also support

00:30:57,659 --> 00:31:03,039
alternative runtimes as well so remote

00:31:01,179 --> 00:31:06,580
plugins you could kind of think of this

00:31:03,039 --> 00:31:11,259
as the webhook stage or pre-configured

00:31:06,580 --> 00:31:12,610
web hooks today just totally separate

00:31:11,259 --> 00:31:16,059
processes that are running somewhere

00:31:12,610 --> 00:31:19,059
else but that adhere to a strict and

00:31:16,059 --> 00:31:22,480
stable interface inside of spinnaker so

00:31:19,059 --> 00:31:27,129
you could say I want to create a cloud

00:31:22,480 --> 00:31:29,460
provider while that cloud provider hmm

00:31:27,129 --> 00:31:31,809
let's say let's say ECS for example

00:31:29,460 --> 00:31:35,230
Amazon wants to create an ECS cloud

00:31:31,809 --> 00:31:36,789
provider well today people have to make

00:31:35,230 --> 00:31:40,889
a bunch of pull requests into cloud

00:31:36,789 --> 00:31:43,600
driver Orca gate maybe well maybe gate

00:31:40,889 --> 00:31:45,940
DAC all these different services get it

00:31:43,600 --> 00:31:47,600
approved by people inside of the core

00:31:45,940 --> 00:31:51,710
contributors

00:31:47,600 --> 00:31:54,110
and then continue to iterate like with

00:31:51,710 --> 00:31:56,960
everyone having to accept the risk of

00:31:54,110 --> 00:31:59,810
some nascent code getting into spinnaker

00:31:56,960 --> 00:32:01,190
with plugins they'll be able to develop

00:31:59,810 --> 00:32:03,470
that all out-of-band

00:32:01,190 --> 00:32:05,150
make it stable and then there could be

00:32:03,470 --> 00:32:07,460
the potential of bringing it into core

00:32:05,150 --> 00:32:11,950
or maybe it just stays out in a plug-in

00:32:07,460 --> 00:32:15,160
forever and spinnaker itself can just

00:32:11,950 --> 00:32:17,780
stay stable with the ecosystem growing

00:32:15,160 --> 00:32:25,070
exponentially out without needing to

00:32:17,780 --> 00:32:28,010
know the JVM all right so all these

00:32:25,070 --> 00:32:31,760
internal changes are well and good but

00:32:28,010 --> 00:32:35,680
what if disaster strikes what if this

00:32:31,760 --> 00:32:39,650
creepy girl sets your house on fire

00:32:35,680 --> 00:32:41,930
we're working on multi-region in inside

00:32:39,650 --> 00:32:45,110
of Netflix we already have two data

00:32:41,930 --> 00:32:46,310
centers or for our production spinnaker

00:32:45,110 --> 00:32:47,090
installation they are running an

00:32:46,310 --> 00:32:51,190
active-active

00:32:47,090 --> 00:32:53,870
so we have a spinnaker deployment in

00:32:51,190 --> 00:32:55,790
u.s. West 2 and a spinnaker deployment

00:32:53,870 --> 00:32:58,160
in u.s. East 1 they can both receive

00:32:55,790 --> 00:33:02,120
production traffic although one is only

00:32:58,160 --> 00:33:04,870
responsible for processing triggers and

00:33:02,120 --> 00:33:07,940
that kind of thing but this is a

00:33:04,870 --> 00:33:10,400
relatively new development for us inside

00:33:07,940 --> 00:33:13,220
of Netflix if if your service becomes

00:33:10,400 --> 00:33:16,280
unavailable because of a regional outage

00:33:13,220 --> 00:33:18,140
that's a pretty bad look most services

00:33:16,280 --> 00:33:21,170
inside the inside of Netflix are

00:33:18,140 --> 00:33:22,700
designed to be cross region and active

00:33:21,170 --> 00:33:26,540
active across all those and the fact

00:33:22,700 --> 00:33:30,400
that spinnaker wasn't active active

00:33:26,540 --> 00:33:34,970
across regions was causing some

00:33:30,400 --> 00:33:38,900
heartburn for us like when will us West

00:33:34,970 --> 00:33:40,910
to die and then Netflix loses the

00:33:38,900 --> 00:33:44,230
capability of operating the production

00:33:40,910 --> 00:33:46,850
services that's that's a big deal so

00:33:44,230 --> 00:33:48,440
having these two regions now has

00:33:46,850 --> 00:33:51,500
actually already saved us a couple of

00:33:48,440 --> 00:33:53,600
times now it's been it's been very

00:33:51,500 --> 00:33:56,270
valuable right now we are just using

00:33:53,600 --> 00:33:58,570
traditional sequel replication and

00:33:56,270 --> 00:34:01,070
they're totally isolated so if a

00:33:58,570 --> 00:34:01,460
pipeline execution goes into datacenter

00:34:01,070 --> 00:34:05,180
one

00:34:01,460 --> 00:34:07,070
it'll stay there forever and we'll

00:34:05,180 --> 00:34:08,780
replicate the data to at the other

00:34:07,070 --> 00:34:11,320
region so that there's still visibility

00:34:08,780 --> 00:34:13,310
if you were to land in that other region

00:34:11,320 --> 00:34:16,010
but we're starting to experiment with

00:34:13,310 --> 00:34:18,170
cockroach DB for certain specific use

00:34:16,010 --> 00:34:22,930
cases such as front 50s durable storage

00:34:18,170 --> 00:34:22,930
where Layton sees are less of a problem

00:34:24,790 --> 00:34:28,910
if there's a problem inside the

00:34:26,990 --> 00:34:31,790
spinnaker chances are high that we've

00:34:28,910 --> 00:34:34,160
already seen them and cut our teeth on

00:34:31,790 --> 00:34:37,400
its operating spinnaker is not easy

00:34:34,160 --> 00:34:43,370
there is no easy button but we do have

00:34:37,400 --> 00:34:45,020
some recommendations spinnaker is a

00:34:43,370 --> 00:34:46,460
critical service if you are running it

00:34:45,020 --> 00:34:48,020
you need to treat it like a critical

00:34:46,460 --> 00:34:49,850
service and a critical service will

00:34:48,020 --> 00:34:52,070
always have monitoring it will always

00:34:49,850 --> 00:34:56,120
have dashboards it'll have alerting and

00:34:52,070 --> 00:35:00,440
centralized logging it kind of seems

00:34:56,120 --> 00:35:04,430
like this is a no-brainer but it's very

00:35:00,440 --> 00:35:07,040
common when when people are asking

00:35:04,430 --> 00:35:09,800
inside the spinnaker open source slack

00:35:07,040 --> 00:35:13,490
that people don't have these things set

00:35:09,800 --> 00:35:15,830
up and having these set up will make it

00:35:13,490 --> 00:35:18,230
much much more easy for us to help you

00:35:15,830 --> 00:35:20,660
in whatever problem you're having inside

00:35:18,230 --> 00:35:23,390
of your spinnaker insulation and also

00:35:20,660 --> 00:35:25,370
this help yourselves there's a plethora

00:35:23,390 --> 00:35:28,340
of errors that could possibly happen

00:35:25,370 --> 00:35:34,280
inside of spinnaker and being able to

00:35:28,340 --> 00:35:36,680
centrally log everything and see all all

00:35:34,280 --> 00:35:38,450
log messages for a particular request or

00:35:36,680 --> 00:35:40,310
a particular pipe like execution is

00:35:38,450 --> 00:35:46,540
immensely helpful when debugging a

00:35:40,310 --> 00:35:49,760
customer support issue great easy ones

00:35:46,540 --> 00:35:52,760
migrate to sequel like I said earlier

00:35:49,760 --> 00:35:54,920
it's available for Orca Cloud Ivor echo

00:35:52,760 --> 00:35:57,320
front 50 and other services will follow

00:35:54,920 --> 00:36:00,530
all these migrations do not require

00:35:57,320 --> 00:36:02,420
downtime as I have mentioned Netflix is

00:36:00,530 --> 00:36:04,610
not going to wait for us to stop we have

00:36:02,420 --> 00:36:07,610
to do everything all of our changes have

00:36:04,610 --> 00:36:08,960
to be done live and there's migrators

00:36:07,610 --> 00:36:13,250
for everything and there's documentation

00:36:08,960 --> 00:36:14,830
for all these services one key thing to

00:36:13,250 --> 00:36:19,000
take away from this is that

00:36:14,830 --> 00:36:22,090
eko is now its scheduler is now highly

00:36:19,000 --> 00:36:23,920
available which is a change from the

00:36:22,090 --> 00:36:26,530
Redis era where you could only have a

00:36:23,920 --> 00:36:28,750
single echo instance running for doing

00:36:26,530 --> 00:36:30,520
scheduling now you can have as many echo

00:36:28,750 --> 00:36:36,340
schedulers as you want it'll be

00:36:30,520 --> 00:36:39,310
perfectly fine investments first thing

00:36:36,340 --> 00:36:41,290
is spinnaker as a platform in 2020 we're

00:36:39,310 --> 00:36:43,270
going to be focusing immensely on this

00:36:41,290 --> 00:36:45,610
concept of spinnaker as a platform and

00:36:43,270 --> 00:36:48,580
that is adding extension points that

00:36:45,610 --> 00:36:51,100
people can create plugins which enables

00:36:48,580 --> 00:36:54,820
federated inter sourcing for that it's

00:36:51,100 --> 00:36:57,640
for us it's we as a delivery engineering

00:36:54,820 --> 00:36:59,740
organization are a central service that

00:36:57,640 --> 00:37:03,160
people want to integrate with but we

00:36:59,740 --> 00:37:04,780
don't want to do the work for them which

00:37:03,160 --> 00:37:07,060
has historically been the case because

00:37:04,780 --> 00:37:09,040
of how it's a layered cake development

00:37:07,060 --> 00:37:10,930
process and we also want to enable

00:37:09,040 --> 00:37:13,480
extended experiences people to build

00:37:10,930 --> 00:37:14,830
higher level abstractions on top of

00:37:13,480 --> 00:37:18,250
spinnaker and the primitives that we

00:37:14,830 --> 00:37:20,920
offer we also want to simplify the

00:37:18,250 --> 00:37:23,020
developer experience not only just for

00:37:20,920 --> 00:37:25,060
us but also for the community we've

00:37:23,020 --> 00:37:26,380
already added some things in terms of

00:37:25,060 --> 00:37:28,210
coding standards but we want to go

00:37:26,380 --> 00:37:29,920
deeper we want to have more static code

00:37:28,210 --> 00:37:32,380
analysis we want to have better testing

00:37:29,920 --> 00:37:35,290
tools we have want to reduce and

00:37:32,380 --> 00:37:37,960
consolidate dependencies Spinnaker's

00:37:35,290 --> 00:37:40,240
built on spring boot so we want to get

00:37:37,960 --> 00:37:41,890
rid of dependencies that kind of

00:37:40,240 --> 00:37:43,510
duplicate functionality that spring boot

00:37:41,890 --> 00:37:44,980
already provides we want to simplify

00:37:43,510 --> 00:37:46,450
things as much as possible so that's

00:37:44,980 --> 00:37:50,320
easier for people to get in and get

00:37:46,450 --> 00:37:52,930
started and then manage delivery is

00:37:50,320 --> 00:37:55,210
another major major part of the future

00:37:52,930 --> 00:37:56,830
of spinnaker and that is through

00:37:55,210 --> 00:37:59,020
declarative infrastructure declarative

00:37:56,830 --> 00:38:00,790
delivery and managed delivery I'm not

00:37:59,020 --> 00:38:02,950
going to go over this because there was

00:38:00,790 --> 00:38:05,850
a talk earlier and I'm also running

00:38:02,950 --> 00:38:11,350
short on time but check it out

00:38:05,850 --> 00:38:12,670
so spinnaker at Netflix in 2020 we're

00:38:11,350 --> 00:38:14,580
going to be focusing on spinnaker as a

00:38:12,670 --> 00:38:17,110
platform we're gonna continue

00:38:14,580 --> 00:38:19,420
consolidating around sequel we're going

00:38:17,110 --> 00:38:22,720
to continue making operational

00:38:19,420 --> 00:38:25,900
improvements and that is going to most

00:38:22,720 --> 00:38:27,700
prominently be seen through Auto

00:38:25,900 --> 00:38:28,630
generation of configuration

00:38:27,700 --> 00:38:30,910
documentation

00:38:28,630 --> 00:38:32,860
as well as metrics so people will be

00:38:30,910 --> 00:38:35,670
able to actually know what all these

00:38:32,860 --> 00:38:39,250
flags that our secret sauce for Netflix

00:38:35,670 --> 00:38:41,410
without having to sleuth through code

00:38:39,250 --> 00:38:44,730
and then we're also focusing on a

00:38:41,410 --> 00:38:48,370
concept of secure by default where

00:38:44,730 --> 00:38:50,500
authorization models enabled by Fiat art

00:38:48,370 --> 00:38:54,900
just enabled by default rather than

00:38:50,500 --> 00:38:57,240
being enough in thing it's reiterate

00:38:54,900 --> 00:39:00,220
incentives and motivators for this is

00:38:57,240 --> 00:39:02,950
operational simplicity the less time

00:39:00,220 --> 00:39:05,680
that we spend fighting fires and less

00:39:02,950 --> 00:39:08,050
time we spend just maintain the status

00:39:05,680 --> 00:39:09,700
quo means more time that we can be

00:39:08,050 --> 00:39:12,340
developing features and delivering value

00:39:09,700 --> 00:39:16,500
to our customers inside of Netflix and

00:39:12,340 --> 00:39:20,740
that goes the same for all of you and

00:39:16,500 --> 00:39:24,490
also organizational stickiness the more

00:39:20,740 --> 00:39:26,050
you can customize spinnaker to meet or

00:39:24,490 --> 00:39:29,070
to integrate with different systems

00:39:26,050 --> 00:39:31,900
inside of your organization the more

00:39:29,070 --> 00:39:35,620
valuable it will become spinnaker out of

00:39:31,900 --> 00:39:37,960
the box is kind of a freemium sort of

00:39:35,620 --> 00:39:40,210
thing it it does a lot of cool stuff but

00:39:37,960 --> 00:39:42,250
it isn't anywhere close to the power

00:39:40,210 --> 00:39:43,960
that you can get if you tightly

00:39:42,250 --> 00:39:46,780
integrate it with other systems inside

00:39:43,960 --> 00:39:48,880
of your organization and through all

00:39:46,780 --> 00:39:51,640
this we're hoping that we'll get more

00:39:48,880 --> 00:39:55,600
increased OSS contribution for us

00:39:51,640 --> 00:39:58,710
selfishly we have open source spinnaker

00:39:55,600 --> 00:40:01,210
because it's a it's filling a gap that

00:39:58,710 --> 00:40:03,780
really didn't exist inside of the open

00:40:01,210 --> 00:40:07,030
source community and it's a bet that

00:40:03,780 --> 00:40:09,310
through your contributions will be able

00:40:07,030 --> 00:40:12,340
to get innovation that we never would

00:40:09,310 --> 00:40:15,250
have even dreamt of and similarly for

00:40:12,340 --> 00:40:17,710
all of you more OSS contributions means

00:40:15,250 --> 00:40:19,720
the same thing you'll be able to get

00:40:17,710 --> 00:40:23,800
innovation that you wouldn't have been

00:40:19,720 --> 00:40:27,340
able to get by doing it yourself we

00:40:23,800 --> 00:40:29,050
should keep in touch a special interest

00:40:27,340 --> 00:40:31,300
group participation if you're interested

00:40:29,050 --> 00:40:33,340
in literally anything that was mentioned

00:40:31,300 --> 00:40:35,890
in here there's a sig for it especially

00:40:33,340 --> 00:40:40,240
the internals bit there's a new

00:40:35,890 --> 00:40:42,190
platform cig which covers all kinds of

00:40:40,240 --> 00:40:45,150
things there's a new cig that was just

00:40:42,190 --> 00:40:49,120
proposed I think today around operations

00:40:45,150 --> 00:40:51,700
there's a cig for UI UX manage delivery

00:40:49,120 --> 00:40:53,200
security all kinds of things the

00:40:51,700 --> 00:40:56,350
technical oversight committee has also

00:40:53,200 --> 00:40:58,210
started a new experiment around an open

00:40:56,350 --> 00:40:59,710
forum where people can just come and

00:40:58,210 --> 00:41:02,680
talk to us

00:40:59,710 --> 00:41:06,040
voice concerns ask questions it's

00:41:02,680 --> 00:41:08,200
totally open any question is fair game

00:41:06,040 --> 00:41:10,780
and then meetups and conferences like

00:41:08,200 --> 00:41:47,880
this and then next year spinnaker summit

00:41:10,780 --> 00:41:47,880
2020 thank you the PR process

00:41:52,290 --> 00:41:58,180
mm okay so that's that's pretty complex

00:41:56,170 --> 00:42:01,570
because each individual service is going

00:41:58,180 --> 00:42:04,600
to have its own meantimes recovery as

00:42:01,570 --> 00:42:07,570
well as its own disaster recovery story

00:42:04,600 --> 00:42:10,000
and hopefully through consolidating

00:42:07,570 --> 00:42:11,290
around sequel and adding more admin

00:42:10,000 --> 00:42:14,860
endpoints and that kind of thing we can

00:42:11,290 --> 00:42:18,040
kind of simplify and make that process a

00:42:14,860 --> 00:42:20,980
little more homogenous since we are in a

00:42:18,040 --> 00:42:24,880
more active active capacity now in terms

00:42:20,980 --> 00:42:26,830
of regions mean time to recovery would

00:42:24,880 --> 00:42:29,530
really be evacuate another region which

00:42:26,830 --> 00:42:31,390
is a DNS change so however long it takes

00:42:29,530 --> 00:42:32,860
for the dns to propagate so that people

00:42:31,390 --> 00:42:36,190
are no longer getting the old service

00:42:32,860 --> 00:42:40,000
and then enabling like Igor polling for

00:42:36,190 --> 00:42:41,770
example and pub/sub processing inside

00:42:40,000 --> 00:42:47,320
the other region and that's a

00:42:41,770 --> 00:42:48,760
configuration change which is instant so

00:42:47,320 --> 00:42:51,820
that's that's kind of where we're at

00:42:48,760 --> 00:42:55,119
right now we could definitely do better

00:42:51,820 --> 00:42:57,000
as far as disaster recovery game days

00:42:55,119 --> 00:42:59,500
and that's something that our team is

00:42:57,000 --> 00:43:02,770
interested in doing more frequently on a

00:42:59,500 --> 00:43:05,020
regular basis because every single time

00:43:02,770 --> 00:43:07,810
we've done a game day we've come onwe

00:43:05,020 --> 00:43:11,160
with a lot of learning and subsequently

00:43:07,810 --> 00:43:11,160
a more stable service

00:43:28,160 --> 00:43:37,380
we have designed the the services to

00:43:33,410 --> 00:43:40,619
take advantage of the my sequel query

00:43:37,380 --> 00:43:42,119
per query optimizer so we're not sure

00:43:40,619 --> 00:43:44,820
what the performance characteristics

00:43:42,119 --> 00:43:47,130
would be if you were to transfer to say

00:43:44,820 --> 00:43:50,820
Postgres or or anything else but it

00:43:47,130 --> 00:43:53,130
should work since we're doing the

00:43:50,820 --> 00:43:55,560
cockroach DB experimentation we've

00:43:53,130 --> 00:43:57,540
already noticed that in front 50 for

00:43:55,560 --> 00:43:59,840
example some of the ways that we had

00:43:57,540 --> 00:44:02,100
designed the schema was not exactly

00:43:59,840 --> 00:44:04,610
compatible so we've had to make a few

00:44:02,100 --> 00:44:06,840
modifications there but it should work

00:44:04,610 --> 00:44:09,770
maybe with a little bit of elbow grease

00:44:06,840 --> 00:44:13,190
but we've only tested it with my sequel

00:44:09,770 --> 00:44:13,190
it is yes

00:44:35,799 --> 00:44:43,279
I'm not an expert on that at all so

00:44:40,089 --> 00:44:47,509
knowing that I'm not an expert we we

00:44:43,279 --> 00:44:50,449
decided to adopt PF 4j which is a very

00:44:47,509 --> 00:44:55,909
well battle-tested plug-in framework

00:44:50,449 --> 00:44:59,179
that's used in solar for example so it's

00:44:55,909 --> 00:45:02,359
I can't give you a really good answer

00:44:59,179 --> 00:45:03,739
other than or standing on the shoulders

00:45:02,359 --> 00:45:06,309
of people that have already cut their

00:45:03,739 --> 00:45:06,309

YouTube URL: https://www.youtube.com/watch?v=8IaWCQ0J13g


