Title: Pere Kyle, Jacques Thomas "Spinnaker at Scale for Snap's Multi Cloud Service Mesh"
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	The third annual Spinnaker Summit (Diamond Sponsors: Netflix, Google and Armory) was held at the Hard Rock Hotel in San Diego, CA from November 15-17, 2019 and welcomed over 500 members of the rapidly growing Spinnaker open source community.
Captions: 
	00:00:00,740 --> 00:00:07,319
all right can everyone hear me all right

00:00:04,080 --> 00:00:09,389
there we go anyway just a quick

00:00:07,319 --> 00:00:10,170
introduction my name is Perry I work at

00:00:09,389 --> 00:00:12,360
snap

00:00:10,170 --> 00:00:14,880
as a software engineer in our service

00:00:12,360 --> 00:00:17,640
master team which is a part of our core

00:00:14,880 --> 00:00:19,160
infrastructure team and hi I'm Jack I'm

00:00:17,640 --> 00:00:21,330
in the infrastructure security team and

00:00:19,160 --> 00:00:23,779
we're like heavily involved in the mesh

00:00:21,330 --> 00:00:26,880
effort yeah I work along with Barry yes

00:00:23,779 --> 00:00:28,769
so what I want to talk to to today is

00:00:26,880 --> 00:00:30,630
about our service mesh and how that

00:00:28,769 --> 00:00:32,000
works so we'll go over this talk will be

00:00:30,630 --> 00:00:34,140
as some assumptions made about

00:00:32,000 --> 00:00:37,079
kubernetes that you might know or not

00:00:34,140 --> 00:00:40,170
know so if you get a little bit lost

00:00:37,079 --> 00:00:42,540
it's fine we'll loop it back but anyway

00:00:40,170 --> 00:00:44,969
I wanted to introduce the problem we had

00:00:42,540 --> 00:00:48,719
at snapchat when I got there so we had a

00:00:44,969 --> 00:00:50,910
monolith problem it was basically this

00:00:48,719 --> 00:00:51,690
big mono world of everything in one

00:00:50,910 --> 00:00:55,020
place

00:00:51,690 --> 00:00:58,469
so basically snapchat surprisingly all

00:00:55,020 --> 00:01:00,750
ran off of one API as of basically a

00:00:58,469 --> 00:01:03,719
little over one year ago so we have one

00:01:00,750 --> 00:01:05,460
App Engine project in one region with a

00:01:03,719 --> 00:01:06,630
few different micro services you could

00:01:05,460 --> 00:01:08,250
call them but at the end of the day it

00:01:06,630 --> 00:01:10,409
was the same API that just took

00:01:08,250 --> 00:01:12,810
different requests and you had this one

00:01:10,409 --> 00:01:13,080
big region and you can see the problem

00:01:12,810 --> 00:01:15,270
here

00:01:13,080 --> 00:01:17,009
snapchat would go down a lot because if

00:01:15,270 --> 00:01:19,799
that region and Google went down so did

00:01:17,009 --> 00:01:21,840
we so we wanted to make a concerted

00:01:19,799 --> 00:01:23,670
effort to move towards a service message

00:01:21,840 --> 00:01:27,060
so what's a service mesh it's kind of

00:01:23,670 --> 00:01:28,590
like a way to easily develop micro

00:01:27,060 --> 00:01:30,479
services so one of the worst parts about

00:01:28,590 --> 00:01:32,369
micro service right if you've ever built

00:01:30,479 --> 00:01:35,009
one you're like how do I authenticate to

00:01:32,369 --> 00:01:36,540
everyone how do I route to people like

00:01:35,009 --> 00:01:38,490
when like I like provision a load

00:01:36,540 --> 00:01:40,079
balancer and then like your service

00:01:38,490 --> 00:01:41,579
dependency changes their load balancer

00:01:40,079 --> 00:01:43,140
and break to you and that's what the

00:01:41,579 --> 00:01:46,680
service mesh is trying to solve it's a

00:01:43,140 --> 00:01:49,680
way to like model dependencies and so we

00:01:46,680 --> 00:01:52,710
got into this model world so why does it

00:01:49,680 --> 00:01:54,899
what does a snap mesh solve so basically

00:01:52,710 --> 00:01:56,490
we had a big ad-hoc thing so we had App

00:01:54,899 --> 00:01:57,990
Engine but we also had people that

00:01:56,490 --> 00:02:00,659
decided they hated App Engine and built

00:01:57,990 --> 00:02:02,640
their own thing we had deployment things

00:02:00,659 --> 00:02:05,460
all over the place we had Jenkins we had

00:02:02,640 --> 00:02:09,619
people's laptops that's not my favorite

00:02:05,460 --> 00:02:12,720
one we had basically regionalization and

00:02:09,619 --> 00:02:13,170
security was kind of retrofitted and

00:02:12,720 --> 00:02:16,020
hard

00:02:13,170 --> 00:02:18,180
to figure out and the authentication

00:02:16,020 --> 00:02:19,739
thing there was many different ways

00:02:18,180 --> 00:02:21,930
people thought you could alter Nikkei

00:02:19,739 --> 00:02:25,709
service most of them were probably not

00:02:21,930 --> 00:02:27,000
approved by Joc here so how could we

00:02:25,709 --> 00:02:29,310
solve this so that's where the mesh

00:02:27,000 --> 00:02:31,830
comes in because the mesh allows us to

00:02:29,310 --> 00:02:33,989
standardize and design the system

00:02:31,830 --> 00:02:36,660
holistically and kind of force people

00:02:33,989 --> 00:02:38,819
into this world that is safe and secure

00:02:36,660 --> 00:02:41,540
so like where's the security team fit in

00:02:38,819 --> 00:02:45,780
here so it'll have job talk about that

00:02:41,540 --> 00:02:47,370
right so the Becky alluded to it's like

00:02:45,780 --> 00:02:50,459
there's been a lot of work to retrofit

00:02:47,370 --> 00:02:52,709
security and so our goals actually in

00:02:50,459 --> 00:02:55,019
getting over the service mesh was to

00:02:52,709 --> 00:02:56,640
take whatever we've been trying to do

00:02:55,019 --> 00:02:58,980
but actually make it standard and

00:02:56,640 --> 00:03:01,860
default like zero trust network to make

00:02:58,980 --> 00:03:03,480
lateral movement harder I've been

00:03:01,860 --> 00:03:05,220
personally leading Crusader get along

00:03:03,480 --> 00:03:07,709
storm credentials so we got rid of that

00:03:05,220 --> 00:03:11,100
also for spinnaker and for most of the

00:03:07,709 --> 00:03:14,190
mesh and overall defragment the

00:03:11,100 --> 00:03:18,019
production infrastructure that's like on

00:03:14,190 --> 00:03:24,959
all aspects so it's easier to defend and

00:03:18,019 --> 00:03:26,670
spinnaker for us it really helps because

00:03:24,959 --> 00:03:29,060
it helps us provide this like paved road

00:03:26,670 --> 00:03:31,590
like netflix advocates where it's like

00:03:29,060 --> 00:03:35,280
here's a standard way to deploy to a

00:03:31,590 --> 00:03:39,030
standard runtime you use that your life

00:03:35,280 --> 00:03:40,560
is really easy and easier for us to you

00:03:39,030 --> 00:03:42,299
can do something else but then it's more

00:03:40,560 --> 00:03:45,120
complicated so we are like heavily in

00:03:42,299 --> 00:03:46,799
voting to make sure this whole path is

00:03:45,120 --> 00:03:52,260
as secure as possible and as easy as

00:03:46,799 --> 00:03:54,120
possible so now I'm gonna get into a

00:03:52,260 --> 00:03:55,829
quick little thing so give you a nice

00:03:54,120 --> 00:03:57,090
architectural diagram it might be a

00:03:55,829 --> 00:03:58,859
little bit too much if you've not

00:03:57,090 --> 00:03:59,880
familiar kubernetes or envoy but I

00:03:58,859 --> 00:04:01,049
thought it'd be cool to throw it in

00:03:59,880 --> 00:04:03,209
there you can always download the slides

00:04:01,049 --> 00:04:04,980
later and reference them so this is the

00:04:03,209 --> 00:04:07,890
first time we've I think ever presented

00:04:04,980 --> 00:04:10,680
this to a public audience so this is a

00:04:07,890 --> 00:04:12,090
snap service mesh so what's going on

00:04:10,680 --> 00:04:14,519
here I'll try to simplify as much as

00:04:12,090 --> 00:04:16,470
possible so external traffic as you can

00:04:14,519 --> 00:04:18,810
see that's like the app or like you

00:04:16,470 --> 00:04:20,910
making a snap it goes into these two

00:04:18,810 --> 00:04:23,250
different envoy fleets API gateway

00:04:20,910 --> 00:04:24,870
ingress gateway they're both used for

00:04:23,250 --> 00:04:26,670
different things API gateway is what

00:04:24,870 --> 00:04:28,320
you're basically your

00:04:26,670 --> 00:04:30,120
application your phone would connect to

00:04:28,320 --> 00:04:32,040
ingress gateway is how we go cross

00:04:30,120 --> 00:04:33,660
clouds so when you go over the link it

00:04:32,040 --> 00:04:36,450
goes through like a private link to the

00:04:33,660 --> 00:04:38,760
ingress gateway your services are all

00:04:36,450 --> 00:04:41,190
deployed in a common sidecar pattern in

00:04:38,760 --> 00:04:43,080
kubernetes so boy is a sidecar

00:04:41,190 --> 00:04:45,180
I'm boys just a proxy think of it as

00:04:43,080 --> 00:04:47,910
like a proxy or nginx you can also build

00:04:45,180 --> 00:04:49,050
a mess with nginx technically and then

00:04:47,910 --> 00:04:51,420
we have some stuff that's build that

00:04:49,050 --> 00:04:52,950
snaps so most a lot of you probably are

00:04:51,420 --> 00:04:54,390
familiar or heard about Sto

00:04:52,950 --> 00:04:56,130
it's kind of similar to that but it

00:04:54,390 --> 00:04:57,720
predates this Geo

00:04:56,130 --> 00:04:59,750
it's similar in a lot of ways but we

00:04:57,720 --> 00:05:02,490
have a lot of features SEO doesn't have

00:04:59,750 --> 00:05:04,320
that we needed as with respect to like

00:05:02,490 --> 00:05:05,970
multi cloud and like our circuit breaker

00:05:04,320 --> 00:05:08,400
pattern had some specific needs and

00:05:05,970 --> 00:05:09,750
authentication needs so that's what we

00:05:08,400 --> 00:05:11,820
built the control planes built in the

00:05:09,750 --> 00:05:14,100
house it's an envoy x GS server if

00:05:11,820 --> 00:05:16,200
you're into envoy switchboard is a

00:05:14,100 --> 00:05:19,080
front-end UI that we have our developers

00:05:16,200 --> 00:05:21,300
interact with so that's how a service

00:05:19,080 --> 00:05:23,370
owner like declares your service

00:05:21,300 --> 00:05:25,020
dependencies with like a nice UI so that

00:05:23,370 --> 00:05:27,300
way they don't have to like mess up like

00:05:25,020 --> 00:05:29,940
the configs and wear spinnaker fit in

00:05:27,300 --> 00:05:32,040
spinnaker deploys your service

00:05:29,940 --> 00:05:33,690
so spinnaker is able to deploy into the

00:05:32,040 --> 00:05:36,690
service mesh which at the end of day is

00:05:33,690 --> 00:05:38,220
a collection of kubernetes clusters so

00:05:36,690 --> 00:05:39,420
that's kind of how it looks and we'll

00:05:38,220 --> 00:05:41,790
get into like what this looks like a

00:05:39,420 --> 00:05:43,650
little bit more later so one thing I get

00:05:41,790 --> 00:05:45,420
a lot and was a big problem in the

00:05:43,650 --> 00:05:48,120
beginning this project is that snap has

00:05:45,420 --> 00:05:50,460
no way to say you must use something so

00:05:48,120 --> 00:05:52,500
this is a question I got quite a lot in

00:05:50,460 --> 00:05:53,760
the organization and it was always over

00:05:52,500 --> 00:05:57,420
and over again there was like why didn't

00:05:53,760 --> 00:05:59,490
you Jing connects or blah blah so here's

00:05:57,420 --> 00:06:02,610
kind of like where spankers unique I

00:05:59,490 --> 00:06:03,720
believe so we had to sell this vision of

00:06:02,610 --> 00:06:05,340
a deployment tool that we could

00:06:03,720 --> 00:06:07,020
standardize around and everyone has

00:06:05,340 --> 00:06:08,610
stuff everywhere so like they might be

00:06:07,020 --> 00:06:10,350
on the mesh but they still got that app

00:06:08,610 --> 00:06:13,080
engine service or they still got a VM

00:06:10,350 --> 00:06:14,220
they got a deploy to and Spinnaker's

00:06:13,080 --> 00:06:16,350
really the only thing I could think of

00:06:14,220 --> 00:06:17,610
that or fine that really had a way to

00:06:16,350 --> 00:06:19,260
integrate all these things together

00:06:17,610 --> 00:06:21,990
because there's cloud buy-in from all

00:06:19,260 --> 00:06:23,790
this we also had all these likely that

00:06:21,990 --> 00:06:25,290
goes into like deployment scripts people

00:06:23,790 --> 00:06:27,270
had these weird logics that they didn't

00:06:25,290 --> 00:06:29,280
want to keep but they're good with using

00:06:27,270 --> 00:06:30,660
some standardized stuff in the future so

00:06:29,280 --> 00:06:32,610
we had to give them a way to like

00:06:30,660 --> 00:06:34,290
migrate without them being like hard

00:06:32,610 --> 00:06:36,140
migration because we had some dangerous

00:06:34,290 --> 00:06:38,340
deployments we had to do very early on

00:06:36,140 --> 00:06:39,529
and then like people just didn't know

00:06:38,340 --> 00:06:41,509
about kubernetes

00:06:39,529 --> 00:06:44,089
they're like what is this like I don't

00:06:41,509 --> 00:06:46,069
see a UI like was a CLI and then you

00:06:44,089 --> 00:06:48,169
have to figure out where the deployment

00:06:46,069 --> 00:06:50,479
kind is and like why your thing is pod

00:06:48,169 --> 00:06:51,589
that crash back off and everyone was

00:06:50,479 --> 00:06:52,729
like what is this stuff and you don't

00:06:51,589 --> 00:06:54,829
even know it's crashing

00:06:52,729 --> 00:06:56,989
so speaker allows us to like give people

00:06:54,829 --> 00:06:59,149
a way to visualize and roll out their

00:06:56,989 --> 00:07:00,859
deployments and have a it's like an easy

00:06:59,149 --> 00:07:03,529
way so you can kind of like understand

00:07:00,859 --> 00:07:05,209
what's going on your system and get good

00:07:03,529 --> 00:07:06,709
alerts and like you kind of know what's

00:07:05,209 --> 00:07:08,659
going on and you get this one sort of

00:07:06,709 --> 00:07:10,909
purview of your system and your

00:07:08,659 --> 00:07:13,039
application and seeing like it all in

00:07:10,909 --> 00:07:14,449
one place is a very valuable tool I was

00:07:13,039 --> 00:07:16,279
actually very surprised about this like

00:07:14,449 --> 00:07:18,019
I used to talk with Eric and Maggie at

00:07:16,279 --> 00:07:19,579
Google and I was like I don't ever use

00:07:18,019 --> 00:07:21,349
the infrastructure tab our big teams

00:07:19,579 --> 00:07:22,939
were like we never use it but then

00:07:21,349 --> 00:07:24,739
actually all of our new teams really

00:07:22,939 --> 00:07:26,239
loved it they sit in there all day like

00:07:24,739 --> 00:07:28,039
looking at their deployment and trying

00:07:26,239 --> 00:07:29,479
to figure out what's going on I was very

00:07:28,039 --> 00:07:31,189
surprised about this feedback when I

00:07:29,479 --> 00:07:31,939
went to my customers and I was like like

00:07:31,189 --> 00:07:34,699
I never use it

00:07:31,939 --> 00:07:37,129
so I was like oh cool so it's a it's a

00:07:34,699 --> 00:07:38,629
pretty cool feature so yeah so it sounds

00:07:37,129 --> 00:07:40,219
like wow this is gonna be a great thing

00:07:38,629 --> 00:07:41,719
it's just going to work and everything's

00:07:40,219 --> 00:07:44,479
would be great well it didn't really

00:07:41,719 --> 00:07:47,569
quite work like that so I'll get into

00:07:44,479 --> 00:07:48,919
like some of the bad sides first so when

00:07:47,569 --> 00:07:51,589
we started the project there was a lot

00:07:48,919 --> 00:07:54,229
of features that we just didn't quite

00:07:51,589 --> 00:07:56,360
have or weren't quite there yet so at

00:07:54,229 --> 00:07:59,149
snap we use graphite we didn't use any

00:07:56,360 --> 00:08:00,889
of the supported cut Yenta things so and

00:07:59,149 --> 00:08:02,629
then one of our first customers our big

00:08:00,889 --> 00:08:04,729
hard requirement is that they wanted a

00:08:02,629 --> 00:08:08,419
canary system and they needed it like

00:08:04,729 --> 00:08:10,039
immediately so we had to build graphite

00:08:08,419 --> 00:08:13,609
support so we had snap actually built

00:08:10,039 --> 00:08:14,809
that and open sourced it and we also

00:08:13,609 --> 00:08:17,779
open sourced some other things around

00:08:14,809 --> 00:08:19,819
Caliente I won't get into but just like

00:08:17,779 --> 00:08:23,389
tweaks here and there and adding offsets

00:08:19,819 --> 00:08:24,979
and stuff like that so cloud driver was

00:08:23,389 --> 00:08:27,019
the Cooper Knights provider was in v1

00:08:24,979 --> 00:08:29,389
when we started this and v2 was just

00:08:27,019 --> 00:08:31,789
starting to be rolled out by Google and

00:08:29,389 --> 00:08:33,229
it just had a little bit it wasn't quite

00:08:31,789 --> 00:08:35,599
the future parody yet let's just say

00:08:33,229 --> 00:08:37,189
that so there was a lot of weird little

00:08:35,599 --> 00:08:40,309
things that would gremlins that would

00:08:37,189 --> 00:08:42,050
pop up and cloud driver didn't dynamic

00:08:40,309 --> 00:08:43,370
onboard accounts which was kind of a

00:08:42,050 --> 00:08:46,220
bummer but we kind of worked through it

00:08:43,370 --> 00:08:48,019
for a few years like a few months now of

00:08:46,220 --> 00:08:49,699
course like there's a nice talk from

00:08:48,019 --> 00:08:51,459
Scott about this this all works now and

00:08:49,699 --> 00:08:53,460
we'll talk about how to use it later and

00:08:51,459 --> 00:08:55,950
how you just didn't have

00:08:53,460 --> 00:08:57,300
way to deploy the kubernetes kinds that

00:08:55,950 --> 00:08:59,460
we needed we need to be able to use

00:08:57,300 --> 00:09:01,800
affinities and sidecars and Tyler didn't

00:08:59,460 --> 00:09:03,450
technically support it so that was

00:09:01,800 --> 00:09:05,910
something that was added in conjunction

00:09:03,450 --> 00:09:07,830
with Google so we had some more open

00:09:05,910 --> 00:09:09,600
questions like how are we going to

00:09:07,830 --> 00:09:12,300
handle like the multi clouds so

00:09:09,600 --> 00:09:14,310
spinnaker has to deploy across two

00:09:12,300 --> 00:09:16,950
different clouds maybe even three maybe

00:09:14,310 --> 00:09:18,300
even like datacenter in the future if we

00:09:16,950 --> 00:09:20,040
acquire a company we have to have

00:09:18,300 --> 00:09:21,900
something that will go anywhere and

00:09:20,040 --> 00:09:23,670
spinnaker is great for that but we

00:09:21,900 --> 00:09:26,460
weren't sure how to do it

00:09:23,670 --> 00:09:28,800
Fiat I couldn't find anyone using it

00:09:26,460 --> 00:09:31,650
online so I wasn't sure if anyone was

00:09:28,800 --> 00:09:33,030
using it in any sort of scale but we

00:09:31,650 --> 00:09:36,300
needed it because we need to have a ways

00:09:33,030 --> 00:09:37,620
secure the applications there wasn't a

00:09:36,300 --> 00:09:39,360
great amount of documentation at the

00:09:37,620 --> 00:09:41,490
time of around operational tasks like

00:09:39,360 --> 00:09:43,050
what was going to happen if things went

00:09:41,490 --> 00:09:45,420
wrong at how would you resolve them or

00:09:43,050 --> 00:09:46,770
what people post mortems from people

00:09:45,420 --> 00:09:48,510
that said like this is saying you need

00:09:46,770 --> 00:09:50,520
to do there's a lot of information on

00:09:48,510 --> 00:09:52,230
the community about that now and like

00:09:50,520 --> 00:09:53,970
Spinnaker's assumed the world were

00:09:52,230 --> 00:09:55,890
you're on the various secured VM with

00:09:53,970 --> 00:09:58,350
security groups so static credentials

00:09:55,890 --> 00:10:00,360
were all over the place wasn't great

00:09:58,350 --> 00:10:01,860
I was selling that to Joc was not an

00:10:00,360 --> 00:10:03,840
easy one I was like hey we just put the

00:10:01,860 --> 00:10:06,480
JSON for like the root credentials of

00:10:03,840 --> 00:10:09,570
all snapchat and adjacent file it's

00:10:06,480 --> 00:10:12,600
totally gonna work so scaling challenges

00:10:09,570 --> 00:10:14,100
we had quite a few of them read this was

00:10:12,600 --> 00:10:15,780
our main one I just probably plenty

00:10:14,100 --> 00:10:16,920
people in this audience have been bitten

00:10:15,780 --> 00:10:19,740
by Redis before

00:10:16,920 --> 00:10:21,570
so whether this has just always been a

00:10:19,740 --> 00:10:23,220
problem child for me like it worked

00:10:21,570 --> 00:10:25,050
really great until it doesn't and you

00:10:23,220 --> 00:10:26,460
abused it and cloud driver was kind of

00:10:25,050 --> 00:10:27,990
abusing it once you get to a certain

00:10:26,460 --> 00:10:30,420
amount of scale and I'm like cloud

00:10:27,990 --> 00:10:31,740
resources it's very hard to scale it you

00:10:30,420 --> 00:10:34,590
have to like start doing all kinds of

00:10:31,740 --> 00:10:36,510
crazy workarounds Fiat just crashed all

00:10:34,590 --> 00:10:38,970
the time in the beginning it's still

00:10:36,510 --> 00:10:40,680
kind of is very slow that's something

00:10:38,970 --> 00:10:42,270
that we've all as stamp really

00:10:40,680 --> 00:10:44,220
interested in fixing in the future and

00:10:42,270 --> 00:10:47,880
then cloud driver deserves its own slide

00:10:44,220 --> 00:10:49,260
so if you can scale cloud driver you can

00:10:47,880 --> 00:10:53,520
scale anything it's like riding a bike

00:10:49,260 --> 00:10:55,710
so cloud driver has a lot of different

00:10:53,520 --> 00:10:57,780
use cases and it's very poor to lump it

00:10:55,710 --> 00:10:59,460
in it's the opposite of a micro service

00:10:57,780 --> 00:11:03,000
it does everything it's caching it's

00:10:59,460 --> 00:11:05,370
doing offline jobs it's serving a UI you

00:11:03,000 --> 00:11:05,840
have to be very careful about it and

00:11:05,370 --> 00:11:09,230
it's

00:11:05,840 --> 00:11:11,150
very bad to put it into one instance you

00:11:09,230 --> 00:11:14,300
need to split it by workloads

00:11:11,150 --> 00:11:16,040
luckily halyard provides a very nice way

00:11:14,300 --> 00:11:19,100
to do this that we'll get into in a

00:11:16,040 --> 00:11:20,750
minute but we had a very very bad

00:11:19,100 --> 00:11:22,820
allergies that are very very hard to

00:11:20,750 --> 00:11:24,500
mitigate when we had even a lot of cloud

00:11:22,820 --> 00:11:26,720
driver pods because if there's a bug in

00:11:24,500 --> 00:11:28,640
for instance serving the UI you could

00:11:26,720 --> 00:11:30,590
take out caching or no one could deploy

00:11:28,640 --> 00:11:33,170
anymore because now cloud driver is in a

00:11:30,590 --> 00:11:35,540
stuck bad state and you want to avoid

00:11:33,170 --> 00:11:38,300
that so you want to keep the failure

00:11:35,540 --> 00:11:40,730
domains into like a into like a nice

00:11:38,300 --> 00:11:42,470
window so that way yeah did

00:11:40,730 --> 00:11:44,150
infrastructure tab is broken it's kind

00:11:42,470 --> 00:11:45,710
of bad but like no one's gonna be

00:11:44,150 --> 00:11:47,870
blocked they can still deploy they can

00:11:45,710 --> 00:11:49,460
still like look at what's going on the

00:11:47,870 --> 00:11:52,190
cloud but if you can't deploy that's

00:11:49,460 --> 00:11:53,780
really really really bad and the cache

00:11:52,190 --> 00:11:55,850
can be really bad because some providers

00:11:53,780 --> 00:11:57,890
rely on the cache to deploy so it's a

00:11:55,850 --> 00:11:59,150
very bad dependency to take so we had to

00:11:57,890 --> 00:12:00,440
go back to the drawing board so I was

00:11:59,150 --> 00:12:02,180
like how in the world we're gonna scale

00:12:00,440 --> 00:12:05,450
this thing we had 30 kubernetes clusters

00:12:02,180 --> 00:12:07,160
it was using a ton of heap and we're

00:12:05,450 --> 00:12:08,840
trying to figure out we hit the limits

00:12:07,160 --> 00:12:10,490
of mems store on Google cloud like we

00:12:08,840 --> 00:12:13,280
couldn't do it anymore

00:12:10,490 --> 00:12:15,020
so what do we do luckily Netflix also

00:12:13,280 --> 00:12:16,670
ran into similar problems so they

00:12:15,020 --> 00:12:18,950
started moving everything to my sequel

00:12:16,670 --> 00:12:22,610
so I was like hooray we have a database

00:12:18,950 --> 00:12:24,470
now so one thing I'd like to put there

00:12:22,610 --> 00:12:26,570
is always address is not database it has

00:12:24,470 --> 00:12:27,590
things that can make it a database but

00:12:26,570 --> 00:12:30,460
you probably don't want to use it as a

00:12:27,590 --> 00:12:32,570
database it's meant for a shared memory

00:12:30,460 --> 00:12:33,650
there's some very good use cases for it

00:12:32,570 --> 00:12:36,200
I'll get into a minute that we still use

00:12:33,650 --> 00:12:37,700
it for so like durability of execution

00:12:36,200 --> 00:12:39,770
history we had a great outage in our

00:12:37,700 --> 00:12:42,170
first month where read is flushed and

00:12:39,770 --> 00:12:44,140
everyone lost their pipelines not a

00:12:42,170 --> 00:12:46,610
great experience for the user

00:12:44,140 --> 00:12:49,490
try to move Orkut to cloud driver

00:12:46,610 --> 00:12:50,810
immediately cloud driver orchid to my

00:12:49,490 --> 00:12:52,720
sequel and then cloud driver than my

00:12:50,810 --> 00:12:54,590
sequel if you have in scale issues

00:12:52,720 --> 00:12:57,220
splitting cloud driver and cloud driver

00:12:54,590 --> 00:13:00,260
AJ is not not hard you should just do it

00:12:57,220 --> 00:13:01,820
it's not you you know there's a higher

00:13:00,260 --> 00:13:04,730
command I think this natively does it

00:13:01,820 --> 00:13:05,630
now and we'll get into that architecture

00:13:04,730 --> 00:13:07,190
in a minute

00:13:05,630 --> 00:13:08,750
and then one thing we also did is like

00:13:07,190 --> 00:13:11,270
each of the services you got treat them

00:13:08,750 --> 00:13:12,920
as their own thing so you need to use

00:13:11,270 --> 00:13:14,270
purpose-built node pools for all these

00:13:12,920 --> 00:13:15,620
so what's that mean so like just

00:13:14,270 --> 00:13:17,060
different instance types right make

00:13:15,620 --> 00:13:18,620
sense right some of these things don't

00:13:17,060 --> 00:13:19,220
use hardly any resources cloud driver

00:13:18,620 --> 00:13:21,139
needs

00:13:19,220 --> 00:13:23,089
a whole bunch you don't want to have to

00:13:21,139 --> 00:13:25,040
scale up your whole thing with 100

00:13:23,089 --> 00:13:27,500
gigabyte nodes because cloud drivers

00:13:25,040 --> 00:13:29,060
having an issue and then echos sitting

00:13:27,500 --> 00:13:32,180
there on this big note doing nothing all

00:13:29,060 --> 00:13:33,980
day so it's very important to try and

00:13:32,180 --> 00:13:36,860
design the note pool or design the node

00:13:33,980 --> 00:13:38,329
you're deploying to for the use case and

00:13:36,860 --> 00:13:39,500
then yeah like the static credentials

00:13:38,329 --> 00:13:41,300
everywhere we kind of had to get rid of

00:13:39,500 --> 00:13:43,490
that before I got the like keys to the

00:13:41,300 --> 00:13:45,290
kingdom at snapchat and we'll get into

00:13:43,490 --> 00:13:47,810
that in a minute how we did that so yeah

00:13:45,290 --> 00:13:49,610
this is a weird trick here that can

00:13:47,810 --> 00:13:51,740
eliminate static credentials this is

00:13:49,610 --> 00:13:56,540
what we did it's very exciting

00:13:51,740 --> 00:13:58,160
so we use sidecars so sidecars are just

00:13:56,540 --> 00:14:01,129
at the day a docker container running

00:13:58,160 --> 00:14:02,540
together with a shared sort of mount

00:14:01,129 --> 00:14:04,040
together they're all running it's just

00:14:02,540 --> 00:14:05,360
the same as if you ran docker run four

00:14:04,040 --> 00:14:07,910
times and you mounted the shared

00:14:05,360 --> 00:14:10,610
directory so what does that allow us to

00:14:07,910 --> 00:14:12,860
do so that allows us to have other

00:14:10,610 --> 00:14:16,160
processes or basically behaving as a

00:14:12,860 --> 00:14:18,319
daemon to do other things for us that

00:14:16,160 --> 00:14:20,300
helps spinnaker out without modifying

00:14:18,319 --> 00:14:22,550
the application that's to the joy of it

00:14:20,300 --> 00:14:23,750
so we have these different things here

00:14:22,550 --> 00:14:26,089
I'll go of them quickly what the arc

00:14:23,750 --> 00:14:28,250
wack is an internal thing that me and

00:14:26,089 --> 00:14:30,829
security built it just takes an open ID

00:14:28,250 --> 00:14:33,110
token and exchange is a jupe account for

00:14:30,829 --> 00:14:35,449
an AWS account every 30 minutes or

00:14:33,110 --> 00:14:36,920
something so that it basically rotates

00:14:35,449 --> 00:14:39,050
the credential into the shared directory

00:14:36,920 --> 00:14:41,870
so like your the SDKs that spinnaker

00:14:39,050 --> 00:14:44,360
uses relies on these known directories

00:14:41,870 --> 00:14:45,949
for better or worse so it says like home

00:14:44,360 --> 00:14:47,629
AWS credentials that's where my

00:14:45,949 --> 00:14:49,040
credentials are so if you just rotate

00:14:47,629 --> 00:14:51,019
credentials into that mount and you

00:14:49,040 --> 00:14:52,819
share it with a docker container you can

00:14:51,019 --> 00:14:54,649
just mount it and you could pull it from

00:14:52,819 --> 00:14:57,230
whatever source you guys use for

00:14:54,649 --> 00:14:59,059
credentials on voice so this is how we

00:14:57,230 --> 00:15:00,649
kind of got integrated with the service

00:14:59,059 --> 00:15:02,660
master cells like these guys sat next to

00:15:00,649 --> 00:15:04,579
means and yeah oh when I was like that

00:15:02,660 --> 00:15:06,589
sounds great I want to add off service

00:15:04,579 --> 00:15:08,329
and service off and this this thing

00:15:06,589 --> 00:15:09,350
doesn't know how to do it like well we

00:15:08,329 --> 00:15:11,120
can just like figure out the way to make

00:15:09,350 --> 00:15:13,430
envoy work so envoy allows us to

00:15:11,120 --> 00:15:17,269
essentially proxy requests out of the

00:15:13,430 --> 00:15:20,389
spinnaker into snaps infrastructure and

00:15:17,269 --> 00:15:22,100
use like a well-defined JWT token so

00:15:20,389 --> 00:15:23,540
well you don't have to build it into

00:15:22,100 --> 00:15:25,279
cloud driver cloud Rose knows nothing

00:15:23,540 --> 00:15:27,740
about it it just talks on localhost and

00:15:25,279 --> 00:15:29,449
like sends stuff out and it gets to the

00:15:27,740 --> 00:15:31,519
right like Jenkins cluster for instance

00:15:29,449 --> 00:15:32,960
or graphite and that allows us to use

00:15:31,519 --> 00:15:34,850
the proper stuff

00:15:32,960 --> 00:15:36,620
monitoring Damon is what there's always

00:15:34,850 --> 00:15:38,930
employees a sidecar

00:15:36,620 --> 00:15:40,700
we just used Prometheus internally and

00:15:38,930 --> 00:15:43,550
we're working on another thing that we

00:15:40,700 --> 00:15:45,680
might that internally to ship these two

00:15:43,550 --> 00:15:47,300
graphite as well and the great thing is

00:15:45,680 --> 00:15:49,490
halyard supports all these now natively

00:15:47,300 --> 00:15:52,040
so you can just define side cards you

00:15:49,490 --> 00:15:55,430
can divide post aliases mounts if you're

00:15:52,040 --> 00:15:57,590
into that so it's it's quite first-class

00:15:55,430 --> 00:15:59,150
citizen and this is great because we

00:15:57,590 --> 00:16:01,070
actually have no static credentials for

00:15:59,150 --> 00:16:02,720
the cloud everything is rotated by our

00:16:01,070 --> 00:16:05,150
cloud provider and DCP siding and the

00:16:02,720 --> 00:16:09,950
quack sidecars is constantly rotating in

00:16:05,150 --> 00:16:11,450
new AWS keys so yeah so now Jacque was

00:16:09,950 --> 00:16:13,160
happy he's like we can start going to

00:16:11,450 --> 00:16:16,460
prod but there is more stuff that will

00:16:13,160 --> 00:16:18,530
have him get into essentially when I'm

00:16:16,460 --> 00:16:21,380
not working on my projects I'm talking

00:16:18,530 --> 00:16:24,430
with him that's there's been a lot of

00:16:21,380 --> 00:16:29,030
time on it spinnaker so yeah so the

00:16:24,430 --> 00:16:30,470
direct access to essentially what he was

00:16:29,030 --> 00:16:32,510
explaining is one of the key things

00:16:30,470 --> 00:16:35,630
we've done is remove all the direct

00:16:32,510 --> 00:16:37,580
credentials and make sure that we only

00:16:35,630 --> 00:16:39,740
use instance credentials so our

00:16:37,580 --> 00:16:43,340
deployment is honed in GCP and strictly

00:16:39,740 --> 00:16:45,200
federates into AWS that federation path

00:16:43,340 --> 00:16:50,210
actually we've contributed fixes on the

00:16:45,200 --> 00:16:51,590
AWS side and and and so essentially like

00:16:50,210 --> 00:16:53,270
we've been pushing for that for a long

00:16:51,590 --> 00:16:55,790
time and like with the site cards were

00:16:53,270 --> 00:16:58,250
able to use that without changing how

00:16:55,790 --> 00:16:59,330
spinnaker works but we now bolted it to

00:16:58,250 --> 00:17:00,920
the ground like there's no credentials

00:16:59,330 --> 00:17:04,010
that are worth stealing because they all

00:17:00,920 --> 00:17:06,770
ephemeral we had to build perimeter

00:17:04,010 --> 00:17:09,080
security also to limit the impact of

00:17:06,770 --> 00:17:10,610
some privilege escalation problems that

00:17:09,080 --> 00:17:13,640
happens with every software so there's

00:17:10,610 --> 00:17:15,980
some we mention later and so for that we

00:17:13,640 --> 00:17:19,610
put a ap which is essentially the

00:17:15,980 --> 00:17:23,089
authenticating proxy from DCP in front

00:17:19,610 --> 00:17:25,760
of spinnaker and we had to do some very

00:17:23,089 --> 00:17:27,770
deserves the credit but he there's some

00:17:25,760 --> 00:17:29,930
work there to essentially merge Dec and

00:17:27,770 --> 00:17:33,310
gate as like one domain because you have

00:17:29,930 --> 00:17:35,810
to do that to make it work behind IEP

00:17:33,310 --> 00:17:37,880
feel free to come to us later we love to

00:17:35,810 --> 00:17:39,350
talk about those things and another

00:17:37,880 --> 00:17:41,270
thing we found out when we started more

00:17:39,350 --> 00:17:43,490
like pen testing is gates should be a

00:17:41,270 --> 00:17:46,790
choke point that's like part of the

00:17:43,490 --> 00:17:49,160
design and changing from VM

00:17:46,790 --> 00:17:51,260
- Carini's deployment which is a big

00:17:49,160 --> 00:17:53,510
shift essentially in how spinnaker has

00:17:51,260 --> 00:17:55,490
deployed these days made it so that this

00:17:53,510 --> 00:17:58,160
is not enforced by default so we added

00:17:55,490 --> 00:18:00,950
like a network policy to make sure that

00:17:58,160 --> 00:18:03,740
in unlikely case there's a problem with

00:18:00,950 --> 00:18:05,180
Dec what I still can only talk to gate I

00:18:03,740 --> 00:18:08,390
supposed to talk to all the internal

00:18:05,180 --> 00:18:10,160
services and yeah yet

00:18:08,390 --> 00:18:12,760
took some time to figure out and we're

00:18:10,160 --> 00:18:15,800
still not sure we fully figured it out

00:18:12,760 --> 00:18:20,120
and so that's something we'd love to

00:18:15,800 --> 00:18:21,760
discuss later so yeah so I was saying

00:18:20,120 --> 00:18:23,870
like we spend quite some time

00:18:21,760 --> 00:18:25,790
essentially looking also at paths for

00:18:23,870 --> 00:18:29,480
privilege escalation to not like dismiss

00:18:25,790 --> 00:18:30,710
insider threat and as I was saying so

00:18:29,480 --> 00:18:33,680
gate has a choke point something that

00:18:30,710 --> 00:18:36,080
has to be enforced the probably the most

00:18:33,680 --> 00:18:39,410
scary finding was you need to use the

00:18:36,080 --> 00:18:41,860
whitelist for web books you need it

00:18:39,410 --> 00:18:44,090
especially in a hosted VM environment

00:18:41,860 --> 00:18:46,670
because that can potentially get to

00:18:44,090 --> 00:18:48,080
instance metadata so again as long as

00:18:46,670 --> 00:18:51,500
you got perimeter security that's just

00:18:48,080 --> 00:18:52,910
an insider problem but you might still

00:18:51,500 --> 00:18:56,720
want to mitigate that make sure you got

00:18:52,910 --> 00:18:58,340
this one checked Fiat also has semantics

00:18:56,720 --> 00:19:01,580
that historically met if you can view a

00:18:58,340 --> 00:19:04,610
pipeline then you can execute it you can

00:19:01,580 --> 00:19:06,440
trigger it and now there's the execute

00:19:04,610 --> 00:19:08,990
permission which is great but for

00:19:06,440 --> 00:19:10,880
backwards compatibility read means

00:19:08,990 --> 00:19:13,840
execute by default so you do want to

00:19:10,880 --> 00:19:17,690
change that and make read mean only read

00:19:13,840 --> 00:19:19,160
if you can afford that and one thing

00:19:17,690 --> 00:19:22,520
that was concerned for us for quite some

00:19:19,160 --> 00:19:26,570
time was a spell and like what it could

00:19:22,520 --> 00:19:28,100
do so in terms of expels the pipeline

00:19:26,570 --> 00:19:31,190
expression language and you can

00:19:28,100 --> 00:19:32,210
essentially decide the spring expression

00:19:31,190 --> 00:19:34,580
language and you can use that to

00:19:32,210 --> 00:19:36,320
generate pipelines dynamically it's not

00:19:34,580 --> 00:19:38,810
that scary in terms of an embedded

00:19:36,320 --> 00:19:40,820
language what's still potentially scary

00:19:38,810 --> 00:19:43,160
depending how you operate is that it can

00:19:40,820 --> 00:19:45,650
pull dynamically pipeline configurations

00:19:43,160 --> 00:19:47,300
from arbitrary endpoints but the web

00:19:45,650 --> 00:19:49,790
work restrictions can also lock that

00:19:47,300 --> 00:19:52,130
down so again you probably want to use

00:19:49,790 --> 00:19:55,430
that and for pipeline triggers we rely

00:19:52,130 --> 00:19:56,840
heavily on pub/sub and back to things

00:19:55,430 --> 00:19:59,340
were not sure how to mitigate yet with

00:19:56,840 --> 00:20:00,900
Fiat is making sure that the trigger

00:19:59,340 --> 00:20:03,330
we receive is actually a legitimate

00:20:00,900 --> 00:20:05,250
trigger like we don't really have a way

00:20:03,330 --> 00:20:07,530
to hook that into spinnaker so the best

00:20:05,250 --> 00:20:10,350
we can do is to do very tight access

00:20:07,530 --> 00:20:12,090
control on the pub subtopics to make

00:20:10,350 --> 00:20:19,290
sure only authorized parties can trigger

00:20:12,090 --> 00:20:21,840
our pipelines all right so I'm gonna get

00:20:19,290 --> 00:20:23,940
into a little bit about what our setup

00:20:21,840 --> 00:20:26,040
looks like now so I'll get into some

00:20:23,940 --> 00:20:28,740
detail we'll show you the beginnings of

00:20:26,040 --> 00:20:30,870
where we started and then what we did to

00:20:28,740 --> 00:20:32,910
mitigate these issues so we kind of

00:20:30,870 --> 00:20:35,190
referenced the problem of Redis in the

00:20:32,910 --> 00:20:38,880
beginning so this is kind of like where

00:20:35,190 --> 00:20:40,200
our v1 prod cluster looks like if you're

00:20:38,880 --> 00:20:41,700
not familiar with all these micro

00:20:40,200 --> 00:20:43,560
service names mean yeah you're new the

00:20:41,700 --> 00:20:45,330
spinnaker just I'll go over it a little

00:20:43,560 --> 00:20:47,550
bit it's not super important in this

00:20:45,330 --> 00:20:50,520
slide so in general like orca cloud

00:20:47,550 --> 00:20:53,850
driver Fiat they used Redis shared

00:20:50,520 --> 00:20:55,290
memory Redis and Fiat used our Google

00:20:53,850 --> 00:20:57,720
Groups which is a great integration

00:20:55,290 --> 00:20:59,340
because now all of our Gmail accounts

00:20:57,720 --> 00:21:02,610
now work inside spinnaker which was

00:20:59,340 --> 00:21:05,100
actually very nice echo we only allowed

00:21:02,610 --> 00:21:07,260
it to use pub/sub so we kind of came up

00:21:05,100 --> 00:21:08,730
with our own pub sub topic format that

00:21:07,260 --> 00:21:10,830
translated into artifacts

00:21:08,730 --> 00:21:13,320
there's the haproxy trick we talked

00:21:10,830 --> 00:21:14,430
about earlier so that also the nice

00:21:13,320 --> 00:21:17,640
thing about that is it kind of

00:21:14,430 --> 00:21:19,170
chokepoints that one place so all the

00:21:17,640 --> 00:21:21,330
people come through IP which is very

00:21:19,170 --> 00:21:23,250
secure then it hits a hop proxy so we

00:21:21,330 --> 00:21:25,980
know that you can't just get right into

00:21:23,250 --> 00:21:28,320
like the portal gate directly it allows

00:21:25,980 --> 00:21:30,630
us add some logging and metrics as well

00:21:28,320 --> 00:21:31,950
egor which is like what interacts with

00:21:30,630 --> 00:21:35,130
Jenkins essentially as probably most

00:21:31,950 --> 00:21:36,750
people use it for we still had people on

00:21:35,130 --> 00:21:38,460
Jenkins for things or they want to

00:21:36,750 --> 00:21:41,880
trigger like an integration test it's

00:21:38,460 --> 00:21:43,260
very useful for that and the front 50 is

00:21:41,880 --> 00:21:45,330
basically just where it stores all the

00:21:43,260 --> 00:21:47,970
configs so that's stored in Google Cloud

00:21:45,330 --> 00:21:49,740
storage so this is our initial solution

00:21:47,970 --> 00:21:51,750
so you can see the problem here we had a

00:21:49,740 --> 00:21:55,200
single point of failure in Redis that

00:21:51,750 --> 00:21:56,850
was pretty bad now that was the one that

00:21:55,200 --> 00:21:58,680
business multiple times and also the

00:21:56,850 --> 00:21:59,790
cloud drivers were all and one big cloud

00:21:58,680 --> 00:22:02,130
driver pod

00:21:59,790 --> 00:22:05,370
they weren't split by service type yet

00:22:02,130 --> 00:22:11,490
so then where did we go from here so

00:22:05,370 --> 00:22:13,049
this is the more salient slide so this

00:22:11,490 --> 00:22:14,220
is where we are today so it's a lot

00:22:13,049 --> 00:22:16,139
going on here so I'll take some more

00:22:14,220 --> 00:22:19,230
time on this slide to explain what's

00:22:16,139 --> 00:22:20,159
going on here so the top level is so you

00:22:19,230 --> 00:22:21,360
can see these multiple different

00:22:20,159 --> 00:22:24,149
kubernetes know pools

00:22:21,360 --> 00:22:26,129
represented by these dashed lines and

00:22:24,149 --> 00:22:27,720
like the kubernetes logo so each of

00:22:26,129 --> 00:22:29,100
these represent a note pool that has a

00:22:27,720 --> 00:22:30,960
certain permission or they have a

00:22:29,100 --> 00:22:32,909
certain instance type they're using so

00:22:30,960 --> 00:22:35,639
I'll go over what all these means so the

00:22:32,909 --> 00:22:36,929
cloud driver note pool is the most

00:22:35,639 --> 00:22:38,429
privileged note pool this is the one

00:22:36,929 --> 00:22:42,779
that has the service account that can

00:22:38,429 --> 00:22:44,279
get into AWS Google wherever and it's

00:22:42,779 --> 00:22:47,279
very very dangerous and needs the most

00:22:44,279 --> 00:22:49,889
memory the most CPU and all that

00:22:47,279 --> 00:22:53,429
goodness there so all of these pods are

00:22:49,889 --> 00:22:56,009
split by anti affinity so that means

00:22:53,429 --> 00:22:57,929
they all reject each other I find that

00:22:56,009 --> 00:23:00,090
in general all the services would prefer

00:22:57,929 --> 00:23:01,440
to be one one instance because there's a

00:23:00,090 --> 00:23:03,239
whole lot of weird things that can go on

00:23:01,440 --> 00:23:05,609
sometimes in kubernetes with scheduling

00:23:03,239 --> 00:23:07,710
so I would recommend using an affinities

00:23:05,609 --> 00:23:09,450
you can look it up online but you can

00:23:07,710 --> 00:23:11,309
use all the server selectors to say like

00:23:09,450 --> 00:23:13,109
reject all other cloud driver pods on

00:23:11,309 --> 00:23:15,090
this node and then make a tolerance to

00:23:13,109 --> 00:23:17,369
say only two cloud driver can come here

00:23:15,090 --> 00:23:19,830
so that's what we did with the cloud

00:23:17,369 --> 00:23:21,960
driver thing as you can see there's a my

00:23:19,830 --> 00:23:24,899
sequel so we split my sequel into two of

00:23:21,960 --> 00:23:26,730
them the cloud driver one is the latest

00:23:24,899 --> 00:23:29,639
one because that's the biggest one that

00:23:26,730 --> 00:23:32,100
allows us to deprecated the reddest

00:23:29,639 --> 00:23:33,509
cache as you can see Redis is still here

00:23:32,100 --> 00:23:35,850
though and I'll get into what that means

00:23:33,509 --> 00:23:39,029
so there's a kind of bug with cloud

00:23:35,850 --> 00:23:40,350
sequel that doesn't work with the driver

00:23:39,029 --> 00:23:43,470
it used it doesn't work with cloud

00:23:40,350 --> 00:23:45,269
sequel but it's fine because the Redis

00:23:43,470 --> 00:23:47,249
scheduler is actually pretty good for

00:23:45,269 --> 00:23:49,320
what Redis is meant for so the caching

00:23:47,249 --> 00:23:50,789
agents and cloud driver they all go to

00:23:49,320 --> 00:23:52,169
Redis and say I want to start caching

00:23:50,789 --> 00:23:54,149
something please take a lock and then

00:23:52,169 --> 00:23:55,710
they do do work and then put it into my

00:23:54,149 --> 00:23:56,609
sequel which is red this is really good

00:23:55,710 --> 00:23:58,460
dad doesn't matter if it goes down

00:23:56,609 --> 00:24:00,749
they'll just pick up its work again

00:23:58,460 --> 00:24:03,179
orchid still uses it to the scheduled

00:24:00,749 --> 00:24:04,379
tasks because it's a very good queue but

00:24:03,179 --> 00:24:06,359
it has its own mind sequel which is

00:24:04,379 --> 00:24:08,309
important so splitting these two is good

00:24:06,359 --> 00:24:10,080
because we have backups of both of them

00:24:08,309 --> 00:24:11,789
like we don't really need the backup

00:24:10,080 --> 00:24:14,429
maybe the cloud driver one that could

00:24:11,789 --> 00:24:15,869
probably rebuild itself but the working

00:24:14,429 --> 00:24:17,669
ones very important make sure you have

00:24:15,869 --> 00:24:19,379
it in my sequel make sure it has backups

00:24:17,669 --> 00:24:21,690
if you're gonna go to prod it's very

00:24:19,379 --> 00:24:23,580
important the privilege note pool over

00:24:21,690 --> 00:24:25,710
here is similar to the cloud driver one

00:24:23,580 --> 00:24:26,759
that has very more privileged

00:24:25,710 --> 00:24:29,489
provincials so these

00:24:26,759 --> 00:24:31,499
gentles can talk inside of the snapchat

00:24:29,489 --> 00:24:32,609
infrastructure using the sidecar we

00:24:31,499 --> 00:24:35,399
talked about but it doesn't necessarily

00:24:32,609 --> 00:24:38,369
have deployment permissions but it can

00:24:35,399 --> 00:24:41,639
like talk to Jenkins and do things and

00:24:38,369 --> 00:24:43,079
then tokens and stuff like that then we

00:24:41,639 --> 00:24:44,879
didn't change anything on echo or front

00:24:43,079 --> 00:24:47,009
50 those stayed the same luckily and

00:24:44,879 --> 00:24:48,749
then one thing we also did here is we

00:24:47,009 --> 00:24:49,649
have a public note pool this is the one

00:24:48,749 --> 00:24:52,019
that has a service account that

00:24:49,649 --> 00:24:53,429
literally can't do anything if you got

00:24:52,019 --> 00:24:55,079
onto the pod here you wouldn't be able

00:24:53,429 --> 00:24:57,779
to do much so there may be curl Gate a

00:24:55,079 --> 00:25:00,089
little bit and then this one also has a

00:24:57,779 --> 00:25:02,459
network calico network policy associated

00:25:00,089 --> 00:25:03,809
with it that blocks them from talking to

00:25:02,459 --> 00:25:06,569
anything but what it's supposed to talk

00:25:03,809 --> 00:25:08,549
to so H a proxy can only talk to Gate

00:25:06,569 --> 00:25:10,589
and deck directly gate and deck can't

00:25:08,549 --> 00:25:12,389
talk to anything else except for where

00:25:10,589 --> 00:25:13,829
it's supposed to talk to deck can't talk

00:25:12,389 --> 00:25:15,059
to anything at all so if you got into

00:25:13,829 --> 00:25:18,119
the deck pod you couldn't even do

00:25:15,059 --> 00:25:20,309
anything and that allows us to like keep

00:25:18,119 --> 00:25:21,569
the security model inside the cluster a

00:25:20,309 --> 00:25:23,699
lot better because we have this choke

00:25:21,569 --> 00:25:26,279
point now and then the off-note pool is

00:25:23,699 --> 00:25:28,289
a special one it has a different scope

00:25:26,279 --> 00:25:29,999
that allows it to talk to G suite

00:25:28,289 --> 00:25:31,529
there's a special Olaf scope in Google

00:25:29,999 --> 00:25:34,139
Cloud you can give to the talk to G

00:25:31,529 --> 00:25:35,459
suite and it has its own service account

00:25:34,139 --> 00:25:37,259
that has no cloud permissions but it has

00:25:35,459 --> 00:25:39,839
very privileged permissions on G suite

00:25:37,259 --> 00:25:42,389
to pool Google Groups and that's kind of

00:25:39,839 --> 00:25:44,159
the overall architecture we have here so

00:25:42,389 --> 00:25:46,339
this gives us a lot better of a failure

00:25:44,159 --> 00:25:49,769
domain than we had before

00:25:46,339 --> 00:25:51,569
so now that like cloud driver comp keeps

00:25:49,769 --> 00:25:53,549
to itself there is still Redis there

00:25:51,569 --> 00:25:56,099
that could go down but it's used so

00:25:53,549 --> 00:25:58,229
sparsely now that I'm not too worried

00:25:56,099 --> 00:26:00,209
about it because at the other day it's

00:25:58,229 --> 00:26:03,149
not it's a very low QPS now it's just

00:26:00,209 --> 00:26:05,009
scheduling jobs in Orca has backups and

00:26:03,149 --> 00:26:07,559
in my sequel which is the only way to

00:26:05,009 --> 00:26:09,839
deploy it in my opinion so this is a

00:26:07,559 --> 00:26:12,479
much better architecture now also if

00:26:09,839 --> 00:26:16,139
like like for instance we have a lot on

00:26:12,479 --> 00:26:17,969
and then ro deck gets overloaded it

00:26:16,139 --> 00:26:19,409
won't take down like deployments they

00:26:17,969 --> 00:26:20,789
still run like maybe like the

00:26:19,409 --> 00:26:22,379
infrastructure tabs broken but users

00:26:20,789 --> 00:26:24,539
don't know this the problem and then

00:26:22,379 --> 00:26:27,059
they usually reboot this hasn't happened

00:26:24,539 --> 00:26:28,529
in a very long time but since we've done

00:26:27,059 --> 00:26:30,659
a lot a lot of fixes have gone into

00:26:28,529 --> 00:26:33,419
spinnaker that made this work a whole

00:26:30,659 --> 00:26:34,829
heck of a lot better so then like we

00:26:33,419 --> 00:26:38,759
have all this thing deployed out it's

00:26:34,829 --> 00:26:40,679
pretty stable what do we do next year so

00:26:38,759 --> 00:26:42,929
this is where it gets more interesting

00:26:40,679 --> 00:26:48,389
this is how we're integrating things in

00:26:42,929 --> 00:26:50,309
better with the service mesh so here's a

00:26:48,389 --> 00:26:51,990
little quick slide we'll go over about

00:26:50,309 --> 00:26:56,369
like the services that surround

00:26:51,990 --> 00:26:58,980
spinnaker so users represent like a

00:26:56,369 --> 00:27:01,200
developer at snapchat they come into the

00:26:58,980 --> 00:27:04,759
switchboard which is the user interface

00:27:01,200 --> 00:27:07,200
that we integrate with the service mesh

00:27:04,759 --> 00:27:09,210
so this is like where all our users our

00:27:07,200 --> 00:27:11,639
developers will talk to that's what they

00:27:09,210 --> 00:27:13,019
that's what they feel the service mesh

00:27:11,639 --> 00:27:16,710
is they talk to this thing and this is

00:27:13,019 --> 00:27:18,299
what they associated with under the hood

00:27:16,710 --> 00:27:21,389
switchboard has a bunch of sort of

00:27:18,299 --> 00:27:23,730
helper services that live in the mesh as

00:27:21,389 --> 00:27:25,200
well that it talks to one of them is a

00:27:23,730 --> 00:27:27,990
cloud provisioning service called

00:27:25,200 --> 00:27:31,309
Hatchery so what tatri does is

00:27:27,990 --> 00:27:34,679
essentially apply like terraform and

00:27:31,309 --> 00:27:37,679
cloud formation templates to to bring up

00:27:34,679 --> 00:27:40,499
new GCP or new AWS kubernetes clusters

00:27:37,679 --> 00:27:43,710
do like cluster operations for the user

00:27:40,499 --> 00:27:46,679
like upgrading node pools adding new

00:27:43,710 --> 00:27:49,230
node pools and then deleting clusters

00:27:46,679 --> 00:27:51,090
that sort of stuff so also that way the

00:27:49,230 --> 00:27:54,240
user no longer interacts with the cloud

00:27:51,090 --> 00:27:55,740
directly this allows us to control what

00:27:54,240 --> 00:27:57,570
the naming scheme of clusters our

00:27:55,740 --> 00:27:58,919
control what the off scheme of the

00:27:57,570 --> 00:28:00,899
clusters are we can install our own

00:27:58,919 --> 00:28:04,789
roles inside of the kubernetes our back

00:28:00,899 --> 00:28:06,869
and that allows a lot of different

00:28:04,789 --> 00:28:08,669
integrations now and one of those is

00:28:06,869 --> 00:28:10,230
convoys so convoys the service I built

00:28:08,669 --> 00:28:12,720
at snap

00:28:10,230 --> 00:28:15,600
it's a spinnaker automation thing that

00:28:12,720 --> 00:28:18,840
sits on top of us maker at today's it's

00:28:15,600 --> 00:28:20,340
pretty simple so the problem we had is

00:28:18,840 --> 00:28:21,840
that we have all these services in the

00:28:20,340 --> 00:28:24,389
mesh they all have this like snapchat

00:28:21,840 --> 00:28:26,759
authentication scheme we have to convert

00:28:24,389 --> 00:28:28,259
that into the IP authentication scheme

00:28:26,759 --> 00:28:30,570
it's actually quite heavy to do that

00:28:28,259 --> 00:28:31,740
handshake on all the requests and of

00:28:30,570 --> 00:28:33,210
course it doesn't integrate with the

00:28:31,740 --> 00:28:35,909
mesh because it has to do this like oh

00:28:33,210 --> 00:28:38,009
well rigmarole so what we did is like we

00:28:35,909 --> 00:28:39,899
kind of convoys sitting there as a choke

00:28:38,009 --> 00:28:41,309
point that integrates with our ecosystem

00:28:39,899 --> 00:28:44,509
inside of snapchat so we have our own

00:28:41,309 --> 00:28:46,830
internal ecosystem and this allows us to

00:28:44,509 --> 00:28:48,570
do a bunch of interesting things around

00:28:46,830 --> 00:28:50,999
like does this service account have this

00:28:48,570 --> 00:28:52,680
access can it access this pipeline or

00:28:50,999 --> 00:28:54,840
can it trigger this pipeline going

00:28:52,680 --> 00:28:56,910
to the pipeline integrity so we choke

00:28:54,840 --> 00:29:00,210
all of the triggers actually is a web

00:28:56,910 --> 00:29:02,220
hook now to convoy and we can check like

00:29:00,210 --> 00:29:04,350
verify the token it's coming from within

00:29:02,220 --> 00:29:06,990
snapchats infrastructure and it's valid

00:29:04,350 --> 00:29:09,720
and about validate that that service

00:29:06,990 --> 00:29:11,220
owner has granted that permission to

00:29:09,720 --> 00:29:13,890
that service account so like they've

00:29:11,220 --> 00:29:16,710
added it to this internal akka list this

00:29:13,890 --> 00:29:18,150
enables us to like validate triggers a

00:29:16,710 --> 00:29:20,910
lot better and choke it in the one spot

00:29:18,150 --> 00:29:23,160
so we have a full sort of view of what's

00:29:20,910 --> 00:29:25,260
going on it makes it a lot easier for us

00:29:23,160 --> 00:29:27,690
to manage like triggers and how people

00:29:25,260 --> 00:29:29,520
are using their pipeline one thing that

00:29:27,690 --> 00:29:32,190
pictured here is that we also have Bend

00:29:29,520 --> 00:29:34,620
like an internal simple going COI that

00:29:32,190 --> 00:29:36,900
abstract away some of the stuff we

00:29:34,620 --> 00:29:38,670
actually have a few teams using spin CLI

00:29:36,900 --> 00:29:40,230
I'd highly recommend it if you're

00:29:38,670 --> 00:29:42,440
automating things there's a lot of teams

00:29:40,230 --> 00:29:45,180
that actually use spin CLI and scripts

00:29:42,440 --> 00:29:47,490
to automate their pipeline creation or

00:29:45,180 --> 00:29:50,070
updates but we'll get into like what how

00:29:47,490 --> 00:29:54,900
we see about updating pipelines in the

00:29:50,070 --> 00:29:57,750
mesh in just a minute here so how do

00:29:54,900 --> 00:30:01,740
snap developers provision a new

00:29:57,750 --> 00:30:03,690
kubernetes cluster so this is like

00:30:01,740 --> 00:30:06,330
something that we've spent a lot of time

00:30:03,690 --> 00:30:08,490
on like it was very hard because people

00:30:06,330 --> 00:30:11,700
would provision a cluster in the

00:30:08,490 --> 00:30:13,380
beginning and it would just be like all

00:30:11,700 --> 00:30:14,640
kinds of messed up they would do stuff

00:30:13,380 --> 00:30:16,410
wrong they would put it in like the

00:30:14,640 --> 00:30:18,960
wrong security group and then jocks tool

00:30:16,410 --> 00:30:21,870
would have to go and like find the bad

00:30:18,960 --> 00:30:23,820
open network policy or the bad open the

00:30:21,870 --> 00:30:25,230
firewall rule and they would put it in

00:30:23,820 --> 00:30:27,480
the wrong role and lock themselves out

00:30:25,230 --> 00:30:28,650
of the cluster I can't tell you how many

00:30:27,480 --> 00:30:30,450
times I've actually had the use

00:30:28,650 --> 00:30:31,950
Spinnaker's super role to bring a

00:30:30,450 --> 00:30:34,590
cluster back to life for people that

00:30:31,950 --> 00:30:37,230
lock themselves out it's pretty easy to

00:30:34,590 --> 00:30:38,900
do if you edit the our back roles and

00:30:37,230 --> 00:30:40,950
then lock everyone out by accident

00:30:38,900 --> 00:30:43,980
luckily spinnaker has a role that can

00:30:40,950 --> 00:30:45,870
get into it so we thought about this how

00:30:43,980 --> 00:30:47,190
would we have a user provision of

00:30:45,870 --> 00:30:49,500
cluster and it made sense to put it

00:30:47,190 --> 00:30:51,780
inside of the UI that users use to

00:30:49,500 --> 00:30:54,330
interact with their service so I'll give

00:30:51,780 --> 00:30:56,310
a quick so the animated gif so we'll see

00:30:54,330 --> 00:30:59,940
how this works so here's a switchboard

00:30:56,310 --> 00:31:01,920
UI this is kind of where service owners

00:30:59,940 --> 00:31:03,240
interact with their service it's a

00:31:01,920 --> 00:31:05,160
looping gift so you don't have to follow

00:31:03,240 --> 00:31:06,420
it the first time so this is kind of how

00:31:05,160 --> 00:31:10,560
users

00:31:06,420 --> 00:31:12,660
create a new kubernetes cluster as you

00:31:10,560 --> 00:31:14,220
can see here so what's the salient

00:31:12,660 --> 00:31:15,570
pieces here so it just basically allows

00:31:14,220 --> 00:31:18,360
them to choose all the same things they

00:31:15,570 --> 00:31:20,430
might choose inside the AWS or GCP CLI

00:31:18,360 --> 00:31:22,200
but what's happening here on the back

00:31:20,430 --> 00:31:23,940
end is like once it gets provisioned

00:31:22,200 --> 00:31:25,170
there's a lot of things that happen on

00:31:23,940 --> 00:31:28,020
the back end here so you can see the

00:31:25,170 --> 00:31:29,250
service mesh that's actually our staging

00:31:28,020 --> 00:31:30,990
server semester so you can see the

00:31:29,250 --> 00:31:32,700
dependency graph of what's going on and

00:31:30,990 --> 00:31:36,150
the whole mesh which is an interesting

00:31:32,700 --> 00:31:38,340
UI so what's happening on the back end

00:31:36,150 --> 00:31:39,840
here so when we provision the things we

00:31:38,340 --> 00:31:42,090
choke people in one point there's many

00:31:39,840 --> 00:31:44,310
different ways you could do this as like

00:31:42,090 --> 00:31:46,020
an owner of your infrastructure you

00:31:44,310 --> 00:31:47,700
could choke people with like a certain

00:31:46,020 --> 00:31:48,930
type of like terraform script you could

00:31:47,700 --> 00:31:50,970
add some automations around that you

00:31:48,930 --> 00:31:53,190
could use like deployment manager on DCP

00:31:50,970 --> 00:31:56,310
whatever you want to use but what this

00:31:53,190 --> 00:32:00,360
allows is that the cluster metadata is

00:31:56,310 --> 00:32:02,070
now stored with the service metadata so

00:32:00,360 --> 00:32:03,570
now we know like who owns this cluster

00:32:02,070 --> 00:32:05,300
now we have a Google Group associated

00:32:03,570 --> 00:32:07,530
with we know it's your project we know

00:32:05,300 --> 00:32:09,930
alerting key there's a lot of things

00:32:07,530 --> 00:32:11,550
that we can integrate now we also know

00:32:09,930 --> 00:32:12,990
the cluster is gonna be auto upgraded

00:32:11,550 --> 00:32:15,480
because it's part of our managed

00:32:12,990 --> 00:32:17,370
clusters now and we know that spinnaker

00:32:15,480 --> 00:32:19,530
has its are back roles installed in the

00:32:17,370 --> 00:32:22,800
cluster by default so that way we're

00:32:19,530 --> 00:32:24,750
ready to go to deploy to it so basically

00:32:22,800 --> 00:32:26,640
that what would happen in after this is

00:32:24,750 --> 00:32:29,010
you just wait for about 10 20 minutes

00:32:26,640 --> 00:32:31,230
for the cloud provider to spin it up and

00:32:29,010 --> 00:32:33,240
then you'll see a green little dot and

00:32:31,230 --> 00:32:36,720
that's when you can start provisioning a

00:32:33,240 --> 00:32:39,060
deployable cluster so that's what we'll

00:32:36,720 --> 00:32:41,100
get into next is like how do snap

00:32:39,060 --> 00:32:42,630
developers deploy to this new cluster so

00:32:41,100 --> 00:32:45,060
now this cluster shows up in your

00:32:42,630 --> 00:32:47,610
services view you know you've made your

00:32:45,060 --> 00:32:49,140
service dependency thing so you've asked

00:32:47,610 --> 00:32:52,290
your service dependencies hey please

00:32:49,140 --> 00:32:53,580
grant this lease and then they'll say

00:32:52,290 --> 00:32:55,650
like yeah cool you can talk to me now

00:32:53,580 --> 00:32:57,930
you're ready to go and now that means

00:32:55,650 --> 00:32:59,820
under the hood envoy has the routing

00:32:57,930 --> 00:33:02,010
tables associated installed

00:32:59,820 --> 00:33:03,750
automatically in your service and also

00:33:02,010 --> 00:33:05,520
all the authentication keys are white

00:33:03,750 --> 00:33:07,590
listed now so that means you can

00:33:05,520 --> 00:33:09,720
authenticate back and forth and which

00:33:07,590 --> 00:33:10,740
well it's unidirectional bi-directional

00:33:09,720 --> 00:33:13,410
I could go either way

00:33:10,740 --> 00:33:15,000
and you're ready to go so you're alright

00:33:13,410 --> 00:33:17,370
we're cool we've set up everything my

00:33:15,000 --> 00:33:19,970
metadata for my service it's all ready

00:33:17,370 --> 00:33:21,080
to go how do I then

00:33:19,970 --> 00:33:23,929
deployed to it so that's what a

00:33:21,080 --> 00:33:26,299
spinnaker comes in so what would look

00:33:23,929 --> 00:33:27,710
like today is that the services owner

00:33:26,299 --> 00:33:30,769
can go into their system so here's our

00:33:27,710 --> 00:33:32,149
example service so they have a cluster

00:33:30,769 --> 00:33:37,100
here which is basically just a logical

00:33:32,149 --> 00:33:40,100
grouping of accounts so here's sort of a

00:33:37,100 --> 00:33:41,809
UI we built on top of spinnaker so you

00:33:40,100 --> 00:33:45,620
can select all your clusters you're

00:33:41,809 --> 00:33:47,389
doing here you can select like what the

00:33:45,620 --> 00:33:49,159
composition of options you want in your

00:33:47,389 --> 00:33:49,759
pipelines so you can like say I want a

00:33:49,159 --> 00:33:52,639
canary

00:33:49,759 --> 00:33:54,740
I want a zonal rollout or in like health

00:33:52,639 --> 00:33:56,679
checks we'll get into like what that

00:33:54,740 --> 00:33:59,389
means in just a minute

00:33:56,679 --> 00:34:00,710
but there's a lot of stuff going on this

00:33:59,389 --> 00:34:02,240
UI but the salient piece for the

00:34:00,710 --> 00:34:04,279
spinnaker is like so there's three

00:34:02,240 --> 00:34:05,990
different things you can do you can look

00:34:04,279 --> 00:34:08,060
up pipelines so we're gonna start

00:34:05,990 --> 00:34:09,710
integrating Spinnaker's like status and

00:34:08,060 --> 00:34:11,030
- here - so like you can go into like a

00:34:09,710 --> 00:34:13,399
service owners page and be like did

00:34:11,030 --> 00:34:15,200
their deployment fail like Kesari go in

00:34:13,399 --> 00:34:16,700
here and like deploy this application if

00:34:15,200 --> 00:34:18,710
they need to if there's like a

00:34:16,700 --> 00:34:20,329
mitigating issue can must as an

00:34:18,710 --> 00:34:22,669
infrastructure team know what's going on

00:34:20,329 --> 00:34:24,440
with all of our applications that's kind

00:34:22,669 --> 00:34:27,020
of where we want to go with it but for

00:34:24,440 --> 00:34:29,089
now we just provision pipelines and we

00:34:27,020 --> 00:34:30,589
provision and you can like update

00:34:29,089 --> 00:34:33,470
pipelines so what's happening under the

00:34:30,589 --> 00:34:35,389
hood is convoy has a list of just in

00:34:33,470 --> 00:34:39,139
golang it's just like a go templates of

00:34:35,389 --> 00:34:41,510
all these different stages and it just

00:34:39,139 --> 00:34:43,909
kind of like loops through you could

00:34:41,510 --> 00:34:46,849
maybe accomplish something similar with

00:34:43,909 --> 00:34:49,970
manage pipeline v2 but we originally did

00:34:46,849 --> 00:34:51,619
it this way and going just because we

00:34:49,970 --> 00:34:53,540
had a whole bunch of loops and stuff and

00:34:51,619 --> 00:34:55,599
spell and loops and all this other

00:34:53,540 --> 00:34:58,910
madness I didn't feel too comfortable

00:34:55,599 --> 00:35:00,589
making all that work but there's a lot

00:34:58,910 --> 00:35:01,970
of other things on the horizon I think

00:35:00,589 --> 00:35:03,470
for making this work better but also

00:35:01,970 --> 00:35:05,119
another thing you can do here is trigger

00:35:03,470 --> 00:35:06,859
I think quite get into that so that

00:35:05,119 --> 00:35:08,329
allow you to like trigger a pipeline if

00:35:06,859 --> 00:35:10,069
you're like what I talked about earlier

00:35:08,329 --> 00:35:11,990
if like SRS or like oh we need to

00:35:10,069 --> 00:35:13,460
trigger the rollback of this thing you

00:35:11,990 --> 00:35:14,810
could technically trigger it through

00:35:13,460 --> 00:35:17,329
here then that will hit the convoy

00:35:14,810 --> 00:35:19,700
service and combo Bend the new pub/sub

00:35:17,329 --> 00:35:22,609
token we're also going with this in the

00:35:19,700 --> 00:35:24,050
future is that the triggering we want to

00:35:22,609 --> 00:35:27,500
get to the point where our developers

00:35:24,050 --> 00:35:29,089
just need to know a image to like them

00:35:27,500 --> 00:35:30,619
once they have their app we should be

00:35:29,089 --> 00:35:32,540
able to deploy it on the mesh and that's

00:35:30,619 --> 00:35:33,619
where we're gonna keep working these

00:35:32,540 --> 00:35:36,109
integrations like

00:35:33,619 --> 00:35:36,650
deeply and try to improve upon them in

00:35:36,109 --> 00:35:39,650
the future

00:35:36,650 --> 00:35:41,180
so then after hitting save what does it

00:35:39,650 --> 00:35:44,960
look like in spinnaker so this is what

00:35:41,180 --> 00:35:46,880
it would kind of look like so this is

00:35:44,960 --> 00:35:50,990
like the spinnaker installs are actual

00:35:46,880 --> 00:35:53,299
prods banker so you would like there's

00:35:50,990 --> 00:35:56,990
the cluster we've provision that's

00:35:53,299 --> 00:35:58,970
already there and there's all your

00:35:56,990 --> 00:36:00,710
checks so basically what's happened

00:35:58,970 --> 00:36:02,690
under the hood is convoy just composes

00:36:00,710 --> 00:36:05,269
all these steps it says hey I can query

00:36:02,690 --> 00:36:07,730
your metadata I know you're in ABC and

00:36:05,269 --> 00:36:09,950
US central one and you're in BCD and

00:36:07,730 --> 00:36:12,589
like AP South East one like it knows

00:36:09,950 --> 00:36:13,940
like the regions and the zones that your

00:36:12,589 --> 00:36:15,319
note pools are in because it deployed

00:36:13,940 --> 00:36:16,549
them so that's an interesting

00:36:15,319 --> 00:36:18,769
integration because it knows exactly

00:36:16,549 --> 00:36:20,839
what's which deployment selectors and

00:36:18,769 --> 00:36:23,269
which note selectors it needs to use to

00:36:20,839 --> 00:36:25,970
vend your deployment this allows us to

00:36:23,269 --> 00:36:27,259
in the future this will just all be and

00:36:25,970 --> 00:36:29,450
hopefully in the world of man delivery

00:36:27,259 --> 00:36:32,690
this will just be me applying this thing

00:36:29,450 --> 00:36:34,249
and it will just go out the cool thing

00:36:32,690 --> 00:36:35,809
about this is is like we've used a lot

00:36:34,249 --> 00:36:38,960
of the mesh integrations here right so

00:36:35,809 --> 00:36:41,140
we know the cluster name the cool thing

00:36:38,960 --> 00:36:44,119
about that is Conway will put this into

00:36:41,140 --> 00:36:46,039
github and use the spring cloud config

00:36:44,119 --> 00:36:47,749
thing which is a new feature that I

00:36:46,039 --> 00:36:49,910
think Scott's talking with Frederick is

00:36:47,749 --> 00:36:51,980
talking about tomorrow this allows us to

00:36:49,910 --> 00:36:54,170
actually onboard things automatically so

00:36:51,980 --> 00:36:55,910
it only takes about eight minutes for a

00:36:54,170 --> 00:36:58,130
new kubernetes cluster to show up in our

00:36:55,910 --> 00:37:00,259
prods been occurred now which is faster

00:36:58,130 --> 00:37:01,819
than usually a people know that it's

00:37:00,259 --> 00:37:04,279
available inside of switchboard so it's

00:37:01,819 --> 00:37:06,140
essentially instantaneous new clusters

00:37:04,279 --> 00:37:08,630
show up in eight minutes and that allows

00:37:06,140 --> 00:37:10,519
us to like know how to like just naming

00:37:08,630 --> 00:37:12,019
scheme is everything computers so if you

00:37:10,519 --> 00:37:14,599
have the naming scheme down you can do

00:37:12,019 --> 00:37:16,579
everything so that's the important part

00:37:14,599 --> 00:37:17,779
here is that you know what the networks

00:37:16,579 --> 00:37:19,519
are named you know what the cluster is

00:37:17,779 --> 00:37:20,839
named you know who owns them you know

00:37:19,519 --> 00:37:24,380
where they should be going and you know

00:37:20,839 --> 00:37:26,569
how to deploy it and that's it and we're

00:37:24,380 --> 00:37:28,130
gonna continue kind of working on this

00:37:26,569 --> 00:37:30,380
over the next year and making it even

00:37:28,130 --> 00:37:31,460
more away from the user so then you have

00:37:30,380 --> 00:37:34,819
to know how this works

00:37:31,460 --> 00:37:36,890
so that gets into a few future

00:37:34,819 --> 00:37:40,489
improvements that we're I just started

00:37:36,890 --> 00:37:42,079
talking about a little bit you know I'll

00:37:40,489 --> 00:37:44,349
let Jacque go about the access control

00:37:42,079 --> 00:37:44,349
stuff

00:37:45,440 --> 00:37:50,150
convoy that you saw the all that

00:37:48,080 --> 00:37:52,550
automation of pipeline creation pipeline

00:37:50,150 --> 00:37:54,920
triggering it's like it's awesome what

00:37:52,550 --> 00:37:56,920
it does it's also terrifying from my

00:37:54,920 --> 00:38:00,560
perspective because to make it work

00:37:56,920 --> 00:38:02,600
essentially we got two choices Ivor we

00:38:00,560 --> 00:38:04,460
because we use Google Groups like we're

00:38:02,600 --> 00:38:06,470
on GC P so here's Google for everything

00:38:04,460 --> 00:38:08,660
and typically like teams use their own

00:38:06,470 --> 00:38:11,420
Google Groups to like the team internal

00:38:08,660 --> 00:38:15,140
group to put on the cluster and so now

00:38:11,420 --> 00:38:16,190
if we want this robot to access pinnacle

00:38:15,140 --> 00:38:17,870
we got two choices

00:38:16,190 --> 00:38:20,750
I've heard that robot can read the

00:38:17,870 --> 00:38:22,460
internal mail of every team or that

00:38:20,750 --> 00:38:24,500
robot comes and currently that's what

00:38:22,460 --> 00:38:26,810
happens comes as full admin on spinnaker

00:38:24,500 --> 00:38:28,400
which means now that robot as we told

00:38:26,810 --> 00:38:29,810
you like we're consolidating everything

00:38:28,400 --> 00:38:32,600
on spinnaker so there's more than the

00:38:29,810 --> 00:38:35,150
mesh so now that robot I would like it

00:38:32,600 --> 00:38:38,570
to only touch the mesh if possible and

00:38:35,150 --> 00:38:39,590
so we're we feel ok with the posture

00:38:38,570 --> 00:38:44,090
right now but it's something that we

00:38:39,590 --> 00:38:47,210
would like to make better and a related

00:38:44,090 --> 00:38:49,790
thing is for the the webhook stages or

00:38:47,210 --> 00:38:51,440
for trigger validations like it seems

00:38:49,790 --> 00:38:55,880
like right now we're not able to express

00:38:51,440 --> 00:38:57,200
really the kind of security checks that

00:38:55,880 --> 00:39:00,740
we would like to get out of the system

00:38:57,200 --> 00:39:03,680
so I started discussing that a bunch in

00:39:00,740 --> 00:39:05,810
the on the security sig we got to kinda

00:39:03,680 --> 00:39:08,870
like a list of like irritating problems

00:39:05,810 --> 00:39:10,100
and I'd love to discuss that more with

00:39:08,870 --> 00:39:11,930
whoever's interested like we're trying

00:39:10,100 --> 00:39:14,240
to see if we can do a get-together at

00:39:11,930 --> 00:39:17,030
some point during the summit to see like

00:39:14,240 --> 00:39:18,620
what makes sense as a community to make

00:39:17,030 --> 00:39:21,530
this a little bit better like the

00:39:18,620 --> 00:39:23,750
analogy I use this maybe give

00:39:21,530 --> 00:39:25,670
spinnaker can like the selinux but light

00:39:23,750 --> 00:39:27,290
treatment and we don't necessarily to

00:39:25,670 --> 00:39:30,190
involve the NSA or to go like that and

00:39:27,290 --> 00:39:34,460
depth but get a good idea of the surface

00:39:30,190 --> 00:39:36,770
that we want to cover and and maybe that

00:39:34,460 --> 00:39:39,380
fits with the plug initiatives I make

00:39:36,770 --> 00:39:40,970
the enforcement pluggable we're

00:39:39,380 --> 00:39:43,040
definitely the first kind it is Fiat

00:39:40,970 --> 00:39:44,870
there's no question about it but then

00:39:43,040 --> 00:39:46,790
it's like is Fiat the one that can grow

00:39:44,870 --> 00:39:48,320
to capture everything what do we need to

00:39:46,790 --> 00:39:50,990
remail a sec conversation I would love

00:39:48,320 --> 00:39:52,370
to have whoever come find me or find

00:39:50,990 --> 00:39:54,680
armory and find me like we're really

00:39:52,370 --> 00:39:56,120
looking at it yeah and one of the

00:39:54,680 --> 00:39:57,560
biggest problems actually it's an

00:39:56,120 --> 00:39:58,490
interesting use case on nothing when

00:39:57,560 --> 00:39:58,940
solved a here

00:39:58,490 --> 00:40:01,339
del

00:39:58,940 --> 00:40:03,920
it so we're building like our own

00:40:01,339 --> 00:40:05,150
internal infrastructure team you know

00:40:03,920 --> 00:40:07,670
when you're on the cloud right you have

00:40:05,150 --> 00:40:09,079
like a cloud engineer someone is like

00:40:07,670 --> 00:40:10,700
provisioning things for you under the

00:40:09,079 --> 00:40:12,470
hood right they're upgrading or maybe

00:40:10,700 --> 00:40:15,170
you could Renee's master or the control

00:40:12,470 --> 00:40:17,420
plane below which your container is

00:40:15,170 --> 00:40:18,740
scheduled on and since we are actually

00:40:17,420 --> 00:40:20,780
as an infrastructure team essentially

00:40:18,740 --> 00:40:22,250
managing these sidecars and managing

00:40:20,780 --> 00:40:24,410
these damon sets we install in people's

00:40:22,250 --> 00:40:26,690
clusters we need a good way to actually

00:40:24,410 --> 00:40:28,970
be able to update these in a safe manner

00:40:26,690 --> 00:40:30,950
so that's been a very tough problem to

00:40:28,970 --> 00:40:32,839
solve like how do we roll out new config

00:40:30,950 --> 00:40:35,359
updates and test them without taking out

00:40:32,839 --> 00:40:38,690
like old memories that would be really

00:40:35,359 --> 00:40:40,490
bad so and how do we manage like upgrade

00:40:38,690 --> 00:40:42,470
non-void and on knowing that we haven't

00:40:40,490 --> 00:40:44,270
broken somebody or detecting it very

00:40:42,470 --> 00:40:45,200
quickly in rolling back and that's

00:40:44,270 --> 00:40:46,549
something that we've been trying to

00:40:45,200 --> 00:40:48,079
figure out it's like it's kind of a

00:40:46,549 --> 00:40:49,400
weird use case we're not deploying the

00:40:48,079 --> 00:40:51,260
actual application we're deploying the

00:40:49,400 --> 00:40:53,869
things around the application and we

00:40:51,260 --> 00:40:55,579
need a safe way to do that if like with

00:40:53,869 --> 00:40:58,430
validations and stuff and I feel like

00:40:55,579 --> 00:41:01,660
there's something there we could do and

00:40:58,430 --> 00:41:08,569
it's been a tough one to figure out like

00:41:01,660 --> 00:41:10,250
yeah and now I would say okay because

00:41:08,569 --> 00:41:11,630
one of the things that really sold us on

00:41:10,250 --> 00:41:14,150
the infrastructure security side was

00:41:11,630 --> 00:41:15,880
division from envoi from Matt client

00:41:14,150 --> 00:41:18,650
which is you know you do this thing

00:41:15,880 --> 00:41:20,420
where you put like a service mesh and

00:41:18,650 --> 00:41:22,369
you essentially you take back yard

00:41:20,420 --> 00:41:24,890
control of your network because like

00:41:22,369 --> 00:41:26,960
right now essentially we have like TLS

00:41:24,890 --> 00:41:29,289
between all the nodes we're about to

00:41:26,960 --> 00:41:30,890
switch on em TLS in most places and

00:41:29,289 --> 00:41:33,559
that's beautiful

00:41:30,890 --> 00:41:36,619
but the one thing that's really holding

00:41:33,559 --> 00:41:39,440
us from the most from rolling those nice

00:41:36,619 --> 00:41:43,190
features out is we're not yet fully able

00:41:39,440 --> 00:41:46,250
to slow bake those changes and Cannery

00:41:43,190 --> 00:41:48,980
them the right way like one region one

00:41:46,250 --> 00:41:51,049
canary box per region per service let it

00:41:48,980 --> 00:41:53,690
simmer while the other deployments go

00:41:51,049 --> 00:41:55,279
for the main app and eventually change

00:41:53,690 --> 00:41:58,130
all the side cars that's a vexing

00:41:55,279 --> 00:42:00,170
problem right now so it's a kind of a

00:41:58,130 --> 00:42:01,700
toughy we've still been trying to figure

00:42:00,170 --> 00:42:03,920
that one out like how do you roll allow

00:42:01,700 --> 00:42:06,380
app to roll forward while you're testing

00:42:03,920 --> 00:42:08,599
something and then you don't know which

00:42:06,380 --> 00:42:09,690
is what you're testing it's been a tough

00:42:08,599 --> 00:42:11,819
one

00:42:09,690 --> 00:42:14,700
currently unsolved so it's something

00:42:11,819 --> 00:42:16,260
we're trying to figure out next year but

00:42:14,700 --> 00:42:18,750
yeah that kind of concludes the main

00:42:16,260 --> 00:42:20,369
slides here I've left some time allotted

00:42:18,750 --> 00:42:21,809
for questions and stuff because it's

00:42:20,369 --> 00:42:23,839
kind of a lot of content and people

00:42:21,809 --> 00:42:30,079
might have specific questions or a

00:42:23,839 --> 00:42:30,079
clarifying answer on anything but yeah

00:42:33,290 --> 00:42:37,500
thank you very much Perry and Jacque I

00:42:35,910 --> 00:42:39,150
also just wanted to confirm everybody

00:42:37,500 --> 00:42:40,890
that their slides will be available in

00:42:39,150 --> 00:42:42,750
the spinnaker summit app later we'll get

00:42:40,890 --> 00:42:44,369
them up there but also these guys are

00:42:42,750 --> 00:42:46,589
great they'd love to talk to you go find

00:42:44,369 --> 00:42:48,869
them so yeah I'm always in the spinnaker

00:42:46,589 --> 00:42:50,730
slack and I think my email was on that

00:42:48,869 --> 00:42:54,119
slide somewhere so now I'll get tons of

00:42:50,730 --> 00:42:55,440
recruiter emails so I just realized that

00:42:54,119 --> 00:42:57,839
today when I looked at that slide that

00:42:55,440 --> 00:42:59,040
actually logged with my real account so

00:42:57,839 --> 00:43:03,480
that's what happens when you're making

00:42:59,040 --> 00:43:05,369
slides than overnight so poor warned but

00:43:03,480 --> 00:43:09,900
happy to have anyone here to talk to me

00:43:05,369 --> 00:43:14,099
so hi I'm Phillip Aaron's idiot

00:43:09,900 --> 00:43:17,099
Salesforce I was curious about your mesh

00:43:14,099 --> 00:43:19,319
but why did you choose to go down this

00:43:17,099 --> 00:43:21,240
path instead of just doing sto did you

00:43:19,319 --> 00:43:22,740
evaluate sto and stuff

00:43:21,240 --> 00:43:27,839
oh yeah that's a very good question you

00:43:22,740 --> 00:43:29,220
want to take it or me okay the very

00:43:27,839 --> 00:43:30,270
short version is when we looked at it

00:43:29,220 --> 00:43:33,000
Steele

00:43:30,270 --> 00:43:34,380
I wouldn't go cross cloud and I was like

00:43:33,000 --> 00:43:36,869
deal breaker like that's the top level

00:43:34,380 --> 00:43:39,359
one like actually I got pitched to join

00:43:36,869 --> 00:43:41,520
snap like over three years ago

00:43:39,359 --> 00:43:45,990
and the division was look so we're on

00:43:41,520 --> 00:43:47,339
GCP and we wanted to book GCP n awsm I

00:43:45,990 --> 00:43:49,710
guess that crazy that's awesome let's do

00:43:47,339 --> 00:43:52,190
it and Esther was like essentially a big

00:43:49,710 --> 00:43:54,510
deal breaker in the middle at that point

00:43:52,190 --> 00:43:56,520
yeah there's also a couple other things

00:43:54,510 --> 00:43:59,160
so snap scale is actually quite

00:43:56,520 --> 00:44:01,140
staggering as far as QPS and like the

00:43:59,160 --> 00:44:02,910
stuff like that so the SEO team was even

00:44:01,140 --> 00:44:04,890
scared a bit when we're like well we

00:44:02,910 --> 00:44:07,049
might need like 10 million QPS they're

00:44:04,890 --> 00:44:09,450
like whoa whoa we haven't tested

00:44:07,049 --> 00:44:10,710
anything near that so that's why we

00:44:09,450 --> 00:44:13,109
built it internally there's a bunch of

00:44:10,710 --> 00:44:15,000
other features if I had my head of the

00:44:13,109 --> 00:44:16,319
mesh here he'd probably sit here for an

00:44:15,000 --> 00:44:18,809
hour talking about all the features that

00:44:16,319 --> 00:44:19,950
he needed but that was the main ones

00:44:18,809 --> 00:44:23,040
just crossed out

00:44:19,950 --> 00:44:25,049
so I do just want to say that the the

00:44:23,040 --> 00:44:26,270
keynote starts in 15 minutes but happy

00:44:25,049 --> 00:44:28,770
to keep passing this microphone around

00:44:26,270 --> 00:44:33,329
not that I'm really authorized to change

00:44:28,770 --> 00:44:36,420
anything but hey you talked about alt

00:44:33,329 --> 00:44:38,549
sidecar mm-hmm and you mentioned about

00:44:36,420 --> 00:44:43,559
provides service to service auth tokens

00:44:38,549 --> 00:44:46,049
can you just give a little bit sure so

00:44:43,559 --> 00:44:50,250
essentially we got several kinds of

00:44:46,049 --> 00:44:52,049
identification the main one that you see

00:44:50,250 --> 00:44:54,089
with the off-site car is actually

00:44:52,049 --> 00:44:58,559
dealing with service to service where we

00:44:54,089 --> 00:45:00,480
essentially build our own JWT so look

00:44:58,559 --> 00:45:02,339
yeah there's a few fields that are not

00:45:00,480 --> 00:45:04,619
exactly like if we did a second tech

00:45:02,339 --> 00:45:10,829
would probably clean some fields but

00:45:04,619 --> 00:45:12,480
essentially is just using on Google we

00:45:10,829 --> 00:45:14,280
rely a lot on the service account keys

00:45:12,480 --> 00:45:16,829
that are authoritative because again we

00:45:14,280 --> 00:45:18,750
don't want long-term credentials and you

00:45:16,829 --> 00:45:20,309
send it just a sign token with claims

00:45:18,750 --> 00:45:24,990
saying I claim to be desperate this

00:45:20,309 --> 00:45:26,040
service talking to that service knock

00:45:24,990 --> 00:45:28,290
knock please let me in

00:45:26,040 --> 00:45:30,030
that's that's the gist of it and off

00:45:28,290 --> 00:45:31,829
site that's the the authentication we do

00:45:30,030 --> 00:45:34,680
and the off-site car essentially

00:45:31,829 --> 00:45:37,520
encapsulate that logic to vendor token

00:45:34,680 --> 00:45:41,869
and to verify them on the receipt again

00:45:37,520 --> 00:45:41,869
this Tara dress you're okay

00:45:46,040 --> 00:45:50,670
no not yet

00:45:48,090 --> 00:45:54,540
it's it's some of it we probably

00:45:50,670 --> 00:45:56,610
wouldn't be some of it is everything got

00:45:54,540 --> 00:45:58,830
like written really as fast as

00:45:56,610 --> 00:46:00,420
reasonably possible and so now that all

00:45:58,830 --> 00:46:03,630
of it is like ready to be open sourced

00:46:00,420 --> 00:46:05,970
not that we don't want to and we already

00:46:03,630 --> 00:46:09,240
have some efforts to open source part of

00:46:05,970 --> 00:46:10,790
our software as we find the time as we

00:46:09,240 --> 00:46:15,930
finally also to just do the big rewrite

00:46:10,790 --> 00:46:17,460
yeah okay

00:46:15,930 --> 00:46:19,240
thank you very much everybody right

00:46:17,460 --> 00:46:26,140
thank you thank you out

00:46:19,240 --> 00:46:26,140

YouTube URL: https://www.youtube.com/watch?v=u5osLmRx48A


