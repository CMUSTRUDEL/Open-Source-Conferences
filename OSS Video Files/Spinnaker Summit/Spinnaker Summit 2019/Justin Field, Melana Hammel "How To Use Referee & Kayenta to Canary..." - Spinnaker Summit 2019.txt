Title: Justin Field, Melana Hammel "How To Use Referee & Kayenta to Canary..." - Spinnaker Summit 2019
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	The third annual Spinnaker Summit (Diamond Sponsors: Netflix, Google and Armory) was held at the Hard Rock Hotel in San Diego, CA from November 15-17, 2019 and welcomed over 500 members of the rapidly growing Spinnaker open source community.
Captions: 
	00:00:06,330 --> 00:00:10,980
[Laughter]

00:00:13,670 --> 00:00:19,580
you're at work sitting at your desk and

00:00:16,730 --> 00:00:21,500
it seems that all as well you just

00:00:19,580 --> 00:00:24,529
deployed a minor code change where you

00:00:21,500 --> 00:00:26,660
added additional unit tests you glance

00:00:24,529 --> 00:00:27,380
at your production graphs and everything

00:00:26,660 --> 00:00:29,480
looks healthy

00:00:27,380 --> 00:00:33,440
so you direct your attention to more

00:00:29,480 --> 00:00:37,180
urgent work a few hours later your phone

00:00:33,440 --> 00:00:39,770
starts buzzing your phone is blowing up

00:00:37,180 --> 00:00:42,350
turns out your recent deploy has caused

00:00:39,770 --> 00:00:44,660
your service to go haywire users are

00:00:42,350 --> 00:00:48,710
receiving 500 errors and you have no

00:00:44,660 --> 00:00:51,260
idea why your floor this was just a

00:00:48,710 --> 00:00:54,260
simple unit test and now you're facing a

00:00:51,260 --> 00:00:58,370
p1 incident how in the world are you

00:00:54,260 --> 00:01:02,920
going to fix this how many of you have

00:00:58,370 --> 00:01:07,190
ever experienced anything like this and

00:01:02,920 --> 00:01:11,150
what did you do to solve it maybe you

00:01:07,190 --> 00:01:15,020
dug into log yuri ran your tests you

00:01:11,150 --> 00:01:18,619
crossed your fingers but what if this

00:01:15,020 --> 00:01:23,299
wasn't the way it had to be what if

00:01:18,619 --> 00:01:25,220
there was another way a better way where

00:01:23,299 --> 00:01:30,500
you would never even get to this point

00:01:25,220 --> 00:01:31,880
to begin with my name is Melina Hamill

00:01:30,500 --> 00:01:34,600
and I'm Jessica

00:01:31,880 --> 00:01:37,369
we are both software engineers at Nike

00:01:34,600 --> 00:01:40,070
Nikes ecommerce business is in the

00:01:37,369 --> 00:01:43,759
billions so this means that for any

00:01:40,070 --> 00:01:46,219
given outage or production issue for one

00:01:43,759 --> 00:01:48,950
of our consumer facing services there's

00:01:46,219 --> 00:01:50,750
a lot of money on the line we work on a

00:01:48,950 --> 00:01:52,759
platform team where we create cloud

00:01:50,750 --> 00:01:55,520
native solutions to enable other

00:01:52,759 --> 00:01:58,759
engineers at Nike and encourage C ICD

00:01:55,520 --> 00:02:00,860
best practices we wanted to reduce the

00:01:58,759 --> 00:02:01,870
risk of four deployments for the

00:02:00,860 --> 00:02:05,170
engineers that

00:02:01,870 --> 00:02:07,810
serve so our search led us to what we

00:02:05,170 --> 00:02:11,320
are here today to share with you use

00:02:07,810 --> 00:02:12,720
Canaria deployments what our canary

00:02:11,320 --> 00:02:15,400
deployments

00:02:12,720 --> 00:02:17,380
well canary deployments introduced a

00:02:15,400 --> 00:02:20,020
code change to a small percentage of

00:02:17,380 --> 00:02:22,840
your traffic where does the term canary

00:02:20,020 --> 00:02:24,820
come from canary in a coalmine the

00:02:22,840 --> 00:02:26,560
reference goes back to when miners would

00:02:24,820 --> 00:02:29,350
bring Canaries down into the mines with

00:02:26,560 --> 00:02:32,440
them if the Canaries stopped singing

00:02:29,350 --> 00:02:33,910
because it died the miners knew to get

00:02:32,440 --> 00:02:36,550
out of the mine because there is a

00:02:33,910 --> 00:02:39,130
carbon monoxide leak so Canaries are now

00:02:36,550 --> 00:02:42,580
known as early indicators of potential

00:02:39,130 --> 00:02:45,280
failure or dangers and for software

00:02:42,580 --> 00:02:48,310
deployments Canaries offer a lot of

00:02:45,280 --> 00:02:50,140
value they can help catch mistakes or

00:02:48,310 --> 00:02:52,750
errors that previously would have been

00:02:50,140 --> 00:02:54,940
deployed straight to production they

00:02:52,750 --> 00:02:57,400
offer a way to test new code in

00:02:54,940 --> 00:02:59,910
production while limiting the blast

00:02:57,400 --> 00:03:02,110
radius if anything goes wrong and

00:02:59,910 --> 00:03:05,040
because canary deployments can be

00:03:02,110 --> 00:03:07,660
automated they save engineers time

00:03:05,040 --> 00:03:10,030
because engineers don't have to monitor

00:03:07,660 --> 00:03:13,180
logs or watch charts as often as they

00:03:10,030 --> 00:03:14,760
used to so we needed the ability to

00:03:13,180 --> 00:03:17,230
canary at Nike

00:03:14,760 --> 00:03:21,030
now Justin will walk you through our

00:03:17,230 --> 00:03:21,030
journey to find a canary solution

00:03:22,569 --> 00:03:25,989
all right so we knew we wanted to the

00:03:24,280 --> 00:03:28,540
first glass canary experience for

00:03:25,989 --> 00:03:30,489
continuous delivery solution in that

00:03:28,540 --> 00:03:32,980
search for a canary solution we faced

00:03:30,489 --> 00:03:35,019
the classic bill versus buy scenario we

00:03:32,980 --> 00:03:36,579
had an internal solution but we knew it

00:03:35,019 --> 00:03:38,140
was gonna need major work to get the

00:03:36,579 --> 00:03:40,870
user experience to be where we wanted it

00:03:38,140 --> 00:03:43,599
to be we could spend the time and energy

00:03:40,870 --> 00:03:45,310
to uplift that internal solution but

00:03:43,599 --> 00:03:48,030
because of our belief in open source

00:03:45,310 --> 00:03:50,620
software and the benefits from cross

00:03:48,030 --> 00:03:52,120
company collaboration we looked into the

00:03:50,620 --> 00:03:55,180
open source space to see what was going

00:03:52,120 --> 00:03:57,549
on our search led us to Spinnaker's

00:03:55,180 --> 00:03:59,519
Kayenta spinnaker as you probably know

00:03:57,549 --> 00:04:02,500
by now is an open source multi-cloud

00:03:59,519 --> 00:04:04,569
continuous delivery system spinnaker is

00:04:02,500 --> 00:04:07,120
composed of many micro services that

00:04:04,569 --> 00:04:11,500
work together one of those micro

00:04:07,120 --> 00:04:14,409
services is canta canta is a platform

00:04:11,500 --> 00:04:16,209
for canary analysis it's capable of

00:04:14,409 --> 00:04:17,560
integrating many metrics sources in

00:04:16,209 --> 00:04:20,349
doing AV style

00:04:17,560 --> 00:04:22,180
statistical testing we identified it as

00:04:20,349 --> 00:04:25,030
an industry standard for canary analysis

00:04:22,180 --> 00:04:26,440
having the benefit from the work of data

00:04:25,030 --> 00:04:29,260
scientists from multiple companies

00:04:26,440 --> 00:04:31,419
across the industry this was a great

00:04:29,260 --> 00:04:35,199
opportunity for us to align on the work

00:04:31,419 --> 00:04:37,060
of the open source community so we found

00:04:35,199 --> 00:04:39,639
ourselves at a third option collaborate

00:04:37,060 --> 00:04:41,530
so we couldn't use the complete canary

00:04:39,639 --> 00:04:44,229
experience that spinnaker offered as a

00:04:41,530 --> 00:04:46,690
whole so we just wanted to use cayenne

00:04:44,229 --> 00:04:48,580
to as a standalone service canta did

00:04:46,690 --> 00:04:50,440
almost everything we wanted and we

00:04:48,580 --> 00:04:51,880
really liked the user experience that

00:04:50,440 --> 00:04:52,349
spinnaker offers for carrying out of

00:04:51,880 --> 00:04:55,330
Hawkes

00:04:52,349 --> 00:04:57,970
so we decided we wanted to extend

00:04:55,330 --> 00:04:59,590
Kayenta to meet our needs and contribute

00:04:57,970 --> 00:05:01,990
any of that work upstream so that others

00:04:59,590 --> 00:05:03,250
could benefit from it too we were at

00:05:01,990 --> 00:05:04,720
last year's spinnaker summit as

00:05:03,250 --> 00:05:06,880
attendees and we had some early

00:05:04,720 --> 00:05:08,500
conversations with the community about

00:05:06,880 --> 00:05:10,690
using account as a standalone service

00:05:08,500 --> 00:05:14,710
and we're here to we're here a year

00:05:10,690 --> 00:05:15,849
later presenting so what did we come

00:05:14,710 --> 00:05:18,759
here to tell you today

00:05:15,849 --> 00:05:20,979
canary use canary deployments when

00:05:18,759 --> 00:05:23,470
making changes to production make your

00:05:20,979 --> 00:05:25,570
life easier as a developer spend less

00:05:23,470 --> 00:05:28,270
time manually watching graphs and logs

00:05:25,570 --> 00:05:30,250
which are prone to human error prevent

00:05:28,270 --> 00:05:33,550
large-scale production issues use canary

00:05:30,250 --> 00:05:35,740
deployments so you can canary two

00:05:33,550 --> 00:05:37,960
different ways you can canary now with

00:05:35,740 --> 00:05:39,820
which has great first-class support out

00:05:37,960 --> 00:05:41,770
of the box you can also cut

00:05:39,820 --> 00:05:43,990
you can also canary using chi anta as a

00:05:41,770 --> 00:05:45,940
standalone service which is the process

00:05:43,990 --> 00:05:48,940
that we helped enable and the process

00:05:45,940 --> 00:05:50,740
we're going to share with you today so

00:05:48,940 --> 00:05:53,020
which path should you take to canary

00:05:50,740 --> 00:05:54,370
stand-alone Kayenta or just using the

00:05:53,020 --> 00:05:56,500
out-of-the-box features with the

00:05:54,370 --> 00:05:57,910
spinnaker there's a spectrum of

00:05:56,500 --> 00:06:00,100
scenarios where it's going to make sense

00:05:57,910 --> 00:06:01,450
to use kind of stand-alone service to

00:06:00,100 --> 00:06:03,250
where you'd be much better off just

00:06:01,450 --> 00:06:06,700
using the automated canary analysis

00:06:03,250 --> 00:06:09,100
stage and spinnaker the first scenario

00:06:06,700 --> 00:06:12,100
is when you have an organization that is

00:06:09,100 --> 00:06:14,200
is not using or planning to use

00:06:12,100 --> 00:06:16,090
spinnaker this group of people is gonna

00:06:14,200 --> 00:06:19,570
get the most value out of just using Chi

00:06:16,090 --> 00:06:21,490
into Sam alone the second scenario is

00:06:19,570 --> 00:06:23,020
where you have adopted spinnaker but

00:06:21,490 --> 00:06:23,800
you're not gonna adopt it for all of

00:06:23,020 --> 00:06:26,470
your frameworks

00:06:23,800 --> 00:06:27,970
slash infrastructure for example you're

00:06:26,470 --> 00:06:29,500
gonna use it for virtual machine

00:06:27,970 --> 00:06:32,080
instances but you're not going to use it

00:06:29,500 --> 00:06:33,160
for service or containers this group of

00:06:32,080 --> 00:06:35,940
people is still going to get a lot of

00:06:33,160 --> 00:06:38,020
value for music Sam alone okay in tech

00:06:35,940 --> 00:06:40,480
the third scenario is when you're

00:06:38,020 --> 00:06:41,830
transitioning to spinnaker and you still

00:06:40,480 --> 00:06:44,110
have a bunch of legacy deployment

00:06:41,830 --> 00:06:45,820
processes lying around so depending on

00:06:44,110 --> 00:06:47,860
your transition timelines

00:06:45,820 --> 00:06:49,120
it could make sense to use sam'l encanta

00:06:47,860 --> 00:06:50,620
but you're probably going to be better

00:06:49,120 --> 00:06:54,160
off using the canary features that's

00:06:50,620 --> 00:06:55,870
been occurring the final scenario is

00:06:54,160 --> 00:06:57,610
where you've gone all-in on spinnaker in

00:06:55,870 --> 00:06:59,440
this situation it's not going to make

00:06:57,610 --> 00:07:00,550
sense to use thermal encanta at all

00:06:59,440 --> 00:07:02,320
you're going to be much better off just

00:07:00,550 --> 00:07:04,650
using the canary features in spinnaker

00:07:02,320 --> 00:07:04,650
pipelines

00:07:05,660 --> 00:07:10,690
so let's drill into standalone Panta

00:07:08,630 --> 00:07:13,460
we're gonna talk about three things

00:07:10,690 --> 00:07:17,600
theory how do canary deployments work

00:07:13,460 --> 00:07:19,310
with Kayenta implementation how does one

00:07:17,600 --> 00:07:22,030
implement canary analysis with

00:07:19,310 --> 00:07:24,170
standalone client effort Enterprise

00:07:22,030 --> 00:07:25,700
experience when theory and

00:07:24,170 --> 00:07:31,010
implementation is combined

00:07:25,700 --> 00:07:33,200
what is this unlock for your users let's

00:07:31,010 --> 00:07:35,450
dive in kai anta is a platform for

00:07:33,200 --> 00:07:37,850
canary analysis and it ultimately judges

00:07:35,450 --> 00:07:39,290
whether a canary is good or bad so let's

00:07:37,850 --> 00:07:43,430
see how a canary deployment works with

00:07:39,290 --> 00:07:44,870
Kayenta before we can even start to

00:07:43,430 --> 00:07:46,670
think about canary deployments though

00:07:44,870 --> 00:07:48,140
we're gonna need to be able to find what

00:07:46,670 --> 00:07:50,480
makes our application healthy and

00:07:48,140 --> 00:07:52,790
measure and report it it is important to

00:07:50,480 --> 00:07:54,470
define metrics web metrics matter for

00:07:52,790 --> 00:07:56,930
your service as this is the foundation

00:07:54,470 --> 00:07:59,690
for canary configuration that Kayenta

00:07:56,930 --> 00:08:01,430
uses to judge the help of Canaries when

00:07:59,690 --> 00:08:03,620
creating canary configuration the

00:08:01,430 --> 00:08:05,150
spinnaker canary Guide recommends that

00:08:03,620 --> 00:08:09,470
you use three out of four of the golden

00:08:05,150 --> 00:08:12,530
signals of monitoring those signals are

00:08:09,470 --> 00:08:15,170
saturation these group of metrics inform

00:08:12,530 --> 00:08:17,870
how full your service are is they are

00:08:15,170 --> 00:08:20,840
metrics such as CPU memory and disk

00:08:17,870 --> 00:08:22,640
usage then you have latency these groups

00:08:20,840 --> 00:08:24,410
of metrics inform you know how long your

00:08:22,640 --> 00:08:27,170
service is taken to fulfill requests and

00:08:24,410 --> 00:08:30,280
then you have errors these tell you at

00:08:27,170 --> 00:08:32,719
the rate of which requests are failing

00:08:30,280 --> 00:08:34,729
okay now we can start thinking about

00:08:32,719 --> 00:08:36,410
canary deployments let's take a look at

00:08:34,729 --> 00:08:38,390
the state of our application before a

00:08:36,410 --> 00:08:39,890
canary deployment we have traffic being

00:08:38,390 --> 00:08:42,349
routed to our production server group

00:08:39,890 --> 00:08:44,120
all the instances in that production

00:08:42,349 --> 00:08:46,220
server group are reporting their metrics

00:08:44,120 --> 00:08:51,230
to some metrics score weather signal

00:08:46,220 --> 00:08:53,960
effects New Relic or Prometheus phase

00:08:51,230 --> 00:08:54,500
one is where we deploy the baseline in

00:08:53,960 --> 00:08:56,600
canary

00:08:54,500 --> 00:08:59,000
groups this is where we let a small

00:08:56,600 --> 00:09:00,920
percentage of traffic flow to our

00:08:59,000 --> 00:09:03,530
experiment and see if our little canary

00:09:00,920 --> 00:09:08,180
lives or dies but what does this mean

00:09:03,530 --> 00:09:10,400
what is the baseline in canary the

00:09:08,180 --> 00:09:12,170
baseline is a fresh version of what is

00:09:10,400 --> 00:09:14,480
currently deployed to production this

00:09:12,170 --> 00:09:17,360
can be a baked a me or a docker image

00:09:14,480 --> 00:09:19,430
it's some sort of deployable artifact we

00:09:17,360 --> 00:09:21,470
do this to eliminate cold boot issues

00:09:19,430 --> 00:09:23,360
such as cache warming and help make

00:09:21,470 --> 00:09:25,760
metrics between the baseline and Canaria

00:09:23,360 --> 00:09:27,770
equivalent whereas the canary is the

00:09:25,760 --> 00:09:30,140
noodle oil artifact so as you can see

00:09:27,770 --> 00:09:31,850
above here the baseline and the

00:09:30,140 --> 00:09:33,920
production is running the old version

00:09:31,850 --> 00:09:35,300
one point two point one and then we when

00:09:33,920 --> 00:09:38,960
we do our canary we're gonna launch one

00:09:35,300 --> 00:09:40,760
point three point O in this situation we

00:09:38,960 --> 00:09:42,110
have multiple server groups deployed

00:09:40,760 --> 00:09:44,780
with the same application version

00:09:42,110 --> 00:09:47,030
running so we can't just use the version

00:09:44,780 --> 00:09:49,190
to differentiate between the production

00:09:47,030 --> 00:09:50,780
group and the baseline we need to make

00:09:49,190 --> 00:09:52,700
sure we're sending metadata with our

00:09:50,780 --> 00:09:56,270
metrics that includes a unique ID for

00:09:52,700 --> 00:09:58,280
each server group this is so that the

00:09:56,270 --> 00:10:01,810
metrics for the baseline can be queried

00:09:58,280 --> 00:10:01,810
separately from the current production

00:10:02,410 --> 00:10:07,520
the baseline in the canary also needs to

00:10:05,630 --> 00:10:08,930
receive the same amount of traffic for

00:10:07,520 --> 00:10:11,360
certain types of metrics to be

00:10:08,930 --> 00:10:13,430
equivalent metrics such as air counts

00:10:11,360 --> 00:10:15,170
would be meaningless in scenarios where

00:10:13,430 --> 00:10:18,050
the volume of requests are different for

00:10:15,170 --> 00:10:20,210
the canary and the baseline you could

00:10:18,050 --> 00:10:22,640
maybe convert these metrics to rates but

00:10:20,210 --> 00:10:24,620
then saturation style metrics CPU usage

00:10:22,640 --> 00:10:26,990
memory usage they start to get weird

00:10:24,620 --> 00:10:28,310
when you make them rates as well so

00:10:26,990 --> 00:10:32,300
let's take a look at two routing

00:10:28,310 --> 00:10:34,580
strategies the first routing strategy is

00:10:32,300 --> 00:10:37,130
size based routing we sometimes refer to

00:10:34,580 --> 00:10:38,750
this as coarse-grained routing since the

00:10:37,130 --> 00:10:41,150
weights are controlled by instance

00:10:38,750 --> 00:10:42,590
counts the strategy is the most common

00:10:41,150 --> 00:10:44,990
one we've come across because it's the

00:10:42,590 --> 00:10:46,700
simplest the strategy is simply to

00:10:44,990 --> 00:10:49,310
round-robin request through some sort of

00:10:46,700 --> 00:10:51,620
load balancer or on the client in the

00:10:49,310 --> 00:10:54,380
case of service discovery so as you can

00:10:51,620 --> 00:10:55,700
see in this diagram 80% of the incoming

00:10:54,380 --> 00:10:57,560
traffic is going to be flowing through

00:10:55,700 --> 00:10:58,850
the production server group this is

00:10:57,560 --> 00:11:01,040
because it has eight out of ten of the

00:10:58,850 --> 00:11:03,020
instances and then we have one instance

00:11:01,040 --> 00:11:04,460
for the canary in one instance for the

00:11:03,020 --> 00:11:04,920
baseline so those are going to get 10%

00:11:04,460 --> 00:11:09,240
of

00:11:04,920 --> 00:11:11,610
to each the second option is going to be

00:11:09,240 --> 00:11:13,079
some sort of flying green weighted

00:11:11,610 --> 00:11:15,149
routing so this strategy relies on

00:11:13,079 --> 00:11:17,220
something smarter then round rounding

00:11:15,149 --> 00:11:19,709
it's going to allow you to assess

00:11:17,220 --> 00:11:22,889
specific percentages to your canary and

00:11:19,709 --> 00:11:24,600
your baseline so Amazon Web Services API

00:11:22,889 --> 00:11:26,730
gateway has features around this for

00:11:24,600 --> 00:11:28,829
example but this involves a more

00:11:26,730 --> 00:11:33,930
complicated infrastructure set up in the

00:11:28,829 --> 00:11:36,060
deployment process all right now phase

00:11:33,930 --> 00:11:39,269
two we have our infrastructure set up

00:11:36,060 --> 00:11:40,740
we've deployed a baseline and canary the

00:11:39,269 --> 00:11:42,870
baseline and canary are receiving equal

00:11:40,740 --> 00:11:44,910
amounts of traffic the baseline and

00:11:42,870 --> 00:11:47,070
canary are reporting their metrics with

00:11:44,910 --> 00:11:49,139
metadata that allow for their metrics to

00:11:47,070 --> 00:11:51,000
be queried independently we can actually

00:11:49,139 --> 00:11:55,380
start the canary phase of the deployment

00:11:51,000 --> 00:11:57,360
process now the first step of this phase

00:11:55,380 --> 00:11:59,579
is to wait and allow traffic to flow

00:11:57,360 --> 00:12:02,880
through the system and delet metrics

00:11:59,579 --> 00:12:05,100
flow through the system as well after

00:12:02,880 --> 00:12:07,199
some amount of time we can query the

00:12:05,100 --> 00:12:09,360
metrics source for the baseline in

00:12:07,199 --> 00:12:12,390
canary and do a statistical comparison

00:12:09,360 --> 00:12:14,690
to see if the canary metrics has changed

00:12:12,390 --> 00:12:14,690
for the worse

00:12:16,470 --> 00:12:22,110
at the metrics for the canary are bad we

00:12:19,500 --> 00:12:24,740
can fail fast and exit the canary

00:12:22,110 --> 00:12:27,180
analysis phase with a fail status

00:12:24,740 --> 00:12:28,470
however if the metrics with it for the

00:12:27,180 --> 00:12:30,480
canary are within a configurable

00:12:28,470 --> 00:12:32,790
threshold we can continue into another

00:12:30,480 --> 00:12:34,830
wait and judge cycle and repeat this

00:12:32,790 --> 00:12:38,970
process for n intervals over some

00:12:34,830 --> 00:12:41,010
lifetime at the end of this process we

00:12:38,970 --> 00:12:43,440
make one last judgment and produce a

00:12:41,010 --> 00:12:45,720
pass for fail result this can then be

00:12:43,440 --> 00:12:47,340
reacted downstream whether that's

00:12:45,720 --> 00:12:49,350
automatically rolling forward with your

00:12:47,340 --> 00:12:51,930
new version or a new application version

00:12:49,350 --> 00:12:53,760
or rolling back or even stopping and

00:12:51,930 --> 00:12:58,740
allowing for some sort of manual user

00:12:53,760 --> 00:13:01,500
decision so let's recap how canary

00:12:58,740 --> 00:13:02,970
deployments work in Kayenta we have

00:13:01,500 --> 00:13:05,280
surround our app to report the golden

00:13:02,970 --> 00:13:08,490
signals so we could use those later to

00:13:05,280 --> 00:13:10,710
judge the health of a canary we do we

00:13:08,490 --> 00:13:13,080
deploy the baseline and canary alongside

00:13:10,710 --> 00:13:15,750
the existing production group we do

00:13:13,080 --> 00:13:17,790
canary analysis over some lifetime and

00:13:15,750 --> 00:13:20,310
perform multiple judgments failing fast

00:13:17,790 --> 00:13:22,320
if we need to and then finally we get an

00:13:20,310 --> 00:13:25,860
aggregated judgment result and we react

00:13:22,320 --> 00:13:27,930
it out to the results all right milena

00:13:25,860 --> 00:13:30,900
all this theory is great but how do you

00:13:27,930 --> 00:13:35,460
get it to work well it is just a simple

00:13:30,900 --> 00:13:37,200
matter of programming let's talk about

00:13:35,460 --> 00:13:39,570
what you can use to canary with

00:13:37,200 --> 00:13:43,350
standalone Kayenta and then what you'll

00:13:39,570 --> 00:13:46,280
have to build so what can you use in

00:13:43,350 --> 00:13:48,180
terms of open source software - canary

00:13:46,280 --> 00:13:50,520
we build referee

00:13:48,180 --> 00:13:53,040
an open source UI for viewing reports

00:13:50,520 --> 00:13:56,370
and using developer productivity tools

00:13:53,040 --> 00:13:58,259
cool logo we know

00:13:56,370 --> 00:14:00,839
we hadn't realized we needed an

00:13:58,259 --> 00:14:02,790
additional layer for our customers to be

00:14:00,839 --> 00:14:05,670
able to process the results from Kayenta

00:14:02,790 --> 00:14:08,100
we try to get deck Spinnaker's UI

00:14:05,670 --> 00:14:10,110
working for just Kayenta but it is too

00:14:08,100 --> 00:14:12,480
tightly coupled with this complete

00:14:10,110 --> 00:14:15,059
spinnaker solution so we built our own

00:14:12,480 --> 00:14:15,899
UI that's just for kai anta throughout

00:14:15,059 --> 00:14:17,519
the whole process

00:14:15,899 --> 00:14:19,980
we partnered with members of the

00:14:17,519 --> 00:14:22,290
spinnaker UI community and engaged and

00:14:19,980 --> 00:14:24,209
collaborated on eye designs and

00:14:22,290 --> 00:14:25,379
implementation ideas with them we

00:14:24,209 --> 00:14:30,180
couldn't have done this work without

00:14:25,379 --> 00:14:32,670
their support breviary includes a canary

00:14:30,180 --> 00:14:35,430
configuration generator tool to enable

00:14:32,670 --> 00:14:36,809
teams to build their canary config a lot

00:14:35,430 --> 00:14:39,749
of this is based off of the user

00:14:36,809 --> 00:14:43,860
experience that is in the spinnaker deck

00:14:39,749 --> 00:14:46,079
UI as you can see in this canary config

00:14:43,860 --> 00:14:48,689
some of the golden signals of latency

00:14:46,079 --> 00:14:52,290
and error counts are used as metrics to

00:14:48,689 --> 00:14:54,629
indicate the health of the service then

00:14:52,290 --> 00:14:57,389
there is the retrospective analysis tool

00:14:54,629 --> 00:14:59,870
we build this tool to enable teams to

00:14:57,389 --> 00:15:02,040
rapidly iterate on their canary configs

00:14:59,870 --> 00:15:04,499
retrospective analysis is a way to

00:15:02,040 --> 00:15:07,170
conduct canary analysis on historical

00:15:04,499 --> 00:15:09,389
events or data it can be used to speed

00:15:07,170 --> 00:15:11,429
up testing so when users test small

00:15:09,389 --> 00:15:13,889
changes in their can say they don't have

00:15:11,429 --> 00:15:17,189
to wait for a full canary run every time

00:15:13,889 --> 00:15:20,129
in this example we can see the scope is

00:15:17,189 --> 00:15:21,480
two different server groups one is the

00:15:20,129 --> 00:15:25,379
baseline and

00:15:21,480 --> 00:15:27,209
is the canary then we built a report

00:15:25,379 --> 00:15:29,850
viewer for our users to see their canary

00:15:27,209 --> 00:15:31,740
results in this example we can tell from

00:15:29,850 --> 00:15:33,869
the green along the left-hand side of

00:15:31,740 --> 00:15:37,800
the screen that the canary passed and

00:15:33,869 --> 00:15:40,019
all the metrics look healthy finally we

00:15:37,800 --> 00:15:42,509
also have an internal documentation site

00:15:40,019 --> 00:15:44,970
in referee so you can host off specific

00:15:42,509 --> 00:15:47,730
to your organization in the same place

00:15:44,970 --> 00:15:50,670
as the other existing tools in addition

00:15:47,730 --> 00:15:53,279
to this custom kaientai user interface

00:15:50,670 --> 00:15:57,230
we made some contributions to client

00:15:53,279 --> 00:15:57,230
itself that Justin will walk you through

00:15:58,160 --> 00:16:03,149
so our engineering teams use signal

00:16:01,259 --> 00:16:04,529
effects to monitor and report those

00:16:03,149 --> 00:16:06,749
golden signals we talked about earlier

00:16:04,529 --> 00:16:09,360
so when we first started Cayenne Ted

00:16:06,749 --> 00:16:12,300
didn't have signal effects support so we

00:16:09,360 --> 00:16:15,149
added that we also added signal signal

00:16:12,300 --> 00:16:16,499
effects support in Dec Spinnaker's UI so

00:16:15,149 --> 00:16:18,480
that the open source community can use

00:16:16,499 --> 00:16:20,009
signal effects as well and then lastly

00:16:18,480 --> 00:16:22,350
we added signal effects support to

00:16:20,009 --> 00:16:25,290
halyard the CLI for managing spinnaker

00:16:22,350 --> 00:16:26,879
so that the community can configure

00:16:25,290 --> 00:16:30,240
signal effects when managing there's

00:16:26,879 --> 00:16:32,730
been appear instances our engineering

00:16:30,240 --> 00:16:34,350
teams also using new relic and it was on

00:16:32,730 --> 00:16:36,209
our list of metric sources to contribute

00:16:34,350 --> 00:16:38,220
upstream so when we saw that the

00:16:36,209 --> 00:16:40,319
community opened a PR for the initial

00:16:38,220 --> 00:16:42,119
integration of New Relic we hopped on

00:16:40,319 --> 00:16:44,129
that PR to help review it and we

00:16:42,119 --> 00:16:48,240
contributed some additional features to

00:16:44,129 --> 00:16:50,939
that user experience in Kayenta and then

00:16:48,240 --> 00:16:53,100
we did our initial canary research with

00:16:50,939 --> 00:16:54,959
spinnaker and we liked that we liked the

00:16:53,100 --> 00:16:57,420
user experience that that automated

00:16:54,959 --> 00:16:59,040
canary analysis stage offered that stage

00:16:57,420 --> 00:17:01,230
is a user experience that has spread

00:16:59,040 --> 00:17:03,389
across multiple micro-services so we

00:17:01,230 --> 00:17:05,159
created a new endpoint in Kayenta that

00:17:03,389 --> 00:17:07,579
consolidates that user experience into a

00:17:05,159 --> 00:17:10,319
single API endpoint that users can call

00:17:07,579 --> 00:17:12,419
so if you recall our phase 2 process

00:17:10,319 --> 00:17:14,730
chart for how canary deployments work

00:17:12,419 --> 00:17:15,730
with canta canta x' endpoint originally

00:17:14,730 --> 00:17:17,980
just had an endpoint

00:17:15,730 --> 00:17:21,069
enables the fetch and judge portion of

00:17:17,980 --> 00:17:23,110
that face to and then the rest of that

00:17:21,069 --> 00:17:24,909
phase to logic was in another micro

00:17:23,110 --> 00:17:27,730
service called Orca that gets triggered

00:17:24,909 --> 00:17:29,830
the spinnaker pipelines but then with

00:17:27,730 --> 00:17:31,809
the with the new standalone endpoint you

00:17:29,830 --> 00:17:34,720
can do that aggregated analysis over

00:17:31,809 --> 00:17:37,149
multiple iterations just using the Kyoto

00:17:34,720 --> 00:17:39,669
API we had originally implemented this

00:17:37,149 --> 00:17:41,019
as an internal Cayenne to plugin but we

00:17:39,669 --> 00:17:43,059
noticed a lot of people on the spinnaker

00:17:41,019 --> 00:17:45,970
slack and a github asking how they could

00:17:43,059 --> 00:17:49,059
do this so we made a PR and your tribute

00:17:45,970 --> 00:17:51,070
it as an official end point in kinda so

00:17:49,059 --> 00:17:52,450
this covers our open source

00:17:51,070 --> 00:17:54,039
contributions for the most part and

00:17:52,450 --> 00:17:55,960
Melina's gonna walk you through what you

00:17:54,039 --> 00:18:02,200
have to build to integrate all this in

00:17:55,960 --> 00:18:04,659
practice using Kayenta is not as simple

00:18:02,200 --> 00:18:06,820
as just making an API call if you are

00:18:04,659 --> 00:18:08,919
using the spinnaker to connect to canary

00:18:06,820 --> 00:18:11,799
there's a lot of integration work built

00:18:08,919 --> 00:18:14,110
in for you to use however if you're

00:18:11,799 --> 00:18:15,580
using standalone Kayenta there are some

00:18:14,110 --> 00:18:19,330
technical challenges that you will have

00:18:15,580 --> 00:18:21,340
to figure out on the enterprise level to

00:18:19,330 --> 00:18:23,500
use standalone Kayenta you will have to

00:18:21,340 --> 00:18:25,450
stand up infrastructure to orchestrate

00:18:23,500 --> 00:18:27,970
Canaria deployments shown here by the

00:18:25,450 --> 00:18:29,679
phase one diagram at Nike

00:18:27,970 --> 00:18:32,529
we made a pipeline that stands up the

00:18:29,679 --> 00:18:34,929
infrastructure calls and pulls the

00:18:32,529 --> 00:18:38,289
standalone endpoint and then generates a

00:18:34,929 --> 00:18:40,510
report from the canary results after the

00:18:38,289 --> 00:18:42,880
canary judgment has been made if the

00:18:40,510 --> 00:18:45,250
canary passed we want it to be deployed

00:18:42,880 --> 00:18:47,889
to production and if the canary failed

00:18:45,250 --> 00:18:50,080
we want it to be rolled back so we also

00:18:47,889 --> 00:18:51,029
orchestrated the process for promotions

00:18:50,080 --> 00:18:54,519
and rollbacks

00:18:51,029 --> 00:18:55,809
we made manual and automatic modes for

00:18:54,519 --> 00:18:58,090
the promotions and rollbacks

00:18:55,809 --> 00:19:01,029
to enable our users to do what works for

00:18:58,090 --> 00:19:03,130
them in automatic mode the canary will

00:19:01,029 --> 00:19:06,490
be automatically promoted if it passed

00:19:03,130 --> 00:19:08,769
and rolled back if it failed in manual

00:19:06,490 --> 00:19:11,200
mode the user will be prompted to make a

00:19:08,769 --> 00:19:13,510
final decision and determine that

00:19:11,200 --> 00:19:13,980
promotion or rollback regardless of

00:19:13,510 --> 00:19:17,850
whether

00:19:13,980 --> 00:19:19,470
marry pastor failed you will also have

00:19:17,850 --> 00:19:22,289
to update your foundation metrics

00:19:19,470 --> 00:19:26,370
configuration so it reports server group

00:19:22,289 --> 00:19:28,679
and location metadata unlike e we

00:19:26,370 --> 00:19:30,059
additionally wired up notifications so

00:19:28,679 --> 00:19:32,309
that users are notified when their

00:19:30,059 --> 00:19:34,350
canary is completely we did this by

00:19:32,309 --> 00:19:38,880
implementing an event listener in a

00:19:34,350 --> 00:19:40,370
custom kind of plugin to recap this is

00:19:38,880 --> 00:19:44,010
what is available for you to use

00:19:40,370 --> 00:19:47,370
standalone Kayenta referee and metrics

00:19:44,010 --> 00:19:50,039
source integrations and this is what you

00:19:47,370 --> 00:19:52,019
will have to build you have to set up

00:19:50,039 --> 00:19:54,570
your orchestration process which is

00:19:52,019 --> 00:19:58,139
phase one you will have to update your

00:19:54,570 --> 00:20:02,279
metrics configuration and optionally you

00:19:58,139 --> 00:20:04,799
can wire up notifications we moved

00:20:02,279 --> 00:20:07,950
through theory and implementation now

00:20:04,799 --> 00:20:09,990
experience bol Canarian with standalone

00:20:07,950 --> 00:20:13,679
hi anta actually look like for your

00:20:09,990 --> 00:20:16,230
engineers let's just show you our own

00:20:13,679 --> 00:20:18,299
example so working on canoeing and

00:20:16,230 --> 00:20:20,669
referee isn't actually just in a nice

00:20:18,299 --> 00:20:23,340
day job we work on another product

00:20:20,669 --> 00:20:26,210
called Cerberus an open-source secure

00:20:23,340 --> 00:20:29,970
property store for cloud applications

00:20:26,210 --> 00:20:32,340
shameless plug check us out we wanted to

00:20:29,970 --> 00:20:34,110
use canary analysis with Cerberus so

00:20:32,340 --> 00:20:36,600
while we're developing the whole process

00:20:34,110 --> 00:20:38,549
of standalone cayenne toe with referee

00:20:36,600 --> 00:20:41,490
we are actually wiring it up the whole

00:20:38,549 --> 00:20:44,159
time of Cerberus so we currently use our

00:20:41,490 --> 00:20:48,389
entire canary analysis process in

00:20:44,159 --> 00:20:51,360
production with Cerberus so what do we

00:20:48,389 --> 00:20:54,269
do in order to canary we had an

00:20:51,360 --> 00:20:57,149
instrument our metrics integrate into

00:20:54,269 --> 00:20:59,970
our pipelines create our canary config

00:20:57,149 --> 00:21:03,029
and test it and then we were able to run

00:20:59,970 --> 00:21:04,440
canary dove women's so we have a canary

00:21:03,029 --> 00:21:07,710
deployments working for a couple of

00:21:04,440 --> 00:21:10,620
months but then one day we got a slack

00:21:07,710 --> 00:21:12,149
notification our canary failed and we

00:21:10,620 --> 00:21:12,980
are confused because we had made a

00:21:12,149 --> 00:21:17,090
fairly and

00:21:12,980 --> 00:21:18,680
he was code change now if this is

00:21:17,090 --> 00:21:20,990
starting to sound familiar you are

00:21:18,680 --> 00:21:22,850
absolutely correct this is the exact

00:21:20,990 --> 00:21:25,490
same situation that we outlined in the

00:21:22,850 --> 00:21:29,690
beginning of the presentation because it

00:21:25,490 --> 00:21:32,780
actually happened to us but this time we

00:21:29,690 --> 00:21:35,380
had canary so we clicked on the report

00:21:32,780 --> 00:21:38,000
and we investigated the failed canary

00:21:35,380 --> 00:21:40,550
the canary appeared to have a memory

00:21:38,000 --> 00:21:43,070
weak as we can see here from this graph

00:21:40,550 --> 00:21:45,320
on memory usage where the canary is

00:21:43,070 --> 00:21:48,500
represented by the orange line spiking

00:21:45,320 --> 00:21:50,990
across your screen so we thought our

00:21:48,500 --> 00:21:53,630
memory was going out of control but we

00:21:50,990 --> 00:21:55,610
had no idea why the code change we made

00:21:53,630 --> 00:21:59,420
was minor and it shouldn't have affected

00:21:55,610 --> 00:22:01,700
memory weary baked a new a me with what

00:21:59,420 --> 00:22:04,160
was last working in production and we

00:22:01,700 --> 00:22:05,930
did another canary deployment expecting

00:22:04,160 --> 00:22:07,730
the canary to pass because this was

00:22:05,930 --> 00:22:10,460
being done on code that had already been

00:22:07,730 --> 00:22:13,790
working in production but the canary

00:22:10,460 --> 00:22:16,160
failed again what was going on we

00:22:13,790 --> 00:22:17,870
hypothesized that something external had

00:22:16,160 --> 00:22:20,270
changed so we worked with our

00:22:17,870 --> 00:22:23,600
foundational infrastructure team to do a

00:22:20,270 --> 00:22:25,940
root cause analysis after digging deeper

00:22:23,600 --> 00:22:29,330
we realized that a middleware agent on

00:22:25,940 --> 00:22:31,580
the system had had a security update so

00:22:29,330 --> 00:22:34,640
our team rolled that change back and we

00:22:31,580 --> 00:22:37,700
baked a new an e with the old agent but

00:22:34,640 --> 00:22:40,100
the new code we were you ran the canary

00:22:37,700 --> 00:22:44,090
and the canary past so we finally could

00:22:40,100 --> 00:22:46,400
deploy canary prevented a potential

00:22:44,090 --> 00:22:48,740
production outage because we used

00:22:46,400 --> 00:22:50,840
standalone canta and referee this change

00:22:48,740 --> 00:22:53,210
never fully went out to production and

00:22:50,840 --> 00:22:56,060
we avoided a large-scale memory leak

00:22:53,210 --> 00:22:58,250
many teams use Cerberus and if we have

00:22:56,060 --> 00:23:01,300
an outage that has ripple effects for

00:22:58,250 --> 00:23:04,600
all of the services that depend on us

00:23:01,300 --> 00:23:07,160
this was our story but it could be yours

00:23:04,600 --> 00:23:10,040
make your life easier as a developer

00:23:07,160 --> 00:23:13,130
spend less time manually watching graphs

00:23:10,040 --> 00:23:17,570
and logs prevent large-scale production

00:23:13,130 --> 00:23:19,100
issues canary justin is going to walk

00:23:17,570 --> 00:23:21,250
you through how you can get started

00:23:19,100 --> 00:23:21,250
today

00:23:22,070 --> 00:23:28,110
all right so what can you do today you

00:23:26,400 --> 00:23:29,790
can you can start canary today list

00:23:28,110 --> 00:23:32,220
vinegar because it has first-class

00:23:29,790 --> 00:23:33,990
features right other box if you want to

00:23:32,220 --> 00:23:36,480
use standalone Kayenta we've documented

00:23:33,990 --> 00:23:38,430
a lot of what we discuss today and with

00:23:36,480 --> 00:23:40,290
more technical details as marked down

00:23:38,430 --> 00:23:40,920
documentation in the Cayenne 2 project

00:23:40,290 --> 00:23:43,080
itself

00:23:40,920 --> 00:23:45,060
check out the referee project if you're

00:23:43,080 --> 00:23:48,270
interested in a UI that was made just

00:23:45,060 --> 00:23:50,100
for Kayenta and then join us on the

00:23:48,270 --> 00:23:52,410
Cayenne 2 channel in the spinnaker slack

00:23:50,100 --> 00:23:53,730
we're very active there and I'm

00:23:52,410 --> 00:23:56,160
constantly interacting with people that

00:23:53,730 --> 00:23:57,570
have questions feel free to email us and

00:23:56,160 --> 00:23:59,100
if you liked our story

00:23:57,570 --> 00:24:02,850
Becky's always looking for good

00:23:59,100 --> 00:24:04,310
engineers here are some of the materials

00:24:02,850 --> 00:24:06,540
that we found useful during our journey

00:24:04,310 --> 00:24:08,070
these are all clickable links and we're

00:24:06,540 --> 00:24:10,320
gonna and the deck should be available

00:24:08,070 --> 00:24:14,040
online email us reach out to us and

00:24:10,320 --> 00:24:18,950
slack if it's not alright thank you

00:24:14,040 --> 00:24:18,950
and then who has the first question

00:24:23,510 --> 00:24:27,660
so for those of you who also came we

00:24:25,890 --> 00:24:30,540
want to let you know that in the back we

00:24:27,660 --> 00:24:32,730
have stickers of all the logos that you

00:24:30,540 --> 00:24:34,590
saw today so on your way out please feel

00:24:32,730 --> 00:24:37,800
free to take some stickers of refereeing

00:24:34,590 --> 00:24:41,570
service but you had a question yes yeah

00:24:37,800 --> 00:24:41,570
we stole it because there's two of us

00:24:45,360 --> 00:24:51,090
I'm curious when indicating metrics to

00:24:49,650 --> 00:24:53,940
you to decide whether things are good

00:24:51,090 --> 00:24:55,290
for any microservices land and we're

00:24:53,940 --> 00:24:57,410
testing some service way down at the

00:24:55,290 --> 00:25:00,150
bottom to be fine

00:24:57,410 --> 00:25:02,010
breaks things in other places do you

00:25:00,150 --> 00:25:03,300
kind on a thing when you look at its top

00:25:02,010 --> 00:25:08,220
level metrics of the whole system are

00:25:03,300 --> 00:25:11,220
you looking for services so in our

00:25:08,220 --> 00:25:13,740
personal example of service we are just

00:25:11,220 --> 00:25:15,690
looking at metrics related to to service

00:25:13,740 --> 00:25:17,940
but let's take I was thinking about this

00:25:15,690 --> 00:25:19,920
actually earlier so let's take spinnaker

00:25:17,940 --> 00:25:22,580
outside example so it's a system as

00:25:19,920 --> 00:25:25,470
opposed to many many micro services so

00:25:22,580 --> 00:25:29,250
one of the cool things that kind of

00:25:25,470 --> 00:25:30,540
supports is in my templates see you and

00:25:29,250 --> 00:25:32,250
that's just basically the ability

00:25:30,540 --> 00:25:34,080
whatever query to find what

00:25:32,250 --> 00:25:36,539
so you could potentially eat all a

00:25:34,080 --> 00:25:39,780
satellite how's the number of successful

00:25:36,539 --> 00:25:41,220
ploys decreased or increased and then

00:25:39,780 --> 00:25:42,929
you could look at that like let's say

00:25:41,220 --> 00:25:46,669
you made a change to one Center career

00:25:42,929 --> 00:25:46,669
services you could potentially

00:25:57,950 --> 00:26:03,860
I imagine using retrospective data the

00:26:01,850 --> 00:26:10,040
releases not be applied to their

00:26:03,860 --> 00:26:10,610
configurations how that works yeah no

00:26:10,040 --> 00:26:14,000
problem

00:26:10,610 --> 00:26:16,820
so we sorry

00:26:14,000 --> 00:26:20,330
so the oh we have hi Anza API allows you

00:26:16,820 --> 00:26:22,040
to put through a canary analysis request

00:26:20,330 --> 00:26:23,960
where you can put timestamps that

00:26:22,040 --> 00:26:26,570
happened in the past so it's really

00:26:23,960 --> 00:26:28,010
awesome because like we had said in the

00:26:26,570 --> 00:26:33,080
presentation you can make modifications

00:26:28,010 --> 00:26:35,630
to your canary config and you can keep

00:26:33,080 --> 00:26:38,050
testing it on the same set of data and

00:26:35,630 --> 00:26:40,700
you can iterate really quickly and

00:26:38,050 --> 00:26:43,550
what's also cool about retrospective

00:26:40,700 --> 00:26:47,510
analysis is that if something did go

00:26:43,550 --> 00:26:50,570
wrong in your service you can keep using

00:26:47,510 --> 00:26:53,360
that historical event as a case study

00:26:50,570 --> 00:26:54,800
for yourself okay we know when you went

00:26:53,360 --> 00:26:57,500
through that failure we want to do

00:26:54,800 --> 00:26:59,720
better next time are the changes that

00:26:57,500 --> 00:27:02,390
we've made to our system since then

00:26:59,720 --> 00:27:05,390
would they have alerted us of that

00:27:02,390 --> 00:27:08,450
problem so you can use important crucial

00:27:05,390 --> 00:27:12,380
events in your own software's timeline

00:27:08,450 --> 00:27:15,880
and you're able to check back in and

00:27:12,380 --> 00:27:18,710
maybe another I'm not sure

00:27:15,880 --> 00:27:20,809
different way but let's say I didn't

00:27:18,710 --> 00:27:24,110
marry and it passed but then there was a

00:27:20,809 --> 00:27:26,600
production so obviously it wasn't

00:27:24,110 --> 00:27:28,909
on us not so you then now you use that

00:27:26,600 --> 00:27:31,399
horrible day but and do a retrospective

00:27:28,909 --> 00:27:33,620
analysis and tweak that could be a

00:27:31,399 --> 00:27:36,399
telophase where we fail there that's

00:27:33,620 --> 00:27:39,260
prevented that the government attention

00:27:36,399 --> 00:27:41,240
another rule thing about some of the

00:27:39,260 --> 00:27:43,370
teams in the here is they're using

00:27:41,240 --> 00:27:46,159
retrospective analysis really judge

00:27:43,370 --> 00:27:47,990
performance NC so they all like body

00:27:46,159 --> 00:27:49,909
performance taxed and I don't run any

00:27:47,990 --> 00:27:51,470
game with the same parameters both with

00:27:49,909 --> 00:27:54,260
tweaks and then you can like

00:27:51,470 --> 00:27:56,480
retrospectively analyze that with ship

00:27:54,260 --> 00:27:59,210
you time ago so they were running on the

00:27:56,480 --> 00:28:01,779
same time where you still invisible

00:27:59,210 --> 00:28:01,779
person

00:28:10,010 --> 00:28:14,500
a few more of them

00:28:14,970 --> 00:28:24,950
did did you take the top vinegar mother

00:28:20,030 --> 00:28:26,879
you just expose the venom of a bi spit

00:28:24,950 --> 00:28:30,039
and have y'all

00:28:26,879 --> 00:28:33,970
yeah so we just took open-sourced

00:28:30,039 --> 00:28:36,250
vinegar as it is and we we just run that

00:28:33,970 --> 00:28:38,559
because it's I mean it's a seafood

00:28:36,250 --> 00:28:41,049
market service you can have it produced

00:28:38,559 --> 00:28:42,399
in fact our so we have we have

00:28:41,049 --> 00:28:44,350
confirming that just rugs it that's a

00:28:42,399 --> 00:28:46,690
job when your service on the climate

00:28:44,350 --> 00:28:48,940
zone so we didn't use I just have the

00:28:46,690 --> 00:28:52,450
calendar to initially create the initial

00:28:48,940 --> 00:28:54,549
project okay but dad mom then I just

00:28:52,450 --> 00:28:56,379
like I have that big storm I just

00:28:54,549 --> 00:29:03,629
launched that marker service and then we

00:28:56,379 --> 00:29:03,629
use its API I one behind you in the back

00:29:04,590 --> 00:29:29,279
I can carry analysis services receiving

00:29:20,360 --> 00:29:32,870
traffic so I can perhaps I know then

00:29:29,279 --> 00:29:32,870
like we'll see the problem

00:29:35,430 --> 00:29:47,490
same problem so is there other do yeah a

00:29:44,490 --> 00:29:50,910
little bit I'm not sure maybe persimmons

00:29:47,490 --> 00:29:53,730
and a scolding but we use the static

00:29:50,910 --> 00:29:55,590
traffic so there's this project it's

00:29:53,730 --> 00:29:57,690
open source it's called gallery so we

00:29:55,590 --> 00:29:59,910
actually have a bunch of scenarios

00:29:57,690 --> 00:30:01,530
written that stimulate our users so in

00:29:59,910 --> 00:30:03,420
our test environment it doesn't get a

00:30:01,530 --> 00:30:06,390
lot of traffic but we still like me

00:30:03,420 --> 00:30:08,730
there so while the canary analysis is

00:30:06,390 --> 00:30:11,880
running parallel versus we're injecting

00:30:08,730 --> 00:30:13,050
a bar just like that attracted and we

00:30:11,880 --> 00:30:15,420
actually I was actually a lesson to

00:30:13,050 --> 00:30:17,820
learn because the the thing we showed

00:30:15,420 --> 00:30:20,250
you earlier the only failed production

00:30:17,820 --> 00:30:22,350
it didn't fail in this and then so when

00:30:20,250 --> 00:30:25,440
we went back into synthetic traffic we

00:30:22,350 --> 00:30:27,270
were able to reproduce the linearity so

00:30:25,440 --> 00:30:31,680
the synthetic traffic is our answer

00:30:27,270 --> 00:30:34,200
maybe other people have answers and you

00:30:31,680 --> 00:30:36,060
have asked you know when you know

00:30:34,200 --> 00:30:38,820
questions about how to operate your

00:30:36,060 --> 00:30:40,650
canary and that goes back to what

00:30:38,820 --> 00:30:42,510
justice started the theory section off

00:30:40,650 --> 00:30:44,120
with which was defining what is healthy

00:30:42,510 --> 00:30:46,400
for your service and with Kenny

00:30:44,120 --> 00:30:48,980
it's very important to understand your

00:30:46,400 --> 00:30:50,600
service so your comment on when you get

00:30:48,980 --> 00:30:52,610
three percent or when you get 100

00:30:50,600 --> 00:30:56,240
percent of your traffic all of that

00:30:52,610 --> 00:30:57,650
sometimes it can be institutional

00:30:56,240 --> 00:30:59,740
knowledge about your service that you

00:30:57,650 --> 00:31:02,419
have from just working on it should

00:30:59,740 --> 00:31:06,409
influence how you set up here at canary

00:31:02,419 --> 00:31:08,900
and thinking through when you're gonna

00:31:06,409 --> 00:31:12,169
get hit with peak traffic using

00:31:08,900 --> 00:31:14,570
synthetic traffic is one way to be able

00:31:12,169 --> 00:31:17,270
to simulate that peak traffic and also

00:31:14,570 --> 00:31:18,470
maybe if it's patterned you can make

00:31:17,270 --> 00:31:20,480
sure that you've run your canary when

00:31:18,470 --> 00:31:30,679
you're getting that traffic and be

00:31:20,480 --> 00:31:35,870
conscious of that so if you been having

00:31:30,679 --> 00:31:38,809
problems will crop up over time do you

00:31:35,870 --> 00:31:41,510
ever see application teams of scaling

00:31:38,809 --> 00:31:45,500
the number of nodes that

00:31:41,510 --> 00:31:47,840
canary during the canary test either

00:31:45,500 --> 00:31:49,820
right now because because we're on a

00:31:47,840 --> 00:31:51,710
centralized team and we have to create

00:31:49,820 --> 00:31:54,140
new curated by clients we don't know how

00:31:51,710 --> 00:31:55,700
they're a dirty player so the Indians

00:31:54,140 --> 00:31:58,310
don't have books into that kind of stuff

00:31:55,700 --> 00:32:00,440
so we make them say how many instances

00:31:58,310 --> 00:32:03,380
they want for their experiment in there

00:32:00,440 --> 00:32:06,650
Claire the you no DSL and then that's

00:32:03,380 --> 00:32:10,130
that's it because I know there's always

00:32:06,650 --> 00:32:12,590
like it's been brought up in like art Co

00:32:10,130 --> 00:32:15,410
keystroke internally those people want

00:32:12,590 --> 00:32:19,700
to like go over one percent to a hundred

00:32:15,410 --> 00:32:21,290
percent over time and so we're not

00:32:19,700 --> 00:32:24,560
really we're not doing that has it today

00:32:21,290 --> 00:32:27,380
do you have a view that that's a bad

00:32:24,560 --> 00:32:30,320
idea or you're just not there I mean

00:32:27,380 --> 00:32:33,290
it's a complicated idea so like if I

00:32:30,320 --> 00:32:35,780
edit a slide for screaming that's really

00:32:33,290 --> 00:32:37,640
really easy to implement Sybase so it's

00:32:35,780 --> 00:32:38,810
also really easy to implement decide

00:32:37,640 --> 00:32:41,780
that they and the question you have

00:32:38,810 --> 00:32:44,060
asked it are the I ask myself is this

00:32:41,780 --> 00:32:45,860
adding that extra complexity does

00:32:44,060 --> 00:32:47,540
actually how about you or is it good

00:32:45,860 --> 00:32:49,250
enough just to do one instance of each

00:32:47,540 --> 00:32:54,190
like are you going to get like what's

00:32:49,250 --> 00:32:54,190
the ROI and all that extra complicated

00:32:59,780 --> 00:33:09,780
referee is for low back then adopts

00:33:02,700 --> 00:33:12,260
speaker completely so that is why has a

00:33:09,780 --> 00:33:12,260
knot

00:33:12,980 --> 00:33:19,080
like were they also guys

00:33:15,900 --> 00:33:21,440
I think a lot of people here would like

00:33:19,080 --> 00:33:24,440
to learn

00:33:21,440 --> 00:33:24,440
that

00:33:25,880 --> 00:33:32,809
right so I think first for any company

00:33:28,820 --> 00:33:34,880
or organization they need to choose to

00:33:32,809 --> 00:33:36,710
use the technology and deployment

00:33:34,880 --> 00:33:39,049
processes that work for them at

00:33:36,710 --> 00:33:42,500
particular times and times in that

00:33:39,049 --> 00:33:44,780
company's evolution in CI CV and DevOps

00:33:42,500 --> 00:33:46,610
and best practices so at the time when

00:33:44,780 --> 00:33:49,220
we were considering spinnaker we also

00:33:46,610 --> 00:33:54,429
had some other options that were more

00:33:49,220 --> 00:33:57,350
built-in and ingrained into how Nikes

00:33:54,429 --> 00:34:00,679
architecture was working and we decided

00:33:57,350 --> 00:34:05,090
to commit to that path and our case

00:34:00,679 --> 00:34:07,100
study on creating and enabling Cayenne

00:34:05,090 --> 00:34:08,750
tend to be standalone we're really

00:34:07,100 --> 00:34:11,240
excited that that was something that

00:34:08,750 --> 00:34:15,950
came out of us not using spinnaker

00:34:11,240 --> 00:34:17,540
because we've with the aid of the

00:34:15,950 --> 00:34:20,840
spinnaker and cayenne to open-source

00:34:17,540 --> 00:34:23,690
community we've proven that these micro

00:34:20,840 --> 00:34:25,609
services and spinnaker certainly add

00:34:23,690 --> 00:34:28,010
value in that ecosystem of micro

00:34:25,609 --> 00:34:30,109
services but they also add value

00:34:28,010 --> 00:34:33,649
individually as their own components and

00:34:30,109 --> 00:34:37,190
as their own tools so for teams who are

00:34:33,649 --> 00:34:41,179
working through their own CI CD

00:34:37,190 --> 00:34:43,889
decision-making process I think that you

00:34:41,179 --> 00:34:46,129
can find value and

00:34:43,889 --> 00:34:48,950
many different parts of spinnaker and

00:34:46,129 --> 00:34:52,289
leaning in and collaborating to make

00:34:48,950 --> 00:34:53,579
those technical components work for

00:34:52,289 --> 00:34:55,499
where your team is at and the

00:34:53,579 --> 00:34:57,089
requirements that you have you can add a

00:34:55,499 --> 00:34:57,779
lot of value to the rest of the

00:34:57,089 --> 00:35:00,480
community

00:34:57,779 --> 00:35:02,009
yeah I just want to add to that so the

00:35:00,480 --> 00:35:04,230
reason we're not using spinner here

00:35:02,009 --> 00:35:07,440
today is not because it's not a product

00:35:04,230 --> 00:35:09,299
it's mostly about the fact that we were

00:35:07,440 --> 00:35:12,150
already like I'm just gonna make up a

00:35:09,299 --> 00:35:13,920
number fifty percent into transitioning

00:35:12,150 --> 00:35:16,410
to the next our next generation platform

00:35:13,920 --> 00:35:18,869
okay so we already had and like a

00:35:16,410 --> 00:35:20,999
next-generation platform that specifics

00:35:18,869 --> 00:35:22,739
it's extremely declarative and we

00:35:20,999 --> 00:35:23,309
decided that we wanted everything that

00:35:22,739 --> 00:35:26,269
has cooked

00:35:23,309 --> 00:35:28,559
we didn't want users to have you live

00:35:26,269 --> 00:35:33,690
infrastructure and we're earning halfway

00:35:28,559 --> 00:35:35,279
through the transition into that so our

00:35:33,690 --> 00:35:38,249
leadership was like we don't want to get

00:35:35,279 --> 00:35:42,529
again hard in the middle of it so maybe

00:35:38,249 --> 00:35:42,529
in the future we'll use spinnaker but we

00:35:49,430 --> 00:35:54,080

YouTube URL: https://www.youtube.com/watch?v=cPNuA70pVcA


