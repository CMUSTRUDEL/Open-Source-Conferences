Title: Louis Vernon "A Journey to Safe, Mission Critical Continuous Delivery at Descartes Labs"
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	Studies have shown that high-velocity continuous delivery of services correlates with better service availability, with reduced mean time to recovery and an improved change failure rate. At Descartes Labs we have experienced this first hand. The Descartes Labs platform is comprised of several Kubernetes clusters, dozens of Kubernetes services and tens of thousands of Kubernetes Pods and CPU cores. I will detail a high-velocity service deployment architecture which leverages Spinnaker, Istio & Envoy to allow safe, unsupervised deployment of services on Kubernetes, with a combined deployment frequency exceeding 150 releases per day from a team of 30 engineers.

Real-time data ingest and analytics are an essential part of the Descartes Labs platform, so safe and reliable service deployment is critically important. I will describe the Descartes Labs delivery process and detail an essential component: our automated canary analysis pipeline. This pipeline leverages Istio and Kubernetes metrics, validating service health both at the Kubernetes Pod and L7 level. The pipeline progresses by dynamically patching Istio VirtualServices which control the routing of production traffic between production, baseline and canary Kubernetes deployments in a highly granular fashion.

To support this, we have developed a dynamically templated Spinnaker pipeline architecture, allowing our development teams to provision new continuous delivery deployment pipelines via simple JSON parameter files and un-templated manifests that live alongside the application code. The underlying Kubernetes Service and Istio VirtualService architecture is simple and can be extended to many deployment environments and tools. I will demonstrate the canary analysis pipeline and provide links to generic examples of VirtualService manifests and Spinnaker deployment pipelines that could be extended and applied to many contexts.
Captions: 
	00:00:06,330 --> 00:00:10,980
[Laughter]

00:00:15,110 --> 00:00:22,880
a journey to safe mission-critical

00:00:17,710 --> 00:00:24,200
continuous delivery Descartes labs so

00:00:22,880 --> 00:00:26,240
what's this talk going to be about all

00:00:24,200 --> 00:00:28,580
and by the way the talk slides are

00:00:26,240 --> 00:00:30,470
available in the spinnaker summit 2019

00:00:28,580 --> 00:00:33,320
slack channel if you want to follow at

00:00:30,470 --> 00:00:34,660
all what's this talk about this talks

00:00:33,320 --> 00:00:36,590
about the value of high velocity

00:00:34,660 --> 00:00:40,309
deployments and a short lead time to

00:00:36,590 --> 00:00:42,199
deploy it's about how to approach

00:00:40,309 --> 00:00:44,900
spinnaker with a small engineering team

00:00:42,199 --> 00:00:47,180
and aka the value of self service

00:00:44,900 --> 00:00:49,250
pipeline templating it's about a pattern

00:00:47,180 --> 00:00:52,000
for deployments to kubernetes using the

00:00:49,250 --> 00:00:54,320
v2 provider and it's about how

00:00:52,000 --> 00:00:57,470
application developers can become great

00:00:54,320 --> 00:00:59,510
spinnaker operators and finally it's

00:00:57,470 --> 00:01:02,629
about how kubernetes and sto and

00:00:59,510 --> 00:01:06,830
spinnaker together make for powerful

00:01:02,629 --> 00:01:08,869
canary deployments what's take art labs

00:01:06,830 --> 00:01:10,970
well Descartes Labs is a company that's

00:01:08,869 --> 00:01:13,010
building a data refinery to collect and

00:01:10,970 --> 00:01:16,070
process and analyze sensor data to

00:01:13,010 --> 00:01:17,900
quantify changes in the earth and as you

00:01:16,070 --> 00:01:19,370
might expect with anything earth scale

00:01:17,900 --> 00:01:21,650
that's a lot of data and a lot of

00:01:19,370 --> 00:01:23,300
compute platform drives global scale

00:01:21,650 --> 00:01:26,270
machine learning across more than 10

00:01:23,300 --> 00:01:28,190
petabytes of geospatial data we've got a

00:01:26,270 --> 00:01:29,840
great team with decades of machine

00:01:28,190 --> 00:01:31,690
learning remote sensing large-scale

00:01:29,840 --> 00:01:35,840
computing astrophysics and cosmology

00:01:31,690 --> 00:01:38,120
experience we've got quite a few offices

00:01:35,840 --> 00:01:40,580
but our headquarters are in the in Santa

00:01:38,120 --> 00:01:41,960
Fe New Mexico which is pretty unusual if

00:01:40,580 --> 00:01:43,730
you've never been to that part of the

00:01:41,960 --> 00:01:45,140
world it's beautiful and this is pretty

00:01:43,730 --> 00:01:48,980
much what my drive to work looks like

00:01:45,140 --> 00:01:50,840
every day so what can you do with the

00:01:48,980 --> 00:01:53,570
Descartes Labs platform or - pretty

00:01:50,840 --> 00:01:56,810
recent public examples where we have

00:01:53,570 --> 00:02:00,020
wildfire detection and so this is where

00:01:56,810 --> 00:02:02,330
we have some inference and analysis

00:02:00,020 --> 00:02:04,729
being on data strip run on data streams

00:02:02,330 --> 00:02:07,070
that come from the what's call that goes

00:02:04,729 --> 00:02:09,440
satellite within minutes of the

00:02:07,070 --> 00:02:11,810
satellite passing ever region so within

00:02:09,440 --> 00:02:14,239
you know a few minutes we can tell you

00:02:11,810 --> 00:02:18,040
if there's a probability of some fire

00:02:14,239 --> 00:02:20,629
detected in the region we're also doing

00:02:18,040 --> 00:02:23,180
we also had some recent public methane

00:02:20,629 --> 00:02:24,590
emission studies and this you know we've

00:02:23,180 --> 00:02:26,290
got some ongoing partnerships around

00:02:24,590 --> 00:02:28,460
that and this is where we're looking at

00:02:26,290 --> 00:02:30,830
Sentinel 5-piece at

00:02:28,460 --> 00:02:33,290
data this is large-scale analysis over

00:02:30,830 --> 00:02:36,590
large timeframes big compute jobs as you

00:02:33,290 --> 00:02:39,260
might imagine so what I even mean by

00:02:36,590 --> 00:02:41,270
mission-critical delivery well at any

00:02:39,260 --> 00:02:43,010
given time we have thousands of machine

00:02:41,270 --> 00:02:44,780
learning jobs hitting our api's and

00:02:43,010 --> 00:02:47,150
they're extracting insights from complex

00:02:44,780 --> 00:02:50,030
geospatial datasets that's historical

00:02:47,150 --> 00:02:51,470
data big jobs and also real time so we

00:02:50,030 --> 00:02:53,390
got models running on our platform and

00:02:51,470 --> 00:02:55,250
they can make decisions or generate

00:02:53,390 --> 00:02:58,220
insights within minutes of the satellite

00:02:55,250 --> 00:03:00,590
passing overhead but with this going on

00:02:58,220 --> 00:03:02,750
we also have a background where our API

00:03:00,590 --> 00:03:04,700
is are rapidly evolving we have new

00:03:02,750 --> 00:03:06,830
features coming out bug fixes new and

00:03:04,700 --> 00:03:09,200
revised models and we're actually seeing

00:03:06,830 --> 00:03:10,730
up to five deploys per day for our core

00:03:09,200 --> 00:03:12,590
services and so you've got this

00:03:10,730 --> 00:03:14,540
combination of time sensitive

00:03:12,590 --> 00:03:16,580
intelligence generation and a rapidly

00:03:14,540 --> 00:03:18,200
evolving platform and that means we

00:03:16,580 --> 00:03:22,400
developed some insights into safe

00:03:18,200 --> 00:03:23,930
mission-critical delivery so let's talk

00:03:22,400 --> 00:03:26,540
a little bit about the value of high

00:03:23,930 --> 00:03:28,820
velocity continuous delivery to us well

00:03:26,540 --> 00:03:30,200
we found internally that a high deploy

00:03:28,820 --> 00:03:32,090
frequency correspondent with smaller

00:03:30,200 --> 00:03:34,190
changes that were easier to understand

00:03:32,090 --> 00:03:36,110
debug and reconcile and when those

00:03:34,190 --> 00:03:37,520
changes introduced problems into

00:03:36,110 --> 00:03:39,770
production and of course that still

00:03:37,520 --> 00:03:41,450
happens they tended to be lower impact

00:03:39,770 --> 00:03:43,430
and we can pin the problems down quickly

00:03:41,450 --> 00:03:45,110
and so a couple with the short lead time

00:03:43,430 --> 00:03:46,910
for changes was often very easy for our

00:03:45,110 --> 00:03:49,070
developers to see a problem in

00:03:46,910 --> 00:03:50,870
production and then quickly roll forward

00:03:49,070 --> 00:03:53,510
to fix the problem they could of course

00:03:50,870 --> 00:03:55,490
still roll back and our anecdotal

00:03:53,510 --> 00:03:58,130
experience seems to be consistent the

00:03:55,490 --> 00:03:59,900
they're all generally true let's say the

00:03:58,130 --> 00:04:01,970
accelerate state of DevOps report

00:03:59,900 --> 00:04:03,470
demonstrates their increasing deployment

00:04:01,970 --> 00:04:06,620
frequency and reducing lead time for

00:04:03,470 --> 00:04:07,850
changes correlates with a lower change

00:04:06,620 --> 00:04:09,740
failure rate and a shorter time to

00:04:07,850 --> 00:04:13,070
recover from service incidents and

00:04:09,740 --> 00:04:14,630
defects so as we worked on our

00:04:13,070 --> 00:04:16,760
continuous deployment infrastructure

00:04:14,630 --> 00:04:18,440
early on and this is the SRE team you

00:04:16,760 --> 00:04:20,470
know it's it's worth mentioning that

00:04:18,440 --> 00:04:24,170
we're not a continuous deployment team

00:04:20,470 --> 00:04:26,270
we discovered a few things early on one

00:04:24,170 --> 00:04:28,130
manually creating deployment pipelines

00:04:26,270 --> 00:04:30,770
for each application was error-prone and

00:04:28,130 --> 00:04:32,270
just didn't scale to having s Ari's in

00:04:30,770 --> 00:04:34,430
the critical path for adding and

00:04:32,270 --> 00:04:36,620
configuring specific applications was

00:04:34,430 --> 00:04:38,419
slow and inefficient it was hard to make

00:04:36,620 --> 00:04:40,280
developing pipelines for a new

00:04:38,419 --> 00:04:41,750
application our top priority when we

00:04:40,280 --> 00:04:42,139
have a thousand of things and it's not

00:04:41,750 --> 00:04:44,749
receive

00:04:42,139 --> 00:04:46,610
user traffic at that time and three

00:04:44,749 --> 00:04:47,870
having s eries responsible for

00:04:46,610 --> 00:04:49,430
day-to-day operations a deployment

00:04:47,870 --> 00:04:51,529
pipelines was ineffective and didn't

00:04:49,430 --> 00:04:53,300
scale so just want to give you some

00:04:51,529 --> 00:04:56,659
quick context on our infrastructure

00:04:53,300 --> 00:05:00,110
because it explains some of the patterns

00:04:56,659 --> 00:05:01,819
we described later so first I'd like to

00:05:00,110 --> 00:05:04,189
do an experiment repeat an experiment

00:05:01,819 --> 00:05:08,990
from last year who in the room is

00:05:04,189 --> 00:05:11,990
familiar with this ta okay and who is

00:05:08,990 --> 00:05:13,849
using this do in production all right

00:05:11,990 --> 00:05:17,270
definitely more than last year I saw a

00:05:13,849 --> 00:05:19,219
few hands there so yeah we use history

00:05:17,270 --> 00:05:21,979
and kubernetes all our traffic goes

00:05:19,219 --> 00:05:23,479
through sto and sto is an extremely

00:05:21,979 --> 00:05:25,610
powerful service machine it has some

00:05:23,479 --> 00:05:27,770
great features we use the advanced

00:05:25,610 --> 00:05:29,180
traffic routing rich telemetry so we get

00:05:27,770 --> 00:05:31,669
all seven metrics back from our

00:05:29,180 --> 00:05:34,250
application for free and it also handles

00:05:31,669 --> 00:05:36,259
authentication and authorization both on

00:05:34,250 --> 00:05:38,509
ingress into the cluster service to

00:05:36,259 --> 00:05:40,430
service we use features like you know

00:05:38,509 --> 00:05:42,800
you URI rewriting or request and

00:05:40,430 --> 00:05:44,870
response rewriting but you don't really

00:05:42,800 --> 00:05:46,879
need to know anything about SDO for this

00:05:44,870 --> 00:05:50,810
talk for this talk all you have to think

00:05:46,879 --> 00:05:52,969
about as sto as a reverse proxy traffic

00:05:50,810 --> 00:05:55,069
comes in it routes incoming traffic to

00:05:52,969 --> 00:05:56,810
the desired pods we can figure that

00:05:55,069 --> 00:05:57,860
rooting through kubernetes customer

00:05:56,810 --> 00:06:00,020
resource definitions

00:05:57,860 --> 00:06:02,479
they're called SEO virtual services and

00:06:00,020 --> 00:06:06,349
then sto provides telemetry about the

00:06:02,479 --> 00:06:09,889
traffic another thing I want to talk

00:06:06,349 --> 00:06:11,509
about is staging in our own company we

00:06:09,889 --> 00:06:13,789
found out that people different teams

00:06:11,509 --> 00:06:15,860
have different concepts of what staging

00:06:13,789 --> 00:06:19,370
men and we kind of had a radical shift

00:06:15,860 --> 00:06:20,750
in the past year originally we use

00:06:19,370 --> 00:06:24,110
staging as part of our critical

00:06:20,750 --> 00:06:25,819
deployment pipeline and today we do not

00:06:24,110 --> 00:06:27,740
do that we have a staging classroom but

00:06:25,819 --> 00:06:30,020
we do not test releases there we found

00:06:27,740 --> 00:06:31,250
it was a maintenance burden syncing our

00:06:30,020 --> 00:06:33,680
stage and production environments

00:06:31,250 --> 00:06:35,599
testing stage just wasn't representative

00:06:33,680 --> 00:06:36,649
of our production environment we didn't

00:06:35,599 --> 00:06:38,539
have the same number of database

00:06:36,649 --> 00:06:40,969
connections resource utilization wasn't

00:06:38,539 --> 00:06:43,310
the same and like I said stage became

00:06:40,969 --> 00:06:45,110
part of our high velocity pipelines and

00:06:43,310 --> 00:06:47,149
so you know we're pushing for this high

00:06:45,110 --> 00:06:48,680
frequency deployment but we've now we've

00:06:47,149 --> 00:06:49,819
now now made our stage environment that

00:06:48,680 --> 00:06:52,610
could have called piece of that and

00:06:49,819 --> 00:06:54,349
really impacted what we could do in our

00:06:52,610 --> 00:06:56,360
in our stage environment environment for

00:06:54,349 --> 00:06:56,660
testing and so as we thought about this

00:06:56,360 --> 00:06:58,310
we

00:06:56,660 --> 00:07:00,680
Oh actually this is a really great

00:06:58,310 --> 00:07:03,410
literature around this and I've linked

00:07:00,680 --> 00:07:06,550
to a couple of articles from Cindy or

00:07:03,410 --> 00:07:09,170
copy construct that really explained why

00:07:06,550 --> 00:07:10,580
testing and staging isn't great and why

00:07:09,170 --> 00:07:12,320
we should be able to do testing and

00:07:10,580 --> 00:07:14,960
production it's about two and a half

00:07:12,320 --> 00:07:17,630
hours of reading I'll just warn you so

00:07:14,960 --> 00:07:19,970
today we are deployment pipelines

00:07:17,630 --> 00:07:22,820
deployed to a pre-release endpoint

00:07:19,970 --> 00:07:24,590
that's an endpoint that our internal

00:07:22,820 --> 00:07:27,260
team can hit that's in production that

00:07:24,590 --> 00:07:29,480
customer traffic doesn't go to to a

00:07:27,260 --> 00:07:32,780
release endpoint that's the end point

00:07:29,480 --> 00:07:35,150
that most of the time 100% of traffic is

00:07:32,780 --> 00:07:36,890
going to unless we're in the middle of a

00:07:35,150 --> 00:07:40,040
canary rollout and I'll talk about that

00:07:36,890 --> 00:07:43,280
later we have a dev endpoint and this is

00:07:40,040 --> 00:07:45,380
a infrequently used very sort of well

00:07:43,280 --> 00:07:47,990
guarded end point where we can deploy a

00:07:45,380 --> 00:07:50,600
specially customized version of a

00:07:47,990 --> 00:07:52,400
service just just to tweak our interacts

00:07:50,600 --> 00:07:55,040
with prod joint development and and it's

00:07:52,400 --> 00:07:56,540
short-lived and we also have baseline

00:07:55,040 --> 00:07:59,270
and canary deployments so we can't route

00:07:56,540 --> 00:08:00,560
traffic to there specifically that that

00:07:59,270 --> 00:08:05,990
happens automatically as part of a

00:08:00,560 --> 00:08:07,790
canary deployment so this is what the

00:08:05,990 --> 00:08:10,190
kubernetes cluster looks like from the

00:08:07,790 --> 00:08:12,470
perspective of one service traffic comes

00:08:10,190 --> 00:08:15,110
in it goes to the sto ingress gateway

00:08:12,470 --> 00:08:16,580
and we have three virtual services that

00:08:15,110 --> 00:08:18,919
again they're kubernetes customer

00:08:16,580 --> 00:08:21,110
resource definitions that tell sto how

00:08:18,919 --> 00:08:22,730
we want to move our traffic and so we

00:08:21,110 --> 00:08:25,130
have a dedicated virtual service for our

00:08:22,730 --> 00:08:27,290
pre-release endpoint we have a virtual

00:08:25,130 --> 00:08:29,720
service for our release endpoint and

00:08:27,290 --> 00:08:31,100
like I said that normally 100% of

00:08:29,720 --> 00:08:33,080
traffic's going to the release

00:08:31,100 --> 00:08:35,110
deployment unless we're doing a baseline

00:08:33,080 --> 00:08:37,190
in canary and then we have a separate

00:08:35,110 --> 00:08:39,229
virtual service for our demo endpoint

00:08:37,190 --> 00:08:41,180
and pre release and release are always

00:08:39,229 --> 00:08:42,229
their pre-release always gets deployed

00:08:41,180 --> 00:08:44,660
as part of our continuous deployment

00:08:42,229 --> 00:08:46,490
pipeline and then obviously releases

00:08:44,660 --> 00:08:50,720
where our traffic goes to but the other

00:08:46,490 --> 00:08:52,040
others are ephemeral so as a developer

00:08:50,720 --> 00:08:54,530
at Descartes labs how do you go from

00:08:52,040 --> 00:08:56,630
development to production well we do our

00:08:54,530 --> 00:08:58,610
development in git and we use github we

00:08:56,630 --> 00:09:00,970
have a mono repo with Python rustling go

00:08:58,610 --> 00:09:03,560
in it we use trunk based development and

00:09:00,970 --> 00:09:04,940
usually we're saying we see pull

00:09:03,560 --> 00:09:07,310
requests coming in from short-lived

00:09:04,940 --> 00:09:09,860
branches and so the development time

00:09:07,310 --> 00:09:12,080
there is from hours to days

00:09:09,860 --> 00:09:15,110
assuming your your pull request is

00:09:12,080 --> 00:09:16,940
approved and merged then we trigger our

00:09:15,110 --> 00:09:19,190
build and push stage which happens with

00:09:16,940 --> 00:09:22,490
basil and drone this is triggered by

00:09:19,190 --> 00:09:24,920
github webhook and assuming all the

00:09:22,490 --> 00:09:26,690
tests pass everything goes smoothly it

00:09:24,920 --> 00:09:29,180
sends up with a push to our container

00:09:26,690 --> 00:09:31,430
registry and this triggers the last part

00:09:29,180 --> 00:09:33,320
the part we really care about and this

00:09:31,430 --> 00:09:36,170
is the deploy stage and we use spinnaker

00:09:33,320 --> 00:09:38,800
and this stage typically takes less than

00:09:36,170 --> 00:09:41,270
14 minutes but that's 40 minutes before

00:09:38,800 --> 00:09:43,640
100% of traffic is going to the new

00:09:41,270 --> 00:09:46,760
image it's actually much quicker than

00:09:43,640 --> 00:09:49,580
that for a new image to start receiving

00:09:46,760 --> 00:09:51,410
production traffic so this trigger

00:09:49,580 --> 00:09:53,900
triggered by a pub/sub message and

00:09:51,410 --> 00:09:56,060
deploys to kubernetes through this one

00:09:53,900 --> 00:10:00,590
continuous deployment pipeline now which

00:09:56,060 --> 00:10:02,540
I will describe so we use pipeline

00:10:00,590 --> 00:10:04,400
templating but we're not using manage

00:10:02,540 --> 00:10:08,600
pipeline tempo templating we have our

00:10:04,400 --> 00:10:11,030
own pipeline templating architecture in

00:10:08,600 --> 00:10:12,020
for the for the pipeline templating the

00:10:11,030 --> 00:10:13,550
way it works

00:10:12,020 --> 00:10:15,590
our architecture is we have the

00:10:13,550 --> 00:10:17,780
configuration living alongside the

00:10:15,590 --> 00:10:19,130
application code in mono repo and that

00:10:17,780 --> 00:10:21,920
has some implications we could talk

00:10:19,130 --> 00:10:24,610
about that afterwards but what we have

00:10:21,920 --> 00:10:27,440
is we have a config JSON and that

00:10:24,610 --> 00:10:29,150
statically parameterize azar pipeline

00:10:27,440 --> 00:10:31,670
template for one instance of an

00:10:29,150 --> 00:10:34,550
application so that yeah that configures

00:10:31,670 --> 00:10:36,920
the pipeline for an application and then

00:10:34,550 --> 00:10:39,050
we have the pipeline dynamically

00:10:36,920 --> 00:10:40,810
parameterize our kubernetes manifests

00:10:39,050 --> 00:10:46,460
during our continuous deployment

00:10:40,810 --> 00:10:48,710
execution and I'll explain how that

00:10:46,460 --> 00:10:49,700
works so if you're a developer and you

00:10:48,710 --> 00:10:50,960
want to start deploying a new

00:10:49,700 --> 00:10:52,520
application to our production

00:10:50,960 --> 00:10:54,320
environment here's what you have to do

00:10:52,520 --> 00:10:56,540
you have to copy an example spinnaker

00:10:54,320 --> 00:10:58,760
config JSON and some kubernetes e

00:10:56,540 --> 00:11:00,350
animals into your application folder you

00:10:58,760 --> 00:11:02,840
have to set the deployment name in the

00:11:00,350 --> 00:11:04,940
config JSON and actually that's it

00:11:02,840 --> 00:11:06,440
that's all you have to do automation

00:11:04,940 --> 00:11:08,320
picks up the new files and starts

00:11:06,440 --> 00:11:10,550
templating the deployment pipelines and

00:11:08,320 --> 00:11:12,920
we're using ginger for our pipeline

00:11:10,550 --> 00:11:15,800
templating while there developers on a

00:11:12,920 --> 00:11:18,890
branch they can see the the pipeline's

00:11:15,800 --> 00:11:20,930
available as build artifacts and they

00:11:18,890 --> 00:11:23,430
could they could import that into our

00:11:20,930 --> 00:11:26,459
spinnaker if they wanted to

00:11:23,430 --> 00:11:27,720
and once the the development you know

00:11:26,459 --> 00:11:29,220
they're happy with where it is they

00:11:27,720 --> 00:11:31,500
submitted PR and it's approved and

00:11:29,220 --> 00:11:34,350
merged at that point the pipelines are

00:11:31,500 --> 00:11:36,750
automatically applying via the spin CLI

00:11:34,350 --> 00:11:38,459
and this sets up our kubernetes

00:11:36,750 --> 00:11:41,009
deployments are horizontal Palo Alto

00:11:38,459 --> 00:11:42,630
scalars and the kubernetes services

00:11:41,009 --> 00:11:44,100
along with the ISTE of virtual services

00:11:42,630 --> 00:11:46,410
needed for traffic to get to the service

00:11:44,100 --> 00:11:49,440
and this is a bit scary seeing as this

00:11:46,410 --> 00:11:50,850
is self-service so we have a little bit

00:11:49,440 --> 00:11:53,430
of protection in the form of we

00:11:50,850 --> 00:11:55,259
namespace the spinnaker application

00:11:53,430 --> 00:11:57,899
based on the application folder name in

00:11:55,259 --> 00:12:00,329
our mono repo but that's about it I

00:11:57,899 --> 00:12:02,610
don't mention anything about secrets we

00:12:00,329 --> 00:12:05,310
have a different workflow for secrets we

00:12:02,610 --> 00:12:06,899
use terraform for managing those and we

00:12:05,310 --> 00:12:08,759
have a workflow that uses code owners

00:12:06,899 --> 00:12:10,350
and Atlantis so Atlantis is something

00:12:08,759 --> 00:12:13,170
that interacts with get all that you do

00:12:10,350 --> 00:12:17,459
to terraform in a you know you know nice

00:12:13,170 --> 00:12:20,130
way our ginger pipeline templates

00:12:17,459 --> 00:12:22,110
they're about as readable as raw

00:12:20,130 --> 00:12:24,329
pipeline JSON we do some stuff like we

00:12:22,110 --> 00:12:26,790
substitute variables so we'll insert our

00:12:24,329 --> 00:12:28,050
service name we have conditional stages

00:12:26,790 --> 00:12:29,130
so here's an example

00:12:28,050 --> 00:12:31,380
I don't know hopefully you can see that

00:12:29,130 --> 00:12:33,750
from there if a putt if deployment

00:12:31,380 --> 00:12:35,639
doesn't need a config map it evaluates

00:12:33,750 --> 00:12:38,250
to false and so that stage is disabled

00:12:35,639 --> 00:12:40,529
and we also do loop stage insertion so

00:12:38,250 --> 00:12:42,269
we allow for any number of pre deployed

00:12:40,529 --> 00:12:45,000
jobs that happen before the pre-release

00:12:42,269 --> 00:12:47,550
endpoint is updated and what they can do

00:12:45,000 --> 00:12:49,740
is stuff like smoke tests with

00:12:47,550 --> 00:12:52,380
production they can do schema migrations

00:12:49,740 --> 00:12:54,060
that kind of thing our most complicated

00:12:52,380 --> 00:12:58,110
pipeline template is about five hundred

00:12:54,060 --> 00:13:00,560
lines and that's about as long as our

00:12:58,110 --> 00:13:03,569
most pop but most most complicated

00:13:00,560 --> 00:13:05,490
pipeline parameterised pipeline as you

00:13:03,569 --> 00:13:07,620
might guess generating the pipeline is

00:13:05,490 --> 00:13:09,360
very straightforward there is a little

00:13:07,620 --> 00:13:11,939
bit of pain here right there's something

00:13:09,360 --> 00:13:13,740
worth calling out we're using quite a

00:13:11,939 --> 00:13:16,019
bit of the spinnaker expression language

00:13:13,740 --> 00:13:17,790
and so the debugging process can be a

00:13:16,019 --> 00:13:20,279
little painful because some things are

00:13:17,790 --> 00:13:21,569
evaluated at template time it's like

00:13:20,279 --> 00:13:23,430
statically and then some things are

00:13:21,569 --> 00:13:24,839
evaluated at execution time this is

00:13:23,430 --> 00:13:26,399
where we're parameterizing stuff in our

00:13:24,839 --> 00:13:29,120
manifests and we'll show how that works

00:13:26,399 --> 00:13:31,410
in a minute what is the spin

00:13:29,120 --> 00:13:32,970
configuration look like it's really

00:13:31,410 --> 00:13:36,220
simple this is just it's a perfectly

00:13:32,970 --> 00:13:37,900
representative of an example so we have

00:13:36,220 --> 00:13:39,910
that into deployment configuration and

00:13:37,900 --> 00:13:41,440
pipeline configuration and so in

00:13:39,910 --> 00:13:43,150
deployment configuration we say okay

00:13:41,440 --> 00:13:44,350
what's the service name what's the image

00:13:43,150 --> 00:13:45,550
we're going to be matching against where

00:13:44,350 --> 00:13:47,560
are we deploying this thing

00:13:45,550 --> 00:13:49,390
how many replicas do we want minimum

00:13:47,560 --> 00:13:51,550
what's the minimum number of replicas we

00:13:49,390 --> 00:13:53,620
want for our pre release and our release

00:13:51,550 --> 00:13:55,690
endpoint we also allow you to insert

00:13:53,620 --> 00:13:57,040
dynamically some environment variables

00:13:55,690 --> 00:13:58,600
so if you want to configure the pre

00:13:57,040 --> 00:14:00,820
release deployment slightly differently

00:13:58,600 --> 00:14:02,440
to the release one but we really frown

00:14:00,820 --> 00:14:05,260
on you doing that we'd really want pre

00:14:02,440 --> 00:14:06,400
release to look just like release at the

00:14:05,260 --> 00:14:07,780
bottom we actually specify the

00:14:06,400 --> 00:14:11,770
pipeline's we want to use there's a

00:14:07,780 --> 00:14:13,240
one-click rollback pipeline there's you

00:14:11,770 --> 00:14:15,190
know various other bits and pieces that

00:14:13,240 --> 00:14:19,300
allow us to deploy based on manifest

00:14:15,190 --> 00:14:21,040
changes in addition to image changes but

00:14:19,300 --> 00:14:22,870
the most complicated pipeline here is

00:14:21,040 --> 00:14:25,240
the continuous deployment pipeline and

00:14:22,870 --> 00:14:27,220
here in this example we say okay we

00:14:25,240 --> 00:14:28,930
don't want to canary on this execution

00:14:27,220 --> 00:14:31,390
and that means we need to have a manual

00:14:28,930 --> 00:14:33,340
approval on the deployment and we wanted

00:14:31,390 --> 00:14:34,780
to find some pre deploy jobs we we're

00:14:33,340 --> 00:14:36,550
going to have a job called migrate

00:14:34,780 --> 00:14:39,010
schema and it's of type migrations and

00:14:36,550 --> 00:14:42,490
actually all this does is it tells the

00:14:39,010 --> 00:14:46,660
spinnaker pipeline to make run job or

00:14:42,490 --> 00:14:48,700
deploy run a kubernetes job with a

00:14:46,660 --> 00:14:52,450
manifest and this tells it where to find

00:14:48,700 --> 00:14:54,670
that manifest that that's it so that's

00:14:52,450 --> 00:14:56,320
how we configure the pipeline's and then

00:14:54,670 --> 00:14:58,300
at run time when the pipeline is running

00:14:56,320 --> 00:15:00,190
this is how we can dynamically change

00:14:58,300 --> 00:15:03,190
the properties of our deployment

00:15:00,190 --> 00:15:04,750
manifests and so here you can see we've

00:15:03,190 --> 00:15:07,510
got what looks like a pretty regular

00:15:04,750 --> 00:15:10,510
deployment manifest but we've got custom

00:15:07,510 --> 00:15:13,180
stage and a custom name and so we'll say

00:15:10,510 --> 00:15:17,050
we've got this spell function here route

00:15:13,180 --> 00:15:19,180
and what route means is route says grab

00:15:17,050 --> 00:15:22,420
the variable stage from the stage that's

00:15:19,180 --> 00:15:23,680
executing right now and so when we

00:15:22,420 --> 00:15:26,770
template our pipeline's

00:15:23,680 --> 00:15:28,390
we set the variable stage to be def if

00:15:26,770 --> 00:15:30,760
it's a dev point of deployment we set

00:15:28,390 --> 00:15:33,190
the variables stage to pre-release at

00:15:30,760 --> 00:15:34,600
the release stage and release at the

00:15:33,190 --> 00:15:36,580
sort of pre release at the pre release

00:15:34,600 --> 00:15:39,820
stage and release at the release stage

00:15:36,580 --> 00:15:43,030
and so every time this template every

00:15:39,820 --> 00:15:44,860
time this this manifest is applied we

00:15:43,030 --> 00:15:46,780
get it - we template it for the

00:15:44,860 --> 00:15:48,490
different targets we're trying to deploy

00:15:46,780 --> 00:15:50,140
and that's how we control our dev

00:15:48,490 --> 00:15:52,480
pre-release release Canarian

00:15:50,140 --> 00:15:54,790
and deployments and this is how we can

00:15:52,480 --> 00:16:00,910
use the same manifests for many

00:15:54,790 --> 00:16:03,550
different deployments this is a very

00:16:00,910 --> 00:16:05,290
similar example but with our HPA you can

00:16:03,550 --> 00:16:07,390
see at the bottom were customizing the

00:16:05,290 --> 00:16:09,850
the well we're customizing the name in

00:16:07,390 --> 00:16:11,470
the target name in the same fashion and

00:16:09,850 --> 00:16:14,110
there this like some slightly uglier

00:16:11,470 --> 00:16:16,600
spell here we're doing a two int call on

00:16:14,110 --> 00:16:19,120
the result of the route that pulling

00:16:16,600 --> 00:16:20,950
them in replicas variable from the stage

00:16:19,120 --> 00:16:22,630
that's executing but there's also

00:16:20,950 --> 00:16:24,700
another reason why I wanted to show you

00:16:22,630 --> 00:16:26,260
this it's because at the bottom you you

00:16:24,700 --> 00:16:28,330
may be able to see there's this target

00:16:26,260 --> 00:16:32,130
connection to pearpod that's obviously

00:16:28,330 --> 00:16:35,530
not a real a horizontal pod autoscaler

00:16:32,130 --> 00:16:38,380
configuration but we do scale on the

00:16:35,530 --> 00:16:40,330
number of connections per per deployment

00:16:38,380 --> 00:16:42,730
and we do that using their zalando

00:16:40,330 --> 00:16:44,410
cubemetricks adapter scraping some

00:16:42,730 --> 00:16:48,150
special metrics that we get from using

00:16:44,410 --> 00:16:52,000
sto which uses employ under the hood and

00:16:48,150 --> 00:16:55,810
this will come up later so let's look at

00:16:52,000 --> 00:16:57,760
the continuous deployment pipeline here

00:16:55,810 --> 00:17:00,300
it is there's not too many stages

00:16:57,760 --> 00:17:03,040
involved this is this is you know as

00:17:00,300 --> 00:17:04,570
totally representative of a real

00:17:03,040 --> 00:17:07,150
continuous deployment pipeline for our

00:17:04,570 --> 00:17:09,190
infrastructure and you can see there are

00:17:07,150 --> 00:17:10,570
two entry points to our continuous

00:17:09,190 --> 00:17:12,459
deployment pipeline we allow for a

00:17:10,570 --> 00:17:14,650
manual trigger if a developer wants to

00:17:12,459 --> 00:17:16,810
deploy something special it has to match

00:17:14,650 --> 00:17:18,190
that prefix we showed the image prefix

00:17:16,810 --> 00:17:21,699
but if a developer wants to go back and

00:17:18,190 --> 00:17:23,860
deploy a previous version they can we

00:17:21,699 --> 00:17:26,980
also generally almost all the executions

00:17:23,860 --> 00:17:28,840
run off the pub/sub trigger and you'll

00:17:26,980 --> 00:17:30,670
notice it's kind of interesting those

00:17:28,840 --> 00:17:33,010
two entry points are actually evaluate

00:17:30,670 --> 00:17:35,590
variable stage that that's what we start

00:17:33,010 --> 00:17:37,510
a pipeline off with and so the evaluate

00:17:35,590 --> 00:17:39,670
variable stage lets you set some

00:17:37,510 --> 00:17:42,010
variables that you can consume at other

00:17:39,670 --> 00:17:43,510
stages of your pipeline and we do do

00:17:42,010 --> 00:17:46,960
that you can see here we're setting tag

00:17:43,510 --> 00:17:50,380
at the bottom of the screen but it also

00:17:46,960 --> 00:17:51,880
in our case we we sort of backdoor some

00:17:50,380 --> 00:17:54,190
functionality into it we use the

00:17:51,880 --> 00:17:57,310
evaluate variable stage to evaluate a

00:17:54,190 --> 00:17:59,530
bunch of really quite gnarly spell that

00:17:57,310 --> 00:18:02,050
we don't want hidden away in there the

00:17:59,530 --> 00:18:03,549
pipeline itself we want an operator to

00:18:02,050 --> 00:18:05,470
be able to click here and

00:18:03,549 --> 00:18:10,179
see what was evaluated as the product of

00:18:05,470 --> 00:18:11,950
the spell and so one thing we do is we

00:18:10,179 --> 00:18:14,289
modify the incoming image artifact to

00:18:11,950 --> 00:18:16,869
deploy based on the tag instead of the

00:18:14,289 --> 00:18:18,789
SHA our developers wanted to do that and

00:18:16,869 --> 00:18:21,190
there's some good reasons to do that we

00:18:18,789 --> 00:18:22,570
want to be able to reason about the

00:18:21,190 --> 00:18:25,799
deployments and actually we use some

00:18:22,570 --> 00:18:28,960
properties of the deployments later on

00:18:25,799 --> 00:18:31,059
yeah we use that information about the

00:18:28,960 --> 00:18:34,239
current image tag from the deployment

00:18:31,059 --> 00:18:36,369
later on and so here's an example of

00:18:34,239 --> 00:18:38,289
what the evaluate variables stage looks

00:18:36,369 --> 00:18:40,330
like and it's a little hard to see but

00:18:38,289 --> 00:18:41,919
here we've got you know some spell

00:18:40,330 --> 00:18:44,019
that's not actually shedding a variable

00:18:41,919 --> 00:18:45,549
we're actually evaluating spell and it's

00:18:44,019 --> 00:18:48,009
setting you know the parameters image

00:18:45,549 --> 00:18:50,970
tag and it's updating actually the

00:18:48,009 --> 00:18:53,919
trigger result result expected artifacts

00:18:50,970 --> 00:18:56,820
properties so a little a little gnarly

00:18:53,919 --> 00:19:01,049
like I said but it's it's pretty useful

00:18:56,820 --> 00:19:04,149
so the image tag of our deployed images

00:19:01,049 --> 00:19:06,700
over here on the right-hand side are you

00:19:04,149 --> 00:19:08,769
can see that the image tag includes a

00:19:06,700 --> 00:19:10,869
hash at the end of it and that hash is

00:19:08,769 --> 00:19:15,970
the get Schaaf of their triggering

00:19:10,869 --> 00:19:18,909
commit so what yeah we'll use that later

00:19:15,970 --> 00:19:20,649
on so in the second stage of this

00:19:18,909 --> 00:19:24,659
pipeline we get our image tag of the

00:19:20,649 --> 00:19:28,239
release deployment and this is used for

00:19:24,659 --> 00:19:29,889
one we know the current image that's

00:19:28,239 --> 00:19:31,539
being deployed and we know the image

00:19:29,889 --> 00:19:33,820
that's already at release and so we can

00:19:31,539 --> 00:19:36,070
easily generate a github diff link that

00:19:33,820 --> 00:19:37,720
shows the commits that differ between

00:19:36,070 --> 00:19:38,950
the two versions that you know the

00:19:37,720 --> 00:19:40,809
version that's been deployed and what's

00:19:38,950 --> 00:19:42,909
in production and usually this is pretty

00:19:40,809 --> 00:19:45,940
short because we're deploying so

00:19:42,909 --> 00:19:47,470
frequently but we also need to use this

00:19:45,940 --> 00:19:49,239
image that we retrieve as part of our

00:19:47,470 --> 00:19:52,419
Canarian assets and I'll explain how

00:19:49,239 --> 00:19:54,759
later in this pipeline in this this

00:19:52,419 --> 00:19:56,820
particular example the service didn't

00:19:54,759 --> 00:19:59,049
need a config map and so the way we

00:19:56,820 --> 00:20:02,039
disabled the config map deployment is we

00:19:59,049 --> 00:20:04,239
actually just have them we have

00:20:02,039 --> 00:20:06,249
something evaluate to false and it just

00:20:04,239 --> 00:20:08,460
skips the stage but we could just

00:20:06,249 --> 00:20:12,759
template that out right not include it

00:20:08,460 --> 00:20:14,510
there is a pre deploy job it says es

00:20:12,759 --> 00:20:16,740
migrate schema

00:20:14,510 --> 00:20:19,550
yeah in this case it's a schema

00:20:16,740 --> 00:20:22,320
migration and what this does is it runs

00:20:19,550 --> 00:20:24,330
kubernetes job that does the migration

00:20:22,320 --> 00:20:25,740
we need it checks whether the migration

00:20:24,330 --> 00:20:29,670
was successful and then it cleans itself

00:20:25,740 --> 00:20:31,770
up so then we deploy the pre-release

00:20:29,670 --> 00:20:33,300
endpoint and like I said before the

00:20:31,770 --> 00:20:35,790
pre-release endpoint is something that

00:20:33,300 --> 00:20:39,660
our developers can hit but no production

00:20:35,790 --> 00:20:41,130
traffic goes to and in this case we

00:20:39,660 --> 00:20:43,050
didn't have Canaries enabled so an

00:20:41,130 --> 00:20:44,880
operator must manually approve the

00:20:43,050 --> 00:20:48,510
promotion to production that's a manual

00:20:44,880 --> 00:20:49,890
judgment stage and for the operator you

00:20:48,510 --> 00:20:51,420
can see in the manual judgment

00:20:49,890 --> 00:20:52,860
instructions there's the github diff

00:20:51,420 --> 00:20:54,210
link they can click on it and they'll

00:20:52,860 --> 00:20:55,740
show them the different ago okay yeah

00:20:54,210 --> 00:20:59,310
I'm confident about what's rolling out

00:20:55,740 --> 00:21:01,020
here and so they're happy and they click

00:20:59,310 --> 00:21:02,940
approve and the application is deployed

00:21:01,020 --> 00:21:07,140
to release and this is just a kubernetes

00:21:02,940 --> 00:21:09,030
- rolling update like usual and and this

00:21:07,140 --> 00:21:10,140
release endpoint you know your your new

00:21:09,030 --> 00:21:13,230
image is going to start receiving

00:21:10,140 --> 00:21:15,150
production traffic so we used to

00:21:13,230 --> 00:21:16,890
actually have quite a bit of content in

00:21:15,150 --> 00:21:19,470
our manual judgment stage we had links

00:21:16,890 --> 00:21:20,910
to graph honor and so on but we don't do

00:21:19,470 --> 00:21:22,500
that we've got a very simple stage as

00:21:20,910 --> 00:21:24,300
you saw and we're actually able to

00:21:22,500 --> 00:21:26,460
surface some of our monitoring links

00:21:24,300 --> 00:21:29,340
directly in the clusters view by adding

00:21:26,460 --> 00:21:32,880
custom annotations to our deployments

00:21:29,340 --> 00:21:34,710
and this is a spinnaker future and this

00:21:32,880 --> 00:21:38,790
is what this link would take you to a

00:21:34,710 --> 00:21:40,850
graph on a dashboard for the service so

00:21:38,790 --> 00:21:44,550
let's talk about pipeline operations

00:21:40,850 --> 00:21:46,830
well I don't know about you folks but

00:21:44,550 --> 00:21:49,410
life looked a little like this for us at

00:21:46,830 --> 00:21:51,270
Descartes Labs develop a number of

00:21:49,410 --> 00:21:53,700
developers groups steadily number of

00:21:51,270 --> 00:21:58,890
applications took off exponentially and

00:21:53,700 --> 00:22:00,810
the SRE team very slowly grew so this

00:21:58,890 --> 00:22:02,370
obviously having an SRE team as pipeline

00:22:00,810 --> 00:22:04,740
operators was wasn't going to work and

00:22:02,370 --> 00:22:06,480
with the right sales pitch we were able

00:22:04,740 --> 00:22:08,040
to get good buy-in from our development

00:22:06,480 --> 00:22:09,740
teams who are interested in managing

00:22:08,040 --> 00:22:11,820
their own application deployments and

00:22:09,740 --> 00:22:13,500
honestly I think we all know this

00:22:11,820 --> 00:22:14,940
application developers are often best

00:22:13,500 --> 00:22:16,980
equipped to understand behavior and

00:22:14,940 --> 00:22:18,780
diagnose issues and its really

00:22:16,980 --> 00:22:21,960
reasonable and easy to do when you're

00:22:18,780 --> 00:22:23,850
able to do a deployment so rapidly

00:22:21,960 --> 00:22:25,710
so spinnaker actually provides several

00:22:23,850 --> 00:22:27,390
features that facilitate pushing

00:22:25,710 --> 00:22:28,170
pipeline operation to application

00:22:27,390 --> 00:22:29,670
developers

00:22:28,170 --> 00:22:31,080
there's great authorization we can

00:22:29,670 --> 00:22:33,570
restrict pipeline execution to

00:22:31,080 --> 00:22:35,340
individual teams as a audit trail we can

00:22:33,570 --> 00:22:36,810
clearly track who executed manual

00:22:35,340 --> 00:22:40,140
triggers or approvals or who did that

00:22:36,810 --> 00:22:42,120
rollback and we got rich diagnostics you

00:22:40,140 --> 00:22:44,490
can view the deployment pod health and

00:22:42,120 --> 00:22:47,220
logs from within the spinnaker UI and

00:22:44,490 --> 00:22:48,930
like the biggest wing here so I say this

00:22:47,220 --> 00:22:50,280
this means we can limit kubernetes

00:22:48,930 --> 00:22:52,290
access from developers

00:22:50,280 --> 00:22:55,350
yeah like the the biggest wing here is

00:22:52,290 --> 00:22:57,030
that we have developers that just don't

00:22:55,350 --> 00:22:59,280
know how to use kubernetes they're not

00:22:57,030 --> 00:23:00,870
familiar with the kubernetes UI they get

00:22:59,280 --> 00:23:02,760
everything they need from spinnaker that

00:23:00,870 --> 00:23:04,830
that's that's just a great place to be

00:23:02,760 --> 00:23:08,100
so let's recap some of those early

00:23:04,830 --> 00:23:09,720
experiences manually creating deployment

00:23:08,100 --> 00:23:11,160
pipelines for each application was

00:23:09,720 --> 00:23:13,080
error-prone did not scale well that's

00:23:11,160 --> 00:23:15,630
taken care of buying pipeline templates

00:23:13,080 --> 00:23:16,980
and automated deployments having s or

00:23:15,630 --> 00:23:18,390
easing the critical path routing and

00:23:16,980 --> 00:23:20,100
configuring specific application

00:23:18,390 --> 00:23:21,930
pipelines was slow and inefficient and

00:23:20,100 --> 00:23:23,940
this is taken care of us by our

00:23:21,930 --> 00:23:25,920
self-service pipeline and configuration

00:23:23,940 --> 00:23:28,080
and deployment architecture and finally

00:23:25,920 --> 00:23:29,970
having s re responsible for day-to-day

00:23:28,080 --> 00:23:32,250
operations of deployment pipelines was

00:23:29,970 --> 00:23:35,310
ineffective and also didn't scale and

00:23:32,250 --> 00:23:38,130
that was taken care of by developers as

00:23:35,310 --> 00:23:39,990
pipeline operators I hope this doesn't

00:23:38,130 --> 00:23:42,000
sound bad but the SRE team wants to

00:23:39,990 --> 00:23:44,340
spend as little time on spinnaker as

00:23:42,000 --> 00:23:46,290
possible and in the past year I looked

00:23:44,340 --> 00:23:47,760
at the number of JIRA tickets that had

00:23:46,290 --> 00:23:51,030
mentioned spinnaker at all then it was

00:23:47,760 --> 00:23:53,010
12% I think I see that as a win that

00:23:51,030 --> 00:23:58,260
includes lots of operational tickets

00:23:53,010 --> 00:24:00,450
like yeah spinnaker fell over so let's

00:23:58,260 --> 00:24:04,200
talk about the safe part the safer

00:24:00,450 --> 00:24:06,450
deployments and we achieve safer

00:24:04,200 --> 00:24:08,580
deployments using Canaries what is a

00:24:06,450 --> 00:24:10,890
canary well it's a deployment that

00:24:08,580 --> 00:24:13,860
incrementally rolls our application

00:24:10,890 --> 00:24:16,020
changes to subsets of users is normally

00:24:13,860 --> 00:24:18,180
done by rerouting some fraction of

00:24:16,020 --> 00:24:19,410
traffic and we validate the behavior

00:24:18,180 --> 00:24:21,780
with that fraction of traffic and then

00:24:19,410 --> 00:24:23,550
we update all the production yeah then

00:24:21,780 --> 00:24:25,580
we are then we update all the production

00:24:23,550 --> 00:24:27,870
if we're happy with the behavior we saw

00:24:25,580 --> 00:24:29,300
so a couple of questions you might have

00:24:27,870 --> 00:24:32,130
if you are familiar with kubernetes

00:24:29,300 --> 00:24:35,040
don't kubernetes deployment rollouts use

00:24:32,130 --> 00:24:37,320
Canaries yes they do but the simplistic

00:24:35,040 --> 00:24:39,600
canary criteria based on liveness all

00:24:37,320 --> 00:24:41,490
readiness checks or if your application

00:24:39,600 --> 00:24:43,500
just falls over immediately

00:24:41,490 --> 00:24:46,440
if deployment has rolled out all the way

00:24:43,500 --> 00:24:49,890
and you want to roll back that's that's

00:24:46,440 --> 00:24:51,600
pretty slow and finally and this is a

00:24:49,890 --> 00:24:53,760
pretty important one traffic routing is

00:24:51,600 --> 00:24:56,490
related to pod counts and this is an

00:24:53,760 --> 00:24:57,900
extreme example but if we had three pods

00:24:56,490 --> 00:24:59,100
in our deployment that were serving a

00:24:57,900 --> 00:25:00,809
hundred percent of traffic and we

00:24:59,100 --> 00:25:03,450
updated just one of them with our new

00:25:00,809 --> 00:25:05,250
image that receives 33 percent of

00:25:03,450 --> 00:25:08,429
traffic because it's evenly distributed

00:25:05,250 --> 00:25:09,809
across the pods kubernetes Canaries are

00:25:08,429 --> 00:25:11,760
great and they're really great if your

00:25:09,809 --> 00:25:13,500
application is broken but then they're

00:25:11,760 --> 00:25:16,860
not so useful if your application has a

00:25:13,500 --> 00:25:19,260
higher error rate we want both but yeah

00:25:16,860 --> 00:25:21,450
we want both using you know this do

00:25:19,260 --> 00:25:23,760
provides a mechanism for fine-grained

00:25:21,450 --> 00:25:25,470
canary rollouts along with the high

00:25:23,760 --> 00:25:27,929
level metrics we need regarding service

00:25:25,470 --> 00:25:30,090
behavior and yeah like I was saying we

00:25:27,929 --> 00:25:33,270
meet we want kubernetes Canaries and

00:25:30,090 --> 00:25:34,980
Asteria Canaries at the same time so you

00:25:33,270 --> 00:25:36,480
might also ask your question we're

00:25:34,980 --> 00:25:39,030
talking about Sto and we're using this

00:25:36,480 --> 00:25:40,799
do for fine-grained routing but can't

00:25:39,030 --> 00:25:43,950
spinnaker manage traffic routing and it

00:25:40,799 --> 00:25:46,950
can and I will admit we haven't really

00:25:43,950 --> 00:25:48,870
heavily explored this but the current

00:25:46,950 --> 00:25:50,640
model I believe relies on replica sets

00:25:48,870 --> 00:25:56,490
and it has the same traffic routing

00:25:50,640 --> 00:25:58,169
constraints as I described above in

00:25:56,490 --> 00:26:00,540
order to do a canary you need a baseline

00:25:58,169 --> 00:26:03,480
so what is a baseline well in our

00:26:00,540 --> 00:26:05,160
original canary pipeline we compared the

00:26:03,480 --> 00:26:07,040
canary against our release deployment we

00:26:05,160 --> 00:26:10,950
thought why do we need a baseline and

00:26:07,040 --> 00:26:12,510
this has several issues so one obvious

00:26:10,950 --> 00:26:14,370
issue is the release deployment is

00:26:12,510 --> 00:26:16,290
stable or scaling down while the canary

00:26:14,370 --> 00:26:17,429
is scaling up your release was just

00:26:16,290 --> 00:26:19,620
receiving a hundred percent of traffic

00:26:17,429 --> 00:26:22,860
you start doing a canary it's now

00:26:19,620 --> 00:26:25,260
receiving less traffic you know it might

00:26:22,860 --> 00:26:27,330
be scaling down so we know that the

00:26:25,260 --> 00:26:29,669
canary is seeing more traffic volume per

00:26:27,330 --> 00:26:31,320
pod I sorry there really yeah the canary

00:26:29,669 --> 00:26:33,929
is seeing more traffic volume per pod

00:26:31,320 --> 00:26:36,299
than the release another big issue is

00:26:33,929 --> 00:26:37,950
the releases warmed up you know it's got

00:26:36,299 --> 00:26:39,720
stuff cast it's got open upstream

00:26:37,950 --> 00:26:41,549
connections it's obviously not the same

00:26:39,720 --> 00:26:43,860
as our scaling up Canary deployment and

00:26:41,549 --> 00:26:46,049
then a more subtle problem you can run

00:26:43,860 --> 00:26:49,470
into but the release deployment might

00:26:46,049 --> 00:26:51,360
have slow burning problems so if you

00:26:49,470 --> 00:26:53,970
have a slow memory leak that was in your

00:26:51,360 --> 00:26:54,659
release deployment that's also in your

00:26:53,970 --> 00:26:58,559
new canary

00:26:54,659 --> 00:27:00,419
that's rolling out if we deployed the

00:26:58,559 --> 00:27:02,820
canary or sorry if we start rolling out

00:27:00,419 --> 00:27:04,619
the canary and it's got a more serious

00:27:02,820 --> 00:27:06,899
memory consumption problem a problem

00:27:04,619 --> 00:27:09,599
that's actually going to bite us then

00:27:06,899 --> 00:27:11,129
that problem can be masked because the

00:27:09,599 --> 00:27:12,570
release deployments been out there for a

00:27:11,129 --> 00:27:14,849
while its memory consumptions being

00:27:12,570 --> 00:27:16,919
growing and it hides the newly

00:27:14,849 --> 00:27:20,759
introduced problem and that that's a

00:27:16,919 --> 00:27:22,799
more subtle failure mode so use your

00:27:20,759 --> 00:27:26,460
baseline deployment which is where we

00:27:22,799 --> 00:27:28,739
actually do a deployment a new fresh

00:27:26,460 --> 00:27:30,869
deployment of our release image and

00:27:28,739 --> 00:27:33,840
scale that in just the same way as our

00:27:30,869 --> 00:27:36,679
canary under the same conditions is the

00:27:33,840 --> 00:27:41,849
best way to meaningfully compare

00:27:36,679 --> 00:27:43,409
application versions so how does adding

00:27:41,849 --> 00:27:45,299
a canary change our continuous

00:27:43,409 --> 00:27:48,299
deployment pipeline well it's almost

00:27:45,299 --> 00:27:50,460
exactly the same as before but we've

00:27:48,299 --> 00:27:53,220
replaced the manual approval stage with

00:27:50,460 --> 00:27:55,789
a stage that deploys the baseline which

00:27:53,220 --> 00:27:58,320
again is a copy of our release image and

00:27:55,789 --> 00:28:00,960
the canary which is our new image that's

00:27:58,320 --> 00:28:03,659
been deployed and then we run the canary

00:28:00,960 --> 00:28:06,570
rollout and that's that's running the

00:28:03,659 --> 00:28:08,359
canary rollout it means run a separate

00:28:06,570 --> 00:28:10,710
pipeline and when we run this pipeline

00:28:08,359 --> 00:28:12,479
we already have our baseline we have our

00:28:10,710 --> 00:28:14,309
canary we have you know we have our

00:28:12,479 --> 00:28:16,320
release image we don't need to do any

00:28:14,309 --> 00:28:19,320
more deployments we just need to switch

00:28:16,320 --> 00:28:20,509
traffic to the baseline canary and see

00:28:19,320 --> 00:28:24,029
what happens

00:28:20,509 --> 00:28:26,399
and so at the beginning of this we got a

00:28:24,029 --> 00:28:28,109
canary rollout and it's 10% and that

00:28:26,399 --> 00:28:29,999
that means we're going to send 80% of

00:28:28,109 --> 00:28:32,700
our traffic to release and 10% of

00:28:29,999 --> 00:28:36,509
traffic to our baseline 10% talk and

00:28:32,700 --> 00:28:38,489
area and release again means the

00:28:36,509 --> 00:28:40,019
endpoint that had been up until that

00:28:38,489 --> 00:28:42,359
point receiving a hundred percent of

00:28:40,019 --> 00:28:44,309
customer traffic how do we do this

00:28:42,359 --> 00:28:45,629
change in routing where we use this to

00:28:44,309 --> 00:28:49,799
your virtual services and I'll show you

00:28:45,629 --> 00:28:52,049
how that works in just a moment at the

00:28:49,799 --> 00:28:55,440
analysis stage that this is the first

00:28:52,049 --> 00:28:58,470
analysis stage we warm up for one minute

00:28:55,440 --> 00:29:01,289
and then we collect data for 10 minutes

00:28:58,470 --> 00:29:03,450
and we verify that the canary is healthy

00:29:01,289 --> 00:29:08,099
this is an extremely short window of

00:29:03,450 --> 00:29:08,640
time but it's a good first sanity check

00:29:08,099 --> 00:29:10,620
and

00:29:08,640 --> 00:29:12,510
I was very interested to hear are we

00:29:10,620 --> 00:29:14,460
talking about 15 minute Canaries

00:29:12,510 --> 00:29:18,570
yesterday if you change that it's very

00:29:14,460 --> 00:29:20,700
small you can see that there's a 100

00:29:18,570 --> 00:29:22,920
next to the stage and that means we got

00:29:20,700 --> 00:29:25,890
a score of 100 out of a hundred and I'll

00:29:22,920 --> 00:29:28,049
explain how that works in a moment so

00:29:25,890 --> 00:29:31,710
this past were happy with the canary so

00:29:28,049 --> 00:29:33,330
far and so now we update routing so

00:29:31,710 --> 00:29:35,670
we're sending thirty-four percent of

00:29:33,330 --> 00:29:37,860
traffic to our release endpoint and you

00:29:35,670 --> 00:29:41,370
know 33 percent of the base time and 33

00:29:37,860 --> 00:29:43,110
percent to the canary so this final

00:29:41,370 --> 00:29:46,470
stage were collecting data for 20

00:29:43,110 --> 00:29:48,360
minutes and we have no warm-up and at

00:29:46,470 --> 00:29:51,000
the end of that we verify that the

00:29:48,360 --> 00:29:52,590
canary is healthy and so in this case

00:29:51,000 --> 00:29:54,660
you see the pipe line was a success and

00:29:52,590 --> 00:29:58,500
this would have resulted in our image

00:29:54,660 --> 00:30:00,960
that was being canary being pushed to

00:29:58,500 --> 00:30:03,270
our production endpoint really simple

00:30:00,960 --> 00:30:05,010
and you may be able to see there's a

00:30:03,270 --> 00:30:09,290
little red exclamation mark next to the

00:30:05,010 --> 00:30:13,440
100 score on the canary analysis pane

00:30:09,290 --> 00:30:16,320
that's that's because the the analysis

00:30:13,440 --> 00:30:19,320
is unhappy because we collected less

00:30:16,320 --> 00:30:21,510
than 50 samples so we're using

00:30:19,320 --> 00:30:23,669
stackdriver as our metric source there's

00:30:21,510 --> 00:30:26,429
the way we're doing our creations we end

00:30:23,669 --> 00:30:28,380
up with one sample per minute so for a

00:30:26,429 --> 00:30:31,679
20 minute canary we've got roughly 20

00:30:28,380 --> 00:30:33,600
data points I know I've heard for

00:30:31,679 --> 00:30:36,630
example that Netflix recommends three

00:30:33,600 --> 00:30:39,720
one-hour Canaries and with five deploys

00:30:36,630 --> 00:30:41,700
per day that would be 15 hours so at the

00:30:39,720 --> 00:30:44,010
moment that model wouldn't work for us

00:30:41,700 --> 00:30:48,299
with this sequential deployment pattern

00:30:44,010 --> 00:30:50,220
for our services so you saw we were

00:30:48,299 --> 00:30:51,360
getting 100 out of 100 but what what

00:30:50,220 --> 00:30:54,419
does that even mean where does it come

00:30:51,360 --> 00:30:58,650
from how do we define these metrics and

00:30:54,419 --> 00:31:01,559
and here is how we we set up the default

00:30:58,650 --> 00:31:04,740
metric set for our Canaries and so we

00:31:01,559 --> 00:31:07,020
call this a simple metric so that's what

00:31:04,740 --> 00:31:09,000
this group is called and so you can see

00:31:07,020 --> 00:31:11,640
that we've got just four metrics we

00:31:09,000 --> 00:31:13,169
check two of the mares geometric so we

00:31:11,640 --> 00:31:15,390
look at the number of healthy responses

00:31:13,169 --> 00:31:17,940
and we look at the 95th percentile

00:31:15,390 --> 00:31:19,440
latency for our application and two of

00:31:17,940 --> 00:31:22,320
our metrics that come from kubernetes

00:31:19,440 --> 00:31:25,320
they look at the CPU utilization purple

00:31:22,320 --> 00:31:27,150
and the memory utilization and at the

00:31:25,320 --> 00:31:29,520
bottom here you can see there's metric

00:31:27,150 --> 00:31:32,070
group weights and that sets up how much

00:31:29,520 --> 00:31:33,830
each of your metric how much the result

00:31:32,070 --> 00:31:36,750
of your metric succeeding or failing

00:31:33,830 --> 00:31:38,730
weighs and so the total of that has to

00:31:36,750 --> 00:31:41,820
add up to a hundred and here we've said

00:31:38,730 --> 00:31:44,400
sto metrics of weighed 60 and so that

00:31:41,820 --> 00:31:46,770
means if either of them sto metrics fail

00:31:44,400 --> 00:31:49,170
we lose 30 points and if both fail we

00:31:46,770 --> 00:31:51,810
lose 60 points similarly if we just had

00:31:49,170 --> 00:31:54,240
a kubernetes metrics or setup with a

00:31:51,810 --> 00:31:57,060
weight of 40 and so if either of those

00:31:54,240 --> 00:32:01,920
fail we just lose 20 points hopefully

00:31:57,060 --> 00:32:05,790
that's clear how this actually feeds

00:32:01,920 --> 00:32:07,410
into the pipeline as shown here so on

00:32:05,790 --> 00:32:11,160
the left hand side you can see how we

00:32:07,410 --> 00:32:13,740
configured an individual metric and here

00:32:11,160 --> 00:32:16,620
this is the 95th percentile and so we've

00:32:13,740 --> 00:32:20,700
said this should fail on an increase in

00:32:16,620 --> 00:32:22,920
the latency the metric type here that's

00:32:20,700 --> 00:32:25,230
defined that tells spinnaker what

00:32:22,920 --> 00:32:27,150
specific metric we want to fetch from

00:32:25,230 --> 00:32:30,210
our metric service which in our case is

00:32:27,150 --> 00:32:32,760
stackdriver and then at the bottom the

00:32:30,210 --> 00:32:35,190
filter tells the analysis stage how to

00:32:32,760 --> 00:32:37,770
extract the actual points from that

00:32:35,190 --> 00:32:40,350
metric for the canary versus the point

00:32:37,770 --> 00:32:42,450
we want for the baseline and it does

00:32:40,350 --> 00:32:44,970
this by inserting the values that we

00:32:42,450 --> 00:32:47,850
define for the baseline and the canary

00:32:44,970 --> 00:32:50,040
in the canary analysis configuration and

00:32:47,850 --> 00:32:51,780
that's shown on the right and so the the

00:32:50,040 --> 00:32:54,840
key word you see set for the the

00:32:51,780 --> 00:32:58,200
baseline is my service - baseline and

00:32:54,840 --> 00:33:00,150
when when the canary analysis stage

00:32:58,200 --> 00:33:03,390
wants to fetch the metrics for the

00:33:00,150 --> 00:33:05,970
baseline it's going to insert my service

00:33:03,390 --> 00:33:07,700
baseline as metric label destination

00:33:05,970 --> 00:33:09,780
underscore service underscore name

00:33:07,700 --> 00:33:13,050
hopefully that makes sense so that's how

00:33:09,780 --> 00:33:15,390
the metric analysis is able to the

00:33:13,050 --> 00:33:17,910
canary analysis is able to grab a group

00:33:15,390 --> 00:33:19,110
of points that represent the baseline

00:33:17,910 --> 00:33:22,800
performance and a group of points that

00:33:19,110 --> 00:33:24,690
represent their canary the deployment

00:33:22,800 --> 00:33:26,700
performance and I'm sort of laboring

00:33:24,690 --> 00:33:29,360
this point because it's pretty hard to

00:33:26,700 --> 00:33:29,360
get your head around this

00:33:29,880 --> 00:33:35,200
so we're dynamically steering traffic we

00:33:33,370 --> 00:33:37,090
send 10% to like an area and then we

00:33:35,200 --> 00:33:40,270
send 33% to a canary and you're probably

00:33:37,090 --> 00:33:43,049
wondering what complicated magic we're

00:33:40,270 --> 00:33:45,880
using to do that and it is in fact

00:33:43,049 --> 00:33:48,370
incredibly simple this is the guts of it

00:33:45,880 --> 00:33:50,500
we dynamically steer our traffic with

00:33:48,370 --> 00:33:53,110
spinnaker and sto so on the left you can

00:33:50,500 --> 00:33:54,789
see just a regular deploying manifest

00:33:53,110 --> 00:33:55,299
stage it's really short it's really

00:33:54,789 --> 00:33:57,160
simple

00:33:55,299 --> 00:33:58,690
and in this two-point manifest stage

00:33:57,160 --> 00:34:02,140
we've added a bunch of variables we've

00:33:58,690 --> 00:34:04,570
set release weight to 80 for example and

00:34:02,140 --> 00:34:06,130
that means 80% of traffic goes through

00:34:04,570 --> 00:34:08,560
the release that's what we want it to me

00:34:06,130 --> 00:34:10,570
and the way that works is over on the

00:34:08,560 --> 00:34:12,940
right in our sto virtual service

00:34:10,570 --> 00:34:14,800
manifest we've said okay when you're

00:34:12,940 --> 00:34:17,109
deploying this manifest fetch the

00:34:14,800 --> 00:34:19,330
release weight from the stage that's

00:34:17,109 --> 00:34:20,980
deploying me and insert that value is

00:34:19,330 --> 00:34:23,980
the weight and so that's how we're able

00:34:20,980 --> 00:34:27,040
to set the release to 80 percent the

00:34:23,980 --> 00:34:30,790
baseline to 10 percent and the canary

00:34:27,040 --> 00:34:32,290
weight to 10 percent and this example of

00:34:30,790 --> 00:34:35,020
a virtual service you see where for our

00:34:32,290 --> 00:34:38,320
release M put like for a traffic coming

00:34:35,020 --> 00:34:39,250
into my service we we're out to the

00:34:38,320 --> 00:34:41,740
three endpoints

00:34:39,250 --> 00:34:43,060
that's what we use all the time even

00:34:41,740 --> 00:34:45,429
when we're not in the middle of a canary

00:34:43,060 --> 00:34:49,119
it's the same template we just have 100%

00:34:45,429 --> 00:34:52,210
of traffic going to put our release

00:34:49,119 --> 00:34:56,770
endpoint normally and so like release

00:34:52,210 --> 00:34:58,869
deployment and so yeah we're dependent

00:34:56,770 --> 00:35:00,609
on some pretty complicated issue

00:34:58,869 --> 00:35:03,220
capabilities behind the scenes there's

00:35:00,609 --> 00:35:04,540
virtual serve rule merging and it's kind

00:35:03,220 --> 00:35:06,010
of interesting topic if you want to talk

00:35:04,540 --> 00:35:12,850
about it but you don't really even need

00:35:06,010 --> 00:35:14,650
to know about that so you might have

00:35:12,850 --> 00:35:16,180
some questions about our canary analysis

00:35:14,650 --> 00:35:18,430
and I quit canary roll-out and I

00:35:16,180 --> 00:35:21,430
wouldn't blame you so why 10% and 33%

00:35:18,430 --> 00:35:23,740
well 10% provides enough metric volume

00:35:21,430 --> 00:35:25,630
for many of our services and it does

00:35:23,740 --> 00:35:28,570
still keep the impact surface fairly

00:35:25,630 --> 00:35:30,810
small 33% of traffic is you know we

00:35:28,570 --> 00:35:33,940
found it's statistically representative

00:35:30,810 --> 00:35:35,950
obviously we couldn't go above 50% in

00:35:33,940 --> 00:35:37,990
this existing model where that would be

00:35:35,950 --> 00:35:39,609
50% of the canary and 50% of the

00:35:37,990 --> 00:35:41,710
baseline and at that point nothing would

00:35:39,609 --> 00:35:44,440
be going to the release deployment

00:35:41,710 --> 00:35:46,630
are two stages enough well more stages

00:35:44,440 --> 00:35:50,290
means more complexity and a slower

00:35:46,630 --> 00:35:53,320
deployment rollout are there scaling

00:35:50,290 --> 00:35:56,590
problems well okay one problem that I'm

00:35:53,320 --> 00:35:57,880
sure it definitely happens is that when

00:35:56,590 --> 00:35:59,950
we're doing our Canaria baseline because

00:35:57,880 --> 00:36:02,710
of such a short warm-up in such a short

00:35:59,950 --> 00:36:05,500
analysis stage deployments are often

00:36:02,710 --> 00:36:07,240
scaling up and that's one of the big

00:36:05,500 --> 00:36:08,440
benefits of comparing against the

00:36:07,240 --> 00:36:09,790
baseline that can area against the

00:36:08,440 --> 00:36:12,070
baseline because that's scaling up under

00:36:09,790 --> 00:36:15,670
exactly the same circumstances and I'm

00:36:12,070 --> 00:36:18,070
going to make the crappy argument that

00:36:15,670 --> 00:36:21,310
you know this may actually represent

00:36:18,070 --> 00:36:23,950
more of an extreme load scenario than

00:36:21,310 --> 00:36:27,250
then if it had stabilized which we might

00:36:23,950 --> 00:36:30,040
want another thing to think about is if

00:36:27,250 --> 00:36:32,320
we're at a point where only 34 percent

00:36:30,040 --> 00:36:34,000
of traffic is going to our release and

00:36:32,320 --> 00:36:35,710
the canary fails or even if the canary

00:36:34,000 --> 00:36:37,960
succeeds but let's say the canary fails

00:36:35,710 --> 00:36:40,390
we'll switch to 100% of traffic back to

00:36:37,960 --> 00:36:42,520
release and you know it was 20 minutes

00:36:40,390 --> 00:36:43,900
we were doing that canary analysis for

00:36:42,520 --> 00:36:46,120
our release could have very easily

00:36:43,900 --> 00:36:48,910
scaled down in that timeframe and we

00:36:46,120 --> 00:36:50,860
just tripled the traffic well we kind of

00:36:48,910 --> 00:36:53,080
short-circuit that problem I mentioned

00:36:50,860 --> 00:36:55,600
before that we have a custom HBA that

00:36:53,080 --> 00:36:57,100
scales based on connection counts and so

00:36:55,600 --> 00:36:59,380
what we can do is we just could figure

00:36:57,100 --> 00:37:01,660
the release endpoint to scale both based

00:36:59,380 --> 00:37:03,850
on the total connections across the

00:37:01,660 --> 00:37:05,890
release canary and baseline so that

00:37:03,850 --> 00:37:08,020
never scales down or it only scales

00:37:05,890 --> 00:37:13,060
appropriately with the changes in total

00:37:08,020 --> 00:37:15,220
traffic volume so let's look at some of

00:37:13,060 --> 00:37:17,290
the results of the analysis when you're

00:37:15,220 --> 00:37:18,610
actually looking at the canary analysis

00:37:17,290 --> 00:37:20,620
stages you can click a link and it will

00:37:18,610 --> 00:37:23,110
take you to the results so we'll look at

00:37:20,620 --> 00:37:25,930
a couple of those so here's an example

00:37:23,110 --> 00:37:27,910
of a successful canary analysis you can

00:37:25,930 --> 00:37:31,540
see all the numbers look pretty same and

00:37:27,910 --> 00:37:33,400
I've clicked on CPU utilization now it's

00:37:31,540 --> 00:37:35,050
easier to physically reason about the

00:37:33,400 --> 00:37:37,180
results when you look at the time series

00:37:35,050 --> 00:37:39,040
or the histogram views but it's easier

00:37:37,180 --> 00:37:40,870
to understand the canary judgment when

00:37:39,040 --> 00:37:42,280
you look at the B swarm plot and that's

00:37:40,870 --> 00:37:44,530
what I'm showing you here you can see

00:37:42,280 --> 00:37:46,260
well the canary is pretty statistically

00:37:44,530 --> 00:37:49,260
representative of the baseline

00:37:46,260 --> 00:37:49,260
performance

00:37:49,710 --> 00:37:53,560
here's an example where a developer had

00:37:52,300 --> 00:37:55,630
a really good idea to improve

00:37:53,560 --> 00:37:57,160
performance they realized

00:37:55,630 --> 00:38:00,670
yeah they realized they they had a

00:37:57,160 --> 00:38:02,710
better approach and so it got approved

00:38:00,670 --> 00:38:05,020
it got reviewed approved and merged and

00:38:02,710 --> 00:38:06,640
they were right it's probably hard to

00:38:05,020 --> 00:38:09,190
tell from where you're sitting but the

00:38:06,640 --> 00:38:13,960
number of healthy responses went up 23%

00:38:09,190 --> 00:38:15,849
the the total count and the 23% relative

00:38:13,960 --> 00:38:17,560
to the baseline and their 95th

00:38:15,849 --> 00:38:20,680
percentile latency actually dropped

00:38:17,560 --> 00:38:23,920
dropped by I think it's five five

00:38:20,680 --> 00:38:26,740
percent sorry yeah the latency dropped

00:38:23,920 --> 00:38:30,579
by five percent so so far so good but

00:38:26,740 --> 00:38:33,490
the CPU utilization doubled and the

00:38:30,579 --> 00:38:38,260
memory utilization went up by roughly a

00:38:33,490 --> 00:38:43,089
third and so in this case the the canary

00:38:38,260 --> 00:38:44,859
analysis failed and and we can go away

00:38:43,089 --> 00:38:47,260
we can have a discussion are we willing

00:38:44,859 --> 00:38:49,390
to accept you know this change in

00:38:47,260 --> 00:38:54,430
performance characteristics in order to

00:38:49,390 --> 00:38:56,020
get these gains we might be ok but but

00:38:54,430 --> 00:38:58,810
we'll have to manually by it bypass

00:38:56,020 --> 00:39:00,250
family ok so I'm going to quickly walk

00:38:58,810 --> 00:39:02,380
you through a tale of two Canaries

00:39:00,250 --> 00:39:04,750
so here is we're looking at those two

00:39:02,380 --> 00:39:06,880
canary analysis stages you just viewed

00:39:04,750 --> 00:39:10,390
so here this is showing the traffic

00:39:06,880 --> 00:39:12,730
volume to release at version 1

00:39:10,390 --> 00:39:13,960
deployment that's the Purple Line and at

00:39:12,730 --> 00:39:15,609
the bottom you can see a new canary

00:39:13,960 --> 00:39:17,680
comes out that's at version 2 and the

00:39:15,609 --> 00:39:20,829
baseline comes out and that's obviously

00:39:17,680 --> 00:39:22,300
at version 1 so first we've got 10% of

00:39:20,829 --> 00:39:23,980
traffic going to the canary and baseline

00:39:22,300 --> 00:39:26,109
and 80% to our release endpoint that

00:39:23,980 --> 00:39:27,520
starts to scale down and the canary

00:39:26,109 --> 00:39:31,060
baseline you see the traffic volume

00:39:27,520 --> 00:39:33,310
appear on the graph this first stage

00:39:31,060 --> 00:39:35,380
passes and so now we're in a position

00:39:33,310 --> 00:39:37,480
where we want to send 33% of traffic to

00:39:35,380 --> 00:39:39,430
our canary and baseline and you see them

00:39:37,480 --> 00:39:43,060
scale up as we change the traffic

00:39:39,430 --> 00:39:44,770
routing and this second canary succeeds

00:39:43,060 --> 00:39:46,810
and so we start rolling out a new

00:39:44,770 --> 00:39:48,970
deployment of release using the version

00:39:46,810 --> 00:39:50,500
2 image and you can see all the other

00:39:48,970 --> 00:39:52,329
deployments scaled down in terms of

00:39:50,500 --> 00:39:55,750
traffic volume and the release at

00:39:52,329 --> 00:39:57,609
version 2 scales up and so now we have

00:39:55,750 --> 00:40:01,219
no traffic code on canary no traffic go

00:39:57,609 --> 00:40:04,789
to a baseline 100% to our release

00:40:01,219 --> 00:40:07,369
so now this is the second failed canary

00:40:04,789 --> 00:40:10,400
analysis state you saw later on and in

00:40:07,369 --> 00:40:12,319
this case we we have a canary at version

00:40:10,400 --> 00:40:14,239
three and our release of baseline and

00:40:12,319 --> 00:40:15,829
our at version two we have 10% of

00:40:14,239 --> 00:40:18,950
traffic go to the canary and baseline

00:40:15,829 --> 00:40:22,160
and well this one actually fails at the

00:40:18,950 --> 00:40:23,329
10% analysis stage and so we starts only

00:40:22,160 --> 00:40:25,910
a hundred percent of traffic back to

00:40:23,329 --> 00:40:29,660
release and we delete the canary in

00:40:25,910 --> 00:40:31,420
baseline so do Canaries canary sound

00:40:29,660 --> 00:40:35,089
great do they solve all of our problems

00:40:31,420 --> 00:40:36,589
no they're not a magic bullet we run

00:40:35,089 --> 00:40:39,619
into all sorts of problems you can have

00:40:36,589 --> 00:40:42,109
we've got a ton of node and pod churn in

00:40:39,619 --> 00:40:43,849
our cluster as it scales up and down and

00:40:42,109 --> 00:40:45,709
so we can see problems with kubernetes

00:40:43,849 --> 00:40:47,959
we can see problems with nodes where we

00:40:45,709 --> 00:40:50,119
can't provision a node for the canary or

00:40:47,959 --> 00:40:52,489
baseline of run on or one get scheduled

00:40:50,119 --> 00:40:54,589
and the other dozen we see things like

00:40:52,489 --> 00:40:56,450
429 coming back from the stack driver

00:40:54,589 --> 00:41:00,650
metrics at endpoint we've hit our quota

00:40:56,450 --> 00:41:02,989
and I don't think the canary analysis

00:41:00,650 --> 00:41:05,119
stage retries the pull of metrics and so

00:41:02,989 --> 00:41:08,359
that that throws an error and the canary

00:41:05,119 --> 00:41:11,329
fails but overall Canaries seem to be

00:41:08,359 --> 00:41:12,859
working very well so how does this all

00:41:11,329 --> 00:41:15,680
work out well let's look at some of the

00:41:12,859 --> 00:41:17,299
numbers we got 16 services actively

00:41:15,680 --> 00:41:20,479
deployed bar continues to delivery

00:41:17,299 --> 00:41:22,999
pipelining and we averaged 3 deploys per

00:41:20,479 --> 00:41:24,440
day for each service although I will be

00:41:22,999 --> 00:41:28,999
honest fewer than that make it the

00:41:24,440 --> 00:41:30,739
release endpoint at the at a recent time

00:41:28,999 --> 00:41:32,839
I looked at the number of row backs and

00:41:30,739 --> 00:41:35,479
we haven't had any row backs for three

00:41:32,839 --> 00:41:37,789
months and I guess one of the most

00:41:35,479 --> 00:41:39,890
important points for me is we spent four

00:41:37,789 --> 00:41:42,289
engineer months on spinnaker tickets in

00:41:39,890 --> 00:41:44,289
the past 12 months and that includes all

00:41:42,289 --> 00:41:47,479
of the work you see here pretty much and

00:41:44,289 --> 00:41:50,089
lots of operational time so that's

00:41:47,479 --> 00:41:53,029
fixing cloud driver fell over Redis is

00:41:50,089 --> 00:41:55,219
joking retrofit 500 you know the drill

00:41:53,029 --> 00:41:58,599
and and of course upgrading spinnaker

00:41:55,219 --> 00:42:01,759
when there are new features and releases

00:41:58,599 --> 00:42:03,440
so in conclusion combining kubernetes

00:42:01,759 --> 00:42:05,059
SEO and spinnaker can help you build a

00:42:03,440 --> 00:42:06,529
deployment ecosystem that allows for

00:42:05,059 --> 00:42:09,199
safe mission-critical continuous

00:42:06,529 --> 00:42:10,969
delivery without sacrificing velocity if

00:42:09,199 --> 00:42:12,199
you focus your resources on building out

00:42:10,969 --> 00:42:14,160
our self-service pipeline architecture

00:42:12,199 --> 00:42:15,869
which scales across the

00:42:14,160 --> 00:42:18,720
teams that can increase application

00:42:15,869 --> 00:42:19,920
deployment velocity high frequency

00:42:18,720 --> 00:42:21,660
deployments with a short lead time for

00:42:19,920 --> 00:42:23,339
changes and application level Canaries

00:42:21,660 --> 00:42:25,799
can help provide the confidence you need

00:42:23,339 --> 00:42:27,329
for fully automated deployment into

00:42:25,799 --> 00:42:30,569
production and all these factors

00:42:27,329 --> 00:42:33,990
together improve reliability with quite

00:42:30,569 --> 00:42:35,430
constrained engineering resources what's

00:42:33,990 --> 00:42:38,609
next for descartes what are we thinking

00:42:35,430 --> 00:42:41,339
about doing next well we're we're very

00:42:38,609 --> 00:42:45,210
interested in maintaining SL O's and

00:42:41,339 --> 00:42:46,829
where we have multiple window SL O's and

00:42:45,210 --> 00:42:49,349
we're talking about implementing a

00:42:46,829 --> 00:42:52,260
feature I'm going to call SL whoa I'm

00:42:49,349 --> 00:42:54,289
very sorry for that which will disable

00:42:52,260 --> 00:42:59,220
automated Canaries for new feature

00:42:54,289 --> 00:43:00,240
deployments but but will it still allow

00:42:59,220 --> 00:43:02,640
bug fix history

00:43:00,240 --> 00:43:04,710
speaking of which we want better

00:43:02,640 --> 00:43:06,690
tracking between feature development and

00:43:04,710 --> 00:43:09,030
deploys we want to link our spinnaker

00:43:06,690 --> 00:43:11,400
deployments back to zero which is what

00:43:09,030 --> 00:43:13,950
we use we want to know if how many

00:43:11,400 --> 00:43:17,099
failed Canaries or rollbacks or bug

00:43:13,950 --> 00:43:19,289
fixes or release ended up in and we want

00:43:17,099 --> 00:43:20,880
to have a finally we want to have an

00:43:19,289 --> 00:43:22,470
option to mirror production traffic

00:43:20,880 --> 00:43:25,279
instead of splitting it for some of our

00:43:22,470 --> 00:43:27,750
servers services where that's safe to do

00:43:25,279 --> 00:43:30,299
here's the Descartes lab so sorry team

00:43:27,750 --> 00:43:32,039
Nora Lutz is my partner in crime when it

00:43:30,299 --> 00:43:34,349
comes to spinnaker development and

00:43:32,039 --> 00:43:37,440
operational work Rob's almond is

00:43:34,349 --> 00:43:40,529
terraform wizard and genius and to

00:43:37,440 --> 00:43:42,119
Kelton is our boss and I just go fade

00:43:40,529 --> 00:43:43,829
down in at the end he just joined our

00:43:42,119 --> 00:43:46,140
team and would have changed my numbers

00:43:43,829 --> 00:43:48,630
if I if I included him in terms of

00:43:46,140 --> 00:43:51,210
engineer out hours over yeah damn damn

00:43:48,630 --> 00:43:53,369
they just moved to the ice routine okay

00:43:51,210 --> 00:43:56,069
thank you very much

00:43:53,369 --> 00:43:58,529
I do have any questions I will say

00:43:56,069 --> 00:44:00,930
before I take any questions that we that

00:43:58,529 --> 00:44:04,470
I did put this talk and some example

00:44:00,930 --> 00:44:08,690
stuff in my github repo which you can

00:44:04,470 --> 00:44:08,690
see at the following link okay thank you

00:44:09,670 --> 00:44:20,329
[Applause]

00:44:15,170 --> 00:44:20,329
oK we've got somebody with a mic

00:44:21,990 --> 00:44:30,450
hey I'm tiny I do have questions more of

00:44:25,650 --> 00:44:34,680
my stuff we are using a burst encoded as

00:44:30,450 --> 00:44:39,089
insulin proxy but we are the service

00:44:34,680 --> 00:44:41,190
wish huh but not as a proxy to respect

00:44:39,089 --> 00:44:43,140
authorization so whatever your

00:44:41,190 --> 00:44:44,819
experiences with authorization and how

00:44:43,140 --> 00:44:52,589
because it isn't the best can because

00:44:44,819 --> 00:44:55,800
authorization on spirit okay so I will

00:44:52,589 --> 00:44:57,740
say that we have M TLS mode set

00:44:55,800 --> 00:44:59,790
permissive so it's basically an opt-in

00:44:57,740 --> 00:45:02,220
authorization model for service to

00:44:59,790 --> 00:45:06,450
service the other thing we're doing is

00:45:02,220 --> 00:45:08,869
JWT validation at the ingress I'm not

00:45:06,450 --> 00:45:21,390
sure if that answers your question

00:45:08,869 --> 00:45:22,410
okay and we do okay I just realized

00:45:21,390 --> 00:45:24,540
though I talked more about

00:45:22,410 --> 00:45:27,380
authentication rather than authorization

00:45:24,540 --> 00:45:30,119
so we do have selective routing in

00:45:27,380 --> 00:45:32,819
virtual service configuration based on

00:45:30,119 --> 00:45:35,930
your group membership so that's more of

00:45:32,819 --> 00:45:35,930
an authorization answer

00:45:40,280 --> 00:45:45,680
hey my name is - they see one of your

00:45:44,120 --> 00:45:54,080
pipe that you're talking about the

00:45:45,680 --> 00:45:55,700
schema migration process - that's a

00:45:54,080 --> 00:45:59,950
great question

00:45:55,700 --> 00:46:03,380
so I know we use Alembic I think for our

00:45:59,950 --> 00:46:07,370
elasticsearch schema migrations we

00:46:03,380 --> 00:46:12,260
manage them ourselves but like we we

00:46:07,370 --> 00:46:13,610
just do the we explicitly do the schema

00:46:12,260 --> 00:46:16,100
migration and we always make sure our

00:46:13,610 --> 00:46:18,350
schema migrations what's the right way

00:46:16,100 --> 00:46:20,420
to put it like idempotent and additive

00:46:18,350 --> 00:46:22,250
so we can always roll back that's what

00:46:20,420 --> 00:46:24,140
for es it you could always deploy any

00:46:22,250 --> 00:46:25,820
version of our service and it should

00:46:24,140 --> 00:46:28,040
still work there may be some exceptions

00:46:25,820 --> 00:46:29,420
around that but that that's the

00:46:28,040 --> 00:46:38,180
philosophy there and that simplifies a

00:46:29,420 --> 00:46:42,320
lot a lot of problems yes do we actually

00:46:38,180 --> 00:46:46,910
do we do the snapshots of our es cluster

00:46:42,320 --> 00:46:48,530
every single day but the the point was

00:46:46,910 --> 00:46:50,360
really if we actually deployed an older

00:46:48,530 --> 00:46:52,820
version of our service there are going

00:46:50,360 --> 00:46:55,700
to be exceptions to this but the

00:46:52,820 --> 00:46:57,680
philosophy of our schema changes is such

00:46:55,700 --> 00:47:00,130
that though those older versions should

00:46:57,680 --> 00:47:00,130
still work

00:47:04,150 --> 00:47:08,830
a great presentation so a question I had

00:47:07,360 --> 00:47:11,560
it's something there's a baseline

00:47:08,830 --> 00:47:14,230
templates are those centrally bandaged

00:47:11,560 --> 00:47:17,350
are like can variously application under

00:47:14,230 --> 00:47:18,730
so for example we have secured Z I may

00:47:17,350 --> 00:47:22,060
have a quality team may have all these

00:47:18,730 --> 00:47:24,790
teams do have a medicine cabinet or the

00:47:22,060 --> 00:47:26,650
coming only that sorry block so by the

00:47:24,790 --> 00:47:28,750
baseline templates do you mean the

00:47:26,650 --> 00:47:40,360
configuration for metrics or do you mean

00:47:28,750 --> 00:47:41,490
the deployment manifest see yeah like I

00:47:40,360 --> 00:47:44,730
said that lives a long application

00:47:41,490 --> 00:47:48,910
alongside an application code and

00:47:44,730 --> 00:47:52,210
there's just one size fits all you know

00:47:48,910 --> 00:47:54,130
all developers work within mono repo I

00:47:52,210 --> 00:47:55,660
mean that's not strictly true but all

00:47:54,130 --> 00:47:59,320
developers are deploying to spinnaker

00:47:55,660 --> 00:48:00,490
working on a repo I will say that there

00:47:59,320 --> 00:48:02,560
are some constraints by having

00:48:00,490 --> 00:48:05,530
deployment configuration live alongside

00:48:02,560 --> 00:48:06,910
application code and we have workarounds

00:48:05,530 --> 00:48:10,170
for that but I could talk about that

00:48:06,910 --> 00:48:10,170
offline if you're interested

00:48:14,920 --> 00:48:19,580

YouTube URL: https://www.youtube.com/watch?v=l0geztCsjcA


