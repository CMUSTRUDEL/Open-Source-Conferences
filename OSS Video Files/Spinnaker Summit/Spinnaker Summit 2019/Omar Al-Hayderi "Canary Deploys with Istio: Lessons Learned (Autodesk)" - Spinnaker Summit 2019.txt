Title: Omar Al-Hayderi "Canary Deploys with Istio: Lessons Learned (Autodesk)" - Spinnaker Summit 2019
Publication date: 2019-12-03
Playlist: Spinnaker Summit 2019
Description: 
	The third annual Spinnaker Summit (Diamond Sponsors: Netflix, Google and Armory) was held at the Hard Rock Hotel in San Diego, CA from November 15-17, 2019 and welcomed over 500 members of the rapidly growing Spinnaker open source community.
Captions: 
	00:00:06,330 --> 00:00:10,980
[Laughter]

00:00:14,560 --> 00:00:18,700
as the splash says over the next 45

00:00:17,020 --> 00:00:21,130
minutes I'm going to be talking about

00:00:18,700 --> 00:00:24,369
our story of bringing Canary deployments

00:00:21,130 --> 00:00:28,150
to our CD process and how sto enabled us

00:00:24,369 --> 00:00:29,410
to do so so a little bit about how the

00:00:28,150 --> 00:00:32,320
agenda is going to look I'm going to

00:00:29,410 --> 00:00:34,330
give a quick talk on why do Canary

00:00:32,320 --> 00:00:36,879
deployments at all how do we actually

00:00:34,330 --> 00:00:38,710
implement this in the real world lessons

00:00:36,879 --> 00:00:41,140
learned from getting it into our

00:00:38,710 --> 00:00:43,090
production systems and some next steps

00:00:41,140 --> 00:00:46,030
were thinking about in terms of

00:00:43,090 --> 00:00:49,960
extending it it's a little bit about me

00:00:46,030 --> 00:00:51,820
first I grew up in Ottawa Canada I spent

00:00:49,960 --> 00:00:53,590
most of my young life snowboarding I was

00:00:51,820 --> 00:00:56,020
actually competitive snowboarder through

00:00:53,590 --> 00:00:57,370
high school and college at some point

00:00:56,020 --> 00:00:59,110
there I figured out that that was not

00:00:57,370 --> 00:01:01,820
going to be a very great career choice

00:00:59,110 --> 00:01:05,030
in terms of comfort and not

00:01:01,820 --> 00:01:07,340
um so I decided to move into software

00:01:05,030 --> 00:01:09,050
engineering I moved to the state six

00:01:07,340 --> 00:01:11,510
years ago to work at Yahoo for the

00:01:09,050 --> 00:01:13,400
fantasy sports site and there we were a

00:01:11,510 --> 00:01:15,350
very small team so being a back-end

00:01:13,400 --> 00:01:19,070
engineer I got my hands into a lot of

00:01:15,350 --> 00:01:20,900
DevOps and infrastructure work and two

00:01:19,070 --> 00:01:23,540
years after that I moved to a company

00:01:20,900 --> 00:01:25,190
called plan grid where I started out as

00:01:23,540 --> 00:01:26,750
a back-end engineer and again moved into

00:01:25,190 --> 00:01:28,760
more of an infrastructure DevOps role

00:01:26,750 --> 00:01:31,400
and we were recently acquired by

00:01:28,760 --> 00:01:32,810
Autodesk and that's me at the Autodesk

00:01:31,400 --> 00:01:37,390
softball tournament with my new

00:01:32,810 --> 00:01:40,250
coworkers and that brings me here so

00:01:37,390 --> 00:01:42,920
today's story like every other story

00:01:40,250 --> 00:01:45,380
you've heard at this conference is about

00:01:42,920 --> 00:01:47,380
a problem and a very common problem that

00:01:45,380 --> 00:01:50,900
you're all probably very aware of

00:01:47,380 --> 00:01:53,690
release related incidents so at planned

00:01:50,900 --> 00:01:57,020
grid while we do have a service-oriented

00:01:53,690 --> 00:02:01,580
architecture roughly you know 30 40

00:01:57,020 --> 00:02:04,070
services facing production usage most of

00:02:01,580 --> 00:02:05,990
our traffic is going to a monolith that

00:02:04,070 --> 00:02:08,269
monolith gets released once per week

00:02:05,990 --> 00:02:09,739
there's many people in the same you know

00:02:08,269 --> 00:02:11,630
slack channel together getting it to

00:02:09,739 --> 00:02:13,940
work we have to time it with schema

00:02:11,630 --> 00:02:16,280
migrations so it's still a pretty heavy

00:02:13,940 --> 00:02:18,650
process that you know even through all

00:02:16,280 --> 00:02:20,660
the unit tests and integration tests and

00:02:18,650 --> 00:02:27,019
load tests we do in staging we're still

00:02:20,660 --> 00:02:29,180
crossing her fingers so I can't remember

00:02:27,019 --> 00:02:32,870
exactly when I first heard about Canary

00:02:29,180 --> 00:02:35,000
deployments but I got very interested in

00:02:32,870 --> 00:02:37,489
spinnaker when we bootstrapped it at

00:02:35,000 --> 00:02:39,709
plank or two years ago and I've been you

00:02:37,489 --> 00:02:41,810
know a long time lurker of the slack

00:02:39,709 --> 00:02:42,980
channels there and around then you

00:02:41,810 --> 00:02:45,049
started hearing about this new thing

00:02:42,980 --> 00:02:47,810
called Kayenta and I was really excited

00:02:45,049 --> 00:02:52,310
about it because it provided us with a

00:02:47,810 --> 00:02:54,440
lot of scaling and ability to find these

00:02:52,310 --> 00:02:55,760
issues that unit testing and all of our

00:02:54,440 --> 00:02:59,930
over the other testing processes

00:02:55,760 --> 00:03:02,870
couldn't find so what's Canaries the

00:02:59,930 --> 00:03:04,640
word canary comes from the old tool that

00:03:02,870 --> 00:03:06,260
miners used to use before they had the

00:03:04,640 --> 00:03:07,910
technology of air quality detectors

00:03:06,260 --> 00:03:10,190
they'd bring a small bird in the cage

00:03:07,910 --> 00:03:12,049
down in the mines and when it started

00:03:10,190 --> 00:03:12,620
squeaking and squawking they knew it was

00:03:12,049 --> 00:03:15,200
time to get

00:03:12,620 --> 00:03:16,970
so instead of a bunch of miners getting

00:03:15,200 --> 00:03:19,580
you know long-term health problems it

00:03:16,970 --> 00:03:21,470
was just this poor little bird how does

00:03:19,580 --> 00:03:21,890
that translate into the web application

00:03:21,470 --> 00:03:25,520
world

00:03:21,890 --> 00:03:27,560
well when now when we release instead of

00:03:25,520 --> 00:03:29,750
releasing change to all of our

00:03:27,560 --> 00:03:31,849
production paying users we release it to

00:03:29,750 --> 00:03:33,650
a small Minuten minority and when they

00:03:31,849 --> 00:03:35,660
start squeaking and squawking we know to

00:03:33,650 --> 00:03:39,950
roll back the release and save that from

00:03:35,660 --> 00:03:42,080
the majority so one thing I want to

00:03:39,950 --> 00:03:43,069
focus on in this talk is that you know

00:03:42,080 --> 00:03:46,069
even though I'm gonna talk a lot about

00:03:43,069 --> 00:03:48,110
how you know spinnaker and sto and KY

00:03:46,069 --> 00:03:49,940
anta enable us to do canary deployments

00:03:48,110 --> 00:03:53,209
you don't need to use any of those tools

00:03:49,940 --> 00:03:55,400
really to leverage the idea of exposing

00:03:53,209 --> 00:03:58,819
change silver nori and gathering data to

00:03:55,400 --> 00:04:01,220
decide if that change is ready for the

00:03:58,819 --> 00:04:03,500
main stage in fact you're probably doing

00:04:01,220 --> 00:04:06,560
canary deployments right now if you have

00:04:03,500 --> 00:04:08,780
the developer flow of merging code into

00:04:06,560 --> 00:04:10,340
dev and if getting deployed to staging

00:04:08,780 --> 00:04:12,920
and you have some sort of manual QA

00:04:10,340 --> 00:04:16,970
there that is really a form of canary

00:04:12,920 --> 00:04:18,620
deployments your canary encoder and

00:04:16,970 --> 00:04:19,970
getting feedback there and then

00:04:18,620 --> 00:04:24,470
promoting the release of production

00:04:19,970 --> 00:04:26,600
there's obviously a lot of issues with

00:04:24,470 --> 00:04:28,490
that you're not getting a totally

00:04:26,600 --> 00:04:30,229
accurate test because you're not testing

00:04:28,490 --> 00:04:32,810
in production and that's kind of a new

00:04:30,229 --> 00:04:34,940
trend that I'm seeing is that we can try

00:04:32,810 --> 00:04:36,560
our best to really emulate production in

00:04:34,940 --> 00:04:37,760
our Devon staging environments but

00:04:36,560 --> 00:04:39,680
nothing's going to be like the real

00:04:37,760 --> 00:04:41,690
thing so if we could somehow test in

00:04:39,680 --> 00:04:44,510
production without affecting our end

00:04:41,690 --> 00:04:45,740
users as much as possible then that's

00:04:44,510 --> 00:04:49,760
great that's something we want to strive

00:04:45,740 --> 00:04:51,830
for so as I said you know checks a lot

00:04:49,760 --> 00:04:53,660
of the boxes that other tests can't so

00:04:51,830 --> 00:04:55,700
we have like load and integration tests

00:04:53,660 --> 00:04:58,310
in our staging environment that we try

00:04:55,700 --> 00:04:59,660
to catch all these issues with and if

00:04:58,310 --> 00:05:01,430
anyone has actually tried to implement

00:04:59,660 --> 00:05:03,889
load test at their company it works

00:05:01,430 --> 00:05:05,750
great for your first you know champions

00:05:03,889 --> 00:05:08,389
that really want to use it but asking

00:05:05,750 --> 00:05:10,639
every single feature and service team to

00:05:08,389 --> 00:05:12,770
define end points to find how they're

00:05:10,639 --> 00:05:14,360
going to do authentication and manage

00:05:12,770 --> 00:05:16,460
their tests in a whole other platform

00:05:14,360 --> 00:05:18,620
does it scale we've had very bad

00:05:16,460 --> 00:05:20,000
experience with that integrations are

00:05:18,620 --> 00:05:23,000
the same way there's a lot of assumed

00:05:20,000 --> 00:05:25,380
state you need some project or some user

00:05:23,000 --> 00:05:27,690
that you know exists in this environment

00:05:25,380 --> 00:05:29,070
and it's just really hard to sort of

00:05:27,690 --> 00:05:32,040
bootstrap that and every feature and

00:05:29,070 --> 00:05:33,990
service so what I really love about

00:05:32,040 --> 00:05:35,850
Canaries is that how well it scales

00:05:33,990 --> 00:05:37,290
you're not defining any tests you don't

00:05:35,850 --> 00:05:39,600
have to have any state dependencies

00:05:37,290 --> 00:05:43,710
you're relying completely on your real

00:05:39,600 --> 00:05:46,710
traffic to do your testing so what does

00:05:43,710 --> 00:05:49,380
it actually look like so most web apps

00:05:46,710 --> 00:05:51,270
at a high level look like this we have

00:05:49,380 --> 00:05:54,420
traffic coming from somewhere you know

00:05:51,270 --> 00:05:57,570
the Internet internally something that

00:05:54,420 --> 00:05:59,520
goes through a router whether it's a you

00:05:57,570 --> 00:06:01,650
know reverse proxy load balancer or

00:05:59,520 --> 00:06:04,590
whatever it's taking traffic and moving

00:06:01,650 --> 00:06:06,570
that to your production servers your

00:06:04,590 --> 00:06:09,300
production servers are running some code

00:06:06,570 --> 00:06:13,800
and it is reporting metrics to some

00:06:09,300 --> 00:06:16,050
centralized metrics tool hopefully

00:06:13,800 --> 00:06:17,670
so what Canaries do and how it changes

00:06:16,050 --> 00:06:21,510
things is it introduces two new

00:06:17,670 --> 00:06:22,170
deployments the idea of a baseline and a

00:06:21,510 --> 00:06:24,600
canary

00:06:22,170 --> 00:06:26,970
so the baseline you can see is running

00:06:24,600 --> 00:06:29,400
v1 same exact code as production is

00:06:26,970 --> 00:06:31,650
running and canary is running v2 or your

00:06:29,400 --> 00:06:33,120
release candidate and you'll also notice

00:06:31,650 --> 00:06:35,640
that production is running many servers

00:06:33,120 --> 00:06:37,740
whatever your default you know replica

00:06:35,640 --> 00:06:38,490
set count is at and then baseline and

00:06:37,740 --> 00:06:40,530
canary are gonna be running

00:06:38,490 --> 00:06:42,960
substantially less and I'll go into a

00:06:40,530 --> 00:06:46,050
little bit why why we do it that way it

00:06:42,960 --> 00:06:47,790
has to do with trying to emulate load as

00:06:46,050 --> 00:06:52,080
much as we can without having equal

00:06:47,790 --> 00:06:54,570
traffic going to those deployments the

00:06:52,080 --> 00:06:56,580
last magical piece is that once you have

00:06:54,570 --> 00:06:58,680
all your metrics being pumped into data

00:06:56,580 --> 00:07:00,720
dog New Relic whatever you use you have

00:06:58,680 --> 00:07:02,280
to have some thing that's going to

00:07:00,720 --> 00:07:04,740
ingest those metrics and be able to

00:07:02,280 --> 00:07:09,920
compare between those two sets and make

00:07:04,740 --> 00:07:09,920
a decision on is this change good

00:07:10,190 --> 00:07:14,340
so some goals of actually getting

00:07:12,480 --> 00:07:16,770
Canaries into our production pipelines

00:07:14,340 --> 00:07:18,480
so we want to expose a small set of

00:07:16,770 --> 00:07:21,780
users to the new code right we don't

00:07:18,480 --> 00:07:24,330
want to have to expose you know 50% or

00:07:21,780 --> 00:07:27,240
more traffic to this experiment

00:07:24,330 --> 00:07:30,180
we'd like to use expose the minimal set

00:07:27,240 --> 00:07:32,520
required we want to be able to compare

00:07:30,180 --> 00:07:35,940
these KPIs between versions so you know

00:07:32,520 --> 00:07:37,680
latency and whatever it be we don't want

00:07:35,940 --> 00:07:39,000
to dictate exactly what this key

00:07:37,680 --> 00:07:41,580
guys are but we want to be able to

00:07:39,000 --> 00:07:43,410
compare them in some way same code same

00:07:41,580 --> 00:07:46,289
app two different deployments and

00:07:43,410 --> 00:07:47,729
somehow be able to compare them we also

00:07:46,289 --> 00:07:50,310
want to be able to automatically

00:07:47,729 --> 00:07:53,280
rollback regressions or canary test

00:07:50,310 --> 00:07:55,080
failures and that is very important

00:07:53,280 --> 00:07:57,660
because of the way our dev workflow is

00:07:55,080 --> 00:08:00,000
now is that we're really kind of a get

00:07:57,660 --> 00:08:01,310
up shop where you merge to a branch and

00:08:00,000 --> 00:08:03,630
it gets automatically deployed

00:08:01,310 --> 00:08:05,100
application developers rarely have to

00:08:03,630 --> 00:08:07,530
open spinnaker or look into what's

00:08:05,100 --> 00:08:09,509
actually happening they merge it gets

00:08:07,530 --> 00:08:10,770
deployed and when we introduce Canaries

00:08:09,509 --> 00:08:14,010
we don't want to have to break that

00:08:10,770 --> 00:08:17,880
workflow by injecting some human

00:08:14,010 --> 00:08:19,470
decision step things were trying to

00:08:17,880 --> 00:08:21,449
optimize for when we're generating this

00:08:19,470 --> 00:08:23,729
experiment the first one is limiting

00:08:21,449 --> 00:08:25,500
noise and if you look at a lot of these

00:08:23,729 --> 00:08:27,300
canary best practices you're gonna see

00:08:25,500 --> 00:08:29,760
that it's really recommending deploying

00:08:27,300 --> 00:08:31,680
this baseline deployment and you're

00:08:29,760 --> 00:08:34,229
gonna maybe wonder why would I redeploy

00:08:31,680 --> 00:08:36,390
code that's already out there the idea

00:08:34,229 --> 00:08:38,430
is that you want to minimize any deltas

00:08:36,390 --> 00:08:40,289
between those two things the canary and

00:08:38,430 --> 00:08:41,789
your existing code so you don't want to

00:08:40,289 --> 00:08:43,979
take advantage of long-running process

00:08:41,789 --> 00:08:45,450
things like caching and long enough

00:08:43,979 --> 00:08:48,720
connections you want to launch them at

00:08:45,450 --> 00:08:49,350
the same time and compare the two code

00:08:48,720 --> 00:08:51,690
deployments

00:08:49,350 --> 00:08:53,310
there's also cases where you might make

00:08:51,690 --> 00:08:55,709
configuration changes that haven't been

00:08:53,310 --> 00:08:58,470
deployed to your existing production

00:08:55,709 --> 00:08:59,610
deployment so in that case when you

00:08:58,470 --> 00:09:01,650
launch your baseline you're going to

00:08:59,610 --> 00:09:03,810
notice that change and really the things

00:09:01,650 --> 00:09:05,730
you're comparing for us at least when we

00:09:03,810 --> 00:09:11,580
deploy containers is the actual change

00:09:05,730 --> 00:09:13,740
in code so canary deployments what are

00:09:11,580 --> 00:09:15,330
the challenges I'm gonna go into one way

00:09:13,740 --> 00:09:17,040
that we solve for this and actually

00:09:15,330 --> 00:09:19,050
implemented this but if you want to

00:09:17,040 --> 00:09:20,610
bring this to your organization the one

00:09:19,050 --> 00:09:22,290
thing you have to focus on is solving

00:09:20,610 --> 00:09:25,050
for these three challenges

00:09:22,290 --> 00:09:27,990
the first one is routing you have to

00:09:25,050 --> 00:09:30,600
find a way to route traffic between your

00:09:27,990 --> 00:09:32,220
default production deployments and your

00:09:30,600 --> 00:09:34,700
baseline and canary that's completely

00:09:32,220 --> 00:09:37,320
transparent to your users and clients

00:09:34,700 --> 00:09:39,000
the second challenge is actually

00:09:37,320 --> 00:09:41,070
analyzing the difference so you need

00:09:39,000 --> 00:09:42,630
something to look at the metrics being

00:09:41,070 --> 00:09:44,760
produced from your canary in your

00:09:42,630 --> 00:09:48,450
baseline and you need to have it be able

00:09:44,760 --> 00:09:51,340
to decide is this good or not and the

00:09:48,450 --> 00:09:52,660
final step with a judge Duty is

00:09:51,340 --> 00:09:55,210
what do you do with that information

00:09:52,660 --> 00:09:57,760
what happens with a pass in the fail how

00:09:55,210 --> 00:09:59,350
does it impact your deployment in our

00:09:57,760 --> 00:10:01,540
case we want to have it automatically

00:09:59,350 --> 00:10:06,850
roll back and also automatically promote

00:10:01,540 --> 00:10:09,520
that release candidate to production so

00:10:06,850 --> 00:10:11,110
how do we do it I'm gonna step back here

00:10:09,520 --> 00:10:12,820
and give some context a little bit into

00:10:11,110 --> 00:10:15,220
how we deploy at plan grit and what our

00:10:12,820 --> 00:10:18,070
spinnaker infrastructure looks like so

00:10:15,220 --> 00:10:20,350
we have a mono repo that has all of the

00:10:18,070 --> 00:10:23,500
application deployment configs and the

00:10:20,350 --> 00:10:25,750
pipeline templates in it what that looks

00:10:23,500 --> 00:10:27,940
like if you look on the right there is

00:10:25,750 --> 00:10:31,300
we expose this interface to developers

00:10:27,940 --> 00:10:33,460
that's a very compact yellow file that

00:10:31,300 --> 00:10:35,200
you define a branch in this case master

00:10:33,460 --> 00:10:36,930
it's gonna get it's gonna push to this

00:10:35,200 --> 00:10:39,550
pipeline every time a new commits there

00:10:36,930 --> 00:10:42,040
we name the pipeline prod and it has

00:10:39,550 --> 00:10:43,420
like slack channels to notify users you

00:10:42,040 --> 00:10:45,550
can add your own DNS names and then

00:10:43,420 --> 00:10:47,380
target size and there's several other

00:10:45,550 --> 00:10:50,410
things we expose there and then we have

00:10:47,380 --> 00:10:52,390
a single ginger template that the CM

00:10:50,410 --> 00:10:55,089
will be sam'l values get pushed into and

00:10:52,390 --> 00:10:58,810
we take that template and put it into

00:10:55,089 --> 00:11:01,270
spinnaker so ya deployment runtime all

00:10:58,810 --> 00:11:03,280
defined in that yanil interface and we

00:11:01,270 --> 00:11:06,520
have a one to one to one to one to one

00:11:03,280 --> 00:11:10,450
mapping of github repos deployable web

00:11:06,520 --> 00:11:12,940
services and a default domain name so

00:11:10,450 --> 00:11:15,280
when you're in spinnaker you'll have if

00:11:12,940 --> 00:11:18,370
I create a github repo that's my app

00:11:15,280 --> 00:11:20,560
I'll see and I create a my a PMO file in

00:11:18,370 --> 00:11:23,950
this spinnaker configuration mono repo

00:11:20,560 --> 00:11:25,600
I'll see a my app application with

00:11:23,950 --> 00:11:28,839
whatever pipelines I config and

00:11:25,600 --> 00:11:30,850
spinnaker and we give everyone a

00:11:28,839 --> 00:11:33,339
standard C name basically a repo name

00:11:30,850 --> 00:11:38,890
your end and then I'm not playing front

00:11:33,339 --> 00:11:40,990
postfix going it's a little more about

00:11:38,890 --> 00:11:42,520
how we'd actually like manage these

00:11:40,990 --> 00:11:44,740
deployments or to find these pipelines

00:11:42,520 --> 00:11:46,450
and distribute them that's me as Beavis

00:11:44,740 --> 00:11:47,890
you know creating these new pipeline

00:11:46,450 --> 00:11:50,650
templates and creative changes to how we

00:11:47,890 --> 00:11:51,370
deploy I merge that to a branch on

00:11:50,650 --> 00:11:53,770
github

00:11:51,370 --> 00:11:55,839
Jenkins picks that up and essentially

00:11:53,770 --> 00:11:58,450
just runs Jinja to build your actual

00:11:55,839 --> 00:12:01,030
pipeline JSON and then straight up

00:11:58,450 --> 00:12:02,560
copies it into s3 and that's where

00:12:01,030 --> 00:12:08,110
spinnaker reads from

00:12:02,560 --> 00:12:09,550
to get its pipeline definitions this is

00:12:08,110 --> 00:12:12,459
what our infrastructure looked like when

00:12:09,550 --> 00:12:15,579
I first tried to get canary deployments

00:12:12,459 --> 00:12:18,009
in this is a long time ago basically we

00:12:15,579 --> 00:12:20,889
have the array of clients that we have

00:12:18,009 --> 00:12:23,350
we distribute to our users and our

00:12:20,889 --> 00:12:25,209
kubernetes services and each kubernetes

00:12:23,350 --> 00:12:27,550
service was actually a load balancer

00:12:25,209 --> 00:12:29,949
service that had its own ELB its own DNS

00:12:27,550 --> 00:12:31,559
name so I mean right off the bat you can

00:12:29,949 --> 00:12:33,670
kind of see some problems here is that

00:12:31,559 --> 00:12:35,920
if you want to connect to a new service

00:12:33,670 --> 00:12:38,139
that's a new domain so service discovery

00:12:35,920 --> 00:12:40,449
was a big problem as you'll see soon

00:12:38,139 --> 00:12:41,309
that made routing between deployments a

00:12:40,449 --> 00:12:44,769
big problem

00:12:41,309 --> 00:12:47,559
so first try hack week I playing heard

00:12:44,769 --> 00:12:48,939
we have hack weeks twice per year and

00:12:47,559 --> 00:12:50,949
this is a great time you know step off

00:12:48,939 --> 00:12:53,170
your roadmap and try something new and

00:12:50,949 --> 00:12:54,189
fun like canary deployments this is

00:12:53,170 --> 00:12:56,589
something I've been thinking about for a

00:12:54,189 --> 00:12:59,439
long time and I've had negative free

00:12:56,589 --> 00:13:01,029
time to do so I put on my beanie and got

00:12:59,439 --> 00:13:03,059
my friends in a room and we started

00:13:01,029 --> 00:13:07,120
trying it out so how'd it go

00:13:03,059 --> 00:13:09,519
not well routing is really hard if you

00:13:07,120 --> 00:13:10,990
don't have some sort of centralized API

00:13:09,519 --> 00:13:14,259
gateway or something that can

00:13:10,990 --> 00:13:15,939
intelligently route traffic looking back

00:13:14,259 --> 00:13:19,600
there are some ways you can accomplish

00:13:15,939 --> 00:13:22,269
this using basically coarse-grained

00:13:19,600 --> 00:13:23,889
routing or assume that you have a

00:13:22,269 --> 00:13:26,680
certain set of pods certain serving

00:13:23,889 --> 00:13:28,930
traffic containers ec2 instances

00:13:26,680 --> 00:13:30,610
whatever and you deploy a certain amount

00:13:28,930 --> 00:13:32,410
with the new code and you can assume

00:13:30,610 --> 00:13:34,889
that some percentage of traffic goes

00:13:32,410 --> 00:13:37,180
there however it's very hard to

00:13:34,889 --> 00:13:39,129
communicate that to developers and say

00:13:37,180 --> 00:13:41,559
exactly how much traffic is going to go

00:13:39,129 --> 00:13:44,559
to your canarian baseline so we chose

00:13:41,559 --> 00:13:46,480
not to go that route also at the time we

00:13:44,559 --> 00:13:48,129
didn't have very standard metrics it

00:13:46,480 --> 00:13:50,319
wasn't easy to compare metrics between

00:13:48,129 --> 00:13:52,360
deployments kai anta was also really

00:13:50,319 --> 00:13:54,429
young and the documentation very fresh

00:13:52,360 --> 00:13:55,449
so a lot of it was going into source

00:13:54,429 --> 00:13:57,939
code and actually figure out how things

00:13:55,449 --> 00:14:02,230
worked and it just wasn't realistically

00:13:57,939 --> 00:14:05,459
accomplished within a week okay hack

00:14:02,230 --> 00:14:10,120
week six months later let's try it again

00:14:05,459 --> 00:14:12,370
but this time have some help in that

00:14:10,120 --> 00:14:14,250
time difference in those two hacks weeks

00:14:12,370 --> 00:14:16,019
we released

00:14:14,250 --> 00:14:18,029
an infrastructure change to our

00:14:16,019 --> 00:14:20,279
kubernetes cluster to have all traffic

00:14:18,029 --> 00:14:22,050
go through a single sto gateway and you

00:14:20,279 --> 00:14:24,649
can see on the right here we have a

00:14:22,050 --> 00:14:27,629
single ELB now that instead of having a

00:14:24,649 --> 00:14:30,300
distinct C name we use wild carding so

00:14:27,629 --> 00:14:33,449
that we still distribute you know your

00:14:30,300 --> 00:14:36,000
service name dot some postfix but it all

00:14:33,449 --> 00:14:37,319
goes to the same lb and we defer to sto

00:14:36,000 --> 00:14:40,319
to actually route that to your

00:14:37,319 --> 00:14:42,149
kubernetes service why that was so

00:14:40,319 --> 00:14:45,060
powerful and helped us a lot was because

00:14:42,149 --> 00:14:46,649
that sto gateway act as a traffic

00:14:45,060 --> 00:14:49,019
management system that made it very easy

00:14:46,649 --> 00:14:51,629
to split traffic completely

00:14:49,019 --> 00:14:55,129
transparently to the clients between

00:14:51,629 --> 00:14:55,129
different services and deployments

00:14:55,250 --> 00:14:59,850
success is to work so we actually got

00:14:58,500 --> 00:15:03,300
this working in hack week or at least to

00:14:59,850 --> 00:15:04,740
a POC level which gave me some good

00:15:03,300 --> 00:15:06,600
ammunition to go to the rest of the org

00:15:04,740 --> 00:15:07,949
and say hey if we invest a little more

00:15:06,600 --> 00:15:11,910
in this we can get a huge amount of

00:15:07,949 --> 00:15:15,000
value so let's go back to the challenges

00:15:11,910 --> 00:15:20,579
and see how we solve for them so again

00:15:15,000 --> 00:15:24,689
routing analysis and judgment so routing

00:15:20,579 --> 00:15:26,910
how does it work so what we do is we

00:15:24,689 --> 00:15:29,220
deploy the release candidate to an

00:15:26,910 --> 00:15:32,040
entirely new kubernetes service with

00:15:29,220 --> 00:15:33,630
service name - canary we find the

00:15:32,040 --> 00:15:35,339
existing doctrine container running in

00:15:33,630 --> 00:15:38,579
our default production deployment and

00:15:35,339 --> 00:15:40,680
deploy it as service name baseline this

00:15:38,579 --> 00:15:42,509
helps us a lot to differentiate metrics

00:15:40,680 --> 00:15:45,899
and data dog which is our metrics

00:15:42,509 --> 00:15:47,370
provider and make it really easy to do

00:15:45,899 --> 00:15:50,430
that analysis stage which I'll get into

00:15:47,370 --> 00:15:53,040
and for the actual routing between these

00:15:50,430 --> 00:15:55,410
two deployments we use this Tio's

00:15:53,040 --> 00:15:58,230
traffic management feature to split

00:15:55,410 --> 00:16:00,600
traffic based on weights between those

00:15:58,230 --> 00:16:03,630
deployments and what sto gave us is

00:16:00,600 --> 00:16:05,430
actually fine grain control to split

00:16:03,630 --> 00:16:10,949
traffic down to the single percentile

00:16:05,430 --> 00:16:13,170
point so diving deep into actually what

00:16:10,949 --> 00:16:15,990
this looks like in sto for those not

00:16:13,170 --> 00:16:18,300
familiar virtual services our

00:16:15,990 --> 00:16:21,240
kubernetes c RDS or custom resource

00:16:18,300 --> 00:16:23,220
definitions that you apply in a manifest

00:16:21,240 --> 00:16:27,660
stage or just a cube cuddle apply and

00:16:23,220 --> 00:16:30,420
that defines routing for specific hosts

00:16:27,660 --> 00:16:31,620
so in this case a PMF lenger comm if it

00:16:30,420 --> 00:16:33,960
gets a request with that host header

00:16:31,620 --> 00:16:36,410
it's gonna apply the following HTTP

00:16:33,960 --> 00:16:40,320
routing which in a steady-state is

00:16:36,410 --> 00:16:44,130
routing 100% of traffic to the API

00:16:40,320 --> 00:16:46,670
default deployment now during a canary

00:16:44,130 --> 00:16:49,110
experiment how do we change that

00:16:46,670 --> 00:16:51,660
we basically just have to change the

00:16:49,110 --> 00:16:54,090
destination routing rules so for us what

00:16:51,660 --> 00:16:56,310
we're doing is now we're deploying or

00:16:54,090 --> 00:16:59,370
routing 90% of traffic with that that's

00:16:56,310 --> 00:17:02,280
defined by that weight 90 to the API

00:16:59,370 --> 00:17:08,610
default appointment and then 5% to the

00:17:02,280 --> 00:17:11,510
baseline and 5% to the canary analysis

00:17:08,610 --> 00:17:11,510
how does that work

00:17:12,150 --> 00:17:17,970
so we tagged metrics based on the

00:17:15,150 --> 00:17:19,800
service name send that to data dog we

00:17:17,970 --> 00:17:22,560
configure spinnaker to consume those

00:17:19,800 --> 00:17:25,500
metrics we create default canary

00:17:22,560 --> 00:17:28,650
configurations for every single app that

00:17:25,500 --> 00:17:31,050
consumes those metrics and then create a

00:17:28,650 --> 00:17:35,270
canary analysis stage with KY anta as

00:17:31,050 --> 00:17:37,590
the consumer once routing is configured

00:17:35,270 --> 00:17:40,200
this is a little hard to read I realize

00:17:37,590 --> 00:17:42,890
but this is sort of the things in Sto

00:17:40,200 --> 00:17:44,910
you have to provide to get this to work

00:17:42,890 --> 00:17:48,150
first thing you need to do is create a

00:17:44,910 --> 00:17:49,980
handler and that basically tells the sto

00:17:48,150 --> 00:17:52,950
mixer which is in charge of all of the

00:17:49,980 --> 00:17:55,800
telemetry routing so request tracing and

00:17:52,950 --> 00:17:57,270
all your default HTTP metrics we just

00:17:55,800 --> 00:18:00,420
configure it to send everything to the

00:17:57,270 --> 00:18:02,550
data agents you know append some prefix

00:18:00,420 --> 00:18:04,860
to make it easily searchable and data

00:18:02,550 --> 00:18:06,990
dog and then create our own custom

00:18:04,860 --> 00:18:10,080
metrics in this case I'm demoing request

00:18:06,990 --> 00:18:12,270
count so you reference it here and then

00:18:10,080 --> 00:18:14,070
you define it up there and you can get

00:18:12,270 --> 00:18:16,440
all of the sto metrics that mixer

00:18:14,070 --> 00:18:20,960
provides and encapsulate that into a way

00:18:16,440 --> 00:18:20,960
that is easily consumable and Dana dog

00:18:21,380 --> 00:18:25,740
and this is what that ends up looking

00:18:23,250 --> 00:18:28,200
like so one of the great things about

00:18:25,740 --> 00:18:30,390
that SCO gave us was this kind of master

00:18:28,200 --> 00:18:32,880
data dog dashboard that gives us all of

00:18:30,390 --> 00:18:35,580
our HTTP metrics and we can toggle

00:18:32,880 --> 00:18:38,700
between apps very top that destination

00:18:35,580 --> 00:18:41,640
app tag we can go through any app and

00:18:38,700 --> 00:18:44,280
see you know serve errors cross counts

00:18:41,640 --> 00:18:47,370
latency etc and actually compare that

00:18:44,280 --> 00:18:49,680
between destination apps you'll also see

00:18:47,370 --> 00:18:51,930
there's a source app option up there so

00:18:49,680 --> 00:18:54,810
we can actually find latency but from a

00:18:51,930 --> 00:18:59,210
source not from say the gateway but from

00:18:54,810 --> 00:18:59,210
my app to your app and vice-versa

00:19:00,780 --> 00:19:07,620
what you also need to do is configure

00:19:03,530 --> 00:19:09,090
spinnaker via halyard to ingest these

00:19:07,620 --> 00:19:10,770
metrics and that's pretty

00:19:09,090 --> 00:19:13,230
straightforward we actually apply the

00:19:10,770 --> 00:19:15,060
halyard config and salt so we don't

00:19:13,230 --> 00:19:17,460
actually do the halyard configure and

00:19:15,060 --> 00:19:20,160
then deploy stage we plug in the

00:19:17,460 --> 00:19:22,770
configuration raw and it looks pretty

00:19:20,160 --> 00:19:25,290
straightforward pointed to data dog add

00:19:22,770 --> 00:19:27,560
your super secret secrets and you're

00:19:25,290 --> 00:19:30,600
good to go

00:19:27,560 --> 00:19:31,530
so how canary configs work this is

00:19:30,600 --> 00:19:33,660
something we're still trying to figure

00:19:31,530 --> 00:19:36,120
out how do we enable feature teams to

00:19:33,660 --> 00:19:38,280
get on board with Canaries with the

00:19:36,120 --> 00:19:40,290
least amount of friction and right now

00:19:38,280 --> 00:19:42,090
the path forward is give them a very

00:19:40,290 --> 00:19:44,280
straightforward simple

00:19:42,090 --> 00:19:45,570
canary config and what the config does

00:19:44,280 --> 00:19:48,000
is it tells

00:19:45,570 --> 00:19:50,610
Khai anta what metrics am i ingesting

00:19:48,000 --> 00:19:55,320
and how should I decide if something is

00:19:50,610 --> 00:19:57,420
good or bad so I'm gonna go through this

00:19:55,320 --> 00:19:59,010
and give a high-level view of what this

00:19:57,420 --> 00:20:00,060
would look like in the spinnaker UI if

00:19:59,010 --> 00:20:02,010
you actually wanted to create your own

00:20:00,060 --> 00:20:04,080
canary config first thing is you

00:20:02,010 --> 00:20:05,460
configure a name and then a metric store

00:20:04,080 --> 00:20:08,490
you can have multiple metric stores

00:20:05,460 --> 00:20:10,350
defined in spinnaker to ingest stats at

00:20:08,490 --> 00:20:12,330
the time when we built this there was

00:20:10,350 --> 00:20:13,950
only data dog but they've added a new

00:20:12,330 --> 00:20:15,750
relic since and that's something we're

00:20:13,950 --> 00:20:19,980
actively building towards integrating

00:20:15,750 --> 00:20:22,920
right now you then define your metrics

00:20:19,980 --> 00:20:24,300
those metrics can be put into groups the

00:20:22,920 --> 00:20:27,000
significance of groups is going to be

00:20:24,300 --> 00:20:28,800
apparent in a few slides but for us all

00:20:27,000 --> 00:20:33,390
we do out of the box is latency

00:20:28,800 --> 00:20:34,920
95 P and average latency those are

00:20:33,390 --> 00:20:37,679
defined here and give your metric name

00:20:34,920 --> 00:20:39,630
scope you can also have a little more

00:20:37,679 --> 00:20:41,240
fine-grained control of how powerful you

00:20:39,630 --> 00:20:43,740
want those metrics to be or how

00:20:41,240 --> 00:20:45,540
opinionated you want your canary

00:20:43,740 --> 00:20:49,290
analysis to be in terms of this metric

00:20:45,540 --> 00:20:50,160
so you can imagine surveyors 5-xx errors

00:20:49,290 --> 00:20:51,370
you might want to put a little more

00:20:50,160 --> 00:20:53,769
weight to them

00:20:51,370 --> 00:20:57,460
and fail instantly if that metric fails

00:20:53,769 --> 00:20:59,230
you also want to define if which way is

00:20:57,460 --> 00:21:00,850
bad right so are you doing are you

00:20:59,230 --> 00:21:04,269
comparing uptime then you would want to

00:21:00,850 --> 00:21:06,519
create or at least configure it to pick

00:21:04,269 --> 00:21:08,169
up on decreases but for latency we

00:21:06,519 --> 00:21:14,409
obviously want to know if it increases

00:21:08,169 --> 00:21:16,749
that's a warning sign so now when you

00:21:14,409 --> 00:21:19,389
can do with your groups is give weights

00:21:16,749 --> 00:21:21,940
to them which sort of tells Kai anta how

00:21:19,389 --> 00:21:23,649
important is this group of metrics so

00:21:21,940 --> 00:21:25,509
because we only have latency we're

00:21:23,649 --> 00:21:26,980
giving everything to that but you can

00:21:25,509 --> 00:21:30,429
imagine in the future if you extend this

00:21:26,980 --> 00:21:32,049
to use business kpi's server errors CPU

00:21:30,429 --> 00:21:34,110
and memory usage you might have

00:21:32,049 --> 00:21:36,580
different biases towards each one

00:21:34,110 --> 00:21:41,519
historically based on incidents in your

00:21:36,580 --> 00:21:43,649
organization so the last challenge

00:21:41,519 --> 00:21:48,100
judgment how does it work

00:21:43,649 --> 00:21:50,230
so we want a way to instantly take the

00:21:48,100 --> 00:21:53,590
information Kai antek gives us based on

00:21:50,230 --> 00:21:55,869
how those metrics were differentiated

00:21:53,590 --> 00:21:57,789
between the two deployments and act on

00:21:55,869 --> 00:21:59,649
them either continue the deployment and

00:21:57,789 --> 00:22:01,570
take that release candidate and put it

00:21:59,649 --> 00:22:04,299
into our default production deployment

00:22:01,570 --> 00:22:07,149
facing all of our users or do we want to

00:22:04,299 --> 00:22:09,580
stop immediately roll back and put 100%

00:22:07,149 --> 00:22:14,860
of traffic back to our default

00:22:09,580 --> 00:22:18,159
deployment so how we did it is actually

00:22:14,860 --> 00:22:20,019
instead of destroying get the Canarian

00:22:18,159 --> 00:22:21,460
baseline and then you know redeploying

00:22:20,019 --> 00:22:26,259
or anything we're doing it completely

00:22:21,460 --> 00:22:27,580
with sto routing rules so right here is

00:22:26,259 --> 00:22:30,070
sort of our cleanup stage or our

00:22:27,580 --> 00:22:32,850
steady-state stage for every app this

00:22:30,070 --> 00:22:35,110
gets executed as you can see if the

00:22:32,850 --> 00:22:37,240
deployment pipeline is successful it

00:22:35,110 --> 00:22:39,909
fails whatever as soon as it's over we

00:22:37,240 --> 00:22:42,340
always want to go to steady-state if the

00:22:39,909 --> 00:22:44,679
canary failed we're not going to deploy

00:22:42,340 --> 00:22:46,360
the new code to the default production

00:22:44,679 --> 00:22:48,399
deployment meaning that this will

00:22:46,360 --> 00:22:50,529
actually be routing traffic 100% back to

00:22:48,399 --> 00:22:53,049
your old code if the canary was

00:22:50,529 --> 00:22:56,200
successful then that pipeline

00:22:53,049 --> 00:22:57,789
kaká wette prod brisket is going to

00:22:56,200 --> 00:23:00,009
actually complete the release take your

00:22:57,789 --> 00:23:01,869
release candidate and deploy it to the

00:23:00,009 --> 00:23:04,330
default deployment service name and this

00:23:01,869 --> 00:23:04,690
cleanup pipeline will then route 100% of

00:23:04,330 --> 00:23:08,200
trash

00:23:04,690 --> 00:23:09,999
to it with exception handling kind of

00:23:08,200 --> 00:23:11,259
being a bit shaky and spanker this is

00:23:09,999 --> 00:23:16,330
really the best way that was available

00:23:11,259 --> 00:23:18,849
to us so what we got out of it did all

00:23:16,330 --> 00:23:22,119
this work got Canaries in pat ourselves

00:23:18,849 --> 00:23:25,570
on the back what happened first the good

00:23:22,119 --> 00:23:27,159
I love this slide this was almost

00:23:25,570 --> 00:23:31,509
immediately after we went out in

00:23:27,159 --> 00:23:33,549
production an engineer added a new query

00:23:31,509 --> 00:23:35,440
to our monolith that ended up in a

00:23:33,549 --> 00:23:38,200
sequence can I guess our post course

00:23:35,440 --> 00:23:40,619
database it past unit s the past

00:23:38,200 --> 00:23:43,359
integration tests they passed manual QA

00:23:40,619 --> 00:23:44,679
it passed Canaries in our Devon test

00:23:43,359 --> 00:23:46,599
cluster because it wasn't getting enough

00:23:44,679 --> 00:23:49,840
traffic only when it went to production

00:23:46,599 --> 00:23:52,269
did we see this huge spike right and the

00:23:49,840 --> 00:23:54,070
canary instantly failed when we went and

00:23:52,269 --> 00:23:56,559
looked into New Relic at our APM metrics

00:23:54,070 --> 00:23:58,960
we could find the exact transaction that

00:23:56,559 --> 00:24:01,509
was causing it with a bunch of timeouts

00:23:58,960 --> 00:24:03,429
and that saved us from a production

00:24:01,509 --> 00:24:08,769
incident saving the majority of our

00:24:03,429 --> 00:24:11,440
users from downtime yay the bad so I'm

00:24:08,769 --> 00:24:14,349
gonna go over one big incident that

00:24:11,440 --> 00:24:17,950
actually occurred from releasing

00:24:14,349 --> 00:24:19,539
Canaries into our CD pipelines and this

00:24:17,950 --> 00:24:21,940
was a race condition that was kind of

00:24:19,539 --> 00:24:24,099
triggered by the force cache refresh

00:24:21,940 --> 00:24:25,659
issue and cloud driver so I showed you

00:24:24,099 --> 00:24:28,090
those two pipelines we have our default

00:24:25,659 --> 00:24:31,450
deployment pipeline and then that clean

00:24:28,090 --> 00:24:33,820
up pipeline what happened was we ended

00:24:31,450 --> 00:24:36,729
up deleting or destroying the baseline

00:24:33,820 --> 00:24:39,039
and canary deployments without resetting

00:24:36,729 --> 00:24:41,320
that routing to go 100% to our default

00:24:39,039 --> 00:24:42,729
deployment so we were still routing 5%

00:24:41,320 --> 00:24:46,599
of traffic to things that didn't exist

00:24:42,729 --> 00:24:51,729
and SEO hates that and it the bed

00:24:46,599 --> 00:24:53,619
and we had a production level outage so

00:24:51,729 --> 00:24:54,999
this is what our default deployment

00:24:53,619 --> 00:24:56,739
pipeline looks like it's kind of

00:24:54,999 --> 00:24:59,049
complicated I won't get into most of it

00:24:56,739 --> 00:25:00,340
but you can see that we deploy the

00:24:59,049 --> 00:25:02,830
baseline in canary

00:25:00,340 --> 00:25:05,289
we apply the caca wet manifest which is

00:25:02,830 --> 00:25:07,809
really that traffic management manifest

00:25:05,289 --> 00:25:09,369
that sets 90% of traffic to the default

00:25:07,809 --> 00:25:11,559
five and five to the Canaries and

00:25:09,369 --> 00:25:14,320
baselines and then does canary analysis

00:25:11,559 --> 00:25:16,149
if that last stage fails it will stop

00:25:14,320 --> 00:25:18,609
the deploy and not deploy new code if

00:25:16,149 --> 00:25:20,349
it's successful it will deploy new code

00:25:18,609 --> 00:25:22,629
but no matter what when this is

00:25:20,349 --> 00:25:24,519
completed it's gonna run the clean up

00:25:22,629 --> 00:25:27,339
pipeline which originally looked like

00:25:24,519 --> 00:25:29,379
this so apply kako and manifest again

00:25:27,339 --> 00:25:31,450
sets us to a steady state of applying

00:25:29,379 --> 00:25:33,460
100% of traffic to that canary and

00:25:31,450 --> 00:25:35,259
baseline and then we want to clean up

00:25:33,460 --> 00:25:37,629
after ourselves by destroying the canary

00:25:35,259 --> 00:25:40,269
baseline deployments the problem was is

00:25:37,629 --> 00:25:43,269
we were in the midst of destroying these

00:25:40,269 --> 00:25:44,499
group server groups and cloud driver was

00:25:43,269 --> 00:25:47,049
just spinning and spinning and spinning

00:25:44,499 --> 00:25:49,419
and spinning meanwhile because there's

00:25:47,049 --> 00:25:51,099
no mutex lock or any idea of locking

00:25:49,419 --> 00:25:52,989
down pipelines based on another running

00:25:51,099 --> 00:25:53,649
the default deployment pipelines started

00:25:52,989 --> 00:25:55,450
running again

00:25:53,649 --> 00:25:58,359
and as we're destroying these server

00:25:55,450 --> 00:26:00,159
groups it set that routing rule to route

00:25:58,359 --> 00:26:03,489
5% of traffic to those two things that

00:26:00,159 --> 00:26:05,859
no longer existed so that was a pretty

00:26:03,489 --> 00:26:09,580
hairy problem and going back to the

00:26:05,859 --> 00:26:11,440
pipeline you can see like we apply the

00:26:09,580 --> 00:26:13,479
cockpit manifest and then run canary

00:26:11,440 --> 00:26:15,879
analysis for a long time as it's hanging

00:26:13,479 --> 00:26:17,289
there that we noticed that cloud driver

00:26:15,879 --> 00:26:19,029
was in the midst of destroying something

00:26:17,289 --> 00:26:23,320
and only destroyed it midway through the

00:26:19,029 --> 00:26:24,580
canary test and bad things happened so

00:26:23,320 --> 00:26:26,970
we just changed it to instead of

00:26:24,580 --> 00:26:29,349
cleaning things up just focus on routing

00:26:26,970 --> 00:26:31,089
have 100% of traffic go to the default

00:26:29,349 --> 00:26:34,119
and just keep those canary baselines up

00:26:31,089 --> 00:26:37,349
so in the worst case we aren't routing

00:26:34,119 --> 00:26:41,080
traffic to something that doesn't exist

00:26:37,349 --> 00:26:42,940
the ugly so apart from actually

00:26:41,080 --> 00:26:45,700
operationalizing this and getting this

00:26:42,940 --> 00:26:47,379
to work there's a human problem here

00:26:45,700 --> 00:26:49,119
right this is a big change to how your

00:26:47,379 --> 00:26:50,889
apps get deployed

00:26:49,119 --> 00:26:52,839
that's me getting blamed for a bunch of

00:26:50,889 --> 00:26:56,440
things and my coworker is being

00:26:52,839 --> 00:26:58,210
justified ly pissed off about it these

00:26:56,440 --> 00:27:02,409
all stem from something called

00:26:58,210 --> 00:27:04,269
false negatives so you have your canary

00:27:02,409 --> 00:27:08,679
going and you can figure it with latency

00:27:04,269 --> 00:27:11,220
and server errors and it fails why you

00:27:08,679 --> 00:27:13,690
go in you look at logs a p.m. and

00:27:11,220 --> 00:27:15,269
nothing looks wrong then you go into

00:27:13,690 --> 00:27:18,369
your canary report and you notice that

00:27:15,269 --> 00:27:20,889
one request happened to timeout and

00:27:18,369 --> 00:27:23,230
cayenne two took that as this is bad

00:27:20,889 --> 00:27:25,570
right we were doing things in ten-minute

00:27:23,230 --> 00:27:28,059
windows it saw that as a breaking change

00:27:25,570 --> 00:27:29,260
and killed the pipeline which meant

00:27:28,059 --> 00:27:31,390
engineers would have to stay

00:27:29,260 --> 00:27:33,669
for the entire canary analysis stage

00:27:31,390 --> 00:27:35,650
which for us was an hour and maybe only

00:27:33,669 --> 00:27:38,110
45 minutes in at that fourth interval

00:27:35,650 --> 00:27:40,090
would it fail and then you have to go

00:27:38,110 --> 00:27:43,030
back look and see if our ending was

00:27:40,090 --> 00:27:45,309
wrong and it wasn't and confidence

00:27:43,030 --> 00:27:49,510
started to break down into what kind of

00:27:45,309 --> 00:27:53,200
value Canaries can bring so what we

00:27:49,510 --> 00:27:58,059
learned you have to treat pipeline

00:27:53,200 --> 00:27:59,860
changes as a production change so owning

00:27:58,059 --> 00:28:02,410
these managed deployments and this sort

00:27:59,860 --> 00:28:04,419
of master Jinja template that generates

00:28:02,410 --> 00:28:06,700
all these pipelines there's a lot of

00:28:04,419 --> 00:28:09,220
responsibility there right any change is

00:28:06,700 --> 00:28:11,679
going to affect every single pipeline

00:28:09,220 --> 00:28:13,510
that you deploy so what we ended up

00:28:11,679 --> 00:28:15,700
doing was forking that global templates

00:28:13,510 --> 00:28:17,980
and then opting in apps one by one

00:28:15,700 --> 00:28:20,830
getting champions to onboard and then

00:28:17,980 --> 00:28:23,740
bringing it to the rest of the org first

00:28:20,830 --> 00:28:27,669
time we released it and we did have an

00:28:23,740 --> 00:28:29,410
actual bad release change caught by

00:28:27,669 --> 00:28:31,690
Canaries there was a big question of

00:28:29,410 --> 00:28:33,400
what now because the canary and baseline

00:28:31,690 --> 00:28:35,260
are still putting metrics to the same

00:28:33,400 --> 00:28:36,640
tools and there wasn't a great way to

00:28:35,260 --> 00:28:38,740
differentiate between them

00:28:36,640 --> 00:28:41,110
we had the difference in data dog but

00:28:38,740 --> 00:28:42,700
before Canaries if you release a poorly

00:28:41,110 --> 00:28:44,440
performing deployment the first thing

00:28:42,700 --> 00:28:47,410
our engineers go to is the APM to see

00:28:44,440 --> 00:28:48,700
exactly which endpoint was broken so

00:28:47,410 --> 00:28:50,980
then they can go into github and see

00:28:48,700 --> 00:28:52,720
exactly which commits broke it so then

00:28:50,980 --> 00:28:54,790
they can get blame and yell at someone

00:28:52,720 --> 00:28:56,590
on slack right that's the process that

00:28:54,790 --> 00:28:57,940
you want to start streamlining so what

00:28:56,590 --> 00:29:00,160
we did is we made sure that every time

00:28:57,940 --> 00:29:02,679
we deploy canary and baseline they get

00:29:00,160 --> 00:29:06,610
new apps in all of our tools bugs snag

00:29:02,679 --> 00:29:09,190
log entries new relic so it's very easy

00:29:06,610 --> 00:29:10,630
for engineers to go in and debug the

00:29:09,190 --> 00:29:16,059
same way they would any other

00:29:10,630 --> 00:29:17,590
performance braking change the separate

00:29:16,059 --> 00:29:19,990
cleanup stage we actually originally

00:29:17,590 --> 00:29:22,510
tried to follow the best practices guide

00:29:19,990 --> 00:29:24,400
and have the Canaries analysis and

00:29:22,510 --> 00:29:27,910
deployment and cleanup all in one

00:29:24,400 --> 00:29:31,390
pipeline but spinnaker just doesn't give

00:29:27,910 --> 00:29:34,840
us that you know exception handling we

00:29:31,390 --> 00:29:37,270
need and so it was very difficult to do

00:29:34,840 --> 00:29:39,130
using the sort of catch all that

00:29:37,270 --> 00:29:40,630
spinnaker does provide where you can

00:29:39,130 --> 00:29:41,650
kick off a pipeline based on another

00:29:40,630 --> 00:29:44,670
ending

00:29:41,650 --> 00:29:44,670
we've had a lot of success with that

00:29:44,940 --> 00:29:49,980
data we also tried to release canary

00:29:48,220 --> 00:29:54,040
configurations with a lot of metrics

00:29:49,980 --> 00:29:56,290
latency CPU memory 5-xx everything and

00:29:54,040 --> 00:29:57,700
when you first release that and you're

00:29:56,290 --> 00:29:59,440
trying to tune things it's very

00:29:57,700 --> 00:30:01,240
difficult to do when you have ten knobs

00:29:59,440 --> 00:30:03,070
in front of you so we cut that down to

00:30:01,240 --> 00:30:07,020
just latency and we've had a lot more

00:30:03,070 --> 00:30:07,020
success onboarding apps that way

00:30:07,740 --> 00:30:11,380
intervals so going back to some of the

00:30:09,850 --> 00:30:14,800
goals and things we were optimizing for

00:30:11,380 --> 00:30:17,200
we want to fail fast and deploy fast but

00:30:14,800 --> 00:30:20,380
still get enough sample size to have

00:30:17,200 --> 00:30:23,170
confidence in our signal originally we

00:30:20,380 --> 00:30:25,570
had five ten-minute stages which meant

00:30:23,170 --> 00:30:29,080
your canary analysis would run for 10

00:30:25,570 --> 00:30:30,520
minutes evaluates and decide whether to

00:30:29,080 --> 00:30:34,120
continue or not and they would do that

00:30:30,520 --> 00:30:36,130
five times and we noticed that a small

00:30:34,120 --> 00:30:38,530
amount of noise in that fourth and fifth

00:30:36,130 --> 00:30:40,810
stage could break everything right so

00:30:38,530 --> 00:30:42,820
engineers we're gonna be waiting for 40

00:30:40,810 --> 00:30:46,300
minutes and if it fails you got to kick

00:30:42,820 --> 00:30:49,150
it off again so what we tried instead to

00:30:46,300 --> 00:30:51,460
balance that is to have one stage that

00:30:49,150 --> 00:30:55,080
runs for 10 minutes that looks at very

00:30:51,460 --> 00:30:58,450
heinous changes so server errors only

00:30:55,080 --> 00:31:01,240
and like crashing just like the really

00:30:58,450 --> 00:31:03,880
bad stuff for 10 minutes then 50 minutes

00:31:01,240 --> 00:31:08,680
for more performance issues you know

00:31:03,880 --> 00:31:10,690
latency men usage CPU etc so you still

00:31:08,680 --> 00:31:12,820
look like this we apply the routing roll

00:31:10,690 --> 00:31:15,970
do a canary analysis stage that's

00:31:12,820 --> 00:31:18,670
configured to run 5 10 minute stages and

00:31:15,970 --> 00:31:20,290
then deploy we changed that to have two

00:31:18,670 --> 00:31:22,150
different canary configurations one for

00:31:20,290 --> 00:31:24,130
the initial that's very aggressive over

00:31:22,150 --> 00:31:26,140
10 minutes and then the secondary one

00:31:24,130 --> 00:31:28,240
that runs longer gets more sample size

00:31:26,140 --> 00:31:33,370
and can get better signal on those

00:31:28,240 --> 00:31:36,790
latency metrics break glass deployments

00:31:33,370 --> 00:31:38,470
so something that we saw was deployments

00:31:36,790 --> 00:31:39,940
were now taking over an hour right

00:31:38,470 --> 00:31:42,640
because we wanted to get a large sample

00:31:39,940 --> 00:31:43,870
size and some developers really wanted

00:31:42,640 --> 00:31:44,980
to get something out for instance if

00:31:43,870 --> 00:31:46,360
it's 3:00 in the morning and they get

00:31:44,980 --> 00:31:47,230
paged and they absolutely need to

00:31:46,360 --> 00:31:48,940
release a hotfix

00:31:47,230 --> 00:31:52,720
how do they get around that our

00:31:48,940 --> 00:31:53,790
deployment what we did was add a

00:31:52,720 --> 00:31:56,010
parameter

00:31:53,790 --> 00:31:57,990
into our spinnaker pipeline that would

00:31:56,010 --> 00:32:01,020
allow you to skip all the canary stages

00:31:57,990 --> 00:32:03,150
so if you set that to true you wouldn't

00:32:01,020 --> 00:32:04,650
have that extra step of applying that

00:32:03,150 --> 00:32:06,420
routing deploying the deploying those

00:32:04,650 --> 00:32:09,000
two new deployments having the hour-long

00:32:06,420 --> 00:32:11,460
analysis stage which took us down from

00:32:09,000 --> 00:32:16,080
you know an hour fifteen minutes to sub

00:32:11,460 --> 00:32:17,580
ten-minute deployments some next steps

00:32:16,080 --> 00:32:20,010
things were thinking about in the future

00:32:17,580 --> 00:32:22,710
testing our Canaries so now we have a

00:32:20,010 --> 00:32:24,510
lot of data on containers that have

00:32:22,710 --> 00:32:26,970
successfully passed and shouldn't have

00:32:24,510 --> 00:32:29,450
and containers that didn't pass and

00:32:26,970 --> 00:32:32,490
shouldn't have we want to be able to

00:32:29,450 --> 00:32:34,230
tweak our canary config without waiting

00:32:32,490 --> 00:32:36,720
for an actual production changes that

00:32:34,230 --> 00:32:39,450
come in to assert that it works right we

00:32:36,720 --> 00:32:40,920
want to test with previous deployments

00:32:39,450 --> 00:32:43,620
much like your regression test that you

00:32:40,920 --> 00:32:47,870
would write for your app code we want to

00:32:43,620 --> 00:32:47,870
do the same thing for our canary configs

00:32:48,260 --> 00:32:51,680
adding business KPIs

00:32:50,190 --> 00:32:54,870
so in a micro service architecture

00:32:51,680 --> 00:32:57,060
looking at your own latency and impact

00:32:54,870 --> 00:32:59,400
on clients directly connecting to you

00:32:57,060 --> 00:33:01,350
sometimes isn't enough sometimes there's

00:32:59,400 --> 00:33:04,950
a small change that you make that isn't

00:33:01,350 --> 00:33:06,270
discoverable with HTTP metrics and you

00:33:04,950 --> 00:33:08,250
want to look at more of more of a high

00:33:06,270 --> 00:33:09,900
level especially for a monolith so when

00:33:08,250 --> 00:33:11,610
we make changes we want to see making

00:33:09,900 --> 00:33:13,890
sure that people can still log in and in

00:33:11,610 --> 00:33:21,000
appropriate time upload sheets view

00:33:13,890 --> 00:33:23,040
projects invite users etc chaos analysis

00:33:21,000 --> 00:33:26,790
so we've been talking today about mostly

00:33:23,040 --> 00:33:29,310
Canarian coaches but kite with cayenne

00:33:26,790 --> 00:33:31,710
tea you can compare anything right two

00:33:29,310 --> 00:33:33,930
deployments of anything so what we're

00:33:31,710 --> 00:33:36,330
trying to do now is actually have two

00:33:33,930 --> 00:33:38,820
deployments running the same code but

00:33:36,330 --> 00:33:40,620
using SDO we can actually cut out Redis

00:33:38,820 --> 00:33:42,630
connections for one deployment or make

00:33:40,620 --> 00:33:46,080
Postgres queries to take twice as long

00:33:42,630 --> 00:33:48,330
or kill connections to one of our third

00:33:46,080 --> 00:33:52,770
party AP eyes and we want to make sure

00:33:48,330 --> 00:33:54,750
that for assumptions we make like we can

00:33:52,770 --> 00:33:56,220
survive a Redis outage is that actually

00:33:54,750 --> 00:34:01,680
the case and we can actually automate

00:33:56,220 --> 00:34:03,810
this now with kyta in sto more

00:34:01,680 --> 00:34:06,240
intelligent routing so the way I've

00:34:03,810 --> 00:34:07,200
shown is that we route traffic purely

00:34:06,240 --> 00:34:10,740
based on percent

00:34:07,200 --> 00:34:13,679
it doesn't look at the headers or where

00:34:10,740 --> 00:34:15,240
the request is going it just does like

00:34:13,679 --> 00:34:17,490
sort of round robin with percentile

00:34:15,240 --> 00:34:20,429
points what we want to do in the future

00:34:17,490 --> 00:34:22,770
is actually be able to use more of is

00:34:20,429 --> 00:34:24,120
SEOs traffic management to be a little

00:34:22,770 --> 00:34:26,340
smarter about who we expose to the

00:34:24,120 --> 00:34:29,280
canary so maybe we want to don't want to

00:34:26,340 --> 00:34:31,410
expose 5% of all our users maybe we want

00:34:29,280 --> 00:34:37,800
to only expose the people using our free

00:34:31,410 --> 00:34:39,630
trial right or maybe we maybe we want to

00:34:37,800 --> 00:34:41,790
only expose plan guard users or like

00:34:39,630 --> 00:34:44,400
internal developers and QA right there's

00:34:41,790 --> 00:34:48,480
a lot of options there that you know SEO

00:34:44,400 --> 00:34:50,580
gives us that capability we can also

00:34:48,480 --> 00:34:53,640
canary totally other things DB

00:34:50,580 --> 00:34:55,410
migrations config changes any changes in

00:34:53,640 --> 00:34:57,810
state in our infrastructure we want to

00:34:55,410 --> 00:35:02,910
be able to leverage kai anta to help us

00:34:57,810 --> 00:35:04,830
out so thank you very much taking time

00:35:02,910 --> 00:35:08,520
out of your sunday if you'd like to talk

00:35:04,830 --> 00:35:13,050
more I'm a a hater on LinkedIn github

00:35:08,520 --> 00:35:13,870
and the spinnaker of slack space thank

00:35:13,050 --> 00:35:17,110
you very much

00:35:13,870 --> 00:35:17,110
[Applause]

00:35:20,420 --> 00:35:26,440
[Applause]

00:35:22,690 --> 00:35:26,440
and I think we have time

00:35:30,410 --> 00:35:33,520
[Applause]

00:35:33,790 --> 00:35:53,600
like automatically so I so in our

00:35:51,770 --> 00:35:55,220
default deployment pipelines what

00:35:53,600 --> 00:35:57,170
happens next after that canary analysis

00:35:55,220 --> 00:35:59,570
age ago successful as it goes to the

00:35:57,170 --> 00:36:00,830
deploy stage which is a we use the v1

00:35:59,570 --> 00:36:03,500
kubernetes provider that just does a

00:36:00,830 --> 00:36:05,840
Bluegreen deploy and after that you're

00:36:03,500 --> 00:36:07,490
in that state you have ninety percent of

00:36:05,840 --> 00:36:09,230
traffic going to the default deployment

00:36:07,490 --> 00:36:10,730
which is running your new code five

00:36:09,230 --> 00:36:12,590
percent running to the baseline which is

00:36:10,730 --> 00:36:13,610
running your old code five percent to

00:36:12,590 --> 00:36:15,890
the canary which is running your new

00:36:13,610 --> 00:36:17,690
code that clean up pipeline I showed

00:36:15,890 --> 00:36:19,400
listens to whenever that default

00:36:17,690 --> 00:36:21,200
deployment pipeline ends which would be

00:36:19,400 --> 00:36:23,810
right after that deployment and then

00:36:21,200 --> 00:36:25,790
sets 100% of traffic to the default

00:36:23,810 --> 00:36:28,850
deployment so that will be running your

00:36:25,790 --> 00:36:31,420
new code exposing 100% of traffic to the

00:36:28,850 --> 00:36:31,420
new deployment

00:36:37,940 --> 00:36:41,540
[Music]

00:36:39,280 --> 00:36:43,400
yeah that's a great idea we don't do

00:36:41,540 --> 00:36:45,230
that but that's definitely something we

00:36:43,400 --> 00:36:48,340
could explore

00:36:45,230 --> 00:36:48,340
[Music]

00:37:13,550 --> 00:37:19,560
mm-hmm yeah so really what's what's a

00:37:17,550 --> 00:37:21,450
know what spinnaker does to make that

00:37:19,560 --> 00:37:23,700
really useful for code deployments is

00:37:21,450 --> 00:37:25,710
that we know exactly when new code is

00:37:23,700 --> 00:37:27,990
being deployed to a Canadian baseline so

00:37:25,710 --> 00:37:31,140
we can tell it now is the time to run a

00:37:27,990 --> 00:37:32,910
canary analysis stage but that doesn't

00:37:31,140 --> 00:37:34,590
necessarily need to be the case you can

00:37:32,910 --> 00:37:36,390
kick off spinnaker pipelines you know

00:37:34,590 --> 00:37:39,960
from anywhere so you can have your own

00:37:36,390 --> 00:37:41,790
thing whatever it is doing something and

00:37:39,960 --> 00:37:44,370
then it can make a call to spinnaker

00:37:41,790 --> 00:37:45,810
saying hey I need to run this canary or

00:37:44,370 --> 00:37:47,910
cayenne to state where I want to compare

00:37:45,810 --> 00:37:49,320
metrics from two different things do

00:37:47,910 --> 00:37:51,960
that and just it and then make a

00:37:49,320 --> 00:37:54,510
decision spinnaker just makes it really

00:37:51,960 --> 00:37:57,500
easy to tie it and visualize it with the

00:37:54,510 --> 00:37:57,500
flow of your deployment

00:38:07,920 --> 00:38:20,529
implementation and what about ito

00:38:12,059 --> 00:38:34,630
because so you are how do you make sure

00:38:20,529 --> 00:38:36,999
that he doesn't get yes

00:38:34,630 --> 00:38:39,999
very scary and we haven't really figured

00:38:36,999 --> 00:38:42,849
it out basically the way we have it

00:38:39,999 --> 00:38:45,400
running is the only places that define

00:38:42,849 --> 00:38:48,160
routing rules are in those to deployment

00:38:45,400 --> 00:38:49,839
pipelines I showed you and we vary we

00:38:48,160 --> 00:38:51,489
limit access to our kubernetes

00:38:49,839 --> 00:38:53,470
controllers to a very small set of

00:38:51,489 --> 00:38:55,119
engineers so no one can just go in and

00:38:53,470 --> 00:38:57,099
like cube cut' will apply some some

00:38:55,119 --> 00:38:58,630
broken however it's very possible

00:38:57,099 --> 00:39:00,989
that someone comes into spinnaker and

00:38:58,630 --> 00:39:03,400
goes into one of these pipeline

00:39:00,989 --> 00:39:04,930
configurations and fat-finger or

00:39:03,400 --> 00:39:07,299
something and deploys it by an and

00:39:04,930 --> 00:39:14,380
breaks it it's very difficult to

00:39:07,299 --> 00:39:17,589
restrict that so something we want to do

00:39:14,380 --> 00:39:20,049
is rip out that routing from the

00:39:17,589 --> 00:39:22,660
deployment pipeline and have only one

00:39:20,049 --> 00:39:24,609
single pipeline to find routing and it

00:39:22,660 --> 00:39:27,339
be configurable and then we want to use

00:39:24,609 --> 00:39:29,200
something like Fiat to restrict access

00:39:27,339 --> 00:39:32,049
to that whether it lives in a different

00:39:29,200 --> 00:39:33,640
application or whatever yeah we'd like

00:39:32,049 --> 00:39:35,600
to have that more centralized and a

00:39:33,640 --> 00:39:38,699
little more security around it

00:39:35,600 --> 00:39:38,699
[Music]

00:39:50,240 --> 00:39:55,590
right so first thing we went off was

00:39:53,360 --> 00:39:57,990
google has a great best practices one

00:39:55,590 --> 00:40:01,400
that they've had experience with at ways

00:39:57,990 --> 00:40:04,170
and we went off that and it suggested

00:40:01,400 --> 00:40:05,910
instead of time it looks at number of

00:40:04,170 --> 00:40:08,130
data points that you need to make a

00:40:05,910 --> 00:40:10,290
decision so we started off with an hour

00:40:08,130 --> 00:40:12,090
and we're still rolling with that but we

00:40:10,290 --> 00:40:14,070
are still seeing some issues where we

00:40:12,090 --> 00:40:16,950
don't get enough sample size for non

00:40:14,070 --> 00:40:19,290
monolithic apps so it's a lot of

00:40:16,950 --> 00:40:21,630
tweaking you know a lot of this is sort

00:40:19,290 --> 00:40:23,370
of continuous improvements the good

00:40:21,630 --> 00:40:25,230
thing about canary configs and

00:40:23,370 --> 00:40:26,880
deployments in general is that instead

00:40:25,230 --> 00:40:29,580
of pushing all the work to manage

00:40:26,880 --> 00:40:31,800
testing on to the feature teams who will

00:40:29,580 --> 00:40:33,300
inevitably drop it at least it is a

00:40:31,800 --> 00:40:35,490
centralized place where we can make an

00:40:33,300 --> 00:40:37,530
impact for all services right so if we

00:40:35,490 --> 00:40:40,080
find out that extending it to two hours

00:40:37,530 --> 00:40:42,060
and you know one hour windows greatly

00:40:40,080 --> 00:40:43,320
improve signal for some of our apps we

00:40:42,060 --> 00:40:45,000
can roll that out to any of everyone

00:40:43,320 --> 00:40:46,920
very easily but still a lot of

00:40:45,000 --> 00:40:49,400
experimentation we haven't figured out

00:40:46,920 --> 00:40:49,400
total yet

00:41:07,140 --> 00:41:13,720
we did we had sort of an SEO champion at

00:41:10,960 --> 00:41:15,460
our company who was very comfortable

00:41:13,720 --> 00:41:17,770
with it we actually started using it in

00:41:15,460 --> 00:41:21,700
our staging environment at point zero

00:41:17,770 --> 00:41:23,140
eight point eight version so early

00:41:21,700 --> 00:41:25,690
adopters we got pretty comfortable with

00:41:23,140 --> 00:41:27,820
it we actually used it mostly as an API

00:41:25,690 --> 00:41:29,619
gateway that's that the start we didn't

00:41:27,820 --> 00:41:32,350
deploy sidecars until very recently so

00:41:29,619 --> 00:41:48,850
it was totally used for as basically an

00:41:32,350 --> 00:41:50,470
ingress controller we yeah we actually

00:41:48,850 --> 00:41:52,780
do it at the service level so inside

00:41:50,470 --> 00:41:55,210
that like master pipeline JSON we give

00:41:52,780 --> 00:41:56,980
everyone a sidecar who at first it was

00:41:55,210 --> 00:41:58,570
opt-in right because we want to do some

00:41:56,980 --> 00:42:00,040
incremental changes and not just blast

00:41:58,570 --> 00:42:01,990
our entire infrastructure in production

00:42:00,040 --> 00:42:07,390
with sidecars we want to see how it

00:42:01,990 --> 00:42:09,540
impacted individual apps and actually

00:42:07,390 --> 00:42:11,980
we've found a lot of issues there where

00:42:09,540 --> 00:42:14,590
our apps have had trouble connecting to

00:42:11,980 --> 00:42:16,600
Redis over SSL through sidecars which

00:42:14,590 --> 00:42:18,280
seems to be a known issue so we still

00:42:16,600 --> 00:42:22,960
have a lot of like opt outs for certain

00:42:18,280 --> 00:42:25,060
IPS in sidecars so I would recommend if

00:42:22,960 --> 00:42:27,550
anyone's trying to implement SEO and

00:42:25,060 --> 00:42:29,830
their clouds don't go straight into the

00:42:27,550 --> 00:42:33,040
automatic sidecar and jet injection do

00:42:29,830 --> 00:42:35,140
it app by app in any way that you would

00:42:33,040 --> 00:42:37,440
you know normally rollout infrastructure

00:42:35,140 --> 00:42:37,440
changes

00:42:41,390 --> 00:42:47,950
[Laughter]

00:42:45,890 --> 00:42:47,950

YouTube URL: https://www.youtube.com/watch?v=FLQIJNdsmH4


