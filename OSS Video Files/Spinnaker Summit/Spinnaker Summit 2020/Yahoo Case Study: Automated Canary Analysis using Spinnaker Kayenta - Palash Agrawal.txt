Title: Yahoo Case Study: Automated Canary Analysis using Spinnaker Kayenta - Palash Agrawal
Publication date: 2021-01-28
Playlist: Spinnaker Summit 2020
Description: 
	Canary Analysis is an essential part of any production deployment pipeline. As part of Yahoo/Verizon Media, we have many properties like Yahoo Homepage, Yahoo Mail, Yahoo Finance, Yahoo News, Yahoo Sports, TechCrunch, HuffPost, etc. that serve a large number of users. It is crucial to introduce changes in a consistently safe manner to minimize user impact minutes. At Yahoo/Verizon Media, we are running Spinnaker Kayenta in a Kubernetes Deployment and have integrated it with Screwdriver (https://screwdriver.cd/), an open source build platform for continuous delivery. We have extended Kayenta to support Splunk and an OpenTSDB based internal metrics service. We also built tools to facilitate easy onboarding for applications and designed a configuration syntax based on the Hashicorp Configuration Language (HCL) for running canary analysis. A deployment of Nike’s Referee (https://github.com/Nike-Inc/referee) tool is also set up to allow on-demand canary analysis. In this talk, we’ll share how we adopted, implemented and extended Spinnaker Kayenta to perform automated canary analysis in the Screwdriver CD solution used at Verizon Media.
Captions: 
	00:00:00,530 --> 00:00:03,629
[Music]

00:00:05,759 --> 00:00:08,639
hello everyone

00:00:06,879 --> 00:00:10,559
thank you very much for tuning in i'm

00:00:08,639 --> 00:00:11,440
palash agrawal from yao sports verizon

00:00:10,559 --> 00:00:12,960
media

00:00:11,440 --> 00:00:15,280
and i'm going to talk about how we

00:00:12,960 --> 00:00:18,560
adopted spinnaker kayata to perform

00:00:15,280 --> 00:00:20,880
automated canadian analysis in yahoo

00:00:18,560 --> 00:00:22,480
this is me i'm a principal software

00:00:20,880 --> 00:00:25,199
engineer at yahoo sports

00:00:22,480 --> 00:00:27,599
which is now a brand under verizon media

00:00:25,199 --> 00:00:29,439
i lead the infrastructure team at sports

00:00:27,599 --> 00:00:31,599
i've been at yahoo for about eight and a

00:00:29,439 --> 00:00:32,960
half years i've worked on all things

00:00:31,599 --> 00:00:35,680
back in all these years

00:00:32,960 --> 00:00:37,280
data processing storage apis real-time

00:00:35,680 --> 00:00:39,440
event processing systems

00:00:37,280 --> 00:00:41,200
and now i'm focusing on making all of it

00:00:39,440 --> 00:00:42,640
better by working on our underlying

00:00:41,200 --> 00:00:43,920
infrastructure

00:00:42,640 --> 00:00:45,760
some of the work that the team

00:00:43,920 --> 00:00:46,640
accomplished includes migrating classic

00:00:45,760 --> 00:00:48,559
applications

00:00:46,640 --> 00:00:50,320
from physical machines to kubernetes

00:00:48,559 --> 00:00:52,239
thus saving around one million dollars

00:00:50,320 --> 00:00:54,120
in operating cost annually

00:00:52,239 --> 00:00:55,840
we also made our ci cd pipeline

00:00:54,120 --> 00:00:57,680
manufacturer

00:00:55,840 --> 00:00:59,520
going from one releases a week to

00:00:57,680 --> 00:01:01,359
multiple times a day

00:00:59,520 --> 00:01:03,440
we built a performance testing tool that

00:01:01,359 --> 00:01:04,960
integrates with our cic solution

00:01:03,440 --> 00:01:06,320
we are working on open sourcing it

00:01:04,960 --> 00:01:08,159
hopefully we will have it ready to share

00:01:06,320 --> 00:01:10,640
with the community soon

00:01:08,159 --> 00:01:12,880
if that sounds interesting we are hiring

00:01:10,640 --> 00:01:14,960
please do reach out to me on linkedin

00:01:12,880 --> 00:01:16,640
i did my masters in computer science

00:01:14,960 --> 00:01:19,520
from ucla in 2012

00:01:16,640 --> 00:01:19,840
co-province i'm a huge chelsea fan and

00:01:19,520 --> 00:01:21,439
i'm

00:01:19,840 --> 00:01:23,840
very optimistic about the team under

00:01:21,439 --> 00:01:24,560
frank lampard needless to say i love

00:01:23,840 --> 00:01:25,920
soccer

00:01:24,560 --> 00:01:28,560
and being indian i obviously love

00:01:25,920 --> 00:01:30,640
cricket i like to think that i'm pretty

00:01:28,560 --> 00:01:33,520
good at fifa so if you play on xbox hit

00:01:30,640 --> 00:01:35,040
me up after this presentation for sure

00:01:33,520 --> 00:01:36,960
many of you would be wondering hey

00:01:35,040 --> 00:01:38,320
palace you work for down sports what is

00:01:36,960 --> 00:01:40,799
verizon media

00:01:38,320 --> 00:01:42,079
well verizon media is home to media tech

00:01:40,799 --> 00:01:43,920
and communication products

00:01:42,079 --> 00:01:46,159
that more than a billion people love and

00:01:43,920 --> 00:01:47,200
trust some of the leading brands in its

00:01:46,159 --> 00:01:51,200
portfolio include

00:01:47,200 --> 00:01:54,720
aol huffpost techcrunch yahoo finance

00:01:51,200 --> 00:01:56,880
yahoomail and of course your sports

00:01:54,720 --> 00:01:57,840
so here's the agenda for today we start

00:01:56,880 --> 00:01:59,680
by discussing

00:01:57,840 --> 00:02:01,040
a little bit about the ci cd setup at

00:01:59,680 --> 00:02:02,240
verizon media

00:02:01,040 --> 00:02:05,200
and the state of the metric space

00:02:02,240 --> 00:02:07,360
promotion we briefly introduced kayanta

00:02:05,200 --> 00:02:09,599
and big items considered before deciding

00:02:07,360 --> 00:02:11,440
to use it for calorie analysis

00:02:09,599 --> 00:02:14,160
next we talk about our kind of set of

00:02:11,440 --> 00:02:17,440
ideas what modifications are done etc

00:02:14,160 --> 00:02:19,360
and finally we discuss our future plans

00:02:17,440 --> 00:02:21,280
so let's get started by talking about ci

00:02:19,360 --> 00:02:23,599
cd verizon media

00:02:21,280 --> 00:02:24,959
we use screwdriver as our tool of choice

00:02:23,599 --> 00:02:27,120
for cicd

00:02:24,959 --> 00:02:28,480
it's a homegrown solution which is now

00:02:27,120 --> 00:02:31,680
an open source project

00:02:28,480 --> 00:02:33,120
and it's part of the cd foundation some

00:02:31,680 --> 00:02:33,840
of the salient features of screwdriver

00:02:33,120 --> 00:02:36,959
include

00:02:33,840 --> 00:02:38,560
secure build and deploy of code it

00:02:36,959 --> 00:02:40,959
integrates well with the developer's

00:02:38,560 --> 00:02:42,400
daily software development cycle support

00:02:40,959 --> 00:02:43,599
for pull request builds and status

00:02:42,400 --> 00:02:46,400
checks

00:02:43,599 --> 00:02:47,360
tamil based dsl to declare the pipeline

00:02:46,400 --> 00:02:49,120
as code

00:02:47,360 --> 00:02:50,800
and finally it can run anywhere bring

00:02:49,120 --> 00:02:52,959
your own executors service and data

00:02:50,800 --> 00:02:54,959
store

00:02:52,959 --> 00:02:56,160
so what is metric space promotion as the

00:02:54,959 --> 00:02:57,920
name implies it is

00:02:56,160 --> 00:02:59,280
promoting a new bill to production if

00:02:57,920 --> 00:03:01,200
certain thresholds are met for

00:02:59,280 --> 00:03:03,360
predefined set of metrics

00:03:01,200 --> 00:03:05,200
at ryzen media we had this existing

00:03:03,360 --> 00:03:06,560
solution for that but it had its share

00:03:05,200 --> 00:03:08,720
of problems

00:03:06,560 --> 00:03:10,000
the configuration was often repeated and

00:03:08,720 --> 00:03:11,840
you would end up with a huge pull

00:03:10,000 --> 00:03:14,000
request of json that most people would

00:03:11,840 --> 00:03:16,400
finally approve

00:03:14,000 --> 00:03:17,040
it only supports static thresholds for

00:03:16,400 --> 00:03:19,360
example

00:03:17,040 --> 00:03:20,959
fail the bill if 5x6 errors are more

00:03:19,360 --> 00:03:23,040
than 5

00:03:20,959 --> 00:03:24,640
this is a valid strategy but often

00:03:23,040 --> 00:03:26,879
increases in 5x6 were not

00:03:24,640 --> 00:03:28,319
caused only by the change in code the

00:03:26,879 --> 00:03:31,200
same issues were being offered

00:03:28,319 --> 00:03:32,720
in the production deployment as well it

00:03:31,200 --> 00:03:34,400
was not extensible the tool only

00:03:32,720 --> 00:03:35,440
supported splunk and the internal metric

00:03:34,400 --> 00:03:37,120
store

00:03:35,440 --> 00:03:39,040
the biggest pain point i believe was

00:03:37,120 --> 00:03:40,560
tight dependency on libraries

00:03:39,040 --> 00:03:42,400
we had an issue where the library was

00:03:40,560 --> 00:03:43,760
not upgraded to support note 12 and we

00:03:42,400 --> 00:03:44,319
had to turn off the analysis till the

00:03:43,760 --> 00:03:46,640
time

00:03:44,319 --> 00:03:47,519
the client library was upgraded and even

00:03:46,640 --> 00:03:49,760
after the upgrade there are

00:03:47,519 --> 00:03:51,200
compatibility issues

00:03:49,760 --> 00:03:53,040
so we looked at automated current

00:03:51,200 --> 00:03:54,799
analysis using calendar

00:03:53,040 --> 00:03:56,480
as you would have guessed kinetic is a

00:03:54,799 --> 00:03:57,920
spinnaker service that utilizes

00:03:56,480 --> 00:04:00,799
statistical methods like

00:03:57,920 --> 00:04:02,000
declassification interquartile range etc

00:04:00,799 --> 00:04:03,920
to analyze metrics

00:04:02,000 --> 00:04:05,439
fetched on various data sources for the

00:04:03,920 --> 00:04:06,720
canary and baseline deployments

00:04:05,439 --> 00:04:09,519
and provides score for the canadian

00:04:06,720 --> 00:04:12,000
deployment thresholds can be said

00:04:09,519 --> 00:04:13,439
to pass or fail the deployment based on

00:04:12,000 --> 00:04:15,120
the score

00:04:13,439 --> 00:04:16,959
kind of fit our requirements it was

00:04:15,120 --> 00:04:18,639
extensible it compared two deployments

00:04:16,959 --> 00:04:20,160
receiving slice of production traffic

00:04:18,639 --> 00:04:22,320
and was loosely coupled to the actual

00:04:20,160 --> 00:04:23,840
deployment

00:04:22,320 --> 00:04:25,759
if you've used kanta before you must be

00:04:23,840 --> 00:04:27,120
familiar with this image the traffic is

00:04:25,759 --> 00:04:29,520
split between three deployments

00:04:27,120 --> 00:04:31,040
production baseline and canary

00:04:29,520 --> 00:04:33,040
production has the most servers and

00:04:31,040 --> 00:04:35,360
takes the majority of the traffic

00:04:33,040 --> 00:04:36,560
while baseline and canary have small but

00:04:35,360 --> 00:04:39,680
same number of servers

00:04:36,560 --> 00:04:41,280
and take a small amount of traffic only

00:04:39,680 --> 00:04:43,040
canadian deployment has the new version

00:04:41,280 --> 00:04:44,720
of the software

00:04:43,040 --> 00:04:46,080
the metrics from all these deployments

00:04:44,720 --> 00:04:48,400
are stored in the metric store

00:04:46,080 --> 00:04:49,840
of application owner's choice gallery

00:04:48,400 --> 00:04:51,600
analysis is then performed on these

00:04:49,840 --> 00:04:54,080
metrics

00:04:51,600 --> 00:04:55,840
here's the same thing in a table format

00:04:54,080 --> 00:04:57,280
it is important to note that all three

00:04:55,840 --> 00:04:57,919
deployments are in your production

00:04:57,280 --> 00:05:00,320
environment

00:04:57,919 --> 00:05:02,160
taking live production traffic the

00:05:00,320 --> 00:05:03,440
downstream dependencies are the same as

00:05:02,160 --> 00:05:05,520
production

00:05:03,440 --> 00:05:06,880
only difference is the slice of traffic

00:05:05,520 --> 00:05:08,479
taken by the deployments

00:05:06,880 --> 00:05:10,400
and the fact that canary has a new

00:05:08,479 --> 00:05:12,479
version of the code

00:05:10,400 --> 00:05:14,240
for accurate canary analysis it is very

00:05:12,479 --> 00:05:15,680
important that baseline and canary are

00:05:14,240 --> 00:05:18,080
exactly the same

00:05:15,680 --> 00:05:19,680
number of servers traffic split etc only

00:05:18,080 --> 00:05:20,560
difference is that the query has new

00:05:19,680 --> 00:05:22,720
code

00:05:20,560 --> 00:05:23,680
so the example here assuming you send

00:05:22,720 --> 00:05:26,240
traffic

00:05:23,680 --> 00:05:29,600
ninety percent to production baseline

00:05:26,240 --> 00:05:31,280
and canadians and five percent each

00:05:29,600 --> 00:05:32,639
so how does this look in a continuous

00:05:31,280 --> 00:05:34,000
delivery pipeline

00:05:32,639 --> 00:05:35,600
assuming the traffic is with ninety

00:05:34,000 --> 00:05:36,960
percent production and five percent each

00:05:35,600 --> 00:05:38,720
to catering baseline

00:05:36,960 --> 00:05:40,720
when a new change is merged the new code

00:05:38,720 --> 00:05:42,720
is deployed only to canary

00:05:40,720 --> 00:05:44,320
canary analysis performed next and if

00:05:42,720 --> 00:05:45,600
the canary analysis passes

00:05:44,320 --> 00:05:47,520
baseline and production deployments get

00:05:45,600 --> 00:05:49,360
the new code

00:05:47,520 --> 00:05:51,759
so let's talk about how that is achieved

00:05:49,360 --> 00:05:53,280
at verizon media

00:05:51,759 --> 00:05:55,039
we introduced screwdriver earlier in the

00:05:53,280 --> 00:05:55,680
presentation a typical screwdriver

00:05:55,039 --> 00:05:58,400
pipeline

00:05:55,680 --> 00:05:59,199
looks like the top image a comment leads

00:05:58,400 --> 00:06:01,280
to a curable

00:05:59,199 --> 00:06:02,720
and deployment followed by the staging

00:06:01,280 --> 00:06:04,319
deployment

00:06:02,720 --> 00:06:06,639
assuming the functional and integration

00:06:04,319 --> 00:06:07,680
test paths we move on to canary and

00:06:06,639 --> 00:06:09,680
production deployments in their

00:06:07,680 --> 00:06:11,360
respective data centers

00:06:09,680 --> 00:06:13,280
in a new setup we introduce a

00:06:11,360 --> 00:06:14,160
screwdriver job that performs canary

00:06:13,280 --> 00:06:15,520
analysis

00:06:14,160 --> 00:06:17,600
after the canary deployment in the

00:06:15,520 --> 00:06:19,919
respective data centers

00:06:17,600 --> 00:06:21,120
if the canary analysis passes the code

00:06:19,919 --> 00:06:22,160
is deployed to the baseline

00:06:21,120 --> 00:06:24,319
and production deployments in the

00:06:22,160 --> 00:06:26,479
respective data center

00:06:24,319 --> 00:06:27,840
this is obviously a simplistic view you

00:06:26,479 --> 00:06:29,919
need to make sure that you do not have a

00:06:27,840 --> 00:06:32,000
share because canadian analysis pass

00:06:29,919 --> 00:06:33,199
in one data center and not the other

00:06:32,000 --> 00:06:34,560
don't want to end up with a different

00:06:33,199 --> 00:06:37,360
code and differentiation centers

00:06:34,560 --> 00:06:38,800
production engineering 101. we recommend

00:06:37,360 --> 00:06:40,240
that the canary analysis be performed

00:06:38,800 --> 00:06:41,919
for at least 60 minutes

00:06:40,240 --> 00:06:43,600
or to get enough data points from

00:06:41,919 --> 00:06:45,680
various metric stores

00:06:43,600 --> 00:06:47,520
the statistical methods used are more

00:06:45,680 --> 00:06:48,960
accurate when there is more data and

00:06:47,520 --> 00:06:49,280
there could be false positives if you

00:06:48,960 --> 00:06:52,720
know

00:06:49,280 --> 00:06:54,160
if the analysis is run for your duration

00:06:52,720 --> 00:06:56,479
so let's look at the setup in more

00:06:54,160 --> 00:06:59,199
detail here's the money slide

00:06:56,479 --> 00:07:00,720
this is how we have canada setup we have

00:06:59,199 --> 00:07:02,080
a kubernetes deployment of the candidate

00:07:00,720 --> 00:07:04,400
service with redis running in the

00:07:02,080 --> 00:07:06,080
container as lre cache

00:07:04,400 --> 00:07:08,479
currently uses redis to store

00:07:06,080 --> 00:07:10,880
intermediate results during an execution

00:07:08,479 --> 00:07:11,840
you can run redis as a sidecar too the

00:07:10,880 --> 00:07:14,639
canary configs

00:07:11,840 --> 00:07:16,720
and execution results are archived in an

00:07:14,639 --> 00:07:18,639
aws3 bucket

00:07:16,720 --> 00:07:20,720
a kubernetes deployment of referee is

00:07:18,639 --> 00:07:22,960
also set up to provide a ui to analyze

00:07:20,720 --> 00:07:24,880
the canary analysis results

00:07:22,960 --> 00:07:27,440
on the left we have all the studio jobs

00:07:24,880 --> 00:07:29,360
in order to talk to bottom

00:07:27,440 --> 00:07:31,440
first we have the canary deployment

00:07:29,360 --> 00:07:33,680
followed by the canary analysis job

00:07:31,440 --> 00:07:35,120
the canary analysis job orchestrates the

00:07:33,680 --> 00:07:36,080
creation of canary conflict and

00:07:35,120 --> 00:07:38,880
execution

00:07:36,080 --> 00:07:40,800
of canadian assist with canada if the

00:07:38,880 --> 00:07:42,400
canary analysis job passes

00:07:40,800 --> 00:07:44,160
the baseline and production deployments

00:07:42,400 --> 00:07:45,840
are proceeded

00:07:44,160 --> 00:07:47,120
metrics from the canary baseline and

00:07:45,840 --> 00:07:48,720
production deployments are stored in

00:07:47,120 --> 00:07:51,680
various metric stores like splunk

00:07:48,720 --> 00:07:53,440
prometheus and an internal metric store

00:07:51,680 --> 00:07:57,440
kind of fetches metrics from these

00:07:53,440 --> 00:07:57,440
stores and performs the query analysis

00:07:57,520 --> 00:08:00,879
to make that possible we made some

00:07:59,039 --> 00:08:02,639
modifications to kind of

00:08:00,879 --> 00:08:04,240
we wrote custom plugins to fetch metrics

00:08:02,639 --> 00:08:05,199
from splunk and our internal metric

00:08:04,240 --> 00:08:07,360
store

00:08:05,199 --> 00:08:08,400
we extended the http client to work with

00:08:07,360 --> 00:08:10,000
what coupons

00:08:08,400 --> 00:08:11,199
we also extended it to work with

00:08:10,000 --> 00:08:12,639
services that require empty less

00:08:11,199 --> 00:08:14,160
authentication

00:08:12,639 --> 00:08:15,840
currently current performs canary

00:08:14,160 --> 00:08:17,039
analysis only over a single matrix

00:08:15,840 --> 00:08:18,800
account

00:08:17,039 --> 00:08:20,479
we modify the code to allow for analysis

00:08:18,800 --> 00:08:22,160
by fetching metrics from multiple stores

00:08:20,479 --> 00:08:25,280
like splunk parameters in the same

00:08:22,160 --> 00:08:26,800
career analysis execution

00:08:25,280 --> 00:08:28,560
one of our biggest pain points from the

00:08:26,800 --> 00:08:30,000
existing metrics based promotion

00:08:28,560 --> 00:08:33,120
solution at the company

00:08:30,000 --> 00:08:34,800
was repeated in huge configurations to

00:08:33,120 --> 00:08:36,159
tackle that we wrote a hcl inspired

00:08:34,800 --> 00:08:38,880
domain-specific language

00:08:36,159 --> 00:08:39,440
parent we are huge fans of terraform

00:08:38,880 --> 00:08:42,320
here

00:08:39,440 --> 00:08:44,399
and this was usually inspired by it hcl

00:08:42,320 --> 00:08:46,160
is hashicorp configuration language

00:08:44,399 --> 00:08:47,680
it is more human readable compared to

00:08:46,160 --> 00:08:49,519
json and yamls

00:08:47,680 --> 00:08:51,040
the kineta dsl generates the clarity

00:08:49,519 --> 00:08:53,600
configurations and execution

00:08:51,040 --> 00:08:55,839
configurations required by karenta

00:08:53,600 --> 00:08:57,440
here is an example on the right we allow

00:08:55,839 --> 00:08:59,040
the application owners to specify the

00:08:57,440 --> 00:09:01,760
application name description

00:08:59,040 --> 00:09:03,360
the duration for the analysis we also

00:09:01,760 --> 00:09:05,200
allow them to set up a warmer duration

00:09:03,360 --> 00:09:07,360
to let the caches warm up in the newly

00:09:05,200 --> 00:09:09,760
deployed canary

00:09:07,360 --> 00:09:11,200
application owners define reusable

00:09:09,760 --> 00:09:13,279
blocks as contexts

00:09:11,200 --> 00:09:14,880
which provide a value for canary and

00:09:13,279 --> 00:09:16,800
baseline deployments

00:09:14,880 --> 00:09:18,880
so in the example on the right we have a

00:09:16,800 --> 00:09:20,880
krs context for kubernetes

00:09:18,880 --> 00:09:23,279
where the user specifies the cluster

00:09:20,880 --> 00:09:24,320
namespace and the deployments for canary

00:09:23,279 --> 00:09:26,800
and baseline

00:09:24,320 --> 00:09:28,320
we name this context production east we

00:09:26,800 --> 00:09:30,080
wrote code to figure out

00:09:28,320 --> 00:09:31,839
the latest replica set deployed in

00:09:30,080 --> 00:09:33,600
canary and baseline deployments

00:09:31,839 --> 00:09:36,000
similarly you can have a constant

00:09:33,600 --> 00:09:38,399
context where the values for query and

00:09:36,000 --> 00:09:40,000
baseline can be explicitly set metric

00:09:38,399 --> 00:09:41,839
accounts can be specified for both

00:09:40,000 --> 00:09:44,080
canary and baseline deployment

00:09:41,839 --> 00:09:44,959
this is just for added flexibility we

00:09:44,080 --> 00:09:47,279
name this account

00:09:44,959 --> 00:09:49,279
promote this account in our example

00:09:47,279 --> 00:09:50,880
finally the magic happens in the metrics

00:09:49,279 --> 00:09:52,959
group section

00:09:50,880 --> 00:09:54,720
you can provide a wait for the group and

00:09:52,959 --> 00:09:56,320
other configurations

00:09:54,720 --> 00:09:57,920
and you have multiple metric stores to

00:09:56,320 --> 00:09:59,279
fetch the data from

00:09:57,920 --> 00:10:01,200
you can refer to the metric accounts

00:09:59,279 --> 00:10:02,160
specified earlier promote this account

00:10:01,200 --> 00:10:03,920
in this case

00:10:02,160 --> 00:10:05,680
and list the metrics you want to fetch

00:10:03,920 --> 00:10:07,040
from that account for both canary and

00:10:05,680 --> 00:10:10,000
baseline

00:10:07,040 --> 00:10:11,200
as mentioned before contacts are usable

00:10:10,000 --> 00:10:13,279
and you can refer to them

00:10:11,200 --> 00:10:14,720
and assign them to a variable that can

00:10:13,279 --> 00:10:15,760
be used in the query that is passed to

00:10:14,720 --> 00:10:18,000
calendar

00:10:15,760 --> 00:10:19,440
so in this example the creator's context

00:10:18,000 --> 00:10:22,240
name production east

00:10:19,440 --> 00:10:23,920
is assigned to controller now dollar

00:10:22,240 --> 00:10:24,560
dollar controller can be used in the

00:10:23,920 --> 00:10:26,880
query

00:10:24,560 --> 00:10:28,560
and is passed to kineta which evaluates

00:10:26,880 --> 00:10:29,279
it based on the other configuration

00:10:28,560 --> 00:10:32,240
values

00:10:29,279 --> 00:10:33,680
sent to it as part of the generated json

00:10:32,240 --> 00:10:35,600
core requests for such a configuration

00:10:33,680 --> 00:10:37,680
are so much easier to review

00:10:35,600 --> 00:10:39,519
we have also added validations for all

00:10:37,680 --> 00:10:41,360
the blocks and fields in dsl

00:10:39,519 --> 00:10:43,839
so that runs as an additional check in

00:10:41,360 --> 00:10:45,600
our protocols bills

00:10:43,839 --> 00:10:47,200
the next step was to make the setup

00:10:45,600 --> 00:10:48,880
screwdriver friendly

00:10:47,200 --> 00:10:50,720
studer provides a framework called

00:10:48,880 --> 00:10:52,640
templates that allow subject matter

00:10:50,720 --> 00:10:54,959
experts to share best practices

00:10:52,640 --> 00:10:56,800
for their platform it abstracts the

00:10:54,959 --> 00:10:58,640
build container and builds steps and

00:10:56,800 --> 00:11:02,160
makes creating a job very easy

00:10:58,640 --> 00:11:02,640
for a developer we created a screwdriver

00:11:02,160 --> 00:11:04,959
template

00:11:02,640 --> 00:11:05,680
for a canary analysis solution here is

00:11:04,959 --> 00:11:08,079
an example

00:11:05,680 --> 00:11:08,880
for what a sample entry in screwdriver

00:11:08,079 --> 00:11:10,880
configuration

00:11:08,880 --> 00:11:12,800
would look like the template has the

00:11:10,880 --> 00:11:13,279
following steps the first step is to

00:11:12,800 --> 00:11:14,880
create

00:11:13,279 --> 00:11:16,480
kind of canary and execution

00:11:14,880 --> 00:11:17,839
configurations using the supplied kind

00:11:16,480 --> 00:11:19,760
of dsl

00:11:17,839 --> 00:11:20,880
next it executes the analysis on

00:11:19,760 --> 00:11:22,800
calendar

00:11:20,880 --> 00:11:25,440
the build job is passed or failed based

00:11:22,800 --> 00:11:27,920
on the result from the canary analysis

00:11:25,440 --> 00:11:30,480
we also added a placeholder step that

00:11:27,920 --> 00:11:32,560
can be overridden to handle failures

00:11:30,480 --> 00:11:34,720
finally screwdriver integrates its slack

00:11:32,560 --> 00:11:38,480
so the application owner gets notified

00:11:34,720 --> 00:11:40,399
in slack channel of choice of the result

00:11:38,480 --> 00:11:42,399
here is the sequence diagram showing how

00:11:40,399 --> 00:11:44,480
scooter interacts with canter

00:11:42,399 --> 00:11:46,240
it blocks the can kind of deployment job

00:11:44,480 --> 00:11:48,959
so that we are not analyzing the result

00:11:46,240 --> 00:11:50,480
during a canadian deploy it then creates

00:11:48,959 --> 00:11:50,880
the canary configuration from the

00:11:50,480 --> 00:11:53,600
current

00:11:50,880 --> 00:11:54,480
esl and uploads it to kind of in the

00:11:53,600 --> 00:11:56,320
analysis step

00:11:54,480 --> 00:11:58,160
it waits for the configured analysis

00:11:56,320 --> 00:11:59,120
time to complete before requesting

00:11:58,160 --> 00:12:01,040
calendar

00:11:59,120 --> 00:12:03,200
to perform analysis retroactively on the

00:12:01,040 --> 00:12:04,720
past number of minutes configured

00:12:03,200 --> 00:12:07,920
based on the result from calendar the

00:12:04,720 --> 00:12:07,920
build is passed or failed

00:12:08,079 --> 00:12:11,120
it can be difficult to interpret the

00:12:09,440 --> 00:12:12,399
results from the canary analysis if

00:12:11,120 --> 00:12:14,720
there is no ui

00:12:12,399 --> 00:12:16,480
luckily for us the good people at nike

00:12:14,720 --> 00:12:18,160
open source a tool called referee

00:12:16,480 --> 00:12:20,399
that allows us to perform standalone

00:12:18,160 --> 00:12:22,079
actions against the kind of service

00:12:20,399 --> 00:12:23,600
it is a user interface to check the

00:12:22,079 --> 00:12:26,720
results of the binary analysis

00:12:23,600 --> 00:12:28,560
and also run on-demand canary analysis

00:12:26,720 --> 00:12:30,480
we set up a kubernetes deployment of

00:12:28,560 --> 00:12:31,600
referee to allow users to visualize the

00:12:30,480 --> 00:12:33,440
results

00:12:31,600 --> 00:12:35,680
in the slack update sent by screwdriver

00:12:33,440 --> 00:12:37,279
on job completion we include a permalink

00:12:35,680 --> 00:12:38,800
to this referee console where

00:12:37,279 --> 00:12:41,600
application owners can see the results

00:12:38,800 --> 00:12:43,279
for their execution

00:12:41,600 --> 00:12:44,800
we have also made multiple announcements

00:12:43,279 --> 00:12:45,760
to referee to satisfy some of our use

00:12:44,800 --> 00:12:47,680
cases

00:12:45,760 --> 00:12:49,760
we added a search to look up results by

00:12:47,680 --> 00:12:51,519
execution id this is very useful when

00:12:49,760 --> 00:12:52,959
trying to debug canary analysis failures

00:12:51,519 --> 00:12:55,360
retroactively

00:12:52,959 --> 00:12:56,959
we also added a list of all the credit

00:12:55,360 --> 00:12:57,760
configs created on calendar to the home

00:12:56,959 --> 00:12:59,760
page

00:12:57,760 --> 00:13:01,279
application owners can quickly click on

00:12:59,760 --> 00:13:03,600
the relevant config and load it in

00:13:01,279 --> 00:13:05,360
referee to run on-demand canon analysis

00:13:03,600 --> 00:13:07,279
no need to look for the configuration or

00:13:05,360 --> 00:13:09,200
the configuration json

00:13:07,279 --> 00:13:10,320
finally referee only support default

00:13:09,200 --> 00:13:12,160
scope

00:13:10,320 --> 00:13:14,000
we added support for multiple scopes

00:13:12,160 --> 00:13:17,040
that allows us to query multiple metric

00:13:14,000 --> 00:13:18,800
sources in the same canary analysis

00:13:17,040 --> 00:13:20,320
so in a nutshell these are the things

00:13:18,800 --> 00:13:20,880
that are required by a team at verizon

00:13:20,320 --> 00:13:23,279
media

00:13:20,880 --> 00:13:24,639
to start using canary analysis first

00:13:23,279 --> 00:13:26,240
they need to set up their canary

00:13:24,639 --> 00:13:27,760
baseline and production deployments and

00:13:26,240 --> 00:13:29,519
figure out the traffic routing

00:13:27,760 --> 00:13:31,519
they can use this to virtual service

00:13:29,519 --> 00:13:32,480
apache traffic service dns surrounding

00:13:31,519 --> 00:13:34,000
what have you

00:13:32,480 --> 00:13:35,839
next they write the configuration in

00:13:34,000 --> 00:13:36,959
kind of dsl

00:13:35,839 --> 00:13:38,880
they update their screwdriver

00:13:36,959 --> 00:13:39,920
configuration to use the query analysis

00:13:38,880 --> 00:13:42,560
template that we wrote

00:13:39,920 --> 00:13:43,279
and point it to the cantor dsl and

00:13:42,560 --> 00:13:46,240
finally

00:13:43,279 --> 00:13:47,279
use referee to analyze the results so

00:13:46,240 --> 00:13:49,279
what is coming next

00:13:47,279 --> 00:13:50,959
in this space at verizon media we lost

00:13:49,279 --> 00:13:52,480
our beta in q4 2020

00:13:50,959 --> 00:13:54,560
and our onboarding applications this

00:13:52,480 --> 00:13:56,079
quarter we hope to share results and

00:13:54,560 --> 00:13:58,800
success stories with the community

00:13:56,079 --> 00:14:00,000
once vga we plan to add a static

00:13:58,800 --> 00:14:01,279
thresholds judge to our canadian

00:14:00,000 --> 00:14:02,959
analysis solution

00:14:01,279 --> 00:14:05,279
that combined with comparing baseline

00:14:02,959 --> 00:14:06,560
and canary can be a powerful mix

00:14:05,279 --> 00:14:08,639
there have been requests to fail the

00:14:06,560 --> 00:14:10,240
canary analysis early instead of waiting

00:14:08,639 --> 00:14:12,240
for an hour or more

00:14:10,240 --> 00:14:13,680
if the canary is clearly bad and we are

00:14:12,240 --> 00:14:14,320
able to identify that within the first

00:14:13,680 --> 00:14:16,800
few minutes

00:14:14,320 --> 00:14:18,480
so we work on that next we want to

00:14:16,800 --> 00:14:20,320
introduce another internal data source

00:14:18,480 --> 00:14:23,040
to pull in revenue and business metrics

00:14:20,320 --> 00:14:24,959
like dau cdrs etc

00:14:23,040 --> 00:14:26,560
finally we plan to contribute back all

00:14:24,959 --> 00:14:29,680
the modifications we have done

00:14:26,560 --> 00:14:31,440
for calendar and referee

00:14:29,680 --> 00:14:33,040
that was all i had thank you very much

00:14:31,440 --> 00:14:33,760
for your time i hope this talk was

00:14:33,040 --> 00:14:35,519
useful

00:14:33,760 --> 00:14:37,360
and available in the spinnaker slack and

00:14:35,519 --> 00:14:45,519
on linkedin if you have any questions

00:14:37,360 --> 00:14:45,519

YouTube URL: https://www.youtube.com/watch?v=jExQ_YhqRaM


