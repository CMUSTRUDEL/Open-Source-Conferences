Title: Growing Pains: Taking off the training wheels at Salesforce - Chuck Lane & Trey Morris
Publication date: 2021-01-28
Playlist: Spinnaker Summit 2020
Description: 
	We'll discuss the obstacles and challenges we faced growing Spinnaker from a proof of concept to a production deployment platform running more than a million executions a month across thousands of accounts across multiple substrates. Incorporating Spinnaker into a secure and robust deployment pipeline requires techniques for scaling and improving performance using features and functionality included in the Spinnaker codebase, Spring Boot, and Spring Cloud Config while leveraging public cloud offerings. Additionally, Spinnaker can be extended to using Spring and in other ways that aren't always documented. We'd like to share some of our experience diving into the platform beneath Spinnaker and maybe help you avoid some of your own "Growing Pains".
Captions: 
	00:00:00,530 --> 00:00:03,629
[Music]

00:00:05,359 --> 00:00:08,559
hi

00:00:05,920 --> 00:00:09,120
and uh welcome to our spinnaker summit

00:00:08,559 --> 00:00:11,360
uh

00:00:09,120 --> 00:00:13,599
2020 presentation about all things

00:00:11,360 --> 00:00:16,800
scaling and tuning spinnaker

00:00:13,599 --> 00:00:19,279
um i'm chuck lane i'm a lead software

00:00:16,800 --> 00:00:20,480
engineer on the salesforce continuous

00:00:19,279 --> 00:00:23,119
delivery team

00:00:20,480 --> 00:00:24,000
uh we support all things spinnaker at

00:00:23,119 --> 00:00:26,800
salesforce

00:00:24,000 --> 00:00:27,920
and joining me today is a good friend of

00:00:26,800 --> 00:00:30,640
mine and a

00:00:27,920 --> 00:00:32,559
valuable member of the same team trey

00:00:30,640 --> 00:00:35,440
morris

00:00:32,559 --> 00:00:35,920
hey there a couple of years ago we

00:00:35,440 --> 00:00:37,520
started

00:00:35,920 --> 00:00:40,000
a spinnaker proof of concept for

00:00:37,520 --> 00:00:41,520
creating a system capable of managing

00:00:40,000 --> 00:00:43,680
salesforce applications

00:00:41,520 --> 00:00:45,280
and services across multiple public

00:00:43,680 --> 00:00:46,960
cloud substrates

00:00:45,280 --> 00:00:48,559
at that time we had some small goals we

00:00:46,960 --> 00:00:50,079
wanted to use helm charts and terraform

00:00:48,559 --> 00:00:52,559
for deployments

00:00:50,079 --> 00:00:55,360
to make it easy for our devs we wanted

00:00:52,559 --> 00:00:57,360
transparent account management

00:00:55,360 --> 00:00:59,280
for accountability and we wanted the

00:00:57,360 --> 00:01:02,480
process to be convenient

00:00:59,280 --> 00:01:03,039
and repeatable we had major scaling

00:01:02,480 --> 00:01:06,400
goals

00:01:03,039 --> 00:01:08,560
for the proof of concept as well our

00:01:06,400 --> 00:01:10,560
only real hurdle here

00:01:08,560 --> 00:01:12,880
in the proof of concept was the tooling

00:01:10,560 --> 00:01:15,920
to manage the number of counts

00:01:12,880 --> 00:01:17,840
luckily chuck was around

00:01:15,920 --> 00:01:19,280
so yeah i went to tackle account

00:01:17,840 --> 00:01:22,159
management and uh

00:01:19,280 --> 00:01:23,600
took a first step at that but but as we

00:01:22,159 --> 00:01:25,439
grew out the

00:01:23,600 --> 00:01:27,040
proof of concept we ran into some other

00:01:25,439 --> 00:01:28,799
scaling issues

00:01:27,040 --> 00:01:30,159
um we had all the spinnaker

00:01:28,799 --> 00:01:33,200
microservices

00:01:30,159 --> 00:01:35,280
deployed via halyard the first time and

00:01:33,200 --> 00:01:38,479
what that meant was

00:01:35,280 --> 00:01:40,640
we had a pretty fragile data layer our

00:01:38,479 --> 00:01:41,759
services were backed by redis for the

00:01:40,640 --> 00:01:44,560
most part

00:01:41,759 --> 00:01:45,040
with the exception of front 50 where we

00:01:44,560 --> 00:01:49,119
used

00:01:45,040 --> 00:01:52,079
an s3 object store we had authentication

00:01:49,119 --> 00:01:53,759
but no authorization

00:01:52,079 --> 00:01:55,600
meaning we didn't really have a good way

00:01:53,759 --> 00:01:58,479
to tell who had always done

00:01:55,600 --> 00:01:59,840
things uh the pipelines themselves were

00:01:58,479 --> 00:02:03,520
mostly made by hand

00:01:59,840 --> 00:02:06,960
and uh not necessarily codified or

00:02:03,520 --> 00:02:08,959
uh repeatable um we had a lot a lack of

00:02:06,960 --> 00:02:12,080
metrics in the system as

00:02:08,959 --> 00:02:15,440
so we didn't get a real good idea of

00:02:12,080 --> 00:02:18,480
uh success around pipeline executions or

00:02:15,440 --> 00:02:20,080
even just uh testing the uh

00:02:18,480 --> 00:02:21,599
health of the spinnaker services

00:02:20,080 --> 00:02:23,599
themselves

00:02:21,599 --> 00:02:25,920
and finally back to account management

00:02:23,599 --> 00:02:27,920
um when we wanted to add new accounts we

00:02:25,920 --> 00:02:30,160
had to restart

00:02:27,920 --> 00:02:31,120
at least the cloud driver service which

00:02:30,160 --> 00:02:36,160
led to a lot of

00:02:31,120 --> 00:02:39,680
issues around stability

00:02:36,160 --> 00:02:41,760
uh so we took the uh

00:02:39,680 --> 00:02:45,920
information from the proof of concept

00:02:41,760 --> 00:02:45,920
and we made a plan to scale up

00:02:46,080 --> 00:02:50,560
these were our main needs we had issues

00:02:48,640 --> 00:02:52,400
with traceability because we would lose

00:02:50,560 --> 00:02:53,920
pipeline execution history

00:02:52,400 --> 00:02:56,000
we didn't have any execution

00:02:53,920 --> 00:02:58,319
accountability so we wouldn't know

00:02:56,000 --> 00:02:59,840
if uh certain users or teams were

00:02:58,319 --> 00:03:00,480
running very heavy loads that were

00:02:59,840 --> 00:03:03,280
causing us

00:03:00,480 --> 00:03:04,959
trouble our microservices definitely

00:03:03,280 --> 00:03:06,640
needed some improvements and durability

00:03:04,959 --> 00:03:09,920
and availability

00:03:06,640 --> 00:03:11,280
um we needed lots of metrics we need

00:03:09,920 --> 00:03:14,560
those pretty charts

00:03:11,280 --> 00:03:16,640
we need we need all that data in an

00:03:14,560 --> 00:03:18,159
easily consumable way so we can make

00:03:16,640 --> 00:03:20,159
decisions so we can see

00:03:18,159 --> 00:03:22,159
you know how many pipelines are running

00:03:20,159 --> 00:03:23,760
this month versus last month or are we

00:03:22,159 --> 00:03:25,920
bumping up against some of our

00:03:23,760 --> 00:03:28,400
uh scaling targets um do we need to make

00:03:25,920 --> 00:03:30,400
databases larger these sorts of things

00:03:28,400 --> 00:03:32,319
and we really wanted our um our

00:03:30,400 --> 00:03:36,000
executions to run

00:03:32,319 --> 00:03:37,599
um reliably uh if it took 10 minutes

00:03:36,000 --> 00:03:39,040
last time we wanted to take about 10

00:03:37,599 --> 00:03:40,799
minutes this time we don't want it to

00:03:39,040 --> 00:03:41,519
take 10 minutes last time in two hours

00:03:40,799 --> 00:03:44,080
this time

00:03:41,519 --> 00:03:45,519
that just tends to look bad from a user

00:03:44,080 --> 00:03:47,200
perspective

00:03:45,519 --> 00:03:49,760
so we have some solutions that i'd like

00:03:47,200 --> 00:03:49,760
to dig into

00:03:50,400 --> 00:03:53,760
the first solution was really putting

00:03:52,799 --> 00:03:56,239
sql

00:03:53,760 --> 00:03:58,239
everywhere it became obvious pretty

00:03:56,239 --> 00:03:59,120
quick that redis wasn't going to be the

00:03:58,239 --> 00:04:00,720
way

00:03:59,120 --> 00:04:02,799
so we started by moving our favorite

00:04:00,720 --> 00:04:06,959
micro service cloud driver

00:04:02,799 --> 00:04:10,319
to mi sql pod running alongside

00:04:06,959 --> 00:04:12,000
spinnaker orca front 50 and echo soon

00:04:10,319 --> 00:04:14,000
followed

00:04:12,000 --> 00:04:17,040
and it became even easier once we

00:04:14,000 --> 00:04:18,720
migrated to managed sql instances

00:04:17,040 --> 00:04:21,280
we still noticed a performance benefit

00:04:18,720 --> 00:04:24,080
by using redis for caching

00:04:21,280 --> 00:04:24,960
and we needed to enable persistent

00:04:24,080 --> 00:04:28,080
volumes

00:04:24,960 --> 00:04:28,960
to make that usable as things scaled

00:04:28,080 --> 00:04:31,840
further

00:04:28,960 --> 00:04:32,880
we started to tune the sql usage from

00:04:31,840 --> 00:04:34,800
the client side

00:04:32,880 --> 00:04:36,960
with the hikari connection pool settings

00:04:34,800 --> 00:04:40,000
and service timeouts

00:04:36,960 --> 00:04:41,759
and um we're still tweaking those

00:04:40,000 --> 00:04:43,440
values a little bit we're playing around

00:04:41,759 --> 00:04:46,639
with different sizes of pools

00:04:43,440 --> 00:04:48,720
different numbers of micro services

00:04:46,639 --> 00:04:49,840
and sort of you know playing around with

00:04:48,720 --> 00:04:53,040
you know do you want

00:04:49,840 --> 00:04:54,880
uh do you want five cloud drivers each

00:04:53,040 --> 00:04:56,000
with 20 connections do you want two

00:04:54,880 --> 00:04:58,000
cloud drivers with 100

00:04:56,000 --> 00:05:00,000
connections what's the best way to go

00:04:58,000 --> 00:05:02,000
here what we found is that

00:05:00,000 --> 00:05:03,520
uh sometimes as load changes day-to-day

00:05:02,000 --> 00:05:04,560
certain values make more sense than

00:05:03,520 --> 00:05:06,000
others and so

00:05:04,560 --> 00:05:08,840
we're really just playing around with

00:05:06,000 --> 00:05:11,840
the connection pool settings at this

00:05:08,840 --> 00:05:11,840
time

00:05:12,639 --> 00:05:16,639
so next we're going to come to the

00:05:13,840 --> 00:05:19,039
authorization layer

00:05:16,639 --> 00:05:21,360
basically uh for any of you that have

00:05:19,039 --> 00:05:23,759
have gone and implemented

00:05:21,360 --> 00:05:25,199
fiat authorization what you're

00:05:23,759 --> 00:05:28,160
definitely going to need is

00:05:25,199 --> 00:05:31,280
a source of truth for the role names as

00:05:28,160 --> 00:05:34,080
well as who the individual users are

00:05:31,280 --> 00:05:35,759
in our case we do this via ldap but you

00:05:34,080 --> 00:05:39,280
could also use saml groups

00:05:35,759 --> 00:05:42,240
or github teams next

00:05:39,280 --> 00:05:43,360
is really figuring out where you're

00:05:42,240 --> 00:05:48,160
going to be able

00:05:43,360 --> 00:05:51,520
to set the gates for authorization

00:05:48,160 --> 00:05:53,840
those that's basically controlled at

00:05:51,520 --> 00:05:56,479
the application and the account level

00:05:53,840 --> 00:06:00,000
right now in spinnaker

00:05:56,479 --> 00:06:02,880
you have your create read and write

00:06:00,000 --> 00:06:03,759
permissions that are are pretty much

00:06:02,880 --> 00:06:05,759
everywhere

00:06:03,759 --> 00:06:06,960
and then recently they added an execute

00:06:05,759 --> 00:06:09,280
permission

00:06:06,960 --> 00:06:10,400
that also allows you to gate the ability

00:06:09,280 --> 00:06:12,800
to

00:06:10,400 --> 00:06:15,600
execute specific stages for some cloud

00:06:12,800 --> 00:06:15,600
driver types

00:06:16,639 --> 00:06:21,280
they recently also added the ability to

00:06:19,360 --> 00:06:23,520
use something called composite types

00:06:21,280 --> 00:06:26,720
when it comes to

00:06:23,520 --> 00:06:29,120
the application permissions themselves

00:06:26,720 --> 00:06:30,400
and what this allows you to do is

00:06:29,120 --> 00:06:32,720
basically prevent

00:06:30,400 --> 00:06:34,000
users from creating their own

00:06:32,720 --> 00:06:37,280
applications unless

00:06:34,000 --> 00:06:40,240
they live in uh one of

00:06:37,280 --> 00:06:41,440
a certain set of roles that you want to

00:06:40,240 --> 00:06:44,960
give the ability to

00:06:41,440 --> 00:06:47,280
to do those things as you go

00:06:44,960 --> 00:06:48,400
and you create different entities inside

00:06:47,280 --> 00:06:50,240
of the system

00:06:48,400 --> 00:06:52,639
we found that there were some issues

00:06:50,240 --> 00:06:52,639
around

00:06:52,800 --> 00:06:59,360
reliably getting the permissions cash to

00:06:57,039 --> 00:07:00,880
have appropriate roles assigned to it

00:06:59,360 --> 00:07:02,479
for new entities

00:07:00,880 --> 00:07:04,880
so for example you would create an

00:07:02,479 --> 00:07:06,560
application and then

00:07:04,880 --> 00:07:08,720
immediately it would say that the user

00:07:06,560 --> 00:07:11,520
didn't have read application to the

00:07:08,720 --> 00:07:12,160
to the uh application that you had just

00:07:11,520 --> 00:07:15,840
created

00:07:12,160 --> 00:07:18,560
so to get around that

00:07:15,840 --> 00:07:18,960
in some of our automations we built uh

00:07:18,560 --> 00:07:21,280
we

00:07:18,960 --> 00:07:23,120
we built a call that would force trigger

00:07:21,280 --> 00:07:26,800
uh the roles to sync

00:07:23,120 --> 00:07:28,800
inside of fiat um in addition to that

00:07:26,800 --> 00:07:31,680
there are a couple settings

00:07:28,800 --> 00:07:34,160
uh allow access to unknown applications

00:07:31,680 --> 00:07:37,280
and restrict application creation

00:07:34,160 --> 00:07:38,560
um those are spring profile settings

00:07:37,280 --> 00:07:40,960
that we had to

00:07:38,560 --> 00:07:41,759
configure appropriately to get just the

00:07:40,960 --> 00:07:45,599
right mix

00:07:41,759 --> 00:07:46,720
of uh of visibility while while

00:07:45,599 --> 00:07:50,000
retaining the

00:07:46,720 --> 00:07:52,639
authorization goals that we had um

00:07:50,000 --> 00:07:54,160
in addition to that as we scaled uh the

00:07:52,639 --> 00:07:56,560
services up

00:07:54,160 --> 00:07:58,000
fiat can be particularly noisy just

00:07:56,560 --> 00:08:00,879
about anything you go to do

00:07:58,000 --> 00:08:02,000
inside of the ui um it's going to go

00:08:00,879 --> 00:08:05,120
ahead and request

00:08:02,000 --> 00:08:06,080
uh make a call to fiat to verify that

00:08:05,120 --> 00:08:08,560
you have

00:08:06,080 --> 00:08:10,080
uh authorization to perform the action

00:08:08,560 --> 00:08:13,360
that you're trying to take

00:08:10,080 --> 00:08:15,520
uh as a result like i said these

00:08:13,360 --> 00:08:18,400
services can get very chatty

00:08:15,520 --> 00:08:20,080
and so as we started to see timeouts

00:08:18,400 --> 00:08:22,560
when we scaled things up

00:08:20,080 --> 00:08:23,840
um we had to go into the individual

00:08:22,560 --> 00:08:26,879
services

00:08:23,840 --> 00:08:30,560
and uh and adjust the

00:08:26,879 --> 00:08:31,360
uh the fiat services um spring profile

00:08:30,560 --> 00:08:34,320
settings

00:08:31,360 --> 00:08:35,360
for each microservice to uh allow the

00:08:34,320 --> 00:08:38,640
appropriate time

00:08:35,360 --> 00:08:41,839
for the connection and uh

00:08:38,640 --> 00:08:42,479
the communication to take place uh both

00:08:41,839 --> 00:08:45,920
between

00:08:42,479 --> 00:08:47,440
inside the application and with our ldap

00:08:45,920 --> 00:08:49,839
server where we would actually read the

00:08:47,440 --> 00:08:49,839
roles

00:08:49,920 --> 00:08:54,399
we leveraged the power of echo to give

00:08:52,160 --> 00:08:55,519
us insight into pipeline and task

00:08:54,399 --> 00:08:58,080
performance as well as

00:08:55,519 --> 00:09:00,080
other metrics as i mentioned before we

00:08:58,080 --> 00:09:03,040
needed the ability to see

00:09:00,080 --> 00:09:03,920
what we're doing we didn't have a whole

00:09:03,040 --> 00:09:06,640
lot of

00:09:03,920 --> 00:09:07,760
an idea what users were running what how

00:09:06,640 --> 00:09:09,920
often

00:09:07,760 --> 00:09:11,519
when they were running things or any of

00:09:09,920 --> 00:09:14,320
that until we started

00:09:11,519 --> 00:09:14,320
using echo

00:09:15,440 --> 00:09:18,480
we also added some user experience

00:09:17,120 --> 00:09:21,680
tweaks in there too

00:09:18,480 --> 00:09:23,839
users can now track executions

00:09:21,680 --> 00:09:25,200
with slack notifications and things like

00:09:23,839 --> 00:09:28,959
that

00:09:25,200 --> 00:09:28,959
and we have some triggers for

00:09:30,880 --> 00:09:38,480
pipelines in pub sub and you can also

00:09:33,120 --> 00:09:41,600
use pub sub as a message bus

00:09:38,480 --> 00:09:44,080
uh the echo events gave us a good deal

00:09:41,600 --> 00:09:46,480
of information about pipeline executions

00:09:44,080 --> 00:09:48,480
but in addition to that we also needed

00:09:46,480 --> 00:09:50,800
information about the spinnaker service

00:09:48,480 --> 00:09:53,519
health in general

00:09:50,800 --> 00:09:55,680
to get this we use a combination of

00:09:53,519 --> 00:09:58,399
health checks and liveness probes

00:09:55,680 --> 00:09:59,600
and kubernetes for the services as well

00:09:58,399 --> 00:10:01,920
as

00:09:59,600 --> 00:10:04,320
the monitoring demon that is provided by

00:10:01,920 --> 00:10:05,279
spinnaker out of the box uh with

00:10:04,320 --> 00:10:09,440
prometheus

00:10:05,279 --> 00:10:12,800
specifically as a backend datastore um

00:10:09,440 --> 00:10:15,440
the nice part about that is it comes

00:10:12,800 --> 00:10:18,480
with a set of grafana dashboards uh

00:10:15,440 --> 00:10:20,000
out of the out of the box uh now your

00:10:18,480 --> 00:10:21,360
mileage may vary there there were some

00:10:20,000 --> 00:10:24,480
things that we needed to tweak

00:10:21,360 --> 00:10:27,040
to uh get them to display

00:10:24,480 --> 00:10:29,200
exactly what we needed but uh once we

00:10:27,040 --> 00:10:31,600
got those configured appropriately

00:10:29,200 --> 00:10:33,760
uh it's provided us valuable insight as

00:10:31,600 --> 00:10:36,160
far as

00:10:33,760 --> 00:10:37,040
what the the health of the services are

00:10:36,160 --> 00:10:39,839
themselves and

00:10:37,040 --> 00:10:41,040
give us information about uh what the

00:10:39,839 --> 00:10:45,680
work cue size

00:10:41,040 --> 00:10:47,680
is for orca as an example or

00:10:45,680 --> 00:10:49,760
where where we're experiencing errors

00:10:47,680 --> 00:10:51,279
within the system as the microservices

00:10:49,760 --> 00:10:53,680
talk to one another

00:10:51,279 --> 00:10:55,519
um you know in addition the cloud driver

00:10:53,680 --> 00:10:56,240
metrics can give you good information

00:10:55,519 --> 00:10:59,279
about

00:10:56,240 --> 00:11:02,399
uh the latency uh or or the amount of

00:10:59,279 --> 00:11:05,440
time that calls to your cloud provider

00:11:02,399 --> 00:11:08,640
aws or gcp are taking

00:11:05,440 --> 00:11:10,160
um and and really just uh gives you a

00:11:08,640 --> 00:11:11,600
wealth of information about exactly

00:11:10,160 --> 00:11:13,600
what's going on

00:11:11,600 --> 00:11:15,200
under the covers on in your spinnaker

00:11:13,600 --> 00:11:16,399
services so

00:11:15,200 --> 00:11:19,040
it's valuable and it's definitely

00:11:16,399 --> 00:11:21,440
something we would recommend one of the

00:11:19,040 --> 00:11:22,560
low-hanging fruits that you can easily

00:11:21,440 --> 00:11:25,440
get to

00:11:22,560 --> 00:11:26,000
is by enabling redis persistence we had

00:11:25,440 --> 00:11:29,040
quite a few

00:11:26,000 --> 00:11:31,279
issues as you can see a gate session

00:11:29,040 --> 00:11:33,519
loss was a pretty big one and losing uh

00:11:31,279 --> 00:11:34,480
execution history was pretty rough as

00:11:33,519 --> 00:11:37,680
well

00:11:34,480 --> 00:11:38,320
um and adding uh persistent volume to

00:11:37,680 --> 00:11:40,640
redis

00:11:38,320 --> 00:11:42,160
pretty well fixed all of them it took us

00:11:40,640 --> 00:11:44,000
from losing that pod

00:11:42,160 --> 00:11:45,279
being a blip or being an outage to a

00:11:44,000 --> 00:11:48,800
blip

00:11:45,279 --> 00:11:50,160
um and the essentially kubernetes would

00:11:48,800 --> 00:11:51,839
recover

00:11:50,160 --> 00:11:53,680
in a way that it couldn't do when you

00:11:51,839 --> 00:11:54,959
lost all of the redis data

00:11:53,680 --> 00:11:56,959
we also tweaked the redis connection

00:11:54,959 --> 00:11:58,480
pools over time kind of like we did with

00:11:56,959 --> 00:12:00,240
sql

00:11:58,480 --> 00:12:01,760
we're still playing around with what

00:12:00,240 --> 00:12:06,079
size of pools make the most

00:12:01,760 --> 00:12:08,399
sense based on our redis our redis usage

00:12:06,079 --> 00:12:09,760
right now we're using redis and a pod

00:12:08,399 --> 00:12:12,240
but we'd like to move

00:12:09,760 --> 00:12:14,160
into uh managed redis instance at some

00:12:12,240 --> 00:12:16,079
point in the future

00:12:14,160 --> 00:12:17,360
um so now i'm going to talk a little bit

00:12:16,079 --> 00:12:19,760
more about some of the

00:12:17,360 --> 00:12:21,360
hidden knobs that are present that we

00:12:19,760 --> 00:12:23,120
use to

00:12:21,360 --> 00:12:25,519
configure our service and get the the

00:12:23,120 --> 00:12:28,800
best level of detail out

00:12:25,519 --> 00:12:30,320
first of all for each of the services

00:12:28,800 --> 00:12:32,480
that are that are spring boot based

00:12:30,320 --> 00:12:35,200
which is everything but deck

00:12:32,480 --> 00:12:37,040
you have the ability to set a logging

00:12:35,200 --> 00:12:40,399
level for individual classes

00:12:37,040 --> 00:12:41,600
inside the service what this means is

00:12:40,399 --> 00:12:44,320
that

00:12:41,600 --> 00:12:45,200
you can set the logging level to debug

00:12:44,320 --> 00:12:47,600
for

00:12:45,200 --> 00:12:49,360
uh any of the spinnaker classes so that

00:12:47,600 --> 00:12:52,639
would be like a

00:12:49,360 --> 00:12:56,560
you know either the cloud driver uh

00:12:52,639 --> 00:12:59,600
orca or or even some of the underlying

00:12:56,560 --> 00:13:00,000
libraries like caico and cork and you

00:12:59,600 --> 00:13:02,000
can

00:13:00,000 --> 00:13:03,839
you can set the log level for those to

00:13:02,000 --> 00:13:04,399
debug which sometimes will help you

00:13:03,839 --> 00:13:07,519
gather

00:13:04,399 --> 00:13:08,720
extra information to troubleshoot issues

00:13:07,519 --> 00:13:11,440
around

00:13:08,720 --> 00:13:13,839
integrations or or other uh hard to

00:13:11,440 --> 00:13:16,079
track down problems

00:13:13,839 --> 00:13:17,040
in addition to that there's a set of

00:13:16,079 --> 00:13:19,120
spring framework

00:13:17,040 --> 00:13:21,440
classes that you can enable debugging

00:13:19,120 --> 00:13:24,000
for um you do that in the same way

00:13:21,440 --> 00:13:24,639
uh you know you set your logging level

00:13:24,000 --> 00:13:27,200
um

00:13:24,639 --> 00:13:29,839
and then the name of the class classes

00:13:27,200 --> 00:13:31,200
so class so like or dot spring framework

00:13:29,839 --> 00:13:34,639
dot security

00:13:31,200 --> 00:13:36,480
uh to debug and then you can get uh

00:13:34,639 --> 00:13:38,000
valuable debug information about the

00:13:36,480 --> 00:13:41,360
underlying spring boot

00:13:38,000 --> 00:13:43,199
uh platform that uh that the service is

00:13:41,360 --> 00:13:45,519
running on this is good for

00:13:43,199 --> 00:13:47,440
helping to track down issues around uh

00:13:45,519 --> 00:13:50,720
authentication and authorization

00:13:47,440 --> 00:13:52,320
other parts of uh other parts of the

00:13:50,720 --> 00:13:54,160
service that aren't necessarily

00:13:52,320 --> 00:13:55,360
uh part of the spinnaker code but

00:13:54,160 --> 00:13:57,120
instead

00:13:55,360 --> 00:13:59,360
where spinnaker may farm it out to a

00:13:57,120 --> 00:14:00,480
library to handle the the heavy lifting

00:13:59,360 --> 00:14:02,959
there

00:14:00,480 --> 00:14:04,480
and finally you can set uh just a root

00:14:02,959 --> 00:14:06,800
level of debug

00:14:04,480 --> 00:14:08,399
um which could be useful for getting

00:14:06,800 --> 00:14:11,680
information about

00:14:08,399 --> 00:14:14,000
uh the uh the tomcat

00:14:11,680 --> 00:14:17,120
service that that actually fields the

00:14:14,000 --> 00:14:20,399
http and https requests

00:14:17,120 --> 00:14:21,680
underneath the service and information

00:14:20,399 --> 00:14:23,680
about the headers and things that are

00:14:21,680 --> 00:14:25,519
passed back and forth

00:14:23,680 --> 00:14:28,079
in addition it's sometimes valuable to

00:14:25,519 --> 00:14:30,880
be able to turn those off as well

00:14:28,079 --> 00:14:32,240
you know you may run into a particularly

00:14:30,880 --> 00:14:34,399
chatty class

00:14:32,240 --> 00:14:36,240
uh that just gives you too much

00:14:34,399 --> 00:14:38,399
information and info mode

00:14:36,240 --> 00:14:39,440
uh and so sometimes it's useful to be

00:14:38,399 --> 00:14:42,720
able to

00:14:39,440 --> 00:14:45,279
uh pick a particular class and say you

00:14:42,720 --> 00:14:47,920
only want to see error log messages from

00:14:45,279 --> 00:14:47,920
that class

00:14:48,079 --> 00:14:55,199
one other uh

00:14:51,519 --> 00:14:58,560
lesser known feature is around the

00:14:55,199 --> 00:15:00,560
spring boot health checks um so in a

00:14:58,560 --> 00:15:01,600
uh out of the box the way that the

00:15:00,560 --> 00:15:05,680
health checks work

00:15:01,600 --> 00:15:08,720
is it'll simply give you a return uh

00:15:05,680 --> 00:15:11,519
return message that says the status is

00:15:08,720 --> 00:15:12,800
up or down in practice what we found is

00:15:11,519 --> 00:15:15,199
that this wasn't

00:15:12,800 --> 00:15:16,079
enough information in a lot of cases to

00:15:15,199 --> 00:15:18,639
determine

00:15:16,079 --> 00:15:19,360
what was causing the service to go down

00:15:18,639 --> 00:15:22,880
um

00:15:19,360 --> 00:15:27,279
so there's a uh management

00:15:22,880 --> 00:15:29,519
uh endpoint uh health show details

00:15:27,279 --> 00:15:31,199
uh always option that you can set in

00:15:29,519 --> 00:15:34,720
your spring profile

00:15:31,199 --> 00:15:36,160
and once you have um

00:15:34,720 --> 00:15:38,959
have configured that in your spring

00:15:36,160 --> 00:15:39,759
profile then when you make a call into

00:15:38,959 --> 00:15:42,320
the health check

00:15:39,759 --> 00:15:44,639
endpoint it will break the health check

00:15:42,320 --> 00:15:47,040
down into the various components

00:15:44,639 --> 00:15:49,120
and you can see which part of the health

00:15:47,040 --> 00:15:50,320
check is succeeding and what part is

00:15:49,120 --> 00:15:52,560
failing

00:15:50,320 --> 00:15:54,320
this is this is good if you have a

00:15:52,560 --> 00:15:55,519
service especially that's flapping or

00:15:54,320 --> 00:15:58,079
something like that

00:15:55,519 --> 00:15:58,639
and you want to determine exactly uh

00:15:58,079 --> 00:16:01,199
what the

00:15:58,639 --> 00:16:02,959
problem is that's causing it to flap uh

00:16:01,199 --> 00:16:04,240
sometimes this can help you capture

00:16:02,959 --> 00:16:06,079
those

00:16:04,240 --> 00:16:07,600
kind of lightning in a bottle failures

00:16:06,079 --> 00:16:11,199
and and pinpoint

00:16:07,600 --> 00:16:13,519
the uh root cause of the problem

00:16:11,199 --> 00:16:13,519
um

00:16:15,519 --> 00:16:19,920
in addition to that there there is a set

00:16:18,160 --> 00:16:23,440
of spring management endpoints

00:16:19,920 --> 00:16:24,560
uh that it's also been valuable for us

00:16:23,440 --> 00:16:28,000
to look at

00:16:24,560 --> 00:16:32,000
um basically once you have

00:16:28,000 --> 00:16:35,759
used the uh statement here

00:16:32,000 --> 00:16:37,600
in your spring profile to expose access

00:16:35,759 --> 00:16:39,360
to these web points

00:16:37,600 --> 00:16:41,120
then there's a number of endpoints that

00:16:39,360 --> 00:16:42,160
you can call inside of the spring

00:16:41,120 --> 00:16:46,320
framework

00:16:42,160 --> 00:16:48,399
to uh to get a good idea of exactly

00:16:46,320 --> 00:16:51,120
what the more what the underlying layer

00:16:48,399 --> 00:16:53,120
that's running underneath spinnaker

00:16:51,120 --> 00:16:55,440
looks like and this has helped us

00:16:53,120 --> 00:16:57,440
troubleshoot all kinds of issues

00:16:55,440 --> 00:16:59,839
as we've learned that sometimes it's

00:16:57,440 --> 00:17:02,880
it's better to look at

00:16:59,839 --> 00:17:05,120
spinnaker as a set of spring boot

00:17:02,880 --> 00:17:07,360
applications instead of

00:17:05,120 --> 00:17:08,880
uh you know just trying to look into the

00:17:07,360 --> 00:17:10,640
spinnaker code for

00:17:08,880 --> 00:17:12,400
all the things to solve the problems

00:17:10,640 --> 00:17:14,160
that you run into

00:17:12,400 --> 00:17:17,120
so a few of the endpoints that we found

00:17:14,160 --> 00:17:19,039
to be very useful

00:17:17,120 --> 00:17:20,480
both in in troubleshooting as well as

00:17:19,039 --> 00:17:24,160
development

00:17:20,480 --> 00:17:27,360
are the slash beans endpoint that will

00:17:24,160 --> 00:17:27,919
help explain uh exactly what beans are

00:17:27,360 --> 00:17:30,320
loaded

00:17:27,919 --> 00:17:33,120
by a particular service this can help

00:17:30,320 --> 00:17:36,400
you understand uh if you don't have

00:17:33,120 --> 00:17:37,200
a particular module or part of the

00:17:36,400 --> 00:17:40,640
integration

00:17:37,200 --> 00:17:43,440
in a service that isn't isn't turning on

00:17:40,640 --> 00:17:44,640
uh the the beans endpoint will explain

00:17:43,440 --> 00:17:47,760
to you why

00:17:44,640 --> 00:17:49,360
a certain certain parts of the code

00:17:47,760 --> 00:17:51,280
certain components got loaded and other

00:17:49,360 --> 00:17:53,919
ones didn't

00:17:51,280 --> 00:17:55,520
the config props endpoint displays all

00:17:53,919 --> 00:17:59,120
configuration properties

00:17:55,520 --> 00:18:00,080
uh for the service as well as slash env

00:17:59,120 --> 00:18:02,559
which

00:18:00,080 --> 00:18:03,600
displays the properties particularly

00:18:02,559 --> 00:18:05,760
from

00:18:03,600 --> 00:18:08,080
the spring configurable environment

00:18:05,760 --> 00:18:09,850
itself and i'll talk more about those on

00:18:08,080 --> 00:18:11,679
the next page

00:18:09,850 --> 00:18:13,280
[Music]

00:18:11,679 --> 00:18:15,280
kind of how those relate to the

00:18:13,280 --> 00:18:17,919
configuration of the service

00:18:15,280 --> 00:18:19,200
and there's also a metrics in point that

00:18:17,919 --> 00:18:22,240
will help you get

00:18:19,200 --> 00:18:23,840
some metrics out of the application uh

00:18:22,240 --> 00:18:27,200
particularly from the spring group

00:18:23,840 --> 00:18:29,280
side of things it's not

00:18:27,200 --> 00:18:30,400
that they don't 100 match up with what

00:18:29,280 --> 00:18:33,919
comes out

00:18:30,400 --> 00:18:36,640
of the spectator metrics endpoint

00:18:33,919 --> 00:18:38,960
that feeds into the modern daemon um but

00:18:36,640 --> 00:18:42,000
sometimes they can be useful for

00:18:38,960 --> 00:18:43,280
identifying bottlenecks and other things

00:18:42,000 --> 00:18:45,840
and there's a whole set of spring

00:18:43,280 --> 00:18:47,760
management endpoints uh besides these uh

00:18:45,840 --> 00:18:50,640
these are just some of the key ones

00:18:47,760 --> 00:18:52,559
that i that i called out but uh but yeah

00:18:50,640 --> 00:18:54,559
take some time to look at those

00:18:52,559 --> 00:18:57,200
and uh see if there are other ones that

00:18:54,559 --> 00:18:59,440
you can use to

00:18:57,200 --> 00:19:01,919
get a better look if you're running into

00:18:59,440 --> 00:19:01,919
issues

00:19:04,240 --> 00:19:09,120
a big part of scaling and configuring

00:19:07,200 --> 00:19:10,160
the system correctly is determining what

00:19:09,120 --> 00:19:14,960
can be configured

00:19:10,160 --> 00:19:18,320
and extended inside the spinnaker code

00:19:14,960 --> 00:19:21,440
this takes the form of

00:19:18,320 --> 00:19:24,960
a lot of configuration classes config

00:19:21,440 --> 00:19:26,640
and configuration properties classes

00:19:24,960 --> 00:19:28,240
um essentially as you go through a

00:19:26,640 --> 00:19:30,880
module in the code

00:19:28,240 --> 00:19:32,960
uh you'll in a lot of cases you'll come

00:19:30,880 --> 00:19:35,440
across a config folder

00:19:32,960 --> 00:19:36,400
and in this config folder there will be

00:19:35,440 --> 00:19:39,120
a

00:19:36,400 --> 00:19:39,440
class that ends in either configuration

00:19:39,120 --> 00:19:43,760
or

00:19:39,440 --> 00:19:46,960
properties or both um and in

00:19:43,760 --> 00:19:49,440
in these classes uh they'll usually be

00:19:46,960 --> 00:19:52,640
annotated with a configuration

00:19:49,440 --> 00:19:54,400
uh attribute and basically what that

00:19:52,640 --> 00:19:57,200
means is that

00:19:54,400 --> 00:19:57,679
this is a class that is bound at run

00:19:57,200 --> 00:20:00,720
time

00:19:57,679 --> 00:20:05,280
to the spring profile to define

00:20:00,720 --> 00:20:07,360
the individual properties for this class

00:20:05,280 --> 00:20:08,880
back to the slide that i was talking

00:20:07,360 --> 00:20:12,720
about before this

00:20:08,880 --> 00:20:13,919
on config props um so as you go in and

00:20:12,720 --> 00:20:16,559
you define in your spring

00:20:13,919 --> 00:20:18,960
profile all these properties then you

00:20:16,559 --> 00:20:22,000
can hit the config props endpoint

00:20:18,960 --> 00:20:23,039
and get a holistic view of exactly what

00:20:22,000 --> 00:20:25,840
it is

00:20:23,039 --> 00:20:29,039
uh what what properties are configured

00:20:25,840 --> 00:20:31,280
you know in your service

00:20:29,039 --> 00:20:34,159
let's see in addition to uh the

00:20:31,280 --> 00:20:36,880
configuration classes themselves

00:20:34,159 --> 00:20:38,720
a lot of times you'll see where the

00:20:36,880 --> 00:20:40,159
properties have been moved over into a

00:20:38,720 --> 00:20:42,159
properties class

00:20:40,159 --> 00:20:43,919
and they're referenced via this enable

00:20:42,159 --> 00:20:48,480
configuration properties

00:20:43,919 --> 00:20:51,760
attribute and so for example here for

00:20:48,480 --> 00:20:52,480
the slack config properties um this data

00:20:51,760 --> 00:20:55,280
means that

00:20:52,480 --> 00:20:57,440
the getters and setters are are

00:20:55,280 --> 00:21:00,000
automatically generated for you

00:20:57,440 --> 00:21:01,520
um and then the spring profile all you

00:21:00,000 --> 00:21:06,880
need to do is just say

00:21:01,520 --> 00:21:06,880
you know slack.token or slack.baseurl

00:21:06,960 --> 00:21:15,440
to go ahead and assign values to these

00:21:10,240 --> 00:21:15,440
at runtime through your spring profile

00:21:15,679 --> 00:21:19,600
in addition to the configuration classes

00:21:18,480 --> 00:21:21,360
the older

00:21:19,600 --> 00:21:23,840
the other way to do this that's a little

00:21:21,360 --> 00:21:25,039
bit older but it's still present in some

00:21:23,840 --> 00:21:28,159
places in the code

00:21:25,039 --> 00:21:31,280
as through the use of value statements

00:21:28,159 --> 00:21:34,400
so in this case you'll see uh

00:21:31,280 --> 00:21:37,679
configuration is still used but

00:21:34,400 --> 00:21:38,799
um down to set a value for a particular

00:21:37,679 --> 00:21:40,960
property

00:21:38,799 --> 00:21:42,080
you may see value referenced like this

00:21:40,960 --> 00:21:45,720
and so

00:21:42,080 --> 00:21:47,280
um you know in this case uh the

00:21:45,720 --> 00:21:49,360
pagerduty.tokenproperty

00:21:47,280 --> 00:21:50,320
in your spring profile would be mapped

00:21:49,360 --> 00:21:54,240
to

00:21:50,320 --> 00:21:57,280
uh this particular property for

00:21:54,240 --> 00:22:01,039
uh to configure the page of duty class

00:21:57,280 --> 00:22:05,760
so um so yeah going through

00:22:01,039 --> 00:22:07,919
um both the spinnaker code as well as uh

00:22:05,760 --> 00:22:09,200
sometimes it's useful to look at the the

00:22:07,919 --> 00:22:12,320
spring

00:22:09,200 --> 00:22:14,240
boot classes that are used as well

00:22:12,320 --> 00:22:16,159
like in a stack trace you may be able to

00:22:14,240 --> 00:22:18,240
figure out a way to

00:22:16,159 --> 00:22:19,679
set a property and resolve your error

00:22:18,240 --> 00:22:23,760
without having to

00:22:19,679 --> 00:22:25,520
make any changes to the actual code so

00:22:23,760 --> 00:22:27,360
as you can see today we're a little

00:22:25,520 --> 00:22:29,679
further along we've managed to grow our

00:22:27,360 --> 00:22:31,840
little proof of concept instance

00:22:29,679 --> 00:22:34,559
as well as build other instances to keep

00:22:31,840 --> 00:22:37,440
up with salesforce demands

00:22:34,559 --> 00:22:37,440
so what's next

00:22:38,400 --> 00:22:41,440
as we catch up to our requirements and

00:22:40,320 --> 00:22:43,360
growth targets

00:22:41,440 --> 00:22:44,480
we'd like to improve the user experience

00:22:43,360 --> 00:22:47,039
in general

00:22:44,480 --> 00:22:47,520
maybe introduce some custom skinning do

00:22:47,039 --> 00:22:50,559
some

00:22:47,520 --> 00:22:52,320
upstream improvements we want to improve

00:22:50,559 --> 00:22:52,840
fault tolerance in general by making

00:22:52,320 --> 00:22:55,760
sure

00:22:52,840 --> 00:22:56,799
errors in integrated systems surface

00:22:55,760 --> 00:22:59,520
better

00:22:56,799 --> 00:22:59,840
pipeline failed for unspecified reasons

00:22:59,520 --> 00:23:01,600
is

00:22:59,840 --> 00:23:04,720
not a super helpful error message and

00:23:01,600 --> 00:23:04,720
we'd like to improve on that

00:23:04,799 --> 00:23:11,360
we run into transient errors as well and

00:23:08,159 --> 00:23:12,960
users tend to rerun pipelines to fix

00:23:11,360 --> 00:23:15,440
them

00:23:12,960 --> 00:23:17,679
and we'd like to see if we can make make

00:23:15,440 --> 00:23:20,480
that better

00:23:17,679 --> 00:23:23,840
we're also looking into active active

00:23:20,480 --> 00:23:23,840
high availability instances

00:23:24,000 --> 00:23:27,280
but we've got some concerns there we've

00:23:25,360 --> 00:23:30,240
still got some some issues

00:23:27,280 --> 00:23:30,799
to figure out all right thanks for

00:23:30,240 --> 00:23:33,120
watching

00:23:30,799 --> 00:23:37,840
and stay safe out there thank you very

00:23:33,120 --> 00:23:37,840

YouTube URL: https://www.youtube.com/watch?v=4CG9qnm9ohQ


