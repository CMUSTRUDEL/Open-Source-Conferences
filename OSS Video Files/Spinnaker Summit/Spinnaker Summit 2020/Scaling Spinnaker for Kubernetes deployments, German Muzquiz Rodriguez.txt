Title: Scaling Spinnaker for Kubernetes deployments, German Muzquiz Rodriguez
Publication date: 2021-01-12
Playlist: Spinnaker Summit 2020
Description: 
	German talks about some of the performance problems that Spinnaker faces when used to make deployments to Kubernetes. He will give a brief overview of how the cache is used internally, what actions can be taken to diagnose a performance degradation problem like the infamous "Force Cache Refresh" task, and what are some of the configuration settings that help alleviate these issues.
Captions: 
	00:00:00,530 --> 00:00:03,629
[Music]

00:00:05,200 --> 00:00:09,840
hello everyone

00:00:06,640 --> 00:00:12,320
my name is hermann muskis and i'm going

00:00:09,840 --> 00:00:14,920
i'm going to give you some tips for how

00:00:12,320 --> 00:00:17,920
to scale spinnaker for kubernetes

00:00:14,920 --> 00:00:17,920
deployments

00:00:18,240 --> 00:00:23,199
first of all when you start adding more

00:00:20,800 --> 00:00:25,039
kubernetes accounts to spinnaker

00:00:23,199 --> 00:00:28,000
you may face some performance

00:00:25,039 --> 00:00:30,720
degradation during deployments

00:00:28,000 --> 00:00:33,600
if you add a lot of accounts you may

00:00:30,720 --> 00:00:36,559
even see this kind of behavior

00:00:33,600 --> 00:00:38,559
where force cash refresh takes around 12

00:00:36,559 --> 00:00:41,680
minutes to complete

00:00:38,559 --> 00:00:44,239
and if you click on the task status

00:00:41,680 --> 00:00:45,440
of the pipeline execution you can see

00:00:44,239 --> 00:00:48,079
that this task

00:00:45,440 --> 00:00:51,760
is pretty much taking almost all of the

00:00:48,079 --> 00:00:54,320
time of the execution for this stage

00:00:51,760 --> 00:00:55,280
if you are having this specific problem

00:00:54,320 --> 00:00:58,320
you can go to

00:00:55,280 --> 00:00:59,280
oracle logs and also see a message like

00:00:58,320 --> 00:01:01,440
the below

00:00:59,280 --> 00:01:03,840
the indicating that the force cast

00:01:01,440 --> 00:01:06,640
refresh never finished

00:01:03,840 --> 00:01:08,880
and basically orca gives up and

00:01:06,640 --> 00:01:13,280
continues with the rest of the execution

00:01:08,880 --> 00:01:15,360
for this stage and you may wonder

00:01:13,280 --> 00:01:16,640
what's happening in this forest cast

00:01:15,360 --> 00:01:19,600
refresh task

00:01:16,640 --> 00:01:22,799
why is taking so long and what can i do

00:01:19,600 --> 00:01:22,799
to avoid this behavior

00:01:23,439 --> 00:01:26,960
well first we need to understand a bit

00:01:26,240 --> 00:01:30,000
how

00:01:26,960 --> 00:01:32,320
is spinnaker working internally

00:01:30,000 --> 00:01:35,600
this is a first part of the sequence of

00:01:32,320 --> 00:01:38,640
the force cast refresh task

00:01:35,600 --> 00:01:41,920
and we can see that it begins with orca

00:01:38,640 --> 00:01:44,079
sending the request to cloud driver to

00:01:41,920 --> 00:01:47,600
refresh the cache for a specific

00:01:44,079 --> 00:01:50,000
object for example if we're deploying

00:01:47,600 --> 00:01:51,520
an nginx deployment coordinate

00:01:50,000 --> 00:01:53,520
deployment

00:01:51,520 --> 00:01:55,600
then this request will indicate cloud

00:01:53,520 --> 00:01:58,880
driver that it should refresh

00:01:55,600 --> 00:02:02,079
that object at this point in time

00:01:58,880 --> 00:02:04,719
cloud driver will do a cube ctl get call

00:02:02,079 --> 00:02:06,240
for that object to the kubernetes api

00:02:04,719 --> 00:02:09,280
server

00:02:06,240 --> 00:02:12,879
in the example it will be cube ctl

00:02:09,280 --> 00:02:12,879
get deployment engine x

00:02:12,959 --> 00:02:17,280
right and then it will save this

00:02:16,160 --> 00:02:20,480
response to

00:02:17,280 --> 00:02:20,959
persistent storage in a special region

00:02:20,480 --> 00:02:24,480
called

00:02:20,959 --> 00:02:28,800
on demand the persistent storage

00:02:24,480 --> 00:02:28,800
could be either sql or radius

00:02:28,879 --> 00:02:34,400
the it's important to note that the

00:02:31,760 --> 00:02:36,160
result of this call is not merged to the

00:02:34,400 --> 00:02:38,400
cache immediately

00:02:36,160 --> 00:02:39,840
instead it's saved to this special

00:02:38,400 --> 00:02:42,239
region

00:02:39,840 --> 00:02:44,959
and then the system needs to wait for

00:02:42,239 --> 00:02:48,160
the next catching cycle to complete

00:02:44,959 --> 00:02:50,560
in order to incorporate this specific on

00:02:48,160 --> 00:02:53,040
demand object

00:02:50,560 --> 00:02:54,000
this is why instead of force cash

00:02:53,040 --> 00:02:57,200
refresh

00:02:54,000 --> 00:02:59,519
maybe the force part is not accurate

00:02:57,200 --> 00:03:01,040
because it's not forcing a cash refresh

00:02:59,519 --> 00:03:03,840
it's just

00:03:01,040 --> 00:03:04,640
marking an object to be considered when

00:03:03,840 --> 00:03:08,959
the cache

00:03:04,640 --> 00:03:11,280
executes so when this process is

00:03:08,959 --> 00:03:14,720
completed cloud driver responds with

00:03:11,280 --> 00:03:17,440
202 to orca

00:03:14,720 --> 00:03:18,480
so when that happens orkan knows that

00:03:17,440 --> 00:03:20,879
the object

00:03:18,480 --> 00:03:23,280
is in the process of being refreshed but

00:03:20,879 --> 00:03:26,080
it hasn't finished yet

00:03:23,280 --> 00:03:29,040
so the next thing is that orca enters in

00:03:26,080 --> 00:03:31,440
a continuous loop

00:03:29,040 --> 00:03:34,799
trying to fetch the status of the

00:03:31,440 --> 00:03:38,400
on-demand objects that were saved

00:03:34,799 --> 00:03:39,040
and it will be in this loop orca until

00:03:38,400 --> 00:03:42,799
it sees

00:03:39,040 --> 00:03:43,760
the process time for the object that was

00:03:42,799 --> 00:03:47,599
requested

00:03:43,760 --> 00:03:50,159
set to a valid value and this is

00:03:47,599 --> 00:03:50,959
when happens the timeout of the 12

00:03:50,159 --> 00:03:53,680
minutes

00:03:50,959 --> 00:03:53,680
in this loop

00:03:54,239 --> 00:03:59,519
so let's go let's go to the next part of

00:03:57,439 --> 00:04:01,760
the sequence

00:03:59,519 --> 00:04:04,400
when is processed the object that was

00:04:01,760 --> 00:04:07,280
requested by orca

00:04:04,400 --> 00:04:10,400
well it turns out that it will it will

00:04:07,280 --> 00:04:12,959
wait for a full catching cycle

00:04:10,400 --> 00:04:15,040
of the catching agent to process that

00:04:12,959 --> 00:04:18,000
request

00:04:15,040 --> 00:04:20,799
we're going to see what are the catching

00:04:18,000 --> 00:04:23,840
agents in a minute but basically this is

00:04:20,799 --> 00:04:27,280
the same process for everyone

00:04:23,840 --> 00:04:27,680
every 30 seconds by default the catching

00:04:27,280 --> 00:04:30,880
agent

00:04:27,680 --> 00:04:32,720
awakes and then it will retrieve

00:04:30,880 --> 00:04:34,479
all of the infrastructure from the

00:04:32,720 --> 00:04:36,880
kubernetes api

00:04:34,479 --> 00:04:38,720
that this catching agent is allowed to

00:04:36,880 --> 00:04:42,960
retrieve

00:04:38,720 --> 00:04:45,199
in our example of the nginx deployment

00:04:42,960 --> 00:04:46,479
there's a special caching caching agent

00:04:45,199 --> 00:04:50,240
that will do a call

00:04:46,479 --> 00:04:54,840
like cubectl get deployments spots

00:04:50,240 --> 00:04:56,000
config maps and so on for each specific

00:04:54,840 --> 00:04:59,120
namespace

00:04:56,000 --> 00:05:01,680
so you can see that calls like this

00:04:59,120 --> 00:05:02,479
could take a really long time to

00:05:01,680 --> 00:05:04,639
complete

00:05:02,479 --> 00:05:07,759
if you have a lot of objects in your

00:05:04,639 --> 00:05:07,759
cover net server

00:05:08,160 --> 00:05:12,400
so after that when cl when the catching

00:05:11,039 --> 00:05:15,199
agent retrieves

00:05:12,400 --> 00:05:16,960
the infrastructure then it retrieves the

00:05:15,199 --> 00:05:20,160
on-demand region from

00:05:16,960 --> 00:05:22,000
from persistence to know all of the

00:05:20,160 --> 00:05:23,120
objects that were requested to be

00:05:22,000 --> 00:05:27,520
refreshed

00:05:23,120 --> 00:05:30,880
by orca then the catching agent

00:05:27,520 --> 00:05:33,759
merges the on-demand objects with the

00:05:30,880 --> 00:05:34,800
full infrastructure objects and finally

00:05:33,759 --> 00:05:38,080
saves everything

00:05:34,800 --> 00:05:41,360
to persistence and

00:05:38,080 --> 00:05:43,039
this is a and only at this point is when

00:05:41,360 --> 00:05:46,880
the on demand

00:05:43,039 --> 00:05:49,600
object is processed so the next time

00:05:46,880 --> 00:05:50,960
that orca requests the on-demand objects

00:05:49,600 --> 00:05:53,120
from the cache

00:05:50,960 --> 00:05:55,199
it will see that this one was already

00:05:53,120 --> 00:05:58,800
processed and then can proceed with the

00:05:55,199 --> 00:05:58,800
rest of the pipeline execution

00:05:59,039 --> 00:06:03,039
so as we can see here this force cast

00:06:02,000 --> 00:06:05,919
refresh task

00:06:03,039 --> 00:06:09,120
depends on a full catching cycle for the

00:06:05,919 --> 00:06:11,759
catching agents to be completed

00:06:09,120 --> 00:06:13,520
when you have a really small number of

00:06:11,759 --> 00:06:14,960
kubernetes accounts there's not a

00:06:13,520 --> 00:06:17,120
problem because

00:06:14,960 --> 00:06:18,160
this can usually complete within 30

00:06:17,120 --> 00:06:20,400
seconds

00:06:18,160 --> 00:06:22,000
but as soon as you keep adding more and

00:06:20,400 --> 00:06:24,240
more accounts

00:06:22,000 --> 00:06:26,639
you can spend more time doing this

00:06:24,240 --> 00:06:26,639
process

00:06:28,000 --> 00:06:33,919
so how can we troubleshoot this problem

00:06:31,600 --> 00:06:35,440
it turns out there's a useful endpoint

00:06:33,919 --> 00:06:38,000
in cloud driver

00:06:35,440 --> 00:06:40,240
which is a slash slash cache slash

00:06:38,000 --> 00:06:43,280
introspection

00:06:40,240 --> 00:06:45,600
and the answer is

00:06:43,280 --> 00:06:47,520
this block for each different catching

00:06:45,600 --> 00:06:50,720
agent

00:06:47,520 --> 00:06:53,039
we can see that the most important

00:06:50,720 --> 00:06:55,360
or more most useful information here is

00:06:53,039 --> 00:06:56,080
the last execution duration and the time

00:06:55,360 --> 00:06:58,960
spent

00:06:56,080 --> 00:06:58,960
in cuba ctl

00:06:59,280 --> 00:07:06,800
so we need to see what is

00:07:02,560 --> 00:07:09,680
doing the catching agent we're going to

00:07:06,800 --> 00:07:10,560
see that in a minute but here we have

00:07:09,680 --> 00:07:13,280
the

00:07:10,560 --> 00:07:15,919
the account name so we have different

00:07:13,280 --> 00:07:17,360
catching agents for different accounts

00:07:15,919 --> 00:07:20,240
and we see that this is

00:07:17,360 --> 00:07:22,080
a core catching agent core catching

00:07:20,240 --> 00:07:23,919
agent is the one responsible for

00:07:22,080 --> 00:07:27,919
catching

00:07:23,919 --> 00:07:30,240
core kinds like pots deployments secrets

00:07:27,919 --> 00:07:33,840
and so on

00:07:30,240 --> 00:07:37,120
so we can see that here in this example

00:07:33,840 --> 00:07:39,599
it's taking 30 seconds just in the cube

00:07:37,120 --> 00:07:42,160
city alcohol

00:07:39,599 --> 00:07:44,639
that's a lot of time we need to

00:07:42,160 --> 00:07:46,000
investigate why it's taking some time in

00:07:44,639 --> 00:07:49,199
this call

00:07:46,000 --> 00:07:51,120
one thing that we can do is to exec into

00:07:49,199 --> 00:07:53,680
cloud driver pod

00:07:51,120 --> 00:07:55,280
and then we can issue manually the cube

00:07:53,680 --> 00:07:58,319
ctl command

00:07:55,280 --> 00:08:00,000
that is targeting that specific account

00:07:58,319 --> 00:08:02,479
to see how much time it takes to

00:08:00,000 --> 00:08:02,479
complete

00:08:02,879 --> 00:08:08,720
if the cube ctl time is a lot

00:08:06,800 --> 00:08:11,039
then we need to think if there's some

00:08:08,720 --> 00:08:13,120
issue in the latency between cloud

00:08:11,039 --> 00:08:17,120
driver and the target

00:08:13,120 --> 00:08:18,960
cluster or maybe the the target api

00:08:17,120 --> 00:08:21,360
server is

00:08:18,960 --> 00:08:22,160
is under heavy load and that's why the

00:08:21,360 --> 00:08:25,199
latency

00:08:22,160 --> 00:08:28,080
is increasing maybe there's some

00:08:25,199 --> 00:08:30,080
kind of catching issue in the cube ctl

00:08:28,080 --> 00:08:34,080
binary itself

00:08:30,080 --> 00:08:36,080
and but basically we we can identify

00:08:34,080 --> 00:08:39,200
that the problem is in that part

00:08:36,080 --> 00:08:39,200
of the cycle

00:08:39,279 --> 00:08:42,719
then if we see that in the time spending

00:08:41,919 --> 00:08:45,839
keep ctl

00:08:42,719 --> 00:08:48,320
is reasonable we can check the last

00:08:45,839 --> 00:08:51,680
execution duration

00:08:48,320 --> 00:08:55,839
in this example it took 40 seconds

00:08:51,680 --> 00:08:55,839
so 10 more seconds at the cup city

00:08:56,000 --> 00:09:01,200
and we can also think about increasing

00:08:59,519 --> 00:09:03,440
the resources to this

00:09:01,200 --> 00:09:04,320
instance of cloud driver if we see that

00:09:03,440 --> 00:09:07,519
it's taking

00:09:04,320 --> 00:09:08,000
a long time in executing the full

00:09:07,519 --> 00:09:10,480
caching

00:09:08,000 --> 00:09:10,480
cycle

00:09:11,279 --> 00:09:17,680
so finally in this response we also have

00:09:14,880 --> 00:09:19,519
useful information like the kinds which

00:09:17,680 --> 00:09:22,880
are the kubernetes kinds that this

00:09:19,519 --> 00:09:26,160
specific catching agent is catching

00:09:22,880 --> 00:09:27,519
and also the list of name spaces you can

00:09:26,160 --> 00:09:31,279
have different agents

00:09:27,519 --> 00:09:31,279
catching different namespaces

00:09:31,920 --> 00:09:37,839
and also we can see how many objects

00:09:35,200 --> 00:09:40,160
are saved to the cache or deleted from

00:09:37,839 --> 00:09:40,160
cache

00:09:40,560 --> 00:09:44,160
if you have a kubernetes cluster that

00:09:42,720 --> 00:09:47,519
has hundreds

00:09:44,160 --> 00:09:50,240
or thousands of secrets or config maps

00:09:47,519 --> 00:09:51,200
this could be an issue this could

00:09:50,240 --> 00:09:54,080
negatively

00:09:51,200 --> 00:09:56,320
impact the performance of spinnaker

00:09:54,080 --> 00:09:57,839
because at this point spinnaker is also

00:09:56,320 --> 00:10:01,040
catching those secrets

00:09:57,839 --> 00:10:04,160
and those config maps and

00:10:01,040 --> 00:10:06,640
not only that it turns out that

00:10:04,160 --> 00:10:10,320
when cloud driver does this cube ctl

00:10:06,640 --> 00:10:12,640
call it retrieves the response as json

00:10:10,320 --> 00:10:13,600
so imagine that you're retrieving the

00:10:12,640 --> 00:10:16,880
json

00:10:13,600 --> 00:10:20,160
definition of every secret every config

00:10:16,880 --> 00:10:23,200
map every replica set and basically

00:10:20,160 --> 00:10:25,440
every object in your kubernetes cluster

00:10:23,200 --> 00:10:27,600
and that needs to be processed by cloud

00:10:25,440 --> 00:10:31,120
driver

00:10:27,600 --> 00:10:36,240
we can see where this is heading

00:10:31,120 --> 00:10:38,560
if you have 500 kubernetes accounts

00:10:36,240 --> 00:10:40,399
it will take a lot of resources a lot of

00:10:38,560 --> 00:10:46,079
cpu and memory to process

00:10:40,399 --> 00:10:48,959
all of this information

00:10:46,079 --> 00:10:51,440
so speaking more about the catching

00:10:48,959 --> 00:10:54,240
agent nomenclature

00:10:51,440 --> 00:10:55,360
we we can see that there could be a lot

00:10:54,240 --> 00:10:57,440
of catching agents

00:10:55,360 --> 00:10:58,560
more catching agents that cover nethers

00:10:57,440 --> 00:11:01,360
accounts

00:10:58,560 --> 00:11:04,160
and that is because first a catching

00:11:01,360 --> 00:11:06,959
agent is built using the account

00:11:04,160 --> 00:11:07,360
so if you have 500 accounts you will

00:11:06,959 --> 00:11:10,480
have

00:11:07,360 --> 00:11:14,399
at least 500 catching agents

00:11:10,480 --> 00:11:16,320
then there's a type of catching agent

00:11:14,399 --> 00:11:18,160
there there are at least these three

00:11:16,320 --> 00:11:20,399
types a core

00:11:18,160 --> 00:11:21,360
which is the one that catches the core

00:11:20,399 --> 00:11:24,880
kinds

00:11:21,360 --> 00:11:27,200
as we saw there's another type that

00:11:24,880 --> 00:11:28,000
only catches custom resource definition

00:11:27,200 --> 00:11:31,360
and finally

00:11:28,000 --> 00:11:34,959
one that is specific for metrics

00:11:31,360 --> 00:11:37,440
like when you do top path or

00:11:34,959 --> 00:11:40,160
top note those are the catching agents

00:11:37,440 --> 00:11:40,160
for metrics

00:11:40,640 --> 00:11:44,160
so the catching agents are created based

00:11:43,440 --> 00:11:47,680
on account

00:11:44,160 --> 00:11:51,040
on type and finally on namespace

00:11:47,680 --> 00:11:53,680
if you have by default

00:11:51,040 --> 00:11:54,160
the the catching agents will only have

00:11:53,680 --> 00:11:58,720
one

00:11:54,160 --> 00:11:58,720
thread for catching all namespaces

00:11:59,040 --> 00:12:03,440
but then we can change a setting in the

00:12:01,440 --> 00:12:04,720
kubernetes account configuration to

00:12:03,440 --> 00:12:08,320
increase this number

00:12:04,720 --> 00:12:11,440
and spread the load of catching a single

00:12:08,320 --> 00:12:13,680
account for several threads

00:12:11,440 --> 00:12:15,120
depending on the number of namespaces

00:12:13,680 --> 00:12:18,639
and we'll get

00:12:15,120 --> 00:12:18,639
we'll get to that in a moment

00:12:20,079 --> 00:12:24,320
so once we identify the problem what we

00:12:23,279 --> 00:12:27,680
can do

00:12:24,320 --> 00:12:29,920
these are some of the recommendations

00:12:27,680 --> 00:12:32,959
first you can check the cloud driver

00:12:29,920 --> 00:12:34,880
memory usage and cpu usage

00:12:32,959 --> 00:12:37,680
you may need to increase these values

00:12:34,880 --> 00:12:39,680
because when you're adding more accounts

00:12:37,680 --> 00:12:41,920
basically you're adding more overhead to

00:12:39,680 --> 00:12:44,560
your container and you need to increase

00:12:41,920 --> 00:12:44,560
these values

00:12:44,800 --> 00:12:49,360
also another tip is increase the number

00:12:47,120 --> 00:12:50,880
of replicas

00:12:49,360 --> 00:12:52,560
so what's the difference between

00:12:50,880 --> 00:12:54,399
increasing

00:12:52,560 --> 00:12:56,160
vertically versus increasing

00:12:54,399 --> 00:12:59,200
horizontally horizontally

00:12:56,160 --> 00:12:59,760
a cloud driver well the difference is

00:12:59,200 --> 00:13:02,560
that

00:12:59,760 --> 00:13:03,360
every every replica each replica will

00:13:02,560 --> 00:13:05,760
still

00:13:03,360 --> 00:13:07,120
need to catch all of the kubernetes

00:13:05,760 --> 00:13:11,120
accounts

00:13:07,120 --> 00:13:14,240
they are not spread by replicas

00:13:11,120 --> 00:13:16,800
so generally it has

00:13:14,240 --> 00:13:18,480
more it's more beneficial to increase

00:13:16,800 --> 00:13:21,279
the memory and cpu

00:13:18,480 --> 00:13:23,440
than increase the replicants but also

00:13:21,279 --> 00:13:26,240
increasing the replicas helps

00:13:23,440 --> 00:13:27,040
and the reason is that if you have

00:13:26,240 --> 00:13:29,519
different

00:13:27,040 --> 00:13:30,079
cloud driver instances catching the same

00:13:29,519 --> 00:13:32,480
account

00:13:30,079 --> 00:13:33,360
at different times there's a higher

00:13:32,480 --> 00:13:35,760
chance that

00:13:33,360 --> 00:13:37,040
the cash is refreshed when you are doing

00:13:35,760 --> 00:13:40,160
the deployment

00:13:37,040 --> 00:13:40,959
by one of the replicants instead of

00:13:40,160 --> 00:13:43,680
waiting

00:13:40,959 --> 00:13:44,720
for a full catching cycle for the

00:13:43,680 --> 00:13:48,320
replica

00:13:44,720 --> 00:13:50,720
that is handling the deployment

00:13:48,320 --> 00:13:53,680
so that is why increasing the replicas

00:13:50,720 --> 00:13:53,680
is also helping

00:13:54,720 --> 00:13:59,040
another tip is increasing the number of

00:13:57,839 --> 00:14:02,240
cache threads

00:13:59,040 --> 00:14:04,800
in the account configuration

00:14:02,240 --> 00:14:05,600
as we saw here the cache threads are

00:14:04,800 --> 00:14:09,519
indicated

00:14:05,600 --> 00:14:10,079
indicated here so this is especially

00:14:09,519 --> 00:14:12,880
useful

00:14:10,079 --> 00:14:14,880
if your kubernetes account has a lot of

00:14:12,880 --> 00:14:17,360
name spaces

00:14:14,880 --> 00:14:19,440
and why because then you will have

00:14:17,360 --> 00:14:22,000
different catching agents

00:14:19,440 --> 00:14:24,160
catching different namespaces of the

00:14:22,000 --> 00:14:26,959
same account

00:14:24,160 --> 00:14:27,600
and for example if you have an account

00:14:26,959 --> 00:14:31,040
that has

00:14:27,600 --> 00:14:32,079
10 namespaces and you only have one

00:14:31,040 --> 00:14:35,120
cache thread

00:14:32,079 --> 00:14:37,600
and which is the default then this

00:14:35,120 --> 00:14:41,199
single caching agent

00:14:37,600 --> 00:14:44,560
that is executing in a thread will cache

00:14:41,199 --> 00:14:46,720
the information from the ten spaces

00:14:44,560 --> 00:14:48,399
but then if you increase the cast rates

00:14:46,720 --> 00:14:50,240
to 2

00:14:48,399 --> 00:14:51,760
you will have 2 different catching

00:14:50,240 --> 00:14:55,440
agents

00:14:51,760 --> 00:14:58,160
1 slash 2 and 2 slash 2.

00:14:55,440 --> 00:14:59,120
so one of them will catch just five

00:14:58,160 --> 00:15:03,440
namespaces

00:14:59,120 --> 00:15:03,440
and the other the other five namespaces

00:15:04,480 --> 00:15:07,760
so we can see that if your kubernetes

00:15:06,880 --> 00:15:09,839
accounts

00:15:07,760 --> 00:15:11,600
only have one namespace the default

00:15:09,839 --> 00:15:13,519
namespace

00:15:11,600 --> 00:15:14,880
maybe it's not that useful to increase

00:15:13,519 --> 00:15:16,639
this number

00:15:14,880 --> 00:15:18,240
this is more useful if you are

00:15:16,639 --> 00:15:22,880
increasing if your

00:15:18,240 --> 00:15:24,880
target account has many namespaces

00:15:22,880 --> 00:15:26,720
but then also you need to take into

00:15:24,880 --> 00:15:29,199
account that you're increasing the

00:15:26,720 --> 00:15:32,480
number of threads

00:15:29,199 --> 00:15:33,120
and that also has some performance

00:15:32,480 --> 00:15:35,279
impact

00:15:33,120 --> 00:15:36,959
in thread synchronization in cloud

00:15:35,279 --> 00:15:39,600
driver

00:15:36,959 --> 00:15:40,800
and mostly if you have a large number of

00:15:39,600 --> 00:15:42,880
threads

00:15:40,800 --> 00:15:44,320
you need to keep in mind that each

00:15:42,880 --> 00:15:47,600
thread

00:15:44,320 --> 00:15:48,000
needs to consume some memory so when you

00:15:47,600 --> 00:15:50,959
have

00:15:48,000 --> 00:15:53,440
large amounts of accounts and you

00:15:50,959 --> 00:15:56,079
increase the cash threats

00:15:53,440 --> 00:15:58,079
it's probably that you will need to

00:15:56,079 --> 00:16:00,560
increase the memory at least of club

00:15:58,079 --> 00:16:00,560
driver

00:16:00,880 --> 00:16:04,399
so that's something to keep in mind

00:16:05,600 --> 00:16:11,600
so what to do some other tips

00:16:08,800 --> 00:16:13,839
of course use sql instead of redis

00:16:11,600 --> 00:16:17,360
because it's

00:16:13,839 --> 00:16:18,959
it's faster to use sql and all the

00:16:17,360 --> 00:16:20,160
transactions and the read write

00:16:18,959 --> 00:16:23,600
operations

00:16:20,160 --> 00:16:26,079
are much faster using sql than redis

00:16:23,600 --> 00:16:28,000
and finally there's another setting at

00:16:26,079 --> 00:16:31,199
the account level

00:16:28,000 --> 00:16:31,839
this setting is live manifest calls and

00:16:31,199 --> 00:16:35,360
when you

00:16:31,839 --> 00:16:36,240
enable this basically you are bypassing

00:16:35,360 --> 00:16:39,440
the cache

00:16:36,240 --> 00:16:41,519
while you are doing your deployments so

00:16:39,440 --> 00:16:44,639
orca instead of doing the

00:16:41,519 --> 00:16:47,680
force cache reference task it will query

00:16:44,639 --> 00:16:50,240
directly the coordinates api server

00:16:47,680 --> 00:16:51,040
to know the which is the latest status

00:16:50,240 --> 00:16:54,399
of the object

00:16:51,040 --> 00:16:56,480
object that is being deployed

00:16:54,399 --> 00:16:57,680
and this of course is helpful because

00:16:56,480 --> 00:17:00,240
you can see that

00:16:57,680 --> 00:17:01,440
instead of your stage taking 12 minutes

00:17:00,240 --> 00:17:03,360
to finish

00:17:01,440 --> 00:17:05,280
it will return maybe in less than a

00:17:03,360 --> 00:17:09,679
minute and

00:17:05,280 --> 00:17:12,880
that works that that is a wonder

00:17:09,679 --> 00:17:16,559
but that comes with a k bed

00:17:12,880 --> 00:17:17,839
and that is that if you are not using

00:17:16,559 --> 00:17:19,679
the

00:17:17,839 --> 00:17:22,240
there may be some functionality of

00:17:19,679 --> 00:17:25,199
spinnaker that you are using

00:17:22,240 --> 00:17:27,199
that is reliant on dynamic target

00:17:25,199 --> 00:17:28,960
selection

00:17:27,199 --> 00:17:31,440
dynamic target selection is

00:17:28,960 --> 00:17:34,960
functionality that relies on the latest

00:17:31,440 --> 00:17:37,360
state of some objects in the kubernetes

00:17:34,960 --> 00:17:37,360
account

00:17:38,000 --> 00:17:43,679
so for example we have the disable

00:17:40,960 --> 00:17:45,679
manifest stage

00:17:43,679 --> 00:17:47,600
in the configuration of this stage

00:17:45,679 --> 00:17:49,520
there's a setting where you can choose

00:17:47,600 --> 00:17:51,919
the target dynamically

00:17:49,520 --> 00:17:52,960
and the target could be the largest

00:17:51,919 --> 00:17:56,480
replica set

00:17:52,960 --> 00:17:59,280
or the newest or the oldest

00:17:56,480 --> 00:18:03,039
so now you can see what happens when you

00:17:59,280 --> 00:18:06,320
have live manifest calls enabled

00:18:03,039 --> 00:18:07,840
you're bypassing the cache you proceed

00:18:06,320 --> 00:18:11,039
to the next stage

00:18:07,840 --> 00:18:13,200
assuming that it's a disabled manifest

00:18:11,039 --> 00:18:14,160
the cache is not refreshed and this

00:18:13,200 --> 00:18:16,559
stage

00:18:14,160 --> 00:18:21,600
tries to read the information from the

00:18:16,559 --> 00:18:21,600
cache so it will not behave as expected

00:18:21,760 --> 00:18:26,240
then although live manifest calls is a

00:18:24,640 --> 00:18:29,679
useful flag to set

00:18:26,240 --> 00:18:31,760
to speed up deployments it can have

00:18:29,679 --> 00:18:33,760
side effects that you don't want you

00:18:31,760 --> 00:18:37,360
don't expect

00:18:33,760 --> 00:18:39,520
so you will need to

00:18:37,360 --> 00:18:41,200
if you want to enable this flag you know

00:18:39,520 --> 00:18:44,320
you will need to think carefully

00:18:41,200 --> 00:18:46,480
on what are your pipelines doing and if

00:18:44,320 --> 00:18:49,039
they are relying on dynamic target

00:18:46,480 --> 00:18:49,039
selection

00:18:51,120 --> 00:18:54,160
finally in the latest release of

00:18:53,360 --> 00:18:57,919
spinnaker

00:18:54,160 --> 00:19:01,280
in 1.23 version we're getting rid

00:18:57,919 --> 00:19:04,080
of the force cast refresh task

00:19:01,280 --> 00:19:06,880
we have seen that maintaining this this

00:19:04,080 --> 00:19:08,960
kind of cache where kubernetes provider

00:19:06,880 --> 00:19:10,080
caches all the infrastructure in the

00:19:08,960 --> 00:19:12,720
server is really

00:19:10,080 --> 00:19:14,160
not that useful and instead of helping

00:19:12,720 --> 00:19:18,000
with the performance

00:19:14,160 --> 00:19:19,440
it has a negative impact and the reason

00:19:18,000 --> 00:19:20,559
is because when you are doing

00:19:19,440 --> 00:19:23,600
deployments

00:19:20,559 --> 00:19:24,160
you're not relying on a lot of objects

00:19:23,600 --> 00:19:26,799
different

00:19:24,160 --> 00:19:29,600
objects to continue or to proceed with

00:19:26,799 --> 00:19:29,600
the deployment

00:19:29,919 --> 00:19:34,160
and basically kubernetes is catching a

00:19:32,720 --> 00:19:36,799
lot of objects that

00:19:34,160 --> 00:19:37,600
it doesn't really use or need for

00:19:36,799 --> 00:19:41,039
example

00:19:37,600 --> 00:19:44,320
all of the secrets all of the config

00:19:41,039 --> 00:19:46,880
maps that is information that is

00:19:44,320 --> 00:19:48,160
really not useful for making deployments

00:19:46,880 --> 00:19:50,880
and it's even not

00:19:48,160 --> 00:19:53,520
not shown in the infrastructure tab of

00:19:50,880 --> 00:19:55,360
the ui of spinnaker

00:19:53,520 --> 00:19:57,280
so that's one of the reason that the

00:19:55,360 --> 00:19:59,280
newer versions of spin occur

00:19:57,280 --> 00:20:00,400
the number of objects that are being

00:19:59,280 --> 00:20:03,760
cached is

00:20:00,400 --> 00:20:06,320
much less and that allows to have much

00:20:03,760 --> 00:20:08,320
faster deployments

00:20:06,320 --> 00:20:11,039
if you're not going through the cache

00:20:08,320 --> 00:20:13,919
while you are doing the deployment

00:20:11,039 --> 00:20:16,320
in this version is basically the same as

00:20:13,919 --> 00:20:18,159
having the live manifest call set

00:20:16,320 --> 00:20:20,320
but with the difference that you don't

00:20:18,159 --> 00:20:21,760
have that bug with the dynamic target

00:20:20,320 --> 00:20:23,280
selection

00:20:21,760 --> 00:20:25,120
and the reason is that during the

00:20:23,280 --> 00:20:28,000
execution of the deployment

00:20:25,120 --> 00:20:30,080
spinnaker is smart enough to rate from

00:20:28,000 --> 00:20:32,559
the kubernetes api

00:20:30,080 --> 00:20:34,000
the information that it needs to proceed

00:20:32,559 --> 00:20:38,640
instead of relying on

00:20:34,000 --> 00:20:41,760
the cache and basically the cache

00:20:38,640 --> 00:20:42,799
in newer versions of spinnaker will only

00:20:41,760 --> 00:20:45,919
be used to

00:20:42,799 --> 00:20:49,039
man to maintain the infrastructure tab

00:20:45,919 --> 00:20:51,200
or that is the intention currently that

00:20:49,039 --> 00:20:52,159
infrastructure tab only shows the

00:20:51,200 --> 00:20:55,360
deployments

00:20:52,159 --> 00:20:56,159
replica sets and pots and that's

00:20:55,360 --> 00:20:58,799
basically the

00:20:56,159 --> 00:21:02,960
the main pieces of information that we

00:20:58,799 --> 00:21:04,480
need from the cluster so hopefully in

00:21:02,960 --> 00:21:07,120
newer versions

00:21:04,480 --> 00:21:08,159
uh you don't you don't see again this

00:21:07,120 --> 00:21:10,880
issue

00:21:08,159 --> 00:21:13,760
of the 12 minutes spent in the forecast

00:21:10,880 --> 00:21:13,760
refresh task

00:21:14,640 --> 00:21:18,960
all right i hope that this information

00:21:16,880 --> 00:21:21,200
is helpful to you

00:21:18,960 --> 00:21:22,000
and i think that you have happy

00:21:21,200 --> 00:21:24,720
deployments

00:21:22,000 --> 00:21:31,840
and faster deployments to kubernetes

00:21:24,720 --> 00:21:31,840

YouTube URL: https://www.youtube.com/watch?v=OFRjZUaqY-U


