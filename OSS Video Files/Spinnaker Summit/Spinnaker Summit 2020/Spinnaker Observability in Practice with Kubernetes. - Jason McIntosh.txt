Title: Spinnaker Observability in Practice with Kubernetes. - Jason McIntosh
Publication date: 2021-01-28
Playlist: Spinnaker Summit 2020
Description: 
	In our session, we will talk about how we are enabling the three pillars of Observability: monitoring, logs, and tracing, to ensure high-availability of our Spinnaker environments. Spinnaker can be chatty when it comes to Observability signals, and at Armory we use an observability platform vendor and operate many Spinnaker environments. Observability platform vendors typically charge by the number of metric time series (MTS), data points per minute (DPM), or amount of data ingested (Log Data). We recently released open-sourced a plugin for Spinnaker, the Armory Observability Plugin, that will enable us, among other things, to intelligently control costs with our Observability Vendor. After we talk about how enabling observability of Spinnaker in Kubernetes we will discuss what this enables us to do in practice, dashboards, alerting, canary deployments of Spinnaker services, etc, from an SRE and DevOps perspective.
Captions: 
	00:00:00,530 --> 00:00:03,629
[Music]

00:00:05,359 --> 00:00:09,679
hello

00:00:06,240 --> 00:00:12,080
welcome to a talk on observability and

00:00:09,679 --> 00:00:14,639
practice in kubernetes around spinnaker

00:00:12,080 --> 00:00:16,640
my name is jason mcintosh i am a staff

00:00:14,639 --> 00:00:18,400
engineer at armory

00:00:16,640 --> 00:00:19,760
we're going to be discussing a couple

00:00:18,400 --> 00:00:21,760
concepts based upon

00:00:19,760 --> 00:00:23,519
experience that we have writing the

00:00:21,760 --> 00:00:25,439
observability plug-in

00:00:23,519 --> 00:00:28,560
and overall absorbing has been occurring

00:00:25,439 --> 00:00:31,679
across multiple environments

00:00:28,560 --> 00:00:32,480
i've been in this space oh since the

00:00:31,679 --> 00:00:35,680
early days of

00:00:32,480 --> 00:00:37,920
nagios uh when

00:00:35,680 --> 00:00:40,480
you know the concept of observability is

00:00:37,920 --> 00:00:42,640
okay the server hasn't melted down

00:00:40,480 --> 00:00:44,079
um or oh yeah we don't have temperature

00:00:42,640 --> 00:00:46,000
alarms going off

00:00:44,079 --> 00:00:47,200
well things have changed quite a bit and

00:00:46,000 --> 00:00:48,559
particularly in the last few years

00:00:47,200 --> 00:00:50,399
there's been a lot of new innovations

00:00:48,559 --> 00:00:52,559
and a lot of new concepts so

00:00:50,399 --> 00:00:54,399
we'll go in through and discuss some of

00:00:52,559 --> 00:00:57,440
those today

00:00:54,399 --> 00:00:59,840
but let's start off with of course

00:00:57,440 --> 00:01:00,559
i got a presentation we gotta have the

00:00:59,840 --> 00:01:06,320
paperwork

00:01:00,559 --> 00:01:06,320
but let's get it started

00:01:08,080 --> 00:01:15,200
so i said jason mcintosh staff engineer

00:01:12,880 --> 00:01:17,119
um working at armory particularly on the

00:01:15,200 --> 00:01:18,159
observability plug-in

00:01:17,119 --> 00:01:21,439
but let's start off what is

00:01:18,159 --> 00:01:23,759
observability and this is a door quote

00:01:21,439 --> 00:01:25,119
from charity majors although she's

00:01:23,759 --> 00:01:27,840
pulled it from engineering

00:01:25,119 --> 00:01:28,880
and other systems but observability is a

00:01:27,840 --> 00:01:31,280
measure of how well

00:01:28,880 --> 00:01:32,799
internal states of a system can be

00:01:31,280 --> 00:01:34,960
inferred from the knowledge of its

00:01:32,799 --> 00:01:38,079
external outputs

00:01:34,960 --> 00:01:40,000
so what does that mean when we start

00:01:38,079 --> 00:01:41,520
talking about outputs that's stuff that

00:01:40,000 --> 00:01:43,520
we can see

00:01:41,520 --> 00:01:47,119
coming out of spinnaker that's metrics

00:01:43,520 --> 00:01:50,159
logs events streams from echo

00:01:47,119 --> 00:01:53,119
events or things going on

00:01:50,159 --> 00:01:53,119
with this system

00:01:53,280 --> 00:01:59,040
that's required to observe how well

00:01:56,640 --> 00:02:01,119
uh caching is operating or various

00:01:59,040 --> 00:02:04,240
pieces and parts are actually before

00:02:01,119 --> 00:02:07,439
behaving in the system so

00:02:04,240 --> 00:02:08,080
if you can't get that information you

00:02:07,439 --> 00:02:10,160
can't observe

00:02:08,080 --> 00:02:11,599
it and if you don't have enough

00:02:10,160 --> 00:02:13,280
information you're not going to be able

00:02:11,599 --> 00:02:15,920
to observe it

00:02:13,280 --> 00:02:16,879
so what does that actually look like

00:02:15,920 --> 00:02:20,239
well

00:02:16,879 --> 00:02:20,879
what do we have today matrix spinnaker

00:02:20,239 --> 00:02:23,120
uses

00:02:20,879 --> 00:02:24,160
a library called micrometer which wraps

00:02:23,120 --> 00:02:26,480
spectator

00:02:24,160 --> 00:02:27,280
on fx library and you have gauges

00:02:26,480 --> 00:02:31,040
counters

00:02:27,280 --> 00:02:32,000
basically a field that is a specific

00:02:31,040 --> 00:02:35,680
name

00:02:32,000 --> 00:02:36,720
i.e execution time a lot of tags

00:02:35,680 --> 00:02:40,480
descriptions

00:02:36,720 --> 00:02:42,400
uh labels around that and then a value

00:02:40,480 --> 00:02:43,840
um and then in some cases you may have a

00:02:42,400 --> 00:02:45,599
time stamp with that

00:02:43,840 --> 00:02:47,840
uh depending on you know the metric

00:02:45,599 --> 00:02:49,840
provider and system you're using

00:02:47,840 --> 00:02:52,480
metrics are great you can generate some

00:02:49,840 --> 00:02:54,400
very pretty dashboards that can show

00:02:52,480 --> 00:02:56,160
hey look at how many pipelines we've

00:02:54,400 --> 00:02:58,480
executed over the last day

00:02:56,160 --> 00:03:00,720
they also give you a very quick high

00:02:58,480 --> 00:03:01,840
level visibility in the overall behavior

00:03:00,720 --> 00:03:04,640
of your system

00:03:01,840 --> 00:03:06,239
as long as you have the metrics there

00:03:04,640 --> 00:03:09,280
spinnaker thankfully

00:03:06,239 --> 00:03:10,800
is very verbose on its metrics

00:03:09,280 --> 00:03:12,400
i think in one of the systems i was

00:03:10,800 --> 00:03:15,519
pulling out

00:03:12,400 --> 00:03:18,640
uh 70 000 a minute

00:03:15,519 --> 00:03:20,480
or 70 000 carbon unique metrics with all

00:03:18,640 --> 00:03:21,599
the labels including the cardinality of

00:03:20,480 --> 00:03:23,840
those metrics

00:03:21,599 --> 00:03:25,360
so you can get an inordinate amount of

00:03:23,840 --> 00:03:28,480
data from it

00:03:25,360 --> 00:03:31,040
uh very very useful

00:03:28,480 --> 00:03:32,799
but just be aware it's a number so you

00:03:31,040 --> 00:03:34,959
can be somewhat limited

00:03:32,799 --> 00:03:36,400
you have logging everybody deals with

00:03:34,959 --> 00:03:39,440
logging all day long

00:03:36,400 --> 00:03:41,200
hey i did such and such well logging is

00:03:39,440 --> 00:03:43,280
great it's very useful you can get a

00:03:41,200 --> 00:03:46,480
whole lot of information around

00:03:43,280 --> 00:03:49,200
stock traces and errors

00:03:46,480 --> 00:03:50,560
but it's extremely verbose and it's very

00:03:49,200 --> 00:03:53,920
hard to graph

00:03:50,560 --> 00:03:55,040
uh text data without translating into

00:03:53,920 --> 00:03:58,879
something that you can

00:03:55,040 --> 00:04:00,959
run analytics on it's

00:03:58,879 --> 00:04:03,120
typically stored in an elk type system

00:04:00,959 --> 00:04:04,879
which is a leucine text index

00:04:03,120 --> 00:04:06,879
you can use splunk you can use any of

00:04:04,879 --> 00:04:09,760
your logging frameworks cloudwatch

00:04:06,879 --> 00:04:11,200
similar but if you want to know errors

00:04:09,760 --> 00:04:12,879
that's one of the core places a lot of

00:04:11,200 --> 00:04:16,320
people go to to say

00:04:12,879 --> 00:04:18,239
why did such and such just not work well

00:04:16,320 --> 00:04:20,560
sometimes you just need to say have the

00:04:18,239 --> 00:04:23,840
system tell you why it didn't work

00:04:20,560 --> 00:04:24,800
tracing this is something fewer people

00:04:23,840 --> 00:04:28,160
probably heard of

00:04:24,800 --> 00:04:28,479
so tracing is tracking a request through

00:04:28,160 --> 00:04:31,840
the

00:04:28,479 --> 00:04:34,560
system when somebody hits gates

00:04:31,840 --> 00:04:35,680
the api layer it's tracking through that

00:04:34,560 --> 00:04:38,000
okay gate made a

00:04:35,680 --> 00:04:39,120
call to fiat which then made a call to

00:04:38,000 --> 00:04:40,720
cloud driver

00:04:39,120 --> 00:04:42,160
which returned to fiat and telling you

00:04:40,720 --> 00:04:44,080
the performance

00:04:42,160 --> 00:04:45,440
and the transactions through each one of

00:04:44,080 --> 00:04:48,800
those uh

00:04:45,440 --> 00:04:51,600
pieces and parts in the system uh dapper

00:04:48,800 --> 00:04:53,680
would be an early example from google

00:04:51,600 --> 00:04:56,320
side of things you'll hear zipkin cloud

00:04:53,680 --> 00:04:57,759
sleuth and a lot of your apm agents out

00:04:56,320 --> 00:05:00,240
there in the marketplace will add

00:04:57,759 --> 00:05:02,560
uh distributed tracing uh through

00:05:00,240 --> 00:05:04,639
injection into the jvms

00:05:02,560 --> 00:05:05,600
um or into whatever processes you're

00:05:04,639 --> 00:05:07,759
running

00:05:05,600 --> 00:05:09,199
very useful on all of these this is how

00:05:07,759 --> 00:05:10,320
you can actually trace through and say

00:05:09,199 --> 00:05:12,160
okay

00:05:10,320 --> 00:05:13,360
somebody hit the api why did it take

00:05:12,160 --> 00:05:16,000
five seconds

00:05:13,360 --> 00:05:18,000
well most of your tracing can say okay

00:05:16,000 --> 00:05:19,039
hey gate was fine and fiat was fine but

00:05:18,000 --> 00:05:21,919
cloud driver took

00:05:19,039 --> 00:05:22,800
a second and a half and that's how you

00:05:21,919 --> 00:05:24,800
can narrow in

00:05:22,800 --> 00:05:26,160
on where some of your problems are

00:05:24,800 --> 00:05:26,960
problem with tracing is it doesn't

00:05:26,160 --> 00:05:29,520
always tell you

00:05:26,960 --> 00:05:31,039
why cloud driver took a second and a

00:05:29,520 --> 00:05:34,639
half

00:05:31,039 --> 00:05:36,840
so just before we dive in a little more

00:05:34,639 --> 00:05:39,120
metrics traces and logs are not

00:05:36,840 --> 00:05:41,199
necessarily observability

00:05:39,120 --> 00:05:43,199
they can be used for monitoring metrics

00:05:41,199 --> 00:05:44,639
in particular are very heavily used in

00:05:43,199 --> 00:05:47,680
monitoring

00:05:44,639 --> 00:05:49,919
to know that spinnaker is up and running

00:05:47,680 --> 00:05:51,600
but they don't always tell you or give

00:05:49,919 --> 00:05:54,080
you enough information to know

00:05:51,600 --> 00:05:55,919
hey why did such and such happen in the

00:05:54,080 --> 00:05:58,800
system

00:05:55,919 --> 00:06:00,000
that's an extremely difficult problem to

00:05:58,800 --> 00:06:02,560
solve

00:06:00,000 --> 00:06:03,600
but we can get a lot of information that

00:06:02,560 --> 00:06:06,319
may get us

00:06:03,600 --> 00:06:08,319
observability do be aware there's a

00:06:06,319 --> 00:06:09,680
number of discussions and i'll reference

00:06:08,319 --> 00:06:12,960
a couple of those

00:06:09,680 --> 00:06:14,720
uh in this around observability versus

00:06:12,960 --> 00:06:17,360
what you'll typically hear as the three

00:06:14,720 --> 00:06:19,360
pillars metrics traces and logs

00:06:17,360 --> 00:06:21,039
uh but do be aware there's some debate

00:06:19,360 --> 00:06:23,199
that that doesn't necessarily mean

00:06:21,039 --> 00:06:25,280
observability

00:06:23,199 --> 00:06:27,759
and when we start talking about this

00:06:25,280 --> 00:06:28,639
don't ever forget these systems outside

00:06:27,759 --> 00:06:31,680
of spinnaker

00:06:28,639 --> 00:06:34,400
spinnaker runs and depends on redis it

00:06:31,680 --> 00:06:36,000
depends on my sequel if you're not

00:06:34,400 --> 00:06:37,600
monitoring those platforms

00:06:36,000 --> 00:06:39,039
and you can't observe the behavior of

00:06:37,600 --> 00:06:40,960
those platforms

00:06:39,039 --> 00:06:43,520
you're really not observing the whole

00:06:40,960 --> 00:06:45,520
system just pieces and parts of it

00:06:43,520 --> 00:06:47,280
now sometimes you'll know hey the

00:06:45,520 --> 00:06:49,120
database is slow

00:06:47,280 --> 00:06:50,880
but don't forget to monitor that oh yeah

00:06:49,120 --> 00:06:54,560
your i o rate on the database is

00:06:50,880 --> 00:06:58,080
also possibly problematic

00:06:54,560 --> 00:06:59,199
so metrics we get a lot of metrics from

00:06:58,080 --> 00:07:01,440
spinnaker

00:06:59,199 --> 00:07:02,960
here's an example of one okay http

00:07:01,440 --> 00:07:06,240
request counts

00:07:02,960 --> 00:07:06,880
uh over five minutes but is it useful

00:07:06,240 --> 00:07:08,240
well

00:07:06,880 --> 00:07:10,319
it depends on what you're trying to

00:07:08,240 --> 00:07:13,280
answer from a question perspective

00:07:10,319 --> 00:07:13,599
sometimes um there are certain metrics

00:07:13,280 --> 00:07:15,440
that are

00:07:13,599 --> 00:07:17,280
probably looked at way more heavily than

00:07:15,440 --> 00:07:20,479
others for example q depths

00:07:17,280 --> 00:07:23,280
or execution times

00:07:20,479 --> 00:07:25,039
but some of those we may not use until

00:07:23,280 --> 00:07:28,080
you start trying to trace through like

00:07:25,039 --> 00:07:30,000
hey why did a caching agent take long

00:07:28,080 --> 00:07:31,120
and you start looking at the performance

00:07:30,000 --> 00:07:32,880
of

00:07:31,120 --> 00:07:34,400
some of the kubernetes caching agents

00:07:32,880 --> 00:07:37,520
and timings there

00:07:34,400 --> 00:07:38,240
so you have the metrics but you do have

00:07:37,520 --> 00:07:39,680
to have sort of an

00:07:38,240 --> 00:07:41,520
understanding of what you're looking for

00:07:39,680 --> 00:07:43,440
what you want to try and answer

00:07:41,520 --> 00:07:44,560
and then maybe that metric may make

00:07:43,440 --> 00:07:46,160
sense to you

00:07:44,560 --> 00:07:47,919
and different organizations will have

00:07:46,160 --> 00:07:50,319
different perspectives on

00:07:47,919 --> 00:07:51,759
what is useful there are some considered

00:07:50,319 --> 00:07:53,280
common metrics

00:07:51,759 --> 00:07:54,960
and that's where we have some of the

00:07:53,280 --> 00:07:56,639
centralized dashboards

00:07:54,960 --> 00:07:58,080
so there was a spinnaker monitoring

00:07:56,639 --> 00:08:00,080
project that had quite a few

00:07:58,080 --> 00:08:01,759
um we haven't had much luck with those

00:08:00,080 --> 00:08:03,039
dashboards and it's not as well

00:08:01,759 --> 00:08:05,199
maintained

00:08:03,039 --> 00:08:07,039
uh we've found that the python system

00:08:05,199 --> 00:08:10,560
that it runs with

00:08:07,039 --> 00:08:11,840
to be somewhat problematic so that was

00:08:10,560 --> 00:08:12,720
actually one of the reasons that army

00:08:11,840 --> 00:08:14,479
released the

00:08:12,720 --> 00:08:16,000
observability plug-in was to try and

00:08:14,479 --> 00:08:17,919
resolve problems with that

00:08:16,000 --> 00:08:19,919
so part of that though is there are

00:08:17,919 --> 00:08:22,960
updated dashboards with this

00:08:19,919 --> 00:08:24,800
uh thanks to carl and the unique team

00:08:22,960 --> 00:08:26,240
so i'll have links to that at the end

00:08:24,800 --> 00:08:28,960
and we'll show a couple examples of

00:08:26,240 --> 00:08:32,240
those dashboards here in a minute but

00:08:28,960 --> 00:08:34,479
logging a ton of information

00:08:32,240 --> 00:08:36,719
almost too much the default tends to be

00:08:34,479 --> 00:08:38,240
an info level log

00:08:36,719 --> 00:08:39,440
you want to capture it because if you

00:08:38,240 --> 00:08:40,000
ever do need to try and figure out

00:08:39,440 --> 00:08:41,519
what's going

00:08:40,000 --> 00:08:43,039
on you're probably going to want to look

00:08:41,519 --> 00:08:44,640
through your logs

00:08:43,039 --> 00:08:46,560
but i've seen situations where the logs

00:08:44,640 --> 00:08:47,760
show okay no pointer but it doesn't tell

00:08:46,560 --> 00:08:49,120
you where why

00:08:47,760 --> 00:08:50,959
or any of the details around the

00:08:49,120 --> 00:08:53,040
parameters on it so

00:08:50,959 --> 00:08:54,959
do be careful of it you want it you want

00:08:53,040 --> 00:08:57,279
to be able to look through it

00:08:54,959 --> 00:08:58,640
but there is so much information in the

00:08:57,279 --> 00:09:01,360
logs that it can be just

00:08:58,640 --> 00:09:03,120
overwhelming there is an interesting

00:09:01,360 --> 00:09:06,560
article that talks about

00:09:03,120 --> 00:09:07,120
hey leave your logs at like a warning

00:09:06,560 --> 00:09:09,200
level

00:09:07,120 --> 00:09:11,760
but have the ability to dynamically

00:09:09,200 --> 00:09:13,920
adjust your log levels to improve

00:09:11,760 --> 00:09:15,680
the depth and scope of the information

00:09:13,920 --> 00:09:18,560
so you get much more visibility in an

00:09:15,680 --> 00:09:18,560
error situation

00:09:18,880 --> 00:09:22,640
there's lots of systems lots of

00:09:20,720 --> 00:09:24,560
discussions at one point there was jmx

00:09:22,640 --> 00:09:28,399
connectors you could hook in to

00:09:24,560 --> 00:09:29,839
change log levels uh debug logs when you

00:09:28,399 --> 00:09:31,839
turn them on are going to be

00:09:29,839 --> 00:09:32,880
exceedingly verbose because of the

00:09:31,839 --> 00:09:34,399
spring levels

00:09:32,880 --> 00:09:37,279
so you may also need to target some of

00:09:34,399 --> 00:09:38,959
your logs

00:09:37,279 --> 00:09:40,560
capture them look at them but do be

00:09:38,959 --> 00:09:42,080
aware that yeah

00:09:40,560 --> 00:09:43,839
unless you're having problems you may

00:09:42,080 --> 00:09:44,480
not actually pay that much intention to

00:09:43,839 --> 00:09:45,760
it

00:09:44,480 --> 00:09:47,760
but if you are having problems you're

00:09:45,760 --> 00:09:50,320
going to want to look at them

00:09:47,760 --> 00:09:52,080
so just be aware that you're going to

00:09:50,320 --> 00:09:53,760
have to filter through a whole lot of

00:09:52,080 --> 00:09:57,120
stuff to find those gold nuggets

00:09:53,760 --> 00:09:58,640
and some of these tracing i've got a

00:09:57,120 --> 00:09:59,600
screenshot here from the zipkin home

00:09:58,640 --> 00:10:01,040
page

00:09:59,600 --> 00:10:02,880
and this is an example i talked a little

00:10:01,040 --> 00:10:04,000
bit about before where you can trace

00:10:02,880 --> 00:10:06,880
through

00:10:04,000 --> 00:10:07,360
from the point where a system is entered

00:10:06,880 --> 00:10:09,920
i.e

00:10:07,360 --> 00:10:11,839
gate through the various pieces and

00:10:09,920 --> 00:10:14,000
parts that gate would communicate

00:10:11,839 --> 00:10:16,240
including those pieces and parts it

00:10:14,000 --> 00:10:20,480
keeps track of a span id and it lets you

00:10:16,240 --> 00:10:22,800
track all of those up to a root span

00:10:20,480 --> 00:10:24,640
and then you can see all the timings on

00:10:22,800 --> 00:10:25,680
it which gives you resulting time

00:10:24,640 --> 00:10:27,600
overall

00:10:25,680 --> 00:10:31,440
so from their screenshot it looks like

00:10:27,600 --> 00:10:33,600
their mobile api post locations update

00:10:31,440 --> 00:10:35,440
you know took 56 milliseconds and that

00:10:33,600 --> 00:10:36,640
was the biggest part of it

00:10:35,440 --> 00:10:38,320
and it's really useful when you're

00:10:36,640 --> 00:10:40,320
trying to say okay was it cloud driver

00:10:38,320 --> 00:10:41,440
was it fiat was it worker that took some

00:10:40,320 --> 00:10:44,560
time

00:10:41,440 --> 00:10:45,279
uh pretty useful definitely something

00:10:44,560 --> 00:10:48,320
that you need

00:10:45,279 --> 00:10:49,839
in modern systems so let's take a look

00:10:48,320 --> 00:10:52,399
at all three of these

00:10:49,839 --> 00:10:53,120
uh together as well as some other things

00:10:52,399 --> 00:10:54,800
here

00:10:53,120 --> 00:10:56,320
so as so we came out with the

00:10:54,800 --> 00:10:57,120
observability plug-in because the

00:10:56,320 --> 00:10:59,680
metrics

00:10:57,120 --> 00:11:00,800
that we were seeing we were having

00:10:59,680 --> 00:11:04,000
problems getting them

00:11:00,800 --> 00:11:05,839
some of the cardinality was an issue uh

00:11:04,000 --> 00:11:07,200
further we were looking at a lot of the

00:11:05,839 --> 00:11:08,880
metrics that were coming from the

00:11:07,200 --> 00:11:11,040
monitoring system were broken down

00:11:08,880 --> 00:11:13,200
with services for the metric name which

00:11:11,040 --> 00:11:14,320
made it much more difficult to organize

00:11:13,200 --> 00:11:18,000
filter

00:11:14,320 --> 00:11:20,160
group by etc so

00:11:18,000 --> 00:11:21,920
put out the observability plug-in it

00:11:20,160 --> 00:11:25,120
natively supports prometheus and

00:11:21,920 --> 00:11:27,360
relic it would probably be very easy to

00:11:25,120 --> 00:11:29,760
add some of the other

00:11:27,360 --> 00:11:33,279
you know vendors in there as long as

00:11:29,760 --> 00:11:34,560
it's a micro meter library supported so

00:11:33,279 --> 00:11:36,000
i'm not going to go through the full

00:11:34,560 --> 00:11:38,320
list of that but just be aware those are

00:11:36,000 --> 00:11:41,519
the ones we started with

00:11:38,320 --> 00:11:44,720
does not add tracing as said

00:11:41,519 --> 00:11:46,320
that's your spring cloud sleuth um it's

00:11:44,720 --> 00:11:47,040
something that we've considered talked

00:11:46,320 --> 00:11:49,279
about

00:11:47,040 --> 00:11:50,639
but have been hesitant to add at this

00:11:49,279 --> 00:11:53,040
point

00:11:50,639 --> 00:11:54,800
you can also go get tracing through many

00:11:53,040 --> 00:11:56,560
of your apm vendors

00:11:54,800 --> 00:11:58,480
that would automatically add it to your

00:11:56,560 --> 00:12:01,200
http requests

00:11:58,480 --> 00:12:03,200
logging also not covered by the plugin

00:12:01,200 --> 00:12:05,120
there are so many existing logging

00:12:03,200 --> 00:12:06,399
solutions and typically this runs

00:12:05,120 --> 00:12:07,920
independent of your clustered

00:12:06,399 --> 00:12:10,720
environment

00:12:07,920 --> 00:12:12,160
um with that said you want to learn more

00:12:10,720 --> 00:12:13,680
about logging there's a log back

00:12:12,160 --> 00:12:15,680
framework where you can tweak a lot of

00:12:13,680 --> 00:12:18,000
spinnaker's logs

00:12:15,680 --> 00:12:18,880
but a lot of companies will use a

00:12:18,000 --> 00:12:22,160
fluentd

00:12:18,880 --> 00:12:24,639
to ship logs to a remote platform

00:12:22,160 --> 00:12:25,920
plug-in it's a start we're always open

00:12:24,639 --> 00:12:28,639
to improvements we're

00:12:25,920 --> 00:12:30,639
trying to improve it uh you know pretty

00:12:28,639 --> 00:12:32,480
constantly so if you got

00:12:30,639 --> 00:12:34,480
you know contributions please bring them

00:12:32,480 --> 00:12:38,399
along and again thank you to

00:12:34,480 --> 00:12:40,320
those who are already contributing

00:12:38,399 --> 00:12:41,680
let's take a look at how this works um

00:12:40,320 --> 00:12:43,600
i'm going to borrow an environment that

00:12:41,680 --> 00:12:46,880
i stood up customized patches

00:12:43,600 --> 00:12:48,320
running a local micro case

00:12:46,880 --> 00:12:49,519
there are a couple things make sure your

00:12:48,320 --> 00:12:52,240
environment's up there that you have

00:12:49,519 --> 00:12:54,160
some of your metrics configuration

00:12:52,240 --> 00:12:56,320
i went ahead and installed for example

00:12:54,160 --> 00:12:59,680
the prometheus operator

00:12:56,320 --> 00:13:02,720
the grafana and all of that system and

00:12:59,680 --> 00:13:03,360
built the environment i'm going to do it

00:13:02,720 --> 00:13:04,880
in details

00:13:03,360 --> 00:13:06,959
i will go ahead and say hey when you

00:13:04,880 --> 00:13:10,160
want to enable the plugin

00:13:06,959 --> 00:13:12,079
the docs are on the plug-in sites

00:13:10,160 --> 00:13:14,399
just make sure you watch the spacing i

00:13:12,079 --> 00:13:16,560
just shot myself in the foot because

00:13:14,399 --> 00:13:17,920
i put prometheus up at the config layer

00:13:16,560 --> 00:13:19,440
and it's under the config layer for

00:13:17,920 --> 00:13:22,560
example

00:13:19,440 --> 00:13:25,920
this adds a new endpoint eop prometheus

00:13:22,560 --> 00:13:29,120
to these services one

00:13:25,920 --> 00:13:31,519
caution flag on this gate since it's

00:13:29,120 --> 00:13:32,079
usually publicly accessible and to make

00:13:31,519 --> 00:13:34,560
the

00:13:32,079 --> 00:13:36,000
endpoint publicly accessible you have to

00:13:34,560 --> 00:13:37,760
expose it through the management

00:13:36,000 --> 00:13:39,440
endpoints that makes it accessible

00:13:37,760 --> 00:13:42,800
through curl

00:13:39,440 --> 00:13:45,279
and prometheus scrape for example

00:13:42,800 --> 00:13:47,360
as soon as you do that somebody else in

00:13:45,279 --> 00:13:48,880
the internet could be able to scrape

00:13:47,360 --> 00:13:51,920
your metric data

00:13:48,880 --> 00:13:53,519
you may not want that to happen so you

00:13:51,920 --> 00:13:55,519
would probably want to filter that at

00:13:53,519 --> 00:13:56,320
your edge layer in some fashion either

00:13:55,519 --> 00:13:58,720
through

00:13:56,320 --> 00:13:59,680
an ingress routing rule or some other

00:13:58,720 --> 00:14:01,519
mechanism

00:13:59,680 --> 00:14:03,360
i'm not going to go into the details of

00:14:01,519 --> 00:14:06,399
some of that

00:14:03,360 --> 00:14:09,279
be aware it is though the

00:14:06,399 --> 00:14:10,320
spring management endpoint you can do

00:14:09,279 --> 00:14:12,880
some of the things

00:14:10,320 --> 00:14:14,480
like setting username and password uh

00:14:12,880 --> 00:14:17,519
read the documentation for more

00:14:14,480 --> 00:14:19,600
information on some of those options

00:14:17,519 --> 00:14:20,959
changing the port not really viable at

00:14:19,600 --> 00:14:21,680
this time although there's been some

00:14:20,959 --> 00:14:25,199
discussion

00:14:21,680 --> 00:14:28,720
around it so with those that

00:14:25,199 --> 00:14:30,399
do be cautious of that so

00:14:28,720 --> 00:14:32,800
asset i'm going to borrow some of the

00:14:30,399 --> 00:14:35,120
dashboards thank you to the unique team

00:14:32,800 --> 00:14:36,480
um they require jsonnet and they also

00:14:35,120 --> 00:14:40,399
require prometheus

00:14:36,480 --> 00:14:42,000
and monitoring and fix so

00:14:40,399 --> 00:14:43,519
let's go ahead and bring that up and see

00:14:42,000 --> 00:14:46,639
what that looks like

00:14:43,519 --> 00:14:50,000
so i've got a grafana environment

00:14:46,639 --> 00:14:52,560
stood up right now i also have

00:14:50,000 --> 00:14:54,480
my background a prometheus environment

00:14:52,560 --> 00:14:57,680
where i port forward it

00:14:54,480 --> 00:14:59,360
and what we actually have here is i'll

00:14:57,680 --> 00:15:00,720
start off with a key metrics database

00:14:59,360 --> 00:15:02,399
and i'm not going to go into the details

00:15:00,720 --> 00:15:04,800
of all of the different dashboards and

00:15:02,399 --> 00:15:06,639
information that we get

00:15:04,800 --> 00:15:08,240
but you can see some of them here now

00:15:06,639 --> 00:15:11,199
being a test environment

00:15:08,240 --> 00:15:12,079
i don't have it hooked up to aws and i

00:15:11,199 --> 00:15:14,480
don't have it

00:15:12,079 --> 00:15:15,760
set up for jenkins and bakes so there's

00:15:14,480 --> 00:15:17,440
a number of things that i'm going to be

00:15:15,760 --> 00:15:19,440
missing here

00:15:17,440 --> 00:15:20,880
but you can at least see hey we have

00:15:19,440 --> 00:15:22,959
some data

00:15:20,880 --> 00:15:24,160
color invocation rate we can actually

00:15:22,959 --> 00:15:27,360
view this

00:15:24,160 --> 00:15:31,120
in greater detail and

00:15:27,360 --> 00:15:32,399
let's see here if we want to we can look

00:15:31,120 --> 00:15:36,480
at

00:15:32,399 --> 00:15:36,480
the model for how this was generated

00:15:36,800 --> 00:15:41,680
very useful anybody has access to it

00:15:39,839 --> 00:15:43,839
but we are occasionally finding things

00:15:41,680 --> 00:15:45,279
so feel free to

00:15:43,839 --> 00:15:47,199
look at some of these and offer

00:15:45,279 --> 00:15:50,480
improvements if we actually

00:15:47,199 --> 00:15:51,440
look we can see some of the different

00:15:50,480 --> 00:15:55,040
data that we come

00:15:51,440 --> 00:15:57,680
in and hey we can look at the

00:15:55,040 --> 00:15:59,440
history on it so i'm not going to go

00:15:57,680 --> 00:16:00,160
through all the grafana options at this

00:15:59,440 --> 00:16:02,320
point

00:16:00,160 --> 00:16:04,160
but do be aware there is a ton of data

00:16:02,320 --> 00:16:05,759
that we don't always generate dashboards

00:16:04,160 --> 00:16:08,880
for

00:16:05,759 --> 00:16:12,959
so we look through prometheus

00:16:08,880 --> 00:16:14,720
you can see for example

00:16:12,959 --> 00:16:18,000
one of the ones i look at is long

00:16:14,720 --> 00:16:18,000
garbage collection pauses

00:16:19,199 --> 00:16:24,720
we'll run our query and you can see yeah

00:16:22,320 --> 00:16:25,600
we're all under a second we're perfectly

00:16:24,720 --> 00:16:28,240
fine

00:16:25,600 --> 00:16:29,360
uh from this case there's i said a ton

00:16:28,240 --> 00:16:31,920
of metrics

00:16:29,360 --> 00:16:33,600
feel free to dig through some of these

00:16:31,920 --> 00:16:36,959
find out what you like

00:16:33,600 --> 00:16:40,160
you know request counts um and

00:16:36,959 --> 00:16:41,680
all kinds of other information on it so

00:16:40,160 --> 00:16:43,440
for more information you're welcome to

00:16:41,680 --> 00:16:45,040
ask the monitoring channel look around

00:16:43,440 --> 00:16:46,639
there's blog posts i'll have several

00:16:45,040 --> 00:16:49,680
linked

00:16:46,639 --> 00:16:51,839
but these are some example queries that

00:16:49,680 --> 00:16:53,600
you can run i tend to like to look at

00:16:51,839 --> 00:16:56,240
hey how's the performance

00:16:53,600 --> 00:16:57,279
of you know all of my pipelines are they

00:16:56,240 --> 00:16:59,440
executing how are

00:16:57,279 --> 00:17:01,680
many are failing and breaking them down

00:16:59,440 --> 00:17:05,039
by application in their state

00:17:01,680 --> 00:17:08,079
i do like to look at execution time

00:17:05,039 --> 00:17:11,360
just to keep an idea of what's going on

00:17:08,079 --> 00:17:14,400
but very good dashboards look at them

00:17:11,360 --> 00:17:16,319
use them they're very handy when you're

00:17:14,400 --> 00:17:19,520
trying to figure out what's going on

00:17:16,319 --> 00:17:21,520
uh i'll talk real quick about apm

00:17:19,520 --> 00:17:22,799
um and just show a quick demo i borrowed

00:17:21,520 --> 00:17:25,919
a free new relic

00:17:22,799 --> 00:17:29,440
account to demo this these

00:17:25,919 --> 00:17:33,039
metrics monitoring logging very handy

00:17:29,440 --> 00:17:35,200
but you don't always get full details

00:17:33,039 --> 00:17:37,760
because what happens about a piece of

00:17:35,200 --> 00:17:42,400
code that nobody has added a metric to

00:17:37,760 --> 00:17:44,720
or is an exception that gets just eaten

00:17:42,400 --> 00:17:47,200
there are times where apm agents can be

00:17:44,720 --> 00:17:51,280
very useful on some of this

00:17:47,200 --> 00:17:53,600
so i have just a period demo environment

00:17:51,280 --> 00:17:56,160
where i have added some basic

00:17:53,600 --> 00:17:58,160
transaction monitoring

00:17:56,160 --> 00:17:59,760
apms a lot of times show you different

00:17:58,160 --> 00:18:01,360
endpoints their performance on it for

00:17:59,760 --> 00:18:02,799
example here's cloud driver

00:18:01,360 --> 00:18:04,799
and it breaks down application

00:18:02,799 --> 00:18:06,480
performance into response times and

00:18:04,799 --> 00:18:07,600
gives you a whole lot of detailed

00:18:06,480 --> 00:18:09,440
information

00:18:07,600 --> 00:18:13,280
you can get a lot of this same

00:18:09,440 --> 00:18:16,720
information from your dashboards

00:18:13,280 --> 00:18:16,720
wherever my links are

00:18:19,200 --> 00:18:24,080
i think i may shut down the grifano one

00:18:22,799 --> 00:18:26,160
back to it you can get a lot of the

00:18:24,080 --> 00:18:28,000
information from your dashboards

00:18:26,160 --> 00:18:29,360
um and from your internal metrics but

00:18:28,000 --> 00:18:30,240
there are things that you can do for

00:18:29,360 --> 00:18:32,240
example

00:18:30,240 --> 00:18:34,400
with thread profiling that you wouldn't

00:18:32,240 --> 00:18:38,160
necessarily be able to do

00:18:34,400 --> 00:18:40,240
without having access to an av jvm agent

00:18:38,160 --> 00:18:41,440
these third profiles let you inspect and

00:18:40,240 --> 00:18:44,000
say oh yeah

00:18:41,440 --> 00:18:44,799
what is actually going on well in this

00:18:44,000 --> 00:18:47,840
system

00:18:44,799 --> 00:18:49,520
not much most of the stuff is part

00:18:47,840 --> 00:18:50,880
and just sitting there waiting for

00:18:49,520 --> 00:18:52,400
things to happen

00:18:50,880 --> 00:18:54,799
because guess what it's a test

00:18:52,400 --> 00:18:56,640
environment

00:18:54,799 --> 00:18:58,160
on an actual production system though

00:18:56,640 --> 00:19:00,080
you will see a lot more going on

00:18:58,160 --> 00:19:01,440
including things like

00:19:00,080 --> 00:19:03,200
performance of your different agents

00:19:01,440 --> 00:19:05,360
where time is spent can be

00:19:03,200 --> 00:19:06,240
very useful when you're trying to track

00:19:05,360 --> 00:19:09,440
performance

00:19:06,240 --> 00:19:12,400
and what's going on with your system so

00:19:09,440 --> 00:19:14,080
apms can be very handy but just like

00:19:12,400 --> 00:19:14,880
anything else you have to spend some

00:19:14,080 --> 00:19:16,640
time

00:19:14,880 --> 00:19:18,880
looking at them figuring out how to use

00:19:16,640 --> 00:19:20,799
them but as you can see there's an

00:19:18,880 --> 00:19:21,919
example of a caching agent which shows a

00:19:20,799 --> 00:19:24,960
spike

00:19:21,919 --> 00:19:28,640
oh this is a talking to an

00:19:24,960 --> 00:19:31,120
ec2 instance uh and it took a little

00:19:28,640 --> 00:19:32,320
longer than expected

00:19:31,120 --> 00:19:34,720
doesn't mean it's out of the realm of

00:19:32,320 --> 00:19:37,039
possibility because

00:19:34,720 --> 00:19:38,240
if we look at application time most

00:19:37,039 --> 00:19:39,919
people don't know it

00:19:38,240 --> 00:19:42,720
but it would be useful for debugging

00:19:39,919 --> 00:19:42,720
caching times

00:19:44,640 --> 00:19:49,120
handy also can get you your tracing

00:19:48,080 --> 00:19:50,799
information

00:19:49,120 --> 00:19:53,679
i don't believe this system i have fully

00:19:50,799 --> 00:19:55,600
wired for distributed tracing

00:19:53,679 --> 00:19:58,160
mostly because again this is a test

00:19:55,600 --> 00:20:01,120
environment so there's not a lot of data

00:19:58,160 --> 00:20:03,760
as you can see there's almost no spans

00:20:01,120 --> 00:20:06,799
but if we look at

00:20:03,760 --> 00:20:08,559
an example of a span you can see oh yeah

00:20:06,799 --> 00:20:10,960
hey this came through hit these

00:20:08,559 --> 00:20:12,480
different systems talk to sql

00:20:10,960 --> 00:20:15,200
and one of them was getting load

00:20:12,480 --> 00:20:18,480
balancing that took a little longer

00:20:15,200 --> 00:20:19,440
so again very useful most of your

00:20:18,480 --> 00:20:22,480
systems you can

00:20:19,440 --> 00:20:25,919
absolutely do this it is useful for

00:20:22,480 --> 00:20:25,919
diagnosting problems

00:20:27,360 --> 00:20:30,400
capture all the data you know you never

00:20:29,600 --> 00:20:32,799
know

00:20:30,400 --> 00:20:34,960
what you're not going to know so the

00:20:32,799 --> 00:20:36,240
more you capture with the more depth

00:20:34,960 --> 00:20:38,559
the more you're going to be able to

00:20:36,240 --> 00:20:41,760
identify problems in the system

00:20:38,559 --> 00:20:43,440
this includes things like your logs and

00:20:41,760 --> 00:20:45,280
capturing debug logs and being able to

00:20:43,440 --> 00:20:46,400
dynamically adjust to that when there's

00:20:45,280 --> 00:20:48,080
problems

00:20:46,400 --> 00:20:50,080
be prepared for some of that because

00:20:48,080 --> 00:20:51,679
it's going to happen

00:20:50,080 --> 00:20:53,440
everything has its issues i was just

00:20:51,679 --> 00:20:55,520
debugging today a problem with

00:20:53,440 --> 00:20:56,960
database slowdowns and crash cloud

00:20:55,520 --> 00:20:59,200
driver

00:20:56,960 --> 00:21:00,559
having some of this information you can

00:20:59,200 --> 00:21:03,200
then track through that oh yeah the

00:21:00,559 --> 00:21:06,720
database is slowing down

00:21:03,200 --> 00:21:08,320
do be aware spinnaker is very verbose on

00:21:06,720 --> 00:21:11,679
its metrics so you can get an a

00:21:08,320 --> 00:21:11,679
certain amount of cardinality

00:21:12,320 --> 00:21:16,000
that means even though you may be only

00:21:14,240 --> 00:21:18,720
publishing say five or ten thousand

00:21:16,000 --> 00:21:20,640
metrics a minute because of the unique

00:21:18,720 --> 00:21:21,120
amount of labels generated with each of

00:21:20,640 --> 00:21:23,039
them

00:21:21,120 --> 00:21:24,720
you may have seventy thousand unique

00:21:23,039 --> 00:21:26,400
metrics in any given system over a

00:21:24,720 --> 00:21:28,320
period of time

00:21:26,400 --> 00:21:30,080
it's incredibly useful and provides

00:21:28,320 --> 00:21:32,480
incredible detail

00:21:30,080 --> 00:21:36,240
but there is a caution flag particularly

00:21:32,480 --> 00:21:39,360
if you look at the stackdriver

00:21:36,240 --> 00:21:42,400
uh spinnaker documentation

00:21:39,360 --> 00:21:45,760
so they will even issue a warning

00:21:42,400 --> 00:21:49,440
on some of this

00:21:45,760 --> 00:21:51,440
doesn't mean it's wrong but do be aware

00:21:49,440 --> 00:21:53,919
that there is some gotchas with some of

00:21:51,440 --> 00:21:57,360
these platforms and how they handle

00:21:53,919 --> 00:21:58,640
that so if you get a rather large amount

00:21:57,360 --> 00:22:00,320
at the end of the month don't come back

00:21:58,640 --> 00:22:03,440
and plane

00:22:00,320 --> 00:22:05,200
you've been warned still useful still

00:22:03,440 --> 00:22:06,480
want the tracing you still want the logs

00:22:05,200 --> 00:22:08,960
but

00:22:06,480 --> 00:22:10,880
i said sometimes none of this really

00:22:08,960 --> 00:22:14,400
provides the answer

00:22:10,880 --> 00:22:17,440
apms can help um the other thing is

00:22:14,400 --> 00:22:19,919
look at and turn on debug logs also

00:22:17,440 --> 00:22:22,000
echo events uh echo events will provide

00:22:19,919 --> 00:22:24,480
also quite a bit of information

00:22:22,000 --> 00:22:27,679
at the end of the day ask for help

00:22:24,480 --> 00:22:30,240
there's a lot of things out there that

00:22:27,679 --> 00:22:32,960
information resources i'll link a few

00:22:30,240 --> 00:22:32,960
towards the end

00:22:33,520 --> 00:22:37,440
absolutely amazing blogs there's a slack

00:22:35,600 --> 00:22:40,799
channel we can answer more questions and

00:22:37,440 --> 00:22:40,799
try and provide an analysis

00:22:41,039 --> 00:22:44,159
and the dashboards they're constantly

00:22:42,799 --> 00:22:46,799
expanding so

00:22:44,159 --> 00:22:47,919
as you find things please do pull

00:22:46,799 --> 00:22:50,880
requests

00:22:47,919 --> 00:22:53,200
help share the knowledge there's always

00:22:50,880 --> 00:22:56,320
constant room for improvement

00:22:53,200 --> 00:22:57,200
so a couple of resources i'd like to

00:22:56,320 --> 00:22:59,919
reference

00:22:57,200 --> 00:23:01,600
thank you for netflix and the team who

00:22:59,919 --> 00:23:02,720
have put together some pretty good

00:23:01,600 --> 00:23:06,240
dashboards

00:23:02,720 --> 00:23:08,159
presentations blogs uh the armory

00:23:06,240 --> 00:23:10,400
observability plugin

00:23:08,159 --> 00:23:11,760
this right here is open source you are

00:23:10,400 --> 00:23:14,000
welcome to come download it and give it

00:23:11,760 --> 00:23:17,039
a try please give us some feedback

00:23:14,000 --> 00:23:18,720
a big thanks to charity majors if

00:23:17,039 --> 00:23:20,799
anybody isn't aware

00:23:18,720 --> 00:23:22,320
honeycomb she does some very good talks

00:23:20,799 --> 00:23:23,840
around observability and the fact that

00:23:22,320 --> 00:23:27,280
three pillars aren't

00:23:23,840 --> 00:23:29,200
observability um they're really more

00:23:27,280 --> 00:23:30,880
modern but you can achieve

00:23:29,200 --> 00:23:33,200
some of the observability with some of

00:23:30,880 --> 00:23:35,360
that so um

00:23:33,200 --> 00:23:36,799
i'm trying not to paraphrase some of her

00:23:35,360 --> 00:23:38,559
content on that

00:23:36,799 --> 00:23:40,720
there's some other very good articles on

00:23:38,559 --> 00:23:43,120
there read around ask around

00:23:40,720 --> 00:23:44,480
learn the market is always changing on

00:23:43,120 --> 00:23:45,679
this and there's always new ideas and

00:23:44,480 --> 00:23:49,520
new ways to do things

00:23:45,679 --> 00:23:52,960
so with that said

00:23:49,520 --> 00:23:55,360
here's the plug-in so

00:23:52,960 --> 00:23:56,400
in conclusion observability plug-in with

00:23:55,360 --> 00:23:58,720
armory filters

00:23:56,400 --> 00:24:00,240
helps reduce a little bit of the spend

00:23:58,720 --> 00:24:01,360
can reduce some of the metrics you're

00:24:00,240 --> 00:24:03,760
doing

00:24:01,360 --> 00:24:04,720
it's a great tool highly recommend it

00:24:03,760 --> 00:24:07,360
apm

00:24:04,720 --> 00:24:09,440
application performance monitoring is a

00:24:07,360 --> 00:24:12,799
very good tool to complement

00:24:09,440 --> 00:24:15,679
the observability plug-in for additional

00:24:12,799 --> 00:24:16,840
introspection and visualization into

00:24:15,679 --> 00:24:19,919
your

00:24:16,840 --> 00:24:21,360
jvm again capture all the data that's

00:24:19,919 --> 00:24:23,840
reasonable

00:24:21,360 --> 00:24:27,600
look at everything that you possibly can

00:24:23,840 --> 00:24:29,760
also don't forget about echo events

00:24:27,600 --> 00:24:31,279
you know everything that happens in

00:24:29,760 --> 00:24:32,000
spinnaker should go through an echo

00:24:31,279 --> 00:24:33,520
event

00:24:32,000 --> 00:24:35,279
you can turn on a fire hose and collect

00:24:33,520 --> 00:24:38,000
a whole lot of data

00:24:35,279 --> 00:24:39,679
from that so don't forget any of the

00:24:38,000 --> 00:24:46,480
systems that you have

00:24:39,679 --> 00:24:48,559
for collecting information

00:24:46,480 --> 00:24:48,559

YouTube URL: https://www.youtube.com/watch?v=OgvyA4_O-B0


