Title: Orca v3
Publication date: 2017-09-25
Playlist: Spinnaker Summit 2017
Description: 
	ROB FLETCHER, NETFLIX

We recently did a major rewrite of the foundations of Orca in order to improve the serviceâ€™s operational capabilities. This talk will discuss the limitations of the old Orca, the changes we made, how we operationalize the new implementation, and the opportunities we now have to take things further.

From Spinnaker Summit 2017
Captions: 
	00:00:00,030 --> 00:00:04,920
hi my name's Rob Fletcher I'm an

00:00:03,240 --> 00:00:06,810
engineer on the delivery engineering

00:00:04,920 --> 00:00:09,630
team here at Netflix I've been working

00:00:06,810 --> 00:00:14,700
on spinnaker for about three years now

00:00:09,630 --> 00:00:16,830
a little over so I'm here today to talk

00:00:14,700 --> 00:00:19,140
do a little bit of a semi deep dive on

00:00:16,830 --> 00:00:21,779
Orca which is the central orchestration

00:00:19,140 --> 00:00:24,060
component of spinnaker it's the thing

00:00:21,779 --> 00:00:25,289
that coordinates all of the different

00:00:24,060 --> 00:00:28,920
stages you configure on a spinnaker

00:00:25,289 --> 00:00:31,410
pipeline or runs tasks that you run from

00:00:28,920 --> 00:00:32,700
the UI or from by the API it doesn't

00:00:31,410 --> 00:00:33,809
really do any of the work itself but it

00:00:32,700 --> 00:00:36,020
ties everything together it's the

00:00:33,809 --> 00:00:39,840
workflow engine at the core of a

00:00:36,020 --> 00:00:42,870
spinnaker pipeline and recently we

00:00:39,840 --> 00:00:45,480
rolled out a pre-major rewrite of the

00:00:42,870 --> 00:00:48,270
core of that service so I want to take

00:00:45,480 --> 00:00:50,640
you through some of the history of where

00:00:48,270 --> 00:00:52,770
we started with Orca and why we decided

00:00:50,640 --> 00:00:55,800
we needed to do a real implementation

00:00:52,770 --> 00:01:02,399
and where we are now and where we can go

00:00:55,800 --> 00:01:05,309
from here just in case anyone's not

00:01:02,399 --> 00:01:06,659
super familiar with I have some of the

00:01:05,309 --> 00:01:08,490
internal stuff an orca works I'm going

00:01:06,659 --> 00:01:10,650
to run through some of the basics of

00:01:08,490 --> 00:01:12,420
just how our pipeline operates what

00:01:10,650 --> 00:01:14,640
stages are what tasks are that kind of

00:01:12,420 --> 00:01:15,810
thing so the one I'm talking later on it

00:01:14,640 --> 00:01:20,750
will be clear what I'm talking about

00:01:15,810 --> 00:01:23,220
hopefully so if we look at a typical

00:01:20,750 --> 00:01:28,080
simple pipeline the kind of thing you

00:01:23,220 --> 00:01:29,189
configure in or key yourself you've got

00:01:28,080 --> 00:01:30,600
a pipeline that consists of several

00:01:29,189 --> 00:01:32,549
stages that you configure each of these

00:01:30,600 --> 00:01:33,329
what we call top-level stages so those

00:01:32,549 --> 00:01:34,890
are the ones that you see in your

00:01:33,329 --> 00:01:36,600
configuration view so here in this

00:01:34,890 --> 00:01:38,549
example we have a bake stage a manual

00:01:36,600 --> 00:01:40,650
judgment which is just a you know is

00:01:38,549 --> 00:01:47,250
this okay to proceed and a deploy stage

00:01:40,650 --> 00:01:50,280
pretty straightforward each of those

00:01:47,250 --> 00:01:54,060
stages can decompose internally into

00:01:50,280 --> 00:01:55,979
ocker into other stages so their

00:01:54,060 --> 00:01:58,110
behavior can be implemented partially by

00:01:55,979 --> 00:02:01,259
other stages so these this gives us kind

00:01:58,110 --> 00:02:02,790
of reusable blocks of behavior and we

00:02:01,259 --> 00:02:05,399
call these synthetic stages so what

00:02:02,790 --> 00:02:07,459
these are is a state that is added

00:02:05,399 --> 00:02:10,050
programmatically into the pipeline by

00:02:07,459 --> 00:02:11,459
one of the top-level stages in order to

00:02:10,050 --> 00:02:13,640
get some of its work done in a route

00:02:11,459 --> 00:02:15,530
using the breeze will block and some of

00:02:13,640 --> 00:02:17,980
some of those synthetic stages ago

00:02:15,530 --> 00:02:20,870
shared between multiple different

00:02:17,980 --> 00:02:22,190
different top level stages so in this

00:02:20,870 --> 00:02:25,420
example we've got a deploy stage

00:02:22,190 --> 00:02:27,620
decomposes into two individual deploy

00:02:25,420 --> 00:02:30,500
stages that are targeted at a particular

00:02:27,620 --> 00:02:33,260
region in AWS which is exactly how a

00:02:30,500 --> 00:02:36,560
real deployment part deployment state

00:02:33,260 --> 00:02:38,300
works we will run a deploy for each

00:02:36,560 --> 00:02:42,530
region in parallel as an individual

00:02:38,300 --> 00:02:45,950
synthetic stage a synthetic stage itself

00:02:42,530 --> 00:02:47,540
or any stage may be uses tasks so in

00:02:45,950 --> 00:02:50,030
this case we have a create server group

00:02:47,540 --> 00:02:53,000
task which will just fire the request to

00:02:50,030 --> 00:02:55,850
AWS then we have a monitor deploy task

00:02:53,000 --> 00:02:57,470
which will monitor AWS polling it to see

00:02:55,850 --> 00:02:59,420
that it's accepted that command and that

00:02:57,470 --> 00:03:01,280
the work is in progress and then we have

00:02:59,420 --> 00:03:02,950
another task that's wait for up

00:03:01,280 --> 00:03:05,150
instances so that's waiting for the

00:03:02,950 --> 00:03:06,440
instances in the new server group that's

00:03:05,150 --> 00:03:09,020
been created to actually be up and

00:03:06,440 --> 00:03:10,100
healthy and discovery reporting success

00:03:09,020 --> 00:03:11,750
by their health checks that kind of

00:03:10,100 --> 00:03:14,690
thing there's a slightly simplified view

00:03:11,750 --> 00:03:16,880
of what a real deploy does in terms of

00:03:14,690 --> 00:03:18,860
its tasks but it's it's pretty it's

00:03:16,880 --> 00:03:21,590
pretty accurate and the last thing there

00:03:18,860 --> 00:03:23,330
the disabled old server group is another

00:03:21,590 --> 00:03:26,180
synthetic stage so synthetic stages

00:03:23,330 --> 00:03:28,600
themselves can have synthetic stages and

00:03:26,180 --> 00:03:30,890
we use these typically in the deploy

00:03:28,600 --> 00:03:32,390
stages to implement deployment

00:03:30,890 --> 00:03:34,820
strategies so if you've seen like red

00:03:32,390 --> 00:03:37,220
black or Highlander strategy or the

00:03:34,820 --> 00:03:39,500
custom strategies that you can implement

00:03:37,220 --> 00:03:41,870
yourselves then that's how we do that

00:03:39,500 --> 00:03:44,000
kind of tacks on some extra some extra

00:03:41,870 --> 00:03:45,170
stages at the end of the deploy so in

00:03:44,000 --> 00:03:46,549
this case we're doing a straightforward

00:03:45,170 --> 00:03:47,840
thing is just going to disable the old

00:03:46,549 --> 00:03:50,330
server group and that itself is

00:03:47,840 --> 00:03:51,680
obviously permitted by some tasks so the

00:03:50,330 --> 00:03:53,870
key thing here is that stages are kind

00:03:51,680 --> 00:03:56,930
of groups of tasks that can be then

00:03:53,870 --> 00:03:59,989
further grouped in term into the larger

00:03:56,930 --> 00:04:01,519
stages and tasks the atomic individual

00:03:59,989 --> 00:04:04,580
operations they're very very simple they

00:04:01,519 --> 00:04:06,049
do one thing they can be repeated so a

00:04:04,580 --> 00:04:07,150
very typical pattern we have is

00:04:06,049 --> 00:04:09,500
something that goes and makes a request

00:04:07,150 --> 00:04:11,299
being one task and the following task

00:04:09,500 --> 00:04:14,000
being something that goes in poles to

00:04:11,299 --> 00:04:16,160
see if that work is then done so that

00:04:14,000 --> 00:04:18,769
task will get run over and over and over

00:04:16,160 --> 00:04:21,320
until it's successful on this kind of a

00:04:18,769 --> 00:04:23,060
little delay into a niche he's run but

00:04:21,320 --> 00:04:25,430
some of those can be extremely

00:04:23,060 --> 00:04:27,110
long-running so we have canary stages

00:04:25,430 --> 00:04:29,719
here in Netflix that run for days

00:04:27,110 --> 00:04:32,870
and you know there's literally a task

00:04:29,719 --> 00:04:34,310
spinning they're saying is this canary

00:04:32,870 --> 00:04:35,719
finished yet is this canary finished yet

00:04:34,310 --> 00:04:39,379
give me the latest score that kind of

00:04:35,719 --> 00:04:40,870
thing and we'll talk about how we could

00:04:39,379 --> 00:04:43,360
maybe make some of that process better

00:04:40,870 --> 00:04:50,389
given the new capabilities of Orca

00:04:43,360 --> 00:04:53,569
surely so let's dive into the history or

00:04:50,389 --> 00:04:55,550
kv-1 was released or was you know

00:04:53,569 --> 00:04:56,960
initially put into operation coming on

00:04:55,550 --> 00:04:58,969
for three years ago now it was kind of

00:04:56,960 --> 00:05:00,560
the one of the first components written

00:04:58,969 --> 00:05:03,259
of spinnaker before it was open source

00:05:00,560 --> 00:05:08,840
and that was kind of my task when I

00:05:03,259 --> 00:05:12,770
first joined Netflix and yeah we

00:05:08,840 --> 00:05:14,180
initially had some decisions to make

00:05:12,770 --> 00:05:15,379
about how to implement this thing we

00:05:14,180 --> 00:05:17,090
knew we wanted to workflow

00:05:15,379 --> 00:05:19,159
we knew we did not want to be tied

00:05:17,090 --> 00:05:20,689
specifically to AWS because one of the

00:05:19,159 --> 00:05:23,539
founding principles of spinnaker was

00:05:20,689 --> 00:05:25,520
that we did not want to be running one

00:05:23,539 --> 00:05:26,990
in the AWS specific services because we

00:05:25,520 --> 00:05:30,199
always envisioned this world where

00:05:26,990 --> 00:05:32,839
spinnaker was open sourced and could be

00:05:30,199 --> 00:05:34,669
used to run on multiple clouds

00:05:32,839 --> 00:05:36,560
I think this even predated Google's

00:05:34,669 --> 00:05:39,979
involvement but we had we had been

00:05:36,560 --> 00:05:42,110
bitten writing a scart before where the

00:05:39,979 --> 00:05:44,419
proceeding system that spinnaker

00:05:42,110 --> 00:05:48,289
replaced and what happened there was

00:05:44,419 --> 00:05:50,690
that we get nice people doing Wolfson

00:05:48,289 --> 00:05:52,610
work in the open source community but on

00:05:50,690 --> 00:05:55,939
Forks that were designed specifically

00:05:52,610 --> 00:05:57,139
for like Google Cloud or I don't know

00:05:55,939 --> 00:05:59,180
you tell me what folks were there but

00:05:57,139 --> 00:06:01,250
and we would then not benefit from that

00:05:59,180 --> 00:06:03,620
innovation because it couldn't be folded

00:06:01,250 --> 00:06:06,319
back into the into the core of a star

00:06:03,620 --> 00:06:07,849
very easily so with spinnaker we really

00:06:06,319 --> 00:06:10,240
didn't want to have anything tied

00:06:07,849 --> 00:06:13,009
specifically to AWS we wanted the cloud

00:06:10,240 --> 00:06:17,000
providers to be modular so we didn't

00:06:13,009 --> 00:06:19,580
want to use simple workflow which is the

00:06:17,000 --> 00:06:22,400
Amazon offering firm doing this kind of

00:06:19,580 --> 00:06:25,279
this kind of thing in the end the

00:06:22,400 --> 00:06:27,770
decision was made to experiment with

00:06:25,279 --> 00:06:30,289
using spring back she just seemed at the

00:06:27,770 --> 00:06:32,210
time seems simple enough

00:06:30,289 --> 00:06:34,789
we only spring very well we were using

00:06:32,210 --> 00:06:37,069
spring for the bootstrap and the

00:06:34,789 --> 00:06:39,259
components of spinnaker anyway

00:06:37,069 --> 00:06:40,849
it gave us workflow context we need such

00:06:39,259 --> 00:06:44,389
as branching joining could this

00:06:40,849 --> 00:06:46,580
steps against exception only gave us you

00:06:44,389 --> 00:06:48,439
know this notion of a task that can hold

00:06:46,580 --> 00:06:50,389
something and return are still running

00:06:48,439 --> 00:06:52,669
status and then can role return a

00:06:50,389 --> 00:06:54,259
complete status when it's done

00:06:52,669 --> 00:06:56,479
it gave us things like parameters that

00:06:54,259 --> 00:06:58,490
could be fed into a job and in a context

00:06:56,479 --> 00:06:59,809
that follows along and it gave us life

00:06:58,490 --> 00:07:03,289
like life cycle events that we could

00:06:59,809 --> 00:07:07,309
monitor so you know it didn't turn out

00:07:03,289 --> 00:07:10,429
to be the greatest decision but at the

00:07:07,309 --> 00:07:11,929
time it seemed reasonable and we also

00:07:10,429 --> 00:07:13,999
have to bear in mind that at the time I

00:07:11,929 --> 00:07:15,889
was working in a different time zone a

00:07:13,999 --> 00:07:18,219
time zones away from here and I was kind

00:07:15,889 --> 00:07:20,899
of my communication the team's not as

00:07:18,219 --> 00:07:22,519
perfect as it could have been so you

00:07:20,899 --> 00:07:25,819
know in retrospect we may have made

00:07:22,519 --> 00:07:27,649
different choices but the amazing thing

00:07:25,819 --> 00:07:30,879
is it held together for two years and

00:07:27,649 --> 00:07:33,529
got up to some ridiculous scale before

00:07:30,879 --> 00:07:34,519
before we decided to actually you know

00:07:33,529 --> 00:07:37,669
it was reaching the point where we

00:07:34,519 --> 00:07:38,929
wanted to swap it out so key thing is

00:07:37,669 --> 00:07:40,279
there were a lot of things we didn't

00:07:38,929 --> 00:07:43,039
know at the time we kind of envisioned

00:07:40,279 --> 00:07:44,930
what we wanted to do with pipelines but

00:07:43,039 --> 00:07:46,309
my understanding or I think our

00:07:44,930 --> 00:07:48,259
understanding generally is a team of

00:07:46,309 --> 00:07:50,809
what a pipeline is and what it needs to

00:07:48,259 --> 00:07:53,300
do has changed dramatically over the

00:07:50,809 --> 00:07:54,680
past three years so it's easy to say

00:07:53,300 --> 00:07:56,899
with hindsight you know this was a wrong

00:07:54,680 --> 00:07:58,309
technological choice because what we

00:07:56,899 --> 00:08:00,259
find is there's kind of mismatch and

00:07:58,309 --> 00:08:01,699
intent between spinnaker and spring back

00:08:00,259 --> 00:08:05,119
spring back is not designed for

00:08:01,699 --> 00:08:06,499
distributed systems really it that's not

00:08:05,119 --> 00:08:07,909
entirely true but it's not designed for

00:08:06,499 --> 00:08:11,449
distributed systems in the way we're

00:08:07,909 --> 00:08:13,879
trying to use it it's not designed for

00:08:11,449 --> 00:08:15,050
having thousands of pipelines configured

00:08:13,879 --> 00:08:16,699
in the datastore it's designed for

00:08:15,050 --> 00:08:18,829
having a bunch of a batch jobs

00:08:16,699 --> 00:08:22,669
configured in some static code or annex

00:08:18,829 --> 00:08:24,499
in our it's not designed for flexible

00:08:22,669 --> 00:08:26,599
executions that can change what they're

00:08:24,499 --> 00:08:28,969
doing at runtime and make decisions

00:08:26,599 --> 00:08:31,399
about exactly what their structure are

00:08:28,969 --> 00:08:32,630
going to be at runtime it's not really

00:08:31,399 --> 00:08:37,339
designed for ad hoc work

00:08:32,630 --> 00:08:38,689
it's the waste spinnaker I'm going to

00:08:37,339 --> 00:08:41,269
get into this a little bit more detail

00:08:38,689 --> 00:08:42,560
later but restarting pipelines turned

00:08:41,269 --> 00:08:47,110
out to be really problematic with

00:08:42,560 --> 00:08:50,540
without spring-locks implementation and

00:08:47,110 --> 00:08:52,370
failure handling is not making sure that

00:08:50,540 --> 00:08:53,540
things like you know if we lose an order

00:08:52,370 --> 00:08:54,769
instance in the middle of a deployment

00:08:53,540 --> 00:08:56,569
what state is that

00:08:54,769 --> 00:08:58,879
that it was deploying in what state is

00:08:56,569 --> 00:09:00,429
the previous server group in those kind

00:08:58,879 --> 00:09:02,959
of things are pretty critical and

00:09:00,429 --> 00:09:06,860
wouldn't cumbersome to implement with

00:09:02,959 --> 00:09:08,540
spring back and this is not to say

00:09:06,860 --> 00:09:10,579
spring back to the terrible framework at

00:09:08,540 --> 00:09:12,309
all it's just we were using if something

00:09:10,579 --> 00:09:16,160
for something is not really designed for

00:09:12,309 --> 00:09:17,420
because it had some of these runtime

00:09:16,160 --> 00:09:19,189
features or these this kind of

00:09:17,420 --> 00:09:24,739
topological features that we wanted to

00:09:19,189 --> 00:09:27,199
use in terms of pipeline is but really

00:09:24,739 --> 00:09:28,369
it's not the tool that's fit for the

00:09:27,199 --> 00:09:30,679
purpose we were trying to use it it's

00:09:28,369 --> 00:09:32,329
very good at what it does we were trying

00:09:30,679 --> 00:09:34,790
to extend it way beyond its capability

00:09:32,329 --> 00:09:40,730
something and one of the things we found

00:09:34,790 --> 00:09:47,299
is that also having the batteries one

00:09:40,730 --> 00:09:49,279
flower modulator one of the things we

00:09:47,299 --> 00:09:51,970
found is there's a different granularity

00:09:49,279 --> 00:09:54,980
of concept between spring back and

00:09:51,970 --> 00:09:56,480
spinnaker so spinnaker decomposes things

00:09:54,980 --> 00:09:58,730
as a pipeline of stage in a task

00:09:56,480 --> 00:10:02,740
pipelines have many stages stages have

00:09:58,730 --> 00:10:05,569
many other stages or many tasks or both

00:10:02,740 --> 00:10:09,259
spin a spring batch has just two levels

00:10:05,569 --> 00:10:10,819
it has a job and a step so a job is just

00:10:09,259 --> 00:10:13,429
a big long chain of steps

00:10:10,819 --> 00:10:16,850
it's very procedural it's very

00:10:13,429 --> 00:10:19,490
straightforward but it doesn't really

00:10:16,850 --> 00:10:23,029
have that hierarchy of of composable

00:10:19,490 --> 00:10:25,369
elements that we wanted from spinnaker

00:10:23,029 --> 00:10:27,889
and one of the things that this made it

00:10:25,369 --> 00:10:30,549
really difficult to do was the trigger

00:10:27,889 --> 00:10:32,869
events when a stage started or completed

00:10:30,549 --> 00:10:34,100
because as far as spring mattress

00:10:32,869 --> 00:10:35,209
concerned this is just another step

00:10:34,100 --> 00:10:36,829
running it doesn't know anything about

00:10:35,209 --> 00:10:38,509
the notion of stages so it turned out

00:10:36,829 --> 00:10:39,740
that the member code we had to write to

00:10:38,509 --> 00:10:42,679
get that that's working was just

00:10:39,740 --> 00:10:44,929
ridiculous and this is a something we've

00:10:42,679 --> 00:10:46,790
bumped into again and again it's just we

00:10:44,929 --> 00:10:47,929
it was trying to bash a square peg into

00:10:46,790 --> 00:10:49,189
a round hole you know the amount of code

00:10:47,929 --> 00:10:51,589
we'd have to write to get spring match

00:10:49,189 --> 00:10:55,249
to do the thing we wanted to do turned

00:10:51,589 --> 00:10:56,389
out to be its excessive and more

00:10:55,249 --> 00:10:58,610
complicated than just really doing

00:10:56,389 --> 00:11:03,799
ourselves so yeah there's kind of a

00:10:58,610 --> 00:11:06,169
missing concept there of the stage and

00:11:03,799 --> 00:11:07,990
above all you know Netflix doesn't

00:11:06,169 --> 00:11:11,410
really do stateful services where

00:11:07,990 --> 00:11:14,890
the way we typically operate software in

00:11:11,410 --> 00:11:20,770
the cloud is things are resilient to

00:11:14,890 --> 00:11:23,710
failure things are scaled as neat as

00:11:20,770 --> 00:11:26,980
needed in order to support the traffic

00:11:23,710 --> 00:11:29,140
they're receiving things are not storing

00:11:26,980 --> 00:11:30,760
state and sensitive to being destroyed

00:11:29,140 --> 00:11:32,230
because you know everyone in this room

00:11:30,760 --> 00:11:35,020
has probably heard of chaos monkey the

00:11:32,230 --> 00:11:39,910
service that just will knock instances

00:11:35,020 --> 00:11:41,980
out of the cloud at random did not work

00:11:39,910 --> 00:11:45,220
with Orkut in its initial state because

00:11:41,980 --> 00:11:46,930
failure of an incidence was catastrophic

00:11:45,220 --> 00:11:48,970
if we would lose all the work running on

00:11:46,930 --> 00:11:50,230
that instance because one of the really

00:11:48,970 --> 00:11:52,570
annoying things that spring batch does

00:11:50,230 --> 00:11:55,600
is it actually pins a thread for the

00:11:52,570 --> 00:11:57,070
entire run of the other job it just

00:11:55,600 --> 00:11:58,300
locks a thread visit you know it spring

00:11:57,070 --> 00:12:00,310
backs is intended to be fairly short

00:11:58,300 --> 00:12:03,100
running it you know this envisioned to

00:12:00,310 --> 00:12:06,130
take minutes maybe hours it's only long

00:12:03,100 --> 00:12:07,570
days and it's envisioned to be busy all

00:12:06,130 --> 00:12:09,850
at that time which orca certainly isn't

00:12:07,570 --> 00:12:11,470
because a lot of the time it's sitting

00:12:09,850 --> 00:12:12,970
waiting for some other service to do

00:12:11,470 --> 00:12:14,650
something is waiting for Amazon to

00:12:12,970 --> 00:12:18,370
deploy something into the cloud it's one

00:12:14,650 --> 00:12:20,080
point for Amazon to enable some

00:12:18,370 --> 00:12:21,130
instances is waiting for Jenkins job to

00:12:20,080 --> 00:12:23,920
complete it's waiting for another

00:12:21,130 --> 00:12:25,780
pipeline to run it's waiting for a user

00:12:23,920 --> 00:12:27,400
input or all these kind of things so

00:12:25,780 --> 00:12:28,960
it's you know it turns out the awkward

00:12:27,400 --> 00:12:32,820
active idle for a hell of all the time

00:12:28,960 --> 00:12:37,240
and having it spinning locking a thread

00:12:32,820 --> 00:12:38,320
throughout that time is just nuts one of

00:12:37,240 --> 00:12:41,710
the things we found was that we couldn't

00:12:38,320 --> 00:12:43,210
even read black or code so we couldn't

00:12:41,710 --> 00:12:45,210
you know a typical deployment that

00:12:43,210 --> 00:12:47,620
Netflix is to create a new server group

00:12:45,210 --> 00:12:50,110
enable all the instances it in it as

00:12:47,620 --> 00:12:52,330
soon as they're healthy disable the old

00:12:50,110 --> 00:12:54,070
server group and Leegin leave it around

00:12:52,330 --> 00:12:55,300
for a certain amount of time in order

00:12:54,070 --> 00:12:57,130
that we can roll back very effective

00:12:55,300 --> 00:13:00,100
very easily and that was impossible in

00:12:57,130 --> 00:13:01,990
Orca because we had to drain water

00:13:00,100 --> 00:13:04,300
coffin so we had we ended up with this

00:13:01,990 --> 00:13:06,040
elaborate system where we could have a

00:13:04,300 --> 00:13:07,990
bunch of old server groups running for

00:13:06,040 --> 00:13:10,060
days and days after we rolled new ones

00:13:07,990 --> 00:13:12,070
out with updates and we tend to move

00:13:10,060 --> 00:13:13,930
pretty fast so it's not unusual for us

00:13:12,070 --> 00:13:16,750
to push two or even three versions

00:13:13,930 --> 00:13:19,450
Hawker in a day sometimes into

00:13:16,750 --> 00:13:21,100
production so we would end up like five

00:13:19,450 --> 00:13:21,540
or six also a group draining worked out

00:13:21,100 --> 00:13:23,339
today

00:13:21,540 --> 00:13:24,600
long running Canaries and you gonna look

00:13:23,339 --> 00:13:26,880
at what's running on there it's like one

00:13:24,600 --> 00:13:31,649
job one guy's canary that's been running

00:13:26,880 --> 00:13:33,690
for 72 hours and we you know this is

00:13:31,649 --> 00:13:35,160
just not where we want it to be and it's

00:13:33,690 --> 00:13:37,410
really difficult to apply hot fixes to

00:13:35,160 --> 00:13:39,509
in-flight execution so you know if

00:13:37,410 --> 00:13:41,880
somebody says all my pipelines behaving

00:13:39,509 --> 00:13:43,230
in a weird way and we go and analyze

00:13:41,880 --> 00:13:45,360
what what the problem is and it actually

00:13:43,230 --> 00:13:47,730
turned that we super simple we can't

00:13:45,360 --> 00:13:50,430
ship that fix into production and have

00:13:47,730 --> 00:13:52,139
it pick up under that running execution

00:13:50,430 --> 00:13:53,519
it's impossible you know they'll have to

00:13:52,139 --> 00:13:55,259
wait for that one to complete or cancel

00:13:53,519 --> 00:13:59,389
it and start it again on the new on the

00:13:55,259 --> 00:14:02,610
new hardware so ultimately this is not

00:13:59,389 --> 00:14:04,380
where we want it to be at all you know

00:14:02,610 --> 00:14:09,240
we wanted to be in a world where we're a

00:14:04,380 --> 00:14:10,500
more more typical well-behaved Netflix

00:14:09,240 --> 00:14:12,389
service where we're resilient to

00:14:10,500 --> 00:14:13,680
instance lost were able to restart

00:14:12,389 --> 00:14:16,949
pipelines because that turned out to be

00:14:13,680 --> 00:14:18,029
the enormous problem the way I'm going

00:14:16,949 --> 00:14:20,430
against some of the planning of

00:14:18,029 --> 00:14:21,839
pipelines later but the way spring-back

00:14:20,430 --> 00:14:23,009
requires everything to be statically

00:14:21,839 --> 00:14:24,810
defined up front so you have to know

00:14:23,009 --> 00:14:28,079
exactly every single step the job is to

00:14:24,810 --> 00:14:30,740
take up front turns out to be really

00:14:28,079 --> 00:14:32,910
difficult when you've been mapping

00:14:30,740 --> 00:14:35,339
mapping a pipeline on to that and the

00:14:32,910 --> 00:14:37,889
pipeline may be reactive to differences

00:14:35,339 --> 00:14:39,660
in the in the topology of the cloud that

00:14:37,889 --> 00:14:41,010
here they're pointing to so you know you

00:14:39,660 --> 00:14:42,720
try and restart something for our fight

00:14:41,010 --> 00:14:44,130
that is there's another server group in

00:14:42,720 --> 00:14:46,170
place there are more instances there are

00:14:44,130 --> 00:14:47,370
fewer instances things that things look

00:14:46,170 --> 00:14:49,500
different somebody changed the load

00:14:47,370 --> 00:14:51,089
balances in the meantime you can't just

00:14:49,500 --> 00:14:52,199
rerun all those same steps and assume

00:14:51,089 --> 00:14:56,819
everything's going to work because the

00:14:52,199 --> 00:14:58,440
world has changed and key you know a key

00:14:56,819 --> 00:14:59,790
thing is we wanted to distribute work

00:14:58,440 --> 00:15:02,100
across the cluster we don't want one

00:14:59,790 --> 00:15:03,779
pipeline pinned to a single instance of

00:15:02,100 --> 00:15:05,730
augur for its duration especially some

00:15:03,779 --> 00:15:07,800
of these really long-running was you

00:15:05,730 --> 00:15:10,050
know it's bad enough for a deploy that

00:15:07,800 --> 00:15:12,360
takes ten minutes is terrible for a

00:15:10,050 --> 00:15:13,470
canary that takes 72 hours didn't pin

00:15:12,360 --> 00:15:16,230
down a single instance and in fact

00:15:13,470 --> 00:15:19,730
instance fails it's gone that's you know

00:15:16,230 --> 00:15:21,930
that's not what we wanted to do at all

00:15:19,730 --> 00:15:24,029
so the first step in this that took

00:15:21,930 --> 00:15:27,360
quite a long time was moving from all

00:15:24,029 --> 00:15:29,730
Orca from v1 to v2 so if any of you dug

00:15:27,360 --> 00:15:32,880
into the JSON of a Python execution you

00:15:29,730 --> 00:15:33,910
may have seen like a execution engine

00:15:32,880 --> 00:15:39,100
flag

00:15:33,910 --> 00:15:42,760
so quite so v2 was another problem we

00:15:39,100 --> 00:15:45,190
had was that spring match concepts are

00:15:42,760 --> 00:15:46,810
kind of leaked out of the power of the

00:15:45,190 --> 00:15:50,820
abstraction of pipelines so particularly

00:15:46,810 --> 00:15:53,980
around if anyone written a custom stage

00:15:50,820 --> 00:15:57,190
the the components we used to define

00:15:53,980 --> 00:16:00,730
stages had some spring batch types in

00:15:57,190 --> 00:16:02,230
their API which is bad but we didn't at

00:16:00,730 --> 00:16:03,610
first we didn't have a great abstraction

00:16:02,230 --> 00:16:06,910
and then we just never found the time to

00:16:03,610 --> 00:16:08,500
build a good abstraction until v2 so the

00:16:06,910 --> 00:16:09,730
purpose of v2 was to not change any

00:16:08,500 --> 00:16:12,130
behavior everything still runs and

00:16:09,730 --> 00:16:13,900
spring back everything runs in the same

00:16:12,130 --> 00:16:15,820
way pipeline to composer in the same way

00:16:13,900 --> 00:16:17,950
but all of the spring back toast is is

00:16:15,820 --> 00:16:21,520
isolated and put into a single module in

00:16:17,950 --> 00:16:22,920
Orca and then in theory we can swap that

00:16:21,520 --> 00:16:26,260
module out for a totally different

00:16:22,920 --> 00:16:27,910
execution engine so this turned out to

00:16:26,260 --> 00:16:29,620
be you know it's a transactional step

00:16:27,910 --> 00:16:33,570
it's kind of a thankless task and it

00:16:29,620 --> 00:16:36,130
took a heck of a long time and it was

00:16:33,570 --> 00:16:37,900
you know inevitably there were things

00:16:36,130 --> 00:16:40,780
that didn't work quite as perfectly as

00:16:37,900 --> 00:16:43,570
they did so we wanted to get away from

00:16:40,780 --> 00:16:45,480
that as quickly as possible but it was a

00:16:43,570 --> 00:16:47,950
necessary step you know we had to

00:16:45,480 --> 00:16:51,340
untangle the tentacles of spring back

00:16:47,950 --> 00:16:53,380
from out out of our system and earlier

00:16:51,340 --> 00:16:54,510
this year we started working on all

00:16:53,380 --> 00:16:57,340
curvy three

00:16:54,510 --> 00:17:00,730
so all could be three was to do that

00:16:57,340 --> 00:17:03,340
final step of replacing the spring batch

00:17:00,730 --> 00:17:05,290
module with something else and we

00:17:03,340 --> 00:17:06,850
undenied for two years about what we

00:17:05,290 --> 00:17:08,829
wanted to move to i've been keen to

00:17:06,850 --> 00:17:11,199
write Orcas rewrite august it's almost

00:17:08,829 --> 00:17:14,079
day one of it being out there running

00:17:11,199 --> 00:17:15,579
pipelines when some of those limitations

00:17:14,079 --> 00:17:18,459
really started to become apparent and

00:17:15,579 --> 00:17:20,459
you know we looked at ARCA for quite a

00:17:18,459 --> 00:17:23,260
while

00:17:20,459 --> 00:17:25,420
Arcas really interesting but it turned

00:17:23,260 --> 00:17:28,630
out to be kind of difficult to integrate

00:17:25,420 --> 00:17:30,580
it's clustering with our with Netflix's

00:17:28,630 --> 00:17:32,380
service discovery it's not impossible

00:17:30,580 --> 00:17:35,680
it's just a little cumbersome

00:17:32,380 --> 00:17:37,870
and a little difficult to unit tests and

00:17:35,680 --> 00:17:39,250
different to kind of have any confidence

00:17:37,870 --> 00:17:41,590
you know it's working when you push it

00:17:39,250 --> 00:17:42,910
to production the way we were trying to

00:17:41,590 --> 00:17:44,920
do it

00:17:42,910 --> 00:17:47,050
our codes also it's kind of difficult to

00:17:44,920 --> 00:17:49,060
trace execution through our characters

00:17:47,050 --> 00:17:50,290
that can be running on if you get a

00:17:49,060 --> 00:17:52,840
stack trace for one of them it's not

00:17:50,290 --> 00:17:56,700
terribly useful because you know

00:17:52,840 --> 00:17:59,020
messages are sent across a message bus

00:17:56,700 --> 00:18:01,260
so the stack trace is kind of localized

00:17:59,020 --> 00:18:03,430
to where that message has been received

00:18:01,260 --> 00:18:07,180
and also we're not really clever enough

00:18:03,430 --> 00:18:12,100
to ask our ending so we didn't we didn't

00:18:07,180 --> 00:18:13,570
do that so we came up with a what became

00:18:12,100 --> 00:18:14,800
increasingly apparent is we were kind of

00:18:13,570 --> 00:18:16,570
looking around for a new framework to

00:18:14,800 --> 00:18:18,220
use let's use it let's use some new

00:18:16,570 --> 00:18:20,470
framework in this work for our system

00:18:18,220 --> 00:18:22,840
but none of them were really great fit

00:18:20,470 --> 00:18:24,760
and what we wanted really was something

00:18:22,840 --> 00:18:27,100
that was simpler and more flexible than

00:18:24,760 --> 00:18:28,450
any of these things offered and we

00:18:27,100 --> 00:18:32,980
decided we're just gonna roll our own

00:18:28,450 --> 00:18:34,540
because we know what we need to do we

00:18:32,980 --> 00:18:37,240
we're not going to end up building a

00:18:34,540 --> 00:18:38,470
square paper around whole because we

00:18:37,240 --> 00:18:41,020
know exactly where would one and it's

00:18:38,470 --> 00:18:43,090
actually not as complicated as it may

00:18:41,020 --> 00:18:44,860
have looked a couple years ago now that

00:18:43,090 --> 00:18:47,260
we understand the problem and a whole

00:18:44,860 --> 00:18:49,710
lot better than we did so we came up

00:18:47,260 --> 00:18:53,770
with a simple message queue system and

00:18:49,710 --> 00:18:58,050
we decided to our initially we wanted to

00:18:53,770 --> 00:19:01,450
have it running on dynamite which is a

00:18:58,050 --> 00:19:03,040
Netflix version of methods specific

00:19:01,450 --> 00:19:04,960
wrapper around Redis I guess you could

00:19:03,040 --> 00:19:08,230
describe as that allows for cross region

00:19:04,960 --> 00:19:10,450
replication Rob jump in if I'm talking

00:19:08,230 --> 00:19:12,960
nonsense about dynamite is that I think

00:19:10,450 --> 00:19:15,850
I think that's a pretty fair description

00:19:12,960 --> 00:19:18,640
in the end we went we haven't got there

00:19:15,850 --> 00:19:21,040
yet I'll talk about that shortly we're

00:19:18,640 --> 00:19:22,660
running on Redis just plain Redis which

00:19:21,040 --> 00:19:25,180
is what author was using for its

00:19:22,660 --> 00:19:26,530
persistence generally although the

00:19:25,180 --> 00:19:27,640
persistence of this message queue and

00:19:26,530 --> 00:19:29,740
the persistence of ocker

00:19:27,640 --> 00:19:30,760
or because pipelines are completely

00:19:29,740 --> 00:19:32,200
separate that could be running on

00:19:30,760 --> 00:19:36,100
separate reticences they could be

00:19:32,200 --> 00:19:39,010
running on four different stores we also

00:19:36,100 --> 00:19:40,510
have an S us implementation we used as a

00:19:39,010 --> 00:19:42,100
proof of concept to make sure that we

00:19:40,510 --> 00:19:45,400
weren't making the same mistake we used

00:19:42,100 --> 00:19:46,900
before by like getting redish concept to

00:19:45,400 --> 00:19:50,170
tangled up with the actual

00:19:46,900 --> 00:19:53,470
implementation of or the abstraction we

00:19:50,170 --> 00:19:54,670
were building around this message key so

00:19:53,470 --> 00:19:55,810
we don't use that ourselves but it's

00:19:54,670 --> 00:19:56,760
available in the open source repository

00:19:55,810 --> 00:20:01,150
if anyone's

00:19:56,760 --> 00:20:04,330
on AWS and would rather use sqs than the

00:20:01,150 --> 00:20:06,490
Redis it's there although given that we

00:20:04,330 --> 00:20:07,930
don't use it ourselves how helper it is

00:20:06,490 --> 00:20:11,100
I'm not sure but it's you know it

00:20:07,930 --> 00:20:14,620
fundamentally it does the right thing

00:20:11,100 --> 00:20:16,780
so the key thing we wanted to do with

00:20:14,620 --> 00:20:19,390
this message messaging system is have

00:20:16,780 --> 00:20:22,240
lightweight messages that are very very

00:20:19,390 --> 00:20:24,790
simple carry minimal state processed in

00:20:22,240 --> 00:20:27,340
like maximum of a few seconds and that's

00:20:24,790 --> 00:20:28,990
like the time it would take to make a

00:20:27,340 --> 00:20:31,780
request to cloud driver and get a

00:20:28,990 --> 00:20:32,910
response that's kind of the most the

00:20:31,780 --> 00:20:35,620
longest running what they should do

00:20:32,910 --> 00:20:37,420
those messages should represent like

00:20:35,620 --> 00:20:40,110
either atomic state changes on the

00:20:37,420 --> 00:20:43,900
pipeline or an action to be executed

00:20:40,110 --> 00:20:45,550
preferably not both we want messages

00:20:43,900 --> 00:20:48,480
that can specify delivery time so that

00:20:45,550 --> 00:20:51,370
when we're doing things like waiting for

00:20:48,480 --> 00:20:53,530
a configured amount of time we can just

00:20:51,370 --> 00:20:55,450
push a message that what I only get

00:20:53,530 --> 00:20:58,420
deliver way in the future at the point

00:20:55,450 --> 00:21:00,790
when that waits done or if we're waiting

00:20:58,420 --> 00:21:02,560
for a canary to complete we can push a

00:21:00,790 --> 00:21:05,130
message way in the future not like keep

00:21:02,560 --> 00:21:07,750
having to having to pull it all the time

00:21:05,130 --> 00:21:10,870
we haven't quite got to that yet but we

00:21:07,750 --> 00:21:11,950
will pretty soon I think and we also

00:21:10,870 --> 00:21:14,500
want to work that will just naturally

00:21:11,950 --> 00:21:16,390
kind of flow across the Orca cluster so

00:21:14,500 --> 00:21:18,250
any instance that's active in the or

00:21:16,390 --> 00:21:19,780
conservative group can read a message at

00:21:18,250 --> 00:21:25,300
any time doesn't matter what pipeline it

00:21:19,780 --> 00:21:27,700
belongs it's in reference to when we

00:21:25,300 --> 00:21:29,350
read back or go we want just the work to

00:21:27,700 --> 00:21:32,200
just roll over on that new cell group

00:21:29,350 --> 00:21:36,850
instantly and not have to drain work off

00:21:32,200 --> 00:21:39,520
of all instances for the disabled so we

00:21:36,850 --> 00:21:43,510
this is what we've built using using

00:21:39,520 --> 00:21:46,540
Redis for the store so what we have is a

00:21:43,510 --> 00:21:48,400
queue that is polled constantly by each

00:21:46,540 --> 00:21:50,770
instance in the Orca cluster each active

00:21:48,400 --> 00:21:52,390
instance so when we take an otter

00:21:50,770 --> 00:21:55,560
instance out of service discovery when

00:21:52,390 --> 00:21:58,690
we disable a server code for example we

00:21:55,560 --> 00:22:00,490
it stops polling at that point so it's

00:21:58,690 --> 00:22:01,840
only polling while it's active so as

00:22:00,490 --> 00:22:03,880
soon as we disable things they drain

00:22:01,840 --> 00:22:05,830
work for like a couple of second max and

00:22:03,880 --> 00:22:07,010
then they just become idle in almost

00:22:05,830 --> 00:22:14,100
instantly

00:22:07,010 --> 00:22:15,930
so our message types oil down to as I

00:22:14,100 --> 00:22:17,940
said it's kind of state changes simple

00:22:15,930 --> 00:22:20,550
atomic state changes or it's very simple

00:22:17,940 --> 00:22:22,350
actions so start and country execution

00:22:20,550 --> 00:22:23,580
starting to increase stage so these are

00:22:22,350 --> 00:22:27,750
the things that all they're doing is

00:22:23,580 --> 00:22:29,730
like set the status to running set the

00:22:27,750 --> 00:22:31,830
start time figure out what to do next

00:22:29,730 --> 00:22:34,980
send them a push a message of the music

00:22:31,830 --> 00:22:37,350
that says go be this next so in the case

00:22:34,980 --> 00:22:38,730
of stock start execution it would be set

00:22:37,350 --> 00:22:40,530
the pipeline status to running set the

00:22:38,730 --> 00:22:41,850
start time figure out what is the first

00:22:40,530 --> 00:22:44,760
stage or stages you're going to be

00:22:41,850 --> 00:22:47,700
running and fire start stage messages

00:22:44,760 --> 00:22:49,290
for all of those and that's exactly the

00:22:47,700 --> 00:22:51,930
same all the way down all of this

00:22:49,290 --> 00:22:54,270
hierarchy of messages they only the the

00:22:51,930 --> 00:22:57,120
two that are kind of more slightly more

00:22:54,270 --> 00:23:00,030
complex are the start stage because when

00:22:57,120 --> 00:23:01,770
that gets handled is when the stage

00:23:00,030 --> 00:23:04,350
works out okay what tasks do I have to

00:23:01,770 --> 00:23:06,660
do do I have any synthetic stages that I

00:23:04,350 --> 00:23:08,280
need to run first or afterwards are

00:23:06,660 --> 00:23:10,290
there any things you know other stages I

00:23:08,280 --> 00:23:11,540
need to run in parallel such as if I'm

00:23:10,290 --> 00:23:14,700
doing it to dry up I've got multiple

00:23:11,540 --> 00:23:18,330
reinstituted to deploy and I need to

00:23:14,700 --> 00:23:19,860
create a stage for each of those so

00:23:18,330 --> 00:23:21,840
that's the kind of most complex bit of

00:23:19,860 --> 00:23:23,610
work and then obviously running a task

00:23:21,840 --> 00:23:27,450
which is the point of which actually

00:23:23,610 --> 00:23:29,400
takes that task definition that class

00:23:27,450 --> 00:23:31,860
has been written to perform a particular

00:23:29,400 --> 00:23:35,990
action loads that up and throw some

00:23:31,860 --> 00:23:38,730
context at it and has it do its work so

00:23:35,990 --> 00:23:42,870
forgetting and let's have a look at how

00:23:38,730 --> 00:23:44,670
this differs from how in operation in

00:23:42,870 --> 00:23:46,230
construction of a pipeline and and in

00:23:44,670 --> 00:23:48,480
operation from how it was running on

00:23:46,230 --> 00:23:50,160
spring back so first of all we'll take a

00:23:48,480 --> 00:23:54,930
look at a pipe run in spring batch and

00:23:50,160 --> 00:23:56,550
how it gets built and run so we've got a

00:23:54,930 --> 00:23:58,230
simple example here a baked manual

00:23:56,550 --> 00:24:00,090
judgment deployed those are the top

00:23:58,230 --> 00:24:02,010
level stages that are configured when

00:24:00,090 --> 00:24:03,330
the when the request comes in to Orca

00:24:02,010 --> 00:24:04,890
this is what it knows about that

00:24:03,330 --> 00:24:08,970
pipeline is that it consists of these

00:24:04,890 --> 00:24:11,310
three stages so under spring backs the

00:24:08,970 --> 00:24:13,560
first thing you would do is go and build

00:24:11,310 --> 00:24:15,090
look at that big stage and build the

00:24:13,560 --> 00:24:17,070
synthetic stages and the tasks so the

00:24:15,090 --> 00:24:20,120
little circles here represent tasks and

00:24:17,070 --> 00:24:23,090
the graer stages which i hope

00:24:20,120 --> 00:24:24,980
the screen is okay for are the synthetic

00:24:23,090 --> 00:24:27,680
stages so the bake is decomposed into

00:24:24,980 --> 00:24:30,560
two baits one for each region each of

00:24:27,680 --> 00:24:32,300
which consists of a number of tasks so

00:24:30,560 --> 00:24:34,040
we would go build all those things out

00:24:32,300 --> 00:24:36,290
we figure out that judgment is just a

00:24:34,040 --> 00:24:39,170
single task we then go and build a bunch

00:24:36,290 --> 00:24:42,440
of stages for deploying and then the

00:24:39,170 --> 00:24:44,180
strategies that go behind that and all

00:24:42,440 --> 00:24:46,310
the tasks involved so at that point we'd

00:24:44,180 --> 00:24:49,520
have this complete execution plan and

00:24:46,310 --> 00:24:51,110
then we would start running it and goes

00:24:49,520 --> 00:24:55,790
through all of these stages and all of

00:24:51,110 --> 00:24:57,890
these tasks in order and runs them and

00:24:55,790 --> 00:24:59,870
this is where we get into that notion of

00:24:57,890 --> 00:25:04,970
it being a little bit rigid under spring

00:24:59,870 --> 00:25:07,460
batch it's not flexible if for example

00:25:04,970 --> 00:25:09,590
let's imagine that manual judgment

00:25:07,460 --> 00:25:11,930
doesn't get done nobody approves it

00:25:09,590 --> 00:25:13,730
because it's a weekend and in the

00:25:11,930 --> 00:25:14,810
meantime somebody's been tinkering

00:25:13,730 --> 00:25:17,210
around on the weekend and rolled a

00:25:14,810 --> 00:25:18,500
couple of server groups over and changed

00:25:17,210 --> 00:25:20,990
load balance of configuration by the

00:25:18,500 --> 00:25:23,120
time a state runs the world looks

00:25:20,990 --> 00:25:24,740
different to how we how it did at the

00:25:23,120 --> 00:25:28,460
point we were planning the execution and

00:25:24,740 --> 00:25:29,780
that can totally invalidate the

00:25:28,460 --> 00:25:34,550
assumptions that were made when planning

00:25:29,780 --> 00:25:35,960
the pipeline so it's it's super

00:25:34,550 --> 00:25:40,910
inflexible it's also really bad for

00:25:35,960 --> 00:25:42,410
restarting because if we let's say you

00:25:40,910 --> 00:25:43,790
know the deploy stage fail it's one of

00:25:42,410 --> 00:25:45,170
the deploy stages failed but it fails

00:25:43,790 --> 00:25:46,340
for some and transitory region and

00:25:45,170 --> 00:25:48,980
somebody wants to restart it we let

00:25:46,340 --> 00:25:51,230
people do this we let people restart a

00:25:48,980 --> 00:25:53,660
particular stage the promise of spring

00:25:51,230 --> 00:25:55,330
magic Kant doesn't have any notion of or

00:25:53,660 --> 00:25:57,410
the implementation we built on top of it

00:25:55,330 --> 00:26:01,370
didn't really have any notion of okay

00:25:57,410 --> 00:26:05,060
pick up work from here and carry on so

00:26:01,370 --> 00:26:06,890
we would come in with knowledge of what

00:26:05,060 --> 00:26:08,510
the Python was trying to do before all

00:26:06,890 --> 00:26:10,760
of these stages and things built out and

00:26:08,510 --> 00:26:13,460
we had to kind of reconstruct the spring

00:26:10,760 --> 00:26:15,620
that job underneath it that that was

00:26:13,460 --> 00:26:18,260
trying to do this and it's a different

00:26:15,620 --> 00:26:19,820
thing because you already know all the

00:26:18,260 --> 00:26:21,560
synthetic stages and tasks which you

00:26:19,820 --> 00:26:22,850
didn't when you first ran so it's like

00:26:21,560 --> 00:26:25,580
almost a completely different code path

00:26:22,850 --> 00:26:29,260
and it was just a horrible tangled mess

00:26:25,580 --> 00:26:31,820
of things that would fail quite a lot

00:26:29,260 --> 00:26:33,470
when things restarted it wouldn't behave

00:26:31,820 --> 00:26:37,159
exactly the way it did the first time

00:26:33,470 --> 00:26:39,679
it was pretty horrendous so I was super

00:26:37,159 --> 00:26:41,090
keen to go for that and it was also it

00:26:39,679 --> 00:26:42,980
was really frustrating to try and fix

00:26:41,090 --> 00:26:44,240
you know people would come into our come

00:26:42,980 --> 00:26:45,650
to us on call and say ah you know I

00:26:44,240 --> 00:26:47,360
tried to restart this pipeline and it

00:26:45,650 --> 00:26:49,820
doesn't really it's not really behaving

00:26:47,360 --> 00:26:52,370
the way I thought it would and it was

00:26:49,820 --> 00:26:54,020
you know as somebody who's familiar with

00:26:52,370 --> 00:26:55,490
your car actually know how horrible that

00:26:54,020 --> 00:26:57,409
code is and just don't wanna get in

00:26:55,490 --> 00:26:59,480
there and fix it because you know like

00:26:57,409 --> 00:27:04,159
the only sane thing to do is burn it all

00:26:59,480 --> 00:27:06,230
down and rewrite it so let's have a look

00:27:04,159 --> 00:27:09,650
at the differences with how it runs now

00:27:06,230 --> 00:27:11,720
on the messaging queue system so we come

00:27:09,650 --> 00:27:13,880
in with the same exact notion of the top

00:27:11,720 --> 00:27:15,049
level say years that we did before when

00:27:13,880 --> 00:27:16,669
we start running it the first thing it

00:27:15,049 --> 00:27:18,919
does is okay let's start with the bake

00:27:16,669 --> 00:27:22,100
stage I'm gonna do this in parallel I'm

00:27:18,919 --> 00:27:24,500
gonna plan it or plan those beta stages

00:27:22,100 --> 00:27:25,940
out plan the synthetic stages planning

00:27:24,500 --> 00:27:28,909
the task and then I'm just gonna run

00:27:25,940 --> 00:27:32,929
them don't care what's downstream yeah

00:27:28,909 --> 00:27:35,840
doesn't matter because we can build that

00:27:32,929 --> 00:27:38,480
app at runtime okay that's all done

00:27:35,840 --> 00:27:41,090
let's do the judgment and run it great

00:27:38,480 --> 00:27:43,789
let's do the deploy we'll figure out

00:27:41,090 --> 00:27:45,980
that we have these two parallel the

00:27:43,789 --> 00:27:47,270
poison they have synthetic stages but we

00:27:45,980 --> 00:27:48,530
don't care about what they synthetic

00:27:47,270 --> 00:27:50,179
stages are gonna do we'll just kind of

00:27:48,530 --> 00:27:52,340
create them and we know at the point

00:27:50,179 --> 00:27:54,710
where we fire a start stage message Thor

00:27:52,340 --> 00:27:57,980
for that particular stage it will go and

00:27:54,710 --> 00:28:00,289
work out what it needs to do and then it

00:27:57,980 --> 00:28:02,270
will run it straight away so it's much

00:28:00,289 --> 00:28:04,309
more reactive we don't have that problem

00:28:02,270 --> 00:28:05,480
of if somebody leaks the manual judgment

00:28:04,309 --> 00:28:08,450
sitting there for a weekend and the

00:28:05,480 --> 00:28:10,159
entire cloud changes and the mitigation

00:28:08,450 --> 00:28:12,080
actions that need to be taken by the

00:28:10,159 --> 00:28:13,190
deployment strategy to like to say how

00:28:12,080 --> 00:28:16,640
many server groups does it have to

00:28:13,190 --> 00:28:20,600
disable that could be a different number

00:28:16,640 --> 00:28:22,010
on one day from another day the only

00:28:20,600 --> 00:28:23,840
sensible time to make that decision

00:28:22,010 --> 00:28:25,100
about how many surrogates it needs to

00:28:23,840 --> 00:28:27,200
disable on which server groups they are

00:28:25,100 --> 00:28:30,039
what their IDs are is at the point

00:28:27,200 --> 00:28:30,039
you're just about to do it

00:28:30,429 --> 00:28:36,740
restarting becomes incredibly simple

00:28:33,260 --> 00:28:39,320
because all we do is we delete all of

00:28:36,740 --> 00:28:41,030
the tasks and all of the synthetic

00:28:39,320 --> 00:28:43,130
stages downstream at the point you want

00:28:41,030 --> 00:28:44,720
to restart and then just run it again

00:28:43,130 --> 00:28:46,250
just firing up a start stage message

00:28:44,720 --> 00:28:46,700
message and it goes off and does it

00:28:46,250 --> 00:28:50,090
again

00:28:46,700 --> 00:28:52,850
really really simple there's no locking

00:28:50,090 --> 00:28:54,739
of threads there's no all these messages

00:28:52,850 --> 00:28:59,929
consumed across different instances in

00:28:54,739 --> 00:29:03,739
the cluster um which is really freed up

00:28:59,929 --> 00:29:06,499
a lot of a lot of thing you know we just

00:29:03,739 --> 00:29:08,090
now we read black or Co we can do what

00:29:06,499 --> 00:29:10,220
we like we're not worried about instance

00:29:08,090 --> 00:29:13,970
lasts we can actually restart things

00:29:10,220 --> 00:29:16,580
very very simply we can if we even when

00:29:13,970 --> 00:29:18,259
we do have catastrophic bugs which we

00:29:16,580 --> 00:29:21,080
had a couple of times when in early

00:29:18,259 --> 00:29:22,669
stages of this implementation we can

00:29:21,080 --> 00:29:24,710
actually recover the work pretty easily

00:29:22,669 --> 00:29:26,539
by just you know you poke a couple of

00:29:24,710 --> 00:29:28,249
messages back into these ready spaced

00:29:26,539 --> 00:29:31,779
queues and things just start up again

00:29:28,249 --> 00:29:33,769
there's no complicated trying to map

00:29:31,779 --> 00:29:35,570
pipelines on just springg batch and then

00:29:33,769 --> 00:29:37,609
get it running and get it back running

00:29:35,570 --> 00:29:39,529
in a particular state again it's been

00:29:37,609 --> 00:29:44,480
way way simpler you know we can do it on

00:29:39,529 --> 00:29:46,580
the terminal it's so easy so at the risk

00:29:44,480 --> 00:29:48,440
of going a little too deep I be here I

00:29:46,580 --> 00:29:49,940
thought it might be interesting to have

00:29:48,440 --> 00:29:53,989
a look at how the queue itself actually

00:29:49,940 --> 00:29:55,480
operates and you can see how simple it

00:29:53,989 --> 00:29:58,340
is

00:29:55,480 --> 00:30:00,440
so the queue schema is composed of kind

00:29:58,340 --> 00:30:02,960
of three main buckets in Redis we have

00:30:00,440 --> 00:30:07,369
the queue itself which is a sorted set

00:30:02,960 --> 00:30:08,629
of message IDs with the score being like

00:30:07,369 --> 00:30:12,409
a timestamp of when it should get

00:30:08,629 --> 00:30:14,179
delivered we have the message bucket

00:30:12,409 --> 00:30:17,629
itself which is just just a hash of the

00:30:14,179 --> 00:30:19,220
message ID to a JSON serialized

00:30:17,629 --> 00:30:21,499
representation of the message payload

00:30:19,220 --> 00:30:23,029
and messages as I said have carry

00:30:21,499 --> 00:30:25,700
virtually no state they have like an

00:30:23,029 --> 00:30:28,340
execution ID some of them have stage a

00:30:25,700 --> 00:30:31,519
DS some of them have task IDs and some

00:30:28,340 --> 00:30:34,700
of them have status so things like a

00:30:31,519 --> 00:30:37,340
complete task we have a like success

00:30:34,700 --> 00:30:39,049
video still running kind of status on it

00:30:37,340 --> 00:30:40,279
but that's that's all the state they

00:30:39,049 --> 00:30:44,299
have they're not carrying around like

00:30:40,279 --> 00:30:45,830
huge payloads that's all oh they do have

00:30:44,299 --> 00:30:48,799
some attributes to do runtime stuff like

00:30:45,830 --> 00:30:50,299
traffic shaping and other interesting

00:30:48,799 --> 00:30:52,879
things that I'll briefly mention at the

00:30:50,299 --> 00:30:56,299
end the third bucket is another sorted

00:30:52,879 --> 00:30:57,919
set of message IDs with a score based on

00:30:56,299 --> 00:31:00,470
the time they should get retried so the

00:30:57,919 --> 00:31:01,090
owner of this is this is the bucket

00:31:00,470 --> 00:31:03,769
represent

00:31:01,090 --> 00:31:05,539
messages were currently processing if we

00:31:03,769 --> 00:31:06,700
don't acknowledge them in a timely

00:31:05,539 --> 00:31:10,490
fashion

00:31:06,700 --> 00:31:13,820
something else is polling back sorted

00:31:10,490 --> 00:31:15,320
set that queue and we'll put it put the

00:31:13,820 --> 00:31:17,149
message ID back on the main key bazap

00:31:15,320 --> 00:31:18,769
indicates then or instance fell over in

00:31:17,149 --> 00:31:21,110
the middle of processing this message it

00:31:18,769 --> 00:31:28,429
never they never finished it never

00:31:21,110 --> 00:31:30,139
acknowledged it so let's have a look at

00:31:28,429 --> 00:31:33,379
what successful message handling looks

00:31:30,139 --> 00:31:38,389
like so we have a push operation on the

00:31:33,379 --> 00:31:39,830
queue so this can be coming from you

00:31:38,389 --> 00:31:43,039
know the edge of Orca where it receives

00:31:39,830 --> 00:31:44,690
an API request from the UI or from API

00:31:43,039 --> 00:31:46,340
user and it says okay go start this

00:31:44,690 --> 00:31:48,220
execution so we will push that message

00:31:46,340 --> 00:31:51,409
onto the queue or it can be from

00:31:48,220 --> 00:31:53,360
handlers for other messages so the start

00:31:51,409 --> 00:31:55,639
execution message gets handled and it's

00:31:53,360 --> 00:31:57,919
the result of that is it pushes messages

00:31:55,639 --> 00:32:01,340
to say start these stages and the start

00:31:57,919 --> 00:32:02,899
stages and let pushes message to say go

00:32:01,340 --> 00:32:06,559
start this other synthetics angel ghost

00:32:02,899 --> 00:32:09,169
run this task go you know that's the

00:32:06,559 --> 00:32:11,809
push is kind of done by all of those

00:32:09,169 --> 00:32:14,450
different components and what I push

00:32:11,809 --> 00:32:16,039
does is it puts the Jake the Jason of

00:32:14,450 --> 00:32:18,889
the message onto the messages queue

00:32:16,039 --> 00:32:22,009
assigns an ID and puts the ID into the

00:32:18,889 --> 00:32:23,870
cube with a time at which you should

00:32:22,009 --> 00:32:25,940
deliver it which is by default right now

00:32:23,870 --> 00:32:28,370
but it doesn't have to be it can be in

00:32:25,940 --> 00:32:33,230
any any time the Futurity it's a type

00:32:28,370 --> 00:32:36,500
just of timestamp then we have a pole

00:32:33,230 --> 00:32:38,840
operation so this is reading off from

00:32:36,500 --> 00:32:41,570
the queue this is happening at 10

00:32:38,840 --> 00:32:43,730
millisecond intervals I believe on each

00:32:41,570 --> 00:32:46,759
order instance so then it's processed

00:32:43,730 --> 00:32:48,980
one thread is dedicated to just reading

00:32:46,759 --> 00:32:50,750
this queue and hands takes the message

00:32:48,980 --> 00:32:52,129
and hands it off to a handler which runs

00:32:50,750 --> 00:32:53,720
in a different thread in it from the

00:32:52,129 --> 00:32:55,580
thread pool but this is one thread

00:32:53,720 --> 00:32:57,080
dedicated on each actor for instance is

00:32:55,580 --> 00:32:59,929
just spinning and polling the key all

00:32:57,080 --> 00:33:02,570
the time single thread and what that

00:32:59,929 --> 00:33:04,009
whole operation does is tries to read

00:33:02,570 --> 00:33:06,320
the message and it just moves the ID

00:33:04,009 --> 00:33:09,409
from the key to the processing queue and

00:33:06,320 --> 00:33:11,450
it says ok if this doesn't get processed

00:33:09,409 --> 00:33:14,570
within that so minute I think it is a

00:33:11,450 --> 00:33:16,010
minute then we're gonna try again

00:33:14,570 --> 00:33:18,170
but and that's how it decides what the

00:33:16,010 --> 00:33:21,890
the score will be on that processing

00:33:18,170 --> 00:33:23,600
queue sorted set so then the handler

00:33:21,890 --> 00:33:25,280
goes off on another thread and does

00:33:23,600 --> 00:33:28,160
whatever work it needs to be planning

00:33:25,280 --> 00:33:30,350
stage change them status is pushing more

00:33:28,160 --> 00:33:33,140
messages to the queue that kind of thing

00:33:30,350 --> 00:33:38,180
and then it will come in and knowledge

00:33:33,140 --> 00:33:40,880
the message hopefully and that just

00:33:38,180 --> 00:33:42,200
deletes everything ever get done at that

00:33:40,880 --> 00:33:43,640
point the messages have gone you don't

00:33:42,200 --> 00:33:48,980
need them anymore did all the stages are

00:33:43,640 --> 00:33:50,690
ephemeral so what does fail failed mr.

00:33:48,980 --> 00:33:52,370
Channing look like I've kind of alluded

00:33:50,690 --> 00:33:54,740
to this with the acknowledge and retry

00:33:52,370 --> 00:33:57,050
stuff the push is exactly the same

00:33:54,740 --> 00:33:59,540
message comes in gets assigned an ID put

00:33:57,050 --> 00:34:01,370
on the queue the pole is the same

00:33:59,540 --> 00:34:02,990
someone comes along and reads the

00:34:01,370 --> 00:34:07,310
message and move to that idea across to

00:34:02,990 --> 00:34:08,120
the to the processing queue but one

00:34:07,310 --> 00:34:10,250
minute later

00:34:08,120 --> 00:34:13,190
it hasn't responded it hasn't

00:34:10,250 --> 00:34:15,350
acknowledged the message so the other

00:34:13,190 --> 00:34:16,760
polar that is sitting holding on a

00:34:15,350 --> 00:34:19,400
slightly slower interval than 10

00:34:16,760 --> 00:34:20,570
milliseconds like every 30 seconds or

00:34:19,400 --> 00:34:24,050
something it comes along and looks at

00:34:20,570 --> 00:34:27,080
this processing queue it just no because

00:34:24,050 --> 00:34:28,580
now the timestamp at which we should

00:34:27,080 --> 00:34:31,820
retry is now in the past

00:34:28,580 --> 00:34:32,840
it just says oh ok that's fun that I did

00:34:31,820 --> 00:34:34,460
back on the queue

00:34:32,840 --> 00:34:36,230
simple as that then you get a retry

00:34:34,460 --> 00:34:38,570
because some other in or incidents or

00:34:36,230 --> 00:34:41,240
the same instance is going to go read

00:34:38,570 --> 00:34:45,080
that ID off the queue again it's it's

00:34:41,240 --> 00:34:46,460
super simple this gave us like really

00:34:45,080 --> 00:34:49,340
simple and powerful resilience the

00:34:46,460 --> 00:34:52,669
instance last week we can read back or

00:34:49,340 --> 00:34:56,300
we can lose instances we don't care we

00:34:52,669 --> 00:35:00,200
can even set Castle monkey loose bridges

00:34:56,300 --> 00:35:04,820
which is nice but I guess the big

00:35:00,200 --> 00:35:08,750
question is does this scale because what

00:35:04,820 --> 00:35:10,220
are we trying to do here is its nicely

00:35:08,750 --> 00:35:13,270
summed up by this tweet that somebody

00:35:10,220 --> 00:35:15,620
found yesterday and I'm very much like

00:35:13,270 --> 00:35:16,580
there are only two hard problems in

00:35:15,620 --> 00:35:19,100
distributed systems

00:35:16,580 --> 00:35:20,660
number two exactly once delivery number

00:35:19,100 --> 00:35:23,049
one guaranteed order of messages and

00:35:20,660 --> 00:35:25,010
number two exactly once deliver

00:35:23,049 --> 00:35:26,510
we're not really worrying about message

00:35:25,010 --> 00:35:29,260
ordering here so much but we are trying

00:35:26,510 --> 00:35:34,010
to do exactly what's delivery which is a

00:35:29,260 --> 00:35:36,230
non-trivial problem so how do we you

00:35:34,010 --> 00:35:38,029
know we have typically Netflix right now

00:35:36,230 --> 00:35:41,539
we're running six author instances any

00:35:38,029 --> 00:35:43,069
given time our cluster will probably a

00:35:41,539 --> 00:35:45,039
little over over-provisioned at that to

00:35:43,069 --> 00:35:47,450
be honest but that's kind of historical

00:35:45,039 --> 00:35:49,789
we were using six under swing back from

00:35:47,450 --> 00:35:51,910
just never really taking the time to

00:35:49,789 --> 00:35:55,309
think well maybe don't need that money

00:35:51,910 --> 00:35:56,559
so how do we prevent a message gets

00:35:55,309 --> 00:35:58,700
pushed on the queue how do we prevent

00:35:56,559 --> 00:36:00,160
multiple instances reading that same

00:35:58,700 --> 00:36:02,299
messaging processing at the same time

00:36:00,160 --> 00:36:06,349
given that these things are polling like

00:36:02,299 --> 00:36:08,440
every 10 milliseconds on that key so we

00:36:06,349 --> 00:36:11,720
have kind of concurrent read protection

00:36:08,440 --> 00:36:13,970
so the process works approximately like

00:36:11,720 --> 00:36:15,470
this get the first message ID with the

00:36:13,970 --> 00:36:18,799
delivery time that's less than right now

00:36:15,470 --> 00:36:21,470
off the queue try and set a lot for that

00:36:18,799 --> 00:36:25,039
ID in an atomic fashion based on the

00:36:21,470 --> 00:36:26,059
instance and if you only if you're able

00:36:25,039 --> 00:36:28,279
to set that lock read the message

00:36:26,059 --> 00:36:33,140
payload and move that message ID from

00:36:28,279 --> 00:36:35,720
the queue to the processing queue in

00:36:33,140 --> 00:36:38,450
Redis this looks like this kind of

00:36:35,720 --> 00:36:42,289
pseudocode so we're using Redman

00:36:38,450 --> 00:36:44,569
register set NX operation to set you

00:36:42,289 --> 00:36:46,430
know it sets a value and only if it's

00:36:44,569 --> 00:36:49,160
able to set that value it returns true

00:36:46,430 --> 00:36:51,619
so that the point of convergence is

00:36:49,160 --> 00:36:55,579
there is the register and that

00:36:51,619 --> 00:36:57,349
guarantees that we can only set that

00:36:55,579 --> 00:36:58,880
lock once and only if we were able to

00:36:57,349 --> 00:37:00,410
acquire that lock will we go ahead and

00:36:58,880 --> 00:37:03,289
read that message otherwise the pole

00:37:00,410 --> 00:37:04,819
operation you know on so it's really

00:37:03,289 --> 00:37:07,760
interesting to look at how much this

00:37:04,819 --> 00:37:09,170
saves us we actually have there's an

00:37:07,760 --> 00:37:11,900
else in the real code there's an else

00:37:09,170 --> 00:37:16,579
lock there that sets an Atlas metric

00:37:11,900 --> 00:37:18,109
when this locker fails and this is what

00:37:16,579 --> 00:37:20,029
the graph looks like this I pulled this

00:37:18,109 --> 00:37:22,490
from the data on Friday last week and

00:37:20,029 --> 00:37:24,770
this is how many failed reads we're

00:37:22,490 --> 00:37:26,960
having in a typical minute is like

00:37:24,770 --> 00:37:29,779
across the cluster weird so we got

00:37:26,960 --> 00:37:33,130
failed to reach there like per second

00:37:29,779 --> 00:37:35,740
I'm sorry yeah not a minute so

00:37:33,130 --> 00:37:37,150
this is how many times an instance is

00:37:35,740 --> 00:37:38,519
trying to trying to read a message of

00:37:37,150 --> 00:37:40,690
the key but failing to set the lock

00:37:38,519 --> 00:37:42,069
because another instance has already

00:37:40,690 --> 00:37:45,670
read it at the same time and you can see

00:37:42,069 --> 00:37:48,160
it's like 30 times a second so this is

00:37:45,670 --> 00:37:51,009
saving us a hell of a lot of concurrency

00:37:48,160 --> 00:37:54,670
issues and I'm not really aware of any

00:37:51,009 --> 00:37:57,670
times where this is not working for us I

00:37:54,670 --> 00:37:59,140
mean it's we have a single Redis so we

00:37:57,670 --> 00:38:02,440
don't have a problem of that say an X

00:37:59,140 --> 00:38:07,750
operation being done atomic but it's you

00:38:02,440 --> 00:38:10,029
know it's I kind of it seems to be kind

00:38:07,750 --> 00:38:13,779
of exactly once delivering it seems to

00:38:10,029 --> 00:38:17,890
work pretty well I think we would have

00:38:13,779 --> 00:38:19,420
heard about it by now if it wasn't we

00:38:17,890 --> 00:38:20,799
also have some notion of kind of

00:38:19,420 --> 00:38:23,589
concurrent write protection which is

00:38:20,799 --> 00:38:27,250
more for this lets us be a little bit

00:38:23,589 --> 00:38:28,869
lazy about how the messages work so we

00:38:27,250 --> 00:38:32,019
store a message string to print like a

00:38:28,869 --> 00:38:34,329
CRC of the message on on push and if

00:38:32,019 --> 00:38:36,400
another message that looks exactly the

00:38:34,329 --> 00:38:38,500
same gets pushed later we just ignore it

00:38:36,400 --> 00:38:39,730
bear in mind that these messages get

00:38:38,500 --> 00:38:41,440
destroyed at the point when they're

00:38:39,730 --> 00:38:43,150
successfully read so if we do ever need

00:38:41,440 --> 00:38:47,680
to push a message that does look exactly

00:38:43,150 --> 00:38:50,529
the same we can't do that this means

00:38:47,680 --> 00:38:54,339
that we can do things like when we have

00:38:50,529 --> 00:38:56,589
parallel branches in a in a pipeline at

00:38:54,339 --> 00:38:58,150
the point where this the stage ends in

00:38:56,589 --> 00:39:01,480
any given branch you can send a message

00:38:58,150 --> 00:39:03,069
saying complete execution and then the

00:39:01,480 --> 00:39:04,539
complete execution message handler looks

00:39:03,069 --> 00:39:06,220
a bang goes well I hang on there's a

00:39:04,539 --> 00:39:08,170
bunch of other branches here that are

00:39:06,220 --> 00:39:09,369
not run that are still running so I'm

00:39:08,170 --> 00:39:12,190
not going to I'm not going to do

00:39:09,369 --> 00:39:14,049
anything and we don't care how many

00:39:12,190 --> 00:39:16,329
times different stages fire this

00:39:14,049 --> 00:39:20,650
complete execution message because we

00:39:16,329 --> 00:39:21,759
know ultimately only one get processed

00:39:20,650 --> 00:39:23,470
by the handler at that point when all of

00:39:21,759 --> 00:39:25,269
those stages are finished so this write

00:39:23,470 --> 00:39:29,109
protection is not really for concurrency

00:39:25,269 --> 00:39:30,549
problems so much as just letting us be a

00:39:29,109 --> 00:39:32,589
bit lazy about sending the same message

00:39:30,549 --> 00:39:34,180
a bunch of times and not having to worry

00:39:32,589 --> 00:39:39,299
about it too much and we also have some

00:39:34,180 --> 00:39:41,529
metrics around that and that saves us

00:39:39,299 --> 00:39:43,180
you know we have certain types of

00:39:41,529 --> 00:39:45,290
messages that we totally expect to be

00:39:43,180 --> 00:39:46,970
sending duplicate so we

00:39:45,290 --> 00:39:51,320
see the metrics coming through to that

00:39:46,970 --> 00:39:54,500
and that's a nice one so that's kind of

00:39:51,320 --> 00:39:56,090
how the key works in a nutshell and it's

00:39:54,500 --> 00:39:57,740
really really really straightforward and

00:39:56,090 --> 00:40:00,500
it gives us all of those things we

00:39:57,740 --> 00:40:03,790
wanted around resilience to into this

00:40:00,500 --> 00:40:05,960
last letting work roll across between

00:40:03,790 --> 00:40:09,650
some old server groups into new server

00:40:05,960 --> 00:40:12,860
groups letting work be distributed

00:40:09,650 --> 00:40:14,420
across across all the instances that are

00:40:12,860 --> 00:40:18,110
active so where can we go from here

00:40:14,420 --> 00:40:19,700
what's that what's the future some of

00:40:18,110 --> 00:40:23,150
the way things work in order are still

00:40:19,700 --> 00:40:25,340
very much on built with the assumptions

00:40:23,150 --> 00:40:28,070
that we not the assumptions but the

00:40:25,340 --> 00:40:32,210
restrictions that we had when we were

00:40:28,070 --> 00:40:34,520
running on spring back so for example a

00:40:32,210 --> 00:40:36,920
wake task really does poll every 15

00:40:34,520 --> 00:40:38,120
seconds it doesn't really need to do

00:40:36,920 --> 00:40:40,100
that anymore and at some point we'll get

00:40:38,120 --> 00:40:41,930
to that we'll get to rewrite to now a

00:40:40,100 --> 00:40:43,490
manual judgment stage does the same

00:40:41,930 --> 00:40:45,650
thing polls every 15 seconds to see if

00:40:43,490 --> 00:40:47,230
somebody is click the button and it

00:40:45,650 --> 00:40:50,570
doesn't need to do that because we could

00:40:47,230 --> 00:40:51,920
just stop execution at that point and

00:40:50,570 --> 00:40:53,420
then send another message when somebody

00:40:51,920 --> 00:40:57,950
does click the button just to restart

00:40:53,420 --> 00:41:01,700
that will come pretty soon one of the

00:40:57,950 --> 00:41:05,620
things that's a bit horrible in ocker

00:41:01,700 --> 00:41:07,820
right now is stages can have a notion of

00:41:05,620 --> 00:41:09,800
actions that they take if they ever get

00:41:07,820 --> 00:41:12,260
cancelled and cancellation can either be

00:41:09,800 --> 00:41:13,910
because somebody comes along and presses

00:41:12,260 --> 00:41:15,290
cancel on the pipeline or it can be

00:41:13,910 --> 00:41:18,650
because another stage running in

00:41:15,290 --> 00:41:20,750
parallel failed and that means you want

00:41:18,650 --> 00:41:23,210
to stop execution of everything in that

00:41:20,750 --> 00:41:24,950
pipeline and you want to you know clean

00:41:23,210 --> 00:41:27,530
up anything that it might have been

00:41:24,950 --> 00:41:29,510
doing so canary stages have cancellation

00:41:27,530 --> 00:41:30,890
routines Duke boys they just do a couple

00:41:29,510 --> 00:41:33,980
of others I forget exactly which ones

00:41:30,890 --> 00:41:35,960
but and they were kind of in spurring

00:41:33,980 --> 00:41:37,790
backs because that inflexibility of the

00:41:35,960 --> 00:41:41,030
structure of the job those would just

00:41:37,790 --> 00:41:43,220
run right there in in the current

00:41:41,030 --> 00:41:46,070
executing thread and they didn't really

00:41:43,220 --> 00:41:47,240
you take the pipeline at all so you it

00:41:46,070 --> 00:41:48,350
was kind of difficult to track exactly

00:41:47,240 --> 00:41:50,810
what they were doing it couldn't be

00:41:48,350 --> 00:41:53,450
super complex but couldn't be very

00:41:50,810 --> 00:41:55,430
clever that restriction doesn't exist

00:41:53,450 --> 00:41:57,950
anymore we could we have cancellation

00:41:55,430 --> 00:41:59,630
routines that are adding new stages into

00:41:57,950 --> 00:42:02,060
the pipeline on having new tasks into

00:41:59,630 --> 00:42:04,610
this the existing stage and just running

00:42:02,060 --> 00:42:06,380
like regular stages using exactly the

00:42:04,610 --> 00:42:09,080
same message in semantics that anything

00:42:06,380 --> 00:42:10,370
else does we have a rolling push

00:42:09,080 --> 00:42:12,440
deployment strategy I don't know if

00:42:10,370 --> 00:42:16,370
anyone has ever used that but what that

00:42:12,440 --> 00:42:18,050
does is it's kind of hot deploys of

00:42:16,370 --> 00:42:20,360
server groups so instead of instead of

00:42:18,050 --> 00:42:21,860
creating a new server group and spinning

00:42:20,360 --> 00:42:23,480
up new instances and then disabling the

00:42:21,860 --> 00:42:24,860
old one what it does is it changes the

00:42:23,480 --> 00:42:27,860
launch configuration of the existing

00:42:24,860 --> 00:42:29,630
server group to use a new AMI and then

00:42:27,860 --> 00:42:32,420
it just one by one knocks out each of

00:42:29,630 --> 00:42:34,760
those instances in the server group and

00:42:32,420 --> 00:42:38,390
wait for them to restart my system to

00:42:34,760 --> 00:42:40,550
get restarted by Amazon this because we

00:42:38,390 --> 00:42:43,600
didn't know tell you how many instances

00:42:40,550 --> 00:42:45,380
we're gonna be there at runtime and

00:42:43,600 --> 00:42:47,810
obviously we didn't want to make any

00:42:45,380 --> 00:42:50,540
assumptions about that and also this

00:42:47,810 --> 00:42:52,940
gets used by some pretty complex apps

00:42:50,540 --> 00:42:54,950
that can have like really aggressive

00:42:52,940 --> 00:42:57,350
auto scaling policies switched on and

00:42:54,950 --> 00:43:00,370
can have really non-deterministic

00:42:57,350 --> 00:43:03,050
numbers of instances at any given time

00:43:00,370 --> 00:43:04,700
trying to plan that work like let's

00:43:03,050 --> 00:43:06,200
create one destroy instance tasks for

00:43:04,700 --> 00:43:08,150
each of those instances and then like

00:43:06,200 --> 00:43:10,280
half an hour later we'll hope that they

00:43:08,150 --> 00:43:12,080
actually got all of them is no more work

00:43:10,280 --> 00:43:13,970
so the way the wrong pro strategy worked

00:43:12,080 --> 00:43:15,800
before it would it would create a single

00:43:13,970 --> 00:43:17,990
set of tasks for destroying one instance

00:43:15,800 --> 00:43:19,460
of end of that it would figure out okay

00:43:17,990 --> 00:43:21,920
there any more instances I haven't done

00:43:19,460 --> 00:43:23,690
yet if so go right right background like

00:43:21,920 --> 00:43:25,040
erase the context of these tasks and

00:43:23,690 --> 00:43:27,320
pretend we're running them again from

00:43:25,040 --> 00:43:28,640
from scratch or the new instance ID and

00:43:27,320 --> 00:43:30,650
this was just because we couldn't you

00:43:28,640 --> 00:43:33,500
take the number of tasks that were in

00:43:30,650 --> 00:43:35,390
that stage given the way we had to map a

00:43:33,500 --> 00:43:36,890
pipeline on trusting that shell now we

00:43:35,390 --> 00:43:38,870
totally can do that we can have that

00:43:36,890 --> 00:43:40,640
stage laid track in front of itself and

00:43:38,870 --> 00:43:42,110
just create new tasks as it goes and

00:43:40,640 --> 00:43:46,520
that's something we'll address at some

00:43:42,110 --> 00:43:49,070
point I mentioned dynamite earlier which

00:43:46,520 --> 00:43:52,130
is Netflix is kind of multi-region

00:43:49,070 --> 00:43:55,670
wrapper around Redis so at some point we

00:43:52,130 --> 00:43:57,980
may want to have the queue migrated onto

00:43:55,670 --> 00:44:00,530
onto dynamite there is actually a

00:43:57,980 --> 00:44:02,630
library that's extremely similar in some

00:44:00,530 --> 00:44:03,720
of what it does to how we implemented

00:44:02,630 --> 00:44:06,390
the queue

00:44:03,720 --> 00:44:08,070
gov Dino coos um but it does seem to

00:44:06,390 --> 00:44:10,109
have some issues as far as I can tell

00:44:08,070 --> 00:44:11,130
with transaction transaction allottee

00:44:10,109 --> 00:44:13,260
and concurrency because one of the

00:44:11,130 --> 00:44:17,910
things you can't do in dynamite is have

00:44:13,260 --> 00:44:19,650
transaction operations that depend on in

00:44:17,910 --> 00:44:23,280
in certain circumstances I think Arizona

00:44:19,650 --> 00:44:24,510
multi key transaction operations yeah so

00:44:23,280 --> 00:44:25,290
there are some restrictions there that

00:44:24,510 --> 00:44:26,490
would make it a little bit more

00:44:25,290 --> 00:44:27,630
difficult to make those guarantees

00:44:26,490 --> 00:44:30,810
around exactly once

00:44:27,630 --> 00:44:32,310
hopefully exactly once delivery but at

00:44:30,810 --> 00:44:34,710
some point we might investigate that

00:44:32,310 --> 00:44:37,530
but because given that these messages

00:44:34,710 --> 00:44:39,090
are so ephemeral it may not be that big

00:44:37,530 --> 00:44:42,000
of a deal we definitely want to move the

00:44:39,090 --> 00:44:44,520
storage of pipelines you know the kind

00:44:42,000 --> 00:44:45,960
of execution that is in flight we want

00:44:44,520 --> 00:44:48,390
to move that to model where it's

00:44:45,960 --> 00:44:51,180
replicated across regions so we can do

00:44:48,390 --> 00:44:53,250
evacuations of a region in Orca but the

00:44:51,180 --> 00:44:57,390
the messages on the queue themselves may

00:44:53,250 --> 00:45:00,030
not be as as problematic there we could

00:44:57,390 --> 00:45:02,250
probably just take a dump and of the

00:45:00,030 --> 00:45:03,900
state of those tables and Renison just

00:45:02,250 --> 00:45:06,270
recreate them on a new thing in a couple

00:45:03,900 --> 00:45:08,550
of lines of code so it's it's less of a

00:45:06,270 --> 00:45:10,320
problem but there are some really

00:45:08,550 --> 00:45:12,720
exciting and interesting possibilities

00:45:10,320 --> 00:45:14,940
there's much less code involved now and

00:45:12,720 --> 00:45:19,410
it's much less complicated I think I

00:45:14,940 --> 00:45:21,330
said I wrote it but it's it's a lot less

00:45:19,410 --> 00:45:22,740
horrible to try and debug than it was

00:45:21,330 --> 00:45:25,080
with this stuff that was mapping stuff

00:45:22,740 --> 00:45:26,670
onto spring match and then we don't have

00:45:25,080 --> 00:45:30,660
all these integration tests just Cruden

00:45:26,670 --> 00:45:31,710
that we use in spring backs right yeah

00:45:30,660 --> 00:45:34,950
it's a lot more pleasant to work with

00:45:31,710 --> 00:45:37,670
and you know the future looks pretty

00:45:34,950 --> 00:45:40,619
bright for that kind of things we can do

00:45:37,670 --> 00:45:43,410
and with that I think if anyone has any

00:45:40,619 --> 00:45:45,810
questions it's not you can pay me in the

00:45:43,410 --> 00:45:52,349
slack channel on a handle they're

00:45:45,810 --> 00:45:53,910
horrible on Twitter yes do we have any

00:45:52,349 --> 00:45:59,430
concept forever time two little messages

00:45:53,910 --> 00:46:02,880
we do four we see I think the the lock

00:45:59,430 --> 00:46:04,890
that we set has a time to live on it we

00:46:02,880 --> 00:46:08,640
don't for the other types of messages

00:46:04,890 --> 00:46:10,680
because we presume something or to read

00:46:08,640 --> 00:46:12,089
them on someone and if it didn't we want

00:46:10,680 --> 00:46:14,330
to know we want to be able to find them

00:46:12,089 --> 00:46:16,849
and know about it so we could add that

00:46:14,330 --> 00:46:18,710
we done at the moment

00:46:16,849 --> 00:46:22,700
because Redis doesn't have any

00:46:18,710 --> 00:46:24,109
culminated TTL on sorted sets I don't

00:46:22,700 --> 00:46:26,029
believe because it's not individual keys

00:46:24,109 --> 00:46:27,519
it's like entries in a sorted set so we

00:46:26,029 --> 00:46:29,150
have to kind of roll our own

00:46:27,519 --> 00:46:30,410
implementation for that where it's the

00:46:29,150 --> 00:46:32,799
locking mechanism is done with

00:46:30,410 --> 00:46:35,150
individual keys so we can teach our lap

00:46:32,799 --> 00:46:37,309
yeah I think the reason we don't do it

00:46:35,150 --> 00:46:39,440
is because we would like to know oh

00:46:37,309 --> 00:46:41,619
there are only stale keys it should have

00:46:39,440 --> 00:46:45,729
been processed a month ago

00:46:41,619 --> 00:46:45,729

YouTube URL: https://www.youtube.com/watch?v=GCLtHjJqhOs


