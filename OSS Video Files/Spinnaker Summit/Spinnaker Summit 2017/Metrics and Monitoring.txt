Title: Metrics and Monitoring
Publication date: 2017-09-22
Playlist: Spinnaker Summit 2017
Description: 
	ERIC WISEBLATT, GOOGLE

This talk provides an overview of how Spinnaker deployments themselves are instrumented and monitored. It will discuss a developer perspective for adding additional  instrumentation into the Spinnaker codebase, adding support for a custom monitoring system, and consuming the metrics for operational monitoring.

From Spinnaker Summit 2017
Captions: 
	00:00:06,080 --> 00:00:13,349
all right let's uh let's get started i'm

00:00:09,990 --> 00:00:17,270
eric wise wife I work at Google and

00:00:13,349 --> 00:00:19,500
spinnaker team and along with Adam

00:00:17,270 --> 00:00:20,580
Jordans from Netflix we're going to be

00:00:19,500 --> 00:00:27,449
talking about

00:00:20,580 --> 00:00:29,580
my name's spinnaker and what I think

00:00:27,449 --> 00:00:31,650
this is most interesting they strip but

00:00:29,580 --> 00:00:34,770
think about this in three different ways

00:00:31,650 --> 00:00:37,110
so this this talk is is kind of a

00:00:34,770 --> 00:00:39,390
high-level overview about what's there

00:00:37,110 --> 00:00:42,329
and it's targeted towards three

00:00:39,390 --> 00:00:45,510
different types of people so the first

00:00:42,329 --> 00:00:48,629
part is about instrumenting spinnaker

00:00:45,510 --> 00:00:51,329
itself about the source code so if

00:00:48,629 --> 00:00:53,660
you're adding a new provider or new

00:00:51,329 --> 00:00:57,059
features and you want to be able to get

00:00:53,660 --> 00:00:58,770
in order to be able to monitor them what

00:00:57,059 --> 00:01:02,870
is it that you need to do in order to

00:00:58,770 --> 00:01:07,799
add those metrics in second part is

00:01:02,870 --> 00:01:09,570
targeted towards I am a the company

00:01:07,799 --> 00:01:11,549
running spinnaker I have my own

00:01:09,570 --> 00:01:14,220
monitoring system how do we use that

00:01:11,549 --> 00:01:17,070
monitoring system to monitor my

00:01:14,220 --> 00:01:22,380
spinnaker deployment so the second part

00:01:17,070 --> 00:01:24,630
talk about that in third part Adams

00:01:22,380 --> 00:01:27,030
going to talk about I'm you know an

00:01:24,630 --> 00:01:32,009
operator I want to keep my spinnaker up

00:01:27,030 --> 00:01:33,869
running how do I know the what's going

00:01:32,009 --> 00:01:36,500
on inside of it and are there any

00:01:33,869 --> 00:01:40,619
problems is it healthy and so forth and

00:01:36,500 --> 00:01:43,710
he'll go into how to interpret the

00:01:40,619 --> 00:01:50,100
informations there and show Atlas and

00:01:43,710 --> 00:01:51,990
Saburo world type of stuff so well this

00:01:50,100 --> 00:01:55,350
is it's a high-level overview and I

00:01:51,990 --> 00:01:57,869
thought it would be useful to keep this

00:01:55,350 --> 00:02:00,090
picture in your mind to put the pieces

00:01:57,869 --> 00:02:03,540
together I'm talking about so this is

00:02:00,090 --> 00:02:06,329
just a generic diagram of the different

00:02:03,540 --> 00:02:07,530
pieces gonna be talking about where you

00:02:06,329 --> 00:02:09,599
can spin the cars you know there's

00:02:07,530 --> 00:02:12,959
individual micro services and those who

00:02:09,599 --> 00:02:13,810
are depicted in blue here and then the

00:02:12,959 --> 00:02:17,080
yellow

00:02:13,810 --> 00:02:20,110
are the modules within spinnaker that

00:02:17,080 --> 00:02:24,099
you want to instrument so this is code

00:02:20,110 --> 00:02:25,330
that you're writing or in if you're at

00:02:24,099 --> 00:02:27,430
Cameron's talked yesterday about

00:02:25,330 --> 00:02:31,239
extending spinnaker and he had that

00:02:27,430 --> 00:02:33,720
extension and it added a counter that

00:02:31,239 --> 00:02:39,660
would be in one of these yellow circles

00:02:33,720 --> 00:02:43,840
and then the within the spinnaker

00:02:39,660 --> 00:02:45,790
libraries there's this common sub module

00:02:43,840 --> 00:02:49,060
that's used for monitoring and that's

00:02:45,790 --> 00:02:52,150
the spectator circle depicted in red and

00:02:49,060 --> 00:02:59,489
all those are inside of each row as

00:02:52,150 --> 00:03:01,690
microservice then the there's

00:02:59,489 --> 00:03:04,030
intermediary point which is which is

00:03:01,690 --> 00:03:06,010
optional but I'll talk about it that

00:03:04,030 --> 00:03:07,870
mediates between spinnaker and your

00:03:06,010 --> 00:03:09,330
monitoring system that knows how to pull

00:03:07,870 --> 00:03:11,860
the information from spinnaker

00:03:09,330 --> 00:03:15,220
reinterpret it and stick it inside of

00:03:11,860 --> 00:03:17,590
your monitoring system and finally the

00:03:15,220 --> 00:03:20,410
operators don't interact with spinnaker

00:03:17,590 --> 00:03:24,040
directly for monitoring they use the

00:03:20,410 --> 00:03:25,090
external monitoring system potentially

00:03:24,040 --> 00:03:27,760
the one you're already using where

00:03:25,090 --> 00:03:30,340
ideally the one that you're using to

00:03:27,760 --> 00:03:31,959
monitor the rest of your your fleet and

00:03:30,340 --> 00:03:37,660
and the applications that you're

00:03:31,959 --> 00:03:39,640
actually deploying so so now we talk

00:03:37,660 --> 00:03:44,049
going the first part which is about

00:03:39,640 --> 00:03:46,359
developers so that spectator part I

00:03:44,049 --> 00:03:48,730
talked about is a Netflix open source

00:03:46,359 --> 00:03:51,579
package it's not part of spinnaker it's

00:03:48,730 --> 00:03:55,299
part of that broader suite spinnaker

00:03:51,579 --> 00:03:57,040
uses that and a link there give you more

00:03:55,299 --> 00:03:58,389
information about it or that's you know

00:03:57,040 --> 00:04:00,489
if you want to look at the source code

00:03:58,389 --> 00:04:03,030
and so forth and what that does is it

00:04:00,489 --> 00:04:05,799
provides an abstract interface for

00:04:03,030 --> 00:04:08,049
instrumenting code so it's not tied to

00:04:05,799 --> 00:04:11,739
any particular monitoring system it's

00:04:08,049 --> 00:04:13,660
just an interface and then along with

00:04:11,739 --> 00:04:16,840
that they have concrete implementations

00:04:13,660 --> 00:04:18,940
like for Atlas or in the open source

00:04:16,840 --> 00:04:21,130
version there's metrics tree and and

00:04:18,940 --> 00:04:23,710
other some other things and then the

00:04:21,130 --> 00:04:25,780
Java runtimes which are based on spring

00:04:23,710 --> 00:04:27,180
to use dependency injection to add a

00:04:25,780 --> 00:04:31,150
concrete

00:04:27,180 --> 00:04:34,060
implementation of the particular service

00:04:31,150 --> 00:04:39,370
that one use into into spinnaker so that

00:04:34,060 --> 00:04:40,570
that's the the basic at a low level

00:04:39,370 --> 00:04:45,910
what's going on

00:04:40,570 --> 00:04:50,230
so within spectator there's certain

00:04:45,910 --> 00:04:53,190
terms that you'll find within the API so

00:04:50,230 --> 00:04:56,140
first is is a meter assist which is

00:04:53,190 --> 00:04:58,210
inspect area they call it a device and

00:04:56,140 --> 00:05:03,340
meter they're different types of meters

00:04:58,210 --> 00:05:05,260
you have gauges which measure the value

00:05:03,340 --> 00:05:07,060
at a particular point in time like how

00:05:05,260 --> 00:05:09,490
much memory use it and where the size of

00:05:07,060 --> 00:05:12,130
a Q or something like that your counters

00:05:09,490 --> 00:05:16,540
which are monotonically increasing

00:05:12,130 --> 00:05:17,830
values you know every time you go

00:05:16,540 --> 00:05:19,780
through a function you might increment

00:05:17,830 --> 00:05:22,930
the counter you know by one or if you

00:05:19,780 --> 00:05:26,890
measuring bandwidth or something might

00:05:22,930 --> 00:05:31,030
increment it by larger you know multiple

00:05:26,890 --> 00:05:33,610
values and then also distributions

00:05:31,030 --> 00:05:36,930
timers which is type a counter for

00:05:33,610 --> 00:05:39,280
measuring time and that has some API

00:05:36,930 --> 00:05:42,940
things on it to make it convenient to

00:05:39,280 --> 00:05:45,940
measure how long it takes to run a block

00:05:42,940 --> 00:05:52,690
of code and distribution summaries which

00:05:45,940 --> 00:05:57,010
I won't go into we don't we won't won't

00:05:52,690 --> 00:05:59,320
go into there in spinnaker but we don't

00:05:57,010 --> 00:06:02,170
expose them out at least not yet because

00:05:59,320 --> 00:06:07,600
they adds complexity and didn't seem to

00:06:02,170 --> 00:06:09,220
be that important or that useful which I

00:06:07,600 --> 00:06:12,760
could be wrong we can it we can add them

00:06:09,220 --> 00:06:17,830
in and and so meters have unique ID

00:06:12,760 --> 00:06:21,340
where an ID is a name and collection of

00:06:17,830 --> 00:06:24,000
tag value bindings so what that means is

00:06:21,340 --> 00:06:26,290
that you know multiple meters or

00:06:24,000 --> 00:06:28,510
counters that have the same name but

00:06:26,290 --> 00:06:31,240
they're different because they have

00:06:28,510 --> 00:06:33,880
these different context bindings about

00:06:31,240 --> 00:06:36,490
what they're for and they'll be

00:06:33,880 --> 00:06:38,380
clarified a little later and their

00:06:36,490 --> 00:06:39,740
measurements which is just a value at a

00:06:38,380 --> 00:06:41,210
given point in time

00:06:39,740 --> 00:06:45,530
and registry which is a collection of

00:06:41,210 --> 00:06:47,419
meters and it's the registry that gets

00:06:45,530 --> 00:06:49,610
injected that gives you a concrete

00:06:47,419 --> 00:06:52,550
implementation so if you're using Atlas

00:06:49,610 --> 00:06:54,500
you inject the Atlas registry and then

00:06:52,550 --> 00:06:56,720
all the code and spinnaker that is

00:06:54,500 --> 00:06:58,190
creating meters crates from the registry

00:06:56,720 --> 00:07:05,270
and that's how the dependency injection

00:06:58,190 --> 00:07:09,229
goes in so forth this isn't an example

00:07:05,270 --> 00:07:14,470
within spinnaker there's an interceptor

00:07:09,229 --> 00:07:18,440
that captures every time you you hit an

00:07:14,470 --> 00:07:20,949
HTTP endpoint there's some very variant

00:07:18,440 --> 00:07:30,740
of this this is how it's actually

00:07:20,949 --> 00:07:32,539
monitored so the create ID is is looking

00:07:30,740 --> 00:07:35,990
up an ID so what I didn't mention before

00:07:32,539 --> 00:07:38,270
is that the IDs are essentially single

00:07:35,990 --> 00:07:41,539
things so that two IDs with the same

00:07:38,270 --> 00:07:45,320
name and bindings you know tag value

00:07:41,539 --> 00:07:47,300
bindings are are equivalent and will

00:07:45,320 --> 00:07:49,460
refer to the same neither you won't have

00:07:47,300 --> 00:07:56,530
two meters that have different counts

00:07:49,460 --> 00:07:59,889
but the same ID so so certain tags there

00:07:56,530 --> 00:08:02,900
in this case the controller would be

00:07:59,889 --> 00:08:05,349
like the type of resources that the API

00:08:02,900 --> 00:08:09,199
is for the particular method you called

00:08:05,349 --> 00:08:13,159
this HTTP status code the result and the

00:08:09,199 --> 00:08:15,650
status is you know four hundred two

00:08:13,159 --> 00:08:19,849
hundred series type of type of thing

00:08:15,650 --> 00:08:22,069
and then finally the recording of the

00:08:19,849 --> 00:08:25,909
time this is basically like incrementing

00:08:22,069 --> 00:08:29,180
the time within units within spinnaker

00:08:25,909 --> 00:08:30,889
everything is in nanoseconds but there

00:08:29,180 --> 00:08:35,839
are a few things few things to note here

00:08:30,889 --> 00:08:38,149
is that every different method you call

00:08:35,839 --> 00:08:40,640
or even for the same method a different

00:08:38,149 --> 00:08:44,380
status code so the counter for you know

00:08:40,640 --> 00:08:49,520
meter for 200 and 400 fours are actually

00:08:44,380 --> 00:08:53,050
different there are different metrics as

00:08:49,520 --> 00:08:53,050
social class I'll show

00:08:53,440 --> 00:09:03,020
so this is this is an example the point

00:08:58,520 --> 00:09:08,180
of this slide is to show this was

00:09:03,020 --> 00:09:10,100
captured from class after a deployment

00:09:08,180 --> 00:09:14,209
after I ran some of the some of the

00:09:10,100 --> 00:09:18,200
integration tests about some of the the

00:09:14,209 --> 00:09:21,200
actual like label the tag value bindings

00:09:18,200 --> 00:09:25,040
that that are in there just to give an

00:09:21,200 --> 00:09:28,670
idea of what what you might see or what

00:09:25,040 --> 00:09:35,149
might be available and there are few

00:09:28,670 --> 00:09:36,620
things here to note also is that some of

00:09:35,149 --> 00:09:39,380
these some of these labels even for

00:09:36,620 --> 00:09:41,630
controller implications have specific

00:09:39,380 --> 00:09:45,500
tags for the particular microservice

00:09:41,630 --> 00:09:50,779
therefore so within cloud driver there's

00:09:45,500 --> 00:09:53,510
a tag so when when a message comes in

00:09:50,779 --> 00:09:56,360
for a particular operation on an account

00:09:53,510 --> 00:09:58,610
that's in a separate being a separate

00:09:56,360 --> 00:10:03,709
kind of label and what that lets you do

00:09:58,610 --> 00:10:05,180
is refine or isolate what it what it is

00:10:03,709 --> 00:10:07,339
you're looking at like if you want to

00:10:05,180 --> 00:10:10,310
look at metrics for a particular account

00:10:07,339 --> 00:10:12,500
you can narrow that down without having

00:10:10,310 --> 00:10:16,190
to look at cloud driver as a whole you

00:10:12,500 --> 00:10:19,010
know so or particular region and so

00:10:16,190 --> 00:10:21,470
forth but also being in the same meter

00:10:19,010 --> 00:10:23,690
you can aggregate across so if you don't

00:10:21,470 --> 00:10:25,310
care which account is for and you just

00:10:23,690 --> 00:10:26,990
want to look at a particular region or

00:10:25,310 --> 00:10:30,560
an account in a particular region you

00:10:26,990 --> 00:10:33,260
have that flexibility to within that the

00:10:30,560 --> 00:10:35,420
data model that that's within spinnaker

00:10:33,260 --> 00:10:38,120
you have the flexibility to slice and

00:10:35,420 --> 00:10:43,970
dice the data to get at what it is that

00:10:38,120 --> 00:10:46,730
you're looking for if if you want but

00:10:43,970 --> 00:10:48,829
the downside of that is that the number

00:10:46,730 --> 00:10:50,810
of meters you have is potentially the

00:10:48,829 --> 00:10:54,440
cross product of the cardinalities of

00:10:50,810 --> 00:10:56,450
all the labels which is law so in

00:10:54,440 --> 00:11:01,880
practice it's not quite that bad because

00:10:56,450 --> 00:11:05,360
not all independent so but I don't have

00:11:01,880 --> 00:11:06,960
real numbers offhand but in my mind I

00:11:05,360 --> 00:11:09,540
wanted to say

00:11:06,960 --> 00:11:13,770
in that example I show this is from they

00:11:09,540 --> 00:11:16,410
were like 94 different meters but 4000

00:11:13,770 --> 00:11:19,800
different data points forever at any

00:11:16,410 --> 00:11:27,779
given point in time so there's a lot of

00:11:19,800 --> 00:11:34,860
stuff there in the high dimensions of

00:11:27,779 --> 00:11:41,029
the data so that that's that's spectator

00:11:34,860 --> 00:11:43,529
and within spinnaker within the core

00:11:41,029 --> 00:11:45,570
Netflix says all this and put all this

00:11:43,529 --> 00:11:48,540
instrumentation in there and then they

00:11:45,570 --> 00:11:51,779
inject Atlas directly into spectator and

00:11:48,540 --> 00:11:54,620
push their data into Atlas but for the

00:11:51,779 --> 00:11:57,450
rest of us in the open source community

00:11:54,620 --> 00:11:59,160
that won't work for us because you know

00:11:57,450 --> 00:12:05,630
he's Atlas and we want to use our own

00:11:59,160 --> 00:12:11,040
own systems so what we did is we added a

00:12:05,630 --> 00:12:16,320
an HTTP endpoint that gets injected as

00:12:11,040 --> 00:12:22,200
the as the registry you're using and

00:12:16,320 --> 00:12:23,850
then it allows an external system to

00:12:22,200 --> 00:12:29,300
pull it and what that means is that

00:12:23,850 --> 00:12:33,270
people can use off-the-shelf builds but

00:12:29,300 --> 00:12:36,959
use custom monitoring systems for them

00:12:33,270 --> 00:12:40,170
so so you can add or use your own system

00:12:36,959 --> 00:12:43,560
without having to create your own bill

00:12:40,170 --> 00:12:46,800
to all the all the micro services and

00:12:43,560 --> 00:12:48,650
then there's the daemon I showed you

00:12:46,800 --> 00:12:51,420
we'll talk about a little bit more

00:12:48,650 --> 00:12:53,720
mediates between the services so it

00:12:51,420 --> 00:12:56,370
knows how to scrape and interpret the

00:12:53,720 --> 00:12:59,700
Jason that's returned from that HTTP

00:12:56,370 --> 00:13:04,470
endpoint and translated it and push it

00:12:59,700 --> 00:13:05,959
into the external system and so there

00:13:04,470 --> 00:13:09,690
were some batteries included in this

00:13:05,959 --> 00:13:13,500
that we provide support for for data dog

00:13:09,690 --> 00:13:18,200
Prometheus and stack driver as examples

00:13:13,500 --> 00:13:20,790
but it's built in a way to make it

00:13:18,200 --> 00:13:23,940
hopefully straightforward to modify

00:13:20,790 --> 00:13:25,830
to add your own system if you have your

00:13:23,940 --> 00:13:28,650
own internal system or some other system

00:13:25,830 --> 00:13:31,140
that's not supported and we encourage

00:13:28,650 --> 00:13:39,660
people to do that and contribute back to

00:13:31,140 --> 00:13:42,990
the to the community so the that 80 to

00:13:39,660 --> 00:13:45,120
the end point I returned this is like

00:13:42,990 --> 00:13:48,500
what what a response looks like coming

00:13:45,120 --> 00:13:50,520
back from the server so for that

00:13:48,500 --> 00:13:55,350
interceptor that I showed you before

00:13:50,520 --> 00:13:58,080
these are this is what that might look

00:13:55,350 --> 00:14:00,180
like we're in red or the different keys

00:13:58,080 --> 00:14:05,450
that were there and blue with particular

00:14:00,180 --> 00:14:08,460
values so yeah basically some at both

00:14:05,450 --> 00:14:10,860
the meters with the different

00:14:08,460 --> 00:14:12,930
measurements inside the measurement is

00:14:10,860 --> 00:14:18,090
just the latest latest one and the time

00:14:12,930 --> 00:14:20,340
that that was taken and within Spectator

00:14:18,090 --> 00:14:28,920
the values are always doubles so they're

00:14:20,340 --> 00:14:36,030
no strings or doubles so that that's an

00:14:28,920 --> 00:14:39,680
example of that the the demon has a

00:14:36,030 --> 00:14:43,890
different okay so by gaming I mean the

00:14:39,680 --> 00:14:49,170
external process we talked about before

00:14:43,890 --> 00:14:50,970
so that has that was designed the data

00:14:49,170 --> 00:14:56,580
Mountain there was designed to make it

00:14:50,970 --> 00:14:58,560
more compatible with a wider range of

00:14:56,580 --> 00:15:02,510
back-end monitoring systems so it would

00:14:58,560 --> 00:15:06,660
be easier to to do the integration and

00:15:02,510 --> 00:15:08,700
well though this is how its defined it's

00:15:06,660 --> 00:15:13,650
all encapsulated inside so if you're

00:15:08,700 --> 00:15:15,810
writing your own integration and you

00:15:13,650 --> 00:15:17,370
wish it were different you can raise

00:15:15,810 --> 00:15:20,420
that we're we're you know try to

00:15:17,370 --> 00:15:23,760
refactor it and make it easier to do

00:15:20,420 --> 00:15:25,770
having said that the some of the

00:15:23,760 --> 00:15:29,220
integrations were trivial to do and

00:15:25,770 --> 00:15:33,720
they're all plugged in as if somebody on

00:15:29,220 --> 00:15:36,870
the outside who's doing it on so anyway

00:15:33,720 --> 00:15:39,960
Metrix again counter gage timer doesn't

00:15:36,870 --> 00:15:44,040
have the distribution in there at this

00:15:39,960 --> 00:15:45,960
time and then measurements which are the

00:15:44,040 --> 00:15:50,100
tagged values so the tags are on the

00:15:45,960 --> 00:15:52,530
measurements not on the metrics aren't

00:15:50,100 --> 00:15:59,400
not on the yeah not on the or what would

00:15:52,530 --> 00:16:01,080
be the meters in general they're all

00:15:59,400 --> 00:16:04,800
pushed straight through but there is one

00:16:01,080 --> 00:16:08,210
one timers are very big in spinnaker but

00:16:04,800 --> 00:16:10,530
they have this what I think is weird

00:16:08,210 --> 00:16:13,170
property to them which is one of the

00:16:10,530 --> 00:16:16,260
tags is called statistic and that has a

00:16:13,170 --> 00:16:19,200
value of either account or total time or

00:16:16,260 --> 00:16:21,810
count as a number of time to incremented

00:16:19,200 --> 00:16:25,680
it and the total time is the sum of all

00:16:21,810 --> 00:16:27,900
the values but the problem with this is

00:16:25,680 --> 00:16:29,160
that you can't aggregate over them

00:16:27,900 --> 00:16:31,830
because they're different units and mean

00:16:29,160 --> 00:16:34,560
different things and and it's kind of

00:16:31,830 --> 00:16:39,330
with their different types and stuff so

00:16:34,560 --> 00:16:42,210
what we did is transform those timer

00:16:39,330 --> 00:16:46,500
metrics into two separate metrics

00:16:42,210 --> 00:16:48,270
without the statistic tab but decorating

00:16:46,500 --> 00:16:51,210
the name as to whether it's a count or a

00:16:48,270 --> 00:16:54,300
total time other than that all the other

00:16:51,210 --> 00:16:59,810
meters that flow through or exactly as

00:16:54,300 --> 00:17:04,650
they they they were in spinnaker and so

00:16:59,810 --> 00:17:06,420
with that Jason responds I showed what

00:17:04,650 --> 00:17:09,150
would come out of the daemon for

00:17:06,420 --> 00:17:11,520
Prometheus is is in the protocol this is

00:17:09,150 --> 00:17:13,319
prometheus protocol and the point of

00:17:11,520 --> 00:17:15,240
this slide is to just show the same

00:17:13,319 --> 00:17:17,069
informations in there but with a

00:17:15,240 --> 00:17:24,120
different structure that's expected by

00:17:17,069 --> 00:17:26,730
the backend system and this is the demon

00:17:24,120 --> 00:17:30,510
has algorithms and heuristics in there

00:17:26,730 --> 00:17:32,430
so that it helps transform the code

00:17:30,510 --> 00:17:34,260
basically a visitor pattern that would

00:17:32,430 --> 00:17:36,150
each metric it calls you and say you

00:17:34,260 --> 00:17:42,140
know write this into your into your

00:17:36,150 --> 00:17:42,140
system and

00:17:45,120 --> 00:17:51,549
demon has it's a little different from

00:17:49,299 --> 00:17:55,299
the other spinnaker packages it's it's

00:17:51,549 --> 00:17:59,020
Python but it can be run as either

00:17:55,299 --> 00:18:01,950
server for command-line usually running

00:17:59,020 --> 00:18:08,049
as a server to monitor your deployments

00:18:01,950 --> 00:18:11,230
but if you are adding a new honoring

00:18:08,049 --> 00:18:13,870
system that you know there

00:18:11,230 --> 00:18:17,289
demon is a convenient place to put some

00:18:13,870 --> 00:18:19,630
Administrative Tools into it to refresh

00:18:17,289 --> 00:18:23,650
your dashboards and things like that and

00:18:19,630 --> 00:18:25,860
so because you already have other code

00:18:23,650 --> 00:18:29,740
and integration you need the demon so

00:18:25,860 --> 00:18:32,200
the way when you look at the code its

00:18:29,740 --> 00:18:36,640
structure to support different types of

00:18:32,200 --> 00:18:38,559
use cases and the other thing which I

00:18:36,640 --> 00:18:40,059
encourage you to look at is that has a

00:18:38,559 --> 00:18:45,750
web server on it even when you're

00:18:40,059 --> 00:18:48,850
monitoring on port 8080 that has some

00:18:45,750 --> 00:18:50,380
little some some tools on it that given

00:18:48,850 --> 00:18:54,070
the overview about what metrics are

00:18:50,380 --> 00:18:56,890
there so in a forum this morning

00:18:54,070 --> 00:19:00,549
somebody had mentioned took him a while

00:18:56,890 --> 00:19:03,730
to figure out how spinnaker worked and

00:19:00,549 --> 00:19:06,549
what was in there and so forth that the

00:19:03,730 --> 00:19:09,940
survey within the demon there's some

00:19:06,549 --> 00:19:11,799
some tools that were helpful for me and

00:19:09,940 --> 00:19:14,190
trying to understand what was there and

00:19:11,799 --> 00:19:17,860
so I open sourced them into the daemon

00:19:14,190 --> 00:19:20,860
to give an overview about what metrics

00:19:17,860 --> 00:19:25,750
each provider is exporting or exposing

00:19:20,860 --> 00:19:27,179
and what labels and stuff are in there

00:19:25,750 --> 00:19:29,559
so it's not meant for operation

00:19:27,179 --> 00:19:34,110
operational use but for developers to

00:19:29,559 --> 00:19:37,120
understand what's going on and then

00:19:34,110 --> 00:19:39,730
finally for the game it's pretty simple

00:19:37,120 --> 00:19:47,650
to configure this is actually Guard's

00:19:39,730 --> 00:19:49,480
idea of having each you have a it's

00:19:47,650 --> 00:19:51,970
basically a mo file that tells the

00:19:49,480 --> 00:19:54,950
daemon where to where to pull from and

00:19:51,970 --> 00:19:58,519
then each sir

00:19:54,950 --> 00:20:00,980
or there's a file for each place to pull

00:19:58,519 --> 00:20:03,080
so that when you're deploying the daemon

00:20:00,980 --> 00:20:06,080
typically what we do is we deploy it

00:20:03,080 --> 00:20:09,080
we co-located with the with the

00:20:06,080 --> 00:20:11,629
instances that are being monitored but

00:20:09,080 --> 00:20:13,940
it could it could really you could have

00:20:11,629 --> 00:20:17,409
one daemon monitor several things if you

00:20:13,940 --> 00:20:19,999
wanted you wanted to but co-locating it

00:20:17,409 --> 00:20:21,559
makes discovery you don't have to worry

00:20:19,999 --> 00:20:26,629
about discovery and going through load

00:20:21,559 --> 00:20:29,570
balancers and stuff so it's it's it's

00:20:26,629 --> 00:20:35,539
pretty straightforward so before I pets

00:20:29,570 --> 00:20:40,190
over to Adam we'll talk about be the

00:20:35,539 --> 00:20:42,289
actual metrics within spinnaker just

00:20:40,190 --> 00:20:46,340
want to say like if if you're interested

00:20:42,289 --> 00:20:49,669
in in adding support for a new system

00:20:46,340 --> 00:20:51,799
just you know ping me on slack or and

00:20:49,669 --> 00:20:57,619
I'm happy to help you try to do that

00:20:51,799 --> 00:21:00,679
integration and then finally there's

00:20:57,619 --> 00:21:02,629
some even if you're writing your own I

00:21:00,679 --> 00:21:06,940
would encourage you to use like the

00:21:02,629 --> 00:21:09,830
Prometheus provider the Prometheus

00:21:06,940 --> 00:21:12,830
metrics in there just to see what's

00:21:09,830 --> 00:21:15,440
there and to help understand when you do

00:21:12,830 --> 00:21:18,320
that integration of your own proprietary

00:21:15,440 --> 00:21:20,869
system or unsupported system to get an

00:21:18,320 --> 00:21:23,659
idea of how things work and it should

00:21:20,869 --> 00:21:26,029
help you get started so with that I'm

00:21:23,659 --> 00:21:30,489
gonna turn over to Adam and at the end

00:21:26,029 --> 00:21:30,489
we'll take questions thank you

00:21:30,759 --> 00:21:37,999
alright so I'm Adam gave a talk

00:21:33,470 --> 00:21:40,190
yesterday on on spinnaker today I'm

00:21:37,999 --> 00:21:42,109
gonna talk about how we operating it

00:21:40,190 --> 00:21:45,139
here at Netflix it's been in production

00:21:42,109 --> 00:21:47,480
for about three years and I'm just one

00:21:45,139 --> 00:21:49,309
of well now we've got a dozen people

00:21:47,480 --> 00:21:51,980
that are on the delivery team that are

00:21:49,309 --> 00:21:54,399
charged with both supporting spinnaker

00:21:51,980 --> 00:21:57,820
as well as extending it for all the

00:21:54,399 --> 00:22:00,440
existing and new use cases come about

00:21:57,820 --> 00:22:02,629
from our perspective we have a lot of

00:22:00,440 --> 00:22:04,970
opinions around this but we didn't force

00:22:02,629 --> 00:22:07,279
them on the community we didn't include

00:22:04,970 --> 00:22:08,630
batteries for a lot of things that I

00:22:07,279 --> 00:22:11,050
would consider important

00:22:08,630 --> 00:22:15,130
they're running spinnaker and production

00:22:11,050 --> 00:22:18,170
stuff like logging dashboarding alerting

00:22:15,130 --> 00:22:19,070
for logging there's a few things out in

00:22:18,170 --> 00:22:21,170
the open source community

00:22:19,070 --> 00:22:23,690
we don't actually right now have a great

00:22:21,170 --> 00:22:26,060
solution here at Netflix so we're just

00:22:23,690 --> 00:22:30,110
using an elk stock I would much prefer

00:22:26,060 --> 00:22:32,680
to use Splunk or sumo they're just

00:22:30,110 --> 00:22:35,900
tremendously expensive and we haven't

00:22:32,680 --> 00:22:37,070
bitten that bullet yet but the one thing

00:22:35,900 --> 00:22:39,800
we're missing and at least our

00:22:37,070 --> 00:22:43,790
configuration of elk is the ability to

00:22:39,800 --> 00:22:46,180
alert off of logs in my previous life

00:22:43,790 --> 00:22:48,440
that was actually a super useful thing

00:22:46,180 --> 00:22:50,390
because you can't always have the

00:22:48,440 --> 00:22:52,190
metrics for everything that you want to

00:22:50,390 --> 00:22:56,000
live off of sometimes you just want to

00:22:52,190 --> 00:22:57,680
match log patterns dashboarding if

00:22:56,000 --> 00:22:59,630
there's time at the end I will take a

00:22:57,680 --> 00:23:01,010
look up from with you but I'm gonna walk

00:22:59,630 --> 00:23:03,680
through what we've got currently for

00:23:01,010 --> 00:23:07,040
Atlas and try and show just a cursory

00:23:03,680 --> 00:23:10,240
review of the dashboards we've got set

00:23:07,040 --> 00:23:13,820
up and kind of when they come into play

00:23:10,240 --> 00:23:16,010
which leads me to alerting sure we've

00:23:13,820 --> 00:23:18,140
got dashboards that show the state of

00:23:16,010 --> 00:23:20,930
spinnaker but we're not constantly

00:23:18,140 --> 00:23:23,540
looking at them as an organization here

00:23:20,930 --> 00:23:25,700
we want alerts to be the thing that

00:23:23,540 --> 00:23:28,460
tells us something's up and then we can

00:23:25,700 --> 00:23:30,290
go to a dashboard and have it maybe help

00:23:28,460 --> 00:23:32,090
provide some clarity on what might be

00:23:30,290 --> 00:23:34,180
going wrong in the system we don't want

00:23:32,090 --> 00:23:36,830
to have to be looking at a dashboard and

00:23:34,180 --> 00:23:40,900
recognize a signal and then and then

00:23:36,830 --> 00:23:44,120
cause and have that caused some reaction

00:23:40,900 --> 00:23:47,960
who here has a spinnaker in production

00:23:44,120 --> 00:23:49,490
with actual users and stuff how many

00:23:47,960 --> 00:23:52,570
times do you hear from those users

00:23:49,490 --> 00:23:54,950
that's something is up with spinnaker

00:23:52,570 --> 00:23:56,930
we're kind of the same way there's a

00:23:54,950 --> 00:24:01,910
part of our there's a little bit later

00:23:56,930 --> 00:24:03,590
on about our on-call rotation but we're

00:24:01,910 --> 00:24:05,450
all we're all on call we rotate on call

00:24:03,590 --> 00:24:08,120
throughout the team we wanted her a

00:24:05,450 --> 00:24:10,730
spinnaker Channel and that's kind of a

00:24:08,120 --> 00:24:12,920
secondary alert for us oftentimes can be

00:24:10,730 --> 00:24:14,570
a private work because they know we've

00:24:12,920 --> 00:24:17,480
got so many people dependent on

00:24:14,570 --> 00:24:19,640
spinnaker both from the UI perspective

00:24:17,480 --> 00:24:22,190
or API driven they're gonna know almost

00:24:19,640 --> 00:24:24,860
instantaneously if something is up

00:24:22,190 --> 00:24:27,200
normal verses it might take a little bit

00:24:24,860 --> 00:24:28,940
of time for our metrics to get bad

00:24:27,200 --> 00:24:31,789
enough that it's gonna flip a rolling

00:24:28,940 --> 00:24:34,730
window so that is a that is a signal

00:24:31,789 --> 00:24:36,860
that we that we take and perhaps our

00:24:34,730 --> 00:24:41,750
most powerful signal at least during the

00:24:36,860 --> 00:24:43,250
working hours a few quick tips here make

00:24:41,750 --> 00:24:45,169
sure your - birds are programmatically

00:24:43,250 --> 00:24:47,509
generated I don't know if everything

00:24:45,169 --> 00:24:49,730
works that way but at least with us in

00:24:47,509 --> 00:24:51,789
pantless we've codified all of our

00:24:49,730 --> 00:24:55,610
dashboards it makes it really easy to

00:24:51,789 --> 00:24:58,250
adapt for us anybody who was in my talk

00:24:55,610 --> 00:24:59,960
yesterday I mentioned how we shard cloud

00:24:58,250 --> 00:25:02,750
driver and fire up new cloud driver

00:24:59,960 --> 00:25:05,090
instances or clusters it's much easier

00:25:02,750 --> 00:25:06,409
to have your dashboards pick those up if

00:25:05,090 --> 00:25:08,179
your dashboards are all programmatically

00:25:06,409 --> 00:25:10,070
generated you really shouldn't have to

00:25:08,179 --> 00:25:12,350
go and do much when you're kind of

00:25:10,070 --> 00:25:14,720
altering your infrastructure and that's

00:25:12,350 --> 00:25:16,309
the state that we're at now same with

00:25:14,720 --> 00:25:18,919
alerts we didn't always have alerts

00:25:16,309 --> 00:25:21,080
that's a pretty recent thing we didn't

00:25:18,919 --> 00:25:23,059
always have an SRE or a person on our

00:25:21,080 --> 00:25:24,830
team with an SRE background that's a

00:25:23,059 --> 00:25:26,659
very recent thing so both these two

00:25:24,830 --> 00:25:31,129
things happened at more or less the same

00:25:26,659 --> 00:25:32,419
time shout-out to Asher thanks if

00:25:31,129 --> 00:25:33,350
anybody's interested when these slides

00:25:32,419 --> 00:25:35,389
go public

00:25:33,350 --> 00:25:37,309
we've got logs - a log stash agent

00:25:35,389 --> 00:25:39,320
running on one of our spinnaker

00:25:37,309 --> 00:25:41,629
instances you can kind of take a look at

00:25:39,320 --> 00:25:44,600
what that is very simple probably almost

00:25:41,629 --> 00:25:46,850
what you get out of the box the one

00:25:44,600 --> 00:25:49,190
thing we are looking at internally the

00:25:46,850 --> 00:25:52,190
more structured approach to logs right

00:25:49,190 --> 00:25:54,049
now we just have your typical basically

00:25:52,190 --> 00:25:56,659
console output with just strings and we

00:25:54,049 --> 00:25:58,460
send that off the logs or we send that

00:25:56,659 --> 00:26:00,740
out to elasticsearch and we kind of

00:25:58,460 --> 00:26:03,889
search that you can find a lot of stuff

00:26:00,740 --> 00:26:05,960
but you could do better if you actually

00:26:03,889 --> 00:26:09,440
were dealing with something more

00:26:05,960 --> 00:26:11,929
structured so we're looking at JSON

00:26:09,440 --> 00:26:15,919
appenders and being able to actually put

00:26:11,929 --> 00:26:18,350
more useful bits into those log outputs

00:26:15,919 --> 00:26:20,840
that we can send to elastic search index

00:26:18,350 --> 00:26:24,350
properly and search things like

00:26:20,840 --> 00:26:26,419
execution IDs in Orca getting attached

00:26:24,350 --> 00:26:28,879
to every log statement so you can more

00:26:26,419 --> 00:26:31,399
easily attribute what's happening when

00:26:28,879 --> 00:26:33,710
you're running a pipeline or a task and

00:26:31,399 --> 00:26:36,080
see everything that was happening we'll

00:26:33,710 --> 00:26:38,029
be passing those X those IDs down to

00:26:36,080 --> 00:26:40,220
driver so you could actually see what's

00:26:38,029 --> 00:26:42,649
happening in cloud driver in response to

00:26:40,220 --> 00:26:44,809
actions in Orca this sounds like Zipkin

00:26:42,649 --> 00:26:46,429
and all that kind of stuff so I'm not

00:26:44,809 --> 00:26:47,960
quite sure where we'll land but we've

00:26:46,429 --> 00:26:51,200
got some existing patterns that we'll

00:26:47,960 --> 00:26:53,390
probably just do it the traditional way

00:26:51,200 --> 00:26:56,289
similar to what we're doing when we pass

00:26:53,390 --> 00:26:58,730
authentication details down through gate

00:26:56,289 --> 00:27:01,549
so I would expect some movement on that

00:26:58,730 --> 00:27:04,899
in the next couple of weeks I know

00:27:01,549 --> 00:27:07,370
there's been some experimentation

00:27:04,899 --> 00:27:08,240
on-call rotation absolutely resounding

00:27:07,370 --> 00:27:11,330
yes

00:27:08,240 --> 00:27:13,399
don't voiced the management of your

00:27:11,330 --> 00:27:16,820
spinnaker instance on one single core

00:27:13,399 --> 00:27:19,419
soul it's terrible and it's not what we

00:27:16,820 --> 00:27:23,720
did here except in the early days when

00:27:19,419 --> 00:27:25,970
Andy was our first responder second

00:27:23,720 --> 00:27:28,970
quick tip don't have your manager be

00:27:25,970 --> 00:27:30,230
your first responder for your system

00:27:28,970 --> 00:27:32,990
because he's in meetings all the time

00:27:30,230 --> 00:27:34,970
and stuff so we have on-call rotation we

00:27:32,990 --> 00:27:39,139
started out doing it weekly then we

00:27:34,970 --> 00:27:41,269
transitioned to three and four days so

00:27:39,139 --> 00:27:42,710
three days during kind of a seedy part

00:27:41,269 --> 00:27:44,570
of the week and then a four days

00:27:42,710 --> 00:27:46,490
spanning a weekend

00:27:44,570 --> 00:27:49,880
we're considering moving this to daily

00:27:46,490 --> 00:27:51,799
just to kind of keep things moving it

00:27:49,880 --> 00:27:57,730
and and don't feel like you're you're

00:27:51,799 --> 00:27:57,730
constantly having to respond to people

00:27:58,630 --> 00:28:05,480
just a quick take on on rate-limiting

00:28:03,080 --> 00:28:08,210
it's a powerful tool that we've been

00:28:05,480 --> 00:28:10,789
able to use here because as I mentioned

00:28:08,210 --> 00:28:13,760
in yesterday's talk we have a wide

00:28:10,789 --> 00:28:16,639
variety of users sending wide variety of

00:28:13,760 --> 00:28:20,240
loads it useful to have some knobs that

00:28:16,639 --> 00:28:23,809
you can dial to effect to basically

00:28:20,240 --> 00:28:26,090
smooth out some of the requests a lot of

00:28:23,809 --> 00:28:28,100
this is docked on spinnaker i/o this is

00:28:26,090 --> 00:28:31,070
just kind of the the shortcut so we have

00:28:28,100 --> 00:28:34,549
the ability both inside of Orca and gate

00:28:31,070 --> 00:28:36,980
two in Orca we call it traffic shaping

00:28:34,549 --> 00:28:39,529
but basically rate limit the number of

00:28:36,980 --> 00:28:42,860
messages that can happen as part of a

00:28:39,529 --> 00:28:45,049
pipeline eventually this will kind of

00:28:42,860 --> 00:28:49,010
evolve in the sense of us being able to

00:28:45,049 --> 00:28:49,730
prioritize entire execution so we can

00:28:49,010 --> 00:28:51,740
prioritize

00:28:49,730 --> 00:28:53,570
the pipeline level Razov right now we

00:28:51,740 --> 00:28:56,540
actually prioritize at the individual

00:28:53,570 --> 00:28:58,670
messages I hope people are able to pick

00:28:56,540 --> 00:29:01,390
up on the distinction there but it's not

00:28:58,670 --> 00:29:05,750
more than happy to dig into this offline

00:29:01,390 --> 00:29:08,000
gate is more your traditional rate limit

00:29:05,750 --> 00:29:09,560
if you've seen kind of github

00:29:08,000 --> 00:29:11,360
you'll get headers back that tell you

00:29:09,560 --> 00:29:14,600
how much stuff you've got left in your

00:29:11,360 --> 00:29:17,060
leaky bucket you can specify the amount

00:29:14,600 --> 00:29:19,010
of throughput per period of time so you

00:29:17,060 --> 00:29:22,100
could say you get a hundred requests per

00:29:19,010 --> 00:29:25,910
10 seconds and we've done that in a few

00:29:22,100 --> 00:29:27,200
cases notably for chaos monkey they hit

00:29:25,910 --> 00:29:29,600
us pretty hard so we give them a pretty

00:29:27,200 --> 00:29:32,030
high capacity but it's still like a

00:29:29,600 --> 00:29:35,750
thousand requests every 10 seconds or

00:29:32,030 --> 00:29:38,630
something this is the more kind of

00:29:35,750 --> 00:29:43,550
detailed detailed description of what I

00:29:38,630 --> 00:29:46,810
just talked about they're useful metrics

00:29:43,550 --> 00:29:49,250
so we we Ahmet as Eric was talking about

00:29:46,810 --> 00:29:51,860
that that tool that kind of shows you

00:29:49,250 --> 00:29:55,100
what what was emitted is actually pretty

00:29:51,860 --> 00:29:57,500
pretty useful depending on what platform

00:29:55,100 --> 00:30:00,610
you're using with Atlas you've got a

00:29:57,500 --> 00:30:03,020
reasonably straightforward way to

00:30:00,610 --> 00:30:05,570
interrogate and see what metrics are

00:30:03,020 --> 00:30:08,570
available but if you don't have that

00:30:05,570 --> 00:30:10,760
take a look at what whatever Scott what

00:30:08,570 --> 00:30:15,020
I've outlined here are the metrics that

00:30:10,760 --> 00:30:17,030
we typically look at so every controller

00:30:15,020 --> 00:30:19,160
that that interceptor that Eric was

00:30:17,030 --> 00:30:22,040
talking about every controller emits

00:30:19,160 --> 00:30:23,900
metrics on the controller name the

00:30:22,040 --> 00:30:26,360
controller method that what's called in

00:30:23,900 --> 00:30:28,210
the status code that was returned so

00:30:26,360 --> 00:30:31,190
that's that's a kind of a critical

00:30:28,210 --> 00:30:33,740
that's something we look at in addition

00:30:31,190 --> 00:30:36,680
to the latency latency is important

00:30:33,740 --> 00:30:39,440
you'll see that in Atlas but we have

00:30:36,680 --> 00:30:41,300
alerts on latency increasing over a

00:30:39,440 --> 00:30:43,430
particular threshold for some period of

00:30:41,300 --> 00:30:47,600
time it's a telltale signal that

00:30:43,430 --> 00:30:49,160
something is going kind of badly we've

00:30:47,600 --> 00:30:50,990
got fistrick's circuit breaker library

00:30:49,160 --> 00:30:53,120
i'm spread out through through any

00:30:50,990 --> 00:30:54,530
number of services and there's kind of

00:30:53,120 --> 00:30:56,750
an important metric in there there's a

00:30:54,530 --> 00:30:59,090
number of metrics that get emitted out

00:30:56,750 --> 00:31:01,850
of hysterics the one we care about as

00:30:59,090 --> 00:31:03,570
the telltale signal is the rolling count

00:31:01,850 --> 00:31:05,690
fall back success

00:31:03,570 --> 00:31:09,180
that's basically when you're triggering

00:31:05,690 --> 00:31:11,010
triggering a fall back I'm kind of just

00:31:09,180 --> 00:31:13,530
want to look at that to say hey is there

00:31:11,010 --> 00:31:16,020
a service that's that's emitting more

00:31:13,530 --> 00:31:17,820
fall backs than it should be is it a

00:31:16,020 --> 00:31:21,870
critical service it's going to impact

00:31:17,820 --> 00:31:24,420
people or is it something like Igor

00:31:21,870 --> 00:31:26,730
where maybe just a single Jenkins master

00:31:24,420 --> 00:31:28,200
has done flaky I gave the example in

00:31:26,730 --> 00:31:30,990
yesterday's talk where we actually moved

00:31:28,200 --> 00:31:34,220
the circuit breaker down to Igor to wrap

00:31:30,990 --> 00:31:38,820
it around Jenkins masters in case one

00:31:34,220 --> 00:31:41,460
went out of service orc is a good one

00:31:38,820 --> 00:31:43,680
we've got since since v3 we've got some

00:31:41,460 --> 00:31:45,960
message base metrics in there there's

00:31:43,680 --> 00:31:48,780
also throttle metrics in there but it's

00:31:45,960 --> 00:31:51,030
it can be useful to understand what your

00:31:48,780 --> 00:31:52,710
applications are doing and one of the

00:31:51,030 --> 00:31:54,510
good metrics for that it's something

00:31:52,710 --> 00:31:57,660
called tasks tasks and vacations and

00:31:54,510 --> 00:31:59,070
that includes both the execution type so

00:31:57,660 --> 00:32:01,560
whether it was a pipeline or an

00:31:59,070 --> 00:32:04,230
orchestration and the actual application

00:32:01,560 --> 00:32:06,840
that did it for us historically we had

00:32:04,230 --> 00:32:08,250
issues where we have a it might be a

00:32:06,840 --> 00:32:09,570
half dozen different apps that could all

00:32:08,250 --> 00:32:10,830
be sending us tremendous amounts of

00:32:09,570 --> 00:32:12,960
traffic and we want to actually know

00:32:10,830 --> 00:32:15,900
well which one is being the worst right

00:32:12,960 --> 00:32:18,140
now this was a useful metric with that

00:32:15,900 --> 00:32:21,480
application tag on it

00:32:18,140 --> 00:32:25,860
we also typically in an in an ideal

00:32:21,480 --> 00:32:28,080
world most most operations that Orcas

00:32:25,860 --> 00:32:30,210
doing should happen should it should

00:32:28,080 --> 00:32:31,650
should wrap up pretty quickly we might

00:32:30,210 --> 00:32:33,210
run into troubles when some of these

00:32:31,650 --> 00:32:36,140
heavyweight operations are running for

00:32:33,210 --> 00:32:38,400
longer periods of times so we do have a

00:32:36,140 --> 00:32:40,760
it's it's implemented as a distribution

00:32:38,400 --> 00:32:44,240
summary but we basically do have a

00:32:40,760 --> 00:32:46,470
bucketed metric that buckets tasks into

00:32:44,240 --> 00:32:49,830
the duration that they've been running

00:32:46,470 --> 00:32:51,030
so that we can see I'll show you I'll

00:32:49,830 --> 00:32:54,380
show you how we graph it on our

00:32:51,030 --> 00:32:57,780
dashboard in a second but you can see

00:32:54,380 --> 00:32:59,310
executions broken down in bands of here

00:32:57,780 --> 00:33:00,720
here's the application that has some

00:32:59,310 --> 00:33:02,880
that have been running for more than 30

00:33:00,720 --> 00:33:04,890
minutes which is something that if

00:33:02,880 --> 00:33:07,140
there's an issue going on and an

00:33:04,890 --> 00:33:09,420
application has an awful lot of stuff

00:33:07,140 --> 00:33:12,090
running for long periods of time that's

00:33:09,420 --> 00:33:14,900
the that's the person you go after and

00:33:12,090 --> 00:33:17,310
you'll see hey is everything healthy

00:33:14,900 --> 00:33:20,940
icloud ever is another good one

00:33:17,310 --> 00:33:23,100
we've got six co-driver instances that

00:33:20,940 --> 00:33:25,080
are running cashing agents and generally

00:33:23,100 --> 00:33:26,970
if everything is happy is happy all

00:33:25,080 --> 00:33:29,280
those six cashing agents should more or

00:33:26,970 --> 00:33:31,680
less be doing the same amount of work so

00:33:29,280 --> 00:33:32,730
you should see a pretty smooth band I

00:33:31,680 --> 00:33:34,620
think at six

00:33:32,730 --> 00:33:36,060
right before will see in the graph but

00:33:34,620 --> 00:33:38,720
everything it should be a fairly smooth

00:33:36,060 --> 00:33:41,790
band of activity across all those

00:33:38,720 --> 00:33:43,980
cashing agents if there was an issue in

00:33:41,790 --> 00:33:47,060
our case with something like Etta which

00:33:43,980 --> 00:33:49,800
is what we use in front of the AWS api's

00:33:47,060 --> 00:33:51,810
some of those cashing agents might fall

00:33:49,800 --> 00:33:53,550
down maybe there's an issue in a

00:33:51,810 --> 00:33:55,500
particular region maybe there's an issue

00:33:53,550 --> 00:33:59,270
in a particular account and we want to

00:33:55,500 --> 00:34:01,590
be able to detect that from cloud driver

00:33:59,270 --> 00:34:05,340
there's a couple of Google specific

00:34:01,590 --> 00:34:07,980
api's that expose metrics on success and

00:34:05,340 --> 00:34:12,600
failure rates when talking to the Google

00:34:07,980 --> 00:34:15,090
API is hit up hit up Eric afterwards if

00:34:12,600 --> 00:34:16,620
you have questions about those similarly

00:34:15,090 --> 00:34:22,080
with Rosco we have our own internal

00:34:16,620 --> 00:34:25,800
bakery API equivalent to Rosco but if if

00:34:22,080 --> 00:34:28,770
you're concerned about bakes ever ever

00:34:25,800 --> 00:34:31,320
failing or spikes of bakes which which

00:34:28,770 --> 00:34:33,149
does happen with our bakery it's useful

00:34:31,320 --> 00:34:35,460
to have a metric rather than relying on

00:34:33,149 --> 00:34:37,800
users to come and come and tell you that

00:34:35,460 --> 00:34:40,260
it's always nice to tell it user we know

00:34:37,800 --> 00:34:42,450
we know the problem we can put up an

00:34:40,260 --> 00:34:44,850
alert it's vinegar rather than having 10

00:34:42,450 --> 00:34:47,040
users come to you saying my bigs broken

00:34:44,850 --> 00:34:50,850
my bigs broken spinnaker is unresponsive

00:34:47,040 --> 00:34:53,580
what is up so we try to be proactive or

00:34:50,850 --> 00:34:55,860
ever possible we've got a health check

00:34:53,580 --> 00:34:57,960
service inside of Netflix that just hits

00:34:55,860 --> 00:35:00,330
every single instance running at Netflix

00:34:57,960 --> 00:35:02,220
and pings their health check port and

00:35:00,330 --> 00:35:04,530
then stick symmetric and Atlas for that

00:35:02,220 --> 00:35:07,020
that is something that we alert off of

00:35:04,530 --> 00:35:09,690
because an unhealthy spinnaker instance

00:35:07,020 --> 00:35:11,940
regardless of the service it's in it's

00:35:09,690 --> 00:35:14,430
potentially leading us down the path of

00:35:11,940 --> 00:35:16,650
destruction and it's a kind of a bad

00:35:14,430 --> 00:35:18,090
situation I would have gate and

00:35:16,650 --> 00:35:21,150
everything's got to be sustained

00:35:18,090 --> 00:35:23,070
we don't alert on kind of blips of

00:35:21,150 --> 00:35:25,500
failures because blips can happen all

00:35:23,070 --> 00:35:31,079
the time but if we had a sustained rate

00:35:25,500 --> 00:35:34,410
of 429 or 5 500 happening maybe

00:35:31,079 --> 00:35:35,819
five over the last 10 minutes that's

00:35:34,410 --> 00:35:38,700
gonna trip an alert on us

00:35:35,819 --> 00:35:40,609
the corollary to that is you've got to

00:35:38,700 --> 00:35:44,880
wait a certain amount of time because

00:35:40,609 --> 00:35:47,729
before you get a signal indicating that

00:35:44,880 --> 00:35:49,979
you might be in a failure State a Redis

00:35:47,729 --> 00:35:52,799
I gave the horror story of us having a

00:35:49,979 --> 00:35:54,809
reticence and a memory so it's important

00:35:52,799 --> 00:35:57,180
that you kind of find a way to monitor

00:35:54,809 --> 00:36:00,239
your Redis memories and kind of set

00:35:57,180 --> 00:36:03,839
thresholds sufficiently low that you're

00:36:00,239 --> 00:36:07,650
gonna get warned well in advance of an

00:36:03,839 --> 00:36:16,219
issue happening all right I'm going to

00:36:07,650 --> 00:36:20,160
try first off I want to look at Cabana I

00:36:16,219 --> 00:36:22,109
everybody may be seen Cabana but it's

00:36:20,160 --> 00:36:24,450
typically we keep about seven days worth

00:36:22,109 --> 00:36:26,069
of logs in here and we can go in here we

00:36:24,450 --> 00:36:28,499
can search for pipeline these high

00:36:26,069 --> 00:36:30,089
quantities obviously we can see we can

00:36:28,499 --> 00:36:32,130
see everything there's a whole lot a

00:36:30,089 --> 00:36:36,569
whole lot of logs in here

00:36:32,130 --> 00:36:38,160
Igor is it's pretty chatty but with with

00:36:36,569 --> 00:36:40,130
a little bit of knowledge about what's

00:36:38,160 --> 00:36:44,039
being logged and kind of how to search

00:36:40,130 --> 00:36:46,559
we're able to kind of uncover most

00:36:44,039 --> 00:36:50,690
issues through here it would just be

00:36:46,559 --> 00:36:50,690
nice if we could alert off of this data

00:36:52,940 --> 00:36:59,400
all right so we have dashboard everybody

00:36:58,109 --> 00:37:01,680
you can see that we actually have a

00:36:59,400 --> 00:37:04,229
bunch of dashboards but this is the one

00:37:01,680 --> 00:37:06,660
that we would kind of look at typically

00:37:04,229 --> 00:37:10,799
we would call this you could consider it

00:37:06,660 --> 00:37:14,190
a overview mostly focused around stuff

00:37:10,799 --> 00:37:16,529
in gate gate an orca because everybody's

00:37:14,190 --> 00:37:17,940
hitting gate it's the thing that if

00:37:16,529 --> 00:37:20,130
there's gonna be a failure downstream

00:37:17,940 --> 00:37:23,279
about it shouldn't manifest itself in

00:37:20,130 --> 00:37:26,459
gate if it's gonna be user facing so we

00:37:23,279 --> 00:37:29,249
can see we can see traffic over the last

00:37:26,459 --> 00:37:31,589
three hours it hasn't been hasn't been

00:37:29,249 --> 00:37:34,380
too bad we can probably go back a little

00:37:31,589 --> 00:37:38,940
bit farther so we can look historically

00:37:34,380 --> 00:37:41,729
up to about two weeks I think as you go

00:37:38,940 --> 00:37:44,810
farther back in time there might be some

00:37:41,729 --> 00:37:49,280
aggregation of the metrics happening in

00:37:44,810 --> 00:37:51,140
inside of Atlas kind of we're looking

00:37:49,280 --> 00:37:53,330
for patterns in here and we're also

00:37:51,140 --> 00:37:55,040
alerting off of patterns so I'll just

00:37:53,330 --> 00:37:57,560
kind of do a quick review of what we're

00:37:55,040 --> 00:38:00,410
looking at here so all requests is just

00:37:57,560 --> 00:38:03,230
straight up how many requests are

00:38:00,410 --> 00:38:05,690
successful big blue and how many 6 how

00:38:03,230 --> 00:38:08,120
many requests that are kind of non non

00:38:05,690 --> 00:38:11,120
to hundreds I don't actually know I

00:38:08,120 --> 00:38:17,660
could look offline what that what that

00:38:11,120 --> 00:38:21,500
guy there is a 404 so somebody is

00:38:17,660 --> 00:38:24,080
getting 404s out of gate I could look at

00:38:21,500 --> 00:38:26,720
logs to see to see who that is

00:38:24,080 --> 00:38:29,000
with dashboards you can kind of see you

00:38:26,720 --> 00:38:31,160
can expand and you can see legends or at

00:38:29,000 --> 00:38:32,960
least subsets of legends to try to sort

00:38:31,160 --> 00:38:34,940
all of our metrics so that from a

00:38:32,960 --> 00:38:36,620
dashboard perspective if we expand

00:38:34,940 --> 00:38:39,140
something and we only see the top five

00:38:36,620 --> 00:38:41,900
five results they're at least going to

00:38:39,140 --> 00:38:44,750
be the top five results that we kind of

00:38:41,900 --> 00:38:47,120
that we kind of care about so over here

00:38:44,750 --> 00:38:51,530
we could see that the pipeline

00:38:47,120 --> 00:38:53,300
controller is top dog right now and this

00:38:51,530 --> 00:38:55,790
is just telling us controllers so it's

00:38:53,300 --> 00:38:57,920
not telling us controller methods we did

00:38:55,790 --> 00:39:00,080
if we saw a big spike in here for

00:38:57,920 --> 00:39:01,640
pipeline controllers that might tell us

00:39:00,080 --> 00:39:04,190
somebody's kicking off a lot of

00:39:01,640 --> 00:39:06,470
pipelines or fetching details on a lot

00:39:04,190 --> 00:39:09,590
of pipelines and then we could dig in to

00:39:06,470 --> 00:39:13,070
a more detailed graph around that

00:39:09,590 --> 00:39:14,810
specific metric controller invocation

00:39:13,070 --> 00:39:17,240
times I think are a little bit more

00:39:14,810 --> 00:39:23,150
finer grained so this is looking at

00:39:17,240 --> 00:39:24,650
actual methods on on a controller and we

00:39:23,150 --> 00:39:26,150
have a lot of methods we have a lot of

00:39:24,650 --> 00:39:28,640
controllers so trying to look at

00:39:26,150 --> 00:39:30,620
individual like pixels in they're pretty

00:39:28,640 --> 00:39:33,500
useless but when you have an issue

00:39:30,620 --> 00:39:36,350
you're gonna see a big spike in here and

00:39:33,500 --> 00:39:38,510
you're gonna be like Oh somebody's

00:39:36,350 --> 00:39:40,250
making some bad calls to the cluster

00:39:38,510 --> 00:39:42,290
controller for example maybe they're

00:39:40,250 --> 00:39:46,430
trying to fetch clusters with 30,000

00:39:42,290 --> 00:39:48,590
instances in them like we had and again

00:39:46,430 --> 00:39:50,630
we want this to be quick so that we can

00:39:48,590 --> 00:39:54,880
see these bad patterns and jump into

00:39:50,630 --> 00:39:57,440
them we've got some throttles down here

00:39:54,880 --> 00:40:00,319
we don't do we don't do my

00:39:57,440 --> 00:40:07,040
straddling in gate but if we look back

00:40:00,319 --> 00:40:09,050
over all the hours probably we get

00:40:07,040 --> 00:40:11,089
there's there certain spikes in the

00:40:09,050 --> 00:40:12,500
early early morning there's some other

00:40:11,089 --> 00:40:15,530
processes that are running here at

00:40:12,500 --> 00:40:17,300
Netflix that kind of send us in this

00:40:15,530 --> 00:40:19,400
case I sent us about two or three times

00:40:17,300 --> 00:40:21,369
as much traffic for a little bit of time

00:40:19,400 --> 00:40:24,530
there I'm not just gonna trip our

00:40:21,369 --> 00:40:26,200
default we've got some default rattles

00:40:24,530 --> 00:40:28,660
in there that are set reasonably high

00:40:26,200 --> 00:40:31,760
but yeah they still get tripped and

00:40:28,660 --> 00:40:34,310
nobody has come and talked to us to

00:40:31,760 --> 00:40:38,089
complain so we haven't we haven't done

00:40:34,310 --> 00:40:40,339
anything about it which is another kind

00:40:38,089 --> 00:40:42,619
of tip like you can you can have metrics

00:40:40,339 --> 00:40:44,270
you can have alerts but if they're not

00:40:42,619 --> 00:40:46,640
if they're firing and not indicative of

00:40:44,270 --> 00:40:50,990
a real problem or you're you're graphing

00:40:46,640 --> 00:40:53,540
stuff that isn't important in the to

00:40:50,990 --> 00:40:55,130
help you diagnose something get it out

00:40:53,540 --> 00:40:57,349
of the way it's something that we

00:40:55,130 --> 00:40:58,700
haven't necessarily been the best at

00:40:57,349 --> 00:41:01,910
there's a few graphs on here that

00:40:58,700 --> 00:41:04,099
probably aren't all that useful but in

00:41:01,910 --> 00:41:10,430
times of crisis they can just be

00:41:04,099 --> 00:41:13,760
cluttered Orca this is the pipeline's

00:41:10,430 --> 00:41:17,030
versus versus tasks graph I think we

00:41:13,760 --> 00:41:19,130
have a lot more pipelines running during

00:41:17,030 --> 00:41:21,109
the day pipe bombs typically run for a

00:41:19,130 --> 00:41:22,970
little longer as well and this is I

00:41:21,109 --> 00:41:26,720
think this graph is ticking every time a

00:41:22,970 --> 00:41:29,869
pipeline cycle happens this guy here is

00:41:26,720 --> 00:41:33,770
a neat is a neat one for by applications

00:41:29,869 --> 00:41:35,560
so you can see like our applications

00:41:33,770 --> 00:41:37,910
doing a lot of doing a lot of business

00:41:35,560 --> 00:41:39,710
who's this guy in the green down here

00:41:37,910 --> 00:41:43,400
who's been running stuff for for quite a

00:41:39,710 --> 00:41:45,260
while oh he's he's map or no he's

00:41:43,400 --> 00:41:49,099
session events the colors don't line up

00:41:45,260 --> 00:41:50,390
across across graphs unfortunately he's

00:41:49,099 --> 00:41:53,480
session events oh is that is that

00:41:50,390 --> 00:41:55,250
abnormal forum we could look I would say

00:41:53,480 --> 00:41:58,700
at this point in time nothing's abnormal

00:41:55,250 --> 00:42:02,540
Spinnaker's Spinnaker's fine there's a

00:41:58,700 --> 00:42:03,920
bunch of metrics around queues as you're

00:42:02,540 --> 00:42:05,510
looking at these things whether you're

00:42:03,920 --> 00:42:07,940
looking at it in Atlas or you're looking

00:42:05,510 --> 00:42:10,280
at it and Prometheus you'll get a feel

00:42:07,940 --> 00:42:11,990
for what normal looks like

00:42:10,280 --> 00:42:14,780
and you'll be able to base alerts off of

00:42:11,990 --> 00:42:16,670
it well we set our alerts at it's

00:42:14,780 --> 00:42:17,960
certainly not going to be applicable to

00:42:16,670 --> 00:42:20,000
everybody else

00:42:17,960 --> 00:42:21,920
everybody's gonna learn what that

00:42:20,000 --> 00:42:25,670
threshold should be how long you should

00:42:21,920 --> 00:42:28,040
wait in an elevated error state before

00:42:25,670 --> 00:42:30,190
you alert and start and start running

00:42:28,040 --> 00:42:35,600
around like you like your hair's on fire

00:42:30,190 --> 00:42:38,930
you'll learn that it's a it's fun this

00:42:35,600 --> 00:42:41,780
graph here it's just us looking at that

00:42:38,930 --> 00:42:44,060
message throttling inside of Orca so we

00:42:41,780 --> 00:42:46,850
we do have throttle set for a number of

00:42:44,060 --> 00:42:49,310
our heavy apps and it's okay if they

00:42:46,850 --> 00:42:51,710
spiked up it's typically not okay if

00:42:49,310 --> 00:42:53,510
they spiked up for five ten fifteen

00:42:51,710 --> 00:42:56,660
minutes because that's going to just

00:42:53,510 --> 00:42:58,400
mean that their pipelines are blasting

00:42:56,660 --> 00:43:00,530
for longer and longer and longer

00:42:58,400 --> 00:43:02,870
somebody mentioned and one of the other

00:43:00,530 --> 00:43:04,340
talks that how do ever get slow and then

00:43:02,870 --> 00:43:05,990
your tap I think in the in the panel

00:43:04,340 --> 00:43:08,210
this morning claw driver gets slow and

00:43:05,990 --> 00:43:09,650
your pipelines run for longer well this

00:43:08,210 --> 00:43:11,300
can also make your pipelines run longer

00:43:09,650 --> 00:43:13,070
if you're throttling every second

00:43:11,300 --> 00:43:16,760
message that it's trying to trying to

00:43:13,070 --> 00:43:19,580
send claw driver is probably the last

00:43:16,760 --> 00:43:22,640
one that I'll look at maybe front if you

00:43:19,580 --> 00:43:25,070
got a good example cloud driver because

00:43:22,640 --> 00:43:28,010
we run a whole bunch of different cloud

00:43:25,070 --> 00:43:30,860
driver clusters and this is just a view

00:43:28,010 --> 00:43:33,800
that we can look at to see which

00:43:30,860 --> 00:43:35,870
clusters might be causing problems at

00:43:33,800 --> 00:43:38,330
the very top front and center is the one

00:43:35,870 --> 00:43:40,220
that's actually facing or serving the

00:43:38,330 --> 00:43:42,860
read-only traffic that's coming in the

00:43:40,220 --> 00:43:47,330
front door so if something spiking here

00:43:42,860 --> 00:43:49,250
we're probably affecting either well we

00:43:47,330 --> 00:43:51,260
could be affecting either a API traffic

00:43:49,250 --> 00:43:54,320
or UI traffic so we generally don't want

00:43:51,260 --> 00:43:56,390
to see big spikes here but we're serving

00:43:54,320 --> 00:44:04,670
this is front door traffic this isn't

00:43:56,390 --> 00:44:07,550
anything being generated by Orca down

00:44:04,670 --> 00:44:12,040
here so these cloud drivers here are

00:44:07,550 --> 00:44:17,060
only serving traffic from Orca and

00:44:12,040 --> 00:44:19,790
you'll see a big jump up right here I

00:44:17,060 --> 00:44:22,220
suspect XK else monkey we have chaos

00:44:19,790 --> 00:44:24,050
monkey chaos monkey goes to sleep I

00:44:22,220 --> 00:44:27,950
think

00:44:24,050 --> 00:44:29,570
after 6:00 at night maybe until 9:00 in

00:44:27,950 --> 00:44:31,790
the morning pretty sure it kicks off at

00:44:29,570 --> 00:44:34,370
9:00 immediately so I suspect that

00:44:31,790 --> 00:44:37,400
that's some follow from chaos it looks

00:44:34,370 --> 00:44:40,930
the same every every day so it's not a

00:44:37,400 --> 00:44:45,050
normal thing to us but if these

00:44:40,930 --> 00:44:47,300
invocation times get elevated for five

00:44:45,050 --> 00:44:49,250
minutes probably maybe five out of five

00:44:47,300 --> 00:44:51,290
o to eight or something then we're going

00:44:49,250 --> 00:44:53,960
to alert because this has the potential

00:44:51,290 --> 00:44:56,210
of messing up people's pipelines they're

00:44:53,960 --> 00:44:58,310
taking longer people don't know what's

00:44:56,210 --> 00:45:00,590
going on they're starting to cancel

00:44:58,310 --> 00:45:03,190
things they're leaving their deployed

00:45:00,590 --> 00:45:07,310
former footprint and inconsistent states

00:45:03,190 --> 00:45:08,830
and this actually represents for plow

00:45:07,310 --> 00:45:11,200
driver clusters behind the scenes

00:45:08,830 --> 00:45:15,530
appleís allows us to do regex

00:45:11,200 --> 00:45:17,630
patterns on the queries that back these

00:45:15,530 --> 00:45:20,840
graphs so we can actually match anything

00:45:17,630 --> 00:45:22,940
that because we're using our typical

00:45:20,840 --> 00:45:26,480
naming convention to name our clusters

00:45:22,940 --> 00:45:28,610
we can basically do anything Orca - one

00:45:26,480 --> 00:45:35,080
two three four five it's basically what

00:45:28,610 --> 00:45:35,080
we've got what do we got over here

00:45:39,760 --> 00:45:43,160
so this is the one where I was

00:45:41,660 --> 00:45:45,830
mentioning that I guess we only have

00:45:43,160 --> 00:45:48,380
actually for whatever caching agents it

00:45:45,830 --> 00:45:52,280
should be a pretty smooth band if we

00:45:48,380 --> 00:45:53,630
blow that out over the past week you'll

00:45:52,280 --> 00:45:56,090
kind of be able to see where we do

00:45:53,630 --> 00:46:00,320
deploys because the colors of those

00:45:56,090 --> 00:46:02,990
bands change and that's exactly kind of

00:46:00,320 --> 00:46:06,170
the point in time that that were that

00:46:02,990 --> 00:46:09,370
we're doing roles so we didn't know too

00:46:06,170 --> 00:46:09,370
much of the last three or four days

00:46:17,690 --> 00:46:24,110
front 50 we had some issues with front

00:46:19,760 --> 00:46:25,880
50 historically I talked about some

00:46:24,110 --> 00:46:28,610
inventing work that we did to kind of

00:46:25,880 --> 00:46:30,380
lower the invocation times in there the

00:46:28,610 --> 00:46:32,540
metric was a telltale sign that there

00:46:30,380 --> 00:46:36,470
was something up so if we look at the

00:46:32,540 --> 00:46:39,020
last seven days of this we should see

00:46:36,470 --> 00:46:42,470
like back here we rolled the fix up

00:46:39,020 --> 00:46:45,290
probably around September eighth or mid

00:46:42,470 --> 00:46:46,700
day of September 7th so we can kind of

00:46:45,290 --> 00:46:49,610
see before that there was a lot of

00:46:46,700 --> 00:46:52,310
flakiness we got as bad as from college

00:46:49,610 --> 00:46:54,740
taking 30 20 30 seconds very poor

00:46:52,310 --> 00:46:56,240
experience it was a clear signal this

00:46:54,740 --> 00:46:57,350
metric was a clear signal that hey we

00:46:56,240 --> 00:46:59,210
needed to do something

00:46:57,350 --> 00:47:01,100
there was no degree of like adding more

00:46:59,210 --> 00:47:04,100
instances different 50 that would have

00:47:01,100 --> 00:47:06,170
got us out of this but we can roll out a

00:47:04,100 --> 00:47:08,960
fix and we can use the metrics as a

00:47:06,170 --> 00:47:11,240
signal for hey has it actually done what

00:47:08,960 --> 00:47:13,160
we needed to do we don't have to rely on

00:47:11,240 --> 00:47:15,200
users kind of coming back to us and say

00:47:13,160 --> 00:47:17,120
hey you did a great job fixing my

00:47:15,200 --> 00:47:19,400
problem because users don't typically do

00:47:17,120 --> 00:47:22,520
that tell you when when you've done

00:47:19,400 --> 00:47:24,170
something bad and only the select few

00:47:22,520 --> 00:47:25,820
will actually come and say hey like you

00:47:24,170 --> 00:47:27,680
he did something really good like you

00:47:25,820 --> 00:47:30,230
actually fixed my problem and I want to

00:47:27,680 --> 00:47:32,780
come thank you for that and at this

00:47:30,230 --> 00:47:34,820
point I want to thank everybody for

00:47:32,780 --> 00:47:38,390
coming I'm not going to show anything

00:47:34,820 --> 00:47:56,060
else I don't know if we have time to do

00:47:38,390 --> 00:48:03,650
anything with Prometheus so you made a

00:47:56,060 --> 00:48:07,190
couple points one is that the dashboards

00:48:03,650 --> 00:48:10,190
in Prometheus themselves are really

00:48:07,190 --> 00:48:12,380
intended for developers or to show you

00:48:10,190 --> 00:48:15,230
what's there and then it's kind of left

00:48:12,380 --> 00:48:26,320
for you to devise the dashboard you want

00:48:15,230 --> 00:48:26,320
to use for your I use

00:48:29,109 --> 00:48:35,300
ISOs a developer looking at the

00:48:32,750 --> 00:48:38,480
integration test to understand what's

00:48:35,300 --> 00:48:40,910
going on or to see the details of the

00:48:38,480 --> 00:48:46,460
behavior because that's what we're

00:48:40,910 --> 00:48:48,140
interested in as opposed to the second

00:48:46,460 --> 00:48:52,790
thing you talked about logging which I

00:48:48,140 --> 00:48:56,420
didn't mention at all within spinnaker

00:48:52,790 --> 00:49:00,099
has support for Google Cloud logging but

00:48:56,420 --> 00:49:03,109
that's based on fluent D which I think

00:49:00,099 --> 00:49:05,060
they dog and I mean it's a standard that

00:49:03,109 --> 00:49:06,470
other things are and I think that the

00:49:05,060 --> 00:49:09,800
support that we have in there which is

00:49:06,470 --> 00:49:12,650
just the configuration for it would work

00:49:09,800 --> 00:49:15,290
I haven't I've been meaning to to try to

00:49:12,650 --> 00:49:18,339
verify this and generalize it to fluent

00:49:15,290 --> 00:49:21,260
D in general if that's the case but

00:49:18,339 --> 00:49:26,060
there is sport in spinnaker now for

00:49:21,260 --> 00:49:27,890
pulling logs out into Lisa I know Google

00:49:26,060 --> 00:49:31,190
Cloud logging you can do structure

00:49:27,890 --> 00:49:33,710
analysis and structured search on it and

00:49:31,190 --> 00:49:36,589
the configuration file breaks out flat

00:49:33,710 --> 00:49:40,069
text in in the log files from spinnaker

00:49:36,589 --> 00:49:44,260
into structured data so it might work

00:49:40,069 --> 00:49:46,940
with other logging systems yeah any

00:49:44,260 --> 00:49:52,010
questions from the audience before lunch

00:49:46,940 --> 00:50:01,160
sure the demon that you wrote that talks

00:49:52,010 --> 00:50:03,890
to these I guess they're like indistinct

00:50:01,160 --> 00:50:08,270
it's big it so the spectator is actually

00:50:03,890 --> 00:50:10,190
built into the yes so in each spinnaker

00:50:08,270 --> 00:50:12,740
instance there's an endpoint called

00:50:10,190 --> 00:50:15,760
spectator slash metrics and that will

00:50:12,740 --> 00:50:18,290
it's on by default it will return the

00:50:15,760 --> 00:50:20,660
the document I showed you can go home

00:50:18,290 --> 00:50:22,849
and pull your your deployments now and

00:50:20,660 --> 00:50:24,440
you'll see that come back ok and so

00:50:22,849 --> 00:50:26,119
that's the standard if across all

00:50:24,440 --> 00:50:31,569
Netflix it's not just specific to

00:50:26,119 --> 00:50:38,300
spinnaker no well the spectator in I I

00:50:31,569 --> 00:50:41,500
added that into I added that into the

00:50:38,300 --> 00:50:43,030
cork library imagine I

00:50:41,500 --> 00:50:45,160
so I it proposed it to the spectator

00:50:43,030 --> 00:50:47,350
library but we don't have write access

00:50:45,160 --> 00:50:51,660
into that and so maintenance of that is

00:50:47,350 --> 00:50:54,490
hard so moved it into quark but I think

00:50:51,660 --> 00:50:56,170
but I did they had something in the

00:50:54,490 --> 00:50:58,750
spectator that would be available for

00:50:56,170 --> 00:51:03,550
any network so it says using spectator

00:50:58,750 --> 00:51:06,520
we get I think we get that endpoint

00:51:03,550 --> 00:51:08,820
and then in quark does some spinnaker

00:51:06,520 --> 00:51:11,290
integration to do the the injection

00:51:08,820 --> 00:51:13,390
don't remember exactly was there but

00:51:11,290 --> 00:51:15,490
there's something in spectator and

00:51:13,390 --> 00:51:18,250
there's something in core which is and

00:51:15,490 --> 00:51:21,550
so for those are don't develop spinnaker

00:51:18,250 --> 00:51:24,340
quark is a another spinnaker library

00:51:21,550 --> 00:51:26,050
that's has some shared components that

00:51:24,340 --> 00:51:29,470
are used across although all the micro

00:51:26,050 --> 00:51:33,070
services okay and so the the demon so

00:51:29,470 --> 00:51:35,260
you recommending people will add code so

00:51:33,070 --> 00:51:37,780
they can plug in their own metrics back

00:51:35,260 --> 00:51:41,050
in and so that you've added a driver for

00:51:37,780 --> 00:51:42,940
radius for instance so that would be

00:51:41,050 --> 00:51:47,320
added to that demon what what language

00:51:42,940 --> 00:51:49,680
is the demon written in Python okay so

00:51:47,320 --> 00:51:53,860
you would just you would add your own

00:51:49,680 --> 00:51:56,550
driver basically or another matrix so

00:51:53,860 --> 00:52:00,490
they're two ways to do it so selfishly

00:51:56,550 --> 00:52:03,940
I'm saying the demon potentially but you

00:52:00,490 --> 00:52:05,350
could also do it in Java and then if

00:52:03,940 --> 00:52:08,370
you're doing your own build your own

00:52:05,350 --> 00:52:11,470
custom builds then you can do you know

00:52:08,370 --> 00:52:16,540
Atlas and put it directly into in a

00:52:11,470 --> 00:52:20,620
spinnaker so and like for stackdriver we

00:52:16,540 --> 00:52:22,000
have it both ways you use it more as for

00:52:20,620 --> 00:52:47,760
development the demons a lot more

00:52:22,000 --> 00:52:51,760
convenient build other systems actually

00:52:47,760 --> 00:52:53,519
so at Netflix we everything uses

00:52:51,760 --> 00:52:56,249
spectator

00:52:53,519 --> 00:52:58,769
but we'll push directly to access we

00:52:56,249 --> 00:53:00,539
don't have a have a daemon polling stuff

00:52:58,769 --> 00:53:05,099
we've got a direct integration with what

00:53:00,539 --> 00:53:09,479
LS is Atlas polling from all the

00:53:05,099 --> 00:53:17,640
endpoints all Chris is shaking his head

00:53:09,479 --> 00:53:22,890
now and so it's push publishing to ours

00:53:17,640 --> 00:53:25,949
but yeah in your case we you have the

00:53:22,890 --> 00:53:28,589
collector yeah you could add to your

00:53:25,949 --> 00:53:30,839
instead of running our demons you could

00:53:28,589 --> 00:53:32,640
write that into your collector and hit

00:53:30,839 --> 00:53:35,099
that endpoint so you don't have to use

00:53:32,640 --> 00:53:37,109
any of the human stuff you take

00:53:35,099 --> 00:53:40,969
advantage of that endpoint as opposed to

00:53:37,109 --> 00:53:47,729
writing it so it's like in your case

00:53:40,969 --> 00:53:49,890
your collector or you inject it into the

00:53:47,729 --> 00:53:51,809
binaries themselves but you know in

00:53:49,890 --> 00:53:54,419
reality you would add into your own

00:53:51,809 --> 00:53:58,999
collector you see HTTP endpoint in

00:53:54,419 --> 00:53:58,999
spectator hit that and then push it into

00:54:00,229 --> 00:54:05,939
one more one more question I think you

00:54:02,909 --> 00:54:07,469
were first yeah I'm just wondering if

00:54:05,939 --> 00:54:10,739
you have like any type of like

00:54:07,469 --> 00:54:12,209
self-healing where like let's say there

00:54:10,739 --> 00:54:15,809
are some errors that are pretty really

00:54:12,209 --> 00:54:19,019
higher maybe not errors yeah so do you

00:54:15,809 --> 00:54:21,659
have we don't take advantage of it too

00:54:19,019 --> 00:54:24,659
much but the alerting capability in

00:54:21,659 --> 00:54:28,919
Atlas the thing that we do do is we use

00:54:24,659 --> 00:54:30,959
it to shoot instances so if if say Igor

00:54:28,919 --> 00:54:32,849
is is behaving badly

00:54:30,959 --> 00:54:35,279
it's got elevated like there's there's

00:54:32,849 --> 00:54:37,169
there's issues there will the shoot it

00:54:35,279 --> 00:54:39,209
because we know that that's a service

00:54:37,169 --> 00:54:41,399
that we can do that and everything's

00:54:39,209 --> 00:54:43,799
going to be fine I don't think we do

00:54:41,399 --> 00:54:45,689
that in any of our other services the

00:54:43,799 --> 00:54:49,979
other thing Atlas can do is it can

00:54:45,689 --> 00:54:52,109
basically run kind of programmable run

00:54:49,979 --> 00:54:56,219
books so as an engineer there's some

00:54:52,109 --> 00:54:58,499
runbook infrastructure here and off of

00:54:56,219 --> 00:55:00,709
allert you can run a run book on your

00:54:58,499 --> 00:55:03,329
instance and have it do some remediation

00:55:00,709 --> 00:55:05,519
we haven't done that yet we've always

00:55:03,329 --> 00:55:07,850
talked about it but we do have that

00:55:05,519 --> 00:55:12,030
capability here and that's like

00:55:07,850 --> 00:55:15,210
maybe one more question yeah I just had

00:55:12,030 --> 00:55:18,930
a question so all of the the services I

00:55:15,210 --> 00:55:20,670
talked about were the jvm based services

00:55:18,930 --> 00:55:29,550
do you guys do any specific monitoring

00:55:20,670 --> 00:55:35,430
around deck or do you I know we have

00:55:29,550 --> 00:55:37,860
users for that that is a good point we

00:55:35,430 --> 00:55:40,080
would have a health check on the actual

00:55:37,860 --> 00:55:43,050
instances that are running and serving

00:55:40,080 --> 00:55:45,840
deck that probably at least should be

00:55:43,050 --> 00:55:49,680
able to check hey is is a passionate

00:55:45,840 --> 00:55:56,090
running but other than that I don't know

00:55:49,680 --> 00:55:59,730
that we have anything better we do have

00:55:56,090 --> 00:56:01,800
some alerting hooked into deck so if we

00:55:59,730 --> 00:56:04,080
encounter if people encounter like

00:56:01,800 --> 00:56:06,660
errors that would end up in their

00:56:04,080 --> 00:56:10,020
consoles I know like Chris Baron or UI

00:56:06,660 --> 00:56:13,230
guys get emails about that I'm not quite

00:56:10,020 --> 00:56:14,880
sure how that's hooked in but we do have

00:56:13,230 --> 00:56:18,900
that but we don't have anything to say

00:56:14,880 --> 00:56:30,180
hey deck is fully 100% available right

00:56:18,900 --> 00:56:32,640
now are there any are there any person

00:56:30,180 --> 00:56:36,000
to talk to about that but I can put you

00:56:32,640 --> 00:56:36,900
in touch with our cool I think it's

00:56:36,000 --> 00:56:38,870
lunchtime

00:56:36,900 --> 00:56:42,020
I'll be around for the rest of the day

00:56:38,870 --> 00:56:42,020
or air

00:56:43,370 --> 00:56:46,520

YouTube URL: https://www.youtube.com/watch?v=odlW2HnROJ8


