Title: Canary Analysis at Netflix
Publication date: 2017-09-22
Playlist: Spinnaker Summit 2017
Description: 
	CHRIS SANDEN, GREG BURRELL, NETFLIX

In this talk weâ€™ll discuss how Netflix approaches canary releases. Towards this end, weâ€™ll discuss how we have been able to automate the process. In addition, we will talk about how our canary release process has evolved and discuss some of the lessons learned along the way.

From Spinnaker Summit 2017
Captions: 
	00:00:00,860 --> 00:00:04,879
all right we're gonna get started

00:00:07,010 --> 00:00:13,769
how's it going spinnaker summit to bar

00:00:10,670 --> 00:00:15,030
awesome thank you for attending our

00:00:13,769 --> 00:00:19,190
presentation this afternoon we'll be

00:00:15,030 --> 00:00:19,190
talking about canary analysis at Netflix

00:00:21,650 --> 00:00:25,920
so just maybe buy a quick raise of hands

00:00:24,090 --> 00:00:28,140
who adheres to some type of canary

00:00:25,920 --> 00:00:31,500
release process today or doesn't have a

00:00:28,140 --> 00:00:33,780
canary analysis today all right

00:00:31,500 --> 00:00:35,010
I saw a sake Netflix use it's not put

00:00:33,780 --> 00:00:39,450
their hands up so we're not to have a

00:00:35,010 --> 00:00:40,739
chat later so maybe before we get

00:00:39,450 --> 00:00:43,800
started we'll talk a little bit about

00:00:40,739 --> 00:00:46,140
ourselves first I'm Chris and I'm a

00:00:43,800 --> 00:00:48,570
senior data scientist I work on the

00:00:46,140 --> 00:00:50,160
fault detection engineering team and

00:00:48,570 --> 00:00:53,960
that team was responsible for the canary

00:00:50,160 --> 00:00:53,960
analysis platform we have at Netflix

00:00:55,489 --> 00:00:58,739
I'm Greg Morrell

00:00:57,180 --> 00:01:01,620
I've been here at Netflix where were 12

00:00:58,739 --> 00:01:03,270
years when I was hired I was hard to do

00:01:01,620 --> 00:01:04,890
the testing for a little skunk works

00:01:03,270 --> 00:01:07,500
program that was playing with the idea

00:01:04,890 --> 00:01:09,600
of babies over the internet so you can

00:01:07,500 --> 00:01:12,119
see that turned out pretty well quickly

00:01:09,600 --> 00:01:19,320
that server testing turned into system

00:01:12,119 --> 00:01:21,720
administration No ok so I've been here

00:01:19,320 --> 00:01:24,420
over 12 years of Netflix I started with

00:01:21,720 --> 00:01:26,040
a little experimental project to play

00:01:24,420 --> 00:01:29,130
with the idea movies over the internet

00:01:26,040 --> 00:01:32,009
and along the way I have a work from a

00:01:29,130 --> 00:01:35,400
server tester to a system in building

00:01:32,009 --> 00:01:38,520
these engineering 8 DevOps in there and

00:01:35,400 --> 00:01:42,840
now I'm going to senior sre on the edge

00:01:38,520 --> 00:01:44,340
operating experience of the team I've

00:01:42,840 --> 00:01:46,259
worked for the past several years with

00:01:44,340 --> 00:01:47,780
Chris Tandon we're on separate teams but

00:01:46,259 --> 00:01:50,460
we sort of formed a partnership to

00:01:47,780 --> 00:01:52,920
evolve the idea of canary analysis here

00:01:50,460 --> 00:01:55,530
at Netflix Chris provides a lot of the

00:01:52,920 --> 00:01:56,939
data science behind canary analysis and

00:01:55,530 --> 00:01:58,950
then I take the fruits of his Labor's

00:01:56,939 --> 00:02:01,500
and apply it to the grill world and

00:01:58,950 --> 00:02:04,430
through that partnership we've sort of

00:02:01,500 --> 00:02:04,430
evolved it to where it is

00:02:06,420 --> 00:02:10,390
so if you all represent I've seen a

00:02:08,500 --> 00:02:13,720
presentation by NDTV starts off with

00:02:10,390 --> 00:02:15,040
kind of these big numbers so typically

00:02:13,720 --> 00:02:17,200
we'd like to think that you know we do

00:02:15,040 --> 00:02:18,850
about 4,000 employments a day into our

00:02:17,200 --> 00:02:20,950
production environment here at Netflix

00:02:18,850 --> 00:02:22,570
for the care analysis perspective we

00:02:20,950 --> 00:02:23,410
think of a little bit differently we

00:02:22,570 --> 00:02:25,900
think about the number of kind of

00:02:23,410 --> 00:02:27,970
decisions or judgments we make every

00:02:25,900 --> 00:02:30,220
game and so from that perspective we do

00:02:27,970 --> 00:02:32,440
about 1,200 kind of judgments or

00:02:30,220 --> 00:02:38,260
decisions every day regarding our carry

00:02:32,440 --> 00:02:39,580
deployments and so those are new to

00:02:38,260 --> 00:02:43,690
canary analysis we're gonna take a

00:02:39,580 --> 00:02:47,110
little bit of a kind of exploratory look

00:02:43,690 --> 00:02:48,610
at you know what is canary analysis so

00:02:47,110 --> 00:02:51,580
you can kind of think of canary analysis

00:02:48,610 --> 00:02:56,380
as a deployment strategy or do a

00:02:51,580 --> 00:02:58,090
deployment to kind of its go strategy or

00:02:56,380 --> 00:02:59,770
pattern where a new change is going to

00:02:58,090 --> 00:03:01,240
be rolled out into production to say a

00:02:59,770 --> 00:03:04,180
subset of your infrastructure or to a

00:03:01,240 --> 00:03:05,830
subset of your users and as you do this

00:03:04,180 --> 00:03:08,140
you're gonna perform checkpoints kind of

00:03:05,830 --> 00:03:10,750
along the way and you're gonna examine

00:03:08,140 --> 00:03:11,770
this new canary system and then at each

00:03:10,750 --> 00:03:13,959
of these checkpoint you're gonna make a

00:03:11,770 --> 00:03:15,820
decision and you're gonna see is going

00:03:13,959 --> 00:03:17,890
to be around the risk of releasing this

00:03:15,820 --> 00:03:20,430
new change into production to all of

00:03:17,890 --> 00:03:22,660
your users or all of your environments

00:03:20,430 --> 00:03:24,730
from a canary analysis perspective it is

00:03:22,660 --> 00:03:25,840
important to note that this is this is

00:03:24,730 --> 00:03:27,489
not supposed to be a replacement for

00:03:25,840 --> 00:03:28,780
testing by any means it's not supposed

00:03:27,489 --> 00:03:32,320
to replace unit testing integration

00:03:28,780 --> 00:03:33,850
testing this must be complimentary this

00:03:32,320 --> 00:03:36,070
must be another tool and see the

00:03:33,850 --> 00:03:37,510
operators or developers toolbox to help

00:03:36,070 --> 00:03:38,709
them you know assess the risk of what

00:03:37,510 --> 00:03:40,780
they're doing and their changes in the

00:03:38,709 --> 00:03:43,440
environment I'm so might ask yourself

00:03:40,780 --> 00:03:46,360
well why why should i do canary analysis

00:03:43,440 --> 00:03:47,920
good question they offers you a better

00:03:46,360 --> 00:03:51,310
degree of trust and deployment in your

00:03:47,920 --> 00:03:52,600
in your in your changes you know you're

00:03:51,310 --> 00:03:55,300
rolling these changes out to a small

00:03:52,600 --> 00:03:56,709
segment of your users in to or to a

00:03:55,300 --> 00:03:57,700
small part of your infrastructure and

00:03:56,709 --> 00:03:59,110
this gives you some confidence that

00:03:57,700 --> 00:04:01,450
you're not going to you know cause

00:03:59,110 --> 00:04:04,000
widespread how did your averages or

00:04:01,450 --> 00:04:06,760
issues it can help identify issues with

00:04:04,000 --> 00:04:10,330
your production builds in a faster

00:04:06,760 --> 00:04:12,730
manner and finally it helps to lower or

00:04:10,330 --> 00:04:14,500
even eliminate the cost in simulation

00:04:12,730 --> 00:04:16,459
engineering there'll always be a gap

00:04:14,500 --> 00:04:17,810
between your simulation environment

00:04:16,459 --> 00:04:19,579
in your production environment it can be

00:04:17,810 --> 00:04:21,470
costly to keep those two in alignment so

00:04:19,579 --> 00:04:23,330
in efflux we don't have a simulation

00:04:21,470 --> 00:04:25,789
environment at all we luck rely heavily

00:04:23,330 --> 00:04:29,210
on tools like canary analysis - don't

00:04:25,789 --> 00:04:30,590
fill that gap so there's kind of two

00:04:29,210 --> 00:04:32,740
prerequisites you need to have before

00:04:30,590 --> 00:04:35,330
you can you think about canary analysis

00:04:32,740 --> 00:04:37,310
one is you need a canary release process

00:04:35,330 --> 00:04:39,410
and then after that you need observe

00:04:37,310 --> 00:04:41,419
ability or metrics so let's take a quick

00:04:39,410 --> 00:04:43,639
look at what does a general canary

00:04:41,419 --> 00:04:46,460
analysis sorry I can every employment

00:04:43,639 --> 00:04:48,199
look like so this reflects I'm a

00:04:46,460 --> 00:04:50,360
high-level what a general canary

00:04:48,199 --> 00:04:51,860
deployment looks like in this we're

00:04:50,360 --> 00:04:53,509
gonna release some new canary change

00:04:51,860 --> 00:04:56,870
into production it's gonna sit right

00:04:53,509 --> 00:04:58,039
alongside your production deployment but

00:04:56,870 --> 00:05:00,259
you can see that you know in this

00:04:58,039 --> 00:05:02,120
example we have about five servers that

00:05:00,259 --> 00:05:04,430
are in our canary in a production

00:05:02,120 --> 00:05:06,590
sitting around ninety five servers so if

00:05:04,430 --> 00:05:08,180
we imagine this is a middle tier state

00:05:06,590 --> 00:05:09,889
this web application that's behind a

00:05:08,180 --> 00:05:11,090
load balancer we can see that yellow

00:05:09,889 --> 00:05:13,250
production is still going to take around

00:05:11,090 --> 00:05:15,860
95 percent of the workload and then our

00:05:13,250 --> 00:05:17,659
Canaries you take around 5% this is the

00:05:15,860 --> 00:05:20,300
idea of the canary in the coalmine we're

00:05:17,659 --> 00:05:23,810
gonna push a small build out there we're

00:05:20,300 --> 00:05:25,520
gonna we're gonna feel it out and then

00:05:23,810 --> 00:05:27,169
once we have this infrastructure set up

00:05:25,520 --> 00:05:29,240
then we can you know start to leverage

00:05:27,169 --> 00:05:31,130
it and the leverage it we need some

00:05:29,240 --> 00:05:32,870
observability you need to measure the

00:05:31,130 --> 00:05:35,479
performance or the behavior of these

00:05:32,870 --> 00:05:37,070
systems and so observability or metrics

00:05:35,479 --> 00:05:40,789
become a very critical part to doing any

00:05:37,070 --> 00:05:42,650
type of canary analysis and on top of

00:05:40,789 --> 00:05:44,419
that you need to be able to separate out

00:05:42,650 --> 00:05:46,370
your observability or your metrics you

00:05:44,419 --> 00:05:47,919
need to look at the canary system

00:05:46,370 --> 00:05:50,120
independently of the production system

00:05:47,919 --> 00:05:51,590
you don't want your caring metrics all

00:05:50,120 --> 00:05:52,969
mangled in with your production metrics

00:05:51,590 --> 00:05:54,860
it can make it hard to come and get that

00:05:52,969 --> 00:05:56,990
visibility so this is a critical part to

00:05:54,860 --> 00:05:58,509
kind of carry analysis is the separation

00:05:56,990 --> 00:06:00,530
of metrics as well

00:05:58,509 --> 00:06:02,270
so there are many ways to get started

00:06:00,530 --> 00:06:03,620
with canary analysis and one of the

00:06:02,270 --> 00:06:05,990
easiest ways is you know you use

00:06:03,620 --> 00:06:07,940
spinnaker to create a pipeline that

00:06:05,990 --> 00:06:09,409
takes your build and deploys it into a

00:06:07,940 --> 00:06:11,990
canary build and then you could easily

00:06:09,409 --> 00:06:14,300
as get a few dashboards together they

00:06:11,990 --> 00:06:16,750
make a manual determination but there

00:06:14,300 --> 00:06:16,750
are better ways

00:06:18,660 --> 00:06:23,590
thanks Chris so far we've talked about

00:06:21,490 --> 00:06:25,180
the general canary process but we really

00:06:23,590 --> 00:06:28,270
haven't talked about automation the a

00:06:25,180 --> 00:06:30,760
and AC a that's automated canary

00:06:28,270 --> 00:06:32,260
analysis now when we first started doing

00:06:30,760 --> 00:06:34,330
Canaries we actually it was a very

00:06:32,260 --> 00:06:36,220
manual process and we literally have a

00:06:34,330 --> 00:06:38,140
checklist for the release engineer to go

00:06:36,220 --> 00:06:40,780
through a blank white line and pulling

00:06:38,140 --> 00:06:43,120
up graphs pouring through logs looking

00:06:40,780 --> 00:06:46,450
for signs of bad behaviors in this new

00:06:43,120 --> 00:06:49,300
version this was a very manual and human

00:06:46,450 --> 00:06:51,790
process and it suffered from what we

00:06:49,300 --> 00:06:54,310
call the squishiness of human decisions

00:06:51,790 --> 00:06:56,350
that is you know a person staring at

00:06:54,310 --> 00:06:58,330
graphs for a long time they get fatigued

00:06:56,350 --> 00:07:00,790
their eyes glaze over it's easy to miss

00:06:58,330 --> 00:07:03,370
stuff humans are also very prone to

00:07:00,790 --> 00:07:05,770
confirmation bias you know they see to

00:07:03,370 --> 00:07:07,030
just a draft the lines look pretty close

00:07:05,770 --> 00:07:08,470
well you know what that's good enough

00:07:07,030 --> 00:07:09,810
because I really want to get this push

00:07:08,470 --> 00:07:12,880
out the door and we want something else

00:07:09,810 --> 00:07:14,890
also decisions can vary from one

00:07:12,880 --> 00:07:16,360
engineer to next one engineer may say oh

00:07:14,890 --> 00:07:17,890
my gosh that's terrible and another

00:07:16,360 --> 00:07:19,570
buddy look at the same data and say yeah

00:07:17,890 --> 00:07:22,210
that's that's probably okay so there's

00:07:19,570 --> 00:07:24,400
no consistency do with automation we've

00:07:22,210 --> 00:07:26,940
sort of taken the human decision out of

00:07:24,400 --> 00:07:30,550
this process it becomes very repeatable

00:07:26,940 --> 00:07:35,979
reproducible and has led to increasing

00:07:30,550 --> 00:07:38,169
our deployment velocity so now we've

00:07:35,979 --> 00:07:39,790
talked about up to now we sort of talked

00:07:38,169 --> 00:07:41,470
about general canary now we're going to

00:07:39,790 --> 00:07:44,800
get a little more into the specifics of

00:07:41,470 --> 00:07:46,540
how we do it here in Netflix and that

00:07:44,800 --> 00:07:51,970
starts with the Netflix canary

00:07:46,540 --> 00:07:53,229
deployment promises so this picture is

00:07:51,970 --> 00:07:54,850
very similar to the one we presented

00:07:53,229 --> 00:07:56,860
earlier but it's very there's a slight

00:07:54,850 --> 00:07:58,810
difference here on Netflix with our

00:07:56,860 --> 00:08:02,050
spinnaker deployments our spinnaker

00:07:58,810 --> 00:08:04,750
pipelines we start a canary cluster and

00:08:02,050 --> 00:08:06,250
a baseline cluster at the same time this

00:08:04,750 --> 00:08:08,290
sort of gives us a better apples to

00:08:06,250 --> 00:08:10,479
apples comparison between the canary and

00:08:08,290 --> 00:08:11,740
the baseline the canary cluster may be

00:08:10,479 --> 00:08:13,900
running the new build

00:08:11,740 --> 00:08:17,080
well the baseline is running the old

00:08:13,900 --> 00:08:19,240
bill that's currently in production it's

00:08:17,080 --> 00:08:23,130
a cleaner environment it's easier for us

00:08:19,240 --> 00:08:23,130
to isolate the metrics and effects and

00:08:23,330 --> 00:08:29,669
it in spinnaker the spinnaker pipeline

00:08:28,590 --> 00:08:31,800
will essentially wait for these two

00:08:29,669 --> 00:08:33,330
little clusters to start up and at that

00:08:31,800 --> 00:08:34,979
point they start feeding metrics into

00:08:33,330 --> 00:08:39,930
our metrics collector which can then be

00:08:34,979 --> 00:08:41,789
used by the judgment process so once we

00:08:39,930 --> 00:08:43,610
have the infrastructure stood up we have

00:08:41,789 --> 00:08:45,510
those nice clean canary baseline

00:08:43,610 --> 00:08:47,010
clusters we can start to think about

00:08:45,510 --> 00:08:49,620
making some judgments or risk

00:08:47,010 --> 00:08:51,230
assessments on those and so we think of

00:08:49,620 --> 00:08:53,399
it from having this judgment workflow

00:08:51,230 --> 00:08:54,810
and at the start you know we start

00:08:53,399 --> 00:08:57,300
around gathering metrics like I said

00:08:54,810 --> 00:08:58,770
observability is very important and with

00:08:57,300 --> 00:09:00,510
my Netflix we have a self-service tool

00:08:58,770 --> 00:09:01,980
where our developers and operators can

00:09:00,510 --> 00:09:03,660
go in and define the metrics that are

00:09:01,980 --> 00:09:07,440
important to them as part of carry

00:09:03,660 --> 00:09:09,120
analysis this tool is open all the

00:09:07,440 --> 00:09:11,339
service owners can use it and it really

00:09:09,120 --> 00:09:13,860
just is a general tool for defining

00:09:11,339 --> 00:09:15,839
queries and what to analyze for the

00:09:13,860 --> 00:09:17,399
carry analysis so once our operators

00:09:15,839 --> 00:09:19,920
have defined what is important to them

00:09:17,399 --> 00:09:21,270
and what they should look at we can kind

00:09:19,920 --> 00:09:23,880
of start the analysis workflow or the

00:09:21,270 --> 00:09:26,220
judgment workflow you know we move on to

00:09:23,880 --> 00:09:27,630
compete a validation we want to ensure

00:09:26,220 --> 00:09:30,390
the prior to running in type of canary

00:09:27,630 --> 00:09:32,130
analysis that we have data in the case

00:09:30,390 --> 00:09:33,779
where maybe we have data for the carry

00:09:32,130 --> 00:09:35,459
not the baseline or vice versa

00:09:33,779 --> 00:09:36,750
we want to make sure that we have data

00:09:35,459 --> 00:09:38,550
for thoughts that make an accurate

00:09:36,750 --> 00:09:39,750
judgment so in this case if we don't

00:09:38,550 --> 00:09:42,510
have data for one or the other we may

00:09:39,750 --> 00:09:44,520
terminate the pipeline really on and

00:09:42,510 --> 00:09:46,920
then restarted or go investigate why

00:09:44,520 --> 00:09:48,540
don't we have data once we actually

00:09:46,920 --> 00:09:49,920
validate that we have data we're gonna

00:09:48,540 --> 00:09:51,570
move on to cleaning it we want to create

00:09:49,920 --> 00:09:54,390
some nice clean data to make the

00:09:51,570 --> 00:09:55,620
analysis easier so for us there's some

00:09:54,390 --> 00:09:57,300
various things we do to clean the data

00:09:55,620 --> 00:09:59,220
one of the more important ones is we do

00:09:57,300 --> 00:10:00,870
anomaly detection in an optimum Bubo on

00:09:59,220 --> 00:10:02,790
this data so we look for really

00:10:00,870 --> 00:10:04,320
experienced data points get rid of them

00:10:02,790 --> 00:10:06,990
and try to create create a nice clean

00:10:04,320 --> 00:10:08,420
data set in addition we also look at

00:10:06,990 --> 00:10:10,589
what to do with missing data points

00:10:08,420 --> 00:10:13,230
depending on the way you know telemetry

00:10:10,589 --> 00:10:14,730
stores work and missing data can be

00:10:13,230 --> 00:10:15,930
different across all limits so we have

00:10:14,730 --> 00:10:18,240
to handle those two use cases

00:10:15,930 --> 00:10:20,190
so once we have this nice clean data set

00:10:18,240 --> 00:10:21,870
and these nice clean metrics we can then

00:10:20,190 --> 00:10:23,970
get into the meat of what carry analysis

00:10:21,870 --> 00:10:27,750
does and that's what I produce the

00:10:23,970 --> 00:10:29,540
metric comparison so I like to think of

00:10:27,750 --> 00:10:31,440
metric comparison as one of these

00:10:29,540 --> 00:10:33,260
picture games that you may have played

00:10:31,440 --> 00:10:35,579
we're trying to identify the differences

00:10:33,260 --> 00:10:36,920
but instead of these nice clean pictures

00:10:35,579 --> 00:10:39,710
we're dealing

00:10:36,920 --> 00:10:42,290
time-series metrics I like to call this

00:10:39,710 --> 00:10:43,640
the squiggly line problem within the

00:10:42,290 --> 00:10:45,710
squiggly line problem there are some

00:10:43,640 --> 00:10:47,810
interests and challenges one of them is

00:10:45,710 --> 00:10:49,400
trying to accurately determine if there

00:10:47,810 --> 00:10:52,640
is a difference so I have two examples

00:10:49,400 --> 00:10:55,880
here the top graph shows some synthetic

00:10:52,640 --> 00:10:58,070
data it's maybe CPU utilization or

00:10:55,880 --> 00:11:02,120
something and the red line represents

00:10:58,070 --> 00:11:04,100
the Decarie and the blue line represents

00:11:02,120 --> 00:11:06,110
the our controller baseline and you may

00:11:04,100 --> 00:11:09,050
look at the top graph and say things

00:11:06,110 --> 00:11:10,010
look roughly roughly the same but it

00:11:09,050 --> 00:11:11,990
gets little more challenging when we get

00:11:10,010 --> 00:11:13,550
to the bottom graph you know instead of

00:11:11,990 --> 00:11:16,760
the you know you and the audience may

00:11:13,550 --> 00:11:19,010
say ah it looks good enough and instead

00:11:16,760 --> 00:11:21,410
of using may think that you know it

00:11:19,010 --> 00:11:23,060
looks bad so a challenge in the vector

00:11:21,410 --> 00:11:24,620
comparison is and how do we had to read

00:11:23,060 --> 00:11:26,720
count for this and how do we accurately

00:11:24,620 --> 00:11:32,770
and reproduce you know the same type of

00:11:26,720 --> 00:11:32,770
comparison enter and wait yeah

00:11:33,370 --> 00:11:39,860
the end in in reality we-we-we lever

00:11:38,120 --> 00:11:43,220
just kind of the power of nonparametric

00:11:39,860 --> 00:11:44,870
statistics so within the canary analysis

00:11:43,220 --> 00:11:47,360
and we have your Netflix we use the men

00:11:44,870 --> 00:11:50,660
Whitney you hypothesis test that is a

00:11:47,360 --> 00:11:52,250
mouthful but in reality what we do is we

00:11:50,660 --> 00:11:54,350
look at the distribution of data points

00:11:52,250 --> 00:11:56,300
for the canary and the baseline and

00:11:54,350 --> 00:11:58,670
that's represented by the bar graph here

00:11:56,300 --> 00:11:59,960
we have some histograms of the data and

00:11:58,670 --> 00:12:02,510
what we do is use the mann-whitney u

00:11:59,960 --> 00:12:04,610
test to ask are the two distributions

00:12:02,510 --> 00:12:06,110
similar or dissimilar and it's able to

00:12:04,610 --> 00:12:08,180
give us an answer with some statistical

00:12:06,110 --> 00:12:09,100
certainty M are they the same or are

00:12:08,180 --> 00:12:11,720
they different

00:12:09,100 --> 00:12:13,280
so using this approach we can then you

00:12:11,720 --> 00:12:15,200
know use the mann-whitney u test or

00:12:13,280 --> 00:12:16,520
these nonparametric statistics and

00:12:15,200 --> 00:12:18,950
compare all of the metrics that our

00:12:16,520 --> 00:12:20,000
service owner has clapham configured and

00:12:18,950 --> 00:12:21,980
for each of those we come up with a

00:12:20,000 --> 00:12:22,940
classification we say for each metric

00:12:21,980 --> 00:12:25,850
it's similar

00:12:22,940 --> 00:12:27,830
or is it dissing one more specifically

00:12:25,850 --> 00:12:30,470
we can then say is the canary metric

00:12:27,830 --> 00:12:32,330
higher or lower than the baseline so in

00:12:30,470 --> 00:12:34,880
this case down here an example I have

00:12:32,330 --> 00:12:36,110
the Kerry metrics actually lower and we

00:12:34,880 --> 00:12:38,210
can see that as a ship in that

00:12:36,110 --> 00:12:39,640
distribution was shipped in that mr.

00:12:38,210 --> 00:12:41,960
brown

00:12:39,640 --> 00:12:43,880
so once we've done all the hard work and

00:12:41,960 --> 00:12:45,680
we've classified all those metrics and

00:12:43,880 --> 00:12:47,180
we have those classifications are they

00:12:45,680 --> 00:12:49,250
highs as low as it's similar at

00:12:47,180 --> 00:12:49,700
dissimilar we can then go on and then

00:12:49,250 --> 00:12:52,400
compute

00:12:49,700 --> 00:12:53,900
score this is our risk assessment and

00:12:52,400 --> 00:12:56,390
the score can be you know fairly simple

00:12:53,900 --> 00:12:57,560
compute and Netflix we take more of a

00:12:56,390 --> 00:12:59,690
simpler approach to our score

00:12:57,560 --> 00:13:01,430
calculations what we do is we look at

00:12:59,690 --> 00:13:04,070
the total number of metrics that we're

00:13:01,430 --> 00:13:09,020
similar so the ones that passed and we

00:13:04,070 --> 00:13:11,210
divide it the total number of metrics in

00:13:09,020 --> 00:13:12,740
reality can get more complex here but

00:13:11,210 --> 00:13:14,840
keeping it simple has some advantages in

00:13:12,740 --> 00:13:16,310
the fact that you want it to be

00:13:14,840 --> 00:13:20,000
intuitive and understandable by your

00:13:16,310 --> 00:13:21,830
consumer by operators so now that we

00:13:20,000 --> 00:13:23,420
kind of seen how the judgment works and

00:13:21,830 --> 00:13:25,160
how that we stand up all the

00:13:23,420 --> 00:13:26,990
infrastructure let's let's kind of put

00:13:25,160 --> 00:13:31,580
all the pieces together and let's talk

00:13:26,990 --> 00:13:33,620
about spinnaker thanks Chris

00:13:31,580 --> 00:13:34,910
so and I'll talk a little bit about how

00:13:33,620 --> 00:13:37,160
we actually make this work within

00:13:34,910 --> 00:13:40,520
vinegar and sort of the adopters that we

00:13:37,160 --> 00:13:44,150
put together this is an example of a

00:13:40,520 --> 00:13:45,950
typical Netflix canary pipeline in this

00:13:44,150 --> 00:13:47,960
case you know we have a series of stages

00:13:45,950 --> 00:13:50,050
where we find something - canary maybe

00:13:47,960 --> 00:13:52,760
it's the last bill that passed our

00:13:50,050 --> 00:13:54,170
integration tests and we want to make

00:13:52,760 --> 00:13:56,300
sure that we have not already in canary

00:13:54,170 --> 00:14:00,380
this build once we get past that we

00:13:56,300 --> 00:14:02,150
enter the spinnaker canary stage now at

00:14:00,380 --> 00:14:04,190
this point spinnaker will stand up the

00:14:02,150 --> 00:14:06,620
canary and baseline clusters that we

00:14:04,190 --> 00:14:09,290
defined and it will wait for those

00:14:06,620 --> 00:14:10,700
instances to come up and once that those

00:14:09,290 --> 00:14:13,430
are ready then it will start

00:14:10,700 --> 00:14:16,730
communicating with the canary analysis

00:14:13,430 --> 00:14:18,590
engine to do the evaluation in this

00:14:16,730 --> 00:14:21,140
particular example you can see that we

00:14:18,590 --> 00:14:23,540
are canoeing in three separate aw jeans

00:14:21,140 --> 00:14:25,610
and in fact the canary was quite

00:14:23,540 --> 00:14:29,480
successful a score perfect score of 100

00:14:25,610 --> 00:14:30,980
in u.s. East and us West you West didn't

00:14:29,480 --> 00:14:32,480
do quite as well but it was still good

00:14:30,980 --> 00:14:35,840
enough to pass you can see the score is

00:14:32,480 --> 00:14:38,150
ninety seven and we've competed we

00:14:35,840 --> 00:14:40,880
decided that 97 is good enough we're

00:14:38,150 --> 00:14:43,070
going to call that pass so at this point

00:14:40,880 --> 00:14:45,830
the pipeline would proceed on to a

00:14:43,070 --> 00:14:48,200
deployment because we measured this area

00:14:45,830 --> 00:14:50,330
as very low risk if you're really

00:14:48,200 --> 00:14:53,910
curious about the missing three points

00:14:50,330 --> 00:14:57,900
in the region we can click on the link

00:14:53,910 --> 00:15:00,240
look at the report to get more detail so

00:14:57,900 --> 00:15:01,980
here's an example of a kinnor report

00:15:00,240 --> 00:15:05,790
that corresponds the previous pipeline

00:15:01,980 --> 00:15:08,280
this is the report for the EU region in

00:15:05,790 --> 00:15:10,550
our canary report we list out the

00:15:08,280 --> 00:15:13,050
metrics and the differences between them

00:15:10,550 --> 00:15:15,750
metrics can be different in that they're

00:15:13,050 --> 00:15:17,430
either high or low and that meaning of

00:15:15,750 --> 00:15:19,080
the Highness or lowness really depends

00:15:17,430 --> 00:15:21,540
on the metric for example if we're

00:15:19,080 --> 00:15:23,550
measuring success and success is low in

00:15:21,540 --> 00:15:25,320
the canary then that's bad but if we're

00:15:23,550 --> 00:15:27,960
measuring errors and errors are higher

00:15:25,320 --> 00:15:29,190
in the canary then that's bad so by

00:15:27,960 --> 00:15:30,480
looking at this report you can kind of

00:15:29,190 --> 00:15:31,890
separate out which ones are high and

00:15:30,480 --> 00:15:34,200
which ones are low and make some sense

00:15:31,890 --> 00:15:36,240
of that in this particular instance

00:15:34,200 --> 00:15:38,940
looking at the high metrics we see that

00:15:36,240 --> 00:15:42,600
the right latency deviated from the

00:15:38,940 --> 00:15:44,910
canary to the baseline by 16.9% but this

00:15:42,600 --> 00:15:47,490
is just one metric out of many and so it

00:15:44,910 --> 00:15:49,680
only lowered the score to 97 which we

00:15:47,490 --> 00:15:53,100
feel is good enough to go forward with

00:15:49,680 --> 00:15:56,190
this in contrast to the successful

00:15:53,100 --> 00:15:59,550
canary we also had a failed canary

00:15:56,190 --> 00:16:02,760
pipeline in this example you can see we

00:15:59,550 --> 00:16:04,470
in canary in three regions but it did

00:16:02,760 --> 00:16:07,650
really quite miserable in all three

00:16:04,470 --> 00:16:11,550
regions a zero score across the board

00:16:07,650 --> 00:16:13,200
and so in this case the the pipeline

00:16:11,550 --> 00:16:15,480
would really stop at this stage and not

00:16:13,200 --> 00:16:17,460
proceed any further because this is

00:16:15,480 --> 00:16:20,460
clearly a bill that we do not want to

00:16:17,460 --> 00:16:21,990
put out in production again so at this

00:16:20,460 --> 00:16:23,790
point you know spinnaker would send

00:16:21,990 --> 00:16:26,250
notifications to the developers or

00:16:23,790 --> 00:16:29,400
operators via slack or email or however

00:16:26,250 --> 00:16:30,930
they configured paging and the operator

00:16:29,400 --> 00:16:34,080
could then come in again look at the

00:16:30,930 --> 00:16:36,570
canary report and get some idea of what

00:16:34,080 --> 00:16:38,250
went wrong this is the canary report

00:16:36,570 --> 00:16:40,560
corresponding to one of the regions in

00:16:38,250 --> 00:16:42,720
the previous pipeline you can see it's

00:16:40,560 --> 00:16:45,210
really it has a lot of problems a number

00:16:42,720 --> 00:16:48,000
of metrics are through the roof this is

00:16:45,210 --> 00:16:50,160
this is clearly got problems with the

00:16:48,000 --> 00:16:51,540
build if we had gone forward and push

00:16:50,160 --> 00:16:54,150
this to production it would have caused

00:16:51,540 --> 00:16:55,560
customer impact alerts would have gone

00:16:54,150 --> 00:16:57,750
off we would have scrambled to roll it

00:16:55,560 --> 00:16:59,310
back and undo the change and so by

00:16:57,750 --> 00:17:02,490
stopping the canary at this point we'd

00:16:59,310 --> 00:17:06,110
really sort of prevent ourselves from

00:17:02,490 --> 00:17:06,110
some production incident

00:17:06,970 --> 00:17:11,120
okay so that sort of represents the

00:17:09,380 --> 00:17:13,520
current stage of Canaries here at

00:17:11,120 --> 00:17:15,020
Netflix or watch the typical uses of it

00:17:13,520 --> 00:17:16,670
now Chris is now going to talk a little

00:17:15,020 --> 00:17:26,900
bit about some alternative uses that

00:17:16,670 --> 00:17:28,490
we've come up with Netflix reference so

00:17:26,900 --> 00:17:30,530
we've been able to realize the power of

00:17:28,490 --> 00:17:34,910
both the spinnaker and canary analysis

00:17:30,530 --> 00:17:36,380
in other areas of Netflix and one of

00:17:34,910 --> 00:17:38,120
those areas we've been all to leverage

00:17:36,380 --> 00:17:39,830
these technologies and tools is what we

00:17:38,120 --> 00:17:42,020
refer to as did cup developer Canaries

00:17:39,830 --> 00:17:44,960
this the idea of you as a developer

00:17:42,020 --> 00:17:46,670
you're working on your own branch with a

00:17:44,960 --> 00:17:49,610
developer care you can push that branch

00:17:46,670 --> 00:17:51,890
up into say a build tool get it deployed

00:17:49,610 --> 00:17:53,420
into its own little canary cluster and

00:17:51,890 --> 00:17:53,800
then performs the canary analysis on

00:17:53,420 --> 00:17:56,000
that

00:17:53,800 --> 00:17:58,160
this means that use a developer can get

00:17:56,000 --> 00:18:00,050
a really sense of how does your canary

00:17:58,160 --> 00:18:02,090
build so you're good Albertville differ

00:18:00,050 --> 00:18:04,820
than say the baseline or the production

00:18:02,090 --> 00:18:06,620
build this also allows you to not wait

00:18:04,820 --> 00:18:08,660
for your build to get merged into say

00:18:06,620 --> 00:18:10,250
master or mainline then deployed and

00:18:08,660 --> 00:18:12,580
then to canary so can it can increase

00:18:10,250 --> 00:18:15,470
velocity at an individual development

00:18:12,580 --> 00:18:18,470
level as well it's important to note

00:18:15,470 --> 00:18:21,830
here that this pipeline is spinnaker it

00:18:18,470 --> 00:18:23,330
ends with the canary stage and so in

00:18:21,830 --> 00:18:25,220
this case you know the end is a canary

00:18:23,330 --> 00:18:27,980
report or a canary score this assessment

00:18:25,220 --> 00:18:29,510
of risk and then from that point on you

00:18:27,980 --> 00:18:30,980
know if things are looking good then the

00:18:29,510 --> 00:18:32,960
developer can make a decision to you

00:18:30,980 --> 00:18:34,940
emerge into masteren and so on but this

00:18:32,960 --> 00:18:38,990
pipeline with the adult Canaries ends up

00:18:34,940 --> 00:18:40,850
carry stage another example where we

00:18:38,990 --> 00:18:42,830
kind of leverage this technology and

00:18:40,850 --> 00:18:46,330
these concepts is in what we call it

00:18:42,830 --> 00:18:49,490
past property area as well and so for

00:18:46,330 --> 00:18:51,230
for some context a fast property you can

00:18:49,490 --> 00:18:53,390
think of it as a as a dynamic feature

00:18:51,230 --> 00:18:55,040
play in this case you know you wouldn't

00:18:53,390 --> 00:18:57,050
need to restart an instant or redeploy

00:18:55,040 --> 00:18:58,780
your rebake your application of the user

00:18:57,050 --> 00:19:00,830
comes dynamic and set them on the fly

00:18:58,780 --> 00:19:02,900
but these can potentially cause impacts

00:19:00,830 --> 00:19:04,910
you can enable them and cause you know

00:19:02,900 --> 00:19:06,980
outages and so on so we'd be nice to you

00:19:04,910 --> 00:19:08,510
know be able to canary these so with the

00:19:06,980 --> 00:19:11,360
Netflix we have the ability to you know

00:19:08,510 --> 00:19:13,460
have a fast property carry pipeline in

00:19:11,360 --> 00:19:14,570
which case you take a property and

00:19:13,460 --> 00:19:16,550
you're gonna enable it but it's going to

00:19:14,570 --> 00:19:18,530
get enabled onto a small canary build so

00:19:16,550 --> 00:19:20,060
and then very selective developer

00:19:18,530 --> 00:19:21,500
Karrie that's going to carry and we're

00:19:20,060 --> 00:19:23,120
gonna do that analysis then we're gonna

00:19:21,500 --> 00:19:25,460
judge and what would the impact be on

00:19:23,120 --> 00:19:27,920
enabling this feature flag or this

00:19:25,460 --> 00:19:29,870
feature for all of the users I want to

00:19:27,920 --> 00:19:31,460
get in this this pipeline ends up the

00:19:29,870 --> 00:19:35,980
canary stage again generating a canary

00:19:31,460 --> 00:19:39,230
score or a result some other use cases

00:19:35,980 --> 00:19:40,880
more notable use cases are we've able to

00:19:39,230 --> 00:19:44,600
leverage just Kerry Kerry analysis in

00:19:40,880 --> 00:19:46,280
general for things around our open

00:19:44,600 --> 00:19:47,630
connect team in our open connect team is

00:19:46,280 --> 00:19:50,390
responsible for the content delivery

00:19:47,630 --> 00:19:52,820
network they are the the caches that

00:19:50,390 --> 00:19:54,560
stream the movies to your devices and

00:19:52,820 --> 00:19:57,020
they're all worldwide and so the open

00:19:54,560 --> 00:19:58,760
connect team can use Terry analysis to

00:19:57,020 --> 00:20:00,980
look at what is the impact of upgrading

00:19:58,760 --> 00:20:03,620
the firmware of these caches around the

00:20:00,980 --> 00:20:06,590
world know the use case that we have

00:20:03,620 --> 00:20:08,500
found carry analysis in is with our work

00:20:06,590 --> 00:20:12,230
with the Chaos automation platform

00:20:08,500 --> 00:20:15,560
chapter for short so chap is a platform

00:20:12,230 --> 00:20:17,210
that is injecting a kind of failure into

00:20:15,560 --> 00:20:19,190
our marker service infrastructure and

00:20:17,210 --> 00:20:22,220
then with the integral can be more

00:20:19,190 --> 00:20:24,050
resilient so within chap they use

00:20:22,220 --> 00:20:26,600
automated carry analysis or just carry

00:20:24,050 --> 00:20:29,210
analysis to kind of analyze the results

00:20:26,600 --> 00:20:31,400
of a of a chap run or experiment

00:20:29,210 --> 00:20:35,780
determine if the the injected failure

00:20:31,400 --> 00:20:37,640
has any undesirable impacts so these are

00:20:35,780 --> 00:20:40,370
something just kind of beyond the your

00:20:37,640 --> 00:20:42,110
standard server deployment where we have

00:20:40,370 --> 00:20:43,940
seen value in doing kind of a canary

00:20:42,110 --> 00:20:49,250
release and also automating and doing

00:20:43,940 --> 00:20:51,890
the analysis okay now that we've talked

00:20:49,250 --> 00:20:53,690
a little bit about what we currently do

00:20:51,890 --> 00:20:55,310
here at Netflix we'd like to share some

00:20:53,690 --> 00:20:58,760
of the best practices that we've learned

00:20:55,310 --> 00:21:00,470
along the way and some of those have to

00:20:58,760 --> 00:21:02,930
do with how you configure your pipelines

00:21:00,470 --> 00:21:06,580
how you configure your Canaries and the

00:21:02,930 --> 00:21:09,140
parameters each is his first one

00:21:06,580 --> 00:21:11,210
spinnaker offers the ability to scale up

00:21:09,140 --> 00:21:13,700
your Canaries and why would you want to

00:21:11,210 --> 00:21:15,680
do this well as we said before Canaries

00:21:13,700 --> 00:21:17,090
are all about minimizing risk so you may

00:21:15,680 --> 00:21:20,180
want to start with the very very small

00:21:17,090 --> 00:21:22,100
canary cluster get some validation on

00:21:20,180 --> 00:21:23,680
that and then scale it up to a larger

00:21:22,100 --> 00:21:26,350
cluster to take more traffic

00:21:23,680 --> 00:21:28,840
so in spinnaker under the canary

00:21:26,350 --> 00:21:31,090
configuration you can go in check the

00:21:28,840 --> 00:21:32,680
box tell it how long you want to wait

00:21:31,090 --> 00:21:34,900
before the scale up and then also

00:21:32,680 --> 00:21:36,940
specify what the final size will be a

00:21:34,900 --> 00:21:39,100
spinnaker will wait that long and then

00:21:36,940 --> 00:21:40,660
scale it up and along the way it will

00:21:39,100 --> 00:21:42,610
perform canary analysis to make sure

00:21:40,660 --> 00:21:44,790
that you know the small one is doing

00:21:42,610 --> 00:21:49,390
well before you scale up to larger

00:21:44,790 --> 00:21:52,000
cluster size something else to consider

00:21:49,390 --> 00:21:54,700
is whether you are using too large of a

00:21:52,000 --> 00:21:57,250
canary cluster in this particular

00:21:54,700 --> 00:21:59,080
example and this is a case we run too

00:21:57,250 --> 00:22:02,530
often you see the main cluster at the

00:21:59,080 --> 00:22:04,840
top is six instances so it's a pretty

00:22:02,530 --> 00:22:07,210
small cluster well if which is a canary

00:22:04,840 --> 00:22:08,920
size of three instances that means 25%

00:22:07,210 --> 00:22:11,830
of our traffic is going to the canary

00:22:08,920 --> 00:22:13,330
and what that really means is 25% of our

00:22:11,830 --> 00:22:15,880
customers are exposed to this

00:22:13,330 --> 00:22:18,520
potentially down build so again you know

00:22:15,880 --> 00:22:20,920
this this is not really reducing risk in

00:22:18,520 --> 00:22:22,660
the way we'd hoped a better solution

00:22:20,920 --> 00:22:24,640
would be to use a single instance or

00:22:22,660 --> 00:22:26,860
else put it behind a load balancer epoxy

00:22:24,640 --> 00:22:28,720
where we can steer even a smaller

00:22:26,860 --> 00:22:35,860
percentage of traffic into that canary

00:22:28,720 --> 00:22:39,610
cluster when do when do canary is also a

00:22:35,860 --> 00:22:41,470
consideration this is a example of our

00:22:39,610 --> 00:22:42,940
daily traffic cycle you can see there's

00:22:41,470 --> 00:22:44,200
a peak in the evening when people are

00:22:42,940 --> 00:22:45,760
home watching movies and there's a

00:22:44,200 --> 00:22:48,580
trough in the middle night when people

00:22:45,760 --> 00:22:49,900
are asleep except for those middle of

00:22:48,580 --> 00:22:54,100
the night people like myself like to

00:22:49,900 --> 00:22:56,350
watch Louisa - yeah and while working so

00:22:54,100 --> 00:22:58,060
you may you may feel well I want to

00:22:56,350 --> 00:23:00,190
minimize my risk so why don't I just

00:22:58,060 --> 00:23:02,350
canary during that two a.m. trough that

00:23:00,190 --> 00:23:04,270
way fewer customers are exposed to this

00:23:02,350 --> 00:23:07,180
new build well the question to ask

00:23:04,270 --> 00:23:08,410
yourself is is that - am trough traffic

00:23:07,180 --> 00:23:10,270
really representative of the work

00:23:08,410 --> 00:23:11,410
shouldn't be seen during the day you

00:23:10,270 --> 00:23:13,330
know some of that traffic made the

00:23:11,410 --> 00:23:15,430
devices that are left on overnight and

00:23:13,330 --> 00:23:17,950
it's doing some background tasks and

00:23:15,430 --> 00:23:19,540
things it may not be the same mix of

00:23:17,950 --> 00:23:22,150
requests that we're gonna see during our

00:23:19,540 --> 00:23:23,710
peak hours and silly-- that may just not

00:23:22,150 --> 00:23:26,100
be enough traffic during that time to

00:23:23,710 --> 00:23:29,530
really make accurate determination so

00:23:26,100 --> 00:23:31,030
one thing we try to do is do our

00:23:29,530 --> 00:23:34,350
Canaries during the day when we're

00:23:31,030 --> 00:23:36,299
seeing typical regular normal traffic

00:23:34,350 --> 00:23:40,110
there's a couple of techniques you can

00:23:36,299 --> 00:23:41,490
do with spinnaker one thing we do is put

00:23:40,110 --> 00:23:43,110
a prong trigger on in front of our

00:23:41,490 --> 00:23:45,899
pipelines so they start off during the

00:23:43,110 --> 00:23:47,820
day maybe every morning or at noon or so

00:23:45,899 --> 00:23:50,880
and that way they're running during the

00:23:47,820 --> 00:23:52,380
main hours of the day or something else

00:23:50,880 --> 00:23:54,059
you can do is on the canary stage you

00:23:52,380 --> 00:23:55,529
can put in execution windows up set the

00:23:54,059 --> 00:23:57,659
pipeline we'll stop and wait for that we

00:23:55,529 --> 00:24:04,340
know that Dobin up before starting your

00:23:57,659 --> 00:24:07,129
Canaries warming up Canaries and more

00:24:04,340 --> 00:24:09,899
specifically warming up the instances

00:24:07,129 --> 00:24:12,299
some services they run really hot when

00:24:09,899 --> 00:24:14,789
they're first launched because they're

00:24:12,299 --> 00:24:16,350
busy filling caches establishing

00:24:14,789 --> 00:24:19,620
connection pools doing a lot of garbage

00:24:16,350 --> 00:24:21,240
collection and so their performance

00:24:19,620 --> 00:24:22,919
characteristics that show at the start

00:24:21,240 --> 00:24:25,889
of it's not really typical of what you

00:24:22,919 --> 00:24:28,320
would see during a steady state so in

00:24:25,889 --> 00:24:31,740
spinnaker you can go and tell it hey you

00:24:28,320 --> 00:24:33,659
know my servers need 20 minutes 10

00:24:31,740 --> 00:24:35,340
minutes whatever to start up so don't

00:24:33,659 --> 00:24:36,809
start any kind of canary measurement

00:24:35,340 --> 00:24:39,269
until after that startup idiot

00:24:36,809 --> 00:24:40,950
how long you choose really depends on

00:24:39,269 --> 00:24:42,149
your service some of our services start

00:24:40,950 --> 00:24:44,669
you in two minutes

00:24:42,149 --> 00:24:47,039
some actually take 30 minutes to startup

00:24:44,669 --> 00:24:49,049
is they've got so much work to do in

00:24:47,039 --> 00:24:51,809
this particular example you know you see

00:24:49,049 --> 00:24:53,789
the server started at 416 and by about

00:24:51,809 --> 00:24:56,129
422 it had settled down to a pretty

00:24:53,789 --> 00:24:57,990
stable steady state so we just still

00:24:56,129 --> 00:25:00,389
spinnaker hey you know wait 10 minutes

00:24:57,990 --> 00:25:03,750
and then start your data collection for

00:25:00,389 --> 00:25:06,299
the analysis again this way it really

00:25:03,750 --> 00:25:08,429
gives you a better measurement of what

00:25:06,299 --> 00:25:16,110
your typical server behavior will look

00:25:08,429 --> 00:25:17,669
like I have before the chaos monkey so a

00:25:16,110 --> 00:25:19,799
couple years ago as we were doing

00:25:17,669 --> 00:25:21,960
Canaries we have we noticed these odd

00:25:19,799 --> 00:25:24,299
behaviors canary instances will suddenly

00:25:21,960 --> 00:25:26,669
just terminate and disappear of course

00:25:24,299 --> 00:25:29,840
they get replaced but it was very random

00:25:26,669 --> 00:25:34,259
and we can never really reproduce it and

00:25:29,840 --> 00:25:36,960
the problem is it throws big dip into

00:25:34,259 --> 00:25:38,669
your canary data as one instance

00:25:36,960 --> 00:25:42,179
disappears and there's a gap before the

00:25:38,669 --> 00:25:44,010
next one takes its place and sometimes

00:25:42,179 --> 00:25:45,450
the gap would be in the canary cluster

00:25:44,010 --> 00:25:47,200
only or sometimes the baseline or

00:25:45,450 --> 00:25:48,880
sometimes both were the different times

00:25:47,200 --> 00:25:51,160
and we're really puzzled what was going

00:25:48,880 --> 00:25:52,720
on until we finally figured out months

00:25:51,160 --> 00:25:55,990
later that it was the chaos monkey doing

00:25:52,720 --> 00:25:58,330
his job the chaos monkey is a system

00:25:55,990 --> 00:26:00,550
whose sole purpose is to take out random

00:25:58,330 --> 00:26:02,680
instances from our service and therefore

00:26:00,550 --> 00:26:04,540
therefore it allows us to sort of

00:26:02,680 --> 00:26:06,310
validate the resiliency of our service

00:26:04,540 --> 00:26:08,860
we're resilient against losing

00:26:06,310 --> 00:26:11,740
individual instances but that can really

00:26:08,860 --> 00:26:14,790
play havoc with here in Canary data so

00:26:11,740 --> 00:26:17,410
in this case we felt it was a better

00:26:14,790 --> 00:26:18,880
solution to just tell chaos monkey hey

00:26:17,410 --> 00:26:21,430
you know ignore our canary and waste

00:26:18,880 --> 00:26:22,870
lines don't you don't see them go go

00:26:21,430 --> 00:26:24,610
shoot down there the instances elsewhere

00:26:22,870 --> 00:26:26,650
because we want to get a really good

00:26:24,610 --> 00:26:32,680
measurement without these mysterious

00:26:26,650 --> 00:26:34,060
gaps in the data and finally something

00:26:32,680 --> 00:26:35,260
else that we've kind of evolved here you

00:26:34,060 --> 00:26:37,210
know when we first start doing these a

00:26:35,260 --> 00:26:38,980
canary these canary pipelines and

00:26:37,210 --> 00:26:42,010
spinnaker we're very pedantic about it

00:26:38,980 --> 00:26:44,200
if your canary didn't pass that was it

00:26:42,010 --> 00:26:46,150
you got no second chance and start all

00:26:44,200 --> 00:26:48,450
over again obviously this is really

00:26:46,150 --> 00:26:51,760
frustrating frustrating operators and

00:26:48,450 --> 00:26:55,330
killed our productivity our development

00:26:51,760 --> 00:26:56,710
velocity because you know people have to

00:26:55,330 --> 00:26:58,300
either restart the canary or they

00:26:56,710 --> 00:27:01,240
grumble about it and wait till the next

00:26:58,300 --> 00:27:03,700
day or wait till someone else bit the

00:27:01,240 --> 00:27:07,180
bullet and started the canary instead so

00:27:03,700 --> 00:27:09,970
we sort of arrived at a compromise and

00:27:07,180 --> 00:27:12,970
in this case if our Canaries succeed and

00:27:09,970 --> 00:27:14,560
all successful then the manual judgment

00:27:12,970 --> 00:27:16,690
stage is really optional it'll it'll

00:27:14,560 --> 00:27:19,150
skip spinnaker will bypass that and goes

00:27:16,690 --> 00:27:22,390
directly into the deployment if our

00:27:19,150 --> 00:27:24,870
Canaries fail then spinnaker will

00:27:22,390 --> 00:27:27,100
continue on to the manual judgment stage

00:27:24,870 --> 00:27:29,080
which will notify our you know

00:27:27,100 --> 00:27:31,900
developers and operators via slack or

00:27:29,080 --> 00:27:35,140
whatever and so on an engineer come in

00:27:31,900 --> 00:27:36,940
look at the canary scores let me look at

00:27:35,140 --> 00:27:38,320
the additional data they need and decide

00:27:36,940 --> 00:27:39,700
you know all right this is this was

00:27:38,320 --> 00:27:41,440
pretty good there was maybe a little

00:27:39,700 --> 00:27:45,160
anomaly or glitch in the data that

00:27:41,440 --> 00:27:47,980
didn't get cleaned out and so I feel you

00:27:45,160 --> 00:27:49,990
know it's my opinion that this is a safe

00:27:47,980 --> 00:27:52,480
release to put out there in the world so

00:27:49,990 --> 00:27:53,850
it will prove on the pipeline and it

00:27:52,480 --> 00:27:56,240
will proceed on to the deployment

00:27:53,850 --> 00:27:58,640
portion of the pipeline

00:27:56,240 --> 00:28:00,440
in this particular example you can see

00:27:58,640 --> 00:28:02,570
we have to have a pipeline that has

00:28:00,440 --> 00:28:05,780
three separate canary stages configured

00:28:02,570 --> 00:28:07,370
our critical stage our critical canary

00:28:05,780 --> 00:28:09,770
which corresponds to maybe one

00:28:07,370 --> 00:28:12,200
particular farm he did really well I got

00:28:09,770 --> 00:28:14,240
a hundred score are not critical well if

00:28:12,200 --> 00:28:17,060
he was 89 but that's still good enough

00:28:14,240 --> 00:28:18,860
to pass but if you look at the modern 88

00:28:17,060 --> 00:28:22,310
he just missed the cutoff by one point

00:28:18,860 --> 00:28:24,430
you know so instead of just failing this

00:28:22,310 --> 00:28:26,660
whole canary and stopping everything the

00:28:24,430 --> 00:28:28,640
developer can go and take a look at it

00:28:26,660 --> 00:28:30,590
say yeah it was off by one and I can I

00:28:28,640 --> 00:28:33,170
have a good explanation for why that one

00:28:30,590 --> 00:28:34,910
was missing so I'm gonna prove this to

00:28:33,170 --> 00:28:36,500
go for you notice it comes back down to

00:28:34,910 --> 00:28:39,050
the Netflix freedom and responsibility

00:28:36,500 --> 00:28:42,230
you're free to make that decision and

00:28:39,050 --> 00:28:45,110
you're responsible for the action or the

00:28:42,230 --> 00:28:46,820
consequences okay so these are some of

00:28:45,110 --> 00:28:49,910
the practices that we've learned around

00:28:46,820 --> 00:28:51,680
getting figuring and canary selection

00:28:49,910 --> 00:28:56,600
Chris is gonna talk a little bit about

00:28:51,680 --> 00:29:02,150
some of the practices we thank you very

00:28:56,600 --> 00:29:06,560
much so when it comes to observability

00:29:02,150 --> 00:29:07,610
and kind of doing the canary analysis it

00:29:06,560 --> 00:29:11,990
really doesn't matter what you put into

00:29:07,610 --> 00:29:13,670
the system so metrics selection becomes

00:29:11,990 --> 00:29:15,320
a very critical part to making sure that

00:29:13,670 --> 00:29:17,630
you get accurate results from your

00:29:15,320 --> 00:29:19,130
canary analysis system if you give it

00:29:17,630 --> 00:29:22,160
garbage in you're gonna get almost like

00:29:19,130 --> 00:29:23,840
a garbage joke now whether this metric

00:29:22,160 --> 00:29:25,970
selection process is automated through

00:29:23,840 --> 00:29:28,730
some machine learning approach or if

00:29:25,970 --> 00:29:31,040
it's you know guided by service owners

00:29:28,730 --> 00:29:32,990
or self-service tool now it really does

00:29:31,040 --> 00:29:35,840
matter what you put into it

00:29:32,990 --> 00:29:37,810
and so picking these metrics it can be

00:29:35,840 --> 00:29:40,520
crucial it can be the difference between

00:29:37,810 --> 00:29:43,070
having a successful canary pipeline or

00:29:40,520 --> 00:29:46,910
having a non successful canary pipeline

00:29:43,070 --> 00:29:49,040
and there are some types of metrics you

00:29:46,910 --> 00:29:50,240
probably ought to shy away from for

00:29:49,040 --> 00:29:52,010
example you probably shy weight and

00:29:50,240 --> 00:29:53,690
sparse type of metrics these are metrics

00:29:52,010 --> 00:29:56,240
that have a low reporting interval

00:29:53,690 --> 00:29:58,700
they're very infrequent in the reporting

00:29:56,240 --> 00:30:00,110
and these are just kind of noisy type

00:29:58,700 --> 00:30:02,240
signals these can be signals that are

00:30:00,110 --> 00:30:03,140
hard to analyze make judgments on you

00:30:02,240 --> 00:30:04,310
might want to stay away from them

00:30:03,140 --> 00:30:07,190
because they can give you false

00:30:04,310 --> 00:30:08,270
positives in your system other signals

00:30:07,190 --> 00:30:09,690
you may want to stay away from our

00:30:08,270 --> 00:30:12,299
signals that are going to be all

00:30:09,690 --> 00:30:13,710
is different between Macario baseline so

00:30:12,299 --> 00:30:14,909
you can you know imagine that you have

00:30:13,710 --> 00:30:16,710
some metrics that relate to some

00:30:14,909 --> 00:30:18,990
configuration items between the two

00:30:16,710 --> 00:30:19,889
obvious metrics you want to stay away

00:30:18,990 --> 00:30:21,029
from because they're always gonna be

00:30:19,889 --> 00:30:23,009
different they're not contributing

00:30:21,029 --> 00:30:26,669
anything to the canary or even the score

00:30:23,009 --> 00:30:27,750
now kind of in conscious that there are

00:30:26,669 --> 00:30:29,429
some metrics you probably want to

00:30:27,750 --> 00:30:31,409
include in your carry analysis and these

00:30:29,429 --> 00:30:33,210
are things like error metrics or the

00:30:31,409 --> 00:30:35,490
presence of failures or presence of

00:30:33,210 --> 00:30:36,720
errors in your system at these these can

00:30:35,490 --> 00:30:38,399
be you know an easy indicator that

00:30:36,720 --> 00:30:39,330
there's a problem in your system and as

00:30:38,399 --> 00:30:41,190
you can see in the graph on the right

00:30:39,330 --> 00:30:44,279
you know it can be easy to determine

00:30:41,190 --> 00:30:45,840
that there is a deficient so these are

00:30:44,279 --> 00:30:48,629
some categories on metrics that you may

00:30:45,840 --> 00:30:50,129
want to exclude or you know look at but

00:30:48,629 --> 00:30:51,600
we can take it a step further and we can

00:30:50,129 --> 00:30:52,679
think about you know these transforms

00:30:51,600 --> 00:30:54,840
that we want to also supply their

00:30:52,679 --> 00:30:56,549
metrics maybe of transforms is we want

00:30:54,840 --> 00:30:59,909
to pull the gonna be signal from the

00:30:56,549 --> 00:31:01,350
noise so one example is that there might

00:30:59,909 --> 00:31:03,149
be some natural variation in your load

00:31:01,350 --> 00:31:06,409
bouncing patterns or your load balancing

00:31:03,149 --> 00:31:08,309
application and that means your canary

00:31:06,409 --> 00:31:10,080
cluster or your baseline customer may

00:31:08,309 --> 00:31:13,139
take more or less traffic and it may not

00:31:10,080 --> 00:31:14,909
be actually 50/50 split and to account

00:31:13,139 --> 00:31:16,950
for this what you can do is you can

00:31:14,909 --> 00:31:20,009
transform the metrics we can do what we

00:31:16,950 --> 00:31:21,899
call a metric normalization in this case

00:31:20,009 --> 00:31:24,720
we could take a year CPU utilization

00:31:21,899 --> 00:31:26,460
metric here the week and then look at

00:31:24,720 --> 00:31:28,710
say they requests per second and we can

00:31:26,460 --> 00:31:30,539
divide the CPU utilization by the

00:31:28,710 --> 00:31:32,490
requests per second in essence we're

00:31:30,539 --> 00:31:35,309
creating a ratio here now what does the

00:31:32,490 --> 00:31:36,450
CPU per unit of work we are doing and

00:31:35,309 --> 00:31:38,490
this allows us to account for those

00:31:36,450 --> 00:31:40,889
little variations in the metrics in

00:31:38,490 --> 00:31:43,980
anything coming to our system and help

00:31:40,889 --> 00:31:47,159
us give a more accurate analysis of the

00:31:43,980 --> 00:31:48,960
of the canary now there are so much you

00:31:47,159 --> 00:31:50,220
won't want to do this for obviously if

00:31:48,960 --> 00:31:52,080
you already have a metric that is a

00:31:50,220 --> 00:31:53,600
ratio or a rate you won't want to do

00:31:52,080 --> 00:31:57,149
this type of normalization or

00:31:53,600 --> 00:31:59,460
calculation but there are other types of

00:31:57,149 --> 00:32:00,779
transforms we can do on top of that and

00:31:59,460 --> 00:32:02,279
this is an example where you know you

00:32:00,779 --> 00:32:04,649
maybe you want to apply a very simple

00:32:02,279 --> 00:32:06,809
transform of smoothing or a sort of

00:32:04,649 --> 00:32:08,639
trend calculation on it so the example

00:32:06,809 --> 00:32:10,590
on the left shows us our CPU utilization

00:32:08,639 --> 00:32:12,149
for some care and baseline cluster and

00:32:10,590 --> 00:32:15,419
you know you might look at that and say

00:32:12,149 --> 00:32:16,889
ah they're pretty good but if we apply a

00:32:15,419 --> 00:32:18,779
very simple moving average to and you

00:32:16,889 --> 00:32:20,279
look at the graph on the right you can

00:32:18,779 --> 00:32:21,869
see well once we apply the moving

00:32:20,279 --> 00:32:22,580
average they may actually be a little

00:32:21,869 --> 00:32:24,890
bit different

00:32:22,580 --> 00:32:26,360
so it can be important to apply be such

00:32:24,890 --> 00:32:29,690
to transforms to help pull up that

00:32:26,360 --> 00:32:32,810
signal from the noise and then finally

00:32:29,690 --> 00:32:34,190
not all metrics are created equally it's

00:32:32,810 --> 00:32:35,840
important it's important that you know a

00:32:34,190 --> 00:32:37,820
system allow you to be able to define

00:32:35,840 --> 00:32:39,230
importance in terms of them which

00:32:37,820 --> 00:32:41,810
metrics are more important than others

00:32:39,230 --> 00:32:43,550
so for example maybe error metrics have

00:32:41,810 --> 00:32:45,260
more weight to you or more importance to

00:32:43,550 --> 00:32:46,670
you than your system metrics and by NC

00:32:45,260 --> 00:32:49,430
measures so this is really taken into

00:32:46,670 --> 00:32:51,230
consideration in your system and one way

00:32:49,430 --> 00:32:53,120
you can do that is to apply some type of

00:32:51,230 --> 00:32:54,440
waiting so let's say in this case our

00:32:53,120 --> 00:32:56,990
error metrics are getting to get some

00:32:54,440 --> 00:32:58,370
type of like weight 50 percent and then

00:32:56,990 --> 00:33:00,950
the other two metric categories are

00:32:58,370 --> 00:33:02,480
gonna get say equal 25 percent and then

00:33:00,950 --> 00:33:03,770
when we do the score calculation our

00:33:02,480 --> 00:33:06,830
score is going to be calculated based on

00:33:03,770 --> 00:33:08,270
the weights of these categories now that

00:33:06,830 --> 00:33:10,790
leads us to an interesting point is

00:33:08,270 --> 00:33:13,280
we're getting more complex now and so

00:33:10,790 --> 00:33:14,750
that means I've explained ability or

00:33:13,280 --> 00:33:17,270
what we refer to as explained ability is

00:33:14,750 --> 00:33:19,010
very important and so what explain

00:33:17,270 --> 00:33:20,990
ability refers to is why did we create a

00:33:19,010 --> 00:33:23,420
judgment why do we end up at a score

00:33:20,990 --> 00:33:25,010
that we did and this is important for

00:33:23,420 --> 00:33:26,990
our operators because it builds trust in

00:33:25,010 --> 00:33:28,520
our system is if they can look at our

00:33:26,990 --> 00:33:30,200
report and make a look at why a canary

00:33:28,520 --> 00:33:32,480
had a certain judgment they build trust

00:33:30,200 --> 00:33:35,180
in it and they can rationalize why I've

00:33:32,480 --> 00:33:37,310
got that school so we go back in time

00:33:35,180 --> 00:33:40,160
really long time ago this was our

00:33:37,310 --> 00:33:42,020
original canary analysis report from

00:33:40,160 --> 00:33:45,320
many years ago you couldn't see the

00:33:42,020 --> 00:33:47,120
score at the top left there and this is

00:33:45,320 --> 00:33:49,460
what our operators would normally see

00:33:47,120 --> 00:33:50,810
many many years ago and it contains a

00:33:49,460 --> 00:33:52,610
lot of information there's a lot of

00:33:50,810 --> 00:33:53,990
diagnostic spots there's a lot of

00:33:52,610 --> 00:33:55,010
information in here but just because

00:33:53,990 --> 00:33:56,540
there's a lot of information doesn't

00:33:55,010 --> 00:33:58,730
mean that it's easily consumable and

00:33:56,540 --> 00:34:00,920
even that it explains what is going on

00:33:58,730 --> 00:34:02,420
so we took some lessons and over the

00:34:00,920 --> 00:34:04,700
years we've kind of improved upon this

00:34:02,420 --> 00:34:06,500
to generate our modern and canary report

00:34:04,700 --> 00:34:10,750
that looks like this we tried to you

00:34:06,500 --> 00:34:10,750
know reduce the information overload

00:34:11,110 --> 00:34:17,270
dead battery we try to reduce the

00:34:15,290 --> 00:34:19,550
information overload and also to focus

00:34:17,270 --> 00:34:22,610
on kind of the the explained ability of

00:34:19,550 --> 00:34:24,800
why we ended up at a result we did so if

00:34:22,610 --> 00:34:26,720
we zoom in and look at some of the you

00:34:24,800 --> 00:34:29,060
know the core concepts in this care

00:34:26,720 --> 00:34:30,620
report we can see that you know we we

00:34:29,060 --> 00:34:32,840
offered graphs we offer the kind of

00:34:30,620 --> 00:34:34,250
visualization of the raw input signals

00:34:32,840 --> 00:34:35,909
this can help these service owners or

00:34:34,250 --> 00:34:37,530
operators rationalize what was going

00:34:35,909 --> 00:34:39,179
into the system and how did it put it

00:34:37,530 --> 00:34:41,280
look and how does it how's it feel to

00:34:39,179 --> 00:34:43,169
them in addition we try to offer you

00:34:41,280 --> 00:34:44,940
human readable descriptions of why we

00:34:43,169 --> 00:34:46,889
arrived at a conclusion I'm so on the

00:34:44,940 --> 00:34:49,169
top right we can see that some canary

00:34:46,889 --> 00:34:51,210
failed they got a score of zero and

00:34:49,169 --> 00:34:53,909
we're saying that it failed because the

00:34:51,210 --> 00:34:57,329
uncaught application errors exceeded

00:34:53,909 --> 00:34:59,339
some threshold 100% in addition we also

00:34:57,329 --> 00:35:00,960
get more reason or classifications as we

00:34:59,339 --> 00:35:03,180
drill down into the individual metrics

00:35:00,960 --> 00:35:05,250
themselves so for any individual metric

00:35:03,180 --> 00:35:08,309
we give some reason or justification why

00:35:05,250 --> 00:35:11,089
it received a score or why doctors say

00:35:08,309 --> 00:35:11,089
classified differently

00:35:16,740 --> 00:35:20,670
finally under some practices we learned

00:35:18,900 --> 00:35:23,730
there's a couple of techniques you can

00:35:20,670 --> 00:35:26,309
use to validate your canary config now

00:35:23,730 --> 00:35:27,630
you set up your carry config you choose

00:35:26,309 --> 00:35:30,089
the metrics you want maybe you choose

00:35:27,630 --> 00:35:32,789
some waiting for those and how this is a

00:35:30,089 --> 00:35:35,520
good config well one technique you can

00:35:32,789 --> 00:35:37,260
use is a a testing and that is use the

00:35:35,520 --> 00:35:39,150
same build for both the canary and the

00:35:37,260 --> 00:35:41,760
baseline and when you run that through

00:35:39,150 --> 00:35:43,710
your canary analysis judgment it really

00:35:41,760 --> 00:35:44,880
should come up at a 100% score or very

00:35:43,710 --> 00:35:47,819
close to that and there should be

00:35:44,880 --> 00:35:50,039
repeatable if your score comes up you

00:35:47,819 --> 00:35:52,380
know like 60 or something lower

00:35:50,039 --> 00:35:54,349
well maybe even cluded some metrics that

00:35:52,380 --> 00:35:57,450
really don't accurately measure the

00:35:54,349 --> 00:36:00,569
behavior or performance or risk of your

00:35:57,450 --> 00:36:02,609
canary of your build itself once you

00:36:00,569 --> 00:36:04,140
have a pretty good what you feel is a

00:36:02,609 --> 00:36:06,359
pretty good reproducible canary

00:36:04,140 --> 00:36:09,119
configuration then you can take that and

00:36:06,359 --> 00:36:11,210
using that same AAA he can inject some

00:36:09,119 --> 00:36:13,260
false in to look into say the canary and

00:36:11,210 --> 00:36:16,230
validate that your canary config

00:36:13,260 --> 00:36:18,839
actually finds measures that fault for

00:36:16,230 --> 00:36:21,480
example you know we take the same bill

00:36:18,839 --> 00:36:24,539
use it for canary in baseline add some

00:36:21,480 --> 00:36:27,029
latency into the canary config so that

00:36:24,539 --> 00:36:28,619
latency should therefore be reflected in

00:36:27,029 --> 00:36:30,420
their final report you get you know it

00:36:28,619 --> 00:36:34,740
should say oh your score was 60% because

00:36:30,420 --> 00:36:37,289
latency was or you can add errors to the

00:36:34,740 --> 00:36:39,510
canary cluster and again validate that

00:36:37,289 --> 00:36:41,279
you're the final canary report report

00:36:39,510 --> 00:36:43,529
actually caught that and against

00:36:41,279 --> 00:36:47,210
judgment reflects that failure that you

00:36:43,529 --> 00:36:47,210
deliberately injected

00:36:47,490 --> 00:36:52,990
so this up to now we sort of presented

00:36:50,830 --> 00:36:55,000
the current state of canary analysis

00:36:52,990 --> 00:36:56,890
that Netflix and Chris is going to talk

00:36:55,000 --> 00:37:03,010
about some of the future work that were

00:36:56,890 --> 00:37:04,510
looking at so some some have used we're

00:37:03,010 --> 00:37:06,610
currently exploring kind of our future

00:37:04,510 --> 00:37:08,200
work right now is going back to that

00:37:06,610 --> 00:37:10,150
explain ability is trying to improve

00:37:08,200 --> 00:37:13,600
explain ability in terms of why we

00:37:10,150 --> 00:37:15,940
generated the resulting did and so long

00:37:13,600 --> 00:37:17,830
this kind of in this area we're looking

00:37:15,940 --> 00:37:19,510
at different visualization strategies we

00:37:17,830 --> 00:37:21,040
currently demonstrate you know the the

00:37:19,510 --> 00:37:22,540
time series or show the time series data

00:37:21,040 --> 00:37:24,070
but you know there are other different

00:37:22,540 --> 00:37:25,120
types of visualizations we can use as

00:37:24,070 --> 00:37:28,090
well maybe going back

00:37:25,120 --> 00:37:29,080
Mr Graham or or something like that in

00:37:28,090 --> 00:37:31,420
addition to you know giving different

00:37:29,080 --> 00:37:34,120
types of visualizations it's making the

00:37:31,420 --> 00:37:35,620
report even easier to consume and more

00:37:34,120 --> 00:37:37,690
integrated with things like spinnaker

00:37:35,620 --> 00:37:40,390
and arc and our developers workflows

00:37:37,690 --> 00:37:41,860
today to reduce that cognitive burden of

00:37:40,390 --> 00:37:44,590
switching and complex searching between

00:37:41,860 --> 00:37:46,300
different tools another area we're

00:37:44,590 --> 00:37:48,970
exploring is what I refer to as metric

00:37:46,300 --> 00:37:50,260
context and ontology is right now we

00:37:48,970 --> 00:37:51,880
have the squiggly line problem we're

00:37:50,260 --> 00:37:53,050
just looking at lines in a graph or

00:37:51,880 --> 00:37:55,090
we're looking at time series data

00:37:53,050 --> 00:37:57,070
there's a lot more we could do if we

00:37:55,090 --> 00:37:58,330
knew more about the metrics so for

00:37:57,070 --> 00:38:00,310
example if we knew a certain metric for

00:37:58,330 --> 00:38:01,510
analyzing its air power errors there's

00:38:00,310 --> 00:38:02,980
some interesting assumptions we can make

00:38:01,510 --> 00:38:05,680
about that we know that errors should

00:38:02,980 --> 00:38:08,140
Chimpy below zero or hopefully not be

00:38:05,680 --> 00:38:09,280
negative and from that we can then you

00:38:08,140 --> 00:38:10,960
know make certain assumptions for carry

00:38:09,280 --> 00:38:12,850
analysis that well the canary and maybe

00:38:10,960 --> 00:38:15,640
we only care about increases in errors

00:38:12,850 --> 00:38:16,810
on in our system and so with it's kind

00:38:15,640 --> 00:38:18,940
of this additional context of knowing

00:38:16,810 --> 00:38:20,350
what metrics are and a little bit about

00:38:18,940 --> 00:38:22,150
what they you know potentially their

00:38:20,350 --> 00:38:25,150
behavior we can maybe make better

00:38:22,150 --> 00:38:27,790
accurate decisions and then finally

00:38:25,150 --> 00:38:29,290
we're also rethinking complexity and

00:38:27,790 --> 00:38:31,540
this is a long multiple fronts right now

00:38:29,290 --> 00:38:33,850
there's a lot of complexity in terms of

00:38:31,540 --> 00:38:35,380
setting up the pipeline setting up the

00:38:33,850 --> 00:38:36,820
canary configuration and how can we

00:38:35,380 --> 00:38:38,410
reduce that complexity how can you

00:38:36,820 --> 00:38:40,450
reduce that so that our service owners

00:38:38,410 --> 00:38:42,970
don't have to be burdened with all this

00:38:40,450 --> 00:38:45,580
additional configuration as well we're

00:38:42,970 --> 00:38:47,280
also rethinking our carry or carries and

00:38:45,580 --> 00:38:48,940
canary analysis are they being overused

00:38:47,280 --> 00:38:50,680
we're using them in many different

00:38:48,940 --> 00:38:52,090
places at Netflix but we want to make

00:38:50,680 --> 00:38:53,920
sure that we're not using them in a

00:38:52,090 --> 00:38:56,080
replacement for testing so it's going

00:38:53,920 --> 00:38:57,820
back and validating that that at that

00:38:56,080 --> 00:38:58,910
point we talked about earlier make sure

00:38:57,820 --> 00:39:01,810
we're not replacing it

00:38:58,910 --> 00:39:04,430
replacing testing but augmented testing

00:39:01,810 --> 00:39:05,180
and then finally we're working on

00:39:04,430 --> 00:39:07,550
next-generation

00:39:05,180 --> 00:39:09,770
canary analysis platform that's called

00:39:07,550 --> 00:39:12,500
Kayenta it's a collaboration between us

00:39:09,770 --> 00:39:14,600
and Google so Netflix and Google and one

00:39:12,500 --> 00:39:16,610
of the big goals of Cayenne T is to have

00:39:14,600 --> 00:39:19,130
a better tighter integration with

00:39:16,610 --> 00:39:21,020
spinnaker and along other goals we do

00:39:19,130 --> 00:39:22,340
have plans to open source it and so if

00:39:21,020 --> 00:39:25,610
you're interested in learning more about

00:39:22,340 --> 00:39:27,230
KY anta I highly recommend attending the

00:39:25,610 --> 00:39:30,290
presentation tomorrow at 10:00 a.m. and

00:39:27,230 --> 00:39:32,770
birdcage titled Kyoto automated carry

00:39:30,290 --> 00:39:42,890
analysis from Google and Netflix and

00:39:32,770 --> 00:39:52,190
with that thank you we have some time

00:39:42,890 --> 00:39:54,140
for questions over here it's a good

00:39:52,190 --> 00:39:56,330
question it's very dependent upon our

00:39:54,140 --> 00:39:57,590
services so some of our application

00:39:56,330 --> 00:40:01,400
teams may run only an hour

00:39:57,590 --> 00:40:03,800
canary some teams may run 24 hours to

00:40:01,400 --> 00:40:05,780
capture full kind of cycle of traffic

00:40:03,800 --> 00:40:07,400
through it so it can depend upon each of

00:40:05,780 --> 00:40:09,080
the applications and what the

00:40:07,400 --> 00:40:11,240
application order thinks is going to be

00:40:09,080 --> 00:40:26,270
a good period of time to assess that

00:40:11,240 --> 00:40:27,710
risk it's a very good point something

00:40:26,270 --> 00:40:29,120
we're also considering as well and

00:40:27,710 --> 00:40:31,130
there's different avenues we've been

00:40:29,120 --> 00:40:33,500
trying to think about it how do we

00:40:31,130 --> 00:40:35,660
reduced even the canary time you know

00:40:33,500 --> 00:40:37,310
one one way is you know if you have some

00:40:35,660 --> 00:40:39,410
telemetry or metrics store that captures

00:40:37,310 --> 00:40:41,120
data at same unit granularity scan you

00:40:39,410 --> 00:40:42,680
get better than minute granularity can

00:40:41,120 --> 00:40:44,420
you even get more data samples to make

00:40:42,680 --> 00:40:59,230
them faster more accurate decisions a

00:40:44,420 --> 00:41:01,880
very good question about how isolating a

00:40:59,230 --> 00:41:04,910
service like service to be X to actual

00:41:01,880 --> 00:41:09,080
canary then correspondingly correlating

00:41:04,910 --> 00:41:09,920
that to maybe tell us about how you did

00:41:09,080 --> 00:41:13,069
that from a heat up

00:41:09,920 --> 00:41:15,079
down to steel to offshore some of that

00:41:13,069 --> 00:41:17,210
traffic to a different instance or a

00:41:15,079 --> 00:41:22,130
different cluster sure well let Greg

00:41:17,210 --> 00:41:23,930
take that one okay so what we've done is

00:41:22,130 --> 00:41:25,160
there's a couple of ways there's

00:41:23,930 --> 00:41:27,770
actually two different approaches we've

00:41:25,160 --> 00:41:29,839
done here at Netflix rather three even a

00:41:27,770 --> 00:41:32,150
one is just you know we have our general

00:41:29,839 --> 00:41:35,240
farms and the canary just really shows

00:41:32,150 --> 00:41:37,730
up as an instance in that general pool

00:41:35,240 --> 00:41:39,799
of available instances so zoo would just

00:41:37,730 --> 00:41:41,270
see that new canary instance or

00:41:39,799 --> 00:41:43,970
instances as just part of the general

00:41:41,270 --> 00:41:45,650
farm and would route some percentage of

00:41:43,970 --> 00:41:47,000
the traffic to it in the typical zoo

00:41:45,650 --> 00:41:49,730
fashion right

00:41:47,000 --> 00:41:52,579
another way is Zul actually and I don't

00:41:49,730 --> 00:41:53,960
know if it's available yet but Zul

00:41:52,579 --> 00:41:57,020
actually does allow you to route

00:41:53,960 --> 00:41:59,329
specific types of traffic to a specific

00:41:57,020 --> 00:42:01,880
farm so we could you could say alright I

00:41:59,329 --> 00:42:03,859
want this type of traffic to go you know

00:42:01,880 --> 00:42:06,980
this percentage of it to go specifically

00:42:03,859 --> 00:42:09,200
to this farm another technique would

00:42:06,980 --> 00:42:11,329
start the experiment with a little is

00:42:09,200 --> 00:42:13,700
the idea of sticky Canaries you know we

00:42:11,329 --> 00:42:15,380
might take a customer and always send

00:42:13,700 --> 00:42:17,000
them to that same canary and again zoo

00:42:15,380 --> 00:42:19,099
allows you to set up this kind of

00:42:17,000 --> 00:42:22,069
routing where you can say alright all

00:42:19,099 --> 00:42:24,530
customers in this particular group they

00:42:22,069 --> 00:42:26,599
go to the canary cluster and that way we

00:42:24,530 --> 00:42:29,450
can kind of track those customers across

00:42:26,599 --> 00:42:31,369
the whole lifecycle of their application

00:42:29,450 --> 00:42:35,299
you know from movie discovery to

00:42:31,369 --> 00:42:37,250
playback to you know rating the movie we

00:42:35,299 --> 00:42:39,160
can see how all that went to the canary

00:42:37,250 --> 00:42:41,660
cluster and maybe I'll get measured

00:42:39,160 --> 00:42:43,309
similarly it makes it easier if the you

00:42:41,660 --> 00:42:45,049
know clients are reporting errors that

00:42:43,309 --> 00:42:47,720
are not detectable on the server side

00:42:45,049 --> 00:42:49,609
then we can by having those clients

00:42:47,720 --> 00:42:52,549
stuck to that canary cluster we can

00:42:49,609 --> 00:42:55,099
readily identify those client reported

00:42:52,549 --> 00:43:02,450
errors that for caused by the canary

00:42:55,099 --> 00:43:05,089
system is a group of users but it's her

00:43:02,450 --> 00:43:07,130
foot and your scenario that he described

00:43:05,089 --> 00:43:08,960
for the analysis is really any

00:43:07,130 --> 00:43:12,230
production and related data to get the

00:43:08,960 --> 00:43:13,579
relative metrics behind it so what's

00:43:12,230 --> 00:43:16,160
usually a typical strategy with

00:43:13,579 --> 00:43:18,180
isolating users are set a group for

00:43:16,160 --> 00:43:20,309
users of sample devices

00:43:18,180 --> 00:43:22,950
how do you do Paul make the league to

00:43:20,309 --> 00:43:24,869
say well these devices did well versus a

00:43:22,950 --> 00:43:28,980
group a whole selection of production

00:43:24,869 --> 00:43:31,380
tax based off of a percentage yeah and

00:43:28,980 --> 00:43:33,119
that's really we've kind of left that up

00:43:31,380 --> 00:43:35,700
to the actual application owners to

00:43:33,119 --> 00:43:37,380
decide which makes sense in some cases

00:43:35,700 --> 00:43:39,089
you know if we're dealing with the the

00:43:37,380 --> 00:43:41,309
application that deals with I'm going to

00:43:39,089 --> 00:43:43,349
say Xbox and we may want to you know

00:43:41,309 --> 00:43:45,150
pull out all the Xbox users take some

00:43:43,349 --> 00:43:46,619
little percentage of them may be modded

00:43:45,150 --> 00:43:48,869
on the customer account or whatever and

00:43:46,619 --> 00:43:51,510
send that to the canary cluster at the

00:43:48,869 --> 00:43:53,490
canary cluster you know the metrics that

00:43:51,510 --> 00:43:55,380
it produces are isolated so therefore we

00:43:53,490 --> 00:43:58,799
can really associate those metrics with

00:43:55,380 --> 00:44:00,599
that full of users other times we may

00:43:58,799 --> 00:44:02,609
want to say alright you know what it

00:44:00,599 --> 00:44:04,740
really makes sense to have as broad a

00:44:02,609 --> 00:44:06,900
range of client devices you know phones

00:44:04,740 --> 00:44:09,930
tablets computers laptops gaming devices

00:44:06,900 --> 00:44:12,569
as possible so we may want to set up a

00:44:09,930 --> 00:44:14,789
group such that we get a pretty

00:44:12,569 --> 00:44:19,039
representative distribution of different

00:44:14,789 --> 00:44:19,039
types of devices in that canary pool

00:44:26,329 --> 00:44:40,710
drop it in and say for analysis do you

00:44:38,549 --> 00:44:43,230
guys make sure that you have enough data

00:44:40,710 --> 00:44:45,089
so for whatever reason people enough

00:44:43,230 --> 00:44:46,829
people were using this service will it

00:44:45,089 --> 00:44:48,299
automatically kick it back or do you

00:44:46,829 --> 00:44:51,660
have it just gonna keep looping and

00:44:48,299 --> 00:44:53,940
delay the we have it right now we just

00:44:51,660 --> 00:44:56,069
have to keep looping okay we we have

00:44:53,940 --> 00:44:58,020
experimented with kind of pass/fail if

00:44:56,069 --> 00:44:59,430
you don't have enough data but the way

00:44:58,020 --> 00:45:02,309
we've kind of work around that right now

00:44:59,430 --> 00:45:06,900
is the idea of we extend the analysis

00:45:02,309 --> 00:45:11,460
time until we get enough in the back

00:45:06,900 --> 00:45:15,119
over there um is there do you have any

00:45:11,460 --> 00:45:17,700
concept of being able to say like a

00:45:15,119 --> 00:45:20,849
metric is dissimilar like say error

00:45:17,700 --> 00:45:22,829
rates like if it goes down that's

00:45:20,849 --> 00:45:24,590
acceptable and isn't counted as a

00:45:22,829 --> 00:45:28,410
failure

00:45:24,590 --> 00:45:29,940
yeah so this is part with a work that

00:45:28,410 --> 00:45:32,430
Chris was talking about about that

00:45:29,940 --> 00:45:35,070
metric ontology but we also currently

00:45:32,430 --> 00:45:36,690
encode that into our metric judgment

00:45:35,070 --> 00:45:39,450
system with the set of rules so when I

00:45:36,690 --> 00:45:41,370
say errors lower that's good well you

00:45:39,450 --> 00:45:43,500
want for your errors you know success

00:45:41,370 --> 00:45:45,720
higher that's good CPU lower that's

00:45:43,500 --> 00:45:47,250
probably pretty good but instead of

00:45:45,720 --> 00:45:49,440
having to go and put that on to your

00:45:47,250 --> 00:45:50,970
canary configuration it'd be nicer if we

00:45:49,440 --> 00:45:52,290
could just you know classify certain

00:45:50,970 --> 00:45:54,150
kind of metrics as having certain

00:45:52,290 --> 00:45:57,380
characteristics and that's some of the

00:45:54,150 --> 00:45:57,380
future work that Chris was talking about

00:46:11,650 --> 00:46:14,669
[Music]

00:46:18,770 --> 00:46:24,480
bigger question so we we have our

00:46:22,080 --> 00:46:26,340
internal telemetry tool called Atlas and

00:46:24,480 --> 00:46:28,080
it keeps in memory of all this metrics

00:46:26,340 --> 00:46:29,520
for about two weeks so in essence all

00:46:28,080 --> 00:46:31,320
the metrics will be purged after two

00:46:29,520 --> 00:46:33,570
weeks out of that telemetry store we

00:46:31,320 --> 00:46:35,490
know proactively purging the baseline or

00:46:33,570 --> 00:46:36,870
canary data because it can be important

00:46:35,490 --> 00:46:38,610
you know for service or maybe we've

00:46:36,870 --> 00:46:39,960
failed a pipeline they may well come

00:46:38,610 --> 00:46:42,540
back a couple days later to do some

00:46:39,960 --> 00:46:44,040
investigation on it but they will get

00:46:42,540 --> 00:46:46,230
out of it will go to our kind of

00:46:44,040 --> 00:46:47,880
in-memory data store called Atlas after

00:46:46,230 --> 00:46:49,530
booked two weeks it does end up in our

00:46:47,880 --> 00:46:51,390
big data store eventually so if you

00:46:49,530 --> 00:46:53,520
really wanted to look at it you could go

00:46:51,390 --> 00:46:57,590
into that name extracted data but we

00:46:53,520 --> 00:46:57,590
don't you know practically perspective

00:46:59,210 --> 00:47:01,830
yes

00:47:00,360 --> 00:47:03,240
yeah so you for that whole two weeks you

00:47:01,830 --> 00:47:04,530
can get down to the we saw everything at

00:47:03,240 --> 00:47:06,090
a minute granularity so you can get the

00:47:04,530 --> 00:47:09,000
minute granular you for those hold two

00:47:06,090 --> 00:47:10,650
weeks you can get to it but for

00:47:09,000 --> 00:47:13,580
visualization we may do some roll-ups

00:47:10,650 --> 00:47:13,580
and make it fit on the

00:47:29,760 --> 00:47:46,660
I'm gonna find one here for you wait

00:47:34,300 --> 00:47:48,220
friend do these ones or so the canary

00:47:46,660 --> 00:47:51,250
stage in the spinnaker pipeline actually

00:47:48,220 --> 00:47:53,590
capsulate several sub stages that are

00:47:51,250 --> 00:47:56,560
you know really hidden it within if you

00:47:53,590 --> 00:47:58,210
look to the json after execution you see

00:47:56,560 --> 00:48:01,180
them in there but when you configure it

00:47:58,210 --> 00:48:02,830
you don't see those sub stages so the

00:48:01,180 --> 00:48:04,540
sub stages do things like set up the

00:48:02,830 --> 00:48:06,070
canary and baseline clusters wait for

00:48:04,540 --> 00:48:08,140
the instances to come up start the

00:48:06,070 --> 00:48:10,930
monitoring start be a canary analysis

00:48:08,140 --> 00:48:12,460
and at the end you can configure if your

00:48:10,930 --> 00:48:14,080
Canaries fail you can keep them around

00:48:12,460 --> 00:48:16,030
for a while so you can do some

00:48:14,080 --> 00:48:17,860
post-mortem analysis or you can have

00:48:16,030 --> 00:48:20,080
them just cleaned up right away but in

00:48:17,860 --> 00:48:22,060
the end they'll always be cleaned out so

00:48:20,080 --> 00:48:25,000
both the canary and baseline cluster

00:48:22,060 --> 00:48:46,090
will be torn down and therefore it's not

00:48:25,000 --> 00:48:47,500
really out there good question so that

00:48:46,090 --> 00:48:49,030
we do actually put those in production

00:48:47,500 --> 00:48:50,650
and again this goes back to our freedom

00:48:49,030 --> 00:48:52,300
and responsibility you know is if that's

00:48:50,650 --> 00:48:54,520
a developer you feel pretty good about

00:48:52,300 --> 00:48:56,290
your build go ahead and Canaria in

00:48:54,520 --> 00:48:58,090
production you know granted you know

00:48:56,290 --> 00:49:00,130
probably shouldn't make you know a giant

00:48:58,090 --> 00:49:02,200
canary cluster but you know if you make

00:49:00,130 --> 00:49:04,860
a single instance and and it gets some

00:49:02,200 --> 00:49:08,530
traffic you know that's probably okay

00:49:04,860 --> 00:49:09,940
and hopefully your you know we've got

00:49:08,530 --> 00:49:11,320
your canary config set up such that

00:49:09,940 --> 00:49:13,240
you'll be able to quickly shut it down

00:49:11,320 --> 00:49:15,670
some of the features we built in the

00:49:13,240 --> 00:49:18,160
canary engine itself is you know you can

00:49:15,670 --> 00:49:19,990
shut the canary down prematurely if we

00:49:18,160 --> 00:49:21,310
detect certain conditions like you know

00:49:19,990 --> 00:49:23,110
if errors go through the roof I'll just

00:49:21,310 --> 00:49:25,660
shut this down don't even you know while

00:49:23,110 --> 00:49:27,820
they're continuing or if throughput is

00:49:25,660 --> 00:49:29,770
just miserable or exceptions you know so

00:49:27,820 --> 00:49:33,460
we do have some of these safety nets

00:49:29,770 --> 00:49:35,740
built into the system itself but it also

00:49:33,460 --> 00:49:38,380
comes down to you know developer just

00:49:35,740 --> 00:49:41,160
being responsible for possibly affecting

00:49:38,380 --> 00:49:41,160
production jumping

00:49:44,099 --> 00:49:49,450
not reproduction mr. baseline and

00:49:46,749 --> 00:49:53,440
talking you and if you don't well how

00:49:49,450 --> 00:49:58,509
could you ever have a a police yeah

00:49:53,440 --> 00:50:00,609
that's a good question we used to treat

00:49:58,509 --> 00:50:02,200
production as baseline but often our

00:50:00,609 --> 00:50:03,670
production would make me exhibit some

00:50:02,200 --> 00:50:05,920
slightly different behavior because it's

00:50:03,670 --> 00:50:07,390
been up and running for quite a while so

00:50:05,920 --> 00:50:08,829
by starting the canary and waistline at

00:50:07,390 --> 00:50:10,900
the same time they kind of go through

00:50:08,829 --> 00:50:13,029
the same cycle of behaviors and

00:50:10,900 --> 00:50:14,920
hopefully arrive at a you know roughly

00:50:13,029 --> 00:50:19,989
similar behavior at the same point where

00:50:14,920 --> 00:50:23,229
we start measuring warm up the time yeah

00:50:19,989 --> 00:50:25,029
even with that yeah the other reason is

00:50:23,229 --> 00:50:28,599
we really it makes it a lot easier to

00:50:25,029 --> 00:50:31,839
separate out some of that canary and

00:50:28,599 --> 00:50:33,549
baseline metrics because we can you know

00:50:31,839 --> 00:50:35,049
look at canary metrics we can look at

00:50:33,549 --> 00:50:36,479
baseline cluster metrics then we can

00:50:35,049 --> 00:50:40,509
look at our general farm metrics and

00:50:36,479 --> 00:50:43,479
this kind of gives us a little more data

00:50:40,509 --> 00:50:44,920
to consider you know was the did the

00:50:43,479 --> 00:50:46,420
Kerry fail because the canary was really

00:50:44,920 --> 00:50:49,660
awkward because our baseline cluster was

00:50:46,420 --> 00:50:53,049
off let's look at the let's look at the

00:50:49,660 --> 00:50:54,910
main farm but there you can that said

00:50:53,049 --> 00:50:58,329
you can in spinnaker decide to use your

00:50:54,910 --> 00:50:59,890
main farmers to baseline if you wish it

00:50:58,329 --> 00:51:01,539
really it really again it's really up to

00:50:59,890 --> 00:51:17,319
however you choose it can be either

00:51:01,539 --> 00:51:19,180
pipeline next question well I think last

00:51:17,319 --> 00:51:21,400
time we calculated it we do we do track

00:51:19,180 --> 00:51:22,390
our numbers well I think last time we

00:51:21,400 --> 00:51:24,430
calculate Ithaca River

00:51:22,390 --> 00:51:27,190
14 percent failures and that is a

00:51:24,430 --> 00:51:29,049
pipeline entered into a failure state

00:51:27,190 --> 00:51:32,380
and we halted and you need to do a

00:51:29,049 --> 00:51:34,809
rollback do we use that to kind of push

00:51:32,380 --> 00:51:36,190
back not not currently but it's

00:51:34,809 --> 00:51:39,130
definitely something we could use to go

00:51:36,190 --> 00:51:40,749
in and to validate are we using carries

00:51:39,130 --> 00:51:43,209
for the right thing are they being used

00:51:40,749 --> 00:51:44,579
for testing and vice versa that's a good

00:51:43,209 --> 00:51:47,510
question

00:51:44,579 --> 00:51:50,910
yes

00:51:47,510 --> 00:51:52,650
with if you have like air raid or

00:51:50,910 --> 00:51:56,430
something else like that and it's like

00:51:52,650 --> 00:52:00,930
97% it looks similar so like a rate went

00:51:56,430 --> 00:52:02,400
up just slightly is there any way to

00:52:00,930 --> 00:52:06,000
deal with the fact that happens

00:52:02,400 --> 00:52:09,720
repeatedly over time you keep keeps

00:52:06,000 --> 00:52:11,610
gradually rising 3% because you don't

00:52:09,720 --> 00:52:13,220
you're not comparing it against the

00:52:11,610 --> 00:52:17,910
lifetime just the previous one is there

00:52:13,220 --> 00:52:18,870
anything to help combat that no yet it's

00:52:17,910 --> 00:52:20,310
it's something we've been thinking bout

00:52:18,870 --> 00:52:21,570
it as well it's kind of if you think of

00:52:20,310 --> 00:52:23,760
a canary analysis is being more of a

00:52:21,570 --> 00:52:25,320
micro view right is every time we run a

00:52:23,760 --> 00:52:28,200
canary analysis and send you deviating

00:52:25,320 --> 00:52:29,910
to 3 percent that compounds over time

00:52:28,200 --> 00:52:31,370
potentially and so can we do something

00:52:29,910 --> 00:52:33,450
about that and the answer is probably

00:52:31,370 --> 00:52:37,100
it's just we haven't put any time or

00:52:33,450 --> 00:52:37,100
effort into it just yet

00:52:38,870 --> 00:52:45,860
awesome any last questions well thank

00:52:43,980 --> 00:52:50,110
you very much everybody

00:52:45,860 --> 00:52:50,110

YouTube URL: https://www.youtube.com/watch?v=XB_En3bsUwM


