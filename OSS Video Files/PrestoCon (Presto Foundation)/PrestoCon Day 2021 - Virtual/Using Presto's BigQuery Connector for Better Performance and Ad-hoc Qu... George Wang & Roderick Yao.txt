Title: Using Presto's BigQuery Connector for Better Performance and Ad-hoc Qu... George Wang & Roderick Yao
Publication date: 2021-03-27
Playlist: PrestoCon Day 2021 - Virtual
Description: 
	Using Presto's BigQuery Connector for Better Performance and Ad-hoc Query in the Cloud - George Wang, Ahana Cloud & Roderick Yao, Google

The Google BigQuery connector gives users the ability to query tables in the BigQuery service, Google Cloud's fully managed data warehouse. In this presentation, we'll discuss the BigQuery Connector plugin for Presto which uses the BigQuery Storage API to stream data in parallel, allowing users to query from BigQuery tables via gPRC to achieve a better read performance. We'll also discuss how the connector enables interactive ad-hoc query to join data across distributed systems for data lake analytics. 

For more info about Presto, the open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes, see: https://prestodb.io/
Captions: 
	00:00:00,160 --> 00:00:05,359
all right thank you um so

00:00:03,120 --> 00:00:06,640
i'm i'm very excited to join this top um

00:00:05,359 --> 00:00:08,559
so we're going to talk about uh

00:00:06,640 --> 00:00:10,559
presto uh speed query connector

00:00:08,559 --> 00:00:16,240
implementation um

00:00:10,559 --> 00:00:18,160
next slide

00:00:16,240 --> 00:00:20,800
so first of all i'm gonna briefly

00:00:18,160 --> 00:00:21,760
introduce um about bigquery how it works

00:00:20,800 --> 00:00:24,800
in the architecture

00:00:21,760 --> 00:00:26,720
and and then george is gonna jump into

00:00:24,800 --> 00:00:27,840
um to explain the implementation of the

00:00:26,720 --> 00:00:31,119
connector

00:00:27,840 --> 00:00:33,280
um so if you don't know bigquery is a

00:00:31,119 --> 00:00:36,079
fully managed serverless data warehouse

00:00:33,280 --> 00:00:37,680
um that can process and gain insights

00:00:36,079 --> 00:00:40,879
from petabytes scale up data

00:00:37,680 --> 00:00:41,600
very fast um with the help of you know

00:00:40,879 --> 00:00:44,239
cloud

00:00:41,600 --> 00:00:45,680
access control so you can find um

00:00:44,239 --> 00:00:48,160
gradually control your

00:00:45,680 --> 00:00:50,320
access you can assign permission to

00:00:48,160 --> 00:00:51,840
users group and projects to keep your

00:00:50,320 --> 00:00:54,399
sense that data secure

00:00:51,840 --> 00:00:55,920
and doing so while helping collaboration

00:00:54,399 --> 00:00:58,800
across different teams that they're all

00:00:55,920 --> 00:00:58,800
using bigquery

00:00:58,960 --> 00:01:02,719
so while data at rest it is secure

00:01:01,840 --> 00:01:04,879
through encryption

00:01:02,719 --> 00:01:06,720
and it's being put into a durable and

00:01:04,879 --> 00:01:09,920
highly available storage

00:01:06,720 --> 00:01:12,080
lastly bigquery is fully managed and

00:01:09,920 --> 00:01:15,119
serverless so you don't need to install

00:01:12,080 --> 00:01:16,799
or set up anything so you just you don't

00:01:15,119 --> 00:01:18,240
actually need a database administrator

00:01:16,799 --> 00:01:19,680
so you can just log in and start

00:01:18,240 --> 00:01:27,680
recording the data

00:01:19,680 --> 00:01:30,240
next slide please

00:01:27,680 --> 00:01:32,560
so this is a picture to show you um the

00:01:30,240 --> 00:01:34,479
bigquery architecture looks like

00:01:32,560 --> 00:01:36,400
so bigquery primarily consists of three

00:01:34,479 --> 00:01:39,360
parts um there's suggestion

00:01:36,400 --> 00:01:40,960
there's query and storage the rest of us

00:01:39,360 --> 00:01:43,200
google handed for you

00:01:40,960 --> 00:01:45,600
um so in the ingestion part there are

00:01:43,200 --> 00:01:47,520
lots of ways to get data into bigquery

00:01:45,600 --> 00:01:48,880
um so you can adjust it from a cloud

00:01:47,520 --> 00:01:51,119
storage you can

00:01:48,880 --> 00:01:52,079
adjust from streaming data um you can

00:01:51,119 --> 00:01:54,479
use many

00:01:52,079 --> 00:01:55,759
file formats if you will um including

00:01:54,479 --> 00:01:59,040
csvs json

00:01:55,759 --> 00:01:59,759
avro parking and more um on the query

00:01:59,040 --> 00:02:01,759
side

00:01:59,759 --> 00:02:02,799
um bigquery is a structured data

00:02:01,759 --> 00:02:05,040
warehouse so

00:02:02,799 --> 00:02:06,159
um that means you can use standard sql

00:02:05,040 --> 00:02:08,640
to query

00:02:06,159 --> 00:02:10,319
um to query your data and share the sql

00:02:08,640 --> 00:02:12,000
with other users in bigquery

00:02:10,319 --> 00:02:13,520
um there are also public data that is

00:02:12,000 --> 00:02:15,440
just available um

00:02:13,520 --> 00:02:17,120
so you can just get it run get started

00:02:15,440 --> 00:02:20,000
and crunch the data people manage the

00:02:17,120 --> 00:02:21,840
storage uh for these public datasets

00:02:20,000 --> 00:02:23,280
um next we're gonna talk about storage

00:02:21,840 --> 00:02:25,760
so data is

00:02:23,280 --> 00:02:28,319
um stored in the structured table so

00:02:25,760 --> 00:02:31,200
allowing sql queries and data ls

00:02:28,319 --> 00:02:32,800
so we manage all the storage replication

00:02:31,200 --> 00:02:36,400
scalability

00:02:32,800 --> 00:02:39,440
aspect for you so connecting

00:02:36,400 --> 00:02:43,360
the storage layer and the compute is

00:02:39,440 --> 00:02:45,360
google's head by scale network so

00:02:43,360 --> 00:02:47,680
um we also manage large scale of

00:02:45,360 --> 00:02:49,920
distributed in memory shuffle

00:02:47,680 --> 00:02:50,879
so can achieve really really fast

00:02:49,920 --> 00:02:53,599
performance

00:02:50,879 --> 00:02:53,599
next slide please

00:02:55,760 --> 00:02:59,440
so this is this architecture show you

00:02:57,920 --> 00:03:01,040
how you can utilize bigquery in the

00:02:59,440 --> 00:03:02,640
analytics environment

00:03:01,040 --> 00:03:05,040
as i mentioned earlier data can be

00:03:02,640 --> 00:03:08,000
ingest from many sources in many forms

00:03:05,040 --> 00:03:10,239
so batch streaming or through the

00:03:08,000 --> 00:03:13,280
directly integrated google services

00:03:10,239 --> 00:03:14,159
such as pub sub monitoring they can just

00:03:13,280 --> 00:03:17,200
directly

00:03:14,159 --> 00:03:20,080
integrate into bigquery users will

00:03:17,200 --> 00:03:21,519
process data using either data flow or

00:03:20,080 --> 00:03:23,280
or cloud data proc

00:03:21,519 --> 00:03:24,720
um depends on how you want to process

00:03:23,280 --> 00:03:26,560
what type of data

00:03:24,720 --> 00:03:28,000
and what kind of a technology you're

00:03:26,560 --> 00:03:29,519
familiar with um

00:03:28,000 --> 00:03:31,519
data then will be put into bigquery

00:03:29,519 --> 00:03:33,200
storage and ready to be analyzed

00:03:31,519 --> 00:03:35,120
therefore you can start you know reading

00:03:33,200 --> 00:03:37,040
a query

00:03:35,120 --> 00:03:38,480
through the bigquery in the query engine

00:03:37,040 --> 00:03:40,799
or through the

00:03:38,480 --> 00:03:43,360
bigquery storage api for example in our

00:03:40,799 --> 00:03:46,799
type where is impressive in this case

00:03:43,360 --> 00:03:48,400
next slide please

00:03:46,799 --> 00:03:50,480
bigquery also supports a lot of

00:03:48,400 --> 00:03:52,560
third-party um software

00:03:50,480 --> 00:03:54,799
to allow you to bring your favorite tool

00:03:52,560 --> 00:03:57,680
um to do your analysis

00:03:54,799 --> 00:03:58,400
analysis sorry so next george gonna talk

00:03:57,680 --> 00:04:01,200
about

00:03:58,400 --> 00:04:03,680
how the connector actually works um to

00:04:01,200 --> 00:04:03,680
you george

00:04:04,080 --> 00:04:08,319
okay thank you so like what rogers

00:04:06,480 --> 00:04:10,400
discussed bigquery is a fully

00:04:08,319 --> 00:04:11,920
managed data warehouse on google cloud

00:04:10,400 --> 00:04:14,480
one of the key characteristics

00:04:11,920 --> 00:04:15,519
is the disaggregation of the compute and

00:04:14,480 --> 00:04:17,840
data storage

00:04:15,519 --> 00:04:19,040
which by using bigquery storage api

00:04:17,840 --> 00:04:22,079
allows users to

00:04:19,040 --> 00:04:23,840
read data at scale from other platforms

00:04:22,079 --> 00:04:26,400
where the data will be processed without

00:04:23,840 --> 00:04:28,080
the need to first etl or migrate data

00:04:26,400 --> 00:04:30,880
from google cloud storage

00:04:28,080 --> 00:04:32,400
so in presto the bigquery connector is

00:04:30,880 --> 00:04:35,040
implemented with the

00:04:32,400 --> 00:04:36,960
bigquery storage api to access that

00:04:35,040 --> 00:04:39,600
storage directly as shown in this

00:04:36,960 --> 00:04:40,560
slide in this particular example the

00:04:39,600 --> 00:04:43,040
user with

00:04:40,560 --> 00:04:44,320
analytical dashboards or predictive

00:04:43,040 --> 00:04:48,320
applications

00:04:44,320 --> 00:04:49,040
runs on the presto for ad hoc analytical

00:04:48,320 --> 00:04:50,880
queries

00:04:49,040 --> 00:04:52,080
using bigquery connector to fetch the

00:04:50,880 --> 00:04:55,600
data from

00:04:52,080 --> 00:04:57,680
the bigquery storage the framework for

00:04:55,600 --> 00:04:58,800
bigquery connector in presto implements

00:04:57,680 --> 00:05:01,520
the

00:04:58,800 --> 00:05:02,400
encapsulating apis of presto's plugin

00:05:01,520 --> 00:05:04,800
interface

00:05:02,400 --> 00:05:06,160
so when presto server is at launch phase

00:05:04,800 --> 00:05:09,280
a plugin manager loads

00:05:06,160 --> 00:05:10,400
bigquery plugin via catalog properties

00:05:09,280 --> 00:05:13,440
files from

00:05:10,400 --> 00:05:14,400
catalog directory during connectors

00:05:13,440 --> 00:05:16,639
installation in

00:05:14,400 --> 00:05:19,039
catalog a bigquery connector is

00:05:16,639 --> 00:05:22,080
registered in a map structure as a

00:05:19,039 --> 00:05:24,160
type of connected factory and uses

00:05:22,080 --> 00:05:25,919
create method to instantiate a new

00:05:24,160 --> 00:05:28,639
bigquery connector

00:05:25,919 --> 00:05:30,960
and in terms of type conversion all

00:05:28,639 --> 00:05:32,720
bigquery types are mapped directly to

00:05:30,960 --> 00:05:37,039
their presto counterparts

00:05:32,720 --> 00:05:37,039
with just a few exceptions listed here

00:05:37,360 --> 00:05:41,360
uh here is to demonstrate how to read

00:05:39,840 --> 00:05:43,039
the data required for

00:05:41,360 --> 00:05:45,280
the query in order to optimize the

00:05:43,039 --> 00:05:47,759
performance of the presto tasks

00:05:45,280 --> 00:05:49,039
using the disconnector a bigquery

00:05:47,759 --> 00:05:52,240
connector per each

00:05:49,039 --> 00:05:54,320
presto worker in the cluster streams

00:05:52,240 --> 00:05:55,600
data in parallel directly from bigquery

00:05:54,320 --> 00:05:57,440
via grpc

00:05:55,600 --> 00:06:00,000
without using google cloud storage as an

00:05:57,440 --> 00:06:02,000
intermediary this means that it does not

00:06:00,000 --> 00:06:03,680
leave any temporary files in the google

00:06:02,000 --> 00:06:06,400
cloud storage

00:06:03,680 --> 00:06:07,440
in most cases data are partitioned in

00:06:06,400 --> 00:06:09,840
bigquery tables

00:06:07,440 --> 00:06:10,880
by date or integers in similar fashion

00:06:09,840 --> 00:06:14,720
to hive

00:06:10,880 --> 00:06:16,560
packet orc all rows are read directly

00:06:14,720 --> 00:06:19,600
from bigquery servers using

00:06:16,560 --> 00:06:22,080
an avro wire format

00:06:19,600 --> 00:06:22,800
bigquery uses a column corona storage

00:06:22,080 --> 00:06:25,440
which

00:06:22,800 --> 00:06:26,639
leads a number of advantages for better

00:06:25,440 --> 00:06:29,280
read performance

00:06:26,639 --> 00:06:30,880
features like columns filtering which

00:06:29,280 --> 00:06:33,039
stream data without reading all

00:06:30,880 --> 00:06:34,880
columns efficiently reduce the date and

00:06:33,039 --> 00:06:38,080
read of data that

00:06:34,880 --> 00:06:40,479
are only interested snapshot snapshot

00:06:38,080 --> 00:06:42,639
consistency which allows each query read

00:06:40,479 --> 00:06:43,759
from storage based on snapshot isolation

00:06:42,639 --> 00:06:45,840
model

00:06:43,759 --> 00:06:48,160
streaming multiple reads which allows

00:06:45,840 --> 00:06:51,440
presto as the consumer to

00:06:48,160 --> 00:06:55,120
read each rows from a table using

00:06:51,440 --> 00:06:55,120
multiple streams within a section

00:06:56,160 --> 00:07:00,080
so with an additional enablement of

00:06:58,639 --> 00:07:03,039
bigquery connector

00:07:00,080 --> 00:07:03,759
presto in both the open source domain

00:07:03,039 --> 00:07:06,080
and the

00:07:03,759 --> 00:07:07,360
cloud-based as a service offering allows

00:07:06,080 --> 00:07:10,080
users to join

00:07:07,360 --> 00:07:11,440
data across distributed systems for data

00:07:10,080 --> 00:07:14,160
lake analytics

00:07:11,440 --> 00:07:16,479
now enabling the bigquery connector for

00:07:14,160 --> 00:07:18,880
presto opens up the new use cases

00:07:16,479 --> 00:07:20,639
in the cloud and adds more functionality

00:07:18,880 --> 00:07:23,680
to presto itself

00:07:20,639 --> 00:07:25,599
a single presto query can now combine

00:07:23,680 --> 00:07:28,240
data from multiple sources

00:07:25,599 --> 00:07:28,720
using various connectors to data sources

00:07:28,240 --> 00:07:32,400
in

00:07:28,720 --> 00:07:34,880
relational knowledge and object storage

00:07:32,400 --> 00:07:35,680
scheme allowing real-time access to

00:07:34,880 --> 00:07:39,840
multiple

00:07:35,680 --> 00:07:41,759
sources data by ease of use on unveiling

00:07:39,840 --> 00:07:44,160
physical location of data and

00:07:41,759 --> 00:07:46,000
conjugating into one single federated

00:07:44,160 --> 00:07:49,120
query engine

00:07:46,000 --> 00:07:51,840
was impossible but now with presto

00:07:49,120 --> 00:07:54,000
a rich user interface for data

00:07:51,840 --> 00:07:56,400
exploration and

00:07:54,000 --> 00:07:58,160
data visualization it's a simple piece

00:07:56,400 --> 00:08:00,800
of user interface of

00:07:58,160 --> 00:08:03,280
multi-data source management a user can

00:08:00,800 --> 00:08:05,840
similarly run federated query across

00:08:03,280 --> 00:08:08,319
large-scale bigquery instances and the

00:08:05,840 --> 00:08:12,160
other sources including hdfs

00:08:08,319 --> 00:08:15,280
mysql cassandra kafka and more

00:08:12,160 --> 00:08:18,000
with presto's a bigquery connector user

00:08:15,280 --> 00:08:20,160
can accelerate data and analysis because

00:08:18,000 --> 00:08:22,080
presto takes care of much of the

00:08:20,160 --> 00:08:24,639
overhead requirement to get started with

00:08:22,080 --> 00:08:24,639
the presto

00:08:24,840 --> 00:08:29,520
um i would like to

00:08:26,960 --> 00:08:31,280
take this opportunity to acknowledge the

00:08:29,520 --> 00:08:33,760
contributors who made

00:08:31,280 --> 00:08:34,959
efforts and contributions to this open

00:08:33,760 --> 00:08:37,360
source project

00:08:34,959 --> 00:08:38,159
uh besides so besides myself many thanks

00:08:37,360 --> 00:08:40,800
to david

00:08:38,159 --> 00:08:40,800
zhenshau

00:08:41,360 --> 00:08:46,000
who provided tremendous support to make

00:08:43,680 --> 00:08:49,040
it happen in presto

00:08:46,000 --> 00:08:49,920
the project is being merged and planned

00:08:49,040 --> 00:08:53,440
for

00:08:49,920 --> 00:08:56,399
availability in release 251 now

00:08:53,440 --> 00:08:57,040
if you have any questions or usability

00:08:56,399 --> 00:08:59,760
issues

00:08:57,040 --> 00:09:00,480
feel free to ping in the presto slack

00:08:59,760 --> 00:09:02,160
channel

00:09:00,480 --> 00:09:06,880
and they will be very happy to provide

00:09:02,160 --> 00:09:06,880

YouTube URL: https://www.youtube.com/watch?v=Vl9wNa7jI08


