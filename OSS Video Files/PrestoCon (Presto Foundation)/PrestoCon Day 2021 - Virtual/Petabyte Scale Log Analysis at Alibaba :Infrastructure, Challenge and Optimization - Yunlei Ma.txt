Title: Petabyte Scale Log Analysis at Alibaba :Infrastructure, Challenge and Optimization - Yunlei Ma
Publication date: 2021-03-27
Playlist: PrestoCon Day 2021 - Virtual
Description: 
	Petabyte Scale Log Analysis at Alibaba :Infrastructure, Challenge and Optimization - Yunlei Ma, Alibaba Cloud

1.Yunlei will share the infrastructure for petabytes scale log data collection, storage and analysis at Alibaba . Presto plays a key role in the infrastructure. Presto processes over hundreds of billions of query, about over 1 quadrillion rows every day .
2.Due to the heavy work Presto faced with, there are a few challenges about performance, stability and availability. To deal with the performance issues, Yunlei developed several layers of cache, including data source cache, intermediate cache, result cache. Due to the huge amount of query, coordinator became the bottle neck, Yunlei modified coordinator to a distributed architecture, so coordinator can be expanded horizontally. Yunlei also developed a feature that pushdown predicate to storage layer in order to use inverted index. Some other optimizations will also be included in the slide. 

For more info about Presto, the open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes, see: https://prestodb.io/
Captions: 
	00:00:00,080 --> 00:00:04,080
hello everyone my name is and i work at

00:00:02,879 --> 00:00:06,399
alibaba cloud

00:00:04,080 --> 00:00:07,680
service team today i will share the

00:00:06,399 --> 00:00:10,719
petabytes scale

00:00:07,680 --> 00:00:12,880
located analysis at alibaba group and i

00:00:10,719 --> 00:00:14,880
will share our experience with presto

00:00:12,880 --> 00:00:16,960
and the challenges we faced

00:00:14,880 --> 00:00:19,440
and also the optimizations we did for

00:00:16,960 --> 00:00:19,440
presto

00:00:20,960 --> 00:00:25,039
so today first i will give an

00:00:22,640 --> 00:00:26,560
installation about our team and

00:00:25,039 --> 00:00:28,840
talk about the motivation when we

00:00:26,560 --> 00:00:30,560
provide the analysis service for locking

00:00:28,840 --> 00:00:32,399
data and

00:00:30,560 --> 00:00:34,960
then i will talk about the challenges we

00:00:32,399 --> 00:00:36,239
faced and after that i will share our

00:00:34,960 --> 00:00:39,520
architecture tools

00:00:36,239 --> 00:00:40,320
to achieve scalability and also share

00:00:39,520 --> 00:00:43,440
some

00:00:40,320 --> 00:00:46,239
ways to achieve low latency and high qps

00:00:43,440 --> 00:00:46,960
or high concurrency and after that i

00:00:46,239 --> 00:00:49,039
will

00:00:46,960 --> 00:00:51,760
talk about some additional work based on

00:00:49,039 --> 00:00:53,680
a scalable framework of presto

00:00:51,760 --> 00:00:56,320
and finally i will talk about the future

00:00:53,680 --> 00:00:56,320
root mapper

00:00:57,199 --> 00:01:01,680
so first let me introduce my team and

00:01:00,239 --> 00:01:03,520
let me help you have a better

00:01:01,680 --> 00:01:04,080
understanding of the background and the

00:01:03,520 --> 00:01:06,880
um

00:01:04,080 --> 00:01:07,600
of the motivation of whatever provider

00:01:06,880 --> 00:01:10,880
analysis

00:01:07,600 --> 00:01:12,159
service for locking data and we are

00:01:10,880 --> 00:01:14,799
local analysis

00:01:12,159 --> 00:01:16,320
we are local service team and um we want

00:01:14,799 --> 00:01:18,479
to provide the service of

00:01:16,320 --> 00:01:20,159
being leave infrastructure of all kinds

00:01:18,479 --> 00:01:23,520
of logging data

00:01:20,159 --> 00:01:24,159
inside alibaba group for example linux

00:01:23,520 --> 00:01:28,960
server

00:01:24,159 --> 00:01:31,439
mobile phone web page or kubernetes

00:01:28,960 --> 00:01:33,520
and all this data from different kinds

00:01:31,439 --> 00:01:37,200
of platforms are connected

00:01:33,520 --> 00:01:39,600
to our clusters in real time other than

00:01:37,200 --> 00:01:41,040
the customer can consumer in streaming

00:01:39,600 --> 00:01:42,799
data in real time

00:01:41,040 --> 00:01:45,040
without worrying about different

00:01:42,799 --> 00:01:47,360
platforms

00:01:45,040 --> 00:01:49,439
and after that we want to provide the

00:01:47,360 --> 00:01:53,759
service led as if people were

00:01:49,439 --> 00:01:57,200
operating their logo file in a links

00:01:53,759 --> 00:01:59,759
server for example with known usernames

00:01:57,200 --> 00:02:01,200
engineers often use the command graph to

00:01:59,759 --> 00:02:04,640
search some keywords

00:02:01,200 --> 00:02:05,600
from logo file so in order to provide

00:02:04,640 --> 00:02:08,399
the

00:02:05,600 --> 00:02:12,160
real-time interactive search we build

00:02:08,399 --> 00:02:12,160
the inverted index for local data

00:02:12,400 --> 00:02:18,800
and by using index people can search

00:02:15,760 --> 00:02:21,040
from like a month database with very low

00:02:18,800 --> 00:02:23,440
latency

00:02:21,040 --> 00:02:26,239
and after that we noticed that in length

00:02:23,440 --> 00:02:28,800
or in this era of data explosion

00:02:26,239 --> 00:02:29,440
even the certain results may contain

00:02:28,800 --> 00:02:33,360
thousands

00:02:29,440 --> 00:02:37,760
to millions rows even to billions rules

00:02:33,360 --> 00:02:40,000
that is a large amount of data even

00:02:37,760 --> 00:02:41,040
thousands of data is still a latin

00:02:40,000 --> 00:02:44,160
number for

00:02:41,040 --> 00:02:47,760
human time because people can't

00:02:44,160 --> 00:02:50,480
see all these logs line by line

00:02:47,760 --> 00:02:51,040
so it is not easy for us to fully

00:02:50,480 --> 00:02:52,800
understand

00:02:51,040 --> 00:02:55,040
what is really happening inside the

00:02:52,800 --> 00:02:58,000
large amount of data

00:02:55,040 --> 00:03:01,680
but if we can summarize the term for

00:02:58,000 --> 00:03:06,400
example to some aggregation on a data

00:03:01,680 --> 00:03:09,040
take wherever says log as example

00:03:06,400 --> 00:03:10,400
and if we can calculate the max value of

00:03:09,040 --> 00:03:13,440
the response time

00:03:10,400 --> 00:03:14,560
or average value of response time then

00:03:13,440 --> 00:03:16,159
we can

00:03:14,560 --> 00:03:20,640
be sure that we have a better line

00:03:16,159 --> 00:03:20,640
sending off so whole data

00:03:21,840 --> 00:03:25,760
and so let's use the motivation why we

00:03:24,560 --> 00:03:29,120
provide the analysis

00:03:25,760 --> 00:03:32,080
service for login data and

00:03:29,120 --> 00:03:34,400
we also provided a visualization service

00:03:32,080 --> 00:03:36,640
to visualize the interactive quality

00:03:34,400 --> 00:03:36,640
loud

00:03:36,720 --> 00:03:42,239
and let me help people have a better

00:03:38,560 --> 00:03:44,879
understanding of the data trend

00:03:42,239 --> 00:03:45,519
and so in summary my team provide a

00:03:44,879 --> 00:03:48,159
service of

00:03:45,519 --> 00:03:48,799
multi-tenants in a reactive analysis

00:03:48,159 --> 00:03:54,000
service

00:03:48,799 --> 00:03:54,000
for petabyte scale real-time log

00:03:56,400 --> 00:04:01,439
so what's the challenges we faced the

00:03:59,360 --> 00:04:04,720
first challenge is that

00:04:01,439 --> 00:04:08,400
high qps every day will process

00:04:04,720 --> 00:04:11,200
about 400 million quarries

00:04:08,400 --> 00:04:11,599
per day and at the peak hour there are

00:04:11,200 --> 00:04:15,120
about

00:04:11,599 --> 00:04:18,079
10 000 queries running concurrently

00:04:15,120 --> 00:04:18,079
in one cluster

00:04:18,239 --> 00:04:23,199
and every day press the process about

00:04:20,959 --> 00:04:28,639
one quadrillion rows

00:04:23,199 --> 00:04:31,440
and that is a huge number

00:04:28,639 --> 00:04:31,759
even though the data size is very large

00:04:31,440 --> 00:04:34,880
but

00:04:31,759 --> 00:04:36,240
we can deliver our service with very low

00:04:34,880 --> 00:04:38,720
latency

00:04:36,240 --> 00:04:39,759
the average latitude of regenerative

00:04:38,720 --> 00:04:44,240
quality

00:04:39,759 --> 00:04:44,240
is only about 500 milliseconds

00:04:45,280 --> 00:04:49,840
so how do i deal with these challenges

00:04:47,840 --> 00:04:52,160
how do we achieve the low latency

00:04:49,840 --> 00:04:53,440
and the latest line i will talk about

00:04:52,160 --> 00:04:57,840
some techniques

00:04:53,440 --> 00:04:57,840
were used to deal with those challenges

00:05:03,039 --> 00:05:06,160
in order to provide the service of

00:05:05,199 --> 00:05:08,800
interactive

00:05:06,160 --> 00:05:11,680
analysis the first thing i did is

00:05:08,800 --> 00:05:14,479
introducing oppressors into our system

00:05:11,680 --> 00:05:16,000
many people ask me no question why did

00:05:14,479 --> 00:05:19,360
we choose presto

00:05:16,000 --> 00:05:21,520
in the first day well i think that the

00:05:19,360 --> 00:05:22,840
first and most important really is that

00:05:21,520 --> 00:05:26,000
president is really

00:05:22,840 --> 00:05:28,080
fast it is flexible for interactive

00:05:26,000 --> 00:05:31,360
query

00:05:28,080 --> 00:05:36,080
so it shoots our use case

00:05:31,360 --> 00:05:38,880
and press also has a scalable framework

00:05:36,080 --> 00:05:42,080
we can develop our own connector our own

00:05:38,880 --> 00:05:44,720
function or optimizing room

00:05:42,080 --> 00:05:48,000
so it is easy for us to integrate press

00:05:44,720 --> 00:05:48,000
with our own storage

00:05:48,720 --> 00:05:53,520
and also the architecture and according

00:05:51,120 --> 00:05:56,720
style operator is very elegant

00:05:53,520 --> 00:05:58,960
and the architecture is very simple so

00:05:56,720 --> 00:06:01,919
it it doesn't rely on any external

00:05:58,960 --> 00:06:04,880
components for deployments

00:06:01,919 --> 00:06:07,039
and the coding style is also very simple

00:06:04,880 --> 00:06:09,360
it is easy for us to learn about the

00:06:07,039 --> 00:06:13,120
internal implementation of crystal and

00:06:09,360 --> 00:06:16,240
also easy for us to do modification

00:06:13,120 --> 00:06:16,240
from optimization

00:06:16,319 --> 00:06:20,960
so let's reason why we choose presto in

00:06:18,479 --> 00:06:22,639
the first state

00:06:20,960 --> 00:06:24,720
in order to deal with the large amount

00:06:22,639 --> 00:06:27,039
of data ingested protein

00:06:24,720 --> 00:06:29,120
and every day there are about tens of

00:06:27,039 --> 00:06:30,639
petabytes of data ingested into our

00:06:29,120 --> 00:06:34,240
clusters

00:06:30,639 --> 00:06:35,759
and details are collected from different

00:06:34,240 --> 00:06:39,199
kinds of platforms

00:06:35,759 --> 00:06:42,560
by sdk or agent and

00:06:39,199 --> 00:06:45,360
data is sent to our clusters

00:06:42,560 --> 00:06:47,600
on the unit backend the network keeps

00:06:45,360 --> 00:06:49,440
building inverted index or column for

00:06:47,600 --> 00:06:52,880
metadata for

00:06:49,440 --> 00:06:53,360
the embedded data other than installed

00:06:52,880 --> 00:06:55,759
in a

00:06:53,360 --> 00:06:56,720
distributed file system which is called

00:06:55,759 --> 00:06:59,520
tango

00:06:56,720 --> 00:07:01,120
and restore regime is deployed in the

00:06:59,520 --> 00:07:04,639
same cluster

00:07:01,120 --> 00:07:08,319
that's the inflow of the data ingestion

00:07:04,639 --> 00:07:09,039
and storage when submitting a query

00:07:08,319 --> 00:07:11,919
people use

00:07:09,039 --> 00:07:14,000
sdk or gdp client to send general query

00:07:11,919 --> 00:07:16,800
through our front api server

00:07:14,000 --> 00:07:18,240
to one continue we support the

00:07:16,800 --> 00:07:21,120
distributed code interest

00:07:18,240 --> 00:07:23,360
and i will talk about the detailed

00:07:21,120 --> 00:07:25,520
design of that

00:07:23,360 --> 00:07:26,880
in the back end the president's press

00:07:25,520 --> 00:07:30,319
the server rates

00:07:26,880 --> 00:07:33,919
both embedded index for filtering and

00:07:30,319 --> 00:07:36,960
column for metadata for computing

00:07:33,919 --> 00:07:38,800
let's see the flow of column and

00:07:36,960 --> 00:07:41,440
press the place a k row units

00:07:38,800 --> 00:07:41,440
architecture

00:07:42,160 --> 00:07:45,199
so how to support acquiring larger

00:07:43,840 --> 00:07:48,080
amount of data

00:07:45,199 --> 00:07:50,639
every day we are processing about one

00:07:48,080 --> 00:07:53,919
quadrillion rows

00:07:50,639 --> 00:07:55,840
and 49 presto is a pure computing engine

00:07:53,919 --> 00:07:56,960
so it is really for us to decouple

00:07:55,840 --> 00:08:00,800
computing and

00:07:56,960 --> 00:08:04,319
storage separately so let's we can scale

00:08:00,800 --> 00:08:06,720
press the horizontally and also

00:08:04,319 --> 00:08:08,800
logo log data has a feature that it is

00:08:06,720 --> 00:08:10,800
immutable after injected

00:08:08,800 --> 00:08:12,800
and in the back entrance keeps

00:08:10,800 --> 00:08:13,680
compacting the small file into larger

00:08:12,800 --> 00:08:15,199
file

00:08:13,680 --> 00:08:18,479
on the neural file when the file is

00:08:15,199 --> 00:08:21,680
light enough it becomes immutable

00:08:18,479 --> 00:08:25,680
one immediate profile contains tens of

00:08:21,680 --> 00:08:27,919
millions of rows so

00:08:25,680 --> 00:08:29,280
each time we schedule file we schedule

00:08:27,919 --> 00:08:32,959
each file to

00:08:29,280 --> 00:08:35,760
one place the server and by this way

00:08:32,959 --> 00:08:37,599
we can use a lot of pressure server for

00:08:35,760 --> 00:08:41,279
one query

00:08:37,599 --> 00:08:44,880
and that's it is above us to

00:08:41,279 --> 00:08:46,320
quality among data another performance

00:08:44,880 --> 00:08:50,160
is very significant

00:08:46,320 --> 00:08:53,120
if we run a single group query on 200

00:08:50,160 --> 00:08:54,399
billion rows it only takes about 20

00:08:53,120 --> 00:08:57,440
seconds

00:08:54,399 --> 00:08:58,399
and that is a large amount of data and

00:08:57,440 --> 00:09:01,519
the latency

00:08:58,399 --> 00:09:01,519
is acceptable

00:09:03,519 --> 00:09:08,240
so how to achieve the low latency we use

00:09:06,880 --> 00:09:10,800
a few techniques

00:09:08,240 --> 00:09:12,480
here the first is the column for

00:09:10,800 --> 00:09:14,880
metadata

00:09:12,480 --> 00:09:16,080
in the back end we keeps building column

00:09:14,880 --> 00:09:19,760
for metadata for

00:09:16,080 --> 00:09:20,480
log and we all know that the column data

00:09:19,760 --> 00:09:23,760
is

00:09:20,480 --> 00:09:26,640
helpful for us to reduce in the

00:09:23,760 --> 00:09:28,000
data science to read from disk and it is

00:09:26,640 --> 00:09:31,279
also helpful for

00:09:28,000 --> 00:09:34,320
regularized execution

00:09:31,279 --> 00:09:35,200
so it will make the query faster and

00:09:34,320 --> 00:09:38,399
another

00:09:35,200 --> 00:09:41,360
thing we used is later data locality

00:09:38,399 --> 00:09:41,680
we all know that the network speed is

00:09:41,360 --> 00:09:44,800
far

00:09:41,680 --> 00:09:46,320
easier nonetheless speeding inside the

00:09:44,800 --> 00:09:48,800
same cluster is fast

00:09:46,320 --> 00:09:51,760
it's much more faster than the network

00:09:48,800 --> 00:09:55,360
between two clusters

00:09:51,760 --> 00:09:58,480
so we deploy the presto and the storage

00:09:55,360 --> 00:10:00,800
in the same shared cluster

00:09:58,480 --> 00:10:02,839
and there so that's the data can

00:10:00,800 --> 00:10:06,880
transfer very

00:10:02,839 --> 00:10:10,160
fast and we schedule file

00:10:06,880 --> 00:10:12,720
we by the way every time we

00:10:10,160 --> 00:10:14,240
will schedule file we choose a free node

00:10:12,720 --> 00:10:18,000
by node order of

00:10:14,240 --> 00:10:21,279
machine drag and cluster

00:10:18,000 --> 00:10:22,880
and another technique we use is cache

00:10:21,279 --> 00:10:26,640
and the humanity index

00:10:22,880 --> 00:10:26,640
and i will talk more detail about that

00:10:28,000 --> 00:10:32,000
well known that the cache can make quite

00:10:29,680 --> 00:10:35,120
fast and it is also helpful for us

00:10:32,000 --> 00:10:37,680
to remove some duplicated computation

00:10:35,120 --> 00:10:39,120
and in order to take advantage of the

00:10:37,680 --> 00:10:41,760
cache

00:10:39,120 --> 00:10:43,920
the scheduling algorithm must remember

00:10:41,760 --> 00:10:46,720
scheduling history in memory

00:10:43,920 --> 00:10:48,880
and every time we schedule a file we

00:10:46,720 --> 00:10:50,959
schedule each inimitable file to

00:10:48,880 --> 00:10:52,480
the same order it has ever been

00:10:50,959 --> 00:10:54,880
scheduled before

00:10:52,480 --> 00:10:55,839
unless the overload of that node is much

00:10:54,880 --> 00:10:57,680
too high

00:10:55,839 --> 00:11:01,839
and another case at the left case we

00:10:57,680 --> 00:11:04,880
will choose another free node

00:11:01,839 --> 00:11:05,279
so there are three layers of cache the

00:11:04,880 --> 00:11:08,399
low

00:11:05,279 --> 00:11:10,160
column data cache and the intermediate

00:11:08,399 --> 00:11:12,320
reload cache and then finally on the

00:11:10,160 --> 00:11:14,720
cache

00:11:12,320 --> 00:11:15,600
the final round cash and the rotator

00:11:14,720 --> 00:11:18,079
column cache

00:11:15,600 --> 00:11:18,800
are common solutions pattern the

00:11:18,079 --> 00:11:20,959
immediate account

00:11:18,800 --> 00:11:22,399
is a very rare solution so i'd like to

00:11:20,959 --> 00:11:25,440
talk about detail about

00:11:22,399 --> 00:11:25,440
in the media the cache

00:11:26,560 --> 00:11:30,640
as i mentioned earlier that fire is

00:11:29,360 --> 00:11:32,640
immutable

00:11:30,640 --> 00:11:33,680
so every time i want to create a file we

00:11:32,640 --> 00:11:37,680
schedule each

00:11:33,680 --> 00:11:41,200
mapper with exactly one immutable file

00:11:37,680 --> 00:11:42,880
then after finishing the

00:11:41,200 --> 00:11:45,440
after finishing the partial looking

00:11:42,880 --> 00:11:48,480
operator we start a result of the

00:11:45,440 --> 00:11:50,800
partial operation operator in memory

00:11:48,480 --> 00:11:52,720
and the next time when you run the

00:11:50,800 --> 00:11:53,920
exactly the same partial algorithm

00:11:52,720 --> 00:11:58,160
period

00:11:53,920 --> 00:12:00,560
the same immutable file then we can just

00:11:58,160 --> 00:12:02,240
read the result from the memory and send

00:12:00,560 --> 00:12:03,680
the intermediate result to final

00:12:02,240 --> 00:12:06,560
acquisition operator

00:12:03,680 --> 00:12:07,279
directly without reviewing data from

00:12:06,560 --> 00:12:10,720
disk

00:12:07,279 --> 00:12:12,480
or and computing the partial application

00:12:10,720 --> 00:12:14,480
operator

00:12:12,480 --> 00:12:16,079
we know that the partial aggregation

00:12:14,480 --> 00:12:18,639
period deals with

00:12:16,079 --> 00:12:20,480
large amount of data and only generates

00:12:18,639 --> 00:12:22,800
small amount of data

00:12:20,480 --> 00:12:25,120
so most of the computing happens in a

00:12:22,800 --> 00:12:29,279
partial operation operator

00:12:25,120 --> 00:12:33,120
and by this way we can achieve a

00:12:29,279 --> 00:12:34,320
faster quality and also it will save us

00:12:33,120 --> 00:12:38,800
a lot of

00:12:34,320 --> 00:12:41,920
cpu and io resource

00:12:38,800 --> 00:12:45,120
so what's the performance of cash

00:12:41,920 --> 00:12:46,560
every day there are more than about 100

00:12:45,120 --> 00:12:48,880
million inquiries

00:12:46,560 --> 00:12:50,560
hitting finally on the cash and the

00:12:48,880 --> 00:12:52,560
average latency when hitting will not

00:12:50,560 --> 00:12:56,320
find random run casually only about

00:12:52,560 --> 00:12:57,120
six milliseconds so it is really fast

00:12:56,320 --> 00:12:59,760
and

00:12:57,120 --> 00:13:00,160
what's more the final question can help

00:12:59,760 --> 00:13:05,279
us

00:13:00,160 --> 00:13:08,720
prevent a lot of duplicated computation

00:13:05,279 --> 00:13:10,959
and save us a lot of resource and

00:13:08,720 --> 00:13:12,480
what about the performance of

00:13:10,959 --> 00:13:15,440
intermediate cache

00:13:12,480 --> 00:13:17,120
for one billion row it only takes about

00:13:15,440 --> 00:13:19,680
1.3 seconds

00:13:17,120 --> 00:13:20,880
when hitting intermittent cash but the

00:13:19,680 --> 00:13:22,880
util takes about

00:13:20,880 --> 00:13:24,800
three seconds we're not hitting them in

00:13:22,880 --> 00:13:28,160
cash

00:13:24,800 --> 00:13:31,040
so the comparison of the tool latency is

00:13:28,160 --> 00:13:31,040
very impressive

00:13:33,600 --> 00:13:36,880
another technique we use there is

00:13:35,519 --> 00:13:38,560
inviting index

00:13:36,880 --> 00:13:41,199
in the first day we just use the

00:13:38,560 --> 00:13:42,959
multi-index for searching data

00:13:41,199 --> 00:13:44,399
and after introducing pressure in our

00:13:42,959 --> 00:13:47,600
system

00:13:44,399 --> 00:13:48,079
i push down predicates from preston to a

00:13:47,600 --> 00:13:51,600
storage

00:13:48,079 --> 00:13:54,639
layer and then

00:13:51,600 --> 00:13:57,360
use the imaging next to first

00:13:54,639 --> 00:14:00,000
calculate the mercury match the variety

00:13:57,360 --> 00:14:03,279
and only use the metadata to

00:14:00,000 --> 00:14:08,079
read from disk with the column data

00:14:03,279 --> 00:14:08,079
and the sender match the data to presto

00:14:08,240 --> 00:14:15,440
and this strategy is very effective if

00:14:11,279 --> 00:14:18,959
we can use index to skip some file or

00:14:15,440 --> 00:14:22,480
read only small part of the file

00:14:18,959 --> 00:14:25,839
this will save us a lot of io resource

00:14:22,480 --> 00:14:28,320
and if you if you run queries

00:14:25,839 --> 00:14:29,040
let the count start that will be

00:14:28,320 --> 00:14:32,160
extremely

00:14:29,040 --> 00:14:34,959
fast because we can use only index

00:14:32,160 --> 00:14:34,959
for computing

00:14:37,839 --> 00:14:41,279
every day we are producing processing

00:14:39,760 --> 00:14:44,160
about

00:14:41,279 --> 00:14:46,639
400 million calories and the main

00:14:44,160 --> 00:14:48,160
challenge is on coordinator

00:14:46,639 --> 00:14:49,760
let's picture one thing about the two

00:14:48,160 --> 00:14:53,199
years ago and at that time

00:14:49,760 --> 00:14:55,120
there is only one single coordinator

00:14:53,199 --> 00:14:56,639
this shows the top funnels with the

00:14:55,120 --> 00:14:59,360
highest recipient usage

00:14:56,639 --> 00:15:01,040
and the first picture is the coordinator

00:14:59,360 --> 00:15:03,360
you can notice that

00:15:01,040 --> 00:15:05,920
the superiority of coordinator is much

00:15:03,360 --> 00:15:08,560
higher than normal worker

00:15:05,920 --> 00:15:10,079
and in our cluster one single continuous

00:15:08,560 --> 00:15:12,959
can only process

00:15:10,079 --> 00:15:14,480
about 100 while storing them queries

00:15:12,959 --> 00:15:18,240
running concurrently

00:15:14,480 --> 00:15:20,079
at the most so the single coordinator is

00:15:18,240 --> 00:15:23,360
a bottleneck of the

00:15:20,079 --> 00:15:27,120
whole cluster it has a problem of both

00:15:23,360 --> 00:15:28,880
scalability and availability

00:15:27,120 --> 00:15:30,240
so you want to improve the performance

00:15:28,880 --> 00:15:34,000
of coordinator

00:15:30,240 --> 00:15:37,040
at the fusions first is support

00:15:34,000 --> 00:15:39,680
the distributed scalable cognitives

00:15:37,040 --> 00:15:40,800
and another thing with it is transfer

00:15:39,680 --> 00:15:44,160
the queries out

00:15:40,800 --> 00:15:45,519
from other facility to client directly

00:15:44,160 --> 00:15:48,160
without worrying

00:15:45,519 --> 00:15:48,160
coordinator

00:15:50,000 --> 00:15:53,040
so let's look at the detailed design of

00:15:52,399 --> 00:15:56,560
the

00:15:53,040 --> 00:15:58,160
distributed coordinators and i designed

00:15:56,560 --> 00:16:00,880
the typical units for

00:15:58,160 --> 00:16:01,600
scalability and there are two kinds of

00:16:00,880 --> 00:16:04,240
rules

00:16:01,600 --> 00:16:05,279
in this design the first is the global

00:16:04,240 --> 00:16:08,480
coordinator and

00:16:05,279 --> 00:16:10,800
the second is distributed currency

00:16:08,480 --> 00:16:12,720
the global continental has only one

00:16:10,800 --> 00:16:14,880
instance

00:16:12,720 --> 00:16:15,920
it it is responsible for cluster

00:16:14,880 --> 00:16:18,240
management

00:16:15,920 --> 00:16:19,360
for example detect united states and

00:16:18,240 --> 00:16:21,519
replicate

00:16:19,360 --> 00:16:23,440
other states information to other

00:16:21,519 --> 00:16:26,320
coordinators

00:16:23,440 --> 00:16:30,160
and also check memory usage and assign

00:16:26,320 --> 00:16:30,160
the largest quality through the poll

00:16:30,480 --> 00:16:37,920
and the distributed coordinators

00:16:33,519 --> 00:16:41,040
is responsible for quality management

00:16:37,920 --> 00:16:44,959
for example past the quality

00:16:41,040 --> 00:16:47,839
chronological plan optimize

00:16:44,959 --> 00:16:50,480
logical plan and schedule tasks and

00:16:47,839 --> 00:16:54,480
check the task state

00:16:50,480 --> 00:16:56,320
it is also responsible for a user queue

00:16:54,480 --> 00:16:58,320
every time when submitting a query the

00:16:56,320 --> 00:17:01,120
user selects

00:16:58,320 --> 00:17:01,839
one command to send his query based on

00:17:01,120 --> 00:17:04,959
the hash

00:17:01,839 --> 00:17:07,280
of his username in this way

00:17:04,959 --> 00:17:11,120
the queries belong to one user can

00:17:07,280 --> 00:17:11,120
always be found in one coordinator

00:17:11,679 --> 00:17:15,039
we can notice that this design is not

00:17:14,000 --> 00:17:16,839
00:17:15,039 --> 00:17:18,160
perfect because the global queue is not

00:17:16,839 --> 00:17:21,280
supported

00:17:18,160 --> 00:17:24,400
but you need enough for our scenario

00:17:21,280 --> 00:17:26,959
it helps us to scale the

00:17:24,400 --> 00:17:32,880
continentals horizontally for much

00:17:26,959 --> 00:17:35,679
higher time currency

00:17:32,880 --> 00:17:37,360
another thing we did is optimizing the

00:17:35,679 --> 00:17:39,440
data transfer

00:17:37,360 --> 00:17:41,200
and in the original design the

00:17:39,440 --> 00:17:44,320
continental transfer data

00:17:41,200 --> 00:17:47,200
from other studies to clients so

00:17:44,320 --> 00:17:49,280
the conditional will help to serialize

00:17:47,200 --> 00:17:50,799
the json serializes the correlator to

00:17:49,280 --> 00:17:53,039
json format

00:17:50,799 --> 00:17:56,640
and that will well know that the json

00:17:53,039 --> 00:17:56,640
zero lines he is very slow

00:17:56,880 --> 00:18:01,520
so what about if the container transfers

00:18:00,160 --> 00:18:03,120
like the amount of data for you number

00:18:01,520 --> 00:18:05,840
one ticker battery

00:18:03,120 --> 00:18:07,440
let's we will take a lot of possible

00:18:05,840 --> 00:18:10,720
usage of

00:18:07,440 --> 00:18:12,960
continental so in order to improve the

00:18:10,720 --> 00:18:14,720
performance of continental here is my

00:18:12,960 --> 00:18:18,880
solution

00:18:14,720 --> 00:18:19,600
i the the competitor will tell the

00:18:18,880 --> 00:18:22,400
client all

00:18:19,600 --> 00:18:24,080
addresses of out of the state because

00:18:22,400 --> 00:18:26,960
there may be more than one notes

00:18:24,080 --> 00:18:28,880
in the other states and the client that

00:18:26,960 --> 00:18:32,000
will use addresses to

00:18:28,880 --> 00:18:36,320
talk to the other stage to directly

00:18:32,000 --> 00:18:39,120
fetch data with product buffer formats

00:18:36,320 --> 00:18:39,120
but in this way

00:18:39,600 --> 00:18:48,160
we can make the query fast and also

00:18:43,360 --> 00:18:48,160
save a lot of cpu usage of continuity

00:18:50,880 --> 00:18:58,880
so in summary we did a few optimizations

00:18:54,840 --> 00:19:01,919
and here is the final performance

00:18:58,880 --> 00:19:03,039
we support the distributed continental

00:19:01,919 --> 00:19:07,280
and

00:19:03,039 --> 00:19:10,400
optimize the data transform

00:19:07,280 --> 00:19:13,600
so that we can achieve

00:19:10,400 --> 00:19:17,440
every day we can process about 400

00:19:13,600 --> 00:19:20,160
million queries and we decouple the

00:19:17,440 --> 00:19:20,880
computing and the storage layer to

00:19:20,160 --> 00:19:23,520
support

00:19:20,880 --> 00:19:24,000
quality login data and day we can

00:19:23,520 --> 00:19:27,280
process

00:19:24,000 --> 00:19:30,320
about one quadrillion rows

00:19:27,280 --> 00:19:33,919
and we use the column storage

00:19:30,320 --> 00:19:36,720
data locality cash inverted index

00:19:33,919 --> 00:19:37,679
so that we can achieve a much lower

00:19:36,720 --> 00:19:41,200
latency

00:19:37,679 --> 00:19:46,320
every day your average ability

00:19:41,200 --> 00:19:46,320
is only about 500 milliseconds

00:19:49,039 --> 00:19:52,240
besides those optimizations we also did

00:19:51,919 --> 00:19:54,559
a

00:19:52,240 --> 00:19:57,600
few additional work based on scalable

00:19:54,559 --> 00:20:01,360
framework of presto

00:19:57,600 --> 00:20:04,080
the first thing we did is we developed

00:20:01,360 --> 00:20:05,360
a machine learning library for time

00:20:04,080 --> 00:20:08,240
series data

00:20:05,360 --> 00:20:09,520
and also we noticed that only log data

00:20:08,240 --> 00:20:12,799
is not enough

00:20:09,520 --> 00:20:13,919
for analysis sometimes we have to join

00:20:12,799 --> 00:20:17,520
an updater with some

00:20:13,919 --> 00:20:19,440
external data so i developed a canonical

00:20:17,520 --> 00:20:23,600
reader

00:20:19,440 --> 00:20:26,799
the file um of the oss storage

00:20:23,600 --> 00:20:27,360
and we also use press federated sql

00:20:26,799 --> 00:20:29,360
engine

00:20:27,360 --> 00:20:34,799
we push down our problem key automatic

00:20:29,360 --> 00:20:36,799
promises to qualitative permissions

00:20:34,799 --> 00:20:38,000
in the future there are still a lot of

00:20:36,799 --> 00:20:40,000
things to do

00:20:38,000 --> 00:20:42,400
the the biggest challenge is still a

00:20:40,000 --> 00:20:42,720
coordinator and i will keep optimizing

00:20:42,400 --> 00:20:45,919
the

00:20:42,720 --> 00:20:47,760
performance of coordinator and

00:20:45,919 --> 00:20:50,000
another thing we want to do is to

00:20:47,760 --> 00:20:50,799
increase the availability of this

00:20:50,000 --> 00:20:54,159
current

00:20:50,799 --> 00:20:55,280
finance has only one instance and we

00:20:54,159 --> 00:20:57,840
also

00:20:55,280 --> 00:20:58,559
want to improve the performance of data

00:20:57,840 --> 00:21:02,240
exchange

00:20:58,559 --> 00:21:05,360
efficiency with rpc protocol and

00:21:02,240 --> 00:21:08,400
it will help us to shuffle in to shuffle

00:21:05,360 --> 00:21:08,400
large amount of return

00:21:09,280 --> 00:21:13,600
so i have finished my presentation if

00:21:11,760 --> 00:21:16,480
you have any question you can ask me in

00:21:13,600 --> 00:21:21,440
the qa session or academy by email

00:21:16,480 --> 00:21:21,440

YouTube URL: https://www.youtube.com/watch?v=bNdeIDGGJlM


