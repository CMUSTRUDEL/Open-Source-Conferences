Title: Prism: Presto Gateway Service at Uber - Hitarth Trivedi, Uber
Publication date: 2021-03-27
Playlist: PrestoCon Day 2021 - Virtual
Description: 
	Prism: Presto Gateway Service at Uber - Hitarth Trivedi, Uber

Prism is a gateway service for all Presto queries at Uber. It addresses Uber specific needs in four main areas - resource management, query gating, monitoring, and security. It is responsible for proxying over three million weekly queries from 6000+ weekly active users across all of Uber. Presto has variable execution times due to high multi-tenancy at Uber. Prism helps in overcoming those challenges using features like query routing, load balancing, query gating, session parameter checks, failover clusters which helps in maintaining a 99.9% availability and reliability SLA for Presto at Uber. Functionality - Query Execution: 1. Async execution API returns data stream 2. Async execution API returns File Descriptor - Routing - Prism can route queries to different clusters based on client sources. Other functionalities: Load Balancing, Query Gating, Failover, Session Properties, Security 

For more info about Presto, the open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes, see: https://prestodb.io/
Captions: 
	00:00:00,000 --> 00:00:06,319
hello friends i am hitar trivedi part of

00:00:03,199 --> 00:00:08,960
interactive analytics team at uber

00:00:06,319 --> 00:00:10,719
our team manages and maintains presto

00:00:08,960 --> 00:00:12,719
clusters at uber

00:00:10,719 --> 00:00:14,920
today i'm going to talk about prism

00:00:12,719 --> 00:00:17,440
which is a presto gateway service at

00:00:14,920 --> 00:00:19,520
uber

00:00:17,440 --> 00:00:20,800
following is the agenda i'm going to

00:00:19,520 --> 00:00:24,320
start with

00:00:20,800 --> 00:00:29,359
overview of presto at uber

00:00:24,320 --> 00:00:33,280
why we need to even build prism

00:00:29,359 --> 00:00:36,559
architecture of prism functionalities of

00:00:33,280 --> 00:00:40,480
prism in detail future work

00:00:36,559 --> 00:00:40,480
and i would end with a q a

00:00:42,480 --> 00:00:46,480
so uber's mission is to ignite

00:00:44,800 --> 00:00:47,680
opportunity by setting the world in

00:00:46,480 --> 00:00:51,120
motion

00:00:47,680 --> 00:00:54,559
we are currently active in

00:00:51,120 --> 00:00:58,640
10 000 plus studies around the world we

00:00:54,559 --> 00:01:01,440
have 93 million monthly active users

00:00:58,640 --> 00:01:04,080
and we serve approximately 16 million

00:01:01,440 --> 00:01:07,360
trips per day

00:01:04,080 --> 00:01:10,840
now data is very important and

00:01:07,360 --> 00:01:12,400
it is part of every critical decision at

00:01:10,840 --> 00:01:14,880
uber

00:01:12,400 --> 00:01:16,400
there are community operations team at

00:01:14,880 --> 00:01:19,119
uber which

00:01:16,400 --> 00:01:21,920
uses data these are small teams handling

00:01:19,119 --> 00:01:25,280
specific cities and regions

00:01:21,920 --> 00:01:29,759
marketplace pricing team you also

00:01:25,280 --> 00:01:32,560
heavily uses data each team

00:01:29,759 --> 00:01:35,200
compliance team mainly uses data for

00:01:32,560 --> 00:01:38,079
legal purposes

00:01:35,200 --> 00:01:40,880
growth marketing team and our favorite

00:01:38,079 --> 00:01:42,960
data science teams

00:01:40,880 --> 00:01:44,720
now let's talk about presto and uber

00:01:42,960 --> 00:01:47,520
scale

00:01:44,720 --> 00:01:49,680
we have around 12 000 monthly active

00:01:47,520 --> 00:01:53,280
users

00:01:49,680 --> 00:01:55,119
these users run around 400k queries per

00:01:53,280 --> 00:01:58,159
day

00:01:55,119 --> 00:01:59,119
these queries roughly process around 50

00:01:58,159 --> 00:02:02,640
petabytes of

00:01:59,119 --> 00:02:02,640
hdfs data per day

00:02:03,360 --> 00:02:10,479
restore clusters are present in two data

00:02:07,840 --> 00:02:12,720
they comprise of approximately 4.5 k

00:02:10,479 --> 00:02:12,720
nodes

00:02:12,879 --> 00:02:17,120
and there are 12 different crystal

00:02:14,800 --> 00:02:17,120
clusters

00:02:17,920 --> 00:02:22,400
there are variety of tools available at

00:02:19,920 --> 00:02:25,440
uber to talk to presto

00:02:22,400 --> 00:02:27,680
we have a lot of services in golang

00:02:25,440 --> 00:02:29,840
these services use presto go client to

00:02:27,680 --> 00:02:33,040
talk to desktop

00:02:29,840 --> 00:02:33,599
we also have services in python these

00:02:33,040 --> 00:02:36,959
use

00:02:33,599 --> 00:02:38,720
python rather pi have client to talk to

00:02:36,959 --> 00:02:40,800
presto

00:02:38,720 --> 00:02:42,800
and we also have number of data tools

00:02:40,800 --> 00:02:44,879
like tableau and data grips

00:02:42,800 --> 00:02:47,360
which uses jdbc client to talk to

00:02:44,879 --> 00:02:47,360
customer

00:02:47,599 --> 00:02:51,599
at uber we differentiate all the queries

00:02:50,560 --> 00:02:55,280
into two

00:02:51,599 --> 00:02:55,280
specific types of workload

00:02:55,440 --> 00:02:58,879
the first type of workload we call it

00:02:57,599 --> 00:03:02,319
interactive

00:02:58,879 --> 00:03:02,959
these this workload includes adobe

00:03:02,319 --> 00:03:06,080
queries

00:03:02,959 --> 00:03:06,879
so users coming to a particular service

00:03:06,080 --> 00:03:09,920
and running

00:03:06,879 --> 00:03:12,239
adopt queries then there is a

00:03:09,920 --> 00:03:13,280
another set of queries we call call it

00:03:12,239 --> 00:03:15,760
batch

00:03:13,280 --> 00:03:17,680
these are scheduled queries or queries

00:03:15,760 --> 00:03:19,920
coming from specific services in a

00:03:17,680 --> 00:03:21,840
scheduled manner

00:03:19,920 --> 00:03:24,080
so what we have done is we have

00:03:21,840 --> 00:03:27,680
pre-configured our truster clusters

00:03:24,080 --> 00:03:29,920
for a specific type of workload so for

00:03:27,680 --> 00:03:31,200
interactive workload we have interactive

00:03:29,920 --> 00:03:33,840
crystal cluster

00:03:31,200 --> 00:03:36,720
and for batch workload we have batch

00:03:33,840 --> 00:03:36,720
plus two clusters

00:03:38,159 --> 00:03:43,200
so now let's talk about why we had to

00:03:41,120 --> 00:03:47,280
build prism

00:03:43,200 --> 00:03:50,640
so as you saw that we have multiple

00:03:47,280 --> 00:03:51,519
types of workloads and uh presto

00:03:50,640 --> 00:03:54,959
clusters are

00:03:51,519 --> 00:03:58,560
configured specifically for that type of

00:03:54,959 --> 00:04:02,159
workload so

00:03:58,560 --> 00:04:05,439
like you can see here client one is

00:04:02,159 --> 00:04:08,400
running adobe queries using go client

00:04:05,439 --> 00:04:10,720
and these are supposed to go to

00:04:08,400 --> 00:04:12,640
interactive presto cluster

00:04:10,720 --> 00:04:14,640
client three which is running schedule

00:04:12,640 --> 00:04:16,720
queries using pi hive

00:04:14,640 --> 00:04:17,919
should be going to batch crystal

00:04:16,720 --> 00:04:21,519
clusters

00:04:17,919 --> 00:04:23,600
client two which is using again using

00:04:21,519 --> 00:04:25,680
adobe queries using tableau should be

00:04:23,600 --> 00:04:28,880
going to interactive clusters

00:04:25,680 --> 00:04:30,479
and client four which is using scheduled

00:04:28,880 --> 00:04:32,720
queries using google client

00:04:30,479 --> 00:04:35,199
should be going to batch crystal

00:04:32,720 --> 00:04:35,199
clusters

00:04:35,440 --> 00:04:41,360
now as you can see here each client

00:04:38,639 --> 00:04:41,680
is maintaining which presto cluster it

00:04:41,360 --> 00:04:44,400
is

00:04:41,680 --> 00:04:46,080
talking to so the configurations like

00:04:44,400 --> 00:04:48,400
the coordinator name

00:04:46,080 --> 00:04:49,520
and other configurations which are

00:04:48,400 --> 00:04:53,600
specific to that

00:04:49,520 --> 00:04:57,280
presto cluster lives in the client

00:04:53,600 --> 00:04:59,280
so in in that case uh it is client's

00:04:57,280 --> 00:05:02,160
responsibility to select

00:04:59,280 --> 00:05:02,639
the right cluster we have seen in the

00:05:02,160 --> 00:05:06,240
past

00:05:02,639 --> 00:05:09,680
that in this model a particular

00:05:06,240 --> 00:05:10,000
client may not be selecting the right

00:05:09,680 --> 00:05:13,120
type

00:05:10,000 --> 00:05:13,759
of crystal cluster so let's say for

00:05:13,120 --> 00:05:16,720
example

00:05:13,759 --> 00:05:18,160
client three although it is scheduled it

00:05:16,720 --> 00:05:20,400
is running scheduled queries

00:05:18,160 --> 00:05:22,320
it can be configured to talk to

00:05:20,400 --> 00:05:25,680
interactive clusters

00:05:22,320 --> 00:05:26,960
what can happen in that case is not it

00:05:25,680 --> 00:05:30,560
will not only impact

00:05:26,960 --> 00:05:32,160
the queries which are running on

00:05:30,560 --> 00:05:34,960
interactive clusters

00:05:32,160 --> 00:05:35,919
but will also impact the client the

00:05:34,960 --> 00:05:37,600
other

00:05:35,919 --> 00:05:39,199
clients which are talking to interactive

00:05:37,600 --> 00:05:41,280
cluster as well like client one and

00:05:39,199 --> 00:05:43,440
client two in this case so this

00:05:41,280 --> 00:05:45,840
this has caused issues for us in the

00:05:43,440 --> 00:05:49,039
past

00:05:45,840 --> 00:05:51,680
also uh there are some clients which can

00:05:49,039 --> 00:05:52,400
talk to specific cluster clusters only

00:05:51,680 --> 00:05:55,600
like in

00:05:52,400 --> 00:05:57,919
like tableau you will have to mention

00:05:55,600 --> 00:05:59,360
a particular coordinator now if that

00:05:57,919 --> 00:06:02,560
coordinator is down

00:05:59,360 --> 00:06:05,199
or if that coordinator is taken

00:06:02,560 --> 00:06:06,240
out of rotation all the queries coming

00:06:05,199 --> 00:06:08,960
from the tableau

00:06:06,240 --> 00:06:09,759
will start failing unless they switch to

00:06:08,960 --> 00:06:12,960
another

00:06:09,759 --> 00:06:16,560
cluster so as you can see

00:06:12,960 --> 00:06:18,639
because the client configurations are

00:06:16,560 --> 00:06:20,240
rather the presto configurations are

00:06:18,639 --> 00:06:22,880
stored in the clients

00:06:20,240 --> 00:06:23,520
we have we have multiple entry points

00:06:22,880 --> 00:06:26,880
into

00:06:23,520 --> 00:06:30,319
the cluster and we have seen number of

00:06:26,880 --> 00:06:33,520
issues in the past that is why we

00:06:30,319 --> 00:06:35,039
came up with a single entry point we

00:06:33,520 --> 00:06:38,400
call it prism

00:06:35,039 --> 00:06:39,520
so all the clients are configured to

00:06:38,400 --> 00:06:43,360
talk to

00:06:39,520 --> 00:06:47,680
a single single service prism

00:06:43,360 --> 00:06:50,720
and prism knows which client should

00:06:47,680 --> 00:06:53,840
talk to which particular cluster

00:06:50,720 --> 00:06:54,720
and it it also knows which cluster is up

00:06:53,840 --> 00:06:58,319
and running

00:06:54,720 --> 00:07:01,280
so it can manage uh it can manage

00:06:58,319 --> 00:07:02,160
the resources in a proper way so

00:07:01,280 --> 00:07:04,319
basically

00:07:02,160 --> 00:07:08,000
prism is acting like a cluster of

00:07:04,319 --> 00:07:10,960
cluster all our crystal clusters

00:07:08,000 --> 00:07:12,000
so with prism we can do a lot of stuff

00:07:10,960 --> 00:07:14,560
like

00:07:12,000 --> 00:07:16,240
research meant variegating and even

00:07:14,560 --> 00:07:17,400
monitoring of our entire

00:07:16,240 --> 00:07:19,360
presto ecosystem

00:07:17,400 --> 00:07:23,360
[Music]

00:07:19,360 --> 00:07:25,919
now let's talk about architecture

00:07:23,360 --> 00:07:28,840
prism architecture is pretty simple we

00:07:25,919 --> 00:07:30,319
have a user who submits the query to

00:07:28,840 --> 00:07:33,919
client

00:07:30,319 --> 00:07:36,639
now it's client's job to get a

00:07:33,919 --> 00:07:37,520
token from our security service and pass

00:07:36,639 --> 00:07:40,560
it along

00:07:37,520 --> 00:07:41,919
to prism now prism does a bunch of

00:07:40,560 --> 00:07:46,000
validation

00:07:41,919 --> 00:07:50,160
and identifies the client based on the

00:07:46,000 --> 00:07:53,440
http headers and from that it will

00:07:50,160 --> 00:07:55,680
know which type of workload it is and

00:07:53,440 --> 00:07:57,120
it forwards the query along with the

00:07:55,680 --> 00:08:00,160
delegation token

00:07:57,120 --> 00:08:03,440
to the respective crystal cluster

00:08:00,160 --> 00:08:04,479
now the presto cluster is going to

00:08:03,440 --> 00:08:07,599
validate

00:08:04,479 --> 00:08:09,759
uh the the client using the token which

00:08:07,599 --> 00:08:13,680
it got from prism

00:08:09,759 --> 00:08:16,960
and it will continue to run the query

00:08:13,680 --> 00:08:20,240
once the query completes the results are

00:08:16,960 --> 00:08:22,560
then submitted back to a stream back to

00:08:20,240 --> 00:08:25,440
client

00:08:22,560 --> 00:08:25,759
there is also another option where prism

00:08:25,440 --> 00:08:29,520
can

00:08:25,759 --> 00:08:32,000
actually write the results to hdfs

00:08:29,520 --> 00:08:34,800
and make this hdfs path available to

00:08:32,000 --> 00:08:34,800
user as well

00:08:35,279 --> 00:08:40,320
so that was that so that's the basic

00:08:38,839 --> 00:08:43,200
architecture now

00:08:40,320 --> 00:08:44,560
let's talk about the functionalities

00:08:43,200 --> 00:08:48,240
which are present within

00:08:44,560 --> 00:08:52,080
prism so there are a number of features

00:08:48,240 --> 00:08:55,440
which we'll talk about in detail so

00:08:52,080 --> 00:08:58,720
query execution is one routing

00:08:55,440 --> 00:09:01,920
load balancing failover

00:08:58,720 --> 00:09:03,519
query gating and rate limiting

00:09:01,920 --> 00:09:06,320
now let's talk about each of this

00:09:03,519 --> 00:09:09,440
feature in detail

00:09:06,320 --> 00:09:10,720
first query execution now query

00:09:09,440 --> 00:09:12,720
execution is the

00:09:10,720 --> 00:09:15,279
most important feature of prism because

00:09:12,720 --> 00:09:18,399
it's actually

00:09:15,279 --> 00:09:21,839
running the query on the presto cluster

00:09:18,399 --> 00:09:24,640
and why so prism is written in

00:09:21,839 --> 00:09:27,519
golang so it uses presto go client to

00:09:24,640 --> 00:09:32,160
interact with presto clusters

00:09:27,519 --> 00:09:35,760
there are two main modes of running

00:09:32,160 --> 00:09:36,640
a particular query we call it async

00:09:35,760 --> 00:09:39,519
execution

00:09:36,640 --> 00:09:41,440
using data stream and another one is

00:09:39,519 --> 00:09:44,800
using file descriptor

00:09:41,440 --> 00:09:47,519
now let's see what these two modes mean

00:09:44,800 --> 00:09:49,120
the first one is async execution using

00:09:47,519 --> 00:09:50,480
data stream this is the preferred

00:09:49,120 --> 00:09:53,120
approach and which is

00:09:50,480 --> 00:09:53,920
and 100 of our queries are currently

00:09:53,120 --> 00:09:57,120
using this

00:09:53,920 --> 00:09:59,600
approach in this

00:09:57,120 --> 00:10:00,640
mode user is submitting the query to

00:09:59,600 --> 00:10:05,040
client

00:10:00,640 --> 00:10:08,160
the client is using presto go client to

00:10:05,040 --> 00:10:10,800
talk to prism directly and it's

00:10:08,160 --> 00:10:11,600
and it is submitting the query to prism

00:10:10,800 --> 00:10:15,680
now prism

00:10:11,600 --> 00:10:17,120
knows which cluster to talk to and it

00:10:15,680 --> 00:10:20,160
will forward the

00:10:17,120 --> 00:10:22,959
query to the presto cluster

00:10:20,160 --> 00:10:24,079
and it will make the initial post call

00:10:22,959 --> 00:10:27,839
needed

00:10:24,079 --> 00:10:28,720
to run the query after making the post

00:10:27,839 --> 00:10:31,440
call

00:10:28,720 --> 00:10:32,240
the subsequent get it it will the prism

00:10:31,440 --> 00:10:35,600
is going to

00:10:32,240 --> 00:10:38,240
redirect the uh control

00:10:35,600 --> 00:10:39,680
back to presto go grand and the

00:10:38,240 --> 00:10:42,640
subsequent get calls

00:10:39,680 --> 00:10:43,120
then happen directly from go client to

00:10:42,640 --> 00:10:46,480
the

00:10:43,120 --> 00:10:50,000
chosen presto cluster

00:10:46,480 --> 00:10:51,120
once the query completes the results are

00:10:50,000 --> 00:10:54,000
streamed directly

00:10:51,120 --> 00:10:55,040
back to the client from presto cluster

00:10:54,000 --> 00:11:01,200
and back to

00:10:55,040 --> 00:11:04,640
the user

00:11:01,200 --> 00:11:06,959
the second approach is

00:11:04,640 --> 00:11:08,640
using the file descriptor mode so in

00:11:06,959 --> 00:11:10,640
this mode

00:11:08,640 --> 00:11:11,839
user is submitting the query to the

00:11:10,640 --> 00:11:14,480
client

00:11:11,839 --> 00:11:15,519
the client is then forwarding the query

00:11:14,480 --> 00:11:19,279
to

00:11:15,519 --> 00:11:22,079
prism prism has presto go client

00:11:19,279 --> 00:11:25,279
and using which it is talking to the

00:11:22,079 --> 00:11:25,279
chosen presto cluster

00:11:25,360 --> 00:11:31,839
once the query completes

00:11:28,880 --> 00:11:33,120
prism is going to write the result of

00:11:31,839 --> 00:11:37,600
the query

00:11:33,120 --> 00:11:38,399
into hdfs after the result writing gets

00:11:37,600 --> 00:11:41,680
over

00:11:38,399 --> 00:11:44,480
prism is going to give the client

00:11:41,680 --> 00:11:45,680
back the path hdfs path where the result

00:11:44,480 --> 00:11:48,720
is stored

00:11:45,680 --> 00:11:51,279
the client can now talk to sdfs

00:11:48,720 --> 00:11:53,600
and retrieve the results from the given

00:11:51,279 --> 00:11:53,600
path

00:11:55,120 --> 00:11:59,839
now let's talk about routing as i had

00:11:58,079 --> 00:12:00,959
mentioned earlier there are two main

00:11:59,839 --> 00:12:04,399
types of workload

00:12:00,959 --> 00:12:07,600
interactive and batch and

00:12:04,399 --> 00:12:10,959
prism maintains a source map which

00:12:07,600 --> 00:12:14,240
is which it uses to identify

00:12:10,959 --> 00:12:17,200
which uh the very strong

00:12:14,240 --> 00:12:17,839
particle this particular type of cluster

00:12:17,200 --> 00:12:21,200
so this

00:12:17,839 --> 00:12:23,680
source is uh

00:12:21,200 --> 00:12:24,560
available via http request header to

00:12:23,680 --> 00:12:28,480
prism

00:12:24,560 --> 00:12:32,000
and using its source map it will

00:12:28,480 --> 00:12:32,000
choose the right crystal cluster

00:12:32,959 --> 00:12:37,360
now let's talk about load balancing

00:12:35,200 --> 00:12:39,839
which is another important feature

00:12:37,360 --> 00:12:39,839
in prism

00:12:41,279 --> 00:12:45,279
prism polls the presto coordinators to

00:12:44,480 --> 00:12:48,399
get the

00:12:45,279 --> 00:12:51,839
current load of cluster

00:12:48,399 --> 00:12:52,880
it is getting information like number of

00:12:51,839 --> 00:12:56,079
running queries

00:12:52,880 --> 00:12:58,639
on each cluster number of queued queries

00:12:56,079 --> 00:12:59,519
and another important factor is worker

00:12:58,639 --> 00:13:02,639
count

00:12:59,519 --> 00:13:06,079
now not all clusters

00:13:02,639 --> 00:13:07,360
are of the same size so some cluster can

00:13:06,079 --> 00:13:10,800
be of hundred node

00:13:07,360 --> 00:13:13,600
and some cluster can be a for uh

00:13:10,800 --> 00:13:15,600
some cluster can have 400 nodes in them

00:13:13,600 --> 00:13:17,680
now

00:13:15,600 --> 00:13:19,519
and because of this reason they cannot

00:13:17,680 --> 00:13:23,040
run the same number of queries

00:13:19,519 --> 00:13:26,560
so so prism considers

00:13:23,040 --> 00:13:29,519
number of workers as an important factor

00:13:26,560 --> 00:13:32,079
uh to determine what is the max

00:13:29,519 --> 00:13:35,360
concurrent queries each cluster can run

00:13:32,079 --> 00:13:38,720
so it uses that to determine load

00:13:35,360 --> 00:13:41,760
now based on the load factor

00:13:38,720 --> 00:13:44,000
prism can route the query

00:13:41,760 --> 00:13:49,760
to a particular node which is best

00:13:44,000 --> 00:13:52,240
suited at that particular moment

00:13:49,760 --> 00:13:53,279
another important feature is failovers

00:13:52,240 --> 00:13:57,519
now

00:13:53,279 --> 00:13:58,720
failover is uh is very frequently used

00:13:57,519 --> 00:14:02,079
feature

00:13:58,720 --> 00:14:05,360
for us at uber what failover

00:14:02,079 --> 00:14:08,560
does is it it helps in taking

00:14:05,360 --> 00:14:11,760
a cluster out of rotation

00:14:08,560 --> 00:14:14,880
so let's in this particular case

00:14:11,760 --> 00:14:16,480
we have three clusters and suppose you

00:14:14,880 --> 00:14:18,959
had to take

00:14:16,480 --> 00:14:20,240
let's say cluster three uh for doing

00:14:18,959 --> 00:14:23,440
some maintenance work

00:14:20,240 --> 00:14:24,240
or you are restarting the cluster or you

00:14:23,440 --> 00:14:26,000
are even

00:14:24,240 --> 00:14:28,000
let's say you're deploying new code to

00:14:26,000 --> 00:14:30,720
the cluster in that case

00:14:28,000 --> 00:14:31,760
instead of failing all the way in all

00:14:30,720 --> 00:14:35,040
the queries

00:14:31,760 --> 00:14:37,600
we will do a failover such that

00:14:35,040 --> 00:14:38,959
prism will no longer forward new queries

00:14:37,600 --> 00:14:42,079
to that cluster

00:14:38,959 --> 00:14:42,959
and we can do maintenance on that

00:14:42,079 --> 00:14:44,639
cluster

00:14:42,959 --> 00:14:47,279
without having any impact on the

00:14:44,639 --> 00:14:47,279
reliability

00:14:49,680 --> 00:14:56,800
prism has another other features as well

00:14:53,760 --> 00:14:58,800
all these features are driven by user

00:14:56,800 --> 00:14:59,920
behavior or the problems that we have

00:14:58,800 --> 00:15:03,440
seen

00:14:59,920 --> 00:15:07,519
so one common problem was

00:15:03,440 --> 00:15:11,199
bad user queries so we have seen that

00:15:07,519 --> 00:15:13,199
some users would run run very large

00:15:11,199 --> 00:15:15,199
queries which would occupy

00:15:13,199 --> 00:15:17,680
uh which would consume a lot of memories

00:15:15,199 --> 00:15:19,600
and have impact on other running queries

00:15:17,680 --> 00:15:22,959
as well

00:15:19,600 --> 00:15:26,399
because presto is a shared resource we

00:15:22,959 --> 00:15:27,680
need to identify such users and we can

00:15:26,399 --> 00:15:30,880
add

00:15:27,680 --> 00:15:33,600
add them to our block list so that no

00:15:30,880 --> 00:15:34,079
queries from these users or even sources

00:15:33,600 --> 00:15:37,920
can

00:15:34,079 --> 00:15:37,920
come and run on presto clusters

00:15:38,480 --> 00:15:42,959
another set of problem that we have seen

00:15:40,399 --> 00:15:46,639
is bad session properties

00:15:42,959 --> 00:15:47,920
now at uber we have added a lot of guard

00:15:46,639 --> 00:15:50,880
rails in place

00:15:47,920 --> 00:15:52,000
so uh for things like query time limits

00:15:50,880 --> 00:15:54,639
memory limits

00:15:52,000 --> 00:15:55,279
these are set at cluster level but this

00:15:54,639 --> 00:15:58,880
all these

00:15:55,279 --> 00:16:00,079
properties can actually be changed via

00:15:58,880 --> 00:16:03,040
session properties

00:16:00,079 --> 00:16:04,800
so we saw that certain users are

00:16:03,040 --> 00:16:07,279
misusing this properties

00:16:04,800 --> 00:16:08,800
and they are setting it to a high level

00:16:07,279 --> 00:16:10,880
which breaches the

00:16:08,800 --> 00:16:12,399
card race that we had put place at

00:16:10,880 --> 00:16:15,920
cluster level

00:16:12,399 --> 00:16:17,920
so what we did was prism is

00:16:15,920 --> 00:16:20,880
going to allow only certain session

00:16:17,920 --> 00:16:22,880
properties to be set by users

00:16:20,880 --> 00:16:25,600
while some property ship will not be

00:16:22,880 --> 00:16:25,600
able to set it

00:16:27,120 --> 00:16:32,079
prism also applies session properties

00:16:29,920 --> 00:16:34,320
per source

00:16:32,079 --> 00:16:35,199
and also the value of this session

00:16:34,320 --> 00:16:37,759
properties

00:16:35,199 --> 00:16:38,800
a prism has a way in which it can

00:16:37,759 --> 00:16:41,120
restrict

00:16:38,800 --> 00:16:42,399
the value to a certain range so for

00:16:41,120 --> 00:16:46,240
example

00:16:42,399 --> 00:16:48,000
a query time limit we have set

00:16:46,240 --> 00:16:50,079
let's say we have set a query time limit

00:16:48,000 --> 00:16:53,360
of 30 minutes prism can check

00:16:50,079 --> 00:16:55,600
this value and it can make sure that it

00:16:53,360 --> 00:16:58,800
is not more than 30 minutes so

00:16:55,600 --> 00:17:01,600
it can be less than 30 minutes but it

00:16:58,800 --> 00:17:03,040
it will not be more than it can never be

00:17:01,600 --> 00:17:06,480
more than 30 minutes so that

00:17:03,040 --> 00:17:08,799
users cannot run a query and abuse the

00:17:06,480 --> 00:17:08,799
system

00:17:10,720 --> 00:17:16,480
another issue that we seen was too many

00:17:13,679 --> 00:17:19,600
query on one user our stores so

00:17:16,480 --> 00:17:22,959
uh like

00:17:19,600 --> 00:17:25,679
so in order to tackle that problem we

00:17:22,959 --> 00:17:27,280
added rate limiting feature so since

00:17:25,679 --> 00:17:30,640
prism is

00:17:27,280 --> 00:17:33,280
cluster of clusters it knows the entire

00:17:30,640 --> 00:17:34,960
ecosystem it knows the global view of

00:17:33,280 --> 00:17:37,440
all the running queries from

00:17:34,960 --> 00:17:38,799
a source as well as from a user on all

00:17:37,440 --> 00:17:42,080
the given clusters

00:17:38,799 --> 00:17:44,640
so based on that information uh we can

00:17:42,080 --> 00:17:45,520
set a limit on how many concurrent

00:17:44,640 --> 00:17:48,799
queries

00:17:45,520 --> 00:17:50,080
a particular user or a source can run at

00:17:48,799 --> 00:17:51,760
a time

00:17:50,080 --> 00:17:53,440
so all these features have been really

00:17:51,760 --> 00:17:55,360
useful in

00:17:53,440 --> 00:17:56,480
maintaining the reliability and the

00:17:55,360 --> 00:18:00,000
ability of

00:17:56,480 --> 00:18:03,440
resto cluster at google now let's talk

00:18:00,000 --> 00:18:03,440
about some of the future work

00:18:04,240 --> 00:18:09,120
we are currently working on a feature

00:18:06,559 --> 00:18:12,240
called advanced query gating

00:18:09,120 --> 00:18:12,640
so we have seen that there are there are

00:18:12,240 --> 00:18:15,039
some

00:18:12,640 --> 00:18:17,039
type of queries which continue to fail

00:18:15,039 --> 00:18:19,200
on presto cluster

00:18:17,039 --> 00:18:20,640
so even though they are failing they are

00:18:19,200 --> 00:18:24,160
actually consuming

00:18:20,640 --> 00:18:25,919
resources which can be used for running

00:18:24,160 --> 00:18:27,840
other queries

00:18:25,919 --> 00:18:29,440
which are which we know are going to

00:18:27,840 --> 00:18:31,520
succeed

00:18:29,440 --> 00:18:32,720
so what we are trying to do is we are

00:18:31,520 --> 00:18:35,840
trying to

00:18:32,720 --> 00:18:38,480
identify such query patterns and

00:18:35,840 --> 00:18:42,880
block them to block them from even

00:18:38,480 --> 00:18:44,640
running on the presto cluster

00:18:42,880 --> 00:18:47,919
another feature that we are working on

00:18:44,640 --> 00:18:50,320
is called query warning

00:18:47,919 --> 00:18:52,480
so we have seen that queries which

00:18:50,320 --> 00:18:55,360
succeed today

00:18:52,480 --> 00:18:56,000
might fail tomorrow so they they are

00:18:55,360 --> 00:18:58,480
pretty close

00:18:56,000 --> 00:18:59,520
to the guard rails that we have set for

00:18:58,480 --> 00:19:03,039
each cluster

00:18:59,520 --> 00:19:04,080
like query time limit or even the memory

00:19:03,039 --> 00:19:05,600
limit so

00:19:04,080 --> 00:19:08,080
if they are pretty close to the

00:19:05,600 --> 00:19:08,640
cartridge we can actually send a warning

00:19:08,080 --> 00:19:11,760
message

00:19:08,640 --> 00:19:14,640
back to users so that they can either

00:19:11,760 --> 00:19:16,400
try to optimize their query or fix it

00:19:14,640 --> 00:19:18,000
before it start failing

00:19:16,400 --> 00:19:21,600
so this helps in maintaining the

00:19:18,000 --> 00:19:21,600
reliability of our clusters

00:19:22,559 --> 00:19:30,160
so yeah that's all i had today thank you

00:19:26,640 --> 00:19:30,160
and let's jump to q a

00:19:35,760 --> 00:19:42,880
okay thank you so much for listening um

00:19:38,960 --> 00:19:42,880
i can take some questions

00:19:44,240 --> 00:19:48,720
if there are any i don't see anything in

00:19:46,160 --> 00:19:48,720
the chat

00:19:51,679 --> 00:19:56,080
hey hitar thanks for the great session

00:19:53,679 --> 00:19:56,559
um so a couple of quick questions uh is

00:19:56,080 --> 00:19:59,919
this

00:19:56,559 --> 00:20:03,360
um uh uh open sourced yet

00:19:59,919 --> 00:20:05,280
and um uh what is the plan around that

00:20:03,360 --> 00:20:08,480
that would be helpful if you can share

00:20:05,280 --> 00:20:11,520
um some vision around it yeah go ahead

00:20:08,480 --> 00:20:14,640
yeah sure so prism is

00:20:11,520 --> 00:20:18,559
not open source it has a lot of uh

00:20:14,640 --> 00:20:20,960
uber specific things tied to it

00:20:18,559 --> 00:20:21,919
we do use testo go client heavily which

00:20:20,960 --> 00:20:25,360
is the open source

00:20:21,919 --> 00:20:28,400
uh tool to interact with presto

00:20:25,360 --> 00:20:32,840
via go so uh we have added

00:20:28,400 --> 00:20:35,679
a bunch of features and um

00:20:32,840 --> 00:20:37,280
asked

00:20:35,679 --> 00:20:38,880
there in the open source regarding first

00:20:37,280 --> 00:20:39,679
of all client which we have added in

00:20:38,880 --> 00:20:43,200
prism

00:20:39,679 --> 00:20:46,000
so that is one thing that uh

00:20:43,200 --> 00:20:47,120
we have open source but again like if if

00:20:46,000 --> 00:20:50,400
there is an interest

00:20:47,120 --> 00:20:53,600
we can always try to decouple things and

00:20:50,400 --> 00:20:56,640
try to make it reversible

00:20:53,600 --> 00:20:59,200
got it um then another question about

00:20:56,640 --> 00:21:00,240
the um you know the last year i was

00:20:59,200 --> 00:21:02,320
remembering

00:21:00,240 --> 00:21:03,600
we had two or three discussions about

00:21:02,320 --> 00:21:06,240
presto gateways

00:21:03,600 --> 00:21:07,919
uh there was one that was presented by

00:21:06,240 --> 00:21:10,400
uh i think the twitter team

00:21:07,919 --> 00:21:11,440
uh perhaps uh there was uh there's also

00:21:10,400 --> 00:21:14,880
presto gateway

00:21:11,440 --> 00:21:17,520
that came out of uh lyft uh ecosystem

00:21:14,880 --> 00:21:18,720
and um and i think there were there were

00:21:17,520 --> 00:21:21,679
a couple others

00:21:18,720 --> 00:21:23,520
have as you built this at uber and you

00:21:21,679 --> 00:21:25,760
and the team built this said uber

00:21:23,520 --> 00:21:27,760
um did you look at some of these other

00:21:25,760 --> 00:21:30,240
projects out there and and

00:21:27,760 --> 00:21:30,880
how is this different uh from some of

00:21:30,240 --> 00:21:33,120
those

00:21:30,880 --> 00:21:34,799
uh that are available uh i think john

00:21:33,120 --> 00:21:36,240
had a question about is this generally

00:21:34,799 --> 00:21:37,360
available as well which you kind of

00:21:36,240 --> 00:21:40,159
answered but it's

00:21:37,360 --> 00:21:42,000
uh not today but it'll be helpful for

00:21:40,159 --> 00:21:42,640
for the community to understand how it's

00:21:42,000 --> 00:21:45,360
different

00:21:42,640 --> 00:21:45,760
from some of the uh the gateways that

00:21:45,360 --> 00:21:48,080
are

00:21:45,760 --> 00:21:48,080
out there

00:21:49,200 --> 00:21:54,960
yeah sure nice question so uh yeah

00:21:52,240 --> 00:21:55,440
yeah i think we did take a look at the

00:21:54,960 --> 00:21:58,480
twitter

00:21:55,440 --> 00:22:01,919
gateway and the last question as well uh

00:21:58,480 --> 00:22:05,039
so we had been working on the prism uh

00:22:01,919 --> 00:22:08,320
for around uh two years now

00:22:05,039 --> 00:22:12,720
uh so uh probably at that time we

00:22:08,320 --> 00:22:14,960
did not uh have uh like

00:22:12,720 --> 00:22:16,400
uh insight into the other gateways which

00:22:14,960 --> 00:22:19,520
were uh outside

00:22:16,400 --> 00:22:21,280
um in other companies so uh we were

00:22:19,520 --> 00:22:22,960
trying to mainly solve the issues that

00:22:21,280 --> 00:22:26,000
we were seeing within uber

00:22:22,960 --> 00:22:29,120
and trying to uh solve that problem

00:22:26,000 --> 00:22:29,600
first and see if uh that helps us uh i

00:22:29,120 --> 00:22:32,320
think

00:22:29,600 --> 00:22:33,679
uh when i looked at the presentation

00:22:32,320 --> 00:22:35,440
from the other teams it

00:22:33,679 --> 00:22:37,280
looked pretty similar the kind of

00:22:35,440 --> 00:22:39,440
problems that other

00:22:37,280 --> 00:22:41,200
companies are also facing seemed very

00:22:39,440 --> 00:22:41,679
similar to what we are trying to solve

00:22:41,200 --> 00:22:44,159
so

00:22:41,679 --> 00:22:45,200
yes there is there seems to be a lot of

00:22:44,159 --> 00:22:47,120
overlap

00:22:45,200 --> 00:22:48,320
between the functionalities which we are

00:22:47,120 --> 00:22:51,919
building which

00:22:48,320 --> 00:22:54,000
kind of uh brings us to the point where

00:22:51,919 --> 00:22:55,760
do we want to have like a generic

00:22:54,000 --> 00:22:58,000
solution which is open sourceable

00:22:55,760 --> 00:23:00,159
and which can be shared among multiple

00:22:58,000 --> 00:23:02,480
companies so yeah again it is pointing

00:23:00,159 --> 00:23:03,280
us towards that direction and we would

00:23:02,480 --> 00:23:06,400
be happy

00:23:03,280 --> 00:23:07,200
uh to kind of uh uh yeah absolutely yeah

00:23:06,400 --> 00:23:10,320
we should

00:23:07,200 --> 00:23:11,200
have uh i mean uh chunzu and uh uh being

00:23:10,320 --> 00:23:12,799
on there on the

00:23:11,200 --> 00:23:14,480
or probably listening in right now and

00:23:12,799 --> 00:23:16,080
so we should uh take this up and

00:23:14,480 --> 00:23:17,760
you know have a generic uh gateway

00:23:16,080 --> 00:23:20,000
that's available for everyone a couple

00:23:17,760 --> 00:23:23,679
of quick questions uh jdbc support uh

00:23:20,000 --> 00:23:26,480
for prism uh is does that exist uh

00:23:23,679 --> 00:23:26,480
sully us

00:23:27,520 --> 00:23:31,679
yes so uh we we do have an uh basic jdbc

00:23:31,200 --> 00:23:34,240
support

00:23:31,679 --> 00:23:35,120
uh we are currently working on uh

00:23:34,240 --> 00:23:37,760
binding our

00:23:35,120 --> 00:23:39,520
uh in-house authentication layer into

00:23:37,760 --> 00:23:41,840
jdbc so that it supports

00:23:39,520 --> 00:23:43,279
authentication along with the gdp so

00:23:41,840 --> 00:23:45,200
that's great and the last question

00:23:43,279 --> 00:23:48,960
uh how will you plan to do customer

00:23:45,200 --> 00:23:48,960
warnings or query predictions

00:23:49,679 --> 00:23:53,360
yes so that's a good question right so

00:23:51,840 --> 00:23:56,159
that is the

00:23:53,360 --> 00:23:57,600
uh the that was in the future section

00:23:56,159 --> 00:23:58,400
plan so we have what we're trying to do

00:23:57,600 --> 00:24:01,279
is trying to

00:23:58,400 --> 00:24:01,760
understand the query shape and the query

00:24:01,279 --> 00:24:04,320
uh

00:24:01,760 --> 00:24:06,159
uh query pattern based on their

00:24:04,320 --> 00:24:09,679
historical runs as well as

00:24:06,159 --> 00:24:10,799
uh some other indications um now and if

00:24:09,679 --> 00:24:13,679
we have seen

00:24:10,799 --> 00:24:14,960
uh like that kind of query signature or

00:24:13,679 --> 00:24:18,159
query pattern has

00:24:14,960 --> 00:24:21,279
been failing or it is close to

00:24:18,159 --> 00:24:23,520
uh being considered as a

00:24:21,279 --> 00:24:24,720
bad query or it is going to fail if

00:24:23,520 --> 00:24:27,840
those indicators

00:24:24,720 --> 00:24:29,679
uh uh uh indicate any of that then we

00:24:27,840 --> 00:24:31,200
can send like without even running the

00:24:29,679 --> 00:24:32,240
query on the cluster we can send the

00:24:31,200 --> 00:24:34,320
warning or

00:24:32,240 --> 00:24:36,480
like once we have completed the run we

00:24:34,320 --> 00:24:37,520
can see the results and based on the

00:24:36,480 --> 00:24:39,120
results uh

00:24:37,520 --> 00:24:40,799
we can send the warning saying yeah

00:24:39,120 --> 00:24:42,320
although your query did succeed this

00:24:40,799 --> 00:24:44,480
time but this close to the

00:24:42,320 --> 00:24:45,679
uh the limits that we have on our

00:24:44,480 --> 00:24:48,000
cluster and it is

00:24:45,679 --> 00:24:49,600
probably a better time time to now start

00:24:48,000 --> 00:24:51,200
optimizing your queries so

00:24:49,600 --> 00:24:53,200
yeah that's what that's how we are

00:24:51,200 --> 00:24:56,960
planning to warn the customers

00:24:53,200 --> 00:24:57,360
and uh and make the ecosystem sounds

00:24:56,960 --> 00:24:59,679
good

00:24:57,360 --> 00:25:00,960
uh thanks again for a great session and

00:24:59,679 --> 00:25:01,520
answering the questions there might be a

00:25:00,960 --> 00:25:03,120
few more

00:25:01,520 --> 00:25:05,200
uh in the chat please go ahead and

00:25:03,120 --> 00:25:05,919
answer those in the chat um and uh feel

00:25:05,200 --> 00:25:08,559
free to

00:25:05,919 --> 00:25:10,159
uh engage with uh with the rest of the

00:25:08,559 --> 00:25:13,200
the folks that are listening in

00:25:10,159 --> 00:25:13,200

YouTube URL: https://www.youtube.com/watch?v=0NwUCvOZuHY


