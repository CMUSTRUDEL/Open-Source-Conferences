Title: Presto and Apache Hudi
Publication date: 2020-08-18
Playlist: Presto Events
Description: 
	Speakers:
Bhavani Sudha Saktheeswaran, Software Engineer at Moveworks, Apache Hudi PMC, Ex-Uber
Brandon Scheller, Software Engineer at Amazon Web Services

Apache Hudi is a fast growing data lake storage system that helps organizations build and manage petabyte-scale data lakes. Hudi brings stream style processing to batch-like big data by using primitives such as upserts, deletes and incremental pulls. These features help surface faster, fresher data on a unified serving layer. Hudi can be operated on the Hadoop Distributed File System (HDFS) or cloud stores and integrates well with popular query engines such as Presto, Apache Hive, Apache Spark and Apache Impala.

In this talk we are going to introduce Hudi, discuss different table/query types and how Hudi integrates with Presto to support these queries. We like to share our experience on how this integration has evolved over time and also discuss upcoming file listing and query planning improvements in Presto Hudi queries.

prestodb.io
Captions: 
	00:00:00,000 --> 00:00:02,960
ah thanks dicty

00:00:07,440 --> 00:00:14,000
okay uh as my screen whistle

00:00:10,639 --> 00:00:16,480
yes okay

00:00:14,000 --> 00:00:18,480
uh welcome everyone uh in the next 20

00:00:16,480 --> 00:00:21,520
minutes or so we are going to discuss

00:00:18,480 --> 00:00:22,640
presto db and apache hoodie and how they

00:00:21,520 --> 00:00:25,439
integrate

00:00:22,640 --> 00:00:26,960
uh my name is suga i am a pmc member in

00:00:25,439 --> 00:00:31,439
the apache houdi project

00:00:26,960 --> 00:00:34,079
and along with me here is um

00:00:31,439 --> 00:00:35,280
let's get to the agenda we'll be uh

00:00:34,079 --> 00:00:37,840
talking through

00:00:35,280 --> 00:00:39,360
a houdi like introduce hoodie give a

00:00:37,840 --> 00:00:41,840
high level overview

00:00:39,360 --> 00:00:43,200
and dive into table types and queries

00:00:41,840 --> 00:00:45,120
that houdi support

00:00:43,200 --> 00:00:47,039
and brendan will be talking about

00:00:45,120 --> 00:00:48,719
crystal hoodie integration and how that

00:00:47,039 --> 00:00:50,559
has evolved over time

00:00:48,719 --> 00:00:52,480
and finally we'll talk about some

00:00:50,559 --> 00:00:53,760
interesting road map items that may

00:00:52,480 --> 00:00:56,960
require deep integrated

00:00:53,760 --> 00:01:00,399
integration with presta cool

00:00:56,960 --> 00:01:01,440
uh apache hoodie is a data lake storage

00:01:00,399 --> 00:01:03,840
system

00:01:01,440 --> 00:01:05,680
that allows organizations to build and

00:01:03,840 --> 00:01:09,200
maintain petabyte state

00:01:05,680 --> 00:01:10,960
data lake it brings stream style

00:01:09,200 --> 00:01:13,920
processing on batch data

00:01:10,960 --> 00:01:15,920
by giving three simple primitives upsets

00:01:13,920 --> 00:01:19,280
deletes and incrementals

00:01:15,920 --> 00:01:21,360
and uh let's see how this works

00:01:19,280 --> 00:01:22,720
i've used an illustration here as you

00:01:21,360 --> 00:01:24,960
can see

00:01:22,720 --> 00:01:26,000
rights can be streamed and these are

00:01:24,960 --> 00:01:28,400
usually rdd

00:01:26,000 --> 00:01:30,079
reds of records and usually inserts

00:01:28,400 --> 00:01:31,520
updates or deletes they're streamed into

00:01:30,079 --> 00:01:35,040
a hoodie table

00:01:31,520 --> 00:01:36,640
and queries can leverage this hoodie

00:01:35,040 --> 00:01:38,320
table we have two types of queries a

00:01:36,640 --> 00:01:40,799
snapshot query

00:01:38,320 --> 00:01:42,399
that exposes the latest version of uh

00:01:40,799 --> 00:01:43,280
latest value of all records in that

00:01:42,399 --> 00:01:45,600
table

00:01:43,280 --> 00:01:47,439
and an incremental query which pulls

00:01:45,600 --> 00:01:49,040
changes since the last time

00:01:47,439 --> 00:01:52,000
so for example give me all changes that

00:01:49,040 --> 00:01:54,799
happened as of time t

00:01:52,000 --> 00:01:55,680
so these are the capabilities of hoodie

00:01:54,799 --> 00:01:58,000
hoodie supports

00:01:55,680 --> 00:01:59,520
absurds like we discussed previously but

00:01:58,000 --> 00:02:04,240
to make it faster

00:01:59,520 --> 00:02:07,360
uh we use uh indexing uh index indexing

00:02:04,240 --> 00:02:09,440
schemes and these indexes are pluggable

00:02:07,360 --> 00:02:11,039
like with any other production

00:02:09,440 --> 00:02:14,080
environment rights can always

00:02:11,039 --> 00:02:17,040
fail and to handle uh that

00:02:14,080 --> 00:02:18,720
hoodie allows to publish rights

00:02:17,040 --> 00:02:19,520
atomically and in case of failure it

00:02:18,720 --> 00:02:22,239
provides

00:02:19,520 --> 00:02:23,200
ways to roll back as well to tackle

00:02:22,239 --> 00:02:26,080
concurrency

00:02:23,200 --> 00:02:27,520
uh hoodie provide snapshot isolation

00:02:26,080 --> 00:02:29,520
between readers and writers

00:02:27,520 --> 00:02:31,360
where writer would be an additional job

00:02:29,520 --> 00:02:32,720
and a reader would be queries like

00:02:31,360 --> 00:02:36,319
acquire engines like

00:02:32,720 --> 00:02:38,239
crystal spark hive or impala etc

00:02:36,319 --> 00:02:39,599
hodi provides with save pointing

00:02:38,239 --> 00:02:41,360
capability where

00:02:39,599 --> 00:02:43,440
say points are basically snapshots of

00:02:41,360 --> 00:02:45,280
table as of a given point in time this

00:02:43,440 --> 00:02:47,440
comes in really handy when

00:02:45,280 --> 00:02:48,480
there are like uh disaster situations

00:02:47,440 --> 00:02:50,720
and you really want to

00:02:48,480 --> 00:02:53,120
revert the table to like a safe state

00:02:50,720 --> 00:02:55,680
like a safe zone safe known state

00:02:53,120 --> 00:02:56,239
and hoodie tries to carefully size the

00:02:55,680 --> 00:02:58,159
files

00:02:56,239 --> 00:03:00,480
and lays down lays them out using

00:02:58,159 --> 00:03:03,040
statistics it can compact

00:03:00,480 --> 00:03:04,959
both columnar and row based data using

00:03:03,040 --> 00:03:07,040
async compaction

00:03:04,959 --> 00:03:08,400
and it also maintains a timeline

00:03:07,040 --> 00:03:10,319
metadata of the table

00:03:08,400 --> 00:03:12,159
what that gives you is like really what

00:03:10,319 --> 00:03:15,760
changes happen to the

00:03:12,159 --> 00:03:19,120
table at a high level cool

00:03:15,760 --> 00:03:20,239
so with that with all those capabilities

00:03:19,120 --> 00:03:22,879
what hoodie offers

00:03:20,239 --> 00:03:24,159
is uh basic building blocks to build a

00:03:22,879 --> 00:03:26,879
data lake system

00:03:24,159 --> 00:03:27,760
uh let me dive into this picture here

00:03:26,879 --> 00:03:30,640
hoodie is

00:03:27,760 --> 00:03:31,440
compatible with any dfs type storage

00:03:30,640 --> 00:03:34,879
like

00:03:31,440 --> 00:03:38,239
htfs s3 azure gcp etc

00:03:34,879 --> 00:03:40,000
and it provides a couple of tools

00:03:38,239 --> 00:03:41,519
uh here you can see a delta streamer

00:03:40,000 --> 00:03:43,599
tool that

00:03:41,519 --> 00:03:45,040
aids in interesting data from various

00:03:43,599 --> 00:03:47,280
different sources like

00:03:45,040 --> 00:03:49,280
kafka like microservices sending data to

00:03:47,280 --> 00:03:52,400
kafka or like sources of truth

00:03:49,280 --> 00:03:54,159
uh data coming from cassandra mysql etc

00:03:52,400 --> 00:03:55,920
what data delta streamer does is

00:03:54,159 --> 00:03:58,480
captures these like data

00:03:55,920 --> 00:04:00,239
and uh formats them in a way that hoodie

00:03:58,480 --> 00:04:02,480
understands and stores them into the

00:04:00,239 --> 00:04:04,159
into the data lake uh while storing them

00:04:02,480 --> 00:04:05,680
there is no really any transformation

00:04:04,159 --> 00:04:06,400
these can be like typically these are

00:04:05,680 --> 00:04:09,360
like raw

00:04:06,400 --> 00:04:10,000
uh tables and these raw hoodie tables

00:04:09,360 --> 00:04:12,560
can then

00:04:10,000 --> 00:04:14,159
feed on to uh incremental pipelines that

00:04:12,560 --> 00:04:15,599
build like model tables

00:04:14,159 --> 00:04:17,199
and both the raw tables and the

00:04:15,599 --> 00:04:19,680
incremental tables are then

00:04:17,199 --> 00:04:21,840
like available for all the query engines

00:04:19,680 --> 00:04:23,840
to query

00:04:21,840 --> 00:04:25,280
cool i think i gave a good enough intro

00:04:23,840 --> 00:04:29,040
to hoodie but let me

00:04:25,280 --> 00:04:30,560
dive into more uh more on table types

00:04:29,040 --> 00:04:32,880
and queries

00:04:30,560 --> 00:04:34,960
just before getting in there a little uh

00:04:32,880 --> 00:04:36,560
overview of these concepts would

00:04:34,960 --> 00:04:38,000
help understand the table types and

00:04:36,560 --> 00:04:39,759
queries so

00:04:38,000 --> 00:04:41,040
there are three main components to a

00:04:39,759 --> 00:04:44,479
hoodie table one

00:04:41,040 --> 00:04:47,199
is the data files themselves these are

00:04:44,479 --> 00:04:49,199
columnar files or row base files we use

00:04:47,199 --> 00:04:51,759
parquet for columnar and

00:04:49,199 --> 00:04:52,400
abro for row base files and then there

00:04:51,759 --> 00:04:54,720
is

00:04:52,400 --> 00:04:56,560
indexing component which basically gives

00:04:54,720 --> 00:04:59,440
the mapping between a record id

00:04:56,560 --> 00:05:01,360
and a file id and the third critical

00:04:59,440 --> 00:05:03,280
component is the timeline metadata we

00:05:01,360 --> 00:05:04,400
spoke about this briefly but what it

00:05:03,280 --> 00:05:07,199
gives you

00:05:04,400 --> 00:05:09,280
is how you reason about the changes that

00:05:07,199 --> 00:05:12,000
a table went through for example a t

00:05:09,280 --> 00:05:13,440
time t one rights happened at time t two

00:05:12,000 --> 00:05:14,880
a compaction happened

00:05:13,440 --> 00:05:16,479
time three there were some more rights

00:05:14,880 --> 00:05:18,080
happen and so on so

00:05:16,479 --> 00:05:19,840
in this picture you would see like like

00:05:18,080 --> 00:05:22,160
typically a spark data source is

00:05:19,840 --> 00:05:23,199
written uh like ingested into the hoodie

00:05:22,160 --> 00:05:25,600
table and

00:05:23,199 --> 00:05:27,360
the injection takes care of touching all

00:05:25,600 --> 00:05:29,600
these components and that

00:05:27,360 --> 00:05:31,199
helps serving the like the primitives

00:05:29,600 --> 00:05:32,000
that i discussed earlier the absurds and

00:05:31,199 --> 00:05:34,240
incremental

00:05:32,000 --> 00:05:35,360
primitives and then these are available

00:05:34,240 --> 00:05:38,720
to all these query

00:05:35,360 --> 00:05:41,039
query engines so

00:05:38,720 --> 00:05:42,560
really table types define how data is

00:05:41,039 --> 00:05:44,560
indexed and laid out

00:05:42,560 --> 00:05:46,320
and there are two main table types copy

00:05:44,560 --> 00:05:48,400
and write and merge entry

00:05:46,320 --> 00:05:49,360
copy and write is purely columnar

00:05:48,400 --> 00:05:51,919
formatted data

00:05:49,360 --> 00:05:54,240
parquet and what happens in copy and

00:05:51,919 --> 00:05:57,120
write is inserts are neatly packed into

00:05:54,240 --> 00:05:58,160
new files whereas uh say if there are uh

00:05:57,120 --> 00:06:00,000
updates to those

00:05:58,160 --> 00:06:01,759
uh files there's already existing files

00:06:00,000 --> 00:06:04,000
right like the records in that file

00:06:01,759 --> 00:06:05,680
those are simply like versionized and

00:06:04,000 --> 00:06:07,280
rewritten that's what happens in a copy

00:06:05,680 --> 00:06:09,120
and write update

00:06:07,280 --> 00:06:12,319
for merge on read tables these can be

00:06:09,120 --> 00:06:16,160
both columnar data and row based data

00:06:12,319 --> 00:06:17,199
so what happens is like updates are kind

00:06:16,160 --> 00:06:18,960
of logged to

00:06:17,199 --> 00:06:22,080
what we call as delta files these are

00:06:18,960 --> 00:06:25,280
the log files that has avro based data

00:06:22,080 --> 00:06:27,440
and to aid in quick injection and then

00:06:25,280 --> 00:06:28,479
later a compaction process kicks in to

00:06:27,440 --> 00:06:31,520
kind of merge

00:06:28,479 --> 00:06:32,240
both these parquet data and avro data

00:06:31,520 --> 00:06:34,800
and

00:06:32,240 --> 00:06:35,600
produce as new new versions of columnar

00:06:34,800 --> 00:06:39,039
data

00:06:35,600 --> 00:06:41,280
so that is merge entry table

00:06:39,039 --> 00:06:42,319
there are three different queries one is

00:06:41,280 --> 00:06:44,800
a snapshot query

00:06:42,319 --> 00:06:46,240
incremental query and last one is read

00:06:44,800 --> 00:06:49,039
optimized query

00:06:46,240 --> 00:06:50,960
snapshot queries expose the latest

00:06:49,039 --> 00:06:52,000
snapshot of records in a table as of a

00:06:50,960 --> 00:06:54,000
given time

00:06:52,000 --> 00:06:55,280
what what this means for merge on read

00:06:54,000 --> 00:06:57,440
table is

00:06:55,280 --> 00:06:58,319
uh the queries actually read both the

00:06:57,440 --> 00:07:02,000
parquet table

00:06:58,319 --> 00:07:04,319
and stitch the the abro data on top

00:07:02,000 --> 00:07:06,240
of that on the fly for incremental

00:07:04,319 --> 00:07:09,039
queries this is like

00:07:06,240 --> 00:07:11,120
exposing changes that happen as of a

00:07:09,039 --> 00:07:11,840
given time so this is really useful to

00:07:11,120 --> 00:07:13,520
feed on

00:07:11,840 --> 00:07:14,960
uh incremental pipelines like feed

00:07:13,520 --> 00:07:16,080
change streams to the incremental data

00:07:14,960 --> 00:07:18,639
pipelines

00:07:16,080 --> 00:07:20,000
and read optimized queries are exposing

00:07:18,639 --> 00:07:22,720
snapshot of a table

00:07:20,000 --> 00:07:24,560
but they only come from the latest

00:07:22,720 --> 00:07:26,639
columnar data that is available

00:07:24,560 --> 00:07:27,680
so the query might not have the most

00:07:26,639 --> 00:07:29,840
recent data

00:07:27,680 --> 00:07:31,680
in case of a merge on read table where

00:07:29,840 --> 00:07:33,759
the data went to a log file

00:07:31,680 --> 00:07:35,840
but what it guarantees is like pure

00:07:33,759 --> 00:07:39,520
columnar performance comparable to

00:07:35,840 --> 00:07:41,759
any non-hoodie columnar table

00:07:39,520 --> 00:07:43,680
okay so let's get to a quick

00:07:41,759 --> 00:07:45,520
visualization of how these

00:07:43,680 --> 00:07:47,919
queries and table types work hand in

00:07:45,520 --> 00:07:51,280
hand so here i have a

00:07:47,919 --> 00:07:53,039
copy and write table um like

00:07:51,280 --> 00:07:55,840
this visualization shows the injection

00:07:53,039 --> 00:07:57,919
and queries on copy and write table

00:07:55,840 --> 00:07:59,840
as you can see at the very beginning

00:07:57,919 --> 00:08:00,639
there were like five records and hoodie

00:07:59,840 --> 00:08:02,960
decides to

00:08:00,639 --> 00:08:04,400
put them across three files and then

00:08:02,960 --> 00:08:06,000
there is a batch of updates

00:08:04,400 --> 00:08:08,400
and for copy and write table you can see

00:08:06,000 --> 00:08:11,039
the files are re-returned as file

00:08:08,400 --> 00:08:12,479
uh like for example here a b would be

00:08:11,039 --> 00:08:15,520
the file 0 and 5 1

00:08:12,479 --> 00:08:18,080
are rewritten as file 0 prime and 0 0

00:08:15,520 --> 00:08:19,280
file 1 prime and then this keeps

00:08:18,080 --> 00:08:22,639
happening for every

00:08:19,280 --> 00:08:25,120
batch of updates

00:08:22,639 --> 00:08:26,720
and if you see the queries i'll see that

00:08:25,120 --> 00:08:29,680
the snapshot query

00:08:26,720 --> 00:08:30,560
at any point in time reads the the most

00:08:29,680 --> 00:08:32,399
recent

00:08:30,560 --> 00:08:34,080
value for all the records whereas

00:08:32,399 --> 00:08:35,200
incremental queries will just give the

00:08:34,080 --> 00:08:37,360
changes that happened

00:08:35,200 --> 00:08:38,560
since the previous time so for example

00:08:37,360 --> 00:08:40,560
at time t one you would see the

00:08:38,560 --> 00:08:42,880
incremental query would just yield

00:08:40,560 --> 00:08:45,040
a prime and d prime so that's what

00:08:42,880 --> 00:08:48,080
happens in the copy and write table

00:08:45,040 --> 00:08:51,040
moving on to a merge entry table

00:08:48,080 --> 00:08:52,640
so here inserts like at the very

00:08:51,040 --> 00:08:54,160
beginning there are inserts and

00:08:52,640 --> 00:08:55,839
three new files are created these are

00:08:54,160 --> 00:08:58,560
parque files and

00:08:55,839 --> 00:09:00,160
as updates happen to these files new log

00:08:58,560 --> 00:09:01,120
files are being created which are aggro

00:09:00,160 --> 00:09:02,959
format

00:09:01,120 --> 00:09:04,160
and there are more updates and you'll

00:09:02,959 --> 00:09:06,959
see that

00:09:04,160 --> 00:09:08,000
even certain inserts are tapped onto the

00:09:06,959 --> 00:09:10,000
log files this is

00:09:08,000 --> 00:09:11,519
hoodie basically trying to size the

00:09:10,000 --> 00:09:13,920
files to accommodate

00:09:11,519 --> 00:09:14,880
um like file sizing mechanisms and stuff

00:09:13,920 --> 00:09:18,240
like that

00:09:14,880 --> 00:09:19,920
and compaction what it does is basically

00:09:18,240 --> 00:09:21,040
stitches all these together and produces

00:09:19,920 --> 00:09:24,160
a new version of

00:09:21,040 --> 00:09:25,839
columnar data for the as the time goes

00:09:24,160 --> 00:09:29,200
and this keeps happening

00:09:25,839 --> 00:09:31,200
uh if you see the queries the snapshot

00:09:29,200 --> 00:09:32,000
query at any point in time will always

00:09:31,200 --> 00:09:34,800
give the latest

00:09:32,000 --> 00:09:36,720
values for all records and read

00:09:34,800 --> 00:09:38,880
optimized queries will just give

00:09:36,720 --> 00:09:41,600
the data that is available as the as

00:09:38,880 --> 00:09:44,320
from the most recent columnar data so

00:09:41,600 --> 00:09:45,200
by by that what i mean is like at time 0

00:09:44,320 --> 00:09:47,600
you would see

00:09:45,200 --> 00:09:48,240
a b c d e are the most recent data and

00:09:47,600 --> 00:09:51,440
time 1

00:09:48,240 --> 00:09:53,760
also abcde but after the compaction

00:09:51,440 --> 00:09:55,200
uh the latest the latest values are

00:09:53,760 --> 00:09:56,720
exposed since the compaction kind of

00:09:55,200 --> 00:09:58,080
stitched all that updates together in a

00:09:56,720 --> 00:09:59,920
new columnar data

00:09:58,080 --> 00:10:02,880
and like incremental is same as what i

00:09:59,920 --> 00:10:05,920
explained and cow previously

00:10:02,880 --> 00:10:06,880
cool uh with all these you can ask me

00:10:05,920 --> 00:10:09,600
how do i choose

00:10:06,880 --> 00:10:11,040
a table type so to get into that uh we

00:10:09,600 --> 00:10:13,040
need to understand certain tradeoff

00:10:11,040 --> 00:10:13,920
between copy and write and merge android

00:10:13,040 --> 00:10:16,399
table

00:10:13,920 --> 00:10:18,240
uh so these like the data latency is

00:10:16,399 --> 00:10:19,760
higher in copy and drive table because

00:10:18,240 --> 00:10:21,279
of the parquet injection and having to

00:10:19,760 --> 00:10:23,200
rewrite the files every time

00:10:21,279 --> 00:10:25,600
whereas in merge entry we simply append

00:10:23,200 --> 00:10:29,120
the updates to a log file so

00:10:25,600 --> 00:10:30,640
data latency is kind of lower the update

00:10:29,120 --> 00:10:32,720
cost is higher

00:10:30,640 --> 00:10:33,680
because of the rewriting in copy and

00:10:32,720 --> 00:10:36,560
write and

00:10:33,680 --> 00:10:37,600
for marginal rate it's lower the right

00:10:36,560 --> 00:10:39,120
amplification

00:10:37,600 --> 00:10:40,720
is kind of higher in copy and write

00:10:39,120 --> 00:10:42,720
because even for a few

00:10:40,720 --> 00:10:44,000
bytes of update we'll have to rewrite

00:10:42,720 --> 00:10:46,079
the entire file

00:10:44,000 --> 00:10:47,360
this is again lower on the merchandise

00:10:46,079 --> 00:10:51,120
side so

00:10:47,360 --> 00:10:54,720
if you really uh want to like have

00:10:51,120 --> 00:10:56,880
a a replacement of your existing parking

00:10:54,720 --> 00:10:58,320
table there aren't a lot of updates

00:10:56,880 --> 00:11:00,720
and you understand your workload it's

00:10:58,320 --> 00:11:03,200
pretty smooth there's no busty traffic

00:11:00,720 --> 00:11:04,320
and you do not really worry about having

00:11:03,200 --> 00:11:06,000
real-time data

00:11:04,320 --> 00:11:08,079
then copy and write is probably the best

00:11:06,000 --> 00:11:10,640
choice but it's bound to

00:11:08,079 --> 00:11:12,320
parquet injection performance but on the

00:11:10,640 --> 00:11:14,399
other hand if your workload is like

00:11:12,320 --> 00:11:16,320
pretty unpredictable or you need more

00:11:14,399 --> 00:11:18,800
flexibility you need quicker injection

00:11:16,320 --> 00:11:19,519
and you need you do care about fresher

00:11:18,800 --> 00:11:21,120
data

00:11:19,519 --> 00:11:22,800
then margin read is probably the right

00:11:21,120 --> 00:11:25,200
choice

00:11:22,800 --> 00:11:27,360
okay but that i hand over to brandon to

00:11:25,200 --> 00:11:30,399
talk about crystal hoodie integration

00:11:27,360 --> 00:11:34,480
and how that evolves so far

00:11:30,399 --> 00:11:36,640
thanks suda hi i'm brandon from the aws

00:11:34,480 --> 00:11:39,279
emr hoodie team and i'll be continuing

00:11:36,640 --> 00:11:43,200
the presentation with the evolution of

00:11:39,279 --> 00:11:46,079
hoodie presto

00:11:43,200 --> 00:11:48,000
so the first step in hoodie presto

00:11:46,079 --> 00:11:50,320
integration was to make the standard

00:11:48,000 --> 00:11:52,160
read optimized hoodie queries work

00:11:50,320 --> 00:11:53,920
these are the queries that query just

00:11:52,160 --> 00:11:56,240
the base parquet data

00:11:53,920 --> 00:11:58,320
to do this hoodie introduces its own

00:11:56,240 --> 00:12:01,120
input format class for integration with

00:11:58,320 --> 00:12:03,600
hadoop apps such as presto or pi

00:12:01,120 --> 00:12:05,279
even with this input format integration

00:12:03,600 --> 00:12:07,360
with presto was not immediately

00:12:05,279 --> 00:12:09,600
straightforward because at the time

00:12:07,360 --> 00:12:11,200
presto did not use the splits directly

00:12:09,600 --> 00:12:14,000
from the input format

00:12:11,200 --> 00:12:16,399
instead presto has its own parquet split

00:12:14,000 --> 00:12:18,800
loader implementation

00:12:16,399 --> 00:12:19,680
presto split loader needed to call

00:12:18,800 --> 00:12:22,160
getsplits

00:12:19,680 --> 00:12:23,839
on this hoodie input format or else it

00:12:22,160 --> 00:12:25,920
would read all the parquet data in the

00:12:23,839 --> 00:12:26,959
table path and not only the latest file

00:12:25,920 --> 00:12:28,959
versions

00:12:26,959 --> 00:12:30,240
as hoodie keeps multiple versions of

00:12:28,959 --> 00:12:33,680
parquet files

00:12:30,240 --> 00:12:36,320
this could result in duplicate data

00:12:33,680 --> 00:12:38,639
the hibi community recognized this and

00:12:36,320 --> 00:12:41,040
based on presto community suggestions

00:12:38,639 --> 00:12:42,800
opened a pr to make this work way back

00:12:41,040 --> 00:12:45,440
in 2017.

00:12:42,800 --> 00:12:48,320
this pr allowed hood or allowed presto

00:12:45,440 --> 00:12:50,240
to call inputformat.getsplits

00:12:48,320 --> 00:12:52,000
when it saw the use of the newly

00:12:50,240 --> 00:12:55,279
introduced use file

00:12:52,000 --> 00:12:57,279
splits from input format annotation

00:12:55,279 --> 00:12:58,800
hoodie added this annotation to the

00:12:57,279 --> 00:13:01,360
hoodie input format

00:12:58,800 --> 00:13:03,279
and then basic hoodie queries could then

00:13:01,360 --> 00:13:05,440
work through presto

00:13:03,279 --> 00:13:06,320
at the time presto had no hoodie

00:13:05,440 --> 00:13:07,839
dependency

00:13:06,320 --> 00:13:09,600
so this was achieved by dropping the

00:13:07,839 --> 00:13:13,920
hoodie presto bundle jar

00:13:09,600 --> 00:13:13,920
in the presto hive plugin directory

00:13:15,680 --> 00:13:20,639
but there is room for improvement here

00:13:18,240 --> 00:13:21,600
the call to input format getsplits was

00:13:20,639 --> 00:13:24,800
costly

00:13:21,600 --> 00:13:26,800
it made many rpc calls to the name node

00:13:24,800 --> 00:13:28,800
presto is required to know the file

00:13:26,800 --> 00:13:30,800
status and block locations for each of

00:13:28,800 --> 00:13:33,519
the input slots returned

00:13:30,800 --> 00:13:34,000
this added two extra rpc calls to the

00:13:33,519 --> 00:13:36,320
name node

00:13:34,000 --> 00:13:37,839
for every split times the number of

00:13:36,320 --> 00:13:40,000
partitions loaded

00:13:37,839 --> 00:13:42,160
further for every partition loaded in

00:13:40,000 --> 00:13:44,160
the pressdescript calculation

00:13:42,160 --> 00:13:45,760
getsplits on the hoodie input format

00:13:44,160 --> 00:13:48,160
would be invoked

00:13:45,760 --> 00:13:50,240
this caused redundant hoodie table

00:13:48,160 --> 00:13:52,000
hoodie table metadata listing

00:13:50,240 --> 00:13:54,240
which otherwise could have been reused

00:13:52,000 --> 00:13:57,360
across partitions

00:13:54,240 --> 00:14:00,160
instead sudah here introduced a pr

00:13:57,360 --> 00:14:02,480
for the hoodie ro table path filter and

00:14:00,160 --> 00:14:04,079
presto took a compile time dependency on

00:14:02,480 --> 00:14:07,680
hoodie

00:14:04,079 --> 00:14:10,079
this hoodie ro table path filter

00:14:07,680 --> 00:14:10,880
would filter the versioned parquet files

00:14:10,079 --> 00:14:12,959
for hoodie

00:14:10,880 --> 00:14:15,440
to allow queries to work through presto

00:14:12,959 --> 00:14:17,839
without calling get splits

00:14:15,440 --> 00:14:19,920
additionally the path filter was able to

00:14:17,839 --> 00:14:20,399
cache listing results in order to

00:14:19,920 --> 00:14:22,959
further

00:14:20,399 --> 00:14:26,639
improve query performance for these

00:14:22,959 --> 00:14:26,639
hoodie read optimize queries

00:14:26,720 --> 00:14:30,399
presto hoodie integration however still

00:14:28,560 --> 00:14:31,519
had more to be done which brings us to

00:14:30,399 --> 00:14:35,120
presto support

00:14:31,519 --> 00:14:35,120
for hoodie snapshot queries

00:14:37,199 --> 00:14:41,360
as suda described hoodie provides a

00:14:39,680 --> 00:14:43,120
snapshot query system

00:14:41,360 --> 00:14:44,800
this allows you to query the most recent

00:14:43,120 --> 00:14:46,480
snapshot of your data containing

00:14:44,800 --> 00:14:48,639
real-time updates

00:14:46,480 --> 00:14:49,839
to accomplish this hoodie merges the

00:14:48,639 --> 00:14:52,000
base parquet data

00:14:49,839 --> 00:14:54,959
with an avro update log containing the

00:14:52,000 --> 00:14:56,880
most recent updates to the data

00:14:54,959 --> 00:14:58,720
extra complexity of merging the data

00:14:56,880 --> 00:15:00,800
from two sources requires

00:14:58,720 --> 00:15:02,320
a custom file split and custom record

00:15:00,800 --> 00:15:05,199
reader and is the reason

00:15:02,320 --> 00:15:07,120
and is the reason that up until recently

00:15:05,199 --> 00:15:08,800
presto did not support these snapshot

00:15:07,120 --> 00:15:11,440
queries

00:15:08,800 --> 00:15:11,839
so how is this supposed to work as

00:15:11,440 --> 00:15:14,320
mentioned

00:15:11,839 --> 00:15:15,680
earlier hoodie introduces its own input

00:15:14,320 --> 00:15:18,800
format class

00:15:15,680 --> 00:15:21,120
this hoodie input format has two methods

00:15:18,800 --> 00:15:22,880
get splits and get record reader in the

00:15:21,120 --> 00:15:23,519
case of the hoodie real-time input

00:15:22,880 --> 00:15:26,399
format

00:15:23,519 --> 00:15:27,519
the format used for snapshot queries the

00:15:26,399 --> 00:15:29,839
getsplits method

00:15:27,519 --> 00:15:31,600
returns a custom hoodie split containing

00:15:29,839 --> 00:15:33,680
the extra metadata describing the

00:15:31,600 --> 00:15:35,759
associated avrolog

00:15:33,680 --> 00:15:37,600
the getrecordreader method returns a

00:15:35,759 --> 00:15:39,839
custom record reader that has the logic

00:15:37,600 --> 00:15:42,639
to merge the parquet with the avra data

00:15:39,839 --> 00:15:45,199
to give you the end result

00:15:42,639 --> 00:15:47,040
hive for example calls both of these

00:15:45,199 --> 00:15:49,360
methods out of the box

00:15:47,040 --> 00:15:51,839
presto does not as it has its own native

00:15:49,360 --> 00:15:54,240
equivalents for querying parquet

00:15:51,839 --> 00:15:54,959
as i described earlier some time ago the

00:15:54,240 --> 00:15:57,519
hoodie team

00:15:54,959 --> 00:15:59,199
introduced a special annotation use file

00:15:57,519 --> 00:16:01,639
splits from input formats

00:15:59,199 --> 00:16:03,519
it tells presto to call

00:16:01,639 --> 00:16:06,560
inputformat.getsplits

00:16:03,519 --> 00:16:07,680
this allowed read optimized hoodie

00:16:06,560 --> 00:16:09,759
queries to work

00:16:07,680 --> 00:16:11,360
but the hoodie record reader was still

00:16:09,759 --> 00:16:13,600
not used

00:16:11,360 --> 00:16:14,560
presto's vectorized parquet reading was

00:16:13,600 --> 00:16:16,480
used in said

00:16:14,560 --> 00:16:18,560
which was fine for these read optimized

00:16:16,480 --> 00:16:21,759
queries but not compatible with the

00:16:18,560 --> 00:16:24,160
extra logic for snapshot queries

00:16:21,759 --> 00:16:25,279
tables currently use the parquet high of

00:16:24,160 --> 00:16:27,680
cerade

00:16:25,279 --> 00:16:29,199
in presto code this triggers the use of

00:16:27,680 --> 00:16:31,440
the parquet page source

00:16:29,199 --> 00:16:34,959
which does not use this guest record or

00:16:31,440 --> 00:16:37,759
either method from the input format

00:16:34,959 --> 00:16:39,600
so what changes do we need to make to

00:16:37,759 --> 00:16:40,639
allow real-time queries to work in

00:16:39,600 --> 00:16:42,480
presto

00:16:40,639 --> 00:16:44,639
so let's look at these blocks here the

00:16:42,480 --> 00:16:46,720
hoodie team back in 2017

00:16:44,639 --> 00:16:47,839
has already added the use file splits

00:16:46,720 --> 00:16:50,639
from input format

00:16:47,839 --> 00:16:52,160
annotation to cover this first block of

00:16:50,639 --> 00:16:54,320
getting splits

00:16:52,160 --> 00:16:55,920
so now we need to allow presto to use

00:16:54,320 --> 00:16:56,880
the hoodie record reader for reading

00:16:55,920 --> 00:16:58,959
splits

00:16:56,880 --> 00:17:01,920
but we also have another step we need to

00:16:58,959 --> 00:17:04,000
resolve before we can do this

00:17:01,920 --> 00:17:05,120
presto hive has the concept of the hive

00:17:04,000 --> 00:17:07,520
split class

00:17:05,120 --> 00:17:09,919
presto converts whatever split it gets

00:17:07,520 --> 00:17:12,000
into the serializable hive split cast so

00:17:09,919 --> 00:17:14,880
it can be easily passed around

00:17:12,000 --> 00:17:15,679
because hive split by default only takes

00:17:14,880 --> 00:17:18,720
the standard

00:17:15,679 --> 00:17:20,959
hadoop file split fields and misses any

00:17:18,720 --> 00:17:22,079
extra data contained in custom file

00:17:20,959 --> 00:17:24,559
splits

00:17:22,079 --> 00:17:26,240
as the hoodie real-time file split has

00:17:24,559 --> 00:17:28,400
some of these custom fields

00:17:26,240 --> 00:17:30,240
we need to make sure this gets included

00:17:28,400 --> 00:17:32,559
in the hive split

00:17:30,240 --> 00:17:33,760
to do this we added a custom split

00:17:32,559 --> 00:17:36,000
converter interface

00:17:33,760 --> 00:17:38,000
which takes the custom split and returns

00:17:36,000 --> 00:17:39,760
an easily serializable map containing

00:17:38,000 --> 00:17:41,360
this extra metadata

00:17:39,760 --> 00:17:44,000
we can then include this map in

00:17:41,360 --> 00:17:46,320
instances of the hive split class

00:17:44,000 --> 00:17:47,520
this custom split converter interface

00:17:46,320 --> 00:17:49,440
also has a method

00:17:47,520 --> 00:17:52,240
for merging the standard file split

00:17:49,440 --> 00:17:54,640
fields with the custom split map

00:17:52,240 --> 00:17:56,160
in order to return back the original

00:17:54,640 --> 00:17:58,480
file split

00:17:56,160 --> 00:18:00,000
with this combination we can include the

00:17:58,480 --> 00:18:02,320
hoodie real-time file split

00:18:00,000 --> 00:18:04,240
data within the hive split and then

00:18:02,320 --> 00:18:07,280
build it entirely back again

00:18:04,240 --> 00:18:07,280
once passed around

00:18:08,320 --> 00:18:12,160
so now that we have the hoodie real-time

00:18:10,880 --> 00:18:14,320
file split back

00:18:12,160 --> 00:18:16,000
this middle box is taken care of but we

00:18:14,320 --> 00:18:16,880
still have to use the hoodie record

00:18:16,000 --> 00:18:18,960
reader

00:18:16,880 --> 00:18:20,799
as i mentioned earlier hoodie's use of

00:18:18,960 --> 00:18:22,880
the parquet hive surday

00:18:20,799 --> 00:18:24,720
triggers the use of the parquet page

00:18:22,880 --> 00:18:25,520
source which is not what we want as it

00:18:24,720 --> 00:18:28,240
does not call

00:18:25,520 --> 00:18:30,240
getrecord reader to work around this we

00:18:28,240 --> 00:18:32,400
added another adaptation

00:18:30,240 --> 00:18:34,160
use record reader from input format

00:18:32,400 --> 00:18:36,559
which when seen by presto

00:18:34,160 --> 00:18:40,080
skips use of the parquet page source to

00:18:36,559 --> 00:18:42,240
instead use the hive record cursor class

00:18:40,080 --> 00:18:43,600
this class uses the record rater from

00:18:42,240 --> 00:18:46,000
the input format

00:18:43,600 --> 00:18:46,799
completing our chain at the bottom here

00:18:46,000 --> 00:18:50,000
and allowing

00:18:46,799 --> 00:18:51,919
presto snapshot queries to work

00:18:50,000 --> 00:18:53,760
one last thing to note is that these

00:18:51,919 --> 00:18:54,320
snapshot queries take a different code

00:18:53,760 --> 00:18:56,559
path

00:18:54,320 --> 00:18:58,080
than the standard hoodie queries so that

00:18:56,559 --> 00:19:00,240
performance improvements

00:18:58,080 --> 00:19:02,160
such as suda's path filter are not

00:19:00,240 --> 00:19:04,480
affected

00:19:02,160 --> 00:19:05,520
so now that hoodie presto snapshot

00:19:04,480 --> 00:19:09,840
queries work

00:19:05,520 --> 00:19:10,240
what's next i will let suda take that

00:19:09,840 --> 00:19:12,799
yeah

00:19:10,240 --> 00:19:13,600
thanks brenda so i'm going to uh talk

00:19:12,799 --> 00:19:15,039
you through some

00:19:13,600 --> 00:19:16,799
interesting projects that we are working

00:19:15,039 --> 00:19:18,720
on and we believe these

00:19:16,799 --> 00:19:20,160
uh may need deeper integration in

00:19:18,720 --> 00:19:23,679
bristow as well

00:19:20,160 --> 00:19:26,720
so here uh we call them rfcs in uh howdy

00:19:23,679 --> 00:19:27,600
so here is our first rfc rfc 12 that

00:19:26,720 --> 00:19:31,280
talks about

00:19:27,600 --> 00:19:34,080
how to efficiently bootstrap

00:19:31,280 --> 00:19:35,440
larger tables into houdi so the current

00:19:34,080 --> 00:19:37,200
version of bootstrap

00:19:35,440 --> 00:19:39,440
that we have allows for partial

00:19:37,200 --> 00:19:42,559
migrations or

00:19:39,440 --> 00:19:45,039
wherein the older partitions are not uh

00:19:42,559 --> 00:19:46,640
like hoodie does not support hoodie

00:19:45,039 --> 00:19:47,120
primitives like upsets and incrementals

00:19:46,640 --> 00:19:49,360
whereas

00:19:47,120 --> 00:19:51,360
the newer partitions are completely uh

00:19:49,360 --> 00:19:53,840
hoodie

00:19:51,360 --> 00:19:55,280
what completely hoodie supports and if

00:19:53,840 --> 00:19:57,360
you need full support for the entire

00:19:55,280 --> 00:19:59,440
table we ask users to rewrite the entire

00:19:57,360 --> 00:20:01,200
data set and as you can imagine

00:19:59,440 --> 00:20:03,120
that would be time consuming because of

00:20:01,200 --> 00:20:06,320
the the data copy

00:20:03,120 --> 00:20:09,520
so to to make this more efficient or

00:20:06,320 --> 00:20:11,760
the next-gen bootstrap project is to see

00:20:09,520 --> 00:20:12,880
how we can do a zero copy rewrite of the

00:20:11,760 --> 00:20:15,039
entire table

00:20:12,880 --> 00:20:15,919
uh by that what we mean is like we'll be

00:20:15,039 --> 00:20:18,880
laying out

00:20:15,919 --> 00:20:19,200
skeleton parquet files that would have

00:20:18,880 --> 00:20:22,480
the

00:20:19,200 --> 00:20:23,840
hoodie uh metadata and uh kind of

00:20:22,480 --> 00:20:25,919
internally referenced to the

00:20:23,840 --> 00:20:26,960
uh like the data files from the previous

00:20:25,919 --> 00:20:29,520
source table

00:20:26,960 --> 00:20:30,640
so as you can see here the splits get

00:20:29,520 --> 00:20:32,960
even more interesting

00:20:30,640 --> 00:20:34,080
and this is where we could probably

00:20:32,960 --> 00:20:35,760
leverage uh

00:20:34,080 --> 00:20:38,000
what brendan described for the snapshot

00:20:35,760 --> 00:20:40,240
queries we could leverage that

00:20:38,000 --> 00:20:41,039
changes in presto to also support these

00:20:40,240 --> 00:20:44,400
complex

00:20:41,039 --> 00:20:46,159
splits in the bootstrap project and

00:20:44,400 --> 00:20:47,760
we have the next thing is like

00:20:46,159 --> 00:20:50,000
incremental queries and time travel

00:20:47,760 --> 00:20:53,280
queries these are already supported in

00:20:50,000 --> 00:20:55,360
high even spark for hive we pass these

00:20:53,280 --> 00:20:57,760
configs to the hadoop conf

00:20:55,360 --> 00:21:00,640
and we're we're thinking about how to

00:20:57,760 --> 00:21:03,280
support them in presto

00:21:00,640 --> 00:21:05,120
if custom session configs are not like

00:21:03,280 --> 00:21:05,840
possible to be passed on to hadoop conf

00:21:05,120 --> 00:21:08,159
then

00:21:05,840 --> 00:21:09,120
an initial idea is maybe we can register

00:21:08,159 --> 00:21:11,679
the same table

00:21:09,120 --> 00:21:13,360
uh in a new name that way uh incremental

00:21:11,679 --> 00:21:15,280
queries can hit that table

00:21:13,360 --> 00:21:16,720
and probably use the predict like the

00:21:15,280 --> 00:21:18,400
query predicates to

00:21:16,720 --> 00:21:19,760
get other information that are needed

00:21:18,400 --> 00:21:22,080
for incremental queries

00:21:19,760 --> 00:21:23,120
but we definitely welcome any ideas uh

00:21:22,080 --> 00:21:26,559
if uh

00:21:23,120 --> 00:21:29,120
if you guys have through our rfc process

00:21:26,559 --> 00:21:30,000
uh the next project is improvements in

00:21:29,120 --> 00:21:32,320
query planning

00:21:30,000 --> 00:21:34,559
and listing you can see the full rfc

00:21:32,320 --> 00:21:37,679
here linkedin rfc 15.

00:21:34,559 --> 00:21:41,039
uh what this does aims at is uh

00:21:37,679 --> 00:21:43,760
avoid uh listing as much as possible

00:21:41,039 --> 00:21:45,440
so hodi writes and reads require the

00:21:43,760 --> 00:21:46,799
list status operation and this can be

00:21:45,440 --> 00:21:48,799
really expensive

00:21:46,799 --> 00:21:50,640
when we talk about large datasets that

00:21:48,799 --> 00:21:50,880
has like huge number of partitions and

00:21:50,640 --> 00:21:52,640
he

00:21:50,880 --> 00:21:55,200
each partition has like huge number of

00:21:52,640 --> 00:21:56,799
files so who do you already maintains

00:21:55,200 --> 00:21:59,200
this metadata on

00:21:56,799 --> 00:22:00,960
files like what files went into what

00:21:59,200 --> 00:22:02,880
operations sort of that that

00:22:00,960 --> 00:22:05,520
part is already there what we're looking

00:22:02,880 --> 00:22:08,559
at is to consolidate all of that

00:22:05,520 --> 00:22:11,120
into uh like a metadata snapshot so

00:22:08,559 --> 00:22:13,280
we can avoid uh listing by leveraging

00:22:11,120 --> 00:22:14,400
that and while we are building the data

00:22:13,280 --> 00:22:17,039
infrastructure

00:22:14,400 --> 00:22:18,720
for this we are also thinking about

00:22:17,039 --> 00:22:22,080
exposing range stats for

00:22:18,720 --> 00:22:22,640
columns and that would a aid in pruning

00:22:22,080 --> 00:22:24,960
queries

00:22:22,640 --> 00:22:26,720
uh effectively so this is yet another

00:22:24,960 --> 00:22:29,600
area where we are looking into

00:22:26,720 --> 00:22:30,559
see how presto can leverage this um i

00:22:29,600 --> 00:22:33,760
think

00:22:30,559 --> 00:22:35,840
hive exposes this vega table stats maybe

00:22:33,760 --> 00:22:37,600
something like that would help probably

00:22:35,840 --> 00:22:39,440
uh like building a new presto

00:22:37,600 --> 00:22:42,640
hoodie connector would help we're still

00:22:39,440 --> 00:22:45,919
exploring these ideas in presto

00:22:42,640 --> 00:22:46,960
and uh the last project that i wanted to

00:22:45,919 --> 00:22:50,240
talk about

00:22:46,960 --> 00:22:51,760
was uh record level indexes so i already

00:22:50,240 --> 00:22:54,480
spoke about absurds

00:22:51,760 --> 00:22:57,440
and these are like a popular operation

00:22:54,480 --> 00:22:59,760
in write operation in houdi

00:22:57,440 --> 00:23:01,280
to make them even more faster hoodie

00:22:59,760 --> 00:23:04,480
leverages indexing

00:23:01,280 --> 00:23:06,480
and what indexing does is basically tag

00:23:04,480 --> 00:23:07,600
like provide a record id to file id

00:23:06,480 --> 00:23:10,799
mapping so

00:23:07,600 --> 00:23:12,559
you can easily tag incoming records as

00:23:10,799 --> 00:23:15,280
either updates or

00:23:12,559 --> 00:23:17,039
inserts we have existing index

00:23:15,280 --> 00:23:17,919
implementations like bloom filters key

00:23:17,039 --> 00:23:20,880
ranges and

00:23:17,919 --> 00:23:22,080
hedge face implementation as well bloom

00:23:20,880 --> 00:23:25,360
filters are maintained

00:23:22,080 --> 00:23:28,080
in the parquet data files uh footer so

00:23:25,360 --> 00:23:30,400
but these come in a car at a cost when

00:23:28,080 --> 00:23:31,520
we speak about really large data sets uh

00:23:30,400 --> 00:23:34,400
for example

00:23:31,520 --> 00:23:34,880
consider a non-partitioned hoodie table

00:23:34,400 --> 00:23:37,919
where

00:23:34,880 --> 00:23:40,799
uh do for absurds to work and

00:23:37,919 --> 00:23:42,240
like the every bloom filter in all the

00:23:40,799 --> 00:23:44,559
files have to be scanned

00:23:42,240 --> 00:23:45,919
to kind of use the indexing this can be

00:23:44,559 --> 00:23:48,400
like really expensive

00:23:45,919 --> 00:23:50,960
so we're looking at introducing another

00:23:48,400 --> 00:23:53,360
index implementation record level index

00:23:50,960 --> 00:23:55,679
and uh this would be native and hoodie

00:23:53,360 --> 00:23:57,520
uh the the reason again is like with the

00:23:55,679 --> 00:23:58,960
hp's implementation some users in the

00:23:57,520 --> 00:24:00,640
community feel that it could be

00:23:58,960 --> 00:24:01,520
operationally overhead like operating

00:24:00,640 --> 00:24:04,480
another system

00:24:01,520 --> 00:24:06,640
entirely so that record level index

00:24:04,480 --> 00:24:09,440
would also try to tackle that by

00:24:06,640 --> 00:24:11,200
um like a native uh implementation in

00:24:09,440 --> 00:24:13,679
hoodie that like that hoodie can

00:24:11,200 --> 00:24:15,279
maintain it would uh consider like

00:24:13,679 --> 00:24:16,559
plugable storage like hedge files or

00:24:15,279 --> 00:24:19,360
rock 3b for

00:24:16,559 --> 00:24:21,279
keeping this index uh we believe it

00:24:19,360 --> 00:24:22,960
would be much faster to scan

00:24:21,279 --> 00:24:25,279
these few index files rather than

00:24:22,960 --> 00:24:26,159
scanning like all of the bloom filters

00:24:25,279 --> 00:24:28,159
and all the

00:24:26,159 --> 00:24:29,919
data files and we believe this would

00:24:28,159 --> 00:24:32,159
improve

00:24:29,919 --> 00:24:33,440
the absurd performance way more for

00:24:32,159 --> 00:24:36,159
large data sets

00:24:33,440 --> 00:24:38,400
so this again is another area where uh

00:24:36,159 --> 00:24:39,520
so far we haven't used the indexing on

00:24:38,400 --> 00:24:42,559
the query side to

00:24:39,520 --> 00:24:44,400
prune uh electron or like avoid the

00:24:42,559 --> 00:24:46,000
avoid listing so many files but this is

00:24:44,400 --> 00:24:50,159
again another area where we are looking

00:24:46,000 --> 00:24:52,960
at using this in the query side as well

00:24:50,159 --> 00:24:54,880
cool with that these are the resources

00:24:52,960 --> 00:24:56,480
if you have any ideas suggestions or

00:24:54,880 --> 00:24:59,440
have questions please feel to connect

00:24:56,480 --> 00:25:02,159
with us and we're really excited to

00:24:59,440 --> 00:25:04,080
work in these communities and we can

00:25:02,159 --> 00:25:07,120
open up for questions

00:25:04,080 --> 00:25:08,799
great thanks sudha and brandon

00:25:07,120 --> 00:25:10,320
there are a couple of questions that

00:25:08,799 --> 00:25:11,760
have come in um so

00:25:10,320 --> 00:25:14,240
i will read those out for you so the

00:25:11,760 --> 00:25:17,120
first one is from arun

00:25:14,240 --> 00:25:18,000
where is the timeline metadata stored

00:25:17,120 --> 00:25:20,240
and how is

00:25:18,000 --> 00:25:21,520
uh uh how different is this from table

00:25:20,240 --> 00:25:24,480
stats

00:25:21,520 --> 00:25:26,720
right so the the timeline metadata is

00:25:24,480 --> 00:25:30,159
stored in the dot hoodie

00:25:26,720 --> 00:25:31,360
folder and the table stats would also be

00:25:30,159 --> 00:25:34,720
stored along

00:25:31,360 --> 00:25:36,640
in the same uh directory structure like

00:25:34,720 --> 00:25:38,400
partitions will have their own partition

00:25:36,640 --> 00:25:39,520
level stats and table level stats would

00:25:38,400 --> 00:25:43,520
be stored in the dot

00:25:39,520 --> 00:25:46,320
dot hoodie partition

00:25:43,520 --> 00:25:48,400
all right thank you um and then the next

00:25:46,320 --> 00:25:49,279
question i think uh brandon you may have

00:25:48,400 --> 00:25:52,720
answered it but

00:25:49,279 --> 00:25:54,840
i'll ask it here uh do you use uh native

00:25:52,720 --> 00:25:57,840
avro sturdy so serialization

00:25:54,840 --> 00:25:57,840
decentralization

00:25:59,840 --> 00:26:03,600
oh yeah i mean we i mentioned that we we

00:26:02,640 --> 00:26:06,159
register the

00:26:03,600 --> 00:26:07,520
the hive table as using the power k hive

00:26:06,159 --> 00:26:10,559
saturday

00:26:07,520 --> 00:26:12,880
okay got it

00:26:10,559 --> 00:26:14,159
all right i think those are a couple of

00:26:12,880 --> 00:26:16,240
questions folks if you have more

00:26:14,159 --> 00:26:18,880
questions please go ahead and uh

00:26:16,240 --> 00:26:20,240
ask them in our q a uh the q a panel

00:26:18,880 --> 00:26:22,480
that you see at the bottom

00:26:20,240 --> 00:26:23,520
uh sudha and brandon thanks for sharing

00:26:22,480 --> 00:26:26,080
if you can go ahead and

00:26:23,520 --> 00:26:28,960
stop sharing we'll move on to our our

00:26:26,080 --> 00:26:28,960

YouTube URL: https://www.youtube.com/watch?v=nA3rwOdmm3A


