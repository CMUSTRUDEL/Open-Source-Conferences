Title: Enterprise Distributed Query Service powered by Presto & Alluxio across clouds at WalmartLabs
Publication date: 2020-07-08
Playlist: Presto Events
Description: 
	
Captions: 
	00:00:03,259 --> 00:00:08,910
hello everyone this is Ashish

00:00:06,029 --> 00:00:11,580
just good to be here and present at the

00:00:08,910 --> 00:00:13,710
first data orchestration conference I

00:00:11,580 --> 00:00:17,880
would like to present some interesting

00:00:13,710 --> 00:00:21,179
use cases with which for which we have

00:00:17,880 --> 00:00:24,269
been using Alexio with presto and were

00:00:21,179 --> 00:00:26,849
able to solve some data analytics

00:00:24,269 --> 00:00:32,310
interactive and ad-hoc query solutions

00:00:26,849 --> 00:00:35,730
at Walmart labs so as the agenda will go

00:00:32,310 --> 00:00:37,829
I initially will probably try to lay out

00:00:35,730 --> 00:00:40,019
was the problem statement was why we

00:00:37,829 --> 00:00:42,829
were even looking for a analytical

00:00:40,019 --> 00:00:46,230
solution despite our all existing stores

00:00:42,829 --> 00:00:48,269
while there was a dire need to have a

00:00:46,230 --> 00:00:51,000
solution which can do a federated query

00:00:48,269 --> 00:00:53,929
and do a level of data orchestration as

00:00:51,000 --> 00:00:56,760
we were embarking into a cloud journey

00:00:53,929 --> 00:00:59,609
hybrid and a multi cloud journey then

00:00:56,760 --> 00:01:03,809
I'll probably try to give an overview of

00:00:59,609 --> 00:01:06,500
a high level architecture how we kicked

00:01:03,809 --> 00:01:09,270
off this initial stack which was solving

00:01:06,500 --> 00:01:11,460
most of the problem and how that stack

00:01:09,270 --> 00:01:15,420
got evolved when we get involved in a

00:01:11,460 --> 00:01:18,170
hybrid cloud situation so we try to

00:01:15,420 --> 00:01:21,930
cover an architecture component we'll

00:01:18,170 --> 00:01:24,930
try to give a sense of how we leverages

00:01:21,930 --> 00:01:27,570
these two technologies and where and at

00:01:24,930 --> 00:01:30,270
the other to make other places to make

00:01:27,570 --> 00:01:32,040
it as a enterprise offering we have to

00:01:30,270 --> 00:01:34,890
build certain ecosystem projects

00:01:32,040 --> 00:01:40,860
internally to make this a managed query

00:01:34,890 --> 00:01:43,200
service so to begin with what I've seen

00:01:40,860 --> 00:01:46,410
in Walmart and it's same is the case for

00:01:43,200 --> 00:01:48,720
any larger enterprises so there are data

00:01:46,410 --> 00:01:50,760
is usually reciting a lot of different

00:01:48,720 --> 00:01:53,820
diverse sources and the reason being

00:01:50,760 --> 00:01:58,050
there are no one-stop solution which can

00:01:53,820 --> 00:02:02,010
cater to all the different analytical or

00:01:58,050 --> 00:02:04,620
data query workload so there are systems

00:02:02,010 --> 00:02:07,170
such as Terra data which is good for

00:02:04,620 --> 00:02:10,619
ultra-low latency queries there are

00:02:07,170 --> 00:02:14,430
systems such as SAP HANA I pretty much

00:02:10,619 --> 00:02:15,160
seen every database in sequel no sequel

00:02:14,430 --> 00:02:17,290
technology

00:02:15,160 --> 00:02:18,880
which exists today which being widely

00:02:17,290 --> 00:02:23,040
used at Walmart so this is just a

00:02:18,880 --> 00:02:25,720
smaller snapshot and there has been

00:02:23,040 --> 00:02:27,940
every such technology has its own

00:02:25,720 --> 00:02:30,580
rationale to use that so there are there

00:02:27,940 --> 00:02:33,640
are teams which largely use druid

00:02:30,580 --> 00:02:36,970
clusters for a time series analysis so

00:02:33,640 --> 00:02:39,040
every bit of thing has its own use case

00:02:36,970 --> 00:02:40,570
and we saw that there's no one-stop

00:02:39,040 --> 00:02:44,170
solution and we don't want to solve that

00:02:40,570 --> 00:02:46,390
problem as team and that use cases has a

00:02:44,170 --> 00:02:48,850
different needs to leverage this

00:02:46,390 --> 00:02:51,790
technologies we we wanted to ensure that

00:02:48,850 --> 00:02:55,060
it the this is how this piece is going

00:02:51,790 --> 00:02:57,850
to be and even the cloud coming into a

00:02:55,060 --> 00:03:00,010
picture most of the workloads are moving

00:02:57,850 --> 00:03:07,060
into objects towards and the different

00:03:00,010 --> 00:03:09,730
stores the there were a lot of

00:03:07,060 --> 00:03:12,340
initiatives as part of data like

00:03:09,730 --> 00:03:14,620
offering to ensure that let's say all of

00:03:12,340 --> 00:03:16,870
all of the stores are good which are

00:03:14,620 --> 00:03:21,040
they're serving a purpose to powering

00:03:16,870 --> 00:03:24,070
different applications but when it was

00:03:21,040 --> 00:03:28,209
becoming very cumbersome for any analyst

00:03:24,070 --> 00:03:30,730
to do analyze all of this data so one of

00:03:28,209 --> 00:03:35,170
the initiative was having large enough

00:03:30,730 --> 00:03:38,970
data like cluster which were which which

00:03:35,170 --> 00:03:42,220
was supposed to act as a data hub so

00:03:38,970 --> 00:03:44,200
where all the data from this different

00:03:42,220 --> 00:03:47,110
diverse sources will get source into

00:03:44,200 --> 00:03:49,060
this data like cluster so which was

00:03:47,110 --> 00:03:50,860
working good so it's been I think more

00:03:49,060 --> 00:03:52,810
than three four years at Walmart there

00:03:50,860 --> 00:03:54,910
has been large enough for data like

00:03:52,810 --> 00:03:57,520
cluster where the data from all

00:03:54,910 --> 00:04:00,760
different places were big getting source

00:03:57,520 --> 00:04:04,720
it was obviously powered by Hadoop

00:04:00,760 --> 00:04:07,180
cluster it where the default processing

00:04:04,720 --> 00:04:09,670
engine which were available such as hi

00:04:07,180 --> 00:04:12,850
MapReduce in spark they were working

00:04:09,670 --> 00:04:16,180
well to some extent but even this data

00:04:12,850 --> 00:04:19,419
lake ecosystem powered by Hadoop has its

00:04:16,180 --> 00:04:22,270
own challenges so one of which was it

00:04:19,419 --> 00:04:24,610
was becoming extremely cumbersome to

00:04:22,270 --> 00:04:26,660
source the data from different sources

00:04:24,610 --> 00:04:29,120
to get to our Dukla

00:04:26,660 --> 00:04:32,090
Oobleck even if you set up large pipe

00:04:29,120 --> 00:04:34,610
lies ETL pipe lamps it's it's really a

00:04:32,090 --> 00:04:37,240
tough job with a growing volume of the

00:04:34,610 --> 00:04:40,670
data to ensure that all of that data

00:04:37,240 --> 00:04:42,680
reaches to Hadoop data like clustering

00:04:40,670 --> 00:04:45,290
time because all the analysts will be

00:04:42,680 --> 00:04:47,150
waiting for data to reach to that place

00:04:45,290 --> 00:04:50,270
so that was one challenging aspect

00:04:47,150 --> 00:04:53,630
second was most of this Hadoop data like

00:04:50,270 --> 00:04:57,800
cluster as are built from a bare-metal

00:04:53,630 --> 00:04:59,510
machines so and even though if you have

00:04:57,800 --> 00:05:01,550
a large enough cluster it will always

00:04:59,510 --> 00:05:07,580
get challenged by that upcoming news

00:05:01,550 --> 00:05:09,770
cases new volume being increasing beyond

00:05:07,580 --> 00:05:13,040
what you have already planned for and we

00:05:09,770 --> 00:05:16,580
all know it's not easy for easy to scale

00:05:13,040 --> 00:05:18,860
out the bare metal cluster it's always a

00:05:16,580 --> 00:05:21,290
lot of things you have to do you have to

00:05:18,860 --> 00:05:24,080
buy hardware machines get a Pio

00:05:21,290 --> 00:05:26,660
do a racking stacking so usually scaling

00:05:24,080 --> 00:05:28,580
out on prim cluster takes few days to a

00:05:26,660 --> 00:05:31,730
month's time so these were two

00:05:28,580 --> 00:05:33,640
challenges that you know getting getting

00:05:31,730 --> 00:05:37,430
data source into a data like cluster

00:05:33,640 --> 00:05:39,740
scalability of the existing cluster then

00:05:37,430 --> 00:05:44,240
we were looking of acceleration solution

00:05:39,740 --> 00:05:48,260
said how can we find and acquiring air

00:05:44,240 --> 00:05:51,500
for which we can avoid copying data from

00:05:48,260 --> 00:05:55,450
different sources terms to Hadoop data

00:05:51,500 --> 00:05:58,220
like cluster and how about getting an

00:05:55,450 --> 00:06:00,500
acceleration or a query layer for which

00:05:58,220 --> 00:06:02,630
we don't get bogged by a compute

00:06:00,500 --> 00:06:04,580
capacity of the data like clusters so

00:06:02,630 --> 00:06:06,650
there that was a time we were evaluating

00:06:04,580 --> 00:06:11,840
different technologies and we looked at

00:06:06,650 --> 00:06:14,470
presto presto exactly does that for a

00:06:11,840 --> 00:06:17,510
for a latter use case it has an amazing

00:06:14,470 --> 00:06:22,190
separation of storage and compute so we

00:06:17,510 --> 00:06:25,210
did a lot of pilot of had a remote

00:06:22,190 --> 00:06:28,250
presto cluster not extremely remote a

00:06:25,210 --> 00:06:31,700
probably in the same zone where our data

00:06:28,250 --> 00:06:35,420
a cluster are but external remote presto

00:06:31,700 --> 00:06:38,120
cluster which were able to consume that

00:06:35,420 --> 00:06:41,420
from the data like Hadoop cluster and we

00:06:38,120 --> 00:06:44,630
saw it was quite performant it was to

00:06:41,420 --> 00:06:46,550
some extent it the response time was far

00:06:44,630 --> 00:06:49,580
better than the existing - spark

00:06:46,550 --> 00:06:53,900
pipeline so that gives us and an

00:06:49,580 --> 00:06:56,060
acceleration layer which we can which we

00:06:53,900 --> 00:06:58,640
can put in our stack which will not get

00:06:56,060 --> 00:07:02,360
back by the scalability aspect and the

00:06:58,640 --> 00:07:04,370
other amazing feature which we were when

00:07:02,360 --> 00:07:07,040
we when we looked at it is was the

00:07:04,370 --> 00:07:10,130
ability to do a federated query so when

00:07:07,040 --> 00:07:15,080
we did analyze that anybody who wanted

00:07:10,130 --> 00:07:16,760
to you know query r2 for to generate

00:07:15,080 --> 00:07:19,490
their insights and reporting they had to

00:07:16,760 --> 00:07:22,820
source all of the data to Hadoop data

00:07:19,490 --> 00:07:25,520
like cluster and many a times they are

00:07:22,820 --> 00:07:28,610
interested in to a very fragmentation of

00:07:25,520 --> 00:07:31,520
the data but they still tend to get the

00:07:28,610 --> 00:07:35,120
entire dataset into a Hadoop data like

00:07:31,520 --> 00:07:38,780
clusters so we saw presto already has a

00:07:35,120 --> 00:07:40,460
good federated engine where it can talk

00:07:38,780 --> 00:07:43,490
to and have the ability to query the

00:07:40,460 --> 00:07:47,570
data without moving into Hadoop

00:07:43,490 --> 00:07:49,340
ecosystem so so we it has already

00:07:47,570 --> 00:07:51,770
inbuilt connector for Cassandra

00:07:49,340 --> 00:07:55,640
we have a huge adoption on Tara Nate and

00:07:51,770 --> 00:07:58,700
Cassandra and we saw that it was super

00:07:55,640 --> 00:08:00,500
easy for presto to talk to Cass a talk

00:07:58,700 --> 00:08:05,600
to Cassandra directly without moving

00:08:00,500 --> 00:08:09,050
that workloads into the Hadoop data

00:08:05,600 --> 00:08:11,660
Lakes so for those reasons we looked at

00:08:09,050 --> 00:08:16,850
we we did a pilot with one of our

00:08:11,660 --> 00:08:19,340
e-commerce team which were which were we

00:08:16,850 --> 00:08:23,210
gave them this solution which was a look

00:08:19,340 --> 00:08:26,120
they already had and VI integration tool

00:08:23,210 --> 00:08:29,540
which was talking to - spark through

00:08:26,120 --> 00:08:32,330
Luca so we gave them and cluster where

00:08:29,540 --> 00:08:35,060
Luca and Luca was talking to presto and

00:08:32,330 --> 00:08:37,310
they were amazed by that they they not

00:08:35,060 --> 00:08:40,370
only reported a better response time but

00:08:37,310 --> 00:08:42,229
even they were amazed by the ability of

00:08:40,370 --> 00:08:45,230
doing a federated query without movement

00:08:42,229 --> 00:08:48,700
of data so now that was just one tenant

00:08:45,230 --> 00:08:51,970
then later on we thought that how can

00:08:48,700 --> 00:08:53,740
make it you know available for all the

00:08:51,970 --> 00:08:56,830
different Walmart international

00:08:53,740 --> 00:08:59,980
businesses how about we don't want to be

00:08:56,830 --> 00:09:01,600
in a position where we keep on going and

00:08:59,980 --> 00:09:03,910
spending a different clusters for every

00:09:01,600 --> 00:09:06,880
different team so we wanted to solve

00:09:03,910 --> 00:09:09,010
that problem how do we leverage this

00:09:06,880 --> 00:09:11,640
technology and make an enterprise

00:09:09,010 --> 00:09:14,980
managed squarey service offering very

00:09:11,640 --> 00:09:17,140
teams or users just come in they get on

00:09:14,980 --> 00:09:19,690
boarded and they start wearing the data

00:09:17,140 --> 00:09:24,030
and discover the data all the different

00:09:19,690 --> 00:09:26,410
data sets which are across systems so

00:09:24,030 --> 00:09:29,050
when we thought of building that

00:09:26,410 --> 00:09:31,080
enterprise query offering as a managed

00:09:29,050 --> 00:09:33,130
service there are lot many things beyond

00:09:31,080 --> 00:09:34,240
just pressed to which we have to build

00:09:33,130 --> 00:09:37,450
we have been sure that the

00:09:34,240 --> 00:09:39,520
authentication is taken care of we have

00:09:37,450 --> 00:09:43,450
to ensure that security is not

00:09:39,520 --> 00:09:45,670
compromised make for the different level

00:09:43,450 --> 00:09:47,410
of sensitivity of the data we need to

00:09:45,670 --> 00:09:50,100
ensure that there are strong role based

00:09:47,410 --> 00:09:53,640
access control so most of our

00:09:50,100 --> 00:09:57,040
authorization policies are residing in a

00:09:53,640 --> 00:09:58,870
component called Apache Ranger so we are

00:09:57,040 --> 00:10:00,820
to ensure that we have a the build

00:09:58,870 --> 00:10:02,890
security plug-in which can talk to range

00:10:00,820 --> 00:10:04,960
and and do that thorough validation so

00:10:02,890 --> 00:10:07,630
we don't skip calling across the

00:10:04,960 --> 00:10:11,470
security we have to in short for

00:10:07,630 --> 00:10:13,120
sensitivity of the data that it supports

00:10:11,470 --> 00:10:16,060
encryption over the wire and encryption

00:10:13,120 --> 00:10:17,680
at rest than other non-functional

00:10:16,060 --> 00:10:20,560
requirements that have a robust

00:10:17,680 --> 00:10:22,810
monitoring metering in place to ensure

00:10:20,560 --> 00:10:25,780
that the teams which requires dedicated

00:10:22,810 --> 00:10:30,340
SLA can have we can provision that in

00:10:25,780 --> 00:10:33,520
the manage query service so so with this

00:10:30,340 --> 00:10:35,740
requirement we were able to build that

00:10:33,520 --> 00:10:39,760
manage query service offering leveraging

00:10:35,740 --> 00:10:42,640
presto initially and it was it was

00:10:39,760 --> 00:10:44,830
pretty amazing so when team looked at it

00:10:42,640 --> 00:10:48,520
they hey now they don't have to send a

00:10:44,830 --> 00:10:50,860
capacity request they don't have to log

00:10:48,520 --> 00:10:52,960
in to any edge nodes they can have their

00:10:50,860 --> 00:10:56,080
existing BI tools talk to this query

00:10:52,960 --> 00:10:58,900
service directly even

00:10:56,080 --> 00:11:00,760
maybe they have existing query services

00:10:58,900 --> 00:11:04,360
framework they can just swap and use

00:11:00,760 --> 00:11:09,610
this query service and without can do a

00:11:04,360 --> 00:11:12,490
seamless migration so this is our

00:11:09,610 --> 00:11:15,190
initial architecture component what it

00:11:12,490 --> 00:11:18,910
looks like we as I mentioned we started

00:11:15,190 --> 00:11:21,790
off with a large multi tenant presto

00:11:18,910 --> 00:11:23,260
cluster which was configured with the

00:11:21,790 --> 00:11:27,490
connectors we support so they were

00:11:23,260 --> 00:11:31,720
connectors for definitely hive in

00:11:27,490 --> 00:11:35,080
on-prem it was hive / HDFS and GCS it

00:11:31,720 --> 00:11:38,260
was high backed by GCS tables then they

00:11:35,080 --> 00:11:41,410
were a bunch of other connectors we had

00:11:38,260 --> 00:11:44,980
included mostly the sequel databases or

00:11:41,410 --> 00:11:47,740
actual my sequel then Cassandra a lot of

00:11:44,980 --> 00:11:49,830
team uses a Kafka connector as well to

00:11:47,740 --> 00:11:53,860
do an operational and analytics of how

00:11:49,830 --> 00:11:56,170
their Kafka pipelines are going on all

00:11:53,860 --> 00:11:59,200
of that they were able to do that with

00:11:56,170 --> 00:12:01,210
one unified and the best thing the one

00:11:59,200 --> 00:12:03,550
of the feedback we got is is the one

00:12:01,210 --> 00:12:05,260
sequel language they have to write so as

00:12:03,550 --> 00:12:07,300
and when we keep adding connector they

00:12:05,260 --> 00:12:09,280
don't have to have a pain to go and

00:12:07,300 --> 00:12:13,000
learn different technology it's the one

00:12:09,280 --> 00:12:14,650
and C sequel language and which will do

00:12:13,000 --> 00:12:19,090
the job for it and there are a lot of

00:12:14,650 --> 00:12:21,820
any other analysts who have been used to

00:12:19,090 --> 00:12:23,650
of leveraging bi tool where they are not

00:12:21,820 --> 00:12:26,680
used to of writing any sequels as well

00:12:23,650 --> 00:12:28,240
so if you know tools like looker and B

00:12:26,680 --> 00:12:31,120
you can just connect to your data

00:12:28,240 --> 00:12:33,580
sources and you can just do a slice and

00:12:31,120 --> 00:12:35,830
dice without writing any queries the

00:12:33,580 --> 00:12:37,960
amazing part was press tours already had

00:12:35,830 --> 00:12:40,570
an integration with this this bi tools

00:12:37,960 --> 00:12:44,190
were able to write presto queries so

00:12:40,570 --> 00:12:46,600
that had been also a seamless migration

00:12:44,190 --> 00:12:49,470
there there are a bunch of things you

00:12:46,600 --> 00:12:52,360
would require to make it at an

00:12:49,470 --> 00:12:54,250
enterprise-grade offering apart from

00:12:52,360 --> 00:12:57,550
authentication and authorization you

00:12:54,250 --> 00:13:00,790
want to ensure that you don't provision

00:12:57,550 --> 00:13:03,280
this cluster with something service ID

00:13:00,790 --> 00:13:05,260
which have access to everything so there

00:13:03,280 --> 00:13:07,290
are additional things you would want to

00:13:05,260 --> 00:13:09,590
take a look at where

00:13:07,290 --> 00:13:13,370
especially non-prime or a Hadoop cluster

00:13:09,590 --> 00:13:15,900
you enable impersonation so that this

00:13:13,370 --> 00:13:19,710
clusters run with services which can

00:13:15,900 --> 00:13:21,390
impersonate user than giving or this

00:13:19,710 --> 00:13:23,820
service account all the accesses

00:13:21,390 --> 00:13:31,290
possible which could be compromising

00:13:23,820 --> 00:13:33,480
security on quota to provision dedicated

00:13:31,290 --> 00:13:36,350
quota presto already had some mechanism

00:13:33,480 --> 00:13:40,800
of resource queue which we leverage that

00:13:36,350 --> 00:13:42,750
to short week different so when we

00:13:40,800 --> 00:13:45,660
observed or workloads has been

00:13:42,750 --> 00:13:49,340
widespread as presto is used for a

00:13:45,660 --> 00:13:52,590
generic query layer so there they were

00:13:49,340 --> 00:13:56,580
uses has been a finer lower-level query

00:13:52,590 --> 00:13:58,950
to a minor level ETL jobs as well so we

00:13:56,580 --> 00:14:02,160
do provide them an ability for

00:13:58,950 --> 00:14:04,860
individual teams to configure their

00:14:02,160 --> 00:14:09,030
query configuration separately with the

00:14:04,860 --> 00:14:11,970
mechanism of session managers and our

00:14:09,030 --> 00:14:14,850
offering which was in in cloud we have

00:14:11,970 --> 00:14:20,280
to be very cognizant of cost so what we

00:14:14,850 --> 00:14:23,010
had developers we we actually written a

00:14:20,280 --> 00:14:26,270
lower-level auto-scaling model which

00:14:23,010 --> 00:14:29,790
ensures that our cluster get auto scale

00:14:26,270 --> 00:14:31,680
based on the load average increases when

00:14:29,790 --> 00:14:35,160
the concurrency of queries increases

00:14:31,680 --> 00:14:38,340
there are more queued queries so that in

00:14:35,160 --> 00:14:42,210
that way the clustered or cluster which

00:14:38,340 --> 00:14:45,870
runs in GCP get auto scaled from as low

00:14:42,210 --> 00:14:48,240
as 80 machines to 600 700 machines

00:14:45,870 --> 00:14:51,960
depending on the workload so we can curb

00:14:48,240 --> 00:14:55,130
the cost and keeping the cluster

00:14:51,960 --> 00:14:55,130
utilization high

00:14:55,200 --> 00:15:01,860
even with this operon vaguer we observed

00:14:58,470 --> 00:15:04,410
few of the things what as the

00:15:01,860 --> 00:15:06,090
fundamental of presto which is a good

00:15:04,410 --> 00:15:09,540
thing which it says that it gives you a

00:15:06,090 --> 00:15:11,940
source separation goods to separation of

00:15:09,540 --> 00:15:15,950
storage and compute which comes with the

00:15:11,940 --> 00:15:19,100
problem sometimes this separation makes

00:15:15,950 --> 00:15:20,630
presto for every little query and

00:15:19,100 --> 00:15:24,590
every current query to pull the data

00:15:20,630 --> 00:15:26,960
from the source again and that either in

00:15:24,590 --> 00:15:31,100
on Prem or in cloud where your data is

00:15:26,960 --> 00:15:33,230
sitting over object storage the

00:15:31,100 --> 00:15:37,280
prominent thing what presto needs is a

00:15:33,230 --> 00:15:39,350
higher network i/o so and and if you see

00:15:37,280 --> 00:15:41,060
that network i/o is something which you

00:15:39,350 --> 00:15:43,520
can never cap and guarantee there are

00:15:41,060 --> 00:15:46,010
always even in cloud there are always

00:15:43,520 --> 00:15:47,870
issues of like noisy neighbors there are

00:15:46,010 --> 00:15:52,070
some other application which are choking

00:15:47,870 --> 00:15:53,480
bandwidth which will which will due to

00:15:52,070 --> 00:15:56,390
which you will not have a predictable

00:15:53,480 --> 00:15:59,690
network i/o and which which which was

00:15:56,390 --> 00:16:02,060
reflected and the query performance as

00:15:59,690 --> 00:16:04,310
well so we we have done some analysis

00:16:02,060 --> 00:16:08,330
and we saw we looked at it that we

00:16:04,310 --> 00:16:10,660
scheduled one one query across a days as

00:16:08,330 --> 00:16:13,400
the same query had a very choppy

00:16:10,660 --> 00:16:16,430
performance response time and it was all

00:16:13,400 --> 00:16:18,380
because of the network i/o and and we

00:16:16,430 --> 00:16:20,870
wanted to tackle that problem asked

00:16:18,380 --> 00:16:24,260
because you do not want to have response

00:16:20,870 --> 00:16:26,210
time were very flaky sometimes it the

00:16:24,260 --> 00:16:28,640
same query comebacks in two second and

00:16:26,210 --> 00:16:30,710
at times it takes like more than 15-20

00:16:28,640 --> 00:16:34,880
seconds because of the network i/o and

00:16:30,710 --> 00:16:37,910
then other thing we looked at is most of

00:16:34,880 --> 00:16:43,060
our analytical data was modeled in star

00:16:37,910 --> 00:16:47,060
schema because our analytical data said

00:16:43,060 --> 00:16:49,190
you know have some slowly changing

00:16:47,060 --> 00:16:51,950
dimensions of type two where they cannot

00:16:49,190 --> 00:16:54,350
actually form a denormalize pre join

00:16:51,950 --> 00:16:58,820
tables so when we looked at in the star

00:16:54,350 --> 00:17:01,010
schema model when we analyze how quick

00:16:58,820 --> 00:17:02,840
different query patterns the dimension

00:17:01,010 --> 00:17:05,540
tables are heavily used again and again

00:17:02,840 --> 00:17:08,270
and we saw an opportunity when every

00:17:05,540 --> 00:17:10,790
time a query runs it has to go and fetch

00:17:08,270 --> 00:17:12,980
the dimension table all over again which

00:17:10,790 --> 00:17:17,440
are frequently used so that was a time

00:17:12,980 --> 00:17:21,110
we looked at in a solution of which can

00:17:17,440 --> 00:17:24,800
obviously cache this level of data to

00:17:21,110 --> 00:17:26,690
elevate the network i/o and even queries

00:17:24,800 --> 00:17:29,290
which are frequently running over some

00:17:26,690 --> 00:17:32,360
data set can leverage this case as well

00:17:29,290 --> 00:17:34,640
and when we looked at it there are

00:17:32,360 --> 00:17:36,350
and many solutions out there in the

00:17:34,640 --> 00:17:37,429
market even in the industry in the

00:17:36,350 --> 00:17:40,610
open-source community

00:17:37,429 --> 00:17:42,980
Aleksey was the most prominent one we

00:17:40,610 --> 00:17:46,460
did some quick evaluations and the first

00:17:42,980 --> 00:17:50,260
one it is highlighted there when we have

00:17:46,460 --> 00:17:54,190
presto co-located with Alexio it

00:17:50,260 --> 00:17:57,530
significantly reduces the network i/o so

00:17:54,190 --> 00:18:00,679
definitely we get a consistent through

00:17:57,530 --> 00:18:04,160
port when we have presto with Alexia

00:18:00,679 --> 00:18:06,500
though so that that every query if the

00:18:04,160 --> 00:18:08,809
data of subsequent queries are cached it

00:18:06,500 --> 00:18:12,320
can elevate going to that unpredictable

00:18:08,809 --> 00:18:14,570
Network IO and secondly when when we

00:18:12,320 --> 00:18:17,570
have an Alexia based cache we have when

00:18:14,570 --> 00:18:20,090
we had a finite in-memory cache pool we

00:18:17,570 --> 00:18:22,970
also tried pinning specific dimensions

00:18:20,090 --> 00:18:26,419
data sets which are being used

00:18:22,970 --> 00:18:28,730
frequently so that also gave a

00:18:26,419 --> 00:18:31,429
significant performance boost in the

00:18:28,730 --> 00:18:34,600
query workload of which was leveraging

00:18:31,429 --> 00:18:38,179
this star schema base dimension model

00:18:34,600 --> 00:18:40,970
even otherwise when we did an initial

00:18:38,179 --> 00:18:43,669
benchmarking for a certain way workloads

00:18:40,970 --> 00:18:46,790
having an election caching along with

00:18:43,669 --> 00:18:51,169
presto give a significant improvement up

00:18:46,790 --> 00:18:55,660
to 18 to 25 percent but one of one of

00:18:51,169 --> 00:18:58,490
the things which we saw that especially

00:18:55,660 --> 00:19:00,590
not the larger OLAP queries there are

00:18:58,490 --> 00:19:03,200
smaller queries to a moderate queries

00:19:00,590 --> 00:19:06,380
which there are large number of queries

00:19:03,200 --> 00:19:09,710
running concurrently Alex OH having

00:19:06,380 --> 00:19:12,230
Alexio cache enabled gives us a much

00:19:09,710 --> 00:19:14,179
higher concurrency because the it

00:19:12,230 --> 00:19:16,370
elevates the fact that all different

00:19:14,179 --> 00:19:19,280
workers again has to go and pull the

00:19:16,370 --> 00:19:21,890
same data back from the network so those

00:19:19,280 --> 00:19:24,110
are initial findings

00:19:21,890 --> 00:19:26,720
you know eluded that we incorporated

00:19:24,110 --> 00:19:29,090
Alex saw in our stack even in our

00:19:26,720 --> 00:19:31,160
managed very offering though we did not

00:19:29,090 --> 00:19:33,380
auto scale Alexio clustered but we

00:19:31,160 --> 00:19:36,470
ensure that a finite memory pool is

00:19:33,380 --> 00:19:40,970
always present in the stack which can

00:19:36,470 --> 00:19:43,130
give us all of this benefit so with with

00:19:40,970 --> 00:19:43,940
that alterations or stacked looked like

00:19:43,130 --> 00:19:48,580
this

00:19:43,940 --> 00:19:51,680
we had presto cluster as before we had

00:19:48,580 --> 00:19:56,000
co-located Alexio clusters which went

00:19:51,680 --> 00:19:58,190
Lester and Alexi over presto instead of

00:19:56,000 --> 00:20:01,580
talking to and underlined GCS directly

00:19:58,190 --> 00:20:03,560
what talking with Alexia file system so

00:20:01,580 --> 00:20:05,450
whenever it could find data in the cache

00:20:03,560 --> 00:20:08,870
it was loading from the cache whenever

00:20:05,450 --> 00:20:10,400
it not it was Alexia was taking and

00:20:08,870 --> 00:20:14,780
filling at the cache for the subsequent

00:20:10,400 --> 00:20:17,690
queries to improve the performance rest

00:20:14,780 --> 00:20:20,180
all things remain same as for the

00:20:17,690 --> 00:20:22,220
architecture component same

00:20:20,180 --> 00:20:28,400
authentication authorization plugins

00:20:22,220 --> 00:20:31,790
same there is one thing which as if you

00:20:28,400 --> 00:20:34,370
if you have an table height able which

00:20:31,790 --> 00:20:36,920
is mapped to and GCS and if you want to

00:20:34,370 --> 00:20:38,990
enable Alexio cache there is some pre

00:20:36,920 --> 00:20:42,260
what you have to do you have to create

00:20:38,990 --> 00:20:44,720
that table first create and alexey of

00:20:42,260 --> 00:20:47,180
mount point of the where the data is

00:20:44,720 --> 00:20:49,190
located and then you have to switch that

00:20:47,180 --> 00:20:51,500
table to say instead of pointing to a

00:20:49,190 --> 00:20:54,170
GCS back table you have to mention that

00:20:51,500 --> 00:20:56,270
it is now it is pointing to Alex EFS now

00:20:54,170 --> 00:20:59,810
since we it's a management service

00:20:56,270 --> 00:21:02,570
offering it is giving an exposure to

00:20:59,810 --> 00:21:04,460
thousands of table we we didn't wanted

00:21:02,570 --> 00:21:07,610
our users to hate if you want to use

00:21:04,460 --> 00:21:10,520
Alexia go and create this two steps that

00:21:07,610 --> 00:21:12,830
that wasn't what I'm going to work so we

00:21:10,520 --> 00:21:16,270
had to implement certain auto sync

00:21:12,830 --> 00:21:19,280
mechanism that every tables which are

00:21:16,270 --> 00:21:22,610
listed in our how he met Oscar we

00:21:19,280 --> 00:21:24,590
automatically go and create Alexia

00:21:22,610 --> 00:21:28,640
mounting so this is where this component

00:21:24,590 --> 00:21:30,710
came into a picture so unfortunately I

00:21:28,640 --> 00:21:34,130
what I'm aware that there's no auto

00:21:30,710 --> 00:21:36,770
sinking of this this would be a nice to

00:21:34,130 --> 00:21:39,530
have feature in the lake Co stack as

00:21:36,770 --> 00:21:43,670
well but if it wasn't there we had to

00:21:39,530 --> 00:21:46,580
build that where it actually and call

00:21:43,670 --> 00:21:48,290
back hooks which keep looking at the new

00:21:46,580 --> 00:21:50,840
tables and the alteration in the

00:21:48,290 --> 00:21:52,850
existing hype tables whenever there are

00:21:50,840 --> 00:21:55,730
new tables this component will go and

00:21:52,850 --> 00:21:59,300
create a lecture mount points

00:21:55,730 --> 00:22:01,250
create a table swap that table from

00:21:59,300 --> 00:22:02,180
instead of being a GCSE back to an

00:22:01,250 --> 00:22:07,600
Alexia back

00:22:02,180 --> 00:22:10,430
so whenever fusers and teams query data

00:22:07,600 --> 00:22:14,150
data say it's from our stack they are

00:22:10,430 --> 00:22:17,420
actually they don't even realize that it

00:22:14,150 --> 00:22:22,390
is not a hive table backed by GCS it is

00:22:17,420 --> 00:22:22,390
her high table back by Alexio or layer

00:22:24,460 --> 00:22:31,820
after sometimes we when we have been

00:22:29,660 --> 00:22:35,300
running in public clouds for kites quite

00:22:31,820 --> 00:22:36,620
some time and we faced with some

00:22:35,300 --> 00:22:39,560
challenges there are there are always

00:22:36,620 --> 00:22:42,350
some teams which do to some compliance

00:22:39,560 --> 00:22:44,390
issues because of their data sets are

00:22:42,350 --> 00:22:46,460
highly sensitive they couldn't move

00:22:44,390 --> 00:22:48,760
their workload to the cloud and there

00:22:46,460 --> 00:22:52,940
was lot of requests that they wanted to

00:22:48,760 --> 00:22:56,720
have this acceleration layer powered

00:22:52,940 --> 00:23:02,290
their workload as well and they wanted

00:22:56,720 --> 00:23:05,720
to have use this query service and even

00:23:02,290 --> 00:23:08,030
for burst workloads to get access to

00:23:05,720 --> 00:23:09,590
this query layer in the cloud as well so

00:23:08,030 --> 00:23:12,830
now there was a time where we had to

00:23:09,590 --> 00:23:15,800
actually bridge that so when we have our

00:23:12,830 --> 00:23:19,670
offering which is Preston Alexio cluster

00:23:15,800 --> 00:23:23,060
which is running in cloud and how can it

00:23:19,670 --> 00:23:25,940
we'll go and fetch the data which is an

00:23:23,060 --> 00:23:28,670
on-prem so functionally we can do that

00:23:25,940 --> 00:23:31,100
there was even a good-enough pipe which

00:23:28,670 --> 00:23:33,880
is again not a associate pipe which is

00:23:31,100 --> 00:23:38,300
reused across different there are always

00:23:33,880 --> 00:23:40,760
ETL jobs which are choking most of the

00:23:38,300 --> 00:23:45,740
bandwidth which is transferring this

00:23:40,760 --> 00:23:47,660
data from on-prem to cloud so even

00:23:45,740 --> 00:23:51,500
though that pipe is there we wanted to

00:23:47,660 --> 00:23:53,780
ensure that hey how can we enable a

00:23:51,500 --> 00:23:55,670
functionality where this query layer can

00:23:53,780 --> 00:23:58,160
go query data sets which are actually

00:23:55,670 --> 00:24:01,430
residing on Prem cluster so when we did

00:23:58,160 --> 00:24:05,480
that again we saw that it is

00:24:01,430 --> 00:24:07,760
unpredictable Network IO is not giving a

00:24:05,480 --> 00:24:10,820
good response time when they try

00:24:07,760 --> 00:24:13,280
query the data in on-prem so that is

00:24:10,820 --> 00:24:16,310
where also when we have for those

00:24:13,280 --> 00:24:18,590
specific data set we actually pinned

00:24:16,310 --> 00:24:22,060
those data sets in the Alexio cache and

00:24:18,590 --> 00:24:24,820
which again greatly improved whenever

00:24:22,060 --> 00:24:28,640
those data sets to square it so now

00:24:24,820 --> 00:24:31,940
Alexio is acting as a data or history or

00:24:28,640 --> 00:24:33,590
a storage layer whenever that on-prem

00:24:31,940 --> 00:24:35,840
data sets are queried they are getting

00:24:33,590 --> 00:24:39,020
cached into an election layer and the

00:24:35,840 --> 00:24:40,610
public cloud so which will help and

00:24:39,020 --> 00:24:45,980
boost query performance for the

00:24:40,610 --> 00:24:48,950
subsequent queries and it was critical

00:24:45,980 --> 00:24:51,860
and important that there was a reason

00:24:48,950 --> 00:24:54,370
for the complexities and those teams

00:24:51,860 --> 00:25:01,190
were not supposed to move those datasets

00:24:54,370 --> 00:25:03,770
there so enabling this pipeline was easy

00:25:01,190 --> 00:25:05,810
but we had to ensure that we don't make

00:25:03,770 --> 00:25:08,300
that break that compliance reel and

00:25:05,810 --> 00:25:10,730
people shouldn't start you know

00:25:08,300 --> 00:25:13,640
exploiting this layer by transferring

00:25:10,730 --> 00:25:16,310
this data to public outs altogether so

00:25:13,640 --> 00:25:19,360
we had to implement lot many safeguards

00:25:16,310 --> 00:25:22,340
and apparently the way presto is

00:25:19,360 --> 00:25:23,840
extremely configurable and extensible to

00:25:22,340 --> 00:25:28,220
do that so

00:25:23,840 --> 00:25:31,010
so it has mechanism to implement lot

00:25:28,220 --> 00:25:34,100
many interceptor where you can go and

00:25:31,010 --> 00:25:36,560
tap that hey which particular datasets

00:25:34,100 --> 00:25:38,300
you want the query schemas and tables

00:25:36,560 --> 00:25:40,640
you want the queries to be allowed which

00:25:38,300 --> 00:25:44,020
particular dataset you didn't allow so

00:25:40,640 --> 00:25:47,990
we built a lot of those safeguards that

00:25:44,020 --> 00:25:52,310
highly sensitive datasets were even if

00:25:47,990 --> 00:25:54,410
been queried from cloud you we did not

00:25:52,310 --> 00:25:56,300
allow and right back functionality so

00:25:54,410 --> 00:25:58,580
they should be able to just query it so

00:25:56,300 --> 00:26:01,340
they'll not be able to write back or do

00:25:58,580 --> 00:26:03,980
a create table as or in certain to so

00:26:01,340 --> 00:26:06,080
such interceptors and plugin which we

00:26:03,980 --> 00:26:09,530
write and to Presto's to ensure that

00:26:06,080 --> 00:26:11,210
those layer does not get exploited to

00:26:09,530 --> 00:26:13,760
move this data set directly to the

00:26:11,210 --> 00:26:16,190
public cloud and even to ensure that

00:26:13,760 --> 00:26:18,320
people shouldn't use this query layer to

00:26:16,190 --> 00:26:22,159
ingest the data from on-prem to a query

00:26:18,320 --> 00:26:24,169
layer so this was one of our use case

00:26:22,159 --> 00:26:27,080
which highlighted when we were running

00:26:24,169 --> 00:26:30,350
in the hybrid cloud mechanism now the

00:26:27,080 --> 00:26:32,870
next week we are getting into place

00:26:30,350 --> 00:26:37,700
where we are landing into a multi cloud

00:26:32,870 --> 00:26:40,700
so where as of now we even today we have

00:26:37,700 --> 00:26:44,990
week of Walmart is extensively using two

00:26:40,700 --> 00:26:47,360
public clouds and we will be rolling out

00:26:44,990 --> 00:26:50,149
and in a situation that we have to

00:26:47,360 --> 00:26:53,320
provide this acceleration layer across

00:26:50,149 --> 00:26:55,820
this public routes and even in even

00:26:53,320 --> 00:26:57,769
provision and source the data set from

00:26:55,820 --> 00:27:00,049
on-premise well so that would be an

00:26:57,769 --> 00:27:02,059
instant interesting problem since even

00:27:00,049 --> 00:27:05,179
if it is three different environment

00:27:02,059 --> 00:27:08,570
across multi-cloud in fragments with

00:27:05,179 --> 00:27:12,710
on-prem I think it is super critical to

00:27:08,570 --> 00:27:17,299
have a caching layer which can emulate a

00:27:12,710 --> 00:27:19,730
data otherwise it will be for every

00:27:17,299 --> 00:27:21,769
single query going across to multiple

00:27:19,730 --> 00:27:24,590
clouds or to on-prem getting all of data

00:27:21,769 --> 00:27:27,769
it will be a huge utilization of the

00:27:24,590 --> 00:27:33,110
network bandwidth and even to some

00:27:27,769 --> 00:27:36,769
extent Alexeyev help greatly help is

00:27:33,110 --> 00:27:39,019
that you know there are some tables and

00:27:36,769 --> 00:27:42,070
data sets we do you wouldn't want to

00:27:39,019 --> 00:27:45,679
just create a copies in public cloud so

00:27:42,070 --> 00:27:48,350
so once you have those spend and when

00:27:45,679 --> 00:27:51,799
you have tables which are pointing to

00:27:48,350 --> 00:27:54,470
the similar data sets in on Prem Alex

00:27:51,799 --> 00:27:56,870
you takes care of syncing the data even

00:27:54,470 --> 00:27:59,750
if there is a data on metadata changes

00:27:56,870 --> 00:28:03,080
Alexia sings that so that will elevate

00:27:59,750 --> 00:28:09,830
you from KP making copies of those data

00:28:03,080 --> 00:28:14,120
set again back in the public cloud so

00:28:09,830 --> 00:28:16,100
overall this is how we tackle the

00:28:14,120 --> 00:28:19,429
acceleration query problems by using

00:28:16,100 --> 00:28:21,500
presto and Alexia efficiently so we

00:28:19,429 --> 00:28:22,190
ensure that all of the data is put to

00:28:21,500 --> 00:28:25,940
work

00:28:22,190 --> 00:28:27,929
we gave them an solution where they can

00:28:25,940 --> 00:28:31,440
do a federated query which is

00:28:27,929 --> 00:28:34,740
not a not only across data sources even

00:28:31,440 --> 00:28:37,740
the caching enable they are able to do

00:28:34,740 --> 00:28:39,690
it across environments as well on Prem

00:28:37,740 --> 00:28:42,330
and cloud as well

00:28:39,690 --> 00:28:44,879
people with teams were very happy even

00:28:42,330 --> 00:28:48,059
with the teams of data engineers that

00:28:44,879 --> 00:28:51,679
now the standard feature is they don't

00:28:48,059 --> 00:28:54,509
have to go and talk to every diff every

00:28:51,679 --> 00:28:57,090
source in their native language that was

00:28:54,509 --> 00:28:59,610
one sequel and single language they just

00:28:57,090 --> 00:29:03,059
have to learn once and then they can

00:28:59,610 --> 00:29:05,779
talk to all the different source system

00:29:03,059 --> 00:29:09,710
by leveraging that query language

00:29:05,779 --> 00:29:11,429
definitely they were I've seen a

00:29:09,710 --> 00:29:15,269
testimonials and there were a lot of

00:29:11,429 --> 00:29:19,559
teams reached out to us and say that

00:29:15,269 --> 00:29:21,809
they received huge performance gain in

00:29:19,559 --> 00:29:24,360
response time once they moved here there

00:29:21,809 --> 00:29:25,919
are teams were there where they have

00:29:24,360 --> 00:29:29,009
claimed that their response time is

00:29:25,919 --> 00:29:33,119
increased by 20x when they move to this

00:29:29,009 --> 00:29:36,240
acceleration layer so that both of this

00:29:33,119 --> 00:29:39,240
is possible because of presto as an

00:29:36,240 --> 00:29:44,700
amazing MPP and Jean and Alexia sabe

00:29:39,240 --> 00:29:47,850
a nice caching layer in between as as

00:29:44,700 --> 00:29:52,820
per the managed services this offering

00:29:47,850 --> 00:29:55,200
there are even it is hugely benefited to

00:29:52,820 --> 00:29:56,759
users and teams who just want to do a

00:29:55,200 --> 00:29:58,679
very canned reporting on an ad hoc

00:29:56,759 --> 00:30:01,110
analysis where they don't have to spin

00:29:58,679 --> 00:30:04,100
up large enough cluster we do have a

00:30:01,110 --> 00:30:06,929
mechanism where we it's not an infinite

00:30:04,100 --> 00:30:09,509
cluster resources so we we do have a

00:30:06,929 --> 00:30:12,090
mechanism where we tap into an audit

00:30:09,509 --> 00:30:14,909
every single query which is run by every

00:30:12,090 --> 00:30:18,240
user how much of data that query has

00:30:14,909 --> 00:30:20,610
crunch and we have an cost attribution

00:30:18,240 --> 00:30:23,070
of that since we we have a component

00:30:20,610 --> 00:30:25,889
which looks and tap into how much of

00:30:23,070 --> 00:30:28,409
resources were used by across

00:30:25,889 --> 00:30:31,230
cumulatively across users and then we

00:30:28,409 --> 00:30:32,909
have a cost of attribution mechanism

00:30:31,230 --> 00:30:35,369
which generates the chargeback report

00:30:32,909 --> 00:30:38,480
which we go and then build to the

00:30:35,369 --> 00:30:38,480
individual domain teams

00:30:39,789 --> 00:30:47,840
and yes so the operate ability has been

00:30:43,340 --> 00:30:50,570
increased and Presto's integration with

00:30:47,840 --> 00:30:54,440
most of the BI tools have made things

00:30:50,570 --> 00:30:57,049
super easy AB as far as the adoption I

00:30:54,440 --> 00:30:59,570
know that the heavy analyst users have

00:30:57,049 --> 00:31:01,280
been leveraging tableau and lucre then a

00:30:59,570 --> 00:31:03,740
lot of data scientists have been using

00:31:01,280 --> 00:31:07,010
this acceleration layer from their art

00:31:03,740 --> 00:31:10,820
studio lot of data engineers use we also

00:31:07,010 --> 00:31:13,490
give them a bi tool called superset and

00:31:10,820 --> 00:31:16,340
there are a lot of people use it through

00:31:13,490 --> 00:31:19,580
a notebooks like Jupiter and Zeppelin so

00:31:16,340 --> 00:31:21,679
in that ways it really greatly helped

00:31:19,580 --> 00:31:28,580
where presto has already pre-built

00:31:21,679 --> 00:31:31,450
configuration with those BI tools that's

00:31:28,580 --> 00:31:36,440
pretty much a treasure

00:31:31,450 --> 00:31:36,440

YouTube URL: https://www.youtube.com/watch?v=smsRPZ0tO30


