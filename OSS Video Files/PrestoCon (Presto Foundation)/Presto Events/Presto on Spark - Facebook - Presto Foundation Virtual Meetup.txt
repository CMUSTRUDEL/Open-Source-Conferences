Title: Presto on Spark - Facebook - Presto Foundation Virtual Meetup
Publication date: 2020-07-08
Playlist: Presto Events
Description: 
	Andrii and Wenlei, Presto Team at Facebook

At Facebook, we have spent the past several years in independently building and scaling both Presto and Spark to Facebook scale batch workloads. It is now increasingly evident that there is significant value in coupling Presto's state-of-art low-latency evaluation with Spark's robust and fault tolerant execution engine. In this talk, we'll take a deep dive in Presto and Spark architecture with a focus on key differentiators (e.g., disaggregated shuffle) that are required to further scale Presto
Captions: 
	00:00:00,000 --> 00:00:06,660
hello everybody let me introduce myself

00:00:02,370 --> 00:00:10,880
my name is Andrew Rosa I work in press

00:00:06,660 --> 00:00:16,460
the team and my main area of focus is

00:00:10,880 --> 00:00:16,460
skin pressed to support larger workloads

00:00:16,609 --> 00:00:26,670
and today with my colleague we are going

00:00:23,250 --> 00:00:27,359
to talk about running press neuron on

00:00:26,670 --> 00:00:43,050
spark

00:00:27,359 --> 00:00:45,629
I really have a question today we are

00:00:43,050 --> 00:00:48,719
going to discuss general motivation for

00:00:45,629 --> 00:00:53,910
this effort also we'll have a quick

00:00:48,719 --> 00:00:59,539
overview round prestonsburg design we

00:00:53,910 --> 00:01:02,430
also have got some early results then

00:00:59,539 --> 00:01:04,799
we'll have a quick update on the current

00:01:02,430 --> 00:01:08,100
development status of this project and

00:01:04,799 --> 00:01:16,530
the end this presentation we'll have a

00:01:08,100 --> 00:01:19,380
Q&A session so first I would like to

00:01:16,530 --> 00:01:22,590
start with a quick press to architecture

00:01:19,380 --> 00:01:26,159
overview the current pressed

00:01:22,590 --> 00:01:29,720
architecture so originally it was

00:01:26,159 --> 00:01:32,670
designed to serve interactive use cases

00:01:29,720 --> 00:01:38,659
it has many characteristics of a

00:01:32,670 --> 00:01:42,360
classical a classic MPP database such as

00:01:38,659 --> 00:01:44,610
in-memory streaming shuffle with

00:01:42,360 --> 00:01:47,189
in-memory streaming shuffle it is

00:01:44,610 --> 00:01:53,090
possible to run a more operations in

00:01:47,189 --> 00:01:56,100
parallel that results in lower latencies

00:01:53,090 --> 00:01:59,479
it is also presto is deployed as a

00:01:56,100 --> 00:02:03,270
standalone multi-tenant managed service

00:01:59,479 --> 00:02:10,090
that is always warm so there is no

00:02:03,270 --> 00:02:14,379
startup delay when you issue a query and

00:02:10,090 --> 00:02:18,459
and this model of deployment also allows

00:02:14,379 --> 00:02:21,510
us to better share the resources like

00:02:18,459 --> 00:02:25,810
for example if glasses idle we can

00:02:21,510 --> 00:02:28,900
increase parallelism for for a single or

00:02:25,810 --> 00:02:35,230
for the running queries it also allows

00:02:28,900 --> 00:02:42,060
more efficient memory sharing but of

00:02:35,230 --> 00:02:46,030
course it has some trade-offs so it has

00:02:42,060 --> 00:02:48,280
its own scalability limitations so in

00:02:46,030 --> 00:02:50,829
our current deployment at Facebook our

00:02:48,280 --> 00:02:54,760
distributed memory limit per query is

00:02:50,829 --> 00:02:57,790
set to 5 terabytes and because of the

00:02:54,760 --> 00:03:00,370
streaming nature of shuffle the

00:02:57,790 --> 00:03:03,900
aggregations and joints have to be

00:03:00,370 --> 00:03:06,340
processed all at once so it limits the

00:03:03,900 --> 00:03:14,400
maximum size of join and aggregation

00:03:06,340 --> 00:03:17,889
effectively to 5 terabytes the other

00:03:14,400 --> 00:03:22,049
limitation of this architecture is that

00:03:17,889 --> 00:03:27,880
the number of workers in cluster is

00:03:22,049 --> 00:03:29,950
limited to around 600 and because of the

00:03:27,880 --> 00:03:34,120
end-to-end shuffle the number of

00:03:29,950 --> 00:03:38,400
reducers is even lower currently in our

00:03:34,120 --> 00:03:38,400
deployments it is around 300 reducers

00:03:38,609 --> 00:03:44,709
the number of reducers is lower because

00:03:42,060 --> 00:03:46,989
we're streaming shuffle we need to do

00:03:44,709 --> 00:03:51,010
and do an exchange that doesn't really

00:03:46,989 --> 00:03:52,660
scale well and the number of workers is

00:03:51,010 --> 00:03:55,660
limited by the fact that we are using a

00:03:52,660 --> 00:03:56,980
single coordinator - that is shared by

00:03:55,660 --> 00:04:00,069
all the queries that are currently

00:03:56,980 --> 00:04:05,919
running in a cluster we're actually

00:04:00,069 --> 00:04:09,579
trying to to remove this limitation

00:04:05,919 --> 00:04:11,200
there is a project of trying to scale

00:04:09,579 --> 00:04:13,450
our coordinators to be able to run

00:04:11,200 --> 00:04:15,880
multiple coordinators per cluster but so

00:04:13,450 --> 00:04:20,799
far it is still a single coordinator per

00:04:15,880 --> 00:04:23,650
per cluster for our work clause we did

00:04:20,799 --> 00:04:25,210
some analysis and we found out that

00:04:23,650 --> 00:04:29,530
we're electrons for more than eight

00:04:25,210 --> 00:04:33,520
hours have has only a 50% chance of

00:04:29,530 --> 00:04:35,259
succeeding it is mainly because the fact

00:04:33,520 --> 00:04:39,039
that press that doesn't offer any

00:04:35,259 --> 00:04:40,810
failure recovery also all the credits

00:04:39,039 --> 00:04:42,940
are running multi-tenant environment so

00:04:40,810 --> 00:04:47,410
there's always a chance of like bad

00:04:42,940 --> 00:04:49,360
query kicking in and because of no

00:04:47,410 --> 00:04:51,850
resource isolation that bad query can

00:04:49,360 --> 00:04:54,370
also negatively impact all the other

00:04:51,850 --> 00:04:57,789
queries that are currently running on a

00:04:54,370 --> 00:05:00,930
cluster so as you may already know at

00:04:57,789 --> 00:05:03,220
Facebook we run press the foot badge as

00:05:00,930 --> 00:05:06,000
we not only use press the for

00:05:03,220 --> 00:05:10,300
interactive work laws but also for large

00:05:06,000 --> 00:05:12,880
ETL processing and that's why overcoming

00:05:10,300 --> 00:05:15,490
all these limitations is so important

00:05:12,880 --> 00:05:17,710
because usually batch a large batch ETL

00:05:15,490 --> 00:05:19,930
queries require higher memory limits

00:05:17,710 --> 00:05:22,960
higher parallelism and higher

00:05:19,930 --> 00:05:29,229
reliability due to their long-running

00:05:22,960 --> 00:05:34,060
nature before Preston spark when Leigh

00:05:29,229 --> 00:05:37,150
and I were working on this project

00:05:34,060 --> 00:05:40,300
called Preston unlimited so the idea of

00:05:37,150 --> 00:05:44,400
Cresta unlimited was to bring my

00:05:40,300 --> 00:05:47,490
previous style execution to MPP a

00:05:44,400 --> 00:05:50,349
database architecture so the idea is

00:05:47,490 --> 00:05:53,139
instead of using m2 and screaming

00:05:50,349 --> 00:05:56,229
shuffle the store intermediate data on

00:05:53,139 --> 00:05:58,090
disk and since we store intermediate

00:05:56,229 --> 00:06:00,010
data on this it allows us to more

00:05:58,090 --> 00:06:02,830
granular a process joints and

00:06:00,010 --> 00:06:06,699
aggregations so instead of trying to

00:06:02,830 --> 00:06:09,310
load all data in memory we can load only

00:06:06,699 --> 00:06:14,080
some number of shards at a time and

00:06:09,310 --> 00:06:17,620
process them so that gives us ability to

00:06:14,080 --> 00:06:20,320
run large memory queries that will

00:06:17,620 --> 00:06:24,490
require more than five terabytes of

00:06:20,320 --> 00:06:27,669
memory it also it also increases

00:06:24,490 --> 00:06:29,590
reliability by allowing partial file

00:06:27,669 --> 00:06:31,659
recovery since the intermediate data is

00:06:29,590 --> 00:06:34,870
on disk we can't restart a single

00:06:31,659 --> 00:06:36,529
reducer or in some cases we will also

00:06:34,870 --> 00:06:40,519
need to restart we

00:06:36,529 --> 00:06:45,079
start the whole stage or as we call it

00:06:40,519 --> 00:06:46,849
section and what is really amazing about

00:06:45,079 --> 00:06:50,179
Preston limited that it doesn't require

00:06:46,849 --> 00:06:52,789
anything any additional deployments can

00:06:50,179 --> 00:06:56,959
be run on existing press the cluster

00:06:52,789 --> 00:06:58,219
along a clusters along with with all the

00:06:56,959 --> 00:07:00,669
other queries that are running in the

00:06:58,219 --> 00:07:04,339
streaming home

00:07:00,669 --> 00:07:06,379
but unfortunately pressed on limited has

00:07:04,339 --> 00:07:08,539
its own limitations

00:07:06,379 --> 00:07:11,059
so the first limitation that is very

00:07:08,539 --> 00:07:13,939
important for us is basically the number

00:07:11,059 --> 00:07:15,889
of workers still limited to the number

00:07:13,939 --> 00:07:19,219
of nodes in the cluster that is in our

00:07:15,889 --> 00:07:21,919
case around six hundred nodes there is

00:07:19,219 --> 00:07:25,099
still no resource isolations isolation

00:07:21,919 --> 00:07:30,699
there all the other queries there do

00:07:25,099 --> 00:07:33,829
share the same GBM same cluster so it is

00:07:30,699 --> 00:07:37,069
it is possible that one that query can

00:07:33,829 --> 00:07:38,569
still negatively impact reliability of

00:07:37,069 --> 00:07:43,879
all the other queries that are currently

00:07:38,569 --> 00:07:46,219
running in the cluster in some cases

00:07:43,879 --> 00:07:48,379
oppress them limited still lack it's

00:07:46,219 --> 00:07:50,929
still lacking of granular failure

00:07:48,379 --> 00:07:53,360
recovery as I already said in some cases

00:07:50,929 --> 00:07:55,459
we cannot restart a single member of our

00:07:53,360 --> 00:08:00,939
video server issues that we have to

00:07:55,459 --> 00:08:06,849
still restart the entire stage and that

00:08:00,939 --> 00:08:09,139
it also can be improved and at the last

00:08:06,849 --> 00:08:10,729
most important invitation that we are

00:08:09,139 --> 00:08:12,829
lacking Preston limited is basically

00:08:10,729 --> 00:08:15,799
with current press the deployment model

00:08:12,829 --> 00:08:21,769
it is very hard to be flexible at

00:08:15,799 --> 00:08:27,949
utilizing of big capacities in order in

00:08:21,769 --> 00:08:31,129
other word in other words it's not it is

00:08:27,949 --> 00:08:36,079
hard to easy to add and remove workers

00:08:31,129 --> 00:08:39,019
from an existing presto cluster so with

00:08:36,079 --> 00:08:45,309
all these limitations started to look at

00:08:39,019 --> 00:08:45,309
how SPARC is trying to solve this and

00:08:47,630 --> 00:08:55,410
with spark spark was originally designed

00:08:51,660 --> 00:08:58,560
for this large-scale reliable EDL

00:08:55,410 --> 00:09:00,930
processing so it was built from ground

00:08:58,560 --> 00:09:05,040
up to support with a granular failure

00:09:00,930 --> 00:09:07,800
recovery it also runs queries in

00:09:05,040 --> 00:09:12,290
isolated containers for each query so

00:09:07,800 --> 00:09:15,290
each query has its own runs on its own

00:09:12,290 --> 00:09:15,290
gbm's

00:09:17,060 --> 00:09:23,370
also sparks model of executions of

00:09:20,750 --> 00:09:26,160
supports thousands of mappers and

00:09:23,370 --> 00:09:28,670
reducers that can be run in parallel it

00:09:26,160 --> 00:09:33,030
is due to the fact that a spark

00:09:28,670 --> 00:09:34,620
provisions is separate driver are kind

00:09:33,030 --> 00:09:39,030
of currently coordinator in terms of

00:09:34,620 --> 00:09:41,430
presto for each query also the shuffle

00:09:39,030 --> 00:09:43,320
algorithm in spark is more scalable and

00:09:41,430 --> 00:09:46,080
in addition to death sparked also

00:09:43,320 --> 00:09:49,040
supports plugging in a disaggregated

00:09:46,080 --> 00:09:51,990
shuffle series such as Apache Trail or

00:09:49,040 --> 00:09:56,550
at Facebook we have our own shuffle

00:09:51,990 --> 00:09:59,760
service that is called Koska spark is

00:09:56,550 --> 00:10:03,710
also much more flexible at resource

00:09:59,760 --> 00:10:06,960
allocation so since it was designed for

00:10:03,710 --> 00:10:09,480
when it's gone over failure recovery it

00:10:06,960 --> 00:10:12,570
is very easy to add remove workers by

00:10:09,480 --> 00:10:17,840
simply killing a worker and returning it

00:10:12,570 --> 00:10:24,570
back if it was requested by some other

00:10:17,840 --> 00:10:27,990
job so of course the question is why not

00:10:24,570 --> 00:10:31,200
to simply use spark so this is a very

00:10:27,990 --> 00:10:34,590
good question and answered there's

00:10:31,200 --> 00:10:36,330
actually a couple of reasons to why and

00:10:34,590 --> 00:10:38,600
we wanted to run press that evaluation

00:10:36,330 --> 00:10:43,020
engine in spark instead of like simply

00:10:38,600 --> 00:10:45,420
translating our queries to spark so

00:10:43,020 --> 00:10:47,930
first of all we want to reuse press to

00:10:45,420 --> 00:10:51,450
secure we don't want to translate as

00:10:47,930 --> 00:10:55,410
translation based solutions they usually

00:10:51,450 --> 00:10:58,569
don't scale as we are always hitting

00:10:55,410 --> 00:11:00,920
this long tail of not

00:10:58,569 --> 00:11:07,519
compatible queries or queries with

00:11:00,920 --> 00:11:09,379
slightly different semantics so the

00:11:07,519 --> 00:11:14,149
translation solutions

00:11:09,379 --> 00:11:17,629
unfortunately it's it's not scalable the

00:11:14,149 --> 00:11:21,110
other reason is are the UDF's Resta has

00:11:17,629 --> 00:11:23,300
a set of unique UDF's that would require

00:11:21,110 --> 00:11:26,660
to be translated to spark if we wanted

00:11:23,300 --> 00:11:30,199
to run and translate press the queries

00:11:26,660 --> 00:11:37,309
to spark additionally a press the CQ l

00:11:30,199 --> 00:11:39,350
has some features that spark still

00:11:37,309 --> 00:11:45,649
doesn't support like for example lambdas

00:11:39,350 --> 00:11:47,269
lambda functions are used to process to

00:11:45,649 --> 00:11:51,350
get to do some various operations on

00:11:47,269 --> 00:11:53,870
complex types also press the for example

00:11:51,350 --> 00:11:58,610
it has very powerful geospatial toolkit

00:11:53,870 --> 00:12:00,589
and in addition to that Presta is a very

00:11:58,610 --> 00:12:02,899
very powerful engine for query

00:12:00,589 --> 00:12:08,350
federation so presto allows not only

00:12:02,899 --> 00:12:11,300
query HDFS or hive it allows to join

00:12:08,350 --> 00:12:15,769
different query sources like for example

00:12:11,300 --> 00:12:22,970
all money secured or Cassandra or many

00:12:15,769 --> 00:12:25,879
many other different sources and Presta

00:12:22,970 --> 00:12:27,829
also has a very efficient column or

00:12:25,879 --> 00:12:29,889
evaluation engine the pre simply become

00:12:27,829 --> 00:12:36,230
even more efficient with project area

00:12:29,889 --> 00:12:39,559
and column our evaluation is it can be

00:12:36,230 --> 00:12:42,220
very beneficial for many many different

00:12:39,559 --> 00:12:42,220
workloads

00:12:45,339 --> 00:12:55,040
ok so let's let's quickly discuss the

00:12:51,139 --> 00:12:58,189
Preston SPARC architecture so with

00:12:55,040 --> 00:13:00,199
Preston spark we are trying to run press

00:12:58,189 --> 00:13:06,980
the evaluation engine as a library

00:13:00,199 --> 00:13:09,920
within SPARC execution so it has no

00:13:06,980 --> 00:13:10,940
dependency on existing press the

00:13:09,920 --> 00:13:13,250
deployment model

00:13:10,940 --> 00:13:17,480
so to run customs part you don't

00:13:13,250 --> 00:13:20,150
actually need press the faster only what

00:13:17,480 --> 00:13:23,020
you need is the restaurants part

00:13:20,150 --> 00:13:27,920
packages an existing spark cluster and

00:13:23,020 --> 00:13:34,670
you can think of it as of custom custom

00:13:27,920 --> 00:13:37,970
batch job so what it does spark driver

00:13:34,670 --> 00:13:40,220
we run press the code to parse press the

00:13:37,970 --> 00:13:42,320
query to planet and create a press the

00:13:40,220 --> 00:13:46,900
distributor plan that then further

00:13:42,320 --> 00:13:51,950
getting translated into a spark everyday

00:13:46,900 --> 00:13:56,030
this is a data graph and in the spark so

00:13:51,950 --> 00:13:58,990
then this part is led is run spark

00:13:56,030 --> 00:14:04,010
cluster and press the Appalachian code

00:13:58,990 --> 00:14:10,070
is executed as some Capac mappers and

00:14:04,010 --> 00:14:13,160
reducers in inspired we have a detailed

00:14:10,070 --> 00:14:14,990
detailed design doc so we can if you're

00:14:13,160 --> 00:14:22,270
interested you can have a look and get

00:14:14,990 --> 00:14:25,640
more information so here's an example of

00:14:22,270 --> 00:14:27,620
spark LED generated by a press the query

00:14:25,640 --> 00:14:29,270
so as we can see there is this this is a

00:14:27,620 --> 00:14:33,110
simple query it has a join and

00:14:29,270 --> 00:14:36,380
aggregation on top of it and that's some

00:14:33,110 --> 00:14:39,920
order by at the end so as you can see

00:14:36,380 --> 00:14:43,970
there are two stages that the scanning

00:14:39,920 --> 00:14:47,390
stage 0 and stage 1 then staged to the

00:14:43,970 --> 00:14:50,360
partition by to shuffle data by the

00:14:47,390 --> 00:14:53,210
junkie and does the partition to perform

00:14:50,360 --> 00:14:57,410
join and then it does another level

00:14:53,210 --> 00:15:00,920
around of shuffle to perform aggregation

00:14:57,410 --> 00:15:04,640
on order date and on the end of the day

00:15:00,920 --> 00:15:09,800
does another shuffle to sort the output

00:15:04,640 --> 00:15:12,050
and and produce the results so as I said

00:15:09,800 --> 00:15:14,000
a to run Preston spark we need no

00:15:12,050 --> 00:15:16,490
existing spark cluster so everything can

00:15:14,000 --> 00:15:21,290
be run by simple spark submit command

00:15:16,490 --> 00:15:22,250
and it takes precedent spark packages as

00:15:21,290 --> 00:15:23,939
arguments

00:15:22,250 --> 00:15:27,299
so other arguments are

00:15:23,939 --> 00:15:33,509
the config catalogs configuration and of

00:15:27,299 --> 00:15:38,220
course the sequel query query file so

00:15:33,509 --> 00:15:41,459
we've run some tests on our in our

00:15:38,220 --> 00:15:45,269
internal spark so so far we've been able

00:15:41,459 --> 00:15:46,829
to run a query that would otherwise

00:15:45,269 --> 00:15:50,189
require more than 50 terabytes

00:15:46,829 --> 00:15:56,249
distributed memory in Preston so it was

00:15:50,189 --> 00:15:57,989
like a 10x by the we increase the

00:15:56,249 --> 00:16:01,349
distributed memory limit by a factor of

00:15:57,989 --> 00:16:06,779
10 X we also managed to reduce by the a

00:16:01,349 --> 00:16:11,009
factor of 3 X the execution time of some

00:16:06,779 --> 00:16:16,109
queries by allocating more resources to

00:16:11,009 --> 00:16:19,139
process a query so it was 4,000 workers

00:16:16,109 --> 00:16:21,629
in spark versus 600 workers in Preston

00:16:19,139 --> 00:16:27,149
but unfortunately we've seen some

00:16:21,629 --> 00:16:31,769
increase in CPU time when running presto

00:16:27,149 --> 00:16:36,659
on spark we haven't done detailed

00:16:31,769 --> 00:16:38,609
analysis yet but we suspect that it

00:16:36,659 --> 00:16:43,889
might be related to the fact that spark

00:16:38,609 --> 00:16:47,369
currently requires a spark currently

00:16:43,889 --> 00:16:48,779
does sorting of intermediate data that

00:16:47,369 --> 00:16:52,759
is actually not needed by Preston

00:16:48,779 --> 00:16:56,849
because Presta always does hash join and

00:16:52,759 --> 00:16:58,639
our format for intermediate shuffle is

00:16:56,849 --> 00:17:01,889
not very efficient we basically

00:16:58,639 --> 00:17:06,110
implemented some very basic prototype so

00:17:01,889 --> 00:17:06,110
there is still some room for improvement

00:17:07,039 --> 00:17:13,500
now it's very preliminary and it's also

00:17:09,929 --> 00:17:15,419
very like right like the workflow

00:17:13,500 --> 00:17:17,579
dependence so it's possible that we fix

00:17:15,419 --> 00:17:21,029
the worm config or when lion code it's

00:17:17,579 --> 00:17:23,189
so then it becomes like 20 or 30 percent

00:17:21,029 --> 00:17:26,970
better in terms of performance so please

00:17:23,189 --> 00:17:30,029
don't count on the data the the 35 to 50

00:17:26,970 --> 00:17:33,360
an increase in CPU time also actual

00:17:30,029 --> 00:17:34,600
mentoring is in general like trying to

00:17:33,360 --> 00:17:37,840
you

00:17:34,600 --> 00:17:42,340
so the Map Reduce architect kind of

00:17:37,840 --> 00:17:44,529
increase the CPU time but it also it

00:17:42,340 --> 00:17:46,840
opens it's kind of disaggregated the

00:17:44,529 --> 00:17:49,570
computation from the shuffle so you can

00:17:46,840 --> 00:17:52,330
provide this decoder serverless

00:17:49,570 --> 00:18:01,140
computation which means you can better

00:17:52,330 --> 00:18:03,370
use offline a sort of heap heap of peak

00:18:01,140 --> 00:18:05,110
computational resources so thinking

00:18:03,370 --> 00:18:08,590
about if you have some spare capacity

00:18:05,110 --> 00:18:11,020
you can get more mappers and they can be

00:18:08,590 --> 00:18:12,730
you know killed or post or they can be

00:18:11,020 --> 00:18:14,770
finished in a few minutes

00:18:12,730 --> 00:18:16,929
well it's impossible if the in-memory

00:18:14,770 --> 00:18:18,880
shuffle because the shuffle has to be

00:18:16,929 --> 00:18:20,980
done in all or nothing fashion

00:18:18,880 --> 00:18:23,890
basically you cannot post the shuffle in

00:18:20,980 --> 00:18:26,159
the traditional presto ntp word while in

00:18:23,890 --> 00:18:33,070
the MapReduce order like you have much

00:18:26,159 --> 00:18:34,360
fine granularity computation used and a

00:18:33,070 --> 00:18:36,159
couple of words of the current

00:18:34,360 --> 00:18:38,380
development status so the initial

00:18:36,159 --> 00:18:41,830
version is already merged to the open

00:18:38,380 --> 00:18:43,690
source it doesn't support all query

00:18:41,830 --> 00:18:46,510
shapes yet we're still working on

00:18:43,690 --> 00:18:49,450
supporting more so it doesn't support

00:18:46,510 --> 00:18:51,159
any way join yet it is only two-way

00:18:49,450 --> 00:18:54,159
joint for now it doesn't support

00:18:51,159 --> 00:18:59,350
broadcast joint so and broadcast join is

00:18:54,159 --> 00:19:01,630
often required by cross join and non

00:18:59,350 --> 00:19:04,149
equi-join so it's not supported yet

00:19:01,630 --> 00:19:07,710
we're working on adding support for that

00:19:04,149 --> 00:19:10,480
it also doesn't have an integration with

00:19:07,710 --> 00:19:13,080
marketed tables so it doesn't recognize

00:19:10,480 --> 00:19:15,250
pocketed tables and it would process

00:19:13,080 --> 00:19:16,870
pocketed tables as if they were not

00:19:15,250 --> 00:19:20,049
marketed so we're also working on

00:19:16,870 --> 00:19:24,720
removing this limitation of course we're

00:19:20,049 --> 00:19:27,130
working on improving CPU efficiency and

00:19:24,720 --> 00:19:30,130
we're also working on better integration

00:19:27,130 --> 00:19:32,980
with spark as presto and spark trading

00:19:30,130 --> 00:19:34,779
models are a little different so we're

00:19:32,980 --> 00:19:39,850
still trying to figure out what is going

00:19:34,779 --> 00:19:42,070
to be the best model of integration so

00:19:39,850 --> 00:19:44,450
those are the major working items that

00:19:42,070 --> 00:19:49,580
we are in the middle

00:19:44,450 --> 00:19:51,559
that's pretty much it from us and we can

00:19:49,580 --> 00:20:00,380
have a little Q&A session we still got

00:19:51,559 --> 00:20:02,770
some free time yeah awesome questions

00:20:00,380 --> 00:20:09,350
already

00:20:02,770 --> 00:20:11,210
so key choice question about when let's

00:20:09,350 --> 00:20:13,400
see what's the recommendation for using

00:20:11,210 --> 00:20:16,659
process ingestion eto for processing

00:20:13,400 --> 00:20:20,990
high volume of data processing and

00:20:16,659 --> 00:20:24,799
sparkle versus presto so I think you

00:20:20,990 --> 00:20:27,110
know experience press to press on press

00:20:24,799 --> 00:20:28,850
so let's call the press on presto and

00:20:27,110 --> 00:20:33,140
the press on spark in the spark on spark

00:20:28,850 --> 00:20:35,720
presto presto six walk well when you

00:20:33,140 --> 00:20:42,080
have like a few hundred workers and a

00:20:35,720 --> 00:20:45,110
few PP data then you can still work well

00:20:42,080 --> 00:20:47,600
you know if you try to increase one

00:20:45,110 --> 00:20:50,630
magnet you the large but also you you

00:20:47,600 --> 00:20:53,780
also kind of expect the operational

00:20:50,630 --> 00:20:57,980
overhead and the fine-tuning it's going

00:20:53,780 --> 00:20:59,270
to be much more larger so I think at the

00:20:57,980 --> 00:21:02,510
Facebook could we

00:20:59,270 --> 00:21:06,470
our presto batch handles that like the

00:21:02,510 --> 00:21:12,820
EP scale warehouse but of course the and

00:21:06,470 --> 00:21:15,590
the ways with yeah and I think her with

00:21:12,820 --> 00:21:20,120
but also the operational head is quite a

00:21:15,590 --> 00:21:24,320
lot yeah I think the I think I can say

00:21:20,120 --> 00:21:25,970
the number of pretzel workers is also at

00:21:24,320 --> 00:21:29,510
least one making you to the large then

00:21:25,970 --> 00:21:31,789
after you hungry the workers but but to

00:21:29,510 --> 00:21:39,950
help us this level we feel like we need

00:21:31,789 --> 00:21:42,500
a spark stale so each one has the

00:21:39,950 --> 00:21:44,659
question what do you mean using presto

00:21:42,500 --> 00:21:47,240
as a library on top of spark

00:21:44,659 --> 00:21:49,820
so the Prairie is generated by presto

00:21:47,240 --> 00:21:51,799
and the evaluation code is also

00:21:49,820 --> 00:21:53,570
generated by trust them so thinking

00:21:51,799 --> 00:21:54,530
about who you are launching a custom

00:21:53,570 --> 00:21:58,000
batch job

00:21:54,530 --> 00:22:00,919
I'm spark and that's constant batch job

00:21:58,000 --> 00:22:03,380
basically we are generating those oddity

00:22:00,919 --> 00:22:05,480
graphs but inside the oddity the actual

00:22:03,380 --> 00:22:11,630
evaluation logic is done by calling the

00:22:05,480 --> 00:22:14,870
presto eyebrow we have this question

00:22:11,630 --> 00:22:17,779
about what's a possession you break down

00:22:14,870 --> 00:22:26,980
of batch jobs running on presto vs.

00:22:17,779 --> 00:22:31,220
spark so no no do you want to answer

00:22:26,980 --> 00:22:45,230
this all I can't how much how how

00:22:31,220 --> 00:22:49,460
accurate the data I can share is here so

00:22:45,230 --> 00:22:51,620
it's a high-level kind of the most of

00:22:49,460 --> 00:22:54,140
them takes non travel portion let's say

00:22:51,620 --> 00:22:56,809
I mean I want to say Sparta takes the

00:22:54,140 --> 00:23:00,110
majority of the CPU but the pretzel also

00:22:56,809 --> 00:23:03,220
takes an unstable part of the CPU by

00:23:00,110 --> 00:23:09,830
knowledgeable I mean not to two digits

00:23:03,220 --> 00:23:12,770
and so speaking of the and the Presto's

00:23:09,830 --> 00:23:14,600
from the call side hood from the number

00:23:12,770 --> 00:23:16,730
of queries perspective or number have

00:23:14,600 --> 00:23:18,890
plans expecting actually presto has a

00:23:16,730 --> 00:23:21,649
much larger portion of the eto jobs

00:23:18,890 --> 00:23:23,770
because you may eat your water with our

00:23:21,649 --> 00:23:26,299
observation is like the majority of the

00:23:23,770 --> 00:23:31,120
pipeline's like finishing in one hour

00:23:26,299 --> 00:23:31,120
which is kind of a sweet spot for presto

00:23:32,830 --> 00:23:36,710
next question what has the main

00:23:35,210 --> 00:23:41,710
technique and now technical challenges

00:23:36,710 --> 00:23:41,710
with presence part of that to you expect

00:23:43,210 --> 00:23:52,240
so ok I will start with the

00:23:49,100 --> 00:23:55,909
non-technical session so it's actually

00:23:52,240 --> 00:23:57,980
it's so it's kind of a job that kind of

00:23:55,909 --> 00:24:00,110
needed so when user writes press on

00:23:57,980 --> 00:24:03,320
Spock jobs it's kind of new to be aware

00:24:00,110 --> 00:24:05,690
the PI post press-ons party because the

00:24:03,320 --> 00:24:08,029
real-time is a spark and the evaluation

00:24:05,690 --> 00:24:10,850
library is on presto so I would expect

00:24:08,029 --> 00:24:13,540
her from operation perspective or use of

00:24:10,850 --> 00:24:15,860
perspective how to rotate users problems

00:24:13,540 --> 00:24:17,960
accurately to Sparkle press o team

00:24:15,860 --> 00:24:19,970
instead of first he was asked for a so

00:24:17,960 --> 00:24:22,700
team and in the pathos team says no that

00:24:19,970 --> 00:24:29,300
today it's a problem for Sparkle return

00:24:22,700 --> 00:24:30,890
or like yeah a technical I think we're

00:24:29,300 --> 00:24:33,320
trying to just think Android kind of

00:24:30,890 --> 00:24:36,050
summarized a te he's the last two slides

00:24:33,320 --> 00:24:39,380
including you know what's the threading

00:24:36,050 --> 00:24:42,230
model like how do we the first podcast

00:24:39,380 --> 00:24:45,470
joy and essentially it's about how do we

00:24:42,230 --> 00:24:50,060
were the MTP style to have based on a

00:24:45,470 --> 00:24:56,030
more general dataflow execution energy

00:24:50,060 --> 00:24:57,800
and it's kind of a so that has this

00:24:56,030 --> 00:25:05,030
crash about the row I ran to the format

00:24:57,800 --> 00:25:06,710
in soccer so so in general presto when

00:25:05,030 --> 00:25:09,080
they when president the shuffle is a

00:25:06,710 --> 00:25:12,490
still doing column the shuffle so if you

00:25:09,080 --> 00:25:15,310
look at the shuffle coding presto if

00:25:12,490 --> 00:25:17,750
it's traveling the press of Haiti format

00:25:15,310 --> 00:25:20,060
so what we found is this doesn't have

00:25:17,750 --> 00:25:23,900
this skills well for hundreds of you

00:25:20,060 --> 00:25:26,150
know for hundreds of reducers or you

00:25:23,900 --> 00:25:29,750
know extremely stinks but when you have

00:25:26,150 --> 00:25:31,000
thousands or even tens of thousands so

00:25:29,750 --> 00:25:34,820
because the problem is you have to

00:25:31,000 --> 00:25:38,780
maintain a buffer for every destination

00:25:34,820 --> 00:25:42,290
right so when you have thousands or even

00:25:38,780 --> 00:25:48,350
tens of thousands reducers to keep the

00:25:42,290 --> 00:25:50,780
buffer size and not not to larger you

00:25:48,350 --> 00:25:53,720
you kind of say each destination has to

00:25:50,780 --> 00:25:55,700
be much has to be much smaller which you

00:25:53,720 --> 00:25:58,610
often end up with like you have a page

00:25:55,700 --> 00:26:01,910
that page you only have will have like a

00:25:58,610 --> 00:26:03,800
few rows or a few or a few positions or

00:26:01,910 --> 00:26:07,550
ten finishes then you have to flush it

00:26:03,800 --> 00:26:11,660
out and in presto the overhead for page

00:26:07,550 --> 00:26:14,620
is not triple so instead for instead of

00:26:11,660 --> 00:26:18,860
doing the traditional column-oriented

00:26:14,620 --> 00:26:21,710
shuffling we kind of say hey we every

00:26:18,860 --> 00:26:26,249
time we fresh one row at a time

00:26:21,710 --> 00:26:28,019
and I think now this is a myth here you

00:26:26,249 --> 00:26:30,360
know we want to basically you mind for

00:26:28,019 --> 00:26:32,369
other speakers time as well so maybe we

00:26:30,360 --> 00:26:34,590
can take these we'll have a copy of

00:26:32,369 --> 00:26:36,480
these questions available and I think we

00:26:34,590 --> 00:26:38,009
can just follow up with the attendance

00:26:36,480 --> 00:26:39,960
this was really a good discussion thank

00:26:38,009 --> 00:26:42,830
you all for ok you know having this

00:26:39,960 --> 00:26:42,830

YouTube URL: https://www.youtube.com/watch?v=hpNCwr2O0Mc


