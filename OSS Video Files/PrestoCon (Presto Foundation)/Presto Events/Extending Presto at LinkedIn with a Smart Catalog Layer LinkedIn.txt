Title: Extending Presto at LinkedIn with a Smart Catalog Layer LinkedIn
Publication date: 2020-07-08
Playlist: Presto Events
Description: 
	Walaa Eldin Moustafa, Staff Software Engineer at LinkedIn

In this talk, Walaa describes how LinkedIn extended its Presto Hive Catalog with a smart logical abstraction layer that is capable of reasoning about logical views with UDFs by using two core components, Coral and Transport UDFs. Coral is a view virtualization library, powered by Apache Calcite, that represents views using their logical query plans. Walaa shows how LinkedIn leverages Coral abstractions to decouple view expression language from the execution engine, and hence execute non-Presto-SQL views inside Presto, and achieve on-the-fly query rewrite for data governance and query optimization. Moreover, he describes Transport UDFs, a framework for defining user-defined functions once, and automatically translating them to native UDF versions of multiple engines such as Presto, Spark, Hive, or data formats such as Avro. Both Coral and Transport UDFs are open-source projects. Learn more about them at https://github.com/linkedin/coral and https://github.com/linkedin/transport.
Captions: 
	00:00:00,000 --> 00:00:05,220
thanks very for the introduction so

00:00:01,469 --> 00:00:06,960
today okay so I'm Molly Mustapha and

00:00:05,220 --> 00:00:09,300
today I'm going to talk about coral and

00:00:06,960 --> 00:00:12,599
transport UTS two main building blocks

00:00:09,300 --> 00:00:21,150
in our in our LinkedIn's data processing

00:00:12,599 --> 00:00:22,890
systems and compute engines if we look

00:00:21,150 --> 00:00:25,260
at the life cycle of data applications

00:00:22,890 --> 00:00:27,300
at Lincoln we find that link then runs

00:00:25,260 --> 00:00:29,519
on a suite of online offline and framing

00:00:27,300 --> 00:00:32,219
systems that work together to provide a

00:00:29,519 --> 00:00:34,320
LinkedIn user experience users interact

00:00:32,219 --> 00:00:36,899
with the website maybe to view top posts

00:00:34,320 --> 00:00:39,210
or engage with the news feed side facing

00:00:36,899 --> 00:00:41,460
services capture user actions and

00:00:39,210 --> 00:00:45,059
perform real-time compute persistence

00:00:41,460 --> 00:00:47,190
ordinary streaming events ETL processes

00:00:45,059 --> 00:00:49,440
continuously copy the real-time data to

00:00:47,190 --> 00:00:56,879
offline store it infrastructure but also

00:00:49,440 --> 00:00:58,620
registers the data as davi tables dali

00:00:56,879 --> 00:01:00,750
tables are a collection of files and

00:00:58,620 --> 00:01:03,449
catalan information with well-defined

00:01:00,750 --> 00:01:05,670
contracts contracts include well-defined

00:01:03,449 --> 00:01:07,920
type system conversions between storage

00:01:05,670 --> 00:01:10,830
format type systems and compute engine

00:01:07,920 --> 00:01:12,689
type systems contracts also include well

00:01:10,830 --> 00:01:14,909
we find scheme and evolution rules and

00:01:12,689 --> 00:01:17,340
other semantic information like data

00:01:14,909 --> 00:01:19,170
completeness watermarks and scheme is

00:01:17,340 --> 00:01:22,500
used for typesafe manipulation of the

00:01:19,170 --> 00:01:24,689
data users can also build virtual views

00:01:22,500 --> 00:01:27,060
on top of those tables which we refer to

00:01:24,689 --> 00:01:29,610
as Dali views to earth to further

00:01:27,060 --> 00:01:36,540
present another layer of abstraction on

00:01:29,610 --> 00:01:39,030
top of those tables compute engines such

00:01:36,540 --> 00:01:41,280
as spark and presto access data from

00:01:39,030 --> 00:01:43,530
Dali tables and views perform their

00:01:41,280 --> 00:01:46,350
compute and may store data back to Dali

00:01:43,530 --> 00:01:48,840
tables again Dali tables and views are

00:01:46,350 --> 00:01:50,729
powered by the high administer and table

00:01:48,840 --> 00:01:53,490
metadata is powered by the iceberg

00:01:50,729 --> 00:01:55,710
metadata format file storage layer is

00:01:53,490 --> 00:01:57,960
powered by the Hadoop HDFS file system

00:01:55,710 --> 00:02:02,070
and we are in the process of migrating

00:01:57,960 --> 00:02:04,590
this to either cloud storage going back

00:02:02,070 --> 00:02:07,170
to our data applications insights made

00:02:04,590 --> 00:02:09,929
in our backs compute clusters are pushed

00:02:07,170 --> 00:02:12,180
back to our online systems which again

00:02:09,929 --> 00:02:14,629
give back insights or recommendations to

00:02:12,180 --> 00:02:14,629
our users

00:02:15,590 --> 00:02:21,810
in this talk we will be focusing on

00:02:19,290 --> 00:02:23,550
views and to unlock their power we have

00:02:21,810 --> 00:02:26,010
built frameworks to virtualize their

00:02:23,550 --> 00:02:27,690
logic and capture their semantics using

00:02:26,010 --> 00:02:30,330
a unified representation of their

00:02:27,690 --> 00:02:32,820
logical plans such virtualization layer

00:02:30,330 --> 00:02:34,560
is one is the one-stop shop for our view

00:02:32,820 --> 00:02:40,680
manipulation and understanding and

00:02:34,560 --> 00:02:43,050
rewriting and it actually consists of

00:02:40,680 --> 00:02:45,450
two main components coral for relational

00:02:43,050 --> 00:02:49,850
algebra virtualization and transport you

00:02:45,450 --> 00:02:49,850
the s4 UDF and expression virtualization

00:02:51,620 --> 00:02:57,120
so to understand the view ecosystem at

00:02:54,750 --> 00:02:59,460
LinkedIn our view authoring process is

00:02:57,120 --> 00:03:02,310
follow the standard software development

00:02:59,460 --> 00:03:06,750
process so basically views are offered

00:03:02,310 --> 00:03:09,120
as software projects they are stored and

00:03:06,750 --> 00:03:11,580
manipulated in git repositories the

00:03:09,120 --> 00:03:13,950
contain files to the fund defined abuse

00:03:11,580 --> 00:03:16,350
equal and dependency files to declare

00:03:13,950 --> 00:03:19,170
UDF dependency that that you might want

00:03:16,350 --> 00:03:20,520
to depend on the bloopers develop their

00:03:19,170 --> 00:03:22,890
views through the standard software

00:03:20,520 --> 00:03:25,410
development process we go through code

00:03:22,890 --> 00:03:28,620
reviews testing publishing artifacts and

00:03:25,410 --> 00:03:30,840
deploying but deploying in this case

00:03:28,620 --> 00:03:33,420
refers to having the view live in the

00:03:30,840 --> 00:03:35,610
hive meta store in this case we keep

00:03:33,420 --> 00:03:38,400
track of the view DF by sitting table

00:03:35,610 --> 00:03:40,170
properties that track UDF names classes

00:03:38,400 --> 00:03:42,570
and dependencies that are required to

00:03:40,170 --> 00:03:44,190
load those UDS so on the left hand side

00:03:42,570 --> 00:03:46,590
of this slide you see the input of this

00:03:44,190 --> 00:03:48,690
process which is their software

00:03:46,590 --> 00:03:50,850
development repo that contains the view

00:03:48,690 --> 00:03:52,980
definition and the output is ultimately

00:03:50,850 --> 00:03:55,470
storing the view as a view definition in

00:03:52,980 --> 00:03:58,050
the major store along with a number of

00:03:55,470 --> 00:04:00,030
properties that define metadata that's

00:03:58,050 --> 00:04:03,480
required by the view and mainly those

00:04:00,030 --> 00:04:05,910
are table properties for keeping track

00:04:03,480 --> 00:04:12,450
of function dependences and function

00:04:05,910 --> 00:04:16,460
names and classes and so on so how do we

00:04:12,450 --> 00:04:16,460
layer tour coral on top of our views

00:04:18,350 --> 00:04:22,710
given a view definition coral first

00:04:20,850 --> 00:04:24,990
converts that we will definition to a

00:04:22,710 --> 00:04:26,340
relational algebra representation using

00:04:24,990 --> 00:04:28,169
Apache calcite

00:04:26,340 --> 00:04:33,419
to that intermediate representation as

00:04:28,169 --> 00:04:34,470
coral I are then from the standard

00:04:33,419 --> 00:04:37,050
intermediate relational algebra

00:04:34,470 --> 00:04:38,820
representation coral converts the view

00:04:37,050 --> 00:04:40,979
definition to a corresponding simple

00:04:38,820 --> 00:04:43,830
that can execute on a number of systems

00:04:40,979 --> 00:04:45,780
since currently our input Sippel is a

00:04:43,830 --> 00:04:48,360
dial is a dialect of isequal

00:04:45,780 --> 00:04:50,910
then we use can natively run on hind

00:04:48,360 --> 00:04:53,699
through our coral spark module we

00:04:50,910 --> 00:04:56,040
rewrite the view to execute one spark in

00:04:53,699 --> 00:04:58,110
the case of hi to yellow the query

00:04:56,040 --> 00:05:00,690
syntax is largely the same but the

00:04:58,110 --> 00:05:04,820
module also converts udx and exposes AP

00:05:00,690 --> 00:05:07,410
pies for EDF registration coral presto

00:05:04,820 --> 00:05:09,510
rewrites the relational algebra - presto

00:05:07,410 --> 00:05:10,919
sequel and similarly for coral paid

00:05:09,510 --> 00:05:14,220
coral rewrites in the relational algebra

00:05:10,919 --> 00:05:16,290
- Pig Latin also through other partner

00:05:14,220 --> 00:05:18,690
products and the company views are also

00:05:16,290 --> 00:05:21,460
executable on non-match back-end such as

00:05:18,690 --> 00:05:23,520
red lizard rest services

00:05:21,460 --> 00:05:23,520
you

00:05:24,840 --> 00:05:26,900
you

00:05:26,940 --> 00:05:31,050
as we have seen one side of the

00:05:28,830 --> 00:05:33,960
conversion is hide curve to l to quarrel

00:05:31,050 --> 00:05:35,910
on one side on the other hand choral

00:05:33,960 --> 00:05:39,090
presto converts the choral IR and

00:05:35,910 --> 00:05:41,400
produces an IR that expresses presto

00:05:39,090 --> 00:05:43,290
sequel using calcite drill to sequel

00:05:41,400 --> 00:05:45,390
conversion the corresponding presto

00:05:43,290 --> 00:05:48,180
sequel can be generated that priestess

00:05:45,390 --> 00:05:50,820
equal can include further expression

00:05:48,180 --> 00:05:53,490
that didn't originally exist in the hive

00:05:50,820 --> 00:05:56,220
QL definition that have been rewritten

00:05:53,490 --> 00:05:59,330
to match the same semantics but also

00:05:56,220 --> 00:05:59,330
executable impressed

00:06:00,690 --> 00:06:07,660
similarly for other modules these

00:06:04,360 --> 00:06:09,699
modules would also convert the choral IR

00:06:07,660 --> 00:06:12,130
into an intermediate representation that

00:06:09,699 --> 00:06:15,030
is suitable for some target execution

00:06:12,130 --> 00:06:18,970
engine maybe pig in this case and then

00:06:15,030 --> 00:06:21,370
produce a dialect that is executable

00:06:18,970 --> 00:06:24,060
natively on the other engine in this

00:06:21,370 --> 00:06:24,060
case it's pig latin

00:06:27,020 --> 00:06:35,339
so what kind of capability is such

00:06:29,550 --> 00:06:36,990
infrastructure enable logic

00:06:35,339 --> 00:06:39,419
virtualization with coral animals

00:06:36,990 --> 00:06:41,339
flexible language interfaces for example

00:06:39,419 --> 00:06:43,710
users can choose from a wide variety of

00:06:41,339 --> 00:06:46,169
languages to express the logic such as

00:06:43,710 --> 00:06:48,719
graph QL in Latin dataflow style api's

00:06:46,169 --> 00:06:50,879
or Datalog express their views in those

00:06:48,719 --> 00:06:54,830
languages and then still have them

00:06:50,879 --> 00:06:54,830
executable natively on this engine

00:06:56,219 --> 00:07:00,609
the second capability is workload

00:06:58,449 --> 00:07:02,679
analytics so for example given a cluster

00:07:00,609 --> 00:07:04,299
wide workload we can analyze the

00:07:02,679 --> 00:07:06,239
workload to obtain various insights

00:07:04,299 --> 00:07:08,769
about what most frequently used tables

00:07:06,239 --> 00:07:10,929
operations on those tables or types of

00:07:08,769 --> 00:07:13,289
operations applies to apply to various

00:07:10,929 --> 00:07:13,289
columns

00:07:13,660 --> 00:07:17,350
you

00:07:15,340 --> 00:07:19,810
and with that we can enable materialized

00:07:17,350 --> 00:07:21,550
view selection and optimization so some

00:07:19,810 --> 00:07:23,590
popular sub trees can be chosen to be

00:07:21,550 --> 00:07:26,500
materialized and Weir's can be rewritten

00:07:23,590 --> 00:07:29,350
to execute those sub trees whenever they

00:07:26,500 --> 00:07:31,479
match another like bigger query that can

00:07:29,350 --> 00:07:33,699
leverage the sub tree that has been pre

00:07:31,479 --> 00:07:39,910
computed using at my materialized view

00:07:33,699 --> 00:07:41,620
selection process also it enables data

00:07:39,910 --> 00:07:44,110
governance and honoring user privacy

00:07:41,620 --> 00:07:46,180
settings by joining these tables of

00:07:44,110 --> 00:07:48,130
views with user privacy settings and

00:07:46,180 --> 00:07:50,650
applying preferred user actions before

00:07:48,130 --> 00:07:52,300
propagating data to other operators we

00:07:50,650 --> 00:07:54,060
can transparently apply settings on

00:07:52,300 --> 00:07:56,770
behalf of the requesting application

00:07:54,060 --> 00:07:58,780
those user actions may include filtering

00:07:56,770 --> 00:08:01,080
out froze or obfuscating some column

00:07:58,780 --> 00:08:01,080
values

00:08:04,580 --> 00:08:08,940
mutualization and understanding also

00:08:06,870 --> 00:08:10,920
enables propagating metadata from lower

00:08:08,940 --> 00:08:13,920
base table layers to view level

00:08:10,920 --> 00:08:17,490
abstraction such militate such metadata

00:08:13,920 --> 00:08:24,830
includes personally identifying columns

00:08:17,490 --> 00:08:24,830
and also format specific schema metadata

00:08:27,800 --> 00:08:33,959
so now we move on to the other part of

00:08:31,289 --> 00:08:36,690
the view virtualization equal system

00:08:33,959 --> 00:08:39,250
which is transporting yes

00:08:36,690 --> 00:08:41,260
so as we have seen in the previous slide

00:08:39,250 --> 00:08:43,510
sequel has pretty well understood IR

00:08:41,260 --> 00:08:46,269
which is relational algebra where we can

00:08:43,510 --> 00:08:50,470
represent operators in standard ways

00:08:46,269 --> 00:08:52,060
using scans filters products joins and

00:08:50,470 --> 00:08:55,120
group by operators and so on

00:08:52,060 --> 00:08:57,850
however UDS don't really follow this

00:08:55,120 --> 00:08:59,950
pattern so they are opaque we use

00:08:57,850 --> 00:09:02,970
imperative language and they are not

00:08:59,950 --> 00:09:02,970
portable or translator

00:09:04,140 --> 00:09:11,700
this kind of inconsistency and knew the

00:09:07,620 --> 00:09:13,709
api's and difference between the api's

00:09:11,700 --> 00:09:15,690
that the engines provide to ribose UDF's

00:09:13,709 --> 00:09:18,959
result in a phenomena that we call UDF

00:09:15,690 --> 00:09:21,329
denormalization which basically is a

00:09:18,959 --> 00:09:23,519
manifestation of the duplication of the

00:09:21,329 --> 00:09:25,290
UDF due to having to express the same

00:09:23,519 --> 00:09:28,200
logic multiple times in multiple engines

00:09:25,290 --> 00:09:30,750
just because developers need to adhere

00:09:28,200 --> 00:09:33,380
to the api's of those engines which also

00:09:30,750 --> 00:09:36,060
leads to inconsistency where the

00:09:33,380 --> 00:09:38,820
duplicate UDF sometimes diverge from

00:09:36,060 --> 00:09:40,709
each others and it becomes hard to

00:09:38,820 --> 00:09:43,230
figure out which one is a source of

00:09:40,709 --> 00:09:45,480
truth or which one corresponds to the

00:09:43,230 --> 00:09:49,140
actual logic that is intended to be

00:09:45,480 --> 00:09:51,839
performed from that UDF with that cycle

00:09:49,140 --> 00:09:53,910
of writing multiple UDS for the same

00:09:51,839 --> 00:09:58,529
purpose it decreases that productivity

00:09:53,910 --> 00:10:00,089
of software developers and also gets

00:09:58,529 --> 00:10:02,730
them into this cycle of learning

00:10:00,089 --> 00:10:06,209
multiple UDF api is just to cope with

00:10:02,730 --> 00:10:08,040
new engines and it can also result in

00:10:06,209 --> 00:10:10,920
low performance because sometimes people

00:10:08,040 --> 00:10:14,579
tend to write adapters around those UDF

00:10:10,920 --> 00:10:16,709
so that we can be executed in an engine

00:10:14,579 --> 00:10:18,930
that we weren't originally written for

00:10:16,709 --> 00:10:20,880
so they would have to write adapter that

00:10:18,930 --> 00:10:23,399
would serialize and deserialize the data

00:10:20,880 --> 00:10:25,769
coming into and out of the idea so that

00:10:23,399 --> 00:10:29,690
it can be read by the engine that is

00:10:25,769 --> 00:10:29,690
trying to execute those ideas

00:10:32,180 --> 00:10:41,330
and at the same time the UDF landscape

00:10:37,820 --> 00:10:44,450
is characterized by two criteria which

00:10:41,330 --> 00:10:46,130
we can refer to by api complexity where

00:10:44,450 --> 00:10:47,899
like within each engine

00:10:46,130 --> 00:10:49,970
there could be complexities that users

00:10:47,899 --> 00:10:53,990
have to learn and low-level details such

00:10:49,970 --> 00:10:57,250
as data models or schemas or operations

00:10:53,990 --> 00:10:57,250
or api's that

00:10:57,430 --> 00:11:03,220
together form the identity of the UDF

00:11:00,550 --> 00:11:04,690
for that engine and also API disparity

00:11:03,220 --> 00:11:09,790
which is the difference between the

00:11:04,690 --> 00:11:12,910
api's across engines so for that we have

00:11:09,790 --> 00:11:17,230
been working and actually using in

00:11:12,910 --> 00:11:19,480
production our unified UDA api that is

00:11:17,230 --> 00:11:22,000
applicable and excusable in a number of

00:11:19,480 --> 00:11:22,420
engines which we refer to as transport

00:11:22,000 --> 00:11:25,960
udx

00:11:22,420 --> 00:11:28,870
so test for 2dx provide a high level API

00:11:25,960 --> 00:11:30,580
to users very simple where they can

00:11:28,870 --> 00:11:32,980
express their logic and focus on the

00:11:30,580 --> 00:11:37,000
logic use simple similar simple

00:11:32,980 --> 00:11:39,700
primitives to exam to write the UDF and

00:11:37,000 --> 00:11:41,470
at the same time get native executions

00:11:39,700 --> 00:11:43,660
out of you those UDS that are suitable

00:11:41,470 --> 00:11:47,950
to multiple engines so if we take a look

00:11:43,660 --> 00:11:51,339
at an example UDF that is written in the

00:11:47,950 --> 00:11:53,700
transport api we see that it is written

00:11:51,339 --> 00:11:58,150
or like it consists of four components

00:11:53,700 --> 00:12:00,760
first users extend like the standard UDF

00:11:58,150 --> 00:12:03,850
api and that API is parameterize by the

00:12:00,760 --> 00:12:05,440
number of inputs to the UDF and the

00:12:03,850 --> 00:12:07,870
output as well in this case the api

00:12:05,440 --> 00:12:10,029
would expect or the UDF would expect two

00:12:07,870 --> 00:12:12,010
arrays and then it would output a map

00:12:10,029 --> 00:12:14,790
for those two areas the logic in this

00:12:12,010 --> 00:12:18,550
UDF is essentially taking two arrays

00:12:14,790 --> 00:12:20,500
using one as the source for the keys in

00:12:18,550 --> 00:12:22,990
a map and other as the source for the

00:12:20,500 --> 00:12:26,980
values and merge them together into one

00:12:22,990 --> 00:12:30,130
map and return to the use so the other

00:12:26,980 --> 00:12:32,470
part that user needs to define as part

00:12:30,130 --> 00:12:35,320
of this API is basically related to the

00:12:32,470 --> 00:12:38,320
type validation and inference which is

00:12:35,320 --> 00:12:41,680
the second and third boxes in this

00:12:38,320 --> 00:12:45,310
example where users in this case express

00:12:41,680 --> 00:12:47,140
that the input is of type array of K in

00:12:45,310 --> 00:12:48,850
this case we're expecting an array the

00:12:47,140 --> 00:12:51,370
array can be of any kind that can be

00:12:48,850 --> 00:12:53,470
primitive or nested further that's that

00:12:51,370 --> 00:12:55,990
doesn't matter at this point and the

00:12:53,470 --> 00:12:59,140
second input is basically an array of V

00:12:55,990 --> 00:13:01,600
which is another type than K and then

00:12:59,140 --> 00:13:04,510
the output is simply map from k to V

00:13:01,600 --> 00:13:07,390
which tells the engine or the transport

00:13:04,510 --> 00:13:10,690
UDF framework how to derive the output

00:13:07,390 --> 00:13:14,260
time from the input types that being

00:13:10,690 --> 00:13:16,360
said the last part is basically defining

00:13:14,260 --> 00:13:19,780
how to process the data on a federal

00:13:16,360 --> 00:13:22,690
basis so given two inputs from a row

00:13:19,780 --> 00:13:25,420
during query execution which in this

00:13:22,690 --> 00:13:27,250
case are two arrays a1 and a2 what do we

00:13:25,420 --> 00:13:30,880
do from with them so that we can output

00:13:27,250 --> 00:13:32,560
the corresponding map so basically as

00:13:30,880 --> 00:13:34,660
you can see logic again is pretty simple

00:13:32,560 --> 00:13:36,550
where you add rate on the two arrays get

00:13:34,660 --> 00:13:37,930
one element from the first array another

00:13:36,550 --> 00:13:39,520
element from the second array and then

00:13:37,930 --> 00:13:44,260
put them into the Mac and then return

00:13:39,520 --> 00:13:46,750
the map so this is what users need to

00:13:44,260 --> 00:13:49,210
write in order to define a transport UDF

00:13:46,750 --> 00:13:51,460
but then how do you get how do we how do

00:13:49,210 --> 00:13:54,520
we how do they get to execute that you

00:13:51,460 --> 00:14:00,760
near on another engine or a specific

00:13:54,520 --> 00:14:03,490
engine is very simple what they need to

00:14:00,760 --> 00:14:06,460
do is actually to build a product and in

00:14:03,490 --> 00:14:08,980
this case instead of getting one

00:14:06,460 --> 00:14:11,020
artifact out of their product we get a

00:14:08,980 --> 00:14:13,570
number of artifacts one artifact that

00:14:11,020 --> 00:14:15,880
would produce a presto friendly version

00:14:13,570 --> 00:14:17,590
of the UDF another artifact that would

00:14:15,880 --> 00:14:19,570
produce a hi friendly version of the

00:14:17,590 --> 00:14:22,150
idea and then a third artifact that

00:14:19,570 --> 00:14:24,370
would produce a spark version Olivia we

00:14:22,150 --> 00:14:27,250
have also implemented a group specific

00:14:24,370 --> 00:14:30,040
variant of UDS but since at link them

00:14:27,250 --> 00:14:33,040
there are no engines yet that use arrow

00:14:30,040 --> 00:14:37,050
as their data model that kind of UDF's

00:14:33,040 --> 00:14:37,050
are not widely adopted or used

00:14:39,720 --> 00:14:45,300
but then how does that work basically

00:14:41,540 --> 00:14:46,950
what we have seen as an example of the

00:14:45,300 --> 00:14:50,070
transport UDF that we have seen two

00:14:46,950 --> 00:14:52,650
slides ago represents the middle layer

00:14:50,070 --> 00:14:57,030
of this line which is the user-defined

00:14:52,650 --> 00:14:59,760
transport UDF above this layer we have a

00:14:57,030 --> 00:15:02,640
wrapper that would make the engine that

00:14:59,760 --> 00:15:04,740
UDF look like iu d'f that and a specific

00:15:02,640 --> 00:15:07,680
engine would expect so for example in

00:15:04,740 --> 00:15:11,520
the hive case i've UDF's extend generic

00:15:07,680 --> 00:15:14,370
UTS or a generic UDF class so that way

00:15:11,520 --> 00:15:17,160
what I would see and within that class

00:15:14,370 --> 00:15:19,890
we would call into the transport UDF

00:15:17,160 --> 00:15:23,850
that they would have had written but at

00:15:19,890 --> 00:15:27,120
the same time hi doesn't understand the

00:15:23,850 --> 00:15:30,140
kind of the type of data model that the

00:15:27,120 --> 00:15:34,200
transport UDF is written in and for that

00:15:30,140 --> 00:15:36,960
we basically implement those api's that

00:15:34,200 --> 00:15:39,270
have been exposed to the user as part of

00:15:36,960 --> 00:15:42,240
that transport UDF or in layer two in

00:15:39,270 --> 00:15:46,020
this diagram using hive specific data

00:15:42,240 --> 00:15:50,310
and data types so basically whatever we

00:15:46,020 --> 00:15:52,290
have seen as arrays or Maps data types

00:15:50,310 --> 00:15:54,090
that the user interact with those are

00:15:52,290 --> 00:15:56,910
just interfaces the actual

00:15:54,090 --> 00:15:59,550
implementation is implemented based on

00:15:56,910 --> 00:16:02,610
what engine we would be targeting so in

00:15:59,550 --> 00:16:04,620
the case of hive of presto this would be

00:16:02,610 --> 00:16:08,060
manipulating native data types that

00:16:04,620 --> 00:16:12,480
fresco or hive or spark would understand

00:16:08,060 --> 00:16:14,580
so to the user the UDF would look like a

00:16:12,480 --> 00:16:16,830
platform and dependent function but to

00:16:14,580 --> 00:16:20,480
the engine it would like look like a

00:16:16,830 --> 00:16:20,480
native UDF to that end

00:16:20,540 --> 00:16:28,200
so both coral and transport UDF's are

00:16:24,170 --> 00:16:30,750
open source projects where they are both

00:16:28,200 --> 00:16:32,339
on the under active development there

00:16:30,750 --> 00:16:34,350
are many features we're looking forward

00:16:32,339 --> 00:16:36,270
to add in both specifically for

00:16:34,350 --> 00:16:39,620
transport we're looking forward to

00:16:36,270 --> 00:16:42,930
adding like new UDS that can be user

00:16:39,620 --> 00:16:46,050
suitable to like a large user base of

00:16:42,930 --> 00:16:48,390
machine learning scientists or data

00:16:46,050 --> 00:16:49,470
engineers such as machine learning UDS

00:16:48,390 --> 00:16:51,120
or spatial UDS

00:16:49,470 --> 00:16:53,250
we're looking forward to add new media

00:16:51,120 --> 00:16:57,389
types such as aggregate you

00:16:53,250 --> 00:17:02,459
or table functions and your support is

00:16:57,389 --> 00:17:05,220
always welcome so I think we can pull up

00:17:02,459 --> 00:17:07,289
after the talk if you have use cases or

00:17:05,220 --> 00:17:12,380
interested in learning more about this

00:17:07,289 --> 00:17:15,510
framework so although the words are mine

00:17:12,380 --> 00:17:18,299
that work has been done by a great set

00:17:15,510 --> 00:17:20,850
of colleagues I put their pictures here

00:17:18,299 --> 00:17:26,429
and I guess we're ready to take

00:17:20,850 --> 00:17:29,669
questions thank you so much thanks a lot

00:17:26,429 --> 00:17:33,150
voila so just a quick reminder again

00:17:29,669 --> 00:17:35,159
there is a Q and a button at the bottom

00:17:33,150 --> 00:17:37,559
of you zoom so if you have any questions

00:17:35,159 --> 00:17:39,450
please type your question in there and

00:17:37,559 --> 00:17:43,070
we'll read them out and answer them

00:17:39,450 --> 00:17:48,659
right so we have one question from Davis

00:17:43,070 --> 00:17:52,409
from data brakes his question is is the

00:17:48,659 --> 00:17:54,960
view materializing exact or fuzzy for

00:17:52,409 --> 00:17:57,419
example say I have a join with slightly

00:17:54,960 --> 00:18:00,150
different filters would you treat that

00:17:57,419 --> 00:18:00,720
as same when extracting the materialized

00:18:00,150 --> 00:18:03,210
view

00:18:00,720 --> 00:18:04,559
Thanks so I think quick nod of

00:18:03,210 --> 00:18:07,260
clarification I think is talking about

00:18:04,559 --> 00:18:08,669
things like date constants and so forth

00:18:07,260 --> 00:18:13,799
right exactly that's a real good

00:18:08,669 --> 00:18:15,929
question so just to clarify the blue

00:18:13,799 --> 00:18:19,919
materialisation slide is basically an

00:18:15,929 --> 00:18:23,940
infra that can be enabled by by this

00:18:19,919 --> 00:18:25,500
kind of framework we don't have it in

00:18:23,940 --> 00:18:28,470
production yet we've been talking about

00:18:25,500 --> 00:18:29,940
such feature but we have discussed this

00:18:28,470 --> 00:18:34,740
exact question which is pretty

00:18:29,940 --> 00:18:39,299
interesting how to match or abstract

00:18:34,740 --> 00:18:41,760
away from things or predicates that have

00:18:39,299 --> 00:18:45,539
been instantiated in different views but

00:18:41,760 --> 00:18:48,059
can also be abstracted so there are two

00:18:45,539 --> 00:18:50,730
ways of doing this again all these are

00:18:48,059 --> 00:18:53,400
just like design discussions at this

00:18:50,730 --> 00:18:56,039
point basically remove those predicates

00:18:53,400 --> 00:18:59,909
so that you don't get in your way of

00:18:56,039 --> 00:19:03,179
learning the new or finding patterns in

00:18:59,909 --> 00:19:05,940
the view or also you can learn you could

00:19:03,179 --> 00:19:07,080
learn them somehow by especially in the

00:19:05,940 --> 00:19:09,809
date case

00:19:07,080 --> 00:19:12,750
relating them by the actual time that

00:19:09,809 --> 00:19:16,470
you had rent so for example in many of

00:19:12,750 --> 00:19:21,179
the cases people would maybe run a view

00:19:16,470 --> 00:19:23,610
evaluation on the last two days

00:19:21,179 --> 00:19:24,600
partition or last week partition last

00:19:23,610 --> 00:19:28,620
month and so on

00:19:24,600 --> 00:19:30,510
if you look at when the U has ran and if

00:19:28,620 --> 00:19:33,149
you look also at the time stamp that

00:19:30,510 --> 00:19:37,110
showed up in those views you can

00:19:33,149 --> 00:19:38,880
basically learn a collation between the

00:19:37,110 --> 00:19:41,399
time stamps that appear in the view and

00:19:38,880 --> 00:19:43,830
the actual time and learn that okay so

00:19:41,399 --> 00:19:47,429
maybe the intention of this view was

00:19:43,830 --> 00:19:50,820
calling the now function for the ND and

00:19:47,429 --> 00:19:54,870
then maybe for the started even case if

00:19:50,820 --> 00:19:58,789
it was for the last week then start date

00:19:54,870 --> 00:20:01,769
would be now - 7 let's say right so

00:19:58,789 --> 00:20:03,659
again like I think the answer to this

00:20:01,769 --> 00:20:08,010
question is either like completely

00:20:03,659 --> 00:20:15,779
remove that dates from your view

00:20:08,010 --> 00:20:18,360
predicates or try to learn them yeah so

00:20:15,779 --> 00:20:18,720
we have another question from Steven

00:20:18,360 --> 00:20:21,299
Moya

00:20:18,720 --> 00:20:23,429
so a quick request to all the

00:20:21,299 --> 00:20:26,760
questioners so if you can also add your

00:20:23,429 --> 00:20:28,350
company or your organization that would

00:20:26,760 --> 00:20:31,019
help us as well so everyone can know

00:20:28,350 --> 00:20:33,269
what your interest is as well so

00:20:31,019 --> 00:20:35,850
questions I'm Steven Moore how does

00:20:33,269 --> 00:20:37,679
coral handle the binding operation so

00:20:35,850 --> 00:20:42,990
resolving a symbol to a particular table

00:20:37,679 --> 00:20:45,269
since many engine specific details has

00:20:42,990 --> 00:20:48,450
many session-based key massage paths

00:20:45,269 --> 00:20:51,899
thus the coral IR handle the binding as

00:20:48,450 --> 00:20:56,130
well yeah I actually had a longer

00:20:51,899 --> 00:20:58,110
version of this presentation which might

00:20:56,130 --> 00:20:59,730
answer this question let me bring it up

00:20:58,110 --> 00:21:03,419
from my other presentation it has a

00:20:59,730 --> 00:21:05,220
slide that might explain this and so I

00:21:03,419 --> 00:21:11,880
think this might be the answer to your

00:21:05,220 --> 00:21:14,639
question so basically like our tables we

00:21:11,880 --> 00:21:18,000
saw reside in the I'm a store and then

00:21:14,639 --> 00:21:19,289
to make quorum aware of those tables we

00:21:18,000 --> 00:21:20,429
need to connect with a high ohmmeter

00:21:19,289 --> 00:21:23,610
store and

00:21:20,429 --> 00:21:27,690
buying the tables beer and basically

00:21:23,610 --> 00:21:32,669
that happens do some calcite ad is that

00:21:27,690 --> 00:21:38,700
we implement to make like our LinkedIn

00:21:32,669 --> 00:21:41,249
or dolly specific used understandable by

00:21:38,700 --> 00:21:45,450
calcite and at the same time allow

00:21:41,249 --> 00:21:47,820
calcite to connect to the mega store to

00:21:45,450 --> 00:21:53,190
extract the metadata of those cables and

00:21:47,820 --> 00:21:54,799
schemas and main calcite also understand

00:21:53,190 --> 00:21:59,009
the hive type system so basically

00:21:54,799 --> 00:22:02,220
calcite exposes a number of API which is

00:21:59,009 --> 00:22:04,590
calcite schema or cat's-eye cable which

00:22:02,220 --> 00:22:08,820
can be implemented in a way that allows

00:22:04,590 --> 00:22:10,860
you to supply a meter store client or

00:22:08,820 --> 00:22:12,629
using a minister client that you get

00:22:10,860 --> 00:22:15,179
from the engine in this case it could be

00:22:12,629 --> 00:22:18,929
presto or spark where there is a

00:22:15,179 --> 00:22:20,759
Minister client already in hand and then

00:22:18,929 --> 00:22:23,749
using that Minister client we implement

00:22:20,759 --> 00:22:28,590
the calcite scheme and calcite cable

00:22:23,749 --> 00:22:30,749
using like methods to connect to the

00:22:28,590 --> 00:22:34,830
minister and learn about the tables

00:22:30,749 --> 00:22:37,110
their schemas and then help calcite do

00:22:34,830 --> 00:22:40,559
its semantic analysis on top of them and

00:22:37,110 --> 00:22:43,139
at the same time we have another layer

00:22:40,559 --> 00:22:46,470
that finds our functions that are part

00:22:43,139 --> 00:22:48,929
of the view definition into the calcite

00:22:46,470 --> 00:22:50,129
sequel operator cable through the only

00:22:48,929 --> 00:22:53,249
operator table which is another

00:22:50,129 --> 00:22:54,869
implementation that also makes calcite

00:22:53,249 --> 00:22:59,220
aware of UDF's

00:22:54,869 --> 00:23:01,379
and Iulia inference and validation that

00:22:59,220 --> 00:23:04,249
is required again for the view analysis

00:23:01,379 --> 00:23:04,249

YouTube URL: https://www.youtube.com/watch?v=hbUMX_Qo1no


