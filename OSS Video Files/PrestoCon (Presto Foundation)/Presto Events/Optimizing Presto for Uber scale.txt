Title: Optimizing Presto for Uber scale
Publication date: 2020-07-08
Playlist: Presto Events
Description: 
	Vivek Bharathan, Sr. Software Engineer at Uber

In this talk, we present some of the work streams we have underway at Uber to optimize Presto performance. In particular, we will cover enabling aggregation pushdown in queries in order to use statistics in the file headers/footers, our investigations into and attempts to efficiently executing approximate queries, and our experience with humongous object allocation in Presto. With the broader Presto community's support and pending their interest, we look forward to giving back some of these new features.
Captions: 
	00:00:00,030 --> 00:00:07,020
okay excellent so good morning folks or

00:00:03,799 --> 00:00:11,190
evening or afternoon as the case may be

00:00:07,020 --> 00:00:13,769
this is Vivek from uber and I'm gonna be

00:00:11,190 --> 00:00:15,690
talking to you and giving you a glimpse

00:00:13,769 --> 00:00:18,720
into some of the work streams that we

00:00:15,690 --> 00:00:22,470
have going at over to support operations

00:00:18,720 --> 00:00:24,000
at our scale before we get into that let

00:00:22,470 --> 00:00:26,750
me set up a little context for those of

00:00:24,000 --> 00:00:29,580
you who may have joined us a little late

00:00:26,750 --> 00:00:32,579
so uber is mission obviously is to be a

00:00:29,580 --> 00:00:34,020
transportation platform and we want to

00:00:32,579 --> 00:00:34,800
ignite opportunity by setting the world

00:00:34,020 --> 00:00:37,559
in motion

00:00:34,800 --> 00:00:41,610
as of today we operate in over 10,000

00:00:37,559 --> 00:00:44,550
cities worldwide since in the last 1012

00:00:41,610 --> 00:00:48,239
years we've supported over 15 billion

00:00:44,550 --> 00:00:50,550
trips in total of which 7 billion of

00:00:48,239 --> 00:00:52,230
them came from just 2019 alone right so

00:00:50,550 --> 00:00:56,100
that's some significant exponential

00:00:52,230 --> 00:00:58,800
growth there and as will be obvious to

00:00:56,100 --> 00:01:01,379
most people are we are very data focused

00:00:58,800 --> 00:01:04,379
and data-driven and we have a bunch of

00:01:01,379 --> 00:01:08,250
very diverse teams who rely on the data

00:01:04,379 --> 00:01:10,280
platforms to support their agendas so

00:01:08,250 --> 00:01:14,820
everything from regulatory compliance to

00:01:10,280 --> 00:01:17,460
marketing to pricing and data science

00:01:14,820 --> 00:01:22,439
and so on and a lot of these use cases

00:01:17,460 --> 00:01:26,369
and depend on presto at uber so what is

00:01:22,439 --> 00:01:29,360
presto at uber looked like we currently

00:01:26,369 --> 00:01:32,790
have over 12,000 monthly active users

00:01:29,360 --> 00:01:36,000
using the Presto platform and we support

00:01:32,790 --> 00:01:38,220
about 400,000 queries every day this

00:01:36,000 --> 00:01:43,020
translates to reading about 35 petabytes

00:01:38,220 --> 00:01:47,750
of data from HDFS every day and we have

00:01:43,020 --> 00:01:52,890
12 presto clusters divided up among

00:01:47,750 --> 00:01:54,840
2,600 actual nodes so some pretty

00:01:52,890 --> 00:01:57,960
significant number is up there right so

00:01:54,840 --> 00:01:59,850
what I'm gonna give do for the next 15

00:01:57,960 --> 00:02:01,710
minutes is instead of focusing on one

00:01:59,850 --> 00:02:03,299
particular topic and doing it deep drive

00:02:01,710 --> 00:02:06,450
on that I'm gonna give you a glimpse

00:02:03,299 --> 00:02:08,729
into multiple work streams that we

00:02:06,450 --> 00:02:10,810
currently have going at over to support

00:02:08,729 --> 00:02:15,430
our

00:02:10,810 --> 00:02:18,849
I suppose about operating uber at our

00:02:15,430 --> 00:02:20,530
scale so the first one is partial

00:02:18,849 --> 00:02:24,790
aggregation pushdown so let's start with

00:02:20,530 --> 00:02:26,800
that right so the main motivation here

00:02:24,790 --> 00:02:29,410
is that we have a lot of data

00:02:26,800 --> 00:02:32,680
consistency checks trust checks and

00:02:29,410 --> 00:02:35,230
health checks that run on presto at uber

00:02:32,680 --> 00:02:37,390
and these basically account for about 28

00:02:35,230 --> 00:02:41,170
percent of all crystal traffic right in

00:02:37,390 --> 00:02:42,730
terms of number of queries so the

00:02:41,170 --> 00:02:46,150
petabytes of data that we actually

00:02:42,730 --> 00:02:49,810
process are it's way too expensive for

00:02:46,150 --> 00:02:52,660
us to do any kind of hashing or CRC

00:02:49,810 --> 00:02:55,810
checks on this on this data so what we

00:02:52,660 --> 00:02:58,390
wind up doing is using row count or some

00:02:55,810 --> 00:03:01,330
kind of other simple aggregations on

00:02:58,390 --> 00:03:06,580
tables or table partitions as a proxy

00:03:01,330 --> 00:03:09,250
for the actual validation right so what

00:03:06,580 --> 00:03:10,920
people we had a little light but light

00:03:09,250 --> 00:03:14,200
bulb moment right when we realize that

00:03:10,920 --> 00:03:16,299
all these Hadoop file formats they

00:03:14,200 --> 00:03:18,579
support some kind of statistics as part

00:03:16,299 --> 00:03:22,480
of the metadata in the files header or

00:03:18,579 --> 00:03:24,040
footer and presto does not use that in

00:03:22,480 --> 00:03:27,130
order to compute these aggregations so

00:03:24,040 --> 00:03:28,750
what we do is we are building an

00:03:27,130 --> 00:03:31,989
aggregation pushdown mechanism which is

00:03:28,750 --> 00:03:34,000
going to support using these statistics

00:03:31,989 --> 00:03:37,359
to compute the actual aggregates for

00:03:34,000 --> 00:03:39,639
some simple aggregates so this will save

00:03:37,359 --> 00:03:41,980
time by avoiding reading the whole file

00:03:39,639 --> 00:03:45,459
and doing the initial round of

00:03:41,980 --> 00:03:46,989
aggregation so for people who what is

00:03:45,459 --> 00:03:49,600
really look like right so if you take a

00:03:46,989 --> 00:03:51,790
simple query like so where I have a

00:03:49,600 --> 00:03:53,590
bunch of simple aggregates on a table on

00:03:51,790 --> 00:03:56,260
say a partition column or something like

00:03:53,590 --> 00:03:58,660
that this is a fairly contrived example

00:03:56,260 --> 00:03:59,980
right I just put it there to illustrate

00:03:58,660 --> 00:04:02,350
the fact that we can do multiple

00:03:59,980 --> 00:04:06,280
aggregations like this so we have a

00:04:02,350 --> 00:04:10,019
count star count column Max and min and

00:04:06,280 --> 00:04:12,760
this query plan will be fairly familiar

00:04:10,019 --> 00:04:14,470
for people who work with very optimizer

00:04:12,760 --> 00:04:16,750
interest all right or excitation engine

00:04:14,470 --> 00:04:20,289
so we start off with the table scan at

00:04:16,750 --> 00:04:22,690
the bottom where the actual read happens

00:04:20,289 --> 00:04:24,040
from the table in HDFS there is an

00:04:22,690 --> 00:04:26,140
initial

00:04:24,040 --> 00:04:28,860
round of aggregation the partial

00:04:26,140 --> 00:04:31,780
aggregation which computes the

00:04:28,860 --> 00:04:35,260
aggregates for count and Max and min for

00:04:31,780 --> 00:04:36,790
the file level or and then we have the

00:04:35,260 --> 00:04:38,740
final round of aggregation after the

00:04:36,790 --> 00:04:40,300
exchange buffers which that's the final

00:04:38,740 --> 00:04:43,180
aggregation right so that's going to be

00:04:40,300 --> 00:04:46,570
something like sum of counts or max of

00:04:43,180 --> 00:04:48,490
Max's and things like that so what we do

00:04:46,570 --> 00:04:52,750
here is we basically instead of having

00:04:48,490 --> 00:04:54,430
the partial aggregation we collapse the

00:04:52,750 --> 00:04:56,290
partial aggregation and table scan into

00:04:54,430 --> 00:04:58,510
one step where the table scan instead of

00:04:56,290 --> 00:05:02,730
reading the raw data from the file it

00:04:58,510 --> 00:05:05,830
only picks up the statistics from the

00:05:02,730 --> 00:05:07,570
footer and then propagates them as the

00:05:05,830 --> 00:05:09,760
partial aggregate so basically the

00:05:07,570 --> 00:05:11,890
output of the table scan here would have

00:05:09,760 --> 00:05:15,430
been the output of the partial aggregate

00:05:11,890 --> 00:05:18,010
step in the previous query plan right so

00:05:15,430 --> 00:05:19,780
this is fairly simple we are in the

00:05:18,010 --> 00:05:22,990
process of production izing this we have

00:05:19,780 --> 00:05:25,180
some numbers from from our staging

00:05:22,990 --> 00:05:28,600
clusters for query for certain kinds of

00:05:25,180 --> 00:05:30,280
query shapes we see significant

00:05:28,600 --> 00:05:34,270
improvement as you can imagine right we

00:05:30,280 --> 00:05:37,480
have anything from 2 X 2 for X but we

00:05:34,270 --> 00:05:40,860
still haven't evaluated the impact in

00:05:37,480 --> 00:05:45,840
our production traffic for these things

00:05:40,860 --> 00:05:47,950
one simple gotcha here is that we have

00:05:45,840 --> 00:05:50,200
some processes which may not be

00:05:47,950 --> 00:05:52,210
populating these statistics in some

00:05:50,200 --> 00:05:54,160
cases and we are still trying to figure

00:05:52,210 --> 00:05:56,160
out what the right way of gracefully

00:05:54,160 --> 00:05:58,930
handling that situation would be right

00:05:56,160 --> 00:06:01,090
in our case it's fairly easy but if

00:05:58,930 --> 00:06:02,320
you're talking about open sourcing this

00:06:01,090 --> 00:06:04,200
then we need to figure out some way of

00:06:02,320 --> 00:06:08,190
gracefully handling that situation so

00:06:04,200 --> 00:06:08,190
any ideas are welcome

00:06:08,370 --> 00:06:13,510
so let me pivot and move on to the next

00:06:11,680 --> 00:06:15,400
topic and I'm going to talk to you about

00:06:13,510 --> 00:06:20,890
approximate queries right so this is a

00:06:15,400 --> 00:06:24,430
well-known concept in databases where we

00:06:20,890 --> 00:06:26,740
have and this is typically achieved by

00:06:24,430 --> 00:06:29,350
some kind of a sampling mechanism right

00:06:26,740 --> 00:06:32,919
where the actual the high precision of

00:06:29,350 --> 00:06:35,740
your queries is not as important in a

00:06:32,919 --> 00:06:37,979
lot of use cases and some estimates or

00:06:35,740 --> 00:06:40,419
approximations are perfectly adequate

00:06:37,979 --> 00:06:42,280
so this is the crate of obviously is the

00:06:40,419 --> 00:06:46,150
compute resources versus accuracy of

00:06:42,280 --> 00:06:48,160
your results and most databases achieve

00:06:46,150 --> 00:06:51,970
this by performing some kind of sampling

00:06:48,160 --> 00:06:54,900
on the data right so for example we have

00:06:51,970 --> 00:06:57,580
we have classes of queries that do

00:06:54,900 --> 00:06:58,990
incremental development or testing right

00:06:57,580 --> 00:07:02,009
you're trying to converge on some kind

00:06:58,990 --> 00:07:04,090
of model or exploratory analysis and

00:07:02,009 --> 00:07:07,169
even when you have converging on

00:07:04,090 --> 00:07:10,330
particular model it's perfectly okay to

00:07:07,169 --> 00:07:12,970
trade-off accuracy for compute resources

00:07:10,330 --> 00:07:15,520
as long as you can contain your error

00:07:12,970 --> 00:07:20,349
within some predefined bound and so on

00:07:15,520 --> 00:07:22,000
so in our use cases right what we are

00:07:20,349 --> 00:07:24,580
trying to do is we are trying to create

00:07:22,000 --> 00:07:29,229
a sample version of the relevant data

00:07:24,580 --> 00:07:31,000
sets and the instinct the the motivation

00:07:29,229 --> 00:07:33,460
here is that we want to rewrite our

00:07:31,000 --> 00:07:35,590
queries to use these sampled data sets

00:07:33,460 --> 00:07:37,990
as opposed to the raw table data so let

00:07:35,590 --> 00:07:41,620
me explain why this is important to us

00:07:37,990 --> 00:07:43,240
if you consider some enormous data sets

00:07:41,620 --> 00:07:46,650
right like we typically aren't thinking

00:07:43,240 --> 00:07:48,900
about mobile app event data or

00:07:46,650 --> 00:07:52,210
clickstream data or something like that

00:07:48,900 --> 00:07:55,289
these are tables which contribute

00:07:52,210 --> 00:07:57,789
several billion records every day and

00:07:55,289 --> 00:08:01,599
any kind of meaningful analysis on that

00:07:57,789 --> 00:08:06,250
using presto is it's almost impossible

00:08:01,599 --> 00:08:08,949
for us right so so the the two things we

00:08:06,250 --> 00:08:11,979
want to do here is that the typical of

00:08:08,949 --> 00:08:13,659
the out-of-the-box sampling techniques

00:08:11,979 --> 00:08:16,780
that we get to theta base engines right

00:08:13,659 --> 00:08:19,180
when pesto has a couple table sample and

00:08:16,780 --> 00:08:20,949
so on these things don't quite work

00:08:19,180 --> 00:08:22,720
because they don't they are not

00:08:20,949 --> 00:08:24,699
cognizant of the actual structure of the

00:08:22,720 --> 00:08:27,669
data so if you take mobile app even data

00:08:24,699 --> 00:08:31,840
for example it's perfectly okay in our

00:08:27,669 --> 00:08:34,419
world to sample a 1 percent or 0.1

00:08:31,840 --> 00:08:37,390
percent of the entire data set provided

00:08:34,419 --> 00:08:44,110
we have a consistent view of the whole

00:08:37,390 --> 00:08:46,810
device data right so I if I'm if I have

00:08:44,110 --> 00:08:48,940
a billion users I'm okay with only

00:08:46,810 --> 00:08:50,650
sampling a few thousand of them as long

00:08:48,940 --> 00:08:51,580
as I have the entire history for each of

00:08:50,650 --> 00:08:54,730
these users

00:08:51,580 --> 00:08:58,660
so basically I want all the UUID entries

00:08:54,730 --> 00:09:00,430
for a particular for a unique device so

00:08:58,660 --> 00:09:02,500
getting this right using the existing

00:09:00,430 --> 00:09:05,320
sampling methods is very hard which is

00:09:02,500 --> 00:09:08,410
why we go we are going with the approach

00:09:05,320 --> 00:09:11,440
of creating and maintaining ETL

00:09:08,410 --> 00:09:14,530
pipelines which will do this sampling

00:09:11,440 --> 00:09:16,210
for us so conceptually this is what it

00:09:14,530 --> 00:09:18,040
is going to look like right like a query

00:09:16,210 --> 00:09:20,530
coming in with some session parameters

00:09:18,040 --> 00:09:23,140
set which will let you know that using

00:09:20,530 --> 00:09:25,690
sample data sets is okay we would

00:09:23,140 --> 00:09:28,720
rewrite the query within presto

00:09:25,690 --> 00:09:30,160
using a config service which does which

00:09:28,720 --> 00:09:31,720
has this mapping right basically between

00:09:30,160 --> 00:09:34,690
the sample data sets in your block and

00:09:31,720 --> 00:09:36,550
the raw data sets and and then execute

00:09:34,690 --> 00:09:39,970
the query on christo on the sample data

00:09:36,550 --> 00:09:44,230
sets in addition to doing a simple query

00:09:39,970 --> 00:09:47,400
rewrite with table name replaced we also

00:09:44,230 --> 00:09:51,370
have we are also planning on maintaining

00:09:47,400 --> 00:09:55,480
statistics on the actual sampled process

00:09:51,370 --> 00:09:57,490
right so this way we can have there are

00:09:55,480 --> 00:09:58,240
some the simple case would be like if

00:09:57,490 --> 00:10:00,100
you have a count

00:09:58,240 --> 00:10:01,810
Starke where you coming in if you have

00:10:00,100 --> 00:10:03,940
the sampling percentage now you know

00:10:01,810 --> 00:10:06,220
what the actual count star would be

00:10:03,940 --> 00:10:10,560
right so but if you extrapolate this to

00:10:06,220 --> 00:10:10,560
a few other cases where you can have

00:10:16,710 --> 00:10:20,380
where you can happen sorry if you if you

00:10:19,030 --> 00:10:21,910
extrapolate this to a few other cases

00:10:20,380 --> 00:10:24,040
where you can actually propagate your

00:10:21,910 --> 00:10:25,930
error bounds across join boundaries or

00:10:24,040 --> 00:10:27,610
some other kinds of aggregations this

00:10:25,930 --> 00:10:30,040
would certainly help with maintaining

00:10:27,610 --> 00:10:32,680
and providing a richer class of queries

00:10:30,040 --> 00:10:35,860
and better estimates for various use

00:10:32,680 --> 00:10:38,170
cases so this is this work is still in

00:10:35,860 --> 00:10:39,580
its early stages and we have some ideas

00:10:38,170 --> 00:10:41,140
if there are people who are interested

00:10:39,580 --> 00:10:43,060
or you have experienced something like

00:10:41,140 --> 00:10:44,650
this before again please reach out to us

00:10:43,060 --> 00:10:53,170
we would be happy to have conversations

00:10:44,650 --> 00:10:54,310
with these so a final pivot this is I'm

00:10:53,170 --> 00:10:56,920
going to talk to you a little bit about

00:10:54,310 --> 00:10:58,990
our experiences with the java virtual

00:10:56,920 --> 00:11:03,000
machine and memory management that we

00:10:58,990 --> 00:11:03,000
spent a lot of time on this past year

00:11:03,670 --> 00:11:10,170
so as with most database systems

00:11:08,050 --> 00:11:11,440
high-performing database systems right

00:11:10,170 --> 00:11:13,660
presto

00:11:11,440 --> 00:11:15,700
performs very well and all these things

00:11:13,660 --> 00:11:20,650
perform very well as long as you remain

00:11:15,700 --> 00:11:22,300
within the boundaries of of certain

00:11:20,650 --> 00:11:24,430
things right so the moment you step over

00:11:22,300 --> 00:11:28,500
the boundaries of good performance then

00:11:24,430 --> 00:11:31,450
you fall off the proverbial cliff and so

00:11:28,500 --> 00:11:34,060
this happened this started happening at

00:11:31,450 --> 00:11:35,920
uber consistently last year right we

00:11:34,060 --> 00:11:39,220
started having a lot of these long

00:11:35,920 --> 00:11:41,590
garbage collection pauses these were the

00:11:39,220 --> 00:11:43,510
old generations stop the world garbage

00:11:41,590 --> 00:11:45,820
collection process where everything gets

00:11:43,510 --> 00:11:48,340
frozen up

00:11:45,820 --> 00:11:49,870
none of the jobs are able to make

00:11:48,340 --> 00:11:53,470
progress there is a drop in throughput

00:11:49,870 --> 00:11:55,660
and this is obviously leading to

00:11:53,470 --> 00:11:58,420
variable performance flaky performance

00:11:55,660 --> 00:12:00,400
and it was an operational nightmare

00:11:58,420 --> 00:12:03,340
right there were outages and all kinds

00:12:00,400 --> 00:12:05,020
of things in the shock anecdotally right

00:12:03,340 --> 00:12:06,850
in the short term what we did was we had

00:12:05,020 --> 00:12:08,080
a script which would we had some bunch

00:12:06,850 --> 00:12:11,200
of scripts which would go and look at

00:12:08,080 --> 00:12:13,120
the JB and logs parsed them trying to

00:12:11,200 --> 00:12:15,060
identify when a long GC was happening

00:12:13,120 --> 00:12:18,880
and then bounce the process basically

00:12:15,060 --> 00:12:20,470
kill it and he started to but obviously

00:12:18,880 --> 00:12:22,960
this was a less than desirable solution

00:12:20,470 --> 00:12:24,400
we didn't like it our customers were not

00:12:22,960 --> 00:12:28,450
very happy they're not very happy about

00:12:24,400 --> 00:12:30,310
that either so a fair amount of research

00:12:28,450 --> 00:12:32,470
later what we realized was that there

00:12:30,310 --> 00:12:36,700
was a simple when we got by just

00:12:32,470 --> 00:12:39,700
upgrading our JDK from 8 to 11 so presto

00:12:36,700 --> 00:12:42,730
team at uber is one of the first teams

00:12:39,700 --> 00:12:44,710
that actually did this upgrade from 8 to

00:12:42,730 --> 00:12:47,400
11 and this led to a significant

00:12:44,710 --> 00:12:50,860
improvement right we started seeing our

00:12:47,400 --> 00:12:54,400
peak GC passes go from 20 minutes or

00:12:50,860 --> 00:12:54,850
tens of minutes to 8 or 10 seconds at

00:12:54,400 --> 00:12:57,660
the most

00:12:54,850 --> 00:13:00,880
right this was this was pretty big the

00:12:57,660 --> 00:13:04,570
this also contributed to a much

00:13:00,880 --> 00:13:06,700
increased CPU throughput the main credit

00:13:04,570 --> 00:13:11,380
here goes to the parallelization of the

00:13:06,700 --> 00:13:13,750
full GC in the in the there's a g1

00:13:11,380 --> 00:13:16,940
algorithm that does the parallelization

00:13:13,750 --> 00:13:20,930
of the full GC which which is I think of

00:13:16,940 --> 00:13:22,250
from Java 10 onwards and there's a link

00:13:20,930 --> 00:13:23,900
there to this if you're interested in

00:13:22,250 --> 00:13:28,010
figuring learning more about this so

00:13:23,900 --> 00:13:29,030
anybody who is encountering anybody in

00:13:28,010 --> 00:13:31,330
the community who's encountering

00:13:29,030 --> 00:13:35,840
something like that we would recommend

00:13:31,330 --> 00:13:37,880
upgrading your jdk the second thing that

00:13:35,840 --> 00:13:39,350
we discovered in our analysis right was

00:13:37,880 --> 00:13:44,870
that there was a lot of memory pressure

00:13:39,350 --> 00:13:47,420
being put on due to the park a reader

00:13:44,870 --> 00:13:49,220
itself right so most of our datasets are

00:13:47,420 --> 00:13:52,000
in park a you know but in case I didn't

00:13:49,220 --> 00:13:54,500
mention that before so the park a reader

00:13:52,000 --> 00:13:56,990
there was a lot of large object

00:13:54,500 --> 00:13:58,520
allocation happening in part a so if you

00:13:56,990 --> 00:14:00,470
can see this list here right it's

00:13:58,520 --> 00:14:02,450
basically showing you how the allocation

00:14:00,470 --> 00:14:07,040
of human humongous regions happens in

00:14:02,450 --> 00:14:13,130
the JVM and this mainly count is from

00:14:07,040 --> 00:14:15,140
two reasons right it starts off there

00:14:13,130 --> 00:14:17,900
are two parameters which control how or

00:14:15,140 --> 00:14:19,310
which are supposed to control how memory

00:14:17,900 --> 00:14:21,980
is allocated when you read data from

00:14:19,310 --> 00:14:24,740
park' files one of them is that the

00:14:21,980 --> 00:14:28,820
Parque reader reads data in column

00:14:24,740 --> 00:14:30,350
chunks and these were both and so

00:14:28,820 --> 00:14:32,210
reading in column chunks and reading in

00:14:30,350 --> 00:14:35,720
terms of blocks both of these things

00:14:32,210 --> 00:14:37,670
were actually pre configured in pressed

00:14:35,720 --> 00:14:38,930
all right as in they were not it we were

00:14:37,670 --> 00:14:43,220
not they were not configurable in Cresco

00:14:38,930 --> 00:14:46,160
they were predetermined so what we did

00:14:43,220 --> 00:14:48,230
was we realized that there was an open

00:14:46,160 --> 00:14:51,440
source patch which dynamic which allows

00:14:48,230 --> 00:14:54,110
dynamic sizing for blocks so this was

00:14:51,440 --> 00:14:57,980
previously set to 1024 hard-coded to

00:14:54,110 --> 00:15:01,970
1024 and well this seems like a

00:14:57,980 --> 00:15:04,700
reasonable estimate in most cases if you

00:15:01,970 --> 00:15:06,589
have any kinds of complex data types in

00:15:04,700 --> 00:15:09,230
the row like embedded structures and so

00:15:06,589 --> 00:15:11,480
on this 1024 can still be a pretty big

00:15:09,230 --> 00:15:13,339
number for certain use cases right and

00:15:11,480 --> 00:15:16,160
this winds up adding a lot of allocation

00:15:13,339 --> 00:15:19,250
to the this leads to large memory

00:15:16,160 --> 00:15:21,380
allocations the second thing was presto

00:15:19,250 --> 00:15:23,270
was dealing with reading data in column

00:15:21,380 --> 00:15:26,810
chunks and that was the lowest atomic

00:15:23,270 --> 00:15:28,880
value that you were reading and changing

00:15:26,810 --> 00:15:30,500
that to changing this reader to consume

00:15:28,880 --> 00:15:32,990
at one page at a time

00:15:30,500 --> 00:15:35,930
was was is another project that we have

00:15:32,990 --> 00:15:43,970
in progress to reduce the pressure on

00:15:35,930 --> 00:15:46,280
memory from from park a breeder so these

00:15:43,970 --> 00:15:48,290
these two things are still not yet in

00:15:46,280 --> 00:15:52,070
production we are going to push it to

00:15:48,290 --> 00:15:55,550
production and we will have results on

00:15:52,070 --> 00:15:58,340
these soon so this is about all I had

00:15:55,550 --> 00:16:08,060
for today if we have a few minutes I

00:15:58,340 --> 00:16:10,310
will take some questions this is a myth

00:16:08,060 --> 00:16:11,990
here you have about like three or so

00:16:10,310 --> 00:16:14,450
minutes if you find a question in the

00:16:11,990 --> 00:16:16,040
Q&A panel that's relevant to you know

00:16:14,450 --> 00:16:18,080
what you just talked and if you just

00:16:16,040 --> 00:16:20,090
read it out and answer that otherwise

00:16:18,080 --> 00:16:21,440
you can take it offline go through the

00:16:20,090 --> 00:16:23,570
questions and we can I'm wrong wrong

00:16:21,440 --> 00:16:26,720
actually come and present at a session

00:16:23,570 --> 00:16:30,290
now sounds good I think I have a couple

00:16:26,720 --> 00:16:32,210
of questions here from Alexi so he asks

00:16:30,290 --> 00:16:35,630
this partial aggregations work with

00:16:32,210 --> 00:16:37,970
hoody so in in the in the car in in our

00:16:35,630 --> 00:16:39,500
world we use presto with hoody right and

00:16:37,970 --> 00:16:41,690
he also has a follower question do you

00:16:39,500 --> 00:16:45,050
use presto go through D we use presto

00:16:41,690 --> 00:16:46,550
with hoody the file formats are

00:16:45,050 --> 00:16:48,320
different right so I think what we are

00:16:46,550 --> 00:16:53,210
thinking about in terms of statistics is

00:16:48,320 --> 00:16:56,570
for us to make this work for parque and

00:16:53,210 --> 00:16:58,040
arc oversee and for hoody and there is a

00:16:56,570 --> 00:16:59,740
separate project where we are going to

00:16:58,040 --> 00:17:02,000
be talking of we are planning on

00:16:59,740 --> 00:17:04,760
supporting full-fledged statistics

00:17:02,000 --> 00:17:07,340
across tables right not just in terms of

00:17:04,760 --> 00:17:09,080
files or partitions so there is a

00:17:07,340 --> 00:17:11,660
project on our plate where we will be

00:17:09,080 --> 00:17:14,560
hooking presto up with full hoody

00:17:11,660 --> 00:17:14,560
statistics

00:17:16,980 --> 00:17:19,040

YouTube URL: https://www.youtube.com/watch?v=N9y-GeuzqtY


