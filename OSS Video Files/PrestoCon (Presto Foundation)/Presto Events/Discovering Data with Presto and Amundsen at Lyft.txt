Title: Discovering Data with Presto and Amundsen at Lyft
Publication date: 2021-01-18
Playlist: Presto Events
Description: 
	Speaker: Tao Feng, software engineer at Databricks, Apache Airflow PMC, Tech Lead of Amundsen, Ex-Lyft.

Amundsen is an open-source data discovery and metadata platform which is part of LF AI & Data foundation. In this talk, we will deep dive into Amundsen's architecture and how we integrate Amundsen with Presto to power the data preview and data exploration.

In addition, we will discuss how Amundsen could be customized and extended to other companies' data ecosystem. Lastly, we will close with the future roadmap of the project, what problems remain unsolved, and how we can work together to solve them.
Captions: 
	00:00:00,080 --> 00:00:04,799
everyone's so uh thanks for the invites

00:00:02,639 --> 00:00:08,480
today i'm going to talk a bit about like

00:00:04,799 --> 00:00:12,160
data discovery amazon and uh and uh

00:00:08,480 --> 00:00:14,240
presto so before that so currently i

00:00:12,160 --> 00:00:16,080
just recently joined databricks focusing

00:00:14,240 --> 00:00:18,400
on

00:00:16,080 --> 00:00:20,320
the late house and a bit on still on

00:00:18,400 --> 00:00:23,279
data discovery data catalog

00:00:20,320 --> 00:00:24,160
previously i was an engineer at the leaf

00:00:23,279 --> 00:00:26,160
data this

00:00:24,160 --> 00:00:27,359
platform and tools team led various

00:00:26,160 --> 00:00:30,800
projects i'm also

00:00:27,359 --> 00:00:33,920
apache airflow pmc members

00:00:30,800 --> 00:00:36,719
so today's agenda i'm going to talk uh

00:00:33,920 --> 00:00:38,239
give a break of intros about amazons

00:00:36,719 --> 00:00:41,440
then go into the

00:00:38,239 --> 00:00:42,719
architectures lastly i will talk a bit

00:00:41,440 --> 00:00:46,160
about like the project's

00:00:42,719 --> 00:00:49,520
impact and future work and

00:00:46,160 --> 00:00:49,520
q and a sounds good

00:00:49,760 --> 00:00:56,800
so so first let me talk about

00:00:53,039 --> 00:00:59,039
ammunitions so before amazon exists

00:00:56,800 --> 00:01:01,039
how's the data discoveries looks like

00:00:59,039 --> 00:01:06,159
and live here is how it looks like

00:01:01,039 --> 00:01:09,360
so before that we we have like in 2018

00:01:06,159 --> 00:01:10,000
we have we started uh enter the hyper

00:01:09,360 --> 00:01:13,920
growth

00:01:10,000 --> 00:01:15,200
in a we start have more data scientists

00:01:13,920 --> 00:01:18,560
data engineer to do

00:01:15,200 --> 00:01:21,119
like various discovery uh

00:01:18,560 --> 00:01:24,320
we only have a static wiki conference

00:01:21,119 --> 00:01:25,040
wiki page to host like about a 20 wiki

00:01:24,320 --> 00:01:29,119
page of

00:01:25,040 --> 00:01:31,200
20 tables are listed and

00:01:29,119 --> 00:01:32,240
and then the metadata refresh is through

00:01:31,200 --> 00:01:35,280
a cron jobs

00:01:32,240 --> 00:01:36,320
no and cannot be added by humans uh

00:01:35,280 --> 00:01:38,880
curations

00:01:36,320 --> 00:01:40,240
the metadata is also pretty limited like

00:01:38,880 --> 00:01:44,479
including like

00:01:40,240 --> 00:01:48,560
uh owners like codes and descriptions

00:01:44,479 --> 00:01:48,560
and it's not very easy to extend and

00:01:50,000 --> 00:01:53,840
so you could see it like the data

00:01:52,240 --> 00:01:55,840
starter experience is not very

00:01:53,840 --> 00:01:58,640
productive like we have a

00:01:55,840 --> 00:01:58,960
previously our product manager have done

00:01:58,640 --> 00:02:02,960
a

00:01:58,960 --> 00:02:06,079
study to uh research studies to see like

00:02:02,960 --> 00:02:07,680
how much time spent by a data scientist

00:02:06,079 --> 00:02:09,280
they found that about 30 percent of

00:02:07,680 --> 00:02:12,400
their time actually spending

00:02:09,280 --> 00:02:14,959
in data discovery so now

00:02:12,400 --> 00:02:16,080
only very limited time on like the data

00:02:14,959 --> 00:02:18,000
analysis

00:02:16,080 --> 00:02:19,599
so as you can see like data discovery

00:02:18,000 --> 00:02:21,599
itself doesn't provide

00:02:19,599 --> 00:02:23,120
much value because you would like the

00:02:21,599 --> 00:02:25,120
scientists to

00:02:23,120 --> 00:02:27,200
spend most of their time doing impactful

00:02:25,120 --> 00:02:27,840
work like doing like the actual analysis

00:02:27,200 --> 00:02:30,319
for

00:02:27,840 --> 00:02:32,640
to make like decision making so how

00:02:30,319 --> 00:02:35,040
could we reduce the data discovery time

00:02:32,640 --> 00:02:38,560
the answer is like using a data catalog

00:02:35,040 --> 00:02:41,519
a metadata tool like amazon

00:02:38,560 --> 00:02:43,599
so what you saw in a nutshell it's a

00:02:41,519 --> 00:02:44,800
data discovery metadata platform to

00:02:43,599 --> 00:02:47,760
improve the data

00:02:44,800 --> 00:02:48,560
productivity for data scientists data

00:02:47,760 --> 00:02:51,840
engineers

00:02:48,560 --> 00:02:55,200
who interact with data it's currently

00:02:51,840 --> 00:02:58,720
hosted at uh linux foundation ai and

00:02:55,200 --> 00:03:00,640
and and data as a inc as uh

00:02:58,720 --> 00:03:01,760
incubation projects and it's

00:03:00,640 --> 00:03:04,720
well-defined

00:03:01,760 --> 00:03:06,239
open governance and ifc process for

00:03:04,720 --> 00:03:09,519
detail you could take a look at

00:03:06,239 --> 00:03:09,519
the link in the blog post

00:03:09,760 --> 00:03:13,920
so so how's this look like for amazon

00:03:12,720 --> 00:03:16,640
here is like the

00:03:13,920 --> 00:03:18,000
home page so you could see like you have

00:03:16,640 --> 00:03:20,239
it come with a search bar

00:03:18,000 --> 00:03:22,239
allow you to search any uh with a

00:03:20,239 --> 00:03:23,760
resource entity that have been indexed

00:03:22,239 --> 00:03:27,200
into a month

00:03:23,760 --> 00:03:29,040
you could see like uh book tags

00:03:27,200 --> 00:03:31,680
like basically you want to classify some

00:03:29,040 --> 00:03:34,159
of the resource

00:03:31,680 --> 00:03:35,040
that grouped by certain texts bookmarks

00:03:34,159 --> 00:03:38,480
some of the

00:03:35,040 --> 00:03:40,480
resources like table dashboards

00:03:38,480 --> 00:03:41,599
that has that you are follow and

00:03:40,480 --> 00:03:43,680
bookmark

00:03:41,599 --> 00:03:45,599
lastly you say you come with a popular

00:03:43,680 --> 00:03:48,640
tables it's like

00:03:45,599 --> 00:03:50,560
based on the uh usage you want to

00:03:48,640 --> 00:03:52,879
service and mode the most uh

00:03:50,560 --> 00:03:55,599
data set that have been used across the

00:03:52,879 --> 00:03:55,599
whole company

00:03:56,319 --> 00:04:00,000
yeah you could come with like a search

00:03:59,760 --> 00:04:02,560
bar

00:04:00,000 --> 00:04:03,760
that you could do any arbitrary first

00:04:02,560 --> 00:04:06,560
research to

00:04:03,760 --> 00:04:07,840
to search any uh terms like you could

00:04:06,560 --> 00:04:10,000
search like schema name

00:04:07,840 --> 00:04:12,640
table names description columns

00:04:10,000 --> 00:04:15,439
dashboarding etc

00:04:12,640 --> 00:04:17,359
once you found one you could see the

00:04:15,439 --> 00:04:20,560
detail of the data set

00:04:17,359 --> 00:04:24,240
so here is a

00:04:20,560 --> 00:04:25,520
one here is a table page that we check

00:04:24,240 --> 00:04:29,600
like the usage log

00:04:25,520 --> 00:04:32,479
for for amazon's backup

00:04:29,600 --> 00:04:33,759
you have like you know like you ha it is

00:04:32,479 --> 00:04:35,759
a hype table

00:04:33,759 --> 00:04:37,600
it is a you know like what's the

00:04:35,759 --> 00:04:40,240
discussion about it's like checking user

00:04:37,600 --> 00:04:43,840
action so for us to better understand

00:04:40,240 --> 00:04:46,160
how uh internal users are using amazon

00:04:43,840 --> 00:04:46,960
and you have like you could see like the

00:04:46,160 --> 00:04:49,680
day range

00:04:46,960 --> 00:04:50,560
basically the low water mark the higher

00:04:49,680 --> 00:04:53,440
water marks

00:04:50,560 --> 00:04:55,280
certain tax lots update bit time stamp

00:04:53,440 --> 00:04:57,919
when it has been last updated

00:04:55,280 --> 00:05:00,160
some of the frequent user owner on

00:04:57,919 --> 00:05:01,919
owners of the tables

00:05:00,160 --> 00:05:03,520
on the right hand side you could see

00:05:01,919 --> 00:05:07,039
like columns like

00:05:03,520 --> 00:05:10,479
what are the column names types

00:05:07,039 --> 00:05:13,520
descriptions and uh while the

00:05:10,479 --> 00:05:16,639
dashboard has been using these tables

00:05:13,520 --> 00:05:18,479
on the on the

00:05:16,639 --> 00:05:20,880
netbar is you will see like what's a

00:05:18,479 --> 00:05:24,479
github source file for this page

00:05:20,880 --> 00:05:26,639
and as data preview as well data

00:05:24,479 --> 00:05:29,120
explorations

00:05:26,639 --> 00:05:31,120
if you click some of the columns and you

00:05:29,120 --> 00:05:33,919
will

00:05:31,120 --> 00:05:35,199
include the column statistic you will

00:05:33,919 --> 00:05:37,919
also surface the

00:05:35,199 --> 00:05:39,440
column description in markdown format as

00:05:37,919 --> 00:05:42,720
well as the column statistic

00:05:39,440 --> 00:05:44,400
as well yeah

00:05:42,720 --> 00:05:46,720
so this mentioned before like the

00:05:44,400 --> 00:05:47,600
description use uh resurface like you

00:05:46,720 --> 00:05:51,440
could see like which

00:05:47,600 --> 00:05:53,680
dashboard has been using these tables

00:05:51,440 --> 00:05:54,960
yeah you could also search dashboard

00:05:53,680 --> 00:05:58,160
search dashboard as well

00:05:54,960 --> 00:06:01,120
dashboard uh is very useful for uh and

00:05:58,160 --> 00:06:03,280
it's for their user to see and report

00:06:01,120 --> 00:06:05,520
their analysis

00:06:03,280 --> 00:06:07,039
once you click that you could see the

00:06:05,520 --> 00:06:08,319
dashboard page here

00:06:07,039 --> 00:06:10,720
let's say you can include some of the

00:06:08,319 --> 00:06:13,759
metadata like what's the owners

00:06:10,720 --> 00:06:14,880
uh when has been created when it's last

00:06:13,759 --> 00:06:17,039
updated

00:06:14,880 --> 00:06:19,840
when it's last run so this help us

00:06:17,039 --> 00:06:21,600
understand like

00:06:19,840 --> 00:06:22,960
but basically whether this dashboard is

00:06:21,600 --> 00:06:26,240
kind of stellar you say

00:06:22,960 --> 00:06:26,880
whether it has real uh uh refresh and

00:06:26,240 --> 00:06:29,360
runs

00:06:26,880 --> 00:06:30,240
recently or it's already like stopped

00:06:29,360 --> 00:06:32,000
working

00:06:30,240 --> 00:06:34,000
and you also get some of the nice

00:06:32,000 --> 00:06:35,840
metadata like real count like how many

00:06:34,000 --> 00:06:37,199
user has been viewing using this

00:06:35,840 --> 00:06:39,120
dashboard

00:06:37,199 --> 00:06:41,199
on the on the right hand side you know

00:06:39,120 --> 00:06:42,479
like the which table has been used by

00:06:41,199 --> 00:06:45,360
this dashboard

00:06:42,479 --> 00:06:47,120
what are queries or the charts for this

00:06:45,360 --> 00:06:50,479
given dashboard

00:06:47,120 --> 00:06:52,560
you could even see the uh co-workers

00:06:50,479 --> 00:06:55,440
like you could search like if you

00:06:52,560 --> 00:06:56,800
enable the user uh resource index you

00:06:55,440 --> 00:06:58,080
can also search like some of the

00:06:56,800 --> 00:07:01,280
employees

00:06:58,080 --> 00:07:03,599
to see what the tape what the tables uh

00:07:01,280 --> 00:07:05,520
they have been used they have been owned

00:07:03,599 --> 00:07:09,360
and they have been frequently

00:07:05,520 --> 00:07:09,360
uh they've been bookmarked

00:07:09,520 --> 00:07:13,280
and you also come with in the home page

00:07:11,520 --> 00:07:14,960
you also come with a plug-in client to

00:07:13,280 --> 00:07:17,840
support announcements so

00:07:14,960 --> 00:07:19,680
oftentimes you want to use amazon as a

00:07:17,840 --> 00:07:22,639
central data product

00:07:19,680 --> 00:07:24,160
and to report any of the new features

00:07:22,639 --> 00:07:25,680
you could use it to report like for

00:07:24,160 --> 00:07:27,680
example

00:07:25,680 --> 00:07:28,720
any new feature performance as well as

00:07:27,680 --> 00:07:30,960
for example

00:07:28,720 --> 00:07:32,000
some of the new data set has been

00:07:30,960 --> 00:07:34,479
recently built

00:07:32,000 --> 00:07:36,639
and you want to announce the user to use

00:07:34,479 --> 00:07:36,639
it

00:07:37,039 --> 00:07:41,039
and it also comes with the integration

00:07:39,360 --> 00:07:44,160
with

00:07:41,039 --> 00:07:45,759
jira and some of the usual report

00:07:44,160 --> 00:07:48,319
clients

00:07:45,759 --> 00:07:49,199
and at least we use amazon as a central

00:07:48,319 --> 00:07:52,560
data quality

00:07:49,199 --> 00:07:55,360
issue portal meaning like um user

00:07:52,560 --> 00:07:56,400
what once they re found like data called

00:07:55,360 --> 00:07:58,960
the issue

00:07:56,400 --> 00:08:01,199
they could uh they could report through

00:07:58,960 --> 00:08:04,400
a monsoon table page as well

00:08:01,199 --> 00:08:07,520
and you automatically create a ticket

00:08:04,400 --> 00:08:10,560
and and then user could see

00:08:07,520 --> 00:08:12,639
some of the past issues that have been

00:08:10,560 --> 00:08:14,639
reported by this uh

00:08:12,639 --> 00:08:16,720
table as well as the current uh data

00:08:14,639 --> 00:08:20,080
card issue that is report the currently

00:08:16,720 --> 00:08:22,960
has been still investigating

00:08:20,080 --> 00:08:23,759
and you support a data profile data

00:08:22,960 --> 00:08:26,879
preview

00:08:23,759 --> 00:08:30,000
plug-in client as well so

00:08:26,879 --> 00:08:31,840
at least we our bi 2 is apache

00:08:30,000 --> 00:08:34,320
we leverage like superset and mod

00:08:31,840 --> 00:08:34,959
analytics so we build a plug-in client

00:08:34,320 --> 00:08:38,080
with

00:08:34,959 --> 00:08:40,399
superset tools to service the data

00:08:38,080 --> 00:08:41,919
data preview so that you don't need you

00:08:40,399 --> 00:08:42,399
don't need to like say every user every

00:08:41,919 --> 00:08:44,560
time you

00:08:42,399 --> 00:08:47,360
want to see the data share run like

00:08:44,560 --> 00:08:50,000
select star from a given table

00:08:47,360 --> 00:08:50,000
many times

00:08:51,200 --> 00:08:56,240
if you want to do like a complex data

00:08:53,600 --> 00:08:59,839
explorations after seeing the data shape

00:08:56,240 --> 00:09:01,760
data profile you could even like

00:08:59,839 --> 00:09:03,600
click the explore button and then

00:09:01,760 --> 00:09:05,680
redirect to

00:09:03,600 --> 00:09:06,959
some bi tool for example here you say

00:09:05,680 --> 00:09:10,320
the sql lab for

00:09:06,959 --> 00:09:13,360
superset so we you could uh and we

00:09:10,320 --> 00:09:15,360
placed some sql template for you to to

00:09:13,360 --> 00:09:18,000
do data exploration

00:09:15,360 --> 00:09:18,399
you could modify and then uh do like com

00:09:18,000 --> 00:09:23,440
uh

00:09:18,399 --> 00:09:25,839
other data set joining etc

00:09:23,440 --> 00:09:27,760
given this is a presto meetup let's also

00:09:25,839 --> 00:09:31,519
talk a bit about like presto

00:09:27,760 --> 00:09:31,760
at lyft and how amazon has been utilized

00:09:31,519 --> 00:09:35,040
and

00:09:31,760 --> 00:09:38,720
integrated with presto so uh

00:09:35,040 --> 00:09:41,040
suppresso uh live

00:09:38,720 --> 00:09:41,760
so presto primary has been used for ad

00:09:41,040 --> 00:09:44,800
hoc

00:09:41,760 --> 00:09:47,680
uh query use case

00:09:44,800 --> 00:09:48,080
and recently you start to evolve and

00:09:47,680 --> 00:09:51,440
have

00:09:48,080 --> 00:09:54,880
a lot more like etl uh use case

00:09:51,440 --> 00:09:57,360
as well so traditionally uh the etl

00:09:54,880 --> 00:09:58,560
is primarily on high and a little bit on

00:09:57,360 --> 00:10:01,680
spark

00:09:58,560 --> 00:10:03,839
but then recently like the team the

00:10:01,680 --> 00:10:06,320
presto team at lyft has been

00:10:03,839 --> 00:10:07,760
building and working on enable like etl

00:10:06,320 --> 00:10:10,720
use case for

00:10:07,760 --> 00:10:12,399
for presto as well so you can see like

00:10:10,720 --> 00:10:14,000
on the left hand side there are not sure

00:10:12,399 --> 00:10:15,440
you could see my mouse but then there

00:10:14,000 --> 00:10:19,200
are two primary use case

00:10:15,440 --> 00:10:22,399
uh like ad hoc as a etl like our etl

00:10:19,200 --> 00:10:24,240
orchestration and live is airflow

00:10:22,399 --> 00:10:26,000
they all issue like certain query

00:10:24,240 --> 00:10:29,200
against the presto

00:10:26,000 --> 00:10:30,000
gateway and then the gateway will based

00:10:29,200 --> 00:10:33,040
on the

00:10:30,000 --> 00:10:36,399
query header to the

00:10:33,040 --> 00:10:39,440
delegate acquire request to either a

00:10:36,399 --> 00:10:43,040
hog cluster as well as a or

00:10:39,440 --> 00:10:46,640
etl cluster based on the use case

00:10:43,040 --> 00:10:49,839
i believe i i believe uh cluster

00:10:46,640 --> 00:10:51,920
the query timeout will be about like 10

00:10:49,839 --> 00:10:53,600
minutes while the etl use case intel

00:10:51,920 --> 00:10:56,800
customer will have a longer

00:10:53,600 --> 00:10:57,839
query timeouts and all these queries

00:10:56,800 --> 00:11:01,040
will be locked

00:10:57,839 --> 00:11:01,360
through a central query lock which will

00:11:01,040 --> 00:11:05,839
show

00:11:01,360 --> 00:11:08,720
surface let's say uh which etl jobs

00:11:05,839 --> 00:11:09,279
has been triggered this press required

00:11:08,720 --> 00:11:11,360
or

00:11:09,279 --> 00:11:12,800
which user has been running this press

00:11:11,360 --> 00:11:15,120
query

00:11:12,800 --> 00:11:17,200
and all the espresso uh query law will

00:11:15,120 --> 00:11:19,440
be consumed by amazon

00:11:17,200 --> 00:11:20,240
i must use uh the presto query

00:11:19,440 --> 00:11:23,120
requirement

00:11:20,240 --> 00:11:24,399
for a few reasons first when we build

00:11:23,120 --> 00:11:26,959
the search index

00:11:24,399 --> 00:11:28,000
we want to service the most relevant

00:11:26,959 --> 00:11:32,160
tables based on

00:11:28,000 --> 00:11:32,720
usage so we will based on the query law

00:11:32,160 --> 00:11:35,279
usage

00:11:32,720 --> 00:11:36,160
we will do a ranking on the on the

00:11:35,279 --> 00:11:40,839
search index

00:11:36,160 --> 00:11:44,160
and wait those data center has a higher

00:11:40,839 --> 00:11:44,560
usage we also like surface like rich etl

00:11:44,160 --> 00:11:47,279
job

00:11:44,560 --> 00:11:49,360
like create which tables with some of

00:11:47,279 --> 00:11:51,680
the customization on airflow

00:11:49,360 --> 00:11:52,800
and which will get locked in the query

00:11:51,680 --> 00:11:54,560
log

00:11:52,800 --> 00:11:56,079
and we also want to service for example

00:11:54,560 --> 00:11:57,839
you will see the previously the page

00:11:56,079 --> 00:11:59,120
we want the surface that which user has

00:11:57,839 --> 00:12:01,440
been reading

00:11:59,120 --> 00:12:04,399
rich tables so we want to service this

00:12:01,440 --> 00:12:04,399
information as well

00:12:04,639 --> 00:12:08,880
now let me get into a bit on the

00:12:06,480 --> 00:12:11,120
architectures

00:12:08,880 --> 00:12:13,519
amazon is coming with live stream

00:12:11,120 --> 00:12:14,079
services like front-end metadata and

00:12:13,519 --> 00:12:16,880
search

00:12:14,079 --> 00:12:18,800
i would and then the uh ingestion will

00:12:16,880 --> 00:12:21,920
uh the default injection is come with a

00:12:18,800 --> 00:12:25,360
data builder ingestion library

00:12:21,920 --> 00:12:27,600
the front end is written like a

00:12:25,360 --> 00:12:30,480
pretty important data from a stack like

00:12:27,600 --> 00:12:33,839
written in typescript with react.js

00:12:30,480 --> 00:12:37,600
using redux for state management

00:12:33,839 --> 00:12:38,079
the metadata services could connect with

00:12:37,600 --> 00:12:41,440
different

00:12:38,079 --> 00:12:45,440
uh data store currently it uh

00:12:41,440 --> 00:12:48,720
it comes with neo4j which is a cyber

00:12:45,440 --> 00:12:49,920
cyber query base aws neptune which are

00:12:48,720 --> 00:12:51,920
grammarly based

00:12:49,920 --> 00:12:53,040
it also supports average atlas which is

00:12:51,920 --> 00:12:56,800
a pretty

00:12:53,040 --> 00:12:58,720
uh popular metadata metadata

00:12:56,800 --> 00:13:01,600
store engines as the back another

00:12:58,720 --> 00:13:01,600
backend as well

00:13:01,920 --> 00:13:06,560
you support live rest api for a live

00:13:04,240 --> 00:13:07,360
with support rs api for other services

00:13:06,560 --> 00:13:10,160
to pushing

00:13:07,360 --> 00:13:14,160
and pulling metadata directly and the

00:13:10,160 --> 00:13:17,360
authorization is through like on voip

00:13:14,160 --> 00:13:18,320
the search services is a com also come

00:13:17,360 --> 00:13:20,560
with a support

00:13:18,320 --> 00:13:21,360
using like elasticsearch in the back

00:13:20,560 --> 00:13:23,120
back end

00:13:21,360 --> 00:13:24,800
to support a few different search

00:13:23,120 --> 00:13:26,959
patterns like fascist search

00:13:24,800 --> 00:13:28,639
as well multi-facets as such first

00:13:26,959 --> 00:13:30,160
research is like in the homepage if you

00:13:28,639 --> 00:13:32,639
don't know what you are trying to search

00:13:30,160 --> 00:13:33,760
you only know certain terms are eta

00:13:32,639 --> 00:13:36,000
dispatch

00:13:33,760 --> 00:13:36,800
you will you can't you could use that to

00:13:36,000 --> 00:13:38,639
modify this

00:13:36,800 --> 00:13:40,880
search you say if you know you want to

00:13:38,639 --> 00:13:42,399
search certain schema

00:13:40,880 --> 00:13:44,079
table under certain schema you could

00:13:42,399 --> 00:13:46,480
limit the result set under certain

00:13:44,079 --> 00:13:48,720
conditions

00:13:46,480 --> 00:13:49,839
so the data builder is a default

00:13:48,720 --> 00:13:53,040
ingestion library

00:13:49,839 --> 00:13:54,399
if you're using neo4j or neo4j as a

00:13:53,040 --> 00:13:56,399
backend store

00:13:54,399 --> 00:13:57,760
and it's very inspired by a project

00:13:56,399 --> 00:14:00,639
called apache government

00:13:57,760 --> 00:14:02,160
it comes with four stages like extractor

00:14:00,639 --> 00:14:04,880
transformer loader and publish

00:14:02,160 --> 00:14:06,480
extractor is like you connect because

00:14:04,880 --> 00:14:07,120
there are so many different hectogenius

00:14:06,480 --> 00:14:09,839
source

00:14:07,120 --> 00:14:11,680
we define a generic interface but allow

00:14:09,839 --> 00:14:13,279
you to extend for different hydrogens

00:14:11,680 --> 00:14:14,240
you could build your own snowflake

00:14:13,279 --> 00:14:17,519
extractor

00:14:14,240 --> 00:14:18,560
bigquery extractor even like uh superset

00:14:17,519 --> 00:14:21,680
extractor or

00:14:18,560 --> 00:14:24,320
uh or redux extractor for example

00:14:21,680 --> 00:14:25,440
which will extract record and fit into a

00:14:24,320 --> 00:14:27,199
given model

00:14:25,440 --> 00:14:29,279
the transformer stage is like if you

00:14:27,199 --> 00:14:30,560
want to enrich this model data

00:14:29,279 --> 00:14:32,560
then you could put some of the

00:14:30,560 --> 00:14:36,240
customization transformer

00:14:32,560 --> 00:14:38,240
on and that will change the model

00:14:36,240 --> 00:14:39,519
then lastly pause it to load the loader

00:14:38,240 --> 00:14:42,800
is try to load

00:14:39,519 --> 00:14:43,279
the record into a staging area which

00:14:42,800 --> 00:14:46,639
will be

00:14:43,279 --> 00:14:48,959
later easy to do a batch publish and uh

00:14:46,639 --> 00:14:52,480
push the data into the downstream store

00:14:48,959 --> 00:14:52,480
like neo4j omnipotent

00:14:52,639 --> 00:14:57,519
and how is data builder orchestrate so

00:14:55,680 --> 00:15:00,160
at least we use like

00:14:57,519 --> 00:15:00,720
uh we use airflow which is a pretty

00:15:00,160 --> 00:15:02,880
popular

00:15:00,720 --> 00:15:05,920
orchestration framework to orchestrate

00:15:02,880 --> 00:15:07,680
so this graph is drawn like in 2018

00:15:05,920 --> 00:15:09,440
for now i think it's already become

00:15:07,680 --> 00:15:10,160
pretty huge it's like we have many

00:15:09,440 --> 00:15:13,360
different

00:15:10,160 --> 00:15:14,880
data source and and

00:15:13,360 --> 00:15:16,399
one of the nice things for airflow is

00:15:14,880 --> 00:15:17,519
that allow you to specify different

00:15:16,399 --> 00:15:20,160
dependency

00:15:17,519 --> 00:15:22,079
so you want certain metadata to to

00:15:20,160 --> 00:15:25,199
trigger first before

00:15:22,079 --> 00:15:27,199
trigger the downstream uh metadata like

00:15:25,199 --> 00:15:29,040
if we want in this case we want like we

00:15:27,199 --> 00:15:29,519
want to index and make sure the table

00:15:29,040 --> 00:15:33,120
meta

00:15:29,519 --> 00:15:35,600
data job has been finished first

00:15:33,120 --> 00:15:37,680
and thanks to a community we now have

00:15:35,600 --> 00:15:39,360
many different building connectors

00:15:37,680 --> 00:15:42,160
to connect with any different

00:15:39,360 --> 00:15:45,040
hectogenous source

00:15:42,160 --> 00:15:46,320
so a bit of deep dive for the uh the

00:15:45,040 --> 00:15:49,839
amazon

00:15:46,320 --> 00:15:52,639
so the metadata model so

00:15:49,839 --> 00:15:53,279
it currently it comes with three types

00:15:52,639 --> 00:15:56,560
of

00:15:53,279 --> 00:16:00,160
resource indexing so data sets people

00:15:56,560 --> 00:16:03,199
and dashboard we model the data set uh

00:16:00,160 --> 00:16:04,800
as a graph so the central of the data

00:16:03,199 --> 00:16:06,880
set will be a table nose

00:16:04,800 --> 00:16:08,639
and it has a different different extent

00:16:06,880 --> 00:16:10,480
metadata

00:16:08,639 --> 00:16:11,680
you connect with like for example you

00:16:10,480 --> 00:16:14,320
connect with the columns

00:16:11,680 --> 00:16:15,360
like with the column has column

00:16:14,320 --> 00:16:17,440
relationship

00:16:15,360 --> 00:16:18,800
connect with the discretionary that has

00:16:17,440 --> 00:16:21,519
this question

00:16:18,800 --> 00:16:22,000
this allows us to easily extend if we

00:16:21,519 --> 00:16:25,120
want to

00:16:22,000 --> 00:16:29,360
add uh

00:16:25,120 --> 00:16:32,160
other metadata with just like adding a

00:16:29,360 --> 00:16:33,440
model like that in a graph allow us to

00:16:32,160 --> 00:16:35,519
easily extend

00:16:33,440 --> 00:16:37,120
later on if we want to add more

00:16:35,519 --> 00:16:38,480
additional metadata we just need to

00:16:37,120 --> 00:16:40,240
place the additional metadata

00:16:38,480 --> 00:16:42,480
as a node in the graph and view our

00:16:40,240 --> 00:16:47,120
edges

00:16:42,480 --> 00:16:47,120
edges and connect to the existing nodes

00:16:47,519 --> 00:16:54,880
if we consider metadata both

00:16:51,279 --> 00:16:58,079
uh uh the metadata are

00:16:54,880 --> 00:17:00,399
very important it comes with two uh uh

00:16:58,079 --> 00:17:02,720
uh two approach security one is manual

00:17:00,399 --> 00:17:04,720
created one one is programmatic already

00:17:02,720 --> 00:17:07,039
programmatic quality is typically it's

00:17:04,720 --> 00:17:08,799
like you want to ingest the

00:17:07,039 --> 00:17:10,160
metadata from the source of truth like

00:17:08,799 --> 00:17:12,079
high media store

00:17:10,160 --> 00:17:14,079
manual queries can sometimes come with

00:17:12,079 --> 00:17:17,120
less for example like user create

00:17:14,079 --> 00:17:20,799
tags for given data set or

00:17:17,120 --> 00:17:23,199
updated descriptions et cetera

00:17:20,799 --> 00:17:24,959
we found us some of the challenges with

00:17:23,199 --> 00:17:26,240
phase and live is that not every data

00:17:24,959 --> 00:17:28,640
set define

00:17:26,240 --> 00:17:30,000
the same set of metadata so you it's not

00:17:28,640 --> 00:17:32,400
easy to standardize

00:17:30,000 --> 00:17:33,520
this metadata across all the data sets

00:17:32,400 --> 00:17:36,720
for example some

00:17:33,520 --> 00:17:42,080
some data set define xla or

00:17:36,720 --> 00:17:42,080
tier like t0 t1 but others don't

00:17:42,160 --> 00:17:48,640
and user user we think like user has the

00:17:46,400 --> 00:17:49,600
most context and tribal knowledge around

00:17:48,640 --> 00:17:52,720
data sets

00:17:49,600 --> 00:17:55,360
so we index users

00:17:52,720 --> 00:17:57,679
uh dashboard dashboard is represent a

00:17:55,360 --> 00:18:00,720
user study or user analysis

00:17:57,679 --> 00:18:03,039
so it's as you can see after we index

00:18:00,720 --> 00:18:03,919
dashboard the graph could be extend to

00:18:03,039 --> 00:18:07,760
connect

00:18:03,919 --> 00:18:10,880
between table and dashboard as well

00:18:07,760 --> 00:18:13,039
and here are some of the metadatas uh we

00:18:10,880 --> 00:18:15,360
integrate currently like owner

00:18:13,039 --> 00:18:19,440
discourages

00:18:15,360 --> 00:18:19,440
summertime stamp for a given dashboard

00:18:19,679 --> 00:18:26,160
and it pushed at versus pool so

00:18:23,120 --> 00:18:28,320
this defines how we ingest the metadata

00:18:26,160 --> 00:18:29,200
the pro model is like pure they update

00:18:28,320 --> 00:18:33,760
the

00:18:29,200 --> 00:18:36,880
the metadata by pulling the system

00:18:33,760 --> 00:18:38,640
so this is this kind of model is okay if

00:18:36,880 --> 00:18:41,760
you for example you will want to

00:18:38,640 --> 00:18:42,480
uh if you don't need like very near real

00:18:41,760 --> 00:18:45,679
time like

00:18:42,480 --> 00:18:48,960
one minute minutes levels as uh

00:18:45,679 --> 00:18:51,919
refresh sla metadata

00:18:48,960 --> 00:18:52,720
and it's also easy to bootstrap for push

00:18:51,919 --> 00:18:56,000
push model

00:18:52,720 --> 00:18:59,679
it's like if if you define for example

00:18:56,000 --> 00:19:00,400
sdk or connect with a hook the metadata

00:18:59,679 --> 00:19:02,480
will

00:19:00,400 --> 00:19:03,760
once refresh you will push to a message

00:19:02,480 --> 00:19:07,039
queue and

00:19:03,760 --> 00:19:09,039
you have a uh for example cop card

00:19:07,039 --> 00:19:11,280
consumer to connect this message to and

00:19:09,039 --> 00:19:13,360
persist the information into a graph

00:19:11,280 --> 00:19:15,520
that's preferred if you have a large

00:19:13,360 --> 00:19:17,440
volume of uh

00:19:15,520 --> 00:19:19,200
large volume or you will acquire near

00:19:17,440 --> 00:19:21,360
real-time indexing

00:19:19,200 --> 00:19:22,559
this this also requires if you have a

00:19:21,360 --> 00:19:26,799
clear

00:19:22,559 --> 00:19:30,000
interface it exists across the board

00:19:26,799 --> 00:19:32,720
so currently neo4 if you come with uh

00:19:30,000 --> 00:19:35,840
using neo4j or neptune as back and store

00:19:32,720 --> 00:19:38,400
we default use a pool model but it's

00:19:35,840 --> 00:19:41,520
easy to extend to a push and pull model

00:19:38,400 --> 00:19:45,200
because i live we have an

00:19:41,520 --> 00:19:45,760
internal version which is an external

00:19:45,200 --> 00:19:48,320
service

00:19:45,760 --> 00:19:50,840
will push some of the metadata to kafka

00:19:48,320 --> 00:19:52,320
and you can leverage the data builder

00:19:50,840 --> 00:19:54,880
consume

00:19:52,320 --> 00:19:57,679
consume the kafka topic and then persist

00:19:54,880 --> 00:19:57,679
to neo4j

00:19:57,840 --> 00:20:03,360
if you're using alice as a backend that

00:20:00,480 --> 00:20:05,360
is using the default using push module

00:20:03,360 --> 00:20:06,720
the downside of alice is that it doesn't

00:20:05,360 --> 00:20:10,000
support like external

00:20:06,720 --> 00:20:12,080
use source like ratio if the

00:20:10,000 --> 00:20:14,640
the excel source doesn't support

00:20:12,080 --> 00:20:16,960
interface

00:20:14,640 --> 00:20:18,640
uh just a quick time check uh if you

00:20:16,960 --> 00:20:20,240
could wrap up in a minute or two that'll

00:20:18,640 --> 00:20:24,000
be good we can answer some questions

00:20:20,240 --> 00:20:25,200
and that okay okay i guess i could skip

00:20:24,000 --> 00:20:28,480
this one

00:20:25,200 --> 00:20:31,840
so uh let me talk a bit about the impact

00:20:28,480 --> 00:20:32,559
and the future books so i live with like

00:20:31,840 --> 00:20:36,799
we

00:20:32,559 --> 00:20:39,840
we index many uh we have a high wau and

00:20:36,799 --> 00:20:41,760
index of what more than 150k of tables

00:20:39,840 --> 00:20:42,880
and our community in slack is have more

00:20:41,760 --> 00:20:45,840
than one thousand

00:20:42,880 --> 00:20:46,320
and a more than 25 company has official

00:20:45,840 --> 00:20:49,520
claim

00:20:46,320 --> 00:20:51,360
using productions for future we are

00:20:49,520 --> 00:20:54,240
currently at uh building like

00:20:51,360 --> 00:20:54,960
uh native lineage supports here is some

00:20:54,240 --> 00:20:57,440
of them

00:20:54,960 --> 00:20:58,720
ux mock you will see like we come with

00:20:57,440 --> 00:21:02,960
table image

00:20:58,720 --> 00:21:05,200
as the mvp others is like for example

00:21:02,960 --> 00:21:07,360
servicing ml features as

00:21:05,200 --> 00:21:09,280
uh resource entity as well for detail

00:21:07,360 --> 00:21:12,400
you could see like the road map

00:21:09,280 --> 00:21:16,320
as well rfc so

00:21:12,400 --> 00:21:18,880
any questions

00:21:16,320 --> 00:21:20,640
yes there are a couple of questions so

00:21:18,880 --> 00:21:22,320
um yeah i think we have time for a

00:21:20,640 --> 00:21:25,520
couple before our next talk

00:21:22,320 --> 00:21:28,080
um the uh first question is uh

00:21:25,520 --> 00:21:29,840
presto can connect to a range of

00:21:28,080 --> 00:21:30,640
different catalogs as well and data

00:21:29,840 --> 00:21:33,760
sources

00:21:30,640 --> 00:21:37,039
uh and so presto can connect to mysql

00:21:33,760 --> 00:21:38,640
um you know postgres data lakes amazon

00:21:37,039 --> 00:21:40,159
obviously does the same as well and

00:21:38,640 --> 00:21:43,200
connects to presto

00:21:40,159 --> 00:21:45,280
when how does it what is the recommended

00:21:43,200 --> 00:21:46,080
approach do you connect directly to a

00:21:45,280 --> 00:21:48,480
mudsin

00:21:46,080 --> 00:21:49,919
uh or do you connect to uh presto and

00:21:48,480 --> 00:21:53,840
use the catalogs that are

00:21:49,919 --> 00:21:56,320
attached uh

00:21:53,840 --> 00:21:58,080
i mean depends on your use case are you

00:21:56,320 --> 00:22:01,200
talking about you're running a

00:21:58,080 --> 00:22:03,280
uh query uh exploring

00:22:01,200 --> 00:22:05,600
uh then you will use like for example

00:22:03,280 --> 00:22:08,240
presto and then connect to different

00:22:05,600 --> 00:22:09,360
other catalog like but you will want to

00:22:08,240 --> 00:22:11,760
index

00:22:09,360 --> 00:22:13,679
the metadata then you could leverage

00:22:11,760 --> 00:22:16,640
like the default like

00:22:13,679 --> 00:22:17,520
if you want to use amazon for data uh

00:22:16,640 --> 00:22:19,120
metadata in

00:22:17,520 --> 00:22:20,640
indexing then you could use like the

00:22:19,120 --> 00:22:23,039
default data builder

00:22:20,640 --> 00:22:23,840
uh injection library that offered by

00:22:23,039 --> 00:22:25,679
amazon

00:22:23,840 --> 00:22:27,280
so we come with like for example even

00:22:25,679 --> 00:22:30,240
like presto connector

00:22:27,280 --> 00:22:33,200
presto extractor mysql extractor as well

00:22:30,240 --> 00:22:36,480
that could automatically

00:22:33,200 --> 00:22:36,480
ingest those metadata

00:22:36,799 --> 00:22:43,840
got it thank you another question

00:22:40,240 --> 00:22:44,480
is um around the etl jobs uh so the

00:22:43,840 --> 00:22:47,360
question is

00:22:44,480 --> 00:22:49,360
for the etl jobs that are being run uh

00:22:47,360 --> 00:22:51,679
how complex are these jobs and do you

00:22:49,360 --> 00:22:54,720
have any metrics that can help

00:22:51,679 --> 00:22:57,679
understand how much how much time

00:22:54,720 --> 00:22:59,679
uh it takes to process this and how it's

00:22:57,679 --> 00:23:03,039
related with uh

00:22:59,679 --> 00:23:03,039
with a mudsin

00:23:03,600 --> 00:23:09,120
uh yeah so so uh

00:23:06,640 --> 00:23:10,240
so each metadata job is just like python

00:23:09,120 --> 00:23:12,320
callable

00:23:10,240 --> 00:23:13,679
so you could make it a multi-process a

00:23:12,320 --> 00:23:16,559
single process

00:23:13,679 --> 00:23:18,240
and then it depends the time to run it

00:23:16,559 --> 00:23:20,799
really depends on how much

00:23:18,240 --> 00:23:23,039
data you have in the back if you for

00:23:20,799 --> 00:23:27,120
example have like 1 000 table that off

00:23:23,039 --> 00:23:29,200
of course you will be pretty um uh

00:23:27,120 --> 00:23:31,200
pretty quick but you will have a very

00:23:29,200 --> 00:23:33,760
large volume and metadata and you

00:23:31,200 --> 00:23:35,360
do it sequentially that will be take

00:23:33,760 --> 00:23:37,840
like uh maybe

00:23:35,360 --> 00:23:40,000
hours for example one or two hours so

00:23:37,840 --> 00:23:41,440
you really depends and you could

00:23:40,000 --> 00:23:43,120
that's also one of the nice things for

00:23:41,440 --> 00:23:45,360
airflow other orchestration you could

00:23:43,120 --> 00:23:47,440
paralyze some of metadata in parallel

00:23:45,360 --> 00:23:48,799
leveraging the workflow orchestration so

00:23:47,440 --> 00:23:52,080
i live we say

00:23:48,799 --> 00:23:52,559
we don't really have a high uh we really

00:23:52,080 --> 00:23:55,200
don't

00:23:52,559 --> 00:23:56,480
we mostly have a 12 hour sla for

00:23:55,200 --> 00:23:58,960
metadata refresh so

00:23:56,480 --> 00:24:02,240
we oftentimes just run the metadata job

00:23:58,960 --> 00:24:02,240

YouTube URL: https://www.youtube.com/watch?v=EdH-mT23wd0


