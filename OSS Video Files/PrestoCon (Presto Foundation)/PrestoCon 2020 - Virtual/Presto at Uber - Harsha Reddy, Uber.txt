Title: Presto at Uber - Harsha Reddy, Uber
Publication date: 2020-09-30
Playlist: PrestoCon 2020 - Virtual
Description: 
	Presto at Uber - Harsha Reddy, Uber

Speakers: Harsha Reddy

This talk will cover:
- Overall Presto ecosystem at Uber
- Micro-service ecosystem supporting Presto-Proxy, Automation framework and others
- Presto for ETL
- Ongoing projects and Roadmap
Captions: 
	00:00:00,560 --> 00:00:04,000
good morning everyone my name is harsha

00:00:02,480 --> 00:00:04,880
reddy and today we're going to talk

00:00:04,000 --> 00:00:08,480
about

00:00:04,880 --> 00:00:08,480
presto etls at uber

00:00:08,720 --> 00:00:12,559
let's start off with the mission

00:00:09,840 --> 00:00:13,519
statement uber's mission is to ignite

00:00:12,559 --> 00:00:17,119
opportunity

00:00:13,519 --> 00:00:20,240
by setting the world in motion

00:00:17,119 --> 00:00:21,359
we operate in more than 10 000 cities we

00:00:20,240 --> 00:00:24,720
have more than 100 million

00:00:21,359 --> 00:00:26,960
active users and we do about 18 million

00:00:24,720 --> 00:00:29,920
trips a day

00:00:26,960 --> 00:00:32,480
data informs every decision at uber let

00:00:29,920 --> 00:00:36,320
it be community operations

00:00:32,480 --> 00:00:39,760
marketplace pricing eats

00:00:36,320 --> 00:00:42,640
compliance growth marketing

00:00:39,760 --> 00:00:42,640
or data science

00:00:43,280 --> 00:00:49,200
let's look at presto scale at uber

00:00:47,039 --> 00:00:50,320
we have around 12 000 monthly active

00:00:49,200 --> 00:00:53,600
users

00:00:50,320 --> 00:00:56,399
who run about 400 000 queries per day

00:00:53,600 --> 00:00:57,120
and operate on more than 35 terabytes of

00:00:56,399 --> 00:01:00,160
data

00:00:57,120 --> 00:01:02,640
every day and now

00:01:00,160 --> 00:01:03,840
a glimpse into the footprint we have two

00:01:02,640 --> 00:01:07,840
data centers

00:01:03,840 --> 00:01:11,439
with 3000 nodes spread across 12

00:01:07,840 --> 00:01:13,119
clusters in this section

00:01:11,439 --> 00:01:14,560
i'm going to do a deep dive into the

00:01:13,119 --> 00:01:16,640
presto ecosystem

00:01:14,560 --> 00:01:18,159
which is the ecosystem with the

00:01:16,640 --> 00:01:19,360
microservices and the infrastructure

00:01:18,159 --> 00:01:22,560
components

00:01:19,360 --> 00:01:22,560
for etls

00:01:25,119 --> 00:01:29,759
we have two types of deployments one is

00:01:27,360 --> 00:01:32,000
neutrino and the other one is photon

00:01:29,759 --> 00:01:33,200
neutrino is presto for real-time

00:01:32,000 --> 00:01:36,799
analytics

00:01:33,200 --> 00:01:40,159
the storage for neutrino is pino

00:01:36,799 --> 00:01:41,040
and the other one is photon photon is a

00:01:40,159 --> 00:01:43,680
presto for

00:01:41,040 --> 00:01:45,040
interactive and batch analytics and the

00:01:43,680 --> 00:01:48,960
storage for photon

00:01:45,040 --> 00:01:51,360
is predominantly hdfs we have

00:01:48,960 --> 00:01:53,280
uh orchestration system which is odin

00:01:51,360 --> 00:01:56,399
which does the cluster orchestration

00:01:53,280 --> 00:01:56,399
for photon

00:01:57,920 --> 00:02:01,200
data central is our admin console

00:02:00,560 --> 00:02:04,079
wherein

00:02:01,200 --> 00:02:05,280
we get a bird's eye view of our entire

00:02:04,079 --> 00:02:07,439
fleet

00:02:05,280 --> 00:02:09,759
so if you look at these tabs each of

00:02:07,439 --> 00:02:11,440
them represent a cluster

00:02:09,759 --> 00:02:13,040
interactive clusters batch clusters and

00:02:11,440 --> 00:02:15,440
custom clusters

00:02:13,040 --> 00:02:16,959
and some of this information is masked

00:02:15,440 --> 00:02:18,879
it is intentionally masked

00:02:16,959 --> 00:02:20,640
but if you see we have the state of the

00:02:18,879 --> 00:02:22,319
cluster like the running queries the

00:02:20,640 --> 00:02:25,120
cube queries

00:02:22,319 --> 00:02:26,160
the active workers and drivers and so on

00:02:25,120 --> 00:02:28,239
and once we click

00:02:26,160 --> 00:02:31,280
into one of these clusters it takes us

00:02:28,239 --> 00:02:33,680
into a customized coordinated ui

00:02:31,280 --> 00:02:35,280
which gives us the queries the actual

00:02:33,680 --> 00:02:38,400
queries that are running

00:02:35,280 --> 00:02:40,720
um who's running those queries and

00:02:38,400 --> 00:02:41,840
what the resource utilization looks like

00:02:40,720 --> 00:02:44,640
for those queries

00:02:41,840 --> 00:02:46,080
like the memory cpu utilization and what

00:02:44,640 --> 00:02:50,400
the source from where they're coming and

00:02:46,080 --> 00:02:54,080
so on

00:02:50,400 --> 00:02:56,080
prism is gateway to presto at vancouver

00:02:54,080 --> 00:02:58,560
uh to rephrase that better it is the

00:02:56,080 --> 00:03:01,040
only gateway to presto at uber

00:02:58,560 --> 00:03:02,000
what that means is we don't encourage

00:03:01,040 --> 00:03:04,480
presto cli

00:03:02,000 --> 00:03:06,000
access jdbc access or any other means of

00:03:04,480 --> 00:03:08,879
access

00:03:06,000 --> 00:03:09,760
prism does query routing which is

00:03:08,879 --> 00:03:12,080
routing

00:03:09,760 --> 00:03:14,400
uh queries to different clusters based

00:03:12,080 --> 00:03:16,400
on specific rules

00:03:14,400 --> 00:03:18,239
it does load balancing like the

00:03:16,400 --> 00:03:19,120
concurrency management so that none of

00:03:18,239 --> 00:03:22,879
our clusters

00:03:19,120 --> 00:03:26,480
are overburdened it does query gating

00:03:22,879 --> 00:03:28,400
which is like blocking um bad users who

00:03:26,480 --> 00:03:31,040
can potentially bring down the cluster

00:03:28,400 --> 00:03:34,720
or bad queries which can have like uh

00:03:31,040 --> 00:03:34,720
impact on our cluster resources

00:03:35,360 --> 00:03:41,920
uwork is our etl orchestration platform

00:03:39,040 --> 00:03:43,599
it is based on an open source project

00:03:41,920 --> 00:03:46,080
called streamline

00:03:43,599 --> 00:03:47,120
so you work is basically a ui based

00:03:46,080 --> 00:03:49,360
self-service

00:03:47,120 --> 00:03:51,360
platform where users can come in and

00:03:49,360 --> 00:03:53,439
author deploy and monitor their

00:03:51,360 --> 00:03:55,360
workflows it's not just presto it even

00:03:53,439 --> 00:03:58,959
serves like every other

00:03:55,360 --> 00:03:58,959
engine at uber

00:03:59,519 --> 00:04:05,040
modulator is presto's uh infra

00:04:02,159 --> 00:04:08,159
management and automation service

00:04:05,040 --> 00:04:10,080
um one quick point here is

00:04:08,159 --> 00:04:11,680
more than half of the company uses

00:04:10,080 --> 00:04:15,120
presto on a daily basis to do

00:04:11,680 --> 00:04:17,519
to do their job so what that means is

00:04:15,120 --> 00:04:18,799
if we have a bad deployment the there is

00:04:17,519 --> 00:04:20,479
going to be a significant business

00:04:18,799 --> 00:04:23,120
impact

00:04:20,479 --> 00:04:24,080
uh so we there's a need for doing a

00:04:23,120 --> 00:04:26,400
sophisticated

00:04:24,080 --> 00:04:27,280
amount of testing before we do any

00:04:26,400 --> 00:04:28,800
change

00:04:27,280 --> 00:04:30,320
and at the same time not compromise on

00:04:28,800 --> 00:04:31,040
the velocity of our deployments because

00:04:30,320 --> 00:04:33,680
the changes

00:04:31,040 --> 00:04:34,800
in anywhere in in our ecosystem whether

00:04:33,680 --> 00:04:38,400
it be presto

00:04:34,800 --> 00:04:43,199
or our micro services uh could impact

00:04:38,400 --> 00:04:47,040
the depressor status or health

00:04:43,199 --> 00:04:49,040
so we do shadow tests uh with modulator

00:04:47,040 --> 00:04:50,560
we do verification and integration tests

00:04:49,040 --> 00:04:51,600
which are two different types of tests

00:04:50,560 --> 00:04:54,479
integration with

00:04:51,600 --> 00:04:55,600
the other micro services and presto and

00:04:54,479 --> 00:04:58,080
verification test

00:04:55,600 --> 00:05:00,000
is like um an alternative a customized

00:04:58,080 --> 00:05:01,440
alternative to the open source presto

00:05:00,000 --> 00:05:03,280
verifier

00:05:01,440 --> 00:05:05,919
we also have intro management and

00:05:03,280 --> 00:05:08,720
automations in modulator which are like

00:05:05,919 --> 00:05:09,520
doing rolling cluster restarts uh doing

00:05:08,720 --> 00:05:12,800
automated

00:05:09,520 --> 00:05:14,560
uh clustered deployments um identifying

00:05:12,800 --> 00:05:16,800
slow workers and all those kinds of

00:05:14,560 --> 00:05:16,800
things

00:05:18,479 --> 00:05:23,600
in this section uh i'm going to give and

00:05:21,280 --> 00:05:26,800
give a high level overview of

00:05:23,600 --> 00:05:29,919
the etls at uber and how it's been going

00:05:26,800 --> 00:05:29,919
on the last couple of months

00:05:30,720 --> 00:05:34,000
why do we need to do etl with presto in

00:05:32,800 --> 00:05:36,720
the first place

00:05:34,000 --> 00:05:38,639
it's a common question that comes up and

00:05:36,720 --> 00:05:41,440
one key reason is that we can unify

00:05:38,639 --> 00:05:42,639
all sql today we already do real-time

00:05:41,440 --> 00:05:44,400
analytics

00:05:42,639 --> 00:05:47,120
we do batch and interactive analytics

00:05:44,400 --> 00:05:49,840
with presto and for etl users go ahead

00:05:47,120 --> 00:05:50,800
and they use a different type of sql we

00:05:49,840 --> 00:05:52,240
could avoid that

00:05:50,800 --> 00:05:55,120
it could be it could add a lot of

00:05:52,240 --> 00:05:57,759
convenience and more importantly

00:05:55,120 --> 00:05:58,880
it is it results in faster development

00:05:57,759 --> 00:06:00,960
life cycle

00:05:58,880 --> 00:06:02,240
because the p90 latencies for presto are

00:06:00,960 --> 00:06:06,000
much lower

00:06:02,240 --> 00:06:09,759
than high and it results in a faster

00:06:06,000 --> 00:06:10,240
run debug cycle when i say etl it's a

00:06:09,759 --> 00:06:12,319
pretty

00:06:10,240 --> 00:06:14,240
broad term so when you say etl i'm

00:06:12,319 --> 00:06:14,960
talking about the self-serve tools that

00:06:14,240 --> 00:06:18,639
we provide

00:06:14,960 --> 00:06:21,360
for creating uh setting up the pipelines

00:06:18,639 --> 00:06:22,319
for the schema evolution supporting

00:06:21,360 --> 00:06:24,479
backfills

00:06:22,319 --> 00:06:26,160
and we support only those queries that

00:06:24,479 --> 00:06:27,759
fit in memory

00:06:26,160 --> 00:06:29,360
and we have full support which is

00:06:27,759 --> 00:06:31,600
support for security and compliance

00:06:29,360 --> 00:06:33,680
related initiatives

00:06:31,600 --> 00:06:35,039
so basically uh when users come to you

00:06:33,680 --> 00:06:37,520
work they just

00:06:35,039 --> 00:06:38,960
select a custom template for presto and

00:06:37,520 --> 00:06:41,759
most of the information

00:06:38,960 --> 00:06:43,039
with the recommended properties of the

00:06:41,759 --> 00:06:44,720
session properties are already

00:06:43,039 --> 00:06:45,759
pre-filled the user just needs to change

00:06:44,720 --> 00:06:49,840
their queries

00:06:45,759 --> 00:06:49,840
and set that detail pipeline

00:06:51,280 --> 00:06:55,440
so when i say faster development life

00:06:52,880 --> 00:06:58,080
cycle what happens today is

00:06:55,440 --> 00:07:00,160
users start at presto they do their

00:06:58,080 --> 00:07:03,520
analytics in presto first

00:07:00,160 --> 00:07:04,880
and um after a point they decide that uh

00:07:03,520 --> 00:07:07,039
it's uh it's something that they want to

00:07:04,880 --> 00:07:07,840
materialize they convert their presto

00:07:07,039 --> 00:07:10,720
sql

00:07:07,840 --> 00:07:12,639
into hive sql they go back to hive and

00:07:10,720 --> 00:07:13,919
v-line they do their testing

00:07:12,639 --> 00:07:16,400
and then they go back to eta

00:07:13,919 --> 00:07:18,880
orchestration workflow system and ingest

00:07:16,400 --> 00:07:20,240
and start the injection and then once

00:07:18,880 --> 00:07:21,360
the data is ingested and materialized

00:07:20,240 --> 00:07:22,400
they can come back to custodial to use

00:07:21,360 --> 00:07:24,639
the same table

00:07:22,400 --> 00:07:26,160
right they again come back there so what

00:07:24,639 --> 00:07:26,880
we are proposing or what we're doing

00:07:26,160 --> 00:07:29,360
right now

00:07:26,880 --> 00:07:30,479
uh is that it's pretty simple just

00:07:29,360 --> 00:07:33,440
eliminating hive from

00:07:30,479 --> 00:07:34,639
picture we just do analytics and etl

00:07:33,440 --> 00:07:36,880
using presto

00:07:34,639 --> 00:07:39,039
and uh they directly configure their

00:07:36,880 --> 00:07:40,720
pipelines using the orchestration system

00:07:39,039 --> 00:07:42,080
and they don't need to go to hive for

00:07:40,720 --> 00:07:45,120
any case so

00:07:42,080 --> 00:07:47,599
in this new workflow is going to result

00:07:45,120 --> 00:07:49,039
in an improved developer velocity

00:07:47,599 --> 00:07:51,199
but this this particular model is

00:07:49,039 --> 00:07:52,560
suitable only for some use cases it's

00:07:51,199 --> 00:07:53,759
not going to be suitable for every use

00:07:52,560 --> 00:07:56,879
case where we can eliminate

00:07:53,759 --> 00:07:59,840
high or batch processing systems

00:07:56,879 --> 00:07:59,840
as such

00:08:00,639 --> 00:08:04,560
so one of those use cases where it can

00:08:02,319 --> 00:08:06,240
be replaced is this one click edl

00:08:04,560 --> 00:08:08,479
workflow which is an ideal workflow that

00:08:06,240 --> 00:08:11,199
we're trying to come up with

00:08:08,479 --> 00:08:12,560
today what happens is analytics users in

00:08:11,199 --> 00:08:14,479
the production environment that is the

00:08:12,560 --> 00:08:17,039
top half of the stack room

00:08:14,479 --> 00:08:18,800
analytics users come to query builder

00:08:17,039 --> 00:08:21,520
which is like an interface to run

00:08:18,800 --> 00:08:22,960
sql they run their queries and at a

00:08:21,520 --> 00:08:24,240
point they decide to convert into a

00:08:22,960 --> 00:08:26,080
pipeline

00:08:24,240 --> 00:08:27,759
and then there is an integration from

00:08:26,080 --> 00:08:29,680
query printer to uwork

00:08:27,759 --> 00:08:31,599
where they convert their doc analysis to

00:08:29,680 --> 00:08:34,800
ethel pipelines

00:08:31,599 --> 00:08:37,279
and um they set up a custom pipeline

00:08:34,800 --> 00:08:38,719
in your work but this one click etl

00:08:37,279 --> 00:08:40,800
workflow proposal is that

00:08:38,719 --> 00:08:42,320
they don't even need to copy paste the

00:08:40,800 --> 00:08:42,959
queries into u-work or set up any

00:08:42,320 --> 00:08:46,160
pipelines

00:08:42,959 --> 00:08:49,519
rather they just select

00:08:46,160 --> 00:08:52,320
an option in the user interface layer

00:08:49,519 --> 00:08:55,760
to say that they just want a pipeline

00:08:52,320 --> 00:08:58,399
and just provide the url the query url

00:08:55,760 --> 00:08:58,800
and the rest of the boilerplate code

00:08:58,399 --> 00:09:01,279
like

00:08:58,800 --> 00:09:02,880
um inserts creates partition and all

00:09:01,279 --> 00:09:03,440
those kinds of statements are added by

00:09:02,880 --> 00:09:06,800
us

00:09:03,440 --> 00:09:09,920
based on some rules and and

00:09:06,800 --> 00:09:12,640
they have they had they get a pipeline

00:09:09,920 --> 00:09:13,360
and the output table generated for them

00:09:12,640 --> 00:09:16,399
this adds

00:09:13,360 --> 00:09:18,240
a lot of convenience uh to the users and

00:09:16,399 --> 00:09:19,360
definitely improves their productivity

00:09:18,240 --> 00:09:22,080
to a great extent

00:09:19,360 --> 00:09:23,920
and it also improves presto because once

00:09:22,080 --> 00:09:25,680
the data sets are materialized

00:09:23,920 --> 00:09:27,279
uh the cost to run queries on the

00:09:25,680 --> 00:09:27,920
metallized data set is much lower than

00:09:27,279 --> 00:09:31,360
running it

00:09:27,920 --> 00:09:31,360
on the raw data sets

00:09:32,160 --> 00:09:36,160
um you see that prism redirects cluster

00:09:34,560 --> 00:09:37,120
like queries appropriately

00:09:36,160 --> 00:09:38,320
and here's a glimpse of our

00:09:37,120 --> 00:09:39,440
non-production environment that

00:09:38,320 --> 00:09:41,760
modulated is

00:09:39,440 --> 00:09:43,600
uh replaying traffic from production

00:09:41,760 --> 00:09:46,800
onto staging or different uh

00:09:43,600 --> 00:09:48,399
clusters by using a kafka consumer

00:09:46,800 --> 00:09:52,240
which listens to all the queries that

00:09:48,399 --> 00:09:52,240
are running on our production clusters

00:09:52,399 --> 00:09:55,519
so the kinds of use cases that that

00:09:54,880 --> 00:09:57,760
we're doing

00:09:55,519 --> 00:09:59,040
for presto etl are the one click equal

00:09:57,760 --> 00:10:01,839
kinds of use cases

00:09:59,040 --> 00:10:04,079
today it's not that yet but people are

00:10:01,839 --> 00:10:06,480
doing some copy pasting into u-work

00:10:04,079 --> 00:10:07,760
and then the python it's also self-serve

00:10:06,480 --> 00:10:08,560
where they're able to create their

00:10:07,760 --> 00:10:09,680
pipelines

00:10:08,560 --> 00:10:11,920
but we're going to be trying to

00:10:09,680 --> 00:10:14,240
eliminate even that part

00:10:11,920 --> 00:10:15,760
and the business pipelines is the usual

00:10:14,240 --> 00:10:17,279
use case wherein the data engineering

00:10:15,760 --> 00:10:18,320
pipelines for specific business needs

00:10:17,279 --> 00:10:22,560
are being created

00:10:18,320 --> 00:10:24,720
directly in the um injection system

00:10:22,560 --> 00:10:25,839
we want to replace legacy system related

00:10:24,720 --> 00:10:28,160
etls like

00:10:25,839 --> 00:10:29,040
doing some google sheet extracts and

00:10:28,160 --> 00:10:31,839
doing

00:10:29,040 --> 00:10:34,320
vertica ets with presto etl wherever it

00:10:31,839 --> 00:10:36,720
can be applied

00:10:34,320 --> 00:10:37,440
data sampling is like a project where we

00:10:36,720 --> 00:10:41,279
want to create

00:10:37,440 --> 00:10:42,880
one percent sample data sets um

00:10:41,279 --> 00:10:44,320
without actually having the need to run

00:10:42,880 --> 00:10:46,320
analytics on a huge data set

00:10:44,320 --> 00:10:48,240
users could run analytics on this sample

00:10:46,320 --> 00:10:51,279
version

00:10:48,240 --> 00:10:53,600
the next use case is when users want to

00:10:51,279 --> 00:10:56,800
load data from hdfs to pino

00:10:53,600 --> 00:10:58,079
what happens is users write a presto sql

00:10:56,800 --> 00:11:01,360
query

00:10:58,079 --> 00:11:03,839
it gets written as an avro file

00:11:01,360 --> 00:11:06,320
and from avro uh the presto lo the pino

00:11:03,839 --> 00:11:08,959
load takes off

00:11:06,320 --> 00:11:09,360
the last use case is our favorite where

00:11:08,959 --> 00:11:12,079
um

00:11:09,360 --> 00:11:13,519
it's a platform driven use cases like uh

00:11:12,079 --> 00:11:15,200
there are some users who run

00:11:13,519 --> 00:11:16,959
who try to stream these large result

00:11:15,200 --> 00:11:18,880
sets or who run long-running attack

00:11:16,959 --> 00:11:21,120
queries with a huge partition

00:11:18,880 --> 00:11:23,200
range and so on right which are

00:11:21,120 --> 00:11:25,200
essentially with which queries

00:11:23,200 --> 00:11:27,600
are supposed to be etl queries but are

00:11:25,200 --> 00:11:29,440
being run um as adult queries

00:11:27,600 --> 00:11:31,120
taking up too many resources so we are

00:11:29,440 --> 00:11:34,640
pushing those queries to

00:11:31,120 --> 00:11:34,640
um to be materialized

00:11:35,360 --> 00:11:42,240
we did a launch uh last july and the

00:11:38,959 --> 00:11:46,000
adoption has been phenomenal so um

00:11:42,240 --> 00:11:48,560
our users uh went from around 200

00:11:46,000 --> 00:11:49,680
to um to about to more than 700 in just

00:11:48,560 --> 00:11:51,440
two months

00:11:49,680 --> 00:11:52,800
so there's like a very good demand for

00:11:51,440 --> 00:11:54,959
presta etl right now

00:11:52,800 --> 00:11:56,240
and users want to get that convenience

00:11:54,959 --> 00:11:59,519
factor added into

00:11:56,240 --> 00:11:59,519
that daily jobs

00:12:00,079 --> 00:12:06,240
the current state of etl is that we run

00:12:02,320 --> 00:12:09,839
around 250 000 queries per week

00:12:06,240 --> 00:12:11,680
we have around 700 etl pipelines which

00:12:09,839 --> 00:12:14,399
includes all those use cases

00:12:11,680 --> 00:12:15,279
uh that we discussed earlier the user

00:12:14,399 --> 00:12:18,160
adoption

00:12:15,279 --> 00:12:20,800
is more than 300 percentage uh just in

00:12:18,160 --> 00:12:20,800
beta phase

00:12:21,200 --> 00:12:26,560
and we have about 280

00:12:24,639 --> 00:12:28,079
presto park pipelines which is like the

00:12:26,560 --> 00:12:29,360
parking rights itself

00:12:28,079 --> 00:12:31,200
not everything that is doing parque

00:12:29,360 --> 00:12:33,040
right in the overall etls it's doing

00:12:31,200 --> 00:12:34,880
algorithms doing it right into extracts

00:12:33,040 --> 00:12:37,680
and so on

00:12:34,880 --> 00:12:38,000
so uh the parquet rights leads us to uh

00:12:37,680 --> 00:12:41,440
our

00:12:38,000 --> 00:12:44,959
next topic which is parque

00:12:41,440 --> 00:12:47,920
right so parque um is widely used at

00:12:44,959 --> 00:12:48,639
uber it is a columnar storage format

00:12:47,920 --> 00:12:51,360
it's

00:12:48,639 --> 00:12:53,120
pretty good with nested data types got

00:12:51,360 --> 00:12:57,279
very good support for it

00:12:53,120 --> 00:12:58,720
uh the data is stored as uh rogue groups

00:12:57,279 --> 00:13:00,639
which are like horizontal partitions of

00:12:58,720 --> 00:13:02,399
rows inside these row groups

00:13:00,639 --> 00:13:04,000
there are column chunks for each of the

00:13:02,399 --> 00:13:05,200
column in the data set

00:13:04,000 --> 00:13:08,399
and the column chunks are further

00:13:05,200 --> 00:13:11,839
divided into pages

00:13:08,399 --> 00:13:14,000
so um for party writers we have

00:13:11,839 --> 00:13:15,680
like two writers at the moment one is

00:13:14,000 --> 00:13:17,279
the parquet record writer which is the

00:13:15,680 --> 00:13:18,959
default option

00:13:17,279 --> 00:13:20,959
and the other one is the native parque

00:13:18,959 --> 00:13:23,600
writer which was recently

00:13:20,959 --> 00:13:24,800
released in open source community and

00:13:23,600 --> 00:13:26,639
requests for the author of the pull

00:13:24,800 --> 00:13:28,959
request

00:13:26,639 --> 00:13:31,200
native party writer is definitely faster

00:13:28,959 --> 00:13:33,360
and it's better

00:13:31,200 --> 00:13:34,480
but 4k record writer also has full

00:13:33,360 --> 00:13:36,560
feature support

00:13:34,480 --> 00:13:39,440
it works as expected it's just that it's

00:13:36,560 --> 00:13:41,519
not resource efficient

00:13:39,440 --> 00:13:42,959
so for a parking record writer what it

00:13:41,519 --> 00:13:44,320
tries to do is

00:13:42,959 --> 00:13:46,560
the default option what it tries to do

00:13:44,320 --> 00:13:48,800
is it tries to convert

00:13:46,560 --> 00:13:51,680
the data types to hide types and tries

00:13:48,800 --> 00:13:53,760
to write like a parque hybrid one

00:13:51,680 --> 00:13:54,959
however the native writer what it does

00:13:53,760 --> 00:13:57,680
is it

00:13:54,959 --> 00:13:58,399
tries to write the parquet structures

00:13:57,680 --> 00:14:02,079
from memory

00:13:58,399 --> 00:14:04,560
directly onto the file

00:14:02,079 --> 00:14:05,839
so we naturally did a comparison of both

00:14:04,560 --> 00:14:09,600
these writers

00:14:05,839 --> 00:14:12,880
to see which one works better and

00:14:09,600 --> 00:14:13,360
in this benchmark on the left side you

00:14:12,880 --> 00:14:16,560
can see

00:14:13,360 --> 00:14:16,959
different types of compressions and on

00:14:16,560 --> 00:14:19,199
top

00:14:16,959 --> 00:14:21,279
we have different types of different

00:14:19,199 --> 00:14:22,880
data types

00:14:21,279 --> 00:14:24,480
so if you see on the left side of the

00:14:22,880 --> 00:14:28,240
spectrum

00:14:24,480 --> 00:14:31,040
the blue indicates the the new writer

00:14:28,240 --> 00:14:32,560
and gray is the old writer so the new

00:14:31,040 --> 00:14:34,480
writer the native writer

00:14:32,560 --> 00:14:36,720
performs better for certain data types

00:14:34,480 --> 00:14:38,000
and certain types of compression

00:14:36,720 --> 00:14:40,240
but if you look at the right side of the

00:14:38,000 --> 00:14:42,959
spectrum the performance

00:14:40,240 --> 00:14:43,760
is pretty comparable they both behave

00:14:42,959 --> 00:14:47,199
exactly

00:14:43,760 --> 00:14:48,800
the same so this benchmark wasn't

00:14:47,199 --> 00:14:51,920
conclusive for us to go ahead

00:14:48,800 --> 00:14:52,240
and put native writer into production so

00:14:51,920 --> 00:14:53,360
we

00:14:52,240 --> 00:14:55,920
went ahead and did some more

00:14:53,360 --> 00:14:59,760
verification and we launched tests

00:14:55,920 --> 00:15:02,000
and what it looks like is

00:14:59,760 --> 00:15:03,360
the native writer results in more than

00:15:02,000 --> 00:15:05,760
29 percentage

00:15:03,360 --> 00:15:07,040
improvement in average cpu time and 32

00:15:05,760 --> 00:15:10,079
percentage improvement

00:15:07,040 --> 00:15:12,639
in uh query latency

00:15:10,079 --> 00:15:13,440
so it's definitely um a huge win for us

00:15:12,639 --> 00:15:16,000
to use the

00:15:13,440 --> 00:15:17,120
native writer but we haven't put this

00:15:16,000 --> 00:15:18,639
into production yet

00:15:17,120 --> 00:15:21,199
because we're working through some

00:15:18,639 --> 00:15:22,240
teething issues with the new writer

00:15:21,199 --> 00:15:25,040
let's say like something like

00:15:22,240 --> 00:15:28,959
compatibility issues with the reader

00:15:25,040 --> 00:15:30,240
um like issues uh with data corruption

00:15:28,959 --> 00:15:33,120
uh which is something we're trying to

00:15:30,240 --> 00:15:35,519
reproduce and like performance tests

00:15:33,120 --> 00:15:37,680
in general to see if we can do some

00:15:35,519 --> 00:15:40,639
level of uh

00:15:37,680 --> 00:15:43,120
like a test summary before we just

00:15:40,639 --> 00:15:43,120
launch it

00:15:44,560 --> 00:15:49,199
the next topic is parque encryption so

00:15:47,759 --> 00:15:51,199
this is very important because the

00:15:49,199 --> 00:15:54,320
writer needs to encrypt the data

00:15:51,199 --> 00:15:55,920
and the reader needs to decrypt the data

00:15:54,320 --> 00:15:58,480
so when i say encryption it is

00:15:55,920 --> 00:16:01,759
encrypting the data at rest

00:15:58,480 --> 00:16:04,560
we um we do column level access control

00:16:01,759 --> 00:16:06,000
uh based on tagging what it means is uh

00:16:04,560 --> 00:16:06,800
tagging is nothing but the level of

00:16:06,000 --> 00:16:09,199
sensitivity

00:16:06,800 --> 00:16:11,839
of pii data whether it's an ssn or like

00:16:09,199 --> 00:16:14,240
a location or so on

00:16:11,839 --> 00:16:14,959
and based on our benchmarks what we see

00:16:14,240 --> 00:16:17,360
is that

00:16:14,959 --> 00:16:18,560
there is a very low encryption and

00:16:17,360 --> 00:16:20,959
decryption overhead

00:16:18,560 --> 00:16:22,560
for encryption the overhead is around

00:16:20,959 --> 00:16:23,839
seven to eight percentage and for

00:16:22,560 --> 00:16:24,880
decryption it's around three to four

00:16:23,839 --> 00:16:26,399
percentage

00:16:24,880 --> 00:16:28,639
uh one thing is we don't enter and

00:16:26,399 --> 00:16:29,759
decrypt all columns they're only the pi

00:16:28,639 --> 00:16:31,199
columns or the sensitive columns that

00:16:29,759 --> 00:16:33,440
need to be encrypted this is a common

00:16:31,199 --> 00:16:36,240
level access control

00:16:33,440 --> 00:16:36,800
so the on on the right side um you could

00:16:36,240 --> 00:16:40,160
see

00:16:36,800 --> 00:16:41,680
that i'll walk through the legend so

00:16:40,160 --> 00:16:43,600
that you guys can follow along

00:16:41,680 --> 00:16:45,120
uh what happens in the right path and in

00:16:43,600 --> 00:16:47,120
the read path

00:16:45,120 --> 00:16:48,639
so ek is the encryption key that is used

00:16:47,120 --> 00:16:50,959
for encrypting the data

00:16:48,639 --> 00:16:51,839
mk is the master key that is used for

00:16:50,959 --> 00:16:54,959
encrypting the

00:16:51,839 --> 00:16:56,720
encryption key and eek is the encrypted

00:16:54,959 --> 00:16:59,920
encryption key that's produced

00:16:56,720 --> 00:17:02,399
after doing that encryption

00:16:59,920 --> 00:17:04,640
so in the right path what happens is a

00:17:02,399 --> 00:17:06,959
plain text encryption key is generated

00:17:04,640 --> 00:17:09,039
that is passed to ranger kms which is

00:17:06,959 --> 00:17:11,120
the key management service

00:17:09,039 --> 00:17:13,199
um the encryption key along with the

00:17:11,120 --> 00:17:14,959
master key metadata is passed when i say

00:17:13,199 --> 00:17:16,079
master key metadata is nothing but the

00:17:14,959 --> 00:17:18,240
master key name

00:17:16,079 --> 00:17:21,120
or the identifier of the master key no

00:17:18,240 --> 00:17:24,000
it's not the key itself

00:17:21,120 --> 00:17:25,679
ranger kms has the actual master key so

00:17:24,000 --> 00:17:27,760
based on the identifier it gets and the

00:17:25,679 --> 00:17:29,919
key that it gets it encrypts the

00:17:27,760 --> 00:17:32,240
encryption key with the master key

00:17:29,919 --> 00:17:35,360
generates an encrypted encryption key

00:17:32,240 --> 00:17:37,520
and sends it back to the application and

00:17:35,360 --> 00:17:38,559
this encrypted encryption key is stored

00:17:37,520 --> 00:17:41,760
along with the data

00:17:38,559 --> 00:17:44,400
in the file now

00:17:41,760 --> 00:17:45,600
when read happens the encrypted

00:17:44,400 --> 00:17:46,160
encryption key is already there in the

00:17:45,600 --> 00:17:49,200
file

00:17:46,160 --> 00:17:52,480
so um it's sent to

00:17:49,200 --> 00:17:55,039
ranger kms and uh

00:17:52,480 --> 00:17:55,679
using the master key it pre decrypts the

00:17:55,039 --> 00:17:58,400
um

00:17:55,679 --> 00:17:59,200
the key and sends it back to the

00:17:58,400 --> 00:18:01,280
application

00:17:59,200 --> 00:18:02,559
with the plain text encryption key which

00:18:01,280 --> 00:18:06,240
it uses to

00:18:02,559 --> 00:18:06,799
decrypt and read the data so credits to

00:18:06,240 --> 00:18:11,760
the

00:18:06,799 --> 00:18:11,760
encryption contribution in open source

00:18:12,840 --> 00:18:16,799
here

00:18:15,120 --> 00:18:18,640
um in this section i'm just going to

00:18:16,799 --> 00:18:20,160
give a glimpse of

00:18:18,640 --> 00:18:22,720
the ongoing initiatives all the cool

00:18:20,160 --> 00:18:25,200
initiatives that the team is working on

00:18:22,720 --> 00:18:26,640
the first one is data sampling as i

00:18:25,200 --> 00:18:28,720
briefly covered earlier

00:18:26,640 --> 00:18:29,679
it's like creating sample version of

00:18:28,720 --> 00:18:32,000
data sets

00:18:29,679 --> 00:18:33,280
but the the cool fact about this is that

00:18:32,000 --> 00:18:36,000
the users

00:18:33,280 --> 00:18:36,880
don't need to know what the sample table

00:18:36,000 --> 00:18:38,640
name is

00:18:36,880 --> 00:18:40,240
they just have to check a check box

00:18:38,640 --> 00:18:43,600
saying that i want to

00:18:40,240 --> 00:18:46,160
analyze sampled versions right and

00:18:43,600 --> 00:18:46,960
we do the replacement of the sample

00:18:46,160 --> 00:18:49,039
table

00:18:46,960 --> 00:18:50,799
the actual table with the sample table

00:18:49,039 --> 00:18:53,840
so that their analytics happen

00:18:50,799 --> 00:18:55,679
like uh much faster right

00:18:53,840 --> 00:18:57,280
uh if they were if they want to create a

00:18:55,679 --> 00:18:59,360
much bigger table um

00:18:57,280 --> 00:19:00,640
instead of running it up on a sampler

00:18:59,360 --> 00:19:02,240
sample table

00:19:00,640 --> 00:19:04,480
uh their analytics time would take like

00:19:02,240 --> 00:19:06,400
probably uh more than 20-30 minutes

00:19:04,480 --> 00:19:08,720
with sampling it would come down to

00:19:06,400 --> 00:19:10,240
about a minute

00:19:08,720 --> 00:19:12,720
we're working on parquet reader

00:19:10,240 --> 00:19:13,200
optimizations uh which is to reduce the

00:19:12,720 --> 00:19:17,200
overall

00:19:13,200 --> 00:19:20,640
p90 latencies of our of our

00:19:17,200 --> 00:19:22,160
reads we're working on a presto

00:19:20,640 --> 00:19:24,240
connector for google studio

00:19:22,160 --> 00:19:26,080
this is to replace those kinds of

00:19:24,240 --> 00:19:26,799
tableau kinds of cases and bi kinds of

00:19:26,080 --> 00:19:30,000
cases

00:19:26,799 --> 00:19:34,240
uh where we could use google studio

00:19:30,000 --> 00:19:36,320
for bi we're doing presto on flink

00:19:34,240 --> 00:19:37,600
so we already do real-time analytics

00:19:36,320 --> 00:19:41,280
batch analytics

00:19:37,600 --> 00:19:43,760
now etl but for ingestion and for

00:19:41,280 --> 00:19:45,280
streaming we use a different type of

00:19:43,760 --> 00:19:46,720
sequel

00:19:45,280 --> 00:19:48,480
restaurant fling is going to address

00:19:46,720 --> 00:19:50,799
that so that we can use presta for that

00:19:48,480 --> 00:19:50,799
as well

00:19:51,120 --> 00:19:55,440
then we're working on presto on spark so

00:19:53,919 --> 00:19:57,919
there are only certain types of

00:19:55,440 --> 00:20:00,000
queries that presto etl today supports

00:19:57,919 --> 00:20:02,159
which fit in memory and so on the long

00:20:00,000 --> 00:20:04,080
running kinds of etl jobs which is

00:20:02,159 --> 00:20:05,840
typical characteristic of etls

00:20:04,080 --> 00:20:07,840
are something that we don't uh support

00:20:05,840 --> 00:20:09,280
investor like queries that run more than

00:20:07,840 --> 00:20:11,280
an hour or so

00:20:09,280 --> 00:20:12,400
so that's where pressed on spark comes

00:20:11,280 --> 00:20:15,440
in so

00:20:12,400 --> 00:20:17,200
in this uh kind of deployment presta

00:20:15,440 --> 00:20:18,240
becomes a library which runs in spark

00:20:17,200 --> 00:20:20,159
environment

00:20:18,240 --> 00:20:21,280
and it is going to handle all those long

00:20:20,159 --> 00:20:24,880
tail kinds of

00:20:21,280 --> 00:20:24,880
queries which we don't support today

00:20:25,120 --> 00:20:28,480
the next initiative is to dockerize

00:20:27,360 --> 00:20:31,120
presto

00:20:28,480 --> 00:20:32,320
uh and set up an orchestration system so

00:20:31,120 --> 00:20:35,679
that we intelligently

00:20:32,320 --> 00:20:38,559
uh orchestrate orchestrated with minimal

00:20:35,679 --> 00:20:38,559
manual effort

00:20:39,760 --> 00:20:43,200
the last one is we're doing

00:20:41,440 --> 00:20:46,320
pattern-based query gating

00:20:43,200 --> 00:20:48,640
today gating happens um based on

00:20:46,320 --> 00:20:49,679
users and to an action based on queries

00:20:48,640 --> 00:20:51,679
but the pattern based

00:20:49,679 --> 00:20:53,200
gating is so cool that when we see a

00:20:51,679 --> 00:20:54,480
similar query which can potentially

00:20:53,200 --> 00:20:55,440
bring down the cluster we block it

00:20:54,480 --> 00:20:58,640
before it even

00:20:55,440 --> 00:21:00,640
uh runs on the cluster so

00:20:58,640 --> 00:21:02,960
uh those are the uh cool initiatives

00:21:00,640 --> 00:21:04,799
that are going on right now

00:21:02,960 --> 00:21:07,120
i'm really glad to have this opportunity

00:21:04,799 --> 00:21:10,320
to present today thanks a lot

00:21:07,120 --> 00:21:10,320
i'm open for any questions

00:21:11,200 --> 00:21:16,480
wow that was some dense material very

00:21:13,360 --> 00:21:18,080
very good stuff thank you so much harsha

00:21:16,480 --> 00:21:20,080
when you talk about the pattern based

00:21:18,080 --> 00:21:21,440
query gating can you go into some detail

00:21:20,080 --> 00:21:24,400
on that real quick

00:21:21,440 --> 00:21:24,400
so what are we trying right

00:21:27,120 --> 00:21:30,799
all right is it better now okay so what

00:21:30,000 --> 00:21:34,640
we're trying to do

00:21:30,799 --> 00:21:34,640
is i'm going to come up with

00:21:38,400 --> 00:21:46,080
okay again okay turn off your own

00:21:42,640 --> 00:21:48,240
yeah yeah so uh so what we're trying to

00:21:46,080 --> 00:21:51,360
do is trying to come up with like um

00:21:48,240 --> 00:21:52,400
uh like coming up with signatures for

00:21:51,360 --> 00:21:55,200
each queries

00:21:52,400 --> 00:21:55,840
so that we have a pattern identified and

00:21:55,200 --> 00:21:57,679
this is like

00:21:55,840 --> 00:21:59,200
um abstracting all those values that

00:21:57,679 --> 00:22:01,760
come in a query

00:21:59,200 --> 00:22:03,440
um and having certain rules as well so

00:22:01,760 --> 00:22:04,159
when we see a similar query that comes

00:22:03,440 --> 00:22:06,400
in let's say

00:22:04,159 --> 00:22:08,240
some bad kind of a pattern with the

00:22:06,400 --> 00:22:09,120
signature the same thing comes in again

00:22:08,240 --> 00:22:11,360
in future

00:22:09,120 --> 00:22:12,240
we try to block it and there are certain

00:22:11,360 --> 00:22:14,960
rules as well

00:22:12,240 --> 00:22:15,919
which again uh coming to like like

00:22:14,960 --> 00:22:19,120
augment the pattern

00:22:15,919 --> 00:22:19,120
based uh blocking

00:22:20,559 --> 00:22:24,840
or we always have to worry about bad

00:22:22,159 --> 00:22:26,000
actors out there these days at scale

00:22:24,840 --> 00:22:28,559
right

00:22:26,000 --> 00:22:30,400
yeah yeah that's always the case right

00:22:28,559 --> 00:22:32,400
one bad actor can bring down

00:22:30,400 --> 00:22:34,320
like the cluster and cause like a lot of

00:22:32,400 --> 00:22:35,200
problems so yeah we're doing a lot of

00:22:34,320 --> 00:22:38,880
things on that

00:22:35,200 --> 00:22:40,080
front thank you so much for

00:22:38,880 --> 00:22:43,360
revolutionizing

00:22:40,080 --> 00:22:43,360
uh transportation

00:22:44,240 --> 00:22:47,200
different world now i mean you guys

00:22:45,679 --> 00:22:48,559
really change the game in so many

00:22:47,200 --> 00:22:50,480
different ways and it's the

00:22:48,559 --> 00:22:52,640
the speed of the platform that really

00:22:50,480 --> 00:22:55,600
makes it all work right

00:22:52,640 --> 00:22:56,240
yes yes definitely i think we feel uh so

00:22:55,600 --> 00:22:59,520
proud

00:22:56,240 --> 00:23:02,320
to be able to make changes uh to the way

00:22:59,520 --> 00:23:02,320
transportation works

00:23:03,280 --> 00:23:06,960
that you can see where your driver is

00:23:04,960 --> 00:23:07,440
coming from that really changed the game

00:23:06,960 --> 00:23:10,000
and really

00:23:07,440 --> 00:23:11,600
changed expectations for people about

00:23:10,000 --> 00:23:13,679
what level of service they should be

00:23:11,600 --> 00:23:15,679
getting and frankly the taxi companies

00:23:13,679 --> 00:23:17,200
everywhere really had to innovate and

00:23:15,679 --> 00:23:20,159
some of them have but most of them have

00:23:17,200 --> 00:23:23,840
not but this is a classic case where

00:23:20,159 --> 00:23:25,039
a data foundation a fast data foundation

00:23:23,840 --> 00:23:27,600
can fundamentally

00:23:25,039 --> 00:23:29,360
change a service bring better pricing

00:23:27,600 --> 00:23:30,240
bring better service across the board

00:23:29,360 --> 00:23:33,360
right

00:23:30,240 --> 00:23:34,960
definitely definitely convenience to a

00:23:33,360 --> 00:23:36,080
sense drives innovation

00:23:34,960 --> 00:23:38,640
so i think things have become so

00:23:36,080 --> 00:23:44,400
convenient with uber that

00:23:38,640 --> 00:23:44,400

YouTube URL: https://www.youtube.com/watch?v=7gIf2Q69qW8


