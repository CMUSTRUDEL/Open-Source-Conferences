Title: PrestoDB TSC Chair Keynote - Nezih Yigitbasi, Facebook
Publication date: 2020-09-30
Playlist: PrestoCon 2020 - Virtual
Description: 
	PrestoDB TSC Chair Keynote - Nezih Yigitbasi, Facebook
Captions: 
	00:00:00,560 --> 00:00:05,040
hello everyone welcome to the first

00:00:03,120 --> 00:00:07,440
press the conference

00:00:05,040 --> 00:00:10,000
and i'm very happy and very excited to

00:00:07,440 --> 00:00:13,200
be delivering the keynote today

00:00:10,000 --> 00:00:15,599
and in this keynote i'm gonna talk about

00:00:13,200 --> 00:00:17,119
how we are taking presto to infinity and

00:00:15,599 --> 00:00:18,320
beyond with our work in the presto

00:00:17,119 --> 00:00:21,439
foundation

00:00:18,320 --> 00:00:22,880
and my name is nezi heath bashu and i am

00:00:21,439 --> 00:00:24,720
i'm the chair of the presto foundation

00:00:22,880 --> 00:00:26,000
technical steering committee and i'm

00:00:24,720 --> 00:00:29,119
also an engineering manager in the

00:00:26,000 --> 00:00:29,119
presto team at facebook

00:00:29,439 --> 00:00:33,920
so we launched the presto foundation

00:00:31,279 --> 00:00:36,399
together with our partners alibaba

00:00:33,920 --> 00:00:37,040
twitter and uber to tackle some

00:00:36,399 --> 00:00:39,280
difficult

00:00:37,040 --> 00:00:40,879
large-scale distributed data processing

00:00:39,280 --> 00:00:42,840
problems that we are encountering

00:00:40,879 --> 00:00:45,520
in our warehouse scale presto

00:00:42,840 --> 00:00:47,840
deployments and it has been

00:00:45,520 --> 00:00:49,600
one year since we launched the presto

00:00:47,840 --> 00:00:51,600
foundation so

00:00:49,600 --> 00:00:53,760
happy birthday presto foundation i think

00:00:51,600 --> 00:00:56,399
it has been an amazing first year

00:00:53,760 --> 00:00:58,239
and we have done we have done a great

00:00:56,399 --> 00:01:00,079
amount of work and we have released

00:00:58,239 --> 00:01:01,920
plenty of innovative features and we

00:01:00,079 --> 00:01:03,760
will continue to do so

00:01:01,920 --> 00:01:05,840
and i would like to also thank our

00:01:03,760 --> 00:01:07,680
partners in the in the presto foundation

00:01:05,840 --> 00:01:09,760
and our collaborators and our community

00:01:07,680 --> 00:01:11,040
to to support and enable this great work

00:01:09,760 --> 00:01:13,840
and i'm totally looking forward to a

00:01:11,040 --> 00:01:13,840
great second year as well

00:01:14,159 --> 00:01:17,520
so here's a here's a recap of our first

00:01:16,479 --> 00:01:18,960
year

00:01:17,520 --> 00:01:21,920
so since we launched the presto

00:01:18,960 --> 00:01:24,400
foundation in september 2019

00:01:21,920 --> 00:01:26,720
we have onboarded two new companies ahan

00:01:24,400 --> 00:01:29,759
and alexio to our foundation

00:01:26,720 --> 00:01:32,320
and if you look at our uh project

00:01:29,759 --> 00:01:33,439
we have an additional of 2000 comments

00:01:32,320 --> 00:01:34,960
that has plenty of

00:01:33,439 --> 00:01:36,799
different features and fixes and

00:01:34,960 --> 00:01:38,799
optimizations and

00:01:36,799 --> 00:01:40,000
our source code repository has been for

00:01:38,799 --> 00:01:43,360
an additional 500

00:01:40,000 --> 00:01:45,520
times and like and it has been start an

00:01:43,360 --> 00:01:47,280
additional 1500 times as well so it's

00:01:45,520 --> 00:01:48,720
getting more popular and getting more

00:01:47,280 --> 00:01:50,720
traction there as well

00:01:48,720 --> 00:01:53,360
and in in the past year we have released

00:01:50,720 --> 00:01:54,960
14 new different press to db versions

00:01:53,360 --> 00:01:56,159
where we release these these innovative

00:01:54,960 --> 00:01:58,479
features that i'm going to talk about

00:01:56,159 --> 00:01:58,479
today

00:01:58,960 --> 00:02:03,520
so overall it was an innovative first

00:02:01,280 --> 00:02:04,320
year we have basically like focused on

00:02:03,520 --> 00:02:07,119
three major

00:02:04,320 --> 00:02:08,800
areas and those are around scalability

00:02:07,119 --> 00:02:10,640
efficiency and latency

00:02:08,800 --> 00:02:11,760
those are basically the concerns that

00:02:10,640 --> 00:02:15,120
you will have when you have a

00:02:11,760 --> 00:02:17,280
large-scale deployment of presto

00:02:15,120 --> 00:02:19,280
but i i just want to do a quick rundown

00:02:17,280 --> 00:02:20,879
of the projects here and i'm going to do

00:02:19,280 --> 00:02:23,520
a little bit of a deep dive later on in

00:02:20,879 --> 00:02:25,200
my talk today so project area is

00:02:23,520 --> 00:02:26,800
something we released last year and it

00:02:25,200 --> 00:02:29,200
is a project where we improve the

00:02:26,800 --> 00:02:30,959
efficiency of pressure significantly

00:02:29,200 --> 00:02:33,040
and we have published multiple blog

00:02:30,959 --> 00:02:36,319
posts on project area that you can read

00:02:33,040 --> 00:02:38,560
on our website prestodb.io

00:02:36,319 --> 00:02:39,599
raptor x is another project that i'm

00:02:38,560 --> 00:02:41,519
really excited about

00:02:39,599 --> 00:02:43,120
and we have released that as well and

00:02:41,519 --> 00:02:44,800
it's a project where that is like a

00:02:43,120 --> 00:02:46,879
successor of the existing

00:02:44,800 --> 00:02:48,319
press director deployment where we

00:02:46,879 --> 00:02:50,160
deliver low latency in

00:02:48,319 --> 00:02:51,680
a cost efficient manner and i'm going to

00:02:50,160 --> 00:02:52,879
talk about both of these projects today

00:02:51,680 --> 00:02:54,959
later

00:02:52,879 --> 00:02:57,200
we have preston spark there is another

00:02:54,959 --> 00:03:00,640
project that i'm extremely excited about

00:02:57,200 --> 00:03:01,120
and it's basically a project where you

00:03:00,640 --> 00:03:03,519
can

00:03:01,120 --> 00:03:05,760
run your presto queries unchanged on

00:03:03,519 --> 00:03:08,080
spark and get a number of benefits like

00:03:05,760 --> 00:03:10,560
from false tolerance to scalability etc

00:03:08,080 --> 00:03:13,840
etc

00:03:10,560 --> 00:03:15,360
we are currently working on a project to

00:03:13,840 --> 00:03:18,080
scale press the coordinator

00:03:15,360 --> 00:03:19,599
significantly the the project name is

00:03:18,080 --> 00:03:21,680
fireball and i'm going to talk about

00:03:19,599 --> 00:03:23,360
today is basically re-architecting

00:03:21,680 --> 00:03:24,720
presto and really taking press to

00:03:23,360 --> 00:03:27,120
the next level of scale that we are

00:03:24,720 --> 00:03:27,120
looking for

00:03:27,280 --> 00:03:32,400
and we have a bunch of other projects

00:03:29,840 --> 00:03:34,720
that we have released sql functions

00:03:32,400 --> 00:03:36,879
and udf support user defined function

00:03:34,720 --> 00:03:38,560
support those are basically

00:03:36,879 --> 00:03:40,239
going to improve our developer

00:03:38,560 --> 00:03:43,760
efficiency significantly

00:03:40,239 --> 00:03:46,319
and will enable you new use cases

00:03:43,760 --> 00:03:48,319
and we have released a pinot connector

00:03:46,319 --> 00:03:50,319
so this is such a great work from

00:03:48,319 --> 00:03:52,400
our partners at uber and it's going to

00:03:50,319 --> 00:03:55,200
enable additional very low latency use

00:03:52,400 --> 00:03:57,519
cases with presto

00:03:55,200 --> 00:03:58,959
finally like we have other connectors

00:03:57,519 --> 00:04:01,200
that we have released last year

00:03:58,959 --> 00:04:02,560
again duret is an impressive system for

00:04:01,200 --> 00:04:04,640
low latency use cases

00:04:02,560 --> 00:04:06,640
and we release the connector around then

00:04:04,640 --> 00:04:08,000
and we have many more many more features

00:04:06,640 --> 00:04:10,480
like you can learn more about these

00:04:08,000 --> 00:04:12,400
features in our website prestodb.io

00:04:10,480 --> 00:04:15,200
and i'm gonna do a little bit of a deep

00:04:12,400 --> 00:04:17,840
dive in some of these projects later on

00:04:15,200 --> 00:04:19,040
but before that i would like to spend a

00:04:17,840 --> 00:04:22,400
little bit of a time

00:04:19,040 --> 00:04:23,040
about presto foundation so as i

00:04:22,400 --> 00:04:24,639
mentioned

00:04:23,040 --> 00:04:26,080
we launched the presto foundation in

00:04:24,639 --> 00:04:28,720
september 2019

00:04:26,080 --> 00:04:30,720
with four founding partners alibaba

00:04:28,720 --> 00:04:32,639
facebook twitter and uber

00:04:30,720 --> 00:04:34,080
and then we onboarded ahan and alexa

00:04:32,639 --> 00:04:35,919
later on now we are a

00:04:34,080 --> 00:04:37,280
foundation of six companies working

00:04:35,919 --> 00:04:39,120
together and

00:04:37,280 --> 00:04:40,639
presta foundation is a non-profit

00:04:39,120 --> 00:04:42,000
organization that is hosted

00:04:40,639 --> 00:04:44,000
under the umbrella of the linux

00:04:42,000 --> 00:04:46,080
foundation

00:04:44,000 --> 00:04:47,280
and linux foundation has a lot of

00:04:46,080 --> 00:04:48,880
experience in running

00:04:47,280 --> 00:04:50,320
running open source projects if you look

00:04:48,880 --> 00:04:52,960
at the portfolio

00:04:50,320 --> 00:04:54,560
they have more than 100 open source

00:04:52,960 --> 00:04:56,400
projects that is backed by more than a

00:04:54,560 --> 00:04:58,240
thousand companies and

00:04:56,400 --> 00:05:00,240
around the world there are tens of

00:04:58,240 --> 00:05:01,440
thousands of developers contributing to

00:05:00,240 --> 00:05:03,440
these projects

00:05:01,440 --> 00:05:05,919
and these projects are in totally

00:05:03,440 --> 00:05:09,039
different domains from cloud networking

00:05:05,919 --> 00:05:10,080
all the way to iot et cetera and linux

00:05:09,039 --> 00:05:12,400
foundation also has

00:05:10,080 --> 00:05:13,680
a lot of experience hosting other

00:05:12,400 --> 00:05:15,440
foundation like there is

00:05:13,680 --> 00:05:17,039
other well-known foundations under the

00:05:15,440 --> 00:05:18,960
linux foundation such as

00:05:17,039 --> 00:05:20,880
cloud native computing foundation which

00:05:18,960 --> 00:05:22,320
hosts the kubernetes project for example

00:05:20,880 --> 00:05:24,240
and there are many other

00:05:22,320 --> 00:05:26,160
high-profile foundation as well and so

00:05:24,240 --> 00:05:29,280
they have a lot of experience about

00:05:26,160 --> 00:05:30,240
open source and how to you know uh how

00:05:29,280 --> 00:05:32,240
to create

00:05:30,240 --> 00:05:34,240
governance models and and we are

00:05:32,240 --> 00:05:38,720
basically relying on that experience to

00:05:34,240 --> 00:05:40,960
surround the presto foundation as well

00:05:38,720 --> 00:05:42,160
presto foundation is a community driven

00:05:40,960 --> 00:05:44,240
foundation like we have

00:05:42,160 --> 00:05:45,919
we have the six companies working

00:05:44,240 --> 00:05:48,400
together and they have

00:05:45,919 --> 00:05:51,680
different teams working on presto it's

00:05:48,400 --> 00:05:53,759
basically a single large combined team

00:05:51,680 --> 00:05:55,600
that can innovate basically more than

00:05:53,759 --> 00:05:57,520
any of us any of the single companies

00:05:55,600 --> 00:05:59,440
can innovate and that is what basically

00:05:57,520 --> 00:06:02,400
helps us accelerate our roadmap and push

00:05:59,440 --> 00:06:02,400
the technology forward

00:06:02,960 --> 00:06:06,319
and i want to spend a little bit of a

00:06:04,639 --> 00:06:07,440
time on the structure of the presto

00:06:06,319 --> 00:06:09,600
foundation so

00:06:07,440 --> 00:06:12,080
currently the presto foundation has

00:06:09,600 --> 00:06:14,080
three main bodies

00:06:12,080 --> 00:06:17,199
the first body is the governing board

00:06:14,080 --> 00:06:19,039
where each member company has a seat in

00:06:17,199 --> 00:06:20,720
and basically the governing board is

00:06:19,039 --> 00:06:22,400
responsible for setting the

00:06:20,720 --> 00:06:23,840
non-technical direction of the project

00:06:22,400 --> 00:06:27,120
and deals with funding and

00:06:23,840 --> 00:06:29,199
other non-technical issues second body

00:06:27,120 --> 00:06:30,639
is the technical steering committee

00:06:29,199 --> 00:06:32,880
and technical steering comedy is

00:06:30,639 --> 00:06:34,639
basically the committers of the project

00:06:32,880 --> 00:06:36,319
who are the expert individuals who

00:06:34,639 --> 00:06:37,759
define the roadmap and build this

00:06:36,319 --> 00:06:39,919
technology

00:06:37,759 --> 00:06:41,360
and the final uh body in the presta

00:06:39,919 --> 00:06:43,199
foundation is the outreach committee

00:06:41,360 --> 00:06:45,520
which basically deals with community

00:06:43,199 --> 00:06:49,840
engagements and events and basically

00:06:45,520 --> 00:06:51,840
engages with other user groups

00:06:49,840 --> 00:06:53,840
you can also join the presta foundation

00:06:51,840 --> 00:06:54,800
and be part of this work and the journey

00:06:53,840 --> 00:06:57,120
to really support

00:06:54,800 --> 00:06:58,880
the growth of the press ecosystem and

00:06:57,120 --> 00:07:00,800
with the membership basically like once

00:06:58,880 --> 00:07:02,720
you become part of the presto foundation

00:07:00,800 --> 00:07:04,240
you will get a number of benefits from

00:07:02,720 --> 00:07:05,039
marketing amplification and brand

00:07:04,240 --> 00:07:06,560
awareness

00:07:05,039 --> 00:07:08,000
and you'll have a better chance of

00:07:06,560 --> 00:07:09,039
engaging with the presto community

00:07:08,000 --> 00:07:11,199
through the events

00:07:09,039 --> 00:07:12,400
that we are organizing and also you will

00:07:11,199 --> 00:07:14,319
have a chance to really be

00:07:12,400 --> 00:07:16,080
part of the be part of the thought

00:07:14,319 --> 00:07:17,759
leadership and also like help with the

00:07:16,080 --> 00:07:20,240
direction of the foundation

00:07:17,759 --> 00:07:21,759
and if you also like join the tsc in

00:07:20,240 --> 00:07:23,360
then you can also contribute to the

00:07:21,759 --> 00:07:25,520
technical direction of the technology as

00:07:23,360 --> 00:07:25,520
well

00:07:26,800 --> 00:07:31,840
okay so let's talk about the technology

00:07:29,280 --> 00:07:31,840
itself

00:07:31,919 --> 00:07:35,360
as i mentioned like we have that we have

00:07:33,759 --> 00:07:36,240
done a great amount of work in these

00:07:35,360 --> 00:07:38,000
three areas

00:07:36,240 --> 00:07:39,759
and i'm gonna go through each area

00:07:38,000 --> 00:07:41,919
separately and talk about some of the

00:07:39,759 --> 00:07:45,599
impressive work that we have been doing

00:07:41,919 --> 00:07:47,840
and like give more details around that

00:07:45,599 --> 00:07:50,639
so the first bucket of work is around

00:07:47,840 --> 00:07:50,639
scalability

00:07:51,520 --> 00:07:55,599
so project fireball is is one of the

00:07:54,160 --> 00:07:57,360
great work that we are doing in in the

00:07:55,599 --> 00:08:00,160
scalability area

00:07:57,360 --> 00:08:01,759
if you are familiar with uh with a

00:08:00,160 --> 00:08:03,599
traditional presto deployment you know

00:08:01,759 --> 00:08:06,879
that there is a single coordinator

00:08:03,599 --> 00:08:09,120
and and the number of workers and

00:08:06,879 --> 00:08:10,240
what you notice is like as you grow your

00:08:09,120 --> 00:08:12,400
cluster

00:08:10,240 --> 00:08:14,319
over time as your workflows grow user

00:08:12,400 --> 00:08:15,840
base grows et cetera et cetera you

00:08:14,319 --> 00:08:18,160
start deploying more workers and

00:08:15,840 --> 00:08:19,759
eventually what happens is like you hit

00:08:18,160 --> 00:08:21,680
a scalability wall

00:08:19,759 --> 00:08:23,840
what that means is beyond a certain

00:08:21,680 --> 00:08:25,840
number of machines the coordinator

00:08:23,840 --> 00:08:27,840
cannot scale and it has because then

00:08:25,840 --> 00:08:31,120
that is multiple reasons

00:08:27,840 --> 00:08:32,560
so the the the number the the threshold

00:08:31,120 --> 00:08:34,320
where the scaling stops is actually

00:08:32,560 --> 00:08:35,919
around 1 000 workers but that really

00:08:34,320 --> 00:08:37,440
depends on your workload and probably

00:08:35,919 --> 00:08:38,719
other factors so

00:08:37,440 --> 00:08:41,120
and there are two reasons for the

00:08:38,719 --> 00:08:43,200
scalability wall one is the coordinator

00:08:41,120 --> 00:08:46,320
having a lot of responsibilities from

00:08:43,200 --> 00:08:46,880
parsing the sql to do analysis etc and

00:08:46,320 --> 00:08:49,279
then

00:08:46,880 --> 00:08:50,800
all the way to launching the tasks on

00:08:49,279 --> 00:08:52,480
the workers and then doing the task

00:08:50,800 --> 00:08:53,839
tracking doing the resource management

00:08:52,480 --> 00:08:55,760
like there's a lot of functionality

00:08:53,839 --> 00:08:58,480
going on on the coordinator

00:08:55,760 --> 00:09:00,000
and as the cluster sizes grow like you

00:08:58,480 --> 00:09:02,000
do a lot more and the coordinator

00:09:00,000 --> 00:09:04,160
basically spends a lot of resources on

00:09:02,000 --> 00:09:05,839
on really dealing with the workloads

00:09:04,160 --> 00:09:08,480
that is scheduling

00:09:05,839 --> 00:09:10,880
and the second reason is the coordinator

00:09:08,480 --> 00:09:11,680
to worker protocol and it's a text-based

00:09:10,880 --> 00:09:14,240
protocol

00:09:11,680 --> 00:09:14,880
that is json running on top of the http

00:09:14,240 --> 00:09:16,560
protocol

00:09:14,880 --> 00:09:18,720
and this extremely heavyweight and it's

00:09:16,560 --> 00:09:21,920
extremely resource intensive

00:09:18,720 --> 00:09:23,920
so these two primary reasons cause the

00:09:21,920 --> 00:09:24,959
scalability wall and project fireball

00:09:23,920 --> 00:09:27,360
basically

00:09:24,959 --> 00:09:28,480
resolves these two issues this was the

00:09:27,360 --> 00:09:30,720
first issue by

00:09:28,480 --> 00:09:32,000
scaling out the coordinator horizontally

00:09:30,720 --> 00:09:33,600
so basically we are

00:09:32,000 --> 00:09:35,920
extracting the resource manager out of

00:09:33,600 --> 00:09:37,680
the coordinator and then within a single

00:09:35,920 --> 00:09:40,160
cluster we will then have

00:09:37,680 --> 00:09:41,200
one resource manager and then multiple

00:09:40,160 --> 00:09:44,320
coordinators

00:09:41,200 --> 00:09:46,320
and many many workers and we address the

00:09:44,320 --> 00:09:48,560
second problem by basically revamping

00:09:46,320 --> 00:09:51,600
the rpc stack entirely so we have done

00:09:48,560 --> 00:09:53,760
some work so far by introducing

00:09:51,600 --> 00:09:54,800
uh the smile encoding we did some work

00:09:53,760 --> 00:09:57,200
with http

00:09:54,800 --> 00:09:58,720
etc but now we are at the point where we

00:09:57,200 --> 00:10:00,720
have to revamp the entire stack to

00:09:58,720 --> 00:10:02,959
really reap the benefits of a

00:10:00,720 --> 00:10:04,800
more efficient protocol and basically

00:10:02,959 --> 00:10:06,240
here tricks comes to the rescue here and

00:10:04,800 --> 00:10:07,040
we are we are doing some work right now

00:10:06,240 --> 00:10:10,399
to really

00:10:07,040 --> 00:10:12,320
transform our pc stack and this is how

00:10:10,399 --> 00:10:13,920
uh the presta architecture is going to

00:10:12,320 --> 00:10:15,440
look like when we are done with the

00:10:13,920 --> 00:10:17,440
fireball project

00:10:15,440 --> 00:10:19,200
so on the left you basically see the

00:10:17,440 --> 00:10:20,240
resource manager that is extracted out

00:10:19,200 --> 00:10:21,920
of the coordinator

00:10:20,240 --> 00:10:23,920
and it has a bunch of responsibilities

00:10:21,920 --> 00:10:26,000
listed here around resource management

00:10:23,920 --> 00:10:27,120
and we will have a bunch of coordinators

00:10:26,000 --> 00:10:28,480
that you see on the right as the

00:10:27,120 --> 00:10:30,160
coordinator pool

00:10:28,480 --> 00:10:31,680
again having a bunch of responsibilities

00:10:30,160 --> 00:10:34,560
around task tracking

00:10:31,680 --> 00:10:36,480
to to other state management and we have

00:10:34,560 --> 00:10:38,880
a number of workers at the bottom

00:10:36,480 --> 00:10:39,920
that is basically like the worker pool

00:10:38,880 --> 00:10:42,560
in your cluster

00:10:39,920 --> 00:10:44,880
and this is basically going to help us

00:10:42,560 --> 00:10:46,079
scale to many many towns of machines per

00:10:44,880 --> 00:10:48,560
cluster

00:10:46,079 --> 00:10:50,240
and project fireball is in progress and

00:10:48,560 --> 00:10:51,600
we've made great progress with it and we

00:10:50,240 --> 00:10:53,440
are expecting to have a beta version

00:10:51,600 --> 00:10:55,440
available in q4 so stay tuned

00:10:53,440 --> 00:10:57,760
and if when it is out please give it a

00:10:55,440 --> 00:10:57,760
try

00:10:58,000 --> 00:11:01,040
so in the scalable scalability bucket

00:10:59,760 --> 00:11:03,440
the second project

00:11:01,040 --> 00:11:05,440
is pressed on spark and i'm very excited

00:11:03,440 --> 00:11:08,480
about this project basically because

00:11:05,440 --> 00:11:10,480
it's really uh it's really attacking

00:11:08,480 --> 00:11:12,160
a problem in a different way and the

00:11:10,480 --> 00:11:14,560
problem that is attacking is what i call

00:11:12,160 --> 00:11:16,560
the workload complexity wall

00:11:14,560 --> 00:11:18,480
so similar to the scalability wall like

00:11:16,560 --> 00:11:21,120
as your workloads

00:11:18,480 --> 00:11:22,399
as your workload complexity grows then

00:11:21,120 --> 00:11:24,480
you start seeing huge

00:11:22,399 --> 00:11:26,800
issues around like hitting the local and

00:11:24,480 --> 00:11:29,360
the global memory limits of presto

00:11:26,800 --> 00:11:31,120
or if your workload results in too many

00:11:29,360 --> 00:11:34,079
tasks or stages then you start seeing

00:11:31,120 --> 00:11:36,480
other reliability problems etc etc

00:11:34,079 --> 00:11:38,079
and at a high level basically there are

00:11:36,480 --> 00:11:40,560
two ways to solve

00:11:38,079 --> 00:11:42,399
this fundamental problem and one is to

00:11:40,560 --> 00:11:43,279
basically re-architect presto

00:11:42,399 --> 00:11:46,240
significantly

00:11:43,279 --> 00:11:48,480
to address these fundamental issues or

00:11:46,240 --> 00:11:50,079
you can just reuse an existing system

00:11:48,480 --> 00:11:52,160
that provides a solution for these

00:11:50,079 --> 00:11:53,680
problems and we are actually with the

00:11:52,160 --> 00:11:55,600
preston spark project we are

00:11:53,680 --> 00:11:57,760
picking the letter path which is less

00:11:55,600 --> 00:12:00,240
expensive than the first one

00:11:57,760 --> 00:12:02,160
and what we do is basically rely on the

00:12:00,240 --> 00:12:02,959
spark execution engine because it

00:12:02,160 --> 00:12:05,040
already is

00:12:02,959 --> 00:12:07,440
fault tolerant and it's scalable and it

00:12:05,040 --> 00:12:10,639
has solutions around all these issues

00:12:07,440 --> 00:12:12,320
and since both presta and spark run on

00:12:10,639 --> 00:12:13,279
the java virtual machine what we do is

00:12:12,320 --> 00:12:16,560
basically

00:12:13,279 --> 00:12:18,639
we run the presta code as is on the

00:12:16,560 --> 00:12:19,839
spark executor jvms

00:12:18,639 --> 00:12:22,639
that is basically we are running the

00:12:19,839 --> 00:12:26,079
same code and basically you can run

00:12:22,639 --> 00:12:29,120
your press presto sql code unchanged

00:12:26,079 --> 00:12:31,600
the sql code unchanged with this feature

00:12:29,120 --> 00:12:34,000
and this is already available in open

00:12:31,600 --> 00:12:36,240
source and i put a link here

00:12:34,000 --> 00:12:38,000
on prestodb.io website that you can

00:12:36,240 --> 00:12:39,360
navigate and then you can basically give

00:12:38,000 --> 00:12:41,040
it a try

00:12:39,360 --> 00:12:42,639
so not all the query shapes are

00:12:41,040 --> 00:12:44,639
supported yet

00:12:42,639 --> 00:12:46,160
like you need all but like we are doing

00:12:44,639 --> 00:12:47,760
some more work there and we are trying

00:12:46,160 --> 00:12:50,880
to address that gap and it will be

00:12:47,760 --> 00:12:52,240
probably available around end of q4 or

00:12:50,880 --> 00:12:55,040
early next year for as

00:12:52,240 --> 00:12:55,040
a ga release

00:12:55,680 --> 00:13:00,240
okay let's move on to the second bucket

00:12:58,079 --> 00:13:02,240
efficiency

00:13:00,240 --> 00:13:04,839
in the efficiency bucket we have

00:13:02,240 --> 00:13:07,360
released project aria

00:13:04,839 --> 00:13:09,760
so what project aria does

00:13:07,360 --> 00:13:11,839
is basically optimizes the existing

00:13:09,760 --> 00:13:12,639
table scan and repartitioning operators

00:13:11,839 --> 00:13:15,120
in presto

00:13:12,639 --> 00:13:17,440
and when we look at our workflows in

00:13:15,120 --> 00:13:19,600
facebook what we notice is like

00:13:17,440 --> 00:13:20,639
half of the cpu is consumed by the scan

00:13:19,600 --> 00:13:22,720
operators

00:13:20,639 --> 00:13:24,160
and 10 percent of the cpu is consumed by

00:13:22,720 --> 00:13:26,160
every partitioning operator and that's

00:13:24,160 --> 00:13:27,440
why the aria project first attacks these

00:13:26,160 --> 00:13:29,519
two operators

00:13:27,440 --> 00:13:30,880
and applies a number of different

00:13:29,519 --> 00:13:34,079
optimization techniques

00:13:30,880 --> 00:13:36,639
to to reap some benefits and in

00:13:34,079 --> 00:13:38,560
in the for the scan operator like it

00:13:36,639 --> 00:13:40,959
optimizes techniques such as subfield

00:13:38,560 --> 00:13:43,120
pruning where for example complex types

00:13:40,959 --> 00:13:44,560
let's say you have a struct and then you

00:13:43,120 --> 00:13:45,760
access a field of destruct then

00:13:44,560 --> 00:13:47,040
basically you don't need to read the

00:13:45,760 --> 00:13:49,040
whole thing like you just need to read

00:13:47,040 --> 00:13:50,880
the field and then prune the rest

00:13:49,040 --> 00:13:52,240
and then it applies adaptive filter

00:13:50,880 --> 00:13:54,240
ordering custom

00:13:52,240 --> 00:13:56,160
you know runtime profiling to understand

00:13:54,240 --> 00:13:57,839
the selectivity of the filters

00:13:56,160 --> 00:14:00,240
and then applies the most selective

00:13:57,839 --> 00:14:01,120
filters early on etc etc so there's a

00:14:00,240 --> 00:14:03,440
bunch of

00:14:01,120 --> 00:14:05,600
optimizations that are basically around

00:14:03,440 --> 00:14:07,360
efficiently skipping data

00:14:05,600 --> 00:14:09,120
and not reading the data that's not

00:14:07,360 --> 00:14:11,120
treated and having some more mechanical

00:14:09,120 --> 00:14:13,440
sympathy with the code basically

00:14:11,120 --> 00:14:15,279
and that that that yields a lot of

00:14:13,440 --> 00:14:16,880
impressive results like if you

00:14:15,279 --> 00:14:18,480
when we deploy this technology to

00:14:16,880 --> 00:14:20,160
production basically for our batch

00:14:18,480 --> 00:14:21,839
workloads

00:14:20,160 --> 00:14:23,199
in our data infrastructure at facebook

00:14:21,839 --> 00:14:25,360
we have seen an

00:14:23,199 --> 00:14:26,959
overall 10 percent cpu improvement and

00:14:25,360 --> 00:14:28,240
that is that is impressive for facebook

00:14:26,959 --> 00:14:30,639
scale basically

00:14:28,240 --> 00:14:33,120
and for our interactive deployments at

00:14:30,639 --> 00:14:35,680
facebook we have observed around 20

00:14:33,120 --> 00:14:39,120
cpu wins and that those are huge numbers

00:14:35,680 --> 00:14:39,120
if you think about the facebook scale

00:14:39,920 --> 00:14:43,199
and similarly for the re-partitioning of

00:14:42,160 --> 00:14:46,320
optimizations

00:14:43,199 --> 00:14:47,680
so aria project implements a number of

00:14:46,320 --> 00:14:49,279
optimizations around

00:14:47,680 --> 00:14:51,040
reducing the memory allocations and

00:14:49,279 --> 00:14:53,040
unnecessary copies and making the

00:14:51,040 --> 00:14:54,720
operator basically leaner and consume

00:14:53,040 --> 00:14:56,560
less memory and cpu

00:14:54,720 --> 00:14:58,800
and that also results in an additional

00:14:56,560 --> 00:14:59,440
five percent overall cpu in across the

00:14:58,800 --> 00:15:01,440
board

00:14:59,440 --> 00:15:03,360
so these are like impressive efficiency

00:15:01,440 --> 00:15:05,199
bins and if you have a large presto

00:15:03,360 --> 00:15:06,480
deployment like you will basically see

00:15:05,199 --> 00:15:08,880
this as

00:15:06,480 --> 00:15:10,800
capex reductions in your infrastructure

00:15:08,880 --> 00:15:13,279
and project aria is basically

00:15:10,800 --> 00:15:15,120
available for the org file format right

00:15:13,279 --> 00:15:16,560
now and our partners at twitter and uber

00:15:15,120 --> 00:15:20,560
are exploring it for the parquet file

00:15:16,560 --> 00:15:20,560
format as well so stay tuned for that

00:15:22,000 --> 00:15:25,120
another project in the efficiency bucket

00:15:24,320 --> 00:15:27,120
is

00:15:25,120 --> 00:15:28,399
an idea that we started exploring

00:15:27,120 --> 00:15:31,600
recently

00:15:28,399 --> 00:15:32,000
so it's about building a reusable native

00:15:31,600 --> 00:15:34,079
engine

00:15:32,000 --> 00:15:35,199
that we can use across different compute

00:15:34,079 --> 00:15:37,680
engines or systems

00:15:35,199 --> 00:15:39,360
so in a traditional or in an established

00:15:37,680 --> 00:15:41,279
data warehouse you'll see that there's a

00:15:39,360 --> 00:15:42,160
number of different compute engines such

00:15:41,279 --> 00:15:44,639
as presto

00:15:42,160 --> 00:15:46,320
spark and then you can have some stream

00:15:44,639 --> 00:15:48,800
processing engine like fling

00:15:46,320 --> 00:15:50,160
and many probably other systems that are

00:15:48,800 --> 00:15:52,079
built in-house

00:15:50,160 --> 00:15:53,199
so the reason that we are exploring this

00:15:52,079 --> 00:15:55,279
idea is basically

00:15:53,199 --> 00:15:57,199
are for there are four reasons one is

00:15:55,279 --> 00:15:58,880
having some reusable core components

00:15:57,199 --> 00:16:00,880
because all these different systems

00:15:58,880 --> 00:16:03,040
are reinventing the wheel over and over

00:16:00,880 --> 00:16:04,959
again they have a core eval library core

00:16:03,040 --> 00:16:06,399
evaluation engine they have file format

00:16:04,959 --> 00:16:08,079
readers and writers

00:16:06,399 --> 00:16:09,600
they have metadata layers that look

00:16:08,079 --> 00:16:10,800
similar etc so

00:16:09,600 --> 00:16:12,720
you want to have some reusable

00:16:10,800 --> 00:16:13,440
components that we can reuse across

00:16:12,720 --> 00:16:15,120
these systems

00:16:13,440 --> 00:16:16,639
and also like when we build future

00:16:15,120 --> 00:16:17,839
engines we can still reuse these

00:16:16,639 --> 00:16:20,079
libraries

00:16:17,839 --> 00:16:21,839
and the second reason for this is to

00:16:20,079 --> 00:16:23,199
have some tighter control on

00:16:21,839 --> 00:16:24,959
vectorization like if you

00:16:23,199 --> 00:16:27,199
if you're familiar with the java virtual

00:16:24,959 --> 00:16:29,040
machine so you have to write your code

00:16:27,199 --> 00:16:31,360
in a specific structure and in a

00:16:29,040 --> 00:16:33,839
specific shape that the jvm

00:16:31,360 --> 00:16:35,600
can emit native code that uses the

00:16:33,839 --> 00:16:36,560
vector instructions on the modern

00:16:35,600 --> 00:16:38,959
architectures

00:16:36,560 --> 00:16:40,480
however when you do some small innocent

00:16:38,959 --> 00:16:42,800
changes to your code

00:16:40,480 --> 00:16:44,000
it can easily produce non-vectorized

00:16:42,800 --> 00:16:44,800
code and then what you will get is

00:16:44,000 --> 00:16:47,360
basically

00:16:44,800 --> 00:16:49,600
fluctuations in performance and that is

00:16:47,360 --> 00:16:50,320
undesirable so with the native engine

00:16:49,600 --> 00:16:52,800
basically

00:16:50,320 --> 00:16:53,920
with the lower level uh programming

00:16:52,800 --> 00:16:55,519
language runtime

00:16:53,920 --> 00:16:57,199
we will have a much tighter control on

00:16:55,519 --> 00:16:59,279
the generated vectorized code

00:16:57,199 --> 00:17:00,959
and similarly we will have a tighter

00:16:59,279 --> 00:17:02,880
control on memory as well

00:17:00,959 --> 00:17:04,400
so it's non-trivial with the jvm to

00:17:02,880 --> 00:17:06,880
really have a tight control on memory

00:17:04,400 --> 00:17:09,919
and do accounting and enforcement

00:17:06,880 --> 00:17:11,760
and finally jvm as its name implies it's

00:17:09,919 --> 00:17:13,439
a virtual machine so it has its own

00:17:11,760 --> 00:17:15,520
overhead when you run applications on

00:17:13,439 --> 00:17:17,360
top of it applications like presto

00:17:15,520 --> 00:17:18,880
so there is also an overhead of the vm

00:17:17,360 --> 00:17:19,439
that you want to get rid of to make to

00:17:18,880 --> 00:17:20,640
make

00:17:19,439 --> 00:17:22,640
the presto deployments much more

00:17:20,640 --> 00:17:24,640
efficient so

00:17:22,640 --> 00:17:26,799
we recently started exploring this and

00:17:24,640 --> 00:17:28,400
we are building a prototype and as we

00:17:26,799 --> 00:17:30,160
find out more we will continue sharing

00:17:28,400 --> 00:17:33,840
with the with the community in our

00:17:30,160 --> 00:17:33,840
virtual events

00:17:34,000 --> 00:17:39,760
so the third and the next bucket is

00:17:37,120 --> 00:17:39,760
latency

00:17:40,240 --> 00:17:44,480
so in the latency bucket we released

00:17:42,960 --> 00:17:46,640
project raptor x

00:17:44,480 --> 00:17:48,960
so it's basically a successor of the

00:17:46,640 --> 00:17:51,280
legacy raptor connector

00:17:48,960 --> 00:17:53,280
and here on this slide you see on the

00:17:51,280 --> 00:17:54,799
left our hive connector

00:17:53,280 --> 00:17:56,640
and on the right you see the raptor

00:17:54,799 --> 00:17:58,480
connector so in our hive deployments

00:17:56,640 --> 00:17:59,679
like in our traditional hive deployments

00:17:58,480 --> 00:18:01,919
what we do is

00:17:59,679 --> 00:18:03,360
basically have a remote file system a

00:18:01,919 --> 00:18:06,480
distributed file system

00:18:03,360 --> 00:18:07,919
and the higher metastore and in in a

00:18:06,480 --> 00:18:08,400
raptor deployment that you see on the

00:18:07,919 --> 00:18:11,120
right

00:18:08,400 --> 00:18:12,960
the data the primary data is stored

00:18:11,120 --> 00:18:14,960
locally on the ssds

00:18:12,960 --> 00:18:16,720
to give very low latency for for

00:18:14,960 --> 00:18:18,320
specific use cases

00:18:16,720 --> 00:18:20,400
and there's a bunch of background

00:18:18,320 --> 00:18:21,200
processes running like doing compacting

00:18:20,400 --> 00:18:23,600
the data or

00:18:21,200 --> 00:18:25,200
backing up to a remote source system and

00:18:23,600 --> 00:18:27,600
it has a dedicated

00:18:25,200 --> 00:18:29,360
custom raptor metal store as well and

00:18:27,600 --> 00:18:31,600
the way we have been using the legacy

00:18:29,360 --> 00:18:34,240
raptor connector is basically

00:18:31,600 --> 00:18:35,760
etl the data from the high warehouse to

00:18:34,240 --> 00:18:37,600
to the raptor clusters

00:18:35,760 --> 00:18:39,280
and make it available for consumption so

00:18:37,600 --> 00:18:42,000
that is the traditional

00:18:39,280 --> 00:18:43,919
legacy raptor deployment and project

00:18:42,000 --> 00:18:46,400
raptor x basically

00:18:43,919 --> 00:18:48,240
uh modernizes this architecture and

00:18:46,400 --> 00:18:49,520
makes it a lot more cost effective and

00:18:48,240 --> 00:18:51,120
basically

00:18:49,520 --> 00:18:53,919
the main problem basically with the

00:18:51,120 --> 00:18:56,720
legacy raptor connector is

00:18:53,919 --> 00:18:58,320
the the local ssds the flash drives that

00:18:56,720 --> 00:18:59,679
are storing the data locally on the

00:18:58,320 --> 00:19:02,000
workers

00:18:59,679 --> 00:19:03,280
and then storing the primary data it's

00:19:02,000 --> 00:19:04,400
not it's not a

00:19:03,280 --> 00:19:05,840
cache or something like this just the

00:19:04,400 --> 00:19:08,080
primary data they're stored locally and

00:19:05,840 --> 00:19:11,440
the ss is extremely costly

00:19:08,080 --> 00:19:13,280
and as like as the clusters grow as your

00:19:11,440 --> 00:19:15,760
data sizes grow you basically need

00:19:13,280 --> 00:19:17,760
a lot more storage space and the cost

00:19:15,760 --> 00:19:19,520
becomes unbearable

00:19:17,760 --> 00:19:20,880
and there's also another problem with

00:19:19,520 --> 00:19:23,520
this like

00:19:20,880 --> 00:19:26,400
since we store the primer data on local

00:19:23,520 --> 00:19:28,480
disks the clusters become storage bound

00:19:26,400 --> 00:19:30,880
and if you start thinking about capacity

00:19:28,480 --> 00:19:32,799
planning you start focusing on how to

00:19:30,880 --> 00:19:34,799
provision enough storage

00:19:32,799 --> 00:19:36,480
and that means you deploy a lot of

00:19:34,799 --> 00:19:38,799
machines to give the storage space

00:19:36,480 --> 00:19:40,559
but then your compute starts getting

00:19:38,799 --> 00:19:41,520
idle and that is that is not you know

00:19:40,559 --> 00:19:43,440
desirable

00:19:41,520 --> 00:19:44,960
and it becomes extremely costly and

00:19:43,440 --> 00:19:47,520
efficient over time

00:19:44,960 --> 00:19:49,679
and raptor x solves this problem by

00:19:47,520 --> 00:19:51,600
disaggregating the storage from compute

00:19:49,679 --> 00:19:53,120
making it look similar to the existing

00:19:51,600 --> 00:19:55,679
hive connector

00:19:53,120 --> 00:19:57,280
so again the data is going to be stored

00:19:55,679 --> 00:19:58,720
in a remote distributed file system

00:19:57,280 --> 00:20:01,760
similar to the hive connector

00:19:58,720 --> 00:20:04,880
so we get rid of basically the

00:20:01,760 --> 00:20:06,640
primary data storage on ssds we still

00:20:04,880 --> 00:20:07,520
have the ssds but now we are going to

00:20:06,640 --> 00:20:10,240
use it for

00:20:07,520 --> 00:20:11,200
caching because as you store the data

00:20:10,240 --> 00:20:12,559
remotely

00:20:11,200 --> 00:20:15,360
this means you will have higher

00:20:12,559 --> 00:20:18,559
latencies reading the data

00:20:15,360 --> 00:20:19,760
and to to reduce that latency like we

00:20:18,559 --> 00:20:22,559
had to introduce

00:20:19,760 --> 00:20:24,000
different layers of caching to to do

00:20:22,559 --> 00:20:26,400
like metadata caching

00:20:24,000 --> 00:20:27,360
class caching the data blocks on local

00:20:26,400 --> 00:20:29,520
ssds

00:20:27,360 --> 00:20:31,120
plus doing some affinity scheduling to

00:20:29,520 --> 00:20:33,600
maximize the cache hit race

00:20:31,120 --> 00:20:34,159
and with this new architecture we can

00:20:33,600 --> 00:20:36,320
get

00:20:34,159 --> 00:20:38,720
similar or even better latencies in some

00:20:36,320 --> 00:20:41,760
cases than the legacy raptor connector

00:20:38,720 --> 00:20:43,600
and in a much more cost efficient way

00:20:41,760 --> 00:20:45,440
and you you can you can see the github

00:20:43,600 --> 00:20:46,880
issue of the raptorx project where you

00:20:45,440 --> 00:20:48,400
can see a lot of details about the

00:20:46,880 --> 00:20:49,200
performance numbers and the design

00:20:48,400 --> 00:20:50,400
details

00:20:49,200 --> 00:20:52,240
and this feature is also already

00:20:50,400 --> 00:20:55,280
available and it can be used for low

00:20:52,240 --> 00:20:55,280
latency use cases

00:20:56,320 --> 00:20:59,440
another project in the in the low

00:20:57,760 --> 00:21:01,280
latency bucket that i'm excited about is

00:20:59,440 --> 00:21:04,320
the pinot connector

00:21:01,280 --> 00:21:06,320
so pinot connector pinot is a low

00:21:04,320 --> 00:21:07,120
latency scalable distributed all that

00:21:06,320 --> 00:21:10,480
data store

00:21:07,120 --> 00:21:12,320
and it powers a number of different low

00:21:10,480 --> 00:21:13,200
latency use cases like dashboarding for

00:21:12,320 --> 00:21:15,919
example

00:21:13,200 --> 00:21:17,120
and the press for pinot connector

00:21:15,919 --> 00:21:19,440
basically

00:21:17,120 --> 00:21:20,240
introduces a connector that you can read

00:21:19,440 --> 00:21:21,919
data

00:21:20,240 --> 00:21:24,240
from it from an existing pinot

00:21:21,919 --> 00:21:25,840
deployment and and it has a number of

00:21:24,240 --> 00:21:28,000
benefits for example

00:21:25,840 --> 00:21:29,600
the connector can push down certain

00:21:28,000 --> 00:21:31,200
operations like aggregations

00:21:29,600 --> 00:21:33,520
predicates and limits all the way down

00:21:31,200 --> 00:21:36,080
to pino and with that

00:21:33,520 --> 00:21:38,000
pinot can run those operations to really

00:21:36,080 --> 00:21:39,679
summarize the data process the data

00:21:38,000 --> 00:21:41,440
and then return a small amount of data

00:21:39,679 --> 00:21:42,080
to presto so that you can have an

00:21:41,440 --> 00:21:43,600
efficient

00:21:42,080 --> 00:21:45,200
you know pipeline from pinot the

00:21:43,600 --> 00:21:48,000
president very and get

00:21:45,200 --> 00:21:49,440
and get can get very low latency numbers

00:21:48,000 --> 00:21:52,240
and he also gets the benefit

00:21:49,440 --> 00:21:52,880
of the presto sql interface so the

00:21:52,240 --> 00:21:55,120
existing

00:21:52,880 --> 00:21:56,720
pino query language is not as flexible

00:21:55,120 --> 00:21:58,559
as sql so you get a much more flexible

00:21:56,720 --> 00:22:00,559
interface with this connector

00:21:58,559 --> 00:22:02,799
and finally like similar to the other

00:22:00,559 --> 00:22:05,679
connectors you get the benefit of

00:22:02,799 --> 00:22:06,080
being able to process data from pino and

00:22:05,679 --> 00:22:09,120
from

00:22:06,080 --> 00:22:11,120
other data sources this means

00:22:09,120 --> 00:22:13,520
you can for example read data from some

00:22:11,120 --> 00:22:15,600
pinot tables and then you can

00:22:13,520 --> 00:22:17,280
join it from some hive data and from

00:22:15,600 --> 00:22:19,760
some data from an elastic search

00:22:17,280 --> 00:22:22,559
connector etc in the same query

00:22:19,760 --> 00:22:24,159
and basically this new connector is

00:22:22,559 --> 00:22:26,240
enabling

00:22:24,159 --> 00:22:28,000
very low latency use cases so you can

00:22:26,240 --> 00:22:28,640
basically power your dashboards let's

00:22:28,000 --> 00:22:30,799
say

00:22:28,640 --> 00:22:32,880
through pino and store the data through

00:22:30,799 --> 00:22:35,440
presto and storing the data on the pino

00:22:32,880 --> 00:22:35,440
data store

00:22:37,919 --> 00:22:42,000
okay so i have gone through three

00:22:40,320 --> 00:22:44,080
different buckets scalability

00:22:42,000 --> 00:22:45,760
efficiency and latency and talk about

00:22:44,080 --> 00:22:48,880
the different technologies that we have

00:22:45,760 --> 00:22:51,360
built or we are building right now and

00:22:48,880 --> 00:22:52,840
we have published a number of blog posts

00:22:51,360 --> 00:22:54,880
about these technologies on the presto

00:22:52,840 --> 00:22:56,480
prestodb.io website where we share the

00:22:54,880 --> 00:22:58,080
details about these technologies and

00:22:56,480 --> 00:22:59,440
share some numbers and performance

00:22:58,080 --> 00:23:01,320
numbers etc

00:22:59,440 --> 00:23:03,200
so you can read those blog posts in the

00:23:01,320 --> 00:23:05,679
prestodb.io website

00:23:03,200 --> 00:23:07,679
we are also frequently talking about

00:23:05,679 --> 00:23:09,600
these topics in our virtual events

00:23:07,679 --> 00:23:11,919
virtual meetups that we organize every

00:23:09,600 --> 00:23:13,280
month so as we as we

00:23:11,919 --> 00:23:14,880
build these technologies we will

00:23:13,280 --> 00:23:16,799
continue to to share it with the

00:23:14,880 --> 00:23:17,600
community so plea please keep an eye on

00:23:16,799 --> 00:23:19,760
the meetup

00:23:17,600 --> 00:23:20,960
group as well and you will you will see

00:23:19,760 --> 00:23:22,720
we are going to talk about these

00:23:20,960 --> 00:23:25,440
technologies next

00:23:22,720 --> 00:23:27,120
so let's switch gears and let's talk

00:23:25,440 --> 00:23:30,000
about the future how the future looks

00:23:27,120 --> 00:23:30,000
like for presto

00:23:30,960 --> 00:23:35,120
so the first item the first area that we

00:23:33,360 --> 00:23:37,600
want to continue investing in

00:23:35,120 --> 00:23:39,520
is to scaling to much larger clusters

00:23:37,600 --> 00:23:42,559
and larger workloads so

00:23:39,520 --> 00:23:43,200
with project fireball as i mentioned we

00:23:42,559 --> 00:23:44,480
are

00:23:43,200 --> 00:23:46,880
scaling the presta coordinator

00:23:44,480 --> 00:23:50,080
significantly and

00:23:46,880 --> 00:23:51,679
basically what happens is when our

00:23:50,080 --> 00:23:53,919
workflows grow when we

00:23:51,679 --> 00:23:55,440
you know take on new users new use cases

00:23:53,919 --> 00:23:57,520
we basically start deploying

00:23:55,440 --> 00:23:59,440
more and more workers and when we hit

00:23:57,520 --> 00:24:01,279
the scalability wall so far what we have

00:23:59,440 --> 00:24:03,279
done is very simple we just

00:24:01,279 --> 00:24:06,080
started splitting physically splitting

00:24:03,279 --> 00:24:08,240
our clusters to sustain the growth

00:24:06,080 --> 00:24:09,520
however that results in a large number

00:24:08,240 --> 00:24:11,919
of clusters and

00:24:09,520 --> 00:24:13,919
easily turns into an operational problem

00:24:11,919 --> 00:24:16,320
and then you need to have a lot of great

00:24:13,919 --> 00:24:18,960
tooling and great automation to really

00:24:16,320 --> 00:24:20,080
or you know manage this huge number of

00:24:18,960 --> 00:24:22,480
clusters

00:24:20,080 --> 00:24:23,679
and also like another problem with that

00:24:22,480 --> 00:24:26,640
approach is basically you get a

00:24:23,679 --> 00:24:28,720
fragmentation in in your resources and

00:24:26,640 --> 00:24:30,480
that fragmented resource view basically

00:24:28,720 --> 00:24:32,559
results in sub-optimal resource

00:24:30,480 --> 00:24:33,919
management decisions at higher levels

00:24:32,559 --> 00:24:35,520
and both of these problems are

00:24:33,919 --> 00:24:37,279
undesirable so what we want is to

00:24:35,520 --> 00:24:40,080
basically

00:24:37,279 --> 00:24:40,799
in the end have a small number of large

00:24:40,080 --> 00:24:42,320
clusters

00:24:40,799 --> 00:24:44,400
and basically like maybe have one

00:24:42,320 --> 00:24:46,240
cluster consistent

00:24:44,400 --> 00:24:47,760
consisting of tens of thousands of

00:24:46,240 --> 00:24:49,760
workers

00:24:47,760 --> 00:24:51,279
then basically we will be able to take

00:24:49,760 --> 00:24:52,559
much efficient resource management

00:24:51,279 --> 00:24:54,000
decisions locally

00:24:52,559 --> 00:24:56,559
and then we will have a lot less

00:24:54,000 --> 00:24:57,120
clusters to manage so this is basically

00:24:56,559 --> 00:25:00,559
like

00:24:57,120 --> 00:25:02,240
uh similar to how hadoop evolved like

00:25:00,559 --> 00:25:03,600
if you are familiar with the history of

00:25:02,240 --> 00:25:05,360
hadoop like

00:25:03,600 --> 00:25:07,200
in hadoop version 1 there was the job

00:25:05,360 --> 00:25:08,960
tracker the coordinator

00:25:07,200 --> 00:25:10,480
and it couldn't scale beyond a certain

00:25:08,960 --> 00:25:13,279
number of machines

00:25:10,480 --> 00:25:14,400
and certain size and eventually like the

00:25:13,279 --> 00:25:16,799
hadoop community

00:25:14,400 --> 00:25:18,320
build hadoop version 2 which is the yarn

00:25:16,799 --> 00:25:20,240
resource management system

00:25:18,320 --> 00:25:22,080
and basically they built an architecture

00:25:20,240 --> 00:25:24,000
similar to what fireball is building

00:25:22,080 --> 00:25:25,600
and we are also like inspired getting

00:25:24,000 --> 00:25:27,120
inspiration from them as well

00:25:25,600 --> 00:25:28,960
like they have resource managers

00:25:27,120 --> 00:25:31,120
application masters for applications etc

00:25:28,960 --> 00:25:34,000
etc and now they are able to sustain

00:25:31,120 --> 00:25:35,760
huge clusters and huge infrastructures

00:25:34,000 --> 00:25:37,440
so we are going to continue investing in

00:25:35,760 --> 00:25:39,120
this bucket

00:25:37,440 --> 00:25:42,640
so the second bucket that we are going

00:25:39,120 --> 00:25:44,880
to invest is fault tolerance

00:25:42,640 --> 00:25:47,039
so presto is currently not fault

00:25:44,880 --> 00:25:48,159
tolerant it's depending on the clients

00:25:47,039 --> 00:25:49,919
to

00:25:48,159 --> 00:25:51,200
rerun their queries when something goes

00:25:49,919 --> 00:25:52,960
bad so this

00:25:51,200 --> 00:25:54,400
this was a conscious design decision

00:25:52,960 --> 00:25:55,760
from the early days of presto because

00:25:54,400 --> 00:25:56,559
presto was built for interactive

00:25:55,760 --> 00:25:59,360
workflows

00:25:56,559 --> 00:26:00,880
and the trade-offs was made for latency

00:25:59,360 --> 00:26:01,760
instead of false tolerance and it makes

00:26:00,880 --> 00:26:04,799
sense

00:26:01,760 --> 00:26:07,120
however it's it's not really desirable

00:26:04,799 --> 00:26:09,279
for multiple reasons first of all even

00:26:07,120 --> 00:26:11,760
for interactive workloads

00:26:09,279 --> 00:26:14,080
like uh to give a specific example let's

00:26:11,760 --> 00:26:16,080
say you have an interactive cluster

00:26:14,080 --> 00:26:18,240
and and traditionally like presto

00:26:16,080 --> 00:26:20,080
clusters are multi-tenant that is like

00:26:18,240 --> 00:26:21,760
multiple users are accessing the cluster

00:26:20,080 --> 00:26:23,039
at the same time so in an interactive

00:26:21,760 --> 00:26:25,440
cluster like that

00:26:23,039 --> 00:26:26,880
let's say you have a machine failure but

00:26:25,440 --> 00:26:29,360
currently what happens is that

00:26:26,880 --> 00:26:30,480
all the queries running in that cluster

00:26:29,360 --> 00:26:33,279
are going to fail

00:26:30,480 --> 00:26:34,880
and this means all the users in that

00:26:33,279 --> 00:26:37,120
clusters are going to be affected

00:26:34,880 --> 00:26:38,880
so first of all there is bad experience

00:26:37,120 --> 00:26:40,960
right there bad user experience

00:26:38,880 --> 00:26:42,640
and secondly since all the all the

00:26:40,960 --> 00:26:46,400
queries fail there's also some

00:26:42,640 --> 00:26:47,200
wasted cpu there and this problem of cpu

00:26:46,400 --> 00:26:50,240
waste

00:26:47,200 --> 00:26:53,279
gets a lot worse if you have

00:26:50,240 --> 00:26:54,880
longer running queries so basically like

00:26:53,279 --> 00:26:56,000
if you are running batch workloads or

00:26:54,880 --> 00:26:58,240
etl workflows

00:26:56,000 --> 00:27:00,960
like having some failures like that is

00:26:58,240 --> 00:27:02,880
going to result in huge waste of cpu

00:27:00,960 --> 00:27:04,640
and that is basically a cost incurred in

00:27:02,880 --> 00:27:07,120
your infrastructure

00:27:04,640 --> 00:27:10,000
and finally you also need fault

00:27:07,120 --> 00:27:12,480
tolerance for for basically

00:27:10,000 --> 00:27:13,679
having much better elasticity because if

00:27:12,480 --> 00:27:15,600
you look at

00:27:13,679 --> 00:27:17,200
presto deployments on infrastructure as

00:27:15,600 --> 00:27:19,440
a service

00:27:17,200 --> 00:27:20,720
in environments basically the resources

00:27:19,440 --> 00:27:22,799
come and go

00:27:20,720 --> 00:27:25,600
and then you really want some uh

00:27:22,799 --> 00:27:28,080
capabilities in the engine to to

00:27:25,600 --> 00:27:30,080
be able to you know utilize the you know

00:27:28,080 --> 00:27:32,240
new load and utilize the new machines

00:27:30,080 --> 00:27:33,679
and then be tolerant to the loss of

00:27:32,240 --> 00:27:37,279
existing machines

00:27:33,679 --> 00:27:40,000
so fault tolerance is a big area

00:27:37,279 --> 00:27:41,679
and it is it's a non-trivial problem

00:27:40,000 --> 00:27:42,960
it's a non-trivial systems problem and

00:27:41,679 --> 00:27:44,480
we are going to explore this

00:27:42,960 --> 00:27:47,600
and we are going to invest some more

00:27:44,480 --> 00:27:50,799
time on addressing this problem as well

00:27:47,600 --> 00:27:52,960
so the third bucket is latency but

00:27:50,799 --> 00:27:54,799
basically sub second latency

00:27:52,960 --> 00:27:57,120
so i mentioned that with raptor x we

00:27:54,799 --> 00:27:58,480
have we have like good latency numbers

00:27:57,120 --> 00:28:00,320
for certain use cases

00:27:58,480 --> 00:28:02,480
but we want to go beyond that we want to

00:28:00,320 --> 00:28:04,480
really go to sub second range

00:28:02,480 --> 00:28:06,240
millisecond range

00:28:04,480 --> 00:28:07,840
and that is basically required by

00:28:06,240 --> 00:28:09,600
certain use cases

00:28:07,840 --> 00:28:10,960
and that is basically that's going to

00:28:09,600 --> 00:28:12,320
basically make pressure a truly

00:28:10,960 --> 00:28:14,880
interactive engine

00:28:12,320 --> 00:28:16,000
and for that like we are gonna basically

00:28:14,880 --> 00:28:18,960
explore

00:28:16,000 --> 00:28:20,399
a number of ideas like ideas around

00:28:18,960 --> 00:28:22,559
metadata

00:28:20,399 --> 00:28:25,440
and ideas around like reducing ladies on

00:28:22,559 --> 00:28:26,960
the coordinator or rpc layer etc etc but

00:28:25,440 --> 00:28:28,080
there's a lot to investigate there and

00:28:26,960 --> 00:28:31,679
we are going to do that

00:28:28,080 --> 00:28:34,559
in the in the short feature

00:28:31,679 --> 00:28:35,120
efficiency as i mentioned we we release

00:28:34,559 --> 00:28:36,799
aria

00:28:35,120 --> 00:28:38,240
and we are working on a native engine

00:28:36,799 --> 00:28:40,640
and we can do more there

00:28:38,240 --> 00:28:41,840
and like we can do much better workload

00:28:40,640 --> 00:28:44,080
placement we can

00:28:41,840 --> 00:28:45,520
explore ideas around adaptive execution

00:28:44,080 --> 00:28:47,200
or other techniques around metadata

00:28:45,520 --> 00:28:48,960
management so there's a lot of

00:28:47,200 --> 00:28:50,480
ideas that we are going to explore and

00:28:48,960 --> 00:28:51,279
we want to make presto much more

00:28:50,480 --> 00:28:53,360
efficient

00:28:51,279 --> 00:28:55,039
and basically the reason for that is

00:28:53,360 --> 00:28:56,240
like if you have a sufficiently sized

00:28:55,039 --> 00:28:58,080
presto cluster

00:28:56,240 --> 00:29:00,159
that you really want your system to be

00:28:58,080 --> 00:29:00,720
extremely efficient any any cpu savings

00:29:00,159 --> 00:29:02,880
will

00:29:00,720 --> 00:29:04,880
result in significant capex cost and

00:29:02,880 --> 00:29:06,720
that is one area where we keep focusing

00:29:04,880 --> 00:29:09,520
on

00:29:06,720 --> 00:29:10,000
and the final area is the presto sql for

00:29:09,520 --> 00:29:12,720
all

00:29:10,000 --> 00:29:14,000
so basically as i mentioned in

00:29:12,720 --> 00:29:15,760
established deployments you have a

00:29:14,000 --> 00:29:16,640
number of different engines like presto

00:29:15,760 --> 00:29:18,880
spark

00:29:16,640 --> 00:29:20,480
fling for stream processing etc etc and

00:29:18,880 --> 00:29:22,000
all these systems have different

00:29:20,480 --> 00:29:24,080
language interfaces and it's extremely

00:29:22,000 --> 00:29:26,480
confusing for users and very difficult

00:29:24,080 --> 00:29:27,760
to switch between systems like the users

00:29:26,480 --> 00:29:29,279
basically need to learn all these

00:29:27,760 --> 00:29:30,720
different dialects to be able to use all

00:29:29,279 --> 00:29:33,120
these systems proficiently

00:29:30,720 --> 00:29:34,880
so basically like the question is how

00:29:33,120 --> 00:29:36,080
how useful and cool it would be if you

00:29:34,880 --> 00:29:38,960
could just run

00:29:36,080 --> 00:29:40,720
if you could just write a sql query once

00:29:38,960 --> 00:29:42,960
and run it anywhere

00:29:40,720 --> 00:29:44,559
but that would be super useful and super

00:29:42,960 --> 00:29:46,240
cool and there is an area where we

00:29:44,559 --> 00:29:49,039
started exploring recently and we will

00:29:46,240 --> 00:29:49,039
do more work there

00:29:49,200 --> 00:29:52,399
and with that i would like to conclude

00:29:50,880 --> 00:29:54,159
my keynote here

00:29:52,399 --> 00:29:55,840
and if you have any questions about any

00:29:54,159 --> 00:29:58,000
of these technologies and if you want to

00:29:55,840 --> 00:30:00,640
get involved with our community you can

00:29:58,000 --> 00:30:02,000
go to prestodb.io our website and you

00:30:00,640 --> 00:30:03,679
can click the community link and you

00:30:02,000 --> 00:30:04,480
will see an invitation to our slack

00:30:03,679 --> 00:30:06,159
channel there

00:30:04,480 --> 00:30:08,559
so please join our slack channel and you

00:30:06,159 --> 00:30:10,720
can ask your questions and get involved

00:30:08,559 --> 00:30:12,640
and i would like to thank you all again

00:30:10,720 --> 00:30:13,679
for joining us today for the first press

00:30:12,640 --> 00:30:15,919
the conference

00:30:13,679 --> 00:30:17,679
and we have a great lineup of speakers

00:30:15,919 --> 00:30:19,440
and we have a great lineup of panel

00:30:17,679 --> 00:30:21,440
discussions as well so please

00:30:19,440 --> 00:30:25,440
enjoy the rest of the event and enjoy

00:30:21,440 --> 00:30:25,440

YouTube URL: https://www.youtube.com/watch?v=iWImzXnfVBc


