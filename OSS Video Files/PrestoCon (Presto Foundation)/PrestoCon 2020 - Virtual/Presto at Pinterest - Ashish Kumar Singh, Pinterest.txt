Title: Presto at Pinterest - Ashish Kumar Singh, Pinterest
Publication date: 2020-09-30
Playlist: PrestoCon 2020 - Virtual
Description: 
	Presto at Pinterest - Ashish Kumar Singh, Pinterest

Speakers: Ashish Kumar Singh

As a data-driven company, many critical business decisions are made at Pinterest based on insights from data. Presto has played a key role to enable interactive querying at Pinterest. Operating Presto at Pinterestâ€™s scale has involved resolving quite a few challenges. In this talk, Ashish Singh will share Pinterest's journey on adopting, using and enhancing Presto to meet Pinterest's interactive querying needs.
Captions: 
	00:00:00,160 --> 00:00:03,199
good afternoon everyone thanks for

00:00:01,599 --> 00:00:05,680
joining us this afternoon

00:00:03,199 --> 00:00:07,680
to learn about preschool at pinterest in

00:00:05,680 --> 00:00:10,800
this talk we will be talking about

00:00:07,680 --> 00:00:12,960
uh presto how it is used at pinterest

00:00:10,800 --> 00:00:15,200
the challenges we faced in our journey

00:00:12,960 --> 00:00:18,160
to happy querying with presto

00:00:15,200 --> 00:00:21,119
at pinterest and more importantly how we

00:00:18,160 --> 00:00:23,119
solve those issues

00:00:21,119 --> 00:00:24,400
however let's start with the most

00:00:23,119 --> 00:00:27,760
important thing first

00:00:24,400 --> 00:00:29,279
which is about me i'm ashied saying

00:00:27,760 --> 00:00:30,960
i'm a technique on big data freight

00:00:29,279 --> 00:00:32,800
platform at pinterest

00:00:30,960 --> 00:00:34,239
i've been with pinterest for almost four

00:00:32,800 --> 00:00:36,000
years trying to make querying at

00:00:34,239 --> 00:00:37,920
pinterest a happy experience

00:00:36,000 --> 00:00:39,920
prior to that i was at florida working

00:00:37,920 --> 00:00:42,480
in similar space

00:00:39,920 --> 00:00:45,120
now that we know enough about me let's

00:00:42,480 --> 00:00:47,360
get started

00:00:45,120 --> 00:00:48,160
at princess we build world's catalogue

00:00:47,360 --> 00:00:51,280
of ideas

00:00:48,160 --> 00:00:53,280
by helping people discover and do what

00:00:51,280 --> 00:00:55,920
they love in reality

00:00:53,280 --> 00:00:56,719
like fashion travel design cooking etc

00:00:55,920 --> 00:01:00,320
i'm pretty sure

00:00:56,719 --> 00:01:03,359
we all know about this but to

00:01:00,320 --> 00:01:05,760
enable printers to do so we rely on

00:01:03,359 --> 00:01:07,680
scalable efficient and reliable data

00:01:05,760 --> 00:01:09,760
processing

00:01:07,680 --> 00:01:11,200
pinterest is a data driven company and

00:01:09,760 --> 00:01:15,600
so these capabilities

00:01:11,200 --> 00:01:17,840
are really critical for the business

00:01:15,600 --> 00:01:19,840
data processing capability is important

00:01:17,840 --> 00:01:20,400
but ease of expressing computational

00:01:19,840 --> 00:01:23,040
needs

00:01:20,400 --> 00:01:27,680
is equally important this is where sql

00:01:23,040 --> 00:01:29,520
comes in

00:01:27,680 --> 00:01:32,880
i'm sure the audience here gets most

00:01:29,520 --> 00:01:34,320
excited by scalability challenges

00:01:32,880 --> 00:01:36,240
so let's take a look at some of the

00:01:34,320 --> 00:01:37,759
scale numbers at printers relevant to

00:01:36,240 --> 00:01:39,680
the stock

00:01:37,759 --> 00:01:41,520
it's only fair to start with business

00:01:39,680 --> 00:01:43,520
scale at printrest

00:01:41,520 --> 00:01:45,360
we have over 400 million monthly active

00:01:43,520 --> 00:01:48,880
users over 200 billion

00:01:45,360 --> 00:01:50,079
pins and over 4 billion boards

00:01:48,880 --> 00:01:53,360
let's see how that business scale

00:01:50,079 --> 00:01:56,040
translates to scale of data

00:01:53,360 --> 00:01:58,159
we have over 400 petabytes of data in

00:01:56,040 --> 00:02:01,920
aws3

00:01:58,159 --> 00:02:05,119
we run close to 100 000 hadoop jobs

00:02:01,920 --> 00:02:09,440
per day on multiple

00:02:05,119 --> 00:02:11,680
of tens of thousands of hadoop nodes

00:02:09,440 --> 00:02:13,040
more relevant to this talk we have over

00:02:11,680 --> 00:02:15,520
500 nodes

00:02:13,040 --> 00:02:17,599
serving presta queries that enable

00:02:15,520 --> 00:02:19,920
queries going from hundreds of thousands

00:02:17,599 --> 00:02:22,400
of hive tables

00:02:19,920 --> 00:02:23,840
we have all of our offline data sets on

00:02:22,400 --> 00:02:26,160
aws s3

00:02:23,840 --> 00:02:29,200
and all of our compute instances are on

00:02:26,160 --> 00:02:29,200
aws ec2

00:02:30,640 --> 00:02:34,239
before we talk specifically about

00:02:32,239 --> 00:02:36,400
customer printers let's take a look at

00:02:34,239 --> 00:02:38,959
brief history of how query and data

00:02:36,400 --> 00:02:41,440
platform evolved at pinterest

00:02:38,959 --> 00:02:42,720
it was not until 2016 when we started

00:02:41,440 --> 00:02:44,800
exploring presta

00:02:42,720 --> 00:02:46,239
before that our interactive querying

00:02:44,800 --> 00:02:49,200
needs were served by

00:02:46,239 --> 00:02:49,200
aws redshift

00:02:49,360 --> 00:02:53,200
we started exploring presto while

00:02:51,200 --> 00:02:55,680
building our in-house data platform

00:02:53,200 --> 00:02:58,400
as of 2018 we have a completely in-house

00:02:55,680 --> 00:03:04,480
maintained and operated data platform

00:02:58,400 --> 00:03:05,840
and presto is a critical piece of it

00:03:04,480 --> 00:03:07,599
we are looking at presto's monthly

00:03:05,840 --> 00:03:09,840
active users and weekly active users

00:03:07,599 --> 00:03:12,560
over time

00:03:09,840 --> 00:03:13,440
we have close to 1500 monthly active

00:03:12,560 --> 00:03:15,200
users

00:03:13,440 --> 00:03:16,640
which is pretty close to our employee

00:03:15,200 --> 00:03:19,920
strength

00:03:16,640 --> 00:03:22,800
which essentially means that we have

00:03:19,920 --> 00:03:23,840
almost all of our employees across all

00:03:22,800 --> 00:03:25,760
the orgs

00:03:23,840 --> 00:03:28,000
including non-engineering organizations

00:03:25,760 --> 00:03:29,760
using presto

00:03:28,000 --> 00:03:31,599
the popularity of presto can easily be

00:03:29,760 --> 00:03:33,360
attributed to its speed

00:03:31,599 --> 00:03:35,920
and finding answers from petabytes of

00:03:33,360 --> 00:03:37,599
data customer serves over a million

00:03:35,920 --> 00:03:40,799
queries each month

00:03:37,599 --> 00:03:43,760
at pinterest and number of queries

00:03:40,799 --> 00:03:47,840
and use cases that are served by cluster

00:03:43,760 --> 00:03:47,840
continues to rise

00:03:49,920 --> 00:03:53,120
let's look at pressure deployment at

00:03:51,360 --> 00:03:55,680
pinterest we have

00:03:53,120 --> 00:03:58,239
multiple cluster clusters each serving a

00:03:55,680 --> 00:04:00,879
particular category of code queries

00:03:58,239 --> 00:04:02,720
for example we have a dedicated clusters

00:04:00,879 --> 00:04:04,319
to serve predictable query patterns like

00:04:02,720 --> 00:04:07,680
scheduled queries

00:04:04,319 --> 00:04:09,760
we call them app clusters some

00:04:07,680 --> 00:04:12,319
are more meant to handle bursty traffic

00:04:09,760 --> 00:04:15,840
patterns like ad hoc queries

00:04:12,319 --> 00:04:15,840
which are simply called ad hoc clusters

00:04:16,560 --> 00:04:21,199
all these clusters are behind a service

00:04:18,479 --> 00:04:22,800
uh known as presser gateway

00:04:21,199 --> 00:04:25,280
this service helps keeping clients

00:04:22,800 --> 00:04:26,720
oblivious of various cluster clusters

00:04:25,280 --> 00:04:28,160
and provides critical functionalities

00:04:26,720 --> 00:04:31,199
that we are going to talk about

00:04:28,160 --> 00:04:34,080
in next three slides each cluster has

00:04:31,199 --> 00:04:36,320
capability to have dedicated workers

00:04:34,080 --> 00:04:38,000
and transient workers on a kubernetes

00:04:36,320 --> 00:04:40,560
cluster

00:04:38,000 --> 00:04:41,280
we have another service known as presta

00:04:40,560 --> 00:04:42,800
controller

00:04:41,280 --> 00:04:44,320
that helps maintaining a watch on

00:04:42,800 --> 00:04:47,520
various clusters and perform

00:04:44,320 --> 00:04:49,120
admin level tasks we are going to talk

00:04:47,520 --> 00:04:52,880
about that service as well in

00:04:49,120 --> 00:04:54,000
next few slides users query from places

00:04:52,880 --> 00:04:57,840
like hive tables

00:04:54,000 --> 00:04:59,520
mysql druid and some theft services

00:04:57,840 --> 00:05:01,520
however majority of the use cases

00:04:59,520 --> 00:05:05,199
querying uh query from

00:05:01,520 --> 00:05:07,039
hive tables that points to data on s3

00:05:05,199 --> 00:05:08,639
while querying from druid we create

00:05:07,039 --> 00:05:10,479
directly from the druid segment files

00:05:08,639 --> 00:05:12,639
and avoid putting any extra load on

00:05:10,479 --> 00:05:15,280
android servers

00:05:12,639 --> 00:05:16,560
as i just mentioned presto controller is

00:05:15,280 --> 00:05:19,280
one of the critical pieces

00:05:16,560 --> 00:05:20,160
of our cluster deployment infrastructure

00:05:19,280 --> 00:05:21,919
it's an in-house

00:05:20,160 --> 00:05:23,520
service which is critical to our

00:05:21,919 --> 00:05:26,160
processor deployments monitoring and

00:05:23,520 --> 00:05:26,160
self-healing

00:05:26,479 --> 00:05:29,919
on there it serves major functionalities

00:05:29,440 --> 00:05:32,720
like

00:05:29,919 --> 00:05:34,240
health check of individual clusters slow

00:05:32,720 --> 00:05:37,440
worker detection

00:05:34,240 --> 00:05:39,199
heavy query reduction it also handles

00:05:37,440 --> 00:05:41,520
like when it detects heavy queries which

00:05:39,199 --> 00:05:43,919
essentially kill them

00:05:41,520 --> 00:05:45,600
rolling restart of the plaster clusters

00:05:43,919 --> 00:05:48,639
and scaling clusters when we

00:05:45,600 --> 00:05:49,840
have a need to do so cluster gateway is

00:05:48,639 --> 00:05:53,120
a service that sets

00:05:49,840 --> 00:05:55,440
between clients and cluster clusters

00:05:53,120 --> 00:05:57,120
it essentially is a smart http proxy

00:05:55,440 --> 00:05:59,520
server

00:05:57,120 --> 00:06:01,120
and because it is stateless it's very

00:05:59,520 --> 00:06:03,600
easy for us to scale it up

00:06:01,120 --> 00:06:03,600
or down

00:06:04,240 --> 00:06:09,280
we use presta gateway to route queries

00:06:07,919 --> 00:06:12,400
to different clusters

00:06:09,280 --> 00:06:13,600
based on rules and these rules we

00:06:12,400 --> 00:06:18,160
control dynamically

00:06:13,600 --> 00:06:20,240
um so we can we can handle the traffic

00:06:18,160 --> 00:06:21,680
uh based on available clusters and the

00:06:20,240 --> 00:06:24,800
incoming load

00:06:21,680 --> 00:06:26,319
um very dynamically for example if we

00:06:24,800 --> 00:06:29,919
see that or there's a

00:06:26,319 --> 00:06:31,120
uh a query a source which is becoming

00:06:29,919 --> 00:06:34,000
more active

00:06:31,120 --> 00:06:34,639
and it's it's uh overflowing or like

00:06:34,000 --> 00:06:36,160
it's

00:06:34,639 --> 00:06:38,639
it's going beyond the resources

00:06:36,160 --> 00:06:40,960
available on a particular cluster

00:06:38,639 --> 00:06:42,160
we can start routing those uh portion of

00:06:40,960 --> 00:06:44,319
those queries

00:06:42,160 --> 00:06:46,560
from that servers to other clusters as

00:06:44,319 --> 00:06:46,560
well

00:06:47,360 --> 00:06:50,479
we also provide resource usage

00:06:48,960 --> 00:06:54,000
visibility for users

00:06:50,479 --> 00:06:57,520
organizations through pressure gateway

00:06:54,000 --> 00:07:00,639
and we also provide uh overall cluster

00:06:57,520 --> 00:07:03,360
health visibility to our users and to

00:07:00,639 --> 00:07:05,680
for various uh sle calculations uh

00:07:03,360 --> 00:07:08,960
through professor gateway

00:07:05,680 --> 00:07:11,919
there are a few more uh uh more

00:07:08,960 --> 00:07:13,199
female uh things uh free mode services

00:07:11,919 --> 00:07:13,759
are delivered by pressure gateway that

00:07:13,199 --> 00:07:16,240
we'll be

00:07:13,759 --> 00:07:18,639
talking about uh in in the following

00:07:16,240 --> 00:07:18,639
slides

00:07:20,319 --> 00:07:24,160
this is a screenshot of presser gateway

00:07:22,800 --> 00:07:26,160
uh as you can see

00:07:24,160 --> 00:07:27,440
you can see various backends are various

00:07:26,160 --> 00:07:30,800
clusters behind

00:07:27,440 --> 00:07:32,000
this gateway vs resource scripts um and

00:07:30,800 --> 00:07:34,160
uh and we'll be talking about the

00:07:32,000 --> 00:07:36,880
resource good part a little bit more

00:07:34,160 --> 00:07:38,639
but it gives you a very good uh view of

00:07:36,880 --> 00:07:41,759
various clusters um

00:07:38,639 --> 00:07:44,960
that that are serving uh queries uh

00:07:41,759 --> 00:07:46,319
in this case ad hoc queries just just on

00:07:44,960 --> 00:07:47,840
one page

00:07:46,319 --> 00:07:49,039
let's take a look at the various

00:07:47,840 --> 00:07:49,919
customer clusters that we have at

00:07:49,039 --> 00:07:53,199
printers

00:07:49,919 --> 00:07:54,080
uh our presto version is based off press

00:07:53,199 --> 00:07:58,560
sequel

00:07:54,080 --> 00:08:00,639
version 320. we typically use um

00:07:58,560 --> 00:08:02,800
high mysql droid and thrift connectors

00:08:00,639 --> 00:08:06,720
but most of our use cases are

00:08:02,800 --> 00:08:09,199
on height height connected

00:08:06,720 --> 00:08:10,479
ad hoc workers we have them running on

00:08:09,199 --> 00:08:14,560
coordinators part

00:08:10,479 --> 00:08:16,080
whereas app or scheduled cluster

00:08:14,560 --> 00:08:18,160
workers are running on dedicated

00:08:16,080 --> 00:08:21,280
instances

00:08:18,160 --> 00:08:23,440
we have in total five clusters and

00:08:21,280 --> 00:08:25,199
around 500 workers distributed between

00:08:23,440 --> 00:08:28,240
these

00:08:25,199 --> 00:08:29,680
these clusters our coordinators are

00:08:28,240 --> 00:08:33,440
roughly around 400

00:08:29,680 --> 00:08:34,320
around uh 488 gigabytes and memory 64

00:08:33,440 --> 00:08:36,640
cores

00:08:34,320 --> 00:08:38,640
our workers are between 340 to 384

00:08:36,640 --> 00:08:41,839
gigabytes of memory and between 43 and

00:08:38,640 --> 00:08:41,839
48 codes

00:08:43,519 --> 00:08:46,720
now that we looked at the overview of

00:08:45,040 --> 00:08:48,800
breastwear printers let's take a look at

00:08:46,720 --> 00:08:51,839
the challenges we faced

00:08:48,800 --> 00:08:51,839
and how we solve them

00:08:52,640 --> 00:08:55,440
let's start with one of the primary

00:08:54,000 --> 00:08:56,800
issues we faced at pinterest while

00:08:55,440 --> 00:08:59,680
onboarding cluster

00:08:56,800 --> 00:09:02,480
this was caused due to very large nested

00:08:59,680 --> 00:09:02,480
theft schemas

00:09:03,200 --> 00:09:06,720
impressive coordinator fetches table

00:09:05,120 --> 00:09:08,880
schema and then sends over the

00:09:06,720 --> 00:09:10,240
serialized schema as part of each task

00:09:08,880 --> 00:09:12,080
request

00:09:10,240 --> 00:09:13,360
this design has the advantage that

00:09:12,080 --> 00:09:15,839
prevents high medister

00:09:13,360 --> 00:09:19,279
or any service serving schemas do not

00:09:15,839 --> 00:09:21,920
get bombarded with requests from workers

00:09:19,279 --> 00:09:24,880
however this leads to overwhelming of

00:09:21,920 --> 00:09:28,000
network and coordinator memory

00:09:24,880 --> 00:09:31,200
when schemas are very large this can

00:09:28,000 --> 00:09:32,480
easily happen which was the case for us

00:09:31,200 --> 00:09:34,080
theft scheme was a primary way of

00:09:32,480 --> 00:09:35,040
defining and maintaining schemas at

00:09:34,080 --> 00:09:38,160
printers

00:09:35,040 --> 00:09:40,320
we also support users users to create

00:09:38,160 --> 00:09:41,200
hive tables by just pointing to a thrift

00:09:40,320 --> 00:09:43,600
schema

00:09:41,200 --> 00:09:45,360
we support all the schema evaluation

00:09:43,600 --> 00:09:47,839
rules supported by their schema on hive

00:09:45,360 --> 00:09:49,680
tables as well

00:09:47,839 --> 00:09:50,959
this makes it very easy for people to

00:09:49,680 --> 00:09:53,440
define new schemas

00:09:50,959 --> 00:09:54,160
uh and then use them from end to end

00:09:53,440 --> 00:09:56,399
like in the

00:09:54,160 --> 00:09:59,519
in the online services and for offline

00:09:56,399 --> 00:10:01,680
data processing as well

00:09:59,519 --> 00:10:03,920
however with threat schema that is very

00:10:01,680 --> 00:10:04,880
easy for our schemas to grow in size

00:10:03,920 --> 00:10:06,880
over a period of time

00:10:04,880 --> 00:10:08,240
schema that printers have grown grown

00:10:06,880 --> 00:10:10,959
significantly

00:10:08,240 --> 00:10:12,640
and has challenged various components of

00:10:10,959 --> 00:10:16,480
our data platform to rethink

00:10:12,640 --> 00:10:16,480
schema handling including presto

00:10:17,760 --> 00:10:21,440
we distribute theft schemas at pinterest

00:10:19,920 --> 00:10:24,880
as jar

00:10:21,440 --> 00:10:26,800
artifacts these jars are distributed and

00:10:24,880 --> 00:10:28,880
loaded by all services that need to

00:10:26,800 --> 00:10:32,000
access these schemas

00:10:28,880 --> 00:10:34,000
impresto coordinators and workers

00:10:32,000 --> 00:10:35,920
all have these jars loaded during

00:10:34,000 --> 00:10:38,480
startup time

00:10:35,920 --> 00:10:40,560
this made us think that why do we even

00:10:38,480 --> 00:10:41,200
need to send these large schemas as part

00:10:40,560 --> 00:10:44,000
of

00:10:41,200 --> 00:10:44,000
task requests

00:10:44,480 --> 00:10:48,160
we modify task requests to replace

00:10:46,240 --> 00:10:50,480
serialized schemas with just

00:10:48,160 --> 00:10:52,079
thrift class names and made workers

00:10:50,480 --> 00:10:55,440
reconstruct thrift schemas

00:10:52,079 --> 00:10:57,600
from thrift class names they solved a

00:10:55,440 --> 00:11:00,399
bunch of issues we were seeing along

00:10:57,600 --> 00:11:01,279
coordinator out of memory errors and

00:11:00,399 --> 00:11:02,880
increase

00:11:01,279 --> 00:11:05,600
increase our presta cluster throughput

00:11:02,880 --> 00:11:05,600
tremendously

00:11:07,760 --> 00:11:11,440
the next issue we struggle with is bad

00:11:10,800 --> 00:11:14,880
workers

00:11:11,440 --> 00:11:16,880
investor clusters

00:11:14,880 --> 00:11:18,880
presto gains a part of its efficiency

00:11:16,880 --> 00:11:20,480
and speed from the fact that it always

00:11:18,880 --> 00:11:24,480
has jvms up and

00:11:20,480 --> 00:11:26,720
ready to start running tasks on workers

00:11:24,480 --> 00:11:29,600
a single jvm is shared by multiple tasks

00:11:26,720 --> 00:11:32,399
from multiple queries on a presta worker

00:11:29,600 --> 00:11:33,760
to ensure slow tasks aren't slowing down

00:11:32,399 --> 00:11:36,160
all the tasks on a worker

00:11:33,760 --> 00:11:36,800
cluster uses multi-level feedback key

00:11:36,160 --> 00:11:38,640
however

00:11:36,800 --> 00:11:41,279
this leads to scenarios where slow tasks

00:11:38,640 --> 00:11:43,200
can accumulate over time

00:11:41,279 --> 00:11:45,440
as quick task would be prioritized and

00:11:43,200 --> 00:11:47,839
will cook finishly

00:11:45,440 --> 00:11:50,240
i'm sorry we'll finish we'll finish

00:11:47,839 --> 00:11:50,240
quickly

00:11:52,720 --> 00:11:56,160
iotas are a good example of slow task

00:11:55,040 --> 00:11:59,839
that can accumulate

00:11:56,160 --> 00:12:01,600
on a worker with all data on s3

00:11:59,839 --> 00:12:04,639
and the fact that s3 throttles down

00:12:01,600 --> 00:12:08,079
request when a prefix has hit hard

00:12:04,639 --> 00:12:09,200
leaf tasks can easily slow down slowness

00:12:08,079 --> 00:12:12,240
easily spreads

00:12:09,200 --> 00:12:14,079
from a slow worker to other workers

00:12:12,240 --> 00:12:15,279
waiting on the pages from from the slow

00:12:14,079 --> 00:12:17,440
worker

00:12:15,279 --> 00:12:18,959
and slowly entire clusters throughput

00:12:17,440 --> 00:12:21,040
gets down to zero

00:12:18,959 --> 00:12:23,279
we used to see this very frequently

00:12:21,040 --> 00:12:26,000
sometimes two to three times a day

00:12:23,279 --> 00:12:27,839
and the solution we had at that point

00:12:26,000 --> 00:12:31,279
just restart the coordinator

00:12:27,839 --> 00:12:34,399
which means killing all the queries

00:12:31,279 --> 00:12:37,040
not uh happy querying a place where we

00:12:34,399 --> 00:12:37,040
wanted to be

00:12:37,360 --> 00:12:40,560
solving this problem requires a good

00:12:39,200 --> 00:12:42,959
mechanism for detecting

00:12:40,560 --> 00:12:44,399
bad workers and a fair resolution

00:12:42,959 --> 00:12:47,760
mechanism when when

00:12:44,399 --> 00:12:50,480
when a bad work is detected

00:12:47,760 --> 00:12:52,639
today detect a bad worker we look at

00:12:50,480 --> 00:12:54,880
following criterias

00:12:52,639 --> 00:12:58,160
if a worker cpu utilization is lower

00:12:54,880 --> 00:13:00,160
than cluster's average cpu utilization

00:12:58,160 --> 00:13:01,200
if lots of queries are failing with

00:13:00,160 --> 00:13:02,720
internal errors

00:13:01,200 --> 00:13:05,519
indicating failure while talking to a

00:13:02,720 --> 00:13:08,000
particular worker

00:13:05,519 --> 00:13:10,160
if a worker has open file descriptors

00:13:08,000 --> 00:13:12,000
higher than a threshold

00:13:10,160 --> 00:13:14,079
we had to tune these thresholds a bit to

00:13:12,000 --> 00:13:16,480
get this right

00:13:14,079 --> 00:13:19,360
when any of these criterias are met a

00:13:16,480 --> 00:13:21,120
graceful shutdown is first attempted

00:13:19,360 --> 00:13:22,800
however failure to gracefully shut down

00:13:21,120 --> 00:13:25,200
a worker in a few attempts

00:13:22,800 --> 00:13:26,560
will lead to controller pressure control

00:13:25,200 --> 00:13:29,120
we talked about

00:13:26,560 --> 00:13:29,680
forcibly terminating the ec2 instance

00:13:29,120 --> 00:13:33,120
for

00:13:29,680 --> 00:13:34,720
dedicated workers or shutting down the

00:13:33,120 --> 00:13:36,959
kubernetes part

00:13:34,720 --> 00:13:39,279
for uh for workers hosted on kubernetes

00:13:36,959 --> 00:13:39,279
pods

00:13:41,760 --> 00:13:45,120
while talking about thrift schemas we

00:13:43,680 --> 00:13:46,399
mentioned the need for restarting

00:13:45,120 --> 00:13:49,279
clusters

00:13:46,399 --> 00:13:51,199
there are many other similar

00:13:49,279 --> 00:13:52,959
dependencies which rely on clustered key

00:13:51,199 --> 00:13:55,279
stocks

00:13:52,959 --> 00:13:56,079
these dependencies make need for cluster

00:13:55,279 --> 00:13:59,120
results

00:13:56,079 --> 00:13:59,120
an important operation

00:13:59,760 --> 00:14:03,519
we restart presto clusters each night

00:14:02,560 --> 00:14:06,480
due to

00:14:03,519 --> 00:14:07,760
these reasons however to support

00:14:06,480 --> 00:14:10,000
seamless querying experience

00:14:07,760 --> 00:14:10,800
graceful shutdowns and controlling query

00:14:10,000 --> 00:14:14,839
traffic

00:14:10,800 --> 00:14:17,839
is what we rely on on performing cluster

00:14:14,839 --> 00:14:17,839
restarts

00:14:18,000 --> 00:14:21,600
we utilize presto controller to perform

00:14:19,920 --> 00:14:23,360
rolling restarts of clusters

00:14:21,600 --> 00:14:25,040
and presser gateway to control query

00:14:23,360 --> 00:14:26,800
traffic

00:14:25,040 --> 00:14:28,720
most of the organizations who operate

00:14:26,800 --> 00:14:31,839
cluster at large scale

00:14:28,720 --> 00:14:31,839
have similar solutions

00:14:35,440 --> 00:14:39,040
even though we support graceful clusters

00:14:37,120 --> 00:14:40,160
restarts we still try to avoid plus

00:14:39,040 --> 00:14:43,040
restarts

00:14:40,160 --> 00:14:43,519
as much as we can as doesn't results

00:14:43,040 --> 00:14:47,279
mean

00:14:43,519 --> 00:14:47,279
lower throughput overall

00:14:47,920 --> 00:14:52,079
apart from the need to avoid restarting

00:14:49,600 --> 00:14:54,480
clusters we also want to ensure

00:14:52,079 --> 00:14:59,760
that same version of dependencies are

00:14:54,480 --> 00:15:03,120
loaded on coordinators and workers

00:14:59,760 --> 00:15:03,600
to do so we version the jars and include

00:15:03,120 --> 00:15:05,680
version

00:15:03,600 --> 00:15:06,800
in the node infos of coordinator and

00:15:05,680 --> 00:15:09,199
workers

00:15:06,800 --> 00:15:10,720
we broadcast coordinator node version to

00:15:09,199 --> 00:15:13,360
all workers

00:15:10,720 --> 00:15:14,800
we restart a worker only if java version

00:15:13,360 --> 00:15:18,399
does not match

00:15:14,800 --> 00:15:21,600
with coordinators advertised version

00:15:18,399 --> 00:15:22,480
while restarting worker pulls the right

00:15:21,600 --> 00:15:25,199
version

00:15:22,480 --> 00:15:26,079
uh write jars for that particular

00:15:25,199 --> 00:15:29,760
version

00:15:26,079 --> 00:15:29,760
as broadcasted by the coordinator

00:15:32,000 --> 00:15:35,040
so far we only talked about improving

00:15:33,759 --> 00:15:37,920
system

00:15:35,040 --> 00:15:40,240
however a lot is there to be achieved by

00:15:37,920 --> 00:15:43,360
enabling users to write better queries

00:15:40,240 --> 00:15:46,320
and enable them to diagnose issues with

00:15:43,360 --> 00:15:46,320
their queries and time

00:15:48,480 --> 00:15:52,399
for that we utilize a cluster warning

00:15:50,800 --> 00:15:53,519
system

00:15:52,399 --> 00:15:55,600
we utilize that to deliver

00:15:53,519 --> 00:15:56,480
recommendations to our users as soon as

00:15:55,600 --> 00:15:58,480
possible

00:15:56,480 --> 00:16:00,240
which means real time as soon as uh one

00:15:58,480 --> 00:16:02,880
of the criterias are hit

00:16:00,240 --> 00:16:04,720
we let the users know oh your query is

00:16:02,880 --> 00:16:06,000
doing badly or performed poorly because

00:16:04,720 --> 00:16:07,600
of this reason

00:16:06,000 --> 00:16:09,440
are you quitting failed because of this

00:16:07,600 --> 00:16:12,639
reason the

00:16:09,440 --> 00:16:15,040
these warnings are uh are in

00:16:12,639 --> 00:16:16,720
uh can be divided in two categories one

00:16:15,040 --> 00:16:20,560
is query authoring

00:16:16,720 --> 00:16:20,560
and the second one is query diagnosing

00:16:21,279 --> 00:16:24,560
some examples for query authoring

00:16:23,040 --> 00:16:27,360
warnings would be um

00:16:24,560 --> 00:16:28,959
if a user doesn't specify a partition

00:16:27,360 --> 00:16:32,480
predicate

00:16:28,959 --> 00:16:34,480
while quitting from a partition table we

00:16:32,480 --> 00:16:36,079
throw a warning and ask the users to

00:16:34,480 --> 00:16:37,759
seriously think about using a partition

00:16:36,079 --> 00:16:39,759
pretty good

00:16:37,759 --> 00:16:41,199
if the user is creating a very large

00:16:39,759 --> 00:16:42,800
amount of data set like hundreds of

00:16:41,199 --> 00:16:45,839
thousands of rows

00:16:42,800 --> 00:16:49,600
uh we ask them to

00:16:45,839 --> 00:16:52,480
to limit the number of output rows

00:16:49,600 --> 00:16:53,199
similarly for query diagnosing uh we

00:16:52,480 --> 00:16:56,639
have

00:16:53,199 --> 00:16:59,600
uh warnings that's that gets thrown when

00:16:56,639 --> 00:17:01,040
for example someone is going from a

00:16:59,600 --> 00:17:04,000
table

00:17:01,040 --> 00:17:05,919
uh which is not using karma file format

00:17:04,000 --> 00:17:09,039
and it's going from something like text

00:17:05,919 --> 00:17:10,880
our sequence file and we encourage them

00:17:09,039 --> 00:17:13,760
to use corner format

00:17:10,880 --> 00:17:13,760
version of the table

00:17:14,959 --> 00:17:18,799
or if like they we see that like there

00:17:16,880 --> 00:17:21,439
is a very high cpu consumption

00:17:18,799 --> 00:17:22,400
and or the user did not specify the

00:17:21,439 --> 00:17:25,839
right

00:17:22,400 --> 00:17:26,959
uh so script config uh we again remind

00:17:25,839 --> 00:17:28,640
them or like

00:17:26,959 --> 00:17:30,480
ask them to fix those kind of

00:17:28,640 --> 00:17:33,840
information as soon as we detect

00:17:30,480 --> 00:17:33,840
that's an issue

00:17:34,640 --> 00:17:38,799
and those were just some examples we are

00:17:36,799 --> 00:17:42,559
building on these warnings and

00:17:38,799 --> 00:17:44,720
uh we are trying to improve upon uh

00:17:42,559 --> 00:17:46,400
make our users better educated and in

00:17:44,720 --> 00:17:47,280
the process we are trying to reduce the

00:17:46,400 --> 00:17:50,559
support load

00:17:47,280 --> 00:17:51,840
on the platform team this is a

00:17:50,559 --> 00:17:54,880
screenshot of

00:17:51,840 --> 00:17:56,000
of one of the clients that we have it's

00:17:54,880 --> 00:17:59,280
called data hub it's

00:17:56,000 --> 00:18:00,559
recently open sourced and you can see

00:17:59,280 --> 00:18:03,760
the warnings here

00:18:00,559 --> 00:18:05,600
um so when a user is running the query

00:18:03,760 --> 00:18:06,960
uh while the query is running uh these

00:18:05,600 --> 00:18:09,039
warnings they come in

00:18:06,960 --> 00:18:10,640
and the users can see and they can

00:18:09,039 --> 00:18:11,679
decide to even kill the query and then

00:18:10,640 --> 00:18:15,200
like restart with

00:18:11,679 --> 00:18:17,600
after addressing these warnings but this

00:18:15,200 --> 00:18:19,039
definitely improved the type of queries

00:18:17,600 --> 00:18:22,640
we are seeing

00:18:19,039 --> 00:18:25,760
definitely reduce the query failure

00:18:22,640 --> 00:18:29,200
failures due to user errors

00:18:25,760 --> 00:18:31,440
or in some cases reduced our

00:18:29,200 --> 00:18:32,640
query failures due to uh insufficient

00:18:31,440 --> 00:18:35,360
resources

00:18:32,640 --> 00:18:37,600
uh things like that um so we are really

00:18:35,360 --> 00:18:41,520
optimistic about the opportunities this

00:18:37,600 --> 00:18:42,000
uh this opens up uh to work very closely

00:18:41,520 --> 00:18:45,039
with our

00:18:42,000 --> 00:18:46,559
users without uh without answering the

00:18:45,039 --> 00:18:47,280
same question again again for each and

00:18:46,559 --> 00:18:50,000
every

00:18:47,280 --> 00:18:50,000
individual user

00:18:52,240 --> 00:18:56,080
with diverse use cases being powered by

00:18:54,400 --> 00:18:57,840
cluster the query patterns and the

00:18:56,080 --> 00:18:58,640
resource requirements are very diverse

00:18:57,840 --> 00:19:00,080
too

00:18:58,640 --> 00:19:01,679
how to best serve those queries

00:19:00,080 --> 00:19:04,559
efficiently remains a challenge

00:19:01,679 --> 00:19:05,919
that we continue to strive to solve

00:19:04,559 --> 00:19:08,559
time-based variants

00:19:05,919 --> 00:19:10,640
are majorly seen on ad hoc usages

00:19:08,559 --> 00:19:12,799
however even for scheduled use cases the

00:19:10,640 --> 00:19:14,559
users tend to have durations on peak

00:19:12,799 --> 00:19:16,480
usage it's very common for you

00:19:14,559 --> 00:19:19,840
people to schedule their jobs to run at

00:19:16,480 --> 00:19:19,840
utc 0 for example

00:19:22,160 --> 00:19:26,480
the route traffic between different

00:19:24,880 --> 00:19:27,440
clusters even how to hawk and schedule

00:19:26,480 --> 00:19:30,480
clusters

00:19:27,440 --> 00:19:33,520
um to handle the traffic variants uh

00:19:30,480 --> 00:19:35,360
that's uh based on time uh

00:19:33,520 --> 00:19:37,760
we still try to maintain the slas for

00:19:35,360 --> 00:19:39,600
our scheduled queries though

00:19:37,760 --> 00:19:41,360
the analysis another thing that we do is

00:19:39,600 --> 00:19:42,640
like we publish the traffic patterns to

00:19:41,360 --> 00:19:46,000
the users and then

00:19:42,640 --> 00:19:46,799
um and then suggest them to schedule

00:19:46,000 --> 00:19:50,000
their queries

00:19:46,799 --> 00:19:53,440
in the in the slots time slots

00:19:50,000 --> 00:19:55,360
of low activity or low traffic the other

00:19:53,440 --> 00:19:58,080
variance that we see is in the resource

00:19:55,360 --> 00:19:59,280
requirement between users

00:19:58,080 --> 00:20:01,120
different users from different

00:19:59,280 --> 00:20:01,919
organizations have different level of

00:20:01,120 --> 00:20:05,520
resource

00:20:01,919 --> 00:20:08,559
requirements we used to have per user

00:20:05,520 --> 00:20:09,280
resource script and that did not work

00:20:08,559 --> 00:20:11,200
well

00:20:09,280 --> 00:20:14,640
it led to inefficient utilization of

00:20:11,200 --> 00:20:14,640
available resources

00:20:14,720 --> 00:20:18,480
to solve requirement variants between

00:20:16,799 --> 00:20:20,799
users we moved from

00:20:18,480 --> 00:20:21,760
per user-based resource restrictions to

00:20:20,799 --> 00:20:25,120
arc based

00:20:21,760 --> 00:20:27,120
resource restrictions through presto

00:20:25,120 --> 00:20:27,840
gateway we started providing visibility

00:20:27,120 --> 00:20:30,000
to users

00:20:27,840 --> 00:20:31,520
on who in their org is consuming most of

00:20:30,000 --> 00:20:34,799
the resources

00:20:31,520 --> 00:20:37,120
and this enables

00:20:34,799 --> 00:20:39,360
us to discuss usage within themselves

00:20:37,120 --> 00:20:41,280
without pulling in the platform team

00:20:39,360 --> 00:20:43,200
if the organizations need more resources

00:20:41,280 --> 00:20:44,559
we have a workflow to request that as

00:20:43,200 --> 00:20:47,039
well

00:20:44,559 --> 00:20:50,159
we also wanted users to have control on

00:20:47,039 --> 00:20:52,400
speed versus resource trade-offs

00:20:50,159 --> 00:20:53,760
we now allow users to specify query

00:20:52,400 --> 00:20:56,559
characteristics

00:20:53,760 --> 00:20:57,039
via a session property users can say

00:20:56,559 --> 00:20:59,919
that

00:20:57,039 --> 00:21:00,799
they their query is equipment and need

00:20:59,919 --> 00:21:03,679
very small

00:21:00,799 --> 00:21:04,960
amount of resources we prioritize those

00:21:03,679 --> 00:21:06,640
kind of queries

00:21:04,960 --> 00:21:08,559
users can also come in and say that

00:21:06,640 --> 00:21:10,640
their query is resource heavy

00:21:08,559 --> 00:21:12,480
and it is okay to run it slowly we

00:21:10,640 --> 00:21:16,080
provide higher source allocations

00:21:12,480 --> 00:21:18,320
uh limits for those queries

00:21:16,080 --> 00:21:20,799
but they are expected to run a little

00:21:18,320 --> 00:21:20,799
slower

00:21:21,280 --> 00:21:25,440
we also provide feedback to users

00:21:23,120 --> 00:21:27,280
through a warning if a particular query

00:21:25,440 --> 00:21:29,520
could have a better uh could have been

00:21:27,280 --> 00:21:32,320
better served as a specific character's

00:21:29,520 --> 00:21:34,080
characteristic specified for example if

00:21:32,320 --> 00:21:35,280
someone said oh my query should be in

00:21:34,080 --> 00:21:36,559
fast lane but

00:21:35,280 --> 00:21:38,400
actually it's consuming a lot of

00:21:36,559 --> 00:21:40,320
resources we actually

00:21:38,400 --> 00:21:41,679
uh ask them that hey you should have

00:21:40,320 --> 00:21:45,039
used expensive

00:21:41,679 --> 00:21:46,799
source curve in this case

00:21:45,039 --> 00:21:49,039
this is a snapshot of our cluster

00:21:46,799 --> 00:21:52,159
gateways resource groups page

00:21:49,039 --> 00:21:52,880
each enables us site which enables users

00:21:52,159 --> 00:21:54,960
to see

00:21:52,880 --> 00:21:56,000
how much resources various args are

00:21:54,960 --> 00:21:58,400
consuming

00:21:56,000 --> 00:21:59,919
they can even click through and find who

00:21:58,400 --> 00:22:01,120
from a particular org is consuming the

00:21:59,919 --> 00:22:03,679
most

00:22:01,120 --> 00:22:05,280
to make tester successful at pinterest

00:22:03,679 --> 00:22:06,640
we have invested in solving not just a

00:22:05,280 --> 00:22:08,320
system issues

00:22:06,640 --> 00:22:10,400
but also in building solutions to

00:22:08,320 --> 00:22:11,120
empower professor users to write better

00:22:10,400 --> 00:22:14,720
queries

00:22:11,120 --> 00:22:16,320
and understand issues with their queries

00:22:14,720 --> 00:22:18,559
this not only helped our users to

00:22:16,320 --> 00:22:21,600
improve their querying skills

00:22:18,559 --> 00:22:26,000
but also support load from uh

00:22:21,600 --> 00:22:27,520
from the cluster platform team

00:22:26,000 --> 00:22:29,440
will continue to make progress in both

00:22:27,520 --> 00:22:32,480
these directions to make cluster users

00:22:29,440 --> 00:22:32,480
happier at pinterest

00:22:32,960 --> 00:22:37,120
that is all i have thanks for listening

00:22:35,200 --> 00:22:41,840
through the talk so patiently

00:22:37,120 --> 00:22:41,840
we can now open for questions

00:22:47,520 --> 00:22:52,720
thank you ashish that was a great talk

00:22:51,280 --> 00:22:54,480
there's a lot of questions that you've

00:22:52,720 --> 00:22:57,200
been answering as the

00:22:54,480 --> 00:22:58,640
as the recording was going on as well um

00:22:57,200 --> 00:23:00,080
let's let's try to take another i think

00:22:58,640 --> 00:23:02,559
there's a lot of questions about the

00:23:00,080 --> 00:23:03,520
the graceful uh shutdown right and how

00:23:02,559 --> 00:23:06,080
that works

00:23:03,520 --> 00:23:07,600
um the relationship between uh the

00:23:06,080 --> 00:23:08,720
coordinator the workers

00:23:07,600 --> 00:23:10,240
when do you need to shut down the

00:23:08,720 --> 00:23:11,360
coordinator versus when do you need to

00:23:10,240 --> 00:23:12,880
shut down the workers

00:23:11,360 --> 00:23:14,880
and how that actually works maybe you

00:23:12,880 --> 00:23:18,159
can talk a little bit about that

00:23:14,880 --> 00:23:19,679
yeah um definitely so when we say gso

00:23:18,159 --> 00:23:22,000
shutdown there are two

00:23:19,679 --> 00:23:23,360
parts to it the first is gracefully

00:23:22,000 --> 00:23:26,720
shutting down the workers

00:23:23,360 --> 00:23:28,240
which is uh uh as and i say

00:23:26,720 --> 00:23:30,880
pointed out it's already supported in

00:23:28,240 --> 00:23:34,000
the open search uh presto as well

00:23:30,880 --> 00:23:35,919
um so basically shutting down uh

00:23:34,000 --> 00:23:37,440
workers is something we rely on to

00:23:35,919 --> 00:23:39,360
perform

00:23:37,440 --> 00:23:41,440
uh when we direct the bad workers at

00:23:39,360 --> 00:23:45,039
that point what we do is like we

00:23:41,440 --> 00:23:48,720
uh we start the case uh termination of

00:23:45,039 --> 00:23:49,679
those workers uh that gives uh gets a

00:23:48,720 --> 00:23:52,640
little room

00:23:49,679 --> 00:23:53,120
for that worker to breathe essentially

00:23:52,640 --> 00:23:55,360
just

00:23:53,120 --> 00:23:57,360
do what it is doing is without taking on

00:23:55,360 --> 00:24:00,000
the new task

00:23:57,360 --> 00:24:01,279
and uh once they are done uh finishing

00:24:00,000 --> 00:24:04,720
up the existing task

00:24:01,279 --> 00:24:07,840
uh they just like restart um

00:24:04,720 --> 00:24:08,880
and for the coordinator we even there we

00:24:07,840 --> 00:24:11,919
do the graceful

00:24:08,880 --> 00:24:12,480
restart and that's majorly uh something

00:24:11,919 --> 00:24:15,120
we do

00:24:12,480 --> 00:24:17,039
while doing a rolling restart um as i

00:24:15,120 --> 00:24:20,320
mentioned in the in the presentation

00:24:17,039 --> 00:24:24,640
uh one thing uh that we need

00:24:20,320 --> 00:24:26,720
at pinterest uh for uh for the schema

00:24:24,640 --> 00:24:27,760
or the table definitions is the thrift

00:24:26,720 --> 00:24:30,320
schemas

00:24:27,760 --> 00:24:31,919
which are uh which are loaded into the

00:24:30,320 --> 00:24:34,960
jvm memory uh

00:24:31,919 --> 00:24:37,279
or the class path uh during the during

00:24:34,960 --> 00:24:39,520
the coordinator startup time right

00:24:37,279 --> 00:24:40,960
and uh because of that reason and

00:24:39,520 --> 00:24:42,080
because the schemas keep changing over

00:24:40,960 --> 00:24:45,200
the period of day

00:24:42,080 --> 00:24:47,760
uh we try to refresh those jars

00:24:45,200 --> 00:24:48,880
at least once a day and usually in the

00:24:47,760 --> 00:24:50,799
night

00:24:48,880 --> 00:24:53,039
we restart all the clusters but we do

00:24:50,799 --> 00:24:56,400
the rolling restart of them so that

00:24:53,039 --> 00:24:58,320
we uh we don't affect that

00:24:56,400 --> 00:25:01,840
so during that point we do the rolling

00:24:58,320 --> 00:25:01,840
restart of the coordinators

00:25:03,279 --> 00:25:07,760
thank you that was that was helpful a

00:25:05,679 --> 00:25:10,080
couple more questions coming in so

00:25:07,760 --> 00:25:10,799
there's a question about multi-tenancy

00:25:10,080 --> 00:25:13,279
you see it

00:25:10,799 --> 00:25:14,960
in the chat there to handle you have

00:25:13,279 --> 00:25:16,400
many organizations you are

00:25:14,960 --> 00:25:18,720
uh handling all of those different

00:25:16,400 --> 00:25:19,760
organizations uh how do you support

00:25:18,720 --> 00:25:21,520
multi-tenancy

00:25:19,760 --> 00:25:23,679
uh perhaps is at a you know cluster

00:25:21,520 --> 00:25:24,880
isolation level perhaps uh you're doing

00:25:23,679 --> 00:25:25,840
something else maybe you can talk a

00:25:24,880 --> 00:25:28,640
little bit about it

00:25:25,840 --> 00:25:30,640
and then how do you provide slas to

00:25:28,640 --> 00:25:33,200
those organizations

00:25:30,640 --> 00:25:33,760
right that's a great question uh in fact

00:25:33,200 --> 00:25:35,760
after

00:25:33,760 --> 00:25:36,799
resolving a lot of the stability issues

00:25:35,760 --> 00:25:40,320
uh with prester

00:25:36,799 --> 00:25:43,919
uh now the the biggest problem we

00:25:40,320 --> 00:25:47,600
we spend our times on is how to

00:25:43,919 --> 00:25:49,120
uh do the resource uh uh grouping on the

00:25:47,600 --> 00:25:51,440
social side solution for these different

00:25:49,120 --> 00:25:54,799
operations essentially multi-tenancy

00:25:51,440 --> 00:25:58,640
uh problem so what we do is like uh we

00:25:54,799 --> 00:26:02,400
have a tester gateway uh layer in front

00:25:58,640 --> 00:26:04,400
of all the presser clusters and

00:26:02,400 --> 00:26:06,480
all the clients they send their queries

00:26:04,400 --> 00:26:09,200
to uh presser gateway

00:26:06,480 --> 00:26:10,720
and while this in the query they also

00:26:09,200 --> 00:26:11,679
have to provide their credentials and

00:26:10,720 --> 00:26:16,000
like uh

00:26:11,679 --> 00:26:19,440
their information like which uh arg

00:26:16,000 --> 00:26:21,440
this user belongs to uh and then we also

00:26:19,440 --> 00:26:21,760
keep track of like what kind of query is

00:26:21,440 --> 00:26:24,720
it

00:26:21,760 --> 00:26:25,919
uh is it a scheduled query or is it a ad

00:26:24,720 --> 00:26:29,360
hoc query

00:26:25,919 --> 00:26:32,159
uh so based on these uh these

00:26:29,360 --> 00:26:33,440
uh information and like we on the

00:26:32,159 --> 00:26:36,880
presser gateway we have

00:26:33,440 --> 00:26:39,360
a dynamic header uh based rule engine

00:26:36,880 --> 00:26:40,400
where we keep track of or likely to find

00:26:39,360 --> 00:26:43,440
rules uh

00:26:40,400 --> 00:26:45,279
that like okay if this uh for

00:26:43,440 --> 00:26:46,720
this user who is part of this

00:26:45,279 --> 00:26:50,000
organization

00:26:46,720 --> 00:26:52,720
how much resources are being consumed

00:26:50,000 --> 00:26:55,279
on a particular cluster uh for example

00:26:52,720 --> 00:26:57,360
like if a user from data og is

00:26:55,279 --> 00:26:58,559
asking to run ad hoc query then we check

00:26:57,360 --> 00:27:01,279
like oh how much of

00:26:58,559 --> 00:27:02,480
uh ad hoc clusters uh resources are

00:27:01,279 --> 00:27:05,760
already used by

00:27:02,480 --> 00:27:07,440
uh by the data arg if and we have like

00:27:05,760 --> 00:27:09,279
limits uh specified uh

00:27:07,440 --> 00:27:10,400
it's configurable and then like if we

00:27:09,279 --> 00:27:11,200
see that like it's already being

00:27:10,400 --> 00:27:14,240
utilized at the

00:27:11,200 --> 00:27:16,080
uh at the peak uh then we cue the start

00:27:14,240 --> 00:27:16,880
queueing the queries uh if not we just

00:27:16,080 --> 00:27:20,159
let it go

00:27:16,880 --> 00:27:23,440
uh to the clusters um so we

00:27:20,159 --> 00:27:26,880
have been doing uh arg level uh

00:27:23,440 --> 00:27:27,840
service grouping and uh before prior to

00:27:26,880 --> 00:27:30,720
that we were doing

00:27:27,840 --> 00:27:32,080
uh per user level uh service scripts and

00:27:30,720 --> 00:27:33,360
that didn't work well

00:27:32,080 --> 00:27:35,440
uh all probability service groups is

00:27:33,360 --> 00:27:37,760
like uh has been working well because we

00:27:35,440 --> 00:27:40,000
are able to identify which organizations

00:27:37,760 --> 00:27:42,320
uh for example like data scientists

00:27:40,000 --> 00:27:43,120
they still they depend on cluster a lot

00:27:42,320 --> 00:27:45,840
so for them

00:27:43,120 --> 00:27:47,520
it made us made sense uh that we allow

00:27:45,840 --> 00:27:48,240
them to run a little more expensive

00:27:47,520 --> 00:27:50,559
queries than

00:27:48,240 --> 00:27:51,919
than uh some of some of the other

00:27:50,559 --> 00:27:54,240
organizations

00:27:51,919 --> 00:27:55,120
so uh that has been working uh out well

00:27:54,240 --> 00:27:56,640
for us

00:27:55,120 --> 00:27:58,480
right and then there's some related

00:27:56,640 --> 00:28:00,559
questions right about real-time

00:27:58,480 --> 00:28:01,760
ad hoc discovery so they may be

00:28:00,559 --> 00:28:04,559
expensive as you said

00:28:01,760 --> 00:28:05,840
maybe data science uh the the fastlane

00:28:04,559 --> 00:28:08,480
queries uh that you

00:28:05,840 --> 00:28:10,159
bypass down um maybe talk a little bit

00:28:08,480 --> 00:28:13,600
more about that

00:28:10,159 --> 00:28:16,399
right for real time we don't really do

00:28:13,600 --> 00:28:17,840
uh any real time um we do have a druid

00:28:16,399 --> 00:28:20,960
uh integration uh

00:28:17,840 --> 00:28:24,399
so even there the expectation

00:28:20,960 --> 00:28:25,039
is we won't put any load on droid uh

00:28:24,399 --> 00:28:28,559
brokers

00:28:25,039 --> 00:28:30,399
uh or the coordinators we would just go

00:28:28,559 --> 00:28:31,919
create the segment files uh similar to

00:28:30,399 --> 00:28:34,880
twitter i think

00:28:31,919 --> 00:28:36,320
uh so we the uh the reason we added the

00:28:34,880 --> 00:28:39,200
the druid connector there

00:28:36,320 --> 00:28:40,720
was so that like we can like for example

00:28:39,200 --> 00:28:43,120
different engineering teams have

00:28:40,720 --> 00:28:44,320
sometimes they have to do uh debugging

00:28:43,120 --> 00:28:47,840
of uh

00:28:44,320 --> 00:28:49,600
and during that uh time they

00:28:47,840 --> 00:28:50,960
uh there were some incidents caused

00:28:49,600 --> 00:28:53,360
because they were

00:28:50,960 --> 00:28:55,120
using druid extensively and that caused

00:28:53,360 --> 00:28:58,320
it to miss some slas

00:28:55,120 --> 00:29:00,720
for the end users and so now

00:28:58,320 --> 00:29:02,320
those kind of use cases uh which are a

00:29:00,720 --> 00:29:04,960
little more extensive than what droid

00:29:02,320 --> 00:29:08,159
can support uh they go through presto

00:29:04,960 --> 00:29:08,960
uh but yeah we the presta use case at

00:29:08,159 --> 00:29:12,080
pinterest is

00:29:08,960 --> 00:29:15,279
uh mostly around how

00:29:12,080 --> 00:29:18,159
uh to supporting the

00:29:15,279 --> 00:29:19,440
interactive query as much as we can uh

00:29:18,159 --> 00:29:21,840
to

00:29:19,440 --> 00:29:22,559
uh like definitely like when we started

00:29:21,840 --> 00:29:26,000
the support

00:29:22,559 --> 00:29:27,600
uh we only started with like uh

00:29:26,000 --> 00:29:29,440
queries that were supposed to be

00:29:27,600 --> 00:29:32,480
finished within like half an hour

00:29:29,440 --> 00:29:34,080
uh but we kept getting like as far to

00:29:32,480 --> 00:29:35,440
extend the time period essentially run

00:29:34,080 --> 00:29:37,840
more heavy quiz

00:29:35,440 --> 00:29:40,559
uh we are now allowing people to run up

00:29:37,840 --> 00:29:42,159
to one hour of queries um but i think

00:29:40,559 --> 00:29:44,480
even that is not enough for people and

00:29:42,159 --> 00:29:46,480
then like we are constantly evaluating

00:29:44,480 --> 00:29:48,240
what we can do to speed up the queries

00:29:46,480 --> 00:29:50,720
is it more resources

00:29:48,240 --> 00:29:51,840
uh or should we go and let people run

00:29:50,720 --> 00:29:53,840
longer queries

00:29:51,840 --> 00:29:55,600
that's a constant process that we keep

00:29:53,840 --> 00:29:58,480
going yeah there's no

00:29:55,600 --> 00:29:59,039
not necessarily an easy answer to that

00:29:58,480 --> 00:30:00,480
one

00:29:59,039 --> 00:30:02,080
um there's a question about etl

00:30:00,480 --> 00:30:04,799
workloads so um

00:30:02,080 --> 00:30:06,559
you see here do you run edl workloads on

00:30:04,799 --> 00:30:08,159
your schedule query clusters as well is

00:30:06,559 --> 00:30:11,279
there a separate cluster for that

00:30:08,159 --> 00:30:14,000
how do you manage resources uh for that

00:30:11,279 --> 00:30:16,159
right uh so for etl clusters we

00:30:14,000 --> 00:30:16,960
officially don't support etl workloads

00:30:16,159 --> 00:30:22,640
on presta

00:30:16,960 --> 00:30:25,520
uh however uh not yet right not yet

00:30:22,640 --> 00:30:27,120
but users love president so much uh

00:30:25,520 --> 00:30:28,480
because it's so easy for them to like

00:30:27,120 --> 00:30:30,080
interactively develop the query on

00:30:28,480 --> 00:30:31,520
cluster and then like just to

00:30:30,080 --> 00:30:34,320
make it run on detail they have to

00:30:31,520 --> 00:30:35,440
transpose that onto sagebox equal or

00:30:34,320 --> 00:30:38,399
high

00:30:35,440 --> 00:30:40,320
so they they are workarounds that have

00:30:38,399 --> 00:30:42,000
been developed within pinterest uh

00:30:40,320 --> 00:30:44,000
where people can run them in edl for

00:30:42,000 --> 00:30:47,039
example like jupiter reports

00:30:44,000 --> 00:30:50,399
uh we have some uh in-house scheduler

00:30:47,039 --> 00:30:53,440
uh service called flow hub

00:30:50,399 --> 00:30:55,440
tableau so people have started

00:30:53,440 --> 00:30:57,600
scheduling their etl jobs there

00:30:55,440 --> 00:30:59,120
um so what we do is like whenever we

00:30:57,600 --> 00:31:01,760
have a scheduled job come in

00:30:59,120 --> 00:31:03,200
we have a separate dedicated cluster uh

00:31:01,760 --> 00:31:05,519
and we don't auto scale

00:31:03,200 --> 00:31:06,240
ski uh up or down uh that question

00:31:05,519 --> 00:31:09,279
because uh

00:31:06,240 --> 00:31:12,159
etl workloads are very predictable

00:31:09,279 --> 00:31:13,760
so because of that we are able to have a

00:31:12,159 --> 00:31:16,480
much better control or

00:31:13,760 --> 00:31:17,120
or like a visibility into how much of

00:31:16,480 --> 00:31:20,399
this

00:31:17,120 --> 00:31:21,679
cluster needs or just use and plan

00:31:20,399 --> 00:31:23,519
according to that

00:31:21,679 --> 00:31:24,880
yeah there's probably a lot of different

00:31:23,519 --> 00:31:26,960
configuration parameters would be

00:31:24,880 --> 00:31:29,120
completely different as well uh

00:31:26,960 --> 00:31:30,480
query execution times etcetera so it's

00:31:29,120 --> 00:31:33,360
isolated at the

00:31:30,480 --> 00:31:34,799
cluster level um is the plan for now uh

00:31:33,360 --> 00:31:37,440
great one last question

00:31:34,799 --> 00:31:38,720
um there's a question about governance

00:31:37,440 --> 00:31:40,320
and i think it is uh

00:31:38,720 --> 00:31:42,080
you know it's related with how you're

00:31:40,320 --> 00:31:46,000
handling authentication

00:31:42,080 --> 00:31:48,080
authorization a variety of those things

00:31:46,000 --> 00:31:49,600
at the presto level and does the gateway

00:31:48,080 --> 00:31:51,760
take care of some of that

00:31:49,600 --> 00:31:53,760
uh or do you just you know push it down

00:31:51,760 --> 00:31:57,039
uh to the presto coordinator

00:31:53,760 --> 00:31:58,080
uh yeah so this is something we are

00:31:57,039 --> 00:32:01,039
looking into right now

00:31:58,080 --> 00:32:01,440
as i said like all of our data is on s3

00:32:01,039 --> 00:32:04,720
um

00:32:01,440 --> 00:32:07,200
so when it comes to uh to

00:32:04,720 --> 00:32:07,840
data access control it becomes a little

00:32:07,200 --> 00:32:11,600
tricky

00:32:07,840 --> 00:32:14,080
um so because then you can

00:32:11,600 --> 00:32:16,080
we can integrate with uh with table

00:32:14,080 --> 00:32:19,200
access control mechanisms

00:32:16,080 --> 00:32:21,760
but then because our data is on s3 uh

00:32:19,200 --> 00:32:23,519
it becomes and this data set these data

00:32:21,760 --> 00:32:24,159
sets are also accessed by other systems

00:32:23,519 --> 00:32:27,039
like

00:32:24,159 --> 00:32:28,240
spark or hive or mapreduce and other

00:32:27,039 --> 00:32:31,279
other processing

00:32:28,240 --> 00:32:33,600
frameworks so though that unless we

00:32:31,279 --> 00:32:35,840
uh kind of lock down the data sets which

00:32:33,600 --> 00:32:36,880
is on s3 there is no point in securing

00:32:35,840 --> 00:32:40,080
uh data from

00:32:36,880 --> 00:32:43,200
from presto so we are

00:32:40,080 --> 00:32:46,320
uh there's an enhanced project uh

00:32:43,200 --> 00:32:48,240
where we we have something called cbs uh

00:32:46,320 --> 00:32:52,000
it's a credential vending service which

00:32:48,240 --> 00:32:54,399
uh gives uh essays token uh it's an aws

00:32:52,000 --> 00:32:58,240
sds token so with that you can

00:32:54,399 --> 00:33:02,080
lock uh what prefixes of your

00:32:58,240 --> 00:33:02,080
s3 bucket you're giving

00:33:02,559 --> 00:33:06,799
so we are utilizing that to give limited

00:33:06,000 --> 00:33:10,559
scoped

00:33:06,799 --> 00:33:13,279
access to data either via queries

00:33:10,559 --> 00:33:14,640
are through like direct access or like

00:33:13,279 --> 00:33:18,480
access from other

00:33:14,640 --> 00:33:18,480

YouTube URL: https://www.youtube.com/watch?v=G4Hr3890B64


