Title: The Practice of Presto & Alluxio in E-Commerce Big Data Platform - Wenjun Tao, JD.COM
Publication date: 2020-10-02
Playlist: PrestoCon 2020 - Virtual
Description: 
	The Practice of Presto & Alluxio in E-Commerce Big Data Platform - Wenjun Tao, JD.COM

Speakers: Wenjun Tao

JD.com is one of the largest e-commerce. In big data platform of JD.com, there are tens of thousands of nodes and tens of petabytes off-line data that require millions of spark and MapReduce jobs to process every day. As the main query engine, thousands of machines work as Presto nodes and Presto plays an import role in the field of In-place analysis and BI tools. Meanwhile, Alluxio is deployed to improve the performance of Presto. The practice of Presto & Alluxio in JD.com benefits a lot of engineers and analysts.
Captions: 
	00:00:00,399 --> 00:00:04,480
hello everyone my name is wenjinto a

00:00:02,720 --> 00:00:07,040
senior software engineer

00:00:04,480 --> 00:00:09,760
jd company i'm also the core member of

00:00:07,040 --> 00:00:12,240
our prosta team it is my honor to share

00:00:09,760 --> 00:00:15,280
our works on preston and lasso

00:00:12,240 --> 00:00:17,600
now let us divide into today's topic the

00:00:15,280 --> 00:00:21,840
practice of presto and a lassoing

00:00:17,600 --> 00:00:21,840
e-commerce big data platform

00:00:22,880 --> 00:00:30,400
i will show you from the four aspects

00:00:26,480 --> 00:00:31,039
first of all introduction of jd complex

00:00:30,400 --> 00:00:34,559
pdp

00:00:31,039 --> 00:00:38,480
architecture secondly

00:00:34,559 --> 00:00:42,079
practice with presto in bdp

00:00:38,480 --> 00:00:44,559
third place analyzer stack

00:00:42,079 --> 00:00:47,120
last but long least our going on

00:00:44,559 --> 00:00:49,680
exploration

00:00:47,120 --> 00:00:51,199
first of all i will give a brief

00:00:49,680 --> 00:00:54,399
introduction to bdp

00:00:51,199 --> 00:00:54,399
of jingdong

00:00:56,640 --> 00:01:01,680
kingdom is one of the largest e-commerce

00:00:59,120 --> 00:01:02,399
companies there are attempts of silence

00:01:01,680 --> 00:01:04,479
notes and

00:01:02,399 --> 00:01:07,280
tens of people of online dates which

00:01:04,479 --> 00:01:10,240
requires millions of

00:01:07,280 --> 00:01:12,159
smart and maple-reduced jobs to process

00:01:10,240 --> 00:01:15,200
every day

00:01:12,159 --> 00:01:16,960
and there are hundreds of pb dates and a

00:01:15,200 --> 00:01:20,000
test of pivot data in

00:01:16,960 --> 00:01:20,799
increase there are tens of business

00:01:20,000 --> 00:01:24,320
units

00:01:20,799 --> 00:01:27,759
hundreds of data models

00:01:24,320 --> 00:01:30,880
this is our bdp architecture

00:01:27,759 --> 00:01:35,200
we deploy hdfs as our

00:01:30,880 --> 00:01:38,479
under layer distributes file system

00:01:35,200 --> 00:01:43,759
hive plaster and spark serves as

00:01:38,479 --> 00:01:46,560
our computing stack yang serves as

00:01:43,759 --> 00:01:47,280
resource manager and job scheduler also

00:01:46,560 --> 00:01:50,880
works as

00:01:47,280 --> 00:01:53,439
a bridge works as bridge

00:01:50,880 --> 00:01:56,079
between hdfs and the computing

00:01:53,439 --> 00:01:56,079
frameworks

00:01:57,840 --> 00:02:04,640
well let's make sure you practice with

00:02:00,640 --> 00:02:07,360
presto in our production environment

00:02:04,640 --> 00:02:08,319
as is new into our plastic presto

00:02:07,360 --> 00:02:11,200
cluster

00:02:08,319 --> 00:02:12,160
consists of coordinates and multiple

00:02:11,200 --> 00:02:15,440
workers

00:02:12,160 --> 00:02:18,640
coordinates is responsible for

00:02:15,440 --> 00:02:22,160
reserving queries from clients

00:02:18,640 --> 00:02:25,360
analysis plan queries and

00:02:22,160 --> 00:02:27,760
inactive with higher methods and workers

00:02:25,360 --> 00:02:29,680
responsible for it data from their

00:02:27,760 --> 00:02:32,000
stores such as hdfs

00:02:29,680 --> 00:02:33,120
and processes and then returns the

00:02:32,000 --> 00:02:36,160
results to

00:02:33,120 --> 00:02:39,440
coordinates then according the return

00:02:36,160 --> 00:02:39,440
results to the client

00:02:39,840 --> 00:02:43,680
ignore to a top press store into our

00:02:42,319 --> 00:02:47,120
production environment

00:02:43,680 --> 00:02:49,519
we make some modifications since we use

00:02:47,120 --> 00:02:50,239
young as unified resource manager we

00:02:49,519 --> 00:02:53,440
deploy

00:02:50,239 --> 00:02:56,160
a customized

00:02:53,440 --> 00:02:56,800
preston year cluster in our production

00:02:56,160 --> 00:03:00,640
environment

00:02:56,800 --> 00:03:05,200
which is easy to maintaining and scaling

00:03:00,640 --> 00:03:07,280
user usa have high demands on venus and

00:03:05,200 --> 00:03:11,200
isolation so in play

00:03:07,280 --> 00:03:13,760
we implemented the job isolation

00:03:11,200 --> 00:03:14,239
job isolating schedule and to make the

00:03:13,760 --> 00:03:17,840
class

00:03:14,239 --> 00:03:23,519
more stable in addition we are we

00:03:17,840 --> 00:03:26,560
an erp authorization system with presto

00:03:23,519 --> 00:03:27,760
in order to allow dynamic management a

00:03:26,560 --> 00:03:31,200
powerful

00:03:27,760 --> 00:03:35,599
uh power server server was deployed

00:03:31,200 --> 00:03:38,480
and developed with this system

00:03:35,599 --> 00:03:40,640
with this system we can modify

00:03:38,480 --> 00:03:43,440
configurations of cluster

00:03:40,640 --> 00:03:46,000
at the runtime and will give a detail

00:03:43,440 --> 00:03:49,680
and i'll give a detailed exploration of

00:03:46,000 --> 00:03:49,680
this system nature

00:03:49,760 --> 00:03:54,400
according to the query feature we also

00:03:52,799 --> 00:03:57,920
developed the query

00:03:54,400 --> 00:03:57,920
results cache function

00:03:59,120 --> 00:04:03,439
this is our preston architecture with

00:04:02,799 --> 00:04:06,720
this

00:04:03,439 --> 00:04:06,720
with this unified

00:04:07,120 --> 00:04:11,840
with this unified resource manager we

00:04:09,920 --> 00:04:15,840
can dynamically scale the

00:04:11,840 --> 00:04:15,840
number of workers

00:04:16,000 --> 00:04:22,400
and well we benefit so much

00:04:19,440 --> 00:04:24,639
from it first it implements a set of

00:04:22,400 --> 00:04:25,360
physical resources around several types

00:04:24,639 --> 00:04:27,520
of

00:04:25,360 --> 00:04:29,120
applications we don't need to deploy

00:04:27,520 --> 00:04:32,880
plastic

00:04:29,120 --> 00:04:37,600
and plaster classes specifically just

00:04:32,880 --> 00:04:37,600
submit a young job secondly

00:04:38,080 --> 00:04:42,000
we can use the ammo to do flexibility

00:04:40,960 --> 00:04:45,360
scale the size

00:04:42,000 --> 00:04:48,400
of preston classes while managing

00:04:45,360 --> 00:04:50,560
results in a unified manner

00:04:48,400 --> 00:04:51,440
third we don't need to consider the

00:04:50,560 --> 00:04:54,479
resource

00:04:51,440 --> 00:04:57,680
compensation between the manager

00:04:54,479 --> 00:05:00,720
between manage and

00:04:57,680 --> 00:05:00,720
presto low

00:05:02,479 --> 00:05:11,840
here is our customers powerful

00:05:06,320 --> 00:05:11,840
system it can manage plugging

00:05:13,360 --> 00:05:16,960
for plaster classes that can be used to

00:05:16,320 --> 00:05:20,160
export

00:05:16,960 --> 00:05:20,160
query results

00:05:20,240 --> 00:05:24,160
collect to different data stores and

00:05:23,120 --> 00:05:26,220
more

00:05:24,160 --> 00:05:27,840
and the erp authorization

00:05:26,220 --> 00:05:30,400
[Music]

00:05:27,840 --> 00:05:31,120
is used to check the negligible query

00:05:30,400 --> 00:05:34,800
tasks

00:05:31,120 --> 00:05:37,280
and facilitate auditing of user query

00:05:34,800 --> 00:05:37,280
tasks

00:05:37,520 --> 00:05:42,560
with this with the dynamic configuration

00:05:40,240 --> 00:05:45,840
feature we can dynamically adjust

00:05:42,560 --> 00:05:48,880
to the routine luna and resource grouper

00:05:45,840 --> 00:05:53,199
of a job query jobs to offload

00:05:48,880 --> 00:05:56,240
jobs to different clusters

00:05:53,199 --> 00:05:59,199
in addition and to start

00:05:56,240 --> 00:06:00,960
all stopper plaster cluster power server

00:05:59,199 --> 00:06:07,120
can also expand

00:06:00,960 --> 00:06:09,919
or shrink the cluster

00:06:07,120 --> 00:06:10,880
this is our wordpress schedule with

00:06:09,919 --> 00:06:14,560
smart job

00:06:10,880 --> 00:06:18,000
isolation practice we use

00:06:14,560 --> 00:06:20,560
a presto resource group feature

00:06:18,000 --> 00:06:21,120
to limit the cpu and the memory usage of

00:06:20,560 --> 00:06:24,639
different

00:06:21,120 --> 00:06:27,440
users resource group yeah

00:06:24,639 --> 00:06:28,880
for some test jobs and exam resources

00:06:27,440 --> 00:06:32,400
intensitive jobs

00:06:28,880 --> 00:06:32,400
we provide a specific

00:06:33,280 --> 00:06:40,639
specif a research group for these tasks

00:06:37,919 --> 00:06:42,560
and the jobs assigned to this group will

00:06:40,639 --> 00:06:46,639
run on a set of independent

00:06:42,560 --> 00:06:47,039
nodes to avoid the same jobs of effect

00:06:46,639 --> 00:06:51,360
and

00:06:47,039 --> 00:06:53,280
performing performance of the entire

00:06:51,360 --> 00:06:55,440
avoid stem jobs affecting the

00:06:53,280 --> 00:06:59,599
performance of the entire cluster

00:06:55,440 --> 00:07:03,520
this is our job as job isolation feature

00:06:59,599 --> 00:07:05,440
for resource group

00:07:03,520 --> 00:07:06,720
because learner managers run different

00:07:05,440 --> 00:07:11,039
types of services

00:07:06,720 --> 00:07:11,039
center they may cause works to file

00:07:11,599 --> 00:07:17,759
to perform query tasks to do this

00:07:15,280 --> 00:07:18,560
the work of her degree we post the

00:07:17,759 --> 00:07:22,400
status of

00:07:18,560 --> 00:07:24,000
loads to muscle and the master will

00:07:22,400 --> 00:07:27,840
temporarily

00:07:24,000 --> 00:07:31,840
exclude these workers loads to ensure

00:07:27,840 --> 00:07:31,840
stability of query tasks

00:07:33,039 --> 00:07:37,280
our presto class is many for practically

00:07:36,560 --> 00:07:40,800
queries

00:07:37,280 --> 00:07:45,520
these queries are used

00:07:40,800 --> 00:07:48,479
to general everyday reports

00:07:45,520 --> 00:07:49,440
the their features are controllable data

00:07:48,479 --> 00:07:52,400
range

00:07:49,440 --> 00:07:54,160
high freq high quality frequency high

00:07:52,400 --> 00:07:57,120
data reduce rates

00:07:54,160 --> 00:07:57,120
high proportion

00:07:57,360 --> 00:08:03,120
occurringly used with

00:08:00,560 --> 00:08:04,240
presto to make customer queries these

00:08:03,120 --> 00:08:07,199
features are just

00:08:04,240 --> 00:08:07,199
optimized

00:08:10,240 --> 00:08:13,599
according to the query feature we

00:08:12,319 --> 00:08:16,960
developed the query

00:08:13,599 --> 00:08:19,919
stock edge function the advantage is

00:08:16,960 --> 00:08:20,639
advantages are real recalculation for

00:08:19,919 --> 00:08:23,599
workers

00:08:20,639 --> 00:08:27,120
return the query results quickly renew

00:08:23,599 --> 00:08:30,319
cluster responder

00:08:27,120 --> 00:08:33,440
first step we develop a case cache

00:08:30,319 --> 00:08:34,800
function based on ttl or into so many

00:08:33,440 --> 00:08:38,000
users there will

00:08:34,800 --> 00:08:38,719
be many identical circle in the same

00:08:38,000 --> 00:08:41,760
time

00:08:38,719 --> 00:08:46,320
periods however if table updates

00:08:41,760 --> 00:08:50,000
by users the cash results may be dirty

00:08:46,320 --> 00:08:53,120
so we develop a cash

00:08:50,000 --> 00:08:55,200
function based on the high math store

00:08:53,120 --> 00:08:56,480
according to the last modification of

00:08:55,200 --> 00:08:59,680
time our main states

00:08:56,480 --> 00:09:00,640
we can judge judge whether date has been

00:08:59,680 --> 00:09:03,519
updated

00:09:00,640 --> 00:09:04,959
so as to determine whether the cash is

00:09:03,519 --> 00:09:09,279
invalid

00:09:04,959 --> 00:09:12,480
however uh there's your same room

00:09:09,279 --> 00:09:13,200
for um for improvements in cash hit

00:09:12,480 --> 00:09:17,040
rates

00:09:13,200 --> 00:09:21,839
so we developed a query pre-executes

00:09:17,040 --> 00:09:26,480
such as and predictably executed the

00:09:21,839 --> 00:09:26,480
frequency queries

00:09:27,519 --> 00:09:31,040
this graph shows the query status of

00:09:30,399 --> 00:09:34,880
last

00:09:31,040 --> 00:09:38,160
6 months there are about

00:09:34,880 --> 00:09:43,040
1 million queries every day and

00:09:38,160 --> 00:09:43,040
each query costs about 5 seconds

00:09:45,920 --> 00:09:50,000
oh let's begin with the important part

00:09:48,800 --> 00:09:53,120
of our presentation

00:09:50,000 --> 00:09:56,959
today to speed up our

00:09:53,120 --> 00:09:59,839
queries and make our servers more stable

00:09:56,959 --> 00:10:01,519
we have deployed a lasso in our presta

00:09:59,839 --> 00:10:04,800
production environment

00:10:01,519 --> 00:10:07,440
two years ago then i talked about our

00:10:04,800 --> 00:10:07,440
experience

00:10:07,839 --> 00:10:11,519
alastair is a virtual distributor file

00:10:10,800 --> 00:10:14,640
system that

00:10:11,519 --> 00:10:18,240
unifies data access between storage

00:10:14,640 --> 00:10:20,640
and computing and the offers memory

00:10:18,240 --> 00:10:22,240
speech performance while accessing

00:10:20,640 --> 00:10:25,519
remotes

00:10:22,240 --> 00:10:29,279
while or when working on remote states

00:10:25,519 --> 00:10:32,480
we have developed also in production and

00:10:29,279 --> 00:10:35,680
test environments including plus lasso

00:10:32,480 --> 00:10:38,880
and job history on alaska to access

00:10:35,680 --> 00:10:42,000
user query and

00:10:38,880 --> 00:10:42,000
login analysis

00:10:45,600 --> 00:10:49,760
take advantage of a lasso's caching

00:10:48,079 --> 00:10:53,200
feature we can achieve high

00:10:49,760 --> 00:10:53,920
quality throughputs different number of

00:10:53,200 --> 00:10:57,600
jobs are

00:10:53,920 --> 00:10:58,839
running at different times so our hdfs

00:10:57,600 --> 00:11:02,320
class have

00:10:58,839 --> 00:11:05,519
inconsistent response time

00:11:02,320 --> 00:11:07,440
after we deployed a lasso we get a

00:11:05,519 --> 00:11:10,640
consistent

00:11:07,440 --> 00:11:11,760
um low querying agency furthermore

00:11:10,640 --> 00:11:14,480
benefits from

00:11:11,760 --> 00:11:15,440
caching policy we also eliminate network

00:11:14,480 --> 00:11:19,040
traffic

00:11:15,440 --> 00:11:20,320
that that's we is that's we think it is

00:11:19,040 --> 00:11:23,519
based to run past

00:11:20,320 --> 00:11:23,519
and also together

00:11:25,040 --> 00:11:28,560
first let me introduce our improvements

00:11:27,279 --> 00:11:31,519
to a lasso

00:11:28,560 --> 00:11:33,760
these are some features contributed by

00:11:31,519 --> 00:11:38,720
jindo

00:11:33,760 --> 00:11:42,160
such as adding new web ui features

00:11:38,720 --> 00:11:47,440
improving short commands

00:11:42,160 --> 00:11:47,440
checking fire questions consistency

00:11:47,760 --> 00:11:52,560
what's a victor strategy

00:11:52,880 --> 00:11:59,120
and jvm post monitor

00:11:56,079 --> 00:12:02,320
there are also sandbag fixes and

00:11:59,120 --> 00:12:05,200
tests last i'll give a brief

00:12:02,320 --> 00:12:08,399
introduction too much watermark eventual

00:12:05,200 --> 00:12:08,399
stretch in the cache

00:12:08,839 --> 00:12:12,720
consistency

00:12:10,800 --> 00:12:14,160
there are two models of enriching a

00:12:12,720 --> 00:12:17,200
lasso

00:12:14,160 --> 00:12:20,079
and synchronized in a synchronized

00:12:17,200 --> 00:12:20,880
for synchronized module each time us

00:12:20,079 --> 00:12:24,000
requires

00:12:20,880 --> 00:12:27,360
requests to load a

00:12:24,000 --> 00:12:30,399
file in the change whether the storage

00:12:27,360 --> 00:12:32,399
is is sufficient if it's not it's in

00:12:30,399 --> 00:12:36,160
enough

00:12:32,399 --> 00:12:37,680
it will release base according to a iu

00:12:36,160 --> 00:12:40,959
algorithm

00:12:37,680 --> 00:12:44,240
whether workloads are right heavy they

00:12:40,959 --> 00:12:46,800
require meant to use the

00:12:44,240 --> 00:12:48,959
synchronized erection a signal efficient

00:12:46,800 --> 00:12:52,240
renaissance are predictably

00:12:48,959 --> 00:12:55,519
space resource rates in each world crime

00:12:52,240 --> 00:12:55,519
to enrich the dates

00:12:56,079 --> 00:13:01,360
it waits until the work storage attorney

00:12:59,120 --> 00:13:04,160
reached which are configurable high

00:13:01,360 --> 00:13:05,120
water marker then it is the data based

00:13:04,160 --> 00:13:07,880
on

00:13:05,120 --> 00:13:10,480
it's a policy until it reaches the

00:13:07,880 --> 00:13:13,120
configurable row

00:13:10,480 --> 00:13:13,120
what mark

00:13:15,600 --> 00:13:20,959
to ensure that duty data is not rich we

00:13:18,800 --> 00:13:22,639
must maintain data consistency between

00:13:20,959 --> 00:13:25,760
alaska and

00:13:22,639 --> 00:13:29,519
hdfs there are three ways to trick

00:13:25,760 --> 00:13:34,240
fire consistency such as us to

00:13:29,519 --> 00:13:38,560
us call rpc api such as

00:13:34,240 --> 00:13:41,519
data fields get filing for list status

00:13:38,560 --> 00:13:41,519
and so

00:13:42,000 --> 00:13:49,040
second is a risk for api by risk

00:13:45,600 --> 00:13:53,120
view such as reload methods to trigger

00:13:49,040 --> 00:13:56,399
a lasso to renounce or metadata

00:13:53,120 --> 00:13:59,839
so when a lasso starts up it will check

00:13:56,399 --> 00:13:59,839
method consistency

00:14:00,480 --> 00:14:05,440
also will traverse all files in

00:14:02,959 --> 00:14:06,079
directory to determine whether the file

00:14:05,440 --> 00:14:08,720
exists

00:14:06,079 --> 00:14:09,360
and the file size and file modification

00:14:08,720 --> 00:14:11,839
time

00:14:09,360 --> 00:14:14,160
are consistent with the under file

00:14:11,839 --> 00:14:16,480
system

00:14:14,160 --> 00:14:17,279
the press download has leads to more

00:14:16,480 --> 00:14:20,000
than 10

00:14:17,279 --> 00:14:23,519
times performance to performance

00:14:20,000 --> 00:14:23,519
improvements on average

00:14:24,880 --> 00:14:31,839
alexa had

00:14:27,920 --> 00:14:33,279
been running in our production

00:14:31,839 --> 00:14:36,720
environment so

00:14:33,279 --> 00:14:37,199
on more than hundreds of loads for two

00:14:36,720 --> 00:14:40,000
years

00:14:37,199 --> 00:14:41,680
after use we use alaska and prostate

00:14:40,000 --> 00:14:45,279
cancer reserves

00:14:41,680 --> 00:14:48,880
have got many benefits such as

00:14:45,279 --> 00:14:49,680
high performance consistent no querying

00:14:48,880 --> 00:14:52,880
entity

00:14:49,680 --> 00:14:58,480
eliminates network traffic

00:14:52,880 --> 00:15:01,760
there are also other

00:14:58,480 --> 00:15:04,880
benefits what's more

00:15:01,760 --> 00:15:07,680
a lasso is a full tolerance black

00:15:04,880 --> 00:15:10,079
ball and automation component so we

00:15:07,680 --> 00:15:13,040
don't need to shut down cluster to

00:15:10,079 --> 00:15:13,680
upper grades hover cluster and we can

00:15:13,040 --> 00:15:16,480
reduce

00:15:13,680 --> 00:15:17,680
late work consumers by using and also

00:15:16,480 --> 00:15:21,360
external

00:15:17,680 --> 00:15:24,160
caching and capabilities

00:15:21,360 --> 00:15:27,839
to provide better support for ad hoc and

00:15:24,160 --> 00:15:33,040
real time stream computing

00:15:27,839 --> 00:15:33,040
please see the left of this page first

00:15:33,680 --> 00:15:38,800
before using a lasso and press the

00:15:36,320 --> 00:15:38,800
worker

00:15:38,880 --> 00:15:43,440
may read the data from remote data

00:15:40,959 --> 00:15:44,480
loading not proper energy it will cost

00:15:43,440 --> 00:15:47,519
many time for

00:15:44,480 --> 00:15:50,480
later work latency and then fill

00:15:47,519 --> 00:15:52,560
rights parts after using an i suppress

00:15:50,480 --> 00:15:55,839
the workers can read the cache data from

00:15:52,560 --> 00:15:55,839
a last store

00:15:56,480 --> 00:16:01,839
on the same load it can reduce remote

00:15:59,040 --> 00:16:04,240
rates from hdrs and isolates high loads

00:16:01,839 --> 00:16:06,079
steady loads waste cash which can

00:16:04,240 --> 00:16:09,360
improve access performance

00:16:06,079 --> 00:16:13,199
therefore lasso can bring local

00:16:09,360 --> 00:16:13,199
locality and isolation

00:16:13,839 --> 00:16:17,759
as we can see in this picture

00:16:17,839 --> 00:16:22,240
for the first time presto want to read

00:16:20,160 --> 00:16:23,199
data from an also but i'll also kind of

00:16:22,240 --> 00:16:26,639
find it in

00:16:23,199 --> 00:16:30,800
memory so i have to read it from

00:16:26,639 --> 00:16:30,800
i accelerated from hdfs

00:16:33,680 --> 00:16:39,600
instead of but after

00:16:36,959 --> 00:16:40,639
the first reading presto gets the data

00:16:39,600 --> 00:16:44,639
from

00:16:40,639 --> 00:16:48,079
from the memory uh lasso

00:16:44,639 --> 00:16:51,120
instead of instead of fro instead of

00:16:48,079 --> 00:16:53,759
from the disk of hdfs

00:16:51,120 --> 00:16:56,240
we change their prestological redis

00:16:53,759 --> 00:16:59,440
release so it can access

00:16:56,240 --> 00:17:00,079
hdf automatically when a last service is

00:16:59,440 --> 00:17:02,800
not

00:17:00,079 --> 00:17:03,279
available our worker also extends a lot

00:17:02,800 --> 00:17:06,480
and

00:17:03,279 --> 00:17:11,439
enhance the synchronize between a lasso

00:17:06,480 --> 00:17:11,439
and htfs consistency

00:17:11,520 --> 00:17:16,160
this press this is a presser client from

00:17:14,480 --> 00:17:19,600
commander 9 we excuse

00:17:16,160 --> 00:17:20,799
the same servo query query same circle

00:17:19,600 --> 00:17:24,640
query for many times

00:17:20,799 --> 00:17:27,679
into presto cluster

00:17:24,640 --> 00:17:29,039
the left one press the access hdf

00:17:27,679 --> 00:17:33,039
directory the right

00:17:29,039 --> 00:17:36,880
one president's used also as a memory

00:17:33,039 --> 00:17:36,880
crash a reason

00:17:37,520 --> 00:17:40,960
rectangle is a circle custom the left

00:17:40,160 --> 00:17:45,200
one should

00:17:40,960 --> 00:17:45,200
cost about 10 seconds every time

00:17:45,280 --> 00:17:50,160
and the bachelor rights one which after

00:17:49,120 --> 00:17:54,480
using a lasso it

00:17:50,160 --> 00:17:54,480
spent 20 seconds for the first time

00:17:55,120 --> 00:18:00,080
26 for the first time and

00:17:58,240 --> 00:18:02,400
but less than them sequencing the

00:18:00,080 --> 00:18:04,240
following

00:18:02,400 --> 00:18:06,720
the left is lower than the right

00:18:04,240 --> 00:18:07,120
obviously because we use a lasso to

00:18:06,720 --> 00:18:09,679
cache

00:18:07,120 --> 00:18:11,360
the table and the partition files so

00:18:09,679 --> 00:18:15,840
after first time i lost okay

00:18:11,360 --> 00:18:19,600
accelerates the queries

00:18:15,840 --> 00:18:20,640
this is another view it is press the web

00:18:19,600 --> 00:18:25,760
ui we can get

00:18:20,640 --> 00:18:25,760
the same results with the command line

00:18:25,919 --> 00:18:30,559
we can see the query cost from 10 10

00:18:28,720 --> 00:18:34,000
seconds to less than

00:18:30,559 --> 00:18:38,240
one less than one and

00:18:34,000 --> 00:18:38,240
second it is very exciting

00:18:40,720 --> 00:18:44,640
it's nine chart is described as the

00:18:43,840 --> 00:18:47,919
summer

00:18:44,640 --> 00:18:47,919
summary of these tests

00:18:50,080 --> 00:18:57,200
we can see the preston also

00:18:54,799 --> 00:18:58,400
can reduce the read time after first

00:18:57,200 --> 00:19:02,640
read and

00:18:58,400 --> 00:19:06,640
run much faster than the plaster cluster

00:19:02,640 --> 00:19:08,480
apparently we are exploring how to

00:19:06,640 --> 00:19:10,559
use presta and allows you to get a

00:19:08,480 --> 00:19:13,360
better performance

00:19:10,559 --> 00:19:14,080
okay i'll introduce some other

00:19:13,360 --> 00:19:18,160
challenges

00:19:14,080 --> 00:19:19,200
we are going on press the master loading

00:19:18,160 --> 00:19:21,919
balance here

00:19:19,200 --> 00:19:23,520
as the amount of data grows the class

00:19:21,919 --> 00:19:26,320
size becomes larger and

00:19:23,520 --> 00:19:26,799
the number of query stats increases must

00:19:26,320 --> 00:19:29,360
become

00:19:26,799 --> 00:19:31,280
the performance bottleneck to achieve

00:19:29,360 --> 00:19:32,080
low dependency how to improve her

00:19:31,280 --> 00:19:35,039
stories

00:19:32,080 --> 00:19:37,360
will be a challenger through three naval

00:19:35,039 --> 00:19:40,160
resource isolation

00:19:37,360 --> 00:19:43,240
the execution task running on the

00:19:40,160 --> 00:19:47,120
workers compete for resources

00:19:43,240 --> 00:19:50,160
expressionist jobs in the tester frame

00:19:47,120 --> 00:19:51,280
if we can restrict the execution tasker

00:19:50,160 --> 00:19:54,480
with secrets

00:19:51,280 --> 00:19:59,919
it will reduce the male mutual impaired

00:19:54,480 --> 00:20:02,559
monkeys unified large cluster

00:19:59,919 --> 00:20:03,679
large scale class help improve research

00:20:02,559 --> 00:20:06,799
out in addition

00:20:03,679 --> 00:20:07,600
in past year we have reduced the number

00:20:06,799 --> 00:20:10,720
of classes

00:20:07,600 --> 00:20:14,400
from more than

00:20:10,720 --> 00:20:17,120
100 to 20

00:20:14,400 --> 00:20:18,880
within ensure very efficient we will

00:20:17,120 --> 00:20:23,600
further increase the class

00:20:18,880 --> 00:20:23,600
size and reduce the number of classes

00:20:24,880 --> 00:20:33,440
this is also exploration explorable

00:20:29,520 --> 00:20:37,440
application and three scenarios

00:20:33,440 --> 00:20:39,840
stall map reduce spark shaft footage

00:20:37,440 --> 00:20:41,360
to reduce disk storage pressure and

00:20:39,840 --> 00:20:44,480
speed up access

00:20:41,360 --> 00:20:47,760
to the shaft dates third

00:20:44,480 --> 00:20:51,919
parting hdfs answering assaulting

00:20:47,760 --> 00:20:55,360
dedication to alaska we are going to put

00:20:51,919 --> 00:20:58,559
port custom orientation on our

00:20:55,360 --> 00:21:02,159
dfs to also third hdfs

00:20:58,559 --> 00:21:03,360
rbf or alassa we have tried to use hdr

00:21:02,159 --> 00:21:05,919
zoo's best favorite

00:21:03,360 --> 00:21:07,840
but the performance didn't meet our

00:21:05,919 --> 00:21:11,440
online requirements

00:21:07,840 --> 00:21:13,080
we found that also also has the

00:21:11,440 --> 00:21:15,280
forwarding compat

00:21:13,080 --> 00:21:16,880
compatibilities and hopefully that and

00:21:15,280 --> 00:21:20,799
also will perform bits

00:21:16,880 --> 00:21:30,880
let's that's what we are doing

00:21:20,799 --> 00:21:33,520
and that's all thank you for your time

00:21:30,880 --> 00:21:34,320
great thank you so much uh appreciate

00:21:33,520 --> 00:21:36,799
taking time

00:21:34,320 --> 00:21:37,520
any specific questions you want to get

00:21:36,799 --> 00:21:40,799
the chat

00:21:37,520 --> 00:21:42,480
since you're online okay we have

00:21:40,799 --> 00:21:44,559
about four minutes before the next

00:21:42,480 --> 00:21:47,840
speaker comes in

00:21:44,559 --> 00:21:50,960
okay i i okay explain what's uh erica

00:21:47,840 --> 00:21:51,760
asked us and his his question is also

00:21:50,960 --> 00:21:55,120
question

00:21:51,760 --> 00:21:57,120
the hdf update in alaska or is

00:21:55,120 --> 00:21:59,600
it is caching the circle results in

00:21:57,120 --> 00:22:04,000
alaska

00:21:59,600 --> 00:22:07,440
in our production environments um

00:22:04,000 --> 00:22:08,480
also used was used as former and caching

00:22:07,440 --> 00:22:12,559
the

00:22:08,480 --> 00:22:16,880
hdfs states and we catch the

00:22:12,559 --> 00:22:20,559
um circle results in the um

00:22:16,880 --> 00:22:23,919
looks db uh in the master node

00:22:20,559 --> 00:22:27,039
locally uh just as just at

00:22:23,919 --> 00:22:30,640
the master know the

00:22:27,039 --> 00:22:33,840
disk um or we can

00:22:30,640 --> 00:22:38,080
cache the circle results in space

00:22:33,840 --> 00:22:41,880
and so that's um multiple multiple

00:22:38,080 --> 00:22:44,880
cluster can share the query results

00:22:41,880 --> 00:22:44,880

YouTube URL: https://www.youtube.com/watch?v=5RXJ0-MTS-k


