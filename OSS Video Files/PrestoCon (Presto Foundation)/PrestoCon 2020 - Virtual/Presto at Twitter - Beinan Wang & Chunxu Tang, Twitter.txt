Title: Presto at Twitter - Beinan Wang & Chunxu Tang, Twitter
Publication date: 2020-09-30
Playlist: PrestoCon 2020 - Virtual
Description: 
	Presto at Twitter - Beinan Wang & Chunxu Tang, Twitter

Speakers: Chunxu Tang, Beinan Wang

At Twitter, engineers are maintaining 9 on-prem/on-cloud Presto clusters with over 3000 nodes. With the experience of developing and maintaining these clusters, Beinan and Chunxu would like to share the stories of team efforts to develop the Presto-Druid connector, empowering SQL queries for real-time data analytics; the Presto router service, consolidating Presto clusters to establish a federation system; and the query predictor service, introducing machine learning techniques to the Presto ecosystem.​​​​
Captions: 
	00:00:00,480 --> 00:00:06,319
hi everyone uh this is trenshu tan

00:00:03,280 --> 00:00:09,200
from the twitter presto team and today

00:00:06,319 --> 00:00:09,920
one of my colleagues fenan and i will

00:00:09,200 --> 00:00:12,719
give you

00:00:09,920 --> 00:00:13,920
folks uh just a tech talk about the

00:00:12,719 --> 00:00:16,640
presto at twitter

00:00:13,920 --> 00:00:17,840
which are about some stories pursuing

00:00:16,640 --> 00:00:20,960
for scalability

00:00:17,840 --> 00:00:24,080
and availability at twitter so

00:00:20,960 --> 00:00:26,880
here are three major parts

00:00:24,080 --> 00:00:27,519
of our preventive presentation today at

00:00:26,880 --> 00:00:29,279
first

00:00:27,519 --> 00:00:30,960
we will we i would like to talk about

00:00:29,279 --> 00:00:32,800
the story of president twitter

00:00:30,960 --> 00:00:34,640
i will introduce some basic information

00:00:32,800 --> 00:00:36,880
of presto clusters

00:00:34,640 --> 00:00:38,160
and then i will talk about the story of

00:00:36,880 --> 00:00:41,360
queer predictor

00:00:38,160 --> 00:00:43,600
which is a quite innovative

00:00:41,360 --> 00:00:46,000
service that applies machine learning

00:00:43,600 --> 00:00:49,039
techniques to try to predict the cost

00:00:46,000 --> 00:00:51,760
of each crystal query and finally they

00:00:49,039 --> 00:00:52,960
now will introduce a story of the presto

00:00:51,760 --> 00:00:55,920
juice connector

00:00:52,960 --> 00:00:58,079
which will empower the sql queries on

00:00:55,920 --> 00:01:01,199
real-time analytics

00:00:58,079 --> 00:01:03,199
so let's begin so first of all i will

00:01:01,199 --> 00:01:03,920
talk about the story of preschool and

00:01:03,199 --> 00:01:06,640
twitter

00:01:03,920 --> 00:01:07,840
which is actually a long path pursuing

00:01:06,640 --> 00:01:11,200
for scalability

00:01:07,840 --> 00:01:12,080
and availability so here are some basic

00:01:11,200 --> 00:01:15,040
statistics

00:01:12,080 --> 00:01:17,520
of presto clusters at twitter we have

00:01:15,040 --> 00:01:20,159
but now we have more than nine clusters

00:01:17,520 --> 00:01:21,200
including on-prem clusters and on gcp

00:01:20,159 --> 00:01:24,240
clusters

00:01:21,200 --> 00:01:27,439
we have more than 3000 pressed workers

00:01:24,240 --> 00:01:28,159
and for now uh all the presto clusters

00:01:27,439 --> 00:01:30,799
have

00:01:28,159 --> 00:01:33,119
ram more than 10 million presto sql

00:01:30,799 --> 00:01:35,840
queries

00:01:33,119 --> 00:01:36,400
and to maintain and develop so large

00:01:35,840 --> 00:01:39,680
presto

00:01:36,400 --> 00:01:40,159
clusters we have set up a framework

00:01:39,680 --> 00:01:43,119
called

00:01:40,159 --> 00:01:44,320
presto calibration where we have a

00:01:43,119 --> 00:01:46,720
transfer router

00:01:44,320 --> 00:01:47,920
sitting between the client and the press

00:01:46,720 --> 00:01:50,720
of customers

00:01:47,920 --> 00:01:51,759
so one client would like to query a

00:01:50,720 --> 00:01:54,720
presto

00:01:51,759 --> 00:01:56,240
cluster he or she will first send the

00:01:54,720 --> 00:01:59,200
query to the password router

00:01:56,240 --> 00:02:00,719
and the router will will tell the

00:01:59,200 --> 00:02:04,079
specific cluster

00:02:00,719 --> 00:02:04,960
to query so in that case it will give us

00:02:04,079 --> 00:02:07,119
the benefits

00:02:04,960 --> 00:02:08,560
regarding scalar scalability and

00:02:07,119 --> 00:02:10,879
availability

00:02:08,560 --> 00:02:13,360
for scalability it makes it very easy to

00:02:10,879 --> 00:02:16,560
be stilled horizontally we can have

00:02:13,360 --> 00:02:18,480
multiple presto clusters at one time

00:02:16,560 --> 00:02:21,599
and also at the same time the reaching

00:02:18,480 --> 00:02:23,520
queue will be splitted to sub-clusters

00:02:21,599 --> 00:02:26,000
and also at the same time we have

00:02:23,520 --> 00:02:29,200
provided aggregated ui

00:02:26,000 --> 00:02:33,040
and apis to to customers so

00:02:29,200 --> 00:02:36,080
for the availability part as we have

00:02:33,040 --> 00:02:38,000
multiple presto clusters

00:02:36,080 --> 00:02:39,519
it and we also apply the rolling

00:02:38,000 --> 00:02:42,000
deployment strategy

00:02:39,519 --> 00:02:44,239
in that case even though at one time

00:02:42,000 --> 00:02:47,280
maybe one specific press or cluster

00:02:44,239 --> 00:02:50,239
is offline but the client can still

00:02:47,280 --> 00:02:52,720
carry other press or clusters so it can

00:02:50,239 --> 00:02:55,760
also help us to improve the availability

00:02:52,720 --> 00:02:59,040
of our preschool ecosystem so for now

00:02:55,760 --> 00:03:00,560
we have open source all the source code

00:02:59,040 --> 00:03:02,800
of the preschool router

00:03:00,560 --> 00:03:04,080
so here is uh we have put in all the

00:03:02,800 --> 00:03:07,760
code on the github

00:03:04,080 --> 00:03:07,760
so feel free to having the track

00:03:07,920 --> 00:03:12,959
at the same time we are applying some

00:03:10,879 --> 00:03:15,920
basic scheduling mechanism

00:03:12,959 --> 00:03:17,920
onto the presto router to schedule these

00:03:15,920 --> 00:03:20,400
queries across these

00:03:17,920 --> 00:03:20,959
threshold clusters so now there are two

00:03:20,400 --> 00:03:24,000
different

00:03:20,959 --> 00:03:26,560
mechanisms one is round-robin approach

00:03:24,000 --> 00:03:27,280
and the other is fixed routing why we

00:03:26,560 --> 00:03:29,599
have the fixed

00:03:27,280 --> 00:03:32,000
routing is that for some specific

00:03:29,599 --> 00:03:34,480
queries we may have a very specific

00:03:32,000 --> 00:03:35,599
cluster for example we have a presto

00:03:34,480 --> 00:03:38,080
tableau cluster

00:03:35,599 --> 00:03:39,200
which is especially created for tableau

00:03:38,080 --> 00:03:42,080
clients

00:03:39,200 --> 00:03:43,120
so we also find some problems about this

00:03:42,080 --> 00:03:45,360
kind of

00:03:43,120 --> 00:03:46,879
approach for example it may cause a

00:03:45,360 --> 00:03:50,720
starvation problem

00:03:46,879 --> 00:03:54,480
that we find we notice that sometimes

00:03:50,720 --> 00:03:57,360
we the most all of the queries are quite

00:03:54,480 --> 00:03:58,000
quite short which means that they could

00:03:57,360 --> 00:04:00,959
be

00:03:58,000 --> 00:04:01,280
completed in like in just a few minutes

00:04:00,959 --> 00:04:03,599
but

00:04:01,280 --> 00:04:04,480
we also observed that there are some

00:04:03,599 --> 00:04:07,680
quite large

00:04:04,480 --> 00:04:11,760
very resource consuming values which may

00:04:07,680 --> 00:04:14,720
cost maybe like a few hours to complete

00:04:11,760 --> 00:04:16,720
in that case this results consuming

00:04:14,720 --> 00:04:18,880
queries may occupy a lot of

00:04:16,720 --> 00:04:20,320
resources in one specific press or

00:04:18,880 --> 00:04:22,639
cluster but as

00:04:20,320 --> 00:04:25,040
we are using the real grouping approach

00:04:22,639 --> 00:04:27,520
some new requests are still forwarded

00:04:25,040 --> 00:04:29,840
to this cluster so it will cause some

00:04:27,520 --> 00:04:32,720
problems like width of resources

00:04:29,840 --> 00:04:34,479
and the very long wait waiting time and

00:04:32,720 --> 00:04:35,360
also this kind of information cancel

00:04:34,479 --> 00:04:38,240
views from

00:04:35,360 --> 00:04:39,280
the aggregated ui that we have provided

00:04:38,240 --> 00:04:42,639
from the presto

00:04:39,280 --> 00:04:45,919
router so to solve these problems

00:04:42,639 --> 00:04:47,919
we are working on an intelligent presto

00:04:45,919 --> 00:04:51,040
cluster management system

00:04:47,919 --> 00:04:53,280
where we added two other components

00:04:51,040 --> 00:04:55,280
to solve the problem one is query

00:04:53,280 --> 00:04:56,479
projector and the other is query

00:04:55,280 --> 00:04:58,880
scheduler

00:04:56,479 --> 00:05:01,600
for the query predictor this is a

00:04:58,880 --> 00:05:04,400
service to try to predict the cost

00:05:01,600 --> 00:05:05,759
for example the resource usage the cpu

00:05:04,400 --> 00:05:08,720
time or the peak

00:05:05,759 --> 00:05:10,639
memory bytes of a specific sql query and

00:05:08,720 --> 00:05:11,759
this information will be returned to the

00:05:10,639 --> 00:05:14,240
presta router

00:05:11,759 --> 00:05:15,919
and the presto router will utilize

00:05:14,240 --> 00:05:18,160
another component named

00:05:15,919 --> 00:05:20,240
query scheduler and the query scheduler

00:05:18,160 --> 00:05:22,800
at the same time will collect some

00:05:20,240 --> 00:05:25,199
cluster statistics from these clusters

00:05:22,800 --> 00:05:25,919
so in that case it can use this kind of

00:05:25,199 --> 00:05:28,720
information

00:05:25,919 --> 00:05:30,000
to schedule the queries across this

00:05:28,720 --> 00:05:33,120
custom press

00:05:30,000 --> 00:05:35,520
this clusters more intelligently so

00:05:33,120 --> 00:05:38,960
that's the whole picture of our design

00:05:35,520 --> 00:05:41,919
and this will lead to the net story

00:05:38,960 --> 00:05:42,639
of our presentation today which is about

00:05:41,919 --> 00:05:45,600
the story

00:05:42,639 --> 00:05:47,199
of query projector service at twitter so

00:05:45,600 --> 00:05:49,440
this is about the story about

00:05:47,199 --> 00:05:50,800
how to use machine learning techniques

00:05:49,440 --> 00:05:54,320
to predict the cost

00:05:50,800 --> 00:05:54,960
of a sql query so for the query

00:05:54,320 --> 00:05:57,600
predictor

00:05:54,960 --> 00:05:59,120
at twitter for now it has been used by

00:05:57,600 --> 00:06:02,160
many different services

00:05:59,120 --> 00:06:04,720
for example we have created the web app

00:06:02,160 --> 00:06:06,800
for this query predictor it can also be

00:06:04,720 --> 00:06:09,199
used by some bi tools

00:06:06,800 --> 00:06:10,639
at the same time it is used by some

00:06:09,199 --> 00:06:13,600
notebook tools

00:06:10,639 --> 00:06:15,919
at the same time absolutely it is used

00:06:13,600 --> 00:06:16,639
by the presto router to schedule these

00:06:15,919 --> 00:06:20,880
queries

00:06:16,639 --> 00:06:24,400
across these preschool classes

00:06:20,880 --> 00:06:28,080
and here this is the high level design

00:06:24,400 --> 00:06:30,479
of this service so first we have some

00:06:28,080 --> 00:06:32,960
historical queries and these queries

00:06:30,479 --> 00:06:34,160
have generated some request logs which

00:06:32,960 --> 00:06:36,479
installed

00:06:34,160 --> 00:06:38,479
and then with these presto logs we

00:06:36,479 --> 00:06:41,680
trained two different models

00:06:38,479 --> 00:06:44,560
with some machine learning algorithms

00:06:41,680 --> 00:06:45,199
so these two models are cpu model and a

00:06:44,560 --> 00:06:48,240
memory

00:06:45,199 --> 00:06:49,360
model here the cpu model is used to

00:06:48,240 --> 00:06:52,720
predict

00:06:49,360 --> 00:06:55,919
the cpu time of a specific sql query

00:06:52,720 --> 00:06:58,800
and the memory model is used to predict

00:06:55,919 --> 00:07:00,560
the peak memory bytes of a simple query

00:06:58,800 --> 00:07:03,440
and for now we have

00:07:00,560 --> 00:07:05,120
open sourced our code and it is in the

00:07:03,440 --> 00:07:08,240
twitter folk repository

00:07:05,120 --> 00:07:08,639
so here is a link of our project so feel

00:07:08,240 --> 00:07:11,120
free

00:07:08,639 --> 00:07:14,479
have a try of this project and any

00:07:11,120 --> 00:07:14,479
suggestion will be welcome

00:07:14,720 --> 00:07:17,840
and to solve the problem first we need

00:07:16,880 --> 00:07:19,360
to define

00:07:17,840 --> 00:07:20,880
what is the problem that we want to

00:07:19,360 --> 00:07:23,199
solve right

00:07:20,880 --> 00:07:24,880
so for each query we want to predict the

00:07:23,199 --> 00:07:28,240
cpu time and the peak

00:07:24,880 --> 00:07:29,360
memory bytes from our observation and

00:07:28,240 --> 00:07:32,960
our purposes

00:07:29,360 --> 00:07:35,039
we don't need to get a very specific

00:07:32,960 --> 00:07:38,160
value for cpu time or peak

00:07:35,039 --> 00:07:40,960
memory bytes what we need maybe see

00:07:38,160 --> 00:07:41,280
uh say whether this is a long query or

00:07:40,960 --> 00:07:43,680
like

00:07:41,280 --> 00:07:45,280
short query this is a very exciting you

00:07:43,680 --> 00:07:47,440
consuming query or

00:07:45,280 --> 00:07:49,599
this is not a secure consuming query

00:07:47,440 --> 00:07:51,599
right so in that case we don't

00:07:49,599 --> 00:07:52,639
we just need to put them into different

00:07:51,599 --> 00:07:55,680
categories

00:07:52,639 --> 00:07:58,400
for example for cpu time here we just

00:07:55,680 --> 00:07:59,520
put each query into four different

00:07:58,400 --> 00:08:01,919
categories

00:07:59,520 --> 00:08:02,960
like shorter than 30 seconds 30 seconds

00:08:01,919 --> 00:08:06,639
to 1 hour

00:08:02,960 --> 00:08:09,840
1 hour to 5 hour and larger than 5 hours

00:08:06,639 --> 00:08:12,560
for peak memory bytes we have 3 buckets

00:08:09,840 --> 00:08:13,840
smaller than 1 megabytes one megabytes

00:08:12,560 --> 00:08:17,039
to one gigabyte

00:08:13,840 --> 00:08:18,479
and larger than one tablet so in that

00:08:17,039 --> 00:08:21,759
case we convert

00:08:18,479 --> 00:08:22,479
a regression task into a classification

00:08:21,759 --> 00:08:24,879
task

00:08:22,479 --> 00:08:26,000
where we can apply apply different

00:08:24,879 --> 00:08:29,599
classifiers

00:08:26,000 --> 00:08:31,680
to train the models so here is the

00:08:29,599 --> 00:08:32,560
pipeline of the whole curve predictor

00:08:31,680 --> 00:08:34,800
service

00:08:32,560 --> 00:08:36,080
first we have some pressure logs and

00:08:34,800 --> 00:08:38,640
then we will apply

00:08:36,080 --> 00:08:39,599
some data discretization approach to

00:08:38,640 --> 00:08:42,000
convert them

00:08:39,599 --> 00:08:43,200
into some categories and then we'll do

00:08:42,000 --> 00:08:46,240
some data cleaning

00:08:43,200 --> 00:08:46,959
and then split the data set into a trend

00:08:46,240 --> 00:08:49,200
data set

00:08:46,959 --> 00:08:50,320
and the testing data set for the

00:08:49,200 --> 00:08:52,160
training data set

00:08:50,320 --> 00:08:54,240
we'll apply some vectorization

00:08:52,160 --> 00:08:57,519
techniques such as the tf

00:08:54,240 --> 00:08:58,560
idf term frequency inverse the document

00:08:57,519 --> 00:09:02,160
frequency

00:08:58,560 --> 00:09:04,240
which is a commonly used approach in

00:09:02,160 --> 00:09:05,760
information retrieval and natural

00:09:04,240 --> 00:09:08,240
language processing

00:09:05,760 --> 00:09:09,440
and then after this step we'll do some

00:09:08,240 --> 00:09:12,560
training

00:09:09,440 --> 00:09:13,680
here at twitter we especially use a

00:09:12,560 --> 00:09:17,200
mechanism called

00:09:13,680 --> 00:09:18,320
xg boost which is a gradient boosting

00:09:17,200 --> 00:09:20,880
decision tree

00:09:18,320 --> 00:09:21,360
approach in that case it will generate

00:09:20,880 --> 00:09:24,640
some

00:09:21,360 --> 00:09:27,279
models and then for the testing data set

00:09:24,640 --> 00:09:29,040
we'll do a data transformation based on

00:09:27,279 --> 00:09:32,480
the vectorizer we have

00:09:29,040 --> 00:09:34,720
used and then we'll do some evaluation

00:09:32,480 --> 00:09:36,880
on the models which have been trained

00:09:34,720 --> 00:09:39,279
from our platform

00:09:36,880 --> 00:09:40,240
so that's the whole picture of the query

00:09:39,279 --> 00:09:44,080
predictors

00:09:40,240 --> 00:09:46,000
service pipeline and then for the model

00:09:44,080 --> 00:09:49,279
evaluation we need to see like

00:09:46,000 --> 00:09:51,519
whether the the model we we get

00:09:49,279 --> 00:09:53,279
is good on knowledge right so there are

00:09:51,519 --> 00:09:55,519
some different metrics which could be

00:09:53,279 --> 00:09:58,640
used to evaluate a model

00:09:55,519 --> 00:10:01,440
so here first uh i would have introduced

00:09:58,640 --> 00:10:02,560
the accuracy to evaluate these models

00:10:01,440 --> 00:10:04,800
and we find that

00:10:02,560 --> 00:10:06,480
uh the models are pretty good for the

00:10:04,800 --> 00:10:10,640
cpu time the

00:10:06,480 --> 00:10:14,360
accuracy is as high as 94.2 percent

00:10:10,640 --> 00:10:17,200
and for the memory model the accuracy is

00:10:14,360 --> 00:10:20,000
92.4

00:10:17,200 --> 00:10:20,959
but here i also want to mention that

00:10:20,000 --> 00:10:23,440
accuracy

00:10:20,959 --> 00:10:25,680
is not always be the best metric to

00:10:23,440 --> 00:10:29,200
evaluate a machine learning model

00:10:25,680 --> 00:10:31,920
sometimes we also need to consider other

00:10:29,200 --> 00:10:33,440
metrics especially for some imbalanced

00:10:31,920 --> 00:10:35,440
datasets

00:10:33,440 --> 00:10:37,279
so for example here i also do the

00:10:35,440 --> 00:10:40,399
precessions and recalls

00:10:37,279 --> 00:10:43,200
to evaluate these two different models

00:10:40,399 --> 00:10:43,600
for example as we especially care about

00:10:43,200 --> 00:10:47,120
some

00:10:43,600 --> 00:10:49,839
really results consuming queries i i

00:10:47,120 --> 00:10:50,320
especially put uh the cpu time range

00:10:49,839 --> 00:10:53,200
like

00:10:50,320 --> 00:10:53,680
longer than five hours precession recall

00:10:53,200 --> 00:10:56,800
and one

00:10:53,680 --> 00:10:59,360
fps for here for an evaluation so

00:10:56,800 --> 00:11:00,800
overall with our approach we get a quite

00:10:59,360 --> 00:11:04,800
good

00:11:00,800 --> 00:11:04,800
models for prediction

00:11:05,360 --> 00:11:09,200
and also we have created a current

00:11:07,440 --> 00:11:12,079
predictor web app

00:11:09,200 --> 00:11:13,920
so here is uh we have provided a window

00:11:12,079 --> 00:11:14,720
and where the user can fill in some

00:11:13,920 --> 00:11:16,640
queries

00:11:14,720 --> 00:11:17,760
and then the user can just click the

00:11:16,640 --> 00:11:20,800
predict button

00:11:17,760 --> 00:11:23,839
to get some prediction of the cpu time

00:11:20,800 --> 00:11:27,120
and the peak memory bytes of it so

00:11:23,839 --> 00:11:29,600
we have also included this web

00:11:27,120 --> 00:11:31,680
application in the south code so feel

00:11:29,600 --> 00:11:35,279
free to have a try and see

00:11:31,680 --> 00:11:38,240
uh whether it can predict your queries

00:11:35,279 --> 00:11:39,920
okay so in the next part uh beyond we'll

00:11:38,240 --> 00:11:43,360
talk about the story of

00:11:39,920 --> 00:11:47,200
presto jewish connector thank you

00:11:43,360 --> 00:11:47,200
thank you yeah next page please

00:11:48,079 --> 00:11:51,440
yeah today i'm going to share the story

00:11:50,480 --> 00:11:54,480
about

00:11:51,440 --> 00:11:58,160
presto plus druid in twitter yeah

00:11:54,480 --> 00:12:00,639
in twitter we used to use the

00:11:58,160 --> 00:12:02,800
original version of the druid connector

00:12:00,639 --> 00:12:07,680
uh it works just like uh

00:12:02,800 --> 00:12:07,680
the price to have connector it used uh

00:12:07,760 --> 00:12:13,279
screw it as a metastar and it will scan

00:12:10,399 --> 00:12:15,760
the data on hdfs

00:12:13,279 --> 00:12:17,519
for this kind of architecture it's super

00:12:15,760 --> 00:12:20,639
good for the table scan

00:12:17,519 --> 00:12:23,600
but with this picture we cannot

00:12:20,639 --> 00:12:24,880
use it as a real-time analysis engine

00:12:23,600 --> 00:12:27,360
because

00:12:24,880 --> 00:12:29,279
we have we do some early research on

00:12:27,360 --> 00:12:32,079
this kind of things we found

00:12:29,279 --> 00:12:32,800
uh if we want to scan uh one gigabytes

00:12:32,079 --> 00:12:36,079
memory

00:12:32,800 --> 00:12:37,279
uh it will takes at least 250

00:12:36,079 --> 00:12:39,760
milliseconds

00:12:37,279 --> 00:12:40,800
yeah so that means if we just do the

00:12:39,760 --> 00:12:43,760
scan

00:12:40,800 --> 00:12:46,240
it's really difficult to reach the uh

00:12:43,760 --> 00:12:47,839
the latency within one second

00:12:46,240 --> 00:12:50,160
even though maybe some queries that

00:12:47,839 --> 00:12:50,720
small does get a very small amount of

00:12:50,160 --> 00:12:53,839
data

00:12:50,720 --> 00:12:56,959
you can still not achieve the

00:12:53,839 --> 00:13:00,320
the query within one second

00:12:56,959 --> 00:13:02,000
so uh we developed the second version of

00:13:00,320 --> 00:13:04,880
our druid connector

00:13:02,000 --> 00:13:06,959
it uses a different approach it pushes

00:13:04,880 --> 00:13:09,760
down the the aggregation

00:13:06,959 --> 00:13:11,200
and the filter from presto side to

00:13:09,760 --> 00:13:14,720
suicide

00:13:11,200 --> 00:13:17,920
the next page yeah

00:13:14,720 --> 00:13:20,320
so within this approach

00:13:17,920 --> 00:13:21,839
that means the on the price tall side we

00:13:20,320 --> 00:13:24,320
won't scan the data

00:13:21,839 --> 00:13:25,519
we uh take advantage of the the computer

00:13:24,320 --> 00:13:28,560
engine if we drew it

00:13:25,519 --> 00:13:32,240
we push down the sql query

00:13:28,560 --> 00:13:32,240
from presto to druid

00:13:32,560 --> 00:13:39,760
by doing that we can generate

00:13:36,480 --> 00:13:40,880
a c chord called druid sql dql uh from

00:13:39,760 --> 00:13:43,920
presto

00:13:40,880 --> 00:13:46,959
uh to join and we

00:13:43,920 --> 00:13:46,959
take advantage of the

00:13:47,040 --> 00:13:53,760
the presto connector

00:13:50,320 --> 00:13:57,680
plant optimizer it provides an interface

00:13:53,760 --> 00:14:00,639
uh just within our connector

00:13:57,680 --> 00:14:02,320
the the price will pass in uh the

00:14:00,639 --> 00:14:05,440
logical plan uh

00:14:02,320 --> 00:14:08,880
from coordinator to our

00:14:05,440 --> 00:14:11,040
uh druid connector the

00:14:08,880 --> 00:14:12,320
the logic plan does something uh on the

00:14:11,040 --> 00:14:14,000
left we have

00:14:12,320 --> 00:14:16,639
something like an aggregation node the

00:14:14,000 --> 00:14:19,760
filter node and the projection node

00:14:16,639 --> 00:14:20,160
during our optimizer we optimize this

00:14:19,760 --> 00:14:22,880
cut

00:14:20,160 --> 00:14:23,839
node we scratch this kind of node

00:14:22,880 --> 00:14:25,920
together as

00:14:23,839 --> 00:14:29,360
one table scan node actually in this

00:14:25,920 --> 00:14:30,959
node we just send a dql from presto to

00:14:29,360 --> 00:14:33,440
join

00:14:30,959 --> 00:14:34,560
that means the the most the computer

00:14:33,440 --> 00:14:36,959
part will

00:14:34,560 --> 00:14:38,160
push down to through the part the druid

00:14:36,959 --> 00:14:41,600
will use their own

00:14:38,160 --> 00:14:43,680
index uh to do the calculation

00:14:41,600 --> 00:14:45,600
so on jewish side it will just get a

00:14:43,680 --> 00:14:50,639
very small amount of data

00:14:45,600 --> 00:14:53,519
and return the results to presto

00:14:50,639 --> 00:14:55,120
and within this approach we can also

00:14:53,519 --> 00:14:58,720
make the the jury table

00:14:55,120 --> 00:14:59,360
join with some other table like hive

00:14:58,720 --> 00:15:02,880
table

00:14:59,360 --> 00:15:06,480
peanut table the mysql table

00:15:02,880 --> 00:15:07,120
the table used by other connectors you

00:15:06,480 --> 00:15:10,639
can just

00:15:07,120 --> 00:15:14,240
use their own optimizer

00:15:10,639 --> 00:15:17,040
do the similar optimize so uh

00:15:14,240 --> 00:15:17,760
that means that the presto just passed

00:15:17,040 --> 00:15:20,639
in

00:15:17,760 --> 00:15:22,639
the the logic plot of the druid part

00:15:20,639 --> 00:15:23,440
into the druid connector it will pass

00:15:22,639 --> 00:15:25,839
the

00:15:23,440 --> 00:15:28,720
uh the logic plan may be something like

00:15:25,839 --> 00:15:29,600
a peanut or mexico to the mysql peanut

00:15:28,720 --> 00:15:32,079
connector

00:15:29,600 --> 00:15:33,680
so uh this kind of optimization kind of

00:15:32,079 --> 00:15:36,560
working together

00:15:33,680 --> 00:15:38,480
so we can we can join the data from

00:15:36,560 --> 00:15:41,440
multiple data source and

00:15:38,480 --> 00:15:42,880
also achieve a very low latency

00:15:41,440 --> 00:15:45,519
requirement

00:15:42,880 --> 00:15:45,519
next page

00:15:47,279 --> 00:15:51,199
we also do something on the dc

00:15:49,920 --> 00:15:54,320
optimization

00:15:51,199 --> 00:15:57,360
actually we are high pay with g1 dc

00:15:54,320 --> 00:15:58,959
for years because it's really good for

00:15:57,360 --> 00:16:03,360
the

00:15:58,959 --> 00:16:06,800
large heap and high throughput

00:16:03,360 --> 00:16:10,240
the upper chart in yellow is the java 11

00:16:06,800 --> 00:16:11,199
with t1 tc we can see generally it's

00:16:10,240 --> 00:16:14,639
good but

00:16:11,199 --> 00:16:17,759
we have some huge specs with 2000

00:16:14,639 --> 00:16:19,839
milliseconds something like that so

00:16:17,759 --> 00:16:21,519
something this is not something not good

00:16:19,839 --> 00:16:25,519
for the real-time analysis

00:16:21,519 --> 00:16:28,560
so we uh involved the gtc

00:16:25,519 --> 00:16:28,959
uh to presto the lower chart in blue is

00:16:28,560 --> 00:16:32,000
the

00:16:28,959 --> 00:16:32,800
the dgc and we can see we have a less

00:16:32,000 --> 00:16:35,040
back

00:16:32,800 --> 00:16:36,800
less bikes and the larger specs that

00:16:35,040 --> 00:16:39,600
just

00:16:36,800 --> 00:16:41,519
60 milliseconds and the most of the dc

00:16:39,600 --> 00:16:44,240
time is below

00:16:41,519 --> 00:16:46,480
20 milliseconds so that's really good

00:16:44,240 --> 00:16:49,759
for the

00:16:46,480 --> 00:16:52,880
real-time analysis i'm not saying that

00:16:49,759 --> 00:16:54,320
this is always good because we also we

00:16:52,880 --> 00:16:57,920
can also see that

00:16:54,320 --> 00:17:00,959
we have five to ten times more

00:16:57,920 --> 00:17:03,519
dc count and we can also see the

00:17:00,959 --> 00:17:04,720
some swim food deduction but just for

00:17:03,519 --> 00:17:09,039
the real-time

00:17:04,720 --> 00:17:11,520
analysis we can use gtc to reduce the

00:17:09,039 --> 00:17:13,120
latency and we can get less back the

00:17:11,520 --> 00:17:15,679
latency

00:17:13,120 --> 00:17:15,679
next page

00:17:17,439 --> 00:17:22,319
yeah this is another task in the uh

00:17:20,480 --> 00:17:25,679
presto druid connector

00:17:22,319 --> 00:17:32,160
uh we also use uh presto to

00:17:25,679 --> 00:17:35,520
do some data ingestion uh to draw it um

00:17:32,160 --> 00:17:37,919
this work we can just write one sequel

00:17:35,520 --> 00:17:39,280
uh to collect data from other data

00:17:37,919 --> 00:17:43,120
sources like have

00:17:39,280 --> 00:17:45,520
varicose like mysql like peanuts and

00:17:43,120 --> 00:17:47,360
we can manipulate the data we can join

00:17:45,520 --> 00:17:51,520
the data with some other tables

00:17:47,360 --> 00:17:53,520
and then we can we can output this data

00:17:51,520 --> 00:17:56,799
to some storage

00:17:53,520 --> 00:18:00,880
uh like hdfs or tcs story and

00:17:56,799 --> 00:18:04,080
uh presto will call them

00:18:00,880 --> 00:18:06,720
some api called druid indexer

00:18:04,080 --> 00:18:09,360
which will load the data we just

00:18:06,720 --> 00:18:13,280
generated in parallel

00:18:09,360 --> 00:18:15,760
uh by doing so

00:18:13,280 --> 00:18:17,360
uh in twitter we can just uh create a

00:18:15,760 --> 00:18:20,720
true table or we can say

00:18:17,360 --> 00:18:21,440
inject data from uh have data warehouse

00:18:20,720 --> 00:18:24,559
tool

00:18:21,440 --> 00:18:28,080
to do it within just one sql yeah

00:18:24,559 --> 00:18:30,640
it's super easy to do the ingestion job

00:18:28,080 --> 00:18:30,640
next page

00:18:33,280 --> 00:18:38,240
the the original approach of the two

00:18:35,440 --> 00:18:41,120
ingestion just ask the user to

00:18:38,240 --> 00:18:42,240
prepare something like a injustice bag

00:18:41,120 --> 00:18:44,880
yeah

00:18:42,240 --> 00:18:46,000
but for the new playstore through the

00:18:44,880 --> 00:18:49,039
connector

00:18:46,000 --> 00:18:52,559
this bag can be just generated

00:18:49,039 --> 00:18:55,200
by the sequel if you select some colors

00:18:52,559 --> 00:18:56,160
from the data source it can help you to

00:18:55,200 --> 00:18:59,440
generate

00:18:56,160 --> 00:19:01,840
uh this slack it can generate the

00:18:59,440 --> 00:19:03,200
information for each column like the

00:19:01,840 --> 00:19:06,799
dimension color

00:19:03,200 --> 00:19:09,280
as well as the matrix colors so

00:19:06,799 --> 00:19:10,160
this kind of spec can be generated

00:19:09,280 --> 00:19:12,559
automatically

00:19:10,160 --> 00:19:13,280
by the price store through the connector

00:19:12,559 --> 00:19:15,520
and

00:19:13,280 --> 00:19:17,600
the the price of google connector will

00:19:15,520 --> 00:19:18,400
upload this back to through the query

00:19:17,600 --> 00:19:22,000
will

00:19:18,400 --> 00:19:25,039
read this back and load the data

00:19:22,000 --> 00:19:30,320
yeah i think that's all from my side

00:19:25,039 --> 00:19:30,320
and yeah we can go to the q a session

00:19:30,960 --> 00:19:34,320
thank you thank you guys thank you thank

00:19:33,679 --> 00:19:36,320
you

00:19:34,320 --> 00:19:38,000
um now talk a little bit more than a

00:19:36,320 --> 00:19:40,720
bunch of questions about uh

00:19:38,000 --> 00:19:41,760
the routing as well as the pretty uh the

00:19:40,720 --> 00:19:43,760
query predictor

00:19:41,760 --> 00:19:45,760
and these are two different repos two

00:19:43,760 --> 00:19:48,640
different projects two different uh

00:19:45,760 --> 00:19:50,240
um you know the things which are also

00:19:48,640 --> 00:19:51,520
integrated right talk a little bit more

00:19:50,240 --> 00:19:55,039
about that

00:19:51,520 --> 00:19:58,080
uh yeah yeah sure uh the

00:19:55,039 --> 00:20:01,120
router product at twitter has been

00:19:58,080 --> 00:20:01,440
created from last year so that's product

00:20:01,120 --> 00:20:03,919
is

00:20:01,440 --> 00:20:06,080
uh used to because we have so many

00:20:03,919 --> 00:20:08,320
different presto clusters at twitter

00:20:06,080 --> 00:20:10,720
and it's very difficult to maintain so

00:20:08,320 --> 00:20:14,880
many clusters because of scalability

00:20:10,720 --> 00:20:17,440
and availability so we have uh the

00:20:14,880 --> 00:20:18,480
router project which is used to route

00:20:17,440 --> 00:20:21,120
different requests

00:20:18,480 --> 00:20:23,120
to some specific plastic classmates but

00:20:21,120 --> 00:20:24,880
there is also a problem that we have

00:20:23,120 --> 00:20:26,799
observed while maintaining these

00:20:24,880 --> 00:20:28,480
clusters that is sometimes

00:20:26,799 --> 00:20:31,840
especially considering there are some

00:20:28,480 --> 00:20:34,640
really like results consuming queries

00:20:31,840 --> 00:20:35,360
this queries may occupy too many

00:20:34,640 --> 00:20:38,080
resources

00:20:35,360 --> 00:20:39,840
in one specific cluster but because we

00:20:38,080 --> 00:20:42,159
are applying wrong

00:20:39,840 --> 00:20:44,240
ruling approach in routing these

00:20:42,159 --> 00:20:46,240
requests to different clusters

00:20:44,240 --> 00:20:47,280
sometimes for example in one cluster

00:20:46,240 --> 00:20:50,000
there are like two

00:20:47,280 --> 00:20:51,360
or three uh quite results consuming

00:20:50,000 --> 00:20:53,840
queries and this

00:20:51,360 --> 00:20:55,679
this queries may use too many resources

00:20:53,840 --> 00:20:58,400
in the specific cluster

00:20:55,679 --> 00:20:59,280
this will make the whole cluster be like

00:20:58,400 --> 00:21:02,000
like a

00:20:59,280 --> 00:21:04,240
waste of resources because one cluster

00:21:02,000 --> 00:21:07,200
is very busy but the others are quite

00:21:04,240 --> 00:21:09,520
added at the same time so we had the

00:21:07,200 --> 00:21:11,200
idea of creating a query predictor

00:21:09,520 --> 00:21:13,919
service to try to help us

00:21:11,200 --> 00:21:14,880
to predict the cost of each query

00:21:13,919 --> 00:21:16,720
beforehand

00:21:14,880 --> 00:21:18,320
so we started this project at the very

00:21:16,720 --> 00:21:21,440
beginning of this year

00:21:18,320 --> 00:21:24,320
and we have also open source of this

00:21:21,440 --> 00:21:24,960
the current predictor project as well as

00:21:24,320 --> 00:21:27,360
the

00:21:24,960 --> 00:21:30,080
router project so the current predictor

00:21:27,360 --> 00:21:33,280
is directly used to predict the cost

00:21:30,080 --> 00:21:34,240
the resource usage of each query so with

00:21:33,280 --> 00:21:37,200
this information

00:21:34,240 --> 00:21:39,679
and then with our router service we can

00:21:37,200 --> 00:21:42,320
route requests to different clusters

00:21:39,679 --> 00:21:43,520
more intelligently so that's the whole

00:21:42,320 --> 00:21:46,320
picture of it

00:21:43,520 --> 00:21:47,840
yeah there's another question about

00:21:46,320 --> 00:21:50,720
rolling deployments

00:21:47,840 --> 00:21:52,720
and so you know how these are done with

00:21:50,720 --> 00:21:54,240
fixed router predictor strategies

00:21:52,720 --> 00:21:56,000
uh maybe you can talk a little bit about

00:21:54,240 --> 00:21:57,520
it

00:21:56,000 --> 00:21:59,200
they know would you like to talk about

00:21:57,520 --> 00:22:03,120
that oh i can

00:21:59,200 --> 00:22:03,120
what's your idea okay

00:22:03,760 --> 00:22:10,799
okay okay yes uh so

00:22:06,799 --> 00:22:13,840
so the thing is that uh there's a

00:22:10,799 --> 00:22:15,919
opportunity for example when we apply

00:22:13,840 --> 00:22:17,600
so so the first thing is that the router

00:22:15,919 --> 00:22:19,039
service and the credit predictive

00:22:17,600 --> 00:22:22,000
service are two

00:22:19,039 --> 00:22:22,320
independent services so they don't have

00:22:22,000 --> 00:22:25,039
uh

00:22:22,320 --> 00:22:25,520
influence they are not a hard dependency

00:22:25,039 --> 00:22:28,159
on

00:22:25,520 --> 00:22:28,880
each other so for example suppose that

00:22:28,159 --> 00:22:31,760
the curve

00:22:28,880 --> 00:22:32,640
query predictor is offline because some

00:22:31,760 --> 00:22:35,200
like

00:22:32,640 --> 00:22:36,320
issues but at this time the the router

00:22:35,200 --> 00:22:39,360
can still work

00:22:36,320 --> 00:22:42,480
uh it may like roll back to it may

00:22:39,360 --> 00:22:45,280
fall back to like the wrong ruling

00:22:42,480 --> 00:22:47,280
approach to route this request to

00:22:45,280 --> 00:22:51,360
specific press or passwords

00:22:47,280 --> 00:22:54,480
if the the uh the the the

00:22:51,360 --> 00:22:56,880
one specific presto cluster is offline

00:22:54,480 --> 00:22:59,600
in that case the router will take care

00:22:56,880 --> 00:23:00,320
of that it will adjust the forward

00:22:59,600 --> 00:23:03,520
request

00:23:00,320 --> 00:23:05,440
to other available clusters it is also

00:23:03,520 --> 00:23:08,799
possible that when the query

00:23:05,440 --> 00:23:11,360
is executing in one specific press or

00:23:08,799 --> 00:23:11,919
cluster but at this time the cluster is

00:23:11,360 --> 00:23:13,919
done

00:23:11,919 --> 00:23:16,080
in that case uh the the queries will

00:23:13,919 --> 00:23:18,400
just be retried for the

00:23:16,080 --> 00:23:20,799
from the client side to to resend the

00:23:18,400 --> 00:23:22,799
query to the

00:23:20,799 --> 00:23:24,480
router and then to a specific presto

00:23:22,799 --> 00:23:27,600
cluster so that's the

00:23:24,480 --> 00:23:27,600
main story of it

00:23:29,919 --> 00:23:36,799
okay thank you um i think there

00:23:33,600 --> 00:23:38,480
is a question about uh prism and the

00:23:36,799 --> 00:23:39,679
presto router i'm not sure if you heard

00:23:38,480 --> 00:23:43,360
about the prism

00:23:39,679 --> 00:23:46,080
um update earlier in the day um any

00:23:43,360 --> 00:23:48,720
do you know how they are related are

00:23:46,080 --> 00:23:51,840
they similar

00:23:48,720 --> 00:23:54,080
i'm not sure that's all right yeah

00:23:51,840 --> 00:23:55,919
yeah we have a little knowledge about

00:23:54,080 --> 00:23:58,080
prism but

00:23:55,919 --> 00:23:59,200
across the router it's just a very

00:23:58,080 --> 00:24:02,480
lightweight you know

00:23:59,200 --> 00:24:04,960
loading system for price tool yes it's

00:24:02,480 --> 00:24:08,320
easy to use and easy to deploy

00:24:04,960 --> 00:24:10,720
and it's transparent to the

00:24:08,320 --> 00:24:11,919
to the users yeah so there's no burden

00:24:10,720 --> 00:24:14,000
from

00:24:11,919 --> 00:24:16,240
our client side it's just like a normal

00:24:14,000 --> 00:24:16,240
class

00:24:16,960 --> 00:24:20,080
got it got it a question on the query

00:24:19,279 --> 00:24:22,480
prediction

00:24:20,080 --> 00:24:23,840
um and so did you consider io

00:24:22,480 --> 00:24:25,039
requirements did you consider i o

00:24:23,840 --> 00:24:26,480
requirements uh

00:24:25,039 --> 00:24:29,039
for executing the query and the

00:24:26,480 --> 00:24:32,080
associated costs i see i see yeah

00:24:29,039 --> 00:24:36,000
uh for now we don't consider uh

00:24:32,080 --> 00:24:39,120
i o uh there are two things we are

00:24:36,000 --> 00:24:41,840
predicting the first one is a cpu time

00:24:39,120 --> 00:24:42,400
of each sql query and the other is the

00:24:41,840 --> 00:24:45,600
peak

00:24:42,400 --> 00:24:46,640
memory bytes of each query because we

00:24:45,600 --> 00:24:49,760
find that those

00:24:46,640 --> 00:24:53,279
uh these two metrics are quite uh

00:24:49,760 --> 00:24:58,400
crucial for for the prediction uh

00:24:53,279 --> 00:24:58,400
of uh the the incu incoming crystal

00:24:58,840 --> 00:25:02,880
queries

00:25:01,360 --> 00:25:04,799
got it i think that there's a related

00:25:02,880 --> 00:25:06,960
question on what happens if

00:25:04,799 --> 00:25:08,000
a when a query is predicted to use more

00:25:06,960 --> 00:25:10,640
resources

00:25:08,000 --> 00:25:11,520
right um uh what happens if that that's

00:25:10,640 --> 00:25:14,480
the case

00:25:11,520 --> 00:25:15,200
uh i'm not sure whether the question i

00:25:14,480 --> 00:25:18,559
understand

00:25:15,200 --> 00:25:21,120
the question correctly uh i i i

00:25:18,559 --> 00:25:21,600
i guess the quest question means that uh

00:25:21,120 --> 00:25:24,640
what's

00:25:21,600 --> 00:25:28,000
if the query uh

00:25:24,640 --> 00:25:31,440
is predicted to need more resources over

00:25:28,000 --> 00:25:34,480
the cluster can offer to it so

00:25:31,440 --> 00:25:37,360
in that case the answer will be that

00:25:34,480 --> 00:25:39,200
uh when we do the classification we need

00:25:37,360 --> 00:25:42,640
to have some buckets

00:25:39,200 --> 00:25:43,039
so in that case we'll just uh so in that

00:25:42,640 --> 00:25:47,200
case

00:25:43,039 --> 00:25:49,679
that this uh situation usually won't

00:25:47,200 --> 00:25:52,240
happen because the package is defined

00:25:49,679 --> 00:25:55,200
based on how many resources we have

00:25:52,240 --> 00:25:56,000
so but there is also a chance that for

00:25:55,200 --> 00:25:59,440
example

00:25:56,000 --> 00:26:02,320
the maximum for example the

00:25:59,440 --> 00:26:02,640
memory is uh 10 gigabytes but the query

00:26:02,320 --> 00:26:05,919
may

00:26:02,640 --> 00:26:09,600
need like 11 gigabytes in that case

00:26:05,919 --> 00:26:12,000
the query will just fail so in that case

00:26:09,600 --> 00:26:14,080
the client may retry or will

00:26:12,000 --> 00:26:15,279
let the client know that the query

00:26:14,080 --> 00:26:17,760
consume too many

00:26:15,279 --> 00:26:19,360
resources so that the user may need to

00:26:17,760 --> 00:26:22,480
use what may for example

00:26:19,360 --> 00:26:23,440
may need to like optimize the query to

00:26:22,480 --> 00:26:26,559
consume

00:26:23,440 --> 00:26:26,559
less resources on it

00:26:27,279 --> 00:26:31,679
got it thanks chunzu um a question about

00:26:30,559 --> 00:26:33,360
the scheduler

00:26:31,679 --> 00:26:34,799
uh is there can you share a little bit

00:26:33,360 --> 00:26:36,159
more about the scheduler that you're

00:26:34,799 --> 00:26:38,000
using

00:26:36,159 --> 00:26:39,279
within the presto router or are you just

00:26:38,000 --> 00:26:42,320
leaving it to the coordinator

00:26:39,279 --> 00:26:45,679
the pesto coordinator

00:26:42,320 --> 00:26:47,600
uh now would you like to talk a little

00:26:45,679 --> 00:26:49,679
bit about it

00:26:47,600 --> 00:26:51,840
actually we don't have a scheduler

00:26:49,679 --> 00:26:54,880
inside the price tag right

00:26:51,840 --> 00:26:56,000
but in twitter our customers actually

00:26:54,880 --> 00:26:58,799
they're

00:26:56,000 --> 00:27:00,400
using various scheduler something like

00:26:58,799 --> 00:27:03,440
sapling like tableau

00:27:00,400 --> 00:27:03,760
or some current jobs or some etl job

00:27:03,440 --> 00:27:06,559
just

00:27:03,760 --> 00:27:07,520
made by themselves yeah they can be from

00:27:06,559 --> 00:27:09,919
anywhere

00:27:07,520 --> 00:27:10,960
just you they are using some price to

00:27:09,919 --> 00:27:13,200
query

00:27:10,960 --> 00:27:15,120
uh to generate their production data or

00:27:13,200 --> 00:27:15,919
something very maybe some more critical

00:27:15,120 --> 00:27:19,120
jobs yeah

00:27:15,919 --> 00:27:21,840
this kind of query can be scheduled by

00:27:19,120 --> 00:27:21,840
their application

00:27:23,200 --> 00:27:27,120
right so the router is essentially just

00:27:25,039 --> 00:27:29,600
handling the workload that's sent to it

00:27:27,120 --> 00:27:30,559
right and then yeah and then the queries

00:27:29,600 --> 00:27:33,039
are um

00:27:30,559 --> 00:27:35,279
the coordinator takes care of the rest

00:27:33,039 --> 00:27:38,880
once it's routed to the right cluster

00:27:35,279 --> 00:27:39,600
yes true we just want to split that we

00:27:38,880 --> 00:27:43,360
can keep

00:27:39,600 --> 00:27:45,679
to multiple queries and also in twitter

00:27:43,360 --> 00:27:47,279
a lot of our customers they love the ui

00:27:45,679 --> 00:27:49,760
we love the price to ui

00:27:47,279 --> 00:27:51,120
but the price of us really hiding

00:27:49,760 --> 00:27:53,440
sometimes it will make

00:27:51,120 --> 00:27:54,159
you know drag the the price of calling

00:27:53,440 --> 00:27:57,520
her down

00:27:54,159 --> 00:28:00,880
so we also use this router to handle

00:27:57,520 --> 00:28:03,600
maybe 99 percent ui request

00:28:00,880 --> 00:28:04,640
so the our customer can just visit the

00:28:03,600 --> 00:28:08,159
doctor to see

00:28:04,640 --> 00:28:11,039
the status of their virus

00:28:08,159 --> 00:28:12,240
got it uh thank you let's take one last

00:28:11,039 --> 00:28:14,240
question and then um

00:28:12,240 --> 00:28:16,240
and then wrap up there's a question

00:28:14,240 --> 00:28:18,320
about use cases so you're obviously

00:28:16,240 --> 00:28:21,520
using a range of different connectors

00:28:18,320 --> 00:28:23,919
and systems you're using kafka druid

00:28:21,520 --> 00:28:25,679
data lake um can you talk a little bit

00:28:23,919 --> 00:28:27,600
more about

00:28:25,679 --> 00:28:29,440
the use cases analytical or would you

00:28:27,600 --> 00:28:32,080
use presto for etl jobs

00:28:29,440 --> 00:28:32,080
uh as well

00:28:33,760 --> 00:28:39,600
that's a good question on twitter

00:28:36,799 --> 00:28:41,520
at the very beginning we designed oh we

00:28:39,600 --> 00:28:42,559
can see we developed price tall through

00:28:41,520 --> 00:28:45,120
twitter just for the

00:28:42,559 --> 00:28:46,720
top queries as other components yeah the

00:28:45,120 --> 00:28:49,200
user just send us some

00:28:46,720 --> 00:28:51,120
employee query to presto and they can

00:28:49,200 --> 00:28:54,159
just reach maybe from their

00:28:51,120 --> 00:28:57,360
data visualizer or something they can

00:28:54,159 --> 00:29:00,640
see the results in a couple of seconds

00:28:57,360 --> 00:29:03,039
but we are running prices really well

00:29:00,640 --> 00:29:04,960
and the the servers become very stable

00:29:03,039 --> 00:29:07,679
so more and more customers

00:29:04,960 --> 00:29:08,559
want to use crystal to do some data

00:29:07,679 --> 00:29:10,320
ingestion

00:29:08,559 --> 00:29:11,600
someone just used crystal to collect

00:29:10,320 --> 00:29:14,080
data from her

00:29:11,600 --> 00:29:14,799
warehouse and write data to some other

00:29:14,080 --> 00:29:17,200
data source

00:29:14,799 --> 00:29:19,200
like watercolor or something else yeah

00:29:17,200 --> 00:29:22,320
so they they have something like

00:29:19,200 --> 00:29:24,080
btr job but i have to say it's not real

00:29:22,320 --> 00:29:27,600
etl job it just

00:29:24,080 --> 00:29:30,720
some guys make something for further

00:29:27,600 --> 00:29:32,880
easy implementation because it's really

00:29:30,720 --> 00:29:34,559
convenient just request so

00:29:32,880 --> 00:29:35,919
can get multiple data sources and

00:29:34,559 --> 00:29:38,399
collect the data together

00:29:35,919 --> 00:29:40,399
and also processors have a strong we're

00:29:38,399 --> 00:29:41,360
gonna see some data ingestion features

00:29:40,399 --> 00:29:43,360
we can just implement

00:29:41,360 --> 00:29:45,039
something like the druid connector we

00:29:43,360 --> 00:29:48,240
can ingest this how to do it

00:29:45,039 --> 00:29:48,240
something like that yeah

00:29:48,399 --> 00:29:54,799
so i have seen 99 are still

00:29:52,080 --> 00:29:55,520
cars are still running on the highway

00:29:54,799 --> 00:29:58,640
hot data

00:29:55,520 --> 00:30:00,080
houses yeah but some corner case look at

00:29:58,640 --> 00:30:02,720
this current case that's some

00:30:00,080 --> 00:30:04,480
user us yeah also used price go for some

00:30:02,720 --> 00:30:07,200
interest

00:30:04,480 --> 00:30:07,200

YouTube URL: https://www.youtube.com/watch?v=biHP8-d7Bb4


