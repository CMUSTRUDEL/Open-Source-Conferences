Title: DjangoCon US 2017 - End-to-End Django on Kubernetes by Frank Wiles
Publication date: 2017-09-07
Playlist: DjangoCon US 2017
Description: 
	DjangoCon US 2017 - End-to-End Django on Kubernetes by Frank Wiles

Not only is Kubernetes a great way to deploy Django and all of its dependencies, itâ€™s actually the easiest way! Really!

Deploying multi-layer applications with multiple dependencies is exactly what Kubernetes is designed to do. You can replace pages of Django and PostgreSQL configuration templates with a simple Kubernetes config, OpenShift template or Helm chart, and then stand up the entire stack for your application in a single command. In this presentation, we will walk you through the setup required to deploy and scale Django, including:

Replicated PostgreSQL with persistent storage and automated failover
Scalable Django application servers
Front-ends and DNS routing

The templates covered in this presentation should be applicable to developing your own Kubernetes deployments, and the concepts will apply to anyone looking at any container orchestration platform.

This talk was presented at: https://2017.djangocon.us/talks/end-to-end-django-on-kubernetes/

LINKS:
Follow Frank Wiles ğŸ‘‡
On Twitter: https://twitter.com/fwiles
Official homepage: http://www.revsys.com

Follow Josh Berkus ğŸ‘‡
On Twitter: https://twitter.com/fuzzychef
Official homepage: https://jberkus.github.io

Follow DjangCon US ğŸ‘‡
https://twitter.com/djangocon

Follow DEFNA ğŸ‘‡
https://twitter.com/defnado
https://www.defna.org/
Captions: 
	00:00:00,000 --> 00:00:16,330
[Music]

00:00:13,320 --> 00:00:18,040
good afternoon everybody um thanks for

00:00:16,330 --> 00:00:21,960
thanks for having me this may come as a

00:00:18,040 --> 00:00:23,740
surprise to you but I am NOT Josh burkas

00:00:21,960 --> 00:00:25,660
but the more I thought about it I

00:00:23,740 --> 00:00:29,140
realized that you could be easily

00:00:25,660 --> 00:00:32,259
confused because Josh's name's on the

00:00:29,140 --> 00:00:34,510
program still we both have beards we

00:00:32,259 --> 00:00:36,910
both like Django and Postgres and

00:00:34,510 --> 00:00:40,810
kubernetes we even both have the same

00:00:36,910 --> 00:00:43,720
damn glasses but there's some

00:00:40,810 --> 00:00:46,180
differences over here on the left we've

00:00:43,720 --> 00:00:47,860
got kind of the average user of Postgres

00:00:46,180 --> 00:00:49,510
his level of knowledge and then I know a

00:00:47,860 --> 00:00:52,510
little bit more and then Josh knows a

00:00:49,510 --> 00:00:53,770
whole lot more but about now you're

00:00:52,510 --> 00:00:55,120
probably asking yourself what the hell

00:00:53,770 --> 00:00:57,970
this has to do with anything about the

00:00:55,120 --> 00:00:59,920
talk and there's one final important

00:00:57,970 --> 00:01:05,110
difference that that does pertain to

00:00:59,920 --> 00:01:07,570
this talk my back works pretty okay josh

00:01:05,110 --> 00:01:09,369
is not so much which is why I'm up here

00:01:07,570 --> 00:01:11,740
to talk to you about Django in

00:01:09,369 --> 00:01:13,900
kubernetes see Josh managed to hurt his

00:01:11,740 --> 00:01:16,659
back making these lovely speaker gifts

00:01:13,900 --> 00:01:18,429
for all of the Django Con attendees josh

00:01:16,659 --> 00:01:21,369
is at an excellent Potter as well as

00:01:18,429 --> 00:01:23,319
open source geek and last week he hurt

00:01:21,369 --> 00:01:26,710
his back finishing these up so he wasn't

00:01:23,319 --> 00:01:28,749
able to make it and the Django town team

00:01:26,710 --> 00:01:31,569
was like hey any chance you want to give

00:01:28,749 --> 00:01:33,280
a talk on Tuesday so this one would

00:01:31,569 --> 00:01:36,639
probably not be my most polished talk

00:01:33,280 --> 00:01:40,380
ever but onto the topic that you are all

00:01:36,639 --> 00:01:42,880
here for kubernetes kubernetes is

00:01:40,380 --> 00:01:45,159
arguably the best and most popular

00:01:42,880 --> 00:01:46,929
container orchestration system out there

00:01:45,159 --> 00:01:48,249
today that doesn't mean that it won't

00:01:46,929 --> 00:01:50,139
get replaced in a year by something

00:01:48,249 --> 00:01:51,939
cooler and better but right now this is

00:01:50,139 --> 00:01:54,850
kind of what everybody is playing with

00:01:51,939 --> 00:01:56,709
for the most part but before we dive too

00:01:54,850 --> 00:01:59,859
deeply into kubernetes we have to get

00:01:56,709 --> 00:02:03,700
into some terminology so the name

00:01:59,859 --> 00:02:06,459
kubernetes what does it mean is it Greek

00:02:03,700 --> 00:02:07,869
for ship captain or is it that Google

00:02:06,459 --> 00:02:10,899
learned it's less than naming things you

00:02:07,869 --> 00:02:12,819
have to go or is it three all of the

00:02:10,899 --> 00:02:15,010
above if you picked three you are

00:02:12,819 --> 00:02:16,569
correct it's Greek for ship captain and

00:02:15,010 --> 00:02:20,980
I do think that they named it because

00:02:16,569 --> 00:02:24,130
they had so much trouble with go today

00:02:20,980 --> 00:02:26,950
most everybody is looking at or moving

00:02:24,130 --> 00:02:28,660
toward containers in some form or

00:02:26,950 --> 00:02:30,520
and containers are great about being

00:02:28,660 --> 00:02:32,560
able to package up all of our

00:02:30,520 --> 00:02:34,300
dependencies for an app into a thing

00:02:32,560 --> 00:02:36,610
that we can then move around and share

00:02:34,300 --> 00:02:40,480
and use but working with them on their

00:02:36,610 --> 00:02:42,220
own is is not exactly the most

00:02:40,480 --> 00:02:44,170
user-friendly thing if you had to

00:02:42,220 --> 00:02:46,600
remember which ports am i used in what

00:02:44,170 --> 00:02:48,670
volumes do i mouth where that stuff gets

00:02:46,600 --> 00:02:51,930
complicated over time and that's why

00:02:48,670 --> 00:02:54,100
container orchestration systems exist

00:02:51,930 --> 00:02:56,230
some examples of those if you're not

00:02:54,100 --> 00:02:58,380
familiar or like docker compose is

00:02:56,230 --> 00:03:01,959
really a container orchestration system

00:02:58,380 --> 00:03:04,720
docker swarm bezos AWS container service

00:03:01,959 --> 00:03:06,730
and kubernetes are all handled

00:03:04,720 --> 00:03:08,040
orchestrating these containers that you

00:03:06,730 --> 00:03:11,280
want to use together

00:03:08,040 --> 00:03:13,360
so what's container orchestration well

00:03:11,280 --> 00:03:15,060
event loops are used by kubernetes

00:03:13,360 --> 00:03:17,560
components to reconcile things between

00:03:15,060 --> 00:03:22,360
local machines and the desired cluster

00:03:17,560 --> 00:03:24,250
state so what does that mean to us we

00:03:22,360 --> 00:03:26,230
basically tell kubernetes this is how I

00:03:24,250 --> 00:03:27,760
would like the world to look and then

00:03:26,230 --> 00:03:30,610
kubernetes sits there and spins in a

00:03:27,760 --> 00:03:30,940
loop and tries to make that happen for

00:03:30,610 --> 00:03:32,680
you

00:03:30,940 --> 00:03:34,090
it can't always do it but it will

00:03:32,680 --> 00:03:40,480
continually keep trying to make it

00:03:34,090 --> 00:03:41,980
happen so I'm not going to little so it

00:03:40,480 --> 00:03:44,290
the other term they use from the

00:03:41,980 --> 00:03:46,540
documentation here is a control loop

00:03:44,290 --> 00:03:47,970
which in robotics is kind of the main

00:03:46,540 --> 00:03:50,980
process that's just sitting there going

00:03:47,970 --> 00:03:53,049
am i standing up and I if I hit a wall

00:03:50,980 --> 00:03:55,450
do I need to back up what's happening

00:03:53,049 --> 00:03:56,859
right now and and that's basically what

00:03:55,450 --> 00:03:58,690
kubernetes is constantly doing is

00:03:56,859 --> 00:04:00,970
everything that's supposed to be running

00:03:58,690 --> 00:04:02,410
running can everything talk to each

00:04:00,970 --> 00:04:02,880
other that's supposed to be talking to

00:04:02,410 --> 00:04:04,930
each other

00:04:02,880 --> 00:04:07,269
and I'm not gonna lie to you and say

00:04:04,930 --> 00:04:09,489
that kubernetes is super easy to learn

00:04:07,269 --> 00:04:12,400
we are definitely not going to learn it

00:04:09,489 --> 00:04:14,769
all in 40 minutes today but it's a big

00:04:12,400 --> 00:04:17,620
complicated system it's a bear it's a

00:04:14,769 --> 00:04:20,109
big scary bear but my goal is to change

00:04:17,620 --> 00:04:23,650
your impression of it from this to more

00:04:20,109 --> 00:04:25,270
this No

00:04:23,650 --> 00:04:27,850
when I started playing with kubernetes

00:04:25,270 --> 00:04:30,280
the terminology is what tripped me up

00:04:27,850 --> 00:04:31,539
there's a bunch of new terms there's a

00:04:30,280 --> 00:04:33,100
whole bunch of new concepts that you

00:04:31,539 --> 00:04:34,840
just don't tend to think about but we've

00:04:33,100 --> 00:04:36,930
all done before but the way they talk

00:04:34,840 --> 00:04:39,760
about them sometimes is confusing so

00:04:36,930 --> 00:04:41,020
most of today is going to be coming up

00:04:39,760 --> 00:04:42,460
and getting comfortable what these

00:04:41,020 --> 00:04:44,260
different terms mean and then we'll

00:04:42,460 --> 00:04:51,100
piece it all together and have a working

00:04:44,260 --> 00:04:53,680
django app towards the end sorry these

00:04:51,100 --> 00:04:56,710
sites move slow when there's big images

00:04:53,680 --> 00:05:01,060
so kubernetes has a concept of masters

00:04:56,710 --> 00:05:02,770
and nodes worker nodes the masters are

00:05:01,060 --> 00:05:04,420
where all the kubernetes magic happens

00:05:02,770 --> 00:05:06,220
of what should be running where and the

00:05:04,420 --> 00:05:08,950
nodes are where your containers actually

00:05:06,220 --> 00:05:11,020
run you can have differing numbers

00:05:08,950 --> 00:05:14,620
they're not a one-to-one relationship so

00:05:11,020 --> 00:05:18,610
this would be a simple three master

00:05:14,620 --> 00:05:21,100
cluster with three worker nodes a more

00:05:18,610 --> 00:05:24,250
real-world scenario would be something

00:05:21,100 --> 00:05:26,620
like this so inside of an AWS region you

00:05:24,250 --> 00:05:28,800
have a master per availability zone

00:05:26,620 --> 00:05:31,330
those three are clustered together and

00:05:28,800 --> 00:05:33,840
then you have some number of worker

00:05:31,330 --> 00:05:38,080
nodes also in each availability zone

00:05:33,840 --> 00:05:40,180
this is so that underneath kubernetes is

00:05:38,080 --> 00:05:43,630
really just a net CD cluster that has a

00:05:40,180 --> 00:05:45,760
nice API over top of it so when you say

00:05:43,630 --> 00:05:47,620
I want you to run these sets of

00:05:45,760 --> 00:05:49,660
containers it puts it that data into the

00:05:47,620 --> 00:05:52,750
sed cluster which gets then clustered

00:05:49,660 --> 00:05:55,210
between all of the masters and then the

00:05:52,750 --> 00:05:57,760
API and the scheduler little Damon's

00:05:55,210 --> 00:05:59,410
that run on these nodes constantly are

00:05:57,760 --> 00:06:00,640
looking at that and saying am i running

00:05:59,410 --> 00:06:01,630
what I should be running is there

00:06:00,640 --> 00:06:02,980
something out there that needs to be

00:06:01,630 --> 00:06:08,650
running that's not running where should

00:06:02,980 --> 00:06:10,300
we run it one of the things that trips

00:06:08,650 --> 00:06:13,240
people up is authenticating to your

00:06:10,300 --> 00:06:16,030
kubernetes cluster almost everything

00:06:13,240 --> 00:06:18,460
happens through this config file in your

00:06:16,030 --> 00:06:22,330
home directory effectively no and his

00:06:18,460 --> 00:06:23,920
coop config there are multiple

00:06:22,330 --> 00:06:26,410
authentication with kubernetes is fairly

00:06:23,920 --> 00:06:28,420
pluggable but not as pluggable and

00:06:26,410 --> 00:06:31,690
easy-to-use as say like Django's

00:06:28,420 --> 00:06:34,210
authentication backends but out of the

00:06:31,690 --> 00:06:36,940
box you tend to have one user with a

00:06:34,210 --> 00:06:38,860
password and one set of SSL keys to talk

00:06:36,940 --> 00:06:41,770
to it and you share that amongst

00:06:38,860 --> 00:06:45,670
multiple susceptance that's kind of the

00:06:41,770 --> 00:06:47,290
default configuration it feels wrong and

00:06:45,670 --> 00:06:49,270
messy and sick and it is kind of wrong

00:06:47,290 --> 00:06:50,740
and messy and sick but it works there

00:06:49,270 --> 00:06:53,380
are other systems you can authenticate

00:06:50,740 --> 00:06:55,990
against Google if you

00:06:53,380 --> 00:06:57,580
use google apps so you can give just

00:06:55,990 --> 00:06:59,920
certain people access to the cluster

00:06:57,580 --> 00:07:01,780
there are other schemes that you can

00:06:59,920 --> 00:07:04,270
employ I'm not gonna get too far into

00:07:01,780 --> 00:07:06,940
that but it is important that you have a

00:07:04,270 --> 00:07:08,890
coop config and that it is properly set

00:07:06,940 --> 00:07:10,780
up pointing at your cluster and you can

00:07:08,890 --> 00:07:12,400
have multiple of them so that you can

00:07:10,780 --> 00:07:13,990
switch between multiple clusters of

00:07:12,400 --> 00:07:14,530
which one you're talking to at any given

00:07:13,990 --> 00:07:17,710
time

00:07:14,530 --> 00:07:21,460
and so you can access your cluster by

00:07:17,710 --> 00:07:23,170
proxy so if you run coop CTL proxy it

00:07:21,460 --> 00:07:24,730
looks for that default cube config it

00:07:23,170 --> 00:07:26,890
looks for the cluster that you're

00:07:24,730 --> 00:07:28,720
currently pointing at so I have five or

00:07:26,890 --> 00:07:30,040
six clusters in my config so I pick

00:07:28,720 --> 00:07:31,870
which one I'm going to be playing with

00:07:30,040 --> 00:07:36,360
it any given moment and then if I run

00:07:31,870 --> 00:07:43,420
coop CTL proxy I am then able to access

00:07:36,360 --> 00:07:45,130
the API of the actual masters if you go

00:07:43,420 --> 00:07:48,250
if your cluster is running the

00:07:45,130 --> 00:07:52,180
kubernetes dashboard you can then access

00:07:48,250 --> 00:07:54,510
it with this I need to move this to a

00:07:52,180 --> 00:07:54,510
different

00:08:08,740 --> 00:08:13,840
yeah that's not going to let me play but

00:08:11,830 --> 00:08:15,460
my browser is it well I guess we were

00:08:13,840 --> 00:08:18,550
going to skip the dashboard portion of

00:08:15,460 --> 00:08:21,190
the evening the dashboard is a really

00:08:18,550 --> 00:08:23,290
decent web interface to the cluster as a

00:08:21,190 --> 00:08:25,300
whole you can see all of the various

00:08:23,290 --> 00:08:28,000
components you can see all of the config

00:08:25,300 --> 00:08:29,950
you can make edits to the config you can

00:08:28,000 --> 00:08:31,870
see resource utilization across your

00:08:29,950 --> 00:08:34,300
nodes which ones have high CPU or high

00:08:31,870 --> 00:08:36,960
memory usage and you can just get a nice

00:08:34,300 --> 00:08:40,900
kind of dashboard state of your cluster

00:08:36,960 --> 00:08:42,550
all of that same information all the

00:08:40,900 --> 00:08:44,740
information for the dashboard comes from

00:08:42,550 --> 00:08:46,360
the API which is also all the same exact

00:08:44,740 --> 00:08:48,040
information that you get on command line

00:08:46,360 --> 00:08:52,690
tools that you can also then access from

00:08:48,040 --> 00:08:55,570
Python and go and other languages one of

00:08:52,690 --> 00:08:57,130
the ways that you keep things sane in a

00:08:55,570 --> 00:09:00,400
kubernetes cluster is by using

00:08:57,130 --> 00:09:05,430
namespaces a namespace in kubernetes is

00:09:00,400 --> 00:09:08,890
a offence between other containers so

00:09:05,430 --> 00:09:12,790
pods in a namespace containers in a

00:09:08,890 --> 00:09:16,180
namespace can only easily access other

00:09:12,790 --> 00:09:18,040
containers in that same namespace so you

00:09:16,180 --> 00:09:21,370
can use it as kind of a light mechanism

00:09:18,040 --> 00:09:23,140
to for multi-tenancy you can also use

00:09:21,370 --> 00:09:25,390
those kinds of namespaces as a light way

00:09:23,140 --> 00:09:28,570
to kind of separate dev from stage from

00:09:25,390 --> 00:09:31,140
prag and all inside the same cluster but

00:09:28,570 --> 00:09:34,420
it's not as hard and fast of a wall as

00:09:31,140 --> 00:09:36,250
true multi-tenant separation like your

00:09:34,420 --> 00:09:38,800
containers could end up talking to each

00:09:36,250 --> 00:09:44,410
other it's a it's a balsa wood fence not

00:09:38,800 --> 00:09:46,930
a brick wall in kubernetes you define

00:09:44,410 --> 00:09:49,480
resources using Yambol so this little

00:09:46,930 --> 00:09:51,270
bit of snippet at the top there is all

00:09:49,480 --> 00:09:56,500
the y amyl you need to create a

00:09:51,270 --> 00:09:58,060
namespace called rebus - rocks you

00:09:56,500 --> 00:10:02,350
created in the cluster by just running

00:09:58,060 --> 00:10:03,820
Kubb CTL apply - eff pointed to the file

00:10:02,350 --> 00:10:06,490
you created and that will come back and

00:10:03,820 --> 00:10:08,620
say I've created this namespace if it's

00:10:06,490 --> 00:10:09,790
already created it'll say I configured

00:10:08,620 --> 00:10:13,090
this namespace because it was already

00:10:09,790 --> 00:10:14,890
configured and you can reapply these

00:10:13,090 --> 00:10:17,260
same files because all you're doing is

00:10:14,890 --> 00:10:19,150
adding state into the system and so if

00:10:17,260 --> 00:10:21,320
the state is the same nothing changes

00:10:19,150 --> 00:10:25,810
but if there is a new state

00:10:21,320 --> 00:10:30,020
get taken deployments deployments are a

00:10:25,810 --> 00:10:32,720
template of how you would like the world

00:10:30,020 --> 00:10:34,850
to look so you say I want to have this

00:10:32,720 --> 00:10:36,680
django app and I want to run this

00:10:34,850 --> 00:10:39,200
particular container and I want to use

00:10:36,680 --> 00:10:43,910
these environment variables and I want

00:10:39,200 --> 00:10:45,650
to run three copies of them this is kind

00:10:43,910 --> 00:10:48,380
of hard to read

00:10:45,650 --> 00:10:51,530
size-wise but you'll see we just call it

00:10:48,380 --> 00:10:53,270
a kind of deployment and then we say we

00:10:51,530 --> 00:10:56,620
want to have two replicas and the

00:10:53,270 --> 00:10:59,390
template is this particular container

00:10:56,620 --> 00:11:03,980
and we want to open that container

00:10:59,390 --> 00:11:05,510
support 82 inside the cluster and just

00:11:03,980 --> 00:11:08,270
like with the namespace we do the exact

00:11:05,510 --> 00:11:14,720
same apply command to actually put that

00:11:08,270 --> 00:11:16,760
into the cluster services are in Cooper

00:11:14,720 --> 00:11:19,670
Nettie's are what we think of as a

00:11:16,760 --> 00:11:21,350
service i i've just created a django app

00:11:19,670 --> 00:11:22,700
running in those couple of containers

00:11:21,350 --> 00:11:24,830
now i need to tell the rest of the

00:11:22,700 --> 00:11:30,380
cluster that there is this web service

00:11:24,830 --> 00:11:33,440
out there and i define that like this we

00:11:30,380 --> 00:11:37,970
create a service notice all of these are

00:11:33,440 --> 00:11:41,420
in the same namespace rebus rocks i tend

00:11:37,970 --> 00:11:43,850
to use the same name for the namespace

00:11:41,420 --> 00:11:46,190
and the app and the service and

00:11:43,850 --> 00:11:47,570
everything to just keep myself sane you

00:11:46,190 --> 00:11:49,700
could call them things differently i

00:11:47,570 --> 00:11:52,420
know that one of my co-workers steven

00:11:49,700 --> 00:11:55,460
would probably call this service HTTP or

00:11:52,420 --> 00:11:58,340
whiskey where i would call it the name

00:11:55,460 --> 00:12:01,600
of the actual service that i'm thinking

00:11:58,340 --> 00:12:04,850
of it as the website right there's no

00:12:01,600 --> 00:12:06,770
hard or fast rule here but all we're

00:12:04,850 --> 00:12:08,930
doing is saying hey for this service i'm

00:12:06,770 --> 00:12:13,700
gonna open up for a tea and it needs to

00:12:08,930 --> 00:12:15,680
go to the container port 80 Kunis has a

00:12:13,700 --> 00:12:17,090
concept called an ingress controller so

00:12:15,680 --> 00:12:19,340
so far everything that we've done is

00:12:17,090 --> 00:12:20,810
available inside of our cluster two

00:12:19,340 --> 00:12:22,190
other things running in the cluster but

00:12:20,810 --> 00:12:23,870
it is not available to the outside world

00:12:22,190 --> 00:12:26,240
in any way shape or form opening that

00:12:23,870 --> 00:12:28,940
port 80 did not open port 80 on an

00:12:26,240 --> 00:12:31,880
external IP address anywhere so the

00:12:28,940 --> 00:12:35,180
ingress controllers map outside world

00:12:31,880 --> 00:12:36,710
things - inside the cluster now

00:12:35,180 --> 00:12:38,840
depending upon where you host this

00:12:36,710 --> 00:12:41,510
changes what kind of ingress controller

00:12:38,840 --> 00:12:44,570
you can use so if you're on AWS it would

00:12:41,510 --> 00:12:47,300
use an ELB or an alb as your ingress

00:12:44,570 --> 00:12:50,240
controller and it manages what points

00:12:47,300 --> 00:12:51,830
where when so you just say I want one of

00:12:50,240 --> 00:12:53,750
these and it'll go out and create one

00:12:51,830 --> 00:12:55,760
and then you start putting pointing DNS

00:12:53,750 --> 00:12:59,000
at the cname right but you don't have to

00:12:55,760 --> 00:13:01,820
actually go configure it at all which

00:12:59,000 --> 00:13:04,910
leads to a quick aside here we use a

00:13:01,820 --> 00:13:07,010
controller called kublai go which

00:13:04,910 --> 00:13:10,460
handles everything about let's encrypt

00:13:07,010 --> 00:13:12,470
in certificates you install this into

00:13:10,460 --> 00:13:13,970
your cluster and then in these llamo

00:13:12,470 --> 00:13:16,070
definitions you can use what are called

00:13:13,970 --> 00:13:19,700
annotations and they're basically just

00:13:16,070 --> 00:13:21,980
keys in the ammo that kubernetes itself

00:13:19,700 --> 00:13:23,750
is not particularly looking for right so

00:13:21,980 --> 00:13:25,970
a controller is just something that is

00:13:23,750 --> 00:13:28,010
listening to this API looking for state

00:13:25,970 --> 00:13:30,230
changes and taking some action

00:13:28,010 --> 00:13:32,030
the default kubernetes ones handle

00:13:30,230 --> 00:13:34,460
things like ah I need to be running this

00:13:32,030 --> 00:13:36,740
container over on this node but you can

00:13:34,460 --> 00:13:38,630
create your own annotations and take

00:13:36,740 --> 00:13:41,210
other actions so in this case somebody

00:13:38,630 --> 00:13:44,510
created a system to handle let's encrypt

00:13:41,210 --> 00:13:45,770
certificates so you say hey I want to

00:13:44,510 --> 00:13:47,390
let's encrypt certificate for this

00:13:45,770 --> 00:13:49,070
particular hostname it'll go out and

00:13:47,390 --> 00:13:50,870
register it it hijacks the dot

00:13:49,070 --> 00:13:53,270
well-known location

00:13:50,870 --> 00:13:55,550
handles all the key management stores it

00:13:53,270 --> 00:13:58,030
inside the cluster and presents that to

00:13:55,550 --> 00:14:00,500
the world as let's encrypted SSL

00:13:58,030 --> 00:14:02,720
connection from then on and you

00:14:00,500 --> 00:14:05,620
literally have to do just a couple of

00:14:02,720 --> 00:14:10,010
lines of config so this is an ingress

00:14:05,620 --> 00:14:11,800
controller definition and you'll see we

00:14:10,010 --> 00:14:14,330
have the similar kind of pattern name

00:14:11,800 --> 00:14:16,970
namespace right

00:14:14,330 --> 00:14:19,160
then there's the rules there and it's a

00:14:16,970 --> 00:14:21,140
host this is the domain is actually Rev

00:14:19,160 --> 00:14:22,580
so stout dot rocks the one of the new

00:14:21,140 --> 00:14:24,200
top-level domain so don't get confused

00:14:22,580 --> 00:14:27,140
that could be calm or dot-org or

00:14:24,200 --> 00:14:28,430
whatever but then we have a little part

00:14:27,140 --> 00:14:30,380
that I want to highlight and this is the

00:14:28,430 --> 00:14:32,750
kublai go part so we just have an

00:14:30,380 --> 00:14:35,630
annotation there saying hey I want TLS

00:14:32,750 --> 00:14:38,450
Acme and I want to use an engine X

00:14:35,630 --> 00:14:41,540
ingress controller and its host should

00:14:38,450 --> 00:14:44,750
be Rev sister ox and I want you to store

00:14:41,540 --> 00:14:47,430
that as a secret named Reb sis - rocks -

00:14:44,750 --> 00:14:49,250
TLS

00:14:47,430 --> 00:14:52,320
and I don't have to do anything else

00:14:49,250 --> 00:14:54,360
when it comes up it gets the cert if it

00:14:52,320 --> 00:14:55,529
needs renewal it'll renew and I don't

00:14:54,360 --> 00:14:56,910
have to deal with that and I don't have

00:14:55,529 --> 00:14:59,310
to deal with that on a per application

00:14:56,910 --> 00:15:01,410
basis or even per container basis I

00:14:59,310 --> 00:15:03,029
could throw a couple of rails projects

00:15:01,410 --> 00:15:05,610
and a go project into this cluster and

00:15:03,029 --> 00:15:07,500
they have let's encrypt and it's totally

00:15:05,610 --> 00:15:12,839
independent of whatever I'm doing in my

00:15:07,500 --> 00:15:16,800
containers so one of the last pieces of

00:15:12,839 --> 00:15:19,110
terminology is a pod when we create

00:15:16,800 --> 00:15:21,779
deployments all the containers in a

00:15:19,110 --> 00:15:23,520
deployment forma pod pods are sets of

00:15:21,779 --> 00:15:25,589
containers that are deployed together on

00:15:23,520 --> 00:15:27,899
a host so if you have things in a pod

00:15:25,589 --> 00:15:29,670
and it has four containers to it all

00:15:27,899 --> 00:15:31,860
four of those is going to are going to

00:15:29,670 --> 00:15:34,050
be deployed on host a if for some reason

00:15:31,860 --> 00:15:36,209
it can't deploy on host a or host a dies

00:15:34,050 --> 00:15:38,670
they will all be picked up and run

00:15:36,209 --> 00:15:41,730
together on host B all right so they are

00:15:38,670 --> 00:15:45,570
always a set together like a pod of

00:15:41,730 --> 00:15:47,550
whales this could be useful for lots of

00:15:45,570 --> 00:15:49,050
scenarios in all the examples I have

00:15:47,550 --> 00:15:51,810
today we are only using one single

00:15:49,050 --> 00:15:54,029
container so the difference there does

00:15:51,810 --> 00:15:56,370
not become particularly apparent but if

00:15:54,029 --> 00:15:58,110
you needed additional containers that

00:15:56,370 --> 00:15:59,459
only talk to each other and not

00:15:58,110 --> 00:16:00,990
necessarily the outside world this can

00:15:59,459 --> 00:16:04,140
be very efficient you can do things like

00:16:00,990 --> 00:16:05,370
share a UNIX socket on a host then you

00:16:04,140 --> 00:16:07,200
wouldn't be able to do because you

00:16:05,370 --> 00:16:09,690
couldn't guarantee that they would be

00:16:07,200 --> 00:16:12,089
running on the same host so if you have

00:16:09,690 --> 00:16:14,520
like I'm one container Jango app with

00:16:12,089 --> 00:16:16,500
one memcache instance you could have

00:16:14,520 --> 00:16:18,900
that talk over a UNIX domain socket

00:16:16,500 --> 00:16:21,000
instead of a TCP socket and get a little

00:16:18,900 --> 00:16:22,260
bit better performance and you can only

00:16:21,000 --> 00:16:26,490
do that because you know they're always

00:16:22,260 --> 00:16:28,950
running on the same hosts together so at

00:16:26,490 --> 00:16:31,980
a high level view kubernetes is the

00:16:28,950 --> 00:16:35,930
masters run this API and store this

00:16:31,980 --> 00:16:38,250
cluster state and the nodes run pods

00:16:35,930 --> 00:16:40,529
which provides services inside the

00:16:38,250 --> 00:16:45,089
cluster and ingress controllers map the

00:16:40,529 --> 00:16:48,420
outside world to the inside world so if

00:16:45,089 --> 00:16:50,339
you have a host let's say we have three

00:16:48,420 --> 00:16:52,770
worker nodes and we have some stuff

00:16:50,339 --> 00:16:55,800
running on host a and some stuff running

00:16:52,770 --> 00:16:58,110
on host B if you shoot host a in the

00:16:55,800 --> 00:17:00,329
head a WS just terminates the instance

00:16:58,110 --> 00:17:00,990
the other masters are gonna go wait a

00:17:00,329 --> 00:17:03,510
second

00:17:00,990 --> 00:17:06,360
the pods that were scheduled on host a

00:17:03,510 --> 00:17:08,730
are no longer running on a host because

00:17:06,360 --> 00:17:10,410
we can no longer see host a I need to

00:17:08,730 --> 00:17:12,990
schedule them somewhere where do they

00:17:10,410 --> 00:17:15,630
fit okay host C is pretty empty I'm

00:17:12,990 --> 00:17:17,700
going to run them over here change all

00:17:15,630 --> 00:17:19,440
of the pointers all the different proxy

00:17:17,700 --> 00:17:21,300
ports the ingress controller all that

00:17:19,440 --> 00:17:24,750
stuff gets changed over and you're back

00:17:21,300 --> 00:17:27,270
up and running so you can do things like

00:17:24,750 --> 00:17:30,450
upgrade your worker nodes from when one

00:17:27,270 --> 00:17:33,210
AWS instance size to another and never

00:17:30,450 --> 00:17:33,870
have any of your stuff go down and not

00:17:33,210 --> 00:17:36,270
have to change any of your

00:17:33,870 --> 00:17:37,980
configurations or IP address it gets you

00:17:36,270 --> 00:17:41,490
away from pointing at IP addresses and

00:17:37,980 --> 00:17:44,550
and and temporary host names and and and

00:17:41,490 --> 00:17:48,960
makes things move move a little more

00:17:44,550 --> 00:17:52,890
smoothly so how do you run kubernetes in

00:17:48,960 --> 00:17:54,120
the real world there are kind of three

00:17:52,890 --> 00:17:57,990
different things you might interact with

00:17:54,120 --> 00:18:02,850
one is called cops KO PS it's a utility

00:17:57,990 --> 00:18:04,590
for spinning up Kubb clusters in AWS it

00:18:02,850 --> 00:18:07,080
works really well it handles all of the

00:18:04,590 --> 00:18:08,460
AWS specific nature of kubernetes

00:18:07,080 --> 00:18:09,360
closer's so one of the hardest things

00:18:08,460 --> 00:18:12,780
about kubernetes

00:18:09,360 --> 00:18:14,670
is getting a cluster to start it is not

00:18:12,780 --> 00:18:16,770
easy to turn on it is really hard to

00:18:14,670 --> 00:18:18,150
kill once you've turned them on and

00:18:16,770 --> 00:18:20,910
that's kind of its job and getting them

00:18:18,150 --> 00:18:22,560
turned on is kind of involved and prone

00:18:20,910 --> 00:18:25,440
to error so people have created these

00:18:22,560 --> 00:18:28,700
wrapper utilities to make the process a

00:18:25,440 --> 00:18:31,170
little more turnkey for us mere mortals

00:18:28,700 --> 00:18:32,880
the other option and this is the option

00:18:31,170 --> 00:18:35,660
I would encourage you to play with first

00:18:32,880 --> 00:18:38,100
if you have an interest in kubernetes

00:18:35,660 --> 00:18:40,230
play with it on google container engine

00:18:38,100 --> 00:18:43,530
which is a hosted version of kubernetes

00:18:40,230 --> 00:18:45,420
with google the reason i suggest it is

00:18:43,530 --> 00:18:48,150
then you know that you have a well

00:18:45,420 --> 00:18:50,130
working good to go kubernetes cluster to

00:18:48,150 --> 00:18:52,470
play with any problems you're having are

00:18:50,130 --> 00:18:55,260
your misunderstanding of how kubernetes

00:18:52,470 --> 00:18:58,190
works or a configuration mistake and not

00:18:55,260 --> 00:19:01,080
perhaps you set up the cluster poorly

00:18:58,190 --> 00:19:04,860
and then there's also mini cube which

00:19:01,080 --> 00:19:07,380
runs a single cooperate single node

00:19:04,860 --> 00:19:10,380
kubernetes cluster on your laptop using

00:19:07,380 --> 00:19:12,960
vagrant or VirtualBox and

00:19:10,380 --> 00:19:14,299
and Linux systems on your laptop and

00:19:12,960 --> 00:19:16,980
that's a great way to play with

00:19:14,299 --> 00:19:18,570
kubernetes in the small four developer

00:19:16,980 --> 00:19:20,820
environments you can use those same

00:19:18,570 --> 00:19:23,460
definitions to define which containers

00:19:20,820 --> 00:19:25,890
to run at which services to expose on

00:19:23,460 --> 00:19:27,270
your laptop and then just move and use

00:19:25,890 --> 00:19:30,990
them then you're in your production

00:19:27,270 --> 00:19:32,520
clusters so one of the things you got to

00:19:30,990 --> 00:19:34,289
be able to do with containers is

00:19:32,520 --> 00:19:36,330
configure them right we're all 12 factor

00:19:34,289 --> 00:19:38,669
apps now right so we've got to be able

00:19:36,330 --> 00:19:40,380
to push this configuration into these

00:19:38,669 --> 00:19:43,440
containers and communities provides

00:19:40,380 --> 00:19:46,549
several different ways environment

00:19:43,440 --> 00:19:49,110
variables of course we can just define

00:19:46,549 --> 00:19:50,730
environment into the yamo there towards

00:19:49,110 --> 00:19:52,980
the bottom I'll highlight this a little

00:19:50,730 --> 00:19:55,190
bit you can see we've just defined an

00:19:52,980 --> 00:19:57,659
environmental name and we've put in a

00:19:55,190 --> 00:20:00,299
value and that gets injected into the

00:19:57,659 --> 00:20:02,159
containers environment that's great and

00:20:00,299 --> 00:20:04,679
all but a lot of times we don't want to

00:20:02,159 --> 00:20:06,690
expose all of that into our

00:20:04,679 --> 00:20:09,630
configuration we can also use what's

00:20:06,690 --> 00:20:13,080
called config Maps and this lets us map

00:20:09,630 --> 00:20:15,870
sets of variable like things whole files

00:20:13,080 --> 00:20:18,360
or entire directories of files into our

00:20:15,870 --> 00:20:20,220
pods so maybe we don't want to have to

00:20:18,360 --> 00:20:22,500
list every single environment variable

00:20:20,220 --> 00:20:25,409
in that deployment gamal we can say

00:20:22,500 --> 00:20:27,780
here's a config map of 25 environment

00:20:25,409 --> 00:20:29,909
variables take these and apply them into

00:20:27,780 --> 00:20:31,890
this pot and you can pick which map goes

00:20:29,909 --> 00:20:34,320
to which container and it just does them

00:20:31,890 --> 00:20:36,450
all for you you can also do things like

00:20:34,320 --> 00:20:39,419
I want to use this nginx configuration

00:20:36,450 --> 00:20:42,150
file put it here on disk and it will

00:20:39,419 --> 00:20:44,520
grab it from kubernetes configuration

00:20:42,150 --> 00:20:50,370
secrets store and pop it into the pod at

00:20:44,520 --> 00:20:52,710
runtime Cooney's also has a concept of

00:20:50,370 --> 00:20:54,780
secrets these are great we could

00:20:52,710 --> 00:20:56,520
obviously put our database password in

00:20:54,780 --> 00:20:58,950
our API keys into those environment

00:20:56,520 --> 00:21:00,390
variables in our deployment gamal but

00:20:58,950 --> 00:21:02,280
that means that everybody gets to see

00:21:00,390 --> 00:21:04,559
them perhaps we don't want our

00:21:02,280 --> 00:21:05,970
developers to know those and just the

00:21:04,559 --> 00:21:08,159
ops people should hold onto those so

00:21:05,970 --> 00:21:10,260
kubernetes lets you define secrets

00:21:08,159 --> 00:21:12,780
secrets are available like most things

00:21:10,260 --> 00:21:14,789
only inside the namespace that they're

00:21:12,780 --> 00:21:19,559
defined in so you can't share secrets

00:21:14,789 --> 00:21:21,500
across that fence but unfortunately for

00:21:19,559 --> 00:21:24,120
secrets they're not particularly secure

00:21:21,500 --> 00:21:28,440
right now kubernetes stores them as

00:21:24,120 --> 00:21:30,660
base64 encoded text on the master so

00:21:28,440 --> 00:21:34,470
they're not as secret as you might want

00:21:30,660 --> 00:21:36,750
now to be fair they are working towards

00:21:34,470 --> 00:21:38,400
real secrets encrypted on the master

00:21:36,750 --> 00:21:43,440
secrets and this was just a stepping

00:21:38,400 --> 00:21:45,780
stone to to to getting there but it does

00:21:43,440 --> 00:21:48,420
keep secrets that should not be on a

00:21:45,780 --> 00:21:50,190
node from getting to that node so until

00:21:48,420 --> 00:21:52,080
a pod needs to run there that needs

00:21:50,190 --> 00:21:54,690
access to that secret that secret won't

00:21:52,080 --> 00:21:56,880
exist on that node so it does keep them

00:21:54,690 --> 00:21:58,260
off places where they have absolutely no

00:21:56,880 --> 00:22:02,120
business it's just that once they're

00:21:58,260 --> 00:22:02,120
there they're not particularly secret

00:22:02,150 --> 00:22:08,850
and so this is how you use secrets in a

00:22:06,320 --> 00:22:10,230
environment variable so we say hey I

00:22:08,850 --> 00:22:13,320
want to have this database password

00:22:10,230 --> 00:22:15,960
environment variable get its value from

00:22:13,320 --> 00:22:17,730
the secret named rebus project DB

00:22:15,960 --> 00:22:23,220
password and the key in there of

00:22:17,730 --> 00:22:24,690
password you can also use I mean this is

00:22:23,220 --> 00:22:27,930
just a set of hosts right so you could

00:22:24,690 --> 00:22:30,750
run a vault cluster in your kubernetes

00:22:27,930 --> 00:22:33,420
cluster and get your secrets from vault

00:22:30,750 --> 00:22:38,550
or some other kind of really truly

00:22:33,420 --> 00:22:40,140
secure secret storage so because we

00:22:38,550 --> 00:22:42,240
don't know where our containers are

00:22:40,140 --> 00:22:44,400
going to be running centralized logging

00:22:42,240 --> 00:22:46,830
becomes terribly important for being

00:22:44,400 --> 00:22:49,260
able to figure out what's going on so if

00:22:46,830 --> 00:22:50,550
you use Google container engine the logs

00:22:49,260 --> 00:22:52,350
from your cluster goes straight into

00:22:50,550 --> 00:22:54,930
Google's logging tools their stack

00:22:52,350 --> 00:22:56,850
driver or logging system that works fine

00:22:54,930 --> 00:23:00,000
we've had good luck with the efk stack

00:22:56,850 --> 00:23:02,520
which is elasticsearch fluent D or

00:23:00,000 --> 00:23:06,450
fluent bit which is a smaller C version

00:23:02,520 --> 00:23:07,740
of the fluent daemon and Cabana but

00:23:06,450 --> 00:23:09,840
you've got to have this you're not gonna

00:23:07,740 --> 00:23:12,420
tell what's going on right I don't even

00:23:09,840 --> 00:23:14,460
know where Rev says rocks which host

00:23:12,420 --> 00:23:16,500
it's running on I'd have to go and dig

00:23:14,460 --> 00:23:18,060
and find out where it's running to even

00:23:16,500 --> 00:23:19,620
get on to the hosts to then look at logs

00:23:18,060 --> 00:23:23,060
so having centralized Logging's

00:23:19,620 --> 00:23:26,430
important and so for part of that we've

00:23:23,060 --> 00:23:28,320
lightly open-source to this we use it I

00:23:26,430 --> 00:23:32,190
don't know how useful it will be for you

00:23:28,320 --> 00:23:34,230
all but it's a slog for kubb and it

00:23:32,190 --> 00:23:36,500
configures gonna corn and your Python

00:23:34,230 --> 00:23:39,980
apps to use JSON

00:23:36,500 --> 00:23:42,470
logging to standard out and includes

00:23:39,980 --> 00:23:45,049
information that is kubernetes specific

00:23:42,470 --> 00:23:47,450
so like what was the pods IP address

00:23:45,049 --> 00:23:49,720
inside the cluster what host wasn't

00:23:47,450 --> 00:23:52,450
running on what was the name of the pod

00:23:49,720 --> 00:23:55,039
what app is it in those kinds of

00:23:52,450 --> 00:23:56,809
metadata that's kubernetes specific gets

00:23:55,039 --> 00:24:01,909
added into the JSON that's emitted in

00:23:56,809 --> 00:24:04,909
your logging so data persistence is

00:24:01,909 --> 00:24:08,779
pretty important and there's a couple of

00:24:04,909 --> 00:24:13,190
ways to handle it with kubernetes the

00:24:08,779 --> 00:24:16,070
hard way is with persistent volumes this

00:24:13,190 --> 00:24:17,659
works but it's kind of hard to manage

00:24:16,070 --> 00:24:20,750
and it's kind of hard to wrap your brain

00:24:17,659 --> 00:24:22,340
around this would be this is advanced

00:24:20,750 --> 00:24:25,100
kubernetes here what you're doing is

00:24:22,340 --> 00:24:27,350
you're saying I have this volume and I

00:24:25,100 --> 00:24:31,549
want to provide a certain amount of

00:24:27,350 --> 00:24:33,649
space and then your apps claim they make

00:24:31,549 --> 00:24:35,990
a persistent volume claim of how much

00:24:33,649 --> 00:24:38,840
space they need and kubernetes tries to

00:24:35,990 --> 00:24:41,120
match up the claims with the volumes as

00:24:38,840 --> 00:24:43,159
efficiently as it can and then will

00:24:41,120 --> 00:24:46,730
mount those volumes on the hosts or

00:24:43,159 --> 00:24:48,259
those pods with the claims run and then

00:24:46,730 --> 00:24:50,509
if those pods get evicted for some

00:24:48,259 --> 00:24:52,279
reason or the host dies it then remounts

00:24:50,509 --> 00:24:54,169
that volume to the new host where these

00:24:52,279 --> 00:24:56,210
things run and in a perfect world that's

00:24:54,169 --> 00:24:58,399
exactly how it works in a tenet and it

00:24:56,210 --> 00:25:01,399
works that smoothly I have yet to

00:24:58,399 --> 00:25:03,620
experience that perfect world so the

00:25:01,399 --> 00:25:06,080
easier solution is to do off cluster

00:25:03,620 --> 00:25:08,299
storage and this is where I encourage

00:25:06,080 --> 00:25:10,490
people to start and all this is is the

00:25:08,299 --> 00:25:12,740
existing way you were doing storage you

00:25:10,490 --> 00:25:14,360
have a database server somewhere then

00:25:12,740 --> 00:25:16,789
all of your containers then connect to

00:25:14,360 --> 00:25:19,519
and you manage your database server as

00:25:16,789 --> 00:25:21,169
bare-metal or you use Amazon RDS or

00:25:19,519 --> 00:25:24,830
something like that for those kinds of

00:25:21,169 --> 00:25:27,429
persistent data stores one of the things

00:25:24,830 --> 00:25:30,220
that Joshua is going to talk about is

00:25:27,429 --> 00:25:35,029
Patroni Patroni is a templating system

00:25:30,220 --> 00:25:37,759
for highly available Postgres the idea

00:25:35,029 --> 00:25:40,220
is that you could keep a master running

00:25:37,759 --> 00:25:43,490
in the cluster and slaves running in the

00:25:40,220 --> 00:25:45,710
cluster and replicate your data from one

00:25:43,490 --> 00:25:47,779
to another and as containers were killed

00:25:45,710 --> 00:25:49,140
off or nodes died you can keep that

00:25:47,779 --> 00:25:51,150
replication

00:25:49,140 --> 00:25:53,760
working between the nodes to the point

00:25:51,150 --> 00:25:55,080
where you didn't lose data I've heard

00:25:53,760 --> 00:25:57,000
good things about it I've never actually

00:25:55,080 --> 00:25:58,590
played with it and so I wasn't

00:25:57,000 --> 00:26:00,240
comfortable showing you how to do it

00:25:58,590 --> 00:26:01,710
having never done it myself

00:26:00,240 --> 00:26:03,660
but I do want to mention it in case

00:26:01,710 --> 00:26:08,910
you're interested in playing a little

00:26:03,660 --> 00:26:11,160
fast and loose with your data oh and

00:26:08,910 --> 00:26:13,650
this was out of this slide is out of

00:26:11,160 --> 00:26:15,510
order I'm sorry the idea here is your

00:26:13,650 --> 00:26:17,820
persistent data instance is just an

00:26:15,510 --> 00:26:20,910
instance there outside of your actual

00:26:17,820 --> 00:26:23,850
blue kubernetes cluster just inside the

00:26:20,910 --> 00:26:27,750
same VPC so that the cluster can access

00:26:23,850 --> 00:26:30,240
it but it's not actually running on

00:26:27,750 --> 00:26:32,910
kubernetes it's just a bare metal node

00:26:30,240 --> 00:26:37,520
using ansible or puppet or whatever you

00:26:32,910 --> 00:26:40,530
want to use or do it by hand right so

00:26:37,520 --> 00:26:43,410
one thing that I do not have a ton of

00:26:40,530 --> 00:26:46,350
experience with but I know is useful is

00:26:43,410 --> 00:26:48,630
helm helm is a package management system

00:26:46,350 --> 00:26:53,400
for kubernetes you can think of it as

00:26:48,630 --> 00:26:56,040
template emotes but it's useful in more

00:26:53,400 --> 00:26:58,470
complicated scenarios so you can say run

00:26:56,040 --> 00:27:00,630
me a console cluster and I want to have

00:26:58,470 --> 00:27:03,030
this many nodes and it will figure out

00:27:00,630 --> 00:27:05,460
what all it needs to be applied to the

00:27:03,030 --> 00:27:08,610
kubernetes api to get you up and working

00:27:05,460 --> 00:27:11,160
console cluster with the Federation and

00:27:08,610 --> 00:27:12,870
the leader election and handle all that

00:27:11,160 --> 00:27:15,690
stuff for you so you can build these

00:27:12,870 --> 00:27:18,000
template able systems to the point where

00:27:15,690 --> 00:27:21,600
I should be able to take your system and

00:27:18,000 --> 00:27:23,310
helm install it and I just have that

00:27:21,600 --> 00:27:24,330
then running and working on my classroom

00:27:23,310 --> 00:27:26,130
and I shouldn't really have to do

00:27:24,330 --> 00:27:32,430
anything else other than maybe a little

00:27:26,130 --> 00:27:34,910
bit of Secrets management so one of the

00:27:32,430 --> 00:27:40,830
things because kubernetes is just an API

00:27:34,910 --> 00:27:43,440
really we can use the API from Python so

00:27:40,830 --> 00:27:46,740
this is all you have to write if you've

00:27:43,440 --> 00:27:49,050
got coop CTL proxy running on your

00:27:46,740 --> 00:27:51,570
localhost or you have a well-formed coop

00:27:49,050 --> 00:27:53,730
config file in your home directory all

00:27:51,570 --> 00:27:56,580
you have to write to get a list of all

00:27:53,730 --> 00:27:59,310
the pods running in your cluster and you

00:27:56,580 --> 00:28:01,380
can and I'm just printing out their pods

00:27:59,310 --> 00:28:03,650
IP address the namespace and the name of

00:28:01,380 --> 00:28:03,650
the pod

00:28:04,250 --> 00:28:10,740
this is a generated API off the swagger

00:28:08,220 --> 00:28:13,529
Doc's from kubernetes and it's kept up

00:28:10,740 --> 00:28:16,110
to date with releases so this you should

00:28:13,529 --> 00:28:19,140
have always have full access of the API

00:28:16,110 --> 00:28:21,590
from Python so you do not have to build

00:28:19,140 --> 00:28:24,000
your tooling and go unless you want to

00:28:21,590 --> 00:28:26,220
so what does that look like here's the

00:28:24,000 --> 00:28:28,740
output for that is just I ran this on

00:28:26,220 --> 00:28:32,970
our web sis production cluster and you

00:28:28,740 --> 00:28:34,919
can see various namespaces we've created

00:28:32,970 --> 00:28:38,070
there in the middle and the various pot

00:28:34,919 --> 00:28:40,919
IP addresses and then the names of the

00:28:38,070 --> 00:28:43,260
actual pods you'll see that it takes the

00:28:40,919 --> 00:28:46,139
name that I gave them like rebus rocks

00:28:43,260 --> 00:28:48,360
and then appends a uniqueness to it and

00:28:46,139 --> 00:28:50,669
that's that particular instance of that

00:28:48,360 --> 00:28:53,700
pod so every time a new pod comes up it

00:28:50,669 --> 00:28:56,130
gets its own unique name and if it gets

00:28:53,700 --> 00:28:58,289
killed a new one comes up so you can see

00:28:56,130 --> 00:29:01,019
a differentiation of logs even if it's

00:28:58,289 --> 00:29:02,460
the same container one got evicted and a

00:29:01,019 --> 00:29:07,919
new one got created you'll see that name

00:29:02,460 --> 00:29:10,769
change happen so everything in

00:29:07,919 --> 00:29:14,159
kubernetes works with a operator or a

00:29:10,769 --> 00:29:16,769
controller and why would you want to

00:29:14,159 --> 00:29:20,010
create your own well like kublai go you

00:29:16,769 --> 00:29:22,919
can create your own tooling that takes

00:29:20,010 --> 00:29:24,779
action when these things happen right so

00:29:22,919 --> 00:29:28,049
you add a little bit of annotation of

00:29:24,779 --> 00:29:29,669
your own and you can watch the cluster

00:29:28,049 --> 00:29:32,490
using that little bit of Python and say

00:29:29,669 --> 00:29:34,500
ah I'm seeing a new pod come up that's

00:29:32,490 --> 00:29:37,860
annotated Frank needs to do something to

00:29:34,500 --> 00:29:39,419
it and I see that and I can go take

00:29:37,860 --> 00:29:41,039
action either inside the cluster or

00:29:39,419 --> 00:29:43,320
outside the cluster however I need to

00:29:41,039 --> 00:29:45,299
see whatever I want to have happen when

00:29:43,320 --> 00:29:48,450
that annotation comes up I can make

00:29:45,299 --> 00:29:51,269
happen so here's some examples of

00:29:48,450 --> 00:29:52,919
operators you could build type of

00:29:51,269 --> 00:29:55,200
message into slack anytime somebody

00:29:52,919 --> 00:29:57,630
creates a new deployment when somebody's

00:29:55,200 --> 00:29:59,279
watching a whole new thing pop a note

00:29:57,630 --> 00:30:02,460
into slack so we know that that happened

00:29:59,279 --> 00:30:04,590
or maybe we want to check any time pods

00:30:02,460 --> 00:30:06,360
come up and down for whatever reason we

00:30:04,590 --> 00:30:08,010
want to get that message in slack that

00:30:06,360 --> 00:30:10,049
would be like you know 10 or 15 lines of

00:30:08,010 --> 00:30:11,850
Python nothing particularly hard package

00:30:10,049 --> 00:30:15,690
that up in a container tell kubernetes

00:30:11,850 --> 00:30:18,660
to run it you could watch your Django

00:30:15,690 --> 00:30:22,160
and look for the database connection

00:30:18,660 --> 00:30:24,840
information and automatically backup any

00:30:22,160 --> 00:30:27,030
databases that are running and being

00:30:24,840 --> 00:30:28,950
used by your cluster without having to

00:30:27,030 --> 00:30:31,260
go in and define each one and you just

00:30:28,950 --> 00:30:34,350
say oh look here comes a new Frank's

00:30:31,260 --> 00:30:36,510
test system 47 just came up its

00:30:34,350 --> 00:30:38,580
annotated as a backup equals true so I'm

00:30:36,510 --> 00:30:40,800
gonna go back it up and put it to s3 and

00:30:38,580 --> 00:30:42,360
I have one centralized system for

00:30:40,800 --> 00:30:43,350
dealing with that it's just like we have

00:30:42,360 --> 00:30:45,630
centralized logging we can have

00:30:43,350 --> 00:30:47,220
centralized control because we've

00:30:45,630 --> 00:30:51,440
abstracted out the kind of the whole

00:30:47,220 --> 00:30:51,440
concept of ops to this API we can watch

00:30:51,890 --> 00:30:57,090
maybe you have really complicated and

00:30:54,180 --> 00:30:59,280
rollout scenarios where you have no six

00:30:57,090 --> 00:31:01,350
hours of collect static runs before

00:30:59,280 --> 00:31:03,840
finally things come up well maybe you

00:31:01,350 --> 00:31:05,820
want to handle low amounts of downtime

00:31:03,840 --> 00:31:08,430
by spinning up an entirely new service

00:31:05,820 --> 00:31:10,200
once it's already spin down the old one

00:31:08,430 --> 00:31:11,640
and move traffic over to the new one you

00:31:10,200 --> 00:31:12,870
could orchestrate that with just a

00:31:11,640 --> 00:31:14,850
little bit of Python even if it's

00:31:12,870 --> 00:31:17,390
something kubernetes itself doesn't

00:31:14,850 --> 00:31:17,390
really support

00:31:21,080 --> 00:31:25,220
hopefully that is enough information to

00:31:23,090 --> 00:31:28,940
make you interested in kubernetes but

00:31:25,220 --> 00:31:32,720
I'm sure you probably have questions so

00:31:28,940 --> 00:31:34,279
how do you handle like CPU like I have

00:31:32,720 --> 00:31:36,769
some service it's gonna need a lot of

00:31:34,279 --> 00:31:39,110
CPU and I don't want to run you know

00:31:36,769 --> 00:31:40,970
these two services on the same node

00:31:39,110 --> 00:31:43,190
because they are gonna conflict by each

00:31:40,970 --> 00:31:44,539
other and that kind of stuff so in the

00:31:43,190 --> 00:31:46,460
effort of fitting things on the slide

00:31:44,539 --> 00:31:49,909
and not melting your brain too much I

00:31:46,460 --> 00:31:51,619
left out resource quotas which are just

00:31:49,909 --> 00:31:53,960
items that are in that same yeah Mille

00:31:51,619 --> 00:31:56,210
and you say this takes this much memory

00:31:53,960 --> 00:31:57,980
it has a soft limit of this and a hard

00:31:56,210 --> 00:32:01,639
limit of this and you can have that for

00:31:57,980 --> 00:32:03,259
CPU memory and storage and Carre's will

00:32:01,639 --> 00:32:05,960
handle it much like any other kind of

00:32:03,259 --> 00:32:08,749
quota system so if it reaches its soft

00:32:05,960 --> 00:32:10,639
limit information goes into the API of

00:32:08,749 --> 00:32:12,350
I'm at my soft limit and my hard limit

00:32:10,639 --> 00:32:16,759
it actually kills the pod and then

00:32:12,350 --> 00:32:19,429
recreates it so you can in tag pods by

00:32:16,759 --> 00:32:20,809
how much resources they should use and

00:32:19,429 --> 00:32:23,029
then where you want to stop them if they

00:32:20,809 --> 00:32:25,789
grow beyond that you can also then

00:32:23,029 --> 00:32:29,419
target nodes so you may want to have a

00:32:25,789 --> 00:32:31,759
cluster with some memory heavy AWS

00:32:29,419 --> 00:32:34,759
instances and some CPU heavy AWS

00:32:31,759 --> 00:32:36,320
instances and you can say this pod needs

00:32:34,759 --> 00:32:38,779
to run on one of my memory heavy

00:32:36,320 --> 00:32:41,359
instances and these should run on my CPU

00:32:38,779 --> 00:32:45,409
heavy instances only and that's how you

00:32:41,359 --> 00:32:48,529
can do that and so it will stack as best

00:32:45,409 --> 00:32:50,869
it can into those nodes based on the

00:32:48,529 --> 00:32:54,080
values you've given it it will overflow

00:32:50,869 --> 00:32:55,879
if you don't put any resources so like

00:32:54,080 --> 00:32:57,799
in my examples it will just keep packing

00:32:55,879 --> 00:33:01,429
them into nodes and you will eventually

00:32:57,799 --> 00:33:03,859
hit swap and things like that but if it

00:33:01,429 --> 00:33:05,299
can't then find a spot to put it because

00:33:03,859 --> 00:33:07,519
there's not enough resources to put a

00:33:05,299 --> 00:33:09,590
pod it will continually try to find a

00:33:07,519 --> 00:33:11,029
spot for it and you will see information

00:33:09,590 --> 00:33:12,649
in the dashboard and the logs that I

00:33:11,029 --> 00:33:14,840
can't run this pod I do not have

00:33:12,649 --> 00:33:16,519
resources to do it you add another node

00:33:14,840 --> 00:33:20,330
to your cluster and it immediately puts

00:33:16,519 --> 00:33:23,570
it right there sorry I'm totally new to

00:33:20,330 --> 00:33:26,659
kubernetes on the ingress point does it

00:33:23,570 --> 00:33:28,609
come to the nodes or to the pods and

00:33:26,659 --> 00:33:30,259
then so it comes to the ingress

00:33:28,609 --> 00:33:31,999
controller right and then there's

00:33:30,259 --> 00:33:34,429
proxied inside the cluster from there

00:33:31,999 --> 00:33:36,889
but you would think oh this is gonna

00:33:34,429 --> 00:33:39,350
this extra hoppiness yeah pain it really

00:33:36,889 --> 00:33:42,200
in practice at a really really huge

00:33:39,350 --> 00:33:44,059
scale it matters but for your day-to-day

00:33:42,200 --> 00:33:45,649
use no one's ever gonna notice that that

00:33:44,059 --> 00:33:47,990
extra hop is in there it's a little go

00:33:45,649 --> 00:33:51,230
proxy and it's super efficient so

00:33:47,990 --> 00:33:53,779
considering that the pods auto replicate

00:33:51,230 --> 00:33:56,269
I guess the load do I still need a load

00:33:53,779 --> 00:33:58,070
balancer in front of it or no you need

00:33:56,269 --> 00:34:00,409
uh so the ingress controller from the

00:33:58,070 --> 00:34:02,869
outside will be the load balancers right

00:34:00,409 --> 00:34:04,669
and so then it comes to the cluster and

00:34:02,869 --> 00:34:06,409
then it load balances from there and

00:34:04,669 --> 00:34:08,179
that handles all that where is this pod

00:34:06,409 --> 00:34:10,220
running and it just shoots it to where

00:34:08,179 --> 00:34:11,329
it needs to go inside the cluster and

00:34:10,220 --> 00:34:14,829
you don't have to think about it

00:34:11,329 --> 00:34:19,159
so in thinking about your application

00:34:14,829 --> 00:34:22,250
what what are some ways to make a

00:34:19,159 --> 00:34:25,429
decision on does this solve more

00:34:22,250 --> 00:34:27,829
problems than it creates in terms of how

00:34:25,429 --> 00:34:29,929
do you decide at what point this is

00:34:27,829 --> 00:34:31,579
gonna do that for me it's gonna solve

00:34:29,929 --> 00:34:33,290
more problems than it creates I mean

00:34:31,579 --> 00:34:35,929
that's it's a tough call that's why with

00:34:33,290 --> 00:34:38,119
any other two ways on one level I think

00:34:35,929 --> 00:34:39,589
it's easier sometimes to SSH into a box

00:34:38,119 --> 00:34:41,659
and I've got to install the thing and

00:34:39,589 --> 00:34:44,030
then do it by hand but that's not

00:34:41,659 --> 00:34:45,980
reproducible with anyway and so like I

00:34:44,030 --> 00:34:48,649
mean every tool has its pros and cons

00:34:45,980 --> 00:34:50,720
the thing I like about this one is that

00:34:48,649 --> 00:34:52,399
I'm gonna be doing stuff with containers

00:34:50,720 --> 00:34:54,919
so I need some sort of container based

00:34:52,399 --> 00:34:58,190
system for the most part and most other

00:34:54,919 --> 00:34:59,420
non you know ansible and puppeted chef

00:34:58,190 --> 00:35:01,309
and things like that are not kind of

00:34:59,420 --> 00:35:02,750
container focused so I don't see them as

00:35:01,309 --> 00:35:09,109
good tools for solving things around

00:35:02,750 --> 00:35:11,150
containers that much where you pick like

00:35:09,109 --> 00:35:13,700
which one you use or if you use one at

00:35:11,150 --> 00:35:16,010
all is hard the thing I like about this

00:35:13,700 --> 00:35:18,020
is it really does free me up to not

00:35:16,010 --> 00:35:20,510
think about the mundane things like

00:35:18,020 --> 00:35:22,910
where is this going to run and what pork

00:35:20,510 --> 00:35:24,859
is available to open on it and how do i

00:35:22,910 --> 00:35:25,940
proxy from this port to that port I

00:35:24,859 --> 00:35:29,599
don't have to think about any of those

00:35:25,940 --> 00:35:32,150
details and dump it does give me the

00:35:29,599 --> 00:35:33,890
power to listen on the API for when

00:35:32,150 --> 00:35:35,630
things change and take some sort of

00:35:33,890 --> 00:35:38,270
action so I like it for that I don't

00:35:35,630 --> 00:35:41,599
know that you know if you have one app

00:35:38,270 --> 00:35:44,210
and you run it and you do a deploy once

00:35:41,599 --> 00:35:47,250
a week this is probably overkill if

00:35:44,210 --> 00:35:50,070
you're managing 50 micro-services

00:35:47,250 --> 00:35:51,810
and you deploy ten times a day you

00:35:50,070 --> 00:35:53,220
probably already you're building

00:35:51,810 --> 00:35:55,800
something like this or using something

00:35:53,220 --> 00:35:58,410
like this to manage all that thanks very

00:35:55,800 --> 00:36:01,170
much for the introduction I was learning

00:35:58,410 --> 00:36:02,490
what the next step might be how did you

00:36:01,170 --> 00:36:03,690
go about learning this do you have

00:36:02,490 --> 00:36:05,310
resources that you thought were

00:36:03,690 --> 00:36:07,140
particularly helpful and could you

00:36:05,310 --> 00:36:10,140
recommend them soku Bernese is a very

00:36:07,140 --> 00:36:13,230
fast-moving Beast I think I first

00:36:10,140 --> 00:36:16,680
started playing with it like 1.2 and

00:36:13,230 --> 00:36:19,890
it's already AI 1.7 and they come out

00:36:16,680 --> 00:36:21,870
about every six months and what they

00:36:19,890 --> 00:36:24,120
have actually a fairly nice process of

00:36:21,870 --> 00:36:26,520
things come out in they are marked as

00:36:24,120 --> 00:36:29,550
alpha and then they are marked as beta

00:36:26,520 --> 00:36:31,410
and then there eventually becomes stable

00:36:29,550 --> 00:36:33,270
and once they get to beta the yamo

00:36:31,410 --> 00:36:34,770
configuration for the most part doesn't

00:36:33,270 --> 00:36:36,840
change and you can pretty much just pick

00:36:34,770 --> 00:36:38,700
it up and move it over the Alpha stuff

00:36:36,840 --> 00:36:43,470
is pretty alpha and good luck if it

00:36:38,700 --> 00:36:46,140
works but so the documentation often

00:36:43,470 --> 00:36:48,780
lags behind the version just a bit on

00:36:46,140 --> 00:36:51,270
the newer stuff or this stuff that just

00:36:48,780 --> 00:36:53,070
recently changed a lot so the

00:36:51,270 --> 00:36:56,010
documentation should be an amazingly

00:36:53,070 --> 00:36:57,690
great resource and it is as long as you

00:36:56,010 --> 00:37:00,090
keep in mind that if this thing came out

00:36:57,690 --> 00:37:02,100
in the last version the docs may be

00:37:00,090 --> 00:37:05,370
wrong or if it just moved from alpha to

00:37:02,100 --> 00:37:08,220
beta the docs may be slightly off right

00:37:05,370 --> 00:37:09,930
and so that's but there's really it's

00:37:08,220 --> 00:37:11,160
mostly blog post in tutorials where

00:37:09,930 --> 00:37:12,990
you're like how did somebody else go

00:37:11,160 --> 00:37:15,210
about this let me go look at their

00:37:12,990 --> 00:37:15,960
kubernetes configuration and then some

00:37:15,210 --> 00:37:18,030
playing around

00:37:15,960 --> 00:37:20,520
there is no really great here's the book

00:37:18,030 --> 00:37:23,130
on kubernetes and and and it's and it

00:37:20,520 --> 00:37:24,530
solves all of your problems thank you

00:37:23,130 --> 00:37:33,099
Frank

00:37:24,530 --> 00:37:33,099

YouTube URL: https://www.youtube.com/watch?v=4LpaxvKsSlo


