Title: DjangoCon US 2017 - The denormalized query engine design pattern by Simon WIllison
Publication date: 2017-09-08
Playlist: DjangoCon US 2017
Description: 
	DjangoCon US 2017 - The denormalized query engine design pattern by Simon WIllison

Most web applications need to offer search functionality. Open source tools like Solr and Elasticsearch are a powerful option for building custom search enginesâ€¦ but it turns out they can be used for way more than just search.

By treating your search engine as a denormalization layer, you can use it to answer queries that would be too expensive to answer using your core relational database. Questions like â€œWhat are the top twenty tags used by my users from Spain?â€ or â€œWhat are the most common times of day for events to start?â€ or â€œWhich articles contain addresses within 500 miles of Toronto?â€.

With the denormalized query engine design pattern, modifications to relational data are published to a denormalized schema in Elasticsearch or Solr. Data queries can then be answered using either the relational database or the search engine, depending on the nature of the specific query. The search engine returns database IDs, which are inflated from the database before being displayed to a user - ensuring that users never see stale data even if the search engine is not 100% up to date with the latest changes. This opens up all kinds of new capabilities for slicing, dicing and exploring data.

In this talk, Iâ€™ll be illustrating this pattern by focusing on Elasticsearch - showing how it can be used with Django to bring new capabilities to your application. Iâ€™ll discuss the challenge of keeping data synchronized between a relational database and a search engine, and show examples of features that become much easier to build once you have this denormalization layer in place.

Use-cases I explore will include:

Finding interesting patterns in your data
Building a recommendation engine
Advanced geographical search and filtering
Reacting to recent user activity on your site
Analyzing a new large dataset using Elasticsearch and Kibana

This talk was presented at: https://2017.djangocon.us/talks/the-denormalized-query-engine-design-pattern/

LINKS:
Follow Carlos Martinez ğŸ‘‡
On Twitter: https://twitter.com/simonw
Official homepage: http://lanyrd.com/profile/simonw/
Github: https://github.com/simonw/

Follow DjangCon US ğŸ‘‡
https://twitter.com/djangocon

Follow DEFNA ğŸ‘‡
https://twitter.com/defnado
https://www.defna.org/
Captions: 
	00:00:00,000 --> 00:00:16,440
[Music]

00:00:13,460 --> 00:00:18,360
so yeah so I will start with a little

00:00:16,440 --> 00:00:20,250
bit of a career introduction which I

00:00:18,360 --> 00:00:21,960
promise is from is very relevant to the

00:00:20,250 --> 00:00:24,689
talk I'll be talking about a design

00:00:21,960 --> 00:00:26,880
pattern that is sort of I've stalked

00:00:24,689 --> 00:00:29,730
throughout my career and so I started

00:00:26,880 --> 00:00:31,560
out many years ago at the long at a tiny

00:00:29,730 --> 00:00:33,720
little local newspaper in Kansas called

00:00:31,560 --> 00:00:35,399
the Lawrence Journal world working on a

00:00:33,720 --> 00:00:38,220
web framework that eventually became

00:00:35,399 --> 00:00:40,530
Django about a year after I left that I

00:00:38,220 --> 00:00:42,510
moved on to work at Yahoo where I was I

00:00:40,530 --> 00:00:44,430
briefly tinkered with the Flickr team

00:00:42,510 --> 00:00:46,469
and then worked on various like product

00:00:44,430 --> 00:00:47,969
development and research projects I did

00:00:46,469 --> 00:00:50,699
data journalism at the Guardian which

00:00:47,969 --> 00:00:52,499
was the most fun job ever because it's

00:00:50,699 --> 00:00:54,420
you get to work with data on like

00:00:52,499 --> 00:00:55,739
journalism deadlines which you know that

00:00:54,420 --> 00:00:57,870
sort of ties back into the original

00:00:55,739 --> 00:00:59,699
tagline for Django as well and then

00:00:57,870 --> 00:01:01,859
after the Guardian I did a startup

00:00:59,699 --> 00:01:03,960
my co-founded lanyard with my wife

00:01:01,859 --> 00:01:05,790
Natalie who's there in the front ran

00:01:03,960 --> 00:01:08,490
that for three years and then sold that

00:01:05,790 --> 00:01:11,700
to Eventbrite so now through a various

00:01:08,490 --> 00:01:13,590
paths of different different machine

00:01:11,700 --> 00:01:16,440
Asians I'm an engineering director of

00:01:13,590 --> 00:01:18,240
Eventbrite over in San Francisco but the

00:01:16,440 --> 00:01:20,930
thing I want to talk about today is a

00:01:18,240 --> 00:01:23,460
design pattern and design patterns

00:01:20,930 --> 00:01:25,590
really the power of design patterns is

00:01:23,460 --> 00:01:27,810
almost entirely in the name you know

00:01:25,590 --> 00:01:29,520
nobody really invents design patterns

00:01:27,810 --> 00:01:30,900
you more sort of look at something that

00:01:29,520 --> 00:01:32,730
other people are doing and you slap a

00:01:30,900 --> 00:01:34,500
name on it and then it becomes something

00:01:32,730 --> 00:01:36,090
which people can talk about and the

00:01:34,500 --> 00:01:37,950
pattern I want to describe today is one

00:01:36,090 --> 00:01:39,630
which to my surprise no one else seems

00:01:37,950 --> 00:01:41,130
to have slapped a name on yet so I'm

00:01:39,630 --> 00:01:42,600
slapping the name on this and I want to

00:01:41,130 --> 00:01:43,980
start getting discussions going because

00:01:42,600 --> 00:01:46,770
I think it's a pattern that can help out

00:01:43,980 --> 00:01:50,220
with a lot of different projects in in a

00:01:46,770 --> 00:01:51,960
lot of different ways and the name I've

00:01:50,220 --> 00:01:54,540
picked for this pattern is the

00:01:51,960 --> 00:01:55,890
denormalized query engine which I hope

00:01:54,540 --> 00:01:58,350
is just snappy enough that it'll work

00:01:55,890 --> 00:02:01,590
for people and essentially this is a way

00:01:58,350 --> 00:02:04,140
of working of taking a system built on a

00:02:01,590 --> 00:02:06,480
relational database and enhancing it

00:02:04,140 --> 00:02:08,239
using a search index such that you can

00:02:06,480 --> 00:02:11,220
do a huge amount of additional

00:02:08,239 --> 00:02:12,660
interesting things with it so the key

00:02:11,220 --> 00:02:14,010
idea is you have your relational

00:02:12,660 --> 00:02:16,650
database as your single point of truth

00:02:14,010 --> 00:02:18,390
and you know we we all got kind of

00:02:16,650 --> 00:02:19,950
infatuated with no sequel a few years

00:02:18,390 --> 00:02:21,780
ago I feel like that infatuation has

00:02:19,950 --> 00:02:23,700
worn off a little bit it turns out that

00:02:21,780 --> 00:02:25,330
forty years of computer science has made

00:02:23,700 --> 00:02:27,670
relational databases have to tick

00:02:25,330 --> 00:02:29,830
li like reliable place to keep the data

00:02:27,670 --> 00:02:31,900
that you care about but anyway you have

00:02:29,830 --> 00:02:34,180
your data and your relational database

00:02:31,900 --> 00:02:36,430
and you then denormalize the relevant

00:02:34,180 --> 00:02:38,350
data into a separate search index so you

00:02:36,430 --> 00:02:40,000
take all of that data you think think

00:02:38,350 --> 00:02:42,640
about the bits that would make most

00:02:40,000 --> 00:02:44,410
sense to b2b denormalized to be

00:02:42,640 --> 00:02:46,420
queryable in different ways and you get

00:02:44,410 --> 00:02:48,250
those into a search index and then you

00:02:46,420 --> 00:02:50,290
invest an enormous amount of effort in

00:02:48,250 --> 00:02:51,640
synchronization between the two making

00:02:50,290 --> 00:02:53,410
sure that whenever somebody changes

00:02:51,640 --> 00:02:55,450
something in that database you get that

00:02:53,410 --> 00:02:57,490
into the search indexes reliably and as

00:02:55,450 --> 00:02:59,140
quickly as possible and that's the hard

00:02:57,490 --> 00:03:01,050
bit I'll be talking a little bit more

00:02:59,140 --> 00:03:05,260
about some strategies for doing that

00:03:01,050 --> 00:03:06,790
towards the end of this talk and what

00:03:05,260 --> 00:03:08,950
why would you want to do this well

00:03:06,790 --> 00:03:11,140
really this is a way of addressing some

00:03:08,950 --> 00:03:13,570
of the weaknesses that most relational

00:03:11,140 --> 00:03:15,100
databases have first one which I'd

00:03:13,570 --> 00:03:16,900
imagine many people have run into is

00:03:15,100 --> 00:03:18,910
relational databases aren't really very

00:03:16,900 --> 00:03:20,709
good at counting things if anyone's ever

00:03:18,910 --> 00:03:22,420
implemented pagination where you have

00:03:20,709 --> 00:03:24,610
like two hundred thousand rows in a

00:03:22,420 --> 00:03:26,620
table and you want to do page one page

00:03:24,610 --> 00:03:28,630
two page three you'll find that the bit

00:03:26,620 --> 00:03:30,519
way you can't select where you count

00:03:28,630 --> 00:03:31,930
star against that table is the bit that

00:03:30,519 --> 00:03:34,570
actually starts to hurt you first

00:03:31,930 --> 00:03:36,489
because the database has to scan through

00:03:34,570 --> 00:03:40,269
all 200 thousand rows just to generate

00:03:36,489 --> 00:03:41,769
that count as a general rule anytime

00:03:40,269 --> 00:03:44,440
you're doing something that that end

00:03:41,769 --> 00:03:46,120
users are going to be accessing you need

00:03:44,440 --> 00:03:48,600
to avoid queries that we'd more than say

00:03:46,120 --> 00:03:50,950
a few thousand rows at a time and

00:03:48,600 --> 00:03:53,709
relational databases are insanely fast

00:03:50,950 --> 00:03:55,540
at primary key lookups and they're

00:03:53,709 --> 00:03:57,220
insanely fast at range queries against

00:03:55,540 --> 00:03:59,110
an index but if you've got a query that

00:03:57,220 --> 00:04:00,820
needs to quit needs to invest it needs

00:03:59,110 --> 00:04:03,130
to look at ten thousand rows that's

00:04:00,820 --> 00:04:04,180
going to add up to one to three seconds

00:04:03,130 --> 00:04:07,000
and it's going to be something you can't

00:04:04,180 --> 00:04:09,760
deploy in a in an application that end

00:04:07,000 --> 00:04:11,230
users hating all of the time and there's

00:04:09,760 --> 00:04:13,360
one this is currently specific to my

00:04:11,230 --> 00:04:16,180
secret I believe post-crisis fix this

00:04:13,360 --> 00:04:17,650
one but my sequel can only use one one

00:04:16,180 --> 00:04:20,530
of the indexes to find in the database

00:04:17,650 --> 00:04:22,450
full query that's being executed so you

00:04:20,530 --> 00:04:26,820
might think that you can snap indexes on

00:04:22,450 --> 00:04:29,950
the Aged column and on the and on the

00:04:26,820 --> 00:04:31,240
and on the the job title column do

00:04:29,950 --> 00:04:33,070
searches across both of those at once

00:04:31,240 --> 00:04:34,930
but actually it'll pick one of those two

00:04:33,070 --> 00:04:36,669
indexes and it'll use that to speed up

00:04:34,930 --> 00:04:38,680
your query so actually the moment you

00:04:36,669 --> 00:04:39,520
start doing more complicated lookups the

00:04:38,680 --> 00:04:41,500
database index

00:04:39,520 --> 00:04:43,780
scheme really starts starts undermining

00:04:41,500 --> 00:04:47,379
you meanwhile search engines have a

00:04:43,780 --> 00:04:49,419
whole bunch of strengths firstly modern

00:04:47,379 --> 00:04:50,919
search engines and I'm mainly talking

00:04:49,419 --> 00:04:52,270
I'll be mainly talking about elastic

00:04:50,919 --> 00:04:54,699
search in this talk but the same is true

00:04:52,270 --> 00:04:56,379
for solar and other search engines as

00:04:54,699 --> 00:04:58,990
well are really good at scaling

00:04:56,379 --> 00:05:01,030
horizontally like you you can take a

00:04:58,990 --> 00:05:03,280
system like elastic search and literally

00:05:01,030 --> 00:05:05,199
just throw more machines throw more

00:05:03,280 --> 00:05:07,479
nodes at it and it will rebalance across

00:05:05,199 --> 00:05:09,039
that full cluster and give you more read

00:05:07,479 --> 00:05:12,069
performance more write performance and

00:05:09,039 --> 00:05:14,259
and just general them and improvements

00:05:12,069 --> 00:05:16,590
in your capacity as you scale that up

00:05:14,259 --> 00:05:19,060
and they're really good at counting

00:05:16,590 --> 00:05:20,580
database is not so it's not so great at

00:05:19,060 --> 00:05:23,110
counting search engines are really

00:05:20,580 --> 00:05:24,729
really fast at this and they're great at

00:05:23,110 --> 00:05:27,360
aggregations as well which I'll talk

00:05:24,729 --> 00:05:29,560
about in more detail in a moment

00:05:27,360 --> 00:05:31,479
you can run queries across multiple

00:05:29,560 --> 00:05:33,430
indexed fields so if you have a super

00:05:31,479 --> 00:05:35,650
complicated query where you're you're

00:05:33,430 --> 00:05:36,819
you're looking at like four or five

00:05:35,650 --> 00:05:38,650
different fields and combining those

00:05:36,819 --> 00:05:41,139
together a search engine will make short

00:05:38,650 --> 00:05:42,879
work of that and they're unsurprisingly

00:05:41,139 --> 00:05:44,710
very good at relevance calculations and

00:05:42,879 --> 00:05:47,289
scoring because that's kind of the

00:05:44,710 --> 00:05:48,669
nature of the beast and they give you

00:05:47,289 --> 00:05:50,919
text search you get all of these

00:05:48,669 --> 00:05:52,900
benefits and you can implement full text

00:05:50,919 --> 00:05:55,330
search as well I deliberately left that

00:05:52,900 --> 00:05:56,860
one to last because my interest in

00:05:55,330 --> 00:05:59,469
search engines goes way beyond just

00:05:56,860 --> 00:06:01,509
using them to to search for text that

00:05:59,469 --> 00:06:02,919
users have entered in I think that this

00:06:01,509 --> 00:06:04,210
entire design pattern revolves around

00:06:02,919 --> 00:06:05,919
the fact that search engines have

00:06:04,210 --> 00:06:10,599
strengths beyond just being able to

00:06:05,919 --> 00:06:14,110
implement a full-text search so I'm

00:06:10,599 --> 00:06:15,759
going to roll back in time to 2005 to to

00:06:14,110 --> 00:06:17,830
talk about the first time I saw this

00:06:15,759 --> 00:06:22,240
pattern in the wild and that was at

00:06:17,830 --> 00:06:25,389
Flickr the photo photo sharing site so

00:06:22,240 --> 00:06:27,819
back in 2005 Flickr were having enormous

00:06:25,389 --> 00:06:30,580
scaling problems because it was the

00:06:27,819 --> 00:06:32,020
birth of web 2.0 was social myths it was

00:06:30,580 --> 00:06:33,490
I don't think social media was even the

00:06:32,020 --> 00:06:35,740
term back then they had an enormous

00:06:33,490 --> 00:06:37,210
quantity of users coming into the annex

00:06:35,740 --> 00:06:39,039
of the service and uploading photos and

00:06:37,210 --> 00:06:41,169
there's an industry we haven't really

00:06:39,039 --> 00:06:42,550
figured out how to do this web scale

00:06:41,169 --> 00:06:44,949
engineering thing yet

00:06:42,550 --> 00:06:46,990
sites like Flickr Flickr were having to

00:06:44,949 --> 00:06:48,669
figure this stuff stuff out from scratch

00:06:46,990 --> 00:06:50,770
and figure out how to scale up to handle

00:06:48,669 --> 00:06:52,539
these these giant and giant numbers

00:06:50,770 --> 00:06:53,680
these enormous numbers of users and huge

00:06:52,539 --> 00:06:56,860
amounts of data

00:06:53,680 --> 00:06:58,390
the CTO at flicker Cal Henderson wrote a

00:06:56,860 --> 00:07:01,300
book about this called building scalable

00:06:58,390 --> 00:07:03,130
websites which came out over a decade

00:07:01,300 --> 00:07:04,900
ago now and I think is still very

00:07:03,130 --> 00:07:06,610
relevant today because it essentially

00:07:04,900 --> 00:07:09,070
talks through the lessons they learned

00:07:06,610 --> 00:07:10,930
at Flickr figuring out how to how to

00:07:09,070 --> 00:07:15,280
scale these things up how to how to

00:07:10,930 --> 00:07:16,480
build a scalable web application and the

00:07:15,280 --> 00:07:19,030
technique that they used at Flickr that

00:07:16,480 --> 00:07:22,120
got them out of their hole was was

00:07:19,030 --> 00:07:23,590
database sharding so this is a very

00:07:22,120 --> 00:07:25,420
common technique and to this day

00:07:23,590 --> 00:07:28,120
essentially what you do is you say okay

00:07:25,420 --> 00:07:29,410
we had one my sequel database and we

00:07:28,120 --> 00:07:31,510
couldn't keep up there were just too

00:07:29,410 --> 00:07:33,460
many writes coming into this database so

00:07:31,510 --> 00:07:35,200
what we'll do is we'll split it into

00:07:33,460 --> 00:07:37,180
multiple databases and we'll put

00:07:35,200 --> 00:07:39,670
different users on different shards so

00:07:37,180 --> 00:07:42,010
maybe we'll put users 1 through 10,000

00:07:39,670 --> 00:07:44,320
on this database 10,000 20,000 on this

00:07:42,010 --> 00:07:46,900
demo database and so on and so forth and

00:07:44,320 --> 00:07:48,310
this is a very naive description of

00:07:46,900 --> 00:07:51,520
shouting but I hope it illustrates the

00:07:48,310 --> 00:07:53,560
the concept so if you do this life

00:07:51,520 --> 00:07:55,090
becomes a lot easier because you can as

00:07:53,560 --> 00:07:57,310
your user base grows you just add more

00:07:55,090 --> 00:07:59,350
hardware you add more databases and if

00:07:57,310 --> 00:08:01,060
you want to do things like show me the

00:07:59,350 --> 00:08:03,040
most recent photographs uploaded by

00:08:01,060 --> 00:08:04,780
Simon you say okay well Simon's on shard

00:08:03,040 --> 00:08:07,060
three so I'll go to South Korea and I'll

00:08:04,780 --> 00:08:09,100
select photos from there ordered by

00:08:07,060 --> 00:08:12,460
whatever and that that'll give me an

00:08:09,100 --> 00:08:15,340
answer to my question so that sounds

00:08:12,460 --> 00:08:17,200
well it sounds relatively

00:08:15,340 --> 00:08:19,750
straightforward but there's one massive

00:08:17,200 --> 00:08:21,490
problem which is what you do with data

00:08:19,750 --> 00:08:23,560
that happened that lives across multiple

00:08:21,490 --> 00:08:26,410
different shards a great example at

00:08:23,560 --> 00:08:28,360
Flickr Flickr Flickr were very early

00:08:26,410 --> 00:08:30,880
adopters of the idea of sort of user

00:08:28,360 --> 00:08:32,620
provided tags and so you can go to

00:08:30,880 --> 00:08:34,090
Flickr today and you can see all of the

00:08:32,620 --> 00:08:36,430
photos they've been tagged racket tags

00:08:34,090 --> 00:08:38,229
with the raccoons tag and see those see

00:08:36,430 --> 00:08:38,800
the most recent uploads and all of that

00:08:38,229 --> 00:08:40,720
kind of stuff

00:08:38,800 --> 00:08:43,120
and the obvious problem here is if

00:08:40,720 --> 00:08:44,890
you've got photos across five or six or

00:08:43,120 --> 00:08:46,930
a dozen or a hundred different shadow

00:08:44,890 --> 00:08:48,790
databases and you need to find all of

00:08:46,930 --> 00:08:50,470
the raccoon pho all of the photos tagged

00:08:48,790 --> 00:08:52,360
raccoons are you going to do a query

00:08:50,470 --> 00:08:53,650
that hits a hundred databases at once

00:08:52,360 --> 00:08:55,720
and then try and combine the results as

00:08:53,650 --> 00:08:57,840
they come back that's not really a sort

00:08:55,720 --> 00:09:00,700
of practical way of solving this problem

00:08:57,840 --> 00:09:02,260
so what the Flickr team did is they took

00:09:00,700 --> 00:09:04,600
advantage of the fact that they were now

00:09:02,260 --> 00:09:05,620
within within Yahoo and they leaned on a

00:09:04,600 --> 00:09:08,290
piece of Yahoo and

00:09:05,620 --> 00:09:10,450
technology called Vespa which was pretty

00:09:08,290 --> 00:09:12,400
much what elasticsearch is today but ten

00:09:10,450 --> 00:09:16,270
years ago and written in C++ and kind of

00:09:12,400 --> 00:09:17,890
gnarly to work with and so what they did

00:09:16,270 --> 00:09:19,720
is they said okay we're going to have

00:09:17,890 --> 00:09:21,279
our shoulder database will have

00:09:19,720 --> 00:09:22,990
different users photos will different

00:09:21,279 --> 00:09:25,870
live in different places that that's all

00:09:22,990 --> 00:09:27,910
fine and then we'll have a search index

00:09:25,870 --> 00:09:30,490
which we load all of the photos from all

00:09:27,910 --> 00:09:32,320
of the shards into and this was on Vespa

00:09:30,490 --> 00:09:34,210
which could scale horizontally and gave

00:09:32,320 --> 00:09:37,120
them all of those benefits and then we

00:09:34,210 --> 00:09:38,890
can when somebody makes a query against

00:09:37,120 --> 00:09:41,320
Flickr we can make a decision we can say

00:09:38,890 --> 00:09:43,060
if it's you and you're looking at your

00:09:41,320 --> 00:09:45,130
own photos that's going to be a database

00:09:43,060 --> 00:09:46,330
query if it's you looking at other

00:09:45,130 --> 00:09:47,980
people's photos or if it's you're

00:09:46,330 --> 00:09:49,660
looking at every public photo tagged

00:09:47,980 --> 00:09:53,050
raccoons will turn that into a search

00:09:49,660 --> 00:09:55,120
query instead and this is a diagram from

00:09:53,050 --> 00:09:56,470
Aaron's trap cope one of the engineers

00:09:55,120 --> 00:09:59,140
at Flickr who worked on this at the time

00:09:56,470 --> 00:10:01,060
and this turns out to work really really

00:09:59,140 --> 00:10:03,370
well there's a very there's a key

00:10:01,060 --> 00:10:05,529
concept embedded in here which is the

00:10:03,370 --> 00:10:06,970
this idea of smart query routing because

00:10:05,529 --> 00:10:08,680
if you're building software that human

00:10:06,970 --> 00:10:10,570
beings use one of the worst things you

00:10:08,680 --> 00:10:13,000
can do is have a bug where somebody

00:10:10,570 --> 00:10:15,760
makes an edit and then they refresh the

00:10:13,000 --> 00:10:17,709
page or they go to the UI refreshes and

00:10:15,760 --> 00:10:20,200
then edit hasn't isn't shown back to

00:10:17,709 --> 00:10:22,120
them if that if you use a tags of photo

00:10:20,200 --> 00:10:23,470
raccoons and then goes and looks at

00:10:22,120 --> 00:10:25,000
their photos tank raccoons and it's not

00:10:23,470 --> 00:10:28,330
in there this is a bug and they're

00:10:25,000 --> 00:10:31,000
they're justifiably annoyed by it so the

00:10:28,330 --> 00:10:32,950
the solution Flickr the solution Flickr

00:10:31,000 --> 00:10:34,630
used was to say if you're looking at

00:10:32,950 --> 00:10:36,370
your own data that should be a

00:10:34,630 --> 00:10:38,290
relational database hit because we can't

00:10:36,370 --> 00:10:40,120
guarantee that the search index has got

00:10:38,290 --> 00:10:42,310
those changes yet generally with these

00:10:40,120 --> 00:10:43,930
systems it can take a few seconds up to

00:10:42,310 --> 00:10:46,839
a few minutes for the underlying search

00:10:43,930 --> 00:10:48,190
index to reflect those changes if you're

00:10:46,839 --> 00:10:49,779
looking in other people's data you're

00:10:48,190 --> 00:10:51,790
just not going you're never going to

00:10:49,779 --> 00:10:54,250
know if your friend just uploaded photo

00:10:51,790 --> 00:10:55,630
tag raccoons like five seconds ago and

00:10:54,250 --> 00:10:57,010
you can't see it yet that's not

00:10:55,630 --> 00:10:58,990
something you'll be able to observe so

00:10:57,010 --> 00:11:01,750
at that point it's safe for us to use

00:10:58,990 --> 00:11:03,370
such use the search index for public and

00:11:01,750 --> 00:11:08,940
other people's better and the relational

00:11:03,370 --> 00:11:12,209
database for our own data so the next

00:11:08,940 --> 00:11:16,540
we'll fast forward a few years to 2010

00:11:12,209 --> 00:11:18,220
when we used as we used solar another

00:11:16,540 --> 00:11:19,170
open source search engine to solve a

00:11:18,220 --> 00:11:22,740
similar kind of problem

00:11:19,170 --> 00:11:23,820
lanyard so we launched lanyards and my

00:11:22,740 --> 00:11:25,950
wife and I launched landed on our

00:11:23,820 --> 00:11:28,260
honeymoon it was supposed to be a side

00:11:25,950 --> 00:11:31,560
project and it ended up growing way way

00:11:28,260 --> 00:11:34,709
beyond that but the the the idea was we

00:11:31,560 --> 00:11:37,199
were we were in Casablanca in Morocco

00:11:34,709 --> 00:11:38,810
and we had food poisoning and were

00:11:37,199 --> 00:11:41,220
unable to keep on traveling and

00:11:38,810 --> 00:11:42,660
Casablanca it was during Ramadan when

00:11:41,220 --> 00:11:43,829
none of the restaurants were open so we

00:11:42,660 --> 00:11:45,329
couldn't get anything to eat anywhere

00:11:43,829 --> 00:11:46,740
else either so we figured okay we'll

00:11:45,329 --> 00:11:48,209
rent an apartment for two weeks we'll

00:11:46,740 --> 00:11:50,010
look after ourselves we'll we'll cook

00:11:48,209 --> 00:11:51,540
ourselves better and we'll try and ship

00:11:50,010 --> 00:11:54,269
this side project that we'd been working

00:11:51,540 --> 00:11:56,010
on and we made a mistake of building a

00:11:54,269 --> 00:11:58,050
side project with with user account

00:11:56,010 --> 00:12:00,660
logins which you should never do because

00:11:58,050 --> 00:12:02,339
users have expectations and we also

00:12:00,660 --> 00:12:03,899
built it on top of Twitter and so the

00:12:02,339 --> 00:12:05,399
core feature of lanyard when we launched

00:12:03,899 --> 00:12:07,889
was you sign in with your Twitter

00:12:05,399 --> 00:12:09,240
account and we show you conferences that

00:12:07,889 --> 00:12:10,740
your Twitter friends the people you

00:12:09,240 --> 00:12:14,880
follow on Twitter are speaking at or

00:12:10,740 --> 00:12:16,920
attending which when you think about

00:12:14,880 --> 00:12:18,839
that in terms of a database query ends

00:12:16,920 --> 00:12:20,760
up being a sequel query where you say

00:12:18,839 --> 00:12:22,620
select star from events where it's in

00:12:20,760 --> 00:12:24,660
the future and at least one of the

00:12:22,620 --> 00:12:26,040
attendees is in this list of a thousand

00:12:24,660 --> 00:12:27,329
IDs that I've pulled back from Twitter

00:12:26,040 --> 00:12:29,550
this is the kind of thing that

00:12:27,329 --> 00:12:32,760
relational databases are incredibly bad

00:12:29,550 --> 00:12:34,740
at and so we we launched we got a flurry

00:12:32,760 --> 00:12:37,199
of initial activity we ended up on

00:12:34,740 --> 00:12:39,420
TechCrunch UK unexpectedly and the site

00:12:37,199 --> 00:12:41,339
just died instantly because the most

00:12:39,420 --> 00:12:43,019
popular page on our site was the page

00:12:41,339 --> 00:12:46,380
with the most expensive database query

00:12:43,019 --> 00:12:48,060
and so the way we resolved this was we'd

00:12:46,380 --> 00:12:49,890
started using solar to provide search

00:12:48,060 --> 00:12:51,300
and we thought well maybe this is

00:12:49,890 --> 00:12:54,810
something we can redefine as a search

00:12:51,300 --> 00:12:57,570
problem so we turned this feature from a

00:12:54,810 --> 00:12:59,490
giant hairy sequel query into a solar

00:12:57,570 --> 00:13:01,440
search where we said hey solar I want

00:12:59,490 --> 00:13:04,920
events that are in the future where the

00:13:01,440 --> 00:13:08,310
attendee IDs field in solar matches

00:13:04,920 --> 00:13:09,740
anyone of this list of up to 2000 IDs

00:13:08,310 --> 00:13:12,180
that I've pulled back from Twitter and

00:13:09,740 --> 00:13:13,769
do that and order by date

00:13:12,180 --> 00:13:15,810
we built this thinking this will

00:13:13,769 --> 00:13:17,100
probably do ass for like a few months

00:13:15,810 --> 00:13:19,050
until we can figure out a better

00:13:17,100 --> 00:13:20,490
solution and now five years later that's

00:13:19,050 --> 00:13:22,380
still how this page works because it

00:13:20,490 --> 00:13:25,040
turns out search engines are incredibly

00:13:22,380 --> 00:13:27,329
good at exactly that kind of query and

00:13:25,040 --> 00:13:29,220
normally a search engine expects to be

00:13:27,329 --> 00:13:30,990
dealing with words because users type

00:13:29,220 --> 00:13:32,590
words in but actually things like user

00:13:30,990 --> 00:13:34,870
IDs or other terms

00:13:32,590 --> 00:13:36,490
work exactly as well and the underlying

00:13:34,870 --> 00:13:40,330
architecture of the search engine is

00:13:36,490 --> 00:13:42,580
really good at dividing those up at

00:13:40,330 --> 00:13:44,950
merging together all of the documents in

00:13:42,580 --> 00:13:47,250
the index that have fields that match

00:13:44,950 --> 00:13:50,980
whatever criteria does you're passing in

00:13:47,250 --> 00:13:52,510
so this is a good time to switch over to

00:13:50,980 --> 00:13:56,320
talking a little bit about elastic

00:13:52,510 --> 00:13:58,360
search as I mentioned lanyard so Flickr

00:13:56,320 --> 00:13:59,740
used Vesper which I looked this morning

00:13:58,360 --> 00:14:01,720
and it sounds like Yahoo have actually

00:13:59,740 --> 00:14:03,820
open sourced it which is kind of kind of

00:14:01,720 --> 00:14:05,530
interesting but um I don't know if it's

00:14:03,820 --> 00:14:08,230
it's gained an enormous amount of

00:14:05,530 --> 00:14:10,210
community adoption yet we used solar at

00:14:08,230 --> 00:14:13,510
lanyard which is a very fine search

00:14:10,210 --> 00:14:16,240
engine albeit one which was clearly a it

00:14:13,510 --> 00:14:17,650
was clearly a it's a sort of product at

00:14:16,240 --> 00:14:20,380
the time in which it was designed it's

00:14:17,650 --> 00:14:22,780
very vote it's very XML heavy they sort

00:14:20,380 --> 00:14:25,330
of added JSON as a as a as a as a later

00:14:22,780 --> 00:14:27,340
later detail elasticsearch is what you

00:14:25,330 --> 00:14:29,560
would get if you design solar today if

00:14:27,340 --> 00:14:32,200
you said ok clearly the world speaks

00:14:29,560 --> 00:14:33,100
JSON and the world speaks HTTP and we're

00:14:32,200 --> 00:14:34,720
going to want things to scale

00:14:33,100 --> 00:14:36,010
horizontally let's combine all those

00:14:34,720 --> 00:14:39,460
things together and build a really good

00:14:36,010 --> 00:14:40,900
search engine and so it's an open source

00:14:39,460 --> 00:14:43,840
search engine it's built on top of the

00:14:40,900 --> 00:14:45,130
same Lucene search library that solar

00:14:43,840 --> 00:14:48,160
and other projects have used in the past

00:14:45,130 --> 00:14:49,780
the interface is entirely JSON over HTTP

00:14:48,160 --> 00:14:51,400
which is great because it means you can

00:14:49,780 --> 00:14:53,380
talk to it from any programming language

00:14:51,400 --> 00:14:54,790
that speaks those two things which I'm

00:14:53,380 --> 00:14:57,130
pretty sure is everything these days

00:14:54,790 --> 00:14:59,560
it's very and it makes it very easy to

00:14:57,130 --> 00:15:01,540
use from for you can use it as a sort of

00:14:59,560 --> 00:15:02,890
central point between different

00:15:01,540 --> 00:15:06,550
programming languages very easily as

00:15:02,890 --> 00:15:08,530
well the marketing bump all claims to be

00:15:06,550 --> 00:15:10,120
a real-time search engine in practice

00:15:08,530 --> 00:15:11,890
it's close enough we're talking a few

00:15:10,120 --> 00:15:13,360
seconds between you submitting a

00:15:11,890 --> 00:15:15,190
document into elasticsearch and that

00:15:13,360 --> 00:15:16,660
document becoming available across the

00:15:15,190 --> 00:15:19,210
cluster for you to run queries against

00:15:16,660 --> 00:15:20,680
it has an insanely powerful query

00:15:19,210 --> 00:15:23,170
language I'll show you a little bit of

00:15:20,680 --> 00:15:25,150
that as we go along but essentially

00:15:23,170 --> 00:15:27,040
there's a domain-specific language

00:15:25,150 --> 00:15:29,110
written in JSON that lets you construct

00:15:27,040 --> 00:15:31,570
extremely powerful and complicated

00:15:29,110 --> 00:15:33,850
search queries and it also has a very

00:15:31,570 --> 00:15:35,500
strong focus on analytics if you go to

00:15:33,850 --> 00:15:37,320
the elasticsearch website you'll see

00:15:35,500 --> 00:15:40,000
that they mainly talk about it as a

00:15:37,320 --> 00:15:41,890
analytics engine for things like log

00:15:40,000 --> 00:15:43,000
like log analysis and so forth and

00:15:41,890 --> 00:15:44,040
that's where a lot of their focus has

00:15:43,000 --> 00:15:46,300
been

00:15:44,040 --> 00:15:47,890
as I said earlier this is the thing that

00:15:46,300 --> 00:15:49,930
it sights me about search engines is

00:15:47,890 --> 00:15:51,040
sure full text searches nice but being

00:15:49,930 --> 00:15:52,630
able to do these more complicated

00:15:51,040 --> 00:15:54,610
analytical queries is where they get

00:15:52,630 --> 00:15:57,490
really interesting and then finally the

00:15:54,610 --> 00:15:59,290
elastic really does mean elastic elastic

00:15:57,490 --> 00:16:01,510
search scales horizontally I've actually

00:15:59,290 --> 00:16:03,850
in the past I've run a cluster of four

00:16:01,510 --> 00:16:05,170
nodes and just killed one of them at

00:16:03,850 --> 00:16:07,300
random to see what would happen and you

00:16:05,170 --> 00:16:09,279
can watch the documents rebalancing

00:16:07,300 --> 00:16:11,200
across the remaining lines in real time

00:16:09,279 --> 00:16:13,330
using various some visualization plugins

00:16:11,200 --> 00:16:17,470
so it's it does live up to the to the E

00:16:13,330 --> 00:16:19,420
in its name so I've talked a little bit

00:16:17,470 --> 00:16:21,670
about how I'm excited about more than

00:16:19,420 --> 00:16:23,430
just search the feature that I'm put

00:16:21,670 --> 00:16:25,750
specifically excited about is

00:16:23,430 --> 00:16:29,020
aggregations and the best way to

00:16:25,750 --> 00:16:31,440
illustrate those is with an example so

00:16:29,020 --> 00:16:33,149
this is a project that I built last year

00:16:31,440 --> 00:16:34,959
it's a little side project

00:16:33,149 --> 00:16:36,010
embarrassingly since I'm speaking at

00:16:34,959 --> 00:16:37,810
django con this is actually the only

00:16:36,010 --> 00:16:40,420
thing I've ever written in flask because

00:16:37,810 --> 00:16:41,860
I decided to try out and see what flask

00:16:40,420 --> 00:16:44,620
look like flask worked very well it was

00:16:41,860 --> 00:16:47,890
a very nice way of building this and so

00:16:44,620 --> 00:16:51,540
what this is is it's called DC in box

00:16:47,890 --> 00:16:55,600
Explorer and there is a project at

00:16:51,540 --> 00:16:58,630
Stephens University is it Stevens which

00:16:55,600 --> 00:17:01,209
called DC in box which collects the

00:16:58,630 --> 00:17:04,179
emails that senators and congresspeople

00:17:01,209 --> 00:17:06,309
send out to their own constituents so

00:17:04,179 --> 00:17:07,780
there's this was searches subscribed to

00:17:06,309 --> 00:17:09,760
all of these different mailing lists and

00:17:07,780 --> 00:17:13,569
gathers all of those emails and puts

00:17:09,760 --> 00:17:15,610
them in a giant giant a giant JSON file

00:17:13,569 --> 00:17:17,260
that you can then use to run research

00:17:15,610 --> 00:17:19,449
about who's emailing about what and when

00:17:17,260 --> 00:17:21,400
and because it was a giant JSON file it

00:17:19,449 --> 00:17:23,439
was very easy for me to take this and

00:17:21,400 --> 00:17:24,640
import it into elasticsearch the source

00:17:23,439 --> 00:17:26,020
code for this is all available and

00:17:24,640 --> 00:17:27,309
available and get herb and there's not

00:17:26,020 --> 00:17:29,770
very much of it so if you want to see

00:17:27,309 --> 00:17:31,600
how this work how this works and and get

00:17:29,770 --> 00:17:33,309
an example of a Vlasic search this is a

00:17:31,600 --> 00:17:36,510
pretty good starting point so basically

00:17:33,309 --> 00:17:38,500
what this does is it shows you all

00:17:36,510 --> 00:17:40,750
57,000 emails that have been collected

00:17:38,500 --> 00:17:43,270
by this project and it lets you search

00:17:40,750 --> 00:17:46,350
so I can search for say security and

00:17:43,270 --> 00:17:48,580
then I get back 15,000 results it shows

00:17:46,350 --> 00:17:50,140
the number of emails sent by month the

00:17:48,580 --> 00:17:53,140
top and then down the side this is the

00:17:50,140 --> 00:17:55,720
this is my sort of pet favorite feature

00:17:53,140 --> 00:17:57,700
of any piece of search software it has

00:17:55,720 --> 00:18:00,070
these things which sometimes called

00:17:57,700 --> 00:18:01,750
assets sometimes called filters so what

00:18:00,070 --> 00:18:05,680
this is saying is that without a search

00:18:01,750 --> 00:18:07,540
term there are 56 thousand emails 36

00:18:05,680 --> 00:18:09,850
7,000 of those were sent by Republicans

00:18:07,540 --> 00:18:11,800
19 thousand was sent by Democrats 280

00:18:09,850 --> 00:18:13,540
was sent by independence you can see

00:18:11,800 --> 00:18:15,790
them broken down by by representative

00:18:13,540 --> 00:18:18,910
versus senators by the states that the

00:18:15,790 --> 00:18:21,280
that politician represents and if I then

00:18:18,910 --> 00:18:23,080
search for say security those numbers

00:18:21,280 --> 00:18:27,370
updates and I can see that the emails

00:18:23,080 --> 00:18:29,170
that mention the term security 2,500 of

00:18:27,370 --> 00:18:31,240
those were sent by Senator if I click on

00:18:29,170 --> 00:18:33,790
that I'm now seeing emails sent by a

00:18:31,240 --> 00:18:36,910
senator that mentions security and I can

00:18:33,790 --> 00:18:38,860
see that of those 810 Democrats 1600

00:18:36,910 --> 00:18:41,260
Republicans I can now see that the

00:18:38,860 --> 00:18:42,760
states that is most concerned about the

00:18:41,260 --> 00:18:44,800
state whose sentences care that most of

00:18:42,760 --> 00:18:46,150
our securities are practically Emme is

00:18:44,800 --> 00:18:49,240
that Maine

00:18:46,150 --> 00:18:51,070
and I get this and and so I can keep on

00:18:49,240 --> 00:18:53,950
drilling down say okay sir emails

00:18:51,070 --> 00:18:55,780
mentioning security from Maine by I've

00:18:53,950 --> 00:18:58,150
got gender in there I can look at by

00:18:55,780 --> 00:18:59,860
male or female senators I can see the

00:18:58,150 --> 00:19:02,830
actual sense themselves so Susan Collins

00:18:59,860 --> 00:19:04,420
is the most prolific emailer from the

00:19:02,830 --> 00:19:07,540
state of Maine on the subject of

00:19:04,420 --> 00:19:09,490
security but the key thing to a list I'm

00:19:07,540 --> 00:19:11,590
I'm illustrating here is that when

00:19:09,490 --> 00:19:13,690
you've got a search engine like elastic

00:19:11,590 --> 00:19:17,050
search these kinds of calculations these

00:19:13,690 --> 00:19:19,030
aggregate counts become essentially free

00:19:17,050 --> 00:19:20,080
for you to that their vote that they

00:19:19,030 --> 00:19:22,300
become very easy for an implementation

00:19:20,080 --> 00:19:24,670
point of view the performance is super

00:19:22,300 --> 00:19:26,500
is as super fast so you can build this

00:19:24,670 --> 00:19:28,360
kind of highly interactive interface

00:19:26,500 --> 00:19:30,040
very easily on top of that underlying

00:19:28,360 --> 00:19:32,260
engine and if you go on sites like

00:19:30,040 --> 00:19:34,180
Amazon and booking.com and so forth they

00:19:32,260 --> 00:19:37,870
all make very extensive use of this

00:19:34,180 --> 00:19:38,830
faceted navigation pattern if you try to

00:19:37,870 --> 00:19:40,780
do this with a relational database

00:19:38,830 --> 00:19:42,040
you're likely to run into some pretty

00:19:40,780 --> 00:19:45,700
nasty performance problems pretty

00:19:42,040 --> 00:19:47,740
quickly what I can also show you is

00:19:45,700 --> 00:19:49,570
under the hood I'll show you a little

00:19:47,740 --> 00:19:51,160
bit of what elasticsearch itself looks

00:19:49,570 --> 00:19:53,560
like to work with this is a tool called

00:19:51,160 --> 00:19:56,110
sense which is kind of an IDE for

00:19:53,560 --> 00:19:57,820
talking to elasticsearch and as I

00:19:56,110 --> 00:20:00,820
mentioned elasticsearch is all JSON and

00:19:57,820 --> 00:20:03,250
and HTTP so I'm going to do a get

00:20:00,820 --> 00:20:05,100
against the / email slash underscore

00:20:03,250 --> 00:20:07,030
search endpoint and this returns

00:20:05,100 --> 00:20:08,590
essentially all of the emails it's

00:20:07,030 --> 00:20:11,230
paginate it I think it returns 10 to a

00:20:08,590 --> 00:20:15,460
page and you can see at the very top it

00:20:11,230 --> 00:20:17,350
says that there are 58 that 56,000 of

00:20:15,460 --> 00:20:18,820
them this is what an email looks like

00:20:17,350 --> 00:20:21,130
it's got all of this data that I

00:20:18,820 --> 00:20:22,960
ingested when I indexed the documents

00:20:21,130 --> 00:20:25,450
and so then I can say actually you know

00:20:22,960 --> 00:20:26,980
I want to run search so this is

00:20:25,450 --> 00:20:28,930
illustrating the elasticsearch

00:20:26,980 --> 00:20:30,970
domain-specific language I'm saying

00:20:28,930 --> 00:20:34,000
don't you get against the email slash

00:20:30,970 --> 00:20:35,890
search I want the I'm it's a query and I

00:20:34,000 --> 00:20:38,980
want to match the term security in the

00:20:35,890 --> 00:20:41,170
body field and I'll run that and now it

00:20:38,980 --> 00:20:42,730
gives me back 15,000 emails and those

00:20:41,170 --> 00:20:44,920
are the ones that match this particular

00:20:42,730 --> 00:20:47,980
query there's a slight oddity of

00:20:44,920 --> 00:20:50,650
elasticsearch this is an HTTP GET which

00:20:47,980 --> 00:20:52,240
includes a body as if it was an HTTP

00:20:50,650 --> 00:20:53,890
POST I had no idea this was even

00:20:52,240 --> 00:20:56,380
possible but apparently it is and

00:20:53,890 --> 00:20:58,240
elasticsearch uses it for everything so

00:20:56,380 --> 00:20:59,920
I'm so there's something that I learnt

00:20:58,240 --> 00:21:01,900
in playing around with this and but

00:20:59,920 --> 00:21:03,100
let's go a step further and say ok we're

00:21:01,900 --> 00:21:05,740
going to search for all of the emails

00:21:03,100 --> 00:21:08,230
matching security but I want to also get

00:21:05,740 --> 00:21:11,290
numbers are broken down by role type

00:21:08,230 --> 00:21:13,930
which is Senator versus representative

00:21:11,290 --> 00:21:16,390
and by party and if I run this search

00:21:13,930 --> 00:21:17,890
here I get back all of these different

00:21:16,390 --> 00:21:19,840
search results and then at the bottom I

00:21:17,890 --> 00:21:22,180
get this aggregations block where it

00:21:19,840 --> 00:21:23,800
says roll type representative has 12 and

00:21:22,180 --> 00:21:26,230
a half thousand senator has two and a

00:21:23,800 --> 00:21:29,170
half thousand Republicans 10,000

00:21:26,230 --> 00:21:30,250
Democrat 5,000 independence 123 these

00:21:29,170 --> 00:21:31,750
are the numbers that you saw in the

00:21:30,250 --> 00:21:34,330
interface earlier but this illustrates

00:21:31,750 --> 00:21:36,520
how it's just a little bit of extra JSON

00:21:34,330 --> 00:21:40,270
that you answer your query and the query

00:21:36,520 --> 00:21:43,720
time for this was 4 milliseconds which i

00:21:40,270 --> 00:21:45,310
think is pretty good you know I'll show

00:21:43,720 --> 00:21:47,170
one last example just to illustrate

00:21:45,310 --> 00:21:48,970
something that I think is unique to

00:21:47,170 --> 00:21:50,440
elastic search which is that elastic

00:21:48,970 --> 00:21:52,360
search lets you take these aggregations

00:21:50,440 --> 00:21:55,390
and nest them so here what I'm doing is

00:21:52,360 --> 00:21:58,120
I'm saying I want to aggregate counts by

00:21:55,390 --> 00:22:00,370
the party and then within that party I'd

00:21:58,120 --> 00:22:02,620
like to do counts by the roll type so if

00:22:00,370 --> 00:22:04,780
I run this I get back results where you

00:22:02,620 --> 00:22:07,690
can see that the Republicans have sent

00:22:04,780 --> 00:22:09,220
37,000 emails of those 31,000 was sent

00:22:07,690 --> 00:22:11,830
by representatives 5,000 sent by

00:22:09,220 --> 00:22:13,720
Senators the Democrats 19,000 emails of

00:22:11,830 --> 00:22:15,100
which 15,000 representatives and for

00:22:13,720 --> 00:22:17,290
thousands of Senators and then the

00:22:15,100 --> 00:22:19,690
independence at the bottom is apparently

00:22:17,290 --> 00:22:21,370
one the email sent by an independent

00:22:19,690 --> 00:22:23,919
representative and the rest were all

00:22:21,370 --> 00:22:27,600
sent by Senators I'm intrigued let's

00:22:23,919 --> 00:22:31,470
have a look so if I do independent here

00:22:27,600 --> 00:22:35,919
and then representatives sure enough

00:22:31,470 --> 00:22:39,309
representative Gregorio Sablin has sent

00:22:35,919 --> 00:22:41,409
a single email apologizing for spam

00:22:39,309 --> 00:22:44,980
which considering he's only sent one

00:22:41,409 --> 00:22:53,679
email it's a little bit surprising so

00:22:44,980 --> 00:22:55,330
there we go so that's sort of some of

00:22:53,679 --> 00:22:58,539
the power that you get once you start

00:22:55,330 --> 00:22:59,440
adding an engine like elasticsearch into

00:22:58,539 --> 00:23:02,110
your stack

00:22:59,440 --> 00:23:04,450
I said either earlier I talked about the

00:23:02,110 --> 00:23:06,039
difficult problem which is the the

00:23:04,450 --> 00:23:08,320
synchronization strategy between that

00:23:06,039 --> 00:23:09,789
your relational database and I've tried

00:23:08,320 --> 00:23:11,649
a whole bunch of different ways of doing

00:23:09,789 --> 00:23:13,720
this the three that I've had the most

00:23:11,649 --> 00:23:14,710
luck with these three and so I'll talk

00:23:13,720 --> 00:23:19,899
through these in a little bit more

00:23:14,710 --> 00:23:21,039
detail but to to sort of to to repeat

00:23:19,899 --> 00:23:22,659
the problem we're trying to solve here

00:23:21,039 --> 00:23:24,190
is you've got users who are making

00:23:22,659 --> 00:23:25,809
changes in your relational database

00:23:24,190 --> 00:23:27,549
they're updating things adding things

00:23:25,809 --> 00:23:28,929
you want those to be reflected in your

00:23:27,549 --> 00:23:32,049
search index as quickly as possible

00:23:28,929 --> 00:23:33,429
because any delay could result in like a

00:23:32,049 --> 00:23:34,720
strange behavior that the users don't

00:23:33,429 --> 00:23:36,509
understand

00:23:34,720 --> 00:23:38,679
and you want to do this in a way that is

00:23:36,509 --> 00:23:40,570
performant and efficient and doesn't

00:23:38,679 --> 00:23:43,090
cause too much overhead on the on the

00:23:40,570 --> 00:23:45,909
various parts of your stack so the

00:23:43,090 --> 00:23:47,769
simplest way to do this is to basically

00:23:45,909 --> 00:23:50,950
keep it in the database and it's to have

00:23:47,769 --> 00:23:53,769
a last touched or a changed timestamp in

00:23:50,950 --> 00:23:55,750
the in the L on the actual rows of your

00:23:53,769 --> 00:23:58,059
database which gets updated anytime

00:23:55,750 --> 00:24:00,610
somebody changes that row this is a very

00:23:58,059 --> 00:24:02,679
common pattern here's what it might look

00:24:00,610 --> 00:24:04,929
like in the Django ORM I've got the last

00:24:02,679 --> 00:24:06,700
touched column it's a date/time field DB

00:24:04,929 --> 00:24:07,899
and the score index equals true it's

00:24:06,700 --> 00:24:08,950
important to take an index on this

00:24:07,899 --> 00:24:11,110
because you're going to be pulling this

00:24:08,950 --> 00:24:12,429
from a cron job like once a minute so it

00:24:11,110 --> 00:24:16,049
needs to be able to return results

00:24:12,429 --> 00:24:18,669
quickly and you set it to default to now

00:24:16,049 --> 00:24:20,320
and that's fine and then if once you've

00:24:18,669 --> 00:24:21,700
got to set up the simplest thing to do

00:24:20,320 --> 00:24:25,240
is just have a cron that runs once a

00:24:21,700 --> 00:24:27,549
minute select star from from a table

00:24:25,240 --> 00:24:29,110
where the where change date is within

00:24:27,549 --> 00:24:31,899
the last minute and then we indexes

00:24:29,110 --> 00:24:33,130
those items the nice thing about having

00:24:31,899 --> 00:24:35,350
this is a time step

00:24:33,130 --> 00:24:38,260
is that an indexer can keep track of the

00:24:35,350 --> 00:24:40,090
last last time that it polled the last

00:24:38,260 --> 00:24:41,770
thing it saw so if your index it doesn't

00:24:40,090 --> 00:24:43,420
run for five minutes when it runs again

00:24:41,770 --> 00:24:46,180
it can catch up on five minutes worth of

00:24:43,420 --> 00:24:48,490
changes all at once there is a subtlety

00:24:46,180 --> 00:24:50,110
to this which is that quite often when

00:24:48,490 --> 00:24:51,730
you're building a search index from a

00:24:50,110 --> 00:24:54,310
relational database there are changes

00:24:51,730 --> 00:24:56,410
that happen to other tables which still

00:24:54,310 --> 00:24:58,570
should trigger an update so on lanyards

00:24:56,410 --> 00:25:01,270
we have a concept called a guide where a

00:24:58,570 --> 00:25:02,860
guide is a somebody might create my

00:25:01,270 --> 00:25:04,720
guides to JavaScript conferences in

00:25:02,860 --> 00:25:06,550
Europe and they'll then add conference

00:25:04,720 --> 00:25:08,500
it they'll add events into that guide

00:25:06,550 --> 00:25:10,690
the problem at that and we try and

00:25:08,500 --> 00:25:12,640
include the name of that guide in

00:25:10,690 --> 00:25:14,410
searches so if you search for JavaScript

00:25:12,640 --> 00:25:15,850
Europe as long as an event has been

00:25:14,410 --> 00:25:17,830
added to the JavaScript in Europe guide

00:25:15,850 --> 00:25:20,770
it should show up what that means though

00:25:17,830 --> 00:25:22,480
is anytime somebody changes the guide we

00:25:20,770 --> 00:25:24,760
need to reinvent all of the events that

00:25:22,480 --> 00:25:26,200
link to that guide because there's a bit

00:25:24,760 --> 00:25:28,360
of dependent data that has now been

00:25:26,200 --> 00:25:29,980
updated and if you're using the last

00:25:28,360 --> 00:25:32,350
touch mechanism that's pretty easy you

00:25:29,980 --> 00:25:34,510
can say any time a guide is edited to

00:25:32,350 --> 00:25:35,350
guide doc conferences that all find all

00:25:34,510 --> 00:25:37,570
the conference's attached

00:25:35,350 --> 00:25:40,780
and update their last touch to date to

00:25:37,570 --> 00:25:42,520
the current timestamp as well for the

00:25:40,780 --> 00:25:43,870
most part that works fantastically well

00:25:42,520 --> 00:25:46,930
and this means that you get these sort

00:25:43,870 --> 00:25:48,430
of cascading cascading changes happening

00:25:46,930 --> 00:25:50,620
within your database which your search

00:25:48,430 --> 00:25:53,170
index can then then track and catch up

00:25:50,620 --> 00:25:57,370
on so a slightly more sophisticated way

00:25:53,170 --> 00:25:59,320
of doing this is with a queue as a jet

00:25:57,370 --> 00:26:02,050
you can have application logic that says

00:25:59,320 --> 00:26:03,820
anytime somebody updates an event write

00:26:02,050 --> 00:26:06,040
that of or updates a document write that

00:26:03,820 --> 00:26:07,180
documents ID into a queue and then have

00:26:06,040 --> 00:26:08,740
something at the other end of the queue

00:26:07,180 --> 00:26:11,890
which is consuming from it and reindex

00:26:08,740 --> 00:26:14,530
in those documents a really nice side

00:26:11,890 --> 00:26:16,300
effect of this is that you can have D

00:26:14,530 --> 00:26:18,760
duping although you get D jeeping with

00:26:16,300 --> 00:26:20,410
the the previous mechanism as well but

00:26:18,760 --> 00:26:22,030
you can write your indexes so that it

00:26:20,410 --> 00:26:24,160
says okay there's been a flurry of

00:26:22,030 --> 00:26:26,710
activity around this particular document

00:26:24,160 --> 00:26:28,690
but I'm going to batch those up and a

00:26:26,710 --> 00:26:30,550
few seconds later I'll do one in one we

00:26:28,690 --> 00:26:35,590
index and call to recreate that in

00:26:30,550 --> 00:26:36,970
elasticsearch so I've built this a few

00:26:35,590 --> 00:26:38,920
times I've built this on top of Redis

00:26:36,970 --> 00:26:41,890
which worked great as a little Heroku

00:26:38,920 --> 00:26:43,630
app I've built this on Eventbrite we use

00:26:41,890 --> 00:26:45,520
Kafka for this and in fact we we have a

00:26:43,630 --> 00:26:47,350
slightly more sophisticated system which

00:26:45,520 --> 00:26:49,420
I'll dive into in a moment

00:26:47,350 --> 00:26:51,100
another nice thing about cues is if you

00:26:49,420 --> 00:26:53,530
have a persistent cue you get that

00:26:51,100 --> 00:26:55,240
replayability as well so you can replay

00:26:53,530 --> 00:26:58,630
all of the indexing changes from the

00:26:55,240 --> 00:27:00,910
past five minutes this is the most

00:26:58,630 --> 00:27:02,580
sophisticated way that I've seen itself

00:27:00,910 --> 00:27:04,990
and this is what we do at Eventbrite

00:27:02,580 --> 00:27:07,600
which is tap into the databases

00:27:04,990 --> 00:27:09,700
replication log itself so my sequel has

00:27:07,600 --> 00:27:12,640
a very robust replication it's very easy

00:27:09,700 --> 00:27:14,530
to have a my sequel leader database and

00:27:12,640 --> 00:27:16,510
then select multiple replicas that that

00:27:14,530 --> 00:27:19,990
reapply all of the changes made to that

00:27:16,510 --> 00:27:22,630
leader it turns out the replication

00:27:19,990 --> 00:27:24,370
stream is this sort of slightly weird

00:27:22,630 --> 00:27:25,570
binary protocol but if you know what

00:27:24,370 --> 00:27:26,320
you're doing you can tap into that

00:27:25,570 --> 00:27:28,000
yourself

00:27:26,320 --> 00:27:29,530
and you can write your own code that

00:27:28,000 --> 00:27:31,260
reacts to changes that have been made to

00:27:29,530 --> 00:27:33,340
the database there's a fantastic

00:27:31,260 --> 00:27:34,600
open-source Python library that we use

00:27:33,340 --> 00:27:37,630
for this and event prank-called and

00:27:34,600 --> 00:27:39,070
python my sequel replication so remember

00:27:37,630 --> 00:27:43,090
like we built a system called dilithium

00:27:39,070 --> 00:27:46,330
and dilithium is essentially a way of

00:27:43,090 --> 00:27:48,820
listening to sorry about this it's a way

00:27:46,330 --> 00:27:50,950
of listening to those database changes

00:27:48,820 --> 00:27:53,650
and using them to trigger other actions

00:27:50,950 --> 00:27:55,930
around event the Eventbrite system so it

00:27:53,650 --> 00:27:57,430
works is you have your master my sequel

00:27:55,930 --> 00:27:59,890
database and with all of the rights

00:27:57,430 --> 00:28:01,630
going to it you have a replica my sequel

00:27:59,890 --> 00:28:04,360
database that's that's replicating off

00:28:01,630 --> 00:28:06,610
of that and then dilithium listens to

00:28:04,360 --> 00:28:08,620
that replica so it's replicating from a

00:28:06,610 --> 00:28:11,950
replica to figure out what changes are

00:28:08,620 --> 00:28:15,460
going on it sees things like event row

00:28:11,950 --> 00:28:17,170
57 has been updated attendee row

00:28:15,460 --> 00:28:19,600
so-and-so's had these fields changed and

00:28:17,170 --> 00:28:22,170
it takes the it takes that int that flow

00:28:19,600 --> 00:28:24,940
of data and turns it into more sort of

00:28:22,170 --> 00:28:27,250
more intensive into what we call

00:28:24,940 --> 00:28:28,780
interesting moments because we can't use

00:28:27,250 --> 00:28:30,880
the word events at Eventbrite because

00:28:28,780 --> 00:28:33,400
it's already taken by one of our main

00:28:30,880 --> 00:28:35,650
domain objects so those moments that

00:28:33,400 --> 00:28:38,710
come through we translate to things like

00:28:35,650 --> 00:28:42,070
event 57 has been updated event 23 has

00:28:38,710 --> 00:28:44,800
been created order 37 has been placed

00:28:42,070 --> 00:28:48,220
those we then right into Kafka which is

00:28:44,800 --> 00:28:50,260
a very robust high performance message

00:28:48,220 --> 00:28:52,960
queue that LinkedIn put out a few years

00:28:50,260 --> 00:28:54,940
ago and the our search indexes are one

00:28:52,960 --> 00:28:56,710
of many different components that can

00:28:54,940 --> 00:28:58,420
then listen to that Kafka queue and

00:28:56,710 --> 00:29:00,130
decide what decide when they need to

00:28:58,420 --> 00:29:02,290
reinvest things so it's

00:29:00,130 --> 00:29:04,150
a pretty complicated flow of data once

00:29:02,290 --> 00:29:06,400
you stick it in a diagram but um

00:29:04,150 --> 00:29:08,800
essentially what this means is anytime

00:29:06,400 --> 00:29:10,840
any piece of coded Eventbrite updates

00:29:08,800 --> 00:29:12,610
one of the rows in our events table the

00:29:10,840 --> 00:29:14,890
dilithium will pick that out will turn

00:29:12,610 --> 00:29:16,000
that into a capital message our indexing

00:29:14,890 --> 00:29:18,520
code will listen to that will say oh

00:29:16,000 --> 00:29:20,500
event 57 has been updated it'll then

00:29:18,520 --> 00:29:22,540
query the database to figure out the

00:29:20,500 --> 00:29:24,100
current details of that particular event

00:29:22,540 --> 00:29:27,880
and then write those changes into

00:29:24,100 --> 00:29:29,500
elasticsearch so the end result is we

00:29:27,880 --> 00:29:31,720
have something which scales extremely

00:29:29,500 --> 00:29:32,560
well which can be run on many different

00:29:31,720 --> 00:29:35,890
machines at once

00:29:32,560 --> 00:29:38,080
and gives us a very robust very robust

00:29:35,890 --> 00:29:41,800
path from initial database change to

00:29:38,080 --> 00:29:44,050
updates in our elastic search index so

00:29:41,800 --> 00:29:48,340
I've got a few tips and tricks that I

00:29:44,050 --> 00:29:49,690
wanted to dive into just little bits and

00:29:48,340 --> 00:29:51,490
pieces that I've picked up that have

00:29:49,690 --> 00:29:55,620
helped with implementing this overall

00:29:51,490 --> 00:29:58,120
and overall pattern and the first one is

00:29:55,620 --> 00:30:01,120
one that can really help avoid serving

00:29:58,120 --> 00:30:02,950
stale data spell data to your users and

00:30:01,120 --> 00:30:05,680
that's to do everything with your search

00:30:02,950 --> 00:30:08,170
engine in terms of object IDs as opposed

00:30:05,680 --> 00:30:09,820
to the raw data itself generally with

00:30:08,170 --> 00:30:11,320
with your search index you'll be writing

00:30:09,820 --> 00:30:12,910
a lot of data into it you know it needs

00:30:11,320 --> 00:30:14,440
to know the titles of things the

00:30:12,910 --> 00:30:16,000
descriptions of things any fields that

00:30:14,440 --> 00:30:17,410
you might want to search by and so

00:30:16,000 --> 00:30:19,630
there's a temptation to hit the search

00:30:17,410 --> 00:30:22,510
index get that data back and then use

00:30:19,630 --> 00:30:24,550
that to construct objects that you would

00:30:22,510 --> 00:30:25,570
present back to your user the moment you

00:30:24,550 --> 00:30:27,640
do that though you're setting yourself

00:30:25,570 --> 00:30:30,430
up some for some really nasty nasty

00:30:27,640 --> 00:30:32,200
latency risks because as we as I said

00:30:30,430 --> 00:30:33,970
earlier there's going to be a three to

00:30:32,200 --> 00:30:35,500
five maybe 10 second delay between

00:30:33,970 --> 00:30:37,330
changes in your database and changing

00:30:35,500 --> 00:30:39,580
your index you really don't want to be

00:30:37,330 --> 00:30:41,410
showing that stale data to your users so

00:30:39,580 --> 00:30:43,480
the trick here is very simple and when

00:30:41,410 --> 00:30:45,610
you run searches you all you ask back

00:30:43,480 --> 00:30:47,470
from the search engine and the IDS of

00:30:45,610 --> 00:30:48,910
the underlying records so you get you

00:30:47,470 --> 00:30:51,100
run a search you get back a list of say

00:30:48,910 --> 00:30:53,500
twenty into your IDs you can then hit

00:30:51,100 --> 00:30:57,820
the database directly to inflate those

00:30:53,500 --> 00:31:00,100
into actual finished objects and it

00:30:57,820 --> 00:31:01,600
sounds like I mean the downside of this

00:31:00,100 --> 00:31:03,700
is you're adding additional load to your

00:31:01,600 --> 00:31:05,560
database the good news is that databases

00:31:03,700 --> 00:31:07,360
are insanely quick at primary key

00:31:05,560 --> 00:31:09,400
lookups anytime you're you're doing

00:31:07,360 --> 00:31:10,900
primary key lookups or look ups against

00:31:09,400 --> 00:31:14,020
an index that's going to return really

00:31:10,900 --> 00:31:17,560
fast so with Jango you can use Django's

00:31:14,020 --> 00:31:19,570
bulk may help method and make extensive

00:31:17,560 --> 00:31:21,220
use of prefetch related as well which

00:31:19,570 --> 00:31:24,130
again is a very fast way of retrieving

00:31:21,220 --> 00:31:26,320
data and you can set it up so that your

00:31:24,130 --> 00:31:28,030
users will never see stale data because

00:31:26,320 --> 00:31:29,620
that's fail data even if the data was

00:31:28,030 --> 00:31:31,000
stale in the index by the time it's

00:31:29,620 --> 00:31:34,410
pulled from the database it's going to

00:31:31,000 --> 00:31:37,180
be the most recent version of things a

00:31:34,410 --> 00:31:38,980
related concept of this is what to do if

00:31:37,180 --> 00:31:40,390
there's so if you're doing this if

00:31:38,980 --> 00:31:41,830
you're pulling things directly from your

00:31:40,390 --> 00:31:43,210
database what do you do if something's

00:31:41,830 --> 00:31:45,280
been deleted what do you do if your

00:31:43,210 --> 00:31:46,420
search engine gives you back ID 57 and

00:31:45,280 --> 00:31:49,000
then when you hit fetch from the

00:31:46,420 --> 00:31:50,950
database ID 57 has been deleted in the

00:31:49,000 --> 00:31:53,290
time it took for you to for that that

00:31:50,950 --> 00:31:54,520
that search to come through and the way

00:31:53,290 --> 00:31:56,500
we've handled this in the past is

00:31:54,520 --> 00:31:58,180
essentially to have a self repairing

00:31:56,500 --> 00:32:00,040
mechanism the code that queries the

00:31:58,180 --> 00:32:01,870
database can notice when there's an ID

00:32:00,040 --> 00:32:03,820
that doesn't matter that that's missing

00:32:01,870 --> 00:32:07,150
and then stick it on a salary queue or

00:32:03,820 --> 00:32:08,830
delete it or or some other mechanism so

00:32:07,150 --> 00:32:11,310
the search index knows to then remove

00:32:08,830 --> 00:32:13,810
that document from the index entirely

00:32:11,310 --> 00:32:16,260
the downside of this is somebody might

00:32:13,810 --> 00:32:18,640
ask for a page with ten results on and

00:32:16,260 --> 00:32:20,290
one of those results is missing so you

00:32:18,640 --> 00:32:22,090
you end up giving them at nine results

00:32:20,290 --> 00:32:24,910
and then quietly filing that tenth way

00:32:22,090 --> 00:32:26,290
to be deleted my hunch is that no one

00:32:24,910 --> 00:32:27,880
will ever notice this I don't think

00:32:26,290 --> 00:32:29,530
people go around counting the number of

00:32:27,880 --> 00:32:33,310
results they get on a page so it's

00:32:29,530 --> 00:32:36,220
probably okay then one last trick which

00:32:33,310 --> 00:32:38,950
again it ties into this idea this is

00:32:36,220 --> 00:32:41,140
something I've been I'm using on a

00:32:38,950 --> 00:32:42,580
project at work at the moment and I'm

00:32:41,140 --> 00:32:46,210
calling the accurate filter trick

00:32:42,580 --> 00:32:47,680
essentially this is a way of solve it's

00:32:46,210 --> 00:32:49,480
an additional way of solving for this

00:32:47,680 --> 00:32:51,400
latency between your search index

00:32:49,480 --> 00:32:54,820
between your database and your search

00:32:51,400 --> 00:32:56,740
index so imagine if you will that you're

00:32:54,820 --> 00:33:00,160
building a system where users can have

00:32:56,740 --> 00:33:01,510
you users can save events so they'll see

00:33:00,160 --> 00:33:03,310
an event that they want to go to they

00:33:01,510 --> 00:33:06,420
hit a Save button and that event is is

00:33:03,310 --> 00:33:09,400
saved to their account in some way I

00:33:06,420 --> 00:33:11,680
would like to be able to pull vote to

00:33:09,400 --> 00:33:13,450
answer queries about and what events

00:33:11,680 --> 00:33:14,920
this user has saved by hitting search

00:33:13,450 --> 00:33:16,450
index because if I can hit the search

00:33:14,920 --> 00:33:18,130
index I could combine it with all of

00:33:16,450 --> 00:33:20,350
these other benefits I can let users

00:33:18,130 --> 00:33:21,940
search with it search for text within

00:33:20,350 --> 00:33:23,680
the events they've saved I can do

00:33:21,940 --> 00:33:25,330
filters by geography there's all sorts

00:33:23,680 --> 00:33:27,420
of of useful things I can do with this

00:33:25,330 --> 00:33:29,520
and that's easy enough to implement you

00:33:27,420 --> 00:33:31,770
a field on your event document in

00:33:29,520 --> 00:33:34,080
elasticsearch wit containing the IDS of

00:33:31,770 --> 00:33:35,340
the users who have saved that event and

00:33:34,080 --> 00:33:37,800
then you can do a search like this you

00:33:35,340 --> 00:33:41,250
can say search for events where one of

00:33:37,800 --> 00:33:43,830
these saved by users values is the the

00:33:41,250 --> 00:33:45,480
user ID that I'm dealing with it's just

00:33:43,830 --> 00:33:47,760
one obvious problem with this if a user

00:33:45,480 --> 00:33:49,140
saves an event and then goes and looks

00:33:47,760 --> 00:33:51,270
at their list of saved events within a

00:33:49,140 --> 00:33:52,980
few seconds and it's not that and that

00:33:51,270 --> 00:33:55,080
event isn't shown to them then obviously

00:33:52,980 --> 00:33:56,850
something is broken you know that this

00:33:55,080 --> 00:33:58,320
is the latency problem that we've been

00:33:56,850 --> 00:34:01,410
fighting since since the beginning of

00:33:58,320 --> 00:34:03,450
the talk so what you can do is you can

00:34:01,410 --> 00:34:05,310
say okay any time I'm running that query

00:34:03,450 --> 00:34:06,990
the first thing I'm going to do is hit

00:34:05,310 --> 00:34:08,340
my relational database to figure out

00:34:06,990 --> 00:34:10,710
what are the events that this user has

00:34:08,340 --> 00:34:13,260
saved in the last X minutes let's say

00:34:10,710 --> 00:34:15,270
the last five minutes so this is

00:34:13,260 --> 00:34:17,610
guaranteed to give me an accurate model

00:34:15,270 --> 00:34:20,190
of the users recent activity and it'll

00:34:17,610 --> 00:34:22,290
give me back say four or five IDs of

00:34:20,190 --> 00:34:24,120
documents that we know that the user has

00:34:22,290 --> 00:34:25,290
saved once you've got that list these

00:34:24,120 --> 00:34:27,030
are the ones that were saved in the last

00:34:25,290 --> 00:34:29,160
five minutes you can construct an

00:34:27,030 --> 00:34:32,250
elastic search query where you say give

00:34:29,160 --> 00:34:33,840
me back any event where either the user

00:34:32,250 --> 00:34:36,419
is listed in the list of users who have

00:34:33,840 --> 00:34:37,950
saved this event or the event itself was

00:34:36,419 --> 00:34:40,860
is one of these five that we know that

00:34:37,950 --> 00:34:43,080
they've saved recently and as a search

00:34:40,860 --> 00:34:44,669
query this will run crazy-fast it gives

00:34:43,080 --> 00:34:47,310
you all of those benefits but this is

00:34:44,669 --> 00:34:49,800
guaranteed to be exactly up-to-date with

00:34:47,310 --> 00:34:51,900
the activity of your users and you can

00:34:49,800 --> 00:34:53,760
this you say it saved my users is one

00:34:51,900 --> 00:34:55,290
obvious application of this there are a

00:34:53,760 --> 00:34:58,670
whole bunch of other things where if you

00:34:55,290 --> 00:35:00,750
want to get a precise up to the date

00:34:58,670 --> 00:35:02,940
reflection of the state of your system

00:35:00,750 --> 00:35:06,330
you can use tricks like this to pull

00:35:02,940 --> 00:35:09,300
those out of elasticsearch so a few more

00:35:06,330 --> 00:35:11,040
use cases that I've applied in

00:35:09,300 --> 00:35:12,990
particularly I've applied elastic search

00:35:11,040 --> 00:35:14,490
to one that we used Eventbrite is for

00:35:12,990 --> 00:35:16,520
recommendations because it turns out

00:35:14,490 --> 00:35:19,160
recommending events to a user is

00:35:16,520 --> 00:35:21,480
essentially just another search problem

00:35:19,160 --> 00:35:23,490
you can there are a bunch of ways you

00:35:21,480 --> 00:35:25,680
can do this you can say find events

00:35:23,490 --> 00:35:27,150
where one of my friends has saved that

00:35:25,680 --> 00:35:29,640
event this is the way we did our

00:35:27,150 --> 00:35:30,840
calendar affair lanyard earlier and you

00:35:29,640 --> 00:35:32,970
could also say find events that are

00:35:30,840 --> 00:35:35,520
similar to the last ten events that I've

00:35:32,970 --> 00:35:37,920
saved a very straightforward way of

00:35:35,520 --> 00:35:40,829
doing that is to look at the last ten

00:35:37,920 --> 00:35:42,420
events of my user collect together the

00:35:40,829 --> 00:35:44,249
text from the title and description of

00:35:42,420 --> 00:35:45,900
all of those events into a giant blob of

00:35:44,249 --> 00:35:47,670
words and then just search for those

00:35:45,900 --> 00:35:49,920
words with their boolean or clause which

00:35:47,670 --> 00:35:51,569
is enough for elastic searches relevance

00:35:49,920 --> 00:35:53,519
to kick in and it'll give you back other

00:35:51,569 --> 00:35:55,410
events that are that are similar in

00:35:53,519 --> 00:35:58,589
textual content the events that the user

00:35:55,410 --> 00:36:00,299
has saved as well and search engines are

00:35:58,589 --> 00:36:02,430
really good at relevant scoring and

00:36:00,299 --> 00:36:06,269
boosting so you can fine-tune this stuff

00:36:02,430 --> 00:36:07,920
very to a huge huge extent using the

00:36:06,269 --> 00:36:09,180
tools that are built into the search

00:36:07,920 --> 00:36:10,739
engine

00:36:09,180 --> 00:36:14,009
another thing elastic search is greater

00:36:10,739 --> 00:36:16,079
is geographic search it's got built-in

00:36:14,009 --> 00:36:17,640
support for geo you can add latitude and

00:36:16,079 --> 00:36:19,950
longitude points to your documents and

00:36:17,640 --> 00:36:21,930
then you can do things like all event or

00:36:19,950 --> 00:36:24,239
all all documents within five kilometers

00:36:21,930 --> 00:36:25,619
of this rate radius point you can even

00:36:24,239 --> 00:36:27,719
send it a polygon and say here is the

00:36:25,619 --> 00:36:29,910
shape of Canada give me everything that

00:36:27,719 --> 00:36:31,709
falls into that that polygon shape and

00:36:29,910 --> 00:36:33,029
then again this is stuff which if you're

00:36:31,709 --> 00:36:34,349
not using post squares can be quite

00:36:33,029 --> 00:36:36,690
difficult to do with a relational

00:36:34,349 --> 00:36:38,339
database but more importantly you can

00:36:36,690 --> 00:36:39,930
combine these with all of the other

00:36:38,339 --> 00:36:41,640
search and filters so if I've got a

00:36:39,930 --> 00:36:43,559
recommendation system built on elastic

00:36:41,640 --> 00:36:45,539
search I can say recommend me events

00:36:43,559 --> 00:36:50,069
similar to these events that fall within

00:36:45,539 --> 00:36:52,219
this geographic area and and further

00:36:50,069 --> 00:36:54,630
combine that with other options as well

00:36:52,219 --> 00:36:56,309
elastic search is search engine the

00:36:54,630 --> 00:36:57,930
general are great for visualizations you

00:36:56,309 --> 00:37:00,119
saw this earlier with the DC Inbox

00:36:57,930 --> 00:37:01,799
Explorer I've got this little graph at

00:37:00,119 --> 00:37:03,690
the top where you actually generated

00:37:01,799 --> 00:37:05,369
from yet from just another one of these

00:37:03,690 --> 00:37:07,289
aggregations when I searched against

00:37:05,369 --> 00:37:09,869
elastic search I can say give me back

00:37:07,289 --> 00:37:11,400
counts per month for this time period

00:37:09,869 --> 00:37:13,469
and also I've got those counts back I

00:37:11,400 --> 00:37:16,019
can turn those into into a bar chart and

00:37:13,469 --> 00:37:17,849
if you look at the way elastic search is

00:37:16,019 --> 00:37:20,249
used for log analysis people do some

00:37:17,849 --> 00:37:21,930
really exciting visualizations on top of

00:37:20,249 --> 00:37:24,420
the raw data that's being collected by

00:37:21,930 --> 00:37:26,699
these aggregations and one way to think

00:37:24,420 --> 00:37:29,069
about this is it's kind of like having a

00:37:26,699 --> 00:37:31,289
real-time MapReduce engine if you've

00:37:29,069 --> 00:37:33,150
ever used MapReduce on something like

00:37:31,289 --> 00:37:35,069
Hadoop it's a very powerful way of

00:37:33,150 --> 00:37:36,839
running a query across many different

00:37:35,069 --> 00:37:38,609
many different machines getting results

00:37:36,839 --> 00:37:39,869
but it's generally something you want to

00:37:38,609 --> 00:37:41,640
run as a batch job because it might

00:37:39,869 --> 00:37:43,920
takes 30 seconds to a minute for it to

00:37:41,640 --> 00:37:45,690
return results elasticsearch under the

00:37:43,920 --> 00:37:47,670
hood is doing pretty much exactly that

00:37:45,690 --> 00:37:50,039
it's got if you give it a search it will

00:37:47,670 --> 00:37:51,989
spread it out across the nodes in your

00:37:50,039 --> 00:37:54,190
cluster combine the results together and

00:37:51,989 --> 00:37:55,990
use that to return return

00:37:54,190 --> 00:37:57,550
returned documents to you but it's

00:37:55,990 --> 00:38:00,040
designed to work in real time you're

00:37:57,550 --> 00:38:01,570
getting like response times measured

00:38:00,040 --> 00:38:03,810
within milliseconds which means that you

00:38:01,570 --> 00:38:07,150
can expose these directly to your users

00:38:03,810 --> 00:38:08,680
so in summary you should do normalize

00:38:07,150 --> 00:38:10,300
your data to a query engine it's

00:38:08,680 --> 00:38:11,500
definitely a good idea it lets you build

00:38:10,300 --> 00:38:13,360
all sorts of things you couldn't build

00:38:11,500 --> 00:38:15,340
before and lost exert it turns out it's

00:38:13,360 --> 00:38:17,230
a pretty good option for this and I've

00:38:15,340 --> 00:38:25,620
left lots of time for questions so thank

00:38:17,230 --> 00:38:28,720
you very much I'll take that question

00:38:25,620 --> 00:38:30,910
thank you you were talking about how you

00:38:28,720 --> 00:38:33,040
could use a cue to sort of D duplicate

00:38:30,910 --> 00:38:35,680
repeated indexer actions can you speak a

00:38:33,040 --> 00:38:38,160
little more to that sure so the thing

00:38:35,680 --> 00:38:40,870
that you want to avoid is 500 feet like

00:38:38,160 --> 00:38:42,640
500 people interact with something in

00:38:40,870 --> 00:38:44,890
your database and then you've sent 500

00:38:42,640 --> 00:38:48,690
updates to your elastic search index in

00:38:44,890 --> 00:38:51,640
it in a giant flurry so really this is

00:38:48,690 --> 00:38:54,520
is it DGP yeah its DGP it's rate

00:38:51,640 --> 00:38:56,590
limiting it so it's being able to get

00:38:54,520 --> 00:38:58,420
smart about this and say ok there were

00:38:56,590 --> 00:38:59,950
500 updates within a short space of time

00:38:58,420 --> 00:39:02,520
but I'm actually going to turn that into

00:38:59,950 --> 00:39:05,140
a single combined update to the index

00:39:02,520 --> 00:39:07,720
the way I've built this on top of a

00:39:05,140 --> 00:39:10,510
queue is you're the code that listens to

00:39:07,720 --> 00:39:13,420
the cue well so actually one way that I

00:39:10,510 --> 00:39:16,270
built this against Redis was to say when

00:39:13,420 --> 00:39:17,890
an indexing request comes in if I have

00:39:16,270 --> 00:39:19,990
if the index I hasn't done anything in

00:39:17,890 --> 00:39:22,000
if you're in in five seconds just index

00:39:19,990 --> 00:39:23,620
that thing straight away if the index R

00:39:22,000 --> 00:39:24,670
has run within the past five seconds

00:39:23,620 --> 00:39:25,690
that suggests that there's a lot of

00:39:24,670 --> 00:39:27,910
activity going on

00:39:25,690 --> 00:39:29,500
so then hold out for a couple of seconds

00:39:27,910 --> 00:39:31,510
to see if other updates come in for the

00:39:29,500 --> 00:39:32,860
same event ID and if they do bundle

00:39:31,510 --> 00:39:36,940
those together and send that all at once

00:39:32,860 --> 00:39:38,530
I think actually the M the D duping

00:39:36,940 --> 00:39:40,330
becomes a lot easier if you use the last

00:39:38,530 --> 00:39:41,830
modified time stamp because then it's

00:39:40,330 --> 00:39:44,080
just your cron job run runs once a

00:39:41,830 --> 00:39:45,430
minute and if there were 500 updates to

00:39:44,080 --> 00:39:50,890
an event in the last minute you'll still

00:39:45,430 --> 00:39:53,740
only we index it once hey I was

00:39:50,890 --> 00:39:59,130
wondering are you the only person who's

00:39:53,740 --> 00:40:01,540
like put this into terms and eyes

00:39:59,130 --> 00:40:03,790
educating people about this as far as I

00:40:01,540 --> 00:40:06,190
know I am which I find really surprising

00:40:03,790 --> 00:40:07,140
because I've seen lots of places doing

00:40:06,190 --> 00:40:08,309
exactly this I just

00:40:07,140 --> 00:40:10,049
think anyone's put a name on it before

00:40:08,309 --> 00:40:11,609
and so if somebody does have an

00:40:10,049 --> 00:40:13,319
alternative name their hand I'd love to

00:40:11,609 --> 00:40:15,239
hear about it but yeah as far as I can

00:40:13,319 --> 00:40:17,249
tell I'm the only person who's said

00:40:15,239 --> 00:40:19,559
let's let's give this a name and start

00:40:17,249 --> 00:40:21,869
discussing this as a general strategy so

00:40:19,559 --> 00:40:25,140
there's no books no buts

00:40:21,869 --> 00:40:29,819
bugs bugs Oh books definitely not yet no

00:40:25,140 --> 00:40:32,160
I I should write a blog entry yes yes

00:40:29,819 --> 00:40:35,400
please write a blog entry that would be

00:40:32,160 --> 00:40:36,569
great so you talked a lot about getting

00:40:35,400 --> 00:40:39,089
stuff out

00:40:36,569 --> 00:40:42,239
it's wondering like specifically for

00:40:39,089 --> 00:40:44,759
your DC mailbox example do you have to

00:40:42,239 --> 00:40:46,710
do a lot of pre-processing of the data

00:40:44,759 --> 00:40:48,900
before it goes in when you're actually

00:40:46,710 --> 00:40:50,400
structuring your documents and stuff

00:40:48,900 --> 00:40:53,369
like that is there an extra step on that

00:40:50,400 --> 00:40:55,079
side yeah so I'm one concept I didn't

00:40:53,369 --> 00:40:56,910
really talk about is there's a thing in

00:40:55,079 --> 00:40:58,739
lastik surgical - mapping which is

00:40:56,910 --> 00:41:01,170
basically the same thing as a sequel

00:40:58,739 --> 00:41:02,819
schema you don't have to use mappings

00:41:01,170 --> 00:41:04,920
you can just start blasting JSON

00:41:02,819 --> 00:41:06,329
documents at it and it'll it'll work but

00:41:04,920 --> 00:41:07,920
if you actually want to be able to

00:41:06,329 --> 00:41:09,839
differentiate date and times from

00:41:07,920 --> 00:41:11,700
Geographic points and so forth you need

00:41:09,839 --> 00:41:14,160
to use that and so here's the mapping

00:41:11,700 --> 00:41:16,230
for the DC Inbox

00:41:14,160 --> 00:41:18,180
thing and actually this one's there's a

00:41:16,230 --> 00:41:20,130
lot of data here so it's got caucuses

00:41:18,180 --> 00:41:21,900
and Congress numbers and c-span IDs and

00:41:20,130 --> 00:41:25,739
so forth the source code for this is all

00:41:21,900 --> 00:41:29,609
available on github so you can see so I

00:41:25,739 --> 00:41:31,710
yeah I create I actually used a library

00:41:29,609 --> 00:41:33,960
called elasticsearch DSL for this which

00:41:31,710 --> 00:41:35,730
is a Python library that tries to be a

00:41:33,960 --> 00:41:38,160
slightly high-level way of working with

00:41:35,730 --> 00:41:39,839
elasticsearch you can also compose this

00:41:38,160 --> 00:41:41,519
as a JSON blob and then post that to

00:41:39,839 --> 00:41:43,289
elasticsearch itself but yeah so this is

00:41:41,519 --> 00:41:44,999
your first step is going to be designing

00:41:43,289 --> 00:41:47,369
a mapping for your data the actual

00:41:44,999 --> 00:41:48,569
indexing is pretty trivial once you've

00:41:47,369 --> 00:41:50,069
got the mapping in place because you

00:41:48,569 --> 00:41:51,960
really are just constructing JSON

00:41:50,069 --> 00:41:53,970
documents and then posting them back up

00:41:51,960 --> 00:41:56,819
to the server but yeah the mapping

00:41:53,970 --> 00:41:57,839
design is is quite important thanks for

00:41:56,819 --> 00:41:59,700
the talk it was really great

00:41:57,839 --> 00:42:02,339
um so I I know that you're talking about

00:41:59,700 --> 00:42:04,380
how performant this is to to find

00:42:02,339 --> 00:42:06,480
documents based on whatever criteria so

00:42:04,380 --> 00:42:08,519
is it equally performant if I wanted to

00:42:06,480 --> 00:42:10,650
do something like aggregate so to get

00:42:08,519 --> 00:42:12,059
like average scores or standard standard

00:42:10,650 --> 00:42:14,309
deviations on something inside the

00:42:12,059 --> 00:42:17,339
document absolutely this is the strength

00:42:14,309 --> 00:42:19,320
of aggregations is that they're insanely

00:42:17,339 --> 00:42:21,570
fast for stuff I mean I

00:42:19,320 --> 00:42:23,610
as you get more complicated with the

00:42:21,570 --> 00:42:25,830
aggregations the performance can it can

00:42:23,610 --> 00:42:27,480
start to add up but honestly the the

00:42:25,830 --> 00:42:29,250
most complex queries I've come up with a

00:42:27,480 --> 00:42:31,320
in the order of 100 milliseconds as

00:42:29,250 --> 00:42:33,090
opposed to 10 milliseconds so generally

00:42:31,320 --> 00:42:34,500
the performance is really good and yeah

00:42:33,090 --> 00:42:35,790
there are a lot of aggregations I've

00:42:34,500 --> 00:42:37,800
shown so far what are called bucket

00:42:35,790 --> 00:42:39,060
aggregations where you divide documents

00:42:37,800 --> 00:42:41,700
into different named buckets

00:42:39,060 --> 00:42:43,050
there are also aggregations for metrics

00:42:41,700 --> 00:42:45,660
like that can calculate things like

00:42:43,050 --> 00:42:48,660
standard deviations and sums and medians

00:42:45,660 --> 00:42:50,130
and even like in geo special spatial

00:42:48,660 --> 00:42:51,960
terms there are aggregates that will

00:42:50,130 --> 00:42:53,730
calculate a bounding box around all of

00:42:51,960 --> 00:42:55,470
the documents so there's a whole bunch

00:42:53,730 --> 00:42:58,110
of additional power and flexibility you

00:42:55,470 --> 00:42:59,530
get around that all right thank you so

00:42:58,110 --> 00:43:06,729
much Simon thank you

00:42:59,530 --> 00:43:06,729

YouTube URL: https://www.youtube.com/watch?v=NzcvewgqYog


