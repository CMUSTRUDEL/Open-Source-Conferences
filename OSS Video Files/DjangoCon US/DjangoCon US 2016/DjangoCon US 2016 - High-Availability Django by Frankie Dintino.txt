Title: DjangoCon US 2016 - High-Availability Django by Frankie Dintino
Publication date: 2016-08-24
Playlist: DjangoCon US 2016
Description: 
	High-Availability Django by Frankie Dintino

One year ago we completed a years-long project of migrating theatlantic.com from a sprawling PHP codebase to a Python application built on Django. Our first attempt at a load-balanced Python stack had serious flaws, as we quickly learned. Since then we have completely remade our stack from the bottom up; we have built tools that improve our ability to monitor for performance and service degradation; and we have developed a deployment process that incorporates automated testing and that allows us to push out updates without incurring any downtime. I will discuss the mistakes we made, the steps we took to identify performance problems and server resource issues, what our current stack looks like, and how we achieved the holy grail of zero-downtime deploys.
Captions: 
	00:00:14,970 --> 00:00:21,029
so today I'm going to be talking about

00:00:18,050 --> 00:00:22,680
basically the the performance issues

00:00:21,029 --> 00:00:25,320
that we dealt with at the Atlantic over

00:00:22,680 --> 00:00:26,970
the last year how we overcame them and

00:00:25,320 --> 00:00:30,029
some of the things that we learned on

00:00:26,970 --> 00:00:31,829
the way there so I'd like to start off

00:00:30,029 --> 00:00:34,530
by thanking the organizers of django con

00:00:31,829 --> 00:00:37,380
for taking interest in my proposal all

00:00:34,530 --> 00:00:39,030
of you for coming to listen to it and i

00:00:37,380 --> 00:00:40,680
think one of the most valuable things

00:00:39,030 --> 00:00:42,750
about this conference on par with the

00:00:40,680 --> 00:00:44,309
talks in the events is the opportunity

00:00:42,750 --> 00:00:48,030
presents to meet and talk to some really

00:00:44,309 --> 00:00:49,800
smart people in fact the work i did in

00:00:48,030 --> 00:00:52,079
the past year what eventually became

00:00:49,800 --> 00:00:53,910
this talk was thanks to a conversation i

00:00:52,079 --> 00:00:55,589
had with one of the founders of op e if

00:00:53,910 --> 00:00:57,030
you're not familiar with them they're a

00:00:55,589 --> 00:00:58,769
service for monitoring performance of

00:00:57,030 --> 00:01:03,659
web apps similar to New Relic but with a

00:00:58,769 --> 00:01:04,860
special focus on Django at the time in

00:01:03,659 --> 00:01:06,090
September of last year we were

00:01:04,860 --> 00:01:08,280
struggling with growing pains in our

00:01:06,090 --> 00:01:10,050
django application we had just completed

00:01:08,280 --> 00:01:12,180
a huge project the porting of over six

00:01:10,050 --> 00:01:14,670
years of legacy code a mix of PHP and

00:01:12,180 --> 00:01:16,439
perl to a Django powered CMS in front

00:01:14,670 --> 00:01:17,880
end there were a few performance hiccups

00:01:16,439 --> 00:01:19,350
at launch but we eventually overcame

00:01:17,880 --> 00:01:23,040
most of those with one important

00:01:19,350 --> 00:01:24,869
exception our servers basically melted

00:01:23,040 --> 00:01:26,549
when we deployed at the time we were

00:01:24,869 --> 00:01:28,200
using mod whiskey so our process was

00:01:26,549 --> 00:01:30,829
basically tee up the code using a fabric

00:01:28,200 --> 00:01:34,320
script and then touch the whiskey file

00:01:30,829 --> 00:01:38,759
today as you can see and you're reading

00:01:34,320 --> 00:01:41,880
that correctly here 80 second response

00:01:38,759 --> 00:01:45,240
times and now down to 150 milliseconds

00:01:41,880 --> 00:01:48,000
so I'm gonna first cover the things

00:01:45,240 --> 00:01:49,950
there wasn't one single thing that we

00:01:48,000 --> 00:01:52,049
did that addressed our performance

00:01:49,950 --> 00:01:53,909
issues it was a bunch of small things a

00:01:52,049 --> 00:01:56,250
couple things that that were actually

00:01:53,909 --> 00:01:58,289
low-hanging fruit that like we're great

00:01:56,250 --> 00:02:00,270
wins but in general it was a bunch of

00:01:58,289 --> 00:02:04,350
small things that build up to fixing our

00:02:00,270 --> 00:02:06,659
performance problems so the first thing

00:02:04,350 --> 00:02:08,039
is monitoring and profiling if you don't

00:02:06,659 --> 00:02:10,920
understand what your bottlenecks are and

00:02:08,039 --> 00:02:14,010
you don't understand where the slowness

00:02:10,920 --> 00:02:16,260
is occurring or where the app is

00:02:14,010 --> 00:02:18,840
freezing up then you're just sort of

00:02:16,260 --> 00:02:20,280
stabbing in the dark and I also think

00:02:18,840 --> 00:02:21,180
it's important to tackle the easy stuff

00:02:20,280 --> 00:02:23,760
first

00:02:21,180 --> 00:02:24,330
in quotes here query optimization in

00:02:23,760 --> 00:02:27,480
caching or

00:02:24,330 --> 00:02:31,080
sort of pretty easy ways to get quick

00:02:27,480 --> 00:02:32,700
wins with the application and also you

00:02:31,080 --> 00:02:34,560
know this talk is very focused on the

00:02:32,700 --> 00:02:36,090
server side of things but it's important

00:02:34,560 --> 00:02:37,590
not to neglect front-end performance as

00:02:36,090 --> 00:02:39,630
well which is like a completely

00:02:37,590 --> 00:02:41,280
different ballgame but you can have a

00:02:39,630 --> 00:02:42,960
super fast server and still your

00:02:41,280 --> 00:02:47,000
JavaScript takes 20 seconds to load and

00:02:42,960 --> 00:02:51,360
nobody visits your site so for profiling

00:02:47,000 --> 00:02:54,390
we found the most use with hosted

00:02:51,360 --> 00:02:57,540
services new relic or offbeat or both

00:02:54,390 --> 00:03:00,470
comparable and great services and we

00:02:57,540 --> 00:03:03,300
identified a number of really important

00:03:00,470 --> 00:03:05,459
performance issues in our site with

00:03:03,300 --> 00:03:07,170
those also everybody I'm sure is

00:03:05,459 --> 00:03:09,120
familiar with Django debug toolbar I

00:03:07,170 --> 00:03:10,580
recently came across Django silk which

00:03:09,120 --> 00:03:13,440
seems like a really promising

00:03:10,580 --> 00:03:15,690
application for a sort of different way

00:03:13,440 --> 00:03:17,490
of profiling your Django application and

00:03:15,690 --> 00:03:20,070
if your masochist

00:03:17,490 --> 00:03:23,430
you can always try see profile pie prof

00:03:20,070 --> 00:03:24,750
to call tree and cake ash grind and also

00:03:23,430 --> 00:03:27,570
there's some whiskey middleware which

00:03:24,750 --> 00:03:31,560
I've had mixed experience with there was

00:03:27,570 --> 00:03:33,330
like one bit of code that I was able to

00:03:31,560 --> 00:03:34,680
like kind of figure out where things

00:03:33,330 --> 00:03:36,209
were going wrong with that but it was

00:03:34,680 --> 00:03:39,540
really difficult to set up and probably

00:03:36,209 --> 00:03:41,280
not worth the effort monitoring their

00:03:39,540 --> 00:03:44,670
galantine notifications some of them

00:03:41,280 --> 00:03:47,880
hosted a lot of the same services that

00:03:44,670 --> 00:03:50,280
provide performance monitoring new relic

00:03:47,880 --> 00:03:52,410
effete and then also alert ranch artbeat

00:03:50,280 --> 00:03:54,269
I'll provide the service as well and

00:03:52,410 --> 00:03:56,430
then you can have self hosted services

00:03:54,269 --> 00:04:00,510
like Nagios and its many Forks and also

00:03:56,430 --> 00:04:05,550
others like zabbix so with the easy

00:04:00,510 --> 00:04:07,230
stuff caching using a CDN we at the time

00:04:05,550 --> 00:04:08,430
that we launched we did have a CDN that

00:04:07,230 --> 00:04:12,540
we weren't using our cache headers

00:04:08,430 --> 00:04:15,420
properly and so kind of being really

00:04:12,540 --> 00:04:18,120
strategic about where we said what to

00:04:15,420 --> 00:04:20,549
bury the cache on and when and how long

00:04:18,120 --> 00:04:23,100
to catch the page particularly for

00:04:20,549 --> 00:04:25,800
archival pieces really helped us with

00:04:23,100 --> 00:04:28,440
some of the traffic we got from crawlers

00:04:25,800 --> 00:04:30,110
and bots which is the majority the

00:04:28,440 --> 00:04:32,520
traffic that gets past our CDN

00:04:30,110 --> 00:04:34,289
it's important I think to have a proxy

00:04:32,520 --> 00:04:37,370
cache whether it's nginx squid or

00:04:34,289 --> 00:04:40,040
varnish we use nginx and we've had

00:04:37,370 --> 00:04:42,470
a great success with one of the really

00:04:40,040 --> 00:04:44,150
great things about the nginx proxy cache

00:04:42,470 --> 00:04:46,460
is you can actually set it up to just

00:04:44,150 --> 00:04:49,940
cache for five seconds or two seconds

00:04:46,460 --> 00:04:53,479
but to hold the information in the cache

00:04:49,940 --> 00:04:55,639
as long as there until there's a 200

00:04:53,479 --> 00:04:59,570
response so if your application goes

00:04:55,639 --> 00:05:01,960
down that two-second cache he will hold

00:04:59,570 --> 00:05:05,960
up until your site comes back up again

00:05:01,960 --> 00:05:08,060
we've had this really save us like you

00:05:05,960 --> 00:05:09,979
know somebody deployed some bad code or

00:05:08,060 --> 00:05:11,960
there's an issue with the database and

00:05:09,979 --> 00:05:13,970
the site nobody really realized that the

00:05:11,960 --> 00:05:15,710
site was down because we had this two

00:05:13,970 --> 00:05:17,740
second cache that basically lasted all

00:05:15,710 --> 00:05:17,740
night

00:05:19,220 --> 00:05:23,300
also with caching the stuff that comes

00:05:21,560 --> 00:05:26,270
built in with Django like the cache

00:05:23,300 --> 00:05:29,020
template tag at cache property cache

00:05:26,270 --> 00:05:31,160
static file storage to cache the

00:05:29,020 --> 00:05:33,380
information about where the static files

00:05:31,160 --> 00:05:34,610
are located on the filesystem and the

00:05:33,380 --> 00:05:37,850
cache template loader which does the

00:05:34,610 --> 00:05:40,850
same thing for the template finder page

00:05:37,850 --> 00:05:43,130
caching frameworks we actually have one

00:05:40,850 --> 00:05:44,690
that I thought we had open sourced and I

00:05:43,130 --> 00:05:47,479
realized today that we have a closed

00:05:44,690 --> 00:05:48,830
source for some reason so I talked to my

00:05:47,479 --> 00:05:51,800
coworker and we're gonna open source it

00:05:48,830 --> 00:05:55,070
called Django cash cow in the spirit of

00:05:51,800 --> 00:05:58,520
sort of Pawnee stupid names for Django

00:05:55,070 --> 00:06:01,160
projects it's kind of loosely based on

00:05:58,520 --> 00:06:03,080
Django jimmy page which is hasn't been

00:06:01,160 --> 00:06:05,450
updated in three years as still as alpha

00:06:03,080 --> 00:06:07,520
so maybe it's an improvement on it but

00:06:05,450 --> 00:06:11,449
there's also you know a number of really

00:06:07,520 --> 00:06:14,449
decent page caching frameworks or I'm

00:06:11,449 --> 00:06:17,300
cashing like django cash machine I feel

00:06:14,449 --> 00:06:20,630
like it's a mixed bag and I'll actually

00:06:17,300 --> 00:06:23,300
get to that in the next slide so for

00:06:20,630 --> 00:06:25,250
query optimization there's the obvious

00:06:23,300 --> 00:06:28,340
stuff like prefetch related and select

00:06:25,250 --> 00:06:31,130
related prefetch related objects which

00:06:28,340 --> 00:06:33,080
is in 1.10 public though it's accessible

00:06:31,130 --> 00:06:36,580
in earlier versions of Django that

00:06:33,080 --> 00:06:40,610
allows you to pass a call 'evil and

00:06:36,580 --> 00:06:43,729
filter the objects that you prefetch on

00:06:40,610 --> 00:06:46,490
so that you can prefetch conditionally

00:06:43,729 --> 00:06:49,159
items in the query set for instance in a

00:06:46,490 --> 00:06:51,360
generic foreign key you can prefetch all

00:06:49,159 --> 00:06:53,519
of the instances of one

00:06:51,360 --> 00:06:56,759
this field and not have to worry about

00:06:53,519 --> 00:06:58,019
it conflicting with other results in the

00:06:56,759 --> 00:07:00,659
query set that might be from different

00:06:58,019 --> 00:07:02,879
models that do not have that field dot

00:07:00,659 --> 00:07:05,729
value send dot values list I found this

00:07:02,879 --> 00:07:09,449
really surprising when you're profiling

00:07:05,729 --> 00:07:11,009
Django one of the things that we found

00:07:09,449 --> 00:07:12,929
to have like a huge impact but which

00:07:11,009 --> 00:07:15,439
doesn't really show up in any profiling

00:07:12,929 --> 00:07:18,089
is model instantiation and hydration

00:07:15,439 --> 00:07:20,429
because and this this is what ties back

00:07:18,089 --> 00:07:23,669
into like Django cash machine

00:07:20,429 --> 00:07:25,799
we had a query that basically we stored

00:07:23,669 --> 00:07:27,989
in the database we had all the different

00:07:25,799 --> 00:07:30,029
ad slots on our site and then the break

00:07:27,989 --> 00:07:33,389
points that they were enabled and the

00:07:30,029 --> 00:07:35,879
sizes of the ad units and all together

00:07:33,389 --> 00:07:37,829
with the prefetch relating and the you

00:07:35,879 --> 00:07:41,089
know select related this amounted to I

00:07:37,829 --> 00:07:43,589
think like seven hundred instances being

00:07:41,089 --> 00:07:46,829
loaded into memory on every request

00:07:43,589 --> 00:07:49,069
which came out to about 80 milliseconds

00:07:46,829 --> 00:07:51,659
one hundred milliseconds per request

00:07:49,069 --> 00:07:53,610
when we switch this to just dot values

00:07:51,659 --> 00:07:55,579
and you know manipulated addicts

00:07:53,610 --> 00:07:58,439
basically simulate the prefetch related

00:07:55,579 --> 00:08:00,149
like I said 80 millisecond drop in every

00:07:58,439 --> 00:08:02,759
request so that was like a huge gain

00:08:00,149 --> 00:08:05,459
basically made our response times about

00:08:02,759 --> 00:08:07,229
25 percent faster at that point and then

00:08:05,459 --> 00:08:09,300
obviously you know but much more

00:08:07,229 --> 00:08:12,599
difficult database parameter tuning and

00:08:09,300 --> 00:08:14,129
judicious indexing and you can always

00:08:12,599 --> 00:08:17,219
throw more hardware at the problem that

00:08:14,129 --> 00:08:20,159
was you know part of our speed gains

00:08:17,219 --> 00:08:22,860
there was cheating we upgraded from like

00:08:20,159 --> 00:08:25,739
four year old servers to brand new

00:08:22,860 --> 00:08:29,219
servers that ran you know twice as fast

00:08:25,739 --> 00:08:31,919
so you know that cut our response times

00:08:29,219 --> 00:08:34,889
in half now you might notice from the

00:08:31,919 --> 00:08:38,039
earlier thing the only thing in this

00:08:34,889 --> 00:08:41,699
chart is request queuing what is request

00:08:38,039 --> 00:08:43,559
queuing so to explain that I'm going to

00:08:41,699 --> 00:08:46,740
rewind a little bit and just kind of

00:08:43,559 --> 00:08:51,300
talk about how we were set up so we had

00:08:46,740 --> 00:08:53,279
I think what's a fairly standard way of

00:08:51,300 --> 00:08:54,720
load balancing at Django application we

00:08:53,279 --> 00:08:57,779
had nginx in front of it as a proxy

00:08:54,720 --> 00:08:59,220
cache and then we had a couple of

00:08:57,779 --> 00:09:02,939
application servers in this case they're

00:08:59,220 --> 00:09:04,170
running mod whiskey behind it the nginx

00:09:02,939 --> 00:09:05,730
was listening on port

00:09:04,170 --> 00:09:07,200
and then we had the different

00:09:05,730 --> 00:09:09,360
applications listening on different

00:09:07,200 --> 00:09:12,630
ports and defined up streams and the

00:09:09,360 --> 00:09:16,769
nginx config across the different app

00:09:12,630 --> 00:09:19,260
servers we also had a staged version of

00:09:16,769 --> 00:09:21,540
our site which we would use to test out

00:09:19,260 --> 00:09:23,070
code in production on the production

00:09:21,540 --> 00:09:26,339
environment and we just had like a

00:09:23,070 --> 00:09:27,779
different port for it so you know our

00:09:26,339 --> 00:09:29,339
directory structure looks something like

00:09:27,779 --> 00:09:32,370
this

00:09:29,339 --> 00:09:35,339
the discussion I had last year at django

00:09:32,370 --> 00:09:37,350
con with founder Bob beat you know so I

00:09:35,339 --> 00:09:39,360
was explaining him this problem we had

00:09:37,350 --> 00:09:40,740
we would load everything up we were

00:09:39,360 --> 00:09:43,050
trying to preload as much as we could

00:09:40,740 --> 00:09:44,730
then we would touch the whiskey file and

00:09:43,050 --> 00:09:46,680
then kind of cross our fingers and if we

00:09:44,730 --> 00:09:49,740
were we happen to be slammed by a bot at

00:09:46,680 --> 00:09:53,760
that time the site all the code that it

00:09:49,740 --> 00:09:55,740
took to load on our local dev machines

00:09:53,760 --> 00:09:57,630
just loading up one worker process it

00:09:55,740 --> 00:10:00,000
would take maybe 10 seconds but on the

00:09:57,630 --> 00:10:03,029
server loading 40 workers simultaneously

00:10:00,000 --> 00:10:04,740
across you know five VMs which are all

00:10:03,029 --> 00:10:07,290
actually sitting on three bare metal

00:10:04,740 --> 00:10:09,329
machines this could take 30 seconds 45

00:10:07,290 --> 00:10:11,730
seconds and during that that period of

00:10:09,329 --> 00:10:16,170
time all these requests are queuing up

00:10:11,730 --> 00:10:17,820
and basically the server by the time the

00:10:16,170 --> 00:10:20,370
server is responding it's still being

00:10:17,820 --> 00:10:21,630
overwhelmed by requests and there were a

00:10:20,370 --> 00:10:23,160
few times where we had to do like

00:10:21,630 --> 00:10:25,350
rolling restarts of our server and

00:10:23,160 --> 00:10:27,149
actually bring them down and take them

00:10:25,350 --> 00:10:30,540
out of rotation an inch next just that

00:10:27,149 --> 00:10:33,000
they could kind of cool off and and be

00:10:30,540 --> 00:10:35,940
able to respond again so what he

00:10:33,000 --> 00:10:37,860
suggested was why don't you rather than

00:10:35,940 --> 00:10:43,220
having like a stage and Ally that are

00:10:37,860 --> 00:10:47,459
separate and you know doing the switch

00:10:43,220 --> 00:10:50,600
basically like at the deploy time load

00:10:47,459 --> 00:10:55,560
the code and touch the whiskey first and

00:10:50,600 --> 00:10:57,569
then switch the symlinks basically have

00:10:55,560 --> 00:11:01,889
like a and B sim link to stage and live

00:10:57,569 --> 00:11:05,790
so that you have the a and the B folders

00:11:01,889 --> 00:11:07,529
are always production ready in any given

00:11:05,790 --> 00:11:10,079
case one of them is either one version

00:11:07,529 --> 00:11:12,089
ahead or one version behind what's

00:11:10,079 --> 00:11:14,310
currently on production but in every

00:11:12,089 --> 00:11:16,920
other respect the you know the CDN path

00:11:14,310 --> 00:11:18,690
that they're using the database their

00:11:16,920 --> 00:11:20,910
production

00:11:18,690 --> 00:11:23,130
you force as much code as possible to

00:11:20,910 --> 00:11:25,170
load before defining your deaf

00:11:23,130 --> 00:11:26,790
application your whiskey file this is

00:11:25,170 --> 00:11:29,460
less important for mod whiskey but we

00:11:26,790 --> 00:11:31,230
switch to you whiskey and one of the

00:11:29,460 --> 00:11:34,530
things that's nice when you're using you

00:11:31,230 --> 00:11:37,080
whiskey and you set up again with the

00:11:34,530 --> 00:11:40,560
sort of silly names for Python and

00:11:37,080 --> 00:11:43,980
Django Projects Zurg mode where you have

00:11:40,560 --> 00:11:46,590
like the way Zurg mode works your

00:11:43,980 --> 00:11:48,420
berserk server which is really very

00:11:46,590 --> 00:11:51,360
bare-bones all it does is it just like

00:11:48,420 --> 00:11:53,760
listens for workers and then the Zergs

00:11:51,360 --> 00:11:55,230
when they come online they communicate

00:11:53,760 --> 00:11:57,390
to the Zurich server that they are

00:11:55,230 --> 00:11:59,490
accepting requests and it's only at that

00:11:57,390 --> 00:12:01,740
point that the zerk server routes

00:11:59,490 --> 00:12:03,570
request to them so this gives them the

00:12:01,740 --> 00:12:05,160
freedom to sort of you know take as long

00:12:03,570 --> 00:12:08,940
as they need basically to get everything

00:12:05,160 --> 00:12:11,880
loaded up into memory and preload

00:12:08,940 --> 00:12:13,680
everything before it even needs to deal

00:12:11,880 --> 00:12:17,250
with the incoming requests and the

00:12:13,680 --> 00:12:19,140
Zurich server handles the queue and then

00:12:17,250 --> 00:12:22,410
before swapping the upstream one after

00:12:19,140 --> 00:12:26,130
they've been pre-loaded we warm them up

00:12:22,410 --> 00:12:29,760
with a number of simultaneous concurrent

00:12:26,130 --> 00:12:32,340
HTTP requests that basically make sure

00:12:29,760 --> 00:12:34,830
that all the workers get a request with

00:12:32,340 --> 00:12:38,550
a cache busting URL us that they're all

00:12:34,830 --> 00:12:42,180
primed so why did we switch from mod

00:12:38,550 --> 00:12:46,380
whiskey to you whiskey there's I think a

00:12:42,180 --> 00:12:49,260
lot of misinformation about whiskey

00:12:46,380 --> 00:12:50,900
servers and you know performance

00:12:49,260 --> 00:12:52,830
benchmarks and things like that I think

00:12:50,900 --> 00:12:55,560
generally the comparisons the

00:12:52,830 --> 00:12:57,360
configurations aren't very reliable and

00:12:55,560 --> 00:12:58,590
the thing being tested often doesn't

00:12:57,360 --> 00:13:00,390
apply to the real world there's

00:12:58,590 --> 00:13:03,390
something like how many concurrent hello

00:13:00,390 --> 00:13:07,050
world responses can you serve up at any

00:13:03,390 --> 00:13:08,850
given moment mod whiskey in general it's

00:13:07,050 --> 00:13:11,250
comparable for horn it's like I said I

00:13:08,850 --> 00:13:15,240
runs on an apache HTTP server which is

00:13:11,250 --> 00:13:18,180
you know whatever mostly the work of a

00:13:15,240 --> 00:13:19,980
single developer the documentation is

00:13:18,180 --> 00:13:22,290
terminally out-of-date and important

00:13:19,980 --> 00:13:23,970
configuration options are missing or

00:13:22,290 --> 00:13:25,970
only discoverable in the release notes

00:13:23,970 --> 00:13:29,310
which is something that's openly

00:13:25,970 --> 00:13:31,470
admitted in the documentation kind of

00:13:29,310 --> 00:13:34,379
like the summary of the github pages

00:13:31,470 --> 00:13:36,569
kind of demonstrates the balance between

00:13:34,379 --> 00:13:40,230
where the community's gone with Maude

00:13:36,569 --> 00:13:42,139
whisk in USD the you whiskey brought an

00:13:40,230 --> 00:13:43,769
active community Vera serve

00:13:42,139 --> 00:13:46,079
documentation in fact is kind of

00:13:43,769 --> 00:13:54,509
overwhelming but also highly

00:13:46,079 --> 00:13:56,009
configurable and so when I was

00:13:54,509 --> 00:13:58,139
rehearsing this talk there were a couple

00:13:56,009 --> 00:14:01,050
slides that I don't think I'm gonna have

00:13:58,139 --> 00:14:04,290
time to get through but I do have a

00:14:01,050 --> 00:14:07,079
github repository which has some demo

00:14:04,290 --> 00:14:10,319
code that kind of demonstrates how we're

00:14:07,079 --> 00:14:13,350
using you whiskey emperor mode and Zurg

00:14:10,319 --> 00:14:15,389
mode and the you whiskey stat server to

00:14:13,350 --> 00:14:17,720
kind of monitor our whiskey applications

00:14:15,389 --> 00:14:20,250
preload everything using a fabric script

00:14:17,720 --> 00:14:23,579
so you know I'll just sort of reference

00:14:20,250 --> 00:14:27,180
everyone refer everyone to that and sort

00:14:23,579 --> 00:14:29,879
of as a teaser kind of show what are you

00:14:27,180 --> 00:14:31,259
whisky monitoring page looks like so it

00:14:29,879 --> 00:14:33,300
looks something like this

00:14:31,259 --> 00:14:35,149
we use emperor mode emperor mode

00:14:33,300 --> 00:14:37,709
basically is a way where you can

00:14:35,149 --> 00:14:39,600
dynamically set up multiple

00:14:37,709 --> 00:14:42,360
configurations for a single code base so

00:14:39,600 --> 00:14:45,180
we have like one really large code base

00:14:42,360 --> 00:14:48,389
that hosts all the Atlantic properties

00:14:45,180 --> 00:14:49,920
citilab the wire the Atlantic comm we

00:14:48,389 --> 00:14:52,920
have a profile site which is for people

00:14:49,920 --> 00:14:54,829
to manage their print subscriptions we

00:14:52,920 --> 00:14:57,089
have a sponsored section of the site and

00:14:54,829 --> 00:14:59,699
we have an A and a B version of all

00:14:57,089 --> 00:15:02,100
those things and then also we have a CMS

00:14:59,699 --> 00:15:04,620
that's for our editors and a CMS called

00:15:02,100 --> 00:15:08,550
Waldo that's for outside video

00:15:04,620 --> 00:15:12,980
contributors so we go to this page we

00:15:08,550 --> 00:15:12,980
can see in real time as requests come in

00:15:19,250 --> 00:15:25,470
it sort of changes you can sort of hover

00:15:21,779 --> 00:15:27,360
over the little dots to see information

00:15:25,470 --> 00:15:30,959
that's pulled from the you whiskey stat

00:15:27,360 --> 00:15:32,610
server if the site starts to get

00:15:30,959 --> 00:15:34,740
overwhelmed is a little Hugh monitors

00:15:32,610 --> 00:15:36,029
down at the bottom generally if the

00:15:34,740 --> 00:15:39,089
queue fills up a little bit that's

00:15:36,029 --> 00:15:41,670
normal that's sort of expected and we

00:15:39,089 --> 00:15:45,300
have one of the things you can do with

00:15:41,670 --> 00:15:47,400
the Zerg mode is dynamically spin

00:15:45,300 --> 00:15:50,040
instances based on how busy the other

00:15:47,400 --> 00:15:52,230
worker processes that are so you know

00:15:50,040 --> 00:15:55,950
when these requests come in bonds new

00:15:52,230 --> 00:15:59,190
processes and it's able to handle the in

00:15:55,950 --> 00:16:02,520
you know the increased volume that's

00:15:59,190 --> 00:16:06,870
coming in so up here these are the URLs

00:16:02,520 --> 00:16:09,030
to the github repository I actually

00:16:06,870 --> 00:16:10,620
haven't pushed it yet but by the time

00:16:09,030 --> 00:16:13,650
this video is online it'll be there I

00:16:10,620 --> 00:16:16,110
think it'll probably be sometime later

00:16:13,650 --> 00:16:18,450
this week at Frank you don t know I

00:16:16,110 --> 00:16:20,490
don't really use Twitter much but it's

00:16:18,450 --> 00:16:23,210
an easy way to get hold of me or you can

00:16:20,490 --> 00:16:27,630
email me Frankie at the Atlantic comm

00:16:23,210 --> 00:16:28,830
and with the time I have left I'll open

00:16:27,630 --> 00:16:30,720
it up any questions

00:16:28,830 --> 00:16:32,730
great talk thank you um so first

00:16:30,720 --> 00:16:35,460
question or just the question is is

00:16:32,730 --> 00:16:39,840
basically all end-user traffic that hits

00:16:35,460 --> 00:16:41,430
the Atlantic com served like through CDN

00:16:39,840 --> 00:16:43,440
but then that CDN is basically talking

00:16:41,430 --> 00:16:45,060
to Jango across the board so is the

00:16:43,440 --> 00:16:46,050
entire site served off of Jango I guess

00:16:45,060 --> 00:16:49,260
it's the high level question

00:16:46,050 --> 00:16:51,660
yeah well mostly I mean there are a

00:16:49,260 --> 00:16:53,790
couple of really really random legacy

00:16:51,660 --> 00:16:56,850
pages that look really old and they are

00:16:53,790 --> 00:17:00,330
really old but 99% of the traffic goes

00:16:56,850 --> 00:17:03,750
through Jenga and like home page yeah

00:17:00,330 --> 00:17:06,209
and as far as the CDN is concerned so a

00:17:03,750 --> 00:17:09,150
small portion of the requests will get

00:17:06,209 --> 00:17:10,920
through the CDN you know I we generally

00:17:09,150 --> 00:17:12,630
have about a three minute page cache for

00:17:10,920 --> 00:17:14,760
every page and then we have longer ones

00:17:12,630 --> 00:17:18,510
for some you know older pages that

00:17:14,760 --> 00:17:20,250
haven't been updated in a while by most

00:17:18,510 --> 00:17:22,920
of the requests that get passed that are

00:17:20,250 --> 00:17:24,360
actually BOTS and crawlers we have an

00:17:22,920 --> 00:17:27,000
archive I mean we've been printing since

00:17:24,360 --> 00:17:28,350
1857 and we basically have everything

00:17:27,000 --> 00:17:30,810
online that we have the rights to you

00:17:28,350 --> 00:17:32,400
would think that a print magazine would

00:17:30,810 --> 00:17:33,720
have digital publishing rights to all

00:17:32,400 --> 00:17:36,540
their articles but you would be wrong

00:17:33,720 --> 00:17:38,130
that it's not the case but everything

00:17:36,540 --> 00:17:41,400
that we do have the rights to and that

00:17:38,130 --> 00:17:43,410
we've digitized is free online so you

00:17:41,400 --> 00:17:45,480
know we don't generally have an issue

00:17:43,410 --> 00:17:46,680
and people crawl our site if they're

00:17:45,480 --> 00:17:48,600
doing in good faith but occasionally

00:17:46,680 --> 00:17:51,330
there's like an AWS instance that'll

00:17:48,600 --> 00:17:54,870
make like 200 concurrent requests every

00:17:51,330 --> 00:17:56,940
second and sort of makes our servers

00:17:54,870 --> 00:17:58,830
melt and so we have to kind of scramble

00:17:56,940 --> 00:17:59,130
to block it and so a lot of this was

00:17:58,830 --> 00:18:00,540
like

00:17:59,130 --> 00:18:02,520
kind of dealing with that and getting us

00:18:00,540 --> 00:18:04,260
in a place where we could handle those

00:18:02,520 --> 00:18:04,710
sorts of situations without breaking a

00:18:04,260 --> 00:18:10,770
sweat

00:18:04,710 --> 00:18:12,810
great thank you hi so you talked about

00:18:10,770 --> 00:18:13,440
using mod whiskey and switching over to

00:18:12,810 --> 00:18:15,210
you was he

00:18:13,440 --> 00:18:16,830
there's a third option that a lot of

00:18:15,210 --> 00:18:18,450
people use which is Gianna curtain I was

00:18:16,830 --> 00:18:20,640
just wondering if you thought about

00:18:18,450 --> 00:18:22,740
unicorn didn't go with it or just didn't

00:18:20,640 --> 00:18:24,600
have time and you had seen how big you

00:18:22,740 --> 00:18:27,900
whiskey was you have an opinion on that

00:18:24,600 --> 00:18:30,570
I don't know actually I know people have

00:18:27,900 --> 00:18:31,860
had a lot of success with it I don't

00:18:30,570 --> 00:18:34,140
really have a strong opinion one way or

00:18:31,860 --> 00:18:38,220
the other just kind of us you work for

00:18:34,140 --> 00:18:39,690
us but I think I think it's it's on from

00:18:38,220 --> 00:18:41,430
what I understand gene you acquire G

00:18:39,690 --> 00:18:43,410
unicorn is on par with you whisky in

00:18:41,430 --> 00:18:45,390
terms of features and performance so you

00:18:43,410 --> 00:18:46,650
didn't you didn't like look at unicorn

00:18:45,390 --> 00:18:52,260
and decide like oh that's not gonna work

00:18:46,650 --> 00:18:54,780
for us now okay thanks okay yeah how you

00:18:52,260 --> 00:18:57,360
mentioned earlier that's you thought

00:18:54,780 --> 00:18:59,040
some arm caching was kind of a mixed bag

00:18:57,360 --> 00:18:59,490
can you go into more detail why you

00:18:59,040 --> 00:19:03,420
think so

00:18:59,490 --> 00:19:04,860
sure yeah so when we we ran Jango cash

00:19:03,420 --> 00:19:06,930
machine for a really long time on our

00:19:04,860 --> 00:19:12,060
site and then actually kind of

00:19:06,930 --> 00:19:13,050
accidentally we turned it off and and

00:19:12,060 --> 00:19:15,030
this wasn't the first time we

00:19:13,050 --> 00:19:16,440
accidentally turned off caching but in

00:19:15,030 --> 00:19:19,050
this case there was really no

00:19:16,440 --> 00:19:22,460
performance difference and we kind of

00:19:19,050 --> 00:19:25,740
speculate about why that might be and

00:19:22,460 --> 00:19:28,800
the conclusion we came to is that a lot

00:19:25,740 --> 00:19:30,390
of the like so our situation is maybe

00:19:28,800 --> 00:19:32,850
unique because we're not in the cloud

00:19:30,390 --> 00:19:34,860
we're in a data center it's all you know

00:19:32,850 --> 00:19:36,660
in Reston Virginia all like connected

00:19:34,860 --> 00:19:38,670
via fiber channels so there's not much

00:19:36,660 --> 00:19:41,370
latency between the servers and the

00:19:38,670 --> 00:19:43,260
database and the database is pretty well

00:19:41,370 --> 00:19:45,630
tuned for reads because we don't have a

00:19:43,260 --> 00:19:47,220
lot of Rights because we're you know the

00:19:45,630 --> 00:19:48,660
editors publish maybe 20 stories a day

00:19:47,220 --> 00:19:51,960
versus people reading millions of

00:19:48,660 --> 00:19:54,560
stories a day so the database is

00:19:51,960 --> 00:19:57,240
generally not a bottleneck for us and

00:19:54,560 --> 00:19:59,760
the model instantiation and hydration

00:19:57,240 --> 00:20:01,770
and basically the unpicking and creating

00:19:59,760 --> 00:20:04,170
of the model instances was taking almost

00:20:01,770 --> 00:20:07,710
as much time as if we were doing it raw

00:20:04,170 --> 00:20:09,800
o RM queries to the database so it what

00:20:07,710 --> 00:20:12,330
like really didn't make a big difference

00:20:09,800 --> 00:20:13,140
but I think that in cases where you're

00:20:12,330 --> 00:20:15,420
like on a cloud

00:20:13,140 --> 00:20:17,360
the latency between your application

00:20:15,420 --> 00:20:20,000
servers and your database servers or

00:20:17,360 --> 00:20:22,740
when you have situations where like

00:20:20,000 --> 00:20:25,320
writes and reads are more balanced then

00:20:22,740 --> 00:20:29,720
I think there might be a like I said a

00:20:25,320 --> 00:20:29,720

YouTube URL: https://www.youtube.com/watch?v=lAMlZviIPw4


