Title: DjangoCon US 2015 - Performance Testing for Modern Apps by Dustin Whittle
Publication date: 2017-11-03
Playlist: DjangoCon US 2015
Description: 
	Performance Testing for Modern Apps

The performance of your application affects your business more than you might think. Top engineering organizations think of performance not as a nice-to-have, but as a crucial feature of their product. Unfortunately, most engineering teams do not regularly test the performance and scalability of their infrastructure. Dustin Whittle shares the latest techniques and tools for performance testing modern web and mobile applications. Join this session and learn how to capacity plan and evaluate performance and the scalability of the server-side through Siege, Bees with Machine Guns, and Locust.io. We will dive into modern performance testing on the client-side and how to leverage navigation/resource timing apis and tools like Google PageSpeed and SiteSpeed.io to understand the real world performance of your users. We will cover how HTTP2 and modern browsers change the game for performance optimization with new best practices. Take back an understanding of how to automate performance and load testing and evaluate the impact it has on performance and your business.
Captions: 
	00:00:15,560 --> 00:00:19,740
alright thanks everyone for joining

00:00:17,460 --> 00:00:21,539
again this is performance testing for

00:00:19,740 --> 00:00:23,160
modern apps and we're going to be

00:00:21,539 --> 00:00:24,900
talking about how some tools of the

00:00:23,160 --> 00:00:25,949
trade for testing performance on the

00:00:24,900 --> 00:00:28,769
server side but also understanding

00:00:25,949 --> 00:00:30,269
client-side performance so I have a ton

00:00:28,769 --> 00:00:31,349
of content there's a bunch of notes at

00:00:30,269 --> 00:00:33,239
the bottom of my slides i'll make them

00:00:31,349 --> 00:00:34,590
all available online so if i go a little

00:00:33,239 --> 00:00:37,020
bit fast know that there's plenty of

00:00:34,590 --> 00:00:38,579
notes for later so the reality is that

00:00:37,020 --> 00:00:40,200
top engineering organizations they could

00:00:38,579 --> 00:00:41,820
perform it's not as a nice to have but

00:00:40,200 --> 00:00:43,320
is a critical feature of their product

00:00:41,820 --> 00:00:45,329
and it's because they understand that it

00:00:43,320 --> 00:00:47,489
has a direct impact on their business's

00:00:45,329 --> 00:00:49,530
bottom line most of the time developers

00:00:47,489 --> 00:00:51,270
really don't think of this until you go

00:00:49,530 --> 00:00:53,370
to launch and really i'm here to help

00:00:51,270 --> 00:00:55,320
change that so you can find out a little

00:00:53,370 --> 00:00:58,289
bit about me follow me on twitter at

00:00:55,320 --> 00:01:00,300
Dustin whittler Dustin widdle calm so

00:00:58,289 --> 00:01:01,559
why does performance matter Microsoft

00:01:00,300 --> 00:01:03,329
found that being searches that were two

00:01:01,559 --> 00:01:05,070
seconds solar resulted in the 4.3

00:01:03,329 --> 00:01:06,930
percent drop in revenue per user and

00:01:05,070 --> 00:01:08,460
when Mozilla shaved 2.2 seconds off

00:01:06,930 --> 00:01:11,070
their landing page experience Firefox

00:01:08,460 --> 00:01:12,630
downloads increased 15.4 percent so they

00:01:11,070 --> 00:01:15,299
got 60 million more downloads just the

00:01:12,630 --> 00:01:16,740
kids the page was a bit faster in making

00:01:15,299 --> 00:01:18,540
Barack Obama's website sixty percent

00:01:16,740 --> 00:01:20,430
faster increased donation conversions by

00:01:18,540 --> 00:01:22,740
fourteen percent but the most impressive

00:01:20,430 --> 00:01:24,689
metric that I've come across is that by

00:01:22,740 --> 00:01:27,000
decreasing the end user latency of

00:01:24,689 --> 00:01:28,680
amazon.com retail operations by 100

00:01:27,000 --> 00:01:31,439
milliseconds results in a one percent

00:01:28,680 --> 00:01:35,040
improvement in revenue so whether it's

00:01:31,439 --> 00:01:36,710
Yahoo shopzilla AOL amazon com all of

00:01:35,040 --> 00:01:38,759
these engineering companies all these

00:01:36,710 --> 00:01:40,470
engineering organizations understand

00:01:38,759 --> 00:01:43,799
that ultimately performance directly

00:01:40,470 --> 00:01:46,560
impacts the bottom line so the question

00:01:43,799 --> 00:01:48,299
is how fast is fast enough so point 1

00:01:46,560 --> 00:01:49,770
seconds it feels instantaneous it feels

00:01:48,299 --> 00:01:51,990
like you're flipping a page in a book or

00:01:49,770 --> 00:01:53,820
100 milliseconds so you should really

00:01:51,990 --> 00:01:56,250
strive to keep your load times in this

00:01:53,820 --> 00:01:58,079
range but one second allows you to think

00:01:56,250 --> 00:01:59,610
seamlessly and after 10 seconds you

00:01:58,079 --> 00:02:01,920
really start to lose the attention of

00:01:59,610 --> 00:02:03,329
your users so there's been a bunch of

00:02:01,920 --> 00:02:05,250
performance studies to understand

00:02:03,329 --> 00:02:07,469
basically the attention span and

00:02:05,250 --> 00:02:09,149
applications and user experiences and

00:02:07,469 --> 00:02:10,950
what the Rose is that performance really

00:02:09,149 --> 00:02:13,090
is key to a great user experience I

00:02:10,950 --> 00:02:14,560
think everyone's probably had the

00:02:13,090 --> 00:02:15,970
experience where you go to check out in

00:02:14,560 --> 00:02:17,590
the e-commerce store and you click the

00:02:15,970 --> 00:02:19,390
checkout button and then it just waits

00:02:17,590 --> 00:02:21,580
for a long time and you really quickly

00:02:19,390 --> 00:02:23,140
start to lose faith I think the engineer

00:02:21,580 --> 00:02:26,800
and all the snows to just wait it out or

00:02:23,140 --> 00:02:29,500
going to get charged again so again how

00:02:26,800 --> 00:02:31,150
fast is fast enough 100 milliseconds is

00:02:29,500 --> 00:02:32,620
again instantaneous it feels like

00:02:31,150 --> 00:02:34,599
flipping a page in a book 100

00:02:32,620 --> 00:02:36,310
milliseconds at 300 milliseconds the

00:02:34,599 --> 00:02:38,019
delay is perceptible so your users are

00:02:36,310 --> 00:02:39,790
going to start to notice and after about

00:02:38,019 --> 00:02:43,090
one second you really start to interrupt

00:02:39,790 --> 00:02:44,860
the users flow so users expect a site to

00:02:43,090 --> 00:02:46,390
load in two seconds and after three

00:02:44,860 --> 00:02:47,950
seconds forty percent will bend in your

00:02:46,390 --> 00:02:49,780
site and this comes from a nielsen

00:02:47,950 --> 00:02:51,549
performance survey that they did a long

00:02:49,780 --> 00:02:55,510
time ago and the realities that mobile

00:02:51,549 --> 00:02:56,829
applications are even worse this is

00:02:55,510 --> 00:02:58,720
really hard to do because modern

00:02:56,829 --> 00:03:00,519
applications are really complex when you

00:02:58,720 --> 00:03:02,230
have 100 microservices talking to each

00:03:00,519 --> 00:03:03,940
other and you're making calls to

00:03:02,230 --> 00:03:05,470
external providers a shipping provider

00:03:03,940 --> 00:03:07,299
payment processing provider fraud

00:03:05,470 --> 00:03:09,640
detection provider it's really hard to

00:03:07,299 --> 00:03:11,590
have great performance so with

00:03:09,640 --> 00:03:12,910
application complex the exploding how do

00:03:11,590 --> 00:03:16,840
you manage this and how do you test for

00:03:12,910 --> 00:03:18,670
this I think most companies treat

00:03:16,840 --> 00:03:20,380
performance and really up time as a

00:03:18,670 --> 00:03:21,430
critical differentiator and if you look

00:03:20,380 --> 00:03:23,560
at some of the major enterprise

00:03:21,430 --> 00:03:24,940
companies they all treat up time as a

00:03:23,560 --> 00:03:26,769
critical metric and I think we've all

00:03:24,940 --> 00:03:28,299
encountered this if you provide a

00:03:26,769 --> 00:03:31,540
service to others it's enforced by an

00:03:28,299 --> 00:03:33,459
SLA so what's the goal here the goals

00:03:31,540 --> 00:03:34,569
treat performance as a feature and we're

00:03:33,459 --> 00:03:36,010
going to talk about some tools of the

00:03:34,569 --> 00:03:38,319
trade for performance testing to do

00:03:36,010 --> 00:03:40,120
exactly that so the first day I like to

00:03:38,319 --> 00:03:43,150
start with is how to understand your

00:03:40,120 --> 00:03:46,019
baseline performance so just rock python

00:03:43,150 --> 00:03:48,489
is going to be pretty cool pretty fast

00:03:46,019 --> 00:03:49,810
when you start out django you have a

00:03:48,489 --> 00:03:51,099
framework layer that's going to add a

00:03:49,810 --> 00:03:52,780
bit of overhead and then you're going to

00:03:51,099 --> 00:03:54,609
have your application stack that's going

00:03:52,780 --> 00:03:55,870
to add a bit more of overhead really

00:03:54,609 --> 00:03:57,340
what you want to understand is what's

00:03:55,870 --> 00:03:58,959
your baseline performance on some

00:03:57,340 --> 00:04:00,160
hardware on the specific set of hardware

00:03:58,959 --> 00:04:02,650
that you're going to run in production

00:04:00,160 --> 00:04:04,540
what's the performance of a static asset

00:04:02,650 --> 00:04:07,030
being served not from Python what's this

00:04:04,540 --> 00:04:09,069
what's the performance of Python hello

00:04:07,030 --> 00:04:10,720
world so a very simple script what's the

00:04:09,069 --> 00:04:13,120
overhead of your framework and then

00:04:10,720 --> 00:04:15,160
what's your actual application because

00:04:13,120 --> 00:04:16,299
oftentimes what you'll find is that the

00:04:15,160 --> 00:04:18,010
business transactions and your

00:04:16,299 --> 00:04:19,539
applications or have very different

00:04:18,010 --> 00:04:21,130
performance the home page is going to be

00:04:19,539 --> 00:04:22,360
highly cash whereas the checkout process

00:04:21,130 --> 00:04:24,130
is going to talk to a bunch of

00:04:22,360 --> 00:04:25,840
third-party providers so it's going to

00:04:24,130 --> 00:04:26,800
be inherently slower so you should

00:04:25,840 --> 00:04:28,539
understand the performance

00:04:26,800 --> 00:04:30,879
each one of these transactions and

00:04:28,539 --> 00:04:32,560
really understand how that affects your

00:04:30,879 --> 00:04:34,330
users so I like to do that by

00:04:32,560 --> 00:04:36,550
understanding what the static threshold

00:04:34,330 --> 00:04:39,129
is with the hello world baseline is and

00:04:36,550 --> 00:04:40,479
what the application benchmark is so if

00:04:39,129 --> 00:04:42,310
you've ever installed Apache you've

00:04:40,479 --> 00:04:44,470
probably backs s2 Apache vents if not

00:04:42,310 --> 00:04:46,389
you can apt-get install apache2 utils on

00:04:44,470 --> 00:04:48,400
most Linux platforms and it becomes

00:04:46,389 --> 00:04:50,379
available Apache bench is a very simple

00:04:48,400 --> 00:04:52,330
tool for benchmarking the performance of

00:04:50,379 --> 00:04:53,889
applications this isn't specific to

00:04:52,330 --> 00:04:55,419
django this isn't specifically Python

00:04:53,889 --> 00:04:57,900
you can use most of these tools across

00:04:55,419 --> 00:05:00,340
any different app application platform

00:04:57,900 --> 00:05:01,539
so Apache bench is really crude and dead

00:05:00,340 --> 00:05:03,520
simple so if you just want to get an

00:05:01,539 --> 00:05:05,500
idea of how fast the particular

00:05:03,520 --> 00:05:06,789
transaction is like your home page it's

00:05:05,500 --> 00:05:08,530
really easy to test the single

00:05:06,789 --> 00:05:10,690
concurrent user so in this case what

00:05:08,530 --> 00:05:12,759
you're seeing is Apache bench Dashti for

00:05:10,690 --> 00:05:15,159
concurrency so I want to test one user

00:05:12,759 --> 00:05:17,979
going as fast as possible for 10 seconds

00:05:15,159 --> 00:05:20,229
to acne demo app com so we'll run apache

00:05:17,979 --> 00:05:21,940
bench dash to concurrency of one for a

00:05:20,229 --> 00:05:23,560
time of 10 seconds and dash k's

00:05:21,940 --> 00:05:25,629
benchmark mode so I want no delay

00:05:23,560 --> 00:05:27,490
between the requests and which will

00:05:25,629 --> 00:05:29,830
start to get is a response that looks

00:05:27,490 --> 00:05:31,990
like this so you'll get the out requests

00:05:29,830 --> 00:05:33,250
per second which is a use very useful

00:05:31,990 --> 00:05:35,259
metric but more importantly is the

00:05:33,250 --> 00:05:38,380
latency so what's the average response

00:05:35,259 --> 00:05:40,150
time per transaction and the balance to

00:05:38,380 --> 00:05:41,529
figuring out how much capacity or

00:05:40,150 --> 00:05:43,360
infrastructure can support is

00:05:41,529 --> 00:05:45,490
understanding when you max out your

00:05:43,360 --> 00:05:47,650
requests per second and the latency

00:05:45,490 --> 00:05:49,630
starts to rise so you can always serve

00:05:47,650 --> 00:05:50,889
more transactions just very slowly but

00:05:49,630 --> 00:05:52,870
you don't want your users waiting ten

00:05:50,889 --> 00:05:54,430
seconds just because there's a thousand

00:05:52,870 --> 00:05:55,860
of them shutting up so you really need

00:05:54,430 --> 00:05:58,029
to understand the balance of this and

00:05:55,860 --> 00:06:00,099
with Apache bench it's really easy to

00:05:58,029 --> 00:06:01,509
start to increase the concurrency so in

00:06:00,099 --> 00:06:03,849
this case I'm going to start testing 10

00:06:01,509 --> 00:06:05,919
users so we'll just change the

00:06:03,849 --> 00:06:08,050
concurrency level here to 10 and test

00:06:05,919 --> 00:06:10,659
again for 10 seconds and what you'll see

00:06:08,050 --> 00:06:12,460
is that we have more requests per second

00:06:10,659 --> 00:06:14,800
in this case we have 65 requests per

00:06:12,460 --> 00:06:18,340
second but the time per request has gone

00:06:14,800 --> 00:06:20,349
up to an average of 151 milliseconds now

00:06:18,340 --> 00:06:22,300
Apache bench is great it's a quick and

00:06:20,349 --> 00:06:25,240
dirty it makes it very easy to load test

00:06:22,300 --> 00:06:28,900
a server and but i prefer siege so see

00:06:25,240 --> 00:06:31,719
jaws ave a similar format if you can app

00:06:28,900 --> 00:06:33,370
install seeds on most platforms or port

00:06:31,719 --> 00:06:35,770
and brew install seeds it's pretty

00:06:33,370 --> 00:06:38,229
straightforward and it has a very

00:06:35,770 --> 00:06:40,630
similar format so you can run siege dash

00:06:38,229 --> 00:06:43,840
C again concurrency of 10 users

00:06:40,630 --> 00:06:45,310
and for a time of 10 seconds now what

00:06:43,840 --> 00:06:47,350
you'll notice is in these examples I'm

00:06:45,310 --> 00:06:49,240
load testing one endpoint I'm only load

00:06:47,350 --> 00:06:51,010
testing the home page but that's only so

00:06:49,240 --> 00:06:53,440
useful and we'll get metrics that were

00:06:51,010 --> 00:06:55,000
very similar to Apache bench so for ten

00:06:53,440 --> 00:06:57,400
concurrent users we get about 65

00:06:55,000 --> 00:06:59,740
transactions a second and the average

00:06:57,400 --> 00:07:04,660
response time is about 30 milliseconds I

00:06:59,740 --> 00:07:06,180
think our 150 milliseconds rather so you

00:07:04,660 --> 00:07:08,860
can keep increasing the concurrency

00:07:06,180 --> 00:07:10,540
until you max it out and really well

00:07:08,860 --> 00:07:12,190
until you can start to see the latency

00:07:10,540 --> 00:07:13,330
start to skyrocket really what you want

00:07:12,190 --> 00:07:16,570
to understand is what's the maximum

00:07:13,330 --> 00:07:19,000
request per per second that the machine

00:07:16,570 --> 00:07:21,850
can get well before the average response

00:07:19,000 --> 00:07:23,740
time starts to increase now this is fine

00:07:21,850 --> 00:07:25,240
for a very simple application but most

00:07:23,740 --> 00:07:27,070
of the time applications aren't one

00:07:25,240 --> 00:07:29,080
endpoint its many different endpoints

00:07:27,070 --> 00:07:31,330
you have functionality to you have the

00:07:29,080 --> 00:07:33,790
home page you have login logout add to

00:07:31,330 --> 00:07:36,760
your cart and checkout process in order

00:07:33,790 --> 00:07:38,530
etc so there's a quick tip to be able to

00:07:36,760 --> 00:07:40,480
call the entire application to discover

00:07:38,530 --> 00:07:42,250
all the URL endpoints because most of us

00:07:40,480 --> 00:07:43,750
inherit applications and we don't have

00:07:42,250 --> 00:07:45,730
the ability to just build them from

00:07:43,750 --> 00:07:47,610
scratch every time so you often don't

00:07:45,730 --> 00:07:50,170
know all the functionality that exists

00:07:47,610 --> 00:07:53,050
so there's a tool called s proxy which

00:07:50,170 --> 00:07:54,370
is a transparent HTTP proxy basically

00:07:53,050 --> 00:07:56,740
what it allows you to do is make a

00:07:54,370 --> 00:07:59,020
request through a proxy and will love

00:07:56,740 --> 00:08:00,700
the URL that you're requesting so if you

00:07:59,020 --> 00:08:02,320
want to interact with an application and

00:08:00,700 --> 00:08:04,810
you want to crawl all the endpoints in

00:08:02,320 --> 00:08:08,140
the application it's very easy to use s

00:08:04,810 --> 00:08:10,120
proxy and W get to basically emulate a

00:08:08,140 --> 00:08:13,300
search engine spider and the goal here

00:08:10,120 --> 00:08:15,130
is that we want to find all the URLs of

00:08:13,300 --> 00:08:16,960
the application so it starts with s

00:08:15,130 --> 00:08:19,840
proxied a show and all it's going to do

00:08:16,960 --> 00:08:21,670
is run an HTTP proxy on port 9000 one

00:08:19,840 --> 00:08:25,900
and all the URLs that we access are

00:08:21,670 --> 00:08:27,130
going to be put into a URLs txt file the

00:08:25,900 --> 00:08:29,680
next thing we're going to do is use W

00:08:27,130 --> 00:08:31,390
get so W get has a spider mode that

00:08:29,680 --> 00:08:33,310
allows you to emulate a search engine

00:08:31,390 --> 00:08:35,560
spider and we'll go to the home page and

00:08:33,310 --> 00:08:36,669
recursively crawl all of the links so if

00:08:35,560 --> 00:08:38,110
you really want a quick and easy way to

00:08:36,669 --> 00:08:40,690
discover all the functionality of the

00:08:38,110 --> 00:08:42,700
application you can run s proxy and W

00:08:40,690 --> 00:08:44,680
get and discover all the public URLs in

00:08:42,700 --> 00:08:46,330
your application and what you end up

00:08:44,680 --> 00:08:48,520
with will sort the list so we have a

00:08:46,330 --> 00:08:49,540
unique list of URLs at the end and you

00:08:48,520 --> 00:08:51,339
end up with something that looks like

00:08:49,540 --> 00:08:52,720
this this is just a simple ecommerce

00:08:51,339 --> 00:08:54,250
application that i'm using but there's

00:08:52,720 --> 00:08:56,350
an about page of you cards

00:08:54,250 --> 00:08:58,870
change the currency login register for

00:08:56,350 --> 00:09:01,600
an account view by category view by tag

00:08:58,870 --> 00:09:03,190
etc and we're really what we want to

00:09:01,600 --> 00:09:04,930
understand is what is the how do we

00:09:03,190 --> 00:09:06,820
benchmark traffic across all the unique

00:09:04,930 --> 00:09:08,980
URLs so that we can understand the

00:09:06,820 --> 00:09:11,410
performance of each transaction because

00:09:08,980 --> 00:09:13,180
the processing an order is going to be a

00:09:11,410 --> 00:09:14,590
lot slower than going to the home page

00:09:13,180 --> 00:09:17,170
especially when the home page is highly

00:09:14,590 --> 00:09:18,820
cached so you can use seeds again this

00:09:17,170 --> 00:09:20,800
time feeding in the list of the URLs

00:09:18,820 --> 00:09:24,670
text file so that we can benchmark

00:09:20,800 --> 00:09:25,690
traffic across all the unique URLs so

00:09:24,670 --> 00:09:27,550
we're going to run the same command that

00:09:25,690 --> 00:09:30,130
we ran earlier so this case it's going

00:09:27,550 --> 00:09:31,750
to besieged ashby for verbose dashes

00:09:30,130 --> 00:09:33,250
concurrency again we want to run 50

00:09:31,750 --> 00:09:35,020
concurrent users for three minutes

00:09:33,250 --> 00:09:38,740
across all the different URLs in the

00:09:35,020 --> 00:09:40,060
URLs text file and what we'll end up

00:09:38,740 --> 00:09:41,230
with is something pretty straight

00:09:40,060 --> 00:09:42,430
forward to the output that we had

00:09:41,230 --> 00:09:44,050
earlier which is the average response

00:09:42,430 --> 00:09:47,500
time for each one of these transactions

00:09:44,050 --> 00:09:49,240
and the transaction rate now this is

00:09:47,500 --> 00:09:50,950
only so useful because often times you

00:09:49,240 --> 00:09:52,570
don't want to just go through a list of

00:09:50,950 --> 00:09:55,270
URLs you actually want to perform a

00:09:52,570 --> 00:09:57,520
transaction in that case let's talk

00:09:55,270 --> 00:09:59,110
about once I recognize so I'm a big fan

00:09:57,520 --> 00:10:00,339
of multi recognized it's an open source

00:09:59,110 --> 00:10:02,140
framework for performance and load

00:10:00,339 --> 00:10:03,910
testing but what makes it really useful

00:10:02,140 --> 00:10:05,740
as it allows you to script transaction

00:10:03,910 --> 00:10:08,020
so I can actually post these credentials

00:10:05,740 --> 00:10:10,120
to the login form all maintained state

00:10:08,020 --> 00:10:12,010
by got creating a cookie jar maintaining

00:10:10,120 --> 00:10:13,750
the HTTP session along the way so I can

00:10:12,010 --> 00:10:15,370
log in then I can add something to the

00:10:13,750 --> 00:10:17,680
car and then I can check out and I can

00:10:15,370 --> 00:10:19,600
actually manage the entire HTTP

00:10:17,680 --> 00:10:21,430
transaction or a set of transactions and

00:10:19,600 --> 00:10:25,510
then I can start to load test those as

00:10:21,430 --> 00:10:27,160
well so it's very easy to install its

00:10:25,510 --> 00:10:30,070
python tool so you can simply pip

00:10:27,160 --> 00:10:31,720
install multi mechanize and then you'll

00:10:30,070 --> 00:10:33,400
have access to a command-line library

00:10:31,720 --> 00:10:36,190
and you can just bootstrap a new project

00:10:33,400 --> 00:10:37,510
pretty straightforward now I presume

00:10:36,190 --> 00:10:38,650
everyone here is pretty familiar with

00:10:37,510 --> 00:10:40,030
Python so this should be pretty

00:10:38,650 --> 00:10:41,770
straightforward the idea here is we're

00:10:40,030 --> 00:10:43,930
going to import the HTP request library

00:10:41,770 --> 00:10:45,520
so it's an HTP client for Python and

00:10:43,930 --> 00:10:47,710
then the only thing that multi

00:10:45,520 --> 00:10:49,330
mechanized provides is a transaction

00:10:47,710 --> 00:10:51,160
class and what you do inside of the

00:10:49,330 --> 00:10:52,480
transaction class it doesn't matter it's

00:10:51,160 --> 00:10:53,800
just going to run many of these

00:10:52,480 --> 00:10:55,870
transactions at a high level of

00:10:53,800 --> 00:10:57,580
concurrency so I can do this for a

00:10:55,870 --> 00:10:59,440
single URL and point where I can run a

00:10:57,580 --> 00:11:00,790
series of year old so well this is going

00:10:59,440 --> 00:11:03,220
to be the same example that we just ran

00:11:00,790 --> 00:11:05,230
earlier with siege or patchy bench well

00:11:03,220 --> 00:11:05,990
I'll just want to load test acne demo

00:11:05,230 --> 00:11:08,540
app com

00:11:05,990 --> 00:11:10,670
as a single endpoint so here we'll

00:11:08,540 --> 00:11:13,370
import the requests library will define

00:11:10,670 --> 00:11:15,860
a try the method run and then we'll

00:11:13,370 --> 00:11:18,470
simply make a get request to acne dental

00:11:15,860 --> 00:11:19,730
appt calm but oftentimes you want to do

00:11:18,470 --> 00:11:22,040
much more than that you actually want to

00:11:19,730 --> 00:11:24,830
script a user scenario so I actually

00:11:22,040 --> 00:11:27,260
want to go login to acne dental appt

00:11:24,830 --> 00:11:30,440
calm go to the cart page and then start

00:11:27,260 --> 00:11:32,690
to script this out so here you can

00:11:30,440 --> 00:11:34,100
import multi mechanize again the only

00:11:32,690 --> 00:11:37,100
thing that matters is what you put

00:11:34,100 --> 00:11:38,899
inside of the run method so in this case

00:11:37,100 --> 00:11:40,430
we have a transaction and we just want

00:11:38,899 --> 00:11:42,529
to run this transaction out high levels

00:11:40,430 --> 00:11:44,660
of concurrency so here we instantiate

00:11:42,529 --> 00:11:46,160
the multi mechan- browser this is what's

00:11:44,660 --> 00:11:48,740
going to allow us to emulate the HTTP

00:11:46,160 --> 00:11:50,330
state it maintain a cookie jar and we're

00:11:48,740 --> 00:11:53,029
going to disable the robot so it doesn't

00:11:50,330 --> 00:11:55,220
block public endpoints by using the

00:11:53,029 --> 00:11:57,170
robots exclude and then where it has a

00:11:55,220 --> 00:11:58,850
custom timers API so this is where it

00:11:57,170 --> 00:12:01,160
gets really useful because based on the

00:11:58,850 --> 00:12:02,660
custom timers it will generate fancy

00:12:01,160 --> 00:12:04,399
graphs that allow you to understand for

00:12:02,660 --> 00:12:06,980
each one of these transactions the

00:12:04,399 --> 00:12:09,350
average response time and the number of

00:12:06,980 --> 00:12:10,970
requests per second so for each one of

00:12:09,350 --> 00:12:12,230
these will create a custom timer and

00:12:10,970 --> 00:12:13,730
will create a custom timer for the home

00:12:12,230 --> 00:12:15,829
page and the custom timer for the card

00:12:13,730 --> 00:12:17,060
functionality and obviously you can

00:12:15,829 --> 00:12:20,480
script this for the specifics of your

00:12:17,060 --> 00:12:22,760
application it becomes quite easy to to

00:12:20,480 --> 00:12:25,070
script complex transactions so here we

00:12:22,760 --> 00:12:27,320
create start time and we're going to use

00:12:25,070 --> 00:12:29,089
the cookie jar to open acne demo app com

00:12:27,320 --> 00:12:30,649
we're going to read the response we're

00:12:29,089 --> 00:12:32,000
going to record the latency so the start

00:12:30,649 --> 00:12:33,440
time minus the end time gives us our

00:12:32,000 --> 00:12:36,770
latency and then we're going to add that

00:12:33,440 --> 00:12:37,790
to the list of custom timers and then

00:12:36,770 --> 00:12:39,950
we're going to be the same thing but

00:12:37,790 --> 00:12:42,620
gota fetching the cart we could very

00:12:39,950 --> 00:12:44,510
easily do a post post the login fetch a

00:12:42,620 --> 00:12:46,700
bunch of products by their tag add those

00:12:44,510 --> 00:12:49,670
all those products into the cart and

00:12:46,700 --> 00:12:53,750
then process an order but that doesn't

00:12:49,670 --> 00:12:54,950
fit on a slide so once you have the

00:12:53,750 --> 00:12:56,660
script you can create as many of these

00:12:54,950 --> 00:12:58,220
scenarios as you want so this is just

00:12:56,660 --> 00:13:00,770
one transaction so I could have a home

00:12:58,220 --> 00:13:02,540
page transaction I could have a buy buy

00:13:00,770 --> 00:13:04,430
stuff from a shopping cart transaction

00:13:02,540 --> 00:13:06,260
philo support request transaction in all

00:13:04,430 --> 00:13:08,300
these different user scenarios and then

00:13:06,260 --> 00:13:10,579
once I want to run these scenarios I can

00:13:08,300 --> 00:13:11,810
use the multi mechanized command line to

00:13:10,579 --> 00:13:14,690
run these at different levels of

00:13:11,810 --> 00:13:16,940
concurrency so here we'll simply run

00:13:14,690 --> 00:13:19,010
multi neck run demo so this files called

00:13:16,940 --> 00:13:19,779
demo pi and it just as the transaction

00:13:19,010 --> 00:13:24,100
class inside of it

00:13:19,779 --> 00:13:25,360
and once you run that it will run to

00:13:24,100 --> 00:13:27,370
whatever level of concurrency you've

00:13:25,360 --> 00:13:29,259
configured and then we'll generate a

00:13:27,370 --> 00:13:31,540
list of output that looks like this for

00:13:29,259 --> 00:13:33,970
each one of the custom timers you'll get

00:13:31,540 --> 00:13:35,560
the response time plotted else so the

00:13:33,970 --> 00:13:38,620
average response time versus the

00:13:35,560 --> 00:13:41,439
requests per second now by a show of

00:13:38,620 --> 00:13:43,480
hands so this is pretty easy for any

00:13:41,439 --> 00:13:44,529
scripted transaction but what you'll

00:13:43,480 --> 00:13:47,680
notice is that you're running all these

00:13:44,529 --> 00:13:49,779
transactions off of a single server by

00:13:47,680 --> 00:13:51,610
show of hands how many of you have a

00:13:49,779 --> 00:13:55,660
real production application that's more

00:13:51,610 --> 00:14:00,129
than one machine right how many of you

00:13:55,660 --> 00:14:01,689
live in the cloud okay so we're going to

00:14:00,129 --> 00:14:05,649
be talking about my favorite open source

00:14:01,689 --> 00:14:07,480
project bees with machine guns big fan

00:14:05,649 --> 00:14:08,829
of talking about this one one because it

00:14:07,480 --> 00:14:10,449
comes with a notice that says it's a

00:14:08,829 --> 00:14:12,009
felony to do this against any other site

00:14:10,449 --> 00:14:13,120
except for your own because effectively

00:14:12,009 --> 00:14:14,949
it's a distributed denial-of-service

00:14:13,120 --> 00:14:17,709
attack it's brought to you by the fine

00:14:14,949 --> 00:14:20,199
folks at the Chicago Tribune so what is

00:14:17,709 --> 00:14:22,360
bees with machine guns its utility for

00:14:20,199 --> 00:14:24,490
arming mini bees to attack targets we're

00:14:22,360 --> 00:14:26,920
creating micro ec2 instances and the

00:14:24,490 --> 00:14:28,899
Amazon Web Services to load test web

00:14:26,920 --> 00:14:30,939
applications so when you look behind a

00:14:28,899 --> 00:14:32,709
load balancer and you have hundreds of

00:14:30,939 --> 00:14:34,089
hundreds of machines behind a load

00:14:32,709 --> 00:14:36,100
balancer how do you generate enough

00:14:34,089 --> 00:14:37,300
traffic to actually test that you're not

00:14:36,100 --> 00:14:38,889
going to do that off a single machine

00:14:37,300 --> 00:14:40,300
you're not going to do after your

00:14:38,889 --> 00:14:42,279
macbook you're not even to do that often

00:14:40,300 --> 00:14:43,899
ec2 instance you'll max out your network

00:14:42,279 --> 00:14:45,970
throughput before you can generate

00:14:43,899 --> 00:14:47,410
enough load so the idea of bees with

00:14:45,970 --> 00:14:50,709
machine guns is allows you to run a

00:14:47,410 --> 00:14:54,370
distributed load test and it's very easy

00:14:50,709 --> 00:15:01,899
to use so again Python so simply pip

00:14:54,370 --> 00:15:05,199
install these with machine guns no

00:15:01,899 --> 00:15:07,569
reason this can't be fun folks and then

00:15:05,199 --> 00:15:08,649
he uses the AWS photo library so if

00:15:07,569 --> 00:15:11,259
you're not familiar with amazon web

00:15:08,649 --> 00:15:13,199
services you can go to AWS amazon com /

00:15:11,259 --> 00:15:15,819
free and get two free years worth of

00:15:13,199 --> 00:15:17,259
computing power and then all you need to

00:15:15,819 --> 00:15:20,499
do is configure your access key and

00:15:17,259 --> 00:15:23,410
secret here and that'll give the boat o

00:15:20,499 --> 00:15:24,699
library could AWS credentials so bees

00:15:23,410 --> 00:15:27,220
with machine guns relies on the boto

00:15:24,699 --> 00:15:29,439
library to spin up ec2 instances once

00:15:27,220 --> 00:15:31,089
you configure this you're good to go and

00:15:29,439 --> 00:15:33,980
you can pick whichever region you want

00:15:31,089 --> 00:15:36,570
in this case I'm running it off the west

00:15:33,980 --> 00:15:38,040
now you can spin up to machines or you

00:15:36,570 --> 00:15:39,450
can spend up to hundred it just depends

00:15:38,040 --> 00:15:41,280
on the level concurrency that you want

00:15:39,450 --> 00:15:42,570
to generate so in this case it looks

00:15:41,280 --> 00:15:44,160
pretty fancy but the only thing that

00:15:42,570 --> 00:15:47,040
really matters here is it called bees up

00:15:44,160 --> 00:15:48,360
s to so I want to spin up two servers so

00:15:47,040 --> 00:15:50,040
I can run a distributed load test

00:15:48,360 --> 00:15:51,180
there's some other stuff in here

00:15:50,040 --> 00:15:52,920
basically I want to be in the default

00:15:51,180 --> 00:15:54,720
security or group i want to spend up

00:15:52,920 --> 00:15:56,790
these two servers in the u.s. west to be

00:15:54,720 --> 00:15:59,430
data center I want to use this am I

00:15:56,790 --> 00:16:01,440
instanceid and these SSH keys and

00:15:59,430 --> 00:16:02,970
username but all that other stuff is

00:16:01,440 --> 00:16:05,160
irrelevant which you really need to know

00:16:02,970 --> 00:16:07,980
is bees up dash s2 so I want to spend up

00:16:05,160 --> 00:16:09,480
to servers to start load testing these

00:16:07,980 --> 00:16:10,410
people have a great sense of humor so

00:16:09,480 --> 00:16:11,970
they're going to connect to the hive

00:16:10,410 --> 00:16:13,140
attempt to call up to B's they're going

00:16:11,970 --> 00:16:14,310
to wait for the beads to load their

00:16:13,140 --> 00:16:15,990
machine guns and then they're going to

00:16:14,310 --> 00:16:18,960
be ready for attack the swarm has

00:16:15,990 --> 00:16:20,400
assembled to bees and then you can check

00:16:18,960 --> 00:16:21,720
how many machines are actually up and

00:16:20,400 --> 00:16:23,820
running and ready to be used so you can

00:16:21,720 --> 00:16:26,010
call bees report and I'll tell you that

00:16:23,820 --> 00:16:28,710
you have two bees from the roster and

00:16:26,010 --> 00:16:30,330
the IP addresses so it's we're going to

00:16:28,710 --> 00:16:32,430
start the load test with just a thousand

00:16:30,330 --> 00:16:34,380
requests so in this case we want to load

00:16:32,430 --> 00:16:36,780
test the single endpoint so we'll call

00:16:34,380 --> 00:16:38,190
bees attack and then the end the number

00:16:36,780 --> 00:16:40,500
of requests I want to fire a thousand

00:16:38,190 --> 00:16:42,990
requests with the concurrency level 50

00:16:40,500 --> 00:16:45,510
at a time against this URL end point so

00:16:42,990 --> 00:16:47,520
I want to basically throw 50 concurrent

00:16:45,510 --> 00:16:50,880
users until I hit a thousand requests at

00:16:47,520 --> 00:16:52,620
acme demo app com so something goes

00:16:50,880 --> 00:16:54,960
somewhere that we do it ceej just loathe

00:16:52,620 --> 00:16:56,250
a bit more concurrency and then we'll

00:16:54,960 --> 00:16:57,600
get this response so it's going to read

00:16:56,250 --> 00:16:59,370
to bees from the roster connect to the

00:16:57,600 --> 00:17:01,560
high of assemble bees each of the two

00:16:59,370 --> 00:17:03,990
bees will 50 500 rounds 25 at a time

00:17:01,560 --> 00:17:05,460
seeing you all so we cash for our attack

00:17:03,990 --> 00:17:08,610
so whenever you use a framework like

00:17:05,460 --> 00:17:10,110
genuine ever use there are many web

00:17:08,610 --> 00:17:11,610
application sucks you should prime the

00:17:10,110 --> 00:17:13,770
cash because you're going to do a lot of

00:17:11,610 --> 00:17:15,000
things on the first request that you you

00:17:13,770 --> 00:17:16,140
want to when you're doing a load test

00:17:15,000 --> 00:17:17,700
you want to understand the real world

00:17:16,140 --> 00:17:19,620
performance so you want to have one cast

00:17:17,700 --> 00:17:20,970
request that gets fired and bees with

00:17:19,620 --> 00:17:23,490
machine guns will automatically do this

00:17:20,970 --> 00:17:25,590
so they'll create one you or one request

00:17:23,490 --> 00:17:27,900
that URL to prime the cash and then it

00:17:25,590 --> 00:17:30,350
will start load testing and then you'll

00:17:27,900 --> 00:17:32,660
see the requested for a second act 50-50

00:17:30,350 --> 00:17:35,730
concurrent requests for a second we get

00:17:32,660 --> 00:17:38,850
306 requests per second with an average

00:17:35,730 --> 00:17:40,530
time of 163 milliseconds so here again

00:17:38,850 --> 00:17:42,240
the goals that we want to increase the

00:17:40,530 --> 00:17:44,190
level of concurrency until we start to

00:17:42,240 --> 00:17:44,460
see the latency start to spike and that

00:17:44,190 --> 00:17:46,410
will

00:17:44,460 --> 00:17:49,620
us with the this particular hardware

00:17:46,410 --> 00:17:51,680
running this application can support but

00:17:49,620 --> 00:17:55,470
you can very easily increase this to

00:17:51,680 --> 00:17:57,300
10,000 or 100,000 or 500,000 very easily

00:17:55,470 --> 00:17:59,100
so in this case we're just going to

00:17:57,300 --> 00:18:00,810
crank up the concurrency to a thousand

00:17:59,100 --> 00:18:03,000
requests a second and we'll call this

00:18:00,810 --> 00:18:04,740
again so bees attack this time I want to

00:18:03,000 --> 00:18:07,890
fire off a hundred thousand requests a

00:18:04,740 --> 00:18:10,320
thousand at a time at acne demo app com

00:18:07,890 --> 00:18:12,210
so bees attacked a shed in 100,000 g a

00:18:10,320 --> 00:18:14,310
thousand so concurrency of a thousand

00:18:12,210 --> 00:18:15,810
and this time we're going to do the same

00:18:14,310 --> 00:18:17,790
thing and we will find is that the

00:18:15,810 --> 00:18:19,200
requests per second now we support 502

00:18:17,790 --> 00:18:20,760
request for a second but the latency

00:18:19,200 --> 00:18:23,310
start to spike so now the requests are

00:18:20,760 --> 00:18:24,870
averaging out at 360 milliseconds so

00:18:23,310 --> 00:18:26,610
that just means that yes while we you're

00:18:24,870 --> 00:18:28,470
not going to fail over the users are

00:18:26,610 --> 00:18:29,550
going to wait longer so you decide how

00:18:28,470 --> 00:18:31,170
much money you want to spend and how

00:18:29,550 --> 00:18:33,420
much the business can support on the

00:18:31,170 --> 00:18:35,460
threshold there but ultimately you want

00:18:33,420 --> 00:18:37,200
to be able to start to add hardware when

00:18:35,460 --> 00:18:38,550
the latency starts to spike especially

00:18:37,200 --> 00:18:41,910
this is going to be sustained normal

00:18:38,550 --> 00:18:44,220
workload again they have a great sense

00:18:41,910 --> 00:18:45,630
of humor so sometimes things don't

00:18:44,220 --> 00:18:48,180
always work out each of the two bees

00:18:45,630 --> 00:18:52,820
will fire over 550,000 rounds 500 at a

00:18:48,180 --> 00:18:55,380
time once the offense is completes

00:18:52,820 --> 00:18:57,930
actually we missed it the target crush

00:18:55,380 --> 00:19:00,930
to be offensive so the server survived

00:18:57,930 --> 00:19:03,000
basically and if things don't go so well

00:19:00,930 --> 00:19:04,770
no bees completed the mission apparently

00:19:03,000 --> 00:19:05,940
a bezoar peace-loving hippies so the

00:19:04,770 --> 00:19:09,450
storm is going to wait some new orders

00:19:05,940 --> 00:19:11,070
and then the benefit of the cloud is

00:19:09,450 --> 00:19:13,650
that costs scale linearly with demand so

00:19:11,070 --> 00:19:15,600
you only pay for what you use so you can

00:19:13,650 --> 00:19:17,430
call bees down and we'll tear out the

00:19:15,600 --> 00:19:19,590
two instances you just spun up so if you

00:19:17,430 --> 00:19:22,020
really want to easily run a production

00:19:19,590 --> 00:19:25,380
load test you can spin up a couple of

00:19:22,020 --> 00:19:26,580
machines for an hour so and then tear

00:19:25,380 --> 00:19:28,740
them right down when you're finished

00:19:26,580 --> 00:19:30,330
sobeys down is simply going to turn off

00:19:28,740 --> 00:19:33,630
the two instances that it's spun up to

00:19:30,330 --> 00:19:35,400
create the load test alright so we

00:19:33,630 --> 00:19:37,230
talked a little bit about apache bench

00:19:35,400 --> 00:19:39,570
and siege when you want to load test a

00:19:37,230 --> 00:19:41,520
single endpoint we talked about multi

00:19:39,570 --> 00:19:42,960
mechanize when you want to script

00:19:41,520 --> 00:19:44,460
different endpoints and we talked about

00:19:42,960 --> 00:19:45,930
bees with machine guns when you want to

00:19:44,460 --> 00:19:47,580
generate massive amounts of concurrency

00:19:45,930 --> 00:19:50,190
but sometimes you want to do all of

00:19:47,580 --> 00:19:53,370
those things at the same time so enter

00:19:50,190 --> 00:19:54,810
locust IL so this is relatively new open

00:19:53,370 --> 00:19:55,879
source framework for load testing I'm a

00:19:54,810 --> 00:19:57,769
big fan

00:19:55,879 --> 00:20:00,139
so again written in Python the go to a

00:19:57,769 --> 00:20:02,809
website locust io pretty much all the

00:20:00,139 --> 00:20:04,879
slides are directly copied from there so

00:20:02,809 --> 00:20:06,559
you can pimp and stop pip install locust

00:20:04,879 --> 00:20:09,609
io because again it's a Python tool and

00:20:06,559 --> 00:20:12,619
it gives you the benefit of both worlds

00:20:09,609 --> 00:20:14,809
one it allows you to run low tests from

00:20:12,619 --> 00:20:17,899
multiple servers but it also allows you

00:20:14,809 --> 00:20:19,759
to have complex transactions and then

00:20:17,899 --> 00:20:21,499
even better is it has a bunch of great

00:20:19,759 --> 00:20:23,119
reporting and chips with the web

00:20:21,499 --> 00:20:25,639
application so that you can do this very

00:20:23,119 --> 00:20:27,049
easily in an automated fashion so the

00:20:25,639 --> 00:20:29,029
reason I talk about performance testing

00:20:27,049 --> 00:20:30,469
is this isn't a one off most of the time

00:20:29,029 --> 00:20:31,879
the problem with performance testing is

00:20:30,469 --> 00:20:33,769
you only want to do it right before

00:20:31,879 --> 00:20:37,339
launch how many of you know about

00:20:33,769 --> 00:20:39,499
healthcare gov so this is the only time

00:20:37,339 --> 00:20:41,119
I've seen a president go on live TV and

00:20:39,499 --> 00:20:42,889
apologize for a broken website because

00:20:41,119 --> 00:20:44,419
they didn't do any testing and they

00:20:42,889 --> 00:20:46,399
spend some obscene amount of money on it

00:20:44,419 --> 00:20:50,149
they've done some testing ready today

00:20:46,399 --> 00:20:54,319
and realize they were it wasn't going to

00:20:50,149 --> 00:20:56,659
go well and it didn't and a lot of

00:20:54,319 --> 00:20:59,059
people got in trouble for that right so

00:20:56,659 --> 00:21:00,289
the goal is don't just do this when its

00:20:59,059 --> 00:21:01,129
launch time this is something that

00:21:00,289 --> 00:21:03,079
should be part of the software

00:21:01,129 --> 00:21:04,759
development lifecycle it's a continuous

00:21:03,079 --> 00:21:06,079
process everything that you do affects

00:21:04,759 --> 00:21:08,239
the performance of your code you should

00:21:06,079 --> 00:21:09,919
understand that in the same way that you

00:21:08,239 --> 00:21:12,259
have a continuous integration set up and

00:21:09,919 --> 00:21:13,609
you write unit and functional tests you

00:21:12,259 --> 00:21:15,199
should write performance tests because

00:21:13,609 --> 00:21:16,819
you should understand when you're going

00:21:15,199 --> 00:21:18,829
to significantly impact the end user

00:21:16,819 --> 00:21:20,449
experience as part of every release

00:21:18,829 --> 00:21:22,239
process so we'll get more into that

00:21:20,449 --> 00:21:24,889
later but for now more about locust I oh

00:21:22,239 --> 00:21:26,599
so the idea behind locust oil is that

00:21:24,889 --> 00:21:28,429
you can very easily script a website

00:21:26,599 --> 00:21:30,649
toss so the same way that multi

00:21:28,429 --> 00:21:34,159
mechanize has a transaction you can

00:21:30,649 --> 00:21:36,109
define a set of task in this case yeah

00:21:34,159 --> 00:21:37,759
I'll just go right into this so you have

00:21:36,109 --> 00:21:39,559
a user behavior in this case we just

00:21:37,759 --> 00:21:41,869
want to log in and fetch the profile and

00:21:39,559 --> 00:21:43,549
on every request we need to login so on

00:21:41,869 --> 00:21:46,579
start just like you have a setup and

00:21:43,549 --> 00:21:48,139
teardown you have a non start we're

00:21:46,579 --> 00:21:49,909
automatically going to login and this is

00:21:48,139 --> 00:21:53,929
just going to do an HTTP POST request to

00:21:49,909 --> 00:21:57,919
the H the login endpoint so if you see

00:21:53,929 --> 00:21:59,449
back here it's just a call self client

00:21:57,919 --> 00:22:01,339
top post and post the username and

00:21:59,449 --> 00:22:03,469
password to the login endpoint at the

00:22:01,339 --> 00:22:05,629
start of every request and then we want

00:22:03,469 --> 00:22:07,699
to run the test number two which is to

00:22:05,629 --> 00:22:08,869
fetch the homepage and then finally the

00:22:07,699 --> 00:22:09,530
third task which is the fetch your

00:22:08,869 --> 00:22:11,060
profile

00:22:09,530 --> 00:22:12,170
want to login then I want to go to the

00:22:11,060 --> 00:22:16,070
homepage and then I want to go to the

00:22:12,170 --> 00:22:18,320
profile now I we can get into some of

00:22:16,070 --> 00:22:20,330
the rest but the idea here is that it

00:22:18,320 --> 00:22:21,620
also comes with a great UI because for

00:22:20,330 --> 00:22:23,000
each one of these endpoints you want to

00:22:21,620 --> 00:22:24,800
understand what's the average requests

00:22:23,000 --> 00:22:26,690
per second and the average latency for

00:22:24,800 --> 00:22:29,030
each one of these transactions by

00:22:26,690 --> 00:22:31,100
scripting them both focused i lo and by

00:22:29,030 --> 00:22:32,840
giving you a nice UI to run these tests

00:22:31,100 --> 00:22:35,240
they make it very easy to automate all

00:22:32,840 --> 00:22:37,370
the stuff very easy to script complex

00:22:35,240 --> 00:22:40,730
transactions and then generate high

00:22:37,370 --> 00:22:43,070
levels of concurrency as well so check

00:22:40,730 --> 00:22:45,350
it out locust IL but the idea here is

00:22:43,070 --> 00:22:47,750
there are many tools it's not just going

00:22:45,350 --> 00:22:49,430
to be a bunch of Python tools that i

00:22:47,750 --> 00:22:52,340
hand-picked for this talk but there's

00:22:49,430 --> 00:22:53,960
gallon work song there's a ton of

00:22:52,340 --> 00:22:55,430
different server side tools the ideas

00:22:53,960 --> 00:22:58,130
that you use what works well for you in

00:22:55,430 --> 00:22:59,930
your use case but that's just the server

00:22:58,130 --> 00:23:01,670
side the reality is what about the

00:22:59,930 --> 00:23:03,650
client side most people when they think

00:23:01,670 --> 00:23:05,000
about performance they think about only

00:23:03,650 --> 00:23:07,370
the server side because that's the

00:23:05,000 --> 00:23:08,570
application stack but which you don't

00:23:07,370 --> 00:23:10,880
realize that in modern web applications

00:23:08,570 --> 00:23:12,650
more latency comes from the client side

00:23:10,880 --> 00:23:14,270
than the server side though 200

00:23:12,650 --> 00:23:16,220
milliseconds that you're waiting for the

00:23:14,270 --> 00:23:17,540
response from the server is nothing

00:23:16,220 --> 00:23:19,610
compared to the two seconds that you're

00:23:17,540 --> 00:23:21,680
downloading JavaScript CSS resources

00:23:19,610 --> 00:23:23,420
waiting for the page to execute

00:23:21,680 --> 00:23:25,940
JavaScript and paint the actual page to

00:23:23,420 --> 00:23:27,440
become usable when you talk about the

00:23:25,940 --> 00:23:30,320
end user experience time it's about

00:23:27,440 --> 00:23:31,370
users perceive experience it doesn't

00:23:30,320 --> 00:23:32,810
matter to them whether it's on the

00:23:31,370 --> 00:23:34,640
server side of the client side it's all

00:23:32,810 --> 00:23:36,290
about am I waiting in my browser can I

00:23:34,640 --> 00:23:38,630
use this right now can i suck on that

00:23:36,290 --> 00:23:40,010
box to start searching that's why

00:23:38,630 --> 00:23:41,720
google's homepage loads very quickly

00:23:40,010 --> 00:23:43,180
it's just the box i just wanna be able

00:23:41,720 --> 00:23:45,320
to type in the box as fast as possible

00:23:43,180 --> 00:23:47,090
right so when you think about

00:23:45,320 --> 00:23:49,130
performance testing don't just focus on

00:23:47,090 --> 00:23:51,230
the server side also understand how the

00:23:49,130 --> 00:23:52,850
client side is impacted so google is

00:23:51,230 --> 00:23:53,780
invested a ton of money and performance

00:23:52,850 --> 00:23:54,890
engineering because it's really

00:23:53,780 --> 00:23:56,210
important for them because they

00:23:54,890 --> 00:23:58,310
understand that performance impacts

00:23:56,210 --> 00:23:59,720
their business and one of the great

00:23:58,310 --> 00:24:01,250
tools that they've released is PageSpeed

00:23:59,720 --> 00:24:02,900
insights so Google PageSpeed insights

00:24:01,250 --> 00:24:05,030
allows you to analyze and optimize your

00:24:02,900 --> 00:24:07,220
website it's using the PageSpeed rules

00:24:05,030 --> 00:24:08,900
so what it will tell you is hey you

00:24:07,220 --> 00:24:11,030
should be using fire features expires

00:24:08,900 --> 00:24:12,890
headers for your CSS and JavaScript you

00:24:11,030 --> 00:24:15,050
should be using gzip compression for

00:24:12,890 --> 00:24:16,250
delivering your CSS and JavaScript but

00:24:15,050 --> 00:24:19,760
even better they've made it extremely

00:24:16,250 --> 00:24:21,830
easy to automate this piece with engine

00:24:19,760 --> 00:24:23,090
X PageSpeed and apache PageSpeed so

00:24:21,830 --> 00:24:25,880
these are modules which will read

00:24:23,090 --> 00:24:28,130
your responses in real time to make them

00:24:25,880 --> 00:24:30,230
more optimized so most of the time I

00:24:28,130 --> 00:24:31,940
don't recommend using a patch year

00:24:30,230 --> 00:24:34,160
engine X PageSpeed this is the cheap

00:24:31,940 --> 00:24:35,780
lazy way to do it cheap as an

00:24:34,160 --> 00:24:37,010
engineering time and lazy as and you

00:24:35,780 --> 00:24:38,570
don't actually have to fix anything you

00:24:37,010 --> 00:24:40,250
just install a module and it does all

00:24:38,570 --> 00:24:42,650
the hard work for you you should really

00:24:40,250 --> 00:24:43,910
use automation and front-end automation

00:24:42,650 --> 00:24:46,040
to make this part of your development

00:24:43,910 --> 00:24:47,390
cycle so that when you package your

00:24:46,040 --> 00:24:49,520
application up to deploy to production

00:24:47,390 --> 00:24:51,980
you automatically minify your CSS and

00:24:49,520 --> 00:24:53,930
JavaScript so that you automatically set

00:24:51,980 --> 00:24:56,060
the engine X configuration up to serve

00:24:53,930 --> 00:24:57,890
it compressed so you can do that very

00:24:56,060 --> 00:24:59,450
easily with engine X PageSpeed but

00:24:57,890 --> 00:25:00,980
you're not going to really fix the root

00:24:59,450 --> 00:25:03,890
of the problem unless you fix it in your

00:25:00,980 --> 00:25:07,460
software development stock so it's not

00:25:03,890 --> 00:25:10,100
just a page speed a module for your web

00:25:07,460 --> 00:25:11,690
server they also have a great website so

00:25:10,100 --> 00:25:13,520
that you can go and get actionable

00:25:11,690 --> 00:25:16,190
advice on how to improve the performance

00:25:13,520 --> 00:25:19,010
so you can go to developers.google.com /

00:25:16,190 --> 00:25:20,480
page speed and type in your URL and

00:25:19,010 --> 00:25:22,460
we'll give you tips on how to improve

00:25:20,480 --> 00:25:24,080
the performance and this is the end

00:25:22,460 --> 00:25:27,650
users perceive performance is what

00:25:24,080 --> 00:25:29,630
usually matters the most so it's

00:25:27,650 --> 00:25:31,640
available as a web page and send alone

00:25:29,630 --> 00:25:34,250
API it says available as a chrome

00:25:31,640 --> 00:25:35,750
developer tool so it's very easy as to

00:25:34,250 --> 00:25:40,670
make part of your development setup and

00:25:35,750 --> 00:25:42,860
it's also available as a gold package so

00:25:40,670 --> 00:25:45,770
you can if you're familiar with new jsu

00:25:42,860 --> 00:25:47,480
can NPM install PSI and this will allow

00:25:45,770 --> 00:25:50,150
you to automate this as part of your

00:25:47,480 --> 00:25:52,490
front-end workflow so what that will

00:25:50,150 --> 00:25:54,290
look like once you npm install PSI you

00:25:52,490 --> 00:25:56,750
have access to the PSI command line and

00:25:54,290 --> 00:25:58,520
then you can simply run PSI and then the

00:25:56,750 --> 00:26:00,110
domain and what this will do is run

00:25:58,520 --> 00:26:02,240
PageSpeed insights using the PageSpeed

00:26:00,110 --> 00:26:04,940
API underneath and give you some

00:26:02,240 --> 00:26:06,890
formatted output it will tell you your

00:26:04,940 --> 00:26:09,830
page speed score so 73 is not too

00:26:06,890 --> 00:26:11,510
stiller the amount of CSS and JavaScript

00:26:09,830 --> 00:26:13,310
that you're using how many are cash on

00:26:11,510 --> 00:26:15,380
cash how much is compressed uncompressed

00:26:13,310 --> 00:26:17,060
etc so this steadily is a good way to

00:26:15,380 --> 00:26:19,130
integrate with your continuous

00:26:17,060 --> 00:26:21,650
integration setup so the old fire this

00:26:19,130 --> 00:26:23,510
off is just another task and then start

00:26:21,650 --> 00:26:25,220
to do some benchmarks across this so for

00:26:23,510 --> 00:26:28,340
our page we ever goes over five hundred

00:26:25,220 --> 00:26:29,930
kilobytes then we should probably have

00:26:28,340 --> 00:26:33,680
something fail like a tests would fail

00:26:29,930 --> 00:26:35,120
and then if you want to just integrate

00:26:33,680 --> 00:26:36,980
directly of course they have a REST API

00:26:35,120 --> 00:26:39,210
that's available

00:26:36,980 --> 00:26:40,620
so sometimes you want to actually

00:26:39,210 --> 00:26:43,170
understand what's going on in the

00:26:40,620 --> 00:26:44,490
JavaScript world so W bench is a ruby

00:26:43,170 --> 00:26:46,950
tool that makes it really easy to

00:26:44,490 --> 00:26:49,440
understand the what's happening inside

00:26:46,950 --> 00:26:51,720
the browser so it lets you understand

00:26:49,440 --> 00:26:53,970
the page load times so this is a ruby

00:26:51,720 --> 00:26:57,480
tool so you can simply gem install w

00:26:53,970 --> 00:26:59,820
bench and w bench uses effectively a

00:26:57,480 --> 00:27:01,530
chrome and webdriver so that it'll run

00:26:59,820 --> 00:27:05,580
inside an actual browser instance and

00:27:01,530 --> 00:27:06,690
then record the the PageSpeed times so

00:27:05,580 --> 00:27:08,429
in this case I'm just going to run it

00:27:06,690 --> 00:27:11,160
across the very wonderful Dustin Widow

00:27:08,429 --> 00:27:15,600
calm and you'll see how horribly it's

00:27:11,160 --> 00:27:17,190
built so I on google has a google fav

00:27:15,600 --> 00:27:19,140
icon service that they don't really

00:27:17,190 --> 00:27:22,140
publicly expose but each one of those

00:27:19,140 --> 00:27:23,370
requests ends up in a redirect which

00:27:22,140 --> 00:27:25,860
means everything takes a really long

00:27:23,370 --> 00:27:27,210
time on my website to load and what you

00:27:25,860 --> 00:27:30,450
can see here was really the end user

00:27:27,210 --> 00:27:33,540
timings that yes so the user timing AP

00:27:30,450 --> 00:27:35,400
is so w3c has a set of timing API as one

00:27:33,540 --> 00:27:37,470
is the user timing API so how much time

00:27:35,400 --> 00:27:39,540
I spent doing dns lookups how much time

00:27:37,470 --> 00:27:41,580
I spent doing SSL negotiation how much

00:27:39,540 --> 00:27:43,710
time I spent waiting for the browser to

00:27:41,580 --> 00:27:45,480
download CSS and JavaScript waiting for

00:27:43,710 --> 00:27:48,270
the page to paint and for the page to

00:27:45,480 --> 00:27:50,490
become available and then they also have

00:27:48,270 --> 00:27:52,440
a resources timing API so if you go to

00:27:50,490 --> 00:27:54,300
added a social widget on your website

00:27:52,440 --> 00:27:55,920
like the twitter share button or a

00:27:54,300 --> 00:27:57,480
linkedin share button you'll realize

00:27:55,920 --> 00:28:00,330
they really start to impact performance

00:27:57,480 --> 00:28:01,800
so that gives you performance timings

00:28:00,330 --> 00:28:03,990
for the individual javascript and CSS

00:28:01,800 --> 00:28:05,490
that you include on your site and then

00:28:03,990 --> 00:28:06,570
they're just releasing a custom timing

00:28:05,490 --> 00:28:09,390
API so that you can do whatever

00:28:06,570 --> 00:28:10,980
measurements you want in JavaScript only

00:28:09,390 --> 00:28:12,420
tell you all this so they can tell you

00:28:10,980 --> 00:28:13,830
this which is to automate your client

00:28:12,420 --> 00:28:16,770
side performance testing with grunt and

00:28:13,830 --> 00:28:18,420
gulp so this talks can be viewed a

00:28:16,770 --> 00:28:20,309
little bit but use Bower for managing

00:28:18,420 --> 00:28:22,530
dependencies use grunt or gulp for

00:28:20,309 --> 00:28:24,390
automation front-end automate your

00:28:22,530 --> 00:28:25,740
front-end workflows and you and then for

00:28:24,390 --> 00:28:27,330
bootstrapping a project so if you're

00:28:25,740 --> 00:28:28,980
getting something new today there's a

00:28:27,330 --> 00:28:30,660
bunch of ye o man generators for Django

00:28:28,980 --> 00:28:31,860
they make it really easy to do all the

00:28:30,660 --> 00:28:34,590
stuff that I'm talking about with best

00:28:31,860 --> 00:28:35,910
practices built-in by default grunt or

00:28:34,590 --> 00:28:38,690
gulp doesn't really matter pick what's

00:28:35,910 --> 00:28:41,040
your flavor which flavor works for you

00:28:38,690 --> 00:28:42,840
okay so by a show of hands how many

00:28:41,040 --> 00:28:47,160
people understand how many of you would

00:28:42,840 --> 00:28:49,200
call yourselves professionals yeah

00:28:47,160 --> 00:28:51,299
hopefully all of you

00:28:49,200 --> 00:28:53,639
and then how many of you understand that

00:28:51,299 --> 00:28:57,080
you have a problem when a user complains

00:28:53,639 --> 00:28:59,309
directly to you or be a customer support

00:28:57,080 --> 00:29:01,830
which says you have no level of

00:28:59,309 --> 00:29:03,929
performance monitoring it all built in

00:29:01,830 --> 00:29:06,659
you find out when users go and say hey I

00:29:03,929 --> 00:29:08,850
can't check out into your website so

00:29:06,659 --> 00:29:12,029
really the reality is that most people

00:29:08,850 --> 00:29:13,649
don't measure performance and I work for

00:29:12,029 --> 00:29:15,720
a performance company so I'm not here to

00:29:13,649 --> 00:29:17,100
pitch that my goal here is to simply say

00:29:15,720 --> 00:29:18,899
that this is really impactful and

00:29:17,100 --> 00:29:20,669
affects your businesses more than you

00:29:18,899 --> 00:29:21,779
might know so you should be tracking

00:29:20,669 --> 00:29:24,059
performance and development and

00:29:21,779 --> 00:29:25,769
production even those companies the few

00:29:24,059 --> 00:29:27,330
companies that do track performance they

00:29:25,769 --> 00:29:30,179
only track performance in production and

00:29:27,330 --> 00:29:32,460
they use some variety of APM tools out

00:29:30,179 --> 00:29:34,529
there and but if you're going to do load

00:29:32,460 --> 00:29:37,409
testing the whole goal load testing is

00:29:34,529 --> 00:29:38,850
to generate useful information you can

00:29:37,409 --> 00:29:41,460
only capture that useful information if

00:29:38,850 --> 00:29:43,230
you're monitoring so before you go and

00:29:41,460 --> 00:29:44,820
start doing the load testing the

00:29:43,230 --> 00:29:46,919
client-side performance testing and

00:29:44,820 --> 00:29:48,269
Stroh in everything and sure enter code

00:29:46,919 --> 00:29:50,220
and instrument your databases your

00:29:48,269 --> 00:29:52,110
caches cues third party services in your

00:29:50,220 --> 00:29:53,399
infrastructure because oftentimes it's

00:29:52,110 --> 00:29:55,710
not the application stack that breaks

00:29:53,399 --> 00:29:59,010
it's not the django app that's the issue

00:29:55,710 --> 00:30:01,049
it's logging errors with some error

00:29:59,010 --> 00:30:03,419
provider making API calls the sum

00:30:01,049 --> 00:30:05,730
payment provider or the cache hit miss

00:30:03,419 --> 00:30:07,559
or database lookup so unless you're

00:30:05,730 --> 00:30:09,299
instrumenting that once you generate all

00:30:07,559 --> 00:30:11,130
this load you're not going to get any

00:30:09,299 --> 00:30:12,990
meaningful insights out of that so the

00:30:11,130 --> 00:30:14,519
whole goal is you should instrument your

00:30:12,990 --> 00:30:16,559
applications your infrastructure so that

00:30:14,519 --> 00:30:19,350
when you run a performance test you get

00:30:16,559 --> 00:30:20,970
useful insights so there's a ton of

00:30:19,350 --> 00:30:22,919
different open source tools pick your

00:30:20,970 --> 00:30:24,149
favorite stack but the reality is that

00:30:22,919 --> 00:30:26,130
you should be doing some level of

00:30:24,149 --> 00:30:28,679
performance testing and some level

00:30:26,130 --> 00:30:31,559
monitoring I'm a big fan of chef and Sen

00:30:28,679 --> 00:30:32,789
su stats degraff I anger fauna so you

00:30:31,559 --> 00:30:34,620
can have a cool dashboard that looks

00:30:32,789 --> 00:30:36,539
like this I know I'm covering a lot of

00:30:34,620 --> 00:30:38,519
this stuff very quickly but the idea

00:30:36,539 --> 00:30:40,139
here is that if you can instrument your

00:30:38,519 --> 00:30:41,880
applications and you can instrument to

00:30:40,139 --> 00:30:44,909
all the API calls that you're making all

00:30:41,880 --> 00:30:46,679
the database lookups your cash he

00:30:44,909 --> 00:30:48,809
hiddenness ratio if you're talking to

00:30:46,679 --> 00:30:50,370
memcache lettuce you can really start to

00:30:48,809 --> 00:30:52,019
understand where you need to focus on

00:30:50,370 --> 00:30:54,389
improving performance when you do run a

00:30:52,019 --> 00:30:56,130
load test running a load test to

00:30:54,389 --> 00:30:57,480
understand like capacity planning and

00:30:56,130 --> 00:30:59,340
whether you're going to fail on launch

00:30:57,480 --> 00:31:01,080
day is one thing but making it apart

00:30:59,340 --> 00:31:02,490
another level functional testing for

00:31:01,080 --> 00:31:04,380
your applications is really the

00:31:02,490 --> 00:31:05,520
goal and you can only do that if you

00:31:04,380 --> 00:31:07,410
actually have some level of

00:31:05,520 --> 00:31:11,220
instrumentation to give meaningful

00:31:07,410 --> 00:31:12,809
insights and then the last part is again

00:31:11,220 --> 00:31:14,790
it's about the end users perceived

00:31:12,809 --> 00:31:16,980
performance so you should track

00:31:14,790 --> 00:31:19,230
performance of your end users because if

00:31:16,980 --> 00:31:21,480
you have an application hosted an Amazon

00:31:19,230 --> 00:31:23,370
Web Services and the west coast and you

00:31:21,480 --> 00:31:25,830
have a bunch of users in Australia well

00:31:23,370 --> 00:31:27,360
a bunch of users on telstra service and

00:31:25,830 --> 00:31:28,620
australia might be having a very serious

00:31:27,360 --> 00:31:30,890
problem and you're never going to be

00:31:28,620 --> 00:31:32,760
able to see that sorry to be specific

00:31:30,890 --> 00:31:34,800
you're never going to be able to see

00:31:32,760 --> 00:31:37,679
that unless you actually monitor the end

00:31:34,800 --> 00:31:40,080
users experience there's a bunch of open

00:31:37,679 --> 00:31:41,640
source tools that make that possible so

00:31:40,080 --> 00:31:43,260
there's the episodes framework from the

00:31:41,640 --> 00:31:45,540
one of the Steve Souders one of the guys

00:31:43,260 --> 00:31:47,220
behind Google PageSpeed and what it

00:31:45,540 --> 00:31:50,190
essentially does is it captures the

00:31:47,220 --> 00:31:52,980
JavaScript timings from inside everyone

00:31:50,190 --> 00:31:55,350
your browser real users browsers so

00:31:52,980 --> 00:31:56,790
you'll see very quickly if users in a

00:31:55,350 --> 00:31:58,920
specific location or on a specific

00:31:56,790 --> 00:32:01,020
carrier or having performance problems

00:31:58,920 --> 00:32:02,790
because that's that wouldn't show up and

00:32:01,020 --> 00:32:04,500
on the Python statue that's not going to

00:32:02,790 --> 00:32:06,780
be an exception at century or whatever

00:32:04,500 --> 00:32:07,950
air longing platform you use it's just

00:32:06,780 --> 00:32:09,330
going to be a user who has a bad

00:32:07,950 --> 00:32:10,950
experience who complains the customer

00:32:09,330 --> 00:32:12,990
support because they think it's your

00:32:10,950 --> 00:32:16,410
full even when it could be their care it

00:32:12,990 --> 00:32:19,800
could be a particular browser so check

00:32:16,410 --> 00:32:21,510
it out and then webpagetest org so I'm a

00:32:19,800 --> 00:32:23,309
big fan of webpagetest org for a couple

00:32:21,510 --> 00:32:25,290
of reasons one that allows you to very

00:32:23,309 --> 00:32:27,210
easily check performance across the

00:32:25,290 --> 00:32:29,670
different array of browsers so you can

00:32:27,210 --> 00:32:31,980
check chrome performance on the west

00:32:29,670 --> 00:32:33,809
from specific locations over specific

00:32:31,980 --> 00:32:37,860
network conditions so I want to check

00:32:33,809 --> 00:32:40,080
chrome from New Zealand over 3g how does

00:32:37,860 --> 00:32:42,600
my application load and more importantly

00:32:40,080 --> 00:32:46,260
what was the rendering process look like

00:32:42,600 --> 00:32:49,050
so one web page hesterberg you can just

00:32:46,260 --> 00:32:50,340
fill out a URL and tell where you want

00:32:49,050 --> 00:32:53,370
to run the test and from what browser

00:32:50,340 --> 00:32:55,020
and then it will generate test results

00:32:53,370 --> 00:32:56,340
so one of the things that I will awake

00:32:55,020 --> 00:32:59,070
because this is a collaboration amongst

00:32:56,340 --> 00:33:01,650
a bunch of technology companies so it's

00:32:59,070 --> 00:33:03,870
available free and it will give you a

00:33:01,650 --> 00:33:05,550
both you can use it for QA purpose so

00:33:03,870 --> 00:33:07,110
it'll give you a screenshot of the final

00:33:05,550 --> 00:33:09,809
rendered form but it will also give you

00:33:07,110 --> 00:33:12,120
a video of the rendering process so

00:33:09,809 --> 00:33:14,450
oftentimes what you want is the to

00:33:12,120 --> 00:33:16,910
optimize the critical path so

00:33:14,450 --> 00:33:19,520
the key parts so the browser can paint

00:33:16,910 --> 00:33:21,680
the page as fast as possible that's what

00:33:19,520 --> 00:33:24,290
matters most and you'll very quickly be

00:33:21,680 --> 00:33:26,210
able to see oftentimes what you'll see

00:33:24,290 --> 00:33:27,800
is the sudden flash and then the page

00:33:26,210 --> 00:33:29,840
gets painted and that can be a

00:33:27,800 --> 00:33:31,700
one-second delay or you can see

00:33:29,840 --> 00:33:33,830
immediately the page show up and then it

00:33:31,700 --> 00:33:35,450
gets progressively built beneath it all

00:33:33,830 --> 00:33:36,680
depends on how you architect it often

00:33:35,450 --> 00:33:38,630
times it's very hard to test those

00:33:36,680 --> 00:33:42,890
things webpagetest org makes it very

00:33:38,630 --> 00:33:45,410
easy to test and QA and like i said if

00:33:42,890 --> 00:33:47,500
you've ever used the developer console

00:33:45,410 --> 00:33:49,790
and Chrome or Firefox developer tools

00:33:47,500 --> 00:33:51,950
you've probably used to the waterfall

00:33:49,790 --> 00:33:54,080
view of loading your client side

00:33:51,950 --> 00:33:55,700
resources this will allow you to see

00:33:54,080 --> 00:33:58,910
where you're spending time like I said I

00:33:55,700 --> 00:34:00,530
steal the Google fav icon service which

00:33:58,910 --> 00:34:02,660
ends in a bunch of redirects which kills

00:34:00,530 --> 00:34:03,830
my performance and without this tool i

00:34:02,660 --> 00:34:05,570
would have never realized that was

00:34:03,830 --> 00:34:07,880
happening but it basically makes two

00:34:05,570 --> 00:34:10,130
requests for 20 different icons on my

00:34:07,880 --> 00:34:14,510
home page and my home page is only 20

00:34:10,130 --> 00:34:16,670
icon so it's a bit of a waste okay so I

00:34:14,510 --> 00:34:17,750
think we've covered a lot here the final

00:34:16,670 --> 00:34:21,050
thing we're going to talk about is sites

00:34:17,750 --> 00:34:24,380
video so site speed dial is very much

00:34:21,050 --> 00:34:27,050
like Google PageSpeed except for it's

00:34:24,380 --> 00:34:29,660
much more automated and much more

00:34:27,050 --> 00:34:32,930
comprehensive so again you can NPM

00:34:29,660 --> 00:34:34,490
install sites pio and what this will

00:34:32,930 --> 00:34:36,320
allow you to do is analyze your website

00:34:34,490 --> 00:34:37,610
speed performance so the same way that

00:34:36,320 --> 00:34:39,530
page speed will give you actionable

00:34:37,610 --> 00:34:42,710
advice on how to improve the performance

00:34:39,530 --> 00:34:44,780
like you should cash these CSS and

00:34:42,710 --> 00:34:46,610
JavaScript files or manuf I them there's

00:34:44,780 --> 00:34:49,160
a bunch of rules that site speed

00:34:46,610 --> 00:34:50,360
incorporates and will test for so this

00:34:49,160 --> 00:34:52,730
is really great because when i talk

00:34:50,360 --> 00:34:54,200
about automating all this you want to

00:34:52,730 --> 00:34:56,360
integrate this in your jenkins flow your

00:34:54,200 --> 00:34:57,740
travis CI workflow and you want to be

00:34:56,360 --> 00:34:59,900
able to have some role metrics to use

00:34:57,740 --> 00:35:01,490
site speed makes it very easy to capture

00:34:59,900 --> 00:35:04,070
each one of these as individual metrics

00:35:01,490 --> 00:35:06,620
and then fail based on that so you

00:35:04,070 --> 00:35:08,690
should fail if the average end-user

00:35:06,620 --> 00:35:10,550
experience time is over two seconds or

00:35:08,690 --> 00:35:12,440
you're including more than 500 k

00:35:10,550 --> 00:35:15,020
JavaScript or whatever the conditions

00:35:12,440 --> 00:35:16,910
make sense for your site it's a pretty

00:35:15,020 --> 00:35:18,980
comprehensive listing of tools so it's

00:35:16,910 --> 00:35:21,890
not just CSS and JavaScript pastor

00:35:18,980 --> 00:35:24,590
uncashed gzip there on a compressed or

00:35:21,890 --> 00:35:27,140
uncompressed it's also how many dns

00:35:24,590 --> 00:35:28,310
lookups are using ssl negotiation times

00:35:27,140 --> 00:35:32,090
it's very comprehend

00:35:28,310 --> 00:35:33,440
so take a look at it and then again you

00:35:32,090 --> 00:35:36,170
should be monitoring production

00:35:33,440 --> 00:35:37,370
applications you know there's I think a

00:35:36,170 --> 00:35:39,710
lot of you are familiar with probably

00:35:37,370 --> 00:35:41,300
New Relic AppDynamics is a company that

00:35:39,710 --> 00:35:44,960
works with enterprise applications

00:35:41,300 --> 00:35:46,870
there's compu where's another one

00:35:44,960 --> 00:35:49,310
there's rux it there's a bunch of these

00:35:46,870 --> 00:35:50,780
performance management companies the

00:35:49,310 --> 00:35:52,970
goal of all these companies is to

00:35:50,780 --> 00:35:54,290
effectively map out both the server-side

00:35:52,970 --> 00:35:56,600
performance and the client-side

00:35:54,290 --> 00:35:58,250
performance and allow you to understand

00:35:56,600 --> 00:35:59,990
exactly what's happening I've talked

00:35:58,250 --> 00:36:01,730
about a bunch of open-source tools that

00:35:59,990 --> 00:36:04,160
allow you to do various pieces of this

00:36:01,730 --> 00:36:05,780
the gold all these tools is to tell you

00:36:04,160 --> 00:36:07,910
where your traffic is coming from in

00:36:05,780 --> 00:36:09,890
real time and then being able to capture

00:36:07,910 --> 00:36:11,480
the individual metrics and then give you

00:36:09,890 --> 00:36:13,070
visibility into the client side it

00:36:11,480 --> 00:36:14,540
doesn't really matter whether you roll

00:36:13,070 --> 00:36:15,920
open source or you buy a commercial

00:36:14,540 --> 00:36:19,580
package but you should be doing some

00:36:15,920 --> 00:36:21,440
level performance monitoring and then

00:36:19,580 --> 00:36:23,630
often times I've talked about a bunch of

00:36:21,440 --> 00:36:25,400
open source tools for writing all of

00:36:23,630 --> 00:36:27,590
these load tests but oftentimes it's

00:36:25,400 --> 00:36:30,050
much easier to buy this than to build it

00:36:27,590 --> 00:36:31,520
yourself so we'll just run through a

00:36:30,050 --> 00:36:34,490
couple of load testing services from the

00:36:31,520 --> 00:36:35,900
cloud so I'm a big fan of pica just

00:36:34,490 --> 00:36:37,370
because they have a global footprint and

00:36:35,900 --> 00:36:39,230
they make it very easy to just write to

00:36:37,370 --> 00:36:41,900
load tests and say hey I want you to go

00:36:39,230 --> 00:36:43,900
to this URL and send 50,000 users and

00:36:41,900 --> 00:36:46,790
give me a report on what that looks like

00:36:43,900 --> 00:36:48,590
blitz ro is probably one of my favorite

00:36:46,790 --> 00:36:51,940
in terms of just easy to get started

00:36:48,590 --> 00:36:54,290
blitz do its they give you free service

00:36:51,940 --> 00:36:56,060
it's paid and free but they give you a

00:36:54,290 --> 00:36:57,740
decent free plan where you can just say

00:36:56,060 --> 00:36:59,960
hey I want to spend up 500 users and

00:36:57,740 --> 00:37:02,780
then this URL and then plot the response

00:36:59,960 --> 00:37:05,120
time versus the level of concurrency and

00:37:02,780 --> 00:37:07,520
then blaze meteor which works well if

00:37:05,120 --> 00:37:09,800
you have complex interactions or you

00:37:07,520 --> 00:37:12,440
want to test mobile devices so sometimes

00:37:09,800 --> 00:37:14,480
I know it's you know in the vein of open

00:37:12,440 --> 00:37:16,310
source that we try to write all of our

00:37:14,480 --> 00:37:17,360
code ourselves but it often times it's a

00:37:16,310 --> 00:37:19,760
business problem that you're trying to

00:37:17,360 --> 00:37:21,860
solve and you should remember that

00:37:19,760 --> 00:37:26,750
that's not always code isn't always the

00:37:21,860 --> 00:37:29,150
solution I can't emphasize this enough

00:37:26,750 --> 00:37:30,680
but tests for failures the reality is

00:37:29,150 --> 00:37:32,810
that most people test under ideal

00:37:30,680 --> 00:37:34,670
conditions but at some point in life you

00:37:32,810 --> 00:37:36,980
realize that it's an imperfect world and

00:37:34,670 --> 00:37:39,710
things will happen so you should be

00:37:36,980 --> 00:37:41,030
prepared for them so Netflix understands

00:37:39,710 --> 00:37:41,930
this very much which is they've released

00:37:41,030 --> 00:37:43,130
a suite of tools

00:37:41,930 --> 00:37:45,170
called the simian army and one of those

00:37:43,130 --> 00:37:46,880
tools is chaos monkey what chaos monkey

00:37:45,170 --> 00:37:49,849
does is it goes and creates chaos inside

00:37:46,880 --> 00:37:51,200
of your AWS setup so what happens if you

00:37:49,849 --> 00:37:52,790
lose your caching layer how does your

00:37:51,200 --> 00:37:54,020
website respond if you drop your

00:37:52,790 --> 00:37:56,720
memcache instance or your reticence

00:37:54,020 --> 00:38:01,220
tances what happens if paypals api that

00:37:56,720 --> 00:38:03,650
you rely on slows down 3x do you retry

00:38:01,220 --> 00:38:05,839
and exponentially back off UQ things up

00:38:03,650 --> 00:38:08,089
and or do you block on them you don't

00:38:05,839 --> 00:38:09,980
really discover those issues until you

00:38:08,089 --> 00:38:11,720
have a production outage oftentimes

00:38:09,980 --> 00:38:13,130
that's when you learn the most when

00:38:11,720 --> 00:38:14,869
things are failing and it's the most

00:38:13,130 --> 00:38:16,130
painful to learn the lesson is like hey

00:38:14,869 --> 00:38:18,200
we really probably should have queued

00:38:16,130 --> 00:38:19,819
that up and not blocked on that or we

00:38:18,200 --> 00:38:22,760
should retry that request if it fails

00:38:19,819 --> 00:38:24,260
not just ignore it so you should really

00:38:22,760 --> 00:38:26,000
test for failures there's a bunch of

00:38:24,260 --> 00:38:27,470
tools that make this very easy to do but

00:38:26,000 --> 00:38:28,880
what happens if you run a load test and

00:38:27,470 --> 00:38:31,670
you kill off your memcache instances

00:38:28,880 --> 00:38:35,150
does it sir john the database in your

00:38:31,670 --> 00:38:36,859
database handle that etc so performance

00:38:35,150 --> 00:38:38,740
matters hopefully I've emphasize this

00:38:36,859 --> 00:38:41,420
enough so treat performance as a feature

00:38:38,740 --> 00:38:42,950
there's a bunch of tools and tips on how

00:38:41,420 --> 00:38:44,990
to improve front-end performance but

00:38:42,950 --> 00:38:46,220
basically use the 14k rule so you have

00:38:44,990 --> 00:38:49,520
instant loading so you want to make one

00:38:46,220 --> 00:38:51,020
HP route one small requests and get all

00:38:49,520 --> 00:38:52,430
everything you need to paint the page

00:38:51,020 --> 00:38:54,619
and then progressively build a page from

00:38:52,430 --> 00:38:56,510
there and there's a bunch of tools to do

00:38:54,619 --> 00:38:58,160
this but the core of this is use task

00:38:56,510 --> 00:39:00,920
runners to build and deploy production

00:38:58,160 --> 00:39:02,569
code so grunt and gulp where there's a

00:39:00,920 --> 00:39:04,190
ton of tools for Django that make it

00:39:02,569 --> 00:39:06,079
just as easy to incorporate these best

00:39:04,190 --> 00:39:08,690
practices like minifying CSS and

00:39:06,079 --> 00:39:12,319
javascript and combining all this stuff

00:39:08,690 --> 00:39:14,630
and then best practices for performance

00:39:12,319 --> 00:39:16,819
testing capacity plan and load test the

00:39:14,630 --> 00:39:19,040
server side optimizing performance

00:39:16,819 --> 00:39:20,540
that's the client side again you spend

00:39:19,040 --> 00:39:22,130
more time in the client side than you do

00:39:20,540 --> 00:39:24,170
on the server side so don't just think

00:39:22,130 --> 00:39:25,490
because the server can respond to a

00:39:24,170 --> 00:39:27,500
request that your users are going to

00:39:25,490 --> 00:39:28,880
have a good experience understand you're

00:39:27,500 --> 00:39:29,869
struggling point you can't understand

00:39:28,880 --> 00:39:31,849
why you're failing less you know you're

00:39:29,869 --> 00:39:33,770
starting from so instrument everything

00:39:31,849 --> 00:39:35,750
and don't just instrument and monitor

00:39:33,770 --> 00:39:37,640
production instrument and monitor your

00:39:35,750 --> 00:39:39,710
development and production environments

00:39:37,640 --> 00:39:41,270
because you don't want to fix it when

00:39:39,710 --> 00:39:42,770
it's red you want to fix it in the

00:39:41,270 --> 00:39:44,809
yellow you want to fix it before it goes

00:39:42,770 --> 00:39:46,970
live it's very easy to learn these

00:39:44,809 --> 00:39:48,319
lessons when the executive teams yelling

00:39:46,970 --> 00:39:49,760
at you because you're losing all this

00:39:48,319 --> 00:39:51,349
money in revenue per hour because

00:39:49,760 --> 00:39:52,970
everything is broken you get a lot of

00:39:51,349 --> 00:39:55,010
resources but it's also the most painful

00:39:52,970 --> 00:39:56,570
time to do it so try to do it earlier

00:39:55,010 --> 00:39:58,280
and then measure the difference of every

00:39:56,570 --> 00:40:00,050
change it's not just upgrading from

00:39:58,280 --> 00:40:02,510
Django from 17 to 18 that's going to

00:40:00,050 --> 00:40:04,760
impact performance or upgrading for your

00:40:02,510 --> 00:40:06,230
version of post grass or Linux or moving

00:40:04,760 --> 00:40:08,240
from one data center to the next

00:40:06,230 --> 00:40:09,710
everything affects performance and

00:40:08,240 --> 00:40:11,300
everybody cares about performance they

00:40:09,710 --> 00:40:13,520
just care about a different perspective

00:40:11,300 --> 00:40:15,560
this is people they care about revenue

00:40:13,520 --> 00:40:17,330
the operations team they care about

00:40:15,560 --> 00:40:18,860
uptime engineers they care about the end

00:40:17,330 --> 00:40:20,620
user experience that you just need to

00:40:18,860 --> 00:40:22,610
carve off the perspective that's useful

00:40:20,620 --> 00:40:24,200
so you should measure out the difference

00:40:22,610 --> 00:40:25,550
of every change and then automate

00:40:24,200 --> 00:40:28,100
performance testing your build and

00:40:25,550 --> 00:40:29,300
development process it should really be

00:40:28,100 --> 00:40:31,370
part of the software development process

00:40:29,300 --> 00:40:32,930
and something that's automated that

00:40:31,370 --> 00:40:34,940
happens every time and then ultimately

00:40:32,930 --> 00:40:38,240
understand how failures impact

00:40:34,940 --> 00:40:40,940
performance now there's been a lot of

00:40:38,240 --> 00:40:43,370
goldman's recently HTV to specifically a

00:40:40,940 --> 00:40:45,170
lot of the best practices for the HTTP

00:40:43,370 --> 00:40:47,630
one world are actually going to hurt you

00:40:45,170 --> 00:40:50,090
in the hd2 world so starting

00:40:47,630 --> 00:40:51,860
concatenating all your CSS CSS and

00:40:50,090 --> 00:40:54,940
JavaScript writing a bunch of images

00:40:51,860 --> 00:40:58,430
into one file these are all hacks around

00:40:54,940 --> 00:40:59,750
shortcomings in HTTP one hp2 eliminates

00:40:58,430 --> 00:41:01,130
the need for most of these hats so just

00:40:59,750 --> 00:41:02,840
be conscious that the protocols are

00:41:01,130 --> 00:41:04,510
changing and that you also should adopt

00:41:02,840 --> 00:41:06,680
your development techniques accordingly

00:41:04,510 --> 00:41:07,750
Igor Gork has a great book on

00:41:06,680 --> 00:41:09,500
high-performance browser

00:41:07,750 --> 00:41:10,490
high-performance browser networking that

00:41:09,500 --> 00:41:13,400
I'd highly recommend if you're

00:41:10,490 --> 00:41:14,960
interested so again integrate automated

00:41:13,400 --> 00:41:17,000
performance testing into continuous

00:41:14,960 --> 00:41:18,770
integration for both the server side and

00:41:17,000 --> 00:41:20,060
the client side understand the

00:41:18,770 --> 00:41:22,430
performance implications of every

00:41:20,060 --> 00:41:24,080
deployment package upgrade and monitor

00:41:22,430 --> 00:41:26,260
the end-user experience from end to end

00:41:24,080 --> 00:41:28,880
and development and in production and

00:41:26,260 --> 00:41:31,300
that's all i have for everyone any

00:41:28,880 --> 00:41:31,300
questions

00:41:35,550 --> 00:41:43,110
thank you very much the talk um I had a

00:41:38,650 --> 00:41:45,930
question you about whether you think

00:41:43,110 --> 00:41:48,700
given the complexity of modeling and

00:41:45,930 --> 00:41:51,910
involved in creating a load test and the

00:41:48,700 --> 00:41:54,550
an enormous possible investment of

00:41:51,910 --> 00:41:57,250
developer time whether there's any value

00:41:54,550 --> 00:41:59,290
in taking a feature ramp approach with

00:41:57,250 --> 00:42:00,940
regards to and I know she did mention I

00:41:59,290 --> 00:42:03,340
don't know if you have a opinions about

00:42:00,940 --> 00:42:06,340
that so most of the time when I try to

00:42:03,340 --> 00:42:08,640
do is take the basically replay

00:42:06,340 --> 00:42:10,720
production traffic production late don't

00:42:08,640 --> 00:42:12,160
don't model them out based on what you

00:42:10,720 --> 00:42:13,990
actually what you think your users are

00:42:12,160 --> 00:42:15,790
doing because your users will very

00:42:13,990 --> 00:42:18,400
quickly surprise you go and take a look

00:42:15,790 --> 00:42:20,380
at production access logs and find the

00:42:18,400 --> 00:42:22,690
most common user paths and then replay

00:42:20,380 --> 00:42:24,730
that traffic the feature branch the feel

00:42:22,690 --> 00:42:26,620
like when I write a new feature test out

00:42:24,730 --> 00:42:28,030
this feature does work quite well but

00:42:26,620 --> 00:42:30,640
what you're going to miss is the

00:42:28,030 --> 00:42:32,140
complexity of the overall so the most

00:42:30,640 --> 00:42:35,860
common use cases are things that you

00:42:32,140 --> 00:42:37,750
built originally and anything net new

00:42:35,860 --> 00:42:40,440
that works well especially as a way to

00:42:37,750 --> 00:42:42,520
have sustainable performance testing and

00:42:40,440 --> 00:42:44,650
but I definitely think that is something

00:42:42,520 --> 00:42:46,180
that you should replay production

00:42:44,650 --> 00:42:47,890
traffic versus just what you think users

00:42:46,180 --> 00:42:49,930
are going to do and then when you go

00:42:47,890 --> 00:42:52,450
building a sort of the feature branch

00:42:49,930 --> 00:42:54,370
approach or when you build individual

00:42:52,450 --> 00:42:56,590
features and you try to just test the

00:42:54,370 --> 00:42:58,540
net new functionality the users often

00:42:56,590 --> 00:42:59,950
use that ways that you don't expect so I

00:42:58,540 --> 00:43:01,840
think you rely on the source of truth

00:42:59,950 --> 00:43:03,310
which is those access logs so if you use

00:43:01,840 --> 00:43:07,600
some sort of load balancing solution

00:43:03,310 --> 00:43:10,650
pull it off of that I'll ask follow-up

00:43:07,600 --> 00:43:13,060
as long as no one else I'd so go for um

00:43:10,650 --> 00:43:14,350
when I say feature ramp I I meant

00:43:13,060 --> 00:43:17,770
something I meant something very

00:43:14,350 --> 00:43:20,560
specific so releasing to say one percent

00:43:17,770 --> 00:43:24,280
of you of your users and then okay yeah

00:43:20,560 --> 00:43:25,900
yeah um yeah so most companies well a

00:43:24,280 --> 00:43:27,280
lot of us Motor Company's end up to

00:43:25,900 --> 00:43:29,230
slowly releasing features through that

00:43:27,280 --> 00:43:31,420
that's a great way to actually test that

00:43:29,230 --> 00:43:34,510
out especially a multivariate testing or

00:43:31,420 --> 00:43:35,740
you see that a lot with very substantial

00:43:34,510 --> 00:43:38,830
changes where you just want to slowly

00:43:35,740 --> 00:43:40,420
roll them out I will write some sort of

00:43:38,830 --> 00:43:43,150
automated performance tests but you can

00:43:40,420 --> 00:43:44,950
very quickly learn by doing custom

00:43:43,150 --> 00:43:46,420
instrumentation around metrics that

00:43:44,950 --> 00:43:48,400
matter for those scenarios so if you're

00:43:46,420 --> 00:43:50,440
going to introduce a new feature

00:43:48,400 --> 00:43:52,270
now you can use like stats d to create

00:43:50,440 --> 00:43:53,980
some custom metrics around that and

00:43:52,270 --> 00:43:55,780
that's what I would that's generally how

00:43:53,980 --> 00:43:57,250
we release new features is when I want

00:43:55,780 --> 00:43:59,290
to release something new will track some

00:43:57,250 --> 00:44:01,540
important measurements for that specific

00:43:59,290 --> 00:44:03,550
feature and then we'll slowly roll it

00:44:01,540 --> 00:44:09,730
out to the users yeah that's a much

00:44:03,550 --> 00:44:13,210
better approach anyone else let's not

00:44:09,730 --> 00:44:15,720
ask it once all right that's our talk

00:44:13,210 --> 00:44:15,720

YouTube URL: https://www.youtube.com/watch?v=aGtizJ42Vgo


