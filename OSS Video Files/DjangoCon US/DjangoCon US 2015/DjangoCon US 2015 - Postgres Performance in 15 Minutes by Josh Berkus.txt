Title: DjangoCon US 2015 - Postgres Performance in 15 Minutes by Josh Berkus
Publication date: 2017-11-03
Playlist: DjangoCon US 2015
Description: 
	In 15 minutes, plus Q&A time, Postgres expert Josh Berkus will explain the essentials of making your database performance "good enough" that you can ignore it and move on to other things. This will include:

Why database configuration is less than 20% of performance
The 14 settings most people need
Why connection pooling is essential
Avoiding bad hardware
DB performance for the public cloud
Stupid things your app does which kills performance
Enjoy this fast-paced roundup of PostgreSQL performance essentials.
Captions: 
	00:00:17,150 --> 00:00:23,670
I'm I'm going to talk a little bit about

00:00:21,810 --> 00:00:27,090
some basic things you can do to improve

00:00:23,670 --> 00:00:30,420
Postgres performance for to your django

00:00:27,090 --> 00:00:34,590
application we got our running elephant

00:00:30,420 --> 00:00:37,050
here few people know that elephants can

00:00:34,590 --> 00:00:41,000
run at 20 to 25 miles an hour they're

00:00:37,050 --> 00:00:43,680
actually quite fast and Postgres can do

00:00:41,000 --> 00:00:45,660
thousands of requests per second so if

00:00:43,680 --> 00:00:48,140
you're not getting thousands of requests

00:00:45,660 --> 00:00:52,829
per second there's probably a reason why

00:00:48,140 --> 00:00:55,829
so what you do is you log into post res

00:00:52,829 --> 00:00:58,230
and you go into PostgreSQL conf and you

00:00:55,829 --> 00:01:02,989
set the hidden parameter go faster to

00:00:58,230 --> 00:01:04,769
ten and then you save it and you're done

00:01:02,989 --> 00:01:10,290
okay so we're done here we have

00:01:04,769 --> 00:01:14,880
questions no seriously unfortunately

00:01:10,290 --> 00:01:16,740
it's not that easy the we actually spend

00:01:14,880 --> 00:01:18,689
in posters of posters development world

00:01:16,740 --> 00:01:21,060
we spend a lot of time talking about how

00:01:18,689 --> 00:01:22,830
can we make things faster as a matter of

00:01:21,060 --> 00:01:24,920
equity spend more time talking about

00:01:22,830 --> 00:01:29,490
that than just about anything else

00:01:24,920 --> 00:01:32,610
except maybe the commit cue and whose

00:01:29,490 --> 00:01:35,670
turn it is to review things and as a

00:01:32,610 --> 00:01:37,680
result if there was something that we

00:01:35,670 --> 00:01:41,579
could do by default to make posters

00:01:37,680 --> 00:01:43,259
faster we probably already did it so the

00:01:41,579 --> 00:01:45,210
things that you're going to do to make

00:01:43,259 --> 00:01:48,270
Postgres faster are going to require

00:01:45,210 --> 00:01:51,000
work you can't just change a few

00:01:48,270 --> 00:01:52,740
configuration settings in fact one of

00:01:51,000 --> 00:01:54,149
the things that I do to get paid as I do

00:01:52,740 --> 00:01:55,560
a lot of performance tuning and

00:01:54,149 --> 00:01:58,229
performance engineering on different

00:01:55,560 --> 00:02:01,070
sites and this is more or less how I

00:01:58,229 --> 00:02:03,299
spend my time it varies a lot per site

00:02:01,070 --> 00:02:06,299
but more or less I spend my time and

00:02:03,299 --> 00:02:09,149
you'll notice that tuning the

00:02:06,299 --> 00:02:12,360
configuration is a pretty small minority

00:02:09,149 --> 00:02:13,890
of how I spend my time and the affect

00:02:12,360 --> 00:02:16,920
the tuning the configuration has

00:02:13,890 --> 00:02:18,209
is an even smaller minority as a matter

00:02:16,920 --> 00:02:19,860
of fact sometimes and in some

00:02:18,209 --> 00:02:21,980
environments changing the post Korsakov

00:02:19,860 --> 00:02:24,720
settings will have no effect no

00:02:21,980 --> 00:02:27,569
measurable effect at all on database

00:02:24,720 --> 00:02:29,700
throughput so instead we're going to

00:02:27,569 --> 00:02:33,200
talk about some of the other things you

00:02:29,700 --> 00:02:35,990
can do which have much larger effects on

00:02:33,200 --> 00:02:41,970
database throughput and responsiveness

00:02:35,990 --> 00:02:43,860
the first one is do less the fastest

00:02:41,970 --> 00:02:49,440
database request is the one that you

00:02:43,860 --> 00:02:51,090
don't make at all anytime that you're

00:02:49,440 --> 00:02:53,550
adding a piece of code that's going to

00:02:51,090 --> 00:02:55,739
work with with data you know anytime

00:02:53,550 --> 00:02:57,840
you're referencing the or M or whatever

00:02:55,739 --> 00:03:00,120
you say first of all do I really need

00:02:57,840 --> 00:03:02,160
the database to answer that is this

00:03:00,120 --> 00:03:05,670
something that I could be answering in

00:03:02,160 --> 00:03:09,330
the individual Django session without

00:03:05,670 --> 00:03:10,230
calling out to the database now one of

00:03:09,330 --> 00:03:13,650
the things that we're talking about

00:03:10,230 --> 00:03:15,030
there is caching of course I mean the

00:03:13,650 --> 00:03:16,500
obvious one is to just look in the

00:03:15,030 --> 00:03:20,060
results cache and actually make use of

00:03:16,500 --> 00:03:22,320
the results cache that seems obvious but

00:03:20,060 --> 00:03:25,350
for some reason people don't do it as

00:03:22,320 --> 00:03:27,570
much as they could if the results cache

00:03:25,350 --> 00:03:29,160
isn't enough because you actually need

00:03:27,570 --> 00:03:29,730
to share things among several different

00:03:29,160 --> 00:03:31,590
backends

00:03:29,730 --> 00:03:34,739
then you can use things like Redis and

00:03:31,590 --> 00:03:38,940
memcached also don't forget about using

00:03:34,739 --> 00:03:40,290
CD ends for caching large objects even

00:03:38,940 --> 00:03:41,730
if the large objects are being stored in

00:03:40,290 --> 00:03:43,170
the database there's some django systems

00:03:41,730 --> 00:03:44,730
out there people storing images in the

00:03:43,170 --> 00:03:46,739
database they're storing compressed data

00:03:44,730 --> 00:03:50,040
and that sort of thing give that data

00:03:46,739 --> 00:03:53,130
you know when you've retrieved it once

00:03:50,040 --> 00:03:57,180
copy it out of Postgres loaded up into a

00:03:53,130 --> 00:03:59,760
CDN reference it in the CDN by file name

00:03:57,180 --> 00:04:02,640
instead of retrieving it from the

00:03:59,760 --> 00:04:05,010
database all the time because there are

00:04:02,640 --> 00:04:06,630
a lot it's a lot easier to scale a CDN

00:04:05,010 --> 00:04:09,150
than it is to scale a relational

00:04:06,630 --> 00:04:11,070
database so the things that you don't

00:04:09,150 --> 00:04:13,230
need a relational database for scale

00:04:11,070 --> 00:04:15,209
them elsewhere so caching is sort of our

00:04:13,230 --> 00:04:17,760
first part of thing and do as much

00:04:15,209 --> 00:04:20,130
caching as you reasonably can with your

00:04:17,760 --> 00:04:24,510
concurrency and sort of data consistency

00:04:20,130 --> 00:04:27,380
model the other thing that we see a lot

00:04:24,510 --> 00:04:29,790
in doing less is actually some

00:04:27,380 --> 00:04:32,990
anti-patterns better better exhibited

00:04:29,790 --> 00:04:37,380
through common mistakes that people make

00:04:32,990 --> 00:04:39,630
one of which is polling now this is a

00:04:37,380 --> 00:04:42,630
very simplified example but this is

00:04:39,630 --> 00:04:44,850
something I see a lot with celery based

00:04:42,630 --> 00:04:48,240
apps and others which is let's have

00:04:44,850 --> 00:04:51,690
every back-end pull the database as fast

00:04:48,240 --> 00:04:54,240
as it can so that is polling for new

00:04:51,690 --> 00:04:56,000
jobs with no weight or with a little

00:04:54,240 --> 00:04:59,039
tiny wait like 10 milliseconds

00:04:56,000 --> 00:05:00,449
well this generates thousands to

00:04:59,039 --> 00:05:03,510
hundreds of thousands of database

00:05:00,449 --> 00:05:05,070
requests per second frequently the

00:05:03,510 --> 00:05:07,020
majority of your traffic it's not not a

00:05:05,070 --> 00:05:12,870
good thing to do another anti-pattern is

00:05:07,020 --> 00:05:14,550
requesting data you already have believe

00:05:12,870 --> 00:05:18,330
it or not this is from a real-life

00:05:14,550 --> 00:05:21,930
example let's look up users by the user

00:05:18,330 --> 00:05:24,419
ID and then return the user ID more or

00:05:21,930 --> 00:05:28,590
less the SQL equivalent of select ID

00:05:24,419 --> 00:05:31,050
from users where ID equals something see

00:05:28,590 --> 00:05:33,270
that a lot the database does not need to

00:05:31,050 --> 00:05:37,110
be in the loop here at all you can save

00:05:33,270 --> 00:05:40,560
yourself that request also data you

00:05:37,110 --> 00:05:42,150
don't need for example returning an

00:05:40,560 --> 00:05:44,550
entire table in order to get the first

00:05:42,150 --> 00:05:46,919
row looks great when there's only one

00:05:44,550 --> 00:05:49,490
row on the table when there's 10 million

00:05:46,919 --> 00:05:53,370
rows in the table does not work so well

00:05:49,490 --> 00:05:57,300
so avoid some of this if you have very

00:05:53,370 --> 00:05:58,919
wide tables use some of the values list

00:05:57,300 --> 00:06:02,159
methods in order to return only the

00:05:58,919 --> 00:06:04,500
columns you need particularly if the

00:06:02,159 --> 00:06:06,090
table has large objects in it you know

00:06:04,500 --> 00:06:08,599
by day fields large text that sort of

00:06:06,090 --> 00:06:11,280
thing that can make a huge difference

00:06:08,599 --> 00:06:14,940
another thing that we see a lot a lot is

00:06:11,280 --> 00:06:17,669
doing joins in the application code so

00:06:14,940 --> 00:06:19,590
that is i've got--let let's get this

00:06:17,669 --> 00:06:21,000
list of things and then let's take an

00:06:19,590 --> 00:06:23,610
idea from that list of things and let's

00:06:21,000 --> 00:06:26,610
loop over that and request each related

00:06:23,610 --> 00:06:29,909
set of rows from the database one at a

00:06:26,610 --> 00:06:32,639
time this means that if my roster

00:06:29,909 --> 00:06:35,250
actually has 150 players in it that I'm

00:06:32,639 --> 00:06:37,979
going to be actually making 150 separate

00:06:35,250 --> 00:06:39,389
requests to the database each round-trip

00:06:37,979 --> 00:06:40,590
with its own latency to get those

00:06:39,389 --> 00:06:44,550
players games

00:06:40,590 --> 00:06:46,710
you know we've spent a good 15 20 years

00:06:44,550 --> 00:06:49,530
in post grassland optimizing joints and

00:06:46,710 --> 00:06:53,910
trying to make them perform fast let us

00:06:49,530 --> 00:06:56,070
do our job you know use the multi model

00:06:53,910 --> 00:06:58,650
stuff so that it gets passed down as a

00:06:56,070 --> 00:07:00,900
join into the back end of the database

00:06:58,650 --> 00:07:03,540
and have the database returned a joined

00:07:00,900 --> 00:07:07,490
result set instead of doing what amounts

00:07:03,540 --> 00:07:12,390
to an S loop join in your application

00:07:07,490 --> 00:07:14,100
now having limited everything and

00:07:12,390 --> 00:07:16,440
starting to do less the second thing

00:07:14,100 --> 00:07:19,050
that we can actually look at is let's

00:07:16,440 --> 00:07:22,110
get rid of some resource hungry requests

00:07:19,050 --> 00:07:24,810
or fix them if you actually do a lot of

00:07:22,110 --> 00:07:26,820
database performance optimization like I

00:07:24,810 --> 00:07:28,980
do one of these you discover is that a

00:07:26,820 --> 00:07:32,040
tiny fraction of the requests against

00:07:28,980 --> 00:07:35,280
the database consume a vast majority of

00:07:32,040 --> 00:07:37,590
the system resources and that you really

00:07:35,280 --> 00:07:39,180
only care about those yeah maybe all of

00:07:37,590 --> 00:07:40,650
that stuff and that longtail is not as

00:07:39,180 --> 00:07:42,120
efficient as it could be but you don't

00:07:40,650 --> 00:07:44,100
care because it won't make that much of

00:07:42,120 --> 00:07:46,140
a difference to fix it so let's find

00:07:44,100 --> 00:07:47,550
those one of the really good tools to

00:07:46,140 --> 00:07:50,220
actually find these and Postgres is this

00:07:47,550 --> 00:07:52,830
thing called PG badger and what happened

00:07:50,220 --> 00:07:56,580
how PG badger works is that you turn on

00:07:52,830 --> 00:07:59,250
all of the logging for Postgres on every

00:07:56,580 --> 00:08:00,630
logging option it's got and then you

00:07:59,250 --> 00:08:02,250
cook those logs and after a while and

00:08:00,630 --> 00:08:04,190
you run through this program it is purl

00:08:02,250 --> 00:08:06,630
but you don't have to hack it to use it

00:08:04,190 --> 00:08:08,190
and then it provides you with this

00:08:06,630 --> 00:08:11,010
incredibly detailed report of everything

00:08:08,190 --> 00:08:12,480
you're doing and what is you really care

00:08:11,010 --> 00:08:14,550
about for the slow request things it's

00:08:12,480 --> 00:08:16,740
got this lovely sort of top query report

00:08:14,550 --> 00:08:18,240
you know so it's individual queries

00:08:16,740 --> 00:08:19,620
generally time consuming queries most

00:08:18,240 --> 00:08:22,680
frequent queries etc so this is like

00:08:19,620 --> 00:08:24,950
like the queries that overall you know

00:08:22,680 --> 00:08:27,000
through repetition or through

00:08:24,950 --> 00:08:28,140
individually running slow took up the

00:08:27,000 --> 00:08:29,940
most resources and those are the ones

00:08:28,140 --> 00:08:31,320
that you want to fix and then once

00:08:29,940 --> 00:08:33,270
you've looked at those you start looking

00:08:31,320 --> 00:08:36,570
at ways to fix them one of the big ways

00:08:33,270 --> 00:08:38,550
is adding indexes because you discover

00:08:36,570 --> 00:08:40,590
hey I'm doing this look up on this one

00:08:38,550 --> 00:08:42,570
column that has no indexes all the time

00:08:40,590 --> 00:08:46,830
and now that my table is a million rows

00:08:42,570 --> 00:08:50,250
that's kind of bad or fixing your filter

00:08:46,830 --> 00:08:52,080
expressions fixing you know your

00:08:50,250 --> 00:08:53,610
searches and that sort of thing so that

00:08:52,080 --> 00:08:57,510
they can use indexes

00:08:53,610 --> 00:09:00,720
because for example if you're comparing

00:08:57,510 --> 00:09:03,480
a date value to something via using date

00:09:00,720 --> 00:09:04,649
time in Python then we're not gonna be

00:09:03,480 --> 00:09:07,140
able to use an index on back-end

00:09:04,649 --> 00:09:09,779
Postgres to do that you're gonna have to

00:09:07,140 --> 00:09:13,140
slurp up the whole table sometimes

00:09:09,779 --> 00:09:15,029
Postgres stats get out of date that can

00:09:13,140 --> 00:09:16,980
be because you've got a weird update

00:09:15,029 --> 00:09:18,959
pattern so auto analyzes and keeping up

00:09:16,980 --> 00:09:21,630
it could be because you turn to auto

00:09:18,959 --> 00:09:26,190
vacuum / auto analyze off which is a bad

00:09:21,630 --> 00:09:27,630
idea and some you know or you just bulk

00:09:26,190 --> 00:09:33,560
load at a table and need to run a manual

00:09:27,630 --> 00:09:35,820
analyze some other specialty tips the

00:09:33,560 --> 00:09:38,640
Gengo methods allow you to do a lot of

00:09:35,820 --> 00:09:41,370
text searching an important thing to

00:09:38,640 --> 00:09:43,320
understand is that by default that text

00:09:41,370 --> 00:09:47,940
searching is not supported by indexes in

00:09:43,320 --> 00:09:50,550
most databases even starts with if your

00:09:47,940 --> 00:09:53,899
database is not built with C encoding

00:09:50,550 --> 00:09:56,880
that if it's built with actual Unicode

00:09:53,899 --> 00:09:58,470
then you actually need to create a

00:09:56,880 --> 00:09:59,850
special index that will support starts

00:09:58,470 --> 00:10:03,029
with which is this thing called ver care

00:09:59,850 --> 00:10:03,990
pattern ops search on that string

00:10:03,029 --> 00:10:06,029
there's there's more detailed

00:10:03,990 --> 00:10:07,980
instructions you'll find by Google and

00:10:06,029 --> 00:10:09,630
how to do that if you need to do

00:10:07,980 --> 00:10:11,790
case-insensitive then you need to either

00:10:09,630 --> 00:10:14,700
use a function or use case insensitive

00:10:11,790 --> 00:10:17,339
text and Postgres or for i contains

00:10:14,700 --> 00:10:20,399
you'd basically need to look at some of

00:10:17,339 --> 00:10:25,410
Django's extensions that support

00:10:20,399 --> 00:10:28,170
Postgres full-text search now most

00:10:25,410 --> 00:10:30,600
people don't use explicit transactions

00:10:28,170 --> 00:10:32,160
with their django applications but some

00:10:30,600 --> 00:10:33,779
people do if you're dealing you know

00:10:32,160 --> 00:10:35,820
financial stuff or other issues where

00:10:33,779 --> 00:10:37,860
you need to actually have atomic multi

00:10:35,820 --> 00:10:44,220
statement transactions you can get into

00:10:37,860 --> 00:10:46,230
trouble in a few areas one is don't do

00:10:44,220 --> 00:10:50,100
write read read read read read read read

00:10:46,230 --> 00:10:52,260
read read commit because we're waiting

00:10:50,100 --> 00:10:54,870
for all of those reads before we do the

00:10:52,260 --> 00:10:56,630
commit if those reads don't need to be

00:10:54,870 --> 00:11:01,500
part of the transaction take them out a

00:10:56,630 --> 00:11:04,320
worse thing is don't do begin you know

00:11:01,500 --> 00:11:05,940
let's begin let's write some stuff let's

00:11:04,320 --> 00:11:06,990
read some stuff now let's render some

00:11:05,940 --> 00:11:10,140
pages for the user

00:11:06,990 --> 00:11:15,480
look at oh now let's commit the

00:11:10,140 --> 00:11:17,459
transaction and the worst would be let's

00:11:15,480 --> 00:11:18,930
wait for a user response and now send

00:11:17,459 --> 00:11:20,760
another query in the same transaction

00:11:18,930 --> 00:11:22,020
because well that's happening what's

00:11:20,760 --> 00:11:23,520
happening on the database servers we

00:11:22,020 --> 00:11:25,800
have what's called an idle transaction

00:11:23,520 --> 00:11:30,360
and idle transactions pin-down resources

00:11:25,800 --> 00:11:32,399
especially locks and the issue with

00:11:30,360 --> 00:11:35,850
locks is that locks can block other

00:11:32,399 --> 00:11:37,350
activity this is a quick query check to

00:11:35,850 --> 00:11:39,630
check for queries that are being blocked

00:11:37,350 --> 00:11:41,370
by locks and the terrible thing about

00:11:39,630 --> 00:11:42,990
being blocked by locks is there is no

00:11:41,370 --> 00:11:45,290
amount of system resources you can throw

00:11:42,990 --> 00:11:49,680
at things that will make stuff better

00:11:45,290 --> 00:11:51,029
because locks are self-limiting so this

00:11:49,680 --> 00:11:52,860
is something to look at and something to

00:11:51,029 --> 00:11:57,120
think about if you're doing concurrent

00:11:52,860 --> 00:11:59,339
writes now second portion is let's get

00:11:57,120 --> 00:12:00,930
some adequate hardware you notice I'm

00:11:59,339 --> 00:12:03,149
not saying the best hardwood because

00:12:00,930 --> 00:12:05,010
really the best hardware is the hardware

00:12:03,149 --> 00:12:07,320
that is fast enough for what you need

00:12:05,010 --> 00:12:08,820
and no more because you don't want to

00:12:07,320 --> 00:12:12,690
spend extra money for performance you

00:12:08,820 --> 00:12:16,950
don't actually need now the corollary to

00:12:12,690 --> 00:12:19,890
this is that at its best postcodes will

00:12:16,950 --> 00:12:22,470
be as fast as your hardware we can't be

00:12:19,890 --> 00:12:25,829
faster than your hardware right if

00:12:22,470 --> 00:12:29,010
you're throwing it on an AWS tee one

00:12:25,829 --> 00:12:31,140
tiny do not expect to serve 25,000

00:12:29,010 --> 00:12:35,399
requests per second it's not going to

00:12:31,140 --> 00:12:37,970
happen one of the areas where people

00:12:35,399 --> 00:12:42,630
tend to under resource chronically is IO

00:12:37,970 --> 00:12:44,700
partly because in hosting and virtual

00:12:42,630 --> 00:12:46,740
hosting the various hosts make IO your

00:12:44,700 --> 00:12:48,920
most expensive resource and for that

00:12:46,740 --> 00:12:51,510
reason people tend to under allocate it

00:12:48,920 --> 00:12:52,740
and as a result they end up with crappy

00:12:51,510 --> 00:12:56,100
performance even though they have plenty

00:12:52,740 --> 00:12:58,920
of CPU and RAM left available posters

00:12:56,100 --> 00:13:00,839
write stuff all the time you know

00:12:58,920 --> 00:13:04,260
obviously stuff like writes and commits

00:13:00,839 --> 00:13:06,300
but even on a read mostly workload

00:13:04,260 --> 00:13:08,430
Postgres is doing things like writing to

00:13:06,300 --> 00:13:09,690
support replication doing this thing

00:13:08,430 --> 00:13:11,190
called hint bits which I don't want to

00:13:09,690 --> 00:13:12,360
explain but it does involve a lot of

00:13:11,190 --> 00:13:15,690
sort of constant background writes

00:13:12,360 --> 00:13:17,339
writing statistics about what's in the

00:13:15,690 --> 00:13:21,480
database so

00:13:17,339 --> 00:13:23,249
if you are limited by I opsin throughput

00:13:21,480 --> 00:13:25,050
on your system that is going to limit

00:13:23,249 --> 00:13:27,899
post-chorus performance so get adequate

00:13:25,050 --> 00:13:31,470
IO examples here if you're on your own

00:13:27,899 --> 00:13:32,699
hardware just go ahead and move to SSDs

00:13:31,470 --> 00:13:34,230
if you haven't already there's no good

00:13:32,699 --> 00:13:36,269
reason not to they're not even more

00:13:34,230 --> 00:13:39,300
expensive anymore

00:13:36,269 --> 00:13:40,920
if you're in the cloud look at your eye

00:13:39,300 --> 00:13:43,620
ops allocation and the storage that

00:13:40,920 --> 00:13:45,569
you're at again increasing it is not

00:13:43,620 --> 00:13:47,339
that expensive and can make a quantum a

00:13:45,569 --> 00:13:49,980
an order of magnitude difference in

00:13:47,339 --> 00:13:54,180
performance also for stuff anybody

00:13:49,980 --> 00:13:58,139
they're still using ext3 on linux thank

00:13:54,180 --> 00:14:00,059
goodness there's also been some Linux

00:13:58,139 --> 00:14:02,910
kernel issues in the recent past you can

00:14:00,059 --> 00:14:07,649
read about these that make for terrible

00:14:02,910 --> 00:14:11,129
i/o performance now in terms of RAM it's

00:14:07,649 --> 00:14:12,569
completely thresholded you basically

00:14:11,129 --> 00:14:15,120
have sort of your three thresholds of

00:14:12,569 --> 00:14:16,290
Ram for a functional system one is that

00:14:15,120 --> 00:14:18,059
you can cache the data that you need

00:14:16,290 --> 00:14:19,410
most of the time the second is you can

00:14:18,059 --> 00:14:22,290
cache the whole database and the third

00:14:19,410 --> 00:14:27,079
is that it fits in Postgres is dedicated

00:14:22,290 --> 00:14:30,240
cache which is a minority of ram the and

00:14:27,079 --> 00:14:33,000
you know where this fits into allocating

00:14:30,240 --> 00:14:34,559
ram is if you're in one of these sort of

00:14:33,000 --> 00:14:36,329
thresholds and by getting just a little

00:14:34,559 --> 00:14:38,279
bit more ram you could actually move up

00:14:36,329 --> 00:14:41,370
to the better threshold it's generally

00:14:38,279 --> 00:14:46,439
worth doing some tips here for Amazon

00:14:41,370 --> 00:14:48,300
Web Services use the currently general

00:14:46,439 --> 00:14:50,879
provision over allocate the heck out of

00:14:48,300 --> 00:14:52,620
your storage and you will actually get

00:14:50,879 --> 00:14:54,540
better throughput then than you do with

00:14:52,620 --> 00:14:56,910
other options

00:14:54,540 --> 00:14:57,839
Postgres as a service which is offered

00:14:56,910 --> 00:15:02,370
by various companies

00:14:57,839 --> 00:15:04,230
gondor Heroku etc that saves you an

00:15:02,370 --> 00:15:06,449
administration it doesn't help you with

00:15:04,230 --> 00:15:08,009
performance necessarily so if you are

00:15:06,449 --> 00:15:10,829
actually performance constrained by the

00:15:08,009 --> 00:15:13,499
database then you know you might end up

00:15:10,829 --> 00:15:16,050
actually branching off on your own oh

00:15:13,499 --> 00:15:21,179
and make sure that your app servers and

00:15:16,050 --> 00:15:26,040
I postgrads are in the same availability

00:15:21,179 --> 00:15:27,449
zone because latency can kill you talk a

00:15:26,040 --> 00:15:28,559
little bit about scaling infrastructure

00:15:27,449 --> 00:15:30,360
because assume that you've actually got

00:15:28,559 --> 00:15:30,790
adequate hardware and that's not doing

00:15:30,360 --> 00:15:32,470
it for you

00:15:30,790 --> 00:15:34,960
we actually need to scale infrastructure

00:15:32,470 --> 00:15:37,690
out so here's our first sort of easy

00:15:34,960 --> 00:15:39,760
things one is use the latest version to

00:15:37,690 --> 00:15:41,410
PostgreSQL we put performance

00:15:39,760 --> 00:15:44,410
improvements of various kinds in every

00:15:41,410 --> 00:15:46,000
release so upgrading is worthwhile and

00:15:44,410 --> 00:15:49,420
make sure the Postgres is running on its

00:15:46,000 --> 00:15:51,100
own server instance databases tend to

00:15:49,420 --> 00:15:52,540
use all the resources which means they

00:15:51,100 --> 00:15:57,820
don't share well with other kinds of

00:15:52,540 --> 00:16:00,370
applications then the other thing is use

00:15:57,820 --> 00:16:02,260
PG bouncer with connection pooling

00:16:00,370 --> 00:16:06,010
specifically the transaction pooling

00:16:02,260 --> 00:16:08,230
with your application because on post

00:16:06,010 --> 00:16:10,780
rez extra connections even if they're

00:16:08,230 --> 00:16:12,040
idle cost resources and so if you have

00:16:10,780 --> 00:16:14,260
hundreds of extra connections you're

00:16:12,040 --> 00:16:17,320
paying for that the way a PD bouncer

00:16:14,260 --> 00:16:19,570
works is it's an event based Pooler that

00:16:17,320 --> 00:16:21,250
connections come in but they only get

00:16:19,570 --> 00:16:24,010
allocated a real database connection

00:16:21,250 --> 00:16:26,410
when you actually have a query to run or

00:16:24,010 --> 00:16:27,790
when you're in a transaction and that

00:16:26,410 --> 00:16:31,960
saves you a lot of resources on the

00:16:27,790 --> 00:16:34,210
database side if that's not enough then

00:16:31,960 --> 00:16:36,640
if you have replicas anyway for

00:16:34,210 --> 00:16:38,200
redundancy let's look at load balancing

00:16:36,640 --> 00:16:47,830
some of those read requests to your

00:16:38,200 --> 00:16:49,090
replicas the and you know the now load

00:16:47,830 --> 00:16:50,950
balancing is requires some kind of a

00:16:49,090 --> 00:16:53,350
proxy there are various third-party

00:16:50,950 --> 00:16:55,150
proxies out there you can use PG bounce

00:16:53,350 --> 00:16:57,100
are kind of in this way H a proxy that

00:16:55,150 --> 00:16:59,550
sort of thing but really the easiest way

00:16:57,100 --> 00:17:02,140
is to actually just use django routes

00:16:59,550 --> 00:17:04,480
and it's the most effective way because

00:17:02,140 --> 00:17:06,250
only within the application code do you

00:17:04,480 --> 00:17:09,490
know if you're about to do a reader or

00:17:06,250 --> 00:17:11,050
write and for that reason using django

00:17:09,490 --> 00:17:13,090
routes and actually having a read

00:17:11,050 --> 00:17:16,630
connection and a write connection is

00:17:13,090 --> 00:17:19,450
going to help you a lot for this now you

00:17:16,630 --> 00:17:22,180
can load balanced read you can load

00:17:19,450 --> 00:17:24,460
balanced generically but it's much more

00:17:22,180 --> 00:17:26,560
efficient to actually load balanced

00:17:24,460 --> 00:17:28,660
specific portions of your workload for

00:17:26,560 --> 00:17:30,700
example if you can move any sort of

00:17:28,660 --> 00:17:33,130
large reports and analytics you do off

00:17:30,700 --> 00:17:36,220
on to a separate server a separate read

00:17:33,130 --> 00:17:38,170
replica you can actually post go as

00:17:36,220 --> 00:17:40,120
performance optimizes on its own a lot

00:17:38,170 --> 00:17:42,040
better when it has a consistent workload

00:17:40,120 --> 00:17:43,510
and then you can actually even do some

00:17:42,040 --> 00:17:44,380
manual tuning for that consistent

00:17:43,510 --> 00:17:45,790
workload

00:17:44,380 --> 00:17:48,310
as opposed to a mix in completely

00:17:45,790 --> 00:17:49,870
chaotic workload and so moving say

00:17:48,310 --> 00:17:51,910
reporting off onto its own machine

00:17:49,870 --> 00:17:53,500
moving the machine where you're actually

00:17:51,910 --> 00:17:56,190
pulling entire tables in order to

00:17:53,500 --> 00:17:58,450
refresh cash on to its own read replica

00:17:56,190 --> 00:18:00,190
moving queueing if anybody is doing like

00:17:58,450 --> 00:18:02,890
celery backed by postcards or whatever

00:18:00,190 --> 00:18:04,570
that has its own very specific access

00:18:02,890 --> 00:18:07,000
pattern that's a lot easier for

00:18:04,570 --> 00:18:08,530
postcards to cope with if that's all

00:18:07,000 --> 00:18:13,000
that that particular instance the post

00:18:08,530 --> 00:18:14,050
course is doing now having gone through

00:18:13,000 --> 00:18:15,400
all the stuff to scale the

00:18:14,050 --> 00:18:16,900
infrastructure I am actually going to

00:18:15,400 --> 00:18:18,220
have some notes in Postma's Kampf but

00:18:16,900 --> 00:18:19,420
I'm doing this last because like I said

00:18:18,220 --> 00:18:22,480
it really is actually the least

00:18:19,420 --> 00:18:25,210
important thing so I've got some

00:18:22,480 --> 00:18:28,030
configuration stuff in here I will put

00:18:25,210 --> 00:18:29,410
up the slides later on so you don't

00:18:28,030 --> 00:18:31,750
actually need to write this out there's

00:18:29,410 --> 00:18:33,850
basically post Chris has two hundred and

00:18:31,750 --> 00:18:37,590
thirty-some configuration variables you

00:18:33,850 --> 00:18:41,290
only care about a tiny handful of these

00:18:37,590 --> 00:18:43,630
here's a few of them one is we can't

00:18:41,290 --> 00:18:45,580
determine in Postgres automatically how

00:18:43,630 --> 00:18:46,870
much ram is available to Postgres so

00:18:45,580 --> 00:18:48,760
there's a few settings that you need to

00:18:46,870 --> 00:18:51,400
configure based on the amount of RAM

00:18:48,760 --> 00:18:52,840
available to Postgres one of them is

00:18:51,400 --> 00:18:54,490
shared buffers which is post courses

00:18:52,840 --> 00:18:57,940
dedicated cash should be about a quarter

00:18:54,490 --> 00:19:00,730
of Ram work memory is non shared memory

00:18:57,940 --> 00:19:05,320
limits for doing per query operations

00:19:00,730 --> 00:19:07,690
like sorts you know again 8 to 32

00:19:05,320 --> 00:19:10,420
megabytes for your basic web application

00:19:07,690 --> 00:19:13,270
maybe 128 to one gigabyte for reporting

00:19:10,420 --> 00:19:14,920
analytics application don't over

00:19:13,270 --> 00:19:16,900
allocate city you don't actually run out

00:19:14,920 --> 00:19:17,920
of RAM the reason we have a limit there

00:19:16,900 --> 00:19:21,580
is because you don't want to run out of

00:19:17,920 --> 00:19:23,200
RAM other ones are simpler effective

00:19:21,580 --> 00:19:24,190
cache size just basically tells post

00:19:23,200 --> 00:19:30,310
cores how much space you have for

00:19:24,190 --> 00:19:32,580
caching so 3/4 of RAM we've got one in

00:19:30,310 --> 00:19:34,810
there called wall buffers and that's

00:19:32,580 --> 00:19:37,870
it's a complicated explanation but just

00:19:34,810 --> 00:19:39,550
set it to 64 gigabytes maintenance work

00:19:37,870 --> 00:19:42,030
memory is the memory available for

00:19:39,550 --> 00:19:44,350
things like auto vacuum auto analyze

00:19:42,030 --> 00:19:47,650
there are some settings in the determine

00:19:44,350 --> 00:19:50,380
the size and the rate of refresh for the

00:19:47,650 --> 00:19:52,210
transaction log the defaults for this

00:19:50,380 --> 00:19:55,330
are kind of low at least until post was

00:19:52,210 --> 00:19:57,760
9.5 comes out and so you generally want

00:19:55,330 --> 00:20:00,220
to actually bump them up

00:19:57,760 --> 00:20:03,279
and a few other settings moving stats

00:20:00,220 --> 00:20:05,019
the statistics to a ram disk can improve

00:20:03,279 --> 00:20:09,760
responsiveness a lot especially in the

00:20:05,019 --> 00:20:11,649
cloud for SSDs and for cloud you

00:20:09,760 --> 00:20:14,049
actually want to decrease random page

00:20:11,649 --> 00:20:15,669
cost which is the cost factor of

00:20:14,049 --> 00:20:17,580
postcodes looks in am I going to use an

00:20:15,669 --> 00:20:19,269
index Reming in ascii in the table

00:20:17,580 --> 00:20:22,870
same thing with effective i/o

00:20:19,269 --> 00:20:24,820
concurrency this by the way just put in

00:20:22,870 --> 00:20:26,799
here for I said turn on all the logging

00:20:24,820 --> 00:20:29,409
settings for PG Bajor this is that set

00:20:26,799 --> 00:20:35,139
of settings and it's in the slide so

00:20:29,409 --> 00:20:36,519
that you know it for later so recap to

00:20:35,139 --> 00:20:38,380
get postal performance number one do

00:20:36,519 --> 00:20:41,320
less querying fix your resource hungry

00:20:38,380 --> 00:20:43,389
requests get adequate Hardware scale

00:20:41,320 --> 00:20:44,590
your infrastructure and then finally you

00:20:43,389 --> 00:20:45,820
know if you've done all of those things

00:20:44,590 --> 00:20:48,179
or if you're waiting for some of those

00:20:45,820 --> 00:20:51,179
things tune the configuration file so

00:20:48,179 --> 00:20:51,179
questions

00:20:56,580 --> 00:21:02,160
you have three minutes for questions so

00:20:59,410 --> 00:21:02,160
if anyone has a question

00:21:03,210 --> 00:21:08,940
nobody everything is running as fast as

00:21:06,070 --> 00:21:08,940
you could possibly want

00:21:14,100 --> 00:21:22,240
are there any performance hits for all

00:21:17,140 --> 00:21:23,950
that logging I there there is

00:21:22,240 --> 00:21:25,180
performance overhead involved in the

00:21:23,950 --> 00:21:27,160
logging because you're doing a lot of

00:21:25,180 --> 00:21:28,960
writing that performance overhead is

00:21:27,160 --> 00:21:30,309
variable depending on how many queries

00:21:28,960 --> 00:21:31,809
you're actually running per second how

00:21:30,309 --> 00:21:34,630
long in those queries that sort of thing

00:21:31,809 --> 00:21:37,330
and is the activity log being stored on

00:21:34,630 --> 00:21:40,840
the same IO resource as the rest of the

00:21:37,330 --> 00:21:44,170
database so I've seen that overhead be

00:21:40,840 --> 00:21:47,230
anywhere from not measurable to you know

00:21:44,170 --> 00:21:49,300
i if you are already close to i/o

00:21:47,230 --> 00:21:51,370
saturated and it's on the same resources

00:21:49,300 --> 00:21:59,290
the database to actually causing serious

00:21:51,370 --> 00:22:02,020
problems so it varies could you expand a

00:21:59,290 --> 00:22:04,150
little more on analyzing and vacuuming

00:22:02,020 --> 00:22:08,530
and Auto analyzing and Auto vacuuming

00:22:04,150 --> 00:22:10,690
and what you know when you I know

00:22:08,530 --> 00:22:12,520
there's a some table you can query that

00:22:10,690 --> 00:22:14,650
shows you the last time that was

00:22:12,520 --> 00:22:16,960
performed on certain tables and yeah

00:22:14,650 --> 00:22:20,500
what stops look yes there's too much of

00:22:16,960 --> 00:22:22,540
this vacuuming is deferred is garbage

00:22:20,500 --> 00:22:24,670
collection for Postgres what we're doing

00:22:22,540 --> 00:22:26,590
is were garbage collecting all of the

00:22:24,670 --> 00:22:29,230
rows that are dead because they've been

00:22:26,590 --> 00:22:30,730
replaced by new rows are deleted and

00:22:29,230 --> 00:22:32,200
cleaning up some other stuff cleaning up

00:22:30,730 --> 00:22:34,270
there the index reference and that sort

00:22:32,200 --> 00:22:36,640
of thing and we do that asynchronously

00:22:34,270 --> 00:22:39,460
deferred because we don't want user

00:22:36,640 --> 00:22:43,080
requests to wait on that vacuum but if

00:22:39,460 --> 00:22:45,640
vacuum can't keep up for some reason

00:22:43,080 --> 00:22:47,950
then you end up getting what we call

00:22:45,640 --> 00:22:49,420
bloated tables and bloated indexes that

00:22:47,950 --> 00:22:54,750
have a lot of dead space in them that

00:22:49,420 --> 00:22:58,030
hasn't been collected now analyze is

00:22:54,750 --> 00:22:59,770
updating the Postgres statistics about

00:22:58,030 --> 00:23:02,290
what's in your table so that we can

00:22:59,770 --> 00:23:05,530
actually plan to execute their queries

00:23:02,290 --> 00:23:06,790
in the fastest way now both of these

00:23:05,530 --> 00:23:08,790
things are normally handled by something

00:23:06,790 --> 00:23:13,860
called the auto vacuum demon that you

00:23:08,790 --> 00:23:16,140
is I that uses threshold algorithm to

00:23:13,860 --> 00:23:19,740
determine when it needs to vacuum and

00:23:16,140 --> 00:23:22,410
analyze various tables but if you have

00:23:19,740 --> 00:23:24,030
an atypical usage pattern sometimes the

00:23:22,410 --> 00:23:28,170
auto vacuum daemon doesn't recognize

00:23:24,030 --> 00:23:29,580
when it needs to work on things or if

00:23:28,170 --> 00:23:31,230
you just have an enormous amount of

00:23:29,580 --> 00:23:33,090
activity under the default settings the

00:23:31,230 --> 00:23:34,710
auto vacuum daemon might not keep up and

00:23:33,090 --> 00:23:36,960
you just need to bump up the number of

00:23:34,710 --> 00:23:38,430
workers and their frequency of work how

00:23:36,960 --> 00:23:40,860
do you identify that like if it's not

00:23:38,430 --> 00:23:42,090
keeping up so bloated tables and search

00:23:40,860 --> 00:23:43,230
for post cards bloated tables and

00:23:42,090 --> 00:23:47,520
there's some query examples for how you

00:23:43,230 --> 00:23:48,420
find it and if you're seeing a lot of

00:23:47,520 --> 00:23:49,890
queries and unfortunately you have to

00:23:48,420 --> 00:23:51,600
get into sort of analyzing the queries

00:23:49,890 --> 00:23:56,300
to find out the queries are bad because

00:23:51,600 --> 00:23:56,300
your stats are bad okay thanks yep

00:23:56,420 --> 00:24:02,970
you mentioned no more ext3 do you have a

00:24:00,900 --> 00:24:06,060
preferred recommendation for a file

00:24:02,970 --> 00:24:07,890
system for Postgres instance and does

00:24:06,060 --> 00:24:10,500
that decision change whether you're

00:24:07,890 --> 00:24:12,870
talking about physical disk or SSD yeah

00:24:10,500 --> 00:24:17,970
it doesn't change in terms of physical

00:24:12,870 --> 00:24:22,740
disk versus SSD but I generally say I

00:24:17,970 --> 00:24:27,540
use XFS for smaller transactional

00:24:22,740 --> 00:24:32,000
databases if I have a choice and I use

00:24:27,540 --> 00:24:34,890
ZFS a lot for data warehousing analytics

00:24:32,000 --> 00:24:36,900
and the reason for that is that ZFS has

00:24:34,890 --> 00:24:40,050
a lot of nice tools for volume extension

00:24:36,900 --> 00:24:41,820
and copying and that sort of thing but

00:24:40,050 --> 00:24:46,200
it tends to be a little slow on small

00:24:41,820 --> 00:24:47,580
writes and ext4 is fine if that's what's

00:24:46,200 --> 00:24:54,720
installed and you have to get special

00:24:47,580 --> 00:24:56,430
permission to use XFS there you

00:24:54,720 --> 00:24:58,770
mentioned SSDs if you're running your

00:24:56,430 --> 00:25:02,970
own hardware but cloud providers are

00:24:58,770 --> 00:25:04,860
also provided offering SSDs yeah so you

00:25:02,970 --> 00:25:06,660
weren't saying not to do it there it's

00:25:04,860 --> 00:25:09,270
not well yeah I meant I meant actually

00:25:06,660 --> 00:25:15,810
hardware versus versus virtual hardware

00:25:09,270 --> 00:25:18,180
versus yes yeah the there used to be

00:25:15,810 --> 00:25:21,510
some sort of trade-off vs HDD versus SSD

00:25:18,180 --> 00:25:22,620
these days the only reason why you would

00:25:21,510 --> 00:25:24,720
use an HDD

00:25:22,620 --> 00:25:26,520
is if you have large volumes of data as

00:25:24,720 --> 00:25:28,290
in multiple terabytes and you're willing

00:25:26,520 --> 00:25:29,730
to live with it being slow in order to

00:25:28,290 --> 00:25:34,230
save some money that would be the only

00:25:29,730 --> 00:25:36,690
reason to use spinning disk these days I

00:25:34,230 --> 00:25:38,850
come from a sad world of Microsoft where

00:25:36,690 --> 00:25:40,470
index fragmentation matters and is

00:25:38,850 --> 00:25:44,250
wondering if that matters in Postgres

00:25:40,470 --> 00:25:51,180
and how you deal with it yes it does it

00:25:44,250 --> 00:25:56,100
does I in the there's a couple of things

00:25:51,180 --> 00:25:57,420
there one is put if if postcodes is

00:25:56,100 --> 00:26:01,190
being stored in like a SAN or other

00:25:57,420 --> 00:26:04,559
network share give it its own partition

00:26:01,190 --> 00:26:06,480
so that you get less fragmentation and

00:26:04,559 --> 00:26:08,370
interleaving with other files stored on

00:26:06,480 --> 00:26:16,980
the sand because that can be actually

00:26:08,370 --> 00:26:19,590
pretty bad the the only thing that the

00:26:16,980 --> 00:26:21,059
other things to reduce fragmentation not

00:26:19,590 --> 00:26:24,830
really a lot of other things you can do

00:26:21,059 --> 00:26:24,830
to reduce fragmentation except for

00:26:25,190 --> 00:26:29,780
changing how data gets into the database

00:26:27,360 --> 00:26:33,270
which is obviously an application change

00:26:29,780 --> 00:26:37,679
you know or you know obviously re

00:26:33,270 --> 00:26:38,880
copying stuff so that is actually one of

00:26:37,679 --> 00:26:41,640
the problems with running Postgres and

00:26:38,880 --> 00:26:43,860
NTFS for whatever reason because of how

00:26:41,640 --> 00:26:45,210
NTFS writes files for AG mentation zuv

00:26:43,860 --> 00:26:47,730
getting much worse than it is underneath

00:26:45,210 --> 00:26:49,470
a Linux file systems and we haven't

00:26:47,730 --> 00:26:51,540
honestly really looked into why because

00:26:49,470 --> 00:26:52,620
our population of people who run posters

00:26:51,540 --> 00:26:54,630
and windows and care about performance

00:26:52,620 --> 00:26:58,429
is pretty small okay that's all the time

00:26:54,630 --> 00:26:58,429

YouTube URL: https://www.youtube.com/watch?v=dBeXS5aFLNc


