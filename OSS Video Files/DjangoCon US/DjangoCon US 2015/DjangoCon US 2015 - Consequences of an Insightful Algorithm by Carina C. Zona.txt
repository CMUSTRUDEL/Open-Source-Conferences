Title: DjangoCon US 2015 - Consequences of an Insightful Algorithm by Carina C. Zona
Publication date: 2017-11-03
Playlist: DjangoCon US 2015
Description: 
	Consequences of an Insightful Algorithm

We have ethical responsibilities when coding. We’re able to extract remarkably precise intuitions about an individual. But do we have a right to know what they didn’t consent to share, even when they willingly shared the data that leads us there? A major retailer’s data-driven marketing accidentially revealed to a teen’s family that she was pregnant. Eek.

What are our obligations to people who did not expect themselves to be so intimately known without sharing directly? How do we mitigate against unintended outcomes? For instance, an activity tracker carelessly revealed users’ sexual activity data to search engines. A social network’s algorithm accidentally triggered painful memories for grieving families who’d recently experienced death of their child and other loved ones.

We design software for humans. Balancing human needs and business specs can be tough. It’s crucial that we learn how to build in systematic empathy.

In this talk, we’ll delve into specific examples of uncritical programming, and painful results from using insightful data in ways that were benignly intended. You’ll learn ways we can integrate practices for examining how our code might harm individuals. We’ll look at how to flip the paradigm, netting consequences that can be better for everyone.

Help us caption & translate this video!

http://amara.org/v/HIYS/
Captions: 
	00:00:15,830 --> 00:00:21,960
we didn't do a mic level check it does

00:00:18,150 --> 00:00:24,869
work yay okay alright so yeah this talk

00:00:21,960 --> 00:00:27,180
is meant to be a tool kit for empathetic

00:00:24,869 --> 00:00:28,680
coding and we're gonna be delving into

00:00:27,180 --> 00:00:30,150
some pretty specific examples of

00:00:28,680 --> 00:00:31,949
uncritical programming so the first

00:00:30,150 --> 00:00:33,540
thing I want to do because we're gonna

00:00:31,949 --> 00:00:35,880
be talking about painful results of

00:00:33,540 --> 00:00:37,829
doing things in ways that are perfectly

00:00:35,880 --> 00:00:39,360
benignly intended and then sides white

00:00:37,829 --> 00:00:41,520
people is I'm not gonna do that to you

00:00:39,360 --> 00:00:43,620
so start with a Content warning here

00:00:41,520 --> 00:00:45,210
we're gonna be delving into examples

00:00:43,620 --> 00:00:47,910
that include a bunch of things including

00:00:45,210 --> 00:00:49,640
grief PTSD depression miscarriage

00:00:47,910 --> 00:00:52,170
infertility racial profiling

00:00:49,640 --> 00:00:54,120
concentration camps surveillance sexual

00:00:52,170 --> 00:00:55,649
history consent and assault and while we

00:00:54,120 --> 00:00:58,590
won't be dwelling on those that is

00:00:55,649 --> 00:01:00,449
topics that are going to come up all

00:00:58,590 --> 00:01:03,570
right so algorithms impose consequences

00:01:00,449 --> 00:01:06,450
on people all the time we're able to

00:01:03,570 --> 00:01:08,909
extract such remarkably precise insights

00:01:06,450 --> 00:01:10,380
about any individual but we haven't

00:01:08,909 --> 00:01:12,510
really asked ourselves enough do we have

00:01:10,380 --> 00:01:14,460
a right to know what they didn't consent

00:01:12,510 --> 00:01:17,009
to share with us even when they

00:01:14,460 --> 00:01:19,080
willingly share other data that ends up

00:01:17,009 --> 00:01:20,490
leading us there and we have to be

00:01:19,080 --> 00:01:23,220
asking ourselves more about how do we

00:01:20,490 --> 00:01:26,520
mitigate against unintended consequences

00:01:23,220 --> 00:01:28,080
of that stuff so it's helpful to step

00:01:26,520 --> 00:01:29,610
back a little and start with asking just

00:01:28,080 --> 00:01:31,890
a really basic question what is an

00:01:29,610 --> 00:01:34,470
algorithm and generically speaking it's

00:01:31,890 --> 00:01:36,509
just a step by step set of operations

00:01:34,470 --> 00:01:38,430
for predictively arriving at a

00:01:36,509 --> 00:01:41,700
conclusion and predictably is a really

00:01:38,430 --> 00:01:43,259
operative word in that so obviously when

00:01:41,700 --> 00:01:44,970
we talk about algorithms usually we meet

00:01:43,259 --> 00:01:46,860
in the computer science sense in the

00:01:44,970 --> 00:01:49,920
mathematics sense patterns of

00:01:46,860 --> 00:01:52,049
instructions articulated in code or in

00:01:49,920 --> 00:01:53,579
formulas but you can also think of

00:01:52,049 --> 00:01:55,649
algorithms as being part of everyday

00:01:53,579 --> 00:01:57,450
life all the time they're just patterns

00:01:55,649 --> 00:02:02,390
of instructions articulated in other

00:01:57,450 --> 00:02:06,600
ways such as for instance a recipe or a

00:02:02,390 --> 00:02:10,229
lovely shawl you know if your worst code

00:02:06,600 --> 00:02:12,440
nightmare looks like this you're doing

00:02:10,229 --> 00:02:12,440
well

00:02:13,120 --> 00:02:20,319
okay so deep learning is a particular

00:02:16,450 --> 00:02:22,870
new actually new old style of algorithms

00:02:20,319 --> 00:02:24,849
so the technology goes back to in theory

00:02:22,870 --> 00:02:28,420
at least the 1950s but there's been some

00:02:24,849 --> 00:02:30,340
recent breakthroughs since 2012 and 2013

00:02:28,420 --> 00:02:32,829
that have really reshaped the landscape

00:02:30,340 --> 00:02:34,510
of what's possible with machine learning

00:02:32,829 --> 00:02:37,329
it's really just the new hotness because

00:02:34,510 --> 00:02:41,290
of this oversimplified you can think of

00:02:37,329 --> 00:02:43,180
machine deep learning as algorithms for

00:02:41,290 --> 00:02:45,849
fast trainable artificial neural

00:02:43,180 --> 00:02:47,829
networks it's a technology that you know

00:02:45,849 --> 00:02:50,500
like I said it's been around in academia

00:02:47,829 --> 00:02:53,140
for instance for a long time but really

00:02:50,500 --> 00:02:57,129
mostly a theoretical scale breakthroughs

00:02:53,140 --> 00:02:58,900
in parallelization in availability of

00:02:57,129 --> 00:03:00,849
processors at massive scale these things

00:02:58,900 --> 00:03:02,200
are making it much more viable to

00:03:00,849 --> 00:03:05,590
actually take this now into production

00:03:02,200 --> 00:03:07,930
use so because of that deep learning has

00:03:05,590 --> 00:03:10,750
become realistically able to extract

00:03:07,930 --> 00:03:12,489
insights out of vast big data in ways

00:03:10,750 --> 00:03:16,060
that we've never really even been able

00:03:12,489 --> 00:03:17,590
to imagine about before deep learning is

00:03:16,060 --> 00:03:19,480
a particular approach to building and

00:03:17,590 --> 00:03:20,829
training artificial neural networks you

00:03:19,480 --> 00:03:24,819
could think of them as decision-making

00:03:20,829 --> 00:03:26,530
black boxes so what does that mean okay

00:03:24,819 --> 00:03:28,540
so we have some inputs you're just

00:03:26,530 --> 00:03:32,319
giving an array of numbers representing

00:03:28,540 --> 00:03:34,209
words concepts objects in some sense and

00:03:32,319 --> 00:03:36,609
then executing it by running a series of

00:03:34,209 --> 00:03:38,470
functions that you write and executing

00:03:36,609 --> 00:03:42,489
against the array and the outputs you

00:03:38,470 --> 00:03:44,379
get are the machine learning predicting

00:03:42,489 --> 00:03:46,780
properties that it thinks will be useful

00:03:44,379 --> 00:03:49,239
in the future for drawing intuitions

00:03:46,780 --> 00:03:50,859
about similar data sets so you give it

00:03:49,239 --> 00:03:52,480
this training data set it says hmm I

00:03:50,859 --> 00:03:54,099
noticed some patterns here this is

00:03:52,480 --> 00:03:55,930
probably how you can figure out similar

00:03:54,099 --> 00:03:58,180
results next time and then you can throw

00:03:55,930 --> 00:04:01,169
much larger j-just sets at it and it's

00:03:58,180 --> 00:04:03,129
able to come up with similar predictions

00:04:01,169 --> 00:04:05,739
so what would that look like for

00:04:03,129 --> 00:04:10,810
instance okay so there's a few problems

00:04:05,739 --> 00:04:14,319
here black box we'll get back to that in

00:04:10,810 --> 00:04:15,819
a second so because deep learning relies

00:04:14,319 --> 00:04:17,829
on an artificial neural networks

00:04:15,819 --> 00:04:19,930
automated discovery of patterns within

00:04:17,829 --> 00:04:21,459
that training data set it gets to apply

00:04:19,930 --> 00:04:24,010
those discoveries to draw intuitions

00:04:21,459 --> 00:04:25,480
about the future inputs however that

00:04:24,010 --> 00:04:27,400
means that every flaw or a so

00:04:25,480 --> 00:04:28,630
in the training data set or in those

00:04:27,400 --> 00:04:30,760
original functions that you've written

00:04:28,630 --> 00:04:32,470
are going to have unrecognized influence

00:04:30,760 --> 00:04:34,360
on the algorithms and on the outcomes

00:04:32,470 --> 00:04:36,190
that they generate so that's something

00:04:34,360 --> 00:04:37,780
that we're gonna revisit but let's look

00:04:36,190 --> 00:04:41,080
at a really just very sick practical

00:04:37,780 --> 00:04:43,060
example of deep learning ma Rio it's an

00:04:41,080 --> 00:04:46,270
artificial neural network that teaches

00:04:43,060 --> 00:04:48,460
itself how to play Super Mario World

00:04:46,270 --> 00:04:51,610
it starts with absolutely no clue

00:04:48,460 --> 00:04:54,010
whatsoever there's a great YouTube video

00:04:51,610 --> 00:04:57,010
of this by the way in action it spends

00:04:54,010 --> 00:04:58,780
24 hours simply manipulating numbers and

00:04:57,010 --> 00:05:00,550
seeing what happens when it changes like

00:04:58,780 --> 00:05:03,250
pixel locations for instance and

00:05:00,550 --> 00:05:08,320
discovers that after 24 hours by golly

00:05:03,250 --> 00:05:13,930
it can play the whole game I told you it

00:05:08,320 --> 00:05:15,640
was amazing so it's learning it's it is

00:05:13,930 --> 00:05:17,860
not being given rules it's not being

00:05:15,640 --> 00:05:23,770
given information it's exploring its

00:05:17,860 --> 00:05:25,840
world so can you imagine on the data

00:05:23,770 --> 00:05:28,540
sets that we have access to what we

00:05:25,840 --> 00:05:31,120
could do with that learning to identify

00:05:28,540 --> 00:05:33,550
patterns and then using patterns to draw

00:05:31,120 --> 00:05:39,570
insights this is something that we try

00:05:33,550 --> 00:05:39,570
to do all the time so speaking of games

00:05:41,100 --> 00:05:46,260
oops

00:05:42,460 --> 00:05:50,130
ruin my punchline here there we go

00:05:46,260 --> 00:05:50,130
would you like to play a nice game

00:05:51,360 --> 00:05:57,220
global Thorburn what is a global

00:05:53,770 --> 00:05:59,200
thermonuclear war yeah I love that word

00:05:57,220 --> 00:06:02,380
we're gonna play a different game

00:05:59,200 --> 00:06:04,240
and we'll call it bingo it is the

00:06:02,380 --> 00:06:06,940
screwiest game of bingo that you will

00:06:04,240 --> 00:06:09,340
ever play there are definitely odd

00:06:06,940 --> 00:06:11,140
angles but we're gonna be able to cover

00:06:09,340 --> 00:06:12,460
this board real well so let's take a

00:06:11,140 --> 00:06:16,870
look at some of the ways that we have

00:06:12,460 --> 00:06:18,160
serious pitfalls to deal with will we

00:06:16,870 --> 00:06:19,840
need to appreciate how many of those

00:06:18,160 --> 00:06:21,760
exist why they present new challenges

00:06:19,840 --> 00:06:23,230
why we have to be able to consider and

00:06:21,760 --> 00:06:25,150
deal with them in the very near future

00:06:23,230 --> 00:06:27,030
so let's go ahead and play some rounds

00:06:25,150 --> 00:06:29,560
using some case studies as examples

00:06:27,030 --> 00:06:33,640
target many people have heard of this

00:06:29,560 --> 00:06:35,230
one right they had figured out that hey

00:06:33,640 --> 00:06:37,360
you can get people to radically change

00:06:35,230 --> 00:06:39,280
their buying habits if you catch them in

00:06:37,360 --> 00:06:40,660
the second trimester of pregnancy

00:06:39,280 --> 00:06:43,210
something that specific but how do you

00:06:40,660 --> 00:06:45,730
find out that you have a customer who is

00:06:43,210 --> 00:06:48,130
in her second month of pregnancy the

00:06:45,730 --> 00:06:49,480
second month so trimester

00:06:48,130 --> 00:06:51,790
you know that's certainly not something

00:06:49,480 --> 00:06:53,470
they ask on the visitor cards so they

00:06:51,790 --> 00:06:56,200
found that there were a few data points

00:06:53,470 --> 00:06:58,000
that in fact would reliably predict this

00:06:56,200 --> 00:07:01,000
and so they started sending out ad

00:06:58,000 --> 00:07:03,280
circulars and those ad circulars were

00:07:01,000 --> 00:07:05,680
very focused on things that you need in

00:07:03,280 --> 00:07:08,889
your second trimester and as you are

00:07:05,680 --> 00:07:10,450
planning your little nest one day man

00:07:08,889 --> 00:07:12,550
comes in he's actually furious

00:07:10,450 --> 00:07:14,020
why did my teenage daughter get this

00:07:12,550 --> 00:07:17,140
this is outrageous are you trying to

00:07:14,020 --> 00:07:19,090
encourage her to have sex and then

00:07:17,140 --> 00:07:21,640
supposedly he came back a day or two

00:07:19,090 --> 00:07:22,900
later and said Wow I'm so sorry I didn't

00:07:21,640 --> 00:07:26,140
know it was going on in my household my

00:07:22,900 --> 00:07:28,330
daughter is in fact right good so we

00:07:26,140 --> 00:07:31,690
have this you know really useful

00:07:28,330 --> 00:07:34,060
predictability target took a lesson from

00:07:31,690 --> 00:07:36,400
that conversation the lesson they took

00:07:34,060 --> 00:07:40,960
was we really should disguise the intent

00:07:36,400 --> 00:07:44,919
of these ads better so now what you get

00:07:40,960 --> 00:07:47,590
is you get diaper add but you also get

00:07:44,919 --> 00:07:50,229
like lawnmower add and you know like

00:07:47,590 --> 00:07:51,910
auto oil add so you don't know that in

00:07:50,229 --> 00:07:53,650
fact only one of these ads is the one

00:07:51,910 --> 00:07:56,020
that's being actually targeted at you

00:07:53,650 --> 00:07:58,630
so here's there's a lesson about you

00:07:56,020 --> 00:08:00,789
know mining data is let's be honest

00:07:58,630 --> 00:08:03,100
about what we're doing this was not

00:08:00,789 --> 00:08:05,800
actually the way to solve this problem

00:08:03,100 --> 00:08:08,789
that we knew too much there are lots of

00:08:05,800 --> 00:08:11,680
different ways we could deal with it

00:08:08,789 --> 00:08:13,630
Shutterfly this one is somewhat less

00:08:11,680 --> 00:08:16,990
known but in somewhat similar territory

00:08:13,630 --> 00:08:21,010
they sent out an ad or an email saying

00:08:16,990 --> 00:08:24,510
congratulations new parent time to pick

00:08:21,010 --> 00:08:27,310
out cards to send us thank-you notes

00:08:24,510 --> 00:08:31,030
unfortunately they did not really

00:08:27,310 --> 00:08:32,919
predict well thanks Shutterfly for the

00:08:31,030 --> 00:08:36,400
congratulations of my new bundle of joy

00:08:32,919 --> 00:08:41,020
I'm horribly infertile but hey I'm

00:08:36,400 --> 00:08:42,820
adopting a cat so I lost a baby in

00:08:41,020 --> 00:08:46,510
November who would have been due this

00:08:42,820 --> 00:08:48,660
week it was like hitting a wall all over

00:08:46,510 --> 00:08:48,660
again

00:08:49,470 --> 00:08:53,110
Shutterfly responded the intent of the

00:08:52,330 --> 00:08:54,970
email was

00:08:53,110 --> 00:09:01,209
to target customers who've recently had

00:08:54,970 --> 00:09:04,120
a baby yeah I get your intent false

00:09:01,209 --> 00:09:06,670
positives are a problem we can't treat

00:09:04,120 --> 00:09:08,709
them as edge cases what is the impact of

00:09:06,670 --> 00:09:10,779
a false positive what is the impact of a

00:09:08,709 --> 00:09:13,510
false negative they didn't really

00:09:10,779 --> 00:09:16,660
apologize for having targeted wrong just

00:09:13,510 --> 00:09:18,730
for you having received something mark

00:09:16,660 --> 00:09:19,990
zuckerberg of all people has a little

00:09:18,730 --> 00:09:22,000
something to say about this a couple of

00:09:19,990 --> 00:09:24,250
months ago he announced that he's going

00:09:22,000 --> 00:09:26,560
to get father soon what he also wrote

00:09:24,250 --> 00:09:30,339
about are the three miscarriages that

00:09:26,560 --> 00:09:32,829
they experienced before that and he said

00:09:30,339 --> 00:09:35,260
you feel so hopeful when you learn

00:09:32,829 --> 00:09:37,029
you're going to have a child you start

00:09:35,260 --> 00:09:39,459
imagining who they'll become and

00:09:37,029 --> 00:09:42,959
dreaming of hopes for their future you

00:09:39,459 --> 00:09:49,089
start making plans and then they're gone

00:09:42,959 --> 00:09:51,700
it's a lonely experience I can't imagine

00:09:49,089 --> 00:09:54,700
that the Mark Zuckerberg of 10 years ago

00:09:51,700 --> 00:09:57,640
would have understood this and I think

00:09:54,700 --> 00:10:00,160
of all the VCS who focus on hiring the

00:09:57,640 --> 00:10:03,279
Marcus soccer Berg's of 10 years ago who

00:10:00,160 --> 00:10:06,430
believe that people who have had not

00:10:03,279 --> 00:10:08,980
nearly enough experiences of hard things

00:10:06,430 --> 00:10:12,880
in life are the best people to develop

00:10:08,980 --> 00:10:14,410
apps for people who have this is the

00:10:12,880 --> 00:10:16,570
Mark Zuckerberg whose far more

00:10:14,410 --> 00:10:18,910
interesting to me he's got a much better

00:10:16,570 --> 00:10:22,500
insight on all the possible ways that we

00:10:18,910 --> 00:10:26,230
can screw up or at least one of them

00:10:22,500 --> 00:10:28,390
Facebook your interview for years they

00:10:26,230 --> 00:10:30,370
have been doing this but each year

00:10:28,390 --> 00:10:32,680
they've kind of tweaked how just

00:10:30,370 --> 00:10:35,320
surfacing some posts from the past year

00:10:32,680 --> 00:10:38,740
that they feel were kind of the

00:10:35,320 --> 00:10:42,310
highlights reel of your year problem is

00:10:38,740 --> 00:10:45,100
that not everything that had a lot of

00:10:42,310 --> 00:10:47,589
likes in the past year is something that

00:10:45,100 --> 00:10:49,449
necessarily was positive for you not

00:10:47,589 --> 00:10:50,740
everything that people commented on was

00:10:49,449 --> 00:10:52,870
something you want to remember and

00:10:50,740 --> 00:10:55,209
sometimes the things that were back then

00:10:52,870 --> 00:10:57,610
exactly that exciting and memorable and

00:10:55,209 --> 00:10:59,860
wonderful have changed over the course

00:10:57,610 --> 00:11:01,810
of a year we have changes in our

00:10:59,860 --> 00:11:04,930
relationships in our jobs in our

00:11:01,810 --> 00:11:06,819
location in our feelings having the

00:11:04,930 --> 00:11:09,129
belief that we can project help you

00:11:06,819 --> 00:11:11,799
people are feeling now about something

00:11:09,129 --> 00:11:16,079
that happened in the past is so absurdly

00:11:11,799 --> 00:11:19,600
arrogant Eric Raymond coined the turn

00:11:16,079 --> 00:11:21,429
inadvertent algorithmic cruelty and he

00:11:19,600 --> 00:11:23,049
defines it as the result of code that

00:11:21,429 --> 00:11:25,359
works in the overwhelming majority of

00:11:23,049 --> 00:11:29,109
cases but doesn't take into account

00:11:25,359 --> 00:11:31,479
other use cases and the reason that he

00:11:29,109 --> 00:11:35,799
gets to coin.this is because he's one of

00:11:31,479 --> 00:11:40,269
the people that happened to his daughter

00:11:35,799 --> 00:11:44,259
died and your interview brought that

00:11:40,269 --> 00:11:49,709
back and kept bringing it back over and

00:11:44,259 --> 00:11:53,319
over there's no obvious way to stop it

00:11:49,709 --> 00:11:55,470
this is an algorithm to this was a

00:11:53,319 --> 00:11:57,879
predictable result right

00:11:55,470 --> 00:11:59,889
someone didn't predict that not everyone

00:11:57,879 --> 00:12:04,959
wants to have this coming forth it

00:11:59,889 --> 00:12:06,850
unwillingly so his recommendation is

00:12:04,959 --> 00:12:09,069
that we need to increase awareness and

00:12:06,850 --> 00:12:11,889
consideration of failure modes the edge

00:12:09,069 --> 00:12:14,199
cases worst case scenarios and so I'm

00:12:11,889 --> 00:12:17,559
trying to do that today what we can do

00:12:14,199 --> 00:12:19,029
is I hope accomplish a little something

00:12:17,559 --> 00:12:21,429
a little bit moving forward that

00:12:19,029 --> 00:12:23,409
conversation and with that in mind my

00:12:21,429 --> 00:12:25,869
first recommendation here for avoiding

00:12:23,409 --> 00:12:30,729
this particular specific pitfall along

00:12:25,869 --> 00:12:33,909
with a few others is be humble we cannot

00:12:30,729 --> 00:12:39,759
intuit the inner state emotions and

00:12:33,909 --> 00:12:44,139
private subjectivity of our users not

00:12:39,759 --> 00:12:44,909
yet anyway which brings us to issues of

00:12:44,139 --> 00:12:47,409
consent

00:12:44,909 --> 00:12:52,089
especially Wendy anonymizing private

00:12:47,409 --> 00:12:54,579
data Fitbit you know you think of it as

00:12:52,089 --> 00:12:56,919
the tracker of all sorts of activities

00:12:54,579 --> 00:13:00,699
right one of the things that it used to

00:12:56,919 --> 00:13:04,739
track is your sex life um it doesn't

00:13:00,699 --> 00:13:04,739
anymore and there's a reason for that

00:13:07,409 --> 00:13:18,960
yeah so it was in your profile and the

00:13:13,749 --> 00:13:21,310
profiles were public and yeah

00:13:18,960 --> 00:13:24,640
the profiles are pretty interesting too

00:13:21,310 --> 00:13:29,230
I can't decide whether be really happy

00:13:24,640 --> 00:13:30,610
or really sad for this person but I mean

00:13:29,230 --> 00:13:35,470
you think about how people are feeling

00:13:30,610 --> 00:13:37,810
about the Ashley Madison exposure okay

00:13:35,470 --> 00:13:41,740
some people would want to brag and feel

00:13:37,810 --> 00:13:45,400
competitive you know for other people

00:13:41,740 --> 00:13:48,780
this is a horrible violation of their

00:13:45,400 --> 00:13:52,090
privacy and their expectation of privacy

00:13:48,780 --> 00:13:55,180
this was something crucially what's the

00:13:52,090 --> 00:13:57,910
matter here is default you need to be

00:13:55,180 --> 00:14:01,060
thinking through what is default private

00:13:57,910 --> 00:14:03,610
what is default public and what are the

00:14:01,060 --> 00:14:09,370
consequences of making a poor default

00:14:03,610 --> 00:14:11,110
decision so hilariously how they dealt

00:14:09,370 --> 00:14:14,920
with this you know deep technical

00:14:11,110 --> 00:14:17,770
problem is a change to robots org dot

00:14:14,920 --> 00:14:20,890
txt so you know

00:14:17,770 --> 00:14:26,800
problem solved I'm sure you all use your

00:14:20,890 --> 00:14:28,810
fitbit's yes so this is why they no

00:14:26,800 --> 00:14:30,100
longer do sex tracking because they

00:14:28,810 --> 00:14:32,530
really weren't willing to put in the

00:14:30,100 --> 00:14:34,000
effort to do it well and you know what I

00:14:32,530 --> 00:14:35,560
actually think that that is a fair

00:14:34,000 --> 00:14:37,450
decision to make if we're not prepared

00:14:35,560 --> 00:14:39,130
to handle data really thoughtfully and

00:14:37,450 --> 00:14:41,230
well especially if it's something that

00:14:39,130 --> 00:14:42,910
can harm them then yeah opt out of doing

00:14:41,230 --> 00:14:48,640
it all together that's a reasonable

00:14:42,910 --> 00:14:51,040
product decision so in uber had God view

00:14:48,640 --> 00:14:52,600
and if you hadn't heard of this you know

00:14:51,040 --> 00:14:54,250
in many ways it's similar to anyone

00:14:52,600 --> 00:14:56,590
elses admin tools you know we always

00:14:54,250 --> 00:14:58,120
need stuff for monitoring analytics bla

00:14:56,590 --> 00:15:01,170
bla bla you know making sure that the

00:14:58,120 --> 00:15:03,280
service has is doing what you expect

00:15:01,170 --> 00:15:04,570
Uber's just happened to be called god

00:15:03,280 --> 00:15:07,450
for you well it didn't happen me it was

00:15:04,570 --> 00:15:09,130
called up you the problem here was not

00:15:07,450 --> 00:15:12,310
that they had some sort of monitoring

00:15:09,130 --> 00:15:15,550
system but the way they used it and the

00:15:12,310 --> 00:15:17,830
way they used it was for funsies they

00:15:15,550 --> 00:15:20,170
used it to show potential investors for

00:15:17,830 --> 00:15:21,880
instance at dinner parties hey look

00:15:20,170 --> 00:15:26,520
let's attract all of the guests coming

00:15:21,880 --> 00:15:28,690
to the party they don't need to know

00:15:26,520 --> 00:15:29,010
some of the people were not okay with

00:15:28,690 --> 00:15:32,380
that

00:15:29,010 --> 00:15:34,900
they used it for things like an exam

00:15:32,380 --> 00:15:37,150
judith was impatient that a reporter was

00:15:34,900 --> 00:15:40,450
on the way and taking a little too long

00:15:37,150 --> 00:15:42,520
so he tracked her in order to meet her

00:15:40,450 --> 00:15:44,920
at the lobby and let her know and share

00:15:42,520 --> 00:15:46,930
her data with her these are not

00:15:44,920 --> 00:15:49,690
necessary uses of data we need to be

00:15:46,930 --> 00:15:52,360
thinking about how much access is

00:15:49,690 --> 00:15:54,610
actually made to private data and does

00:15:52,360 --> 00:15:56,830
it really be need to be used in the way

00:15:54,610 --> 00:15:59,620
that we're using it we make tools and

00:15:56,830 --> 00:16:02,230
then we expand too much on how they're

00:15:59,620 --> 00:16:04,390
used and Guber was certainly a case of

00:16:02,230 --> 00:16:06,610
that I think it's a little bit telling

00:16:04,390 --> 00:16:08,800
that they do really evil things like

00:16:06,610 --> 00:16:10,960
this in their code this is actual God

00:16:08,800 --> 00:16:19,270
view code I mean look at that I will

00:16:10,960 --> 00:16:21,820
play true if you remember OkCupid as

00:16:19,270 --> 00:16:24,070
blog they don't really do it anymore but

00:16:21,820 --> 00:16:26,020
the research group at the dating site

00:16:24,070 --> 00:16:27,100
OkCupid they used to blog about things

00:16:26,020 --> 00:16:29,620
that they were learning from the

00:16:27,100 --> 00:16:30,970
aggregate data trends and they would

00:16:29,620 --> 00:16:33,070
just focus on sharing some sort of

00:16:30,970 --> 00:16:35,620
little insight into simple ways that an

00:16:33,070 --> 00:16:36,940
OkCupid user could use their dating site

00:16:35,620 --> 00:16:39,750
better right

00:16:36,940 --> 00:16:43,560
uber used to blog about their data too

00:16:39,750 --> 00:16:46,240
crucially different a few things

00:16:43,560 --> 00:16:48,730
theirs was not about improving customers

00:16:46,240 --> 00:16:51,370
experience of the service theirs was

00:16:48,730 --> 00:16:54,280
about invading people's privacy in order

00:16:51,370 --> 00:16:56,890
to pass judgment on their sex lives this

00:16:54,280 --> 00:16:59,470
is not a predictable consequence of

00:16:56,890 --> 00:17:01,540
signing up for a transport service you

00:16:59,470 --> 00:17:04,000
don't expect that you're gonna have

00:17:01,540 --> 00:17:06,250
someone being drawing conclusions about

00:17:04,000 --> 00:17:07,600
your private life and again you know

00:17:06,250 --> 00:17:09,819
it's so easy to look at something like

00:17:07,600 --> 00:17:11,410
Ashley Madison say okay well those

00:17:09,819 --> 00:17:13,540
people have some sort of plausible

00:17:11,410 --> 00:17:14,350
deniability if I didn't do what you said

00:17:13,540 --> 00:17:17,410
I did

00:17:14,350 --> 00:17:20,699
uber is saying we know for sure we think

00:17:17,410 --> 00:17:26,470
our data definitely predicts that you

00:17:20,699 --> 00:17:28,660
wear a last night this is again a

00:17:26,470 --> 00:17:34,930
lot of opportunity for consequences for

00:17:28,660 --> 00:17:37,990
the user did they expect it no don't do

00:17:34,930 --> 00:17:40,410
consent like uber architect for real

00:17:37,990 --> 00:17:43,060
consent architect for informed consent

00:17:40,410 --> 00:17:45,190
so what does that mean informed consent

00:17:43,060 --> 00:17:47,350
is permission freely granted

00:17:45,190 --> 00:17:49,720
we're know is a totally consequence-free

00:17:47,350 --> 00:17:51,970
alternative and it's the default value

00:17:49,720 --> 00:17:55,060
and it's giving informed appreciation

00:17:51,970 --> 00:17:57,090
and understanding ahead of time of the

00:17:55,060 --> 00:17:59,860
facts implications and consequences

00:17:57,090 --> 00:18:02,500
involved in giving a yes and if that

00:17:59,860 --> 00:18:06,460
sounds overly burdensome come on look at

00:18:02,500 --> 00:18:08,350
the tos we can set aside a little bit of

00:18:06,460 --> 00:18:10,570
information to genuinely say like yo

00:18:08,350 --> 00:18:12,610
when you sign up we also do this really

00:18:10,570 --> 00:18:13,960
fun stuff where we look at your data and

00:18:12,610 --> 00:18:15,760
we say interesting things about it you

00:18:13,960 --> 00:18:18,490
okay with that this doesn't have to be

00:18:15,760 --> 00:18:20,230
tremendously hard but we do have to make

00:18:18,490 --> 00:18:21,700
some sort of effort to give people

00:18:20,230 --> 00:18:25,000
better disclosure of what they're

00:18:21,700 --> 00:18:27,100
getting into Google AdWords is another

00:18:25,000 --> 00:18:33,010
really interesting example there was

00:18:27,100 --> 00:18:35,800
Harvard research into basically it took

00:18:33,010 --> 00:18:37,780
two sets of names one that's highly

00:18:35,800 --> 00:18:39,250
correlated with people who are black one

00:18:37,780 --> 00:18:41,410
that is highly correlated with people

00:18:39,250 --> 00:18:44,590
who are white and then found real names

00:18:41,410 --> 00:18:47,410
of working academics who had those first

00:18:44,590 --> 00:18:51,670
names and some sort of last name so if

00:18:47,410 --> 00:18:53,950
we say that Latoya is a name highly

00:18:51,670 --> 00:18:56,500
correlated with black women then we say

00:18:53,950 --> 00:18:58,870
okay so Latoya Adams is a real academic

00:18:56,500 --> 00:19:01,570
and we do some searches on her and what

00:18:58,870 --> 00:19:04,090
they found were that when you search for

00:19:01,570 --> 00:19:07,300
names of white people versus black

00:19:04,090 --> 00:19:10,510
people you have ads far more likely to

00:19:07,300 --> 00:19:14,440
show up that imply that the black person

00:19:10,510 --> 00:19:15,790
has an arrest record now think about

00:19:14,440 --> 00:19:17,800
what that means because AdWords

00:19:15,790 --> 00:19:19,960
algorithm is focused on predicting

00:19:17,800 --> 00:19:20,830
behavior and that's it it doesn't care

00:19:19,960 --> 00:19:22,510
about content

00:19:20,830 --> 00:19:25,060
it just has a template to fill in and

00:19:22,510 --> 00:19:27,190
select which one its job is just to

00:19:25,060 --> 00:19:29,800
figure out what was gonna make us click

00:19:27,190 --> 00:19:31,660
what based on what it knows about us and

00:19:29,800 --> 00:19:33,520
what it knows about others behavior were

00:19:31,660 --> 00:19:40,810
those same ads so what is this

00:19:33,520 --> 00:19:42,520
reflecting back on us deep learning is

00:19:40,810 --> 00:19:45,630
also really useful for doing photo

00:19:42,520 --> 00:19:50,380
analysis we're already seeing some uses

00:19:45,630 --> 00:19:52,420
photo recognition facial recognition

00:19:50,380 --> 00:19:54,100
this actually was not using deep

00:19:52,420 --> 00:19:56,590
learning but you know we're familiar

00:19:54,100 --> 00:19:58,450
with some of the interesting trends and

00:19:56,590 --> 00:20:00,780
possibilities that you can see right

00:19:58,450 --> 00:20:05,290
um it's getting a lot better you know

00:20:00,780 --> 00:20:07,000
iPhoto is more helpful now but we're

00:20:05,290 --> 00:20:09,340
also seeing that even with far more

00:20:07,000 --> 00:20:11,470
sophisticated implementations that are

00:20:09,340 --> 00:20:14,860
using deep learning this is from just

00:20:11,470 --> 00:20:22,330
two months ago this is not in fact a

00:20:14,860 --> 00:20:23,010
jungle gym it's ash Wits this is not an

00:20:22,330 --> 00:20:26,230
animal

00:20:23,010 --> 00:20:33,180
this is Flickr doing Auto tagging as

00:20:26,230 --> 00:20:33,180
well this is Google doing auto tagging

00:20:33,840 --> 00:20:39,190
to their credit they acted on it really

00:20:36,460 --> 00:20:40,900
quickly what's told not to their credit

00:20:39,190 --> 00:20:42,880
this was a month after Flickr had had

00:20:40,900 --> 00:20:47,620
that happen with black people being Apes

00:20:42,880 --> 00:20:49,600
so learn pay attention to the news they

00:20:47,620 --> 00:20:51,970
should have been acting on that a month

00:20:49,600 --> 00:20:54,730
sooner and they did get it taken care of

00:20:51,970 --> 00:20:56,140
within 24 hours but that says that 29

00:20:54,730 --> 00:21:00,460
days earlier this could have never

00:20:56,140 --> 00:21:02,710
happened so why does something like this

00:21:00,460 --> 00:21:06,340
happen at all and the answer goes back

00:21:02,710 --> 00:21:08,080
to the 1950s when Kodak was first

00:21:06,340 --> 00:21:11,020
developing ways to have really

00:21:08,080 --> 00:21:12,400
calibrated photos so in order to make

00:21:11,020 --> 00:21:15,010
sure that photos were consistently

00:21:12,400 --> 00:21:16,780
rendered with nice accurate colors they

00:21:15,010 --> 00:21:18,760
created what are called shirley cards

00:21:16,780 --> 00:21:22,420
and these were cards used to color

00:21:18,760 --> 00:21:25,540
balance the printer and you might notice

00:21:22,420 --> 00:21:29,230
something about them shirley cards were

00:21:25,540 --> 00:21:32,110
of white women film stock was optimized

00:21:29,230 --> 00:21:35,890
to accurately reproduce details in white

00:21:32,110 --> 00:21:38,530
skinned people the digital sensors of

00:21:35,890 --> 00:21:42,670
today are still trying to mimic the

00:21:38,530 --> 00:21:44,230
results of film processing if we saw

00:21:42,670 --> 00:21:46,510
something radically different we'd be

00:21:44,230 --> 00:21:48,100
complaining that the sensors suck the

00:21:46,510 --> 00:21:50,980
problem is that that means that we have

00:21:48,100 --> 00:21:54,070
decade after decade of legacy data and

00:21:50,980 --> 00:21:58,600
current data that is all really poor

00:21:54,070 --> 00:22:00,910
data if it's a dark-skinned to people we

00:21:58,600 --> 00:22:02,980
think we have objective vast data sets

00:22:00,910 --> 00:22:06,430
but the data set is polluted with the

00:22:02,980 --> 00:22:08,080
noise I'm gonna have to skip a few

00:22:06,430 --> 00:22:10,920
because we're running low on time so I'm

00:22:08,080 --> 00:22:10,920
gonna jump ahead a bit

00:22:13,880 --> 00:22:18,840
so rationales for an algorithm can only

00:22:16,830 --> 00:22:22,050
be seen from inside of that black box

00:22:18,840 --> 00:22:26,160
that I talked about and so that's great

00:22:22,050 --> 00:22:29,090
let's look at the rationale that's a

00:22:26,160 --> 00:22:33,240
picture of the inside of the black box

00:22:29,090 --> 00:22:34,890
it's in there I swear thinking about

00:22:33,240 --> 00:22:37,500
making decisions inside that black box

00:22:34,890 --> 00:22:39,750
is there's no accountability there's no

00:22:37,500 --> 00:22:41,400
way to check our work and there's no

00:22:39,750 --> 00:22:44,910
recourse for people when they tell us

00:22:41,400 --> 00:22:45,960
we've gotten it wrong so there are some

00:22:44,910 --> 00:22:47,610
things that we can do to take some

00:22:45,960 --> 00:22:49,230
lessons from that and they're actually

00:22:47,610 --> 00:22:50,610
our professional ethicists including

00:22:49,230 --> 00:22:52,530
ones in our own profession

00:22:50,610 --> 00:22:54,810
I sure hadn't heard about them and

00:22:52,530 --> 00:22:56,610
probably you haven't either I adapted a

00:22:54,810 --> 00:22:58,290
few rules from various sources including

00:22:56,610 --> 00:23:00,300
the Association for Computing Machinery

00:22:58,290 --> 00:23:02,940
so let's take a look at a few of those

00:23:00,300 --> 00:23:05,010
one avoid harm to others this might

00:23:02,940 --> 00:23:07,980
sound a lot like say medical ethics

00:23:05,010 --> 00:23:13,950
rules how do we do that

00:23:07,980 --> 00:23:15,870
well consider decisions potential impact

00:23:13,950 --> 00:23:17,910
on others while we're doing development

00:23:15,870 --> 00:23:19,310
and while we're making decisions project

00:23:17,910 --> 00:23:22,110
the likelihood of consequences

00:23:19,310 --> 00:23:24,690
contribute to human well-being minimize

00:23:22,110 --> 00:23:27,390
negative consequences to others it's not

00:23:24,690 --> 00:23:29,280
enough to just say ah this is a problem

00:23:27,390 --> 00:23:31,500
minimize minimize what is the least

00:23:29,280 --> 00:23:33,300
invasive solution we need to be really

00:23:31,500 --> 00:23:35,520
honest and trustworthy even more so

00:23:33,300 --> 00:23:36,870
because we are gonna screw up at some

00:23:35,520 --> 00:23:39,750
point we're gonna need to be able to

00:23:36,870 --> 00:23:41,640
sincerely apologize and be believed we

00:23:39,750 --> 00:23:43,710
need to have been gendered enough trust

00:23:41,640 --> 00:23:46,680
that we can keep moving forward after

00:23:43,710 --> 00:23:48,210
we've made a fail we need to provide

00:23:46,680 --> 00:23:50,490
others with full disclosure of the

00:23:48,210 --> 00:23:53,280
limitations of what we do and call

00:23:50,490 --> 00:23:55,350
attention to signs of risk we also need

00:23:53,280 --> 00:23:58,050
to actively counter bias and inequality

00:23:55,350 --> 00:24:01,290
because code is made by people it's not

00:23:58,050 --> 00:24:04,710
objective it reflects our tunnel vision

00:24:01,290 --> 00:24:06,630
it replicates our flaws algorithms

00:24:04,710 --> 00:24:08,670
always have underlying assumptions about

00:24:06,630 --> 00:24:10,830
meaning about accuracy about the world

00:24:08,670 --> 00:24:13,040
in which they're generated about how

00:24:10,830 --> 00:24:15,180
code should assign meaning to data

00:24:13,040 --> 00:24:16,860
underlying assumptions influence

00:24:15,180 --> 00:24:20,130
outcomes and consequences being

00:24:16,860 --> 00:24:21,180
generated always so we need to do things

00:24:20,130 --> 00:24:24,509
like audit outcomes

00:24:21,180 --> 00:24:28,739
you remember that adage trust but verify

00:24:24,509 --> 00:24:35,089
you know I don't apply here that's

00:24:28,739 --> 00:24:35,089
insufficient because photo of black box

00:24:35,329 --> 00:24:41,940
instead what we have to do is not trust

00:24:38,369 --> 00:24:43,859
but audit constantly this has to be part

00:24:41,940 --> 00:24:46,169
of our practices of checking what's

00:24:43,859 --> 00:24:50,609
coming out is it what it should be is it

00:24:46,169 --> 00:24:52,349
what's expected is it unbiased so we're

00:24:50,609 --> 00:24:54,749
in this arms race right now because

00:24:52,349 --> 00:24:56,399
Google Microsoft Apple Facebook they're

00:24:54,749 --> 00:24:58,469
all deeply investing in these

00:24:56,399 --> 00:24:59,669
technologies and they're doubling down

00:24:58,469 --> 00:25:01,859
and they're already rolling out

00:24:59,669 --> 00:25:03,659
implementations that are fascinating but

00:25:01,859 --> 00:25:06,419
they're moving forward very quickly and

00:25:03,659 --> 00:25:07,979
the problem with that is that they are

00:25:06,419 --> 00:25:10,169
both getting more precise in their

00:25:07,979 --> 00:25:12,239
correctness and more damaging and their

00:25:10,169 --> 00:25:14,009
wrongness so we need to be really

00:25:12,239 --> 00:25:15,329
thoughtful about how we move forward

00:25:14,009 --> 00:25:17,339
with this to make sure that we are

00:25:15,329 --> 00:25:19,109
focused on being more precise in our

00:25:17,339 --> 00:25:22,139
correctness and using that corrective

00:25:19,109 --> 00:25:23,219
information very carefully we have to

00:25:22,139 --> 00:25:25,739
insist on getting this right because we

00:25:23,219 --> 00:25:27,749
want to be empathetic right so this is

00:25:25,739 --> 00:25:29,219
what we need to do we have to have

00:25:27,749 --> 00:25:31,019
decision-making authority in the hands

00:25:29,219 --> 00:25:33,619
of highly diverse teams who can

00:25:31,019 --> 00:25:35,789
anticipate diverse ways to screw up and

00:25:33,619 --> 00:25:38,249
finally we have to commit to

00:25:35,789 --> 00:25:41,159
transparency we have to be able to apply

00:25:38,249 --> 00:25:43,799
our own expertise to say no to things

00:25:41,159 --> 00:25:46,169
that are going to harm users at all we

00:25:43,799 --> 00:25:48,509
have to be able to say no over and over

00:25:46,169 --> 00:25:51,239
again to pitfalls like these as much as

00:25:48,509 --> 00:25:54,209
it's necessary because we understand the

00:25:51,239 --> 00:25:57,709
consequences refuse to play along with

00:25:54,209 --> 00:25:57,709

YouTube URL: https://www.youtube.com/watch?v=5e-mZnYAih8


