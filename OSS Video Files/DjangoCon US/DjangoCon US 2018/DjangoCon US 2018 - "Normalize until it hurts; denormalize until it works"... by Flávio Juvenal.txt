Title: DjangoCon US 2018 - "Normalize until it hurts; denormalize until it works"... by FlÃ¡vio Juvenal
Publication date: 2018-11-08
Playlist: DjangoCon US 2018
Description: 
	DjangoCon US 2018 - "Normalize until it hurts; denormalize until it works" in Django by FlÃ¡vio Juvenal

Thereâ€™s a good practice that says â€œa database is a representer of factsâ€. If thereâ€™s more than one way to extract a single fact from the database, then thereâ€™s a redundancy in it. Every redundancy can cause different anomalies in the data, which in turn cause bugs in the application. To avoid that, thereâ€™s a process called normalization, which involves following sets of rules to restructure the database to remove redundancies without losing the original facts. The traditional set of normalization rules are the so-called Normal Forms: First Normal Form, Second, Third, etc. Unfortunately, those are frequently overlooked by developers due to their excessive formalism. But in fact, even the Normal Forms arenâ€™t enough to avoid anomalies, since theyâ€™re concerned about redundancies only in a single table*. Since cross-table dependencies are very common in modern applications, we must go beyond normal forms to prevent problems.

In this talk, weâ€™ll present normalization rules on a friendly language, going beyond normal forms. Weâ€™ll understand how the software requirements cause dependencies in database tables, both in-table and cross-tables. Weâ€™ll show real examples of non-trivial dependencies that happen on Django models. Weâ€™ll discuss how normalization prevents redundancies, inconsistencies, anomalies, and bugs. Knowing that normalization can cause slowdowns in queries, weâ€™ll present how to increase performance with denormalization, which is not the same of not normalizing. Instead, denormalization means being able to represent data in multiple ways to speed up queries without introducing inconsistencies. Weâ€™ll discuss Django-related denormalization tools that use cronjobs, indexes, caching, materialized views and triggers, and NoSQL.

*Itâ€™s common to ignore the fact that normal forms only discuss redundancies inside a single table/record/relval. More about this in this article reviewed by Codd, Fagin and Date, key figures of the relational model.

This talk was presented at: https://2018.djangocon.us/talk/normalize-until-it-hurts-denormalize-it/

LINKS:
Follow FlÃ¡vio Juvenal ðŸ‘‡
On Twitter: https://twitter.com/flaviojuvenal
Official homepage: https://www.vinta.com.br

Follow DjangCon US ðŸ‘‡
https://twitter.com/djangocon

Follow DEFNA ðŸ‘‡
https://twitter.com/defnado
https://www.defna.org/
Captions: 
	00:00:00,050 --> 00:00:07,709
[Music]

00:00:15,590 --> 00:00:22,310
thank you thank you very much I'm very

00:00:17,789 --> 00:00:24,869
excited to be here at Jango : again so

00:00:22,310 --> 00:00:26,609
we'll talk about normalization and the

00:00:24,869 --> 00:00:29,250
normalization and how that applies to

00:00:26,609 --> 00:00:33,510
Jango and that's my Twitter handle I'll

00:00:29,250 --> 00:00:36,360
post the slides there after the talk so

00:00:33,510 --> 00:00:38,969
to give a bit of context about me

00:00:36,360 --> 00:00:42,210
I'm Flavia juvenile I'm from his city

00:00:38,969 --> 00:00:44,610
Brazil so I came for this conference and

00:00:42,210 --> 00:00:47,969
I was before at pagoda at New York I

00:00:44,610 --> 00:00:51,239
work with Jango for seven years now

00:00:47,969 --> 00:00:54,870
since version 1.3 and I'm a partner at

00:00:51,239 --> 00:00:57,149
winter software we are a team of experts

00:00:54,870 --> 00:00:59,910
from Brazil that works mostly with

00:00:57,149 --> 00:01:03,510
companies from the US and we help our

00:00:59,910 --> 00:01:07,409
clients to evolve their products with

00:01:03,510 --> 00:01:10,740
top-notch UX and development techniques

00:01:07,409 --> 00:01:13,890
we do mostly a Django and react web

00:01:10,740 --> 00:01:15,720
development so we are going to talk

00:01:13,890 --> 00:01:19,500
about normalization and normalization

00:01:15,720 --> 00:01:22,619
has everything to do with anomalies and

00:01:19,500 --> 00:01:25,590
the best way I think to think about a

00:01:22,619 --> 00:01:29,009
database is that it represents facts

00:01:25,590 --> 00:01:33,479
about data so for example this table

00:01:29,009 --> 00:01:35,939
here we have some toppings on some pizza

00:01:33,479 --> 00:01:37,619
Ria's and our rating for them and the

00:01:35,939 --> 00:01:41,850
phone of the pizzeria so it's the

00:01:37,619 --> 00:01:44,220
topping rating table if we ask the

00:01:41,850 --> 00:01:46,350
question the fact what's the pits area

00:01:44,220 --> 00:01:49,740
known as phone we have two answers for

00:01:46,350 --> 00:01:52,049
this question in this table because the

00:01:49,740 --> 00:01:54,509
phone of the pizzeria Nonna appears two

00:01:52,049 --> 00:01:58,649
two times because known as has two

00:01:54,509 --> 00:02:02,670
toppings evaluated revealed on that

00:01:58,649 --> 00:02:05,280
rating table that means that our

00:02:02,670 --> 00:02:09,030
resident ways of caring pizzeria knowns

00:02:05,280 --> 00:02:12,060
phone on this table and not in in the

00:02:09,030 --> 00:02:14,180
table structure prevents a conflict on

00:02:12,060 --> 00:02:17,430
those values

00:02:14,180 --> 00:02:20,460
so the question what's pizzeria nones

00:02:17,430 --> 00:02:23,630
known as phone again it's impossible to

00:02:20,460 --> 00:02:27,120
answer with disorder a normal stable and

00:02:23,630 --> 00:02:29,460
this state happens if one forgets to

00:02:27,120 --> 00:02:32,430
update both values together if we are

00:02:29,460 --> 00:02:36,480
updating this area known as phone on

00:02:32,430 --> 00:02:39,690
this table so redundancy can cause

00:02:36,480 --> 00:02:43,200
anomalies that can cause incorrect facts

00:02:39,690 --> 00:02:46,380
and that can cause bugs and proper

00:02:43,200 --> 00:02:50,190
normalization prevents data anomalies in

00:02:46,380 --> 00:02:53,910
the database what kinds of anomalies can

00:02:50,190 --> 00:02:56,640
happen can happen update anomalies where

00:02:53,910 --> 00:02:59,550
we need to update our rules where the

00:02:56,640 --> 00:03:02,520
same pizza reappears to update the phone

00:02:59,550 --> 00:03:05,010
of the pizzeria insertion anomaly where

00:03:02,520 --> 00:03:08,459
we can't add a phone to a pizzeria with

00:03:05,010 --> 00:03:11,550
all adding a topping to and deletion

00:03:08,459 --> 00:03:13,680
anomaly where we can't keep a phone to a

00:03:11,550 --> 00:03:15,780
pizzeria if all toppings are deleted

00:03:13,680 --> 00:03:18,000
which doesn't make sense but that's

00:03:15,780 --> 00:03:20,940
happening because of the way we are

00:03:18,000 --> 00:03:24,180
storing the phone's data on this table

00:03:20,940 --> 00:03:27,450
and the solution is perhaps third normal

00:03:24,180 --> 00:03:30,570
form boyce-codd normal form no no no

00:03:27,450 --> 00:03:32,970
normal forms are boring normal forms are

00:03:30,570 --> 00:03:36,000
like this every non-prime attribute of

00:03:32,970 --> 00:03:38,640
armors is no transitively depth

00:03:36,000 --> 00:03:42,590
bah-bah-bah so we are like that when we

00:03:38,640 --> 00:03:46,769
are reading normal forms the solution is

00:03:42,590 --> 00:03:48,840
actually simple we can state the basic

00:03:46,769 --> 00:03:50,880
idea behind normal forms in a simpler

00:03:48,840 --> 00:03:53,489
way which is if there are multiple ways

00:03:50,880 --> 00:03:57,209
to extract the same fact from the

00:03:53,489 --> 00:04:00,090
database there's a redundancy and to

00:03:57,209 --> 00:04:02,670
remove redundancy we need to raise

00:04:00,090 --> 00:04:04,560
structure tables and columns we need to

00:04:02,670 --> 00:04:06,930
divide them we need to combine them we

00:04:04,560 --> 00:04:10,980
need to do something to remove the

00:04:06,930 --> 00:04:14,190
redundancies we find in our databases on

00:04:10,980 --> 00:04:15,630
the data example direction the solution

00:04:14,190 --> 00:04:18,109
is to create another table just for

00:04:15,630 --> 00:04:21,299
pizzerias and to start the founder and

00:04:18,109 --> 00:04:25,229
by doing this we can't have repeated

00:04:21,299 --> 00:04:27,389
phones or if we have repeated phones we

00:04:25,229 --> 00:04:30,960
like even at our tables so

00:04:27,389 --> 00:04:34,020
it will be handled by by the database we

00:04:30,960 --> 00:04:36,330
have a structure that either allows or

00:04:34,020 --> 00:04:39,389
prohibits these for us and that's

00:04:36,330 --> 00:04:42,860
exactly what we want so normalization is

00:04:39,389 --> 00:04:46,169
the process of restructuring a database

00:04:42,860 --> 00:04:49,800
to decrease rendered and see and ensure

00:04:46,169 --> 00:04:52,830
integrity of data but is this really a

00:04:49,800 --> 00:04:56,069
problem for jingle without RM we work

00:04:52,830 --> 00:04:57,569
mostly with Business Objects so at the

00:04:56,069 --> 00:04:59,699
first place probably we do something

00:04:57,569 --> 00:05:02,279
like that will create a pizzeria model

00:04:59,699 --> 00:05:04,800
and have the name and the phone there so

00:05:02,279 --> 00:05:08,279
like we would have the problem of

00:05:04,800 --> 00:05:10,259
putting phone inside a topping rating

00:05:08,279 --> 00:05:12,629
table because we at the first place we

00:05:10,259 --> 00:05:15,930
would design with Business Objects

00:05:12,629 --> 00:05:18,060
like bits earlier like rating yes that

00:05:15,930 --> 00:05:20,400
specific case probably wouldn't happen

00:05:18,060 --> 00:05:20,969
in general but another problem would do

00:05:20,400 --> 00:05:24,060
too

00:05:20,969 --> 00:05:28,400
due to migration conservatism as we all

00:05:24,060 --> 00:05:32,370
know all kinds of conservatism is bad

00:05:28,400 --> 00:05:34,610
just a joke but this kind of migration

00:05:32,370 --> 00:05:38,250
conservatives out also bad because

00:05:34,610 --> 00:05:40,409
imagine you have an employee table with

00:05:38,250 --> 00:05:44,699
name and Department and we have choices

00:05:40,409 --> 00:05:46,919
for a department ok and the client asks

00:05:44,699 --> 00:05:48,870
for the system to store department

00:05:46,919 --> 00:05:51,509
address so we had employees on

00:05:48,870 --> 00:05:53,639
departments and now the client asks for

00:05:51,509 --> 00:05:55,469
department address developers are

00:05:53,639 --> 00:05:57,360
conservative about migrations they don't

00:05:55,469 --> 00:05:59,250
want to do to create like complex

00:05:57,360 --> 00:06:01,939
migrations move data around so they

00:05:59,250 --> 00:06:04,819
would do this they would just add

00:06:01,939 --> 00:06:08,909
Department address in the employee table

00:06:04,819 --> 00:06:12,689
and that that's a problem because the

00:06:08,909 --> 00:06:15,300
fact address of Department X is repeated

00:06:12,689 --> 00:06:18,149
for every employee of department X on

00:06:15,300 --> 00:06:20,509
this table again all sorts of anomalies

00:06:18,149 --> 00:06:24,889
can happen and instead we should

00:06:20,509 --> 00:06:27,120
actually create an order model

00:06:24,889 --> 00:06:28,860
department even though we didn't help at

00:06:27,120 --> 00:06:33,210
the first place we need to create it now

00:06:28,860 --> 00:06:35,580
and have at the address field there so

00:06:33,210 --> 00:06:38,330
we need to reject the rule on every new

00:06:35,580 --> 00:06:40,950
feature we develop on every new

00:06:38,330 --> 00:06:42,780
requirement we need to check if

00:06:40,950 --> 00:06:44,910
just introduce it another way of

00:06:42,780 --> 00:06:47,190
extracting the same fact from the

00:06:44,910 --> 00:06:50,460
database if so we need to restructure

00:06:47,190 --> 00:06:52,950
the tables I want to talk also about

00:06:50,460 --> 00:06:56,970
historical data versus normalization

00:06:52,950 --> 00:07:01,140
that's something quite complex imagine

00:06:56,970 --> 00:07:04,680
when you have this we have order and the

00:07:01,140 --> 00:07:06,480
order is made by a user it has a related

00:07:04,680 --> 00:07:08,970
product and the total of the order and

00:07:06,480 --> 00:07:12,270
suppose that todo is computed from

00:07:08,970 --> 00:07:15,120
products price okay when a warder is

00:07:12,270 --> 00:07:19,110
created we grab product price and add to

00:07:15,120 --> 00:07:21,750
total then it looks like if the product

00:07:19,110 --> 00:07:24,560
is the same the total will always be the

00:07:21,750 --> 00:07:28,260
same so we have a redundancy here right

00:07:24,560 --> 00:07:29,510
if product is the same total will always

00:07:28,260 --> 00:07:33,660
be the same really

00:07:29,510 --> 00:07:36,540
not if we consider time product price

00:07:33,660 --> 00:07:39,390
tends to change if we don't start todo

00:07:36,540 --> 00:07:42,660
at order we will lose data when a

00:07:39,390 --> 00:07:45,690
product price changes so Toro is

00:07:42,660 --> 00:07:48,810
actually effect about the product at the

00:07:45,690 --> 00:07:51,270
order moment what was the order Toru is

00:07:48,810 --> 00:07:55,800
the different fact then what's the

00:07:51,270 --> 00:07:57,960
current product price due to time so

00:07:55,800 --> 00:08:01,170
that's the idea behind the phrase

00:07:57,960 --> 00:08:03,720
accountants don't use erasers historical

00:08:01,170 --> 00:08:06,180
data actually should not be normalized

00:08:03,720 --> 00:08:09,470
because it represents a fact about the

00:08:06,180 --> 00:08:12,810
moment it was created it was inserted

00:08:09,470 --> 00:08:16,740
other common examples of historical data

00:08:12,810 --> 00:08:19,650
are an address field achievement if the

00:08:16,740 --> 00:08:22,740
user changes his address you don't want

00:08:19,650 --> 00:08:24,660
to change all pest user shipments

00:08:22,740 --> 00:08:27,630
because that that already happened or

00:08:24,660 --> 00:08:31,050
that already went to an integration with

00:08:27,630 --> 00:08:33,180
some shipping carrier and let's look few

00:08:31,050 --> 00:08:35,160
data blog post if we publish at our blog

00:08:33,180 --> 00:08:37,530
post shared it on Twitter or something

00:08:35,160 --> 00:08:39,840
on a newsletter we don't want to change

00:08:37,530 --> 00:08:41,640
change the slug if we change the name

00:08:39,840 --> 00:08:44,550
otherwise you have broken links

00:08:41,640 --> 00:08:47,160
so sometimes historical data is just

00:08:44,550 --> 00:08:49,380
data publish it to outside your database

00:08:47,160 --> 00:08:51,600
and you should be careful that data

00:08:49,380 --> 00:08:54,350
looks like resident but it actually

00:08:51,600 --> 00:08:56,610
isn't because of time

00:08:54,350 --> 00:08:59,820
okay now let's talk about query

00:08:56,610 --> 00:09:01,740
expressions in how they help us to not

00:08:59,820 --> 00:09:04,950
do normalize and to keep things

00:09:01,740 --> 00:09:07,500
normalizing imagine now we have this

00:09:04,950 --> 00:09:11,070
other example here

00:09:07,500 --> 00:09:13,350
todo ordered and todo ordered is really

00:09:11,070 --> 00:09:16,620
redundant here it's not historical data

00:09:13,350 --> 00:09:19,320
it's just the sum of all or their todos

00:09:16,620 --> 00:09:22,740
for the user so we have users that make

00:09:19,320 --> 00:09:25,260
many orders and we have we want to know

00:09:22,740 --> 00:09:28,950
the total ordered by the user so we have

00:09:25,260 --> 00:09:31,350
to sum all the totals the other order

00:09:28,950 --> 00:09:34,460
todos related to that user so this is

00:09:31,350 --> 00:09:37,710
redundant the way it is right here

00:09:34,460 --> 00:09:40,860
todo order is redundant because we can

00:09:37,710 --> 00:09:44,250
extract it from order but how we can do

00:09:40,860 --> 00:09:47,340
this with query expressions so instead

00:09:44,250 --> 00:09:50,580
of having that field we could annotate

00:09:47,340 --> 00:09:53,780
that at query time using query

00:09:50,580 --> 00:09:58,290
expressions and we will do a sum over

00:09:53,780 --> 00:10:00,210
order storeroom so don't be afraid of

00:09:58,290 --> 00:10:02,040
query expressions you can filter you can

00:10:00,210 --> 00:10:04,740
order with them you can do much more

00:10:02,040 --> 00:10:06,330
without needing the normalize it

00:10:04,740 --> 00:10:09,230
computed fields so instead of like

00:10:06,330 --> 00:10:12,330
adding to that field every time you

00:10:09,230 --> 00:10:15,960
create a new order for the user you can

00:10:12,330 --> 00:10:19,530
just annotate a sum of all the user or a

00:10:15,960 --> 00:10:21,390
todos and grab that at query time if

00:10:19,530 --> 00:10:24,350
necessary you can even create your own

00:10:21,390 --> 00:10:26,850
DB level functions and custom lookups

00:10:24,350 --> 00:10:29,850
for example imagine I have this problem

00:10:26,850 --> 00:10:33,060
where I want to start the name of the

00:10:29,850 --> 00:10:35,910
person and but I also want to store the

00:10:33,060 --> 00:10:38,550
name on you need the code I'll explain

00:10:35,910 --> 00:10:40,590
that but imagine that's for filtering

00:10:38,550 --> 00:10:41,130
easier filtering searching and things

00:10:40,590 --> 00:10:43,470
like that

00:10:41,130 --> 00:10:45,420
you know the code is this if you use

00:10:43,470 --> 00:10:49,740
that library you need the code you can

00:10:45,420 --> 00:10:53,240
pass it a accented name like mine and

00:10:49,740 --> 00:10:56,760
get rabbit without accents with just SC

00:10:53,240 --> 00:10:59,640
characters and if you want to do that

00:10:56,760 --> 00:11:01,500
like the norm in the den now you

00:10:59,640 --> 00:11:04,020
normalize that way you have to do that

00:11:01,500 --> 00:11:06,510
every time you change name to update

00:11:04,020 --> 00:11:07,450
naming you need the code but you can do

00:11:06,510 --> 00:11:12,240
that at the

00:11:07,450 --> 00:11:15,730
level if you create all with posters

00:11:12,240 --> 00:11:18,700
function you need a code and you can

00:11:15,730 --> 00:11:21,850
even write Python code inside it if you

00:11:18,700 --> 00:11:23,950
use the Python extension for posters you

00:11:21,850 --> 00:11:27,370
can write fight Python code inside the

00:11:23,950 --> 00:11:30,940
function and you can declare that on the

00:11:27,370 --> 00:11:33,610
Django side and annotate that that you

00:11:30,940 --> 00:11:37,060
are you want to compute name unit the

00:11:33,610 --> 00:11:39,670
code at query time by using the function

00:11:37,060 --> 00:11:41,830
unit the code use the final at your

00:11:39,670 --> 00:11:45,820
database this works for Possible's

00:11:41,830 --> 00:11:49,870
probably other other database systems

00:11:45,820 --> 00:11:51,760
have similar things and this way we

00:11:49,870 --> 00:11:55,120
don't need to denormalize and create our

00:11:51,760 --> 00:11:57,640
own name even at the code but isn't that

00:11:55,120 --> 00:12:00,580
slow we are you are calling a function

00:11:57,640 --> 00:12:03,490
at query time for our rows you are

00:12:00,580 --> 00:12:05,800
returning okay that can be slow when

00:12:03,490 --> 00:12:09,130
that slow it's time for the

00:12:05,800 --> 00:12:11,110
normalization and we shouldn't do the

00:12:09,130 --> 00:12:14,040
normalization blindly there are the

00:12:11,110 --> 00:12:17,320
normalization patterns we can use

00:12:14,040 --> 00:12:21,040
first we must normalize until it hurts

00:12:17,320 --> 00:12:24,100
and then the normalize until it works we

00:12:21,040 --> 00:12:27,550
should not confuse denormalize it with

00:12:24,100 --> 00:12:30,190
non normalizing you cannot be normalize

00:12:27,550 --> 00:12:33,610
without normalizing the normalizing is

00:12:30,190 --> 00:12:35,800
fighting for consistency while not

00:12:33,610 --> 00:12:38,740
normalizing is dropping consistency

00:12:35,800 --> 00:12:41,260
altogether as there are rules for

00:12:38,740 --> 00:12:43,780
normalization there are patterns for D

00:12:41,260 --> 00:12:46,270
normalization and the normalization

00:12:43,780 --> 00:12:48,400
patterns will talk about rx10 aggregate

00:12:46,270 --> 00:12:49,690
and fetch I'll explain each of them I

00:12:48,400 --> 00:12:52,210
didn't event them

00:12:49,690 --> 00:12:54,040
I got will grab this from a blog post

00:12:52,210 --> 00:12:58,480
you can check on the references later

00:12:54,040 --> 00:13:01,060
and it's very well known blog post ok

00:12:58,480 --> 00:13:05,320
again this example of name and name you

00:13:01,060 --> 00:13:08,170
need echoed from name we produce name

00:13:05,320 --> 00:13:11,860
you need echoed this pattern is called X

00:13:08,170 --> 00:13:15,280
10 that means from one or more fields of

00:13:11,860 --> 00:13:18,910
an instance we extend them into another

00:13:15,280 --> 00:13:21,130
field so from name or at the same row at

00:13:18,910 --> 00:13:23,770
the same model instance I'm

00:13:21,130 --> 00:13:25,870
producing another field name you need a

00:13:23,770 --> 00:13:28,420
code that's extent that's the extent

00:13:25,870 --> 00:13:32,740
pattern and we could do that simply by

00:13:28,420 --> 00:13:34,690
ourselves coding that logic yes

00:13:32,740 --> 00:13:36,010
adding fields for implementing extended

00:13:34,690 --> 00:13:40,150
school but you know what's cooler

00:13:36,010 --> 00:13:43,090
indexes we could like remove that

00:13:40,150 --> 00:13:47,440
additional field and create an index on

00:13:43,090 --> 00:13:49,390
DB level that uses the function we

00:13:47,440 --> 00:13:52,930
define it at the B level you need a code

00:13:49,390 --> 00:13:55,000
and if we do that the database will

00:13:52,930 --> 00:13:57,240
create an index considering that

00:13:55,000 --> 00:14:00,310
expression Postgres will create an index

00:13:57,240 --> 00:14:02,710
considering that expression so index is

00:14:00,310 --> 00:14:05,470
like on this case I did normalize a

00:14:02,710 --> 00:14:07,840
table that the database keeps inseam for

00:14:05,470 --> 00:14:11,800
you that's what happening at the B level

00:14:07,840 --> 00:14:14,710
the index is like another table where we

00:14:11,800 --> 00:14:17,410
have the names without accent and that

00:14:14,710 --> 00:14:17,970
that other table that index is pointing

00:14:17,410 --> 00:14:22,210
to the right

00:14:17,970 --> 00:14:26,680
IDs of our real table with the accented

00:14:22,210 --> 00:14:28,750
names and that's like Al for free for

00:14:26,680 --> 00:14:30,760
you you just need to declare the index

00:14:28,750 --> 00:14:34,450
it's not completely free because we are

00:14:30,760 --> 00:14:39,220
trading insert update speed for select

00:14:34,450 --> 00:14:41,470
speed we'll have faster selects but but

00:14:39,220 --> 00:14:44,910
slower inserts and updates because now

00:14:41,470 --> 00:14:48,580
the database needs to update both the

00:14:44,910 --> 00:14:50,770
the actual table and the index but the

00:14:48,580 --> 00:14:53,500
same would happen with an additional

00:14:50,770 --> 00:14:54,940
field the same will happen because you'd

00:14:53,500 --> 00:14:57,250
have an additional field and you have

00:14:54,940 --> 00:15:02,680
also to compute it and update it every

00:14:57,250 --> 00:15:05,190
time that the extension of the dependent

00:15:02,680 --> 00:15:07,750
field is being updated

00:15:05,190 --> 00:15:10,540
however in excess don't work for

00:15:07,750 --> 00:15:13,780
aggregations on other table rules like

00:15:10,540 --> 00:15:17,560
the user total ordered example we saw

00:15:13,780 --> 00:15:19,210
before and computing o'the user total

00:15:17,560 --> 00:15:21,510
water is actually another

00:15:19,210 --> 00:15:25,270
denormalization pattern called aggregate

00:15:21,510 --> 00:15:27,790
that means on the one side of a

00:15:25,270 --> 00:15:31,000
one-to-many relationship we aggregate

00:15:27,790 --> 00:15:33,850
data from the many into a new field yeah

00:15:31,000 --> 00:15:35,110
that's that's complex but I explained so

00:15:33,850 --> 00:15:38,320
we have this okay

00:15:35,110 --> 00:15:40,240
we have total wordid at user it's an

00:15:38,320 --> 00:15:44,440
additional field is bad because we have

00:15:40,240 --> 00:15:46,540
to keep it updated and everything what

00:15:44,440 --> 00:15:48,640
we are doing here is that from many

00:15:46,540 --> 00:15:52,180
totals for that user

00:15:48,640 --> 00:15:55,630
we are aggregating them into total order

00:15:52,180 --> 00:15:57,790
okay so we are on the we are on the one

00:15:55,630 --> 00:16:00,899
side of a one-to-many relationship and

00:15:57,790 --> 00:16:04,180
we are grabbing many stuff and adding

00:16:00,899 --> 00:16:09,850
and computing that and adding to a field

00:16:04,180 --> 00:16:11,500
of or one side too so aggregate we could

00:16:09,850 --> 00:16:15,220
like to do it manually again but we

00:16:11,500 --> 00:16:16,690
could use also materialized views if you

00:16:15,220 --> 00:16:19,540
are using Postgres you can try the

00:16:16,690 --> 00:16:22,000
Django PG views library materialized

00:16:19,540 --> 00:16:25,450
views have nothing to do with Django

00:16:22,000 --> 00:16:27,220
views the views you handle requests and

00:16:25,450 --> 00:16:30,550
things like that no that's a DB level

00:16:27,220 --> 00:16:34,000
thing where you can declare with Django

00:16:30,550 --> 00:16:36,399
PG views a materialized view and here we

00:16:34,000 --> 00:16:39,519
are declaring a user report materialized

00:16:36,399 --> 00:16:42,040
view it's like a model we have to set

00:16:39,519 --> 00:16:45,310
the fields but unfortunately we have to

00:16:42,040 --> 00:16:47,980
write the sequel code for it but if we

00:16:45,310 --> 00:16:51,130
do that we can use this user report

00:16:47,980 --> 00:16:54,100
class as a regular query set everywhere

00:16:51,130 --> 00:16:56,980
we want and by doing that we are

00:16:54,100 --> 00:17:01,920
denormalizing without actually having to

00:16:56,980 --> 00:17:04,780
handle the denormalization sink code

00:17:01,920 --> 00:17:07,110
basically we just use user report as a

00:17:04,780 --> 00:17:10,209
regular model and then once in awhile

00:17:07,110 --> 00:17:11,799
with grunts our bit signal whatever you

00:17:10,209 --> 00:17:13,929
want you need to refresh your

00:17:11,799 --> 00:17:15,910
materialized view so that's like a query

00:17:13,929 --> 00:17:18,220
where the results are stored on your

00:17:15,910 --> 00:17:20,169
database and you can even like filter

00:17:18,220 --> 00:17:23,890
over that query and use it like a

00:17:20,169 --> 00:17:26,500
regular set for a more automatic

00:17:23,890 --> 00:17:28,900
solution of aggregate we can use the

00:17:26,500 --> 00:17:33,070
library Django the norm and libraries

00:17:28,900 --> 00:17:36,220
awesome is quite magic if we we can

00:17:33,070 --> 00:17:40,660
remove Torah water okay and we can

00:17:36,220 --> 00:17:44,650
declare a Torah ordered function with

00:17:40,660 --> 00:17:47,020
the normalized decorator and we say that

00:17:44,650 --> 00:17:49,070
we are creating a decimal field that

00:17:47,020 --> 00:17:51,980
depends on related order

00:17:49,070 --> 00:17:55,600
and then we write the code to grab the

00:17:51,980 --> 00:17:59,030
actual total order and if we do that

00:17:55,600 --> 00:18:01,880
this library will execute that function

00:17:59,030 --> 00:18:04,760
only when it needs only when related

00:18:01,880 --> 00:18:07,660
change things change like related orders

00:18:04,760 --> 00:18:10,730
change it will execute that code again

00:18:07,660 --> 00:18:12,980
for doing that Django the norm uses

00:18:10,730 --> 00:18:15,790
database triggers so it detects with

00:18:12,980 --> 00:18:18,140
triggers at the database level if

00:18:15,790 --> 00:18:20,060
related things are being updated so you

00:18:18,140 --> 00:18:23,150
don't have to worry about signals

00:18:20,060 --> 00:18:26,480
it handles that at the DB level but it

00:18:23,150 --> 00:18:29,120
does what it does is it Mack marks the

00:18:26,480 --> 00:18:33,260
instances as dirty adding and then it

00:18:29,120 --> 00:18:35,330
needs run code to recompute the actual

00:18:33,260 --> 00:18:37,280
values actually normalized values and

00:18:35,330 --> 00:18:41,000
you can do that with a post request

00:18:37,280 --> 00:18:43,790
middleware or with a periodic task but

00:18:41,000 --> 00:18:45,280
it needs to mark it automatically marks

00:18:43,790 --> 00:18:48,590
things are the s Dury

00:18:45,280 --> 00:18:51,650
to recompute the normalized fields but

00:18:48,590 --> 00:18:54,710
you have to run that somehow either per

00:18:51,650 --> 00:18:57,500
request or like with the periodic tasks

00:18:54,710 --> 00:19:01,040
if you can support eventual consistency

00:18:57,500 --> 00:19:03,800
on those eventual consistency on those

00:19:01,040 --> 00:19:06,830
fields from our lightweight but less

00:19:03,800 --> 00:19:10,900
powerful solution you can try libraries

00:19:06,830 --> 00:19:13,430
that just use signals like those too and

00:19:10,900 --> 00:19:16,520
finally the last pattern of the

00:19:13,430 --> 00:19:19,520
normalization here imagine if we have a

00:19:16,520 --> 00:19:22,790
dress user address that a reference to

00:19:19,520 --> 00:19:25,090
user has the actual written address and

00:19:22,790 --> 00:19:28,280
a lot to demonstrate for that Alvers

00:19:25,090 --> 00:19:31,460
this is headland because from the

00:19:28,280 --> 00:19:34,100
address we can compute plot to the ocean

00:19:31,460 --> 00:19:35,980
yes there is redundancy here but is

00:19:34,100 --> 00:19:38,720
quite difficult to get rid of this

00:19:35,980 --> 00:19:41,570
technically if we had a geo coder on our

00:19:38,720 --> 00:19:43,370
server we could address would be

00:19:41,570 --> 00:19:45,770
actually a foreign key to a geocoding

00:19:43,370 --> 00:19:48,260
table with latitude and longitude fields

00:19:45,770 --> 00:19:50,450
so it could like make this joint to grab

00:19:48,260 --> 00:19:53,150
the lot to them understood but in

00:19:50,450 --> 00:19:56,930
practice geocoding is not foreign key

00:19:53,150 --> 00:20:00,290
problem and it's very slow so we need to

00:19:56,930 --> 00:20:02,090
fetch from the geo coder we need to from

00:20:00,290 --> 00:20:02,809
the many side of a one-to-many

00:20:02,090 --> 00:20:05,450
relationship

00:20:02,809 --> 00:20:06,980
so multiple addresses can have the same

00:20:05,450 --> 00:20:09,649
latitude and longitude so we are on the

00:20:06,980 --> 00:20:13,039
mini side now from the mini side we need

00:20:09,649 --> 00:20:15,409
to fetch data from there one side into a

00:20:13,039 --> 00:20:17,539
new field on our mini side so we have

00:20:15,409 --> 00:20:19,279
like many addresses with the same

00:20:17,539 --> 00:20:22,340
latitude understood and from those

00:20:19,279 --> 00:20:23,840
addresses we need to fetch data we need

00:20:22,340 --> 00:20:25,429
to fetch the latitude and longitude

00:20:23,840 --> 00:20:30,440
that's the fetch pattern it's like the

00:20:25,429 --> 00:20:32,690
opposite of the aggregate one and using

00:20:30,440 --> 00:20:34,999
address we can fetch lot to the

00:20:32,690 --> 00:20:39,110
longitude from a geo coder that's the

00:20:34,999 --> 00:20:42,289
idea here and actually a lot of business

00:20:39,110 --> 00:20:44,960
logic is fetch so far so we fetch I

00:20:42,289 --> 00:20:47,809
wouldn't recommend custom tools third

00:20:44,960 --> 00:20:50,809
part libraries I think it's just better

00:20:47,809 --> 00:20:53,480
to be expressed as explicit as possible

00:20:50,809 --> 00:20:55,850
and handed handle it yourself on your

00:20:53,480 --> 00:20:58,730
own code and write writing test and

00:20:55,850 --> 00:21:01,159
everything but how to do that you can

00:20:58,730 --> 00:21:03,379
just like create helper functions to

00:21:01,159 --> 00:21:05,869
update that model instances and just

00:21:03,379 --> 00:21:07,999
update them with those helper functions

00:21:05,869 --> 00:21:11,210
and of course write test for that and

00:21:07,999 --> 00:21:14,210
everything you could do it on a lower

00:21:11,210 --> 00:21:16,549
level and override save and use

00:21:14,210 --> 00:21:19,460
something like Django model to use field

00:21:16,549 --> 00:21:21,169
tracker to track changes on addresses to

00:21:19,460 --> 00:21:23,210
update the latitude understood every

00:21:21,169 --> 00:21:25,909
time the address changes or you can even

00:21:23,210 --> 00:21:28,999
try Django lifecycle hooks and that's

00:21:25,909 --> 00:21:32,480
like a rails inspired idea of if that

00:21:28,999 --> 00:21:33,740
few changes run this code you can do it

00:21:32,480 --> 00:21:37,519
quite declaratively

00:21:33,740 --> 00:21:40,090
with this library there are other

00:21:37,519 --> 00:21:42,649
concerns we overlooked in this talk that

00:21:40,090 --> 00:21:46,940
when normalizing you should be careful

00:21:42,649 --> 00:21:49,909
with concurrency because it's quite it's

00:21:46,940 --> 00:21:52,039
quite ironic but it's actually easier to

00:21:49,909 --> 00:21:54,559
handle concurrency if you keep it all

00:21:52,039 --> 00:21:56,269
into a single row because like you are

00:21:54,559 --> 00:21:59,899
operating a single row so it's difficult

00:21:56,269 --> 00:22:01,279
for two different processes update the

00:21:59,899 --> 00:22:03,590
row at the same time but if you are

00:22:01,279 --> 00:22:05,210
dealing with multiple tables you have to

00:22:03,590 --> 00:22:08,899
be more careful with locking and

00:22:05,210 --> 00:22:10,850
transactions and before the normalize

00:22:08,899 --> 00:22:13,369
you should actually profile what slow

00:22:10,850 --> 00:22:16,070
you shouldn't be normalize blindly just

00:22:13,369 --> 00:22:19,369
trying to denormalize

00:22:16,070 --> 00:22:21,440
and using aggregations that you compute

00:22:19,369 --> 00:22:23,509
manually because you think it's so you

00:22:21,440 --> 00:22:25,729
should profile and check if it's really

00:22:23,509 --> 00:22:29,350
slow and you can even track caching

00:22:25,729 --> 00:22:31,369
before properly denormalizing stuff and

00:22:29,350 --> 00:22:33,350
after the normalizing you have to

00:22:31,369 --> 00:22:36,580
profile to see if it really work do you

00:22:33,350 --> 00:22:39,799
don't don't you can't do it blindly

00:22:36,580 --> 00:22:43,789
there are other de normalization ideas I

00:22:39,799 --> 00:22:46,340
don't have time to dive into but one

00:22:43,789 --> 00:22:51,080
great idea is to separate transactional

00:22:46,340 --> 00:22:53,599
data from analytical data and this was

00:22:51,080 --> 00:22:55,729
people talking about this at Django come

00:22:53,599 --> 00:22:58,070
before it last year Django Cohn the

00:22:55,729 --> 00:23:00,559
de-normalized querying engine design

00:22:58,070 --> 00:23:03,440
pattern from Samoa listen and yesterday

00:23:00,559 --> 00:23:07,399
Kate Glickman kind of talked about that

00:23:03,440 --> 00:23:09,859
where she accelerated Django admins

00:23:07,399 --> 00:23:11,720
filtering and aggregations using

00:23:09,859 --> 00:23:14,269
elasticsearch so basically they are

00:23:11,720 --> 00:23:16,849
separating transactional data the actual

00:23:14,269 --> 00:23:19,789
real data from analytical data that is

00:23:16,849 --> 00:23:23,690
computed data and can even be ventral

00:23:19,789 --> 00:23:27,859
eventually consistently and there's like

00:23:23,690 --> 00:23:30,200
a crazy idea of transaction log plus

00:23:27,859 --> 00:23:32,419
materialized views which is the

00:23:30,200 --> 00:23:34,729
transaction log is the real truth about

00:23:32,419 --> 00:23:37,580
the data and materialized views are

00:23:34,729 --> 00:23:39,559
updated after the transaction log is

00:23:37,580 --> 00:23:43,369
updated that's like a Big Data thing

00:23:39,559 --> 00:23:45,440
that manages to to keep consistency

00:23:43,369 --> 00:23:48,470
consistency even though it's like

00:23:45,440 --> 00:23:50,529
handling a lot of data check this talk

00:23:48,470 --> 00:23:53,809
here for more information

00:23:50,529 --> 00:23:56,450
those are the references and thank you

00:23:53,809 --> 00:23:58,220
if you have any questions I don't think

00:23:56,450 --> 00:24:00,190
we have time but you can ask me at the

00:23:58,220 --> 00:24:07,959
hall thank you very much

00:24:00,190 --> 00:24:07,959

YouTube URL: https://www.youtube.com/watch?v=01Hm2-NAM3w


