Title: A Gentle Introduction to Machine Learning and React Native - React Native - June 2019
Publication date: 2019-06-30
Playlist: React Native London
Description: 
	Presented by Valentin Nagacevschi

A gentle introduction to AI and Machine Learning and how to make them work together in your React Native app. We will cover some common patterns and will also touch on the latest Apple CoreML technologies.

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,060 --> 00:00:07,710
hi everybody thank you for having me

00:00:02,669 --> 00:00:12,840
here thank his markets from for hosting

00:00:07,710 --> 00:00:16,049
us and I just want to tell you that five

00:00:12,840 --> 00:00:19,710
months back when I first moved when I

00:00:16,049 --> 00:00:22,250
moved in London Radney tip was the first

00:00:19,710 --> 00:00:24,840
meetup I joined and I was amazed about

00:00:22,250 --> 00:00:28,760
the number of smart people's which can

00:00:24,840 --> 00:00:33,329
fit in one room five months back

00:00:28,760 --> 00:00:36,120
laughter and the IQ is just going up so

00:00:33,329 --> 00:00:40,800
yeah I'm really impressed

00:00:36,120 --> 00:00:43,230
also I want to thank Brian because this

00:00:40,800 --> 00:00:45,480
couldn't happen without his dedication

00:00:43,230 --> 00:00:53,010
hard work so let's give it a round of

00:00:45,480 --> 00:00:56,370
applause to Brian okay so today I'm

00:00:53,010 --> 00:00:59,760
going to to talk about react native and

00:00:56,370 --> 00:01:02,789
machine learning which is a pretty

00:00:59,760 --> 00:01:07,049
strange subject and I hope I already

00:01:02,789 --> 00:01:12,000
have your attention it's a summary we're

00:01:07,049 --> 00:01:14,040
going to cover the introduction a gentle

00:01:12,000 --> 00:01:15,990
introduction emotional learning and then

00:01:14,040 --> 00:01:18,869
how to use a machine learning with react

00:01:15,990 --> 00:01:22,200
native specifically how to design a

00:01:18,869 --> 00:01:26,880
model in in Python and save it or just

00:01:22,200 --> 00:01:31,530
use pre trained model 18 tensorflow core

00:01:26,880 --> 00:01:34,880
ml or and then built our custom react

00:01:31,530 --> 00:01:38,509
native components and used later on

00:01:34,880 --> 00:01:43,200
applicable neural network for for

00:01:38,509 --> 00:01:46,520
personalization why do you think it's

00:01:43,200 --> 00:01:48,450
important to to talk a little bit about

00:01:46,520 --> 00:01:52,680
artificial intelligence machine learning

00:01:48,450 --> 00:01:55,200
so there are a couple of reasons first

00:01:52,680 --> 00:01:59,119
reason is that the machine will come

00:01:55,200 --> 00:02:02,280
after about jobs so probably we're all

00:01:59,119 --> 00:02:05,399
compassionate about you know the uber

00:02:02,280 --> 00:02:07,530
drivers and the truck drivers and Tesco

00:02:05,399 --> 00:02:09,479
cashier losing their jobs but the

00:02:07,530 --> 00:02:10,880
reality that a lot of white collar's

00:02:09,479 --> 00:02:13,310
jobs are under threat

00:02:10,880 --> 00:02:16,370
when I'm saying that we're talking about

00:02:13,310 --> 00:02:20,180
five to ten weeks here's max and the

00:02:16,370 --> 00:02:23,480
doctors already perform worse than the

00:02:20,180 --> 00:02:29,270
machining diagnosis and imagistic sand

00:02:23,480 --> 00:02:32,120
reading CT scans and MRIs lawyers are

00:02:29,270 --> 00:02:35,960
doom already because the machines are

00:02:32,120 --> 00:02:40,640
much much performant in supporting text

00:02:35,960 --> 00:02:42,260
and finding similar cases because not

00:02:40,640 --> 00:02:44,980
everybody is on the court you know

00:02:42,260 --> 00:02:49,280
saying there's your taxes or whatever

00:02:44,980 --> 00:02:51,560
and really if you think about it it's

00:02:49,280 --> 00:02:54,410
it's much easier to to generate code

00:02:51,560 --> 00:02:58,700
then you know to replace a plumber or a

00:02:54,410 --> 00:03:01,430
nurse so that's that's pretty important

00:02:58,700 --> 00:03:04,370
then I usually crop everyone attention

00:03:01,430 --> 00:03:06,140
when I'm talking about their jobs second

00:03:04,370 --> 00:03:07,970
reason is that we're talking a lot about

00:03:06,140 --> 00:03:11,780
a lot of money

00:03:07,970 --> 00:03:14,750
by the end of 2017 we reach five

00:03:11,780 --> 00:03:18,790
billions in investments and as you see

00:03:14,750 --> 00:03:22,550
in early stages it's only growing this

00:03:18,790 --> 00:03:25,490
this kind of investment and the third

00:03:22,550 --> 00:03:27,470
reason that if you don't do it someone

00:03:25,490 --> 00:03:30,950
else will do it in your place

00:03:27,470 --> 00:03:32,960
so thinking I really need to ask you to

00:03:30,950 --> 00:03:34,940
think about artificial intelligence

00:03:32,960 --> 00:03:39,040
machine learning not like the internet

00:03:34,940 --> 00:03:42,320
revolution but mainly mostly like

00:03:39,040 --> 00:03:47,930
electricity powering everything so it

00:03:42,320 --> 00:03:50,600
will be literally ever okay so what is

00:03:47,930 --> 00:03:53,930
machine learning six years back a guy

00:03:50,600 --> 00:03:57,380
named Arthur Samuel this courted first

00:03:53,930 --> 00:03:59,450
time this this term and said that it

00:03:57,380 --> 00:04:00,500
gives the computer or the computers the

00:03:59,450 --> 00:04:03,350
ability to learn without being

00:04:00,500 --> 00:04:06,440
explicitly programmed so we have 60

00:04:03,350 --> 00:04:09,650
years back the first you know sign that

00:04:06,440 --> 00:04:14,300
we our servers are not really required

00:04:09,650 --> 00:04:19,239
you know well it's pretty fancy as as a

00:04:14,300 --> 00:04:22,100
definition but what's really the machine

00:04:19,239 --> 00:04:25,130
in essence is just like a you know like

00:04:22,100 --> 00:04:27,680
a big label so it's just labeling things

00:04:25,130 --> 00:04:30,470
so you have like I don't know an image

00:04:27,680 --> 00:04:34,190
and then you say okay it's a cat or it's

00:04:30,470 --> 00:04:39,260
a car or it's a I don't know a terrorist

00:04:34,190 --> 00:04:42,790
or it's a dog or it's a not hot dog so I

00:04:39,260 --> 00:04:46,300
don't know how many of you so they're

00:04:42,790 --> 00:04:49,880
not hot dog episode in Silicon Valley

00:04:46,300 --> 00:04:53,660
well not so many so it's ready let's

00:04:49,880 --> 00:04:56,810
have a look what do we you say if I told

00:04:53,660 --> 00:05:01,190
you there is an app on the map a support

00:04:56,810 --> 00:05:03,220
just demo okay let's start with a hot

00:05:01,190 --> 00:05:03,220
dog

00:05:07,490 --> 00:05:16,729
oh

00:05:13,180 --> 00:05:19,099
for the observed jinyang my

00:05:16,729 --> 00:05:21,500
beautiful little Asiatic friend I'm

00:05:19,099 --> 00:05:23,750
going to buy you the palapa of your life

00:05:21,500 --> 00:05:26,240
we will have twelve posts rated palm

00:05:23,750 --> 00:05:27,560
leaves you'll never feel exposed again

00:05:26,240 --> 00:05:31,360
I'm gonna be raped

00:05:27,560 --> 00:05:34,360
 you guilfoyle do pizza yes - pizza

00:05:31,360 --> 00:05:34,360
Hey

00:05:38,430 --> 00:05:47,470
not hot dog wait what the that's it

00:05:44,140 --> 00:05:54,370
it only does hot dogs no and a not hot

00:05:47,470 --> 00:05:57,910
dog okay so it's it's very funny but

00:05:54,370 --> 00:06:00,730
actually it's pretty true because it's

00:05:57,910 --> 00:06:03,490
really doing not hot dogs so this model

00:06:00,730 --> 00:06:06,340
was trained with images which are

00:06:03,490 --> 00:06:08,920
different from hot dogs and labeled as

00:06:06,340 --> 00:06:11,110
not hot dogs so a not hot dog is a

00:06:08,920 --> 00:06:12,850
special category for images which are

00:06:11,110 --> 00:06:17,260
really not hot dogs so it's not a

00:06:12,850 --> 00:06:19,390
fallback so this is a react native

00:06:17,260 --> 00:06:23,980
application and I think you can still

00:06:19,390 --> 00:06:27,820
find the source of non gait so whoever

00:06:23,980 --> 00:06:31,030
is you know curious please please get a

00:06:27,820 --> 00:06:33,730
coat and that's not the only a react

00:06:31,030 --> 00:06:35,200
native application which is related to

00:06:33,730 --> 00:06:38,710
the machine learning there is another

00:06:35,200 --> 00:06:40,990
one which is called meek or not and Nick

00:06:38,710 --> 00:06:43,480
thought it was about him and I told him

00:06:40,990 --> 00:06:45,790
that actually it's about Nicolas Cage so

00:06:43,480 --> 00:06:47,890
actually this app is recognizing Nicolas

00:06:45,790 --> 00:06:49,900
Cage so I can only think of I don't know

00:06:47,890 --> 00:06:56,260
20 use cases but all of them involve

00:06:49,900 --> 00:07:00,190
obviously Nicolas Cage so that's that's

00:06:56,260 --> 00:07:05,200
the ideas but the question here is okay

00:07:00,190 --> 00:07:08,290
how really the machine working learn

00:07:05,200 --> 00:07:10,510
works so let's look a little bit about

00:07:08,290 --> 00:07:12,700
the mass I know it's late but I really

00:07:10,510 --> 00:07:16,870
want to you know see your brand

00:07:12,700 --> 00:07:20,080
expanding so the idea behind it is that

00:07:16,870 --> 00:07:23,170
we have I don't know a set of features

00:07:20,080 --> 00:07:26,350
of properties of some things so think

00:07:23,170 --> 00:07:29,410
about you know houses in London yeah so

00:07:26,350 --> 00:07:33,160
the X represent the parameters of the

00:07:29,410 --> 00:07:35,740
house like the surface the location the

00:07:33,160 --> 00:07:40,060
number of rooms the number of bathrooms

00:07:35,740 --> 00:07:43,390
and so on and the Y represent a price so

00:07:40,060 --> 00:07:46,620
we want to figure out what's the

00:07:43,390 --> 00:07:49,280
function so how can we predict future

00:07:46,620 --> 00:07:52,910
let's say prices of the

00:07:49,280 --> 00:07:53,480
houses in London based on the data we

00:07:52,910 --> 00:07:55,550
can

00:07:53,480 --> 00:07:58,700
I don't know collected or a scrap from

00:07:55,550 --> 00:08:01,190
from the web so the approach is very

00:07:58,700 --> 00:08:04,190
simple it's just a linear thing so we

00:08:01,190 --> 00:08:07,850
just multiplied the features by some

00:08:04,190 --> 00:08:12,470
weights and add some biases yeah and we

00:08:07,850 --> 00:08:17,270
get result which is which we try to make

00:08:12,470 --> 00:08:20,000
it as as close to to the real output as

00:08:17,270 --> 00:08:23,930
possible just by guessing the weights

00:08:20,000 --> 00:08:26,660
and biases so that's basically the whole

00:08:23,930 --> 00:08:29,840
thing so imagine that there is a scatter

00:08:26,660 --> 00:08:32,780
you know points and we try to figure out

00:08:29,840 --> 00:08:36,020
a line which is going through through

00:08:32,780 --> 00:08:38,930
this point and it's a the best

00:08:36,020 --> 00:08:40,550
approximation so we're going to use what

00:08:38,930 --> 00:08:42,830
it's called a loss function and we are

00:08:40,550 --> 00:08:48,290
trying to minimize it so that the line

00:08:42,830 --> 00:08:51,290
you know is the best approximation now

00:08:48,290 --> 00:08:55,940
probably you think that okay not many

00:08:51,290 --> 00:08:59,960
things in life are linear okay so I I

00:08:55,940 --> 00:09:03,050
really understand that so for that we

00:08:59,960 --> 00:09:06,320
have this this deep neural networks

00:09:03,050 --> 00:09:09,530
which are here to help us so the idea is

00:09:06,320 --> 00:09:11,360
that if we have in the left part the

00:09:09,530 --> 00:09:15,380
input so the features which we were

00:09:11,360 --> 00:09:18,080
talking about we are going to derive new

00:09:15,380 --> 00:09:21,320
features in the hidden layers in order

00:09:18,080 --> 00:09:23,780
to approximate you know some nonlinear

00:09:21,320 --> 00:09:26,960
functions these nodes which are in the

00:09:23,780 --> 00:09:28,850
hidden layers are called perceptrons but

00:09:26,960 --> 00:09:32,210
they are not very complicated just

00:09:28,850 --> 00:09:34,730
performing this linear equations or

00:09:32,210 --> 00:09:38,540
multiplying the inputs with some weights

00:09:34,730 --> 00:09:41,240
summing them all together but the linear

00:09:38,540 --> 00:09:44,150
combination of linear things that it's

00:09:41,240 --> 00:09:47,839
just a linear thing right so we really

00:09:44,150 --> 00:09:49,520
need to add some non-linearity there so

00:09:47,839 --> 00:09:52,520
this is what it's called activation

00:09:49,520 --> 00:09:55,089
function so at the end we are going to

00:09:52,520 --> 00:09:58,160
have at the output for the next layers

00:09:55,089 --> 00:10:02,089
no linear combination of the input so

00:09:58,160 --> 00:10:03,170
that's the whole idea but okay I think

00:10:02,089 --> 00:10:06,709
that it's not enough

00:10:03,170 --> 00:10:09,320
or for today I think you you can really

00:10:06,709 --> 00:10:11,750
break the ice of any party just by

00:10:09,320 --> 00:10:14,120
talking about it let's see on the

00:10:11,750 --> 00:10:17,660
engineer side how can we really do that

00:10:14,120 --> 00:10:20,209
and machine learning algorithm is not

00:10:17,660 --> 00:10:23,110
better than the data you are using it so

00:10:20,209 --> 00:10:27,649
the idea that if you have quality data

00:10:23,110 --> 00:10:31,220
so labeled everything you can split the

00:10:27,649 --> 00:10:33,800
data into training and test data use the

00:10:31,220 --> 00:10:38,510
training data to train the model and

00:10:33,800 --> 00:10:44,230
then the test data to evaluate the model

00:10:38,510 --> 00:10:46,279
and see how accurate it was okay

00:10:44,230 --> 00:10:47,930
fortunately we don't have to do it by

00:10:46,279 --> 00:10:50,029
hand and there are a lot of frameworks

00:10:47,930 --> 00:10:53,110
sucio which you can do in most of them

00:10:50,029 --> 00:10:55,579
are in Python but don't worry because

00:10:53,110 --> 00:10:59,060
it's really simple even the data

00:10:55,579 --> 00:11:04,250
scientists can use it so any data

00:10:59,060 --> 00:11:06,260
scientist you okay so I'm going to give

00:11:04,250 --> 00:11:08,750
you an example on how to do a deep

00:11:06,260 --> 00:11:11,510
neural network in just five line of code

00:11:08,750 --> 00:11:17,300
so we're talking about a data set of

00:11:11,510 --> 00:11:20,959
images 28 by 28 pixels grayscale split

00:11:17,300 --> 00:11:24,949
across ten categories yeah like sandals

00:11:20,959 --> 00:11:27,649
like sneakers and so on the training set

00:11:24,949 --> 00:11:30,649
on this and test set are 60,000 and

00:11:27,649 --> 00:11:32,690
10,000 so how are we going to do that so

00:11:30,649 --> 00:11:36,980
the first thing is just load the data

00:11:32,690 --> 00:11:41,029
into the train and test set separate

00:11:36,980 --> 00:11:43,760
feature the eggs and labels the Y then

00:11:41,029 --> 00:11:46,519
we are going to build a model which is

00:11:43,760 --> 00:11:49,490
very clear so it's a sequence of layers

00:11:46,519 --> 00:11:54,069
the first layer is just flattening the

00:11:49,490 --> 00:11:56,899
image into a big factor of 784 values

00:11:54,069 --> 00:11:59,769
the second layer is a hidden layer which

00:11:56,899 --> 00:12:03,880
we talked about which is decreasing this

00:11:59,769 --> 00:12:06,410
dimensionality to only 128 using

00:12:03,880 --> 00:12:08,420
nonlinear activation called real you

00:12:06,410 --> 00:12:13,029
don't have to worry about that just you

00:12:08,420 --> 00:12:15,440
know get go to the whatever

00:12:13,029 --> 00:12:20,810
activations are available and just play

00:12:15,440 --> 00:12:22,790
with them until the accuracy is the

00:12:20,810 --> 00:12:25,250
right thing to do and then the last

00:12:22,790 --> 00:12:28,430
layer is the 10 categories so it is

00:12:25,250 --> 00:12:31,190
output layer so again it's a different

00:12:28,430 --> 00:12:33,950
activations with call softmax and then

00:12:31,190 --> 00:12:37,550
we are going to compile this model using

00:12:33,950 --> 00:12:41,600
an optimizer and then a loss function

00:12:37,550 --> 00:12:44,750
and we're look going to look about the

00:12:41,600 --> 00:12:47,959
metrics in here in the accuracy the

00:12:44,750 --> 00:12:51,410
fourth line is just the training so

00:12:47,959 --> 00:12:55,399
we're going to feed the model with a.m.

00:12:51,410 --> 00:12:57,920
to train sent so the takes and the y and

00:12:55,399 --> 00:13:01,160
we're going to tell that the model to

00:12:57,920 --> 00:13:04,130
try it in 10 epic so 10 a box means 10

00:13:01,160 --> 00:13:07,700
times redo the same thing so train and

00:13:04,130 --> 00:13:10,010
train and train again and very very

00:13:07,700 --> 00:13:12,800
important we are going to save this

00:13:10,010 --> 00:13:16,490
because after training we are going to

00:13:12,800 --> 00:13:20,480
retrain to the tracker it to keep in

00:13:16,490 --> 00:13:23,779
mind to save the values of the weights

00:13:20,480 --> 00:13:25,880
and the structure of the model so as you

00:13:23,779 --> 00:13:28,610
see it's pretty explanatory even if you

00:13:25,880 --> 00:13:33,100
don't know Python how many know Python

00:13:28,610 --> 00:13:39,860
oh okay okay so sorry about it I was

00:13:33,100 --> 00:13:42,350
trying to work smart so this is extra

00:13:39,860 --> 00:13:46,430
you can later on to a model evaluation

00:13:42,350 --> 00:13:49,720
and then you just get the test accuracy

00:13:46,430 --> 00:13:54,529
of 88% which is not bad giving just one

00:13:49,720 --> 00:13:56,870
hidden layer and we can also play with a

00:13:54,529 --> 00:14:01,970
little bit with the prediction and take

00:13:56,870 --> 00:14:04,640
the prediction of the whole test set and

00:14:01,970 --> 00:14:08,300
then randomly take the prediction of the

00:14:04,640 --> 00:14:10,970
fifties element and obviously or

00:14:08,300 --> 00:14:13,420
apparently the the category is number

00:14:10,970 --> 00:14:21,550
nine which probably sandals or whatever

00:14:13,420 --> 00:14:21,550
okay so so much with a with a Python and

00:14:22,790 --> 00:14:27,960
memorials let's see what you can do with

00:14:25,860 --> 00:14:29,700
react native and emotional areas there

00:14:27,960 --> 00:14:31,680
are a couple of components which are

00:14:29,700 --> 00:14:34,290
ready there but all of them are either

00:14:31,680 --> 00:14:37,650
outdated or incomplete or they didn't

00:14:34,290 --> 00:14:40,350
try to solve any problem so they just

00:14:37,650 --> 00:14:43,710
poured some some frameworks and say okay

00:14:40,350 --> 00:14:46,170
you can do the training in JavaScript on

00:14:43,710 --> 00:14:49,200
the interact native by just using this

00:14:46,170 --> 00:14:53,910
dis component this function so they're

00:14:49,200 --> 00:14:58,320
not very useful or so well you saw the

00:14:53,910 --> 00:14:59,120
not hot dog and Nico not what's the

00:14:58,320 --> 00:15:02,010
alternative

00:14:59,120 --> 00:15:04,410
well the alternative is to build your

00:15:02,010 --> 00:15:07,590
own reg native component focusing

00:15:04,410 --> 00:15:11,820
especially on what your application are

00:15:07,590 --> 00:15:15,660
looking for so in our case the idea is

00:15:11,820 --> 00:15:18,210
to use a pre trained model either from

00:15:15,660 --> 00:15:22,130
the saved one which you compare it from

00:15:18,210 --> 00:15:25,650
from Python or using from a native

00:15:22,130 --> 00:15:29,340
frameworks and just do the prediction

00:15:25,650 --> 00:15:31,740
yeah so there are two ways to do that on

00:15:29,340 --> 00:15:36,630
the Android is using the tensorflow

00:15:31,740 --> 00:15:40,830
which already hit version two and on the

00:15:36,630 --> 00:15:44,730
iOS the caramel was was launched pretty

00:15:40,830 --> 00:15:48,240
recent as version 3 and because we're in

00:15:44,730 --> 00:15:51,450
London and because everybody have iPhone

00:15:48,240 --> 00:15:55,500
X at least we are going to cover the

00:15:51,450 --> 00:15:57,900
core ml so what's Cora Mouse it's a

00:15:55,500 --> 00:16:00,900
native framework which is available in

00:15:57,900 --> 00:16:03,030
the iOS which provide a couple of things

00:16:00,900 --> 00:16:05,700
a lot of pre trained model especially

00:16:03,030 --> 00:16:09,240
which especially covering this three

00:16:05,700 --> 00:16:13,770
areas of vision so imaging speech and

00:16:09,240 --> 00:16:20,220
sound and text processing so and I'll be

00:16:13,770 --> 00:16:22,020
your own language processing also they

00:16:20,220 --> 00:16:24,930
just launched what's called the

00:16:22,020 --> 00:16:28,740
updatable neural network and the tools

00:16:24,930 --> 00:16:31,710
for converting your own models which you

00:16:28,740 --> 00:16:34,710
just build in in Python with

00:16:31,710 --> 00:16:36,890
frameworks also it's worth mentioning

00:16:34,710 --> 00:16:40,020
that they have a tool which is called

00:16:36,890 --> 00:16:43,740
create ml you can install it on your own

00:16:40,020 --> 00:16:48,720
Mac and play with either their models or

00:16:43,740 --> 00:16:51,089
build your own so these are the models

00:16:48,720 --> 00:16:53,399
which are ready available and there are

00:16:51,089 --> 00:16:56,040
a lot of them they're very specialized

00:16:53,399 --> 00:16:59,399
the inception from one to four version

00:16:56,040 --> 00:17:02,670
four for imaging obviously you have

00:16:59,399 --> 00:17:06,270
recurrent neural networks you have bird

00:17:02,670 --> 00:17:11,900
for text we have you have deep search

00:17:06,270 --> 00:17:18,870
and wavenet for speech and for sound

00:17:11,900 --> 00:17:20,970
just just by google these model names

00:17:18,870 --> 00:17:22,980
and you'll find out how they are

00:17:20,970 --> 00:17:26,640
optimizing how they're specifically

00:17:22,980 --> 00:17:29,400
solving different problems the second

00:17:26,640 --> 00:17:31,830
thing we want to cover is a update to

00:17:29,400 --> 00:17:33,710
put new around letter this is very

00:17:31,830 --> 00:17:35,820
important because you have already the

00:17:33,710 --> 00:17:38,550
models which are pre-trained

00:17:35,820 --> 00:17:40,880
and they're ready to use but you want to

00:17:38,550 --> 00:17:44,309
add some sort of personal

00:17:40,880 --> 00:17:48,720
personalization you know so let the

00:17:44,309 --> 00:17:51,170
application use the user input in order

00:17:48,720 --> 00:17:54,929
to tweak a little bit your model and

00:17:51,170 --> 00:17:56,429
behave in a personalized way so how is

00:17:54,929 --> 00:17:58,770
this possible

00:17:56,429 --> 00:18:02,820
first of all we have a couple of layers

00:17:58,770 --> 00:18:06,300
which are not fix are not frozen with

00:18:02,820 --> 00:18:08,309
their pre trained weights second one we

00:18:06,300 --> 00:18:13,890
are going to feed the training data from

00:18:08,309 --> 00:18:16,460
the user through the whole model but

00:18:13,890 --> 00:18:21,540
also independent in order to calculate

00:18:16,460 --> 00:18:23,670
the loss and third step through the

00:18:21,540 --> 00:18:26,460
optimizer we're going to update and

00:18:23,670 --> 00:18:29,309
change and tweak the weights in the last

00:18:26,460 --> 00:18:32,970
layers right so in this way we are going

00:18:29,309 --> 00:18:36,780
to do that to tweak our model in a

00:18:32,970 --> 00:18:39,090
personalized way based on the user input

00:18:36,780 --> 00:18:41,130
the beauty of this thing is that this

00:18:39,090 --> 00:18:42,930
can happen in the application itself

00:18:41,130 --> 00:18:45,810
because it's not costly

00:18:42,930 --> 00:18:48,510
in terms of performance and it also

00:18:45,810 --> 00:18:50,760
comes with a benefit of you know privacy

00:18:48,510 --> 00:18:53,490
because we're not going to you to send

00:18:50,760 --> 00:18:59,370
this information across the net or the

00:18:53,490 --> 00:19:04,770
cloud okay so on the on the tools we

00:18:59,370 --> 00:19:07,260
have this way of converting the the

00:19:04,770 --> 00:19:10,500
models which we eventually design or

00:19:07,260 --> 00:19:13,740
maybe we can have pretty models because

00:19:10,500 --> 00:19:16,020
there are a lot of them and then we can

00:19:13,740 --> 00:19:18,780
convert them while preserving their

00:19:16,020 --> 00:19:23,130
weights and biases and also we can

00:19:18,780 --> 00:19:26,310
modify them by adding future layers

00:19:23,130 --> 00:19:30,060
which will be updatable so could be

00:19:26,310 --> 00:19:33,270
personalized as they take away for

00:19:30,060 --> 00:19:37,590
tonight there are a couple of things I

00:19:33,270 --> 00:19:41,270
want to remember you to remember first

00:19:37,590 --> 00:19:43,770
is that python is a language for the

00:19:41,270 --> 00:19:46,710
machine learning at the facto language

00:19:43,770 --> 00:19:49,440
salon I mean you can try with anything

00:19:46,710 --> 00:19:51,990
else but really all these new things are

00:19:49,440 --> 00:19:56,150
read in the Python so it's pointless to

00:19:51,990 --> 00:19:59,610
try to do hipster things second one is

00:19:56,150 --> 00:20:01,950
always use as much as possible pre train

00:19:59,610 --> 00:20:04,140
models why is that because you will

00:20:01,950 --> 00:20:07,050
never have let's say the chance of

00:20:04,140 --> 00:20:10,980
having I don't know billions of images

00:20:07,050 --> 00:20:14,460
with cats like Google have right so even

00:20:10,980 --> 00:20:17,430
if you your model is means brilliant you

00:20:14,460 --> 00:20:21,660
don't have enough data the third one is

00:20:17,430 --> 00:20:24,510
that build your own native react native

00:20:21,660 --> 00:20:27,390
custom component is focused on what it's

00:20:24,510 --> 00:20:30,810
supposed to be you know don't try to you

00:20:27,390 --> 00:20:32,640
know use react native to design a new

00:20:30,810 --> 00:20:35,160
model and train it from scratch it's

00:20:32,640 --> 00:20:39,380
pointless and obviously you burn the

00:20:35,160 --> 00:20:42,180
battery out and the first one is just

00:20:39,380 --> 00:20:45,030
use the updatable models for for

00:20:42,180 --> 00:20:48,960
personalization and preserve the privacy

00:20:45,030 --> 00:20:51,600
of the user and the last one is keep

00:20:48,960 --> 00:20:55,910
learning new things thank you very much

00:20:51,600 --> 00:20:57,050
and I hope I kept you awake

00:20:55,910 --> 00:21:02,210
you

00:20:57,050 --> 00:21:02,210

YouTube URL: https://www.youtube.com/watch?v=doPpnA_91tc


