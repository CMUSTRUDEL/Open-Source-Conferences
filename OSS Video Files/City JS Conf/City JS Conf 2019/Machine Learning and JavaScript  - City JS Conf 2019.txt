Title: Machine Learning and JavaScript  - City JS Conf 2019
Publication date: 2019-05-14
Playlist: City JS Conf 2019
Description: 
	Elle Haproff

JavaScript probably isn't the first language which springs to mind when you say 'machine learning', but that does seem to be changing, especially with the release of TensorFlow.js last year.
 
This talk gives a high level overview of what machine learning and neural networks are, shows how to get started with TensorFlow.js, and demos a number of projects which use JavaScript and AI.

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,030 --> 00:00:04,140
for that lovely introduction I work at

00:00:02,490 --> 00:00:05,819
Marvel app not Marvel not that one

00:00:04,140 --> 00:00:09,570
Marvel app which is a collaborative

00:00:05,819 --> 00:00:11,580
design platform as summoner mentioned

00:00:09,570 --> 00:00:14,299
I'm the co-creator of AIG Esther rocks

00:00:11,580 --> 00:00:18,470
which is a collection of a I powered

00:00:14,299 --> 00:00:21,480
JavaScript apps and demos and I also ran

00:00:18,470 --> 00:00:24,510
AI JavaScript London which is a meet-up

00:00:21,480 --> 00:00:27,029
for JavaScript developers with talks and

00:00:24,510 --> 00:00:28,980
workshops and all things AI and machine

00:00:27,029 --> 00:00:32,669
learning so I'm mostly a front-end

00:00:28,980 --> 00:00:35,190
developer I use react day-to-day but

00:00:32,669 --> 00:00:39,000
recently I've been delving into the

00:00:35,190 --> 00:00:41,820
realms of data science AI and machine

00:00:39,000 --> 00:00:44,250
learning so why would a JavaScript

00:00:41,820 --> 00:00:46,860
developer care about machine learning

00:00:44,250 --> 00:00:49,890
and when I go to machine learning

00:00:46,860 --> 00:00:52,739
meetups and other events people say to

00:00:49,890 --> 00:00:56,850
me but machine learning is for Python

00:00:52,739 --> 00:00:58,859
developers or maybe Java or Scala they

00:00:56,850 --> 00:01:02,699
say javascript is for building web apps

00:00:58,859 --> 00:01:04,439
you build farms you make API requests

00:01:02,699 --> 00:01:07,020
you shouldn't be concerning yourselves

00:01:04,439 --> 00:01:12,810
with hard core algorithms and big data

00:01:07,020 --> 00:01:14,640
sets and okay perhaps JavaScript isn't

00:01:12,810 --> 00:01:16,790
the first language which springs to mind

00:01:14,640 --> 00:01:18,540
when you say machine learning for

00:01:16,790 --> 00:01:21,659
reasons such at which I'll go into

00:01:18,540 --> 00:01:24,000
shortly but that does seem to be

00:01:21,659 --> 00:01:25,860
changing especially with the

00:01:24,000 --> 00:01:28,140
announcement of tensorflow jeaious which

00:01:25,860 --> 00:01:29,909
was unveiled last year so the fit for

00:01:28,140 --> 00:01:32,250
the first time

00:01:29,909 --> 00:01:33,439
major machine learning library ie

00:01:32,250 --> 00:01:37,229
tensorflow

00:01:33,439 --> 00:01:40,710
has offered a fully supported entirely

00:01:37,229 --> 00:01:42,890
javascript based version so this is big

00:01:40,710 --> 00:01:45,869
news for JavaScript developers

00:01:42,890 --> 00:01:48,090
tensorflow j/s works in the browser and

00:01:45,869 --> 00:01:53,040
you know which language works in the

00:01:48,090 --> 00:01:55,200
browser you guessed it so before I talk

00:01:53,040 --> 00:01:56,549
more about JavaScript or machine

00:01:55,200 --> 00:01:59,549
learning I'm going to give a brief

00:01:56,549 --> 00:02:02,490
high-level overview into some areas of

00:01:59,549 --> 00:02:04,140
machine learning learning as somebody

00:02:02,490 --> 00:02:05,790
from our web development background who

00:02:04,140 --> 00:02:08,069
has been learning about data science AI

00:02:05,790 --> 00:02:10,289
machine learning I wanted to clarify

00:02:08,069 --> 00:02:12,840
some of the concepts and definitions

00:02:10,289 --> 00:02:13,140
which I personally founded they look a

00:02:12,840 --> 00:02:14,850
few

00:02:13,140 --> 00:02:17,220
along the way let's talk about the

00:02:14,850 --> 00:02:19,770
differences between data science machine

00:02:17,220 --> 00:02:22,440
learning and AI so datacite data science

00:02:19,770 --> 00:02:25,290
involves solving complex problems using

00:02:22,440 --> 00:02:27,540
data so this can cover things such as

00:02:25,290 --> 00:02:29,400
analytics mining visualization

00:02:27,540 --> 00:02:32,010
statistics and more artificial

00:02:29,400 --> 00:02:34,740
intelligence is the simulation of a

00:02:32,010 --> 00:02:37,650
human brain function by machines so this

00:02:34,740 --> 00:02:39,780
would cover perception so vision touch

00:02:37,650 --> 00:02:41,820
hearing actions and movements or

00:02:39,780 --> 00:02:45,000
robotics and the ability to move and

00:02:41,820 --> 00:02:49,320
manipulate objects natural language

00:02:45,000 --> 00:02:51,480
processing so speech and text planning

00:02:49,320 --> 00:02:54,420
for example playing chess and predicting

00:02:51,480 --> 00:02:57,000
moves and reasoning and knowledge so I'd

00:02:54,420 --> 00:02:59,150
be up for example IBM Watson playing the

00:02:57,000 --> 00:03:02,220
quiz show Jeopardy so machine learning

00:02:59,150 --> 00:03:05,040
involves looking at data and finding

00:03:02,220 --> 00:03:07,950
insights without specifically having to

00:03:05,040 --> 00:03:09,739
be told what to look for so this is

00:03:07,950 --> 00:03:12,239
different to traditional computer

00:03:09,739 --> 00:03:14,730
algorithms as it specifically involves

00:03:12,239 --> 00:03:16,850
learning so a subfield of both data

00:03:14,730 --> 00:03:19,500
science and AI is machine learning

00:03:16,850 --> 00:03:22,290
machine learning itself has three main

00:03:19,500 --> 00:03:24,060
subfields supervised unsupervised and

00:03:22,290 --> 00:03:25,739
reinforcement learning and whilst there

00:03:24,060 --> 00:03:29,310
are many machine learning algorithms

00:03:25,739 --> 00:03:30,870
which solve specific problems the main

00:03:29,310 --> 00:03:32,280
thing which you should focus on with

00:03:30,870 --> 00:03:35,250
starting your journey into machine

00:03:32,280 --> 00:03:37,140
learning is artificial neural networks

00:03:35,250 --> 00:03:38,610
now I've no doubt that many of you have

00:03:37,140 --> 00:03:40,230
already heard of artificial neural

00:03:38,610 --> 00:03:42,570
networks perhaps you think they sound

00:03:40,230 --> 00:03:44,549
interesting you'd like to know more or

00:03:42,570 --> 00:03:46,080
maybe you've even already been reading

00:03:44,549 --> 00:03:48,330
or learning about them already I can

00:03:46,080 --> 00:03:50,850
tell you that artificial neural networks

00:03:48,330 --> 00:03:52,980
are the most exciting thing happening

00:03:50,850 --> 00:03:54,360
within the field of machine learning

00:03:52,980 --> 00:03:57,900
right now they're incredibly powerful

00:03:54,360 --> 00:04:00,030
and they're even moving into other AI

00:03:57,900 --> 00:04:03,000
sub domains such as natural language

00:04:00,030 --> 00:04:04,739
processing and robotics where previously

00:04:03,000 --> 00:04:07,380
applied algorithms are being replaced

00:04:04,739 --> 00:04:08,730
with new neural networks to great

00:04:07,380 --> 00:04:11,940
success so they've actually been

00:04:08,730 --> 00:04:13,380
theorized since the 1950s but it's only

00:04:11,940 --> 00:04:15,390
recently that we've had enough

00:04:13,380 --> 00:04:17,519
processing power to actually get them to

00:04:15,390 --> 00:04:19,919
work so artificial neural networks are

00:04:17,519 --> 00:04:21,180
inspired by biological networks so

00:04:19,919 --> 00:04:24,150
they're based on the brain and how we

00:04:21,180 --> 00:04:25,889
think this is what a real biological

00:04:24,150 --> 00:04:28,110
neuron looks like the

00:04:25,889 --> 00:04:30,479
dendrites are the inputs the axons of

00:04:28,110 --> 00:04:33,749
the outputs would enough dendrites fire

00:04:30,479 --> 00:04:35,400
then the axons will fire outwards your

00:04:33,749 --> 00:04:37,830
brain has about 100 billion of these

00:04:35,400 --> 00:04:39,889
neurons or connected together in a

00:04:37,830 --> 00:04:43,499
neural network so an artificial neuron

00:04:39,889 --> 00:04:45,620
works much the same it's essentially an

00:04:43,499 --> 00:04:48,389
algorithm which receives a set of values

00:04:45,620 --> 00:04:51,029
and if these values are high enough it

00:04:48,389 --> 00:04:53,909
will activate so in this case we have

00:04:51,029 --> 00:04:56,520
two inputs point three point seven and

00:04:53,909 --> 00:04:58,529
then the next step is to assign weights

00:04:56,520 --> 00:05:00,569
to these inputs which represents how

00:04:58,529 --> 00:05:04,050
important they are usually especially

00:05:00,569 --> 00:05:05,789
using a library such as tensor flow I it

00:05:04,050 --> 00:05:07,830
would and it would initialize all of

00:05:05,789 --> 00:05:09,449
these weights randomly and then they

00:05:07,830 --> 00:05:11,550
would be automatically adjusted to

00:05:09,449 --> 00:05:13,680
reflect the importance of the inputs

00:05:11,550 --> 00:05:18,000
based on the findings of the neural

00:05:13,680 --> 00:05:19,529
network you'd also add a bias to inputs

00:05:18,000 --> 00:05:21,960
with zero and then we have something

00:05:19,529 --> 00:05:23,939
called an activation function which is

00:05:21,960 --> 00:05:26,999
what's gonna make the neuron fire or

00:05:23,939 --> 00:05:30,000
activate if the incoming inputs and

00:05:26,999 --> 00:05:31,469
weights reach a certain threshold so the

00:05:30,000 --> 00:05:33,779
inputs get multiplied by their weights

00:05:31,469 --> 00:05:36,150
and then added together and pass into

00:05:33,779 --> 00:05:37,770
the activation function for now our

00:05:36,150 --> 00:05:40,349
active activation function is going to

00:05:37,770 --> 00:05:43,020
be really simple if the sum of the

00:05:40,349 --> 00:05:45,149
inputs times weights is positive the

00:05:43,020 --> 00:05:47,039
neuron will activate if it's negative it

00:05:45,149 --> 00:05:49,469
or not so in this case the neuron

00:05:47,039 --> 00:05:51,659
receives a positive value of 2.7 and it

00:05:49,469 --> 00:05:54,810
will activate the activation function

00:05:51,659 --> 00:05:56,550
returns a one and not a zero so this

00:05:54,810 --> 00:05:59,250
activation function is good a threshold

00:05:56,550 --> 00:06:01,500
function and looks like this the problem

00:05:59,250 --> 00:06:03,469
with this activation function however is

00:06:01,500 --> 00:06:05,909
that you only have to be a tiny amount

00:06:03,469 --> 00:06:08,219
either side of the point of the middle

00:06:05,909 --> 00:06:10,589
to yield completely different results

00:06:08,219 --> 00:06:13,379
so there's many activation functions are

00:06:10,589 --> 00:06:16,680
the common ones are sigmoid and reloj

00:06:13,379 --> 00:06:18,930
both of which allow smaller T changes to

00:06:16,680 --> 00:06:21,870
be taken into account when deciding

00:06:18,930 --> 00:06:23,669
whether a neuron activates or not so if

00:06:21,870 --> 00:06:26,129
you connect many neurons together in

00:06:23,669 --> 00:06:29,490
rows and layers you make a neural

00:06:26,129 --> 00:06:32,159
network here we have four layers the

00:06:29,490 --> 00:06:36,060
purple layer is the input layer there's

00:06:32,159 --> 00:06:37,350
two hidden layers and the out of the

00:06:36,060 --> 00:06:39,570
blue and the green and the

00:06:37,350 --> 00:06:42,570
the output layer is the red one so your

00:06:39,570 --> 00:06:44,400
input layers represent your real data if

00:06:42,570 --> 00:06:45,660
you were predicting house prices for

00:06:44,400 --> 00:06:48,170
example then these might represent

00:06:45,660 --> 00:06:50,880
number of rooms the age of the house

00:06:48,170 --> 00:06:53,220
square footage and so on the red layer

00:06:50,880 --> 00:06:54,660
on the right is your output layer it's

00:06:53,220 --> 00:06:57,150
just the thing you're trying to predict

00:06:54,660 --> 00:07:00,570
so in this case we only have one output

00:06:57,150 --> 00:07:02,880
the price of the house we also have

00:07:00,570 --> 00:07:06,540
these two hidden layers which receive

00:07:02,880 --> 00:07:09,210
inputs and pass outputs these two hidden

00:07:06,540 --> 00:07:11,820
layers are called dense layers as

00:07:09,210 --> 00:07:15,180
they're fully connected so again we

00:07:11,820 --> 00:07:17,250
would assign weights to each input then

00:07:15,180 --> 00:07:19,890
a data flows from the input layer hits

00:07:17,250 --> 00:07:21,720
the Neuros in the next layer and then if

00:07:19,890 --> 00:07:23,220
it's activated based on the weights it

00:07:21,720 --> 00:07:25,620
will send the outputs forward to

00:07:23,220 --> 00:07:27,720
connected neurons in the next layer when

00:07:25,620 --> 00:07:29,490
training and your network and you need

00:07:27,720 --> 00:07:32,280
to train in your network to for it to be

00:07:29,490 --> 00:07:34,650
useful in most cases you would already

00:07:32,280 --> 00:07:36,810
know the expected result which is known

00:07:34,650 --> 00:07:38,430
as supervised learning you can then use

00:07:36,810 --> 00:07:40,650
something called a loss function to

00:07:38,430 --> 00:07:42,720
calculate how far off the models output

00:07:40,650 --> 00:07:44,820
is from the correct output so in this

00:07:42,720 --> 00:07:47,790
case for example the model predicted 3

00:07:44,820 --> 00:07:49,140
but we were expecting 8 and then we

00:07:47,790 --> 00:07:51,420
would use something called an optimizer

00:07:49,140 --> 00:07:54,750
algorithm which is the thing doing the

00:07:51,420 --> 00:07:56,940
actual training its job is to go back

00:07:54,750 --> 00:07:59,220
and update the weights and biases in

00:07:56,940 --> 00:08:01,500
order to get the outputs closer to the

00:07:59,220 --> 00:08:04,050
expected results so you would repeat

00:08:01,500 --> 00:08:06,110
this a bunch of times with many sets of

00:08:04,050 --> 00:08:08,730
inputs and expected outputs and

00:08:06,110 --> 00:08:11,280
eventually you get a trained model which

00:08:08,730 --> 00:08:14,310
can receive inputs and give accurate

00:08:11,280 --> 00:08:16,020
predictions so why are there hiked with

00:08:14,310 --> 00:08:18,510
neural networks well they're actually

00:08:16,020 --> 00:08:21,240
quite general you can learn the basis

00:08:18,510 --> 00:08:23,070
and then apply the same techniques and

00:08:21,240 --> 00:08:25,740
tools to a wide range of different

00:08:23,070 --> 00:08:27,780
problems with only a little tweaking and

00:08:25,740 --> 00:08:30,840
not much domain-specific knowledge

00:08:27,780 --> 00:08:32,849
required neural networks work best with

00:08:30,840 --> 00:08:35,849
labelled data and more and more

00:08:32,849 --> 00:08:36,479
available data is becoming available

00:08:35,849 --> 00:08:39,960
every day

00:08:36,479 --> 00:08:41,940
this means that developers researchers

00:08:39,960 --> 00:08:44,520
and businesses can make use of this

00:08:41,940 --> 00:08:47,220
ever-increasing computing power and

00:08:44,520 --> 00:08:49,680
available data and they can use neural

00:08:47,220 --> 00:08:51,120
networks to find patterns which they

00:08:49,680 --> 00:08:53,279
never would have been able to

00:08:51,120 --> 00:08:55,560
by just searching and analyzing the data

00:08:53,279 --> 00:08:57,900
themselves so how do we actually carry

00:08:55,560 --> 00:08:59,520
out machine learning I previously

00:08:57,900 --> 00:09:02,010
demonstrated how an artificial neural

00:08:59,520 --> 00:09:05,070
network works but what about the

00:09:02,010 --> 00:09:07,920
technology we as developers can use to

00:09:05,070 --> 00:09:09,870
implement this more specifically what

00:09:07,920 --> 00:09:12,779
can we as JavaScript developers use to

00:09:09,870 --> 00:09:16,350
implement this well until recently

00:09:12,779 --> 00:09:18,150
not so much compared to Python the go-to

00:09:16,350 --> 00:09:20,610
language for scientific computing a

00:09:18,150 --> 00:09:22,260
machine learning JavaScript is barely a

00:09:20,610 --> 00:09:26,700
fraction of the tools and resources

00:09:22,260 --> 00:09:28,950
available setting up models involves

00:09:26,700 --> 00:09:31,100
configuring how you pass a set of inputs

00:09:28,950 --> 00:09:34,770
through a set of functions and

00:09:31,100 --> 00:09:37,320
algorithms so if that's all there is to

00:09:34,770 --> 00:09:39,900
it then why has a java script been a

00:09:37,320 --> 00:09:42,540
contender with other languages when it

00:09:39,900 --> 00:09:43,980
comes to machine learning javascript has

00:09:42,540 --> 00:09:47,660
matured a lot over the last five years

00:09:43,980 --> 00:09:50,850
and it has a huge community around him

00:09:47,660 --> 00:09:53,250
it's also everywhere it's in the browser

00:09:50,850 --> 00:09:57,860
in the cloud it's building front ends

00:09:53,250 --> 00:10:01,350
backends games mobile apps desktop apps

00:09:57,860 --> 00:10:07,529
it was crowned the most popular language

00:10:01,350 --> 00:10:10,339
or of 2018 2017 2016 2015 and 2014 as

00:10:07,529 --> 00:10:13,860
per the stack overflow developer survey

00:10:10,339 --> 00:10:16,410
however despite JavaScript's popularity

00:10:13,860 --> 00:10:19,890
the widespread pop adoption and the

00:10:16,410 --> 00:10:21,990
continuous stream of new tools packages

00:10:19,890 --> 00:10:25,080
and frameworks there's been this great

00:10:21,990 --> 00:10:27,630
big closed-off domain to JavaScript

00:10:25,080 --> 00:10:31,709
developers that of AI and machine

00:10:27,630 --> 00:10:32,970
learning so why order why have all these

00:10:31,709 --> 00:10:35,339
things we saw out of reach for

00:10:32,970 --> 00:10:37,470
JavaScript developers javascript is a

00:10:35,339 --> 00:10:39,810
general-purpose language you can and

00:10:37,470 --> 00:10:40,700
people do build anything and everything

00:10:39,810 --> 00:10:42,930
out of him

00:10:40,700 --> 00:10:46,290
Python was also designed to be a

00:10:42,930 --> 00:10:48,150
high-level general-purpose language but

00:10:46,290 --> 00:10:50,070
the main reason that Python has been

00:10:48,150 --> 00:10:52,410
crowned king of machine learning is

00:10:50,070 --> 00:10:56,130
because of the awesome community around

00:10:52,410 --> 00:10:58,380
him Python was designed right from the

00:10:56,130 --> 00:10:59,250
start to be syntactically elegant and

00:10:58,380 --> 00:11:02,250
easy to learn

00:10:59,250 --> 00:11:02,880
so academics and researchers picked it

00:11:02,250 --> 00:11:04,730
up you

00:11:02,880 --> 00:11:08,190
was easy for them to test ideas and

00:11:04,730 --> 00:11:11,970
allowed them to focus on research and

00:11:08,190 --> 00:11:14,040
findings javascript has only recently

00:11:11,970 --> 00:11:16,650
become syntactically elegant with things

00:11:14,040 --> 00:11:21,420
such as generators and async/await being

00:11:16,650 --> 00:11:22,770
added I had of course been some

00:11:21,420 --> 00:11:24,750
interesting open source JavaScript

00:11:22,770 --> 00:11:26,970
projects for scientific computing and

00:11:24,750 --> 00:11:31,170
machine learning range is an app to

00:11:26,970 --> 00:11:34,020
jeaious propel ml GPU GS and so on but

00:11:31,170 --> 00:11:35,760
generally speaking javascript just

00:11:34,020 --> 00:11:38,070
doesn't compare when it comes to the

00:11:35,760 --> 00:11:40,770
number of tools and packages for machine

00:11:38,070 --> 00:11:44,370
learning but it doesn't have to be that

00:11:40,770 --> 00:11:48,240
way especially now we have tensorflow

00:11:44,370 --> 00:11:51,570
Jas so tensorflow Jas was announced the

00:11:48,240 --> 00:11:53,450
30th of March last year and version 1.0

00:11:51,570 --> 00:11:56,610
was unveiled at the tensorflow

00:11:53,450 --> 00:11:59,010
dev summit a couple few weeks ago I

00:11:56,610 --> 00:12:01,290
suppose it's still fairly new but it's

00:11:59,010 --> 00:12:03,420
arguably the most exciting thing to

00:12:01,290 --> 00:12:06,600
happen with javascript and machine

00:12:03,420 --> 00:12:08,610
learning so far so before I show you

00:12:06,600 --> 00:12:13,080
more about tensorflow Jas let's look at

00:12:08,610 --> 00:12:15,660
what tensorflow is tensorflow

00:12:13,080 --> 00:12:18,510
is an incredibly powerful machine

00:12:15,660 --> 00:12:21,240
learning and deep learning library it

00:12:18,510 --> 00:12:24,210
allows dataflow programming across a

00:12:21,240 --> 00:12:27,030
wide range of tasks and as well as being

00:12:24,210 --> 00:12:29,160
a symbolic maths library it's also used

00:12:27,030 --> 00:12:32,430
for machine learning applications such

00:12:29,160 --> 00:12:34,800
as neural networks tensorflow was

00:12:32,430 --> 00:12:37,430
developed by Google brain for internal

00:12:34,800 --> 00:12:42,090
use and it was open sourced in November

00:12:37,430 --> 00:12:43,920
2015 since then it has quickly become

00:12:42,090 --> 00:12:46,680
one of the most popular machine learning

00:12:43,920 --> 00:12:48,030
frameworks on the scene tensor flows

00:12:46,680 --> 00:12:50,160
github allow

00:12:48,030 --> 00:12:51,390
repo has nearly four times the number of

00:12:50,160 --> 00:12:53,220
stars of scikit-learn

00:12:51,390 --> 00:12:56,040
which is the next most stirred machine

00:12:53,220 --> 00:12:58,830
learning project tensorflow allows

00:12:56,040 --> 00:13:01,020
developers to create large-scale neural

00:12:58,830 --> 00:13:02,850
networks with many layers and it can

00:13:01,020 --> 00:13:04,400
process and create modules for things

00:13:02,850 --> 00:13:07,490
such as voice and sound recognition

00:13:04,400 --> 00:13:10,710
language detection and a whole lot more

00:13:07,490 --> 00:13:13,520
NASA I use tensorflow to find new

00:13:10,710 --> 00:13:15,990
planets orbiting stars and more recently

00:13:13,520 --> 00:13:18,209
students have been using tensorflow

00:13:15,990 --> 00:13:21,029
to map craters to try and figure out

00:13:18,209 --> 00:13:24,330
where matter has existed has existed in

00:13:21,029 --> 00:13:26,640
various places across different times to

00:13:24,330 --> 00:13:29,730
try and understand the very origins of

00:13:26,640 --> 00:13:32,010
our solar system it's even being used to

00:13:29,730 --> 00:13:34,950
prevent illegal deforestation in the

00:13:32,010 --> 00:13:36,930
Amazon so solar powered up cycled phones

00:13:34,950 --> 00:13:38,550
are hidden high up in trees and they're

00:13:36,930 --> 00:13:41,130
trained using tenths of load to

00:13:38,550 --> 00:13:44,580
recognize the sound of chainsaws or

00:13:41,130 --> 00:13:48,450
logging trucks and alert the Rangers who

00:13:44,580 --> 00:13:51,089
police the forest so now we have

00:13:48,450 --> 00:13:53,970
tensorflow j/s so this is tensorflow

00:13:51,089 --> 00:13:56,430
completely rewritten in JavaScript and

00:13:53,970 --> 00:13:57,510
despite the shared methods and almost

00:13:56,430 --> 00:13:59,820
identical api

00:13:57,510 --> 00:14:03,470
you don't need tensorflow or anything

00:13:59,820 --> 00:14:05,880
else is stored to use tensorflow js

00:14:03,470 --> 00:14:09,420
so let's look at how tensorflow GS came

00:14:05,880 --> 00:14:11,870
about well firstly because of the

00:14:09,420 --> 00:14:14,610
popularity of the tensorflow playground

00:14:11,870 --> 00:14:18,089
this is an in-browser interactive

00:14:14,610 --> 00:14:19,709
visualization of a neural network so you

00:14:18,089 --> 00:14:22,350
can see how adding layers and neurons

00:14:19,709 --> 00:14:26,459
work and how changing these yield

00:14:22,350 --> 00:14:28,649
different results so for example here

00:14:26,459 --> 00:14:30,990
we're trying to find the difference

00:14:28,649 --> 00:14:32,339
between the yellow and blue dots and at

00:14:30,990 --> 00:14:34,500
first it doesn't do particularly well

00:14:32,339 --> 00:14:37,860
and then it adds more neurons another

00:14:34,500 --> 00:14:43,290
layer and then it has no problem finally

00:14:37,860 --> 00:14:44,880
the line of separation so the code used

00:14:43,290 --> 00:14:46,980
to make this website is all open source

00:14:44,880 --> 00:14:49,260
there's a repo on the tensorflow github

00:14:46,980 --> 00:14:50,190
page and this was turned into a library

00:14:49,260 --> 00:14:53,279
called deep land

00:14:50,190 --> 00:14:55,829
jeaious which has now become tensorflow

00:14:53,279 --> 00:15:00,029
jess so joseph no jess is just the next

00:14:55,829 --> 00:15:02,970
iteration of that work d-plan was only

00:15:00,029 --> 00:15:04,290
released in August 2017 and in the short

00:15:02,970 --> 00:15:06,899
time before joining the tensorflow

00:15:04,290 --> 00:15:10,279
family they released about released a

00:15:06,899 --> 00:15:12,810
bunch of demos such as style transfer

00:15:10,279 --> 00:15:16,470
where you can apply the style of famous

00:15:12,810 --> 00:15:18,930
artists work to any photo this here is

00:15:16,470 --> 00:15:23,610
applying the style of Francis Picabia to

00:15:18,930 --> 00:15:26,279
a photo of Scarlett Johansson d-plan

00:15:23,610 --> 00:15:28,920
jeaious also created teachable machine

00:15:26,279 --> 00:15:29,630
where you can train a neural network to

00:15:28,920 --> 00:15:31,760
use your compete

00:15:29,630 --> 00:15:34,220
his camera with things such as like a

00:15:31,760 --> 00:15:37,340
hand or head movement to trigger the

00:15:34,220 --> 00:15:39,260
loading of set sounds or images so you

00:15:37,340 --> 00:15:41,840
capture frames for holding down one of

00:15:39,260 --> 00:15:43,760
the colored train buttons whilst doing

00:15:41,840 --> 00:15:45,740
the action and then after the training

00:15:43,760 --> 00:15:47,690
the neural network will trigger one of

00:15:45,740 --> 00:15:49,700
the outputs on the right when you repeat

00:15:47,690 --> 00:15:50,840
to this action so this day I meant

00:15:49,700 --> 00:15:53,090
really cool actually and it's on the

00:15:50,840 --> 00:15:55,430
tensorflow jeaious website if you wanna

00:15:53,090 --> 00:15:57,350
have a play so tensorflow jess is able

00:15:55,430 --> 00:16:01,460
to train models in the browser and it

00:15:57,350 --> 00:16:03,410
uses WebGL to do that this is an API

00:16:01,460 --> 00:16:06,830
used by browsers to access your graphics

00:16:03,410 --> 00:16:09,440
card or your GPU so running in the

00:16:06,830 --> 00:16:13,190
browser means no setting up drivers or

00:16:09,440 --> 00:16:16,190
in stores and it works across all

00:16:13,190 --> 00:16:20,390
devices tensorflow

00:16:16,190 --> 00:16:22,220
jst also works server-side they released

00:16:20,390 --> 00:16:24,320
nodejs bindings for tensorflow

00:16:22,220 --> 00:16:28,270
which means the same JavaScript code can

00:16:24,320 --> 00:16:28,270
work on both the browser and the server

00:16:28,960 --> 00:16:35,540
so what can you actually do with

00:16:31,070 --> 00:16:38,210
tensorflow jeaious with three things you

00:16:35,540 --> 00:16:42,350
can create and train models in the

00:16:38,210 --> 00:16:44,390
browser or server-side you can run

00:16:42,350 --> 00:16:47,900
pre-trained models in the browser in

00:16:44,390 --> 00:16:50,330
what's called inference mode and you can

00:16:47,900 --> 00:16:52,520
retrain an existing model which is

00:16:50,330 --> 00:16:55,280
called transfer learning so there are

00:16:52,520 --> 00:16:58,430
many AI and machine learning JavaScript

00:16:55,280 --> 00:17:00,140
apps and demos popping up now many of

00:16:58,430 --> 00:17:04,100
which are featured on AI just a trip

00:17:00,140 --> 00:17:07,670
Docs so do check it out if you can and

00:17:04,100 --> 00:17:10,130
have a play around many of them let you

00:17:07,670 --> 00:17:13,520
interact by drawing uploading an image

00:17:10,130 --> 00:17:15,980
or by accessing your camera for image

00:17:13,520 --> 00:17:18,920
detection or even capturing your

00:17:15,980 --> 00:17:23,810
movements as a controller and there are

00:17:18,920 --> 00:17:25,850
also games such as this one so this is a

00:17:23,810 --> 00:17:28,250
pacman game where you can use your head

00:17:25,850 --> 00:17:30,440
movements as the game controller so this

00:17:28,250 --> 00:17:31,970
is my friend a sim who was very keen to

00:17:30,440 --> 00:17:34,010
be in this in this demo because he had

00:17:31,970 --> 00:17:36,560
just bought this new hat and he was very

00:17:34,010 --> 00:17:38,930
proud of this new hat so azzam is the

00:17:36,560 --> 00:17:41,720
other creator of AI gesture rocks and

00:17:38,930 --> 00:17:42,680
here he is capturing images using his

00:17:41,720 --> 00:17:46,730
webcam

00:17:42,680 --> 00:17:48,710
ready to play the pac-man game so the

00:17:46,730 --> 00:17:51,650
app loads and Mart model which has

00:17:48,710 --> 00:17:53,660
already been trained but it requires the

00:17:51,650 --> 00:17:56,030
user to do this additional training

00:17:53,660 --> 00:17:59,140
session so the model can update itself

00:17:56,030 --> 00:18:01,310
based on the user's specific movements

00:17:59,140 --> 00:18:04,670
so here you can see the model captures

00:18:01,310 --> 00:18:07,490
extra frames for up down left and right

00:18:04,670 --> 00:18:11,330
and then it gets trained with these

00:18:07,490 --> 00:18:14,900
right inside the browser and then you

00:18:11,330 --> 00:18:17,800
can play the game using movements as

00:18:14,900 --> 00:18:17,800
controllers like this

00:18:36,539 --> 00:18:42,220
okay so another fun demo I want to show

00:18:38,980 --> 00:18:43,870
you is move mirror which is made by the

00:18:42,220 --> 00:18:46,539
smart people at the Google Creative Lab

00:18:43,870 --> 00:18:49,240
which lets you explore pictures in a fun

00:18:46,539 --> 00:18:52,210
way so you turn on your webcam and move

00:18:49,240 --> 00:18:54,640
around and the computer will pull up

00:18:52,210 --> 00:18:57,490
pictures of poses which match yours in

00:18:54,640 --> 00:19:02,919
real time from a database of more than

00:18:57,490 --> 00:19:07,929
80,000 images and of course his awesome

00:19:02,919 --> 00:19:10,179
again trying it out so this game uses a

00:19:07,929 --> 00:19:12,370
pre trained model called pose nap which

00:19:10,179 --> 00:19:15,940
detects positions of 17 points on the

00:19:12,370 --> 00:19:18,250
body such as eyes ears wrists and knees

00:19:15,940 --> 00:19:22,870
it runs entirely on the browser and it

00:19:18,250 --> 00:19:25,510
can be used with any webcam so I also

00:19:22,870 --> 00:19:27,510
want to show you sketch RNN which is a

00:19:25,510 --> 00:19:30,669
generative recurrent neural network

00:19:27,510 --> 00:19:33,909
capable of producing sketches of common

00:19:30,669 --> 00:19:36,909
objects as you can see here I started

00:19:33,909 --> 00:19:40,539
destroying by adding one line and the

00:19:36,909 --> 00:19:41,799
model will then continue my drawing so

00:19:40,539 --> 00:19:43,289
I've selected a bird here but there's

00:19:41,799 --> 00:19:46,539
many different subjects to choose from

00:19:43,289 --> 00:19:50,409
so this was made by david ha with the

00:19:46,539 --> 00:19:53,320
goal of training models to draw and

00:19:50,409 --> 00:19:55,140
generalize abstract concept concepts in

00:19:53,320 --> 00:19:57,549
a similar way to humans

00:19:55,140 --> 00:20:01,870
it was trained on a data set of

00:19:57,549 --> 00:20:03,880
handwritten hand-drawn sketches and it

00:20:01,870 --> 00:20:06,210
has the potential to help artists with

00:20:03,880 --> 00:20:08,289
their work or help people learn to draw

00:20:06,210 --> 00:20:14,380
so this is a JavaScript implementation

00:20:08,289 --> 00:20:17,110
of magentas sketch or Ihnen model so

00:20:14,380 --> 00:20:18,940
magenta is a research project exploring

00:20:17,110 --> 00:20:22,169
the role of machine learning in the

00:20:18,940 --> 00:20:24,520
process of creating art and music

00:20:22,169 --> 00:20:26,500
primarily this involves developing

00:20:24,520 --> 00:20:29,590
new deep learning and reinforcement

00:20:26,500 --> 00:20:33,149
learning algorithms for generating songs

00:20:29,590 --> 00:20:35,860
images drawings and other materials and

00:20:33,149 --> 00:20:38,550
there is magenta jeaious which uses

00:20:35,860 --> 00:20:40,950
tensorflow JS to run the magenta models

00:20:38,550 --> 00:20:43,200
so if you're interested in the creative

00:20:40,950 --> 00:20:46,280
side of machine learning your JavaScript

00:20:43,200 --> 00:20:46,280
then you should definitely check it out

00:20:47,300 --> 00:20:52,470
so it's super easy to add tensorflow J's

00:20:49,890 --> 00:20:55,170
to your project you can use NPM or yarn

00:20:52,470 --> 00:20:57,390
or you can link to the CDN and a script

00:20:55,170 --> 00:20:59,220
tag right inside your browser and your

00:20:57,390 --> 00:21:01,680
in your HTML file and then you have

00:20:59,220 --> 00:21:03,780
access to the global TF for object where

00:21:01,680 --> 00:21:07,380
you can carry out various methods and

00:21:03,780 --> 00:21:08,640
operations for training models the

00:21:07,380 --> 00:21:11,930
library consists of two different

00:21:08,640 --> 00:21:15,510
packages the core API and the layers API

00:21:11,930 --> 00:21:17,430
a tensor is the central use-it unit of

00:21:15,510 --> 00:21:20,130
data and tensorflow GS and it's

00:21:17,430 --> 00:21:23,490
basically a multi-dimensional array of

00:21:20,130 --> 00:21:25,410
numbers so you could create a tensor by

00:21:23,490 --> 00:21:28,650
doing tf2 tensor and passing in an array

00:21:25,410 --> 00:21:31,500
of values and a shape of rows and

00:21:28,650 --> 00:21:34,920
columns or you can infer the shape with

00:21:31,500 --> 00:21:37,380
nested arrays so then you can perform

00:21:34,920 --> 00:21:39,330
maths operations on these for example a

00:21:37,380 --> 00:21:41,330
dot square and of course you can use

00:21:39,330 --> 00:21:44,940
chaining as with any other JavaScript

00:21:41,330 --> 00:21:47,430
the flexible low-level core API is

00:21:44,940 --> 00:21:48,150
syntactically very similar to the

00:21:47,430 --> 00:21:50,790
tensorflow

00:21:48,150 --> 00:21:53,040
Python library and involves doing data

00:21:50,790 --> 00:21:54,990
using tensors then applying maths

00:21:53,040 --> 00:21:57,450
operations on them to represent the work

00:21:54,990 --> 00:21:58,980
the model does is this an example of

00:21:57,450 --> 00:22:01,020
doing Pauline polynomial regression

00:21:58,980 --> 00:22:04,020
which is like linear regression but a

00:22:01,020 --> 00:22:06,150
curve and here we are predicting what

00:22:04,020 --> 00:22:08,910
the Y value will be knowing the value of

00:22:06,150 --> 00:22:10,830
X so we can plot it on a graph so this

00:22:08,910 --> 00:22:12,210
is not deep learning this is regression

00:22:10,830 --> 00:22:15,000
is generally referred to as shallow

00:22:12,210 --> 00:22:17,460
learning setting up layers for a neural

00:22:15,000 --> 00:22:19,200
neural network with these maths

00:22:17,460 --> 00:22:24,120
operations it's a lot more complicated

00:22:19,200 --> 00:22:26,910
so let's look at the layers API so this

00:22:24,120 --> 00:22:29,700
higher level caris inspired layers api

00:22:26,910 --> 00:22:32,760
makes it a whole lot easier to build and

00:22:29,700 --> 00:22:34,860
train models you can create a neural

00:22:32,760 --> 00:22:37,350
network with pre-constructed layers such

00:22:34,860 --> 00:22:38,790
as in this example here we create a

00:22:37,350 --> 00:22:42,170
dense layer which means it's fully

00:22:38,790 --> 00:22:44,400
connected and we pass it a config object

00:22:42,170 --> 00:22:46,080
with some data such as the number of

00:22:44,400 --> 00:22:48,770
units for the dimensionality of the

00:22:46,080 --> 00:22:51,390
output space and the activation function

00:22:48,770 --> 00:22:52,200
so in this case we're using reloj if you

00:22:51,390 --> 00:22:55,669
remember what that

00:22:52,200 --> 00:22:57,989
it's like from our neural network intro

00:22:55,669 --> 00:22:59,789
so the activation function is going to

00:22:57,989 --> 00:23:02,369
tell each neuron in this layer whether

00:22:59,789 --> 00:23:05,970
to activate or not based on the inputs

00:23:02,369 --> 00:23:09,059
and input weights we also need to record

00:23:05,970 --> 00:23:12,659
how far off the expected output we we

00:23:09,059 --> 00:23:14,190
are using a loss function mean squared

00:23:12,659 --> 00:23:17,129
error is a common and straight forward

00:23:14,190 --> 00:23:19,169
loss function so we'll use up we also

00:23:17,129 --> 00:23:20,700
need to use an optimizer and this is

00:23:19,169 --> 00:23:22,289
going to go back and adjust as the

00:23:20,700 --> 00:23:25,559
weights so hopefully when the model

00:23:22,289 --> 00:23:27,659
iterates over and over again it will get

00:23:25,559 --> 00:23:31,169
a lower loss rate and the outputs will

00:23:27,659 --> 00:23:33,059
be closer to the expected ones you would

00:23:31,169 --> 00:23:35,159
call model compile with your loss

00:23:33,059 --> 00:23:37,619
function your optimizer and model dot

00:23:35,159 --> 00:23:40,679
fit with your data X is your data Y is

00:23:37,619 --> 00:23:43,289
the corresponding labels because

00:23:40,679 --> 00:23:45,450
tensorflow J s uses the GPU to

00:23:43,289 --> 00:23:46,950
accelerate maths operations you do have

00:23:45,450 --> 00:23:49,980
to think a bit about memory management

00:23:46,950 --> 00:23:51,509
when creating loads of tensors you will

00:23:49,980 --> 00:23:54,419
need to clean them up after your

00:23:51,509 --> 00:23:56,249
function has run this sounds scary to

00:23:54,419 --> 00:23:58,440
JavaScript developers but don't worry

00:23:56,249 --> 00:24:00,149
it's nothing like the hard core memory

00:23:58,440 --> 00:24:03,029
management that C developers have to

00:24:00,149 --> 00:24:07,619
have to do you simply wrap your function

00:24:03,029 --> 00:24:09,869
with TF duck tidy or use dispose which

00:24:07,619 --> 00:24:11,909
is similar but call directly on the

00:24:09,869 --> 00:24:13,529
tenses or variables and then your

00:24:11,909 --> 00:24:18,929
browser won't crash because of a memory

00:24:13,529 --> 00:24:20,820
leak so I've talked about how to train a

00:24:18,929 --> 00:24:22,679
model with tensorflow J s let's see

00:24:20,820 --> 00:24:26,609
about how to load a pre trained model

00:24:22,679 --> 00:24:28,440
straight into your browser with the

00:24:26,609 --> 00:24:31,409
pre-trained module we simply called TF

00:24:28,440 --> 00:24:33,239
dot load model and parse the URL so this

00:24:31,409 --> 00:24:36,320
is a URL to a pre trained model called

00:24:33,239 --> 00:24:37,919
mobile nap which is an open source

00:24:36,320 --> 00:24:40,230
convolutional neural network

00:24:37,919 --> 00:24:42,989
architecture relating to image

00:24:40,230 --> 00:24:44,609
recognition there are many types of

00:24:42,989 --> 00:24:47,429
neural network but whenever you hear

00:24:44,609 --> 00:24:50,850
convolutional neural network most of the

00:24:47,429 --> 00:24:53,129
time think image detection mobile net

00:24:50,850 --> 00:24:56,009
was trained on a large subset of image

00:24:53,129 --> 00:24:58,919
database a huge huge image database

00:24:56,009 --> 00:25:00,929
called image net and it was optimized to

00:24:58,919 --> 00:25:04,110
perform image detection on mobile

00:25:00,929 --> 00:25:06,160
devices and embedded applications

00:25:04,110 --> 00:25:08,730
there's more information about mobile

00:25:06,160 --> 00:25:10,510
net on texif locate github under

00:25:08,730 --> 00:25:12,730
examples and it's a really interesting

00:25:10,510 --> 00:25:14,530
one to have a play around with them so

00:25:12,730 --> 00:25:16,810
you don't need any prior machine

00:25:14,530 --> 00:25:18,820
learning knowledge you can simply load

00:25:16,810 --> 00:25:22,270
this model then you were passed it in

00:25:18,820 --> 00:25:24,520
HTML image video or canvas tag and it

00:25:22,270 --> 00:25:26,560
will return an array of the most likely

00:25:24,520 --> 00:25:30,640
predictions and the percentage of

00:25:26,560 --> 00:25:32,320
confidence so as well as training a

00:25:30,640 --> 00:25:34,300
model and loading a pre trained model

00:25:32,320 --> 00:25:37,120
the third thing you can do with

00:25:34,300 --> 00:25:40,510
tensorflow GS is a chat is transfer

00:25:37,120 --> 00:25:44,140
learning this is where you train the end

00:25:40,510 --> 00:25:45,970
of tail end of an existing model so

00:25:44,140 --> 00:25:49,390
using the mobile net example once again

00:25:45,970 --> 00:25:51,730
you would load the model and set it to a

00:25:49,390 --> 00:25:53,200
variable and then you would call get

00:25:51,730 --> 00:25:55,540
layer on this and pass at the name of

00:25:53,200 --> 00:25:58,870
the layer once you have this last layer

00:25:55,540 --> 00:26:00,520
you can create your own model just like

00:25:58,870 --> 00:26:03,400
we did in the previous examples and

00:26:00,520 --> 00:26:05,470
train it with your own data so the

00:26:03,400 --> 00:26:07,510
pac-man demo was using mobile net and

00:26:05,470 --> 00:26:10,150
train retraining it with the user's

00:26:07,510 --> 00:26:14,740
extra images captured during the

00:26:10,150 --> 00:26:18,130
training so there you have it three ways

00:26:14,740 --> 00:26:20,650
you can use tensorflow j/s training and

00:26:18,130 --> 00:26:25,540
module loading a pre trained module and

00:26:20,650 --> 00:26:27,490
retraining an existing modem so if

00:26:25,540 --> 00:26:29,770
you're wondering about performance this

00:26:27,490 --> 00:26:31,870
is a benchmark for running a mobile net

00:26:29,770 --> 00:26:33,580
new model in inference mode comparing

00:26:31,870 --> 00:26:36,520
the JavaScript and Python versions of

00:26:33,580 --> 00:26:37,780
tensorflow so if you remember inference

00:26:36,520 --> 00:26:39,210
mode means running a model which is

00:26:37,780 --> 00:26:42,100
already being trained

00:26:39,210 --> 00:26:44,590
so the standard Python tensorflow when

00:26:42,100 --> 00:26:47,590
used with a powerful CUDA graphics card

00:26:44,590 --> 00:26:48,880
gets less than 3 milliseconds and just

00:26:47,590 --> 00:26:51,340
using the CPU or something like a

00:26:48,880 --> 00:26:55,030
MacBook Pro would take about 60

00:26:51,340 --> 00:26:57,940
milliseconds compared to JavaScript to

00:26:55,030 --> 00:27:00,250
compare to tensorflow j/s running with

00:26:57,940 --> 00:27:02,860
the fancy graphics card in it results in

00:27:00,250 --> 00:27:05,350
just under 11 minutes seconds and for

00:27:02,860 --> 00:27:08,890
running on the integrated graphics card

00:27:05,350 --> 00:27:12,100
of a laptop it's around 100 milliseconds

00:27:08,890 --> 00:27:14,200
but these are milliseconds so it's

00:27:12,100 --> 00:27:15,820
important to realize that even 100

00:27:14,200 --> 00:27:18,700
milliseconds is not that bad and it's

00:27:15,820 --> 00:27:21,910
only going to improve as both the

00:27:18,700 --> 00:27:23,290
technology and the web improve and you

00:27:21,910 --> 00:27:26,020
can already build some amazing

00:27:23,290 --> 00:27:27,850
applications with this already so how a

00:27:26,020 --> 00:27:29,440
world of potential applications of

00:27:27,850 --> 00:27:31,450
javascript and machine learning has

00:27:29,440 --> 00:27:34,210
opened up with the announcement of

00:27:31,450 --> 00:27:35,890
tensorflow Jas and I also want to

00:27:34,210 --> 00:27:40,390
mention the availability of machine

00:27:35,890 --> 00:27:43,179
learning api's Google Microsoft AWS IBM

00:27:40,390 --> 00:27:45,640
and many other companies now provide a

00:27:43,179 --> 00:27:47,400
range of api's for things such as

00:27:45,640 --> 00:27:50,530
natural language processing

00:27:47,400 --> 00:27:52,630
recommendations forecasting image image

00:27:50,530 --> 00:27:54,460
and video analysis and they're all

00:27:52,630 --> 00:27:56,799
pretty easy to use when it comes to

00:27:54,460 --> 00:27:58,929
building machine learning apps whilst

00:27:56,799 --> 00:28:01,330
matter of machine learning is maths

00:27:58,929 --> 00:28:03,820
that's probability functions and

00:28:01,330 --> 00:28:07,000
algorithms machine learning is not done

00:28:03,820 --> 00:28:08,940
in isolation it is often applied in some

00:28:07,000 --> 00:28:12,280
kind of application context such as

00:28:08,940 --> 00:28:14,919
speech or text processing or computer

00:28:12,280 --> 00:28:18,210
vision with this data being used as the

00:28:14,919 --> 00:28:21,040
input to the machine learning models

00:28:18,210 --> 00:28:23,290
javascript is the de facto language to

00:28:21,040 --> 00:28:27,429
build apps and interfaces which can

00:28:23,290 --> 00:28:31,000
consume user interaction data so Mouse

00:28:27,429 --> 00:28:33,130
events text or voice inputs video edit

00:28:31,000 --> 00:28:35,830
video inputs by the camera and then

00:28:33,130 --> 00:28:38,590
potentially GPS gyroscopes and more

00:28:35,830 --> 00:28:42,940
depending on your device with tensorflow

00:28:38,590 --> 00:28:45,790
j/s or other api's data generated from

00:28:42,940 --> 00:28:48,490
these user events can be processed and

00:28:45,790 --> 00:28:50,350
sent to a model server side or it can

00:28:48,490 --> 00:28:52,330
just stay in the browser so as

00:28:50,350 --> 00:28:54,070
JavaScript developers we are not

00:28:52,330 --> 00:28:56,080
necessarily trying to play catch-up with

00:28:54,070 --> 00:28:58,390
Python when it comes to AI and machine

00:28:56,080 --> 00:28:59,950
learning at least when it comes to

00:28:58,390 --> 00:29:03,220
building machine learning applications

00:28:59,950 --> 00:29:06,040
we have our own place as masters of the

00:29:03,220 --> 00:29:07,780
browser and user interaction data if you

00:29:06,040 --> 00:29:09,520
want to become really good at machine

00:29:07,780 --> 00:29:11,500
learning then yes you still have to

00:29:09,520 --> 00:29:14,710
learn the map maths linear algebra

00:29:11,500 --> 00:29:16,320
formula and so on but if you want to see

00:29:14,710 --> 00:29:18,760
what all the fuss is about have a play

00:29:16,320 --> 00:29:20,680
maybe build something really cool really

00:29:18,760 --> 00:29:22,030
quickly you can just

00:29:20,680 --> 00:29:23,950
you you could just load an existing

00:29:22,030 --> 00:29:26,590
model or use a machine-learning

00:29:23,950 --> 00:29:28,900
API and come up with new and creative

00:29:26,590 --> 00:29:31,120
ways to send data to it via an interface

00:29:28,900 --> 00:29:33,160
or app and then if you want to go a bit

00:29:31,120 --> 00:29:36,790
deeper you could try retraining an

00:29:33,160 --> 00:29:38,500
existing model with new data as it's so

00:29:36,790 --> 00:29:41,620
easy to use in the browser you can just

00:29:38,500 --> 00:29:44,080
open a new cut code pen or and and I'd

00:29:41,620 --> 00:29:47,710
the CDN or import package and start

00:29:44,080 --> 00:29:49,900
playing with it right away so I hope

00:29:47,710 --> 00:29:52,270
that this has encouraged you to pursue

00:29:49,900 --> 00:29:53,980
you trying to build or at least try out

00:29:52,270 --> 00:29:56,740
a couple of projects using machine

00:29:53,980 --> 00:29:58,920
learning and JavaScript I highly

00:29:56,740 --> 00:30:01,090
recommend going to the official website

00:29:58,920 --> 00:30:02,710
jstellar org and working your way

00:30:01,090 --> 00:30:05,530
through some of the tutorials on there

00:30:02,710 --> 00:30:09,160
as well as going through the tensorflow

00:30:05,530 --> 00:30:12,100
github especially on the TF GS examples

00:30:09,160 --> 00:30:14,350
repo as there are many examples for you

00:30:12,100 --> 00:30:16,330
to try out including mobile nap there of

00:30:14,350 --> 00:30:18,340
course a bunch of interactive demos you

00:30:16,330 --> 00:30:21,400
can have a play with on AI guest at

00:30:18,340 --> 00:30:24,220
rocks and I also want to mention ml 5gs

00:30:21,400 --> 00:30:27,190
which is a wrapper around tensorflow JS

00:30:24,220 --> 00:30:28,600
which makes it even easier to load and

00:30:27,190 --> 00:30:30,940
access pre-trained models in your

00:30:28,600 --> 00:30:32,860
browser and if you're looking for video

00:30:30,940 --> 00:30:34,690
tutorials do you check out Daniel

00:30:32,860 --> 00:30:36,640
Schiffman from the coding train he's got

00:30:34,690 --> 00:30:39,100
some excellent series on YouTube where

00:30:36,640 --> 00:30:41,050
he trains the module using tableau j/s

00:30:39,100 --> 00:30:44,890
as well as some one training a neural

00:30:41,050 --> 00:30:46,650
network using vanilla JavaScript and as

00:30:44,890 --> 00:30:50,380
I mentioned before there's several api's

00:30:46,650 --> 00:30:52,120
such as Google's Auto ml and Microsoft's

00:30:50,380 --> 00:30:55,270
cognitive services which you can find

00:30:52,120 --> 00:30:58,480
tutorials for online so the future of

00:30:55,270 --> 00:31:01,270
machine learning and JavaScript well I

00:30:58,480 --> 00:31:04,330
certainly bribe the examples are shown

00:31:01,270 --> 00:31:06,520
are mostly fan or creative rather than

00:31:04,330 --> 00:31:09,430
solving real world or enterprise

00:31:06,520 --> 00:31:11,440
problems but I think that we'll see more

00:31:09,430 --> 00:31:13,750
examples of solving these problems over

00:31:11,440 --> 00:31:16,330
the coming months there's a lot of

00:31:13,750 --> 00:31:19,210
potential to build accessible apps using

00:31:16,330 --> 00:31:22,060
head movements or voice as controllers

00:31:19,210 --> 00:31:24,820
for example and also potential to build

00:31:22,060 --> 00:31:26,920
privacy friendly apps so running models

00:31:24,820 --> 00:31:29,110
in the browser means that the user data

00:31:26,920 --> 00:31:31,510
doesn't have to be sent server-side

00:31:29,110 --> 00:31:33,610
anywhere this is great for things such

00:31:31,510 --> 00:31:33,970
as health apps where data privacy is

00:31:33,610 --> 00:31:36,490
really in

00:31:33,970 --> 00:31:38,890
Orton yes no doubt that as more and more

00:31:36,490 --> 00:31:41,230
JavaScript developers get involved in

00:31:38,890 --> 00:31:43,630
machine learning further tools will

00:31:41,230 --> 00:31:45,880
evolve as the community grows and I'm

00:31:43,630 --> 00:31:49,320
hoping that this community will include

00:31:45,880 --> 00:31:49,320

YouTube URL: https://www.youtube.com/watch?v=Ii0o633QQIo


