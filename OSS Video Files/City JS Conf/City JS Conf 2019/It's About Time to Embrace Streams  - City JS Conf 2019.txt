Title: It's About Time to Embrace Streams  - City JS Conf 2019
Publication date: 2019-05-14
Playlist: City JS Conf 2019
Description: 
	Luciano Mammino

In my experience with JavaScript/Node.js devs I find out that Streams is a topic still mostly unexplored and sometimes even feared. This is probably one of the most beautiful features of Node.js and it can be adopted also in the browser.
 
Once you grasp the fundamentals, you'll be able to solve some ordinary programming challenges in a much more elegant and efficient way, so this is a very interesting topic. In this talk, I will cover the following topics: Streams: when and how;
 
Different types of streams; Built-in and custom streams; Composability; Stream utils; Streams in the browser.

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,060 --> 00:00:04,380
so this is that JavaScript conference

00:00:02,040 --> 00:00:06,629
but I'm quite sure that many of you here

00:00:04,380 --> 00:00:08,519
have experience also with not you yes or

00:00:06,629 --> 00:00:10,860
that at some point you had to do some

00:00:08,519 --> 00:00:13,080
not GS in your life so I want to start

00:00:10,860 --> 00:00:15,150
with a node.js example here and it's

00:00:13,080 --> 00:00:17,100
about copying files or copying data from

00:00:15,150 --> 00:00:18,840
one place to another and when you do

00:00:17,100 --> 00:00:20,520
that you know J yes I've seen many many

00:00:18,840 --> 00:00:23,279
times people doing something like this

00:00:20,520 --> 00:00:25,769
you just include these libraries these

00:00:23,279 --> 00:00:27,840
functions from the FS module read file

00:00:25,769 --> 00:00:30,090
sync right file sync then you have a

00:00:27,840 --> 00:00:31,920
source and a destination you read the

00:00:30,090 --> 00:00:33,690
data from this source using read file

00:00:31,920 --> 00:00:37,920
sync and then you write it to the

00:00:33,690 --> 00:00:41,100
destination using rightful sync so this

00:00:37,920 --> 00:00:44,370
is good we do this all the time and it's

00:00:41,100 --> 00:00:46,020
okay but sometimes and that sometime is

00:00:44,370 --> 00:00:47,010
probably the worst moment in your life

00:00:46,020 --> 00:00:48,899
when you're doing something very

00:00:47,010 --> 00:00:51,059
important or you are sleeping or you I

00:00:48,899 --> 00:00:54,210
don't know Li day this will happen in

00:00:51,059 --> 00:00:56,280
our production application so the file

00:00:54,210 --> 00:00:59,520
size is greater than the possible buffer

00:00:56,280 --> 00:01:02,010
and in order to understand why allow me

00:00:59,520 --> 00:01:03,930
to give you an amazing analogy including

00:01:02,010 --> 00:01:07,530
Super Mario so please if you do what the

00:01:03,930 --> 00:01:09,479
work for Nintendo don't sue me okay so

00:01:07,530 --> 00:01:11,400
imagine that you are Mario and you have

00:01:09,479 --> 00:01:13,140
bytes and these bytes are are the blocks

00:01:11,400 --> 00:01:15,930
here and you have to move these blocks

00:01:13,140 --> 00:01:18,630
from one place to another so what

00:01:15,930 --> 00:01:20,790
happens is that if you give Mario only

00:01:18,630 --> 00:01:23,070
four blocks it's probably not you has

00:01:20,790 --> 00:01:25,170
enough strength to move them only one go

00:01:23,070 --> 00:01:27,960
from one place to another but if you

00:01:25,170 --> 00:01:29,670
give Mario too many blocks of course it

00:01:27,960 --> 00:01:32,909
doesn't know how to do them in what only

00:01:29,670 --> 00:01:35,250
one go so what can we do to help Mario

00:01:32,909 --> 00:01:37,890
to move all the blocks to one place to

00:01:35,250 --> 00:01:40,110
another and essentially we can move them

00:01:37,890 --> 00:01:42,180
one by one which means we are streaming

00:01:40,110 --> 00:01:44,369
these blocks from one place to another

00:01:42,180 --> 00:01:46,890
and today we're gonna talk about

00:01:44,369 --> 00:01:51,840
streaming in no js' and then how to use

00:01:46,890 --> 00:01:54,479
the same ideas in the browser I am

00:01:51,840 --> 00:01:57,119
Luciano I guess at this point you know

00:01:54,479 --> 00:01:59,310
or you kind of guessed already that I am

00:01:57,119 --> 00:02:02,369
Italian my accent Super Mario and

00:01:59,310 --> 00:02:04,890
everything else my name as well but I

00:02:02,369 --> 00:02:06,719
live in Ireland in Dublin where I work

00:02:04,890 --> 00:02:09,899
for an American company which is called

00:02:06,719 --> 00:02:11,510
Vectra and what we do what Vectra is

00:02:09,899 --> 00:02:13,849
essentially

00:02:11,510 --> 00:02:15,620
security we do automated a threat

00:02:13,849 --> 00:02:17,930
detection using AI and machine learning

00:02:15,620 --> 00:02:19,549
we also build a number of solutions in

00:02:17,930 --> 00:02:21,530
the cloud to help security team to

00:02:19,549 --> 00:02:23,450
troubleshoot security issues and figure

00:02:21,530 --> 00:02:25,760
out why specifically attacks were

00:02:23,450 --> 00:02:27,110
possible so if you're curious about this

00:02:25,760 --> 00:02:29,900
stuff please come to talk to me

00:02:27,110 --> 00:02:31,549
afterwards or you can connect with me in

00:02:29,900 --> 00:02:34,430
my social channel and ask me any

00:02:31,549 --> 00:02:36,560
question about that also a Simona

00:02:34,430 --> 00:02:38,329
mentioned I am happy to say that I've

00:02:36,560 --> 00:02:40,250
been the co-author of this book there is

00:02:38,329 --> 00:02:42,470
also in that section dedicated to stream

00:02:40,250 --> 00:02:46,040
so if you happen to read it please give

00:02:42,470 --> 00:02:47,690
me some feedback all the slides are

00:02:46,040 --> 00:02:50,000
already online I'm gonna show you a lot

00:02:47,690 --> 00:02:51,590
of code examples so if you want to get

00:02:50,000 --> 00:02:53,870
the slides already so that you can look

00:02:51,590 --> 00:02:56,269
them in your mobile phone and maybe

00:02:53,870 --> 00:02:58,489
spend some time in more slides to ask me

00:02:56,269 --> 00:03:00,290
questions later you can do it also there

00:02:58,489 --> 00:03:02,810
are all the examples are on github so

00:03:00,290 --> 00:03:04,159
please get all these links I will tweet

00:03:02,810 --> 00:03:08,000
them later if you don't have time to

00:03:04,159 --> 00:03:09,739
scan the picture now so let's go back

00:03:08,000 --> 00:03:11,359
again and talk about the difference

00:03:09,739 --> 00:03:13,459
between a buffered approach and a

00:03:11,359 --> 00:03:15,500
streaming approach we used before this

00:03:13,459 --> 00:03:17,120
functionality called read file sync and

00:03:15,500 --> 00:03:19,849
to really understand what it does

00:03:17,120 --> 00:03:21,769
essentially you can see here that every

00:03:19,849 --> 00:03:23,629
time used that that functionality all

00:03:21,769 --> 00:03:25,879
the content of the given file will be

00:03:23,629 --> 00:03:28,129
loaded in memory in this data structure

00:03:25,879 --> 00:03:32,419
that is called buffer which represents

00:03:28,129 --> 00:03:34,790
all the data in a binary format when you

00:03:32,419 --> 00:03:36,680
study you streams and you can do that in

00:03:34,790 --> 00:03:39,349
this case with the create read stream

00:03:36,680 --> 00:03:41,419
function what happens is that you get

00:03:39,349 --> 00:03:42,949
another abstraction that is actually

00:03:41,419 --> 00:03:44,930
called the stream instance and this

00:03:42,949 --> 00:03:46,819
instance is lazy it's not going to load

00:03:44,930 --> 00:03:48,949
all the data in one go it's just gonna

00:03:46,819 --> 00:03:51,230
load a little bit of data wait for you

00:03:48,949 --> 00:03:53,269
to consume it or help you to consume it

00:03:51,230 --> 00:03:55,459
in different ways and then move the data

00:03:53,269 --> 00:03:59,060
on so you only load fractions of the

00:03:55,459 --> 00:04:00,889
data every single time so if we want to

00:03:59,060 --> 00:04:02,959
reemployment this copy file

00:04:00,889 --> 00:04:04,730
functionality from the buffered approach

00:04:02,959 --> 00:04:07,250
to the streaming approach it's very

00:04:04,730 --> 00:04:09,889
simple we just have to replace a few

00:04:07,250 --> 00:04:12,139
functions so we now import create read

00:04:09,889 --> 00:04:14,060
stream and create write stream then we

00:04:12,139 --> 00:04:15,949
use those two functionalities to create

00:04:14,060 --> 00:04:18,590
the stream instances so the source and

00:04:15,949 --> 00:04:21,409
the destination and then we can use this

00:04:18,590 --> 00:04:23,090
interface that says every time there is

00:04:21,409 --> 00:04:25,340
new data available in the source stream

00:04:23,090 --> 00:04:27,620
called this callback and inside this

00:04:25,340 --> 00:04:29,480
Colbeck I get the data and I write it to

00:04:27,620 --> 00:04:31,250
the destination stream this

00:04:29,480 --> 00:04:33,380
implementation is not perfect I will

00:04:31,250 --> 00:04:34,699
tell you exactly why later on but for

00:04:33,380 --> 00:04:37,340
now it's a good abstraction to

00:04:34,699 --> 00:04:39,650
understand the idea of streaming so that

00:04:37,340 --> 00:04:41,479
data there is never all the data in the

00:04:39,650 --> 00:04:44,630
file but it's just pieces of data that

00:04:41,479 --> 00:04:46,340
you can move over time so if we want to

00:04:44,630 --> 00:04:47,990
understand what is the memory

00:04:46,340 --> 00:04:50,510
implication with the two different

00:04:47,990 --> 00:04:53,870
implementations you can use for instance

00:04:50,510 --> 00:04:56,180
the chrome tools also to to see in the

00:04:53,870 --> 00:04:58,430
bug no J scripts in case you didn't know

00:04:56,180 --> 00:04:59,990
this is awesome so if you do that with

00:04:58,430 --> 00:05:02,300
the buffer ed approach and you try to

00:04:59,990 --> 00:05:04,729
copy a file that is about 600 megabytes

00:05:02,300 --> 00:05:07,430
you can see that the maximum amount of

00:05:04,729 --> 00:05:09,020
memory allocated is about 600 megabytes

00:05:07,430 --> 00:05:11,389
so at some point you have all the data

00:05:09,020 --> 00:05:13,880
memory is taking 600 megabytes of your

00:05:11,389 --> 00:05:16,100
RAM if you compare the same

00:05:13,880 --> 00:05:18,620
implementation using streams and you

00:05:16,100 --> 00:05:20,650
still call be 600 megabyte file you will

00:05:18,620 --> 00:05:22,970
consume only about 3 megabyte of memory

00:05:20,650 --> 00:05:24,590
maximum so you can see already that

00:05:22,970 --> 00:05:27,199
there is a huge advantage in terms of

00:05:24,590 --> 00:05:29,660
memory efficiency there we know already

00:05:27,199 --> 00:05:31,520
because we saw it before the to load if

00:05:29,660 --> 00:05:33,530
you try to copy a file that is even

00:05:31,520 --> 00:05:35,990
bigger than 600 megabytes let's say

00:05:33,530 --> 00:05:38,780
gigabytes it will explode with the

00:05:35,990 --> 00:05:40,849
buffered implementation but if we use

00:05:38,780 --> 00:05:42,410
the streaming implementation it still

00:05:40,849 --> 00:05:45,680
works and it's still gonna consume

00:05:42,410 --> 00:05:47,570
roughly 3 megabyte maximum so you can

00:05:45,680 --> 00:05:50,720
see now why streams are a much better

00:05:47,570 --> 00:05:53,090
idea than just buffering everything but

00:05:50,720 --> 00:05:55,580
just to summarize better this concept so

00:05:53,090 --> 00:05:58,039
with streams you get kind of a lower

00:05:55,580 --> 00:06:00,020
memory footprint so generally better but

00:05:58,039 --> 00:06:02,270
there is another very good advantage

00:06:00,020 --> 00:06:04,400
there that is with streams you can start

00:06:02,270 --> 00:06:06,740
to process data as soon as the data is

00:06:04,400 --> 00:06:08,570
available and just to go back to the

00:06:06,740 --> 00:06:10,700
super magic sample imagine that Mario

00:06:08,570 --> 00:06:13,070
also asked to paint every single block

00:06:10,700 --> 00:06:15,500
in blue what happens with the buffered

00:06:13,070 --> 00:06:17,000
approach moves all the blocks and only

00:06:15,500 --> 00:06:19,310
at the end you can start to paint all

00:06:17,000 --> 00:06:21,200
the blocks one by one instead if you are

00:06:19,310 --> 00:06:22,910
streaming you can take one block paint

00:06:21,200 --> 00:06:24,770
it in blue then take the other block

00:06:22,910 --> 00:06:27,050
pended in blue so you can start to see

00:06:24,770 --> 00:06:29,270
results hey Leon you don't have to wait

00:06:27,050 --> 00:06:31,800
for every single step to finish before

00:06:29,270 --> 00:06:34,710
you can start the other step

00:06:31,800 --> 00:06:36,960
so there are different types of streams

00:06:34,710 --> 00:06:39,090
in the GS so I want to show you what are

00:06:36,960 --> 00:06:42,990
the main ones and what are the api's you

00:06:39,090 --> 00:06:45,000
can use to deal with them but before

00:06:42,990 --> 00:06:46,919
doing that let me tell you that every

00:06:45,000 --> 00:06:49,159
stream instance is just an event

00:06:46,919 --> 00:06:51,870
emitters and if you have been using

00:06:49,159 --> 00:06:54,300
javascript and not GS you are definitely

00:06:51,870 --> 00:06:56,370
used to this concept where essentially

00:06:54,300 --> 00:06:58,500
you have objects these objects can just

00:06:56,370 --> 00:07:00,599
fire events and some other piece of code

00:06:58,500 --> 00:07:02,039
can listen to those events and do

00:07:00,599 --> 00:07:03,990
something with the data that is coming

00:07:02,039 --> 00:07:06,930
from the event so it's a very reactive

00:07:03,990 --> 00:07:10,979
way of thinking about how the programmer

00:07:06,930 --> 00:07:13,139
rolls so the first type of streams I

00:07:10,979 --> 00:07:15,210
want to describe to you is readable

00:07:13,139 --> 00:07:18,120
streams and you can see the double

00:07:15,210 --> 00:07:20,729
streams as input so an abstraction over

00:07:18,120 --> 00:07:22,229
some data that you want to consume and I

00:07:20,729 --> 00:07:23,659
already showed you the example of

00:07:22,229 --> 00:07:26,009
readable streams from the file system

00:07:23,659 --> 00:07:27,960
but if you're doing a console

00:07:26,009 --> 00:07:29,729
application even the standard in boot

00:07:27,960 --> 00:07:32,009
can be consumed as a stream if you're

00:07:29,729 --> 00:07:33,719
doing HTTP application you have some

00:07:32,009 --> 00:07:35,400
stream function analysis there you can

00:07:33,719 --> 00:07:37,560
read the data in a streaming way and

00:07:35,400 --> 00:07:41,159
also if you're using for instance the

00:07:37,560 --> 00:07:43,199
AWS SDK industry you can read files from

00:07:41,159 --> 00:07:45,210
history as a stream so you can read very

00:07:43,199 --> 00:07:48,270
big files without having too much memory

00:07:45,210 --> 00:07:50,490
issues there are two different ways to

00:07:48,270 --> 00:07:52,650
consume readable streams flowing and

00:07:50,490 --> 00:07:54,240
pause mode I'm just gonna describe today

00:07:52,650 --> 00:07:57,110
the flowing mode just because I think is

00:07:54,240 --> 00:08:00,120
the easiest and probably the most common

00:07:57,110 --> 00:08:02,039
so a readable stream flowing mode more

00:08:00,120 --> 00:08:04,409
or less looks like this you have these

00:08:02,039 --> 00:08:06,300
three elements the first one represents

00:08:04,409 --> 00:08:08,849
your actual source of data let's say

00:08:06,300 --> 00:08:10,560
sonnet drive with a file system then you

00:08:08,849 --> 00:08:12,509
have this instance in node.js that

00:08:10,560 --> 00:08:14,340
represents the stream helps you to load

00:08:12,509 --> 00:08:15,659
the data from this file and then

00:08:14,340 --> 00:08:17,669
eventually you have your business logic

00:08:15,659 --> 00:08:19,550
where you will have some data listener

00:08:17,669 --> 00:08:22,500
that is waiting for the data to arrive

00:08:19,550 --> 00:08:24,270
so when you create those when you have

00:08:22,500 --> 00:08:26,669
out those three entities the flow of

00:08:24,270 --> 00:08:28,469
data looks more or less like this so you

00:08:26,669 --> 00:08:30,270
create the stream as soon as you create

00:08:28,469 --> 00:08:31,979
the stream instance this stream is going

00:08:30,270 --> 00:08:34,229
to start to consume the data from the

00:08:31,979 --> 00:08:36,750
source and store it internally in its

00:08:34,229 --> 00:08:39,270
own memory and you have different chance

00:08:36,750 --> 00:08:41,399
and as soon as one chunk is loaded that

00:08:39,270 --> 00:08:43,770
will be sent to the data listener with a

00:08:41,399 --> 00:08:45,240
data event so inside the data event you

00:08:43,770 --> 00:08:47,459
will have all the the

00:08:45,240 --> 00:08:50,070
data for that specific chunk and this

00:08:47,459 --> 00:08:51,990
will go on and on and on until all the

00:08:50,070 --> 00:08:54,029
data is consumed and at that point there

00:08:51,990 --> 00:08:56,700
is another event that is called end that

00:08:54,029 --> 00:08:58,709
is sent to the data listener to indicate

00:08:56,700 --> 00:09:01,980
that all the data's being consumed from

00:08:58,709 --> 00:09:04,290
source so I want to give you a more

00:09:01,980 --> 00:09:06,540
practical example and a very funny one

00:09:04,290 --> 00:09:08,910
imagine you have text files and you want

00:09:06,540 --> 00:09:11,250
to know how many emojis there are in a

00:09:08,910 --> 00:09:13,860
specific text file I'm using here a

00:09:11,250 --> 00:09:15,959
library from NPM that is called emoji so

00:09:13,860 --> 00:09:18,570
you can use that it's amazing you can

00:09:15,959 --> 00:09:20,250
get all the emojis and then what I'm

00:09:18,570 --> 00:09:21,779
doing is essentially using this

00:09:20,250 --> 00:09:23,850
primitive that I just described before

00:09:21,779 --> 00:09:26,670
as soon as I have a readable stream from

00:09:23,850 --> 00:09:28,470
the file system I can say own data and I

00:09:26,670 --> 00:09:30,810
know that with that data comes the

00:09:28,470 --> 00:09:33,029
current chunk a chunk is nothing else

00:09:30,810 --> 00:09:35,010
than a buffer in ogs so it's some binary

00:09:33,029 --> 00:09:36,750
data and in this case I'm just saying

00:09:35,010 --> 00:09:39,240
since I know it's a text file so it's

00:09:36,750 --> 00:09:42,000
gonna be strings I can read every single

00:09:39,240 --> 00:09:44,160
character as you df8 and checking if

00:09:42,000 --> 00:09:46,709
that character is an emoji using the

00:09:44,160 --> 00:09:49,020
library then what I'm gonna do is just

00:09:46,709 --> 00:09:51,540
attach OOP sorry whew event listeners

00:09:49,020 --> 00:09:53,579
there at the end to check okay when it's

00:09:51,540 --> 00:09:56,760
finished actually tell me what is the

00:09:53,579 --> 00:09:59,310
current counter so if you run this code

00:09:56,760 --> 00:10:01,680
against this amazing emoji art that of

00:09:59,310 --> 00:10:03,329
course is a tribute to Ireland you will

00:10:01,680 --> 00:10:05,910
see that in this file there are 57

00:10:03,329 --> 00:10:07,860
emojis and of course you can use this

00:10:05,910 --> 00:10:09,240
even with gigabytes of data it's just

00:10:07,860 --> 00:10:12,529
gonna work and it's going to be very

00:10:09,240 --> 00:10:15,660
efficient because of the streaming model

00:10:12,529 --> 00:10:17,250
so another abstraction is rideable

00:10:15,660 --> 00:10:19,770
stream and it's kind of the opposite

00:10:17,250 --> 00:10:23,160
idea of readable stream so if readable

00:10:19,770 --> 00:10:25,470
streams represent input writable streams

00:10:23,160 --> 00:10:27,959
represent output so destinations where

00:10:25,470 --> 00:10:29,750
you want to write your data and again I

00:10:27,959 --> 00:10:32,250
already mentioned the file system but

00:10:29,750 --> 00:10:33,899
going back to the command line you can

00:10:32,250 --> 00:10:37,110
have standard out with a standard error

00:10:33,899 --> 00:10:40,230
as writable streams also HTTP again if

00:10:37,110 --> 00:10:42,300
you are writing requests to a server you

00:10:40,230 --> 00:10:44,010
can do that in a streaming way in s3 as

00:10:42,300 --> 00:10:48,329
well if you want to create a file

00:10:44,010 --> 00:10:51,690
industry you can do that as a stream so

00:10:48,329 --> 00:10:54,360
hi heavier an example that allows you to

00:10:51,690 --> 00:10:56,790
use the barebone HTTP module from logic

00:10:54,360 --> 00:10:58,120
so I'm not using any special like

00:10:56,790 --> 00:11:01,720
request or Axios

00:10:58,120 --> 00:11:03,550
that libraries to do HTTP and this will

00:11:01,720 --> 00:11:07,270
give you streaming primitives so when

00:11:03,550 --> 00:11:08,920
you do this HTTP request will return you

00:11:07,270 --> 00:11:11,290
an object which is essentially a

00:11:08,920 --> 00:11:12,850
writable stream and on that object you

00:11:11,290 --> 00:11:15,070
can attach a bunch of events to

00:11:12,850 --> 00:11:17,110
understand how the state of the stream

00:11:15,070 --> 00:11:18,730
evolves so when it's finished when the

00:11:17,110 --> 00:11:20,740
connection is closed if there was an

00:11:18,730 --> 00:11:22,450
error and then at that point you can

00:11:20,740 --> 00:11:24,850
start to write data on that stream which

00:11:22,450 --> 00:11:26,589
means you created the channel you are

00:11:24,850 --> 00:11:28,270
making a request to the server and you

00:11:26,589 --> 00:11:30,790
can start to write data on that channel

00:11:28,270 --> 00:11:32,920
and you have two methods to do that the

00:11:30,790 --> 00:11:34,570
first one is write that you can call as

00:11:32,920 --> 00:11:36,910
many times as you want to keep writing

00:11:34,570 --> 00:11:39,339
both data and the other one is dot end

00:11:36,910 --> 00:11:40,870
and this dot end you will call it to say

00:11:39,339 --> 00:11:43,150
this is the last time I'm going to write

00:11:40,870 --> 00:11:45,820
after that please cause close the stream

00:11:43,150 --> 00:11:47,529
and consider the request completed so

00:11:45,820 --> 00:11:50,350
I'm just saying writing some content and

00:11:47,529 --> 00:11:52,750
then last right and close the stream if

00:11:50,350 --> 00:11:54,700
you run this on the client side so if

00:11:52,750 --> 00:11:56,320
you just run the node.js script this is

00:11:54,700 --> 00:11:58,570
what you're gonna see requests and

00:11:56,320 --> 00:12:00,580
server responded with 200 and connection

00:11:58,570 --> 00:12:02,200
closed but if you want to observe it

00:12:00,580 --> 00:12:04,240
from the server side this is what the

00:12:02,200 --> 00:12:06,040
server is gonna received and you will

00:12:04,240 --> 00:12:08,350
see that the body of the request will be

00:12:06,040 --> 00:12:10,270
writing some content last right and

00:12:08,350 --> 00:12:12,160
close the stream now imagine that you

00:12:10,270 --> 00:12:14,470
can use the same idea to transfer big

00:12:12,160 --> 00:12:16,089
files you can keep appending more and

00:12:14,470 --> 00:12:18,100
more data without having to allocate

00:12:16,089 --> 00:12:19,660
that all in one go and the server will

00:12:18,100 --> 00:12:21,670
receive the first chance of the data

00:12:19,660 --> 00:12:23,350
immediately without having to wait for

00:12:21,670 --> 00:12:27,130
you to load all the data and then send

00:12:23,350 --> 00:12:28,779
it all in one go but every time we

00:12:27,130 --> 00:12:31,060
talked about streams there is this

00:12:28,779 --> 00:12:33,130
specific keyword that always comes up

00:12:31,060 --> 00:12:35,920
and is a bit scary that is called back

00:12:33,130 --> 00:12:40,420
pressure and I want to describe it with

00:12:35,920 --> 00:12:42,850
an analogy so imagine that going back to

00:12:40,420 --> 00:12:43,690
the example of copying one file from one

00:12:42,850 --> 00:12:44,800
place to another

00:12:43,690 --> 00:12:46,779
imagine that you have two different

00:12:44,800 --> 00:12:49,660
drugs the first one is very very fast

00:12:46,779 --> 00:12:51,790
like an SSD drive the other one is low

00:12:49,660 --> 00:12:54,250
so either a network drive or a magnetic

00:12:51,790 --> 00:12:55,870
disk and while you are copying the data

00:12:54,250 --> 00:12:58,029
of course you're going to be very faster

00:12:55,870 --> 00:13:00,880
reading but you're gonna be slower at

00:12:58,029 --> 00:13:03,550
writing so on the writing side you will

00:13:00,880 --> 00:13:05,320
keep accumulating data and which means

00:13:03,550 --> 00:13:07,209
that if it's too slow eventually you

00:13:05,320 --> 00:13:09,250
will accumulate ten gigabytes of data in

00:13:07,209 --> 00:13:11,560
the 10 gigabyte file example so it's not

00:13:09,250 --> 00:13:13,330
efficient anymore as we wanted

00:13:11,560 --> 00:13:16,000
so betfred should is Sunday that allows

00:13:13,330 --> 00:13:18,130
us to deal with that and in the node.js

00:13:16,000 --> 00:13:19,630
streams you have ways to figure out if

00:13:18,130 --> 00:13:22,450
you are essentially overloading the

00:13:19,630 --> 00:13:24,310
destination and you have to stop so when

00:13:22,450 --> 00:13:26,560
I told you that that first example

00:13:24,310 --> 00:13:28,300
wasn't perfect this is why because we

00:13:26,560 --> 00:13:30,040
were not dealing with back pressure we

00:13:28,300 --> 00:13:31,660
were just riding more and more data so

00:13:30,040 --> 00:13:33,190
if you you were using that in two

00:13:31,660 --> 00:13:36,100
different discs with different speeds

00:13:33,190 --> 00:13:37,779
you might end up with this issue so if

00:13:36,100 --> 00:13:40,570
we want to implement that in a better

00:13:37,779 --> 00:13:42,310
way dealing with back pressure we can do

00:13:40,570 --> 00:13:44,680
something like this every time you call

00:13:42,310 --> 00:13:47,050
dot right in your destination stream you

00:13:44,680 --> 00:13:49,240
will get back a boolean and that boolean

00:13:47,050 --> 00:13:51,370
essentially is telling you if it's safe

00:13:49,240 --> 00:13:53,380
to keep sending more data or if you

00:13:51,370 --> 00:13:55,839
should stop because you are overloading

00:13:53,380 --> 00:13:57,550
the destination so I'm calling here this

00:13:55,839 --> 00:14:00,779
variable can continue and then I'm

00:13:57,550 --> 00:14:03,700
basically say if I should not continue I

00:14:00,779 --> 00:14:05,800
want to pause my suit stream which means

00:14:03,700 --> 00:14:08,350
I'm not gonna read anymore data until

00:14:05,800 --> 00:14:09,640
it's safe to keep writing again but how

00:14:08,350 --> 00:14:11,800
do we know when it's safe to keep

00:14:09,640 --> 00:14:14,290
writing again how do we know when the

00:14:11,800 --> 00:14:16,480
destination is flashing all the data to

00:14:14,290 --> 00:14:18,220
the actual disk and we can do that by

00:14:16,480 --> 00:14:20,440
listening to an event that is called

00:14:18,220 --> 00:14:22,900
drain that comes from the destination

00:14:20,440 --> 00:14:24,640
stream and drain means okay I flushed

00:14:22,900 --> 00:14:27,070
all my data and now it's safe for you to

00:14:24,640 --> 00:14:29,170
start sending data again and with that

00:14:27,070 --> 00:14:32,980
callback we can just say su stream you

00:14:29,170 --> 00:14:34,990
can restart and keep reading data so we

00:14:32,980 --> 00:14:39,280
are dealing with bad pressure and it's

00:14:34,990 --> 00:14:41,350
not difficult so much well of course

00:14:39,280 --> 00:14:43,390
there are other types of streams and the

00:14:41,350 --> 00:14:45,339
most famous are duplex and transform

00:14:43,390 --> 00:14:47,560
stream you can see duplex stream as a

00:14:45,339 --> 00:14:50,440
combination of a readable stream and a

00:14:47,560 --> 00:14:52,030
writable stream together they are very

00:14:50,440 --> 00:14:53,980
very good for instance when you want to

00:14:52,030 --> 00:14:55,510
represent network pipes or channels

00:14:53,980 --> 00:14:57,970
where you are both reading and writing

00:14:55,510 --> 00:14:59,860
at the same time and also there is a

00:14:57,970 --> 00:15:02,589
special type of double extreme which is

00:14:59,860 --> 00:15:04,570
transform stream which again is a

00:15:02,589 --> 00:15:07,150
combination of readable writable but

00:15:04,570 --> 00:15:10,930
looks more or less like this so you

00:15:07,150 --> 00:15:13,000
write data on one side the data inside

00:15:10,930 --> 00:15:14,680
the stream gets changed or transformed

00:15:13,000 --> 00:15:15,940
somehow and then when you read it on the

00:15:14,680 --> 00:15:17,320
other side you are not reading the

00:15:15,940 --> 00:15:19,630
original data anymore but you are

00:15:17,320 --> 00:15:22,180
reading a transformation of the data if

00:15:19,630 --> 00:15:25,030
you are wondering how is that useful for

00:15:22,180 --> 00:15:27,040
instance gzipping so you can say I

00:15:25,030 --> 00:15:29,020
have this transformed stream which

00:15:27,040 --> 00:15:31,090
essentially is able to gzip whatever

00:15:29,020 --> 00:15:33,760
data I'm gonna write into it so I can

00:15:31,090 --> 00:15:35,920
write data that is not zipped is gonna

00:15:33,760 --> 00:15:37,390
take care of gzipping it and then on the

00:15:35,920 --> 00:15:40,720
other end when you read the data it's

00:15:37,390 --> 00:15:42,610
already gzipped so with this concept in

00:15:40,720 --> 00:15:45,490
mind you can combine this operator in

00:15:42,610 --> 00:15:49,420
our copy file example and you will copy

00:15:45,490 --> 00:15:52,440
file and you treat them in real time so

00:15:49,420 --> 00:15:55,270
I already described you api's to do

00:15:52,440 --> 00:15:56,950
listen for date on one side and write on

00:15:55,270 --> 00:15:58,390
the other side so you are already

00:15:56,950 --> 00:16:00,610
thinking okay now I have three

00:15:58,390 --> 00:16:02,760
components a source data at a storm

00:16:00,610 --> 00:16:05,410
stream and a destination stream right

00:16:02,760 --> 00:16:07,990
what do I have to do to actually combine

00:16:05,410 --> 00:16:09,970
them all together I'm adding a listener

00:16:07,990 --> 00:16:12,040
to the first one and saying when there

00:16:09,970 --> 00:16:13,690
is data write it to the second one but

00:16:12,040 --> 00:16:15,160
also I have to add a listener to the

00:16:13,690 --> 00:16:17,680
second one and say when there is data

00:16:15,160 --> 00:16:19,300
here write it to the third one and also

00:16:17,680 --> 00:16:21,550
you have to deal with back pressure and

00:16:19,300 --> 00:16:24,070
this is all the code you end up writing

00:16:21,550 --> 00:16:26,080
like a lot of code I don't expect you to

00:16:24,070 --> 00:16:28,030
even read it and I'm gonna tell you I'm

00:16:26,080 --> 00:16:30,880
not even handling error here so it's

00:16:28,030 --> 00:16:33,640
like dangerous it will be even more code

00:16:30,880 --> 00:16:36,700
if we keep adding adding all the error

00:16:33,640 --> 00:16:38,530
handling so this is not good seriously

00:16:36,700 --> 00:16:40,150
there must be a better way otherwise we

00:16:38,530 --> 00:16:42,910
wouldn't be talking about streams today

00:16:40,150 --> 00:16:45,460
and of course there is a better way and

00:16:42,910 --> 00:16:47,290
it's called the pipe operator and the

00:16:45,460 --> 00:16:49,180
pipe operator is super simple and it

00:16:47,290 --> 00:16:50,680
looks like this so essentially when you

00:16:49,180 --> 00:16:53,170
have a readable stream you can just say

00:16:50,680 --> 00:16:55,600
dot pipe and pass to that function

00:16:53,170 --> 00:16:57,190
another stream and that stream can be

00:16:55,600 --> 00:16:59,050
either a transformed stream or a

00:16:57,190 --> 00:17:02,070
writable stream and you can combine

00:16:59,050 --> 00:17:03,820
multiple call to pipe and do multiple

00:17:02,070 --> 00:17:05,950
transformation and eventually you're

00:17:03,820 --> 00:17:07,990
gonna write your data somewhere this

00:17:05,950 --> 00:17:11,350
will take care of creating for you all

00:17:07,990 --> 00:17:12,760
that on data events all the writes so

00:17:11,350 --> 00:17:14,620
you don't have to worry about all these

00:17:12,760 --> 00:17:18,220
things is gonna handle back pressure for

00:17:14,620 --> 00:17:21,460
you and it's much more user friendly so

00:17:18,220 --> 00:17:23,890
if we want to write this code that takes

00:17:21,460 --> 00:17:26,830
some data gzipped it and write somewhere

00:17:23,890 --> 00:17:29,350
else we can just do this like so stream

00:17:26,830 --> 00:17:32,050
pipe it to a gzipstream and pipe it to a

00:17:29,350 --> 00:17:34,090
destination stream this is much better

00:17:32,050 --> 00:17:37,620
writing and easier so probably don't do

00:17:34,090 --> 00:17:40,529
silly mistakes by writing a lot of code

00:17:37,620 --> 00:17:42,870
and this is actually the most common way

00:17:40,529 --> 00:17:46,200
you will see people using no jet streams

00:17:42,870 --> 00:17:47,909
and for instance here you have more

00:17:46,200 --> 00:17:49,140
complicated pipelines where you are

00:17:47,909 --> 00:17:51,630
reading data from somewhere

00:17:49,140 --> 00:17:53,580
for instance s3 you know this data is

00:17:51,630 --> 00:17:55,380
compressed but also encrypted so you

00:17:53,580 --> 00:17:56,640
have to decompress it and decrypted

00:17:55,380 --> 00:17:58,559
before you can actually do some

00:17:56,640 --> 00:18:00,330
transformation and then maybe you want

00:17:58,559 --> 00:18:02,100
to store it somewhere else industry and

00:18:00,330 --> 00:18:04,620
has to be encrypted and compress it

00:18:02,100 --> 00:18:06,690
again so you can build this pipeline in

00:18:04,620 --> 00:18:10,679
a very declarative way by using this

00:18:06,690 --> 00:18:12,360
pipe functionality of course nothing is

00:18:10,679 --> 00:18:14,159
perfect in this world so there is a

00:18:12,360 --> 00:18:15,149
problem with this and the problem is

00:18:14,159 --> 00:18:18,899
that this API

00:18:15,149 --> 00:18:21,630
every time you say pipe into something

00:18:18,899 --> 00:18:23,970
will return you a new instance of a

00:18:21,630 --> 00:18:25,470
stream which essentially means okay this

00:18:23,970 --> 00:18:27,750
is the stream that represents the two

00:18:25,470 --> 00:18:29,610
previous streams combined but now since

00:18:27,750 --> 00:18:33,299
it's a new instance you have all sorts

00:18:29,610 --> 00:18:35,789
of different error and events handlers

00:18:33,299 --> 00:18:37,980
that have to be attached there so to

00:18:35,789 --> 00:18:39,899
actually do proper error handling you

00:18:37,980 --> 00:18:41,549
would have to say own error in every

00:18:39,899 --> 00:18:43,890
single step of the stream because you

00:18:41,549 --> 00:18:45,960
will get different distances and this is

00:18:43,890 --> 00:18:48,480
super ugly but also complicated because

00:18:45,960 --> 00:18:50,309
in your error handling function you

00:18:48,480 --> 00:18:53,450
might want to deal with the error but

00:18:50,309 --> 00:18:56,460
also destroy the streams also close any

00:18:53,450 --> 00:18:58,649
potential I don't know file pointer or

00:18:56,460 --> 00:19:00,809
resource that you might have opened so

00:18:58,649 --> 00:19:05,429
it gets very complicated to handle all

00:19:00,809 --> 00:19:06,659
these errors together so yeah seriously

00:19:05,429 --> 00:19:08,610
we don't want to do all of this

00:19:06,659 --> 00:19:11,520
otherwise we don't want to use streams

00:19:08,610 --> 00:19:14,399
at all and there is a better way also to

00:19:11,520 --> 00:19:16,049
solve this case to talk about that I'm

00:19:14,399 --> 00:19:18,149
gonna introduce a specific stream

00:19:16,049 --> 00:19:21,600
utility that is called stream pipeline

00:19:18,149 --> 00:19:23,760
and Stream pipeline essentially will

00:19:21,600 --> 00:19:25,529
allow you to do the same thing that we

00:19:23,760 --> 00:19:27,029
were doing with the dot v dot pipe with

00:19:25,529 --> 00:19:30,570
the pipe with a slightly different

00:19:27,029 --> 00:19:32,370
syntax so we can just say pipeline then

00:19:30,570 --> 00:19:33,929
we can pass a number of streams that we

00:19:32,370 --> 00:19:36,480
want to be piped together in the

00:19:33,929 --> 00:19:38,460
specific order and then at the end we

00:19:36,480 --> 00:19:41,070
can pass an extra callback and that

00:19:38,460 --> 00:19:42,750
callback is gonna tell us that when the

00:19:41,070 --> 00:19:44,789
stream is finished either because there

00:19:42,750 --> 00:19:47,010
was an error or because all the data was

00:19:44,789 --> 00:19:49,380
moved from one place to another

00:19:47,010 --> 00:19:51,360
and when there is an error this is

00:19:49,380 --> 00:19:53,040
actually the important part this

00:19:51,360 --> 00:19:55,320
implementation will take care of

00:19:53,040 --> 00:19:56,970
destroying all the resources that we are

00:19:55,320 --> 00:19:58,800
located in all the stream so you don't

00:19:56,970 --> 00:20:01,650
have to worry anymore about dealing with

00:19:58,800 --> 00:20:03,750
every single step individually so here

00:20:01,650 --> 00:20:05,550
you just check if error or do something

00:20:03,750 --> 00:20:07,310
otherwise it means everything was fine

00:20:05,550 --> 00:20:09,870
so I really recommend you to use this

00:20:07,310 --> 00:20:11,820
functionality it's by default in not ten

00:20:09,870 --> 00:20:14,190
but there are libraries that will allow

00:20:11,820 --> 00:20:17,790
you to do that if you are using previous

00:20:14,190 --> 00:20:21,570
versions of nodejs then there is another

00:20:17,790 --> 00:20:23,400
very good utility that is called

00:20:21,570 --> 00:20:25,230
redouble stream the name is a little bit

00:20:23,400 --> 00:20:26,700
misleading because it's not just the

00:20:25,230 --> 00:20:28,370
readable streams you actually have all

00:20:26,700 --> 00:20:30,540
the streams code in there and

00:20:28,370 --> 00:20:32,820
essentially if you get the last version

00:20:30,540 --> 00:20:34,890
of no js' for instance today I think is

00:20:32,820 --> 00:20:36,840
12.1 there will be a specific

00:20:34,890 --> 00:20:39,420
implementation of stream of course the

00:20:36,840 --> 00:20:41,880
stream package has evolved over time so

00:20:39,420 --> 00:20:43,890
with this code you are sure that you are

00:20:41,880 --> 00:20:46,140
using a specific implementation so you

00:20:43,890 --> 00:20:47,700
decide which version you want to use so

00:20:46,140 --> 00:20:49,350
if you don't know what is the runtime

00:20:47,700 --> 00:20:51,180
you are going to eventually use to run

00:20:49,350 --> 00:20:53,010
your application you should be using

00:20:51,180 --> 00:20:54,780
this module to be sure you are

00:20:53,010 --> 00:20:59,280
essentially using a compatible

00:20:54,780 --> 00:21:01,440
implementation of streams and now let's

00:20:59,280 --> 00:21:03,630
move on to the last topic which is we

00:21:01,440 --> 00:21:06,990
have been using the default streams or

00:21:03,630 --> 00:21:10,140
HTTP file system what if you want to

00:21:06,990 --> 00:21:12,000
write your own custom streams and I'm

00:21:10,140 --> 00:21:14,220
gonna give you a kind of realistic

00:21:12,000 --> 00:21:16,440
example if you want to be seen so you

00:21:14,220 --> 00:21:19,320
want to stream emoji to the browser and

00:21:16,440 --> 00:21:22,140
by streaming emojis I mean that one

00:21:19,320 --> 00:21:24,840
element is an emoji with its own

00:21:22,140 --> 00:21:26,700
description for instance emoji lemon and

00:21:24,840 --> 00:21:28,770
lemon then we want to do a

00:21:26,700 --> 00:21:31,050
transformation step we want to make this

00:21:28,770 --> 00:21:33,590
description uppercase and then finally

00:21:31,050 --> 00:21:36,030
we want to write this data into the Dom

00:21:33,590 --> 00:21:37,800
so if we have another element who's

00:21:36,030 --> 00:21:39,810
gonna do all the steps and eventually

00:21:37,800 --> 00:21:42,990
it's gonna end up in this list in the

00:21:39,810 --> 00:21:45,120
Dom so the package readable streams is

00:21:42,990 --> 00:21:46,950
gonna give us different classes that we

00:21:45,120 --> 00:21:48,270
can extend to implement all these

00:21:46,950 --> 00:21:51,150
different types of streams

00:21:48,270 --> 00:21:53,790
the first one is readable so you can

00:21:51,150 --> 00:21:55,230
extend this readable class and at that

00:21:53,790 --> 00:21:58,080
point you have to implement that

00:21:55,230 --> 00:21:59,940
underscore read functionality which is

00:21:58,080 --> 00:22:02,220
basically telling the

00:21:59,940 --> 00:22:04,380
the implementation how to consume the

00:22:02,220 --> 00:22:06,870
data source so in this case we are going

00:22:04,380 --> 00:22:09,630
to use it to read data emojis from an

00:22:06,870 --> 00:22:12,720
array for instance then there is the

00:22:09,630 --> 00:22:14,760
transform stream which is something you

00:22:12,720 --> 00:22:16,980
can extend and read eclair this method

00:22:14,760 --> 00:22:19,110
called underscore transform here you

00:22:16,980 --> 00:22:21,810
have to you receive three parameters

00:22:19,110 --> 00:22:24,690
chunk encoding and dawn chunk is the

00:22:21,810 --> 00:22:27,480
actual data that you is passing over the

00:22:24,690 --> 00:22:29,730
different step at this time and Don is a

00:22:27,480 --> 00:22:31,050
callback that you have to invoke to say

00:22:29,730 --> 00:22:33,120
that you finish to do your

00:22:31,050 --> 00:22:35,070
transformation so you can also do a

00:22:33,120 --> 00:22:37,200
synchronous transformations there and

00:22:35,070 --> 00:22:39,360
then finally the writable is very

00:22:37,200 --> 00:22:40,770
similar to the transform one you instead

00:22:39,360 --> 00:22:43,410
you have to write the underscore right

00:22:40,770 --> 00:22:46,200
and you get the same type of inputs

00:22:43,410 --> 00:22:48,240
there and another important element is

00:22:46,200 --> 00:22:50,670
that inside all these functions you can

00:22:48,240 --> 00:22:53,280
use that these dot push data to send

00:22:50,670 --> 00:22:56,430
data from the current step to the next

00:22:53,280 --> 00:22:58,200
step so let's implement the first

00:22:56,430 --> 00:23:01,440
readable stream again I'm using that

00:22:58,200 --> 00:23:03,390
emoji library now I am requiring the

00:23:01,440 --> 00:23:06,750
readable class from the readable stream

00:23:03,390 --> 00:23:10,740
library and then I am extending this

00:23:06,750 --> 00:23:12,780
readable also here since I want to keep

00:23:10,740 --> 00:23:14,100
track I have an array of emojis and I

00:23:12,780 --> 00:23:16,230
want to keep track of what is the

00:23:14,100 --> 00:23:18,060
current emoji that I have to emit of

00:23:16,230 --> 00:23:21,060
course I have to allocate an index

00:23:18,060 --> 00:23:22,620
somewhere the current index so I am

00:23:21,060 --> 00:23:25,050
doing that in the constructor and when

00:23:22,620 --> 00:23:27,390
you have to implement a constructor just

00:23:25,050 --> 00:23:29,340
remember to call the super constructor

00:23:27,390 --> 00:23:31,430
and pass the options there so people

00:23:29,340 --> 00:23:34,050
using your custom stream can still pass

00:23:31,430 --> 00:23:35,640
specific stream options like I don't

00:23:34,050 --> 00:23:38,220
know the buffer size and all these kind

00:23:35,640 --> 00:23:39,900
of things and then I mean implement in

00:23:38,220 --> 00:23:42,960
the underscore read where essentially

00:23:39,900 --> 00:23:45,840
I'm just doing two things if I reach the

00:23:42,960 --> 00:23:47,910
end of my array which means the index is

00:23:45,840 --> 00:23:50,100
bigger than the length then it means

00:23:47,910 --> 00:23:52,550
everything is done I can just push null

00:23:50,100 --> 00:23:55,050
which means the stream is finished or if

00:23:52,550 --> 00:23:58,670
something in between I just pushed the

00:23:55,050 --> 00:24:01,710
current element and increment the index

00:23:58,670 --> 00:24:04,170
so super simple stuff the transform

00:24:01,710 --> 00:24:06,780
stream is even simpler again we import a

00:24:04,170 --> 00:24:08,940
class from this package we extend that

00:24:06,780 --> 00:24:10,980
class we implement the transfer method

00:24:08,940 --> 00:24:13,500
and here what I'm doing I'm just saying

00:24:10,980 --> 00:24:15,690
I'm gonna push this

00:24:13,500 --> 00:24:17,730
as a string and make it uppercase and

00:24:15,690 --> 00:24:19,950
then it's done because it's just a

00:24:17,730 --> 00:24:22,860
synchronous operation I don't have to do

00:24:19,950 --> 00:24:25,650
anything a synchronous here the writable

00:24:22,860 --> 00:24:28,650
stream is super symbol as well you will

00:24:25,650 --> 00:24:30,750
receive you again important class extend

00:24:28,650 --> 00:24:32,760
it and now you create this underscore

00:24:30,750 --> 00:24:35,220
right method you will receive a chunk

00:24:32,760 --> 00:24:38,510
and that chunk will be a text with an

00:24:35,220 --> 00:24:40,290
emoji and some upper classified text and

00:24:38,510 --> 00:24:42,600
essentially we want to display that

00:24:40,290 --> 00:24:45,900
inside a list so we have to create a Dom

00:24:42,600 --> 00:24:47,730
element and then we add the text and

00:24:45,900 --> 00:24:50,400
then eventually we append this Dom

00:24:47,730 --> 00:24:52,620
element into another element called list

00:24:50,400 --> 00:24:54,120
so we are essentially adding more list

00:24:52,620 --> 00:24:56,250
elements every time we are going to

00:24:54,120 --> 00:25:00,300
write something and finally we are going

00:24:56,250 --> 00:25:02,520
to invoke node so how of course this is

00:25:00,300 --> 00:25:05,430
a browser example how do we use this

00:25:02,520 --> 00:25:09,420
code that is obviously written from for

00:25:05,430 --> 00:25:12,240
no GS into the browser and first of all

00:25:09,420 --> 00:25:13,980
how do you use it in general like how do

00:25:12,240 --> 00:25:15,630
you combine these three different

00:25:13,980 --> 00:25:18,630
streams and it's super easy you just

00:25:15,630 --> 00:25:20,400
import them you create instances of for

00:25:18,630 --> 00:25:22,470
every type of stream and then you just

00:25:20,400 --> 00:25:24,930
pipe one to another I could have used

00:25:22,470 --> 00:25:26,520
here also the pipeline but it was just

00:25:24,930 --> 00:25:28,890
easier to do that because I know in this

00:25:26,520 --> 00:25:30,540
case there will not be errors like I'm

00:25:28,890 --> 00:25:33,570
not able to read from an array or stuff

00:25:30,540 --> 00:25:35,880
like that and then of course we have to

00:25:33,570 --> 00:25:37,860
do some step to transpile this code for

00:25:35,880 --> 00:25:40,350
with with something that works in the

00:25:37,860 --> 00:25:42,360
browser I'm using here a lot of requires

00:25:40,350 --> 00:25:43,830
so how do you transform those requires

00:25:42,360 --> 00:25:46,440
into something that the browser can

00:25:43,830 --> 00:25:48,120
understand and in this example I'm using

00:25:46,440 --> 00:25:48,960
webpack but you can use roll-up

00:25:48,120 --> 00:25:51,140
browserify

00:25:48,960 --> 00:25:53,910
whatever you like it's gonna work and

00:25:51,140 --> 00:25:56,700
last piece that we need is some HTML

00:25:53,910 --> 00:25:59,070
that is going to wrap this code and also

00:25:56,700 --> 00:26:02,370
it's going to expose the list where we

00:25:59,070 --> 00:26:04,290
want to append our items so I think I

00:26:02,370 --> 00:26:06,540
have time to show you a very weak demo

00:26:04,290 --> 00:26:09,270
how this is going to look like this is a

00:26:06,540 --> 00:26:11,850
ready render but if I reload the page

00:26:09,270 --> 00:26:13,010
you can see that elements get added back

00:26:11,850 --> 00:26:16,830
one by one

00:26:13,010 --> 00:26:18,600
in reality this will be much faster the

00:26:16,830 --> 00:26:20,520
code that I show you is not actually the

00:26:18,600 --> 00:26:23,580
code here here I'm intentionally slowing

00:26:20,520 --> 00:26:25,560
down the stream so that you can see the

00:26:23,580 --> 00:26:26,730
effect and also adding some CSS

00:26:25,560 --> 00:26:29,040
animation so

00:26:26,730 --> 00:26:32,820
you can see the elements being appended

00:26:29,040 --> 00:26:36,929
one by one I don't always use emojis but

00:26:32,820 --> 00:26:40,230
when I do it should be with streams so

00:26:36,929 --> 00:26:42,299
just to finish up streams are amazing in

00:26:40,230 --> 00:26:45,270
my opinion and especially good if you

00:26:42,299 --> 00:26:47,040
have to worry about being very memory

00:26:45,270 --> 00:26:48,720
efficient or if you have application

00:26:47,040 --> 00:26:50,760
where you are not sure what is gonna be

00:26:48,720 --> 00:26:52,740
the amount of data you are gonna receive

00:26:50,760 --> 00:26:54,390
over time maybe you have starting very

00:26:52,740 --> 00:26:56,100
very small so you are not that concerned

00:26:54,390 --> 00:26:57,720
but you know that eventually you might

00:26:56,100 --> 00:26:59,280
have to deal with huge amounts of data

00:26:57,720 --> 00:27:01,860
it's better to just implement

00:26:59,280 --> 00:27:03,540
straightaway a streaming solution then

00:27:01,860 --> 00:27:05,100
the other good thing about streams is

00:27:03,540 --> 00:27:06,570
that you can process that as soon as

00:27:05,100 --> 00:27:09,419
it's available so you can start to have

00:27:06,570 --> 00:27:12,540
early results rather than waiting for

00:27:09,419 --> 00:27:14,669
every single step to be completed so you

00:27:12,540 --> 00:27:16,410
can also deal with infinite streams so

00:27:14,669 --> 00:27:17,190
if you are always receiving data and

00:27:16,410 --> 00:27:18,840
it's never-ending

00:27:17,190 --> 00:27:20,760
this is actually the only way you can

00:27:18,840 --> 00:27:22,620
produce results on the other side and

00:27:20,760 --> 00:27:24,299
then the other thing I really like about

00:27:22,620 --> 00:27:26,010
streams is that they are an amazing

00:27:24,299 --> 00:27:28,020
abstraction so you can create those

00:27:26,010 --> 00:27:30,510
pipelines and you have very clear

00:27:28,020 --> 00:27:32,820
building blocks so you can see readable

00:27:30,510 --> 00:27:34,740
streams as input transform stream as

00:27:32,820 --> 00:27:36,900
your main business logic where you are

00:27:34,740 --> 00:27:38,850
described or describing how to process

00:27:36,900 --> 00:27:40,710
the data and then eventually you have

00:27:38,850 --> 00:27:42,720
rideable stream to write this data

00:27:40,710 --> 00:27:46,650
somewhere maybe a file system a database

00:27:42,720 --> 00:27:48,330
or even the Dom in my example and if you

00:27:46,650 --> 00:27:50,280
are curious to learn more about streams

00:27:48,330 --> 00:27:52,140
I can promise you there is a lot more to

00:27:50,280 --> 00:27:54,210
learn so I already mentioned the first

00:27:52,140 --> 00:27:56,100
book the second one is also an amazing

00:27:54,210 --> 00:27:58,110
book very practical for not just

00:27:56,100 --> 00:28:00,240
developers and there is as well a

00:27:58,110 --> 00:28:02,309
chapter dedicated to streams with all

00:28:00,240 --> 00:28:04,320
sorts of best practices and then the

00:28:02,309 --> 00:28:06,450
third book is more generic it's not an

00:28:04,320 --> 00:28:08,250
address book if nothing else there are

00:28:06,450 --> 00:28:10,140
Java examples so a little bit scary

00:28:08,250 --> 00:28:12,179
there but it's gonna teach you a lot

00:28:10,140 --> 00:28:13,980
more about the mindset of processing

00:28:12,179 --> 00:28:17,100
stream of data so it's an amazing book

00:28:13,980 --> 00:28:18,840
so very interesting so if you are still

00:28:17,100 --> 00:28:21,000
not convinced or if you're sleeping

00:28:18,840 --> 00:28:24,030
allow me to wake you up with this

00:28:21,000 --> 00:28:26,220
example I don't know if you ever seen

00:28:24,030 --> 00:28:27,960
this before so you can just do a cool

00:28:26,220 --> 00:28:30,600
there and this will happen in your

00:28:27,960 --> 00:28:32,220
stamina so this is actually implemented

00:28:30,600 --> 00:28:34,559
with node.js streams in a way that I

00:28:32,220 --> 00:28:36,299
find very original but I think it will

00:28:34,559 --> 00:28:37,740
if you go and check out the code on

00:28:36,299 --> 00:28:39,670
github you will understand how much

00:28:37,740 --> 00:28:41,830
powerful it is and

00:28:39,670 --> 00:28:43,630
can do all sorts of things with that so

00:28:41,830 --> 00:28:45,340
please go and check out the code there

00:28:43,630 --> 00:28:48,250
and you will be amazed about what you

00:28:45,340 --> 00:28:50,380
can do with streams so if you are

00:28:48,250 --> 00:28:53,550
sleeping now please wake up and go you

00:28:50,380 --> 00:28:58,520
some streams thank you

00:28:53,550 --> 00:28:58,520

YouTube URL: https://www.youtube.com/watch?v=NMqO0JwEEQU


