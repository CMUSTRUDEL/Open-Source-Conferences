Title: NodeJS Scalibility Tips - Luciano Mammino - CityJS Conf 2020
Publication date: 2020-10-03
Playlist: CityJS Conf 2020
Description: 
	"You finally built that amazing start up idea you had in mind for years and you did it using Node.js! That's Great! You just launched it on Hacker News and you are very happy and proud... but now more and more people are using it and you start to have a nasty fear that Node.js won't scale, because you now... it's single threaded! Is your project doomed now? Do you have to invest your time on rewriting it in something like C++ or maybe Rust or even Go? You'd rather invest your time on adding valuable features for your users rather than learning a new language and rewriting everything from scratch, but what if nothing works anymore? And... by the way, what the heck ""single threaded"" really means?!
Fear no more, dear fellow developer! In this talk we will discuss the architecture of Node.js going through its strengths and its weaknesses. We will then talk about scalability and I will share some valuable tips and tricks to make your Node.js app scale! Spoiler alert: you probably won't need Go or Rust :)"

Bio:
Luciano Mammino wrote his first line of code at the age of 12 on his father's old i386. Since then, he has never stopped coding. He is currently working at  FabFitFun as Principal Software Engineer where he is building microservices to serve millions of users every day. Luciano is co-author of Node.js Design Patterns (nodejsdp.link) and he runs Fullstack Bulletin (fstack.link), a free weekly newsletter for fullstack developers.

_________________________________________________________________

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_________________________________________________________________

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:18,080 --> 00:00:23,760
[Music]

00:00:22,640 --> 00:00:26,480
hello everyone

00:00:23,760 --> 00:00:29,519
and welcome to this talk called node.js

00:00:26,480 --> 00:00:31,599
scalability tips

00:00:29,519 --> 00:00:33,120
my name is dusano and i work as a

00:00:31,599 --> 00:00:35,200
principal software engineer

00:00:33,120 --> 00:00:37,040
in dublin ireland for a company called

00:00:35,200 --> 00:00:39,680
fabfitfun

00:00:37,040 --> 00:00:42,000
i am also the co-author of this book

00:00:39,680 --> 00:00:44,399
called node.js design patterns

00:00:42,000 --> 00:00:46,559
and actually during the conference you

00:00:44,399 --> 00:00:47,120
might have a chance to to win a copy of

00:00:46,559 --> 00:00:49,840
this book

00:00:47,120 --> 00:00:51,760
so make sure you visit the website and

00:00:49,840 --> 00:00:53,360
also you check out the conference

00:00:51,760 --> 00:00:55,440
twitter profile

00:00:53,360 --> 00:00:57,520
to find out what are the rules so that

00:00:55,440 --> 00:00:59,359
you can try to win this book

00:00:57,520 --> 00:01:01,120
and if you are unlucky and you happen

00:00:59,359 --> 00:01:03,359
not to win the book and we are still

00:01:01,120 --> 00:01:05,519
interested in buying it if you are on

00:01:03,359 --> 00:01:07,520
amazon.com you can get 20

00:01:05,519 --> 00:01:08,960
discount with the link here in the

00:01:07,520 --> 00:01:11,119
slides

00:01:08,960 --> 00:01:12,080
and make sure to connect with me you can

00:01:11,119 --> 00:01:13,920
check out my blog

00:01:12,080 --> 00:01:15,920
my twitter profile and my github and

00:01:13,920 --> 00:01:17,200
feel free to reach out and ask more

00:01:15,920 --> 00:01:19,520
questions about the book

00:01:17,200 --> 00:01:21,680
or the topics of this presentation later

00:01:19,520 --> 00:01:21,680
on

00:01:22,159 --> 00:01:26,000
so the slides for these talks this talk

00:01:25,040 --> 00:01:27,759
are already online

00:01:26,000 --> 00:01:29,439
and you can get them either by using

00:01:27,759 --> 00:01:32,000
this qr code here

00:01:29,439 --> 00:01:33,600
or by clicking the link you see there

00:01:32,000 --> 00:01:35,200
and the reason why i like to share the

00:01:33,600 --> 00:01:37,280
slides early is because i'm going to

00:01:35,200 --> 00:01:38,960
provide a lot of links and additional

00:01:37,280 --> 00:01:41,200
materials that you can check out

00:01:38,960 --> 00:01:42,159
after this talk so make sure to get the

00:01:41,200 --> 00:01:44,079
slides now

00:01:42,159 --> 00:01:46,079
and it will be easy for you to access

00:01:44,079 --> 00:01:49,280
the extra content

00:01:46,079 --> 00:01:52,560
okay speaking of node.js and scalability

00:01:49,280 --> 00:01:54,159
it's a very large topic what can we

00:01:52,560 --> 00:01:57,520
actually cover in a meaningful

00:01:54,159 --> 00:01:59,360
way in just about half an hour so when i

00:01:57,520 --> 00:02:01,520
started to prepare this talk i wasn't

00:01:59,360 --> 00:02:02,640
really sure about what is the most

00:02:01,520 --> 00:02:05,280
important thing to

00:02:02,640 --> 00:02:06,719
to discuss for people so i went off on

00:02:05,280 --> 00:02:09,440
twitter and i asked

00:02:06,719 --> 00:02:11,280
my friend developers if i say node.js

00:02:09,440 --> 00:02:13,360
and scalability what are the first few

00:02:11,280 --> 00:02:14,640
things and ideas and challenges that

00:02:13,360 --> 00:02:17,200
come to your mind

00:02:14,640 --> 00:02:19,760
and i got some interesting answers for

00:02:17,200 --> 00:02:20,959
instance mateo is also a speaker at this

00:02:19,760 --> 00:02:24,080
conference so make sure

00:02:20,959 --> 00:02:25,840
you check out his talk mention caching

00:02:24,080 --> 00:02:27,120
head of line blocking number of file

00:02:25,840 --> 00:02:30,640
descriptors

00:02:27,120 --> 00:02:32,160
promises bad code then we have luca that

00:02:30,640 --> 00:02:35,280
tries to connect

00:02:32,160 --> 00:02:36,000
scalability with architecture then we

00:02:35,280 --> 00:02:38,480
have kevin

00:02:36,000 --> 00:02:40,080
that talks more about struggles with

00:02:38,480 --> 00:02:42,800
javascript in general

00:02:40,080 --> 00:02:44,239
like spaghetti code refactoring due to

00:02:42,800 --> 00:02:47,440
lack of typing

00:02:44,239 --> 00:02:49,920
uh luxed attitude around testing

00:02:47,440 --> 00:02:52,640
and again we have issues with javascript

00:02:49,920 --> 00:02:54,720
like debugging the event loop

00:02:52,640 --> 00:02:56,080
against scalability in the context of

00:02:54,720 --> 00:02:58,640
architecture

00:02:56,080 --> 00:02:59,840
memory leaks event loop again cpu

00:02:58,640 --> 00:03:03,040
intensive stuff

00:02:59,840 --> 00:03:05,760
that can block the main thread caching

00:03:03,040 --> 00:03:06,640
again and finally the best one and of

00:03:05,760 --> 00:03:08,400
course a joke

00:03:06,640 --> 00:03:10,800
arguments with java people thank you

00:03:08,400 --> 00:03:10,800
trevor

00:03:10,959 --> 00:03:16,400
okay so let's try to to start by

00:03:13,599 --> 00:03:18,319
defining what scalability actually means

00:03:16,400 --> 00:03:20,560
if we look at wikipedia that's the

00:03:18,319 --> 00:03:22,720
definition we get for scalability

00:03:20,560 --> 00:03:25,200
scalability is the property of a system

00:03:22,720 --> 00:03:28,400
to handle a growing amount of work

00:03:25,200 --> 00:03:30,239
by adding resources to the system there

00:03:28,400 --> 00:03:31,760
is actually a slightly better definition

00:03:30,239 --> 00:03:34,640
that i prefer to this one

00:03:31,760 --> 00:03:37,040
that comes from werner vogels who says

00:03:34,640 --> 00:03:39,440
that a service is said to be scalable

00:03:37,040 --> 00:03:40,319
if when we increase the resources in a

00:03:39,440 --> 00:03:42,799
system

00:03:40,319 --> 00:03:44,400
it results in an increased performance

00:03:42,799 --> 00:03:45,760
in a manner proportional to the

00:03:44,400 --> 00:03:47,920
resources added

00:03:45,760 --> 00:03:48,959
so that the key difference here from the

00:03:47,920 --> 00:03:51,680
previous

00:03:48,959 --> 00:03:54,159
definition is that you are adding more

00:03:51,680 --> 00:03:55,280
resources to an architecture to increase

00:03:54,159 --> 00:03:57,519
performance

00:03:55,280 --> 00:03:59,439
but the amount of increased performance

00:03:57,519 --> 00:04:00,080
should be proportional to the amount of

00:03:59,439 --> 00:04:03,040
resources

00:04:00,080 --> 00:04:07,200
added i think this is very important to

00:04:03,040 --> 00:04:07,200
actually define a system as scalable

00:04:07,439 --> 00:04:12,720
okay let's start by defining some tips

00:04:10,720 --> 00:04:14,640
the first tip i want to give you is

00:04:12,720 --> 00:04:18,079
establish a baseline

00:04:14,640 --> 00:04:19,759
what does it mean let's try to take an

00:04:18,079 --> 00:04:22,160
example application first

00:04:19,759 --> 00:04:23,600
so i built this very simple application

00:04:22,160 --> 00:04:27,520
it's a web server that

00:04:23,600 --> 00:04:28,960
will print a qr code on your browser

00:04:27,520 --> 00:04:31,919
and basically the idea is that you can

00:04:28,960 --> 00:04:33,680
pass it some data using a query string

00:04:31,919 --> 00:04:35,520
parameter called data

00:04:33,680 --> 00:04:37,120
and then the qr code you will see will

00:04:35,520 --> 00:04:40,400
actually render the data

00:04:37,120 --> 00:04:42,080
inside the qr code so it is a super

00:04:40,400 --> 00:04:44,639
simple application and you can actually

00:04:42,080 --> 00:04:47,840
play with it live at that url down there

00:04:44,639 --> 00:04:50,560
but let's actually see the code for it

00:04:47,840 --> 00:04:52,240
and it's barely 15 lines of code so it

00:04:50,560 --> 00:04:55,520
is super simple

00:04:52,240 --> 00:04:57,919
what we do is we import a bunch of

00:04:55,520 --> 00:05:00,160
node.js modules for instance the http

00:04:57,919 --> 00:05:01,360
module in particular they create server

00:05:00,160 --> 00:05:03,520
functionality

00:05:01,360 --> 00:05:04,639
then we import the url to be able to

00:05:03,520 --> 00:05:07,120
parse urls

00:05:04,639 --> 00:05:08,720
and finally we import this qr code from

00:05:07,120 --> 00:05:12,560
an external library called

00:05:08,720 --> 00:05:14,240
qr code actually then we define our http

00:05:12,560 --> 00:05:17,039
server

00:05:14,240 --> 00:05:19,360
and we use the create server function to

00:05:17,039 --> 00:05:20,880
bootstrap a server and we listen to port

00:05:19,360 --> 00:05:22,720
00:05:20,880 --> 00:05:24,880
and finally we can write inside the

00:05:22,720 --> 00:05:25,840
under function our business logic for

00:05:24,880 --> 00:05:28,800
the server

00:05:25,840 --> 00:05:30,080
so the first thing we do we pass the

00:05:28,800 --> 00:05:33,199
current url

00:05:30,080 --> 00:05:36,800
the url of the current request

00:05:33,199 --> 00:05:38,080
and we try to extrapolate the data query

00:05:36,800 --> 00:05:40,320
string parameter

00:05:38,080 --> 00:05:43,039
of course if there is no data then this

00:05:40,320 --> 00:05:45,120
is a bad request so we just return 400

00:05:43,039 --> 00:05:47,680
and we close the connection

00:05:45,120 --> 00:05:48,960
otherwise we start to return some

00:05:47,680 --> 00:05:51,759
headers and we say

00:05:48,960 --> 00:05:53,280
okay this is going to be a 200 response

00:05:51,759 --> 00:05:56,720
it's going to be a png

00:05:53,280 --> 00:05:59,360
and finally we use our qr code library

00:05:56,720 --> 00:06:02,479
to create a stream that will contain the

00:05:59,360 --> 00:06:05,199
png data for the qr code

00:06:02,479 --> 00:06:08,560
built around the data that was passed

00:06:05,199 --> 00:06:12,080
through the query string parameter

00:06:08,560 --> 00:06:12,720
so that's all our web server implemented

00:06:12,080 --> 00:06:14,960
here

00:06:12,720 --> 00:06:15,759
how do we know if this web server is

00:06:14,960 --> 00:06:18,800
performed

00:06:15,759 --> 00:06:20,400
well we need to benchmark it somehow

00:06:18,800 --> 00:06:22,960
and there are different ways we can do

00:06:20,400 --> 00:06:23,759
that the the main idea is that we will

00:06:22,960 --> 00:06:26,639
want to

00:06:23,759 --> 00:06:28,080
to fire a number of concurrent requests

00:06:26,639 --> 00:06:31,360
for a period of time

00:06:28,080 --> 00:06:33,199
and see how the the server reacts to it

00:06:31,360 --> 00:06:34,720
and there are two amazing tools that you

00:06:33,199 --> 00:06:37,120
could use to to do this

00:06:34,720 --> 00:06:38,240
the first one is autocannon another one

00:06:37,120 --> 00:06:40,720
is wlk

00:06:38,240 --> 00:06:42,319
actually there are many more these are

00:06:40,720 --> 00:06:44,319
just my favorites

00:06:42,319 --> 00:06:45,759
and i actually really like the first one

00:06:44,319 --> 00:06:46,960
autocannon because it's written in

00:06:45,759 --> 00:06:49,360
node.js

00:06:46,960 --> 00:06:50,800
and therefore it's easier to integrate

00:06:49,360 --> 00:06:52,800
with node.js projects

00:06:50,800 --> 00:06:55,520
for instance in just one liner like the

00:06:52,800 --> 00:06:59,199
one you see here you could say

00:06:55,520 --> 00:07:02,639
run 200 requests in parallel

00:06:59,199 --> 00:07:04,960
and run the specific script and start

00:07:02,639 --> 00:07:05,680
the processing only when you see that

00:07:04,960 --> 00:07:09,759
script

00:07:05,680 --> 00:07:12,639
listening to a board so

00:07:09,759 --> 00:07:15,520
we actually want to do this and see how

00:07:12,639 --> 00:07:18,960
our qr code server performs

00:07:15,520 --> 00:07:21,599
so if we run this command and you can

00:07:18,960 --> 00:07:24,319
see here we are also specifying

00:07:21,599 --> 00:07:26,160
the parameter we want to pass as data

00:07:24,319 --> 00:07:29,120
parameter

00:07:26,160 --> 00:07:30,000
and this is going to run 200 requests in

00:07:29,120 --> 00:07:33,280
parallel for

00:07:30,000 --> 00:07:35,039
10 seconds and see what is the latency

00:07:33,280 --> 00:07:36,000
and what is the number of requests per

00:07:35,039 --> 00:07:38,479
second

00:07:36,000 --> 00:07:39,520
so the average latency is almost two

00:07:38,479 --> 00:07:42,000
seconds

00:07:39,520 --> 00:07:42,560
while the number of requests per second

00:07:42,000 --> 00:07:45,759
is

00:07:42,560 --> 00:07:47,599
about 100 requests per second so these

00:07:45,759 --> 00:07:49,919
numbers are not

00:07:47,599 --> 00:07:50,720
great but they don't look bad either

00:07:49,919 --> 00:07:53,199
actually

00:07:50,720 --> 00:07:54,800
how do we even know like what is good

00:07:53,199 --> 00:07:56,319
and what is bad like what is our

00:07:54,800 --> 00:07:58,000
framework of reference

00:07:56,319 --> 00:07:59,440
also keep in mind that we are running

00:07:58,000 --> 00:08:02,879
this application on my

00:07:59,440 --> 00:08:03,919
development machine so yeah how do we

00:08:02,879 --> 00:08:06,000
know

00:08:03,919 --> 00:08:06,960
how to make sense from these numbers

00:08:06,000 --> 00:08:08,560
right

00:08:06,960 --> 00:08:10,400
one thing that i like to do and this is

00:08:08,560 --> 00:08:13,919
kind of a sub tip

00:08:10,400 --> 00:08:15,120
or tip 1bs if you like is find out what

00:08:13,919 --> 00:08:17,440
is the ceiling

00:08:15,120 --> 00:08:18,639
and by that i mean that yes we are

00:08:17,440 --> 00:08:20,560
developing in

00:08:18,639 --> 00:08:22,639
a specific machine we are developing a

00:08:20,560 --> 00:08:24,080
specific type of application which is a

00:08:22,639 --> 00:08:28,000
web framework

00:08:24,080 --> 00:08:29,919
like what is the best possible

00:08:28,000 --> 00:08:31,520
performance we could achieve in this

00:08:29,919 --> 00:08:34,719
specific scenario

00:08:31,520 --> 00:08:37,440
and in order to do that we will easily

00:08:34,719 --> 00:08:39,360
write a simple http server that just

00:08:37,440 --> 00:08:42,000
returns an hello world

00:08:39,360 --> 00:08:42,719
as you can see here and then if we

00:08:42,000 --> 00:08:45,600
benchmark

00:08:42,719 --> 00:08:46,399
this server in the same way as we did

00:08:45,600 --> 00:08:48,959
here

00:08:46,399 --> 00:08:50,640
we could find what kind of numbers we

00:08:48,959 --> 00:08:53,279
get from this experiment

00:08:50,640 --> 00:08:54,640
and for instance here we get 2.71

00:08:53,279 --> 00:08:58,560
milliseconds

00:08:54,640 --> 00:09:00,480
and 53 000 and more requests per second

00:08:58,560 --> 00:09:02,640
so we could see that these are much much

00:09:00,480 --> 00:09:06,320
better numbers than the previous ones

00:09:02,640 --> 00:09:09,040
which means that our machine is capable

00:09:06,320 --> 00:09:11,279
of achieving this kind of performance

00:09:09,040 --> 00:09:12,480
when building a very very simple http

00:09:11,279 --> 00:09:14,800
web server

00:09:12,480 --> 00:09:16,399
now of course the qr code web server is

00:09:14,800 --> 00:09:19,519
doing a lot more than just

00:09:16,399 --> 00:09:21,120
another word but basically at this point

00:09:19,519 --> 00:09:23,760
we know that there is

00:09:21,120 --> 00:09:25,600
potentially some margin for improvement

00:09:23,760 --> 00:09:26,080
and potentially this is the best we can

00:09:25,600 --> 00:09:29,519
do

00:09:26,080 --> 00:09:32,480
without changing the way node.js work

00:09:29,519 --> 00:09:32,880
dramatically okay but now that we know

00:09:32,480 --> 00:09:35,040
that

00:09:32,880 --> 00:09:36,000
probably we will be improving something

00:09:35,040 --> 00:09:38,399
because there is a lot

00:09:36,000 --> 00:09:40,399
of margin there how do we actually find

00:09:38,399 --> 00:09:41,040
where to start so basically how do we

00:09:40,399 --> 00:09:43,680
find

00:09:41,040 --> 00:09:44,959
the bottleneck of our application and i

00:09:43,680 --> 00:09:46,800
want to introduce

00:09:44,959 --> 00:09:48,240
a suite of tools that is called

00:09:46,800 --> 00:09:51,200
clinic.js

00:09:48,240 --> 00:09:53,360
that allows us to actually troubleshoot

00:09:51,200 --> 00:09:57,440
and find out the bottlenecks of

00:09:53,360 --> 00:10:00,000
node.js applications it is uh

00:09:57,440 --> 00:10:01,760
very complicated now to to give you like

00:10:00,000 --> 00:10:04,640
a comprehensive demo so i really

00:10:01,760 --> 00:10:06,399
recommend you to check out the website

00:10:04,640 --> 00:10:08,079
in the website there is actually a video

00:10:06,399 --> 00:10:09,120
that is worth watching because it's

00:10:08,079 --> 00:10:11,360
going to go through

00:10:09,120 --> 00:10:13,040
all the tools with a real application

00:10:11,360 --> 00:10:14,240
and show you a number of improvements

00:10:13,040 --> 00:10:16,000
you can do

00:10:14,240 --> 00:10:17,519
here i just want to show you which kind

00:10:16,000 --> 00:10:20,240
of tools are available and

00:10:17,519 --> 00:10:22,800
how you could make use of them for

00:10:20,240 --> 00:10:24,560
performance reasons

00:10:22,800 --> 00:10:26,800
so the first tool is called clinic

00:10:24,560 --> 00:10:29,519
doctor and this tool uses

00:10:26,800 --> 00:10:31,200
autocannon behind the scene you can run

00:10:29,519 --> 00:10:34,240
it again to

00:10:31,200 --> 00:10:35,519
fire requests at your server this time

00:10:34,240 --> 00:10:38,320
the difference is that

00:10:35,519 --> 00:10:40,000
while you fire all these requests clinic

00:10:38,320 --> 00:10:41,200
doctor is going to collect a number of

00:10:40,000 --> 00:10:43,839
useful metrics

00:10:41,200 --> 00:10:45,040
for instance the cpu usage over time the

00:10:43,839 --> 00:10:47,279
memory usage

00:10:45,040 --> 00:10:49,040
the number of active handles and even

00:10:47,279 --> 00:10:51,680
the event loop delays

00:10:49,040 --> 00:10:52,160
on top of that clinic doctor is gonna

00:10:51,680 --> 00:10:54,160
run

00:10:52,160 --> 00:10:55,680
machine learning algorithms to try to

00:10:54,160 --> 00:10:58,720
spot patterns

00:10:55,680 --> 00:11:00,000
that will indicate common mistakes

00:10:58,720 --> 00:11:01,760
and it's going to also give you

00:11:00,000 --> 00:11:03,959
suggestions for instance here

00:11:01,760 --> 00:11:05,519
is figuring out that there are

00:11:03,959 --> 00:11:08,399
significantly long

00:11:05,519 --> 00:11:10,000
event loop delays so probably we are

00:11:08,399 --> 00:11:11,120
doing something that is blocking the

00:11:10,000 --> 00:11:13,440
event loop

00:11:11,120 --> 00:11:14,160
and it's recommending us to use another

00:11:13,440 --> 00:11:17,760
tool from

00:11:14,160 --> 00:11:19,440
zim suit to figure out what is actually

00:11:17,760 --> 00:11:22,240
blocking the event loop

00:11:19,440 --> 00:11:23,120
and this tool is called clinic flame and

00:11:22,240 --> 00:11:25,120
clinic flame

00:11:23,120 --> 00:11:28,480
again will do exactly the same thing

00:11:25,120 --> 00:11:31,839
it's gonna put your server under stress

00:11:28,480 --> 00:11:35,200
and in this case is gonna measure

00:11:31,839 --> 00:11:35,519
um all the functions and it's gonna try

00:11:35,200 --> 00:11:38,959
to

00:11:35,519 --> 00:11:42,000
display where is actually where

00:11:38,959 --> 00:11:43,839
the code is actually taking most time

00:11:42,000 --> 00:11:45,440
and for instance here with this flame

00:11:43,839 --> 00:11:48,399
chart we can see that

00:11:45,440 --> 00:11:49,360
this functionality called the filter

00:11:48,399 --> 00:11:52,800
some path

00:11:49,360 --> 00:11:56,320
from the png js library is taking about

00:11:52,800 --> 00:11:58,399
26 actually almost 27 percent of the

00:11:56,320 --> 00:12:00,560
entire benchmark time

00:11:58,399 --> 00:12:02,800
so this is the hottest function so to

00:12:00,560 --> 00:12:05,839
speak which probably means that

00:12:02,800 --> 00:12:09,440
if we can optimize this function

00:12:05,839 --> 00:12:10,880
we could get a very good benefit because

00:12:09,440 --> 00:12:12,720
it's gonna affect most of our

00:12:10,880 --> 00:12:14,720
computation

00:12:12,720 --> 00:12:16,560
now to be honest i didn't even try to

00:12:14,720 --> 00:12:17,040
see if this function is something we

00:12:16,560 --> 00:12:20,639
could

00:12:17,040 --> 00:12:22,880
optimize but what i can i could try

00:12:20,639 --> 00:12:24,000
what i'm thinking to try basically at

00:12:22,880 --> 00:12:27,279
this point is okay

00:12:24,000 --> 00:12:29,519
maybe this png js is taking a lot of

00:12:27,279 --> 00:12:31,120
time because we have to render a png

00:12:29,519 --> 00:12:32,800
maybe there are other libraries that

00:12:31,120 --> 00:12:35,040
could be more efficient i could just try

00:12:32,800 --> 00:12:37,360
to swap it for another library

00:12:35,040 --> 00:12:39,279
or maybe i can just try to render an svg

00:12:37,360 --> 00:12:40,639
rather than a png maybe that could be

00:12:39,279 --> 00:12:44,320
faster

00:12:40,639 --> 00:12:46,240
so basically this is

00:12:44,320 --> 00:12:48,320
the kind of tool that will give you an

00:12:46,240 --> 00:12:49,200
indication of where to start where is

00:12:48,320 --> 00:12:52,399
the bottleneck

00:12:49,200 --> 00:12:55,360
right there is one more tool

00:12:52,399 --> 00:12:56,639
in the clinic gis suite that is called

00:12:55,360 --> 00:12:58,560
clinic bubble

00:12:56,639 --> 00:13:00,160
this tool as well could be used for

00:12:58,560 --> 00:13:03,920
performance reason

00:13:00,160 --> 00:13:05,839
but it is more suited to basically give

00:13:03,920 --> 00:13:07,920
you a visibility of the relationship

00:13:05,839 --> 00:13:10,160
between the different asynchronous calls

00:13:07,920 --> 00:13:11,920
so i thought it was worth mentioning but

00:13:10,160 --> 00:13:14,079
make sure you check out the demo

00:13:11,920 --> 00:13:16,000
in the website to really understand how

00:13:14,079 --> 00:13:18,240
you can take advantage of this other

00:13:16,000 --> 00:13:21,440
tool

00:13:18,240 --> 00:13:25,440
okay let's move to another tip now

00:13:21,440 --> 00:13:26,240
now that we know um how to find specific

00:13:25,440 --> 00:13:29,279
bottlenecks

00:13:26,240 --> 00:13:30,079
let's talk about defining some goals and

00:13:29,279 --> 00:13:32,480
understanding

00:13:30,079 --> 00:13:34,480
our goals for instance when we try to

00:13:32,480 --> 00:13:36,480
optimize an application

00:13:34,480 --> 00:13:38,240
we might be optimizing for different

00:13:36,480 --> 00:13:40,320
things for instance we could try to

00:13:38,240 --> 00:13:42,160
improve the throughput we could try to

00:13:40,320 --> 00:13:43,600
decrease the amount of memory consumed

00:13:42,160 --> 00:13:46,720
by the application

00:13:43,600 --> 00:13:48,880
we might want to improve the latency now

00:13:46,720 --> 00:13:50,720
chances are that if you try to optimize

00:13:48,880 --> 00:13:52,720
for one of those dimensions

00:13:50,720 --> 00:13:53,760
you might have to compromise another

00:13:52,720 --> 00:13:56,880
dimension

00:13:53,760 --> 00:13:59,279
i'll give you an example if we want to

00:13:56,880 --> 00:14:01,440
increase the throughput maybe what we

00:13:59,279 --> 00:14:03,519
will do is to be very very aggressive

00:14:01,440 --> 00:14:06,240
with caching so we can put a caching

00:14:03,519 --> 00:14:08,720
layer in front of our application

00:14:06,240 --> 00:14:10,639
and what what is going to happen is that

00:14:08,720 --> 00:14:12,160
basically

00:14:10,639 --> 00:14:14,240
our memory consumption is going to

00:14:12,160 --> 00:14:14,800
increase because all the responses will

00:14:14,240 --> 00:14:17,279
be

00:14:14,800 --> 00:14:19,279
saved in memory so that next time we can

00:14:17,279 --> 00:14:21,199
 them very quickly

00:14:19,279 --> 00:14:22,480
so in this case what we are doing we are

00:14:21,199 --> 00:14:25,040
trading memory

00:14:22,480 --> 00:14:26,720
for throughput and this is not very

00:14:25,040 --> 00:14:28,800
uncommon that you are basically trading

00:14:26,720 --> 00:14:30,720
one dimension for another

00:14:28,800 --> 00:14:32,079
so make sure that you understand what is

00:14:30,720 --> 00:14:34,320
your primary objective

00:14:32,079 --> 00:14:36,720
because probably if you try to optimize

00:14:34,320 --> 00:14:38,639
for everything you are not going to get

00:14:36,720 --> 00:14:40,320
great results probably you're just going

00:14:38,639 --> 00:14:41,440
to do something very average but it's

00:14:40,320 --> 00:14:43,199
not going to excel

00:14:41,440 --> 00:14:45,440
in the specific metric you want to

00:14:43,199 --> 00:14:47,600
achieve so

00:14:45,440 --> 00:14:49,279
the point is try to make sure you really

00:14:47,600 --> 00:14:50,240
understand what is your primary

00:14:49,279 --> 00:14:52,560
objective

00:14:50,240 --> 00:14:53,760
optimize for that first and only after

00:14:52,560 --> 00:14:57,040
that try to look at

00:14:53,760 --> 00:14:57,040
secondary objectives

00:14:57,760 --> 00:15:02,800
now the the next tip is always observe

00:15:00,959 --> 00:15:04,320
and this is an interesting one because

00:15:02,800 --> 00:15:06,399
so far we have been looking

00:15:04,320 --> 00:15:08,000
at one web server a web server

00:15:06,399 --> 00:15:11,519
application running only

00:15:08,000 --> 00:15:13,680
on my machines or development machine

00:15:11,519 --> 00:15:16,000
so always observe what i want to suggest

00:15:13,680 --> 00:15:19,360
by that is actually to look

00:15:16,000 --> 00:15:22,240
at the application running in production

00:15:19,360 --> 00:15:24,320
so when we talk about visibility or if

00:15:22,240 --> 00:15:26,480
you prefer observability in production

00:15:24,320 --> 00:15:29,199
generally there are three concepts that

00:15:26,480 --> 00:15:32,480
are worth exploring

00:15:29,199 --> 00:15:35,759
logs metrics and traces of course

00:15:32,480 --> 00:15:37,519
you all know logs and with that i

00:15:35,759 --> 00:15:39,360
basically mean that your application

00:15:37,519 --> 00:15:41,759
needs to have meaningful logs for

00:15:39,360 --> 00:15:43,519
instance in the case of a web server

00:15:41,759 --> 00:15:46,160
you should be logging all sorts of

00:15:43,519 --> 00:15:48,320
errors happening in the application

00:15:46,160 --> 00:15:49,839
probably you would want to log requests

00:15:48,320 --> 00:15:51,920
and responses

00:15:49,839 --> 00:15:54,880
and different parameters of those

00:15:51,920 --> 00:15:58,720
requested responses like status codes

00:15:54,880 --> 00:16:01,360
part query string parameters

00:15:58,720 --> 00:16:03,360
the ip of the incoming request and

00:16:01,360 --> 00:16:05,279
things like that

00:16:03,360 --> 00:16:07,279
and logs are basically the first place

00:16:05,279 --> 00:16:08,000
where you will go to try to see if an

00:16:07,279 --> 00:16:10,959
application is

00:16:08,000 --> 00:16:14,240
performing is behaving correctly or if

00:16:10,959 --> 00:16:16,800
there is an error try to have a first

00:16:14,240 --> 00:16:18,959
description of what the action the

00:16:16,800 --> 00:16:21,040
actual error was

00:16:18,959 --> 00:16:22,160
then we have metrics metrics are

00:16:21,040 --> 00:16:24,639
something you should look

00:16:22,160 --> 00:16:26,240
more in aggregate basically metrics

00:16:24,639 --> 00:16:27,920
could be either system

00:16:26,240 --> 00:16:30,399
performance metrics i don't know how

00:16:27,920 --> 00:16:31,680
much cpu are we consuming at a given

00:16:30,399 --> 00:16:33,759
moment in time

00:16:31,680 --> 00:16:35,680
or there could also be business magic

00:16:33,759 --> 00:16:36,240
for instance how many users are logged

00:16:35,680 --> 00:16:39,519
in

00:16:36,240 --> 00:16:40,720
at a certain point in time but metrics

00:16:39,519 --> 00:16:41,519
generally something that you will

00:16:40,720 --> 00:16:43,759
consume

00:16:41,519 --> 00:16:45,600
through dashboards so for instance you

00:16:43,759 --> 00:16:48,240
could build a dashboard for

00:16:45,600 --> 00:16:50,959
the average number of requests per

00:16:48,240 --> 00:16:52,480
second that we are serving over time

00:16:50,959 --> 00:16:54,720
and that could be very useful for

00:16:52,480 --> 00:16:56,800
instance every time you deploy a new

00:16:54,720 --> 00:16:59,199
release of your application you will see

00:16:56,800 --> 00:17:00,800
if this number is going up or down

00:16:59,199 --> 00:17:02,639
which could basically tell you if you

00:17:00,800 --> 00:17:04,400
manage to figure out some

00:17:02,639 --> 00:17:06,880
performance improvement or if you have

00:17:04,400 --> 00:17:09,919
some performance degradation

00:17:06,880 --> 00:17:13,839
and finally we have traces and traces

00:17:09,919 --> 00:17:15,439
are something very similar to the flame

00:17:13,839 --> 00:17:17,520
chart we saw before

00:17:15,439 --> 00:17:18,480
but while the flame chart is focused

00:17:17,520 --> 00:17:21,919
only on your

00:17:18,480 --> 00:17:23,839
actual node.js application a trace will

00:17:21,919 --> 00:17:24,720
try to cover an entire distributed

00:17:23,839 --> 00:17:26,720
system

00:17:24,720 --> 00:17:28,799
so the moment you you start a new

00:17:26,720 --> 00:17:31,919
request to the system

00:17:28,799 --> 00:17:33,679
and where that request actually ends up

00:17:31,919 --> 00:17:36,240
for instance you could

00:17:33,679 --> 00:17:37,919
end up in a web server first then maybe

00:17:36,240 --> 00:17:40,960
the web server could issue

00:17:37,919 --> 00:17:42,400
an api request to an external api

00:17:40,960 --> 00:17:44,720
maybe at some point it's going to go to

00:17:42,400 --> 00:17:47,039
a database and make a query

00:17:44,720 --> 00:17:49,120
with traces you will be able to see in

00:17:47,039 --> 00:17:51,360
every single step how much time

00:17:49,120 --> 00:17:53,039
are we spending in order to be able to

00:17:51,360 --> 00:17:54,559
answer to that request

00:17:53,039 --> 00:17:56,559
and if something goes wrong you will

00:17:54,559 --> 00:17:58,559
probably see

00:17:56,559 --> 00:18:00,080
what is the source of the error or if

00:17:58,559 --> 00:18:01,679
something is slowing down

00:18:00,080 --> 00:18:04,000
the entire response you can probably

00:18:01,679 --> 00:18:06,640
pinpoint what is the main bottleneck in

00:18:04,000 --> 00:18:09,360
the entire distributed system

00:18:06,640 --> 00:18:09,840
now i don't want to advocate for any

00:18:09,360 --> 00:18:12,240
tool

00:18:09,840 --> 00:18:13,760
to implement logs metric and traces i'm

00:18:12,240 --> 00:18:15,840
just recommending you

00:18:13,760 --> 00:18:17,679
to make sure you have enough coverage in

00:18:15,840 --> 00:18:20,240
all those three areas

00:18:17,679 --> 00:18:22,000
but there are many many tools you can

00:18:20,240 --> 00:18:24,400
pick from the open source

00:18:22,000 --> 00:18:25,919
to implement logs metric and traces just

00:18:24,400 --> 00:18:28,480
make sure you do

00:18:25,919 --> 00:18:30,400
that in our production applications now

00:18:28,480 --> 00:18:32,160
the final tip which is actually probably

00:18:30,400 --> 00:18:34,080
the most important tip and we're gonna

00:18:32,160 --> 00:18:35,760
be spending probably the last 10 minutes

00:18:34,080 --> 00:18:38,799
of this presentation on that

00:18:35,760 --> 00:18:40,720
is scale your architecture i think that

00:18:38,799 --> 00:18:42,960
this is the most important tip

00:18:40,720 --> 00:18:43,840
because so far we've been talking about

00:18:42,960 --> 00:18:46,080
performance

00:18:43,840 --> 00:18:47,679
which is basically we have an

00:18:46,080 --> 00:18:50,080
application running somewhere

00:18:47,679 --> 00:18:51,520
either my development machine or a

00:18:50,080 --> 00:18:54,000
production server

00:18:51,520 --> 00:18:55,840
and we are trying to squeeze the most

00:18:54,000 --> 00:18:57,840
out of that application basically means

00:18:55,840 --> 00:19:01,280
we are changing the code

00:18:57,840 --> 00:19:03,679
so that in the same setup so the same

00:19:01,280 --> 00:19:05,919
laptop or the same server

00:19:03,679 --> 00:19:07,039
that same application performs better

00:19:05,919 --> 00:19:08,960
and better

00:19:07,039 --> 00:19:11,760
of course there is only so much you can

00:19:08,960 --> 00:19:15,280
optimize at some point you're gonna

00:19:11,760 --> 00:19:18,640
get the most out of the language and the

00:19:15,280 --> 00:19:19,120
libraries and your design so at some

00:19:18,640 --> 00:19:20,960
point

00:19:19,120 --> 00:19:22,480
we have to think differently and we have

00:19:20,960 --> 00:19:25,600
to start to think okay

00:19:22,480 --> 00:19:27,600
in terms of scalability which means we

00:19:25,600 --> 00:19:29,520
need to add more resources to the an

00:19:27,600 --> 00:19:31,679
entire system so that we can

00:19:29,520 --> 00:19:33,440
respond to an increasing amount of

00:19:31,679 --> 00:19:35,760
demand right

00:19:33,440 --> 00:19:37,840
so let's forget about performance which

00:19:35,760 --> 00:19:38,960
was a useful exercise and let's start to

00:19:37,840 --> 00:19:42,320
focus more on

00:19:38,960 --> 00:19:44,720
scalability so let's start to think

00:19:42,320 --> 00:19:45,840
how can we design an entire architecture

00:19:44,720 --> 00:19:48,960
where

00:19:45,840 --> 00:19:49,360
at some point if we hit a limit we can

00:19:48,960 --> 00:19:51,760
add

00:19:49,360 --> 00:19:54,720
more into the infrastructure so that we

00:19:51,760 --> 00:19:57,039
can bump that limit higher and higher

00:19:54,720 --> 00:19:58,720
so in other words how can we scale a

00:19:57,039 --> 00:20:00,880
system by adding resources

00:19:58,720 --> 00:20:02,159
which is the definition we gave for

00:20:00,880 --> 00:20:04,159
scalability

00:20:02,159 --> 00:20:06,400
and there is a framework that i actually

00:20:04,159 --> 00:20:08,640
like a lot that is called the scale cube

00:20:06,400 --> 00:20:09,760
that can help us to identify different

00:20:08,640 --> 00:20:12,080
techniques for

00:20:09,760 --> 00:20:13,360
basically building architecture that

00:20:12,080 --> 00:20:15,760
scale

00:20:13,360 --> 00:20:17,600
and this framework comes from this book

00:20:15,760 --> 00:20:20,000
called the art of scalability

00:20:17,600 --> 00:20:21,440
which is actually not the newest book in

00:20:20,000 --> 00:20:23,679
the scene but it's still

00:20:21,440 --> 00:20:25,679
very very fresh so i really recommend

00:20:23,679 --> 00:20:27,840
you to get a copy and read it

00:20:25,679 --> 00:20:28,799
because all the lessons there are super

00:20:27,840 --> 00:20:30,720
valuable

00:20:28,799 --> 00:20:31,840
especially if you happen to be working

00:20:30,720 --> 00:20:34,960
on systems that

00:20:31,840 --> 00:20:37,840
will probably need to scale very quickly

00:20:34,960 --> 00:20:38,799
now it is called scale cube because it

00:20:37,840 --> 00:20:41,360
tries to

00:20:38,799 --> 00:20:42,320
define three main techniques and these

00:20:41,360 --> 00:20:44,960
three techniques

00:20:42,320 --> 00:20:46,799
are represented as the three axis of a

00:20:44,960 --> 00:20:49,280
cube

00:20:46,799 --> 00:20:50,640
the first technique which is the x-axis

00:20:49,280 --> 00:20:52,960
is called cloning

00:20:50,640 --> 00:20:53,919
then we have the y-axis with functional

00:20:52,960 --> 00:20:56,000
decomposition

00:20:53,919 --> 00:20:57,760
and finally we have the z-axis with

00:20:56,000 --> 00:20:59,919
partitioning

00:20:57,760 --> 00:21:01,840
let's start by discussing cloning and

00:20:59,919 --> 00:21:03,120
cloning is particularly interesting for

00:21:01,840 --> 00:21:05,440
node.js

00:21:03,120 --> 00:21:06,960
because we know that with node.js you're

00:21:05,440 --> 00:21:10,000
generally running

00:21:06,960 --> 00:21:11,840
one process and you have the event loop

00:21:10,000 --> 00:21:13,840
taking care of mostly a synchronous

00:21:11,840 --> 00:21:15,360
stuff for you but

00:21:13,840 --> 00:21:17,360
probably if you have a multi-core

00:21:15,360 --> 00:21:19,280
machine you are not going to exploit

00:21:17,360 --> 00:21:20,400
fully all the resources available in the

00:21:19,280 --> 00:21:22,000
system

00:21:20,400 --> 00:21:24,240
so one very common technique with

00:21:22,000 --> 00:21:26,480
node.js is to basically take the same

00:21:24,240 --> 00:21:29,280
web server the same process

00:21:26,480 --> 00:21:30,480
and spin it up multiple times and then

00:21:29,280 --> 00:21:31,520
basically i have something like a

00:21:30,480 --> 00:21:33,280
reverse proxy

00:21:31,520 --> 00:21:34,640
in front of all these processes to

00:21:33,280 --> 00:21:36,320
distribute the

00:21:34,640 --> 00:21:37,760
the load among all the available

00:21:36,320 --> 00:21:39,120
processes

00:21:37,760 --> 00:21:41,600
and this is something that once you do

00:21:39,120 --> 00:21:44,400
for one machine and you

00:21:41,600 --> 00:21:46,080
you basically saturate on the computing

00:21:44,400 --> 00:21:48,320
capacity of that machine

00:21:46,080 --> 00:21:50,240
you can start to create more machines

00:21:48,320 --> 00:21:53,039
and clone also the number of machine

00:21:50,240 --> 00:21:54,799
with multiple processes running inside

00:21:53,039 --> 00:21:57,600
and you can see the left case is

00:21:54,799 --> 00:22:00,400
basically when you are cloning

00:21:57,600 --> 00:22:01,520
one machine and the case on the right is

00:22:00,400 --> 00:22:03,760
basically when

00:22:01,520 --> 00:22:05,520
you are cloning that machine multiple

00:22:03,760 --> 00:22:06,960
times and you use a lot balancer to

00:22:05,520 --> 00:22:09,919
distribute the load

00:22:06,960 --> 00:22:11,919
now cloning a single uh process is

00:22:09,919 --> 00:22:13,919
something that node.js can be achieved

00:22:11,919 --> 00:22:16,640
in multiple ways

00:22:13,919 --> 00:22:19,120
and one of the easiest way in my opinion

00:22:16,640 --> 00:22:22,320
is by using the cluster module

00:22:19,120 --> 00:22:23,840
so the cluster module is built in in the

00:22:22,320 --> 00:22:26,480
node.js standard library

00:22:23,840 --> 00:22:27,679
and allows you to create a master

00:22:26,480 --> 00:22:29,760
process

00:22:27,679 --> 00:22:31,760
which can spin up multiple worker

00:22:29,760 --> 00:22:34,480
processes and then coordinate

00:22:31,760 --> 00:22:35,600
the the request the incoming request and

00:22:34,480 --> 00:22:37,280
the responses

00:22:35,600 --> 00:22:38,720
which basically means that when a new

00:22:37,280 --> 00:22:40,320
request comes in

00:22:38,720 --> 00:22:42,880
it's going to select the first worker

00:22:40,320 --> 00:22:45,120
process and delegate

00:22:42,880 --> 00:22:46,240
that worker browser to respond to the

00:22:45,120 --> 00:22:48,559
request

00:22:46,240 --> 00:22:50,640
and as more requests come in is going to

00:22:48,559 --> 00:22:53,679
basically distribute the request to all

00:22:50,640 --> 00:22:55,600
the other available workers

00:22:53,679 --> 00:22:57,200
now i want to show you very very quickly

00:22:55,600 --> 00:22:58,480
how to implement this because i think

00:22:57,200 --> 00:23:00,720
it's very easy

00:22:58,480 --> 00:23:02,240
therefore you should probably consider

00:23:00,720 --> 00:23:03,919
using this technique anyway

00:23:02,240 --> 00:23:05,520
just because it doesn't cost you much to

00:23:03,919 --> 00:23:08,159
do it

00:23:05,520 --> 00:23:08,960
and basically what we have to do is

00:23:08,159 --> 00:23:12,159
first of all

00:23:08,960 --> 00:23:14,960
to import the cluster module

00:23:12,159 --> 00:23:17,679
then we can use the os module to get the

00:23:14,960 --> 00:23:19,039
number of cores available in the system

00:23:17,679 --> 00:23:21,679
and then this is actually the most

00:23:19,039 --> 00:23:24,559
interesting bit we do this if statement

00:23:21,679 --> 00:23:26,000
with this cluster.this master that

00:23:24,559 --> 00:23:28,880
allows us to branch

00:23:26,000 --> 00:23:29,760
the two different types of executions

00:23:28,880 --> 00:23:32,080
either you are

00:23:29,760 --> 00:23:32,799
the master process and you want to do

00:23:32,080 --> 00:23:34,720
something

00:23:32,799 --> 00:23:36,159
or you are the worker and you want to do

00:23:34,720 --> 00:23:38,320
something else

00:23:36,159 --> 00:23:40,320
so if you are in the master case the

00:23:38,320 --> 00:23:40,799
only thing we want to do right now is to

00:23:40,320 --> 00:23:42,960
spawn

00:23:40,799 --> 00:23:44,640
all the workers and here we are

00:23:42,960 --> 00:23:47,840
basically doing a loop that is

00:23:44,640 --> 00:23:50,000
running cluster fork for every car

00:23:47,840 --> 00:23:52,080
available in the system

00:23:50,000 --> 00:23:53,520
when we run a cluster fork what actually

00:23:52,080 --> 00:23:55,120
is going to happen behind the scene is

00:23:53,520 --> 00:23:58,000
that this entire script

00:23:55,120 --> 00:23:59,039
is going to be executed again this time

00:23:58,000 --> 00:24:02,400
we are going to end up

00:23:59,039 --> 00:24:04,000
in the second part of the branch

00:24:02,400 --> 00:24:05,440
where we actually have to implement the

00:24:04,000 --> 00:24:07,679
worker code

00:24:05,440 --> 00:24:09,360
since we already have written our worker

00:24:07,679 --> 00:24:09,919
code which is essentially the business

00:24:09,360 --> 00:24:13,120
logic

00:24:09,919 --> 00:24:14,559
to create an http server which can

00:24:13,120 --> 00:24:16,320
render qr codes

00:24:14,559 --> 00:24:18,400
basically we just need to require that

00:24:16,320 --> 00:24:22,720
file and that will bootstrap

00:24:18,400 --> 00:24:25,200
all that code in the context of worker

00:24:22,720 --> 00:24:26,559
now if you run this code on my machine

00:24:25,200 --> 00:24:28,320
which has eight cores

00:24:26,559 --> 00:24:30,880
and you run the benchmarks again you

00:24:28,320 --> 00:24:32,880
will see a throughput improvement that

00:24:30,880 --> 00:24:36,159
is from 3 to 4 time

00:24:32,880 --> 00:24:37,279
just by doing this slightly less than 10

00:24:36,159 --> 00:24:40,000
lines of

00:24:37,279 --> 00:24:41,600
wrapping so again i recommend you to

00:24:40,000 --> 00:24:43,440
look into this technique

00:24:41,600 --> 00:24:45,440
because it can make a lot of difference

00:24:43,440 --> 00:24:48,080
with a very simple

00:24:45,440 --> 00:24:49,520
raptor code now this is not the only way

00:24:48,080 --> 00:24:51,919
to implement this stuff

00:24:49,520 --> 00:24:53,600
because since node 14 we can also use

00:24:51,919 --> 00:24:57,200
worker threads

00:24:53,600 --> 00:24:59,440
and worker threads allow you to create

00:24:57,200 --> 00:25:01,120
real trends so not just processes but

00:24:59,440 --> 00:25:02,880
actually threads where you can actually

00:25:01,120 --> 00:25:03,520
even share memory between the different

00:25:02,880 --> 00:25:05,760
threads

00:25:03,520 --> 00:25:07,440
so in some cases this might be a more

00:25:05,760 --> 00:25:08,960
flexible approach

00:25:07,440 --> 00:25:11,120
and there are a bunch of interesting

00:25:08,960 --> 00:25:11,600
libraries but one that i think stands

00:25:11,120 --> 00:25:14,559
out

00:25:11,600 --> 00:25:15,279
is piscina which implements a bunch of

00:25:14,559 --> 00:25:18,159
patterns

00:25:15,279 --> 00:25:20,000
to spin up workers and distribute load

00:25:18,159 --> 00:25:21,760
between all the available workers

00:25:20,000 --> 00:25:23,600
and there is even an example that is

00:25:21,760 --> 00:25:26,799
very similar to the

00:25:23,600 --> 00:25:29,200
the web server scenario i just described

00:25:26,799 --> 00:25:32,320
with my example

00:25:29,200 --> 00:25:34,320
so keep this library in mind and make

00:25:32,320 --> 00:25:38,159
sure to to give a try to work

00:25:34,320 --> 00:25:40,159
threads with node.js now cloning is

00:25:38,159 --> 00:25:41,679
probably the easiest strategy to scale a

00:25:40,159 --> 00:25:43,840
service

00:25:41,679 --> 00:25:45,760
but you need to be sure that your

00:25:43,840 --> 00:25:48,880
application is stateless

00:25:45,760 --> 00:25:49,600
what does it mean it means that you have

00:25:48,880 --> 00:25:52,240
a user

00:25:49,600 --> 00:25:54,080
and this user is probably eating your

00:25:52,240 --> 00:25:57,840
server multiple times

00:25:54,080 --> 00:25:58,240
during a session right if it's important

00:25:57,840 --> 00:26:00,400
for

00:25:58,240 --> 00:26:02,080
your application to remember information

00:26:00,400 --> 00:26:04,400
about this user

00:26:02,080 --> 00:26:05,360
doing stuff on your application you're

00:26:04,400 --> 00:26:07,919
probably gonna

00:26:05,360 --> 00:26:09,279
keep some information in memory

00:26:07,919 --> 00:26:12,240
somewhere

00:26:09,279 --> 00:26:14,000
for instance is this user logged in

00:26:12,240 --> 00:26:15,600
probably we have created a session for

00:26:14,000 --> 00:26:16,880
this user there is a session id

00:26:15,600 --> 00:26:20,159
somewhere

00:26:16,880 --> 00:26:22,320
and if this user is doing stuff we are

00:26:20,159 --> 00:26:23,440
going to associate information with this

00:26:22,320 --> 00:26:25,440
session

00:26:23,440 --> 00:26:28,320
now if this session is implemented in

00:26:25,440 --> 00:26:30,880
memory for every single server

00:26:28,320 --> 00:26:32,080
for every single instance of the server

00:26:30,880 --> 00:26:33,919
what is going to happen is that the

00:26:32,080 --> 00:26:35,840
first request is going to hit one server

00:26:33,919 --> 00:26:37,200
and the user maybe is going to log in

00:26:35,840 --> 00:26:39,440
the single request is going to hit

00:26:37,200 --> 00:26:41,679
another instance of the server

00:26:39,440 --> 00:26:43,440
and in that instance we don't have this

00:26:41,679 --> 00:26:45,440
information about the user session so

00:26:43,440 --> 00:26:48,250
the login is not valid anymore

00:26:45,440 --> 00:26:49,360
so you get the point if we have to store

00:26:48,250 --> 00:26:52,400
[Music]

00:26:49,360 --> 00:26:54,400
stateful information about the user flow

00:26:52,400 --> 00:26:56,480
that information shouldn't leave in

00:26:54,400 --> 00:26:59,919
memory but it needs to live in a

00:26:56,480 --> 00:27:01,520
shared storage that can be a radius that

00:26:59,919 --> 00:27:03,679
can be a database

00:27:01,520 --> 00:27:05,279
you choose but it needs to be something

00:27:03,679 --> 00:27:07,600
at every single instance

00:27:05,279 --> 00:27:09,840
that you are cloning is able to access

00:27:07,600 --> 00:27:11,760
to and get the information

00:27:09,840 --> 00:27:13,760
now let's talk about the second

00:27:11,760 --> 00:27:15,600
technique which is the y-axis

00:27:13,760 --> 00:27:17,440
and it's called functional decomposition

00:27:15,600 --> 00:27:18,880
but it's probably most well known as

00:27:17,440 --> 00:27:20,480
microservices

00:27:18,880 --> 00:27:22,000
i'm not going to spend a huge amount of

00:27:20,480 --> 00:27:23,200
time because you're probably familiar

00:27:22,000 --> 00:27:26,320
with the terminology

00:27:23,200 --> 00:27:28,399
so i'm just going to give you an example

00:27:26,320 --> 00:27:30,480
imagine we are building an ecommerce and

00:27:28,399 --> 00:27:34,000
at some point

00:27:30,480 --> 00:27:35,840
we need to to scale this ecommerce and

00:27:34,000 --> 00:27:37,520
we realized that it might be convenient

00:27:35,840 --> 00:27:40,960
to start to split

00:27:37,520 --> 00:27:43,760
a monolithic application into multiple

00:27:40,960 --> 00:27:46,840
smaller applications

00:27:43,760 --> 00:27:49,279
and for instance here we identified

00:27:46,840 --> 00:27:51,360
that we are going to create a slash

00:27:49,279 --> 00:27:53,520
products

00:27:51,360 --> 00:27:55,760
web server that deals with all the

00:27:53,520 --> 00:27:58,880
regards the product domain

00:27:55,760 --> 00:28:01,039
and then we can create a slash cart

00:27:58,880 --> 00:28:03,279
web server that deals with all the

00:28:01,039 --> 00:28:04,960
things relevant for managing the user

00:28:03,279 --> 00:28:06,880
card

00:28:04,960 --> 00:28:08,640
and then we can put an api gateway in

00:28:06,880 --> 00:28:10,880
front of these

00:28:08,640 --> 00:28:12,640
two services to kind of delegate the

00:28:10,880 --> 00:28:14,320
traffic

00:28:12,640 --> 00:28:16,480
of course every service is going to have

00:28:14,320 --> 00:28:18,480
its own database and try to be as much

00:28:16,480 --> 00:28:20,480
decoupled as possible from the other

00:28:18,480 --> 00:28:21,760
services

00:28:20,480 --> 00:28:23,200
now what is interesting about this

00:28:21,760 --> 00:28:24,320
technique is that at some point you

00:28:23,200 --> 00:28:27,440
might realize okay

00:28:24,320 --> 00:28:29,279
i might need to scale the card service

00:28:27,440 --> 00:28:31,440
more because now i am receiving a lot of

00:28:29,279 --> 00:28:32,720
requests only to that specific part of

00:28:31,440 --> 00:28:34,480
the application

00:28:32,720 --> 00:28:36,640
and you can do that you can use the

00:28:34,480 --> 00:28:38,320
cloning technique

00:28:36,640 --> 00:28:39,760
so you are basically combining the

00:28:38,320 --> 00:28:41,520
functional decomposition with the

00:28:39,760 --> 00:28:43,760
cloning technique

00:28:41,520 --> 00:28:45,520
and scale the different services

00:28:43,760 --> 00:28:47,600
independently so creating multiple

00:28:45,520 --> 00:28:49,600
instances and distributing the load

00:28:47,600 --> 00:28:50,960
of incoming requests between all the

00:28:49,600 --> 00:28:52,720
available instances

00:28:50,960 --> 00:28:54,000
and you could do the same for products

00:28:52,720 --> 00:28:56,559
as well

00:28:54,000 --> 00:28:58,559
now node.js is great for microservices

00:28:56,559 --> 00:29:02,159
this is of course just my opinion

00:28:58,559 --> 00:29:04,480
but i feel that the whole

00:29:02,159 --> 00:29:06,880
let's create simple software that can be

00:29:04,480 --> 00:29:08,799
easily combined

00:29:06,880 --> 00:29:11,039
which is the the unix philosophy but

00:29:08,799 --> 00:29:14,399
also the philosophy that node.js rely

00:29:11,039 --> 00:29:16,320
very heavily on it suits very very well

00:29:14,399 --> 00:29:18,080
the idea of microservices

00:29:16,320 --> 00:29:20,000
and you will probably find that as you

00:29:18,080 --> 00:29:22,080
build not just microservices

00:29:20,000 --> 00:29:24,000
this kind of mindset will help you out

00:29:22,080 --> 00:29:25,279
to build simple system that can be

00:29:24,000 --> 00:29:27,919
composed together

00:29:25,279 --> 00:29:29,279
but at the same time one thing that is

00:29:27,919 --> 00:29:32,240
very important to highlight

00:29:29,279 --> 00:29:34,080
is that micro services are useful to

00:29:32,240 --> 00:29:36,320
scale and architecture but i believe

00:29:34,080 --> 00:29:37,679
they are even more useful to scale your

00:29:36,320 --> 00:29:39,440
organization

00:29:37,679 --> 00:29:41,520
which means basically organize your

00:29:39,440 --> 00:29:42,960
teams around the world that the company

00:29:41,520 --> 00:29:47,120
has to do to to grow

00:29:42,960 --> 00:29:49,120
the service and to grow the organization

00:29:47,120 --> 00:29:50,200
so just keep it in mind that i think you

00:29:49,120 --> 00:29:52,640
should start to think about

00:29:50,200 --> 00:29:54,559
microservices when you realize

00:29:52,640 --> 00:29:55,760
you have problems with scaling your

00:29:54,559 --> 00:29:58,880
organizations or

00:29:55,760 --> 00:30:00,640
scaling your teams rather than just

00:29:58,880 --> 00:30:03,679
thinking okay i need microservices

00:30:00,640 --> 00:30:05,440
because my architecture needs to scale

00:30:03,679 --> 00:30:07,120
and another reason why i say that is

00:30:05,440 --> 00:30:09,600
because microservices are not

00:30:07,120 --> 00:30:10,880
silver bullets they also add a lot of

00:30:09,600 --> 00:30:13,039
complexity

00:30:10,880 --> 00:30:15,520
and just to name few areas where you're

00:30:13,039 --> 00:30:17,520
going to have a lot of added complexity

00:30:15,520 --> 00:30:18,559
for instance observability how do we

00:30:17,520 --> 00:30:21,279
collect

00:30:18,559 --> 00:30:23,200
metrics from many grinding instances of

00:30:21,279 --> 00:30:25,279
different types

00:30:23,200 --> 00:30:27,120
how do we do deployments especially if

00:30:25,279 --> 00:30:28,720
there are services that rely on each

00:30:27,120 --> 00:30:30,799
other how do we make sure that we can

00:30:28,720 --> 00:30:32,799
deploy a new version without breaking

00:30:30,799 --> 00:30:35,279
existing services

00:30:32,799 --> 00:30:36,000
so versioning again can become a concern

00:30:35,279 --> 00:30:38,000
how do we keep

00:30:36,000 --> 00:30:40,000
different version running how do we even

00:30:38,000 --> 00:30:43,200
know which version are running for

00:30:40,000 --> 00:30:45,360
every instance and

00:30:43,200 --> 00:30:46,240
final integration when we have all these

00:30:45,360 --> 00:30:48,559
complicated

00:30:46,240 --> 00:30:50,480
systems different versions of different

00:30:48,559 --> 00:30:53,200
services running all the time

00:30:50,480 --> 00:30:55,039
how do we connect the dots and make sure

00:30:53,200 --> 00:30:56,320
every service is talking with the right

00:30:55,039 --> 00:30:59,840
service

00:30:56,320 --> 00:31:02,000
okay let's talk now about the final

00:30:59,840 --> 00:31:04,080
scalability technique and this is

00:31:02,000 --> 00:31:06,640
partitioning which is the z-axis

00:31:04,080 --> 00:31:09,679
of the scale cube and partition is

00:31:06,640 --> 00:31:12,240
basically an idea that allows you to

00:31:09,679 --> 00:31:13,440
basically distribute data along customer

00:31:12,240 --> 00:31:15,600
boundaries

00:31:13,440 --> 00:31:16,880
what does it mean let's try to describe

00:31:15,600 --> 00:31:19,600
it with

00:31:16,880 --> 00:31:21,360
again the ecommerce example so let's try

00:31:19,600 --> 00:31:22,799
to think that at some point we have so

00:31:21,360 --> 00:31:25,440
many products that

00:31:22,799 --> 00:31:26,880
our databases are struggling to keep up

00:31:25,440 --> 00:31:28,640
with our queries

00:31:26,880 --> 00:31:30,240
we have terabytes and database of data

00:31:28,640 --> 00:31:31,840
in our database

00:31:30,240 --> 00:31:34,799
one thing we could do is to actually

00:31:31,840 --> 00:31:37,519
start to split our databases into

00:31:34,799 --> 00:31:39,360
two or more databases and for instance

00:31:37,519 --> 00:31:42,880
one thing that we could split on

00:31:39,360 --> 00:31:43,600
is we could simply say all the products

00:31:42,880 --> 00:31:46,480
which id

00:31:43,600 --> 00:31:48,080
starts with letters from a to l we'll go

00:31:46,480 --> 00:31:49,840
to database 1

00:31:48,080 --> 00:31:52,640
and all the others will go to database

00:31:49,840 --> 00:31:54,880
2. and this is called shard partitioning

00:31:52,640 --> 00:31:57,039
because we are basically creating

00:31:54,880 --> 00:31:58,559
distributing our data across different

00:31:57,039 --> 00:32:01,600
charts

00:31:58,559 --> 00:32:02,880
now again partitioning is not something

00:32:01,600 --> 00:32:05,279
super simple because

00:32:02,880 --> 00:32:06,559
once we do this if we have to do a query

00:32:05,279 --> 00:32:08,880
that will go across

00:32:06,559 --> 00:32:11,039
all the products basically we will need

00:32:08,880 --> 00:32:13,120
to split this query in two systems and

00:32:11,039 --> 00:32:15,039
then aggregate the results

00:32:13,120 --> 00:32:16,799
so of course partitioning would add

00:32:15,039 --> 00:32:18,720
additional complexity

00:32:16,799 --> 00:32:20,880
but you get the point that basically at

00:32:18,720 --> 00:32:22,000
this point here we are distributing the

00:32:20,880 --> 00:32:24,240
load

00:32:22,000 --> 00:32:25,440
or that initially was managed by one

00:32:24,240 --> 00:32:28,399
single database

00:32:25,440 --> 00:32:29,360
now it is distributed to two databases

00:32:28,399 --> 00:32:31,840
so we could

00:32:29,360 --> 00:32:34,720
of course split more and more to adapt

00:32:31,840 --> 00:32:36,559
to the the increasing demand

00:32:34,720 --> 00:32:38,399
so partition is generally used as a

00:32:36,559 --> 00:32:40,159
technique to scale databases but it

00:32:38,399 --> 00:32:42,480
could also be used to scale

00:32:40,159 --> 00:32:43,279
such software geographically for

00:32:42,480 --> 00:32:47,679
instance you

00:32:43,279 --> 00:32:49,120
if you ever use amazon or aws

00:32:47,679 --> 00:32:51,360
you probably notice that you have

00:32:49,120 --> 00:32:53,679
different instances of

00:32:51,360 --> 00:32:54,480
the entire application in different

00:32:53,679 --> 00:32:56,799
regions

00:32:54,480 --> 00:32:58,320
for instance you have different

00:32:56,799 --> 00:33:01,039
deployments of amazon

00:32:58,320 --> 00:33:02,399
regarding if you are in italy or in uk

00:33:01,039 --> 00:33:05,039
or in the states

00:33:02,399 --> 00:33:06,960
or if you are using aws you can log into

00:33:05,039 --> 00:33:09,120
different regions and you can place your

00:33:06,960 --> 00:33:11,679
instances in different regions

00:33:09,120 --> 00:33:12,960
most likely the implementation behind

00:33:11,679 --> 00:33:15,519
this

00:33:12,960 --> 00:33:17,279
regional version of the application are

00:33:15,519 --> 00:33:18,640
literally a clone of the entire

00:33:17,279 --> 00:33:21,039
application

00:33:18,640 --> 00:33:22,080
copied in different places in different

00:33:21,039 --> 00:33:25,279
networks

00:33:22,080 --> 00:33:28,399
spread across different regions so let's

00:33:25,279 --> 00:33:30,480
try to summarize what we discussed today

00:33:28,399 --> 00:33:31,519
the first tip i gave you is establish a

00:33:30,480 --> 00:33:33,919
baseline

00:33:31,519 --> 00:33:34,720
when you have a new application and you

00:33:33,919 --> 00:33:37,919
want to know

00:33:34,720 --> 00:33:38,320
how it is performing before starting to

00:33:37,919 --> 00:33:40,480
change

00:33:38,320 --> 00:33:41,360
anything in your application try to

00:33:40,480 --> 00:33:43,600
benchmark it

00:33:41,360 --> 00:33:45,679
and see what is your starting point so

00:33:43,600 --> 00:33:48,080
every time you're gonna change something

00:33:45,679 --> 00:33:49,519
you can have a point of reference that

00:33:48,080 --> 00:33:50,960
allows you to compare

00:33:49,519 --> 00:33:53,519
if you're doing better or if you're

00:33:50,960 --> 00:33:55,679
actually even doing worse

00:33:53,519 --> 00:33:57,440
then find your bottleneck don't just try

00:33:55,679 --> 00:34:00,240
to optimize randomly

00:33:57,440 --> 00:34:02,320
try to find what is the most meaningful

00:34:00,240 --> 00:34:03,919
piece of code or functionality that is

00:34:02,320 --> 00:34:06,159
taking most of the time

00:34:03,919 --> 00:34:07,760
because chances are that if you manage

00:34:06,159 --> 00:34:08,639
to optimize that you're probably gonna

00:34:07,760 --> 00:34:11,679
get the most

00:34:08,639 --> 00:34:12,560
gain so it is probably the the place

00:34:11,679 --> 00:34:16,079
where it's worth

00:34:12,560 --> 00:34:18,480
starting your exploration

00:34:16,079 --> 00:34:19,919
then try to understand your goals so

00:34:18,480 --> 00:34:20,399
don't just try to optimize for

00:34:19,919 --> 00:34:22,399
everything

00:34:20,399 --> 00:34:25,040
try to figure out whether you want to

00:34:22,399 --> 00:34:26,879
optimize for memory throughput latency

00:34:25,040 --> 00:34:29,200
and make sure you have a primary goal

00:34:26,879 --> 00:34:31,359
and focus only on that goal first

00:34:29,200 --> 00:34:33,200
only when you achieve your goal you can

00:34:31,359 --> 00:34:35,599
maybe try to spend some additional time

00:34:33,200 --> 00:34:37,839
in looking at secondary goals

00:34:35,599 --> 00:34:40,560
always observe so make sure you have a

00:34:37,839 --> 00:34:42,240
lot of visibility also in production

00:34:40,560 --> 00:34:44,079
visibility in production is probably

00:34:42,240 --> 00:34:45,440
even more important than visibility in

00:34:44,079 --> 00:34:48,320
your own machine

00:34:45,440 --> 00:34:48,720
and make sure your observe all the time

00:34:48,320 --> 00:34:50,800
so

00:34:48,720 --> 00:34:52,000
if something starts to go wrong you

00:34:50,800 --> 00:34:54,480
should be able to know this

00:34:52,000 --> 00:34:55,040
as soon as possible and react to

00:34:54,480 --> 00:34:56,960
something

00:34:55,040 --> 00:34:58,720
wrong and at the same time if something

00:34:56,960 --> 00:35:00,320
improves dramatically you should be able

00:34:58,720 --> 00:35:04,560
to see that improvement

00:35:00,320 --> 00:35:06,560
and realize what contributed to it

00:35:04,560 --> 00:35:08,480
and finally scale your architecture

00:35:06,560 --> 00:35:10,160
which means try to use this technique

00:35:08,480 --> 00:35:13,280
cloning the composition

00:35:10,160 --> 00:35:15,920
and partitioning and design

00:35:13,280 --> 00:35:17,680
your architecture in such a way that by

00:35:15,920 --> 00:35:19,599
using these techniques you can keep

00:35:17,680 --> 00:35:22,400
adding more resources

00:35:19,599 --> 00:35:25,599
if you realize your system needs to

00:35:22,400 --> 00:35:27,920
respond to increasing load of requests

00:35:25,599 --> 00:35:29,440
so that's basically it for me thank you

00:35:27,920 --> 00:35:31,680
very much for staying with me

00:35:29,440 --> 00:35:32,720
i hope it was interesting and if you

00:35:31,680 --> 00:35:34,560
have any question

00:35:32,720 --> 00:35:36,320
we should have some time for question

00:35:34,560 --> 00:35:38,880
and answer but if not

00:35:36,320 --> 00:35:39,599
feel free to reach out online to me on

00:35:38,880 --> 00:35:42,800
twitter

00:35:39,599 --> 00:35:45,839
and we can have a chat there thank you

00:35:42,800 --> 00:35:45,839
very much

00:35:56,150 --> 00:36:03,920
[Music]

00:36:00,880 --> 00:36:06,960
so luciano author of uh nodejs patterns

00:36:03,920 --> 00:36:09,280
great book recommended um thank you

00:36:06,960 --> 00:36:10,320
and uh it's one of the first notebooks i

00:36:09,280 --> 00:36:11,680
bought and it's

00:36:10,320 --> 00:36:14,480
i still refer to it i think it's a great

00:36:11,680 --> 00:36:15,280
book um wait for some questions to come

00:36:14,480 --> 00:36:18,000
in but maybe

00:36:15,280 --> 00:36:19,440
uh if you could just tell me what you've

00:36:18,000 --> 00:36:21,920
seen

00:36:19,440 --> 00:36:23,440
in the wild as sort of the most common

00:36:21,920 --> 00:36:24,560
scalability problems

00:36:23,440 --> 00:36:26,560
i mean you've given us some good

00:36:24,560 --> 00:36:28,079
guidance on performance and

00:36:26,560 --> 00:36:30,079
and solutions maybe you've got a horror

00:36:28,079 --> 00:36:31,200
story you'd like to you'd like to tell

00:36:30,079 --> 00:36:33,119
us quickly about

00:36:31,200 --> 00:36:34,880
something you've seen that has just

00:36:33,119 --> 00:36:36,560
blown your mind about how bad it was for

00:36:34,880 --> 00:36:39,280
scalability

00:36:36,560 --> 00:36:40,560
sure yeah so i guess one of the most

00:36:39,280 --> 00:36:43,440
common things i've seen

00:36:40,560 --> 00:36:44,480
is that you end up deploying your

00:36:43,440 --> 00:36:46,560
application maybe

00:36:44,480 --> 00:36:48,880
i don't know in an ec2 instance that has

00:36:46,560 --> 00:36:50,400
a bunch of calls available

00:36:48,880 --> 00:36:53,040
and then you don't use something like

00:36:50,400 --> 00:36:55,680
the cluster module to actually

00:36:53,040 --> 00:36:58,079
try to exploit as much of the computing

00:36:55,680 --> 00:37:01,119
power available in that machine

00:36:58,079 --> 00:37:01,839
so even just doing that you can see a

00:37:01,119 --> 00:37:04,560
dramatic

00:37:01,839 --> 00:37:05,920
improvement of performance so that

00:37:04,560 --> 00:37:08,480
that's something that

00:37:05,920 --> 00:37:10,160
i don't think is that much well known in

00:37:08,480 --> 00:37:12,240
the node.js community

00:37:10,160 --> 00:37:14,000
so i do recommend people to if you've

00:37:12,240 --> 00:37:15,680
never seen that before to just check it

00:37:14,000 --> 00:37:16,480
out and it's super easy to use and

00:37:15,680 --> 00:37:18,320
you'll probably

00:37:16,480 --> 00:37:20,160
see a lot of gain in your real-life

00:37:18,320 --> 00:37:22,160
application by just applying that

00:37:20,160 --> 00:37:25,119
pattern which is super simple

00:37:22,160 --> 00:37:26,079
so so do you think that people have like

00:37:25,119 --> 00:37:28,400
there's a

00:37:26,079 --> 00:37:29,760
people don't have knowledge of what

00:37:28,400 --> 00:37:30,880
node.js is capable

00:37:29,760 --> 00:37:32,400
of doing performance and

00:37:30,880 --> 00:37:33,200
scalability-wise sometimes do you think

00:37:32,400 --> 00:37:36,160
that's

00:37:33,200 --> 00:37:36,640
one of the bigger problems i think

00:37:36,160 --> 00:37:38,960
that's

00:37:36,640 --> 00:37:40,720
true maybe only in some environments

00:37:38,960 --> 00:37:43,680
like when you don't have

00:37:40,720 --> 00:37:45,520
senior people in a node.js team there is

00:37:43,680 --> 00:37:46,400
a little bit of a meet that node.js

00:37:45,520 --> 00:37:48,800
doesn't scale

00:37:46,400 --> 00:37:49,520
so it's very easy to just fell for the

00:37:48,800 --> 00:37:52,000
meet

00:37:49,520 --> 00:37:53,599
and disregard no js entirely just

00:37:52,000 --> 00:37:54,720
because you heard somewhere oh it's

00:37:53,599 --> 00:37:57,520
single traded

00:37:54,720 --> 00:37:58,560
we need to do very intensive stuff so

00:37:57,520 --> 00:38:01,200
this

00:37:58,560 --> 00:38:02,640
system is not gonna scale with node.js i

00:38:01,200 --> 00:38:04,320
don't think that's always true

00:38:02,640 --> 00:38:06,079
of course there might be cases where

00:38:04,320 --> 00:38:08,160
you're actually doing a lot of cpu

00:38:06,079 --> 00:38:11,200
intensive stuff where other languages

00:38:08,160 --> 00:38:12,160
might perform better than node.js but i

00:38:11,200 --> 00:38:15,200
would say that for

00:38:12,160 --> 00:38:18,160
most use cases i think node.js scales

00:38:15,200 --> 00:38:18,560
very very well so again my advice would

00:38:18,160 --> 00:38:21,760
be

00:38:18,560 --> 00:38:24,160
just don't be naive about uh just

00:38:21,760 --> 00:38:25,440
hearing voices and whispers saying oh no

00:38:24,160 --> 00:38:27,040
just doesn't scale

00:38:25,440 --> 00:38:29,119
actually try to understand what is the

00:38:27,040 --> 00:38:30,000
node.js model what the event loop

00:38:29,119 --> 00:38:32,720
actually does

00:38:30,000 --> 00:38:33,520
and how you can you take advantage of it

00:38:32,720 --> 00:38:36,000
and then

00:38:33,520 --> 00:38:36,800
you should be able to evaluate that uh

00:38:36,000 --> 00:38:39,599
maybe for your

00:38:36,800 --> 00:38:41,280
uk is going to be good enough or not and

00:38:39,599 --> 00:38:43,359
then you can decide whether using

00:38:41,280 --> 00:38:46,400
node.js or not

00:38:43,359 --> 00:38:48,160
yep good good so one in from one in from

00:38:46,400 --> 00:38:50,000
the uh the audience here

00:38:48,160 --> 00:38:51,760
where would you start with this on a

00:38:50,000 --> 00:38:52,800
large production mode app

00:38:51,760 --> 00:38:54,240
i mean if you've got if you've got

00:38:52,800 --> 00:38:55,680
something in production you want to do

00:38:54,240 --> 00:38:57,440
some

00:38:55,680 --> 00:38:59,119
it's large units you assume there's some

00:38:57,440 --> 00:39:00,800
scalability going on there

00:38:59,119 --> 00:39:03,119
are there any quick wings or systematic

00:39:00,800 --> 00:39:04,960
approaches that work

00:39:03,119 --> 00:39:07,200
yeah i guess one place where i would

00:39:04,960 --> 00:39:09,200
start and this is a complicated question

00:39:07,200 --> 00:39:11,599
so i guess there might be

00:39:09,200 --> 00:39:13,520
many different good answers i'll try to

00:39:11,599 --> 00:39:16,000
give you my point of view

00:39:13,520 --> 00:39:17,680
maybe one first thing i will try to see

00:39:16,000 --> 00:39:18,400
is whether this application is a

00:39:17,680 --> 00:39:20,240
monolith

00:39:18,400 --> 00:39:21,440
or is already like a micro service

00:39:20,240 --> 00:39:24,079
architecture

00:39:21,440 --> 00:39:25,440
because depending on that probably will

00:39:24,079 --> 00:39:28,640
follow different parts

00:39:25,440 --> 00:39:29,680
as a first step supposedly if it's some

00:39:28,640 --> 00:39:31,839
micro service

00:39:29,680 --> 00:39:33,920
maybe you um sorry if it's a monolith

00:39:31,839 --> 00:39:35,440
maybe you can start with the

00:39:33,920 --> 00:39:37,040
benchmarks that i showed at the

00:39:35,440 --> 00:39:38,560
beginning of the talk just to start to

00:39:37,040 --> 00:39:40,480
have a feeling on

00:39:38,560 --> 00:39:42,320
which parts of the application are

00:39:40,480 --> 00:39:45,200
actually the heaviest ones

00:39:42,320 --> 00:39:45,680
so whenever you start to find requests

00:39:45,200 --> 00:39:48,640
where

00:39:45,680 --> 00:39:50,400
is actually the computing time spent and

00:39:48,640 --> 00:39:51,760
you can actually at that point realize

00:39:50,400 --> 00:39:53,200
okay

00:39:51,760 --> 00:39:55,359
maybe i don't know accessing the

00:39:53,200 --> 00:39:56,000
database is what takes most of the time

00:39:55,359 --> 00:39:59,280
so probably

00:39:56,000 --> 00:40:00,880
my bottleneck is in that layer therefore

00:39:59,280 --> 00:40:02,560
it's probably worth starting by

00:40:00,880 --> 00:40:04,079
exploring what's happening in the

00:40:02,560 --> 00:40:05,760
database maybe

00:40:04,079 --> 00:40:07,359
i don't know you you're not adding the

00:40:05,760 --> 00:40:11,440
right indexes in

00:40:07,359 --> 00:40:13,359
in your queries or

00:40:11,440 --> 00:40:15,200
i don't know maybe you need to scale

00:40:13,359 --> 00:40:17,280
more than the database

00:40:15,200 --> 00:40:19,599
or you need to design the tables in a

00:40:17,280 --> 00:40:22,000
different way

00:40:19,599 --> 00:40:23,119
so i think there isn't like a silver

00:40:22,000 --> 00:40:24,560
bullet

00:40:23,119 --> 00:40:26,800
i think the most important thing to do

00:40:24,560 --> 00:40:27,359
is to start to measure and start to find

00:40:26,800 --> 00:40:30,319
out

00:40:27,359 --> 00:40:30,720
what is actually take the most time and

00:40:30,319 --> 00:40:33,359
then

00:40:30,720 --> 00:40:34,880
at that point you can d deep dive and

00:40:33,359 --> 00:40:38,720
from there figure out if you can do

00:40:34,880 --> 00:40:38,720
anything to improve

00:40:41,119 --> 00:40:45,599
good um no thank you um

00:40:46,319 --> 00:40:50,240
another question about uh what people

00:40:48,400 --> 00:40:52,960
should do can i or should

00:40:50,240 --> 00:40:55,599
i automate some kind automate some kind

00:40:52,960 --> 00:40:59,280
of performance testing like this

00:40:55,599 --> 00:41:01,520
i think that it is generally a good idea

00:40:59,280 --> 00:41:03,440
it can also be expensive so what i would

00:41:01,520 --> 00:41:08,319
recommend just to be practical

00:41:03,440 --> 00:41:10,160
is um try to make sure that you do that

00:41:08,319 --> 00:41:13,119
only when you have business requirements

00:41:10,160 --> 00:41:15,440
that will try to enforce you that maybe

00:41:13,119 --> 00:41:17,359
you have i don't know an sla that forces

00:41:15,440 --> 00:41:19,680
you to keep a specific

00:41:17,359 --> 00:41:22,560
uh latency for instance at that point

00:41:19,680 --> 00:41:25,280
it's definitely super valuable to have

00:41:22,560 --> 00:41:26,880
testing like benchmark tests automated

00:41:25,280 --> 00:41:28,720
that you can run over time but most

00:41:26,880 --> 00:41:30,640
importantly that you have metrics

00:41:28,720 --> 00:41:32,800
so that you can actually observe if you

00:41:30,640 --> 00:41:35,040
are respecting your sla over time

00:41:32,800 --> 00:41:35,839
and how close you are to the actual

00:41:35,040 --> 00:41:37,680
limit

00:41:35,839 --> 00:41:39,119
and at that point maybe it's a warning

00:41:37,680 --> 00:41:40,160
sign that you should start to do

00:41:39,119 --> 00:41:41,839
something to

00:41:40,160 --> 00:41:43,599
to scale more and make sure you are

00:41:41,839 --> 00:41:45,920
always within your boundaries

00:41:43,599 --> 00:41:47,920
so i wouldn't recommend you to just do a

00:41:45,920 --> 00:41:49,440
lot of automated performance testing

00:41:47,920 --> 00:41:52,319
just because they're complicated

00:41:49,440 --> 00:41:53,760
to get right and also very expensive but

00:41:52,319 --> 00:41:55,599
make sure that if you have business

00:41:53,760 --> 00:41:57,760
requirements that

00:41:55,599 --> 00:41:59,440
enforces you to actually respect some

00:41:57,760 --> 00:42:00,400
limits that you do everything in your

00:41:59,440 --> 00:42:02,960
power

00:42:00,400 --> 00:42:04,880
to kind of cover those ones so don't

00:42:02,960 --> 00:42:05,920
don't just go wild and try to see and

00:42:04,880 --> 00:42:07,440
cover everything but

00:42:05,920 --> 00:42:09,680
try to focus again on what are the

00:42:07,440 --> 00:42:12,960
primary objectives

00:42:09,680 --> 00:42:14,319
good good okay uh they're coming in

00:42:12,960 --> 00:42:16,240
thick and fast now uh

00:42:14,319 --> 00:42:18,640
and getting more complex as we go so

00:42:16,240 --> 00:42:22,319
when running in containers orchestration

00:42:18,640 --> 00:42:23,599
kubernetes etc if the app is not cpu or

00:42:22,319 --> 00:42:26,240
memory bound

00:42:23,599 --> 00:42:27,920
okay but instead relays traffic to an

00:42:26,240 --> 00:42:30,400
underlying api

00:42:27,920 --> 00:42:33,200
what would the metric what would be the

00:42:30,400 --> 00:42:35,839
metric to auto scale on

00:42:33,200 --> 00:42:37,839
that's a good one i guess one thing that

00:42:35,839 --> 00:42:40,960
maybe i will do

00:42:37,839 --> 00:42:44,000
is to try to have a feeling of

00:42:40,960 --> 00:42:45,920
for every pod or match requests can you

00:42:44,000 --> 00:42:47,599
take per second

00:42:45,920 --> 00:42:49,280
maybe at that point that's a good enough

00:42:47,599 --> 00:42:51,359
indicator like when you see that you are

00:42:49,280 --> 00:42:52,800
saturating that the number of requests

00:42:51,359 --> 00:42:53,680
per second that you can take for all

00:42:52,800 --> 00:42:55,119
your pods

00:42:53,680 --> 00:42:57,680
maybe it's just the time to add more

00:42:55,119 --> 00:42:58,160
points that point you maybe who start to

00:42:57,680 --> 00:43:00,560
have

00:42:58,160 --> 00:43:02,960
issues with load balancers so at some

00:43:00,560 --> 00:43:04,400
point if you really have a huge website

00:43:02,960 --> 00:43:06,480
you'll need to start to scale load

00:43:04,400 --> 00:43:09,119
balancers as well which probably means

00:43:06,480 --> 00:43:10,160
doing i don't know dns's in such a way

00:43:09,119 --> 00:43:12,000
that you will

00:43:10,160 --> 00:43:13,839
pick and choose different load balancer

00:43:12,000 --> 00:43:15,440
and distribute even more

00:43:13,839 --> 00:43:17,520
but yeah that's a tricky one with

00:43:15,440 --> 00:43:19,119
kubernetes i'm not the most familiar and

00:43:17,520 --> 00:43:21,040
i'm learning like right now

00:43:19,119 --> 00:43:22,319
so i might be missing something obvious

00:43:21,040 --> 00:43:23,920
there okay

00:43:22,319 --> 00:43:26,160
there's this almost a follow-up question

00:43:23,920 --> 00:43:27,760
here so uh from enrique he's saying

00:43:26,160 --> 00:43:29,839
what's what's your opinion about using

00:43:27,760 --> 00:43:33,200
node.js clusters versus

00:43:29,839 --> 00:43:33,839
running n node.js processors with pm2 or

00:43:33,200 --> 00:43:37,680
docker

00:43:33,839 --> 00:43:41,599
for example pros and cons right

00:43:37,680 --> 00:43:44,560
i think it's the same principle so

00:43:41,599 --> 00:43:46,240
it's probably just a different way of

00:43:44,560 --> 00:43:48,160
implementing the same idea

00:43:46,240 --> 00:43:50,960
so i'm not sure if in practice there is

00:43:48,160 --> 00:43:53,359
like a big difference

00:43:50,960 --> 00:43:54,160
i guess if you are already using docker

00:43:53,359 --> 00:43:57,520
probably

00:43:54,160 --> 00:43:58,160
you don't need to use pm2 or the cluster

00:43:57,520 --> 00:44:01,520
module

00:43:58,160 --> 00:44:03,359
or something else just because uh docker

00:44:01,520 --> 00:44:05,200
kubernetes or whatever

00:44:03,359 --> 00:44:08,960
container runtime you're using is gonna

00:44:05,200 --> 00:44:11,359
give you these abstractions for free

00:44:08,960 --> 00:44:12,960
for simpler and smaller application or

00:44:11,359 --> 00:44:16,480
maybe if you are just running on

00:44:12,960 --> 00:44:18,400
bear with virtual machines pm2

00:44:16,480 --> 00:44:19,520
i think it's kind of the easy way to get

00:44:18,400 --> 00:44:22,560
everything going and

00:44:19,520 --> 00:44:24,560
have metrics and add things like

00:44:22,560 --> 00:44:26,640
automatically respecting the application

00:44:24,560 --> 00:44:28,720
after a deployment

00:44:26,640 --> 00:44:30,319
so pm2 is a little bit more feature

00:44:28,720 --> 00:44:31,040
heavy than it is running your own

00:44:30,319 --> 00:44:34,319
cluster

00:44:31,040 --> 00:44:35,839
implementation so maybe the the the

00:44:34,319 --> 00:44:37,599
what i would do is for something very

00:44:35,839 --> 00:44:39,440
simple i will go with cluster

00:44:37,599 --> 00:44:41,599
if i want to have a bunch more features

00:44:39,440 --> 00:44:42,960
on top of that maybe pm2 you just

00:44:41,599 --> 00:44:44,880
install it and use it

00:44:42,960 --> 00:44:46,160
or if you have containers you're

00:44:44,880 --> 00:44:49,280
probably going to have this feature in

00:44:46,160 --> 00:44:51,040
your container on time

00:44:49,280 --> 00:44:52,400
right we're going to run out of time and

00:44:51,040 --> 00:44:54,800
we've got to make space for

00:44:52,400 --> 00:44:56,400
materials talk in a minute so i'm going

00:44:54,800 --> 00:44:58,240
to leave it there

00:44:56,400 --> 00:44:59,760
and uh thank you very much for all the

00:44:58,240 --> 00:45:01,920
hot tips there's so many deep

00:44:59,760 --> 00:45:02,800
and complex questions around uh scaling

00:45:01,920 --> 00:45:05,119
and performance

00:45:02,800 --> 00:45:06,640
and i think if you're around on the

00:45:05,119 --> 00:45:08,480
tables maybe people can uh

00:45:06,640 --> 00:45:10,640
find you there and ask you some more

00:45:08,480 --> 00:45:12,560
direct questions if that's all right

00:45:10,640 --> 00:45:14,720
sure thanks again for your talk and big

00:45:12,560 --> 00:45:16,480
clap all around from all over the world

00:45:14,720 --> 00:45:18,240
thank you for having me and very good to

00:45:16,480 --> 00:45:30,880
see you again

00:45:18,240 --> 00:45:30,880

YouTube URL: https://www.youtube.com/watch?v=LpHoTEw2vKg


