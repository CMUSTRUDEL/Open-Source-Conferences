Title: Simulating Network Conditions with JavaScript - Patrick Kunka - CityJS Conf 2020
Publication date: 2020-10-03
Playlist: CityJS Conf 2020
Description: 
	When working with video streaming, being able to accurately simulate customer network conditions for development and testing is crucial. In this talk, I'll be looking at how we can use containers and server-side JavaScript to programmatically shape network conditions and gain valuable insights about the behaviour and performance of our client-side JavaScript - from Adaptive Bitrate video players to entire single-page applications.

Bio
Principal Software Engineer at DAZN

_________________________________________________________________

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_________________________________________________________________

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:18,080 --> 00:00:21,910
[Music]

00:00:22,000 --> 00:00:24,640
hello ctjs conference and thanks for

00:00:23,840 --> 00:00:26,320
having me

00:00:24,640 --> 00:00:27,920
a bit about me to begin with my name is

00:00:26,320 --> 00:00:30,000
patrick kunker and i'm a principal

00:00:27,920 --> 00:00:31,840
software engineer at the zone which is a

00:00:30,000 --> 00:00:33,280
global sports streaming platform with

00:00:31,840 --> 00:00:35,680
our head offices

00:00:33,280 --> 00:00:37,520
here in london my background is in

00:00:35,680 --> 00:00:39,120
javascript and front-end web development

00:00:37,520 --> 00:00:40,879
i've spent the last six years or so

00:00:39,120 --> 00:00:42,879
specializing in video playback on the

00:00:40,879 --> 00:00:44,000
web and in this talk i'm going to be

00:00:42,879 --> 00:00:45,920
speaking about how how

00:00:44,000 --> 00:00:47,280
we're developing and testing video

00:00:45,920 --> 00:00:49,520
players at the zone using

00:00:47,280 --> 00:00:52,079
in-house javascript based network

00:00:49,520 --> 00:00:53,760
simulation tooling

00:00:52,079 --> 00:00:55,199
so you may already have some idea of how

00:00:53,760 --> 00:00:57,120
a modern video player works

00:00:55,199 --> 00:00:58,800
simply through observation when you're

00:00:57,120 --> 00:01:00,239
watching your favorite streaming service

00:00:58,800 --> 00:01:02,719
you might have seen things

00:01:00,239 --> 00:01:03,680
like an initial 10 seconds or so visibly

00:01:02,719 --> 00:01:05,840
compressed

00:01:03,680 --> 00:01:08,080
low quality video like this picture on

00:01:05,840 --> 00:01:10,320
the left before the player then jumps up

00:01:08,080 --> 00:01:12,080
to something more high definition

00:01:10,320 --> 00:01:14,240
and this behavior is what we call

00:01:12,080 --> 00:01:16,640
adaptive bitrate or abr

00:01:14,240 --> 00:01:18,320
video and the scenario i just described

00:01:16,640 --> 00:01:19,280
is not entirely necessary and can be

00:01:18,320 --> 00:01:20,479
mitigated

00:01:19,280 --> 00:01:22,560
but it's a good way to quickly

00:01:20,479 --> 00:01:24,720
illustrate the fundamentals

00:01:22,560 --> 00:01:27,200
an abr player has many different video

00:01:24,720 --> 00:01:29,360
and audio quality levels available to it

00:01:27,200 --> 00:01:30,640
each quality level is encoded at a

00:01:29,360 --> 00:01:32,240
different bitrate

00:01:30,640 --> 00:01:34,560
which means the amount of data that is

00:01:32,240 --> 00:01:35,920
used to represent one second of video

00:01:34,560 --> 00:01:37,759
and audio

00:01:35,920 --> 00:01:40,479
and this is a function of compression

00:01:37,759 --> 00:01:42,399
resolution and frame rate combined

00:01:40,479 --> 00:01:44,720
within each quality level the video and

00:01:42,399 --> 00:01:45,600
audio is then broken up into small files

00:01:44,720 --> 00:01:47,680
or segments

00:01:45,600 --> 00:01:49,680
each one typically only a few seconds

00:01:47,680 --> 00:01:51,840
long and this means that the file size

00:01:49,680 --> 00:01:54,560
for a segment of two seconds of video

00:01:51,840 --> 00:01:56,560
can vary from a few hundred sec a few

00:01:54,560 --> 00:01:58,320
hundred kilobytes at the lowest bit rate

00:01:56,560 --> 00:02:00,079
all the way up to a couple of megabytes

00:01:58,320 --> 00:02:02,159
for the highest bit rate

00:02:00,079 --> 00:02:04,240
and between these there may be ten or

00:02:02,159 --> 00:02:07,119
more quality profiles for the player to

00:02:04,240 --> 00:02:08,479
choose from at any point in time

00:02:07,119 --> 00:02:10,239
the choice of bit rates and

00:02:08,479 --> 00:02:11,680
chronologically ordered segments which

00:02:10,239 --> 00:02:13,440
are provided to the player via a

00:02:11,680 --> 00:02:15,040
text-based manifest file

00:02:13,440 --> 00:02:17,599
which is essentially a structured list

00:02:15,040 --> 00:02:18,640
of urls gives the player the ability to

00:02:17,599 --> 00:02:20,879
smoothly navigate

00:02:18,640 --> 00:02:22,160
varied and changing network conditions

00:02:20,879 --> 00:02:23,840
and pick the most appropriate

00:02:22,160 --> 00:02:25,520
quality profile available for the

00:02:23,840 --> 00:02:26,319
customer's internet connection at any

00:02:25,520 --> 00:02:27,680
time

00:02:26,319 --> 00:02:29,360
so if we could visualize a player's

00:02:27,680 --> 00:02:30,480
bandwidth estimation and its choice of

00:02:29,360 --> 00:02:32,319
quality profile

00:02:30,480 --> 00:02:33,920
we would see something like this chart

00:02:32,319 --> 00:02:35,840
showing the player gradually stepping

00:02:33,920 --> 00:02:38,319
down through quality profiles as the

00:02:35,840 --> 00:02:40,720
bandwidth deteriorates

00:02:38,319 --> 00:02:42,160
so generally speaking an abr player has

00:02:40,720 --> 00:02:45,040
four primary goals

00:02:42,160 --> 00:02:46,879
firstly start up quickly secondly

00:02:45,040 --> 00:02:47,840
maintain the highest quality possible

00:02:46,879 --> 00:02:50,720
given the current

00:02:47,840 --> 00:02:51,920
estimated bandwidth thirdly avoid

00:02:50,720 --> 00:02:54,720
rebuffering

00:02:51,920 --> 00:02:55,840
and finally recover from failures so the

00:02:54,720 --> 00:02:57,200
interesting part for me

00:02:55,840 --> 00:02:59,360
about working in the field of video

00:02:57,200 --> 00:03:01,120
player development is that these

00:02:59,360 --> 00:03:02,640
these first three goals are actually

00:03:01,120 --> 00:03:04,560
often conflicting

00:03:02,640 --> 00:03:06,720
so in order to create a great customer

00:03:04,560 --> 00:03:08,480
experience we must develop models and

00:03:06,720 --> 00:03:10,959
algorithms to understand how to make the

00:03:08,480 --> 00:03:12,640
correct trade-offs and sacrifices

00:03:10,959 --> 00:03:14,800
and also take the best decisions at the

00:03:12,640 --> 00:03:16,879
right moments if our estimates are

00:03:14,800 --> 00:03:18,319
inaccurate or if we react too late

00:03:16,879 --> 00:03:21,280
then the customer is likely to

00:03:18,319 --> 00:03:22,480
experience rebuffering mid playback

00:03:21,280 --> 00:03:24,319
it's a process that's highly

00:03:22,480 --> 00:03:26,480
asynchronous in nature and one that has

00:03:24,319 --> 00:03:28,560
to reliably function across a huge range

00:03:26,480 --> 00:03:30,319
of network conditions

00:03:28,560 --> 00:03:33,120
thanks to modern web standards such as

00:03:30,319 --> 00:03:34,319
html video and the newer media source

00:03:33,120 --> 00:03:36,159
extensions api

00:03:34,319 --> 00:03:37,920
this logic can now live entirely in

00:03:36,159 --> 00:03:40,239
javascript which is how i became

00:03:37,920 --> 00:03:42,080
involved in this area

00:03:40,239 --> 00:03:44,080
in order therefore to build reliable

00:03:42,080 --> 00:03:46,000
testable behavior in our video players

00:03:44,080 --> 00:03:47,840
we need ways to deterministically

00:03:46,000 --> 00:03:49,599
simulate every aspect of a real

00:03:47,840 --> 00:03:51,440
customer's internet connection

00:03:49,599 --> 00:03:53,840
and assert that a player behaves as

00:03:51,440 --> 00:03:55,439
expected every time

00:03:53,840 --> 00:03:57,280
so you'll most likely be familiar with

00:03:55,439 --> 00:03:59,360
network throttling options

00:03:57,280 --> 00:04:00,879
such as chrome dev tools or software

00:03:59,360 --> 00:04:02,400
like child proxy

00:04:00,879 --> 00:04:04,000
perhaps you've even experimented with

00:04:02,400 --> 00:04:05,360
the low-level linux traffic control

00:04:04,000 --> 00:04:06,959
protocol

00:04:05,360 --> 00:04:09,120
all of these are highly valid and

00:04:06,959 --> 00:04:11,040
valuable and and often very powerful

00:04:09,120 --> 00:04:11,840
tools but many have shortcomings when it

00:04:11,040 --> 00:04:14,159
comes to

00:04:11,840 --> 00:04:14,959
creating a holistic playback testing

00:04:14,159 --> 00:04:16,639
stack

00:04:14,959 --> 00:04:18,320
so in this presentation i'm going to

00:04:16,639 --> 00:04:19,680
talk about how my team at design built

00:04:18,320 --> 00:04:22,720
our own network emulation

00:04:19,680 --> 00:04:24,800
testing tool using typescript and dino

00:04:22,720 --> 00:04:28,080
that can be run locally on any laptop or

00:04:24,800 --> 00:04:30,400
remotely in the cloud via a ci pipeline

00:04:28,080 --> 00:04:31,759
so firstly why did we want to do that

00:04:30,400 --> 00:04:33,759
well we wanted something that was

00:04:31,759 --> 00:04:35,280
platform agnostic to begin with

00:04:33,759 --> 00:04:36,800
chrome's throttling options are great

00:04:35,280 --> 00:04:39,280
but not much use if you want to test

00:04:36,800 --> 00:04:41,600
playback in safari or on a smart tv with

00:04:39,280 --> 00:04:43,600
an oem embedded browser

00:04:41,600 --> 00:04:45,440
secondly we wanted something that could

00:04:43,600 --> 00:04:47,520
run either on our local machines

00:04:45,440 --> 00:04:49,280
during development for testing or just

00:04:47,520 --> 00:04:51,199
hacking around with new ideas

00:04:49,280 --> 00:04:52,479
but also remotely by a continuous

00:04:51,199 --> 00:04:55,040
integration pipeline

00:04:52,479 --> 00:04:56,880
and at scale ideally running hundreds of

00:04:55,040 --> 00:04:58,400
simulations in parallel as quickly as

00:04:56,880 --> 00:05:00,320
possible

00:04:58,400 --> 00:05:02,000
finally we wanted to democratize the

00:05:00,320 --> 00:05:03,280
tool and encourage contribution from

00:05:02,000 --> 00:05:05,360
across the team

00:05:03,280 --> 00:05:07,280
the design playback team is primarily a

00:05:05,360 --> 00:05:08,240
team of javascript and web front-end

00:05:07,280 --> 00:05:10,320
specialists

00:05:08,240 --> 00:05:12,880
so anything requiring physical hardware

00:05:10,320 --> 00:05:14,400
complex setup or low-level native code

00:05:12,880 --> 00:05:16,400
was to be avoided to prevent a

00:05:14,400 --> 00:05:18,160
reluctance with uptake and contribution

00:05:16,400 --> 00:05:20,479
that we'd encountered the past in the

00:05:18,160 --> 00:05:20,479
past

00:05:21,199 --> 00:05:25,199
so what did we want our tool to do one

00:05:24,000 --> 00:05:26,880
of the most important goals of the

00:05:25,199 --> 00:05:28,080
project as i mentioned earlier was to

00:05:26,880 --> 00:05:30,240
create something as close to

00:05:28,080 --> 00:05:32,240
deterministic as possible

00:05:30,240 --> 00:05:33,520
so when we test using chrome devtools or

00:05:32,240 --> 00:05:35,039
charles proxy

00:05:33,520 --> 00:05:36,960
we typically used to test against

00:05:35,039 --> 00:05:39,440
specialized test content downloaded from

00:05:36,960 --> 00:05:41,280
a real cdn on the internet

00:05:39,440 --> 00:05:42,960
these tests are useful but they can be

00:05:41,280 --> 00:05:45,680
unreliable because they're susceptible

00:05:42,960 --> 00:05:47,199
to influence from external processes

00:05:45,680 --> 00:05:49,120
such as sudden changes in network

00:05:47,199 --> 00:05:50,720
conditions due to competing traffic on

00:05:49,120 --> 00:05:52,800
the same network

00:05:50,720 --> 00:05:54,960
or a background system software update

00:05:52,800 --> 00:05:56,240
kicking off in the middle of a test

00:05:54,960 --> 00:05:57,280
so we realized that if we could remove

00:05:56,240 --> 00:05:58,880
the internet from the equation

00:05:57,280 --> 00:06:00,720
completely we should in theory be able

00:05:58,880 --> 00:06:02,720
to run the same test over and over with

00:06:00,720 --> 00:06:04,400
the same results

00:06:02,720 --> 00:06:06,000
another goal was to be as granular as

00:06:04,400 --> 00:06:06,639
possible in what the simulation could

00:06:06,000 --> 00:06:08,400
affect

00:06:06,639 --> 00:06:09,759
reliable bandwidth simulation or

00:06:08,400 --> 00:06:10,560
throttling is probably the most

00:06:09,759 --> 00:06:12,720
important

00:06:10,560 --> 00:06:14,160
and most obvious feature of such a tool

00:06:12,720 --> 00:06:14,960
but we wanted a tool that could also do

00:06:14,160 --> 00:06:16,400
a lot more

00:06:14,960 --> 00:06:18,240
and really simulate anything that the

00:06:16,400 --> 00:06:21,440
real world could throw at us

00:06:18,240 --> 00:06:23,039
i've got a few examples listed down here

00:06:21,440 --> 00:06:24,880
finally we wanted a tool that was

00:06:23,039 --> 00:06:27,280
effortless to use by testers and

00:06:24,880 --> 00:06:27,919
software engineers alike so controlled

00:06:27,280 --> 00:06:31,280
either by

00:06:27,919 --> 00:06:35,120
very easy to build no code json test

00:06:31,280 --> 00:06:37,120
test files or manually by a simple ui

00:06:35,120 --> 00:06:38,560
and as easy as possible to develop for

00:06:37,120 --> 00:06:40,479
and to add features to

00:06:38,560 --> 00:06:42,479
meaning a familiar stack and tooling for

00:06:40,479 --> 00:06:45,039
javascript developers

00:06:42,479 --> 00:06:46,639
so let's take a look at the stack you'll

00:06:45,039 --> 00:06:47,360
most likely be familiar with most of the

00:06:46,639 --> 00:06:49,280
software

00:06:47,360 --> 00:06:50,960
shown here so i'm going to be focusing

00:06:49,280 --> 00:06:52,319
on those probably at least familiar to

00:06:50,960 --> 00:06:54,240
front-end developers

00:06:52,319 --> 00:06:56,319
those being dino and the javascript

00:06:54,240 --> 00:06:58,319
streams api but i'll talk a bit about

00:06:56,319 --> 00:07:00,880
how we're using containers first

00:06:58,319 --> 00:07:02,000
so docker compose or amazon ecs are

00:07:00,880 --> 00:07:03,759
container orchestration

00:07:02,000 --> 00:07:06,160
tools which facilitate the running of

00:07:03,759 --> 00:07:08,080
multiple docker containers or services

00:07:06,160 --> 00:07:09,759
and enable communication between them

00:07:08,080 --> 00:07:11,199
via a virtual network

00:07:09,759 --> 00:07:13,280
you can hopefully already see where this

00:07:11,199 --> 00:07:15,039
is going so the initial concept was that

00:07:13,280 --> 00:07:16,800
by using docker compose locally or

00:07:15,039 --> 00:07:18,560
amazon ecs in the cloud

00:07:16,800 --> 00:07:20,400
we could create different services with

00:07:18,560 --> 00:07:22,160
very specific goals and have them talk

00:07:20,400 --> 00:07:23,120
to each other in a way that would mimic

00:07:22,160 --> 00:07:26,080
a client

00:07:23,120 --> 00:07:26,960
a player as if you were talking to a cdn

00:07:26,080 --> 00:07:29,440
with the internet

00:07:26,960 --> 00:07:31,039
in between dino was something that we

00:07:29,440 --> 00:07:32,960
added to the project fairly recently

00:07:31,039 --> 00:07:34,160
replacing a lot of node.js code in the

00:07:32,960 --> 00:07:36,000
process

00:07:34,160 --> 00:07:37,680
um if you haven't heard of it before

00:07:36,000 --> 00:07:38,880
dino is a new typescript native

00:07:37,680 --> 00:07:40,639
server-side runtime

00:07:38,880 --> 00:07:42,560
from ryan dahl the original creator of

00:07:40,639 --> 00:07:44,160
node.js

00:07:42,560 --> 00:07:45,759
and amongst many other great features it

00:07:44,160 --> 00:07:47,280
comes with a powerful standard library

00:07:45,759 --> 00:07:48,479
that actually contain almost everything

00:07:47,280 --> 00:07:50,160
we needed for this project

00:07:48,479 --> 00:07:52,720
out of the box without having to import

00:07:50,160 --> 00:07:54,400
any third-party modules

00:07:52,720 --> 00:07:56,319
another great feature for this project

00:07:54,400 --> 00:07:58,000
is its optimized approach to handling

00:07:56,319 --> 00:08:00,080
back pressure in streams which we'll

00:07:58,000 --> 00:08:02,319
come to later on

00:08:00,080 --> 00:08:03,919
so let's start with a high level view of

00:08:02,319 --> 00:08:05,440
the architecture

00:08:03,919 --> 00:08:07,360
within our virtual local network we have

00:08:05,440 --> 00:08:08,720
a number of services communicating via

00:08:07,360 --> 00:08:10,160
http

00:08:08,720 --> 00:08:12,240
firstly on the left we have our player

00:08:10,160 --> 00:08:14,720
under test it exists with an h

00:08:12,240 --> 00:08:16,160
within an html test harness loaded into

00:08:14,720 --> 00:08:18,160
a headless browser and driven by

00:08:16,160 --> 00:08:20,800
selenium and a custom javascript

00:08:18,160 --> 00:08:21,280
instrumentation layer which essentially

00:08:20,800 --> 00:08:24,000
um

00:08:21,280 --> 00:08:24,560
controls the player calling play pause

00:08:24,000 --> 00:08:27,520
seek

00:08:24,560 --> 00:08:29,199
rewind other public public api methods

00:08:27,520 --> 00:08:30,000
and at the same time collects metrics

00:08:29,199 --> 00:08:31,440
back

00:08:30,000 --> 00:08:32,640
so this service is known as the runner

00:08:31,440 --> 00:08:34,320
and it's also responsible for

00:08:32,640 --> 00:08:36,640
orchestrating tests and returning a

00:08:34,320 --> 00:08:38,320
final pass or fail based on evaluation

00:08:36,640 --> 00:08:40,240
of the events and metrics collected from

00:08:38,320 --> 00:08:41,919
the player during the test

00:08:40,240 --> 00:08:43,599
on the right over here we have our file

00:08:41,919 --> 00:08:45,600
server which is a simple static file

00:08:43,599 --> 00:08:47,519
server with access to a mounted volume

00:08:45,600 --> 00:08:49,120
containing our video and audio segments

00:08:47,519 --> 00:08:50,640
and manifests

00:08:49,120 --> 00:08:52,160
given the huge choice of technology that

00:08:50,640 --> 00:08:53,680
docker facilitates we really could have

00:08:52,160 --> 00:08:55,680
used anything for this such as a single

00:08:53,680 --> 00:08:57,680
line of python but we opted for dino's

00:08:55,680 --> 00:08:59,519
standard library http server

00:08:57,680 --> 00:09:01,040
with the aim of a typescript everywhere

00:08:59,519 --> 00:09:03,200
approach

00:09:01,040 --> 00:09:04,080
the proxy service in the middle again

00:09:03,200 --> 00:09:05,839
built with dino

00:09:04,080 --> 00:09:07,600
sits between the player and the file

00:09:05,839 --> 00:09:08,880
server and can be thought of as another

00:09:07,600 --> 00:09:10,560
http server

00:09:08,880 --> 00:09:12,160
routing all requests from the player

00:09:10,560 --> 00:09:13,920
straight through to the file server

00:09:12,160 --> 00:09:15,440
while throttling throughput or affecting

00:09:13,920 --> 00:09:17,600
network behavior in some way

00:09:15,440 --> 00:09:18,880
as data is streamed through it i'll be

00:09:17,600 --> 00:09:20,720
taking you through the implementation

00:09:18,880 --> 00:09:22,560
here in some detail later on

00:09:20,720 --> 00:09:23,839
but suffice to say having our bandwidth

00:09:22,560 --> 00:09:26,240
throttling codes sit within a

00:09:23,839 --> 00:09:27,839
typescript-based http server made it

00:09:26,240 --> 00:09:28,640
pretty trivial to extend the tool with

00:09:27,839 --> 00:09:30,480
other features

00:09:28,640 --> 00:09:32,959
like latency simulation and error

00:09:30,480 --> 00:09:34,560
simulation later on

00:09:32,959 --> 00:09:36,160
so let's take a look at the request

00:09:34,560 --> 00:09:38,160
lifecycle of an http

00:09:36,160 --> 00:09:39,920
request coming from the player within

00:09:38,160 --> 00:09:41,839
our docker compose or ecs network

00:09:39,920 --> 00:09:43,920
each service has its own address or port

00:09:41,839 --> 00:09:45,920
on localhost for example the file server

00:09:43,920 --> 00:09:47,680
could be located at port 3000

00:09:45,920 --> 00:09:49,040
and the proxy could be located at port

00:09:47,680 --> 00:09:50,720
00:09:49,040 --> 00:09:52,480
in other words any file on the file

00:09:50,720 --> 00:09:53,920
server will be available at the same

00:09:52,480 --> 00:09:55,600
path name on either port

00:09:53,920 --> 00:09:57,360
but the behavior of how that request is

00:09:55,600 --> 00:09:59,760
fulfilled will be different

00:09:57,360 --> 00:10:01,600
so when a request is made via the proxy

00:09:59,760 --> 00:10:03,680
we simply need to add the intermediate

00:10:01,600 --> 00:10:05,920
step of a server side fetch request

00:10:03,680 --> 00:10:07,760
from the proxy to the file server shown

00:10:05,920 --> 00:10:09,839
here with the dotted line

00:10:07,760 --> 00:10:11,440
and stream the response directly back

00:10:09,839 --> 00:10:12,560
into the pending client requests from

00:10:11,440 --> 00:10:14,399
the player

00:10:12,560 --> 00:10:17,279
we now have everything in place to begin

00:10:14,399 --> 00:10:19,040
plugging in our network simulation code

00:10:17,279 --> 00:10:20,320
so how do we build a bandwidth throttle

00:10:19,040 --> 00:10:22,880
in javascript

00:10:20,320 --> 00:10:24,640
firstly our throttle must sit within the

00:10:22,880 --> 00:10:25,600
proxy and ensure that the speed at which

00:10:24,640 --> 00:10:28,560
packets can flow

00:10:25,600 --> 00:10:30,480
out of it is rate limited and any chunks

00:10:28,560 --> 00:10:32,480
of data arriving from the file server

00:10:30,480 --> 00:10:34,240
faster than that rate are buffered up in

00:10:32,480 --> 00:10:35,440
memory and then released at a steady

00:10:34,240 --> 00:10:36,640
constant rate

00:10:35,440 --> 00:10:39,760
so this is where we get into the

00:10:36,640 --> 00:10:42,160
javascript javascript streams api

00:10:39,760 --> 00:10:44,000
a stream in dino node.js and modern

00:10:42,160 --> 00:10:45,920
browsers is an interface for dealing

00:10:44,000 --> 00:10:48,000
with asynchronous data transfer

00:10:45,920 --> 00:10:49,839
typically large amounts of data

00:10:48,000 --> 00:10:51,600
the most common examples would be io

00:10:49,839 --> 00:10:52,560
reading or writing to disk and network

00:10:51,600 --> 00:10:54,480
requests

00:10:52,560 --> 00:10:56,240
a stream allows us to break up a large

00:10:54,480 --> 00:10:58,240
resource such as a network request or

00:10:56,240 --> 00:10:58,959
file read into small chunks of binary

00:10:58,240 --> 00:11:00,640
data

00:10:58,959 --> 00:11:02,560
and handle the process handle and

00:11:00,640 --> 00:11:04,079
process them one by one

00:11:02,560 --> 00:11:06,000
rather than having to wait for all data

00:11:04,079 --> 00:11:07,519
to arrive before processing it

00:11:06,000 --> 00:11:09,680
monolithically

00:11:07,519 --> 00:11:11,760
this allows for granular immediate and

00:11:09,680 --> 00:11:12,720
regular data processing and is much more

00:11:11,760 --> 00:11:14,640
memory efficient

00:11:12,720 --> 00:11:16,399
particularly in the case of large files

00:11:14,640 --> 00:11:18,399
as memory allocation in the javascript

00:11:16,399 --> 00:11:20,079
layer is kept to a minimum

00:11:18,399 --> 00:11:22,800
the streams api should not be confused

00:11:20,079 --> 00:11:24,720
with streaming video or rxjs observable

00:11:22,800 --> 00:11:27,120
streams streams are simply chainable

00:11:24,720 --> 00:11:28,160
api services which can be piped together

00:11:27,120 --> 00:11:31,360
to efficiently move

00:11:28,160 --> 00:11:33,600
binary data between inputs and outputs

00:11:31,360 --> 00:11:35,360
i've got a couple of api examples here

00:11:33,600 --> 00:11:36,959
so in the first one data is streamed

00:11:35,360 --> 00:11:39,120
from a readable input stream

00:11:36,959 --> 00:11:41,279
into a writable output stream using the

00:11:39,120 --> 00:11:42,880
readable stream's pipe2 method

00:11:41,279 --> 00:11:44,959
which is a means of connecting multiple

00:11:42,880 --> 00:11:46,560
stream implementations together

00:11:44,959 --> 00:11:48,720
once all data has been written the

00:11:46,560 --> 00:11:50,639
returned promise is resolved

00:11:48,720 --> 00:11:52,399
in the second example we chain in an

00:11:50,639 --> 00:11:54,079
additional processing step

00:11:52,399 --> 00:11:55,920
using the pipe through method which

00:11:54,079 --> 00:11:57,920
returns another readable stream which

00:11:55,920 --> 00:12:00,480
can again be piped into a writable

00:11:57,920 --> 00:12:02,560
output the distinction here between pipe

00:12:00,480 --> 00:12:04,079
two and pipe through is necessary due to

00:12:02,560 --> 00:12:05,920
the fact that one is final

00:12:04,079 --> 00:12:08,800
and returns a promise while the other is

00:12:05,920 --> 00:12:10,160
chainable and returns another stream

00:12:08,800 --> 00:12:12,000
so let's take a look at what our

00:12:10,160 --> 00:12:13,839
throttle will look like implemented with

00:12:12,000 --> 00:12:15,760
the streams api

00:12:13,839 --> 00:12:17,040
firstly the what working group streams

00:12:15,760 --> 00:12:19,680
api standard defines

00:12:17,040 --> 00:12:20,160
three types of streams firstly readable

00:12:19,680 --> 00:12:22,720
streams

00:12:20,160 --> 00:12:24,079
streams from which data can be read

00:12:22,720 --> 00:12:25,920
writable streams streams

00:12:24,079 --> 00:12:27,519
to which data can be written and

00:12:25,920 --> 00:12:29,040
transform streams which are both

00:12:27,519 --> 00:12:30,399
writable and readable and which can

00:12:29,040 --> 00:12:32,480
manipulate the data pass

00:12:30,399 --> 00:12:34,720
through them it's worth noting here that

00:12:32,480 --> 00:12:36,560
node.js also implements a very similar

00:12:34,720 --> 00:12:37,760
but older and non-standard version of

00:12:36,560 --> 00:12:39,279
the above which you'll probably be

00:12:37,760 --> 00:12:41,600
familiar with if you've ever worked with

00:12:39,279 --> 00:12:43,600
server side javascript before

00:12:41,600 --> 00:12:45,680
so in the case of our dino proxy the

00:12:43,600 --> 00:12:46,800
server side http request gives us a

00:12:45,680 --> 00:12:49,519
readable stream

00:12:46,800 --> 00:12:50,800
and the client http response gives us a

00:12:49,519 --> 00:12:52,480
writable stream

00:12:50,800 --> 00:12:54,880
our throttle will sit between these two

00:12:52,480 --> 00:12:55,760
existing streams making it both writable

00:12:54,880 --> 00:12:57,519
and readable

00:12:55,760 --> 00:12:59,200
and will be implemented therefore as a

00:12:57,519 --> 00:13:00,560
transformed stream

00:12:59,200 --> 00:13:02,399
one of the many brilliant design

00:13:00,560 --> 00:13:03,839
decisions taken by the dino team was

00:13:02,399 --> 00:13:06,000
their decision to implement standard

00:13:03,839 --> 00:13:08,560
space web apis wherever possible

00:13:06,000 --> 00:13:10,320
over proprietary ones because of this we

00:13:08,560 --> 00:13:11,440
get native access to things like fetch

00:13:10,320 --> 00:13:14,240
and the streams api

00:13:11,440 --> 00:13:15,519
on the server for free so as per the

00:13:14,240 --> 00:13:17,519
stream's api standard

00:13:15,519 --> 00:13:19,519
dino exposes a transform stream

00:13:17,519 --> 00:13:21,440
constructor in the global namespace

00:13:19,519 --> 00:13:23,519
that we can instantiate with custom

00:13:21,440 --> 00:13:25,120
transform and flush callbacks in which

00:13:23,519 --> 00:13:26,639
our throttle's business logic can be

00:13:25,120 --> 00:13:29,760
implemented

00:13:26,639 --> 00:13:31,600
so firstly the transform callback is

00:13:29,760 --> 00:13:33,360
invoked by the platform whenever an

00:13:31,600 --> 00:13:34,079
individual chunk of data arrives from

00:13:33,360 --> 00:13:36,240
the underlying

00:13:34,079 --> 00:13:38,160
readable input stream giving us the

00:13:36,240 --> 00:13:39,760
ability to process or manipulate that

00:13:38,160 --> 00:13:41,760
data in some way

00:13:39,760 --> 00:13:43,279
the platform will only send more data

00:13:41,760 --> 00:13:44,480
once we signal that we are done with the

00:13:43,279 --> 00:13:46,639
transform step

00:13:44,480 --> 00:13:47,920
this is achieved either by instantly

00:13:46,639 --> 00:13:49,279
returning void which would be

00:13:47,920 --> 00:13:51,199
synchronous processing

00:13:49,279 --> 00:13:52,959
or returning a promise which is resolved

00:13:51,199 --> 00:13:54,639
after processing is complete

00:13:52,959 --> 00:13:55,680
which would be asynchronous processing

00:13:54,639 --> 00:13:57,519
and that's what i've shown in this

00:13:55,680 --> 00:13:59,199
example here

00:13:57,519 --> 00:14:01,120
the amount of data contained within a

00:13:59,199 --> 00:14:03,040
chunk is arbitrary and arrives as a

00:14:01,120 --> 00:14:05,920
typed array of binary data that can be

00:14:03,040 --> 00:14:07,839
iterated over and processed as needed

00:14:05,920 --> 00:14:09,600
so via these two callbacks the platform

00:14:07,839 --> 00:14:11,440
also provides us with a reference to a

00:14:09,600 --> 00:14:13,360
controller object which is really just

00:14:11,440 --> 00:14:15,279
an api surface which we can

00:14:13,360 --> 00:14:17,199
write our output to once we're done

00:14:15,279 --> 00:14:19,120
processing using the controller's

00:14:17,199 --> 00:14:21,199
enqueue method

00:14:19,120 --> 00:14:23,040
once all coming incoming data has been

00:14:21,199 --> 00:14:23,839
pushed into the transform stream by the

00:14:23,040 --> 00:14:26,000
platform

00:14:23,839 --> 00:14:27,360
the flush callback will be invoked and

00:14:26,000 --> 00:14:29,279
this gives our implementation the

00:14:27,360 --> 00:14:30,959
opportunity to tell the platform to wait

00:14:29,279 --> 00:14:33,519
until we've finished processing

00:14:30,959 --> 00:14:34,000
and any cleanup steps before destroying

00:14:33,519 --> 00:14:36,399
the stream

00:14:34,000 --> 00:14:38,000
and releasing memory we do this again by

00:14:36,399 --> 00:14:39,360
returning a promise and resolving it

00:14:38,000 --> 00:14:40,880
when we're done

00:14:39,360 --> 00:14:42,320
in the case of our throttle we don't

00:14:40,880 --> 00:14:44,160
actually want to limit the rate at which

00:14:42,320 --> 00:14:46,079
data is written into the throttle only

00:14:44,160 --> 00:14:48,240
the rate that is read out

00:14:46,079 --> 00:14:50,079
therefore all transfer on callbacks will

00:14:48,240 --> 00:14:51,120
need to behave synchronously and return

00:14:50,079 --> 00:14:52,880
immediately

00:14:51,120 --> 00:14:54,399
as incoming data builds up in the

00:14:52,880 --> 00:14:56,800
throttle's internal buffer

00:14:54,399 --> 00:14:58,320
we'll use a simple tiner timer to push

00:14:56,800 --> 00:15:00,720
data back out to a reader

00:14:58,320 --> 00:15:02,560
at a configurable maximum rate via the

00:15:00,720 --> 00:15:04,240
enqueue method

00:15:02,560 --> 00:15:05,839
it's worth noting that during this time

00:15:04,240 --> 00:15:07,040
data will be accumulating within the

00:15:05,839 --> 00:15:09,199
javascript memory

00:15:07,040 --> 00:15:11,680
known as back pressure where data is

00:15:09,199 --> 00:15:13,120
coming in faster than we can push it out

00:15:11,680 --> 00:15:14,800
and it must be handled carefully to

00:15:13,120 --> 00:15:16,800
avoid performance bottlenecks

00:15:14,800 --> 00:15:17,839
javascript's typed arrays can help here

00:15:16,800 --> 00:15:19,920
giving us efficient

00:15:17,839 --> 00:15:21,600
write and read access to pre-allocated

00:15:19,920 --> 00:15:23,279
raw memory

00:15:21,600 --> 00:15:24,800
so let's take a quick look at a

00:15:23,279 --> 00:15:27,120
simplified

00:15:24,800 --> 00:15:29,279
implementation of a throttle implemented

00:15:27,120 --> 00:15:30,800
as a transform stream

00:15:29,279 --> 00:15:32,560
we have on the left here factory

00:15:30,800 --> 00:15:34,240
function create throttle which returns

00:15:32,560 --> 00:15:35,839
an implementation of a transformed

00:15:34,240 --> 00:15:37,600
stream

00:15:35,839 --> 00:15:39,120
firstly we have some parameters a

00:15:37,600 --> 00:15:41,279
content length

00:15:39,120 --> 00:15:43,120
parameter which is the number of bytes

00:15:41,279 --> 00:15:46,720
which we expect

00:15:43,120 --> 00:15:47,120
our request to be and we're going to use

00:15:46,720 --> 00:15:49,600
that

00:15:47,120 --> 00:15:50,880
to allocate all necessary raw memory up

00:15:49,600 --> 00:15:52,480
front

00:15:50,880 --> 00:15:53,920
and we also have a couple of optional

00:15:52,480 --> 00:15:55,680
parameters which are going to define

00:15:53,920 --> 00:15:56,880
basically what our throttling rate is

00:15:55,680 --> 00:15:59,600
and that's and that's done as a

00:15:56,880 --> 00:16:01,199
combination of the processing interval

00:15:59,600 --> 00:16:02,720
which is how frequently we want to write

00:16:01,199 --> 00:16:04,800
data out of the proxy

00:16:02,720 --> 00:16:06,399
and the number of bytes per interval to

00:16:04,800 --> 00:16:06,880
be pushed out which is basically the

00:16:06,399 --> 00:16:08,160
rate

00:16:06,880 --> 00:16:09,920
and you can see that defaults to

00:16:08,160 --> 00:16:11,680
infinity meaning that by default

00:16:09,920 --> 00:16:13,360
no throttling is applied data is just

00:16:11,680 --> 00:16:16,000
going to be allowed through

00:16:13,360 --> 00:16:17,040
unlimited so then we set up a few

00:16:16,000 --> 00:16:18,399
variables

00:16:17,040 --> 00:16:20,000
our buffer which is probably the most

00:16:18,399 --> 00:16:21,360
important thing where we're going to

00:16:20,000 --> 00:16:22,959
store the

00:16:21,360 --> 00:16:24,399
the data that's building up and we're

00:16:22,959 --> 00:16:26,720
going to use a typed array for that and

00:16:24,399 --> 00:16:28,320
the content make the parameter again

00:16:26,720 --> 00:16:30,320
we're using that to define how much

00:16:28,320 --> 00:16:32,000
memory we want to allocate

00:16:30,320 --> 00:16:33,839
we have a deferred promise called done

00:16:32,000 --> 00:16:36,320
which is going to allow us to affect the

00:16:33,839 --> 00:16:39,040
control flow of that um

00:16:36,320 --> 00:16:40,320
that flush callback outside of the

00:16:39,040 --> 00:16:41,759
callback

00:16:40,320 --> 00:16:43,120
we have a reference to the controller

00:16:41,759 --> 00:16:44,720
which we're going to hoist the

00:16:43,120 --> 00:16:46,240
controller into later and then we have a

00:16:44,720 --> 00:16:47,680
couple of indexes which are going to

00:16:46,240 --> 00:16:49,279
allow us to keep track

00:16:47,680 --> 00:16:51,600
of where we're writing to the buffer and

00:16:49,279 --> 00:16:54,480
when we're reading from it

00:16:51,600 --> 00:16:56,000
the most the second most important thing

00:16:54,480 --> 00:16:57,120
of this implementation is probably the

00:16:56,000 --> 00:16:58,800
processing loop

00:16:57,120 --> 00:17:00,320
and i've extracted the process function

00:16:58,800 --> 00:17:01,839
out to the right there so that we can

00:17:00,320 --> 00:17:03,759
focus on it individually

00:17:01,839 --> 00:17:04,880
but essentially we have an interval a

00:17:03,759 --> 00:17:07,039
timer there that's

00:17:04,880 --> 00:17:08,720
that's running at that process interval

00:17:07,039 --> 00:17:10,480
and it's calling that process function

00:17:08,720 --> 00:17:12,079
every time

00:17:10,480 --> 00:17:13,280
we then have the transform callback

00:17:12,079 --> 00:17:14,959
which is going to we're going to use

00:17:13,280 --> 00:17:15,919
that to receive chunks and push them

00:17:14,959 --> 00:17:17,360
into the buffer

00:17:15,919 --> 00:17:18,640
and it also gives us access to that

00:17:17,360 --> 00:17:20,400
controller reference which we're going

00:17:18,640 --> 00:17:21,839
to hoist into the parent scope

00:17:20,400 --> 00:17:24,079
and each time we push data into the

00:17:21,839 --> 00:17:25,439
buffer we increment the right index

00:17:24,079 --> 00:17:27,679
to make sure that the next chunk is

00:17:25,439 --> 00:17:29,440
written into the right place

00:17:27,679 --> 00:17:30,799
finally we have our flush function and

00:17:29,440 --> 00:17:32,000
as i mentioned before we're using that

00:17:30,799 --> 00:17:34,000
deferred promise there to

00:17:32,000 --> 00:17:35,440
just have the platform wait until all

00:17:34,000 --> 00:17:38,160
data has been pushed out

00:17:35,440 --> 00:17:39,919
before we return and once that happens

00:17:38,160 --> 00:17:40,960
we also clear the interval and stop that

00:17:39,919 --> 00:17:42,880
timer

00:17:40,960 --> 00:17:44,160
finally we return the implementation of

00:17:42,880 --> 00:17:47,120
the transform stream

00:17:44,160 --> 00:17:48,880
with the callbacks provided so looking

00:17:47,120 --> 00:17:50,400
at the process function now on the right

00:17:48,880 --> 00:17:52,320
um the first thing we want to do is

00:17:50,400 --> 00:17:53,360
check whether all data has been written

00:17:52,320 --> 00:17:56,080
or processed

00:17:53,360 --> 00:17:56,720
um if it has we can resolve the deferred

00:17:56,080 --> 00:18:00,000
promise and

00:17:56,720 --> 00:18:02,799
end otherwise we need to take a chunk

00:18:00,000 --> 00:18:04,400
out of the buffer based on the defined

00:18:02,799 --> 00:18:06,160
rates the bytes per interval

00:18:04,400 --> 00:18:07,840
so we use the start read index and then

00:18:06,160 --> 00:18:09,039
we create an end read index using that

00:18:07,840 --> 00:18:11,280
bytes principle

00:18:09,039 --> 00:18:13,200
we take that chunk and then we push it

00:18:11,280 --> 00:18:15,039
out using the enqueue method and again

00:18:13,200 --> 00:18:18,640
incrementing that read index so the next

00:18:15,039 --> 00:18:21,679
read is taken from the correct position

00:18:18,640 --> 00:18:23,440
so um we can now take a look at how the

00:18:21,679 --> 00:18:25,760
throttle would fit into the life cycle

00:18:23,440 --> 00:18:27,039
of an incoming http request in our dna

00:18:25,760 --> 00:18:28,160
proxy

00:18:27,039 --> 00:18:29,840
the first thing to note is how

00:18:28,160 --> 00:18:31,280
minimalist the api of dino's standard

00:18:29,840 --> 00:18:33,120
library server is

00:18:31,280 --> 00:18:34,880
the server instance is implemented as an

00:18:33,120 --> 00:18:36,240
async iterable meaning that incoming

00:18:34,880 --> 00:18:37,600
requests can be handled without having

00:18:36,240 --> 00:18:39,760
to add a single event listener or

00:18:37,600 --> 00:18:41,600
callback which is really nice

00:18:39,760 --> 00:18:43,280
we firstly take the url of our incoming

00:18:41,600 --> 00:18:44,080
request and redirect it to the file

00:18:43,280 --> 00:18:45,840
server

00:18:44,080 --> 00:18:47,440
we then kick off a server side fetch

00:18:45,840 --> 00:18:49,679
request which gives us access

00:18:47,440 --> 00:18:51,440
to that readable body stream and also

00:18:49,679 --> 00:18:54,080
headers and status code if we want to

00:18:51,440 --> 00:18:55,679
forward them back to the client

00:18:54,080 --> 00:18:57,120
using that content length header we get

00:18:55,679 --> 00:18:58,559
that content length in bytes which we're

00:18:57,120 --> 00:18:59,360
going to use to create our throttle for

00:18:58,559 --> 00:19:02,320
the request

00:18:59,360 --> 00:19:03,520
and allocate that raw memory we can then

00:19:02,320 --> 00:19:05,600
um

00:19:03,520 --> 00:19:07,200
basically pipe the the body from the

00:19:05,600 --> 00:19:08,880
fetch request through the throttle and

00:19:07,200 --> 00:19:10,080
out into a reader and the reader is just

00:19:08,880 --> 00:19:12,480
an interface

00:19:10,080 --> 00:19:14,720
that allows us to provide that that

00:19:12,480 --> 00:19:16,080
stream back to the request response

00:19:14,720 --> 00:19:16,640
method which we can see at the bottom

00:19:16,080 --> 00:19:18,880
here

00:19:16,640 --> 00:19:21,039
so we'll respond to the request with the

00:19:18,880 --> 00:19:23,120
throttle reader which is the

00:19:21,039 --> 00:19:26,320
stream and we're forwarding on the

00:19:23,120 --> 00:19:29,039
status code and the headers

00:19:26,320 --> 00:19:30,240
so that's a pretty simple invitation um

00:19:29,039 --> 00:19:32,400
it works well for

00:19:30,240 --> 00:19:34,240
consecutive non-overlapping requests but

00:19:32,400 --> 00:19:35,200
in order to accurately simulate the real

00:19:34,240 --> 00:19:37,360
world where a client

00:19:35,200 --> 00:19:39,200
might actually make multiple overlapping

00:19:37,360 --> 00:19:41,440
http requests in parallel

00:19:39,200 --> 00:19:43,360
and we need to therefore correctly

00:19:41,440 --> 00:19:46,400
distribute the bandwidth between them

00:19:43,360 --> 00:19:48,559
all quite a lot of additional complexity

00:19:46,400 --> 00:19:50,240
is required and there's obviously many

00:19:48,559 --> 00:19:51,679
other edge cases and gotchas too that

00:19:50,240 --> 00:19:54,160
have to be accounted for

00:19:51,679 --> 00:19:56,000
so um i actually ended up open sourcing

00:19:54,160 --> 00:19:56,480
the final throttling library and it can

00:19:56,000 --> 00:19:59,039
be found

00:19:56,480 --> 00:20:00,400
at github at patrick concor bandwidth

00:19:59,039 --> 00:20:02,559
throttle stream

00:20:00,400 --> 00:20:04,400
and it's actually compatible with deno

00:20:02,559 --> 00:20:05,919
and node.js which was quite an

00:20:04,400 --> 00:20:06,640
interesting challenge and to build a

00:20:05,919 --> 00:20:08,080
library

00:20:06,640 --> 00:20:10,559
that could work in both but that's

00:20:08,080 --> 00:20:12,720
something for another talk

00:20:10,559 --> 00:20:13,760
so time for demo and let's take a look

00:20:12,720 --> 00:20:17,039
at the final tool

00:20:13,760 --> 00:20:18,960
in use so

00:20:17,039 --> 00:20:20,720
what we're looking at here is basically

00:20:18,960 --> 00:20:22,000
the ui which we're going to use to

00:20:20,720 --> 00:20:25,440
configure the

00:20:22,000 --> 00:20:27,919
the proxy um i've got a throttle

00:20:25,440 --> 00:20:29,919
um set up here and it's set to six

00:20:27,919 --> 00:20:31,679
megabits per second of throughput

00:20:29,919 --> 00:20:34,080
i can also add some random jitter to

00:20:31,679 --> 00:20:36,159
that again kind of creating

00:20:34,080 --> 00:20:37,919
more of an element of the real world

00:20:36,159 --> 00:20:38,320
rather than the sort of lab conditions

00:20:37,919 --> 00:20:41,520
so

00:20:38,320 --> 00:20:42,240
basically it will randomly fluctuate uh

00:20:41,520 --> 00:20:44,240
00:20:42,240 --> 00:20:45,520
plus and minus above six megabits if i

00:20:44,240 --> 00:20:48,240
added that

00:20:45,520 --> 00:20:50,640
um i can add latency so basically

00:20:48,240 --> 00:20:52,480
simulate a cdn that's really far away

00:20:50,640 --> 00:20:54,320
stick 400 milliseconds of latency in

00:20:52,480 --> 00:20:55,520
front of every request

00:20:54,320 --> 00:20:56,720
a couple of other things you'll notice

00:20:55,520 --> 00:20:58,000
on these is we've got this pattern

00:20:56,720 --> 00:20:59,679
matching field

00:20:58,000 --> 00:21:01,760
so for example if i wanted to add

00:20:59,679 --> 00:21:04,400
latency i need to mp4

00:21:01,760 --> 00:21:05,280
files or only to dash manifests i could

00:21:04,400 --> 00:21:07,679
do that

00:21:05,280 --> 00:21:08,320
very easily a few other options we've

00:21:07,679 --> 00:21:11,120
got here

00:21:08,320 --> 00:21:12,880
is the ability to have specific requests

00:21:11,120 --> 00:21:14,880
fail with a 404

00:21:12,880 --> 00:21:16,720
this is really useful especially

00:21:14,880 --> 00:21:18,720
combined with this percentage so

00:21:16,720 --> 00:21:21,039
i can you know set that down to 10 and

00:21:18,720 --> 00:21:23,760
that will basically mean that one in 10

00:21:21,039 --> 00:21:24,320
requests let's say to a manifest file

00:21:23,760 --> 00:21:26,480
are going to

00:21:24,320 --> 00:21:28,799
fail with the 404 and this just allows

00:21:26,480 --> 00:21:30,799
us to to really build out and test

00:21:28,799 --> 00:21:33,280
behavior like retries

00:21:30,799 --> 00:21:35,679
um and other sort of resilience measures

00:21:33,280 --> 00:21:38,320
that we have to have in players to keep

00:21:35,679 --> 00:21:39,840
to keep playback going at all times uh

00:21:38,320 --> 00:21:41,280
finally we have this um

00:21:39,840 --> 00:21:44,080
hang behavior as well which just

00:21:41,280 --> 00:21:45,520
simulates that cdn under load where it

00:21:44,080 --> 00:21:47,760
just doesn't respond to the client

00:21:45,520 --> 00:21:50,159
request and just leaves it open

00:21:47,760 --> 00:21:52,000
which allows us to build behavior like

00:21:50,159 --> 00:21:53,840
cancel and retry

00:21:52,000 --> 00:21:56,559
so if i just clean this up and we'll go

00:21:53,840 --> 00:21:58,000
back to our throttle to begin with

00:21:56,559 --> 00:21:59,200
i'm going to open up these metrics on

00:21:58,000 --> 00:22:01,520
the right which you're going to we're

00:21:59,200 --> 00:22:03,760
going to look at in a second

00:22:01,520 --> 00:22:05,200
uh in this next tab here i have the

00:22:03,760 --> 00:22:06,640
player and the test so this is just a

00:22:05,200 --> 00:22:08,880
very basic

00:22:06,640 --> 00:22:10,320
test harness i have a video html5 video

00:22:08,880 --> 00:22:13,520
element here with my

00:22:10,320 --> 00:22:16,720
custom javascript um player um

00:22:13,520 --> 00:22:18,799
kind of instantiated over the top um

00:22:16,720 --> 00:22:20,480
i've got a link here to a manifest which

00:22:18,799 --> 00:22:21,360
is sitting on my file server behind the

00:22:20,480 --> 00:22:22,960
proxy

00:22:21,360 --> 00:22:25,120
if i initialize that and just start that

00:22:22,960 --> 00:22:26,960
video going we can see the test video

00:22:25,120 --> 00:22:30,400
playing i'm now going to go

00:22:26,960 --> 00:22:32,320
back to the controls and we can see some

00:22:30,400 --> 00:22:35,039
data here coming through so the player

00:22:32,320 --> 00:22:35,840
is playing uh there's some http requests

00:22:35,039 --> 00:22:37,760
being made

00:22:35,840 --> 00:22:39,679
and the player has a bandwidth estimate

00:22:37,760 --> 00:22:42,159
of around six megabits per second which

00:22:39,679 --> 00:22:44,000
is exactly what we have configured it to

00:22:42,159 --> 00:22:45,840
so just to kind of demonstrate again we

00:22:44,000 --> 00:22:47,280
can add some latency here

00:22:45,840 --> 00:22:48,799
and we should be able to see that coming

00:22:47,280 --> 00:22:49,440
through in these this gray section on

00:22:48,799 --> 00:22:51,840
this chart

00:22:49,440 --> 00:22:54,799
here the uh the waiting time there for

00:22:51,840 --> 00:22:56,480
around 400 milliseconds per request

00:22:54,799 --> 00:22:58,799
and we can reduce the throughput and see

00:22:56,480 --> 00:23:01,039
how the player reacts to that as well

00:22:58,799 --> 00:23:02,960
um so what we'll see is the bandwidth

00:23:01,039 --> 00:23:05,600
estimation dropping down

00:23:02,960 --> 00:23:07,120
um and the the active bit rate here

00:23:05,600 --> 00:23:08,000
which is basically the chosen quality

00:23:07,120 --> 00:23:10,320
profile

00:23:08,000 --> 00:23:11,840
will be reduced as needed so this is

00:23:10,320 --> 00:23:12,880
looking quite a lot like that diagram

00:23:11,840 --> 00:23:14,000
that i showed you at the beginning of

00:23:12,880 --> 00:23:18,320
the talk

00:23:14,000 --> 00:23:20,880
so let's go back and stop this test

00:23:18,320 --> 00:23:22,480
what we've been looking at just there is

00:23:20,880 --> 00:23:26,159
me using the tool

00:23:22,480 --> 00:23:27,760
through the um the manual ui

00:23:26,159 --> 00:23:29,440
but a lot of what i talked about earlier

00:23:27,760 --> 00:23:30,559
was how we can use this to automate

00:23:29,440 --> 00:23:33,679
tests so

00:23:30,559 --> 00:23:34,000
we also have this json format here and

00:23:33,679 --> 00:23:36,640
this

00:23:34,000 --> 00:23:38,640
this network periods section is

00:23:36,640 --> 00:23:40,159
basically a data model that describes

00:23:38,640 --> 00:23:42,159
everything that i was just doing on the

00:23:40,159 --> 00:23:44,000
ui but over time

00:23:42,159 --> 00:23:45,760
so i can say that i want to test that

00:23:44,000 --> 00:23:49,039
runs for 30 seconds

00:23:45,760 --> 00:23:51,440
and from 0 seconds up to 15 seconds i'm

00:23:49,039 --> 00:23:53,039
gonna have six megabits per second

00:23:51,440 --> 00:23:54,640
of throughput through a throttle

00:23:53,039 --> 00:23:56,640
behavior and then i'm gonna drop it down

00:23:54,640 --> 00:23:59,440
to three megabits per second

00:23:56,640 --> 00:24:01,039
uh and i can i can add latency and um

00:23:59,440 --> 00:24:04,320
errored requests and any other behavior

00:24:01,039 --> 00:24:06,159
that's available through this syntax

00:24:04,320 --> 00:24:08,080
and then i can use test periods here to

00:24:06,159 --> 00:24:10,320
basically

00:24:08,080 --> 00:24:11,600
evaluate whether the player has behaved

00:24:10,320 --> 00:24:13,120
as expected

00:24:11,600 --> 00:24:15,679
over that time so in this case i'm

00:24:13,120 --> 00:24:17,679
looking at that 15 to 30 second period

00:24:15,679 --> 00:24:19,520
after the bandwidth has been reduced i'm

00:24:17,679 --> 00:24:21,919
checking that the player adapted

00:24:19,520 --> 00:24:23,360
which is a term that basically means

00:24:21,919 --> 00:24:25,039
that it changed quality level

00:24:23,360 --> 00:24:26,400
and i've got a syntax here that say okay

00:24:25,039 --> 00:24:29,520
it must have adapted

00:24:26,400 --> 00:24:32,799
at least once and no more than

00:24:29,520 --> 00:24:36,640
two times um so that kind of gives you

00:24:32,799 --> 00:24:37,600
an idea of um you know how how we would

00:24:36,640 --> 00:24:39,520
how we would build

00:24:37,600 --> 00:24:41,600
uh our automated test scenarios and

00:24:39,520 --> 00:24:43,440
those um would be typically run as part

00:24:41,600 --> 00:24:48,240
of our ci pipeline

00:24:43,440 --> 00:24:48,240
so i'm just going to return to the deck

00:24:48,320 --> 00:24:53,760
um um

00:24:51,840 --> 00:24:55,520
i guess another thing worth mentioning

00:24:53,760 --> 00:24:56,080
is that everything we've looked at so

00:24:55,520 --> 00:24:57,840
far

00:24:56,080 --> 00:25:00,080
um has really been in relation to video

00:24:57,840 --> 00:25:02,159
playback but you may already be thinking

00:25:00,080 --> 00:25:04,400
that the ability to automatically drive

00:25:02,159 --> 00:25:06,159
any html page through simulated network

00:25:04,400 --> 00:25:06,880
conditions could be just as beneficial

00:25:06,159 --> 00:25:09,600
for

00:25:06,880 --> 00:25:11,200
applications in general so for example

00:25:09,600 --> 00:25:14,000
being able to programmatically test

00:25:11,200 --> 00:25:15,919
image loading failover behavior um or

00:25:14,000 --> 00:25:18,320
retries on a failing apa

00:25:15,919 --> 00:25:19,600
api endpoint could be really useful and

00:25:18,320 --> 00:25:21,360
to be able to do that all in a kind of

00:25:19,600 --> 00:25:22,400
declarative scriptable fashion

00:25:21,360 --> 00:25:24,559
so that's something we haven't really

00:25:22,400 --> 00:25:26,000
investigated yet um this is really just

00:25:24,559 --> 00:25:28,000
a playback tool but um

00:25:26,000 --> 00:25:29,760
we definitely think that there's a wider

00:25:28,000 --> 00:25:33,440
use case for it

00:25:29,760 --> 00:25:34,080
um so um yeah overall the project and

00:25:33,440 --> 00:25:36,720
its tool

00:25:34,080 --> 00:25:38,000
still in its infancy um i feel we're

00:25:36,720 --> 00:25:39,039
really only scratching the surface of

00:25:38,000 --> 00:25:41,600
what's possible with it

00:25:39,039 --> 00:25:43,440
um but the good thing is the project's

00:25:41,600 --> 00:25:45,039
already created a lot of enthusiasm and

00:25:43,440 --> 00:25:46,320
curiosity within the team

00:25:45,039 --> 00:25:48,080
which for me really validates the

00:25:46,320 --> 00:25:51,360
decision to go with a javascript

00:25:48,080 --> 00:25:52,720
implementation is perhaps an unusual use

00:25:51,360 --> 00:25:54,159
case for javascript

00:25:52,720 --> 00:25:56,720
but one that proves hopefully we can

00:25:54,159 --> 00:25:58,720
take familiar dx friendly tooling

00:25:56,720 --> 00:26:00,080
and use it to solve unfamiliar problems

00:25:58,720 --> 00:26:02,400
that would typically be sold

00:26:00,080 --> 00:26:03,360
low-level native code and would be out

00:26:02,400 --> 00:26:06,159
of the reach

00:26:03,360 --> 00:26:07,919
for front-end engineers so thanks a lot

00:26:06,159 --> 00:26:08,720
for watching and i am looking forward to

00:26:07,919 --> 00:26:11,840
the q a

00:26:08,720 --> 00:26:11,840
thank you

00:26:21,480 --> 00:26:28,240
[Music]

00:26:26,240 --> 00:26:30,000
it's an amazing talk thank you very much

00:26:28,240 --> 00:26:30,559
for delivering it as part of this year's

00:26:30,000 --> 00:26:33,200
event

00:26:30,559 --> 00:26:34,000
um i i think i learned a lot about

00:26:33,200 --> 00:26:36,640
balancing

00:26:34,000 --> 00:26:37,600
video content on servers i'm sure a lot

00:26:36,640 --> 00:26:39,600
of people do

00:26:37,600 --> 00:26:41,200
um you touched on at the end there the

00:26:39,600 --> 00:26:42,640
question i had written down while we're

00:26:41,200 --> 00:26:45,279
waiting for other people to

00:26:42,640 --> 00:26:46,880
put their questions in is this really

00:26:45,279 --> 00:26:47,440
something that's only applicable to

00:26:46,880 --> 00:26:50,159
video

00:26:47,440 --> 00:26:52,880
content delivery or would you be able to

00:26:50,159 --> 00:26:55,760
suggest a few use cases for using

00:26:52,880 --> 00:26:56,640
your library that you've created outside

00:26:55,760 --> 00:26:59,120
um yeah

00:26:56,640 --> 00:26:59,760
yeah it's a really good question so um

00:26:59,120 --> 00:27:02,000
obviously

00:26:59,760 --> 00:27:03,679
the whole project was was um built with

00:27:02,000 --> 00:27:05,120
playback testing in mind and there's a

00:27:03,679 --> 00:27:08,799
lot of features there that are

00:27:05,120 --> 00:27:11,200
quite video player centric but um

00:27:08,799 --> 00:27:12,960
you know any any team building front a

00:27:11,200 --> 00:27:14,159
front end applications especially single

00:27:12,960 --> 00:27:17,440
page applications

00:27:14,159 --> 00:27:18,240
um i think is probably already doing

00:27:17,440 --> 00:27:19,919
some kind of

00:27:18,240 --> 00:27:22,399
performance or network simulation

00:27:19,919 --> 00:27:25,440
testing probably with chrome dev tools

00:27:22,399 --> 00:27:27,679
at a minimum um what we

00:27:25,440 --> 00:27:28,640
can do with this tool is obviously

00:27:27,679 --> 00:27:31,200
introduce a much

00:27:28,640 --> 00:27:32,399
higher level of granularity there so the

00:27:31,200 --> 00:27:35,360
internet isn't always

00:27:32,399 --> 00:27:37,279
like this this linear thing in terms of

00:27:35,360 --> 00:27:39,039
what our applications experience so

00:27:37,279 --> 00:27:41,039
if you're loading images from from a

00:27:39,039 --> 00:27:42,320
certain cdn over here and videos from

00:27:41,039 --> 00:27:45,600
another cdn

00:27:42,320 --> 00:27:47,200
that the behavior of those two um cdns

00:27:45,600 --> 00:27:49,039
might be completely different

00:27:47,200 --> 00:27:50,480
so it's not it's not consistent so being

00:27:49,039 --> 00:27:53,200
able to kind of

00:27:50,480 --> 00:27:53,919
uh create create this sort of proxy

00:27:53,200 --> 00:27:56,640
layer and

00:27:53,919 --> 00:27:58,080
and different behaviors depending on

00:27:56,640 --> 00:27:59,679
what content your application is

00:27:58,080 --> 00:28:03,120
downloading i think i think has

00:27:59,679 --> 00:28:05,039
quite a broader set of applications um

00:28:03,120 --> 00:28:06,720
so yeah as i said at the end we haven't

00:28:05,039 --> 00:28:09,279
we haven't really started

00:28:06,720 --> 00:28:11,200
using that yet but um i i could see that

00:28:09,279 --> 00:28:13,120
it's all being extended

00:28:11,200 --> 00:28:14,399
um certainly with that in mind in the

00:28:13,120 --> 00:28:16,159
future

00:28:14,399 --> 00:28:18,159
okay so it sounds like it's something

00:28:16,159 --> 00:28:20,000
that's going to be carried on

00:28:18,159 --> 00:28:21,520
that you're going to carry on developing

00:28:20,000 --> 00:28:23,600
it is that right

00:28:21,520 --> 00:28:25,440
yeah i mean it's it's still a work in

00:28:23,600 --> 00:28:29,440
progress at the moment as well um

00:28:25,440 --> 00:28:30,559
we kind of um we we using it mostly in a

00:28:29,440 --> 00:28:32,799
kind of local

00:28:30,559 --> 00:28:34,559
development setting we haven't quite

00:28:32,799 --> 00:28:35,919
realized the ambitions of running it at

00:28:34,559 --> 00:28:37,039
scale yet so

00:28:35,919 --> 00:28:39,760
that's actually what we're working on at

00:28:37,039 --> 00:28:42,640
the moment is is moving moving it to

00:28:39,760 --> 00:28:44,320
aws um and kind of getting the tests

00:28:42,640 --> 00:28:46,799
running in parallel okay

00:28:44,320 --> 00:28:48,559
um so that we can basically have it

00:28:46,799 --> 00:28:49,760
running in our ci pipelines which is

00:28:48,559 --> 00:28:51,679
that that's kind of

00:28:49,760 --> 00:28:53,200
from day one that was the goal and it

00:28:51,679 --> 00:28:54,559
took a lot of work just to kind of prove

00:28:53,200 --> 00:28:56,240
the concept and

00:28:54,559 --> 00:28:58,000
and get to where we are before we can

00:28:56,240 --> 00:28:58,799
then actually you know put it put it

00:28:58,000 --> 00:29:01,039
into the uh

00:28:58,799 --> 00:29:02,480
into cloud infrastructure and hook it up

00:29:01,039 --> 00:29:04,080
to our pipelines

00:29:02,480 --> 00:29:05,679
so is it something that you'd be willing

00:29:04,080 --> 00:29:07,600
to take pull requests

00:29:05,679 --> 00:29:09,200
against or is it not necessarily

00:29:07,600 --> 00:29:12,320
something for the open source world

00:29:09,200 --> 00:29:14,720
um yeah i guess i guess i should um

00:29:12,320 --> 00:29:15,520
so the link that i posted in the talk uh

00:29:14,720 --> 00:29:17,120
to github

00:29:15,520 --> 00:29:19,200
is just like a small part of the whole

00:29:17,120 --> 00:29:21,840
tool that's the um basically the

00:29:19,200 --> 00:29:23,200
the throttle stream which is kind of

00:29:21,840 --> 00:29:26,159
fairly agnostic

00:29:23,200 --> 00:29:27,360
um it's not it's not playback specific

00:29:26,159 --> 00:29:30,320
it's something you can install

00:29:27,360 --> 00:29:30,960
in a in a node.js server or dino server

00:29:30,320 --> 00:29:32,799
and

00:29:30,960 --> 00:29:34,240
basically use it to slow down traffic

00:29:32,799 --> 00:29:36,320
that's coming through so

00:29:34,240 --> 00:29:37,760
quite general purpose so that that's

00:29:36,320 --> 00:29:39,520
open source there's nothing really

00:29:37,760 --> 00:29:41,200
design specific in there

00:29:39,520 --> 00:29:42,960
um the tool that i demoed in the video

00:29:41,200 --> 00:29:45,520
at the end is is a kind of like

00:29:42,960 --> 00:29:46,799
internal playback testing stack which

00:29:45,520 --> 00:29:48,799
has various different design

00:29:46,799 --> 00:29:50,880
dependencies built into it so

00:29:48,799 --> 00:29:52,320
not something that we would be open

00:29:50,880 --> 00:29:54,799
sourcing but

00:29:52,320 --> 00:29:56,240
the idea the idea i guess is that i'm

00:29:54,799 --> 00:29:58,559
trying to kind of show how

00:29:56,240 --> 00:29:59,520
somebody could use use that that

00:29:58,559 --> 00:30:02,159
throttle uh

00:29:59,520 --> 00:30:03,200
and build build a similar kind of tool

00:30:02,159 --> 00:30:05,520
out of it

00:30:03,200 --> 00:30:07,039
for different purposes okay and speaking

00:30:05,520 --> 00:30:08,960
of tools you mentioned that

00:30:07,039 --> 00:30:10,480
you were using dino as part of that

00:30:08,960 --> 00:30:12,960
stack have you got any

00:30:10,480 --> 00:30:13,760
thoughts on dino do you do you think

00:30:12,960 --> 00:30:15,520
it's yeah

00:30:13,760 --> 00:30:17,120
from the prime time tell me your

00:30:15,520 --> 00:30:20,320
thoughts

00:30:17,120 --> 00:30:21,600
so i was back in may that i started

00:30:20,320 --> 00:30:23,840
i've been following the dna project for

00:30:21,600 --> 00:30:25,520
a long time with um a lot of enthusiasm

00:30:23,840 --> 00:30:28,720
because i'm a massive typescript fan

00:30:25,520 --> 00:30:30,320
um and i continually get frustrated at

00:30:28,720 --> 00:30:31,760
some of the things in some things in

00:30:30,320 --> 00:30:34,159
node.js that

00:30:31,760 --> 00:30:35,360
we have to deal with so um i thought i'd

00:30:34,159 --> 00:30:36,960
kind of

00:30:35,360 --> 00:30:38,399
you know just play around with it and i

00:30:36,960 --> 00:30:38,880
realized it would actually be a really

00:30:38,399 --> 00:30:40,480
good

00:30:38,880 --> 00:30:42,240
this project would be a good use case

00:30:40,480 --> 00:30:43,200
for it because we have this very simple

00:30:42,240 --> 00:30:47,120
little

00:30:43,200 --> 00:30:48,640
uh proxy server and um basically we

00:30:47,120 --> 00:30:50,240
the initial prototype we had was using

00:30:48,640 --> 00:30:51,679
node.js um

00:30:50,240 --> 00:30:53,200
and we had to leverage all sorts of

00:30:51,679 --> 00:30:55,440
libraries just to get things to work we

00:30:53,200 --> 00:30:57,360
were we were using express and

00:30:55,440 --> 00:30:59,039
uh we were using various websocket

00:30:57,360 --> 00:31:00,880
libraries and things and

00:30:59,039 --> 00:31:02,240
um obviously we were writing in

00:31:00,880 --> 00:31:03,760
typescript so we had a whole build

00:31:02,240 --> 00:31:05,440
system and everything

00:31:03,760 --> 00:31:07,519
and then i looked at dino and i realized

00:31:05,440 --> 00:31:09,519
we could do everything we needed

00:31:07,519 --> 00:31:11,679
completely out of the box without having

00:31:09,519 --> 00:31:14,559
to leverage any um

00:31:11,679 --> 00:31:16,080
third-party modules uh don't need a

00:31:14,559 --> 00:31:18,480
build system you just run it it's

00:31:16,080 --> 00:31:20,480
typescript it runs natively

00:31:18,480 --> 00:31:22,640
and then on top of that you get we had

00:31:20,480 --> 00:31:23,440
quite a lot of performance optimizations

00:31:22,640 --> 00:31:25,039
as well

00:31:23,440 --> 00:31:26,880
but specifically because of the way that

00:31:25,039 --> 00:31:28,960
dino handles streams

00:31:26,880 --> 00:31:30,559
under the hood which is it's basically

00:31:28,960 --> 00:31:33,039
like a pool based approach

00:31:30,559 --> 00:31:35,200
versus a push-based approach so it it's

00:31:33,039 --> 00:31:35,440
uh it's kind of waiting until the system

00:31:35,200 --> 00:31:37,360
is

00:31:35,440 --> 00:31:39,120
is free before it processes rather than

00:31:37,360 --> 00:31:42,080
just kind of letting you throw

00:31:39,120 --> 00:31:43,120
data at it which is apparently a common

00:31:42,080 --> 00:31:45,039
performance bottleneck

00:31:43,120 --> 00:31:46,799
in in node and something that the dna

00:31:45,039 --> 00:31:49,279
team were trying to optimize for so

00:31:46,799 --> 00:31:50,960
yeah it was i loved dino um it wasn't it

00:31:49,279 --> 00:31:54,000
wasn't the easiest

00:31:50,960 --> 00:31:54,399
um learning curve because when i started

00:31:54,000 --> 00:31:57,039
working

00:31:54,399 --> 00:31:58,240
in back in may i think it was still

00:31:57,039 --> 00:32:00,159
fairly experimental

00:31:58,240 --> 00:32:01,760
some of the stuff was unstable it still

00:32:00,159 --> 00:32:04,000
is to an extent but

00:32:01,760 --> 00:32:05,760
um and there wasn't a lot of like blog

00:32:04,000 --> 00:32:08,480
posts out there and things in terms of

00:32:05,760 --> 00:32:09,360
how to do x y and z but uh yeah it was

00:32:08,480 --> 00:32:11,679
it was rewarding

00:32:09,360 --> 00:32:13,200
i would recommend it uh especially on r

00:32:11,679 --> 00:32:16,000
d projects like this maybe

00:32:13,200 --> 00:32:16,320
things that aren't customer facing um

00:32:16,000 --> 00:32:18,880
that

00:32:16,320 --> 00:32:20,320
you know fairly limited in scope it's

00:32:18,880 --> 00:32:22,240
this was the perfect kind of

00:32:20,320 --> 00:32:24,240
application for it so i love that you

00:32:22,240 --> 00:32:24,880
said limited in scope there do you have

00:32:24,240 --> 00:32:28,000
any

00:32:24,880 --> 00:32:30,000
kind of idea of the amount of terabytes

00:32:28,000 --> 00:32:32,320
of video which is going through this

00:32:30,000 --> 00:32:35,760
piece of software

00:32:32,320 --> 00:32:38,080
um so yeah that's a good question

00:32:35,760 --> 00:32:39,360
the the little boxing video there that

00:32:38,080 --> 00:32:43,360
was in the presentation

00:32:39,360 --> 00:32:45,679
is it's five minutes long

00:32:43,360 --> 00:32:47,279
it's we've encoded it so that it matches

00:32:45,679 --> 00:32:50,159
the sports content that

00:32:47,279 --> 00:32:51,360
real design customers see so all the

00:32:50,159 --> 00:32:53,519
quality profiles

00:32:51,360 --> 00:32:54,880
match and so on and and just that five

00:32:53,519 --> 00:32:57,519
minutes of video is about

00:32:54,880 --> 00:32:59,200
five gigabytes uh when you take all the

00:32:57,519 --> 00:33:00,320
different quarter levels and sum them

00:32:59,200 --> 00:33:03,440
all together

00:33:00,320 --> 00:33:05,120
so um you know if we're running if we're

00:33:03,440 --> 00:33:07,919
running um

00:33:05,120 --> 00:33:09,200
thousands of tests uh many times a day

00:33:07,919 --> 00:33:10,799
in the pipeline yeah

00:33:09,200 --> 00:33:12,240
there will be many terabytes going

00:33:10,799 --> 00:33:14,799
through it um

00:33:12,240 --> 00:33:16,000
but uh yeah we're using we're using aws

00:33:14,799 --> 00:33:17,360
to kind of

00:33:16,000 --> 00:33:19,200
make sure that's done in a performant

00:33:17,360 --> 00:33:20,960
way and um it's not

00:33:19,200 --> 00:33:22,240
going through like multiple instances at

00:33:20,960 --> 00:33:24,399
once so

00:33:22,240 --> 00:33:26,480
hopefully no bottlenecks there but yeah

00:33:24,399 --> 00:33:27,360
that this is just a microcosm of what

00:33:26,480 --> 00:33:30,320
actually happens in our

00:33:27,360 --> 00:33:31,440
production video pipeline which is

00:33:30,320 --> 00:33:34,080
obviously

00:33:31,440 --> 00:33:35,039
an order of magnitude yeah i can imagine

00:33:34,080 --> 00:33:37,600
it's a

00:33:35,039 --> 00:33:38,320
very complex system and so we have

00:33:37,600 --> 00:33:41,760
actually had a

00:33:38,320 --> 00:33:43,519
question come in now how frequently uh

00:33:41,760 --> 00:33:44,880
i was worried the subject might be too

00:33:43,519 --> 00:33:47,360
niche yeah invite

00:33:44,880 --> 00:33:48,399
invite q a but thank you yeah so

00:33:47,360 --> 00:33:50,399
philippa put one in

00:33:48,399 --> 00:33:52,240
uh how frequently do you plan to run

00:33:50,399 --> 00:33:56,399
testing in the pipeline

00:33:52,240 --> 00:34:00,399
or on demand or at set times

00:33:56,399 --> 00:34:02,159
um so it kind of depends on the outcome

00:34:00,399 --> 00:34:05,039
of this parallelization work

00:34:02,159 --> 00:34:06,399
we're doing right now we don't um yeah

00:34:05,039 --> 00:34:08,560
we're going to be using

00:34:06,399 --> 00:34:09,520
something called aws step functions with

00:34:08,560 --> 00:34:12,320
fan out so

00:34:09,520 --> 00:34:13,839
basically we can we what you saw in that

00:34:12,320 --> 00:34:15,919
little json test file was about

00:34:13,839 --> 00:34:17,280
a test was 30 seconds long and we want

00:34:15,919 --> 00:34:18,399
to have thousands of tests because

00:34:17,280 --> 00:34:20,560
there's all sorts of things that need to

00:34:18,399 --> 00:34:22,800
be tested which aren't right now

00:34:20,560 --> 00:34:24,240
so the idea is that you know maybe

00:34:22,800 --> 00:34:25,679
within a minute or two

00:34:24,240 --> 00:34:27,520
you actually complete your whole test

00:34:25,679 --> 00:34:29,200
suite um even though each test is

00:34:27,520 --> 00:34:31,359
running in real time

00:34:29,200 --> 00:34:32,720
so if that if that if that works

00:34:31,359 --> 00:34:34,879
correctly as planned

00:34:32,720 --> 00:34:36,240
um you know the whole thing should just

00:34:34,879 --> 00:34:39,440
take a minute or two

00:34:36,240 --> 00:34:40,480
um and in that case we'll just run it on

00:34:39,440 --> 00:34:43,440
demand basically

00:34:40,480 --> 00:34:45,119
um we have quite a nice ci pipeline most

00:34:43,440 --> 00:34:48,399
of the teams do at the zone

00:34:45,119 --> 00:34:50,399
um so devs are checking in um

00:34:48,399 --> 00:34:52,480
every time a dev pushes code it will run

00:34:50,399 --> 00:34:55,119
either the entire pipeline or a kind of

00:34:52,480 --> 00:34:56,639
small subset of the pipeline um so i

00:34:55,119 --> 00:34:58,000
think i think the goal would be to

00:34:56,639 --> 00:34:59,599
to be running it there as long as it

00:34:58,000 --> 00:35:00,960
doesn't slow down pipelines too much as

00:34:59,599 --> 00:35:02,240
i said if we can parallelize it

00:35:00,960 --> 00:35:04,160
effectively it shouldn't

00:35:02,240 --> 00:35:05,839
but if we have some limitations there or

00:35:04,160 --> 00:35:07,680
maybe cost limitations

00:35:05,839 --> 00:35:09,200
and we want to set like a maximum number

00:35:07,680 --> 00:35:10,560
of instances then

00:35:09,200 --> 00:35:11,839
in that case it might take a bit longer

00:35:10,560 --> 00:35:13,599
and we'll think about moving it to

00:35:11,839 --> 00:35:16,800
nightly

00:35:13,599 --> 00:35:17,119
but the goal was always to kind of make

00:35:16,800 --> 00:35:19,040
this

00:35:17,119 --> 00:35:20,720
like just a really unintrusive part of

00:35:19,040 --> 00:35:22,880
our developer workflow

00:35:20,720 --> 00:35:24,320
that um people don't even have to think

00:35:22,880 --> 00:35:25,119
about it's just something that's there

00:35:24,320 --> 00:35:27,520
and just like

00:35:25,119 --> 00:35:29,119
any other testing tool that we use uh

00:35:27,520 --> 00:35:31,359
right now that that's the goal

00:35:29,119 --> 00:35:32,720
ideal that was a perfect answer and and

00:35:31,359 --> 00:35:34,079
with that we're going to have to leave

00:35:32,720 --> 00:35:35,680
it there because the next talk will be

00:35:34,079 --> 00:35:37,680
starting very very shortly but i believe

00:35:35,680 --> 00:35:38,240
that people can come and meet the design

00:35:37,680 --> 00:35:40,000
family

00:35:38,240 --> 00:35:42,160
on their sponsor stand get their secret

00:35:40,000 --> 00:35:43,839
code for their free books

00:35:42,160 --> 00:35:45,280
and will you be hanging for a little bit

00:35:43,839 --> 00:35:47,200
longer if anyone wants to come and find

00:35:45,280 --> 00:35:48,880
you yeah i'll be on remote probably in

00:35:47,200 --> 00:35:51,359
the zone lounge somewhere

00:35:48,880 --> 00:35:52,079
uh for a while so uh yeah drop me a

00:35:51,359 --> 00:35:54,000
message if

00:35:52,079 --> 00:35:55,680
anyone wants any more information

00:35:54,000 --> 00:35:57,520
awesome patrick thank you very much for

00:35:55,680 --> 00:35:58,079
your talk and for your q a session it

00:35:57,520 --> 00:36:13,839
was

00:35:58,079 --> 00:36:13,839

YouTube URL: https://www.youtube.com/watch?v=xTNDa6OMI_0


