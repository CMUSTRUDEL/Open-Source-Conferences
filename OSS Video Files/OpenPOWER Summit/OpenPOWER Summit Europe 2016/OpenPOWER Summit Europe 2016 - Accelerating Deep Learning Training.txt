Title: OpenPOWER Summit Europe 2016 - Accelerating Deep Learning Training
Publication date: 2016-11-17
Playlist: OpenPOWER Summit Europe 2016
Description: 
	
Captions: 
	00:00:11,830 --> 00:00:17,070
now

00:00:14,750 --> 00:00:19,800
well I'm very glad you played that video

00:00:17,070 --> 00:00:22,230
that made me feel like I needed to stand

00:00:19,800 --> 00:00:23,340
up and salute actually because I'm also

00:00:22,230 --> 00:00:26,220
going to talk about deep learning

00:00:23,340 --> 00:00:28,949
solutions and I'm going to talk very

00:00:26,220 --> 00:00:32,700
specifically today about using deep

00:00:28,949 --> 00:00:35,940
learning solutions on power 8 with p100

00:00:32,700 --> 00:00:37,890
and envy link so by way of introduction

00:00:35,940 --> 00:00:40,680
my name is Scott Suter on part of our

00:00:37,890 --> 00:00:42,899
product management team within IBM I

00:00:40,680 --> 00:00:45,030
have two products one is our deep

00:00:42,899 --> 00:00:47,399
learning solution and one is an

00:00:45,030 --> 00:00:49,440
accelerated database solution so we

00:00:47,399 --> 00:00:56,069
heard a great great example of that

00:00:49,440 --> 00:00:58,499
earlier today so this is sort of an

00:00:56,069 --> 00:01:02,190
existential question how do you teach a

00:00:58,499 --> 00:01:05,670
computer to perceive I mean programming

00:01:02,190 --> 00:01:09,390
a computer to perceive is a virtually

00:01:05,670 --> 00:01:12,540
impossible task but what if we could

00:01:09,390 --> 00:01:15,690
teach a computer to learn teach a

00:01:12,540 --> 00:01:19,650
computer to go through a very similar

00:01:15,690 --> 00:01:24,200
process to what you did as a child when

00:01:19,650 --> 00:01:26,280
you figure out well what's a bicycle

00:01:24,200 --> 00:01:29,250
what are the characteristics of a

00:01:26,280 --> 00:01:31,740
bicycle that are unique how is it

00:01:29,250 --> 00:01:34,640
different from a motorcycle how's it

00:01:31,740 --> 00:01:34,640
different car

00:01:34,670 --> 00:01:40,530
this is something that we do intuitively

00:01:37,830 --> 00:01:43,140
through neural process and it's

00:01:40,530 --> 00:01:46,020
something is replicated through what is

00:01:43,140 --> 00:01:48,840
called deep learning then I'll back up

00:01:46,020 --> 00:01:53,550
just a little bit deep learning is a

00:01:48,840 --> 00:01:57,570
subset of AI and machine learning and it

00:01:53,550 --> 00:02:00,630
is really designed around the idea that

00:01:57,570 --> 00:02:04,440
if you put a mini layered neural process

00:02:00,630 --> 00:02:08,760
in place you can teach a computer how to

00:02:04,440 --> 00:02:11,400
learn it will sift through millions of

00:02:08,760 --> 00:02:14,340
different items different data points

00:02:11,400 --> 00:02:17,670
and it will learn to figure out which

00:02:14,340 --> 00:02:20,750
are the important ones and so it is also

00:02:17,670 --> 00:02:22,890
as Tim mentioned something that has been

00:02:20,750 --> 00:02:27,360
phenomenally successful and has

00:02:22,890 --> 00:02:30,870
phenomenal uptake on GPUs and luckily we

00:02:27,360 --> 00:02:32,130
have a good place to put a GPU so this

00:02:30,870 --> 00:02:33,690
is really a tale of two different

00:02:32,130 --> 00:02:36,630
infrastructures within deep learning

00:02:33,690 --> 00:02:39,600
there is the training aspect which is

00:02:36,630 --> 00:02:42,540
that neural network analysis and then

00:02:39,600 --> 00:02:47,070
there is the inference piece which acts

00:02:42,540 --> 00:02:49,320
on what the system has learned acts on a

00:02:47,070 --> 00:02:50,700
fully built neural network today I'm

00:02:49,320 --> 00:02:54,840
really going to talk about the data

00:02:50,700 --> 00:02:56,640
center part of the equation which is the

00:02:54,840 --> 00:02:59,430
training element and I'm going to talk

00:02:56,640 --> 00:03:03,090
about how that works today on the IBM sa

00:02:59,430 --> 00:03:07,739
22 LC for hpc well known and loved as

00:03:03,090 --> 00:03:10,590
the Minsky so IBM strategy around deep

00:03:07,739 --> 00:03:13,170
learning is really comes down to three

00:03:10,590 --> 00:03:15,330
things first of all this is adjacent to

00:03:13,170 --> 00:03:18,480
what you would probably first think

00:03:15,330 --> 00:03:20,489
about when you consider IBM and AI we're

00:03:18,480 --> 00:03:23,519
adjacent to Watson we think that's a

00:03:20,489 --> 00:03:26,310
great great solution but there's also a

00:03:23,519 --> 00:03:30,000
vibrant community out there of open

00:03:26,310 --> 00:03:32,880
source developers sponsored by very very

00:03:30,000 --> 00:03:35,640
large entities who are adding tremendous

00:03:32,880 --> 00:03:37,320
innovation in this space are offering

00:03:35,640 --> 00:03:41,310
around deep learning is very focused on

00:03:37,320 --> 00:03:43,160
this we are working with the deep

00:03:41,310 --> 00:03:44,970
learning distribution or frameworks

00:03:43,160 --> 00:03:47,250
modifying them

00:03:44,970 --> 00:03:49,740
and adding innovations so we're trying

00:03:47,250 --> 00:03:51,510
to help change the frameworks we're

00:03:49,740 --> 00:03:54,480
trying to add additional capabilities

00:03:51,510 --> 00:03:58,500
such as give them the ability to scale

00:03:54,480 --> 00:04:01,200
out and then also as a systems provider

00:03:58,500 --> 00:04:04,080
figure out how to build differentiated

00:04:01,200 --> 00:04:06,960
gpu-accelerated systems that use envy

00:04:04,080 --> 00:04:10,320
link and give the unique capability that

00:04:06,960 --> 00:04:16,350
that brings to intrasystem communication

00:04:10,320 --> 00:04:19,350
to help speed up the neural networks so

00:04:16,350 --> 00:04:21,420
what are we doing we have created two

00:04:19,350 --> 00:04:24,930
versions of this already we're about to

00:04:21,420 --> 00:04:27,870
drop number three and what we have done

00:04:24,930 --> 00:04:30,360
is built a distribution of the four most

00:04:27,870 --> 00:04:34,830
popular deep learning frameworks cafe

00:04:30,360 --> 00:04:36,630
torch theano tensorflow and some of the

00:04:34,830 --> 00:04:41,310
libraries that support their operation

00:04:36,630 --> 00:04:43,080
within a linux on power system we are

00:04:41,310 --> 00:04:47,190
making these available and we're up

00:04:43,080 --> 00:04:50,310
streaming the port to the open source

00:04:47,190 --> 00:04:52,740
community we want to expand the we want

00:04:50,310 --> 00:04:54,870
to expand the world here we are also

00:04:52,740 --> 00:04:57,210
starting to add innovations from IBM

00:04:54,870 --> 00:04:59,750
into these libraries we're adding

00:04:57,210 --> 00:05:02,340
innovations from our research institute

00:04:59,750 --> 00:05:04,229
institutes around the globe and our

00:05:02,340 --> 00:05:05,850
systems Institute's help bring more

00:05:04,229 --> 00:05:08,070
capability of the neural networks

00:05:05,850 --> 00:05:10,050
themselves we won't be up streaming all

00:05:08,070 --> 00:05:14,940
those but we will be making them

00:05:10,050 --> 00:05:16,530
available at IBM com and we are pairing

00:05:14,940 --> 00:05:19,110
these up with systems that are generally

00:05:16,530 --> 00:05:22,320
available today pairing them up with the

00:05:19,110 --> 00:05:26,070
Minsky systems that are started selling

00:05:22,320 --> 00:05:29,240
last quarter and are available from your

00:05:26,070 --> 00:05:32,100
IBM business partner from IBM direct or

00:05:29,240 --> 00:05:34,410
systems that are enabled with IBM p8 and

00:05:32,100 --> 00:05:36,520
be linked in p100 three-year open power

00:05:34,410 --> 00:05:39,690
partners

00:05:36,520 --> 00:05:42,550
and we are right around the corner from

00:05:39,690 --> 00:05:45,280
having our initial availability of the

00:05:42,550 --> 00:05:47,979
frameworks and I would not be surprised

00:05:45,280 --> 00:05:51,310
if we said something very big in the

00:05:47,979 --> 00:05:53,650
next couple of weeks so our first goal

00:05:51,310 --> 00:05:56,560
with developing this we need to simplify

00:05:53,650 --> 00:06:01,139
access in installation for these

00:05:56,560 --> 00:06:03,789
products especially on an architecture

00:06:01,139 --> 00:06:05,949
that has unique capabilities such as

00:06:03,789 --> 00:06:09,580
power we want to make sure that all of

00:06:05,949 --> 00:06:12,159
the underlying libraries and software

00:06:09,580 --> 00:06:14,710
frame capabilities that come about in

00:06:12,159 --> 00:06:17,889
CUDA cudn n and then some of the

00:06:14,710 --> 00:06:19,750
mathematical accelerators are available

00:06:17,889 --> 00:06:22,960
optimized and see the benefit of envy

00:06:19,750 --> 00:06:25,509
link we want to simplify the

00:06:22,960 --> 00:06:27,969
installation process make sure that this

00:06:25,509 --> 00:06:30,490
is as easy as giving either a single deb

00:06:27,969 --> 00:06:33,460
package and be able to put it on your

00:06:30,490 --> 00:06:35,379
bun to system or bit by bit build it

00:06:33,460 --> 00:06:37,180
from source with a set of instructions

00:06:35,379 --> 00:06:41,380
that we will make available on the open

00:06:37,180 --> 00:06:43,080
power foundation web page and then over

00:06:41,380 --> 00:06:46,210
time again as i mentioned earlier

00:06:43,080 --> 00:06:48,090
optimize bring more of our IP into this

00:06:46,210 --> 00:06:51,159
find a way to improve the network's

00:06:48,090 --> 00:06:57,190
build it into a into an infrastructure

00:06:51,159 --> 00:06:59,680
that scales out better we heard a little

00:06:57,190 --> 00:07:01,449
bit about NV link earlier and I think

00:06:59,680 --> 00:07:04,210
that the important point that i want to

00:07:01,449 --> 00:07:06,779
add here not only in building this very

00:07:04,210 --> 00:07:08,830
balanced system do we create the

00:07:06,779 --> 00:07:10,599
capability to move data through the

00:07:08,830 --> 00:07:14,560
system very quickly but it reduces

00:07:10,599 --> 00:07:18,479
significantly the latency that you see

00:07:14,560 --> 00:07:22,870
within a deep learning training scenario

00:07:18,479 --> 00:07:28,719
we can remove roughly four times the

00:07:22,870 --> 00:07:30,819
data between GPU and memory using NV

00:07:28,719 --> 00:07:33,099
link and using those 40 gigabyte

00:07:30,819 --> 00:07:37,150
bi-directional connections between the

00:07:33,099 --> 00:07:40,719
powerade CPU and the GPU as you can on a

00:07:37,150 --> 00:07:43,539
PCIe system we measured the results at

00:07:40,719 --> 00:07:48,669
about 4 gigabyte gigabytes per second

00:07:43,539 --> 00:07:49,780
using PCIe attached GPUs and 17

00:07:48,669 --> 00:07:55,360
gigabytes a second

00:07:49,780 --> 00:07:58,780
using envy link attached GPS what that

00:07:55,360 --> 00:08:01,510
gives you 0 is a significant reduction

00:07:58,780 --> 00:08:04,240
in communication time and what that

00:08:01,510 --> 00:08:06,520
brings you today is the ability to run

00:08:04,240 --> 00:08:07,960
your neural networks faster you neither

00:08:06,520 --> 00:08:10,210
include more data in your training

00:08:07,960 --> 00:08:12,220
routines or you can close out your work

00:08:10,210 --> 00:08:15,610
quicker but it's a very tangible

00:08:12,220 --> 00:08:18,990
expression of where envy link brings the

00:08:15,610 --> 00:08:21,970
benefit of lower communication

00:08:18,990 --> 00:08:24,220
communication time to your computer and

00:08:21,970 --> 00:08:27,820
this is before we start talking about

00:08:24,220 --> 00:08:30,120
some of the benefits of the p100 GPU

00:08:27,820 --> 00:08:35,440
itself which is a tremendous tremendous

00:08:30,120 --> 00:08:37,719
tremendous tool for deep learning so in

00:08:35,440 --> 00:08:41,890
practice what that has given us is a

00:08:37,719 --> 00:08:45,330
system that is about 2.2 times faster

00:08:41,890 --> 00:08:49,089
than the previous generation of an m40

00:08:45,330 --> 00:08:53,950
an m40 based deep learning system that's

00:08:49,089 --> 00:08:58,900
pretty good for a one-to-one comparison

00:08:53,950 --> 00:09:02,080
what I don't show here is that a 4 p

00:08:58,900 --> 00:09:04,870
100mb link system is about thirty

00:09:02,080 --> 00:09:08,350
percent faster than an h eight GPU and

00:09:04,870 --> 00:09:11,710
40 based system so again tremendous

00:09:08,350 --> 00:09:14,830
tremendous increase in capability both

00:09:11,710 --> 00:09:17,530
through the through the p100

00:09:14,830 --> 00:09:20,760
enhancements but also through the

00:09:17,530 --> 00:09:20,760
capability of envy link

00:09:22,860 --> 00:09:27,630
so as a company we really wanted to

00:09:25,260 --> 00:09:30,600
focus on creating business value around

00:09:27,630 --> 00:09:32,640
deep learning we see our opportunity

00:09:30,600 --> 00:09:34,860
here as helping enterprise customers who

00:09:32,640 --> 00:09:37,890
are all thinking about how they're going

00:09:34,860 --> 00:09:41,250
to attack this problem find a way to put

00:09:37,890 --> 00:09:44,280
it into production so we bring the value

00:09:41,250 --> 00:09:47,040
of helping the data scientists get to

00:09:44,280 --> 00:09:49,470
production quickly reduce that training

00:09:47,040 --> 00:09:51,360
time through enhanced performance reduce

00:09:49,470 --> 00:09:55,950
that provisioning time through

00:09:51,360 --> 00:09:59,210
simplification of the packaging and of

00:09:55,950 --> 00:10:01,950
the distribution we also bring a

00:09:59,210 --> 00:10:04,980
remarkable amount of flexibility to your

00:10:01,950 --> 00:10:08,310
infrastructure the minsky systems are

00:10:04,980 --> 00:10:11,960
extremely cost competitive so when you

00:10:08,310 --> 00:10:14,190
look at them versus a more purpose-built

00:10:11,960 --> 00:10:17,400
purpose-built deep learning system like

00:10:14,190 --> 00:10:21,660
the DG x1 you can get roughly two for

00:10:17,400 --> 00:10:25,070
one to two for the price of one which is

00:10:21,660 --> 00:10:28,290
a heck of a deal and even more important

00:10:25,070 --> 00:10:30,510
these are flexible flexible systems they

00:10:28,290 --> 00:10:32,340
can be a part of your HPC infrastructure

00:10:30,510 --> 00:10:35,840
can be a part of your deep learning and

00:10:32,340 --> 00:10:39,950
infrastructure they can be core to your

00:10:35,840 --> 00:10:44,570
accelerated accelerated database and

00:10:39,950 --> 00:10:48,660
OLAP infrastructure and also importantly

00:10:44,570 --> 00:10:50,880
we're first to market because as Tim

00:10:48,660 --> 00:10:55,340
mentioned we had the foresight to

00:10:50,880 --> 00:10:59,190
recognize a fantastic technology we are

00:10:55,340 --> 00:11:02,340
we are now about 60 days into selling or

00:10:59,190 --> 00:11:05,910
minsky systems and so this is probably

00:11:02,340 --> 00:11:07,920
the best quickest and I would submit

00:11:05,910 --> 00:11:12,690
fastest way to get your whole get

00:11:07,920 --> 00:11:14,160
yourself a hold of a p100 GPU I will be

00:11:12,690 --> 00:11:16,830
available here to talk a little bit more

00:11:14,160 --> 00:11:19,500
about the offering I'm happy to talk at

00:11:16,830 --> 00:11:21,300
more detail about what went into some of

00:11:19,500 --> 00:11:23,250
the benchmarks because I understand

00:11:21,300 --> 00:11:25,830
those could be technically interesting

00:11:23,250 --> 00:11:29,090
to folks and I really appreciate your

00:11:25,830 --> 00:11:29,090
time thank you

00:11:42,870 --> 00:11:44,900

YouTube URL: https://www.youtube.com/watch?v=pt-ERjy8aMU


