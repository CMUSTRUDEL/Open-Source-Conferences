Title: OpenPOWER Summit Europe 2018: What They Didn't Tell You When You Started Developing on POWER
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Richard Heyns, CEO, Brytlyt, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,359 --> 00:00:04,859
my name is Richard Haines I'm the

00:00:03,000 --> 00:00:07,379
founder and CEO of a company called

00:00:04,859 --> 00:00:11,040
bright lights and I'm going to be

00:00:07,379 --> 00:00:14,280
talking along with Jakob who's our head

00:00:11,040 --> 00:00:17,609
of R&D about some of our experiences

00:00:14,280 --> 00:00:19,560
with porting to power a lot of the

00:00:17,609 --> 00:00:22,109
presentation is based on our experience

00:00:19,560 --> 00:00:26,519
with power 8 and recently we've updated

00:00:22,109 --> 00:00:29,099
that to power 9 as well and very much

00:00:26,519 --> 00:00:32,030
focused on getting the most out of the

00:00:29,099 --> 00:00:35,010
GPU side of things and of course NV link

00:00:32,030 --> 00:00:41,219
and NV link between the host and the

00:00:35,010 --> 00:00:42,270
device so the company is called bright

00:00:41,219 --> 00:00:45,120
lights I'm going to give you a little

00:00:42,270 --> 00:00:46,829
spiel about the company to start off a

00:00:45,120 --> 00:00:49,620
bit of an intro and we are using

00:00:46,829 --> 00:00:52,050
graphics processing units to massively

00:00:49,620 --> 00:00:55,350
accelerate data processing and deliver

00:00:52,050 --> 00:00:58,969
speed of thought at a linic analytics at

00:00:55,350 --> 00:01:01,320
scale bright light is a pose grace clone

00:00:58,969 --> 00:01:04,470
so from a user perspective you're

00:01:01,320 --> 00:01:05,670
interacting with a post grades tool but

00:01:04,470 --> 00:01:07,830
everything in the background has been

00:01:05,670 --> 00:01:09,689
developed to run on graphics processor

00:01:07,830 --> 00:01:11,640
units so you get that massive

00:01:09,689 --> 00:01:13,619
improvement in performance and there's a

00:01:11,640 --> 00:01:16,740
couple other tricks in our bag related

00:01:13,619 --> 00:01:20,659
to AI as well but if we could just take

00:01:16,740 --> 00:01:24,150
a step back for a moment Bob Dylan

00:01:20,659 --> 00:01:26,759
talked about times are changing and that

00:01:24,150 --> 00:01:30,509
there was a while ago and I think the

00:01:26,759 --> 00:01:33,930
context of that comment was maybe a bit

00:01:30,509 --> 00:01:36,659
deeper but times are changing things are

00:01:33,930 --> 00:01:38,280
moving forwards and the data that we

00:01:36,659 --> 00:01:39,930
have available and the hardware and

00:01:38,280 --> 00:01:42,990
technology that we have available to us

00:01:39,930 --> 00:01:47,880
today is like nothing before and that is

00:01:42,990 --> 00:01:50,369
moving on massively that's a falcon

00:01:47,880 --> 00:01:53,159
heavy-lift rocket taking off I think

00:01:50,369 --> 00:01:55,500
it's an awesome photo but really that

00:01:53,159 --> 00:01:57,210
symbolizes the kind of hardware

00:01:55,500 --> 00:02:01,380
platforms that we have available to us

00:01:57,210 --> 00:02:02,250
today fantastic hard way that Falcon 9

00:02:01,380 --> 00:02:04,649
rocket

00:02:02,250 --> 00:02:08,789
sorry the the heavy lift the biggest

00:02:04,649 --> 00:02:12,989
rockets in operation today and the

00:02:08,789 --> 00:02:13,910
analogy to that is the IBM power 9

00:02:12,989 --> 00:02:17,540
hardware

00:02:13,910 --> 00:02:19,910
formats designed to connect GPUs

00:02:17,540 --> 00:02:23,930
directly to the motherboard using envy

00:02:19,910 --> 00:02:28,010
link so fantastic hardware and complete

00:02:23,930 --> 00:02:29,810
resources available to us today a little

00:02:28,010 --> 00:02:32,210
bit about how that plays into what

00:02:29,810 --> 00:02:34,580
bright light can do we're a

00:02:32,210 --> 00:02:37,280
gpu-accelerated database and it delivers

00:02:34,580 --> 00:02:40,460
a new paradigm in time to value for

00:02:37,280 --> 00:02:43,070
users the two graphics they're the two

00:02:40,460 --> 00:02:46,120
graphs are performance and memory

00:02:43,070 --> 00:02:49,540
bandwidth the orange lines are GPU

00:02:46,120 --> 00:02:51,950
compared to CPU performance over time

00:02:49,540 --> 00:02:54,710
and the two really interesting

00:02:51,950 --> 00:02:57,620
characteristics there is one GPUs are

00:02:54,710 --> 00:03:01,910
incredibly more capable devices than

00:02:57,620 --> 00:03:05,060
CPUs and that gap is widening over time

00:03:01,910 --> 00:03:07,310
so in the next two to three years cheap

00:03:05,060 --> 00:03:09,320
use for certain data processing

00:03:07,310 --> 00:03:11,510
workloads are going to totally dominate

00:03:09,320 --> 00:03:13,000
what is possible and will be a hardware

00:03:11,510 --> 00:03:16,460
platform for choice

00:03:13,000 --> 00:03:20,830
leaving GPU databases at the pinnacle in

00:03:16,460 --> 00:03:20,830
the evolution of database technologies

00:03:21,850 --> 00:03:28,280
differentiates us for bright lights so

00:03:24,490 --> 00:03:31,580
where our mission statement shall I say

00:03:28,280 --> 00:03:33,790
is to empower organizations through

00:03:31,580 --> 00:03:36,290
transformational data analytics and

00:03:33,790 --> 00:03:39,590
deliver to deliver speed of thought at

00:03:36,290 --> 00:03:40,490
scale and we do that by being the

00:03:39,590 --> 00:03:44,030
recognized

00:03:40,490 --> 00:03:46,580
world leader the fastest database from

00:03:44,030 --> 00:03:48,260
independent benchmarking we've spent

00:03:46,580 --> 00:03:50,620
four years over four years in research

00:03:48,260 --> 00:03:53,060
and development perfecting the product

00:03:50,620 --> 00:03:54,890
we are the only vendor to have

00:03:53,060 --> 00:03:58,820
patent-pending internet intellectual

00:03:54,890 --> 00:04:00,340
property and our fourth generation GPU

00:03:58,820 --> 00:04:04,880
manager bridges the gap between

00:04:00,340 --> 00:04:06,860
artificial intelligence and sequel the

00:04:04,880 --> 00:04:08,690
true value of what bright light can

00:04:06,860 --> 00:04:10,190
deliver is not just the performance or

00:04:08,690 --> 00:04:12,340
being able to run on GPU it's actually

00:04:10,190 --> 00:04:15,290
how their performance is packaged up and

00:04:12,340 --> 00:04:19,310
how we present that capability to our

00:04:15,290 --> 00:04:23,020
users this is just a publicly available

00:04:19,310 --> 00:04:26,870
benchmark done by marketed Pinchak

00:04:23,020 --> 00:04:30,470
pretty straightforward benchmark

00:04:26,870 --> 00:04:33,860
but sir standard benchmarks 1.1 billion

00:04:30,470 --> 00:04:37,580
rows of taxi data for queries pretty

00:04:33,860 --> 00:04:40,070
standard aggregation all queries and

00:04:37,580 --> 00:04:43,880
he's run that on a range of different

00:04:40,070 --> 00:04:46,400
technologies and hardware platforms I've

00:04:43,880 --> 00:04:49,040
just selected out there to right at the

00:04:46,400 --> 00:04:51,949
top is bright light and right at the top

00:04:49,040 --> 00:04:54,139
of that is bright light running on IBM

00:04:51,949 --> 00:04:58,430
Minsky which was the power a platform

00:04:54,139 --> 00:05:01,039
five nodes for GPUs each twenty GPUs and

00:04:58,430 --> 00:05:04,550
compare that to an eleven node eleven

00:05:01,039 --> 00:05:06,710
node AWS cluster running Spock those

00:05:04,550 --> 00:05:09,289
times are in seconds so for that first

00:05:06,710 --> 00:05:11,750
query we were returning an answer in

00:05:09,289 --> 00:05:14,570
five milliseconds on one when one

00:05:11,750 --> 00:05:18,110
billion rows with data spark was taking

00:05:14,570 --> 00:05:20,810
ten seconds to return an equivalent

00:05:18,110 --> 00:05:26,510
result so just transformational between

00:05:20,810 --> 00:05:30,620
600 and 1.2 sorry 1200 times faster than

00:05:26,510 --> 00:05:32,630
spark and this means that queries that

00:05:30,620 --> 00:05:33,200
might run in 5 hours can now run in

00:05:32,630 --> 00:05:35,479
under a minute

00:05:33,200 --> 00:05:37,580
and even queries they might have run 10

00:05:35,479 --> 00:05:40,750
hours on that spark platform can now run

00:05:37,580 --> 00:05:43,099
in under a minute so transformational

00:05:40,750 --> 00:05:46,370
experience for analysts that are using

00:05:43,099 --> 00:05:50,900
these kinds of platforms and processing

00:05:46,370 --> 00:05:54,460
this kind of data there are a range of

00:05:50,900 --> 00:05:58,449
vendors who are offering GPU

00:05:54,460 --> 00:06:00,580
acceleration bright light is one of them

00:05:58,449 --> 00:06:03,620
screams in the room as well

00:06:00,580 --> 00:06:07,580
Karnataka is in lots of Connaught occurs

00:06:03,620 --> 00:06:09,260
here anybody from Karnataka okay so and

00:06:07,580 --> 00:06:13,810
different different vendors are taking a

00:06:09,260 --> 00:06:17,770
different approach to this GPU

00:06:13,810 --> 00:06:20,479
capability right light is looking at

00:06:17,770 --> 00:06:22,220
very much a GPU centric we get the data

00:06:20,479 --> 00:06:24,590
onto the GPUs and use it exclusively

00:06:22,220 --> 00:06:28,190
there that means we can really tap into

00:06:24,590 --> 00:06:31,000
that power and we can deliver one sorry

00:06:28,190 --> 00:06:32,870
4 terabytes a second performance

00:06:31,000 --> 00:06:35,270
Karnataka are very much focused on

00:06:32,870 --> 00:06:37,909
keeping data on CPU and shifting it on

00:06:35,270 --> 00:06:39,950
to GPU which means that they are limited

00:06:37,909 --> 00:06:42,830
by CPU Ram and that

00:06:39,950 --> 00:06:44,890
impact on performance and scream from

00:06:42,830 --> 00:06:48,320
the benchmarks that I've seen

00:06:44,890 --> 00:06:50,150
transferring data from disk to GPU and

00:06:48,320 --> 00:06:52,970
so some of those performance

00:06:50,150 --> 00:06:55,870
characteristics are related to the

00:06:52,970 --> 00:06:59,000
constraints around getting data off disk

00:06:55,870 --> 00:07:00,910
so other delivers extreme performance

00:06:59,000 --> 00:07:05,300
that is enterprise-ready

00:07:00,910 --> 00:07:06,200
real-time performance that is packaged

00:07:05,300 --> 00:07:08,210
up in Postgres

00:07:06,200 --> 00:07:10,580
so all the good things that you can

00:07:08,210 --> 00:07:14,810
imagine that you get out of post grades

00:07:10,580 --> 00:07:17,360
easy integration existing code existing

00:07:14,810 --> 00:07:18,800
investments and SQL code dashboards

00:07:17,360 --> 00:07:21,920
stuff that you might be doing on tableau

00:07:18,800 --> 00:07:29,030
and so on are all capable and will work

00:07:21,920 --> 00:07:31,090
out the box with bright light some

00:07:29,030 --> 00:07:34,010
features and benefits with the product

00:07:31,090 --> 00:07:37,420
so the full integration with Postgres so

00:07:34,010 --> 00:07:40,760
we can take on a range of SQL workloads

00:07:37,420 --> 00:07:42,980
very tight integration with pi torch so

00:07:40,760 --> 00:07:44,930
we can also take on AI workloads and

00:07:42,980 --> 00:07:47,750
this tight integration is actually

00:07:44,930 --> 00:07:49,730
fundamental to the architecture so if

00:07:47,750 --> 00:07:52,790
you think about what pi torches who's

00:07:49,730 --> 00:07:53,600
familiar with pi torch here okay one two

00:07:52,790 --> 00:07:55,070
three four

00:07:53,600 --> 00:07:57,650
okay so pi torches one of the AI

00:07:55,070 --> 00:08:00,110
frameworks like google tensorflow Theano

00:07:57,650 --> 00:08:02,600
care for you those kinds of things and

00:08:00,110 --> 00:08:04,490
these AI frameworks basically work with

00:08:02,600 --> 00:08:07,190
tensors so that's the kind of data

00:08:04,490 --> 00:08:08,870
structure that they consume it's in the

00:08:07,190 --> 00:08:10,480
name Google tensor flow so there's the

00:08:08,870 --> 00:08:13,970
tensor in the actual name of the product

00:08:10,480 --> 00:08:16,730
but a tensor essentially looks like a

00:08:13,970 --> 00:08:18,260
list of data very much like you would

00:08:16,730 --> 00:08:21,280
see a list of data in a column of a

00:08:18,260 --> 00:08:24,380
database and so we actually use tensors

00:08:21,280 --> 00:08:26,930
for the columns in our tables in our

00:08:24,380 --> 00:08:29,540
Postgres database and we also use

00:08:26,930 --> 00:08:32,810
tensors for the machine learning and AI

00:08:29,540 --> 00:08:35,840
workloads and actually those tensors are

00:08:32,810 --> 00:08:38,300
all pi torch tensors

00:08:35,840 --> 00:08:40,130
so we've taken PI torch the memory

00:08:38,300 --> 00:08:42,320
management module within it extended it

00:08:40,130 --> 00:08:45,590
and enhanced it tied it into our post

00:08:42,320 --> 00:08:48,260
grids capability and so you can run

00:08:45,590 --> 00:08:52,070
sequel workloads on GPU using post grids

00:08:48,260 --> 00:08:53,760
and then immediately start running PI

00:08:52,070 --> 00:08:56,040
torch workloads on

00:08:53,760 --> 00:08:57,480
data in those columns without without

00:08:56,040 --> 00:08:59,450
needing to copy it out of the columns

00:08:57,480 --> 00:09:02,190
even so pi torch

00:08:59,450 --> 00:09:06,840
you know believes that those columns or

00:09:02,190 --> 00:09:08,070
actually its own tensors so that's the

00:09:06,840 --> 00:09:09,570
introduction of the product I've got a

00:09:08,070 --> 00:09:10,860
question over there Jakob is about to

00:09:09,570 --> 00:09:13,220
come up but let me just answer that

00:09:10,860 --> 00:09:13,220
question

00:09:34,910 --> 00:09:38,030
sorry could you repeat the question I

00:09:37,040 --> 00:09:47,060
couldn't quite hear what you're saying

00:09:38,030 --> 00:09:49,340
they okay let me let me pick that one up

00:09:47,060 --> 00:09:53,300
first so when we say zero copy we

00:09:49,340 --> 00:09:57,350
literally mean zero copy so when we

00:09:53,300 --> 00:09:59,630
declare a memory space to store data in

00:09:57,350 --> 00:10:02,420
a column for the database for Postgres

00:09:59,630 --> 00:10:05,000
table we will declare a column of data

00:10:02,420 --> 00:10:07,730
basically an array or vector or a tensor

00:10:05,000 --> 00:10:11,360
whatever you like we can then flip a

00:10:07,730 --> 00:10:14,570
pointer and that column can be consumed

00:10:11,360 --> 00:10:17,810
directly into pi torch code as if it was

00:10:14,570 --> 00:10:20,690
a pi torch tensor so you're literally

00:10:17,810 --> 00:10:23,750
reading the data directly from the

00:10:20,690 --> 00:10:26,930
column in the database on GPU into your

00:10:23,750 --> 00:10:29,180
PI torch model that you're not you're

00:10:26,930 --> 00:10:31,820
not copying it out of the column across

00:10:29,180 --> 00:10:33,740
the MV linked or PCIe or you know you're

00:10:31,820 --> 00:10:34,750
not even copying it onto another column

00:10:33,740 --> 00:10:37,760
on the GPU

00:10:34,750 --> 00:10:40,660
there is no copy it directly reads it

00:10:37,760 --> 00:10:43,660
out of the column into the PI torch

00:10:40,660 --> 00:10:43,660
machine

00:11:06,160 --> 00:11:10,190
okay that's probably a bit more

00:11:07,970 --> 00:11:11,600
technical than I can handle so I'll

00:11:10,190 --> 00:11:15,820
leave that to you cool so was there a

00:11:11,600 --> 00:11:15,820
question over here yes sir

00:11:20,050 --> 00:11:23,770
correct yes that's right

00:11:29,560 --> 00:11:37,160
exactly right exactly right that's right

00:11:32,209 --> 00:11:42,709
so so that's our let me just get on to

00:11:37,160 --> 00:11:44,630
that slide this one yeah so when we talk

00:11:42,709 --> 00:11:48,529
about being able to read four terabytes

00:11:44,630 --> 00:11:52,640
a second that's on a a single minsky or

00:11:48,529 --> 00:11:54,380
power nine machine with four GPUs each

00:11:52,640 --> 00:11:58,070
of those GPUs can read data at one

00:11:54,380 --> 00:12:01,190
terabyte a second so that's how we get

00:11:58,070 --> 00:12:03,860
that sort of throughput and you would

00:12:01,190 --> 00:12:05,779
then stack those up if you needed a

00:12:03,860 --> 00:12:08,120
larger data set and you would stack

00:12:05,779 --> 00:12:10,300
those machines up so when we did our

00:12:08,120 --> 00:12:14,150
benchmark for the billion right data set

00:12:10,300 --> 00:12:18,040
we had five machines four GPUs each that

00:12:14,150 --> 00:12:18,040
gave us the memory footprint

00:12:22,630 --> 00:12:31,070
theoretically as far as the terabytes so

00:12:27,920 --> 00:12:34,339
each GPU has its own place is managing

00:12:31,070 --> 00:12:38,270
it and is addressed by an IP and a port

00:12:34,339 --> 00:12:40,490
and we have a controller a hub that

00:12:38,270 --> 00:12:42,709
knows about all the GPUs that are in its

00:12:40,490 --> 00:12:44,630
cluster and how data has been

00:12:42,709 --> 00:12:47,209
distributed onto them so that hub then

00:12:44,630 --> 00:12:50,690
sends commands to the GPUs for pushed on

00:12:47,209 --> 00:12:54,079
processing aggregations sorting for

00:12:50,690 --> 00:12:55,970
sorting filtering push down joints all

00:12:54,079 --> 00:12:59,360
happen on the GPS and the result is so

00:12:55,970 --> 00:13:00,550
this is an analytic database yeah does

00:12:59,360 --> 00:13:03,550
that make sense

00:13:00,550 --> 00:13:03,550
cool

00:13:05,950 --> 00:13:22,840
I'm gonna hand over to Jakob research in

00:13:19,960 --> 00:13:24,670
bright light maybe I start to try to

00:13:22,840 --> 00:13:29,820
answer the question about unified memory

00:13:24,670 --> 00:13:32,770
and possibly manage memory so actually

00:13:29,820 --> 00:13:35,050
we don't need to use unifies memory

00:13:32,770 --> 00:13:37,480
because we store data in a GPO memory

00:13:35,050 --> 00:13:41,080
where we are focusing only to having

00:13:37,480 --> 00:13:43,300
data persistent and a GPO but in a new

00:13:41,080 --> 00:13:46,630
version of our application we will also

00:13:43,300 --> 00:13:48,940
have a mechanism of isometric columns so

00:13:46,630 --> 00:13:52,450
to will be able to keep a part of a

00:13:48,940 --> 00:13:55,840
column in a CPU RAM and the rest of a

00:13:52,450 --> 00:13:59,440
column and in GPU Ram but to swap the

00:13:55,840 --> 00:14:01,630
parts of memory we have our own indexing

00:13:59,440 --> 00:14:02,440
like mechanism using accelerated

00:14:01,630 --> 00:14:06,880
structures

00:14:02,440 --> 00:14:10,180
so if unified memory the way how unified

00:14:06,880 --> 00:14:11,950
memory is swap memory pages is

00:14:10,180 --> 00:14:15,030
implemented by Nvidia and basic and some

00:14:11,950 --> 00:14:17,890
heuristics and because we are a database

00:14:15,030 --> 00:14:20,710
user know what exactly he will be

00:14:17,890 --> 00:14:24,780
sorting on and how he is going to do

00:14:20,710 --> 00:14:27,670
operation he can send up indices indexes

00:14:24,780 --> 00:14:30,880
so we have our own acceleration

00:14:27,670 --> 00:14:33,340
structure to manage memory transfer but

00:14:30,880 --> 00:14:38,220
at this moment we are we are strictly

00:14:33,340 --> 00:14:42,450
strictly GPU memory based products yes

00:14:38,220 --> 00:14:46,840
ok ok so I'm going to tell a few words

00:14:42,450 --> 00:14:49,870
how we moved and how our pipeline for

00:14:46,840 --> 00:14:52,210
compiling for a power 8 and power 9

00:14:49,870 --> 00:14:56,350
looks like so last year we did a

00:14:52,210 --> 00:14:58,770
benchmarks for power 8 recently we

00:14:56,350 --> 00:15:02,260
received an access to power 9 machines

00:14:58,770 --> 00:15:04,120
in general it should be quite easy

00:15:02,260 --> 00:15:07,350
instead first straightforward process

00:15:04,120 --> 00:15:11,230
but it's sometimes it was a little bit

00:15:07,350 --> 00:15:14,160
bumpy road so maybe I start talking

00:15:11,230 --> 00:15:17,910
about how how we did it so basically

00:15:14,160 --> 00:15:19,870
because at the beginning we didn't know

00:15:17,910 --> 00:15:24,250
and usually

00:15:19,870 --> 00:15:27,190
the application creators does not know

00:15:24,250 --> 00:15:30,910
the the vendor of a hardware it's better

00:15:27,190 --> 00:15:33,640
to build software in our own system so

00:15:30,910 --> 00:15:37,300
we need to do a cross compilation to be

00:15:33,640 --> 00:15:40,450
able to compile on x64 machines for

00:15:37,300 --> 00:15:44,050
power machines so to do that we need to

00:15:40,450 --> 00:15:47,380
set up the environment we decided to set

00:15:44,050 --> 00:15:50,710
up the environment within the docker x64

00:15:47,380 --> 00:15:54,130
docker image because it's an easy way to

00:15:50,710 --> 00:15:56,290
prepare some to prepare a whole setup

00:15:54,130 --> 00:15:58,450
for building application and skip it and

00:15:56,290 --> 00:16:02,410
docker happens all developers in our

00:15:58,450 --> 00:16:04,630
company has access to prepared working

00:16:02,410 --> 00:16:07,840
environments so we just started from a

00:16:04,630 --> 00:16:11,620
standard Nvidia imaging Nvidia docker

00:16:07,840 --> 00:16:17,140
repository installed we installed cross

00:16:11,620 --> 00:16:20,830
built tools in that and this was the

00:16:17,140 --> 00:16:23,530
beginning of some problems that's a hard

00:16:20,830 --> 00:16:25,890
ones but the standard we have but we had

00:16:23,530 --> 00:16:28,690
a problems with his installation of

00:16:25,890 --> 00:16:32,920
packages require libraries required for

00:16:28,690 --> 00:16:37,000
our application in to cross-platform

00:16:32,920 --> 00:16:37,780
built using standard apt repository so

00:16:37,000 --> 00:16:40,270
in Terraria

00:16:37,780 --> 00:16:42,760
the syntax here up get installed package

00:16:40,270 --> 00:16:45,970
architecture should work but it didn't

00:16:42,760 --> 00:16:48,970
so many we had to install the libraries

00:16:45,970 --> 00:16:51,430
in target machines and just copy

00:16:48,970 --> 00:16:55,420
libraries and Ben binaries into our

00:16:51,430 --> 00:16:58,560
folders into folders in our in our x64

00:16:55,420 --> 00:17:02,260
image and the same situation was about

00:16:58,560 --> 00:17:05,709
CUDA libraries when we have to have

00:17:02,260 --> 00:17:07,510
compiled libraries for link arena within

00:17:05,709 --> 00:17:15,280
this image so we also just copy that

00:17:07,510 --> 00:17:19,810
from from our target machine also there

00:17:15,280 --> 00:17:21,730
was some work which we had to do if with

00:17:19,810 --> 00:17:23,920
our see Mike files so our application is

00:17:21,730 --> 00:17:27,040
basic and somatic which is very

00:17:23,920 --> 00:17:29,580
convenient for us so here's a list of

00:17:27,040 --> 00:17:29,580
flags

00:17:29,739 --> 00:17:36,450
which we had to add to our build system

00:17:32,999 --> 00:17:39,099
it's quite easy to forget about one and

00:17:36,450 --> 00:17:42,609
have also some problems if additional

00:17:39,099 --> 00:17:45,249
problems with with compilation but in

00:17:42,609 --> 00:17:49,389
general this this list here is a recipe

00:17:45,249 --> 00:17:52,229
how how it can be done quite easily so

00:17:49,389 --> 00:17:58,059
as you can see it's it's a built for

00:17:52,229 --> 00:18:02,559
sm-70 and compiler since also libraries

00:17:58,059 --> 00:18:04,389
for CUDA for power these are the

00:18:02,559 --> 00:18:07,539
libraries which we just copied from a

00:18:04,389 --> 00:18:15,340
target machine into that targets folder

00:18:07,539 --> 00:18:17,440
and subfolder of PPC 64 array so okay so

00:18:15,340 --> 00:18:21,369
our deployment looks like that we have

00:18:17,440 --> 00:18:23,710
64 image we built the bynars here we

00:18:21,369 --> 00:18:27,809
have also in a repose a rigid repository

00:18:23,710 --> 00:18:31,809
and a docker image for power 8 power 9

00:18:27,809 --> 00:18:34,440
we keep that image it can happen during

00:18:31,809 --> 00:18:38,409
how about 90 build so this is automated

00:18:34,440 --> 00:18:40,299
automated part and during our night

00:18:38,409 --> 00:18:42,429
builds the property which can be

00:18:40,299 --> 00:18:46,779
deployed to docker happened next

00:18:42,429 --> 00:18:54,789
downloaded to power I power 8 or power 9

00:18:46,779 --> 00:18:56,950
server yes but so it it was my start we

00:18:54,789 --> 00:19:00,580
have our binaries built and everything

00:18:56,950 --> 00:19:02,229
prepared but we we were it was

00:19:00,580 --> 00:19:04,479
impossible for us to achieve that

00:19:02,229 --> 00:19:06,970
transfer which they promised to us

00:19:04,479 --> 00:19:09,299
actually when we look at look at the

00:19:06,970 --> 00:19:13,809
memory bandwidth it was very similar to

00:19:09,299 --> 00:19:14,909
PCIe transfers so we power 8 so what

00:19:13,809 --> 00:19:17,769
actually happened

00:19:14,909 --> 00:19:20,080
so the next part will be a little bit

00:19:17,769 --> 00:19:23,409
about Numa architecture so as you

00:19:20,080 --> 00:19:25,690
obviously know with power I T power

00:19:23,409 --> 00:19:29,169
rated power 9 system we have a to nuuma

00:19:25,690 --> 00:19:32,950
nodes and to each processor there is an

00:19:29,169 --> 00:19:40,509
animal in connection with nvidia gpus v

00:19:32,950 --> 00:19:42,250
100 in case of power 9 so and also this

00:19:40,509 --> 00:19:45,070
is a diagram of

00:19:42,250 --> 00:19:47,590
power8 machine which we used for our

00:19:45,070 --> 00:19:50,950
benchmarks which which are presented so

00:19:47,590 --> 00:19:54,549
also to each memory memory is connected

00:19:50,950 --> 00:19:56,980
also to each to each Numa node and it

00:19:54,549 --> 00:19:59,799
was and the problem which happens when

00:19:56,980 --> 00:20:02,200
we looked at the locks in a visual

00:19:59,799 --> 00:20:05,110
profiler we didn't have a situation when

00:20:02,200 --> 00:20:09,820
something was serialized it looked like

00:20:05,110 --> 00:20:12,460
just so everything was memory transfer

00:20:09,820 --> 00:20:14,200
was happening in parallel but two

00:20:12,460 --> 00:20:16,510
transfers we cannot observe for

00:20:14,200 --> 00:20:19,720
super-quick with a bandwidth which we

00:20:16,510 --> 00:20:24,130
expected at sea and two other for two

00:20:19,720 --> 00:20:25,840
others GPU were super long and we first

00:20:24,130 --> 00:20:29,230
look we didn't know why it's happening

00:20:25,840 --> 00:20:33,610
and obviously what happened was that we

00:20:29,230 --> 00:20:36,909
allocate memory on host in a memory

00:20:33,610 --> 00:20:40,690
assigned to one man out and we tries to

00:20:36,909 --> 00:20:43,270
do a house to device copy to a stack to

00:20:40,690 --> 00:20:46,809
a GPU connected to second human out so

00:20:43,270 --> 00:20:50,919
okay we are identified that problem by

00:20:46,809 --> 00:20:53,860
using visual profiler but so entirely

00:20:50,919 --> 00:20:59,409
using new Mazda case should be quite

00:20:53,860 --> 00:21:01,720
easy to fix it but it also wasn't so yes

00:20:59,409 --> 00:21:04,090
you have basically it's not a output

00:21:01,720 --> 00:21:07,690
from power machine standards to know my

00:21:04,090 --> 00:21:10,750
notes machine so you can check it check

00:21:07,690 --> 00:21:13,870
the notes how many new my notes and then

00:21:10,750 --> 00:21:16,000
numbers of new my notes using just a

00:21:13,870 --> 00:21:17,679
less CPU flag so this is a standard

00:21:16,000 --> 00:21:21,070
server with to know my notes

00:21:17,679 --> 00:21:24,700
X 60 X 64 architecture and this is how

00:21:21,070 --> 00:21:27,490
the output looks like and you can

00:21:24,700 --> 00:21:30,909
control the assignment of process

00:21:27,490 --> 00:21:35,140
actually not count and not not control

00:21:30,909 --> 00:21:38,650
it's a this API gives you only hints how

00:21:35,140 --> 00:21:42,130
you can provide you a possibility to

00:21:38,650 --> 00:21:43,809
give a hint on which in onenote your

00:21:42,130 --> 00:21:45,400
program your program will be executed

00:21:43,809 --> 00:21:49,240
your process will be executed on and

00:21:45,400 --> 00:21:52,780
learn where you will actually allocate

00:21:49,240 --> 00:21:55,000
your memory and there are two ways to do

00:21:52,780 --> 00:21:56,050
that one way is you can control that

00:21:55,000 --> 00:21:59,740
process from a higher

00:21:56,050 --> 00:22:02,940
level from a level of of operating

00:21:59,740 --> 00:22:06,130
systems so you can just use Numa control

00:22:02,940 --> 00:22:12,370
commands and I will show an example

00:22:06,130 --> 00:22:14,620
later how to use it so we can bind a

00:22:12,370 --> 00:22:19,480
process to a certain to two and one out

00:22:14,620 --> 00:22:22,750
and also try to allocate memory in a ROM

00:22:19,480 --> 00:22:24,250
connected to a node and this is a suture

00:22:22,750 --> 00:22:26,740
dimensional our application is

00:22:24,250 --> 00:22:29,680
cloud-based so we can sort of first

00:22:26,740 --> 00:22:31,750
approach which works quite good was not

00:22:29,680 --> 00:22:33,910
to modify the code of argot of

00:22:31,750 --> 00:22:37,030
application but just we created two

00:22:33,910 --> 00:22:39,310
separate nodes on one machine so - so

00:22:37,030 --> 00:22:41,680
we're on two instances of our

00:22:39,310 --> 00:22:44,500
application on a single machine and so

00:22:41,680 --> 00:22:46,960
we just used that so this is our

00:22:44,500 --> 00:22:48,700
application called view manager is this

00:22:46,960 --> 00:22:51,340
is a process which is responsible for

00:22:48,700 --> 00:22:56,520
computing in our database and we just

00:22:51,340 --> 00:23:00,400
use normal control methods with binding

00:22:56,520 --> 00:23:03,310
CPU and memory to a single lumen note

00:23:00,400 --> 00:23:04,720
and start when we started process and so

00:23:03,310 --> 00:23:08,530
instead of having one process

00:23:04,720 --> 00:23:11,050
controlling whole gpo's we had we just

00:23:08,530 --> 00:23:15,010
created two processes of our application

00:23:11,050 --> 00:23:16,630
its application is is distributed so

00:23:15,010 --> 00:23:19,000
these two process can communicate with

00:23:16,630 --> 00:23:21,220
with a hub so it's actually that doesn't

00:23:19,000 --> 00:23:23,740
matter a lot if you have to process this

00:23:21,220 --> 00:23:25,720
running or a single one of management

00:23:23,740 --> 00:23:27,280
inside a code so this is one one

00:23:25,720 --> 00:23:31,110
approach how we can deal with that

00:23:27,280 --> 00:23:35,260
problem with that problem and the second

00:23:31,110 --> 00:23:37,240
second possibility is to use application

00:23:35,260 --> 00:23:40,720
level animal management so there's also

00:23:37,240 --> 00:23:43,870
a Numa library and set of functions

00:23:40,720 --> 00:23:49,720
which you can use inside your code to do

00:23:43,870 --> 00:23:52,540
that assignment so okay so maybe and it

00:23:49,720 --> 00:23:57,580
was also not again not such easy as we

00:23:52,540 --> 00:24:01,680
thought because there was a lot there

00:23:57,580 --> 00:24:04,780
was a kind of magic in that that a

00:24:01,680 --> 00:24:06,610
certain combination of of these

00:24:04,780 --> 00:24:09,190
functions from this Numa rapid it work

00:24:06,610 --> 00:24:09,970
correctly some other combinations which

00:24:09,190 --> 00:24:12,820
should

00:24:09,970 --> 00:24:16,420
do exactly the same things didn't work

00:24:12,820 --> 00:24:19,380
and actually we didn't know why so I can

00:24:16,420 --> 00:24:23,410
present you a quick example how we can

00:24:19,380 --> 00:24:25,660
allocate memory do a transference do

00:24:23,410 --> 00:24:29,710
computation using using that

00:24:25,660 --> 00:24:31,810
architecture so to do that I use just a

00:24:29,710 --> 00:24:34,620
function run lambda by fret it's not a

00:24:31,810 --> 00:24:37,030
super complicated function it just

00:24:34,620 --> 00:24:44,410
running and lambda by fret using

00:24:37,030 --> 00:24:47,890
standard fret library okay so to do to

00:24:44,410 --> 00:24:50,470
do allocation we just we just first need

00:24:47,890 --> 00:24:53,640
to allocate memory on host to do that we

00:24:50,470 --> 00:24:57,400
use an numeron and note and standard

00:24:53,640 --> 00:25:00,550
default one allocation of memory with

00:24:57,400 --> 00:25:04,480
new operator and this is first link you

00:25:00,550 --> 00:25:07,420
can find in a in documentation that

00:25:04,480 --> 00:25:09,880
there is a function Numa a lock on on

00:25:07,420 --> 00:25:12,610
note which in turn should allocate

00:25:09,880 --> 00:25:16,240
memory for you on a note which we choose

00:25:12,610 --> 00:25:19,240
but unfortunately we found that it was

00:25:16,240 --> 00:25:21,490
work on other architectures here we had

00:25:19,240 --> 00:25:25,110
a problem that with that line of code

00:25:21,490 --> 00:25:29,740
nothing was happening actually but with

00:25:25,110 --> 00:25:34,330
just numeron on note inside threatens

00:25:29,740 --> 00:25:37,330
and and standard allocation it started

00:25:34,330 --> 00:25:40,810
working as we expected and we checked by

00:25:37,330 --> 00:25:49,330
a new markets utility that what it had

00:25:40,810 --> 00:25:52,960
been correctly executed second thing is

00:25:49,330 --> 00:25:54,640
host to device transfer and again by

00:25:52,960 --> 00:25:57,940
looking at Nvidia documentation you can

00:25:54,640 --> 00:26:01,630
entirely find the solution with streams

00:25:57,940 --> 00:26:04,990
so to do a synchronous memory allocation

00:26:01,630 --> 00:26:09,150
and transfer in theory you should we

00:26:04,990 --> 00:26:12,940
should create a streams at addition at

00:26:09,150 --> 00:26:15,580
some flux to compiler use that streams

00:26:12,940 --> 00:26:18,970
use memory allocation and kudamon copy I

00:26:15,580 --> 00:26:22,240
think so we did it exactly and as it was

00:26:18,970 --> 00:26:23,240
in on a Nvidia blog but unfortunately

00:26:22,240 --> 00:26:26,179
again

00:26:23,240 --> 00:26:30,410
when we look at the visual profiler it

00:26:26,179 --> 00:26:33,110
looked like a serial execution so so we

00:26:30,410 --> 00:26:37,160
had a memory transfer one after one

00:26:33,110 --> 00:26:40,820
after one yes until we started playing

00:26:37,160 --> 00:26:43,490
with that a little bit more so so the

00:26:40,820 --> 00:26:47,020
solution proposed an Nvidia block was

00:26:43,490 --> 00:26:50,059
not without using frets just just

00:26:47,020 --> 00:26:52,640
additional flag to compiler memory as in

00:26:50,059 --> 00:26:55,730
copies and and streams but it didn't

00:26:52,640 --> 00:26:58,130
work in this case the solution which

00:26:55,730 --> 00:27:01,720
works correctly was again to set up a

00:26:58,130 --> 00:27:05,809
new man out we didn't need to create

00:27:01,720 --> 00:27:08,840
separate context within the fret what

00:27:05,809 --> 00:27:12,530
was also suggested on some forums and in

00:27:08,840 --> 00:27:16,230
some tutorials but no more anon note was

00:27:12,530 --> 00:27:17,650
enough and could as a device and just

00:27:16,230 --> 00:27:21,380
[Music]

00:27:17,650 --> 00:27:23,330
could analog to allocate memory and so

00:27:21,380 --> 00:27:29,510
this is a standard way but we've

00:27:23,330 --> 00:27:34,520
selected note and this is this is a copy

00:27:29,510 --> 00:27:36,800
part so we didn't manually I created as

00:27:34,520 --> 00:27:41,300
streams but we use default streams and

00:27:36,800 --> 00:27:45,920
and also inside the Run lambda by first

00:27:41,300 --> 00:27:48,860
function we called a copy I think and it

00:27:45,920 --> 00:27:52,250
was a successful solution we achieve we

00:27:48,860 --> 00:27:56,260
achieved this transfer which we expected

00:27:52,250 --> 00:28:00,380
so in total it was 120 gigs per second

00:27:56,260 --> 00:28:03,890
so this is a report from power 9 you can

00:28:00,380 --> 00:28:06,860
easily check check your transfer by just

00:28:03,890 --> 00:28:09,140
so the easiest way to for us to use

00:28:06,860 --> 00:28:11,630
visual profiler everything was doctor

00:28:09,140 --> 00:28:14,150
eyes on external servers so there is

00:28:11,630 --> 00:28:16,850
easiest wise way for us was just to

00:28:14,150 --> 00:28:21,020
generate the output file of visual

00:28:16,850 --> 00:28:23,720
profiler with that comment and download

00:28:21,020 --> 00:28:26,600
that file and next in your local machine

00:28:23,720 --> 00:28:28,880
you can open that file in a visual

00:28:26,600 --> 00:28:32,780
profiler and the transfers which we get

00:28:28,880 --> 00:28:38,080
was was a total parallel transfer of of

00:28:32,780 --> 00:28:38,080
30 I think something is

00:28:38,150 --> 00:28:44,310
yeah III says 41 gigs

00:28:41,190 --> 00:28:47,370
yes 41 gigs per cartons for faces as it

00:28:44,310 --> 00:28:50,010
was power items it was full parallel

00:28:47,370 --> 00:28:55,380
transfer so in total we achieved what

00:28:50,010 --> 00:28:58,590
they promised us so about 124 gigs per

00:28:55,380 --> 00:28:59,880
second and yesterday so it's not

00:28:58,590 --> 00:29:03,030
included in this presentation but

00:28:59,880 --> 00:29:05,250
yesterday we made the same test on power

00:29:03,030 --> 00:29:07,740
night and and we achieved a parallel

00:29:05,250 --> 00:29:12,120
transfer of 70 gigs per second per per

00:29:07,740 --> 00:29:16,760
card so 280 in total so so quite nice

00:29:12,120 --> 00:29:20,300
results I think okay and I think this is

00:29:16,760 --> 00:29:23,340
this is all what I wanted to say about

00:29:20,300 --> 00:29:30,440
about memory transfer and how to achieve

00:29:23,340 --> 00:29:42,050
a promised promised results so reject

00:29:30,440 --> 00:29:42,050
questions thank circle so oh there we go

00:29:59,850 --> 00:30:05,860
okay so for us it's important to be able

00:30:03,429 --> 00:30:08,440
to load data to database quickly to do

00:30:05,860 --> 00:30:09,970
some aggregations so host device is more

00:30:08,440 --> 00:30:12,549
important because we need to load data

00:30:09,970 --> 00:30:14,769
at the beginning of a setup to run

00:30:12,549 --> 00:30:17,830
memory and next quickly transfer the

00:30:14,769 --> 00:30:20,379
data to GPU do some computations and

00:30:17,830 --> 00:30:22,029
your the output of this computation is a

00:30:20,379 --> 00:30:26,409
couple of numbers results of

00:30:22,029 --> 00:30:29,620
aggregations so results were or words

00:30:26,409 --> 00:30:32,909
for a device to house it was in power

00:30:29,620 --> 00:30:36,190
nine inches about 40 instead of 70 but

00:30:32,909 --> 00:30:39,629
but it wasn't very important from a

00:30:36,190 --> 00:30:39,629
point of view of our application

00:30:50,630 --> 00:30:53,630
okay

00:30:54,799 --> 00:31:04,220
okay okay okay I will look at it yeah

00:31:06,229 --> 00:31:13,559
okay so that's how we achieved full

00:31:11,639 --> 00:31:16,799
bandwidth or close to full bandwidth on

00:31:13,559 --> 00:31:20,609
on envy link from the motherboard

00:31:16,799 --> 00:31:22,710
through the GPS to the hosts and

00:31:20,609 --> 00:31:25,950
realized the fantastic potential that we

00:31:22,710 --> 00:31:27,269
have with power 9 just a little bit

00:31:25,950 --> 00:31:31,830
about how that then ties into the

00:31:27,269 --> 00:31:33,509
product itself so bright lights as

00:31:31,830 --> 00:31:37,349
opposed to his clone we've done a lot of

00:31:33,509 --> 00:31:40,559
work rewriting the database engine to

00:31:37,349 --> 00:31:44,190
actually run commands via our GPU

00:31:40,559 --> 00:31:45,989
manager the GPU manager controls the

00:31:44,190 --> 00:31:49,019
GPUs and we actually have a dedicated

00:31:45,989 --> 00:31:51,720
GPU manager for each collection of GPU

00:31:49,019 --> 00:31:54,029
so in the example that Jakob was talking

00:31:51,720 --> 00:31:56,460
about there are two GPU managers on a

00:31:54,029 --> 00:31:59,489
single power nine server and you might

00:31:56,460 --> 00:32:01,409
have five servers so you'd have 10 GPU

00:31:59,489 --> 00:32:06,419
managers and they all interact with each

00:32:01,409 --> 00:32:10,669
other using IP GPG manager controls the

00:32:06,419 --> 00:32:13,289
GPUs but from a user perspective

00:32:10,669 --> 00:32:16,470
everything is Postgres so they interact

00:32:13,289 --> 00:32:21,929
with post greatest tools all the third

00:32:16,470 --> 00:32:24,029
party tools and pieces of software that

00:32:21,929 --> 00:32:26,429
are out there will you will work so for

00:32:24,029 --> 00:32:30,389
instance tableau has post grades

00:32:26,429 --> 00:32:33,359
connector a standard and you use that to

00:32:30,389 --> 00:32:37,259
connect to a bright light database we

00:32:33,359 --> 00:32:39,570
use PG admin as our command console and

00:32:37,259 --> 00:32:42,239
we use P SQL for our command line so all

00:32:39,570 --> 00:32:44,909
those things work out the box things

00:32:42,239 --> 00:32:51,690
like stored procedures all of the

00:32:44,909 --> 00:32:54,929
Postgres PL PG SQL works curses

00:32:51,690 --> 00:32:57,139
Jason user-defined functions we use a

00:32:54,929 --> 00:32:59,220
lot of user-defined functions ourselves

00:32:57,139 --> 00:33:01,349
foreign data wrappers is also something

00:32:59,220 --> 00:33:03,749
that's really cool with with post grades

00:33:01,349 --> 00:33:06,720
it makes it very easy to connect to

00:33:03,749 --> 00:33:08,670
existing data sources with a single line

00:33:06,720 --> 00:33:11,310
of code you can

00:33:08,670 --> 00:33:13,440
present them on the platform as a table

00:33:11,310 --> 00:33:15,090
and you can start running SQL and

00:33:13,440 --> 00:33:20,940
extracting data from existing data

00:33:15,090 --> 00:33:23,000
sources so how does the whole stack so

00:33:20,940 --> 00:33:26,640
together in the in the middle you've got

00:33:23,000 --> 00:33:29,160
bright light software the bright light

00:33:26,640 --> 00:33:31,590
API basically that the GPU manager and

00:33:29,160 --> 00:33:33,950
that can scale out in blue you've got

00:33:31,590 --> 00:33:37,110
all the tools that you can use to

00:33:33,950 --> 00:33:39,450
connect to the database with starting

00:33:37,110 --> 00:33:41,340
with most grits so full-blown everything

00:33:39,450 --> 00:33:44,130
you need from an SQL perspective with

00:33:41,340 --> 00:33:46,370
Postgres on the right-hand side all the

00:33:44,130 --> 00:33:49,650
visualization tools tableau

00:33:46,370 --> 00:33:52,170
MicroStrategy power bi they all come

00:33:49,650 --> 00:33:54,840
with Postgres connectors so they will

00:33:52,170 --> 00:33:58,050
work after box except they will work no

00:33:54,840 --> 00:33:59,790
thousand times faster we also developed

00:33:58,050 --> 00:34:04,050
our own visualization capability as well

00:33:59,790 --> 00:34:06,960
because for two reasons one is products

00:34:04,050 --> 00:34:09,000
like tableau do have some overheads and

00:34:06,960 --> 00:34:11,880
even when you hook them up to very fast

00:34:09,000 --> 00:34:14,370
databases tableau slows things down so

00:34:11,880 --> 00:34:16,679
we've got a visualization tool it gets

00:34:14,370 --> 00:34:19,560
around some of those overheads but even

00:34:16,679 --> 00:34:21,600
more importantly very few visualization

00:34:19,560 --> 00:34:23,880
tools today can actually cope with large

00:34:21,600 --> 00:34:26,040
amounts of geospatial data and that's

00:34:23,880 --> 00:34:29,159
because they designed to run the

00:34:26,040 --> 00:34:31,380
rendering on client so for tableau all

00:34:29,159 --> 00:34:33,600
the images you see are actually rendered

00:34:31,380 --> 00:34:35,700
from data that comes and lands on the

00:34:33,600 --> 00:34:39,060
client and if you've got a billion rows

00:34:35,700 --> 00:34:40,679
of geospatial data to start mapping

00:34:39,060 --> 00:34:41,850
those points onto a map it's just not

00:34:40,679 --> 00:34:45,630
going to work because it would have to

00:34:41,850 --> 00:34:49,169
all be done on the client and then

00:34:45,630 --> 00:34:51,030
there's torch PI torch and Jupiter tight

00:34:49,169 --> 00:34:53,850
integration which gives us the zero copy

00:34:51,030 --> 00:34:54,570
between sequel workloads and AI

00:34:53,850 --> 00:34:57,120
workloads

00:34:54,570 --> 00:34:59,160
at the bottom very easy to get data into

00:34:57,120 --> 00:35:02,270
the platform using foreign data wrappers

00:34:59,160 --> 00:35:04,910
as I mentioned a little bit earlier

00:35:02,270 --> 00:35:09,150
those quizzes are open source database

00:35:04,910 --> 00:35:11,820
there are something like 60 foreign data

00:35:09,150 --> 00:35:14,490
wrappers in the community today and so

00:35:11,820 --> 00:35:16,050
pretty much every single data source

00:35:14,490 --> 00:35:17,310
that is out there has a foreign data

00:35:16,050 --> 00:35:18,780
wrapper that you can then connect you

00:35:17,310 --> 00:35:21,680
directly from bright light to start

00:35:18,780 --> 00:35:26,190
getting data into the platform

00:35:21,680 --> 00:35:29,309
two main products is the database which

00:35:26,190 --> 00:35:31,260
is a Postgres clone so from a user

00:35:29,309 --> 00:35:34,319
perspective it looks like those graves

00:35:31,260 --> 00:35:37,950
and acts like Postgres except it's much

00:35:34,319 --> 00:35:39,690
much faster running on GPU and then

00:35:37,950 --> 00:35:42,630
spotlight which is our analytics

00:35:39,690 --> 00:35:44,849
workbench tight integration with PI

00:35:42,630 --> 00:35:47,609
torch so you can combine SQL and AI

00:35:44,849 --> 00:35:51,329
workloads very tight integration with

00:35:47,609 --> 00:35:53,520
Jupiter so you can using a browser

00:35:51,329 --> 00:35:55,140
front-end do all your visual analytics

00:35:53,520 --> 00:35:58,260
so you're charting your geo spatial

00:35:55,140 --> 00:35:59,819
analytics sequel editor for when its

00:35:58,260 --> 00:36:02,549
sequel editor you can do all your DML

00:35:59,819 --> 00:36:04,829
DDL right stored procedures user-defined

00:36:02,549 --> 00:36:07,880
functions in that SQL editor and this

00:36:04,829 --> 00:36:10,680
was all within a browser single sign-in

00:36:07,880 --> 00:36:13,440
you've got tight integration with

00:36:10,680 --> 00:36:17,569
Jupiter so you can write all your Python

00:36:13,440 --> 00:36:22,440
PI torch code and we also have a tool

00:36:17,569 --> 00:36:24,750
for task orchestration and creating data

00:36:22,440 --> 00:36:27,660
pipelines so you can connect up tasks

00:36:24,750 --> 00:36:30,720
hook them up in in any combination and

00:36:27,660 --> 00:36:32,670
started to build ETL pipelines data

00:36:30,720 --> 00:36:35,630
enrichment pipelines machine learning

00:36:32,670 --> 00:36:39,390
pipelines all of which is running SQL or

00:36:35,630 --> 00:36:45,150
PI torch but everything's happening on

00:36:39,390 --> 00:36:47,609
GPU very very efficiently so that is the

00:36:45,150 --> 00:36:55,440
end of the presentation I hope it was

00:36:47,609 --> 00:36:57,619
useful any any questions comments yes

00:36:55,440 --> 00:36:57,619
sir

00:37:12,930 --> 00:37:20,450
what do you do is I hope you don't mind

00:37:17,280 --> 00:37:25,230
me taking that coupe there's an answer

00:37:20,450 --> 00:37:28,110
let's jump on to that so actually it's

00:37:25,230 --> 00:37:31,730
gone to the architecture so when you

00:37:28,110 --> 00:37:33,990
create a table create table statement

00:37:31,730 --> 00:37:36,000
you either create a normal post grist

00:37:33,990 --> 00:37:37,680
table and then it's status that data

00:37:36,000 --> 00:37:40,260
goes into disk and and you would use it

00:37:37,680 --> 00:37:43,440
as opposed to a table or you say I'm

00:37:40,260 --> 00:37:45,390
gonna create a GPE table and the memory

00:37:43,440 --> 00:37:48,240
gets allocated the columns get allocated

00:37:45,390 --> 00:37:51,960
space gets allocated and the data gets

00:37:48,240 --> 00:37:54,240
piped onto GPU so you create the space

00:37:51,960 --> 00:37:55,950
and the data goes on to raw data on to

00:37:54,240 --> 00:37:59,850
GPU the array or there are some elements

00:37:55,950 --> 00:38:01,050
where we compress the data so there are

00:37:59,850 --> 00:38:06,510
columns that we have different

00:38:01,050 --> 00:38:09,870
compression methods just for state space

00:38:06,510 --> 00:38:12,240
space and performance but otherwise the

00:38:09,870 --> 00:38:14,970
raw data goes onto the GPUs you then

00:38:12,240 --> 00:38:16,610
send a query whatever those queries are

00:38:14,970 --> 00:38:18,990
might be an aggregation might be

00:38:16,610 --> 00:38:23,180
aggregation and a group by a filter and

00:38:18,990 --> 00:38:23,180
a join with some expressions

00:38:37,359 --> 00:38:40,359
so

00:38:49,900 --> 00:38:55,030
yeah absolutely

00:38:51,590 --> 00:38:55,030
yes yes

00:39:20,260 --> 00:39:28,190
yeah there's a there's yep okay I got

00:39:25,040 --> 00:39:29,150
the question so the other question is so

00:39:28,190 --> 00:39:31,190
there's there's a bunch of things that

00:39:29,150 --> 00:39:33,950
you're asking what some of them are

00:39:31,190 --> 00:39:36,220
technical issues around GPU perform a

00:39:33,950 --> 00:39:38,180
GPU footprint memory footprint

00:39:36,220 --> 00:39:40,790
architecture how you can bring as much

00:39:38,180 --> 00:39:42,560
cheap you foot ram into play and then

00:39:40,790 --> 00:39:44,120
also you know hardly good data sets

00:39:42,560 --> 00:39:46,970
you're working with just that's a

00:39:44,120 --> 00:39:49,100
philosophy philosophy cool question that

00:39:46,970 --> 00:39:52,490
I'm going to answer leave to the end but

00:39:49,100 --> 00:39:59,000
the first thing is so GPUs today can get

00:39:52,490 --> 00:40:03,710
32 gig ok power 9 has got 4 sometimes 6

00:39:59,000 --> 00:40:05,960
GPUs the power 9 architecture has NV

00:40:03,710 --> 00:40:08,210
link to the motherboard which no other

00:40:05,960 --> 00:40:10,220
architecture has which means that you

00:40:08,210 --> 00:40:14,380
can actually in a modular way stack

00:40:10,220 --> 00:40:19,400
those servers up and we can bring 10

00:40:14,380 --> 00:40:22,610
power 9 servers into play each with 128

00:40:19,400 --> 00:40:24,320
gig of GPU Ram so 10 servers are going

00:40:22,610 --> 00:40:27,050
to give you one point two terabytes of

00:40:24,320 --> 00:40:29,210
GPU Ram but if you look at some of the

00:40:27,050 --> 00:40:33,860
other platforms that are on the market

00:40:29,210 --> 00:40:36,800
today Nvidia have released GTX - it's

00:40:33,860 --> 00:40:40,760
got 16 GPUs each with 32 gig of ram

00:40:36,800 --> 00:40:44,960
that's half a terabyte of GPU Ram for a

00:40:40,760 --> 00:40:46,040
single machine ok so so it's about 300k

00:40:44,960 --> 00:40:49,150
right so it's quite an expensive machine

00:40:46,040 --> 00:40:51,190
right so it's not so cheap but I'm

00:40:49,150 --> 00:40:54,980
that's where we are today

00:40:51,190 --> 00:40:56,780
ai machine learning is driving a huge

00:40:54,980 --> 00:40:58,730
investment from Nvidia so that's not the

00:40:56,780 --> 00:40:59,840
stylet that's not the that you know two

00:40:58,730 --> 00:41:03,860
or three years time that's going to

00:40:59,840 --> 00:41:06,230
double again and the same comment was

00:41:03,860 --> 00:41:07,820
made about a memory databases five years

00:41:06,230 --> 00:41:09,380
ago you know who would have thought that

00:41:07,820 --> 00:41:11,570
you put the whole database into in

00:41:09,380 --> 00:41:14,500
memory but people do that in some

00:41:11,570 --> 00:41:17,540
circumstances there's there's a gaming

00:41:14,500 --> 00:41:19,250
company in the UK King you do candy

00:41:17,540 --> 00:41:22,190
crush and so on they've got a huge

00:41:19,250 --> 00:41:23,240
investment in in memory databases with

00:41:22,190 --> 00:41:27,230
something like a

00:41:23,240 --> 00:41:29,740
a 10 or 20 terabyte in-memory solution

00:41:27,230 --> 00:41:31,369
and when you look at some of the spark

00:41:29,740 --> 00:41:35,390
installations those are awesome a

00:41:31,369 --> 00:41:39,200
terabyte so one can definitely start to

00:41:35,390 --> 00:41:42,080
see GPUs moving in that way as well the

00:41:39,200 --> 00:41:44,750
other thing is yes there is there are

00:41:42,080 --> 00:41:46,940
instances where you know the cost is

00:41:44,750 --> 00:41:51,020
just too great and one wants to then

00:41:46,940 --> 00:41:53,660
dial back and maybe lean on CPU RAM and

00:41:51,020 --> 00:41:54,770
absolutely architectures can then cope

00:41:53,660 --> 00:41:56,840
with that but obviously there's then a

00:41:54,770 --> 00:41:58,730
performance compromise because you're

00:41:56,840 --> 00:42:01,280
just starting to make greater use of

00:41:58,730 --> 00:42:03,140
slower Hardware but the other thing is I

00:42:01,280 --> 00:42:07,220
think you know the philosophy cool side

00:42:03,140 --> 00:42:10,520
of things and data is we've been almost

00:42:07,220 --> 00:42:12,260
spoiled by Harvard OOP with with the

00:42:10,520 --> 00:42:15,830
advent of a deep where it was actually

00:42:12,260 --> 00:42:17,470
very cheap to gather and store huge

00:42:15,830 --> 00:42:20,950
amounts of data and really the whole

00:42:17,470 --> 00:42:23,270
selling of the deep has relied on us

00:42:20,950 --> 00:42:26,420
gathering and storing huge amounts of

00:42:23,270 --> 00:42:28,550
data but if you look at data and you

00:42:26,420 --> 00:42:31,970
look at its value you look at relevance

00:42:28,550 --> 00:42:34,070
sometime Venus there's a there's a small

00:42:31,970 --> 00:42:37,430
section of really high value data that's

00:42:34,070 --> 00:42:40,250
relevant and timely and then a huge tale

00:42:37,430 --> 00:42:43,160
of redundant data you know who cares

00:42:40,250 --> 00:42:44,240
what ray blocks is were shoving out two

00:42:43,160 --> 00:42:46,700
years ago what we're really interested

00:42:44,240 --> 00:42:48,440
in say from a retail perspective is what

00:42:46,700 --> 00:42:50,180
our customers were doing yesterday and

00:42:48,440 --> 00:42:52,010
today or what our web logs are doing

00:42:50,180 --> 00:42:54,530
right now and that's a much smaller

00:42:52,010 --> 00:42:57,790
segment of information but for higher

00:42:54,530 --> 00:42:57,790
value so what we

00:44:28,210 --> 00:44:30,930
yes sir

00:44:51,730 --> 00:44:55,420
yes that's right yeah

00:45:05,480 --> 00:45:10,440
well not not in a file system because

00:45:08,010 --> 00:45:12,810
it's all it's all on GPU so if you turn

00:45:10,440 --> 00:45:15,410
the Machine off and then you've got to

00:45:12,810 --> 00:45:15,410
reload from

00:46:22,749 --> 00:46:27,109
you know I agree that there's there's

00:46:25,369 --> 00:46:29,269
different the different use cases and

00:46:27,109 --> 00:46:32,059
approaches and and I think absolutely

00:46:29,269 --> 00:46:35,450
scream is taking narrator from technical

00:46:32,059 --> 00:46:37,460
approach to - to the solution and that

00:46:35,450 --> 00:46:39,739
means that the use cases that they look

00:46:37,460 --> 00:46:41,210
at are very different so we're not we're

00:46:39,739 --> 00:46:44,359
not looking at petabyte databases

00:46:41,210 --> 00:46:47,329
definitely not that's not what we are so

00:46:44,359 --> 00:46:49,039
I would say up to 10 20 terabytes

00:46:47,329 --> 00:46:52,849
it does depend a little bit on the

00:46:49,039 --> 00:46:55,279
budget of the customer but if you know

00:46:52,849 --> 00:46:57,349
some of the banks have 40,000 40,000

00:46:55,279 --> 00:46:59,839
GPUs to run

00:46:57,349 --> 00:47:02,599
that's for Monte Carlo right but there

00:46:59,839 --> 00:47:11,420
are organizations that invest very large

00:47:02,599 --> 00:47:13,970
amounts on GP par yeah 40 terabytes is

00:47:11,420 --> 00:47:20,450
not what we are that's for scream scream

00:47:13,970 --> 00:47:22,839
can take out those workloads there we

00:47:20,450 --> 00:47:22,839
could look at

00:47:35,520 --> 00:47:41,350
no-no-no though it would be distributed

00:47:38,440 --> 00:47:44,310
so each column will be distributed

00:47:41,350 --> 00:47:44,310
across all the GPS

00:49:13,359 --> 00:49:18,839
director

00:49:15,980 --> 00:49:23,180
the distribution key so that's a that's

00:49:18,839 --> 00:49:25,499
a being solved with MPP databases so you

00:49:23,180 --> 00:49:27,630
take a key and you distribute it on that

00:49:25,499 --> 00:49:36,710
key so it must be it might be customer

00:49:27,630 --> 00:49:36,710
ID for instance depends on the query

00:50:22,500 --> 00:50:28,660
so just the just the the

00:50:26,250 --> 00:50:31,599
distinction about row orientated and

00:50:28,660 --> 00:50:34,740
column orientated : orientated was very

00:50:31,599 --> 00:50:38,349
popular with spinach spinning media

00:50:34,740 --> 00:50:42,040
because of reading the way the data was

00:50:38,349 --> 00:50:46,119
read and seek times but with SSDs and

00:50:42,040 --> 00:50:52,300
nvme random access is almost as quick as

00:50:46,119 --> 00:50:53,890
a serial axis so storing data on disk in

00:50:52,300 --> 00:50:55,750
a commoner format doesn't really matter

00:50:53,890 --> 00:50:58,510
you can do it on row based or or

00:50:55,750 --> 00:51:00,819
columnar but I think what's important is

00:50:58,510 --> 00:51:02,740
we are focusing on what happens on the

00:51:00,819 --> 00:51:05,530
GPUs to get the best performance out of

00:51:02,740 --> 00:51:08,530
them we are common columnar because

00:51:05,530 --> 00:51:11,589
we're an analytic database like Jakob

00:51:08,530 --> 00:51:13,540
said once that data is on GPU there is a

00:51:11,589 --> 00:51:16,510
mechanism to drop it to disk and reload

00:51:13,540 --> 00:51:20,339
it very quickly as fast as it'll go over

00:51:16,510 --> 00:51:24,760
the nvme link so that sorry the env link

00:51:20,339 --> 00:51:26,020
which is 380 Giga second per server so

00:51:24,760 --> 00:51:29,410
once you've got three servers you got

00:51:26,020 --> 00:51:31,770
close to the terabytes per second for

00:51:29,410 --> 00:51:31,770
data loading

00:52:02,610 --> 00:52:06,640
so I think we're talking a lot about

00:52:04,810 --> 00:52:08,310
scream which is a you know a different

00:52:06,640 --> 00:52:10,720
solution

00:52:08,310 --> 00:52:16,480
no no but we've talked about it quite a

00:52:10,720 --> 00:52:18,850
bit now yeah they can help you out

00:52:16,480 --> 00:52:29,590
answer those questions as anybody I saw

00:52:18,850 --> 00:52:32,770
there was some other questions okay we

00:52:29,590 --> 00:52:35,800
don't support transactional workloads so

00:52:32,770 --> 00:52:39,120
very much analytic workloads load and

00:52:35,800 --> 00:52:39,120
then and then read basically

00:52:57,480 --> 00:53:03,010
yet safety if you if you look at so you

00:53:00,820 --> 00:53:05,800
could run this our software is on AWS

00:53:03,010 --> 00:53:08,920
and if you did exactly we actually there

00:53:05,800 --> 00:53:11,740
was a Twitter was a Twitter engagement

00:53:08,920 --> 00:53:13,840
between mathy and bright light when that

00:53:11,740 --> 00:53:17,140
was announced they made exactly the same

00:53:13,840 --> 00:53:19,180
point and we did say we have run on AWS

00:53:17,140 --> 00:53:21,370
and if you want to through the similar

00:53:19,180 --> 00:53:24,070
benchmark using the same AWS odd way

00:53:21,370 --> 00:53:26,080
which we've got in the second line there

00:53:24,070 --> 00:53:29,280
that's AWS you're welcome to do that

00:53:26,080 --> 00:53:41,080
they didn't take up that opportunity I

00:53:29,280 --> 00:53:42,940
imagine that's because not the map the

00:53:41,080 --> 00:53:45,370
map T is a very fast database they're

00:53:42,940 --> 00:53:50,400
also a GPU database so we about for

00:53:45,370 --> 00:53:53,370
about four times faster than mathy yeah

00:53:50,400 --> 00:53:55,600
so performance was pretty comparable

00:53:53,370 --> 00:53:59,110
there happens to be also the only one

00:53:55,600 --> 00:54:00,190
that's focused on GPU acceleration where

00:53:59,110 --> 00:54:01,150
the other vendors are looking at

00:54:00,190 --> 00:54:02,590
different solutions

00:54:01,150 --> 00:54:04,690
you know we've discussed our scream is

00:54:02,590 --> 00:54:10,990
very much able to take on large

00:54:04,690 --> 00:54:12,550
workloads that rely on disk as well but

00:54:10,990 --> 00:54:14,230
when you look at the dedicated GPU

00:54:12,550 --> 00:54:16,450
databases bright light and that they are

00:54:14,230 --> 00:54:19,390
really the only ones that have that kind

00:54:16,450 --> 00:54:21,850
of capability and the differentiator

00:54:19,390 --> 00:54:24,190
between us we don't just think about

00:54:21,850 --> 00:54:25,630
performance we think about how usable

00:54:24,190 --> 00:54:28,090
the platform is the tool sets that

00:54:25,630 --> 00:54:30,370
really empowered so performance is one

00:54:28,090 --> 00:54:33,880
thing but without an effective tool

00:54:30,370 --> 00:54:35,530
chain with SQL pose Greer's store

00:54:33,880 --> 00:54:39,160
procedures user-defined functions and

00:54:35,530 --> 00:54:41,080
all the the PI torch tool chain you know

00:54:39,160 --> 00:54:42,790
you don't get to be able to use that

00:54:41,080 --> 00:54:49,120
performance with bright light you

00:54:42,790 --> 00:54:50,800
absolutely can no we haven't haven't

00:54:49,120 --> 00:54:56,920
been able to it's quite a difficult one

00:54:50,800 --> 00:54:59,380
to get off Hana will say that if you

00:54:56,920 --> 00:55:01,300
speak to X or soul they they've got some

00:54:59,380 --> 00:55:02,560
very interesting T PCH benchmarks they

00:55:01,300 --> 00:55:05,370
will say in memory and they're probably

00:55:02,560 --> 00:55:05,370
faster then

00:55:16,180 --> 00:55:25,280
yeah kdb I think is in memory Spock that

00:55:21,230 --> 00:55:30,220
this this Spock was in memory that's an

00:55:25,280 --> 00:55:33,430
in every older memory yep

00:55:30,220 --> 00:55:33,430

YouTube URL: https://www.youtube.com/watch?v=mhehBBS9F9k


