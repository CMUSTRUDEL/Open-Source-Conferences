Title: OpenPOWER Summit Europe 2018: Combine Out of Band Monitoring with AI and Big Data
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Andrea Bartolini, University of Bologna, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:06,160 --> 00:00:09,460
good morning everyone so I'm Andrew

00:00:08,320 --> 00:00:12,070
Bertolini I'm assistant professor

00:00:09,460 --> 00:00:14,740
University of Bologna and today I'm here

00:00:12,070 --> 00:00:16,690
to present our work on combining how to

00:00:14,740 --> 00:00:19,210
have been monitoring with a I am big

00:00:16,690 --> 00:00:22,330
data for data center automation in open

00:00:19,210 --> 00:00:25,930
power so during this presentation I will

00:00:22,330 --> 00:00:28,180
start seeing start by presenting you the

00:00:25,930 --> 00:00:31,690
concept of data center automation what

00:00:28,180 --> 00:00:33,520
is 10 force and what its its aim then I

00:00:31,690 --> 00:00:35,469
will go through the work that we have

00:00:33,520 --> 00:00:38,050
done on The Da Vida system open power

00:00:35,469 --> 00:00:41,370
system located in Vilonia so on Pettifer

00:00:38,050 --> 00:00:43,929
system for improving out us out of banda

00:00:41,370 --> 00:00:46,359
monitoring and also combine it with meet

00:00:43,929 --> 00:00:49,059
bit big data technology for supporting

00:00:46,359 --> 00:00:52,239
the data center itself monitoring then i

00:00:49,059 --> 00:00:54,370
will go through the dissolution and the

00:00:52,239 --> 00:00:56,859
approach that we take towards using

00:00:54,370 --> 00:00:58,989
combining with data and hey I for

00:00:56,859 --> 00:01:01,809
anomaly detection and center automation

00:00:58,989 --> 00:01:03,809
in this purpose and then I will briefly

00:01:01,809 --> 00:01:07,090
discuss our future works and direction

00:01:03,809 --> 00:01:08,590
so what is that the center automation so

00:01:07,090 --> 00:01:10,329
in this picture what I depict is the

00:01:08,590 --> 00:01:13,060
fact that you have a data center in

00:01:10,329 --> 00:01:15,700
which we have several clusters and racks

00:01:13,060 --> 00:01:17,890
all of the ML sensors have our

00:01:15,700 --> 00:01:19,390
information that can be collected then

00:01:17,890 --> 00:01:22,119
in the same room you have a crack

00:01:19,390 --> 00:01:24,640
cooling system PDU and environmental

00:01:22,119 --> 00:01:26,109
sensors so what we need the - in order

00:01:24,640 --> 00:01:27,909
to automate your data center you can

00:01:26,109 --> 00:01:31,810
leverage all these de sensors to create

00:01:27,909 --> 00:01:33,789
a aim an infrastructure in which all the

00:01:31,810 --> 00:01:36,640
information are feeding through these

00:01:33,789 --> 00:01:39,880
are an ecosystem on software's which

00:01:36,640 --> 00:01:43,119
allows you to actually learn extract

00:01:39,880 --> 00:01:45,399
information from the from the data

00:01:43,119 --> 00:01:48,729
center itself and combine them with

00:01:45,399 --> 00:01:50,950
machine learning tools performance

00:01:48,729 --> 00:01:53,350
analytics tools in order to end data

00:01:50,950 --> 00:01:55,359
visualization order to improve improve

00:01:53,350 --> 00:01:58,659
the efficiency of the systems where the

00:01:55,359 --> 00:02:00,340
target is trying to improve its energy

00:01:58,659 --> 00:02:02,049
efficiency but also the usage of the

00:02:00,340 --> 00:02:06,249
resources and the performance of the

00:02:02,049 --> 00:02:08,140
system itself the use case scenario that

00:02:06,249 --> 00:02:11,560
we envision for that central summation

00:02:08,140 --> 00:02:14,470
are two of them are depicted here we

00:02:11,560 --> 00:02:15,910
have on there on the Left we have the

00:02:14,470 --> 00:02:18,190
case in which we have our computing

00:02:15,910 --> 00:02:20,090
nodes which is own internal component we

00:02:18,190 --> 00:02:25,459
have CPU accelerator GPU

00:02:20,090 --> 00:02:27,230
and and we have normally user the blue

00:02:25,459 --> 00:02:29,720
and the red which run application on the

00:02:27,230 --> 00:02:32,330
undeceive on this on the our door then

00:02:29,720 --> 00:02:35,660
we have a Malan tation eight users which

00:02:32,330 --> 00:02:38,300
may attack the system we have a we have

00:02:35,660 --> 00:02:40,190
components mechanical components which

00:02:38,300 --> 00:02:43,489
may have faults which can decorate their

00:02:40,190 --> 00:02:45,560
performance and and we have the

00:02:43,489 --> 00:02:47,390
monitoring su tools that here the pic

00:02:45,560 --> 00:02:49,640
the deer so of standard tools for

00:02:47,390 --> 00:02:52,280
monitoring the system our coarse grain

00:02:49,640 --> 00:02:54,470
in sense that they do monitor things the

00:02:52,280 --> 00:02:56,780
taverns which happens really with the

00:02:54,470 --> 00:02:58,819
not a frequency and so you are really

00:02:56,780 --> 00:03:01,670
not too much capable of discriminating

00:02:58,819 --> 00:03:03,500
the different heavens and things which

00:03:01,670 --> 00:03:05,269
happens whereas with fine grained

00:03:03,500 --> 00:03:08,030
monitoring what you aim for is to have a

00:03:05,269 --> 00:03:10,099
higher resolution on the on the on the

00:03:08,030 --> 00:03:12,470
telemetry of the system which allows you

00:03:10,099 --> 00:03:15,470
to be capable of discriminating things

00:03:12,470 --> 00:03:18,200
so separating events and effects on the

00:03:15,470 --> 00:03:22,099
system and the purpose of this is that

00:03:18,200 --> 00:03:24,109
to command is to then enable to enable

00:03:22,099 --> 00:03:26,840
actually the capability of being capable

00:03:24,109 --> 00:03:29,180
of evaluating if you're not computing

00:03:26,840 --> 00:03:31,310
nodes is in spec of out of spec if it's

00:03:29,180 --> 00:03:33,410
misconfigured or if it's aging and

00:03:31,310 --> 00:03:35,450
wearing out and also enable predictive

00:03:33,410 --> 00:03:37,310
maintenance on the other side you have

00:03:35,450 --> 00:03:38,690
instead another usage scenario for at

00:03:37,310 --> 00:03:40,220
the center automation which is the fact

00:03:38,690 --> 00:03:43,340
in which you have a data center you have

00:03:40,220 --> 00:03:46,040
several user which submit jobs and use

00:03:43,340 --> 00:03:48,350
your system and then you have physical

00:03:46,040 --> 00:03:50,150
constraints which come from there from

00:03:48,350 --> 00:03:52,400
environmental which are actual may be

00:03:50,150 --> 00:03:55,280
the fact that you want to create the

00:03:52,400 --> 00:03:58,040
system under a specific power envelope

00:03:55,280 --> 00:04:00,019
so he wants to among time be capable of

00:03:58,040 --> 00:04:03,709
operating your entire data center within

00:04:00,019 --> 00:04:06,859
a specific total power budget which may

00:04:03,709 --> 00:04:09,799
change in times and maybe in from Hinda

00:04:06,859 --> 00:04:11,510
this can happen due to fact that you

00:04:09,799 --> 00:04:13,310
have a transition installations in your

00:04:11,510 --> 00:04:14,900
data center so you have that you need to

00:04:13,310 --> 00:04:17,060
shift the power budget to a new system

00:04:14,900 --> 00:04:18,919
which is coming up which is coming you

00:04:17,060 --> 00:04:20,510
have may have grid service level

00:04:18,919 --> 00:04:22,639
agreement which tells you that you

00:04:20,510 --> 00:04:25,580
cannot consume a higher amount of power

00:04:22,639 --> 00:04:29,690
than if there are some natural disaster

00:04:25,580 --> 00:04:30,800
or some shortage of of green energy you

00:04:29,690 --> 00:04:33,140
need to reduce your total power

00:04:30,800 --> 00:04:34,490
consumption and what you want to do is

00:04:33,140 --> 00:04:37,340
that trying to

00:04:34,490 --> 00:04:38,510
have the highest service level on your

00:04:37,340 --> 00:04:41,300
data centers we want to actually

00:04:38,510 --> 00:04:42,830
maximize your productivity within these

00:04:41,300 --> 00:04:44,780
constraints which are happening so you

00:04:42,830 --> 00:04:47,180
need automated tool for supporting your

00:04:44,780 --> 00:04:49,010
decision it cannot be just the system in

00:04:47,180 --> 00:04:50,720
straighter that changes things in your C

00:04:49,010 --> 00:04:52,640
in your in your infrastructure because

00:04:50,720 --> 00:04:54,920
it cannot cope with all these complex

00:04:52,640 --> 00:04:57,080
constraints and similarly if you want to

00:04:54,920 --> 00:04:59,210
actually verify of your large-scale

00:04:57,080 --> 00:05:01,340
system if you have a Mis configuration

00:04:59,210 --> 00:05:03,050
if you have a things which are wearing

00:05:01,340 --> 00:05:06,800
out you cannot just provide a simple

00:05:03,050 --> 00:05:08,180
visualization because then because it

00:05:06,800 --> 00:05:10,220
will take time a lot of time it's

00:05:08,180 --> 00:05:12,380
actually taking time and a lot of effort

00:05:10,220 --> 00:05:15,430
economical I for Tina in Stata Center

00:05:12,380 --> 00:05:19,160
autumn in data center system to actually

00:05:15,430 --> 00:05:20,720
be capable to money to look at the

00:05:19,160 --> 00:05:23,120
traces that you are monitoring if you

00:05:20,720 --> 00:05:24,980
have a lot of data and be capable to

00:05:23,120 --> 00:05:26,210
extract useful information from that you

00:05:24,980 --> 00:05:28,190
spend a lot of time and system

00:05:26,210 --> 00:05:31,760
administration the system administrator

00:05:28,190 --> 00:05:33,980
in digging through a ticket from a user

00:05:31,760 --> 00:05:35,960
that tells you that ok my job wasn't

00:05:33,980 --> 00:05:38,030
running as expected then you have to dig

00:05:35,960 --> 00:05:39,830
down discover if was the job which was

00:05:38,030 --> 00:05:41,660
compiled not in the proper way or was

00:05:39,830 --> 00:05:43,370
the order it was out of spec it was a

00:05:41,660 --> 00:05:44,930
Mis configuration in the node that when

00:05:43,370 --> 00:05:47,420
I have updated the filmer in all my

00:05:44,930 --> 00:05:49,190
nodes one node was not really responsive

00:05:47,420 --> 00:05:50,570
in the network and of with and was

00:05:49,190 --> 00:05:52,430
staying at the old theorem are so you

00:05:50,570 --> 00:05:53,980
don't really get all these things and

00:05:52,430 --> 00:05:56,620
you want to do it in automated fashion

00:05:53,980 --> 00:05:58,880
so if we do a data center automation

00:05:56,620 --> 00:06:00,680
based on the monitoring system we said

00:05:58,880 --> 00:06:02,260
before our components are we have the

00:06:00,680 --> 00:06:04,990
nodes we have the infrastructure

00:06:02,260 --> 00:06:07,520
components in the system and then we do

00:06:04,990 --> 00:06:10,100
we do collect all the hold information

00:06:07,520 --> 00:06:12,380
and we send it to a centralized mind

00:06:10,100 --> 00:06:15,440
which is collecting all the data and

00:06:12,380 --> 00:06:17,780
with analytics ok nice good however we

00:06:15,440 --> 00:06:22,640
have bottlenecks as the system scale in

00:06:17,780 --> 00:06:24,860
terms of perform the system and if you

00:06:22,640 --> 00:06:26,570
start to even increase the frequency at

00:06:24,860 --> 00:06:29,600
which we monitor events you will

00:06:26,570 --> 00:06:31,880
silently handed up in having problems in

00:06:29,600 --> 00:06:34,130
collecting all the data you can't have

00:06:31,880 --> 00:06:35,480
over it in the bandwidth through that to

00:06:34,130 --> 00:06:36,800
collect all the data and then you start

00:06:35,480 --> 00:06:38,870
to have of software overheads in

00:06:36,800 --> 00:06:41,510
collecting this data from the computing

00:06:38,870 --> 00:06:43,900
resources a different approach will be

00:06:41,510 --> 00:06:46,790
to actually leverage

00:06:43,900 --> 00:06:47,910
head computing in some sense you can

00:06:46,790 --> 00:06:51,600
actually

00:06:47,910 --> 00:06:53,970
try to add computational resources to

00:06:51,600 --> 00:06:56,160
your node that are capable just to take

00:06:53,970 --> 00:06:58,530
care of the father of a small

00:06:56,160 --> 00:07:00,480
computation for the system that ISM the

00:06:58,530 --> 00:07:02,490
Agron that you train on there on the

00:07:00,480 --> 00:07:04,500
largest amount of data and then try to

00:07:02,490 --> 00:07:06,950
actually distribute them to the process

00:07:04,500 --> 00:07:10,710
to them to the node itself to actually

00:07:06,950 --> 00:07:13,440
just just actually communicate to the

00:07:10,710 --> 00:07:15,240
centralized entity the events so if

00:07:13,440 --> 00:07:17,430
you're if you're if you're misconfigured

00:07:15,240 --> 00:07:20,100
or if you have if you're aging out so

00:07:17,430 --> 00:07:22,520
you need to transfer the knowledge down

00:07:20,100 --> 00:07:27,540
to the compute node itself without

00:07:22,520 --> 00:07:29,370
harming their own performance so let's

00:07:27,540 --> 00:07:32,550
see how we do it on the Abaddon on the

00:07:29,370 --> 00:07:34,710
double system so that the system is a is

00:07:32,550 --> 00:07:36,960
a system that was a has been installed a

00:07:34,710 --> 00:07:38,550
steer in Naknek in Italy has been ranked

00:07:36,960 --> 00:07:41,910
in top five and in five and there is an

00:07:38,550 --> 00:07:45,060
open power system it is based on the IBM

00:07:41,910 --> 00:07:47,550
Minsky system it has a true power aides

00:07:45,060 --> 00:07:50,480
to power it sees through power power

00:07:47,550 --> 00:07:53,670
eight sockets and for NVIDIA P hundred

00:07:50,480 --> 00:07:56,490
system it is liquid cooled and and it

00:07:53,670 --> 00:07:58,200
has an additional board component that

00:07:56,490 --> 00:07:59,760
has been designed in University of

00:07:58,200 --> 00:08:01,980
Bologna in collaboration with it with

00:07:59,760 --> 00:08:05,100
the with the THD auric to actually

00:08:01,980 --> 00:08:07,230
improve the self-awareness of the node

00:08:05,100 --> 00:08:09,210
and to enable fine grained power and

00:08:07,230 --> 00:08:12,390
performance monitoring and analytics on

00:08:09,210 --> 00:08:15,450
the node itself this component this

00:08:12,390 --> 00:08:17,220
out-of-band component is actually a an

00:08:15,450 --> 00:08:19,919
embedded system which is connected to

00:08:17,220 --> 00:08:21,390
the to the node itself so we have added

00:08:19,919 --> 00:08:23,850
one of the system in each of the nodes

00:08:21,390 --> 00:08:26,310
and is actually itself for several

00:08:23,850 --> 00:08:28,200
purposes so in Powys the power measuring

00:08:26,310 --> 00:08:30,900
block so it's all frequency power

00:08:28,200 --> 00:08:34,950
measurement on the node itself it is

00:08:30,900 --> 00:08:37,110
based on on a Bedan system and are an

00:08:34,950 --> 00:08:39,270
embedded system which is capable to do

00:08:37,110 --> 00:08:41,400
additional tasks on top of the one that

00:08:39,270 --> 00:08:43,530
just monitoring the power such as also

00:08:41,400 --> 00:08:45,750
connecting to the all the performance

00:08:43,530 --> 00:08:48,540
metrics which are within the nodes and

00:08:45,750 --> 00:08:49,950
collecting them it it has it does

00:08:48,540 --> 00:08:51,870
support

00:08:49,950 --> 00:08:54,570
scaleable communication interfaces such

00:08:51,870 --> 00:08:57,240
as the MQTT protocol insulator and it

00:08:54,570 --> 00:08:59,550
also support to offload some of the

00:08:57,240 --> 00:09:01,470
analytics directly on the edge on the

00:08:59,550 --> 00:09:03,690
dismal system to

00:09:01,470 --> 00:09:06,390
slowly process the data which are

00:09:03,690 --> 00:09:13,110
collected and just generate events based

00:09:06,390 --> 00:09:14,370
on let's say based on on be capable of

00:09:13,110 --> 00:09:17,460
actually understanding if you have

00:09:14,370 --> 00:09:19,650
anomalies which are going on the system

00:09:17,460 --> 00:09:21,420
is actually looking at the finder in

00:09:19,650 --> 00:09:22,590
performance is pretty unique so it's

00:09:21,420 --> 00:09:25,260
capable of sampling the power

00:09:22,590 --> 00:09:27,810
consumption of 50 keywords per second so

00:09:25,260 --> 00:09:29,420
we have 50 kilos of 50 kilos sample per

00:09:27,810 --> 00:09:32,940
second of the power consumption is eye

00:09:29,420 --> 00:09:35,310
accuracy so super precision and with the

00:09:32,940 --> 00:09:38,190
supporting time synchronous protocol so

00:09:35,310 --> 00:09:40,740
NTP mptp so we can have that actually

00:09:38,190 --> 00:09:42,780
all the nodes in all the nodes of the

00:09:40,740 --> 00:09:44,670
cluster we can actually align all the

00:09:42,780 --> 00:09:46,770
power consumption all the power

00:09:44,670 --> 00:09:48,210
consumption readings between all the

00:09:46,770 --> 00:09:50,040
nodes of the system so we can actually

00:09:48,210 --> 00:09:53,100
have a kind of a live of Shila scope for

00:09:50,040 --> 00:09:54,840
the entire cluster and it's better with

00:09:53,100 --> 00:09:57,900
compared to state-of-the-art monitoring

00:09:54,840 --> 00:10:00,120
system of competitive product okay but

00:09:57,900 --> 00:10:04,260
nice what we do with this high frequency

00:10:00,120 --> 00:10:07,110
power monitoring so first an inside what

00:10:04,260 --> 00:10:09,090
you can do we can gain by actually using

00:10:07,110 --> 00:10:11,790
a frequency a power monitoring is the

00:10:09,090 --> 00:10:15,630
fact that now we can actually we can

00:10:11,790 --> 00:10:17,730
actually do a lively spectral analysis

00:10:15,630 --> 00:10:20,120
of the of the power consumption we can

00:10:17,730 --> 00:10:22,800
actually we can actually discriminate

00:10:20,120 --> 00:10:24,090
application which runs on the different

00:10:22,800 --> 00:10:25,680
different application which runs on the

00:10:24,090 --> 00:10:27,690
node by looking at their spectral

00:10:25,680 --> 00:10:29,250
components so we actually have

00:10:27,690 --> 00:10:31,350
signatures of application which are

00:10:29,250 --> 00:10:33,570
running on the system this is actually

00:10:31,350 --> 00:10:35,010
an interesting features because if one

00:10:33,570 --> 00:10:36,930
of the two application is the normal

00:10:35,010 --> 00:10:38,340
application the other is an attacker we

00:10:36,930 --> 00:10:40,440
can actually have a lightweight

00:10:38,340 --> 00:10:42,120
intrusion detection system in our in our

00:10:40,440 --> 00:10:44,040
in our fridge of the node which can

00:10:42,120 --> 00:10:46,770
actually enable the detection of

00:10:44,040 --> 00:10:49,560
unwanted usage of the compute node so

00:10:46,770 --> 00:10:52,500
this can be an additional an additional

00:10:49,560 --> 00:10:56,280
actually way of checking if the system

00:10:52,500 --> 00:10:59,010
is doing what is expected to do okay but

00:10:56,280 --> 00:11:01,170
okay now you can compute an FFT on the

00:10:59,010 --> 00:11:03,240
power trace that you can monitor lively

00:11:01,170 --> 00:11:06,990
on the system but can I automate all

00:11:03,240 --> 00:11:09,750
this behavior so this goes to them to

00:11:06,990 --> 00:11:12,120
combine in Big Data and they I enabled

00:11:09,750 --> 00:11:13,560
automated attack anomaly detection on

00:11:12,120 --> 00:11:15,210
the system this is done through them

00:11:13,560 --> 00:11:17,250
through the to the monitor

00:11:15,210 --> 00:11:19,380
framework which which that we added to

00:11:17,250 --> 00:11:20,910
the davido system which is an

00:11:19,380 --> 00:11:23,010
open-source project of M University of

00:11:20,910 --> 00:11:25,980
Bologna is called examine which is

00:11:23,010 --> 00:11:28,800
actually combining mqtt streams of

00:11:25,980 --> 00:11:33,840
sensors which with scaleable big data

00:11:28,800 --> 00:11:35,820
technology such as such as we have

00:11:33,840 --> 00:11:38,820
Cassandra the basis Kairos DB time

00:11:35,820 --> 00:11:40,980
traces and then connecting on top of

00:11:38,820 --> 00:11:44,430
these of these components we connect

00:11:40,980 --> 00:11:46,710
actually hey I told SPARC Kafka tensor

00:11:44,430 --> 00:11:49,320
flow to actually do automated analytics

00:11:46,710 --> 00:11:51,810
on this data so we have the switch of

00:11:49,320 --> 00:11:55,110
the sensor location is publishing data

00:11:51,810 --> 00:11:57,690
and then we have a capability of a heavy

00:11:55,110 --> 00:12:00,150
kind of flexible and scalable storage of

00:11:57,690 --> 00:12:03,450
this data for accreting datasets based

00:12:00,150 --> 00:12:05,940
on the live as vital signs of the data

00:12:03,450 --> 00:12:08,970
center and then do analytics on top of

00:12:05,940 --> 00:12:12,270
that and QT 2 is a good tool for sending

00:12:08,970 --> 00:12:14,190
for propagating telemetry data because

00:12:12,270 --> 00:12:16,080
it's really lightweight in terms of the

00:12:14,190 --> 00:12:17,520
protocols and it allows you to

00:12:16,080 --> 00:12:19,470
communicate create communication

00:12:17,520 --> 00:12:22,440
channels which are effective in this in

00:12:19,470 --> 00:12:24,020
the system so how we do it in examine is

00:12:22,440 --> 00:12:26,540
basically we have different sensors

00:12:24,020 --> 00:12:29,610
these different collectors of data

00:12:26,540 --> 00:12:31,440
position along the death system and then

00:12:29,610 --> 00:12:32,670
we have data brokers which collects all

00:12:31,440 --> 00:12:35,100
the information in communication

00:12:32,670 --> 00:12:37,710
channels and these are these are

00:12:35,100 --> 00:12:39,450
actually inserted in table in the in the

00:12:37,710 --> 00:12:43,230
Cassandra databases that then can be

00:12:39,450 --> 00:12:47,070
used later for a for a for a for enable

00:12:43,230 --> 00:12:49,020
automation on the system if you look at

00:12:47,070 --> 00:12:52,040
how we configure it on that we do we

00:12:49,020 --> 00:12:54,360
have data all the node at its own

00:12:52,040 --> 00:12:55,830
embedded system monitoring board

00:12:54,360 --> 00:12:58,680
which is based on the big urban black

00:12:55,830 --> 00:13:00,510
platform and is collecting data both ah

00:12:58,680 --> 00:13:03,360
high frequency power date and also

00:13:00,510 --> 00:13:07,110
telemetry sensor data and this is a

00:13:03,360 --> 00:13:09,680
these are propagated to to a database

00:13:07,110 --> 00:13:12,870
which runs on them on a management node

00:13:09,680 --> 00:13:14,640
and when together with the sense with

00:13:12,870 --> 00:13:17,400
the date of the set of the computing

00:13:14,640 --> 00:13:19,470
node we have also have slurm so the job

00:13:17,400 --> 00:13:21,390
scheduler which also propagates Evan so

00:13:19,470 --> 00:13:24,120
if the job is starting is being required

00:13:21,390 --> 00:13:26,790
they request of the jobs and insert this

00:13:24,120 --> 00:13:29,130
in the database and also we also we also

00:13:26,790 --> 00:13:31,230
combine we also collect the

00:13:29,130 --> 00:13:34,680
coming from the IPMI sensors and also

00:13:31,230 --> 00:13:36,930
from the cooling system so we have

00:13:34,680 --> 00:13:38,760
annalistic collection of data in a

00:13:36,930 --> 00:13:40,110
central point which allows to correlate

00:13:38,760 --> 00:13:43,590
between different things which are

00:13:40,110 --> 00:13:46,320
happening and then we can based on that

00:13:43,590 --> 00:13:48,780
we can create an elite we can we can use

00:13:46,320 --> 00:13:51,180
this source of data for analytics this

00:13:48,780 --> 00:13:53,970
we can do actually by combining the data

00:13:51,180 --> 00:13:58,650
sources with with the panda's data frame

00:13:53,970 --> 00:14:01,050
and then enabling with also spark pi

00:13:58,650 --> 00:14:02,270
towards tensorflow and kafka tools to

00:14:01,050 --> 00:14:04,830
analyze the data that we have collected

00:14:02,270 --> 00:14:07,470
to create actually an entire framework

00:14:04,830 --> 00:14:09,510
for anomaly detection so that's one

00:14:07,470 --> 00:14:11,820
coming to the next slide so how is

00:14:09,510 --> 00:14:14,610
always working so we have different

00:14:11,820 --> 00:14:16,530
nodes which have which which have

00:14:14,610 --> 00:14:18,150
additional components with them which

00:14:16,530 --> 00:14:19,770
are capable of collecting all the

00:14:18,150 --> 00:14:21,780
information with a frequency that our

00:14:19,770 --> 00:14:23,910
telemetry data of the computing nodes

00:14:21,780 --> 00:14:25,800
this goes to the monitor infrastructure

00:14:23,910 --> 00:14:29,220
that just described before and all the

00:14:25,800 --> 00:14:31,950
information are are collected then we

00:14:29,220 --> 00:14:34,110
have we have the capability of keeping a

00:14:31,950 --> 00:14:36,600
small buffer or a frequency data and to

00:14:34,110 --> 00:14:39,740
create some historical traces and based

00:14:36,600 --> 00:14:43,140
on these historical traces we do train

00:14:39,740 --> 00:14:45,000
deep learning models for actually which

00:14:43,140 --> 00:14:47,160
represents the data which we have

00:14:45,000 --> 00:14:49,380
collected then once we have trained the

00:14:47,160 --> 00:14:51,360
model we can offload the model that we

00:14:49,380 --> 00:14:53,220
have directly on this component that we

00:14:51,360 --> 00:14:56,070
have to the nodes which can actually run

00:14:53,220 --> 00:14:57,890
the interference of the based on the on

00:14:56,070 --> 00:15:01,950
the live data which are collected and

00:14:57,890 --> 00:15:04,110
actually understand if the node is

00:15:01,950 --> 00:15:06,300
behaving normally or if it's an under an

00:15:04,110 --> 00:15:09,420
anomaly does it work

00:15:06,300 --> 00:15:12,060
so we have implemented it on da vida so

00:15:09,420 --> 00:15:13,680
we use the as a component for our for

00:15:12,060 --> 00:15:15,420
doing anomaly detection we started with

00:15:13,680 --> 00:15:17,820
the simplest tool which is a hangout

00:15:15,420 --> 00:15:20,250
encoder now the encoder is nice because

00:15:17,820 --> 00:15:24,720
it's a enable you to have a set of

00:15:20,250 --> 00:15:26,250
features and and recreate recreate the

00:15:24,720 --> 00:15:27,600
representation of this feature by

00:15:26,250 --> 00:15:29,790
actually trying to understand the

00:15:27,600 --> 00:15:32,250
correlation between the these features

00:15:29,790 --> 00:15:34,050
and if you train these madhi's are these

00:15:32,250 --> 00:15:37,110
diffuse and out encoder you are capable

00:15:34,050 --> 00:15:38,970
basically how having represented of

00:15:37,110 --> 00:15:41,700
trying to estimate based on the feature

00:15:38,970 --> 00:15:42,899
the same features and if you're if

00:15:41,700 --> 00:15:44,550
you're on this if you're a press

00:15:42,899 --> 00:15:46,559
patience so if your if your model is

00:15:44,550 --> 00:15:48,660
good you are actually capturing the

00:15:46,559 --> 00:15:50,639
behavior of the system and this you can

00:15:48,660 --> 00:15:52,680
do it without knowing the physics and

00:15:50,639 --> 00:15:54,269
the laws which combines all the sensor

00:15:52,680 --> 00:15:57,059
so it's done automatically by the tool

00:15:54,269 --> 00:16:00,269
so the idea is that an out encoder is

00:15:57,059 --> 00:16:03,439
capable to recreate and seen input

00:16:00,269 --> 00:16:08,149
generation may between unseen inputs and

00:16:03,439 --> 00:16:11,660
so if is capable of recreating the

00:16:08,149 --> 00:16:15,119
unseen inputs if this one are actually

00:16:11,660 --> 00:16:18,360
behaving correctly but if this is not

00:16:15,119 --> 00:16:21,779
gonna be capable of actually under

00:16:18,360 --> 00:16:24,629
recreate the input if the system is not

00:16:21,779 --> 00:16:26,730
behaved in them in the proper condition

00:16:24,629 --> 00:16:28,350
as it has been trained so basically that

00:16:26,730 --> 00:16:30,209
is the two trained out encoder with a

00:16:28,350 --> 00:16:32,910
normal behavior of an AI performance

00:16:30,209 --> 00:16:34,379
computing system and to reconstruct and

00:16:32,910 --> 00:16:36,480
to use the reconstruction error to

00:16:34,379 --> 00:16:38,339
detect if there is an anomalies in fact

00:16:36,480 --> 00:16:40,529
so when you try to build an anomaly

00:16:38,339 --> 00:16:41,970
detection system an HPC system the

00:16:40,529 --> 00:16:44,550
problem that you usually have is the

00:16:41,970 --> 00:16:46,410
fact that you have a lot of data of the

00:16:44,550 --> 00:16:47,970
normal behaviors but you don't have a

00:16:46,410 --> 00:16:49,679
lot of data on the anomalies because the

00:16:47,970 --> 00:16:51,540
the anomalies that are happening are are

00:16:49,679 --> 00:16:53,339
really faults that happen in the system

00:16:51,540 --> 00:16:54,959
so they're really scarce there is parts

00:16:53,339 --> 00:16:57,120
in time and so we don't collect them at

00:16:54,959 --> 00:16:59,910
all so that is that you can create

00:16:57,120 --> 00:17:02,249
models of the normal behaviors that will

00:16:59,910 --> 00:17:04,500
tells you then when you are outs of the

00:17:02,249 --> 00:17:07,589
normal behaviors and and then you use

00:17:04,500 --> 00:17:11,189
them this information to detect if the

00:17:07,589 --> 00:17:12,779
system is behaving correctly or not so

00:17:11,189 --> 00:17:14,429
how we did it so basically create a

00:17:12,779 --> 00:17:17,699
really simple out encoder with just

00:17:14,429 --> 00:17:20,279
relay here and which and we train it on

00:17:17,699 --> 00:17:21,870
two months of normal date of davido so

00:17:20,279 --> 00:17:23,549
we just have all this our infrastructure

00:17:21,870 --> 00:17:25,919
we just have two months of normal

00:17:23,549 --> 00:17:29,070
behavior of the davida nodes and we and

00:17:25,919 --> 00:17:31,880
we train a model which is capable

00:17:29,070 --> 00:17:33,870
actually to tell it's easy to to

00:17:31,880 --> 00:17:36,419
understand the correlation between the

00:17:33,870 --> 00:17:39,570
sensors to create a model a certain step

00:17:36,419 --> 00:17:41,970
of the understanding of the sensors then

00:17:39,570 --> 00:17:43,770
how do we validate we validate by

00:17:41,970 --> 00:17:46,100
artificially inject anomalies on the

00:17:43,770 --> 00:17:49,049
nodes and these anomalies are actually

00:17:46,100 --> 00:17:50,700
can be of different sources in this

00:17:49,049 --> 00:17:52,620
presentation focus with anomalies which

00:17:50,700 --> 00:17:54,750
are actually miss configuration of the

00:17:52,620 --> 00:17:57,200
node so in which I actually four by

00:17:54,750 --> 00:17:59,360
artificially ever

00:17:57,200 --> 00:18:01,520
I have become misconfigure the node such

00:17:59,360 --> 00:18:04,010
as I have placed a different power

00:18:01,520 --> 00:18:06,830
governors in my operating system or

00:18:04,010 --> 00:18:08,420
histories aim or if there is a problem

00:18:06,830 --> 00:18:10,670
in the system such as and the power

00:18:08,420 --> 00:18:13,030
governor cannot work so have two

00:18:10,670 --> 00:18:16,220
anomalies which are two different

00:18:13,030 --> 00:18:22,490
settings and I'm trying to apply them

00:18:16,220 --> 00:18:24,500
the the anomaly detection algorithm I my

00:18:22,490 --> 00:18:27,080
out encoders if we can actually detect

00:18:24,500 --> 00:18:29,420
it so in a live system so these are

00:18:27,080 --> 00:18:31,730
unseen data for the system however for

00:18:29,420 --> 00:18:33,500
my computing nodes hi ever I'm looking

00:18:31,730 --> 00:18:36,590
at the receiver so the capability of the

00:18:33,500 --> 00:18:39,380
model to actually predict the real input

00:18:36,590 --> 00:18:41,630
traces and if the system is behaving

00:18:39,380 --> 00:18:43,370
correctly have really low residual in

00:18:41,630 --> 00:18:46,010
Mahara ISM but if I have injecting

00:18:43,370 --> 00:18:47,300
faults I can see that actually then the

00:18:46,010 --> 00:18:48,920
model is no more capable for

00:18:47,300 --> 00:18:49,850
representing the truth of the system

00:18:48,920 --> 00:18:53,000
because the system is behaving

00:18:49,850 --> 00:18:54,559
differently and I can can easily

00:18:53,000 --> 00:18:57,020
recognize that the system is behaving

00:18:54,559 --> 00:18:59,120
not in the proper condition so now I can

00:18:57,020 --> 00:19:02,240
have an automatic flag for anomalies

00:18:59,120 --> 00:19:03,620
which happening in the system so it's

00:19:02,240 --> 00:19:04,880
behaving good so we tested in several

00:19:03,620 --> 00:19:07,700
nodes of the Aveda

00:19:04,880 --> 00:19:09,290
so we have a good prediction of the

00:19:07,700 --> 00:19:12,050
normal and the normally behaviour and

00:19:09,290 --> 00:19:14,150
you can also move this computation so

00:19:12,050 --> 00:19:15,830
the inferences within them embedded

00:19:14,150 --> 00:19:18,410
computers that we have on each of the

00:19:15,830 --> 00:19:20,570
nodes so we can actually execute easily

00:19:18,410 --> 00:19:23,650
with the tensorflow on the beyond black

00:19:20,570 --> 00:19:25,520
then the in feelings with the 1111

00:19:23,650 --> 00:19:27,830
milliseconds so we can actually do it

00:19:25,520 --> 00:19:29,750
lively on the on the system we can

00:19:27,830 --> 00:19:32,720
actually use them to teetee to just

00:19:29,750 --> 00:19:35,000
propagate they if the system is behaving

00:19:32,720 --> 00:19:36,670
cop comfort is behaving normally of it

00:19:35,000 --> 00:19:38,650
is Mississippi is off it is

00:19:36,670 --> 00:19:41,330
misconfigured

00:19:38,650 --> 00:19:43,910
so this actually proof that it's

00:19:41,330 --> 00:19:46,820
possible to use the scaleable monitoring

00:19:43,910 --> 00:19:49,700
system powered by and the out-of-band

00:19:46,820 --> 00:19:52,330
monitoring system of them of the open

00:19:49,700 --> 00:19:55,429
power system to actually create

00:19:52,330 --> 00:19:57,590
automated way of the of data center

00:19:55,429 --> 00:19:59,750
automation structure to automate the

00:19:57,590 --> 00:20:01,610
work of system administrator in checking

00:19:59,750 --> 00:20:03,650
if the node is configured properly or

00:20:01,610 --> 00:20:06,290
not and this is just the beginning so we

00:20:03,650 --> 00:20:10,670
are working now in trying to improve

00:20:06,290 --> 00:20:12,440
this this technique so we are working in

00:20:10,670 --> 00:20:13,700
so we actually in conclusion what we

00:20:12,440 --> 00:20:15,620
have done we've presented that an

00:20:13,700 --> 00:20:17,810
approach which combined out of been

00:20:15,620 --> 00:20:19,810
monitoring big data and AI to enable

00:20:17,810 --> 00:20:22,430
data center automation will prove that

00:20:19,810 --> 00:20:24,440
our approach is a keeper for enable

00:20:22,430 --> 00:20:26,120
automated the anomaly detection on

00:20:24,440 --> 00:20:27,950
computing nodes and we are actually

00:20:26,120 --> 00:20:30,530
working on extending this approach to

00:20:27,950 --> 00:20:34,040
our security to try to see if we can use

00:20:30,530 --> 00:20:36,020
this approach to detect bad usage of the

00:20:34,040 --> 00:20:39,050
system not allowed users to the system

00:20:36,020 --> 00:20:40,730
also to see if we can actually how to

00:20:39,050 --> 00:20:44,090
make it out skipping tasks in the data

00:20:40,730 --> 00:20:46,910
centers and then we are exploring to to

00:20:44,090 --> 00:20:50,030
take advantage of the open BMC community

00:20:46,910 --> 00:20:52,880
to see if we can move part of desire of

00:20:50,030 --> 00:20:54,920
this solution to them close by to the

00:20:52,880 --> 00:20:56,870
computing node and also looking for a

00:20:54,920 --> 00:20:58,580
open order to see if we can improve the

00:20:56,870 --> 00:21:00,140
efficiency of the solution and we are

00:20:58,580 --> 00:21:03,350
looking of course for partnership to

00:21:00,140 --> 00:21:04,730
bring this solution empower 9 systems so

00:21:03,350 --> 00:21:07,460
this concludes my talks if you have

00:21:04,730 --> 00:21:09,650
question I'm here this is the team so I

00:21:07,460 --> 00:21:12,640
acknowledge to the team it's working

00:21:09,650 --> 00:21:12,640
with me in this topic

00:21:12,660 --> 00:21:17,489

YouTube URL: https://www.youtube.com/watch?v=SwcbxhhbdGM


